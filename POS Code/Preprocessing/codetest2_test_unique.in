from __future__ import division , print_function , unicode_literals \n 
\n 
from collections import OrderedDict \n 
from brainstorm . layers . base_layer import Layer \n 
from brainstorm . structure . buffer_structure import ( BufferStructure , \n 
StructureTemplate ) \n 
from brainstorm . structure . construction import ConstructionWrapper \n 
from brainstorm . utils import flatten_all_but_last \n 
def BatchNorm ( name = None , decay = 0.9 , epsilon = 1.0e-5 ) : \n 
return ConstructionWrapper . create ( BatchNormLayerImpl , \n 
name = name , \n 
decay = decay , \n 
epsilon = epsilon ) \n 
~~ class BatchNormLayerImpl ( Layer ) : \n 
~~~ expected_inputs = { : StructureTemplate ( , , ) } \n 
expected_kwargs = { , } \n 
def setup ( self , kwargs , in_shapes ) : \n 
~~~ self . epsilon = kwargs . get ( , 1.0e-5 ) \n 
self . decay = kwargs . get ( , 0.9 ) \n 
outputs = OrderedDict ( ) \n 
outputs [ ] = in_shapes [ ] \n 
parameters = OrderedDict ( ) \n 
buf = BufferStructure ( self . in_shapes [ ] . feature_shape [ - 1 ] ) \n 
parameters [ ] = buf \n 
internals = OrderedDict ( ) \n 
internals [ ] = buf \n 
internals [ ] = self . in_shapes [ ] \n 
return outputs , parameters , internals \n 
~~ def forward_pass ( self , buffers , training_pass = True ) : \n 
~~~ _h = self . handler \n 
sigma_b , centered , x_hat = buffers . internals \n 
gamma , beta , mu , sigma = buffers . parameters \n 
inputs = flatten_all_but_last ( buffers . inputs . default ) \n 
centered = flatten_all_but_last ( centered ) \n 
x_hat = flatten_all_but_last ( x_hat ) \n 
out = flatten_all_but_last ( buffers . outputs . default ) \n 
m = inputs . shape [ 0 ] \n 
if training_pass : \n 
_h . sum_t ( inputs , 0 , mu_b ) \n 
_h . mult_st ( - 1.0 / m , mu_b , mu_b ) \n 
_h . mult_st ( self . decay , mu , mu ) \n 
_h . mult_add_st ( 1.0 - self . decay , mu_b , mu ) \n 
mu = mu_b \n 
~~ _h . add_mv ( inputs , mu . reshape ( ( 1 , mu . size ) ) , centered ) \n 
_h . mult_tt ( centered , centered , centered2 ) \n 
_h . sum_t ( centered2 , 0 , sigma2 ) \n 
_h . sqrt_t ( sigma2 , sigma_b ) \n 
_h . mult_st ( self . decay , sigma , sigma ) \n 
_h . mult_add_st ( 1.0 - self . decay , sigma_b , sigma ) \n 
sigma = sigma_b \n 
~~ _h . divide_mv ( centered , sigma . reshape ( ( 1 , sigma . size ) ) , x_hat ) \n 
_h . mult_mv ( x_hat , gamma . reshape ( ( 1 , gamma . size ) ) , out ) \n 
_h . add_mv ( out , beta . reshape ( ( 1 , beta . size ) ) , out ) \n 
~~ def backward_pass ( self , buffers ) : \n 
gamma = buffers . parameters . gamma \n 
dgamma = buffers . gradients . gamma \n 
dbeta = buffers . gradients . beta \n 
outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n 
indeltas = flatten_all_but_last ( buffers . input_deltas . default ) \n 
m = outdeltas . shape [ 0 ] \n 
tmp = big_tmp \n 
dgamma_tmp = small_tmp \n 
_h . mult_tt ( outdeltas , x_hat , tmp ) \n 
_h . sum_t ( tmp , axis = 0 , out = dgamma_tmp ) \n 
_h . add_tt ( dgamma_tmp , dgamma , dgamma ) \n 
_h . mult_st ( 1 / m , dgamma_tmp , dgamma_tmp ) \n 
term1 = big_tmp \n 
_h . mult_mv ( x_hat , dgamma_tmp . reshape ( ( 1 , gamma . size ) ) , term1 ) \n 
dbeta_tmp = small_tmp \n 
_h . sum_t ( outdeltas , axis = 0 , out = dbeta_tmp ) \n 
_h . add_tt ( dbeta_tmp , dbeta , dbeta ) \n 
_h . mult_st ( 1 / m , dbeta_tmp , dbeta_tmp ) \n 
term2 = big_tmp \n 
term3 = big_tmp \n 
_h . subtract_tt ( outdeltas , term1 , term2 ) \n 
_h . subtract_mv ( term2 , dbeta_tmp . reshape ( ( 1 , dbeta . size ) ) , term3 ) \n 
coeff = small_tmp \n 
_h . divide_tt ( gamma , sigma_b , coeff ) \n 
term4 = big_tmp \n 
_h . mult_mv ( term3 , coeff . reshape ( ( 1 , coeff . size ) ) , term4 ) \n 
_h . add_tt ( term4 , indeltas , indeltas ) \n 
~~ ~~ from __future__ import division , print_function , unicode_literals \n 
import numpy as np \n 
from brainstorm . describable import Describable \n 
class Scorer ( Describable ) : \n 
~~~ def __init__ ( self , out_name = , targets_name = , mask_name = , \n 
name = None ) : \n 
~~~ self . out_name = out_name \n 
self . targets_name = targets_name \n 
self . mask_name = mask_name \n 
self . __name__ = name if name is not None else self . __class__ . __name__ \n 
~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ pass \n 
~~ @ staticmethod \n 
def aggregate ( errors ) : \n 
~~~ errors = np . array ( errors ) \n 
assert errors . ndim == 2 and errors . shape [ 1 ] == 2 \n 
return np . sum ( errors [ : , 1 ] ) / np . sum ( errors [ : , 0 ] ) \n 
~~ ~~ def gather_losses_and_scores ( net , scorers , scores , out_name = , \n 
targets_name = , mask_name = ) : \n 
~~~ ls = net . get_loss_values ( ) \n 
for name , loss in ls . items ( ) : \n 
~~~ scores [ name ] . append ( ( net . _buffer_manager . batch_size , loss ) ) \n 
~~ for sc in scorers : \n 
~~~ name = sc . __name__ \n 
predicted = net . get ( sc . out_name or out_name or net . output_name ) \n 
true_labels = net . get_input ( sc . targets_name ) if sc . targets_name else net . get_input ( targets_name ) \n 
mask = net . get_input ( sc . mask_name ) if sc . mask_name else ( net . get_input ( mask_name ) if mask_name else None ) \n 
predicted = _flatten_all_but_last ( predicted ) \n 
true_labels = _flatten_all_but_last ( true_labels ) \n 
mask = _flatten_all_but_last ( mask ) \n 
weight = mask . sum ( ) if mask is not None else predicted . shape [ 0 ] \n 
scores [ name ] . append ( ( weight , sc ( true_labels , predicted , mask ) ) ) \n 
~~ ~~ def aggregate_losses_and_scores ( scores , net , scorers ) : \n 
~~~ results = OrderedDict ( ) \n 
for name in net . get_loss_values ( ) : \n 
~~~ results [ name ] = _weighted_average ( scores [ name ] ) \n 
~~~ results [ sc . __name__ ] = sc . aggregate ( scores [ sc . __name__ ] ) \n 
~~ return results \n 
~~ class Accuracy ( Scorer ) : \n 
~~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ if predicted . shape [ 1 ] > 1 : \n 
~~~ predicted = predicted . argmax ( 1 ) . reshape ( - 1 , 1 ) \n 
~~ correct = ( predicted == true_labels ) . astype ( np . float ) \n 
if mask is not None : \n 
~~~ correct *= mask \n 
~~ return np . sum ( correct ) \n 
~~ ~~ class Hamming ( Scorer ) : \n 
~~~ def __init__ ( self , threshold = 0.5 , out_name = , targets_name = , \n 
mask_name = , name = None ) : \n 
~~~ super ( Hamming , self ) . __init__ ( out_name , targets_name , mask_name , name ) \n 
self . threshold = threshold \n 
~~~ correct = np . logical_xor ( predicted < self . threshold , \n 
true_labels ) . astype ( np . float ) \n 
~~ return np . sum ( correct ) / true_labels . shape [ 1 ] \n 
~~ ~~ class MeanSquaredError ( Scorer ) : \n 
~~~ errors = ( true_labels - predicted ) ** 2 \n 
~~~ errors *= mask \n 
~~ return 0.5 * np . sum ( errors ) \n 
~~ ~~ def _flatten_all_but_last ( a ) : \n 
~~~ if a is None : \n 
~~~ return None \n 
~~ return a . reshape ( - 1 , a . shape [ - 1 ] ) \n 
~~ def _weighted_average ( errors ) : \n 
return np . sum ( errors [ : , 1 ] * errors [ : , 0 ] / np . sum ( errors [ : , 0 ] ) ) \n 
~~ from __future__ import division , print_function , unicode_literals \n 
import pytest \n 
import six \n 
from brainstorm . training . schedules import Exponential , Linear , MultiStep \n 
def test_linear ( ) : \n 
~~~ sch = Linear ( initial_value = 1.0 , final_value = 0.5 , num_changes = 5 ) \n 
epochs = [ 0 ] * 2 + [ 1 ] * 2 + [ 2 ] * 2 + [ 3 ] * 2 + [ 4 ] * 2 \n 
updates = range ( 10 ) \n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 , 1.0 , 0.9 , 0.9 , 0.8 , 0.8 , 0.7 , 0.7 , 0.6 , 0.6 ] \n 
assert values == [ 1.0 , 0.9 , 0.8 , 0.7 , 0.6 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ] \n 
values = [ sch ( epoch , update , , 3 , None , None , None ) \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.9 , 0.9 , 0.9 , 0.8 , 0.8 , 0.8 , 0.7 ] \n 
~~ def test_exponential ( ) : \n 
~~~ sch = Exponential ( initial_value = 1.0 , factor = 0.99 , minimum = 0.97 ) \n 
epochs = [ 0 ] * 4 + [ 1 ] * 4 + [ 2 ] * 4 \n 
updates = range ( 12 ) \n 
assert values == [ 1.0 ] * 4 + [ 0.99 ] * 4 + [ 0.99 * 0.99 ] * 4 \n 
assert values == [ 1.0 * ( 0.99 ** x ) for x in range ( 4 ) ] + [ 0.97 ] * 8 \n 
assert values == [ 1.0 ] * 3 + [ 0.99 ] * 3 + [ 0.9801 ] * 3 + [ 0.99 ** 3 ] * 3 \n 
~~ def test_multistep ( ) : \n 
~~~ sch = MultiStep ( initial_value = 1.0 , steps = [ 3 , 5 , 8 ] , \n 
values = [ 0.1 , 0.01 , 0.001 ] ) \n 
assert values == [ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.1 , 0.1 ] \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.01 , 0.01 , 0.01 , 0.001 , 0.001 ] \n 
with pytest . raises ( AssertionError ) : \n 
~~~ _ = sch ( 0 , 0 , , 3 , None , None , None ) \n 
# \n 
~~ ~~ import os \n 
import sys \n 
try : \n 
~~~ from unittest . mock import MagicMock \n 
~~ except ImportError : \n 
~~~ from mock import Mock as MagicMock \n 
~~ class Mock ( MagicMock ) : \n 
~~~ @ classmethod \n 
def __getattr__ ( cls , name ) : \n 
~~~ return Mock ( ) \n 
~~ ~~ MOCK_MODULES = [ , ] \n 
sys . modules . update ( ( mod_name , Mock ( ) ) for mod_name in MOCK_MODULES ) \n 
cwd = os . getcwd ( ) \n 
parent = os . path . dirname ( cwd ) \n 
sys . path . insert ( 0 , parent ) \n 
import brainstorm \n 
extensions = [ , , \n 
] \n 
templates_path = [ ] \n 
source_suffix = \n 
master_doc = \n 
project = \n 
copyright = \n 
version = brainstorm . __version__ \n 
release = brainstorm . __version__ \n 
exclude_patterns = [ ] \n 
pygments_style = \n 
on_rtd = os . environ . get ( , None ) == \n 
if not on_rtd : \n 
~~~ try : \n 
~~~ import sphinx_rtd_theme \n 
html_theme = \n 
html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n 
~~~ html_theme = \n 
~~ ~~ html_static_path = [ ] \n 
htmlhelp_basename = \n 
latex_elements = { \n 
} \n 
latex_documents = [ \n 
( , , , \n 
, ) , \n 
man_pages = [ \n 
[ ] , 1 ) \n 
texinfo_documents = [ \n 
, , , \n 
) , \n 
import theano . tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams \n 
from theano . tensor . nnet . conv import conv2d \n 
from theano . tensor . signal . downsample import max_pool_2d \n 
from theano . tensor . shared_randomstreams import RandomStreams \n 
from toolbox import * \n 
from modelbase import * \n 
class LM_gru ( ModelLMBase ) : \n 
~~~ def __init__ ( self , data , hp ) : \n 
~~~ super ( LM_gru , self ) . __init__ ( self . __class__ . __name__ , data , hp ) \n 
self . n_h = 256 \n 
self . dropout = 0.5 \n 
self . params = Parameters ( ) \n 
self . hiddenstates = Parameters ( ) \n 
n_tokens = self . data [ ] \n 
n_h = self . n_h \n 
scale = hp . init_scale \n 
gates = 3 \n 
with self . hiddenstates : \n 
~~~ b1_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
~~ if hp . load_model and os . path . isfile ( self . filename ) : \n 
~~~ self . params . load ( self . filename ) \n 
~~ else : \n 
~~~ with self . params : \n 
~~~ W_emb = shared_normal ( ( n_tokens , n_h ) , scale = scale ) \n 
W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b1 = shared_zeros ( ( n_h * gates ) ) \n 
W2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b2 = shared_zeros ( ( n_h * gates , ) ) \n 
~~ ~~ def lstm ( X , h , c , W , U , b ) : \n 
~~~ g_on = T . dot ( X , W ) + T . dot ( h , U ) + b \n 
i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n 
f_on = T . nnet . sigmoid ( g_on [ : , n_h : 2 * n_h ] ) \n 
o_on = T . nnet . sigmoid ( g_on [ : , 2 * n_h : 3 * n_h ] ) \n 
c = f_on * c + i_on * T . tanh ( g_on [ : , 3 * n_h : ] ) \n 
h = o_on * T . tanh ( c ) \n 
return h , c \n 
~~ def gru ( X , h , W , U , b ) : \n 
~~~ z_t = T . nnet . sigmoid ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
r_t = T . nnet . sigmoid ( T . dot ( X , W [ : , n_h : 2 * n_h ] ) + T . dot ( h , U [ : , n_h : 2 * n_h ] ) + b [ n_h : 2 * n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 2 * n_h : 3 * n_h ] ) + r_t * T . dot ( h , U [ : , 2 * n_h : 3 * n_h ] ) + b [ 2 * n_h : 3 * n_h ] ) \n 
return ( 1 - z_t ) * h + z_t * h_t \n 
~~ def sgru ( X , h , W , U , b ) : \n 
~~~ z_t = T . tanh ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 1 * n_h : 2 * n_h ] ) + T . dot ( h , U [ : , 1 * n_h : 2 * n_h ] ) + b [ 1 * n_h : 2 * n_h ] ) \n 
return z_t * h_t \n 
~~ def model ( x , p , p_dropout ) : \n 
~~~ input_size = x . shape [ 1 ] \n 
h0 = dropout ( h0 , p_dropout ) \n 
cost , h1 , h2 = [ 0. , b1_h , b2_h ] \n 
for t in xrange ( 0 , self . hp . seq_size ) : \n 
~~~ if t >= self . hp . warmup_size : \n 
~~~ pyx = softmax ( T . dot ( dropout ( h2 , p_dropout ) , T . transpose ( p . W_emb ) ) ) \n 
cost += T . sum ( T . nnet . categorical_crossentropy ( pyx , theano_one_hot ( x [ t ] , n_tokens ) ) ) \n 
~~ h1 = gru ( h0 [ t ] , h1 , p . W1 , p . V1 , p . b1 ) \n 
h2 = gru ( dropout ( h1 , p_dropout ) , h2 , p . W2 , p . V2 , p . b2 ) \n 
~~ h_updates = [ ( b1_h , h1 ) , ( b2_h , h2 ) ] \n 
return cost , h_updates \n 
~~ cost , h_updates = model ( self . X , self . params , self . dropout ) \n 
te_cost , te_h_updates = model ( self . X , self . params , 0. ) \n 
self . compile ( cost , te_cost , h_updates , te_h_updates ) \n 
~~ ~~ from collections import OrderedDict \n 
from . . import utils \n 
__all__ = [ \n 
"Layer" , \n 
"MergeLayer" , \n 
class Layer ( object ) : \n 
def __init__ ( self , incoming , name = None ) : \n 
~~~ if isinstance ( incoming , tuple ) : \n 
~~~ self . input_shape = incoming \n 
self . input_layer = None \n 
~~~ self . input_shape = incoming . output_shape \n 
self . input_layer = incoming \n 
~~ self . name = name \n 
self . params = OrderedDict ( ) \n 
self . get_output_kwargs = [ ] \n 
if any ( d is not None and d <= 0 for d in self . input_shape ) : \n 
~~~ raise ValueError ( ( \n 
self . input_shape , self . name ) ) \n 
~~ ~~ @ property \n 
def output_shape ( self ) : \n 
~~~ shape = self . get_output_shape_for ( self . input_shape ) \n 
if any ( isinstance ( s , T . Variable ) for s in shape ) : \n 
"dimensions." % ( self . __class__ . __name__ , shape ) ) \n 
~~ return shape \n 
~~ def get_params ( self , ** tags ) : \n 
result = list ( self . params . keys ( ) ) \n 
only = set ( tag for tag , value in tags . items ( ) if value ) \n 
if only : \n 
~~~ result = [ param for param in result \n 
if not ( only - self . params [ param ] ) ] \n 
~~ exclude = set ( tag for tag , value in tags . items ( ) if not value ) \n 
if exclude : \n 
if not ( self . params [ param ] & exclude ) ] \n 
~~ return utils . collect_shared_vars ( result ) \n 
~~ def get_output_shape_for ( self , input_shape ) : \n 
return input_shape \n 
~~ def get_output_for ( self , input , ** kwargs ) : \n 
raise NotImplementedError \n 
~~ def add_param ( self , spec , shape , name = None , ** tags ) : \n 
if name is not None : \n 
~~~ if self . name is not None : \n 
~~~ name = "%s.%s" % ( self . name , name ) \n 
~~ ~~ param = utils . create_param ( spec , shape , name ) \n 
tags [ ] = tags . get ( , True ) \n 
self . params [ param ] = set ( tag for tag , value in tags . items ( ) if value ) \n 
return param \n 
~~ ~~ class MergeLayer ( Layer ) : \n 
def __init__ ( self , incomings , name = None ) : \n 
~~~ self . input_shapes = [ incoming if isinstance ( incoming , tuple ) \n 
else incoming . output_shape \n 
for incoming in incomings ] \n 
self . input_layers = [ None if isinstance ( incoming , tuple ) \n 
else incoming \n 
self . name = name \n 
~~ @ Layer . output_shape . getter \n 
~~~ shape = self . get_output_shape_for ( self . input_shapes ) \n 
~~ def get_output_shape_for ( self , input_shapes ) : \n 
~~ def get_output_for ( self , inputs , ** kwargs ) : \n 
~~ ~~ from mock import Mock \n 
import numpy \n 
import theano \n 
class TestAutocrop : \n 
~~~ def test_autocrop_array_shapes ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop_array_shapes \n 
crop0 = None \n 
crop1 = [ None , , , ] \n 
crop2 = [ , ] \n 
crop_bad = [ , , , ] \n 
assert autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop0 ) == [ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop1 ) == [ ( 1 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) ] \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop2 ) == [ ( 1 , 2 , 3 , 4 ) , ( 1 , 2 , 7 , 8 ) , ( 1 , 2 , 3 , 2 ) ] \n 
with pytest . raises ( ValueError ) : \n 
~~~ autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop_bad ) \n 
~~ with pytest . raises ( ValueError ) : \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 ) , ( 5 , 4 , 3 , 2 , 10 ) ] , crop1 ) \n 
~~ ~~ def test_crop_inputs ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop \n 
from numpy . testing import assert_array_equal \n 
crop_0 = None \n 
crop_1 = [ None , , , ] \n 
crop_l = [ , , , ] \n 
crop_c = [ , , , ] \n 
crop_u = [ , , , ] \n 
crop_x = [ , ] \n 
x0 = numpy . random . random ( ( 2 , 3 , 5 , 7 ) ) \n 
x1 = numpy . random . random ( ( 1 , 2 , 3 , 4 ) ) \n 
x2 = numpy . random . random ( ( 6 , 3 , 4 , 2 ) ) \n 
def crop_test ( cropping , inputs , expected ) : \n 
~~~ inputs = [ theano . shared ( x ) for x in inputs ] \n 
outs = autocrop ( inputs , cropping ) \n 
outs = [ o . eval ( ) for o in outs ] \n 
assert len ( outs ) == len ( expected ) \n 
for o , e in zip ( outs , expected ) : \n 
~~~ assert_array_equal ( o , e ) \n 
~~ ~~ crop_test ( crop_0 , [ x0 , x1 ] , \n 
[ x0 , x1 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 4 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 1 : 5 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_0 , [ x0 , x2 ] , \n 
[ x0 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 5 : ] , x2 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , : 2 ] , x2 [ : 2 , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 2 : 4 ] , x2 [ 2 : 4 , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x2 ] , \n 
[ x0 [ : , : , 1 : , 5 : ] , x2 [ 4 : , : , : , : ] ] ) \n 
crop_test ( crop_0 , [ x0 , x1 , x2 ] , \n 
[ x0 , x1 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 , x2 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ : , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 2 ] , x1 [ : , : , : , : 2 ] , x2 [ : 1 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 2 : 4 ] , x1 [ : , : , : , 1 : 3 ] , x2 [ 2 : 3 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 , x2 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ 5 : , 1 : , 1 : , : ] ] ) \n 
crop_test ( crop_x , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] , \n 
x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
~~~ crop_test ( crop_bad , [ x0 , x1 , x2 ] , \n 
~~~ crop_test ( crop_bad , [ x0 [ : , : , : , 0 ] , x1 , x2 [ : , : , : , : , None ] ] , \n 
~~ ~~ ~~ class TestConcatLayer : \n 
~~~ @ pytest . fixture \n 
def layer ( self ) : \n 
~~~ from lasagne . layers . merge import ConcatLayer \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 ) \n 
~~ @ pytest . fixture \n 
def crop_layer_0 ( self ) : \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 0 , \n 
cropping = [ ] * 2 ) \n 
def crop_layer_1 ( self ) : \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 , \n 
~~ def test_get_output_shape_for ( self , layer ) : \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , None ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 5 ) ] ) == ( None , 7 ) \n 
~~~ layer . get_output_shape_for ( [ ( 4 , None ) , ( 3 , 5 ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 4 , None ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) , ( 4 , 5 ) ] ) \n 
~~ ~~ def test_get_output_shape_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ input_shapes = [ ( 3 , 2 ) , ( 4 , 5 ) ] \n 
result_0 = crop_layer_0 . get_output_shape_for ( input_shapes ) \n 
result_1 = crop_layer_1 . get_output_shape_for ( input_shapes ) \n 
assert result_0 == ( 7 , 2 ) \n 
assert result_1 == ( 3 , 7 ) \n 
~~ def test_get_output_for ( self , layer ) : \n 
~~~ inputs = [ theano . shared ( numpy . ones ( ( 3 , 3 ) ) ) , \n 
theano . shared ( numpy . ones ( ( 3 , 2 ) ) ) ] \n 
result = layer . get_output_for ( inputs ) \n 
result_eval = result . eval ( ) \n 
desired_result = numpy . hstack ( [ input . get_value ( ) for input in inputs ] ) \n 
assert ( result_eval == desired_result ) . all ( ) \n 
~~ def test_get_output_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
x1 = numpy . random . random ( ( 4 , 2 ) ) \n 
inputs = [ theano . shared ( x0 ) , \n 
theano . shared ( x1 ) ] \n 
result_0 = crop_layer_0 . get_output_for ( inputs ) . eval ( ) \n 
result_1 = crop_layer_1 . get_output_for ( inputs ) . eval ( ) \n 
desired_result_0 = numpy . concatenate ( [ x0 [ : , : 2 ] , x1 [ : , : 2 ] ] , axis = 0 ) \n 
desired_result_1 = numpy . concatenate ( [ x0 [ : 4 , : ] , x1 [ : 4 , : ] ] , axis = 1 ) \n 
assert ( result_0 == desired_result_0 ) . all ( ) \n 
assert ( result_1 == desired_result_1 ) . all ( ) \n 
~~ ~~ class TestElemwiseSumLayer : \n 
~~~ from lasagne . layers . merge import ElemwiseSumLayer \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] ) \n 
def crop_layer ( self ) : \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] , \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 2 ) ] ) == ( None , 2 ) \n 
~~~ layer . get_output_shape_for ( [ ( 3 , None ) , ( 4 , 2 ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) , ( 4 , 2 ) ] ) \n 
~~ ~~ def test_get_output_for ( self , layer ) : \n 
~~~ a = numpy . array ( [ [ 0 , 1 ] , [ 2 , 3 ] ] ) \n 
b = numpy . array ( [ [ 1 , 2 ] , [ 4 , 5 ] ] ) \n 
inputs = [ theano . shared ( a ) , \n 
theano . shared ( b ) ] \n 
desired_result = 2 * a - b \n 
~~ def test_get_output_for_cropped ( self , crop_layer ) : \n 
~~~ from numpy . testing import assert_array_almost_equal as aeq \n 
x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
result = crop_layer . get_output_for ( inputs ) . eval ( ) \n 
desired_result = 2 * x0 [ : 4 , : 2 ] - x1 [ : 4 , : 2 ] \n 
aeq ( result , desired_result ) \n 
~~ def test_bad_coeffs_fails ( self , layer ) : \n 
~~~ ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , 3 , - 1 ] ) \n 
~~ ~~ ~~ class TestElemwiseMergeLayerMul : \n 
~~~ import theano . tensor as T \n 
from lasagne . layers . merge import ElemwiseMergeLayer \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . mul ) \n 
desired_result = a * b \n 
~~ ~~ class TestElemwiseMergeLayerMaximum : \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . maximum ) \n 
desired_result = numpy . maximum ( a , b ) \n 
def tokenProgressFunc ( state = "update" , action = None , text = None , tick = 0 ) : \n 
~~ def build ( \n 
documentPath , \n 
outputUFOFormatVersion = 2 , \n 
roundGeometry = True , \n 
verbose = True , \n 
logPath = None , \n 
progressFunc = None , \n 
) : \n 
from mutatorMath . ufo . document import DesignSpaceDocumentReader \n 
import os , glob \n 
if os . path . isdir ( documentPath ) : \n 
~~~ todo = glob . glob ( os . path . join ( documentPath , "*.designspace" ) ) \n 
~~~ todo = [ documentPath ] \n 
~~ results = [ ] \n 
for path in todo : \n 
~~~ reader = DesignSpaceDocumentReader ( \n 
path , \n 
ufoVersion = outputUFOFormatVersion , \n 
roundGeometry = roundGeometry , \n 
verbose = verbose , \n 
logPath = logPath , \n 
progressFunc = progressFunc \n 
) \n 
reader . process ( ) \n 
results . append ( reader . results ) \n 
~~ reader = None \n 
return results \n 
from __future__ import print_function \n 
import argparse \n 
import datetime \n 
import json \n 
import multiprocessing \n 
import os \n 
import random \n 
import threading \n 
import time \n 
from PIL import Image \n 
import six . moves . cPickle as pickle \n 
from six . moves import queue \n 
import chainer \n 
from chainer import computational_graph \n 
from chainer import cuda \n 
from chainer import optimizers \n 
from chainer import serializers \n 
parser = argparse . ArgumentParser ( \n 
description = ) \n 
parser . add_argument ( , help = ) \n 
parser . add_argument ( , , default = , \n 
help = ) \n 
parser . add_argument ( , , type = int , default = 32 , \n 
parser . add_argument ( , , type = int , default = 250 , \n 
parser . add_argument ( , , default = 10 , type = int , \n 
parser . add_argument ( , , default = - 1 , type = int , \n 
parser . add_argument ( , , default = 20 , type = int , \n 
parser . add_argument ( , default = , \n 
args = parser . parse_args ( ) \n 
if args . gpu >= 0 : \n 
~~~ cuda . check_cuda_available ( ) \n 
~~ xp = cuda . cupy if args . gpu >= 0 else np \n 
assert 50000 % args . val_batchsize == 0 \n 
def load_image_list ( path , root ) : \n 
~~~ tuples = [ ] \n 
for line in open ( path ) : \n 
~~~ pair = line . strip ( ) . split ( ) \n 
tuples . append ( ( os . path . join ( root , pair [ 0 ] ) , np . int32 ( pair [ 1 ] ) ) ) \n 
~~ return tuples \n 
~~ train_list = load_image_list ( args . train , args . root ) \n 
val_list = load_image_list ( args . val , args . root ) \n 
mean_image = pickle . load ( open ( args . mean , ) ) \n 
if args . arch == : \n 
~~~ import nin \n 
model = nin . NIN ( ) \n 
~~ elif args . arch == : \n 
~~~ import alex \n 
model = alex . Alex ( ) \n 
~~~ import alexbn \n 
model = alexbn . AlexBN ( ) \n 
~~~ import googlenet \n 
model = googlenet . GoogLeNet ( ) \n 
~~~ import googlenetbn \n 
model = googlenetbn . GoogLeNetBN ( ) \n 
~~~ raise ValueError ( ) \n 
~~ if args . gpu >= 0 : \n 
~~~ cuda . get_device ( args . gpu ) . use ( ) \n 
model . to_gpu ( ) \n 
~~ optimizer = optimizers . MomentumSGD ( lr = 0.01 , momentum = 0.9 ) \n 
optimizer . setup ( model ) \n 
if args . initmodel : \n 
~~~ print ( , args . initmodel ) \n 
serializers . load_hdf5 ( args . initmodel , model ) \n 
~~ if args . resume : \n 
~~~ print ( , args . resume ) \n 
serializers . load_hdf5 ( args . resume , optimizer ) \n 
~~ data_q = queue . Queue ( maxsize = 1 ) \n 
res_q = queue . Queue ( ) \n 
cropwidth = 256 - model . insize \n 
def read_image ( path , center = False , flip = False ) : \n 
~~~ image = np . asarray ( Image . open ( path ) ) . transpose ( 2 , 0 , 1 ) \n 
if center : \n 
~~~ top = left = cropwidth / 2 \n 
~~~ top = random . randint ( 0 , cropwidth - 1 ) \n 
left = random . randint ( 0 , cropwidth - 1 ) \n 
~~ bottom = model . insize + top \n 
right = model . insize + left \n 
image = image [ : , top : bottom , left : right ] . astype ( np . float32 ) \n 
image -= mean_image [ : , top : bottom , left : right ] \n 
image /= 255 \n 
if flip and random . randint ( 0 , 1 ) == 0 : \n 
~~~ return image [ : , : , : : - 1 ] \n 
~~~ return image \n 
~~ ~~ def feed_data ( ) : \n 
~~~ i = 0 \n 
count = 0 \n 
x_batch = np . ndarray ( \n 
( args . batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
y_batch = np . ndarray ( ( args . batchsize , ) , dtype = np . int32 ) \n 
val_x_batch = np . ndarray ( \n 
( args . val_batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
val_y_batch = np . ndarray ( ( args . val_batchsize , ) , dtype = np . int32 ) \n 
batch_pool = [ None ] * args . batchsize \n 
val_batch_pool = [ None ] * args . val_batchsize \n 
pool = multiprocessing . Pool ( args . loaderjob ) \n 
data_q . put ( ) \n 
for epoch in six . moves . range ( 1 , 1 + args . epoch ) : \n 
~~~ print ( , epoch , file = sys . stderr ) \n 
print ( , optimizer . lr , file = sys . stderr ) \n 
perm = np . random . permutation ( len ( train_list ) ) \n 
for idx in perm : \n 
~~~ path , label = train_list [ idx ] \n 
batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n 
y_batch [ i ] = label \n 
i += 1 \n 
if i == args . batchsize : \n 
~~~ for j , x in enumerate ( batch_pool ) : \n 
~~~ x_batch [ j ] = x . get ( ) \n 
~~ data_q . put ( ( x_batch . copy ( ) , y_batch . copy ( ) ) ) \n 
i = 0 \n 
~~ count += 1 \n 
if count % 1000 == 0 : \n 
~~~ data_q . put ( ) \n 
j = 0 \n 
for path , label in val_list : \n 
~~~ val_batch_pool [ j ] = pool . apply_async ( \n 
read_image , ( path , True , False ) ) \n 
val_y_batch [ j ] = label \n 
j += 1 \n 
if j == args . val_batchsize : \n 
~~~ for k , x in enumerate ( val_batch_pool ) : \n 
~~~ val_x_batch [ k ] = x . get ( ) \n 
~~ data_q . put ( ( val_x_batch . copy ( ) , val_y_batch . copy ( ) ) ) \n 
~~ ~~ data_q . put ( ) \n 
~~ ~~ optimizer . lr *= 0.97 \n 
~~ pool . close ( ) \n 
pool . join ( ) \n 
~~ def log_result ( ) : \n 
~~~ train_count = 0 \n 
train_cur_loss = 0 \n 
train_cur_accuracy = 0 \n 
begin_at = time . time ( ) \n 
val_begin_at = None \n 
while True : \n 
~~~ result = res_q . get ( ) \n 
if result == : \n 
~~~ print ( file = sys . stderr ) \n 
break \n 
~~ elif result == : \n 
train = True \n 
if val_begin_at is not None : \n 
~~~ begin_at += time . time ( ) - val_begin_at \n 
~~ continue \n 
train = False \n 
val_count = val_loss = val_accuracy = 0 \n 
val_begin_at = time . time ( ) \n 
continue \n 
~~ loss , accuracy = result \n 
if train : \n 
~~~ train_count += 1 \n 
duration = time . time ( ) - begin_at \n 
throughput = train_count * args . batchsize / duration \n 
sys . stderr . write ( \n 
. format ( train_count , train_count * args . batchsize , \n 
datetime . timedelta ( seconds = duration ) , throughput ) ) \n 
train_cur_loss += loss \n 
train_cur_accuracy += accuracy \n 
if train_count % 1000 == 0 : \n 
~~~ mean_loss = train_cur_loss / 1000 \n 
mean_error = 1 - train_cur_accuracy / 1000 \n 
print ( file = sys . stderr ) \n 
print ( json . dumps ( { : , : train_count , \n 
: mean_error , : mean_loss } ) ) \n 
sys . stdout . flush ( ) \n 
~~ ~~ else : \n 
~~~ val_count += args . val_batchsize \n 
duration = time . time ( ) - val_begin_at \n 
throughput = val_count / duration \n 
. format ( val_count / args . val_batchsize , val_count , \n 
val_loss += loss \n 
val_accuracy += accuracy \n 
if val_count == 50000 : \n 
~~~ mean_loss = val_loss * args . val_batchsize / 50000 \n 
mean_error = 1 - val_accuracy * args . val_batchsize / 50000 \n 
~~ ~~ ~~ ~~ def train_loop ( ) : \n 
~~~ graph_generated = False \n 
~~~ while data_q . empty ( ) : \n 
~~~ time . sleep ( 0.1 ) \n 
~~ inp = data_q . get ( ) \n 
~~~ res_q . put ( ) \n 
model . train = True \n 
serializers . save_hdf5 ( args . out , model ) \n 
serializers . save_hdf5 ( args . outstate , optimizer ) \n 
model . train = False \n 
~~ volatile = if model . train else \n 
x = chainer . Variable ( xp . asarray ( inp [ 0 ] ) , volatile = volatile ) \n 
t = chainer . Variable ( xp . asarray ( inp [ 1 ] ) , volatile = volatile ) \n 
if model . train : \n 
~~~ optimizer . update ( model , x , t ) \n 
if not graph_generated : \n 
~~~ with open ( , ) as o : \n 
~~~ o . write ( computational_graph . build_computational_graph ( \n 
( model . loss , ) ) . dump ( ) ) \n 
~~ print ( , file = sys . stderr ) \n 
graph_generated = True \n 
~~~ model ( x , t ) \n 
~~ res_q . put ( ( float ( model . loss . data ) , float ( model . accuracy . data ) ) ) \n 
del x , t \n 
~~ ~~ feeder = threading . Thread ( target = feed_data ) \n 
feeder . daemon = True \n 
feeder . start ( ) \n 
logger = threading . Thread ( target = log_result ) \n 
logger . daemon = True \n 
logger . start ( ) \n 
train_loop ( ) \n 
feeder . join ( ) \n 
logger . join ( ) \n 
import lxmls . readers . pos_corpus as pcc \n 
from os import path \n 
import pickle \n 
corpus = pcc . PostagCorpus ( ) \n 
input_data = path . join ( \n 
path . dirname ( __file__ ) , \n 
"../../data/train-02-21.conll" ) \n 
train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = 15 , max_nr_sent = 1000 ) \n 
pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( , ) ) \n 
with open ( , ) as output : \n 
~~~ for seq in train_seq : \n 
~~~ words = [ corpus . word_dict . get_label_name ( seq . x [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
tags = [ corpus . tag_dict . get_label_name ( seq . y [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
s = . join ( [ . join ( [ word , tag ] ) for word , tag in zip ( words , tags ) ] ) \n 
output . write ( s + ) \n 
~~ ~~ import sys \n 
from lxmls . parsing . dependency_reader import * \n 
from lxmls . parsing . dependency_writer import * \n 
from lxmls . parsing . dependency_features import * \n 
from lxmls . parsing . dependency_decoder import * \n 
from lxmls . util . my_math_utils import * \n 
class DependencyParser ( ) : \n 
~~~ \n 
def __init__ ( self ) : \n 
~~~ self . trained = False \n 
self . projective = False \n 
self . language = "" \n 
self . weights = [ ] \n 
self . decoder = DependencyDecoder ( ) \n 
self . reader = DependencyReader ( ) \n 
self . writer = DependencyWriter ( ) \n 
self . features = DependencyFeatures ( ) \n 
~~ def read_data ( self , language ) : \n 
~~~ self . language = language \n 
self . reader . load ( language ) \n 
self . features . create_dictionary ( self . reader . train_instances ) \n 
~~ def train_perceptron ( self , n_epochs ) : \n 
self . weights = np . zeros ( self . features . n_feats ) \n 
total = np . zeros ( self . features . n_feats ) \n 
for epoch in range ( n_epochs ) : \n 
n_mistakes = 0 \n 
n_tokens = 0 \n 
n_instances = 0 \n 
for instance in self . reader . train_instances : \n 
~~~ feats = self . features . create_features ( instance ) \n 
scores = self . features . compute_scores ( feats , self . weights ) \n 
if self . projective : \n 
~~~ heads_pred = self . decoder . parse_proj ( scores ) \n 
~~~ heads_pred = self . decoder . parse_nonproj ( scores ) \n 
~~ for m in range ( np . size ( heads_pred ) ) : \n 
~~~ for f in feats [ instance . heads [ m ] ] [ m ] : \n 
~~~ if f < 0 : \n 
~~~ continue \n 
~~ self . weights [ f ] += 1.0 \n 
~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~ self . weights [ f ] -= 1.0 \n 
~~ n_mistakes += 1 \n 
~~ n_tokens += 1 \n 
~~ n_instances += 1 \n 
total += self . weights \n 
~~ self . weights = total / np . double ( n_epochs ) \n 
~~ def train_crf_sgd ( self , n_epochs , sigma , eta0 = 0.001 ) : \n 
t = 0 \n 
t0 = 1.0 / ( sigma * eta0 ) \n 
objective = 0.0 \n 
~~~ eta = 1.0 / ( sigma * ( t + t0 ) ) \n 
feats = self . features . create_features ( instance ) \n 
marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n 
for h in range ( np . size ( marginals , 0 ) ) : \n 
~~~ for m in range ( 1 , np . size ( marginals , 1 ) ) : \n 
~~~ if feats [ h ] [ m ] == None : \n 
~~ for f in feats [ h ] [ m ] : \n 
~~ self . weights [ f ] -= eta * marginals [ h , m ] \n 
~~ ~~ ~~ score_corr = 0.0 \n 
for m in range ( 1 , np . size ( instance . heads ) ) : \n 
~~~ h = instance . heads [ m ] \n 
score_corr += scores [ h , m ] \n 
for f in feats [ h ] [ m ] : \n 
~~ self . weights [ f ] += eta \n 
~~ ~~ objective += 0.5 * sigma * np . dot ( self . weights , self . weights ) - score_corr + logZ \n 
n_instances += 1 \n 
t += 1 \n 
~~ ~~ def test ( self ) : \n 
~~~ n_mistakes = 0 \n 
arr_heads_pred = [ ] ; \n 
for instance in self . reader . test_instances : \n 
~~ ~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~ ~~ n_mistakes += 1 \n 
arr_heads_pred . append ( heads_pred ) \n 
self . writer . save ( self . language , arr_heads_pred ) \n 
~~ ~~ from lxmls . sequences . label_dictionary import * \n 
import pdb \n 
################# \n 
class IDFeatures : \n 
def __init__ ( self , dataset ) : \n 
self . feature_dict = LabelDictionary ( ) \n 
self . feature_list = [ ] \n 
self . add_features = False \n 
self . dataset = dataset \n 
self . node_feature_cache = { } \n 
self . initial_state_feature_cache = { } \n 
self . final_state_feature_cache = { } \n 
self . edge_feature_cache = { } \n 
~~ def get_num_features ( self ) : \n 
~~~ return len ( self . feature_dict ) \n 
~~ def build_features ( self ) : \n 
self . add_features = True \n 
for sequence in self . dataset . seq_list : \n 
~~~ initial_features , transition_features , final_features , emission_features = self . get_sequence_features ( sequence ) \n 
self . feature_list . append ( [ initial_features , transition_features , final_features , emission_features ] ) \n 
~~ self . add_features = False \n 
~~ def get_sequence_features ( self , sequence ) : \n 
emission_features = [ ] \n 
initial_features = [ ] \n 
transition_features = [ ] \n 
final_features = [ ] \n 
features = [ ] \n 
features = self . add_initial_features ( sequence , sequence . y [ 0 ] , features ) \n 
initial_features . append ( features ) \n 
for pos , tag in enumerate ( sequence . y ) : \n 
~~~ features = [ ] \n 
features = self . add_emission_features ( sequence , pos , sequence . y [ pos ] , features ) \n 
emission_features . append ( features ) \n 
if pos > 0 : \n 
~~~ prev_tag = sequence . y [ pos - 1 ] \n 
features = self . add_transition_features ( sequence , pos - 1 , tag , prev_tag , features ) \n 
transition_features . append ( features ) \n 
~~ ~~ features = [ ] \n 
features = self . add_final_features ( sequence , sequence . y [ - 1 ] , features ) \n 
final_features . append ( features ) \n 
return initial_features , transition_features , final_features , emission_features \n 
#f(t,y_t,X) \n 
~~ def get_emission_features ( self , sequence , pos , y ) : \n 
~~~ all_feat = [ ] \n 
x = sequence . x [ pos ] \n 
if ( x not in self . node_feature_cache ) : \n 
~~~ self . node_feature_cache [ x ] = { } \n 
~~ if ( y not in self . node_feature_cache [ x ] ) : \n 
~~~ node_idx = [ ] \n 
node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n 
self . node_feature_cache [ x ] [ y ] = node_idx \n 
~~ idx = self . node_feature_cache [ x ] [ y ] \n 
all_feat = idx [ : ] \n 
return all_feat \n 
#f(t,y_t,y_(t-1),X) \n 
~~ def get_transition_features ( self , sequence , pos , y , y_prev ) : \n 
~~~ assert ( pos >= 0 and pos < len ( sequence . x ) ) , pdb . set_trace ( ) \n 
if ( y not in self . edge_feature_cache ) : \n 
~~~ self . edge_feature_cache [ y ] = { } \n 
~~ if ( y_prev not in self . edge_feature_cache [ y ] ) : \n 
~~~ edge_idx = [ ] \n 
edge_idx = self . add_transition_features ( sequence , pos , y , y_prev , edge_idx ) \n 
self . edge_feature_cache [ y ] [ y_prev ] = edge_idx \n 
~~ return self . edge_feature_cache [ y ] [ y_prev ] \n 
~~ def get_initial_features ( self , sequence , y ) : \n 
~~~ if ( y not in self . initial_state_feature_cache ) : \n 
edge_idx = self . add_initial_features ( sequence , y , edge_idx ) \n 
self . initial_state_feature_cache [ y ] = edge_idx \n 
~~ return self . initial_state_feature_cache [ y ] \n 
~~ def get_final_features ( self , sequence , y_prev ) : \n 
~~~ if ( y_prev not in self . final_state_feature_cache ) : \n 
edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n 
self . final_state_feature_cache [ y_prev ] = edge_idx \n 
~~ return self . final_state_feature_cache [ y_prev ] \n 
~~ def add_initial_features ( self , sequence , y , features ) : \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y ) \n 
feat_name = "init_tag:%s" % ( y_name ) \n 
feat_id = self . add_feature ( feat_name ) \n 
if ( feat_id != - 1 ) : \n 
~~~ features . append ( feat_id ) \n 
~~ return features \n 
~~ def add_final_features ( self , sequence , y_prev , features ) : \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
feat_name = "final_prev_tag:%s" % ( y_name ) \n 
~~ def add_emission_features ( self , sequence , pos , y , features ) : \n 
y_name = self . dataset . y_dict . get_label_name ( y ) \n 
x_name = self . dataset . x_dict . get_label_name ( x ) \n 
feat_name = "id:%s::%s" % ( x_name , y_name ) \n 
if feat_id != - 1 : \n 
~~ def add_transition_features ( self , sequence , pos , y , y_prev , features ) : \n 
assert pos < len ( sequence . x ) - 1 , pdb . set_trace ( ) \n 
y_prev_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
feat_name = "prev_tag:%s::%s" % ( y_prev_name , y_name ) \n 
~~ def add_feature ( self , feat_name ) : \n 
if ( feat_name in self . feature_dict ) : \n 
~~~ return self . feature_dict [ feat_name ] \n 
~~ if not self . add_features : \n 
~~~ return - 1 \n 
~~ return self . feature_dict . add ( feat_name ) \n 
~~ ~~ from __future__ import division \n 
from collections import defaultdict \n 
import itertools \n 
from sklearn import cluster , preprocessing , manifold \n 
from datetime import datetime \n 
class KeplerMapper ( object ) : \n 
~~~ def __init__ ( self , verbose = 2 ) : \n 
~~~ self . verbose = verbose \n 
self . chunk_dist = [ ] \n 
self . overlap_dist = [ ] \n 
self . d = [ ] \n 
self . nr_cubes = 0 \n 
self . overlap_perc = 0 \n 
self . clusterer = False \n 
~~ def fit_transform ( self , X , projection = "sum" , scaler = preprocessing . MinMaxScaler ( ) ) : \n 
~~~ self . scaler = scaler \n 
self . projection = str ( projection ) \n 
~~~ reducer = projection \n 
if self . verbose > 0 : \n 
~~~ projection . set_params ( ** { "verbose" : self . verbose } ) \n 
~~ except : \n 
~~ X = reducer . fit_transform ( X ) \n 
~~ if isinstance ( projection , str ) : \n 
~~~ if self . verbose > 0 : \n 
~~~ X = np . sum ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . mean ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . median ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . max ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . min ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . std ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X_mean = np . mean ( X , axis = 0 ) \n 
X = np . sum ( np . sqrt ( ( X - X_mean ) ** 2 ) , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~ ~~ if isinstance ( projection , list ) : \n 
~~ X = X [ : , np . array ( projection ) ] \n 
~~ if scaler is not None : \n 
~~ X = scaler . fit_transform ( X ) \n 
~~ return X \n 
~~ def map ( self , projected_X , inverse_X = None , clusterer = cluster . DBSCAN ( eps = 0.5 , min_samples = 3 ) , nr_cubes = 10 , overlap_perc = 0.1 ) : \n 
~~~ start = datetime . now ( ) \n 
def cube_coordinates_all ( nr_cubes , nr_dimensions ) : \n 
~~~ l = [ ] \n 
for x in range ( nr_cubes ) : \n 
~~~ l += [ x ] * nr_dimensions \n 
~~ return [ np . array ( list ( f ) ) for f in sorted ( set ( itertools . permutations ( l , nr_dimensions ) ) ) ] \n 
~~ nodes = defaultdict ( list ) \n 
links = defaultdict ( list ) \n 
complex = { } \n 
self . nr_cubes = nr_cubes \n 
self . clusterer = clusterer \n 
self . overlap_perc = overlap_perc \n 
~~ if inverse_X is None : \n 
~~~ inverse_X = projected_X \n 
~~ self . chunk_dist = ( np . max ( projected_X , axis = 0 ) - np . min ( projected_X , axis = 0 ) ) / nr_cubes \n 
self . overlap_dist = self . overlap_perc * self . chunk_dist \n 
self . d = np . min ( projected_X , axis = 0 ) \n 
di = np . array ( [ x for x in range ( projected_X . shape [ 1 ] ) ] ) \n 
ids = np . array ( [ x for x in range ( projected_X . shape [ 0 ] ) ] ) \n 
projected_X = np . c_ [ ids , projected_X ] \n 
inverse_X = np . c_ [ ids , inverse_X ] \n 
~~~ total_cubes = len ( cube_coordinates_all ( nr_cubes , projected_X . shape [ 1 ] ) ) \n 
~~ for i , coor in enumerate ( cube_coordinates_all ( nr_cubes , di . shape [ 0 ] ) ) : \n 
~~~ hypercube = projected_X [ np . invert ( np . any ( ( projected_X [ : , di + 1 ] >= self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) & \n 
( projected_X [ : , di + 1 ] < self . d [ di ] + ( coor * self . chunk_dist [ di ] ) + self . chunk_dist [ di ] + self . overlap_dist [ di ] ) == False , axis = 1 ) ) ] \n 
if self . verbose > 1 : \n 
( hypercube . shape [ 0 ] , i , total_cubes , self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) ) \n 
~~ if hypercube . shape [ 0 ] > 0 : \n 
~~~ inverse_x = inverse_X [ [ int ( nn ) for nn in hypercube [ : , 0 ] ] ] \n 
clusterer . fit ( inverse_x [ : , 1 : ] ) \n 
~~ for a in np . c_ [ hypercube [ : , 0 ] , clusterer . labels_ ] : \n 
nodes [ cluster_id ] . append ( int ( a [ 0 ] ) ) \n 
~~ ~~ ~~ else : \n 
~~~ if self . verbose > 1 : \n 
~~ ~~ ~~ candidates = itertools . combinations ( nodes . keys ( ) , 2 ) \n 
for candidate in candidates : \n 
~~~ if len ( nodes [ candidate [ 0 ] ] + nodes [ candidate [ 1 ] ] ) != len ( set ( nodes [ candidate [ 0 ] ] + nodes [ candidate [ 1 ] ] ) ) : \n 
~~~ links [ candidate [ 0 ] ] . append ( candidate [ 1 ] ) \n 
~~ ~~ if self . verbose > 0 : \n 
~~~ nr_links = 0 \n 
for k in links : \n 
~~~ nr_links += len ( links [ k ] ) \n 
~~ complex [ "nodes" ] = nodes \n 
complex [ "links" ] = links \n 
complex [ "meta" ] = self . projection \n 
return complex \n 
graph_link_distance = 30 , graph_gravity = 0.1 , graph_charge = - 120 , custom_tooltips = None , width_html = 0 , \n 
height_html = 0 , show_tooltips = True , show_title = True , show_meta = True ) : \n 
~~~ json_s = { } \n 
json_s [ "nodes" ] = [ ] \n 
json_s [ "links" ] = [ ] \n 
k2e = { } \n 
for e , k in enumerate ( complex [ "nodes" ] ) : \n 
~~~ if custom_tooltips is not None : \n 
if color_function == "average_signal_cluster" : \n 
~~~ tooltip_i = int ( ( ( sum ( [ f for f in custom_tooltips [ complex [ "nodes" ] [ k ] ] ] ) / len ( custom_tooltips [ complex [ "nodes" ] [ k ] ] ) ) * 30 ) ) \n 
json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex [ "nodes" ] [ k ] ) ) ) , "color" : str ( tooltip_i ) } ) \n 
~~~ json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex [ "nodes" ] [ k ] ) ) ) , "color" : str ( k . split ( "_" ) [ 0 ] ) } ) \n 
json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex [ "nodes" ] [ k ] ) ) ) , "color" : str ( k . split ( "_" ) [ 0 ] ) } ) \n 
~~ k2e [ k ] = e \n 
~~ for k in complex [ "links" ] : \n 
~~~ for link in complex [ "links" ] [ k ] : \n 
~~~ json_s [ "links" ] . append ( { "source" : k2e [ k ] , "target" : k2e [ link ] , "value" : 1 } ) \n 
~~ ~~ if width_html == 0 : \n 
~~~ width_css = "100%" \n 
width_js = \'document.getElementById("holder").offsetWidth-20\' \n 
~~~ width_css = "%spx" % width_html \n 
width_js = "%s" % width_html \n 
~~ if height_html == 0 : \n 
~~~ height_css = "100%" \n 
height_js = \'document.getElementById("holder").offsetHeight-20\' \n 
~~~ height_css = "%spx" % height_html \n 
height_js = "%s" % height_html \n 
~~ if show_tooltips == False : \n 
~~~ tooltips_display = "" \n 
~~ if show_meta == False : \n 
~~~ meta_display = "" \n 
~~ if show_title == False : \n 
~~~ title_display = "" \n 
~~ with open ( path_html , "wb" ) as outfile : \n 
outfile . write ( html . encode ( "utf-8" ) ) \n 
~~ if self . verbose > 0 : \n 
~~ ~~ ~~ import sys , os \n 
sys . path . insert ( 0 , os . path . abspath ( ) ) \n 
needs_sphinx = \n 
extensions = [ , , , ] \n 
import pkg_resources \n 
~~~ release = pkg_resources . get_distribution ( ) . version \n 
~~ except pkg_resources . DistributionNotFound : \n 
~~~ print \n 
print \n 
sys . exit ( 1 ) \n 
~~ del pkg_resources \n 
version = . join ( release . split ( ) [ : 2 ] ) \n 
html_static_path = [ ] \n 
html_use_smartypants = True \n 
import math \n 
import glob \n 
import re \n 
import subprocess \n 
from shutil import rmtree \n 
import logging \n 
from mrec import load_sparse_matrix , save_recommender \n 
class ItemSimilarityRunner ( object ) : \n 
~~~ def run ( self , view , model , input_format , trainfile , num_engines , simsdir , overwrite , max_sims , simsfile , modelfile ) : \n 
~~~ logging . info ( ) \n 
dataset = load_sparse_matrix ( input_format , trainfile ) \n 
num_users , num_items = dataset . shape \n 
del dataset \n 
logging . info ( , num_users , num_items ) \n 
logging . info ( . format ( simsdir ) ) \n 
subprocess . check_call ( [ , , simsdir ] ) \n 
done = [ ] \n 
if not overwrite : \n 
done . extend ( self . find_done ( simsdir ) ) \n 
if done : \n 
~~~ logging . info ( . format ( len ( done ) ) ) \n 
~~ ~~ logging . info ( ) \n 
tasks = self . create_tasks ( model , input_format , trainfile , simsdir , num_items , num_engines , max_sims , done ) \n 
if num_engines > 0 : \n 
~~~ logging . info ( \n 
, len ( tasks ) ) \n 
async_job = view . map_async ( process , tasks , retries = 2 ) \n 
results = async_job . get ( ) \n 
results = [ process ( task ) for task in tasks ] \n 
~~ logging . info ( ) \n 
done = self . find_done ( simsdir ) \n 
remaining = len ( tasks ) - len ( done ) \n 
if remaining == 0 : \n 
logging . info ( . format ( len ( done ) ) ) \n 
paths = [ os . path . join ( simsdir , . format ( start , end ) ) for start , end in done ] \n 
cmd = [ ] + paths \n 
subprocess . check_call ( cmd , stdout = open ( simsfile , ) ) \n 
logging . info ( ) \n 
rmtree ( simsdir ) \n 
logging . info ( , \n 
num_items , type ( model ) . __name__ , simsfile ) \n 
model . load_similarity_matrix ( simsfile , num_items ) \n 
save_recommender ( model , modelfile ) \n 
~~~ logging . error ( . format ( remaining , len ( tasks ) ) ) \n 
logging . error ( ) \n 
~~ ~~ def find_done ( self , outdir ) : \n 
~~~ success_files = glob . glob ( os . path . join ( outdir , ) ) \n 
r = re . compile ( ) \n 
for path in success_files : \n 
~~~ m = r . match ( path ) \n 
start = int ( m . group ( 1 ) ) \n 
end = int ( m . group ( 2 ) ) \n 
done . append ( ( start , end ) ) \n 
~~ return done \n 
~~ def create_tasks ( self , model , input_format , trainfile , outdir , num_items , num_engines , max_similar_items , done ) : \n 
~~~ if num_engines == 0 : \n 
~~~ num_engines = 1 \n 
~~ items_per_engine = int ( math . ceil ( float ( num_items ) / num_engines ) ) \n 
tasks = [ ] \n 
for start in xrange ( 0 , num_items , items_per_engine ) : \n 
~~~ end = min ( num_items , start + items_per_engine ) \n 
if ( start , end ) not in done : \n 
~~~ tasks . append ( ( model , input_format , trainfile , outdir , start , end , max_similar_items ) ) \n 
~~ ~~ return tasks \n 
~~ ~~ def process ( task ) : \n 
from mrec import load_fast_sparse_matrix \n 
model , input_format , trainfile , outdir , start , end , max_similar_items = task \n 
dataset = load_fast_sparse_matrix ( input_format , trainfile ) \n 
if hasattr ( model , ) : \n 
~~~ model . similarity_matrix = None \n 
~~ outfile = os . path . join ( outdir , . format ( start , end ) ) \n 
out = open ( outfile , ) \n 
for j in xrange ( start , end ) : \n 
~~~ w = model . get_similar_items ( j , max_similar_items = max_similar_items , dataset = dataset ) \n 
for k , v in w : \n 
~~ ~~ out . close ( ) \n 
cmd = [ , os . path . join ( outdir , . format ( start , end ) ) ] \n 
subprocess . check_call ( cmd ) \n 
return start , end \n 
import inspect \n 
NO_DEFAULT = object ( ) \n 
def memoize_default ( default = NO_DEFAULT , evaluator_is_first_arg = False , second_arg_is_evaluator = False ) : \n 
def func ( function ) : \n 
~~~ def wrapper ( obj , * args , ** kwargs ) : \n 
~~~ if evaluator_is_first_arg : \n 
~~~ cache = obj . memoize_cache \n 
~~~ cache = args [ 0 ] . memoize_cache \n 
~~~ cache = obj . _evaluator . memoize_cache \n 
~~ try : \n 
~~~ memo = cache [ function ] \n 
~~ except KeyError : \n 
~~~ memo = { } \n 
cache [ function ] = memo \n 
~~ key = ( obj , args , frozenset ( kwargs . items ( ) ) ) \n 
if key in memo : \n 
~~~ return memo [ key ] \n 
~~~ if default is not NO_DEFAULT : \n 
~~~ memo [ key ] = default \n 
~~ rv = function ( obj , * args , ** kwargs ) \n 
if inspect . isgenerator ( rv ) : \n 
~~~ rv = list ( rv ) \n 
~~ memo [ key ] = rv \n 
return rv \n 
~~ ~~ return wrapper \n 
~~ return func \n 
~~ class CachedMetaClass ( type ) : \n 
@ memoize_default ( None , second_arg_is_evaluator = True ) \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ return super ( CachedMetaClass , self ) . __call__ ( * args , ** kwargs ) \n 
from __future__ import absolute_import \n 
import __main__ \n 
from collections import namedtuple \n 
from jedi import Interpreter \n 
from jedi . api . helpers import completion_parts \n 
from jedi . parser . user_context import UserContext \n 
def setup_readline ( namespace_module = __main__ ) : \n 
class JediRL ( object ) : \n 
~~~ def complete ( self , text , state ) : \n 
if state == 0 : \n 
~~~ sys . path . insert ( 0 , os . getcwd ( ) ) \n 
~~~ interpreter = Interpreter ( text , [ namespace_module . __dict__ ] ) \n 
path = UserContext ( text , ( 1 , len ( text ) ) ) . get_path_until_cursor ( ) \n 
path , dot , like = completion_parts ( path ) \n 
before = text [ : len ( text ) - len ( like ) ] \n 
completions = interpreter . completions ( ) \n 
~~ finally : \n 
~~~ sys . path . pop ( 0 ) \n 
~~ self . matches = [ before + c . name_with_symbols for c in completions ] \n 
~~~ return self . matches [ state ] \n 
~~ except IndexError : \n 
~~ ~~ ~~ try : \n 
~~~ import readline \n 
~~~ readline . set_completer ( JediRL ( ) . complete ) \n 
readline . set_completer_delims ( ) \n 
~~ ~~ def version_info ( ) : \n 
Version = namedtuple ( , ) \n 
from jedi import __version__ \n 
tupl = re . findall ( , __version__ ) \n 
return Version ( * [ x if i == 3 else int ( x ) for i , x in enumerate ( tupl ) ] ) \n 
~~ import pandas \n 
import util \n 
import matplotlib . pyplot as plt \n 
import scipy as sp \n 
import scipy . stats \n 
cur_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
def from_custom_file ( data_file , learn_options ) : \n 
data = pandas . read_csv ( data_file ) \n 
mandatory_columns = [ , , , ] \n 
for col in mandatory_columns : \n 
~~ Xdf = pandas . DataFrame ( data ) \n 
Xdf [ ] = Xdf [ ] \n 
Xdf = Xdf . set_index ( [ , ] ) \n 
Xdf . index . names = [ , ] \n 
Xdf [ ] = [ % i for i in range ( Xdf . shape [ 0 ] ) ] \n 
Xdf = Xdf . set_index ( , append = True ) \n 
Y = None \n 
gene_position = Xdf [ [ , ] ] \n 
target_genes = np . unique ( Xdf . index . levels [ 1 ] ) \n 
learn_options = set_V2_target_names ( learn_options ) \n 
return Xdf , Y , gene_position , target_genes \n 
~~ def from_file ( data_file , learn_options , data_file2 = None , data_file3 = None ) : \n 
annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options ) \n 
learn_options [ ] = \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , learn_options ) \n 
xx = Xdf [ ] . values \n 
yy = Y [ ] . values \n 
rr , pp = sp . stats . pearsonr ( xx , yy ) \n 
~~~ learn_options [ ] = \n 
learn_options [ ] = None \n 
Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
Xdf , Y , gene_position , target_genes = merge_all ( data_file , data_file2 , data_file3 , learn_options ) \n 
~~ elif learn_options [ ] == 5 : \n 
gene_position , target_genes , Xdf , Y = read_xu_et_al ( data_file3 ) \n 
~~ Xdf [ "30mer" ] = Xdf [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
~~ def set_V2_target_names ( learn_options ) : \n 
~~~ if not in learn_options . keys ( ) : \n 
~~ if not in learn_options . keys ( ) : \n 
~~ learn_options [ ] = \n 
return learn_options \n 
~~ def combine_organisms ( human_data , mouse_data ) : \n 
~~~ cd13 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ , ] ) \n 
cd33 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ , , ] ) \n 
cd15 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ ] ) \n 
mouse_X = pandas . DataFrame ( ) \n 
mouse_Y = pandas . DataFrame ( ) \n 
for k in mouse_data . index . levels [ 1 ] : \n 
mouse_X = pandas . concat ( [ mouse_X , X ] , axis = 0 ) \n 
mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = 0 ) \n 
~~ X = pandas . concat ( [ X_CD13 , X_CD15 , X_CD33 , mouse_X ] , axis = 0 ) \n 
Y = pandas . concat ( [ Y_CD13 , Y_CD15 , Y_CD33 , mouse_Y ] , axis = 0 ) \n 
return X , Y \n 
~~ def read_V1_data ( data_file , learn_options , AML_file = cur_dir + "/data/V1_suppl_data.txt" ) : \n 
~~~ if data_file is None : \n 
~~~ data_file = cur_dir + "/data/V1_data.xlsx" \n 
~~ human_data = pandas . read_excel ( data_file , sheetname = 0 , index_col = [ 0 , 1 ] ) \n 
mouse_data = pandas . read_excel ( data_file , sheetname = 1 , index_col = [ 0 , 1 ] ) \n 
Xdf , Y = combine_organisms ( human_data , mouse_data ) \n 
annotations = pandas . read_csv ( AML_file , delimiter = , index_col = [ 0 , 4 ] ) \n 
annotations . index . names = Xdf . index . names \n 
gene_position = pandas . merge ( Xdf , annotations , how = "inner" , left_index = True , right_index = True ) \n 
gene_position = util . impute_gene_position ( gene_position ) \n 
gene_position = gene_position [ [ , , ] ] \n 
Y = Y . loc [ gene_position . index ] \n 
Xdf = Xdf . loc [ gene_position . index ] \n 
target_genes = Y [ ] . unique ( ) \n 
Y . index . names = [ , ] \n 
if learn_options is not None and learn_options [ "flipV1target" ] : \n 
~~~ print "************************************************************************" \n 
print "************************************************************************" \n 
import ipdb \n 
ipdb . set_trace ( ) \n 
~~ return annotations , gene_position , target_genes , Xdf , Y \n 
~~ def rank_transform ( x ) : \n 
~~~ return 1.0 - sp . stats . mstats . rankdata ( x ) / sp . stats . mstats . rankdata ( x ) . max ( ) \n 
~~ def read_xu_et_al ( data_file , learn_options = None , verbose = True , subsetting = ) : \n 
~~~ data_file = \n 
~~ datasets = [ , , ] \n 
aggregated = None \n 
for d in datasets : \n 
~~~ data_efficient = pandas . read_excel ( data_file , sheetname = % d , skiprows = 2 ) \n 
data_inefficient = pandas . read_excel ( data_file , sheetname = % d , skiprows = 2 ) \n 
data_efficient [ ] = 1. \n 
data_inefficient [ ] = 0. \n 
exp_data = pandas . concat ( ( data_efficient , data_inefficient ) ) \n 
exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( rank_transform ) \n 
if aggregated is None : \n 
~~~ aggregated = exp_data \n 
~~~ aggregated = pandas . concat ( ( aggregated , exp_data ) ) \n 
~~ ~~ if subsetting == : \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x [ 6 : - 4 ] ) \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x [ 10 : ] ) \n 
~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x . upper ( ) ) \n 
aggregated . rename ( columns = { "sequence(target+3\'+5\')" : , : , : } , inplace = True ) \n 
aggregated [ ] . loc [ aggregated [ ] == ] = \n 
aggregated [ ] = aggregated [ [ , ] ] . mean ( axis = 1 ) \n 
df = aggregated \n 
df = df . rename ( columns = { : , : } ) \n 
df [ ] = \n 
df [ ] = 1 \n 
df = df . set_index ( [ , , ] ) \n 
df [ ] = df . index . get_level_values ( 0 ) \n 
df [ ] = df . index . get_level_values ( 1 ) \n 
df [ ] = df [ ] \n 
df [ ] = 0 \n 
target_genes = np . unique ( df [ ] . values ) \n 
return df [ [ , , ] ] , target_genes , df [ [ , ] ] , df [ [ , , , ] ] \n 
~~ def read_V2_data ( data_file , learn_options = None , verbose = True ) : \n 
~~~ data_file = cur_dir + "/data/V2_data.xlsx" \n 
~~ data = pandas . read_excel ( data_file , sheetname = "ResultsFiltered" , skiprows = range ( 0 , 6 + 1 ) , index_col = [ 0 , 4 ] ) \n 
Xdf = pandas . DataFrame ( ) \n 
known_pairs = { : [ , , , ] , \n 
: [ ] , \n 
: [ , , , ] } \n 
drugs_to_genes = { : [ , , , ] , \n 
if learn_options is not None : \n 
if learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , ] ) \n 
~~ elif learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , , , , ] ) \n 
~~ ~~ count = 0 \n 
for drug in drugs_to_genes . keys ( ) : \n 
~~~ genes = drugs_to_genes [ drug ] \n 
for g in genes : \n 
~~~ Xtmp = data . copy ( ) . xs ( g , level = , drop_level = False ) \n 
Xtmp [ ] = drug \n 
if g in known_pairs [ drug ] : \n 
~~~ Xtmp [ ] = 1. \n 
~~~ Xtmp [ ] = 0. \n 
~~ count = count + Xtmp . shape [ 0 ] \n 
Xdf = pandas . concat ( [ Xdf , Xtmp ] , axis = 0 ) \n 
if verbose : \n 
~~ ~~ ~~ Xdf = Xdf . set_index ( , append = True ) \n 
Y = pandas . DataFrame ( Xdf . pop ( "score" ) ) \n 
Y . columns . names = [ "score" ] \n 
test_gene = pandas . DataFrame ( Xdf . pop ( ) ) \n 
Y = pandas . concat ( ( Y , target , test_gene ) , axis = 1 ) \n 
y_rank = pandas . DataFrame ( ) \n 
y_threshold = pandas . DataFrame ( ) \n 
y_quant = pandas . DataFrame ( ) \n 
~~~ gene_list = drugs_to_genes [ drug ] \n 
for gene in gene_list : \n 
y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = 0.8 , prefix = "score_drug_gene" , flip = False ) \n 
y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n 
y_threshold = pandas . concat ( ( y_threshold , y_thresholdtmp ) , axis = 0 ) \n 
y_quant = pandas . concat ( ( y_quant , y_quanttmp ) , axis = 0 ) \n 
~~ ~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
Y = pandas . merge ( Y , yall , how = , left_index = True , right_index = True ) \n 
~~~ ytmp = pandas . DataFrame ( Y . xs ( drug , level = "drug" , drop_level = False ) [ ] ) \n 
y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = 0.8 , prefix = "score_drug" , flip = False ) \n 
~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
PLOT = False \n 
if PLOT : \n 
~~~ labels = [ "score" , "score_drug_gene_rank" , "score_drug_rank" , "score_drug_gene_threshold" , "score_drug_threshold" ] \n 
for label in labels : \n 
~~~ plt . figure ( ) \n 
plt . plot ( Xdf [ ] . values , Y [ label ] . values , ) \n 
r , pearp = sp . stats . pearsonr ( Xdf [ ] . values . flatten ( ) , Y [ label ] . values . flatten ( ) ) \n 
plt . title ( label + % ( r , pearp ) ) \n 
plt . ylabel ( label ) \n 
~~ ~~ gene_position = util . impute_gene_position ( gene_position ) \n 
if learn_options is not None and learn_options [ "weighted" ] == "variance" : \n 
data = pandas . read_excel ( data_file , sheetname = "Normalized" , skiprows = range ( 0 , 6 + 1 ) , index_col = [ 0 , 4 ] ) \n 
experiments = { } \n 
experiments [ ] = [ , , , ] \n 
variance = None \n 
~~~ data_tmp = data . iloc [ data . index . get_level_values ( ) . isin ( drugs_to_genes [ drug ] ) ] [ experiments [ drug ] ] \n 
data_tmp [ "drug" ] = drug \n 
data_tmp = data_tmp . set_index ( , append = True ) \n 
data_tmp [ "variance" ] = np . var ( data_tmp . values , axis = 1 ) \n 
if variance is None : \n 
~~~ variance = data_tmp [ "variance" ] . copy ( ) \n 
~~~ variance = pandas . concat ( ( variance , data_tmp [ "variance" ] ) , axis = 0 ) \n 
~~ ~~ orig_index = Y . index . copy ( ) \n 
Y = pandas . merge ( Y , pandas . DataFrame ( variance ) , how = "inner" , left_index = True , right_index = True ) \n 
Y = Y . ix [ orig_index ] \n 
print "done." \n 
return Xdf , drugs_to_genes , target_genes , Y , gene_position \n 
~~ def merge_all ( data_file = None , data_file2 = None , data_file3 = None , learn_options = None ) : \n 
~~~ Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n 
Xdf = pandas . concat ( ( Xdf , Xdf_xu ) ) \n 
Y = pandas . concat ( ( Y , Y_xu ) ) \n 
gene_position = pandas . concat ( ( gene_position , gene_position_xu ) ) \n 
target_genes = np . concatenate ( ( target_genes , target_genes_xu ) ) \n 
~~ def mergeV1_V2 ( data_file , data_file2 , learn_options ) : \n 
annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n 
Xdf2 , drugs_to_genes , target_genes2 , Y2 , gene_position2 = read_V2_data ( data_file2 ) \n 
Y1 [ "drug" ] = [ "nodrug" for x in range ( Y1 . shape [ 0 ] ) ] \n 
Y1 = Y1 . set_index ( , append = True ) \n 
Y1 . index . names = [ , , ] \n 
Y_cols_to_keep = np . unique ( [ , , , ] ) \n 
Y1 = Y1 [ Y_cols_to_keep ] \n 
Y2 = Y2 [ Y_cols_to_keep ] \n 
Xdf1 [ "drug" ] = [ "nodrug" for x in range ( Xdf1 . shape [ 0 ] ) ] \n 
Xdf1 = Xdf1 . set_index ( , append = True ) \n 
X_cols_to_keep = [ , ] \n 
Xdf1 = Xdf1 [ X_cols_to_keep ] \n 
Xdf2 = Xdf2 [ X_cols_to_keep ] \n 
gene_position1 [ "drug" ] = [ "nodrug" for x in range ( gene_position1 . shape [ 0 ] ) ] \n 
gene_position1 = gene_position1 . set_index ( , append = True ) \n 
gene_position1 . index . names = [ , , ] \n 
cols_to_keep = [ , ] \n 
gene_position1 = gene_position1 [ cols_to_keep ] \n 
gene_position2 = gene_position2 [ cols_to_keep ] \n 
Y = pandas . concat ( ( Y1 , Y2 ) , axis = 0 ) \n 
Xdf = pandas . concat ( ( Xdf1 , Xdf2 ) , axis = 0 ) \n 
gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n 
target_genes = np . concatenate ( ( target_genes1 , target_genes2 ) ) \n 
save_to_file = False \n 
if save_to_file : \n 
~~~ Y . index . names = [ , , ] \n 
onedupind = np . where ( Y . index . duplicated ( ) ) [ 0 ] [ 0 ] \n 
alldupind = np . where ( Y . index . get_level_values ( 0 ) . values == Y . index [ onedupind ] [ 0 ] ) [ 0 ] \n 
newindex = Y . index . tolist ( ) \n 
newindex [ onedupind ] = ( newindex [ onedupind ] [ 0 ] , newindex [ onedupind ] [ 1 ] , "nodrug2" ) \n 
Y . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
XandY = pandas . merge ( Xdf , Y , how = "inner" , left_index = True , right_index = True ) \n 
gene_position_tmp = gene_position . copy ( ) \n 
gene_position_tmp . index . names = [ , , ] \n 
gene_position_tmp . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
XandY = pandas . merge ( XandY , gene_position_tmp , how = "inner" , left_index = True , right_index = True ) \n 
XandY [ "30mer" ] = XandY [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
XandY . to_csv ( ) \n 
~~ return Xdf , Y , gene_position , target_genes \n 
~~ def get_V1_genes ( data_file = None ) : \n 
~~~ annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n 
return target_genes \n 
~~ def get_V2_genes ( data_file = None ) : \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , verbose = False ) \n 
~~ def get_V3_genes ( data_fileV1 = None , data_fileV2 = None ) : \n 
~~~ target_genes = np . concatenate ( ( get_V1_genes ( data_fileV1 ) , get_V2_genes ( data_fileV2 ) ) ) \n 
~~ def get_xu_genes ( data_file = None ) : \n 
~~~ return read_xu_et_al ( data_file ) [ 1 ] \n 
~~ def get_mouse_genes ( data_file = None ) : \n 
return Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
~~ def get_human_genes ( data_file = None ) : \n 
mouse_genes = Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
all_genes = get_V3_genes ( None , None ) \n 
return np . setdiff1d ( all_genes , mouse_genes ) \n 
~~ from __future__ import absolute_import \n 
from . classification import * \n 
from . generic import * \n 
from . job import ImageDatasetJob \n 
from . images import * \n 
from . job import InferenceJob \n 
from . caffe_train import CaffeTrainTask \n 
from . torch_train import TorchTrainTask \n 
from . train import TrainTask \n 
import flask \n 
from flask . ext . socketio import SocketIO \n 
from gevent import monkey ; monkey . patch_all ( ) \n 
from . config import config_value \n 
from digits import utils \n 
import digits . scheduler \n 
app = flask . Flask ( __name__ ) \n 
app . config [ ] = True \n 
app . config [ ] = False \n 
app . config [ ] = config_value ( ) \n 
app . url_map . redirect_defaults = False \n 
socketio = SocketIO ( app ) \n 
scheduler = digits . scheduler . Scheduler ( config_value ( ) , True ) \n 
app . jinja_env . globals [ ] = config_value ( ) \n 
app . jinja_env . globals [ ] = digits . __version__ \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_diff \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_since \n 
app . jinja_env . filters [ ] = utils . sizeof_fmt \n 
app . jinja_env . filters [ ] = utils . auth . has_permission \n 
app . jinja_env . trim_blocks = True \n 
app . jinja_env . lstrip_blocks = True \n 
import digits . views \n 
app . register_blueprint ( digits . views . blueprint ) \n 
import digits . dataset . views \n 
app . register_blueprint ( digits . dataset . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . views \n 
app . register_blueprint ( digits . dataset . images . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . classification . views \n 
app . register_blueprint ( digits . dataset . images . classification . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . generic . views \n 
app . register_blueprint ( digits . dataset . images . generic . views . blueprint , url_prefix = ) \n 
import digits . model . views \n 
app . register_blueprint ( digits . model . views . blueprint , url_prefix = ) \n 
import digits . model . images . views \n 
app . register_blueprint ( digits . model . images . views . blueprint , url_prefix = ) \n 
import digits . model . images . classification . views \n 
app . register_blueprint ( digits . model . images . classification . views . blueprint , url_prefix = ) \n 
import digits . model . images . generic . views \n 
app . register_blueprint ( digits . model . images . generic . views . blueprint , url_prefix = ) \n 
def username_decorator ( f ) : \n 
~~~ from functools import wraps \n 
@ wraps ( f ) \n 
def decorated ( * args , ** kwargs ) : \n 
~~~ this_username = flask . request . cookies . get ( , None ) \n 
app . jinja_env . globals [ ] = this_username \n 
return f ( * args , ** kwargs ) \n 
~~ return decorated \n 
~~ for endpoint , function in app . view_functions . iteritems ( ) : \n 
~~~ app . view_functions [ endpoint ] = username_decorator ( function ) \n 
~~ scheduler . load_past_jobs ( ) \n 
import xml . etree . ElementTree as ET \n 
import cPickle \n 
def parse_rec ( filename ) : \n 
tree = ET . parse ( filename ) \n 
objects = [ ] \n 
for obj in tree . findall ( ) : \n 
~~~ obj_struct = { } \n 
obj_struct [ ] = obj . find ( ) . text \n 
obj_struct [ ] = int ( obj . find ( ) . text ) \n 
bbox = obj . find ( ) \n 
obj_struct [ ] = [ int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) ] \n 
objects . append ( obj_struct ) \n 
~~ return objects \n 
~~ def voc_ap ( rec , prec , use_07_metric = False ) : \n 
if use_07_metric : \n 
~~~ ap = 0. \n 
for t in np . arange ( 0. , 1.1 , 0.1 ) : \n 
~~~ if np . sum ( rec >= t ) == 0 : \n 
~~~ p = 0 \n 
~~~ p = np . max ( prec [ rec >= t ] ) \n 
~~ ap = ap + p / 11. \n 
~~~ mrec = np . concatenate ( ( [ 0. ] , rec , [ 1. ] ) ) \n 
mpre = np . concatenate ( ( [ 0. ] , prec , [ 0. ] ) ) \n 
for i in range ( mpre . size - 1 , 0 , - 1 ) : \n 
~~~ mpre [ i - 1 ] = np . maximum ( mpre [ i - 1 ] , mpre [ i ] ) \n 
~~ i = np . where ( mrec [ 1 : ] != mrec [ : - 1 ] ) [ 0 ] \n 
ap = np . sum ( ( mrec [ i + 1 ] - mrec [ i ] ) * mpre [ i + 1 ] ) \n 
~~ return ap \n 
~~ def voc_eval ( detpath , \n 
annopath , \n 
imagesetfile , \n 
classname , \n 
cachedir , \n 
ovthresh = 0.5 , \n 
use_07_metric = False ) : \n 
if not os . path . isdir ( cachedir ) : \n 
~~~ os . mkdir ( cachedir ) \n 
~~ cachefile = os . path . join ( cachedir , ) \n 
with open ( imagesetfile , ) as f : \n 
~~~ lines = f . readlines ( ) \n 
~~ imagenames = [ x . strip ( ) for x in lines ] \n 
if not os . path . isfile ( cachefile ) : \n 
~~~ recs = { } \n 
for i , imagename in enumerate ( imagenames ) : \n 
~~~ recs [ imagename ] = parse_rec ( annopath . format ( imagename ) ) \n 
if i % 100 == 0 : \n 
~~~ print . format ( \n 
i + 1 , len ( imagenames ) ) \n 
~~ ~~ print . format ( cachefile ) \n 
with open ( cachefile , ) as f : \n 
~~~ cPickle . dump ( recs , f ) \n 
~~~ with open ( cachefile , ) as f : \n 
~~~ recs = cPickle . load ( f ) \n 
~~ ~~ class_recs = { } \n 
npos = 0 \n 
for imagename in imagenames : \n 
~~~ R = [ obj for obj in recs [ imagename ] if obj [ ] == classname ] \n 
bbox = np . array ( [ x [ ] for x in R ] ) \n 
difficult = np . array ( [ x [ ] for x in R ] ) . astype ( np . bool ) \n 
det = [ False ] * len ( R ) \n 
npos = npos + sum ( ~ difficult ) \n 
class_recs [ imagename ] = { : bbox , \n 
: difficult , \n 
: det } \n 
~~ detfile = detpath . format ( classname ) \n 
with open ( detfile , ) as f : \n 
~~ splitlines = [ x . strip ( ) . split ( ) for x in lines ] \n 
image_ids = [ x [ 0 ] for x in splitlines ] \n 
confidence = np . array ( [ float ( x [ 1 ] ) for x in splitlines ] ) \n 
BB = np . array ( [ [ float ( z ) for z in x [ 2 : ] ] for x in splitlines ] ) \n 
sorted_ind = np . argsort ( - confidence ) \n 
BB = BB [ sorted_ind , : ] \n 
image_ids = [ image_ids [ x ] for x in sorted_ind ] \n 
nd = len ( image_ids ) \n 
tp = np . zeros ( nd ) \n 
fp = np . zeros ( nd ) \n 
for d in range ( nd ) : \n 
~~~ R = class_recs [ image_ids [ d ] ] \n 
bb = BB [ d , : ] . astype ( float ) \n 
ovmax = - np . inf \n 
BBGT = R [ ] . astype ( float ) \n 
if BBGT . size > 0 : \n 
~~~ ixmin = np . maximum ( BBGT [ : , 0 ] , bb [ 0 ] ) \n 
iymin = np . maximum ( BBGT [ : , 1 ] , bb [ 1 ] ) \n 
ixmax = np . minimum ( BBGT [ : , 2 ] , bb [ 2 ] ) \n 
iymax = np . minimum ( BBGT [ : , 3 ] , bb [ 3 ] ) \n 
iw = np . maximum ( ixmax - ixmin + 1. , 0. ) \n 
ih = np . maximum ( iymax - iymin + 1. , 0. ) \n 
inters = iw * ih \n 
uni = ( ( bb [ 2 ] - bb [ 0 ] + 1. ) * ( bb [ 3 ] - bb [ 1 ] + 1. ) + \n 
( BBGT [ : , 2 ] - BBGT [ : , 0 ] + 1. ) * \n 
( BBGT [ : , 3 ] - BBGT [ : , 1 ] + 1. ) - inters ) \n 
overlaps = inters / uni \n 
ovmax = np . max ( overlaps ) \n 
jmax = np . argmax ( overlaps ) \n 
~~ if ovmax > ovthresh : \n 
~~~ if not R [ ] [ jmax ] : \n 
~~~ tp [ d ] = 1. \n 
R [ ] [ jmax ] = 1 \n 
~~~ fp [ d ] = 1. \n 
~~ ~~ fp = np . cumsum ( fp ) \n 
tp = np . cumsum ( tp ) \n 
rec = tp / float ( npos ) \n 
prec = tp / ( tp + fp + 1e-10 ) \n 
ap = voc_ap ( rec , prec , use_07_metric ) \n 
return rec , prec , ap \n 
#!/usr/bin/python \n 
~~ import numpy as np \n 
from ipdb import set_trace \n 
from struct import pack , unpack \n 
def ceil_div ( x , y ) : \n 
~~~ return - ( - x // y ) \n 
~~ def out_dim ( S , X , padding , strides ) : \n 
~~~ return ceil_div ( X - S + 1 + 2 * padding , strides ) \n 
~~ def strip_mantissa ( val ) : \n 
~~~ i = unpack ( , pack ( , val ) ) [ 0 ] & 0x7f800000 \n 
f = unpack ( , pack ( , i ) ) [ 0 ] \n 
return f \n 
~~ def quantize ( ary , bits , sign = 1 ) : \n 
~~~ maxval = float ( np . max ( np . absolute ( ary ) ) ) \n 
scale = strip_mantissa ( maxval ) / float ( 1 << ( bits - sign - 1 ) ) \n 
ary = np . around ( ary * ( 1.0 / scale ) ) . astype ( np . int64 ) \n 
return ary , np . float64 ( scale ) \n 
~~ def fconv_slice ( q , S , X , padding , strides ) : \n 
~~~ f1 = 0 \n 
f2 = S - 1 \n 
x1 = q * strides - padding \n 
x2 = x1 + f2 \n 
if x1 < 0 : \n 
~~~ f1 = - x1 \n 
x1 = 0 \n 
~~ if x2 >= X : \n 
~~~ dif = x2 - X + 1 \n 
f2 -= dif \n 
x2 -= dif \n 
~~ return ( slice ( f1 , f2 + 1 ) , slice ( x1 , x2 + 1 ) , f2 - f1 + 1 ) \n 
~~ def bconv_slice ( x , S , Q , padding , strides ) : \n 
~~~ qs = x - ( S - padding - 1 ) \n 
firstF = None \n 
~~~ q = qs + s \n 
if q % strides == 0 : \n 
~~~ q strides \n 
if q >= 0 and q < Q : \n 
~~~ if firstF is None : \n 
~~~ firstF = s \n 
firstE = q \n 
~~ lastF = s \n 
lastE = q \n 
~~ ~~ ~~ return ( slice ( firstF , lastF + 1 , strides ) , slice ( firstE , lastE + 1 , strides ) , 0 ) \n 
~~ def xprop_direct ( I , F , O , padding , strides , backward = False ) : \n 
~~~ if all ( x == 1 for x in F . shape [ 1 : 3 ] ) : \n 
~~~ C = F . shape [ 0 ] \n 
K = F . shape [ 4 ] \n 
if backward : \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) , I . reshape ( ( K , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) . T , I . reshape ( ( C , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~ return \n 
~~ if backward : \n 
~~~ F = np . transpose ( F [ : , : : - 1 , : : - 1 , : ] , ( 3 , 1 , 2 , 0 ) ) . copy ( ) \n 
xconv_slice = bconv_slice \n 
~~~ xconv_slice = fconv_slice \n 
~~ C , Y , X , N = I . shape \n 
C , R , S , K = F . shape \n 
K , P , Q , N = O . shape \n 
qSlice = [ xconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
for p in range ( P ) : \n 
~~~ sliceR , sliceY , _ = xconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
for q in range ( Q ) : \n 
~~~ sliceS , sliceX , _ = qSlice [ q ] \n 
slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - 1 , K ) ) \n 
slicedI = I [ : , sliceY , sliceX , : ] . reshape ( ( - 1 , N ) ) \n 
O [ : , p , q , : ] = np . dot ( slicedF . T , slicedI ) \n 
~~ ~~ ~~ def updat_direct ( I , E , U , padding , strides ) : \n 
~~~ C , Y , X , N = I . shape \n 
K , P , Q , N = E . shape \n 
C , R , S , K = U . shape \n 
if all ( x == 1 for x in ( R , S ) ) : \n 
~~~ U [ : ] = np . dot ( I . reshape ( ( C , - 1 ) ) , E . reshape ( ( K , - 1 ) ) . T ) . reshape ( ( U . shape ) ) \n 
return \n 
~~ U . fill ( 0.0 ) \n 
qSlice = [ fconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
~~~ sliceR , sliceY , rlen = fconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
~~~ sliceS , sliceX , slen = qSlice [ q ] \n 
slicedE = E [ : , p , q , : ] \n 
U [ : , sliceR , sliceS , : ] += np . dot ( slicedI , slicedE . T ) . reshape ( ( C , rlen , slen , K ) ) \n 
~~ ~~ ~~ I_4x4_3x3 = ( \n 
np . array ( [ \n 
[ 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 4.0 , - 4.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , - 4.0 , - 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 , - 1.0 , 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 2.0 , - 1.0 , - 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 ] ] ) , \n 
[ 1.0 , 0.0 , - 5.0 / 4.0 , 0.0 , 1.0 / 4.0 , 0.0 ] , \n 
[ 0.0 , 2.0 / 3.0 , 2.0 / 3.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 / 3.0 , 2.0 / 3.0 , 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 12. , - 1.0 / 24. , 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , 1.0 / 12. , - 1.0 / 24. , - 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
F_4x4_3x3 = ( \n 
[ 1.0 / 4.0 , 0.0 , 0.0 ] , \n 
[ - 1.0 / 6.0 , - 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ - 1.0 / 6.0 , 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , - 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 0.0 , 0.0 , 1.0 ] ] ) , \n 
[ 1.0 , 0.0 , 0.0 ] , \n 
[ 1.0 , 1.0 , 1.0 ] , \n 
[ 1.0 , - 1.0 , 1.0 ] , \n 
[ 1.0 , 2.0 , 4.0 ] , \n 
[ 1.0 , - 2.0 , 4.0 ] , \n 
O_4x4_3x3 = ( \n 
[ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 2.0 , - 2.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , 1.0 , 4.0 , 4.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 8.0 , - 8.0 , 1.0 ] ] ) , \n 
[ 1.0 / 4.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 24. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 12. , - 1.0 / 12. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 3.0 , - 1.0 / 3.0 , 1.0 ] ] ) , \n 
rcp3 = 1.0 / 3.0 \n 
rcp4 = 1.0 / 4.0 \n 
rcp6 = 1.0 / 6.0 \n 
rcp12 = 1.0 / 12.0 \n 
rcp24 = 1.0 / 24.0 \n 
def trans_I_4x4_3x3 ( Iw , I , minimal = False , trans = False ) : \n 
~~~ if minimal : \n 
~~~ T0 = np . empty ( ( 6 , 6 ) ) \n 
T1 = np . empty ( ( 6 , 6 ) ) \n 
for O , I in ( ( T0 , I ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = ( I [ 2 , : ] * 4.0 - I [ 4 , : ] ) * rcp6 \n 
t1 = ( I [ 1 , : ] * 4.0 - I [ 3 , : ] ) * rcp6 \n 
t2 = ( I [ 4 , : ] - I [ 2 , : ] ) * rcp24 \n 
t3 = ( I [ 3 , : ] - I [ 1 , : ] ) * rcp12 \n 
O [ 0 , : ] = I [ 0 , : ] + ( I [ 2 , : ] * - 5.0 + I [ 4 , : ] ) * rcp4 \n 
O [ 1 , : ] = t0 + t1 \n 
O [ 2 , : ] = t0 - t1 \n 
O [ 3 , : ] = t2 + t3 \n 
O [ 4 , : ] = t2 - t3 \n 
O [ 5 , : ] = I [ 1 , : ] * 4.0 - I [ 3 , : ] * 5.0 + I [ 5 , : ] \n 
~~ Iw [ : ] = T1 . T \n 
~~~ Iw [ : ] = np . dot ( np . dot ( I_4x4_3x3 [ trans [ 0 ] ] , I ) , I_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_F_4x4_3x3 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 6 , 3 ) ) \n 
for O , I in ( ( T0 , F ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = I [ 0 , : ] + I [ 2 , : ] \n 
t1 = I [ 0 , : ] + I [ 2 , : ] * 4.0 \n 
O [ 0 , : ] = I [ 0 , : ] \n 
O [ 1 , : ] = t0 + I [ 1 , : ] \n 
O [ 2 , : ] = t0 - I [ 1 , : ] \n 
O [ 3 , : ] = t1 + I [ 1 , : ] * 2.0 \n 
O [ 4 , : ] = t1 - I [ 1 , : ] * 2.0 \n 
O [ 5 , : ] = I [ 2 , : ] \n 
~~ Fw [ : ] = T1 . T \n 
~~~ Fw [ : ] = np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] , F ) , F_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_O_4x4_3x3 ( Mw , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 4 , 6 ) ) \n 
T1 = np . empty ( ( 4 , 4 ) ) \n 
for O , I in ( ( T0 , Mw ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = I [ 1 , : ] + I [ 2 , : ] \n 
t1 = I [ 3 , : ] + I [ 4 , : ] \n 
t2 = I [ 1 , : ] - I [ 2 , : ] \n 
t3 = I [ 3 , : ] - I [ 4 , : ] \n 
O [ 0 , : ] = t0 + t1 + I [ 0 , : ] \n 
O [ 1 , : ] = t2 + t3 * 2.0 \n 
O [ 2 , : ] = t0 + t1 * 4.0 \n 
O [ 3 , : ] = t2 + t3 * 8.0 + I [ 5 , : ] \n 
~~ return T1 . T \n 
~~~ return np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] , Mw ) , O_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_F_3x3_4x4 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 6 , 4 ) ) \n 
t2 = I [ 1 , : ] + I [ 3 , : ] \n 
t3 = I [ 1 , : ] + I [ 3 , : ] * 4.0 \n 
O [ 1 , : ] = t0 + t2 \n 
O [ 2 , : ] = t0 - t2 \n 
O [ 3 , : ] = t1 + t3 * 2.0 \n 
O [ 4 , : ] = t1 - t3 * 2.0 \n 
O [ 5 , : ] = I [ 3 , : ] \n 
~~~ Fw [ : ] = np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] . T , F ) , O_4x4_3x3 [ trans [ 1 ] ] ) \n 
~~ ~~ def trans_O_3x3_4x4 ( Mw , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 3 , 6 ) ) \n 
T1 = np . empty ( ( 3 , 3 ) ) \n 
O [ 0 , : ] = I [ 0 , : ] + t0 + t1 \n 
O [ 1 , : ] = I [ 1 , : ] - I [ 2 , : ] + 2 * ( I [ 3 , : ] - I [ 4 , : ] ) \n 
O [ 2 , : ] = t0 + 4 * t1 + I [ 5 , : ] \n 
~~~ return np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] . T , Mw ) , F_4x4_3x3 [ trans [ 1 ] ] ) \n 
~~ ~~ def image_slice ( x , X , B , D , pad = 0 ) : \n 
~~~ start = x * B - pad \n 
stop = start + D \n 
pad = [ 0 , 0 ] \n 
if start < 0 : \n 
~~~ pad [ 0 ] = - start \n 
start = 0 \n 
~~ if stop - 1 >= X : \n 
~~~ pad [ 1 ] = stop - X \n 
~~ return start , stop , pad \n 
~~ def output_slice ( p , P , B ) : \n 
~~~ p0 = p * B \n 
p1 = p0 + B \n 
if p1 > P : \n 
~~~ p1 = P \n 
~~ return p0 , p1 , p1 - p0 \n 
~~ def xprop_winograd ( I , F , O , padding , minimal = False , trans = False , backward = False ) : \n 
~~~ if backward : \n 
padding = [ 2 - p for p in padding ] \n 
B = 4 \n 
D = B + 2 \n 
Yw = ceil_div ( P , B ) \n 
Xw = ceil_div ( Q , B ) \n 
Fw = np . empty ( ( D , D , C , K ) ) \n 
Iw = np . empty ( ( D , D , C , Yw , Xw , N ) ) \n 
for c in range ( C ) : \n 
~~~ for k in range ( K ) : \n 
~~~ trans_F_4x4_3x3 ( Fw [ : , : , c , k ] , F [ c , : , : , k ] , minimal , trans ) \n 
~~ ~~ for y in range ( Yw ) : \n 
~~~ start_y , stop_y , pad_y = image_slice ( y , Y , B , D , padding [ 0 ] ) \n 
for x in range ( Xw ) : \n 
~~~ start_x , stop_x , pad_x = image_slice ( x , X , B , D , padding [ 1 ] ) \n 
sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n 
if any ( pad_y ) or any ( pad_x ) : \n 
~~~ sliceI = np . pad ( sliceI , ( ( 0 , 0 ) , pad_y , pad_x , ( 0 , 0 ) ) , ) \n 
~~ for c in range ( C ) : \n 
~~~ for n in range ( N ) : \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , c , y , x , n ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ for s in range ( D ) : \n 
~~~ for t in range ( D ) : \n 
~~~ Mw [ s , t ] = np . dot ( Fw [ s , t ] . T , Iw [ s , t ] . reshape ( C , - 1 ) ) . reshape ( ( K , Yw , Xw , N ) ) \n 
~~~ p0 , p1 , plen = output_slice ( y , P , B ) \n 
~~~ q0 , q1 , qlen = output_slice ( x , Q , B ) \n 
for k in range ( K ) : \n 
~~~ Out = trans_O_4x4_3x3 ( Mw [ : , : , k , y , x , n ] , minimal , trans ) \n 
O [ k , p0 : p1 , q0 : q1 , n ] = Out [ 0 : plen , 0 : qlen ] \n 
~~ ~~ ~~ ~~ ~~ def updat_winograd ( I , E , U , padding , minimal = False , trans = False , inner = True ) : \n 
Iw = np . empty ( ( D , D , N , C ) ) \n 
Ew = np . empty ( ( D , D , N , K ) ) \n 
if inner : \n 
~~~ Mw = np . empty ( ( D , D , C , K ) ) \n 
U . fill ( 0.0 ) \n 
~~~ Mw = np . zeros ( ( D , D , C , K ) ) \n 
~~ for y in range ( Yw ) : \n 
start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n 
start_q , stop_q , pad_q = image_slice ( x , Q , B , B ) \n 
sliceE = E [ : , start_p : stop_p , start_q : stop_q , : ] \n 
~~ if any ( pad_p ) or any ( pad_q ) : \n 
~~~ sliceE = np . pad ( sliceE , ( ( 0 , 0 ) , pad_p , pad_q , ( 0 , 0 ) ) , ) \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , n , c ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
~~ ~~ for k in range ( K ) : \n 
~~~ trans_F_3x3_4x4 ( Ew [ : , : , n , k ] , sliceE [ k , : , : , n ] , minimal , trans ) \n 
~~ ~~ for s in range ( D ) : \n 
~~~ if inner : \n 
~~~ Mw [ s , t ] = np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
~~~ Mw [ s , t ] += np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
~~ ~~ ~~ if inner : \n 
~~~ for c in range ( C ) : \n 
~~~ U [ c , : , : , k ] += trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ ~~ if not inner : \n 
~~~ U [ c , : , : , k ] = trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ np . set_printoptions ( threshold = 8192 * 4 , linewidth = 600 , formatter = { : lambda x : "%6.3f" % x } ) \n 
minimal = 1 \n 
trans = ( 2 , 2 ) \n 
ones = 0 \n 
N = 32 \n 
C , K = 32 , 32 \n 
Y , X = 6 , 6 \n 
P = out_dim ( R , Y , padding [ 0 ] , strides [ 0 ] ) \n 
Q = out_dim ( S , X , padding [ 1 ] , strides [ 1 ] ) \n 
print P , Q \n 
dimI = ( C , Y , X , N ) \n 
dimF = ( C , R , S , K ) \n 
dimO = ( K , P , Q , N ) \n 
if ones : \n 
~~~ I = np . zeros ( dimI ) \n 
F = np . ones ( dimF ) \n 
E = np . zeros ( dimO ) \n 
for p , q in np . ndindex ( ( Y , X ) ) : \n 
~~~ I [ : , p , q , : ] = np . identity ( N ) \n 
~~ for p , q in np . ndindex ( ( P , Q ) ) : \n 
~~~ E [ : , p , q , n ] = range ( K ) \n 
~~~ I = np . random . uniform ( - 1.0 , 1.0 , dimI ) \n 
F = np . random . uniform ( - 1.0 , 1.0 , dimF ) \n 
E = np . random . uniform ( - 1.0 , 1.0 , dimO ) \n 
~~ Od = np . empty ( dimO ) \n 
Bd = np . empty ( dimI ) \n 
Ud = np . empty ( dimF ) \n 
Uw = np . empty ( dimF ) \n 
xprop_direct ( I , F , Od , padding , strides ) \n 
xprop_winograd ( I , F , Ow , padding , minimal = minimal , trans = trans ) \n 
xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n 
xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n 
updat_direct ( I , E , Ud , padding , strides ) \n 
updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n 
difO = Od - Ow \n 
difB = Bd - Bw \n 
difU = Ud - Uw \n 
print abs ( difO ) . max ( ) / Od . max ( ) \n 
print abs ( difB ) . max ( ) / Bd . max ( ) \n 
print abs ( difU ) . max ( ) / Ud . max ( ) \n 
from neon . layers . layer import ( Linear , Bias , Affine , Conv , Convolution , GeneralizedCost , Dropout , \n 
Pooling , Activation , DataTransform , BatchNorm , BatchNormAutodiff , \n 
Deconv , Deconvolution , GeneralizedCostMask , LookupTable , \n 
BranchNode , SkipNode , LRN , ColorNoise ) \n 
from neon . layers . recurrent import ( Recurrent , LSTM , GRU , RecurrentSum , RecurrentMean , RecurrentLast , \n 
BiRNN , BiLSTM , DeepBiRNN , DeepBiLSTM ) \n 
from neon . layers . container import ( Tree , Sequential , MergeMultistream , MergeBroadcast , Multicost , \n 
RoiPooling , MergeSum , SingleOutputTree ) \n 
from neon . util . argparser import NeonArgparser \n 
from neon . initializers import Constant , Gaussian \n 
from neon . layers import Conv , Dropout , Pooling , GeneralizedCost , Affine \n 
from neon . optimizers import GradientDescentMomentum , MultiOptimizer , Schedule \n 
from neon . transforms import Rectlin , Softmax , CrossEntropyMulti , TopKMisclassification \n 
from neon . models import Model \n 
from neon . data import ImageLoader \n 
from neon . callbacks . callbacks import Callbacks \n 
parser = NeonArgparser ( __doc__ ) \n 
img_set_options = dict ( repo_dir = args . data_dir , \n 
inner_size = 224 , \n 
subset_pct = 0.09990891117239205 ) \n 
train = ImageLoader ( set_name = , scale_range = ( 256 , 256 ) , shuffle = False , \n 
do_transforms = False , ** img_set_options ) \n 
test = ImageLoader ( set_name = , scale_range = ( 256 , 384 ) , shuffle = False , \n 
layers = [ Conv ( ( 11 , 11 , 64 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 3 , strides = 4 ) , \n 
Pooling ( 3 , strides = 2 ) , \n 
Conv ( ( 5 , 5 , 192 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , \n 
activation = Rectlin ( ) , padding = 2 ) , \n 
Conv ( ( 3 , 3 , 384 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 1 ) , \n 
Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n 
Affine ( nout = 4096 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , activation = Rectlin ( ) ) , \n 
Dropout ( keep = 1.0 ) , \n 
Affine ( nout = 1000 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( - 7 ) , activation = Softmax ( ) ) ] \n 
model = Model ( layers = layers ) \n 
weight_sched = Schedule ( [ 22 , 44 , 65 ] , ( 1 / 250. ) ** ( 1 / 3. ) ) \n 
opt_gdm = GradientDescentMomentum ( 0.01 / 10 , 0.9 , wdecay = 0.0005 , schedule = weight_sched , \n 
stochastic_round = args . rounding ) \n 
opt_biases = GradientDescentMomentum ( 0.02 / 10 , 0.9 , schedule = Schedule ( [ 44 ] , 0.1 ) , \n 
opt = MultiOptimizer ( { : opt_gdm , : opt_biases } ) \n 
valmetric = TopKMisclassification ( k = 5 ) \n 
callbacks = Callbacks ( model , eval_set = test , metric = valmetric , ** args . callback_args ) \n 
cost = GeneralizedCost ( costfunc = CrossEntropyMulti ( ) ) \n 
model . fit ( train , optimizer = opt , num_epochs = args . epochs , cost = cost , callbacks = callbacks ) \n 
import itertools as itt \n 
from neon import NervanaObject \n 
from neon . layers . layer import Pooling \n 
from tests . utils import allclose_with_out \n 
def pytest_generate_tests ( metafunc ) : \n 
~~~ np . random . seed ( 1 ) \n 
if metafunc . config . option . all : \n 
~~~ bsz_rng = [ 32 , 64 ] \n 
~~~ bsz_rng = [ 128 ] \n 
~~ if in metafunc . fixturenames : \n 
~~~ fargs = [ ] \n 
~~~ fs_rng = [ 2 , 3 , 5 ] \n 
pad_rng = [ 0 , 1 ] \n 
nifm_rng = [ 16 , 32 ] \n 
in_sz_rng = [ 8 , 16 ] \n 
~~~ fs_rng = [ 2 , 4 ] \n 
nifm_rng = [ 8 ] \n 
in_sz_rng = [ 8 ] \n 
~~ fargs_ = [ ] \n 
for fs in fs_rng : \n 
~~~ stride_rng = set ( [ 1 , fs / 2 , fs ] ) \n 
fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n 
~~ fargs = itt . chain ( * fargs_ ) \n 
metafunc . parametrize ( , fargs ) \n 
~~ ~~ def ref_pooling ( inp , inp_shape , fshape , padding , strides , be , ncheck = None ) : \n 
~~~ inp_lshape = list ( inp_shape ) \n 
bsz = inp . shape [ - 1 ] \n 
if ncheck is None : \n 
~~~ check_inds = np . arange ( bsz ) \n 
~~ elif type ( ncheck ) is int : \n 
~~~ check_inds = np . random . permutation ( bsz ) \n 
check_inds = check_inds [ 0 : ncheck ] \n 
~~~ check_inds = ncheck \n 
~~ check_inds = np . sort ( check_inds ) \n 
inp_lshape . append ( bsz ) \n 
inpa = inp . get ( ) . reshape ( inp_lshape ) \n 
outshape = ( inp_lshape [ 0 ] , \n 
be . output_dim ( inp_lshape [ 1 ] , fshape [ 0 ] , padding , strides [ 0 ] , pooling = True ) , \n 
be . output_dim ( inp_lshape [ 2 ] , fshape [ 1 ] , padding , strides [ 1 ] , pooling = True ) , \n 
len ( check_inds ) ) \n 
if padding > 0 : \n 
~~~ padded_shape = ( inp_lshape [ 0 ] , \n 
inp_lshape [ 1 ] + 2 * padding , \n 
inp_lshape [ 2 ] + 2 * padding , \n 
inp_lshape [ - 1 ] ) \n 
inp_pad = np . zeros ( padded_shape ) \n 
inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , 0 : , 0 : , : ] \n 
~~~ inp_pad = inpa \n 
~~ out_exp = np . zeros ( outshape ) \n 
for indC in range ( outshape [ 0 ] ) : \n 
~~~ for indh in range ( outshape [ 1 ] ) : \n 
~~~ hrng = ( indh * strides [ 0 ] , indh * strides [ 0 ] + fshape [ 0 ] ) \n 
for indw in range ( outshape [ 2 ] ) : \n 
~~~ wrng = ( indw * strides [ 1 ] , indw * strides [ 1 ] + fshape [ 1 ] ) \n 
for cnt , indb in enumerate ( check_inds ) : \n 
~~~ inp_check = inp_pad [ indC , hrng [ 0 ] : hrng [ 1 ] , wrng [ 0 ] : wrng [ 1 ] , indb ] \n 
out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n 
~~ ~~ ~~ ~~ return ( out_exp , check_inds ) \n 
~~ def test_padding ( backend_default , poolargs ) : \n 
~~~ fshape , nifm , padding , stride , in_sz , batch_size = poolargs \n 
NervanaObject . be . bsz = batch_size \n 
inshape = ( nifm , in_sz , in_sz ) \n 
insize = np . prod ( inshape ) \n 
neon_layer = Pooling ( fshape = fshape , strides = stride , padding = padding ) \n 
inp = neon_layer . be . array ( np . random . random ( ( insize , batch_size ) ) ) \n 
inp . lshape = inshape \n 
neon_layer . configure ( inshape ) \n 
neon_layer . prev_layer = True \n 
neon_layer . allocate ( ) \n 
neon_layer . set_deltas ( [ neon_layer . be . iobuf ( inshape ) ] ) \n 
out = neon_layer . fprop ( inp ) . get ( ) \n 
ncheck = [ 0 , batch_size / 2 , batch_size - 1 ] \n 
( out_exp , check_inds ) = ref_pooling ( inp , inp . lshape , \n 
( fshape , fshape ) , \n 
padding , \n 
( stride , stride ) , \n 
neon_layer . be , \n 
ncheck = ncheck ) \n 
out_shape = list ( out_exp . shape [ 0 : 3 ] ) \n 
out_shape . append ( batch_size ) \n 
outa = out . reshape ( out_shape ) \n 
assert allclose_with_out ( out_exp , outa [ : , : , : , check_inds ] , atol = 0.0 , rtol = 0.0 ) \n 
~~ \n 
from __future__ import absolute_import , division , print_function \n 
from neo . core . container import Container \n 
class RecordingChannelGroup ( Container ) : \n 
_container_child_objects = ( , ) \n 
_data_child_objects = ( , ) \n 
_multi_child_objects = ( , ) \n 
_single_parent_objects = ( , ) \n 
_recommended_attrs = ( ( ( , np . ndarray , 1 , np . dtype ( ) ) , \n 
( , np . ndarray , 1 , np . dtype ( ) ) ) + \n 
Container . _recommended_attrs ) \n 
def __init__ ( self , channel_names = None , channel_indexes = None , name = None , \n 
description = None , file_origin = None , ** annotations ) : \n 
super ( RecordingChannelGroup , self ) . __init__ ( name = name , \n 
description = description , \n 
file_origin = file_origin , \n 
** annotations ) \n 
if channel_indexes is None : \n 
~~~ channel_indexes = np . array ( [ ] , dtype = np . int ) \n 
~~ if channel_names is None : \n 
~~~ channel_names = np . array ( [ ] , dtype = ) \n 
~~ self . channel_names = channel_names \n 
self . channel_indexes = channel_indexes \n 
import ctypes \n 
~~~ file \n 
~~ except NameError : \n 
~~~ import io \n 
file = io . BufferedReader \n 
import quantities as pq \n 
from neo . io . baseio import BaseIO \n 
from neo . core import Segment , AnalogSignal , SpikeTrain , EventArray \n 
class NeuroshareError ( Exception ) : \n 
~~~ def __init__ ( self , lib , errno ) : \n 
~~~ self . lib = lib \n 
self . errno = errno \n 
pszMsgBuffer = ctypes . create_string_buffer ( 256 ) \n 
self . lib . ns_GetLastErrorMsg ( pszMsgBuffer , ctypes . c_uint32 ( 256 ) ) \n 
errstr = . format ( errno , pszMsgBuffer . value ) \n 
Exception . __init__ ( self , errstr ) \n 
~~ ~~ class DllWithError ( ) : \n 
~~~ def __init__ ( self , lib ) : \n 
~~ def __getattr__ ( self , attr ) : \n 
~~~ f = getattr ( self . lib , attr ) \n 
return self . decorate_with_error ( f ) \n 
~~ def decorate_with_error ( self , f ) : \n 
~~~ def func_with_error ( * args ) : \n 
~~~ errno = f ( * args ) \n 
if errno != ns_OK : \n 
~~~ raise NeuroshareError ( self . lib , errno ) \n 
~~ return errno \n 
~~ return func_with_error \n 
~~ ~~ class NeurosharectypesIO ( BaseIO ) : \n 
is_readable = True \n 
is_writable = False \n 
supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n 
readable_objects = [ Segment ] \n 
writeable_objects = [ ] \n 
has_header = False \n 
is_streameable = False \n 
read_params = { Segment : [ ] } \n 
write_params = None \n 
name = \n 
extensions = [ ] \n 
mode = \n 
def __init__ ( self , filename = , dllname = ) : \n 
BaseIO . __init__ ( self ) \n 
self . dllname = dllname \n 
self . filename = filename \n 
~~ def read_segment ( self , import_neuroshare_segment = True , \n 
lazy = False , cascade = True ) : \n 
seg = Segment ( file_origin = os . path . basename ( self . filename ) , ) \n 
if sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . windll . LoadLibrary ( self . dllname ) \n 
~~ elif sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . cdll . LoadLibrary ( self . dllname ) \n 
~~ neuroshare = DllWithError ( neuroshare ) \n 
info = ns_LIBRARYINFO ( ) \n 
neuroshare . ns_GetLibraryInfo ( ctypes . byref ( info ) , ctypes . sizeof ( info ) ) \n 
seg . annotate ( neuroshare_version = str ( info . dwAPIVersionMaj ) + + str ( info . dwAPIVersionMin ) ) \n 
if not cascade : \n 
~~~ return seg \n 
~~ hFile = ctypes . c_uint32 ( 0 ) \n 
neuroshare . ns_OpenFile ( ctypes . c_char_p ( self . filename ) , ctypes . byref ( hFile ) ) \n 
fileinfo = ns_FILEINFO ( ) \n 
neuroshare . ns_GetFileInfo ( hFile , ctypes . byref ( fileinfo ) , ctypes . sizeof ( fileinfo ) ) \n 
for dwEntityID in range ( fileinfo . dwEntityCount ) : \n 
~~~ entityInfo = ns_ENTITYINFO ( ) \n 
neuroshare . ns_GetEntityInfo ( hFile , dwEntityID , ctypes . byref ( entityInfo ) , ctypes . sizeof ( entityInfo ) ) \n 
if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pEventInfo = ns_EVENTINFO ( ) \n 
neuroshare . ns_GetEventInfo ( hFile , dwEntityID , ctypes . byref ( pEventInfo ) , ctypes . sizeof ( pEventInfo ) ) \n 
if pEventInfo . dwEventType == 0 : #TEXT \n 
~~~ pData = ctypes . create_string_buffer ( pEventInfo . dwMaxDataLength ) \n 
~~ elif pEventInfo . dwEventType == 1 : #CVS \n 
~~~ pData = ctypes . c_byte ( 0 ) \n 
~~~ pData = ctypes . c_int16 ( 0 ) \n 
~~~ pData = ctypes . c_int32 ( 0 ) \n 
~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
pdwDataRetSize = ctypes . c_uint32 ( 0 ) \n 
ea = EventArray ( name = str ( entityInfo . szEntityLabel ) , ) \n 
if not lazy : \n 
~~~ times = [ ] \n 
labels = [ ] \n 
for dwIndex in range ( entityInfo . dwItemCount ) : \n 
~~~ neuroshare . ns_GetEventData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , ctypes . byref ( pData ) , \n 
ctypes . sizeof ( pData ) , ctypes . byref ( pdwDataRetSize ) ) \n 
times . append ( pdTimeStamp . value ) \n 
labels . append ( str ( pData . value ) ) \n 
~~ ea . times = times * pq . s \n 
ea . labels = np . array ( labels , dtype = ) \n 
~~~ ea . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . eventarrays . append ( ea ) \n 
~~ if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pAnalogInfo = ns_ANALOGINFO ( ) \n 
neuroshare . ns_GetAnalogInfo ( hFile , dwEntityID , ctypes . byref ( pAnalogInfo ) , ctypes . sizeof ( pAnalogInfo ) ) \n 
dwIndexCount = entityInfo . dwItemCount \n 
if lazy : \n 
~~~ signal = [ ] * pq . Quantity ( 1 , pAnalogInfo . szUnits ) \n 
~~~ pdwContCount = ctypes . c_uint32 ( 0 ) \n 
pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
total_read = 0 \n 
while total_read < entityInfo . dwItemCount : \n 
~~~ dwStartIndex = ctypes . c_uint32 ( total_read ) \n 
dwStopIndex = ctypes . c_uint32 ( entityInfo . dwItemCount - total_read ) \n 
neuroshare . ns_GetAnalogData ( hFile , dwEntityID , dwStartIndex , \n 
dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n 
total_read += pdwContCount . value \n 
~~ signal = pq . Quantity ( pData , units = pAnalogInfo . szUnits , copy = False ) \n 
#t_start \n 
~~ dwIndex = 0 \n 
pdTime = ctypes . c_double ( 0 ) \n 
neuroshare . ns_GetTimeByIndex ( hFile , dwEntityID , dwIndex , ctypes . byref ( pdTime ) ) \n 
anaSig = AnalogSignal ( signal , \n 
sampling_rate = pAnalogInfo . dSampleRate * pq . Hz , \n 
t_start = pdTime . value * pq . s , \n 
name = str ( entityInfo . szEntityLabel ) , \n 
anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n 
~~~ anaSig . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . analogsignals . append ( anaSig ) \n 
#segment \n 
~~ if entity_types [ entityInfo . dwEntityType ] == and import_neuroshare_segment : \n 
~~~ pdwSegmentInfo = ns_SEGMENTINFO ( ) \n 
if not str ( entityInfo . szEntityLabel ) . startswith ( ) : \n 
~~ neuroshare . ns_GetSegmentInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pdwSegmentInfo ) , ctypes . sizeof ( pdwSegmentInfo ) ) \n 
nsource = pdwSegmentInfo . dwSourceCount \n 
neuroshare . ns_GetLastErrorMsg ( ctypes . byref ( pszMsgBuffer ) , 256 ) \n 
for dwSourceID in range ( pdwSegmentInfo . dwSourceCount ) : \n 
~~~ pSourceInfo = ns_SEGSOURCEINFO ( ) \n 
neuroshare . ns_GetSegmentSourceInfo ( hFile , dwEntityID , dwSourceID , \n 
ctypes . byref ( pSourceInfo ) , ctypes . sizeof ( pSourceInfo ) ) \n 
~~ if lazy : \n 
~~~ sptr = SpikeTrain ( times , name = str ( entityInfo . szEntityLabel ) , t_stop = 0. * pq . s ) \n 
sptr . lazy_shape = entityInfo . dwItemCount \n 
~~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
dwDataBufferSize = pdwSegmentInfo . dwMaxSampleCount * pdwSegmentInfo . dwSourceCount \n 
pData = np . zeros ( ( dwDataBufferSize ) , dtype = ) \n 
pdwSampleCount = ctypes . c_uint32 ( 0 ) \n 
pdwUnitID = ctypes . c_uint32 ( 0 ) \n 
nsample = int ( dwDataBufferSize ) \n 
times = np . empty ( ( entityInfo . dwItemCount ) , dtype = ) \n 
waveforms = np . empty ( ( entityInfo . dwItemCount , nsource , nsample ) , dtype = ) \n 
~~~ neuroshare . ns_GetSegmentData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) , \n 
dwDataBufferSize * 8 , ctypes . byref ( pdwSampleCount ) , \n 
ctypes . byref ( pdwUnitID ) ) \n 
times [ dwIndex ] = pdTimeStamp . value \n 
waveforms [ dwIndex , : , : ] = pData [ : nsample * nsource ] . reshape ( nsample , nsource ) . transpose ( ) \n 
~~ sptr = SpikeTrain ( times = pq . Quantity ( times , units = , copy = False ) , \n 
t_stop = times . max ( ) , \n 
waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo . szUnits ) , copy = False ) , \n 
left_sweep = nsample / 2. / float ( pdwSegmentInfo . dSampleRate ) * pq . s , \n 
sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n 
~~ seg . spiketrains . append ( sptr ) \n 
~~~ pNeuralInfo = ns_NEURALINFO ( ) \n 
neuroshare . ns_GetNeuralInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n 
~~~ times = [ ] * pq . s \n 
t_stop = 0 * pq . s \n 
~~~ pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
dwStartIndex = 0 \n 
neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n 
dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n 
times = pData * pq . s \n 
t_stop = times . max ( ) \n 
~~ sptr = SpikeTrain ( times , t_stop = t_stop , \n 
name = str ( entityInfo . szEntityLabel ) , ) \n 
~~~ sptr . lazy_shape = entityInfo . dwItemCount \n 
~~ ~~ neuroshare . ns_CloseFile ( hFile ) \n 
seg . create_many_to_one_relationship ( ) \n 
return seg \n 
~~ ~~ class ns_FILEDESC ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_char * 32 ) , \n 
( , ctypes . c_char * 8 ) , \n 
( , ctypes . c_char * 16 ) , \n 
~~ class ns_LIBRARYINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 64 ) , \n 
( , ns_FILEDESC * 16 ) , \n 
~~ class ns_FILEINFO ( ctypes . Structure ) : \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_char * 256 ) , \n 
~~ class ns_ENTITYINFO ( ctypes . Structure ) : \n 
~~ entity_types = { 0 : , \n 
1 : , \n 
2 : , \n 
3 : , \n 
4 : , \n 
class ns_EVENTINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ \n 
( , ctypes . c_char * 128 ) , \n 
~~ class ns_ANALOGINFO ( ctypes . Structure ) : \n 
~~ class ns_SEGMENTINFO ( ctypes . Structure ) : \n 
( , ctypes . c_char * 32 ) , \n 
~~ class ns_SEGSOURCEINFO ( ctypes . Structure ) : \n 
~~ class ns_NEURALINFO ( ctypes . Structure ) : \n 
~~~ import unittest2 as unittest \n 
~~~ import unittest \n 
~~~ from IPython . lib . pretty import pretty \n 
~~ except ImportError as err : \n 
~~~ HAVE_IPYTHON = False \n 
~~~ HAVE_IPYTHON = True \n 
~~ from neo . core . segment import Segment \n 
from neo . core import ( AnalogSignalArray , Block , \n 
Epoch , EpochArray , \n 
RecordingChannelGroup , SpikeTrain , Unit ) \n 
from neo . core . container import filterdata \n 
from neo . test . tools import ( assert_neo_object_is_compliant , \n 
assert_same_sub_schema ) \n 
from neo . test . generate_datasets import ( fake_neo , get_fake_value , \n 
get_fake_values , get_annotations , \n 
clone_object , TEST_ANNOTATIONS ) \n 
class Test__generate_datasets ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ np . random . seed ( 0 ) \n 
self . annotations = dict ( [ ( str ( x ) , TEST_ANNOTATIONS [ x ] ) for x in \n 
range ( len ( TEST_ANNOTATIONS ) ) ] ) \n 
~~ def test__get_fake_values ( self ) : \n 
~~~ self . annotations [ ] = 0 \n 
file_datetime = get_fake_value ( , datetime , seed = 0 ) \n 
rec_datetime = get_fake_value ( , datetime , seed = 1 ) \n 
index = get_fake_value ( , int , seed = 2 ) \n 
name = get_fake_value ( , str , seed = 3 , obj = Segment ) \n 
description = get_fake_value ( , str , seed = 4 , obj = ) \n 
file_origin = get_fake_value ( , str ) \n 
attrs1 = { : file_datetime , \n 
: rec_datetime , \n 
: index , \n 
: name , \n 
: description , \n 
: file_origin } \n 
attrs2 = attrs1 . copy ( ) \n 
attrs2 . update ( self . annotations ) \n 
res11 = get_fake_values ( Segment , annotate = False , seed = 0 ) \n 
res12 = get_fake_values ( , annotate = False , seed = 0 ) \n 
res21 = get_fake_values ( Segment , annotate = True , seed = 0 ) \n 
res22 = get_fake_values ( , annotate = True , seed = 0 ) \n 
self . assertEqual ( res11 , attrs1 ) \n 
self . assertEqual ( res12 , attrs1 ) \n 
self . assertEqual ( res21 , attrs2 ) \n 
self . assertEqual ( res22 , attrs2 ) \n 
~~ def test__fake_neo__cascade ( self ) : \n 
~~~ self . annotations [ ] = None \n 
obj_type = Segment \n 
cascade = True \n 
res = fake_neo ( obj_type = obj_type , cascade = cascade ) \n 
self . assertTrue ( isinstance ( res , Segment ) ) \n 
assert_neo_object_is_compliant ( res ) \n 
self . assertEqual ( res . annotations , self . annotations ) \n 
self . assertEqual ( len ( res . analogsignalarrays ) , 1 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 1 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 1 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 1 ) \n 
self . assertEqual ( len ( res . spikes ) , 1 ) \n 
self . assertEqual ( len ( res . events ) , 1 ) \n 
self . assertEqual ( len ( res . epochs ) , 1 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 1 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 1 ) \n 
for child in res . children : \n 
~~~ del child . annotations [ ] \n 
del child . annotations [ ] \n 
~~ self . assertEqual ( res . analogsignalarrays [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . analogsignals [ 0 ] . annotations , \n 
self . assertEqual ( res . irregularlysampledsignals [ 0 ] . annotations , \n 
self . assertEqual ( res . spiketrains [ 0 ] . annotations , \n 
self . assertEqual ( res . spikes [ 0 ] . annotations , \n 
self . assertEqual ( res . events [ 0 ] . annotations , \n 
self . assertEqual ( res . epochs [ 0 ] . annotations , \n 
self . assertEqual ( res . eventarrays [ 0 ] . annotations , \n 
self . assertEqual ( res . epocharrays [ 0 ] . annotations , \n 
~~ def test__fake_neo__nocascade ( self ) : \n 
obj_type = \n 
cascade = False \n 
self . assertEqual ( len ( res . analogsignalarrays ) , 0 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 0 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 0 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 0 ) \n 
self . assertEqual ( len ( res . spikes ) , 0 ) \n 
self . assertEqual ( len ( res . events ) , 0 ) \n 
self . assertEqual ( len ( res . epochs ) , 0 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 0 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 0 ) \n 
~~ ~~ class TestSegment ( unittest . TestCase ) : \n 
~~~ self . nchildren = 2 \n 
blk = fake_neo ( Block , seed = 0 , n = self . nchildren ) \n 
self . unit1 , self . unit2 , self . unit3 , self . unit4 = blk . list_units \n 
self . seg1 , self . seg2 = blk . segments \n 
self . targobj = self . seg1 \n 
self . seed1 = self . seg1 . annotations [ ] \n 
self . seed2 = self . seg2 . annotations [ ] \n 
del self . seg1 . annotations [ ] \n 
del self . seg2 . annotations [ ] \n 
self . sigs1 = self . seg1 . analogsignals \n 
self . sigs2 = self . seg2 . analogsignals \n 
self . sigarrs1 = self . seg1 . analogsignalarrays \n 
self . sigarrs2 = self . seg2 . analogsignalarrays \n 
self . irsigs1 = self . seg1 . irregularlysampledsignals \n 
self . irsigs2 = self . seg2 . irregularlysampledsignals \n 
self . spikes1 = self . seg1 . spikes \n 
self . spikes2 = self . seg2 . spikes \n 
self . trains1 = self . seg1 . spiketrains \n 
self . trains2 = self . seg2 . spiketrains \n 
self . epcs1 = self . seg1 . epochs \n 
self . epcs2 = self . seg2 . epochs \n 
self . epcas1 = self . seg1 . epocharrays \n 
self . epcas2 = self . seg2 . epocharrays \n 
self . evts1 = self . seg1 . events \n 
self . evts2 = self . seg2 . events \n 
self . evtas1 = self . seg1 . eventarrays \n 
self . evtas2 = self . seg2 . eventarrays \n 
self . sigs1a = clone_object ( self . sigs1 ) \n 
self . sigarrs1a = clone_object ( self . sigarrs1 , n = 2 ) \n 
self . irsigs1a = clone_object ( self . irsigs1 ) \n 
self . spikes1a = clone_object ( self . spikes1 ) \n 
self . trains1a = clone_object ( self . trains1 ) \n 
self . epcs1a = clone_object ( self . epcs1 ) \n 
self . epcas1a = clone_object ( self . epcas1 ) \n 
self . evts1a = clone_object ( self . evts1 ) \n 
self . evtas1a = clone_object ( self . evtas1 ) \n 
for obj , obja in zip ( self . sigs1 + self . sigarrs1 , \n 
self . sigs1a + self . sigarrs1a ) : \n 
~~~ obja . channel_index = obj . channel_index \n 
~~ ~~ def test_init ( self ) : \n 
~~~ seg = Segment ( name = , index = 3 ) \n 
assert_neo_object_is_compliant ( seg ) \n 
self . assertEqual ( seg . name , ) \n 
self . assertEqual ( seg . file_origin , None ) \n 
self . assertEqual ( seg . index , 3 ) \n 
~~ def check_creation ( self , seg ) : \n 
~~~ assert_neo_object_is_compliant ( seg ) \n 
seed = seg . annotations [ ] \n 
targ0 = get_fake_value ( , datetime , seed = seed + 0 ) \n 
self . assertEqual ( seg . file_datetime , targ0 ) \n 
targ1 = get_fake_value ( , datetime , seed = seed + 1 ) \n 
self . assertEqual ( seg . rec_datetime , targ1 ) \n 
targ2 = get_fake_value ( , int , seed = seed + 2 ) \n 
self . assertEqual ( seg . index , targ2 ) \n 
targ3 = get_fake_value ( , str , seed = seed + 3 , obj = Segment ) \n 
self . assertEqual ( seg . name , targ3 ) \n 
targ4 = get_fake_value ( , str , \n 
seed = seed + 4 , obj = Segment ) \n 
self . assertEqual ( seg . description , targ4 ) \n 
targ5 = get_fake_value ( , str ) \n 
self . assertEqual ( seg . file_origin , targ5 ) \n 
targ6 = get_annotations ( ) \n 
targ6 [ ] = seed \n 
self . assertEqual ( seg . annotations , targ6 ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertEqual ( len ( seg . analogsignals ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . analogsignalarrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . irregularlysampledsignals ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . epochs ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . epocharrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . events ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . eventarrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . spikes ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . spiketrains ) , self . nchildren ** 2 ) \n 
~~ def test__creation ( self ) : \n 
~~~ self . check_creation ( self . seg1 ) \n 
self . check_creation ( self . seg2 ) \n 
~~ def test__merge ( self ) : \n 
~~~ seg1a = fake_neo ( Block , seed = self . seed1 , n = self . nchildren ) . segments [ 0 ] \n 
assert_same_sub_schema ( self . seg1 , seg1a ) \n 
seg1a . spikes . append ( self . spikes2 [ 0 ] ) \n 
seg1a . epocharrays . append ( self . epcas2 [ 0 ] ) \n 
seg1a . annotate ( seed = self . seed2 ) \n 
seg1a . merge ( self . seg2 ) \n 
assert_same_sub_schema ( self . sigs1a + self . sigs2 , seg1a . analogsignals ) \n 
assert_same_sub_schema ( self . sigarrs1a + self . sigarrs2 , \n 
seg1a . analogsignalarrays ) \n 
assert_same_sub_schema ( self . irsigs1a + self . irsigs2 , \n 
seg1a . irregularlysampledsignals ) \n 
assert_same_sub_schema ( self . epcs1 + self . epcs2 , seg1a . epochs ) \n 
assert_same_sub_schema ( self . epcas1 + self . epcas2 , seg1a . epocharrays ) \n 
assert_same_sub_schema ( self . evts1 + self . evts2 , seg1a . events ) \n 
assert_same_sub_schema ( self . evtas1 + self . evtas2 , seg1a . eventarrays ) \n 
assert_same_sub_schema ( self . spikes1 + self . spikes2 , seg1a . spikes ) \n 
assert_same_sub_schema ( self . trains1 + self . trains2 , seg1a . spiketrains ) \n 
~~ def test__children ( self ) : \n 
~~~ blk = Block ( name = ) \n 
blk . segments = [ self . seg1 ] \n 
blk . create_many_to_one_relationship ( force = True ) \n 
assert_neo_object_is_compliant ( self . seg1 ) \n 
assert_neo_object_is_compliant ( blk ) \n 
childobjs = ( , , \n 
, , \n 
, \n 
, ) \n 
childconts = ( , , \n 
self . assertEqual ( self . seg1 . _container_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _single_parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _child_properties , ( ) ) \n 
self . assertEqual ( self . seg1 . _single_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _container_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_parent_containers , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _parent_containers , ( , ) ) \n 
self . assertEqual ( len ( self . seg1 . _single_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children_recur ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children_recur ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . _multi_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children_recur ) , 0 ) \n 
children = ( self . sigs1a + self . sigarrs1a + \n 
self . epcs1a + self . epcas1a + \n 
self . evts1a + self . evtas1a + \n 
self . irsigs1a + \n 
self . spikes1a + self . trains1a ) \n 
assert_same_sub_schema ( list ( self . seg1 . _single_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children_recur ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children_recur ) , children ) \n 
self . assertEqual ( len ( self . seg1 . parents ) , 1 ) \n 
self . assertEqual ( self . seg1 . parents [ 0 ] . name , ) \n 
~~ def test__size ( self ) : \n 
~~~ targ1 = { "epochs" : self . nchildren , "events" : self . nchildren , \n 
"analogsignals" : self . nchildren ** 2 , \n 
"irregularlysampledsignals" : self . nchildren ** 2 , \n 
"spikes" : self . nchildren ** 2 , \n 
"spiketrains" : self . nchildren ** 2 , \n 
"epocharrays" : self . nchildren , "eventarrays" : self . nchildren , \n 
"analogsignalarrays" : self . nchildren } \n 
self . assertEqual ( self . targobj . size , targ1 ) \n 
~~ def test__filter_none ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( ) \n 
res1 = self . targobj . filter ( { } ) \n 
res2 = self . targobj . filter ( [ ] ) \n 
res3 = self . targobj . filter ( [ { } ] ) \n 
res4 = self . targobj . filter ( [ { } , { } ] ) \n 
res5 = self . targobj . filter ( [ { } , { } ] ) \n 
res6 = self . targobj . filter ( targdict = { } ) \n 
res7 = self . targobj . filter ( targdict = [ ] ) \n 
res8 = self . targobj . filter ( targdict = [ { } ] ) \n 
res9 = self . targobj . filter ( targdict = [ { } , { } ] ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
assert_same_sub_schema ( res5 , targ ) \n 
assert_same_sub_schema ( res6 , targ ) \n 
assert_same_sub_schema ( res7 , targ ) \n 
assert_same_sub_schema ( res8 , targ ) \n 
assert_same_sub_schema ( res9 , targ ) \n 
~~ def test__filter_annotation_single ( self ) : \n 
~~~ targ = ( self . sigs1a + self . sigarrs1a + \n 
[ self . epcs1a [ 0 ] , self . epcas1a [ 0 ] ] + \n 
[ self . evts1a [ 0 ] , self . evtas1a [ 0 ] ] + \n 
res0 = self . targobj . filter ( j = 0 ) \n 
res1 = self . targobj . filter ( { : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : 0 } ) \n 
res3 = self . targobj . filter ( [ { : 0 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 0 } ] ) \n 
~~ def test__filter_single_annotation_nores ( self ) : \n 
res0 = self . targobj . filter ( j = 5 ) \n 
res1 = self . targobj . filter ( { : 5 } ) \n 
res2 = self . targobj . filter ( targdict = { : 5 } ) \n 
res3 = self . targobj . filter ( [ { : 5 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 5 } ] ) \n 
~~ def test__filter_attribute_single ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } ) \n 
~~ def test__filter_attribute_single_nores ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs2 [ 0 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs2 [ 0 ] . name } ) \n 
~~ def test__filter_multi ( self ) : \n 
self . spikes1a + self . trains1a + \n 
[ self . epcs1a [ 1 ] ] ) \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name , \n 
: 0 } ) \n 
~~ def test__filter_multi_nores ( self ) : \n 
res0 = self . targobj . filter ( [ { : 0 } , { } ] ) \n 
res1 = self . targobj . filter ( { } , ttype = 0 ) \n 
res2 = self . targobj . filter ( [ { } ] , ttype = 0 ) \n 
res3 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
j = 0 ) \n 
res5 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
targdict = { : 0 } ) \n 
res6 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name , \n 
: 5 } ) \n 
res9 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name } , \n 
j = 5 ) \n 
res11 = self . targobj . filter ( name = self . epcs2 [ 1 ] . name , \n 
targdict = { : 5 } ) \n 
res12 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
res14 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
assert_same_sub_schema ( res10 , targ ) \n 
assert_same_sub_schema ( res11 , targ ) \n 
assert_same_sub_schema ( res12 , targ ) \n 
assert_same_sub_schema ( res13 , targ ) \n 
assert_same_sub_schema ( res14 , targ ) \n 
~~ def test__filter_multi_partres ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res3 = self . targobj . filter ( [ { : 1 } , { : 2 } ] ) \n 
res4 = self . targobj . filter ( { : 1 } , i = 2 ) \n 
res5 = self . targobj . filter ( [ { : 1 } ] , i = 2 ) \n 
~~ def test__filter_single_annotation_obj_single ( self ) : \n 
res0 = self . targobj . filter ( j = 1 , objects = ) \n 
res1 = self . targobj . filter ( j = 1 , objects = Epoch ) \n 
res2 = self . targobj . filter ( j = 1 , objects = [ ] ) \n 
res3 = self . targobj . filter ( j = 1 , objects = [ Epoch ] ) \n 
res4 = self . targobj . filter ( j = 1 , objects = [ Epoch , \n 
RecordingChannelGroup ] ) \n 
~~ def test__filter_single_annotation_obj_multi ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , objects = [ , EpochArray ] ) \n 
~~ def test__filter_single_annotation_obj_none ( self ) : \n 
res0 = self . targobj . filter ( j = 1 , objects = RecordingChannelGroup ) \n 
res1 = self . targobj . filter ( j = 1 , objects = ) \n 
~~ def test__filter_single_annotation_norecur ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] , \n 
self . evts1a [ 1 ] , self . evtas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , \n 
recursive = False ) \n 
~~ def test__filter_single_attribute_norecur ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
~~ def test__filter_single_annotation_nodata ( self ) : \n 
res0 = self . targobj . filter ( j = 0 , \n 
data = False ) \n 
~~ def test__filter_single_attribute_nodata ( self ) : \n 
~~ def test__filter_single_annotation_nodata_norecur ( self ) : \n 
data = False , recursive = False ) \n 
~~ def test__filter_single_attribute_nodata_norecur ( self ) : \n 
~~ def test__filter_single_annotation_container ( self ) : \n 
container = True ) \n 
~~ def test__filter_single_attribute_container ( self ) : \n 
~~ def test__filter_single_annotation_container_norecur ( self ) : \n 
container = True , recursive = False ) \n 
~~ def test__filter_single_attribute_container_norecur ( self ) : \n 
~~ def test__filter_single_annotation_nodata_container ( self ) : \n 
data = False , container = True ) \n 
~~ def test__filter_single_attribute_nodata_container ( self ) : \n 
~~ def test__filter_single_annotation_nodata_container_norecur ( self ) : \n 
data = False , container = True , \n 
~~ def test__filter_single_attribute_nodata_container_norecur ( self ) : \n 
data = self . targobj . children_recur \n 
targ = ( self . sigs1a + self . sigarrs1a + \n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
~~ def test__filterdata_multi_nores ( self ) : \n 
~~~ data = self . targobj . children_recur \n 
targ = [ ] \n 
res0 = filterdata ( data , [ { : 0 } , { } ] ) \n 
res1 = filterdata ( data , { } , ttype = 0 ) \n 
res2 = filterdata ( data , [ { } ] , ttype = 0 ) \n 
res3 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res5 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 0 } ) \n 
res6 = filterdata ( data , name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = filterdata ( data , { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res9 = filterdata ( data , { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res11 = filterdata ( data , name = self . epcs2 [ 1 ] . name , targdict = { : 5 } ) \n 
res12 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res14 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 5 } ) \n 
~~ def test__filterdata_multi_partres ( self ) : \n 
targ = [ self . epcs1a [ 1 ] ] \n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res3 = filterdata ( data , [ { : 1 } , { : 2 } ] ) \n 
res4 = filterdata ( data , { : 1 } , i = 2 ) \n 
res5 = filterdata ( data , [ { : 1 } ] , i = 2 ) \n 
def test__pretty ( self ) : \n 
~~~ ann = get_annotations ( ) \n 
ann [ ] = self . seed1 \n 
ann = pretty ( ann ) . replace ( , ) \n 
res = pretty ( self . seg1 ) \n 
sig0 = pretty ( self . sigs1 [ 0 ] ) \n 
sig1 = pretty ( self . sigs1 [ 1 ] ) \n 
sig2 = pretty ( self . sigs1 [ 2 ] ) \n 
sig3 = pretty ( self . sigs1 [ 3 ] ) \n 
sig0 = sig0 . replace ( , ) \n 
sig1 = sig1 . replace ( , ) \n 
sig2 = sig2 . replace ( , ) \n 
sig3 = sig3 . replace ( , ) \n 
sigarr0 = pretty ( self . sigarrs1 [ 0 ] ) \n 
sigarr1 = pretty ( self . sigarrs1 [ 1 ] ) \n 
sigarr0 = sigarr0 . replace ( , ) \n 
sigarr1 = sigarr1 . replace ( , ) \n 
( len ( self . sigs1a ) , len ( self . sigarrs1a ) ) ) + \n 
( len ( self . epcs1a ) , len ( self . epcas1a ) ) ) + \n 
( len ( self . evts1a ) , len ( self . evtas1a ) ) ) + \n 
len ( self . irsigs1a ) ) + \n 
( len ( self . spikes1a ) , len ( self . trains1a ) ) ) + \n 
( self . seg1 . name , self . seg1 . description ) \n 
) + \n 
( % ( 0 , sig0 ) ) + \n 
( % ( 1 , sig1 ) ) + \n 
( % ( 2 , sig2 ) ) + \n 
( % ( 3 , sig3 ) ) + \n 
( % ( 0 , sigarr0 ) ) + \n 
( % ( 1 , sigarr1 ) ) ) \n 
self . assertEqual ( res , targ ) \n 
~~ def test__construct_subsegment_by_unit ( self ) : \n 
~~~ nb_seg = 3 \n 
nb_unit = 7 \n 
unit_with_sig = np . array ( [ 0 , 2 , 5 ] ) \n 
signal_types = [ , ] \n 
sig_len = 100 \n 
#recordingchannelgroups \n 
rcgs = [ RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) , \n 
RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) ] \n 
all_unit = [ ] \n 
for u in range ( nb_unit ) : \n 
~~~ un = Unit ( name = % u , channel_indexes = np . array ( [ u ] ) ) \n 
assert_neo_object_is_compliant ( un ) \n 
all_unit . append ( un ) \n 
~~ blk = Block ( ) \n 
blk . recordingchannelgroups = rcgs \n 
for s in range ( nb_seg ) : \n 
~~~ seg = Segment ( name = % s ) \n 
for j in range ( nb_unit ) : \n 
~~~ st = SpikeTrain ( [ 1 , 2 ] , units = , \n 
t_start = 0. , t_stop = 10 ) \n 
st . unit = all_unit [ j ] \n 
~~ for t in signal_types : \n 
~~~ anasigarr = AnalogSignalArray ( np . zeros ( ( sig_len , \n 
len ( unit_with_sig ) ) ) , \n 
units = , \n 
sampling_rate = 1000. * pq . Hz , \n 
channel_indexes = unit_with_sig ) \n 
seg . analogsignalarrays . append ( anasigarr ) \n 
~~ ~~ blk . create_many_to_one_relationship ( ) \n 
for unit in all_unit : \n 
~~~ assert_neo_object_is_compliant ( unit ) \n 
~~ for rcg in rcgs : \n 
~~~ assert_neo_object_is_compliant ( rcg ) \n 
~~ assert_neo_object_is_compliant ( blk ) \n 
newseg = seg . construct_subsegment_by_unit ( all_unit [ : 4 ] ) \n 
assert_neo_object_is_compliant ( newseg ) \n 
~~ def test_segment_take_spikes_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spikes_by_unit ( ) \n 
result21 = self . seg1 . take_spikes_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spikes_by_unit ( [ self . unit2 ] ) \n 
self . assertEqual ( result1 , [ ] ) \n 
assert_same_sub_schema ( result21 , [ self . spikes1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . spikes1a [ 1 ] ] ) \n 
~~ def test_segment_take_spiketrains_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spiketrains_by_unit ( ) \n 
result21 = self . seg1 . take_spiketrains_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spiketrains_by_unit ( [ self . unit2 ] ) \n 
assert_same_sub_schema ( result21 , [ self . trains1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . trains1a [ 1 ] ] ) \n 
~~ def test_segment_take_analogsignal_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_analogsignal_by_unit ( ) \n 
result21 = self . seg1 . take_analogsignal_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_unit ( [ self . unit2 ] ) \n 
assert_same_sub_schema ( result21 , [ self . sigs1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . sigs1a [ 1 ] ] ) \n 
~~ def test_segment_take_analogsignal_by_channelindex ( self ) : \n 
~~~ ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind2 = self . unit2 . channel_indexes [ 0 ] \n 
result1 = self . seg1 . take_analogsignal_by_channelindex ( ) \n 
result21 = self . seg1 . take_analogsignal_by_channelindex ( [ ind1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n 
~~ def test_seg_take_slice_of_analogsignalarray_by_unit ( self ) : \n 
~~~ seg = self . seg1 \n 
result1 = seg . take_slice_of_analogsignalarray_by_unit ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit3 ] ) \n 
targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ False ] ) ] ] \n 
targ3 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ False ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ True ] ) ] ] \n 
assert_same_sub_schema ( result21 , targ1 ) \n 
assert_same_sub_schema ( result23 , targ3 ) \n 
~~ def test_seg_take_slice_of_analogsignalarray_by_channelindex ( self ) : \n 
ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind3 = self . unit3 . channel_indexes [ 0 ] \n 
result1 = seg . take_slice_of_analogsignalarray_by_channelindex ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind3 ] ) \n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
from __future__ import absolute_import , division \n 
~~ from neo . io import NeuroScopeIO \n 
from neo . test . iotest . common_io_test import BaseTestIO \n 
class TestNeuroScopeIO ( BaseTestIO , unittest . TestCase , ) : \n 
~~~ ioclass = NeuroScopeIO \n 
files_to_test = [ ] \n 
files_to_download = [ , \n 
~~ if __name__ == "__main__" : \n 
~~ import theano \n 
from theano import tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams as RandomStreams \n 
from load import mnist \n 
srng = RandomStreams ( ) \n 
def floatX ( X ) : \n 
~~~ return np . asarray ( X , dtype = theano . config . floatX ) \n 
~~ def init_weights ( shape ) : \n 
~~~ return theano . shared ( floatX ( np . random . randn ( * shape ) * 0.01 ) ) \n 
~~ def rectify ( X ) : \n 
~~~ return T . maximum ( X , 0. ) \n 
~~ def softmax ( X ) : \n 
~~~ e_x = T . exp ( X - X . max ( axis = 1 ) . dimshuffle ( 0 , ) ) \n 
return e_x / e_x . sum ( axis = 1 ) . dimshuffle ( 0 , ) \n 
~~ def RMSprop ( cost , params , lr = 0.001 , rho = 0.9 , epsilon = 1e-6 ) : \n 
~~~ grads = T . grad ( cost = cost , wrt = params ) \n 
updates = [ ] \n 
for p , g in zip ( params , grads ) : \n 
~~~ acc = theano . shared ( p . get_value ( ) * 0. ) \n 
acc_new = rho * acc + ( 1 - rho ) * g ** 2 \n 
gradient_scaling = T . sqrt ( acc_new + epsilon ) \n 
g = g / gradient_scaling \n 
updates . append ( ( acc , acc_new ) ) \n 
updates . append ( ( p , p - lr * g ) ) \n 
~~ return updates \n 
~~ def dropout ( X , p = 0. ) : \n 
~~~ if p > 0 : \n 
~~~ retain_prob = 1 - p \n 
X *= srng . binomial ( X . shape , p = retain_prob , dtype = theano . config . floatX ) \n 
X /= retain_prob \n 
~~ def model ( X , w_h , w_h2 , w_o , p_drop_input , p_drop_hidden ) : \n 
~~~ X = dropout ( X , p_drop_input ) \n 
h = rectify ( T . dot ( X , w_h ) ) \n 
h = dropout ( h , p_drop_hidden ) \n 
h2 = rectify ( T . dot ( h , w_h2 ) ) \n 
h2 = dropout ( h2 , p_drop_hidden ) \n 
py_x = softmax ( T . dot ( h2 , w_o ) ) \n 
return h , h2 , py_x \n 
~~ trX , teX , trY , teY = mnist ( onehot = True ) \n 
X = T . fmatrix ( ) \n 
Y = T . fmatrix ( ) \n 
w_h = init_weights ( ( 784 , 625 ) ) \n 
w_h2 = init_weights ( ( 625 , 625 ) ) \n 
w_o = init_weights ( ( 625 , 10 ) ) \n 
noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , 0.2 , 0.5 ) \n 
h , h2 , py_x = model ( X , w_h , w_h2 , w_o , 0. , 0. ) \n 
y_x = T . argmax ( py_x , axis = 1 ) \n 
cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n 
params = [ w_h , w_h2 , w_o ] \n 
updates = RMSprop ( cost , params , lr = 0.001 ) \n 
train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n 
predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n 
for i in range ( 100 ) : \n 
~~~ for start , end in zip ( range ( 0 , len ( trX ) , 128 ) , range ( 128 , len ( trX ) , 128 ) ) : \n 
~~~ cost = train ( trX [ start : end ] , trY [ start : end ] ) \n 
~~ print np . mean ( np . argmax ( teY , axis = 1 ) == predict ( teX ) ) \n 
from matplotlib import pyplot as plt \n 
import oscaar \n 
import astrometry \n 
import photometry \n 
import dataBank \n 
import systematics \n 
import IO \n 
import pyfits \n 
plt . ion ( ) \n 
data = dataBank . dataBank ( ) \n 
allStars = data . getDict ( ) \n 
outputPath = data . outputPath \n 
N_exposures = len ( data . getPaths ( ) ) \n 
meanDarkFrame = data . getMeanDarkFrame ( ) \n 
masterFlat = data . masterFlat \n 
plottingThings , statusBarFig , statusBarAx = IO . plottingSettings ( data . trackPlots , data . photPlots ) \n 
for expNumber in xrange ( N_exposures ) : \n 
~~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
~~~ plt . cla ( ) \n 
statusBarAx . set_title ( ) \n 
statusBarAx . set_xlim ( [ 0 , 100 ] ) \n 
statusBarAx . set_xlabel ( ) \n 
statusBarAx . get_yaxis ( ) . set_ticks ( [ ] ) \n 
statusBarAx . barh ( [ 0 ] , [ 100.0 * expNumber / len ( data . getPaths ( ) ) ] , \n 
[ 1 ] , color = ) \n 
~~ image = ( pyfits . getdata ( data . getPaths ( ) [ expNumber ] ) - meanDarkFrame ) / masterFlat \n 
data . storeTime ( expNumber ) \n 
for star in allStars : \n 
~~~ est_x , est_y = data . centroidInitialGuess ( expNumber , star ) \n 
x , y , radius , trackFlag = astrometry . trackSmooth ( image , est_x , est_y , \n 
data . smoothConst , \n 
plottingThings , \n 
zoom = data . trackingZoom , \n 
plots = data . trackPlots ) \n 
data . storeCentroid ( star , expNumber , x , y ) \n 
fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n 
data . apertureRadii , \n 
ccdGain = data . ccdGain , \n 
plots = data . photPlots ) \n 
photFlag = any ( photFlags ) \n 
data . storeFluxes ( star , expNumber , fluxes , errors ) \n 
if trackFlag or photFlag and not data . getFlag ( ) : \n 
~~~ data . setFlag ( star , False ) \n 
~~ if data . trackPlots or data . photPlots : \n 
~~~ plt . draw ( ) \n 
~~ ~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
~~ ~~ plt . close ( ) \n 
data . scaleFluxes_multirad ( ) \n 
meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n 
lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n 
meanComparisonStarErrors ) \n 
oscaar . IO . save ( data , outputPath ) \n 
data . plotLightCurve_multirad ( ) \n 
import warnings \n 
from openmdao . components . exec_comp import ExecComp \n 
class ConstraintComp ( ExecComp ) : \n 
def __init__ ( self , expr , out = ) : \n 
~~~ warnings . simplefilter ( , DeprecationWarning ) \n 
DeprecationWarning , stacklevel = 2 ) \n 
warnings . simplefilter ( , DeprecationWarning ) \n 
newexpr = _combined_expr ( expr ) \n 
~~ ~~ def _combined_expr ( expr ) : \n 
lhs , op , rhs = _parse_constraint ( expr ) \n 
first , second = ( rhs , lhs ) if op . startswith ( ) else ( lhs , rhs ) \n 
~~~ if float ( first ) == 0 : \n 
~~~ return "-(%s)" % second \n 
~~ ~~ except Exception : \n 
~~~ if float ( second ) == 0. : \n 
~~~ return first \n 
~~ return % ( first , second ) \n 
~~ def _parse_constraint ( expr_string ) : \n 
for comparator in [ , , , , , ] : \n 
~~~ parts = expr_string . split ( comparator ) \n 
if len ( parts ) == 2 : \n 
~~~ if comparator == : \n 
~~~ break \n 
~~ return ( parts [ 0 ] . strip ( ) , comparator , parts [ 1 ] . strip ( ) ) \n 
raise ValueError ( msg ) \n 
import traceback \n 
from itertools import chain \n 
from six import iteritems , itervalues \n 
from six . moves import cStringIO \n 
import networkx as nx \n 
from openmdao . core . system import System \n 
from openmdao . core . group import Group \n 
from openmdao . core . component import Component \n 
from openmdao . core . parallel_group import ParallelGroup \n 
from openmdao . core . parallel_fd_group import ParallelFDGroup \n 
from openmdao . core . basic_impl import BasicImpl \n 
from openmdao . core . _checks import check_connections , _both_names \n 
from openmdao . core . driver import Driver \n 
from openmdao . core . mpi_wrap import MPI , under_mpirun , debug \n 
from openmdao . core . relevance import Relevance \n 
from openmdao . components . indep_var_comp import IndepVarComp \n 
from openmdao . solvers . scipy_gmres import ScipyGMRES \n 
from openmdao . solvers . ln_direct import DirectSolver \n 
from openmdao . solvers . ln_gauss_seidel import LinearGaussSeidel \n 
from openmdao . units . units import get_conversion_tuple \n 
from openmdao . util . string_util import get_common_ancestor , nearest_child , name_relative_to \n 
from openmdao . util . graph import plain_bfs \n 
from openmdao . util . options import OptionsDictionary \n 
force_check = os . environ . get ( ) \n 
trace = os . environ . get ( ) \n 
class _ProbData ( object ) : \n 
~~~ self . top_lin_gs = False \n 
self . in_complex_step = False \n 
~~ ~~ class Problem ( object ) : \n 
def __init__ ( self , root = None , driver = None , impl = None , comm = None ) : \n 
~~~ super ( Problem , self ) . __init__ ( ) \n 
self . root = root \n 
self . _probdata = _ProbData ( ) \n 
if MPI : \n 
~~~ from openmdao . core . petsc_impl import PetscImpl \n 
if impl != PetscImpl : \n 
~~ ~~ if impl is None : \n 
~~~ self . _impl = BasicImpl \n 
~~~ self . _impl = impl \n 
~~ self . comm = comm \n 
if driver is None : \n 
~~~ self . driver = Driver ( ) \n 
~~~ self . driver = driver \n 
~~ self . pathname = \n 
~~ def __getitem__ ( self , name ) : \n 
if name in self . root . unknowns : \n 
~~~ return self . root . unknowns [ name ] \n 
~~ elif name in self . root . params : \n 
~~~ return self . root . params [ name ] \n 
~~ elif name in self . root . _sysdata . to_abs_pnames : \n 
~~~ for p in self . root . _sysdata . to_abs_pnames [ name ] : \n 
~~~ return self . _rec_get_param ( p ) \n 
~~ ~~ elif name in self . _dangling : \n 
~~~ for p in self . _dangling [ name ] : \n 
~~ ~~ def _rec_get_param ( self , absname ) : \n 
~~~ parts = absname . rsplit ( , 1 ) \n 
if len ( parts ) == 1 : \n 
~~~ return self . root . params [ absname ] \n 
~~~ grp = self . root . _subsystem ( parts [ 0 ] ) \n 
return grp . params [ parts [ 1 ] ] \n 
~~ ~~ def __setitem__ ( self , name , val ) : \n 
~~~ self . root . unknowns [ name ] = val \n 
~~ elif name in self . _dangling : \n 
~~~ parts = p . rsplit ( , 1 ) \n 
~~~ self . root . params [ p ] = val \n 
grp . params [ parts [ 1 ] ] = val \n 
~~ ~~ def _setup_connections ( self , params_dict , unknowns_dict ) : \n 
to_prom_name = self . _probdata . to_prom_name \n 
connections = self . root . _get_explicit_connections ( ) \n 
prom_noconns = self . _add_implicit_connections ( connections ) \n 
input_graph = nx . DiGraph ( ) \n 
self . _dangling = { } \n 
to_abs_pnames = self . root . _sysdata . to_abs_pnames \n 
usrcs = set ( ) \n 
for tgt , srcs in iteritems ( connections ) : \n 
~~~ for src , idxs in srcs : \n 
~~~ input_graph . add_edge ( src , tgt , idxs = idxs ) \n 
if src in unknowns_dict : \n 
~~~ usrcs . add ( src ) \n 
~~ ~~ ~~ for prom , plist in iteritems ( to_abs_pnames ) : \n 
~~~ input_graph . add_nodes_from ( plist ) \n 
if prom in prom_noconns : \n 
~~~ start = plist [ 0 ] \n 
input_graph . add_edges_from ( ( ( start , p ) for p in plist [ 1 : ] ) , \n 
idxs = None ) \n 
~~ ~~ newconns = { } \n 
for src in usrcs : \n 
~~~ newconns [ src ] = None \n 
src_idxs = { src : None } \n 
for s , t in nx . dfs_edges ( input_graph , src ) : \n 
~~~ tidxs = input_graph [ s ] [ t ] [ ] \n 
sidxs = src_idxs [ s ] \n 
if tidxs is None : \n 
~~~ tidxs = sidxs \n 
~~ elif sidxs is not None : \n 
~~~ tidxs = np . array ( sidxs ) [ tidxs ] \n 
~~ src_idxs [ t ] = tidxs \n 
if t in newconns : \n 
~~~ newconns [ t ] . append ( ( src , tidxs ) ) \n 
~~~ newconns [ t ] = [ ( src , tidxs ) ] \n 
~~ ~~ ~~ self . _input_inputs = { } \n 
for node in input_graph . nodes_iter ( ) : \n 
~~~ if node not in newconns and len ( input_graph . pred [ node ] ) == 0 : \n 
~~~ nosrc = [ node ] \n 
for s , t in nx . dfs_edges ( input_graph , node ) : \n 
~~~ src = newconns [ t ] [ 0 ] [ 0 ] \n 
for n in nosrc : \n 
~~~ newconns [ n ] = [ ( src , None ) ] \n 
~~ break \n 
~~~ nosrc . append ( t ) \n 
~~~ set_nosrc = set ( nosrc ) \n 
~~~ self . _dangling [ to_prom_name [ n ] ] = set_nosrc \n 
self . _input_inputs [ n ] = nosrc \n 
~~ ~~ ~~ ~~ connections = OrderedDict ( ) \n 
for tgt , srcs in sorted ( newconns . items ( ) ) : \n 
~~~ if srcs is not None : \n 
~~~ if len ( srcs ) > 1 : \n 
~~~ src_names = ( n for n , idx in srcs ) \n 
( tgt , sorted ( src_names ) ) ) \n 
~~ connections [ tgt ] = srcs [ 0 ] \n 
~~ ~~ return connections \n 
~~ def _check_input_diffs ( self , connections , params_dict , unknowns_dict ) : \n 
for tgt , connected_inputs in iteritems ( self . _input_inputs ) : \n 
~~~ tgt_idx = connected_inputs . index ( tgt ) \n 
units = [ params_dict [ n ] . get ( ) for n in connected_inputs ] \n 
vals = [ params_dict [ n ] [ ] for n in connected_inputs ] \n 
diff_units = [ ] \n 
for i , u in enumerate ( units ) : \n 
~~~ if i != tgt_idx and u != units [ tgt_idx ] : \n 
~~~ if units [ tgt_idx ] is None : \n 
~~~ sname , s = connected_inputs [ i ] , u \n 
tname , t = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
~~~ sname , s = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
tname , t = connected_inputs [ i ] , u \n 
~~ diff_units . append ( ( connected_inputs [ i ] , u ) ) \n 
~~ ~~ if isinstance ( vals [ tgt_idx ] , np . ndarray ) : \n 
~~~ diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if not \n 
( isinstance ( v , np . ndarray ) and \n 
v . shape == vals [ tgt_idx ] . shape and \n 
( v == vals [ tgt_idx ] ) . all ( ) ) ] \n 
~~~ vtype = type ( vals [ tgt_idx ] ) \n 
diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if vtype != type ( v ) or \n 
v != vals [ tgt_idx ] ] \n 
~~ if diff_units : \n 
~~~ filt = set ( [ u for n , u in diff_units ] ) \n 
if None in filt : \n 
~~~ filt . remove ( None ) \n 
~~ if filt : \n 
~~~ proms = set ( [ params_dict [ item ] [ ] for item in connected_inputs ] ) \n 
if len ( proms ) == 1 : \n 
correct_src = params_dict [ connected_inputs [ 0 ] ] [ ] \n 
self . _setup_errors . append ( msg ) \n 
~~ ~~ if diff_vals : \n 
( sorted ( [ ( tgt , params_dict [ tgt ] [ ] ) ] + \n 
diff_vals ) ) ) \n 
~~ ~~ for promname , absnames in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ if len ( absnames ) > 1 : \n 
~~~ step_sizes , step_types , forms = { } , { } , { } \n 
for name in absnames : \n 
~~~ meta = self . root . _params_dict [ name ] \n 
ss = meta . get ( ) \n 
if ss is not None : \n 
~~~ step_sizes [ ss ] = name \n 
~~ st = meta . get ( ) \n 
if st is not None : \n 
~~~ step_types [ st ] = name \n 
~~ f = meta . get ( ) \n 
if f is not None : \n 
~~~ forms [ f ] = name \n 
~~ ~~ if len ( step_sizes ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in step_sizes . items ( ) ] ) ) ) \n 
~~ if len ( step_types ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in step_types . items ( ) ] ) ) ) \n 
~~ if len ( forms ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n 
~~ ~~ ~~ ~~ def _get_ubc_vars ( self , connections ) : \n 
full_order = { s . pathname : i for i , s in \n 
enumerate ( self . root . subsystems ( recurse = True ) ) } \n 
ubcs = [ ] \n 
~~~ tsys = tgt . rsplit ( , 1 ) [ 0 ] \n 
ssys = srcs [ 0 ] . rsplit ( , 1 ) [ 0 ] \n 
if full_order [ ssys ] > full_order [ tsys ] : \n 
~~~ ubcs . append ( tgt ) \n 
~~ ~~ return ubcs \n 
~~ def setup ( self , check = True , out_stream = sys . stdout ) : \n 
self . _setup_errors = [ ] \n 
tree_changed = False \n 
meta_changed = False \n 
if isinstance ( self . root . ln_solver , LinearGaussSeidel ) : \n 
~~~ self . _probdata . top_lin_gs = True \n 
~~ self . driver . root = self . root \n 
self . root . _init_sys_data ( self . pathname , self . _probdata ) \n 
self . _setup_communicators ( ) \n 
params_dict , unknowns_dict = self . root . _setup_variables ( ) \n 
self . _probdata . params_dict = params_dict \n 
self . _probdata . unknowns_dict = unknowns_dict \n 
self . _probdata . to_prom_name = self . root . _sysdata . to_prom_name \n 
connections = self . _setup_connections ( params_dict , unknowns_dict ) \n 
self . _probdata . connections = connections \n 
for tgt , ( src , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ tgt ] \n 
if not in tmeta or not tmeta [ ] : \n 
~~~ if tmeta [ ] == ( ) : \n 
~~~ smeta = unknowns_dict [ src ] \n 
if idxs is not None : \n 
~~~ size = len ( idxs ) \n 
tmeta [ ] = ( size , ) \n 
tmeta [ ] = size \n 
tmeta [ ] = smeta [ ] [ np . array ( idxs ) ] \n 
~~~ tmeta [ ] = smeta [ ] \n 
tmeta [ ] = smeta [ ] \n 
~~ ~~ if idxs is not None : \n 
~~~ if isinstance ( idxs , np . ndarray ) : \n 
~~~ tmeta [ ] = idxs \n 
~~~ tmeta [ ] = np . array ( idxs , \n 
dtype = self . _impl . idx_arr_type ) \n 
~~ ~~ ~~ ~~ if MPI : \n 
~~~ for s in self . root . components ( recurse = True ) : \n 
~~~ if hasattr ( s , ) or ( \n 
hasattr ( s , ) and ( s . setup_distrib \n 
is not Component . setup_distrib ) ) : \n 
~~~ meta_changed = True \n 
~~ ~~ ~~ if tree_changed : \n 
~~~ return self . setup ( check = check , out_stream = out_stream ) \n 
~~ elif meta_changed : \n 
~~~ params_dict , unknowns_dict = self . root . _setup_variables ( compute_indices = True ) \n 
~~ self . _setup_errors . extend ( check_connections ( connections , params_dict , \n 
unknowns_dict , \n 
self . root . _sysdata . to_prom_name ) ) \n 
self . _setup_units ( connections , params_dict , unknowns_dict ) \n 
to_prom_name = self . root . _sysdata . to_prom_name \n 
self . _probdata . to_prom_name = to_prom_name \n 
for path , meta in iteritems ( params_dict ) : \n 
~~~ meta [ ] = to_prom_name [ path ] \n 
if path not in connections : \n 
~~~ if not in meta or not meta [ ] : \n 
~~~ if meta [ ] == ( ) : \n 
~~ ~~ ~~ ~~ for path , meta in iteritems ( unknowns_dict ) : \n 
~~ param_owners = _assign_parameters ( connections ) \n 
pois = self . driver . desvars_of_interest ( ) \n 
oois = self . driver . outputs_of_interest ( ) \n 
self . _driver_vois = set ( ) \n 
for tup in chain ( pois , oois ) : \n 
~~~ self . _driver_vois . update ( tup ) \n 
~~ promoted_unknowns = self . root . _sysdata . to_abs_uname \n 
parallel_p = False \n 
for vnames in pois : \n 
~~~ if len ( vnames ) > 1 : \n 
~~~ parallel_p = True \n 
~~ for v in vnames : \n 
~~~ if v not in promoted_unknowns : \n 
~~ ~~ ~~ parallel_u = False \n 
for vnames in oois : \n 
~~~ parallel_u = True \n 
~~ ~~ ~~ mode = self . _check_for_parallel_derivs ( pois , oois , parallel_u , parallel_p ) \n 
self . _probdata . relevance = Relevance ( self . root , params_dict , \n 
unknowns_dict , connections , \n 
pois , oois , mode ) \n 
for s in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if not s . _order_set : \n 
~~~ order = None \n 
broken_edges = None \n 
if self . comm . rank == 0 : \n 
~~~ order , broken_edges = s . list_auto_order ( ) \n 
~~ if MPI : \n 
~~~ if trace : \n 
~~ order , broken_edges = self . comm . bcast ( ( order , broken_edges ) , root = 0 ) \n 
if trace : \n 
~~ ~~ s . set_order ( order ) \n 
for edge in broken_edges : \n 
~~~ cname = edge [ 1 ] \n 
head_sys = self . root \n 
for name in cname . split ( ) : \n 
~~~ head_sys = getattr ( head_sys , name ) \n 
~~ head_sys . _run_apply = True \n 
~~ ~~ ~~ self . _check_input_diffs ( connections , params_dict , unknowns_dict ) \n 
alloc_derivs = not self . root . fd_options [ ] \n 
for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ alloc_derivs = alloc_derivs or sub . nl_solver . supports [ ] \n 
~~ self . root . _setup_vectors ( param_owners , impl = self . _impl , alloc_derivs = alloc_derivs ) \n 
self . driver . _setup ( ) \n 
self . _poi_indices , self . _qoi_indices = self . driver . _map_voi_indices ( ) \n 
~~~ sub . nl_solver . setup ( sub ) \n 
sub . ln_solver . setup ( sub ) \n 
~~ self . _check_solvers ( ) \n 
self . _start_recorders ( ) \n 
if self . _setup_errors : \n 
~~~ stream = cStringIO ( ) \n 
for err in self . _setup_errors : \n 
~~~ stream . write ( "%s\\n" % err ) \n 
~~ raise RuntimeError ( stream . getvalue ( ) ) \n 
~~ OptionsDictionary . locked = True \n 
if check or force_check : \n 
~~~ return self . check_setup ( out_stream ) \n 
~~ return { } \n 
~~ def cleanup ( self ) : \n 
self . driver . cleanup ( ) \n 
self . root . cleanup ( ) \n 
~~ def _check_solvers ( self ) : \n 
iterated_states = set ( ) \n 
group_states = [ ] \n 
has_iter_solver = { } \n 
for group in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ has_iter_solver [ group . pathname ] = ( group . ln_solver . options [ ] > 1 ) \n 
~~~ if isinstance ( group . ln_solver , DirectSolver ) : \n 
~~~ has_iter_solver [ group . pathname ] = ( True ) \n 
~~ ~~ opt = group . fd_options \n 
if opt [ ] == True and opt [ ] == : \n 
~~~ if group . name != : \n 
~~ for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if hasattr ( sub . nl_solver , ) : \n 
self . _setup_errors . append ( msg . format ( sub . name ) ) \n 
~~ ~~ ~~ parts = group . pathname . split ( ) \n 
for i in range ( len ( parts ) ) : \n 
~~~ if has_iter_solver [ . join ( parts [ : i ] ) ] : \n 
~~~ is_iterated_somewhere = True \n 
~~~ is_iterated_somewhere = False \n 
~~ if is_iterated_somewhere : \n 
~~ if isinstance ( group . ln_solver , LinearGaussSeidel ) and group . ln_solver . options [ ] == 1 : \n 
~~~ graph = group . _get_sys_graph ( ) \n 
strong = [ sorted ( s ) for s in nx . strongly_connected_components ( graph ) \n 
if len ( s ) > 1 ] \n 
if strong : \n 
"recommended)." \n 
% ( group . pathname , strong ) ) \n 
~~ ~~ states = [ n for n , m in iteritems ( group . _unknowns_dict ) if m . get ( ) ] \n 
if states : \n 
~~~ group_states . append ( ( group , states ) ) \n 
if isinstance ( group . ln_solver , DirectSolver ) or group . ln_solver . options [ ] > 1 : \n 
~~~ iterated_states . update ( states ) \n 
~~~ for s in states : \n 
~~~ if s not in iterated_states : \n 
~~~ cname = s . rsplit ( , 1 ) [ 0 ] \n 
comp = self . root \n 
~~~ comp = getattr ( comp , name ) \n 
~~ if not _needs_iteration ( comp ) : \n 
~~~ iterated_states . add ( s ) \n 
~~ ~~ ~~ ~~ ~~ ~~ for group , states in group_states : \n 
~~~ uniterated_states = [ s for s in states if s not in iterated_states ] \n 
if uniterated_states : \n 
( group . pathname , uniterated_states ) ) \n 
~~ ~~ ~~ def _check_dangling_params ( self , out_stream = sys . stdout ) : \n 
dangling_params = sorted ( set ( [ \n 
to_prom_name [ p ] for p , m in iteritems ( self . root . _params_dict ) \n 
if p not in self . root . connections \n 
] ) ) \n 
if dangling_params : \n 
file = out_stream ) \n 
for d in dangling_params : \n 
~~~ print ( d , file = out_stream ) \n 
~~ ~~ return dangling_params \n 
~~ def _check_mode ( self , out_stream = sys . stdout ) : \n 
if self . _calculated_mode != self . root . _probdata . relevance . mode : \n 
self . _calculated_mode , \n 
self . _p_length , \n 
self . _u_length ) , \n 
~~ return ( self . root . _probdata . relevance . mode , self . _calculated_mode ) \n 
~~ def _check_no_unknown_comps ( self , out_stream = sys . stdout ) : \n 
nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n 
local = True ) \n 
if len ( c . unknowns ) == 0 ] ) \n 
if nocomps : \n 
for n in nocomps : \n 
~~~ print ( n , file = out_stream ) \n 
~~ ~~ return nocomps \n 
~~ def _check_no_recorders ( self , out_stream = sys . stdout ) : \n 
recorders = [ ] \n 
recorders . extend ( self . driver . recorders ) \n 
for grp in self . root . subgroups ( recurse = True , local = True , \n 
include_self = True ) : \n 
~~~ recorders . extend ( grp . nl_solver . recorders ) \n 
recorders . extend ( grp . ln_solver . recorders ) \n 
~~ if not recorders : \n 
~~ return recorders \n 
~~ def _check_no_connect_comps ( self , out_stream = sys . stdout ) : \n 
conn_comps = set ( [ t . rsplit ( , 1 ) [ 0 ] \n 
for t in self . root . connections ] ) \n 
conn_comps . update ( [ s . rsplit ( , 1 ) [ 0 ] \n 
for s , i in itervalues ( self . root . connections ) ] ) \n 
noconn_comps = sorted ( [ c . pathname \n 
for c in self . root . components ( recurse = True , local = True ) \n 
if c . pathname not in conn_comps ] ) \n 
if noconn_comps : \n 
for comp in noconn_comps : \n 
~~~ print ( comp , file = out_stream ) \n 
~~ ~~ return noconn_comps \n 
~~ def _check_mpi ( self , out_stream = sys . stdout ) : \n 
if under_mpirun ( ) : \n 
~~~ parr = True \n 
~~~ for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if ( isinstance ( grp , ParallelGroup ) or \n 
isinstance ( grp , ParallelFDGroup ) ) : \n 
~~~ parr = False \n 
~~ mincpu , maxcpu = self . root . get_req_procs ( ) \n 
if maxcpu is not None and self . comm . size > maxcpu : \n 
( self . comm . size , maxcpu ) ) \n 
~~ return ( self . comm . size , maxcpu , parr ) \n 
~~~ pargrps = [ ] \n 
for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if isinstance ( grp , ParallelGroup ) : \n 
grp . pathname , file = out_stream ) \n 
pargrps . append ( grp . pathname ) \n 
~~ ~~ return sorted ( pargrps ) \n 
~~ ~~ def _check_graph ( self , out_stream = sys . stdout ) : \n 
cycles = [ ] \n 
ooo = [ ] \n 
~~~ graph = grp . _get_sys_graph ( ) \n 
strong = [ s for s in nx . strongly_connected_components ( graph ) \n 
~~~ relstrong = [ ] \n 
for slist in strong : \n 
~~~ relstrong . append ( [ ] ) \n 
for s in slist : \n 
~~~ relstrong [ - 1 ] . append ( nearest_child ( grp . pathname , s ) ) \n 
subs = [ s for s in grp . _subsystems ] \n 
tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - 1 ] ] ) \n 
relstrong [ - 1 ] = [ t [ 1 ] for t in tups ] \n 
( grp . pathname , relstrong ) , file = out_stream ) \n 
cycles . append ( relstrong ) \n 
~~ graph , _ = grp . _break_cycles ( grp . list_order ( ) , graph ) \n 
visited = set ( ) \n 
out_of_order = { } \n 
for sub in itervalues ( grp . _subsystems ) : \n 
~~~ visited . add ( sub . pathname ) \n 
for u , v in nx . dfs_edges ( graph , sub . pathname ) : \n 
~~~ if v in visited : \n 
~~~ out_of_order . setdefault ( nearest_child ( grp . pathname , v ) , \n 
set ( ) ) . add ( sub . pathname ) \n 
~~ ~~ ~~ if out_of_order : \n 
~~~ for name in out_of_order : \n 
~~~ out_of_order [ name ] = sorted ( [ \n 
nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n 
] ) \n 
for n , subs in iteritems ( out_of_order ) : \n 
~~ ooo . append ( ( grp . pathname , list ( iteritems ( out_of_order ) ) ) ) \n 
~~ ~~ return ( cycles , sorted ( ooo ) ) \n 
~~ def _check_gmres_under_mpi ( self , out_stream = sys . stdout ) : \n 
~~~ has_parallel = False \n 
~~~ if isinstance ( s , ParallelGroup ) : \n 
~~~ has_parallel = True \n 
~~ ~~ if has_parallel and isinstance ( self . root . ln_solver , ScipyGMRES ) : \n 
~~ ~~ ~~ def _check_ubcs ( self , out_stream = sys . stdout ) : \n 
~~~ ubcs = self . _get_ubc_vars ( self . root . connections ) \n 
if ubcs : \n 
~~ return ubcs \n 
~~ def _check_unmarked_pbos ( self , out_stream = sys . stdout ) : \n 
~~~ pbos = [ ] \n 
for comp in self . root . components ( recurse = True , include_self = True ) : \n 
~~~ if comp . _pbo_warns : \n 
~~~ pbos . append ( ( comp . pathname , comp . _pbo_warns ) ) \n 
~~ ~~ if pbos : \n 
for cname , pbo_warns in sorted ( pbos , key = lambda x : x [ 0 ] ) : \n 
~~~ for vname , val in pbo_warns : \n 
type ( val ) . __name__ ) , file = out_stream ) \n 
~~ ~~ ~~ return pbos \n 
~~ def _check_relevant_pbos ( self , out_stream = sys . stdout ) : \n 
if self . driver . __class__ is Driver or self . driver . supports [ ] is False or self . root . fd_options [ ] : \n 
~~~ return [ ] \n 
~~ vec = self . root . unknowns \n 
pbos = [ var for var in vec if vec . metadata ( var ) . get ( ) ] \n 
rels = set ( ) \n 
for key , rel in iteritems ( self . _probdata . relevance . relevant ) : \n 
~~~ rels . update ( rel ) \n 
~~ rel_pbos = rels . intersection ( pbos ) \n 
if rel_pbos : \n 
~~~ rel_conns = [ ] \n 
for src in rel_pbos : \n 
~~~ for tgt , src_tuple in iteritems ( self . root . connections ) : \n 
~~~ if src_tuple [ 0 ] == src and tgt in rels : \n 
~~~ rel_conns . append ( ( src , tgt ) ) \n 
~~ ~~ ~~ if rel_conns : \n 
for src , tgt in rel_conns : \n 
~~~ val = vec [ src ] \n 
~~ return list ( rel_pbos ) \n 
~~ def check_setup ( self , out_stream = sys . stdout ) : \n 
print ( "##############################################" , file = out_stream ) \n 
results [ ] = self . _check_no_recorders ( out_stream ) \n 
results [ ] = self . _check_mpi ( out_stream ) \n 
results [ ] = self . _check_dangling_params ( out_stream ) \n 
results [ ] = self . _check_mode ( out_stream ) \n 
results [ ] = self . _check_no_unknown_comps ( out_stream ) \n 
results [ ] = self . _check_no_connect_comps ( out_stream ) \n 
results [ ] , results [ ] = self . _check_graph ( out_stream ) \n 
results [ ] = self . _check_ubcs ( out_stream ) \n 
results [ ] = self . _check_gmres_under_mpi ( out_stream ) \n 
results [ ] = self . _check_unmarked_pbos ( out_stream ) \n 
results [ ] = self . _check_relevant_pbos ( out_stream ) \n 
for s in self . root . subsystems ( recurse = True , local = True , include_self = True ) : \n 
s . check_setup ( out_stream = stream ) \n 
content = stream . getvalue ( ) \n 
if content : \n 
~~~ print ( "%s:\\n%s\\n" % ( s . pathname , content ) , file = out_stream ) \n 
results [ "@%s" % s . pathname ] = content \n 
print ( "##############################################\\n" , file = out_stream ) \n 
~~ def pre_run_check ( self ) : \n 
if not self . root . fd_options . locked : \n 
raise RuntimeError ( msg ) \n 
~~ ~~ def run ( self ) : \n 
self . pre_run_check ( ) \n 
if self . root . is_active ( ) : \n 
~~~ self . driver . run ( self ) \n 
self . root . comm . barrier ( ) \n 
~~ ~~ ~~ def run_once ( self ) : \n 
root = self . root \n 
driver = self . driver \n 
if root . is_active ( ) : \n 
~~~ driver . run_once ( self ) \n 
with root . _dircontext : \n 
~~~ root . apply_nonlinear ( root . params , root . unknowns , root . resids , \n 
metadata = driver . metadata ) \n 
root . comm . barrier ( ) \n 
~~ ~~ ~~ def _mode ( self , mode , indep_list , unknown_list ) : \n 
self . _p_length = 0 \n 
self . _u_length = 0 \n 
uset = set ( ) \n 
for unames in unknown_list : \n 
~~~ if isinstance ( unames , tuple ) : \n 
~~~ uset . update ( unames ) \n 
~~~ uset . add ( unames ) \n 
~~ ~~ pset = set ( ) \n 
for pnames in indep_list : \n 
~~~ if isinstance ( pnames , tuple ) : \n 
~~~ pset . update ( pnames ) \n 
~~~ pset . add ( pnames ) \n 
~~ ~~ to_prom_name = self . root . _sysdata . to_prom_name \n 
for path , meta in chain ( iteritems ( self . root . _unknowns_dict ) , \n 
iteritems ( self . root . _params_dict ) ) : \n 
~~~ prom_name = to_prom_name [ path ] \n 
if prom_name in uset : \n 
~~~ self . _u_length += meta [ ] \n 
uset . remove ( prom_name ) \n 
~~ if prom_name in pset : \n 
~~~ self . _p_length += meta [ ] \n 
pset . remove ( prom_name ) \n 
~~ ~~ if uset : \n 
~~ if pset : \n 
~~ if self . _p_length > self . _u_length : \n 
~~~ self . _calculated_mode = \n 
~~ if mode == : \n 
~~~ mode = self . root . ln_solver . options [ ] \n 
if mode == : \n 
~~~ mode = self . _calculated_mode \n 
~~ ~~ return mode \n 
~~ def calc_gradient ( self , indep_list , unknown_list , mode = , \n 
return_format = , dv_scale = None , cn_scale = None , \n 
sparsity = None ) : \n 
if mode not in [ , , , ] : \n 
~~ if return_format not in [ , ] : \n 
~~ with self . root . _dircontext : \n 
~~~ if mode == or self . root . fd_options [ ] : \n 
~~~ return self . _calc_gradient_fd ( indep_list , unknown_list , \n 
return_format , dv_scale = dv_scale , \n 
cn_scale = cn_scale , sparsity = sparsity ) \n 
~~~ return self . _calc_gradient_ln_solver ( indep_list , unknown_list , \n 
return_format , mode , \n 
dv_scale = dv_scale , \n 
cn_scale = cn_scale , \n 
sparsity = sparsity ) \n 
~~ ~~ ~~ def _calc_gradient_fd ( self , indep_list , unknown_list , return_format , \n 
dv_scale = None , cn_scale = None , sparsity = None ) : \n 
unknowns = root . unknowns \n 
params = root . params \n 
to_prom_name = root . _sysdata . to_prom_name \n 
to_abs_pnames = root . _sysdata . to_abs_pnames \n 
to_abs_uname = root . _sysdata . to_abs_uname \n 
if dv_scale is None : \n 
~~~ dv_scale = { } \n 
~~ if cn_scale is None : \n 
~~~ cn_scale = { } \n 
~~ abs_params = [ ] \n 
fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n 
pass_unknowns = [ var for var in unknown_list if var in indep_list ] \n 
for name in indep_list : \n 
~~~ if name in unknowns : \n 
~~~ name = to_abs_uname [ name ] \n 
~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if name == src : \n 
~~~ name = tgt \n 
~~ ~~ abs_params . append ( name ) \n 
~~ Jfd = root . fd_jacobian ( params , unknowns , root . resids , total_derivs = True , \n 
fd_params = abs_params , fd_unknowns = fd_unknowns , \n 
pass_unknowns = pass_unknowns , \n 
poi_indices = self . _poi_indices , \n 
qoi_indices = self . _qoi_indices ) \n 
def get_fd_ikey ( ikey ) : \n 
~~~ if isinstance ( ikey , tuple ) : \n 
~~~ ikey = ikey [ 0 ] \n 
~~ fd_ikey = ikey \n 
if fd_ikey not in params : \n 
~~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if src == ikey : \n 
~~~ fd_ikey = tgt \n 
~~ ~~ if fd_ikey not in params : \n 
~~~ for key , meta in iteritems ( params ) : \n 
~~~ if to_prom_name [ key ] == fd_ikey : \n 
~~~ fd_ikey = meta [ ] \n 
~~ ~~ ~~ ~~ return fd_ikey \n 
~~ if return_format == : \n 
~~~ J = OrderedDict ( ) \n 
for okey in unknown_list : \n 
~~~ J [ okey ] = OrderedDict ( ) \n 
for j , ikey in enumerate ( indep_list ) : \n 
~~~ if sparsity is not None : \n 
~~~ if ikey not in sparsity [ okey ] : \n 
~~ ~~ abs_ikey = abs_params [ j ] \n 
fd_ikey = get_fd_ikey ( abs_ikey ) \n 
if ( okey , fd_ikey ) not in Jfd : \n 
~~~ fd_ikey = to_abs_pnames [ fd_ikey ] [ 0 ] \n 
~~ J [ okey ] [ ikey ] = Jfd [ ( okey , fd_ikey ) ] \n 
if ikey in dv_scale : \n 
~~~ J [ okey ] [ ikey ] *= dv_scale [ ikey ] \n 
~~ if okey in cn_scale : \n 
~~~ J [ okey ] [ ikey ] *= cn_scale [ okey ] \n 
~~ ~~ ~~ ~~ else : \n 
~~~ usize = 0 \n 
psize = 0 \n 
for u in unknown_list : \n 
~~~ if u in self . _qoi_indices : \n 
~~~ idx = self . _qoi_indices [ u ] \n 
usize += len ( idx ) \n 
~~~ usize += self . root . unknowns . metadata ( u ) [ ] \n 
~~ ~~ for p in indep_list : \n 
~~~ if p in self . _poi_indices : \n 
~~~ idx = self . _poi_indices [ p ] \n 
psize += len ( idx ) \n 
~~~ psize += self . root . unknowns . metadata ( p ) [ ] \n 
~~ ~~ J = np . zeros ( ( usize , psize ) ) \n 
ui = 0 \n 
~~~ pi = 0 \n 
for j , p in enumerate ( indep_list ) : \n 
~~~ abs_ikey = abs_params [ j ] \n 
if ( u , fd_ikey ) not in Jfd : \n 
~~ pd = Jfd [ u , fd_ikey ] \n 
rows , cols = pd . shape \n 
for row in range ( 0 , rows ) : \n 
~~~ for col in range ( 0 , cols ) : \n 
~~~ J [ ui + row ] [ pi + col ] = pd [ row ] [ col ] \n 
if p in dv_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= dv_scale [ p ] \n 
~~ if u in cn_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= cn_scale [ u ] \n 
~~ ~~ ~~ pi += cols \n 
~~ ui += rows \n 
~~ ~~ return J \n 
~~ def _calc_gradient_ln_solver ( self , indep_list , unknown_list , return_format , mode , \n 
relevance = root . _probdata . relevance \n 
unknowns_dict = root . _unknowns_dict \n 
comm = root . comm \n 
iproc = comm . rank \n 
nproc = comm . size \n 
owned = root . _owning_ranks \n 
~~ mode = self . _mode ( mode , indep_list , unknown_list ) \n 
fwd = mode == \n 
root . clear_dparams ( ) \n 
for names in root . _probdata . relevance . vars_of_interest ( mode ) : \n 
~~~ for name in names : \n 
~~~ if name in root . dumat : \n 
~~~ root . dumat [ name ] . vec [ : ] = 0.0 \n 
root . drmat [ name ] . vec [ : ] = 0.0 \n 
~~ ~~ ~~ root . dumat [ None ] . vec [ : ] = 0.0 \n 
root . drmat [ None ] . vec [ : ] = 0.0 \n 
root . _sys_linearize ( root . params , unknowns , root . resids ) \n 
if return_format == : \n 
for okeys in unknown_list : \n 
~~~ if isinstance ( okeys , str ) : \n 
~~~ okeys = ( okeys , ) \n 
~~ for okey in okeys : \n 
for ikeys in indep_list : \n 
~~~ if isinstance ( ikeys , str ) : \n 
~~~ ikeys = ( ikeys , ) \n 
~~ for ikey in ikeys : \n 
~~ ~~ J [ okey ] [ ikey ] = None \n 
~~ ~~ ~~ ~~ ~~ else : \n 
Jslices = OrderedDict ( ) \n 
~~~ start = usize \n 
if u in self . _qoi_indices : \n 
~~ Jslices [ u ] = slice ( start , usize ) \n 
~~ for p in indep_list : \n 
~~~ start = psize \n 
if p in self . _poi_indices : \n 
~~~ psize += unknowns . metadata ( p ) [ ] \n 
~~ Jslices [ p ] = slice ( start , psize ) \n 
~~ J = np . zeros ( ( usize , psize ) ) \n 
~~ if fwd : \n 
~~~ input_list , output_list = indep_list , unknown_list \n 
poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = dv_scale , cn_scale \n 
~~~ input_list , output_list = unknown_list , indep_list \n 
qoi_indices , poi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = cn_scale , dv_scale \n 
~~ all_vois = self . root . _probdata . relevance . vars_of_interest ( mode ) \n 
input_set = set ( ) \n 
for inp in input_list : \n 
~~~ if isinstance ( inp , str ) : \n 
~~~ input_set . add ( inp ) \n 
~~~ input_set . update ( inp ) \n 
~~ ~~ voi_sets = [ ] \n 
for voi_set in all_vois : \n 
~~~ for voi in voi_set : \n 
~~~ if voi in input_set : \n 
~~~ voi_sets . append ( voi_set ) \n 
~~ ~~ ~~ flat_voi = [ item for sublist in all_vois for item in sublist ] \n 
for items in input_list : \n 
~~~ if isinstance ( items , str ) : \n 
~~~ items = ( items , ) \n 
~~ for item in items : \n 
~~~ if item not in flat_voi : \n 
~~~ voi_sets . append ( ( item , ) ) \n 
~~ ~~ ~~ voi_srcs = { } \n 
for params in voi_sets : \n 
~~~ rhs = OrderedDict ( ) \n 
voi_idxs = { } \n 
old_size = None \n 
for voi in params : \n 
~~~ vkey = self . _get_voi_key ( voi , params ) \n 
duvec = self . root . dumat [ vkey ] \n 
rhs [ vkey ] = np . empty ( ( len ( duvec . vec ) , ) ) \n 
voi_srcs [ vkey ] = voi \n 
if voi in duvec : \n 
~~~ in_idxs = duvec . _get_local_idxs ( voi , poi_indices ) \n 
~~~ in_idxs = [ ] \n 
~~ if len ( in_idxs ) == 0 : \n 
~~~ if voi in poi_indices : \n 
~~~ in_idxs = duvec . to_idx_array ( poi_indices [ voi ] ) \n 
~~~ in_idxs = np . arange ( 0 , unknowns_dict [ to_abs_uname [ voi ] ] [ ] , dtype = int ) \n 
~~ ~~ if old_size is None : \n 
~~~ old_size = len ( in_idxs ) \n 
~~ elif old_size != len ( in_idxs ) : \n 
~~ voi_idxs [ vkey ] = in_idxs \n 
~~ for i in range ( len ( in_idxs ) ) : \n 
~~~ for voi in params : \n 
rhs [ vkey ] [ : ] = 0.0 \n 
if self . root . _owning_ranks [ voi_srcs [ vkey ] ] == iproc : \n 
~~~ rhs [ vkey ] [ voi_idxs [ vkey ] [ i ] ] = - 1.0 \n 
~~ ~~ dx_mat = root . ln_solver . solve ( rhs , root , mode ) \n 
for param , dx in iteritems ( dx_mat ) : \n 
~~~ vkey = self . _get_voi_key ( param , params ) \n 
if param is None : \n 
~~~ param = params [ 0 ] \n 
~~ for item in output_list : \n 
~~~ if fwd and param not in sparsity [ item ] : \n 
~~ elif not fwd and item not in sparsity [ param ] : \n 
~~ ~~ if relevance . is_relevant ( vkey , item ) : \n 
~~~ if fwd or owned [ item ] == iproc : \n 
~~~ out_idxs = self . root . dumat [ vkey ] . _get_local_idxs ( item , \n 
qoi_indices , \n 
get_slice = True ) \n 
dxval = dx [ out_idxs ] \n 
if dxval . size == 0 : \n 
~~~ dxval = None \n 
~~ if nproc > 1 : \n 
( dxval , owned [ item ] , param , item ) ) \n 
~~ dxval = comm . bcast ( dxval , root = owned [ item ] ) \n 
~~~ if item in qoi_indices : \n 
~~~ zsize = len ( qoi_indices [ item ] ) \n 
~~~ zsize = unknowns . metadata ( item ) [ ] \n 
~~ dxval = np . zeros ( zsize ) \n 
~~ if dxval is not None : \n 
~~~ nk = len ( dxval ) \n 
~~~ if fwd : \n 
~~~ if J [ item ] [ param ] is None : \n 
~~~ J [ item ] [ param ] = np . zeros ( ( nk , len ( in_idxs ) ) ) \n 
~~ J [ item ] [ param ] [ : , i ] = dxval \n 
if param in in_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= in_scale [ param ] \n 
~~ if item in un_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= un_scale [ item ] \n 
~~~ if J [ param ] [ item ] is None : \n 
~~~ J [ param ] [ item ] = np . zeros ( ( len ( in_idxs ) , nk ) ) \n 
~~ J [ param ] [ item ] [ i , : ] = dxval \n 
~~~ J [ param ] [ item ] [ i , : ] *= in_scale [ param ] \n 
~~~ J [ param ] [ item ] [ i , : ] *= un_scale [ item ] \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] = dxval \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= in_scale [ param ] \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= un_scale [ item ] \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] = dxval \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= in_scale [ param ] \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= un_scale [ item ] \n 
~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ root . clear_dparams ( ) \n 
return J \n 
~~ def _get_voi_key ( self , voi , grp ) : \n 
if ( voi in self . _driver_vois and \n 
isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n 
~~~ if ( len ( grp ) > 1 or \n 
self . root . ln_solver . options [ ] ) : \n 
~~~ return voi \n 
~~ ~~ return None \n 
~~ def check_partial_derivatives ( self , out_stream = sys . stdout , comps = None , \n 
compact_print = False ) : \n 
if self . driver . iter_count < 1 : \n 
~~~ out_stream . write ( ) \n 
self . run_once ( ) \n 
~~ root . _sys_linearize ( root . params , root . unknowns , root . resids ) \n 
if out_stream is not None : \n 
~~ data = { } \n 
voi = None \n 
allcomps = root . components ( recurse = True ) \n 
if comps is None : \n 
~~~ comps = allcomps \n 
~~~ allcompnames = set ( [ c . pathname for c in allcomps ] ) \n 
requested = set ( comps ) \n 
diff = requested . difference ( allcompnames ) \n 
if diff : \n 
~~~ sorted_diff = list ( diff ) \n 
sorted_diff . sort ( ) \n 
msg += str ( sorted_diff ) \n 
~~ comps = [ root . _subsystem ( c_name ) for c_name in comps ] \n 
~~ for comp in comps : \n 
~~~ cname = comp . pathname \n 
opt = comp . fd_options \n 
fwd_rev = True \n 
if opt [ ] : \n 
~~~ f_d_2 = True \n 
fd_desc = opt [ ] \n 
fd_desc2 = opt [ ] \n 
~~~ f_d_2 = False \n 
fd_desc = None \n 
fd_desc2 = None \n 
~~ if opt [ ] : \n 
~~~ if not f_d_2 : \n 
~~ fwd_rev = False \n 
~~ if isinstance ( comp , IndepVarComp ) : \n 
~~ data [ cname ] = { } \n 
jac_fwd = OrderedDict ( ) \n 
jac_rev = OrderedDict ( ) \n 
jac_fd = OrderedDict ( ) \n 
jac_fd2 = OrderedDict ( ) \n 
params = comp . params \n 
unknowns = comp . unknowns \n 
resids = comp . resids \n 
dparams = comp . dpmat [ voi ] \n 
dunknowns = comp . dumat [ voi ] \n 
dresids = comp . drmat [ voi ] \n 
states = comp . states \n 
if len ( dparams ) == 0 : \n 
~~ param_list = [ item for item in dparams if not dparams . metadata ( item ) . get ( ) ] \n 
param_list . extend ( states ) \n 
unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( ) ] \n 
~~~ out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
~~ for p_name in param_list : \n 
~~~ if not fwd_rev : \n 
~~ dinputs = dunknowns if p_name in states else dparams \n 
p_size = np . size ( dinputs [ p_name ] ) \n 
for u_name in unkn_list : \n 
~~~ u_size = np . size ( dunknowns [ u_name ] ) \n 
if comp . _jacobian_cache : \n 
~~~ if ( u_name , p_name ) in comp . _jacobian_cache : \n 
~~~ user = comp . _jacobian_cache [ ( u_name , p_name ) ] . shape \n 
if len ( user ) < 2 : \n 
~~~ user = ( user [ 0 ] , 1 ) \n 
~~ if user [ 0 ] != u_size or user [ 1 ] != p_size : \n 
msg = msg . format ( cname , u_name , p_name , ( u_size , p_size ) , user ) \n 
~~ ~~ ~~ jac_fwd [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
jac_rev [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
~~ ~~ if fwd_rev : \n 
~~~ for u_name in unkn_list : \n 
for idx in range ( u_size ) : \n 
~~~ dresids . vec [ : ] = 0.0 \n 
dunknowns . vec [ : ] = 0.0 \n 
dresids . _dat [ u_name ] . val [ idx ] = 1.0 \n 
~~~ comp . apply_linear ( params , unknowns , dparams , \n 
dunknowns , dresids , ) \n 
~~~ dparams . _apply_unit_derivatives ( ) \n 
~~~ dinputs = dunknowns if p_name in states else dparams \n 
jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n 
~~ ~~ ~~ ~~ if fwd_rev : \n 
~~~ for p_name in param_list : \n 
for idx in range ( p_size ) : \n 
dinputs . _dat [ p_name ] . val [ idx ] = 1.0 \n 
dparams . _apply_unit_derivatives ( ) \n 
comp . apply_linear ( params , unknowns , dparams , \n 
for u_name , u_val in dresids . vec_val_iter ( ) : \n 
~~~ jac_fwd [ ( u_name , p_name ) ] [ : , idx ] = u_val \n 
~~ ~~ ~~ ~~ dresids . vec [ : ] = 0.0 \n 
if opt [ ] == : \n 
~~~ fd_func = comp . complex_step_jacobian \n 
~~~ fd_func = comp . fd_jacobian \n 
~~ jac_fd = fd_func ( params , unknowns , resids ) \n 
if f_d_2 : \n 
~~ save_form = opt [ ] \n 
OptionsDictionary . locked = False \n 
opt [ ] = opt [ ] \n 
jac_fd2 = fd_func ( params , unknowns , resids ) \n 
opt [ ] = save_form \n 
OptionsDictionary . locked = True \n 
~~ _assemble_deriv_data ( chain ( dparams , states ) , resids , data [ cname ] , \n 
jac_fwd , jac_rev , jac_fd , out_stream , \n 
c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n 
fd_desc2 = fd_desc2 , compact_print = compact_print ) \n 
~~ return data \n 
~~ def check_total_derivatives ( self , out_stream = sys . stdout ) : \n 
if driver . iter_count < 1 : \n 
~~ if out_stream is not None : \n 
~~ if len ( driver . _desvars ) > 0 : \n 
~~~ param_srcs = list ( driver . _desvars . keys ( ) ) \n 
to_abs_name = root . _sysdata . to_abs_uname \n 
indep_list = [ p for p in param_srcs if not root . _unknowns_dict [ to_abs_name [ p ] ] . get ( ) ] \n 
~~~ abs_indep_list = root . _get_fd_params ( ) \n 
param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( ) ] \n 
indep_list = [ \n 
to_prom_name [ p ] for p , idxs in param_srcs \n 
~~ if len ( driver . _objs ) > 0 or len ( driver . _cons ) > 0 : \n 
~~~ unknown_list = list ( driver . _objs . keys ( ) ) \n 
unknown_list . extend ( list ( driver . _cons . keys ( ) ) ) \n 
unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n 
~~~ unknown_list = root . _get_fd_unknowns ( ) \n 
~~ if root . ln_solver . options . get ( ) : \n 
~~~ mode = self . _mode ( , indep_list , unknown_list ) \n 
~~~ fwd , rev = True , False \n 
Jrev = None \n 
out_stream . write ( ) \n 
~~~ fwd , rev = False , True \n 
Jfor = None \n 
~~~ fwd = rev = True \n 
~~~ Jfor = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
return_format = ) \n 
Jfor = _jac_to_flat_dict ( Jfor ) \n 
~~ if rev : \n 
~~~ Jrev = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
Jrev = _jac_to_flat_dict ( Jrev ) \n 
~~ Jfd = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
Jfd = _jac_to_flat_dict ( Jfd ) \n 
data = { } \n 
_assemble_deriv_data ( indep_list , unknown_list , data , \n 
Jfor , Jrev , Jfd , out_stream ) \n 
return data \n 
~~ def _start_recorders ( self ) : \n 
self . driver . recorders . startup ( self . root ) \n 
self . driver . recorders . record_metadata ( self . root ) \n 
~~~ for solver in ( group . nl_solver , group . ln_solver ) : \n 
~~~ solver . recorders . startup ( group ) \n 
solver . recorders . record_metadata ( self . root ) \n 
~~ ~~ ~~ def _check_for_parallel_derivs ( self , params , unknowns , par_u , par_p ) : \n 
mode = self . _mode ( , params , unknowns ) \n 
~~~ has_parallel_derivs = par_p \n 
~~~ has_parallel_derivs = par_u \n 
~~ if ( isinstance ( self . root . ln_solver , LinearGaussSeidel ) and \n 
self . root . ln_solver . options [ ] ) and has_parallel_derivs : \n 
~~~ for sub in self . root . subgroups ( recurse = True ) : \n 
~~~ sub_mode = sub . ln_solver . options [ ] \n 
if isinstance ( sub . ln_solver , LinearGaussSeidel ) and sub_mode not in ( mode , ) : \n 
msg = msg . format ( name = sub . name , submode = sub_mode , rootmode = mode ) \n 
~~ ~~ ~~ return mode \n 
~~ def _json_system_tree ( self ) : \n 
def _tree_dict ( system ) : \n 
~~~ dct = OrderedDict ( ) \n 
for s in system . subsystems ( recurse = True ) : \n 
~~~ if isinstance ( s , Group ) : \n 
~~~ dct [ s . name ] = _tree_dict ( s ) \n 
~~~ dct [ s . name ] = OrderedDict ( ) \n 
for vname , meta in iteritems ( s . unknowns ) : \n 
~~~ dct [ s . name ] [ vname ] = m = meta . copy ( ) \n 
for mname in m : \n 
~~~ if isinstance ( m [ mname ] , np . ndarray ) : \n 
~~~ m [ mname ] = m [ mname ] . tolist ( ) \n 
~~ ~~ ~~ ~~ ~~ return dct \n 
~~ tree = OrderedDict ( ) \n 
tree [ ] = _tree_dict ( self . root ) \n 
return json . dumps ( tree ) \n 
~~ def _setup_communicators ( self ) : \n 
~~~ if self . comm is None : \n 
~~~ self . comm = self . _impl . world_comm ( ) \n 
~~ minproc , maxproc = self . driver . get_req_procs ( ) \n 
~~~ if not ( maxproc is None or maxproc >= self . comm . size ) : \n 
( self . comm . size , minproc , maxproc ) ) \n 
~~ elif self . comm . size < minproc : \n 
~~~ if maxproc is None : \n 
~~~ maxproc = \n 
~~ ~~ self . driver . _setup_communicators ( self . comm , os . getcwd ( ) ) \n 
~~ def _setup_units ( self , connections , params_dict , unknowns_dict ) : \n 
for target , ( source , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ target ] \n 
smeta = unknowns_dict [ source ] \n 
if not in tmeta or not in smeta : \n 
~~ src_unit = smeta [ ] \n 
tgt_unit = tmeta [ ] \n 
~~~ scale , offset = get_conversion_tuple ( src_unit , tgt_unit ) \n 
~~ except TypeError as err : \n 
_both_names ( smeta , to_prom_name ) , \n 
tgt_unit , \n 
_both_names ( tmeta , to_prom_name ) ) \n 
~~~ raise \n 
~~ ~~ if scale != 1.0 or offset != 0.0 : \n 
~~~ tmeta [ ] = ( scale , offset ) \n 
~~ ~~ ~~ def _add_implicit_connections ( self , connections ) : \n 
dangling = set ( ) \n 
abs_unames = self . root . _sysdata . to_abs_uname \n 
for prom_name , pabs_list in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ for pabs in pabs_list : \n 
~~~ connections . setdefault ( pabs , [ ] ) . append ( ( abs_unames [ prom_name ] , None ) ) \n 
~~~ dangling . add ( prom_name ) \n 
~~ ~~ return dangling \n 
~~ def print_all_convergence ( self ) : \n 
root . ln_solver . print_all_convergence ( ) \n 
root . nl_solver . print_all_convergence ( ) \n 
for grp in root . subgroups ( recurse = True ) : \n 
~~~ grp . ln_solver . print_all_convergence ( ) \n 
grp . nl_solver . print_all_convergence ( ) \n 
~~ ~~ ~~ def _assign_parameters ( connections ) : \n 
param_owners = { } \n 
for par , ( unk , idxs ) in iteritems ( connections ) : \n 
~~~ param_owners . setdefault ( get_common_ancestor ( par , unk ) , set ( ) ) . add ( par ) \n 
~~ return param_owners \n 
~~ def _jac_to_flat_dict ( jac ) : \n 
new_jac = OrderedDict ( ) \n 
for key1 , val1 in iteritems ( jac ) : \n 
~~~ for key2 , val2 in iteritems ( val1 ) : \n 
~~~ new_jac [ ( key1 , key2 ) ] = val2 \n 
~~ ~~ return new_jac \n 
~~ def _pad_name ( name , pad_num = 13 , quotes = True ) : \n 
l_name = len ( name ) \n 
if l_name < pad_num : \n 
~~~ pad = pad_num - l_name \n 
if quotes : \n 
~~~ pad_str = "\'{name}\'{sep:<{pad}}" \n 
~~~ pad_str = "{name}{sep:<{pad}}" \n 
~~ pad_name = pad_str . format ( name = name , sep = , pad = pad ) \n 
return pad_name \n 
~~~ return . format ( name ) \n 
~~ ~~ def _assemble_deriv_data ( params , resids , cdata , jac_fwd , jac_rev , jac_fd , \n 
out_stream , c_name = , jac_fd2 = None , fd_desc = None , \n 
fd_desc2 = None , compact_print = False ) : \n 
started = False \n 
for p_name in params : \n 
~~~ for u_name in resids : \n 
~~~ key = ( u_name , p_name ) \n 
if key not in jac_fd : \n 
~~ ldata = cdata [ key ] = { } \n 
Jsub_fd = jac_fd [ key ] \n 
ldata [ ] = Jsub_fd \n 
magfd = np . linalg . norm ( Jsub_fd ) \n 
if jac_fwd : \n 
~~~ Jsub_for = jac_fwd [ key ] \n 
ldata [ ] = Jsub_for \n 
magfor = np . linalg . norm ( Jsub_for ) \n 
~~~ magfor = None \n 
~~ if jac_rev : \n 
~~~ Jsub_rev = jac_rev [ key ] \n 
ldata [ ] = Jsub_rev \n 
magrev = np . linalg . norm ( Jsub_rev ) \n 
~~~ magrev = None \n 
~~ if jac_fd2 : \n 
~~~ Jsub_fd2 = jac_fd2 [ key ] \n 
ldata [ ] = Jsub_fd2 \n 
magfd2 = np . linalg . norm ( Jsub_fd2 ) \n 
~~~ magfd2 = None \n 
~~ ldata [ ] = ( magfor , magrev , magfd ) \n 
~~~ abs1 = np . linalg . norm ( Jsub_for - Jsub_fd ) \n 
~~~ abs1 = None \n 
~~~ abs2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) \n 
~~~ abs2 = None \n 
~~ if jac_fwd and jac_rev : \n 
~~~ abs3 = np . linalg . norm ( Jsub_for - Jsub_rev ) \n 
~~~ abs3 = None \n 
~~~ abs4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) \n 
~~~ abs4 = None \n 
~~ ldata [ ] = ( abs1 , abs2 , abs3 ) \n 
if magfd == 0.0 : \n 
~~~ rel1 = rel2 = rel3 = rel4 = float ( ) \n 
~~~ if jac_fwd : \n 
~~~ rel1 = np . linalg . norm ( Jsub_for - Jsub_fd ) / magfd \n 
~~~ rel1 = None \n 
~~~ rel2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) / magfd \n 
~~~ rel2 = None \n 
~~~ rel3 = np . linalg . norm ( Jsub_for - Jsub_rev ) / magfd \n 
~~~ rel3 = None \n 
~~~ rel4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) / magfd \n 
~~~ rel4 = None \n 
~~ ~~ ldata [ ] = ( rel1 , rel2 , rel3 ) \n 
if out_stream is None : \n 
~~ if compact_print : \n 
~~~ if jac_fwd and jac_rev : \n 
~~~ if not started : \n 
out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) \n 
out_stream . write ( out_str ) \n 
out_stream . write ( * len ( out_str ) + ) \n 
started = True \n 
out_stream . write ( tmp1 . format ( _pad_name ( u_name ) , _pad_name ( p_name ) , \n 
magfor , magrev , magfd , abs1 , abs2 , \n 
rel1 , rel2 ) ) \n 
~~ elif jac_fd and jac_fd2 : \n 
_pad_name ( , 13 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) \n 
magfd , magfd2 , abs4 , rel4 ) ) \n 
~~~ if started : \n 
~~~ out_stream . write ( * 30 + ) \n 
~~~ started = True \n 
~~~ out_stream . write ( % magfor ) \n 
~~~ out_stream . write ( % magrev ) \n 
~~ if not jac_fwd and not jac_rev : \n 
~~ if jac_fd : \n 
~~~ out_stream . write ( % magfd ) \n 
if fd_desc : \n 
~~~ out_stream . write ( % fd_desc ) \n 
~~ out_stream . write ( ) \n 
~~~ out_stream . write ( % magfd2 ) \n 
if fd_desc2 : \n 
~~~ out_stream . write ( % fd_desc2 ) \n 
~~~ out_stream . write ( % abs1 ) \n 
~~~ out_stream . write ( % abs2 ) \n 
~~~ out_stream . write ( % abs3 ) \n 
~~~ out_stream . write ( % abs4 ) \n 
~~~ out_stream . write ( % rel1 ) \n 
~~~ out_stream . write ( % rel2 ) \n 
~~~ out_stream . write ( % rel3 ) \n 
~~~ out_stream . write ( % rel4 ) \n 
out_stream . write ( str ( Jsub_for ) ) \n 
out_stream . write ( str ( Jsub_rev ) ) \n 
out_stream . write ( str ( Jsub_fd ) ) \n 
if jac_fd2 : \n 
out_stream . write ( str ( Jsub_fd2 ) ) \n 
~~ ~~ ~~ ~~ ~~ def _needs_iteration ( comp ) : \n 
if isinstance ( comp , Component ) and comp . is_active ( ) and comp . states : \n 
~~~ for klass in comp . __class__ . __mro__ : \n 
~~~ if klass is Component : \n 
~~ if in klass . __dict__ : \n 
~~~ return False \n 
~~ ~~ return True \n 
~~ return False \n 
~~ def _get_gmres_name ( ) : \n 
~~~ if MPI : \n 
~~~ return \n 
from sqlitedict import SqliteDict \n 
from openmdao . recorders . base_recorder import BaseRecorder \n 
from openmdao . util . record_util import format_iteration_coordinate \n 
from openmdao . core . mpi_wrap import MPI \n 
class SqliteRecorder ( BaseRecorder ) : \n 
def __init__ ( self , out , ** sqlite_dict_args ) : \n 
~~~ super ( SqliteRecorder , self ) . __init__ ( ) \n 
if MPI and MPI . COMM_WORLD . rank > 0 : \n 
~~~ self . _open_close_sqlitedict = False \n 
~~~ self . _open_close_sqlitedict = True \n 
~~ if self . _open_close_sqlitedict : \n 
~~~ sqlite_dict_args . setdefault ( , True ) \n 
sqlite_dict_args . setdefault ( , ) \n 
self . out = SqliteDict ( filename = out , flag = , ** sqlite_dict_args ) \n 
~~~ self . out = None \n 
~~ ~~ def record_metadata ( self , group ) : \n 
params = group . params . iteritems ( ) \n 
resids = group . resids . iteritems ( ) \n 
unknowns = group . unknowns . iteritems ( ) \n 
data = OrderedDict ( [ ( , dict ( params ) ) , \n 
( , dict ( unknowns ) ) , \n 
self . out [ ] = data \n 
~~ def record_iteration ( self , params , unknowns , resids , metadata ) : \n 
data = OrderedDict ( ) \n 
iteration_coordinate = metadata [ ] \n 
timestamp = metadata [ ] \n 
group_name = format_iteration_coordinate ( iteration_coordinate ) \n 
data [ ] = timestamp \n 
data [ ] = metadata [ ] \n 
if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( params , , iteration_coordinate ) \n 
~~ if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( unknowns , , iteration_coordinate ) \n 
~~~ data [ ] = self . _filter_vector ( resids , , iteration_coordinate ) \n 
~~ self . out [ group_name ] = data \n 
~~ def record_derivatives ( self , derivs , metadata ) : \n 
group_name = % group_name \n 
data [ ] = derivs \n 
self . out [ group_name ] = data \n 
~~ def close ( self ) : \n 
if self . _open_close_sqlitedict : \n 
~~~ if self . out is not None : \n 
~~~ self . out . close ( ) \n 
self . out = None \n 
from numpy import atleast_2d as array2d \n 
from scipy import linalg \n 
from scipy . optimize import minimize \n 
from scipy . spatial . distance import squareform \n 
from openmdao . surrogate_models . surrogate_model import MultiFiSurrogateModel \n 
_logger = logging . getLogger ( ) \n 
THETA0_DEFAULT = 0.5 \n 
THETAL_DEFAULT = 1e-5 \n 
THETAU_DEFAULT = 50 \n 
if hasattr ( linalg , ) : \n 
~~~ solve_triangular = linalg . solve_triangular \n 
~~~ def solve_triangular ( x , y , lower = True ) : \n 
~~~ return linalg . solve ( x , y ) \n 
~~ ~~ def constant_regression ( x ) : \n 
x = np . asarray ( x , dtype = np . float ) \n 
n_eval = x . shape [ 0 ] \n 
f = np . ones ( [ n_eval , 1 ] ) \n 
~~ def linear_regression ( x ) : \n 
f = np . hstack ( [ np . ones ( [ n_eval , 1 ] ) , x ] ) \n 
~~ def squared_exponential_correlation ( theta , d ) : \n 
theta = np . asarray ( theta , dtype = np . float ) \n 
d = np . asarray ( d , dtype = np . float ) \n 
if d . ndim > 1 : \n 
~~~ n_features = d . shape [ 1 ] \n 
~~~ n_features = 1 \n 
~~ if theta . size == 1 : \n 
~~~ return np . exp ( - theta [ 0 ] * np . sum ( d ** 2 , axis = 1 ) ) \n 
~~ elif theta . size != n_features : \n 
~~~ return np . exp ( - np . sum ( theta . reshape ( 1 , n_features ) * d ** 2 , axis = 1 ) ) \n 
~~ ~~ def l1_cross_distances ( X , Y = None ) : \n 
if Y is None : \n 
~~~ X = array2d ( X ) \n 
n_samples , n_features = X . shape \n 
n_nonzero_cross_dist = n_samples * ( n_samples - 1 ) // 2 \n 
D = np . zeros ( ( n_nonzero_cross_dist , n_features ) ) \n 
ll_1 = 0 \n 
for k in range ( n_samples - 1 ) : \n 
~~~ ll_0 = ll_1 \n 
ll_1 = ll_0 + n_samples - k - 1 \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - X [ ( k + 1 ) : ] ) \n 
~~ return D \n 
Y = array2d ( Y ) \n 
n_samples_X , n_features_X = X . shape \n 
n_samples_Y , n_features_Y = Y . shape \n 
if n_features_X != n_features_Y : \n 
~~ n_features = n_features_X \n 
n_nonzero_cross_dist = n_samples_X * n_samples_Y \n 
for k in range ( n_samples_X ) : \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - Y ) \n 
~~ ~~ class MultiFiCoKriging ( object ) : \n 
_regression_types = { \n 
: constant_regression , \n 
: linear_regression } \n 
def __init__ ( self , regr = , rho_regr = , \n 
theta = None , theta0 = None , thetaL = None , thetaU = None ) : \n 
~~~ self . corr = squared_exponential_correlation \n 
self . regr = regr \n 
self . rho_regr = rho_regr \n 
self . theta = theta \n 
self . theta0 = theta0 \n 
self . thetaL = thetaL \n 
self . thetaU = thetaU \n 
self . _nfev = 0 \n 
~~ def _build_R ( self , lvl , theta ) : \n 
D = self . D [ lvl ] \n 
n_samples = self . n_samples [ lvl ] \n 
R = np . eye ( n_samples ) * ( 1. + NUGGET ) \n 
corr = squareform ( self . corr ( theta , D ) ) \n 
R = R + corr \n 
return R \n 
~~ def fit ( self , X , y , \n 
initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n 
self . _check_list_structure ( X , y ) \n 
self . _check_params ( ) \n 
X = self . X \n 
y = self . y \n 
nlevel = self . nlevel \n 
n_samples = self . n_samples \n 
self . beta = nlevel * [ 0 ] \n 
self . beta_rho = nlevel * [ None ] \n 
self . beta_regr = nlevel * [ None ] \n 
self . C = nlevel * [ 0 ] \n 
self . D = nlevel * [ 0 ] \n 
self . F = nlevel * [ 0 ] \n 
self . p = nlevel * [ 0 ] \n 
self . q = nlevel * [ 0 ] \n 
self . G = nlevel * [ 0 ] \n 
self . sigma2 = nlevel * [ 0 ] \n 
self . _R_adj = nlevel * [ None ] \n 
y_best = y [ nlevel - 1 ] \n 
for i in range ( nlevel - 1 ) [ : : - 1 ] : \n 
~~~ y_best = np . concatenate ( ( y [ i ] [ : - n_samples [ i + 1 ] ] , y_best ) ) \n 
~~ self . y_best = y_best \n 
self . y_mean = np . zeros ( 1 ) \n 
self . y_std = np . ones ( 1 ) \n 
self . X_mean = np . zeros ( 1 ) \n 
self . X_std = np . ones ( 1 ) \n 
for lvl in range ( nlevel ) : \n 
~~~ self . D [ lvl ] = l1_cross_distances ( X [ lvl ] ) \n 
if ( np . min ( np . sum ( self . D [ lvl ] , axis = 1 ) ) == 0. ) : \n 
~~ self . F [ lvl ] = self . regr ( X [ lvl ] ) \n 
self . p [ lvl ] = self . F [ lvl ] . shape [ 1 ] \n 
if lvl > 0 : \n 
~~~ F_rho = self . rho_regr ( X [ lvl ] ) \n 
self . q [ lvl ] = F_rho . shape [ 1 ] \n 
self . F [ lvl ] = np . hstack ( ( F_rho * np . dot ( ( self . y [ lvl - 1 ] ) [ - n_samples [ lvl ] : ] , \n 
np . ones ( ( 1 , self . q [ lvl ] ) ) ) , self . F [ lvl ] ) ) \n 
~~~ self . q [ lvl ] = 0 \n 
~~ n_samples_F_i = self . F [ lvl ] . shape [ 0 ] \n 
if n_samples_F_i != n_samples [ lvl ] : \n 
~~ if int ( self . p [ lvl ] + self . q [ lvl ] ) >= n_samples_F_i : \n 
% ( n_samples [ i ] , self . p [ lvl ] + self . q [ lvl ] ) ) \n 
~~ ~~ self . X = X \n 
self . y = y \n 
self . rlf_value = np . zeros ( nlevel ) \n 
~~~ if self . theta [ lvl ] is None : \n 
~~~ sol = self . _max_rlf ( lvl = lvl , initial_range = initial_range , tol = tol ) \n 
self . theta [ lvl ] = sol [ ] \n 
self . rlf_value [ lvl ] = sol [ ] \n 
if np . isinf ( self . rlf_value [ lvl ] ) : \n 
~~~ self . rlf_value [ lvl ] = self . rlf ( lvl = lvl ) \n 
~~ ~~ ~~ return \n 
~~ def rlf ( self , lvl , theta = None ) : \n 
if theta is None : \n 
~~~ theta = self . theta [ lvl ] \n 
~~ rlf_value = 1e20 \n 
y = self . y [ lvl ] \n 
F = self . F [ lvl ] \n 
p = self . p [ lvl ] \n 
q = self . q [ lvl ] \n 
R = self . _build_R ( lvl , theta ) \n 
~~~ C = linalg . cholesky ( R , lower = True ) \n 
~~ except linalg . LinAlgError : \n 
~~~ _logger . warning ( ( % lvl ) + \n 
+ str ( theta ) ) \n 
return rlf_value \n 
~~ Ft = solve_triangular ( C , F , lower = True ) \n 
Yt = solve_triangular ( C , y , lower = True ) \n 
~~~ Q , G = linalg . qr ( Ft , econ = True ) \n 
~~~ Q , G = linalg . qr ( Ft , mode = ) \n 
pass \n 
~~ beta = solve_triangular ( G , np . dot ( Q . T , Yt ) ) \n 
err = Yt - np . dot ( Ft , beta ) \n 
err2 = np . dot ( err . T , err ) [ 0 , 0 ] \n 
self . _err = err \n 
sigma2 = err2 / ( n_samples - p - q ) \n 
detR = ( ( np . diag ( C ) ) ** ( 2. / n_samples ) ) . prod ( ) \n 
rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n 
self . beta_rho [ lvl ] = beta [ : q ] \n 
self . beta_regr [ lvl ] = beta [ q : ] \n 
self . beta [ lvl ] = beta \n 
self . sigma2 [ lvl ] = sigma2 \n 
self . C [ lvl ] = C \n 
self . G [ lvl ] = G \n 
~~ def _max_rlf ( self , lvl , initial_range , tol ) : \n 
thetaL = self . thetaL [ lvl ] \n 
thetaU = self . thetaU [ lvl ] \n 
def rlf_transform ( x ) : \n 
~~~ return self . rlf ( theta = 10. ** x , lvl = lvl ) \n 
~~ theta0 = self . theta0 [ lvl ] \n 
x0 = np . log10 ( theta0 [ 0 ] ) \n 
constraints = [ ] \n 
for i in range ( theta0 . size ) : \n 
~~~ constraints . append ( { : , : lambda log10t , i = i : \n 
log10t [ i ] - np . log10 ( thetaL [ 0 ] [ i ] ) } ) \n 
constraints . append ( { : , : lambda log10t , i = i : \n 
np . log10 ( thetaU [ 0 ] [ i ] ) - log10t [ i ] } ) \n 
~~ constraints = tuple ( constraints ) \n 
sol = minimize ( rlf_transform , x0 , method = , \n 
constraints = constraints , \n 
options = { : initial_range , \n 
: tol , : 0 } ) \n 
log10_optimal_x = sol [ ] \n 
optimal_rlf_value = sol [ ] \n 
self . _nfev += sol [ ] \n 
optimal_theta = 10. ** log10_optimal_x \n 
res = { } \n 
res [ ] = optimal_theta \n 
res [ ] = optimal_rlf_value \n 
return res \n 
~~ def predict ( self , X , eval_MSE = True ) : \n 
X = array2d ( X ) \n 
n_eval , n_features_X = X . shape \n 
mu = np . zeros ( ( n_eval , nlevel ) ) \n 
f = self . regr ( X ) \n 
f0 = self . regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ 0 ] ) \n 
F = self . F [ 0 ] \n 
C = self . C [ 0 ] \n 
beta = self . beta [ 0 ] \n 
Ft = solve_triangular ( C , F , lower = True ) \n 
yt = solve_triangular ( C , self . y [ 0 ] , lower = True ) \n 
r_ = self . corr ( self . theta [ 0 ] , dx ) . reshape ( n_eval , self . n_samples [ 0 ] ) \n 
gamma = solve_triangular ( C . T , yt - np . dot ( Ft , beta ) , lower = False ) \n 
mu [ : , 0 ] = ( np . dot ( f , beta ) + np . dot ( r_ , gamma ) ) . ravel ( ) \n 
if eval_MSE : \n 
~~~ self . sigma2_rho = nlevel * [ None ] \n 
MSE = np . zeros ( ( n_eval , nlevel ) ) \n 
r_t = solve_triangular ( C , r_ . T , lower = True ) \n 
G = self . G [ 0 ] \n 
u_ = solve_triangular ( G . T , f . T - np . dot ( Ft . T , r_t ) , lower = True ) \n 
MSE [ : , 0 ] = self . sigma2 [ 0 ] * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) + ( u_ ** 2 ) . sum ( axis = 0 ) ) \n 
~~ for i in range ( 1 , nlevel ) : \n 
~~~ C = self . C [ i ] \n 
F = self . F [ i ] \n 
g = self . rho_regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n 
r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n 
f = np . vstack ( ( g . T * mu [ : , i - 1 ] , f0 . T ) ) \n 
yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n 
G = self . G [ i ] \n 
beta = self . beta [ i ] \n 
mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n 
~~~ Q_ = ( np . dot ( ( yt - np . dot ( Ft , beta ) ) . T , yt - np . dot ( Ft , beta ) ) ) [ 0 , 0 ] \n 
u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n 
sigma2_rho = np . dot ( g , self . sigma2 [ i ] * linalg . inv ( np . dot ( G . T , G ) ) [ : self . q [ i ] , : self . q [ i ] ] + np . dot ( beta [ : self . q [ i ] ] , beta [ : self . q [ i ] ] . T ) ) \n 
sigma2_rho = ( sigma2_rho * g ) . sum ( axis = 1 ) \n 
MSE [ : , i ] = sigma2_rho * MSE [ : , i - 1 ] + Q_ / ( 2 * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) ) + self . sigma2 [ i ] * ( u_ ** 2 ) . sum ( axis = 0 ) \n 
~~~ mu [ : , i ] = self . y_mean + self . y_std * mu [ : , i ] \n 
~~~ MSE [ : , i ] = self . y_std ** 2 * MSE [ : , i ] \n 
~~ ~~ if eval_MSE : \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) , MSE [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
~~ ~~ def _check_list_structure ( self , X , y ) : \n 
~~~ if type ( X ) is not list : \n 
~~~ nlevel = 1 \n 
X = [ X ] \n 
~~~ nlevel = len ( X ) \n 
~~ if type ( y ) is not list : \n 
~~~ y = [ y ] \n 
~~ if len ( X ) != len ( y ) : \n 
~~ n_samples = np . zeros ( nlevel , dtype = int ) \n 
n_features = np . zeros ( nlevel , dtype = int ) \n 
n_samples_y = np . zeros ( nlevel , dtype = int ) \n 
for i in range ( nlevel ) : \n 
~~~ n_samples [ i ] , n_features [ i ] = X [ i ] . shape \n 
if i > 1 and n_features [ i ] != n_features [ i - 1 ] : \n 
~~ y [ i ] = np . asarray ( y [ i ] ) . ravel ( ) [ : , np . newaxis ] \n 
n_samples_y [ i ] = y [ i ] . shape [ 0 ] \n 
if n_samples [ i ] != n_samples_y [ i ] : \n 
~~ ~~ self . n_features = n_features [ 0 ] \n 
if type ( self . theta ) is not list : \n 
~~~ self . theta = nlevel * [ self . theta ] \n 
~~ elif len ( self . theta ) != nlevel : \n 
~~ if type ( self . theta0 ) is not list : \n 
~~~ self . theta0 = nlevel * [ self . theta0 ] \n 
~~ elif len ( self . theta0 ) != nlevel : \n 
~~ if type ( self . thetaL ) is not list : \n 
~~~ self . thetaL = nlevel * [ self . thetaL ] \n 
~~ elif len ( self . thetaL ) != nlevel : \n 
~~ if type ( self . thetaU ) is not list : \n 
~~~ self . thetaU = nlevel * [ self . thetaU ] \n 
~~ elif len ( self . thetaU ) != nlevel : \n 
~~ self . nlevel = nlevel \n 
self . X = X [ : ] \n 
self . y = y [ : ] \n 
self . n_samples = n_samples \n 
~~ def _check_params ( self ) : \n 
~~~ if not callable ( self . regr ) : \n 
~~~ if self . regr in self . _regression_types : \n 
~~~ self . regr = self . _regression_types [ self . regr ] \n 
% ( self . _regression_types . keys ( ) , self . regr ) ) \n 
~~ ~~ if not callable ( self . rho_regr ) : \n 
~~~ if self . rho_regr in self . _regression_types : \n 
~~~ self . rho_regr = self . _regression_types [ self . rho_regr ] \n 
% ( self . _regression_types . keys ( ) , self . rho_regr ) ) \n 
~~ ~~ for i in range ( self . nlevel ) : \n 
~~~ if self . theta [ i ] is not None : \n 
~~~ self . theta [ i ] = array2d ( self . theta [ i ] ) \n 
if np . any ( self . theta [ i ] <= 0 ) : \n 
~~ ~~ if self . theta0 [ i ] is not None : \n 
~~~ self . theta0 [ i ] = array2d ( self . theta0 [ i ] ) \n 
if np . any ( self . theta0 [ i ] <= 0 ) : \n 
~~~ self . theta0 [ i ] = array2d ( self . n_features * [ THETA0_DEFAULT ] ) \n 
~~ lth = self . theta0 [ i ] . size \n 
if self . thetaL [ i ] is not None : \n 
~~~ self . thetaL [ i ] = array2d ( self . thetaL [ i ] ) \n 
if self . thetaL [ i ] . size != lth : \n 
~~~ self . thetaL [ i ] = array2d ( self . n_features * [ THETAL_DEFAULT ] ) \n 
~~ if self . thetaU [ i ] is not None : \n 
~~~ self . thetaU [ i ] = array2d ( self . thetaU [ i ] ) \n 
if self . thetaU [ i ] . size != lth : \n 
~~~ self . thetaU [ i ] = array2d ( self . n_features * [ THETAU_DEFAULT ] ) \n 
~~ if np . any ( self . thetaL [ i ] <= 0 ) or np . any ( self . thetaU [ i ] < self . thetaL [ i ] ) : \n 
"thetaU." ) \n 
~~ ~~ return \n 
~~ ~~ class MultiFiCoKrigingSurrogate ( MultiFiSurrogateModel ) : \n 
theta = None , theta0 = None , thetaL = None , thetaU = None , \n 
tolerance = TOLERANCE_DEFAULT , initial_range = INITIAL_RANGE_DEFAULT ) : \n 
~~~ super ( MultiFiCoKrigingSurrogate , self ) . __init__ ( ) \n 
self . tolerance = tolerance \n 
self . initial_range = initial_range \n 
self . model = MultiFiCoKriging ( regr = regr , rho_regr = rho_regr , theta = theta , \n 
theta0 = theta0 , thetaL = thetaL , thetaU = thetaU ) \n 
~~ def predict ( self , new_x ) : \n 
Y_pred , MSE = self . model . predict ( [ new_x ] ) \n 
return Y_pred , np . sqrt ( np . abs ( MSE ) ) \n 
~~ def train_multifi ( self , X , Y ) : \n 
X , Y = self . _fit_adapter ( X , Y ) \n 
self . model . fit ( X , Y , tol = self . tolerance , initial_range = self . initial_range ) \n 
~~ def _fit_adapter ( self , X , Y ) : \n 
~~~ if len ( np . shape ( np . array ( X [ 0 ] ) ) ) == 1 : \n 
~~~ X = [ X ] \n 
Y = [ Y ] \n 
~~ X = [ np . array ( x ) for x in reversed ( X ) ] \n 
Y = [ np . array ( y ) for y in reversed ( Y ) ] \n 
return ( X , Y ) \n 
~~ ~~ class FloatMultiFiCoKrigingSurrogate ( MultiFiCoKrigingSurrogate ) : \n 
def predict ( self , new_x ) : \n 
~~~ dist = super ( FloatMultiFiCoKrigingSurrogate , self ) . predict ( new_x ) \n 
return dist . mu \n 
~~~ import doctest \n 
doctest . testmod ( ) \n 
from six . moves import range \n 
from pyparsing import CaselessLiteral , Combine , OneOrMore , Optional , TokenConverter , Word , nums , oneOf , printables , ParserElement , alphanums \n 
__all__ = [ , ] \n 
def _getformat ( val ) : \n 
~~~ if int ( val ) == val : \n 
~~~ return "%.1f" \n 
~~~ return "%.16g" \n 
~~ ~~ class _SubHelper ( object ) : \n 
~~~ self . newtext = "" \n 
self . replace_location = 0 \n 
self . current_location = 0 \n 
self . counter = 0 \n 
self . start_location = 0 \n 
self . end_location = 0 \n 
~~ def set ( self , newtext , location ) : \n 
self . newtext = newtext \n 
self . replace_location = location \n 
~~ def set_array ( self , newtext , start_location , end_location ) : \n 
self . start_location = start_location \n 
self . end_location = end_location \n 
~~ def replace ( self , text ) : \n 
self . current_location += 1 \n 
if self . current_location == self . replace_location : \n 
~~~ if isinstance ( self . newtext , float ) : \n 
~~~ return _getformat ( self . newtext ) % self . newtext \n 
~~~ return str ( self . newtext ) \n 
~~~ return text . group ( ) \n 
~~ ~~ def replace_array ( self , text ) : \n 
end = len ( self . newtext ) \n 
if self . current_location >= self . start_location and self . current_location <= self . end_location and self . counter < end : \n 
~~~ if isinstance ( self . newtext [ self . counter ] , float ) : \n 
~~~ val = self . newtext [ self . counter ] \n 
newval = _getformat ( val ) % val \n 
~~~ newval = str ( self . newtext [ self . counter ] ) \n 
~~ self . counter += 1 \n 
return newval \n 
~~ ~~ ~~ class ToInteger ( TokenConverter ) : \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
return int ( tokenlist [ 0 ] ) \n 
~~ ~~ class ToFloat ( TokenConverter ) : \n 
return float ( tokenlist [ 0 ] . replace ( , ) ) \n 
~~ ~~ class ToNan ( TokenConverter ) : \n 
return float ( ) \n 
~~ ~~ class ToInf ( TokenConverter ) : \n 
~~ ~~ class InputFileGenerator ( object ) : \n 
~~~ self . template_filename = [ ] \n 
self . output_filename = [ ] \n 
self . reg = re . compile ( ) \n 
self . data = [ ] \n 
self . current_row = 0 \n 
self . anchored = False \n 
~~ def set_template_file ( self , filename ) : \n 
self . template_filename = filename \n 
templatefile = open ( filename , ) \n 
self . data = templatefile . readlines ( ) \n 
templatefile . close ( ) \n 
~~ def set_generated_file ( self , filename ) : \n 
self . output_filename = filename \n 
~~ def set_delimiters ( self , delimiter ) : \n 
self . delimiter = delimiter \n 
self . reg = re . compile ( + delimiter + ) \n 
~~ def mark_anchor ( self , anchor , occurrence = 1 ) : \n 
if not isinstance ( occurrence , int ) : \n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ count = 0 \n 
max_lines = len ( self . data ) \n 
for index in range ( self . current_row , max_lines ) : \n 
~~~ line = self . data [ index ] \n 
if count == 0 and self . anchored : \n 
~~~ line = line . split ( anchor ) [ - 1 ] \n 
~~ if line . find ( anchor ) > - 1 : \n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ self . current_row += count \n 
self . anchored = True \n 
~~ ~~ count += 1 \n 
~~ ~~ elif occurrence < 0 : \n 
~~~ max_lines = len ( self . data ) - 1 \n 
count = max_lines \n 
for index in range ( max_lines , - 1 , - 1 ) : \n 
if count == max_lines and self . anchored : \n 
~~~ line = line . split ( anchor ) [ 0 ] \n 
~~~ instance += - 1 \n 
~~~ self . current_row = count \n 
~~ ~~ count -= 1 \n 
~~ def reset_anchor ( self ) : \n 
~~ def transfer_var ( self , value , row , field ) : \n 
j = self . current_row + row \n 
line = self . data [ j ] \n 
sub = _SubHelper ( ) \n 
sub . set ( value , field ) \n 
newline = re . sub ( self . reg , sub . replace , line ) \n 
self . data [ j ] = newline \n 
~~ def transfer_array ( self , value , row_start , field_start , field_end , \n 
if row_end is None : \n 
~~~ row_end = row_start \n 
~~ sub = _SubHelper ( ) \n 
for row in range ( row_start , row_end + 1 ) : \n 
~~~ j = self . current_row + row \n 
if row == row_end : \n 
~~~ f_end = field_end \n 
~~~ f_end = 99999 \n 
~~ sub . set_array ( value , field_start , f_end ) \n 
field_start = 0 \n 
newline = re . sub ( self . reg , sub . replace_array , line ) \n 
~~ if sub . counter < len ( value ) : \n 
~~~ for val in value [ sub . counter : ] : \n 
~~~ newline = newline . rstrip ( ) + sep + str ( val ) \n 
~~ self . data [ j ] = newline \n 
~~ elif sub . counter > len ( value ) : \n 
~~ self . data [ j ] += "\\n" \n 
~~ def transfer_2Darray ( self , value , row_start , row_end , field_start , \n 
field_end ) : \n 
sub . set_array ( value [ i , : ] , field_start , field_end ) \n 
sub . current_location = 0 \n 
sub . counter = 0 \n 
~~ ~~ def clearline ( self , row ) : \n 
self . data [ self . current_row + row ] = "\\n" \n 
~~ def generate ( self ) : \n 
infile = open ( self . output_filename , ) \n 
infile . writelines ( self . data ) \n 
infile . close ( ) \n 
~~ ~~ class FileParser ( object ) : \n 
def __init__ ( self , end_of_line_comment_char = None , full_line_comment_char = None ) : \n 
~~~ self . filename = [ ] \n 
self . end_of_line_comment_char = end_of_line_comment_char \n 
self . full_line_comment_char = full_line_comment_char \n 
self . set_delimiters ( self . delimiter ) \n 
~~ def set_file ( self , filename ) : \n 
inputfile = open ( filename , ) \n 
if not self . end_of_line_comment_char and not self . full_line_comment_char : \n 
~~~ self . data = inputfile . readlines ( ) \n 
~~~ self . data = [ ] \n 
for line in inputfile : \n 
~~~ if line [ 0 ] == self . full_line_comment_char : \n 
~~ self . data . append ( line . split ( self . end_of_line_comment_char ) [ 0 ] ) \n 
~~ ~~ inputfile . close ( ) \n 
if delimiter != "columns" : \n 
~~~ ParserElement . setDefaultWhitespaceChars ( str ( delimiter ) ) \n 
~~ self . _reset_tokens ( ) \n 
~~ if anchor in line : \n 
~~ def transfer_line ( self , row ) : \n 
return self . data [ self . current_row + row ] . rstrip ( ) \n 
~~ def transfer_var ( self , row , field , fieldend = None ) : \n 
if self . delimiter == "columns" : \n 
~~~ if not fieldend : \n 
~~~ line = line [ ( field - 1 ) : ] \n 
~~~ line = line [ ( field - 1 ) : ( fieldend ) ] \n 
~~ data = self . _parse_line ( ) . parseString ( line ) \n 
if len ( data ) > 1 : \n 
~~~ return line \n 
~~~ return data [ 0 ] \n 
~~~ data = self . _parse_line ( ) . parseString ( line ) \n 
return data [ field - 1 ] \n 
~~ ~~ def transfer_keyvar ( self , key , field , occurrence = 1 , rowoffset = 0 ) : \n 
if not isinstance ( occurrence , int ) or occurrence == 0 : \n 
~~~ row = 0 \n 
for line in self . data [ self . current_row : ] : \n 
~~~ if line . find ( key ) > - 1 : \n 
~~ ~~ row += 1 \n 
~~~ row = - 1 \n 
for line in reversed ( self . data [ self . current_row : ] ) : \n 
~~ ~~ row -= 1 \n 
~~ ~~ j = self . current_row + row + rowoffset \n 
fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n 
return fields [ field ] \n 
~~ def transfer_array ( self , rowstart , fieldstart , rowend = None , fieldend = None ) : \n 
j1 = self . current_row + rowstart \n 
if rowend is None : \n 
~~~ j2 = j1 + 1 \n 
~~~ j2 = self . current_row + rowend + 1 \n 
~~ if not fieldend : \n 
~~ lines = self . data [ j1 : j2 ] \n 
data = np . zeros ( shape = ( 0 , 0 ) ) \n 
for i , line in enumerate ( lines ) : \n 
~~~ if self . delimiter == "columns" : \n 
~~~ line = line [ ( fieldstart - 1 ) : fieldend ] \n 
line = line . strip ( ) \n 
parsed = self . _parse_line ( ) . parseString ( line ) \n 
newdata = np . array ( parsed [ : ] ) \n 
if newdata . dtype . type is np . str_ : \n 
~~~ newdata = np . array ( line ) \n 
~~ data = np . append ( data , newdata ) \n 
~~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
if i == j2 - j1 - 1 : \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) ) \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : ] ) ) \n 
~~ fieldstart = 1 \n 
~~ ~~ return data \n 
~~ def transfer_2Darray ( self , rowstart , fieldstart , rowend , fieldend = None ) : \n 
if fieldend and ( fieldstart > fieldend ) : \n 
~~ if rowstart > rowend : \n 
~~ j1 = self . current_row + rowstart \n 
j2 = self . current_row + rowend + 1 \n 
lines = list ( self . data [ j1 : j2 ] ) \n 
~~~ if fieldend : \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : fieldend ] \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : ] \n 
~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
row = np . array ( parsed [ : ] ) \n 
data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
data [ 0 , : ] = row \n 
for i , line in enumerate ( list ( lines [ 1 : ] ) ) : \n 
~~~ line = line [ ( fieldstart - 1 ) : ] \n 
data [ i + 1 , : ] = np . array ( parsed [ : ] ) \n 
~~~ parsed = self . _parse_line ( ) . parseString ( lines [ 0 ] ) \n 
if fieldend : \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ print ( data ) \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ ~~ ~~ return data \n 
~~ def _parse_line ( self ) : \n 
return self . line_parse_token \n 
~~ def _reset_tokens ( self ) : \n 
if self . delimiter . isspace ( ) : \n 
~~~ textchars = printables \n 
~~~ textchars = alphanums \n 
symbols = [ , , , , , , , , , , \n 
, , , , , , , , , , \n 
, , , , , , ] \n 
for symbol in symbols : \n 
~~~ if symbol not in self . delimiter : \n 
~~~ textchars = textchars + symbol \n 
~~ ~~ ~~ digits = Word ( nums ) \n 
dot = "." \n 
ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n 
num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n 
num_float = ToFloat ( Combine ( Optional ( sign ) + \n 
( ( digits + dot + Optional ( digits ) ) | \n 
( dot + digits ) ) + \n 
Optional ( ee + Optional ( sign ) + digits ) \n 
) ) \n 
mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n 
string_text = Word ( textchars ) \n 
self . line_parse_token = ( OneOrMore ( ( nan | num_float | mixed_exp | num_int | \n 
string_text ) ) ) \n 
from OpenPNM . Geometry import models as gm \n 
from OpenPNM . Geometry import GenericGeometry \n 
class SGL10 ( GenericGeometry ) : \n 
def __init__ ( self , ** kwargs ) : \n 
~~~ super ( ) . __init__ ( ** kwargs ) \n 
self . _generate ( ) \n 
~~ def _generate ( self ) : \n 
~~~ self . models . add ( propname = , \n 
model = gm . pore_misc . random , \n 
num_range = [ 0 , 0.8834 ] , \n 
regen_mode = ) \n 
self . models . add ( propname = , \n 
model = gm . throat_misc . neighbor , \n 
pore_prop = , \n 
mode = ) \n 
model = gm . pore_diameter . sphere , \n 
psd_name = , \n 
psd_shape = 3.07 , \n 
psd_loc = 1.97e-6 , \n 
psd_scale = 1.6e-5 , \n 
psd_offset = 18e-6 ) \n 
model = gm . pore_area . spherical ) \n 
model = gm . pore_volume . sphere ) \n 
model = gm . throat_diameter . cylinder , \n 
tsd_name = , \n 
tsd_shape = 3.07 , \n 
tsd_loc = 1.97e-6 , \n 
tsd_scale = 1.6e-5 , \n 
tsd_offset = 18e-6 ) \n 
model = gm . throat_length . straight ) \n 
model = gm . throat_volume . cylinder ) \n 
model = gm . throat_area . cylinder ) \n 
model = gm . throat_surface_area . cylinder ) \n 
import scipy as _sp \n 
def pore_to_pore ( geometry , ** kwargs ) : \n 
network = geometry . _net \n 
throats = network . throats ( geometry . name ) \n 
pores = network . find_connected_pores ( throats , flatten = False ) \n 
C0 = network [ ] [ pores , 0 ] \n 
C1 = network [ ] [ pores , 1 ] \n 
V = C1 - C0 \n 
L = _sp . array ( _sp . sqrt ( _sp . sum ( V [ : , : ] ** 2 , axis = 1 ) ) , ndmin = 1 ) \n 
value = V / _sp . array ( L , ndmin = 2 ) . T \n 
return value \n 
def standard ( phase , \n 
pore_MW = , \n 
pore_density = , \n 
** kwargs ) : \n 
MW = phase [ pore_MW ] \n 
rho = phase [ pore_density ] \n 
value = rho / MW \n 
~~ def ideal_gas ( phase , \n 
pore_pressure = , \n 
pore_temperature = , \n 
R = 8.31447 \n 
P = phase [ pore_pressure ] \n 
T = phase [ pore_temperature ] \n 
value = P / ( R * T ) \n 
~~ def vanderwaals ( phase , \n 
pore_P = , \n 
pore_T = , \n 
pore_Pc = , \n 
pore_Tc = , \n 
P = phase [ pore_P ] / 100000 \n 
T = phase [ pore_T ] \n 
Tc = phase [ pore_Tc ] \n 
R = 83.1447 \n 
a = 27 * ( R ** 2 ) * ( Tc ** 2 ) / ( 64 * Pc ) \n 
b = R * Tc / ( 8 * Pc ) \n 
a1 = - 1 / b \n 
a2 = ( R * T + b * P ) / ( a * b ) \n 
a3 = - P / ( a * b ) \n 
a0 = sp . ones ( sp . shape ( a1 ) ) \n 
coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n 
density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n 
~~ import os \n 
from distutils . util import convert_path \n 
~~~ from setuptools import setup \n 
~~~ from distutils . core import setup \n 
~~ sys . path . append ( os . getcwd ( ) ) \n 
main_ = { } \n 
ver_path = convert_path ( ) \n 
with open ( ver_path ) as f : \n 
~~~ for line in f : \n 
~~~ if line . startswith ( ) : \n 
~~~ exec ( line , main_ ) \n 
~~ ~~ ~~ setup ( \n 
name = , \n 
description = version = main_ [ ] , \n 
classifiers = [ \n 
] , \n 
packages = [ \n 
install_requires = [ \n 
author = , \n 
author_email = , \n 
download_url = , \n 
url = \n 
class PoreCentroidTest : \n 
~~~ def test_voronoi ( self ) : \n 
~~ ~~ import pytest \n 
import OpenPNM \n 
class GenericPhaseTest : \n 
~~~ def setup_class ( self ) : \n 
~~~ self . net = OpenPNM . Network . Cubic ( shape = [ 5 , 5 , 5 ] ) \n 
~~ def test_init_w_no_network ( self ) : \n 
~~~ OpenPNM . Phases . GenericPhase ( ) \n 
~~ def test_init_w_components ( self ) : \n 
~~~ comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
OpenPNM . Phases . GenericPhase ( network = self . net , \n 
components = [ comp1 , comp2 ] ) \n 
~~ def test_set_component_add ( self ) : \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase . set_component ( comp1 ) \n 
phase . set_component ( comp2 ) \n 
~~ def test_set_component_add_twice ( self ) : \n 
with pytest . raises ( Exception ) : \n 
~~~ phase . set_components ( comp1 ) \n 
~~ ~~ def test_set_component_remove ( self ) : \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net , \n 
phase . set_component ( comp1 , mode = ) \n 
phase . set_component ( comp2 , mode = ) \n 
~~ def test_set_component_remove_twice ( self ) : \n 
~~~ phase . set_component ( comp1 , mode = ) \n 
~~ ~~ ~~ from __future__ import print_function \n 
from time import time \n 
MROW = 1000 * 1000. \n 
COLDCACHE = 5 \n 
WARMCACHE = 5 \n 
rdm_cod = [ , ] \n 
def get_nrows ( nrows_str ) : \n 
~~~ if nrows_str . endswith ( "k" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 ) \n 
~~ elif nrows_str . endswith ( "m" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 ) \n 
~~ elif nrows_str . endswith ( "g" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 * 1000 ) \n 
~~~ raise ValueError ( \n 
~~ ~~ class DB ( object ) : \n 
~~~ def __init__ ( self , nrows , rng , userandom ) : \n 
~~~ global step , scale \n 
self . step = STEP \n 
self . scale = SCALE \n 
self . rng = rng \n 
self . userandom = userandom \n 
self . filename = . join ( [ rdm_cod [ userandom ] , nrows ] ) \n 
self . nrows = get_nrows ( nrows ) \n 
~~ def get_db_size ( self ) : \n 
stdout = subprocess . PIPE ) . stdout \n 
line = [ l for l in sout ] [ 0 ] \n 
return int ( line . split ( ) [ 0 ] ) \n 
~~ def print_mtime ( self , t1 , explain ) : \n 
~~~ mtime = time ( ) - t1 \n 
print ( "%s:" % explain , round ( mtime , 6 ) ) \n 
print ( "Krows/s:" , round ( ( self . nrows / 1000. ) / mtime , 6 ) ) \n 
~~ def print_qtime ( self , colname , ltimes ) : \n 
print ( "Mrows/s:" , round ( ( self . nrows / ( MROW ) ) / qtime1 , 6 ) ) \n 
~~ def norm_times ( self , ltimes ) : \n 
lmean = ltimes . mean ( ) \n 
lstd = ltimes . std ( ) \n 
ntimes = ltimes [ ltimes < lmean + lstd ] \n 
nmean = ntimes . mean ( ) \n 
nstd = ntimes . std ( ) \n 
return nmean , nstd \n 
~~ def print_qtime_idx ( self , colname , ltimes , repeated , verbose ) : \n 
~~~ if repeated : \n 
~~ ltimes = numpy . array ( ltimes ) \n 
ntimes = len ( ltimes ) \n 
ctimes = ltimes [ 1 : COLDCACHE ] \n 
cmean , cstd = self . norm_times ( ctimes ) \n 
wtimes = ltimes [ WARMCACHE : ] \n 
wmean , wstd = self . norm_times ( wtimes ) \n 
numpy . histogram ( wtimes ) ) \n 
round ( qtime1 , prec ) ) \n 
round ( cmean , prec ) , "+-" , round ( cstd , prec ) ) \n 
round ( wmean , prec ) , "+-" , round ( wstd , prec ) ) \n 
~~ def print_db_sizes ( self , init , filled , indexed ) : \n 
~~~ table_size = ( filled - init ) / 1024. \n 
indexes_size = ( indexed - filled ) / 1024. \n 
~~ def fill_arrays ( self , start , stop ) : \n 
~~~ arr_f8 = numpy . arange ( start , stop , dtype = ) \n 
arr_i4 = numpy . arange ( start , stop , dtype = ) \n 
if self . userandom : \n 
~~~ arr_f8 += numpy . random . normal ( 0 , stop * self . scale , \n 
size = stop - start ) \n 
arr_i4 = numpy . array ( arr_f8 , dtype = ) \n 
~~ return arr_i4 , arr_f8 \n 
~~ def create_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ self . con = self . open_db ( remove = 1 ) \n 
self . create_table ( self . con ) \n 
init_size = self . get_db_size ( ) \n 
t1 = time ( ) \n 
self . fill_table ( self . con ) \n 
table_size = self . get_db_size ( ) \n 
self . print_mtime ( t1 , ) \n 
self . index_db ( dtype , kind , optlevel , verbose ) \n 
indexes_size = self . get_db_size ( ) \n 
self . print_db_sizes ( init_size , table_size , indexes_size ) \n 
self . close_db ( self . con ) \n 
~~ def index_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ if dtype == "int" : \n 
~~~ idx_cols = [ ] \n 
~~ elif dtype == "float" : \n 
~~~ idx_cols = [ , ] \n 
~~ for colname in idx_cols : \n 
~~~ t1 = time ( ) \n 
self . index_col ( self . con , colname , kind , optlevel , verbose ) \n 
self . print_mtime ( t1 , % colname ) \n 
~~ ~~ def query_db ( self , niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) : \n 
~~~ self . con = self . open_db ( ) \n 
if dtype == "int" : \n 
~~~ reg_cols = [ ] \n 
idx_cols = [ ] \n 
~~~ reg_cols = [ , ] \n 
idx_cols = [ , ] \n 
~~ if avoidfscache : \n 
~~~ rseed = int ( numpy . random . randint ( self . nrows ) ) \n 
~~~ rseed = 19 \n 
~~ numpy . random . seed ( rseed ) \n 
base = numpy . random . randint ( self . nrows ) \n 
if not onlyidxquery : \n 
~~~ for colname in reg_cols : \n 
~~~ ltimes = [ ] \n 
random . seed ( rseed ) \n 
for i in range ( NI_NTIMES ) : \n 
results = self . do_query ( self . con , colname , base , inkernel ) \n 
ltimes . append ( time ( ) - t1 ) \n 
~~ if verbose : \n 
~~ self . print_qtime ( colname , ltimes ) \n 
~~ self . close_db ( self . con ) \n 
self . con = self . open_db ( ) \n 
~~ if not onlynonidxquery : \n 
~~~ for colname in idx_cols : \n 
numpy . random . seed ( rseed ) \n 
rndbase = numpy . random . randint ( self . nrows , size = niter ) \n 
for i in range ( niter ) : \n 
~~~ base = rndbase [ i ] \n 
~~ self . print_qtime_idx ( colname , ltimes , False , verbose ) \n 
ltimes = [ ] \n 
~~ ~~ self . close_db ( self . con ) \n 
~~ def close_db ( self , con ) : \n 
~~~ con . close ( ) \n 
~~~ import sys \n 
import getopt \n 
~~~ import psyco \n 
psyco_imported = 1 \n 
~~~ psyco_imported = 0 \n 
~~~ opts , pargs = getopt . getopt ( \n 
sys . argv [ 1 : ] , ) \n 
~~~ sys . stderr . write ( usage ) \n 
~~ usepytables = 0 \n 
usepostgres = 0 \n 
verbose = 0 \n 
doprofile = 0 \n 
dokprofile = 0 \n 
usepsyco = 0 \n 
userandom = 0 \n 
docreate = 0 \n 
optlevel = 0 \n 
kind = "medium" \n 
docompress = 0 \n 
complib = "zlib" \n 
doquery = False \n 
onlyidxquery = False \n 
onlynonidxquery = False \n 
inkernel = True \n 
avoidfscache = 0 \n 
rng = [ - 1000 , - 1000 ] \n 
repeatquery = 0 \n 
repeatvalue = 0 \n 
krows = \n 
niter = READ_TIMES \n 
dtype = "all" \n 
datadir = "data.nobackup" \n 
for option in opts : \n 
~~~ if option [ 0 ] == : \n 
~~~ usepytables = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ usepostgres = 1 \n 
~~~ verbose = 1 \n 
~~~ doprofile = 1 \n 
~~~ dokprofile = 1 \n 
~~~ usepsyco = 1 \n 
~~~ userandom = 1 \n 
~~~ docreate = 1 \n 
~~~ doquery = True \n 
onlyidxquery = True \n 
onlynonidxquery = True \n 
inkernel = False \n 
~~~ avoidfscache = 1 \n 
~~~ docompress = int ( option [ 1 ] ) \n 
~~~ complib = option [ 1 ] \n 
~~~ rng = [ int ( i ) for i in option [ 1 ] . split ( "," ) ] \n 
~~~ niter = int ( option [ 1 ] ) \n 
~~~ krows = option [ 1 ] \n 
~~~ datadir = option [ 1 ] \n 
~~~ optlevel = int ( option [ 1 ] ) \n 
~~~ if option [ 1 ] in ( , , , ) : \n 
~~~ kind = option [ 1 ] \n 
"\'ultralight\'" ) \n 
~~ ~~ elif option [ 0 ] == : \n 
~~~ if option [ 1 ] in ( , ) : \n 
~~~ dtype = option [ 1 ] \n 
~~~ repeatquery = 1 \n 
repeatvalue = int ( option [ 1 ] ) \n 
~~ ~~ if not usepytables and not usepostgres : \n 
~~ if usepytables : \n 
~~~ from pytables_backend import PyTables_DB \n 
db = PyTables_DB ( krows , rng , userandom , datadir , \n 
docompress , complib , kind , optlevel ) \n 
~~ elif usepostgres : \n 
~~~ from postgres_backend import Postgres_DB \n 
db = Postgres_DB ( krows , rng , userandom ) \n 
~~ if not avoidfscache : \n 
~~~ numpy . random . seed ( 20 ) \n 
~~~ if userandom : \n 
~~ if onlyidxquery : \n 
~~ ~~ if psyco_imported and usepsyco : \n 
~~~ psyco . bind ( db . create_db ) \n 
psyco . bind ( db . query_db ) \n 
~~ if docreate : \n 
~~~ if verbose : \n 
~~ db . create_db ( dtype , kind , optlevel , verbose ) \n 
~~ if doquery : \n 
if doprofile : \n 
~~~ import pstats \n 
import cProfile as prof \n 
prof . run ( \n 
stats = pstats . Stats ( ) \n 
stats . strip_dirs ( ) \n 
stats . sort_stats ( , ) \n 
~~~ stats . print_stats ( ) \n 
~~~ stats . print_stats ( 20 ) \n 
~~ ~~ elif dokprofile : \n 
~~~ from cProfile import Profile \n 
import lsprofcalltree \n 
prof = Profile ( ) \n 
kcg = lsprofcalltree . KCacheGrind ( prof ) \n 
ofile = open ( , ) \n 
kcg . output ( ofile ) \n 
ofile . close ( ) \n 
~~ elif doprofile : \n 
~~~ import hotshot \n 
import hotshot . stats \n 
prof = hotshot . Profile ( "indexed_search.prof" ) \n 
benchtime , stones = prof . run ( \n 
prof . close ( ) \n 
stats = hotshot . stats . load ( "indexed_search.prof" ) \n 
stats . print_stats ( 20 ) \n 
~~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) \n 
~~ ~~ if repeatquery : \n 
~~~ db . rng = [ 1 , 1 ] \n 
~~~ print ( "range:" , db . rng ) \n 
~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
for i in range ( repeatvalue ) : \n 
~~~ for j in ( 1 , 2 , 5 ) : \n 
~~~ rng = j * 10 ** i \n 
db . rng = [ - rng / 2 , rng / 2 ] \n 
import matplotlib as mpl \n 
from pylab import * \n 
KB_ = 1024 \n 
MB_ = 1024 * KB_ \n 
GB_ = 1024 * MB_ \n 
linewidth = 2 \n 
markers = [ , , , , , , , , , ] \n 
markersize = 8 \n 
def get_values ( filename ) : \n 
~~~ f = open ( filename ) \n 
values = { "memcpyw" : [ ] , "memcpyr" : [ ] } \n 
for line in f : \n 
~~~ tmp = line . split ( ) [ 1 ] \n 
nthreads , size , elsize , sbits , codec , shuffle = [ i for i in tmp . split ( ) ] \n 
nthreads , size , elsize , sbits = map ( int , ( nthreads , size , elsize , sbits ) ) \n 
values [ "size" ] = size * NCHUNKS / MB_ ; \n 
values [ "elsize" ] = elsize ; \n 
values [ "sbits" ] = sbits ; \n 
values [ "codec" ] = codec \n 
values [ "shuffle" ] = shuffle \n 
( ratios , speedsw , speedsr ) = ( [ ] , [ ] , [ ] ) \n 
values [ nthreads ] = ( ratios , speedsw , speedsr ) \n 
~~ elif line . startswith ( ) : \n 
memcpyw = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyw" ] . append ( memcpyw ) \n 
memcpyr = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyr" ] . append ( memcpyr ) \n 
speedw = float ( tmp . split ( ) [ 1 ] ) \n 
ratio = float ( line . split ( ) [ - 1 ] ) \n 
speedsw . append ( speedw ) \n 
ratios . append ( ratio ) \n 
speedr = float ( tmp . split ( ) [ 1 ] ) \n 
speedsr . append ( speedr ) \n 
if "OK" not in line : \n 
~~ ~~ ~~ f . close ( ) \n 
return nthreads , values \n 
~~ def show_plot ( plots , yaxis , legends , gtitle , xmax = None ) : \n 
~~~ xlabel ( ) \n 
ylabel ( ) \n 
title ( gtitle ) \n 
xlim ( 0 , xmax ) \n 
ylim ( 0 , None ) \n 
grid ( True ) \n 
legend ( [ p [ 0 ] for p in plots \n 
if not isinstance ( p , mpl . lines . Line2D ) ] , \n 
legends , loc = "best" ) \n 
if outfile : \n 
savefig ( outfile , dpi = 64 ) \n 
~~~ show ( ) \n 
~~ ~~ if __name__ == : \n 
~~~ from optparse import OptionParser \n 
compress_title = \n 
decompress_title = \n 
yaxis = \n 
parser = OptionParser ( usage = usage ) \n 
parser . add_option ( , \n 
dest = , \n 
help = ( \n 
help = , ) \n 
help = , \n 
default = None ) \n 
parser . add_option ( , , action = , \n 
default = False ) \n 
( options , args ) = parser . parse_args ( ) \n 
if len ( args ) == 0 : \n 
~~ elif len ( args ) > 1 : \n 
~~ if options . report and options . outfile : \n 
~~ if options . dspeed and options . cspeed : \n 
~~ elif options . cspeed : \n 
~~~ options . dspeed = False \n 
plot_title = compress_title \n 
~~~ options . dspeed = True \n 
plot_title = decompress_title \n 
~~ filename = args [ 0 ] \n 
cspeed = options . cspeed \n 
dspeed = options . dspeed \n 
if options . outfile : \n 
~~~ outfile = options . outfile \n 
~~ elif options . report : \n 
~~~ if cspeed : \n 
~~~ outfile = filename [ : filename . rindex ( ) ] + \n 
~~~ outfile = None \n 
~~ plots = [ ] \n 
legends = [ ] \n 
nthreads , values = get_values ( filename ) \n 
if options . limit : \n 
~~~ thread_range = eval ( options . limit ) \n 
~~~ thread_range = range ( 1 , nthreads + 1 ) \n 
~~ if options . title : \n 
~~~ plot_title = options . title \n 
~~ gtitle = plot_title \n 
for nt in thread_range : \n 
~~~ ( ratios , speedw , speedr ) = values [ nt ] \n 
if cspeed : \n 
~~~ speed = speedw \n 
~~~ speed = speedr \n 
~~ plot_ = plot ( ratios , speed , linewidth = 2 ) \n 
plots . append ( plot_ ) \n 
nmarker = nt \n 
if nt >= len ( markers ) : \n 
~~~ nmarker = nt % len ( markers ) \n 
~~ setp ( plot_ , marker = markers [ nmarker ] , markersize = markersize , \n 
linewidth = linewidth ) \n 
~~ if cspeed : \n 
~~~ mean = np . mean ( values [ "memcpyw" ] ) \n 
~~~ mean = np . mean ( values [ "memcpyr" ] ) \n 
~~ plot_ = axhline ( mean , linewidth = 3 , linestyle = , color = ) \n 
text ( 1.0 , mean + 50 , message ) \n 
show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n 
options . xmax else None ) \n 
import tables \n 
fileh = tables . open_file ( "attributes1.h5" , mode = "w" , \n 
root = fileh . root \n 
a = np . array ( [ 1 , 2 , 4 ] , np . int32 ) \n 
hdfarray . attrs . char = "1" \n 
hdfarray . attrs . int = 12 \n 
hdfarray . attrs . float = 12.32 \n 
hdfarray . attrs . object = { "a" : 32.1 , "b" : 1 , "c" : [ 1 , 2 ] } \n 
fileh . close ( ) \n 
def setUp ( filename ) : \n 
fileh . enable_undo ( ) \n 
return fileh \n 
~~ def tearDown ( fileh ) : \n 
~~~ fileh . disable_undo ( ) \n 
~~ def demo_6times3marks ( ) : \n 
fileh = setUp ( "undo-redo-6times3marks.h5" ) \n 
fileh . mark ( ) \n 
fileh . undo ( ) \n 
assert "/otherarray1" in fileh \n 
assert "/otherarray2" in fileh \n 
assert "/otherarray3" in fileh \n 
assert "/otherarray4" in fileh \n 
assert "/otherarray5" not in fileh \n 
assert "/otherarray6" not in fileh \n 
assert "/otherarray3" not in fileh \n 
assert "/otherarray4" not in fileh \n 
assert "/otherarray1" not in fileh \n 
assert "/otherarray2" not in fileh \n 
fileh . redo ( ) \n 
assert "/otherarray5" in fileh \n 
assert "/otherarray6" in fileh \n 
tearDown ( fileh ) \n 
~~ def demo_manyops ( ) : \n 
fileh = setUp ( "undo-redo-manyops.h5" ) \n 
new_node = fileh . copy_node ( , ) \n 
new_node = fileh . copy_children ( , , recursive = 1 ) \n 
fileh . rename_node ( , ) \n 
fileh . remove_node ( ) \n 
assert not in fileh \n 
assert in fileh \n 
assert fileh . root . agroup . anarray3 is new_node \n 
~~ if __name__ == : \n 
~~~ demo_6times3marks ( ) \n 
demo_manyops ( ) \n 
######################################################################## \n 
import functools \n 
from . registry import class_name_dict , class_id_dict \n 
from . exceptions import ( ClosedNodeError , NodeError , UndoRedoWarning , \n 
PerformanceWarning ) \n 
from . path import join_path , split_path , isvisiblepath \n 
from . utils import lazyattr \n 
from . undoredo import move_to_shadow \n 
from . attributeset import AttributeSet , NotLoggedAttributeSet \n 
__docformat__ = \n 
def _closedrepr ( oldmethod ) : \n 
@ functools . wraps ( oldmethod ) \n 
def newmethod ( self ) : \n 
~~~ if not self . _v_isopen : \n 
~~~ cmod = self . __class__ . __module__ \n 
cname = self . __class__ . __name__ \n 
addr = hex ( id ( self ) ) \n 
return % ( cmod , cname , addr ) \n 
~~ return oldmethod ( self ) \n 
~~ return newmethod \n 
~~ class MetaNode ( type ) : \n 
def __new__ ( class_ , name , bases , dict_ ) : \n 
~~~ for mname in [ , ] : \n 
~~~ if mname in dict_ : \n 
~~~ dict_ [ mname ] = _closedrepr ( dict_ [ mname ] ) \n 
~~ ~~ return type . __new__ ( class_ , name , bases , dict_ ) \n 
~~ def __init__ ( class_ , name , bases , dict_ ) : \n 
~~~ super ( MetaNode , class_ ) . __init__ ( name , bases , dict_ ) \n 
class_name_dict [ class_ . __name__ ] = class_ \n 
cid = getattr ( class_ , , None ) \n 
if cid is not None : \n 
~~~ for base in bases : \n 
~~~ pcid = getattr ( base , , None ) \n 
if pcid == cid : \n 
~~~ class_id_dict [ cid ] = class_ \n 
~~ ~~ ~~ ~~ class Node ( six . with_metaclass ( MetaNode , object ) ) : \n 
_AttributeSet = AttributeSet \n 
def _g_getparent ( self ) : \n 
( parentpath , nodename ) = split_path ( self . _v_pathname ) \n 
return self . _v_file . _get_node ( parentpath ) \n 
~~ _v_parent = property ( _g_getparent ) \n 
@ lazyattr \n 
def _v_attrs ( self ) : \n 
return self . _AttributeSet ( self ) \n 
~~ def _g_gettitle ( self ) : \n 
if hasattr ( self . _v_attrs , ) : \n 
~~~ return self . _v_attrs . TITLE \n 
~~ ~~ def _g_settitle ( self , title ) : \n 
~~~ self . _v_attrs . TITLE = title \n 
~~ _v_title = property ( _g_gettitle , _g_settitle ) \n 
_v_isopen = False \n 
def __init__ ( self , parentnode , name , _log = True ) : \n 
~~~ if isinstance ( parentnode , class_name_dict [ ] ) : \n 
~~~ parentnode = parentnode . dereference ( ) \n 
~~ self . _v_file = None \n 
self . _v_isopen = False \n 
self . _v_pathname = None \n 
self . _v_name = None \n 
self . _v_depth = None \n 
self . _v_maxtreedepth = parentnode . _v_file . params [ ] \n 
self . _v__deleting = False \n 
self . _v_objectid = None \n 
self . _g_check_group ( parentnode ) \n 
parentnode . _g_check_open ( ) \n 
file_ = parentnode . _v_file \n 
if new : \n 
~~~ file_ . _check_writable ( ) \n 
~~ if new : \n 
~~~ parentnode . _g_refnode ( self , name , validate ) \n 
~~ self . _g_set_location ( parentnode , name ) \n 
~~~ self . _g_new ( parentnode , name , init = True ) \n 
~~~ self . _v_objectid = self . _g_create ( ) \n 
~~~ self . _v_objectid = self . _g_open ( ) \n 
~~ if new and _log and file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_log_create ( ) \n 
~~ self . _g_post_init_hook ( ) \n 
~~~ self . _f_close ( ) \n 
raise \n 
~~ ~~ def _g_log_create ( self ) : \n 
~~~ self . _v_file . _log ( , self . _v_pathname ) \n 
~~ def __del__ ( self ) : \n 
~~ self . _v__deleting = True \n 
~~~ node_manager = self . _v_file . _node_manager \n 
node_manager . drop_node ( self , check_unregistered = False ) \n 
~~~ if self . _v_isopen : \n 
~~~ self . _v__deleting = True \n 
self . _f_close ( ) \n 
~~ ~~ ~~ def _g_pre_kill_hook ( self ) : \n 
~~ def _g_create ( self ) : \n 
~~ def _g_open ( self ) : \n 
~~ def _g_check_open ( self ) : \n 
if not self . _v_isopen : \n 
~~ def _g_set_location ( self , parentnode , name ) : \n 
parentdepth = parentnode . _v_depth \n 
self . _v_file = file_ \n 
self . _v_isopen = True \n 
root_uep = file_ . root_uep \n 
if name . startswith ( root_uep ) : \n 
~~~ assert parentdepth == 0 \n 
if root_uep == "/" : \n 
~~~ self . _v_pathname = name \n 
~~~ self . _v_pathname = name [ len ( root_uep ) : ] \n 
~~ _ , self . _v_name = split_path ( name ) \n 
self . _v_depth = name . count ( "/" ) - root_uep . count ( "/" ) + 1 \n 
~~~ self . _v_name = name \n 
self . _v_pathname = join_path ( parentnode . _v_pathname , name ) \n 
self . _v_depth = parentdepth + 1 \n 
~~ if parentdepth >= self . _v_maxtreedepth : \n 
% ( self . _v_pathname , self . _v_maxtreedepth ) , \n 
~~ if self . _v_pathname != : \n 
~~~ file_ . _node_manager . cache_node ( self , self . _v_pathname ) \n 
~~ ~~ def _g_update_location ( self , newparentpath ) : \n 
oldpath = self . _v_pathname \n 
newpath = join_path ( newparentpath , self . _v_name ) \n 
newdepth = newpath . count ( ) \n 
self . _v_pathname = newpath \n 
self . _v_depth = newdepth \n 
if newdepth > self . _v_maxtreedepth : \n 
% ( self . _v_maxtreedepth , ) , PerformanceWarning ) \n 
~~ node_manager = self . _v_file . _node_manager \n 
node_manager . rename_node ( oldpath , newpath ) \n 
self . _g_update_dependent ( ) \n 
~~ def _g_del_location ( self ) : \n 
node_manager = self . _v_file . _node_manager \n 
pathname = self . _v_pathname \n 
if not self . _v__deleting : \n 
~~~ node_manager . drop_from_cache ( pathname ) \n 
node_manager . registry . pop ( pathname , None ) \n 
~~ def _g_post_init_hook ( self ) : \n 
~~ def _g_update_dependent ( self ) : \n 
if in self . __dict__ : \n 
~~~ self . _v_attrs . _g_update_node_location ( self ) \n 
~~ ~~ def _f_close ( self ) : \n 
~~ myDict = self . __dict__ \n 
if in myDict : \n 
~~~ self . _v_attrs . _g_close ( ) \n 
~~ self . _g_del_location ( ) \n 
myDict . clear ( ) \n 
~~ def _g_remove ( self , recursive , force ) : \n 
parent = self . _v_parent \n 
parent . _g_unrefnode ( self . _v_name ) \n 
self . _g_delete ( parent ) \n 
~~ def _f_remove ( self , recursive = False , force = False ) : \n 
self . _g_check_open ( ) \n 
file_ = self . _v_file \n 
file_ . _check_writable ( ) \n 
if file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_remove_and_log ( recursive , force ) \n 
~~~ self . _g_remove ( recursive , force ) \n 
~~ ~~ def _g_remove_and_log ( self , recursive , force ) : \n 
~~~ file_ = self . _v_file \n 
oldpathname = self . _v_pathname \n 
file_ . _log ( , oldpathname ) \n 
move_to_shadow ( file_ , oldpathname ) \n 
~~ def _g_move ( self , newparent , newname ) : \n 
oldparent = self . _v_parent \n 
oldname = self . _v_name \n 
newparent . _g_refnode ( self , newname ) \n 
oldparent . _g_unrefnode ( oldname ) \n 
self . _g_del_location ( ) \n 
self . _g_set_location ( newparent , newname ) \n 
self . _g_new ( newparent , self . _v_name , init = False ) \n 
self . _v_parent . _g_move_node ( oldparent . _v_objectid , oldname , \n 
newparent . _v_objectid , newname , \n 
oldpathname , self . _v_pathname ) \n 
~~ def _f_rename ( self , newname , overwrite = False ) : \n 
self . _f_move ( newname = newname , overwrite = overwrite ) \n 
~~ def _f_move ( self , newparent = None , newname = None , \n 
overwrite = False , createparents = False ) : \n 
if newparent is None and newname is None : \n 
~~ if newparent is None : \n 
~~~ newparent = oldparent \n 
~~ if newname is None : \n 
~~~ newname = oldname \n 
~~~ newfile = newparent . _v_file \n 
newpath = newparent . _v_pathname \n 
~~~ newfile = file_ \n 
newpath = newparent \n 
% ( newparent , ) ) \n 
~~ if newfile is not file_ : \n 
~~ file_ . _check_writable ( ) \n 
oldpath = oldparent . _v_pathname \n 
if newpath == oldpath and newname == oldname : \n 
~~ self . _g_check_not_contains ( newpath ) \n 
newparent = file_ . _get_or_create_path ( newparent , createparents ) \n 
self . _g_maybe_remove ( newparent , newname , overwrite ) \n 
self . _g_move ( newparent , newname ) \n 
~~~ self . _g_log_move ( oldpathname ) \n 
~~ ~~ def _g_log_move ( self , oldpathname ) : \n 
~~~ self . _v_file . _log ( , oldpathname , self . _v_pathname ) \n 
~~ def _g_copy ( self , newparent , newname , recursive , _log = True , ** kwargs ) : \n 
~~ def _g_copy_as_child ( self , newparent , ** kwargs ) : \n 
return self . _g_copy ( newparent , self . _v_name , \n 
recursive = False , _log = False , ** kwargs ) \n 
~~ def _f_copy ( self , newparent = None , newname = None , \n 
overwrite = False , recursive = False , createparents = False , \n 
srcfile = self . _v_file \n 
srcparent = self . _v_parent \n 
srcname = self . _v_name \n 
dstparent = newparent \n 
dstname = newname \n 
if dstparent is None and dstname is None : \n 
~~ if dstparent is None : \n 
~~~ dstparent = srcparent \n 
~~ if dstname is None : \n 
~~~ dstname = srcname \n 
~~~ dstfile = dstparent . _v_file \n 
dstpath = dstparent . _v_pathname \n 
~~~ dstfile = srcfile \n 
dstpath = dstparent \n 
% ( dstparent , ) ) \n 
~~ if dstfile is srcfile : \n 
~~~ srcpath = srcparent . _v_pathname \n 
if dstpath == srcpath and dstname == srcname : \n 
~~~ raise NodeError ( \n 
% self . _v_pathname ) \n 
~~ if recursive : \n 
~~~ self . _g_check_not_contains ( dstpath ) \n 
~~ ~~ dstparent = srcfile . _get_or_create_path ( dstparent , createparents ) \n 
if dstfile is not srcfile and srcfile . is_undo_enabled ( ) : \n 
UndoRedoWarning ) \n 
~~ self . _g_maybe_remove ( dstparent , dstname , overwrite ) \n 
return self . _g_copy ( dstparent , dstname , recursive , ** kwargs ) \n 
~~ def _f_isvisible ( self ) : \n 
return isvisiblepath ( self . _v_pathname ) \n 
~~ def _g_check_group ( self , node ) : \n 
~~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
% node . _v_pathname ) \n 
~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
~~ ~~ def _g_check_not_contains ( self , pathname ) : \n 
~~~ mypathname = self . _v_pathname \n 
or pathname == mypathname \n 
or pathname . startswith ( mypathname + ) ) : \n 
~~ ~~ def _g_maybe_remove ( self , parent , name , overwrite ) : \n 
~~~ if name in parent : \n 
~~~ if not overwrite : \n 
~~ parent . _f_get_child ( name ) . _f_remove ( True ) \n 
~~ ~~ def _g_check_name ( self , name ) : \n 
if name . startswith ( ) : \n 
~~ ~~ def _f_getattr ( self , name ) : \n 
return getattr ( self . _v_attrs , name ) \n 
~~ def _f_setattr ( self , name , value ) : \n 
setattr ( self . _v_attrs , name , value ) \n 
~~ def _f_delattr ( self , name ) : \n 
delattr ( self . _v_attrs , name ) \n 
~~ ~~ class NotLoggedMixin : \n 
~~~ _AttributeSet = NotLoggedAttributeSet \n 
def _g_log_create ( self ) : \n 
~~ def _g_log_move ( self , oldpathname ) : \n 
~~ def _g_remove_and_log ( self , recursive , force ) : \n 
~~ ~~ from __future__ import print_function \n 
from tables import IsDescription , StringCol , BoolCol , IntCol , FloatCol \n 
from tables . node import NotLoggedMixin \n 
from tables . path import join_path \n 
from tables . tests import common \n 
from tables . tests . common import unittest \n 
from tables . tests . common import PyTablesTestCase as TestCase \n 
class BasicTestCase ( common . TempFileMixin , TestCase ) : \n 
_reopen_flag = False \n 
def _do_reopen ( self ) : \n 
~~~ if self . _reopen_flag : \n 
~~~ self . _reopen ( ) \n 
~~ ~~ def setUp ( self ) : \n 
~~~ super ( BasicTestCase , self ) . setUp ( ) \n 
h5file = self . h5file \n 
root = h5file . root \n 
~~ def test00_simple ( self ) : \n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
~~ self . h5file . enable_undo ( ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray" not in self . h5file ) \n 
self . assertEqual ( self . h5file . _curaction , 0 ) \n 
self . assertEqual ( self . h5file . _curmark , 0 ) \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( ) \n 
~~ self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . _curaction , 1 ) \n 
~~ def test01_twice ( self ) : \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . _curaction , 2 ) \n 
~~ def test02_twice2 ( self ) : \n 
self . h5file . mark ( ) \n 
self . assertEqual ( self . h5file . _curaction , 3 ) \n 
self . assertEqual ( self . h5file . _curmark , 1 ) \n 
~~ def test03_6times3marks ( self ) : \n 
self . __class__ . __name__ ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" in self . h5file ) \n 
self . assertTrue ( "/otherarray6" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ 6 , 7 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray5 . read ( ) , [ 7 , 8 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray6 . read ( ) , [ 8 , 9 ] ) \n 
~~ def test04_6times3marksro ( self ) : \n 
~~ self . h5file . mark ( ) \n 
~~ def test05_destructive ( self ) : \n 
~~ def test05b_destructive ( self ) : \n 
~~ def test05c_destructive ( self ) : \n 
~~ def test05d_destructive ( self ) : \n 
self . h5file . undo ( 0 ) \n 
~~ def test05e_destructive ( self ) : \n 
~~ def test05f_destructive ( self ) : \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
self . assertTrue ( not in self . h5file ) \n 
newarr = self . h5file . create_array ( , , [ 1 ] ) \n 
self . assertTrue ( in self . h5file ) \n 
if not self . _reopen_flag : \n 
~~~ self . assertTrue ( self . h5file . root . newarray is newarr ) \n 
~~ ~~ def test06_totalunwind ( self ) : \n 
~~ def test07_totalrewind ( self ) : \n 
self . h5file . redo ( - 1 ) \n 
~~ def test08_marknames ( self ) : \n 
self . h5file . mark ( "first" ) \n 
self . h5file . mark ( "second" ) \n 
self . h5file . mark ( "third" ) \n 
self . h5file . undo ( "first" ) \n 
self . h5file . redo ( "third" ) \n 
self . h5file . undo ( "second" ) \n 
~~ def test08_initialmark ( self ) : \n 
initmid = self . h5file . get_current_mark ( ) \n 
self . h5file . undo ( initmid ) \n 
~~ def test09_marknames ( self ) : \n 
with self . assertRaises ( tables . UndoRedoError ) : \n 
~~~ self . h5file . undo ( "third" ) \n 
~~ self . h5file . redo ( "third" ) \n 
~~~ self . h5file . redo ( "second" ) \n 
~~ self . assertTrue ( "/otherarray1" in self . h5file ) \n 
~~ def test10_goto ( self ) : \n 
self . h5file . goto ( "first" ) \n 
self . h5file . goto ( "third" ) \n 
self . h5file . goto ( "second" ) \n 
self . h5file . goto ( - 1 ) \n 
~~ def test10_gotoint ( self ) : \n 
self . h5file . goto ( 1 ) \n 
self . h5file . goto ( 0 ) \n 
self . h5file . goto ( 3 ) \n 
self . h5file . goto ( 2 ) \n 
~~ def test11_contiguous ( self ) : \n 
m1 = self . h5file . mark ( ) \n 
m2 = self . h5file . mark ( ) \n 
self . assertNotEqual ( m1 , m2 ) \n 
self . h5file . undo ( m1 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m1 ) \n 
self . h5file . redo ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n 
self . h5file . goto ( m1 ) \n 
self . h5file . goto ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , 0 ) \n 
~~ def test12_keepMark ( self ) : \n 
mid = self . h5file . mark ( ) \n 
self . assertTrue ( mid is not None ) \n 
~~ def test13_severalEnableDisable ( self ) : \n 
self . h5file . disable_undo ( ) \n 
self . h5file . enable_undo ( ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , mid ) \n 
~~ ~~ class PersistenceTestCase ( BasicTestCase ) : \n 
_reopen_flag = True \n 
~~ class CreateArrayTestCase ( common . TempFileMixin , TestCase ) : \n 
def setUp ( self ) : \n 
~~~ super ( CreateArrayTestCase , self ) . setUp ( ) \n 
~~ def test00 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 1 , 2 ] ) \n 
~~ def test01 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
~~ def test02 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 3 , 4 ] ) \n 
~~ def test03 ( self ) : \n 
self . h5file . create_array ( , , \n 
self . assertTrue ( "/agroup/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/agroup/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . title , \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . title , \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . read ( ) , \n 
[ 3 , 4 ] ) \n 
~~ ~~ class CreateGroupTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CreateGroupTestCase , self ) . setUp ( ) \n 
self . assertTrue ( "/othergroup1" not in self . h5file ) \n 
self . assertTrue ( "/othergroup1" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n 
self . assertTrue ( "/othergroup2" not in self . h5file ) \n 
self . assertTrue ( "/othergroup2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup2 . _v_title , \n 
self . assertTrue ( "/othergroup3" not in self . h5file ) \n 
self . assertTrue ( "/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup3 . _v_title , \n 
self . h5file . create_group ( \n 
self . assertTrue ( "/othergroup1/othergroup2" not in self . h5file ) \n 
self . assertTrue ( \n 
"/othergroup1/othergroup2/othergroup3" not in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2" in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . othergroup2 . _v_title , \n 
self . assertEqual ( \n 
self . h5file . root . othergroup1 . othergroup2 . othergroup3 . _v_title , \n 
~~ ~~ minRowIndex = 10 \n 
def populateTable ( where , name ) : \n 
class Indexed ( IsDescription ) : \n 
~~~ var1 = StringCol ( itemsize = 4 , dflt = b"" , pos = 1 ) \n 
var2 = BoolCol ( dflt = 0 , pos = 2 ) \n 
var3 = IntCol ( dflt = 0 , pos = 3 ) \n 
var4 = FloatCol ( dflt = 0 , pos = 4 ) \n 
~~ nrows = minRowIndex \n 
table = where . _v_file . create_table ( where , name , Indexed , "Indexed" , \n 
None , nrows ) \n 
for i in range ( nrows ) : \n 
~~~ table . row [ ] = str ( i ) \n 
table . row [ ] = i % 2 \n 
table . row [ ] = i \n 
table . row [ ] = float ( nrows - i - 1 ) \n 
table . row . append ( ) \n 
~~ table . flush ( ) \n 
indexrows = table . cols . var1 . create_index ( ) \n 
indexrows = table . cols . var2 . create_index ( ) \n 
indexrows = table . cols . var3 . create_index ( ) \n 
~~ ~~ class RenameNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( RenameNodeTestCase , self ) . setUp ( ) \n 
populateTable ( self . h5file . root , ) \n 
self . h5file . rename_node ( , ) \n 
self . assertTrue ( "/agroup2" in self . h5file ) \n 
self . assertTrue ( "/agroup3" not in self . h5file ) \n 
self . assertTrue ( "/agroup2" not in self . h5file ) \n 
self . assertTrue ( "/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup3/agroup3" in self . h5file ) \n 
~~ def test01b ( self ) : \n 
self . assertTrue ( "/agroup4" not in self . h5file ) \n 
self . assertTrue ( "/agroup4" in self . h5file ) \n 
self . assertTrue ( "/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup4/agroup3" in self . h5file ) \n 
self . assertTrue ( "/anarray" in self . h5file ) \n 
self . assertTrue ( "/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/anarray" not in self . h5file ) \n 
self . assertTrue ( "/anarray2" in self . h5file ) \n 
self . assertTrue ( "/table" in self . h5file ) \n 
table = self . h5file . root . table \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertTrue ( "/table2" not in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table . title , "Indexed" ) \n 
self . assertTrue ( "/table" not in self . h5file ) \n 
self . assertTrue ( "/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table2 . title , "Indexed" ) \n 
table = self . h5file . root . table2 \n 
~~ ~~ class MoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( MoveNodeTestCase , self ) . setUp ( ) \n 
self . h5file . move_node ( , ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . anarray . title , \n 
self . h5file . move_node ( , , ) \n 
self . assertTrue ( "/agroup2/agroup3" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup3 . _v_title , \n 
self . assertTrue ( "/agroup2/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup4 . _v_title , \n 
self . assertTrue ( "/agroup2/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . table2 . title , "Indexed" ) \n 
table = self . h5file . root . agroup2 . table2 \n 
~~ ~~ class RemoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( RemoveNodeTestCase , self ) . setUp ( ) \n 
self . h5file . remove_node ( ) \n 
~~ def test00b ( self ) : \n 
self . assertTrue ( "/agroup/anarray2" not in self . h5file ) \n 
~~ def test00c ( self ) : \n 
self . h5file . remove_node ( , recursive = 1 ) \n 
self . assertTrue ( "/agroup/anarray1" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" not in self . h5file ) \n 
~~ ~~ class CopyNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CopyNodeTestCase , self ) . setUp ( ) \n 
~~ def test00_copyLeaf ( self ) : \n 
new_node = self . h5file . copy_node ( , ) \n 
self . assertTrue ( self . h5file . root . agroup . agroup3 . anarray is new_node ) \n 
~~ def test00b_copyTable ( self ) : \n 
warnings . filterwarnings ( "ignore" , category = UserWarning ) \n 
table = self . h5file . copy_node ( \n 
, , propindexes = True ) \n 
warnings . filterwarnings ( "default" , category = UserWarning ) \n 
self . assertTrue ( "/agroup/agroup3/table" in self . h5file ) \n 
table = self . h5file . root . agroup . agroup3 . table \n 
self . assertEqual ( table . title , "Indexed" ) \n 
self . assertTrue ( "/agroup/agroup3/table" not in self . h5file ) \n 
~~ def test01_copyGroup ( self ) : \n 
new_node = self . h5file . copy_node ( \n 
, newname = , recursive = True ) \n 
self . assertTrue ( self . h5file . root . acopy is new_node ) \n 
~~ def test02_copyLeafOverwrite ( self ) : \n 
oldNode = self . h5file . root . agroup \n 
, newname = , overwrite = True ) \n 
self . assertTrue ( self . h5file . root . agroup is oldNode ) \n 
self . assertTrue ( self . h5file . root . agroup is new_node ) \n 
~~ def test03_copyChildren ( self ) : \n 
self . h5file . copy_children ( , , recursive = True ) \n 
~~ ~~ class ComplexTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( ComplexTestCase , self ) . setUp ( ) \n 
self . h5file . create_array ( self . h5file . root , , \n 
new_node = self . h5file . copy_children ( \n 
, , recursive = 1 ) \n 
self . assertTrue ( self . h5file . root . agroup . anarray3 is new_node ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 1 ] ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 4 ] ) \n 
self . h5file . create_group ( self . h5file . root . agroup2 , , \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup5 . _v_title , \n 
self . h5file . create_group ( self . h5file . root . agroup , , \n 
~~ def test03b ( self ) : \n 
self . h5file . create_group ( self . h5file . root . agroup3 , , \n 
~~ ~~ class AttributesTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( AttributesTestCase , self ) . setUp ( ) \n 
array = self . h5file . create_array ( , , [ 1 , 2 ] ) \n 
attrs = array . attrs \n 
attrs . attr_1 = 10 \n 
attrs . attr_2 = 20 \n 
attrs . attr_3 = 30 \n 
~~ def test00_setAttr ( self ) : \n 
~~ array = self . h5file . root . array \n 
setattr ( attrs , , 0 ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_0 , 0 ) \n 
self . assertTrue ( not in attrs ) \n 
~~ def test01_setAttrExisting ( self ) : \n 
setattr ( attrs , , 11 ) \n 
self . assertEqual ( attrs . attr_1 , 11 ) \n 
self . assertEqual ( attrs . attr_1 , 10 ) \n 
~~ def test02_delAttr ( self ) : \n 
delattr ( attrs , ) \n 
~~ def test03_copyNodeAttrs ( self ) : \n 
~~ rattrs = self . h5file . root . _v_attrs \n 
rattrs . attr_0 = 0 \n 
rattrs . attr_1 = 100 \n 
array = self . h5file . root . array \n 
attrs . _f_copy ( self . h5file . root ) \n 
self . assertEqual ( rattrs . attr_0 , 0 ) \n 
self . assertEqual ( rattrs . attr_1 , 10 ) \n 
self . assertEqual ( rattrs . attr_2 , 20 ) \n 
self . assertEqual ( rattrs . attr_3 , 30 ) \n 
self . assertEqual ( rattrs . attr_1 , 100 ) \n 
self . assertTrue ( not in rattrs ) \n 
~~ def test04_replaceNode ( self ) : \n 
attrs . attr_1 = 11 \n 
arr = self . h5file . create_array ( , , [ 1 ] ) \n 
arr . attrs . attr_1 = 12 \n 
self . assertTrue ( in self . h5file . root . array . attrs ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 10 ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 12 ) \n 
~~ ~~ class NotLoggedTestCase ( common . TempFileMixin , TestCase ) : \n 
class NotLoggedArray ( NotLoggedMixin , tables . Array ) : \n 
~~ def test00_hierarchy ( self ) : \n 
self . h5file . create_group ( , ) \n 
arr = self . NotLoggedArray ( self . h5file . root , , \n 
[ 1 ] , self . _getMethodName ( ) ) \n 
arr . move ( ) \n 
arr . remove ( ) \n 
~~ def test01_attributes ( self ) : \n 
arr . _v_attrs . foo = \n 
self . assertEqual ( arr . _v_attrs . foo , ) \n 
del arr . _v_attrs . foo \n 
self . assertRaises ( AttributeError , getattr , arr . _v_attrs , ) \n 
~~ ~~ class CreateParentsTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CreateParentsTestCase , self ) . setUp ( ) \n 
g1 = self . h5file . create_group ( , ) \n 
self . h5file . create_group ( g1 , ) \n 
~~ def existing ( self , paths ) : \n 
return frozenset ( path for path in paths if path in self . h5file ) \n 
~~ def basetest ( self , doit , pre , post ) : \n 
~~~ pre ( ) \n 
paths = [ , , , ] \n 
for newpath in paths : \n 
~~~ before = self . existing ( paths ) \n 
doit ( newpath ) \n 
after = self . existing ( paths ) \n 
self . assertTrue ( after . issuperset ( before ) ) \n 
post ( newpath ) \n 
self . assertEqual ( after , before ) \n 
~~ ~~ def test00_create ( self ) : \n 
def pre ( ) : \n 
~~ def doit ( newpath ) : \n 
~~~ self . h5file . create_array ( newpath , , [ 1 ] , createparents = True ) \n 
self . assertTrue ( join_path ( newpath , ) in self . h5file ) \n 
~~ def post ( newpath ) : \n 
~~~ self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ self . basetest ( doit , pre , post ) \n 
~~ def test01_move ( self ) : \n 
~~~ self . h5file . create_array ( , , [ 1 ] ) \n 
~~~ self . h5file . move_node ( , newpath , createparents = True ) \n 
~~~ self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ def test02_copy ( self ) : \n 
~~~ self . h5file . copy_node ( , newpath , createparents = True ) \n 
~~~ g = self . h5file . create_group ( , ) \n 
self . h5file . create_array ( g , , [ 1 ] ) \n 
~~~ self . h5file . copy_children ( , newpath , createparents = True ) \n 
~~ ~~ def suite ( ) : \n 
~~~ theSuite = unittest . TestSuite ( ) \n 
niter = 1 \n 
for n in range ( niter ) : \n 
~~~ theSuite . addTest ( unittest . makeSuite ( BasicTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( PersistenceTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateArrayTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateGroupTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RenameNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( MoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RemoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CopyNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( AttributesTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( ComplexTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( NotLoggedTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateParentsTestCase ) ) \n 
~~ if common . heavy : \n 
~~ return theSuite \n 
common . parse_argv ( sys . argv ) \n 
common . print_versions ( ) \n 
unittest . main ( defaultTest = ) \n 
def _print_after_skip ( skip , it = None , dist = None , etime = None ) : \n 
~~~ if it is None : \n 
~~~ msg = "{i:<13}{d:<15}{t:<17}" . format ( i = "Iteration" , \n 
d = "Distance" , \n 
print ( msg ) \n 
print ( "-" * len ( msg ) ) \n 
~~ if it % skip == 0 : \n 
~~~ if etime is None : \n 
~~~ msg = "{i:<13}{d:<15.3e}{t:<18.3e}" \n 
print ( msg . format ( i = it , d = dist , t = etime ) ) \n 
~~ def compute_fixed_point ( T , v , error_tol = 1e-3 , max_iter = 50 , verbose = 1 , \n 
print_skip = 5 , * args , ** kwargs ) : \n 
iterate = 0 \n 
error = error_tol + 1 \n 
~~~ start_time = time . time ( ) \n 
_print_after_skip ( print_skip , it = None ) \n 
~~ while iterate < max_iter and error > error_tol : \n 
~~~ new_v = T ( v , * args , ** kwargs ) \n 
iterate += 1 \n 
error = np . max ( np . abs ( new_v - v ) ) \n 
~~~ etime = time . time ( ) - start_time \n 
_print_after_skip ( print_skip , iterate , error , etime ) \n 
~~~ v [ : ] = new_v \n 
~~ except TypeError : \n 
~~~ v = new_v \n 
~~ ~~ return v \n 
import unittest \n 
from scipy . linalg import LinAlgError \n 
from numpy . testing import assert_allclose \n 
from quantecon . lqcontrol import LQ \n 
from quantecon . robustlq import RBLQ \n 
class TestRBLQControl ( unittest . TestCase ) : \n 
~~~ a_0 = 100 \n 
a_1 = 0.5 \n 
rho = 0.9 \n 
sigma_d = 0.05 \n 
beta = 0.95 \n 
c = 2 \n 
gamma = 50.0 \n 
theta = 0.002 \n 
ac = ( a_0 - c ) / 2.0 \n 
R = np . array ( [ [ 0 , ac , 0 ] , \n 
[ ac , - a_1 , 0.5 ] , \n 
[ 0. , 0.5 , 0 ] ] ) \n 
R = - R \n 
Q = gamma / 2 \n 
A = np . array ( [ [ 1. , 0. , 0. ] , \n 
[ 0. , 1. , 0. ] , \n 
[ 0. , 0. , rho ] ] ) \n 
B = np . array ( [ [ 0. ] , \n 
[ 1. ] , \n 
[ 0. ] ] ) \n 
C = np . array ( [ [ 0. ] , \n 
[ 0. ] , \n 
[ sigma_d ] ] ) \n 
self . rblq_test = RBLQ ( Q , R , A , B , C , beta , theta ) \n 
self . lq_test = LQ ( Q , R , A , B , C , beta ) \n 
self . Fr , self . Kr , self . Pr = self . rblq_test . robust_rule ( ) \n 
~~ def tearDown ( self ) : \n 
~~~ del self . rblq_test \n 
~~ def test_robust_rule_vs_simple ( self ) : \n 
~~~ rblq = self . rblq_test \n 
Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n 
Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = 1e-12 ) \n 
assert_allclose ( Fr , Fs , rtol = 1e-4 ) \n 
assert_allclose ( Kr , Ks , rtol = 1e-4 ) \n 
assert_allclose ( Pr , Ps , rtol = 1e-4 ) \n 
~~ def test_f2k_and_k2f ( self ) : \n 
K_f2k , P_f2k = rblq . F_to_K ( Fr ) \n 
F_k2f , P_k2f = rblq . K_to_F ( Kr ) \n 
assert_allclose ( K_f2k , Kr , rtol = 1e-4 ) \n 
assert_allclose ( F_k2f , Fr , rtol = 1e-4 ) \n 
assert_allclose ( P_f2k , P_k2f , rtol = 1e-4 ) \n 
~~ def test_evaluate_F ( self ) : \n 
Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n 
assert_allclose ( Pf , Pr ) \n 
assert_allclose ( Kf , Kr ) \n 
~~~ suite = unittest . TestLoader ( ) . loadTestsFromTestCase ( TestRBLQControl ) \n 
unittest . TextTestRunner ( verbosity = 2 , stream = sys . stderr ) . run ( suite ) \n 
import tables as pt \n 
fileName = "defaultAlphaFileName.h5" \n 
h5f = [ ] \n 
group = [ ] \n 
table = [ ] \n 
opened = False \n 
ctr = float ( 0.0 ) \n 
class AlphaDataModelClass ( pt . IsDescription ) : \n 
~~~ symbol = pt . StringCol ( 30 ) \n 
exchange = pt . StringCol ( 10 ) \n 
alphaValue = pt . Float32Col ( ) \n 
timestamp = pt . Time64Col ( ) \n 
~~ ~~ def openFile ( newFileName ) : \n 
global fileName , h5f , group , table , opened , ctr \n 
if newFileName is None : \n 
~~~ if ( len ( newFileName ) > 0 ) : \n 
~~~ fileName = str ( newFileName ) \n 
~~ ~~ if not opened : \n 
~~~ h5f = pt . openFile ( str ( fileName ) , mode = "w" ) \n 
group = h5f . createGroup ( "/" , ) \n 
table = h5f . createTable ( group , , AlphaDataModelClass ) \n 
opened = True \n 
~~ ~~ def addRow ( currSymbol , currExchange , currAlphaVal , currTS ) : \n 
global ctr \n 
if opened : \n 
~~~ ctr = ctr + 1 \n 
row = table . row \n 
row [ ] = currSymbol \n 
row [ ] = currExchange \n 
row [ ] = currAlphaVal \n 
row [ ] = currTS \n 
row . append ( ) \n 
~~~ ctr = 0 \n 
raise IOError \n 
~~ ~~ def closeFile ( ) : \n 
table . flush ( ) \n 
h5f . close ( ) \n 
import pickle as pkl \n 
import qstkutil . utils as utils \n 
import dircache \n 
def main ( ) : \n 
~~~ print "Starting..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
~~~ rootdir = os . environ [ ] \n 
~~ fileExtensionToRemove = ".csv" \n 
listOfInputPaths = list ( ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NASDAQ/" ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NYSE/" ) \n 
listOfOutputPaths = list ( ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/AMEX/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NASDAQ/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NYSE/" ) \n 
for path in listOfOutputPaths : \n 
~~~ if not ( os . access ( path , os . F_OK ) ) : \n 
if ( len ( listOfInputPaths ) != len ( listOfOutputPaths ) ) : \n 
sys . exit ( "FAILURE" ) \n 
~~ path_ctr = - 1 ; \n 
for path in listOfInputPaths : \n 
~~~ path_ctr = path_ctr + 1 ; \n 
stocks_at_this_path = dircache . listdir ( str ( path ) ) \n 
filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , filtered_names ) \n 
stock_ctr = - 1 \n 
for stock in filtered_names : \n 
~~~ stock_ctr = stock_ctr + 1 \n 
stock_data = np . loadtxt ( path + stock + ".csv" , np . float , None , "," , None , 1 , use_cols ) \n 
stock_data_shape = stock_data . shape \n 
f = open ( listOfOutputPaths [ path_ctr ] + filtered_names [ stock_ctr ] + ".pkl" , "wb" ) \n 
pkl . dump ( stock_data , f , - 1 ) \n 
f . close ( ) \n 
~~ ~~ print "Finished..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
~~~ main ( ) \n 
~~ import cPickle \n 
from pandas import DataMatrix \n 
import datetime as dt \n 
import qstkutil . DataAccess as da \n 
import qstkutil . qsdateutil as du \n 
if __name__ == "__main__" : \n 
symbols = list ( [ ] ) \n 
t = map ( int , sys . argv [ 1 ] . split ( ) ) \n 
startday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
t = map ( int , sys . argv [ 2 ] . split ( ) ) \n 
endday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
timeofday = dt . timedelta ( hours = 16 ) \n 
timestamps = du . getNYSEdays ( startday , endday , timeofday ) \n 
dataobj = da . DataAccess ( ) \n 
historic = dataobj . get_data ( timestamps , symbols , "close" ) \n 
alloc_val = random . random ( ) \n 
alloc = DataMatrix ( index = [ historic . index [ 0 ] ] , data = [ alloc_val ] , columns = symbols ) \n 
for date in range ( 1 , len ( historic . index ) ) : \n 
~~~ alloc_val = 1 #random.random() \n 
alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols ~~ alloc [ ] = 1 - alloc [ symbols [ 0 ] ] \n 
output = open ( sys . argv [ 3 ] , "wb" ) \n 
cPickle . dump ( alloc , output ) \n 
import QSTK . qstkutil . DataAccess as da \n 
from itertools import izip \n 
def getStocks ( listOfPaths ) : \n 
~~~ listOfStocks = list ( ) \n 
fileExtensionToRemove = ".h5" \n 
for path in listOfPaths : \n 
~~~ stocksAtThisPath = list ( ) \n 
stocksAtThisPath = dircache . listdir ( str ( path ) ) \n 
stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , stocksAtThisPath \n 
for stock in stocksAtThisPath : \n 
~~~ listOfStocks . append ( stock ) \n 
~~ return listOfStocks \n 
~~~ print "Starting..." \n 
dataItemsList = [ ] \n 
dataItemsList . append ( ) \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NASDAQ/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/Delisted_US_Recent/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/OTC/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_AMEX/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_Delisted/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NYSE/") \n 
listOfStocks = list ( ) \n 
#listOfStocks.append("AAPL") \n 
#listOfStocks.append("YHOO") \n 
#listOfStocks.append("AMZN") \n 
listOfPaths = list ( ) \n 
listOfPaths . append ( "C:\\\\test\\\\temp\\\\" ) \n 
#listOfPaths.append("C:\\\\test\\\\hdf\\\\") \n 
listOfStocks = getStocks ( listOfPaths ) \n 
tslist = list ( alpha . getTimestampArray ( ) ) \n 
listOfTS = alpha . getTimestampArray ( ) \n 
for stock in [ "AAPL" ] : \n 
~~~ alphaList = alpha . getStockDataList ( stock , ) \n 
ctr = 0 \n 
for val in alphaList : \n 
ctr += 1 \n 
~~ ~~ print "DONE!" \n 
~~ import unittest \n 
import collections_and_iterators \n 
class TestObjectMethods ( unittest . TestCase ) : \n 
~~~ self . singleLinkList = collections_and_iterators . SinglyLinkedList ( ) \n 
self . singleLinkListData = collections_and_iterators . SinglyLinkedList ( ) \n 
self . singleLinkListData . append ( "Cosmo" ) \n 
self . singleLinkListData . append ( "Allie" ) \n 
self . singleLinkListData . append ( "Watson" ) \n 
self . doubleLinkList = collections_and_iterators . DoublyLinkedList ( ) \n 
self . doubleLinkListData = collections_and_iterators . DoublyLinkedList ( ) \n 
~~ def test_empty_single_list ( self ) : \n 
~~~ self . assertEqual ( 0 , self . singleLinkList . size ) \n 
self . assertIsNone ( self . singleLinkList . head ) \n 
self . assertIsNone ( self . singleLinkList . cursor ) \n 
~~ def test_contains_success ( self ) : \n 
~~~ self . assertTrue ( "Cosmo" in self . singleLinkListData ) \n 
self . assertTrue ( "Allie" in self . singleLinkListData ) \n 
self . assertTrue ( "Watson" in self . singleLinkListData ) \n 
~~ def test_contains_failure ( self ) : \n 
~~~ self . assertFalse ( "Gabby" in self . singleLinkListData ) \n 
self . assertFalse ( "Thomas" in self . singleLinkListData ) \n 
~~ def test_append_success ( self ) : \n 
~~~ self . assertEqual ( "Cosmo" , self . singleLinkListData [ 0 ] ) \n 
self . assertEqual ( "Allie" , self . singleLinkListData [ 1 ] ) \n 
self . assertEqual ( "Watson" , self . singleLinkListData [ 2 ] ) \n 
~~ def test_append_failure ( self ) : \n 
~~~ with self . assertRaises ( IndexError ) : \n 
~~~ self . singleLinkListData [ 3 ] \n 
~~ self . singleLinkListData . append ( "Foley" ) \n 
self . assertEqual ( "Foley" , self . singleLinkListData [ 3 ] ) \n 
~~~ self . assertEqual ( "Cosmo" , self . singleLinkListData . __getitem__ ( 0 ) ) \n 
self . assertEqual ( "Allie" , self . singleLinkListData . __getitem__ ( 1 ) ) \n 
self . assertEqual ( "Watson" , self . singleLinkListData . __getitem__ ( 2 ) ) \n 
~~ def test_getitem_failure ( self ) : \n 
~~~ self . singleLinkListData . __getitem__ ( 3 ) \n 
self . singleLinkListData . __getitem__ ( - 3 ) \n 
~~ ~~ def test_setitem_success ( self ) : \n 
self . singleLinkListData [ 0 ] = "Smalls" \n 
self . assertEqual ( "Smalls" , self . singleLinkListData [ 0 ] ) \n 
~~ def test_setitem_failure ( self ) : \n 
~~~ self . singleLinkListData [ 5 ] = "Bruno" \n 
self . singleLinkListData [ - 1 ] = "Lucie" \n 
~~ ~~ def test_empty_double_list ( self ) : \n 
~~~ self . assertEqual ( 0 , self . doubleLinkList . size ) \n 
self . assertIsNone ( self . doubleLinkList . head ) \n 
self . assertIsNone ( self . doubleLinkList . cursor ) \n 
~~ def test_insert_success ( self ) : \n 
~~ def test_insert_fauilure ( self ) : \n 
~~~ unittest . main ( verbosity = 2 ) \n 
~~ from types import FunctionType \n 
from rdflib . graph import ConjunctiveGraph \n 
from rdflib . graph import Graph \n 
from rdflib . term import BNode \n 
from rdflib . term import Literal \n 
from rdflib . term import URIRef \n 
from rdflib . term import Variable \n 
from rdflib . namespace import NamespaceManager \n 
from rdfextras . sparql import _questChar \n 
from rdfextras . sparql import SPARQLError \n 
from rdflib . util import check_object \n 
from rdflib . util import check_subject \n 
__all__ = [ , , ] \n 
class SPARQLGraph ( object ) : \n 
SPARQL_DATASET = 0 \n 
NAMED_GRAPH = 1 \n 
__slots__ = ( "graphVariable" , \n 
"DAWG_DATASET_COMPLIANCE" , \n 
"identifier" , \n 
"graphKind" , \n 
"graph" ) \n 
def __init__ ( self , graph , graphVariable = None , dSCompliance = False ) : \n 
~~~ assert not graphVariable or graphVariable [ 0 ] != , repr ( graphVariable ) \n 
self . graphVariable = graphVariable \n 
self . DAWG_DATASET_COMPLIANCE = dSCompliance \n 
self . graphKind = None \n 
if graph is not None : \n 
if isinstance ( graph , ConjunctiveGraph ) : \n 
~~~ self . graphKind = self . SPARQL_DATASET \n 
self . identifier = graph . default_context . identifier \n 
~~~ self . graphKind = self . NAMED_GRAPH \n 
self . identifier = graph . identifier \n 
~~ ~~ ~~ def setupGraph ( self , store , graphKind = None ) : \n 
~~~ gKind = graphKind and graphKind or self . graphKind \n 
self . graph = gKind ( store , self . identifier ) \n 
~~ def __reduce__ ( self ) : \n 
~~~ return ( SPARQLGraph , \n 
( None , \n 
self . graphVariable , \n 
self . DAWG_DATASET_COMPLIANCE ) , \n 
self . __getstate__ ( ) ) \n 
~~ def __getstate__ ( self ) : \n 
~~~ return ( self . graphVariable , \n 
self . DAWG_DATASET_COMPLIANCE , \n 
self . identifier ) #, \n 
~~ def __setstate__ ( self , arg ) : \n 
~~~ gVar , flag , identifier = arg \n 
self . graphVariable = gVar \n 
self . DAWG_DATASET_COMPLIANCE = flag \n 
self . identifier = identifier \n 
########################################################################## \n 
~~ def _clusterForward ( self , seed , Cluster ) : \n 
~~~ for ( p , o ) in self . graph . predicate_objects ( seed ) : \n 
~~~ if not ( seed , p , o ) in Cluster . graph : \n 
~~~ Cluster . add ( ( seed , p , o ) ) \n 
self . _clusterForward ( p , Cluster ) \n 
self . _clusterForward ( o , Cluster ) \n 
~~ ~~ ~~ except : \n 
~~ ~~ def clusterForward ( self , seed , Cluster = None ) : \n 
if Cluster == None : \n 
~~~ Cluster = SPARQLGraph ( ) \n 
self . _clusterForward ( seed , Cluster ) \n 
return Cluster \n 
~~ def _clusterBackward ( self , seed , Cluster ) : \n 
~~~ for ( s , p ) in self . graph . subject_predicates ( seed ) : \n 
~~~ if not ( s , p , seed ) in Cluster . graph : \n 
~~~ Cluster . add ( ( s , p , seed ) ) \n 
self . _clusterBackward ( s , Cluster ) \n 
self . _clusterBackward ( p , Cluster ) \n 
~~ ~~ def clusterBackward ( self , seed , Cluster = None ) : \n 
self . _clusterBackward ( seed , Cluster ) \n 
~~ def cluster ( self , seed ) : \n 
return self . clusterBackward ( seed ) + self . clusterForward ( seed ) \n 
def _createResource ( v ) : \n 
if isinstance ( v , Literal ) or isinstance ( v , BNode ) or isinstance ( v , URIRef ) : \n 
~~~ return v \n 
~~ ~~ def _isResQuest ( r ) : \n 
if r and isinstance ( r , basestring ) and r [ 0 ] == _questChar : \n 
~~~ return True \n 
~~ class GraphPattern : \n 
def __init__ ( self , patterns = [ ] ) : \n 
self . patterns = [ ] \n 
self . constraints = [ ] \n 
self . unbounds = [ ] \n 
self . bnodes = { } \n 
if type ( patterns ) == list : \n 
~~~ self . addPatterns ( patterns ) \n 
~~ elif type ( patterns ) == tuple : \n 
~~~ self . addPattern ( patterns ) \n 
~~~ raise SPARQLError ( \n 
~~ ~~ def _generatePattern ( self , tupl ) : \n 
if type ( tupl ) != tuple : \n 
~~ if len ( tupl ) != 3 and len ( tupl ) != 4 : \n 
~~ if len ( tupl ) == 3 : \n 
~~~ ( s , p , o ) = tupl \n 
f = None \n 
~~~ ( s , p , o , f ) = tupl \n 
~~ final = [ ] \n 
for c in ( s , p , o ) : \n 
~~~ if _isResQuest ( c ) : \n 
~~~ if not c in self . unbounds : \n 
~~~ self . unbounds . append ( c ) \n 
~~ final . append ( c ) \n 
~~ elif isinstance ( c , BNode ) : \n 
~~~ final . append ( c ) \n 
~~~ final . append ( _createResource ( c ) ) \n 
~~ ~~ final . append ( f ) \n 
return tuple ( final ) \n 
~~ def addPattern ( self , tupl ) : \n 
self . patterns . append ( self . _generatePattern ( tupl ) ) \n 
~~ def insertPattern ( self , tupl ) : \n 
self . patterns . insert ( 0 , self . _generatePattern ( tupl ) ) \n 
~~ def addPatterns ( self , lst ) : \n 
for l in lst : \n 
~~~ self . addPattern ( l ) \n 
~~ ~~ def insertPatterns ( self , lst ) : \n 
for i in xrange ( len ( lst ) - 1 , - 1 , - 1 ) : \n 
~~~ self . insertPattern ( lst [ i ] ) \n 
~~ ~~ def addConstraint ( self , func ) : \n 
if type ( func ) == FunctionType : \n 
~~~ self . constraints . append ( func ) \n 
~~ ~~ def addConstraints ( self , lst ) : \n 
~~~ self . addConstraint ( l ) \n 
~~ ~~ def construct ( self , tripleStore , bindings ) : \n 
localBnodes = { } \n 
for c in self . bnodes : \n 
~~~ localBnodes [ c ] = BNode ( ) \n 
~~ def bind ( st ) : \n 
~~~ if _isResQuest ( st ) : \n 
~~~ if st in bindings : \n 
~~~ return bindings [ st ] \n 
~~~ if isinstance ( self , GraphPattern ) : \n 
~~~ return st \n 
~~ ~~ ~~ elif isinstance ( st , BNode ) : \n 
~~~ for c in self . bnodes : \n 
~~~ if self . bnodes [ c ] == st : \n 
~~~ return localBnodes [ c ] \n 
~~ ~~ return st \n 
~~ ~~ for pattern in self . patterns : \n 
~~~ ( s , p , o , f ) = pattern \n 
triplet = [ ] \n 
valid = True \n 
for res in ( s , p , o ) : \n 
~~~ val = bind ( res ) \n 
if val != None : \n 
~~~ triplet . append ( val ) \n 
~~~ valid = False \n 
~~ ~~ if valid : \n 
~~~ tripleStore . add ( tuple ( triplet ) ) \n 
~~ ~~ ~~ def __add__ ( self , other ) : \n 
retval = GraphPattern ( ) \n 
retval += self \n 
retval += other \n 
return retval \n 
~~ def __iadd__ ( self , other ) : \n 
self . patterns += other . patterns \n 
self . constraints += other . constraints \n 
for c in other . unbounds : \n 
~~ ~~ for c in other . bnodes : \n 
~~~ if not c in self . bnodes : \n 
~~~ self . bnodes [ c ] = other . bnodes [ c ] \n 
~~ ~~ return self \n 
~~ def __str__ ( self ) : \n 
~~~ return self . __repr__ ( ) \n 
~~ def isEmpty ( self ) : \n 
return len ( self . patterns ) == 0 \n 
~~ ~~ class BasicGraphPattern ( GraphPattern ) : \n 
def __init__ ( self , patterns = [ ] , prolog = None ) : \n 
GraphPattern . __init__ ( self , patterns ) \n 
self . prolog = prolog \n 
~~ def canonicalTerm ( self , term ) : \n 
~~~ if isinstance ( term , URIRef ) : \n 
~~~ if self . prolog is not None : \n 
~~~ namespace_manager = NamespaceManager ( Graph ( ) ) \n 
for prefix , uri in self . prolog . prefixBindings . items ( ) : \n 
~~~ namespace_manager . bind ( prefix , uri , override = False ) \n 
~~~ prefix , uri , localName = namespace_manager . compute_qname ( term ) \n 
~~~ return term \n 
~~ if prefix not in self . prolog . prefixBindings : \n 
~~~ return . join ( [ prefix , localName ] ) \n 
~~ ~~ elif isinstance ( term , Literal ) : \n 
~~~ return term . n3 ( ) \n 
~~ elif isinstance ( term , BNode ) : \n 
~~~ assert isinstance ( term , Variable ) \n 
return term . n3 ( ) \n 
~~ ~~ def __repr__ ( self ) : \n 
~~~ if self . constraints : \n 
. join ( [ . join ( [ \n 
self . canonicalTerm ( pat [ 0 ] ) , \n 
self . canonicalTerm ( pat [ 1 ] ) , \n 
self . canonicalTerm ( pat [ 2 ] ) ] \n 
for pat in self . patterns ] ) ) \n 
~~~ return "BGP(%s)" % ( \n 
. join ( [ + . join ( [ \n 
self . canonicalTerm ( s ) , \n 
self . canonicalTerm ( p ) , \n 
self . canonicalTerm ( o ) ] \n 
for s , p , o , f in self . patterns ] ) ) \n 
~~ def _generatePattern ( self , tupl ) : \n 
~~~ if isinstance ( c , Variable ) : \n 
~~ def fetchTerminalExpression ( self ) : \n 
~~~ yield self \n 
~~~ from rdfextras . sparql . evaluate import Unbound \n 
v1 = Variable ( "a" ) \n 
u1 = Unbound ( "a" ) \n 
g = BasicGraphPattern ( \n 
[ ( "a" , "?b" , 24 ) , ( "?r" , "?c" , 12345 ) , ( v1 , "?c" , 3333 ) , ( u1 , "?c" , 9999 ) ] ) \n 
print g \n 
~~ from rdflib import ConjunctiveGraph , plugin \n 
from rdflib . store import Store \n 
from StringIO import StringIO \n 
class JSON ( unittest . TestCase ) : \n 
~~~ self . graph = ConjunctiveGraph ( plugin . get ( , Store ) ( ) ) \n 
self . graph . parse ( StringIO ( test_data ) , format = "n3" ) \n 
~~ def testComma ( self ) : \n 
results = self . graph . query ( test_query ) \n 
result_json = results . serialize ( format = ) \n 
self . failUnless ( result_json . find ( correct ) > 0 ) \n 
~~ def testHeader ( self ) : \n 
results = self . graph . query ( test_header_query ) \n 
self . failUnless ( result_json . find ( \'"x",\' ) == - 1 ) \n 
from rdflib import plugin \n 
from rdflib . namespace import Namespace , RDF , RDFS \n 
from cStringIO import StringIO \n 
from rdflib import Graph \n 
import rdflib \n 
~~~ set \n 
~~~ from sets import Set as set \n 
class AdvancedTests ( unittest . TestCase ) : \n 
~~~ memStore = plugin . get ( , Store ) ( ) \n 
self . testGraph = Graph ( memStore ) \n 
self . testGraph . parse ( StringIO ( testGraph1N3 ) , format = ) \n 
~~ def testNamedGraph ( self ) : \n 
~~~ OWL_NS = Namespace ( "http://www.w3.org/2002/07/owl#" ) \n 
rt = self . testGraph . query ( sparqlQ4 ) \n 
self . assertEquals ( set ( rt ) , set ( ( x , ) for x in [ OWL_NS . DatatypeProperty , OWL_NS . ObjectProperty , \n 
~~ def testScopedBNodes ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ1 ) \n 
self . assertEquals ( list ( rt ) [ 0 ] [ 0 ] , URIRef ( "http://test/foo" ) ) \n 
~~ def testCollectionContentWithinAndWithout ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ3 ) \n 
self . assertEquals ( list ( rt ) [ 0 ] [ 0 ] , URIRef ( "http://test/bar" ) ) \n 
~~ def testCollectionAsObject ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ2 ) \n 
self . assertEquals ( 1 , len ( rt ) ) \n 
~~~ suite = unittest . makeSuite ( AdvancedTests ) \n 
unittest . TextTestRunner ( verbosity = 3 ) . run ( suite ) \n 
~~ from urllib2 import URLError \n 
~~~ from Ft . Lib import UriException \n 
~~~ from urllib2 import URLError as UriException \n 
from rdflib import ConjunctiveGraph , URIRef \n 
class SPARQLloadContextsTest ( unittest . TestCase ) : \n 
~~~ def test_dSet_parsed_as_URL_raises_Exception ( self ) : \n 
graph = ConjunctiveGraph ( ) \n 
graph . get_context ( URIRef ( "http://test/" ) \n 
) . parse ( "http://www.w3.org/People/Berners-Lee/card.rdf" ) \n 
self . assertRaises ( ( URLError , UriException ) , \n 
graph . query , ( querystr ) , loadContexts = False ) \n 
~~ def test_dSet_parsed_as_context_returns_results ( self ) : \n 
graph . get_context ( URIRef ( ) \n 
r = graph . query ( querystr , loadContexts = True ) \n 
self . assert_ ( len ( r . bindings ) is not 0 ) \n 
from rdflib import Graph , RDF , RDFS , Literal \n 
from rdflib . namespace import FOAF \n 
if __name__ == : \n 
~~~ g = Graph ( ) \n 
bob = g . resource ( ) \n 
bob . set ( FOAF . name , Literal ( "Bob" ) ) \n 
bill = g . resource ( ) \n 
bill . add ( RDF . type , FOAF . Agent ) \n 
bill . set ( RDFS . label , Literal ( "Bill" ) ) \n 
bill . add ( FOAF . knows , bob ) \n 
for friend in bill [ FOAF . knows ] : \n 
for friend in bill [ FOAF . knows / FOAF . name ] : \n 
~~~ print friend \n 
~~ bill [ RDFS . label ] = Literal ( "William" ) \n 
print g . serialize ( format = ) \n 
from codecs import getreader \n 
from rdflib . py3compat import b \n 
from rdflib import ConjunctiveGraph \n 
from rdflib . plugins . parsers . ntriples import NTriplesParser \n 
from rdflib . plugins . parsers . ntriples import ParseError \n 
from rdflib . plugins . parsers . ntriples import r_tail \n 
from rdflib . plugins . parsers . ntriples import r_wspace \n 
from rdflib . plugins . parsers . ntriples import r_wspaces \n 
__all__ = [ ] \n 
class NQuadsParser ( NTriplesParser ) : \n 
~~~ def parse ( self , inputsource , sink , ** kwargs ) : \n 
self . sink = ConjunctiveGraph ( store = sink . store , identifier = sink . identifier ) \n 
source = inputsource . getByteStream ( ) \n 
if not hasattr ( source , ) : \n 
~~ source = getreader ( ) ( source ) \n 
self . file = source \n 
self . buffer = \n 
~~~ self . line = __line = self . readline ( ) \n 
if self . line is None : \n 
~~~ self . parseline ( ) \n 
~~ except ParseError , msg : \n 
~~ ~~ return self . sink \n 
~~ def parseline ( self ) : \n 
~~~ self . eat ( r_wspace ) \n 
if ( not self . line ) or self . line . startswith ( ( ) ) : \n 
~~ subject = self . subject ( ) \n 
self . eat ( r_wspace ) \n 
predicate = self . predicate ( ) \n 
obj = self . object ( ) \n 
context = self . uriref ( ) or self . nodeid ( ) or self . sink . identifier \n 
self . eat ( r_tail ) \n 
if self . line : \n 
~~ self . sink . get_context ( context ) . add ( ( subject , predicate , obj ) ) \n 
import codecs \n 
import csv \n 
from rdflib import Variable , BNode , URIRef , Literal , py3compat \n 
from rdflib . query import Result , ResultSerializer , ResultParser \n 
class CSVResultParser ( ResultParser ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . delim = "," \n 
~~ def parse ( self , source ) : \n 
~~~ r = Result ( ) \n 
if isinstance ( source . read ( 0 ) , py3compat . bytestype ) : \n 
~~~ source = codecs . getreader ( ) ( source ) \n 
~~ reader = csv . reader ( source , delimiter = self . delim ) \n 
r . vars = [ Variable ( x ) for x in reader . next ( ) ] \n 
r . bindings = [ ] \n 
for row in reader : \n 
~~~ r . bindings . append ( self . parseRow ( row , r . vars ) ) \n 
~~ return r \n 
~~ def parseRow ( self , row , v ) : \n 
~~~ return dict ( ( var , val ) \n 
for var , val in zip ( v , [ self . convertTerm ( t ) \n 
for t in row ] ) if val is not None ) \n 
~~ def convertTerm ( self , t ) : \n 
~~~ if t == "" : \n 
~~ if t . startswith ( "_:" ) : \n 
~~~ return URIRef ( t ) \n 
~~ return Literal ( t ) \n 
~~ ~~ class CSVResultSerializer ( ResultSerializer ) : \n 
~~~ def __init__ ( self , result ) : \n 
~~~ ResultSerializer . __init__ ( self , result ) \n 
self . delim = "," \n 
if result . type != "SELECT" : \n 
~~~ raise Exception ( \n 
~~ ~~ def serialize ( self , stream , encoding = ) : \n 
~~~ if py3compat . PY3 : \n 
~~~ import codecs \n 
stream = codecs . getwriter ( encoding ) ( stream ) \n 
~~ out = csv . writer ( stream , delimiter = self . delim ) \n 
vs = [ self . serializeTerm ( v , encoding ) for v in self . result . vars ] \n 
out . writerow ( vs ) \n 
for row in self . result . bindings : \n 
~~~ out . writerow ( [ self . serializeTerm ( \n 
row . get ( v ) , encoding ) for v in self . result . vars ] ) \n 
~~ ~~ def serializeTerm ( self , term , encoding ) : \n 
~~~ if term is None : \n 
~~~ return "" \n 
~~ if not py3compat . PY3 : \n 
~~~ return term . encode ( encoding ) \n 
NOSE_ARGS = [ \n 
COVERAGE_EXTRA_ARGS = [ \n 
DEFAULT_ATTRS = [ ] \n 
DEFAULT_DIRS = [ , ] \n 
~~~ from sys import argv , exit , stderr \n 
try : import nose \n 
except ImportError : \n 
~~ if in argv : \n 
~~~ try : import coverage \n 
argv . remove ( ) \n 
~~~ NOSE_ARGS += COVERAGE_EXTRA_ARGS \n 
~~ ~~ if True not in [ a . startswith ( ) or a . startswith ( ) for a in argv ] : \n 
~~~ argv . append ( + . join ( DEFAULT_ATTRS ) ) \n 
~~ if not [ a for a in argv [ 1 : ] if not a . startswith ( ) ] : \n 
~~~ argv += DEFAULT_DIRS \n 
~~ finalArgs = argv + NOSE_ARGS \n 
nose . run_exit ( argv = finalArgs ) \n 
~~ import sys \n 
from tempfile import mkdtemp , mkstemp \n 
from rdflib import RDF , RDFS , URIRef , BNode , Variable , plugin \n 
from rdflib . graph import QuotedGraph , ConjunctiveGraph \n 
implies = URIRef ( "http://www.w3.org/2000/10/swap/log#implies" ) \n 
from nose . tools import nottest \n 
from nose . exc import SkipTest \n 
def testFormulaStore ( store = "default" , configString = None ) : \n 
~~~ g = ConjunctiveGraph ( store = store ) \n 
~~ if configString : \n 
~~~ g . destroy ( configString ) \n 
g . open ( configString ) \n 
~~~ if store == : \n 
~~~ _ , path = mkstemp ( prefix = , dir = , suffix = ) \n 
g . open ( path , create = True ) \n 
~~~ g . open ( mkdtemp ( ) , create = True ) \n 
~~ ~~ g . parse ( data = testN3 , format = "n3" ) \n 
~~~ for s , p , o in g . triples ( ( None , implies , None ) ) : \n 
~~~ formulaA = s \n 
formulaB = o \n 
~~ assert type ( formulaA ) == QuotedGraph and type ( formulaB ) == QuotedGraph \n 
b = URIRef ( ) \n 
c = URIRef ( ) \n 
d = URIRef ( ) \n 
v = Variable ( ) \n 
universe = ConjunctiveGraph ( g . store ) \n 
assert len ( list ( universe . triples ( ( formulaA , implies , formulaB ) ) ) ) == 1 \n 
assert len ( list ( formulaB . triples ( ( None , None , v ) ) ) ) == 1 \n 
for s , p , o in formulaB . triples ( ( None , d , None ) ) : \n 
~~~ if o != c : \n 
~~~ assert isinstance ( o , Variable ) \n 
assert o == v \n 
~~ ~~ s = list ( universe . subjects ( RDF . type , RDFS . Class ) ) [ 0 ] \n 
assert isinstance ( s , BNode ) \n 
assert len ( list ( universe . triples ( ( None , implies , None ) ) ) ) == 1 \n 
assert len ( list ( universe . triples ( ( None , RDF . type , None ) ) ) ) == 1 \n 
assert len ( list ( formulaA . triples ( ( None , RDF . type , None ) ) ) ) == 1 \n 
assert len ( list ( formulaA . triples ( ( None , None , None ) ) ) ) == 2 \n 
assert len ( list ( formulaB . triples ( ( None , None , None ) ) ) ) == 2 \n 
assert len ( list ( universe . triples ( ( None , None , None ) ) ) ) == 3 \n 
assert len ( list ( formulaB . triples ( \n 
( None , URIRef ( ) , None ) ) ) ) == 2 \n 
assert len ( list ( universe . triples ( \n 
( None , URIRef ( ) , None ) ) ) ) == 1 \n 
universe . remove ( ( None , implies , None ) ) \n 
assert len ( list ( universe . triples ( ( None , implies , None ) ) ) ) == 0 \n 
formulaA . remove ( ( None , b , None ) ) \n 
assert len ( list ( formulaA . triples ( ( None , None , None ) ) ) ) == 1 \n 
formulaA . remove ( ( None , RDF . type , None ) ) \n 
assert len ( list ( formulaA . triples ( ( None , None , None ) ) ) ) == 0 \n 
universe . remove ( ( None , RDF . type , RDFS . Class ) ) \n 
universe . remove_context ( formulaB ) \n 
assert len ( list ( universe . triples ( ( None , RDF . type , None ) ) ) ) == 0 \n 
assert len ( universe ) == 1 \n 
assert len ( formulaB ) == 0 \n 
universe . remove ( ( None , None , None ) ) \n 
assert len ( universe ) == 0 \n 
g . close ( ) \n 
if store == : \n 
~~~ os . unlink ( path ) \n 
~~~ g . store . destroy ( configString ) \n 
~~ ~~ except : \n 
~~~ g . close ( ) \n 
~~ raise \n 
~~ ~~ def testFormulaStores ( ) : \n 
~~~ pluginname = None \n 
~~~ if len ( sys . argv ) > 1 : \n 
~~~ pluginname = sys . argv [ 1 ] \n 
~~ ~~ for s in plugin . plugins ( pluginname , plugin . Store ) : \n 
~~~ if s . name in ( \n 
~~ if not s . getClass ( ) . formula_aware : \n 
~~ yield testFormulaStore , s . name \n 
~~~ import nose \n 
nose . main ( defaultTest = sys . argv [ 0 ] ) \n 
~~ import rdflib \n 
def test_time_child_element ( ) : \n 
g = rdflib . Graph ( ) \n 
g . parse ( data = html , format = ) \n 
date = g . value ( \n 
rdflib . URIRef ( "http://example.com/" ) , \n 
rdflib . URIRef ( "http://schema.org/dateCreated" ) \n 
assert len ( g ) == 3 \n 
assert date == rdflib . term . Literal ( "2016-01-01" ) \n 
~~ from rdflib . term import URIRef , BNode \n 
from rdflib . namespace import RDFS \n 
from rdflib . plugins . serializers . rdfxml import XMLSerializer \n 
~~~ from io import BytesIO \n 
~~~ from StringIO import StringIO as BytesIO \n 
~~ class SerializerTestBase ( object ) : \n 
~~~ repeats = 8 \n 
def setup ( self ) : \n 
~~~ graph = ConjunctiveGraph ( ) \n 
graph . parse ( data = self . testContent , format = self . testContentFormat ) \n 
self . sourceGraph = graph \n 
~~ def test_serialize_and_reparse ( self ) : \n 
~~~ reparsedGraph = serialize_and_load ( self . sourceGraph , self . serializer ) \n 
_assert_equal_graphs ( self . sourceGraph , reparsedGraph ) \n 
~~ def test_multiple ( self ) : \n 
~~~ self . test_serialize_and_reparse ( ) \n 
~~ ~~ ~~ def _assert_equal_graphs ( g1 , g2 ) : \n 
g1copy = _mangled_copy ( g1 ) \n 
g2copy = _mangled_copy ( g2 ) \n 
g1copy -= _mangled_copy ( g2 ) \n 
g2copy -= _mangled_copy ( g1 ) \n 
~~ _blank = BNode ( ) \n 
def _mangled_copy ( g ) : \n 
gcopy = ConjunctiveGraph ( ) \n 
isbnode = lambda v : isinstance ( v , BNode ) \n 
for s , p , o in g : \n 
~~~ if isbnode ( s ) : s = _blank \n 
if isbnode ( p ) : p = _blank \n 
if isbnode ( o ) : o = _blank \n 
gcopy . add ( ( s , p , o ) ) \n 
~~ return gcopy \n 
~~ def serialize ( sourceGraph , makeSerializer , getValue = True , extra_args = { } ) : \n 
~~~ serializer = makeSerializer ( sourceGraph ) \n 
stream = BytesIO ( ) \n 
serializer . serialize ( stream , ** extra_args ) \n 
return getValue and stream . getvalue ( ) or stream \n 
~~ def serialize_and_load ( sourceGraph , makeSerializer ) : \n 
~~~ stream = serialize ( sourceGraph , makeSerializer , False ) \n 
stream . seek ( 0 ) \n 
reparsedGraph = ConjunctiveGraph ( ) \n 
reparsedGraph . load ( stream ) \n 
return reparsedGraph \n 
~~ class TestXMLSerializer ( SerializerTestBase ) : \n 
~~~ serializer = XMLSerializer \n 
testContentFormat = \n 
def test_result_fragments ( self ) : \n 
~~~ rdfXml = serialize ( self . sourceGraph , self . serializer ) \n 
~~ def test_result_fragments_with_base ( self ) : \n 
~~~ rdfXml = serialize ( self . sourceGraph , self . serializer , \n 
extra_args = { : "http://example.org/" , : "http://example.org/" } ) \n 
assert b ( \'xml:base="http://example.org/"\' ) in rdfXml \n 
~~ def test_subClassOf_objects ( self ) : \n 
_assert_expected_object_types_for_predicates ( reparsedGraph , \n 
[ RDFS . seeAlso , RDFS . subClassOf ] , \n 
[ URIRef , BNode ] ) \n 
~~ ~~ def _assert_expected_object_types_for_predicates ( graph , predicates , types ) : \n 
~~~ for s , p , o in graph : \n 
~~~ if p in predicates : \n 
~~~ someTrue = [ isinstance ( o , t ) for t in types ] \n 
~~ ~~ ~~ from __future__ import division \n 
import random as rd \n 
from . import common_args \n 
from . . util import scale_samples , read_param_file , compute_groups_matrix \n 
from . optimal_trajectories import return_max_combo \n 
from . morris_util import * \n 
from operator import or_ \n 
~~~ from gurobipy import * \n 
~~~ _has_gurobi = False \n 
~~~ _has_gurobi = True \n 
~~ def sample ( problem , N , num_levels , grid_jump , optimal_trajectories = None , local_optimization = False ) : \n 
if grid_jump >= num_levels : \n 
~~ if problem . get ( ) : \n 
~~~ sample = sample_groups ( problem , N , num_levels , grid_jump ) \n 
~~~ sample = sample_oat ( problem , N , num_levels , grid_jump ) \n 
~~ if optimal_trajectories : \n 
if optimal_trajectories < 2 : \n 
~~ if _has_gurobi == False and local_optimization == False and optimal_trajectories > 10 : \n 
~~ sample = compute_optimised_trajectories ( problem , \n 
sample , \n 
N , \n 
optimal_trajectories , \n 
local_optimization ) \n 
~~ scale_samples ( sample , problem [ ] ) \n 
return sample \n 
~~ def sample_oat ( problem , N , num_levels , grid_jump ) : \n 
~~~ D = problem [ ] \n 
B = np . tril ( np . ones ( [ D + 1 , D ] , dtype = int ) , - 1 ) + np . triu ( - 1 * np . ones ( [ D + 1 , D ] , dtype = int ) ) \n 
delta = grid_jump / ( num_levels - 1 ) \n 
X = np . empty ( [ N * ( D + 1 ) , D ] ) \n 
for j in range ( N ) : \n 
~~~ DM = np . diag ( [ rd . choice ( [ - 1 , 1 ] ) for _ in range ( D ) ] ) \n 
perm = np . random . permutation ( D ) \n 
P = np . zeros ( [ D , D ] ) \n 
for i in range ( D ) : \n 
~~~ P [ i , perm [ i ] ] = 1 \n 
~~ x_base = np . empty ( [ D + 1 , D ] ) \n 
~~~ x_base [ : , i ] = ( \n 
rd . choice ( np . arange ( num_levels - grid_jump ) ) ) / ( num_levels - 1 ) \n 
~~ index_list = np . arange ( D + 1 ) + j * ( D + 1 ) \n 
delta_diag = np . diag ( [ delta for _ in range ( D ) ] ) \n 
X [ index_list , : ] = 0.5 * ( np . mat ( B ) * np . mat ( P ) * np . mat ( DM ) + 1 ) * np . mat ( delta_diag ) + np . mat ( x_base ) \n 
~~ def sample_groups ( problem , N , num_levels , grid_jump ) : \n 
G , group_names = compute_groups_matrix ( problem [ ] , problem [ ] ) \n 
if G is None : \n 
~~ if type ( G ) is not np . matrixlib . defmatrix . matrix : \n 
~~ k = G . shape [ 0 ] \n 
g = G . shape [ 1 ] \n 
sample = np . empty ( ( N * ( g + 1 ) , k ) ) \n 
sample = np . array ( [ generate_trajectory ( G , num_levels , grid_jump ) for n in range ( N ) ] ) \n 
return sample . reshape ( ( N * ( g + 1 ) , k ) ) \n 
~~ def compute_optimised_trajectories ( problem , input_sample , N , k_choices , local_optimization = False ) : ~~~ \n 
num_params = problem [ ] \n 
groups = compute_groups_matrix ( problem [ ] , num_params ) \n 
if np . any ( ( input_sample < 0 ) | ( input_sample > 1 ) ) : \n 
~~ if _has_gurobi == True and local_optimization == False : \n 
~~~ maximum_combo = return_max_combo ( input_sample , \n 
num_params , \n 
k_choices , \n 
groups ) \n 
~~~ maximum_combo = find_optimum_combination ( input_sample , \n 
groups , \n 
~~ num_groups = None \n 
if groups is not None : \n 
~~~ num_groups = groups [ 0 ] . shape [ 1 ] \n 
~~ output = compile_output ( input_sample , \n 
maximum_combo , \n 
num_groups ) \n 
return output \n 
~~~ parser = common_args . create ( ) \n 
parser . add_argument ( \n 
, , type = int , required = True , help = ) \n 
parser . add_argument ( , , type = int , required = False , \n 
default = 4 , help = ) \n 
parser . add_argument ( , type = int , required = False , \n 
default = 2 , help = ) \n 
default = None , help = ) \n 
parser . add_argument ( , , type = bool , required = True , \n 
default = False , help = args = parser . parse_args ( ) \n 
np . random . seed ( args . seed ) \n 
rd . seed ( args . seed ) \n 
problem = read_param_file ( args . paramfile ) \n 
param_values = sample ( problem , args . samples , args . levels , args . grid_jump , args . k_optimal , args . local ) \n 
np . savetxt ( args . output , param_values , delimiter = args . delimiter , \n 
fmt = + str ( args . precision ) + ) \n 
from distutils . command . build import build as _build \n 
from distutils . command . sdist import sdist as _sdist \n 
from distutils . core import Command \n 
import os , sys , re , subprocess , errno \n 
versionfile_source = None \n 
versionfile_build = None \n 
tag_prefix = None \n 
parentdir_prefix = None \n 
VCS = None \n 
LONG_VERSION_PY = { } \n 
def run_command ( commands , args , cwd = None , verbose = False , hide_stderr = False ) : \n 
~~~ assert isinstance ( commands , list ) \n 
p = None \n 
for c in commands : \n 
~~~ p = subprocess . Popen ( [ c ] + args , cwd = cwd , stdout = subprocess . PIPE , \n 
stderr = ( subprocess . PIPE if hide_stderr \n 
else None ) ) \n 
~~ except EnvironmentError : \n 
~~~ e = sys . exc_info ( ) [ 1 ] \n 
if e . errno == errno . ENOENT : \n 
print ( e ) \n 
~~ return None \n 
~~ stdout = p . communicate ( ) [ 0 ] . strip ( ) \n 
if sys . version >= : \n 
~~~ stdout = stdout . decode ( ) \n 
~~ if p . returncode != 0 : \n 
~~ return stdout \n 
def git_get_keywords ( versionfile_abs ) : \n 
~~~ keywords = { } \n 
~~~ f = open ( versionfile_abs , "r" ) \n 
for line in f . readlines ( ) : \n 
~~~ mo = re . search ( r\'=\\s*"(.*)"\' , line ) \n 
if mo : \n 
~~~ keywords [ "refnames" ] = mo . group ( 1 ) \n 
~~~ keywords [ "full" ] = mo . group ( 1 ) \n 
~~ return keywords \n 
~~ def git_versions_from_keywords ( keywords , tag_prefix , verbose = False ) : \n 
~~~ if not keywords : \n 
~~ refnames = keywords [ "refnames" ] . strip ( ) \n 
if refnames . startswith ( "$Format" ) : \n 
~~ refs = set ( [ r . strip ( ) for r in refnames . strip ( "()" ) . split ( "," ) ] ) \n 
tags = set ( [ r [ len ( TAG ) : ] for r in refs if r . startswith ( TAG ) ] ) \n 
if not tags : \n 
~~~ tags = set ( [ r for r in refs if re . search ( , r ) ] ) \n 
~~ ~~ if verbose : \n 
~~ for ref in sorted ( tags ) : \n 
~~~ if ref . startswith ( tag_prefix ) : \n 
~~~ r = ref [ len ( tag_prefix ) : ] \n 
~~ return { "version" : r , \n 
"full" : keywords [ "full" ] . strip ( ) } \n 
~~ return { "version" : keywords [ "full" ] . strip ( ) , \n 
~~ def git_versions_from_vcs ( tag_prefix , root , verbose = False ) : \n 
~~~ if not os . path . exists ( os . path . join ( root , ".git" ) ) : \n 
~~ GITS = [ "git" ] \n 
if sys . platform == "win32" : \n 
~~~ GITS = [ "git.cmd" , "git.exe" ] \n 
~~ stdout = run_command ( GITS , [ "describe" , "--tags" , "--dirty" , "--always" ] , \n 
cwd = root ) \n 
if stdout is None : \n 
~~~ return { } \n 
~~ if not stdout . startswith ( tag_prefix ) : \n 
~~ tag = stdout [ len ( tag_prefix ) : ] \n 
stdout = run_command ( GITS , [ "rev-parse" , "HEAD" ] , cwd = root ) \n 
~~ full = stdout . strip ( ) \n 
if tag . endswith ( "-dirty" ) : \n 
~~~ full += "-dirty" \n 
~~ return { "version" : tag , "full" : full } \n 
~~ def do_vcs_install ( manifest_in , versionfile_source , ipy ) : \n 
~~~ GITS = [ "git" ] \n 
~~ files = [ manifest_in , versionfile_source ] \n 
if ipy : \n 
~~~ files . append ( ipy ) \n 
~~~ me = __file__ \n 
if me . endswith ( ".pyc" ) or me . endswith ( ".pyo" ) : \n 
~~~ me = os . path . splitext ( me ) [ 0 ] + ".py" \n 
~~ versioneer_file = os . path . relpath ( me ) \n 
~~~ versioneer_file = "versioneer.py" \n 
~~ files . append ( versioneer_file ) \n 
present = False \n 
~~~ f = open ( ".gitattributes" , "r" ) \n 
~~~ if line . strip ( ) . startswith ( versionfile_source ) : \n 
~~~ if "export-subst" in line . strip ( ) . split ( ) [ 1 : ] : \n 
~~~ present = True \n 
~~ if not present : \n 
~~~ f = open ( ".gitattributes" , "a+" ) \n 
files . append ( ".gitattributes" ) \n 
~~ run_command ( GITS , [ "add" , "--" ] + files ) \n 
~~ def versions_from_parentdir ( parentdir_prefix , root , verbose = False ) : \n 
~~~ dirname = os . path . basename ( root ) \n 
if not dirname . startswith ( parentdir_prefix ) : \n 
( root , dirname , parentdir_prefix ) ) \n 
~~ return { "version" : dirname [ len ( parentdir_prefix ) : ] , "full" : "" } \n 
DEFAULT = { "version" : "unknown" , "full" : "unknown" } \n 
def versions_from_file ( filename ) : \n 
~~~ versions = { } \n 
~~~ with open ( filename ) as f : \n 
~~~ for line in f . readlines ( ) : \n 
~~~ versions [ "version" ] = mo . group ( 1 ) \n 
~~~ versions [ "full" ] = mo . group ( 1 ) \n 
~~ ~~ ~~ ~~ except EnvironmentError : \n 
~~ return versions \n 
~~ def write_to_version_file ( filename , versions ) : \n 
~~~ with open ( filename , "w" ) as f : \n 
~~~ f . write ( SHORT_VERSION_PY % versions ) \n 
~~ def get_root ( ) : \n 
~~~ return os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
~~~ return os . path . dirname ( os . path . abspath ( sys . argv [ 0 ] ) ) \n 
~~ ~~ def vcs_function ( vcs , suffix ) : \n 
~~~ return getattr ( sys . modules [ __name__ ] , % ( vcs , suffix ) , None ) \n 
~~ def get_versions ( default = DEFAULT , verbose = False ) : \n 
root = get_root ( ) \n 
versionfile_abs = os . path . join ( root , versionfile_source ) \n 
get_keywords_f = vcs_function ( VCS , "get_keywords" ) \n 
versions_from_keywords_f = vcs_function ( VCS , "versions_from_keywords" ) \n 
if get_keywords_f and versions_from_keywords_f : \n 
~~~ vcs_keywords = get_keywords_f ( versionfile_abs ) \n 
ver = versions_from_keywords_f ( vcs_keywords , tag_prefix ) \n 
if ver : \n 
return ver \n 
~~ ~~ ver = versions_from_file ( versionfile_abs ) \n 
~~ versions_from_vcs_f = vcs_function ( VCS , "versions_from_vcs" ) \n 
if versions_from_vcs_f : \n 
~~~ ver = versions_from_vcs_f ( tag_prefix , root , verbose ) \n 
~~ ~~ ver = versions_from_parentdir ( parentdir_prefix , root , verbose ) \n 
return default \n 
~~ def get_version ( verbose = False ) : \n 
~~~ return get_versions ( verbose = verbose ) [ "version" ] \n 
~~ class cmd_version ( Command ) : \n 
user_options = [ ] \n 
boolean_options = [ ] \n 
def initialize_options ( self ) : \n 
~~ def finalize_options ( self ) : \n 
~~ def run ( self ) : \n 
~~~ ver = get_version ( verbose = True ) \n 
~~ ~~ class cmd_build ( _build ) : \n 
~~~ def run ( self ) : \n 
~~~ versions = get_versions ( verbose = True ) \n 
_build . run ( self ) \n 
if versionfile_build : \n 
~~~ target_versionfile = os . path . join ( self . build_lib , versionfile_build ) \n 
os . unlink ( target_versionfile ) \n 
with open ( target_versionfile , "w" ) as f : \n 
~~~ from cx_Freeze . dist import build_exe as _build_exe \n 
class cmd_build_exe ( _build_exe ) : \n 
target_versionfile = versionfile_source \n 
~~ _build_exe . run ( self ) \n 
with open ( versionfile_source , "w" ) as f : \n 
LONG = LONG_VERSION_PY [ VCS ] \n 
f . write ( LONG % { "DOLLAR" : "$" , \n 
"TAG_PREFIX" : tag_prefix , \n 
"PARENTDIR_PREFIX" : parentdir_prefix , \n 
"VERSIONFILE_SOURCE" : versionfile_source , \n 
} ) \n 
~~ ~~ ~~ ~~ class cmd_sdist ( _sdist ) : \n 
self . _versioneer_generated_versions = versions \n 
self . distribution . metadata . version = versions [ "version" ] \n 
return _sdist . run ( self ) \n 
~~ def make_release_tree ( self , base_dir , files ) : \n 
~~~ _sdist . make_release_tree ( self , base_dir , files ) \n 
target_versionfile = os . path . join ( base_dir , versionfile_source ) \n 
~~~ f . write ( SHORT_VERSION_PY % self . _versioneer_generated_versions ) \n 
class cmd_update_files ( Command ) : \n 
~~ ipy = os . path . join ( os . path . dirname ( versionfile_source ) , "__init__.py" ) \n 
if os . path . exists ( ipy ) : \n 
~~~ with open ( ipy , "r" ) as f : \n 
~~~ old = f . read ( ) \n 
~~ ~~ except EnvironmentError : \n 
~~~ old = "" \n 
~~ if INIT_PY_SNIPPET not in old : \n 
with open ( ipy , "a" ) as f : \n 
~~~ f . write ( INIT_PY_SNIPPET ) \n 
ipy = None \n 
~~ manifest_in = os . path . join ( get_root ( ) , "MANIFEST.in" ) \n 
simple_includes = set ( ) \n 
~~~ with open ( manifest_in , "r" ) as f : \n 
~~~ for include in line . split ( ) [ 1 : ] : \n 
~~~ simple_includes . add ( include ) \n 
~~ ~~ ~~ ~~ ~~ except EnvironmentError : \n 
~~ if "versioneer.py" not in simple_includes : \n 
with open ( manifest_in , "a" ) as f : \n 
~~ if versionfile_source not in simple_includes : \n 
versionfile_source ) \n 
~~ do_vcs_install ( manifest_in , versionfile_source , ipy ) \n 
~~ ~~ def get_cmdclass ( ) : \n 
~~~ cmds = { : cmd_version , \n 
: cmd_update_files , \n 
: cmd_build , \n 
: cmd_sdist , \n 
~~~ cmds [ ] = cmd_build_exe \n 
del cmds [ ] \n 
~~ return cmds \n 
~~ from paramz import Param \n 
from . priorizable import Priorizable \n 
from paramz . transformations import __fixed__ \n 
import logging , numpy as np \n 
class Param ( Param , Priorizable ) : \n 
from ... util . linalg import pdinv \n 
from . posterior import Posterior \n 
from . import LatentFunctionInference \n 
log_2_pi = np . log ( 2 * np . pi ) \n 
class VarGauss ( LatentFunctionInference ) : \n 
def __init__ ( self , alpha , beta ) : \n 
self . alpha , self . beta = alpha , beta \n 
~~ def inference ( self , kern , X , likelihood , Y , mean_function = None , Y_metadata = None , Z = None ) : \n 
~~~ if mean_function is not None : \n 
~~~ raise NotImplementedError \n 
~~ num_data , output_dim = Y . shape \n 
K = kern . K ( X ) \n 
m = K . dot ( self . alpha ) \n 
KB = K * self . beta [ : , None ] \n 
BKB = KB * self . beta [ None , : ] \n 
A = np . eye ( num_data ) + BKB \n 
Ai , LA , _ , Alogdet = pdinv ( A ) \n 
F , dF_dm , dF_dv , dF_dthetaL = likelihood . variational_expectations ( Y , m , var , Y_metadata = Y_metadata if dF_dthetaL is not None : \n 
~~~ dL_dthetaL = dF_dthetaL . sum ( 1 ) . sum ( 1 ) \n 
~~~ dL_dthetaL = np . array ( [ ] ) \n 
~~ dF_da = np . dot ( K , dF_dm ) \n 
SigmaB = Sigma * self . beta \n 
dF_db = - 2 * np . sum ( Sigma ** 2 * ( dF_dv * self . beta ) , 0 ) \n 
KL = 0.5 * ( Alogdet + np . trace ( Ai ) - num_data + np . sum ( m * self . alpha ) ) \n 
dKL_da = m \n 
A_A2 = Ai - Ai . dot ( Ai ) \n 
dKL_db = np . diag ( np . dot ( KB . T , A_A2 ) ) \n 
log_marginal = F . sum ( ) - KL \n 
self . alpha . gradient = dF_da - dKL_da \n 
self . beta . gradient = dF_db - dKL_db \n 
dKL_dK = 0.5 * ( self . alpha * self . alpha . T + self . beta [ : , None ] * self . beta [ None , : ] * A_A2 ) \n 
tmp = Ai * self . beta [ : , None ] / self . beta [ None , : ] \n 
dF_dK = self . alpha * dF_dm . T + np . dot ( tmp * dF_dv , tmp . T ) \n 
return Posterior ( mean = m , cov = Sigma , K = K ) , log_marginal , { : dF_dK - dKL_dK , : dL_dthetaL } \n 
~~ ~~ import numpy as np \n 
from . kern import Kern \n 
from ... util . linalg import mdot \n 
from ... util . decorators import silence_errors \n 
from ... core . parameterization . param import Param \n 
from paramz . transformations import Logexp \n 
class Periodic ( Kern ) : \n 
super ( Periodic , self ) . __init__ ( input_dim , active_dims , name ) \n 
self . input_dim = input_dim \n 
self . lower , self . upper = lower , upper \n 
self . n_freq = n_freq \n 
self . n_basis = 2 * n_freq \n 
self . variance = Param ( , np . float64 ( variance ) , Logexp ( ) ) \n 
self . lengthscale = Param ( , np . float64 ( lengthscale ) , Logexp ( ) ) \n 
self . period = Param ( , np . float64 ( period ) , Logexp ( ) ) \n 
self . link_parameters ( self . variance , self . lengthscale , self . period ) \n 
~~ def _cos ( self , alpha , omega , phase ) : \n 
~~~ def f ( x ) : \n 
~~~ return alpha * np . cos ( omega * x + phase ) \n 
~~ return f \n 
~~ @ silence_errors \n 
def _cos_factorization ( self , alpha , omega , phase ) : \n 
~~~ r1 = np . sum ( alpha * np . cos ( phase ) , axis = 1 ) [ : , None ] \n 
r2 = np . sum ( alpha * np . sin ( phase ) , axis = 1 ) [ : , None ] \n 
r = np . sqrt ( r1 ** 2 + r2 ** 2 ) \n 
psi = np . where ( r1 != 0 , ( np . arctan ( r2 / r1 ) + ( r1 < 0. ) * np . pi ) , np . arcsin ( r2 ) ) \n 
return r , omega [ : , 0 : 1 ] , psi \n 
def _int_computation ( self , r1 , omega1 , phi1 , r2 , omega2 , phi2 ) : \n 
~~~ Gint1 = 1. / ( omega1 + omega2 . T ) * ( np . sin ( ( omega1 + omega2 . T ) * self . upper + phi1 + phi2 . T ) - np . sin ( ( omega1 Gint2 = 1. / ( omega1 + omega2 . T ) * ( np . sin ( ( omega1 + omega2 . T ) * self . upper + phi1 + phi2 . T ) - np . sin ( ( omega1 Gint = np . dot ( r1 , r2 . T ) / 2 * np . where ( np . isnan ( Gint1 ) , Gint2 , Gint1 ) \n 
return Gint \n 
~~ def K ( self , X , X2 = None ) : \n 
~~~ FX = self . _cos ( self . basis_alpha [ None , : ] , self . basis_omega [ None , : ] , self . basis_phi [ None , : ] ) ( X ) \n 
if X2 is None : \n 
~~~ FX2 = FX \n 
~~~ FX2 = self . _cos ( self . basis_alpha [ None , : ] , self . basis_omega [ None , : ] , self . basis_phi [ None , : ] ~~ return mdot ( FX , self . Gi , FX2 . T ) \n 
~~ def Kdiag ( self , X ) : \n 
~~~ return np . diag ( self . K ( X ) ) \n 
~~ ~~ class PeriodicExponential ( Periodic ) : \n 
def __init__ ( self , input_dim = 1 , variance = 1. , lengthscale = 1. , period = 2. * np . pi , n_freq = 10 , lower = 0. ~~~ super ( PeriodicExponential , self ) . __init__ ( input_dim , variance , lengthscale , period , n_freq , \n 
~~ def parameters_changed ( self ) : \n 
~~~ self . a = [ 1. / self . lengthscale , 1. ] \n 
self . b = [ 1 ] \n 
self . basis_alpha = np . ones ( ( self . n_basis , ) ) \n 
self . basis_omega = ( 2 * np . pi * np . arange ( 1 , self . n_freq + 1 ) / self . period ) . repeat ( 2 ) \n 
self . basis_phi = np . zeros ( self . n_freq * 2 ) \n 
self . basis_phi [ : : 2 ] = - np . pi / 2 \n 
self . G = self . Gram_matrix ( ) \n 
self . Gi = np . linalg . inv ( self . G ) \n 
~~ def Gram_matrix ( self ) : \n 
~~~ La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega ) ) \n 
Lo = np . column_stack ( ( self . basis_omega , self . basis_omega ) ) \n 
Lp = np . column_stack ( ( self . basis_phi , self . basis_phi + np . pi / 2 ) ) \n 
r , omega , phi = self . _cos_factorization ( La , Lo , Lp ) \n 
Gint = self . _int_computation ( r , omega , phi , r , omega , phi ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : return ( self . lengthscale / ( 2 * self . variance ) * Gint + 1. / self . variance * np . dot ( Flower , Flower . T ) ) \n 
def update_gradients_full ( self , dL_dK , X , X2 = None ) : \n 
FX = self . _cos ( self . basis_alpha [ None , : ] , self . basis_omega [ None , : ] , self . basis_phi [ None , : ] ) ( X ) FX2 = self . _cos ( self . basis_alpha [ None , : ] , self . basis_omega [ None , : ] , self . basis_phi [ None , : ] ) ( X2 \n 
La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega ) ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : \n 
#dK_dvar \n 
dK_dvar = 1. / self . variance * mdot ( FX , self . Gi , FX2 . T ) \n 
#dK_dlen \n 
da_dlen = [ - 1. / self . lengthscale ** 2 , 0. ] \n 
dLa_dlen = np . column_stack ( ( da_dlen [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , da_dlen [ 1 ] * self . basis_omega r1 , omega1 , phi1 = self . _cos_factorization ( dLa_dlen , Lo , Lp ) \n 
dGint_dlen = self . _int_computation ( r1 , omega1 , phi1 , r , omega , phi ) \n 
dGint_dlen = dGint_dlen + dGint_dlen . T \n 
dG_dlen = 1. / 2 * Gint + self . lengthscale / 2 * dGint_dlen \n 
dK_dlen = - mdot ( FX , self . Gi , dG_dlen / self . variance , self . Gi , FX2 . T ) \n 
#dK_dper \n 
dFX_dper = self . _cos ( - self . basis_alpha [ None , : ] * self . basis_omega [ None , : ] / self . period * X , self dFX2_dper = self . _cos ( - self . basis_alpha [ None , : ] * self . basis_omega [ None , : ] / self . period * X2 , self \n 
dLa_dper = np . column_stack ( ( - self . a [ 0 ] * self . basis_omega / self . period , - self . a [ 1 ] * self . basis_omega dLp_dper = np . column_stack ( ( self . basis_phi + np . pi / 2 , self . basis_phi + np . pi ) ) \n 
r1 , omega1 , phi1 = self . _cos_factorization ( dLa_dper , Lo , dLp_dper ) \n 
IPPint = np . where ( np . isnan ( IPPint1 ) , IPPint2 , IPPint1 ) \n 
dLa_dper2 = np . column_stack ( ( - self . a [ 1 ] * self . basis_omega / self . period ) ) \n 
dLp_dper2 = np . column_stack ( ( self . basis_phi + np . pi / 2 ) ) \n 
r2 , omega2 , phi2 = dLa_dper2 . T , Lo [ : , 0 : 1 ] , dLp_dper2 . T \n 
dGint_dper = np . dot ( r , r1 . T ) / 2 * ( IPPprim - IPPint ) + self . _int_computation ( r2 , omega2 , phi2 , r dGint_dper = dGint_dper + dGint_dper . T \n 
dFlower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega / self . period \n 
dG_dper = 1. / self . variance * ( self . lengthscale / 2 * dGint_dper + self . b [ 0 ] * ( np . dot ( dFlower_dper , Flower \n 
dK_dper = mdot ( dFX_dper , self . Gi , FX2 . T ) - mdot ( FX , self . Gi , dG_dper , self . Gi , FX2 . T ) + mdot ( FX , self \n 
self . variance . gradient = np . sum ( dK_dvar * dL_dK ) \n 
self . lengthscale . gradient = np . sum ( dK_dlen * dL_dK ) \n 
self . period . gradient = np . sum ( dK_dper * dL_dK ) \n 
~~ ~~ class PeriodicMatern32 ( Periodic ) : \n 
def __init__ ( self , input_dim = 1 , variance = 1. , lengthscale = 1. , period = 2. * np . pi , n_freq = 10 , lower = 0. ~~~ super ( PeriodicMatern32 , self ) . __init__ ( input_dim , variance , lengthscale , period , n_freq , lower ~~ def parameters_changed ( self ) : \n 
~~~ self . a = [ 3. / self . lengthscale ** 2 , 2 * np . sqrt ( 3 ) / self . lengthscale , 1. ] \n 
self . b = [ 1 , self . lengthscale ** 2 / 3 ] \n 
~~~ La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega , self . a [ Lo = np . column_stack ( ( self . basis_omega , self . basis_omega , self . basis_omega ) ) \n 
Lp = np . column_stack ( ( self . basis_phi , self . basis_phi + np . pi / 2 , self . basis_phi + np . pi ) ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : F1lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega , self . basis_omega , self . basis_phi return ( self . lengthscale ** 3 / ( 12 * np . sqrt ( 3 ) * self . variance ) * Gint + 1. / self . variance * np . dot ( Flower \n 
def update_gradients_full ( self , dL_dK , X , X2 ) : \n 
La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega , self . a [ Lo = np . column_stack ( ( self . basis_omega , self . basis_omega , self . basis_omega ) ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : F1lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega , self . basis_omega , self . basis_phi \n 
da_dlen = [ - 6 / self . lengthscale ** 3 , - 2 * np . sqrt ( 3 ) / self . lengthscale ** 2 , 0. ] \n 
db_dlen = [ 0. , 2 * self . lengthscale / 3. ] \n 
dG_dlen = self . lengthscale ** 2 / ( 4 * np . sqrt ( 3 ) ) * Gint + self . lengthscale ** 3 / ( 12 * np . sqrt ( 3 ) ) * dGint_dlen dK_dlen = - mdot ( FX , self . Gi , dG_dlen / self . variance , self . Gi , FX2 . T ) \n 
dLa_dper = np . column_stack ( ( - self . a [ 0 ] * self . basis_omega / self . period , - self . a [ 1 ] * self . basis_omega dLp_dper = np . column_stack ( ( self . basis_phi + np . pi / 2 , self . basis_phi + np . pi , self . basis_phi + np . pi r1 , omega1 , phi1 = self . _cos_factorization ( dLa_dper , Lo , dLp_dper ) \n 
IPPprim1 = self . upper * ( 1. / ( omega + omega1 . T ) * np . cos ( ( omega + omega1 . T ) * self . upper + phi + phi1 . T - np IPPprim1 -= self . lower * ( 1. / ( omega + omega1 . T ) * np . cos ( ( omega + omega1 . T ) * self . lower + phi + phi1 . T - np IPPprim2 = self . upper * ( 1. / ( omega + omega1 . T ) * np . cos ( ( omega + omega1 . T ) * self . upper + phi + phi1 . T - np IPPprim2 -= self . lower * ( 1. / ( omega + omega1 . T ) * np . cos ( ( omega + omega1 . T ) * self . lower + phi + phi1 . T - np IPPprim = np . where ( np . isnan ( IPPprim1 ) , IPPprim2 , IPPprim1 ) \n 
IPPint1 = 1. / ( omega + omega1 . T ) ** 2 * np . cos ( ( omega + omega1 . T ) * self . upper + phi + phi1 . T - np . pi ) + 1. IPPint1 -= 1. / ( omega + omega1 . T ) ** 2 * np . cos ( ( omega + omega1 . T ) * self . lower + phi + phi1 . T - np . pi ) + 1. IPPint2 = 1. / ( omega + omega1 . T ) ** 2 * np . cos ( ( omega + omega1 . T ) * self . upper + phi + phi1 . T - np . pi ) + 1. IPPint2 -= 1. / ( omega + omega1 . T ) ** 2 * np . cos ( ( omega + omega1 . T ) * self . lower + phi + phi1 . T - np . pi ) + 1. IPPint = np . where ( np . isnan ( IPPint1 ) , IPPint2 , IPPint1 ) \n 
dLa_dper2 = np . column_stack ( ( - self . a [ 1 ] * self . basis_omega / self . period , - 2 * self . a [ 2 ] * self . basis_omega dLp_dper2 = np . column_stack ( ( self . basis_phi + np . pi / 2 , self . basis_phi + np . pi ) ) \n 
r2 , omega2 , phi2 = self . _cos_factorization ( dLa_dper2 , Lo [ : , 0 : 2 ] , dLp_dper2 ) \n 
dGint_dper = np . dot ( r , r1 . T ) / 2 * ( IPPprim - IPPint ) + self . _int_computation ( r2 , omega2 , phi2 , dGint_dper = dGint_dper + dGint_dper . T \n 
dFlower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega / self . period dF1lower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega ** 2 / self . period \n 
dG_dper = 1. / self . variance * ( self . lengthscale ** 3 / ( 12 * np . sqrt ( 3 ) ) * dGint_dper + self . b [ 0 ] * ( np . dot \n 
~~ ~~ class PeriodicMatern52 ( Periodic ) : \n 
def __init__ ( self , input_dim = 1 , variance = 1. , lengthscale = 1. , period = 2. * np . pi , n_freq = 10 , lower = 0. ~~~ super ( PeriodicMatern52 , self ) . __init__ ( input_dim , variance , lengthscale , period , n_freq , lower \n 
~~~ self . a = [ 5 * np . sqrt ( 5 ) / self . lengthscale ** 3 , 15. / self . lengthscale ** 2 , 3 * np . sqrt ( 5 ) / self . lengthscale self . b = [ 9. / 8 , 9 * self . lengthscale ** 4 / 200. , 3 * self . lengthscale ** 2 / 5. , 3 * self . lengthscale ** 2 \n 
self . basis_alpha = np . ones ( ( 2 * self . n_freq , ) ) \n 
~~~ La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega , self . Lo = np . column_stack ( ( self . basis_omega , self . basis_omega , self . basis_omega , self . basis_omega Lp = np . column_stack ( ( self . basis_phi , self . basis_phi + np . pi / 2 , self . basis_phi + np . pi , self . basis_phi r , omega , phi = self . _cos_factorization ( La , Lo , Lp ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : F1lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega , self . basis_omega , self . basis_phi F2lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega ** 2 , self . basis_omega , self . basis_phi lower_terms = self . b [ 0 ] * np . dot ( Flower , Flower . T ) + self . b [ 1 ] * np . dot ( F2lower , F2lower . T ) + self return ( 3 * self . lengthscale ** 5 / ( 400 * np . sqrt ( 5 ) * self . variance ) * Gint + 1. / self . variance * lower_terms \n 
~~~ if X2 is None : X2 = X \n 
La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega , self . Lo = np . column_stack ( ( self . basis_omega , self . basis_omega , self . basis_omega , self . basis_omega Lp = np . column_stack ( ( self . basis_phi , self . basis_phi + np . pi / 2 , self . basis_phi + np . pi , self . basis_phi r , omega , phi = self . _cos_factorization ( La , Lo , Lp ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : F1lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega , self . basis_omega , self . basis_phi F2lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega ** 2 , self . basis_omega , self . basis_phi \n 
da_dlen = [ - 3 * self . a [ 0 ] / self . lengthscale , - 2 * self . a [ 1 ] / self . lengthscale , - self . a [ 2 ] / self . lengthscale db_dlen = [ 0. , 4 * self . b [ 1 ] / self . lengthscale , 2 * self . b [ 2 ] / self . lengthscale , 2 * self . b [ 3 ] / self . dLa_dlen = np . column_stack ( ( da_dlen [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , da_dlen [ 1 ] * self . basis_omega r1 , omega1 , phi1 = self . _cos_factorization ( dLa_dlen , Lo , Lp ) \n 
dlower_terms_dlen = db_dlen [ 0 ] * np . dot ( Flower , Flower . T ) + db_dlen [ 1 ] * np . dot ( F2lower , F2lower . T dG_dlen = 15 * self . lengthscale ** 4 / ( 400 * np . sqrt ( 5 ) ) * Gint + 3 * self . lengthscale ** 5 / ( 400 * np . sqrt ( dK_dlen = - mdot ( FX , self . Gi , dG_dlen / self . variance , self . Gi , FX2 . T ) \n 
dLa_dper2 = np . column_stack ( ( - self . a [ 1 ] * self . basis_omega / self . period , - 2 * self . a [ 2 ] * self . basis_omega dLp_dper2 = np . column_stack ( ( self . basis_phi + np . pi / 2 , self . basis_phi + np . pi , self . basis_phi + np r2 , omega2 , phi2 = self . _cos_factorization ( dLa_dper2 , Lo [ : , 0 : 2 ] , dLp_dper2 ) \n 
dFlower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega / self . period dF1lower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega ** 2 / self . period dF2lower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega ** 3 / self . period \n 
dlower_terms_dper = self . b [ 0 ] * ( np . dot ( dFlower_dper , Flower . T ) + np . dot ( Flower . T , dFlower_dper dlower_terms_dper += self . b [ 1 ] * ( np . dot ( dF2lower_dper , F2lower . T ) + np . dot ( F2lower , dF2lower_dper dlower_terms_dper += self . b [ 2 ] * ( np . dot ( dF1lower_dper , F1lower . T ) + np . dot ( F1lower , dF1lower_dper dlower_terms_dper += self . b [ 3 ] * ( np . dot ( dF2lower_dper , Flower . T ) + np . dot ( F2lower , dFlower_dper dlower_terms_dper += self . b [ 4 ] * ( np . dot ( dFlower_dper , F2lower . T ) + np . dot ( Flower , dF2lower_dper \n 
dG_dper = 1. / self . variance * ( 3 * self . lengthscale ** 5 / ( 400 * np . sqrt ( 5 ) ) * dGint_dper + 0.5 * dlower_terms_dper dK_dper = mdot ( dFX_dper , self . Gi , FX2 . T ) - mdot ( FX , self . Gi , dG_dper , self . Gi , FX2 . T ) + mdot ( FX , self \n 
~~ ~~ from GPy . core . mapping import Mapping \n 
from GPy . core import Param \n 
class PiecewiseLinear ( Mapping ) : \n 
def __init__ ( self , input_dim , output_dim , values , breaks , name = ) : \n 
~~~ assert input_dim == 1 \n 
assert output_dim == 1 \n 
Mapping . __init__ ( self , input_dim , output_dim , name ) \n 
values , breaks = np . array ( values ) . flatten ( ) , np . array ( breaks ) . flatten ( ) \n 
assert values . size == breaks . size \n 
self . values = Param ( , values ) \n 
self . breaks = Param ( , breaks ) \n 
self . link_parameter ( self . values ) \n 
self . link_parameter ( self . breaks ) \n 
~~~ self . order = np . argsort ( self . breaks ) * 1 \n 
self . reverse_order = np . zeros_like ( self . order ) \n 
self . reverse_order [ self . order ] = np . arange ( self . order . size ) \n 
self . sorted_breaks = self . breaks [ self . order ] \n 
self . sorted_values = self . values [ self . order ] \n 
self . grads = np . diff ( self . sorted_values ) / np . diff ( self . sorted_breaks ) \n 
~~ def f ( self , X ) : \n 
~~~ x = X . flatten ( ) \n 
y = x . copy ( ) \n 
y [ x < self . sorted_breaks [ 0 ] ] = x [ x < self . sorted_breaks [ 0 ] ] + self . sorted_values [ 0 ] - self . sorted_breaks \n 
y [ x > self . sorted_breaks [ - 1 ] ] = x [ x > self . sorted_breaks [ - 1 ] ] + self . sorted_values [ - 1 ] - self . sorted_breaks \n 
for low , up , g , v in zip ( self . sorted_breaks [ : - 1 ] , self . sorted_breaks [ 1 : ] , self . grads , self . ~~~ i = np . logical_and ( x > low , x < up ) \n 
y [ i ] = v + ( x [ i ] - low ) * g \n 
~~ return y . reshape ( - 1 , 1 ) \n 
~~ def update_gradients ( self , dL_dF , X ) : \n 
dL_dF = dL_dF . flatten ( ) \n 
dL_db = np . zeros ( self . sorted_breaks . size ) \n 
dL_dv = np . zeros ( self . sorted_values . size ) \n 
xx = x [ index ] \n 
grad = dL_dF [ index ] \n 
span = up - low \n 
dL_dv [ i ] += np . sum ( grad * ( ( low - xx ) / span + 1 ) ) \n 
dL_dv [ i + 1 ] += np . sum ( grad * ( xx - low ) / span ) \n 
dL_db [ i ] += np . sum ( grad * g * ( xx - up ) / span ) \n 
dL_db [ i + 1 ] += np . sum ( grad * g * ( low - xx ) / span ) \n 
~~ dL_db [ 0 ] -= np . sum ( dL_dF [ x < self . sorted_breaks [ 0 ] ] ) \n 
dL_db [ - 1 ] -= np . sum ( dL_dF [ x > self . sorted_breaks [ - 1 ] ] ) \n 
dL_dv [ 0 ] += np . sum ( dL_dF [ x < self . sorted_breaks [ 0 ] ] ) \n 
dL_dv [ - 1 ] += np . sum ( dL_dF [ x > self . sorted_breaks [ - 1 ] ] ) \n 
self . breaks . gradient = dL_db [ self . reverse_order ] \n 
self . values . gradient = dL_dv [ self . reverse_order ] \n 
~~ def gradients_X ( self , dL_dF , X ) : \n 
dL_dX [ i ] = dL_dF [ i ] * g \n 
~~ return dL_dX . reshape ( - 1 , 1 ) \n 
from . . util . warping_functions import * \n 
from . . core import GP \n 
from . . import likelihoods \n 
from GPy . util . warping_functions import TanhWarpingFunction_d \n 
from GPy import kern \n 
class WarpedGP ( GP ) : \n 
~~~ def __init__ ( self , X , Y , kernel = None , warping_function = None , warping_terms = 3 ) : \n 
~~~ if kernel is None : \n 
~~~ kernel = kern . RBF ( X . shape [ 1 ] ) \n 
~~ if warping_function == None : \n 
~~~ self . warping_function = TanhWarpingFunction_d ( warping_terms ) \n 
self . warping_params = ( np . random . randn ( self . warping_function . n_terms * 3 + 1 ) * 1 ) \n 
~~~ self . warping_function = warping_function \n 
~~ self . scale_data = False \n 
if self . scale_data : \n 
~~~ Y = self . _scale_data ( Y ) \n 
~~ self . Y_untransformed = Y . copy ( ) \n 
self . predict_in_warped_space = True \n 
likelihood = likelihoods . Gaussian ( ) \n 
GP . __init__ ( self , X , self . transform_data ( ) , likelihood = likelihood , kernel = kernel ) \n 
self . link_parameter ( self . warping_function ) \n 
~~ def _scale_data ( self , Y ) : \n 
~~~ self . _Ymax = Y . max ( ) \n 
self . _Ymin = Y . min ( ) \n 
return ( Y - self . _Ymin ) / ( self . _Ymax - self . _Ymin ) - 0.5 \n 
~~ def _unscale_data ( self , Y ) : \n 
~~~ return ( Y + 0.5 ) * ( self . _Ymax - self . _Ymin ) + self . _Ymin \n 
~~~ self . Y [ : ] = self . transform_data ( ) \n 
super ( WarpedGP , self ) . parameters_changed ( ) \n 
Kiy = self . posterior . woodbury_vector . flatten ( ) \n 
grad_y = self . warping_function . fgrad_y ( self . Y_untransformed ) \n 
grad_y_psi , grad_psi = self . warping_function . fgrad_y_psi ( self . Y_untransformed , \n 
return_covar_chain = True ) \n 
djac_dpsi = ( ( 1.0 / grad_y [ : , : , None , None ] ) * grad_y_psi ) . sum ( axis = 0 ) . sum ( axis = 0 ) \n 
dquad_dpsi = ( Kiy [ : , None , None , None ] * grad_psi ) . sum ( axis = 0 ) . sum ( axis = 0 ) \n 
warping_grads = - dquad_dpsi + djac_dpsi \n 
self . warping_function . psi . gradient [ : ] = warping_grads [ : , : - 1 ] \n 
self . warping_function . d . gradient [ : ] = warping_grads [ 0 , - 1 ] \n 
~~ def transform_data ( self ) : \n 
~~~ Y = self . warping_function . f ( self . Y_untransformed . copy ( ) ) . copy ( ) \n 
return Y \n 
~~ def log_likelihood ( self ) : \n 
~~~ ll = GP . log_likelihood ( self ) \n 
jacobian = self . warping_function . fgrad_y ( self . Y_untransformed ) \n 
return ll + np . log ( jacobian ) . sum ( ) \n 
~~ def plot_warping ( self ) : \n 
~~~ self . warping_function . plot ( self . Y_untransformed . min ( ) , self . Y_untransformed . max ( ) ) \n 
~~ def _get_warped_term ( self , mean , std , gh_samples , pred_init = None ) : \n 
~~~ arg1 = gh_samples . dot ( std . T ) * np . sqrt ( 2 ) \n 
arg2 = np . ones ( shape = gh_samples . shape ) . dot ( mean . T ) \n 
return self . warping_function . f_inv ( arg1 + arg2 , y = pred_init ) \n 
~~ def _get_warped_mean ( self , mean , std , pred_init = None , deg_gauss_hermite = 100 ) : \n 
gh_samples , gh_weights = np . polynomial . hermite . hermgauss ( deg_gauss_hermite ) \n 
gh_samples = gh_samples [ : , None ] \n 
gh_weights = gh_weights [ None , : ] \n 
return gh_weights . dot ( self . _get_warped_term ( mean , std , gh_samples ) ) / np . sqrt ( np . pi ) \n 
~~ def _get_warped_variance ( self , mean , std , pred_init = None , deg_gauss_hermite = 100 ) : \n 
arg1 = gh_weights . dot ( self . _get_warped_term ( mean , std , gh_samples , \n 
pred_init = pred_init ) ** 2 ) / np . sqrt ( np . pi ) \n 
arg2 = self . _get_warped_mean ( mean , std , pred_init = pred_init , \n 
deg_gauss_hermite = deg_gauss_hermite ) \n 
return arg1 - ( arg2 ** 2 ) \n 
~~ def predict ( self , Xnew , which_parts = , pred_init = None , full_cov = False , Y_metadata = None , \n 
median = False , deg_gauss_hermite = 100 ) : \n 
~~~ mu , var = GP . _raw_predict ( self , Xnew ) \n 
mean , var = self . likelihood . predictive_values ( mu , var ) \n 
if self . predict_in_warped_space : \n 
~~~ std = np . sqrt ( var ) \n 
if median : \n 
~~~ wmean = self . warping_function . f_inv ( mean , y = pred_init ) \n 
~~~ wmean = self . _get_warped_mean ( mean , std , pred_init = pred_init , \n 
deg_gauss_hermite = deg_gauss_hermite ) . T \n 
~~ wvar = self . _get_warped_variance ( mean , std , pred_init = pred_init , \n 
~~~ wmean = mean \n 
wvar = var \n 
~~ if self . scale_data : \n 
~~~ pred = self . _unscale_data ( pred ) \n 
~~ return wmean , wvar \n 
~~ def predict_quantiles ( self , X , quantiles = ( 2.5 , 97.5 ) , Y_metadata = None ) : \n 
m , v = self . _raw_predict ( X , full_cov = False ) \n 
if self . normalizer is not None : \n 
~~~ m , v = self . normalizer . inverse_mean ( m ) , self . normalizer . inverse_variance ( v ) \n 
~~ a , b = self . likelihood . predictive_quantiles ( m , v , quantiles , Y_metadata ) \n 
if not self . predict_in_warped_space : \n 
~~~ return [ a , b ] \n 
~~ new_a = self . warping_function . f_inv ( a ) \n 
new_b = self . warping_function . f_inv ( b ) \n 
return [ new_a , new_b ] \n 
~~~ X = np . random . randn ( 100 , 1 ) \n 
Y = np . sin ( X ) + np . random . randn ( 100 , 1 ) * 0.05 \n 
m = WarpedGP ( X , Y ) \n 
~~~ from matplotlib import pyplot as pb \n 
from matplotlib . patches import Polygon \n 
from matplotlib . collections import PatchCollection \n 
~~~ __IPYTHON__ \n 
pb . ion ( ) \n 
~~ import re \n 
def plot ( shape_records , facecolor = , edgecolor = , linewidths = .5 , ax = None , xlims = None , ylims = None ) : \n 
if ax is None : \n 
~~~ fig = pb . figure ( ) \n 
ax = fig . add_subplot ( 111 ) \n 
~~ for srec in shape_records : \n 
~~~ points = np . vstack ( srec . shape . points ) \n 
sparts = srec . shape . parts \n 
par = list ( sparts ) + [ points . shape [ 0 ] ] \n 
polygs = [ ] \n 
for pj in range ( len ( sparts ) ) : \n 
~~~ polygs . append ( Polygon ( points [ par [ pj ] : par [ pj + 1 ] ] ) ) \n 
~~ ax . add_collection ( PatchCollection ( polygs , facecolor = facecolor , edgecolor = edgecolor , linewidths \n 
~~ _box = np . vstack ( [ srec . shape . bbox for srec in shape_records ] ) \n 
minx , miny = np . min ( _box [ : , : 2 ] , 0 ) \n 
maxx , maxy = np . max ( _box [ : , 2 : ] , 0 ) \n 
if xlims is not None : \n 
~~~ minx , maxx = xlims \n 
~~ if ylims is not None : \n 
~~~ miny , maxy = ylims \n 
~~ ax . set_xlim ( minx , maxx ) \n 
ax . set_ylim ( miny , maxy ) \n 
~~ def string_match ( sf , regex , field = 2 ) : \n 
index = [ ] \n 
shape_records = [ ] \n 
for rec in enumerate ( sf . shapeRecords ( ) ) : \n 
~~~ m = re . search ( regex , rec [ 1 ] . record [ field ] ) \n 
if m is not None : \n 
~~~ index . append ( rec [ 0 ] ) \n 
shape_records . append ( rec [ 1 ] ) \n 
~~ ~~ return index , shape_records \n 
~~ def bbox_match ( sf , bbox , inside_only = True ) : \n 
A , B , C , D = bbox \n 
~~~ a , b , c , d = rec [ 1 ] . shape . bbox \n 
if inside_only : \n 
~~~ if A <= a and B <= b and C >= c and D >= d : \n 
~~~ cond1 = A <= a and B <= b and C >= a and D >= b \n 
cond2 = A <= c and B <= d and C >= c and D >= d \n 
cond3 = A <= a and D >= d and C >= a and B <= d \n 
cond4 = A <= c and D >= b and C >= c and B <= b \n 
cond5 = a <= C and b <= B and d >= D \n 
cond6 = c <= A and b <= B and d >= D \n 
cond7 = d <= B and a <= A and c >= C \n 
cond8 = b <= D and a <= A and c >= C \n 
if cond1 or cond2 or cond3 or cond4 or cond5 or cond6 or cond7 or cond8 : \n 
~~ ~~ ~~ return index , shape_records \n 
~~ def plot_bbox ( sf , bbox , inside_only = True ) : \n 
index , shape_records = bbox_match ( sf , bbox , inside_only ) \n 
plot ( shape_records , xlims = [ bbox [ 0 ] , bbox [ 2 ] ] , ylims = [ bbox [ 1 ] , bbox [ 3 ] ] ) \n 
~~ def plot_string_match ( sf , regex , field , ** kwargs ) : \n 
index , shape_records = string_match ( sf , regex , field ) \n 
plot ( shape_records , ** kwargs ) \n 
~~ def new_shape_string ( sf , name , regex , field = 2 , type = None ) : \n 
~~~ import shapefile \n 
if type is None : \n 
~~~ type = shapefile . POINT \n 
~~ newshp = shapefile . Writer ( shapeType = sf . shapeType ) \n 
newshp . autoBalance = 1 \n 
_fi = [ sf . fields [ j ] for j in index ] \n 
for f in _fi : \n 
~~~ newshp . field ( name = f [ 0 ] , fieldType = f [ 1 ] , size = f [ 2 ] , decimal = f [ 3 ] ) \n 
~~ _shre = shape_records \n 
for sr in _shre : \n 
~~~ _points = [ ] \n 
_parts = [ ] \n 
for point in sr . shape . points : \n 
~~~ _points . append ( point ) \n 
~~ _parts . append ( _points ) \n 
newshp . line ( parts = _parts ) \n 
newshp . records . append ( sr . record ) \n 
print ( len ( sr . record ) ) \n 
~~ newshp . save ( name ) \n 
print ( index ) \n 
~~ def apply_bbox ( sf , ax ) : \n 
limits = sf . bbox \n 
xlim = limits [ 0 ] , limits [ 2 ] \n 
ylim = limits [ 1 ] , limits [ 3 ] \n 
ax . set_xlim ( xlim ) \n 
ax . set_ylim ( ylim ) \n 
import unittest , itertools \n 
import tempfile \n 
from GPy . examples . dimensionality_reduction import mrd_simulation \n 
from GPy . core . parameterization . variational import NormalPosterior \n 
from GPy . models . gp_regression import GPRegression \n 
import GPy \n 
from nose import SkipTest \n 
def toy_model ( ) : \n 
~~~ X = np . linspace ( 0 , 1 , 50 ) [ : , None ] \n 
Y = np . sin ( X ) \n 
m = GPRegression ( X = X , Y = Y ) \n 
return m \n 
~~ class ListDictTestCase ( unittest . TestCase ) : \n 
~~~ def assertListDictEquals ( self , d1 , d2 , msg = None ) : \n 
~~~ for k , v in d1 . items ( ) : \n 
~~~ self . assertListEqual ( list ( v ) , list ( d2 [ k ] ) , msg ) \n 
~~ ~~ def assertArrayListEquals ( self , l1 , l2 ) : \n 
~~~ for a1 , a2 in zip ( l1 , l2 ) : \n 
~~~ np . testing . assert_array_equal ( a1 , a2 ) \n 
~~ ~~ ~~ class Test ( ListDictTestCase ) : \n 
~~~ @ SkipTest \n 
def test_load_pickle ( self ) : \n 
~~~ import os \n 
m = GPy . load ( os . path . join ( os . path . abspath ( os . path . split ( __file__ ) [ 0 ] ) , ) self . assertTrue ( m . checkgrad ( ) ) \n 
self . assertEqual ( m . log_likelihood ( ) , - 4.7351019830022087 ) \n 
~~ def test_model ( self ) : \n 
~~~ par = toy_model ( ) \n 
pcopy = par . copy ( ) \n 
self . assertListEqual ( par . param_array . tolist ( ) , pcopy . param_array . tolist ( ) ) \n 
np . testing . assert_allclose ( par . gradient_full , pcopy . gradient_full ) \n 
self . assertSequenceEqual ( str ( par ) , str ( pcopy ) ) \n 
self . assertIsNot ( par . param_array , pcopy . param_array ) \n 
self . assertIsNot ( par . gradient_full , pcopy . gradient_full ) \n 
self . assertTrue ( pcopy . checkgrad ( ) ) \n 
self . assert_ ( np . any ( pcopy . gradient != 0.0 ) ) \n 
with tempfile . TemporaryFile ( ) as f : \n 
~~~ par . pickle ( f ) \n 
f . seek ( 0 ) \n 
pcopy = pickle . load ( f ) \n 
~~ self . assertListEqual ( par . param_array . tolist ( ) , pcopy . param_array . tolist ( ) ) \n 
self . assert_ ( pcopy . checkgrad ( ) ) \n 
~~ def test_modelrecreation ( self ) : \n 
pcopy = GPRegression ( par . X . copy ( ) , par . Y . copy ( ) , kernel = par . kern . copy ( ) ) \n 
np . testing . assert_allclose ( par . param_array , pcopy . param_array ) \n 
np . testing . assert_allclose ( pcopy . param_array , par . param_array , atol = 1e-6 ) \n 
par . randomize ( ) \n 
~~ np . testing . assert_allclose ( par . param_array , pcopy . param_array ) \n 
np . testing . assert_allclose ( par . gradient_full , pcopy . gradient_full , atol = 1e-6 ) \n 
~~ def test_posterior ( self ) : \n 
~~~ X = np . random . randn ( 3 , 5 ) \n 
Xv = np . random . rand ( * X . shape ) \n 
par = NormalPosterior ( X , Xv ) \n 
par . gradient = 10 \n 
pcopy . gradient = 10 \n 
self . assertListEqual ( par . gradient_full . tolist ( ) , pcopy . gradient_full . tolist ( ) ) \n 
np . testing . assert_allclose ( pcopy . mean . gradient_full , 10 ) \n 
~~ def test_model_concat ( self ) : \n 
~~~ par = mrd_simulation ( optimize = 0 , plot = 0 , plot_sim = 0 ) \n 
self . assertTrue ( par . checkgrad ( ) ) \n 
~~ def _callback ( self , what , which ) : \n 
~~~ what . count += 1 \n 
~~~ import pylab \n 
import matplotlib \n 
~~ from numpy . linalg . linalg import LinAlgError \n 
from operator import setitem \n 
from functools import reduce \n 
class PCA ( object ) : \n 
def __init__ ( self , X ) : \n 
~~~ self . mu = None \n 
self . sigma = None \n 
X = self . center ( X ) \n 
if X . shape [ 0 ] >= X . shape [ 1 ] : \n 
~~~ self . eigvals , self . eigvectors = self . _primal_eig ( X ) \n 
~~~ self . eigvals , self . eigvectors = self . _dual_eig ( X ) \n 
~~ self . sort = numpy . argsort ( self . eigvals ) [ : : - 1 ] \n 
self . eigvals = self . eigvals [ self . sort ] \n 
self . eigvectors = self . eigvectors [ : , self . sort ] \n 
self . fracs = self . eigvals / self . eigvals . sum ( ) \n 
self . Q = self . eigvals . shape [ 0 ] \n 
~~ def center ( self , X ) : \n 
X = X . copy ( ) \n 
inan = numpy . isnan ( X ) \n 
if self . mu is None : \n 
~~~ X_ = numpy . ma . masked_array ( X , inan ) \n 
self . mu = X_ . mean ( 0 ) . base \n 
self . sigma = X_ . std ( 0 ) . base \n 
~~ reduce ( lambda y , x : setitem ( x [ 0 ] , x [ 1 ] , x [ 2 ] ) , zip ( X . T , inan . T , self . mu ) , None ) \n 
X = X - self . mu \n 
X = X / numpy . where ( self . sigma == 0 , 1e-30 , self . sigma ) \n 
return X \n 
~~ def _primal_eig ( self , X ) : \n 
~~~ return numpy . linalg . eigh ( numpy . einsum ( , X , X ) ) \n 
~~ def _dual_eig ( self , X ) : \n 
~~~ dual_eigvals , dual_eigvects = numpy . linalg . eigh ( numpy . einsum ( , X , X ) ) \n 
relevant_dimensions = numpy . argsort ( numpy . abs ( dual_eigvals ) ) [ - X . shape [ 1 ] : ] \n 
eigvals = dual_eigvals [ relevant_dimensions ] \n 
eigvects = dual_eigvects [ : , relevant_dimensions ] \n 
eigvects = ( 1. / numpy . sqrt ( X . shape [ 0 ] * numpy . abs ( eigvals ) ) ) * X . T . dot ( eigvects ) \n 
eigvects /= numpy . sqrt ( numpy . diag ( eigvects . T . dot ( eigvects ) ) ) \n 
return eigvals , eigvects \n 
~~ def project ( self , X , Q = None ) : \n 
if Q is None : \n 
~~~ Q = self . Q \n 
~~ if Q > X . shape [ 1 ] : \n 
~~ X = self . center ( X ) \n 
return X . dot ( self . eigvectors [ : , : Q ] ) \n 
~~ def plot_fracs ( self , Q = None , ax = None , fignum = None ) : \n 
from . . plotting import Tango \n 
Tango . reset ( ) \n 
col = Tango . nextMedium ( ) \n 
~~~ fig = pylab . figure ( fignum ) \n 
~~ if Q is None : \n 
~~ ticks = numpy . arange ( Q ) \n 
bar = ax . bar ( ticks - .4 , self . fracs [ : Q ] , color = col ) \n 
ax . set_xticks ( ticks , map ( lambda x : r"${}$" . format ( x ) , ticks + 1 ) ) \n 
ax . set_xlabel ( "PC" ) \n 
ax . set_ylim ( 0 , ax . get_ylim ( ) [ 1 ] ) \n 
ax . set_xlim ( ticks . min ( ) - .5 , ticks . max ( ) + .5 ) \n 
~~~ pylab . tight_layout ( ) \n 
~~ return bar \n 
~~ def plot_2d ( self , X , labels = None , s = 20 , marker = , \n 
dimensions = ( 0 , 1 ) , ax = None , colors = None , \n 
if cmap is None : \n 
~~~ cmap = matplotlib . cm . jet \n 
~~ if ax is None : \n 
~~ if labels is None : \n 
~~~ labels = numpy . zeros ( X . shape [ 0 ] ) \n 
~~ ulabels = [ ] \n 
for lab in labels : \n 
~~~ if not lab in ulabels : \n 
~~~ ulabels . append ( lab ) \n 
~~ ~~ nlabels = len ( ulabels ) \n 
if colors is None : \n 
~~~ colors = iter ( [ cmap ( float ( i ) / nlabels ) for i in range ( nlabels ) ] ) \n 
~~~ colors = iter ( colors ) \n 
~~ X_ = self . project ( X , self . Q ) [ : , dimensions ] \n 
kwargs . update ( dict ( s = s ) ) \n 
plots = list ( ) \n 
for i , l in enumerate ( ulabels ) : \n 
~~~ kwargs . update ( dict ( color = colors . next ( ) , marker = marker [ i % len ( marker ) ] ) ) \n 
plots . append ( ax . scatter ( * X_ [ labels == l , : ] . T , label = str ( l ) , ** kwargs ) ) \n 
~~ ax . set_xlabel ( r"PC$_1$" ) \n 
ax . set_ylabel ( r"PC$_2$" ) \n 
~~ return plots \n 
~~ ~~ __author__ = \n 
from pypet import Environment \n 
from pypet . utils . explore import cartesian_product \n 
def multiply ( traj ) : \n 
z = traj . x * traj . y \n 
traj . f_add_result ( , z , comment = ) \n 
~~ filename = os . path . join ( , ) \n 
env = Environment ( trajectory = , \n 
filename = filename , \n 
overwrite_file = True , \n 
file_title = , \n 
comment = , \n 
traj = env . trajectory \n 
traj . f_add_parameter ( , 1 , comment = ) \n 
traj . f_explore ( cartesian_product ( { : [ 1 , 2 , 3 , 4 ] , : [ 6 , 7 , 8 ] } ) ) \n 
env . run ( multiply ) \n 
from pypet . trajectory import Trajectory \n 
del traj \n 
env . disable_logging ( ) \n 
del env \n 
traj = Trajectory ( filename = filename ) \n 
traj . f_load ( index = - 1 , load_parameters = 2 , load_results = 2 ) \n 
print ( ) \n 
print ( traj . run_00000001 . z ) \n 
__author__ = \n 
from pypet import Environment , cartesian_product \n 
from pypet import pypetconstants \n 
traj . f_add_result ( , z = z , comment = ) \n 
~~ def main ( ) : \n 
filename = os . path . join ( , ) \n 
log_stdout = True , \n 
multiproc = True , \n 
wrap_mode = pypetconstants . WRAP_MODE_LOCAL , \n 
overwrite_file = True ) \n 
traj . f_add_parameter ( , 1.0 , comment = ) \n 
traj . f_explore ( cartesian_product ( { : [ float ( x ) for x in range ( 20 ) ] , \n 
: [ float ( y ) for y in range ( 20 ) ] } ) ) \n 
assert traj . f_is_completed ( ) \n 
~~~ main ( ) __author__ = \n 
~~ from pypet . tests . testutils . ioutils import run_suite , discover_tests , TEST_IMPORT_ERROR , parse_args \n 
tests_include = set ( ( , \n 
big_suite_1 = discover_tests ( lambda class_name , test_name , tags : class_name in tests_include ) \n 
tests_include = set ( ( \n 
big_suite_2 = discover_tests ( lambda class_name , test_name , tags : class_name in tests_include ) \n 
big_suite_3 = discover_tests ( lambda class_name , test_name , tags : class_name in tests_include ) \n 
big_suite_4 = discover_tests ( lambda class_name , test_name , tags : class_name in tests_include ) \n 
suite_dict = { : big_suite_1 , : big_suite_2 , : big_suite_3 , : big_suite_4 } \n 
~~~ opt_dict = parse_args ( ) \n 
suite = None \n 
if in opt_dict : \n 
~~~ suite_no = opt_dict . pop ( ) \n 
suite = suite_dict [ suite_no ] \n 
~~ if suite is None : \n 
~~~ pred = lambda class_name , test_name , tags : ( in tags and \n 
class_name != TEST_IMPORT_ERROR ) \n 
suite = discover_tests ( pred ) \n 
~~ run_suite ( suite = suite , ** opt_dict ) __author__ = \n 
~~ from pypet import Environment , Trajectory \n 
from pypet . tests . testutils . ioutils import make_temp_dir , get_log_config \n 
def job ( traj ) : \n 
~~~ traj . f_ares ( , 42 , comment = ) \n 
~~ def get_runtime ( length ) : \n 
~~~ filename = os . path . join ( , , ) \n 
with Environment ( filename = filename , \n 
log_levels = 50 , report_progress = ( 0.0002 , , 50 ) , \n 
overwrite_file = True , purge_duplicate_comments = False , \n 
log_stdout = False , \n 
multiproc = False , ncores = 2 , use_pool = True , \n 
wrap_mode = , #freeze_input=True, \n 
summary_tables = False , small_overview_tables = False ) as env : \n 
~~~ traj = env . v_traj \n 
traj . par . f_apar ( , 0 , ) \n 
traj . f_explore ( { : range ( length ) } ) \n 
max_run = 1000 \n 
for idx in range ( len ( traj ) ) : \n 
~~~ if idx > max_run : \n 
~~~ traj . f_get_run_information ( idx , copy = False ) [ ] = 1 \n 
~~ ~~ start = time . time ( ) \n 
env . f_run ( job ) \n 
end = time . time ( ) \n 
~~ total = end - start \n 
return total / float ( min ( len ( traj ) , max_run ) ) , total / float ( min ( len ( traj ) , max_run ) ) * len ( traj ) \n 
~~~ lengths = [ 100000 , 50000 , 10000 , 5000 , 1000 , 500 , 100 , 50 , 10 , 5 , 1 ] \n 
runtimes = [ get_runtime ( x ) for x in lengths ] \n 
avg_runtimes = [ x [ 0 ] for x in runtimes ] \n 
summed_runtime = [ x [ 1 ] for x in runtimes ] \n 
plt . subplot ( 2 , 1 , 1 ) \n 
plt . semilogx ( list ( reversed ( lengths ) ) , list ( reversed ( avg_runtimes ) ) , linewidth = 2 ) \n 
plt . xlabel ( ) \n 
plt . ylabel ( ) \n 
plt . title ( ) \n 
plt . grid ( ) \n 
plt . subplot ( 2 , 1 , 2 ) \n 
plt . loglog ( lengths , summed_runtime , linewidth = 2 ) \n 
plt . savefig ( ) \n 
plt . show ( ) \n 
if ( sys . version_info < ( 2 , 7 , 0 ) ) : \n 
~~~ import cPickle as pickle \n 
~~~ import pickle \n 
~~ from pypet . pypetlogging import LoggingManager \n 
from pypet . tests . testutils . ioutils import get_log_config , run_suite , parse_args \n 
from pypet . utils . comparisons import nested_equal \n 
class FakeTraj ( object ) : \n 
~~~ self . v_environment_name = \n 
self . v_name = \n 
~~ def f_wildcard ( self , card ) : \n 
~~ ~~ class LoggingManagerTest ( unittest . TestCase ) : \n 
~~~ tags = , , \n 
def test_pickling ( self ) : \n 
~~~ manager = LoggingManager ( log_config = get_log_config ( ) , log_stdout = True ) \n 
manager . extract_replacements ( FakeTraj ( ) ) \n 
manager . check_log_config ( ) \n 
manager . make_logging_handlers_and_tools ( ) \n 
dump = pickle . dumps ( manager ) \n 
new_manager = pickle . loads ( dump ) \n 
manager . finalize ( ) \n 
~~~ opt_args = parse_args ( ) \n 
run_suite ( ** opt_args ) \n 
~~ from __future__ import absolute_import , print_function , division \n 
from six . moves import xrange \n 
k = T . iscalar ( "k" ) ; A = T . vector ( "A" ) \n 
def inner_fct ( prior_result , A ) : return prior_result * A \n 
result , updates = theano . scan ( fn = inner_fct , \n 
outputs_info = T . ones_like ( A ) , \n 
non_sequences = A , n_steps = k ) \n 
final_result = result [ - 1 ] \n 
power = theano . function ( inputs = [ A , k ] , outputs = final_result , \n 
updates = updates ) \n 
print ( power ( list ( range ( 10 ) ) , 2 ) ) \n 
from __future__ import absolute_import , print_function , division \n 
from nose . plugins . skip import SkipTest \n 
from theano import config \n 
from theano import gof \n 
import theano . tensor \n 
from theano . compat import exc_message \n 
from theano . compile import debugmode \n 
import theano . compile \n 
from theano . tests import unittest_tools as utt \n 
def test0 ( ) : \n 
~~~ x = theano . tensor . dvector ( ) \n 
f = theano . function ( [ x ] , ( ( 2. * x ) + 7 ) / 2. , mode = debugmode . DebugMode ( ) ) \n 
f ( [ 1 , 2 ] ) \n 
~~ class BROKEN_ON_PURPOSE_Add ( gof . Op ) : \n 
~~~ __props__ = ( "py_offset" , ) \n 
def __init__ ( self , py_offset ) : \n 
~~~ gof . Op . __init__ ( self ) \n 
self . py_offset = py_offset \n 
~~ def make_node ( self , a , b ) : \n 
~~~ a = theano . tensor . as_tensor_variable ( a ) \n 
b = theano . tensor . as_tensor_variable ( b ) \n 
assert a . type . dtype == \n 
assert a . type . dtype == b . type . dtype \n 
assert a . type . ndim == 1 \n 
r = gof . Apply ( self , [ a , b ] , [ a . type ( ) ] ) \n 
return r \n 
~~ def perform ( self , node , inp , out_ ) : \n 
~~~ a , b = inp \n 
out , = out_ \n 
z = a + b \n 
if self . py_offset : \n 
~~~ out [ 0 ] = z + 0.5 \n 
~~~ out [ 0 ] = z \n 
~~ ~~ def c_code_cache_version ( self ) : \n 
~~~ return ( 1 , ) \n 
~~ def c_code ( self , node , name , inp , out , sub ) : \n 
z , = out \n 
~~ ~~ inconsistent = BROKEN_ON_PURPOSE_Add ( False ) \n 
off_by_half = BROKEN_ON_PURPOSE_Add ( True ) \n 
class WeirdBrokenOp ( gof . Op ) : \n 
__props__ = ( "behaviour" , ) \n 
def __init__ ( self , behaviour ) : \n 
self . behaviour = behaviour \n 
~~ def make_node ( self , a ) : \n 
~~~ a_ = theano . tensor . as_tensor_variable ( a ) \n 
r = gof . Apply ( self , [ a_ ] , [ a_ . type ( ) ] ) \n 
~~ def dontuse_perform ( self , node , inp , out_ ) : \n 
~~~ a , = inp \n 
if self . behaviour == : \n 
~~~ out [ 0 ] = a * 2 \n 
~~ elif self . behaviour == : \n 
~~~ out [ 0 ] = a \n 
out [ 0 ] *= 2 \n 
~~~ out [ 0 ] = a * 1 \n 
~~~ raise ValueError ( self . behaviour ) \n 
if "inplace" in self . behaviour : \n 
~~~ behaviour = "" \n 
total = ( ( z_code + prep_vars + behaviour + prep_vars2 ) \n 
% dict ( locals ( ) , ** sub ) ) \n 
return total \n 
~~ ~~ wb2i = WeirdBrokenOp ( ) \n 
wb2 = WeirdBrokenOp ( ) \n 
wb1i = WeirdBrokenOp ( ) \n 
wb1 = WeirdBrokenOp ( ) \n 
def test_badthunkoutput ( ) : \n 
~~~ a = theano . tensor . dvector ( ) \n 
b = theano . tensor . dvector ( ) \n 
f_good = theano . function ( [ a , b ] , \n 
off_by_half ( a , b ) , \n 
mode = debugmode . DebugMode ( check_c_code = theano . config . cxx ) ) \n 
f_inconsistent = theano . function ( [ a , b ] , \n 
inconsistent ( a , b ) , \n 
f_good ( [ 1.0 , 2.0 , 3.0 ] , [ 2 , 3 , 4 ] ) \n 
if not theano . config . cxx : \n 
~~~ f_inconsistent ( [ 1.0 , 2.0 , 3.0 ] , [ 2 , 3 , 4 ] ) \n 
~~ except debugmode . BadThunkOutput as e : \n 
~~~ assert e . r . owner . op is inconsistent \n 
~~ def test_badoptimization ( ) : \n 
~~~ @ gof . local_optimizer ( [ theano . tensor . add ] ) \n 
def insert_broken_add ( node ) : \n 
~~~ if node . op == theano . tensor . add : \n 
~~~ return [ off_by_half ( * node . inputs ) ] \n 
~~ edb = gof . EquilibriumDB ( ) \n 
edb . register ( , insert_broken_add , ) \n 
opt = edb . query ( ) \n 
a = theano . tensor . dvector ( ) \n 
f = theano . function ( [ a , b ] , a + b , \n 
mode = debugmode . DebugMode ( optimizer = opt ) ) \n 
~~~ f ( [ 1.0 , 2.0 , 3.0 ] , [ 2 , 3 , 4 ] , ) \n 
~~ except debugmode . BadOptimization as e : \n 
~~~ assert str ( e . reason ) == \n 
~~ assert False \n 
~~ def test_badoptimization_opt_err ( ) : \n 
@ gof . local_optimizer ( [ theano . tensor . add ] ) \n 
def insert_bigger_b_add ( node ) : \n 
~~~ inputs = list ( node . inputs ) \n 
if inputs [ - 1 ] . owner is None : \n 
~~~ inputs [ - 1 ] = theano . tensor . concatenate ( ( inputs [ - 1 ] , \n 
inputs [ - 1 ] ) ) \n 
return [ node . op ( * inputs ) ] \n 
~~ ~~ return False \n 
edb . register ( , insert_bigger_b_add , ) \n 
~~ except Exception as e : \n 
~~~ assert in exc_message ( e ) \n 
~~ def test_stochasticoptimization ( ) : \n 
~~~ last_time_replaced = [ False ] \n 
def insert_broken_add_sometimes ( node ) : \n 
~~~ last_time_replaced [ 0 ] = not last_time_replaced [ 0 ] \n 
if last_time_replaced [ 0 ] : \n 
edb . register ( \n 
insert_broken_add_sometimes , \n 
~~~ theano . function ( [ a , b ] , \n 
theano . tensor . add ( a , b ) , \n 
mode = debugmode . DebugMode ( \n 
optimizer = opt , \n 
check_c_code = True , \n 
stability_patience = max ( 2 , config . DebugMode . patience ) ) ) \n 
~~ except debugmode . StochasticOrder : \n 
~~ def test_just_c_code ( ) : \n 
~~~ if not theano . config . cxx : \n 
~~ x = theano . tensor . dvector ( ) \n 
f = theano . function ( [ x ] , wb2 ( x ) , \n 
mode = debugmode . DebugMode ( check_py_code = False ) ) \n 
assert numpy . all ( f ( [ 1 , 2 ] ) == [ 2 , 4 ] ) \n 
~~ def test_baddestroymap ( ) : \n 
~~~ class BadAdd ( gof . Op ) : \n 
~~~ def make_node ( self , a , b ) : \n 
~~~ c = a . type ( ) \n 
return gof . Apply ( self , [ a , b ] , [ c ] ) \n 
~~ def perform ( self , node , inp , out ) : \n 
c , = out \n 
c [ 0 ] = a \n 
c [ 0 ] += b \n 
~~ ~~ x = theano . tensor . dvector ( ) \n 
y = theano . tensor . dvector ( ) \n 
f = theano . function ( [ x , y ] , BadAdd ( ) ( x , y ) , mode = ) \n 
~~~ f ( [ 1 , 2 ] , [ 3 , 4 ] ) \n 
~~ except debugmode . BadDestroyMap : \n 
~~ ~~ def test_baddestroymap_c ( ) : \n 
f = theano . function ( [ x ] , wb2i ( x ) , \n 
~~~ assert numpy . all ( f ( [ 1 , 2 ] ) == [ 2 , 4 ] ) \n 
~~ ~~ class Test_ViewMap ( unittest . TestCase ) : \n 
~~~ class BadAddRef ( gof . Op ) : \n 
~~~ c = b . type ( ) \n 
c [ 0 ] = b \n 
~~ ~~ class BadAddSlice ( gof . Op ) : \n 
c [ 0 ] = b [ 1 : 3 ] \n 
~~ ~~ def test_badviewmap_ref ( self ) : \n 
f = theano . function ( [ x , y ] , self . BadAddRef ( ) ( x , y ) , mode = ) \n 
~~ except debugmode . BadViewMap : \n 
~~ ~~ def test_badviewmap_slice ( self ) : \n 
f = theano . function ( [ x , y ] , self . BadAddSlice ( ) ( x , y ) , \n 
~~ ~~ def test_goodviewmap ( self ) : \n 
~~~ goodop = self . BadAddRef ( ) \n 
goodop . view_map = { 0 : [ 1 ] } \n 
x = theano . tensor . dvector ( ) \n 
f = theano . function ( [ x , y ] , goodop ( x , y ) , mode = ) \n 
~~~ f ( [ 1 , 5 , 1 ] , [ 3 , 4 , 2 , 1 , 4 ] ) \n 
~~ ~~ def test_badviewmap_c ( self ) : \n 
f = theano . function ( [ x ] , wb1i ( x ) , \n 
~~~ f ( [ 1 , 2 ] ) \n 
~~ ~~ def test_aliased_outputs_ok ( self ) : \n 
~~~ class CustomOp ( gof . Op ) : \n 
~~~ view_map = { 0 : [ 0 ] , 1 : [ 0 ] } \n 
def make_node ( self , a , b ) : \n 
d = a . type ( ) \n 
return gof . Apply ( self , [ a , b ] , [ c , d ] ) \n 
c , d = out \n 
d [ 0 ] = a [ 1 : ] \n 
f = theano . function ( [ x , y ] , CustomOp ( ) ( x , y ) , mode = ) \n 
r0 , r1 = f ( [ 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 ] ) \n 
assert numpy . all ( r0 == [ 1 , 2 , 3 , 4 ] ) \n 
assert numpy . all ( r1 == [ 2 , 3 , 4 ] ) \n 
~~ def test_aliased_outputs_ok_output ( self ) : \n 
r = a * 2 \n 
c [ 0 ] = r \n 
d [ 0 ] = r [ 1 : ] \n 
assert numpy . all ( r0 == [ 2 , 4 , 6 , 8 ] ) \n 
assert numpy . all ( r1 == [ 4 , 6 , 8 ] ) \n 
~~ def test_aliased_outputs_ok_shadow ( self ) : \n 
r = a * 1 \n 
f = theano . function ( [ x , y ] , CustomOp ( ) ( x , y ) [ 0 ] * 2 , mode = ) \n 
r0 = f ( [ 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 ] ) \n 
~~ def test_aliased_outputs_bad ( self ) : \n 
c [ 0 ] = r [ : - 1 ] \n 
~~ ~~ custom_op = CustomOp ( ) \n 
bad_xy0 , bad_xy1 = custom_op ( x , y ) \n 
out = bad_xy0 * 2 + bad_xy1 * 2 \n 
f = theano . function ( [ x , y ] , out , mode = ) \n 
~~~ f ( [ 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 ] ) \n 
~~ ~~ ~~ class Test_check_isfinite ( unittest . TestCase ) : \n 
~~~ self . old_ts = theano . tensor . TensorType . filter_checks_isfinite \n 
self . old_dm = theano . compile . mode . predefined_modes [ \n 
] . check_isfinite \n 
~~~ theano . tensor . TensorType . filter_checks_isfinite = self . old_ts \n 
theano . compile . mode . predefined_modes [ \n 
] . check_isfinite = self . old_dm \n 
~~ def test_check_isfinite ( self ) : \n 
~~~ x = theano . tensor . vector ( ) \n 
f = theano . function ( [ x ] , ( x + 2 ) * 5 , mode = ) \n 
g = theano . function ( [ x ] , theano . tensor . log ( x ) , mode = ) \n 
f ( numpy . log ( [ 3 , 4 , 5 ] ) . astype ( config . floatX ) ) \n 
self . assertRaises ( debugmode . InvalidValueError , f , \n 
numpy . log ( [ 3 , - 4 , 5 ] ) . astype ( config . floatX ) ) \n 
( numpy . asarray ( [ 0 , 1.0 , 0 ] ) / 0 ) . astype ( config . floatX ) ) \n 
( numpy . asarray ( [ 1.0 , 1.0 , 1.0 ] ) / 0 ) . astype ( config . floatX ) ) \n 
self . assertRaises ( debugmode . InvalidValueError , g , \n 
numpy . asarray ( [ 3 , - 4 , 5 ] , dtype = config . floatX ) ) \n 
theano . tensor . TensorType . filter_checks_isfinite = False \n 
] . check_isfinite = False \n 
f ( numpy . asarray ( numpy . asarray ( [ 1.0 , 1.0 , 1.0 ] ) / 0 , \n 
dtype = config . floatX ) ) \n 
~~ def test_check_isfinite_disabled ( self ) : \n 
f = theano . function ( [ x ] , ( x + 2 ) * 5 , \n 
mode = debugmode . DebugMode ( check_isfinite = False ) ) \n 
f ( numpy . log ( [ 3 , - 4 , 5 ] ) ) \n 
infs = numpy . asarray ( [ 1.0 , 1. , 1. ] ) / 0 \n 
f ( infs ) \n 
~~ ~~ class BrokenCImplementationAdd ( gof . Op ) : \n 
~~~ __props__ = ( ) \n 
assert a . type . ndim == 2 \n 
out [ 0 ] = z \n 
~~ def c_code_cache_version ( self ) : \n 
debug = 0 \n 
~~ ~~ class VecAsRowAndCol ( gof . Op ) : \n 
__props__ = ( ) \n 
def make_node ( self , v ) : \n 
~~~ if not isinstance ( v , gof . Variable ) : \n 
~~~ v = theano . tensor . as_tensor_variable ( v ) \n 
~~ assert v . type . ndim == 1 \n 
type_class = type ( v . type ) \n 
out_r_type = type_class ( dtype = v . dtype , broadcastable = ( True , False ) ) \n 
out_c_type = type_class ( dtype = v . dtype , broadcastable = ( False , True ) ) \n 
return gof . Apply ( self , [ v ] , [ out_r_type ( ) , out_c_type ( ) ] ) \n 
~~~ v , = inp \n 
r , c = out \n 
lv = v . shape [ 0 ] \n 
if ( r [ 0 ] is None ) or ( r [ 0 ] . shape != ( 1 , lv ) ) : \n 
~~~ r [ 0 ] = node . outputs [ 0 ] . type . value_zeros ( ( 1 , lv ) ) \n 
~~ if ( c [ 0 ] is None ) or ( c [ 0 ] . shape != ( lv , 1 ) ) : \n 
~~~ c [ 0 ] = node . outputs [ 1 ] . type . value_zeros ( ( lv , 1 ) ) \n 
~~ for i in range ( lv ) : \n 
~~~ r [ 0 ] [ 0 , i ] = v [ i ] \n 
c [ 0 ] [ i , 0 ] = v [ i ] \n 
~~ ~~ ~~ class Test_preallocated_output ( unittest . TestCase ) : \n 
~~~ self . rng = numpy . random . RandomState ( seed = utt . fetch_seed ( ) ) \n 
~~ def test_f_contiguous ( self ) : \n 
~~~ a = theano . tensor . fmatrix ( ) \n 
b = theano . tensor . fmatrix ( ) \n 
z = BrokenCImplementationAdd ( ) ( a , b ) \n 
out = theano . tensor . dot ( z , numpy . eye ( 7 ) ) \n 
a_val = self . rng . randn ( 7 , 7 ) . astype ( ) \n 
b_val = self . rng . randn ( 7 , 7 ) . astype ( ) \n 
check_preallocated_output = [ ] ) \n 
f = theano . function ( [ a , b ] , out , mode = mode ) \n 
f ( a_val , b_val ) \n 
if theano . config . cxx : \n 
~~~ self . assertRaises ( debugmode . BadThunkOutput , f , a_val , b_val ) \n 
~~~ f ( a_val , b_val ) \n 
~~ ~~ def test_f_contiguous_out ( self ) : \n 
out = BrokenCImplementationAdd ( ) ( a , b ) \n 
~~ ~~ def test_output_broadcast_tensor ( self ) : \n 
~~~ v = theano . tensor . fvector ( ) \n 
c , r = VecAsRowAndCol ( ) ( v ) \n 
f = theano . function ( [ v ] , [ c , r ] ) \n 
v_val = self . rng . randn ( 5 ) . astype ( ) \n 
f ( v_val ) \n 
~~ def test_output_broadcast_cuda ( self ) : \n 
~~~ from theano . sandbox import cuda \n 
if not cuda . cuda_available : \n 
~~ if cuda . use . device_number is None : \n 
~~~ cuda . use ( "gpu" , \n 
force = True , \n 
default_to_move_computation_to_gpu = False , \n 
move_shared_float32_to_gpu = False , \n 
enable_cuda = False ) \n 
~~ v = cuda . fvector ( ) \n 
v_val = cuda . CudaNdarray ( self . rng . randn ( 5 ) . astype ( ) ) \n 
~~ ~~ from __future__ import absolute_import , print_function , division \n 
import errno \n 
from theano . compat import PY3 \n 
from theano . gof . compilelock import get_lock , release_lock \n 
from . import cmodule \n 
if os . path . exists ( os . path . join ( config . compiledir , ) ) : \n 
~~~ os . remove ( os . path . join ( config . compiledir , ) ) \n 
~~ def compile_cutils_code ( ) : \n 
~~~ types = [ + t for t in [ , , , , , \n 
, , , , \n 
] ] \n 
complex_types = [ + t for t in [ , , \n 
, ] ] \n 
fns = . join ( [ inplace_map_template % { : t , : t . upper ( ) , \n 
: floatadd % { : t } } \n 
for t in types ] + \n 
[ inplace_map_template % { : t , : t . upper ( ) , \n 
: complexadd % { : t } } \n 
for t in complex_types ] ) \n 
def gen_binop ( type , typen ) : \n 
. join ( [ gen_binop ( type = t , typen = t . upper ( ) ) \n 
for t in types + complex_types ] ) + "NULL};\\n" ) \n 
def gen_num ( typen ) : \n 
. join ( [ gen_num ( typen = t . upper ( ) ) \n 
for t in types + complex_types ] ) + "-1000};" ) \n 
return code \n 
~~ def compile_cutils ( ) : \n 
code += compile_cutils_code ( ) \n 
if PY3 : \n 
~~~ code = code . replace ( "<Python.h>" , \'"numpy/npy_3kcompat.h"\' , 1 ) \n 
code = code . replace ( "PyCObject" , "NpyCapsule" ) \n 
~~ loc = os . path . join ( config . compiledir , ) \n 
if not os . path . exists ( loc ) : \n 
~~~ os . mkdir ( loc ) \n 
~~ except OSError as e : \n 
~~~ assert e . errno == errno . EEXIST \n 
assert os . path . exists ( loc ) , loc \n 
~~ ~~ args = cmodule . GCC_compiler . compile_args ( ) \n 
cmodule . GCC_compiler . compile_str ( , code , location = loc , \n 
preargs = args ) \n 
~~~ sys . path . insert ( 0 , config . compiledir ) \n 
location = os . path . join ( config . compiledir , ) \n 
if not os . path . exists ( location ) : \n 
~~~ os . mkdir ( location ) \n 
assert os . path . exists ( location ) , location \n 
~~ ~~ if not os . path . exists ( os . path . join ( location , ) ) : \n 
~~~ open ( os . path . join ( location , ) , ) . close ( ) \n 
~~~ get_lock ( ) \n 
~~~ compile_cutils ( ) \n 
~~ ~~ finally : \n 
~~~ release_lock ( ) \n 
~~ ~~ ~~ finally : \n 
~~~ if sys . path [ 0 ] == config . compiledir : \n 
~~~ del sys . path [ 0 ] \n 
from theano import Op , Apply \n 
from theano . tensor import TensorType \n 
from theano . gof . type import CDataType \n 
class ProdOp ( Op ) : \n 
def make_node ( self , i ) : \n 
~~~ return Apply ( self , [ i ] , [ CDataType ( , ) ( ) ] ) \n 
~~ def c_support_code ( self ) : \n 
~~ def c_code ( self , node , name , inps , outs , sub ) : \n 
~~~ return ( 0 , ) \n 
~~ ~~ class GetOp ( Op ) : \n 
def make_node ( self , c ) : \n 
~~~ return Apply ( self , [ c ] , [ TensorType ( , ( False , ) ) ( ) ] ) \n 
~~ ~~ def test_cdata ( ) : \n 
~~ i = TensorType ( , ( False , ) ) ( ) \n 
c = ProdOp ( ) ( i ) \n 
i2 = GetOp ( ) ( c ) \n 
mode = None \n 
if theano . config . mode == "FAST_COMPILE" : \n 
~~~ mode = "FAST_RUN" \n 
~~ f = theano . function ( [ i ] , i2 , mode = mode ) \n 
v = numpy . random . randn ( 9 ) . astype ( ) \n 
v2 = f ( v ) \n 
assert ( v2 == v ) . all ( ) \n 
from theano . compat import izip \n 
from theano . gof import Op , Apply , local_optimizer , EquilibriumDB \n 
from theano . gof . utils import hash_from_dict \n 
from theano . sandbox . cuda import GpuElemwise , CudaNdarrayType , GpuOp \n 
from theano . sandbox . cuda . basic_ops import ( as_cuda_ndarray_variable , \n 
gpu_contiguous ) \n 
from theano . sandbox . cuda . opt import gpu_seqopt \n 
import pycuda \n 
from pycuda . compiler import SourceModule \n 
import pycuda . gpuarray \n 
from . import pycuda_init \n 
if not pycuda_init . pycuda_available : \n 
~~ def _replace_npy_types ( c_arg ) : \n 
~~~ c_arg = c_arg . replace ( , ) \n 
c_arg = c_arg . replace ( , ) \n 
return c_arg \n 
~~ def theano_parse_c_arg ( c_arg ) : \n 
~~~ c_arg = _replace_npy_types ( c_arg ) \n 
return pycuda . tools . parse_c_arg ( c_arg ) \n 
class PycudaElemwiseSourceModuleOp ( GpuOp ) : \n 
~~~ nin = property ( lambda self : self . scalar_op . nin ) \n 
nout = property ( lambda self : self . scalar_op . nout ) \n 
def __init__ ( self , scalar_op , inplace_pattern = None , name = None ) : \n 
~~~ if inplace_pattern is None : \n 
~~~ inplace_pattern = { } \n 
self . scalar_op = scalar_op \n 
self . inplace_pattern = inplace_pattern \n 
~~~ if self . name is None : \n 
~~~ if self . inplace_pattern : \n 
~~~ items = list ( self . inplace_pattern . items ( ) ) \n 
items . sort ( ) \n 
return self . __class__ . __name__ + "{%s}%s" % ( self . scalar_op , \n 
str ( items ) ) \n 
~~~ return self . __class__ . __name__ + "{%s}" % ( self . scalar_op ) \n 
~~~ return self . name \n 
~~ ~~ def __eq__ ( self , other ) : \n 
~~~ return ( type ( self ) == type ( other ) and \n 
self . scalar_op == other . scalar_op and \n 
self . inplace_pattern == other . inplace_pattern ) \n 
~~ def __hash__ ( self ) : \n 
~~~ return ( hash ( type ( self ) ) ^ hash ( self . scalar_op ) ^ \n 
hash_from_dict ( self . inplace_pattern ) ) \n 
~~ def make_node ( self , * inputs ) : \n 
~~~ _inputs = [ gpu_contiguous ( as_cuda_ndarray_variable ( i ) ) for i in inputs ] \n 
if self . nin > 0 and len ( _inputs ) != self . nin : \n 
~~~ raise TypeError ( , ( self . nin , len ( _inputs ) ) ) \n 
~~ for i in _inputs [ 1 : ] : \n 
~~~ if i . type . ndim != inputs [ 0 ] . type . ndim : \n 
~~~ raise TypeError ( ) \n 
~~ ~~ if any ( [ any ( i . type . broadcastable ) for i in inputs ] ) : \n 
otype = CudaNdarrayType ( broadcastable = [ False ] * _inputs [ 0 ] . type . ndim ) \n 
assert self . nout == 1 \n 
fct_name = "pycuda_elemwise_%s" % str ( self . scalar_op ) \n 
out_node = Apply ( self , _inputs , [ otype ( ) for o in xrange ( self . nout ) ] ) \n 
in_name = [ "i" + str ( id ) for id in range ( len ( inputs ) ) ] \n 
out_name = [ "o" + str ( id ) for id in range ( self . nout ) ] \n 
c_code = self . scalar_op . c_code ( out_node , "some_name" , \n 
tuple ( [ n + "[i]" for n in in_name ] ) , \n 
tuple ( n + "[i]" for n in out_name ) , { } ) \n 
for var , name in chain ( izip ( inputs , in_name ) , \n 
izip ( out_node . outputs , out_name ) ) ] + \n 
self . pycuda_fct = mod . get_function ( fct_name ) \n 
return out_node \n 
~~ def perform ( self , node , inputs , out ) : \n 
~~~ z , = out \n 
if ( z [ 0 ] is None or \n 
z [ 0 ] . shape != inputs [ 0 ] . shape or \n 
not z [ 0 ] . is_c_contiguous ( ) ) : \n 
~~~ z [ 0 ] = theano . sandbox . cuda . CudaNdarray . zeros ( inputs [ 0 ] . shape ) \n 
~~ if inputs [ 0 ] . shape != inputs [ 1 ] . shape : \n 
~~~ raise TypeError ( "PycudaElemwiseSourceModuleOp:" \n 
~~ if inputs [ 0 ] . size > 512 : \n 
~~~ grid = ( int ( numpy . ceil ( inputs [ 0 ] . size / 512. ) ) , 1 ) \n 
block = ( 512 , 1 , 1 ) \n 
~~~ grid = ( 1 , 1 ) \n 
block = ( inputs [ 0 ] . shape [ 0 ] , inputs [ 0 ] . shape [ 1 ] , 1 ) \n 
~~ self . pycuda_fct ( inputs [ 0 ] , inputs [ 1 ] , z [ 0 ] , \n 
numpy . intc ( inputs [ 1 ] . size ) , block = block , grid = grid ) \n 
~~ ~~ class PycudaElemwiseSourceModuleMakeThunkOp ( Op ) : \n 
__props__ = ( "scalar_op" , "inplace_pattern" ) \n 
~~~ return hash ( ( type ( self ) , hash ( self . scalar_op ) , \n 
hash_from_dict ( self . inplace_pattern ) ) ) \n 
~~ ~~ def make_node ( self , * inputs ) : \n 
~~~ assert self . nout == 1 \n 
_inputs = [ gpu_contiguous ( as_cuda_ndarray_variable ( i ) ) for i in inputs ] \n 
~~ otype = CudaNdarrayType ( broadcastable = [ False ] * _inputs [ 0 ] . type . ndim ) \n 
~~ def make_thunk ( self , node , storage_map , _ , _2 ) : \n 
~~~ fct_name = "pycuda_elemwise_%s" % str ( self . scalar_op ) \n 
in_name = [ "i" + str ( id ) for id in range ( len ( node . inputs ) ) ] \n 
c_code = self . scalar_op . c_code ( node , "some_name" , \n 
for var , name in chain ( izip ( node . inputs , in_name ) , \n 
izip ( node . outputs , out_name ) ) ] + \n 
pycuda_fct = mod . get_function ( fct_name ) \n 
inputs = [ storage_map [ v ] for v in node . inputs ] \n 
outputs = [ storage_map [ v ] for v in node . outputs ] \n 
def thunk ( ) : \n 
~~~ z = outputs [ 0 ] \n 
z [ 0 ] . shape != inputs [ 0 ] [ 0 ] . shape or \n 
~~~ z [ 0 ] = theano . sandbox . cuda . CudaNdarray . zeros ( \n 
inputs [ 0 ] [ 0 ] . shape ) \n 
~~ if inputs [ 0 ] [ 0 ] . shape != inputs [ 1 ] [ 0 ] . shape : \n 
~~~ raise TypeError ( "PycudaElemwiseSourceModuleMakeThunkOp:" \n 
~~ if inputs [ 0 ] [ 0 ] . size > 512 : \n 
~~~ grid = ( int ( numpy . ceil ( inputs [ 0 ] [ 0 ] . size / 512. ) ) , 1 ) \n 
block = ( inputs [ 0 ] [ 0 ] . shape [ 0 ] , inputs [ 0 ] [ 0 ] . shape [ 1 ] , 1 ) \n 
~~ pycuda_fct ( inputs [ 0 ] [ 0 ] , inputs [ 1 ] [ 0 ] , z [ 0 ] , \n 
numpy . intc ( inputs [ 1 ] [ 0 ] . size ) , block = block , \n 
grid = grid ) \n 
~~ thunk . inputs = inputs \n 
thunk . outputs = outputs \n 
thunk . lazy = False \n 
return thunk \n 
~~ ~~ pycuda_optimizer = EquilibriumDB ( ) \n 
gpu_seqopt . register ( "pycuda_optimizer" , pycuda_optimizer , 1.5 , "fast_run" ) \n 
@ local_optimizer ( [ GpuElemwise ] ) \n 
def local_pycuda_gpu_elemwise ( node ) : \n 
if isinstance ( node . op , GpuElemwise ) : \n 
~~~ if ( not any ( [ any ( i . type . broadcastable ) for i in node . inputs ] ) and \n 
all ( [ i . ndim <= 2 for i in node . inputs ] ) ) : \n 
~~~ new_op = PycudaElemwiseSourceModuleOp ( node . op . scalar_op , \n 
node . op . inplace_pattern ) ( \n 
* node . inputs ) \n 
return [ new_op ] \n 
~~ ~~ ~~ pycuda_optimizer . register ( "local_pycuda_gpu_elemwise" , \n 
local_pycuda_gpu_elemwise ) \n 
import string \n 
from theano . sandbox . cuda import cuda_available , GpuOp \n 
from theano . ifelse import ifelse \n 
from theano . misc . pycuda_init import pycuda_available \n 
if cuda_available : \n 
~~~ from theano . sandbox . cuda import ( basic_ops , CudaNdarrayType , \n 
CudaNdarray ) \n 
~~ if pycuda_available : \n 
~~~ import pycuda . gpuarray \n 
~~~ import scikits . cuda \n 
from scikits . cuda import fft , cublas \n 
scikits . cuda . misc . init ( ) \n 
scikits_cuda_available = True \n 
~~ except ( ImportError , Exception ) : \n 
~~~ scikits_cuda_available = False \n 
~~ class ScikitsCudaOp ( GpuOp ) : \n 
~~~ def __eq__ ( self , other ) : \n 
~~~ return type ( self ) == type ( other ) \n 
~~~ return hash ( type ( self ) ) \n 
~~~ return self . __class__ . __name__ \n 
~~ def output_type ( self , inp ) : \n 
~~ def make_node ( self , inp ) : \n 
~~~ inp = basic_ops . gpu_contiguous ( \n 
basic_ops . as_cuda_ndarray_variable ( inp ) ) \n 
assert inp . dtype == "float32" \n 
return theano . Apply ( self , [ inp ] , [ self . output_type ( inp ) ( ) ] ) \n 
~~~ if not scikits_cuda_available : \n 
~~~ raise RuntimeError ( \n 
~~ ~~ ~~ class CuFFTOp ( ScikitsCudaOp ) : \n 
~~~ def output_type ( self , inp ) : \n 
~~~ return CudaNdarrayType ( \n 
broadcastable = [ False ] * ( inp . type . ndim + 1 ) ) \n 
~~~ super ( CuFFTOp , self ) . make_thunk ( node , storage_map , _ , _2 ) \n 
from theano . misc . pycuda_utils import to_gpuarray \n 
plan_input_shape = [ None ] \n 
plan = [ None ] \n 
~~~ input_shape = inputs [ 0 ] [ 0 ] . shape \n 
output_shape = list ( input_shape ) \n 
output_shape [ - 1 ] = output_shape [ - 1 ] // 2 + 1 \n 
output_shape += [ 2 ] \n 
output_shape = tuple ( output_shape ) \n 
z = outputs [ 0 ] \n 
if z [ 0 ] is None or z [ 0 ] . shape != output_shape : \n 
~~~ z [ 0 ] = CudaNdarray . zeros ( output_shape ) \n 
~~ input_pycuda = to_gpuarray ( inputs [ 0 ] [ 0 ] ) \n 
output_pycuda = to_gpuarray ( z [ 0 ] ) \n 
if plan [ 0 ] is None or plan_input_shape [ 0 ] != input_shape : \n 
~~~ plan_input_shape [ 0 ] = input_shape \n 
plan [ 0 ] = fft . Plan ( input_shape [ 1 : ] , np . float32 , np . complex64 , \n 
batch = input_shape [ 0 ] ) \n 
~~ fft . fft ( input_pycuda , output_pycuda , plan [ 0 ] ) \n 
~~ ~~ class CuIFFTOp ( ScikitsCudaOp ) : \n 
broadcastable = [ False ] * ( inp . type . ndim - 1 ) ) \n 
~~~ super ( CuIFFTOp , self ) . make_thunk ( node , storage_map , _ , _2 ) \n 
output_shape = list ( input_shape [ : - 1 ] ) \n 
output_shape [ - 1 ] = ( output_shape [ - 1 ] - 1 ) * 2 \n 
plan [ 0 ] = fft . Plan ( output_shape [ 1 : ] , np . complex64 , np . float32 , \n 
batch = output_shape [ 0 ] ) \n 
~~ fft . ifft ( input_pycuda , output_pycuda , plan [ 0 ] ) \n 
~~ ~~ def to_complex_gpuarray ( x , copyif = False ) : \n 
if not isinstance ( x , CudaNdarray ) : \n 
~~~ assert x . shape [ - 1 ] == 2 \n 
assert x . dtype == \n 
size = 1 \n 
c_contiguous = True \n 
for i in range ( x . ndim - 1 , - 1 , - 1 ) : \n 
~~~ if x . shape [ i ] == 1 : \n 
~~ if x . _strides [ i ] != size : \n 
~~~ c_contiguous = False \n 
~~ size *= x . shape [ i ] \n 
~~ if not c_contiguous : \n 
~~~ if copyif : \n 
~~~ x = x . copy ( ) \n 
~~ ~~ px = pycuda . gpuarray . GPUArray ( x . shape [ : - 1 ] , np . complex64 , base = x , \n 
gpudata = x . gpudata ) \n 
return px \n 
~~ ~~ def bptrs ( a ) : \n 
return pycuda . gpuarray . arange ( a . ptr , a . ptr + a . shape [ 0 ] * a . strides [ 0 ] , \n 
a . strides [ 0 ] , dtype = cublas . ctypes . c_void_p ) \n 
~~ def sc_complex_dot_batched ( bx_gpu , by_gpu , bc_gpu , transa = , transb = , \n 
handle = None ) : \n 
if handle is None : \n 
~~~ handle = scikits . cuda . misc . _global_cublas_handle \n 
~~ assert len ( bx_gpu . shape ) == 3 \n 
assert len ( by_gpu . shape ) == 3 \n 
assert len ( bc_gpu . shape ) == 3 \n 
assert bx_gpu . dtype == np . complex64 \n 
assert by_gpu . dtype == np . complex64 \n 
assert bc_gpu . dtype == np . complex64 \n 
bx_shape = bx_gpu . shape \n 
by_shape = by_gpu . shape \n 
alpha = np . complex64 ( 1.0 ) \n 
beta = np . complex64 ( 0.0 ) \n 
transa = string . lower ( transa ) \n 
transb = string . lower ( transb ) \n 
if transb in [ , ] : \n 
~~~ N , m , k = by_shape \n 
~~ elif transb in [ ] : \n 
~~~ N , k , m = by_shape \n 
~~ if transa in [ , ] : \n 
~~~ N2 , l , n = bx_shape \n 
~~ elif transa in [ ] : \n 
~~~ N2 , n , l = bx_shape \n 
~~ if l != k : \n 
~~ if N != N2 : \n 
~~ if transb == : \n 
~~~ lda = max ( 1 , m ) \n 
~~~ lda = max ( 1 , k ) \n 
~~ if transa == : \n 
~~~ ldb = max ( 1 , k ) \n 
~~~ ldb = max ( 1 , n ) \n 
~~ ldc = max ( 1 , m ) \n 
bx_arr = bptrs ( bx_gpu ) \n 
by_arr = bptrs ( by_gpu ) \n 
bc_arr = bptrs ( bc_gpu ) \n 
cublas . cublasCgemmBatched ( handle , transb , transa , m , n , k , alpha , \n 
by_arr . gpudata , lda , bx_arr . gpudata , ldb , \n 
beta , bc_arr . gpudata , ldc , N ) \n 
~~ class BatchedComplexDotOp ( ScikitsCudaOp ) : \n 
def make_node ( self , inp1 , inp2 ) : \n 
~~~ inp1 = basic_ops . gpu_contiguous ( \n 
basic_ops . as_cuda_ndarray_variable ( inp1 ) ) \n 
inp2 = basic_ops . gpu_contiguous ( \n 
basic_ops . as_cuda_ndarray_variable ( inp2 ) ) \n 
assert inp1 . dtype == "float32" \n 
assert inp2 . dtype == "float32" \n 
assert inp2 . ndim == 4 \n 
return theano . Apply ( self , [ inp1 , inp2 ] , [ self . output_type ( inp1 ) ( ) ] ) \n 
~~~ return CudaNdarrayType ( broadcastable = [ False ] * inp . type . ndim ) \n 
~~~ super ( BatchedComplexDotOp , self ) . make_thunk ( node , storage_map , _ , _2 ) \n 
~~~ bx = inputs [ 0 ] \n 
by = inputs [ 1 ] \n 
output_shape = ( input_shape_x [ 0 ] , input_shape_x [ 1 ] , \n 
bz = outputs [ 0 ] \n 
if bz [ 0 ] is None or bz [ 0 ] . shape != output_shape : \n 
~~~ bz [ 0 ] = CudaNdarray . zeros ( output_shape ) \n 
~~ input_bx_pycuda = to_complex_gpuarray ( bx [ 0 ] ) \n 
input_by_pycuda = to_complex_gpuarray ( by [ 0 ] ) \n 
output_b_pycuda = to_complex_gpuarray ( bz [ 0 ] ) \n 
sc_complex_dot_batched ( input_bx_pycuda , input_by_pycuda , \n 
output_b_pycuda ) \n 
~~ ~~ cufft = CuFFTOp ( ) \n 
cuifft = CuIFFTOp ( ) \n 
batched_complex_dot = BatchedComplexDotOp ( ) \n 
def mult_and_reduce ( input_fft_v , filters_fft_v , input_shape = None , \n 
filter_shape = None ) : \n 
if input_shape is None : \n 
~~ if filter_shape is None : \n 
~~ b , ic , i0 , i1_f , _ = input_shape \n 
oc = filter_shape [ 0 ] \n 
input_r = input_fft_v . reshape ( ( b , ic , i0 * i1_f , 2 ) ) \n 
filters_r = filters_fft_v . reshape ( ( oc , ic , i0 * i1_f , 2 ) ) \n 
output_s = batched_complex_dot ( input_s , filters_s ) \n 
output_r = output_s . dimshuffle ( 1 , 2 , 0 , 3 ) \n 
output = output_r . reshape ( ( b , oc , i0 , i1_f , 2 ) ) \n 
~~ def conv2d_fft ( input , filters , image_shape = None , filter_shape = None , \n 
border_mode = , pad_last_dim = False ) : \n 
if image_shape is None : \n 
~~~ image_shape = input . shape \n 
~~~ filter_shape = filters . shape \n 
~~ b , ic , i0 , i1 = image_shape \n 
oc , ic_ , f0 , f1 = filter_shape \n 
if border_mode == : \n 
~~~ o0 = i0 \n 
if pad_last_dim : \n 
~~~ o1 = i1 + 1 \n 
input_padded = T . zeros ( ( b , ic , o0 , o1 ) , dtype = ) \n 
input_padded = T . set_subtensor ( input_padded [ : , : , : i0 , : i1 ] , \n 
input ) \n 
~~~ o1 = i1 \n 
input_padded = input \n 
~~ filters_padded = T . zeros ( ( oc , ic , o0 , o1 ) , dtype = ) \n 
filters_padded = T . set_subtensor ( filters_padded [ : , : , : f0 , : f1 ] , \n 
filters ) \n 
~~ elif border_mode == : \n 
~~~ o0 = i0 + 2 * ( f0 - 1 ) \n 
o1 = i1 + 2 * ( f1 - 1 ) \n 
~~~ o1 = o1 + 1 \n 
input_padded = T . set_subtensor ( input_padded [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 ) , ( f1 - 1 ) : ( f1 - 1 + input ) \n 
input_padded , T . eq ( o1 % 2 , 0 ) ) \n 
input_flat = input_padded . reshape ( ( b * ic , o0 , o1 ) ) \n 
filters_flat = filters_padded . reshape ( ( oc * ic , o0 , o1 ) ) \n 
input_fft_v_shape = ( b , ic , o0 , o1 // 2 + 1 , 2 ) \n 
filters_fft_v_shape = ( oc , ic , o0 , o1 // 2 + 1 , 2 ) \n 
input_fft_v = input_fft_flat . reshape ( input_fft_v_shape ) \n 
filters_fft_v = filters_fft_flat . reshape ( filters_fft_v_shape ) \n 
output_fft_s = mult_and_reduce ( input_fft_v , filters_fft_v , \n 
input_shape = input_fft_v_shape , \n 
filter_shape = filters_fft_v_shape ) \n 
output_fft_flat = output_fft_s . reshape ( ( b * oc , o0 , o1 // 2 + 1 , 2 ) ) \n 
~~~ output = output_circ [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 - f0 + 1 ) , ( f1 - 1 ) : ( f1 - 1 + i1 - f1 + 1 ) ] \n 
~~~ output = output_circ [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 + f0 - 1 ) , ( f1 - 1 ) : ( f1 - 1 + i1 + f1 - 1 ) ] \n 
~~ output = ( 1.0 / T . cast ( o0 * o1 , ) ) * output \n 
return basic_ops . as_cuda_ndarray_variable ( output ) \n 
~~ def conv3d_fft ( input , filters , image_shape = None , filter_shape = None , \n 
~~ b , ic , i0 , i1 , i2 = image_shape \n 
oc , ic_ , f0 , f1 , f2 = filter_shape \n 
is_odd = T . eq ( T . mod ( input . shape [ 4 ] , 2 ) , 1 ) \n 
o1 = i1 \n 
o2 = i2 \n 
~~~ o2 = ifelse ( is_odd , o2 + 1 , o2 ) \n 
input_padded = T . zeros ( ( b , ic , o0 , o1 , o2 ) , dtype = ) \n 
input_padded = T . set_subtensor ( input_padded [ : , : , : i0 , : i1 , : i2 ] , \n 
~~ filters_padded = T . zeros ( ( oc , ic , o0 , o1 , o2 ) , dtype = ) \n 
filters_padded = T . set_subtensor ( filters_padded [ : , : , : f0 , : f1 , : f2 ] , \n 
o2 = i2 + 2 * ( f2 - 1 ) \n 
~~ input_flat = input_padded . reshape ( ( b * ic , o0 , o1 , o2 ) ) \n 
filters_flat = filters_padded . reshape ( ( oc * ic , o0 , o1 , o2 ) ) \n 
input_fft_v_shape = ( b , ic , o0 * o1 , o2 // 2 + 1 , 2 ) \n 
filters_fft_v_shape = ( oc , ic , o0 * o1 , o2 // 2 + 1 , 2 ) \n 
output_fft_flat = output_fft_s . reshape ( ( b * oc , o0 , o1 , o2 // 2 + 1 , 2 ) ) \n 
~~~ output = output_circ [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 - f0 + 1 ) , ( f1 - 1 ) : ( f1 - 1 + i1 - f1 + 1 ) , ( f2 - 1 ) : ( f2 - 1 + ~~ elif border_mode == : \n 
~~~ output = output_circ [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 + f0 - 1 ) , ( f1 - 1 ) : ( f1 - 1 + i1 + f1 - 1 ) , ( f2 - 1 ) : ( f2 - 1 + ~~ else : \n 
~~ output = ( 1.0 / T . cast ( o0 * o1 * o2 , ) ) * output \n 
import theano . tests . unittest_tools as utt \n 
import theano . sandbox . cuda as cuda \n 
~~~ raise SkipTest ( ) \n 
~~ if theano . config . mode == : \n 
~~~ mode_with_gpu = theano . compile . mode . get_mode ( ) . including ( ) \n 
mode_without_gpu = theano . compile . mode . get_mode ( ) \n 
~~~ mode_with_gpu = theano . compile . mode . get_default_mode ( ) . including ( ) \n 
mode_without_gpu = theano . compile . mode . get_default_mode ( ) . excluding ( ) \n 
~~ def test_GpuCrossentropySoftmaxArgmax1HotWithBias ( ) : \n 
n_in = 1000 \n 
batch_size = 4097 \n 
n_out = 1250 \n 
if not isinstance ( mode_with_gpu , theano . compile . DebugMode ) : \n 
~~~ n_in = 4098 \n 
n_out = 4099 \n 
~~ y = T . lvector ( ) \n 
b = T . fvector ( ) \n 
dot_result = T . fmatrix ( ) \n 
utt . seed_rng ( ) \n 
xx = numpy . asarray ( numpy . random . rand ( batch_size , n_in ) , \n 
dtype = numpy . float32 ) \n 
yy = numpy . ones ( ( batch_size , ) , dtype = ) \n 
b_values = numpy . zeros ( ( n_out , ) , dtype = ) \n 
W_values = numpy . asarray ( numpy . random . rand ( n_in , n_out ) , dtype = ) \n 
dot_value = numpy . asarray ( numpy . dot ( xx , W_values ) , dtype = ) \n 
del W_values \n 
p_y_given_x = T . nnet . softmax ( dot_result + b ) \n 
y_pred = T . argmax ( p_y_given_x , axis = - 1 ) \n 
loss = - T . mean ( T . log ( p_y_given_x ) [ T . arange ( y . shape [ 0 ] ) , y ] ) \n 
dW = T . grad ( loss , dot_result ) \n 
classify = theano . function ( inputs = [ y , b , dot_result ] , \n 
outputs = [ loss , y_pred , dW ] , \n 
mode = mode_without_gpu ) \n 
classify_gpu = theano . function ( inputs = [ y , b , dot_result ] , \n 
mode = mode_with_gpu ) \n 
assert any ( [ isinstance ( node . op , \n 
T . nnet . CrossentropySoftmaxArgmax1HotWithBias ) \n 
for node in classify . maker . fgraph . toposort ( ) ] ) \n 
cuda . nnet . GpuCrossentropySoftmaxArgmax1HotWithBias ) \n 
for node in classify_gpu . maker . fgraph . toposort ( ) ] ) \n 
out = classify ( yy , b_values , dot_value ) \n 
gout = classify_gpu ( yy , b_values , dot_value ) \n 
assert len ( out ) == len ( gout ) == 3 \n 
assert numpy . allclose ( out [ 0 ] , gout [ 0 ] ) \n 
assert numpy . allclose ( out [ 2 ] , gout [ 2 ] , atol = 3e-6 ) , numpy . absolute ( \n 
gout - out ) . max ( ) \n 
assert numpy . allclose ( out [ 1 ] , gout [ 1 ] ) , [ ( id , out [ 1 ] [ id ] , gout [ 1 ] [ id ] , val ) \n 
for id , val in enumerate ( out [ 1 ] - \n 
gout [ 1 ] ) \n 
if val != 0 ] \n 
~~ def test_GpuCrossentropySoftmax1HotWithBiasDx ( ) : \n 
~~~ n_out = 4099 \n 
~~ utt . seed_rng ( ) \n 
softmax_output_value = numpy . random . rand ( batch_size , \n 
n_out ) . astype ( ) \n 
dnll_value = numpy . asarray ( numpy . random . rand ( batch_size ) , dtype = ) \n 
y_idx_value = numpy . random . randint ( low = 0 , high = 5 , size = batch_size ) \n 
softmax_output = T . fmatrix ( ) \n 
softmax_output /= softmax_output . sum ( axis = 1 ) . reshape ( \n 
softmax_output . shape [ 1 ] , 1 ) \n 
op = theano . tensor . nnet . crossentropy_softmax_1hot_with_bias_dx ( \n 
dnll_value , \n 
softmax_output , \n 
y_idx_value ) \n 
cpu_f = theano . function ( [ softmax_output ] , op , mode = mode_without_gpu ) \n 
gpu_f = theano . function ( [ softmax_output ] , op , mode = mode_with_gpu ) \n 
assert any ( [ isinstance ( node . op , T . nnet . CrossentropySoftmax1HotWithBiasDx ) \n 
for node in cpu_f . maker . fgraph . toposort ( ) ] ) \n 
cuda . nnet . GpuCrossentropySoftmax1HotWithBiasDx ) \n 
for node in gpu_f . maker . fgraph . toposort ( ) ] ) \n 
cpu_out = cpu_f ( softmax_output_value ) \n 
gpu_out = gpu_f ( softmax_output_value ) \n 
rtol = 1e-5 \n 
atol = 1e-6 \n 
if not numpy . allclose ( cpu_out , gpu_out , rtol = rtol , atol = atol ) : \n 
~~~ abs_err , rel_err = T . numeric_grad . abs_rel_err ( cpu_out , gpu_out ) \n 
scaled_err = numpy . minimum ( abs_err / atol , rel_err / rtol ) \n 
max_i = scaled_err . argmax ( ) \n 
print ( , max_i , max_i / batch_size , end = ) \n 
print ( max_i % batch_size , max_i / n_out , max_i & n_out ) \n 
print ( , scaled_err . flatten ( ) [ max_i ] ) \n 
print ( , abs_err . flatten ( ) [ max_i ] ) \n 
print ( , rel_err . flatten ( ) [ max_i ] ) \n 
print ( , cpu_out . flatten ( ) [ max_i ] ) \n 
print ( , gpu_out . flatten ( ) [ max_i ] ) \n 
print ( , softmax_output_value . flatten ( ) [ max_i ] ) \n 
print ( , dnll_value [ max_i / n_out ] ) \n 
print ( , y_idx_value [ max_i / n_out ] ) \n 
rtol , atol ) \n 
~~ ~~ def test_softmax_with_bias ( ) : \n 
x = T . fmatrix ( ) \n 
z = T . nnet . softmax_with_bias ( x , T . arange ( x . shape [ 1 ] * 2 , \n 
dtype = ) [ : : 2 ] ) \n 
f = theano . function ( [ x ] , z , mode = mode_without_gpu ) \n 
f_gpu = theano . function ( [ x ] , z , mode = mode_with_gpu ) \n 
assert f . maker . fgraph . toposort ( ) [ - 1 ] . op == T . nnet . softmax_with_bias \n 
assert isinstance ( f_gpu . maker . fgraph . toposort ( ) [ - 2 ] . op , \n 
cuda . nnet . GpuSoftmaxWithBias ) \n 
def cmp ( n , m ) : \n 
~~~ data = numpy . arange ( n * m , dtype = ) . reshape ( n , m ) \n 
out = f ( data ) \n 
gout = f_gpu ( data ) \n 
assert numpy . allclose ( out , gout ) , numpy . absolute ( out - gout ) \n 
~~ cmp ( 2 , 5 ) \n 
cmp ( 2 << 15 , 5 ) \n 
cmp ( 4074 , 400 ) \n 
cmp ( 0 , 10 ) \n 
cmp ( 784 , 784 ) \n 
cmp ( 4 , 1000 ) \n 
cmp ( 4 , 1024 ) \n 
cmp ( 4 , 2000 ) \n 
cmp ( 4 , 2024 ) \n 
cmp ( 4 , 4074 ) \n 
cmp ( 2 , 10000 ) \n 
cmp ( 128 , 16 * 1024 ) \n 
cmp ( 128 , 64 * 1024 ) \n 
~~ class test_SoftMax ( unittest . TestCase ) : \n 
~~~ gpu_op = cuda . nnet . GpuSoftmax \n 
mode = mode_with_gpu . excluding ( "cudnn" ) \n 
do_big = True \n 
do_0 = True \n 
topo_idx = - 2 \n 
def _test_softmax ( \n 
self , \n 
x , \n 
x_gpu , \n 
f_z , \n 
f_gpu_z , \n 
cmp , \n 
check_types \n 
f_z_out = f_z ( x ) \n 
f_gpu_z_out = f_gpu_z ( x_gpu ) \n 
f = theano . function ( [ x ] , f_z_out , mode = mode_without_gpu ) \n 
f_gpu = theano . function ( [ x_gpu ] , f_gpu_z_out , mode = self . mode ) \n 
check_types ( f , f_gpu ) \n 
cmp ( 1 , 5 , f , f_gpu ) \n 
cmp ( 2 , 5 , f , f_gpu ) \n 
cmp ( 10 , 5 , f , f_gpu ) \n 
cmp ( 100 , 5 , f , f_gpu ) \n 
cmp ( 1000 , 5 , f , f_gpu ) \n 
cmp ( 10000 , 5 , f , f_gpu ) \n 
cmp ( 4074 , 400 , f , f_gpu ) \n 
cmp ( 784 , 784 , f , f_gpu ) \n 
cmp ( 4 , 1000 , f , f_gpu ) \n 
cmp ( 4 , 1024 , f , f_gpu ) \n 
cmp ( 4 , 2000 , f , f_gpu ) \n 
cmp ( 4 , 2024 , f , f_gpu ) \n 
cmp ( 4 , 4074 , f , f_gpu ) \n 
cmp ( 2 , 10000 , f , f_gpu ) \n 
cmp ( 128 , 16 * 1024 , f , f_gpu ) \n 
cmp ( 128 , 64 * 1024 , f , f_gpu ) \n 
cmp ( ( 2 << 15 ) - 1 , 5 , f , f_gpu ) \n 
cmp ( 5 , 2 << 15 , f , f_gpu ) \n 
return f , f_gpu \n 
~~ def _cmp ( self , n , m , f , f_gpu ) : \n 
utt . assert_allclose ( out , gout ) \n 
~~ def _check_types ( self , graph , graph_gpu , f_type , f_gpu_type ) : \n 
~~~ assert isinstance ( graph . maker . fgraph . toposort ( ) [ - 1 ] . op , f_type ) \n 
assert isinstance ( \n 
graph_gpu . maker . fgraph . toposort ( ) [ self . topo_idx ] . op , \n 
f_gpu_type \n 
~~ def test_softmax ( self ) : \n 
~~~ x = T . fmatrix ( ) \n 
z = T . nnet . softmax_op \n 
def check_types ( graph , graph_gpu ) : \n 
~~~ self . _check_types ( \n 
graph , \n 
graph_gpu , \n 
type ( z ) , \n 
self . gpu_op \n 
~~ f , f_gpu = self . _test_softmax ( \n 
z , \n 
self . _cmp , \n 
if self . do_big : \n 
~~~ self . _cmp ( 2 << 15 , 5 , f , f_gpu ) \n 
~~ if self . do_0 : \n 
~~~ self . _cmp ( 0 , 10 , f , f_gpu ) \n 
~~ ~~ ~~ from __future__ import absolute_import , print_function , division \n 
import copy \n 
from theano import tensor , scalar , gof \n 
from theano . compile import optdb \n 
from theano . compile . ops import shape_i \n 
from theano . gof import ( local_optimizer , EquilibriumDB , TopoOptimizer , \n 
SequenceDB , Optimizer , toolbox ) \n 
from theano . gof . optdb import LocalGroupDB \n 
from theano . ifelse import IfElse \n 
from theano . scalar . basic import Scalar , Pow , Cast \n 
from theano . scan_module import scan_utils , scan_op , scan_opt \n 
from theano . tensor . nnet . conv import ConvOp \n 
from theano . tensor . nnet . blocksparse import SparseBlockGemv , SparseBlockOuter \n 
from theano . tensor . nnet . abstract_conv import ( AbstractConv2d , \n 
AbstractConv2d_gradWeights , \n 
AbstractConv2d_gradInputs ) \n 
from theano . tests . breakpoint import PdbBreakpoint \n 
from . type import ( GpuArrayType , GpuArrayConstant , get_context , \n 
ContextNotDefined ) \n 
from . basic_ops import ( as_gpuarray_variable , infer_context_name , \n 
host_from_gpu , GpuToGpu , \n 
HostFromGpu , GpuFromHost , \n 
GpuSplit , GpuContiguous , gpu_contiguous , \n 
GpuAlloc , GpuAllocEmpty , GpuReshape , \n 
GpuEye , gpu_join , GpuJoin ) \n 
from . blas import ( gpu_dot22 , GpuGemv , GpuGemm , GpuGer , GpuGemmBatch , \n 
gpugemm_no_inplace , gpugemmbatch_no_inplace ) \n 
from . blocksparse import GpuSparseBlockGemv , GpuSparseBlockOuter \n 
from . nnet import ( GpuCrossentropySoftmaxArgmax1HotWithBias , \n 
GpuCrossentropySoftmax1HotWithBiasDx , \n 
GpuSoftmaxWithBias , GpuSoftmax ) \n 
from . elemwise import ( GpuElemwise , GpuDimShuffle , GpuCAReduceCuda , \n 
GpuCAReduceCPY ) \n 
from . subtensor import ( GpuIncSubtensor , GpuSubtensor , \n 
GpuAdvancedSubtensor1 , \n 
GpuAdvancedIncSubtensor1 , \n 
GpuAdvancedIncSubtensor1_dev20 ) \n 
from . opt_util import alpha_merge , output_merge \n 
_logger = logging . getLogger ( "theano.sandbox.gpuarray.opt" ) \n 
gpu_optimizer = EquilibriumDB ( ) \n 
gpu_cut_copies = EquilibriumDB ( ) \n 
gpu_seqopt = SequenceDB ( ) \n 
conv_groupopt = LocalGroupDB ( ) \n 
conv_groupopt . __name__ = "gpua_conv_opts" \n 
gpu_seqopt . register ( , gpu_optimizer , 1 , \n 
, , ) \n 
gpu_seqopt . register ( , gpu_cut_copies , 2 , \n 
optdb . register ( , gpu_seqopt , \n 
optdb . __position__ . get ( , 49.5 ) - 1 , \n 
def register_opt ( * tags , ** kwargs ) : \n 
~~~ def f ( local_opt ) : \n 
~~~ name = ( kwargs and kwargs . pop ( ) ) or local_opt . __name__ \n 
gpu_optimizer . register ( name , local_opt , , , * tags ) \n 
return local_opt \n 
~~ def register_inplace ( * tags , ** kwargs ) : \n 
optdb . register ( \n 
name , TopoOptimizer ( \n 
local_opt , failure_callback = TopoOptimizer . warn_inplace ) , \n 
60 , , , , * tags ) \n 
~~ register_opt ( ) ( theano . tensor . opt . local_track_shape_i ) \n 
register_opt ( final_opt = True , name = ) ( \n 
tensor . opt . constant_folding ) \n 
gpu_optimizer . register ( , \n 
theano . tensor . opt . local_remove_all_assert , \n 
def safe_to_gpu ( x , ctx_name ) : \n 
~~~ if isinstance ( x . type , tensor . TensorType ) : \n 
~~~ return GpuFromHost ( ctx_name ) ( x ) \n 
~~~ return x \n 
~~ ~~ def safe_to_cpu ( x ) : \n 
~~~ if isinstance ( x . type , GpuArrayType ) : \n 
~~~ return host_from_gpu ( x ) \n 
~~ ~~ def op_lifter ( OP , cuda_only = False ) : \n 
def f ( maker ) : \n 
~~~ def local_opt ( node ) : \n 
~~~ if type ( node . op ) in OP : \n 
~~~ replace = False \n 
context_name = None \n 
for i in node . inputs : \n 
~~~ if i . owner and i . owner . op == host_from_gpu : \n 
~~~ context_name = i . owner . inputs [ 0 ] . type . context_name \n 
replace = True \n 
~~ ~~ if not replace : \n 
~~~ clients = [ c for o in node . outputs for c in o . clients ] \n 
replace = len ( clients ) != 0 \n 
for c , idx in clients : \n 
~~~ if ( c == or \n 
not isinstance ( c . op , GpuFromHost ) ) : \n 
~~ ~~ if replace : \n 
~~~ context_name = clients [ 0 ] [ 0 ] . op . context_name \n 
~~ ~~ if ( not replace or \n 
( cuda_only and \n 
get_context ( context_name ) . kind != ) ) : \n 
~~ for i in node . inputs : \n 
~~~ i . tag . context_name = context_name \n 
~~ new_op = maker ( node , context_name ) \n 
if new_op and new_op != node . op : \n 
~~~ if isinstance ( new_op , theano . Op ) : \n 
~~~ return [ safe_to_cpu ( o ) for o in \n 
new_op ( * node . inputs , return_list = True ) ] \n 
~~ elif isinstance ( new_op , ( tuple , list ) ) : \n 
~~~ return [ safe_to_cpu ( o ) for o in new_op ] \n 
~~~ return [ host_from_gpu ( new_op ) ] \n 
~~ ~~ ~~ return False \n 
~~ local_opt . __name__ = maker . __name__ \n 
return local_optimizer ( OP ) ( local_opt ) \n 
~~ class InputToGpuOptimizer ( Optimizer ) : \n 
def add_requirements ( self , fgraph ) : \n 
~~~ fgraph . attach_feature ( toolbox . ReplaceValidate ( ) ) \n 
~~ def apply ( self , fgraph ) : \n 
~~~ for input in fgraph . inputs : \n 
~~~ if isinstance ( input . type , GpuArrayType ) : \n 
~~ if ( all ( cl [ 0 ] == or isinstance ( cl [ 0 ] . op , GpuFromHost ) \n 
for cl in input . clients ) ) : \n 
~~ target = getattr ( input . tag , , None ) \n 
if target == : \n 
~~~ new_input = host_from_gpu ( GpuFromHost ( target ) ( input ) ) \n 
fgraph . replace_validate ( input , new_input , \n 
"InputToGpuOptimizer" ) \n 
~~ except ContextNotDefined : \n 
~~~ if hasattr ( input . tag , ) : \n 
~~ pass \n 
~~ ~~ ~~ ~~ gpu_seqopt . register ( , InputToGpuOptimizer ( ) , \n 
0 , , , ) \n 
@ local_optimizer ( [ GpuFromHost , GpuToGpu , host_from_gpu ] ) \n 
def local_cut_gpu_transfers ( node ) : \n 
~~~ if ( isinstance ( node . op , GpuFromHost ) and \n 
node . inputs [ 0 ] . owner and \n 
isinstance ( node . inputs [ 0 ] . owner . op , HostFromGpu ) ) : \n 
~~~ other = node . inputs [ 0 ] . owner . inputs [ 0 ] \n 
if node . op . context_name == other . type . context_name : \n 
~~~ return [ other ] \n 
~~~ return [ GpuToGpu ( node . op . context_name ) ( other ) ] \n 
~~ ~~ elif ( isinstance ( node . op , HostFromGpu ) and \n 
node . inputs [ 0 ] . owner ) : \n 
~~~ n2 = node . inputs [ 0 ] . owner \n 
if isinstance ( n2 . op , GpuFromHost ) : \n 
~~~ return [ n2 . inputs [ 0 ] ] \n 
~~ if isinstance ( n2 . op , GpuToGpu ) : \n 
~~~ return [ host_from_gpu ( n2 . inputs [ 0 ] ) ] \n 
~~ ~~ elif isinstance ( node . op , GpuToGpu ) : \n 
~~~ if node . inputs [ 0 ] . type . context_name == node . op . context_name : \n 
~~~ return [ node . inputs [ 0 ] ] \n 
~~ if node . inputs [ 0 ] . owner : \n 
~~~ return [ GpuFromHost ( node . op . context_name ) ( n2 . inputs [ 0 ] ) ] \n 
~~~ if node . op . context_name == n2 . inputs [ 0 ] . type . context_name : \n 
~~~ return [ node . op ( n2 . inputs [ 0 ] ) ] \n 
~~ ~~ ~~ ~~ ~~ gpu_cut_copies . register ( , local_cut_gpu_transfers , \n 
, , , ) \n 
gpu_cut_copies . register ( , \n 
tensor . opt . constant_folding , \n 
optdb [ ] . register ( , \n 
local_cut_gpu_transfers , \n 
@ register_opt ( ) \n 
@ local_optimizer ( [ tensor . Alloc ] ) \n 
def local_gpuaalloc2 ( node ) : \n 
~~~ get_context ( None ) \n 
~~ if ( isinstance ( node . op , tensor . Alloc ) and \n 
all ( c != and \n 
c . op == tensor . join and \n 
all ( i . owner and \n 
i . owner . op in [ host_from_gpu , tensor . alloc ] \n 
for i in c . inputs [ 1 : ] ) \n 
for c , idx in node . outputs [ 0 ] . clients ) ) : \n 
~~~ return [ host_from_gpu ( GpuAlloc ( None ) ( * node . inputs ) ) ] \n 
~~ ~~ @ register_opt ( ) \n 
@ op_lifter ( [ tensor . Alloc ] ) \n 
def local_gpuaalloc ( node , context_name ) : \n 
~~~ return GpuAlloc ( context_name ) ( * node . inputs ) \n 
~~ @ register_opt ( ) \n 
@ op_lifter ( [ tensor . AllocEmpty ] ) \n 
def local_gpuaallocempty ( node , context_name ) : \n 
~~~ return GpuAllocEmpty ( context_name = context_name , \n 
** node . op . _props_dict ( ) ) ( * node . inputs ) \n 
@ local_optimizer ( [ GpuAlloc ] ) \n 
def local_gpualloc_memset_0 ( node ) : \n 
~~~ if isinstance ( node . op , GpuAlloc ) and not node . op . memset_0 : \n 
~~~ inp = node . inputs [ 0 ] \n 
if ( isinstance ( inp , GpuArrayConstant ) and \n 
inp . data . size == 1 and \n 
( numpy . asarray ( inp . data ) == 0 ) . all ( ) ) : \n 
~~~ new_op = GpuAlloc ( node . op . context_name , memset_0 = True ) \n 
return [ new_op ( * node . inputs ) ] \n 
~~ ~~ ~~ @ gof . local_optimizer ( [ GpuAllocEmpty ] ) \n 
def local_gpua_alloc_empty_to_zeros ( node ) : \n 
~~~ if isinstance ( node . op , GpuAllocEmpty ) : \n 
~~~ context_name = infer_context_name ( * node . inputs ) \n 
z = numpy . asarray ( 0 , dtype = node . outputs [ 0 ] . dtype ) \n 
return [ GpuAlloc ( ) ( as_gpuarray_variable ( z , context_name ) , \n 
* node . inputs ) ] \n 
~~ ~~ optdb . register ( , \n 
theano . tensor . opt . in2out ( local_gpua_alloc_empty_to_zeros ) , \n 
49.3 , \n 
@ local_optimizer ( [ GpuContiguous ] ) \n 
def local_gpu_contiguous_gpu_contiguous ( node ) : \n 
if isinstance ( node . op , GpuContiguous ) : \n 
if inp . owner and isinstance ( inp . owner . op , GpuContiguous ) : \n 
~~~ return [ inp ] \n 
~~ ~~ ~~ @ register_opt ( ) \n 
@ op_lifter ( [ tensor . extra_ops . CpuContiguous ] ) \n 
def local_gpu_contiguous ( node , context_name ) : \n 
~~~ return gpu_contiguous \n 
@ op_lifter ( [ tensor . Reshape ] ) \n 
def local_gpureshape ( node , context_name ) : \n 
~~~ op = node . op \n 
name = op . name \n 
if name : \n 
~~~ name = + name \n 
~~ res = GpuReshape ( op . ndim , op . name ) \n 
@ op_lifter ( [ tensor . Rebroadcast ] ) \n 
def local_gpu_rebroadcast ( node , context_name ) : \n 
~~~ return node . op ( as_gpuarray_variable ( node . inputs [ 0 ] , context_name ) ) \n 
@ op_lifter ( [ tensor . Flatten ] ) \n 
def local_gpuflatten ( node , context_name ) : \n 
shp = [ ] \n 
if op . outdim != 1 : \n 
~~~ shp = [ node . inputs [ 0 ] . shape [ i ] for i in range ( op . outdim - 1 ) ] \n 
~~ shp += [ - 1 ] \n 
res = GpuReshape ( op . outdim , None ) \n 
o = res ( node . inputs [ 0 ] , theano . tensor . as_tensor_variable ( shp ) ) \n 
return o \n 
@ op_lifter ( [ tensor . Elemwise ] ) \n 
def local_gpu_elemwise ( node , context_name ) : \n 
scal_op = op . scalar_op \n 
~~ if len ( node . outputs ) > 1 : \n 
~~ res = GpuElemwise ( scal_op , name = name , \n 
inplace_pattern = copy . copy ( op . inplace_pattern ) , \n 
nfunc_spec = op . nfunc_spec ) \n 
if isinstance ( op . scalar_op , Pow ) : \n 
~~~ out_dtype = node . outputs [ 0 ] . dtype \n 
if out_dtype not in [ , , ] : \n 
~~ new_inputs = [ ] \n 
for inp in node . inputs : \n 
~~~ if inp . dtype != out_dtype : \n 
~~~ gpu_cast_op = GpuElemwise ( Cast ( Scalar ( out_dtype ) ) ) \n 
new_inputs . append ( gpu_cast_op ( as_gpuarray_variable ( inp , context_name ) ) ) \n 
~~~ new_inputs . append ( as_gpuarray_variable ( inp , context_name ) ) \n 
~~ ~~ gpu_output = res ( * new_inputs ) \n 
cpu_output = host_from_gpu ( gpu_output ) \n 
return [ cpu_output ] \n 
~~~ return res \n 
~~ ~~ def max_inputs_to_GpuElemwise ( node ) : \n 
~~~ ptr_size = 8 \n 
int_size = 4 \n 
argument_limit = 232 \n 
ndim = node . inputs [ 0 ] . type . ndim \n 
size_param_mandatory = ( int_size * ( ndim + 1 ) ) + ( ptr_size + int_size * ndim ) * len ( node . outputs ) \n 
nb_bytes_avail = argument_limit - size_param_mandatory \n 
nb_bytes_per_input = ptr_size + ndim * int_size \n 
max_nb_inputs = nb_bytes_avail // nb_bytes_per_input \n 
return max_nb_inputs \n 
~~ gpu_local_elemwise_fusion = tensor . opt . local_elemwise_fusion_op ( \n 
GpuElemwise , \n 
max_inputs_to_GpuElemwise ) \n 
optdb . register ( , \n 
tensor . opt . FusionOptimizer ( gpu_local_elemwise_fusion ) , 71.00 , \n 
inplace_gpu_elemwise_opt = tensor . opt . inplace_elemwise_optimizer_op ( \n 
GpuElemwise ) \n 
optdb . register ( , inplace_gpu_elemwise_opt , 75 , \n 
@ op_lifter ( [ tensor . DimShuffle ] ) \n 
def local_gpua_dimshuffle ( node , context_name ) : \n 
~~~ return GpuDimShuffle ( node . op . input_broadcastable , \n 
node . op . new_order ) \n 
@ op_lifter ( [ tensor . SpecifyShape ] ) \n 
def local_gpua_specifyShape ( node , context_name ) : \n 
~~~ if isinstance ( node . inputs [ 0 ] . type , GpuArrayType ) : \n 
~~ inp = [ GpuFromHost ( context_name ) ( node . inputs [ 0 ] ) ] + node . inputs [ 1 : ] \n 
return tensor . specify_shape ( * inp ) \n 
@ op_lifter ( [ theano . compile . ops . Shape ] ) \n 
def local_gpua_shape ( node , context_name ) : \n 
~~ return [ GpuFromHost ( context_name ) ( node . inputs [ 0 ] ) . shape ] \n 
~~ def gpu_print_wrapper ( op , cnda ) : \n 
~~~ op . old_op . global_fn ( op . old_op , numpy . asarray ( cnda ) ) \n 
@ op_lifter ( [ tensor . printing . Print ] ) \n 
def local_gpu_print_op ( node , context_name ) : \n 
~~~ x , = node . inputs \n 
gpu_x = as_gpuarray_variable ( x , context_name = context_name ) \n 
new_op = node . op . __class__ ( global_fn = gpu_print_wrapper ) \n 
new_op . old_op = node . op \n 
return new_op ( gpu_x ) \n 
@ local_optimizer ( [ PdbBreakpoint ] ) \n 
def local_gpu_pdbbreakpoint_op ( node ) : \n 
~~~ if isinstance ( node . op , PdbBreakpoint ) : \n 
~~~ old_inputs = node . inputs \n 
old_outputs = node . outputs \n 
new_inputs = node . inputs [ : 1 ] \n 
input_transfered = [ ] \n 
nb_monitored_vars = len ( node . outputs ) \n 
for i in range ( nb_monitored_vars ) : \n 
~~~ inp = old_inputs [ i + 1 ] \n 
out = old_outputs [ i ] \n 
input_is_from_gpu = ( inp . owner and \n 
isinstance ( inp . owner . op , HostFromGpu ) ) \n 
output_goes_to_gpu = False \n 
for c in out . clients : \n 
~~~ if c == : \n 
~~ if isinstance ( c [ 0 ] . op , GpuFromHost ) : \n 
~~~ output_goes_to_gpu = True \n 
context_name = c [ 0 ] . op . context_name \n 
~~ ~~ if input_is_from_gpu : \n 
~~~ new_inputs . append ( inp . owner . inputs [ 0 ] ) \n 
input_transfered . append ( True ) \n 
~~ elif output_goes_to_gpu : \n 
~~~ new_inputs . append ( GpuFromHost ( context_name ) ( inp ) ) \n 
~~~ new_inputs . append ( inp ) \n 
input_transfered . append ( False ) \n 
~~ ~~ if not any ( input_transfered ) : \n 
~~ new_op_outputs = node . op ( * new_inputs , return_list = True ) \n 
new_outputs = [ ] \n 
for i in range ( len ( new_op_outputs ) ) : \n 
~~~ if input_transfered [ i ] : \n 
~~~ new_outputs . append ( host_from_gpu ( new_op_outputs [ i ] ) ) \n 
~~~ new_outputs . append ( new_op_outputs [ i ] ) \n 
~~ ~~ return new_outputs \n 
@ op_lifter ( [ IfElse ] ) \n 
def local_gpua_lazy_ifelse ( node , context_name ) : \n 
~~~ if node . op . gpu : \n 
~~ c = node . inputs [ 0 ] \n 
inps = [ ] \n 
for v in node . inputs [ 1 : ] : \n 
~~~ if isinstance ( v . type , ( tensor . TensorType , GpuArrayType ) ) : \n 
~~~ inps . append ( as_gpuarray_variable ( v , context_name ) ) \n 
~~~ inps . append ( v ) \n 
~~ ~~ return IfElse ( node . op . n_outs , gpu = True ) ( c , * inps , return_list = True ) \n 
@ op_lifter ( [ tensor . Join ] ) \n 
def local_gpua_join ( node , context_name ) : \n 
~~~ return gpu_join \n 
@ local_optimizer ( [ GpuJoin ] ) \n 
def local_gpuajoin_1 ( node ) : \n 
~~~ if ( isinstance ( node . op , GpuJoin ) and \n 
len ( node . inputs ) == 2 ) : \n 
~~~ return [ node . inputs [ 1 ] ] \n 
@ op_lifter ( [ tensor . Split ] ) \n 
def local_gpua_split ( node , context_name ) : \n 
~~~ return GpuSplit ( node . op . len_splits ) \n 
@ op_lifter ( [ tensor . Subtensor ] ) \n 
def local_gpua_subtensor ( node , context_name ) : \n 
~~~ x = node . inputs [ 0 ] \n 
if ( x . owner and isinstance ( x . owner . op , HostFromGpu ) ) : \n 
~~~ gpu_x = x . owner . inputs [ 0 ] \n 
if ( gpu_x . owner and \n 
isinstance ( gpu_x . owner . op , GpuFromHost ) and \n 
not gpu_x . owner . inputs [ 0 ] . owner ) : \n 
~~~ if len ( x . clients ) == 1 : \n 
~~~ if any ( [ n == or any ( [ isinstance ( v . type , GpuArrayType ) \n 
for v in n . inputs + n . outputs ] ) \n 
for n , _ in node . outputs [ 0 ] . clients ] ) : \n 
~~~ return [ host_from_gpu ( gpu_x . owner . op ( node . outputs [ 0 ] ) ) ] \n 
~~ ~~ ~~ ~~ return GpuSubtensor ( node . op . idx_list ) \n 
@ op_lifter ( [ tensor . IncSubtensor ] ) \n 
def local_gpua_incsubtensor ( node , context_name ) : \n 
~~~ op = GpuIncSubtensor ( node . op . idx_list , node . op . inplace , \n 
node . op . set_instead_of_inc , \n 
node . op . destroyhandler_tolerate_aliased ) \n 
ret = op ( * node . inputs ) \n 
val = getattr ( node . outputs [ 0 ] . tag , , True ) \n 
ret . tag . nan_guard_mode_check = val \n 
return ret \n 
@ op_lifter ( [ tensor . AdvancedSubtensor1 ] ) \n 
def local_gpua_advanced_subtensor ( node , context_name ) : \n 
~~~ return GpuAdvancedSubtensor1 ( ) \n 
@ op_lifter ( [ tensor . AdvancedIncSubtensor1 ] ) \n 
def local_gpua_advanced_incsubtensor ( node , context_name ) : \n 
~~~ context = get_context ( context_name ) \n 
if context . kind != : \n 
~~ x , y , ilist = node . inputs \n 
if ( x . type . dtype != y . type . dtype ) : \n 
~~~ dtype = scalar . upcast ( x . type . dtype , y . type . dtype ) \n 
if x . type . dtype != dtype : \n 
~~~ x = tensor . cast ( x , dtype ) \n 
~~ if y . type . dtype != dtype : \n 
~~~ y = tensor . cast ( y , dtype ) \n 
~~ ~~ set_instead_of_inc = node . op . set_instead_of_inc \n 
compute_capability = int ( context . bin_id [ - 2 ] ) \n 
if ( compute_capability < 2 or x . ndim != 2 or y . ndim != 2 ) : \n 
~~~ return GpuAdvancedIncSubtensor1 ( \n 
set_instead_of_inc = set_instead_of_inc ) \n 
~~~ return GpuAdvancedIncSubtensor1_dev20 ( \n 
@ op_lifter ( [ tensor . CAReduce , tensor . Sum , tensor . elemwise . Prod ] ) \n 
def local_gpua_careduce ( node , context_name ) : \n 
~~~ if isinstance ( node . op . scalar_op , ( scalar . Add , scalar . Mul , \n 
scalar . Maximum , scalar . Minimum ) ) : \n 
~~~ ctx = get_context ( context_name ) \n 
if ctx . kind == : \n 
~~~ op = GpuCAReduceCPY \n 
if node . op . scalar_op not in [ scalar . add , scalar . mul ] : \n 
~~ ~~ elif ctx . kind == : \n 
~~~ op = GpuCAReduceCuda \n 
~~ x , = node . inputs \n 
greduce = op ( \n 
node . op . scalar_op , axis = node . op . axis , \n 
dtype = getattr ( node . op , , None ) , \n 
acc_dtype = getattr ( node . op , , None ) ) \n 
gvar = greduce ( x ) \n 
if ( op is GpuCAReduceCPY or \n 
gvar . owner . op . supports_c_code ( [ GpuFromHost ( context_name ) ( x ) ] ) ) : \n 
~~~ return greduce \n 
~~~ if node . op . axis is None : \n 
~~~ reduce_mask = [ 1 ] * x . type . ndim \n 
~~~ reduce_mask = [ 0 ] * x . type . ndim \n 
for a in node . op . axis : \n 
~~~ assert reduce_mask [ a ] == 0 \n 
reduce_mask [ a ] = 1 \n 
~~ ~~ shape_of = node . fgraph . shape_feature . shape_of \n 
x_shape = shape_of [ x ] \n 
new_in_shp = [ x_shape [ 0 ] ] \n 
new_mask = [ reduce_mask [ 0 ] ] \n 
for i in xrange ( 1 , x . type . ndim ) : \n 
~~~ if reduce_mask [ i ] == reduce_mask [ i - 1 ] : \n 
~~~ new_in_shp [ - 1 ] *= x_shape [ i ] \n 
~~~ new_mask . append ( reduce_mask [ i ] ) \n 
new_in_shp . append ( x_shape [ i ] ) \n 
~~ ~~ new_axis = [ ] \n 
for idx , m in enumerate ( new_mask ) : \n 
~~~ if m == 1 : \n 
~~~ new_axis . append ( idx ) \n 
~~ ~~ greduce = op ( \n 
node . op . scalar_op , \n 
axis = new_axis , reduce_mask = new_mask , \n 
reshaped_x = x . reshape ( tensor . stack ( new_in_shp ) ) \n 
gpu_reshaped_x = GpuFromHost ( context_name ) ( reshaped_x ) \n 
gvar = greduce ( gpu_reshaped_x ) \n 
reshaped_gpu_inputs = [ gpu_reshaped_x ] \n 
if greduce . supports_c_code ( reshaped_gpu_inputs ) : \n 
~~~ reduce_reshaped_x = host_from_gpu ( \n 
greduce ( gpu_reshaped_x ) ) \n 
if reduce_reshaped_x . ndim != node . outputs [ 0 ] . ndim : \n 
~~~ unreshaped_reduce = reduce_reshaped_x . reshape ( \n 
tensor . stack ( shape_of [ node . outputs [ 0 ] ] ) ) \n 
~~~ unreshaped_reduce = reduce_reshaped_x \n 
~~ return [ unreshaped_reduce ] \n 
~~ ~~ ~~ ~~ @ register_opt ( ) \n 
@ op_lifter ( [ tensor . blas . Gemv , tensor . blas_c . CGemv ] ) \n 
def local_gpua_gemv ( node , context_name ) : \n 
~~~ return GpuGemv ( inplace = node . op . inplace ) \n 
@ op_lifter ( [ tensor . blas . Gemm ] ) \n 
def local_gpua_gemm ( node , context_name ) : \n 
~~~ return GpuGemm ( inplace = node . op . inplace ) \n 
@ op_lifter ( [ tensor . blas . BatchedDot ] ) \n 
def local_gpua_gemmbatch ( node , context_name ) : \n 
~~~ a , b = node . inputs \n 
c = tensor . AllocEmpty ( a . dtype ) ( a . shape [ 0 ] , a . shape [ 1 ] , b . shape [ 2 ] ) \n 
return gpugemmbatch_no_inplace ( c , 1.0 , a , b , 0.0 ) \n 
@ op_lifter ( [ tensor . basic . Dot ] ) \n 
def local_gpua_hgemm ( node , context_name ) : \n 
~~~ from theano . sandbox . cuda import nvcc_compiler \n 
if nvcc_compiler . nvcc_version < : \n 
~~ A = node . inputs [ 0 ] \n 
B = node . inputs [ 1 ] \n 
if ( A . ndim == 2 and B . ndim == 2 and \n 
A . dtype == and B . dtype == ) : \n 
~~~ fgraph = node . inputs [ 0 ] . fgraph \n 
C = GpuAllocEmpty ( dtype = , context_name = context_name ) ( \n 
shape_i ( A , 0 , fgraph ) , \n 
shape_i ( B , 1 , fgraph ) ) \n 
return gpugemm_no_inplace ( C , 1.0 , A , B , 0.0 ) \n 
@ alpha_merge ( GpuGemm , alpha_in = 1 , beta_in = 4 ) \n 
def local_gpuagemm_alpha_merge ( node , * inputs ) : \n 
~~~ return [ gpugemm_no_inplace ( * inputs ) ] \n 
@ output_merge ( GpuGemm , alpha_in = 1 , beta_in = 4 , out_in = 0 ) \n 
def local_gpuagemm_output_merge ( node , * inputs ) : \n 
@ alpha_merge ( GpuGemmBatch , alpha_in = 1 , beta_in = 4 ) \n 
def local_gpuagemmbatch_alpha_merge ( node , * inputs ) : \n 
~~~ return [ gpugemmbatch_no_inplace ( * inputs ) ] \n 
@ output_merge ( GpuGemmBatch , alpha_in = 1 , beta_in = 4 , out_in = 0 ) \n 
def local_gpuagemmbatch_output_merge ( node , * inputs ) : \n 
@ op_lifter ( [ tensor . blas . Ger , tensor . blas_c . CGer , tensor . blas_scipy . ScipyGer ] ) \n 
def local_gpua_ger ( node , context_name ) : \n 
~~~ return GpuGer ( inplace = node . op . destructive ) \n 
@ op_lifter ( [ tensor . blas . Dot22 ] ) \n 
def local_gpua_dot22 ( node , context_name ) : \n 
~~~ return gpu_dot22 \n 
@ op_lifter ( [ tensor . blas . Dot22Scalar ] ) \n 
def local_gpua_dot22scalar ( node , context_name ) : \n 
~~~ x , y , a = node . inputs \n 
x = as_gpuarray_variable ( x , context_name ) \n 
y = as_gpuarray_variable ( y , context_name ) \n 
z = GpuAllocEmpty ( x . dtype , context_name ) ( x . shape [ 0 ] , y . shape [ 1 ] ) \n 
return [ GpuGemm ( inplace = False ) ( z , a , x , y , 0 ) ] \n 
@ op_lifter ( [ tensor . basic . Eye ] ) \n 
def local_gpua_eye ( node , context_name ) : \n 
~~~ return GpuEye ( dtype = node . op . dtype , context_name = context_name ) \n 
@ op_lifter ( [ tensor . nnet . CrossentropySoftmaxArgmax1HotWithBias ] , cuda_only = True ) \n 
def local_gpua_crossentropysoftmaxargmax1hotwithbias ( node , context_name ) : \n 
~~~ return GpuCrossentropySoftmaxArgmax1HotWithBias ( ) \n 
@ op_lifter ( [ tensor . nnet . CrossentropySoftmax1HotWithBiasDx ] , cuda_only = True ) \n 
def local_gpua_crossentropysoftmax1hotwithbiasdx ( node , context_name ) : \n 
~~~ return GpuCrossentropySoftmax1HotWithBiasDx ( ) \n 
@ op_lifter ( [ tensor . nnet . Softmax ] , cuda_only = True ) \n 
def local_gpua_softmax ( node , context_name ) : \n 
~~~ return GpuSoftmax ( ) \n 
@ op_lifter ( [ tensor . nnet . SoftmaxWithBias ] , cuda_only = True ) \n 
def local_gpua_softmaxwithbias ( node , context_name ) : \n 
~~~ return GpuSoftmaxWithBias ( ) \n 
@ op_lifter ( [ theano . tensor . opt . Assert ] ) \n 
def local_assert ( node , context_name ) : \n 
~~~ return [ host_from_gpu ( node . op ( as_gpuarray_variable ( node . inputs [ 0 ] , \n 
context_name ) , \n 
* node . inputs [ 1 : ] ) ) ] \n 
@ op_lifter ( [ ConvOp ] ) \n 
def local_error_convop ( node , context_name ) : \n 
@ op_lifter ( [ SparseBlockGemv ] ) \n 
def local_lift_sparseblockgemv ( node , context_name ) : \n 
~~~ return GpuSparseBlockGemv ( node . op . inplace ) \n 
@ op_lifter ( [ SparseBlockOuter ] ) \n 
def local_lift_sparseblockouter ( node , context_name ) : \n 
~~~ return GpuSparseBlockOuter ( node . op . inplace ) \n 
~~ @ register_inplace ( ) \n 
@ local_optimizer ( [ GpuSparseBlockGemv ] , inplace = True ) \n 
def local_inplace_sparseblockgemv ( node ) : \n 
~~~ if isinstance ( node . op , GpuSparseBlockGemv ) and not node . op . inplace : \n 
~~~ return [ GpuSparseBlockGemv ( inplace = True ) ( * node . inputs ) ] \n 
~~ ~~ @ register_inplace ( ) \n 
@ local_optimizer ( [ GpuSparseBlockOuter ] , inplace = True ) \n 
def local_inplace_sparseblockouter ( node ) : \n 
~~~ if isinstance ( node . op , GpuSparseBlockOuter ) and not node . op . inplace : \n 
~~~ return [ GpuSparseBlockOuter ( inplace = True ) ( * node . inputs ) ] \n 
@ op_lifter ( [ AbstractConv2d , \n 
AbstractConv2d_gradInputs ] ) \n 
def local_lift_abstractconv2d ( node , context_name ) : \n 
~~~ if isinstance ( node . outputs [ 0 ] . type , GpuArrayType ) : \n 
~~ inps = list ( node . inputs ) \n 
inps [ 0 ] = as_gpuarray_variable ( node . inputs [ 0 ] , \n 
context_name = context_name ) \n 
inps [ 1 ] = as_gpuarray_variable ( node . inputs [ 1 ] , \n 
return [ node . op ( * inps ) ] \n 
~~ register_opt ( ) ( conv_groupopt ) \n 
@ register_opt ( "low_memory" ) \n 
@ local_optimizer ( [ GpuCAReduceCuda ] ) \n 
def local_gpu_elemwise_careduce ( node ) : \n 
if ( isinstance ( node . op , GpuCAReduceCuda ) and \n 
node . op . pre_scalar_op is None and \n 
isinstance ( node . inputs [ 0 ] . owner . op , GpuElemwise ) and \n 
isinstance ( node . inputs [ 0 ] . owner . op . scalar_op , scalar . basic . Sqr ) ) : \n 
inp = node . inputs [ 0 ] . owner . inputs [ 0 ] \n 
return [ GpuCAReduceCuda ( scalar_op = op . scalar_op , \n 
axis = op . axis , \n 
reduce_mask = op . reduce_mask , \n 
pre_scalar_op = scalar . basic . sqr ) ( inp ) ] \n 
~~ ~~ def tensor_to_gpu ( x , context_name ) : \n 
~~~ y = GpuArrayType ( broadcastable = x . type . broadcastable , \n 
context_name = context_name , \n 
dtype = x . type . dtype ) ( ) \n 
if x . name : \n 
~~~ y . name = x . name + \n 
~~ return y \n 
~~ ~~ def gpu_safe_new ( x , tag = ) : \n 
if hasattr ( x , ) and x . name is not None : \n 
~~~ nw_name = x . name + tag \n 
~~~ nw_name = None \n 
~~ if isinstance ( x , theano . Constant ) : \n 
~~~ return x . clone ( ) \n 
~~ nw_x = x . type ( ) \n 
nw_x . name = nw_name \n 
return nw_x \n 
~~ def gpu_reconstruct_graph ( inputs , outputs , tag = None ) : \n 
if tag is None : \n 
~~~ tag = \n 
~~ nw_inputs = [ gpu_safe_new ( x , tag ) for x in inputs ] \n 
givens = { } \n 
for nw_x , x in zip ( nw_inputs , inputs ) : \n 
~~~ givens [ x ] = nw_x \n 
~~ nw_outputs = scan_utils . clone ( outputs , replace = givens ) \n 
return ( nw_inputs , nw_outputs ) \n 
~~ @ register_opt ( , ) \n 
@ op_lifter ( [ scan_op . Scan ] ) \n 
def local_scan_to_gpua ( node , context_name ) : \n 
~~~ info = copy . deepcopy ( node . op . info ) \n 
if info . get ( , False ) : \n 
~~ info [ ] = True \n 
nw_ins = [ node . inputs [ 0 ] ] \n 
e = ( 1 + \n 
node . op . n_seqs + \n 
node . op . n_mit_mot + \n 
node . op . n_mit_sot + \n 
node . op . n_sit_sot + \n 
node . op . n_shared_outs ) \n 
nw_ins += [ safe_to_gpu ( x , context_name ) for x in node . inputs [ 1 : e ] ] \n 
b = e \n 
e = e + node . op . n_nit_sot \n 
nw_ins += node . inputs [ b : e ] \n 
nw_ins += [ safe_to_gpu ( x , context_name ) for x in node . inputs [ e : ] ] \n 
scan_ins = [ tensor_to_gpu ( x , context_name ) for x in node . op . inputs ] \n 
if node . op . info [ ] : \n 
~~~ scan_outs = [ safe_to_gpu ( x , context_name ) for x in node . op . outputs [ : - 1 ] ] \n 
scan_outs += [ node . op . outputs [ - 1 ] ] \n 
~~~ scan_outs = [ safe_to_gpu ( x , context_name ) for x in node . op . outputs ] \n 
~~ scan_outs = scan_utils . clone ( \n 
scan_outs , \n 
replace = list ( zip ( node . op . inputs , \n 
( safe_to_cpu ( x ) for x in scan_ins ) ) ) ) \n 
tmp_in , tmp_out = gpu_reconstruct_graph ( scan_ins , scan_outs ) \n 
local_fgraph = gof . FunctionGraph ( tmp_in , tmp_out , clone = True ) \n 
_cmodule_key = gof . CLinker ( ) . cmodule_key_ ( local_fgraph , [ ] ) \n 
info [ ] = hash ( _cmodule_key ) \n 
def typebuild ( dtype , broadcastable , context_name = context_name ) : \n 
~~~ return GpuArrayType ( dtype = dtype , broadcastable = broadcastable , \n 
~~ nw_op = scan_op . Scan ( scan_ins , scan_outs , info , \n 
typeConstructor = typebuild ) . make_node ( * nw_ins ) \n 
return nw_op . outputs \n 
~~ def _scan_type_infer ( node ) : \n 
~~ return typebuild \n 
~~ optdb . register ( , \n 
scan_opt . ScanInplaceOptimizer ( typeInfer = _scan_type_infer , \n 
gpua_flag = True ) , \n 
75 , \n 
from six import integer_types \n 
from theano import Op , Apply , shared , config , Variable \n 
from theano import gradient , function \n 
from theano import tensor \n 
from theano . tensor import ( TensorType , as_tensor_variable , get_vector_length , \n 
cast , opt , scal ) \n 
from theano . tensor import sqrt , log , sin , cos , join , prod \n 
from theano . gof import local_optimizer \n 
from . import multinomial \n 
import theano . sandbox . cuda \n 
from theano . sandbox . cuda import GpuOp \n 
from theano . sandbox . gpuarray . basic_ops import GpuKernelBase , Kernel \n 
from theano . sandbox . gpuarray . type import GpuArrayType \n 
from theano . sandbox . gpuarray . fp16_help import write_w \n 
from theano . sandbox . gpuarray . opt import ( register_opt as register_gpua , \n 
host_from_gpu as host_from_gpua ) \n 
if theano . sandbox . cuda . cuda_available : \n 
~~~ from theano . sandbox . cuda import ( CudaNdarrayType , \n 
float32_shared_constructor ) \n 
~~ def matVecModM ( A , s , m ) : \n 
~~~ assert A . dtype == \n 
return numpy . int32 ( numpy . sum ( ( A * s ) % m , 1 ) % m ) \n 
~~ def multMatVect ( v , A , m1 , B , m2 ) : \n 
if multMatVect . dot_modulo is None : \n 
~~~ A_sym = tensor . lmatrix ( ) \n 
s_sym = tensor . ivector ( ) \n 
m_sym = tensor . iscalar ( ) \n 
A2_sym = tensor . lmatrix ( ) \n 
s2_sym = tensor . ivector ( ) \n 
m2_sym = tensor . iscalar ( ) \n 
o = DotModulo ( ) ( A_sym , s_sym , m_sym , A2_sym , s2_sym , m2_sym ) \n 
multMatVect . dot_modulo = function ( \n 
[ A_sym , s_sym , m_sym , A2_sym , s2_sym , m2_sym ] , o , profile = False ) \n 
~~ f = multMatVect . dot_modulo \n 
f . input_storage [ 0 ] . storage [ 0 ] = A \n 
f . input_storage [ 1 ] . storage [ 0 ] = v [ : 3 ] \n 
f . input_storage [ 2 ] . storage [ 0 ] = m1 \n 
f . input_storage [ 3 ] . storage [ 0 ] = B \n 
f . input_storage [ 4 ] . storage [ 0 ] = v [ 3 : ] \n 
f . input_storage [ 5 ] . storage [ 0 ] = m2 \n 
f . fn ( ) \n 
r = f . output_storage [ 0 ] . storage [ 0 ] \n 
~~ multMatVect . dot_modulo = None \n 
class DotModulo ( Op ) : \n 
def make_node ( self , A , s , m , A2 , s2 , m2 ) : \n 
~~~ return Apply ( self , [ A , s , m , A2 , s2 , m2 ] , [ s . type ( ) ] ) \n 
~~ def perform ( self , node , inputs , outputs ) : \n 
~~~ ( A , s , m , A2 , s2 , m2 ) = inputs \n 
( out , ) = outputs \n 
o1 = matVecModM ( A , s , m ) \n 
o2 = matVecModM ( A2 , s2 , m2 ) \n 
out [ 0 ] = numpy . concatenate ( ( o1 , o2 ) ) \n 
~~~ return ( 6 , ) \n 
~~ def c_code ( self , node , name , inputs , outputs , sub ) : \n 
~~~ ( _A , _s , _m , _A2 , _s2 , _m2 ) = inputs \n 
( _z , ) = outputs \n 
MULT2 = numpy . int32 ( 21069 ) \n 
A1p72 = numpy . asarray ( [ [ 1516919229 , 758510237 , 499121365 ] , \n 
[ 1884998244 , 1516919229 , 335398200 ] , \n 
[ 601897748 , 1884998244 , 358115744 ] ] , \n 
dtype = ) \n 
A2p72 = numpy . asarray ( [ [ 1228857673 , 1496414766 , 954677935 ] , \n 
[ 1133297478 , 1407477216 , 1496414766 ] , \n 
[ 2002613992 , 1639496704 , 1407477216 ] ] , \n 
A1p134 = numpy . asarray ( \n 
[ [ 1702500920 , 1849582496 , 1656874625 ] , \n 
[ 828554832 , 1702500920 , 1512419905 ] , \n 
[ 1143731069 , 828554832 , 102237247 ] ] , \n 
A2p134 = numpy . asarray ( \n 
[ [ 796789021 , 1464208080 , 607337906 ] , \n 
[ 1241679051 , 1431130166 , 1464208080 ] , \n 
[ 1401213391 , 1178684362 , 1431130166 ] ] , \n 
np_int32_vals = [ numpy . int32 ( i ) for i in ( 0 , 7 , 9 , 15 , 16 , 22 , 24 ) ] \n 
def ff_2p134 ( rstate ) : \n 
~~~ return multMatVect ( rstate , A1p134 , M1 , A2p134 , M2 ) \n 
~~ def ff_2p72 ( rstate ) : \n 
~~~ return multMatVect ( rstate , A1p72 , M1 , A2p72 , M2 ) \n 
~~ def mrg_next_value ( rstate , new_rstate ) : \n 
~~~ x11 , x12 , x13 , x21 , x22 , x23 = rstate \n 
assert type ( x11 ) == numpy . int32 \n 
i0 , i7 , i9 , i15 , i16 , i22 , i24 = np_int32_vals \n 
y1 = ( ( ( x12 & MASK12 ) << i22 ) + ( x12 >> i9 ) + \n 
( ( x13 & MASK13 ) << i7 ) + ( x13 >> i24 ) ) \n 
assert type ( y1 ) == numpy . int32 \n 
~~~ y1 -= M1 \n 
~~ y1 += x13 \n 
if ( y1 < 0 or y1 >= M1 ) : \n 
~~ x13 = x12 \n 
x12 = x11 \n 
x11 = y1 \n 
y1 = ( ( x21 & MASK2 ) << i15 ) + ( MULT2 * ( x21 >> i16 ) ) \n 
if ( y1 < 0 or y1 >= M2 ) : \n 
~~~ y1 -= M2 \n 
~~ y2 = ( ( x23 & MASK2 ) << i15 ) + ( MULT2 * ( x23 >> i16 ) ) \n 
assert type ( y2 ) == numpy . int32 \n 
if ( y2 < 0 or y2 >= M2 ) : \n 
~~~ y2 -= M2 \n 
~~ y2 += x23 \n 
~~ y2 += y1 \n 
~~ x23 = x22 \n 
x22 = x21 \n 
x21 = y2 \n 
new_rstate [ ... ] = [ x11 , x12 , x13 , x21 , x22 , x23 ] \n 
assert new_rstate . dtype == numpy . int32 \n 
if ( x11 <= x21 ) : \n 
~~~ return ( x11 - x21 + M1 ) * NORM \n 
~~~ return ( x11 - x21 ) * NORM \n 
~~ ~~ class mrg_uniform_base ( Op ) : \n 
~~~ __props__ = ( "output_type" , "inplace" ) \n 
def __init__ ( self , output_type , inplace = False ) : \n 
~~~ Op . __init__ ( self ) \n 
self . output_type = output_type \n 
self . inplace = inplace \n 
if inplace : \n 
~~~ self . destroy_map = { 0 : [ 0 ] } \n 
~~ self . warned_numpy_version = False \n 
~~~ if self . inplace : \n 
~~~ s = "inplace" \n 
~~~ s = "no_inplace" \n 
~~ return self . __class__ . __name__ + "{%s,%s}" % ( self . output_type , s ) \n 
~~ def make_node ( self , rstate , size ) : \n 
~~~ return Apply ( self , \n 
[ rstate , size ] , \n 
[ rstate . type ( ) , self . output_type ( ) ] ) \n 
~~ def grad ( self , inputs , ograd ) : \n 
~~~ return [ gradient . grad_undefined ( self , k , inp , \n 
for k , inp in enumerate ( inputs ) ] \n 
~~ def R_op ( self , inputs , eval_points ) : \n 
~~~ return [ None for i in eval_points ] \n 
~~ ~~ class mrg_uniform ( mrg_uniform_base ) : \n 
def new ( cls , rstate , ndim , dtype , size ) : \n 
~~~ v_size = as_tensor_variable ( size ) \n 
if ndim is None : \n 
~~~ ndim = get_vector_length ( v_size ) \n 
~~ op = cls ( TensorType ( dtype , ( False , ) * ndim ) ) \n 
return op ( rstate , v_size ) \n 
~~~ rstate , size = inp \n 
o_rstate , o_sample = out \n 
n_elements = 1 \n 
for s in size : \n 
~~~ n_elements *= s \n 
~~ if n_elements > M1 : \n 
if not self . inplace : \n 
~~~ rstate = rstate . copy ( ) \n 
~~ n_streams , _ = rstate . shape \n 
rval = numpy . zeros ( n_elements , dtype = self . output_type . dtype ) \n 
err_orig = numpy . seterr ( over = ) \n 
~~~ for i in xrange ( n_elements ) : \n 
~~~ sample = mrg_next_value ( rstate [ i % n_streams ] , \n 
rstate [ i % n_streams ] ) \n 
rval [ i ] = sample \n 
~~~ numpy . seterr ( ** err_orig ) \n 
~~ o_rstate [ 0 ] = node . outputs [ 0 ] . type . filter ( rstate ) \n 
o_sample [ 0 ] = node . outputs [ 1 ] . type . filter ( rval . reshape ( size ) ) \n 
assert isinstance ( node . inputs [ 0 ] . type , TensorType ) \n 
if self . inplace : \n 
~~~ o_rstate_requirement = ( \n 
~~ ndim = self . output_type . ndim \n 
o_type_num = numpy . asarray ( 0 , dtype = self . output_type . dtype ) . dtype . num \n 
fail = sub [ ] \n 
if self . output_type . dtype == : \n 
~~~ otype = \n 
NORM = \n 
~~~ return ( 8 , ) \n 
~~ ~~ class GPU_mrg_uniform ( mrg_uniform_base , GpuOp ) : \n 
~~ op = cls ( CudaNdarrayType ( ( False , ) * ndim ) ) \n 
~~ def c_support_code_apply ( self , node , nodename ) : \n 
~~~ if self . output_type . dtype == : \n 
~~ def c_code ( self , node , nodename , inp , out , sub ) : \n 
inplace = int ( self . inplace ) \n 
ndim = self . output_type . ndim \n 
~~ SYNC = "CNDA_THREAD_SYNC" \n 
~~~ return ( 12 , ) \n 
~~ ~~ class GPUA_mrg_uniform ( GpuKernelBase , mrg_uniform_base ) : \n 
~~~ _f16_ok = True \n 
def get_params ( self , node ) : \n 
~~~ return node . inputs [ 0 ] . type . context \n 
~~ @ classmethod \n 
~~ op = cls ( GpuArrayType ( dtype , ( False , ) * ndim ) ) \n 
~~ def c_headers ( self ) : \n 
~~~ return super ( GPUA_mrg_uniform , self ) . c_headers ( ) + [ ] \n 
~~ def gpu_kernels ( self , node , name ) : \n 
~~~ write = write_w ( self . output_type . dtype ) \n 
mask = \n 
~~ elif self . output_type . dtype == : \n 
~~~ raise ValueError ( , \n 
self . output_type . dtype ) \n 
from pygpu import gpuarray \n 
return [ Kernel ( code = code , name = "mrg_uniform" , \n 
params = [ gpuarray . GpuArray , gpuarray . GpuArray , \n 
, ] , \n 
flags = Kernel . get_flags ( self . output_type . dtype , ) ) \n 
ctx = sub [ ] \n 
kname = self . gpu_kernels ( node , nodename ) [ 0 ] . objvar \n 
otypecode = str ( self . output_type . typecode ) \n 
~~~ return ( 11 , ) \n 
~~ ~~ def guess_n_streams ( size , warn = False ) : \n 
if ( isinstance ( size , ( tuple , list ) ) and \n 
all ( [ isinstance ( i , integer_types ) for i in size ] ) ) : \n 
~~~ r = 1 \n 
~~~ r *= s \n 
~~ if r > 6 : \n 
~~ return min ( r , 60 * 256 ) \n 
~~~ if warn : \n 
~~~ warnings . warn ( \n 
stacklevel = 3 ) \n 
~~ return 60 * 256 \n 
~~ ~~ class MRG_RandomStreams ( object ) : \n 
def updates ( self ) : \n 
~~~ return list ( self . state_updates ) \n 
~~ def __init__ ( self , seed = 12345 , use_cuda = None ) : \n 
~~~ self . state_updates = [ ] \n 
super ( MRG_RandomStreams , self ) . __init__ ( ) \n 
self . default_instance_seed = seed \n 
self . set_rstate ( seed ) \n 
if use_cuda is None : \n 
~~~ self . use_cuda = theano . sandbox . cuda . cuda_enabled \n 
~~~ self . use_cuda = use_cuda \n 
~~ ~~ def set_rstate ( self , seed ) : \n 
~~~ if isinstance ( seed , integer_types ) : \n 
~~~ if seed == 0 : \n 
~~~ raise ValueError ( , seed ) \n 
~~ elif seed >= M2 : \n 
~~~ raise ValueError ( % M2 , seed ) \n 
~~ self . rstate = numpy . asarray ( [ seed ] * 6 , dtype = ) \n 
~~ elif len ( seed ) == 6 : \n 
~~~ if seed [ 0 ] == 0 and seed [ 1 ] == 0 and seed [ 2 ] == 0 : \n 
, seed ) \n 
~~ if seed [ 3 ] == 0 and seed [ 4 ] == 0 and seed [ 5 ] == 0 : \n 
~~ if seed [ 0 ] >= M1 or seed [ 1 ] >= M1 or seed [ 2 ] >= M1 : \n 
% M1 , \n 
seed ) \n 
~~ if seed [ 3 ] >= M2 or seed [ 4 ] >= M2 or seed [ 5 ] >= M2 : \n 
% M2 , \n 
~~ self . rstate = numpy . asarray ( seed , dtype = ) \n 
~~ ~~ def seed ( self , seed = None ) : \n 
if seed is None : \n 
~~~ seed = self . default_instance_seed \n 
~~ self . set_rstate ( seed ) \n 
for old_r , new_r , size , nstreams in self . state_updates : \n 
~~~ if nstreams is None : \n 
~~~ nstreams = self . n_streams ( size ) \n 
~~ rstates = self . get_substream_rstates ( nstreams , \n 
new_r . owner . outputs [ 1 ] . dtype ) \n 
assert ( old_r . get_value ( borrow = True , \n 
return_internal_type = True ) . shape == \n 
rstates . shape ) \n 
assert rstates . dtype == old_r . dtype \n 
old_r . set_value ( rstates , borrow = True ) \n 
~~ ~~ def inc_rstate ( self ) : \n 
self . rstate = multMatVect ( self . rstate , A1p134 , M1 , A2p134 , M2 ) \n 
assert self . rstate . dtype == numpy . int32 \n 
~~ @ theano . configparser . change_flags ( compute_test_value = ) \n 
def get_substream_rstates ( self , n_streams , dtype , inc_rstate = True ) : \n 
assert isinstance ( dtype , str ) \n 
assert n_streams < 2 ** 72 \n 
assert n_streams > 0 \n 
rval = numpy . zeros ( ( n_streams , 6 ) , dtype = ) \n 
rval [ 0 ] = self . rstate \n 
~~~ multMatVect ( rval [ 0 ] , A1p72 , M1 , A2p72 , M2 ) \n 
f . input_storage [ 0 ] . storage [ 0 ] = A1p72 \n 
f . input_storage [ 2 ] . storage [ 0 ] = M1 \n 
f . input_storage [ 3 ] . storage [ 0 ] = A2p72 \n 
f . input_storage [ 5 ] . storage [ 0 ] = M2 \n 
for i in xrange ( 1 , n_streams ) : \n 
~~~ v = rval [ i - 1 ] \n 
rval [ i ] = f . output_storage [ 0 ] . storage [ 0 ] \n 
~~ if inc_rstate : \n 
~~~ self . inc_rstate ( ) \n 
~~ if self . use_cuda and dtype == : \n 
~~~ rval = rval . flatten ( ) \n 
tmp_float_buf = numpy . frombuffer ( rval . data , dtype = ) \n 
assert tmp_float_buf . shape == rval . shape \n 
assert ( tmp_float_buf . view ( ) == rval ) . all ( ) \n 
rval = tmp_float_buf \n 
~~ return rval \n 
~~ def n_streams ( self , size ) : \n 
~~~ return guess_n_streams ( size ) \n 
~~ def pretty_return ( self , node_rstate , new_rstate , sample , size , nstreams ) : \n 
~~~ sample . rstate = node_rstate \n 
sample . update = ( node_rstate , new_rstate ) \n 
self . state_updates . append ( ( node_rstate , new_rstate , size , nstreams ) ) \n 
node_rstate . default_update = new_rstate \n 
~~ def uniform ( self , size , low = 0.0 , high = 1.0 , ndim = None , dtype = None , \n 
nstreams = None ) : \n 
low = as_tensor_variable ( low ) \n 
high = as_tensor_variable ( high ) \n 
if dtype is None : \n 
~~~ dtype = scal . upcast ( config . floatX , low . dtype , high . dtype ) \n 
~~ low = cast ( low , dtype = dtype ) \n 
high = cast ( high , dtype = dtype ) \n 
if isinstance ( size , tuple ) : \n 
assert all ( [ isinstance ( i , ( numpy . integer , integer_types , Variable ) ) \n 
for i in size ] ) , msg \n 
if any ( [ isinstance ( i , ( numpy . integer , integer_types ) ) and i <= 0 \n 
for i in size ] ) : \n 
size ) \n 
~~~ if not ( isinstance ( size , Variable ) and size . ndim == 1 ) : \n 
~~ ~~ orig_nstreams = nstreams \n 
if nstreams is None : \n 
~~ rstates = self . get_substream_rstates ( nstreams , dtype ) \n 
if self . use_cuda and dtype == : \n 
~~~ node_rstate = float32_shared_constructor ( rstates ) \n 
assert isinstance ( node_rstate . type , CudaNdarrayType ) \n 
u = self . pretty_return ( node_rstate , \n 
* GPU_mrg_uniform . new ( node_rstate , \n 
ndim , dtype , size ) , \n 
size = size , nstreams = orig_nstreams ) \n 
~~~ node_rstate = shared ( rstates ) \n 
* mrg_uniform . new ( node_rstate , \n 
~~ node_rstate . tag . is_rng = True \n 
r = u * ( high - low ) + low \n 
if u . type . broadcastable != r . type . broadcastable : \n 
~~~ raise NotImplementedError ( \n 
~~ assert r . dtype == dtype \n 
~~ def binomial ( self , size = None , n = 1 , p = 0.5 , ndim = None , dtype = , \n 
~~~ if n == 1 : \n 
~~~ if dtype == and self . use_cuda : \n 
~~~ x = self . uniform ( size = size , dtype = dtype , nstreams = nstreams ) \n 
~~~ x = self . uniform ( size = size , nstreams = nstreams ) \n 
~~ return cast ( x < p , dtype ) \n 
~~ ~~ def multinomial ( self , size = None , n = 1 , pvals = None , ndim = None , dtype = , \n 
if pvals is None : \n 
~~ pvals = as_tensor_variable ( pvals ) \n 
if size is not None : \n 
~~~ if any ( [ isinstance ( i , integer_types ) and i <= 0 for i in size ] ) : \n 
~~ ~~ if size is not None : \n 
~~ if ndim is not None : \n 
~~ if pvals . ndim == 2 : \n 
~~~ size = pvals [ : , 0 ] . shape * n \n 
unis = self . uniform ( size = size , ndim = 1 , nstreams = nstreams ) \n 
op = multinomial . MultinomialFromUniform ( dtype ) \n 
n_samples = as_tensor_variable ( n ) \n 
return op ( pvals , unis , n_samples ) \n 
~~ ~~ def multinomial_wo_replacement ( self , size = None , n = 1 , pvals = None , \n 
ndim = None , dtype = , nstreams = None ) : \n 
op = multinomial . MultinomialWOReplacementFromUniform ( dtype ) \n 
~~ ~~ def normal ( self , size , avg = 0.0 , std = 1.0 , ndim = None , \n 
dtype = None , nstreams = None ) : \n 
avg = as_tensor_variable ( avg ) \n 
std = as_tensor_variable ( std ) \n 
~~~ dtype = scal . upcast ( config . floatX , avg . dtype , std . dtype ) \n 
~~ avg = cast ( avg , dtype ) \n 
std = cast ( std , dtype ) \n 
evened = False \n 
constant = False \n 
if ( isinstance ( size , tuple ) and \n 
all ( [ isinstance ( i , ( numpy . integer , integer_types ) ) for i in size ] ) ) : \n 
~~~ constant = True \n 
n_samples = numpy . prod ( size , dtype = ) \n 
if n_samples % 2 == 1 : \n 
~~~ n_samples += 1 \n 
evened = True \n 
~~~ n_samples = prod ( size ) + ( prod ( size ) % 2 ) \n 
~~ flattened = self . uniform ( size = ( n_samples , ) , dtype = dtype , \n 
nstreams = nstreams ) \n 
if constant : \n 
~~~ U1 = flattened [ : n_samples // 2 ] \n 
U2 = flattened [ n_samples // 2 : ] \n 
~~~ U1 = flattened [ : prod ( flattened . shape ) // 2 ] \n 
U2 = flattened [ prod ( flattened . shape ) // 2 : ] \n 
~~ sqrt_ln_U1 = sqrt ( - 2.0 * log ( U1 ) ) \n 
first_half = sqrt_ln_U1 * cos ( \n 
numpy . array ( 2.0 * numpy . pi , dtype = dtype ) * U2 ) \n 
second_half = sqrt_ln_U1 * sin ( \n 
normal_samples = join ( 0 , first_half , second_half ) \n 
final_samples = None \n 
if evened : \n 
~~~ final_samples = normal_samples [ : - 1 ] \n 
~~ elif constant : \n 
~~~ final_samples = normal_samples \n 
~~~ final_samples = normal_samples [ : prod ( size ) ] \n 
~~ if not size : \n 
~~~ size = tensor . constant ( size , dtype = ) \n 
~~ final_samples = final_samples . reshape ( size ) \n 
final_samples = avg + std * final_samples \n 
assert final_samples . dtype == dtype \n 
return final_samples \n 
~~ ~~ @ register_gpua ( ) \n 
@ local_optimizer ( [ mrg_uniform ] ) \n 
def local_gpua_mrg ( node ) : \n 
~~~ if ( type ( node . op ) == mrg_uniform and \n 
isinstance ( node . inputs [ 0 ] . type , GpuArrayType ) ) : \n 
~~~ outs = GPUA_mrg_uniform . new ( node . inputs [ 0 ] , \n 
node . op . output_type . ndim , \n 
node . op . output_type . dtype , \n 
node . inputs [ 1 ] ) \n 
return [ outs [ 0 ] , host_from_gpua ( outs [ 1 ] ) ] \n 
~~ ~~ MRG_RNGs = ( mrg_uniform , GPU_mrg_uniform , GPUA_mrg_uniform ) \n 
@ local_optimizer ( MRG_RNGs ) \n 
def mrg_random_make_inplace ( node ) : \n 
if isinstance ( op , MRG_RNGs ) and not op . inplace : \n 
~~~ new_op = op . __class__ ( op . output_type , inplace = True ) \n 
return new_op . make_node ( * node . inputs ) . outputs \n 
opt . in2out ( mrg_random_make_inplace , ignore_newtrees = True ) , \n 
99 , , ) \n 
import scipy . sparse \n 
from theano import gof , tensor \n 
from theano . tensor . opt import register_specialize \n 
from theano . sparse . basic import ( \n 
as_sparse_variable , SparseType , add_s_s , neg , \n 
mul_s_s , mul_s_d , dot , \n 
CSMProperties , CSM , \n 
_is_sparse_variable , _is_dense_variable , CSC , CSR , \n 
csm_properties , csm_data , csm_indices , csm_indptr , csm_shape , \n 
_is_sparse , \n 
Remove0 , remove0 , \n 
Cast , bcast , wcast , icast , lcast , fcast , dcast , ccast , zcast , \n 
HStack , hstack , VStack , vstack , \n 
AddSSData , add_s_s_data , \n 
MulSV , mul_s_v , \n 
structured_monoid , \n 
structured_sigmoid , structured_exp , structured_log , structured_pow , \n 
structured_minimum , structured_maximum , structured_add , \n 
StructuredAddSV , structured_add_s_v , \n 
SamplingDot , sampling_dot ) \n 
from theano . sparse . opt import ( \n 
MulSDCSC , mul_s_d_csc , MulSDCSR , mul_s_d_csr , \n 
MulSVCSR , mul_s_v_csr , \n 
StructuredAddSVCSR , structured_add_s_v_csr , \n 
SamplingDotCSR , sampling_dot_csr , \n 
local_mul_s_d , local_mul_s_v , \n 
local_structured_add_s_v , local_sampling_dot_csr ) \n 
EliminateZeros = Remove0 \n 
eliminate_zeros = remove0 \n 
class Poisson ( gof . op . Op ) : \n 
def make_node ( self , x ) : \n 
~~~ x = as_sparse_variable ( x ) \n 
return gof . Apply ( self , [ x ] , [ x . type ( ) ] ) \n 
~~~ ( x , ) = inputs \n 
assert _is_sparse ( x ) \n 
assert x . format in [ "csr" , "csc" ] \n 
out [ 0 ] = x . copy ( ) \n 
out [ 0 ] . data = numpy . asarray ( numpy . random . poisson ( out [ 0 ] . data ) , \n 
dtype = x . dtype ) \n 
out [ 0 ] . eliminate_zeros ( ) \n 
~~ def grad ( self , inputs , outputs_gradients ) : \n 
return [ theano . gradient . grad_undefined ( op = self , x_pos = 0 , x = inputs [ 0 ] , \n 
comment = comment ) ] \n 
~~ def infer_shape ( self , node , ins_shapes ) : \n 
~~~ return ins_shapes \n 
~~ ~~ poisson = Poisson ( ) \n 
class Binomial ( gof . op . Op ) : \n 
__props__ = ( "format" , "dtype" ) \n 
def __init__ ( self , format , dtype ) : \n 
~~~ self . format = format \n 
self . dtype = dtype \n 
~~ def make_node ( self , n , p , shape ) : \n 
~~~ n = tensor . as_tensor_variable ( n ) \n 
p = tensor . as_tensor_variable ( p ) \n 
shape = tensor . as_tensor_variable ( shape ) \n 
return gof . Apply ( self , [ n , p , shape ] , \n 
[ SparseType ( dtype = self . dtype , \n 
format = self . format ) ( ) ] ) \n 
~~~ ( n , p , shape ) = inputs \n 
binomial = numpy . random . binomial ( n , p , size = shape ) \n 
csx_matrix = getattr ( scipy . sparse , self . format + ) \n 
out [ 0 ] = csx_matrix ( binomial , dtype = self . dtype ) \n 
~~ def connection_pattern ( self , node ) : \n 
~~~ return [ [ True ] , [ True ] , [ False ] ] \n 
~~ def grad ( self , inputs , gout ) : \n 
( gz , ) = gout \n 
return [ theano . gradient . grad_undefined ( op = self , x_pos = 0 , x = n , \n 
comment = comment_n ) , \n 
theano . gradient . grad_undefined ( op = self , x_pos = 1 , x = p , \n 
comment = comment_p ) , \n 
theano . gradient . disconnected_type ( ) ] \n 
~~~ return [ ( node . inputs [ 2 ] [ 0 ] , node . inputs [ 2 ] [ 1 ] ) ] \n 
~~ ~~ csr_fbinomial = Binomial ( , ) \n 
csc_fbinomial = Binomial ( , ) \n 
csr_dbinomial = Binomial ( , ) \n 
csc_dbinomial = Binomial ( , ) \n 
class Multinomial ( gof . op . Op ) : \n 
def make_node ( self , n , p ) : \n 
p = as_sparse_variable ( p ) \n 
assert p . format in [ "csr" , "csc" ] \n 
return gof . Apply ( self , [ n , p ] , [ p . type ( ) ] ) \n 
~~~ ( n , p ) = inputs \n 
assert _is_sparse ( p ) \n 
if p . format != : \n 
~~~ raise NotImplemented ( ) \n 
~~ out [ 0 ] = p . copy ( ) \n 
if n . ndim == 0 : \n 
~~~ for i in xrange ( p . shape [ 0 ] ) : \n 
~~~ k , l = p . indptr [ i ] , p . indptr [ i + 1 ] \n 
out [ 0 ] . data [ k : l ] = numpy . random . multinomial ( n , p . data [ k : l ] ) \n 
~~ ~~ elif n . ndim == 1 : \n 
~~~ if n . shape [ 0 ] != p . shape [ 0 ] : \n 
~~ for i in xrange ( p . shape [ 0 ] ) : \n 
out [ 0 ] . data [ k : l ] = numpy . random . multinomial ( n [ i ] , p . data [ k : l ] ) \n 
~~ ~~ ~~ def grad ( self , inputs , outputs_gradients ) : \n 
theano . gradient . grad_undefined ( op = self , x_pos = 1 , x = inputs [ 1 ] , \n 
comment = comment_p ) ] \n 
~~~ return [ ins_shapes [ 1 ] ] \n 
~~ ~~ multinomial = Multinomial ( ) \n 
import numpy as N \n 
from theano . tensor import basic as T \n 
from theano . tensor . blas_headers import blas_header_text , blas_header_version \n 
from theano . tensor . blas import ldflags \n 
from theano . misc import strutil \n 
from theano . gradient import grad_undefined \n 
class Conv3D ( theano . Op ) : \n 
def c_code_cache_version ( self ) : \n 
~~~ return ( 3 , blas_header_version ( ) ) \n 
~~ def make_node ( self , V , W , b , d ) : \n 
V_ = T . as_tensor_variable ( V ) \n 
W_ = T . as_tensor_variable ( W ) \n 
b_ = T . as_tensor_variable ( b ) \n 
d_ = T . as_tensor_variable ( d ) \n 
bcast = ( V_ . broadcastable [ 0 ] , False , False , False , W_ . broadcastable [ 0 ] ) \n 
node = theano . Apply ( self , inputs = [ V_ , W_ , b_ , d_ ] , \n 
outputs = [ T . TensorType ( V_ . dtype , bcast ) ( ) ] ) \n 
return node \n 
~~ def grad ( self , inputs , output_gradients ) : \n 
~~~ V , W , b , d = inputs \n 
dCdH , = output_gradients \n 
dCdV = theano . tensor . nnet . convTransp3D ( \n 
W , T . zeros_like ( V [ 0 , 0 , 0 , 0 , : ] ) , d , dCdH , V . shape [ 1 : 4 ] ) \n 
dCdV = T . patternbroadcast ( dCdV , V . broadcastable ) \n 
WShape = W . shape \n 
dCdW = theano . tensor . nnet . convGrad3D ( V , d , WShape , dCdH ) \n 
dCdW = T . patternbroadcast ( dCdW , W . broadcastable ) \n 
dCdb = T . sum ( dCdH , axis = ( 0 , 1 , 2 , 3 ) ) \n 
dCdb = T . patternbroadcast ( dCdb , b . broadcastable ) \n 
dCdd = grad_undefined ( \n 
self , 3 , inputs [ 3 ] , \n 
if in dir ( dCdH ) and dCdH . name is not None : \n 
~~~ dCdH_name = dCdH . name \n 
~~~ dCdH_name = \n 
~~ if in dir ( V ) and V . name is not None : \n 
~~~ V_name = V . name \n 
~~~ V_name = \n 
~~ if in dir ( W ) and W . name is not None : \n 
~~~ W_name = W . name \n 
~~~ W_name = \n 
~~ if in dir ( b ) and b . name is not None : \n 
~~~ b_name = b . name \n 
~~~ b_name = \n 
~~ dCdV . name = + dCdH_name + + V_name + \n 
dCdW . name = ( + dCdH_name + + V_name + \n 
+ W_name + ) \n 
dCdb . name = ( + dCdH_name + + V_name + \n 
+ W_name + + b_name + ) \n 
return [ dCdV , dCdW , dCdb , dCdd ] \n 
~~ def perform ( self , node , inputs , output_storage ) : \n 
output_storage [ 0 ] [ 0 ] = computeH ( V , W , b , d ) \n 
~~ def infer_shape ( self , node , input_shapes ) : \n 
~~~ V , W , b , d = node . inputs \n 
V_shape , W_shape , b_shape , d_shape = input_shapes \n 
dr = d [ 0 ] \n 
dc = d [ 1 ] \n 
dt = d [ 2 ] \n 
batch_size = V_shape [ 0 ] \n 
output_channels = W_shape [ 0 ] \n 
vidHeight = V_shape [ 1 ] \n 
filterHeight = W_shape [ 1 ] \n 
vidWidth = V_shape [ 2 ] \n 
filterWidth = W_shape [ 2 ] \n 
vidDur = V_shape [ 3 ] \n 
filterDur = W_shape [ 3 ] \n 
output_height = ( ( vidHeight - filterHeight ) // dr ) + 1 \n 
output_width = ( ( vidWidth - filterWidth ) // dc ) + 1 \n 
output_dur = ( ( vidDur - filterDur ) // dt ) + 1 \n 
rval = ( batch_size , output_height , output_width , output_dur , output_channels ) \n 
return [ rval ] \n 
~~~ return blas_header_text ( ) \n 
~~ def c_libraries ( self ) : \n 
~~~ return ldflags ( ) \n 
~~ def c_compile_args ( self ) : \n 
~~~ flags = ldflags ( libs = False , flags = True ) \n 
return flags \n 
~~ def c_lib_dirs ( self ) : \n 
~~~ return ldflags ( libs = False , libs_dir = True ) \n 
~~ def c_header_dirs ( self ) : \n 
~~~ return ldflags ( libs = False , include_dir = True ) \n 
~~ def c_code ( self , node , nodename , inputs , outputs , sub ) : \n 
H = outputs [ 0 ] \n 
VV , WV , bv , dv = node . inputs \n 
HV = node . outputs [ 0 ] \n 
if ( theano . config . blas . ldflags and \n 
VV . dtype == WV . dtype and HV . dtype == VV . dtype ) : \n 
~~~ if VV . dtype == : \n 
~~~ gemv = \n 
~~ elif VV . dtype == : \n 
~~~ raise Exception ( + V . value . dtype ) \n 
return strutil . render_string ( codeSource , locals ( ) ) \n 
~~ ~~ _conv3D = Conv3D ( ) \n 
def conv3D ( V , W , b , d ) : \n 
return _conv3D ( V , W , b , d ) \n 
~~ def computeH ( V , W , b , d ) : \n 
~~~ assert len ( W . shape ) == 5 \n 
assert len ( V . shape ) == 5 \n 
if len ( b . shape ) != 1 : \n 
~~~ print ( b . shape ) \n 
assert False \n 
~~ assert len ( d ) == 3 \n 
batchSize = V . shape [ 0 ] \n 
outputChannels = W . shape [ 0 ] \n 
inputChannels = V . shape [ 4 ] \n 
if W . shape [ 4 ] != inputChannels : \n 
filterWidth = W . shape [ 2 ] \n 
filterDur = W . shape [ 3 ] \n 
vidHeight = V . shape [ 1 ] \n 
vidWidth = V . shape [ 2 ] \n 
vidDur = V . shape [ 3 ] \n 
assert vidHeight >= filterHeight \n 
assert vidWidth >= filterWidth \n 
assert vidDur >= filterDur \n 
dx , dy , dt = d \n 
assert dx > 0 \n 
assert dy > 0 \n 
assert dt > 0 \n 
outputHeight = int ( ( vidHeight - filterHeight ) / dx ) + 1 \n 
outputWidth = int ( ( vidWidth - filterWidth ) / dy ) + 1 \n 
outputDur = int ( ( vidDur - filterDur ) / dt ) + 1 \n 
H = N . zeros ( ( batchSize , outputHeight , \n 
outputWidth , outputDur , outputChannels ) , dtype = V . dtype ) \n 
for i in xrange ( 0 , H . shape [ 0 ] ) : \n 
~~~ for j in xrange ( 0 , H . shape [ 4 ] ) : \n 
~~~ for x in xrange ( 0 , H . shape [ 1 ] ) : \n 
~~~ for y in xrange ( 0 , H . shape [ 2 ] ) : \n 
~~~ for t in xrange ( 0 , H . shape [ 3 ] ) : \n 
~~~ H [ i , x , y , t , j ] = b [ j ] \n 
for k in xrange ( 0 , filterHeight ) : \n 
~~~ for l in xrange ( 0 , filterWidth ) : \n 
~~~ for m in xrange ( 0 , filterDur ) : \n 
~~~ for z in xrange ( 0 , inputChannels ) : \n 
v = V [ i , d [ 0 ] * x + k , d [ 1 ] * y + l , d [ 2 ] * t + m , z ] \n 
H [ i , x , y , t , j ] += w * v \n 
~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ return H \n 
from theano import shared , function \n 
from theano . tensor . nnet . neighbours import images2neibs , neibs2images , Images2Neibs \n 
from theano . tests import unittest_tools \n 
class T_Images2Neibs ( unittest_tools . InferShapeTester ) : \n 
~~~ mode = mode_without_gpu \n 
op = Images2Neibs \n 
dtypes = [ , , ] \n 
def test_neibs ( self ) : \n 
~~~ for shape , pshape in [ ( ( 10 , 7 , 18 , 18 ) , ( 2 , 2 ) ) , \n 
( ( 10 , 7 , 6 , 18 ) , ( 3 , 2 ) ) , \n 
( ( 5 , 7 , 66 , 66 ) , ( 33 , 33 ) ) , \n 
( ( 5 , 7 , 68 , 66 ) , ( 34 , 33 ) ) \n 
] : \n 
~~~ for border in [ , ] : \n 
~~~ for dtype in self . dtypes : \n 
~~~ images = shared ( \n 
numpy . arange ( numpy . prod ( shape ) , dtype = dtype \n 
) . reshape ( shape ) ) \n 
neib_shape = T . as_tensor_variable ( pshape ) \n 
f = function ( [ ] , \n 
images2neibs ( images , neib_shape , mode = border ) , \n 
mode = self . mode ) \n 
neibs = f ( ) \n 
g = function ( [ ] , \n 
neibs2images ( neibs , neib_shape , images . shape ) , \n 
assert any ( [ isinstance ( node . op , self . op ) \n 
for node in f . maker . fgraph . toposort ( ) ] ) \n 
assert numpy . allclose ( images . get_value ( borrow = True ) , g ( ) ) \n 
~~ ~~ ~~ ~~ def test_neibs_manual ( self ) : \n 
~~~ shape = ( 2 , 3 , 4 , 4 ) \n 
for dtype in self . dtypes : \n 
neib_shape = T . as_tensor_variable ( ( 2 , 2 ) ) \n 
for border in [ , ] : \n 
~~~ f = function ( [ ] , images2neibs ( images , neib_shape , mode = border ) , \n 
assert numpy . allclose ( neibs , \n 
[ [ 0 , 1 , 4 , 5 ] , \n 
[ 2 , 3 , 6 , 7 ] , \n 
[ 8 , 9 , 12 , 13 ] , \n 
[ 10 , 11 , 14 , 15 ] , \n 
[ 16 , 17 , 20 , 21 ] , \n 
[ 18 , 19 , 22 , 23 ] , \n 
[ 24 , 25 , 28 , 29 ] , \n 
[ 26 , 27 , 30 , 31 ] , \n 
[ 32 , 33 , 36 , 37 ] , \n 
[ 34 , 35 , 38 , 39 ] , \n 
[ 40 , 41 , 44 , 45 ] , \n 
[ 42 , 43 , 46 , 47 ] , \n 
[ 48 , 49 , 52 , 53 ] , \n 
[ 50 , 51 , 54 , 55 ] , \n 
[ 56 , 57 , 60 , 61 ] , \n 
[ 58 , 59 , 62 , 63 ] , \n 
[ 64 , 65 , 68 , 69 ] , \n 
[ 66 , 67 , 70 , 71 ] , \n 
[ 72 , 73 , 76 , 77 ] , \n 
[ 74 , 75 , 78 , 79 ] , \n 
[ 80 , 81 , 84 , 85 ] , \n 
[ 82 , 83 , 86 , 87 ] , \n 
[ 88 , 89 , 92 , 93 ] , \n 
[ 90 , 91 , 94 , 95 ] ] ) \n 
g = function ( [ ] , neibs2images ( neibs , neib_shape , images . shape ) , \n 
~~ ~~ ~~ def test_neibs_manual_step ( self ) : \n 
~~~ shape = ( 2 , 3 , 5 , 5 ) \n 
~~~ images = shared ( numpy . asarray ( numpy . arange ( numpy . prod ( \n 
shape ) ) . reshape ( shape ) , dtype = dtype ) ) \n 
neib_shape = T . as_tensor_variable ( ( 3 , 3 ) ) \n 
neib_step = T . as_tensor_variable ( ( 2 , 2 ) ) \n 
~~~ f = function ( [ ] , \n 
images2neibs ( images , neib_shape , neib_step , \n 
mode = border ) , \n 
assert self . op in [ type ( node . op ) \n 
for node in f . maker . fgraph . toposort ( ) ] \n 
[ [ 0 , 1 , 2 , 5 , 6 , 7 , 10 , 11 , 12 ] , \n 
[ 2 , 3 , 4 , 7 , 8 , 9 , 12 , 13 , 14 ] , \n 
[ 10 , 11 , 12 , 15 , 16 , 17 , 20 , 21 , 22 ] , \n 
[ 12 , 13 , 14 , 17 , 18 , 19 , 22 , 23 , 24 ] , \n 
[ 25 , 26 , 27 , 30 , 31 , 32 , 35 , 36 , 37 ] , \n 
[ 27 , 28 , 29 , 32 , 33 , 34 , 37 , 38 , 39 ] , \n 
[ 35 , 36 , 37 , 40 , 41 , 42 , 45 , 46 , 47 ] , \n 
[ 37 , 38 , 39 , 42 , 43 , 44 , 47 , 48 , 49 ] , \n 
[ 50 , 51 , 52 , 55 , 56 , 57 , 60 , 61 , 62 ] , \n 
[ 52 , 53 , 54 , 57 , 58 , 59 , 62 , 63 , 64 ] , \n 
[ 60 , 61 , 62 , 65 , 66 , 67 , 70 , 71 , 72 ] , \n 
[ 62 , 63 , 64 , 67 , 68 , 69 , 72 , 73 , 74 ] , \n 
[ 75 , 76 , 77 , 80 , 81 , 82 , 85 , 86 , 87 ] , \n 
[ 77 , 78 , 79 , 82 , 83 , 84 , 87 , 88 , 89 ] , \n 
[ 85 , 86 , 87 , 90 , 91 , 92 , 95 , 96 , 97 ] , \n 
[ 87 , 88 , 89 , 92 , 93 , 94 , 97 , 98 , 99 ] , \n 
[ 100 , 101 , 102 , 105 , 106 , 107 , 110 , 111 , 112 ] , \n 
[ 102 , 103 , 104 , 107 , 108 , 109 , 112 , 113 , 114 ] , \n 
[ 110 , 111 , 112 , 115 , 116 , 117 , 120 , 121 , 122 ] , \n 
[ 112 , 113 , 114 , 117 , 118 , 119 , 122 , 123 , 124 ] , \n 
[ 125 , 126 , 127 , 130 , 131 , 132 , 135 , 136 , 137 ] , \n 
[ 127 , 128 , 129 , 132 , 133 , 134 , 137 , 138 , 139 ] , \n 
[ 135 , 136 , 137 , 140 , 141 , 142 , 145 , 146 , 147 ] , \n 
[ 137 , 138 , 139 , 142 , 143 , 144 , 147 , 148 , 149 ] ] ) \n 
~~ ~~ ~~ def test_neibs_bad_shape ( self ) : \n 
~~~ shape = ( 2 , 3 , 10 , 10 ) \n 
~~~ images = shared ( numpy . arange ( \n 
numpy . prod ( shape ) , dtype = dtype \n 
for neib_shape in [ ( 3 , 2 ) , ( 2 , 3 ) ] : \n 
~~~ neib_shape = T . as_tensor_variable ( neib_shape ) \n 
f = function ( [ ] , images2neibs ( images , neib_shape ) , \n 
self . assertRaises ( TypeError , f ) \n 
images2neibs ( images , neib_shape , \n 
mode = ) , \n 
f ( ) \n 
~~ ~~ ~~ def test_neibs_wrap_centered_step_manual ( self ) : \n 
~~~ expected1 = [ [ 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 ] , \n 
[ 21 , 22 , 23 , 1 , 2 , 3 , 6 , 7 , 8 ] , \n 
[ 23 , 24 , 20 , 3 , 4 , 0 , 8 , 9 , 5 ] , \n 
[ 9 , 5 , 6 , 14 , 10 , 11 , 19 , 15 , 16 ] , \n 
[ 6 , 7 , 8 , 11 , 12 , 13 , 16 , 17 , 18 ] , \n 
[ 8 , 9 , 5 , 13 , 14 , 10 , 18 , 19 , 15 ] , \n 
[ 19 , 15 , 16 , 24 , 20 , 21 , 4 , 0 , 1 ] , \n 
[ 16 , 17 , 18 , 21 , 22 , 23 , 1 , 2 , 3 ] , \n 
[ 18 , 19 , 15 , 23 , 24 , 20 , 3 , 4 , 0 ] ] \n 
expected2 = [ [ 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 ] , \n 
[ 22 , 23 , 24 , 2 , 3 , 4 , 7 , 8 , 9 ] , \n 
[ 14 , 10 , 11 , 19 , 15 , 16 , 24 , 20 , 21 ] , \n 
[ 12 , 13 , 14 , 17 , 18 , 19 , 22 , 23 , 24 ] ] \n 
expected3 = [ [ 19 , 15 , 16 , 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 , 14 , 10 , 11 ] , \n 
[ 17 , 18 , 19 , 22 , 23 , 24 , 2 , 3 , 4 , 7 , 8 , 9 , 12 , 13 , 14 ] , \n 
[ 9 , 5 , 6 , 14 , 10 , 11 , 19 , 15 , 16 , 24 , 20 , 21 , 4 , 0 , 1 ] , \n 
[ 7 , 8 , 9 , 12 , 13 , 14 , 17 , 18 , 19 , 22 , 23 , 24 , 2 , 3 , 4 ] ] \n 
expected4 = [ [ 23 , 24 , 20 , 21 , 22 , 3 , 4 , 0 , 1 , 2 , 8 , 9 , 5 , 6 , 7 ] , \n 
[ 21 , 22 , 23 , 24 , 20 , 1 , 2 , 3 , 4 , 0 , 6 , 7 , 8 , 9 , 5 ] , \n 
[ 13 , 14 , 10 , 11 , 12 , 18 , 19 , 15 , 16 , 17 , 23 , 24 , 20 , 21 , 22 ] , \n 
[ 11 , 12 , 13 , 14 , 10 , 16 , 17 , 18 , 19 , 15 , 21 , 22 , 23 , 24 , 20 ] ] \n 
expected5 = [ [ 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 ] , \n 
[ 7 , 8 , 9 , 12 , 13 , 14 , 17 , 18 , 19 ] , \n 
[ 17 , 18 , 19 , 22 , 23 , 24 , 2 , 3 , 4 ] ] \n 
expected6 = [ [ 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 ] , \n 
[ 11 , 12 , 13 , 16 , 17 , 18 , 21 , 22 , 23 ] , \n 
[ 13 , 14 , 10 , 18 , 19 , 15 , 23 , 24 , 20 ] ] \n 
for shp_idx , ( shape , neib_shape , neib_step , expected ) in enumerate ( [ \n 
[ ( 7 , 8 , 5 , 5 ) , ( 3 , 3 ) , ( 2 , 2 ) , expected1 ] , \n 
[ ( 7 , 8 , 5 , 5 ) , ( 3 , 3 ) , ( 3 , 3 ) , expected2 ] , \n 
[ ( 7 , 8 , 5 , 5 ) , ( 5 , 3 ) , ( 3 , 3 ) , expected3 ] , \n 
[ ( 7 , 8 , 5 , 5 ) , ( 3 , 5 ) , ( 3 , 3 ) , expected4 ] , \n 
[ ( 80 , 90 , 5 , 5 ) , ( 3 , 3 ) , ( 2 , 3 ) , expected5 ] , \n 
[ ( 1025 , 9 , 5 , 5 ) , ( 3 , 3 ) , ( 3 , 2 ) , expected6 ] , \n 
[ ( 1 , 1 , 5 , 1035 ) , ( 3 , 3 ) , ( 3 , 3 ) , None ] , \n 
[ ( 1 , 1 , 1045 , 5 ) , ( 3 , 3 ) , ( 3 , 3 ) , None ] , \n 
] ) : \n 
neib_shape = T . as_tensor_variable ( neib_shape ) \n 
neib_step = T . as_tensor_variable ( neib_step ) \n 
expected = numpy . asarray ( expected ) \n 
f = function ( [ ] , images2neibs ( images , neib_shape , neib_step , \n 
mode = "wrap_centered" ) , \n 
if expected . size > 1 : \n 
~~~ for i in range ( shape [ 0 ] * shape [ 1 ] ) : \n 
~~~ assert numpy . allclose ( \n 
neibs [ i * expected . shape [ 0 ] : \n 
( i + 1 ) * expected . shape [ 0 ] , : ] , \n 
expected + 25 * i ) , "wrap_centered" \n 
~~ ~~ assert self . op in [ type ( node . op ) \n 
~~ ~~ ~~ def test_neibs_bad_shape_wrap_centered ( self ) : \n 
f = function ( [ ] , images2neibs ( images , neib_shape , \n 
~~ for shape in [ ( 2 , 3 , 2 , 3 ) , ( 2 , 3 , 3 , 2 ) ] : \n 
~~~ images = shared ( numpy . arange ( numpy . prod ( shape ) ) . reshape ( shape ) ) \n 
~~ shape = ( 2 , 3 , 3 , 3 ) \n 
images = shared ( numpy . arange ( numpy . prod ( shape ) ) . reshape ( shape ) ) \n 
images2neibs ( images , neib_shape , mode = "wrap_centered" ) , \n 
~~ ~~ def test_grad_wrap_centered ( self ) : \n 
~~~ shape = ( 2 , 3 , 6 , 6 ) \n 
images_val = numpy . random . rand ( * shape ) . astype ( ) \n 
def fn ( images ) : \n 
~~~ return images2neibs ( images , ( 3 , 3 ) , mode = ) \n 
~~ self . assertRaises ( TypeError , unittest_tools . verify_grad , \n 
fn , [ images_val ] , mode = self . mode ) \n 
~~ def test_grad_valid ( self ) : \n 
~~~ return images2neibs ( images , ( 2 , 2 ) ) \n 
~~ unittest_tools . verify_grad ( fn , [ images_val ] , mode = self . mode , \n 
eps = 0.1 ) \n 
~~~ return images2neibs ( images , ( 3 , 2 ) , ( 1 , 2 ) ) \n 
~~~ return images2neibs ( images , ( 1 , 2 ) , ( 5 , 2 ) ) \n 
~~ def test_grad_ignore_border ( self ) : \n 
~~~ return images2neibs ( images , ( 2 , 2 ) , \n 
~~ def test_neibs2images_grad ( self ) : \n 
~~~ neibs_val = numpy . random . rand ( 150 , 4 ) \n 
def fn ( neibs ) : \n 
~~~ return neibs2images ( neibs , ( 2 , 2 ) , ( 2 , 3 , 10 , 10 ) ) \n 
~~ unittest_tools . verify_grad ( fn , [ neibs_val ] , mode = self . mode , \n 
~~ def test_neibs_valid_with_inconsistent_borders ( self ) : \n 
images = T . dtensor4 ( ) \n 
images_val = numpy . arange ( numpy . prod ( shape ) , \n 
dtype = ) . reshape ( shape ) \n 
~~~ return T . sum ( T . sqr ( images2neibs ( images , ( 2 , 2 ) , mode = ) ) , \n 
axis = [ 0 , 1 ] ) \n 
~~ f = theano . function ( [ images ] , \n 
T . sqr ( images2neibs ( images , ( 2 , 2 ) , mode = ) ) , \n 
self . assertRaises ( TypeError , f , images_val ) \n 
~~ def speed_neibs ( self ) : \n 
~~~ shape = ( 100 , 40 , 18 , 18 ) \n 
images = shared ( numpy . arange ( numpy . prod ( shape ) , \n 
dtype = ) . reshape ( shape ) ) \n 
for i in range ( 1000 ) : \n 
~~~ f ( ) \n 
~~ ~~ def speed_neibs_wrap_centered ( self ) : \n 
~~ ~~ def test_infer_shape ( self ) : \n 
~~~ shape = ( 100 , 40 , 6 , 3 ) \n 
images = numpy . ones ( shape ) . astype ( ) \n 
x = T . ftensor4 ( ) \n 
f = self . _compile_and_check ( [ x ] , \n 
[ images2neibs ( \n 
x , neib_shape = ( 2 , 1 ) , \n 
mode = ) ] , \n 
[ images ] , \n 
Images2Neibs \n 
x , neib_shape = ( 2 , 3 ) , \n 
shape = ( 100 , 40 , 5 , 4 ) \n 
shape = ( 100 , 40 , 5 , 3 ) \n 
shape = ( 100 , 40 , 6 , 7 ) \n 
x , neib_shape = ( 2 , 2 ) , \n 
shape = ( 100 , 40 , 5 , 10 ) \n 
x , neib_shape = ( 3 , 3 ) , \n 
from theano . tensor import * \n 
from numpy . testing import dec \n 
class TestRealImag ( unittest . TestCase ) : \n 
~~~ def test0 ( self ) : \n 
~~~ x = zvector ( ) \n 
rng = numpy . random . RandomState ( 23 ) \n 
xval = numpy . asarray ( list ( numpy . complex ( rng . randn ( ) , rng . randn ( ) ) \n 
for i in xrange ( 10 ) ) ) \n 
assert numpy . all ( xval . real == theano . function ( [ x ] , real ( x ) ) ( xval ) ) \n 
assert numpy . all ( xval . imag == theano . function ( [ x ] , imag ( x ) ) ( xval ) ) \n 
~~ def test_on_real_input ( self ) : \n 
~~~ x = dvector ( ) \n 
xval = rng . randn ( 10 ) \n 
numpy . all ( 0 == theano . function ( [ x ] , imag ( x ) ) ( xval ) ) \n 
numpy . all ( xval == theano . function ( [ x ] , real ( x ) ) ( xval ) ) \n 
x = imatrix ( ) \n 
xval = numpy . asarray ( rng . randn ( 3 , 3 ) * 100 , dtype = ) \n 
~~ def test_cast ( self ) : \n 
self . assertRaises ( TypeError , cast , x , ) \n 
~~ def test_complex ( self ) : \n 
~~~ rng = numpy . random . RandomState ( 2333 ) \n 
m = fmatrix ( ) \n 
c = complex ( m [ 0 ] , m [ 1 ] ) \n 
assert c . type == cvector \n 
r , i = [ real ( c ) , imag ( c ) ] \n 
assert r . type == fvector \n 
assert i . type == fvector \n 
f = theano . function ( [ m ] , [ r , i ] ) \n 
mval = numpy . asarray ( rng . randn ( 2 , 5 ) , dtype = ) \n 
rval , ival = f ( mval ) \n 
assert numpy . all ( rval == mval [ 0 ] ) , ( rval , mval [ 0 ] ) \n 
assert numpy . all ( ival == mval [ 1 ] ) , ( ival , mval [ 1 ] ) \n 
def test_complex_grads ( self ) : \n 
~~~ def f ( m ) : \n 
~~~ c = complex ( m [ 0 ] , m [ 1 ] ) \n 
return .5 * real ( c ) + .9 * imag ( c ) \n 
~~ rng = numpy . random . RandomState ( 9333 ) \n 
mval = numpy . asarray ( rng . randn ( 2 , 5 ) ) \n 
utt . verify_grad ( f , [ mval ] ) \n 
def test_mul_mixed0 ( self ) : \n 
~~~ def f ( a ) : \n 
~~~ ac = complex ( a [ 0 ] , a [ 1 ] ) \n 
return abs ( ( ac ) ** 2 ) . sum ( ) \n 
aval = numpy . asarray ( rng . randn ( 2 , 5 ) ) \n 
~~~ utt . verify_grad ( f , [ aval ] ) \n 
~~ except utt . verify_grad . E_grad as e : \n 
~~~ print ( e . num_grad . gf ) \n 
print ( e . analytic_grad ) \n 
def test_mul_mixed1 ( self ) : \n 
return abs ( ac ) . sum ( ) \n 
def test_mul_mixed ( self ) : \n 
~~~ def f ( a , b ) : \n 
return abs ( ( ac * b ) ** 2 ) . sum ( ) \n 
bval = rng . randn ( 5 ) \n 
~~~ utt . verify_grad ( f , [ aval , bval ] ) \n 
def test_polar_grads ( self ) : \n 
~~~ c = complex_from_polar ( abs ( m [ 0 ] ) , m [ 1 ] ) \n 
def test_abs_grad ( self ) : \n 
return .5 * abs ( c ) \n 
from numpy . testing import assert_equal , assert_string_equal \n 
import theano . tensor as tt \n 
from theano . tensor import ( Subtensor , AdvancedSubtensor , AdvancedSubtensor1 , \n 
IncSubtensor , AdvancedIncSubtensor , \n 
AdvancedIncSubtensor1 ) \n 
def test_numpy_method ( ) : \n 
~~~ x = tt . dmatrix ( ) \n 
data = np . random . rand ( 5 , 5 ) \n 
x . tag . test_value = data \n 
for fct in [ np . arccos , np . arccosh , np . arcsin , np . arcsinh , \n 
np . arctan , np . arctanh , np . ceil , np . cos , np . cosh , np . deg2rad , \n 
np . exp , np . exp2 , np . expm1 , np . floor , np . log , \n 
np . log10 , np . log1p , np . log2 , np . rad2deg , \n 
np . sin , np . sinh , np . sqrt , np . tan , np . tanh , np . trunc ] : \n 
~~~ y = fct ( x ) \n 
f = theano . function ( [ x ] , y ) \n 
utt . assert_allclose ( np . nan_to_num ( f ( data ) ) , \n 
np . nan_to_num ( fct ( data ) ) ) \n 
~~ ~~ def test_copy ( ) : \n 
y = x . copy ( name = ) \n 
assert_equal ( f ( data ) , data ) \n 
assert_string_equal ( y . name , ) \n 
~~ def test_None_dimShuffle_replace ( ) : \n 
y = x [ : , None , : ] \n 
for elem in f . maker . fgraph . toposort ( ) : \n 
~~~ assert type ( elem . op ) not in [ Subtensor , AdvancedSubtensor , \n 
AdvancedSubtensor1 , IncSubtensor , \n 
AdvancedIncSubtensor , \n 
AdvancedIncSubtensor1 ] \n 
~~ x = tt . tensor3 ( ) \n 
y1 = x [ : , : , None , : ] \n 
y2 = x [ None , : , : , None , : ] \n 
y3 = x [ : , : , None , : , None , None ] \n 
f = theano . function ( [ x ] , [ y1 , y2 , y3 ] ) \n 
from six . moves import StringIO \n 
import theano . tensor as tensor \n 
from theano . printing import min_informative_str , debugprint \n 
def test_pydotprint_cond_highlight ( ) : \n 
if not theano . printing . pydot_imported : \n 
~~ x = tensor . dvector ( ) \n 
f = theano . function ( [ x ] , x * 2 ) \n 
f ( [ 1 , 2 , 3 , 4 ] ) \n 
s = StringIO ( ) \n 
new_handler = logging . StreamHandler ( s ) \n 
new_handler . setLevel ( logging . DEBUG ) \n 
orig_handler = theano . logging_default_handler \n 
theano . theano_logger . removeHandler ( orig_handler ) \n 
theano . theano_logger . addHandler ( new_handler ) \n 
~~~ theano . printing . pydotprint ( f , cond_highlight = True , \n 
print_output_file = False ) \n 
~~~ theano . theano_logger . addHandler ( orig_handler ) \n 
theano . theano_logger . removeHandler ( new_handler ) \n 
~~ assert ( s . getvalue ( ) == \n 
~~ def test_pydotprint_return_image ( ) : \n 
~~~ if not theano . printing . pydot_imported : \n 
ret = theano . printing . pydotprint ( x * 2 , return_image = True ) \n 
assert isinstance ( ret , ( str , bytes ) ) \n 
~~ def test_pydotprint_variables ( ) : \n 
~~~ theano . printing . pydotprint ( x * 2 ) \n 
if not theano . printing . pd . __name__ == "pydot_ng" : \n 
~~~ theano . printing . pydotprint_variables ( x * 2 ) \n 
~~ ~~ def test_pydotprint_long_name ( ) : \n 
mode = theano . compile . mode . get_default_mode ( ) . excluding ( "fusion" ) \n 
f = theano . function ( [ x ] , [ x * 2 , x + x ] , mode = mode ) \n 
theano . printing . pydotprint ( f , max_label_size = 5 , \n 
theano . printing . pydotprint ( [ x * 2 , x + x ] , \n 
max_label_size = 5 , \n 
~~ def test_pydotprint_profile ( ) : \n 
~~ A = tensor . matrix ( ) \n 
f = theano . function ( [ A ] , A + 1 , mode = ) \n 
theano . printing . pydotprint ( f , print_output_file = False ) \n 
~~ def test_min_informative_str ( ) : \n 
A = tensor . matrix ( name = ) \n 
B = tensor . matrix ( name = ) \n 
C = A + B \n 
C . name = \n 
D = tensor . matrix ( name = ) \n 
E = tensor . matrix ( name = ) \n 
F = D + E \n 
G = C + F \n 
if mis != reference : \n 
~~~ print ( + mis + ) \n 
print ( + reference + ) \n 
~~ assert mis == reference \n 
~~ def test_debugprint ( ) : \n 
~~~ A = tensor . matrix ( name = ) \n 
mode = theano . compile . get_default_mode ( ) . including ( ) \n 
g = theano . function ( [ A , B , D , E ] , G , mode = mode ) \n 
debugprint ( G ) \n 
debugprint ( G , file = s , ids = ) \n 
s = s . getvalue ( ) \n 
reference = . join ( [ \n 
] ) + \n 
if s != reference : \n 
~~~ print ( + s + ) \n 
~~ assert s == reference \n 
reference = "\\n" . join ( [ \n 
debugprint ( G , file = s , ids = , stop_on_name = True ) \n 
debugprint ( g , file = s , ids = , print_storage = True ) \n 
~~ def test_scan_debugprint1 ( ) : \n 
~~~ k = tensor . iscalar ( "k" ) \n 
A = tensor . dvector ( "A" ) \n 
result , updates = theano . scan ( fn = lambda prior_result , A : prior_result * A , \n 
outputs_info = tensor . ones_like ( A ) , \n 
non_sequences = A , \n 
n_steps = k ) \n 
output_str = theano . printing . debugprint ( final_result , file = ) \n 
lines = [ ] \n 
for line in output_str . split ( ) : \n 
~~~ lines += [ line ] \n 
for truth , out in zip ( expected_output . split ( "\\n" ) , lines ) : \n 
~~~ assert truth . strip ( ) == out . strip ( ) \n 
~~ ~~ def test_scan_debugprint2 ( ) : \n 
~~~ coefficients = theano . tensor . vector ( "coefficients" ) \n 
x = tensor . scalar ( "x" ) \n 
max_coefficients_supported = 10000 \n 
components , updates = theano . scan ( fn = lambda coefficient , power , \n 
free_variable : \n 
coefficient * ( free_variable ** power ) , \n 
outputs_info = None , \n 
sequences = [ \n 
coefficients , \n 
theano . tensor . arange ( \n 
max_coefficients_supported ) ] , \n 
non_sequences = x ) \n 
polynomial = components . sum ( ) \n 
output_str = theano . printing . debugprint ( polynomial , file = ) \n 
~~ ~~ def test_scan_debugprint3 ( ) : \n 
~~~ coefficients = theano . tensor . dvector ( "coefficients" ) \n 
max_coefficients_supported = 10 \n 
k = tensor . iscalar ( "k" ) \n 
def compute_A_k ( A , k ) : \n 
~~~ result , updates = theano . scan ( fn = lambda prior_result , \n 
A : prior_result * A , \n 
A_k = result [ - 1 ] \n 
return A_k \n 
~~ components , updates = theano . scan ( fn = lambda coefficient , \n 
power , some_A , some_k : \n 
coefficient * \n 
( compute_A_k ( some_A , some_k ) ** power ) , \n 
non_sequences = [ A , k ] ) \n 
final_result = polynomial \n 
~~ ~~ def test_scan_debugprint4 ( ) : \n 
~~~ def fn ( a_m2 , a_m1 , b_m2 , b_m1 ) : \n 
~~~ return a_m1 + a_m2 , b_m1 + b_m2 \n 
~~ a0 = theano . shared ( numpy . arange ( 2 , dtype = ) ) \n 
b0 = theano . shared ( numpy . arange ( 2 , dtype = ) ) \n 
( a , b ) , _ = theano . scan ( \n 
fn , outputs_info = [ { : a0 , : [ - 2 , - 1 ] } , \n 
{ : b0 , : [ - 2 , - 1 ] } ] , \n 
n_steps = 5 ) \n 
final_result = a + b \n 
~~ ~~ def test_scan_debugprint5 ( ) : \n 
final_result = tensor . grad ( result [ - 1 ] . sum ( ) , A ) \n 
~~ ~~ def test_printing_scan ( ) : \n 
~~ def f_pow2 ( x_tm1 ) : \n 
~~~ return 2 * x_tm1 \n 
~~ state = theano . tensor . scalar ( ) \n 
n_steps = theano . tensor . iscalar ( ) \n 
output , updates = theano . scan ( f_pow2 , \n 
[ ] , \n 
state , \n 
n_steps = n_steps , \n 
truncate_gradient = - 1 , \n 
go_backwards = False ) \n 
f = theano . function ( [ state , n_steps ] , \n 
output , \n 
updates = updates , \n 
allow_input_downcast = True ) \n 
theano . printing . pydotprint ( output , scan_graphs = True ) \n 
theano . printing . pydotprint ( f , scan_graphs = True ) \n 
from __future__ import division , print_function , absolute_import \n 
from scipy . interpolate import interp1d \n 
from . import fetch_rrlyrae_templates , fetch_rrlyrae \n 
class RRLyraeGenerated ( object ) : \n 
lcdata = None \n 
templates = None \n 
ext_correction = { : 1.810 , \n 
: 1.400 , \n 
: 1.0 , \n 
: 0.759 , \n 
: 0.561 } \n 
@ classmethod \n 
def _fetch_data ( cls ) : \n 
~~~ if cls . lcdata is None : \n 
~~~ cls . lcdata = fetch_rrlyrae ( ) \n 
cls . templates = fetch_rrlyrae_templates ( ) \n 
~~ ~~ @ classmethod \n 
def _template_func ( cls , num , band , mu = 0 , A = 1 ) : \n 
~~~ template_id = "{0:.0f}{1}" . format ( num , band ) \n 
phase , amp = cls . templates . get_template ( template_id ) \n 
phase = np . concatenate ( [ phase , [ 1 ] ] ) \n 
amp = np . concatenate ( [ amp , amp [ - 1 : ] ] ) \n 
return interp1d ( phase , mu + A * amp ) \n 
~~ def __init__ ( self , lcid , random_state = None ) : \n 
~~~ self . _fetch_data ( ) \n 
self . lcid = lcid \n 
self . meta = self . lcdata . get_metadata ( lcid ) \n 
self . obsmeta = self . lcdata . get_obsmeta ( lcid ) \n 
self . rng = np . random . RandomState ( random_state ) \n 
~~ @ property \n 
def period ( self ) : \n 
return self . meta [ ] \n 
~~ def observed ( self , band , corrected = True ) : \n 
if band not in : \n 
~~ i = . find ( band ) \n 
t , y , dy = self . lcdata . get_lightcurve ( self . lcid , return_1d = False ) \n 
if corrected : \n 
~~~ ext = self . obsmeta [ ] * self . ext_correction [ band ] \n 
~~~ ext = 0 \n 
~~ return t [ : , i ] , y [ : , i ] - ext , dy [ : , i ] \n 
~~ def generated ( self , band , t , err = None , corrected = True ) : \n 
t = np . asarray ( t ) \n 
num = self . meta [ band + ] \n 
mu = self . meta [ band + ] \n 
amp = self . meta [ band + ] \n 
t0 = self . meta [ band + ] \n 
bad_vals = np . isnan ( t ) | np . isinf ( t ) \n 
t [ bad_vals ] = t0 \n 
~~ func = self . _template_func ( num , band , mu + ext , amp ) \n 
mag = func ( ( ( t - t0 ) / self . period ) % 1 ) \n 
mag [ bad_vals ] = np . nan \n 
if err is not None : \n 
~~~ mag += self . rng . normal ( 0 , err , t . shape ) \n 
~~ return mag \n 
import urllib2 \n 
DATA_URL = ( \n 
LOCAL_FILE = \n 
password_mgr = urllib2 . HTTPPasswordMgrWithDefaultRealm ( ) \n 
password_mgr . add_password ( None , DATA_URL , , ) \n 
handler = urllib2 . HTTPBasicAuthHandler ( password_mgr ) \n 
opener = urllib2 . build_opener ( handler ) \n 
if not os . path . exists ( LOCAL_FILE ) : \n 
fhandle = opener . open ( DATA_URL ) \n 
open ( LOCAL_FILE , ) . write ( fhandle . read ( ) ) \n 
from itertools import cycle \n 
import pylab as pl \n 
from sklearn . datasets import load_iris \n 
from sklearn . decomposition import PCA \n 
def plot_2D ( data , target , target_names ) : \n 
~~~ colors = cycle ( ) \n 
target_ids = range ( len ( target_names ) ) \n 
pl . figure ( ) \n 
for i , c , label in zip ( target_ids , colors , target_names ) : \n 
~~~ pl . plot ( data [ target == i , 0 ] , \n 
data [ target == i , 1 ] , , \n 
c = c , label = label ) \n 
~~ pl . legend ( target_names ) \n 
#---------------------------------------------------------------------- \n 
~~ iris = load_iris ( ) \n 
X , y = iris . data , iris . target \n 
pca = PCA ( n_components = 2 , whiten = True ) . fit ( X ) \n 
X_pca = pca . transform ( X ) \n 
plot_2D ( X_pca , iris . target , iris . target_names ) \n 
from sklearn . cluster import KMeans \n 
from numpy . random import RandomState \n 
rng = RandomState ( 42 ) \n 
kmeans = KMeans ( 3 , random_state = rng ) . fit ( X_pca ) \n 
plot_2D ( X_pca , kmeans . labels_ , [ "c0" , "c1" , "c2" ] ) \n 
pl . show ( ) \n 
import setup_logger \n 
from robo . models . gpy_model import GPyModel \n 
from robo . acquisition . ei import EI \n 
from robo . maximizers . cmaes import CMAES \n 
from robo . task . synthetic_functions . branin import Branin \n 
from robo . solver . bayesian_optimization import BayesianOptimization \n 
branin = Branin ( ) \n 
kernel = GPy . kern . Matern52 ( input_dim = branin . n_dims ) \n 
model = GPyModel ( kernel ) \n 
acquisition_func = EI ( model , \n 
X_upper = branin . X_upper , \n 
X_lower = branin . X_lower , \n 
par = 0.1 ) \n 
maximizer = CMAES ( acquisition_func , branin . X_lower , branin . X_upper ) \n 
bo = BayesianOptimization ( acquisition_func = acquisition_func , \n 
model = model , \n 
maximize_func = maximizer , \n 
task = branin ) \n 
bo . run ( 10 ) \n 
import DIRECT \n 
from robo . maximizers . base_maximizer import BaseMaximizer \n 
class Direct ( BaseMaximizer ) : \n 
~~~ def __init__ ( self , objective_function , X_lower , X_upper , \n 
n_func_evals = 400 , n_iters = 200 ) : \n 
self . n_func_evals = n_func_evals \n 
self . n_iters = n_iters \n 
super ( Direct , self ) . __init__ ( objective_function , X_lower , X_upper ) \n 
~~ def _direct_acquisition_fkt_wrapper ( self , acq_f ) : \n 
~~~ def _l ( x , user_data ) : \n 
~~~ return - acq_f ( np . array ( [ x ] ) ) , 0 \n 
~~ return _l \n 
~~ def maximize ( self ) : \n 
x , _ , _ = DIRECT . solve ( \n 
self . _direct_acquisition_fkt_wrapper ( self . objective_func ) , \n 
l = [ self . X_lower ] , \n 
u = [ self . X_upper ] , \n 
maxT = self . n_iters , \n 
maxf = self . n_func_evals ) \n 
return np . array ( [ x ] ) \n 
~~ ~~ import matplotlib . pyplot as plt \n 
import theanets \n 
from utils import load_mnist , plot_layers \n 
train , valid , _ = load_mnist ( labels = True ) \n 
N = 10 \n 
net = theanets . Classifier ( [ 784 , N * N , ( , 10 ) ] ) \n 
net . train ( train , valid , min_improvement = 0.001 , train_batches = 100 ) \n 
plot_layers ( [ net . find ( , ) , net . find ( , ) ] ) \n 
plt . tight_layout ( ) \n 
from . import feedforward \n 
class Regressor ( feedforward . Regressor ) : \n 
INPUT_NDIM = 4 \n 
~~ class Classifier ( feedforward . Classifier ) : \n 
~~ COLOUR_FIGURE = False \n 
data = load_iris ( ) \n 
features = data . data \n 
feature_names = data . feature_names \n 
target = data . target \n 
target_names = data . target_names \n 
labels = target_names [ target ] \n 
is_setosa = ( labels == ) \n 
features = features [ ~ is_setosa ] \n 
labels = labels [ ~ is_setosa ] \n 
is_virginica = ( labels == ) \n 
t = 1.65 \n 
t2 = 1.75 \n 
f0 , f1 = 3 , 2 \n 
if COLOUR_FIGURE : \n 
~~~ area1c = ( 1. , .8 , .8 ) \n 
area2c = ( .8 , .8 , 1. ) \n 
~~~ area1c = ( 1. , 1 , 1 ) \n 
area2c = ( .7 , .7 , .7 ) \n 
~~ x0 = features [ : , f0 ] . min ( ) * .9 \n 
x1 = features [ : , f0 ] . max ( ) * 1.1 \n 
y0 = features [ : , f1 ] . min ( ) * .9 \n 
y1 = features [ : , f1 ] . max ( ) * 1.1 \n 
fig , ax = plt . subplots ( ) \n 
ax . fill_between ( [ t , x1 ] , [ y0 , y0 ] , [ y1 , y1 ] , color = area2c ) \n 
ax . fill_between ( [ x0 , t ] , [ y0 , y0 ] , [ y1 , y1 ] , color = area1c ) \n 
ax . plot ( [ t , t ] , [ y0 , y1 ] , , lw = 2 ) \n 
ax . plot ( [ t2 , t2 ] , [ y0 , y1 ] , , lw = 2 ) \n 
ax . scatter ( features [ is_virginica , f0 ] , \n 
features [ is_virginica , f1 ] , c = , marker = , s = 40 ) \n 
ax . scatter ( features [ ~ is_virginica , f0 ] , \n 
features [ ~ is_virginica , f1 ] , c = , marker = , s = 40 ) \n 
ax . set_ylim ( y0 , y1 ) \n 
ax . set_xlim ( x0 , x1 ) \n 
ax . set_xlabel ( feature_names [ f0 ] ) \n 
ax . set_ylabel ( feature_names [ f1 ] ) \n 
fig . tight_layout ( ) \n 
fig . savefig ( ) \n 
start_time = time . time ( ) \n 
from sklearn . metrics import classification_report \n 
from sklearn . metrics import precision_recall_curve , roc_curve , auc \n 
from sklearn . cross_validation import KFold \n 
from sklearn import neighbors \n 
from data import chosen , chosen_meta \n 
from utils import plot_pr \n 
from utils import plot_feat_importance \n 
from utils import load_meta \n 
from utils import fetch_posts \n 
from utils import plot_feat_hist \n 
from utils import plot_bias_variance \n 
from utils import plot_k_complexity \n 
meta , id_to_idx , idx_to_id = load_meta ( chosen_meta ) \n 
import nltk \n 
all_questions = sorted ( [ q for q , v in meta . items ( ) if v [ ] == - 1 ] ) \n 
all_answers = sorted ( [ q for q , v in meta . items ( ) if v [ ] != - 1 ] ) \n 
feature_names = np . array ( ( \n 
def prepare_sent_features ( ) : \n 
~~~ for pid , text in fetch_posts ( chosen , with_index = True ) : \n 
~~~ if not text : \n 
~~~ meta [ pid ] [ ] = meta [ pid ] [ ] = 0 \n 
~~~ from platform import python_version \n 
if python_version ( ) . startswith ( ) : \n 
~~~ text = text . decode ( ) \n 
~~ sent_lens = [ len ( nltk . word_tokenize ( \n 
sent ) ) for sent in nltk . sent_tokenize ( text ) ] \n 
meta [ pid ] [ ] = np . mean ( sent_lens ) \n 
meta [ pid ] [ ] = np . mean ( \n 
[ len ( w ) for w in nltk . word_tokenize ( text ) ] ) \n 
~~ meta [ pid ] [ ] = np . sum ( \n 
[ word . isupper ( ) for word in nltk . word_tokenize ( text ) ] ) \n 
meta [ pid ] [ ] = text . count ( ) \n 
~~ ~~ prepare_sent_features ( ) \n 
def get_features ( aid ) : \n 
~~~ return tuple ( meta [ aid ] [ fn ] for fn in feature_names ) \n 
~~ qa_X = np . asarray ( [ get_features ( aid ) for aid in all_answers ] ) \n 
classifying_answer = "good" \n 
if classifying_answer == "good" : \n 
~~~ qa_Y = np . asarray ( [ meta [ aid ] [ ] > 0 for aid in all_answers ] ) \n 
~~ elif classifying_answer == "poor" : \n 
~~~ qa_Y = np . asarray ( [ meta [ aid ] [ ] <= 0 for aid in all_answers ] ) \n 
classifying_answer ) \n 
~~ for idx , feat in enumerate ( feature_names ) : \n 
~~~ plot_feat_hist ( [ ( qa_X [ : , idx ] , feat ) ] ) \n 
~~ avg_scores_summary = [ ] \n 
def measure ( clf_class , parameters , name , data_size = None , plot = False ) : \n 
~~~ start_time_clf = time . time ( ) \n 
if data_size is None : \n 
~~~ X = qa_X \n 
Y = qa_Y \n 
~~~ X = qa_X [ : data_size ] \n 
Y = qa_Y [ : data_size ] \n 
~~ cv = KFold ( n = len ( X ) , n_folds = 10 , indices = True ) \n 
train_errors = [ ] \n 
test_errors = [ ] \n 
scores = [ ] \n 
roc_scores = [ ] \n 
fprs , tprs = [ ] , [ ] \n 
pr_scores = [ ] \n 
precisions , recalls , thresholds = [ ] , [ ] , [ ] \n 
for fold_idx , ( train , test ) in enumerate ( cv ) : \n 
~~~ X_train , y_train = X [ train ] , Y [ train ] \n 
X_test , y_test = X [ test ] , Y [ test ] \n 
only_one_class_in_train = len ( set ( y_train ) ) == 1 \n 
only_one_class_in_test = len ( set ( y_test ) ) == 1 \n 
if only_one_class_in_train or only_one_class_in_test : \n 
~~ clf = clf_class ( ** parameters ) \n 
clf . fit ( X_train , y_train ) \n 
train_score = clf . score ( X_train , y_train ) \n 
test_score = clf . score ( X_test , y_test ) \n 
train_errors . append ( 1 - train_score ) \n 
test_errors . append ( 1 - test_score ) \n 
scores . append ( test_score ) \n 
proba = clf . predict_proba ( X_test ) \n 
label_idx = 1 \n 
fpr , tpr , roc_thresholds = roc_curve ( y_test , proba [ : , label_idx ] ) \n 
precision , recall , pr_thresholds = precision_recall_curve ( \n 
y_test , proba [ : , label_idx ] ) \n 
roc_scores . append ( auc ( fpr , tpr ) ) \n 
fprs . append ( fpr ) \n 
tprs . append ( tpr ) \n 
pr_scores . append ( auc ( recall , precision ) ) \n 
precisions . append ( precision ) \n 
recalls . append ( recall ) \n 
thresholds . append ( pr_thresholds ) \n 
threshold_for_detecting_good_answers = 0.59 \n 
print ( classification_report ( y_test , proba [ : , label_idx ] > \n 
threshold_for_detecting_good_answers , target_names = [ , ] ) ) \n 
medium = np . argsort ( scores_to_sort ) [ len ( scores_to_sort ) / 2 ] \n 
if plot : \n 
~~~ plot_pr ( pr_scores [ medium ] , name , precisions [ medium ] , \n 
if hasattr ( clf , ) : \n 
~~~ plot_feat_importance ( feature_names , clf , name ) \n 
~~ ~~ summary = ( name , \n 
np . mean ( scores ) , np . std ( scores ) , \n 
np . mean ( roc_scores ) , np . std ( roc_scores ) , \n 
np . mean ( pr_scores ) , np . std ( pr_scores ) , \n 
time . time ( ) - start_time_clf ) \n 
print ( summary ) \n 
avg_scores_summary . append ( summary ) \n 
precisions = precisions [ medium ] \n 
recalls = recalls [ medium ] \n 
thresholds = np . hstack ( ( [ 0 ] , thresholds [ medium ] ) ) \n 
idx80 = precisions >= 0.8 \n 
idx80 ] [ 0 ] , thresholds [ idx80 ] [ 0 ] ) ) \n 
return np . mean ( train_errors ) , np . mean ( test_errors ) \n 
~~ def bias_variance_analysis ( clf_class , parameters , name ) : \n 
~~~ data_sizes = np . arange ( 60 , 2000 , 4 ) \n 
for data_size in data_sizes : \n 
~~~ train_error , test_error = measure ( \n 
clf_class , parameters , name , data_size = data_size ) \n 
train_errors . append ( train_error ) \n 
test_errors . append ( test_error ) \n 
~~ plot_bias_variance ( data_sizes , train_errors , \n 
~~ def k_complexity_analysis ( clf_class , parameters ) : \n 
~~~ ks = np . hstack ( ( np . arange ( 1 , 20 ) , np . arange ( 21 , 100 , 5 ) ) ) \n 
for k in ks : \n 
~~~ parameters [ ] = k \n 
train_error , test_error = measure ( \n 
clf_class , parameters , "%dNN" % k , data_size = 2000 ) \n 
~~ plot_k_complexity ( ks , train_errors , test_errors ) \n 
~~ for k in [ 5 ] : \n 
~~~ bias_variance_analysis ( neighbors . KNeighborsClassifier , { \n 
: k } , "%iNN" % k ) \n 
k_complexity_analysis ( neighbors . KNeighborsClassifier , { : k } ) \n 
~~ from sklearn . linear_model import LogisticRegression \n 
for C in [ 0.1 ] : \n 
bias_variance_analysis ( LogisticRegression , { : , : C } , name ) \n 
measure ( LogisticRegression , { : , : C } , name , plot = True ) \n 
~~ print ( "=" * 50 ) \n 
from operator import itemgetter \n 
for s in reversed ( sorted ( avg_scores_summary , key = itemgetter ( 1 ) ) ) : \n 
~~~ print ( "%-20s\\t%.5f\\t%.5f\\t%.5f\\t%.5f\\t%.5f\\t%.5f\\t%.5f" % s ) \n 
from load_ml100k import load \n 
data = load ( ) \n 
plt . gray ( ) \n 
plt . imshow ( data [ : 200 , : 200 ] , interpolation = ) \n 
from jug import TaskGenerator \n 
from time import sleep \n 
@ TaskGenerator \n 
def double ( x ) : \n 
~~~ sleep ( 4 ) \n 
return 2 * x \n 
~~ @ TaskGenerator \n 
def add ( a , b ) : \n 
~~~ return a + b \n 
def print_final_result ( oname , value ) : \n 
~~~ with open ( oname , ) as output : \n 
~~ ~~ input = 2 \n 
y = double ( input ) \n 
z = double ( y ) \n 
y2 = double ( 7 ) \n 
z2 = double ( y2 ) \n 
print_final_result ( , add ( z , z2 ) ) \n 
from SimpleCV . base import * \n 
class HaarCascade ( ) : \n 
_mCascade = None \n 
_mName = None \n 
_cache = { } \n 
_fhandle = None \n 
def __init__ ( self , fname = None , name = None ) : \n 
~~~ if ( name is None ) : \n 
~~~ self . _mName = fname \n 
~~~ self . _mName = name \n 
~~ if fname is not None : \n 
~~~ if os . path . exists ( fname ) : \n 
~~~ self . _fhandle = os . path . abspath ( fname ) \n 
~~~ self . _fhandle = os . path . join ( LAUNCH_PATH , , , fname ) \n 
if ( not os . path . exists ( self . _fhandle ) ) : \n 
~~ ~~ self . _mCascade = cv . Load ( self . _fhandle ) \n 
if HaarCascade . _cache . has_key ( self . _fhandle ) : \n 
~~~ self . _mCascade = HaarCascade . _cache [ self . _fhandle ] \n 
~~ HaarCascade . _cache [ self . _fhandle ] = self . _mCascade \n 
~~ ~~ def load ( self , fname = None , name = None ) : \n 
~~~ self . _mCascade = HaarCascade . _cache [ fname ] \n 
~~ ~~ def getCascade ( self ) : \n 
~~~ return self . _mCascade \n 
~~ def getName ( self ) : \n 
~~~ return self . _mName \n 
~~ def setName ( self , name ) : \n 
~~ def getFHandle ( self ) : \n 
~~~ return self . _fhandle \n 
~~ ~~ from SimpleCV . base import * \n 
from SimpleCV . ImageClass import Image , ImageSet \n 
from SimpleCV . DrawingLayer import * \n 
from SimpleCV . Features import FeatureExtractorBase \n 
class TreeClassifier : \n 
mClassNames = [ ] \n 
mDataSetRaw = [ ] \n 
mDataSetOrange = [ ] \n 
mClassifier = None \n 
mLearner = None \n 
mTree = None \n 
mFeatureExtractors = None \n 
mOrangeDomain = None \n 
mFlavorParams = None \n 
mTreeTypeDict = { \n 
mforestFlavorDict = { \n 
mBoostedFlavorDict = { \n 
mBaggedFlavorDict = { \n 
def __init__ ( self , featureExtractors = [ ] , flavor = , flavorDict = None ) : \n 
if not ORANGE_ENABLED : \n 
~~ self . mClassNames = [ ] \n 
self . mDataSetRaw = [ ] \n 
self . mDataSetOrange = [ ] \n 
self . mClassifier = None \n 
self . mLearner = None \n 
self . mTree = None \n 
self . mFeatureExtractors = None \n 
self . mOrangeDomain = None \n 
self . mFlavorParams = None \n 
self . mFlavor = self . mTreeTypeDict [ flavor ] \n 
if ( flavorDict is None ) : \n 
~~~ if ( self . mFlavor == self . mTreeTypeDict [ "Bagged" ] ) : \n 
~~~ self . mFlavorParams = self . mBaggedFlavorDict \n 
~~ elif ( self . mFlavor == self . mTreeTypeDict [ "Forest" ] ) : \n 
~~~ self . mFlavorParams = self . mBoostedFlavorDict \n 
~~~ self . mFlavorParams = flavorDict \n 
~~ self . mFeatureExtractors = featureExtractors \n 
~~ def load ( cls , fname ) : \n 
return pickle . load ( file ( fname ) ) \n 
~~ load = classmethod ( load ) \n 
def save ( self , fname ) : \n 
output = open ( fname , ) \n 
output . close ( ) \n 
~~~ mydict = self . __dict__ . copy ( ) \n 
self . mDataSetOrange = None \n 
del mydict [ ] \n 
return mydict \n 
~~ def __setstate__ ( self , mydict ) : \n 
~~~ self . __dict__ = mydict \n 
colNames = [ ] \n 
for extractor in self . mFeatureExtractors : \n 
~~~ colNames . extend ( extractor . getFieldNames ( ) ) \n 
~~ self . mOrangeDomain = orange . Domain ( map ( orange . FloatVariable , colNames ) , orange . EnumVariable ( "type" self . mDataSetOrange = orange . ExampleTable ( self . mOrangeDomain , self . mDataSetRaw ) \n 
if ( self . mFlavor == 0 ) : \n 
~~~ self . mLearner = orange . TreeLearner ( ) \n 
self . mClassifier = self . mLearner ( self . mDataSetOrange ) \n 
~~ elif ( self . mFlavor == 1 ) : #bagged \n 
~~~ self . mTree = orange . TreeLearner ( ) \n 
self . mLearner = orngEnsemble . BaggedLearner ( self . mTree , t = self . mFlavorParams [ "NClassifiers" self . mClassifier = self . mLearner ( self . mDataSetOrange ) \n 
~~ elif ( self . mFlavor == 2 ) : #forest \n 
self . mLearner = orngEnsemble . RandomForestLearner ( trees = self . mFlavorParams [ "NTrees" ] , attributes self . mClassifier = self . mLearner ( self . mDataSetOrange ) \n 
~~ elif ( self . mFlavor == 3 ) : #boosted \n 
self . mLearner = orngEnsemble . BoostedLearner ( self . mTree , t = self . mFlavorParams [ "NClassifiers" self . mClassifier = self . mLearner ( self . mDataSetOrange ) \n 
~~ ~~ def classify ( self , image ) : \n 
featureVector = [ ] \n 
~~~ feats = extractor . extract ( image ) \n 
if ( feats is not None ) : \n 
~~~ featureVector . extend ( feats ) \n 
~~ ~~ featureVector . extend ( [ self . mClassNames [ 0 ] ] ) \n 
test = orange . ExampleTable ( self . mOrangeDomain , [ featureVector ] ) \n 
c = self . mClassifier ( test [ 0 ] ) #classify \n 
~~ def setFeatureExtractors ( self , extractors ) : \n 
self . mFeatureExtractors = extractors \n 
return None \n 
~~ def _trainPath ( self , path , className , subset , disp , verbose ) : \n 
files = [ ] \n 
for ext in IMAGE_FORMATS : \n 
~~~ files . extend ( glob . glob ( os . path . join ( path , ext ) ) ) \n 
~~ if ( subset > 0 ) : \n 
~~~ nfiles = min ( subset , len ( files ) ) \n 
~~~ nfiles = len ( files ) \n 
~~ badFeat = False \n 
for i in range ( nfiles ) : \n 
~~~ infile = files [ i ] \n 
~~ img = Image ( infile ) \n 
~~~ feats = extractor . extract ( img ) \n 
~~~ badFeat = True \n 
~~ ~~ if ( badFeat ) : \n 
~~~ badFeat = False \n 
~~ featureVector . extend ( [ className ] ) \n 
self . mDataSetRaw . append ( featureVector ) \n 
text = + className \n 
self . _WriteText ( disp , img , text , Color . WHITE ) \n 
count = count + 1 \n 
del img \n 
~~ return count \n 
~~ def _trainImageSet ( self , imageset , className , subset , disp , verbose ) : \n 
badFeat = False \n 
if ( subset > 0 ) : \n 
~~~ imageset = imageset [ 0 : subset ] \n 
~~ for img in imageset : \n 
~~ featureVector = [ ] \n 
~~ def train ( self , images , classNames , disp = None , subset = - 1 , savedata = None , verbose = True ) : \n 
self . mClassNames = classNames \n 
for i in range ( len ( classNames ) ) : \n 
~~~ if ( isinstance ( images [ i ] , str ) ) : \n 
~~~ count = count + self . _trainPath ( images [ i ] , classNames [ i ] , subset , disp , verbose ) \n 
~~~ count = count + self . _trainImageSet ( images [ i ] , classNames [ i ] , subset , disp , verbose ) \n 
~~ ~~ colNames = [ ] \n 
~~ if ( count <= 0 ) : \n 
if ( savedata is not None ) : \n 
~~~ orange . saveTabDelimited ( savedata , self . mDataSetOrange ) \n 
~~ if ( self . mFlavor == 0 ) : \n 
~~ correct = 0 \n 
incorrect = 0 \n 
for i in range ( count ) : \n 
~~~ c = self . mClassifier ( self . mDataSetOrange [ i ] ) \n 
test = self . mDataSetOrange [ i ] . getclass ( ) \n 
~~ if ( test == c ) : \n 
~~~ correct = correct + 1 \n 
~~~ incorrect = incorrect + 1 \n 
~~ ~~ good = 100 * ( float ( correct ) / float ( count ) ) \n 
bad = 100 * ( float ( incorrect ) / float ( count ) ) \n 
confusion = 0 \n 
if ( len ( self . mClassNames ) > 2 ) : \n 
~~~ crossValidator = orngTest . learnAndTestOnLearnData ( [ self . mLearner ] , self . mDataSetOrange ) \n 
confusion = orngStat . confusionMatrices ( crossValidator ) [ 0 ] \n 
if ( confusion != 0 ) : \n 
~~~ classes = self . mDataSetOrange . domain . classVar . values \n 
print "\\t" + "\\t" . join ( classes ) \n 
for className , classConfusions in zip ( classes , confusion ) : \n 
~~~ print ( "%s" + ( "\\t%i" * len ( classes ) ) ) % ( ( className , ) + tuple ( classConfusions \n 
~~ ~~ ~~ if ( self . mFlavor == 0 ) : \n 
~~~ self . _PrintTree ( self . mClassifier ) \n 
~~ return [ good , bad , confusion ] \n 
~~ def test ( self , images , classNames , disp = None , subset = - 1 , savedata = None , verbose = True ) : \n 
correct = 0 \n 
if ( self . mOrangeDomain is None ) : \n 
~~~ self . mOrangeDomain = orange . Domain ( map ( orange . FloatVariable , colNames ) , orange . EnumVariable \n 
~~ ~~ dataset = [ ] \n 
~~~ [ dataset , cnt , crct ] = self . _testPath ( images [ i ] , classNames [ i ] , dataset , subset , disp , verbose count = count + cnt \n 
correct = correct + crct \n 
~~~ [ dataset , cnt , crct ] = self . _testImageSet ( images [ i ] , classNames [ i ] , dataset , subset , disp , verbose count = count + cnt \n 
~~ ~~ testData = orange . ExampleTable ( self . mOrangeDomain , dataset ) \n 
if savedata is not None : \n 
~~~ orange . saveTabDelimited ( savedata , testData ) \n 
~~ confusion = 0 \n 
~~~ crossValidator = orngTest . learnAndTestOnTestData ( [ self . mLearner ] , self . mDataSetOrange , testData confusion = orngStat . confusionMatrices ( crossValidator ) [ 0 ] \n 
~~ good = 100 * ( float ( correct ) / float ( count ) ) \n 
bad = 100 * ( float ( count - correct ) / float ( count ) ) \n 
~~~ print ( "%s" + ( "\\t%i" * len ( classes ) ) ) % ( ( className , ) + tuple ( classConfusions ~~ ~~ ~~ return [ good , bad , confusion ] \n 
~~ def _testPath ( self , path , className , dataset , subset , disp , verbose ) : \n 
~~ for i in range ( nfiles ) : \n 
~~~ del img \n 
dataset . append ( featureVector ) \n 
c = self . mClassifier ( test [ 0 ] ) \n 
testClass = test [ 0 ] . getclass ( ) \n 
if ( testClass == c ) : \n 
self . _WriteText ( disp , img , text , Color . GREEN ) \n 
correct = correct + 1 \n 
self . _WriteText ( disp , img , text , Color . RED ) \n 
~~ count = count + 1 \n 
~~ return ( [ dataset , count , correct ] ) \n 
~~ def _testImageSet ( self , imageset , className , dataset , subset , disp , verbose ) : \n 
~~ def _WriteText ( self , disp , img , txt , color ) : \n 
~~~ if ( disp is not None ) : \n 
~~~ txt = + txt + \n 
img = img . adaptiveScale ( disp . resolution ) \n 
layer = DrawingLayer ( ( img . width , img . height ) ) \n 
layer . setFontSize ( 60 ) \n 
layer . ezViewText ( txt , ( 20 , 20 ) , fgcolor = color ) \n 
img . addDrawingLayer ( layer ) \n 
img . applyLayers ( ) \n 
img . save ( disp ) \n 
~~ ~~ def _PrintTree ( self , x ) : \n 
~~~ if type ( x ) == orange . TreeClassifier : \n 
~~~ self . _PrintTree0 ( x . tree , 0 ) \n 
~~ elif type ( x ) == orange . TreeNode : \n 
~~~ self . _PrintTree0 ( x , 0 ) \n 
~~ ~~ def _PrintTree0 ( self , node , level ) : \n 
~~~ if not node : \n 
~~ if node . branchSelector : \n 
~~~ nodeDesc = node . branchSelector . classVar . name \n 
nodeCont = node . distribution \n 
for i in range ( len ( node . branches ) ) : \n 
self . _PrintTree0 ( node . branches [ i ] , level + 1 ) \n 
~~~ nodeCont = node . distribution \n 
majorClass = node . nodeClassifier . defaultValue \n 
~~ ~~ ~~ import os \n 
import socket \n 
import types \n 
import SocketServer \n 
import zipfile \n 
import colorsys \n 
import pygame as pg \n 
import scipy . ndimage as ndimage \n 
import scipy . cluster . vq as scv \n 
import scipy . spatial . distance as spsd \n 
import platform \n 
from numpy import linspace \n 
from scipy . interpolate import UnivariateSpline \n 
from warnings import warn \n 
from copy import copy \n 
from math import * \n 
from pkg_resources import load_entry_point \n 
from SimpleHTTPServer import SimpleHTTPRequestHandler \n 
from types import IntType , LongType , FloatType , InstanceType \n 
from numpy import int32 \n 
from numpy import uint8 \n 
from EXIF import * \n 
from pygame import gfxdraw \n 
from pickle import * \n 
~~~ import cv2 . cv as cv \n 
~~~ import cv \n 
~~ ~~ PIL_ENABLED = True \n 
~~~ from PIL import Image as pil \n 
from PIL import ImageFont as pilImageFont \n 
from PIL import ImageDraw as pilImageDraw \n 
from PIL import GifImagePlugin \n 
getheader = GifImagePlugin . getheader \n 
getdata = GifImagePlugin . getdata \n 
~~~ import Image as pil \n 
from GifImagePlugin import getheader , getdata \n 
~~~ PIL_ENABLED = False \n 
~~ ~~ FREENECT_ENABLED = True \n 
~~~ import freenect \n 
~~~ FREENECT_ENABLED = False \n 
~~ ZXING_ENABLED = True \n 
~~~ import zxing \n 
~~~ ZXING_ENABLED = False \n 
~~ OCR_ENABLED = True \n 
~~~ import tesseract \n 
~~~ OCR_ENABLED = False \n 
~~ PYSCREENSHOT_ENABLED = True \n 
~~~ import pyscreenshot \n 
~~~ PYSCREENSHOT_ENABLED = False \n 
~~ ORANGE_ENABLED = True \n 
~~~ import orange \n 
~~~ import Orange ; import orange \n 
import orngStat \n 
~~~ ORANGE_ENABLED = False \n 
~~ VIMBA_ENABLED = True \n 
~~~ import pymba \n 
~~~ VIMBA_ENABLED = False \n 
~~ except Exception : \n 
~~ class InitOptionsHandler ( object ) : \n 
~~~ self . on_notebook = False \n 
self . headless = False \n 
~~ def enable_notebook ( self ) : \n 
~~~ self . on_notebook = True \n 
~~ def set_headless ( self ) : \n 
~~~ os . environ [ "SDL_VIDEODRIVER" ] = "dummy" \n 
self . headless = True \n 
~~ ~~ init_options_handler = InitOptionsHandler ( ) \n 
~~~ import pygame as pg \n 
~~~ init_options_handler . set_headless ( ) \n 
~~ def is_number ( n ) : \n 
return type ( n ) in ( IntType , LongType , FloatType ) \n 
~~ def is_tuple ( n ) : \n 
return type ( n ) == tuple \n 
~~ def reverse_tuple ( n ) : \n 
return tuple ( reversed ( n ) ) \n 
~~ def find ( f , seq ) : \n 
for item in seq : \n 
~~~ if ( f == item ) : \n 
~~ def test ( ) : \n 
~~ def download_and_extract ( URL ) : \n 
if URL == None : \n 
~~ tmpdir = tempfile . mkdtemp ( ) \n 
filename = os . path . basename ( URL ) \n 
path = tmpdir + "/" + filename \n 
zdata = urllib2 . urlopen ( URL ) \n 
with open ( path , "wb" ) as local_file : \n 
~~~ local_file . write ( zdata . read ( ) ) \n 
~~ zfile = zipfile . ZipFile ( path ) \n 
~~~ zfile . extractall ( tmpdir ) \n 
~~ return tmpdir \n 
~~ def int_to_bin ( i ) : \n 
i1 = i % 256 \n 
i2 = int ( i / 256 ) \n 
return chr ( i1 ) + chr ( i2 ) \n 
~~ def npArray2cvMat ( inputMat , dataType = cv . CV_32FC1 ) : \n 
if ( type ( inputMat ) == np . ndarray ) : \n 
~~~ sz = len ( inputMat . shape ) \n 
temp_mat = None \n 
if ( dataType == cv . CV_32FC1 or dataType == cv . CV_32FC2 or dataType == cv . CV_32FC3 or dataType ~~~ temp_mat = np . array ( inputMat , dtype = ) \n 
~~ elif ( dataType == cv . CV_8UC1 or dataType == cv . CV_8UC2 or dataType == cv . CV_8UC3 or dataType ~~~ temp_mat = np . array ( inputMat , dtype = ) \n 
~~~ retVal = cv . CreateMat ( inputMat . shape [ 0 ] , 1 , dataType ) \n 
cv . SetData ( retVal , temp_mat . tostring ( ) , temp_mat . dtype . itemsize * temp_mat . shape [ 0 ] ) \n 
~~ elif ( sz == 2 ) : \n 
~~~ retVal = cv . CreateMat ( temp_mat . shape [ 0 ] , temp_mat . shape [ 1 ] , dataType ) \n 
cv . SetData ( retVal , temp_mat . tostring ( ) , temp_mat . dtype . itemsize * temp_mat . shape [ 1 ] ) \n 
~~ elif ( sz > 2 ) : \n 
~~ return retVal \n 
~~ ~~ consoleHandler = logging . StreamHandler ( ) \n 
formatter = logging . Formatter ( ) \n 
consoleHandler . setFormatter ( formatter ) \n 
logger = logging . getLogger ( ) \n 
logger . addHandler ( consoleHandler ) \n 
~~~ import IPython \n 
ipython_version = IPython . __version__ \n 
~~~ ipython_version = None \n 
~~ def exception_handler ( excType , excValue , traceback ) : \n 
~~~ logger . error ( "" , exc_info = ( excType , excValue , traceback ) ) \n 
~~ sys . excepthook = exception_handler \n 
def ipython_exception_handler ( shell , excType , excValue , traceback , tb_offset = 0 ) : \n 
~~ def init_logging ( log_level ) : \n 
~~~ logger . setLevel ( log_level ) \n 
~~ def read_logging_level ( log_level ) : \n 
~~~ levels_dict = { \n 
1 : logging . DEBUG , "debug" : logging . DEBUG , \n 
2 : logging . INFO , "info" : logging . INFO , \n 
3 : logging . WARNING , "warning" : logging . WARNING , \n 
4 : logging . ERROR , "error" : logging . ERROR , \n 
5 : logging . CRITICAL , "critical" : logging . CRITICAL \n 
if isinstance ( log_level , str ) : \n 
~~~ log_level = log_level . lower ( ) \n 
~~ if log_level in levels_dict : \n 
~~~ return levels_dict [ log_level ] \n 
~~ ~~ def get_logging_level ( ) : \n 
levels_dict = { \n 
10 : "DEBUG" , \n 
20 : "INFO" , \n 
30 : "WARNING" , \n 
40 : "ERROR" , \n 
50 : "CRITICAL" \n 
~~ def set_logging ( log_level , myfilename = None ) : \n 
if myfilename and ipython_version : \n 
~~~ if ipython_version . startswith ( "0.10" ) : \n 
~~~ __IPYTHON__ . set_custom_exc ( ( Exception , ) , ipython_exception_handler ) \n 
~~~ ip = get_ipython ( ) \n 
ip . set_custom_exc ( ( Exception , ) , ipython_exception_handler ) \n 
~~~ sys . exc_clear ( ) \n 
~~ ~~ level = read_logging_level ( log_level ) \n 
if level and myfilename : \n 
~~~ fileHandler = logging . FileHandler ( filename = myfilename ) \n 
fileHandler . setLevel ( level ) \n 
fileHandler . setFormatter ( formatter ) \n 
logger . addHandler ( fileHandler ) \n 
~~ elif level : \n 
~~ logger . setLevel ( level ) \n 
~~ def system ( ) : \n 
~~~ import platform \n 
~~~ from cv2 import __version__ \n 
~~ if ( PIL_ENABLED ) : \n 
~~ if ( ORANGE_ENABLED ) : \n 
~~ ~~ except ImportError : \n 
~~ class LazyProperty ( object ) : \n 
~~~ def __init__ ( self , func ) : \n 
~~~ self . _func = func \n 
self . __name__ = func . __name__ \n 
self . __doc__ = func . __doc__ \n 
~~ def __get__ ( self , obj , klass = None ) : \n 
~~~ if obj is None : return None \n 
result = obj . __dict__ [ self . __name__ ] = self . _func ( obj ) \n 
return result \n 
~~ ~~ IMAGE_FORMATS = ( , , , , \n 
, , , , , \n 
import SimpleCV \n 
import Tkinter \n 
Tkinter . Tk ( ) \n 
photo = ImageTk . PhotoImage ( image . getPIL ( ) ) \n 
label = Tkinter . Label ( image = photo ) \n 
time . sleep ( 5 ) \n 
import freenect \n 
import matplotlib . pyplot as mp \n 
import signal \n 
import frame_convert \n 
mp . ion ( ) \n 
image_rgb = None \n 
image_depth = None \n 
keep_running = True \n 
def display_depth ( dev , data , timestamp ) : \n 
~~~ global image_depth \n 
data = frame_convert . pretty_depth ( data ) \n 
mp . gray ( ) \n 
mp . figure ( 1 ) \n 
if image_depth : \n 
~~~ image_depth . set_data ( data ) \n 
~~~ image_depth = mp . imshow ( data , interpolation = , animated = True ) \n 
~~ mp . draw ( ) \n 
~~ def display_rgb ( dev , data , timestamp ) : \n 
~~~ global image_rgb \n 
mp . figure ( 2 ) \n 
if image_rgb : \n 
~~~ image_rgb . set_data ( data ) \n 
~~~ image_rgb = mp . imshow ( data , interpolation = , animated = True ) \n 
~~ def body ( * args ) : \n 
~~~ if not keep_running : \n 
~~~ raise freenect . Kill \n 
~~ ~~ def handler ( signum , frame ) : \n 
~~~ global keep_running \n 
keep_running = False \n 
~~ print ( ) \n 
signal . signal ( signal . SIGINT , handler ) \n 
freenect . runloop ( depth = display_depth , \n 
video = display_rgb , \n 
body = body ) \n 
from sklearn . datasets . samples_generator import make_blobs \n 
def plot_kmeans ( ) : \n 
~~~ X , y = make_blobs ( n_samples = 300 , centers = 4 , \n 
random_state = 0 , cluster_std = 0.60 ) \n 
y_pred = KMeans ( 4 ) . fit ( X ) . predict ( X ) \n 
fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 6 ) ) \n 
ax [ 0 ] . scatter ( X [ : , 0 ] , X [ : , 1 ] ) \n 
ax [ 0 ] . set_title ( ) \n 
ax [ 1 ] . scatter ( X [ : , 0 ] , X [ : , 1 ] , c = y ) \n 
ax [ 1 ] . set_title ( ) \n 
DATA_DIR = os . path . join ( \n 
os . path . dirname ( os . path . realpath ( __file__ ) ) , "data" ) \n 
CHART_DIR = os . path . join ( \n 
os . path . dirname ( os . path . realpath ( __file__ ) ) , "charts" ) \n 
for d in [ DATA_DIR , CHART_DIR ] : \n 
~~~ if not os . path . exists ( d ) : \n 
~~~ os . mkdir ( d ) \n 
warned_of_error = False \n 
def create_cloud ( oname , words , maxsize = 120 , fontname = ) : \n 
~~~ from pytagcloud import create_tag_image , make_tags \n 
~~~ if not warned_of_error : \n 
~~ words = [ ( w , int ( v * 10000 ) ) for v , w in words ] \n 
tags = make_tags ( words , maxsize = maxsize ) \n 
create_tag_image ( tags , oname , size = ( 1800 , 1200 ) , fontname = fontname ) \n 
from gzip import GzipFile \n 
dataset = [ [ int ( tok ) for tok in line . strip ( ) . split ( ) ] \n 
for line in GzipFile ( ) ] \n 
counts = defaultdict ( int ) \n 
for elem in chain ( * dataset ) : \n 
~~~ counts [ elem ] += 1 \n 
~~ counts = np . array ( list ( counts . values ( ) ) ) \n 
bins = [ 1 , 2 , 4 , 8 , 16 , 32 , 64 , 128 , 512 ] \n 
print ( . format ( , ) ) \n 
for i in range ( len ( bins ) ) : \n 
~~~ bot = bins [ i ] \n 
top = ( bins [ i + 1 ] if ( i + 1 ) < len ( bins ) else 100000000000 ) \n 
print ( . format ( \n 
bot , ( top if top < 1000 else ) , np . sum ( ( counts >= bot ) & ( counts < top ) ) ) ) \n 
from matplotlib import pylab \n 
from scipy . stats import norm , entropy \n 
from utils import CHART_DIR \n 
def mutual_info ( x , y , bins = 10 ) : \n 
~~~ counts_xy , bins_x , bins_y = np . histogram2d ( x , y , bins = ( bins , bins ) ) \n 
counts_x , bins = np . histogram ( x , bins = bins ) \n 
counts_y , bins = np . histogram ( y , bins = bins ) \n 
counts_xy += 1 \n 
counts_x += 1 \n 
counts_y += 1 \n 
P_xy = counts_xy / np . sum ( counts_xy , dtype = float ) \n 
P_x = counts_x / np . sum ( counts_x , dtype = float ) \n 
P_y = counts_y / np . sum ( counts_y , dtype = float ) \n 
I_xy = np . sum ( P_xy * np . log2 ( P_xy / ( P_x . reshape ( - 1 , 1 ) * P_y ) ) ) \n 
return I_xy / ( entropy ( counts_x ) + entropy ( counts_y ) ) \n 
~~ def plot_entropy ( ) : \n 
~~~ pylab . clf ( ) \n 
pylab . figure ( num = None , figsize = ( 5 , 4 ) ) \n 
pylab . title ( title ) \n 
pylab . ylabel ( "$H(X)$" ) \n 
pylab . xlim ( xmin = 0 , xmax = 1.1 ) \n 
x = np . arange ( 0.001 , 1 , 0.001 ) \n 
y = - x * np . log2 ( x ) - ( 1 - x ) * np . log2 ( 1 - x ) \n 
pylab . plot ( x , y ) \n 
pylab . autoscale ( tight = True ) \n 
pylab . grid ( True ) \n 
filename = "entropy_demo.png" \n 
pylab . savefig ( os . path . join ( CHART_DIR , filename ) , bbox_inches = "tight" ) \n 
~~ def _plot_mi_func ( x , y ) : \n 
~~~ mi = mutual_info ( x , y ) \n 
pylab . scatter ( x , y ) \n 
pylab . xlabel ( "$X_1$" ) \n 
pylab . ylabel ( "$X_2$" ) \n 
~~ def plot_mi_demo ( ) : \n 
pylab . clf ( ) \n 
pylab . figure ( num = None , figsize = ( 8 , 8 ) ) \n 
x = np . arange ( 0 , 10 , 0.2 ) \n 
pylab . subplot ( 221 ) \n 
y = 0.5 * x + norm . rvs ( 1 , scale = .01 , size = len ( x ) ) \n 
_plot_mi_func ( x , y ) \n 
pylab . subplot ( 222 ) \n 
y = 0.5 * x + norm . rvs ( 1 , scale = .1 , size = len ( x ) ) \n 
pylab . subplot ( 223 ) \n 
y = 0.5 * x + norm . rvs ( 1 , scale = 1 , size = len ( x ) ) \n 
pylab . subplot ( 224 ) \n 
y = norm . rvs ( 1 , scale = 10 , size = len ( x ) ) \n 
filename = "mi_demo_1.png" \n 
x = np . arange ( - 5 , 5 , 0.2 ) \n 
y = 0.5 * x ** 2 + norm . rvs ( 1 , scale = .01 , size = len ( x ) ) \n 
y = 0.5 * x ** 2 + norm . rvs ( 1 , scale = .1 , size = len ( x ) ) \n 
y = 0.5 * x ** 2 + norm . rvs ( 1 , scale = 1 , size = len ( x ) ) \n 
y = 0.5 * x ** 2 + norm . rvs ( 1 , scale = 10 , size = len ( x ) ) \n 
filename = "mi_demo_2.png" \n 
~~~ plot_entropy ( ) \n 
plot_mi_demo ( ) \n 
~~ from boto . exception import BotoServerError \n 
class InternalServerException ( BotoServerError ) : \n 
~~ class LimitExceededException ( BotoServerError ) : \n 
~~ class IdempotentParameterMismatchException ( BotoServerError ) : \n 
~~ class ResourceInUseException ( BotoServerError ) : \n 
~~ class ResourceNotFoundException ( BotoServerError ) : \n 
~~ class PredictorNotMountedException ( BotoServerError ) : \n 
~~ class InvalidInputException ( BotoServerError ) : \n 
from matplotlib . colors import ListedColormap \n 
from load import load_dataset \n 
from sklearn . neighbors import KNeighborsClassifier \n 
feature_names = [ \n 
def plot_decision ( features , labels , num_neighbors = 1 ) : \n 
y0 , y1 = features [ : , 2 ] . min ( ) * .9 , features [ : , 2 ] . max ( ) * 1.1 \n 
x0 , x1 = features [ : , 0 ] . min ( ) * .9 , features [ : , 0 ] . max ( ) * 1.1 \n 
X = np . linspace ( x0 , x1 , 1000 ) \n 
Y = np . linspace ( y0 , y1 , 1000 ) \n 
X , Y = np . meshgrid ( X , Y ) \n 
model = KNeighborsClassifier ( num_neighbors ) \n 
model . fit ( features [ : , ( 0 , 2 ) ] , labels ) \n 
C = model . predict ( np . vstack ( [ X . ravel ( ) , Y . ravel ( ) ] ) . T ) . reshape ( X . shape ) \n 
~~~ cmap = ListedColormap ( [ ( 1. , .7 , .7 ) , ( .7 , 1. , .7 ) , ( .7 , .7 , 1. ) ] ) \n 
~~~ cmap = ListedColormap ( [ ( 1. , 1. , 1. ) , ( .2 , .2 , .2 ) , ( .6 , .6 , .6 ) ] ) \n 
~~ fig , ax = plt . subplots ( ) \n 
ax . set_xlabel ( feature_names [ 0 ] ) \n 
ax . set_ylabel ( feature_names [ 2 ] ) \n 
ax . pcolormesh ( X , Y , C , cmap = cmap ) \n 
~~~ cmap = ListedColormap ( [ ( 1. , .0 , .0 ) , ( .1 , .6 , .1 ) , ( .0 , .0 , 1. ) ] ) \n 
ax . scatter ( features [ : , 0 ] , features [ : , 2 ] , c = labels , cmap = cmap ) \n 
~~~ for lab , ma in zip ( range ( 3 ) , "Do^" ) : \n 
~~~ ax . plot ( features [ labels == lab , 0 ] , features [ \n 
labels == lab , 2 ] , ma , c = ( 1. , 1. , 1. ) , ms = 6 ) \n 
~~ ~~ return fig , ax \n 
~~ features , labels = load_dataset ( ) \n 
names = sorted ( set ( labels ) ) \n 
labels = np . array ( [ names . index ( ell ) for ell in labels ] ) \n 
fig , ax = plot_decision ( features , labels ) \n 
features -= features . mean ( 0 ) \n 
features /= features . std ( 0 ) \n 
fig , ax = plot_decision ( features , labels , 11 ) \n 
from data import CHART_DIR \n 
from scipy . stats import norm \n 
from matplotlib import pyplot \n 
np . random . seed ( 3 ) \n 
num_per_class = 40 \n 
X = np . hstack ( ( norm . rvs ( 2 , size = num_per_class , scale = 2 ) , \n 
norm . rvs ( 8 , size = num_per_class , scale = 3 ) ) ) \n 
y = np . hstack ( ( np . zeros ( num_per_class ) , \n 
np . ones ( num_per_class ) ) ) \n 
def lr_model ( clf , X ) : \n 
~~~ return 1.0 / ( 1.0 + np . exp ( - ( clf . intercept_ + clf . coef_ * X ) ) ) \n 
logclf = LogisticRegression ( ) \n 
print ( logclf ) \n 
logclf . fit ( X . reshape ( num_per_class * 2 , 1 ) , y ) \n 
print ( np . exp ( logclf . intercept_ ) , np . exp ( logclf . coef_ . ravel ( ) ) ) \n 
print ( "P(x=-1)=%.2f\\tP(x=7)=%.2f" % \n 
( lr_model ( logclf , - 1 ) , lr_model ( logclf , 7 ) ) ) \n 
X_test = np . arange ( - 5 , 20 , 0.1 ) \n 
pyplot . figure ( figsize = ( 10 , 4 ) ) \n 
pyplot . xlim ( ( - 5 , 20 ) ) \n 
pyplot . scatter ( X , y , c = y ) \n 
pyplot . ylabel ( "class" ) \n 
pyplot . grid ( True , linestyle = , color = ) \n 
pyplot . savefig ( \n 
os . path . join ( CHART_DIR , "log_reg_example_data.png" ) , bbox_inches = "tight" ) \n 
def lin_model ( clf , X ) : \n 
~~~ return clf . intercept_ + clf . coef_ * X \n 
~~ from sklearn . linear_model import LinearRegression \n 
clf = LinearRegression ( ) \n 
print ( clf ) \n 
clf . fit ( X . reshape ( num_per_class * 2 , 1 ) , y ) \n 
X_odds = np . arange ( 0 , 1 , 0.001 ) \n 
pyplot . subplot ( 1 , 2 , 1 ) \n 
pyplot . plot ( X_test , lin_model ( clf , X_test ) ) \n 
X_ext = np . hstack ( ( X , norm . rvs ( 20 , size = 100 , scale = 5 ) ) ) \n 
y_ext = np . hstack ( ( y , np . ones ( 100 ) ) ) \n 
clf . fit ( X_ext . reshape ( num_per_class * 2 + 100 , 1 ) , y_ext ) \n 
pyplot . subplot ( 1 , 2 , 2 ) \n 
pyplot . scatter ( X_ext , y_ext , c = y_ext ) \n 
pyplot . plot ( X_ext , lin_model ( clf , X_ext ) ) \n 
os . path . join ( CHART_DIR , "log_reg_log_linear_fit.png" ) , bbox_inches = "tight" ) \n 
pyplot . plot ( X_test , lr_model ( logclf , X_test ) . ravel ( ) ) \n 
pyplot . plot ( X_test , np . ones ( X_test . shape [ 0 ] ) * 0.5 , "--" ) \n 
os . path . join ( CHART_DIR , "log_reg_example_fitted.png" ) , bbox_inches = "tight" ) \n 
X = np . arange ( 0 , 1 , 0.001 ) \n 
pyplot . xlim ( ( 0 , 1 ) ) \n 
pyplot . ylim ( ( 0 , 10 ) ) \n 
pyplot . plot ( X , X / ( 1 - X ) ) \n 
pyplot . xlabel ( "P" ) \n 
pyplot . plot ( X , np . log ( X / ( 1 - X ) ) ) \n 
os . path . join ( CHART_DIR , "log_reg_log_odds.png" ) , bbox_inches = "tight" ) \n 
def load ( ) : \n 
from scipy import sparse \n 
if not path . exists ( ) : \n 
~~ data = np . loadtxt ( ) \n 
ij = data [ : , : 2 ] \n 
values = data [ : , 2 ] \n 
reviews = sparse . csc_matrix ( ( values , ij . T ) ) . astype ( float ) \n 
return reviews . toarray ( ) \n 
~~ def get_train_test ( reviews = None , random_state = None ) : \n 
r = random . Random ( random_state ) \n 
if reviews is None : \n 
~~~ reviews = load ( ) \n 
~~ U , M = np . where ( reviews ) \n 
test_idxs = np . array ( r . sample ( range ( len ( U ) ) , len ( U ) // 10 ) ) \n 
train = reviews . copy ( ) \n 
train [ U [ test_idxs ] , M [ test_idxs ] ] = 0 \n 
test = np . zeros_like ( reviews ) \n 
test [ U [ test_idxs ] , M [ test_idxs ] ] = reviews [ U [ test_idxs ] , M [ test_idxs ] ] \n 
return train , test \n 
~~ from SimpleCV . base import * \n 
class NaiveBayesClassifier : \n 
def __init__ ( self , featureExtractors ) : \n 
~~~ if not ORANGE_ENABLED : \n 
self . mClassNames = [ ] \n 
~~ def classify ( self , image ) : \n 
~~ self . mClassifier = orange . BayesLearner ( self . mDataSetOrange ) \n 
~~~ crossValidator = orngTest . learnAndTestOnLearnData ( [ orange . BayesLearner ] , self . mDataSetOrange confusion = orngStat . confusionMatrices ( crossValidator ) [ 0 ] \n 
classes = self . mDataSetOrange . domain . classVar . values \n 
~~~ print ( "%s" + ( "\\t%i" * len ( classes ) ) ) % ( ( className , ) + tuple ( classConfusions ) ) \n 
~~ ~~ return [ good , bad , confusion ] \n 
self . mOrangeDomain = orange . Domain ( map ( orange . FloatVariable , colNames ) , orange . EnumVariable \n 
~~ dataset = [ ] \n 
~~~ crossValidator = orngTest . learnAndTestOnTestData ( [ orange . BayesLearner ( ) ] , self . mDataSetOrange confusion = orngStat . confusionMatrices ( crossValidator ) [ 0 ] \n 
~~ ~~ ~~ import math \n 
import operator \n 
def calcShannonEnt ( dataSet ) : \n 
~~~ numEntries = len ( dataSet ) \n 
labelCounts = { } \n 
for featVec in dataSet : \n 
~~~ currentLabel = featVec [ - 1 ] \n 
if currentLabel not in labelCounts . keys ( ) : \n 
~~~ labelCounts [ currentLabel ] = 0 \n 
~~ labelCounts [ currentLabel ] += 1 \n 
~~ shannonEnt = 0.0 \n 
for key in labelCounts : \n 
~~~ prob = float ( labelCounts [ key ] ) / numEntries \n 
shannonEnt -= prob * math . log ( prob , 2 ) \n 
~~ return shannonEnt \n 
~~ def createDataSet ( ) : \n 
~~~ dataSet = [ [ 1 , 0 , ] , [ 1 , 1 , ] , [ 0 , 1 , ] , [ 0 , 0 , ] ] \n 
labels = [ , ] \n 
return dataSet , labels \n 
~~ def splitDataSet ( dataSet , axis , value ) : \n 
~~~ retDataSet = [ ] \n 
~~~ if featVec [ axis ] == value : \n 
reducedFeatVec . extend ( featVec [ axis + 1 : ] ) \n 
retDataSet . append ( reducedFeatVec ) \n 
~~ ~~ return retDataSet \n 
~~ def chooseBestFeatureToSplit ( dataSet ) : \n 
baseEntropy = calcShannonEnt ( dataSet ) \n 
bestInfoGain = 0.0 ; bestFeature = - 1 \n 
newEntropy = 0.0 \n 
for value in uniqueVals : \n 
~~~ subDataSet = splitDataSet ( dataSet , i , value ) \n 
prob = len ( subDataSet ) / float ( len ( dataSet ) ) \n 
newEntropy += prob * calcShannonEnt ( subDataSet ) \n 
bestFeature = i \n 
~~ def majorityCnt ( classList ) : \n 
~~~ classCount = { } \n 
for vote in classList : \n 
~~~ if vote not in classCount . keys ( ) : classCount [ vote ] = 0 \n 
classCount [ vote ] += 1 \n 
~~ sortedClassCount = sorted ( classCount . iteritems ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) \n 
return sortedClassCount [ 0 ] [ 0 ] \n 
~~ def createTree ( dataSet , labels ) : \n 
~~~ classList = [ example [ - 1 ] for example in dataSet ] \n 
if classList . count ( classList [ 0 ] ) == len ( classList ) : \n 
~~~ return majorityCnt ( classList ) \n 
~~ bestFeat = chooseBestFeatureToSplit ( dataSet ) \n 
bestFeatLabel = labels [ bestFeat ] \n 
myTree = { bestFeatLabel : { } } \n 
del ( labels [ bestFeat ] ) \n 
featValues = [ example [ bestFeat ] for example in dataSet ] \n 
uniqueVals = set ( featValues ) \n 
~~~ subLabels = labels [ : ] \n 
myTree [ bestFeatLabel ] [ value ] = createTree ( splitDataSet ( dataSet , bestFeat , value ) , subLabels ) \n 
~~ return myTree \n 
~~ def classify ( inputTree , featLabels , testVec ) : \n 
~~~ firstStr = inputTree . keys ( ) [ 0 ] \n 
secondDict = inputTree [ firstStr ] \n 
featIndex = featLabels . index ( firstStr ) \n 
key = testVec [ featIndex ] \n 
valueOfFeat = secondDict [ key ] \n 
if isinstance ( valueOfFeat , dict ) : \n 
~~~ classLabel = classify ( valueOfFeat , featLabels , testVec ) \n 
~~ else : classLabel = valueOfFeat \n 
return classLabel \n 
~~ def getResult ( ) : \n 
~~~ dataSet , labels = createDataSet ( ) \n 
chooseBestFeatureToSplit ( dataSet ) \n 
mtree = createTree ( dataSet , labels ) \n 
print mtree \n 
print classify ( mtree , [ , ] , [ 0 , 0 ] ) \n 
~~~ getResult ( ) \n 
def load_dataset ( dataset_name ) : \n 
data = [ ] \n 
with open ( . format ( dataset_name ) ) as ifile : \n 
~~~ for line in ifile : \n 
~~~ tokens = line . strip ( ) . split ( ) \n 
data . append ( [ float ( tk ) for tk in tokens [ : - 1 ] ] ) \n 
labels . append ( tokens [ - 1 ] ) \n 
~~ ~~ data = np . array ( data ) \n 
labels = np . array ( labels ) \n 
return data , labels \n 
~~ import time \n 
from sklearn . cross_validation import ShuffleSplit \n 
from utils import load_sanders_data \n 
from utils import tweak_labels \n 
from sklearn . feature_extraction . text import TfidfVectorizer \n 
from sklearn . pipeline import Pipeline \n 
from sklearn . naive_bayes import MultinomialNB \n 
def create_ngram_model ( ) : \n 
~~~ tfidf_ngrams = TfidfVectorizer ( ngram_range = ( 1 , 3 ) , \n 
analyzer = "word" , binary = False ) \n 
clf = MultinomialNB ( ) \n 
pipeline = Pipeline ( [ ( , tfidf_ngrams ) , ( , clf ) ] ) \n 
return pipeline \n 
~~~ cv = ShuffleSplit ( \n 
n = len ( X ) , n_iter = 10 , test_size = 0.3 , random_state = 0 ) \n 
for train , test in cv : \n 
clf = clf_factory ( ) \n 
fpr , tpr , roc_thresholds = roc_curve ( y_test , proba [ : , 1 ] ) \n 
y_test , proba [ : , 1 ] ) \n 
~~ scores_to_sort = pr_scores \n 
median = np . argsort ( scores_to_sort ) [ len ( scores_to_sort ) / 2 ] \n 
~~~ plot_pr ( pr_scores [ median ] , name , "01" , precisions [ median ] , \n 
recalls [ median ] , label = name ) \n 
summary = ( np . mean ( scores ) , np . std ( scores ) , \n 
np . mean ( pr_scores ) , np . std ( pr_scores ) ) \n 
print ( "%.3f\\t%.3f\\t%.3f\\t%.3f\\t" % summary ) \n 
~~ return np . mean ( train_errors ) , np . mean ( test_errors ) \n 
~~ def print_incorrect ( clf , X , Y ) : \n 
~~~ Y_hat = clf . predict ( X ) \n 
wrong_idx = Y_hat != Y \n 
X_wrong = X [ wrong_idx ] \n 
Y_wrong = Y [ wrong_idx ] \n 
Y_hat_wrong = Y_hat [ wrong_idx ] \n 
for idx in range ( len ( X_wrong ) ) : \n 
( X_wrong [ idx ] , Y_hat_wrong [ idx ] , Y_wrong [ idx ] ) ) \n 
~~~ X_orig , Y_orig = load_sanders_data ( ) \n 
classes = np . unique ( Y_orig ) \n 
for c in classes : \n 
pos_neg = np . logical_or ( Y_orig == "positive" , Y_orig == "negative" ) \n 
X = X_orig [ pos_neg ] \n 
Y = Y_orig [ pos_neg ] \n 
Y = tweak_labels ( Y , [ "positive" ] ) \n 
X = X_orig \n 
Y = tweak_labels ( Y_orig , [ "positive" , "negative" ] ) \n 
Y = tweak_labels ( Y_orig , [ "positive" ] ) \n 
Y = tweak_labels ( Y_orig , [ "negative" ] ) \n 
~~ from __future__ import print_function \n 
def nn_movie ( ureviews , reviews , uid , mid , k = 1 ) : \n 
X = ureviews \n 
y = ureviews [ mid ] . copy ( ) \n 
y -= y . mean ( ) \n 
y /= ( y . std ( ) + 1e-5 ) \n 
corrs = np . dot ( X , y ) \n 
likes = corrs . argsort ( ) \n 
likes = likes [ : : - 1 ] \n 
c = 0 \n 
pred = 3. \n 
for ell in likes : \n 
~~~ if ell == mid : \n 
~~ if reviews [ uid , ell ] > 0 : \n 
~~~ pred = reviews [ uid , ell ] \n 
if c == k : \n 
~~~ return pred \n 
~~ c += 1 \n 
~~ ~~ return pred \n 
~~ def all_estimates ( reviews , k = 1 ) : \n 
reviews = reviews . astype ( float ) \n 
k -= 1 \n 
nusers , nmovies = reviews . shape \n 
estimates = np . zeros_like ( reviews ) \n 
for u in range ( nusers ) : \n 
~~~ ureviews = np . delete ( reviews , u , axis = 0 ) \n 
ureviews -= ureviews . mean ( 0 ) \n 
ureviews /= ( ureviews . std ( 0 ) + 1e-5 ) \n 
ureviews = ureviews . T . copy ( ) \n 
for m in np . where ( reviews [ u ] > 0 ) [ 0 ] : \n 
~~~ estimates [ u , m ] = nn_movie ( ureviews , reviews , u , m , k ) \n 
~~ ~~ return estimates \n 
~~~ from load_ml100k import load \n 
reviews = load ( ) \n 
estimates = all_estimates ( reviews ) \n 
error = ( estimates - reviews ) \n 
error 2 \n 
error = error [ reviews > 0 ] \n 
rmse = np . sqrt ( error . mean ( ) ) \n 
~~ from SimpleCV import * \n 
print "" \n 
if not ( inp == "" or inp . lower ( ) == "y" ) : \n 
sys . exit ( ) \n 
~~ machine_learning_data_set = "https://github.com/downloads/sightmachine/SimpleCV/machine_learning_dataset.zip" data_path = download_and_extract ( machine_learning_data_set ) \n 
w = 800 \n 
h = 600 \n 
n = 50 \n 
display = Display ( resolution = ( w , h ) ) \n 
hue = HueHistogramFeatureExtractor ( mNBins = 16 ) \n 
edge = EdgeHistogramFeatureExtractor ( ) \n 
bof = BOFFeatureExtractor ( ) \n 
bof . load ( ) \n 
haar = HaarLikeFeatureExtractor ( fname = "../Features/haar.txt" ) \n 
morph = MorphologyFeatureExtractor ( ) \n 
spath = data_path + "/data/structured/" \n 
upath = data_path + "/data/unstructured/" \n 
ball_path = spath + "ball/" \n 
basket_path = spath + "basket/" \n 
boat_path = spath + "boat/" \n 
cactus_path = spath + "cactus/" \n 
cup_path = spath + "cup/" \n 
duck_path = spath + "duck/" \n 
gb_path = spath + "greenblock/" \n 
match_path = spath + "matches/" \n 
rb_path = spath + "redblock/" \n 
s1_path = spath + "stuffed/" \n 
s2_path = spath + "stuffed2/" \n 
s3_path = spath + "stuffed3/" \n 
arbor_path = upath + "arborgreens/" \n 
football_path = upath + "football/" \n 
sanjuan_path = upath + "sanjuans/" \n 
extractors = [ hue ] \n 
path = [ cactus_path , cup_path , basket_path ] \n 
classes = [ , , ] \n 
props = { \n 
classifierSVMP = SVMClassifier ( extractors , props ) \n 
for p in path : \n 
~~~ data . append ( ImageSet ( p ) ) \n 
~~ classifierSVMP . train ( data , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierSVMP . test ( data , classes , disp = display , subset = n ) \n 
~~~ files . extend ( glob . glob ( os . path . join ( path [ 0 ] , ext ) ) ) \n 
~~ for i in range ( 10 ) : \n 
~~~ img = Image ( files [ i ] ) \n 
cname = classifierSVMP . classify ( img ) \n 
print ( files [ i ] + + cname ) \n 
~~ classifierSVMP . save ( ) \n 
testSVM = SVMClassifier . load ( ) \n 
#testSVM.setFeatureExtractors(extractors) \n 
files = glob . glob ( os . path . join ( path [ 0 ] , ) ) \n 
for i in range ( 10 ) : \n 
cname = testSVM . classify ( img ) \n 
extractors = [ hue , edge ] \n 
classifierSVMRBF = SVMClassifier ( extractors , props ) \n 
~~ classifierSVMRBF . train ( data , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierSVMRBF . test ( data , classes , disp = display , subset = n ) \n 
cname = classifierSVMRBF . classify ( img ) \n 
~~ classifierSVMRBF . save ( ) \n 
testSVMRBF = SVMClassifier . load ( ) \n 
#testSVMRBF.setFeatureExtractors(extractors) \n 
cname = testSVMRBF . classify ( img ) \n 
extractors = [ haar ] \n 
classifierBayes = NaiveBayesClassifier ( extractors ) # \n 
path = [ arbor_path , football_path , sanjuan_path ] \n 
classifierBayes . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierBayes . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierBayes . classify ( img ) \n 
~~ classifierBayes . save ( ) \n 
testBayes = NaiveBayesClassifier . load ( ) \n 
testBayes . setFeatureExtractors ( extractors ) \n 
cname = testBayes . classify ( img ) \n 
extractors = [ morph ] \n 
classifierForest = TreeClassifier ( extractors , flavor = ) # \n 
path = [ s1_path , s2_path , s3_path ] \n 
classifierForest . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierForest . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierForest . classify ( img ) \n 
~~ classifierForest . save ( ) \n 
testForest = TreeClassifier . load ( ) \n 
testForest . setFeatureExtractors ( extractors ) \n 
cname = testForest . classify ( img ) \n 
classifierBagTree = TreeClassifier ( extractors , flavor = ) # \n 
classifierBagTree . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierBagTree . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierBagTree . classify ( img ) \n 
~~ classifierBagTree . save ( ) \n 
testBagTree = TreeClassifier . load ( ) \n 
testBagTree . setFeatureExtractors ( extractors ) \n 
cname = testBagTree . classify ( img ) \n 
classifierTree = TreeClassifier ( featureExtractors = extractors ) \n 
classifierTree . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierTree . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierTree . classify ( img ) \n 
classifierTree . save ( ) \n 
testTree = TreeClassifier . load ( ) \n 
testTree . setFeatureExtractors ( extractors ) \n 
cname = testTree . classify ( img ) \n 
classifierBTree = TreeClassifier ( extractors , flavor = ) # \n 
classifierBTree . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierBTree . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierBTree . classify ( img ) \n 
~~ classifierBTree . save ( ) \n 
testBoostTree = TreeClassifier . load ( ) \n 
testBoostTree . setFeatureExtractors ( extractors ) \n 
cname = testBoostTree . classify ( img ) \n 
classifierKNN = KNNClassifier ( extractors ) # \n 
classifierKNN . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierKNN . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierKNN . classify ( img ) \n 
~~ classifierKNN . save ( ) \n 
testKNN = KNNClassifier . load ( ) \n 
testKNN . setFeatureExtractors ( extractors ) \n 
cname = testKNN . classify ( img ) \n 
~~ print "" \n 
from sklearn . cross_validation import cross_val_score \n 
from sklearn . preprocessing import StandardScaler \n 
features , labels = load_dataset ( ) \n 
ks = np . arange ( 1 , 161 ) \n 
classifier = KNeighborsClassifier ( ) \n 
classifier = Pipeline ( [ ( , StandardScaler ( ) ) , ( , classifier ) ] ) \n 
accuracies = [ ] \n 
~~~ classifier . set_params ( knn__n_neighbors = k ) \n 
crossed = cross_val_score ( classifier , features , labels ) \n 
accuracies . append ( crossed . mean ( ) ) \n 
~~ accuracies = np . array ( accuracies ) \n 
plt . plot ( ks , accuracies * 100 ) \n 
from utils import log_false_positives \n 
from utils import load_sent_word_net \n 
sent_word_net = load_sent_word_net ( ) \n 
phase = "03" \n 
emo_repl = { \n 
emo_repl_order = [ k for ( k_len , k ) in reversed ( \n 
sorted ( [ ( len ( k ) , k ) for k in list ( emo_repl . keys ( ) ) ] ) ) ] \n 
re_repl = { \n 
r"\\br\\b" : "are" , \n 
r"\\bu\\b" : "you" , \n 
r"\\bhaha\\b" : "ha" , \n 
r"\\bhahaha\\b" : "ha" , \n 
def create_ngram_model ( params = None ) : \n 
~~~ def preprocessor ( tweet ) : \n 
~~~ global emoticons_replaced \n 
tweet = tweet . lower ( ) \n 
for k in emo_repl_order : \n 
~~~ tweet = tweet . replace ( k , emo_repl [ k ] ) \n 
~~ for r , repl in re_repl . items ( ) : \n 
~~~ tweet = re . sub ( r , repl , tweet ) \n 
~~ return tweet \n 
~~ tfidf_ngrams = TfidfVectorizer ( preprocessor = preprocessor , \n 
analyzer = "word" ) \n 
if params : \n 
~~~ pipeline . set_params ( ** params ) \n 
~~ return pipeline \n 
clfs . append ( clf ) \n 
~~ if plot : \n 
~~~ scores_to_sort = pr_scores \n 
plot_pr ( pr_scores [ median ] , name , phase , precisions [ median ] , \n 
log_false_positives ( clfs [ median ] , X_test , y_test , name ) \n 
~~ summary = ( np . mean ( scores ) , np . std ( scores ) , \n 
~~ ~~ def get_best_model ( ) : \n 
~~~ best_params = dict ( tfidf__ngram_range = ( 1 , 2 ) , \n 
tfidf__min_df = 1 , \n 
tfidf__stop_words = None , \n 
tfidf__smooth_idf = False , \n 
tfidf__use_idf = False , \n 
tfidf__sublinear_tf = True , \n 
tfidf__binary = False , \n 
clf__alpha = 0.01 , \n 
best_clf = create_ngram_model ( best_params ) \n 
return best_clf \n 
plot = True ) \n 
from sklearn . metrics import precision_recall_curve , roc_curve \n 
from sklearn . metrics import auc \n 
from sklearn . metrics import confusion_matrix \n 
from utils import plot_pr , plot_roc , plot_confusion_matrix , GENRE_LIST \n 
from fft import read_fft \n 
genre_list = GENRE_LIST \n 
def train_model ( clf_factory , X , Y , name , plot = False ) : \n 
~~~ labels = np . unique ( Y ) \n 
cv = ShuffleSplit ( \n 
n = len ( X ) , n_iter = 1 , test_size = 0.3 , indices = True , random_state = 0 ) \n 
pr_scores = defaultdict ( list ) \n 
precisions , recalls , thresholds = defaultdict ( \n 
list ) , defaultdict ( list ) , defaultdict ( list ) \n 
roc_scores = defaultdict ( list ) \n 
tprs = defaultdict ( list ) \n 
fprs = defaultdict ( list ) \n 
cms = [ ] \n 
y_pred = clf . predict ( X_test ) \n 
cm = confusion_matrix ( y_test , y_pred ) \n 
cms . append ( cm ) \n 
~~~ y_label_test = np . asarray ( y_test == label , dtype = int ) \n 
proba_label = proba [ : , label ] \n 
y_label_test , proba_label ) \n 
pr_scores [ label ] . append ( auc ( recall , precision ) ) \n 
precisions [ label ] . append ( precision ) \n 
recalls [ label ] . append ( recall ) \n 
thresholds [ label ] . append ( pr_thresholds ) \n 
fpr , tpr , roc_thresholds = roc_curve ( y_label_test , proba_label ) \n 
roc_scores [ label ] . append ( auc ( fpr , tpr ) ) \n 
tprs [ label ] . append ( tpr ) \n 
fprs [ label ] . append ( fpr ) \n 
~~ ~~ if plot : \n 
~~~ for label in labels : \n 
scores_to_sort = roc_scores [ label ] \n 
plot_pr ( pr_scores [ label ] [ median ] , desc , precisions [ label ] [ median ] , \n 
recalls [ label ] [ median ] , label = % genre_list [ label ] ) \n 
plot_roc ( roc_scores [ label ] [ median ] , desc , tprs [ label ] [ median ] , \n 
fprs [ label ] [ median ] , label = % genre_list [ label ] ) \n 
~~ ~~ all_pr_scores = np . asarray ( pr_scores . values ( ) ) . flatten ( ) \n 
np . mean ( all_pr_scores ) , np . std ( all_pr_scores ) ) \n 
return np . mean ( train_errors ) , np . mean ( test_errors ) , np . asarray ( cms ) \n 
~~ def create_model ( ) : \n 
~~~ from sklearn . linear_model . logistic import LogisticRegression \n 
clf = LogisticRegression ( ) \n 
return clf \n 
~~~ X , y = read_fft ( genre_list ) \n 
train_avg , test_avg , cms = train_model ( \n 
cm_avg = np . mean ( cms , axis = 0 ) \n 
cm_norm = cm_avg / np . sum ( cm_avg , axis = 0 ) \n 
plot_confusion_matrix ( cm_norm , genre_list , "fft" , \n 
from SimpleCV . Features . Features import Feature , FeatureSet \n 
from SimpleCV . Color import Color \n 
from SimpleCV . ImageClass import Image \n 
from SimpleCV . Features . Detection import ShapeContextDescriptor \n 
import scipy . stats as sps \n 
class ShapeContextClassifier ( ) : \n 
~~~ def __init__ ( self , images , labels ) : \n 
~~~ from sklearn import neighbors \n 
~~ self . imgMap = { } \n 
self . ptMap = { } \n 
self . descMap = { } \n 
self . knnMap = { } \n 
self . blobCount = { } \n 
self . labels = labels \n 
self . images = images \n 
warnings . simplefilter ( "ignore" ) \n 
for i in range ( 0 , len ( images ) ) : \n 
self . imgMap [ labels [ i ] ] = images [ i ] \n 
pts , desc , count = self . _image2FeatureVector ( images [ i ] ) \n 
self . blobCount [ labels [ i ] ] = count \n 
self . ptMap [ labels [ i ] ] = pts \n 
self . descMap [ labels [ i ] ] = desc \n 
knn = neighbors . KNeighborsClassifier ( ) \n 
knn . fit ( desc , range ( 0 , len ( pts ) ) ) \n 
self . knnMap [ labels [ i ] ] = knn \n 
~~ ~~ def _image2FeatureVector ( self , img ) : \n 
fulllist = [ ] \n 
raw_descriptors = [ ] \n 
blobs = img . findBlobs ( minsize = 50 ) \n 
if ( blobs is not None ) : \n 
~~~ count = len ( blobs ) \n 
for b in blobs : \n 
~~~ fulllist += b . _filterSCPoints ( ) \n 
raw_descriptors = blobs [ 0 ] . _generateSC ( fulllist ) \n 
~~ ~~ return fulllist , raw_descriptors , count \n 
~~ def _getMatch ( self , model_scd , test_scd ) : \n 
~~~ correspondence , distance = self . _doMatching ( model_scd , test_scd ) \n 
return self . _matchQuality ( distances ) \n 
~~ def _doMatching ( self , model_name , test_scd ) : \n 
~~~ myPts = len ( test_scd ) \n 
otPts = len ( self . ptMap [ model_name ] ) \n 
distance = [ ] \n 
results = [ ] \n 
for sample in test_scd : \n 
~~~ best = self . knnMap [ model_name ] . predict ( sample ) \n 
scd = self . descMap [ model_name ] [ idx ] \n 
temp = np . sqrt ( np . sum ( ( ( sample - scd ) ** 2 ) ) ) \n 
if ( math . isnan ( temp ) ) : \n 
~~~ temp = sys . maxint \n 
~~ distance . append ( temp ) \n 
~~ return [ otherIdx , distance ] \n 
~~ def _matchQuality ( self , distances ) : \n 
~~~ tmean = np . mean ( distances ) \n 
std = np . std ( distances ) \n 
return tmean , std \n 
~~ def _buildMatchDict ( self , image , countBlobs ) : \n 
~~~ points , descriptors , count = self . _image2FeatureVector ( image ) \n 
matchDict = { } \n 
matchStd = { } \n 
for key , value in self . descMap . items ( ) : \n 
~~~ correspondence , distances = self . _doMatching ( key , descriptors ) \n 
result , std = self . _matchQuality ( distances ) \n 
matchDict [ key ] = result \n 
matchStd [ key ] = std \n 
~~ elif ( not countBlobs ) : \n 
~~ ~~ return points , descriptors , count , matchDict , matchStd \n 
~~ def classify ( self , image , blobFilter = True ) : \n 
points , descriptors , count , matchDict , matchStd = self . _buildMatchDict ( image , blobFilter ) \n 
best = sys . maxint \n 
for k , v in matchDict . items ( ) : \n 
~~~ if ( v < best ) : \n 
~~~ best = v \n 
best_name = k \n 
~~ ~~ return best_name , best , matchDict , matchStd \n 
~~ def getTopNMatches ( self , image , n = 3 , blobFilter = True ) : \n 
n = np . clip ( n , 1 , len ( self . labels ) ) \n 
best_matches = list ( sorted ( matchDict , key = matchDict . __getitem__ ) ) \n 
retList = [ ] \n 
for k in best_matches : \n 
~~~ retList . append ( ( k , matchDict [ k ] ) ) \n 
~~ return retList [ 0 : n ] , matchDict , matchStd \n 
~~ ~~ from SimpleCV . Color import Color \n 
from SimpleCV . base import time , cv , np \n 
class TrackSet ( FeatureSet ) : \n 
~~~ import cv2 \n 
~~ def __init__ ( self ) : \n 
~~~ self . kalman = None \n 
self . predict_pt = ( 0 , 0 ) \n 
self . __kalman ( ) \n 
~~ def append ( self , f ) : \n 
list . append ( self , f ) \n 
ts = self \n 
if ts [ 0 ] . area <= 0 : \n 
~~ f . sizeRatio = float ( ts [ - 1 ] . area ) / float ( ts [ 0 ] . area ) \n 
f . vel = self . __pixelVelocity ( ) \n 
f . rt_vel = self . __pixleVelocityRealTime ( ) \n 
self . __setKalman ( ) \n 
self . __predictKalman ( ) \n 
self . __changeMeasure ( ) \n 
self . __correctKalman ( ) \n 
f . predict_pt = self . predict_pt \n 
f . state_pt = self . state_pt \n 
~~ def trimList ( self , num ) : \n 
for i in range ( num ) : \n 
~~~ ts . pop ( 0 ) \n 
~~ ~~ def areaRatio ( self ) : \n 
return np . array ( [ f . areaRatio for f in self ] ) \n 
~~ def drawPath ( self , color = Color . GREEN , thickness = 2 ) : \n 
img = self [ - 1 ] . image \n 
for i in range ( len ( ts ) - 1 ) : \n 
~~~ img . drawLine ( ( ts [ i ] . center ) , ( ts [ i + 1 ] . center ) , color = color , thickness = thickness ) \n 
~~ ~~ def draw ( self , color = Color . GREEN , rad = 1 , thickness = 1 ) : \n 
f = self [ - 1 ] \n 
f . image . drawCircle ( f . center , rad , color , thickness ) \n 
~~ def drawBB ( self , color = Color . GREEN , thickness = 3 ) : \n 
f . image . drawRectangle ( f . bb_x , f . bb_y , f . w , f . h , color , thickness ) \n 
~~ def trackLength ( self ) : \n 
return len ( self ) \n 
~~ def trackImages ( self , cv2_numpy = False ) : \n 
if cv2_numpy : \n 
~~~ return [ f . cv2numpy for f in self ] \n 
~~ return [ f . image for f in self ] \n 
~~ def BBTrack ( self ) : \n 
return [ f . bb for f in self ] \n 
~~ def __pixelVelocity ( self ) : \n 
if len ( ts ) < 2 : \n 
~~~ return ( 0 , 0 ) \n 
~~ dx = ts [ - 1 ] . x - ts [ - 2 ] . x \n 
dy = ts [ - 1 ] . y - ts [ - 2 ] . y \n 
return ( dx , dy ) \n 
~~ def pixelVelocity ( self ) : \n 
return np . array ( [ f . vel for f in self ] ) \n 
~~ def __pixleVelocityRealTime ( self ) : \n 
dt = ts [ - 1 ] . time - ts [ - 2 ] . time \n 
return ( float ( dx ) / dt , float ( dy ) / dt ) \n 
~~ def pixleVelocityRealTime ( self ) : \n 
return np . array ( [ f . rt_vel for f in self ] ) \n 
~~ def showCoordinates ( self , pos = None , color = Color . GREEN , size = None ) : \n 
f = ts [ - 1 ] \n 
img = f . image \n 
if not pos : \n 
~~~ imgsize = img . size ( ) \n 
pos = ( imgsize [ 0 ] - 120 , 10 ) \n 
~~~ size = 16 \n 
img . drawText ( text , pos [ 0 ] , pos [ 1 ] , color , size ) \n 
~~ def showSizeRatio ( self , pos = None , color = Color . GREEN , size = None ) : \n 
pos = ( imgsize [ 0 ] - 120 , 30 ) \n 
~~ def showPixelVelocity ( self , pos = None , color = Color . GREEN , size = None ) : \n 
vel = f . vel \n 
pos = ( imgsize [ 0 ] - 120 , 50 ) \n 
~~ def showPixelVelocityRT ( self , pos = None , color = Color . GREEN , size = None ) : \n 
vel_rt = f . rt_vel \n 
pos = ( imgsize [ 0 ] - 120 , 90 ) \n 
~~ def processTrack ( self , func ) : \n 
return [ func ( f . image ) for f in self ] \n 
~~ def getBackground ( self ) : \n 
imgs = self . trackImages ( cv2_numpy = True ) \n 
f = imgs [ 0 ] \n 
avg = np . float32 ( f ) \n 
for img in imgs [ 1 : ] : \n 
~~~ f = img \n 
cv2 . accumulateWeighted ( f , avg , 0.01 ) \n 
res = cv2 . convertScaleAbs ( avg ) \n 
~~ return Image ( res , cv2image = True ) \n 
~~ def __kalman ( self ) : \n 
~~~ self . kalman = cv . CreateKalman ( 4 , 2 , 0 ) \n 
self . kalman_process_noise = cv . CreateMat ( 4 , 1 , cv . CV_32FC1 ) \n 
self . kalman_measurement = cv . CreateMat ( 2 , 1 , cv . CV_32FC1 ) \n 
~~ def __setKalman ( self ) : \n 
~~~ ts = self \n 
~~~ self . kalman_x = ts [ - 1 ] . x \n 
self . kalman_y = ts [ - 1 ] . y \n 
~~~ self . kalman_x = ts [ - 2 ] . x \n 
self . kalman_y = ts [ - 2 ] . y \n 
~~ self . kalman . state_pre [ 0 , 0 ] = self . kalman_x \n 
self . kalman . state_pre [ 1 , 0 ] = self . kalman_y \n 
self . kalman . state_pre [ 2 , 0 ] = self . predict_pt [ 0 ] \n 
self . kalman . state_pre [ 3 , 0 ] = self . predict_pt [ 1 ] \n 
self . kalman . transition_matrix [ 0 , 0 ] = 1 \n 
self . kalman . transition_matrix [ 0 , 1 ] = 0 \n 
self . kalman . transition_matrix [ 0 , 2 ] = 1 \n 
self . kalman . transition_matrix [ 0 , 3 ] = 0 \n 
self . kalman . transition_matrix [ 1 , 0 ] = 0 \n 
self . kalman . transition_matrix [ 1 , 1 ] = 1 \n 
self . kalman . transition_matrix [ 1 , 2 ] = 0 \n 
self . kalman . transition_matrix [ 1 , 3 ] = 1 \n 
self . kalman . transition_matrix [ 2 , 0 ] = 0 \n 
self . kalman . transition_matrix [ 2 , 1 ] = 0 \n 
self . kalman . transition_matrix [ 2 , 2 ] = 1 \n 
self . kalman . transition_matrix [ 2 , 3 ] = 0 \n 
self . kalman . transition_matrix [ 3 , 0 ] = 0 \n 
self . kalman . transition_matrix [ 3 , 1 ] = 0 \n 
self . kalman . transition_matrix [ 3 , 2 ] = 0 \n 
self . kalman . transition_matrix [ 3 , 3 ] = 1 \n 
cv . SetIdentity ( self . kalman . measurement_matrix , cv . RealScalar ( 1 ) ) \n 
cv . SetIdentity ( self . kalman . process_noise_cov , cv . RealScalar ( 1e-5 ) ) \n 
cv . SetIdentity ( self . kalman . measurement_noise_cov , cv . RealScalar ( 1e-1 ) ) \n 
cv . SetIdentity ( self . kalman . error_cov_post , cv . RealScalar ( 1 ) ) \n 
~~ def __predictKalman ( self ) : \n 
~~~ self . kalman_prediction = cv . KalmanPredict ( self . kalman ) \n 
self . predict_pt = ( self . kalman_prediction [ 0 , 0 ] , self . kalman_prediction [ 1 , 0 ] ) \n 
~~ def __correctKalman ( self ) : \n 
~~~ self . kalman_estimated = cv . KalmanCorrect ( self . kalman , self . kalman_measurement ) \n 
self . state_pt = ( self . kalman_estimated [ 0 , 0 ] , self . kalman_estimated [ 1 , 0 ] ) \n 
~~ def __changeMeasure ( self ) : \n 
self . kalman_measurement [ 0 , 0 ] = ts [ - 1 ] . x \n 
self . kalman_measurement [ 1 , 0 ] = ts [ - 1 ] . y \n 
~~ def predictedCoordinates ( self ) : \n 
return np . array ( [ f . predict_pt for f in self ] ) \n 
~~ def predictX ( self ) : \n 
return np . array ( [ f . predict_pt [ 0 ] for f in self ] ) \n 
~~ def predictY ( self ) : \n 
return np . array ( [ f . predict_pt [ 1 ] for f in self ] ) \n 
~~ def drawPredicted ( self , color = Color . GREEN , rad = 1 , thickness = 1 ) : \n 
f . image . drawCircle ( f . predict_pt , rad , color , thickness ) \n 
~~ def drawCorrected ( self , color = Color . GREEN , rad = 1 , thickness = 1 ) : \n 
f . image . drawCircle ( f . state_pt , rad , color , thickness ) \n 
~~ def drawPredictedPath ( self , color = Color . GREEN , thickness = 2 ) : \n 
for i in range ( 1 , len ( ts ) - 1 ) : \n 
~~~ img . drawLine ( ( ts [ i ] . predict_pt ) , ( ts [ i + 1 ] . predict_pt ) , color = color , thickness = thickness ) \n 
~~ ~~ def showPredictedCoordinates ( self , pos = None , color = Color . GREEN , size = None ) : \n 
pos = ( 5 , 10 ) \n 
~~ def showCorrectedCoordinates ( self , pos = None , color = Color . GREEN , size = None ) : \n 
pos = ( 5 , 40 ) \n 
~~ def correctX ( self ) : \n 
return np . array ( [ f . state_pt [ 0 ] for f in self ] ) \n 
~~ def correctY ( self ) : \n 
return np . array ( [ f . state_pt [ 1 ] for f in self ] ) \n 
~~ def correctedCoordinates ( self ) : \n 
return np . array ( [ f . state_pt for f in self ] ) \n 
~~ def drawCorrectedPath ( self , color = Color . GREEN , thickness = 2 ) : \n 
~~~ img . drawLine ( ( ts [ i ] . state_pt ) , ( ts [ i + 1 ] . state_pt ) , color = color , thickness = thickness ) \n 
~~ ~~ ~~ \n 
print __doc__ \n 
import gtk \n 
class app ( gtk . Window ) : \n 
~~~ edge_threshold = 100 \n 
max_threshold = 500 \n 
min_threshold = 0 \n 
window_width = 500 \n 
window_height = 500 \n 
#Variables \n 
current_image = None \n 
~~~ super ( app , self ) . __init__ ( ) \n 
self . set_position ( gtk . WIN_POS_CENTER ) \n 
self . set_decorated ( True ) \n 
self . set_has_frame ( False ) \n 
self . set_resizable ( False ) \n 
self . set_default_size ( self . window_width , self . window_height ) \n 
self . connect ( "destroy" , gtk . main_quit ) \n 
vbox = gtk . VBox ( spacing = 4 ) \n 
scale = gtk . HScale ( ) \n 
scale . set_range ( self . min_threshold , self . max_threshold ) \n 
scale . set_size_request ( 500 , 25 ) \n 
scale . set_value ( ( self . max_threshold + self . min_threshold ) / 2 ) \n 
scale . connect ( "value-changed" , self . update_threshold ) \n 
vbox . add ( scale ) \n 
info = gtk . Label ( ) \n 
vbox . add ( info ) \n 
new_image = self . process_image ( ) \n 
converted_image = gtk . gdk . pixbuf_new_from_array ( new_image , gtk . gdk . COLORSPACE_RGB , 8 ) \n 
image = gtk . Image ( ) \n 
image . set_from_pixbuf ( converted_image ) \n 
image . show ( ) \n 
vbox . add ( image ) \n 
self . current_image = image \n 
self . add ( vbox ) \n 
self . show_all ( ) \n 
def process_image ( self ) : \n 
~~~ img = SimpleCV . Image ( ) . rotate90 ( ) \n 
edges = img . edges ( self . edge_threshold ) \n 
numpy_img = edges . getNumpy ( ) \n 
return numpy_img \n 
~~ def update_threshold ( self , w ) : \n 
~~~ self . edge_threshold = w . get_value ( ) \n 
updated_image = self . process_image ( ) \n 
converted_image = gtk . gdk . pixbuf_new_from_array ( updated_image , gtk . gdk . COLORSPACE_RGB , 8 ) \n 
self . current_image . set_from_pixbuf ( converted_image ) \n 
~~ ~~ \n 
program1 = app ( ) \n 
program2 = app ( ) \n 
gtk . main ( ) \n 
from SimpleCV import Camera , VideoStream , Color , Display \n 
fname = \n 
outname = \n 
tags = \n 
vs = VideoStream ( fps = 20 , filename = fname , framefill = False ) \n 
cam = Camera ( ) \n 
disp = Display ( ( 800 , 600 ) ) \n 
while disp . isNotDone ( ) : \n 
~~~ img = cam . getImage ( ) \n 
img = img . edges ( ) \n 
vs . writeFrame ( img ) \n 
call ( + params , shell = True ) \n 
import cv \n 
cv . NamedWindow ( ) \n 
ind = 0 \n 
print ( % __doc__ ) \n 
def get_depth ( ind ) : \n 
~~~ return frame_convert . pretty_depth_cv ( freenect . sync_get_depth ( ind ) [ 0 ] ) \n 
~~ def get_video ( ind ) : \n 
~~~ return frame_convert . video_cv ( freenect . sync_get_video ( ind ) [ 0 ] ) \n 
~~ while 1 : \n 
~~~ print ( ind ) \n 
~~~ depth = get_depth ( ind ) \n 
video = get_video ( ind ) \n 
~~~ ind = 0 \n 
~~ ind += 1 \n 
cv . ShowImage ( , depth ) \n 
cv . ShowImage ( , video ) \n 
if cv . WaitKey ( 10 ) == 27 : \n 
from sklearn import svm \n 
from sklearn import linear_model \n 
from sklearn . neighbors . nearest_centroid import NearestCentroid \n 
from Modules . ConsoleOutput import ConsoleOutput \n 
from Modules . NaturalLanguage import NaturalLanguageObject \n 
_MAX_DECIMAL_PLACES = 10 \n 
class NNSentenceStructure : \n 
~~~ trainingData = [ ] \n 
trainingDataResults = [ ] \n 
clf = None \n 
def loadVectorsIntoNetwork ( self , inNormalisedData , targetResult ) : \n 
~~~ self . trainingData . extend ( inNormalisedData ) \n 
self . trainingDataResults . extend ( targetResult ) \n 
~~ def FitNetwork ( self ) : \n 
~~~ countItems = len ( self . trainingDataResults ) \n 
self . _fit ( self . trainingData , self . trainingDataResults ) \n 
self . trainingData = None \n 
self . trainingDataResults = None \n 
~~ def _fit ( self , dataVector , targetVector ) : \n 
self . clf . fit ( np . asarray ( dataVector , dtype = "float" ) , np . asarray ( targetVector , dtype = "float" ) ) \n 
~~ def getPrediction ( self , inNormalisedData ) : \n 
~~~ pred = self . clf . predict ( inNormalisedData ) \n 
return float ( round ( pred [ 0 ] , _MAX_DECIMAL_PLACES ) ) \n 
~~ def getPredictionProbability ( self , inNormalisedData ) : \n 
~~~ predProb = self . clf . predict_proba ( inNormalisedData ) \n 
return predProb \n 
~~~ self . clf = KNeighborsClassifier ( ) \n 
~~ ~~ class NNVocabulary : \n 
_Networks = [ ] \n 
_Vocabulary = None \n 
def loadVectorsIntoNetworkByIndex ( self , index , inNormalisedData , targetResult ) : \n 
~~~ self . trainingData [ index ] . append ( [ inNormalisedData ] ) \n 
self . trainingDataResults [ index ] . append ( targetResult ) \n 
~~ def loadVocab ( self , index , inNormal , inResult ) : \n 
~~~ self . _Vocabulary [ index ] . append ( ( inNormal , inResult ) ) \n 
~~ def _getFromVocab ( self , inIdentifier , inNormal ) : \n 
~~~ for index , i in enumerate ( NaturalLanguageObject . _Identifiers ) : \n 
~~~ if ( inIdentifier == i ) : \n 
~~~ for index2 , val in enumerate ( self . _Vocabulary [ index ] ) : \n 
~~~ if ( val [ 0 ] == inNormal ) : \n 
~~~ return val [ 1 ] \n 
~~ ~~ ~~ ~~ ~~ def FitNetwork ( self ) : \n 
~~~ countItems = 0 \n 
for index , val in enumerate ( self . trainingData ) : \n 
~~~ if ( len ( self . trainingData [ index ] ) > 0 ) : \n 
~~~ self . _fit ( index , self . trainingData [ index ] , self . trainingDataResults [ index ] ) \n 
countItems = countItems + len ( self . trainingData [ index ] ) \n 
print ( "\\n" ) \n 
~~ def _fit ( self , index , dataVector , targetVector ) : \n 
~~~ self . _Networks [ index ] . fit ( np . asarray ( dataVector , dtype = "float" ) , np . asarray ( targetVector , dtype \n 
~~ def getPrediction ( self , inNormalisedData , inIdentifier ) : \n 
~~~ pred = 0 \n 
for index , i in enumerate ( NaturalLanguageObject . _Identifiers ) : \n 
~~~ if ( len ( self . _Vocabulary [ index ] ) > 0 ) : \n 
~~~ pred = self . _Networks [ index ] . predict ( inNormalisedData ) \n 
~~~ return 0 \n 
~~ ~~ ~~ if ( pred == 0 ) : \n 
~~ return float ( round ( pred [ 0 ] , _MAX_DECIMAL_PLACES ) ) \n 
~~ def getPredictionProbability ( self , inNormalisedData , inIdentifier ) : \n 
~~~ pred = [ [ 0 ] ] \n 
~~~ return self . _Networks [ index ] . predict_proba ( inNormalisedData ) \n 
~~ ~~ ~~ return pred \n 
~~ def getPredictedWord ( self , inNormalisedData , inIdentifier ) : \n 
~~~ pred = self . getPrediction ( inNormalisedData , inIdentifier ) \n 
return self . _getFromVocab ( inIdentifier , pred ) \n 
~~~ for index in range ( 0 , len ( NaturalLanguageObject . _Identifiers ) ) : \n 
~~~ nn = KNeighborsClassifier ( ) \n 
self . _Networks . append ( nn ) \n 
~~ self . trainingData = [ list ( ) for _ in range ( len ( NaturalLanguageObject . _Identifiers ) ) ] \n 
self . trainingDataResults = [ list ( ) for _ in range ( len ( NaturalLanguageObject . _Identifiers ) ) ] \n 
self . _Vocabulary = [ list ( ) for _ in range ( len ( NaturalLanguageObject . _Identifiers ) ) ] \n 
from sklearn import neighbors , datasets , linear_model \n 
cmap_light = ListedColormap ( [ , , ] ) \n 
cmap_bold = ListedColormap ( [ , , ] ) \n 
def plot_iris_knn ( ) : \n 
~~~ iris = datasets . load_iris ( ) \n 
y = iris . target \n 
knn = neighbors . KNeighborsClassifier ( n_neighbors = 3 ) \n 
knn . fit ( X , y ) \n 
x_min , x_max = X [ : , 0 ] . min ( ) - .1 , X [ : , 0 ] . max ( ) + .1 \n 
y_min , y_max = X [ : , 1 ] . min ( ) - .1 , X [ : , 1 ] . max ( ) + .1 \n 
xx , yy = np . meshgrid ( np . linspace ( x_min , x_max , 100 ) , \n 
np . linspace ( y_min , y_max , 100 ) ) \n 
Z = knn . predict ( np . c_ [ xx . ravel ( ) , yy . ravel ( ) ] ) \n 
Z = Z . reshape ( xx . shape ) \n 
pl . pcolormesh ( xx , yy , Z , cmap = cmap_light ) \n 
pl . scatter ( X [ : , 0 ] , X [ : , 1 ] , c = y , cmap = cmap_bold ) \n 
pl . xlabel ( ) \n 
pl . ylabel ( ) \n 
pl . axis ( ) \n 
~~ def plot_polynomial_regression ( ) : \n 
~~~ rng = np . random . RandomState ( 0 ) \n 
x = 2 * rng . rand ( 100 ) - 1 \n 
f = lambda t : 1.2 * t ** 2 + .1 * t ** 3 - .4 * t ** 5 - .5 * t ** 9 \n 
y = f ( x ) + .4 * rng . normal ( size = 100 ) \n 
x_test = np . linspace ( - 1 , 1 , 100 ) \n 
pl . scatter ( x , y , s = 4 ) \n 
X = np . array ( [ x ** i for i in range ( 5 ) ] ) . T \n 
X_test = np . array ( [ x_test ** i for i in range ( 5 ) ] ) . T \n 
regr = linear_model . LinearRegression ( ) \n 
regr . fit ( X , y ) \n 
pl . plot ( x_test , regr . predict ( X_test ) , label = ) \n 
X = np . array ( [ x ** i for i in range ( 10 ) ] ) . T \n 
X_test = np . array ( [ x_test ** i for i in range ( 10 ) ] ) . T \n 
pl . legend ( loc = ) \n 
pl . title ( ) \n 
pl . plot ( x_test , f ( x_test ) , label = "truth" ) \n 
~~ from sklearn . datasets import load_iris \n 
labels = data . target_names [ data . target ] \n 
best_acc = - 1.0 \n 
for fi in range ( features . shape [ 1 ] ) : \n 
~~~ thresh = features [ : , fi ] . copy ( ) \n 
thresh . sort ( ) \n 
for t in thresh : \n 
~~~ pred = ( features [ : , fi ] > t ) \n 
acc = ( pred == is_virginica ) . mean ( ) \n 
acc_neg = ( ( ~ pred ) == is_virginica ) . mean ( ) \n 
if acc_neg > acc : \n 
~~~ acc = acc_neg \n 
negated = True \n 
~~~ negated = False \n 
~~ if acc > best_acc : \n 
~~~ best_acc = acc \n 
best_fi = fi \n 
best_t = t \n 
best_is_negated = negated \n 
~~ ~~ ~~ print ( . format best_t , data . feature_names [ best_fi ] , best_fi , best_acc ) ) \n 
import collections \n 
DATA_DIR = "data" \n 
CHART_DIR = "charts" \n 
if not os . path . exists ( DATA_DIR ) : \n 
~~ if not os . path . exists ( CHART_DIR ) : \n 
~~~ os . mkdir ( CHART_DIR ) \n 
~~ def tweak_labels ( Y , pos_sent_list ) : \n 
~~~ pos = Y == pos_sent_list [ 0 ] \n 
for sent_label in pos_sent_list [ 1 : ] : \n 
~~~ pos |= Y == sent_label \n 
~~ Y = np . zeros ( Y . shape [ 0 ] ) \n 
Y [ pos ] = 1 \n 
Y = Y . astype ( int ) \n 
~~ def load_sanders_data ( dirname = "." , line_count = - 1 ) : \n 
topics = [ ] \n 
tweets = [ ] \n 
with open ( os . path . join ( DATA_DIR , dirname , "corpus.csv" ) , "r" ) as csvfile : \n 
~~~ metareader = csv . reader ( csvfile , delimiter = , quotechar = \'"\' ) \n 
for line in metareader : \n 
~~~ count += 1 \n 
if line_count > 0 and count > line_count : \n 
~~ topic , label , tweet_id = line \n 
tweet_fn = os . path . join ( \n 
DATA_DIR , dirname , , % tweet_id ) \n 
~~~ tweet = json . load ( open ( tweet_fn , "r" ) ) \n 
~~ except IOError : \n 
~~ if in tweet and tweet [ ] [ ] == "en" : \n 
~~~ topics . append ( topic ) \n 
labels . append ( label ) \n 
tweets . append ( tweet [ ] ) \n 
~~ ~~ ~~ tweets = np . asarray ( tweets ) \n 
labels = np . asarray ( labels ) \n 
return tweets , labels \n 
~~ def plot_pr ( auc_score , name , phase , precision , recall , label = None ) : \n 
pylab . fill_between ( recall , precision , alpha = 0.5 ) \n 
pylab . plot ( recall , precision , lw = 1 ) \n 
pylab . xlim ( [ 0.0 , 1.0 ] ) \n 
pylab . ylim ( [ 0.0 , 1.0 ] ) \n 
pylab . xlabel ( ) \n 
pylab . ylabel ( ) \n 
pylab . title ( % ( auc_score , label ) ) \n 
pylab . savefig ( os . path . join ( CHART_DIR , "pr_%s_%s.png" % \n 
( filename , phase ) ) , bbox_inches = "tight" ) \n 
~~ def show_most_informative_features ( vectorizer , clf , n = 20 ) : \n 
~~~ c_f = sorted ( zip ( clf . coef_ [ 0 ] , vectorizer . get_feature_names ( ) ) ) \n 
top = list ( zip ( c_f [ : n ] , c_f [ : - ( n + 1 ) : - 1 ] ) ) \n 
for ( c1 , f1 ) , ( c2 , f2 ) in top : \n 
~~~ print ( "\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s" % ( c1 , f1 , c2 , f2 ) ) \n 
~~ ~~ def plot_log ( ) : \n 
pylab . figure ( num = None , figsize = ( 6 , 5 ) ) \n 
y = np . log ( x ) \n 
pylab . title ( ) \n 
filename = \n 
~~ def plot_feat_importance ( feature_names , clf , name ) : \n 
coef_ = clf . coef_ \n 
important = np . argsort ( np . absolute ( coef_ . ravel ( ) ) ) \n 
f_imp = feature_names [ important ] \n 
coef = coef_ . ravel ( ) [ important ] \n 
inds = np . argsort ( coef ) \n 
f_imp = f_imp [ inds ] \n 
coef = coef [ inds ] \n 
xpos = np . array ( list ( range ( len ( coef ) ) ) ) \n 
pylab . bar ( xpos , coef , width = 1 ) \n 
pylab . title ( % ( name ) ) \n 
ax = pylab . gca ( ) \n 
ax . set_xticks ( np . arange ( len ( coef ) ) ) \n 
labels = ax . set_xticklabels ( f_imp ) \n 
~~~ label . set_rotation ( 90 ) \n 
pylab . savefig ( os . path . join ( \n 
CHART_DIR , "feat_imp_%s.png" % filename ) , bbox_inches = "tight" ) \n 
~~ def plot_feat_hist ( data_name_list , filename = None ) : \n 
num_rows = 1 + ( len ( data_name_list ) - 1 ) / 2 \n 
num_cols = 1 if len ( data_name_list ) == 1 else 2 \n 
pylab . figure ( figsize = ( 5 * num_cols , 4 * num_rows ) ) \n 
for i in range ( num_rows ) : \n 
~~~ for j in range ( num_cols ) : \n 
~~~ pylab . subplot ( num_rows , num_cols , 1 + i * num_cols + j ) \n 
x , name = data_name_list [ i * num_cols + j ] \n 
pylab . title ( name ) \n 
max_val = np . max ( x ) \n 
if max_val <= 1.0 : \n 
~~~ bins = 50 \n 
~~ elif max_val > 50 : \n 
~~~ bins = max_val \n 
~~ n , bins , patches = pylab . hist ( \n 
x , bins = bins , normed = 1 , facecolor = , alpha = 0.75 ) \n 
~~ ~~ if not filename : \n 
~~~ filename = "feat_hist_%s.png" % name \n 
~~ pylab . savefig ( os . path . join ( CHART_DIR , filename ) , bbox_inches = "tight" ) \n 
~~ def plot_bias_variance ( data_sizes , train_errors , test_errors , name ) : \n 
pylab . plot ( \n 
data_sizes , train_errors , "-" , data_sizes , test_errors , "--" , lw = 1 ) \n 
pylab . grid ( ) \n 
pylab . savefig ( os . path . join ( CHART_DIR , "bv_" + name + ".png" ) ) \n 
~~ def load_sent_word_net ( ) : \n 
~~~ sent_scores = collections . defaultdict ( list ) \n 
sentiwordnet_path = os . path . join ( DATA_DIR , "SentiWordNet_3.0.0_20130122.txt" ) \n 
if not os . path . exists ( sentiwordnet_path ) : \n 
~~ with open ( sentiwordnet_path , ) as csvfile : \n 
~~~ reader = csv . reader ( csvfile , delimiter = , quotechar = \'"\' ) \n 
for line in reader : \n 
~~~ if line [ 0 ] . startswith ( "#" ) : \n 
~~ if len ( line ) == 1 : \n 
~~ POS , ID , PosScore , NegScore , SynsetTerms , Gloss = line \n 
if len ( POS ) == 0 or len ( ID ) == 0 : \n 
~~~ term = term . split ( "#" ) [ 0 ] \n 
key = "%s/%s" % ( POS , term . split ( "#" ) [ 0 ] ) \n 
sent_scores [ key ] . append ( ( float ( PosScore ) , float ( NegScore ) ) ) \n 
~~ ~~ ~~ for key , value in sent_scores . items ( ) : \n 
~~~ sent_scores [ key ] = np . mean ( value , axis = 0 ) \n 
~~ return sent_scores \n 
~~ def log_false_positives ( clf , X , y , name ) : \n 
~~~ false_positive = clf . predict ( X ) != y \n 
for tweet , false_class in zip ( X [ false_positive ] , y [ false_positive ] ) : \n 
~~~ f . write ( "%s\\t%s\\n" % \n 
( false_class , tweet . encode ( "ascii" , "ignore" ) ) ) \n 
~~ ~~ ~~ if __name__ == : \n 
~~~ plot_log ( ) \n 
~~ ~~ GENRE_DIR = None \n 
GENRE_LIST = [ "classical" , "jazz" , "country" , "pop" , "rock" , "metal" ] \n 
TEST_DIR = None \n 
if GENRE_DIR is None or TEST_DIR is None : \n 
~~ def plot_confusion_matrix ( cm , genre_list , name , title ) : \n 
pylab . matshow ( cm , fignum = False , cmap = , vmin = 0 , vmax = 1.0 ) \n 
ax = pylab . axes ( ) \n 
ax . set_xticks ( range ( len ( genre_list ) ) ) \n 
ax . set_xticklabels ( genre_list ) \n 
ax . xaxis . set_ticks_position ( "bottom" ) \n 
ax . set_yticks ( range ( len ( genre_list ) ) ) \n 
ax . set_yticklabels ( genre_list ) \n 
pylab . colorbar ( ) \n 
pylab . grid ( False ) \n 
pylab . show ( ) \n 
pylab . savefig ( \n 
os . path . join ( CHART_DIR , "confusion_matrix_%s.png" % name ) , bbox_inches = "tight" ) \n 
~~ def plot_pr ( auc_score , name , precision , recall , label = None ) : \n 
os . path . join ( CHART_DIR , "pr_" + filename + ".png" ) , bbox_inches = "tight" ) \n 
~~ def plot_roc ( auc_score , name , tpr , fpr , label = None ) : \n 
pylab . plot ( [ 0 , 1 ] , [ 0 , 1 ] , ) \n 
pylab . plot ( fpr , tpr ) \n 
pylab . fill_between ( fpr , tpr , alpha = 0.5 ) \n 
pylab . title ( % \n 
( auc_score , label ) , verticalalignment = "bottom" ) \n 
os . path . join ( CHART_DIR , "roc_" + filename + ".png" ) , bbox_inches = "tight" ) \n 
top = zip ( c_f [ : n ] , c_f [ : - ( n + 1 ) : - 1 ] ) \n 
xpos = np . array ( range ( len ( coef ) ) ) \n 
~~ import matplotlib . pyplot as plt \n 
from numpy import * \n 
def loadDataSet ( fileName ) : \n 
~~~ dataMat = [ ] ; labelMat = [ ] \n 
fr = open ( fileName ) \n 
for line in fr . readlines ( ) : \n 
~~~ lineArr = line . strip ( ) . split ( ) \n 
dataMat . append ( [ float ( lineArr [ 0 ] ) , float ( lineArr [ 1 ] ) ] ) \n 
labelMat . append ( float ( lineArr [ 2 ] ) ) \n 
~~ return dataMat , labelMat \n 
~~ def selectJrand ( i , m ) : \n 
while ( j == i ) : \n 
~~~ j = int ( random . uniform ( 0 , m ) ) \n 
~~ return j \n 
~~ def clipAlpha ( aj , H , L ) : \n 
~~~ if aj > H : \n 
~~~ aj = H \n 
~~ if L > aj : \n 
~~~ aj = L \n 
~~ return aj \n 
~~ def smoSimple ( dataMatIn , classLabels , C , toler , maxIter ) : \n 
~~~ dataMatrix = mat ( dataMatIn ) ; labelMat = mat ( classLabels ) . transpose ( ) \n 
b = 0 ; m , n = shape ( dataMatrix ) \n 
alphas = mat ( zeros ( ( m , 1 ) ) ) \n 
iter = 0 \n 
while ( iter < maxIter ) : \n 
~~~ alphaPairsChanged = 0 \n 
for i in range ( m ) : \n 
~~~ fXi = float ( multiply ( alphas , labelMat ) . T * ( dataMatrix * dataMatrix [ i , : ] . T ) ) + b \n 
if ( ( labelMat [ i ] * Ei < - toler ) and ( alphas [ i ] < C ) ) or ( ( labelMat [ i ] * Ei > toler ) and ( alphas ~~~ j = selectJrand ( i , m ) \n 
fXj = float ( multiply ( alphas , labelMat ) . T * ( dataMatrix * dataMatrix [ j , : ] . T ) ) + b \n 
Ej = fXj - float ( labelMat [ j ] ) \n 
alphaIold = alphas [ i ] . copy ( ) ; alphaJold = alphas [ j ] . copy ( ) ; \n 
if ( labelMat [ i ] != labelMat [ j ] ) : \n 
~~~ L = max ( 0 , alphas [ j ] - alphas [ i ] ) \n 
H = min ( C , C + alphas [ j ] - alphas [ i ] ) \n 
~~~ L = max ( 0 , alphas [ j ] + alphas [ i ] - C ) \n 
H = min ( C , alphas [ j ] + alphas [ i ] ) \n 
~~ eta = 2.0 * dataMatrix [ i , : ] * dataMatrix [ j , : ] . T - dataMatrix [ i , : ] * dataMatrix [ i , : ] . T - if eta >= 0 : print "eta>=0" ; continue \n 
alphas [ j ] -= labelMat [ j ] * ( Ei - Ej ) / eta \n 
alphas [ j ] = clipAlpha ( alphas [ j ] , H , L ) \n 
elif ( 0 < alphas [ j ] ) and ( C > alphas [ j ] ) : b = b2 \n 
else : b = ( b1 + b2 ) / 2.0 \n 
alphaPairsChanged += 1 \n 
~~ ~~ if ( alphaPairsChanged == 0 ) : iter += 1 \n 
else : iter = 0 \n 
~~ return b , alphas \n 
~~ def matplot ( dataMat , lableMat ) : \n 
~~~ xcord1 = [ ] ; ycord1 = [ ] \n 
xcord2 = [ ] ; ycord2 = [ ] \n 
xcord3 = [ ] ; ycord3 = [ ] \n 
~~~ if lableMat [ i ] == 1 : \n 
~~~ xcord1 . append ( dataMat [ i ] [ 0 ] ) \n 
ycord1 . append ( dataMat [ i ] [ 1 ] ) \n 
~~~ xcord2 . append ( dataMat [ i ] [ 0 ] ) \n 
ycord2 . append ( dataMat [ i ] [ 1 ] ) \n 
~~ ~~ b , alphas = smoSimple ( dataMat , labelMat , 0.6 , 0.001 , 40 ) \n 
for j in range ( 100 ) : \n 
~~~ if alphas [ j ] > 0 : \n 
~~~ xcord3 . append ( dataMat [ j ] [ 0 ] ) \n 
ycord3 . append ( dataMat [ j ] [ 1 ] ) \n 
~~ ~~ fig = plt . figure ( ) \n 
ax . scatter ( xcord1 , ycord1 , s = 30 , c = , marker = ) \n 
ax . scatter ( xcord2 , ycord2 , s = 30 , c = ) \n 
ax . scatter ( xcord3 , ycord3 , s = 80 , c = ) \n 
ax . plot ( ) \n 
plt . xlabel ( ) ; plt . ylabel ( ) ; \n 
~~~ dataMat , labelMat = loadDataSet ( ) \n 
~~ CHART_DIR = os . path . join ( \n 
if not os . path . exists ( CHART_DIR ) : \n 
from sklearn . metrics import mean_squared_error , r2_score \n 
from sklearn . datasets import load_svmlight_file \n 
from sklearn . linear_model import LinearRegression \n 
data , target = load_svmlight_file ( ) \n 
lr = LinearRegression ( ) \n 
lr . fit ( data , target ) \n 
pred = lr . predict ( data ) \n 
print ( . format ( np . sqrt ( mean_squared_error ( target , pred ) ) ) ) \n 
print ( . format ( r2_score ( target , pred ) ) ) \n 
pred = np . zeros_like ( target ) \n 
kf = KFold ( len ( target ) , n_folds = 5 ) \n 
for train , test in kf : \n 
~~~ lr . fit ( data [ train ] , target [ train ] ) \n 
pred [ test ] = lr . predict ( data [ test ] ) \n 
~~ print ( . format ( np . sqrt ( mean_squared_error ( target , pred ) ) ) ) \n 
import mahotas as mh \n 
from glob import glob \n 
from features import texture , chist \n 
from sklearn . linear_model import LogisticRegression \n 
basedir = \n 
haralicks = [ ] \n 
chists = [ ] \n 
images = glob ( . format ( basedir ) ) \n 
for fname in sorted ( images ) : \n 
~~~ imc = mh . imread ( fname ) \n 
haralicks . append ( texture ( mh . colors . rgb2grey ( imc ) ) ) \n 
chists . append ( chist ( imc ) ) \n 
labels . append ( fname [ : - len ( ) ] ) \n 
haralicks = np . array ( haralicks ) \n 
chists = np . array ( chists ) \n 
haralick_plus_chists = np . hstack ( [ chists , haralicks ] ) \n 
clf = Pipeline ( [ ( , StandardScaler ( ) ) , \n 
( , LogisticRegression ( ) ) ] ) \n 
from sklearn import cross_validation \n 
cv = cross_validation . LeaveOneOut ( len ( images ) ) \n 
scores = cross_validation . cross_val_score ( \n 
clf , haralicks , labels , cv = cv ) \n 
scores . mean ( ) ) ) \n 
clf , chists , labels , cv = cv ) \n 
clf , haralick_plus_chists , labels , cv = cv ) \n 
print ( scores . mean ( ) ) ) \n 
def tfidf ( t , d , D ) : \n 
~~~ tf = float ( d . count ( t ) ) / sum ( d . count ( w ) for w in set ( d ) ) \n 
idf = sp . log ( float ( len ( D ) ) / ( len ( [ doc for doc in D if t in doc ] ) ) ) \n 
return tf * idf \n 
~~ a , abb , abc = [ "a" ] , [ "a" , "b" , "b" ] , [ "a" , "b" , "c" ] \n 
D = [ a , abb , abc ] \n 
print ( tfidf ( "a" , a , D ) ) \n 
print ( tfidf ( "b" , abb , D ) ) \n 
print ( tfidf ( "a" , abc , D ) ) \n 
print ( tfidf ( "b" , abc , D ) ) \n 
print ( tfidf ( "c" , abc , D ) ) \n 
from sklearn . linear_model import Lasso \n 
from sklearn . datasets import load_boston \n 
boston = load_boston ( ) \n 
x = boston . data \n 
y = boston . target \n 
las = Lasso ( normalize = 1 ) \n 
alphas = np . logspace ( - 5 , 2 , 1000 ) \n 
alphas , coefs , _ = las . path ( x , y , alphas = alphas ) \n 
ax . plot ( alphas , coefs . T ) \n 
ax . set_xscale ( ) \n 
ax . set_xlim ( alphas . max ( ) , alphas . min ( ) ) \n 
ax . set_xlabel ( ) \n 
ax . set_ylabel ( ) \n 
from scipy . spatial import distance \n 
images . sort ( ) \n 
for fname in images : \n 
imc = imc [ 200 : - 200 , 200 : - 200 ] \n 
~~ haralicks = np . array ( haralicks ) \n 
features = np . hstack ( [ chists , haralicks ] ) \n 
sc = StandardScaler ( ) \n 
features = sc . fit_transform ( features ) \n 
dists = distance . squareform ( distance . pdist ( features ) ) \n 
fig , axes = plt . subplots ( 2 , 9 , figsize = ( 16 , 8 ) ) \n 
for ax in axes . flat : \n 
~~~ ax . set_xticks ( [ ] ) \n 
ax . set_yticks ( [ ] ) \n 
~~ for ci , i in enumerate ( range ( 0 , 90 , 10 ) ) : \n 
~~~ left = images [ i ] \n 
dists_left = dists [ i ] \n 
right = dists_left . argsort ( ) \n 
right = right [ 1 ] \n 
right = images [ right ] \n 
left = mh . imread ( left ) \n 
right = mh . imread ( right ) \n 
axes [ 0 , ci ] . imshow ( left ) \n 
axes [ 1 , ci ] . imshow ( right ) \n 
~~ fig . tight_layout ( ) \n 
fig . savefig ( , dpi = 300 ) \n 
features = data [ ] \n 
target = data [ ] \n 
target_names = data [ ] \n 
plength = features [ : , 2 ] \n 
max_setosa = plength [ is_setosa ] . max ( ) \n 
min_non_setosa = plength [ ~ is_setosa ] . min ( ) \n 
print ( . format ( max_setosa ) ) \n 
print ( . format ( min_non_setosa ) ) \n 
CONSUMER_KEY = None \n 
CONSUMER_SECRET = None \n 
ACCESS_TOKEN_KEY = None \n 
ACCESS_TOKEN_SECRET = None \n 
import scipy \n 
import scipy . io . wavfile \n 
from utils import GENRE_DIR , CHART_DIR \n 
from matplotlib . ticker import EngFormatter \n 
def write_fft ( fft_features , fn ) : \n 
base_fn , ext = os . path . splitext ( fn ) \n 
data_fn = base_fn + ".fft" \n 
np . save ( data_fn , fft_features ) \n 
~~ def create_fft ( fn ) : \n 
~~~ sample_rate , X = scipy . io . wavfile . read ( fn ) \n 
fft_features = abs ( scipy . fft ( X ) [ : 1000 ] ) \n 
write_fft ( fft_features , fn ) \n 
~~ def read_fft ( genre_list , base_dir = GENRE_DIR ) : \n 
~~~ X = [ ] \n 
y = [ ] \n 
for label , genre in enumerate ( genre_list ) : \n 
~~~ genre_dir = os . path . join ( base_dir , genre , "*.fft.npy" ) \n 
file_list = glob . glob ( genre_dir ) \n 
assert ( file_list ) , genre_dir \n 
for fn in file_list : \n 
~~~ fft_features = np . load ( fn ) \n 
X . append ( fft_features [ : 2000 ] ) \n 
y . append ( label ) \n 
~~ ~~ return np . array ( X ) , np . array ( y ) \n 
~~ def plot_wav_fft ( wav_filename , desc = None ) : \n 
~~~ plt . clf ( ) \n 
plt . figure ( num = None , figsize = ( 6 , 4 ) ) \n 
sample_rate , X = scipy . io . wavfile . read ( wav_filename ) \n 
spectrum = np . fft . fft ( X ) \n 
freq = np . fft . fftfreq ( len ( X ) , 1.0 / sample_rate ) \n 
plt . subplot ( 211 ) \n 
num_samples = 200.0 \n 
plt . xlim ( 0 , num_samples / sample_rate ) \n 
plt . title ( desc or wav_filename ) \n 
plt . plot ( np . arange ( num_samples ) / sample_rate , X [ : num_samples ] ) \n 
plt . grid ( True ) \n 
plt . subplot ( 212 ) \n 
plt . xlim ( 0 , 5000 ) \n 
plt . xticks ( np . arange ( 5 ) * 1000 ) \n 
if desc : \n 
~~~ desc = desc . strip ( ) \n 
fft_desc = desc [ 0 ] . lower ( ) + desc [ 1 : ] \n 
~~~ fft_desc = wav_filename \n 
plt . plot ( freq , abs ( spectrum ) , linewidth = 5 ) \n 
rel_filename = os . path . split ( wav_filename ) [ 1 ] \n 
plt . savefig ( "%s_wav_fft.png" % os . path . splitext ( rel_filename ) [ 0 ] , \n 
bbox_inches = ) \n 
~~ def plot_wav_fft_demo ( ) : \n 
~~ def plot_specgram ( ax , fn ) : \n 
ax . specgram ( X , Fs = sample_rate , xextent = ( 0 , 30 ) ) \n 
~~ def plot_specgrams ( base_dir = CHART_DIR ) : \n 
plt . clf ( ) \n 
genres = [ "classical" , "jazz" , "country" , "pop" , "rock" , "metal" ] \n 
num_files = 3 \n 
f , axes = plt . subplots ( len ( genres ) , num_files ) \n 
for genre_idx , genre in enumerate ( genres ) : \n 
~~~ for idx , fn in enumerate ( glob . glob ( os . path . join ( GENRE_DIR , genre , "*.wav" ) ) ) : \n 
~~~ if idx == num_files : \n 
~~ axis = axes [ genre_idx , idx ] \n 
axis . yaxis . set_major_formatter ( EngFormatter ( ) ) \n 
plot_specgram ( axis , fn ) \n 
~~ ~~ specgram_file = os . path . join ( base_dir , "Spectrogram_Genres.png" ) \n 
plt . savefig ( specgram_file , bbox_inches = "tight" ) \n 
~~~ plot_wav_fft_demo ( ) \n 
~~ plot_specgrams ( ) \n 
~~ import timeit \n 
normal_py_sec = timeit . timeit ( , \n 
number = 10000 ) \n 
naive_np_sec = timeit . timeit ( , \n 
good_np_sec = timeit . timeit ( , \n 
import gensim \n 
logging . basicConfig ( \n 
format = , \n 
level = logging . INFO ) \n 
id2word = gensim . corpora . Dictionary . load_from_text ( \n 
mm = gensim . corpora . MmCorpus ( ) \n 
model = gensim . models . hdpmodel . HdpModel ( \n 
corpus = mm , \n 
id2word = id2word , \n 
chunksize = 10000 ) \n 
model . save ( ) \n 
topics = np . zeros ( ( len ( mm ) , model . num_topics ) ) \n 
for di , doc in enumerate ( mm ) : \n 
~~~ doc_top = model [ doc ] \n 
for ti , tv in doc_top : \n 
~~~ topics [ di , ti ] += tv \n 
~~ ~~ np . save ( , topics ) \n 
def apriori ( dataset , minsupport , maxsize ) : \n 
baskets = defaultdict ( list ) \n 
pointers = defaultdict ( list ) \n 
for i , ds in enumerate ( dataset ) : \n 
~~~ for ell in ds : \n 
~~~ pointers [ ell ] . append ( i ) \n 
baskets [ frozenset ( [ ell ] ) ] . append ( i ) \n 
~~ ~~ new_pointers = dict ( ) \n 
for k in pointers : \n 
~~~ if len ( pointers [ k ] ) >= minsupport : \n 
~~~ new_pointers [ k ] = frozenset ( pointers [ k ] ) \n 
~~ ~~ pointers = new_pointers \n 
for k in baskets : \n 
~~~ baskets [ k ] = frozenset ( baskets [ k ] ) \n 
~~ valid = set ( ) \n 
for el , c in baskets . items ( ) : \n 
~~~ if len ( c ) >= minsupport : \n 
~~~ valid . update ( el ) \n 
~~ ~~ itemsets = [ frozenset ( [ v ] ) for v in valid ] \n 
freqsets = [ ] \n 
for i in range ( maxsize - 1 ) : \n 
i , len ( itemsets ) ) ) \n 
newsets = [ ] \n 
for it in itemsets : \n 
~~~ ccounts = baskets [ it ] \n 
for v , pv in pointers . items ( ) : \n 
~~~ if v not in it : \n 
~~~ csup = ( ccounts & pv ) \n 
if len ( csup ) >= minsupport : \n 
~~~ new = frozenset ( it | frozenset ( [ v ] ) ) \n 
if new not in baskets : \n 
~~~ newsets . append ( new ) \n 
baskets [ new ] = csup \n 
~~ ~~ ~~ ~~ ~~ freqsets . extend ( itemsets ) \n 
itemsets = newsets \n 
if not len ( itemsets ) : \n 
~~ ~~ support = { } \n 
~~~ support [ k ] = float ( len ( baskets [ k ] ) ) \n 
~~ return freqsets , support \n 
~~ AssociationRule = namedtuple ( , [ , , , , \n 
def association_rules ( dataset , freqsets , support , minlift ) : \n 
nr_transactions = float ( len ( dataset ) ) \n 
freqsets = [ f for f in freqsets if len ( f ) > 1 ] \n 
for fset in freqsets : \n 
~~~ for f in fset : \n 
~~~ consequent = frozenset ( [ f ] ) \n 
antecendent = fset - consequent \n 
py_x = support [ fset ] / support [ antecendent ] \n 
base = support [ consequent ] / nr_transactions \n 
lift = py_x / base \n 
if lift > minlift : \n 
~~~ yield AssociationRule ( antecendent , consequent , base , py_x , lift ) \n 
~~ ~~ ~~ ~~ from sklearn . feature_selection import RFE \n 
from sklearn . datasets import make_classification \n 
X , y = make_classification ( \n 
n_samples = 100 , n_features = 10 , n_informative = 3 , random_state = 0 ) \n 
clf . fit ( X , y ) \n 
for i in range ( 1 , 11 ) : \n 
~~~ selector = RFE ( clf , i ) \n 
selector = selector . fit ( X , y ) \n 
print ( "%i\\t%s\\t%s" % ( i , selector . support_ , selector . ranking_ ) ) \n 
~~ 