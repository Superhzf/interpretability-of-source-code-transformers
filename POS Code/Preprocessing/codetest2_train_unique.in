\n 
# \n 
from horizon import tabs \n 
class NetworkProfileTab ( tabs . Tab ) : \n 
slug = "network_profile" \n 
template_name = \n 
def get_context_data ( self , request ) : \n 
~~~ return None \n 
~~ ~~ class PolicyProfileTab ( tabs . Tab ) : \n 
slug = "policy_profile" \n 
preload = False \n 
~~ class IndexTabs ( tabs . TabGroup ) : \n 
~~~ slug = "indextabs" \n 
tabs = ( NetworkProfileTab , PolicyProfileTab ) \n 
import weakref \n 
from eventlet import corolocal \n 
class WeakLocal ( corolocal . local ) : \n 
~~~ def __getattribute__ ( self , attr ) : \n 
~~~ rval = corolocal . local . __getattribute__ ( self , attr ) \n 
if rval : \n 
~~~ rval = rval ( ) \n 
~~ return rval \n 
~~ def __setattr__ ( self , attr , value ) : \n 
~~~ value = weakref . ref ( value ) \n 
return corolocal . local . __setattr__ ( self , attr , value ) \n 
~~ ~~ store = WeakLocal ( ) \n 
weak_store = WeakLocal ( ) \n 
strong_store = corolocal . local \n 
import eventlet \n 
eventlet . monkey_patch ( ) \n 
import contextlib \n 
import sys \n 
from oslo . config import cfg \n 
from openstack_dashboard . openstack . common import log as logging \n 
from openstack_dashboard . openstack . common import rpc \n 
from openstack_dashboard . openstack . common . rpc import impl_zmq \n 
CONF = cfg . CONF \n 
CONF . register_opts ( rpc . rpc_opts ) \n 
CONF . register_opts ( impl_zmq . zmq_opts ) \n 
def main ( ) : \n 
~~~ CONF ( sys . argv [ 1 : ] , project = ) \n 
logging . setup ( "oslo" ) \n 
with contextlib . closing ( impl_zmq . ZmqProxy ( CONF ) ) as reactor : \n 
~~~ reactor . consume_in_thread ( ) \n 
reactor . wait ( ) \n 
~~ ~~ from enum import IntEnum \n 
from . component import Component \n 
from . object import field \n 
class ReflectionProbeUsage ( IntEnum ) : \n 
~~~ Off = 0 \n 
BlendProbes = 1 \n 
BlendProbesAndSkybox = 2 \n 
Simple = 3 \n 
~~ class ShadowCastingMode ( IntEnum ) : \n 
On = 1 \n 
TwoSided = 2 \n 
ShadowsOnly = 3 \n 
~~ class Renderer ( Component ) : \n 
~~~ enabled = field ( "m_Enabled" , bool ) \n 
lightmap_index = field ( "m_LightmapIndex" ) \n 
materials = field ( "m_Materials" ) \n 
probe_anchor = field ( "m_ProbeAnchor" ) \n 
receive_shadows = field ( "m_ReceiveShadows" , bool ) \n 
reflection_probe_usage = field ( "m_ReflectionProbeUsage" , ReflectionProbeUsage ) \n 
shadow_casting_mode = field ( "m_CastShadows" , ShadowCastingMode ) \n 
sorting_layer_id = field ( "m_SortingLayerID" ) \n 
sorting_order = field ( "m_SortingOrder" ) \n 
use_light_probes = field ( "m_UseLightProbes" , bool ) \n 
lightmap_index_dynamic = field ( "m_LightmapIndexDynamic" ) \n 
lightmap_tiling_offset = field ( "m_LightmapTilingOffset" ) \n 
lightmap_tiling_offset_dynamic = field ( "m_LightmapTilingOffsetDynamic" ) \n 
static_batch_root = field ( "m_StaticBatchRoot" ) \n 
subset_indices = field ( "m_SubsetIndices" ) \n 
@ property \n 
def material ( self ) : \n 
~~~ return self . materials [ 0 ] \n 
~~ ~~ class ParticleSystemRenderMode ( IntEnum ) : \n 
~~~ Billboard = 0 \n 
Stretch = 1 \n 
HorizontalBillboard = 2 \n 
VerticalBillboard = 3 \n 
Mesh = 4 \n 
~~ class ParticleSystemSortMode ( IntEnum ) : \n 
~~~ None_ = 0 \n 
Distance = 1 \n 
OldestInFront = 2 \n 
YoungestInFront = 3 \n 
~~ class MeshRenderer ( Component ) : \n 
~~~ pass \n 
~~ class ParticleRenderer ( Renderer ) : \n 
~~~ camera_velocity_scale = field ( "m_CameraVelocityScale" ) \n 
length_scale = field ( "m_LengthScale" ) \n 
max_particle_size = field ( "m_MaxParticleSize" ) \n 
velocity_scale = field ( "m_VelocityScale" ) \n 
stretch_particles = field ( "m_StretchParticles" ) \n 
~~ class ParticleSystemRenderer ( Renderer ) : \n 
mesh = field ( "m_Mesh" ) \n 
mesh1 = field ( "m_Mesh1" ) \n 
mesh2 = field ( "m_Mesh2" ) \n 
mesh3 = field ( "m_Mesh3" ) \n 
normal_direction = field ( "m_NormalDirection" ) \n 
render_mode = field ( "m_RenderMode" , ParticleSystemRenderMode ) \n 
sort_mode = field ( "m_SortMode" , ParticleSystemSortMode ) \n 
sorting_fudge = field ( "m_SortingFudge" ) \n 
~~ from ConfigParser import * \n 
from StringIO import * \n 
from Log import Log \n 
import datetime \n 
class Config : \n 
~~~ @ staticmethod \n 
def LoadConfig ( ) : \n 
~~~ Config . parser = ConfigParser ( ) \n 
try : \n 
~~~ sconff = open ( CONFIG_FILE , "r" ) \n 
~~ except : \n 
return \n 
~~ sconf = StringIO ( ) \n 
sconf . write ( "[sysconf]\\n" ) \n 
sconf . write ( sconff . read ( ) ) \n 
sconf . seek ( 0 ) \n 
Config . parser . readfp ( sconf ) \n 
sconff . close ( ) \n 
sconf . close ( ) \n 
~~ @ staticmethod \n 
def GetBoardsFile ( ) : \n 
~~~ return BOARDS_FILE \n 
def GetInt ( name , defval ) : \n 
~~~ if ( Config . parser . has_option ( , name ) ) : \n 
~~~ return Config . parser . getint ( , name ) \n 
~~ else : \n 
~~~ return defval \n 
~~ ~~ @ staticmethod \n 
def GetString ( name , defval ) : \n 
~~~ val = Config . parser . get ( , name ) \n 
if ( val [ 0 ] == \'"\' and val . endswith ( \'"\' ) ) : \n 
~~~ val = val [ 1 : - 1 ] \n 
~~ return val . decode ( ) \n 
~~ ~~ ~~ BBS_ROOT = \n 
BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n 
BBS_XMPP_KEY_FILE = BBS_ROOT + "xmpp.key" \n 
BOARDS_FILE = BBS_ROOT + \n 
STRLEN = 80 \n 
ARTICLE_TITLE_LEN = 60 \n 
BM_LEN = 60 \n 
MAXBOARD = 400 \n 
CONFIG_FILE = BBS_ROOT + \n 
FILENAME_LEN = 20 \n 
OWNER_LEN = 30 \n 
SESSIONID_LEN = 32 \n 
REFRESH_TOKEN_LEN = 128 \n 
NAMELEN = 40 \n 
IDLEN = 12 \n 
MD5PASSLEN = 16 \n 
OLDPASSLEN = 14 \n 
MOBILE_NUMBER_LEN = 17 \n 
MAXCLUB = 128 \n 
MAXUSERS = 20000 \n 
MAX_MSG_SIZE = 1024 \n 
MAXFRIENDS = 400 \n 
MAXMESSAGE = 5 \n 
MAXSIGLINES = 6 \n 
IPLEN = 16 \n 
DEFAULTBOARD = "sysop" \n 
BLESS_BOARD = "happy_birthday" \n 
QUOTED_LINES = 10 \n 
MAXACTIVE = 8000 \n 
USHM_SIZE = MAXACTIVE + 10 \n 
UTMP_HASHSIZE = USHM_SIZE * 4 \n 
UCACHE_SEMLOCK = 0 \n 
LEN_FRIEND_EXP = 15 \n 
SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n 
SESSION_TIMEOUT_SECONDS = 86400 * 30 \n 
XMPP_IDLE_TIME = 300 \n 
XMPP_LONG_IDLE_TIME = 1800 \n 
XMPP_UPDATE_TIME_INTERVAL = 10 \n 
XMPP_PING_TIME_INTERVAL = 60 \n 
PUBLIC_SHMKEY = 3700 \n 
MAX_ATTACHSIZE = 20 * 1024 * 1024 \n 
BMDEL_DECREASE = True \n 
SYSMAIL_BOARD = "sysmail" \n 
ADD_EDITMARK = True \n 
SEARCH_COUNT_LIMIT = 20 \n 
MAIL_SIZE_LIMIT = - 1 \n 
SEC_DELETED_OLDHOME = 3600 * 24 * 3 \n 
SELF_INTRO_MAX_LEN = 800 \n 
import re \n 
import os \n 
import stat \n 
import json \n 
import struct \n 
import time \n 
import Config \n 
import Board \n 
import Post \n 
import BoardManager \n 
from Util import Util \n 
from errors import * \n 
DEFAULT_DIGEST_LIST_COUNT = 20 \n 
class DigestItem : \n 
~~~ def __init__ ( self , basepath ) : \n 
~~~ self . basepath = basepath \n 
self . title = \n 
self . host = \n 
self . port = 0 \n 
self . attachpos = 0 \n 
self . fname = \n 
self . mtitle = \n 
self . items = [ ] \n 
self . update_time = 0 \n 
self . id = 0 \n 
self . sysop_only = 0 \n 
self . bms_only = 0 \n 
self . zixia_only = 0 \n 
~~ def IsDir ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
return stat . S_ISDIR ( st . st_mode ) \n 
~~~ return False \n 
~~ ~~ def IsFile ( self ) : \n 
return stat . S_ISREG ( st . st_mode ) \n 
~~ ~~ def GetModTime ( self ) : \n 
mtime = st . st_mtime \n 
~~~ mtime = time . time ( ) \n 
~~ return mtime \n 
~~ def names_path ( self ) : \n 
~~~ return "%s/.Names" % self . realpath ( ) \n 
~~ def realpath ( self ) : \n 
~~~ return "%s/%s" % ( Config . BBS_ROOT , self . path ( ) ) \n 
~~ def path ( self ) : \n 
~~~ if ( self . fname ) : \n 
~~~ return "%s/%s" % ( self . basepath , self . fname ) \n 
~~~ return self . basepath \n 
~~ ~~ def CheckUpdate ( self ) : \n 
~~~ stat = os . stat ( self . names_path ( ) ) \n 
if ( stat . st_mtime > self . update_time ) : \n 
~~~ self . LoadNames ( ) \n 
~~ ~~ except : \n 
~~ return True \n 
~~ def LoadNames ( self ) : \n 
~~~ f = open ( self . names_path ( ) , "r" ) \n 
~~ except IOError : \n 
~~~ return 0 \n 
~~ stat = os . fstat ( f . fileno ( ) ) \n 
self . update_time = stat . st_mtime \n 
item = DigestItem ( self . path ( ) ) \n 
hostname = \n 
_id = 0 \n 
bms_only = 0 \n 
sysop_only = 0 \n 
zixia_only = 0 \n 
while ( True ) : \n 
~~~ line = f . readline ( ) \n 
if ( line == "" ) : break \n 
npos = line . find ( "\\n" ) \n 
if ( npos != - 1 ) : line = line [ : npos ] \n 
if ( line [ : 1 ] == ) : \n 
~~~ if ( not self . mtitle ) : \n 
~~~ self . mtitle = line [ 8 : ] \n 
~~ ~~ ~~ result = re . match ( , line ) \n 
if ( result ) : \n 
~~~ key = result . group ( 1 ) \n 
value = result . group ( 2 ) \n 
if ( key == "Name" ) : \n 
~~~ item . title = value \n 
item . attachpos = 0 \n 
~~ elif ( key == "Path" ) : \n 
~~~ if ( value [ : 2 ] == "~/" ) : \n 
~~~ item . fname = value [ 2 : ] \n 
~~~ item . fname = value \n 
~~ if ( item . fname . find ( ".." ) != - 1 ) : \n 
~~~ continue \n 
~~~ bms_only += 1 \n 
~~~ sysop_only += 1 \n 
~~~ zixia_only += 1 \n 
~~ if ( item . fname . find ( "!@#$%" ) != - 1 ) : \n 
~~~ parts = re . split ( , item . fname ) \n 
newparts = [ ] \n 
for part in parts : \n 
~~~ if ( part ) : \n 
~~~ newparts += [ part ] \n 
~~ ~~ hostname = newparts [ 0 ] \n 
item . fname = newparts [ 1 ] \n 
~~~ item . port = int ( newparts [ 2 ] ) \n 
~~~ item . port = 0 \n 
~~ ~~ item . id = _id \n 
_id += 1 \n 
item . bms_only = bms_only \n 
item . sysop_only = sysop_only \n 
item . zixia_only = zixia_only \n 
item . host = hostname \n 
self . items += [ item ] \n 
~~ elif ( key == "Host" ) : \n 
~~~ hostname = value \n 
~~ elif ( key == "Port" ) : \n 
~~~ item . port = int ( value ) \n 
~~ ~~ elif ( key == "Attach" ) : \n 
~~~ item . attachpos = int ( value ) \n 
~~~ item . attachpos = 0 \n 
~~ ~~ ~~ ~~ f . close ( ) \n 
return 1 \n 
~~ def GetItem ( self , user , route , has_perm = False , need_perm = False ) : \n 
~~~ self . CheckUpdate ( ) \n 
if ( self . mtitle . find ( "(BM:" ) != - 1 ) : \n 
~~~ if ( Board . Board . IsBM ( user , self . mtitle [ 4 : ] , ) or user . IsSysop ( ) ) : \n 
~~~ has_perm = True \n 
~~ elif ( need_perm and not has_perm ) : \n 
~~ if ( len ( route ) == 0 ) : \n 
~~~ return self \n 
~~ target = route [ 0 ] - 1 \n 
_id = target \n 
if ( _id >= len ( self . items ) ) : \n 
~~ while ( self . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ _id += 1 \n 
~~ ~~ item = self . items [ _id ] \n 
item . mtitle = item . title \n 
if ( len ( route ) == 1 ) : \n 
~~~ return item \n 
~~~ if ( item . IsDir ( ) ) : \n 
~~~ if ( not item . CheckUpdate ( ) ) : \n 
~~ return item . GetItem ( user , route [ 1 : ] , has_perm , need_perm ) \n 
~~ ~~ ~~ def GetRange ( self , user , route , start , end , has_perm = False , need_perm = False ) : \n 
firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n 
if ( not firstitem ) : \n 
~~~ return [ ] \n 
~~ parent = self . GetItem ( user , route , has_perm , need_perm ) \n 
if ( not parent ) : \n 
~~ if ( not parent . IsDir ( ) ) : \n 
~~ result = [ ] \n 
_id = start - 1 \n 
for i in range ( start , end + 1 ) : \n 
~~~ target = i - 1 \n 
if ( _id >= len ( parent . items ) ) : \n 
~~ while ( parent . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ return result \n 
~~ ~~ item = parent . items [ _id ] \n 
result += [ item ] \n 
~~ return result \n 
~~ def EffectiveId ( self , user ) : \n 
~~~ _id = self . id \n 
if ( user . IsSysop ( ) ) : \n 
~~~ return _id \n 
~~ if ( not user . IsSysop ( ) ) : \n 
~~~ _id -= self . sysop_only \n 
~~ if ( not user . IsBM ( ) ) : \n 
~~~ _id -= self . bms_only \n 
~~ if ( not user . IsSECANC ( ) ) : \n 
~~~ _id -= self . zixia_only \n 
~~ return _id \n 
~~ def GetInfo ( self ) : \n 
~~~ info = { } \n 
info [ ] = Util . gbkDec ( self . mtitle ) \n 
info [ ] = Util . gbkDec ( self . title ) \n 
info [ ] = self . attachpos \n 
if ( self . host != ) : \n 
~~~ info [ ] = self . host \n 
info [ ] = self . port \n 
info [ ] = \n 
~~ elif ( self . IsDir ( ) ) : \n 
~~~ info [ ] = \n 
~~ elif ( self . IsFile ( ) ) : \n 
~~ info [ ] = int ( self . GetModTime ( ) ) \n 
return info \n 
~~ def GetInfoForUser ( self , user ) : \n 
~~~ info = self . GetInfo ( ) \n 
info [ ] = self . EffectiveId ( user ) + 1 \n 
~~ def GetAttachLink ( self , session ) : \n 
filename = \n 
for i in range ( 2 ) : \n 
~~~ filename += "%0x" % struct . unpack ( , _hash [ i * 4 : ( i + 1 ) * 4 ] ) \n 
~~ link = "http://%s/bbscon.php?b=xattach&f=%s" % ( session . GetMirror ( Config . Config . GetInt ( , 80 ) ) , filename ) \n 
linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n 
target = "../../%s" % self . path ( ) \n 
~~~ os . symlink ( target , linkfile ) \n 
~~ return link \n 
~~ ~~ class Digest : \n 
~~~ root = DigestItem ( "0Announce" ) \n 
def __init__ ( self , board , path ) : \n 
~~~ self . board = board \n 
self . path = path \n 
self . root = DigestItem ( self . path ) \n 
def GET ( svc , session , params , action ) : \n 
~~~ if ( session is None ) : raise Unauthorized ( ) \n 
user = session . GetUser ( ) \n 
boardname = svc . get_str ( params , , ) \n 
if ( boardname ) : \n 
~~~ board = BoardManager . BoardManager . GetBoard ( boardname ) \n 
if ( board is None ) : raise NotFound ( % boardname ) \n 
if ( not board . CheckReadPerm ( user ) ) : \n 
~~~ raise NoPerm ( ) \n 
~~ basenode = board . digest . root \n 
has_perm = user . IsDigestMgr ( ) or user . IsSysop ( ) or user . IsSuperBM ( ) \n 
~~~ basenode = Digest . root \n 
has_perm = user . IsDigestMgr ( ) \n 
~~ if ( action == "list" ) : \n 
~~~ route = svc . get_str ( params , ) \n 
start = svc . get_int ( params , , 1 ) \n 
end = svc . get_int ( params , , start + DEFAULT_DIGEST_LIST_COUNT - 1 ) \n 
Digest . List ( svc , basenode , route , start , end , session , has_perm ) \n 
~~ elif ( action == "view" ) : \n 
start = svc . get_int ( params , , 0 ) \n 
count = svc . get_int ( params , , 0 ) \n 
Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n 
~~~ raise WrongArgs ( % action ) \n 
def ParseRoute ( route ) : \n 
~~~ ret = [ ] \n 
items = re . split ( , route ) \n 
items = items [ 1 : ] \n 
for item in items : \n 
~~~ ret += [ int ( item ) ] \n 
~~~ raise WrongArgs ( % item ) \n 
~~ ~~ return ret \n 
def List ( svc , basenode , route , start , end , session , has_perm ) : \n 
~~~ route_array = Digest . ParseRoute ( route ) \n 
parent = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
~~~ raise WrongArgs ( % route ) \n 
~~ items = basenode . GetRange ( session . GetUser ( ) , route_array , start , end , has_perm ) \n 
result = { } \n 
result [ ] = parent . GetInfoForUser ( session . GetUser ( ) ) \n 
result [ ] = len ( items ) \n 
result_list = [ ] \n 
~~~ result_list += [ item . GetInfoForUser ( session . GetUser ( ) ) ] \n 
~~ result [ ] = result_list \n 
svc . writedata ( json . dumps ( result ) ) \n 
def View ( svc , basenode , route , session , has_perm , start , count ) : \n 
item = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
if ( not item ) : \n 
~~ if ( not item . IsFile ( ) ) : \n 
~~ result = { } \n 
result [ ] = item . GetInfoForUser ( session . GetUser ( ) ) \n 
postinfo = Post . Post ( item . realpath ( ) , None ) \n 
( result [ ] , result [ ] ) = postinfo . GetContent ( start , count ) \n 
attachlist = postinfo . GetAttachListByType ( ) \n 
result [ ] = attachlist [ 0 ] \n 
result [ ] = attachlist [ 1 ] \n 
if ( attachlist [ 0 ] or attachlist [ 1 ] ) : \n 
~~~ result [ ] = item . GetAttachLink ( session ) \n 
~~ svc . writedata ( json . dumps ( result ) ) \n 
~~ ~~ import time \n 
import UserManager \n 
import UserInfo \n 
from Session import Session \n 
import UCache \n 
import MsgBox \n 
import xmpp \n 
import modes \n 
import Util \n 
import traceback \n 
from xmpp . features import NoRoute \n 
__disco_info_ns__ = \n 
__disco_items_ns__ = \n 
__vcard_ns__ = \n 
STEAL_AFTER_SEEN = 3 \n 
def elem_to_str ( elem ) : \n 
~~ class XMPPServer ( xmpp . Plugin ) : \n 
def __init__ ( self , rosters , host ) : \n 
~~~ self . probed = False \n 
self . _closed = False \n 
self . rosters = rosters \n 
self . _session = None \n 
self . rosters . set_resources ( self . get_resources ( ) ) \n 
self . _fixedjid = UCache . UCache . formalize_jid ( unicode ( self . authJID ) ) \n 
self . _userid = self . _fixedjid . partition ( ) [ 0 ] . encode ( "gbk" ) \n 
if ( not self . rosters . allow_login ( self . authJID . bare ) ) : \n 
self . stream_error ( , ) \n 
if self . authJID . resource [ : - 8 ] != "Resource" and len ( self . authJID . resource ) > 8 : \n 
~~~ routes = self . routes ( self . authJID . bare ) \n 
for route in routes : \n 
~~~ jid = route [ 0 ] \n 
if jid . resource [ : - 8 ] == self . authJID . resource [ : - 8 ] : \n 
~~~ if jid . resource != self . authJID . resource : \n 
route [ 1 ] . stream_error ( , ) \n 
~~ ~~ else : \n 
~~ ~~ ~~ except NoRoute : \n 
~~ self . _user = UserManager . UserManager . LoadUser ( self . _userid ) \n 
if ( self . _user == None ) : \n 
~~ self . _peer_addr = self . getpeername ( ) \n 
self . _session = Session ( self . _user , self . _peer_addr [ 0 ] ) \n 
self . _session . RecordLogin ( ) \n 
self . _userinfo = self . _session . Register ( ) \n 
self . _loginid = self . _session . utmpent \n 
self . _hostname = host \n 
self . bind ( xmpp . ReceivedCloseStream , self . recv_close ) \n 
self . bind ( xmpp . StreamClosed , self . stream_closed ) \n 
self . bind ( xmpp . SentCloseStream , self . sent_close ) \n 
self . rosters . register_conn ( self ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
if self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) is None : \n 
~~~ self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msgbox . GetMsgCount ( all = False ) - msgbox . GetUnreadCount ( ) ) \n 
~~ self . check_msg ( ) \n 
~~ def get_loginid ( self ) : \n 
~~~ return self . _loginid \n 
~~ def recv_close ( self ) : \n 
return self . close ( ) \n 
~~ def stream_closed ( self ) : \n 
~~ def sent_close ( self ) : \n 
~~ def close ( self ) : \n 
~~~ if ( self . _closed ) : \n 
~~ self . _closed = True \n 
if ( self . _session ) : \n 
~~~ self . _session . Unregister ( ) \n 
~~ self . unbind_res ( ) \n 
self . rosters . unregister_conn ( self ) \n 
~~ @ xmpp . iq ( ) \n 
def ping ( self , iq ) : \n 
self . refresh ( ) \n 
return self . iq ( , iq ) \n 
~~ @ xmpp . stanza ( ) \n 
def message ( self , elem ) : \n 
to_jid = elem . get ( ) \n 
from_jid = elem . get ( ) \n 
if ( from_jid == None ) : \n 
~~~ return \n 
~~ text_body = None \n 
for child in elem : \n 
~~~ if ( child . tag . endswith ( ) ) : \n 
~~~ text_body = child . text \n 
~~ ~~ if ( text_body == None ) : \n 
~~ ret = self . rosters . send_msg ( from_jid , to_jid , text_body ) \n 
if ( ret <= 0 ) : \n 
errors = { \n 
if ( ret in errors ) : \n 
~~~ elem = self . E . message ( { : to_jid , \n 
: from_jid , \n 
: } , \n 
self . E . body ( errors [ ret ] ) ) \n 
self . recv ( from_jid , elem ) \n 
~~ ~~ ~~ def make_jid ( self , userid ) : \n 
~~~ return "%s@%s" % ( userid , self . _hostname ) \n 
~~ def refresh ( self ) : \n 
~~~ self . _userinfo . freshtime = int ( time . time ( ) ) \n 
self . _userinfo . save ( ) \n 
~~ def ping_result ( self , iq ) : \n 
~~~ self . refresh ( ) \n 
~~ def ping_client ( self ) : \n 
~~~ pingelem = self . E . ping ( xmlns = ) \n 
return self . iq ( , self . ping_result , pingelem ) \n 
~~ except Exception as e : \n 
Log . debug ( traceback . format_exc ( ) ) \n 
return False \n 
~~ ~~ def get_uid ( self ) : \n 
~~~ return self . _user . GetUID ( ) \n 
~~ def recv_msg ( self , from_ , msgtext ) : \n 
~~~ elem = self . E . message ( { : from_ , : unicode ( self . authJID ) } , \n 
self . E . body ( msgtext ) ) \n 
self . recv ( unicode ( self . authJID ) , elem ) \n 
~~ def check_msg ( self ) : \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
my_pid = os . getpid ( ) \n 
xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n 
if xmpp_read > msg_count : \n 
~~~ xmpp_read = 0 \n 
self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msg_count ) \n 
if xmpp_read < msg_count : \n 
~~~ return xmpp_read \n 
~~~ return - 1 \n 
~~ ~~ def deliver_msg ( self , start ) : \n 
for i in range ( start , msg_count ) : \n 
~~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if msghead . topid == my_pid : \n 
~~~ msgtext = msgbox . LoadMsgText ( msghead ) \n 
self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n 
~~ ~~ ~~ def steal_msg ( self ) : \n 
msg_unread = msgbox . GetUnreadCount ( ) \n 
read_count = msg_count - msg_unread \n 
term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n 
term_stealed = self . rosters . get_term_stealed ( self . get_uid ( ) ) \n 
all_xmpp = True \n 
new_unread = { } \n 
for i in range ( read_count - 1 , msg_count ) : \n 
~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if i >= read_count and all_xmpp : \n 
~~~ if msghead . topid == my_pid : \n 
~~~ msgbox . GetUnreadMsg ( ) \n 
~~~ all_xmpp = False \n 
~~ ~~ if msghead . topid == my_pid : \n 
~~~ session = self . rosters . find_session ( self . authJID . bare , msghead . topid ) \n 
if session is None or session . get_mode ( ) != modes . MSG : \n 
~~ if msghead . topid not in new_unread : \n 
new_unread [ msghead . topid ] = i \n 
~~ ~~ final_unread = { } \n 
to_steal = { } \n 
to_steal_begin = msg_count \n 
for pid in term_read : \n 
~~~ if pid in new_unread : \n 
~~~ if new_unread [ pid ] == term_read [ pid ] [ 0 ] : \n 
~~~ final_unread [ pid ] = ( term_read [ pid ] [ 0 ] , term_read [ pid ] [ 1 ] + 1 ) \n 
if final_unread [ pid ] [ 1 ] > STEAL_AFTER_SEEN : \n 
~~~ to_steal [ pid ] = final_unread [ pid ] \n 
if pid in term_stealed : \n 
~~~ steal_begin = max ( final_unread [ pid ] [ 0 ] , term_stealed [ pid ] + 1 ) \n 
~~~ steal_begin = final_unread [ pid ] [ 0 ] \n 
~~ if steal_begin < to_steal_begin : \n 
~~~ to_steal_begin = steal_begin \n 
~~ ~~ ~~ else : \n 
~~~ final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
pass \n 
~~ ~~ for pid in new_unread : \n 
~~~ if pid not in term_read : \n 
final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
~~ ~~ if to_steal : \n 
for i in range ( to_steal_begin , msg_count ) : \n 
msgbox . GetUnreadMsg ( ) \n 
~~ elif msghead . topid in to_steal : \n 
~~~ if msghead . topid not in term_stealed or i > term_stealed [ msghead . topid ] : \n 
msgtext = msgbox . LoadMsgText ( msghead ) \n 
term_stealed [ msghead . topid ] = i \n 
~~ ~~ ~~ ~~ self . rosters . set_term_read ( self . get_uid ( ) , final_unread ) \n 
def presence ( self , elem ) : \n 
if self . authJID == elem . get ( ) : \n 
~~~ if ( elem . get ( ) == None or ( not self . authJID . match_bare ( elem . get ( ) ) ) ) : \n 
~~~ return self . send_presence ( elem ) \n 
~~ ~~ self . recv_presence ( elem ) \n 
~~ def send_presence ( self , elem ) : \n 
direct = elem . get ( ) \n 
if not direct : \n 
~~~ self . rosters . broadcast ( self , elem ) \n 
if elem . get ( ) != : \n 
~~~ self . recv_presence ( elem ) \n 
~~ if not self . probed : \n 
~~~ self . probed = True \n 
self . rosters . probe ( self ) \n 
~~ ~~ elif not self . rosters . send ( self , direct , elem ) : \n 
~~~ self . send ( direct , elem ) \n 
~~ ~~ def recv_presence ( self , elem ) : \n 
if not self . rosters . recv ( self , elem ) : \n 
self . write ( elem ) \n 
~~ ~~ @ xmpp . iq ( ) \n 
def roster ( self , iq ) : \n 
roster = self . rosters . get ( self ) \n 
method = getattr ( self , % iq . get ( ) ) \n 
return method and method ( iq , roster ) \n 
~~ def get_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( { : } ) \n 
for item in roster . items ( ) : \n 
~~~ query . append ( item ) \n 
~~ return self . iq ( , iq , query ) \n 
~~ def set_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( xmlns = ) \n 
for item in iq [ 0 ] : \n 
~~~ result = roster . set ( item ) \n 
if result is not None : \n 
~~~ query . append ( result ) \n 
~~ ~~ if len ( query ) > 0 : \n 
~~~ self . push ( roster , query ) \n 
~~ return self . iq ( , iq ) \n 
~~ def push ( self , roster , query ) : \n 
for jid in roster . requests ( ) : \n 
~~~ for ( to , route ) in self . routes ( jid ) : \n 
~~~ route . iq ( , self . ignore , query ) \n 
~~ ~~ ~~ def ignore ( self , iq ) : \n 
def vcard ( self , iq ) : \n 
if iq . get ( ) == : \n 
~~~ if ( iq . get ( ) == None ) : \n 
~~~ target = iq . get ( ) \n 
~~ form_target = UCache . UCache . formalize_jid ( target ) \n 
name = form_target . partition ( ) [ 0 ] \n 
user = UserManager . UserManager . LoadUser ( name ) \n 
info = user . GetInfo ( ) \n 
desc = % ( info [ ] , info [ ] , info [ ] , \n 
info [ ] , info [ ] , info [ ] , info [ ] ) \n 
if ( in info ) : \n 
~~~ desc += "Plan:\\r%s" % ( info [ ] . replace ( , ) ) \n 
~~ vcard = self . E . vCard ( { : } , \n 
self . E ( , name ) , \n 
self . E ( , Util . Util . RemoveTags ( info [ ] ) ) , \n 
self . E ( , Util . Util . RemoveTags ( desc ) ) ) \n 
if ( iq . get ( ) == None ) : \n 
~~~ return self . iq ( , iq , vcard ) \n 
~~~ return self . iq ( , iq , vcard , { : iq . get ( ) } ) \n 
~~ ~~ ~~ @ xmpp . iq ( % __disco_info_ns__ ) \n 
def disco_info ( self , iq ) : \n 
target = iq . get ( ) \n 
if ( target . find ( ) < 0 ) : \n 
~~~ query = self . E . query ( { : __disco_info_ns__ } , \n 
self . E . identity ( { : , \n 
: , \n 
: Config . Config . GetString ( , ) , \n 
} ) ) \n 
features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n 
for feature in features : \n 
~~~ query . append ( self . E . feature ( { : feature } ) ) \n 
~~ ~~ return self . iq ( , iq , query , { : target } ) \n 
~~ @ xmpp . iq ( % __disco_items_ns__ ) \n 
def disco_items ( self , iq ) : \n 
~~~ query = self . E . query ( { : __disco_items_ns__ } ) \n 
~~ return self . iq ( , iq , query , { : target } ) \n 
### \n 
~~ ~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
from hpOneView . common import uri \n 
import hpOneView . profile as profile \n 
def acceptEULA ( con ) : \n 
~~~ con . get_eula_status ( ) \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
print ( e ) \n 
~~ ~~ def login ( con , credential ) : \n 
~~~ con . login ( credential ) \n 
~~ ~~ def get_eg_uri_from_arg ( srv , name ) : \n 
~~~ if srv and name : \n 
~~~ if name . startswith ( ) and uri [ ] in name : \n 
~~~ return name \n 
~~~ egs = srv . get_enclosure_groups ( ) \n 
for eg in egs : \n 
~~~ if eg [ ] == name : \n 
~~~ return eg [ ] \n 
~~ ~~ ~~ ~~ return None \n 
~~ def get_sht_from_arg ( srv , name ) : \n 
~~~ shts = srv . get_server_hardware_types ( ) \n 
for sht in shts : \n 
~~~ if sht [ ] == name : \n 
~~~ return sht \n 
~~ def define_profile_template ( \n 
srv , \n 
name , \n 
desc , \n 
sp_desc , \n 
server_hwt , \n 
enc_group , \n 
affinity , \n 
hide_flexnics , \n 
conn_list , \n 
fw_settings , \n 
boot , \n 
bootmode ) : \n 
~~~ if conn_list : \n 
~~~ conn = json . loads ( open ( conn_list ) . read ( ) ) \n 
~~~ conn = [ ] \n 
~~ profile_template = srv . create_server_profile_template ( \n 
name = name , \n 
description = desc , \n 
serverProfileDescription = sp_desc , \n 
serverHardwareTypeUri = server_hwt , \n 
enclosureGroupUri = enc_group , \n 
affinity = affinity , \n 
hideUnusedFlexNics = hide_flexnics , \n 
profileConnectionV4 = conn , \n 
firmwareSettingsV3 = fw_settings , \n 
bootSettings = boot , \n 
bootModeSetting = bootmode ) \n 
if in profile_template : \n 
~~~ print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( ) \n 
for connection in profile_template [ ] : \n 
~~~ print ( , connection [ ] ) \n 
print ( , connection [ ] ) \n 
~~ print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] , ) \n 
~~~ pprint ( profile_template ) \n 
~~ ~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
parser . add_argument ( , dest = , \n 
required = True , \n 
required = False , \n 
required = False , choices = [ , ] , \n 
action = , \n 
nargs = , \n 
choices = [ , , ] , \n 
choices = [ , , , \n 
, ] , \n 
args = parser . parse_args ( ) \n 
credential = { : args . user , : args . passwd } \n 
con = hpov . connection ( args . host ) \n 
srv = hpov . servers ( con ) \n 
sts = hpov . settings ( con ) \n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
eg_uri = get_eg_uri_from_arg ( srv , args . enc_group ) \n 
sht = get_sht_from_arg ( srv , args . server_hwt ) \n 
fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n 
boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n 
args . boot_order , args . boot_mode , args . pxe ) \n 
define_profile_template ( srv , \n 
args . name , \n 
args . desc , \n 
args . sp_desc , \n 
sht [ ] , \n 
eg_uri , \n 
args . affinity , \n 
args . hide_flexnics , \n 
args . conn_list , \n 
bootmode ) \n 
~~ if __name__ == : \n 
~~~ import argparse \n 
sys . exit ( main ( ) ) \n 
~~ from __future__ import print_function \n 
~~ ~~ def get_address_pools ( con , srv , types ) : \n 
~~~ if types == or types == : \n 
~~~ vmac = srv . get_vmac_pool ( ) \n 
for key in sorted ( vmac ) : \n 
~~~ print ( . format ( key , vmac [ key ] ) ) \n 
~~ if in vmac : \n 
~~~ for uri in vmac [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
~~ ~~ ~~ if types == or types == : \n 
~~~ vwwn = srv . get_vwwn_pool ( ) \n 
for key in sorted ( vwwn ) : \n 
~~~ print ( . format ( key , vwwn [ key ] ) ) \n 
~~ if in vwwn : \n 
~~~ for uri in vwwn [ ] : \n 
~~~ vsn = srv . get_vsn_pool ( ) \n 
for key in sorted ( vsn ) : \n 
~~~ print ( . format ( key , vsn [ key ] ) ) \n 
~~ if in vsn : \n 
~~~ for uri in vsn [ ] : \n 
~~ ~~ ~~ ~~ def main ( ) : \n 
choices = [ , , , ] , default = , \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
get_address_pools ( con , srv , args . types ) \n 
~~~ import sys \n 
import argparse \n 
~~ ~~ def get_managed_sans ( fcs ) : \n 
~~~ sans = fcs . get_managed_sans ( ) \n 
pprint ( sans ) \n 
~~ def main ( ) : \n 
fcs = hpov . fcsans ( con ) \n 
get_managed_sans ( fcs ) \n 
~~ ~~ def getpolicy ( sts ) : \n 
~~~ policy = sts . get_storage_vol_template_policy ( ) \n 
print ( policy [ ] ) \n 
getpolicy ( sts ) \n 
from __future__ import print_function \n 
__title__ = \n 
__version__ = \n 
__copyright__ = \n 
__license__ = \n 
__status__ = \n 
from hpOneView . common import * \n 
from hpOneView . connection import * \n 
from hpOneView . activity import * \n 
from hpOneView . exceptions import * \n 
class servers ( object ) : \n 
~~~ def __init__ ( self , con ) : \n 
~~~ self . _con = con \n 
self . _activity = activity ( con ) \n 
########################################################################### \n 
~~ def get_connections ( self , filter = ) : \n 
return get_members ( self . _con . get ( uri [ ] + filter ) ) \n 
~~ def get_connection ( self , server ) : \n 
body = self . _con . get ( server [ ] ) \n 
return body \n 
~~ def get_server_by_bay ( self , baynum ) : \n 
~~~ servers = get_members ( self . _con . get ( uri [ ] ) ) \n 
for server in servers : \n 
~~~ if server [ ] == baynum : \n 
~~~ return server \n 
~~ ~~ ~~ def get_server_by_name ( self , name ) : \n 
~~~ if server [ ] == name : \n 
~~ ~~ ~~ def get_available_servers ( self , server_hardware_type = None , \n 
enclosure_group = None , server_profile = None ) : \n 
~~~ filters = [ ] \n 
if server_hardware_type : \n 
~~~ filters . append ( + server_hardware_type [ ] ) \n 
~~ if enclosure_group : \n 
~~~ filters . append ( + enclosure_group [ ] ) \n 
~~ if server_profile : \n 
~~~ filters . append ( + server_profile [ ] ) \n 
~~ query_string = \n 
if filters : \n 
~~~ query_string = + . join ( filters ) \n 
~~ return self . _con . get ( uri [ ] + query_string ) \n 
~~ def get_servers ( self ) : \n 
~~~ return get_members ( self . _con . get ( uri [ ] ) ) \n 
~~ def get_utilization ( self , server ) : \n 
body = self . _con . get ( server [ ] + ) \n 
~~ def get_env_conf ( self , server ) : \n 
~~ def set_server_powerstate ( self , server , state , force = False , blocking = True , \n 
verbose = False ) : \n 
~~~ if state == and force is True : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == and force is False : \n 
~~ elif state == : \n 
~~ task , body = self . _con . put ( server [ ] + , powerRequest ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 60 , verbose = verbose ) \n 
~~ return task \n 
~~ def delete_server ( self , server , force = False , blocking = True , verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( server [ ] + ) \n 
~~~ task , body = self . _con . delete ( server [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ def update_server ( self , server ) : \n 
~~~ task , body = self . _con . put ( server [ ] , server ) \n 
~~ def add_server ( self , server , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , server ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
server = self . _con . get ( entity [ ] ) \n 
return server \n 
~~ ~~ return task \n 
~~ def get_server_schema ( self ) : \n 
return self . _con . get ( uri [ ] + ) \n 
~~ def get_bios ( self , server ) : \n 
return self . _con . get ( server [ ] + ) \n 
~~ def get_ilo_sso_url ( self , server ) : \n 
~~ def get_java_remote_console_url ( self , server ) : \n 
~~ def get_remote_console_url ( self , server ) : \n 
~~ def get_server_hardware_types ( self ) : \n 
body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
~~ def remove_server_hardware_type ( self , server_hardware_type , force = False , blocking = True , verbose = False ) : \n 
if force : \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] + ) \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] ) \n 
~~ def get_server_type_schema ( self ) : \n 
~~ def get_server_hardware_type ( self , server_type ) : \n 
return self . _con . get ( server_type [ ] ) \n 
~~ def set_server_hardware_type ( self , server_hardware_type , name , description ) : \n 
request = make_server_type_dict ( name , description ) \n 
task , body = self . _con . put ( server_hardware_type [ ] , request ) \n 
return task \n 
~~ def create_server_profile ( self , \n 
affinity = , \n 
biosSettings = None , \n 
bootSettings = None , \n 
bootModeSetting = None , \n 
profileConnectionV4 = None , \n 
description = None , \n 
firmwareSettingsV3 = None , \n 
hideUnusedFlexNics = True , \n 
localStorageSettingsV3 = None , \n 
macType = , \n 
name = None , \n 
sanStorageV3 = None , \n 
serialNumber = None , \n 
serialNumberType = , \n 
serverHardwareTypeUri = None , \n 
serverHardwareUri = None , \n 
serverProfileTemplateUri = None , \n 
uuid = None , \n 
wwnType = , \n 
blocking = True , verbose = False ) : \n 
profile = make_ServerProfileV5 ( affinity , biosSettings , bootSettings , \n 
bootModeSetting , profileConnectionV4 , \n 
description , firmwareSettingsV3 , \n 
hideUnusedFlexNics , \n 
localStorageSettingsV3 , macType , name , \n 
sanStorageV3 , serialNumber , \n 
serialNumberType , serverHardwareTypeUri , \n 
serverHardwareUri , \n 
serverProfileTemplateUri , uuid , wwnType ) \n 
task , body = self . _con . post ( uri [ ] , profile ) \n 
if profile [ ] is None : \n 
~~~ tout = 600 \n 
~~~ tout = 3600 \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
profile = self . _con . get ( entity [ ] ) \n 
return profile \n 
~~ def post_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~ def remove_server_profile ( self , profile , force = False , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile [ ] + ) \n 
~~~ task , body = self . _con . delete ( profile [ ] ) \n 
~~ def get_server_profiles ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
~~ def update_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile [ ] , profile ) \n 
~~~ if profile [ ] [ ] is None : \n 
~~ ~~ except Exception : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileResource [ ] ) \n 
~~ def update_server_profile_from_template ( self , profile , blocking = True , verbose = False ) : \n 
~~~ patch_request = [ { : , : , : } ] \n 
task , body = self . _con . patch ( profile [ ] , patch_request ) \n 
~~ ~~ def get_server_profile_by_name ( self , name ) : \n 
~~~ body = self . _con . get_entity_byfield ( uri [ ] , , name ) \n 
~~ def get_profile_message ( self , profile ) : \n 
message = self . _con . get ( profile [ ] + ) \n 
return message \n 
~~ def get_profile_compliance_preview ( self , profile ) : \n 
return self . _con . get ( profile [ ] + ) \n 
~~ def create_server_profile_template ( \n 
self , \n 
serverProfileDescription = None , \n 
enclosureGroupUri = None , \n 
affinity = None , \n 
hideUnusedFlexNics = None , \n 
blocking = True , \n 
profile_template = make_ServerProfileTemplateV1 ( name , \n 
description , \n 
serverProfileDescription , \n 
serverHardwareTypeUri , \n 
enclosureGroupUri , \n 
profileConnectionV4 , \n 
firmwareSettingsV3 , \n 
bootSettings , \n 
bootModeSetting ) \n 
task , body = self . _con . post ( uri [ ] , profile_template ) \n 
tout = 600 \n 
profile_template = self . _con . get ( entity [ ] ) \n 
return profile_template \n 
~~ def remove_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile_template [ ] ) \n 
~~ return body \n 
~~ def get_server_profile_templates ( self ) : \n 
~~ def get_server_profile_template_by_name ( self , name ) : \n 
~~ def update_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile_template [ ] , profile_template ) \n 
~~ profileTemplateResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileTemplateResource [ ] ) \n 
~~ def get_server_profile_from_template ( self , profile_template ) : \n 
~~~ profile = self . _con . get ( profile_template [ ] + ) \n 
~~ def get_enclosures ( self ) : \n 
~~ def add_enclosure ( self , enclosure , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , enclosure ) \n 
if enclosure [ ] is : \n 
~~ elif enclosure [ ] is None : \n 
enclosure = self . _con . get ( entity [ ] ) \n 
return enclosure \n 
~~ def remove_enclosure ( self , enclosure , force = False , blocking = True , \n 
~~~ task , body = self . _con . delete ( enclosure [ ] + ) \n 
~~~ task , body = self . _con . delete ( enclosure [ ] ) \n 
~~ def create_enclosure_group ( self , associatedLIGs , name , \n 
powerMode = ) : \n 
egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n 
task , body = self . _con . post ( uri [ ] , egroup ) \n 
~~ def delete_enclosure_group ( self , egroup ) : \n 
~~~ self . _con . delete ( egroup [ ] ) \n 
~~ def get_enclosure_groups ( self ) : \n 
~~ def update_enclosure_group ( self , enclosuregroup ) : \n 
~~~ task , body = self . _con . put ( enclosuregroup [ ] , enclosuregroup ) \n 
~~ def get_pool ( self , pooltype ) : \n 
~~~ body = self . _con . get ( uri [ ] + + pooltype ) \n 
~~ def get_vmac_pool ( self ) : \n 
~~ def get_vwwn_pool ( self ) : \n 
~~ def get_vsn_pool ( self ) : \n 
~~ def get_profile_networks ( self ) : \n 
~~ def get_profile_schema ( self ) : \n 
~~~ return self . _con . get ( uri [ ] ) \n 
~~ def get_profile_available_servers ( self ) : \n 
~~ def get_profile_available_storage_systems ( self ) : \n 
~~ def get_profile_ports ( self ) : \n 
~~ def allocate_pool_ids ( self , url , count ) : \n 
~~~ allocatorUrl = % url \n 
allocatorBody = { : count } \n 
task , body = self . _con . put ( allocatorUrl , allocatorBody ) \n 
~~ def release_pool_ids ( self , url , idList ) : \n 
~~~ collectorUrl = % url \n 
collectorBody = { : idList } \n 
task , body = self . _con . put ( collectorUrl , collectorBody ) \n 
~~ def allocate_range_ids ( self , allocatorUrl , count ) : \n 
~~~ task , body = self . _con . put ( allocatorUrl , { : count } ) \n 
~~ def release_range_ids ( self , collectorUrl , idList ) : \n 
~~~ task , body = self . _con . put ( collectorUrl , { : idList } ) \n 
~~ def enable_range ( self , url ) : \n 
~~~ prange = self . _con . get ( url ) \n 
prange [ ] = True \n 
task , body = self . _con . put ( url , prange ) \n 
~~ def disable_range ( self , url ) : \n 
prange [ ] = False \n 
~~ ~~ from . constants import MILLI_MICROS , SECOND_MICROS , MINUTE_MICROS \n 
import calendar \n 
from datetime import datetime \n 
from dateutil import parser \n 
from dateutil . tz import tzlocal \n 
from . error import TimeConstructionError \n 
from . sanedelta import SaneDelta \n 
import pytz \n 
MICROS_TRANSLATIONS = ( \n 
( ( , , , , ) , MINUTE_MICROS ) , \n 
( ( , , , , ) , SECOND_MICROS ) , \n 
( ( , , , , ) , MILLI_MICROS ) , \n 
( ( , , , , ) , 1 ) ) \n 
MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n 
class SaneTime ( object ) : \n 
def __init__ ( self , * args , ** kwargs ) : \n 
super ( time , self ) . __init__ ( ) \n 
uss = set ( ) \n 
tzs = set ( ) \n 
naive_dt = None \n 
avoid_localize = False \n 
for k , v in kwargs . iteritems ( ) : \n 
~~~ if k in ( , ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( v ) ) \n 
~~ elif k in MICROS_TRANSLATION_HASH : \n 
~~~ uss . add ( MICROS_TRANSLATION_HASH [ k ] * v ) \n 
~~ ~~ args = list ( args ) \n 
if len ( args ) > 2 and len ( args ) <= 8 : \n 
~~~ args = [ datetime ( * args ) ] \n 
~~ if len ( args ) == 2 : \n 
~~~ tzs . add ( SaneTime . to_timezone ( args . pop ( ) ) ) \n 
~~ if len ( args ) == 1 : \n 
~~~ arg = args . pop ( ) \n 
if hasattr ( arg , ) : \n 
~~~ uss . add ( int ( arg ) ) \n 
if hasattr ( arg , ) : tzs . add ( arg . tz ) \n 
~~ elif isinstance ( arg , basestring ) : \n 
~~~ parts = arg . strip ( ) . split ( ) \n 
if len ( parts ) > 1 and parts [ - 1 ] . startswith ( ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( parts [ - 1 ] [ 1 : ] ) ) \n 
arg = . join ( parts [ : - 1 ] ) \n 
~~ except : pass \n 
~~ utc = arg . endswith ( ) or arg . endswith ( ) \n 
arg = parser . parse ( arg ) \n 
~~~ if utc : \n 
~~~ tzs . add ( pytz . utc ) \n 
arg = arg . replace ( tzinfo = None ) \n 
~~~ arg = arg . replace ( tzinfo = None ) \n 
~~~ avoid_localize = True \n 
arg = arg . astimezone ( pytz . utc ) . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if type ( arg ) == datetime : \n 
~~~ naive_dt = arg \n 
if naive_dt . tzinfo : \n 
~~~ tzs . add ( SaneTime . to_timezone ( str ( naive_dt . tzinfo ) ) ) \n 
naive_dt = naive_dt . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if len ( tzs ) > 1 : \n 
~~ self . tz = len ( tzs ) and tzs . pop ( ) or pytz . utc \n 
if naive_dt : \n 
~~~ if avoid_localize : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( naive_dt ) ) \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( self . tz . localize ( naive_dt ) . astimezone ( pytz . utc ) ) ) \n 
~~ ~~ if len ( uss ) == 0 : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( datetime . utcnow ( ) ) ) \n 
~~ if len ( uss ) > 1 : \n 
~~ self . us = uss . pop ( ) \n 
if len ( args ) > 0 : \n 
~~ ~~ @ property \n 
def ms ( self ) : return self . us / MILLI_MICROS \n 
epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n 
def s ( self ) : return self . us / SECOND_MICROS \n 
epoch_seconds = epoch_secs = seconds = secs = s \n 
def m ( self ) : return self . us / MINUTE_MICROS \n 
epoch_minutes = epoch_mins = minutes = mins = m \n 
def micros ( self ) : return self . us \n 
epoch_microseconds = epoch_micros = microseconds = micros \n 
def tz_name ( self ) : return self . tz . zone \n 
def tz_abbr ( self ) : return self . tz . _tzname \n 
def set_tz ( self , tz ) : \n 
~~~ self . tz = self . __class__ . to_timezone ( tz ) ; return self \n 
~~ def with_tz ( self , tz ) : \n 
~~~ return self . __class__ ( self . us , tz ) \n 
~~ @ property \n 
def _tuple ( self ) : return ( self . us , self . tz ) \n 
def strftime ( self , * args , ** kwargs ) : return self . datetime . strftime ( * args , ** kwargs ) \n 
def __cmp__ ( self , other ) : \n 
~~~ if not hasattr ( other , ) : other = SaneTime ( other ) \n 
return cmp ( self . us , int ( other ) ) \n 
~~ def __hash__ ( self ) : return self . us . __hash__ ( ) \n 
def __add__ ( self , operand ) : \n 
~~~ if not hasattr ( operand , ) : operand = SaneTime ( operand ) \n 
return self . __class__ ( self . us + int ( operand ) , tz = self . tz ) \n 
~~ def __sub__ ( self , operand ) : \n 
if isinstance ( operand , SaneTime ) : return SaneDelta ( self . us - int ( operand ) ) \n 
return self . __add__ ( - int ( operand ) ) \n 
~~ def __mul__ ( self , operand ) : \n 
~~~ return self . us * int ( operand ) \n 
~~ def __div__ ( self , operand ) : \n 
~~~ return self . us / int ( operand ) \n 
~~ def __int__ ( self ) : return int ( self . us ) \n 
def __long__ ( self ) : return long ( self . us ) \n 
def __repr__ ( self ) : return u"SaneTime(%s,%s)" % ( self . us , repr ( self . tz ) ) \n 
def __str__ ( self ) : return unicode ( self ) . encode ( ) \n 
def __unicode__ ( self ) : \n 
~~~ dt = self . datetime \n 
micros = u".%06d" % dt . microsecond if dt . microsecond else \n 
~~ def clone ( self ) : \n 
return self . __class__ ( self . us , self . tz ) \n 
def ny_str ( self ) : \n 
return self . ny_ndt . strftime ( ) \n 
def utc_datetime ( self ) : return SaneTime . us_to_utc_datetime ( self . us ) \n 
utc_dt = utc_datetime \n 
def utc_naive_datetime ( self ) : return self . utc_datetime . replace ( tzinfo = None ) \n 
utc_ndt = utc_naive_datetime \n 
def to_timezoned_datetime ( self , tz ) : return self . utc_datetime . astimezone ( SaneTime . to_timezone ( tz ) ) \n 
def to_timezoned_naive_datetime ( self , tz ) : return self . to_timezoned_datetime ( tz ) . replace ( tzinfo = None ) \n 
def datetime ( self ) : return self . to_timezoned_datetime ( self . tz ) \n 
dt = datetime \n 
def naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( self . tz ) \n 
ndt = naive_datetime \n 
def ny_datetime ( self ) : return self . to_timezoned_datetime ( ) \n 
ny_dt = ny_datetime \n 
def ny_naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( ) \n 
ny_ndt = ny_naive_datetime \n 
def year ( self ) : return self . dt . year \n 
def month ( self ) : return self . dt . month \n 
def day ( self ) : return self . dt . day \n 
def hour ( self ) : return self . dt . hour \n 
def minute ( self ) : return self . dt . minute \n 
def second ( self ) : return self . dt . second \n 
def microsecond ( self ) : return self . dt . microsecond \n 
@ classmethod \n 
def utc_datetime_to_us ( kls , dt ) : \n 
~~~ return calendar . timegm ( dt . timetuple ( ) ) * 1000 ** 2 + dt . microsecond \n 
~~ @ classmethod \n 
def us_to_utc_datetime ( kls , us ) : \n 
~~~ return pytz . utc . localize ( datetime . utcfromtimestamp ( us / 10 ** 6 ) ) . replace ( microsecond = us % 10 ** 6 ) \n 
def to_timezone ( kls , tz ) : \n 
~~~ if not isinstance ( tz , basestring ) : return tz \n 
return pytz . timezone ( tz ) \n 
~~ ~~ def ntime ( * args , ** kwargs ) : \n 
~~~ if args : \n 
~~~ if args [ 0 ] is None : return None \n 
~~ elif kwargs : \n 
~~~ if None in [ v for k , v in kwargs . iteritems ( ) if k != ] : return None \n 
~~ return SaneTime ( * args , ** kwargs ) \n 
~~ time = sanetime = SaneTime \n 
nsanetime = ntime \n 
from tastypie . authorization import Authorization \n 
from openpds . authentication import OAuth2Authentication \n 
from openpds . core . models import Profile , AuditEntry \n 
import settings \n 
import pdb \n 
class PDSAuthorization ( Authorization ) : \n 
~~~ audit_enabled = True \n 
scope = "" \n 
requester_uuid = "" \n 
def requester ( self ) : \n 
~~~ return self . requester_uuid \n 
~~ def trustWrapper ( self , datastore_owner ) : \n 
~~ def is_authorized ( self , request , object = None ) : \n 
~~~ authenticator = OAuth2Authentication ( self . scope ) \n 
if "datastore_owner__uuid" in request . REQUEST : \n 
~~~ authorized = True \n 
token = request . REQUEST [ "bearer_token" ] if "bearer_token" in request . REQUEST else request . META [ "HTTP_BEARER_TOKEN" ] \n 
datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n 
datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n 
self . requester_uuid = authenticator . get_userinfo_from_token ( token , self . scope ) \n 
if self . requester_uuid is False or self . requester_uuid is None or len ( self . requester_uuid ) == 0 : \n 
~~~ self . requester_uuid = "not-specified" \n 
authorized = False \n 
~~ self . trustWrapper ( datastore_owner ) \n 
~~~ if ( self . audit_enabled ) : \n 
#pdb.set_trace() \n 
~~~ audit_entry = AuditEntry ( token = token ) \n 
audit_entry . method = request . method \n 
audit_entry . scope = self . scope \n 
audit_entry . purpose = request . REQUEST [ "purpose" ] if "purpose" in request . REQUEST else "" \n 
audit_entry . system_entity_toggle = request . REQUEST [ "system_entity" ] if "system_entity" in request . REQUEST else False \n 
audit_entry . datastore_owner = datastore_owner \n 
audit_entry . requester , created = Profile . objects . get_or_create ( uuid = self . requester_uuid ) \n 
audit_entry . script = request . path \n 
audit_entry . save ( ) \n 
~~~ print e \n 
~~ return authorized \n 
~~ return False \n 
~~ def __init__ ( self , scope , audit_enabled = True ) : \n 
~~~ self . scope = scope \n 
self . audit_enabled = audit_enabled \n 
from django import template \n 
register = template . Library ( ) \n 
class VerbatimNode ( template . Node ) : \n 
~~~ def __init__ ( self , text ) : \n 
~~~ self . text = text \n 
~~ def render ( self , context ) : \n 
~~~ return self . text \n 
~~ ~~ @ register . tag \n 
def verbatim ( parser , token ) : \n 
~~~ text = [ ] \n 
while 1 : \n 
~~~ token = parser . tokens . pop ( 0 ) \n 
if token . contents == : \n 
~~~ break \n 
~~ if token . token_type == template . TOKEN_VAR : \n 
~~~ text . append ( ) \n 
~~ elif token . token_type == template . TOKEN_BLOCK : \n 
~~ text . append ( token . contents ) \n 
if token . token_type == template . TOKEN_VAR : \n 
~~ ~~ return VerbatimNode ( . join ( text ) ) \n 
~~ from django . shortcuts import render_to_response \n 
from django . template import RequestContext \n 
from gevent import monkey ; monkey . patch_all ( ) \n 
import gevent \n 
from ws4py . client . geventclient import WebSocketClient \n 
if __name__ == : \n 
~~~ ws = WebSocketClient ( , protocols = [ , ] ) \n 
ws . connect ( ) \n 
print ( ( ws . receive ( ) , ) ) \n 
def incoming ( ) : \n 
~~~ while True : \n 
~~~ m = ws . receive ( ) \n 
if m is not None : \n 
~~~ m = str ( m ) \n 
print ( ( m , len ( m ) ) ) \n 
if len ( m ) == 35 : \n 
~~~ ws . close ( ) \n 
break \n 
~~ def outgoing ( ) : \n 
~~~ for i in range ( 0 , 40 , 5 ) : \n 
~~~ ws . send ( "*" * i ) \n 
~~ ws . send ( "Foobar" ) \n 
~~ greenlets = [ \n 
gevent . spawn ( incoming ) , \n 
gevent . spawn ( outgoing ) , \n 
] \n 
gevent . joinall ( greenlets ) \n 
~~ import os \n 
from ws4py . framing import Frame , OPCODE_CONTINUATION , OPCODE_TEXT , OPCODE_BINARY , OPCODE_CLOSE , OPCODE_PING , OPCODE_PONG \n 
from ws4py . compat import unicode , py3k \n 
__all__ = [ , , , , \n 
, ] \n 
class Message ( object ) : \n 
~~~ def __init__ ( self , opcode , data = , encoding = ) : \n 
self . opcode = opcode \n 
self . _completed = False \n 
self . encoding = encoding \n 
if isinstance ( data , unicode ) : \n 
~~~ if not encoding : \n 
~~ data = data . encode ( encoding ) \n 
~~ elif isinstance ( data , bytearray ) : \n 
~~~ data = bytes ( data ) \n 
~~ elif not isinstance ( data , bytes ) : \n 
~~ self . data = data \n 
~~ def single ( self , mask = False ) : \n 
mask = os . urandom ( 4 ) if mask else None \n 
return Frame ( body = self . data , opcode = self . opcode , \n 
masking_key = mask , fin = 1 ) . build ( ) \n 
~~ def fragment ( self , first = False , last = False , mask = False ) : \n 
fin = 1 if last is True else 0 \n 
opcode = self . opcode if first is True else OPCODE_CONTINUATION \n 
return Frame ( body = self . data , \n 
opcode = opcode , masking_key = mask , \n 
fin = fin ) . build ( ) \n 
def completed ( self ) : \n 
return self . _completed \n 
~~ @ completed . setter \n 
def completed ( self , state ) : \n 
self . _completed = state \n 
~~ def extend ( self , data ) : \n 
if isinstance ( data , bytes ) : \n 
~~~ self . data += data \n 
~~~ self . data += bytes ( data ) \n 
~~ elif isinstance ( data , unicode ) : \n 
~~~ self . data += data . encode ( self . encoding ) \n 
~~ ~~ def __len__ ( self ) : \n 
~~~ return len ( self . __unicode__ ( ) ) \n 
~~ def __str__ ( self ) : \n 
~~~ if py3k : \n 
~~~ return self . data . decode ( self . encoding ) \n 
~~ return self . data \n 
~~ def __unicode__ ( self ) : \n 
~~ ~~ class TextMessage ( Message ) : \n 
~~~ def __init__ ( self , text = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_TEXT , text ) \n 
def is_binary ( self ) : \n 
def is_text ( self ) : \n 
~~~ return True \n 
~~ ~~ class BinaryMessage ( Message ) : \n 
~~~ def __init__ ( self , bytes = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_BINARY , bytes , encoding = None ) \n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . data ) \n 
~~ ~~ class CloseControlMessage ( Message ) : \n 
~~~ def __init__ ( self , code = 1000 , reason = ) : \n 
~~~ data = b"" \n 
if code : \n 
~~~ data += struct . pack ( "!H" , code ) \n 
~~ if reason is not None : \n 
~~~ if isinstance ( reason , unicode ) : \n 
~~~ reason = reason . encode ( ) \n 
~~ data += reason \n 
~~ Message . __init__ ( self , OPCODE_CLOSE , data , ) \n 
self . code = code \n 
self . reason = reason \n 
~~~ return self . reason . decode ( ) \n 
~~ return self . reason \n 
~~~ return self . reason . decode ( self . encoding ) \n 
~~ ~~ class PingControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_PING , data ) \n 
~~ ~~ class PongControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data ) : \n 
~~~ Message . __init__ ( self , OPCODE_PONG , data ) \n 
~~ ~~ import sys \n 
import base64 \n 
import urllib \n 
from struct import unpack \n 
from threading import Lock \n 
from binascii import hexlify \n 
from urlparse import urlparse \n 
from mod_python import apache \n 
from PyAuthenNTLM2 . ntlm_dc_proxy import NTLM_DC_Proxy \n 
from PyAuthenNTLM2 . ntlm_ad_proxy import NTLM_AD_Proxy \n 
use_basic_auth = True \n 
~~~ from PyAuthenNTLM2 . ntlm_client import NTLM_Client \n 
~~ except ImportError : \n 
~~~ use_basic_auth = False \n 
~~ class CacheConnections : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . _mutex = Lock ( ) \n 
self . _cache = { } \n 
~~~ return len ( self . _cache ) \n 
~~ def remove ( self , id ) : \n 
~~~ self . _mutex . acquire ( ) \n 
( proxy , ts ) = self . _cache . get ( id , ( None , None ) ) \n 
if proxy : \n 
~~~ proxy . close ( ) \n 
del self . _cache [ id ] \n 
~~ self . _mutex . release ( ) \n 
~~ def add ( self , id , proxy ) : \n 
self . _cache [ id ] = ( proxy , int ( time . time ( ) ) ) \n 
self . _mutex . release ( ) \n 
~~ def clean ( self ) : \n 
~~~ now = int ( time . time ( ) ) \n 
self . _mutex . acquire ( ) \n 
for id , conn in self . _cache . items ( ) : \n 
~~~ if conn [ 1 ] + 60 < now : \n 
~~~ conn [ 0 ] . close ( ) \n 
~~ ~~ self . _mutex . release ( ) \n 
~~ def has_key ( self , id ) : \n 
~~~ return self . _cache . has_key ( id ) \n 
~~ def get_proxy ( self , id ) : \n 
proxy = self . _cache [ id ] [ 0 ] \n 
return proxy \n 
~~ ~~ class CacheGroups : \n 
~~ def add ( self , group , user ) : \n 
if not self . _cache . has_key ( group ) : \n 
~~~ self . _cache [ group ] = { } \n 
~~ self . _cache [ group ] [ user ] = int ( time . time ( ) ) \n 
old = [ ] \n 
for group , members in self . _cache . items ( ) : \n 
~~~ for user in members : \n 
~~~ if members [ user ] + 3 * 60 * 60 < now : \n 
~~~ old . append ( ( group , user ) ) \n 
~~ ~~ ~~ for group , user in old : \n 
~~~ del self . _cache [ group ] [ user ] \n 
~~ def has ( self , group , user ) : \n 
~~~ if not self . _cache . has_key ( group ) : \n 
~~ return self . _cache [ group ] . has_key ( user ) \n 
~~ ~~ cache = CacheConnections ( ) \n 
cacheGroups = CacheGroups ( ) \n 
def ntlm_message_type ( msg ) : \n 
~~~ if not msg . startswith ( ) or len ( msg ) < 12 : \n 
~~ msg_type = unpack ( , msg [ 8 : 8 + 4 ] ) [ 0 ] \n 
if msg_type not in ( 1 , 2 , 3 ) : \n 
~~ return msg_type \n 
~~ def parse_ntlm_authenticate ( msg ) : \n 
~~~ \n 
NTLMSSP_NEGOTIATE_UNICODE = 0x00000001 \n 
idx = 28 \n 
length , offset = unpack ( , msg [ idx : idx + 8 ] ) \n 
domain = msg [ offset : offset + length ] \n 
idx += 8 \n 
username = msg [ offset : offset + length ] \n 
idx += 24 \n 
flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n 
if flags & NTLMSSP_NEGOTIATE_UNICODE : \n 
~~~ domain = str ( domain . decode ( ) ) \n 
username = str ( username . decode ( ) ) \n 
~~ return username , domain \n 
~~ def set_remote_user ( req , username , domain ) : \n 
~~~ format = req . get_options ( ) . get ( , ) . lower ( ) \n 
if format == : \n 
~~~ req . user = domain + + username \n 
~~~ req . user = username \n 
~~ ~~ def decode_http_authorization_header ( auth ) : \n 
ah = auth . split ( ) \n 
if len ( ah ) == 2 : \n 
~~~ b64 = base64 . b64decode ( ah [ 1 ] ) \n 
if ah [ 0 ] == : \n 
~~~ return ( , b64 ) \n 
~~ elif ah [ 0 ] == and use_basic_auth : \n 
~~~ ( user , password ) = b64 . split ( ) \n 
return ( , user , password ) \n 
~~ ~~ return False \n 
~~ def handle_unauthorized ( req ) : \n 
req . err_headers_out . add ( , ) \n 
if use_basic_auth : \n 
~~ req . err_headers_out . add ( , ) \n 
return apache . HTTP_UNAUTHORIZED \n 
~~ def connect_to_proxy ( req , type1 ) : \n 
~~~ domain = req . get_options ( ) [ ] \n 
pdc = req . get_options ( ) [ ] \n 
bdc = req . get_options ( ) . get ( , False ) \n 
~~ except KeyError , e : \n 
~~~ req . log_error ( % str ( e ) , apache . APLOG_CRIT ) \n 
raise \n 
~~ ntlm_challenge = None \n 
for server in ( pdc , bdc ) : \n 
~~~ if not server : continue \n 
~~~ if server . startswith ( ) : \n 
~~~ url = urlparse ( server ) \n 
decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n 
( url . netloc , domain , decoded_path ) , apache . APLOG_INFO ) \n 
proxy = NTLM_AD_Proxy ( url . netloc , domain , base = decoded_path ) \n 
~~~ req . log_error ( % \n 
( server , domain ) , apache . APLOG_INFO ) \n 
proxy = NTLM_DC_Proxy ( server , domain ) \n 
~~ ntlm_challenge = proxy . negotiate ( type1 ) \n 
~~ except Exception , e : \n 
~~~ req . log_error ( % ( server , str ( e ) ) , apache . APLOG_CRIT ) \n 
~~ if ntlm_challenge : break \n 
proxy . close ( ) \n 
~~ return ( proxy , ntlm_challenge ) \n 
~~ def handle_type1 ( req , ntlm_message ) : \n 
cache . remove ( req . connection . id ) \n 
cache . clean ( ) \n 
~~~ ( proxy , ntlm_challenge ) = connect_to_proxy ( req , ntlm_message ) \n 
~~~ return apache . HTTP_INTERNAL_SERVER_ERROR \n 
~~ cache . add ( req . connection . id , proxy ) \n 
~~ def check_authorization ( req , username , proxy ) : \n 
rules = . join ( req . requires ( ) ) . strip ( ) \n 
if rules == or cacheGroups . has ( rules , username ) : \n 
~~ groups = [ ] \n 
for r in req . requires ( ) : \n 
~~~ users = [ u . strip ( ) for u in r [ 5 : ] . split ( "," ) ] \n 
if username in users : \n 
( username , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
return True \n 
~~~ groups += [ g . strip ( ) for g in r [ 6 : ] . split ( "," ) ] \n 
~~ ~~ if groups : \n 
~~~ res = proxy . check_membership ( username , groups ) \n 
~~~ req . log_error ( % ( username , str ( groups ) , req . unparsed_uri , str ( e ) ) ) \n 
~~ if res : \n 
~~~ cacheGroups . add ( rules , username ) \n 
req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
~~ req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) ) \n 
( username , req . unparsed_uri ) ) \n 
~~ def handle_type3 ( req , ntlm_message ) : \n 
proxy = cache . get_proxy ( req . connection . id ) \n 
~~~ user , domain = parse_ntlm_authenticate ( ntlm_message ) \n 
if not domain : \n 
~~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
~~ result = proxy . authenticate ( ntlm_message ) \n 
user , domain = , \n 
result = False \n 
~~ if not result : \n 
~~~ cache . remove ( req . connection . id ) \n 
req . log_error ( % ( \n 
domain , user , req . unparsed_uri ) ) \n 
return handle_unauthorized ( req ) \n 
~~ req . log_error ( % ( user , domain , req . unparsed_uri ) , apache . APLOG_NOTICE ) \n 
set_remote_user ( req , user , domain ) \n 
result = check_authorization ( req , user , proxy ) \n 
if not result : \n 
~~~ return apache . HTTP_FORBIDDEN \n 
~~ req . connection . notes . add ( , req . user ) \n 
return apache . OK \n 
~~ def handle_basic ( req , user , password ) : \n 
req . log_error ( % ( req . unparsed_uri ) ) \n 
domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
client = NTLM_Client ( user , domain , password ) \n 
type1 = client . make_ntlm_negotiate ( ) \n 
~~~ ( proxy , type2 ) = connect_to_proxy ( req , type1 ) \n 
~~ client . parse_ntlm_challenge ( type2 ) \n 
type3 = client . make_ntlm_authenticate ( ) \n 
if not proxy . authenticate ( type3 ) : \n 
user , domain , req . unparsed_uri ) ) \n 
~~ req . connection . notes . add ( , user + password ) \n 
~~ def authenhandler ( req ) : \n 
req . connection . id , req . method , req . unparsed_uri , len ( cache ) ) , apache . APLOG_INFO ) \n 
auth_headers = req . headers_in . get ( , [ ] ) \n 
if not isinstance ( auth_headers , list ) : \n 
~~~ auth_headers = [ auth_headers ] \n 
~~ user = req . connection . notes . get ( , None ) \n 
if user : \n 
~~~ req . user = user \n 
if auth_headers : \n 
~~~ req . log_error ( % ( \n 
req . connection . id , req . method , req . clength , auth_headers ) , apache . APLOG_INFO ) \n 
if req . method != or req . clength > 0 : \n 
~~~ return apache . OK \n 
~~ ~~ if not auth_headers : \n 
~~~ return handle_unauthorized ( req ) \n 
~~ try : \n 
~~~ for ah in auth_headers : \n 
~~~ ah_data = decode_http_authorization_header ( ah ) \n 
if ah_data : \n 
~~ ~~ ~~ except : \n 
~~~ ah_data = False \n 
~~ if not ah_data : \n 
~~~ req . log_error ( % req . unparsed_uri , apache . APLOG_ERR ) \n 
return apache . HTTP_BAD_REQUEST \n 
~~ if ah_data [ 0 ] == : \n 
~~~ userpwd = req . connection . notes . get ( , None ) \n 
if userpwd : \n 
~~~ if userpwd != ah_data [ 1 ] + ah_data [ 2 ] : \n 
~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
set_remote_user ( req , ah_data [ 1 ] , domain ) \n 
~~ return handle_basic ( req , ah_data [ 1 ] , ah_data [ 2 ] ) \n 
~~~ ntlm_version = ntlm_message_type ( ah_data [ 1 ] ) \n 
if ntlm_version == 1 : \n 
~~~ return handle_type1 ( req , ah_data [ 1 ] ) \n 
~~ if ntlm_version == 3 : \n 
~~~ if cache . has_key ( req . connection . id ) : \n 
~~~ return handle_type3 ( req , ah_data [ 1 ] ) \n 
( req . unparsed_uri ) , apache . APLOG_INFO ) \n 
~~ error = \n 
~~~ error = str ( e ) \n 
( req . unparsed_uri , error ) , apache . APLOG_ERR ) \n 
~~ from celery import Celery \n 
def create_celery_app ( app ) : \n 
~~~ if app . config . get ( ) : \n 
~~~ app . celery = Celery ( __name__ , broker = app . config [ ] ) \n 
app . celery . conf . update ( app . config ) \n 
taskbase = app . celery . Task \n 
class ContextTask ( taskbase ) : \n 
~~~ abstract = True \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ with app . app_context ( ) : \n 
~~~ return taskbase . __call__ ( self , * args , ** kwargs ) \n 
~~ ~~ ~~ app . celery . Task = ContextTask \n 
~~ ~~ import unittest \n 
sys . path . append ( os . path . dirname ( os . path . realpath ( __file__ ) . rsplit ( , 2 ) [ 0 ] ) ) \n 
from app import create_app \n 
app = create_app ( ) \n 
class TestUsers ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . app = app . test_client ( ) \n 
~~ def test_01_add ( self ) : \n 
~~~ rv = self . app . post ( , data = add_data , \n 
content_type = "application/json" ) \n 
assert rv . status_code == 201 \n 
~~ def test_02_read_update ( self ) : \n 
~~~ request = self . app . get ( ) \n 
dict = json . loads ( request . data . decode ( ) ) \n 
id = dict [ ] [ 0 ] [ ] \n 
rv = self . app . patch ( . format ( id ) , \n 
data = update_data , content_type = "application/json" ) \n 
assert rv . status_code == 200 \n 
~~ def test_03_delete ( self ) : \n 
rv = self . app . delete ( . format ( id ) ) \n 
assert rv . status_code == 204 \n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ from django . core . management . base import BaseCommand \n 
from chronam . core . management . commands import configure_logging \n 
from chronam . core . index import index_pages \n 
configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n 
class Command ( BaseCommand ) : \n 
~~~ def handle ( self , ** options ) : \n 
~~~ index_pages ( ) \n 
~~ ~~ import os \n 
from django . conf import settings \n 
from django . http import HttpResponse \n 
class HttpResponseServiceUnavailable ( HttpResponse ) : \n 
~~~ status_code = 503 \n 
~~ class TooBusyMiddleware ( object ) : \n 
~~~ def process_request ( self , request ) : \n 
~~~ one , five , fifteen = os . getloadavg ( ) \n 
if one > settings . TOO_BUSY_LOAD_AVERAGE : \n 
~~ return None \n 
~~ ~~ from os . path import dirname , join \n 
from django . test import TestCase \n 
from chronam . core . ocr_extractor import ocr_extractor \n 
class OcrExtractorTests ( TestCase ) : \n 
~~~ def test_extractor ( self ) : \n 
~~~ dir = join ( dirname ( dirname ( __file__ ) ) , ) \n 
ocr_file = join ( dir , ) \n 
text , coord_info = ocr_extractor ( ocr_file ) \n 
coords = coord_info [ "coords" ] \n 
expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n 
self . assertEqual ( text , expected_text ) \n 
self . assertEqual ( len ( coords . keys ( ) ) , 2150 ) \n 
self . assertEqual ( len ( coords [ ] ) , 3 ) \n 
self . assertTrue ( coords . has_key ( ) ) \n 
self . assertTrue ( not coords . has_key ( ) ) \n 
import logging \n 
from . tests import * \n 
logger = logging . getLogger ( "human_curl" ) \n 
logger . setLevel ( logging . DEBUG ) \n 
handler = logging . StreamHandler ( ) \n 
handler . setFormatter ( formatter ) \n 
logger . addHandler ( handler ) \n 
import asyncio \n 
from zeroservices import ZeroMQMedium , ResourceService \n 
from zeroservices . services import get_http_interface \n 
from zeroservices . discovery import UdpDiscoveryMedium \n 
~~~ loop = asyncio . get_event_loop ( ) \n 
medium = ZeroMQMedium ( loop , UdpDiscoveryMedium ) \n 
service = ResourceService ( , medium ) \n 
application = get_http_interface ( service , loop , port = 5001 , allowed_origins = "*" ) \n 
application = loop . run_until_complete ( application ) \n 
loop . run_until_complete ( service . start ( ) ) \n 
loop . run_forever ( ) \n 
from trello import TrelloClient \n 
from slugify import slugify \n 
from matterllo . utils import config \n 
from matterllo . utils import logger \n 
SETTINGS = config ( ) \n 
LOGGING = logger ( ) \n 
parser . add_argument ( , dest = , action = , help = ) \n 
if not args . cleanup and not args . update and not args . init : \n 
~~~ print parser . print_help ( ) \n 
sys . exit ( 0 ) \n 
~~ client = TrelloClient ( api_key = SETTINGS [ ] , token = SETTINGS [ ] ) \n 
trello_boards = client . list_boards ( ) \n 
boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n 
if args . cleanup or args . init : \n 
~~~ result = [ h . delete ( ) for h in client . list_hooks ( ) ] \n 
LOGGING . info ( . format ( len ( result ) ) ) \n 
~~ if args . update or args . init : \n 
~~~ for board in trello_boards : \n 
~~~ board_name = slugify ( board . name ) \n 
if board_name not in boards_name : \n 
~~ LOGGING . info ( . format ( board_name ) ) \n 
url = SETTINGS [ ] + \n 
result = client . create_hook ( url , board . id ) \n 
LOGGING . info ( . format ( board_name , result ) ) \n 
~~ ~~ ~~ except Exception as e : \n 
~~~ LOGGING . error ( . format ( e ) ) \n 
sys . exit ( 1 ) \n 
~~~ main ( ) \n 
from contextlib import contextmanager \n 
import zlib \n 
~~~ from . import ssl_compat \n 
~~~ ssl_compat = None \n 
~~ _ver = sys . version_info \n 
is_py2 = _ver [ 0 ] == 2 \n 
is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n 
is_py3 = _ver [ 0 ] == 3 \n 
is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n 
@ contextmanager \n 
def ignore_missing ( ) : \n 
~~~ yield \n 
~~ ~~ if is_py2 : \n 
~~~ if is_py2_7_9_or_later : \n 
~~~ import ssl \n 
~~~ ssl = ssl_compat \n 
~~ from urllib import urlencode \n 
from urlparse import urlparse , urlsplit \n 
from itertools import imap \n 
def to_byte ( char ) : \n 
~~~ return ord ( char ) \n 
~~ def decode_hex ( b ) : \n 
~~~ return b . decode ( ) \n 
~~ def write_to_stdout ( data ) : \n 
~~~ sys . stdout . write ( data + ) \n 
sys . stdout . flush ( ) \n 
~~ def zlib_compressobj ( level = 6 , method = zlib . DEFLATED , wbits = 15 , memlevel = 8 , \n 
strategy = zlib . Z_DEFAULT_STRATEGY ) : \n 
~~~ return zlib . compressobj ( level , method , wbits , memlevel , strategy ) \n 
~~ unicode = unicode \n 
bytes = str \n 
~~ elif is_py3 : \n 
~~~ from urllib . parse import urlencode , urlparse , urlsplit \n 
imap = map \n 
~~~ return char \n 
~~~ return bytes . fromhex ( b ) \n 
~~~ sys . stdout . buffer . write ( data + ) \n 
sys . stdout . buffer . flush ( ) \n 
~~ zlib_compressobj = zlib . compressobj \n 
if is_py3_3 : \n 
~~ unicode = str \n 
bytes = bytes \n 
import threading \n 
import socket \n 
from hyper import HTTP20Connection \n 
from hyper . compat import ssl \n 
from hyper . http11 . connection import HTTP11Connection \n 
from hpack . hpack import Encoder \n 
from hpack . huffman import HuffmanEncoder \n 
from hpack . huffman_constants import ( \n 
REQUEST_CODES , REQUEST_CODES_LENGTH \n 
) \n 
from hyper . tls import NPN_PROTOCOL \n 
class SocketServerThread ( threading . Thread ) : \n 
def __init__ ( self , \n 
socket_handler , \n 
host = , \n 
ready_event = None , \n 
h2 = True , \n 
secure = True ) : \n 
~~~ threading . Thread . __init__ ( self ) \n 
self . socket_handler = socket_handler \n 
self . host = host \n 
self . secure = secure \n 
self . ready_event = ready_event \n 
self . daemon = True \n 
if self . secure : \n 
~~~ self . cxt = ssl . SSLContext ( ssl . PROTOCOL_SSLv23 ) \n 
if ssl . HAS_NPN and h2 : \n 
~~~ self . cxt . set_npn_protocols ( [ NPN_PROTOCOL ] ) \n 
~~ self . cxt . load_cert_chain ( certfile = , \n 
keyfile = ) \n 
~~ ~~ def _start_server ( self ) : \n 
~~~ sock = socket . socket ( ) \n 
if sys . platform != : \n 
~~~ sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n 
~~ if self . secure : \n 
~~~ sock = self . cxt . wrap_socket ( sock , server_side = True ) \n 
~~ sock . bind ( ( self . host , 0 ) ) \n 
self . port = sock . getsockname ( ) [ 1 ] \n 
sock . listen ( 1 ) \n 
if self . ready_event : \n 
~~~ self . ready_event . set ( ) \n 
~~ self . socket_handler ( sock ) \n 
sock . close ( ) \n 
~~ def _wrap_socket ( self , sock ) : \n 
~~~ raise NotImplementedError ( ) \n 
~~ def run ( self ) : \n 
~~~ self . server = self . _start_server ( ) \n 
~~ ~~ class SocketLevelTest ( object ) : \n 
def set_up ( self , secure = True , proxy = False ) : \n 
~~~ self . host = None \n 
self . port = None \n 
self . secure = secure if not proxy else False \n 
self . proxy = proxy \n 
self . server_thread = None \n 
~~ def _start_server ( self , socket_handler ) : \n 
ready_event = threading . Event ( ) \n 
self . server_thread = SocketServerThread ( \n 
socket_handler = socket_handler , \n 
ready_event = ready_event , \n 
h2 = self . h2 , \n 
secure = self . secure \n 
self . server_thread . start ( ) \n 
ready_event . wait ( ) \n 
self . host = self . server_thread . host \n 
self . port = self . server_thread . port \n 
self . secure = self . server_thread . secure \n 
~~ def get_connection ( self ) : \n 
~~~ if self . h2 : \n 
~~~ if not self . proxy : \n 
~~~ return HTTP20Connection ( self . host , self . port , self . secure ) \n 
~~~ return HTTP20Connection ( , secure = self . secure , \n 
proxy_host = self . host , \n 
proxy_port = self . port ) \n 
~~~ return HTTP11Connection ( self . host , self . port , self . secure ) \n 
~~~ return HTTP11Connection ( , secure = self . secure , \n 
~~ ~~ ~~ def get_encoder ( self ) : \n 
e = Encoder ( ) \n 
e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n 
return e \n 
~~ def tear_down ( self ) : \n 
self . server_thread . join ( 0.1 ) \n 
import unittest \n 
from SystemConfiguration import * \n 
class SCPreferences ( object ) : \n 
session = None \n 
def __init__ ( self ) : \n 
~~~ super ( SCPreferences , self ) . __init__ ( ) \n 
self . session = SCPreferencesCreate ( None , "set-proxy" , None ) \n 
~~ def save ( self ) : \n 
~~~ if not self . session : \n 
~~ if not SCPreferencesCommitChanges ( self . session ) : \n 
~~ if not SCPreferencesApplyChanges ( self . session ) : \n 
~~ ~~ def set_proxy ( self , enable = True , protocol = "HTTP" , server = "localhost" , port = 3128 ) : \n 
~~~ new_settings = SCPreferencesPathGetValue ( self . session , ) \n 
for interface in new_settings : \n 
~~~ new_settings [ interface ] [ ] [ "%sEnable" % protocol ] = 1 if enable else 0 \n 
if enable : \n 
~~~ new_settings [ interface ] [ ] [ % protocol ] = int ( port ) \n 
new_settings [ interface ] [ ] [ % protocol ] = server \n 
~~ ~~ SCPreferencesPathSetValue ( self . session , , new_settings ) \n 
~~ ~~ class SCPreferencesTests ( unittest . TestCase ) : \n 
~~ import sublime , sublime_plugin , os , re \n 
class StyleSheetSetup : \n 
~~~ def __init__ ( self , extensions , regex , partials = None , index = None ) : \n 
~~~ if partials is None : \n 
~~~ self . partials = False \n 
~~~ self . partials = partials \n 
~~ if index is None : \n 
~~~ self . index = False \n 
~~~ self . index = index \n 
~~ self . extensions = extensions \n 
self . regex = regex \n 
~~ ~~ class ListStylesheetVariables ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit ) : \n 
~~~ settings = sublime . load_settings ( ) \n 
handle_imports = settings . get ( "readImported" ) \n 
read_all_views = settings . get ( "readAllViews" ) \n 
setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n 
chosen_setup = None \n 
self . edit = edit \n 
fn = self . view . file_name ( ) . encode ( "utf_8" ) \n 
for setup in setups : \n 
~~~ for ext in setup . extensions : \n 
~~~ if fn . endswith ( ext ) : \n 
~~~ chosen_setup = setup \n 
~~ ~~ ~~ if chosen_setup == None : \n 
~~ imports = [ ] \n 
imported_vars = [ ] \n 
compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n 
if handle_imports : \n 
file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n 
for i , filename in enumerate ( imports ) : \n 
~~~ has_extension = False \n 
for ext in chosen_setup . extensions : \n 
~~~ if filename . endswith ( ext . decode ( "utf-8" ) ) : \n 
~~~ has_extension = True \n 
~~ ~~ if has_extension == False : \n 
~~~ for ext in chosen_setup . extensions : \n 
~~~ ext = ext . decode ( "utf-8" ) \n 
if os . path . isfile ( os . path . normpath ( file_dir + + filename + ext ) ) : \n 
~~~ filename += ext \n 
~~ if chosen_setup . partials : \n 
~~~ fn_split = os . path . split ( filename ) \n 
partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n 
if os . path . isfile ( os . path . normpath ( file_dir + partial_filename + ext ) ) : \n 
~~~ filename = "_" + filename + ext \n 
~~ ~~ if chosen_setup . index and os . path . isfile ( os . path . normpath ( file_dir + "/" + filename + "/index" + ext ) ) : \n 
~~~ filename += "/index" + ext \n 
~~ ~~ ~~ try : \n 
~~~ f = open ( os . path . normpath ( file_dir + + filename ) , ) \n 
contents = f . read ( ) \n 
f . close ( ) \n 
m = re . findall ( compiled_regex , contents ) \n 
imported_vars = imported_vars + m \n 
~~~ print ( + filename ) \n 
~~ ~~ imported_vars = [ list ( item ) for item in imported_vars ] \n 
~~ self . variables = [ ] \n 
vars_from_views = [ ] \n 
if read_all_views : \n 
~~~ for view in self . view . window ( ) . views ( ) : \n 
~~~ viewfn = self . view . file_name ( ) . encode ( "utf-8" ) \n 
compatible_view = False \n 
~~~ if viewfn . endswith ( ext ) : \n 
~~~ viewvars = [ ] \n 
view . find_all ( chosen_setup . regex , 0 , "$1|$2" , viewvars ) \n 
vars_from_views += viewvars \n 
break ; \n 
~~ ~~ ~~ ~~ else : \n 
~~~ self . view . find_all ( chosen_setup . regex , 0 , "$1|$2" , self . variables ) \n 
~~ self . variables += vars_from_views \n 
self . variables = list ( set ( self . variables ) ) \n 
for i , val in enumerate ( self . variables ) : \n 
~~~ self . variables [ i ] = val . split ( "|" ) \n 
~~ self . variables = imported_vars + self . variables \n 
self . variables . sort ( ) \n 
self . view . window ( ) . show_quick_panel ( self . variables , self . insert_variable , sublime . MONOSPACE_FONT ) \n 
~~ def insert_variable ( self , choice ) : \n 
~~~ if choice == - 1 : \n 
~~ self . view . run_command ( , { : self . variables [ choice ] [ 0 ] } ) \n 
~~ ~~ class InsertText ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit , string = ) : \n 
~~~ for selection in self . view . sel ( ) : \n 
~~~ self . view . insert ( edit , selection . begin ( ) , string ) \n 
~~ ~~ ~~ from driver_base import DriverBase \n 
os . sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import log \n 
class CMDTYPE : \n 
PIXEL_DATA = 2 \n 
BRIGHTNESS = 3 \n 
~~ class RETURN_CODES : \n 
~~ class DriverNetwork ( DriverBase ) : \n 
def __init__ ( self , num = 0 , width = 0 , height = 0 , host = "localhost" , port = 3142 ) : \n 
~~~ super ( DriverNetwork , self ) . __init__ ( num , width , height ) \n 
self . _host = host \n 
self . _port = port \n 
~~ def _generateHeader ( self , cmd , size ) : \n 
~~~ packet = bytearray ( ) \n 
packet . append ( cmd ) \n 
packet . append ( size & 0xFF ) \n 
packet . append ( size >> 8 ) \n 
return packet \n 
~~ def _connect ( self ) : \n 
~~~ s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
s . connect ( ( self . _host , self . _port ) ) \n 
return s \n 
~~ except socket . gaierror : \n 
self . _host ) \n 
log . error ( error ) \n 
raise IOError ( error ) \n 
~~ ~~ def update ( self , data ) : \n 
~~~ s = self . _connect ( ) \n 
count = self . bufByteCount \n 
packet = self . _generateHeader ( CMDTYPE . PIXEL_DATA , count ) \n 
packet . extend ( data ) \n 
s . sendall ( packet ) \n 
resp = ord ( s . recv ( 1 ) ) \n 
s . close ( ) \n 
if resp != RETURN_CODES . SUCCESS : \n 
~~~ log . exception ( e ) \n 
~~ ~~ def setMasterBrightness ( self , brightness ) : \n 
~~~ packet = self . _generateHeader ( CMDTYPE . BRIGHTNESS , 1 ) \n 
packet . append ( brightness ) \n 
s = self . _connect ( ) \n 
~~ ~~ ~~ MANIFEST = [ \n 
{ \n 
"id" : "network" , \n 
"class" : DriverNetwork , \n 
"type" : "driver" , \n 
"display" : "Network" , \n 
"params" : [ { \n 
"id" : "num" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
} , { \n 
"id" : "width" , \n 
"label" : "Width" , \n 
"id" : "height" , \n 
"label" : "Height" , \n 
"id" : "host" , \n 
"type" : "str" , \n 
"default" : "localhost" , \n 
"id" : "port" , \n 
"label" : "Port" , \n 
"default" : 3142 , \n 
} ] \n 
} \n 
_ID = \n 
API = \n 
API_CALL_TIMEOUT = \n 
API_VERSION = \n 
API_VERSION_MAXIMUM = \n 
API_VERSION_MINIMUM = \n 
AREA = \n 
AREA_MAX = \n 
BBOX = \n 
BOUNDS = \n 
CFGSLAB = \n 
CFGVERSION = 1 \n 
CHANGESET = \n 
CHANGESETS = \n 
CHANGESETS_INLINE_SIZE = \n 
CHANGESETS_PER_SLAB = \n 
CHANGESETS_MAX = \n 
CONFIGURATION_SCHEMA_VERSION = \n 
CONTENT_TYPE = \n 
COUCHDB = \n 
DATASTORE = \n 
DATASTORE_BACKEND = \n 
DATASTORE_CONFIG = \n 
DATASTORE_ENCODING = \n 
DBHOST = \n 
DBJOB_ADDELEM = \n 
DBJOB_QUIT = \n 
DBNAME = \n 
DBPORT = \n 
DBURL = \n 
DEFAULT = \n 
ELEMENT = \n 
FRONT_END = \n 
GENERATOR = \n 
GEODOC = \n 
GEODOC_LRU_SIZE = \n 
GEODOC_LRU_THREADS = \n 
GEOHASH_LENGTH = \n 
ID = \n 
JSON = \n 
K = \n 
LAT = \n 
LAT_MAX = + 90.0 \n 
LAT_MIN = - 90.0 \n 
LON = \n 
LON_MAX = + 180.0 \n 
LON_MIN = - 180.0 \n 
MAXIMUM = \n 
MAXIMUM_ELEMENTS = \n 
MAXGHLAT = 89.999999999999992 \n 
MAXLAT = \n 
MAXLON = \n 
MEMBASE = \n 
MEMBASE_MAX_VALUE_LENGTH = 20 * 1024 * 1024 \n 
MEMBER = \n 
MEMBERS = \n 
MINIMUM = \n 
MINLAT = \n 
MINLON = \n 
ND = \n 
NODE = \n 
NODES = \n 
NODES_INLINE_SIZE = \n 
NODES_PER_SLAB = \n 
OSM = \n 
PER_PAGE = \n 
PORT = \n 
PROJECT_DOC = \n 
PROTOBUF = \n 
REF = \n 
REFERENCES = \n 
RELATION = \n 
RELATIONS = \n 
RELATIONS_INLINE_SIZE = \n 
RELATIONS_PER_SLAB = \n 
ROLE = \n 
SCALE_FACTOR = \n 
SECONDS = \n 
SERVER_NAME = \n 
SERVER_VERSION = \n 
SLAB_LRU_SIZE = \n 
SLAB_LRU_THREADS = \n 
SOURCE_REPOSITORY = \n 
STATUS = \n 
TAG = \n 
TAGS = \n 
TEXT_XML = \n 
TIMEOUT = \n 
TRACEPOINTS = \n 
TRACEPOINTS_PER_PAGE = \n 
TYPE = \n 
UTF8 = \n 
V = \n 
VERSION = \n 
WAY = \n 
WAYS = \n 
WAYS_INLINE_SIZE = \n 
WAYS_PER_SLAB = \n 
WAYNODES = \n 
WAYNODES_MAX = \n 
import webapp2 \n 
from urllib import urlencode \n 
import json , urllib2 \n 
from secret import client_id , client_secret \n 
import config \n 
class AuthRedirector ( webapp2 . RequestHandler ) : \n 
~~~ def get ( self ) : \n 
~~~ args = self . request . GET \n 
args [ "client_id" ] = client_id \n 
args [ "redirect_uri" ] = config . auth_redir_uri \n 
url = "https://accounts.google.com/o/oauth2/auth?" + urlencode ( args ) \n 
self . response . location = url \n 
self . response . status_int = 302 \n 
~~ ~~ def query_json ( url , data ) : \n 
if not ( data is str ) : \n 
~~~ data = urlencode ( data ) \n 
~~~ return json . loads ( urllib2 . urlopen ( url , data ) . read ( ) ) \n 
~~~ return json . loads ( e . read ( ) ) \n 
~~ ~~ def json_compactify ( data ) : \n 
~~ class AuthCallback ( webapp2 . RequestHandler ) : \n 
def get ( self ) : \n 
~~~ state = self . request . get ( "state" ) \n 
code = self . request . get ( "code" ) \n 
error = self . request . get ( "error" ) \n 
q = { \n 
"code" : code , \n 
"client_id" : client_id , \n 
"client_secret" : client_secret , \n 
"redirect_uri" : config . auth_redir_uri , \n 
"grant_type" : "authorization_code" , \n 
result = query_json ( "https://accounts.google.com/o/oauth2/token" , q ) \n 
url = ( config . auth_success_page if "access_token" in result \n 
else config . auth_failure_page ) + "#" + urlencode ( result ) \n 
~~ ~~ class AuthRefresh ( webapp2 . RequestHandler ) : \n 
~~~ refresh_token = self . request . get ( "refresh_token" ) \n 
if not refresh_token : \n 
~~~ self . response . status_int = 400 \n 
~~ q = { \n 
"refresh_token" : refresh_token , \n 
"grant_type" : "refresh_token" , \n 
self . response . write ( json_compactify ( result ) ) \n 
~~ ~~ application = webapp2 . WSGIApplication ( [ \n 
( , AuthRedirector ) , \n 
( , AuthCallback ) , \n 
( , AuthRefresh ) , \n 
] , debug = True ) \n 
from django . db import migrations , models \n 
class Migration ( migrations . Migration ) : \n 
~~~ dependencies = [ \n 
( , ) , \n 
operations = [ \n 
migrations . AddField ( \n 
model_name = , \n 
name = , \n 
field = models . EmailField ( help_text = , max_length = 75 , null = True , verbose_name = , blank = True ) , \n 
preserve_default = True , \n 
) , \n 
import string \n 
from jsbeautifier . unpackers import UnpackingError \n 
PRIORITY = 1 \n 
def detect ( source ) : \n 
return source . replace ( , ) . startswith ( ) \n 
~~ def unpack ( source ) : \n 
payload , symtab , radix , count = _filterargs ( source ) \n 
if count != len ( symtab ) : \n 
~~~ raise UnpackingError ( ) \n 
~~~ unbase = Unbaser ( radix ) \n 
~~ except TypeError : \n 
~~ def lookup ( match ) : \n 
word = match . group ( 0 ) \n 
return symtab [ unbase ( word ) ] or word \n 
~~ source = re . sub ( , lookup , payload ) \n 
return _replacestrings ( source ) \n 
~~ def _filterargs ( source ) : \n 
args = re . search ( argsregex , source , re . DOTALL ) . groups ( ) \n 
~~~ return args [ 0 ] , args [ 3 ] . split ( ) , int ( args [ 1 ] ) , int ( args [ 2 ] ) \n 
~~ except ValueError : \n 
~~ ~~ def _replacestrings ( source ) : \n 
if match : \n 
~~~ varname , strings = match . groups ( ) \n 
startpoint = len ( match . group ( 0 ) ) \n 
lookup = strings . split ( \'","\' ) \n 
variable = % varname \n 
for index , value in enumerate ( lookup ) : \n 
~~~ source = source . replace ( variable % index , \'"%s"\' % value ) \n 
~~ return source [ startpoint : ] \n 
~~ return source \n 
~~ class Unbaser ( object ) : \n 
ALPHABET = { \n 
62 : , \n 
def __init__ ( self , base ) : \n 
~~~ self . base = base \n 
if 2 <= base <= 36 : \n 
~~~ self . unbase = lambda string : int ( string , base ) \n 
~~~ self . dictionary = dict ( ( cipher , index ) for \n 
index , cipher in enumerate ( self . ALPHABET [ base ] ) ) \n 
~~ except KeyError : \n 
~~~ raise TypeError ( ) \n 
~~ self . unbase = self . _dictunbaser \n 
~~ ~~ def __call__ ( self , string ) : \n 
~~~ return self . unbase ( string ) \n 
~~ def _dictunbaser ( self , string ) : \n 
ret = 0 \n 
for index , cipher in enumerate ( string [ : : - 1 ] ) : \n 
~~~ ret += ( self . base ** index ) * self . dictionary [ cipher ] \n 
~~ return ret \n 
~~ ~~ import os , sys \n 
parentdir = os . path . dirname ( __file__ ) \n 
sys . path . insert ( 0 , parentdir ) \n 
import executemechanize \n 
class redirection : \n 
~~~ def createarray ( self ) : \n 
~~~ setattr ( self , "redirection_list" , [ ] ) \n 
~~ def appendurl ( self , url ) : \n 
~~~ url = str ( url ) \n 
if not url . endswith ( ".js" ) or url . endswith ( ".json" ) : \n 
~~~ self . redirection_list . append ( url ) ; \n 
self . passarray ( ) \n 
~~ ~~ def passarray ( self ) : \n 
~~~ executemechanize . set_redirection_list ( self . redirection_list ) \n 
~~ ~~ SQL_PORT = 15000 \n 
ZMQ_RPC_PORT = 15598 \n 
HTTP_PORT = 15597 \n 
HTTPS_PORT = 443 \n 
ZMQ_PUBSUB_PORT = 15596 \n 
__author__ = \n 
__maintainer__ = \n 
__email__ = \n 
from . . config import PATHS \n 
from . . entity import Entity \n 
class StrategyConcept ( Entity ) : \n 
collection = \n 
resource = \n 
_relations = { \n 
, \n 
_pull = { \n 
: int , \n 
: Entity . _strpt , \n 
: Entity . _int_to_bool , \n 
_push = _pull . copy ( ) \n 
_push . update ( { \n 
} ) \n 
_readonly = Entity . _readonly | { , } \n 
def __init__ ( self , session , properties = None , ** kwargs ) : \n 
~~~ super ( StrategyConcept , self ) . __init__ ( session , properties , ** kwargs ) \n 
~~ def remove ( self ) : \n 
url = . join ( [ self . collection , \n 
str ( self . id ) , \n 
] ) \n 
self . _post ( PATHS [ ] , rest = url , data = { : self . version } ) \n 
for item in list ( self . properties . keys ( ) ) : \n 
~~~ del self . properties [ item ] \n 
~~ ~~ ~~ from __future__ import print_function \n 
import responses \n 
import requests \n 
from . requests_patch import patched_extract_cookies_to_jar \n 
from terminalone import T1 \n 
mock_credentials = { \n 
API_BASE = \n 
requests . sessions . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
requests . adapters . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
class TestPermissions ( unittest . TestCase ) : \n 
~~~ def setup ( self ) : \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . POST , , \n 
body = fixture , \n 
adding_headers = { \n 
} , \n 
content_type = ) \n 
self . t1 = T1 ( auth_method = , \n 
api_base = API_BASE , \n 
** mock_credentials ) \n 
~~ @ responses . activate \n 
def test_get_permissions ( self ) : \n 
~~~ self . setup ( ) \n 
~~ responses . add ( responses . GET , \n 
content_type = , \n 
match_querystring = True ) \n 
p = self . t1 . get ( , 10000 , child = ) \n 
assert p . _type == , . format ( p . _type ) \n 
assert p . parent_id == 10000 , . format ( p . parent_id ) \n 
def test_remove_advertiser ( self ) : \n 
remove_id = 6 \n 
assert remove_id in p . advertiser . keys ( ) , . format ( remove_id ) \n 
p . remove ( , 6 ) \n 
assert remove_id not in p . advertiser . keys ( ) , . format ( remove_id ) \n 
def test_it_should_remove_child_advertisers_when_removing_agency ( self ) : \n 
remove_ids = [ 6 , 7 ] \n 
for ad_id in remove_ids : \n 
~~~ assert ad_id in p . advertiser . keys ( ) , . format ( ad_id ) \n 
~~ p . remove ( , 3 ) \n 
~~~ assert ad_id not in p . advertiser . keys ( ) , . format ( ad_id ) \n 
~~ ~~ @ responses . activate \n 
def test_it_should_remove_child_agencies_and_advertisers_when_removing_organization ( self ) : \n 
remove_advertiser_ids = [ 8 , 9 , 10 ] \n 
remove_agency_ids = [ 4 , 5 ] \n 
for advertiser_id in remove_advertiser_ids : \n 
~~~ assert advertiser_id in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
~~ for agency_id in remove_agency_ids : \n 
~~~ assert agency_id in p . agency . keys ( ) , . format ( agency_id ) \n 
~~ p . remove ( , 2 ) \n 
~~~ assert advertiser_id not in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
~~~ assert agency_id not in p . agency . keys ( ) , . format ( agency_id ) \n 
def test_it_should_add_entity_ids_on_save ( self ) : \n 
p . add ( , 10 ) \n 
data = p . _generate_save_data ( ) \n 
assert sorted ( data [ ] ) == [ 1 , 2 , 10 ] , data [ ] \n 
def test_it_should_add_access_to_empty_permissions ( self ) : \n 
assert sorted ( data [ ] ) == [ 10 ] , data [ ] \n 
~~ ~~ VERSION = ( 0 , 1 , 9 ) \n 
__version__ = "0.1.9" \n 
import sys as _sys \n 
from operator import itemgetter as _itemgetter \n 
from keyword import iskeyword as _iskeyword \n 
from collections import OrderedDict \n 
################################################################################ \n 
class tagtuple ( tuple ) : \n 
__slots__ = ( ) \n 
def __new__ ( cls , * args ) : \n 
return super ( tagtuple , cls ) . __new__ ( cls , args ) \n 
~~ def __repr__ ( self ) : \n 
return type ( self ) . __name__ + super ( tagtuple , self ) . __repr__ ( ) \n 
~~ def __getnewargs__ ( self ) : \n 
return tuple ( self ) \n 
~~ def __eq__ ( self , other ) : \n 
~~~ return type ( self ) is type ( other ) and super ( tagtuple , self ) . __eq__ ( other ) \n 
~~ def __ne__ ( self , other ) : \n 
~~~ return not self . __eq__ ( other ) \n 
~~ def __getslice__ ( self , i , j ) : \n 
~~~ return type ( self ) ( * super ( tagtuple , self ) . __getslice__ ( i , j ) ) \n 
~~ __add__ = property ( ) \n 
__mul__ = property ( ) \n 
__rmul__ = property ( ) \n 
count = property ( ) \n 
index = property ( ) \n 
~~ _class_template = \n 
_repr_template = \n 
_field_template = \n 
def rectuple ( typename , field_names , verbose = False , rename = False ) : \n 
if isinstance ( field_names , basestring ) : \n 
~~~ field_names = field_names . replace ( , ) . split ( ) \n 
~~ field_names = map ( str , field_names ) \n 
if rename : \n 
~~~ seen = set ( ) \n 
for index , name in enumerate ( field_names ) : \n 
~~~ if ( not all ( c . isalnum ( ) or c == for c in name ) \n 
or _iskeyword ( name ) \n 
or not name \n 
or name [ 0 ] . isdigit ( ) \n 
or name . startswith ( ) \n 
or name in seen ) : \n 
~~~ field_names [ index ] = % index \n 
~~ seen . add ( name ) \n 
~~ ~~ for name in [ typename ] + field_names : \n 
~~~ if not all ( c . isalnum ( ) or c == for c in name ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if _iskeyword ( name ) : \n 
~~ if name [ 0 ] . isdigit ( ) : \n 
~~ ~~ seen = set ( ) \n 
for name in field_names : \n 
~~~ if name . startswith ( ) and not rename : \n 
~~ if name in seen : \n 
~~~ raise ValueError ( % name ) \n 
~~ class_definition = _class_template . format ( \n 
typename = typename , \n 
field_names = tuple ( field_names ) , \n 
num_fields = len ( field_names ) , \n 
arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n 
repr_fmt = . join ( _repr_template . format ( name = name ) \n 
for name in field_names ) , \n 
field_defs = . join ( _field_template . format ( index = index , name = name ) \n 
for index , name in enumerate ( field_names ) ) \n 
if verbose : \n 
~~~ print class_definition \n 
~~ namespace = dict ( _itemgetter = _itemgetter , __name__ = % typename , \n 
OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n 
~~~ exec class_definition in namespace \n 
~~ except SyntaxError as e : \n 
~~~ raise SyntaxError ( e . message + + class_definition ) \n 
~~ result = namespace [ typename ] \n 
~~~ result . __module__ = _sys . _getframe ( 1 ) . f_globals . get ( , ) \n 
~~ except ( AttributeError , ValueError ) : \n 
~~~ import pickle \n 
from itertools import chain , product \n 
print \n 
class A ( tagtuple ) : \n 
~~~ __slots__ = ( ) \n 
~~ class B ( tagtuple ) : \n 
~~ a = A ( 1 , 2 , 3 ) \n 
b = B ( 1 , 2 , 3 ) \n 
t = ( 1 , 2 , 3 ) \n 
d = { } \n 
d [ a ] = 1 \n 
d [ b ] = 2 \n 
d [ t ] = 3 \n 
s = set ( ) \n 
s . add ( a ) \n 
s . add ( b ) \n 
s . add ( t ) \n 
a0 = pickle . loads ( pickle . dumps ( a , 0 ) ) \n 
a1 = pickle . loads ( pickle . dumps ( a , 1 ) ) \n 
a2 = pickle . loads ( pickle . dumps ( a , 2 ) ) \n 
A = rectuple ( , , verbose = True ) \n 
B = rectuple ( , , verbose = True ) \n 
a = A ( 1 , 2 ) \n 
b = B ( 1 , 2 ) \n 
t = ( 1 , 2 ) \n 
~~ import math \n 
def distance ( pa , pb ) : \n 
~~~ ax , ay = pa \n 
bx , by = pb \n 
return math . sqrt ( ( ax - bx ) ** 2 + ( ay - by ) ** 2 ) \n 
~~ def index_of_nearest ( p , hot_points , distance_f = distance ) : \n 
min_dist = None \n 
nearest_hp_i = None \n 
for i , hp in enumerate ( hot_points ) : \n 
~~~ dist = distance_f ( p , hp ) \n 
if min_dist is None or dist < min_dist : \n 
~~~ min_dist = dist \n 
nearest_hp_i = i \n 
~~ ~~ return nearest_hp_i \n 
~~ from fabric import main as fab_main \n 
from cloudferry import fabfile \n 
~~~ fab = fabfile . __file__ \n 
if fab . endswith ( ) : \n 
~~~ fab = fab [ : - 1 ] \n 
~~ fab_main . main ( [ fab ] ) \n 
~~ from cloudferry . lib . base . action import action \n 
DEFAULT = 0 \n 
PATH_ONE = 1 \n 
PATH_TWO = 2 \n 
class IsOption ( action . Action ) : \n 
~~~ def __init__ ( self , init , option_name ) : \n 
~~~ self . option_name = option_name \n 
super ( IsOption , self ) . __init__ ( init ) \n 
~~ def run ( self , ** kwargs ) : \n 
option_value = self . cfg . migrate [ self . option_name ] \n 
if option_value : \n 
~~~ self . set_next_path ( PATH_ONE ) \n 
~~~ self . set_next_path ( PATH_TWO ) \n 
~~ return { } \n 
~~ ~~ from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import log \n 
from cloudferry . lib . utils import utils as utl \n 
LOG = log . getLogger ( __name__ ) \n 
class CheckConfigQuotaNeutron ( action . Action ) : \n 
def run ( self , ** kwargs ) : \n 
~~~ src_cloud = self . src_cloud \n 
dst_cloud = self . dst_cloud \n 
network_src = src_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
identity_dst = dst_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
network_dst = dst_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
search_opts_tenant = kwargs . get ( , { } ) \n 
tenants_src = self . get_src_tenants ( search_opts_tenant ) \n 
list_quotas = network_src . list_quotas ( ) \n 
tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n 
list_quotas ) \n 
if not tenants_without_quotas : \n 
quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n 
quot_default_dst = network_dst . show_quota ( dst_temp_tenant . id ) \n 
is_configs_different = False \n 
identity_dst . delete_tenant ( dst_temp_tenant ) \n 
for item_quot , val_quot in quot . iteritems ( ) : \n 
~~~ if val_quot != quot_default_dst [ item_quot ] : \n 
~~~ is_configs_different = True \n 
quot_default_dst [ item_quot ] ) \n 
~~ ~~ if not is_configs_different : \n 
def get_tenants_without_quotas ( tenants_src , list_quotas ) : \n 
~~~ tenants_ids = tenants_src . keys ( ) \n 
quotas_ids_tenants = [ quota [ "tenant_id" ] for quota in list_quotas ] \n 
return list ( set ( tenants_ids ) - set ( quotas_ids_tenants ) ) \n 
~~ def get_src_tenants ( self , search_opts ) : \n 
~~~ identity_src = self . src_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
if search_opts . get ( ) : \n 
~~~ filter_tenants_ids_list = search_opts [ ] \n 
tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n 
tnt_id in filter_tenants_ids_list ] \n 
~~~ tenants = identity_src . get_tenants_list ( ) \n 
~~ tenants_dict = { tenant . id : tenant . name for tenant in tenants } \n 
return tenants_dict \n 
~~ ~~ import copy \n 
from oslo_config import cfg \n 
from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import utils \n 
LOG = logging . getLogger ( __name__ ) \n 
class DetachVolumesCompute ( action . Action ) : \n 
~~~ def run ( self , info , ** kwargs ) : \n 
~~~ info = copy . deepcopy ( info ) \n 
compute_resource = self . cloud . resources [ utils . COMPUTE_RESOURCE ] \n 
storage_resource = self . cloud . resources [ utils . STORAGE_RESOURCE ] \n 
for instance in info [ utils . INSTANCES_TYPE ] . itervalues ( ) : \n 
instance [ ] [ ] , instance [ ] [ ] ) \n 
if not instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~ for vol in instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~~ volume_status = storage_resource . get_status ( vol [ ] ) \n 
vol [ ] , volume_status ) \n 
if volume_status == : \n 
~~~ compute_resource . detach_volume ( instance [ ] [ ] , \n 
vol [ ] ) \n 
timeout = CONF . migrate . storage_backend_timeout \n 
storage_resource . wait_for_status ( \n 
vol [ ] , storage_resource . get_status , , \n 
timeout = timeout ) \n 
~~ ~~ ~~ return { } \n 
from cloudferry . lib . utils . ssh_util import SshUtil \n 
class RemoteExecution ( action . Action ) : \n 
~~~ def __init__ ( self , cloud , host = None , int_host = None , config_migrate = None ) : \n 
~~~ self . cloud = cloud \n 
self . int_host = int_host \n 
self . config_migrate = config_migrate \n 
self . remote_exec_obj = SshUtil ( self . cloud , \n 
self . config_migrate , \n 
self . host ) \n 
super ( RemoteExecution , self ) . __init__ ( { } ) \n 
~~ def run ( self , command , ** kwargs ) : \n 
~~~ self . remote_exec_obj . execute ( command , self . int_host ) \n 
return { } \n 
~~ ~~ import json \n 
from xml . etree import ElementTree \n 
nova_instances_path = "/var/lib/nova/instances/" \n 
def instance_path ( instance_id ) : \n 
~~~ return os . path . join ( nova_instances_path , instance_id ) \n 
~~ def instance_image_path ( instance_id ) : \n 
~~~ return os . path . join ( instance_path ( instance_id ) , "disk" ) \n 
~~ def _qemu_img_rebase ( src , dst ) : \n 
~~ class QemuBackingFileMover ( object ) : \n 
~~~ def __init__ ( self , runner , src , instance_id ) : \n 
~~~ self . runner = runner \n 
self . src = src \n 
self . dst = instance_image_path ( instance_id ) \n 
~~ def __enter__ ( self ) : \n 
~~~ cmd = _qemu_img_rebase ( self . src , self . dst ) \n 
self . runner . run ( cmd ) \n 
return self \n 
~~ def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n 
~~~ cmd = _qemu_img_rebase ( self . dst , self . src ) \n 
self . runner . run_ignoring_errors ( cmd ) \n 
~~ ~~ class DestNovaInstanceDestroyer ( object ) : \n 
def __init__ ( self , dest_libvirt , dest_nova , libvirt_name , nova_vm_id ) : \n 
~~~ self . dest_libvirt = dest_libvirt \n 
self . dest_nova = dest_nova \n 
self . libvirt_name = libvirt_name \n 
self . nova_vm_id = nova_vm_id \n 
~~~ self . do ( ) \n 
~~~ self . undo ( ) \n 
~~ def do ( self ) : \n 
~~~ self . dest_libvirt . destroy_vm ( self . libvirt_name ) \n 
~~ def undo ( self ) : \n 
self . dest_nova . reset_state ( self . nova_vm_id ) \n 
self . dest_nova . delete_vm_by_id ( self . nova_vm_id ) \n 
~~ except RuntimeError : \n 
~~ ~~ ~~ class Libvirt ( object ) : \n 
~~~ def __init__ ( self , remote_runner ) : \n 
self . runner = remote_runner \n 
~~ def get_backing_file ( self , instance_id ) : \n 
image_path = instance_image_path ( instance_id ) ) ) \n 
~~~ image_info = json . loads ( self . runner . run ( cmd ) ) \n 
return image_info [ ] \n 
~~ except ( ValueError , TypeError ) as e : \n 
instance_id ) \n 
~~ ~~ def get_xml ( self , libvirt_instance_name ) : \n 
inst_name = libvirt_instance_name ) ) \n 
return LibvirtXml ( self . runner . run ( cmd ) ) \n 
~~ def destroy_vm ( self , libvirt_instance_name ) : \n 
~~~ cmds = [ \n 
for cmd in cmds : \n 
~~~ self . runner . run ( cmd ) \n 
~~ ~~ def move_backing_file ( self , source_file , instance_id ) : \n 
~~~ cmd = _qemu_img_rebase ( src = source_file , \n 
dst = instance_image_path ( instance_id ) ) \n 
~~ def live_migrate ( self , libvirt_instance_name , dest_host , migration_xml ) : \n 
dst_host = dest_host , \n 
migration_xml = migration_xml ) ) \n 
~~ ~~ class LibvirtDeviceInterfaceHwAddress ( object ) : \n 
~~~ def __init__ ( self , element ) : \n 
~~~ self . type = element . get ( ) \n 
self . domain = element . get ( ) \n 
self . bus = element . get ( ) \n 
self . slot = element . get ( ) \n 
self . function = element . get ( ) \n 
~~~ return ( isinstance ( other , self . __class__ ) and \n 
self . type == other . type and \n 
self . domain == other . domain and \n 
self . bus == other . bus and \n 
self . slot == other . slot and \n 
self . function == other . function ) \n 
~~ ~~ class LibvirtDeviceInterface ( object ) : \n 
~~~ def __init__ ( self , interface ) : \n 
self . _xml_element = interface \n 
self . mac = interface . find ( ) . get ( ) \n 
self . source_iface = interface . find ( ) . get ( ) \n 
self . target_iface = interface . find ( ) . get ( ) \n 
self . hw_address = LibvirtDeviceInterfaceHwAddress ( \n 
interface . find ( ) ) \n 
self . source_iface == other . source_iface and \n 
self . target_iface == other . target_iface and \n 
self . hw_address == other . hw_address ) \n 
mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n 
def _replace_attr ( cls , element , attr , value ) : \n 
~~~ if element . get ( attr ) != value : \n 
~~~ element . clear ( ) \n 
element . attrib = { attr : value } \n 
~~ ~~ def element ( self ) : \n 
~~~ source = self . _xml_element . find ( ) \n 
target = self . _xml_element . find ( ) \n 
self . _replace_attr ( source , , self . source_iface ) \n 
self . _replace_attr ( target , , self . target_iface ) \n 
return self . _xml_element \n 
~~ ~~ class LibvirtXml ( object ) : \n 
~~~ def __init__ ( self , contents ) : \n 
self . _xml = ElementTree . fromstring ( contents ) \n 
self . _interfaces = [ LibvirtDeviceInterface ( i ) \n 
for i in self . _xml . findall ( ) ] \n 
self . disk_file = self . _get ( , ) \n 
self . serial_file = self . _get ( , ) \n 
self . console_file = self . _get ( , ) \n 
~~ def _get ( self , element , attribute ) : \n 
~~~ el = self . _xml . find ( element ) \n 
if el is not None : \n 
~~~ return el . get ( attribute ) \n 
~~ ~~ def _set ( self , element , attribute , value ) : \n 
~~~ el . set ( attribute , value ) \n 
def interfaces ( self ) : \n 
~~~ return self . _interfaces \n 
~~ @ interfaces . setter \n 
def interfaces ( self , other ) : \n 
if len ( self . interfaces ) != len ( other ) : \n 
~~ for other_iface in other : \n 
~~~ for this_iface in self . interfaces : \n 
~~~ identical = ( this_iface . mac == other_iface . mac ) \n 
if identical : \n 
~~~ this_iface . source_iface = other_iface . source_iface \n 
this_iface . target_iface = other_iface . target_iface \n 
~~ ~~ ~~ ~~ def dump ( self ) : \n 
~~~ self . _set ( , , self . disk_file ) \n 
self . _set ( , , self . serial_file ) \n 
self . _set ( , , self . console_file ) \n 
xml_devices = self . _xml . find ( ) \n 
xml_interfaces = self . _xml . findall ( ) \n 
for iface in xml_interfaces : \n 
~~~ xml_devices . remove ( iface ) \n 
~~ for iface in self . _interfaces : \n 
~~~ xml_devices . append ( iface . element ( ) ) \n 
~~ return ElementTree . tostring ( self . _xml ) \n 
~~ ~~ import abc \n 
from cloudferry . lib . utils import files \n 
from cloudferry . lib . utils import remote_runner \n 
from cloudferry . lib . copy_engines import base \n 
class CopyFailed ( RuntimeError ) : \n 
~~ class CopyMechanism ( object ) : \n 
~~~ __metaclass__ = abc . ABCMeta \n 
@ abc . abstractmethod \n 
def copy ( self , context , source_object , destination_object ) : \n 
~~ ~~ class CopyObject ( object ) : \n 
~~~ def __init__ ( self , host = None , path = None ) : \n 
~~~ self . host = host \n 
~~~ return "{host}:{path}" . format ( host = self . host , path = self . path ) \n 
~~ ~~ class RemoteFileCopy ( CopyMechanism ) : \n 
~~~ data = { \n 
: source_object . host , \n 
: source_object . path , \n 
: destination_object . host , \n 
: destination_object . path \n 
~~~ copier = base . get_copier ( context . src_cloud , \n 
context . dst_cloud , \n 
data ) \n 
copier . transfer ( data ) \n 
~~ except ( base . FileCopyError , \n 
base . CopierCannotBeUsed , \n 
base . CopierNotFound ) as e : \n 
src_host = source_object . host , \n 
src_file = source_object . path , \n 
dst_host = destination_object . host , \n 
dst_file = destination_object . path , \n 
err = e . message ) \n 
raise CopyFailed ( msg ) \n 
~~ ~~ ~~ class CopyRegularFileToBlockDevice ( CopyMechanism ) : \n 
~~~ src_user = context . cfg . src . ssh_user \n 
dst_user = context . cfg . dst . ssh_user \n 
src_host = source_object . host \n 
dst_host = destination_object . host \n 
rr = remote_runner . RemoteRunner ( src_host , src_user ) \n 
ssh_opts = ( \n 
~~~ progress_view = "" \n 
if files . is_installed ( rr , "pv" ) : \n 
~~~ src_file_size = files . remote_file_size ( rr , source_object . path ) \n 
size = src_file_size ) \n 
rr . run ( copy . format ( src_file = source_object . path , \n 
dst_user = dst_user , \n 
dst_host = dst_host , \n 
ssh_opts = ssh_opts , \n 
dst_device = destination_object . path , \n 
progress_view = progress_view ) ) \n 
~~ except remote_runner . RemoteExecutionError as e : \n 
msg = msg . format ( src_object = source_object , \n 
dst_object = destination_object , \n 
error = e . message ) \n 
~~ ~~ ~~ import datetime \n 
from logging import config \n 
from logging import handlers \n 
from fabric import api \n 
import yaml \n 
from cloudferry . lib . utils import sizeof_format \n 
getLogger = logging . getLogger \n 
class StdoutLogger ( object ) : \n 
def __init__ ( self , name = None ) : \n 
~~~ self . log = logging . getLogger ( name or ) \n 
~~ def write ( self , message ) : \n 
~~~ message = message . strip ( ) \n 
if message : \n 
~~~ self . log . info ( message ) \n 
~~ ~~ def flush ( self ) : \n 
~~ ~~ def configure_logging ( log_config = None , debug = None , forward_stdout = None ) : \n 
if log_config is None : \n 
~~~ log_config = CONF . migrate . log_config \n 
~~ if debug is None : \n 
~~~ debug = CONF . migrate . debug \n 
~~ if forward_stdout is None : \n 
~~~ forward_stdout = CONF . migrate . forward_stdout \n 
~~ with open ( log_config , ) as f : \n 
~~~ config . dictConfig ( yaml . load ( f ) ) \n 
~~ if debug : \n 
~~~ logger = logging . getLogger ( ) \n 
for handler in logger . handlers : \n 
~~~ if handler . name == : \n 
~~~ handler . setLevel ( logging . DEBUG ) \n 
~~ ~~ ~~ if forward_stdout : \n 
~~~ sys . stdout = StdoutLogger ( ) \n 
~~ ~~ class RunRotatingFileHandler ( handlers . RotatingFileHandler ) : \n 
filename = , \n 
date_format = , \n 
** kwargs ) : \n 
~~~ self . date_format = date_format \n 
max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n 
super ( RunRotatingFileHandler , self ) . __init__ ( \n 
filename = self . get_filename ( filename ) , \n 
maxBytes = max_bytes , \n 
** kwargs ) \n 
~~ def get_filename ( self , filename ) : \n 
if hasattr ( CONF , ) and hasattr ( CONF . migrate , ) : \n 
~~~ scenario_filename = os . path . basename ( CONF . migrate . scenario ) \n 
scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n 
~~~ scenario = \n 
~~ dt = datetime . datetime . now ( ) . strftime ( self . date_format ) \n 
return filename % { \n 
: scenario , \n 
: dt \n 
~~ ~~ class CurrentTaskFilter ( logging . Filter ) : \n 
def __init__ ( self , name_format = , ** kwargs ) : \n 
~~~ super ( CurrentTaskFilter , self ) . __init__ ( ** kwargs ) \n 
self . name_format = name_format \n 
~~ def filter ( self , record ) : \n 
~~~ current_task = self . name_format % { \n 
: api . env . current_task or , \n 
record . current_task = current_task \n 
import cloudferry_devlab . tests . config as config \n 
from cloudferry_devlab . tests . data_collector import DataCollector \n 
from cloudferry_devlab . tests import functional_test \n 
import cloudferry_devlab . tests . utils as utils \n 
class RollbackVerification ( functional_test . FunctionalTest ) : \n 
~~~ data_collector = DataCollector ( config = config ) \n 
self . data_after = utils . convert ( data_collector . data_collector ( ) ) \n 
file_name = config . rollback_params [ ] [ ] \n 
pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n 
with open ( pre_file_path , "r" ) as f : \n 
~~~ self . pre_data = yaml . load ( f ) \n 
~~ ~~ def test_verify_rollback ( self ) : \n 
self . maxDiff = None \n 
for cloud in self . data_after : \n 
~~~ for service in self . data_after [ cloud ] : \n 
~~~ for resource in self . data_after [ cloud ] [ service ] : \n 
~~~ print ( msg . format ( service . lower ( ) , resource . lower ( ) ) ) \n 
self . assertEqual ( self . data_after [ cloud ] [ service ] [ resource ] , \n 
self . pre_data [ cloud ] [ service ] [ resource ] ) \n 
~~ ~~ ~~ ~~ ~~ import mock \n 
from cloudferry . lib . os . actions import convert_volume_to_image \n 
from tests import test \n 
class ConverterVolumeToImageTest ( test . TestCase ) : \n 
~~~ super ( ConverterVolumeToImageTest , self ) . setUp ( ) \n 
self . fake_src_cloud = mock . Mock ( ) \n 
self . fake_storage = mock . Mock ( ) \n 
self . fake_storage . deploy = mock . Mock ( ) \n 
self . fake_storage . upload_volume_to_image . return_value = ( \n 
, ) \n 
self . fake_storage . get_backend . return_value = \n 
self . fake_image = mock . Mock ( ) \n 
self . fake_image . wait_for_status = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted . return_value = { \n 
: { \n 
: { : , : { } } } } \n 
self . fake_image . patch_image = mock . Mock ( ) \n 
self . fake_src_cloud . resources = { : self . fake_storage , \n 
: self . fake_image } \n 
self . fake_volumes_info = { \n 
} } , \n 
self . fake_dst_cloud = mock . Mock ( ) \n 
self . fake_config = utils . ext_dict ( migrate = utils . ext_dict ( \n 
{ : , \n 
: } ) ) \n 
self . fake_init = { \n 
: self . fake_src_cloud , \n 
: self . fake_dst_cloud , \n 
: self . fake_config \n 
~~ def test_action ( self ) : \n 
~~~ fake_action = convert_volume_to_image . ConvertVolumeToImage ( \n 
self . fake_init , \n 
cloud = ) \n 
res = fake_action . run ( self . fake_volumes_info ) \n 
self . assertEqual ( , \n 
res [ ] [ ] [ ] [ ] ) \n 
res [ ] [ ] [ ] [ ] [ \n 
] [ ] ) \n 
~~ ~~ from cloudferry . lib . utils . cache import Memoized , Cached \n 
class MemoizationTestCase ( test . TestCase ) : \n 
~~~ def test_treats_self_as_separate_objects ( self ) : \n 
~~~ class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
~~ @ Memoized \n 
def get_i ( self ) : \n 
~~~ return self . i \n 
~~ ~~ o1 = C ( 1 ) \n 
o2 = C ( 2 ) \n 
self . assertNotEqual ( o1 . get_i ( ) , o2 . get_i ( ) ) \n 
self . assertEqual ( o1 . get_i ( ) , 1 ) \n 
self . assertEqual ( o2 . get_i ( ) , 2 ) \n 
~~ def test_takes_value_from_cache ( self ) : \n 
~~ def set_i ( self , i ) : \n 
~~ ~~ original = 1 \n 
o = C ( original ) \n 
self . assertEqual ( o . get_i ( ) , original ) \n 
o . set_i ( 10 ) \n 
~~ ~~ class CacheTestCase ( test . TestCase ) : \n 
~~~ def test_resets_cache_when_modifier_called ( self ) : \n 
~~~ @ Cached ( getter = , modifier = ) \n 
class C ( object ) : \n 
~~ def get_i ( self ) : \n 
~~ ~~ o = C ( 1 ) \n 
self . assertEqual ( o . get_i ( ) , 1 ) \n 
o . set_i ( 100 ) \n 
self . assertEqual ( o . get_i ( ) , 100 ) \n 
~~ ~~ from django . http import HttpResponse , HttpResponseRedirect , HttpResponseNotFound \n 
from django . template import Context , loader \n 
from django . core . urlresolvers import reverse \n 
from django . shortcuts import get_object_or_404 , render_to_response \n 
from django . core . exceptions import ObjectDoesNotExist \n 
from tagging . models import Tag , TaggedItem \n 
from django . views . decorators . csrf import csrf_exempt \n 
from django . contrib . auth . models import User \n 
from django . contrib . auth . decorators import login_required \n 
from django . contrib . auth import authenticate , login \n 
from django . core . mail import send_mail \n 
from django . http import Http404 \n 
from django . db . models import Q \n 
from openwatch . recordings . models import Recording \n 
from openwatch import recording_tags \n 
@ login_required \n 
def moderate ( request ) : \n 
response_values = { } \n 
org_tag = request . user . get_profile ( ) . org_tag \n 
if not request . user . is_superuser and ( not request . user . get_profile ( ) . can_moderate or org_tag == ) : \n 
~~~ raise Http404 \n 
~~ if recording_tags . ACLU_NJ in org_tag : \n 
~~~ location = { } \n 
location [ ] = 40.167274 \n 
location [ ] = - 74.616338 \n 
response_values [ ] = location \n 
~~ response_values [ ] = \n 
return render_to_response ( , response_values , context_instance = RequestContext ( request ) ) \n 
~~ def map ( request ) : \n 
~~~ total = "lots!" \n 
return render_to_response ( , { : total } , context_instance = RequestContext ( request ) ) \n 
~~ def size ( request ) : \n 
~~~ featureset = Recording . objects . filter ( ~ Q ( lat = None ) , ~ Q ( lon = None ) , ~ Q ( jtype = ) ) . exclude ( location__exact = ) . exclude ( location__exact = ) . order_by ( ) \n 
total = len ( featureset ) \n 
~~ def redir ( self ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
~~ def map_json ( request ) : \n 
~~~ featureset = Recording . objects . all ( ) . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) [ : 750 ] \n 
resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
~~ @ login_required \n 
def map_json_moderate ( request ) : \n 
~~~ org_tag = request . user . get_profile ( ) . org_tag \n 
if org_tag != : \n 
~~~ featureset = Recording . objects . filter ( org_approved = False , org_flagged = False , tags__contains = org_tag ) \n 
~~~ featureset = Recording . objects . all ( ) \n 
~~ featureset = featureset . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) \n 
~~ def map_location_json ( request , ne_lat = 0 , ne_lon = 0 , sw_lat = 0 , sw_lon = 0 ) : \n 
~~~ ne_lat = float ( ne_lat ) \n 
ne_lon = float ( ne_lon ) \n 
sw_lat = float ( sw_lat ) \n 
sw_lon = float ( sw_lon ) \n 
featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon ) . order_by ( ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) . exclude ( location__exact = ) . exclude ( location__exact = ) [ : 750 ] \n 
if len ( featureset ) < 1 : \n 
~~~ return HttpResponse ( "{\\"objects\\":[]}" , mimetype = "application/json" ) \n 
~~ resp = encode_queryset ( featureset ) \n 
~~ def encode_queryset ( featureset ) : \n 
~~~ resp = \'{"objects":[\' \n 
for obj in featureset : \n 
~~~ resp = resp + json . dumps ( obj . to_dict ( ) ) + \n 
~~ resp = resp [ : - 1 ] + \n 
~~ from django . template import loader , RequestContext \n 
from django . http import HttpResponse , Http404 \n 
from django . http import HttpResponseRedirect , HttpResponsePermanentRedirect \n 
from django . db . models . base import ModelBase \n 
from django . db . models . manager import Manager \n 
from django . db . models . query import QuerySet \n 
from django . core import urlresolvers \n 
from django . utils import six \n 
~~~ import json \n 
def render_to_easy_api_response ( * args , ** kwargs ) : \n 
httpresponse_kwargs = { : kwargs . pop ( , None ) } \n 
context = kwargs . pop ( ) \n 
processors = context . context_processors \n 
request = processors [ ] [ ] \n 
if request . GET . has_key ( ) : \n 
~~~ api_type = request . GET [ ] \n 
for arg in args : \n 
~~~ passed = arg \n 
~~ dump_me = { } \n 
for key in passed . keys ( ) : \n 
~~~ value = passed [ key ] \n 
dump_me [ key ] = dump_object ( value ) \n 
~~ if api_type == : \n 
~~~ def replace_spaces ( dump_me ) : \n 
~~~ new = { } \n 
for k , v in dump_me . iteritems ( ) : \n 
~~~ if isinstance ( v , dict ) : \n 
~~~ v = replace_spaces ( v ) \n 
~~ new [ k . replace ( , ) ] = v \n 
~~ return new \n 
~~ new = replace_spaces ( dump_me ) \n 
dump_me = dict2xml ( new ) \n 
pretty = dom . toprettyxml ( ) \n 
return HttpResponse ( pretty , content_type = ) \n 
~~~ yml = yaml . safe_dump ( dump_me ) \n 
return HttpResponse ( yml , content_type = ) \n 
return HttpResponse ( dump_me , content_type = ) \n 
~~ ~~ return HttpResponse ( loader . render_to_string ( * args , ** kwargs ) , ** httpresponse_kwargs ) \n 
~~ def render_to_response ( * args , ** kwargs ) : \n 
return render_to_easy_api_response ( * args , ** kwargs ) \n 
## \n 
~~ def dump_object ( queryset ) : \n 
~~~ d = DataDumper ( ) \n 
ret = d . dump ( queryset ) \n 
return ret \n 
~~~ modelName = queryset [ 0 ] . __class__ . __name__ \n 
modelNameData = [ ] \n 
fields = get_fields ( queryset [ 0 ] ) \n 
for obj in queryset : \n 
~~~ temp_dict = dict ( ) \n 
for field in fields : \n 
~~~ attribute = getattr ( obj , str ( field ) ) \n 
temp_dict [ field ] = attribute \n 
~~ ~~ modelNameData . append ( temp_dict ) \n 
~~ dthandler = lambda obj : obj . isoformat ( ) if isinstance ( obj , datetime . datetime ) or isinstance ( obj , datetime . date ) else None \n 
return json . loads ( json . dumps ( modelNameData , default = dthandler ) ) \n 
~~ ~~ def get_fields ( model ) : \n 
~~~ if hasattr ( model , "easy_api_fields" ) : \n 
~~~ fields = model . easy_api_fields ( ) \n 
~~~ fields = model . to_dict ( ) . keys ( ) \n 
~~~ fields = model . _meta . get_all_field_names ( ) \n 
~~ ~~ return fields \n 
~~ ~~ class SimpleEngagementCalculator ( object ) : \n 
~~~ def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n 
~~ ~~ ROOT_URLCONF = None \n 
DATABASE_ENGINE = \n 
DATABASE_NAME = \n 
DATABASE_SUPPORTS_TRANSACTIONS = False \n 
INSTALLED_APPS = [ \n 
TEMPLATE_CONTEXT_PROCESSORS = ( \n 
"django.core.context_processors.auth" , \n 
"django.core.context_processors.debug" , \n 
"django.core.context_processors.i18n" , \n 
"django.core.context_processors.media" , \n 
"django.core.context_processors.request" ) \n 
if __name__ == "__main__" : \n 
~~~ os . environ . setdefault ( "DJANGO_SETTINGS_MODULE" , "test_settings" ) \n 
from django . core . management import execute_from_command_line \n 
is_testing = in sys . argv \n 
if is_testing : \n 
~~~ import coverage \n 
cov = coverage . coverage ( include = "django_zappa/*" , omit = [ ] ) \n 
cov . erase ( ) \n 
cov . start ( ) \n 
~~ execute_from_command_line ( sys . argv ) \n 
~~~ cov . stop ( ) \n 
cov . save ( ) \n 
cov . report ( ) \n 
import simplexml , time , sys \n 
from protocol import * \n 
from client import PlugIn \n 
DefaultTimeout = 25 \n 
ID = 0 \n 
class Dispatcher ( PlugIn ) : \n 
~~~ PlugIn . __init__ ( self ) \n 
DBG_LINE = \n 
self . handlers = { } \n 
self . _expected = { } \n 
self . _defaultHandler = None \n 
self . _pendingExceptions = [ ] \n 
self . _eventHandler = None \n 
self . _cycleHandlers = [ ] \n 
self . _exported_methods = [ self . Process , self . RegisterHandler , self . RegisterDefaultHandler , self . RegisterEventHandler , self . UnregisterCycleHandler , self . RegisterCycleHandler , self . RegisterHandlerOnce , self . UnregisterHandler , self . RegisterProtocol , self . WaitForResponse , self . SendAndWaitForResponse , self . send , self . disconnect , self . SendAndCallForResponse , ] \n 
~~ def dumpHandlers ( self ) : \n 
return self . handlers \n 
~~ def restoreHandlers ( self , handlers ) : \n 
self . handlers = handlers \n 
~~ def _init ( self ) : \n 
self . RegisterNamespace ( ) \n 
self . RegisterNamespace ( NS_STREAMS ) \n 
self . RegisterNamespace ( self . _owner . defaultNamespace ) \n 
self . RegisterProtocol ( , Iq ) \n 
self . RegisterProtocol ( , Presence ) \n 
self . RegisterProtocol ( , Message ) \n 
self . RegisterDefaultHandler ( self . returnStanzaHandler ) \n 
self . RegisterHandler ( , self . streamErrorHandler , xmlns = NS_STREAMS ) \n 
~~ def plugin ( self , owner ) : \n 
self . _init ( ) \n 
for method in self . _old_owners_methods : \n 
~~~ if method . __name__ == : self . _owner_send = method ; break \n 
~~ self . _owner . lastErrNode = None \n 
self . _owner . lastErr = None \n 
self . _owner . lastErrCode = None \n 
self . StreamInit ( ) \n 
~~ def plugout ( self ) : \n 
self . Stream . dispatch = None \n 
self . Stream . DEBUG = None \n 
self . Stream . features = None \n 
self . Stream . destroy ( ) \n 
~~ def StreamInit ( self ) : \n 
self . Stream = simplexml . NodeBuilder ( ) \n 
self . Stream . _dispatch_depth = 2 \n 
self . Stream . dispatch = self . dispatch \n 
self . Stream . stream_header_received = self . _check_stream_start \n 
self . _owner . debug_flags . append ( simplexml . DBG_NODEBUILDER ) \n 
self . Stream . DEBUG = self . _owner . DEBUG \n 
self . _metastream = Node ( ) \n 
self . _metastream . setNamespace ( self . _owner . Namespace ) \n 
self . _metastream . setAttr ( , ) \n 
self . _metastream . setAttr ( , NS_STREAMS ) \n 
self . _metastream . setAttr ( , self . _owner . Server ) \n 
~~ def _check_stream_start ( self , ns , tag , attrs ) : \n 
~~~ if ns < > NS_STREAMS or tag < > : \n 
~~~ raise ValueError ( % ( tag , ns ) ) \n 
~~ ~~ def Process ( self , timeout = 0 ) : \n 
for handler in self . _cycleHandlers : handler ( self ) \n 
if len ( self . _pendingExceptions ) > 0 : \n 
~~~ _pendingException = self . _pendingExceptions . pop ( ) \n 
raise _pendingException [ 0 ] , _pendingException [ 1 ] , _pendingException [ 2 ] \n 
~~ if self . _owner . Connection . pending_data ( timeout ) : \n 
~~~ try : data = self . _owner . Connection . receive ( ) \n 
except IOError : return \n 
self . Stream . Parse ( data ) \n 
~~ if data : return len ( data ) \n 
~~ def RegisterNamespace ( self , xmlns , order = ) : \n 
self . handlers [ xmlns ] = { } \n 
self . RegisterProtocol ( , Protocol , xmlns = xmlns ) \n 
~~ def RegisterProtocol ( self , tag_name , Proto , xmlns = None , order = ) : \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . handlers [ xmlns ] [ tag_name ] = { type : Proto , : [ ] } \n 
~~ def RegisterNamespaceHandler ( self , xmlns , handler , typ = , ns = , makefirst = 0 , system = 0 ) : \n 
self . RegisterHandler ( , handler , typ , ns , xmlns , makefirst , system ) \n 
~~ def RegisterHandler ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
if not typ and not ns : typ = \n 
if not self . handlers . has_key ( xmlns ) : self . RegisterNamespace ( xmlns , ) \n 
if not self . handlers [ xmlns ] . has_key ( name ) : self . RegisterProtocol ( name , Protocol , xmlns , ) \n 
if not self . handlers [ xmlns ] [ name ] . has_key ( typ + ns ) : self . handlers [ xmlns ] [ name ] [ typ + ns ] = [ ] \n 
if makefirst : self . handlers [ xmlns ] [ name ] [ typ + ns ] . insert ( 0 , { : handler , : system } ) \n 
else : self . handlers [ xmlns ] [ name ] [ typ + ns ] . append ( { : handler , : system } ) \n 
~~ def RegisterHandlerOnce ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
self . RegisterHandler ( name , handler , typ , ns , xmlns , makefirst , system ) \n 
~~ def UnregisterHandler ( self , name , handler , typ = , ns = , xmlns = None ) : \n 
if not self . handlers . has_key ( xmlns ) : return \n 
for pack in self . handlers [ xmlns ] [ name ] [ typ + ns ] : \n 
~~~ if handler == pack [ ] : break \n 
~~ else : pack = None \n 
try : self . handlers [ xmlns ] [ name ] [ typ + ns ] . remove ( pack ) \n 
except ValueError : pass \n 
~~ def RegisterDefaultHandler ( self , handler ) : \n 
self . _defaultHandler = handler \n 
~~ def RegisterEventHandler ( self , handler ) : \n 
self . _eventHandler = handler \n 
~~ def returnStanzaHandler ( self , conn , stanza ) : \n 
if stanza . getType ( ) in [ , ] : \n 
~~~ conn . send ( Error ( stanza , ERR_FEATURE_NOT_IMPLEMENTED ) ) \n 
~~ ~~ def streamErrorHandler ( self , conn , error ) : \n 
~~~ name , text = , error . getData ( ) \n 
for tag in error . getChildren ( ) : \n 
~~~ if tag . getNamespace ( ) == NS_XMPP_STREAMS : \n 
~~~ if tag . getName ( ) == : text = tag . getData ( ) \n 
else : name = tag . getName ( ) \n 
~~ ~~ if name in stream_exceptions . keys ( ) : exc = stream_exceptions [ name ] \n 
else : exc = StreamError \n 
raise exc ( ( name , text ) ) \n 
~~ def RegisterCycleHandler ( self , handler ) : \n 
if handler not in self . _cycleHandlers : self . _cycleHandlers . append ( handler ) \n 
~~ def UnregisterCycleHandler ( self , handler ) : \n 
if handler in self . _cycleHandlers : self . _cycleHandlers . remove ( handler ) \n 
~~ def Event ( self , realm , event , data ) : \n 
if self . _eventHandler : self . _eventHandler ( realm , event , data ) \n 
~~ def dispatch ( self , stanza , session = None , direct = 0 ) : \n 
if not session : session = self \n 
session . Stream . _mini_dom = None \n 
name = stanza . getName ( ) \n 
if not direct and self . _owner . _route : \n 
~~~ if name == : \n 
~~~ if stanza . getAttr ( ) == None : \n 
~~~ if len ( stanza . getChildren ( ) ) == 1 : \n 
~~~ stanza = stanza . getChildren ( ) [ 0 ] \n 
~~~ for each in stanza . getChildren ( ) : \n 
~~~ self . dispatch ( each , session , direct = 1 ) \n 
~~ return \n 
~~ ~~ ~~ elif name == : \n 
~~ elif name in ( , ) : \n 
~~~ raise UnsupportedStanzaType ( name ) \n 
~~ ~~ if name == : session . Stream . features = stanza \n 
xmlns = stanza . getNamespace ( ) \n 
if not self . handlers . has_key ( xmlns ) : \n 
xmlns = \n 
~~ if not self . handlers [ xmlns ] . has_key ( name ) : \n 
name = \n 
~~ if stanza . __class__ . __name__ == : stanza = self . handlers [ xmlns ] [ name ] [ type ] ( node = stanza ) \n 
typ = stanza . getType ( ) \n 
if not typ : typ = \n 
stanza . props = stanza . getProperties ( ) \n 
ID = stanza . getID ( ) \n 
for prop in stanza . props : \n 
~~~ if self . handlers [ xmlns ] [ name ] . has_key ( prop ) : list . append ( prop ) \n 
~~ chain = self . handlers [ xmlns ] [ ] [ ] \n 
for key in list : \n 
~~~ if key : chain = chain + self . handlers [ xmlns ] [ name ] [ key ] \n 
~~ output = \n 
if session . _expected . has_key ( ID ) : \n 
~~~ user = 0 \n 
if type ( session . _expected [ ID ] ) == type ( ( ) ) : \n 
~~~ cb , args = session . _expected [ ID ] \n 
try : cb ( session , stanza , ** args ) \n 
except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : raise \n 
session . _expected [ ID ] = stanza \n 
~~ ~~ else : user = 1 \n 
for handler in chain : \n 
~~~ if user or handler [ ] : \n 
~~~ handler [ ] ( session , stanza ) \n 
~~ except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : \n 
~~~ self . _pendingExceptions . insert ( 0 , sys . exc_info ( ) ) \n 
~~ user = 0 \n 
~~ ~~ ~~ if user and self . _defaultHandler : self . _defaultHandler ( session , stanza ) \n 
~~ def WaitForResponse ( self , ID , timeout = DefaultTimeout ) : \n 
self . _expected [ ID ] = None \n 
has_timed_out = 0 \n 
abort_time = time . time ( ) + timeout \n 
while not self . _expected [ ID ] : \n 
~~~ if not self . Process ( 0.04 ) : \n 
~~~ self . _owner . lastErr = "Disconnect" \n 
return None \n 
~~ if time . time ( ) > abort_time : \n 
~~~ self . _owner . lastErr = "Timeout" \n 
~~ ~~ response = self . _expected [ ID ] \n 
del self . _expected [ ID ] \n 
if response . getErrorCode ( ) : \n 
~~~ self . _owner . lastErrNode = response \n 
self . _owner . lastErr = response . getError ( ) \n 
self . _owner . lastErrCode = response . getErrorCode ( ) \n 
~~ return response \n 
~~ def SendAndWaitForResponse ( self , stanza , timeout = DefaultTimeout ) : \n 
return self . WaitForResponse ( self . send ( stanza ) , timeout ) \n 
~~ def SendAndCallForResponse ( self , stanza , func , args = { } ) : \n 
self . _expected [ self . send ( stanza ) ] = ( func , args ) \n 
~~ def send ( self , stanza ) : \n 
if type ( stanza ) in [ type ( ) , type ( ) ] : return self . _owner_send ( stanza ) \n 
if not isinstance ( stanza , Protocol ) : _ID = None \n 
elif not stanza . getID ( ) : \n 
~~~ global ID \n 
ID += 1 \n 
_ID = ` ID ` \n 
stanza . setID ( _ID ) \n 
~~ else : _ID = stanza . getID ( ) \n 
if self . _owner . _registered_name and not stanza . getAttr ( ) : stanza . setAttr ( , self . _owner . _registered_name ) \n 
if self . _owner . _route and stanza . getName ( ) != : \n 
~~~ to = self . _owner . Server \n 
if stanza . getTo ( ) and stanza . getTo ( ) . getDomain ( ) : \n 
~~~ to = stanza . getTo ( ) . getDomain ( ) \n 
~~ frm = stanza . getFrom ( ) \n 
if frm . getDomain ( ) : \n 
~~~ frm = frm . getDomain ( ) \n 
~~ route = Protocol ( , to = to , frm = frm , payload = [ stanza ] ) \n 
stanza = route \n 
~~ stanza . setNamespace ( self . _owner . Namespace ) \n 
stanza . setParent ( self . _metastream ) \n 
self . _owner_send ( stanza ) \n 
return _ID \n 
~~ def disconnect ( self ) : \n 
self . _owner_send ( ) \n 
while self . Process ( 1 ) : pass \n 
~~ ~~ from . gl_utils import * \n 
from . texture import VideoTexture \n 
from . widget import Widget , BGUI_DEFAULT , WeakMethod \n 
from . image import Image \n 
class Video ( Image ) : \n 
def __init__ ( self , parent , vid , name = None , play_audio = False , repeat = 0 , aspect = None , size = [ 1 , 1 ] , pos = [ 0 , 0 ] , \n 
sub_theme = , options = BGUI_DEFAULT ) : \n 
Image . __init__ ( self , parent , name , None , aspect , size , pos , sub_theme = sub_theme , options = options ) \n 
self . _texture = VideoTexture ( vid , GL_LINEAR , repeat , play_audio ) \n 
self . _on_finish = None \n 
self . _on_finish_called = False \n 
~~ def play ( self , start , end , use_frames = True , fps = None ) : \n 
~~~ self . _texture . play ( start , end , use_frames , fps ) \n 
def on_finish ( self ) : \n 
return self . _on_finish \n 
~~ @ on_finish . setter \n 
def on_finish ( self , value ) : \n 
~~~ self . _on_finish = WeakMethod ( value ) \n 
~~ def _draw ( self ) : \n 
self . _texture . update ( ) \n 
Image . _draw ( self ) \n 
if self . _texture . video . status == 3 : \n 
~~~ if self . _on_finish and not self . _on_finish_called : \n 
~~~ self . on_finish ( self ) \n 
self . _on_finish_called = Truefrom django import template \n 
~~ ~~ ~~ ~~ from django . conf import settings \n 
class CheckGrappelli ( template . Node ) : \n 
~~~ def __init__ ( self , var_name ) : \n 
~~~ self . var_name = var_name \n 
~~~ context [ self . var_name ] = in settings . INSTALLED_APPS \n 
~~ ~~ def check_grappelli ( parser , token ) : \n 
bits = token . contents . split ( ) \n 
if len ( bits ) != 3 : \n 
~~ if bits [ 1 ] != : \n 
~~ varname = bits [ 2 ] \n 
return CheckGrappelli ( varname ) \n 
~~ register . tag ( check_grappelli ) \n 
from setuptools import setup , find_packages \n 
import sys , os \n 
__description__ = , \n 
__author__ = , \n 
__email__ = , \n 
sys . path . insert ( 0 , os . path . dirname ( __file__ ) ) \n 
REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n 
setup ( \n 
version = __version__ , \n 
url = , \n 
download_url = , \n 
license = __license__ , \n 
author = __author__ , \n 
author_email = __email__ , \n 
description = __description__ , \n 
long_description = __doc__ , \n 
test_suite = , \n 
zip_safe = False , \n 
platforms = , \n 
install_requires = REQUIRES , \n 
packages = find_packages ( exclude = ( , , ) ) , \n 
include_package_data = True , \n 
setup_requires = [ , ] , \n 
classifiers = [ \n 
from mongoengine . base import BaseField \n 
__all__ = ( ) \n 
class WtfBaseField ( BaseField ) : \n 
def __init__ ( self , validators = None , filters = None , ** kwargs ) : \n 
~~~ self . validators = self . _ensure_callable_or_list ( validators , ) \n 
self . filters = self . _ensure_callable_or_list ( filters , ) \n 
BaseField . __init__ ( self , ** kwargs ) \n 
~~ def _ensure_callable_or_list ( self , field , msg_flag ) : \n 
if field is not None : \n 
~~~ if callable ( field ) : \n 
~~~ field = [ field ] \n 
if not isinstance ( field , list ) : \n 
~~~ raise TypeError ( msg ) \n 
~~ ~~ ~~ return field \n 
~~ ~~ from bson import DBRef , SON \n 
from mongoengine . python_support import txt_type \n 
from base import ( \n 
BaseDict , BaseList , EmbeddedDocumentList , \n 
TopLevelDocumentMetaclass , get_document \n 
from fields import ( ReferenceField , ListField , DictField , MapField ) \n 
from connection import get_db \n 
from queryset import QuerySet \n 
from document import Document , EmbeddedDocument \n 
class DeReference ( object ) : \n 
~~~ def __call__ ( self , items , max_depth = 1 , instance = None , name = None ) : \n 
if items is None or isinstance ( items , basestring ) : \n 
~~~ return items \n 
~~ if isinstance ( items , QuerySet ) : \n 
~~~ items = [ i for i in items ] \n 
~~ self . max_depth = max_depth \n 
doc_type = None \n 
if instance and isinstance ( instance , ( Document , EmbeddedDocument , \n 
TopLevelDocumentMetaclass ) ) : \n 
~~~ doc_type = instance . _fields . get ( name ) \n 
while hasattr ( doc_type , ) : \n 
~~~ doc_type = doc_type . field \n 
~~ if isinstance ( doc_type , ReferenceField ) : \n 
~~~ field = doc_type \n 
doc_type = doc_type . document_type \n 
is_list = not hasattr ( items , ) \n 
if is_list and all ( [ i . __class__ == doc_type for i in items ] ) : \n 
~~ elif not is_list and all ( \n 
[ i . __class__ == doc_type for i in items . values ( ) ] ) : \n 
~~ elif not field . dbref : \n 
~~~ if not hasattr ( items , ) : \n 
~~~ def _get_items ( items ) : \n 
~~~ new_items = [ ] \n 
for v in items : \n 
~~~ if isinstance ( v , list ) : \n 
~~~ new_items . append ( _get_items ( v ) ) \n 
~~ elif not isinstance ( v , ( DBRef , Document ) ) : \n 
~~~ new_items . append ( field . to_python ( v ) ) \n 
~~~ new_items . append ( v ) \n 
~~ ~~ return new_items \n 
~~ items = _get_items ( items ) \n 
~~~ items = dict ( [ \n 
( k , field . to_python ( v ) ) \n 
if not isinstance ( v , ( DBRef , Document ) ) else ( k , v ) \n 
for k , v in items . iteritems ( ) ] \n 
~~ ~~ ~~ ~~ self . reference_map = self . _find_references ( items ) \n 
self . object_map = self . _fetch_objects ( doc_type = doc_type ) \n 
return self . _attach_objects ( items , 0 , instance , name ) \n 
~~ def _find_references ( self , items , depth = 0 ) : \n 
reference_map = { } \n 
if not items or depth >= self . max_depth : \n 
~~~ return reference_map \n 
~~ if not hasattr ( items , ) : \n 
~~~ iterator = enumerate ( items ) \n 
~~~ iterator = items . iteritems ( ) \n 
~~ depth += 1 \n 
for k , item in iterator : \n 
~~~ if isinstance ( item , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in item . _fields . iteritems ( ) : \n 
~~~ v = item . _data . get ( field_name , None ) \n 
if isinstance ( v , DBRef ) : \n 
~~~ reference_map . setdefault ( field . document_type , set ( ) ) . add ( v . id ) \n 
~~ elif isinstance ( v , ( dict , SON ) ) and in v : \n 
~~~ reference_map . setdefault ( get_document ( v [ ] ) , set ( ) ) . add ( v [ ] . id ) \n 
~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ field_cls = getattr ( getattr ( field , , None ) , , None ) \n 
references = self . _find_references ( v , depth ) \n 
for key , refs in references . iteritems ( ) : \n 
~~~ if isinstance ( field_cls , ( Document , TopLevelDocumentMetaclass ) ) : \n 
~~~ key = field_cls \n 
~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ ~~ elif isinstance ( item , DBRef ) : \n 
~~~ reference_map . setdefault ( item . collection , set ( ) ) . add ( item . id ) \n 
~~ elif isinstance ( item , ( dict , SON ) ) and in item : \n 
~~~ reference_map . setdefault ( get_document ( item [ ] ) , set ( ) ) . add ( item [ ] . id ) \n 
~~ elif isinstance ( item , ( dict , list , tuple ) ) and depth - 1 <= self . max_depth : \n 
~~~ references = self . _find_references ( item , depth - 1 ) \n 
~~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ return reference_map \n 
~~ def _fetch_objects ( self , doc_type = None ) : \n 
object_map = { } \n 
for collection , dbrefs in self . reference_map . iteritems ( ) : \n 
~~~ col_name = collection . _get_collection_name ( ) \n 
refs = [ dbref for dbref in dbrefs \n 
if ( col_name , dbref ) not in object_map ] \n 
references = collection . objects . in_bulk ( refs ) \n 
for key , doc in references . iteritems ( ) : \n 
~~~ object_map [ ( col_name , key ) ] = doc \n 
~~~ if isinstance ( doc_type , ( ListField , DictField , MapField , ) ) : \n 
~~ refs = [ dbref for dbref in dbrefs \n 
if ( collection , dbref ) not in object_map ] \n 
if doc_type : \n 
~~~ references = doc_type . _get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
for ref in references : \n 
~~~ doc = doc_type . _from_son ( ref ) \n 
object_map [ ( collection , doc . id ) ] = doc \n 
~~~ references = get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
~~~ if in ref : \n 
~~~ doc = get_document ( ref [ "_cls" ] ) . _from_son ( ref ) \n 
~~ elif doc_type is None : \n 
~~~ doc = get_document ( \n 
. join ( x . capitalize ( ) \n 
for x in collection . split ( ) ) ) . _from_son ( ref ) \n 
~~ object_map [ ( collection , doc . id ) ] = doc \n 
~~ ~~ ~~ ~~ return object_map \n 
~~ def _attach_objects ( self , items , depth = 0 , instance = None , name = None ) : \n 
if not items : \n 
~~~ if isinstance ( items , ( BaseDict , BaseList ) ) : \n 
~~ if instance : \n 
~~~ if isinstance ( items , dict ) : \n 
~~~ return BaseDict ( items , instance , name ) \n 
~~~ return BaseList ( items , instance , name ) \n 
~~ ~~ ~~ if isinstance ( items , ( dict , SON ) ) : \n 
~~~ if in items : \n 
~~~ return self . object_map . get ( \n 
( items [ ] . collection , items [ ] . id ) , items ) \n 
~~ elif in items : \n 
~~~ doc = get_document ( items [ ] ) . _from_son ( items ) \n 
_cls = doc . _data . pop ( , None ) \n 
del items [ ] \n 
doc . _data = self . _attach_objects ( doc . _data , depth , doc , None ) \n 
if _cls is not None : \n 
~~~ doc . _data [ ] = _cls \n 
~~ return doc \n 
~~ ~~ if not hasattr ( items , ) : \n 
~~~ is_list = True \n 
list_type = BaseList \n 
if isinstance ( items , EmbeddedDocumentList ) : \n 
~~~ list_type = EmbeddedDocumentList \n 
~~ as_tuple = isinstance ( items , tuple ) \n 
iterator = enumerate ( items ) \n 
data = [ ] \n 
~~~ is_list = False \n 
iterator = items . iteritems ( ) \n 
data = { } \n 
for k , v in iterator : \n 
~~~ if is_list : \n 
~~~ data . append ( v ) \n 
~~~ data [ k ] = v \n 
~~ if k in self . object_map and not is_list : \n 
~~~ data [ k ] = self . object_map [ k ] \n 
~~ elif isinstance ( v , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in v . _fields . iteritems ( ) : \n 
~~~ v = data [ k ] . _data . get ( field_name , None ) \n 
~~~ data [ k ] . _data [ field_name ] = self . object_map . get ( \n 
( v . collection , v . id ) , v ) \n 
( v [ ] . collection , v [ ] . id ) , v ) \n 
~~~ item_name = txt_type ( "{0}.{1}.{2}" ) . format ( name , k , field_name ) \n 
data [ k ] . _data [ field_name ] = self . _attach_objects ( v , depth , instance = instance , name = item_name ) \n 
~~ ~~ ~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ item_name = % ( name , k ) if name else name \n 
data [ k ] = self . _attach_objects ( v , depth - 1 , instance = instance , name = item_name ) \n 
~~ elif hasattr ( v , ) : \n 
~~~ data [ k ] = self . object_map . get ( ( v . collection , v . id ) , v ) \n 
~~ ~~ if instance and name : \n 
~~~ return tuple ( data ) if as_tuple else list_type ( data , instance , name ) \n 
~~ return BaseDict ( data , instance , name ) \n 
return data \n 
sys . path [ 0 : 0 ] = [ "" ] \n 
from mongoengine import * \n 
from mongoengine . connection import get_db \n 
__all__ = ( "GeoFieldTest" , ) \n 
class GeoFieldTest ( unittest . TestCase ) : \n 
~~~ connect ( db = ) \n 
self . db = get_db ( ) \n 
~~ def _test_for_expected_error ( self , Cls , loc , expected ) : \n 
~~~ Cls ( loc = loc ) . validate ( ) \n 
self . fail ( . format ( loc ) ) \n 
~~ except ValidationError as e : \n 
~~~ self . assertEqual ( expected , e . to_dict ( ) [ ] ) \n 
~~ ~~ def test_geopoint_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = GeoPointField ( ) \n 
~~ invalid_coords = [ { "x" : 1 , "y" : 2 } , 5 , "a" ] \n 
expected = \n 
for coord in invalid_coords : \n 
~~~ self . _test_for_expected_error ( Location , coord , expected ) \n 
~~ invalid_coords = [ [ ] , [ 1 ] , [ 1 , 2 , 3 ] ] \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
~~ invalid_coords = [ [ { } , { } ] , ( "a" , "b" ) ] \n 
~~ ~~ def test_point_validation ( self ) : \n 
~~~ loc = PointField ( ) \n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ ] } \n 
invalid_coords = { "type" : "Point" , "coordinates" : [ 1 , 2 , 3 ] } \n 
invalid_coords = [ 5 , "a" ] \n 
~~ Location ( loc = [ 1 , 2 ] ) . validate ( ) \n 
Location ( loc = { \n 
"type" : "Point" , \n 
"coordinates" : [ \n 
81.4471435546875 , \n 
23.61432859499169 \n 
] } ) . validate ( ) \n 
~~ def test_linestring_validation ( self ) : \n 
~~~ loc = LineStringField ( ) \n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
invalid_coords = { "type" : "LineString" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
invalid_coords = [ [ 1 ] ] \n 
invalid_coords = [ [ 1 , 2 , 3 ] ] \n 
invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
~~ Location ( loc = [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ) . validate ( ) \n 
~~ def test_polygon_validation ( self ) : \n 
~~~ loc = PolygonField ( ) \n 
invalid_coords = { "type" : "Polygon" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
invalid_coords = [ [ [ 5 , "a" ] ] ] \n 
invalid_coords = [ [ [ ] ] ] \n 
invalid_coords = [ [ [ 1 , 2 , 3 ] ] ] \n 
invalid_coords = [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] \n 
Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
~~ def test_multipoint_validation ( self ) : \n 
~~~ loc = MultiPointField ( ) \n 
invalid_coords = { "type" : "MultiPoint" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
invalid_coords = [ [ ] ] \n 
invalid_coords = [ [ [ 1 ] ] , [ [ 1 , 2 , 3 ] ] ] \n 
~~ invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
~~ Location ( loc = [ [ 1 , 2 ] ] ) . validate ( ) \n 
"type" : "MultiPoint" , \n 
[ 1 , 2 ] , \n 
[ 81.4471435546875 , 23.61432859499169 ] \n 
~~ def test_multilinestring_validation ( self ) : \n 
~~~ loc = MultiLineStringField ( ) \n 
invalid_coords = { "type" : "MultiLineString" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
invalid_coords = [ [ [ 1 ] ] ] \n 
invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( "a" , "b" ) ] ] ] \n 
~~ Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
~~ def test_multipolygon_validation ( self ) : \n 
~~~ loc = MultiPolygonField ( ) \n 
invalid_coords = { "type" : "MultiPolygon" , "coordinates" : [ [ [ [ 1 , 2 , 3 ] ] ] ] } \n 
invalid_coords = [ [ [ [ 5 , "a" ] ] ] ] \n 
invalid_coords = [ [ [ [ ] ] ] ] \n 
invalid_coords = [ [ [ [ 1 , 2 , 3 ] ] ] ] \n 
invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n 
Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n 
~~ def test_indexes_geopoint ( self ) : \n 
class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
location = GeoPointField ( ) \n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertEqual ( geo_indicies , [ { : [ ( , ) ] } ] ) \n 
~~ def test_geopoint_embedded_indexes ( self ) : \n 
class Venue ( EmbeddedDocument ) : \n 
~~~ location = GeoPointField ( ) \n 
name = StringField ( ) \n 
~~ class Event ( Document ) : \n 
venue = EmbeddedDocumentField ( Venue ) \n 
~~ def test_indexes_2dsphere ( self ) : \n 
point = PointField ( ) \n 
line = LineStringField ( ) \n 
polygon = PolygonField ( ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
~~ def test_indexes_2dsphere_embedded ( self ) : \n 
~~~ name = StringField ( ) \n 
~~ def test_geo_indexes_recursion ( self ) : \n 
~~ class Parent ( Document ) : \n 
location = ReferenceField ( Location ) \n 
~~ Location . drop_collection ( ) \n 
Parent . drop_collection ( ) \n 
Parent ( name = ) . save ( ) \n 
info = Parent . _get_collection ( ) . index_information ( ) \n 
self . assertFalse ( in info ) \n 
info = Location . _get_collection ( ) . index_information ( ) \n 
self . assertTrue ( in info ) \n 
self . assertEqual ( len ( Parent . _geo_indices ( ) ) , 0 ) \n 
self . assertEqual ( len ( Location . _geo_indices ( ) ) , 1 ) \n 
~~ def test_geo_indexes_auto_index ( self ) : \n 
~~~ class Log ( Document ) : \n 
~~~ location = PointField ( auto_index = False ) \n 
datetime = DateTimeField ( ) \n 
meta = { \n 
: [ [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] ] \n 
~~ self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n 
Log . drop_collection ( ) \n 
Log . ensure_indexes ( ) \n 
info = Log . _get_collection ( ) . index_information ( ) \n 
self . assertEqual ( info [ "location_2dsphere_datetime_1" ] [ "key" ] , \n 
[ ( , ) , ( , 1 ) ] ) \n 
class Log ( Document ) : \n 
: [ \n 
{ : [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] } \n 
~~ from south . db import db \n 
from django . db import models \n 
from django_lean . experiments . models import * \n 
class Migration : \n 
~~~ def forwards ( self , orm ) : \n 
~~~ db . create_table ( , ( \n 
( , orm [ ] ) , \n 
) ) \n 
db . send_create_signal ( , [ ] ) \n 
db . create_table ( , ( \n 
db . create_unique ( , [ , ] ) \n 
~~ def backwards ( self , orm ) : \n 
~~~ db . delete_table ( ) \n 
db . delete_table ( ) \n 
db . delete_unique ( , [ , ] ) \n 
~~ models = { \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" , : } ) \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" } ) , \n 
: ( , [ ] , { : } ) \n 
: ( , [ ] , { : "orm[\'auth.Group\']" , : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" , : } ) , \n 
: ( , [ ] , { : , : } ) \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'experiments.Experiment\']" } ) , \n 
: ( , [ ] , { } ) \n 
: ( , [ ] , { : "orm[\'auth.User\']" } ) \n 
complete_apps = [ ] \n 
~~ class SimpleEngagementCalculator ( object ) : \n 
from django . contrib . sites . models import Site \n 
from django . core import mail \n 
from django . db import transaction \n 
from django . utils . functional import LazyObject \n 
def get_current_site ( ) : \n 
~~~ if Site . _meta . installed : \n 
~~~ return Site . objects . get_current ( ) \n 
~~ def in_transaction ( test_ignore = True ) : \n 
~~~ result = transaction . is_managed ( ) \n 
if test_ignore : \n 
~~~ result = result and not hasattr ( mail , ) \n 
~~ @ contextmanager \n 
def patch ( namespace , name , function ) : \n 
if isinstance ( namespace , LazyObject ) : \n 
~~~ if namespace . _wrapped is None : \n 
~~~ namespace . _setup ( ) \n 
~~ namespace = namespace . _wrapped \n 
~~~ original = getattr ( namespace , name ) \n 
~~ except AttributeError : \n 
~~~ original = NotImplemented \n 
~~~ setattr ( namespace , name , function ) \n 
yield \n 
~~ finally : \n 
~~~ if original is NotImplemented : \n 
~~~ delattr ( namespace , name ) \n 
~~~ setattr ( namespace , name , original ) \n 
from construct import * \n 
from ipv4 import IpAddress \n 
echo_payload = Struct ( "echo_payload" , \n 
UBInt16 ( "identifier" ) , \n 
UBInt16 ( "sequence" ) , \n 
dest_unreachable_payload = Struct ( "dest_unreachable_payload" , \n 
Padding ( 2 ) , \n 
UBInt16 ( "next_hop_mtu" ) , \n 
IpAddress ( "host" ) , \n 
Bytes ( "echo" , 8 ) , \n 
dest_unreachable_code = Enum ( Byte ( "code" ) , \n 
Network_unreachable_error = 0 , \n 
Host_unreachable_error = 1 , \n 
Protocol_unreachable_error = 2 , \n 
Port_unreachable_error = 3 , \n 
The_datagram_is_too_big = 4 , \n 
Source_route_failed_error = 5 , \n 
Destination_network_unknown_error = 6 , \n 
Destination_host_unknown_error = 7 , \n 
Source_host_isolated_error = 8 , \n 
Desination_administratively_prohibited = 9 , \n 
Host_administratively_prohibited2 = 10 , \n 
Network_TOS_unreachable = 11 , \n 
Host_TOS_unreachable = 12 , \n 
icmp_header = Struct ( "icmp_header" , \n 
Enum ( Byte ( "type" ) , \n 
Echo_reply = 0 , \n 
Destination_unreachable = 3 , \n 
Source_quench = 4 , \n 
Redirect = 5 , \n 
Alternate_host_address = 6 , \n 
Echo_request = 8 , \n 
Router_advertisement = 9 , \n 
Router_solicitation = 10 , \n 
Time_exceeded = 11 , \n 
Parameter_problem = 12 , \n 
Timestamp_request = 13 , \n 
Timestamp_reply = 14 , \n 
Information_request = 15 , \n 
Information_reply = 16 , \n 
Address_mask_request = 17 , \n 
Address_mask_reply = 18 , \n 
_default_ = Pass , \n 
Switch ( "code" , lambda ctx : ctx . type , \n 
"Destination_unreachable" : dest_unreachable_code , \n 
default = Byte ( "code" ) , \n 
UBInt16 ( "crc" ) , \n 
Switch ( "payload" , lambda ctx : ctx . type , \n 
"Echo_reply" : echo_payload , \n 
"Echo_request" : echo_payload , \n 
"Destination_unreachable" : dest_unreachable_payload , \n 
default = Pass \n 
~~~ cap1 = ( "0800305c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
"63646566676869" ) . decode ( "hex" ) \n 
cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n 
print icmp_header . parse ( cap1 ) \n 
print icmp_header . parse ( cap2 ) \n 
print icmp_header . parse ( cap3 ) \n 
~~ from construct . core import Container \n 
from construct . adapters import Adapter \n 
class AstNode ( Container ) : \n 
~~~ def __init__ ( self , nodetype , ** kw ) : \n 
~~~ Container . __init__ ( self ) \n 
self . nodetype = nodetype \n 
for k , v in sorted ( kw . iteritems ( ) ) : \n 
~~~ setattr ( self , k , v ) \n 
~~ ~~ def accept ( self , visitor ) : \n 
~~~ return getattr ( visitor , "visit_%s" % ( self . nodetype , ) ) ( self ) \n 
~~ ~~ class AstTransformator ( Adapter ) : \n 
~~~ def _decode ( self , obj , context ) : \n 
~~~ return self . to_ast ( obj , context ) \n 
~~ def _encode ( self , obj , context ) : \n 
~~~ return self . to_cst ( obj , context ) \n 
~~ ~~ def pytest_funcarg__setupopts ( request ) : \n 
~~~ return OptsSetup ( request ) \n 
~~ def pytest_addoption ( parser ) : \n 
~~~ parser . addoption ( "--uri-file" , dest = "urifile" , \n 
type = str , default = None , \n 
parser . addoption ( "--use-ns" , dest = "use_ns" , \n 
action = "store_true" , \n 
parser . addoption ( "--create-graph" , dest = "create_graph" , \n 
parser . addoption ( "--num-executors" , dest = "num_exec" , \n 
type = int , default = 0 , \n 
parser . addoption ( "--time" , dest = "time" , \n 
type = str , default = "2:00:00:00" , \n 
parser . addoption ( "--proc" , dest = "proc" , \n 
type = int , default = 8 , \n 
parser . addoption ( "--mem" , dest = "mem" , \n 
type = float , default = 16 , \n 
parser . addoption ( "--ppn" , dest = "ppn" , \n 
parser . addoption ( "--queue" , dest = "queue" , \n 
parser . addoption ( "--restart" , dest = "restart" , \n 
parser . addoption ( "--backup-dir" , dest = "backup_directory" , \n 
type = str , default = ".pipeline-backup" , \n 
~~ class OptsSetup ( ) : \n 
~~~ def __init__ ( self , request ) : \n 
~~~ self . config = request . config \n 
~~ def returnAllOptions ( self ) : \n 
~~~ return self . config . option \n 
~~ def getNumExecutors ( self ) : \n 
~~~ return self . config . option . num_exec \n 
~~ def getTime ( self ) : \n 
~~~ return self . config . option . time \n 
~~ def getProc ( self ) : \n 
~~~ return self . config . option . proc \n 
~~ def getMem ( self ) : \n 
~~~ return self . config . option . mem \n 
~~ def getQueue ( self ) : \n 
~~~ return self . config . option . queue \n 
~~ def getPpn ( self ) : \n 
~~~ return self . config . option . ppn \n 
~~ def getRestart ( self ) : \n 
~~~ return self . config . option . restart \n 
~~ def getBackupDir ( self ) : \n 
~~~ return self . config . option . backup_directory \n 
~~ def returnSampleArgs ( self ) : \n 
~~~ sampleArgArray = [ "TestProgName.py" , "img_A.mnc" , "img_B.mnc" ] \n 
return sampleArgArray \n 
import random \n 
from twisted . python import log \n 
from twisted . web . error import Error as WebError \n 
from opennsa import constants as cnt , config \n 
from opennsa . backends . common import genericbackend \n 
from opennsa . protocols . shared import httpclient \n 
#</service> \n 
LOG_SYSTEM = \n 
class NCSVPNTarget ( object ) : \n 
~~~ def __init__ ( self , router , interface , vlan = None ) : \n 
~~~ self . router = router \n 
self . interface = interface \n 
self . vlan = vlan \n 
~~~ if self . vlan : \n 
~~~ return % ( self . router , self . interface , self . vlan ) \n 
~~~ return % ( self . router , self . interface ) \n 
~~ ~~ ~~ def createVPNPayload ( service_name , source_target , dest_target ) : \n 
~~~ intps = { \n 
: service_name , \n 
: source_target . router , \n 
: source_target . interface , \n 
: dest_target . router , \n 
: dest_target . interface \n 
if source_target . vlan and dest_target . vlan : \n 
~~~ if source_target . vlan == dest_target . vlan : \n 
~~~ intps [ ] = source_target . vlan \n 
payload = ETHERNET_VLAN_VPN_PAYLOAD_BASE % intps \n 
intps [ ] = dest_target . vlan \n 
payload = ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE % intps \n 
~~~ payload = ETHERNET_VPN_PAYLOAD_BASE % intps \n 
~~ return payload \n 
~~ def _extractErrorMessage ( failure ) : \n 
~~~ if isinstance ( failure . value , WebError ) : \n 
~~~ return failure . value . response \n 
~~~ return failure . getErrorMessage ( ) \n 
~~ ~~ class NCSVPNConnectionManager : \n 
~~~ def __init__ ( self , ncs_services_url , user , password , port_map , log_system ) : \n 
~~~ self . ncs_services_url = ncs_services_url \n 
self . user = user \n 
self . password = password \n 
self . port_map = port_map \n 
self . log_system = log_system \n 
~~ def getResource ( self , port , label_type , label_value ) : \n 
~~~ assert label_type in ( None , cnt . ETHERNET_VLAN ) , \n 
~~ def getTarget ( self , port , label_type , label_value ) : \n 
if label_type == cnt . ETHERNET_VLAN : \n 
~~~ vlan = int ( label_value ) \n 
assert 1 <= vlan <= 4095 , % label_value \n 
~~ ri = self . port_map [ port ] \n 
router , interface = ri . split ( ) \n 
return NCSVPNTarget ( router , interface , vlan ) \n 
~~ def createConnectionId ( self , source_target , dest_target ) : \n 
~~~ return + str ( random . randint ( 100000 , 999999 ) ) \n 
~~ def canSwapLabel ( self , label_type ) : \n 
~~~ return label_type == cnt . ETHERNET_VLAN \n 
~~ def _createAuthzHeader ( self ) : \n 
~~~ return + base64 . b64encode ( self . user + + self . password ) \n 
~~ def _createHeaders ( self ) : \n 
~~~ headers = { } \n 
headers [ ] = \n 
headers [ ] = self . _createAuthzHeader ( ) \n 
return headers \n 
~~ def setupLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + NO_OUT_OF_SYNC_CHECK \n 
payload = createVPNPayload ( connection_id , source_target , dest_target ) \n 
headers = self . _createHeaders ( ) \n 
def linkUp ( _ ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
~~ def error ( failure ) : \n 
log . msg ( % _extractErrorMessage ( failure ) , system = self . log_system ) \n 
return failure \n 
~~ d = httpclient . httpRequest ( service_url , payload , headers , method = , timeout = NCS_TIMEOUT ) \n 
d . addCallbacks ( linkUp , error ) \n 
return d \n 
~~ def teardownLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + connection_id + + NO_OUT_OF_SYNC_CHECK \n 
def linkDown ( _ ) : \n 
~~ d = httpclient . httpRequest ( service_url , None , headers , method = , timeout = NCS_TIMEOUT ) \n 
d . addCallbacks ( linkDown , error ) \n 
~~ ~~ def NCSVPNBackend ( network_name , nrm_ports , parent_requester , cfg ) : \n 
~~~ name = % network_name \n 
user = cfg [ config . NCS_USER ] \n 
password = cfg [ config . NCS_PASSWORD ] \n 
cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n 
return genericbackend . GenericBackend ( network_name , nrm_map , cm , parent_requester , name ) \n 
from twisted . python import log , failure \n 
from opennsa import nsa , error \n 
from opennsa . shared import xmlhelper \n 
from opennsa . protocols . shared import minisoap , soapresource \n 
from opennsa . protocols . nsi2 import helper , queryhelper \n 
from opennsa . protocols . nsi2 . bindings import actions , nsiconnection , p2pservices \n 
class ProviderService : \n 
~~~ def __init__ ( self , soap_resource , provider ) : \n 
~~~ self . provider = provider \n 
soap_resource . registerDecoder ( actions . RESERVE , self . reserve ) \n 
soap_resource . registerDecoder ( actions . RESERVE_COMMIT , self . reserveCommit ) \n 
soap_resource . registerDecoder ( actions . RESERVE_ABORT , self . reserveAbort ) \n 
soap_resource . registerDecoder ( actions . PROVISION , self . provision ) \n 
soap_resource . registerDecoder ( actions . RELEASE , self . release ) \n 
soap_resource . registerDecoder ( actions . TERMINATE , self . terminate ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY , self . querySummary ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY_SYNC , self . querySummarySync ) \n 
soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n 
~~ def _createSOAPFault ( self , err , provider_nsa , connection_id = None , service_type = None ) : \n 
~~~ log . msg ( % err . getErrorMessage ( ) , system = LOG_SYSTEM ) \n 
se = helper . createServiceException ( err , provider_nsa , connection_id ) \n 
ex_element = se . xml ( nsiconnection . serviceException ) \n 
soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n 
return soap_fault \n 
~~ def reserve ( self , soap_data , request_info ) : \n 
~~~ t_start = time . time ( ) \n 
header , reservation = helper . parseRequest ( soap_data ) \n 
criteria = reservation . criteria \n 
p2ps = criteria . serviceDefinition \n 
if type ( p2ps ) is not p2pservices . P2PServiceBaseType : \n 
~~~ err = failure . Failure ( error . PayloadError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa , service_type = service_type ) \n 
~~ if p2ps . directionality in ( None , ) : \n 
~~~ err = failure . Failure ( error . MissingParameterError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa ) \n 
~~ start_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . startTime ) if criteria . schedule . startTime is not None else None \n 
end_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . endTime ) if criteria . schedule . endTime is not None else None \n 
schedule = nsa . Schedule ( start_time , end_time ) \n 
src_stp = helper . createSTP ( p2ps . sourceSTP ) \n 
dst_stp = helper . createSTP ( p2ps . destSTP ) \n 
if p2ps . ero : \n 
~~ params = [ ( p . type_ , p . value ) for p in p2ps . parameter ] if p2ps . parameter else None \n 
symmetric = p2ps . symmetricPath or False \n 
sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , None , params ) \n 
crt = nsa . Criteria ( criteria . version , schedule , sd ) \n 
t_delta = time . time ( ) - t_start \n 
log . msg ( % round ( t_delta , 3 ) , profile = True , system = LOG_SYSTEM ) \n 
d = self . provider . reserve ( header , reservation . connectionId , reservation . globalReservationId , reservation . description , crt , request_info ) \n 
def createReserveAcknowledgement ( connection_id ) : \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , None , header . correlation_id ) \n 
reserve_response = nsiconnection . ReserveResponseType ( connection_id ) \n 
reserve_response_element = reserve_response . xml ( nsiconnection . reserveResponse ) \n 
payload = minisoap . createSoapPayload ( reserve_response_element , soap_header_element ) \n 
return payload \n 
~~ d . addCallbacks ( createReserveAcknowledgement , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n 
~~ def reserveCommit ( self , soap_data , request_info ) : \n 
~~~ header , confirm = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveCommit ( header , confirm . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , confirm . connectionId ) ) \n 
~~ def reserveAbort ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveAbort ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n 
~~ def provision ( self , soap_data , request_info ) : \n 
d = self . provider . provision ( header , request . connectionId , request_info ) \n 
~~ def release ( self , soap_data , request_info ) : \n 
d = self . provider . release ( header , request . connectionId , request_info ) \n 
~~ def terminate ( self , soap_data , request_info ) : \n 
d = self . provider . terminate ( header , request . connectionId , request_info ) \n 
~~ def querySummary ( self , soap_data , request_info ) : \n 
~~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummary ( header , query . connectionId , query . globalReservationId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n 
~~ def querySummarySync ( self , soap_data , request_info ) : \n 
~~~ def gotReservations ( reservations , header ) : \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , correlation_id = header . correlation_id ) \n 
qs_reservations = queryhelper . buildQuerySummaryResultType ( reservations ) \n 
qsct = nsiconnection . QuerySummaryConfirmedType ( qs_reservations ) \n 
payload = minisoap . createSoapPayload ( qsct . xml ( nsiconnection . querySummarySyncConfirmed ) , soap_header_element ) \n 
~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummarySync ( header , query . connectionId , query . globalReservationId , request_info ) \n 
d . addCallbacks ( gotReservations , self . _createSOAPFault , callbackArgs = ( header , ) , errbackArgs = ( header . provider_nsa , ) ) \n 
~~ def queryRecursive ( self , soap_data , request_info ) : \n 
d = self . provider . queryRecursive ( header , query . connectionId , query . globalReservationId , request_info ) \n 
~~ ~~ import os , datetime , json \n 
from twisted . trial import unittest \n 
from twisted . internet import defer , task \n 
from opennsa import config , nsa , database \n 
from opennsa . topology import nml \n 
from opennsa . backends import ncsvpn \n 
from . import common \n 
class NCSVPNBackendTest ( unittest . TestCase ) : \n 
~~~ self . clock = task . Clock ( ) \n 
tcf = os . path . expanduser ( ) \n 
tc = json . load ( open ( tcf ) ) \n 
ncs_config = { \n 
config . NCS_SERVICES_URL : tc [ ] , \n 
config . NCS_USER : tc [ ] , \n 
config . NCS_PASSWORD : tc [ ] \n 
self . requester = common . DUDRequester ( ) \n 
self . backend = ncsvpn . NCSVPNBackend ( , self . sr , self . requester , ncs_config ) \n 
self . backend . scheduler . clock = self . clock \n 
self . backend . startService ( ) \n 
database . setupDatabase ( tc [ ] , tc [ ] , tc [ ] ) \n 
self . requester_nsa = nsa . NetworkServiceAgent ( , ) \n 
self . provider_nsa = nsa . NetworkServiceAgent ( , ) \n 
source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , ) ] ) \n 
dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , ) ] ) \n 
start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n 
end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n 
bandwidth = 200 \n 
self . service_params = nsa . ServiceParameters ( start_time , end_time , source_stp , dest_stp , bandwidth ) \n 
~~ @ defer . inlineCallbacks \n 
def tearDown ( self ) : \n 
~~~ from opennsa . backends . common import simplebackend \n 
yield simplebackend . Simplebackendconnection . deleteAll ( ) \n 
yield self . backend . stopService ( ) \n 
def testActivation ( self ) : \n 
~~~ _ , _ , cid , sp = yield self . reserve ( self . requester_nsa , self . provider_nsa , None , None , None , None , self . service_params ) \n 
yield self . backend . reserveCommit ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
yield self . backend . provision ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
self . clock . advance ( 3 ) \n 
connection_id , active , version_consistent , version , timestamp = yield d_up \n 
self . failUnlessEqual ( cid , connection_id ) \n 
self . failUnlessEqual ( active , True ) \n 
self . failUnlessEqual ( version_consistent , True ) \n 
yield self . backend . terminate ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
connection_id , active , version_consistent , version , timestamp = yield d_down \n 
self . failUnlessEqual ( active , False ) \n 
~~ testActivation . skip = \n 
#-- \n 
revision = \n 
down_revision = \n 
from alembic import op \n 
import sqlalchemy as sa \n 
def upgrade ( ) : \n 
~~~ op . add_column ( , sa . Column ( , sa . Integer ) ) \n 
~~ def downgrade ( ) : \n 
~~~ op . drop_column ( , ) \n 
~~ from . import view \n 
from . comp import Card , NewCard \n 
from nagare . i18n import _ \n 
from nagare import presentation , ajax , security , component \n 
from . comp import Gallery , Asset , AssetCropper \n 
def render_image ( self , h , comp , size , randomize = False , ** kw ) : \n 
~~~ metadata = self . assets_manager . get_metadata ( self . filename ) \n 
src = self . assets_manager . get_image_url ( self . filename , size ) \n 
if randomize : \n 
~~~ src += + h . generate_id ( ) \n 
~~ return h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = src , ** kw ) \n 
~~ def render_file ( self , h , comp , size , ** kw ) : \n 
~~~ kw [ ] += \n 
metadata = self . assets_manager . get_metadata ( self . filename ) \n 
res = [ h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = "img/file-icon.jpg" , ** kw ) ] \n 
if size == : \n 
~~~ res . append ( h . span ( metadata [ ] ) ) \n 
~~ return res \n 
~~ CONTENT_TYPES = { : render_image , \n 
: render_image , \n 
: render_image } \n 
@ presentation . render_for ( Gallery ) \n 
def render ( self , h , comp , * args ) : \n 
~~~ with h . div ( id = + self . comp_id ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ h << comp . render ( h , model = ) \n 
~~ with h . div ( id = "card-gallery" ) : \n 
~~~ h << comp . render ( h , self . model ) \n 
~~ ~~ return h . root \n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_Gallery_view ( self , h , comp , model ) : \n 
~~~ model = if security . has_permissions ( , self ) else \n 
for asset in self . assets : \n 
~~~ h << asset . render ( h , model ) \n 
~~ return h . root \n 
def render_Gallery_crop ( self , h , comp , model ) : \n 
~~~ return self . cropper . on_answer ( self . action ) \n 
def render_cover ( self , h , comp , model ) : \n 
~~~ cover = self . get_cover ( ) \n 
if cover : \n 
~~~ h << h . p ( component . Component ( self . get_cover ( ) , model = ) , class_ = ) \n 
~~ @ presentation . render_for ( Gallery , "action" ) \n 
def render_download ( self , h , comp , * args ) : \n 
~~~ if security . has_permissions ( , self ) : \n 
~~~ submit_id = h . generate_id ( "attach_submit" ) \n 
input_id = h . generate_id ( "attach_input" ) \n 
h << h . label ( ( h . i ( class_ = ) , \n 
with h . form : \n 
~~~ h << h . script ( \n 
% \n 
: ajax . py2js ( self . assets_manager . max_size ) , \n 
: ajax . py2js ( input_id ) , \n 
: ajax . py2js ( submit_id ) , \n 
: ajax . py2js ( \n 
_ ( ) \n 
) . decode ( ) \n 
submit_action = ajax . Update ( \n 
render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( ) ) , \n 
component_to_update = + self . comp_id , \n 
h << h . input ( id = input_id , class_ = , type = "file" , name = "file" , multiple = "multiple" , maxlength = "100" , ) . action ( self . add_assets ) \n 
h << h . input ( class_ = , id = submit_id , type = "submit" ) . action ( submit_action ) \n 
~~ @ presentation . render_for ( Gallery , model = ) \n 
def render_gallery_badge ( self , h , * args ) : \n 
if self . assets : \n 
~~~ with h . span ( class_ = ) : \n 
~~~ h << h . span ( h . i ( class_ = ) , , len ( self . assets ) , class_ = ) \n 
~~ @ presentation . render_for ( Asset ) \n 
@ presentation . render_for ( Asset , model = ) \n 
def render_asset ( self , h , comp , model , * args ) : \n 
~~~ res = [ ] \n 
kw = { : True } if model == else { } \n 
kw [ ] = model \n 
if self . is_cover : \n 
~~~ res . append ( h . span ( class_ = ) ) \n 
~~ meth = CONTENT_TYPES . get ( metadata [ ] , render_file ) \n 
res . append ( meth ( self , h , comp , model , ** kw ) ) \n 
return res \n 
~~ @ presentation . render_for ( Asset , model = ) \n 
def render_Asset_thumb ( self , h , comp , model , * args ) : \n 
~~~ action = h . a . action ( lambda : comp . answer ( ( , self ) ) ) . get ( ) \n 
onclick = _ ( ) \n 
with h . a ( class_ = , title = _ ( ) , href = , onclick = onclick ) : \n 
~~~ h << h . i ( class_ = ) \n 
~~ if self . is_image ( ) : \n 
~~~ with h . a ( class_ = , title = _ ( ) ) . action ( lambda : comp . answer ( ( , self ) ) ) : \n 
~~~ if self . is_cover : \n 
~~~ h << { : } \n 
~~ h << h . i ( class_ = ) \n 
~~ ~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , ) \n 
~~ @ presentation . render_for ( Asset , model = "anonymous" ) \n 
def render_asset_anonymous ( self , h , comp , model , * args ) : \n 
~~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , model = "thumb" ) \n 
~~ @ presentation . render_for ( AssetCropper ) \n 
def render_gallery_cropper ( self , h , comp , * args ) : \n 
~~~ h << h . p ( _ ( ) ) \n 
form_id = h . generate_id ( ) \n 
img_id = h . generate_id ( ) \n 
~~~ for crop_name in , , , : \n 
~~~ h << h . input ( type = , id = form_id + + crop_name ) . action ( getattr ( self , crop_name ) ) \n 
~~ h << h . p ( render_image ( self . asset , h , comp , , id = img_id ) ) \n 
h << h . script ( \n 
"YAHOO.util.Event.onContentReady(%s," \n 
ajax . py2js ( img_id ) , \n 
ajax . py2js ( form_id ) , \n 
ajax . py2js ( self . crop_width ( ) ) , \n 
ajax . py2js ( self . crop_height ( ) ) \n 
with h . div ( class_ = ) : \n 
~~~ h << h . button ( _ ( ) , class_ = ) . action ( self . commit , comp ) \n 
if self . asset . is_cover : \n 
~~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . remove_cover , comp ) \n 
~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . cancel , comp ) \n 
~~ class EventHandlerMixIn ( object ) : \n 
def emit_event ( self , comp , kind , data = None ) : \n 
~~~ event = kind ( data , source = [ self ] ) \n 
return comp . answer ( event ) \n 
~~ def handle_event ( self , comp , event ) : \n 
~~~ local_res = None \n 
local_handler = getattr ( self , , None ) \n 
if local_handler : \n 
~~~ local_res = local_handler ( comp , event ) \n 
~~ event . append ( self ) \n 
upper_res = comp . answer ( event ) \n 
return local_res or upper_res \n 
~~ ~~ class Event ( object ) : \n 
def __init__ ( self , data , source = [ ] ) : \n 
self . _source = source \n 
self . data = data \n 
def source ( self ) : \n 
~~~ return self . _source . copy ( ) \n 
def emitter ( self ) : \n 
~~~ return self . _source [ 0 ] \n 
def last_relay ( self ) : \n 
~~~ return self . _source [ - 1 ] \n 
~~ def is_ ( self , kind ) : \n 
~~~ return type ( self ) is kind \n 
~~ def is_kind_of ( self , kind ) : \n 
~~~ return isinstance ( self , kind ) \n 
~~ def append ( self , relay ) : \n 
~~~ self . _source . append ( relay ) \n 
~~ def cast_as ( self , sub_kind ) : \n 
~~~ return sub_kind ( self . data , self . _source ) \n 
~~ ~~ class ColumnDeleted ( Event ) : \n 
~~ class CardClicked ( Event ) : \n 
~~ class PopinClosed ( Event ) : \n 
~~ class CardEditorClosed ( PopinClosed ) : \n 
~~ class CardArchived ( Event ) : \n 
~~ class SearchIndexUpdated ( Event ) : \n 
~~ class CardDisplayed ( Event ) : \n 
~~ class BoardAccessChanged ( Event ) : \n 
~~ class BoardDeleted ( BoardAccessChanged ) : \n 
~~ class BoardArchived ( BoardAccessChanged ) : \n 
~~ class BoardRestored ( BoardAccessChanged ) : \n 
~~ class BoardLeft ( BoardAccessChanged ) : \n 
~~ class ParentTitleNeeded ( Event ) : \n 
~~ class NewTemplateRequested ( Event ) : \n 
~~ from . comp import EditableTitle \n 
from . import view \n 
#!/usr/bin/python2.7 \n 
####################################################################################################################### \n 
import genie2 . client . wrapper \n 
import genie2 . model . ClusterCriteria \n 
import genie2 . model . Job \n 
import genie2 . model . FileAttachment \n 
genie = genie2 . client . wrapper . Genie2 ( "http://localhost:8080/genie" , \n 
genie2 . client . wrapper . RetryPolicy ( \n 
tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) \n 
job = genie2 . model . Job . Job ( ) \n 
job . name = "GenieDockerExamplePigJob2" \n 
job . user = "root" \n 
job . version = "0.14.0" \n 
job . clusterCriterias = list ( ) \n 
cluster_criteria = genie2 . model . ClusterCriteria . ClusterCriteria ( ) \n 
criteria = set ( ) \n 
criteria . add ( "sched:adhoc" ) \n 
criteria . add ( "type:yarn" ) \n 
cluster_criteria . tags = criteria \n 
job . clusterCriterias . append ( cluster_criteria ) \n 
command_criteria = set ( ) \n 
command_criteria . add ( "type:pig" ) \n 
job . commandCriteria = command_criteria \n 
job . fileDependencies = "file:///apps/genie/pig/0.14.0/tutorial/script2-hadoop.pig,file:///apps/genie/pig/0.14.0/tutorial/tutorial.jar" \n 
job . commandArgs = "script2-hadoop.pig" \n 
job = genie . submitJob ( job ) \n 
while job . status != "SUCCEEDED" and job . status != "KILLED" and job . status != "FAILED" : \n 
time . sleep ( 10 ) \n 
job = genie . getJob ( job . id ) \n 
ON = 1 \n 
DISCONNECTED = 20 \n 
CONNECTED = 30 \n 
DEFAULT_EVENT_VERSION = 1 \n 
LOG_LEVEL = "DEBUG" \n 
LOG_FILE = "/var/log/security_monkey/security_monkey-deploy.log" \n 
SQLALCHEMY_DATABASE_URI = \n 
SQLALCHEMY_POOL_SIZE = 50 \n 
SQLALCHEMY_MAX_OVERFLOW = 15 \n 
ENVIRONMENT = \n 
USE_ROUTE53 = False \n 
FQDN = \n 
API_PORT = \n 
WEB_PORT = \n 
WEB_PATH = \n 
FRONTED_BY_NGINX = True \n 
NGINX_PORT = \n 
BASE_URL = . format ( FQDN ) \n 
SECRET_KEY = \n 
MAIL_DEFAULT_SENDER = \n 
SECURITY_REGISTERABLE = True \n 
SECURITY_CONFIRMABLE = False \n 
SECURITY_RECOVERABLE = False \n 
SECURITY_PASSWORD_HASH = \n 
SECURITY_PASSWORD_SALT = \n 
SECURITY_TRACKABLE = True \n 
SECURITY_POST_LOGIN_VIEW = BASE_URL \n 
SECURITY_POST_REGISTER_VIEW = BASE_URL \n 
SECURITY_POST_CONFIRM_VIEW = BASE_URL \n 
SECURITY_POST_RESET_VIEW = BASE_URL \n 
SECURITY_POST_CHANGE_VIEW = BASE_URL \n 
SECURITY_TEAM_EMAIL = [ ] \n 
SES_REGION = \n 
MAIL_SERVER = \n 
MAIL_PORT = 465 \n 
MAIL_USE_SSL = True \n 
MAIL_USERNAME = \n 
MAIL_PASSWORD = \n 
WTF_CSRF_ENABLED = True \n 
WTF_CSRF_METHODS = [ , , , ] \n 
SECURITYGROUP_INSTANCE_DETAIL = \n 
CORE_THREADS = 25 \n 
MAX_THREADS = 30 \n 
from security_monkey . auditor import Auditor \n 
from security_monkey . watchers . rds_security_group import RDSSecurityGroup \n 
from security_monkey . datastore import NetworkWhitelistEntry \n 
from security_monkey . auditors . security_group import _check_rfc_1918 \n 
import ipaddr \n 
class RDSSecurityGroupAuditor ( Auditor ) : \n 
~~~ index = RDSSecurityGroup . index \n 
i_am_singular = RDSSecurityGroup . i_am_singular \n 
i_am_plural = RDSSecurityGroup . i_am_plural \n 
network_whitelist = [ ] \n 
def __init__ ( self , accounts = None , debug = False ) : \n 
~~~ super ( RDSSecurityGroupAuditor , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
~~ def prep_for_audit ( self ) : \n 
~~~ self . network_whitelist = NetworkWhitelistEntry . query . all ( ) \n 
~~ def _check_inclusion_in_network_whitelist ( self , cidr ) : \n 
~~~ for entry in self . network_whitelist : \n 
~~~ if ipaddr . IPNetwork ( cidr ) in ipaddr . IPNetwork ( str ( entry . cidr ) ) : \n 
~~ def check_rds_ec2_rfc1918 ( self , sg_item ) : \n 
severity = 8 \n 
if sg_item . config . get ( "vpc_id" , None ) : \n 
~~ for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and _check_rfc_1918 ( cidr ) : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
~~ ~~ ~~ def check_securitygroup_large_subnet ( self , sg_item ) : \n 
severity = 3 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
if cidr and not self . _check_inclusion_in_network_whitelist ( cidr ) : \n 
~~~ if in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
~~~ mask = int ( cidr . split ( ) [ 1 ] ) \n 
if mask < 24 and mask > 0 : \n 
~~ ~~ ~~ ~~ ~~ def check_securitygroup_zero_subnet ( self , sg_item ) : \n 
severity = 10 \n 
if cidr and in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
if mask == 0 : \n 
~~ ~~ ~~ ~~ def check_securitygroup_any ( self , sg_item ) : \n 
severity = 5 \n 
~~~ cidr = ipr . get ( "cidr_ip" ) \n 
if "0.0.0.0/0" == cidr : \n 
~~ ~~ ~~ def check_securitygroup_10net ( self , sg_item ) : \n 
if "10.0.0.0/8" == cidr : \n 
from security_monkey . datastore import NetworkWhitelistEntry , Account \n 
from security_monkey . tests import SecurityMonkeyTestCase \n 
from security_monkey import db \n 
from security_monkey . watchers . elasticsearch_service import ElasticSearchServiceItem \n 
CONFIG_ONE = { \n 
"name" : "es_test" , \n 
CONFIG_TWO = { \n 
"name" : "es_test_2" , \n 
CONFIG_THREE = { \n 
"name" : "es_test_3" , \n 
CONFIG_FOUR = { \n 
"name" : "es_test_4" , \n 
CONFIG_FIVE = { \n 
"name" : "es_test_5" , \n 
CONFIG_SIX = { \n 
"name" : "es_test_6" , \n 
CONFIG_SEVEN = { \n 
"name" : "es_test_7" , \n 
CONFIG_EIGHT = { \n 
"name" : "es_test_8" , \n 
CONFIG_NINE = { \n 
"name" : "es_test_9" , \n 
WHITELIST_CIDRS = [ \n 
class ElasticSearchServiceTestCase ( SecurityMonkeyTestCase ) : \n 
~~~ self . es_items = [ \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config = CONFIG_ONE ) , \n 
ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config = CONFIG_TWO ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config = CONFIG_THREE ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config = CONFIG_FOUR ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config = CONFIG_FIVE ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config = CONFIG_SIX ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config = CONFIG_SEVEN ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config = CONFIG_EIGHT ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config = CONFIG_NINE ) , \n 
test_account = Account ( ) \n 
test_account . name = "TEST_ACCOUNT" \n 
test_account . s3_name = "TEST_ACCOUNT" \n 
test_account . number = "012345678910" \n 
test_account . role_name = "TEST_ACCOUNT" \n 
db . session . add ( test_account ) \n 
db . session . commit ( ) \n 
~~ def tearDown ( self ) : \n 
~~~ test_account = Account . query . filter ( Account . number == "012345678910" ) . first ( ) \n 
if test_account is not None : \n 
~~~ db . session . delete ( test_account ) \n 
~~ ~~ def test_es_auditor ( self ) : \n 
~~~ from security_monkey . auditors . elasticsearch_service import ElasticSearchServiceAuditor \n 
es_auditor = ElasticSearchServiceAuditor ( accounts = [ "012345678910" ] ) \n 
es_auditor . network_whitelist = [ ] \n 
for cidr in WHITELIST_CIDRS : \n 
~~~ whitelist_cidr = NetworkWhitelistEntry ( ) \n 
whitelist_cidr . cidr = cidr [ 1 ] \n 
whitelist_cidr . name = cidr [ 0 ] \n 
es_auditor . network_whitelist . append ( whitelist_cidr ) \n 
~~ for es_domain in self . es_items : \n 
~~~ es_auditor . check_es_access_policy ( es_domain ) \n 
~~ self . assertEquals ( len ( self . es_items [ 0 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 0 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 1 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 1 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 2 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 1 ] . score , 7 ) \n 
self . assertEquals ( len ( self . es_items [ 3 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 3 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 4 ] . audit_issues ) , 0 ) \n 
self . assertEquals ( len ( self . es_items [ 5 ] . audit_issues ) , 0 ) \n 
self . assertEquals ( len ( self . es_items [ 6 ] . audit_issues ) , 3 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 1 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 2 ] . score , 7 ) \n 
self . assertEquals ( len ( self . es_items [ 7 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 7 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 8 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 0 ] . score , 6 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 1 ] . score , 10 ) \n 
from security_monkey . watcher import Watcher \n 
from security_monkey . watcher import ChangeItem \n 
from security_monkey . constants import TROUBLE_REGIONS \n 
from security_monkey . exceptions import BotoConnectionIssue \n 
from security_monkey import app \n 
from boto . redshift import regions \n 
class Redshift ( Watcher ) : \n 
~~~ index = \n 
i_am_singular = \n 
i_am_plural = \n 
~~~ super ( Redshift , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
~~ def slurp ( self ) : \n 
self . prep_for_slurp ( ) \n 
from security_monkey . common . sts_connect import connect \n 
item_list = [ ] \n 
exception_map = { } \n 
for account in self . accounts : \n 
~~~ for region in regions ( ) : \n 
~~~ redshift = connect ( account , , region = region ) \n 
all_clusters = [ ] \n 
marker = None \n 
while True : \n 
~~~ response = self . wrap_aws_rate_limited_call ( \n 
redshift . describe_clusters , \n 
marker = marker \n 
all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n 
~~~ if region . name not in TROUBLE_REGIONS : \n 
~~~ exc = BotoConnectionIssue ( str ( e ) , , account , region . name ) \n 
self . slurp_exception ( ( self . index , account , region . name ) , exc , exception_map ) ~~ continue \n 
for cluster in all_clusters : \n 
~~~ cluster_id = cluster [ ] \n 
if self . check_ignore_list ( cluster_id ) : \n 
~~ item = RedshiftCluster ( region = region . name , account = account , name = cluster_id , config item_list . append ( item ) \n 
~~ ~~ ~~ return item_list , exception_map \n 
~~ ~~ class RedshiftCluster ( ChangeItem ) : \n 
~~~ def __init__ ( self , region = None , account = None , name = None , config = { } ) : \n 
~~~ super ( RedshiftCluster , self ) . __init__ ( \n 
index = Redshift . index , \n 
region = region , \n 
account = account , \n 
new_config = config ) \n 
from ndscheduler import settings \n 
from ndscheduler . core . datastore . providers import base \n 
class DatastorePostgresql ( base . DatastoreBase ) : \n 
~~~ @ classmethod \n 
def get_db_url ( cls ) : \n 
return % ( \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] ) \n 
from ndscheduler import job \n 
logger = logging . getLogger ( __name__ ) \n 
class CurlJob ( job . JobBase ) : \n 
~~~ TIMEOUT = 10 \n 
def meta_info ( cls ) : \n 
~~~ return { \n 
: % ( cls . __module__ , cls . __name__ ) , \n 
{ : , : } , \n 
{ : , : \n 
] , \n 
~~ def run ( self , url , request_type , * args , ** kwargs ) : \n 
~~~ print ( % ( url ) ) \n 
session = requests . Session ( ) \n 
result = session . request ( request_type , \n 
url , \n 
timeout = self . TIMEOUT , \n 
headers = None , \n 
data = None ) \n 
print ( result . text ) \n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ job = CurlJob . create_test_instance ( ) \n 
job . run ( ) \n 
~~ import unittest \n 
from . mock import MagicMock , Mock \n 
from . util import TrelloElementMock , CommandMock , OperationMock \n 
from operations import * \n 
class BaseOperationTests ( unittest . TestCase ) : \n 
~~~ self . base_operation , self . trello_element = OperationMock . create ( BaseOperation ) \n 
self . class_mock , self . instance_mock = OperationMock . instance ( self . base_operation ) \n 
self . collection = TrelloElementMock . collection ( ) \n 
self . base_operation . collection = TrelloCollection ( self . collection ) \n 
~~ def test_items_sets_the_collection ( self ) : \n 
~~~ self . base_operation . set_collection = MagicMock ( ) \n 
self . base_operation . items ( ) \n 
self . base_operation . set_collection . assert_called_with ( ) \n 
~~ def test_items_returns_every_name_from_the_collection_with_the_added_options ( self ) : \n 
~~ def test_callback_uses_find_to_instantiate_the_operation_if_the_index_is_in_the_collection ( self ) ~~~ self . base_operation . callback ( 3 ) \n 
self . class_mock . assert_called_with ( self . collection [ 0 ] , self . base_operation ) \n 
~~ def test_callback_calls_execute_on_the_operation ( self ) : \n 
~~~ self . base_operation . callback ( 3 ) \n 
self . instance_mock . execute . assert_called_with ( self . base_operation . command ) \n 
~~ def test_callback_doesnt_call_find_if_the_index_is_bigger_than_the_collection_length ( self ) : \n 
~~~ big_index = 55 \n 
self . base_operation . callback ( big_index ) \n 
assert not self . class_mock . called \n 
~~ def test_callback_calls_execute_on_the_previous_operation_if_index_is_0 ( self ) : \n 
~~~ self . base_operation . callback ( 0 ) \n 
self . base_operation . previous_operation . execute . assert_called_with ( ) \n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . base_operation . command . input = MagicMock ( ) \n 
self . base_operation . callback ( 2 ) \n 
self . base_operation . command . input . assert_called_with ( "Name" , self . base_operation . deferred_add \n 
~~ def test_base_add_calls_add_with_the_text_and_cleans_the_cache_for_the_element ( self ) : \n 
~~~ text = "Text" \n 
self . base_operation . add = MagicMock ( ) \n 
self . base_operation . trello_element . reload = MagicMock ( ) \n 
self . base_operation . base_add ( text ) \n 
self . base_operation . add . assert_called_with ( text ) \n 
self . trello_element . reload . assert_called_with ( ) \n 
~~ def test_base_add_calls_add_and_execute_if_renavigate_is_true ( self ) : \n 
self . base_operation . command . renavigate = True \n 
self . base_operation . execute = MagicMock ( ) \n 
self . base_operation . execute . assert_called_with ( ) \n 
~~ ~~ class BoardOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( BoardOperation ) \n 
self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n 
~~ def test_items_returns_every_name_from_the_collection_without_goback ( self ) : \n 
~~~ self . operation . set_collection = MagicMock ( ) \n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "boards" ) \n 
~~ def test_callback_calls_execute_command_with_the_index ( self ) : \n 
~~~ self . operation . execute_command = MagicMock ( ) \n 
self . operation . callback ( 5 ) \n 
self . operation . execute_command . assert_called_with ( 3 ) \n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . operation . command . input = MagicMock ( ) \n 
self . operation . callback ( 1 ) \n 
self . operation . command . input . assert_called_with ( "Name" , self . operation . deferred_add ) \n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , ListOperation ) \n 
~~ def test_add_creates_a_board_with_the_text ( self ) : \n 
self . trello_element . add_board = MagicMock ( ) \n 
self . operation . add ( text ) \n 
self . trello_element . add_board . assert_called_with ( text ) \n 
~~ ~~ class ListOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( ListOperation ) \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "lists" ) \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOperation ) \n 
~~ def test_add_creates_a_list_with_the_text ( self ) : \n 
self . trello_element . add_list = MagicMock ( ) \n 
self . trello_element . add_list . assert_called_with ( text ) \n 
~~ ~~ class CardOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( CardOperation ) \n 
~~ def test_items_returns_every_name_from_the_collection_with_custom_actions ( self ) : \n 
self . assertEqual ( self . operation . items ( ) , [ , , \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "cards" ) \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOptions ) \n 
~~ def test_add_creates_a_card_with_the_text_and_description ( self ) : \n 
self . trello_element . add_card = MagicMock ( ) \n 
self . operation . add ( name , desc ) \n 
self . trello_element . add_card . assert_called_with ( name , desc ) \n 
~~ def test_split_card_contents_returns_the_name_and_description_splitted_by_new_lines ( self ) : \n 
~~~ content = "Name!!\\n\\nDescription\\nYeah!" \n 
name , desc = self . operation . split_card_contents ( content ) \n 
self . assertEqual ( name , "Name!!" ) \n 
self . assertEqual ( desc , "Description\\nYeah!" ) \n 
~~ \n 
DEBUG = env . bool ( , default = True ) \n 
TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n 
SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n 
EMAIL_HOST = \n 
EMAIL_PORT = 1025 \n 
EMAIL_BACKEND = env ( , \n 
default = ) \n 
CACHES = { \n 
: \n 
MIDDLEWARE_CLASSES += ( , ) \n 
INSTALLED_APPS += ( , ) \n 
INTERNAL_IPS = ( , , ) \n 
DEBUG_TOOLBAR_CONFIG = { \n 
: True , \n 
TEST_RUNNER = \n 
from django . contrib import messages \n 
from django . contrib . auth import logout , login , authenticate \n 
from django . http import HttpResponseBadRequest , Http404 \n 
from django . shortcuts import render , redirect , get_object_or_404 \n 
from reddit . forms import UserForm , ProfileForm \n 
from reddit . utils . helpers import post_only \n 
from users . models import RedditUser \n 
def user_profile ( request , username ) : \n 
~~~ user = get_object_or_404 ( User , username = username ) \n 
profile = RedditUser . objects . get ( user = user ) \n 
return render ( request , , { : profile } ) \n 
def edit_profile ( request ) : \n 
~~~ user = RedditUser . objects . get ( user = request . user ) \n 
if request . method == : \n 
~~~ profile_form = ProfileForm ( instance = user ) \n 
~~ elif request . method == : \n 
~~~ profile_form = ProfileForm ( request . POST , instance = user ) \n 
if profile_form . is_valid ( ) : \n 
~~~ profile = profile_form . save ( commit = False ) \n 
profile . update_profile_data ( ) \n 
profile . save ( ) \n 
~~ return render ( request , , { : profile_form } ) \n 
~~ def user_login ( request ) : \n 
if request . user . is_authenticated ( ) : \n 
return render ( request , ) \n 
~~ if request . method == "POST" : \n 
~~~ username = request . POST . get ( ) \n 
password = request . POST . get ( ) \n 
if not username or not password : \n 
~~~ return HttpResponseBadRequest ( ) \n 
~~ user = authenticate ( username = username , \n 
password = password ) \n 
~~~ if user . is_active : \n 
~~~ login ( request , user ) \n 
redirect_url = request . POST . get ( ) or \n 
return redirect ( redirect_url ) \n 
~~~ return render ( request , , \n 
~~ ~~ return render ( request , ) \n 
~~ @ post_only \n 
def user_logout ( request ) : \n 
~~~ redirect_page = request . POST . get ( , ) \n 
logout ( request ) \n 
messages . success ( request , ) \n 
return redirect ( redirect_page ) \n 
~~ return redirect ( ) \n 
~~ def register ( request ) : \n 
user_form = UserForm ( ) \n 
~~~ messages . warning ( request , \n 
return render ( request , , { : user_form } ) \n 
~~~ user_form = UserForm ( request . POST ) \n 
if user_form . is_valid ( ) : \n 
~~~ user = user_form . save ( ) \n 
user . set_password ( user . password ) \n 
user . save ( ) \n 
reddit_user = RedditUser ( ) \n 
reddit_user . user = user \n 
reddit_user . save ( ) \n 
user = authenticate ( username = request . POST [ ] , \n 
password = request . POST [ ] ) \n 
login ( request , user ) \n 
return redirect ( ) \n 
~~ ~~ return render ( request , , { : user_form } ) \n 
~~ import unittest2 \n 
from pymysql . tests import base \n 
from pymysql import util \n 
class TestNextset ( base . PyMySQLTestCase ) : \n 
~~~ super ( TestNextset , self ) . setUp ( ) \n 
self . con = self . connections [ 0 ] \n 
~~ def test_nextset ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
r = cur . nextset ( ) \n 
self . assertTrue ( r ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur ) ) \n 
self . assertIsNone ( cur . nextset ( ) ) \n 
~~ def test_skip_nextset ( self ) : \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur ) ) \n 
~~ def test_ok_and_next ( self ) : \n 
self . assertTrue ( cur . nextset ( ) ) \n 
self . assertFalse ( bool ( cur . nextset ( ) ) ) \n 
~~ @ unittest2 . expectedFailure \n 
def test_multi_cursor ( self ) : \n 
~~~ cur1 = self . con . cursor ( ) \n 
cur2 = self . con . cursor ( ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur1 ) ) \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur2 ) ) \n 
r = cur1 . nextset ( ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur1 ) ) \n 
self . assertIsNone ( cur1 . nextset ( ) ) \n 
~~ def test_multi_statement_warnings ( self ) : \n 
~~~ cursor = self . con . cursor ( ) \n 
~~~ cursor . execute ( \n 
~~~ self . fail ( ) \n 
~~ ~~ ~~ from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import message as _message \n 
from google . protobuf import reflection as _reflection \n 
from google . protobuf import descriptor_pb2 \n 
DESCRIPTOR = _descriptor . FileDescriptor ( \n 
package = , \n 
_PUSHNOTIFICATION = _descriptor . Descriptor ( \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 3 , cpp_type = 2 , label = 1 , \n 
has_default_value = False , default_value = 0 , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
name = , full_name = , index = 1 , \n 
number = 2 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
name = , full_name = , index = 2 , \n 
number = 3 , type = 9 , cpp_type = 9 , label = 1 , \n 
name = , full_name = , index = 3 , \n 
number = 4 , type = 9 , cpp_type = 9 , label = 1 , \n 
extensions = [ \n 
nested_types = [ ] , \n 
enum_types = [ \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
serialized_start = 33 , \n 
serialized_end = 117 , \n 
_BATCHNOTIFICATIONREQUEST = _descriptor . Descriptor ( \n 
number = 1 , type = 11 , cpp_type = 10 , label = 3 , \n 
has_default_value = False , default_value = [ ] , \n 
serialized_start = 119 , \n 
serialized_end = 187 , \n 
_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _BATCHNOTIFICATIONREQUEST \n 
class PushNotification ( _message . Message ) : \n 
~~~ __metaclass__ = _reflection . GeneratedProtocolMessageType \n 
DESCRIPTOR = _PUSHNOTIFICATION \n 
~~ class BatchNotificationRequest ( _message . Message ) : \n 
DESCRIPTOR = _BATCHNOTIFICATIONREQUEST \n 
~~ DESCRIPTOR . has_options = True \n 
import pytest \n 
from pushkin import pushkin_cli \n 
import tornado . web \n 
from pushkin import context \n 
from pushkin . database import database \n 
from pushkin . request . request_processor import RequestProcessor \n 
from pushkin . requesthandlers . events import JsonEventHandler \n 
from pushkin . requesthandlers . notifications import JsonNotificationHandler \n 
from pushkin import test_config_ini_path \n 
from pushkin import config \n 
@ pytest . fixture \n 
def setup_database ( ) : \n 
~~~ database . create_database ( ) \n 
~~ @ pytest . fixture \n 
def mock_processor ( mocker ) : \n 
mocker . patch ( ) \n 
def app ( ) : \n 
~~~ pushkin_cli . CONFIGURATION_FILENAME = test_config_ini_path \n 
pushkin_cli . init ( ) \n 
return pushkin_cli . create_app ( ) \n 
def notification_batch_json ( ) : \n 
def post_notification_url ( base_url ) : \n 
~~~ return base_url + config . json_notification_handler_url \n 
def event_batch_json ( ) : \n 
def post_event_url ( base_url ) : \n 
~~~ return base_url + config . json_event_handler_url \n 
~~ @ pytest . mark . gen_test \n 
@ pytest . mark . parametrize ( "input" , [ \n 
( ) , \n 
def test_post_notification_empty_request ( setup_database , mock_processor , http_client , post_notification_url ~~~ request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = input ) \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
~~ assert not context . request_processor . submit . called \n 
def test_post_notification ( setup_database , mock_processor , http_client , post_notification_url , \n 
notification_batch_json ) : \n 
request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = notification_batch_json response = yield http_client . fetch ( request ) \n 
assert response . code == 200 \n 
assert context . request_processor . submit . called \n 
def test_post_event_empty_request ( setup_database , mock_processor , http_client , post_event_url , input ~~~ \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = input ) \n 
def test_post_event ( setup_database , mock_processor , http_client , post_event_url , event_batch_json ) : \n 
context . request_processor . submit . return_value = True \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = event_batch_json ) \n 
response = yield http_client . fetch ( request ) \n 
def test_post_event_service_unavailable ( setup_database , mock_processor , http_client , post_event_url , app ) : \n 
context . request_processor . submit . return_value = False \n 
RequestProcessor . submit . return_value = False \n 
from pywechat . excepts import WechatError \n 
class Basic ( object ) : \n 
def __init__ ( self , app_id , app_secret ) : \n 
self . __app_id = app_id \n 
self . __app_secret = app_secret \n 
self . __access_token = self . access_token \n 
self . __token_expires_at = None \n 
def access_token ( self ) : \n 
if self . __access_token and self . __token_expires_at : \n 
~~~ if self . __token_expires_at - time . time ( ) > 60 : \n 
~~~ return self . __access_token \n 
~~ ~~ self . _grant_access_token ( ) \n 
return self . __access_token \n 
~~ def _send_request ( self , method , url , ** kwargs ) : \n 
if not kwargs . get ( ) : \n 
~~~ kwargs [ ] = { \n 
"access_token" : self . access_token \n 
~~ if kwargs . get ( ) : \n 
~~~ data = json . dumps ( kwargs [ ] ) . encode ( ) \n 
kwargs [ "data" ] = data \n 
~~ request = requests . request ( \n 
method = method , \n 
url = url , \n 
** kwargs \n 
request . raise_for_status ( ) \n 
json_data = request . json ( ) \n 
self . _check_wechat_error ( json_data ) \n 
return json_data \n 
def _check_wechat_error ( cls , json_data ) : \n 
errcode = json_data . get ( ) \n 
if errcode and errcode != 0 : \n 
~~~ raise WechatError ( errcode , json_data . get ( ) ) \n 
~~ ~~ def _grant_access_token ( self ) : \n 
url = \n 
params = { \n 
"grant_type" : "client_credential" , \n 
"appid" : self . __app_id , \n 
"secret" : self . __app_secret \n 
json_data = self . _send_request ( , url , params = params ) \n 
self . __access_token = json_data . get ( ) \n 
self . __token_expires_at = int ( \n 
time . time ( ) ) + json_data . get ( ) \n 
~~ def _get_wechat_server_ips ( self ) : \n 
url = "https://api.weixin.qq.com/cgi-bin/getcallbackip" \n 
from . . model . hashes import Hashes \n 
from . . one_drive_object_base import OneDriveObjectBase \n 
class File ( OneDriveObjectBase ) : \n 
~~~ def __init__ ( self , prop_dict = { } ) : \n 
~~~ self . _prop_dict = prop_dict \n 
def hashes ( self ) : \n 
if "hashes" in self . _prop_dict : \n 
~~~ if isinstance ( self . _prop_dict [ "hashes" ] , OneDriveObjectBase ) : \n 
~~~ return self . _prop_dict [ "hashes" ] \n 
~~~ self . _prop_dict [ "hashes" ] = Hashes ( self . _prop_dict [ "hashes" ] ) \n 
return self . _prop_dict [ "hashes" ] \n 
~~ ~~ return None \n 
~~ @ hashes . setter \n 
def hashes ( self , val ) : \n 
~~~ self . _prop_dict [ "hashes" ] = val \n 
def mime_type ( self ) : \n 
if "mimeType" in self . _prop_dict : \n 
~~~ return self . _prop_dict [ "mimeType" ] \n 
~~ ~~ @ mime_type . setter \n 
def mime_type ( self , val ) : \n 
~~~ self . _prop_dict [ "mimeType" ] = val \n 
class RequestBuilderBase ( object ) : \n 
~~~ def __init__ ( self , request_url , client ) : \n 
self . _request_url = request_url \n 
self . _client = client \n 
~~ def append_to_request_url ( self , url_segment ) : \n 
return self . _request_url + "/" + url_segment \n 
from . . collection_base import CollectionRequestBase , CollectionResponseBase , CollectionPageBase \n 
from . . request_builder_base import RequestBuilderBase \n 
from . . model . item import Item \n 
class SharedCollectionRequest ( CollectionRequestBase ) : \n 
~~~ def __init__ ( self , request_url , client , options ) : \n 
super ( SharedCollectionRequest , self ) . __init__ ( request_url , client , options ) \n 
~~ def get ( self ) : \n 
self . method = "GET" \n 
collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n 
return self . _page_from_response ( collection_response ) \n 
~~ ~~ class SharedCollectionRequestBuilder ( RequestBuilderBase ) : \n 
~~~ def __getitem__ ( self , key ) : \n 
return ItemRequestBuilder ( self . append_to_request_url ( str ( key ) ) , self . _client ) \n 
~~ def request ( self , expand = None , select = None , top = None , order_by = None , options = None ) : \n 
req = SharedCollectionRequest ( self . _request_url , self . _client , options ) \n 
req . _set_query_options ( expand = expand , select = select , top = top , order_by = order_by ) \n 
return req \n 
return self . request ( ) . get ( ) \n 
~~ ~~ class SharedCollectionResponse ( CollectionResponseBase ) : \n 
~~~ @ property \n 
def collection_page ( self ) : \n 
if self . _collection_page : \n 
~~~ self . _collection_page . _prop_list = self . _prop_dict [ "value" ] \n 
~~~ self . _collection_page = SharedCollectionPage ( self . _prop_dict [ "value" ] ) \n 
~~ return self . _collection_page \n 
~~ ~~ class SharedCollectionPage ( CollectionPageBase ) : \n 
~~~ def __getitem__ ( self , index ) : \n 
return Item ( self . _prop_list [ index ] ) \n 
~~ def shared ( self ) : \n 
for item in self . _prop_list : \n 
~~~ yield Item ( item ) \n 
~~ ~~ def _init_next_page_request ( self , next_page_link , client , options ) : \n 
self . _next_page_request = SharedCollectionRequest ( next_page_link , client , options ) \n 
~~ ~~ from . . request . item_request_builder import ItemRequestBuilder \n 
from __future__ import absolute_import , division , print_function \n 
import glob \n 
import os . path \n 
from cffi import FFI \n 
from cffi . verifier import Verifier \n 
__all__ = [ "ffi" ] \n 
HEADERS = glob . glob ( \n 
os . path . join ( os . path . abspath ( os . path . dirname ( __file__ ) ) , "*.h" ) \n 
ffi = FFI ( ) \n 
for header in sorted ( HEADERS ) : \n 
~~~ with open ( header , "r" ) as hfile : \n 
~~~ ffi . cdef ( hfile . read ( ) ) \n 
~~ ~~ ffi . verifier = Verifier ( \n 
ffi , \n 
libraries = [ "sodium" ] , \n 
ext_package = "nacl._lib" , \n 
class Library ( object ) : \n 
~~~ def __init__ ( self , ffi ) : \n 
~~~ self . ffi = ffi \n 
self . _lib = None \n 
def _compile_module ( * args , ** kwargs ) : \n 
~~ self . ffi . verifier . compile_module = _compile_module \n 
~~ def __getattr__ ( self , name ) : \n 
~~~ if self . _lib is None : \n 
~~~ self . _lib = self . ffi . verifier . load_library ( ) \n 
~~ return getattr ( self . _lib , name ) \n 
~~ ~~ lib = Library ( ffi ) \n 
import sqlite3 \n 
def migrate ( database_path ) : \n 
conn = sqlite3 . connect ( database_path ) \n 
conn . text_factory = str \n 
cursor = conn . cursor ( ) \n 
cursor . execute ( ) \n 
notifications = cursor . fetchall ( ) \n 
for n in notifications : \n 
~~~ cursor . execute ( , ( n [ 0 ] , n [ 1 ] , n [ 2 ] , n [ 3 ] , n [ 4 ] , n [ 5 ] , n [ 6 ] , n [ 7 ] , \n 
~~ cursor . execute ( ) \n 
conn . commit ( ) \n 
conn . close ( ) \n 
DEBUG = 5 \n 
WARNING = 4 \n 
INFO = 3 \n 
ERROR = 2 \n 
CRITICAL = 1 \n 
levels = { "debug" : 5 , "warning" : 4 , "info" : 3 , "error" : 2 , "critical" : 1 } \n 
class FileLogObserver ( log . FileLogObserver ) : \n 
~~~ def __init__ ( self , f = None , level = "info" , default = DEBUG ) : \n 
~~~ log . FileLogObserver . __init__ ( self , f or sys . stdout ) \n 
self . level = levels [ level ] \n 
self . default = default \n 
~~ def emit ( self , eventDict ) : \n 
~~~ ll = eventDict . get ( , self . default ) \n 
if eventDict [ ] or in eventDict or self . level >= ll : \n 
~~~ log . FileLogObserver . emit ( self , eventDict ) \n 
~~ ~~ ~~ class Logger ( object ) : \n 
~~~ def __init__ ( self , ** kwargs ) : \n 
~~~ self . kwargs = kwargs \n 
~~ def msg ( self , message , ** kw ) : \n 
~~~ kw . update ( self . kwargs ) \n 
if in kw and not isinstance ( kw [ ] , str ) : \n 
~~~ kw [ ] = kw [ ] . __class__ . __name__ \n 
~~ log . msg ( message , ** kw ) \n 
~~ def info ( self , message , ** kw ) : \n 
~~~ kw [ ] = INFO \n 
~~ def debug ( self , message , ** kw ) : \n 
~~~ kw [ ] = DEBUG \n 
~~ def warning ( self , message , ** kw ) : \n 
~~~ kw [ ] = WARNING \n 
~~ def error ( self , message , ** kw ) : \n 
~~~ kw [ ] = ERROR \n 
~~ def critical ( self , message , ** kw ) : \n 
~~~ kw [ ] = CRITICAL \n 
~~ ~~ try : \n 
~~~ theLogger \n 
~~ except NameError : \n 
~~~ theLogger = Logger ( ) \n 
msg = theLogger . msg \n 
info = theLogger . info \n 
debug = theLogger . debug \n 
warning = theLogger . warning \n 
error = theLogger . error \n 
critical = theLogger . critical \n 
~~ import sys \n 
_b = sys . version_info [ 0 ] < 3 and ( lambda x : x ) or ( lambda x : x . encode ( ) ) \n 
from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import symbol_database as _symbol_database \n 
_sym_db = _symbol_database . Default ( ) \n 
_sym_db . RegisterFileDescriptor ( DESCRIPTOR ) \n 
_PEERSEEDS = _descriptor . Descriptor ( \n 
number = 1 , type = 12 , cpp_type = 9 , label = 3 , \n 
number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n 
has_default_value = False , default_value = _b ( "" ) , \n 
oneofs = [ \n 
serialized_start = 15 , \n 
serialized_end = 69 , \n 
DESCRIPTOR . message_types_by_name [ ] = _PEERSEEDS \n 
PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n 
DESCRIPTOR = _PEERSEEDS , \n 
__module__ = \n 
_sym_db . RegisterMessage ( PeerSeeds ) \n 
@ register . filter ( name = ) \n 
def get_item ( dictionary , key ) : \n 
~~~ return getattr ( dictionary , key ) \n 
~~ default_app_config = \n 
from . . utils . access_permissions import BaseAccessPermissions \n 
class MediafileAccessPermissions ( BaseAccessPermissions ) : \n 
def can_retrieve ( self , user ) : \n 
return user . has_perm ( ) \n 
~~ def get_serializer_class ( self , user = None ) : \n 
from . serializers import MediafileSerializer \n 
return MediafileSerializer \n 
~~ ~~ from __future__ import unicode_literals \n 
import openslides . utils . models \n 
~~~ initial = True \n 
dependencies = [ \n 
migrations . CreateModel ( \n 
( , models . AutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name ( , models . CharField ( max_length = 128 , verbose_name = ) ) , \n 
( , models . DateTimeField ( blank = True , null = True , verbose_name = ( , models . BooleanField ( \n 
default = False , \n 
help_text = verbose_name = ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 , unique = True ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 50 ) ) , \n 
( , models . TextField ( blank = True , default = ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 100 ) ) , \n 
( , models . BooleanField ( default = True ) ) , \n 
( , models . BooleanField ( default = False ) ) , \n 
( , models . ManyToManyField ( \n 
blank = True , \n 
help_text = related_name = , \n 
related_query_name = , \n 
to = , \n 
verbose_name = ) ) , \n 
help_text = , \n 
related_name = , \n 
options = { \n 
: ( \n 
( , ) ( , ) ) , \n 
: ( ) , \n 
: ( , , ) , \n 
bases = ( openslides . utils . models . RESTModelMixin , models . Model ) , \n 
~~ import json \n 
from django . dispatch import receiver \n 
from rest_framework import status \n 
from rest_framework . test import APIClient \n 
from openslides import __version__ as version \n 
from openslides . core . config import ConfigVariable , config \n 
from openslides . core . models import CustomSlide , Projector \n 
from openslides . core . signals import config_signal \n 
from openslides . utils . rest_api import ValidationError \n 
from openslides . utils . test import TestCase \n 
class ProjectorAPI ( TestCase ) : \n 
def test_slide_on_default_projector ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n 
default_projector . config = { \n 
: { : , : customslide . id } } \n 
default_projector . save ( ) \n 
response = self . client . get ( reverse ( , args = [ ] ) ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
: 1 , \n 
{ : customslide . id , \n 
: } } , \n 
: 0 , \n 
: 0 } ) \n 
~~ def test_invalid_slide_on_default_projector ( self ) : \n 
default_projector = Projector . objects . get ( pk = 1 ) \n 
: { : } } \n 
~~ ~~ class VersionView ( TestCase ) : \n 
def test_get ( self ) : \n 
response = self . client . get ( reverse ( ) ) \n 
: version , \n 
: } ] } ) \n 
~~ ~~ class ConfigViewSet ( TestCase ) : \n 
def test_retrieve ( self ) : \n 
config [ ] = \n 
response = self . client . get ( reverse ( , args = [ ] ) ) self . assertEqual ( \n 
response . data , \n 
: } ) \n 
~~ def test_update ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( config [ ] , ) \n 
~~ def test_update_wrong_datatype ( self ) : \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
~~ def test_update_wrong_datatype_that_can_be_converted ( self ) : \n 
self . client = APIClient ( ) \n 
self . assertEqual ( response . status_code , 200 ) \n 
~~ def test_update_good_choice ( self ) : \n 
~~ def test_update_bad_choice ( self ) : \n 
self . assertEqual ( response . data , { : } ) \n 
~~ def test_update_validator_ok ( self ) : \n 
~~ def test_update_validator_invalid ( self ) : \n 
~~ def test_update_only_with_key ( self ) : \n 
reverse ( , args = [ ] ) ) \n 
~~ def test_metadata_with_hidden ( self ) : \n 
response = self . client . options ( reverse ( ) ) \n 
filter_obj = filter ( \n 
lambda item : item [ ] == , \n 
response . data [ ] [ 0 ] [ ] [ 0 ] [ ] ) \n 
self . assertEqual ( len ( list ( filter_obj ) ) , 0 ) \n 
~~ ~~ def validator_for_testing ( value ) : \n 
if value == : \n 
~~~ raise ValidationError ( { : } ) \n 
~~ ~~ @ receiver ( config_signal , dispatch_uid = ) \n 
def set_simple_config_view_integration_config_test ( sender , ** kwargs ) : \n 
yield ConfigVariable ( \n 
default_value = None , \n 
label = ) \n 
default_value = ) \n 
default_value = 0 , \n 
input_type = ) \n 
default_value = , \n 
input_type = , \n 
choices = ( \n 
{ : , : } , { : , : } ) \n 
validators = ( validator_for_testing , ) ) \n 
label = , \n 
hidden = True ) \n 
~~ from unittest import TestCase \n 
from unittest . mock import MagicMock , patch \n 
from openslides . users . serializers import UserFullSerializer \n 
class UserCreateUpdateSerializerTest ( TestCase ) : \n 
~~~ def test_validate_no_data ( self ) : \n 
serializer = UserFullSerializer ( ) \n 
with self . assertRaises ( ValidationError ) : \n 
~~~ serializer . validate ( data ) \n 
~~ ~~ @ patch ( ) \n 
def test_validate_no_username ( self , generate_username ) : \n 
generate_username . return_value = \n 
data = { : } \n 
new_data = serializer . validate ( data ) \n 
self . assertEqual ( new_data [ ] , ) \n 
~~ def test_validate_no_username_in_patch_request ( self ) : \n 
view = MagicMock ( action = ) \n 
serializer = UserFullSerializer ( context = { : view } ) \n 
self . assertIsNone ( new_data . get ( ) ) \n 
#domain... #localhost... \n 
, re . IGNORECASE ) \n 
USERNAME_REGEX = re . compile ( , re . I ) \n 
FULLNAME_REGEX = re . compile ( , re . U ) \n 
EMAIL_REGEX = re . compile ( , re . IGNORECASE ) \n 
class CheckValue ( object ) : \n 
~~ def length ( self , data , minimum = - 1 , maximum = - 1 ) : \n 
len_input = len ( data ) \n 
if len_input < minimum or maximum != - 1 and len_input > maximum : \n 
~~ def regexp ( self , data , regex , flags = 0 ) : \n 
regex = re . compile ( regex , flags ) \n 
if regex . match ( data ) : \n 
~~ def username ( self , data ) : \n 
if USERNAME_REGEX . match ( data ) : \n 
~~ def full_name ( self , data ) : \n 
if FULLNAME_REGEX . match ( data ) : \n 
~~ def email ( self , data ) : \n 
if EMAIL_REGEX . match ( data ) : \n 
~~ def url ( self , data ) : \n 
if URL_REGEX . match ( data ) : \n 
~~ def url_two ( self , data ) : \n 
regex = re . compile ( , re . IGNORECASE ) \n 
~~ def is_integer ( self , data ) : \n 
~~~ tmp = int ( data ) \n 
~~ ~~ def float ( self , data ) : \n 
~~~ tmp = float ( data ) \n 
~~ ~~ ~~ import logging \n 
from bagpipe . bgp . common import utils \n 
from bagpipe . bgp . common import logDecorator \n 
from bagpipe . bgp . vpn . vpn_instance import VPNInstance \n 
from bagpipe . bgp . engine import RouteEvent \n 
from bagpipe . bgp . vpn . dataplane_drivers import DummyDataplaneDriver as _DummyDataplaneDriver \n 
from bagpipe . bgp . common . looking_glass import LookingGlass , LGMap \n 
from bagpipe . exabgp . structure . vpn import RouteDistinguisher , VPNLabelledPrefix \n 
from bagpipe . exabgp . structure . mpls import LabelStackEntry \n 
from bagpipe . exabgp . structure . address import AFI , SAFI \n 
from bagpipe . exabgp . structure . ip import Inet , Prefix \n 
from bagpipe . exabgp . message . update . route import Route \n 
from bagpipe . exabgp . message . update . attribute . nexthop import NextHop \n 
from bagpipe . exabgp . message . update . attribute . communities import ECommunities \n 
class DummyDataplaneDriver ( _DummyDataplaneDriver ) : \n 
~~ class VRF ( VPNInstance , LookingGlass ) : \n 
~~~ type = "ipvpn" \n 
afi = AFI ( AFI . ipv4 ) \n 
safi = SAFI ( SAFI . mpls_vpn ) \n 
@ logDecorator . log \n 
~~~ VPNInstance . __init__ ( self , * args , ** kwargs ) \n 
self . readvertised = set ( ) \n 
~~ def _routeFrom ( self , prefix , label , rd ) : \n 
~~~ return Route ( VPNLabelledPrefix ( self . afi , self . safi , prefix , rd , \n 
[ LabelStackEntry ( label , True ) ] \n 
~~ def generateVifBGPRoute ( self , macAdress , ipPrefix , prefixLen , label ) : \n 
~~~ route = self . _routeFrom ( Prefix ( self . afi , ipPrefix , prefixLen ) , label , \n 
RouteDistinguisher ( \n 
RouteDistinguisher . TYPE_IP_LOC , None , \n 
self . bgpManager . getLocalAddress ( ) , \n 
self . instanceId ) \n 
return self . _newRouteEntry ( self . afi , self . safi , self . exportRTs , \n 
route . nlri , route . attributes ) \n 
~~ def _getLocalLabels ( self ) : \n 
~~~ for portData in self . macAddress2LocalPortData . itervalues ( ) : \n 
~~~ yield portData [ ] \n 
~~ ~~ def _getRDFromLabel ( self , label ) : \n 
~~~ return RouteDistinguisher ( RouteDistinguisher . TYPE_IP_LOC , None , \n 
10000 + label ) \n 
~~ def _routeForReAdvertisement ( self , prefix , label ) : \n 
~~~ route = self . _routeFrom ( prefix , label , \n 
self . _getRDFromLabel ( label ) ) \n 
nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n 
self . dataplane . driver . getLocalAddress ( ) ) ) \n 
route . attributes . add ( NextHop ( nh ) ) \n 
route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n 
routeEntry = self . _newRouteEntry ( self . afi , self . safi , \n 
self . readvertiseToRTs , \n 
return routeEntry \n 
~~ @ logDecorator . log \n 
def _readvertise ( self , nlri ) : \n 
for label in self . _getLocalLabels ( ) : \n 
nlri . prefix , label ) \n 
routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n 
~~ self . readvertised . add ( nlri . prefix ) \n 
def _readvertiseStop ( self , nlri ) : \n 
self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n 
~~ self . readvertised . remove ( nlri . prefix ) \n 
~~ def vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) : \n 
~~~ VPNInstance . vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) \n 
label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
for prefix in self . readvertised : \n 
prefix ) \n 
routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n 
~~ ~~ def vifUnplugged ( self , macAddress , ipAddressPrefix , advertiseSubnet ) : \n 
~~~ label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
~~ VPNInstance . vifUnplugged ( self , macAddress , ipAddressPrefix , \n 
~~ def _route2trackedEntry ( self , route ) : \n 
~~~ if isinstance ( route . nlri , VPNLabelledPrefix ) : \n 
~~~ return route . nlri . prefix \n 
type ( route . nlri ) ) \n 
~~ ~~ def _toReadvertise ( self , route ) : \n 
~~~ return ( len ( set ( route . routeTargets ) . intersection ( \n 
set ( self . readvertiseFromRTs ) ) ) > 0 ) \n 
~~ def _imported ( self , route ) : \n 
set ( self . importRTs ) ) ) > 0 ) \n 
~~ @ utils . synchronized \n 
def _newBestRoute ( self , entry , newRoute ) : \n 
~~~ prefix = entry \n 
if self . readvertise : \n 
if self . _toReadvertise ( newRoute ) : \n 
self . _readvertise ( newRoute . nlri ) \n 
if not self . _imported ( newRoute ) : \n 
~~ ~~ ~~ encaps = self . _checkEncaps ( newRoute ) \n 
if not encaps : \n 
~~ self . dataplane . setupDataplaneForRemoteEndpoint ( \n 
prefix , newRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n 
def _bestRouteRemoved ( self , entry , oldRoute , last ) : \n 
if self . readvertise and last : \n 
~~~ if self . _toReadvertise ( oldRoute ) : \n 
self . _readvertiseStop ( oldRoute . nlri ) \n 
if not self . _imported ( oldRoute ) : \n 
~~ ~~ ~~ if self . _skipRouteRemoval ( last ) : \n 
~~ self . dataplane . removeDataplaneForRemoteEndpoint ( \n 
prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n 
~~ def getLGMap ( self ) : \n 
"readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n 
self . readvertised ] ) \n 
from bagpipe . exabgp . message . update . attribute import AttributeID , Flag , Attribute \n 
class Origin ( Attribute ) : \n 
~~~ ID = AttributeID . ORIGIN \n 
FLAG = Flag . TRANSITIVE \n 
MULTIPLE = False \n 
IGP = 0x00 \n 
EGP = 0x01 \n 
INCOMPLETE = 0x02 \n 
def __init__ ( self , origin ) : \n 
~~~ self . origin = origin \n 
~~ def pack ( self ) : \n 
~~~ return self . _attribute ( chr ( self . origin ) ) \n 
~~~ return len ( self . pack ( ) ) \n 
~~~ if self . origin == 0x00 : return \n 
if self . origin == 0x01 : return \n 
if self . origin == 0x02 : return \n 
~~~ return str ( self ) \n 
~~ def __cmp__ ( self , other ) : \n 
~~~ if ( not isinstance ( other , Origin ) \n 
or ( self . origin != other . origin ) \n 
) : \n 
import inspect \n 
import signal \n 
from multiprocessing import Process \n 
import tasa \n 
from tasa . worker import BaseWorker \n 
logging . basicConfig ( level = logging . INFO ) \n 
def signal_handler ( signal , frame ) : \n 
~~~ sys . exit ( 0 ) \n 
~~ def _get_argparser ( ) : \n 
~~~ parser = argparse . ArgumentParser ( ) \n 
parser . add_argument ( \n 
, , action = , \n 
version = % ( \n 
tasa . __version__ , sys . version ) ) \n 
return parser \n 
~~ def run ( ) : \n 
~~~ sys . path . insert ( 0 , ) \n 
parser = _get_argparser ( ) \n 
parser . description = \n 
parser . add_argument ( , \n 
type = lambda w : w . partition ( ) [ : : 2 ] , \n 
help = \n 
worker_class_name = args . worker [ 1 ] or \n 
worker_module = __import__ ( args . worker [ 0 ] , globals ( ) , locals ( ) , \n 
[ worker_class_name ] ) \n 
~~~ WorkerClass = getattr ( worker_module , worker_class_name ) \n 
potential_workers = inspect . getmembers ( \n 
worker_module , \n 
lambda x : type ( x ) == type and issubclass ( x , BaseWorker ) ) \n 
if potential_workers : \n 
for name , value in potential_workers : \n 
~~~ print . join ( [ args . worker [ 0 ] , name ] ) \n 
~~ ~~ exit ( 1 ) \n 
~~ worker = WorkerClass ( ) \n 
print % ( args . worker [ 0 ] , \n 
worker . __class__ . __name__ ) \n 
~~~ for job in worker : \n 
~~~ if job : \n 
worker . __class__ . __name__ , \n 
str ( job ) [ : 50 ] ) \n 
~~~ time . sleep ( .3 ) \n 
~~ ~~ ~~ except KeyboardInterrupt : \n 
~~~ print \n 
~~ ~~ def runm ( ) : \n 
signal . signal ( signal . SIGINT , signal_handler ) \n 
count = int ( sys . argv . pop ( 1 ) ) \n 
processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n 
~~~ for p in processes : \n 
~~~ p . start ( ) \n 
~~ ~~ except KeyError : \n 
~~~ p . join ( ) \n 
~~ ~~ ~~ def log ( ) : \n 
~~~ parser = _get_argparser ( ) \n 
raise NotImplemented ( ) \n 
~~~ cmd = if len ( sys . argv ) < 2 else sys . argv . pop ( 1 ) \n 
if cmd == : \n 
~~~ run ( ) \n 
~~ elif cmd == : \n 
~~~ log ( ) \n 
from time import sleep \n 
from flask import Flask \n 
from flask_tut . models import ( \n 
db , \n 
User , \n 
Address , \n 
app = Flask ( __name__ ) \n 
with app . app_context ( ) : \n 
~~~ db . create_all ( ) \n 
~~ i = 0 \n 
while i < 30 : \n 
~~~ address = Address ( description = + str ( i ) . rjust ( 2 , "0" ) ) \n 
db . session . add ( address ) \n 
user = User ( name = + str ( i ) . rjust ( 2 , "0" ) ) \n 
user . address = address \n 
db . session . add ( user ) \n 
sleep ( 1 ) \n 
i += 1 \n 
~~ db . session . commit ( ) \n 
import urllib2 \n 
import google \n 
import pyprind \n 
class Crawler ( object ) : \n 
~~~ version = "1.2.3" \n 
outputDir = "output" \n 
languageDir = "languages" \n 
basicString = "/get.php?username=%s&password=%s&type=m3u&output=mpegts" \n 
def __init__ ( self , language = "it" ) : \n 
self . language = language . lower ( ) \n 
self . parsedUrls = [ ] \n 
self . foundedAccounts = 0 \n 
~~ def change_language ( self , language = "it" ) : \n 
if os . path . isfile ( self . languageDir + "/" + language + ".txt" ) : \n 
~~~ self . language = language \n 
~~ ~~ def search_links ( self ) : \n 
for url in google . search ( self . searchString , num = 30 , stop = 1 ) : \n 
~~~ parsed = urlparse ( url ) \n 
self . parsedUrls . append ( parsed . scheme + "://" + parsed . netloc ) \n 
~~ ~~ def search_accounts ( self , url = None ) : \n 
if not self . parsedUrls : \n 
~~~ if not url : \n 
~~~ url = random . choice ( self . parsedUrls ) \n 
~~ fileName = self . languageDir + "/" + self . language + ".txt" \n 
fileLength = self . file_length ( fileName ) \n 
with open ( fileName ) as f : \n 
~~~ rows = f . readlines ( ) \n 
~~ for row in rows : \n 
~~~ opener = urllib2 . build_opener ( ) \n 
opener . addheaders = [ ( , ) ] \n 
response = opener . open ( url + self . basicString % ( row . rstrip ( ) . lstrip ( ) , row . rstrip ( ) fetched = response . read ( ) \n 
fileLength = fileLength - 1 \n 
progressBar . update ( ) \n 
if len ( fetched ) > 0 : \n 
~~~ newPath = self . outputDir + "/" + url . replace ( "http://" , "" ) \n 
self . create_file ( row , newPath , fetched ) \n 
~~ ~~ self . parsedUrls . remove ( url ) \n 
if self . foundedAccounts != 0 : \n 
~~ ~~ except IOError : \n 
~~ except urllib2 . HTTPError , e : \n 
~~ except urllib2 . URLError , e : \n 
~~ except Exception : \n 
~~ ~~ def create_file ( self , row , newPath , fetched ) : \n 
if os . path . exists ( newPath ) is False : \n 
~~~ os . makedirs ( newPath ) \n 
~~ outputFile = open ( str ( newPath ) + "/tv_channels_%s.m3u" % row . rstrip ( ) . lstrip ( ) , "w" ) \n 
outputFile . write ( fetched ) \n 
self . foundedAccounts = self . foundedAccounts + 1 \n 
outputFile . close ( ) \n 
~~ def file_length ( self , fileName ) : \n 
~~~ for i , l in enumerate ( f ) : \n 
~~ ~~ return i + 1 \n 
~~ ~~ from cStringIO import StringIO \n 
from binascii import b2a_hex \n 
from urllib import quote \n 
import Connecter \n 
~~~ True \n 
~~~ True = 1 \n 
False = 0 \n 
~~ DEBUG = False \n 
protocol_name = \n 
option_pattern = chr ( 0 ) * 8 \n 
def toint ( s ) : \n 
~~~ return long ( b2a_hex ( s ) , 16 ) \n 
~~ def tohex ( s ) : \n 
~~~ return b2a_hex ( s ) . upper ( ) \n 
~~ def make_readable ( s ) : \n 
~~~ if not s : \n 
~~ if quote ( s ) . find ( ) >= 0 : \n 
~~~ return tohex ( s ) \n 
~~ return \'"\' + s + \'"\' \n 
~~ streamno = 0 \n 
class StreamCheck : \n 
~~~ global streamno \n 
self . no = streamno \n 
streamno += 1 \n 
self . buffer = StringIO ( ) \n 
self . next_len , self . next_func = 1 , self . read_header_len \n 
~~ def read_header_len ( self , s ) : \n 
~~~ if ord ( s ) != len ( protocol_name ) : \n 
~~~ print self . no , \n 
~~ return len ( protocol_name ) , self . read_header \n 
~~ def read_header ( self , s ) : \n 
~~~ if s != protocol_name : \n 
~~ return 8 , self . read_reserved \n 
~~ def read_reserved ( self , s ) : \n 
~~~ return 20 , self . read_download_id \n 
~~ def read_download_id ( self , s ) : \n 
~~~ if DEBUG : \n 
~~~ print self . no , + tohex ( s ) \n 
~~ return 20 , self . read_peer_id \n 
~~ def read_peer_id ( self , s ) : \n 
~~~ print self . no , + make_readable ( s ) \n 
~~ return 4 , self . read_len \n 
~~ def read_len ( self , s ) : \n 
~~~ l = toint ( s ) \n 
if l > 2 ** 23 : \n 
~~~ print self . no , + str ( l ) + + s + \n 
~~ return l , self . read_message \n 
~~ def read_message ( self , s ) : \n 
~~~ return 4 , self . read_len \n 
~~ m = s [ 0 ] \n 
if ord ( m ) > 8 : \n 
~~~ print self . no , + str ( ord ( m ) ) \n 
~~ if m == Connecter . REQUEST : \n 
~~~ if len ( s ) != 13 : \n 
~~~ print self . no , + str ( len ( s ) ) \n 
return 4 , self . read_len \n 
~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = toint ( s [ 9 : ] ) \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ elif m == Connecter . CANCEL : \n 
~~ elif m == Connecter . PIECE : \n 
~~~ index = toint ( s [ 1 : 5 ] ) \n 
length = len ( s ) - 9 \n 
~~~ print self . no , + str ( ord ( m ) ) + + str ( len ( s ) ) + \n 
~~ def write ( self , s ) : \n 
~~~ while 1 : \n 
~~~ i = self . next_len - self . buffer . tell ( ) \n 
if i > len ( s ) : \n 
~~~ self . buffer . write ( s ) \n 
~~ self . buffer . write ( s [ : i ] ) \n 
s = s [ i : ] \n 
m = self . buffer . getvalue ( ) \n 
self . buffer . reset ( ) \n 
self . buffer . truncate ( ) \n 
x = self . next_func ( m ) \n 
self . next_len , self . next_func = x \n 
~~ ~~ ~~ from types import * \n 
from cStringIO import StringIO \n 
def splitLine ( line , COLS = 80 , indent = 10 ) : \n 
width = COLS - ( len ( indent ) + 1 ) \n 
if indent and width < 15 : \n 
~~~ width = COLS - 2 \n 
~~ s = StringIO ( ) \n 
i = 0 \n 
for word in line . split ( ) : \n 
~~~ if i == 0 : \n 
~~~ s . write ( indent + word ) \n 
i = len ( word ) \n 
continue \n 
~~ if i + len ( word ) >= width : \n 
~~~ s . write ( + indent + word ) \n 
~~ s . write ( + word ) \n 
i += len ( word ) + 1 \n 
~~ return s . getvalue ( ) \n 
~~ def formatDefinitions ( options , COLS , presets = { } ) : \n 
~~~ s = StringIO ( ) \n 
for ( longname , default , doc ) in options : \n 
~~~ s . write ( + longname + ) \n 
default = presets . get ( longname , default ) \n 
if type ( default ) in ( IntType , LongType ) : \n 
~~~ default = int ( default ) \n 
~~ ~~ if default is not None : \n 
~~~ doc += + repr ( default ) + \n 
~~ s . write ( splitLine ( doc , COLS , 10 ) ) \n 
s . write ( ) \n 
~~ def usage ( string ) : \n 
~~~ raise ValueError ( string ) \n 
~~ def defaultargs ( options ) : \n 
~~~ l = { } \n 
~~~ if default is not None : \n 
~~~ l [ longname ] = default \n 
~~ ~~ return l \n 
~~ def parseargs ( argv , options , minargs = None , maxargs = None , presets = { } ) : \n 
~~~ config = { } \n 
longkeyed = { } \n 
for option in options : \n 
~~~ longname , default , doc = option \n 
longkeyed [ longname ] = option \n 
config [ longname ] = default \n 
~~~ config [ longname ] = presets [ longname ] \n 
~~ options = [ ] \n 
args = [ ] \n 
pos = 0 \n 
while pos < len ( argv ) : \n 
~~~ if argv [ pos ] [ : 2 ] != : \n 
~~~ args . append ( argv [ pos ] ) \n 
pos += 1 \n 
~~~ if pos == len ( argv ) - 1 : \n 
~~~ usage ( ) \n 
~~ key , value = argv [ pos ] [ 2 : ] , argv [ pos + 1 ] \n 
pos += 2 \n 
if not longkeyed . has_key ( key ) : \n 
~~~ usage ( + key ) \n 
~~ longname , default , doc = longkeyed [ key ] \n 
~~~ t = type ( config [ longname ] ) \n 
if t is NoneType or t is StringType : \n 
~~~ config [ longname ] = value \n 
~~ elif t in ( IntType , LongType ) : \n 
~~~ config [ longname ] = long ( value ) \n 
~~ elif t is FloatType : \n 
~~~ config [ longname ] = float ( value ) \n 
~~~ assert 0 \n 
~~ ~~ except ValueError , e : \n 
~~~ usage ( % ( key , str ( e ) ) ) \n 
~~ ~~ ~~ for key , value in config . items ( ) : \n 
~~~ if value is None : \n 
~~ ~~ if minargs is not None and len ( args ) < minargs : \n 
~~ if maxargs is not None and len ( args ) > maxargs : \n 
~~ return ( config , args ) \n 
~~ def test_parseargs ( ) : \n 
~~~ assert parseargs ( ( , , , , , , , , ) , ( ( , , ) , ( , assert parseargs ( [ ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
assert parseargs ( [ , , , ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
~~~ parseargs ( [ ] , [ ( , , ) ] ) \n 
~~~ parseargs ( [ , ] , [ ] ) \n 
~~~ parseargs ( [ ] , [ ] , 1 , 2 ) \n 
~~ assert parseargs ( [ ] , [ ] , 1 , 2 ) == ( { } , [ ] ) \n 
assert parseargs ( [ , ] , [ ] , 1 , 2 ) == ( { } , [ , ] ) \n 
~~~ parseargs ( [ , , ] , [ ] , 1 , 2 ) \n 
~~~ parseargs ( [ , ] , [ ( , 3 , ) ] ) \n 
~~~ parseargs ( [ , ] , [ ( , 2.1 , ) ] ) \n 
~~ ~~ import datetime \n 
from south . db import db \n 
from south . v2 import SchemaMigration \n 
class Migration ( SchemaMigration ) : \n 
~~~ db . add_column ( , , \n 
self . gf ( ) ( to = orm [ keep_default = False ) \n 
~~~ db . delete_column ( , ) \n 
: { : } , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'auth.Permission\']" } , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.Group\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" : ( , [ ] , { : , : } , \n 
: ( , [ ] , { : , : } , \n 
: ( , [ ] , { : "\'taggit_taggeditem_tagged_items\'" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "\'taggit_taggeditem_items\'" } , \n 
: ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.User\']" , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : } \n 
from collections import namedtuple \n 
from shutil import rmtree \n 
from stat import S_IFDIR , S_IFREG , S_IFLNK \n 
from pygit2 import ( clone_repository , Signature , GIT_SORT_TOPOLOGICAL , \n 
GIT_FILEMODE_TREE , GIT_STATUS_CURRENT , \n 
GIT_FILEMODE_LINK , GIT_FILEMODE_BLOB , GIT_BRANCH_REMOTE , \n 
GIT_BRANCH_LOCAL , GIT_FILEMODE_BLOB_EXECUTABLE ) \n 
from six import iteritems \n 
from gitfs . cache import CommitCache \n 
from gitfs . log import log \n 
from gitfs . utils . path import split_path_into_components \n 
from gitfs . utils . commits import CommitsList \n 
DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n 
"first_commits" , "second_commits" ] ) \n 
class Repository ( object ) : \n 
~~~ def __init__ ( self , repository , commits = None ) : \n 
~~~ self . _repo = repository \n 
self . commits = commits or CommitCache ( self ) \n 
self . behind = False \n 
~~ def __getitem__ ( self , item ) : \n 
return self . _repo [ item ] \n 
~~ def __getattr__ ( self , attr ) : \n 
if attr not in self . __dict__ : \n 
~~~ return getattr ( self . _repo , attr ) \n 
~~~ return self . __dict__ [ attr ] \n 
~~ ~~ def ahead ( self , upstream , branch ) : \n 
~~~ ahead , _ = self . diverge ( upstream , branch ) \n 
return ahead \n 
~~ def diverge ( self , upstream , branch ) : \n 
~~~ reference = "{}/{}" . format ( upstream , branch ) \n 
remote_branch = self . lookup_branch ( reference , GIT_BRANCH_REMOTE ) \n 
local_branch = self . lookup_branch ( branch , GIT_BRANCH_LOCAL ) \n 
if remote_branch . target == local_branch . target : \n 
~~~ return False , False \n 
~~ diverge_commits = self . find_diverge_commits ( local_branch , \n 
remote_branch ) \n 
behind = len ( diverge_commits . second_commits ) > 0 \n 
ahead = len ( diverge_commits . first_commits ) > 0 \n 
return ahead , behind \n 
~~ def checkout ( self , ref , * args , ** kwargs ) : \n 
~~~ result = self . _repo . checkout ( ref , * args , ** kwargs ) \n 
self . ignore . update ( ) \n 
status = self . _repo . status ( ) \n 
for path , status in iteritems ( status ) : \n 
~~~ if status == GIT_STATUS_CURRENT : \n 
~~ full_path = self . _full_path ( path ) \n 
if path not in self . _repo . index : \n 
~~~ if path not in self . ignore : \n 
~~~ os . unlink ( full_path ) \n 
~~ except OSError : \n 
~~~ rmtree ( \n 
full_path , \n 
onerror = lambda function , fpath , excinfo : log . info ( \n 
~~ ~~ continue \n 
~~ stats = self . get_git_object_default_stats ( ref , path ) \n 
current_stat = os . lstat ( full_path ) \n 
if stats [ ] != current_stat . st_mode : \n 
~~~ os . chmod ( full_path , current_stat . st_mode ) \n 
self . _repo . index . add ( self . _sanitize ( path ) ) \n 
~~ ~~ return result \n 
~~ def _sanitize ( self , path ) : \n 
~~~ if path is not None and path . startswith ( "/" ) : \n 
~~~ path = path [ 1 : ] \n 
~~ return path \n 
~~ def push ( self , upstream , branch , credentials ) : \n 
remote = self . get_remote ( upstream ) \n 
remote . push ( [ "refs/heads/%s" % ( branch ) ] , callbacks = credentials ) \n 
~~ def fetch ( self , upstream , branch_name , credentials ) : \n 
remote . fetch ( callbacks = credentials ) \n 
_ , behind = self . diverge ( upstream , branch_name ) \n 
self . behind = behind \n 
return behind \n 
~~ def commit ( self , message , author , commiter , parents = None , ref = "HEAD" ) : \n 
if status == { } : \n 
~~ author = Signature ( author [ 0 ] , author [ 1 ] ) \n 
commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n 
tree = self . _repo . index . write_tree ( ) \n 
self . _repo . index . write ( ) \n 
if parents is None : \n 
~~~ parents = [ self . _repo . revparse_single ( ref ) . id ] \n 
~~ return self . _repo . create_commit ( ref , author , commiter , message , \n 
tree , parents ) \n 
def clone ( cls , remote_url , path , branch = None , credentials = None ) : \n 
repo = clone_repository ( remote_url , path , checkout_branch = branch , \n 
callbacks = credentials ) \n 
repo . checkout_head ( ) \n 
return cls ( repo ) \n 
~~ def _is_searched_entry ( self , entry_name , searched_entry , path_components ) : \n 
return ( entry_name == searched_entry and \n 
len ( path_components ) == 1 and \n 
entry_name == path_components [ 0 ] ) \n 
~~ def _get_git_object ( self , tree , obj_name , path_components , modifier ) : \n 
git_obj = None \n 
for entry in tree : \n 
~~~ if self . _is_searched_entry ( entry . name , obj_name , path_components ) : \n 
~~~ return modifier ( entry ) \n 
~~ elif entry . filemode == GIT_FILEMODE_TREE : \n 
~~~ git_obj = self . _get_git_object ( self . _repo [ entry . id ] , obj_name , \n 
path_components [ 1 : ] , modifier ) \n 
if git_obj : \n 
~~~ return git_obj \n 
~~ ~~ ~~ return git_obj \n 
~~ def get_git_object_type ( self , tree , path ) : \n 
path_components = split_path_into_components ( path ) \n 
~~~ return self . _get_git_object ( tree , path_components [ - 1 ] , \n 
path_components , \n 
lambda entry : entry . filemode ) \n 
~~~ return GIT_FILEMODE_TREE \n 
~~ ~~ def get_git_object ( self , tree , path ) : \n 
return self . _get_git_object ( tree , path_components [ - 1 ] , path_components , \n 
lambda entry : self . _repo [ entry . id ] ) \n 
~~ def get_git_object_default_stats ( self , ref , path ) : \n 
~~~ types = { \n 
GIT_FILEMODE_LINK : { \n 
: S_IFLNK | 0o444 , \n 
} , GIT_FILEMODE_TREE : { \n 
: S_IFDIR | 0o555 , \n 
: 2 \n 
} , GIT_FILEMODE_BLOB : { \n 
: S_IFREG | 0o444 , \n 
} , GIT_FILEMODE_BLOB_EXECUTABLE : { \n 
: S_IFREG | 0o555 , \n 
if path == "/" : \n 
~~~ return types [ GIT_FILEMODE_TREE ] \n 
~~ obj_type = self . get_git_object_type ( ref , path ) \n 
if obj_type is None : \n 
~~~ return obj_type \n 
~~ stats = types [ obj_type ] \n 
if obj_type in [ GIT_FILEMODE_BLOB , GIT_FILEMODE_BLOB_EXECUTABLE ] : \n 
~~~ stats [ ] = self . get_blob_size ( ref , path ) \n 
~~ return stats \n 
~~ def get_blob_size ( self , tree , path ) : \n 
return self . get_git_object ( tree , path ) . size \n 
~~ def get_blob_data ( self , tree , path ) : \n 
return self . get_git_object ( tree , path ) . data \n 
~~ def get_commit_dates ( self ) : \n 
return list ( self . commits . keys ( ) ) \n 
~~ def get_commits_by_date ( self , date ) : \n 
return list ( map ( str , self . commits [ date ] ) ) \n 
~~ def walk_branches ( self , sort , * branches ) : \n 
iterators = [ self . _repo . walk ( branch . target , sort ) \n 
for branch in branches ] \n 
stop_iteration = [ False for branch in branches ] \n 
commits = [ ] \n 
for iterator in iterators : \n 
~~~ commit = next ( iterator ) \n 
~~ except StopIteration : \n 
~~~ commit = None \n 
~~ commits . append ( commit ) \n 
~~ yield ( commit for commit in commits ) \n 
while not all ( stop_iteration ) : \n 
~~~ for index , iterator in enumerate ( iterators ) : \n 
commits [ index ] = commit \n 
~~~ stop_iteration [ index ] = True \n 
~~ ~~ if not all ( stop_iteration ) : \n 
~~~ yield ( commit for commit in commits ) \n 
~~ ~~ ~~ def remote_head ( self , upstream , branch ) : \n 
~~~ ref = "%s/%s" % ( upstream , branch ) \n 
remote = self . _repo . lookup_branch ( ref , GIT_BRANCH_REMOTE ) \n 
return remote . get_object ( ) \n 
~~ def get_remote ( self , name ) : \n 
remote = [ remote for remote in self . _repo . remotes \n 
if remote . name == name ] \n 
if not remote : \n 
~~ return remote [ 0 ] \n 
~~ def _full_path ( self , partial ) : \n 
~~~ if partial . startswith ( "/" ) : \n 
~~~ partial = partial [ 1 : ] \n 
~~ return os . path . join ( self . _repo . workdir , partial ) \n 
~~ def find_diverge_commits ( self , first_branch , second_branch ) : \n 
common_parent = None \n 
first_commits = CommitsList ( ) \n 
second_commits = CommitsList ( ) \n 
walker = self . walk_branches ( GIT_SORT_TOPOLOGICAL , \n 
first_branch , second_branch ) \n 
for first_commit , second_commit in walker : \n 
~~~ if ( first_commit in second_commits or \n 
second_commit in first_commits ) : \n 
~~ if first_commit not in first_commits : \n 
~~~ first_commits . append ( first_commit ) \n 
~~ if second_commit not in second_commits : \n 
~~~ second_commits . append ( second_commit ) \n 
~~ if second_commit . hex == first_commit . hex : \n 
~~~ index = second_commits . index ( first_commit ) \n 
~~~ second_commits = second_commits [ : index ] \n 
common_parent = first_commit \n 
~~~ index = first_commits . index ( second_commit ) \n 
~~~ first_commits = first_commits [ : index ] \n 
common_parent = second_commit \n 
~~ return DivergeCommits ( common_parent , first_commits , second_commits ) \n 
~~ ~~ from datetime import datetime \n 
from mock import MagicMock , call \n 
from pygit2 import GIT_SORT_TIME \n 
from gitfs . cache . commits import Commit , CommitCache \n 
class TestCommit ( object ) : \n 
~~~ def test_commit ( self ) : \n 
~~~ commit = Commit ( 1 , 1 , 1 ) \n 
new_commit = Commit ( 2 , 2 , "21111111111" ) \n 
assert new_commit > commit \n 
assert repr ( new_commit ) == "2-2111111111" \n 
~~ ~~ class TestCommitCache ( object ) : \n 
~~~ def test_cache ( self ) : \n 
~~~ mocked_repo = MagicMock ( ) \n 
mocked_commit = MagicMock ( ) \n 
mocked_repo . lookup_reference ( ) . resolve ( ) . target = "head" \n 
mocked_repo . walk . return_value = [ mocked_commit ] \n 
mocked_commit . commit_time = 1411135000 \n 
mocked_commit . hex = \n 
cache = CommitCache ( mocked_repo ) \n 
cache . update ( ) \n 
cache [ ] = Commit ( 1 , 1 , "1111111111" ) \n 
assert sorted ( cache . keys ( ) ) == [ , ] \n 
asserted_time = datetime . fromtimestamp ( mocked_commit . commit_time ) \n 
asserted_time = "{}-{}-{}" . format ( asserted_time . hour , asserted_time . minute , \n 
asserted_time . second ) \n 
assert repr ( cache [ ] ) == % asserted_time \n 
del cache [ ] \n 
for commit_date in cache : \n 
~~~ assert commit_date == \n 
~~ mocked_repo . lookup_reference . has_calls ( [ call ( "HEAD" ) ] ) \n 
mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n 
assert mocked_repo . lookup_reference ( ) . resolve . call_count == 2 \n 
~~ ~~ import pytest \n 
import datetime as dt \n 
from mock import MagicMock \n 
from gitfs . utils . strptime import TimeParser \n 
from gitfs . utils import strptime \n 
class TestDateTimeUtils ( object ) : \n 
~~~ def test_strptime ( self ) : \n 
~~~ date = dt . date ( 2014 , 8 , 21 ) \n 
datetime = dt . datetime ( 2014 , 8 , 21 , 1 , 2 , 3 ) \n 
to_datetime = True ) == datetime \n 
date = dt . date ( 2014 , 8 , 30 ) \n 
datetime = dt . datetime ( 2014 , 8 , 30 , 1 , 2 , 3 ) \n 
date = dt . date ( 1970 , 1 , 1 ) \n 
datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n 
with pytest . raises ( ValueError ) : \n 
~~ ~~ def test_time_parser_match_with_value_error ( self ) : \n 
~~~ mocked_pattern = MagicMock ( ) \n 
mocked_pattern . match . return_value = False \n 
parser . pattern = mocked_pattern \n 
~~~ parser . match ( "daytime" ) \n 
~~ mocked_pattern . match . assert_called_once_with ( "daytime" ) \n 
~~ ~~ from zipa import api_github_com as github \n 
repos = github . orgs . django . repos \n 
for repo in repos [ { : , : } ] : \n 
~~~ print repo . name \n 
~~~ from django . conf . urls import patterns , url \n 
~~ urlpatterns = patterns ( , \n 
url ( , , name = ) , \n 
url ( , , name = ) , ) \n 
from django . http import * \n 
from django . core import serializers \n 
from django . core . exceptions import ValidationError , SuspiciousOperation , ObjectDoesNotExist \n 
from django . db import IntegrityError , connection , transaction \n 
from django . shortcuts import render_to_response \n 
from django . core . context_processors import csrf \n 
from django . contrib . comments . models import Comment \n 
from django . contrib . comments . forms import CommentForm \n 
from django . contrib . contenttypes . models import ContentType \n 
from django . contrib . auth . decorators import login_required , user_passes_test \n 
from django . contrib . sessions . models import Session \n 
from django . contrib . sessions . backends . db import SessionStore \n 
from django . contrib . gis . geos . collections import MultiPolygon \n 
from django . contrib . gis . geos import GEOSGeometry \n 
from django . contrib . gis . gdal import * \n 
from django . contrib . gis . gdal . libgdal import lgdal \n 
from django . contrib import humanize \n 
from django . template import loader , Context as DjangoContext , RequestContext \n 
from django . utils import simplejson as json , translation \n 
from django . utils . translation import ugettext as _ , ungettext as _n \n 
from django . template . defaultfilters import slugify , force_escape \n 
from tagging . utils import parse_tag_input \n 
from datetime import datetime , time , timedelta \n 
from decimal import * \n 
from functools import wraps \n 
from redistricting . calculators import * \n 
from redistricting . models import * \n 
from redistricting . tasks import * \n 
import random , string , math , types , copy , time , threading , traceback , os \n 
import commands , sys , tempfile , csv , hashlib , inflect , logging \n 
import ModestMaps \n 
from PIL import Image , ImageChops , ImageMath \n 
import urllib , urllib2 \n 
from xhtml2pdf . pisa import CreatePDF \n 
import StringIO \n 
UNASSIGNED_DISTRICT_ID = 0 \n 
def using_unique_session ( u ) : \n 
if u . is_anonymous ( ) or u . is_superuser : \n 
~~ sessions = Session . objects . all ( ) \n 
count = 0 \n 
for session in sessions : \n 
~~~ decoded = session . get_decoded ( ) \n 
if in decoded and decoded [ ] == u . id : \n 
~~~ if in decoded and decoded [ ] < datetime . now ( ) : \n 
~~~ Session . objects . filter ( session_key = session . session_key ) . delete ( ) \n 
~~~ count += 1 \n 
~~ ~~ ~~ except SuspiciousOperation : \n 
~~ ~~ for session in sessions : \n 
~~~ websession = SessionStore ( session_key = session . session_key ) \n 
websession [ ] = count \n 
websession . save ( ) \n 
~~ ~~ except SuspiciousOperation : \n 
~~ ~~ return ( count <= 1 ) \n 
~~ def unique_session_or_json_redirect ( function ) : \n 
def decorator ( request , * args , ** kwargs ) : \n 
~~~ def return_nonunique_session_result ( ) : \n 
~~~ status = { : False } \n 
status [ ] = _ ( \n 
status [ ] = \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ if not using_unique_session ( request . user ) : \n 
~~~ return return_nonunique_session_result ( ) \n 
~~~ return function ( request , * args , ** kwargs ) \n 
~~ ~~ return wraps ( function ) ( decorator ) \n 
~~ def is_session_available ( req ) : \n 
if req . user . is_superuser or req . user . is_staff : \n 
~~ sessions = Session . objects . filter ( expire_date__gt = datetime . now ( ) ) \n 
if ( not req . user . is_anonymous ( ) ) and in decoded and decoded [ ~~~ count += 1 \n 
~~ ~~ avail = count < settings . CONCURRENT_SESSIONS \n 
req . session [ ] = avail \n 
return avail \n 
~~ def note_session_activity ( req ) : \n 
window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n 
req . session [ ] = datetime . now ( ) + window \n 
def unloadplan ( request , planid ) : \n 
note_session_activity ( request ) \n 
status = { : False } \n 
ps = Plan . objects . filter ( pk = planid ) \n 
if len ( ps ) > 0 : \n 
~~~ p = ps [ 0 ] \n 
if not can_copy ( request . user , p ) : \n 
~~ if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
~~ ~~ status [ ] = True \n 
@ unique_session_or_json_redirect \n 
def copyplan ( request , planid ) : \n 
if not is_plan_ready ( planid ) : \n 
~~ status = { : False } \n 
p = Plan . objects . get ( pk = planid ) \n 
if ( request . method == "POST" ) : \n 
~~~ newname = request . POST [ "name" ] [ 0 : 200 ] \n 
shared = request . POST . get ( "shared" , False ) \n 
~~ plan_copy = Plan . objects . filter ( name = newname , owner = request . user , legislative_body = p . legislative_body \n 
if len ( plan_copy ) > 0 : \n 
~~ plan_copy = Plan ( name = newname , owner = request . user , is_shared = shared , legislative_body = p . legislative_body plan_copy . create_unassigned = False \n 
plan_copy . save ( ) \n 
districts = p . get_districts_at_version ( p . version , include_geom = True ) \n 
for district in districts : \n 
~~~ district_copy = copy . copy ( district ) \n 
district_copy . id = None \n 
district_copy . version = 0 \n 
district_copy . is_locked = False \n 
district_copy . plan = plan_copy \n 
~~~ district_copy . save ( ) \n 
~~ except Exception as inst : \n 
status [ "exception" ] = inst . message \n 
~~ district_copy . clone_relations_from ( district ) \n 
~~ data = serializers . serialize ( "json" , [ plan_copy ] ) \n 
return HttpResponse ( data , mimetype = ) \n 
def scoreplan ( request , planid ) : \n 
plan = Plan . objects . get ( pk = planid ) \n 
criterion = ValidationCriteria . objects . filter ( legislative_body = plan . legislative_body ) \n 
status [ ] = True \n 
for criteria in criterion : \n 
~~~ score = ComputedPlanScore . compute ( criteria . function , plan ) \n 
~~~ logger . debug ( traceback . format_exc ( ) ) \n 
~~ if not score or not score [ ] : \n 
~~~ status [ ] = False \n 
status [ ] = % ( criteria . get_short_label ( ) , criteria . get_long_description break \n 
~~ ~~ if status [ ] : \n 
~~~ status [ ] = True \n 
plan . is_valid = True \n 
plan . save ( ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ def get_user_info ( user ) : \n 
if user . is_anonymous ( ) : \n 
~~ profile = user . get_profile ( ) \n 
return { \n 
: user . username , \n 
: user . email , \n 
: profile . pass_hint , \n 
: user . first_name , \n 
: user . last_name , \n 
: profile . organization , \n 
: user . id \n 
~~ def commonplan ( request , planid ) : \n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
plan . edited = getutc ( plan . edited ) \n 
levels = plan . legislative_body . get_geolevels ( ) \n 
districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
editable = can_edit ( request . user , plan ) \n 
default_demo = plan . legislative_body . get_default_subject ( ) \n 
max_dists = plan . legislative_body . max_districts \n 
body_member_short_label = plan . legislative_body . get_short_label ( ) \n 
body_member_long_label = plan . legislative_body . get_label ( ) \n 
body_members = plan . legislative_body . get_members_label ( ) \n 
reporting_template = % plan . legislative_body . name if not plan . is_community ( ) \n 
index = body_member_short_label . find ( ) \n 
if index >= 0 : \n 
~~~ body_member_short_label = body_member_short_label [ 0 : index ] \n 
~~ index = body_member_long_label . find ( ) \n 
~~~ body_member_long_label = body_member_long_label [ 0 : index ] \n 
~~ if not editable and not can_view ( request . user , plan ) : \n 
~~~ plan = { } \n 
tags = [ ] \n 
calculator_reports = [ ] \n 
~~~ tags = Tag . objects . filter ( name__startswith = ) . order_by ( ) . values_list ( , flat tags = map ( lambda x : x [ 5 : ] , tags ) \n 
if settings . REPORTS_ENABLED == : \n 
~~~ report_displays = ScoreDisplay . objects . filter ( name = "%s_reports" % plan . legislative_body if len ( report_displays ) > 0 : \n 
~~~ calculator_reports = map ( lambda p : { \n 
: p . __unicode__ ( ) , \n 
: map ( lambda f : { \n 
: f . get_label ( ) , \n 
: f . id \n 
} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n 
levels = list ( ) \n 
districts = { } \n 
editable = False \n 
default_demo = None \n 
max_dists = 0 \n 
body_member_short_label = \n 
body_member_long_label = _ ( ) + \n 
body_members = _n ( , , 2 ) \n 
reporting_template = None \n 
~~ demos = Subject . objects . all ( ) . order_by ( ) [ 0 : 3 ] \n 
layers = [ ] \n 
snaplayers = [ ] \n 
if len ( levels ) > 0 : \n 
~~~ study_area_extent = list ( levels [ 0 ] . geounit_set . extent ( field_name = ) ) \n 
~~~ for lb in LegislativeBody . objects . all ( ) : \n 
~~~ biglevel = lb . get_geolevels ( ) [ 0 ] \n 
if biglevel . geounit_set . count ( ) > 0 : \n 
~~~ study_area_extent = biglevel . geounit_set . extent ( field_name = ) \n 
~~ ~~ ~~ for level in levels : \n 
~~~ snaplayers . append ( { \n 
: level . id , \n 
: level . name , \n 
: + level . name , \n 
: level . get_long_description ( ) , \n 
: level . min_zoom \n 
~~ default_selected = False \n 
for demo in demos : \n 
~~~ isdefault = str ( ( not default_demo is None ) and ( demo . id == default_demo . id ) ) . lower ( ) \n 
if isdefault == : \n 
~~~ default_selected = True \n 
~~ layers . append ( { \n 
: demo . id , \n 
: demo . get_short_label ( ) , \n 
: demo . name , \n 
: isdefault , \n 
: str ( demo . is_displayed ) . lower ( ) \n 
~~ if default_demo and not default_selected : \n 
~~~ layers . insert ( 0 , { \n 
: default_demo . id , \n 
: default_demo . get_short_label ( ) , \n 
: default_demo . name , \n 
: str ( True ) . lower ( ) , \n 
: str ( default_demo . is_displayed ) . lower ( ) \n 
~~ if in settings . __members__ : \n 
~~~ mapserver_protocol = settings . MAP_SERVER_PROTOCOL \n 
~~~ mapserver_protocol = \n 
~~ short_label = body_member_short_label . strip ( ) . lower ( ) \n 
long_label = body_member_long_label . strip ( ) . lower ( ) \n 
has_regions = Region . objects . all ( ) . count ( ) > 1 \n 
bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n 
l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n 
~~~ loader . get_template ( reporting_template ) \n 
~~~ reporting_template = None \n 
~~ return RequestContext ( request , { \n 
: bodies , \n 
: has_regions , \n 
: l_bodies , \n 
: plan , \n 
: districts , \n 
: settings . MAP_SERVER , \n 
: mapserver_protocol , \n 
: settings . BASE_MAPS , \n 
: settings . MAP_SERVER_NS , \n 
: settings . MAP_SERVER_NSHREF , \n 
: settings . FEATURE_LIMIT , \n 
: settings . ADJACENCY , \n 
: settings . CONVEX_CHOROPLETH , \n 
: layers , \n 
: snaplayers , \n 
: UNASSIGNED_DISTRICT_ID , \n 
: request . user . username != and request . user . username != , \n 
: settings . DEBUG and request . user . is_staff , \n 
: get_user_info ( request . user ) , \n 
: editable , \n 
: max_dists + 1 , \n 
: settings . GA_ACCOUNT , \n 
: settings . GA_DOMAIN , \n 
: short_label , \n 
: long_label , \n 
: body_members , \n 
: reporting_template , \n 
: study_area_extent , \n 
: len ( ScoreDisplay . objects . filter ( is_page = True ) ) > 0 , \n 
: json . dumps ( calculator_reports ) , \n 
: ( in settings . __members__ ) , \n 
: tags , \n 
: Site . objects . get_current ( ) , \n 
: translation . get_language ( ) , \n 
~~ def is_plan_ready ( planid ) : \n 
planid = int ( planid ) \n 
return planid == 0 or len ( Plan . objects . filter ( id = planid , processing_state = ProcessingState . READY ) \n 
~~ @ user_passes_test ( using_unique_session ) \n 
def viewplan ( request , planid ) : \n 
if not is_session_available ( request ) or not is_plan_ready ( planid ) : \n 
~~ if not request . user . is_anonymous ( ) and ( int ( planid ) == 0 ) and ( settings . MAX_UNDOS_AFTER_EDIT > 0 ~~~ for p in Plan . objects . filter ( owner = request . user ) : \n 
~~ ~~ return render_to_response ( , commonplan ( request , planid ) ) \n 
def editplan ( request , planid ) : \n 
if request . user . is_anonymous ( ) or not is_session_available ( request ) or not is_plan_ready ( planid ) ~~~ return HttpResponseRedirect ( ) \n 
~~ cfg = commonplan ( request , planid ) \n 
if cfg [ ] == False : \n 
~~~ return HttpResponseRedirect ( % planid ) \n 
~~ plan = Plan . objects . get ( id = planid , owner = request . user ) \n 
cfg [ ] = len ( cfg [ ] ) > plan . legislative_body . max_districts \n 
cfg [ ] = plan . get_available_districts ( ) \n 
if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ plan . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
~~ return render_to_response ( , cfg ) \n 
def printplan ( request , planid ) : \n 
if not is_session_available ( request ) : \n 
sha = hashlib . sha1 ( ) \n 
sha . update ( str ( planid ) + str ( datetime . now ( ) ) ) \n 
cfg [ ] = % sha . hexdigest ( ) \n 
cfg [ ] = % request . META [ ] \n 
~~~ if not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST : \n 
~~ height = 500 * 2 \n 
if in request . REQUEST : \n 
~~~ height = int ( request . REQUEST [ ] ) * 2 \n 
~~ width = 1024 * 2 \n 
~~~ width = int ( request . REQUEST [ ] ) * 2 \n 
~~ opacity = 0.8 \n 
~~~ opacity = float ( request . REQUEST [ ] ) \n 
~~ full_legend = json . loads ( request . REQUEST [ ] ) \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = Plan . objects . get ( id = int ( request . REQUEST [ ] ) ) \n 
cfg [ ] = datetime . now ( ) \n 
bbox = request . REQUEST [ ] . split ( ) \n 
pt1 = Point ( float ( bbox [ 0 ] ) , float ( bbox [ 1 ] ) , srid = 3785 ) \n 
pt1 . transform ( SpatialReference ( ) ) \n 
ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n 
pt2 = Point ( float ( bbox [ 2 ] ) , float ( bbox [ 3 ] ) , srid = 3785 ) \n 
pt2 . transform ( SpatialReference ( ) ) \n 
ur = ModestMaps . Geo . Location ( pt2 . y , pt2 . x ) \n 
dims = ModestMaps . Core . Point ( width , height ) \n 
provider = ModestMaps . OpenStreetMap . Provider ( ) \n 
basemap = ModestMaps . mapByExtent ( provider , ll , ur , dims ) \n 
fullImg = basemap . draw ( ) \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: 512 , \n 
: 512 \n 
overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n 
maskImg = ImageChops . invert ( overlayImg ) \n 
: request . REQUEST [ ] , \n 
overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n 
fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n 
fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n 
t = loader . get_template ( ) \n 
page = t . render ( DjangoContext ( cfg ) ) \n 
result = StringIO . StringIO ( ) \n 
CreatePDF ( page , result , show_error_as_pdf = True ) \n 
response = HttpResponse ( result . getvalue ( ) , mimetype = ) \n 
response [ ] = \n 
return response \n 
~~ ~~ @ login_required \n 
def createplan ( request ) : \n 
if request . method == "POST" : \n 
~~~ name = request . POST [ ] [ 0 : 200 ] \n 
body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n 
plan = Plan ( name = name , owner = request . user , legislative_body = body , processing_state = ProcessingState try : \n 
~~~ plan . save ( ) \n 
status = serializers . serialize ( "json" , [ plan ] ) \n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ @ unique_session_or_json_redirect \n 
def uploadfile ( request ) : \n 
if request . user . is_anonymous ( ) : \n 
~~ status = commonplan ( request , 0 ) \n 
index_file = request . FILES . get ( , False ) \n 
if not index_file : \n 
return render_to_response ( , status ) \n 
~~~ filename = index_file . name \n 
~~ if index_file . size > settings . MAX_UPLOAD_SIZE : \n 
~~~ logger . error ( ) \n 
status [ ] = False \n 
~~ if not filename . endswith ( ( , ) ) : \n 
~~ elif request . POST [ ] == : \n 
~~~ dest = tempfile . NamedTemporaryFile ( mode = , delete = False ) \n 
for chunk in request . FILES [ ] . chunks ( ) : \n 
~~~ dest . write ( chunk ) \n 
~~ dest . close ( ) \n 
if request . FILES [ ] . name . endswith ( ) : \n 
~~~ os . rename ( dest . name , % ( dest . name , ) ) \n 
filename = % ( dest . name , ) \n 
~~~ filename = dest . name \n 
~~ ~~ except Exception as ex : \n 
logger . error ( , ex ) \n 
~~ DistrictIndexFile . index2plan . delay ( request . POST [ ] , request . POST [ \n 
~~ return render_to_response ( , status ) \n 
~~ def generate_report_hash ( qdict ) : \n 
params = qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) \n 
sha . update ( params ) \n 
return sha . hexdigest ( ) \n 
def getreport ( request , planid ) : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~~ status [ ] = _ ( ) \n 
~~ if not can_view ( request . user , plan ) : \n 
~~ if not settings . REPORTS_ENABLED is None : \n 
~~ if request . method != : \n 
~~ stamp = request . POST . get ( , generate_report_hash ( request . POST ) ) \n 
rptstatus = PlanReport . checkreport ( planid , stamp ) \n 
if rptstatus == : \n 
~~~ status = { \n 
: PlanReport . getreport ( planid , stamp ) , \n 
: _ ( ) , \n 
: stamp \n 
~~ elif rptstatus == : \n 
: reverse ( getreport , args = [ planid ] ) , \n 
: 10 , \n 
req = { \n 
: request . POST . get ( , ) , \n 
: request . POST . getlist ( ) , \n 
: request . POST . get ( , ) \n 
PlanReport . markpending ( planid , stamp ) \n 
PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n 
~~~ status [ ] = _ ( \n 
def getcalculatorreport ( request , planid ) : \n 
~~ function_ids = request . POST . get ( , ) \n 
sha . update ( function_ids ) \n 
stamp = request . POST . get ( , sha . hexdigest ( ) ) \n 
rptstatus = CalculatorReport . checkreport ( planid , stamp ) \n 
: CalculatorReport . getreport ( planid , stamp ) , \n 
: reverse ( getcalculatorreport , args = [ planid ] ) , \n 
: 5 , \n 
req = { : function_ids } \n 
CalculatorReport . markpending ( planid , stamp ) \n 
CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n 
def newdistrict ( request , planid ) : \n 
if len ( request . REQUEST . items ( ) ) >= 3 : \n 
~~~ plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
~~~ geolevel = request . REQUEST [ ] \n 
~~~ geolevel = None \n 
~~ if in request . REQUEST : \n 
~~~ geounit_ids = string . split ( request . REQUEST [ ] , ) \n 
~~~ geounit_ids = None \n 
~~~ district_id = int ( request . REQUEST [ ] ) \n 
~~~ district_id = None \n 
~~~ district_short = request . REQUEST [ ] [ 0 : 10 ] \n 
~~ elif not district_id is None : \n 
~~~ district_short = plan . legislative_body . get_short_label ( ) % { : district_id } \n 
~~~ district_short = None \n 
~~~ district_long = request . REQUEST [ ] [ 0 : 256 ] \n 
~~~ district_long = plan . legislative_body . get_label ( ) % { : district_id } \n 
~~~ district_long = None \n 
~~~ version = request . REQUEST [ ] \n 
~~~ version = plan . version \n 
~~ if geolevel and geounit_ids and district_id : \n 
~~~ fixed = plan . add_geounits ( ( district_id , district_short , district_long , ) , geounit_ids \n 
district = plan . district_set . filter ( district_id = district_id , short_label = district_short if plan . legislative_body . multi_members_allowed : \n 
~~~ district . num_members = plan . legislative_body . min_multi_district_members \n 
district . save ( ) \n 
~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
if in request . POST and request . POST [ ] != : \n 
~~~ comment = Comment ( \n 
object_pk = district . id , \n 
content_type = ct , \n 
site_id = Site . objects . get_current ( ) . id , \n 
user_name = request . user . username , \n 
user_email = request . user . email , \n 
comment = request . POST [ ] ) \n 
comment . save ( ) \n 
~~ if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~~ strtags = request . REQUEST . getlist ( ) \n 
for strtag in strtags : \n 
~~~ if strtag == : \n 
~~ if strtag . count ( ) > 0 : \n 
~~~ strtag = \'"type=%s"\' % strtag \n 
~~~ strtag = % strtag \n 
~~ Tag . objects . add_tag ( district , strtag ) \n 
status [ ] = _ ( ) \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
status [ ] = getutc ( plan . edited ) . isoformat ( ) \n 
status [ ] = district_id \n 
status [ ] = plan . version \n 
~~ except ValidationError : \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
@ transaction . commit_manually \n 
def add_districts_to_plan ( request , planid ) : \n 
~~ if not can_edit ( request . user , plan ) : \n 
~~ district_list = request . POST . getlist ( ) \n 
if len ( district_list ) == 0 : \n 
~~~ districts = District . objects . filter ( id__in = district_list ) \n 
version = int ( request . POST . get ( , None ) ) \n 
status [ ] = _ ( ) % { : len ( districts ) } \n 
~~ allowed_districts = plan . get_available_districts ( version = version ) \n 
if len ( districts ) > allowed_districts : \n 
~~~ status [ ] = _ ( ) % { : allowed_districts } \n 
~~~ results = plan . paste_districts ( districts , version = version ) \n 
transaction . commit ( ) \n 
status [ ] = _ ( ) % { status [ ] = plan . version \n 
~~ except Exception as ex : \n 
~~~ transaction . rollback ( ) \n 
status [ ] = str ( ex ) \n 
status [ ] = traceback . format_exc ( ) \n 
def assign_district_members ( request , planid ) : \n 
~~ leg_bod = plan . legislative_body \n 
if ( not leg_bod . multi_members_allowed ) : \n 
~~ districts = request . POST . getlist ( ) \n 
counts = request . POST . getlist ( ) \n 
~~~ changed = 0 \n 
for i in range ( 0 , len ( districts ) ) : \n 
~~~ id = int ( districts [ i ] ) \n 
count = int ( counts [ i ] ) \n 
district = District . objects . filter ( plan = plan , district_id = id , version__lte = version ) . order_by \n 
if district . num_members != count : \n 
~~~ if ( changed == 0 ) : \n 
~~~ if version != plan . version : \n 
~~~ plan . purge ( after = version ) \n 
~~ plan . version = plan . version + 1 \n 
~~ plan . update_num_members ( district , count ) \n 
changed += 1 \n 
~~ ~~ transaction . commit ( ) \n 
status [ ] = changed \n 
) % { : changed } \n 
logger . warn ( ) \n 
def combine_districts ( request , planid ) : \n 
~~ version = int ( request . POST . get ( , plan . version ) ) \n 
from_id = int ( request . POST . get ( , - 1 ) ) \n 
to_id = int ( request . POST . get ( , None ) ) \n 
~~~ all_districts = plan . get_districts_at_version ( version , include_geom = True ) \n 
from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n 
locked = to_district . is_locked \n 
for district in from_districts : \n 
~~~ if district . is_locked : \n 
~~~ locked = True \n 
~~ ~~ if locked : \n 
~~ result = plan . combine_districts ( to_district , from_districts , version = version ) \n 
if result [ 0 ] == True : \n 
status [ ] = result [ 1 ] \n 
~~ ~~ except Exception , ex : \n 
def fix_unassigned ( request , planid ) : \n 
~~~ version = int ( request . POST . get ( , plan . version ) ) \n 
result = plan . fix_unassigned ( version ) \n 
status [ ] = result [ 0 ] \n 
def get_splits ( request , planid , otherid , othertype ) : \n 
otherid = int ( otherid ) \n 
~~ version = int ( request . REQUEST [ ] if in request . REQUEST else plan . version ) \n 
~~~ if othertype == : \n 
~~~ otherplan = Plan . objects . get ( pk = otherid ) \n 
~~ if not can_view ( request . user , otherplan ) : \n 
~~ otherversion = int ( request . REQUEST [ ] if in request . REQUEST splits = plan . find_plan_splits ( otherplan , version , otherversion ) \n 
~~ elif othertype == : \n 
~~~ splits = plan . find_geolevel_splits ( otherid , version ) \n 
~~~ status [ ] = _ ( ) % { : othertype } \n 
~~ split_word = _ ( ) if len ( splits ) == 1 else inflect . engine ( ) . plural ( _ ( ) ) \n 
status [ ] = _ ( ) % { : len ( splits ) , : split_word } \n 
status [ ] = splits \n 
status [ ] = list ( set ( [ i [ 0 ] for i in splits ] ) ) \n 
status [ ] = list ( set ( [ i [ 1 ] for i in splits ] ) ) \n 
~~ def get_processing_status ( request ) : \n 
plan_ids = request . REQUEST . getlist ( ) \n 
if len ( plan_ids ) == 0 : \n 
~~~ statuses = { } \n 
for p in Plan . objects . filter ( id__in = plan_ids ) : \n 
~~~ statuses [ str ( p . id ) ] = p . get_processing_state_display ( ) \n 
~~ status [ ] = True \n 
status [ ] = statuses \n 
~~ def get_splits_report ( request , planid ) : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
~~ if not using_unique_session ( request . user ) or not can_view ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
inverse = request . REQUEST [ ] == if in request . REQUEST else False \n 
extended = request . REQUEST [ ] == if in request . REQUEST else False \n 
layers = request . REQUEST . getlist ( ) \n 
if len ( layers ) == 0 : \n 
~~~ report = loader . get_template ( ) \n 
html = \n 
for layer in layers : \n 
~~~ my_context = { : extended } \n 
my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n 
community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n 
~~~ my_context . update ( community_info ) \n 
~~ calc_context = DjangoContext ( my_context ) \n 
html += report . render ( calc_context ) \n 
if not last_item : \n 
~~~ html += \n 
~~ ~~ return HttpResponse ( html , mimetype = ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
def addtodistrict ( request , planid , districtid ) : \n 
if len ( request . REQUEST . items ( ) ) >= 2 : \n 
~~~ geolevel = request . REQUEST [ "geolevel" ] \n 
geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n 
~~~ status [ ] = traceback . format_exc ( ) \n 
~~~ fixed = plan . add_geounits ( districtid , geounit_ids , geolevel , version ) \n 
status [ ] = True ; \n 
status [ ] = _ ( ) % { : fixed } \n 
status [ ] = fixed \n 
def setdistrictlock ( request , planid , district_id ) : \n 
if request . method != : \n 
~~ lock = request . POST . get ( ) . lower ( ) == \n 
version = request . POST . get ( ) \n 
if lock == None : \n 
~~ elif version == None : \n 
district = plan . district_set . filter ( district_id = district_id , version__lte = version ) . order_by ( ~~ except ObjectDoesNotExist : \n 
~~ if plan . owner != request . user : \n 
~~ district . is_locked = lock \n 
status [ ] = _ ( ) % { : _ ( ) if lock else _ ( ) } \n 
def getdistricts ( request , planid ) : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
~~ districts = plan . get_districts_at_version ( version , include_geom = False ) \n 
status [ ] = [ ] \n 
status [ ] = plan . legislative_body . max_districts - len ( districts ) + 1 \n 
max_version = max ( [ d . version for d in districts ] ) \n 
can_undo = max_version > plan . min_version \n 
~~~ status [ ] . append ( { \n 
: district . district_id , \n 
: . join ( map ( _ , district . short_label . split ( ) ) ) , \n 
: . join ( map ( _ , district . long_label . split ( ) ) ) , \n 
: district . version \n 
~~ status [ ] = can_undo \n 
~~ def simple_district_versioned ( request , planid , district_ids = None ) : \n 
status = { : } \n 
~~ subject_id = None \n 
~~~ subject_id = request . REQUEST [ ] \n 
~~ elif plan . legislative_body . get_default_subject ( ) : \n 
~~~ subject_id = plan . legislative_body . get_default_subject ( ) . id \n 
~~ geolevel = plan . legislative_body . get_geolevels ( ) [ 0 ] . id \n 
~~~ geolevel = int ( request . REQUEST [ ] ) \n 
~~~ district_ids = request . REQUEST [ ] \n 
if len ( district_ids ) > 0 : \n 
~~~ district_ids = district_ids . split ( ) \n 
~~~ district_ids = [ ] \n 
~~ ~~ if subject_id : \n 
~~~ bbox = None \n 
~~~ bbox = request . REQUEST [ ] \n 
bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n 
~~~ bbox = plan . district_set . all ( ) . extent ( field_name = ) \n 
~~ status [ ] = plan . get_wfs_districts ( version , subject_id , bbox , geolevel , district_ids ~~ else : \n 
~~~ status [ ] = [ ] \n 
~~ def get_unlocked_simple_geometries ( request , planid ) : \n 
version = request . POST . get ( , plan . version ) \n 
geolevel = request . POST . get ( , plan . legislative_body . get_geolevels ( ) [ 0 ] . id ) \n 
geom = request . POST . get ( , None ) \n 
if geom is not None : \n 
~~~ wkt = request . POST . get ( , None ) \n 
geom = GEOSGeometry ( wkt ) \n 
~~ except GEOSException : \n 
~~~ wkt = request . REQUEST [ ] . replace ( , ) \n 
wkt = wkt . replace ( , ) . replace ( , ) \n 
~~~ geom = GEOSGeometry ( wkt ) \n 
~~~ geom = None \n 
~~ ~~ selection = Q ( geom__intersects = geom ) \n 
districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n 
locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n 
filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n 
features = [ ] \n 
for feature in filtered : \n 
~~~ geom = feature . simple \n 
if locked and geom . intersects ( locked_buffered ) : \n 
~~~ if feature . geom . within ( locked ) : \n 
~~ if feature . geom . overlaps ( locked ) : \n 
~~ ~~ features . append ( { \n 
: % feature . id , \n 
: json . loads ( geom . json ) , \n 
: feature . name , \n 
: geolevel , \n 
: feature . id \n 
~~ status [ ] = features \n 
def get_statistics ( request , planid ) : \n 
~~~ note_session_activity ( request ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = , status = 500 ) \n 
~~~ display = ScoreDisplay . objects . get ( legislative_body = plan . legislative_body , name = "%s_sidebar_demo" ~~ except : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . POST [ ] ) \n 
logger . warn ( str ( request . POST ) ) \n 
~~~ html = display . render ( plan , request , version = version ) \n 
return HttpResponse ( html , mimetype = ) \n 
~~ ~~ def getutc ( t ) : \n 
t_tuple = t . timetuple ( ) \n 
t_seconds = time . mktime ( t_tuple ) \n 
return t . utcfromtimestamp ( t_seconds ) \n 
def getdistrictfilestatus ( request , planid ) : \n 
if not can_copy ( request . user , plan ) : \n 
~~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n 
status [ ] = file_status \n 
status [ ] = ex \n 
def getdistrictfile ( request , planid ) : \n 
~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
if file_status == : \n 
~~~ if is_shape : \n 
~~~ archive = DistrictShapeFile . plan2shape ( plan ) \n 
~~~ archive = DistrictIndexFile . plan2index ( plan ) \n 
~~ response = HttpResponse ( open ( archive . name ) . read ( ) , content_type = ) \n 
~~~ DistrictShapeFile . plan2shape . delay ( plan ) \n 
~~~ DistrictIndexFile . plan2index . delay ( plan ) \n 
~~ response = HttpResponse ( _ ( \n 
def emaildistrictindexfile ( request , planid ) : \n 
~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ DistrictIndexFile . emailfile . delay ( plan , request . user , request . POST , translation . get_language ( ) ) \n 
return HttpResponse ( json . dumps ( { \n 
: _ ( ) } ) , \n 
mimetype = ) \n 
~~ def getvalidplans ( leg_body , owner = None ) : \n 
pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n 
if owner is not None : \n 
~~~ pfilter = pfilter & Q ( owner = owner ) \n 
~~ return list ( Plan . objects . filter ( pfilter ) ) \n 
~~ def getleaderboarddisplay ( leg_body , owner_filter ) : \n 
~~~ return ScoreDisplay . objects . get ( name = "%s_leader_%s" % ( leg_body . name , owner_filter ) ) \n 
~~ ~~ def getleaderboard ( request ) : \n 
if not using_unique_session ( request . user ) : \n 
~~ owner_filter = request . REQUEST [ ] \n 
body_pk = int ( request . REQUEST [ ] ) ; \n 
leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n 
display = getleaderboarddisplay ( leg_body , owner_filter ) \n 
if display is None : \n 
~~ plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
~~~ html = display . render ( plans , request ) \n 
~~ ~~ def getleaderboardcsv ( request ) : \n 
plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
panels = display . scorepanel_set . all ( ) . order_by ( ) \n 
~~~ response = HttpResponse ( mimetype = ) \n 
writer = csv . writer ( response ) \n 
writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n 
for plan in plans : \n 
~~~ row = [ plan . id , plan . name , plan . owner . username ] \n 
for panel in panels : \n 
~~~ function = panel . score_functions . all ( ) [ 0 ] \n 
score = ComputedPlanScore . compute ( function , plan ) \n 
row . append ( score [ ] ) \n 
~~ writer . writerow ( row ) \n 
~~ ~~ def getplans ( request ) : \n 
~~ if request . method == : \n 
~~~ page = int ( request . POST . get ( , 1 ) ) \n 
rows = int ( request . POST . get ( , 10 ) ) \n 
sidx = request . POST . get ( , ) \n 
sord = request . POST . get ( , ) \n 
owner_filter = request . POST . get ( ) ; \n 
body_pk = request . POST . get ( ) ; \n 
body_pk = int ( body_pk ) if body_pk else body_pk ; \n 
search = request . POST . get ( , False ) ; \n 
search_string = request . POST . get ( , ) ; \n 
is_community = request . POST . get ( , False ) == ; \n 
~~ end = page * rows \n 
start = end - rows \n 
if owner_filter == : \n 
~~~ available = Q ( is_template = True ) \n 
~~ elif owner_filter == : \n 
~~~ available = Q ( is_shared = True ) \n 
~~~ if request . user . is_anonymous ( ) : \n 
~~~ available = Q ( owner__exact = request . user ) \n 
~~ ~~ elif owner_filter == : \n 
~~~ available = Q ( is_template = True ) | Q ( is_shared = True ) \n 
if not request . user . is_anonymous ( ) : \n 
~~~ available = available | Q ( owner__exact = request . user ) \n 
~~ not_creating = ~ Q ( processing_state = ProcessingState . CREATING ) & ~ Q ( processing_state = ProcessingState \n 
if sidx . startswith ( ) : \n 
~~~ sidx = sidx [ len ( ) : ] \n 
~~ if sidx == : \n 
~~~ sidx = \n 
~~ if sord == : \n 
~~~ sidx = + sidx \n 
~~ if search : \n 
~~~ search_filter = Q ( name__icontains = search_string ) | Q ( description__icontains = search_string ~~ else : \n 
~~~ search_filter = None \n 
~~ if body_pk : \n 
~~~ body_filter = Q ( legislative_body = body_pk ) \n 
all_plans = Plan . objects . filter ( available , not_creating , body_filter , search_filter ) . order_by ~~ else : \n 
~~~ community_filter = Q ( legislative_body__is_community = is_community ) \n 
all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n 
~~ if all_plans . count ( ) > 0 : \n 
~~~ total_pages = math . ceil ( all_plans . count ( ) / float ( rows ) ) \n 
~~~ total_pages = 1 \n 
~~ plans = all_plans [ start : end ] \n 
plans_list = list ( ) \n 
~~~ plans_list . append ( { \n 
: plan . id , \n 
: plan . name , \n 
: plan . description , \n 
: time . mktime ( plan . edited . timetuple ( ) ) , \n 
: plan . is_template , \n 
: plan . is_shared , \n 
: plan . owner . username , \n 
: can_edit ( request . user , plan ) , \n 
: plan . legislative_body . get_long_description ( ) , \n 
: plan . get_processing_state_display ( ) \n 
~~ def get_shared_districts ( request , planid ) : \n 
~~ all_districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
~~~ plan = None \n 
all_districts = ( ) \n 
~~ if len ( all_districts ) > 0 : \n 
~~~ total_pages = math . ceil ( len ( all_districts ) / float ( rows ) ) \n 
~~ districts = all_districts [ start : end ] \n 
districts_list = list ( ) \n 
~~~ if not district . is_unassigned : \n 
~~~ districts_list . append ( { \n 
: district . id , \n 
: district . short_label , \n 
: district . long_label , \n 
def editplanattributes ( request , planid ) : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
~~ new_name = request . POST . get ( , None ) \n 
new_description = request . POST . get ( , ) \n 
if not planid or not ( new_name or new_description ) : \n 
~~~ return HttpResponseBadRequest ( \n 
_ ( ) ) \n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if not new_name is None : \n 
~~~ plan . name = new_name \n 
~~ plan . description = new_description \n 
def deleteplan ( request , planid ) : \n 
~~ if not planid : \n 
~~~ return HttpResponseBadRequest ( _ ( ) ) \n 
~~~ plan . delete ( ) \n 
def reaggregateplan ( request , planid ) : \n 
~~~ reaggregate_plan . delay ( plan . id ) \n 
plan . processing_state = ProcessingState . REAGGREGATING \n 
~~ def get_health ( request ) : \n 
~~~ def num_users ( minutes ) : \n 
~~~ users = 0 \n 
for session in Session . objects . all ( ) : \n 
~~~ session . delete ( ) \n 
~~ if in decoded : \n 
~~~ activity_delta = decoded [ ] - timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT if activity_delta > ( datetime . now ( ) - timedelta ( 0 , 0 , 0 , 0 , minutes ) ) : \n 
~~~ users += 1 \n 
~~ ~~ ~~ return users \n 
~~~ result = _ ( ) % { : datetime . now ( ) } \n 
result += _ ( ) % { : Plan . objects . all ( ) . count ( ) } \n 
result += _ ( ) % { : Session . objects . all ( ) . count ( ) , \n 
: settings . CONCURRENT_SESSIONS } \n 
result += _ ( ) % { : num_users ( 10 ) } \n 
space = os . statvfs ( ) \n 
result += _ ( ) % { : ( ( space . f_bsize * space . f_bavail ) / ( 1024 * 1024 ) ) } \n 
result += _ ( ) % { : commands . getoutput ( ) } \n 
return HttpResponse ( result , mimetype = ) \n 
~~ ~~ def statistics_sets ( request , planid ) : \n 
~~~ result = { : False } \n 
if plan . count ( ) == 0 : \n 
~~~ result [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~~ sets = [ ] \n 
scorefunctions = [ ] \n 
user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n 
~~~ if not in f . name . lower ( ) and not in f . name . lower ( ) : \n 
~~~ scorefunctions . append ( { : f . id , : force_escape ( f . get_label ( ) ) } ) \n 
~~ ~~ result [ ] = scorefunctions \n 
admin_display_names = [ \n 
"%s_sidebar_demo" % plan . legislative_body . name , \n 
if plan . legislative_body . is_community : \n 
~~~ admin_display_names . append ( "%s_sidebar_comments" % \n 
plan . legislative_body . name ) \n 
~~~ admin_display_names . append ( "%s_sidebar_basic" % \n 
~~ admin_displays = ScoreDisplay . objects . filter ( \n 
owner__is_superuser = True , \n 
legislative_body = plan . legislative_body , \n 
name__in = admin_display_names \n 
for admin_display in admin_displays : \n 
~~~ sets . append ( { \n 
: admin_display . id , \n 
: force_escape ( admin_display . get_label ( ) ) , \n 
: [ ] , \n 
: False \n 
~~~ user_displays = ScoreDisplay . objects . filter ( \n 
owner = request . user , \n 
is_page = False ) . order_by ( ) \n 
result [ ] = len ( user_displays ) \n 
for display in user_displays : \n 
~~~ functions = [ ] \n 
for panel in display . scorepanel_set . all ( ) : \n 
~~~ if panel . type == : \n 
~~~ functions = map ( lambda x : x . id , panel . score_functions . all ( ) ) \n 
if len ( functions ) == 0 : \n 
~~ ~~ ~~ sets . append ( { : display . id , : force_escape ( display . __unicode__ ( ) ) , ~~ ~~ except Exception , ex : \n 
~~~ result [ ] = _ ( ) % { : request . user } \n 
~~ result [ ] = sets \n 
result [ ] = True \n 
~~ elif request . method == and in request . POST : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . REQUEST . get ( , - 1 ) ) \n 
result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id } \n 
qset = display . scorepanel_set . all ( ) \n 
for panel in qset : \n 
~~~ if panel . displays . count ( ) == 1 : \n 
~~~ panel . delete ( ) \n 
~~ ~~ display . delete ( ) \n 
result [ ] = traceback . format_exc ( ) \n 
~~ ~~ elif request . method == : \n 
~~~ def validate_num ( user , limit = 3 ) : \n 
~~~ return ScoreDisplay . objects . filter ( owner = user , legislative_body = plan . legislative_body , is_page \n 
~~ if in request . POST : \n 
~~~ functions = request . POST . getlist ( ) \n 
functions = map ( lambda x : int ( x ) , functions ) \n 
~~~ display = ScoreDisplay . objects . get ( title = request . POST . get ( ) , owner = request . user display = display . copy_from ( display = display , functions = functions ) \n 
~~~ limit = 3 \n 
if validate_num ( request . user , limit ) : \n 
~~~ demo = ScoreDisplay . objects . filter ( \n 
is_page = False , \n 
title = "Demographics" \n 
for disp in demo : \n 
~~~ has_comments = False \n 
for pnl in disp . scorepanel_set . all ( ) : \n 
~~~ for fn in pnl . score_functions . all ( ) : \n 
~~~ has_comments = has_comments or fn . calculator . endswith ( ) \n 
~~ ~~ if not has_comments : \n 
~~~ demo = disp \n 
~~ ~~ display = ScoreDisplay ( ) \n 
display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n 
~~~ result [ ] = _ ( \n 
) % { : limit } \n 
result [ ] = \n 
~~ ~~ result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id , result [ ] = True \n 
~~ ~~ return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~ def purge_plan_clear_cache ( district , version ) : \n 
district . plan . purge ( after = version ) \n 
district . plan . version = version \n 
district . plan . save ( ) \n 
cache = district . computeddistrictscore_set . filter ( function__calculator__endswith = ) \n 
cache . delete ( ) \n 
def district_info ( request , planid , district_id ) : \n 
version = plan . version \n 
version = min ( plan . version , int ( version ) ) \n 
~~ ~~ district_id = int ( district_id ) \n 
district = plan . get_districts_at_version ( version , include_geom = False ) \n 
district = filter ( lambda d : d . district_id == district_id , district ) \n 
~~~ district = plan . district_set . get ( id = request . POST [ ] ) \n 
district . short_label = request . POST [ ] [ 0 : 10 ] \n 
district . long_label = request . POST [ ] [ 0 : 256 ] \n 
if district . version < version : \n 
district_copy . version = version \n 
district_copy . save ( ) \n 
district_copy . clone_relations_from ( district ) \n 
district = district_copy \n 
~~~ district . save ( ) \n 
~~ has_comment = in request . POST and request . POST [ ] != \n 
if has_comment : \n 
~~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n 
comment = Comment ( \n 
~~ tset = Tag . objects . get_for_object ( district ) . filter ( name__startswith = ) \n 
TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n 
purge_plan_clear_cache ( district , version ) \n 
if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~ ~~ status [ ] = version \n 
~~ def plan_feed ( request ) : \n 
~~~ feed = loader . get_template ( ) \n 
plans = Plan . objects . all ( ) . order_by ( ) [ 0 : 10 ] \n 
geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
extent = geolevel . geounit_set . collect ( ) . extent \n 
if extent [ 2 ] - extent [ 0 ] > extent [ 3 ] - extent [ 1 ] : \n 
~~~ width = 500 \n 
height = int ( 500 * ( extent [ 3 ] - extent [ 1 ] ) / ( extent [ 2 ] - extent [ 0 ] ) ) \n 
~~~ width = int ( 500 * ( extent [ 2 ] - extent [ 0 ] ) / ( extent [ 3 ] - extent [ 1 ] ) ) \n 
height = 500 \n 
~~ mapserver = settings . MAP_SERVER if settings . MAP_SERVER != else request . META [ ] \n 
context = { \n 
: plans , \n 
: mapserver , \n 
: extent , \n 
: width , \n 
: height \n 
xml = feed . render ( DjangoContext ( context ) ) \n 
return HttpResponse ( xml , mimetype = ) \n 
~~ def share_feed ( request ) : \n 
plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n 
if plans . count ( ) < 0 : \n 
~~~ geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
~~~ extent = ( 0 , 0 , 0 , 0 , ) \n 
width = 1 \n 
height = 1 \n 
#------------------------------------------------------------------------------- \n 
~~ DSIZE = 4 \n 
a_offset = 1 * 1024 * 1024 \n 
b_offset = 2 * 1024 * 1024 \n 
iochannel = CoramIoChannel ( idx = 0 , datawidth = 32 ) \n 
channel = CoramChannel ( idx = 0 , datawidth = 8 * DSIZE ) \n 
def st_set_mesh_size ( mesh_size ) : \n 
~~~ channel . write ( mesh_size ) \n 
~~ def st_step ( mesh_size , read_start , write_start ) : \n 
~~~ read_page = 3 \n 
write_page = 0 \n 
read_addr = read_start \n 
mem0 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
mem1 . write ( 0 , read_addr , mesh_size ) \n 
mem2 . write ( 0 , read_addr , mesh_size ) \n 
write_addr = write_start + mesh_size * DSIZE + DSIZE \n 
for i in range ( mesh_size - 2 ) : \n 
~~~ hot_spot = 1 if i == 0 else 0 \n 
pos = ( ( hot_spot << 6 ) | \n 
( ( 0x1 << write_page ) << 4 ) | \n 
( 0x1 << read_page ) ) \n 
mem0 . wait ( ) \n 
mem1 . wait ( ) \n 
mem2 . wait ( ) \n 
mem3 . wait ( ) \n 
channel . write ( pos ) \n 
if read_page == 0 : \n 
~~~ mem0 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 1 : \n 
~~~ mem1 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 2 : \n 
~~~ mem2 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 3 : \n 
~~~ mem3 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ read_page = 0 if read_page == 3 else read_page + 1 \n 
channel . read ( ) \n 
mem_d0 . wait ( ) \n 
mem_d1 . wait ( ) \n 
if write_page == 0 : \n 
~~~ mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ elif write_page == 1 : \n 
~~~ mem_d1 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ write_addr += mesh_size * DSIZE \n 
write_page = 0 if write_page == 1 else write_page + 1 \n 
~~ mem_d0 . wait ( ) \n 
~~ def st_computation ( num_iter , mesh_size ) : \n 
~~~ for i in range ( num_iter / 2 ) : \n 
~~~ st_step ( mesh_size , a_offset , b_offset ) \n 
st_step ( mesh_size , b_offset , a_offset ) \n 
~~ ~~ def st_sum ( mesh_size ) : \n 
~~~ check_sum = 0 \n 
read_addr = a_offset \n 
for i in range ( mesh_size ) : \n 
~~~ mem0 . write ( 0 , read_addr , mesh_size ) \n 
init_sum = 1 if i == 0 else 0 \n 
calc_sum = 1 \n 
pos = ( init_sum << 8 ) | ( calc_sum << 7 ) \n 
check_sum = channel . read ( ) \n 
return check_sum \n 
~~ def st_main ( ) : \n 
~~~ global a_offset \n 
global b_offset \n 
mesh_size = iochannel . read ( ) \n 
num_iter = iochannel . read ( ) \n 
a_offset = iochannel . read ( ) \n 
b_offset = iochannel . read ( ) \n 
st_set_mesh_size ( mesh_size ) \n 
st_computation ( num_iter , mesh_size ) \n 
check_sum = st_sum ( mesh_size ) \n 
iochannel . write ( check_sum ) \n 
~~ while True : \n 
~~~ st_main ( ) \n 
~~~ read_page = 0 \n 
pos = hot_spot \n 
~~ read_page = 0 if read_page == 2 else read_page + 1 \n 
mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
write_addr += mesh_size * DSIZE \n 
~~ ~~ def st_computation ( num_iter , mesh_size ) : \n 
pos = ( init_sum << 2 ) | ( calc_sum << 1 ) \n 
~~ from __future__ import absolute_import \n 
def getRamId ( oid , sid ) : \n 
~~~ if 0 <= sid and sid <= 31 : \n 
~~ if 32 <= sid and sid <= 63 : \n 
~~~ return 1 \n 
~~ if 64 <= sid and sid <= 95 : \n 
~~~ return 2 \n 
~~ if 96 <= sid and sid <= 127 : \n 
~~~ return 3 \n 
~~ ~~ def getRamSubId ( oid , sid ) : \n 
~~~ return sid \n 
~~~ return sid - 32 \n 
~~~ return sid - 64 \n 
~~~ return sid - 96 \n 
~~ ~~ def getChannelId ( oid , sid ) : \n 
~~~ return oid \n 
~~ def getChannelSubId ( oid , sid ) : \n 
~~ def getRegisterId ( oid , sid ) : \n 
~~ def getRegisterSubId ( oid , sid ) : \n 
~~~ f = open ( sys . argv [ 1 ] , ) \n 
lines = f . readlines ( ) \n 
output = [ ] \n 
p_thread = re . compile ( ) \n 
p_thread_id = re . compile ( ) \n 
p_object_id = re . compile ( ) \n 
p_width = re . compile ( ) \n 
p_depth = re . compile ( ) \n 
p_indexwidth = re . compile ( ) \n 
p_logdepth = re . compile ( ) \n 
p_sub_id = re . compile ( ) \n 
module_name = None \n 
thread_name = None \n 
thread_id = None \n 
object_id = None \n 
sub_id = None \n 
width = None \n 
indexwidth = None \n 
depth = None \n 
mode = False \n 
sub_id_num = None \n 
sub_id_base = None \n 
buffer = [ ] \n 
for line in lines : \n 
~~~ if not mode : \n 
~~~ m = p_thread . match ( line ) \n 
if m : \n 
~~~ thread_name = re . match ( \'.*(".*").*\' , m . group ( 2 ) ) . group ( 1 ) \n 
module_name = re . search ( , line ) . group ( 1 ) \n 
mode = True \n 
buffer . append ( line ) \n 
~~~ m = p_thread_id . match ( line ) \n 
~~~ tid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
thread_id = re . match ( , tid_str ) . group ( 2 ) \n 
~~ m = p_object_id . match ( line ) \n 
~~~ oid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
object_id = re . match ( , oid_str ) . group ( 2 ) \n 
~~ m = p_width . match ( line ) \n 
~~~ width_str = m . group ( 2 ) \n 
width = re . match ( , width_str ) . group ( 1 ) \n 
~~ m = p_depth . match ( line ) \n 
~~~ depth_str = m . group ( 2 ) \n 
depth = re . match ( , depth_str ) . group ( 1 ) \n 
~~ m = p_indexwidth . match ( line ) \n 
~~~ indexwidth_str = m . group ( 2 ) \n 
indexwidth = re . match ( , indexwidth_str ) . group ( 1 ) \n 
~~ m = p_logdepth . match ( line ) \n 
~~~ logdepth_str = m . group ( 2 ) \n 
logdepth = re . match ( , logdepth_str ) . group ( 1 ) \n 
~~ m = p_sub_id . match ( line ) \n 
~~~ sid_str = m . group ( 2 ) \n 
sub_id_m = re . search ( , sid_str ) \n 
sub_id = sub_id_m . group ( 0 ) \n 
sub_id_num = sub_id_m . group ( 2 ) \n 
sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n 
16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n 
2 if sub_id_m . group ( 1 ) . count ( "\'b" ) > 0 else \n 
10 ) \n 
~~ ~~ if mode : \n 
if module_name . count ( ) > 0 : \n 
~~ if module_name . count ( ) > 0 : \n 
print ( . join ( buffer [ 1 : ] ) ) \n 
~~ mode = False \n 
print ( line , end = ) \n 
~~ ~~ main ( ) \n 
from optparse import OptionParser \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import pyverilog . utils . version \n 
from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n 
VERSION = pyverilog . utils . version . VERSION \n 
def showVersion ( ) : \n 
~~~ print ( INFO ) \n 
print ( VERSION ) \n 
print ( USAGE ) \n 
sys . exit ( ) \n 
~~ optparser = OptionParser ( ) \n 
optparser . add_option ( "-v" , "--version" , action = "store_true" , dest = "showversion" , \n 
optparser . add_option ( "-I" , "--include" , dest = "include" , action = "append" , \n 
optparser . add_option ( "-D" , dest = "define" , action = "append" , \n 
optparser . add_option ( "-t" , "--top" , dest = "topmodule" , \n 
optparser . add_option ( "--nobind" , action = "store_true" , dest = "nobind" , \n 
optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n 
( options , args ) = optparser . parse_args ( ) \n 
filelist = args \n 
if options . showversion : \n 
~~~ showVersion ( ) \n 
~~ for f in filelist : \n 
~~ if len ( filelist ) == 0 : \n 
~~ analyzer = VerilogDataflowAnalyzer ( filelist , options . topmodule , \n 
noreorder = options . noreorder , \n 
nobind = options . nobind , \n 
preprocess_include = options . include , \n 
preprocess_define = options . define ) \n 
analyzer . generate ( ) \n 
directives = analyzer . get_directives ( ) \n 
for dr in sorted ( directives , key = lambda x : str ( x ) ) : \n 
~~~ print ( dr ) \n 
~~ instances = analyzer . getInstances ( ) \n 
for module , instname in sorted ( instances , key = lambda x : str ( x [ 1 ] ) ) : \n 
~~~ print ( ( module , instname ) ) \n 
~~ if options . nobind : \n 
signals = analyzer . getSignals ( ) \n 
for sig in signals : \n 
~~~ print ( sig ) \n 
consts = analyzer . getConsts ( ) \n 
for con in consts : \n 
~~~ print ( con ) \n 
~~~ terms = analyzer . getTerms ( ) \n 
for tk , tv in sorted ( terms . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ print ( tv . tostr ( ) ) \n 
~~ binddict = analyzer . getBinddict ( ) \n 
for bk , bv in sorted ( binddict . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ for bvi in bv : \n 
~~~ print ( bvi . tostr ( ) ) \n 
~~ ~~ ~~ ~~ if __name__ == : \n 
from pyverilog . dataflow . dataflow import * \n 
def replaceUndefined ( tree , termname ) : \n 
~~~ if tree is None : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFUndefined ) : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFConstant ) : return tree \n 
if isinstance ( tree , DFEvalValue ) : return tree \n 
if isinstance ( tree , DFTerminal ) : return tree \n 
if isinstance ( tree , DFBranch ) : \n 
~~~ condnode = replaceUndefined ( tree . condnode , termname ) \n 
truenode = replaceUndefined ( tree . truenode , termname ) \n 
falsenode = replaceUndefined ( tree . falsenode , termname ) \n 
return DFBranch ( condnode , truenode , falsenode ) \n 
~~ if isinstance ( tree , DFOperator ) : \n 
~~~ nextnodes = [ ] \n 
for n in tree . nextnodes : \n 
~~~ nextnodes . append ( replaceUndefined ( n , termname ) ) \n 
~~ return DFOperator ( tuple ( nextnodes ) , tree . operator ) \n 
~~ if isinstance ( tree , DFPartselect ) : \n 
~~~ msb = replaceUndefined ( tree . msb , termname ) \n 
lsb = replaceUndefined ( tree . lsb , termname ) \n 
var = replaceUndefined ( tree . var , termname ) \n 
return DFPartselect ( var , msb , lsb ) \n 
~~ if isinstance ( tree , DFPointer ) : \n 
~~~ ptr = replaceUndefined ( tree . ptr , termname ) \n 
return DFPointer ( var , ptr ) \n 
~~ if isinstance ( tree , DFConcat ) : \n 
~~ return DFConcat ( tuple ( nextnodes ) ) \n 
~~ raise DefinitionError ( % ( str ( type ( tree ) ) , str ( tree ) ) ) \n 
from pyverilog . dataflow . optimizer import VerilogDataflowOptimizer \n 
from pyverilog . controlflow . controlflow_analyzer import VerilogControlflowAnalyzer \n 
codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n 
def test ( ) : \n 
~~~ filelist = [ codedir + ] \n 
topmodule = \n 
noreorder = False \n 
nobind = False \n 
include = None \n 
define = None \n 
analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n 
noreorder = noreorder , \n 
nobind = nobind , \n 
preprocess_include = include , \n 
preprocess_define = define ) \n 
instances = analyzer . getInstances ( ) \n 
terms = analyzer . getTerms ( ) \n 
binddict = analyzer . getBinddict ( ) \n 
optimizer = VerilogDataflowOptimizer ( terms , binddict ) \n 
optimizer . resolveConstant ( ) \n 
c_analyzer = VerilogControlflowAnalyzer ( topmodule , terms , \n 
binddict , \n 
resolved_terms = optimizer . getResolvedTerms ( ) , \n 
resolved_binddict = optimizer . getResolvedBinddict ( ) , \n 
constlist = optimizer . getConstlist ( ) \n 
for tk in sorted ( c_analyzer . resolved_terms . keys ( ) , key = lambda x : str ( x ) ) : \n 
~~~ tree = c_analyzer . makeTree ( tk ) \n 
output . append ( str ( tk ) + + tree . tocode ( ) ) \n 
~~ rslt = . join ( output ) + \n 
print ( rslt ) \n 
assert ( expected == rslt ) \n 
~~~ test ( ) \n 
import dataflow_example \n 
~~~ test_module = dataflow_example . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
assert ( expected_code == code ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) \n 
from veriloggen import * \n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
interval = m . Parameter ( , 16 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , 8 , initval = 0 ) \n 
count = m . Reg ( , 32 , initval = 0 ) \n 
seq = Seq ( m , , clk , rst ) \n 
seq . add ( Systask ( , , led , count ) ) \n 
seq . add ( count ( count + 1 ) , cond = count < interval - 1 ) \n 
seq . add ( count ( 0 ) , cond = count == interval - 1 ) \n 
seq . add ( led ( led + 1 ) , cond = count == interval - 1 ) \n 
seq . make_always ( ) \n 
return m \n 
~~ def mkTest ( ) : \n 
led = mkLed ( ) \n 
params = m . copy_params ( led ) \n 
ports = m . copy_sim_ports ( led ) \n 
clk = ports [ ] \n 
rst = ports [ ] \n 
uut = m . Instance ( led , , \n 
params = m . connect_params ( led ) , \n 
ports = m . connect_ports ( led ) ) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = 100 ) \n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ \n 
width = m . Parameter ( , 8 ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Reg ( , 32 ) \n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( Cond ( count == 1023 , 0 , count + 1 ) ) \n 
led ( 0 ) \n 
led ( Cond ( count == 1024 - 1 , led + 1 , led ) ) \n 
~~~ led = mkLed ( ) \n 
verilog = led . to_verilog ( ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
def mkSub ( ) : \n 
count = m . OutputReg ( , 32 ) \n 
If ( count == 1023 ) ( \n 
count ( count + 1 ) \n 
~~ def mkLed ( ) : \n 
count = m . Wire ( , 32 ) \n 
sub = mkSub ( ) \n 
m . Instance ( sub , , m . connect_params ( sub ) , m . connect_ports ( sub ) ) \n 
led ( led + 1 ) \n 
inst_sub = m . Reg ( , 32 ) \n 
~~ except ValueError as e : \n 
~~~ print ( e . args [ 0 ] ) \n 
#print(verilog) \n 
If ( count == 1024 - 1 ) ( \n 
led ( led + 1 ) , \n 
SingleStatement ( SystemTask ( , , led ) ) \n 
import veriloggen . dataflow as dataflow \n 
def mkMain ( ) : \n 
~~~ x = dataflow . Variable ( , valid = , ready = , point = 8 ) \n 
y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n 
z = x * y \n 
z . output ( , valid = , ready = ) \n 
df = dataflow . Dataflow ( z ) \n 
m = df . to_module ( ) \n 
main = mkMain ( ) \n 
params = m . copy_params ( main ) \n 
ports = m . copy_sim_ports ( main ) \n 
xdata = ports [ ] \n 
xvalid = ports [ ] \n 
xready = ports [ ] \n 
ydata = ports [ ] \n 
yvalid = ports [ ] \n 
yready = ports [ ] \n 
zdata = ports [ ] \n 
zvalid = ports [ ] \n 
zready = ports [ ] \n 
xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
ydata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
zdata_orig = m . WireLike ( ports [ ] , name = ) \n 
m . Always ( ) ( xdata ( fixed . to_fixed ( xdata_orig , 8 ) ) ) \n 
m . Always ( ) ( ydata ( fixed . to_fixed ( ydata_orig , 4 ) ) ) \n 
m . Assign ( zdata_orig ( fixed . fixed_to_int ( zdata , 8 ) ) ) \n 
uut = m . Instance ( main , , \n 
params = m . connect_params ( main ) , \n 
ports = m . connect_ports ( main ) ) \n 
reset_done = m . Reg ( , initval = 0 ) \n 
reset_stmt = [ ] \n 
reset_stmt . append ( reset_done ( 0 ) ) \n 
reset_stmt . append ( xdata ( 0 ) ) \n 
reset_stmt . append ( xvalid ( 0 ) ) \n 
reset_stmt . append ( ydata ( 0 ) ) \n 
reset_stmt . append ( yvalid ( 0 ) ) \n 
reset_stmt . append ( zready ( 0 ) ) \n 
reset_stmt . append ( xdata_orig ( 0 ) ) \n 
reset_stmt . append ( ydata_orig ( 0 ) ) \n 
simulation . setup_waveform ( m , uut , xdata_orig , ydata_orig , zdata_orig ) \n 
init = simulation . setup_reset ( m , rst , reset_stmt , period = 100 ) \n 
nclk = simulation . next_clock \n 
reset_done ( 1 ) , \n 
nclk ( clk ) , \n 
Delay ( 10000 ) , \n 
def send ( name , data , valid , ready , step = 1 , waitnum = 10 ) : \n 
~~~ fsm = FSM ( m , name + , clk , rst ) \n 
count = m . TmpReg ( 32 , initval = 0 ) \n 
fsm . add ( valid ( 0 ) ) \n 
fsm . goto_next ( cond = reset_done ) \n 
for _ in range ( waitnum ) : \n 
~~~ fsm . goto_next ( ) \n 
~~ fsm . add ( valid ( 1 ) ) \n 
fsm . goto_next ( ) \n 
fsm . add ( data ( data + step ) , cond = ready ) \n 
fsm . add ( count . inc ( ) , cond = ready ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 5 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 5 , ready ) ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 10 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 10 , ready ) ) \n 
fsm . make_always ( ) \n 
~~ def receive ( name , data , valid , ready , waitnum = 10 ) : \n 
fsm . add ( ready ( 0 ) ) \n 
yinit = fsm . current ( ) \n 
fsm . add ( ready ( 1 ) , cond = valid ) \n 
fsm . goto_next ( cond = valid ) \n 
for i in range ( waitnum ) : \n 
~~~ fsm . add ( ready ( 0 ) ) \n 
~~ fsm . goto ( yinit ) \n 
~~ send ( , xdata_orig , xvalid , xready , step = 1 , waitnum = 10 ) \n 
send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n 
receive ( , zdata , zvalid , zready , waitnum = 50 ) \n 
If ( reset_done ) ( \n 
If ( AndList ( xvalid , xready ) ) ( \n 
Systask ( , , xdata_orig ) \n 
If ( AndList ( yvalid , yready ) ) ( \n 
Systask ( , , ydata_orig ) \n 
If ( AndList ( zvalid , zready ) ) ( \n 
Systask ( , , zdata_orig ) \n 
sim = simulation . Simulator ( test ) \n 
#sim.view_waveform(background=True) \n 
import dataflow_mul \n 
~~~ test_module = dataflow_mul . mkTest ( ) \n 
valid = m . OutputReg ( , initval = 0 ) \n 
count = m . Reg ( , width = 32 , initval = 0 ) \n 
up = m . Wire ( ) \n 
down = m . Wire ( ) \n 
m . Assign ( up ( 1 ) ) \n 
m . Assign ( down ( 0 ) ) \n 
fsm = FSM ( m , , clk , rst ) \n 
for i in range ( 4 ) : \n 
~~ c = count >= 16 \n 
fsm . add ( valid ( up ) , cond = c , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . goto_next ( cond = c ) \n 
for i in range ( 8 ) : \n 
~~ c = count >= 32 \n 
~~~ fsm . add ( valid ( up ) , cond = c , delay = 1 , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n 
~~ fsm . make_always ( reset = [ count . reset ( ) ] , body = [ count ( count + 1 ) ] ) \n 
clk = m . Reg ( ) \n 
rst = m . Reg ( ) \n 
valid = m . Wire ( ) \n 
uut = m . Instance ( mkLed ( ) , , \n 
ports = ( ( , clk ) , ( , rst ) , ( , valid ) ) ) \n 
simulation . setup_waveform ( m , uut ) \n 
init = simulation . setup_reset ( m , rst , period = 100 ) \n 
import pipeline_draw_graph \n 
~~~ test_module = pipeline_draw_graph . mkTest ( ) \n 
import read_verilog_module_str \n 
~~~ test_module = read_verilog_module_str . mkTop ( ) \n 
import veriloggen . core . vtypes as vtypes \n 
import veriloggen . core . module as module \n 
def mkMultiplierCore ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ retwidth = lwidth + rwidth \n 
m = module . Module ( % index ) \n 
update = m . Input ( ) \n 
a = m . Input ( , lwidth ) \n 
b = m . Input ( , rwidth ) \n 
c = m . Output ( , retwidth ) \n 
_a = m . Reg ( , lwidth , signed = lsigned ) \n 
_b = m . Reg ( , rwidth , signed = rsigned ) \n 
tmpval = [ m . Reg ( % i , retwidth , signed = True ) for i in range ( depth - 1 ) ] \n 
rslt = m . Wire ( , retwidth , signed = True ) \n 
__a = _a \n 
__b = _b \n 
if not lsigned : \n 
~~~ __a = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _a ) ) \n 
~~ if not rsigned : \n 
~~~ __b = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _b ) ) \n 
~~ m . Assign ( rslt ( __a * __b ) ) \n 
m . Assign ( c ( tmpval [ depth - 2 ] ) ) \n 
m . Always ( vtypes . Posedge ( clk ) ) ( \n 
vtypes . If ( update ) ( \n 
_a ( a ) , \n 
_b ( b ) , \n 
tmpval [ 0 ] ( rslt ) , \n 
[ tmpval [ i ] ( tmpval [ i - 1 ] ) for i in range ( 1 , depth - 1 ) ] \n 
~~ def mkMultiplier ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
retwidth = lwidth + rwidth \n 
mult = mkMultiplierCore ( index , lwidth , rwidth , lsigned , rsigned , depth ) \n 
enable = m . Input ( ) \n 
valid = m . Output ( ) \n 
valid_reg = [ m . Reg ( % i ) for i in range ( depth ) ] \n 
m . Assign ( valid ( valid_reg [ depth - 1 ] ) ) \n 
vtypes . If ( rst ) ( \n 
[ valid_reg [ i ] ( 0 ) for i in range ( depth ) ] \n 
valid_reg [ 0 ] ( enable ) , \n 
[ valid_reg [ i ] ( valid_reg [ i - 1 ] ) for i in range ( 1 , depth ) ] \n 
ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n 
m . Instance ( mult , , ports = ports ) \n 
~~ index_count = 0 \n 
def get_mul ( lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ global index_count \n 
mul = mkMultiplier ( index_count , lwidth , rwidth , lsigned , rsigned , depth ) \n 
index_count += 1 \n 
return mul \n 
~~ def reset ( ) : \n 
index_count = 0 \n 
from pyramid import testing \n 
import mock \n 
class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
~~~ testing . tearDown ( ) \n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_modified \n 
return acl_modified ( event ) \n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ from substanced . audit import AuditLog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
auditlog = AuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
context . __oid__ = 5 \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
event . old_acl = \n 
event . new_acl = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 5 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
: entry [ 2 ] . timestamp , \n 
: { : 1 , : } , \n 
def test_it_nolog ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
~~ ~~ _marker = object ( ) \n 
class Test_content_added_moved_or_duplicated ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_added_moved_or_duplicated \n 
return content_added_moved_or_duplicated ( event ) \n 
def test_it_added ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
event = _makeEvent ( ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
: 5 \n 
def test_it_added_noscribe ( self , mock_get_auditlog ) : \n 
def test_it_moved ( self , mock_get_auditlog ) : \n 
event . moving = True \n 
event . duplicating = None \n 
def test_it_duplicated ( self , mock_get_auditlog ) : \n 
event . moving = None \n 
event . duplicating = True \n 
~~ ~~ class Test_content_removed ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_removed \n 
return content_removed ( event ) \n 
~~ def test_it_moving ( self ) : \n 
~~~ event = Dummy ( ) \n 
~~ ~~ class Test_content_modified ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_modified \n 
return content_modified ( event ) \n 
def test_it_noscribe ( self , mock_get_auditlog ) : \n 
~~ ~~ class Test_logged_in ( unittest . TestCase ) : \n 
~~~ from . . subscribers import logged_in \n 
return logged_in ( event ) \n 
event . request = Dummy ( ) \n 
event . request . context = context \n 
def test_it_user_has_oid ( self , mock_get_auditlog ) : \n 
user = Dummy ( ) \n 
user . __oid__ = 5 \n 
event . user = user \n 
event . login = \n 
self . assertEqual ( entry [ 2 ] . oid , None ) \n 
def test_it_user_has_no_oid ( self , mock_get_auditlog ) : \n 
: None , \n 
~~ ~~ class Test_root_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import root_added \n 
return root_added ( event ) \n 
def test_it ( self , mock_set_auditlog ) : \n 
root = Dummy ( ) \n 
def is_set ( _root ) : \n 
~~~ self . assertEqual ( _root , root ) \n 
~~ mock_set_auditlog . side_effect = is_set \n 
event . object = root \n 
~~ ~~ class Dummy ( object ) : \n 
~~~ def __init__ ( self , kw = None ) : \n 
~~~ if kw : \n 
~~~ self . __dict__ . update ( kw ) \n 
~~ ~~ ~~ class DummyContentRegistry ( object ) : \n 
~~~ def typeof ( self , content ) : \n 
~~ ~~ def _makeAuditLog ( ) : \n 
return auditlog \n 
~~ def _makeRegistry ( ) : \n 
~~~ registry = Dummy ( ) \n 
registry . content = DummyContentRegistry ( ) \n 
return registry \n 
~~ def _makeEvent ( ) : \n 
event . parent = testing . DummyResource ( ) \n 
event . parent . __oid__ = 10 \n 
event . name = \n 
context . __parent__ = event . parent \n 
return event \n 
class Test_root_factory ( unittest . TestCase ) : \n 
~~~ self . config = testing . setUp ( ) \n 
~~ def _callFUT ( self , request , transaction , get_connection , evolve_packages ) : \n 
~~~ from . . import root_factory \n 
return root_factory ( request , transaction , get_connection , \n 
evolve_packages ) \n 
~~ def _makeRequest ( self , app_root = None ) : \n 
~~~ request = Dummy ( ) \n 
request . registry = DummyRegistry ( ) \n 
request . registry . content = Dummy ( ) \n 
request . registry . content . create = lambda * arg : app_root \n 
return request \n 
~~ def test_without_app_root ( self ) : \n 
~~~ txn = DummyTransaction ( ) \n 
root = { } \n 
gc = Dummy_get_connection ( root ) \n 
ep = DummyFunction ( True ) \n 
app_root = object ( ) \n 
request = self . _makeRequest ( app_root ) \n 
result = self . _callFUT ( request , txn , gc , ep ) \n 
self . assertEqual ( result , app_root ) \n 
self . assertTrue ( txn . committed ) \n 
self . assertTrue ( txn . savepointed ) \n 
self . assertTrue ( ep . called ) \n 
~~ def test_with_app_root ( self ) : \n 
root = { : app_root } \n 
request = testing . DummyRequest ( ) \n 
self . assertFalse ( txn . committed ) \n 
~~ ~~ class Test_includeme ( unittest . TestCase ) : \n 
~~~ def test_it ( self ) : \n 
~~~ from . . import ( \n 
includeme , \n 
connection_opened , \n 
connection_will_close , \n 
ZODBConnectionOpened , \n 
ZODBConnectionWillClose , \n 
config = DummyConfig ( ) \n 
includeme ( config ) \n 
config . subscriptions , \n 
[ ( connection_opened , ZODBConnectionOpened ) , \n 
( connection_will_close , ZODBConnectionWillClose ) , \n 
~~ ~~ class Test_connection_opened ( unittest . TestCase ) : \n 
~~~ from . . import connection_opened \n 
event = DummyEvent ( ) \n 
connection_opened ( event ) \n 
self . assertEqual ( event . request . _zodb_tx_counts , ( 0 , 0 ) ) \n 
~~ ~~ class Test_connection_will_close ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event , statsd_incr ) : \n 
~~~ from . . import connection_will_close \n 
return connection_will_close ( event , statsd_incr ) \n 
~~ def test_no_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( ) \n 
result = self . _callFUT ( event , None ) \n 
~~ def test_with_postitive_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 5 , 5 ) \n 
event . request . _zodb_tx_counts = ( 1 , 1 ) \n 
L = [ ] \n 
def statsd_incr ( name , num , registry = None ) : \n 
~~~ L . append ( ( name , num ) ) \n 
~~ self . _callFUT ( event , statsd_incr ) \n 
L , \n 
[ ( , 4 ) , ( , 4 ) ] \n 
~~ def test_with_zero_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 1 , 1 ) \n 
self . _callFUT ( event , None ) \n 
[ ] \n 
~~ ~~ class DummyTransaction ( object ) : \n 
~~~ committed = False \n 
savepointed = False \n 
def commit ( self ) : \n 
~~~ self . committed = True \n 
~~ def savepoint ( self ) : \n 
~~~ self . savepointed = True \n 
~~ ~~ class Dummy_get_connection ( object ) : \n 
~~~ def __init__ ( self , root ) : \n 
~~~ self . _root = root \n 
~~ def root ( self ) : \n 
~~~ return self . _root \n 
~~ def __call__ ( self , request ) : \n 
~~ ~~ class DummyFunction ( object ) : \n 
~~~ called = False \n 
def __init__ ( self , result ) : \n 
~~~ self . result = result \n 
~~ def __call__ ( self , * args , ** kw ) : \n 
~~~ self . called = True \n 
self . args = args \n 
self . kw = kw \n 
return self . result \n 
~~ class DummyRegistry ( object ) : \n 
~~~ def notify ( self , event ) : \n 
~~~ self . event = event \n 
~~ ~~ class DummyConfig ( object ) : \n 
~~~ self . subscriptions = [ ] \n 
~~ def add_subscriber ( self , fn , event_type ) : \n 
~~~ self . subscriptions . append ( ( fn , event_type ) ) \n 
~~ ~~ class DummyConnection ( object ) : \n 
~~~ def __init__ ( self , loads , stores ) : \n 
~~~ self . loads = loads \n 
self . stores = stores \n 
~~ def getTransferCounts ( self ) : \n 
~~~ return ( self . loads , self . stores ) \n 
~~ ~~ class DummyEvent ( object ) : \n 
~~~ def __init__ ( self , loads = 0 , stores = 0 ) : \n 
self . conn = DummyConnection ( loads , stores ) \n 
~~ ~~ import pkg_resources \n 
import mimetypes \n 
import colander \n 
import deform . schema \n 
from pyramid . httpexceptions import HTTPFound \n 
from pyramid . response import Response \n 
from pyramid . security import NO_PERMISSION_REQUIRED \n 
from . . form import FormView \n 
from . . file import ( \n 
FilePropertiesSchema , \n 
FileUploadTempStore , \n 
file_upload_widget , \n 
file_name_node , \n 
USE_MAGIC , \n 
from . . interfaces import ( \n 
IFile , \n 
IFolder , \n 
from . . sdi import mgmt_view \n 
@ mgmt_view ( \n 
context = IFile , \n 
permission = , \n 
tab_condition = False , \n 
http_cache = 0 , \n 
def view_file ( context , request ) : \n 
~~~ return context . get_response ( request = request ) \n 
~~ @ mgmt_view ( \n 
tab_title = , \n 
permission = \n 
def view_tab ( context , request ) : \n 
~~~ return HTTPFound ( location = request . sdiapi . mgmt_path ( context ) ) \n 
~~ class AddFileSchema ( FilePropertiesSchema ) : \n 
~~~ file = colander . SchemaNode ( \n 
deform . schema . FileData ( ) , \n 
widget = file_upload_widget , \n 
missing = colander . null , \n 
~~ @ colander . deferred \n 
def name_or_file ( node , kw ) : \n 
~~~ def _name_or_file ( node , struct ) : \n 
~~~ if not struct [ ] and not struct [ ] : \n 
~~~ raise colander . Invalid ( node , ) \n 
~~ if not struct [ ] : \n 
~~~ filename = struct [ ] . get ( ) \n 
if filename : \n 
~~~ name_node = file_name_node . bind ( \n 
context = kw [ ] , request = kw [ ] \n 
name_node . validator ( node [ ] , filename ) \n 
~~~ raise colander . Invalid ( \n 
node , \n 
~~ ~~ ~~ return _name_or_file \n 
context = IFolder , \n 
renderer = , \n 
addable_content = , \n 
tab_condition = False \n 
class AddFileView ( FormView ) : \n 
~~~ title = \n 
schema = AddFileSchema ( validator = name_or_file ) . clone ( ) \n 
schema [ ] . missing = colander . null \n 
buttons = ( , ) \n 
def _makeob ( self , stream , title , mimetype ) : \n 
~~~ return self . request . registry . content . create ( \n 
stream = stream , \n 
mimetype = mimetype , \n 
title = title , \n 
~~ def add_success ( self , appstruct ) : \n 
~~~ name = appstruct [ ] \n 
title = appstruct [ ] or None \n 
filedata = appstruct [ ] \n 
mimetype = appstruct [ ] or USE_MAGIC \n 
stream = None \n 
filename = None \n 
if filedata : \n 
~~~ filename = filedata [ ] \n 
stream = filedata [ ] \n 
if stream : \n 
~~~ stream . seek ( 0 ) \n 
~~~ stream = None \n 
~~ ~~ name = name or filename \n 
fileob = self . _makeob ( stream , title , mimetype ) \n 
self . context [ name ] = fileob \n 
tmpstore = FileUploadTempStore ( self . request ) \n 
tmpstore . clear ( ) \n 
return HTTPFound ( self . request . sdiapi . mgmt_path ( self . context ) ) \n 
~~ ~~ onepixel = pkg_resources . resource_filename ( \n 
permission = NO_PERMISSION_REQUIRED \n 
def preview_image_upload ( request ) : \n 
~~~ uid = request . subpath [ 0 ] \n 
tempstore = FileUploadTempStore ( request ) \n 
filedata = tempstore . get ( uid , { } ) \n 
fp = filedata . get ( ) \n 
if fp is not None : \n 
~~~ fp . seek ( 0 ) \n 
filename = filedata [ ] \n 
~~ mimetype = mimetypes . guess_type ( filename , strict = False ) [ 0 ] \n 
if not mimetype or not mimetype . startswith ( ) : \n 
~~~ mimetype = \n 
fp = open ( onepixel , ) \n 
~~ response = Response ( content_type = mimetype , app_iter = fp ) \n 
class Test_principal_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import principal_added \n 
return principal_added ( event ) \n 
~~ def test_event_wo_loading_attr ( self ) : \n 
~~~ event = testing . DummyResource ( ) \n 
event . object = testing . DummyResource ( ) \n 
self . assertRaises ( AttributeError , self . _callFUT , event ) \n 
~~ def test_event_w_loading_True ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
~~ def test_wo_principals_service ( self ) : \n 
~~~ from zope . interface import directlyProvides \n 
from ... interfaces import IFolder \n 
event = testing . DummyResource ( loading = False ) \n 
root = testing . DummyResource ( ) \n 
directlyProvides ( root , IFolder ) \n 
event . object = root [ ] = testing . DummyResource ( ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
~~ def test_user_not_in_groups ( self ) : \n 
~~~ from ... testing import make_site \n 
from ... interfaces import IUser \n 
site = make_site ( ) \n 
user = testing . DummyResource ( __provides__ = IUser ) \n 
site [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
~~ def test_user_in_groups ( self ) : \n 
groups = site [ ] [ ] \n 
groups [ ] = testing . DummyResource ( ) \n 
~~ def test_group_not_in_users ( self ) : \n 
group = testing . DummyResource ( ) \n 
site [ ] = group \n 
event = testing . DummyResource ( object = group , loading = False ) \n 
~~ def test_group_in_users ( self ) : \n 
users = site [ ] [ ] \n 
users [ ] = testing . DummyResource ( ) \n 
~~ ~~ class Test_user_will_be_removed ( unittest . TestCase ) : \n 
~~~ from . . subscribers import user_will_be_removed \n 
return user_will_be_removed ( event ) \n 
~~ def test_loading ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True , moving = None ) \n 
~~ def test_moving ( self ) : \n 
~~~ event = testing . DummyResource ( loading = False , moving = True ) \n 
~~ def test_it ( self ) : \n 
~~~ from ... interfaces import IFolder \n 
parent = testing . DummyResource ( __provides__ = IFolder ) \n 
user = testing . DummyResource ( ) \n 
reset = testing . DummyResource ( ) \n 
def commit_suicide ( ) : \n 
~~~ reset . committed = True \n 
~~ reset . commit_suicide = commit_suicide \n 
objectmap = DummyObjectMap ( ( reset , ) ) \n 
parent . __objectmap__ = objectmap \n 
parent [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False , moving = None ) \n 
self . assertTrue ( reset . committed ) \n 
~~~ event = testing . DummyResource ( object = None , loading = False ) \n 
~~ ~~ class Test_user_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import user_added \n 
return user_added ( event ) \n 
~~ def test_it_user_has_no_oid ( self ) : \n 
~~~ user = testing . DummyResource ( ) \n 
event . registry = DummyRegistry ( ) \n 
~~~ from pyramid . security import Allow \n 
user . __oid__ = 1 \n 
user . __acl__ , \n 
[ ( Allow , 1 , ( , \n 
) ) ] ) \n 
~~ ~~ class Test_acl_maybe_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import acl_maybe_added \n 
return acl_maybe_added ( event ) \n 
~~~ event = DummyEvent ( moving = True , loading = False ) \n 
self . assertEqual ( self . _callFUT ( event ) , False ) \n 
~~~ event = DummyEvent ( moving = None , loading = True ) \n 
~~ def test_objectmap_is_None ( self ) : \n 
~~~ event = DummyEvent ( moving = None , object = None , loading = False ) \n 
~~ def test_no_acls ( self ) : \n 
~~~ from substanced . interfaces import IFolder \n 
resource1 = testing . DummyResource ( __provides__ = IFolder ) \n 
resource2 = testing . DummyResource ( ) \n 
resource1 [ ] = resource2 \n 
objectmap = DummyObjectMap ( ) \n 
resource1 . __objectmap__ = objectmap \n 
event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n 
self . assertEqual ( objectmap . connections , [ ] ) \n 
~~ def test_with_acls ( self ) : \n 
~~~ from ... interfaces import PrincipalToACLBearing \n 
from substanced . interfaces import IFolder \n 
resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n 
resource2 . __acl__ = [ ( None , , None ) , ( None , 2 , None ) ] \n 
objectmap . connections , \n 
[ ( 2 , resource2 , PrincipalToACLBearing ) , \n 
( 1 , resource1 , PrincipalToACLBearing ) ] \n 
~~ ~~ class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ event = DummyEvent ( object = None ) \n 
~~ def test_gardenpath ( self ) : \n 
resource = testing . DummyResource ( ) \n 
resource . __objectmap__ = objectmap \n 
event = DummyEvent ( \n 
object = resource , \n 
new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n 
old_acl = [ ( None , , None ) , ( None , 2 , None ) ] , \n 
[ ( 1 , resource , PrincipalToACLBearing ) ] \n 
objectmap . disconnections , \n 
[ ( 2 , resource , PrincipalToACLBearing ) ] \n 
~~ ~~ class DummyObjectMap ( object ) : \n 
~~~ def __init__ ( self , result = ( ) ) : \n 
self . connections = [ ] \n 
self . disconnections = [ ] \n 
~~ def targets ( self , object , reftype ) : \n 
~~~ return self . result \n 
~~ def connect ( self , source , target , reftype ) : \n 
~~~ self . connections . append ( ( source , target , reftype ) ) \n 
~~ def disconnect ( self , source , target , reftype ) : \n 
~~~ self . disconnections . append ( ( source , target , reftype ) ) \n 
~~~ def __init__ ( self , ** kw ) : \n 
~~ ~~ class DummyRegistry ( object ) : \n 
~~~ def subscribers ( self , * arg ) : \n 
~~ ~~ from pyramid . httpexceptions import ( \n 
HTTPForbidden , \n 
HTTPFound \n 
from pyramid . renderers import get_renderer \n 
from pyramid . session import check_csrf_token \n 
from pyramid . security import ( \n 
remember , \n 
forget , \n 
Authenticated , \n 
NO_PERMISSION_REQUIRED , \n 
from ... util import get_oid \n 
from . . import mgmt_view \n 
from substanced . interfaces import IUserLocator \n 
from substanced . principal import DefaultUserLocator \n 
from substanced . event import LoggedIn \n 
context = HTTPForbidden , \n 
permission = NO_PERMISSION_REQUIRED , \n 
effective_principals = Authenticated , \n 
def login ( context , request ) : \n 
~~~ login_url = request . sdiapi . mgmt_path ( request . context , ) \n 
referrer = request . url \n 
if in referrer : \n 
~~~ return HTTPForbidden ( ) \n 
~~ if login_url in referrer : \n 
~~~ referrer = request . sdiapi . mgmt_path ( request . virtual_root ) \n 
~~ came_from = request . session . setdefault ( , referrer ) \n 
login = \n 
password = \n 
if in request . params : \n 
~~~ check_csrf_token ( request ) \n 
~~~ request . sdiapi . flash ( , ) \n 
~~~ login = request . params [ ] \n 
password = request . params [ ] \n 
adapter = request . registry . queryMultiAdapter ( \n 
( context , request ) , \n 
IUserLocator \n 
if adapter is None : \n 
~~~ adapter = DefaultUserLocator ( context , request ) \n 
~~ user = adapter . get_user_by_login ( login ) \n 
if user is not None and user . check_password ( password ) : \n 
~~~ request . session . pop ( , None ) \n 
headers = remember ( request , get_oid ( user ) ) \n 
request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n 
return HTTPFound ( location = came_from , headers = headers ) \n 
~~ request . sdiapi . flash ( , ) \n 
~~ ~~ template = get_renderer ( \n 
) . implementation ( ) \n 
return dict ( \n 
url = request . sdiapi . mgmt_path ( request . virtual_root , ) , \n 
came_from = came_from , \n 
login = login , \n 
password = password , \n 
login_template = template , \n 
def logout ( request ) : \n 
~~~ headers = forget ( request ) \n 
return HTTPFound ( location = request . sdiapi . mgmt_path ( request . context ) , \n 
headers = headers ) \n 
~~ import fnmatch \n 
~~~ import simplejson as json \n 
~~ def dump_sorted_json_string ( input , ** kwargs ) : \n 
return json . dumps ( input , sort_keys = True , separators = ( , ) , ** kwargs ) \n 
~~ def load_extra_plugins ( pathspec ) : \n 
loaded_plugins = [ ] \n 
paths = pathspec . split ( ) \n 
for path in paths : \n 
~~~ loaded_plugins . extend ( _load_plugins_from_dir ( path ) ) \n 
~~ return loaded_plugins \n 
~~ def _load_plugins_from_dir ( path ) : \n 
plugins = [ ] \n 
full_path = os . path . expanduser ( path ) \n 
~~~ filtered = fnmatch . filter ( os . listdir ( path ) , ) \n 
plugins . extend ( filtered ) \n 
~~~ sys . path . insert ( 1 , path ) \n 
~~ for plugin in plugins : \n 
~~~ match = re . search ( , plugin ) \n 
~~~ __import__ ( match . group ( ) , globals ( ) , locals ( ) , [ ] , - 1 ) \n 
~~~ loaded_plugins . append ( match . group ( ) ) \n 
~~ ~~ ~~ return loaded_plugins \n 
~~ __author__ = \n 
import websocket \n 
class Session ( object ) : \n 
~~~ self . running = False \n 
self . thread = None \n 
self . ws = None \n 
self . port = 443 \n 
self . inbound = [ ] \n 
~~ def connect ( self , host , port ) : \n 
~~~ if not self . is_connected ( ) : \n 
~~~ if type ( port ) == int : \n 
~~~ port = str ( port ) \n 
~~ url = + host + + port + \n 
~~~ self . ws = websocket . WebSocket ( ) \n 
self . ws . connect ( url , origin = ) \n 
self . running = True \n 
self . thread = threading . Thread ( name = , target = self . run ) \n 
self . thread . start ( ) \n 
~~~ print ( + url ) \n 
~~~ if self . is_connected ( ) : \n 
~~~ if self . ws . connected : \n 
~~~ self . ws . close ( ) \n 
~~ def is_connected ( self ) : \n 
~~~ return self . running and self . ws . connected \n 
~~~ while self . is_connected ( ) and self . thread == threading . current_thread ( ) : \n 
~~~ data = self . ws . recv ( ) \n 
self . inbound . append ( data ) \n 
~~~ print ( + str ( self . ws . connected ) + + str ( ex ) ) \n 
~~ ~~ ~~ def read ( self ) : \n 
~~~ if len ( self . inbound ) > 0 : \n 
~~~ data = self . inbound [ 0 ] \n 
self . inbound = self . inbound [ 1 : ] \n 
~~ def write ( self , data ) : \n 
~~~ if type ( data ) == bytearray : \n 
~~ if len ( data ) > 0 : \n 
~~~ self . ws . send ( data ) \n 
~~~ print ( + str ( ex ) ) \n 
~~ ~~ ~~ return False \n 
~~~ ses = Session ( ) \n 
################################################################################################# \n 
from pyral import Rally , rallyWorkset \n 
errout = sys . stderr . write \n 
def main ( args ) : \n 
~~~ options = [ opt for opt in args if opt . startswith ( ) ] \n 
args = [ arg for arg in args if arg not in options ] \n 
if not args : \n 
~~ query = "" \n 
target = args [ 0 ] \n 
if target in [ , , ] : \n 
~~~ target = "HierarchicalRequirement" \n 
~~ if in target : \n 
~~~ parent , entity = target . split ( , 1 ) \n 
target = entity \n 
server , username , password , apikey , workspace , project = rallyWorkset ( options ) \n 
~~~ if apikey : \n 
~~~ rally = Rally ( server , apikey = apikey , workspace = workspace , project = project ) \n 
~~~ rally = Rally ( server , user = username , password = password , workspace = workspace , project = project ~~ ~~ except Exception as ex : \n 
~~~ errout ( str ( ex . args [ 0 ] ) ) \n 
~~ typedef = rally . typedef ( target ) \n 
showAttributes ( typedef . Attributes ) \n 
print "" \n 
print "-" * 64 \n 
for ix , ancestor in enumerate ( typedef . inheritanceChain ( ) ) : \n 
~~ ~~ def showAttributes ( attributes ) : \n 
~~~ required = [ ] \n 
optional = [ ] \n 
av_limit = 20 \n 
for attr in attributes : \n 
~~~ name = % attr . ElementName \n 
a_type = % attr . AttributeType \n 
s_type = % attr . SchemaType \n 
s_type = s_type . replace ( , ) \n 
if s_type . upper ( ) == a_type : \n 
~~~ s_type = \n 
~~ reqd = if attr . Required else \n 
rdonly = if attr . ReadOnly else \n 
custom = if attr . Custom else \n 
hidden = if attr . Hidden else \n 
allowedValues = attr . AllowedValues \n 
tank = required if reqd == else optional \n 
num_allowed_values = "" \n 
if len ( allowedValues ) > 0 : \n 
if len ( allowedValues ) == 1 : \n 
~~~ num_allowed_values = num_allowed_values [ : - 1 ] \n 
tank . append ( info ) \n 
if num_allowed_values : \n 
~~ for av in allowedValues [ : av_limit ] : \n 
tank . append ( av_info ) \n 
~~ if len ( allowedValues ) > av_limit : \n 
~~ ~~ for item in required + optional : \n 
~~~ print item . encode ( ) \n 
~~~ main ( sys . argv [ 1 : ] ) \n 
import binascii \n 
import yubi_goog \n 
class TestYubiGoog ( unittest . TestCase ) : \n 
self . test_secret = binascii . hexlify ( . encode ( ) ) \n 
self . test_vectors = [ { : 1111111111 , : } , \n 
{ : 1234567890 , : } , \n 
{ : 2000000000 , : } ] \n 
~~ def test_decode_secret ( self ) : \n 
~~~ decoded = yubi_goog . decode_secret ( self . google_secret ) . upper ( ) \n 
self . assertEqual ( decoded , "6DE9FAF2507F9A99193D" . encode ( ) ) \n 
~~ def test_totp ( self ) : \n 
~~~ for pair in self . test_vectors : \n 
~~~ time = pair [ ] \n 
real_otp = pair [ ] \n 
tm = int ( int ( time ) / 30 ) \n 
tm = struct . pack ( , tm ) \n 
otp = yubi_goog . totp ( self . test_secret , tm ) \n 
self . assertEqual ( otp , real_otp ) \n 
~~ ~~ ~~ if __name__ == : \n 
import itertools \n 
from . core import GroupMixin , Command \n 
from . errors import CommandError \n 
class HelpFormatter : \n 
def __init__ ( self , show_hidden = False , show_check_failure = False , width = 80 ) : \n 
~~~ self . width = width \n 
self . show_hidden = show_hidden \n 
self . show_check_failure = show_check_failure \n 
~~ def has_subcommands ( self ) : \n 
return isinstance ( self . command , GroupMixin ) \n 
~~ def is_bot ( self ) : \n 
return self . command is self . context . bot \n 
~~ def is_cog ( self ) : \n 
return not self . is_bot ( ) and not isinstance ( self . command , Command ) \n 
~~ def shorten ( self , text ) : \n 
if len ( text ) > self . width : \n 
~~~ return text [ : self . width - 3 ] + \n 
~~ return text \n 
def max_name_size ( self ) : \n 
~~~ commands = self . command . commands if not self . is_cog ( ) else self . context . bot . commands \n 
if commands : \n 
~~~ return max ( map ( lambda c : len ( c . name ) , commands . values ( ) ) ) \n 
~~ return 0 \n 
~~~ return len ( self . command . name ) \n 
def clean_prefix ( self ) : \n 
user = self . context . bot . user \n 
return self . context . prefix . replace ( user . mention , + user . name ) \n 
~~ def get_qualified_command_name ( self ) : \n 
entries = [ ] \n 
command = self . command \n 
while command . parent is not None : \n 
~~~ command = command . parent \n 
entries . append ( command . name ) \n 
~~ return . join ( reversed ( entries ) ) \n 
~~ def get_command_signature ( self ) : \n 
result = [ ] \n 
prefix = self . clean_prefix \n 
qualified = self . get_qualified_command_name ( ) \n 
cmd = self . command \n 
if len ( cmd . aliases ) > 0 : \n 
~~~ aliases = . join ( cmd . aliases ) \n 
fmt = \n 
if qualified : \n 
~~~ fmt = \n 
~~ result . append ( fmt . format ( prefix , cmd , aliases , qualified ) ) \n 
~~~ name = prefix + cmd . name if not qualified else prefix + qualified + + cmd . name \n 
result . append ( name ) \n 
~~ params = cmd . clean_params \n 
if len ( params ) > 0 : \n 
~~~ for name , param in params . items ( ) : \n 
~~~ if param . default is not param . empty : \n 
~~~ should_print = param . default if isinstance ( param . default , str ) else param . default if should_print : \n 
~~~ result . append ( . format ( name , param . default ) ) \n 
~~~ result . append ( . format ( name ) ) \n 
~~ ~~ elif param . kind == param . VAR_POSITIONAL : \n 
~~ ~~ ~~ return . join ( result ) \n 
~~ def get_ending_note ( self ) : \n 
~~~ command_name = self . context . invoked_with \n 
~~ def filter_command_list ( self ) : \n 
def predicate ( tuple ) : \n 
~~~ cmd = tuple [ 1 ] \n 
if self . is_cog ( ) : \n 
~~~ if cmd . instance is not self . command : \n 
~~ ~~ if cmd . hidden and not self . show_hidden : \n 
~~ if self . show_check_failure : \n 
~~~ return cmd . can_run ( self . context ) \n 
~~ except CommandError : \n 
~~ ~~ iterator = self . command . commands . items ( ) if not self . is_cog ( ) else self . context . bot . commands return filter ( predicate , iterator ) \n 
~~ def _check_new_page ( self ) : \n 
~~~ if self . _count + len ( self . _current_page ) >= 1980 : \n 
~~~ self . _current_page . append ( ) \n 
self . _pages . append ( . join ( self . _current_page ) ) \n 
self . _current_page = [ ] \n 
self . _count = 4 \n 
~~ def _add_subcommands_to_page ( self , max_width , commands ) : \n 
~~~ for name , command in commands : \n 
~~~ if name in command . aliases : \n 
~~ entry = . format ( name , command . short_doc , width = max_width ) \n 
shortened = self . shorten ( entry ) \n 
self . _count += len ( shortened ) \n 
if self . _check_new_page ( ) : \n 
~~~ self . _count += len ( shortened ) \n 
~~ self . _current_page . append ( shortened ) \n 
~~ ~~ def format_help_for ( self , context , command_or_bot ) : \n 
self . context = context \n 
self . command = command_or_bot \n 
return self . format ( ) \n 
~~ def format ( self ) : \n 
self . _pages = [ ] \n 
description = self . command . description if not self . is_cog ( ) else inspect . getdoc ( self . command \n 
if description : \n 
~~~ self . _current_page . append ( description ) \n 
self . _current_page . append ( ) \n 
self . _count += len ( description ) \n 
~~ if isinstance ( self . command , Command ) : \n 
~~~ signature = self . get_command_signature ( ) \n 
self . _count += 2 + len ( signature ) \n 
self . _current_page . append ( signature ) \n 
if self . command . help : \n 
~~~ self . _count += 2 + len ( self . command . help ) \n 
self . _current_page . append ( self . command . help ) \n 
self . _check_new_page ( ) \n 
~~ if not self . has_subcommands ( ) : \n 
return self . _pages \n 
~~ ~~ max_width = self . max_name_size \n 
def category ( tup ) : \n 
~~~ cog = tup [ 1 ] . cog_name \n 
return cog + if cog is not None else \n 
~~ if self . is_bot ( ) : \n 
~~~ data = sorted ( self . filter_command_list ( ) , key = category ) \n 
for category , commands in itertools . groupby ( data , key = category ) : \n 
~~~ commands = list ( commands ) \n 
if len ( commands ) > 0 : \n 
~~~ self . _current_page . append ( category ) \n 
self . _count += len ( category ) \n 
~~ self . _add_subcommands_to_page ( max_width , commands ) \n 
self . _count += 1 + len ( self . _current_page [ - 1 ] ) \n 
self . _add_subcommands_to_page ( max_width , self . filter_command_list ( ) ) \n 
~~ self . _current_page . append ( ) \n 
ending_note = self . get_ending_note ( ) \n 
self . _count += len ( ending_note ) \n 
self . _current_page . append ( ending_note ) \n 
if len ( self . _current_page ) > 1 : \n 
~~ return self . _pages \n 
~~ ~~ from rx . disposables import Disposable , SingleAssignmentDisposable \n 
from . scheduler import Scheduler \n 
class CatchScheduler ( Scheduler ) : \n 
~~~ def __init__ ( self , scheduler , handler ) : \n 
~~~ self . _scheduler = scheduler \n 
self . _handler = handler \n 
self . _recursive_original = None \n 
self . _recursive_wrapper = None \n 
super ( CatchScheduler , self ) . __init__ ( ) \n 
~~ def local_now ( self ) : \n 
~~~ return self . _scheduler . now ( ) \n 
~~ def schedule_now ( self , state , action ) : \n 
return self . _scheduler . scheduleWithState ( state , self . _wrap ( action ) ) \n 
~~ def schedule_relative ( self , duetime , action , state = None ) : \n 
return self . _scheduler . schedule_relative ( duetime , self . _wrap ( action ) , \n 
state = state ) \n 
~~ def schedule_absolute ( self , duetime , action , state = None ) : \n 
return self . _scheduler . schedule_absolute ( duetime , self . _wrap ( action ) , \n 
~~ def _clone ( self , scheduler ) : \n 
~~~ return CatchScheduler ( scheduler , self . _handler ) \n 
~~ def _wrap ( self , action ) : \n 
~~~ parent = self \n 
def wrapped_action ( self , state ) : \n 
~~~ return action ( parent . _get_recursive_wrapper ( self ) , state ) \n 
~~~ if not parent . _handler ( ex ) : \n 
~~~ raise Exception ( ex ) \n 
~~ return Disposable . empty ( ) \n 
~~ ~~ return wrapped_action \n 
~~ def _get_recursive_wrapper ( self , scheduler ) : \n 
~~~ if self . _recursive_original != scheduler : \n 
~~~ self . _recursive_original = scheduler \n 
wrapper = self . _clone ( scheduler ) \n 
wrapper . _recursive_original = scheduler \n 
wrapper . _recursive_wrapper = wrapper \n 
self . _recursive_wrapper = wrapper \n 
~~ return self . _recursive_wrapper \n 
~~ def schedule_periodic ( self , period , action , state = None ) : \n 
~~~ d = SingleAssignmentDisposable ( ) \n 
failed = [ False ] \n 
def periodic_action ( periodic_state ) : \n 
~~~ if failed [ 0 ] : \n 
~~~ return action ( periodic_state ) \n 
~~~ failed [ 0 ] = True \n 
if not self . _handler ( ex ) : \n 
~~ d . dispose ( ) \n 
~~ ~~ d . disposable = self . _scheduler . schedule_periodic ( periodic_action , \n 
period , state ) \n 
~~ ~~ from . booleandisposable import BooleanDisposable \n 
class SingleAssignmentDisposable ( BooleanDisposable ) : \n 
~~~ super ( SingleAssignmentDisposable , self ) . __init__ ( True ) \n 
~~ ~~ import threading \n 
from rx . blockingobservable import BlockingObservable \n 
from rx . internal import extensionmethod \n 
from rx . internal . enumerator import Enumerator \n 
@ extensionmethod ( BlockingObservable ) \n 
def to_iterable ( self ) : \n 
condition = threading . Condition ( ) \n 
notifications = [ ] \n 
def on_next ( value ) : \n 
condition . acquire ( ) \n 
notifications . append ( value ) \n 
condition . release ( ) \n 
~~ self . observable . materialize ( ) . subscribe ( on_next ) \n 
def gen ( ) : \n 
~~~ condition . acquire ( ) \n 
while not len ( notifications ) : \n 
~~~ condition . wait ( ) \n 
~~ notification = notifications . pop ( 0 ) \n 
if notification . kind == "E" : \n 
~~~ raise notification . exception \n 
~~ if notification . kind == "C" : \n 
~~ condition . release ( ) \n 
yield notification . value \n 
~~ ~~ return Enumerator ( gen ( ) ) \n 
~~ @ extensionmethod ( BlockingObservable ) \n 
def __iter__ ( self ) : \n 
return self . to_iterable ( ) \n 
~~ from rx . observable import Observable \n 
from rx . anonymousobservable import AnonymousObservable \n 
def find_value ( source , predicate , yield_index ) : \n 
~~~ def subscribe ( observer ) : \n 
~~~ i = [ 0 ] \n 
def on_next ( x ) : \n 
~~~ should_run = False \n 
~~~ should_run = predicate ( x , i , source ) \n 
~~~ observer . on_error ( ex ) \n 
~~ if should_run : \n 
~~~ observer . on_next ( i [ 0 ] if yield_index else x ) \n 
observer . on_completed ( ) \n 
~~~ i [ 0 ] += 1 \n 
~~ ~~ def on_completed ( ) : \n 
~~~ observer . on_next ( - 1 if yield_index else None ) \n 
~~ return source . subscribe ( on_next , observer . on_error , on_completed ) \n 
~~ return AnonymousObservable ( subscribe ) \n 
~~ @ extensionmethod ( Observable ) \n 
def find ( self , predicate ) : \n 
return find_value ( self , predicate , False ) \n 
~~ from rx import Observable , AnonymousObservable \n 
from rx . linq . connectableobservable import ConnectableObservable \n 
from rx . disposables import CompositeDisposable \n 
@ extensionmethod ( Observable ) \n 
def multicast ( self , subject = None , subject_selector = None , selector = None ) : \n 
source = self \n 
if subject_selector : \n 
~~~ connectable = source . multicast ( subject = subject_selector ( ) ) \n 
return CompositeDisposable ( selector ( connectable ) . subscribe ( observer ) , connectable . connect \n 
~~~ return ConnectableObservable ( source , subject ) \n 
from rx . observable import Observable \n 
def skip_until_with_time ( self , start_time , scheduler ) : \n 
scheduler = scheduler or timeout_scheduler \n 
if isinstance ( start_time , datetime ) : \n 
~~~ scheduler_method = \n 
~~ def subscribe ( observer ) : \n 
~~~ open = [ False ] \n 
~~~ if open [ 0 ] : \n 
~~~ observer . on_next ( x ) \n 
~~ ~~ subscription = source . subscribe ( on_next , observer . on_error , \n 
observer . on_completed ) \n 
def action ( scheduler , state ) : \n 
~~~ open [ 0 ] = True \n 
~~ disposable = getattr ( scheduler , scheduler_method ) ( start_time , action ) \n 
return CompositeDisposable ( disposable , subscription ) \n 
from rx . concurrency import timeout_scheduler \n 
from rx . subjects import AsyncSubject \n 
from rx . internal import extensionclassmethod \n 
@ extensionclassmethod ( Observable ) \n 
def to_async ( cls , func , scheduler = None ) : \n 
def wrapper ( * args ) : \n 
~~~ subject = AsyncSubject ( ) \n 
~~~ result = func ( * args ) \n 
~~~ subject . on_error ( ex ) \n 
~~ subject . on_next ( result ) \n 
subject . on_completed ( ) \n 
~~ scheduler . schedule ( action ) \n 
return subject . as_observable ( ) \n 
~~ return wrapper \n 
~~ from rx import Lock \n 
class InnerSubscription ( object ) : \n 
~~~ def __init__ ( self , subject , observer ) : \n 
~~~ self . subject = subject \n 
self . observer = observer \n 
self . lock = Lock ( ) \n 
~~ def dispose ( self ) : \n 
~~~ with self . lock : \n 
~~~ if not self . subject . is_disposed and self . observer : \n 
~~~ if self . observer in self . subject . observers : \n 
~~~ self . subject . observers . remove ( self . observer ) \n 
~~ self . observer = None \n 
~~ ~~ ~~ ~~ import unittest \n 
from datetime import datetime , timedelta \n 
from rx . concurrency import NewThreadScheduler \n 
class TestNewThreadScheduler ( unittest . TestCase ) : \n 
~~~ def test_new_thread_now ( self ) : \n 
~~~ scheduler = NewThreadScheduler ( ) \n 
res = scheduler . now ( ) - datetime . utcnow ( ) \n 
assert res < timedelta ( microseconds = 1000 ) \n 
~~ def test_new_thread_schedule_action ( self ) : \n 
ran = [ False ] \n 
~~~ ran [ 0 ] = True \n 
sleep ( 0.1 ) \n 
assert ( ran [ 0 ] == True ) \n 
~~ def test_new_thread_schedule_action_due ( self ) : \n 
starttime = datetime . utcnow ( ) \n 
endtime = [ None ] \n 
~~~ endtime [ 0 ] = datetime . utcnow ( ) \n 
~~ scheduler . schedule_relative ( timedelta ( milliseconds = 200 ) , action ) \n 
sleep ( 0.3 ) \n 
diff = endtime [ 0 ] - starttime \n 
assert ( diff > timedelta ( milliseconds = 180 ) ) \n 
~~ def test_new_thread_schedule_action_cancel ( self ) : \n 
~~~ ran = [ False ] \n 
scheduler = NewThreadScheduler ( ) \n 
~~ d = scheduler . schedule_relative ( timedelta ( milliseconds = 1 ) , action ) \n 
d . dispose ( ) \n 
assert ( not ran [ 0 ] ) \n 
from rx import Observable \n 
from rx . testing import TestScheduler , ReactiveTest , is_prime , MockDisposable \n 
from rx . disposables import Disposable , SerialDisposable \n 
on_next = ReactiveTest . on_next \n 
on_completed = ReactiveTest . on_completed \n 
on_error = ReactiveTest . on_error \n 
subscribe = ReactiveTest . subscribe \n 
subscribed = ReactiveTest . subscribed \n 
disposed = ReactiveTest . disposed \n 
created = ReactiveTest . created \n 
class TestCount ( unittest . TestCase ) : \n 
~~~ def test_count_empty ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_completed ( 250 ) ) \n 
res = scheduler . start ( create = lambda : xs . count ( ) ) . messages \n 
res . assert_equal ( on_next ( 250 , 0 ) , on_completed ( 250 ) ) \n 
~~ def test_count_empty_ii ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_completed ( 250 ) ) \n 
def create ( ) : \n 
~~~ return xs . count ( ) \n 
~~ res = scheduler . start ( create = create ) . messages \n 
res . assert_equal ( on_next ( 250 , 1 ) , on_completed ( 250 ) ) \n 
~~ def test_count_some ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next res = scheduler . start ( create = lambda : xs . count ( ) ) . messages \n 
res . assert_equal ( on_next ( 250 , 3 ) , on_completed ( 250 ) ) \n 
~~ def test_count_throw ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_error ( 210 , ex ) ) \n 
res . assert_equal ( on_error ( 210 , ex ) ) \n 
~~ def test_count_never ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) ) \n 
res . assert_equal ( ) \n 
~~ def test_count_predicate_empty_true ( self ) : \n 
~~~ return xs . count ( lambda _ : True ) \n 
~~ res = scheduler . start ( create = create ) \n 
res . messages . assert_equal ( on_next ( 250 , 0 ) , on_completed ( 250 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 250 ) ) \n 
~~ def test_count_predicate_empty_false ( self ) : \n 
~~~ return xs . count ( lambda _ : False ) \n 
~~ def test_count_predicate_return_true ( self ) : \n 
res . messages . assert_equal ( on_next ( 250 , 1 ) , on_completed ( 250 ) ) \n 
~~ def test_count_predicate_return_false ( self ) : \n 
~~ def test_count_predicate_some_all ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next \n 
~~~ return xs . count ( lambda x : x < 10 ) \n 
res . messages . assert_equal ( on_next ( 250 , 3 ) , on_completed ( 250 ) ) \n 
~~ def test_count_predicate_some_none ( self ) : \n 
~~~ return xs . count ( lambda x : x > 10 ) \n 
~~ def test_count_predicate_some_even ( self ) : \n 
~~~ return xs . count ( lambda x : x % 2 == 0 ) \n 
res . messages . assert_equal ( on_next ( 250 , 2 ) , on_completed ( 250 ) ) \n 
~~ def test_count_predicate_throw_true ( self ) : \n 
res . messages . assert_equal ( on_error ( 210 , ex ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 210 ) ) \n 
~~ def test_count_predicate_throw_false ( self ) : \n 
~~ def test_count_predicate_never ( self ) : \n 
res . messages . assert_equal ( ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 1000 ) ) \n 
~~ def test_count_predicate_predicate_throws ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 230 , 3 ) , on_completed \n 
~~~ def predicate ( x ) : \n 
~~~ if x == 3 : \n 
~~ ~~ return xs . count ( predicate ) \n 
res . messages . assert_equal ( on_error ( 230 , ex ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 230 ) ) \n 
from rx . subjects import Subject \n 
class RxException ( Exception ) : \n 
~~ def _raise ( ex ) : \n 
~~~ raise RxException ( ex ) \n 
~~ class TestTimeInterval ( unittest . TestCase ) : \n 
~~~ def test_interval_timespan_basic ( self ) : \n 
~~~ return Observable . interval ( 100 , scheduler = scheduler ) \n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_next ( 300 , 0 ) , on_next ( 400 , 1 ) , on_next ( 500 , 2 ) , on_next ( 600 \n 
~~ def test_interval_timespan_zero ( self ) : \n 
~~~ return Observable . interval ( 0 , scheduler = scheduler ) \n 
~~ results = scheduler . start ( create , disposed = 210 ) \n 
results . messages . assert_equal ( on_next ( 201 , 0 ) , on_next ( 202 , 1 ) , on_next ( 203 , 2 ) , on_next ( 204 \n 
~~ def test_interval_timespan_negative ( self ) : \n 
~~~ return Observable . interval ( - 1 , scheduler = scheduler ) \n 
~~ def test_interval_timespan_disposed ( self ) : \n 
~~~ return Observable . interval ( 1000 , scheduler = scheduler ) \n 
results . messages . assert_equal ( ) \n 
~~ def test_interval_timespan_observer_throws ( self ) : \n 
xs = Observable . interval ( 1 , scheduler = scheduler ) \n 
xs . subscribe ( lambda x : _raise ( "ex" ) ) \n 
with self . assertRaises ( RxException ) : \n 
~~~ scheduler . start ( ) \n 
~~ ~~ ~~ import unittest \n 
from rx . testing import TestScheduler , ReactiveTest \n 
class TestReplay ( unittest . TestCase ) : \n 
~~~ def test_replay_count_basic ( self ) : \n 
~~~ connection = [ None ] \n 
subscription = [ None ] \n 
ys = [ None ] \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , 3 , None , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
def action1 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( disposed , action2 ) \n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
def action5 ( scheduler , state ) : \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
def action6 ( scheduler , state ) : \n 
~~ scheduler . schedule_absolute ( 550 , action6 ) \n 
def action7 ( scheduler , state ) : \n 
~~ scheduler . schedule_absolute ( 650 , action7 ) \n 
def action8 ( scheduler , state ) : \n 
~~ scheduler . schedule_absolute ( 800 , action8 ) \n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) , on_next ( 521 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_count_error ( self ) : \n 
ex = \n 
~~ scheduler . schedule_absolute ( 800 , action6 ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) , on_next ( 521 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
~~ def test_replay_count_complete ( self ) : \n 
def action1 ( scehduler , state ) : \n 
~~ scheduler . schedule_absolute ( 800 , action ) \n 
~~ def test_replay_count_dispose ( self ) : \n 
~~ scheduler . schedule_absolute ( 475 , action2 ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_count_multiple_connections ( self ) : \n 
~~~ xs = Observable . never ( ) \n 
ys = xs . replay ( None , 3 ) \n 
connection1 = ys . connect ( ) \n 
connection2 = ys . connect ( ) \n 
assert ( connection1 == connection2 ) \n 
connection1 . dispose ( ) \n 
connection2 . dispose ( ) \n 
connection3 = ys . connect ( ) \n 
assert ( connection1 != connection3 ) \n 
~~ def test_replay_count_lambda_zip_complete ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next \n 
def action ( ) : \n 
~~~ def selector ( _xs ) : \n 
~~~ return _xs . take ( 6 ) . repeat ( ) \n 
~~ return xs . replay ( selector , 3 , None , scheduler ) \n 
~~ results = scheduler . start ( action , disposed = 610 ) \n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 600 ) ) \n 
~~ def test_replay_count_lambda_zip_error ( self ) : \n 
~~ def test_replay_count_lambda_zip_dispose ( self ) : \n 
~~ results = scheduler . start ( create , disposed = 470 ) \n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 470 ) ) \n 
~~ def test_replay_time_basic ( self ) : \n 
~~~ subscription = [ None ] \n 
connection = [ None ] \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 150 , scheduler ) \n 
results . messages . assert_equal ( on_next ( 451 , 8 ) , on_next ( 452 , 5 ) , on_next ( 453 , 6 ) , on_next ( 454 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_time_error ( self ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 75 , scheduler ) \n 
results . messages . assert_equal ( on_next ( 451 , 7 ) , on_next ( 521 , 11 ) , on_next ( 561 , 20 ) , on_error ( xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
~~ def test_replay_time_complete ( self ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 85 , scheduler ) \n 
results . messages . assert_equal ( on_next ( 451 , 6 ) , on_next ( 452 , 7 ) , on_next ( 521 , 11 ) , on_next ( 561 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
~~ def test_replay_time_dispose ( self ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 100 , scheduler ) \n 
~~ def test_replay_time_multiple_connections ( self ) : \n 
ys = xs . replay ( None , None , 100 ) \n 
~~ def test_replay_time_lambda_zip_complete ( self ) : \n 
~~ return xs . replay ( selector , None , 50 , scheduler ) \n 
~~ results = scheduler . start ( create , disposed = 610 ) \n 
~~ def test_replay_time_lambda_zip_error ( self ) : \n 
~~ def test_replay_time_lambda_zip_dispose ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next def create ( ) : \n 
~~ class TestTakeUntil ( unittest . TestCase ) : \n 
~~~ def test_take_until_preempt_somedata_next ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 r_msgs = [ on_next ( 150 , 1 ) , on_next ( 225 , 99 ) , on_completed ( 230 ) ] \n 
l = scheduler . create_hot_observable ( l_msgs ) \n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
~~~ return l . take_until ( r ) \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_completed ( 225 ) ) \n 
~~ def test_take_until_preempt_somedata_error ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 r_msgs = [ on_next ( 150 , 1 ) , on_error ( 225 , ex ) ] \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_error ( 225 , ex ) ) \n 
~~ def test_take_until_nopreempt_somedata_empty ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 r_msgs = [ on_next ( 150 , 1 ) , on_completed ( 225 ) ] \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 \n 
~~ def test_take_until_nopreempt_somedata_never ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 l = scheduler . create_hot_observable ( l_msgs ) \n 
r = Observable . never ( ) \n 
~~ def test_take_until_preempt_never_next ( self ) : \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 225 , 2 ) , on_completed ( 250 ) ] \n 
l = Observable . never ( ) \n 
results . messages . assert_equal ( on_completed ( 225 ) ) \n 
~~ def test_take_until_preempt_never_error ( self ) : \n 
r_msgs = [ on_next ( 150 , 1 ) , on_error ( 225 , ex ) ] \n 
results . messages . assert_equal ( on_error ( 225 , ex ) ) \n 
~~ def test_take_until_nopreempt_never_empty ( self ) : \n 
r_msgs = [ on_next ( 150 , 1 ) , on_completed ( 225 ) ] \n 
~~ def test_take_until_nopreempt_never_never ( self ) : \n 
~~ def test_take_until_preempt_beforefirstproduced ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 230 , 2 ) , on_completed ( 240 ) ] \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_completed ( 220 ) ] \n 
results . messages . assert_equal ( on_completed ( 210 ) ) \n 
~~ def test_take_until_preempt_beforefirstproduced_remain_silent_and_proper_disposed ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_error ( 215 , ) , on_completed ( 240 ) ] \n 
source_not_disposed = [ False ] \n 
~~~ source_not_disposed [ 0 ] = True \n 
~~ l = scheduler . create_hot_observable ( l_msgs ) . do_action ( on_next = action ) \n 
assert ( not source_not_disposed [ 0 ] ) \n 
~~ def test_take_until_nopreempt_afterlastproduced_proper_disposed_signal ( self ) : \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 250 , 2 ) , on_completed ( 260 ) ] \n 
signal_not_disposed = [ False ] \n 
~~~ signal_not_disposed [ 0 ] = True \n 
~~ r = scheduler . create_hot_observable ( r_msgs ) . do_action ( on_next = action ) \n 
results . messages . assert_equal ( on_next ( 230 , 2 ) , on_completed ( 240 ) ) \n 
assert ( not signal_not_disposed [ 0 ] ) \n 
~~ ~~ import logging \n 
FORMAT = \n 
logging . basicConfig ( filename = , format = FORMAT , level = logging . DEBUG ) \n 
log = logging . getLogger ( ) \n 
~~~ test_buffer_with_time_or_count_basic ( ) \n 
~~ def prune_old_authorization_codes ( ) : \n 
from . compat import now \n 
from . models import AuthorizationCode \n 
AuthorizationCode . objects . with_expiration_before ( now ( ) ) . delete ( ) \n 
~~ def get_handler ( handler_name ) : \n 
from . conf import options \n 
handlers = options . handlers \n 
for handler in handlers : \n 
~~~ handler_path = handler . split ( "." ) \n 
name = handler_path [ - 2 ] \n 
if handler_name == name : \n 
~~~ handler_module = __import__ ( "." . join ( handler_path [ : - 1 ] ) , { } , { } , str ( handler_path [ - 1 ] ) ) \n 
return getattr ( handler_module , handler_path [ - 1 ] ) ( ) \n 
~~ def request_error_header ( exception ) : \n 
if hasattr ( exception , "error" ) : \n 
~~ if hasattr ( exception , "reason" ) : \n 
~~ return header \n 
~~ def total_seconds ( delta ) : \n 
return delta . days * 86400 + delta . seconds \n 
__all__ = ( \n 
, , , \n 
, , \n 
, , , , \n 
WS_NORMAL = 1000 \n 
WS_GOING_AWAY = 1001 \n 
WS_PROTOCOL_ERROR = 1002 \n 
WS_DATA_CANNOT_ACCEPT = 1003 \n 
WS_RESERVED = 1004 \n 
WS_NO_STATUS_CODE = 1005 \n 
WS_CLOSED_ABNORMALLY = 1006 \n 
WS_MESSAGE_NOT_CONSISTENT = 1007 \n 
WS_MESSAGE_VIOLATE_POLICY = 1008 \n 
WS_MESSAGE_TOO_BIG = 1009 \n 
WS_SERVER_DIDNT_RETURN_EXTENSIONS = 1010 \n 
WS_UNEXPECTED_CONDITION = 1011 \n 
WS_FAILURE_TLS = 1015 \n 
def is_not_used ( code ) : \n 
return 0 <= code <= 999 \n 
~~ def is_reserved ( code ) : \n 
return 1000 <= code <= 2999 \n 
~~ def is_library ( code ) : \n 
return 3000 <= code <= 3999 \n 
~~ def is_private ( code ) : \n 
return 4000 <= code <= 4999 \n 
~~ DATABASES = { \n 
import fileinput \n 
ALPHABET = ( "A" , "C" , "G" , "T" ) \n 
def sequence_count ( string , wordlength ) : \n 
wc = { } \n 
while i < len ( string ) - wordlength + 1 : \n 
~~~ word = string [ i : i + wordlength ] \n 
if word in wc : \n 
~~~ wc [ word ] += 1 \n 
~~~ wc [ word ] = 1 \n 
~~ i += 1 \n 
~~ return wc \n 
~~ def pretty_print ( wordcount_dict , wordlength ) : \n 
output = "" \n 
e = 0 \n 
for w in itertools . product ( ALPHABET , repeat = wordlength ) : \n 
~~~ w = . join ( w ) \n 
~~~ output += "%3.10s%13s\\n" % ( e , wordcount_dict [ w ] ) \n 
~~ except IndexError : \n 
~~~ output += "%s\\t0\\n" \n 
~~ e += 1 \n 
~~ return output \n 
~~ def histogram ( filename , wordlength ) : \n 
for line in open ( filename ) . readlines ( ) : \n 
~~~ wcnew = sequence_count ( line , wordlength ) \n 
wc = { i : wc . get ( i , 0 ) + wcnew . get ( i , 0 ) for i in set ( wc ) | set ( wcnew ) } \n 
~~ return pretty_print ( wc , wordlength ) \n 
~~ if __name__ == "__main__" : \n 
~~~ print histogram ( sys . argv [ 1 ] , 3 ) \n 
~~ from setuptools import setup \n 
version = , \n 
py_modules = [ ] , \n 
install_requires = [ \n 
entry_points = , \n 
"""\nDocuments\n""" \n 
import shutil \n 
import sublime \n 
import zipfile \n 
from . import pyarduino \n 
from . import st_base \n 
from . import st_menu \n 
from . import st_console \n 
def set_pyarduino ( ) : \n 
~~~ user_path = st_base . get_stino_user_path ( ) \n 
package_path = st_base . get_plugin_path ( ) \n 
stino_path = os . path . join ( package_path , ) \n 
pyarduino_path = os . path . join ( stino_path , ) \n 
settings_path = os . path . join ( pyarduino_path , ) \n 
settings = pyarduino . base . settings . Settings ( settings_path ) \n 
settings . set ( , package_path ) \n 
settings . set ( , user_path ) \n 
~~ def load_keywords ( ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
ide_dir = arduino_info . get_ide_dir ( ) \n 
keywords = ide_dir . get_keywords ( ) \n 
for root in arduino_info . get_root_dirs ( ) : \n 
~~~ libraries = root . get_libraries ( ) \n 
for library in libraries : \n 
~~~ keywords += library . get_keywords ( ) \n 
~~ for package in root . get_packages ( ) : \n 
~~~ for platform in package . get_platforms ( ) : \n 
~~~ libraries = platform . get_libraries ( ) \n 
~~ ~~ ~~ ~~ return keywords \n 
~~ def create_completions ( ) : \n 
file_path = os . path . join ( user_path , ) \n 
completions_file = pyarduino . base . json_file . JSONFile ( file_path ) \n 
cpp_keywords = [ , , , , ] \n 
cpp_keywords += [ , , , , ] \n 
keywords = load_keywords ( ) \n 
keyword_ids = [ k . get_id ( ) for k in keywords ] \n 
keyword_ids += cpp_keywords \n 
completions_dict = { : } \n 
completions_dict [ ] = keyword_ids \n 
completions_file . set_data ( completions_dict ) \n 
~~ def create_syntax_file ( ) : \n 
~~~ keywords = load_keywords ( ) \n 
LITERAL1s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
KEYWORD1s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
KEYWORD2s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
KEYWORD3s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
LITERAL1_text = . join ( LITERAL1s ) \n 
KEYWORD1_text = . join ( KEYWORD1s ) \n 
KEYWORD2_text = . join ( KEYWORD2s ) \n 
KEYWORD3_text = . join ( KEYWORD3s ) \n 
preset_path = st_base . get_preset_path ( ) \n 
file_path = os . path . join ( preset_path , ) \n 
template_file = pyarduino . base . abs_file . File ( file_path ) \n 
text = template_file . read ( ) \n 
text = text . replace ( , LITERAL1_text ) \n 
text = text . replace ( , KEYWORD1_text ) \n 
text = text . replace ( , KEYWORD2_text ) \n 
text = text . replace ( , KEYWORD3_text ) \n 
user_path = st_base . get_stino_user_path ( ) \n 
syntax_file = pyarduino . base . abs_file . File ( file_path ) \n 
syntax_file . write ( text ) \n 
~~ def create_sub_menus ( ) : \n 
st_menu . create_sketchbook_menu ( arduino_info ) \n 
st_menu . create_examples_menu ( arduino_info ) \n 
st_menu . create_libraries_menu ( arduino_info ) \n 
st_menu . create_boards_menu ( arduino_info ) \n 
st_menu . create_board_options_menu ( arduino_info ) \n 
st_menu . create_programmers_menu ( arduino_info ) \n 
st_menu . create_serials_menu ( ) \n 
st_menu . create_languages_menu ( ) \n 
~~ def create_menus ( ) : \n 
~~~ st_menu . create_main_menu ( ) \n 
settings = st_base . get_settings ( ) \n 
show_arduino_menu = settings . get ( , True ) \n 
if show_arduino_menu : \n 
~~~ st_menu . create_arduino_menu ( ) \n 
create_sub_menus ( ) \n 
~~~ user_menu_path = st_base . get_user_menu_path ( ) \n 
if os . path . isdir ( user_menu_path ) : \n 
~~~ shutil . rmtree ( user_menu_path ) \n 
~~ ~~ ~~ def update_menu ( ) : \n 
arduino_info . reload ( ) \n 
arduino_info . update ( ) \n 
i18n = st_base . get_i18n ( ) \n 
i18n . load ( ) \n 
create_menus ( ) \n 
create_completions ( ) \n 
create_syntax_file ( ) \n 
~~ def create_sketch ( sketch_name ) : \n 
sketchbook_dir = arduino_info . get_sketchbook_dir ( ) \n 
sketchbook_path = sketchbook_dir . get_path ( ) \n 
sketch_path = os . path . join ( sketchbook_path , sketch_name ) \n 
index = 0 \n 
org_name = sketch_name \n 
while os . path . exists ( sketch_path ) : \n 
~~~ sketch_name = % ( org_name , index ) \n 
index += 1 \n 
~~ os . makedirs ( sketch_path ) \n 
bare_gcc = settings . get ( , False ) \n 
if bare_gcc : \n 
~~~ ext = \n 
~~ template_file_name = + ext \n 
template_file_path = os . path . join ( preset_path , template_file_name ) \n 
template_file = pyarduino . base . abs_file . File ( template_file_path ) \n 
src_code = template_file . read ( ) \n 
src_file_name = sketch_name + ext \n 
src_file_path = os . path . join ( sketch_path , src_file_name ) \n 
src_file = pyarduino . base . abs_file . File ( src_file_path ) \n 
src_code = src_code . replace ( , src_file_name ) \n 
src_file . write ( src_code ) \n 
return sketch_path \n 
~~ def new_sketch ( window , sketch_name ) : \n 
~~~ sketch_path = create_sketch ( sketch_name ) \n 
open_sketch ( window , sketch_path ) \n 
~~ def open_sketch ( window , sketch_path ) : \n 
~~~ project = pyarduino . arduino_project . Project ( sketch_path ) \n 
ino_files = project . list_ino_files ( ) \n 
cpp_files = project . list_cpp_files ( ) \n 
h_files = project . list_h_files ( ) \n 
files = ino_files + cpp_files + h_files \n 
views = [ ] \n 
for f in files : \n 
~~~ view = window . open_file ( f . get_path ( ) ) \n 
views . append ( view ) \n 
~~ if views : \n 
~~~ window . focus_view ( views [ 0 ] ) \n 
~~ project_params = window . project_data ( ) \n 
if project_params is None : \n 
~~~ project_params = { } \n 
~~ folders = project_params . setdefault ( , [ ] ) \n 
folders . append ( { : True , : sketch_path } ) \n 
project_params [ ] = folders \n 
window . set_project_data ( project_params ) \n 
~~ def import_library ( view , edit , library_path ) : \n 
target_arch = arduino_info . get_target_board_info ( ) . get_target_arch ( ) \n 
library = pyarduino . arduino_library . Library ( library_path ) \n 
h_files = library . list_h_files ( target_arch ) \n 
region = sublime . Region ( 0 , view . size ( ) ) \n 
src_text = view . substr ( region ) \n 
headers = pyarduino . arduino_src . list_headers_from_src ( src_text ) \n 
h_files = [ f for f in h_files if not f . get_name ( ) in headers ] \n 
includes = [ % f . get_name ( ) for f in h_files ] \n 
text = . join ( includes ) \n 
if text : \n 
~~~ text += \n 
~~ view . insert ( edit , 0 , text ) \n 
~~ def handle_sketch ( view , func , using_programmer = False ) : \n 
~~~ window = view . window ( ) \n 
views = window . views ( ) \n 
if view not in views : \n 
~~~ view = window . active_view ( ) \n 
~~ if view . file_name ( ) is None : \n 
~~~ tmp_path = pyarduino . base . sys_path . get_tmp_path ( ) \n 
tmp_path = os . path . join ( tmp_path , ) \n 
name = str ( time . time ( ) ) . split ( ) [ 1 ] \n 
sketch_path = os . path . join ( tmp_path , name ) \n 
os . makedirs ( sketch_path ) \n 
~~ src_file_name = name + ext \n 
text = view . substr ( region ) \n 
src_file . write ( text ) \n 
view . set_scratch ( True ) \n 
window = view . window ( ) \n 
window . run_command ( ) \n 
view = window . open_file ( src_file_path ) \n 
~~ if view . is_dirty ( ) : \n 
~~~ view . run_command ( ) \n 
~~ file_path = view . file_name ( ) \n 
sketch_path = os . path . dirname ( file_path ) \n 
func ( view , sketch_path , using_programmer ) \n 
~~ def build_sketch ( view , sketch_path , using_programmer = False ) : \n 
console_name = + sketch_path + + str ( time . time ( ) ) \n 
console = st_console . Console ( window , name = console_name ) \n 
compiler = pyarduino . arduino_compiler . Compiler ( sketch_path , console ) \n 
compiler . build ( ) \n 
~~ def upload_sketch ( view , sketch_path , using_programmer ) : \n 
uploader = pyarduino . arduino_uploader . Uploader ( sketch_path , console ) \n 
uploader . upload ( using_programmer ) \n 
~~ def burn_bootloader ( window ) : \n 
~~~ console_name = + str ( time . time ( ) ) \n 
bootloader = pyarduino . arduino_bootloader . Bootloader ( console ) \n 
bootloader . burn ( ) \n 
~~ def change_board ( window , board_id ) : \n 
arduino_info . change_board ( board_id ) \n 
view = window . active_view ( ) \n 
set_status ( view ) \n 
~~ def change_sub_board ( window , option_index , sub_board_id ) : \n 
arduino_info . change_sub_board ( option_index , sub_board_id ) \n 
~~ def change_programmer ( programmer_id ) : \n 
arduino_info . change_programmer ( programmer_id ) \n 
~~ def archive_sketch ( window , sketch_path ) : \n 
~~~ sketch_name = os . path . basename ( sketch_path ) \n 
console = st_console . Console ( window , str ( time . time ( ) ) ) \n 
message_queue = pyarduino . base . message_queue . MessageQueue ( console ) \n 
message_queue . put ( , sketch_name ) \n 
zip_file_name = sketch_name + \n 
document_path = pyarduino . base . sys_path . get_document_path ( ) \n 
zip_file_path = os . path . join ( document_path , zip_file_name ) \n 
os . chdir ( sketch_path ) \n 
sketch_dir = pyarduino . base . abs_file . Dir ( sketch_path ) \n 
files = sketch_dir . list_files ( ) \n 
file_names = [ f . get_name ( ) for f in files ] \n 
~~~ opened_zipfile = zipfile . ZipFile ( zip_file_path , \n 
, zipfile . ZIP_DEFLATED ) \n 
~~~ text = \n 
message_queue . put ( text ) \n 
~~~ for file_name in file_names : \n 
~~~ opened_zipfile . write ( file_name ) \n 
~~ opened_zipfile . close ( ) \n 
message_queue . put ( , zip_file_path ) \n 
~~ message_queue . print_screen ( one_time = True ) \n 
~~ def get_url ( url ) : \n 
ide_path = ide_dir . get_path ( ) \n 
file_name = url + \n 
reference_folder = os . path . join ( ide_path , ) \n 
reference_file = os . path . join ( reference_folder , file_name ) \n 
if os . path . isfile ( reference_file ) : \n 
~~~ reference_file = reference_file . replace ( os . path . sep , ) \n 
url = + reference_file \n 
~~~ url = \n 
~~ return url \n 
~~ def find_in_ref ( view ) : \n 
id_keyword_dict = ide_dir . get_id_keyword_dict ( ) \n 
ref_list = [ ] \n 
selected_text = get_selected_text_from_view ( view ) \n 
words = get_word_list_from_text ( selected_text ) \n 
for word in words : \n 
~~~ if word in id_keyword_dict : \n 
~~~ keyword = id_keyword_dict . get ( word ) \n 
ref = keyword . get_ref ( ) \n 
if ref and not ref in ref_list : \n 
~~~ ref_list . append ( ref ) \n 
~~ ~~ ~~ for ref in ref_list : \n 
~~~ url = get_url ( ref ) \n 
sublime . run_command ( , { : url } ) \n 
~~ ~~ def get_selected_text_from_view ( view ) : \n 
~~~ selected_text = \n 
region_list = view . sel ( ) \n 
for region in region_list : \n 
~~~ selected_region = view . word ( region ) \n 
selected_text += view . substr ( selected_region ) \n 
selected_text += \n 
~~ return selected_text \n 
~~ def get_word_list_from_text ( text ) : \n 
~~~ pattern_text = \n 
word_list = re . findall ( pattern_text , text ) \n 
return word_list \n 
~~ def is_arduino_ide_path ( dir_path ) : \n 
~~~ path = pyarduino . arduino_root . update_ide_path ( dir_path ) \n 
return pyarduino . arduino_root . is_arduino_ide_path ( path ) \n 
~~ def set_arduino_ide_path ( window , dir_path ) : \n 
~~~ if is_arduino_ide_path ( dir_path ) : \n 
arduino_info . change_ide_path ( dir_path ) \n 
version_name = ide_dir . get_version_name ( ) \n 
text = \n 
message_queue . put ( text , version_name , dir_path ) \n 
message_queue . print_screen ( one_time = True ) \n 
return 0 \n 
~~ ~~ def set_sketchbook_path ( window , dir_path ) : \n 
arduino_info . change_sketchbook_path ( dir_path ) \n 
message_queue . put ( text , dir_path ) \n 
~~ def set_build_path ( window , dir_path ) : \n 
~~~ settings = st_base . get_settings ( ) \n 
settings . set ( , dir_path ) \n 
~~ def select_arduino_dir ( window ) : \n 
~~~ select_dir ( window , func = set_arduino_ide_path , \n 
condition_func = is_arduino_ide_path ) \n 
~~ def change_sketchbook_dir ( window ) : \n 
~~~ select_dir ( window , func = set_sketchbook_path , is_user = True ) \n 
~~ def change_build_dir ( window ) : \n 
~~~ select_dir ( window , func = set_build_path , is_user = True ) \n 
~~ def select_dir ( window , index = - 2 , level = 0 , paths = None , \n 
func = None , condition_func = None , is_user = False ) : \n 
~~~ if index == - 1 : \n 
~~ if level > 0 and index == 0 : \n 
~~~ sel_path = paths [ 0 ] . split ( ) [ 1 ] [ : - 1 ] \n 
if func : \n 
~~~ return_code = func ( window , sel_path ) \n 
if return_code == 0 : \n 
~~~ if index == 1 : \n 
~~~ level -= 1 \n 
~~ elif index > 1 : \n 
~~~ level += 1 \n 
~~ if level <= 0 : \n 
~~~ level = 0 \n 
dir_path = \n 
parent_path = \n 
if is_user : \n 
~~~ paths = pyarduino . base . sys_path . list_user_root_path ( ) \n 
~~~ paths = pyarduino . base . sys_path . list_os_root_path ( ) \n 
~~~ sel_path = paths [ index ] \n 
if sel_path == pyarduino . base . sys_path . ROOT_PATH : \n 
~~~ sel_path = \n 
~~ dir_path = os . path . abspath ( sel_path ) \n 
if condition_func and condition_func ( dir_path ) : \n 
~~~ func ( window , dir_path ) \n 
~~ parent_path = os . path . join ( dir_path , ) \n 
cur_dir = pyarduino . base . abs_file . Dir ( dir_path ) \n 
sub_dirs = cur_dir . list_dirs ( ) \n 
paths = [ d . get_path ( ) for d in sub_dirs ] \n 
~~ paths . insert ( 0 , parent_path ) \n 
paths . insert ( 0 , % dir_path ) \n 
~~ sublime . set_timeout ( lambda : window . show_quick_panel ( \n 
paths , lambda index : select_dir ( window , index , level , paths , \n 
func , condition_func , is_user ) ) , 5 ) \n 
~~ def update_serial_info ( ) : \n 
~~~ st_menu . create_serials_menu ( ) \n 
window = sublime . active_window ( ) \n 
~~ def get_serial_listener ( ) : \n 
~~~ serial_listener = pyarduino . base . serial_listener . SerialListener ( \n 
func = update_serial_info ) \n 
return serial_listener \n 
~~ def toggle_serial_monitor ( window ) : \n 
~~~ monitor_module = pyarduino . base . serial_monitor \n 
serial_monitor = None \n 
serial_port = settings . get ( , ) \n 
serial_ports = pyarduino . base . serial_port . list_serial_ports ( ) \n 
if serial_port in serial_ports : \n 
~~~ if serial_port in monitor_module . serials_in_use : \n 
~~~ serial_monitor = monitor_module . serial_monitor_dict . get ( \n 
serial_port , None ) \n 
~~ if not serial_monitor : \n 
~~~ monitor_view = st_console . MonitorView ( window , serial_port ) \n 
serial_monitor = pyarduino . base . serial_monitor . SerialMonitor ( \n 
serial_port , monitor_view ) \n 
~~ if not serial_monitor . is_running ( ) : \n 
~~~ serial_monitor . start ( ) \n 
if not serial_port in monitor_module . serials_in_use : \n 
~~~ monitor_module . serials_in_use . append ( serial_port ) \n 
~~ monitor_module . serial_monitor_dict [ serial_port ] = serial_monitor \n 
~~~ serial_monitor . stop ( ) \n 
monitor_module . serials_in_use . remove ( serial_port ) \n 
~~ ~~ ~~ def send_serial_message ( text ) : \n 
if serial_port in monitor_module . serials_in_use : \n 
if serial_monitor and serial_monitor . is_running ( ) : \n 
~~~ serial_monitor . send ( text ) \n 
~~ ~~ ~~ def set_status ( view ) : \n 
~~~ infos = [ ] \n 
exts = [ , , , , ] \n 
file_name = view . file_name ( ) \n 
if file_name and file_name . split ( ) [ - 1 ] in exts : \n 
version_name = arduino_info . get_ide_dir ( ) . get_version_name ( ) \n 
version_text = % version_name \n 
infos . append ( version_text ) \n 
target_board_info = arduino_info . get_target_board_info ( ) \n 
target_board = target_board_info . get_target_board ( ) \n 
if target_board : \n 
~~~ target_board_caption = target_board . get_caption ( ) \n 
infos . append ( target_board_caption ) \n 
if target_board . has_options ( ) : \n 
~~~ target_sub_boards = target_board_info . get_target_sub_boards ( ) \n 
for index , target_sub_board in enumerate ( target_sub_boards ) : \n 
~~~ caption_text = target_sub_board . get_caption ( ) \n 
if index == 0 : \n 
~~~ caption_text = + caption_text \n 
~~ if index == len ( target_sub_boards ) - 1 : \n 
~~~ caption_text += \n 
~~ infos . append ( caption_text ) \n 
~~~ target_board_caption = \n 
~~ settings = st_base . get_settings ( ) \n 
target_serial_port = settings . get ( , ) \n 
if not target_serial_port in serial_ports : \n 
~~~ target_serial_port = \n 
~~ serial_text = % target_serial_port \n 
infos . append ( serial_text ) \n 
text = . join ( infos ) \n 
view . set_status ( , text ) \n 
~~ ~~ def show_items_panel ( window , item_type ) : \n 
~~~ sublime . set_timeout ( lambda : window . show_quick_panel ( [ , ] , ppp ) ) \n 
~~ def ppp ( index ) : \n 
~~~ print ( index ) \n 
~~ VERSION = \n 
if os . name == : \n 
~~~ from . serialwin32 import * \n 
~~ elif os . name == : \n 
~~~ from . serialposix import * \n 
~~ protocol_handler_packages = [ \n 
def serial_for_url ( url , * args , ** kwargs ) : \n 
do_open = not in kwargs or not kwargs [ ] \n 
if in kwargs : del kwargs [ ] \n 
klass = Serial \n 
~~~ url_nocase = url . lower ( ) \n 
~~~ if in url_nocase : \n 
~~~ protocol = url_nocase . split ( , 1 ) [ 0 ] \n 
for package_name in protocol_handler_packages : \n 
~~~ module_name = % ( package_name , protocol , ) \n 
~~~ handler_module = __import__ ( module_name ) \n 
~~~ klass = sys . modules [ module_name ] . Serial \n 
~~~ raise ValueError ( % ( protocol , ) ) \n 
~~~ klass = Serial \n 
~~ ~~ instance = klass ( None , * args , ** kwargs ) \n 
instance . port = url \n 
if do_open : \n 
~~~ instance . open ( ) \n 
~~ return instance \n 
~~ import director \n 
import director . objectmodel as om \n 
from director import visualization as vis \n 
from director . visualization import PolyDataItem \n 
from director import filterUtils \n 
from director import ioUtils \n 
from director import meshmanager \n 
from director import transformUtils \n 
from director . uuidutil import newUUID \n 
from director . debugVis import DebugData \n 
from director import vtkAll as vtk \n 
import numpy as np \n 
import uuid \n 
class AffordanceItem ( PolyDataItem ) : \n 
LOCAL_PROPERTY_NAMES = ( ) \n 
def __init__ ( self , name , polyData , view ) : \n 
~~~ PolyDataItem . __init__ ( self , name , polyData , view ) \n 
self . params = { } \n 
self . addProperty ( , newUUID ( ) , attributes = om . PropertyAttributes ( hidden = True ) ) \n 
self . addProperty ( , True ) \n 
self . addProperty ( , [ 0.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 0.0 ] , attributes = om . PropertyAttributes self . addProperty ( , False ) \n 
self . properties . setPropertyIndex ( , 0 ) \n 
self . setProperty ( , om . Icons . Hammer ) \n 
~~ def getPose ( self ) : \n 
~~~ childFrame = self . getChildFrame ( ) \n 
t = childFrame . transform if childFrame else vtk . vtkTransform ( ) \n 
return transformUtils . poseFromTransform ( t ) \n 
~~ def getDescription ( self ) : \n 
~~~ d = OrderedDict ( ) \n 
d [ ] = type ( self ) . __name__ \n 
d . update ( self . properties . _properties ) \n 
d [ ] = self . getPose ( ) \n 
~~ def _onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ PolyDataItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
if propertyName == : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
~~ ~~ def updateGeometryFromProperties ( ) : \n 
~~ def setPolyData ( self , polyData ) : \n 
~~~ if polyData . GetNumberOfPoints ( ) : \n 
~~~ originPose = self . getProperty ( ) \n 
pos , quat = originPose [ : 3 ] , originPose [ 3 : ] \n 
t = transformUtils . transformFromPose ( pos , quat ) \n 
polyData = filterUtils . transformPolyData ( polyData , t . GetLinearInverse ( ) ) \n 
~~ PolyDataItem . setPolyData ( self , polyData ) \n 
~~ def repositionFromDescription ( self , desc ) : \n 
~~~ position , quat = desc [ ] \n 
t = transformUtils . transformFromPose ( position , quat ) \n 
self . getChildFrame ( ) . copyFrame ( t ) \n 
~~ def loadDescription ( self , desc , copyMode = COPY_MODE_ALL ) : \n 
~~~ self . syncProperties ( desc , copyMode ) \n 
self . repositionFromDescription ( desc ) \n 
self . _renderAllViews ( ) \n 
~~ def syncProperties ( self , desc , copyMode = COPY_MODE_ALL ) : \n 
~~~ for propertyName , propertyValue in desc . iteritems ( ) : \n 
~~~ if copyMode == self . COPY_MODE_SKIP_LOCAL : \n 
~~~ if propertyName in self . LOCAL_PROPERTY_NAMES : \n 
~~ ~~ if self . hasProperty ( propertyName ) and ( self . getProperty ( propertyName ) != propertyValue ) : \n 
~~~ self . setProperty ( propertyName , propertyValue ) \n 
~~ ~~ ~~ def onRemoveFromObjectModel ( self ) : \n 
~~~ PolyDataItem . onRemoveFromObjectModel ( self ) \n 
~~ ~~ class BoxAffordanceItem ( AffordanceItem ) : \n 
~~~ def __init__ ( self , name , view ) : \n 
~~~ AffordanceItem . __init__ ( self , name , vtk . vtkPolyData ( ) , view ) \n 
self . addProperty ( , [ 0.25 , 0.25 , 0.25 ] , attributes = om . PropertyAttributes ( decimals self . addProperty ( , 0 , attributes = om . PropertyAttributes ( minimum = 0 , maximum = 1000 self . properties . setPropertyIndex ( , 0 ) \n 
self . properties . setPropertyIndex ( , 1 ) \n 
self . updateGeometryFromProperties ( ) \n 
~~ def updateGeometryFromProperties ( self ) : \n 
~~~ d = DebugData ( ) \n 
d . addCube ( self . getProperty ( ) , ( 0 , 0 , 0 ) , subdivisions = self . getProperty ( self . setPolyData ( d . getPolyData ( ) ) \n 
~~~ AffordanceItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
if propertyName in ( , ) : \n 
~~ ~~ ~~ class SphereAffordanceItem ( AffordanceItem ) : \n 
self . addProperty ( , 0.15 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . properties . setPropertyIndex ( , 0 ) \n 
d . addSphere ( ( 0 , 0 , 0 ) , self . getProperty ( ) ) \n 
self . setPolyData ( d . getPolyData ( ) ) \n 
~~ ~~ ~~ class CylinderAffordanceItem ( AffordanceItem ) : \n 
self . addProperty ( , 0.03 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . addProperty ( , 0.5 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . properties . setPropertyIndex ( , 0 ) \n 
length = self . getProperty ( ) \n 
d . addCylinder ( center = ( 0 , 0 , 0 ) , axis = ( 0 , 0 , 1 ) , length = self . getProperty ( ) , radius = self . getProperty self . setPolyData ( d . getPolyData ( ) ) \n 
~~ ~~ ~~ class CapsuleAffordanceItem ( AffordanceItem ) : \n 
d . addCapsule ( center = ( 0 , 0 , 0 ) , axis = ( 0 , 0 , 1 ) , length = self . getProperty ( ) , radius = self . getProperty self . setPolyData ( d . getPolyData ( ) ) \n 
~~ ~~ ~~ class CapsuleRingAffordanceItem ( AffordanceItem ) : \n 
self . setProperty ( , False ) \n 
self . addProperty ( , 0.15 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . addProperty ( , 0.02 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep self . addProperty ( , 8 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 1 , minimum \n 
self . properties . setPropertyIndex ( , 2 ) \n 
~~~ radius = self . getProperty ( ) \n 
circlePoints = np . linspace ( 0 , 2 * np . pi , self . getProperty ( ) + 1 ) \n 
spokes = [ ( 0.0 , np . sin ( x ) , np . cos ( x ) ) for x in circlePoints ] \n 
spokes = [ radius * np . array ( x ) / np . linalg . norm ( x ) for x in spokes ] \n 
d = DebugData ( ) \n 
for a , b in zip ( spokes , spokes [ 1 : ] ) : \n 
~~~ d . addCapsule ( center = ( a + b ) / 2.0 , axis = ( b - a ) , length = np . linalg . norm ( b - a ) , radius = self . getProperty ~~ self . setPolyData ( d . getPolyData ( ) ) \n 
if propertyName in ( , , ) : \n 
~~ ~~ ~~ class MeshAffordanceItem ( AffordanceItem ) : \n 
~~~ _meshManager = None \n 
def __init__ ( self , name , view ) : \n 
self . addProperty ( , ) \n 
if self . getProperty ( ) and not self . polyData . GetNumberOfPoints ( ) : \n 
~~ ~~ def updateGeometryFromProperties ( self ) : \n 
~~~ filename = self . getProperty ( ) \n 
if not filename : \n 
~~~ polyData = vtk . vtkPolyData ( ) \n 
~~~ polyData = self . getMeshManager ( ) . get ( filename ) \n 
~~ if not polyData : \n 
~~~ if not os . path . isabs ( filename ) : \n 
~~~ filename = os . path . join ( director . getDRCBaseDir ( ) , filename ) \n 
~~ if os . path . isfile ( filename ) : \n 
~~~ polyData = ioUtils . readPolyData ( filename ) \n 
d . addFrame ( vtk . vtkTransform ( ) , scale = 0.1 , tubeRadius = 0.005 ) \n 
polyData = d . getPolyData ( ) \n 
~~ ~~ self . setPolyData ( polyData ) \n 
def getMeshManager ( cls ) : \n 
~~~ if cls . _meshManager is None : \n 
~~~ cls . _meshManager = meshmanager . MeshManager ( ) \n 
~~ return cls . _meshManager \n 
def promotePolyDataItem ( cls , obj ) : \n 
~~~ parent = obj . parent ( ) \n 
view = obj . views [ 0 ] \n 
name = obj . getProperty ( ) \n 
polyData = obj . polyData \n 
props = obj . properties . _properties \n 
childFrame = obj . getChildFrame ( ) \n 
if childFrame : \n 
~~~ t = transformUtils . copyFrame ( childFrame . transform ) \n 
~~~ t = vtk . vtkTransform ( ) \n 
t . PostMultiply ( ) \n 
t . Translate ( filterUtils . computeCentroid ( polyData ) ) \n 
~~ children = [ c for c in obj . children ( ) if c is not childFrame ] \n 
meshId = cls . getMeshManager ( ) . add ( polyData ) \n 
om . removeFromObjectModel ( obj ) \n 
obj = MeshAffordanceItem ( name , view ) \n 
obj . setProperty ( , meshId ) \n 
om . addToObjectModel ( obj , parentObj = parent ) \n 
frame = vis . addChildFrame ( obj ) \n 
frame . copyFrame ( t ) \n 
for child in children : \n 
~~~ om . addToObjectModel ( child , parentObj = obj ) \n 
~~ obj . syncProperties ( props ) \n 
return obj \n 
~~ ~~ ~~ class FrameAffordanceItem ( AffordanceItem ) : \n 
~~~ def setAffordanceParams ( self , params ) : \n 
~~~ self . params = params \n 
~~ def updateParamsFromActorTransform ( self ) : \n 
~~~ t = self . actor . GetUserTransform ( ) \n 
xaxis = np . array ( t . TransformVector ( [ 1 , 0 , 0 ] ) ) \n 
yaxis = np . array ( t . TransformVector ( [ 0 , 1 , 0 ] ) ) \n 
zaxis = np . array ( t . TransformVector ( [ 0 , 0 , 1 ] ) ) \n 
self . params [ ] = xaxis \n 
self . params [ ] = yaxis \n 
self . params [ ] = zaxis \n 
self . params [ ] = t . GetPosition ( ) \n 
import vtkAll as vtk \n 
import math \n 
import types \n 
import functools \n 
from director import lcmUtils \n 
from director . timercallback import TimerCallback \n 
from director . asynctaskqueue import AsyncTaskQueue \n 
from director . fieldcontainer import FieldContainer \n 
from director import objectmodel as om \n 
from director import applogic as app \n 
from director import ik \n 
from director . ikparameters import IkParameters \n 
from director import ikplanner \n 
from director import affordanceitems \n 
from director . simpletimer import SimpleTimer \n 
from director . utime import getUtime \n 
from director import robotstate \n 
from director import robotplanlistener \n 
from director import segmentation \n 
from director import planplayback \n 
from director . footstepsdriver import FootstepRequestGenerator \n 
from director . tasks . taskuserpanel import TaskUserPanel \n 
from director . tasks . taskuserpanel import ImageBasedAffordanceFit \n 
import director . tasks . robottasks as rt \n 
import director . tasks . taskmanagerwidget as tmw \n 
import drc as lcmdrc \n 
from PythonQt import QtCore , QtGui \n 
class DoorDemo ( object ) : \n 
~~~ def __init__ ( self , robotModel , footstepPlanner , manipPlanner , ikPlanner , lhandDriver , rhandDriver ~~~ self . robotModel = robotModel \n 
self . footstepPlanner = footstepPlanner \n 
self . manipPlanner = manipPlanner \n 
self . ikPlanner = ikPlanner \n 
self . lhandDriver = lhandDriver \n 
self . rhandDriver = rhandDriver \n 
self . atlasDriver = atlasDriver \n 
self . multisenseDriver = multisenseDriver \n 
self . affordanceFitFunction = affordanceFitFunction \n 
self . sensorJointController = sensorJointController \n 
self . planPlaybackFunction = planPlaybackFunction \n 
self . showPoseFunction = showPoseFunction \n 
self . graspingHand = \n 
self . endPose = None \n 
self . planFromCurrentRobotState = True \n 
self . visOnly = False \n 
self . useFootstepPlanner = False \n 
self . userPromptEnabled = True \n 
self . constraintSet = None \n 
self . plans = [ ] \n 
self . usePinchGrasp = False \n 
self . pinchDistance = 0.1 \n 
self . doorHandleFrame = None \n 
self . doorHandleGraspFrame = None \n 
self . doorHingeFrame = None \n 
self . handleTouchHeight = 0.0 \n 
self . handleTouchDepth = - 0.08 \n 
self . handleTouchWidth = 0.06 \n 
self . handleReachAngle = 20 \n 
self . handleTurnHeight = - 0.08 \n 
self . handleTurnWidth = 0.01 \n 
self . handleTurnAngle = 60 \n 
self . handleLiftHeight = 0.12 \n 
self . handlePushDepth = 0.0 \n 
self . handlePushAngle = 2 \n 
self . handleOpenDepth = 0.1 \n 
self . handleOpenWidth = 0.4 \n 
self . speedHigh = 60 \n 
self . speedLow = 15 \n 
self . setFootstepThroughDoorParameters ( ) \n 
self . setChopParametersToDefaults ( ) \n 
~~ def setChopParametersToDefaults ( self ) : \n 
~~~ self . preChopDepth = - 0.06 \n 
self . preChopWidth = - 0.08 \n 
self . preChopHeight = 0.10 \n 
self . chopDistance = - 0.15 \n 
self . chopSidewaysDistance = 0.03 \n 
~~ def addPlan ( self , plan ) : \n 
~~~ self . plans . append ( plan ) \n 
~~ def computeGraspOrientation ( self ) : \n 
~~~ return [ 180 + self . handleReachAngle , 0 , 90 ] \n 
~~ def computeGroundFrame ( self , robotModel ) : \n 
t1 = robotModel . getLinkFrame ( ) \n 
t2 = robotModel . getLinkFrame ( ) \n 
pelvisT = robotModel . getLinkFrame ( ) \n 
xaxis = [ 1.0 , 0.0 , 0.0 ] \n 
pelvisT . TransformVector ( xaxis , xaxis ) \n 
xaxis = np . array ( xaxis ) \n 
zaxis = np . array ( [ 0.0 , 0.0 , 1.0 ] ) \n 
yaxis = np . cross ( zaxis , xaxis ) \n 
yaxis /= np . linalg . norm ( yaxis ) \n 
xaxis = np . cross ( yaxis , zaxis ) \n 
stancePosition = ( np . array ( t2 . GetPosition ( ) ) + np . array ( t1 . GetPosition ( ) ) ) / 2.0 \n 
footHeight = 0.0811 \n 
t = transformUtils . getTransformFromAxes ( xaxis , yaxis , zaxis ) \n 
t . Translate ( stancePosition ) \n 
t . Translate ( [ 0.0 , 0.0 , - footHeight ] ) \n 
return t \n 
~~ def computeDoorHandleGraspFrame ( self ) : \n 
~~~ doorSide = 1 if self . graspingHand == else - 1 \n 
graspOrientation = self . computeGraspOrientation ( ) \n 
self . doorHandleAxisFrame = self . computeDoorHandleAxisFrame ( ) \n 
def makeFrame ( name , offset , turnAngle = 0 ) : \n 
~~~ t = transformUtils . frameFromPositionAndRPY ( offset , graspOrientation ) \n 
t . Concatenate ( transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , [ - turnAngle , 0 , 0 ] t . Concatenate ( transformUtils . copyFrame ( self . doorHandleAxisFrame . transform ) ) \n 
return vis . updateFrame ( t , name , parent = self . doorHandleAffordance , visible = False , scale = 0.2 \n 
~~ def makeFrameNew ( name , transforms ) : \n 
~~~ t = transformUtils . concatenateTransforms ( transforms ) \n 
~~ graspToAxisTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
graspOrientation ) \n 
self . doorHandleGraspFrame = makeFrameNew ( , \n 
[ graspToAxisTransform , \n 
self . doorHandleAxisFrame . transform ] ) \n 
if self . usePinchGrasp : \n 
~~~ reachToGraspTransform = transformUtils . frameFromPositionAndRPY ( [ - doorSide * self . handleTouchWidth self . handleTouchDepth , \n 
- self . handleTouchHeight ] [ 0.0 , 0.0 , 0.0 ] ) \n 
self . doorHandleReachFrame = makeFrameNew ( , [ reachToGraspTransform self . doorHandleGraspFrame \n 
handleTurnTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , [ - self . handleTurnAngle self . doorHandleTurnFrame = makeFrameNew ( , [ reachToGraspTransform graspToAxisTransform , \n 
handleTurnTransform , \n 
self . doorHandleAxisFrame \n 
handlePushTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
[ 0 , 0 , self . handlePushAngle ] self . doorHandlePushFrame = makeFrameNew ( , \n 
[ self . doorHandleTurnFrame . transform , \n 
self . doorHingeFrame . transform . GetInverse ( ) , \n 
handlePushTransform , \n 
self . doorHingeFrame . transform ] ) \n 
self . doorHandlePushLiftFrame = makeFrameNew ( , \n 
[ self . doorHandleReachFrame . transform , \n 
self . doorHandlePushLiftAxisFrame = makeFrameNew ( , \n 
[ self . doorHandleAxisFrame . transform , \n 
self . doorHandlePushOpenFrame = makeFrame ( , [ self . handleOpenDepth \n 
t = vtk . vtkTransform ( ) \n 
t . RotateX ( 25 ) \n 
t . Concatenate ( self . doorHandlePushOpenFrame . transform ) \n 
self . doorHandlePushOpenFrame . copyFrame ( t ) \n 
~~~ reachToAxisTransform = transformUtils . frameFromPositionAndRPY ( [ self . preChopDepth , \n 
doorSide * self . preChopWidth , \n 
self . preChopHeight ] , \n 
[ 0 , 90 , - 90 ] ) \n 
obj = om . findObjectByName ( ) \n 
self . doorHandleReachFrame = makeFrameNew ( , \n 
[ reachToAxisTransform , self . doorHandleAxisFrame \n 
if not obj : \n 
~~~ obj = self . doorHandleReachFrame \n 
obj . setProperty ( , True ) \n 
rep = obj . widget . GetRepresentation ( ) \n 
rep . SetRotateAxisEnabled ( 0 , False ) \n 
rep . SetRotateAxisEnabled ( 1 , False ) \n 
rep . SetRotateAxisEnabled ( 2 , False ) \n 
obj . widget . HandleRotationEnabledOff ( ) \n 
obj . setProperty ( , False ) \n 
~~ preChopToReachTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , \n 
- 0.15 , \n 
0.0 ] , \n 
[ 0 , 0 , 0 ] ) \n 
self . doorHandlePreChopFrame = makeFrameNew ( , \n 
[ preChopToReachTransform , self . doorHandleReachFrame \n 
~~ self . doorHandleFrame . frameSync = vis . FrameSync ( ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleFrame ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleAxisFrame ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleGraspFrame , ignoreIncoming = True ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleReachFrame , ignoreIncoming = True ) \n 
~~~ self . doorHandleFrame . frameSync . addFrame ( self . doorHandleTurnFrame , ignoreIncoming = True ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushFrame , ignoreIncoming = True ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushLiftFrame , ignoreIncoming = True self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushLiftAxisFrame , ignoreIncoming self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushOpenFrame , ignoreIncoming = True ~~ else : \n 
~~~ self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePreChopFrame , ignoreIncoming = True \n 
~~ ~~ def computeDoorHandleAxisFrame ( self ) : \n 
~~~ handleLength = self . doorHandleAffordance . getProperty ( ) [ 1 ] \n 
doorSide = 1 if self . graspingHand == else - 1 \n 
t = transformUtils . frameFromPositionAndRPY ( [ 0.0 , doorSide * handleLength / 2.0 , 0.0 ] , [ 0 , 0 , 0 ] ) t . PostMultiply ( ) \n 
t . Concatenate ( transformUtils . copyFrame ( self . doorHandleFrame . transform ) ) \n 
return vis . updateFrame ( t , , parent = self . doorHandleAffordance , \n 
visible = False , scale = 0.2 ) \n 
~~ def computeDoorHingeFrame ( self ) : \n 
doorAffordance = om . findObjectByName ( ) \n 
doorDimensions = doorAffordance . getProperty ( ) \n 
doorDepth = doorDimensions [ 0 ] \n 
doorWidth = doorDimensions [ 1 ] \n 
t = transformUtils . frameFromPositionAndRPY ( [ doorDepth / 2 , - doorSide * doorWidth / 2.0 , 0.0 ] , [ 0 , t . PostMultiply ( ) \n 
t . Concatenate ( transformUtils . copyFrame ( doorAffordance . getChildFrame ( ) . transform ) ) \n 
self . doorHingeFrame = vis . updateFrame ( t , , parent = doorAffordance , \n 
return self . doorHingeFrame \n 
~~ def computeDoorHandleStanceFrame ( self ) : \n 
~~~ graspFrame = self . doorHandleFrame . transform \n 
groundFrame = self . computeGroundFrame ( self . robotModel ) \n 
groundHeight = groundFrame . GetPosition ( ) [ 2 ] \n 
graspPosition = np . array ( graspFrame . GetPosition ( ) ) \n 
yaxis = [ 0.0 , 1.0 , 0.0 ] \n 
zaxis = [ 0 , 0 , 1 ] \n 
graspFrame . TransformVector ( xaxis , xaxis ) \n 
graspFrame . TransformVector ( yaxis , yaxis ) \n 
graspGroundFrame = transformUtils . getTransformFromAxes ( xaxis , yaxis , zaxis ) \n 
graspGroundFrame . PostMultiply ( ) \n 
graspGroundFrame . Translate ( graspPosition [ 0 ] , graspPosition [ 1 ] , groundHeight ) \n 
position = [ - 0.77 , 0.4 , 0.0 ] \n 
rpy = [ 0 , 0 , - 20 ] \n 
t = transformUtils . frameFromPositionAndRPY ( position , rpy ) \n 
t . Concatenate ( graspGroundFrame ) \n 
self . doorHandleStanceFrame = vis . updateFrame ( t , , parent = self . doorHandleAffordance #self.frameSync.addFrame(self.doorHandleStanceFrame) \n 
~~ def moveRobotToStanceFrame ( self ) : \n 
~~~ frame = self . doorHandleStanceFrame . transform \n 
self . sensorJointController . setPose ( ) \n 
stancePosition = frame . GetPosition ( ) \n 
stanceOrientation = frame . GetOrientation ( ) \n 
self . sensorJointController . q [ : 2 ] = [ stancePosition [ 0 ] , stancePosition [ 1 ] ] \n 
self . sensorJointController . q [ 5 ] = math . radians ( stanceOrientation [ 2 ] ) \n 
self . sensorJointController . push ( ) \n 
~~ def planNominal ( self ) : \n 
~~~ startPose = self . getPlanningStartPose ( ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , ) \n 
endPose , info = self . ikPlanner . computeStandPose ( endPose ) \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose ) \n 
self . addPlan ( newPlan ) \n 
~~ def planPreReach ( self ) : \n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedHigh ) \n 
nonGraspingHand = if self . graspingHand == else \n 
startPose = self . getPlanningStartPose ( ) \n 
~~~ endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , \n 
side = self . graspingHand ) \n 
~~ endPose = self . ikPlanner . getMergedPostureFromDatabase ( endPose , , \n 
side = nonGraspingHand ) \n 
endPose , info = self . ikPlanner . computeStandPose ( endPose , ikParameters = ikParameters ) \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose , ikParameters = ikParameters ) \n 
~~ def planUnReach ( self ) : \n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedLow ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , \n 
~~ def planTuckArms ( self ) : \n 
otherSide = if self . graspingHand == else \n 
standPose , info = self . ikPlanner . computeStandPose ( startPose , ikParameters = ikParameters ) \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side = otherSide a = 0.25 \n 
q2 = ( 1.0 - a ) * np . array ( standPose ) + a * q2 \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( q2 , , , side = self . graspingHand a = 0.75 \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side endPose = self . ikPlanner . getMergedPostureFromDatabase ( endPose , , , side = \n 
newPlan = self . ikPlanner . computeMultiPostureGoal ( [ startPose , q2 , endPose ] , ikParameters = ikParameters self . addPlan ( newPlan ) \n 
~~ def planTuckArmsPrePush ( self ) : \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side = self a = 0.25 \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( q2 , , , side = otherSide ) a = 0.75 \n 
~~ def planChop ( self , deltaZ = None , deltaY = None , deltaX = None ) : \n 
if deltaZ is None : \n 
~~~ deltaZ = self . chopDistance \n 
~~ if deltaY is None : \n 
~~~ deltaY = self . chopSidewaysDistance \n 
~~ if deltaX is None : \n 
~~~ deltaX = 0.0 \n 
~~ linkOffsetFrame = self . ikPlanner . getPalmToHandLink ( self . graspingHand ) \n 
handLinkName = self . ikPlanner . getHandLink ( self . graspingHand ) \n 
startFrame = self . ikPlanner . getLinkFrameAtPose ( handLinkName , startPose ) \n 
endToStartTransform = transformUtils . frameFromPositionAndRPY ( [ deltaZ , - deltaX , - deltaY ] , \n 
endFrame = transformUtils . concatenateTransforms ( [ endToStartTransform , startFrame ] ) ; \n 
vis . updateFrame ( endFrame , , parent = self . doorHandleAffordance , visible = False , scale palmToWorld1 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , startFrame ] ) \n 
palmToWorld2 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , endFrame ] ) \n 
constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , palmToWorld2 constraintSet . nominalPoseName = \n 
constraintSet . ikParameters = IkParameters ( usePointwise = False , \n 
maxDegreesPerSecond = self . speedLow , \n 
numberOfAddedKnots = 2 ) \n 
endPose , info = constraintSet . runIk ( ) \n 
motionVector = np . array ( palmToWorld2 . GetPosition ( ) ) - np . array ( palmToWorld1 . GetPosition ( ) ) \n 
motionTargetFrame = transformUtils . getTransformFromOriginAndNormal ( np . array ( palmToWorld2 . GetPosition \n 
p = self . ikPlanner . createLinePositionConstraint ( handLinkName , linkOffsetFrame , motionTargetFrame constraintSet . constraints . append ( p ) \n 
plan = constraintSet . runIkTraj ( ) \n 
self . addPlan ( plan ) \n 
~~ def stopPushing ( self ) : \n 
~~~ startPose = self . getPlanningStartPose \n 
plan = self . robotSystem . ikPlanner . computePostureGoal ( startPose , startPose ) \n 
self . commitManipPlan ( ) \n 
~~ def planReach ( self , reachTargetFrame = None , jointSpeedLimit = None ) : \n 
~~~ if reachTargetFrame is None : \n 
~~~ reachTargetFrame = self . doorHandleReachFrame \n 
~~ if jointSpeedLimit is None : \n 
~~~ jointSpeedLimit = self . speedLow \n 
~~ startPose = self . getPlanningStartPose ( ) \n 
constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , reachTargetFrame constraintSet . nominalPoseName = \n 
linkOffsetFrame = self . ikPlanner . getPalmToHandLink ( self . graspingHand ) \n 
handToWorld1 = self . ikPlanner . getLinkFrameAtPose ( handLinkName , startPose ) \n 
handToWorld2 = self . ikPlanner . getLinkFrameAtPose ( handLinkName , endPose ) \n 
palmToWorld1 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , handToWorld1 ] ) \n 
palmToWorld2 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , handToWorld2 ] ) \n 
~~ def planPreChop ( self ) : \n 
~~~ self . planReach ( self . doorHandlePreChopFrame , self . speedHigh ) \n 
~~ def createHingeConstraint ( self , referenceFrame , axis , linkName , startPose , tspan = [ 0 , 1 ] ) : \n 
~~~ constraints = [ ] \n 
linkFrame = self . ikPlanner . getLinkFrameAtPose ( linkName , startPose ) \n 
#turnTransform, \n 
#referenceFrame.transform]) \n 
def addPivotPoint ( constraints , pivotPoint ) : \n 
~~~ constraints . append ( ik . PositionConstraint ( ) ) \n 
constraints [ - 1 ] . linkName = linkName \n 
constraints [ - 1 ] . referenceFrame = referenceFrame . transform \n 
constraints [ - 1 ] . lowerBound = np . array ( pivotPoint ) \n 
constraints [ - 1 ] . upperBound = np . array ( pivotPoint ) \n 
pivotPointInWorld = referenceFrame . transform . TransformDoublePoint ( pivotPoint ) \n 
constraints [ - 1 ] . pointInLink = linkFrame . GetInverse ( ) . TransformDoublePoint ( pivotPointInWorld constraints [ - 1 ] . tspan = tspan \n 
~~ addPivotPoint ( constraints , [ 0.0 , 0.0 , 0.0 ] ) \n 
addPivotPoint ( constraints , axis ) \n 
return constraints \n 
~~ def planHandleTurn ( self , turnAngle = None ) : \n 
if turnAngle is None : \n 
~~~ turnAngle = self . handleTurnAngle \n 
linkFrame = self . ikPlanner . getLinkFrameAtPose ( self . ikPlanner . getHandLink ( ) , startPose ) \n 
finalGraspToReferenceTransfrom = transformUtils . concatenateTransforms ( \n 
[ self . ikPlanner . getPalmToHandLink ( self . graspingHand ) , linkFrame , \n 
self . doorHandleAxisFrame . transform . GetInverse ( ) ] ) \n 
handleTurnTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
[ doorSide * turnAngle , 0 , 0 ] ) \n 
doorHandleTurnFrame = transformUtils . concatenateTransforms ( [ finalGraspToReferenceTransfrom , \n 
self . doorHandleAxisFrame . transform \n 
vis . updateFrame ( doorHandleTurnFrame , , parent = self . doorHandleAffordance , visible constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , \n 
doorHandleTurnFrame ) \n 
constraintSet . nominalPoseName = \n 
constraints = constraintSet . constraints \n 
constraints . extend ( self . createHingeConstraint ( self . doorHandleAxisFrame , [ 1.0 , 0.0 , 0.0 ] , \n 
self . ikPlanner . getHandLink ( ) , \n 
constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedBasePostureConstraint ( constraintSet . startPoseName constraints . append ( self . ikPlanner . createLockedBackPostureConstraint ( constraintSet . startPoseName constraints . extend ( self . ikPlanner . createFixedFootConstraints ( constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedArmPostureConstraint ( constraintSet . startPoseName \n 
~~ def planDoorPushOpen ( self ) : \n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = 15 ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , , side = nonGraspingHand \n 
~~ def planDoorPushOpenTwist ( self ) : \n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = 60 ) \n 
~~ def planHandlePush ( self ) : \n 
self . doorHingeFrame . transform . GetInverse ( ) ] ) \n 
[ 0 , 0 , - doorSide * self . handlePushAngle doorHandlePushFrame = transformUtils . concatenateTransforms ( [ finalGraspToReferenceTransfrom , \n 
vis . updateFrame ( doorHandlePushFrame , , parent = self . doorHandleAffordance , visible constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , \n 
doorHandlePushFrame ) \n 
constraintSet . ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedLow constraintSet . nominalPoseName = \n 
constraints . extend ( self . createHingeConstraint ( self . doorHingeFrame , [ 0.0 , 0.0 , 1.0 ] , \n 
self . ikPlanner . getHandLink ( side = self . graspingHand constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedBasePostureConstraint ( constraintSet . startPoseName #constraints.append(self.ikPlanner.createLockedBackPostureConstraint(constraintSet.startPoseName)) constraints . extend ( self . ikPlanner . createFixedFootConstraints ( constraintSet . startPoseName ) ) \n 
~~ def planHandlePushLift ( self ) : \n 
~~~ self . planHandleTurn ( - self . handleTurnAngle ) \n 
#self.addPlan(plan) \n 
~~ def planDoorTouch ( self ) : \n 
~~ def planHandlePushOpen ( self ) : \n 
constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , self . doorHandlePushOpenFrame constraintSet . ikParameters = IkParameters ( usePointwise = False , \n 
~~ def planFootstepsToDoor ( self ) : \n 
goalFrame = self . doorHandleStanceFrame . transform \n 
request = self . footstepPlanner . constructFootstepPlanRequest ( startPose , goalFrame ) \n 
self . footstepPlan = self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True \n 
~~ def planFootstepsThroughDoor ( self ) : \n 
goalFrame = self . doorWalkFrame . transform \n 
request . params . nom_step_width = 0.21 \n 
self . footstepPlan = self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True rt . _addPlanItem ( self . footstepPlan , , rt . FootstepPlanItem ) \n 
~~ def setFootstepThroughDoorParameters ( self ) : \n 
~~~ bias = - 0.02 \n 
self . doorFootstepParams = FieldContainer ( \n 
leadingFoot = , \n 
preEntryFootWidth = - 0.12 + bias , \n 
preEntryFootDistance = - 0.6 , \n 
entryFootWidth = 0.07 + bias , \n 
entryFootDistance = - 0.26 , \n 
exitFootWidth = - 0.08 + bias , \n 
exitFootDistance = 0.12 , \n 
exitStepDistance = 0.3 , \n 
endStanceWidth = 0.26 , \n 
numberOfExitSteps = 1 , \n 
centerStepDistance = 0.26 , \n 
centerStanceWidth = 0.20 , \n 
centerLeadingFoot = \n 
~~ def setTestingFootstepThroughDoorParameters ( self ) : \n 
~~~ self . doorFootstepParams = FieldContainer ( \n 
entryFootWidth = 0.12 , \n 
exitFootWidth = - 0.12 , \n 
preEntryFootDistance = - 0.55 , \n 
preEntryFootWidth = - 0.12 \n 
~~ def getRelativeFootstepsThroughDoorWithSway ( self ) : \n 
~~~ p = self . doorFootstepParams \n 
stepFrames = [ \n 
[ p . preEntryFootDistance , p . preEntryFootWidth , 0.0 ] , \n 
[ p . entryFootDistance , p . entryFootWidth , 0.0 ] , \n 
[ p . exitFootDistance , p . exitFootWidth , 0.0 ] , \n 
for i in xrange ( p . numberOfExitSteps ) : \n 
~~~ sign = - 1 if ( i % 2 ) else 1 \n 
stepFrames . append ( [ p . exitFootDistance + ( i + 1 ) * p . exitStepDistance , sign * p . endStanceWidth / \n 
~~ lastStep = list ( stepFrames [ - 1 ] ) \n 
lastStep [ 1 ] *= - 1 \n 
stepFrames . append ( lastStep ) \n 
return FootstepRequestGenerator . makeStepFrames ( stepFrames , relativeFrame = self . doorGroundFrame \n 
~~ def getRelativeFootstepsThroughDoorCentered ( self ) : \n 
stepDistance = p . centerStepDistance \n 
stanceWidth = p . centerStanceWidth \n 
leadingFoot = p . centerLeadingFoot \n 
stepFrames = [ ] \n 
for i in xrange ( 30 ) : \n 
~~~ sign = - 1 if leadingFoot is else 1 \n 
if i % 2 : \n 
~~~ sign = - sign \n 
~~ stepX = ( i + 1 ) * stepDistance \n 
if stepX > 1.5 : \n 
~~~ stepX = 1.5 \n 
~~ stepFrames . append ( [ stepX , sign * stanceWidth / 2.0 , 0.0 ] ) \n 
if stepX == 1.5 : \n 
~~ ~~ lastStep = list ( stepFrames [ - 1 ] ) \n 
stepFrames [ - 1 ] [ 1 ] = np . sign ( stepFrames [ - 1 ] [ 1 ] ) * ( p . endStanceWidth / 2.0 ) \n 
stepFrames [ - 2 ] [ 1 ] = np . sign ( stepFrames [ - 2 ] [ 1 ] ) * ( p . endStanceWidth / 2.0 ) \n 
return FootstepRequestGenerator . makeStepFrames ( stepFrames , relativeFrame = self . doorHandleStanceFrame \n 
~~ def planManualFootstepsTest ( self , stepDistance = 0.26 , stanceWidth = 0.26 , numberOfSteps = 4 , leadingFoot \n 
~~~ stepFrames = [ ] \n 
for i in xrange ( numberOfSteps ) : \n 
~~ stepFrames . append ( [ ( i + 1 ) * stepDistance , sign * stanceWidth / 2.0 , 0.0 ] ) \n 
stanceFrame = FootstepRequestGenerator . getRobotStanceFrame ( self . robotModel ) \n 
stepFrames = FootstepRequestGenerator . makeStepFrames ( stepFrames , relativeFrame = stanceFrame ) \n 
helper = FootstepRequestGenerator ( self . footstepPlanner ) \n 
request = helper . makeFootstepRequest ( startPose , stepFrames , leadingFoot ) \n 
self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True ) \n 
~~ def planFootstepsThroughDoorManual ( self ) : \n 
stepFrames , leadingFoot = self . getRelativeFootstepsThroughDoorCentered ( ) \n 
request = helper . makeFootstepRequest ( startPose , stepFrames , leadingFoot , numberOfFillSteps = 2 \n 
~~ def computeWalkingPlan ( self ) : \n 
self . walkingPlan = self . footstepPlanner . sendWalkingPlanRequest ( self . footstepPlan , startPose , self . addPlan ( self . walkingPlan ) \n 
~~ def commitManipPlan ( self ) : \n 
~~~ self . manipPlanner . commitManipPlan ( self . plans [ - 1 ] ) \n 
~~ def fitDoor ( self , doorGroundFrame ) : \n 
~~~ om . removeFromObjectModel ( om . findObjectByName ( ) ) \n 
self . spawnDoorAffordance ( ) \n 
affordanceFrame = om . findObjectByName ( ) \n 
assert affordanceFrame is not None \n 
affordanceFrame . copyFrame ( doorGroundFrame ) \n 
om . findObjectByName ( ) . setProperty ( , False ) \n 
~~ def showDoorHandlePoints ( self , polyData ) : \n 
~~~ doorHandle = om . findObjectByName ( ) \n 
door = om . findObjectByName ( ) \n 
doorWidth = door . getProperty ( ) [ 1 ] \n 
doorAxes = transformUtils . getAxesFromTransform ( door . getChildFrame ( ) . transform ) \n 
doorOrigin = np . array ( door . getChildFrame ( ) . transform . GetPosition ( ) ) \n 
handleAxes = transformUtils . getAxesFromTransform ( doorHandle . getChildFrame ( ) . transform ) \n 
handleOrigin = np . array ( doorHandle . getChildFrame ( ) . transform . GetPosition ( ) ) \n 
polyData = segmentation . cropToLineSegment ( polyData , doorOrigin - doorAxes [ 0 ] * 0.02 , doorOrigin polyData = segmentation . cropToLineSegment ( polyData , doorOrigin , doorOrigin + doorAxes [ 1 ] * ( doorWidth polyData = segmentation . cropToLineSegment ( polyData , handleOrigin - handleAxes [ 2 ] * 0.1 , handleOrigin \n 
pointsName = \n 
existed = om . findObjectByName ( pointsName ) is not None \n 
obj = vis . updatePolyData ( polyData , pointsName , parent = doorHandle , color = [ 1 , 0 , 0 ] ) \n 
if not existed : \n 
~~~ obj . setProperty ( , 10 ) \n 
~~ ~~ def spawnDoorAffordance ( self ) : \n 
~~~ groundFrame = self . computeGroundFrame ( self . robotModel ) \n 
doorOffsetX = 0.7 \n 
doorOffsetY = 0.0 \n 
doorGroundFrame = transformUtils . frameFromPositionAndRPY ( [ doorOffsetX , 0.0 , 0.0 ] , [ 0.0 , 0.0 , doorGroundFrame . PostMultiply ( ) \n 
doorGroundFrame . Concatenate ( groundFrame ) \n 
stanceFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , [ 0.0 , 0.0 , 0.0 ] ) \n 
stanceFrame . PostMultiply ( ) \n 
stanceFrame . Concatenate ( groundFrame ) \n 
doorWalkFrame = transformUtils . frameFromPositionAndRPY ( [ doorOffsetX + 0.6 , 0.0 , 0.0 ] , [ 0.0 , doorWalkFrame . PostMultiply ( ) \n 
doorWalkFrame . Concatenate ( groundFrame ) \n 
doorWidth = 36 * 0.0254 \n 
doorHeight = 81 * 0.0254 \n 
doorDepth = 0.5 * 0.0254 \n 
handleHeightFromGround = 35 * 0.0254 \n 
handleDistanceFromEdge = 1.625 * 0.0254 \n 
handleDistanceFromDoor = 1.75 * 0.0254 \n 
handleLength = 4.125 * 0.0254 \n 
handleDepth = 0.25 * 0.0254 \n 
doorJamWidth = 0.5 \n 
doorJamDepth = 4.5 * 0.0254 \n 
handleFrame = transformUtils . frameFromPositionAndRPY ( [ - handleDistanceFromDoor - doorDepth / 2.0 handleFrame . PostMultiply ( ) \n 
handleFrame . Concatenate ( doorGroundFrame ) \n 
doorFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , doorHeight / 2.0 ] , [ 0.0 , 0.0 , 0.0 doorFrame . PostMultiply ( ) \n 
doorFrame . Concatenate ( doorGroundFrame ) \n 
leftDoorJamFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , ( doorWidth / 2.0 + doorJamWidth leftDoorJamFrame . PostMultiply ( ) \n 
leftDoorJamFrame . Concatenate ( doorGroundFrame ) \n 
rightDoorJamFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , - ( doorWidth / 2.0 + doorJamWidth rightDoorJamFrame . PostMultiply ( ) \n 
rightDoorJamFrame . Concatenate ( doorGroundFrame ) \n 
desc = dict ( classname = , Name = , \n 
pose = transformUtils . poseFromTransform ( handleFrame ) , Dimensions = [ handleDepth , handleLength handleAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
pose = transformUtils . poseFromTransform ( doorFrame ) , Dimensions = [ doorDepth , doorWidth , doorHeight doorAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
pose = transformUtils . poseFromTransform ( leftDoorJamFrame ) , Dimensions = [ doorJamDepth , doorJamWidth leftDoorJamAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
pose = transformUtils . poseFromTransform ( rightDoorJamFrame ) , Dimensions = [ doorJamDepth , doorJamWidth rightDoorJamAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
doorGroundFrame = vis . showFrame ( doorGroundFrame , , parent = doorAffordance ) stanceFrame = vis . showFrame ( stanceFrame , , parent = doorAffordance ) \n 
doorWalkFrame = vis . showFrame ( doorWalkFrame , , visible = False , parent = doorAffordance \n 
doorFrame = doorAffordance . getChildFrame ( ) \n 
handleFrame = handleAffordance . getChildFrame ( ) \n 
leftDoorJamFrame = leftDoorJamAffordance . getChildFrame ( ) \n 
rightDoorJamFrame = rightDoorJamAffordance . getChildFrame ( ) \n 
self . doorFrameSync = vis . FrameSync ( ) \n 
self . doorFrameSync . addFrame ( doorGroundFrame ) \n 
self . doorFrameSync . addFrame ( stanceFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( doorWalkFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( doorFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( leftDoorJamFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( rightDoorJamFrame , ignoreIncoming = True ) \n 
self . doorHandleFrameSync = vis . FrameSync ( ) \n 
self . doorHandleFrameSync . addFrame ( doorFrame ) \n 
self . doorHandleFrameSync . addFrame ( handleFrame , ignoreIncoming = True ) \n 
self . findDoorHandleAffordance ( ) \n 
self . doorGroundFrame = doorGroundFrame \n 
self . doorHandleStanceFrame = stanceFrame \n 
self . doorWalkFrame = doorWalkFrame \n 
~~ def findDoorHandleAffordance ( self ) : \n 
~~~ self . doorHandleAffordance = om . findObjectByName ( ) \n 
self . doorHandleFrame = self . doorHandleAffordance . getChildFrame ( ) \n 
self . computeDoorHingeFrame ( ) \n 
self . computeDoorHandleGraspFrame ( ) \n 
#self.computeDoorHandleStanceFrame() \n 
~~ def getEstimatedRobotStatePose ( self ) : \n 
~~~ return np . array ( self . sensorJointController . getPose ( ) ) \n 
~~ def getPlanningStartPose ( self ) : \n 
~~~ if self . planFromCurrentRobotState : \n 
~~~ return self . getEstimatedRobotStatePose ( ) \n 
~~~ if self . plans : \n 
~~~ return robotstate . convertStateMessageToDrakePose ( self . plans [ - 1 ] . plan [ - 1 ] ) \n 
~~ ~~ ~~ ~~ class DoorImageFitter ( ImageBasedAffordanceFit ) : \n 
~~~ def __init__ ( self , doorDemo ) : \n 
~~~ ImageBasedAffordanceFit . __init__ ( self , numberOfPoints = 1 ) \n 
self . doorDemo = doorDemo \n 
~~ def fit ( self , polyData , points ) : \n 
~~~ stanceFrame = FootstepRequestGenerator . getRobotStanceFrame ( self . doorDemo . robotModel ) \n 
doorGroundFrame = segmentation . segmentDoorPlane ( polyData , points [ 0 ] , stanceFrame ) \n 
self . doorDemo . fitDoor ( doorGroundFrame ) \n 
self . doorDemo . showDoorHandlePoints ( polyData ) \n 
~~ ~~ class DoorTaskPanel ( TaskUserPanel ) : \n 
~~~ TaskUserPanel . __init__ ( self , windowTitle = ) \n 
self . fitter = DoorImageFitter ( self . doorDemo ) \n 
self . initImageView ( self . fitter . imageView ) \n 
self . addDefaultProperties ( ) \n 
self . addButtons ( ) \n 
self . addTasks ( ) \n 
~~ def addButtons ( self ) : \n 
~~~ self . addManualButton ( , self . doorDemo . spawnDoorAffordance ) \n 
self . addManualSpacer ( ) \n 
self . addManualButton ( , self . doorDemo . planFootstepsToDoor ) \n 
self . addManualButton ( , self . doorDemo . planFootstepsThroughDoor ) \n 
self . addManualButton ( , self . doorDemo . planPreReach ) \n 
self . addManualButton ( , self . doorDemo . planTuckArmsPrePush ) \n 
self . addManualButton ( , self . doorDemo . planTuckArms ) \n 
self . addManualButton ( , self . openPinch ) \n 
self . addManualButton ( , self . closePinch ) \n 
self . addManualButton ( , self . doorDemo . planReach ) \n 
self . addManualButton ( , self . doorDemo . planUnReach ) \n 
self . addManualButton ( , self . doorDemo . planPreChop ) \n 
self . addManualButton ( , self . doorDemo . planChop ) \n 
self . addManualButton ( , functools . partial ( self . doorDemo . planChop , deltaX = - 0.1 , deltaY self . addManualSpacer ( ) \n 
self . addManualButton ( , functools . partial ( self . doorDemo . planHandleTurn , 10 ) ) \n 
self . addManualButton ( , functools . partial ( self . doorDemo . planHandleTurn , - 10 ) ) \n 
self . addManualButton ( , self . doorDemo . planDoorPushOpenTwist ) \n 
self . addManualButton ( , self . doorDemo . commitManipPlan ) \n 
self . addManualButton ( , self . doorDemo . stopPushing ) \n 
~~ def getSide ( self ) : \n 
~~~ return self . params . getPropertyEnumValue ( ) . lower ( ) \n 
~~ def openPinch ( self ) : \n 
~~~ rt . OpenHand ( side = self . getSide ( ) . capitalize ( ) , mode = ) . run ( ) \n 
~~ def closePinch ( self ) : \n 
~~~ rt . CloseHand ( side = self . getSide ( ) . capitalize ( ) , mode = ) . run ( ) \n 
~~ def addDefaultProperties ( self ) : \n 
~~~ self . params . addProperty ( , 0 , attributes = om . PropertyAttributes ( enumNames = [ , self . params . addProperty ( , self . doorDemo . preChopWidth , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . preChopDepth , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . preChopHeight , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . chopDistance , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . chopSidewaysDistance , attributes self . _syncProperties ( ) \n 
~~ def onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ if propertyName == : \n 
~~~ self . taskTree . removeAllTasks ( ) \n 
~~ self . doorDemo . findDoorHandleAffordance ( ) \n 
self . _syncProperties ( ) \n 
~~ def _syncProperties ( self ) : \n 
~~~ self . doorDemo . graspingHand = self . params . getPropertyEnumValue ( ) . lower ( ) \n 
self . doorDemo . ikPlanner . reachingSide = self . doorDemo . graspingHand \n 
if hasattr ( self . doorDemo , ) : \n 
~~~ self . doorDemo . computeDoorHandleGraspFrame ( ) \n 
~~ self . doorDemo . chopDistance = self . params . getProperty ( ) \n 
self . doorDemo . chopSidewaysDistance = self . params . getProperty ( ) \n 
self . doorDemo . preChopWidth = self . params . getProperty ( ) \n 
self . doorDemo . preChopDepth = self . params . getProperty ( ) \n 
self . doorDemo . preChopHeight = self . params . getProperty ( ) \n 
~~ def addTasks ( self ) : \n 
~~~ self . folder = None \n 
def addTask ( task , parent = None ) : \n 
~~~ parent = parent or self . folder \n 
self . taskTree . onAddTask ( task , copy = False , parent = parent ) \n 
~~ def addFunc ( func , name , parent = None ) : \n 
~~~ addTask ( rt . CallbackTask ( callback = func , name = name ) , parent = parent ) \n 
~~ def addFolder ( name , parent = None ) : \n 
~~~ self . folder = self . taskTree . addGroup ( name , parent = parent ) \n 
return self . folder \n 
~~ d = self . doorDemo \n 
self . taskTree . removeAllTasks ( ) \n 
side = self . params . getPropertyEnumValue ( ) \n 
############### \n 
folder = addFolder ( ) \n 
addTask ( rt . CloseHand ( name = , side = ) ) \n 
addTask ( rt . UserPromptTask ( name = , message = addTask ( rt . FindAffordance ( name = , affordanceName = ) ) \n 
addTask ( rt . SetNeckPitch ( name = , angle = 35 ) ) \n 
addTask ( rt . RequestFootstepPlan ( name = , stanceFrameName = addTask ( rt . UserPromptTask ( name = , message = ) addTask ( rt . CommitFootstepPlan ( name = , planName = addTask ( rt . WaitForWalkExecution ( name = ) ) \n 
addTask ( rt . UserPromptTask ( name = , message = \n 
addTask ( rt . OpenHand ( name = , side = side , mode = ) ) \n 
def addManipTask ( name , planFunc , userPrompt = False ) : \n 
~~~ folder = addFolder ( name ) \n 
addFunc ( planFunc , name = ) \n 
if not userPrompt : \n 
~~~ addTask ( rt . CheckPlanInfo ( name = ) ) \n 
~~~ addTask ( rt . UserPromptTask ( name = , message = ~~ addFunc ( d . commitManipPlan , name = ) \n 
addTask ( rt . WaitForManipulationPlanExecution ( name = ) ) \n 
~~ addManipTask ( , d . planPreReach , userPrompt = False ) \n 
addManipTask ( , d . planDoorTouch , userPrompt = False ) \n 
if d . usePinchGrasp : \n 
~~~ addManipTask ( , d . planReach , userPrompt = True ) \n 
addFunc ( self . closePinch , name = ) \n 
addTask ( rt . UserPromptTask ( name = , \n 
message = ) ) \n 
addManipTask ( , d . planHandleTurn , userPrompt = False ) \n 
~~~ addFunc ( self . doorDemo . setChopParametersToDefaults , name = ) \n 
addManipTask ( , d . planReach , userPrompt = True ) \n 
addManipTask ( , d . planChop , userPrompt = True ) \n 
~~ addManipTask ( , d . planHandlePush , userPrompt = False ) \n 
addManipTask ( , d . planHandlePush , userPrompt = False ) \n 
~~~ addManipTask ( , d . planHandlePushLift , userPrompt = False ) \n 
addTask ( rt . CloseHand ( name = , side = side , mode = , amount = 0 ) ) \n 
~~ addManipTask ( , d . planDoorPushOpen , userPrompt = False ) \n 
addTask ( rt . CloseHand ( name = , side = side ) ) \n 
addManipTask ( , d . planTuckArms , userPrompt = False ) \n 
addFunc ( d . planFootstepsThroughDoorManual , name = ) \n 
addTask ( rt . UserPromptTask ( name = , message = ) addTask ( rt . CommitFootstepPlan ( name = , planName = ) addTask ( rt . WaitForWalkExecution ( name = ) ) \n 
addTask ( rt . PlanPostureGoal ( name = , postureGroup = , postureName = addTask ( rt . UserPromptTask ( name = , message = ) ) \n 
addTask ( rt . CommitManipulationPlan ( name = , planName = addTask ( rt . WaitForManipulationPlanExecution ( name = ) ) \n 
~~ ~~ import PythonQt \n 
from PythonQt import QtCore , QtGui , QtUiTools \n 
from time import time \n 
from copy import deepcopy \n 
def addWidgetsToDict ( widgets , d ) : \n 
~~~ for widget in widgets : \n 
~~~ if widget . objectName : \n 
~~~ d [ str ( widget . objectName ) ] = widget \n 
~~ addWidgetsToDict ( widget . children ( ) , d ) \n 
~~ ~~ class WidgetDict ( object ) : \n 
~~~ def __init__ ( self , widgets ) : \n 
~~~ addWidgetsToDict ( widgets , self . __dict__ ) \n 
~~ ~~ class SpindleSpinChecker ( object ) : \n 
~~~ def __init__ ( self , spindleMonitor ) : \n 
~~~ self . spindleMonitor = spindleMonitor \n 
self . timer = TimerCallback ( targetFps = 3 ) \n 
self . timer . callback = self . update \n 
self . warningButton = None \n 
self . action = None \n 
~~ def update ( self ) : \n 
~~~ if abs ( self . spindleMonitor . getAverageSpindleVelocity ( ) ) < 0.2 : \n 
~~~ self . notifyUserStatusBar ( ) \n 
~~~ self . clearStatusBarWarning ( ) \n 
~~ ~~ def start ( self ) : \n 
~~~ self . action . checked = True \n 
self . timer . start ( ) \n 
~~ def stop ( self ) : \n 
~~~ self . action . checked = False \n 
self . timer . stop ( ) \n 
~~ def setupMenuAction ( self ) : \n 
~~~ self . action = app . addMenuAction ( , ) \n 
self . action . setCheckable ( True ) \n 
self . action . checked = self . timer . isActive ( ) \n 
self . action . connect ( , self . onActionChanged ) \n 
~~ def onActionChanged ( self ) : \n 
~~~ if self . action . checked : \n 
~~~ self . start ( ) \n 
~~~ self . stop ( ) \n 
~~ ~~ def clearStatusBarWarning ( self ) : \n 
~~~ if self . warningButton : \n 
~~~ self . warningButton . deleteLater ( ) \n 
~~ ~~ def notifyUserStatusBar ( self ) : \n 
~~ self . warningButton = QtGui . QPushButton ( ) \n 
self . warningButton . setStyleSheet ( "background-color:red" ) \n 
app . getMainWindow ( ) . statusBar ( ) . insertPermanentWidget ( 0 , self . warningButton ) \n 
~~ ~~ class MultisensePanel ( object ) : \n 
~~~ def __init__ ( self , multisenseDriver ) : \n 
~~~ self . multisenseDriver = multisenseDriver \n 
self . multisenseChanged = False \n 
loader = QtUiTools . QUiLoader ( ) \n 
uifile = QtCore . QFile ( ) \n 
assert uifile . open ( uifile . ReadOnly ) \n 
self . widget = loader . load ( uifile ) \n 
self . ui = WidgetDict ( self . widget . children ( ) ) \n 
self . updateTimer = TimerCallback ( targetFps = 2 ) \n 
self . updateTimer . callback = self . updatePanel \n 
self . updateTimer . start ( ) \n 
self . widget . headCamGainSpinner . setEnabled ( False ) \n 
self . widget . headCamExposureSpinner . setEnabled ( False ) \n 
self . widget . spinRateSpinner . valueChanged . connect ( self . spinRateChange ) \n 
self . widget . scanDurationSpinner . valueChanged . connect ( self . scanDurationChange ) \n 
self . widget . headCamFpsSpinner . valueChanged . connect ( self . headCamFpsChange ) \n 
self . widget . headCamGainSpinner . valueChanged . connect ( self . headCamGainChange ) \n 
self . widget . headCamExposureSpinner . valueChanged . connect ( self . headCamExposureChange ) \n 
self . widget . headAutoGainCheck . clicked . connect ( self . headCamAutoGainChange ) \n 
self . widget . ledOnCheck . clicked . connect ( self . ledOnCheckChange ) \n 
self . widget . ledBrightnessSpinner . valueChanged . connect ( self . ledBrightnessChange ) \n 
self . widget . sendButton . clicked . connect ( self . sendButtonClicked ) \n 
self . updatePanel ( ) \n 
~~ def getCameraFps ( self ) : \n 
~~~ return self . widget . headCamFpsSpinner . value \n 
~~ def getCameraGain ( self ) : \n 
~~~ return self . widget . headCamGainSpinner . value \n 
~~ def getCameraExposure ( self ) : \n 
~~~ return self . widget . headCamExposureSpinner . value \n 
~~ def getCameraLedOn ( self ) : \n 
~~~ return self . widget . ledOnCheck . isChecked ( ) \n 
~~ def getCameraLedBrightness ( self ) : \n 
~~~ return self . widget . ledBrightnessSpinner . value \n 
~~ def getCameraAutoGain ( self ) : \n 
~~~ return self . widget . headAutoGainCheck . isChecked ( ) \n 
~~ def getSpinRate ( self ) : \n 
~~~ return self . widget . spinRateSpinner . value \n 
~~ def getScanDuration ( self ) : \n 
~~~ return self . widget . scanDurationSpinner . value \n 
~~ def ledBrightnessChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
~~ def ledOnCheckChange ( self , event ) : \n 
~~ def headCamExposureChange ( self , event ) : \n 
~~ def headCamAutoGainChange ( self , event ) : \n 
self . widget . headCamGainSpinner . setEnabled ( not self . getCameraAutoGain ( ) ) \n 
self . widget . headCamExposureSpinner . setEnabled ( not self . getCameraAutoGain ( ) ) \n 
~~ def headCamFpsChange ( self , event ) : \n 
~~ def headCamGainChange ( self , event ) : \n 
~~ def spinRateChange ( self , event ) : \n 
spinRate = self . getSpinRate ( ) \n 
if spinRate == 0.0 : \n 
~~~ scanDuration = 240.0 \n 
~~~ scanDuration = abs ( 60.0 / ( spinRate * 2 ) ) \n 
~~ if scanDuration > 240.0 : \n 
~~ self . widget . scanDurationSpinner . blockSignals ( True ) \n 
self . widget . scanDurationSpinner . value = scanDuration \n 
self . widget . scanDurationSpinner . blockSignals ( False ) \n 
~~ def scanDurationChange ( self , event ) : \n 
scanDuration = self . getScanDuration ( ) \n 
spinRate = abs ( 60.0 / ( scanDuration * 2 ) ) \n 
self . widget . spinRateSpinner . blockSignals ( True ) \n 
self . widget . spinRateSpinner . value = spinRate \n 
self . widget . spinRateSpinner . blockSignals ( False ) \n 
~~ def sendButtonClicked ( self , event ) : \n 
~~~ self . publishCommand ( ) \n 
~~ def updatePanel ( self ) : \n 
~~~ if not self . widget . isVisible ( ) : \n 
~~ ~~ def publishCommand ( self ) : \n 
~~~ fps = self . getCameraFps ( ) \n 
camGain = self . getCameraGain ( ) \n 
exposure = 1000 * self . getCameraExposure ( ) \n 
ledFlash = self . getCameraLedOn ( ) \n 
ledDuty = self . getCameraLedBrightness ( ) \n 
autoGain = 1 if self . getCameraAutoGain ( ) else 0 \n 
self . multisenseDriver . sendMultisenseCommand ( fps , camGain , exposure , autoGain , spinRate , ledFlash \n 
~~ ~~ def _getAction ( ) : \n 
~~~ return app . getToolBarActions ( ) [ ] \n 
~~ def init ( driver ) : \n 
~~~ global panel \n 
global dock \n 
panel = MultisensePanel ( driver ) \n 
dock = app . addWidgetToDock ( panel . widget , action = _getAction ( ) ) \n 
dock . hide ( ) \n 
return panel \n 
~~ def deepCopy ( dataOb ) : \n 
~~~ newData = dataObject . NewInstance ( ) \n 
newData . DeepCopy ( dataObj ) \n 
return newData \n 
~~ def shallowCopy ( dataObj ) : \n 
~~~ newData = dataObj . NewInstance ( ) \n 
newData . ShallowCopy ( dataObj ) \n 
#!/usr/bin/python \n 
~~ from __future__ import division \n 
from numpy import * \n 
def minBoundingRect ( hull_points_2d ) : \n 
for i in range ( len ( edges ) ) : \n 
~~~ edge_x = hull_points_2d [ i + 1 , 0 ] - hull_points_2d [ i , 0 ] \n 
edge_y = hull_points_2d [ i + 1 , 1 ] - hull_points_2d [ i , 1 ] \n 
edges [ i ] = [ edge_x , edge_y ] \n 
for i in range ( len ( edge_angles ) ) : \n 
~~~ edge_angles [ i ] = math . atan2 ( edges [ i , 1 ] , edges [ i , 0 ] ) \n 
~~ for i in range ( len ( edge_angles ) ) : \n 
~~ edge_angles = unique ( edge_angles ) \n 
min_x = nanmin ( rot_points [ 0 ] , axis = 0 ) \n 
max_x = nanmax ( rot_points [ 0 ] , axis = 0 ) \n 
min_y = nanmin ( rot_points [ 1 ] , axis = 0 ) \n 
max_y = nanmax ( rot_points [ 1 ] , axis = 0 ) \n 
width = max_x - min_x \n 
height = max_y - min_y \n 
area = width * height \n 
~~~ min_bbox = ( edge_angles [ i ] , area , width , height , min_x , max_x , min_y , max_y ) \n 
~~ ~~ angle = min_bbox [ 0 ] \n 
min_x = min_bbox [ 4 ] \n 
max_x = min_bbox [ 5 ] \n 
min_y = min_bbox [ 6 ] \n 
max_y = min_bbox [ 7 ] \n 
center_x = ( min_x + max_x ) / 2 \n 
center_y = ( min_y + max_y ) / 2 \n 
center_point = dot ( [ center_x , center_y ] , R ) \n 
corner_points [ 0 ] = dot ( [ max_x , min_y ] , R ) \n 
corner_points [ 1 ] = dot ( [ min_x , min_y ] , R ) \n 
corner_points [ 2 ] = dot ( [ min_x , max_y ] , R ) \n 
corner_points [ 3 ] = dot ( [ max_x , max_y ] , R ) \n 
~~ from director import atlasdriver \n 
from director import consoleapp \n 
atlasDriver = atlasdriver . init ( ) \n 
w = QtGui . QWidget ( ) \n 
l = QtGui . QVBoxLayout ( w ) \n 
Button = namedtuple ( , [ , , ] ) ; \n 
buttons = [ \n 
Button ( , atlasDriver . sendRecoveryTriggerOn , None ) \n 
for button in buttons : \n 
~~~ qb = QtGui . QPushButton ( button . name ) \n 
qb . connect ( , button . callback ) \n 
qb . setSizePolicy ( QtGui . QSizePolicy . Expanding , QtGui . QSizePolicy . Expanding ) \n 
s = qb . styleSheet \n 
if button . color : \n 
~~ qb . setStyleSheet ( s ) \n 
l . addWidget ( qb ) \n 
~~ w . setWindowTitle ( ) \n 
w . show ( ) \n 
w . resize ( 500 , 600 ) \n 
consoleapp . ConsoleApp . start ( ) \n 
from director . consoleapp import ConsoleApp \n 
from director import roboturdf \n 
from director import jointcontrol \n 
def getArgs ( ) : \n 
parser . add_argument ( , type = str , default = None , help = ) \n 
args , unknown = parser . parse_known_args ( ) \n 
return args \n 
~~ app = ConsoleApp ( ) \n 
view = app . createView ( ) \n 
args = getArgs ( ) \n 
if args . urdf : \n 
~~~ robotModel = roboturdf . openUrdf ( args . urdf , view ) \n 
jointNames = robotModel . model . getJointNames ( ) \n 
jointController = jointcontrol . JointController ( [ robotModel ] , jointNames = jointNames ) \n 
~~~ robotModel , jointController = roboturdf . loadRobotModel ( , view ) \n 
~~ print , robotModel . getProperty ( ) \n 
for joint in robotModel . model . getJointNames ( ) : \n 
~~~ print , joint \n 
~~ for link in robotModel . model . getLinkNames ( ) : \n 
~~~ print , link \n 
robotModel . getLinkFrame ( link ) \n 
~~ if app . getTestingInteractiveEnabled ( ) : \n 
~~~ view . show ( ) \n 
app . start ( ) \n 
~~ from distutils . core import setup \n 
from catkin_pkg . python_setup import generate_distutils_setup \n 
d = generate_distutils_setup ( \n 
packages = [ , , , package_dir = { : } , \n 
setup ( ** d ) \n 
from random import randint \n 
from rosbridge_library . util import json \n 
send_fragment_size = 1000 \n 
receive_fragment_size = 10 \n 
receive_message_intervall = 0.0 \n 
def calculate_service_response ( request ) : \n 
message = { "data" : { "data" : 42.0 } } \n 
response_object = { "op" : "service_response" , \n 
"id" : request_object [ "id" ] , \n 
response_message = json . dumps ( response_object ) \n 
return response_message \n 
~~ buffer = "" \n 
def connect_tcp_socket ( ) : \n 
tcp_sock . connect ( ( rosbridge_ip , rosbridge_port ) ) \n 
return tcp_sock \n 
~~~ advertise_message_object = { "op" : "advertise_service" , \n 
"type" : service_type , \n 
"service" : service_name , \n 
"fragment_size" : receive_fragment_size , \n 
"message_intervall" : receive_message_intervall \n 
advertise_message = json . dumps ( advertise_message_object ) \n 
tcp_socket . send ( str ( advertise_message ) ) \n 
"service" : service_name \n 
unadvertise_message = json . dumps ( unadvertise_message_object ) \n 
tcp_socket . send ( str ( unadvertise_message ) ) \n 
global buffer \n 
~~~ done = False \n 
while not done : \n 
if data_object [ "op" ] == "call_service" : \n 
~~~ data = buffer \n 
done = True \n 
for fragment in result_string : \n 
~~~ if fragment [ 0 ] != "{" : \n 
~~~ fragment = "{" + fragment \n 
~~ if fragment [ len ( fragment ) - 1 ] != "}" : \n 
~~~ fragment = fragment + "}" \n 
~~ result . append ( json . loads ( fragment ) ) \n 
announced = int ( result [ 0 ] [ "total" ] ) \n 
if fragment_count == announced : \n 
~~~ reconstructed = "" \n 
unsorted_result = [ ] \n 
for fragment in result : \n 
~~~ unsorted_result . append ( fragment ) \n 
sorted_result [ int ( fragment [ "num" ] ) ] = fragment \n 
return reconstructed \n 
~~ ~~ except Exception , e : \n 
print e \n 
~~~ print "defrag_error:" , buffer \n 
~~ ~~ ~~ except Exception , e : \n 
~~ return data \n 
while cursor < len ( full_message ) : \n 
~~~ fragment_begin = cursor \n 
if len ( full_message ) < cursor + fragment_size : \n 
~~~ fragment_end = len ( full_message ) \n 
cursor = len ( full_message ) \n 
~~~ fragment_end = cursor + fragment_size \n 
cursor += fragment_size \n 
~~ fragment = full_message [ fragment_begin : fragment_end ] \n 
fragments . append ( fragment ) \n 
"data" : str ( fragment ) , \n 
"num" : count , \n 
"total" : len ( fragments ) \n 
~~ return fragmented_messages_list \n 
for fragment in fragment_list : \n 
from __future__ import absolute_import , division , print_function , with_statement \n 
import collections \n 
import pycurl \n 
from tornado import httputil \n 
from tornado import ioloop \n 
from tornado . log import gen_log \n 
from tornado import stack_context \n 
from tornado . escape import utf8 , native_str \n 
from tornado . httpclient import HTTPResponse , HTTPError , AsyncHTTPClient , main \n 
from tornado . util import bytes_type \n 
~~ class CurlAsyncHTTPClient ( AsyncHTTPClient ) : \n 
~~~ def initialize ( self , io_loop , max_clients = 10 , defaults = None ) : \n 
~~~ super ( CurlAsyncHTTPClient , self ) . initialize ( io_loop , defaults = defaults ) \n 
self . _multi = pycurl . CurlMulti ( ) \n 
self . _multi . setopt ( pycurl . M_TIMERFUNCTION , self . _set_timeout ) \n 
self . _multi . setopt ( pycurl . M_SOCKETFUNCTION , self . _handle_socket ) \n 
self . _curls = [ _curl_create ( ) for i in range ( max_clients ) ] \n 
self . _free_list = self . _curls [ : ] \n 
self . _requests = collections . deque ( ) \n 
self . _fds = { } \n 
self . _timeout = None \n 
self . _force_timeout_callback = ioloop . PeriodicCallback ( \n 
self . _handle_force_timeout , 1000 , io_loop = io_loop ) \n 
self . _force_timeout_callback . start ( ) \n 
dummy_curl_handle = pycurl . Curl ( ) \n 
self . _multi . add_handle ( dummy_curl_handle ) \n 
self . _multi . remove_handle ( dummy_curl_handle ) \n 
~~~ self . _force_timeout_callback . stop ( ) \n 
if self . _timeout is not None : \n 
~~~ self . io_loop . remove_timeout ( self . _timeout ) \n 
~~ for curl in self . _curls : \n 
~~~ curl . close ( ) \n 
~~ self . _multi . close ( ) \n 
super ( CurlAsyncHTTPClient , self ) . close ( ) \n 
~~ def fetch_impl ( self , request , callback ) : \n 
~~~ self . _requests . append ( ( request , callback ) ) \n 
self . _process_queue ( ) \n 
self . _set_timeout ( 0 ) \n 
~~ def _handle_socket ( self , event , fd , multi , data ) : \n 
event_map = { \n 
pycurl . POLL_NONE : ioloop . IOLoop . NONE , \n 
pycurl . POLL_IN : ioloop . IOLoop . READ , \n 
pycurl . POLL_OUT : ioloop . IOLoop . WRITE , \n 
pycurl . POLL_INOUT : ioloop . IOLoop . READ | ioloop . IOLoop . WRITE \n 
if event == pycurl . POLL_REMOVE : \n 
~~~ if fd in self . _fds : \n 
~~~ self . io_loop . remove_handler ( fd ) \n 
del self . _fds [ fd ] \n 
~~~ ioloop_event = event_map [ event ] \n 
if fd in self . _fds : \n 
~~ self . io_loop . add_handler ( fd , self . _handle_events , \n 
ioloop_event ) \n 
self . _fds [ fd ] = ioloop_event \n 
~~ ~~ def _set_timeout ( self , msecs ) : \n 
~~ self . _timeout = self . io_loop . add_timeout ( \n 
self . io_loop . time ( ) + msecs / 1000.0 , self . _handle_timeout ) \n 
~~ def _handle_events ( self , fd , events ) : \n 
action = 0 \n 
if events & ioloop . IOLoop . READ : \n 
~~~ action |= pycurl . CSELECT_IN \n 
~~ if events & ioloop . IOLoop . WRITE : \n 
~~~ action |= pycurl . CSELECT_OUT \n 
~~~ ret , num_handles = self . _multi . socket_action ( fd , action ) \n 
~~ except pycurl . error as e : \n 
~~~ ret = e . args [ 0 ] \n 
~~ if ret != pycurl . E_CALL_MULTI_PERFORM : \n 
~~ ~~ self . _finish_pending_requests ( ) \n 
~~ def _handle_timeout ( self ) : \n 
with stack_context . NullContext ( ) : \n 
~~~ self . _timeout = None \n 
~~~ ret , num_handles = self . _multi . socket_action ( \n 
pycurl . SOCKET_TIMEOUT , 0 ) \n 
~~ new_timeout = self . _multi . timeout ( ) \n 
if new_timeout >= 0 : \n 
~~~ self . _set_timeout ( new_timeout ) \n 
~~ ~~ def _handle_force_timeout ( self ) : \n 
~~~ ret , num_handles = self . _multi . socket_all ( ) \n 
~~ ~~ def _finish_pending_requests ( self ) : \n 
~~~ num_q , ok_list , err_list = self . _multi . info_read ( ) \n 
for curl in ok_list : \n 
~~~ self . _finish ( curl ) \n 
~~ for curl , errnum , errmsg in err_list : \n 
~~~ self . _finish ( curl , errnum , errmsg ) \n 
~~ if num_q == 0 : \n 
~~ ~~ self . _process_queue ( ) \n 
~~ def _process_queue ( self ) : \n 
~~~ with stack_context . NullContext ( ) : \n 
~~~ started = 0 \n 
while self . _free_list and self . _requests : \n 
~~~ started += 1 \n 
curl = self . _free_list . pop ( ) \n 
( request , callback ) = self . _requests . popleft ( ) \n 
curl . info = { \n 
"headers" : httputil . HTTPHeaders ( ) , \n 
"buffer" : BytesIO ( ) , \n 
"request" : request , \n 
"callback" : callback , \n 
"curl_start_time" : time . time ( ) , \n 
_curl_setup_request ( curl , request , curl . info [ "buffer" ] , \n 
curl . info [ "headers" ] ) \n 
self . _multi . add_handle ( curl ) \n 
~~ if not started : \n 
~~ ~~ ~~ ~~ def _finish ( self , curl , curl_error = None , curl_message = None ) : \n 
~~~ info = curl . info \n 
curl . info = None \n 
self . _multi . remove_handle ( curl ) \n 
self . _free_list . append ( curl ) \n 
buffer = info [ "buffer" ] \n 
if curl_error : \n 
~~~ error = CurlError ( curl_error , curl_message ) \n 
code = error . code \n 
effective_url = None \n 
buffer . close ( ) \n 
buffer = None \n 
~~~ error = None \n 
code = curl . getinfo ( pycurl . HTTP_CODE ) \n 
effective_url = curl . getinfo ( pycurl . EFFECTIVE_URL ) \n 
buffer . seek ( 0 ) \n 
~~ time_info = dict ( \n 
queue = info [ "curl_start_time" ] - info [ "request" ] . start_time , \n 
namelookup = curl . getinfo ( pycurl . NAMELOOKUP_TIME ) , \n 
connect = curl . getinfo ( pycurl . CONNECT_TIME ) , \n 
pretransfer = curl . getinfo ( pycurl . PRETRANSFER_TIME ) , \n 
starttransfer = curl . getinfo ( pycurl . STARTTRANSFER_TIME ) , \n 
total = curl . getinfo ( pycurl . TOTAL_TIME ) , \n 
redirect = curl . getinfo ( pycurl . REDIRECT_TIME ) , \n 
~~~ info [ "callback" ] ( HTTPResponse ( \n 
request = info [ "request" ] , code = code , headers = info [ "headers" ] , \n 
buffer = buffer , effective_url = effective_url , error = error , \n 
reason = info [ ] . get ( "X-Http-Reason" , None ) , \n 
request_time = time . time ( ) - info [ "curl_start_time" ] , \n 
time_info = time_info ) ) \n 
~~~ self . handle_callback_exception ( info [ "callback" ] ) \n 
~~ ~~ def handle_callback_exception ( self , callback ) : \n 
~~~ self . io_loop . handle_callback_exception ( callback ) \n 
~~ ~~ class CurlError ( HTTPError ) : \n 
~~~ def __init__ ( self , errno , message ) : \n 
~~~ HTTPError . __init__ ( self , 599 , message ) \n 
self . errno = errno \n 
~~ ~~ def _curl_create ( ) : \n 
~~~ curl = pycurl . Curl ( ) \n 
if gen_log . isEnabledFor ( logging . DEBUG ) : \n 
~~~ curl . setopt ( pycurl . VERBOSE , 1 ) \n 
curl . setopt ( pycurl . DEBUGFUNCTION , _curl_debug ) \n 
~~ return curl \n 
~~ def _curl_setup_request ( curl , request , buffer , headers ) : \n 
~~~ curl . setopt ( pycurl . URL , native_str ( request . url ) ) \n 
if "Expect" not in request . headers : \n 
~~~ request . headers [ "Expect" ] = "" \n 
~~ if "Pragma" not in request . headers : \n 
~~~ request . headers [ "Pragma" ] = "" \n 
~~ if isinstance ( request . headers , httputil . HTTPHeaders ) : \n 
~~~ curl . setopt ( pycurl . HTTPHEADER , \n 
~~ if request . header_callback : \n 
~~~ curl . setopt ( pycurl . HEADERFUNCTION , \n 
lambda line : request . header_callback ( native_str ( line ) ) ) \n 
lambda line : _curl_header_callback ( headers , \n 
native_str ( line ) ) ) \n 
~~ if request . streaming_callback : \n 
~~~ write_function = request . streaming_callback \n 
~~~ write_function = buffer . write \n 
~~~ curl . setopt ( pycurl . WRITEFUNCTION , write_function ) \n 
~~~ curl . setopt ( pycurl . WRITEFUNCTION , lambda s : write_function ( utf8 ( s ) ) ) \n 
~~ curl . setopt ( pycurl . FOLLOWLOCATION , request . follow_redirects ) \n 
curl . setopt ( pycurl . MAXREDIRS , request . max_redirects ) \n 
curl . setopt ( pycurl . CONNECTTIMEOUT_MS , int ( 1000 * request . connect_timeout ) ) \n 
curl . setopt ( pycurl . TIMEOUT_MS , int ( 1000 * request . request_timeout ) ) \n 
if request . user_agent : \n 
~~~ curl . setopt ( pycurl . USERAGENT , native_str ( request . user_agent ) ) \n 
~~ if request . network_interface : \n 
~~~ curl . setopt ( pycurl . INTERFACE , request . network_interface ) \n 
~~ if request . decompress_response : \n 
~~~ curl . setopt ( pycurl . ENCODING , "gzip,deflate" ) \n 
~~~ curl . setopt ( pycurl . ENCODING , "none" ) \n 
~~ if request . proxy_host and request . proxy_port : \n 
~~~ curl . setopt ( pycurl . PROXY , request . proxy_host ) \n 
curl . setopt ( pycurl . PROXYPORT , request . proxy_port ) \n 
if request . proxy_username : \n 
~~~ credentials = % ( request . proxy_username , \n 
request . proxy_password ) \n 
curl . setopt ( pycurl . PROXYUSERPWD , credentials ) \n 
~~~ curl . setopt ( pycurl . PROXY , ) \n 
curl . unsetopt ( pycurl . PROXYUSERPWD ) \n 
~~ if request . validate_cert : \n 
~~~ curl . setopt ( pycurl . SSL_VERIFYPEER , 1 ) \n 
curl . setopt ( pycurl . SSL_VERIFYHOST , 2 ) \n 
~~~ curl . setopt ( pycurl . SSL_VERIFYPEER , 0 ) \n 
curl . setopt ( pycurl . SSL_VERIFYHOST , 0 ) \n 
~~ if request . ca_certs is not None : \n 
~~~ curl . setopt ( pycurl . CAINFO , request . ca_certs ) \n 
~~ if request . allow_ipv6 is False : \n 
~~~ curl . setopt ( pycurl . IPRESOLVE , pycurl . IPRESOLVE_V4 ) \n 
~~~ curl . setopt ( pycurl . IPRESOLVE , pycurl . IPRESOLVE_WHATEVER ) \n 
~~ curl_options = { \n 
"GET" : pycurl . HTTPGET , \n 
"POST" : pycurl . POST , \n 
"PUT" : pycurl . UPLOAD , \n 
"HEAD" : pycurl . NOBODY , \n 
custom_methods = set ( [ "DELETE" , "OPTIONS" , "PATCH" ] ) \n 
for o in curl_options . values ( ) : \n 
~~~ curl . setopt ( o , False ) \n 
~~ if request . method in curl_options : \n 
~~~ curl . unsetopt ( pycurl . CUSTOMREQUEST ) \n 
curl . setopt ( curl_options [ request . method ] , True ) \n 
~~ elif request . allow_nonstandard_methods or request . method in custom_methods : \n 
~~~ curl . setopt ( pycurl . CUSTOMREQUEST , request . method ) \n 
~~~ raise KeyError ( + request . method ) \n 
~~ if request . method in ( "POST" , "PUT" ) : \n 
~~~ if request . body is None : \n 
~~~ raise AssertionError ( \n 
% request . method ) \n 
~~ request_buffer = BytesIO ( utf8 ( request . body ) ) \n 
curl . setopt ( pycurl . READFUNCTION , request_buffer . read ) \n 
~~~ def ioctl ( cmd ) : \n 
~~~ if cmd == curl . IOCMD_RESTARTREAD : \n 
~~~ request_buffer . seek ( 0 ) \n 
~~ ~~ curl . setopt ( pycurl . IOCTLFUNCTION , ioctl ) \n 
curl . setopt ( pycurl . POSTFIELDSIZE , len ( request . body ) ) \n 
~~~ curl . setopt ( pycurl . INFILESIZE , len ( request . body ) ) \n 
~~ ~~ elif request . method == "GET" : \n 
~~~ if request . body is not None : \n 
~~~ raise AssertionError ( ) \n 
~~ ~~ if request . auth_username is not None : \n 
~~~ userpwd = "%s:%s" % ( request . auth_username , request . auth_password or ) \n 
if request . auth_mode is None or request . auth_mode == "basic" : \n 
~~~ curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_BASIC ) \n 
~~ elif request . auth_mode == "digest" : \n 
~~~ curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_DIGEST ) \n 
~~ curl . setopt ( pycurl . USERPWD , native_str ( userpwd ) ) \n 
request . auth_username ) \n 
~~~ curl . unsetopt ( pycurl . USERPWD ) \n 
~~ if request . client_cert is not None : \n 
~~~ curl . setopt ( pycurl . SSLCERT , request . client_cert ) \n 
~~ if request . client_key is not None : \n 
~~~ curl . setopt ( pycurl . SSLKEY , request . client_key ) \n 
~~ if threading . activeCount ( ) > 1 : \n 
~~~ curl . setopt ( pycurl . NOSIGNAL , 1 ) \n 
~~ if request . prepare_curl_callback is not None : \n 
~~~ request . prepare_curl_callback ( curl ) \n 
~~ ~~ def _curl_header_callback ( headers , header_line ) : \n 
~~~ header_line = header_line . strip ( ) \n 
if header_line . startswith ( "HTTP/" ) : \n 
~~~ headers . clear ( ) \n 
~~~ ( __ , __ , reason ) = httputil . parse_response_start_line ( header_line ) \n 
~~ except httputil . HTTPInputError : \n 
~~ ~~ if not header_line : \n 
~~ headers . parse_line ( header_line ) \n 
~~ def _curl_debug ( debug_type , debug_msg ) : \n 
~~~ debug_types = ( , , , , ) \n 
if debug_type == 0 : \n 
~~~ gen_log . debug ( , debug_msg . strip ( ) ) \n 
~~ elif debug_type in ( 1 , 2 ) : \n 
~~~ for line in debug_msg . splitlines ( ) : \n 
~~~ gen_log . debug ( , debug_types [ debug_type ] , line ) \n 
~~ ~~ elif debug_type == 4 : \n 
~~~ gen_log . debug ( , debug_types [ debug_type ] , debug_msg ) \n 
~~~ AsyncHTTPClient . configure ( CurlAsyncHTTPClient ) \n 
main ( ) \n 
import errno \n 
from tornado . log import app_log \n 
from tornado . ioloop import IOLoop \n 
from tornado . iostream import IOStream , SSLIOStream \n 
from tornado . netutil import bind_sockets , add_accept_handler , ssl_wrap_socket \n 
from tornado import process \n 
from tornado . util import errno_from_exception \n 
~~~ ssl = None \n 
~~ class TCPServer ( object ) : \n 
def __init__ ( self , io_loop = None , ssl_options = None , max_buffer_size = None , \n 
read_chunk_size = None ) : \n 
~~~ self . io_loop = io_loop \n 
self . ssl_options = ssl_options \n 
self . _pending_sockets = [ ] \n 
self . _started = False \n 
self . max_buffer_size = max_buffer_size \n 
self . read_chunk_size = None \n 
if self . ssl_options is not None and isinstance ( self . ssl_options , dict ) : \n 
~~~ if not in self . ssl_options : \n 
~~ if not os . path . exists ( self . ssl_options [ ] ) : \n 
self . ssl_options [ ] ) \n 
~~ if ( in self . ssl_options and \n 
not os . path . exists ( self . ssl_options [ ] ) ) : \n 
~~ ~~ ~~ def listen ( self , port , address = "" ) : \n 
sockets = bind_sockets ( port , address = address ) \n 
self . add_sockets ( sockets ) \n 
~~ def add_sockets ( self , sockets ) : \n 
if self . io_loop is None : \n 
~~~ self . io_loop = IOLoop . current ( ) \n 
~~ for sock in sockets : \n 
~~~ self . _sockets [ sock . fileno ( ) ] = sock \n 
add_accept_handler ( sock , self . _handle_connection , \n 
io_loop = self . io_loop ) \n 
~~ ~~ def add_socket ( self , socket ) : \n 
self . add_sockets ( [ socket ] ) \n 
~~ def bind ( self , port , address = None , family = socket . AF_UNSPEC , backlog = 128 ) : \n 
sockets = bind_sockets ( port , address = address , family = family , \n 
backlog = backlog ) \n 
if self . _started : \n 
~~~ self . add_sockets ( sockets ) \n 
~~~ self . _pending_sockets . extend ( sockets ) \n 
~~ ~~ def start ( self , num_processes = 1 ) : \n 
assert not self . _started \n 
self . _started = True \n 
if num_processes != 1 : \n 
~~~ process . fork_processes ( num_processes ) \n 
~~ sockets = self . _pending_sockets \n 
for fd , sock in self . _sockets . items ( ) : \n 
~~ ~~ def handle_stream ( self , stream , address ) : \n 
raise NotImplementedError ( ) \n 
~~ def _handle_connection ( self , connection , address ) : \n 
~~~ if self . ssl_options is not None : \n 
~~~ connection = ssl_wrap_socket ( connection , \n 
self . ssl_options , \n 
server_side = True , \n 
do_handshake_on_connect = False ) \n 
~~ except ssl . SSLError as err : \n 
~~~ if err . args [ 0 ] == ssl . SSL_ERROR_EOF : \n 
~~~ return connection . close ( ) \n 
~~~ raise \n 
~~ ~~ except socket . error as err : \n 
~~~ if errno_from_exception ( err ) in ( errno . ECONNABORTED , errno . EINVAL ) : \n 
~~~ stream = SSLIOStream ( connection , io_loop = self . io_loop , \n 
max_buffer_size = self . max_buffer_size , \n 
read_chunk_size = self . read_chunk_size ) \n 
~~~ stream = IOStream ( connection , io_loop = self . io_loop , \n 
~~ self . handle_stream ( stream , address ) \n 
~~ ~~ ~~ from textwrap import dedent \n 
from flask import abort , redirect , render_template , request , url_for \n 
from pypi_portal . core import flash \n 
from pypi_portal . blueprints import examples_alerts \n 
@ examples_alerts . route ( ) \n 
def index ( ) : \n 
~~~ return render_template ( ) \n 
~~ @ examples_alerts . route ( ) \n 
def modal ( ) : \n 
message_size = request . args . get ( ) \n 
flash_count = request . args . get ( ) \n 
flash_type = request . args . get ( ) \n 
available_types = [ k for k , v in flash . __dict__ . items ( ) if callable ( v ) ] \n 
if flash_type not in available_types : \n 
~~~ abort ( 400 ) \n 
~~ if not str ( flash_count ) . isdigit ( ) or not ( 1 <= int ( flash_count ) <= 10 ) : \n 
~~ if message_size == : \n 
~~ elif message_size == : \n 
~~~ message = \n 
~~ func = getattr ( flash , flash_type ) \n 
for i in range ( int ( flash_count ) ) : \n 
~~~ func ( message ) \n 
~~ return redirect ( url_for ( ) ) \n 
from colorclass . codes import ANSICodeMapping , BASE_CODES \n 
CODE_GROUPS = ( \n 
RE_ANSI = re . compile ( ) \n 
RE_COMBINE = re . compile ( ) \n 
RE_SPLIT = re . compile ( ) \n 
def prune_overridden ( ansi_string ) : \n 
for escape , codes in multi_seqs : \n 
~~~ r_codes = list ( reversed ( codes . split ( ) ) ) \n 
~~~ r_codes = r_codes [ : r_codes . index ( ) + 1 ] \n 
~~ for group in CODE_GROUPS : \n 
~~~ for pos in reversed ( [ i for i , n in enumerate ( r_codes ) if n in group ] [ 1 : ] ) : \n 
~~~ r_codes . pop ( pos ) \n 
~~ ~~ reduced_codes = . join ( sorted ( r_codes , key = int ) ) \n 
if codes != reduced_codes : \n 
~~~ ansi_string = ansi_string . replace ( escape , + reduced_codes + ) \n 
~~ ~~ return ansi_string \n 
~~ def parse_input ( tagged_string , disable_colors ) : \n 
codes = ANSICodeMapping ( tagged_string ) \n 
output_colors = getattr ( tagged_string , , tagged_string ) \n 
for tag , replacement in ( ( + k + , if v is None else % v ) for k , v in codes . ~~~ output_colors = output_colors . replace ( tag , replacement ) \n 
~~ output_no_colors = RE_ANSI . sub ( , output_colors ) \n 
if disable_colors : \n 
~~~ return output_no_colors , output_no_colors \n 
~~~ simplified = RE_COMBINE . sub ( , output_colors ) \n 
if simplified == output_colors : \n 
~~ output_colors = simplified \n 
~~ output_colors = prune_overridden ( output_colors ) \n 
previous_escape = None \n 
segments = list ( ) \n 
for item in ( i for i in RE_SPLIT . split ( output_colors ) if i ) : \n 
~~~ if RE_SPLIT . match ( item ) : \n 
~~~ if item != previous_escape : \n 
~~~ segments . append ( item ) \n 
previous_escape = item \n 
~~ ~~ output_colors = . join ( segments ) \n 
return output_colors , output_no_colors \n 
from terminaltables . tables import AsciiTable , UnixTable \n 
@ pytest . mark . parametrize ( , [ AsciiTable , UnixTable ] ) \n 
def test_empty ( cls ) : \n 
table = cls ( [ ] ) \n 
assert table . padded_table_data == [ ] \n 
table = cls ( [ [ ] ] ) \n 
assert table . padded_table_data == [ [ ] ] \n 
~~ @ pytest . mark . parametrize ( , [ AsciiTable , UnixTable ] ) \n 
def test_simple ( cls ) : \n 
table_data = [ \n 
[ , , ] , \n 
table = cls ( table_data ) \n 
expected = [ \n 
assert table . padded_table_data == expected \n 
table_data . append ( [ , ] ) \n 
table_data . append ( [ ] ) \n 
def test_attributes ( cls ) : \n 
[ , ] \n 
table . justify_columns [ 0 ] = \n 
[ , , ] \n 
table . justify_columns [ 2 ] = \n 
def test_multi_line ( cls ) : \n 
table . justify_columns = { 1 : , 2 : } \n 
~~ from flask . ext . wtf import Form \n 
from wtforms import StringField , TextAreaField , BooleanField , SelectField , SubmitField \n 
from wtforms . validators import Required , Length , Email , Regexp \n 
from wtforms import ValidationError \n 
from flask . ext . pagedown . fields import PageDownField \n 
from . . models import Role , User \n 
class NameForm ( Form ) : \n 
~~~ name = StringField ( , validators = [ Required ( ) ] ) \n 
submit = SubmitField ( ) \n 
~~ class EditProfileForm ( Form ) : \n 
~~~ name = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
location = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
about_me = TextAreaField ( ) \n 
~~ class EditProfileAdminForm ( Form ) : \n 
~~~ email = StringField ( , validators = [ Required ( ) , Length ( 1 , 64 ) , \n 
Email ( ) ] ) \n 
username = StringField ( , validators = [ \n 
Required ( ) , Length ( 1 , 64 ) , Regexp ( , 0 , \n 
) ] ) \n 
confirmed = BooleanField ( ) \n 
role = SelectField ( , coerce = int ) \n 
name = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
def __init__ ( self , user , * args , ** kwargs ) : \n 
~~~ super ( EditProfileAdminForm , self ) . __init__ ( * args , ** kwargs ) \n 
self . role . choices = [ ( role . id , role . name ) \n 
for role in Role . query . order_by ( Role . name ) . all ( ) ] \n 
~~ def validate_email ( self , field ) : \n 
~~~ if field . data != self . user . email and User . query . filter_by ( email = field . data ) . first ( ) : \n 
~~~ raise ValidationError ( ) \n 
~~ ~~ def validate_username ( self , field ) : \n 
~~~ if field . data != self . user . username and User . query . filter_by ( username = field . data ) . first ( ) : \n 
~~ ~~ ~~ class PostForm ( Form ) : \n 
~~ class CommentForm ( Form ) : \n 
~~~ body = StringField ( , validators = [ Required ( ) ] ) \n 
__all__ = [ "filter" , "fnmatch" , "fnmatchcase" , "translate" ] \n 
_cache = { } \n 
_MAXCACHE = 100 \n 
def _purge ( ) : \n 
_cache . clear ( ) \n 
~~ def fnmatch ( name , pat ) : \n 
name = os . path . normcase ( name ) \n 
pat = os . path . normcase ( pat ) \n 
return fnmatchcase ( name , pat ) \n 
~~ def filter ( names , pat ) : \n 
import os , posixpath \n 
~~~ re_pat = _cache [ pat ] \n 
~~~ res = translate ( pat ) \n 
if len ( _cache ) >= _MAXCACHE : \n 
~~~ _cache . clear ( ) \n 
~~ _cache [ pat ] = re_pat = re . compile ( res ) \n 
~~ match = re_pat . match \n 
if os . path is posixpath : \n 
~~~ for name in names : \n 
~~~ if match ( name ) : \n 
~~~ result . append ( name ) \n 
~~~ if match ( os . path . normcase ( name ) ) : \n 
~~ ~~ ~~ return result \n 
~~ def fnmatchcase ( name , pat ) : \n 
~~ return re_pat . match ( name ) is not None \n 
~~ def translate ( pat ) : \n 
i , n = 0 , len ( pat ) \n 
res = \n 
while i < n : \n 
~~~ c = pat [ i ] \n 
i = i + 1 \n 
if c == : \n 
~~~ res = res + \n 
~~ elif c == : \n 
~~~ j = i \n 
if j < n and pat [ j ] == : \n 
~~~ j = j + 1 \n 
~~ if j < n and pat [ j ] == : \n 
~~ while j < n and pat [ j ] != : \n 
~~ if j >= n : \n 
~~~ stuff = pat [ i : j ] . replace ( , ) \n 
i = j + 1 \n 
if stuff [ 0 ] == : \n 
~~~ stuff = + stuff [ 1 : ] \n 
~~ elif stuff [ 0 ] == : \n 
~~~ stuff = + stuff \n 
~~ res = % ( res , stuff ) \n 
~~~ res = res + re . escape ( c ) \n 
~~ ~~ return res + \n 
~~ from contextlib import contextmanager \n 
from sqlalchemy . types import NULLTYPE , Integer \n 
from sqlalchemy import schema as sa_schema \n 
from . import util \n 
from . compat import string_types \n 
from . ddl import impl \n 
__all__ = ( , ) \n 
class Operations ( object ) : \n 
def __init__ ( self , migration_context ) : \n 
self . migration_context = migration_context \n 
self . impl = migration_context . impl \n 
def context ( cls , migration_context ) : \n 
~~~ from . op import _install_proxy , _remove_proxy \n 
op = Operations ( migration_context ) \n 
_install_proxy ( op ) \n 
yield op \n 
_remove_proxy ( ) \n 
~~ def _primary_key_constraint ( self , name , table_name , cols , schema = None ) : \n 
~~~ m = sa_schema . MetaData ( ) \n 
columns = [ sa_schema . Column ( n , NULLTYPE ) for n in cols ] \n 
t1 = sa_schema . Table ( table_name , m , \n 
* columns , \n 
schema = schema ) \n 
p = sa_schema . PrimaryKeyConstraint ( * columns , name = name ) \n 
t1 . append_constraint ( p ) \n 
return p \n 
~~ def _foreign_key_constraint ( self , name , source , referent , \n 
local_cols , remote_cols , \n 
onupdate = None , ondelete = None , \n 
deferrable = None , source_schema = None , \n 
referent_schema = None ) : \n 
if source == referent : \n 
~~~ t1_cols = local_cols + remote_cols \n 
~~~ t1_cols = local_cols \n 
sa_schema . Table ( referent , m , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in remote_cols ] , \n 
schema = referent_schema ) \n 
~~ t1 = sa_schema . Table ( source , m , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in t1_cols ] , \n 
schema = source_schema ) \n 
tname = "%s.%s" % ( referent_schema , referent ) if referent_schema else referent \n 
f = sa_schema . ForeignKeyConstraint ( local_cols , \n 
[ "%s.%s" % ( tname , n ) \n 
for n in remote_cols ] , \n 
onupdate = onupdate , \n 
ondelete = ondelete , \n 
deferrable = deferrable \n 
t1 . append_constraint ( f ) \n 
return f \n 
~~ def _unique_constraint ( self , name , source , local_cols , schema = None , ** kw ) : \n 
~~~ t = sa_schema . Table ( source , sa_schema . MetaData ( ) , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in local_cols ] , \n 
kw [ ] = name \n 
uq = sa_schema . UniqueConstraint ( * [ t . c [ n ] for n in local_cols ] , ** kw ) \n 
t . append_constraint ( uq ) \n 
return uq \n 
~~ def _check_constraint ( self , name , source , condition , schema = None , ** kw ) : \n 
sa_schema . Column ( , Integer ) , schema = schema ) \n 
ck = sa_schema . CheckConstraint ( condition , name = name , ** kw ) \n 
t . append_constraint ( ck ) \n 
return ck \n 
~~ def _table ( self , name , * columns , ** kw ) : \n 
t = sa_schema . Table ( name , m , * columns , ** kw ) \n 
for f in t . foreign_keys : \n 
~~~ self . _ensure_table_for_fk ( m , f ) \n 
~~ return t \n 
~~ def _column ( self , name , type_ , ** kw ) : \n 
~~~ return sa_schema . Column ( name , type_ , ** kw ) \n 
~~ def _index ( self , name , tablename , columns , schema = None , ** kw ) : \n 
~~~ t = sa_schema . Table ( tablename or , sa_schema . MetaData ( ) , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in columns ] , \n 
schema = schema \n 
return sa_schema . Index ( name , * [ t . c [ n ] for n in columns ] , ** kw ) \n 
~~ def _parse_table_key ( self , table_key ) : \n 
~~~ if in table_key : \n 
~~~ tokens = table_key . split ( ) \n 
sname = "." . join ( tokens [ 0 : - 1 ] ) \n 
tname = tokens [ - 1 ] \n 
~~~ tname = table_key \n 
sname = None \n 
~~ return ( sname , tname ) \n 
~~ def _ensure_table_for_fk ( self , metadata , fk ) : \n 
if isinstance ( fk . _colspec , string_types ) : \n 
~~~ table_key , cname = fk . _colspec . rsplit ( , 1 ) \n 
sname , tname = self . _parse_table_key ( table_key ) \n 
if table_key not in metadata . tables : \n 
~~~ rel_t = sa_schema . Table ( tname , metadata , schema = sname ) \n 
~~~ rel_t = metadata . tables [ table_key ] \n 
~~ if cname not in rel_t . c : \n 
~~~ rel_t . append_column ( sa_schema . Column ( cname , NULLTYPE ) ) \n 
~~ ~~ ~~ def get_context ( self ) : \n 
return self . migration_context \n 
~~ def rename_table ( self , old_table_name , new_table_name , schema = None ) : \n 
self . impl . rename_table ( \n 
old_table_name , \n 
new_table_name , \n 
~~ @ util . _with_legacy_names ( [ ( , ) ] ) \n 
def alter_column ( self , table_name , column_name , \n 
nullable = None , \n 
server_default = False , \n 
new_column_name = None , \n 
type_ = None , \n 
autoincrement = None , \n 
existing_type = None , \n 
existing_server_default = False , \n 
existing_nullable = None , \n 
existing_autoincrement = None , \n 
schema = None \n 
compiler = self . impl . dialect . statement_compiler ( \n 
self . impl . dialect , \n 
None \n 
def _count_constraint ( constraint ) : \n 
~~~ return not isinstance ( constraint , sa_schema . PrimaryKeyConstraint ) and ( not constraint . _create_rule or \n 
constraint . _create_rule ( compiler ) ) \n 
~~ if existing_type and type_ : \n 
~~~ t = self . _table ( table_name , \n 
sa_schema . Column ( column_name , existing_type ) , \n 
for constraint in t . constraints : \n 
~~~ if _count_constraint ( constraint ) : \n 
~~~ self . impl . drop_constraint ( constraint ) \n 
~~ ~~ ~~ self . impl . alter_column ( table_name , column_name , \n 
nullable = nullable , \n 
server_default = server_default , \n 
name = new_column_name , \n 
type_ = type_ , \n 
schema = schema , \n 
autoincrement = autoincrement , \n 
existing_type = existing_type , \n 
existing_server_default = existing_server_default , \n 
existing_nullable = existing_nullable , \n 
existing_autoincrement = existing_autoincrement \n 
if type_ : \n 
sa_schema . Column ( column_name , type_ ) , \n 
~~~ self . impl . add_constraint ( constraint ) \n 
~~ ~~ ~~ ~~ def add_column ( self , table_name , column , schema = None ) : \n 
t = self . _table ( table_name , column , schema = schema ) \n 
self . impl . add_column ( \n 
table_name , \n 
column , \n 
~~~ if not isinstance ( constraint , sa_schema . PrimaryKeyConstraint ) : \n 
~~ ~~ ~~ def drop_column ( self , table_name , column_name , ** kw ) : \n 
self . impl . drop_column ( \n 
self . _column ( column_name , NULLTYPE ) , \n 
** kw \n 
~~ def create_primary_key ( self , name , table_name , cols , schema = None ) : \n 
self . impl . add_constraint ( \n 
self . _primary_key_constraint ( name , table_name , cols , \n 
schema ) \n 
~~ def create_foreign_key ( self , name , source , referent , local_cols , \n 
remote_cols , onupdate = None , ondelete = None , \n 
self . _foreign_key_constraint ( name , source , referent , \n 
onupdate = onupdate , ondelete = ondelete , \n 
deferrable = deferrable , source_schema = source_schema , \n 
referent_schema = referent_schema ) \n 
~~ def create_unique_constraint ( self , name , source , local_cols , \n 
schema = None , ** kw ) : \n 
self . _unique_constraint ( name , source , local_cols , \n 
schema = schema , ** kw ) \n 
~~ def create_check_constraint ( self , name , source , condition , \n 
self . _check_constraint ( name , source , condition , schema = schema , ** kw ) \n 
~~ def create_table ( self , name , * columns , ** kw ) : \n 
self . impl . create_table ( \n 
self . _table ( name , * columns , ** kw ) \n 
~~ def drop_table ( self , name , ** kw ) : \n 
self . impl . drop_table ( \n 
self . _table ( name , ** kw ) \n 
~~ def create_index ( self , name , table_name , columns , schema = None , ** kw ) : \n 
self . impl . create_index ( \n 
self . _index ( name , table_name , columns , schema = schema , ** kw ) \n 
def drop_index ( self , name , table_name = None , schema = None ) : \n 
self . impl . drop_index ( \n 
self . _index ( name , table_name , [ ] , schema = schema ) \n 
~~ @ util . _with_legacy_names ( [ ( "type" , "type_" ) ] ) \n 
def drop_constraint ( self , name , table_name , type_ = None , schema = None ) : \n 
t = self . _table ( table_name , schema = schema ) \n 
types = { \n 
: lambda name : sa_schema . ForeignKeyConstraint ( \n 
[ ] , [ ] , name = name ) , \n 
: sa_schema . PrimaryKeyConstraint , \n 
: sa_schema . UniqueConstraint , \n 
: lambda name : sa_schema . CheckConstraint ( "" , name = name ) , \n 
None : sa_schema . Constraint \n 
~~~ const = types [ type_ ] \n 
~~ const = const ( name = name ) \n 
t . append_constraint ( const ) \n 
self . impl . drop_constraint ( const ) \n 
~~ def bulk_insert ( self , table , rows ) : \n 
self . impl . bulk_insert ( table , rows ) \n 
~~ def inline_literal ( self , value , type_ = None ) : \n 
return impl . _literal_bindparam ( None , value , type_ = type_ ) \n 
~~ def execute ( self , sql , execution_options = None ) : \n 
self . migration_context . impl . execute ( sql , \n 
execution_options = execution_options ) \n 
~~ def get_bind ( self ) : \n 
return self . migration_context . impl . bind \n 
import os , re , sys \n 
~~~ from sets import Set as set \n 
~~~ sorted = sorted \n 
~~~ def sorted ( iterable ) : \n 
lst = list ( iterable ) \n 
lst . sort ( ) \n 
return lst \n 
~~~ reversed = reversed \n 
~~~ def reversed ( iterable ) : \n 
return lst [ : : - 1 ] \n 
~~~ "" . rpartition \n 
~~~ def rpartition ( s , sep ) : \n 
i = s . rfind ( sep ) \n 
if i == - 1 : \n 
~~~ return ( , , s ) \n 
~~~ return ( s [ : i ] , sep , s [ i + len ( sep ) : ] ) \n 
return s . rpartition ( sep ) \n 
~~~ from cStringIO import StringIO \n 
BytesIO = StringIO \n 
~~~ from io import StringIO , BytesIO \n 
~~~ string_class = basestring \n 
~~~ string_class = str \n 
~~~ import cPickle as pickle \n 
~~~ range = xrange \n 
~~~ range = range \n 
~~~ { } . iteritems \n 
~~~ def iitems ( d ) : \n 
return d . items ( ) \n 
return d . iteritems ( ) \n 
~~ ~~ if sys . version_info >= ( 3 , 0 ) : \n 
~~~ def exec_code_object ( code , global_map ) : \n 
exec ( code , global_map ) \n 
~~~ eval ( \n 
compile ( \n 
"<exec_function>" , "exec" \n 
~~ if sys . version_info >= ( 3 , 0 ) : \n 
~~~ import tokenize \n 
~~~ from io import TextIOWrapper \n 
def open_source ( fname ) : \n 
buffer = open ( fname , ) \n 
encoding , _ = detect_encoding ( buffer . readline ) \n 
text = TextIOWrapper ( buffer , encoding , line_buffering = True ) \n 
text . mode = \n 
return text \n 
~~~ def open_source ( fname ) : \n 
return open ( fname , "rU" ) \n 
~~~ def to_bytes ( s ) : \n 
return s . encode ( ) \n 
~~ def to_string ( b ) : \n 
return b . decode ( ) \n 
~~ def binary_bytes ( byte_values ) : \n 
return bytes ( byte_values ) \n 
~~ def byte_to_int ( byte_value ) : \n 
return byte_value \n 
~~ def bytes_to_ints ( bytes_value ) : \n 
return bytes_value \n 
return b \n 
return "" . join ( [ chr ( b ) for b in byte_values ] ) \n 
return ord ( byte_value ) \n 
for byte in bytes_value : \n 
~~~ yield ord ( byte ) \n 
~~~ import hashlib \n 
md5 = hashlib . md5 \n 
~~~ import md5 \n 
md5 = md5 . new \n 
~~ from wtforms . fields import TextAreaField \n 
from . widgets import PageDown \n 
class PageDownField ( TextAreaField ) : \n 
~~~ widget = PageDown ( ) \n 
from . . dictionaries_loader import get_dictionary \n 
__all__ = [ \n 
def first_name ( ) : \n 
_dict = get_dictionary ( ) \n 
_dict += get_dictionary ( ) \n 
return random . choice ( _dict ) . strip ( ) \n 
~~ def last_name ( ) : \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
~~ def full_name ( ) : \n 
return first_name ( ) + + last_name ( ) \n 
~~ def male_first_name ( ) : \n 
~~ def female_first_name ( ) : \n 
~~ def company_name ( ) : \n 
~~ def job_title ( ) : \n 
result = random . choice ( get_dictionary ( ) ) . strip ( ) \n 
result = result . replace ( , job_title_suffix ( ) ) \n 
return result \n 
~~ def job_title_suffix ( ) : \n 
~~ def title ( ) : \n 
~~ def suffix ( ) : \n 
~~ def location ( ) : \n 
~~ def industry ( ) : \n 
~~ import errno \n 
from gunicorn import util \n 
from gunicorn . six import string_types \n 
SD_LISTEN_FDS_START = 3 \n 
class BaseSocket ( object ) : \n 
~~~ def __init__ ( self , address , conf , log , fd = None ) : \n 
~~~ self . log = log \n 
self . conf = conf \n 
self . cfg_addr = address \n 
if fd is None : \n 
~~~ sock = socket . socket ( self . FAMILY , socket . SOCK_STREAM ) \n 
~~~ sock = socket . fromfd ( fd , self . FAMILY , socket . SOCK_STREAM ) \n 
~~ self . sock = self . set_options ( sock , bound = ( fd is not None ) ) \n 
~~ def __str__ ( self , name ) : \n 
~~~ return getattr ( self . sock , name ) \n 
~~ def set_options ( self , sock , bound = False ) : \n 
if not bound : \n 
~~~ self . bind ( sock ) \n 
~~ sock . setblocking ( 0 ) \n 
if hasattr ( sock , "set_inheritable" ) : \n 
~~~ sock . set_inheritable ( True ) \n 
~~ sock . listen ( self . conf . backlog ) \n 
return sock \n 
~~ def bind ( self , sock ) : \n 
~~~ sock . bind ( self . cfg_addr ) \n 
~~~ self . sock . close ( ) \n 
~~ except socket . error as e : \n 
~~ del self . sock \n 
~~ ~~ class TCPSocket ( BaseSocket ) : \n 
~~~ FAMILY = socket . AF_INET \n 
def __str__ ( self ) : \n 
~~~ if self . conf . is_ssl : \n 
~~~ scheme = "https" \n 
~~~ scheme = "http" \n 
~~ addr = self . sock . getsockname ( ) \n 
return "%s://%s:%d" % ( scheme , addr [ 0 ] , addr [ 1 ] ) \n 
~~~ sock . setsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY , 1 ) \n 
return super ( TCPSocket , self ) . set_options ( sock , bound = bound ) \n 
~~ ~~ class TCP6Socket ( TCPSocket ) : \n 
~~~ FAMILY = socket . AF_INET6 \n 
~~~ ( host , port , fl , sc ) = self . sock . getsockname ( ) \n 
return "http://[%s]:%d" % ( host , port ) \n 
~~ ~~ class UnixSocket ( BaseSocket ) : \n 
~~~ FAMILY = socket . AF_UNIX \n 
def __init__ ( self , addr , conf , log , fd = None ) : \n 
~~~ if fd is None : \n 
~~~ st = os . stat ( addr ) \n 
~~ except OSError as e : \n 
~~~ if e . args [ 0 ] != errno . ENOENT : \n 
~~~ if stat . S_ISSOCK ( st . st_mode ) : \n 
~~~ os . remove ( addr ) \n 
~~ ~~ ~~ self . parent = os . getpid ( ) \n 
super ( UnixSocket , self ) . __init__ ( addr , conf , log , fd = fd ) \n 
~~~ return "unix:%s" % self . cfg_addr \n 
~~~ old_umask = os . umask ( self . conf . umask ) \n 
sock . bind ( self . cfg_addr ) \n 
util . chown ( self . cfg_addr , self . conf . uid , self . conf . gid ) \n 
os . umask ( old_umask ) \n 
~~~ super ( UnixSocket , self ) . close ( ) \n 
if self . parent == os . getpid ( ) : \n 
~~~ os . unlink ( self . cfg_addr ) \n 
~~ ~~ ~~ def _sock_type ( addr ) : \n 
~~~ if isinstance ( addr , tuple ) : \n 
~~~ if util . is_ipv6 ( addr [ 0 ] ) : \n 
~~~ sock_type = TCP6Socket \n 
~~~ sock_type = TCPSocket \n 
~~ ~~ elif isinstance ( addr , string_types ) : \n 
~~~ sock_type = UnixSocket \n 
~~ return sock_type \n 
~~ def create_sockets ( conf , log ) : \n 
listeners = [ ] \n 
if ( in os . environ \n 
and int ( os . environ . get ( ) ) == os . getpid ( ) ) : \n 
~~~ for i in range ( int ( os . environ . get ( , 0 ) ) ) : \n 
~~~ fd = i + SD_LISTEN_FDS_START \n 
~~~ sock = socket . fromfd ( fd , socket . AF_UNIX , socket . SOCK_STREAM ) \n 
sockname = sock . getsockname ( ) \n 
if isinstance ( sockname , str ) and sockname . startswith ( ) : \n 
~~~ listeners . append ( UnixSocket ( sockname , conf , log , fd = fd ) ) \n 
~~ elif len ( sockname ) == 2 and in sockname [ 0 ] : \n 
~~~ listeners . append ( TCPSocket ( "%s:%s" % sockname , conf , log , \n 
fd = fd ) ) \n 
~~ elif len ( sockname ) == 4 and in sockname [ 0 ] : \n 
~~~ listeners . append ( TCP6Socket ( "[%s]:%s" % sockname [ : 2 ] , conf , \n 
log , fd = fd ) ) \n 
~~ ~~ except socket . error : \n 
~~ ~~ del os . environ [ ] , os . environ [ ] \n 
if listeners : \n 
~~~ log . debug ( , \n 
"," . join ( [ str ( l ) for l in listeners ] ) ) \n 
return listeners \n 
~~ ~~ laddr = conf . address \n 
if conf . certfile and not os . path . exists ( conf . certfile ) : \n 
~~ if conf . keyfile and not os . path . exists ( conf . keyfile ) : \n 
~~ if in os . environ : \n 
~~~ fds = os . environ . pop ( ) . split ( ) \n 
for i , fd in enumerate ( fds ) : \n 
~~~ fd = int ( fd ) \n 
addr = laddr [ i ] \n 
sock_type = _sock_type ( addr ) \n 
~~~ listeners . append ( sock_type ( addr , conf , log , fd = fd ) ) \n 
~~~ if e . args [ 0 ] == errno . ENOTCONN : \n 
~~ ~~ ~~ return listeners \n 
~~ for addr in laddr : \n 
~~~ sock_type = _sock_type ( addr ) \n 
sock = None \n 
for i in range ( 5 ) : \n 
~~~ sock = sock_type ( addr , conf , log ) \n 
~~~ if e . args [ 0 ] == errno . EADDRINUSE : \n 
~~ if e . args [ 0 ] == errno . EADDRNOTAVAIL : \n 
~~ if i < 5 : \n 
time . sleep ( 1 ) \n 
~~ ~~ if sock is None : \n 
~~ listeners . append ( sock ) \n 
~~ return listeners \n 
from requests . compat import ( \n 
is_windows , \n 
bytes , \n 
str , \n 
is_py3 , \n 
is_py26 , \n 
~~~ from urllib . parse import urlsplit \n 
~~~ from urlparse import urlsplit \n 
from . import Extension \n 
from . . blockprocessors import OListProcessor , UListProcessor \n 
class SaneOListProcessor ( OListProcessor ) : \n 
~~~ CHILD_RE = re . compile ( ) \n 
SIBLING_TAGS = [ ] \n 
~~ class SaneUListProcessor ( UListProcessor ) : \n 
~~ class SaneListExtension ( Extension ) : \n 
def extendMarkdown ( self , md , md_globals ) : \n 
md . parser . blockprocessors [ ] = SaneOListProcessor ( md . parser ) \n 
md . parser . blockprocessors [ ] = SaneUListProcessor ( md . parser ) \n 
~~ ~~ def makeExtension ( configs = { } ) : \n 
~~~ return SaneListExtension ( configs = configs ) \n 
~~ from __future__ import absolute_import , division , unicode_literals \n 
from pip . _vendor . six import text_type \n 
import gettext \n 
_ = gettext . gettext \n 
from . . constants import voidElements , spaceCharacters \n 
spaceCharacters = "" . join ( spaceCharacters ) \n 
class TreeWalker ( object ) : \n 
~~~ def __init__ ( self , tree ) : \n 
~~~ self . tree = tree \n 
~~ def __iter__ ( self ) : \n 
~~~ raise NotImplementedError \n 
~~ def error ( self , msg ) : \n 
~~~ return { "type" : "SerializeError" , "data" : msg } \n 
~~ def emptyTag ( self , namespace , name , attrs , hasChildren = False ) : \n 
~~~ assert namespace is None or isinstance ( namespace , text_type ) , type ( namespace ) \n 
assert isinstance ( name , text_type ) , type ( name ) \n 
assert all ( ( namespace is None or isinstance ( namespace , text_type ) ) and \n 
isinstance ( name , text_type ) and \n 
isinstance ( value , text_type ) \n 
for ( namespace , name ) , value in attrs . items ( ) ) \n 
yield { "type" : "EmptyTag" , "name" : name , \n 
"namespace" : namespace , \n 
"data" : attrs } \n 
if hasChildren : \n 
~~ ~~ def startTag ( self , namespace , name , attrs ) : \n 
return { "type" : "StartTag" , \n 
"name" : name , \n 
~~ def endTag ( self , namespace , name ) : \n 
assert isinstance ( name , text_type ) , type ( namespace ) \n 
return { "type" : "EndTag" , \n 
"data" : { } } \n 
~~ def text ( self , data ) : \n 
~~~ assert isinstance ( data , text_type ) , type ( data ) \n 
data = data \n 
middle = data . lstrip ( spaceCharacters ) \n 
left = data [ : len ( data ) - len ( middle ) ] \n 
if left : \n 
~~~ yield { "type" : "SpaceCharacters" , "data" : left } \n 
~~ data = middle \n 
middle = data . rstrip ( spaceCharacters ) \n 
right = data [ len ( middle ) : ] \n 
if middle : \n 
~~~ yield { "type" : "Characters" , "data" : middle } \n 
~~ if right : \n 
~~~ yield { "type" : "SpaceCharacters" , "data" : right } \n 
~~ ~~ def comment ( self , data ) : \n 
return { "type" : "Comment" , "data" : data } \n 
~~ def doctype ( self , name , publicId = None , systemId = None , correct = True ) : \n 
~~~ assert name is None or isinstance ( name , text_type ) , type ( name ) \n 
assert publicId is None or isinstance ( publicId , text_type ) , type ( publicId ) \n 
assert systemId is None or isinstance ( systemId , text_type ) , type ( systemId ) \n 
return { "type" : "Doctype" , \n 
"name" : name if name is not None else "" , \n 
"publicId" : publicId , \n 
"systemId" : systemId , \n 
"correct" : correct } \n 
~~ def entity ( self , name ) : \n 
~~~ assert isinstance ( name , text_type ) , type ( name ) \n 
return { "type" : "Entity" , "name" : name } \n 
~~ def unknown ( self , nodeType ) : \n 
~~ ~~ class RecursiveTreeWalker ( TreeWalker ) : \n 
~~~ def walkChildren ( self , node ) : \n 
~~ def element ( self , node , namespace , name , attrs , hasChildren ) : \n 
~~~ if name in voidElements : \n 
~~~ for token in self . emptyTag ( namespace , name , attrs , hasChildren ) : \n 
~~~ yield token \n 
~~~ yield self . startTag ( name , attrs ) \n 
~~~ for token in self . walkChildren ( node ) : \n 
~~ ~~ yield self . endTag ( name ) \n 
~~ ~~ ~~ from xml . dom import Node \n 
DOCUMENT = Node . DOCUMENT_NODE \n 
DOCTYPE = Node . DOCUMENT_TYPE_NODE \n 
TEXT = Node . TEXT_NODE \n 
ELEMENT = Node . ELEMENT_NODE \n 
COMMENT = Node . COMMENT_NODE \n 
ENTITY = Node . ENTITY_NODE \n 
UNKNOWN = "<#UNKNOWN#>" \n 
class NonRecursiveTreeWalker ( TreeWalker ) : \n 
~~~ def getNodeDetails ( self , node ) : \n 
~~ def getFirstChild ( self , node ) : \n 
~~ def getNextSibling ( self , node ) : \n 
~~ def getParentNode ( self , node ) : \n 
~~~ currentNode = self . tree \n 
while currentNode is not None : \n 
~~~ details = self . getNodeDetails ( currentNode ) \n 
type , details = details [ 0 ] , details [ 1 : ] \n 
hasChildren = False \n 
if type == DOCTYPE : \n 
~~~ yield self . doctype ( * details ) \n 
~~ elif type == TEXT : \n 
~~~ for token in self . text ( * details ) : \n 
~~ ~~ elif type == ELEMENT : \n 
~~~ namespace , name , attributes , hasChildren = details \n 
if name in voidElements : \n 
~~~ for token in self . emptyTag ( namespace , name , attributes , \n 
hasChildren ) : \n 
~~ hasChildren = False \n 
~~~ yield self . startTag ( namespace , name , attributes ) \n 
~~ ~~ elif type == COMMENT : \n 
~~~ yield self . comment ( details [ 0 ] ) \n 
~~ elif type == ENTITY : \n 
~~~ yield self . entity ( details [ 0 ] ) \n 
~~ elif type == DOCUMENT : \n 
~~~ hasChildren = True \n 
~~~ yield self . unknown ( details [ 0 ] ) \n 
~~ if hasChildren : \n 
~~~ firstChild = self . getFirstChild ( currentNode ) \n 
~~~ firstChild = None \n 
~~ if firstChild is not None : \n 
~~~ currentNode = firstChild \n 
~~~ while currentNode is not None : \n 
if type == ELEMENT : \n 
if name not in voidElements : \n 
~~~ yield self . endTag ( namespace , name ) \n 
~~ ~~ if self . tree is currentNode : \n 
~~~ currentNode = None \n 
~~ nextSibling = self . getNextSibling ( currentNode ) \n 
if nextSibling is not None : \n 
~~~ currentNode = nextSibling \n 
~~~ currentNode = self . getParentNode ( currentNode ) \n 
esc = "\\x1b[" \n 
codes = { } \n 
codes [ "" ] = "" \n 
codes [ "reset" ] = esc + "39;49;00m" \n 
codes [ "bold" ] = esc + "01m" \n 
codes [ "faint" ] = esc + "02m" \n 
codes [ "standout" ] = esc + "03m" \n 
codes [ "underline" ] = esc + "04m" \n 
codes [ "blink" ] = esc + "05m" \n 
codes [ "overline" ] = esc + "06m" \n 
dark_colors = [ "black" , "darkred" , "darkgreen" , "brown" , "darkblue" , \n 
"purple" , "teal" , "lightgray" ] \n 
light_colors = [ "darkgray" , "red" , "green" , "yellow" , "blue" , \n 
"fuchsia" , "turquoise" , "white" ] \n 
x = 30 \n 
for d , l in zip ( dark_colors , light_colors ) : \n 
~~~ codes [ d ] = esc + "%im" % x \n 
codes [ l ] = esc + "%i;01m" % x \n 
x += 1 \n 
~~ del d , l , x \n 
codes [ "darkteal" ] = codes [ "turquoise" ] \n 
codes [ "darkyellow" ] = codes [ "brown" ] \n 
codes [ "fuscia" ] = codes [ "fuchsia" ] \n 
codes [ "white" ] = codes [ "bold" ] \n 
def reset_color ( ) : \n 
~~~ return codes [ "reset" ] \n 
~~ def colorize ( color_key , text ) : \n 
~~~ return codes [ color_key ] + text + codes [ "reset" ] \n 
~~ def ansiformat ( attr , text ) : \n 
if attr [ : 1 ] == attr [ - 1 : ] == : \n 
~~~ result . append ( codes [ ] ) \n 
attr = attr [ 1 : - 1 ] \n 
~~ if attr [ : 1 ] == attr [ - 1 : ] == : \n 
~~ result . append ( codes [ attr ] ) \n 
result . append ( text ) \n 
result . append ( codes [ ] ) \n 
return . join ( result ) \n 
CONSTANTS = [ , \n 
FUNCTIONS = [ , \n 
DISTRIBUTIONS = [ , \n 
from pygments . style import Style \n 
from pygments . token import Keyword , Name , Comment , String , Error , Number , Operator , Generic , Whitespace \n 
class DefaultStyle ( Style ) : \n 
background_color = "#f8f8f8" \n 
default_style = "" \n 
styles = { \n 
Whitespace : "#bbbbbb" , \n 
Keyword . Pseudo : "nobold" , \n 
Operator : "#666666" , \n 
Name . Builtin : "#008000" , \n 
Name . Function : "#0000FF" , \n 
Name . Variable : "#19177C" , \n 
Name . Constant : "#880000" , \n 
Name . Label : "#A0A000" , \n 
Name . Attribute : "#7D9029" , \n 
Name . Decorator : "#AA22FF" , \n 
String : "#BA2121" , \n 
String . Doc : "italic" , \n 
String . Regex : "#BB6688" , \n 
String . Symbol : "#19177C" , \n 
String . Other : "#008000" , \n 
Number : "#666666" , \n 
Generic . Deleted : "#A00000" , \n 
Generic . Inserted : "#00A000" , \n 
Generic . Error : "#FF0000" , \n 
Generic . Emph : "italic" , \n 
Generic . Strong : "bold" , \n 
Generic . Output : "#888" , \n 
Generic . Traceback : "#04D" , \n 
Error : "border:#FF0000" \n 
from socket import error as SocketError , timeout as SocketTimeout \n 
~~~ from queue import LifoQueue , Empty , Full \n 
~~~ from Queue import LifoQueue , Empty , Full \n 
~~ from . exceptions import ( \n 
ClosedPoolError , \n 
ConnectTimeoutError , \n 
EmptyPoolError , \n 
HostChangedError , \n 
MaxRetryError , \n 
SSLError , \n 
TimeoutError , \n 
ReadTimeoutError , \n 
ProxyError , \n 
from . packages . ssl_match_hostname import CertificateError \n 
from . packages import six \n 
from . connection import ( \n 
DummyConnection , \n 
HTTPConnection , HTTPSConnection , VerifiedHTTPSConnection , \n 
HTTPException , BaseSSLError , \n 
from . request import RequestMethods \n 
from . response import HTTPResponse \n 
from . util import ( \n 
assert_fingerprint , \n 
get_host , \n 
is_connection_dropped , \n 
Timeout , \n 
xrange = six . moves . xrange \n 
log = logging . getLogger ( __name__ ) \n 
_Default = object ( ) \n 
port_by_scheme = { \n 
: 80 , \n 
: 443 , \n 
class ConnectionPool ( object ) : \n 
scheme = None \n 
QueueCls = LifoQueue \n 
def __init__ ( self , host , port = None ) : \n 
~~~ host = host . strip ( ) \n 
self . port = port \n 
~~~ return % ( type ( self ) . __name__ , \n 
self . host , self . port ) \n 
~~ ~~ _blocking_errnos = set ( [ errno . EAGAIN , errno . EWOULDBLOCK ] ) \n 
class HTTPConnectionPool ( ConnectionPool , RequestMethods ) : \n 
scheme = \n 
ConnectionCls = HTTPConnection \n 
def __init__ ( self , host , port = None , strict = False , \n 
timeout = Timeout . DEFAULT_TIMEOUT , maxsize = 1 , block = False , \n 
headers = None , _proxy = None , _proxy_headers = None ) : \n 
~~~ ConnectionPool . __init__ ( self , host , port ) \n 
RequestMethods . __init__ ( self , headers ) \n 
self . strict = strict \n 
if not isinstance ( timeout , Timeout ) : \n 
~~~ timeout = Timeout . from_float ( timeout ) \n 
~~ self . timeout = timeout \n 
self . pool = self . QueueCls ( maxsize ) \n 
self . block = block \n 
self . proxy = _proxy \n 
self . proxy_headers = _proxy_headers or { } \n 
for _ in xrange ( maxsize ) : \n 
~~~ self . pool . put ( None ) \n 
~~ self . num_connections = 0 \n 
self . num_requests = 0 \n 
~~ def _new_conn ( self ) : \n 
self . num_connections += 1 \n 
( self . num_connections , self . host ) ) \n 
extra_params = { } \n 
~~~ extra_params [ ] = self . strict \n 
~~ return self . ConnectionCls ( host = self . host , port = self . port , \n 
timeout = self . timeout . connect_timeout , \n 
** extra_params ) \n 
~~ def _get_conn ( self , timeout = None ) : \n 
conn = None \n 
~~~ conn = self . pool . get ( block = self . block , timeout = timeout ) \n 
~~ except Empty : \n 
~~~ if self . block : \n 
~~~ raise EmptyPoolError ( self , \n 
~~ pass \n 
~~ if conn and is_connection_dropped ( conn ) : \n 
~~ return conn or self . _new_conn ( ) \n 
~~ def _put_conn ( self , conn ) : \n 
~~~ self . pool . put ( conn , block = False ) \n 
~~ except Full : \n 
% self . host ) \n 
~~ if conn : \n 
~~~ conn . close ( ) \n 
~~ ~~ def _get_timeout ( self , timeout ) : \n 
if timeout is _Default : \n 
~~~ return self . timeout . clone ( ) \n 
~~ if isinstance ( timeout , Timeout ) : \n 
~~~ return timeout . clone ( ) \n 
~~~ return Timeout . from_float ( timeout ) \n 
~~ ~~ def _make_request ( self , conn , method , url , timeout = _Default , \n 
** httplib_request_kw ) : \n 
self . num_requests += 1 \n 
timeout_obj = self . _get_timeout ( timeout ) \n 
~~~ timeout_obj . start_connect ( ) \n 
conn . timeout = timeout_obj . connect_timeout \n 
conn . request ( method , url , ** httplib_request_kw ) \n 
~~ except SocketTimeout : \n 
~~~ raise ConnectTimeoutError ( \n 
( self . host , timeout_obj . connect_timeout ) ) \n 
~~ read_timeout = timeout_obj . read_timeout \n 
if hasattr ( conn , ) : \n 
~~~ if read_timeout == 0 : \n 
~~~ raise ReadTimeoutError ( \n 
self , url , \n 
~~ if read_timeout is Timeout . DEFAULT_TIMEOUT : \n 
~~~ conn . sock . settimeout ( socket . getdefaulttimeout ( ) ) \n 
~~~ conn . sock . settimeout ( read_timeout ) \n 
~~~ httplib_response = conn . getresponse ( buffering = True ) \n 
~~~ httplib_response = conn . getresponse ( ) \n 
~~ ~~ except SocketTimeout : \n 
~~ except BaseSSLError as e : \n 
~~ raise \n 
~~~ if e . errno in _blocking_errnos : \n 
~~ http_version = getattr ( conn , , ) \n 
httplib_response . status , \n 
httplib_response . length ) ) \n 
return httplib_response \n 
old_pool , self . pool = self . pool , None \n 
~~~ conn = old_pool . get ( block = False ) \n 
if conn : \n 
~~ ~~ ~~ except Empty : \n 
~~ ~~ def is_same_host ( self , url ) : \n 
if url . startswith ( ) : \n 
~~ scheme , host , port = get_host ( url ) \n 
if self . port and not port : \n 
~~~ port = port_by_scheme . get ( scheme ) \n 
~~ return ( scheme , host , port ) == ( self . scheme , self . host , self . port ) \n 
~~ def urlopen ( self , method , url , body = None , headers = None , retries = 3 , \n 
redirect = True , assert_same_host = True , timeout = _Default , \n 
pool_timeout = None , release_conn = None , ** response_kw ) : \n 
if headers is None : \n 
~~~ headers = self . headers \n 
~~ if retries < 0 : \n 
~~~ raise MaxRetryError ( self , url ) \n 
~~ if release_conn is None : \n 
~~~ release_conn = response_kw . get ( , True ) \n 
~~ if assert_same_host and not self . is_same_host ( url ) : \n 
~~~ raise HostChangedError ( self , url , retries - 1 ) \n 
~~ conn = None \n 
if self . scheme == : \n 
~~~ headers = headers . copy ( ) \n 
headers . update ( self . proxy_headers ) \n 
~~~ conn = self . _get_conn ( timeout = pool_timeout ) \n 
httplib_response = self . _make_request ( conn , method , url , \n 
timeout = timeout , \n 
body = body , headers = headers ) \n 
response_conn = not release_conn and conn \n 
response = HTTPResponse . from_httplib ( httplib_response , \n 
pool = self , \n 
connection = response_conn , \n 
** response_kw ) \n 
~~~ raise SSLError ( e ) \n 
~~ except CertificateError as e : \n 
~~ except TimeoutError as e : \n 
~~~ conn = None \n 
err = e \n 
if retries == 0 : \n 
~~ ~~ except ( HTTPException , SocketError ) as e : \n 
~~~ if isinstance ( e , SocketError ) and self . proxy is not None : \n 
~~~ raise ProxyError ( \n 
% e ) \n 
~~~ raise MaxRetryError ( self , url , e ) \n 
~~ ~~ finally : \n 
~~~ if release_conn : \n 
~~~ self . _put_conn ( conn ) \n 
~~ ~~ if not conn : \n 
return self . urlopen ( method , url , body , headers , retries - 1 , \n 
redirect , assert_same_host , \n 
timeout = timeout , pool_timeout = pool_timeout , \n 
release_conn = release_conn , ** response_kw ) \n 
~~ redirect_location = redirect and response . get_redirect_location ( ) \n 
if redirect_location : \n 
~~~ if response . status == 303 : \n 
~~~ method = \n 
return self . urlopen ( method , redirect_location , body , headers , \n 
retries - 1 , redirect , assert_same_host , \n 
~~ ~~ class HTTPSConnectionPool ( HTTPConnectionPool ) : \n 
ConnectionCls = HTTPSConnection \n 
def __init__ ( self , host , port = None , \n 
strict = False , timeout = None , maxsize = 1 , \n 
block = False , headers = None , \n 
_proxy = None , _proxy_headers = None , \n 
key_file = None , cert_file = None , cert_reqs = None , \n 
ca_certs = None , ssl_version = None , \n 
assert_hostname = None , assert_fingerprint = None ) : \n 
~~~ HTTPConnectionPool . __init__ ( self , host , port , strict , timeout , maxsize , \n 
block , headers , _proxy , _proxy_headers ) \n 
self . key_file = key_file \n 
self . cert_file = cert_file \n 
self . cert_reqs = cert_reqs \n 
self . ca_certs = ca_certs \n 
self . ssl_version = ssl_version \n 
self . assert_hostname = assert_hostname \n 
self . assert_fingerprint = assert_fingerprint \n 
~~ def _prepare_conn ( self , conn ) : \n 
if isinstance ( conn , VerifiedHTTPSConnection ) : \n 
~~~ conn . set_cert ( key_file = self . key_file , \n 
cert_file = self . cert_file , \n 
cert_reqs = self . cert_reqs , \n 
ca_certs = self . ca_certs , \n 
assert_hostname = self . assert_hostname , \n 
assert_fingerprint = self . assert_fingerprint ) \n 
conn . ssl_version = self . ssl_version \n 
~~ if self . proxy is not None : \n 
~~~ set_tunnel = conn . set_tunnel \n 
~~~ set_tunnel = conn . _set_tunnel \n 
~~ set_tunnel ( self . host , self . port , self . proxy_headers ) \n 
conn . connect ( ) \n 
~~ return conn \n 
% ( self . num_connections , self . host ) ) \n 
if not self . ConnectionCls or self . ConnectionCls is DummyConnection : \n 
~~ actual_host = self . host \n 
actual_port = self . port \n 
if self . proxy is not None : \n 
~~~ actual_host = self . proxy . host \n 
actual_port = self . proxy . port \n 
~~ extra_params = { } \n 
~~ conn = self . ConnectionCls ( host = actual_host , port = actual_port , \n 
return self . _prepare_conn ( conn ) \n 
~~ ~~ def connection_from_url ( url , ** kw ) : \n 
scheme , host , port = get_host ( url ) \n 
if scheme == : \n 
~~~ return HTTPSConnectionPool ( host , port = port , ** kw ) \n 
~~~ return HTTPConnectionPool ( host , port = port , ** kw ) \n 
import platform \n 
from subprocess import Popen , STDOUT \n 
from selenium . common . exceptions import WebDriverException \n 
from selenium . webdriver . common import utils \n 
class FirefoxBinary ( object ) : \n 
~~~ NO_FOCUS_LIBRARY_NAME = "x_ignore_nofocus.so" \n 
def __init__ ( self , firefox_path = None , log_file = None ) : \n 
self . _start_cmd = firefox_path \n 
self . _log_file = log_file or open ( os . devnull , "wb" ) \n 
self . command_line = None \n 
if self . _start_cmd is None : \n 
~~~ self . _start_cmd = self . _get_firefox_start_cmd ( ) \n 
~~ if not self . _start_cmd . strip ( ) : \n 
~~ self . _firefox_env = os . environ . copy ( ) \n 
self . _firefox_env [ "MOZ_CRASHREPORTER_DISABLE" ] = "1" \n 
self . _firefox_env [ "MOZ_NO_REMOTE" ] = "1" \n 
self . _firefox_env [ "NO_EM_RESTART" ] = "1" \n 
~~ def add_command_line_options ( self , * args ) : \n 
~~~ self . command_line = args \n 
~~ def launch_browser ( self , profile ) : \n 
self . profile = profile \n 
self . _start_from_profile_path ( self . profile . path ) \n 
self . _wait_until_connectable ( ) \n 
~~ def kill ( self ) : \n 
if self . process : \n 
~~~ self . process . kill ( ) \n 
self . process . wait ( ) \n 
~~ ~~ def _start_from_profile_path ( self , path ) : \n 
~~~ self . _firefox_env [ "XRE_PROFILE_PATH" ] = path \n 
if platform . system ( ) . lower ( ) == : \n 
~~~ self . _modify_link_library_path ( ) \n 
~~ command = [ self . _start_cmd , "-silent" ] \n 
if self . command_line is not None : \n 
~~~ for cli in self . command_line : \n 
~~~ command . append ( cli ) \n 
~~ ~~ Popen ( command , stdout = self . _log_file , stderr = STDOUT , \n 
env = self . _firefox_env ) . communicate ( ) \n 
command [ 1 ] = \n 
self . process = Popen ( \n 
command , stdout = self . _log_file , stderr = STDOUT , \n 
env = self . _firefox_env ) \n 
~~ def _wait_until_connectable ( self ) : \n 
while not utils . is_connectable ( self . profile . port ) : \n 
~~~ if self . process . poll ( ) is not None : \n 
~~ if count == 30 : \n 
~~~ self . kill ( ) \n 
~~ count += 1 \n 
~~ def _find_exe_in_registry ( self ) : \n 
~~~ from _winreg import OpenKey , QueryValue , HKEY_LOCAL_MACHINE , HKEY_CURRENT_USER \n 
~~~ from winreg import OpenKey , QueryValue , HKEY_LOCAL_MACHINE , HKEY_CURRENT_USER \n 
~~ import shlex \n 
keys = ( \n 
r"SOFTWARE\\Classes\\FirefoxHTML\\shell\\open\\command" , \n 
r"SOFTWARE\\Classes\\Applications\\firefox.exe\\shell\\open\\command" \n 
command = "" \n 
for path in keys : \n 
~~~ key = OpenKey ( HKEY_LOCAL_MACHINE , path ) \n 
command = QueryValue ( key , "" ) \n 
~~~ key = OpenKey ( HKEY_CURRENT_USER , path ) \n 
~~~ return "" \n 
~~ if not command : \n 
~~ return shlex . split ( command ) [ 0 ] \n 
~~ def _get_firefox_start_cmd ( self ) : \n 
start_cmd = "" \n 
if platform . system ( ) == "Darwin" : \n 
~~~ start_cmd = ( "/Applications/Firefox.app/Contents/MacOS/firefox-bin" ) \n 
~~ elif platform . system ( ) == "Windows" : \n 
~~~ start_cmd = ( self . _find_exe_in_registry ( ) or \n 
self . _default_windows_location ( ) ) \n 
~~ elif platform . system ( ) == and os . _name == : \n 
~~~ start_cmd = self . _default_windows_location ( ) \n 
~~~ for ffname in [ "firefox" , "iceweasel" ] : \n 
~~~ start_cmd = self . which ( ffname ) \n 
if start_cmd is not None : \n 
~~ ~~ return start_cmd \n 
~~ def _default_windows_location ( self ) : \n 
for path in program_files : \n 
if os . access ( binary_path , os . X_OK ) : \n 
~~~ return binary_path \n 
~~ ~~ return "" \n 
~~ def _modify_link_library_path ( self ) : \n 
~~~ existing_ld_lib_path = os . environ . get ( , ) \n 
new_ld_lib_path = self . _extract_and_check ( \n 
self . profile , self . NO_FOCUS_LIBRARY_NAME , "x86" , "amd64" ) \n 
new_ld_lib_path += existing_ld_lib_path \n 
self . _firefox_env [ "LD_LIBRARY_PATH" ] = new_ld_lib_path \n 
self . _firefox_env [ ] = self . NO_FOCUS_LIBRARY_NAME \n 
~~ def _extract_and_check ( self , profile , no_focus_so_name , x86 , amd64 ) : \n 
~~~ paths = [ x86 , amd64 ] \n 
built_path = "" \n 
~~~ library_path = os . path . join ( profile . path , path ) \n 
os . makedirs ( library_path ) \n 
shutil . copy ( os . path . join ( os . path . dirname ( __file__ ) , path , \n 
self . NO_FOCUS_LIBRARY_NAME ) , \n 
library_path ) \n 
built_path += library_path + ":" \n 
~~ return built_path \n 
~~ def which ( self , fname ) : \n 
for pe in os . environ [ ] . split ( os . pathsep ) : \n 
~~~ checkname = os . path . join ( pe , fname ) \n 
if os . access ( checkname , os . X_OK ) and not os . path . isdir ( checkname ) : \n 
~~~ return checkname \n 
~~ ~~ from selenium . common . exceptions import NoSuchElementException \n 
from selenium . common . exceptions import NoSuchFrameException \n 
from selenium . common . exceptions import StaleElementReferenceException \n 
from selenium . common . exceptions import NoAlertPresentException \n 
class title_is ( object ) : \n 
def __init__ ( self , title ) : \n 
~~~ self . title = title \n 
~~ def __call__ ( self , driver ) : \n 
~~~ return self . title == driver . title \n 
~~ ~~ class title_contains ( object ) : \n 
~~~ return self . title in driver . title \n 
~~ ~~ class presence_of_element_located ( object ) : \n 
def __init__ ( self , locator ) : \n 
~~~ self . locator = locator \n 
~~~ return _find_element ( driver , self . locator ) \n 
~~ ~~ class visibility_of_element_located ( object ) : \n 
~~~ return _element_if_visible ( _find_element ( driver , self . locator ) ) \n 
~~ except StaleElementReferenceException : \n 
~~ ~~ ~~ class visibility_of ( object ) : \n 
def __init__ ( self , element ) : \n 
~~~ self . element = element \n 
~~ def __call__ ( self , ignored ) : \n 
~~~ return _element_if_visible ( self . element ) \n 
~~ ~~ def _element_if_visible ( element ) : \n 
~~~ return element if element . is_displayed ( ) else False \n 
~~ class presence_of_all_elements_located ( object ) : \n 
~~~ return _find_elements ( driver , self . locator ) \n 
~~ ~~ class text_to_be_present_in_element ( object ) : \n 
def __init__ ( self , locator , text_ ) : \n 
self . text = text_ \n 
~~~ element_text = _find_element ( driver , self . locator ) . text \n 
return self . text in element_text \n 
~~ ~~ ~~ class text_to_be_present_in_element_value ( object ) : \n 
~~~ element_text = _find_element ( driver , \n 
self . locator ) . get_attribute ( "value" ) \n 
if element_text : \n 
~~~ return self . text in element_text \n 
~~ ~~ except StaleElementReferenceException : \n 
~~ ~~ ~~ class frame_to_be_available_and_switch_to_it ( object ) : \n 
~~~ self . frame_locator = locator \n 
~~~ if isinstance ( self . frame_locator , tuple ) : \n 
~~~ driver . switch_to . frame ( _find_element ( driver , \n 
self . frame_locator ) ) \n 
~~~ driver . switch_to . frame ( self . frame_locator ) \n 
~~ except NoSuchFrameException : \n 
~~ ~~ ~~ class invisibility_of_element_located ( object ) : \n 
~~~ return not _find_element ( driver , self . locator ) . is_displayed ( ) \n 
~~ except ( NoSuchElementException , StaleElementReferenceException ) : \n 
~~ ~~ ~~ class element_to_be_clickable ( object ) : \n 
~~~ element = visibility_of_element_located ( self . locator ) ( driver ) \n 
if element and element . is_enabled ( ) : \n 
~~~ return element \n 
~~ ~~ ~~ class staleness_of ( object ) : \n 
~~~ self . element . is_enabled ( ) \n 
~~ except StaleElementReferenceException as expected : \n 
~~ ~~ ~~ class element_to_be_selected ( object ) : \n 
~~~ return self . element . is_selected ( ) \n 
~~ ~~ class element_located_to_be_selected ( object ) : \n 
~~~ return _find_element ( driver , self . locator ) . is_selected ( ) \n 
~~ ~~ class element_selection_state_to_be ( object ) : \n 
def __init__ ( self , element , is_selected ) : \n 
self . is_selected = is_selected \n 
~~~ return self . element . is_selected ( ) == self . is_selected \n 
~~ ~~ class element_located_selection_state_to_be ( object ) : \n 
def __init__ ( self , locator , is_selected ) : \n 
~~~ element = _find_element ( driver , self . locator ) \n 
return element . is_selected ( ) == self . is_selected \n 
~~ ~~ ~~ class alert_is_present ( object ) : \n 
~~~ alert = driver . switch_to . alert \n 
alert . text \n 
return alert \n 
~~ except NoAlertPresentException : \n 
~~ ~~ ~~ def _find_element ( driver , by ) : \n 
~~~ return driver . find_element ( * by ) \n 
~~ except NoSuchElementException as e : \n 
~~~ raise e \n 
~~ except WebDriverException as e : \n 
~~ ~~ def _find_elements ( driver , by ) : \n 
~~~ return driver . find_elements ( * by ) \n 
~~ ~~ from . base import ischema_names \n 
from ... import types as sqltypes \n 
__all__ = ( , , ) \n 
class RangeOperators ( object ) : \n 
class comparator_factory ( sqltypes . Concatenable . Comparator ) : \n 
def __ne__ ( self , other ) : \n 
return self . expr . op ( ) ( other ) \n 
~~ def contains ( self , other , ** kw ) : \n 
~~ def contained_by ( self , other ) : \n 
~~ def overlaps ( self , other ) : \n 
~~ def strictly_left_of ( self , other ) : \n 
~~ __lshift__ = strictly_left_of \n 
def strictly_right_of ( self , other ) : \n 
~~ __rshift__ = strictly_right_of \n 
def not_extend_right_of ( self , other ) : \n 
~~ def not_extend_left_of ( self , other ) : \n 
~~ def adjacent_to ( self , other ) : \n 
~~ def __add__ ( self , other ) : \n 
~~ ~~ ~~ class INT4RANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
__visit_name__ = \n 
~~ ischema_names [ ] = INT4RANGE \n 
class INT8RANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = INT8RANGE \n 
class NUMRANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = NUMRANGE \n 
class DATERANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = DATERANGE \n 
class TSRANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = TSRANGE \n 
class TSTZRANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = TSTZRANGE \n 
from . . import exc , util \n 
_key_to_collection = collections . defaultdict ( dict ) \n 
_collection_to_key = collections . defaultdict ( dict ) \n 
def _collection_gced ( ref ) : \n 
~~~ if not _collection_to_key or ref not in _collection_to_key : \n 
~~ listener_to_key = _collection_to_key . pop ( ref ) \n 
for key in listener_to_key . values ( ) : \n 
~~~ if key in _key_to_collection : \n 
~~~ dispatch_reg = _key_to_collection [ key ] \n 
dispatch_reg . pop ( ref ) \n 
if not dispatch_reg : \n 
~~~ _key_to_collection . pop ( key ) \n 
~~ ~~ ~~ ~~ def _stored_in_collection ( event_key , owner ) : \n 
~~~ key = event_key . _key \n 
dispatch_reg = _key_to_collection [ key ] \n 
owner_ref = owner . ref \n 
listen_ref = weakref . ref ( event_key . _listen_fn ) \n 
if owner_ref in dispatch_reg : \n 
~~ dispatch_reg [ owner_ref ] = listen_ref \n 
listener_to_key = _collection_to_key [ owner_ref ] \n 
listener_to_key [ listen_ref ] = key \n 
~~ def _removed_from_collection ( event_key , owner ) : \n 
dispatch_reg . pop ( owner_ref , None ) \n 
~~~ del _key_to_collection [ key ] \n 
~~ if owner_ref in _collection_to_key : \n 
~~~ listener_to_key = _collection_to_key [ owner_ref ] \n 
listener_to_key . pop ( listen_ref ) \n 
~~ ~~ def _stored_in_collection_multi ( newowner , oldowner , elements ) : \n 
~~~ if not elements : \n 
~~ oldowner = oldowner . ref \n 
newowner = newowner . ref \n 
old_listener_to_key = _collection_to_key [ oldowner ] \n 
new_listener_to_key = _collection_to_key [ newowner ] \n 
for listen_fn in elements : \n 
~~~ listen_ref = weakref . ref ( listen_fn ) \n 
key = old_listener_to_key [ listen_ref ] \n 
if newowner in dispatch_reg : \n 
~~~ assert dispatch_reg [ newowner ] == listen_ref \n 
~~~ dispatch_reg [ newowner ] = listen_ref \n 
~~ new_listener_to_key [ listen_ref ] = key \n 
~~ ~~ def _clear ( owner , elements ) : \n 
~~ owner = owner . ref \n 
listener_to_key = _collection_to_key [ owner ] \n 
key = listener_to_key [ listen_ref ] \n 
dispatch_reg . pop ( owner , None ) \n 
~~ ~~ ~~ class _EventKey ( object ) : \n 
def __init__ ( self , target , identifier , \n 
fn , dispatch_target , _fn_wrap = None ) : \n 
~~~ self . target = target \n 
self . identifier = identifier \n 
self . fn = fn \n 
if isinstance ( fn , types . MethodType ) : \n 
~~~ self . fn_key = id ( fn . __func__ ) , id ( fn . __self__ ) \n 
~~~ self . fn_key = id ( fn ) \n 
~~ self . fn_wrap = _fn_wrap \n 
self . dispatch_target = dispatch_target \n 
def _key ( self ) : \n 
~~~ return ( id ( self . target ) , self . identifier , self . fn_key ) \n 
~~ def with_wrapper ( self , fn_wrap ) : \n 
~~~ if fn_wrap is self . _listen_fn : \n 
~~~ return _EventKey ( \n 
self . target , \n 
self . identifier , \n 
self . fn , \n 
self . dispatch_target , \n 
_fn_wrap = fn_wrap \n 
~~ ~~ def with_dispatch_target ( self , dispatch_target ) : \n 
~~~ if dispatch_target is self . dispatch_target : \n 
dispatch_target , \n 
_fn_wrap = self . fn_wrap \n 
~~ ~~ def listen ( self , * args , ** kw ) : \n 
~~~ once = kw . pop ( "once" , False ) \n 
named = kw . pop ( "named" , False ) \n 
target , identifier , fn = self . dispatch_target , self . identifier , self . _listen_fn \n 
dispatch_descriptor = getattr ( target . dispatch , identifier ) \n 
adjusted_fn = dispatch_descriptor . _adjust_fn_spec ( fn , named ) \n 
self = self . with_wrapper ( adjusted_fn ) \n 
if once : \n 
~~~ self . with_wrapper ( \n 
util . only_once ( self . _listen_fn ) ) . listen ( * args , ** kw ) \n 
~~~ self . dispatch_target . dispatch . _listen ( self , * args , ** kw ) \n 
~~ ~~ def remove ( self ) : \n 
~~~ key = self . _key \n 
if key not in _key_to_collection : \n 
~~~ raise exc . InvalidRequestError ( \n 
( self . target , self . identifier , self . fn ) \n 
~~ dispatch_reg = _key_to_collection . pop ( key ) \n 
for collection_ref , listener_ref in dispatch_reg . items ( ) : \n 
~~~ collection = collection_ref ( ) \n 
listener_fn = listener_ref ( ) \n 
if collection is not None and listener_fn is not None : \n 
~~~ collection . remove ( self . with_wrapper ( listener_fn ) ) \n 
~~ ~~ ~~ def contains ( self ) : \n 
return self . _key in _key_to_collection \n 
~~ def base_listen ( self , propagate = False , insert = False , \n 
named = False ) : \n 
~~~ target , identifier , fn = self . dispatch_target , self . identifier , self . _listen_fn \n 
if insert : \n 
~~~ dispatch_descriptor . for_modify ( target . dispatch ) . insert ( self , propagate ) \n 
~~~ dispatch_descriptor . for_modify ( target . dispatch ) . append ( self , propagate ) \n 
def _listen_fn ( self ) : \n 
~~~ return self . fn_wrap or self . fn \n 
~~ def append_to_list ( self , owner , list_ ) : \n 
~~~ if _stored_in_collection ( self , owner ) : \n 
~~~ list_ . append ( self . _listen_fn ) \n 
~~ ~~ def remove_from_list ( self , owner , list_ ) : \n 
~~~ _removed_from_collection ( self , owner ) \n 
list_ . remove ( self . _listen_fn ) \n 
~~ def prepend_to_list ( self , owner , list_ ) : \n 
~~~ list_ . insert ( 0 , self . _listen_fn ) \n 
~~ ~~ ~~ import operator \n 
from . . sql import operators \n 
from . . import util \n 
class UnevaluatableError ( Exception ) : \n 
~~ _straight_ops = set ( getattr ( operators , op ) \n 
for op in ( , , , \n 
, , , , , ) ) \n 
_notimplemented_ops = set ( getattr ( operators , op ) \n 
, , ) ) \n 
class EvaluatorCompiler ( object ) : \n 
~~~ def __init__ ( self , target_cls = None ) : \n 
~~~ self . target_cls = target_cls \n 
~~ def process ( self , clause ) : \n 
~~~ meth = getattr ( self , "visit_%s" % clause . __visit_name__ , None ) \n 
if not meth : \n 
~~~ raise UnevaluatableError ( \n 
~~ return meth ( clause ) \n 
~~ def visit_grouping ( self , clause ) : \n 
~~~ return self . process ( clause . element ) \n 
~~ def visit_null ( self , clause ) : \n 
~~~ return lambda obj : None \n 
~~ def visit_false ( self , clause ) : \n 
~~~ return lambda obj : False \n 
~~ def visit_true ( self , clause ) : \n 
~~~ return lambda obj : True \n 
~~ def visit_column ( self , clause ) : \n 
~~~ if in clause . _annotations : \n 
~~~ parentmapper = clause . _annotations [ ] \n 
if self . target_cls and not issubclass ( \n 
self . target_cls , parentmapper . class_ ) : \n 
~~~ util . warn ( \n 
~~ key = parentmapper . _columntoproperty [ clause ] . key \n 
~~~ key = clause . key \n 
~~ get_corresponding_attr = operator . attrgetter ( key ) \n 
return lambda obj : get_corresponding_attr ( obj ) \n 
~~ def visit_clauselist ( self , clause ) : \n 
~~~ evaluators = list ( map ( self . process , clause . clauses ) ) \n 
if clause . operator is operators . or_ : \n 
~~~ def evaluate ( obj ) : \n 
~~~ has_null = False \n 
for sub_evaluate in evaluators : \n 
~~~ value = sub_evaluate ( obj ) \n 
if value : \n 
~~ has_null = has_null or value is None \n 
~~ if has_null : \n 
~~ ~~ elif clause . operator is operators . and_ : \n 
~~~ for sub_evaluate in evaluators : \n 
if not value : \n 
~~ ~~ return True \n 
clause . operator ) \n 
~~ return evaluate \n 
~~ def visit_binary ( self , clause ) : \n 
~~~ eval_left , eval_right = list ( map ( self . process , \n 
[ clause . left , clause . right ] ) ) \n 
operator = clause . operator \n 
if operator is operators . is_ : \n 
~~~ return eval_left ( obj ) == eval_right ( obj ) \n 
~~ ~~ elif operator is operators . isnot : \n 
~~~ return eval_left ( obj ) != eval_right ( obj ) \n 
~~ ~~ elif operator in _straight_ops : \n 
~~~ left_val = eval_left ( obj ) \n 
right_val = eval_right ( obj ) \n 
if left_val is None or right_val is None : \n 
~~ return operator ( eval_left ( obj ) , eval_right ( obj ) ) \n 
( type ( clause ) . __name__ , clause . operator ) ) \n 
~~ def visit_unary ( self , clause ) : \n 
~~~ eval_inner = self . process ( clause . element ) \n 
if clause . operator is operators . inv : \n 
~~~ value = eval_inner ( obj ) \n 
if value is None : \n 
~~ return not value \n 
~~ raise UnevaluatableError ( \n 
~~ def visit_bindparam ( self , clause ) : \n 
~~~ val = clause . value \n 
return lambda obj : val \n 
from . elements import ClauseElement \n 
from . visitors import traverse \n 
from . base import Executable , _generative , SchemaVisitor , _bind_or_error \n 
from . . util import topological \n 
from . . import event \n 
from . . import exc \n 
class _DDLCompiles ( ClauseElement ) : \n 
~~~ def _compiler ( self , dialect , ** kw ) : \n 
return dialect . ddl_compiler ( dialect , self , ** kw ) \n 
~~ ~~ class DDLElement ( Executable , _DDLCompiles ) : \n 
_execution_options = Executable . _execution_options . union ( { : True } ) \n 
target = None \n 
on = None \n 
dialect = None \n 
callable_ = None \n 
def _execute_on_connection ( self , connection , multiparams , params ) : \n 
~~~ return connection . _execute_ddl ( self , multiparams , params ) \n 
~~ def execute ( self , bind = None , target = None ) : \n 
if bind is None : \n 
~~~ bind = _bind_or_error ( self ) \n 
~~ if self . _should_execute ( target , bind ) : \n 
~~~ return bind . execute ( self . against ( target ) ) \n 
~~~ bind . engine . logger . info ( \n 
":meth:`.DDLElement.execute_if`." ) \n 
def execute_at ( self , event_name , target ) : \n 
def call_event ( target , connection , ** kw ) : \n 
~~~ if self . _should_execute_deprecated ( event_name , \n 
target , connection , ** kw ) : \n 
~~~ return connection . execute ( self . against ( target ) ) \n 
~~ ~~ event . listen ( target , "" + event_name . replace ( , ) , call_event ) \n 
~~ @ _generative \n 
def against ( self , target ) : \n 
self . target = target \n 
def execute_if ( self , dialect = None , callable_ = None , state = None ) : \n 
self . dialect = dialect \n 
self . callable_ = callable_ \n 
self . state = state \n 
~~ def _should_execute ( self , target , bind , ** kw ) : \n 
~~~ if self . on is not None and not self . _should_execute_deprecated ( None , target , bind , ** kw ) : \n 
~~ if isinstance ( self . dialect , util . string_types ) : \n 
~~~ if self . dialect != bind . engine . name : \n 
~~ ~~ elif isinstance ( self . dialect , ( tuple , list , set ) ) : \n 
~~~ if bind . engine . name not in self . dialect : \n 
~~ ~~ if ( self . callable_ is not None and \n 
not self . callable_ ( self , target , bind , \n 
state = self . state , ** kw ) ) : \n 
~~ def _should_execute_deprecated ( self , event , target , bind , ** kw ) : \n 
~~~ if self . on is None : \n 
~~ elif isinstance ( self . on , util . string_types ) : \n 
~~~ return self . on == bind . engine . name \n 
~~ elif isinstance ( self . on , ( tuple , list , set ) ) : \n 
~~~ return bind . engine . name in self . on \n 
~~~ return self . on ( self , event , target , bind , ** kw ) \n 
~~ ~~ def __call__ ( self , target , bind , ** kw ) : \n 
if self . _should_execute ( target , bind , ** kw ) : \n 
~~ ~~ def _check_ddl_on ( self , on ) : \n 
~~~ if ( on is not None and \n 
( not isinstance ( on , util . string_types + ( tuple , list , set ) ) and \n 
not util . callable ( on ) ) ) : \n 
~~~ raise exc . ArgumentError ( \n 
~~ ~~ def bind ( self ) : \n 
~~~ if self . _bind : \n 
~~~ return self . _bind \n 
~~ ~~ def _set_bind ( self , bind ) : \n 
~~~ self . _bind = bind \n 
~~ bind = property ( bind , _set_bind ) \n 
def _generate ( self ) : \n 
~~~ s = self . __class__ . __new__ ( self . __class__ ) \n 
s . __dict__ = self . __dict__ . copy ( ) \n 
~~ ~~ class DDL ( DDLElement ) : \n 
__visit_name__ = "ddl" \n 
def __init__ ( self , statement , on = None , context = None , bind = None ) : \n 
if not isinstance ( statement , util . string_types ) : \n 
statement ) \n 
~~ self . statement = statement \n 
self . context = context or { } \n 
self . _check_ddl_on ( on ) \n 
self . on = on \n 
self . _bind = bind \n 
~~~ return % ( \n 
type ( self ) . __name__ , id ( self ) , \n 
. join ( [ repr ( self . statement ) ] + \n 
[ % ( key , getattr ( self , key ) ) \n 
for key in ( , ) \n 
if getattr ( self , key ) ] ) ) \n 
~~ ~~ class _CreateDropBase ( DDLElement ) : \n 
def __init__ ( self , element , on = None , bind = None ) : \n 
self . bind = bind \n 
~~ def _create_rule_disable ( self , compiler ) : \n 
~~ ~~ class CreateSchema ( _CreateDropBase ) : \n 
__visit_name__ = "create_schema" \n 
def __init__ ( self , name , quote = None , ** kw ) : \n 
self . quote = quote \n 
super ( CreateSchema , self ) . __init__ ( name , ** kw ) \n 
~~ ~~ class DropSchema ( _CreateDropBase ) : \n 
__visit_name__ = "drop_schema" \n 
def __init__ ( self , name , quote = None , cascade = False , ** kw ) : \n 
self . cascade = cascade \n 
super ( DropSchema , self ) . __init__ ( name , ** kw ) \n 
~~ ~~ class CreateTable ( _CreateDropBase ) : \n 
__visit_name__ = "create_table" \n 
super ( CreateTable , self ) . __init__ ( element , on = on , bind = bind ) \n 
self . columns = [ CreateColumn ( column ) \n 
for column in element . columns \n 
~~ ~~ class _DropView ( _CreateDropBase ) : \n 
__visit_name__ = "drop_view" \n 
~~ class CreateColumn ( _DDLCompiles ) : \n 
~~ ~~ class DropTable ( _CreateDropBase ) : \n 
__visit_name__ = "drop_table" \n 
~~ class CreateSequence ( _CreateDropBase ) : \n 
__visit_name__ = "create_sequence" \n 
~~ class DropSequence ( _CreateDropBase ) : \n 
__visit_name__ = "drop_sequence" \n 
~~ class CreateIndex ( _CreateDropBase ) : \n 
__visit_name__ = "create_index" \n 
~~ class DropIndex ( _CreateDropBase ) : \n 
__visit_name__ = "drop_index" \n 
~~ class AddConstraint ( _CreateDropBase ) : \n 
__visit_name__ = "add_constraint" \n 
def __init__ ( self , element , * args , ** kw ) : \n 
~~~ super ( AddConstraint , self ) . __init__ ( element , * args , ** kw ) \n 
element . _create_rule = util . portable_instancemethod ( \n 
self . _create_rule_disable ) \n 
~~ ~~ class DropConstraint ( _CreateDropBase ) : \n 
__visit_name__ = "drop_constraint" \n 
def __init__ ( self , element , cascade = False , ** kw ) : \n 
~~~ self . cascade = cascade \n 
super ( DropConstraint , self ) . __init__ ( element , ** kw ) \n 
~~ ~~ class DDLBase ( SchemaVisitor ) : \n 
~~~ def __init__ ( self , connection ) : \n 
~~~ self . connection = connection \n 
~~ ~~ class SchemaGenerator ( DDLBase ) : \n 
~~~ def __init__ ( self , dialect , connection , checkfirst = False , \n 
tables = None , ** kwargs ) : \n 
~~~ super ( SchemaGenerator , self ) . __init__ ( connection , ** kwargs ) \n 
self . checkfirst = checkfirst \n 
self . tables = tables \n 
self . preparer = dialect . identifier_preparer \n 
self . memo = { } \n 
~~ def _can_create_table ( self , table ) : \n 
~~~ self . dialect . validate_identifier ( table . name ) \n 
if table . schema : \n 
~~~ self . dialect . validate_identifier ( table . schema ) \n 
~~ return not self . checkfirst or not self . dialect . has_table ( self . connection , \n 
table . name , schema = table . schema ) \n 
~~ def _can_create_sequence ( self , sequence ) : \n 
~~~ return self . dialect . supports_sequences and ( \n 
( not self . dialect . sequences_optional or \n 
not sequence . optional ) and \n 
( \n 
not self . checkfirst or \n 
not self . dialect . has_sequence ( \n 
self . connection , \n 
sequence . name , \n 
schema = sequence . schema ) \n 
~~ def visit_metadata ( self , metadata ) : \n 
~~~ if self . tables is not None : \n 
~~~ tables = self . tables \n 
~~~ tables = list ( metadata . tables . values ( ) ) \n 
~~ collection = [ t for t in sort_tables ( tables ) \n 
if self . _can_create_table ( t ) ] \n 
seq_coll = [ s for s in metadata . _sequences . values ( ) \n 
if s . column is None and self . _can_create_sequence ( s ) ] \n 
metadata . dispatch . before_create ( metadata , self . connection , \n 
tables = collection , \n 
checkfirst = self . checkfirst , \n 
_ddl_runner = self ) \n 
for seq in seq_coll : \n 
~~~ self . traverse_single ( seq , create_ok = True ) \n 
~~ for table in collection : \n 
~~~ self . traverse_single ( table , create_ok = True ) \n 
~~ metadata . dispatch . after_create ( metadata , self . connection , \n 
~~ def visit_table ( self , table , create_ok = False ) : \n 
~~~ if not create_ok and not self . _can_create_table ( table ) : \n 
~~ table . dispatch . before_create ( table , self . connection , \n 
for column in table . columns : \n 
~~~ if column . default is not None : \n 
~~~ self . traverse_single ( column . default ) \n 
~~ ~~ self . connection . execute ( CreateTable ( table ) ) \n 
if hasattr ( table , ) : \n 
~~~ for index in table . indexes : \n 
~~~ self . traverse_single ( index ) \n 
~~ ~~ table . dispatch . after_create ( table , self . connection , \n 
~~ def visit_sequence ( self , sequence , create_ok = False ) : \n 
~~~ if not create_ok and not self . _can_create_sequence ( sequence ) : \n 
~~ self . connection . execute ( CreateSequence ( sequence ) ) \n 
~~ def visit_index ( self , index ) : \n 
~~~ self . connection . execute ( CreateIndex ( index ) ) \n 
~~ ~~ class SchemaDropper ( DDLBase ) : \n 
~~~ super ( SchemaDropper , self ) . __init__ ( connection , ** kwargs ) \n 
~~ collection = [ \n 
t \n 
for t in reversed ( sort_tables ( tables ) ) \n 
if self . _can_drop_table ( t ) \n 
seq_coll = [ \n 
s \n 
for s in metadata . _sequences . values ( ) \n 
if s . column is None and self . _can_drop_sequence ( s ) \n 
metadata . dispatch . before_drop ( \n 
metadata , self . connection , tables = collection , \n 
checkfirst = self . checkfirst , _ddl_runner = self ) \n 
for table in collection : \n 
~~~ self . traverse_single ( table , drop_ok = True ) \n 
~~ for seq in seq_coll : \n 
~~~ self . traverse_single ( seq , drop_ok = True ) \n 
~~ metadata . dispatch . after_drop ( \n 
~~ def _can_drop_table ( self , table ) : \n 
~~ return not self . checkfirst or self . dialect . has_table ( \n 
self . connection , table . name , schema = table . schema ) \n 
~~ def _can_drop_sequence ( self , sequence ) : \n 
~~~ return self . dialect . supports_sequences and ( ( not self . dialect . sequences_optional or \n 
( not self . checkfirst or \n 
self . dialect . has_sequence ( \n 
schema = sequence . schema ) ) \n 
~~~ self . connection . execute ( DropIndex ( index ) ) \n 
~~ def visit_table ( self , table , drop_ok = False ) : \n 
~~~ if not drop_ok and not self . _can_drop_table ( table ) : \n 
~~ table . dispatch . before_drop ( table , self . connection , \n 
~~ ~~ self . connection . execute ( DropTable ( table ) ) \n 
table . dispatch . after_drop ( table , self . connection , \n 
~~ def visit_sequence ( self , sequence , drop_ok = False ) : \n 
~~~ if not drop_ok and not self . _can_drop_sequence ( sequence ) : \n 
~~ self . connection . execute ( DropSequence ( sequence ) ) \n 
~~ ~~ def sort_tables ( tables , skip_fn = None , extra_dependencies = None ) : \n 
tables = list ( tables ) \n 
tuples = [ ] \n 
if extra_dependencies is not None : \n 
~~~ tuples . extend ( extra_dependencies ) \n 
~~ def visit_foreign_key ( fkey ) : \n 
~~~ if fkey . use_alter : \n 
~~ elif skip_fn and skip_fn ( fkey ) : \n 
~~ parent_table = fkey . column . table \n 
if parent_table in tables : \n 
~~~ child_table = fkey . parent . table \n 
if parent_table is not child_table : \n 
~~~ tuples . append ( ( parent_table , child_table ) ) \n 
~~ ~~ ~~ for table in tables : \n 
~~~ traverse ( table , \n 
{ : True } , \n 
{ : visit_foreign_key } ) \n 
tuples . extend ( \n 
[ parent , table ] for parent in table . _extra_dependencies \n 
~~ return list ( topological . sort ( tuples , tables ) ) \n 
~~ from __future__ import unicode_literals \n 
import warnings \n 
from sqlalchemy . orm . exc import NoResultFound \n 
class Unique ( object ) : \n 
field_flags = ( , ) \n 
def __init__ ( self , get_session , model , column , message = None ) : \n 
~~~ warnings . warn ( , DeprecationWarning ) \n 
self . get_session = get_session \n 
self . model = model \n 
self . column = column \n 
self . message = message \n 
~~ def __call__ ( self , form , field ) : \n 
~~~ obj = self . get_session ( ) . query ( self . model ) . filter ( self . column == field . data ) . one ( ) \n 
if not hasattr ( form , ) or not form . _obj == obj : \n 
~~~ if self . message is None : \n 
~~~ self . message = field . gettext ( ) \n 
~~ raise ValidationError ( self . message ) \n 
~~ ~~ except NoResultFound : \n 
~~ ~~ ~~ from pygments . lexers . web import HtmlLexer , XmlLexer , JavascriptLexer , CssLexer \n 
from pygments . lexers . agile import PythonLexer , Python3Lexer \n 
from pygments . lexer import DelegatingLexer , RegexLexer , bygroups , include , using \n 
from pygments . token import Text , Comment , Operator , Keyword , Name , String , Other \n 
from pygments . formatters . html import HtmlFormatter \n 
from pygments import highlight \n 
from mako import compat \n 
class MakoLexer ( RegexLexer ) : \n 
~~~ name = \n 
aliases = [ ] \n 
filenames = [ ] \n 
tokens = { \n 
( , \n 
bygroups ( Text , Comment . Preproc , Keyword , Other ) ) , \n 
bygroups ( Text , Comment . Preproc , using ( PythonLexer ) , Other ) ) , \n 
bygroups ( Text , Comment . Preproc , Other ) ) , \n 
( , Comment . Preproc ) , \n 
bygroups ( Comment . Preproc , Name . Builtin ) , ) , \n 
bygroups ( Comment . Preproc , Name . Builtin , Comment . Preproc ) ) , \n 
( , Comment . Preproc , ) , \n 
bygroups ( Comment . Preproc , using ( PythonLexer ) , Comment . Preproc ) ) , \n 
( , bygroups ( Other , Operator ) ) , \n 
( , Text ) , \n 
( , Name . Builtin ) , \n 
include ( ) , \n 
( r\'((?:\\w+)\\s*=)\\s*(".*?")\' , \n 
bygroups ( Name . Attribute , String ) ) , \n 
( \'".*?"\' , String , ) , \n 
( "\'.*?\'" , String , ) , \n 
( , String , ) , \n 
~~ class MakoHtmlLexer ( DelegatingLexer ) : \n 
def __init__ ( self , ** options ) : \n 
~~~ super ( MakoHtmlLexer , self ) . __init__ ( HtmlLexer , MakoLexer , \n 
** options ) \n 
~~ ~~ class MakoXmlLexer ( DelegatingLexer ) : \n 
~~~ super ( MakoXmlLexer , self ) . __init__ ( XmlLexer , MakoLexer , \n 
~~ ~~ class MakoJavascriptLexer ( DelegatingLexer ) : \n 
aliases = [ , ] \n 
~~~ super ( MakoJavascriptLexer , self ) . __init__ ( JavascriptLexer , \n 
MakoLexer , ** options ) \n 
~~ ~~ class MakoCssLexer ( DelegatingLexer ) : \n 
~~~ super ( MakoCssLexer , self ) . __init__ ( CssLexer , MakoLexer , \n 
~~ ~~ pygments_html_formatter = HtmlFormatter ( cssclass = , \n 
linenos = True ) \n 
def syntax_highlight ( filename = , language = None ) : \n 
~~~ mako_lexer = MakoLexer ( ) \n 
if compat . py3k : \n 
~~~ python_lexer = Python3Lexer ( ) \n 
~~~ python_lexer = PythonLexer ( ) \n 
~~ if filename . startswith ( ) or language == : \n 
~~~ return lambda string : highlight ( string , mako_lexer , \n 
pygments_html_formatter ) \n 
~~ return lambda string : highlight ( string , python_lexer , \n 
~~ from ... import Table , MetaData , Column \n 
from ... types import String , Unicode , UnicodeText , Integer , TypeDecorator \n 
from ... import cast \n 
from ... import util \n 
from ... sql import expression \n 
from ... ext . compiler import compiles \n 
ischema = MetaData ( ) \n 
class CoerceUnicode ( TypeDecorator ) : \n 
~~~ impl = Unicode \n 
def process_bind_param ( self , value , dialect ) : \n 
~~~ if util . py2k and isinstance ( value , util . binary_type ) : \n 
~~~ value = value . decode ( dialect . encoding ) \n 
~~ return value \n 
~~ def bind_expression ( self , bindvalue ) : \n 
~~~ return _cast_on_2005 ( bindvalue ) \n 
~~ ~~ class _cast_on_2005 ( expression . ColumnElement ) : \n 
~~~ def __init__ ( self , bindvalue ) : \n 
~~~ self . bindvalue = bindvalue \n 
~~ ~~ @ compiles ( _cast_on_2005 ) \n 
def _compile ( element , compiler , ** kw ) : \n 
~~~ from . import base \n 
if compiler . dialect . server_version_info < base . MS_2005_VERSION : \n 
~~~ return compiler . process ( element . bindvalue , ** kw ) \n 
~~~ return compiler . process ( cast ( element . bindvalue , Unicode ) , ** kw ) \n 
~~ ~~ schemata = Table ( "SCHEMATA" , ischema , \n 
Column ( "CATALOG_NAME" , CoerceUnicode , key = "catalog_name" ) , \n 
Column ( "SCHEMA_NAME" , CoerceUnicode , key = "schema_name" ) , \n 
Column ( "SCHEMA_OWNER" , CoerceUnicode , key = "schema_owner" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
tables = Table ( "TABLES" , ischema , \n 
Column ( "TABLE_CATALOG" , CoerceUnicode , key = "table_catalog" ) , \n 
Column ( "TABLE_SCHEMA" , CoerceUnicode , key = "table_schema" ) , \n 
Column ( "TABLE_NAME" , CoerceUnicode , key = "table_name" ) , \n 
Column ( "TABLE_TYPE" , String ( convert_unicode = True ) , key = "table_type" ) , \n 
columns = Table ( "COLUMNS" , ischema , \n 
Column ( "COLUMN_NAME" , CoerceUnicode , key = "column_name" ) , \n 
Column ( "IS_NULLABLE" , Integer , key = "is_nullable" ) , \n 
Column ( "DATA_TYPE" , String , key = "data_type" ) , \n 
Column ( "ORDINAL_POSITION" , Integer , key = "ordinal_position" ) , \n 
Column ( "CHARACTER_MAXIMUM_LENGTH" , Integer , key = "character_maximum_length" ) , \n 
Column ( "NUMERIC_PRECISION" , Integer , key = "numeric_precision" ) , \n 
Column ( "NUMERIC_SCALE" , Integer , key = "numeric_scale" ) , \n 
Column ( "COLUMN_DEFAULT" , Integer , key = "column_default" ) , \n 
Column ( "COLLATION_NAME" , String , key = "collation_name" ) , \n 
constraints = Table ( "TABLE_CONSTRAINTS" , ischema , \n 
Column ( "CONSTRAINT_NAME" , CoerceUnicode , key = "constraint_name" ) , \n 
Column ( "CONSTRAINT_TYPE" , String ( convert_unicode = True ) , key = "constraint_type" ) , \n 
column_constraints = Table ( "CONSTRAINT_COLUMN_USAGE" , ischema , \n 
key_constraints = Table ( "KEY_COLUMN_USAGE" , ischema , \n 
ref_constraints = Table ( "REFERENTIAL_CONSTRAINTS" , ischema , \n 
Column ( "CONSTRAINT_CATALOG" , CoerceUnicode , key = "constraint_catalog" ) , \n 
Column ( "CONSTRAINT_SCHEMA" , CoerceUnicode , key = "constraint_schema" ) , \n 
Column ( "UNIQUE_CONSTRAINT_CATLOG" , CoerceUnicode , \n 
key = "unique_constraint_catalog" ) , \n 
Column ( "UNIQUE_CONSTRAINT_SCHEMA" , CoerceUnicode , \n 
key = "unique_constraint_schema" ) , \n 
Column ( "UNIQUE_CONSTRAINT_NAME" , CoerceUnicode , \n 
key = "unique_constraint_name" ) , \n 
Column ( "MATCH_OPTION" , String , key = "match_option" ) , \n 
Column ( "UPDATE_RULE" , String , key = "update_rule" ) , \n 
Column ( "DELETE_RULE" , String , key = "delete_rule" ) , \n 
views = Table ( "VIEWS" , ischema , \n 
Column ( "VIEW_DEFINITION" , CoerceUnicode , key = "view_definition" ) , \n 
Column ( "CHECK_OPTION" , String , key = "check_option" ) , \n 
Column ( "IS_UPDATABLE" , String , key = "is_updatable" ) , \n 
from . import fixtures \n 
class User ( fixtures . ComparableEntity ) : \n 
~~ class Order ( fixtures . ComparableEntity ) : \n 
~~ class Dingaling ( fixtures . ComparableEntity ) : \n 
~~ class EmailUser ( User ) : \n 
~~ class Address ( fixtures . ComparableEntity ) : \n 
~~ class Child1 ( fixtures . ComparableEntity ) : \n 
~~ class Child2 ( fixtures . ComparableEntity ) : \n 
~~ class Parent ( fixtures . ComparableEntity ) : \n 
~~ class Screen ( object ) : \n 
~~~ def __init__ ( self , obj , parent = None ) : \n 
~~~ self . obj = obj \n 
self . parent = parent \n 
~~ ~~ class Foo ( object ) : \n 
~~~ def __init__ ( self , moredata ) : \n 
~~~ self . data = \n 
self . stuff = \n 
self . moredata = moredata \n 
~~ __hash__ = object . __hash__ \n 
def __eq__ ( self , other ) : \n 
~~~ return other . data == self . data and other . stuff == self . stuff and other . moredata == self . moredata \n 
~~ ~~ class Bar ( object ) : \n 
~~~ def __init__ ( self , x , y ) : \n 
~~~ self . x = x \n 
self . y = y \n 
~~~ return other . __class__ is self . __class__ and other . x == self . x and other . y == self . y \n 
~~ ~~ class OldSchool : \n 
~~ ~~ class OldSchoolWithoutCompare : \n 
~~ ~~ class BarWithoutCompare ( object ) : \n 
~~ ~~ class NotComparable ( object ) : \n 
~~~ self . data = data \n 
~~ def __hash__ ( self ) : \n 
~~~ return id ( self ) \n 
~~~ return NotImplemented \n 
~~ ~~ class BrokenComparable ( object ) : \n 
from . . exc import CircularDependencyError \n 
__all__ = [ , , ] \n 
def sort_as_subsets ( tuples , allitems ) : \n 
~~~ edges = util . defaultdict ( set ) \n 
for parent , child in tuples : \n 
~~~ edges [ child ] . add ( parent ) \n 
~~ todo = set ( allitems ) \n 
while todo : \n 
~~~ output = set ( ) \n 
for node in list ( todo ) : \n 
~~~ if not todo . intersection ( edges [ node ] ) : \n 
~~~ output . add ( node ) \n 
~~ ~~ if not output : \n 
~~~ raise CircularDependencyError ( \n 
find_cycles ( tuples , allitems ) , \n 
_gen_edges ( edges ) \n 
~~ todo . difference_update ( output ) \n 
yield output \n 
~~ ~~ def sort ( tuples , allitems ) : \n 
for set_ in sort_as_subsets ( tuples , allitems ) : \n 
~~~ for s in set_ : \n 
~~~ yield s \n 
~~ ~~ ~~ def find_cycles ( tuples , allitems ) : \n 
~~~ edges [ parent ] . add ( child ) \n 
~~ nodes_to_test = set ( edges ) \n 
output = set ( ) \n 
for node in nodes_to_test : \n 
~~~ stack = [ node ] \n 
todo = nodes_to_test . difference ( stack ) \n 
while stack : \n 
~~~ top = stack [ - 1 ] \n 
for node in edges [ top ] : \n 
~~~ if node in stack : \n 
~~~ cyc = stack [ stack . index ( node ) : ] \n 
todo . difference_update ( cyc ) \n 
output . update ( cyc ) \n 
~~ if node in todo : \n 
~~~ stack . append ( node ) \n 
todo . remove ( node ) \n 
~~~ node = stack . pop ( ) \n 
~~ ~~ ~~ return output \n 
~~ def _gen_edges ( edges ) : \n 
~~~ return set ( [ \n 
( right , left ) \n 
for left in edges \n 
for right in edges [ left ] \n 
from kombu import Exchange , Queue \n 
DEBUG = True \n 
TEMPLATE_DEBUG = DEBUG \n 
SESSION_COOKIE_SECURE = False \n 
AWS_ACCESS_KEY_ID = os . environ . get ( , ) \n 
AWS_UPLOAD_CLIENT_KEY = AWS_ACCESS_KEY_ID \n 
AWS_SECRET_ACCESS_KEY = os . environ . get ( , ) \n 
AWS_UPLOAD_CLIENT_SECRET_KEY = AWS_SECRET_ACCESS_KEY \n 
AWS_BUCKET_NAME = os . environ . get ( "AWS_BUCKET_NAME" , "be-dev-uploads" ) \n 
AWS_STORAGE_BUCKET_NAME = AWS_BUCKET_NAME \n 
DATABASES = { \n 
: "127.0.0.1" , \n 
if "test" in sys . argv or "harvest" in sys . argv : \n 
~~~ CACHES = { \n 
: "127.0.0.1:6379" , \n 
: { : 1 } , \n 
: 300 \n 
LOGGING = { \n 
~~ BROKER_URL = \n 
CELERY_RESULT_BACKEND = BROKER_URL \n 
CELERY_DEFAULT_QUEUE = \n 
CELERY_QUEUES = ( \n 
Queue ( \n 
CELERY_DEFAULT_QUEUE , \n 
Exchange ( CELERY_DEFAULT_QUEUE ) , \n 
routing_key = CELERY_DEFAULT_QUEUE \n 
REQUIRE_UNIQUE_EMAIL = False \n 
COMPRESS_ENABLED = False \n 
if "COMPRESS_ENABLED" not in locals ( ) or not COMPRESS_ENABLED : \n 
~~~ COMPRESS_PRECOMPILERS = ( ) \n 
COMPRESS_CSS_FILTERS = [ ] \n 
COMPRESS_JS_FILTERS = [ ] \n 
~~ ALLOWED_HOSTS = [ ] \n 
~~~ import imp \n 
import config . settings \n 
local_untracked_exists = imp . find_module ( \n 
, config . settings . __path__ \n 
~~ if in locals ( ) : \n 
import celery \n 
import raven \n 
from raven . contrib . celery import register_signal , register_logger_signal \n 
os . environ . setdefault ( , ) \n 
class Celery ( celery . Celery ) : \n 
~~~ def on_configure ( self ) : \n 
~~~ client = raven . Client ( settings . RAVEN_CONFIG [ ] ) \n 
register_logger_signal ( client ) \n 
register_signal ( client ) \n 
~~ ~~ ~~ app = Celery ( ) \n 
app . config_from_object ( ) \n 
app . autodiscover_tasks ( lambda : settings . SEED_CORE_APPS ) \n 
~~~ app . start ( ) \n 
from salad . steps . everything import ImportRecord , world , django_url , time , Project \n 
from lettuce import step \n 
@ step ( ) \n 
def i_visit_the_home_page ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:home" ) ) ) \n 
~~ @ step ( ) \n 
def given_i_go_to_the_jasmine_unit_tests_for_the_SEED ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:angular_js_tests" ) ) ) \n 
def then_i_should_see_that_the_tests_passed ( step ) : \n 
~~~ time . sleep ( 2 ) \n 
~~~ assert world . browser . is_element_present_by_css ( ".passingAlert.bar" ) \n 
~~~ time . sleep ( 50 ) \n 
assert len ( world . browser . find_by_css ( ".passingAlert.bar" ) ) > 0 \n 
~~ ~~ @ step ( ) \n 
def when_i_visit_the_projects_page ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:home" ) ) + "#/projects" ) \n 
def then_i_should_see_my_projects ( step ) : \n 
~~~ assert world . browser . is_text_present ( ) \n 
assert world . browser . is_text_present ( ) \n 
def and_i_have_a_project ( step ) : \n 
~~~ Project . objects . create ( \n 
super_organization_id = world . org . id , \n 
owner = world . user \n 
def and_i_have_a_dataset ( step ) : \n 
~~~ ImportRecord . objects . create ( \n 
super_organization = world . org , \n 
def when_i_visit_the_dataset_page ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:home" ) ) + "#/data" ) \n 
def and_i_delete_a_dataset ( step ) : \n 
~~~ delete_icon = world . browser . find_by_css ( ) \n 
delete_icon . click ( ) \n 
alert = world . browser . get_alert ( ) \n 
alert . accept ( ) \n 
def then_i_should_see_no_datasets ( step ) : \n 
~~~ number_of_datasets = len ( world . browser . find_by_css ( ) ) \n 
number_of_datasets = len ( world . browser . find_by_css ( ) ) \n 
assert number_of_datasets == 0 \n 
MAP = { \n 
from django . db import models , migrations \n 
( , models . ForeignKey ( primary_key = True , serialize = False , to = ] , \n 
bases = ( models . Model , ) , \n 
import pprint \n 
import csv \n 
from calendar import timegm \n 
def upload_file ( upload_header , upload_filepath , main_url , upload_dataset_id , upload_datatype ) : \n 
def _upload_file_to_aws ( aws_upload_details ) : \n 
sig_uri = aws_upload_details [ ] \n 
now = dt . datetime . utcnow ( ) \n 
expires = now + dt . timedelta ( hours = 1 ) \n 
now_ts = timegm ( now . timetuple ( ) ) \n 
key = % ( filename , now_ts ) \n 
payload = { } \n 
payload [ ] = expires . isoformat ( ) + \n 
payload [ ] = [ \n 
{ : aws_upload_details [ ] } , \n 
{ : } , \n 
{ : key } \n 
sig_result = requests . post ( main_url + sig_uri , \n 
headers = upload_header , \n 
data = json . dumps ( payload ) ) \n 
if sig_result . status_code != 200 : \n 
raise RuntimeError ( msg ) \n 
~~~ sig_result = sig_result . json ( ) \n 
~~ upload_url = "http://%s.s3.amazonaws.com/" % ( aws_upload_details [ ] ) \n 
s3_payload = [ \n 
( , key ) , \n 
( , aws_upload_details [ ] ) , \n 
( , sig_result [ ] ) , \n 
( , ( filename , open ( upload_filepath , ) ) ) \n 
result = requests . post ( upload_url , \n 
files = s3_payload ) \n 
if result . status_code != 200 : \n 
~~ completion_uri = aws_upload_details [ ] \n 
completion_payload = { \n 
: upload_dataset_id , \n 
: key , \n 
: upload_datatype \n 
return requests . get ( main_url + completion_uri , \n 
params = completion_payload ) \n 
~~ def _upload_file_to_file_system ( upload_details ) : \n 
upload_url = "%s%s" % ( main_url , upload_details [ ] ) \n 
fsysparams = { : upload_filepath , \n 
: upload_datatype } \n 
return requests . post ( upload_url , \n 
params = fsysparams , \n 
files = { : open ( upload_filepath , ) } , \n 
headers = upload_header ) \n 
~~ upload_details = requests . get ( main_url + , headers = upload_header ) \n 
upload_details = upload_details . json ( ) \n 
filename = os . path . basename ( upload_filepath ) \n 
if upload_details [ ] == : \n 
~~~ return _upload_file_to_aws ( upload_details ) \n 
~~ elif upload_details [ ] == : \n 
~~~ return _upload_file_to_file_system ( upload_details ) \n 
upload_details [ ] ) \n 
~~ ~~ def check_status ( resultOut , partmsg , log , PIIDflag = None ) : \n 
if resultOut . status_code in [ 200 , 403 , 401 ] : \n 
~~~ if PIIDflag == : \n 
~~~ msg = pprint . pformat ( resultOut . json ( ) , indent = 2 , width = 70 ) \n 
~~~ if in resultOut . json ( ) . keys ( ) and resultOut . json ( ) [ ] == : \n 
~~~ msg = resultOut . json ( ) [ ] \n 
log . error ( partmsg + ) \n 
log . debug ( msg ) \n 
raise RuntimeError \n 
~~ elif in resultOut . json ( ) . keys ( ) and not resultOut . json ( ) [ ] : \n 
~~~ msg = resultOut . json ( ) \n 
~~~ msg = + str ( len ( resultOut . json ( ) [ ~~ elif PIIDflag == : \n 
~~~ msg = + str ( len ( resultOut . json ( ) [ ] [ 0 ] ) ) \n 
~~ elif PIIDflag == : \n 
~~~ msg = pprint . pformat ( resultOut . json ( ) [ ] , indent = 2 ~~ elif PIIDflag == : \n 
~~~ log . error ( partmsg , ) \n 
log . debug ( ) \n 
~~ ~~ log . info ( partmsg + ) \n 
~~~ msg = resultOut . reason \n 
~~ def check_progress ( mainURL , Header , progress_key ) : \n 
time . sleep ( 5 ) \n 
progressResult = requests . get ( mainURL + , \n 
headers = Header , \n 
data = json . dumps ( { : progress_key } ) ) \n 
if progressResult . json ( ) [ ] == 100 : \n 
~~~ return ( progressResult ) \n 
~~~ progressResult = check_progress ( mainURL , Header , progress_key ) \n 
~~ ~~ def read_map_file ( mapfilePath ) : \n 
mapReader = csv . reader ( open ( mapfilePath , ) ) \n 
maplist = list ( ) \n 
for rowitem in mapReader : \n 
~~~ maplist . append ( rowitem ) \n 
~~ return maplist \n 
~~ def setup_logger ( filename ) : \n 
logging . getLogger ( "requests" ) . setLevel ( logging . WARNING ) \n 
logger = logging . getLogger ( ) \n 
formatter = logging . Formatter ( ) \n 
formatter_console = logging . Formatter ( ) \n 
fh = logging . FileHandler ( filename , mode = ) \n 
fh . setLevel ( logging . DEBUG ) \n 
fh . setFormatter ( formatter ) \n 
logger . addHandler ( fh ) \n 
ch = logging . StreamHandler ( ) \n 
ch . setLevel ( logging . INFO ) \n 
ch . setFormatter ( formatter_console ) \n 
logger . addHandler ( ch ) \n 
return logger \n 
from seed . utils . generic import split_model_fields \n 
class DummyClass ( object ) : \n 
field_one = "field_one" \n 
field_two = "field_two" \n 
~~ class TestGenericUtils ( TestCase ) : \n 
~~~ def test_split_model_fields ( self ) : \n 
f1 = \n 
f2 = \n 
f3 = \n 
f4 = \n 
obj = DummyClass ( ) \n 
fields_to_split = [ f1 , f2 , f3 , f4 ] \n 
obj_fields , non_obj_fields = split_model_fields ( obj , fields_to_split ) \n 
self . assertEqual ( obj_fields , [ f1 , f2 ] ) \n 
self . assertEqual ( non_obj_fields , [ f3 , f4 ] ) \n 
fields_to_split = [ f1 ] \n 
self . assertEqual ( obj_fields , [ f1 ] ) \n 
self . assertEqual ( non_obj_fields , [ ] ) \n 
fields_to_split = [ f4 ] \n 
self . assertEqual ( obj_fields , [ ] ) \n 
self . assertEqual ( non_obj_fields , [ f4 ] ) \n 
~~ ~~ from setuptools import setup , find_packages \n 
packages = find_packages ( ) , \n 
license = , \n 
author = , \n 
author_email = , \n 
description = scripts = [ ] \n 
import time as t \n 
from os . path import join as pjoin \n 
from StringIO import StringIO \n 
import tempfile \n 
from nose . tools import assert_true , assert_equal \n 
from numpy . testing import assert_array_equal \n 
import smartdispatch \n 
from smartdispatch import utils \n 
def test_generate_name_from_command ( ) : \n 
~~~ date_length = 20 \n 
expected = "_" . join ( command . split ( ) ) \n 
assert_equal ( smartdispatch . generate_name_from_command ( command ) [ date_length : ] , expected ) \n 
max_length_arg = 7 \n 
long_arg = "veryverylongarg1" \n 
expected = command . split ( ) \n 
expected [ 1 ] = long_arg [ - max_length_arg : ] \n 
expected = "_" . join ( expected ) \n 
assert_equal ( smartdispatch . generate_name_from_command ( command , max_length_arg ) [ date_length : ] , expected \n 
max_length = 23 \n 
assert_equal ( smartdispatch . generate_name_from_command ( command , max_length = max_length + date_length \n 
expected = "command_pathnumberone_pathnumbertwo" \n 
~~ def test_get_commands_from_file ( ) : \n 
"command2" , \n 
fileobj = StringIO ( "\\n" . join ( commands ) ) \n 
assert_array_equal ( smartdispatch . get_commands_from_file ( fileobj ) , commands ) \n 
fileobj = StringIO ( "\\n" . join ( commands ) + "\\n" ) \n 
~~ def test_unfold_command ( ) : \n 
~~~ cmd = "ls" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "ls" ] ) \n 
~~ def test_replace_uid_tag ( ) : \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( [ command ] ) , [ command ] ) \n 
uid = utils . generate_uid_from_string ( command ) \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( [ command ] ) , [ command . replace ( "{UID}" , uid ) ] ) \n 
uid = utils . generate_uid_from_string ( commands [ 0 ] ) \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( commands ) , [ commands [ 0 ] . replace ( "{UID}" , uid ) ] \n 
~~ def test_get_available_queues ( ) : \n 
~~~ assert_equal ( smartdispatch . get_available_queues ( cluster_name = None ) , { } ) \n 
assert_equal ( smartdispatch . get_available_queues ( cluster_name = "unknown" ) , { } ) \n 
queues_infos = smartdispatch . get_available_queues ( cluster_name = "guillimin" ) \n 
assert_true ( len ( queues_infos ) > 0 ) \n 
queues_infos = smartdispatch . get_available_queues ( cluster_name = "mammouth" ) \n 
~~ def test_get_job_folders ( ) : \n 
~~~ temp_dir = tempfile . mkdtemp ( ) \n 
jobname = "this_is_the_name_of_my_job" \n 
job_folders_paths = smartdispatch . get_job_folders ( temp_dir , jobname ) \n 
path_job , path_job_logs , path_job_commands = job_folders_paths \n 
assert_true ( jobname in path_job ) \n 
assert_true ( os . path . isdir ( path_job ) ) \n 
assert_equal ( os . path . basename ( path_job ) , jobname ) \n 
assert_true ( jobname in path_job_logs ) \n 
assert_true ( os . path . isdir ( path_job_logs ) ) \n 
assert_true ( os . path . isdir ( pjoin ( path_job_logs , ) ) ) \n 
assert_equal ( os . path . basename ( path_job_logs ) , "logs" ) \n 
assert_true ( jobname in path_job_commands ) \n 
assert_true ( os . path . isdir ( path_job_commands ) ) \n 
assert_equal ( os . path . basename ( path_job_commands ) , "commands" ) \n 
jobname += "2" \n 
os . rename ( path_job , path_job + "2" ) \n 
shutil . rmtree ( temp_dir ) \n 
~~ def test_log_command_line ( ) : \n 
command_line_log_file = pjoin ( temp_dir , "command_line.log" ) \n 
smartdispatch . log_command_line ( temp_dir , command_1 ) \n 
assert_true ( os . path . isfile ( command_line_log_file ) ) \n 
lines = open ( command_line_log_file ) . read ( ) . strip ( ) . split ( "\\n" ) \n 
assert_equal ( lines [ 1 ] , command_1 ) \n 
smartdispatch . log_command_line ( temp_dir , command_2 ) \n 
assert_equal ( len ( lines ) , 5 ) \n 
assert_equal ( lines [ 4 ] , command_2 . replace ( \'"\' , r\'\\"\' ) ) \n 
smartdispatch . log_command_line ( temp_dir , command_3 ) \n 
assert_equal ( len ( lines ) , 8 ) \n 
assert_equal ( lines [ 7 ] , re . sub ( , r\'"\\1\\2\\3"\' , command_3 ) ) \n 
~~ from . sgd import SGD \n 
from . adagrad import AdaGrad \n 
from . adam import Adam \n 
from . rmsprop import RMSProp \n 
from . adadelta import Adadelta \n 
from zope . interface import Attribute \n 
from zope . interface import Interface \n 
from zope . interface import implements \n 
class IIndexEvent ( Interface ) : \n 
~~ class IIndexUpdate ( Interface ) : \n 
~~ class IPackageEvent ( IIndexEvent ) : \n 
path = Attribute ( ) \n 
~~ class IPackageAdded ( IPackageEvent ) : \n 
~~ class IPackageRemoved ( IPackageEvent ) : \n 
~~ class IndexEvent ( object ) : \n 
~~~ implements ( IIndexEvent ) \n 
def __init__ ( self , datafile , index ) : \n 
self . datafile = datafile \n 
~~ ~~ class IndexUpdate ( IndexEvent ) : \n 
~~~ implements ( IIndexUpdate ) \n 
~~ class PackageEvent ( object ) : \n 
implements ( IPackageEvent ) \n 
def __init__ ( self , index_manager , path = None , name = None , version = None ) : \n 
~~~ self . name = name \n 
self . version = version \n 
self . im = index_manager \n 
if self . name is None and self . path : \n 
~~~ info = self . im . pkginfo_from_file ( path , self . im . move_on_error ) \n 
self . name = info . name \n 
self . version = info . version \n 
~~ ~~ ~~ class PackageAdded ( PackageEvent ) : \n 
~~~ implements ( IPackageAdded ) \n 
~~ class PackageRemoved ( PackageEvent ) : \n 
~~~ implements ( IPackageRemoved ) \n 
import simplejson as json \n 
import codecs \n 
class Filter ( ) : \n 
~~~ FILTERED_RESOURCES = { } \n 
FILTERED_EXTENSIONS = [ ] \n 
def __init__ ( self , json_repo_path , json_filtered_repo_path , filtered_resources_path , filtered_extensions_path ~~~ self . JSON_REPO_PATH = json_repo_path \n 
self . JSON_REPO_FILTERED_PATH = json_filtered_repo_path \n 
self . FILTERED_RESOURCES_PATH = filtered_resources_path \n 
self . FILTERED_EXTENSIONS_PATH = filtered_extensions_path \n 
self . type = type \n 
self . logger = logger \n 
~~ def get_filtered_files_to_dict ( self ) : \n 
~~~ filtered_file = codecs . open ( self . FILTERED_RESOURCES_PATH , , ) \n 
for line in filtered_file : \n 
~~~ splitted_line = line . split ( ) \n 
ref = splitted_line [ 0 ] . strip ( ) \n 
dir = splitted_line [ 1 ] . strip ( ) \n 
file = splitted_line [ 2 ] . strip ( ) \n 
if Filter . FILTERED_RESOURCES . get ( ref ) : \n 
~~~ dir2file_dict = Filter . FILTERED_RESOURCES . get ( ref ) \n 
if dir == "*" : \n 
~~~ dir2file_dict . update ( { : } ) \n 
Filter . FILTERED_RESOURCES . update ( { ref : dir2file_dict } ) \n 
~~ elif dir2file_dict . get ( dir ) : \n 
~~~ files = dir2file_dict . get ( dir ) \n 
if file == "*" : \n 
~~~ dir2file_dict . update ( { dir : [ ] } ) \n 
~~~ files . append ( file ) \n 
dir2file_dict . update ( { dir : files } ) \n 
~~~ if file == "*" : \n 
~~~ dir2file_dict . update ( { dir : [ file ] } ) \n 
~~~ if dir == "*" : \n 
~~~ Filter . FILTERED_RESOURCES . update ( { ref : { : } } ) \n 
~~~ Filter . FILTERED_RESOURCES . update ( { ref : { dir : [ ] } } ) \n 
~~~ Filter . FILTERED_RESOURCES . update ( { ref : { dir : [ file ] } } ) \n 
~~ ~~ ~~ ~~ filtered_file . close ( ) \n 
~~ def get_filtered_extensions_to_list ( self ) : \n 
~~~ file = codecs . open ( self . FILTERED_EXTENSIONS_PATH , , ) \n 
for line in file : \n 
~~~ ext = line . strip ( ) \n 
Filter . FILTERED_EXTENSIONS . append ( ext ) \n 
~~ file . close ( ) \n 
~~ def is_filtered ( self , ext , ref , dir , file ) : \n 
~~~ found = False \n 
if ext in Filter . FILTERED_EXTENSIONS : \n 
~~~ found = True \n 
~~~ if Filter . FILTERED_RESOURCES . get ( ref ) : \n 
~~~ filtered_dirs_in_ref = Filter . FILTERED_RESOURCES . get ( ref ) \n 
if filtered_dirs_in_ref . get ( ) : \n 
~~~ if filtered_dirs_in_ref . get ( dir ) : \n 
~~~ filtered_files_in_dir = filtered_dirs_in_ref . get ( dir ) \n 
if in filtered_files_in_dir : \n 
~~~ if file in filtered_files_in_dir : \n 
~~ ~~ ~~ ~~ ~~ ~~ return found \n 
~~ def get_dirs ( self , dirs ) : \n 
~~~ if not dirs : \n 
~~~ dirs . append ( ) \n 
~~ return dirs \n 
~~ def select_files ( self ) : \n 
~~~ repo_json = codecs . open ( self . JSON_REPO_PATH , , ) \n 
filtered_repo_json = codecs . open ( self . JSON_REPO_FILTERED_PATH , , ) \n 
for json_line in repo_json : \n 
~~~ json_entry = json . loads ( json_line ) \n 
ref = json_entry . get ( ) \n 
dirs = self . get_dirs ( json_entry . get ( ) ) \n 
ext = json_entry . get ( ) \n 
file = json_entry . get ( ) \n 
filtered = True \n 
for dir in dirs : \n 
~~~ if self . is_filtered ( ext , ref , dir , file ) : \n 
~~~ filtered = False \n 
~~ ~~ if not filtered : \n 
~~~ filtered_repo_json . write ( json . dumps ( json_entry ) + ) \n 
~~ ~~ repo_json . close ( ) \n 
filtered_repo_json . close ( ) \n 
~~ def reject_files ( self ) : \n 
filtered = False \n 
~~~ filtered = True \n 
~~ def filter ( self ) : \n 
~~~ start_time = datetime . now ( ) \n 
if self . FILTERED_EXTENSIONS_PATH : \n 
~~~ self . get_filtered_extensions_to_list ( ) \n 
~~ if self . FILTERED_RESOURCES_PATH : \n 
~~~ self . get_filtered_files_to_dict ( ) \n 
~~ if self . type == "in" : \n 
~~~ self . select_files ( ) \n 
~~ elif self . type == "out" : \n 
~~~ self . reject_files ( ) \n 
~~ end_time = datetime . now ( ) \n 
minutes_and_seconds = divmod ( ( end_time - start_time ) . total_seconds ( ) , 60 ) \n 
from cybox . common import Hash \n 
from cybox . objects . file_object import File \n 
from stix . core import STIXPackage , STIXHeader \n 
~~~ shv = Hash ( ) \n 
shv . simple_hash_value = "4EC0027BEF4D7E1786A04D021FA8A67F" \n 
f = File ( ) \n 
h = Hash ( shv , Hash . TYPE_MD5 ) \n 
f . add_hash ( h ) \n 
stix_package = STIXPackage ( ) \n 
stix_header = STIXHeader ( ) \n 
stix_package . stix_header = stix_header \n 
stix_package . add ( f ) \n 
print ( stix_package . to_xml ( ) ) \n 
from mixbox . binding_utils import * \n 
from stix . bindings import register_extension \n 
import stix . bindings . exploit_target as exploit_target_binding \n 
XML_NS = "http://stix.mitre.org/extensions/Vulnerability#CVRF-1" \n 
@ register_extension \n 
class CVRF1_1InstanceType ( exploit_target_binding . VulnerabilityType ) : \n 
subclass = None \n 
superclass = exploit_target_binding . VulnerabilityType \n 
xmlns = XML_NS \n 
xmlns_prefix = "cvrfVuln" \n 
xml_type = "CVRF1.1InstanceType" \n 
def __init__ ( self , Description = None , CVE_ID = None , OSVDB_ID = None , CVSS_Score = None , cvrfdoc = None ) : ~~~ super ( CVRF1_1InstanceType , self ) . __init__ ( Description = Description , CVE_ID = CVE_ID , OSVDB_ID = OSVDB_ID self . cvrfdoc = cvrfdoc \n 
~~ def factory ( * args_ , ** kwargs_ ) : \n 
~~~ if CVRF1_1InstanceType . subclass : \n 
~~~ return CVRF1_1InstanceType . subclass ( * args_ , ** kwargs_ ) \n 
~~~ return CVRF1_1InstanceType ( * args_ , ** kwargs_ ) \n 
~~ ~~ factory = staticmethod ( factory ) \n 
def get_cvrfdoc ( self ) : return self . cvrfdoc \n 
def set_cvrfdoc ( self , cvrfdoc ) : self . cvrfdoc = cvrfdoc \n 
def hasContent_ ( self ) : \n 
~~~ if ( \n 
self . cvrfdoc is not None or \n 
super ( CVRF1_1InstanceType , self ) . hasContent_ ( ) \n 
~~ ~~ def export ( self , lwrite , level , nsmap , namespace_ = XML_NS , name_ = , namespacedef_ ~~~ if pretty_print : \n 
~~~ eol_ = \n 
~~ showIndent ( lwrite , level , pretty_print ) \n 
lwrite ( % ( nsmap [ namespace_ ] , name_ , namespacedef_ and + namespacedef_ or , already_processed = set ( ) \n 
self . exportAttributes ( lwrite , level , already_processed , namespace_ , name_ = if self . hasContent_ ( ) : \n 
~~~ lwrite ( % ( eol_ , ) ) \n 
self . exportChildren ( lwrite , level + 1 , nsmap , XML_NS , name_ , pretty_print = pretty_print ) \n 
showIndent ( lwrite , level , pretty_print ) \n 
lwrite ( % ( nsmap [ namespace_ ] , name_ , eol_ ) ) \n 
~~ ~~ def exportAttributes ( self , lwrite , level , already_processed , namespace_ = , name_ = ~~~ super ( CVRF1_1InstanceType , self ) . exportAttributes ( lwrite , level , already_processed , namespace_ if not in already_processed : \n 
~~~ already_processed . add ( ) \n 
lwrite ( xmlns ) \n 
~~ if not in already_processed : \n 
lwrite ( xsi_type ) \n 
~~ ~~ def exportChildren ( self , lwrite , level , nsmap , namespace_ = XML_NS , name_ = , fromsubclass_ ~~~ super ( CVRF1_1InstanceType , self ) . exportChildren ( lwrite , level , nsmap , namespace_ , name_ , True if pretty_print : \n 
~~ if self . cvrfdoc is not None : \n 
~~~ showIndent ( lwrite , level , pretty_print ) \n 
lwrite ( etree_ . tostring ( self . cvrfdoc , pretty_print = pretty_print ) ) \n 
~~ ~~ def build ( self , node ) : \n 
~~~ already_processed = set ( ) \n 
self . buildAttributes ( node , node . attrib , already_processed ) \n 
for child in node : \n 
~~~ nodeName_ = Tag_pattern_ . match ( child . tag ) . groups ( ) [ - 1 ] \n 
self . buildChildren ( child , node , nodeName_ ) \n 
~~ ~~ def buildAttributes ( self , node , attrs , already_processed ) : \n 
~~~ super ( CVRF1_1InstanceType , self ) . buildAttributes ( node , attrs , already_processed ) \n 
~~ def buildChildren ( self , child_ , node , nodeName_ , fromsubclass_ = False ) : \n 
~~~ if nodeName_ == : \n 
~~~ self . set_cvrfdoc ( child_ ) \n 
~~ super ( CVRF1_1InstanceType , self ) . buildChildren ( child_ , node , nodeName_ , True ) \n 
~~ ~~ GDSClassesMapping = { } \n 
def usage ( ) : \n 
~~~ print USAGE_TEXT \n 
~~ def get_root_tag ( node ) : \n 
~~~ tag = Tag_pattern_ . match ( node . tag ) . groups ( ) [ - 1 ] \n 
rootClass = GDSClassesMapping . get ( tag ) \n 
if rootClass is None : \n 
~~~ rootClass = globals ( ) . get ( tag ) \n 
~~ return tag , rootClass \n 
~~ def parse ( inFileName ) : \n 
~~~ doc = parsexml_ ( inFileName ) \n 
rootNode = doc . getroot ( ) \n 
rootTag , rootClass = get_root_tag ( rootNode ) \n 
~~~ rootTag = \n 
rootClass = CVRF1_1InstanceType \n 
~~ rootObj = rootClass . factory ( ) \n 
rootObj . build ( rootNode ) \n 
doc = None \n 
rootObj . export ( sys . stdout , 0 , name_ = rootTag , \n 
namespacedef_ = , \n 
pretty_print = True ) \n 
return rootObj \n 
~~ def parseEtree ( inFileName ) : \n 
rootElement = rootObj . to_etree ( None , name_ = rootTag ) \n 
content = etree_ . tostring ( rootElement , pretty_print = True , \n 
xml_declaration = True , encoding = "utf-8" ) \n 
sys . stdout . write ( content ) \n 
sys . stdout . write ( ) \n 
return rootObj , rootElement \n 
~~ def parseString ( inString ) : \n 
~~~ from StringIO import StringIO \n 
doc = parsexml_ ( StringIO ( inString ) ) \n 
rootObj . export ( sys . stdout , 0 , name_ = "CVRF1.1InstanceType" , \n 
namespacedef_ = ) \n 
~~~ args = sys . argv [ 1 : ] \n 
if len ( args ) == 1 : \n 
~~~ parse ( args [ 0 ] ) \n 
~~ __all__ = [ \n 
"CVRF1_1InstanceType" \n 
import stix \n 
from stix . utils . deprecated import idref_deprecated \n 
from stix . campaign import Campaign \n 
from stix . coa import CourseOfAction \n 
from stix . exploit_target import ExploitTarget \n 
from stix . indicator import Indicator \n 
from stix . incident import Incident \n 
from stix . report import Report \n 
from stix . threat_actor import ThreatActor \n 
from stix . bindings import stix_core as stix_core_binding \n 
from stix . bindings import stix_common as stix_common_binding \n 
class Campaigns ( stix . EntityList ) : \n 
~~~ _binding = stix_core_binding \n 
_namespace = \n 
_binding_class = _binding . CampaignsType \n 
_contained_type = Campaign \n 
_binding_var = "Campaign" \n 
_inner_name = "campaigns" \n 
_dict_as_list = True \n 
def _is_valid ( self , value ) : \n 
~~~ idref_deprecated ( value ) \n 
return stix . EntityList . _is_valid ( self , value ) \n 
~~ ~~ class CoursesOfAction ( stix . EntityList ) : \n 
_binding_class = _binding . CoursesOfActionType \n 
_contained_type = CourseOfAction \n 
_binding_var = "Course_Of_Action" \n 
_inner_name = "courses_of_action" \n 
~~ ~~ class ExploitTargets ( stix . EntityList ) : \n 
~~~ _binding = stix_common_binding \n 
_binding_class = _binding . ExploitTargetsType \n 
_contained_type = ExploitTarget \n 
_binding_var = "Exploit_Target" \n 
_inner_name = "exploit_targets" \n 
~~ ~~ class Incidents ( stix . EntityList ) : \n 
_binding_class = _binding . IncidentsType \n 
_contained_type = Incident \n 
_binding_var = "Incident" \n 
_inner_name = "incidents" \n 
~~ ~~ class Indicators ( stix . EntityList ) : \n 
_binding_class = _binding . IndicatorsType \n 
_contained_type = Indicator \n 
_binding_var = "Indicator" \n 
_inner_name = "indicators" \n 
~~ ~~ class ThreatActors ( stix . EntityList ) : \n 
_binding_class = _binding . ThreatActorsType \n 
_contained_type = ThreatActor \n 
_binding_var = "Threat_Actor" \n 
_inner_name = "threat_actors" \n 
~~ ~~ class Reports ( stix . EntityList ) : \n 
_binding_class = _binding . ReportsType \n 
_contained_type = Report \n 
_binding_var = "Report" \n 
_inner_name = "reports" \n 
from cybox . common import Contributor \n 
import stix . utils \n 
import stix . bindings . incident as incident_binding \n 
class Contributors ( stix . EntityList ) : \n 
~~~ _namespace = "http://stix.mitre.org/Incident-1" \n 
_binding = incident_binding \n 
_binding_class = _binding . ContributorsType \n 
_contained_type = Contributor \n 
_binding_var = "Contributor" \n 
_inner_name = "contributors" \n 
from stix . test import EntityTestCase \n 
from stix . test . common import structured_text_tests \n 
from stix . common import InformationSource \n 
class InformationSourceTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = InformationSource \n 
_full_dict = { \n 
: "Spiderman" , \n 
: "Batman" , \n 
: "Superman" , \n 
: "2010-11-12T01:02:03" , \n 
: "2013-12-11T03:02:01" , \n 
: "Web" , \n 
: "Superwebs" , \n 
: "Tubes" , \n 
: "Supertubes" , \n 
~~ class InformationSourceMultiDescTests ( EntityTestCase , unittest . TestCase ) : \n 
: structured_text_tests . StructuredTextListTests . _full_dict \n 
from stix . test import EntityTestCase , assert_warnings \n 
from stix . test import data_marking_test \n 
from stix . test . common import related_test , identity_test , kill_chains_test \n 
from stix . core import STIXPackage \n 
import stix . ttp as ttp \n 
from stix . ttp import ( \n 
resource , infrastructure , exploit_targets , malware_instance , exploit , \n 
attack_pattern , behavior \n 
class ExploitTargetsTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = exploit_targets . ExploitTargets \n 
related_test . RelatedExploitTargetTests . _full_dict \n 
~~ class PersonasTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = resource . Personas \n 
_full_dict = [ \n 
identity_test . IdentityTests . _full_dict \n 
~~ class InfrastructureTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = infrastructure . Infrastructure \n 
: [ , ] , \n 
: 2 , \n 
: "example:Observable-1" \n 
~~ class ResourcesTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = ttp . Resource \n 
: PersonasTests . _full_dict , \n 
: "Tool" \n 
: InfrastructureTests . _full_dict \n 
~~ class MalwareInstanceTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = malware_instance . MalwareInstance \n 
_full_dict = _full_dict = { \n 
: [ , ] \n 
~~ class MalwareInstancesTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . MalwareInstances \n 
MalwareInstanceTests . _full_dict \n 
~~ class ExploitTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = exploit . Exploit \n 
~~ class ExploitsTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . Exploits \n 
ExploitTests . _full_dict \n 
~~ class AttackPatternTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = attack_pattern . AttackPattern \n 
def idref_test ( self ) : \n 
~~~ ap = attack_pattern . AttackPattern ( ) \n 
ap . id_ = \n 
self . assertEqual ( ap . id_ , ) \n 
ap . idref = \n 
self . assertEqual ( ap . idref , ) \n 
self . assertEqual ( ap . id_ , None ) \n 
~~ ~~ class AttackPatternsTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . AttackPatterns \n 
AttackPatternTests . _full_dict \n 
~~ class BehaviorTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . Behavior \n 
: MalwareInstancesTests . _full_dict , \n 
: ExploitsTests . _full_dict , \n 
: AttackPatternsTests . _full_dict \n 
~~ class TTPTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = ttp . TTP \n 
: "TTP1" , \n 
: ResourcesTests . _full_dict , \n 
: data_marking_test . MarkingTests . _full_dict , \n 
: ExploitTargetsTests . _full_dict , \n 
: BehaviorTests . _full_dict , \n 
: related_test . RelatedPackageRefsTests . _full_dict , \n 
: kill_chains_test . KillChainPhasesReferenceTests . _full_dict \n 
def test_add_description ( self ) : \n 
~~~ o1 = self . klass ( ) \n 
o2 = self . klass ( ) \n 
o1 . add_description ( "Test" ) \n 
o2 . descriptions . add ( "Test" ) \n 
o1 . descriptions . to_dict ( ) , \n 
o2 . descriptions . to_dict ( ) \n 
~~ def test_add_short_description ( self ) : \n 
o1 . add_short_description ( "Test" ) \n 
o2 . short_descriptions . add ( "Test" ) \n 
o1 . short_descriptions . to_dict ( ) , \n 
o2 . short_descriptions . to_dict ( ) \n 
~~ @ assert_warnings \n 
def test_deprecated_related_packages ( self ) : \n 
~~~ t = ttp . TTP ( ) \n 
t . related_packages . append ( STIXPackage ( ) ) \n 
self . assertEqual ( len ( t . related_packages ) , 1 ) \n 
~~ class AzureError ( Exception ) : \n 
def __init__ ( self , message ) : \n 
~~~ self . message = message \n 
~~~ return repr ( self . message ) \n 
~~ ~~ class AzureAccountConfigurationError ( AzureError ) : \n 
~~ class AzureAccountDefaultSectionNotFound ( AzureError ) : \n 
~~ class AzureAccountLoadFailed ( AzureError ) : \n 
~~ class AzureBlobServicePropertyError ( AzureError ) : \n 
~~ class AzureCannotInit ( AzureError ) : \n 
~~ class AzureCloudServiceAddCertificateError ( AzureError ) : \n 
~~ class AzureCloudServiceAddressError ( AzureError ) : \n 
~~ class AzureCloudServiceCreateError ( AzureError ) : \n 
~~ class AzureCloudServiceDeleteError ( AzureError ) : \n 
~~ class AzureCloudServiceOpenSSLError ( AzureError ) : \n 
~~ class AzureCommandNotLoaded ( AzureError ) : \n 
~~ class AzureConfigDefaultLinkError ( AzureError ) : \n 
~~ class AzureConfigAccountFileNotFound ( AzureError ) : \n 
~~ class AzureConfigAccountNotFound ( AzureError ) : \n 
~~ class AzureConfigAddAccountSectionError ( AzureError ) : \n 
~~ class AzureConfigAddRegionSectionError ( AzureError ) : \n 
~~ class AzureConfigParseError ( AzureError ) : \n 
~~ class AzureConfigPublishSettingsError ( AzureError ) : \n 
~~ class AzureConfigRegionNotFound ( AzureError ) : \n 
~~ class AzureConfigSectionNotFound ( AzureError ) : \n 
~~ class AzureConfigVariableNotFound ( AzureError ) : \n 
~~ class AzureConfigWriteError ( AzureError ) : \n 
~~ class AzureContainerCreateError ( AzureError ) : \n 
~~ class AzureContainerDeleteError ( AzureError ) : \n 
~~ class AzureContainerListContentError ( AzureError ) : \n 
~~ class AzureContainerListError ( AzureError ) : \n 
~~ class AzureDataDiskCreateError ( AzureError ) : \n 
~~ class AzureDataDiskDeleteError ( AzureError ) : \n 
~~ class AzureDataDiskNoAvailableLun ( AzureError ) : \n 
~~ class AzureDataDiskShowError ( AzureError ) : \n 
~~ class AzureDomainLookupError ( AzureError ) : \n 
~~ class AzureEndpointCreateError ( AzureError ) : \n 
~~ class AzureEndpointDeleteError ( AzureError ) : \n 
~~ class AzureEndpointListError ( AzureError ) : \n 
~~ class AzureEndpointShowError ( AzureError ) : \n 
~~ class AzureFileShareCreateError ( AzureError ) : \n 
~~ class AzureFileShareDeleteError ( AzureError ) : \n 
~~ class AzureFileShareListError ( AzureError ) : \n 
~~ class AzureHelpNoCommandGiven ( AzureError ) : \n 
~~ class AzureImageNotReachableByCloudServiceError ( AzureError ) : \n 
~~ class AzureInvalidCommand ( AzureError ) : \n 
~~ class AzureLoadCommandUndefined ( AzureError ) : \n 
~~ class AzureManagementCertificateNotFound ( AzureError ) : \n 
~~ class AzureOsImageCreateError ( AzureError ) : \n 
~~ class AzureOsImageDeleteError ( AzureError ) : \n 
~~ class AzureOsImageDetailsShowError ( AzureError ) : \n 
~~ class AzureOsImageListError ( AzureError ) : \n 
~~ class AzureOsImagePublishError ( AzureError ) : \n 
~~ class AzureOsImageReplicateError ( AzureError ) : \n 
~~ class AzureOsImageShowError ( AzureError ) : \n 
~~ class AzureOsImageUnReplicateError ( AzureError ) : \n 
~~ class AzureOsImageUpdateError ( AzureError ) : \n 
~~ class AzurePageBlobAlignmentViolation ( AzureError ) : \n 
~~ class AzurePageBlobSetupError ( AzureError ) : \n 
~~ class AzurePageBlobUpdateError ( AzureError ) : \n 
~~ class AzurePageBlobZeroPageError ( AzureError ) : \n 
~~ class AzureRequestError ( AzureError ) : \n 
~~ class AzureRequestStatusError ( AzureError ) : \n 
~~ class AzureRequestTimeout ( AzureError ) : \n 
~~ class AzureReservedIpCreateError ( AzureError ) : \n 
~~ class AzureReservedIpDeleteError ( AzureError ) : \n 
~~ class AzureReservedIpListError ( AzureError ) : \n 
~~ class AzureReservedIpShowError ( AzureError ) : \n 
~~ class AzureSSHKeyFileNotFound ( AzureError ) : \n 
~~ class AzureServiceManagementError ( AzureError ) : \n 
~~ class AzureServiceManagementUrlNotFound ( AzureError ) : \n 
~~ class AzureStorageAccountCreateError ( AzureError ) : \n 
~~ class AzureStorageAccountDeleteError ( AzureError ) : \n 
~~ class AzureStorageAccountListError ( AzureError ) : \n 
~~ class AzureStorageAccountShowError ( AzureError ) : \n 
~~ class AzureStorageAccountUpdateError ( AzureError ) : \n 
~~ class AzureStorageDeleteError ( AzureError ) : \n 
~~ class AzureStorageFileNotFound ( AzureError ) : \n 
~~ class AzureStorageListError ( AzureError ) : \n 
~~ class AzureStorageNotReachableByCloudServiceError ( AzureError ) : \n 
~~ class AzureStorageStreamError ( AzureError ) : \n 
~~ class AzureStorageUploadError ( AzureError ) : \n 
~~ class AzureSubscriptionCertificateDecodeError ( AzureError ) : \n 
~~ class AzureSubscriptionIdNotFound ( AzureError ) : \n 
~~ class AzureSubscriptionPKCS12DecodeError ( AzureError ) : \n 
~~ class AzureSubscriptionParseError ( AzureError ) : \n 
~~ class AzureSubscriptionPrivateKeyDecodeError ( AzureError ) : \n 
~~ class AzureUnknownCommand ( AzureError ) : \n 
~~ class AzureUnknownServiceName ( AzureError ) : \n 
~~ class AzureUnrecognizedManagementUrl ( AzureError ) : \n 
~~ class AzureVmCreateError ( AzureError ) : \n 
~~ class AzureVmDeleteError ( AzureError ) : \n 
~~ class AzureXZError ( AzureError ) : \n 
~~ import time \n 
from . . azurectl_exceptions import ( \n 
AzureRequestStatusError , \n 
AzureRequestTimeout , \n 
AzureRequestError \n 
class RequestResult ( object ) : \n 
def __init__ ( self , request_id ) : \n 
~~~ self . request_id = request_id \n 
self . request_timeout_count = 60 \n 
self . request_timeout = 5 \n 
~~ def status ( self , service ) : \n 
~~~ return service . get_operation_status ( self . request_id ) \n 
~~~ raise AzureRequestStatusError ( \n 
% ( type ( e ) . __name__ , format ( e ) ) \n 
~~ ~~ def wait_for_request_completion ( self , service ) : \n 
result = self . status ( service ) \n 
while result . status == : \n 
~~~ count = count + 1 \n 
if count > self . request_timeout_count : \n 
~~~ raise AzureRequestTimeout ( \n 
% self . request_id \n 
~~ time . sleep ( self . request_timeout ) \n 
~~ if result . status != : \n 
~~~ raise AzureRequestError ( \n 
% ( \n 
self . request_id , \n 
format ( result . error . message ) , \n 
format ( result . error . code ) \n 
~~ ~~ ~~ import dateutil . parser \n 
from mock import patch \n 
from test_helper import * \n 
import azurectl \n 
from azurectl . azurectl_exceptions import * \n 
from azurectl . commands . storage_container import StorageContainerTask \n 
class TestStorageContainerTask : \n 
~~~ sys . argv = [ \n 
sys . argv [ 0 ] , , , \n 
azurectl . commands . storage_container . AzureAccount . storage_names = mock . Mock ( \n 
return_value = mock . Mock ( ) \n 
self . storage = mock . Mock ( ) \n 
self . storage . upload = mock . Mock ( ) \n 
azurectl . commands . storage_container . Container = mock . Mock ( \n 
azurectl . commands . storage_container . Help = mock . Mock ( \n 
self . task = StorageContainerTask ( ) \n 
self . __init_command_args ( ) \n 
~~ def __init_command_args ( self ) : \n 
~~~ self . task . command_args = { } \n 
self . task . command_args [ ] = False \n 
self . task . command_args [ ] = \n 
~~ def test_process_storage_container_delete ( self ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
self . task . container . delete . assert_called_once_with ( \n 
self . task . command_args [ ] \n 
~~ def test_process_storage_container_create ( self ) : \n 
self . task . container . create . assert_called_once_with ( \n 
~~ @ patch ( ) \n 
def test_process_storage_container_show ( self , mock_out ) : \n 
self . task . container . content . assert_called_once_with ( \n 
~~ @ raises ( AzureInvalidCommand ) \n 
def test_start_date_validation ( self ) : \n 
def test_end_date_validation ( self ) : \n 
def test_permissions_validation ( self ) : \n 
def test_process_storage_container_sas ( self , mock_out ) : \n 
start = dateutil . parser . parse ( \n 
expiry = dateutil . parser . parse ( \n 
self . task . container . sas . assert_called_once_with ( \n 
self . task . command_args [ ] , start , expiry , \n 
def test_process_storage_container_sas_now ( self , mock_out ) : \n 
self . task . command_args [ ] , mock . ANY , expiry , \n 
def test_process_storage_container_sas_expire ( self , mock_out ) : \n 
expiry = start + datetime . timedelta ( days = 30 ) \n 
def test_process_storage_container_list ( self , mock_out ) : \n 
self . task . container . list . assert_called_once_with ( ) \n 
def test_process_storage_container_from_cfg_list ( self , mock_out ) : \n 
~~ def test_process_storage_container_help ( self ) : \n 
self . task . manual . show . assert_called_once_with ( \n 
from google . appengine . ext import db , webapp \n 
from google . appengine . ext . webapp import util \n 
import member \n 
class CleanUp ( webapp . RequestHandler ) : \n 
~~~ if self . request . headers . get ( ) == : \n 
~~~ now = datetime . datetime . now ( ) \n 
thenMem = now - deltaMem \n 
query = db . Query ( member . Member ) . filter ( , thenMem ) \n 
mems = [ ] \n 
for m in query : \n 
~~~ mems . append ( m ) \n 
~~ db . delete ( mems ) \n 
~~ ~~ ~~ def main ( ) : \n 
~~~ application = webapp . WSGIApplication ( [ ( , CleanUp ) ] , \n 
debug = True ) \n 
util . run_wsgi_app ( application ) \n 
__all__ = [ ] \n 
from . import api_utils \n 
~~~ from google . appengine . api import app_identity \n 
from google . appengine . ext import ndb \n 
~~ def _make_sync_method ( name ) : \n 
def sync_wrapper ( self , * args , ** kwds ) : \n 
~~~ method = getattr ( self , name ) \n 
future = method ( * args , ** kwds ) \n 
return future . get_result ( ) \n 
~~ return sync_wrapper \n 
~~ def add_sync_methods ( cls ) : \n 
for name in cls . __dict__ . keys ( ) : \n 
~~~ if name . endswith ( ) : \n 
~~~ sync_name = name [ : - 6 ] \n 
if not hasattr ( cls , sync_name ) : \n 
~~~ setattr ( cls , sync_name , _make_sync_method ( name ) ) \n 
~~ ~~ ~~ return cls \n 
~~ class _AE_TokenStorage_ ( ndb . Model ) : \n 
token = ndb . StringProperty ( ) \n 
expires = ndb . FloatProperty ( ) \n 
~~ @ ndb . tasklet \n 
def _make_token_async ( scopes , service_account_id ) : \n 
rpc = app_identity . create_rpc ( ) \n 
app_identity . make_get_access_token_call ( rpc , scopes , service_account_id ) \n 
token , expires_at = yield rpc \n 
raise ndb . Return ( ( token , expires_at ) ) \n 
~~ class _RestApi ( object ) : \n 
def __init__ ( self , scopes , service_account_id = None , token_maker = None , \n 
retry_params = None ) : \n 
if isinstance ( scopes , basestring ) : \n 
~~~ scopes = [ scopes ] \n 
~~ self . scopes = scopes \n 
self . service_account_id = service_account_id \n 
self . make_token_async = token_maker or _make_token_async \n 
if not retry_params : \n 
~~~ retry_params = api_utils . _get_default_retry_params ( ) \n 
~~ self . retry_params = retry_params \n 
self . user_agent = { : retry_params . _user_agent } \n 
self . expiration_headroom = random . randint ( 60 , 240 ) \n 
~~ def __getstate__ ( self ) : \n 
return { : self . scopes , \n 
: self . service_account_id , \n 
: ( None if self . make_token_async == _make_token_async \n 
else self . make_token_async ) , \n 
: self . retry_params , \n 
: self . expiration_headroom } \n 
~~ def __setstate__ ( self , state ) : \n 
self . __init__ ( state [ ] , \n 
service_account_id = state [ ] , \n 
token_maker = state [ ] , \n 
retry_params = state [ ] ) \n 
self . expiration_headroom = state [ ] \n 
def do_request_async ( self , url , method = , headers = None , payload = None , \n 
deadline = None , callback = None ) : \n 
retry_wrapper = api_utils . _RetryWrapper ( \n 
self . retry_params , \n 
retriable_exceptions = api_utils . _RETRIABLE_EXCEPTIONS , \n 
should_retry = api_utils . _should_retry ) \n 
resp = yield retry_wrapper . run ( \n 
self . urlfetch_async , \n 
headers = headers , \n 
payload = payload , \n 
deadline = deadline , \n 
callback = callback , \n 
follow_redirects = False ) \n 
raise ndb . Return ( ( resp . status_code , resp . headers , resp . content ) ) \n 
def get_token_async ( self , refresh = False ) : \n 
key = % ( self . service_account_id , . join ( self . scopes ) ) \n 
ts = yield _AE_TokenStorage_ . get_by_id_async ( \n 
key , use_cache = True , use_memcache = True , \n 
use_datastore = self . retry_params . save_access_token ) \n 
if refresh or ts is None or ts . expires < ( \n 
time . time ( ) + self . expiration_headroom ) : \n 
~~~ token , expires_at = yield self . make_token_async ( \n 
self . scopes , self . service_account_id ) \n 
timeout = int ( expires_at - time . time ( ) ) \n 
ts = _AE_TokenStorage_ ( id = key , token = token , expires = expires_at ) \n 
if timeout > 0 : \n 
~~~ yield ts . put_async ( memcache_timeout = timeout , \n 
use_datastore = self . retry_params . save_access_token , \n 
use_cache = True , use_memcache = True ) \n 
~~ ~~ raise ndb . Return ( ts . token ) \n 
def urlfetch_async ( self , url , method = , headers = None , \n 
payload = None , deadline = None , callback = None , \n 
follow_redirects = False ) : \n 
headers = { } if headers is None else dict ( headers ) \n 
headers . update ( self . user_agent ) \n 
~~~ self . token = yield self . get_token_async ( ) \n 
~~ except app_identity . InternalError , e : \n 
~~~ if os . environ . get ( , ) . endswith ( ) : \n 
~~~ self . token = None \n 
logging . warning ( \n 
~~ ~~ if self . token : \n 
~~~ headers [ ] = + self . token \n 
~~ deadline = deadline or self . retry_params . urlfetch_timeout \n 
ctx = ndb . get_context ( ) \n 
resp = yield ctx . urlfetch ( \n 
url , payload = payload , method = method , \n 
headers = headers , follow_redirects = follow_redirects , \n 
deadline = deadline , callback = callback ) \n 
raise ndb . Return ( resp ) \n 
~~ ~~ _RestApi = add_sync_methods ( _RestApi ) \n 
from . dict_object import DictObject \n 
class UserProfile ( DictObject ) : \n 
~~~ super ( UserProfile , self ) . __init__ ( kwargs ) \n 
~~ ~~ class UserGroupHeader ( DictObject ) : \n 
~~~ super ( UserGroupHeader , self ) . __init__ ( kwargs ) \n 
~~ ~~ class Team ( DictObject ) : \n 
def __init__ ( self , ** kwargs ) : \n 
~~~ super ( Team , self ) . __init__ ( kwargs ) \n 
def getURI ( cls , id ) : \n 
~~~ return % id \n 
~~ def postURI ( self ) : \n 
~~ def putURI ( self ) : \n 
~~ def deleteURI ( self ) : \n 
~~~ return % self . id \n 
~~ def getACLURI ( self ) : \n 
~~ def putACLURI ( self ) : \n 
~~ ~~ class TeamMember ( DictObject ) : \n 
~~~ if in kwargs : \n 
~~~ kwargs [ ] = UserGroupHeader ( ** kwargs [ ] ) \n 
~~ super ( TeamMember , self ) . __init__ ( kwargs ) \n 
~~ ~~ from nose . tools import assert_raises \n 
from synapseclient . evaluation import Evaluation , Submission \n 
def test_Evaluation ( ) : \n 
assert_raises ( ValueError , Evaluation , name = , description = , status = ) \n 
assert_raises ( ValueError , Evaluation , name = , description = , status = , contentSource \n 
ev = Evaluation ( name = , description = , status = , contentSource = ) \n 
assert ( ev [ ] == ev . name ) \n 
assert ( ev [ ] == ev . description ) \n 
assert ( ev [ ] == ev . status ) \n 
~~ def test_Submission ( ) : \n 
assert_raises ( KeyError , Submission , foo = ) \n 
import ssl \n 
__status__ = "Prototype" \n 
class JiraAPI ( object ) : \n 
~~~ def __init__ ( self , server , credentials ) : \n 
~~~ super ( JiraAPI , self ) . __init__ ( ) \n 
self . server = server \n 
self . credentials = credentials \n 
self . verify = None \n 
__location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) \n 
if ( self . server == "jira.exacttarget.com:8443" ) : \n 
~~~ self . verify = os . path . join ( __location__ , ) \n 
~~ ~~ def fetchCommitDetails ( self , url ) : \n 
~~~ r = requests . get ( url , auth = self . auth ( ) , verify = self . verify ) ; \n 
if r . headers [ ] : \n 
~~~ remaining_requests = int ( r . headers [ ] ) \n 
if ( remaining_requests == 0 ) : \n 
~~~ self . _no_more_requests_until = datetime . datetime . fromtimestamp ( float ( r . headers [ \n 
~~ ~~ if ( r . ok ) : \n 
~~~ return r . json ( ) \n 
~~ def jql ( self , query , offset = None ) : \n 
~~~ verbose = False \n 
resource_name = "search" \n 
url = "https://%s/rest/api/latest/%s" % ( self . server , urllib . quote ( resource_name ) ) \n 
params = { "jql" : query } \n 
if offset is not None : \n 
~~~ params [ "startAt" ] = offset \n 
~~ r = requests . get ( url , params = params , headers = { "Authorization" : self . credentials . authorizationHeaderValue if ( r . ok ) : \n 
~~~ results = r . json ( ) \n 
return results \n 
print r . text \n 
~~~ usercredentials_jsonfile = "bugsystems-Jira-usercreds.json" \n 
user_creds_data = open ( usercredentials_jsonfile ) \n 
user_creds = json . load ( user_creds_data ) \n 
user = user_creds [ "user" ] \n 
password = user_creds [ "token" ] \n 
server_url = \n 
jira = JiraAPI ( server_url , user , password ) \n 
import dateutil . parser \n 
from Empire . cloudservices . github import GithubOrg , GithubRepo , GithubCommit \n 
from repos . base import RepoSource , RepoCommit , RepoPatch \n 
from repos . diffparser import DiffParser \n 
owner = \n 
repo = \n 
class GithubSource ( RepoSource ) : \n 
~~~ def __init__ ( self , creds = None , host = , owner = , repo = ) : \n 
~~~ self . _last_date = None \n 
self . _last_identifier = None \n 
self . _no_more_requests_until = None \n 
github_org = GithubOrg ( host , owner , creds ) \n 
self . _github_repo = GithubRepo ( github_org , repo ) \n 
~~ def processSinceIdentifier ( self , identifier , commit_started_callback , patch_callback , commit_finished_callback ~~~ since_datetime = datetime . datetime . utcnow ( ) \n 
since_datetime = since_datetime . replace ( tzinfo = pytz . UTC , hour = 0 , minute = 0 , second = 0 ) \n 
if identifier : \n 
~~~ since_datetime = dateutil . parser . parse ( identifier ) \n 
~~ ~~ commits = self . _github_repo . commits ( since_datetime , path = path ) \n 
self . processCommits ( commits , \n 
commit_started_callback = commit_started_callback , \n 
patch_callback = patch_callback , \n 
commit_finished_callback = commit_finished_callback ) ; \n 
if self . _last_date : \n 
~~~ return self . _last_date . isoformat ( ) \n 
~~ if since_datetime : \n 
~~~ return since_datetime . isoformat ( ) \n 
~~ def processCommits ( self , commits , commit_started_callback , patch_callback , commit_finished_callback ~~~ if commits is None : \n 
~~ commits = commits [ : : - 1 ] \n 
for github_commit in commits : \n 
if github_commit . sha : \n 
~~~ github_commit = self . _github_repo . commit ( github_commit . sha ) \n 
repo_commit = RepoCommit ( ) ; \n 
repo_commit . url = github_commit . html_url \n 
repo_commit . repo_source = self \n 
if github_commit . date is not None : \n 
~~~ self . _last_date = dateutil . parser . parse ( github_commit . date ) \n 
repo_commit . date = self . _last_date \n 
self . _last_date += datetime . timedelta ( seconds = 1 ) \n 
repo_commit . identifier = self . _last_date . isoformat ( ) \n 
repo_commit . committer_email = github_commit . committer_email \n 
repo_commit . committer_name = github_commit . committer_name \n 
repo_commit . username = github_commit . committer_login \n 
repo_commit . message = github_commit . message \n 
repo_commit . sha = github_commit . sha \n 
commit_started_callback ( repo_commit ) \n 
if github_commit . files : \n 
~~~ for file_info in github_commit . files : \n 
~~~ if file_info . get ( ) : \n 
~~~ filename = committer_username = None \n 
diff = DiffParser ( file_info [ ] ) \n 
repo_patch = RepoPatch ( repo_commit = repo_commit ) \n 
repo_patch . diff = diff \n 
repo_patch . filename = file_info . get ( "filename" ) \n 
patch_callback ( repo_patch ) \n 
~~ ~~ ~~ commit_finished_callback ( repo_commit ) \n 
~~ ~~ ~~ logger . debug ( "done" ) \n 
~~~ from Empire . creds import CredentialManager \n 
credentials_file = "credentials.json" \n 
credential_key = os . environ . get ( ) \n 
if credential_key is None : \n 
~~~ credential_key = getpass . getpass ( ) \n 
~~ credential_manager = CredentialManager ( credentials_file , credential_key ) \n 
creds = credential_manager . get_or_create_credentials_for ( "github-mfeldmansf" , "password" ) \n 
test = GithubSource ( creds ) ; \n 
def cstart ( commit ) : \n 
~~~ print commit \n 
~~ def pstart ( patch ) : \n 
~~ def cend ( commit ) : \n 
~~ test . processSinceIdentifier ( "2014-11-12T00:00:00Z" , cstart , pstart , cend ) ; \n 
def upload_test_results ( ) : \n 
~~~ APEXTESTSDB_BASE_URL = os . environ . get ( ) \n 
APEXTESTSDB_USER_ID = os . environ . get ( ) \n 
APEXTESTSDB_TOKEN = os . environ . get ( ) \n 
PACKAGE = os . environ . get ( ) \n 
REPOSITORY_URL = os . environ . get ( ) \n 
BRANCH_NAME = os . environ . get ( ) \n 
COMMIT_SHA = os . environ . get ( ) \n 
EXECUTION_NAME = os . environ . get ( ) \n 
EXECUTION_URL = os . environ . get ( ) \n 
RESULTS_FILE_URL = os . environ . get ( ) \n 
ENVIRONMENT_NAME = os . environ . get ( ) \n 
payload = { \n 
: PACKAGE , \n 
: REPOSITORY_URL , \n 
: BRANCH_NAME , \n 
: COMMIT_SHA , \n 
: EXECUTION_NAME , \n 
: EXECUTION_URL , \n 
: ENVIRONMENT_NAME , \n 
: APEXTESTSDB_USER_ID , \n 
: APEXTESTSDB_TOKEN , \n 
: RESULTS_FILE_URL , \n 
response = requests . post ( APEXTESTSDB_BASE_URL + , data = payload ) \n 
data = json . loads ( response . content ) \n 
return % ( APEXTESTSDB_BASE_URL , data [ ] ) \n 
~~~ execution_detail_url = upload_test_results ( ) \n 
print execution_detail_url \n 
~~~ import traceback \n 
exc_type , exc_value , exc_traceback = sys . exc_info ( ) \n 
print * 60 \n 
traceback . print_exception ( exc_type , exc_value , exc_traceback , file = sys . stdout ) \n 
~~ ~~ from flask import request , session , render_template , redirect , url_for \n 
import glass \n 
import foursquare \n 
app = glass . Application ( \n 
client_id = config . GOOGLE_CLIENT_ID , \n 
client_secret = config . GOOGLE_CLIENT_SECRET , \n 
scopes = config . GOOGLE_SCOPES , \n 
template_folder = "templates" , \n 
static_url_path = , \n 
static_folder = ) \n 
app . web . secret_key = \n 
FOURSQUARE_TOKENS = { } \n 
def foursquare_client ( ) : \n 
~~~ return foursquare . Foursquare ( client_id = config . FOURSQUARE_CLIENT_ID , client_secret = config . FOURSQUARE_CLIENT_SECRET \n 
~~ @ app . web . route ( "/" ) \n 
~~~ return render_template ( "index.html" , auth = False ) \n 
~~ @ app . subscriptions . login \n 
def login ( user ) : \n 
session [ ] = user . token \n 
return redirect ( "/foursquare/authorize" ) \n 
~~ @ app . subscriptions . location \n 
def change_location ( user ) : \n 
~~~ location = user . location ( ) \n 
llat = location . get ( ) \n 
llong = location . get ( ) \n 
client = foursquare . Foursquare ( access_token = FOURSQUARE_TOKENS [ user . token ] ) \n 
venues = client . venues . search ( params = { : llat + + llong , : location . get ( ) } ) \n 
if len ( venues [ ] ) > 0 : \n 
~~~ user . timeline . post_template ( "venue.html" , venue = venues [ ] [ 0 ] , llat = llat , llong = llong ) \n 
~~ ~~ @ app . web . route ( "/foursquare/authorize" ) \n 
def foursquare_authorize ( ) : \n 
~~~ client = foursquare_client ( ) \n 
return redirect ( client . oauth . auth_url ( ) ) \n 
~~ @ app . web . route ( "/foursquare/callback" ) \n 
def foursquare_callback ( ) : \n 
~~~ code = request . args . get ( , None ) \n 
client = foursquare_client ( ) \n 
if code is None or not in session : \n 
~~ access_token = client . oauth . get_token ( code ) \n 
FOURSQUARE_TOKENS [ session [ ] ] = access_token \n 
client . set_access_token ( access_token ) \n 
user = client . users ( ) \n 
username = user [ ] [ ] \n 
userglass = glass . User ( app = app , token = session [ ] ) \n 
return render_template ( "index.html" , auth = True ) \n 
app . run ( port = config . PORT , host = config . HOST ) \n 
~~ import logging \n 
from hashlib import sha1 \n 
from redis . client import StrictRedis \n 
from redis . exceptions import ConnectionError \n 
class Node ( object ) : \n 
redis_client_class = StrictRedis \n 
def __init__ ( self , name , host , port ) : \n 
self . connection = self . redis_client_class ( host , int ( port ) ) \n 
~~ ~~ def executeOnNode ( func ) : \n 
@ wraps ( func ) \n 
def wrapper ( self , key , * args , ** kwargs ) : \n 
~~~ node = self . get_node_for_key ( key ) \n 
nodeFunc = getattr ( node . connection , func . __name__ ) \n 
~~~ return nodeFunc ( key , * args , ** kwargs ) \n 
~~ except ConnectionError : \n 
~~~ node = self . get_master ( node ) \n 
return nodeFunc ( key , * args , ** kwargs ) \n 
~~ ~~ return wrapper \n 
~~ class DisredisClient ( object ) : \n 
sentinel = None \n 
nodes = None \n 
def __init__ ( self , sentinel_addresses ) : \n 
~~~ self . sentinel_addresses = sentinel_addresses \n 
self . _get_nodes ( ) \n 
~~~ address = self . sentinel_addresses . pop ( 0 ) \n 
host , port = address . split ( ":" ) \n 
self . sentinel = self . redis_client_class ( host , int ( port ) ) \n 
self . sentinel_addresses . append ( address ) \n 
~~~ if not self . sentinel_addresses : \n 
~~ ~~ except IndexError : \n 
~~ ~~ ~~ def _execute_sentinel_command ( self , * args , ** kwargs ) : \n 
~~~ if self . sentinel is None : \n 
~~~ self . _connect ( ) \n 
~~ return self . sentinel . execute_command ( "SENTINEL" , * args , \n 
~~~ self . sentinel = None \n 
if self . sentinel_addresses : \n 
~~ ~~ ~~ ~~ def _get_nodes ( self ) : \n 
masterList = self . _execute_sentinel_command ( "MASTERS" ) \n 
self . nodes = [ ] \n 
for master in masterList : \n 
~~~ info = dict ( zip ( master [ : : 2 ] , master [ 1 : : 2 ] ) ) \n 
self . nodes . append ( Node ( info [ "name" ] , info [ "ip" ] , info [ "port" ] ) ) \n 
~~ ~~ def get_master ( self , node ) : \n 
host , port = self . _execute_sentinel_command ( "get-master-addr-by-name" , \n 
node . name ) \n 
if host == node . host and port == node . port : \n 
~~~ return node \n 
~~ newNode = Node ( node . name , host , port ) \n 
self . nodes [ self . nodes . index ( node ) ] = newNode \n 
return newNode \n 
~~ def get_node_for_key ( self , key ) : \n 
if "{" in key and "}" in key : \n 
~~~ key = key [ key . index ( "{" ) + 1 : key . index ( "}" ) ] \n 
~~ return self . nodes [ int ( sha1 ( key ) . hexdigest ( ) , 16 ) % len ( self . nodes ) ] \n 
~~ def set_response_callback ( self , command , callback ) : \n 
~~ def pipeline ( self , transaction = True , shard_hint = None ) : \n 
~~ def transaction ( self , func , * watches , ** kwargs ) : \n 
~~ @ executeOnNode \n 
def lock ( self , name , timeout = None , sleep = 0.1 ) : \n 
def pubsub ( self , shard_hint = None ) : \n 
~~ def bgrewriteaof ( self ) : \n 
~~ def bgsave ( self ) : \n 
~~ def client_kill ( self , address ) : \n 
~~ def client_list ( self ) : \n 
~~ def client_getname ( self ) : \n 
~~ def client_setname ( self , name ) : \n 
~~ def config_get ( self , pattern = "*" ) : \n 
~~ def config_set ( self , name , value ) : \n 
~~ def dbsize ( self ) : \n 
~~ def time ( self ) : \n 
def debug_object ( self , key ) : \n 
~~ def delete ( self , * names ) : \n 
return self . execute_command ( , * names ) \n 
~~ __delitem__ = delete \n 
def echo ( self , value ) : \n 
~~ def flushall ( self ) : \n 
~~ def flushdb ( self ) : \n 
~~ def info ( self , section = None ) : \n 
~~ def lastsave ( self ) : \n 
~~ def object ( self , infotype , key ) : \n 
~~ def ping ( self ) : \n 
~~ def shutdown ( self ) : \n 
~~ def slaveof ( self , host = None , port = None ) : \n 
def append ( self , key , value ) : \n 
def getrange ( self , key , start , end ) : \n 
def bitcount ( self , key , start = None , end = None ) : \n 
~~ def bitop ( self , operation , dest , * keys ) : \n 
def decr ( self , name , amount = 1 ) : \n 
def exists ( self , name ) : \n 
~~ __contains__ = exists \n 
@ executeOnNode \n 
def expire ( self , name , time ) : \n 
def expireat ( self , name , when ) : \n 
def get ( self , name ) : \n 
~~ def __getitem__ ( self , name ) : \n 
value = self . get ( name ) \n 
~~~ return value \n 
~~ raise KeyError ( name ) \n 
def getbit ( self , name , offset ) : \n 
def getset ( self , name , value ) : \n 
def incr ( self , name , amount = 1 ) : \n 
~~ def incrby ( self , name , amount = 1 ) : \n 
return self . incr ( name , amount ) \n 
def incrbyfloat ( self , name , amount = 1.0 ) : \n 
~~ def keys ( self , pattern = ) : \n 
~~ def mget ( self , keys , * args ) : \n 
~~ def mset ( self , mapping ) : \n 
~~ def msetnx ( self , mapping ) : \n 
def move ( self , name , db ) : \n 
def persist ( self , name ) : \n 
def pexpire ( self , name , time ) : \n 
def pexpireat ( self , name , when ) : \n 
def psetex ( self , name , time_ms , value ) : \n 
def pttl ( self , name ) : \n 
~~ def randomkey ( self ) : \n 
~~ def rename ( self , src , dst ) : \n 
~~ def renamenx ( self , src , dst ) : \n 
def set ( self , name , value , ex = None , px = None , nx = False , xx = False ) : \n 
~~ __setitem__ = set \n 
def setbit ( self , name , offset , value ) : \n 
def setex ( self , name , time , value ) : \n 
def setnx ( self , name , value ) : \n 
def setrange ( self , name , offset , value ) : \n 
def strlen ( self , name ) : \n 
def substr ( self , name , start , end = - 1 ) : \n 
def ttl ( self , name ) : \n 
def type ( self , name ) : \n 
~~ def blpop ( self , keys , timeout = 0 ) : \n 
~~ def brpop ( self , keys , timeout = 0 ) : \n 
~~ def brpoplpush ( self , src , dst , timeout = 0 ) : \n 
def lindex ( self , name , index ) : \n 
def linsert ( self , name , where , refvalue , value ) : \n 
def llen ( self , name ) : \n 
def lpop ( self , name ) : \n 
def lpush ( self , name , * values ) : \n 
def lpushx ( self , name , value ) : \n 
def lrange ( self , name , start , end ) : \n 
def lrem ( self , name , count , value ) : \n 
def lset ( self , name , index , value ) : \n 
def ltrim ( self , name , start , end ) : \n 
def rpop ( self , name ) : \n 
~~ def rpoplpush ( self , src , dst ) : \n 
def rpush ( self , name , * values ) : \n 
def rpushx ( self , name , value ) : \n 
def sort ( self , name , start = None , num = None , by = None , get = None , \n 
desc = False , alpha = False , store = None , groups = False ) : \n 
def sadd ( self , name , * values ) : \n 
def scard ( self , name ) : \n 
~~ def sdiff ( self , keys , * args ) : \n 
~~ def sdiffstore ( self , dest , keys , * args ) : \n 
~~ def sinter ( self , keys , * args ) : \n 
~~ def sinterstore ( self , dest , keys , * args ) : \n 
def sismember ( self , name , value ) : \n 
def smembers ( self , name ) : \n 
~~ def smove ( self , src , dst , value ) : \n 
return self . execute_command ( , src , dst , value ) \n 
def spop ( self , name ) : \n 
def srandmember ( self , name , number = None ) : \n 
def srem ( self , name , * values ) : \n 
~~ def sunion ( self , keys , * args ) : \n 
~~ def sunionstore ( self , dest , keys , * args ) : \n 
def zadd ( self , name , * args , ** kwargs ) : \n 
def zcard ( self , name ) : \n 
def zcount ( self , name , min , max ) : \n 
def zincrby ( self , name , value , amount = 1 ) : \n 
~~ def zinterstore ( self , dest , keys , aggregate = None ) : \n 
def zrange ( self , name , start , end , desc = False , withscores = False , \n 
score_cast_func = float ) : \n 
def zrangebyscore ( self , name , min , max , start = None , num = None , \n 
withscores = False , score_cast_func = float ) : \n 
def zrank ( self , name , value ) : \n 
def zrem ( self , name , * values ) : \n 
def zremrangebyrank ( self , name , min , max ) : \n 
def zremrangebyscore ( self , name , min , max ) : \n 
def zrevrange ( self , name , start , num , withscores = False , \n 
def zrevrangebyscore ( self , name , max , min , start = None , num = None , \n 
def zrevrank ( self , name , value ) : \n 
def zscore ( self , name , value ) : \n 
~~ def zunionstore ( self , dest , keys , aggregate = None ) : \n 
def hdel ( self , name , * keys ) : \n 
def hexists ( self , name , key ) : \n 
def hget ( self , name , key ) : \n 
def hgetall ( self , name ) : \n 
def hincrby ( self , name , key , amount = 1 ) : \n 
def hincrbyfloat ( self , name , key , amount = 1.0 ) : \n 
def hkeys ( self , name ) : \n 
def hlen ( self , name ) : \n 
def hset ( self , name , key , value ) : \n 
def hsetnx ( self , name , key , value ) : \n 
def hmset ( self , name , mapping ) : \n 
def hmget ( self , name , keys , * args ) : \n 
def hvals ( self , name ) : \n 
~~ def publish ( self , channel , message ) : \n 
~~ def eval ( self , script , numkeys , * keys_and_args ) : \n 
~~ def evalsha ( self , sha , numkeys , * keys_and_args ) : \n 
~~ def script_exists ( self , * args ) : \n 
~~ def script_flush ( self ) : \n 
~~ def script_kill ( self ) : \n 
~~ def script_load ( self , script ) : \n 
~~ def register_script ( self , script ) : \n 
from service import upload \n 
from utils import base_url \n 
import Image \n 
import ImageOps \n 
import cStringIO \n 
__all__ = [ "handle_item" , "CONTACTS" , "WELCOMES" ] \n 
CONTACTS = [ \n 
"acceptTypes" : "image/*" , \n 
"id" : "instaglass_sepia" , \n 
"displayName" : "Sepia" , \n 
"imageUrls" : [ base_url + "/images/sepia.jpg" ] \n 
WELCOMES = [ \n 
"</article>" ) \n 
def _make_linear_ramp ( white ) : \n 
ramp = [ ] \n 
r , g , b = white \n 
for i in range ( 255 ) : \n 
~~~ ramp . extend ( ( r * i / 255 , g * i / 255 , b * i / 255 ) ) \n 
~~ return ramp \n 
~~ def _apply_sepia_filter ( image ) : \n 
sepia = _make_linear_ramp ( ( 255 , 240 , 192 ) ) \n 
orig_mode = image . mode \n 
if orig_mode != "L" : \n 
~~~ image = image . convert ( "L" ) \n 
~~ image = ImageOps . autocontrast ( image ) \n 
image . putpalette ( sepia ) \n 
~~~ image = image . convert ( orig_mode ) \n 
~~ return image \n 
~~ def handle_item ( item , notification , service , test ) : \n 
if "userActions" in notification : \n 
~~~ for action in notification [ "userActions" ] : \n 
~~~ if "type" in action and action [ "type" ] == "SHARE" : \n 
~~ if "recipients" in item : \n 
~~~ for rec in item [ "recipients" ] : \n 
~~~ if rec [ "id" ] == "instaglass_sepia" : \n 
~~ imageId = None \n 
if "attachments" in item : \n 
~~~ for att in item [ "attachments" ] : \n 
~~~ if att [ "contentType" ] . startswith ( "image/" ) : \n 
~~~ imageId = att [ "id" ] \n 
~~ ~~ ~~ if imageId is None : \n 
~~ attachment_metadata = service . timeline ( ) . attachments ( ) . get ( \n 
itemId = item [ "id" ] , attachmentId = imageId ) . execute ( ) \n 
content_url = attachment_metadata . get ( "contentUrl" ) \n 
resp , content = service . _http . request ( content_url ) \n 
if resp . status != 200 : \n 
~~ tempimg = cStringIO . StringIO ( content ) \n 
im = Image . open ( tempimg ) \n 
new_im = _apply_sepia_filter ( im ) \n 
f = cStringIO . StringIO ( ) \n 
new_im . save ( f , "JPEG" ) \n 
content = f . getvalue ( ) \n 
new_item = { } \n 
new_item [ "menuItems" ] = [ { "action" : "SHARE" } ] \n 
result = upload . multipart_insert ( new_item , content , "image/jpeg" , service , test ) \n 
logging . info ( result ) \n 
from setuptools import setup \n 
"argparse>=1.2.1" , \n 
"requests>=2.4.3" \n 
description = , \n 
packages = ( , ) , \n 
scripts = ( \n 
install_requires = install_requires , \n 
Handler_mapping = { } \n 
def handler ( cmdid ) : \n 
def _module_dec ( cls ) : \n 
~~~ Handler_mapping [ cmdid ] = cls \n 
return cls \n 
import hashlib \n 
if not ( hasattr ( __builtins__ , "bytes" ) ) or str is bytes : \n 
~~~ def bytes ( var , * args ) : \n 
~~~ return . join ( map ( chr , var ) ) \n 
~~~ return map ( ord , var ) \n 
~~ ~~ ~~ __all__ = [ "rollingchecksum" , "weakchecksum" , "patchstream" , "rsyncdelta" , \n 
"blockchecksums" ] \n 
def rsyncdelta ( datastream , remotesignatures , blocksize = 4096 ) : \n 
remote_weak , remote_strong = remotesignatures \n 
match = True \n 
matchblock = - 1 \n 
deltaqueue = collections . deque ( ) \n 
~~~ if match and datastream is not None : \n 
~~~ window = collections . deque ( bytes ( datastream . read ( blocksize ) ) ) \n 
checksum , a , b = weakchecksum ( window ) \n 
~~~ matchblock = remote_weak . index ( checksum , matchblock + 1 ) \n 
stronghash = hashlib . sha256 ( bytes ( window ) ) . hexdigest ( ) \n 
matchblock = remote_strong . index ( stronghash , matchblock ) \n 
deltaqueue . append ( matchblock ) \n 
if datastream . closed : \n 
~~ continue \n 
~~~ match = False \n 
~~~ if datastream : \n 
~~~ newbyte = ord ( datastream . read ( 1 ) ) \n 
window . append ( newbyte ) \n 
~~ ~~ except TypeError : \n 
~~~ newbyte = 0 \n 
tailsize = datastream . tell ( ) % blocksize \n 
datastream = None \n 
~~ if datastream is None and len ( window ) <= tailsize : \n 
~~~ deltaqueue . append ( window ) \n 
~~ oldbyte = window . popleft ( ) \n 
checksum , a , b = rollingchecksum ( oldbyte , newbyte , a , b , blocksize ) \n 
~~~ deltaqueue [ - 1 ] . append ( oldbyte ) \n 
~~ except ( AttributeError , IndexError ) : \n 
~~~ deltaqueue . append ( [ oldbyte ] ) \n 
~~ ~~ ~~ deltastructure = [ blocksize ] \n 
for element in deltaqueue : \n 
~~~ if isinstance ( element , int ) : \n 
~~~ deltastructure . append ( element ) \n 
~~ elif element : \n 
~~~ deltastructure . append ( bytes ( element ) ) \n 
~~ ~~ return deltastructure \n 
~~ def blockchecksums ( instream , blocksize = 4096 ) : \n 
weakhashes = list ( ) \n 
stronghashes = list ( ) \n 
read = instream . read ( blocksize ) \n 
while read : \n 
~~~ weakhashes . append ( weakchecksum ( bytes ( read ) ) [ 0 ] ) \n 
stronghashes . append ( hashlib . sha256 ( read ) . hexdigest ( ) ) \n 
~~ return weakhashes , stronghashes \n 
~~ def patchstream ( instream , outstream , delta ) : \n 
blocksize = delta [ 0 ] \n 
for element in delta [ 1 : ] : \n 
~~~ if isinstance ( element , int ) and blocksize : \n 
~~~ instream . seek ( element * blocksize ) \n 
element = instream . read ( blocksize ) \n 
~~ outstream . write ( element ) \n 
~~ ~~ def rollingchecksum ( removed , new , a , b , blocksize = 4096 ) : \n 
a -= removed - new \n 
b -= removed * blocksize - a \n 
return ( b << 16 ) | a , a , b \n 
~~ def weakchecksum ( data ) : \n 
a = b = 0 \n 
l = len ( data ) \n 
for i in range ( l ) : \n 
~~~ a += data [ i ] \n 
b += ( l - i ) * data [ i ] \n 
~~ return ( b << 16 ) | a , a , b \n 
from shopify_settings import * \n 
SITE_ROOT = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
~~~ from djangoappengine . settings_base import * \n 
USING_APP_ENGINE = True \n 
~~~ USING_APP_ENGINE = False \n 
: os . path . join ( SITE_ROOT , ) , \n 
SITE_ID = 1 \n 
~~ USE_I18N = True \n 
USE_L10N = True \n 
MEDIA_ROOT = \n 
MEDIA_URL = \n 
STATIC_ROOT = \n 
STATIC_URL = \n 
ADMIN_MEDIA_PREFIX = \n 
STATICFILES_DIRS = ( \n 
os . path . join ( SITE_ROOT , ) , \n 
TEMPLATE_LOADERS = ( \n 
if not USING_APP_ENGINE : \n 
~~~ TEMPLATE_CONTEXT_PROCESSORS += ( \n 
~~ MIDDLEWARE_CLASSES = ( \n 
ROOT_URLCONF = \n 
TEMPLATE_DIRS = ( \n 
INSTALLED_APPS = ( \n 
if USING_APP_ENGINE : \n 
~~~ INSTALLED_APPS += ( \n 
~~ LOGGING = { \n 
: False , \n 
from . customer_saved_search import CustomerSavedSearch \n 
class CustomerGroup ( CustomerSavedSearch ) : \n 
~~ from . . base import ShopifyResource \n 
class ShippingZone ( ShopifyResource ) : \n 
~~ import shopify \n 
from test . test_helper import TestCase \n 
from pyactiveresource . activeresource import ActiveResource \n 
from pyactiveresource . util import xml_to_dict \n 
class OrderTest ( TestCase ) : \n 
~~~ def test_should_be_loaded_correctly_from_order_xml ( self ) : \n 
order = shopify . Order ( xml_to_dict ( order_xml ) [ "order" ] ) \n 
self . assertEqual ( 1 , len ( order . note_attributes ) ) \n 
note_attribute = order . note_attributes [ 0 ] \n 
self . assertEqual ( "size" , note_attribute . name ) \n 
self . assertEqual ( "large" , note_attribute . value ) \n 
~~ def test_should_be_able_to_add_note_attributes_to_an_order ( self ) : \n 
~~~ order = shopify . Order ( ) \n 
order . note_attributes = [ ] \n 
order . note_attributes . append ( shopify . NoteAttribute ( { : "color" , : "blue" } ) ) \n 
order_xml = xml_to_dict ( order . to_xml ( ) ) \n 
note_attributes = order_xml [ "order" ] [ "note_attributes" ] \n 
self . assertTrue ( isinstance ( note_attributes , list ) ) \n 
attribute = note_attributes [ 0 ] \n 
self . assertEqual ( "color" , attribute [ "name" ] ) \n 
self . assertEqual ( "blue" , attribute [ "value" ] ) \n 
~~ def test_get_order ( self ) : \n 
~~~ self . fake ( , method = , body = self . load_fixture ( ) ) \n 
order = shopify . Order . find ( 450789469 ) \n 
self . assertEqual ( , order . email ) \n 
~~ def test_get_order_transaction ( self ) : \n 
self . fake ( , method = , body = self . load_fixture ( transactions = order . transactions ( ) \n 
self . assertEqual ( "409.94" , transactions [ 0 ] . amount ) \n 
from os . path import exists , dirname \n 
from . base import Base \n 
from deoplete . util import set_default , get_simple_buffer_config \n 
class Source ( Base ) : \n 
~~~ def __init__ ( self , vim ) : \n 
~~~ Base . __init__ ( self , vim ) \n 
self . name = \n 
self . mark = \n 
self . min_pattern_length = 0 \n 
self . rank = 150 \n 
set_default ( self . vim , , 0 ) \n 
~~ def get_complete_position ( self , context ) : \n 
~~~ pos = context [ ] . rfind ( ) \n 
return pos if pos < 0 else pos + 1 \n 
~~ def gather_candidates ( self , context ) : \n 
~~~ p = self . __longest_path_that_exists ( context [ ] ) \n 
if p in ( None , [ ] ) or p == or re . search ( , p ) : \n 
~~ complete_str = self . __substitute_path ( dirname ( p ) + ) \n 
if not os . path . isdir ( complete_str ) : \n 
~~ hidden = context [ ] . find ( ) == 0 \n 
dirs = [ x for x in os . listdir ( complete_str ) \n 
if os . path . isdir ( complete_str + x ) and \n 
( hidden or x [ 0 ] != ) ] \n 
files = [ x for x in os . listdir ( complete_str ) \n 
if not os . path . isdir ( complete_str + x ) and \n 
return [ { : x , : x + } for x in sorted ( dirs ) \n 
] + [ { : x } for x in sorted ( files ) ] \n 
~~ def __longest_path_that_exists ( self , input_str ) : \n 
~~~ data = re . split ( self . vim . call ( \n 
self . vim . options [ ] ) , input_str ) \n 
existing_paths = list ( filter ( lambda x : exists ( \n 
dirname ( self . __substitute_path ( x ) ) ) , pos ) ) \n 
if existing_paths and len ( existing_paths ) > 0 : \n 
~~~ return sorted ( existing_paths ) [ - 1 ] \n 
~~ def __substitute_path ( self , path ) : \n 
~~~ buffer_path = get_simple_buffer_config ( \n 
self . vim , \n 
m = re . match ( , path ) \n 
~~~ h = self . vim . funcs . repeat ( , len ( m . group ( 1 ) ) ) \n 
return re . sub ( , \n 
self . vim . funcs . fnamemodify ( \n 
( self . vim . funcs . bufname ( ) \n 
if buffer_path \n 
else self . vim . funcs . getcwd ( ) ) , + h ) , \n 
path ) \n 
~~ m = re . match ( , path ) \n 
if m and os . environ . get ( ) : \n 
~~~ return re . sub ( , os . environ . get ( ) , path ) \n 
if m and os . environ . get ( m . group ( 1 ) ) : \n 
~~~ return re . sub ( , os . environ . get ( m . group ( 1 ) ) , path ) \n 
IS_PY3 = sys . version_info [ 0 ] == 3 \n 
def itervalues ( obj , ** kwargs ) : \n 
return iter ( obj . values ( ** kwargs ) ) if IS_PY3 else obj . itervalues ( ** kwargs ) \n 
def assert_errors ( errors , expected_errors ) : \n 
assert len ( errors ) == len ( expected_errors ) \n 
for error , expected in zip ( errors , expected_errors ) : \n 
~~~ assert expected in str ( error ) \n 
from os import path \n 
from hcpsdk . version import _Version \n 
#try: \n 
here = path . abspath ( path . dirname ( __file__ ) ) \n 
with open ( path . normpath ( path . join ( here , ) ) , encoding = ) as f : \n 
~~~ long_description = f . read ( ) \n 
~~ setup ( \n 
version = str ( _Version ( ) ) , \n 
long_description = long_description , \n 
keywords = , \n 
packages = find_packages ( exclude = [ ] ) , \n 
install_requires = [ ] , \n 
class PipelineDefinitionError ( Exception ) : \n 
~~~ def __init__ ( self , msg , definition ) : \n 
~~~ full_msg = ( \n 
super ( PipelineDefinitionError , self ) . __init__ ( full_msg ) \n 
self . msg = msg \n 
self . definition = definition \n 
~~ ~~ def api_to_definition ( definition ) : \n 
~~~ if in definition : \n 
~~~ definition [ ] = _api_to_objects_definition ( \n 
definition . pop ( ) ) \n 
~~ if in definition : \n 
~~~ definition [ ] = _api_to_parameters_definition ( \n 
~~~ definition [ ] = _api_to_values_definition ( \n 
~~ return definition \n 
~~ def definition_to_api_objects ( definition ) : \n 
~~~ if not in definition : \n 
~~ api_elements = [ ] \n 
for element in definition [ ] : \n 
~~~ element_id = element . pop ( ) \n 
json . dumps ( element ) , definition ) \n 
~~ api_object = { : element_id } \n 
name = element . pop ( , element_id ) \n 
api_object [ ] = name \n 
fields = [ ] \n 
for key , value in sorted ( element . items ( ) ) : \n 
~~~ fields . extend ( _parse_each_field ( key , value ) ) \n 
~~ api_object [ ] = fields \n 
api_elements . append ( api_object ) \n 
~~ return api_elements \n 
~~ def definition_to_api_parameters ( definition ) : \n 
~~ parameter_objects = [ ] \n 
~~~ parameter_id = element . pop ( ) \n 
~~ parameter_object = { : parameter_id } \n 
attributes = [ ] \n 
~~~ attributes . extend ( _parse_each_field ( key , value ) ) \n 
~~ parameter_object [ ] = attributes \n 
parameter_objects . append ( parameter_object ) \n 
~~ return parameter_objects \n 
~~ def definition_to_parameter_values ( definition ) : \n 
~~ parameter_values = [ ] \n 
for key in definition [ ] : \n 
~~~ parameter_values . extend ( \n 
_convert_single_parameter_value ( key , definition [ ] [ key ] ) ) \n 
~~ return parameter_values \n 
~~ def _parse_each_field ( key , value ) : \n 
~~~ values = [ ] \n 
if isinstance ( value , list ) : \n 
~~~ for item in value : \n 
~~~ values . append ( _convert_single_field ( key , item ) ) \n 
~~~ values . append ( _convert_single_field ( key , value ) ) \n 
~~ return values \n 
~~ def _convert_single_field ( key , value ) : \n 
~~~ field = { : key } \n 
if isinstance ( value , dict ) and list ( value . keys ( ) ) == [ ] : \n 
~~~ field [ ] = value [ ] \n 
~~~ field [ ] = value \n 
~~ return field \n 
~~ def _convert_single_parameter_value ( key , values ) : \n 
~~~ parameter_values = [ ] \n 
if isinstance ( values , list ) : \n 
~~~ for each_value in values : \n 
~~~ parameter_value = { : key , : each_value } \n 
parameter_values . append ( parameter_value ) \n 
~~~ parameter_value = { : key , : values } \n 
~~ def _api_to_objects_definition ( api_response ) : \n 
~~~ pipeline_objects = [ ] \n 
for element in api_response : \n 
~~~ current = { \n 
: element [ ] , \n 
: element [ ] \n 
for field in element [ ] : \n 
~~~ key = field [ ] \n 
if in field : \n 
~~~ value = field [ ] \n 
~~~ value = { : field [ ] } \n 
~~ _add_value ( key , value , current ) \n 
~~ pipeline_objects . append ( current ) \n 
~~ return pipeline_objects \n 
~~ def _api_to_parameters_definition ( api_response ) : \n 
~~~ parameter_objects = [ ] \n 
for attribute in element [ ] : \n 
~~~ _add_value ( attribute [ ] , attribute [ ] , current ) \n 
~~ parameter_objects . append ( current ) \n 
~~ def _api_to_values_definition ( api_response ) : \n 
~~~ pipeline_values = { } \n 
~~~ _add_value ( element [ ] , element [ ] , pipeline_values ) \n 
~~ return pipeline_values \n 
~~ def _add_value ( key , value , current_map ) : \n 
~~~ if key not in current_map : \n 
~~~ current_map [ key ] = value \n 
~~ elif isinstance ( current_map [ key ] , list ) : \n 
~~~ current_map [ key ] . append ( value ) \n 
~~~ converted_list = [ current_map [ key ] , value ] \n 
current_map [ key ] = converted_list \n 
__docformat__ = \n 
from api import * \n 
from enums import * \n 
from utils import * \n 
from conversion import * \n 
from client import * \n 
from user import * \n 
from call import * \n 
from profile import * \n 
from settings import * \n 
from chat import * \n 
from application import * \n 
from voicemail import * \n 
from sms import * \n 
from filetransfer import * \n 
class APINotifier ( SkypeAPINotifier ) : \n 
~~~ def __init__ ( self , skype ) : \n 
~~~ self . skype = weakref . proxy ( skype ) \n 
~~ def attachment_changed ( self , status ) : \n 
~~~ self . skype . _CallEventHandler ( , status ) \n 
if status == apiAttachRefused : \n 
~~~ raise SkypeAPIError ( ) \n 
~~ ~~ except weakref . ReferenceError : \n 
~~ ~~ def notification_received ( self , notification ) : \n 
~~~ skype = self . skype \n 
skype . _CallEventHandler ( , notification ) \n 
a , b = chop ( notification ) \n 
object_type = None \n 
if a in ( , , , , , , , ~~~ object_type , object_id , prop_name , value = [ a ] + chop ( b , 2 ) \n 
skype . _CacheDict [ str ( object_type ) , str ( object_id ) , str ( prop_name ) ] = value \n 
if object_type == : \n 
~~~ o = User ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ elif prop_name == or prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , value ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o ) \n 
~~ ~~ elif object_type == : \n 
~~~ o = Call ( skype , object_id ) \n 
~~~ skype . _CallEventHandler ( , o , ( value == ) ) \n 
~~~ o = Chat ( skype , object_id ) \n 
~~~ skype . _CallEventHandler ( , o , UserCollection ( skype , split ~~ if prop_name in ( , ) : \n 
~~~ skype . _CallEventHandler ( , o , ( prop_name == ) ) \n 
~~~ o = ChatMember ( skype , object_id ) \n 
~~~ o = ChatMessage ( skype , object_id ) \n 
~~~ o = Application ( skype , object_id ) \n 
~~~ skype . _CallEventHandler ( , o , UserCollection ( skype , split ~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ApplicationStreamCollection ~~ elif prop_name == : \n 
~~~ handle , text = chop ( value ) \n 
skype . _CallEventHandler ( , o , ApplicationStream ( o , handle ~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ApplicationStreamCollection ~~ ~~ elif object_type == : \n 
~~~ o = Group ( skype , object_id ) \n 
~~~ skype . _CallEventHandler ( , o , int ( value ) ) \n 
~~~ o = SmsMessage ( skype , object_id ) \n 
~~~ for t in split ( value , ) : \n 
~~~ number , status = t . split ( ) \n 
skype . _CallEventHandler ( , SmsTarget ( o , number ) , ~~ ~~ ~~ elif object_type == : \n 
~~~ o = FileTransfer ( skype , object_id ) \n 
~~~ o = Voicemail ( skype , object_id ) \n 
~~ ~~ ~~ elif a in ( , ) : \n 
~~~ object_type , object_id , prop_name , value = [ a , ] + chop ( b ) \n 
~~ elif a in ( , , , , ~~~ object_type , object_id , prop_name , value = [ a , , , b ] \n 
~~~ skype . _CallEventHandler ( , value == ) \n 
~~ elif object_type == : \n 
~~~ skype . _CallEventHandler ( , str ( value ) ) \n 
~~~ skype . _CallEventHandler ( , ( value == ) ) \n 
~~ ~~ elif a == : \n 
~~~ skype . _CallEventHandler ( ) \n 
~~ elif a == : \n 
~~~ prop_name , value = chop ( b ) \n 
~~~ skype . _CallEventHandler ( , int ( value ) ) \n 
~~~ object_id , prop_name , value = chop ( b , 2 ) \n 
~~~ skype . _CallEventHandler ( , PluginEvent ( skype , object_id ) ) \n 
~~~ i = value . rfind ( ) \n 
if i >= 0 : \n 
~~~ context = chop ( value [ i + 8 : ] ) [ 0 ] \n 
users = ( ) \n 
context_id = \n 
if context in ( pluginContextContact , pluginContextCall , pluginContextChat ) : \n 
~~~ users = UserCollection ( skype , split ( value [ : i - 1 ] , ) ) \n 
~~ if context in ( pluginContextCall , pluginContextChat ) : \n 
~~~ j = value . rfind ( ) \n 
if j >= 0 : \n 
~~~ context_id = str ( chop ( value [ j + 11 : ] ) [ 0 ] ) \n 
if context == pluginContextCall : \n 
~~~ context_id = int ( context_id ) \n 
~~ ~~ ~~ skype . _CallEventHandler ( , PluginMenuItem ( skype , object_id ~~ ~~ ~~ elif a == : \n 
~~~ skype . _CallEventHandler ( , unicode2path ( b ) ) \n 
~~ ~~ def sending_command ( self , command ) : \n 
~~~ self . skype . _CallEventHandler ( , command ) \n 
~~ except weakref . ReferenceError : \n 
~~ ~~ def reply_received ( self , command ) : \n 
~~ ~~ ~~ class Skype ( EventHandlingBase ) : \n 
def __init__ ( self , Events = None , ** Options ) : \n 
self . _Logger = logging . getLogger ( ) \n 
self . _Logger . info ( ) \n 
EventHandlingBase . __init__ ( self ) \n 
if Events : \n 
~~~ self . _SetEventHandlerObject ( Events ) \n 
~~~ self . _Api = Options . pop ( ) \n 
if Options : \n 
~~~ self . _Api = SkypeAPI ( Options ) \n 
~~ self . _Api . set_notifier ( APINotifier ( self ) ) \n 
Cached . _CreateOwner ( self ) \n 
self . _Cache = True \n 
self . ResetCache ( ) \n 
from api import DEFAULT_TIMEOUT \n 
self . _Timeout = DEFAULT_TIMEOUT \n 
self . _Convert = Conversion ( self ) \n 
self . _Client = Client ( self ) \n 
self . _Settings = Settings ( self ) \n 
self . _Profile = Profile ( self ) \n 
~~ def __del__ ( self ) : \n 
if hasattr ( self , ) : \n 
~~~ self . _Api . close ( ) \n 
~~ self . _Logger . info ( ) \n 
~~ def _DoCommand ( self , Cmd , ExpectedReply = ) : \n 
~~~ command = Command ( Cmd , ExpectedReply , True , self . Timeout ) \n 
self . SendCommand ( command ) \n 
a , b = chop ( command . Reply ) \n 
if a == : \n 
~~~ errnum , errstr = chop ( b ) \n 
self . _CallEventHandler ( , command , int ( errnum ) , errstr ) \n 
raise SkypeError ( int ( errnum ) , errstr ) \n 
~~ if not command . Reply . startswith ( command . Expected ) : \n 
~~~ raise SkypeError ( 0 , % ( command . Reply , command . Expected ) ) \n 
~~ return command . Reply \n 
~~ def _Property ( self , ObjectType , ObjectId , PropName , Set = None , Cache = True ) : \n 
~~~ h = ( str ( ObjectType ) , str ( ObjectId ) , str ( PropName ) ) \n 
arg = ( % h ) . split ( ) \n 
while in arg : \n 
~~~ arg . remove ( ) \n 
~~ jarg = . join ( arg ) \n 
~~~ if Cache and self . _Cache and h in self . _CacheDict : \n 
~~~ return self . _CacheDict [ h ] \n 
~~ value = self . _DoCommand ( % jarg , jarg ) \n 
while arg : \n 
~~~ a , b = chop ( value ) \n 
~~ if a . lower ( ) != arg [ 0 ] . lower ( ) : \n 
~~ del arg [ 0 ] \n 
value = b \n 
~~ if Cache and self . _Cache : \n 
~~~ self . _CacheDict [ h ] = value \n 
~~~ value = unicode ( Set ) \n 
self . _DoCommand ( % ( jarg , value ) , jarg ) \n 
if Cache and self . _Cache : \n 
~~ ~~ ~~ def _Alter ( self , ObjectType , ObjectId , AlterName , Args = None , Reply = None ) : \n 
~~~ cmd = % ( str ( ObjectType ) , str ( ObjectId ) , str ( AlterName ) ) \n 
if Reply is None : \n 
~~~ Reply = cmd \n 
~~ if Args is not None : \n 
~~~ cmd = % ( cmd , tounicode ( Args ) ) \n 
~~ reply = self . _DoCommand ( cmd , Reply ) \n 
arg = cmd . split ( ) \n 
~~~ a , b = chop ( reply ) \n 
reply = b \n 
~~ return reply \n 
~~ def _Search ( self , ObjectType , Args = None ) : \n 
~~~ cmd = % ObjectType \n 
if Args is not None : \n 
~~~ cmd = % ( cmd , Args ) \n 
~~ return split ( chop ( str ( self . _DoCommand ( cmd ) ) ) [ - 1 ] , ) \n 
~~ def ApiSecurityContextEnabled ( self , Context ) : \n 
self . _Api . security_context_enabled ( Context ) \n 
~~ def Application ( self , Name ) : \n 
return Application ( self , Name ) \n 
~~ def _AsyncSearchUsersReplyHandler ( self , Command ) : \n 
~~~ if Command in self . _AsyncSearchUsersCommands : \n 
~~~ self . _AsyncSearchUsersCommands . remove ( Command ) \n 
self . _CallEventHandler ( , Command . Id , \n 
UserCollection ( self , split ( chop ( Command . Reply ) [ - 1 ] , ) ) ) \n 
if len ( self . _AsyncSearchUsersCommands ) == 0 : \n 
~~~ self . UnregisterEventHandler ( , self . _AsyncSearchUsersReplyHandler ) \n 
del self . _AsyncSearchUsersCommands \n 
~~ ~~ ~~ def AsyncSearchUsers ( self , Target ) : \n 
if not hasattr ( self , ) : \n 
~~~ self . _AsyncSearchUsersCommands = [ ] \n 
self . RegisterEventHandler ( , self . _AsyncSearchUsersReplyHandler ) \n 
~~ command = Command ( % tounicode ( Target ) , , False , self . Timeout ) \n 
self . _AsyncSearchUsersCommands . append ( command ) \n 
return command . Id \n 
~~ def Attach ( self , Protocol = 5 , Wait = True ) : \n 
~~~ self . _Api . protocol = Protocol \n 
self . _Api . attach ( self . Timeout , Wait ) \n 
~~ except SkypeAPIError : \n 
~~~ self . ResetCache ( ) \n 
~~ ~~ def Call ( self , Id = 0 ) : \n 
o = Call ( self , Id ) \n 
return o \n 
~~ def Calls ( self , Target = ) : \n 
return CallCollection ( self , self . _Search ( , Target ) ) \n 
~~ def _ChangeUserStatus_UserStatus ( self , Status ) : \n 
~~~ if Status . upper ( ) == self . _ChangeUserStatus_Status : \n 
~~~ self . _ChangeUserStatus_Event . set ( ) \n 
~~ ~~ def ChangeUserStatus ( self , Status ) : \n 
if self . CurrentUserStatus . upper ( ) == Status . upper ( ) : \n 
~~ self . _ChangeUserStatus_Event = threading . Event ( ) \n 
self . _ChangeUserStatus_Status = Status . upper ( ) \n 
self . RegisterEventHandler ( , self . _ChangeUserStatus_UserStatus ) \n 
self . CurrentUserStatus = Status \n 
self . _ChangeUserStatus_Event . wait ( ) \n 
self . UnregisterEventHandler ( , self . _ChangeUserStatus_UserStatus ) \n 
del self . _ChangeUserStatus_Event , self . _ChangeUserStatus_Status \n 
~~ def Chat ( self , Name = ) : \n 
o = Chat ( self , Name ) \n 
~~ def ClearCallHistory ( self , Username = , Type = chsAllCalls ) : \n 
cmd = % ( str ( Type ) , Username ) \n 
self . _DoCommand ( cmd , cmd ) \n 
~~ def ClearChatHistory ( self ) : \n 
cmd = \n 
~~ def ClearVoicemailHistory ( self ) : \n 
self . _DoCommand ( ) \n 
~~ def Command ( self , Command , Reply = , Block = False , Timeout = 30000 , Id = - 1 ) : \n 
from api import Command as CommandClass \n 
return CommandClass ( Command , Reply , Block , Timeout , Id ) \n 
~~ def Conference ( self , Id = 0 ) : \n 
o = Conference ( self , Id ) \n 
if Id <= 0 or not o . Calls : \n 
~~~ raise SkypeError ( 0 , ) \n 
~~ return o \n 
~~ def CreateChatUsingBlob ( self , Blob ) : \n 
return Chat ( self , chop ( self . _DoCommand ( % Blob ) , 2 ) [ 1 ] ) \n 
~~ def CreateChatWith ( self , * Usernames ) : \n 
return Chat ( self , chop ( self . _DoCommand ( % . join ( Usernames ) ) , 2 ) [ 1 ] ) \n 
~~ def CreateGroup ( self , GroupName ) : \n 
groups = self . CustomGroups \n 
self . _DoCommand ( % tounicode ( GroupName ) ) \n 
for g in self . CustomGroups : \n 
~~~ if g not in groups and g . DisplayName == GroupName : \n 
~~~ return g \n 
~~ ~~ raise SkypeError ( 0 , ) \n 
~~ def CreateSms ( self , MessageType , * TargetNumbers ) : \n 
return SmsMessage ( self , chop ( self . _DoCommand ( % ( MessageType , . join ( TargetNumbers \n 
~~ def DeleteGroup ( self , GroupId ) : \n 
self . _DoCommand ( % GroupId ) \n 
~~ def EnableApiSecurityContext ( self , Context ) : \n 
self . _Api . enable_security_context ( Context ) \n 
~~ def FindChatUsingBlob ( self , Blob ) : \n 
~~ def Greeting ( self , Username = ) : \n 
for v in self . Voicemails : \n 
~~~ if Username and v . PartnerHandle != Username : \n 
~~ if v . Type in ( vmtDefaultGreeting , vmtCustomGreeting ) : \n 
~~~ return v \n 
~~ ~~ ~~ def Message ( self , Id = 0 ) : \n 
o = ChatMessage ( self , Id ) \n 
~~ def Messages ( self , Target = ) : \n 
return ChatMessageCollection ( self , self . _Search ( , Target ) ) \n 
~~ def PlaceCall ( self , * Targets ) : \n 
calls = self . ActiveCalls \n 
reply = self . _DoCommand ( % . join ( Targets ) ) \n 
if reply . startswith ( ) : \n 
~~~ return Call ( self , chop ( reply , 2 ) [ 1 ] ) \n 
~~ for c in self . ActiveCalls : \n 
~~~ if c not in calls : \n 
~~~ return c \n 
~~ def Privilege ( self , Name ) : \n 
return ( self . _Property ( , , Name . upper ( ) ) == ) \n 
~~ def Profile ( self , Property , Set = None ) : \n 
return self . _Property ( , , Property , Set ) \n 
~~ def Property ( self , ObjectType , ObjectId , PropName , Set = None ) : \n 
return self . _Property ( ObjectType , ObjectId , PropName , Set ) \n 
~~ def ResetCache ( self ) : \n 
self . _CacheDict = { } \n 
~~ def SearchForUsers ( self , Target ) : \n 
return UserCollection ( self , self . _Search ( , tounicode ( Target ) ) ) \n 
~~ def SendCommand ( self , Command ) : \n 
~~~ self . _Api . send_command ( Command ) \n 
~~ ~~ def SendMessage ( self , Username , Text ) : \n 
return self . CreateChatWith ( Username ) . SendMessage ( Text ) \n 
~~ def SendSms ( self , * TargetNumbers , ** Properties ) : \n 
sms = self . CreateSms ( smsMessageTypeOutgoing , * TargetNumbers ) \n 
for name , value in Properties . items ( ) : \n 
~~~ if isinstance ( getattr ( sms . __class__ , name , None ) , property ) : \n 
~~~ setattr ( sms , name , value ) \n 
~~~ raise TypeError ( % prop ) \n 
~~ ~~ sms . Send ( ) \n 
return sms \n 
~~ def SendVoicemail ( self , Username ) : \n 
if self . _Api . protocol >= 6 : \n 
~~~ self . _DoCommand ( % Username ) \n 
~~ ~~ def User ( self , Username = ) : \n 
if not Username : \n 
~~~ Username = self . CurrentUserHandle \n 
~~ o = User ( self , Username ) \n 
~~ def Variable ( self , Name , Set = None ) : \n 
return self . _Property ( Name , , , Set ) \n 
~~ def Voicemail ( self , Id ) : \n 
o = Voicemail ( self , Id ) \n 
~~ def _GetActiveCalls ( self ) : \n 
~~~ return CallCollection ( self , self . _Search ( ) ) \n 
~~ ActiveCalls = property ( _GetActiveCalls , \n 
def _GetActiveChats ( self ) : \n 
~~~ return ChatCollection ( self , self . _Search ( ) ) \n 
~~ ActiveChats = property ( _GetActiveChats , \n 
def _GetActiveFileTransfers ( self ) : \n 
~~~ return FileTransferCollection ( self , self . _Search ( ) ) \n 
~~ ActiveFileTransfers = property ( _GetActiveFileTransfers , \n 
def _GetApiWrapperVersion ( self ) : \n 
~~~ import pkg_resources \n 
return pkg_resources . get_distribution ( "Skype4Py" ) . version \n 
~~ ApiWrapperVersion = property ( _GetApiWrapperVersion , \n 
def _GetAttachmentStatus ( self ) : \n 
~~~ return self . _Api . attachment_status \n 
~~ AttachmentStatus = property ( _GetAttachmentStatus , \n 
def _GetBookmarkedChats ( self ) : \n 
~~ BookmarkedChats = property ( _GetBookmarkedChats , \n 
def _GetCache ( self ) : \n 
~~~ return self . _Cache \n 
~~ def _SetCache ( self , Value ) : \n 
~~~ self . _Cache = bool ( Value ) \n 
~~ Cache = property ( _GetCache , _SetCache , \n 
def _GetChats ( self ) : \n 
~~ Chats = property ( _GetChats , \n 
def _GetClient ( self ) : \n 
~~~ return self . _Client \n 
~~ Client = property ( _GetClient , \n 
def _GetCommandId ( self ) : \n 
~~ def _SetCommandId ( self , Value ) : \n 
~~~ if not Value : \n 
~~ ~~ CommandId = property ( _GetCommandId , _SetCommandId , \n 
def _GetConferences ( self ) : \n 
~~~ cids = [ ] \n 
for c in self . Calls ( ) : \n 
~~~ cid = c . ConferenceId \n 
if cid > 0 and cid not in cids : \n 
~~~ cids . append ( cid ) \n 
~~ ~~ return ConferenceCollection ( self , cids ) \n 
~~ Conferences = property ( _GetConferences , \n 
def _GetConnectionStatus ( self ) : \n 
~~~ return str ( self . Variable ( ) ) \n 
~~ ConnectionStatus = property ( _GetConnectionStatus , \n 
def _GetConvert ( self ) : \n 
~~~ return self . _Convert \n 
~~ Convert = property ( _GetConvert , \n 
def _GetCurrentUser ( self ) : \n 
~~~ return User ( self , self . CurrentUserHandle ) \n 
~~ CurrentUser = property ( _GetCurrentUser , \n 
def _GetCurrentUserHandle ( self ) : \n 
~~ CurrentUserHandle = property ( _GetCurrentUserHandle , \n 
def _GetCurrentUserProfile ( self ) : \n 
~~~ return self . _Profile \n 
~~ CurrentUserProfile = property ( _GetCurrentUserProfile , \n 
def _GetCurrentUserStatus ( self ) : \n 
~~ def _SetCurrentUserStatus ( self , Value ) : \n 
~~~ self . Variable ( , str ( Value ) ) \n 
~~ CurrentUserStatus = property ( _GetCurrentUserStatus , _SetCurrentUserStatus , \n 
def _GetCustomGroups ( self ) : \n 
~~~ return GroupCollection ( self , self . _Search ( , ) ) \n 
~~ CustomGroups = property ( _GetCustomGroups , \n 
def _GetFileTransfers ( self ) : \n 
~~ FileTransfers = property ( _GetFileTransfers , \n 
def _GetFocusedContacts ( self ) : \n 
~~~ return UserCollection ( self , split ( chop ( self . _DoCommand ( , \n 
~~ FocusedContacts = property ( _GetFocusedContacts , \n 
def _GetFriendlyName ( self ) : \n 
~~~ return self . _Api . friendly_name \n 
~~ def _SetFriendlyName ( self , Value ) : \n 
~~~ self . _Api . set_friendly_name ( tounicode ( Value ) ) \n 
~~ FriendlyName = property ( _GetFriendlyName , _SetFriendlyName , \n 
def _GetFriends ( self ) : \n 
~~~ return UserCollection ( self , self . _Search ( ) ) \n 
~~ Friends = property ( _GetFriends , \n 
def _GetGroups ( self ) : \n 
~~ Groups = property ( _GetGroups , \n 
def _GetHardwiredGroups ( self ) : \n 
~~ HardwiredGroups = property ( _GetHardwiredGroups , \n 
def _GetMissedCalls ( self ) : \n 
~~ MissedCalls = property ( _GetMissedCalls , \n 
def _GetMissedChats ( self ) : \n 
~~ MissedChats = property ( _GetMissedChats , \n 
def _GetMissedMessages ( self ) : \n 
~~~ return ChatMessageCollection ( self , self . _Search ( ) ) \n 
~~ MissedMessages = property ( _GetMissedMessages , \n 
def _GetMissedSmss ( self ) : \n 
~~~ return SmsMessageCollection ( self , self . _Search ( ) ) \n 
~~ MissedSmss = property ( _GetMissedSmss , \n 
def _GetMissedVoicemails ( self ) : \n 
~~~ return VoicemailCollection ( self , self . _Search ( ) ) \n 
~~ MissedVoicemails = property ( _GetMissedVoicemails , \n 
def _GetMute ( self ) : \n 
~~~ return self . Variable ( ) == \n 
~~ def _SetMute ( self , Value ) : \n 
~~~ self . Variable ( , cndexp ( Value , , ) ) \n 
~~ Mute = property ( _GetMute , _SetMute , \n 
def _GetPredictiveDialerCountry ( self ) : \n 
~~ PredictiveDialerCountry = property ( _GetPredictiveDialerCountry , \n 
def _GetProtocol ( self ) : \n 
~~~ return self . _Api . protocol \n 
~~ def _SetProtocol ( self , Value ) : \n 
~~~ self . _DoCommand ( % Value ) \n 
self . _Api . protocol = int ( Value ) \n 
~~ Protocol = property ( _GetProtocol , _SetProtocol , \n 
def _GetRecentChats ( self ) : \n 
~~ RecentChats = property ( _GetRecentChats , \n 
def _GetSettings ( self ) : \n 
~~~ return self . _Settings \n 
~~ Settings = property ( _GetSettings , \n 
def _GetSilentMode ( self ) : \n 
~~~ return self . _Property ( , , , Cache = False ) == \n 
~~ def _SetSilentMode ( self , Value ) : \n 
~~~ self . _Property ( , , , cndexp ( Value , , ) , Cache = False ) \n 
~~ SilentMode = property ( _GetSilentMode , _SetSilentMode , \n 
def _GetSmss ( self ) : \n 
~~ Smss = property ( _GetSmss , \n 
def _GetTimeout ( self ) : \n 
~~~ return self . _Timeout \n 
~~ def _SetTimeout ( self , Value ) : \n 
~~~ if not isinstance ( Value , ( int , long , float ) ) : \n 
~~~ raise TypeError ( % repr ( type ( Value ) ) ) \n 
~~ self . _Timeout = Value \n 
~~ Timeout = property ( _GetTimeout , _SetTimeout , \n 
def _GetUsersWaitingAuthorization ( self ) : \n 
~~ UsersWaitingAuthorization = property ( _GetUsersWaitingAuthorization , \n 
def _GetVersion ( self ) : \n 
~~ Version = property ( _GetVersion , \n 
def _GetVoicemails ( self ) : \n 
~~ Voicemails = property ( _GetVoicemails , \n 
~~ class SkypeEvents ( object ) : \n 
def ApplicationConnecting ( self , App , Users ) : \n 
~~ def ApplicationDatagram ( self , App , Stream , Text ) : \n 
~~ def ApplicationReceiving ( self , App , Streams ) : \n 
~~ def ApplicationSending ( self , App , Streams ) : \n 
~~ def ApplicationStreams ( self , App , Streams ) : \n 
~~ def AsyncSearchUsersFinished ( self , Cookie , Users ) : \n 
~~ def AttachmentStatus ( self , Status ) : \n 
~~ def AutoAway ( self , Automatic ) : \n 
~~ def CallDtmfReceived ( self , Call , Code ) : \n 
~~ def CallHistory ( self ) : \n 
~~ def CallInputStatusChanged ( self , Call , Active ) : \n 
~~ def CallSeenStatusChanged ( self , Call , Seen ) : \n 
~~ def CallStatus ( self , Call , Status ) : \n 
~~ def CallTransferStatusChanged ( self , Call , Status ) : \n 
~~ def CallVideoReceiveStatusChanged ( self , Call , Status ) : \n 
~~ def CallVideoSendStatusChanged ( self , Call , Status ) : \n 
~~ def CallVideoStatusChanged ( self , Call , Status ) : \n 
~~ def ChatMemberRoleChanged ( self , Member , Role ) : \n 
~~ def ChatMembersChanged ( self , Chat , Members ) : \n 
~~ def ChatWindowState ( self , Chat , State ) : \n 
~~ def ClientWindowState ( self , State ) : \n 
~~ def Command ( self , command ) : \n 
~~ def ConnectionStatus ( self , Status ) : \n 
~~ def ContactsFocused ( self , Username ) : \n 
~~ def Error ( self , command , Number , Description ) : \n 
~~ def FileTransferStatusChanged ( self , Transfer , Status ) : \n 
~~ def GroupDeleted ( self , GroupId ) : \n 
~~ def GroupExpanded ( self , Group , Expanded ) : \n 
~~ def GroupUsers ( self , Group , Count ) : \n 
~~ def GroupVisible ( self , Group , Visible ) : \n 
~~ def MessageHistory ( self , Username ) : \n 
~~ def MessageStatus ( self , Message , Status ) : \n 
~~ def Mute ( self , Mute ) : \n 
~~ def Notify ( self , Notification ) : \n 
~~ def OnlineStatus ( self , User , Status ) : \n 
~~ def PluginEventClicked ( self , Event ) : \n 
~~ def PluginMenuItemClicked ( self , MenuItem , Users , PluginContext , ContextId ) : \n 
~~ def Reply ( self , command ) : \n 
~~ def SilentModeStatusChanged ( self , Silent ) : \n 
~~ def SmsMessageStatusChanged ( self , Message , Status ) : \n 
~~ def SmsTargetStatusChanged ( self , Target , Status ) : \n 
~~ def UserAuthorizationRequestReceived ( self , User ) : \n 
~~ def UserMood ( self , User , MoodText ) : \n 
~~ def UserStatus ( self , Status ) : \n 
~~ def VoicemailStatus ( self , Mail , Status ) : \n 
~~ def WallpaperChanged ( self , Path ) : \n 
~~ ~~ Skype . _AddEvents ( SkypeEvents ) \n 
import skype4pytest \n 
from Skype4Py . sms import * \n 
class SmsMessageTest ( skype4pytest . TestCase ) : \n 
~~~ def setUpObject ( self ) : \n 
~~~ self . obj = SmsMessage ( self . skype , ) \n 
~~ def testDelete ( self ) : \n 
~~~ self . api . enqueue ( ) \n 
self . obj . Delete ( ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
~~ def testMarkAsSeen ( self ) : \n 
~~~ self . api . enqueue ( , \n 
self . obj . MarkAsSeen ( ) \n 
~~ def testSend ( self ) : \n 
self . obj . Send ( ) \n 
~~ def testBody ( self ) : \n 
t = self . obj . Body \n 
self . assertInstance ( t , unicode ) \n 
self . assertEqual ( t , ) \n 
self . api . enqueue ( , \n 
self . obj . Body = \n 
~~ def testChunks ( self ) : \n 
t = self . obj . Chunks \n 
self . assertInstance ( t , SmsChunkCollection ) \n 
self . assertEqual ( len ( t ) , 2 ) \n 
~~ def testDatetime ( self ) : \n 
~~~ from datetime import datetime \n 
now = time ( ) \n 
% now ) \n 
t = self . obj . Datetime \n 
self . assertInstance ( t , datetime ) \n 
self . assertEqual ( t , datetime . fromtimestamp ( now ) ) \n 
~~ def testFailureReason ( self ) : \n 
t = self . obj . FailureReason \n 
self . assertInstance ( t , str ) \n 
~~ def testId ( self ) : \n 
~~~ t = self . obj . Id \n 
self . assertInstance ( t , int ) \n 
self . assertEqual ( t , 1234 ) \n 
~~ def testIsFailedUnseen ( self ) : \n 
t = self . obj . IsFailedUnseen \n 
self . assertInstance ( t , bool ) \n 
self . assertEqual ( t , True ) \n 
~~ def testPrice ( self ) : \n 
t = self . obj . Price \n 
self . assertEqual ( t , 123 ) \n 
~~ def testPriceCurrency ( self ) : \n 
t = self . obj . PriceCurrency \n 
~~ def testPricePrecision ( self ) : \n 
t = self . obj . PricePrecision \n 
self . assertEqual ( t , 3 ) \n 
~~ def testPriceToText ( self ) : \n 
t = self . obj . PriceToText \n 
~~ def testPriceValue ( self ) : \n 
t = self . obj . PriceValue \n 
self . assertInstance ( t , float ) \n 
self . assertEqual ( t , 0.123 ) \n 
~~ def testReplyToNumber ( self ) : \n 
t = self . obj . ReplyToNumber \n 
self . obj . ReplyToNumber = \n 
~~ def testSeen ( self ) : \n 
~~~ from warnings import simplefilter \n 
simplefilter ( ) \n 
~~~ self . obj . Seen = True \n 
~~~ simplefilter ( ) \n 
~~ self . failUnless ( self . api . is_empty ( ) ) \n 
~~ def testStatus ( self ) : \n 
t = self . obj . Status \n 
~~ def testTargetNumbers ( self ) : \n 
t = self . obj . TargetNumbers \n 
self . assertInstance ( t , tuple ) \n 
self . obj . TargetNumbers = ( , ) \n 
~~ def testTargets ( self ) : \n 
t = self . obj . Targets \n 
self . assertInstance ( t , SmsTargetCollection ) \n 
~~ def testTimestamp ( self ) : \n 
t = self . obj . Timestamp \n 
self . assertEqual ( t , 123.4 ) \n 
~~ def testType ( self ) : \n 
t = self . obj . Type \n 
~~ ~~ class SmsChunkTest ( skype4pytest . TestCase ) : \n 
~~~ self . obj = SmsChunk ( SmsMessage ( self . skype , ) , 1 ) \n 
~~ def testCharactersLeft ( self ) : \n 
t = self . obj . CharactersLeft \n 
self . assertEqual ( t , 30 ) \n 
self . assertEqual ( t , 1 ) \n 
~~ def testMessage ( self ) : \n 
~~~ t = self . obj . Message \n 
self . assertInstance ( t , SmsMessage ) \n 
self . assertEqual ( t . Id , 1234 ) \n 
~~ def testText ( self ) : \n 
t = self . obj . Text \n 
~~ ~~ class SmsTargetTest ( skype4pytest . TestCase ) : \n 
~~~ self . obj = SmsTarget ( SmsMessage ( self . skype , ) , ) \n 
~~ def testNumber ( self ) : \n 
~~~ t = self . obj . Number \n 
~~ ~~ def suite ( ) : \n 
~~~ return unittest . TestSuite ( [ \n 
unittest . defaultTestLoader . loadTestsFromTestCase ( SmsMessageTest ) , \n 
unittest . defaultTestLoader . loadTestsFromTestCase ( SmsChunkTest ) , \n 
unittest . defaultTestLoader . loadTestsFromTestCase ( SmsTargetTest ) , \n 
~~ from . import convs , widgets , fields \n 
from iktomi . utils . i18n import N_ \n 
class PasswordConv ( convs . Char ) : \n 
~~~ error_mismatch = N_ ( ) \n 
error_required = N_ ( ) \n 
def from_python ( self , value ) : \n 
~~~ return dict ( [ ( field . name , None ) for field in self . field . fields ] ) \n 
~~ def get_initial ( self ) : \n 
~~ def to_python ( self , value ) : \n 
~~~ etalon = value [ list ( value ) [ 0 ] ] \n 
for field in self . field . fields : \n 
~~~ self . assert_ ( value [ field . name ] == etalon , \n 
self . error_mismatch ) \n 
~~ if self . required : \n 
~~~ self . assert_ ( etalon not in ( None , ) , self . error_required ) \n 
~~ elif etalon in ( None , ) : \n 
~~ return etalon \n 
~~ ~~ def PasswordSet ( name = , \n 
min_length = 3 , max_length = 200 , required = False , \n 
password_label = None , confirm_label = , filters = ( ) , \n 
~~~ char = convs . Char ( convs . length ( min_length , max_length ) , * filters , \n 
** dict ( required = required ) ) \n 
items = ( ( , password_label ) , ( , confirm_label ) ) \n 
kwargs [ ] = [ fields . Field ( subfieldname , \n 
conv = char , \n 
label = label , \n 
widget = widgets . PasswordInput ) \n 
for subfieldname , label in items ] \n 
kwargs . setdefault ( , PasswordConv ( required = required ) ) \n 
kwargs . setdefault ( , widgets . FieldSetWidget ( \n 
template = ) ) \n 
return fields . FieldSet ( name , get_initial = lambda : , ** kwargs ) \n 
~~ __all__ = [ , ] \n 
from iktomi . storage import LocalMemStorage , MemcachedStorage \n 
class LocalMemStorageTest ( unittest . TestCase ) : \n 
~~~ def test_set ( self ) : \n 
s = LocalMemStorage ( ) \n 
s . set ( , ) \n 
self . assertEqual ( s . storage [ ] , ) \n 
~~ def test_set_rewrite ( self ) : \n 
~~ def test_get ( self ) : \n 
self . assertEqual ( s . get ( ) , None ) \n 
self . assertEqual ( s . get ( , ) , ) \n 
self . assertEqual ( s . get ( ) , ) \n 
~~ def test_delete ( self ) : \n 
self . assertEqual ( s . delete ( ) , True ) \n 
~~ ~~ class MemcachedStorageTest ( unittest . TestCase ) : \n 
~~~ self . storage = MemcachedStorage ( ) \n 
if not self . storage . storage . set ( , ) : \n 
~~ ~~ def tearDown ( self ) : \n 
~~~ memcached = self . storage . storage \n 
memcached . delete ( ) \n 
memcached . disconnect_all ( ) \n 
~~ def test_set ( self ) : \n 
self . assertEqual ( self . storage . set ( , ) , True ) \n 
self . assertEqual ( self . storage . get ( ) , None ) \n 
self . assertEqual ( self . storage . get ( , ) , ) \n 
self . storage . set ( , ) \n 
self . assertEqual ( self . storage . get ( ) , ) \n 
self . assertEqual ( self . storage . delete ( ) , True ) \n 
name = "SmartlingApiSdk" , \n 
version = "1.2.5" , \n 
author_email = "aartamonov@smartling.com" , \n 
url = "https://docs.smartling.com/display/docs/Files+API" , \n 
packages = [ , "simplejson24" , "example" , "test" ] , \n 
package_data = { \n 
import gc \n 
from datetime import datetime , date , timedelta \n 
from optparse import make_option \n 
from django . core . files . storage import get_storage_class \n 
from django . core . management . base import BaseCommand \n 
from easy_thumbnails . conf import settings \n 
from easy_thumbnails . models import Source \n 
class ThumbnailCollectionCleaner ( object ) : \n 
sources = 0 \n 
thumbnails = 0 \n 
thumbnails_deleted = 0 \n 
source_refs_deleted = 0 \n 
execution_time = 0 \n 
def _get_absolute_path ( self , path ) : \n 
~~~ return os . path . join ( settings . MEDIA_ROOT , path ) \n 
~~ def _get_relative_path ( self , path ) : \n 
~~~ return os . path . relpath ( path , settings . MEDIA_ROOT ) \n 
~~ def _check_if_exists ( self , storage , path ) : \n 
~~~ return storage . exists ( path ) \n 
print ( str ( e ) ) \n 
~~ ~~ def _delete_sources_by_id ( self , ids ) : \n 
~~~ Source . objects . all ( ) . filter ( id__in = ids ) . delete ( ) \n 
~~ def clean_up ( self , dry_run = False , verbosity = 1 , last_n_days = 0 , \n 
cleanup_path = None , storage = None ) : \n 
if dry_run : \n 
~~ if not storage : \n 
~~~ storage = get_storage_class ( settings . THUMBNAIL_DEFAULT_STORAGE ) ( ) \n 
~~ sources_to_delete = [ ] \n 
time_start = time . time ( ) \n 
query = Source . objects . all ( ) \n 
if last_n_days > 0 : \n 
~~~ today = date . today ( ) \n 
query = query . filter ( \n 
modified__range = ( today - timedelta ( days = last_n_days ) , today ) ) \n 
~~ if cleanup_path : \n 
~~~ query = query . filter ( name__startswith = cleanup_path ) \n 
~~ for source in queryset_iterator ( query ) : \n 
~~~ self . sources += 1 \n 
abs_source_path = self . _get_absolute_path ( source . name ) \n 
if not self . _check_if_exists ( storage , abs_source_path ) : \n 
~~~ if verbosity > 0 : \n 
~~ self . source_refs_deleted += 1 \n 
sources_to_delete . append ( source . id ) \n 
for thumb in source . thumbnails . all ( ) : \n 
~~~ self . thumbnails_deleted += 1 \n 
abs_thumbnail_path = self . _get_absolute_path ( thumb . name ) \n 
if self . _check_if_exists ( storage , abs_thumbnail_path ) : \n 
~~~ if not dry_run : \n 
~~~ storage . delete ( abs_thumbnail_path ) \n 
~~ if verbosity > 0 : \n 
~~ ~~ ~~ ~~ if len ( sources_to_delete ) >= 1000 and not dry_run : \n 
~~~ self . _delete_sources_by_id ( sources_to_delete ) \n 
sources_to_delete = [ ] \n 
~~ ~~ if not dry_run : \n 
~~ self . execution_time = round ( time . time ( ) - time_start ) \n 
~~ def print_stats ( self ) : \n 
print ( \n 
"{0:-<48}" . format ( str ( datetime . now ( ) . strftime ( ) ) ) ) \n 
self . thumbnails_deleted ) ) \n 
~~ ~~ def queryset_iterator ( queryset , chunksize = 1000 ) : \n 
primary_key = 0 \n 
last_pk = queryset . order_by ( ) [ 0 ] . pk \n 
queryset = queryset . order_by ( ) \n 
while primary_key < last_pk : \n 
~~~ for row in queryset . filter ( pk__gt = primary_key ) [ : chunksize ] : \n 
~~~ primary_key = row . pk \n 
yield row \n 
~~ gc . collect ( ) \n 
~~ ~~ class Command ( BaseCommand ) : \n 
option_list = BaseCommand . option_list + ( \n 
make_option ( \n 
dest = , \n 
help = ) , \n 
default = 0 , \n 
type = , \n 
def handle ( self , * args , ** options ) : \n 
~~~ tcc = ThumbnailCollectionCleaner ( ) \n 
tcc . clean_up ( \n 
dry_run = options . get ( , False ) , \n 
verbosity = int ( options . get ( , 1 ) ) , \n 
last_n_days = int ( options . get ( , 0 ) ) , \n 
cleanup_path = options . get ( ) ) \n 
tcc . print_stats ( ) \n 
~~~ db . delete_unique ( , [ , ] ) \n 
db . create_unique ( , [ , , ] ) \n 
~~~ db . delete_unique ( , [ , , ] ) \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , } , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : "\'thumbnails\'" : ( , [ ] , { : , } \n 
__credits__ = [ ] \n 
] from django . contrib . auth . models import AbstractBaseUser , PermissionsMixin \n 
from django . contrib . auth . models import UserManager \n 
from django . contrib . flatpages . models import FlatPage \n 
from django . db . models import signals \n 
from django . templatetags . static import static \n 
from django . utils import timezone \n 
from allauth . account . models import EmailAddress \n 
from django_countries . fields import CountryField \n 
from skills . models import Skill , TrainingBit \n 
class CustomUserManager ( UserManager ) : \n 
~~~ def create_superuser ( self , username , password , ** kwargs ) : \n 
~~~ user = self . model ( username = username , is_staff = True , is_superuser = True , ** kwargs ) \n 
user . set_password ( password ) \n 
return user \n 
~~ ~~ class User ( AbstractBaseUser , PermissionsMixin ) : \n 
~~~ USERNAME_FIELD = \n 
datetime_joined = models . DateTimeField ( default = timezone . now ) \n 
username = models . CharField ( max_length = 40 , unique = True , error_messages = { \n 
email = models . CharField ( max_length = 100 , blank = True ) \n 
image = models . ImageField ( upload_to = , null = True , blank = True ) \n 
description = models . TextField ( blank = True ) \n 
skills_in_progress = models . ManyToManyField ( Skill , blank = True , related_name = ) skills_completed = models . ManyToManyField ( Skill , blank = True , related_name = ) \n 
trainingbits_in_progress = models . ManyToManyField ( TrainingBit , blank = True , related_name = trainingbits_completed = models . ManyToManyField ( TrainingBit , blank = True , related_name = \n 
is_active = models . BooleanField ( default = True ) \n 
has_been_welcomed = models . BooleanField ( default = False ) \n 
def is_taking_skill ( self , skill ) : \n 
~~~ return skill in self . skills_in_progress . all ( ) \n 
~~ def complete_skill ( self , completed_skill ) : \n 
~~~ skills = self . complete_skills ( [ complete_skill ] ) \n 
return skills [ 0 ] \n 
~~ def complete_skills ( self , completed_skills ) : \n 
~~~ self . skills_in_progress . remove ( * completed_skills ) \n 
self . skills_completed . add ( * completed_skills ) \n 
return completed_skills \n 
~~ def has_completed_skill ( self , skill ) : \n 
~~~ return skill in self . skills_completed . all ( ) \n 
~~ def is_taking_trainingbit ( self , trainingbit ) : \n 
~~~ return trainingbit in self . trainingbits_in_progress . all ( ) \n 
~~ def complete_trainingbit ( self , completed_trainingbit ) : \n 
~~~ tbs = self . complete_trainingbits ( [ completed_trainingbit ] ) \n 
return tbs [ 0 ] \n 
~~ def complete_trainingbits ( self , completed_trainingbits ) : \n 
~~~ self . trainingbits_in_progress . remove ( * completed_trainingbits ) \n 
self . trainingbits_completed . add ( * completed_trainingbits ) \n 
return completed_trainingbits \n 
~~ def has_completed_trainingbit ( self , trainingbit ) : \n 
~~~ return trainingbit in self . trainingbits_completed . all ( ) \n 
~~ def get_full_name ( self ) : \n 
~~~ return self . email \n 
~~ def get_short_name ( self ) : \n 
~~ objects = CustomUserManager ( ) \n 
def account_verified ( self ) : \n 
~~~ if self . user . is_authenticated : \n 
~~~ result = EmailAddress . objects . get_primary ( self . user ) \n 
if len ( result ) : \n 
~~~ return result [ 0 ] . verified \n 
def is_trainer ( self ) : \n 
~~~ if self . is_admin : \n 
~~ return self . groups . filter ( name = ) \n 
def is_admin ( self ) : \n 
~~~ return self . groups . filter ( name = ) or self . is_superuser \n 
def is_staff ( self ) : \n 
return self . is_admin or self . is_trainer \n 
~~ def get_absolute_url ( self ) : \n 
~~~ return reverse ( , args = [ self . id ] ) \n 
~~ def getImage ( self ) : \n 
~~~ if self . image : \n 
~~~ return self . image . url \n 
~~~ return static ( ) \n 
~~ ~~ ~~ class UserInfo ( models . Model ) : \n 
~~~ SEXES = [ \n 
ORGANISATION_TYPES = [ \n 
sex = models . CharField ( max_length = 20 , choices = SEXES , blank = False ) \n 
country = CountryField ( null = True ) \n 
birthdate = models . DateField ( null = True ) \n 
organization = models . CharField ( max_length = 15 , choices = ORGANISATION_TYPES , blank = False ) \n 
user = models . OneToOneField ( User , null = True , blank = True ) \n 
~~ def get_or_create_userinfo ( user ) : \n 
userinfo , c = UserInfo . objects . get_or_create ( user = user ) \n 
return userinfo \n 
~~ User . userinfo = property ( get_or_create_userinfo ) \n 
class GCLFlatPage ( FlatPage ) : \n 
~~~ show_in_footer = models . BooleanField ( default = False ) \n 
class Meta : \n 
~~~ return reverse ( , args = [ self . url ] ) \n 
~~ ~~ from solo . models import SingletonModel \n 
class SiteConfiguration ( SingletonModel ) : \n 
~~~ analytics_code = models . TextField ( help_text = \n 
~~ class Meta : \n 
~~ ~~ from south . utils import datetime_utils as datetime \n 
from django . utils . text import slugify \n 
~~~ if not db . dry_run : \n 
~~~ for _class in [ orm . Skill , orm . Project , orm . TrainingBit ] : \n 
~~~ for obj in _class . objects . all ( ) : \n 
~~~ objs_with_slug = _class . objects . filter ( slug__exact = obj . slug ) \n 
if obj . slug == : \n 
~~~ obj . slug = slugify ( obj . name ) \n 
~~~ obj . slug = \n 
~~ ~~ if objs_with_slug . count ( ) > 1 : \n 
~~~ existing_slugs = _class . objects . filter ( slug__regex = + obj . slug + ) . values_list ( , flat = True ) \n 
if len ( existing_slugs ) > 0 : \n 
~~~ last_existing_slug = sorted ( existing_slugs ) [ - 1 ] \n 
m = re . match ( , last_existing_slug ) \n 
id_counter = int ( m . group ( 1 ) ) + 1 \n 
~~~ id_counter = 1 \n 
~~ obj . slug = % ( obj . slug , id_counter ) \n 
~~ obj . save ( ) \n 
~~ ~~ ~~ db . create_unique ( , [ ] ) \n 
db . create_unique ( , [ ] ) \n 
~~~ db . delete_unique ( , [ ] ) \n 
db . delete_unique ( , [ ] ) \n 
: ( , [ ] , { : , : } : ( , [ ] , { : , } , \n 
: { : "\'django_content_type\'" , : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : : ( , [ ] , { : ( , [ ] , { : ( , [ ] , { : : ( , [ ] , { : , : } , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : "orm[\'skills.Comment\']" : ( , [ ] , { : "orm[\'skills.Project\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : ( , [ ] , { } ) \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'skills.TrainingBit\']" : ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : : ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : , : ( , [ ] , { } ) \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : : ( , [ ] , { } ) \n 
: { : , : "[\'-created_at\']" } , \n 
: ( , [ ] , { : \'\\\'{"learn":[],"act":[],"share":[]}\\\'\' : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { } ) \n 
~~ from sparkpost import SparkPost \n 
sp = SparkPost ( ) \n 
result = sp . suppression_list . update ( { \n 
print ( result ) \n 
from . utils import wrap_future \n 
from . . transmissions import Transmissions as SyncTransmissions \n 
class Transmissions ( SyncTransmissions ) : \n 
~~~ def get ( self , transmission_id ) : \n 
~~~ results = self . _fetch_get ( transmission_id ) \n 
return wrap_future ( results , lambda f : f [ "transmission" ] ) \n 
from minecraft_data . v1_8 import windows as windows_by_id \n 
from minecraft_data . v1_8 import windows_list \n 
from spockbot . mcdata import constants , get_item_or_block \n 
from spockbot . mcdata . blocks import Block \n 
from spockbot . mcdata . items import Item \n 
from spockbot . mcdata . utils import camel_case , snake_case \n 
def make_slot_check ( wanted ) : \n 
if isinstance ( wanted , types . FunctionType ) : \n 
~~ if isinstance ( wanted , int ) : \n 
~~~ item , meta = wanted , None \n 
~~ elif isinstance ( wanted , Slot ) : \n 
~~ elif isinstance ( wanted , ( Item , Block ) ) : \n 
~~~ item , meta = wanted . id , wanted . metadata \n 
~~ elif isinstance ( wanted , str ) : \n 
~~~ item_or_block = get_item_or_block ( wanted , init = True ) \n 
item , meta = item_or_block . id , item_or_block . metadata \n 
~~~ item , meta = wanted \n 
~~~ raise ValueError ( % wanted ) \n 
~~ ~~ return lambda slot : item == slot . item_id and meta in ( None , slot . damage ) \n 
~~ class Slot ( object ) : \n 
~~~ def __init__ ( self , window , slot_nr , id = constants . INV_ITEMID_EMPTY , \n 
damage = 0 , amount = 0 , enchants = None ) : \n 
~~~ self . window = window \n 
self . slot_nr = slot_nr \n 
self . item_id = id \n 
self . damage = damage \n 
self . amount = amount \n 
self . nbt = enchants \n 
self . item = get_item_or_block ( self . item_id , self . damage ) or Item ( ) \n 
~~ def move_to_window ( self , window , slot_nr ) : \n 
~~~ self . window , self . slot_nr = window , slot_nr \n 
def is_empty ( self ) : \n 
~~~ return self . amount <= 0 \n 
~~ def matches ( self , other ) : \n 
~~~ return make_slot_check ( other ) ( self ) \n 
~~ def stacks_with ( self , other ) : \n 
~~~ if self . item_id != other . item_id : \n 
~~ if self . damage != other . damage : \n 
~~ return self . item . stack_size != 1 \n 
~~ def get_dict ( self ) : \n 
data = { : self . item_id } \n 
if self . item_id != constants . INV_ITEMID_EMPTY : \n 
~~~ data [ ] = self . damage \n 
data [ ] = self . amount \n 
if self . nbt is not None : \n 
~~~ data [ ] = self . nbt \n 
~~ ~~ return data \n 
~~ def copy ( self ) : \n 
~~~ return Slot ( self . window , self . slot_nr , self . item_id , \n 
self . damage , self . amount , self . nbt ) \n 
~~ def __bool__ ( self ) : \n 
~~~ return not self . is_empty \n 
~~~ if self . is_empty : \n 
~~~ s = \n 
~~~ item = self . item \n 
s = % ( self . amount , item . stack_size , str ( item ) ) \n 
~~ if self . slot_nr != - 1 : \n 
~~~ s += % self . slot_nr \n 
~~ if self . window : \n 
~~~ s += % self . window \n 
~~ return % s \n 
~~ ~~ class SlotCursor ( Slot ) : \n 
~~~ def __init__ ( self , id = constants . INV_ITEMID_EMPTY , damage = 0 , amount = 0 , \n 
enchants = None ) : \n 
~~~ window_id = constants . INV_WINID_CURSOR \n 
def __repr__ ( self ) : \n 
~~ ~~ super ( SlotCursor , self ) . __init__ ( \n 
CursorWindow ( ) , constants . INV_SLOT_NR_CURSOR , \n 
id , damage , amount , enchants ) \n 
~~ ~~ class BaseClick ( object ) : \n 
~~~ def get_packet ( self , inv_plugin ) : \n 
~~ def apply ( self , inv_plugin ) : \n 
~~ def on_success ( self , inv_plugin , emit_set_slot ) : \n 
self . dirty = set ( ) \n 
self . apply ( inv_plugin ) \n 
for changed_slot in self . dirty : \n 
~~~ emit_set_slot ( changed_slot ) \n 
~~ ~~ def copy_slot_type ( self , slot_from , slot_to ) : \n 
~~~ slot_to . item_id , slot_to . damage = slot_from . item_id , slot_from . damage \n 
slot_to . nbt = slot_from . nbt \n 
self . mark_dirty ( slot_to ) \n 
~~ def swap_slots ( self , slot_a , slot_b ) : \n 
~~~ slot_a . item_id , slot_b . item_id = slot_b . item_id , slot_a . item_id \n 
slot_a . damage , slot_b . damage = slot_b . damage , slot_a . damage \n 
slot_a . amount , slot_b . amount = slot_b . amount , slot_a . amount \n 
slot_a . nbt , slot_b . nbt = slot_b . nbt , slot_a . nbt \n 
self . mark_dirty ( slot_a ) \n 
self . mark_dirty ( slot_b ) \n 
~~ def transfer ( self , from_slot , to_slot , max_amount ) : \n 
~~~ transfer_amount = min ( max_amount , from_slot . amount , \n 
to_slot . item . stack_size - to_slot . amount ) \n 
if transfer_amount <= 0 : \n 
~~ self . copy_slot_type ( from_slot , to_slot ) \n 
to_slot . amount += transfer_amount \n 
from_slot . amount -= transfer_amount \n 
self . cleanup_if_empty ( from_slot ) \n 
~~ def cleanup_if_empty ( self , slot ) : \n 
~~~ if slot . is_empty : \n 
~~~ empty_slot_at_same_position = Slot ( slot . window , slot . slot_nr ) \n 
self . copy_slot_type ( empty_slot_at_same_position , slot ) \n 
~~ self . mark_dirty ( slot ) \n 
~~ def mark_dirty ( self , slot ) : \n 
~~~ self . dirty . add ( slot ) \n 
~~ ~~ class SingleClick ( BaseClick ) : \n 
~~~ def __init__ ( self , slot , button = constants . INV_BUTTON_LEFT ) : \n 
~~~ self . slot = slot \n 
self . button = button \n 
if button not in ( constants . INV_BUTTON_LEFT , \n 
constants . INV_BUTTON_RIGHT ) : \n 
~~~ raise NotImplementedError ( \n 
% button ) \n 
~~ ~~ def get_packet ( self , inv_plugin ) : \n 
~~~ slot_nr = self . slot . slot_nr \n 
if self . slot == inv_plugin . cursor_slot : \n 
~~~ slot_nr = constants . INV_OUTSIDE_WINDOW \n 
~~ return { \n 
: slot_nr , \n 
: self . button , \n 
: self . slot . get_dict ( ) , \n 
~~~ clicked = self . slot \n 
cursor = inv_plugin . cursor_slot \n 
if clicked == cursor : \n 
~~~ if self . button == constants . INV_BUTTON_LEFT : \n 
~~~ clicked . amount = 0 \n 
~~ elif self . button == constants . INV_BUTTON_RIGHT : \n 
~~~ clicked . amount -= 1 \n 
~~ self . cleanup_if_empty ( clicked ) \n 
~~ elif self . button == constants . INV_BUTTON_LEFT : \n 
~~~ if clicked . stacks_with ( cursor ) : \n 
~~~ self . transfer ( cursor , clicked , cursor . amount ) \n 
~~~ self . swap_slots ( cursor , clicked ) \n 
~~ ~~ elif self . button == constants . INV_BUTTON_RIGHT : \n 
~~~ if cursor . is_empty : \n 
~~~ self . transfer ( clicked , cursor , ( clicked . amount + 1 ) // 2 ) \n 
~~ elif clicked . is_empty or clicked . stacks_with ( cursor ) : \n 
~~~ self . transfer ( cursor , clicked , 1 ) \n 
% self . button ) \n 
~~ ~~ ~~ class DropClick ( BaseClick ) : \n 
~~~ def __init__ ( self , slot , drop_stack = False ) : \n 
self . drop_stack = drop_stack \n 
~~ def get_packet ( self , inv_plugin ) : \n 
~~~ if self . slot == inv_plugin . cursor_slot : \n 
~~ if not inv_plugin . cursor_slot . is_empty : \n 
: self . slot . slot_nr , \n 
: 1 if self . drop_stack else 0 , \n 
: 4 , \n 
: inv_plugin . cursor_slot . get_dict ( ) , \n 
~~~ if self . drop_stack : \n 
~~~ self . slot . amount = 0 \n 
~~~ self . slot . amount -= 1 \n 
~~ self . cleanup_if_empty ( self . slot ) \n 
~~ ~~ class Window ( object ) : \n 
name = None \n 
inv_type = None \n 
inv_data = { } \n 
def __init__ ( self , window_id , title , slot_count , \n 
inv_type = None , persistent_slots = None , eid = None ) : \n 
~~~ assert not inv_type or inv_type == self . inv_type , % ( inv_type , self . inv_type ) \n 
~~~ window_dict = windows_by_id [ inv_type ] \n 
if in window_dict : \n 
~~~ slot_count = max ( slot [ ] + slot . get ( , 1 ) \n 
for slot in window_dict [ ] ) \n 
~~ ~~ self . window_id = window_id \n 
self . title = title \n 
self . slots = [ Slot ( self , slot_nr ) for slot_nr in range ( slot_count ) ] \n 
if persistent_slots is None : \n 
~~~ for slot_nr in range ( constants . INV_SLOTS_PERSISTENT ) : \n 
~~~ self . slots . append ( Slot ( self , slot_nr + slot_count ) ) \n 
~~~ moved_slots = persistent_slots [ - constants . INV_SLOTS_PERSISTENT : ] \n 
for slot_nr , moved_slot in enumerate ( moved_slots ) : \n 
~~~ moved_slot . move_to_window ( self , slot_nr + slot_count ) \n 
self . slots . append ( moved_slot ) \n 
~~ ~~ self . properties = { } \n 
self . __class__ . __name__ , \n 
self . window_id , self . title , len ( self . slots ) ) \n 
def persistent_slots ( self ) : \n 
~~~ return self . slots [ - constants . INV_SLOTS_PERSISTENT : ] \n 
def inventory_slots ( self ) : \n 
~~~ return self . slots [ \n 
- constants . INV_SLOTS_PERSISTENT : - constants . INV_SLOTS_HOTBAR ] \n 
def hotbar_slots ( self ) : \n 
~~~ return self . slots [ - constants . INV_SLOTS_HOTBAR : ] \n 
def window_slots ( self ) : \n 
return self . slots [ : - constants . INV_SLOTS_PERSISTENT ] \n 
~~ ~~ def _make_window ( window_dict ) : \n 
cls_name = % camel_case ( str ( window_dict [ ] ) ) \n 
bases = ( Window , ) \n 
attrs = { \n 
: sys . modules [ __name__ ] , \n 
: str ( window_dict [ ] ) , \n 
: window_dict , \n 
def make_slot_method ( index , size = 1 ) : \n 
~~~ if size == 1 : \n 
~~~ return lambda self : self . slots [ index ] \n 
~~~ return lambda self : self . slots [ index : ( index + size ) ] \n 
~~ ~~ for slots in window_dict . get ( , [ ] ) : \n 
~~~ index = slots [ ] \n 
size = slots . get ( , 1 ) \n 
attr_name = snake_case ( str ( slots [ ] ) ) \n 
attr_name += if size == 1 else \n 
slots_method = make_slot_method ( index , size ) \n 
slots_method . __name__ = attr_name \n 
attrs [ attr_name ] = property ( slots_method ) \n 
~~ for i , prop_name in enumerate ( window_dict . get ( , [ ] ) ) : \n 
~~~ def make_prop_method ( i ) : \n 
~~~ return lambda self : self . properties [ i ] \n 
~~ prop_method = make_prop_method ( i ) \n 
prop_name = snake_case ( str ( prop_name ) ) \n 
prop_method . __name__ = prop_name \n 
attrs [ prop_name ] = property ( prop_method ) \n 
~~ cls = type ( cls_name , bases , attrs ) \n 
setattr ( sys . modules [ __name__ ] , cls_name , cls ) \n 
~~ inv_types = { } \n 
def _create_windows ( ) : \n 
~~~ for window in windows_list : \n 
~~~ cls = _make_window ( window ) \n 
inv_types [ cls . inv_type ] = cls \n 
~~ ~~ _create_windows ( ) \n 
_player_window = sys . modules [ __name__ ] . PlayerWindow \n 
def _player_init ( self , * args , ** kwargs ) : \n 
~~~ super ( _player_window , self ) . __init__ ( \n 
constants . INV_WINID_PLAYER , self . name , constants . INV_SLOTS_PLAYER , \n 
* args , ** kwargs ) \n 
~~ setattr ( _player_window , , _player_init ) \n 
from spockbot . mcdata import blocks , constants as const \n 
from spockbot . mcdata . utils import BoundingBox \n 
from spockbot . plugins . base import PluginBase , pl_announce \n 
from spockbot . plugins . tools import collision \n 
from spockbot . vector import Vector3 \n 
FP_MAGIC = 1e-4 \n 
class PhysicsCore ( object ) : \n 
~~~ def __init__ ( self , pos , vec , abilities ) : \n 
~~~ self . pos = pos \n 
self . vec = vec \n 
self . sprinting = False \n 
self . move_accel = abilities . walking_speed \n 
self . abilities = abilities \n 
self . direction = Vector3 ( ) \n 
~~ def jump ( self ) : \n 
~~~ if self . pos . on_ground : \n 
~~~ if self . sprinting : \n 
~~~ ground_speed = Vector3 ( self . vec . x , 0 , self . vec . z ) \n 
if ground_speed : \n 
~~~ self . vec += ground_speed . norm ( ) * const . PHY_JMP_MUL \n 
~~ ~~ self . vec . y = const . PHY_JMP_ABS \n 
~~ ~~ def walk ( self ) : \n 
~~~ self . sprinting = False \n 
self . move_accel = self . abilities . walking_speed \n 
~~ def sprint ( self ) : \n 
~~~ self . sprinting = True \n 
self . move_accel = self . abilities . walking_speed * const . PHY_SPR_MUL \n 
~~ def move_target ( self , vector ) : \n 
~~~ self . direction = vector - self . pos \n 
self . direction . y = 0 \n 
if self . direction <= Vector3 ( self . vec . x , 0 , self . vec . z ) : \n 
~~ ~~ def move_vector ( self , vector ) : \n 
~~~ vector . y = 0 \n 
self . direction = vector \n 
~~ def move_angle ( self , angle , radians = False ) : \n 
~~~ angle = angle if radians else math . radians ( angle ) \n 
self . direction = Vector3 ( math . sin ( angle ) , 0 , math . cos ( angle ) ) \n 
~~ ~~ @ pl_announce ( ) \n 
class PhysicsPlugin ( PluginBase ) : \n 
~~~ requires = ( , , , ) \n 
events = { \n 
def __init__ ( self , ploader , settings ) : \n 
~~~ super ( PhysicsPlugin , self ) . __init__ ( ploader , settings ) \n 
self . vec = Vector3 ( 0.0 , 0.0 , 0.0 ) \n 
self . col = collision . MTVTest ( \n 
self . world , BoundingBox ( const . PLAYER_WIDTH , const . PLAYER_HEIGHT ) \n 
self . pos = self . clientinfo . position \n 
self . skip_tick = False \n 
self . pc = PhysicsCore ( self . pos , self . vec , self . clientinfo . abilities ) \n 
ploader . provides ( , self . pc ) \n 
~~ def skip_physics ( self , _ = None , __ = None ) : \n 
~~~ self . vec . zero ( ) \n 
self . skip_tick = True \n 
~~ def suspend_physics ( self , _ = None , __ = None ) : \n 
self . event . unreg_event_handler ( , self . physics_tick ) \n 
~~ def resume_physics ( self , _ = None , __ = None ) : \n 
~~~ self . event . reg_event_handler ( , self . physics_tick ) \n 
~~ def client_tick ( self , name , data ) : \n 
~~~ self . net . push_packet ( , \n 
self . clientinfo . position . get_dict ( ) ) \n 
~~ def physics_tick ( self , _ , __ ) : \n 
~~~ if self . skip_tick : \n 
~~~ self . skip_tick = False \n 
~~ self . apply_accel ( ) \n 
mtv = self . get_mtv ( ) \n 
self . apply_vector ( mtv ) \n 
self . pos . on_ground = mtv . y > 0 \n 
self . vec -= Vector3 ( 0 , const . PHY_GAV_ACC , 0 ) \n 
self . apply_drag ( ) \n 
self . pc . direction = Vector3 ( ) \n 
~~ def get_block_slip ( self ) : \n 
~~~ bpos = self . pos . floor ( ) \n 
return blocks . get_block ( * self . world . get_block ( * bpos ) ) . slipperiness \n 
~~ return 1 \n 
~~ def apply_accel ( self ) : \n 
~~~ if not self . pc . direction : \n 
~~ if self . pos . on_ground : \n 
~~~ block_slip = self . get_block_slip ( ) \n 
accel_mod = const . BASE_GND_SLIP ** 3 / block_slip ** 3 \n 
accel = self . pc . move_accel * accel_mod * const . PHY_BASE_DRG \n 
~~~ accel = const . PHY_JMP_ACC \n 
~~ self . vec += self . pc . direction . norm ( ) * accel \n 
~~ def apply_vector ( self , mtv ) : \n 
~~~ self . pos += ( self . vec + mtv ) \n 
self . vec . x = 0 if mtv . x else self . vec . x \n 
self . vec . y = 0 if mtv . y else self . vec . y \n 
self . vec . z = 0 if mtv . z else self . vec . z \n 
~~ def apply_drag ( self ) : \n 
~~~ drag = self . get_block_slip ( ) * const . PHY_DRG_MUL \n 
self . vec . x *= drag \n 
self . vec . z *= drag \n 
self . vec . y *= const . PHY_BASE_DRG \n 
~~ def get_mtv ( self ) : \n 
~~~ pos = self . pos + self . vec \n 
pos = collision . uncenter_position ( pos , self . col . bbox ) \n 
q = collections . deque ( ( Vector3 ( ) , ) ) \n 
while q : \n 
~~~ current_vector = q . popleft ( ) \n 
transform_vectors = self . col . check_collision ( pos , current_vector ) \n 
if not all ( transform_vectors ) : \n 
~~ for vector in transform_vectors : \n 
~~~ test_vec = self . vec + current_vector + vector \n 
if test_vec . dist_sq ( ) <= self . vec . dist_sq ( ) + FP_MAGIC : \n 
~~~ q . append ( current_vector + vector ) \n 
~~~ logger . debug ( ) \n 
self . vec . zero ( ) \n 
return Vector3 ( ) \n 
~~ possible_mtv = [ current_vector ] \n 
~~~ possible_mtv . append ( current_vector ) \n 
~~ ~~ return min ( possible_mtv ) \n 
from beaker . middleware import SessionMiddleware \n 
from paste . cascade import Cascade \n 
from paste . registry import RegistryManager \n 
from paste . urlparser import StaticURLParser \n 
from paste . deploy . converters import asbool \n 
from pylons . middleware import ErrorHandler , StatusCodeRedirect \n 
from pylons . wsgiapp import PylonsApp \n 
from routes . middleware import RoutesMiddleware \n 
from nipapwww . config . environment import load_environment \n 
def make_app ( global_conf , full_stack = True , static_files = True , ** app_conf ) : \n 
config = load_environment ( global_conf , app_conf ) \n 
app = PylonsApp ( config = config ) \n 
app = RoutesMiddleware ( app , config [ ] , singleton = False ) \n 
app = SessionMiddleware ( app , config ) \n 
if asbool ( full_stack ) : \n 
~~~ app = ErrorHandler ( app , global_conf , ** config [ ] ) \n 
if asbool ( config [ ] ) : \n 
~~~ app = StatusCodeRedirect ( app ) \n 
~~~ app = StatusCodeRedirect ( app , [ 400 , 401 , 403 , 404 , 500 ] ) \n 
~~ ~~ app = RegistryManager ( app ) \n 
if asbool ( static_files ) : \n 
~~~ static_app = StaticURLParser ( config [ ] [ ] ) \n 
app = Cascade ( [ static_app , app ] ) \n 
~~ app . config = config \n 
return app \n 
~~ class NipapError ( Exception ) : \n 
error_code = 1000 \n 
~~ class NipapInputError ( NipapError ) : \n 
error_code = 1100 \n 
~~ class NipapMissingInputError ( NipapInputError ) : \n 
error_code = 1110 \n 
~~ class NipapExtraneousInputError ( NipapInputError ) : \n 
error_code = 1120 \n 
~~ class NipapNoSuchOperatorError ( NipapInputError ) : \n 
error_code = 1130 \n 
~~ class NipapValueError ( NipapError ) : \n 
error_code = 1200 \n 
~~ class NipapNonExistentError ( NipapError ) : \n 
error_code = 1300 \n 
~~ class NipapDuplicateError ( NipapError ) : \n 
error_code = 1400 \n 
__version__ = "0.28.4" \n 
__license__ = "MIT" \n 
__status__ = "Development" \n 
__url__ = "http://SpriteLink.github.com/NIPAP" \n 
UMASK = 0 \n 
WORKDIR = "/" \n 
MAXFD = 1024 \n 
if ( hasattr ( os , "devnull" ) ) : \n 
~~~ REDIRECT_TO = os . devnull \n 
~~~ REDIRECT_TO = "/dev/null" \n 
~~ def createDaemon ( ) : \n 
~~~ pid = os . fork ( ) \n 
~~ except OSError , e : \n 
~~~ os . setsid ( ) \n 
~~~ os . chdir ( WORKDIR ) \n 
os . umask ( UMASK ) \n 
maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 1 ] \n 
if ( maxfd == resource . RLIM_INFINITY ) : \n 
~~~ maxfd = MAXFD \n 
return ( 0 ) \n 
~~ def drop_privileges ( uid_name = , gid_name = ) : \n 
~~~ if os . getuid ( ) != 0 : \n 
~~ import pwd , grp \n 
uid = pwd . getpwnam ( uid_name ) . pw_uid \n 
gid = grp . getgrnam ( gid_name ) . gr_gid \n 
os . setgroups ( [ ] ) \n 
os . setgid ( gid ) \n 
os . setuid ( uid ) \n 
old_umask = os . umask ( 0 77 ) \n 
import urllib . request \n 
import urllib . error \n 
import urllib . parse \n 
from . exceptions import InvalidFunctionError \n 
ERR_PARSE = - 32700 \n 
ERR_INVALID_REQ = - 32600 \n 
ERR_METHOD_NOT_FOUND = - 32601 \n 
ERR_INVALID_PARAMS = - 32602 \n 
ERR_INTERNAL = - 32603 \n 
ERR_UNKNOWN = - 32000 \n 
ERR_INVALID_RESP = - 32001 \n 
def contract_from_file ( fname ) : \n 
f = open ( fname ) \n 
j = f . read ( ) \n 
return Contract ( json . loads ( j ) ) \n 
~~ def unpack_method ( method ) : \n 
pos = method . find ( "." ) \n 
if pos == - 1 : \n 
~~ iface_name = method [ : pos ] \n 
func_name = method [ pos + 1 : ] \n 
return iface_name , func_name \n 
~~ def idgen_uuid ( ) : \n 
return uuid . uuid4 ( ) . hex \n 
~~ idgen_seq_counter = itertools . count ( ) \n 
def idgen_seq ( ) : \n 
return str ( next ( idgen_seq_counter ) ) \n 
~~ def err_response ( reqid , code , msg , data = None ) : \n 
err = { "code" : code , "message" : msg } \n 
if data : \n 
~~~ err [ "data" ] = data \n 
~~ return { "jsonrpc" : "2.0" , "id" : reqid , "error" : err } \n 
~~ def safe_get ( d , key , def_val = None ) : \n 
if key in d : \n 
~~~ return d [ key ] \n 
~~~ return def_val \n 
~~ ~~ class RpcException ( Exception , json . JSONEncoder ) : \n 
def __init__ ( self , code , msg = "" , data = None ) : \n 
if self . data : \n 
~~ return s \n 
~~ ~~ class RequestContext ( object ) : \n 
def __init__ ( self , props , req ) : \n 
self . props = props \n 
self . request = req \n 
self . response = None \n 
self . error = None \n 
~~ def func_name ( self ) : \n 
~~~ return unpack_method ( self . request [ "method" ] ) [ 1 ] \n 
~~ def get_prop ( self , key , default_val = None ) : \n 
if key in self . props : \n 
~~~ return self . props [ key ] \n 
~~~ return default_val \n 
~~ ~~ def set_error ( self , code , msg , data = None ) : \n 
self . error = err_response ( self . request [ "id" ] , code , msg , data ) \n 
~~ ~~ class Filter ( object ) : \n 
def pre ( self , context ) : \n 
~~ def post ( self , context ) : \n 
~~ ~~ class Server ( object ) : \n 
def __init__ ( self , contract , validate_request = True , validate_response = True ) : \n 
logging . basicConfig ( ) \n 
self . log = logging . getLogger ( "common.barrister" ) \n 
self . validate_req = validate_request \n 
self . validate_resp = validate_response \n 
self . contract = contract \n 
self . filters = None \n 
~~ def add_handler ( self , iface_name , handler ) : \n 
if self . contract . has_interface ( iface_name ) : \n 
~~~ self . handlers [ iface_name ] = handler \n 
~~ ~~ def set_filters ( self , filters ) : \n 
if filters is None or isinstance ( filters , ( tuple , list ) ) : \n 
~~~ self . filters = filters \n 
~~~ self . filters = [ filters ] \n 
~~ ~~ def call_json ( self , req_json , props = None ) : \n 
~~~ req = json . loads ( req_json ) \n 
return json . dumps ( err_response ( None , - 32700 , msg ) ) \n 
~~ return json . dumps ( self . call ( req , props ) ) \n 
~~ def call ( self , req , props = None ) : \n 
resp = None \n 
if self . log . isEnabledFor ( logging . DEBUG ) : \n 
~~ if isinstance ( req , list ) : \n 
~~~ if len ( req ) < 1 : \n 
~~~ resp = [ self . _call_and_format ( r , props ) for r in req ] \n 
~~~ resp = self . _call_and_format ( req , props ) \n 
~~ if self . log . isEnabledFor ( logging . DEBUG ) : \n 
~~ return resp \n 
~~ def _call_and_format ( self , req , props = None ) : \n 
if not isinstance ( req , dict ) : \n 
~~~ return err_response ( None , ERR_INVALID_REQ , \n 
~~ reqid = None \n 
if "id" in req : \n 
~~~ reqid = req [ "id" ] \n 
~~ if props is None : \n 
~~~ props = { } \n 
~~ context = RequestContext ( props , req ) \n 
if self . filters : \n 
~~~ for f in self . filters : \n 
~~~ f . pre ( context ) \n 
~~ ~~ if context . error : \n 
~~~ return context . error \n 
~~ resp = None \n 
~~~ result = self . _call ( context ) \n 
resp = { "jsonrpc" : "2.0" , "id" : reqid , "result" : result } \n 
~~ except RpcException as e : \n 
~~~ resp = err_response ( reqid , e . code , e . msg , e . data ) \n 
data = { \n 
: str ( e ) \n 
~~ if self . filters : \n 
~~~ context . response = resp \n 
for f in self . filters : \n 
~~~ f . post ( context ) \n 
~~ ~~ return resp \n 
~~ def _call ( self , context ) : \n 
req = context . request \n 
if "method" not in req : \n 
~~ method = req [ "method" ] \n 
if method == "common.barrister-idl" or method == "getIdl" : \n 
~~~ return self . contract . idl_parsed \n 
~~ iface_name , func_name = unpack_method ( method ) \n 
if iface_name in self . handlers : \n 
~~~ iface_impl = self . handlers [ iface_name ] \n 
func = getattr ( iface_impl , func_name ) \n 
~~~ if "params" in req : \n 
~~~ params = req [ "params" ] \n 
~~~ params = [ ] \n 
~~ if self . validate_req : \n 
~~~ self . contract . validate_request ( iface_name , func_name , params ) \n 
~~ if hasattr ( iface_impl , "barrister_pre" ) : \n 
~~~ pre_hook = getattr ( iface_impl , "barrister_pre" ) \n 
pre_hook ( context , params ) \n 
~~ if params : \n 
~~~ result = func ( * params ) \n 
~~~ result = func ( ) \n 
~~ if self . validate_resp : \n 
~~~ self . contract . validate_response ( iface_name , func_name , result ) \n 
raise RpcException ( ERR_METHOD_NOT_FOUND , msg ) \n 
~~ ~~ ~~ class HttpTransport ( object ) : \n 
def __init__ ( self , url , handlers = None , headers = None ) : \n 
if not headers : \n 
~~ headers [ ] = \n 
self . url = url \n 
self . headers = headers \n 
if handlers : \n 
~~~ self . opener = urllib . request . build_opener ( * handlers ) \n 
~~~ self . opener = urllib . request . build_opener ( ) \n 
~~ ~~ def request ( self , req ) : \n 
data = json . dumps ( req ) \n 
req = urllib . request . Request ( self . url , data , self . headers ) \n 
f = self . opener . open ( req ) \n 
resp = f . read ( ) \n 
return json . loads ( resp ) \n 
~~ ~~ class InProcTransport ( object ) : \n 
def __init__ ( self , server ) : \n 
~~ def request ( self , req ) : \n 
return self . server . call ( req ) \n 
~~ ~~ class Client ( object ) : \n 
def __init__ ( self , transport , validate_request = True , validate_response = True , \n 
id_gen = idgen_uuid ) : \n 
self . transport = transport \n 
self . id_gen = id_gen \n 
req = { "jsonrpc" : "2.0" , "method" : "common.barrister-idl" , "id" : "1" } \n 
resp = transport . request ( req ) \n 
self . contract = Contract ( resp [ "result" ] ) \n 
for k , v in list ( self . contract . interfaces . items ( ) ) : \n 
~~~ setattr ( self , k , InterfaceClientProxy ( self , v ) ) \n 
~~ ~~ def get_meta ( self ) : \n 
return self . contract . meta \n 
~~ def call ( self , iface_name , func_name , params ) : \n 
req = self . to_request ( iface_name , func_name , params ) \n 
~~ resp = self . transport . request ( req ) \n 
~~ return self . to_result ( iface_name , func_name , resp ) \n 
~~ def to_request ( self , iface_name , func_name , params ) : \n 
if self . validate_req : \n 
~~ method = "%s.%s" % ( iface_name , func_name ) \n 
reqid = self . id_gen ( ) \n 
return { "jsonrpc" : "2.0" , "id" : reqid , "method" : method , "params" : params } \n 
~~ def to_result ( self , iface_name , func_name , resp ) : \n 
if "error" in resp : \n 
~~~ e = resp [ "error" ] \n 
data = None \n 
if "data" in e : \n 
~~~ data = e [ "data" ] \n 
~~ raise RpcException ( e [ "code" ] , e [ "message" ] , data ) \n 
~~ result = resp [ "result" ] \n 
if self . validate_resp : \n 
~~ def start_batch ( self ) : \n 
return Batch ( self ) \n 
~~ ~~ class InterfaceClientProxy ( object ) : \n 
def __init__ ( self , client , iface ) : \n 
self . client = client \n 
iface_name = iface . name \n 
for func_name , func in list ( iface . functions . items ( ) ) : \n 
~~~ setattr ( self , func_name , self . _caller ( iface_name , func_name ) ) \n 
~~ ~~ def _caller ( self , iface_name , func_name ) : \n 
def caller ( * params ) : \n 
~~~ return self . client . call ( iface_name , func_name , params ) \n 
~~ return caller \n 
~~ ~~ class Batch ( object ) : \n 
def __init__ ( self , client ) : \n 
self . req_list = [ ] \n 
self . sent = False \n 
for k , v in list ( client . contract . interfaces . items ( ) ) : \n 
~~ ~~ def call ( self , iface_name , func_name , params ) : \n 
if self . sent : \n 
~~~ req = self . client . to_request ( iface_name , func_name , params ) \n 
self . req_list . append ( req ) \n 
~~ ~~ def send ( self ) : \n 
~~~ self . sent = True \n 
results = self . client . transport . request ( self . req_list ) \n 
id_to_method = { } \n 
by_id = { } \n 
for res in results : \n 
~~~ reqid = res [ "id" ] \n 
by_id [ reqid ] = res \n 
~~ in_req_order = [ ] \n 
for req in self . req_list : \n 
result = None \n 
error = None \n 
resp = safe_get ( by_id , reqid ) \n 
if resp is None : \n 
error = RpcException ( ERR_INVALID_RESP , msg ) \n 
~~~ r_err = safe_get ( resp , "error" ) \n 
if r_err is None : \n 
~~~ result = resp [ "result" ] \n 
~~~ error = RpcException ( r_err [ "code" ] , r_err [ "message" ] , safe_get ( r_err , "data" ~~ ~~ in_req_order . append ( RpcResponse ( req , result , error ) ) \n 
~~ return in_req_order \n 
~~ ~~ ~~ class RpcResponse ( object ) : \n 
def __init__ ( self , request , result , error ) : \n 
~~~ self . request = request \n 
self . result = result \n 
self . error = error \n 
~~ ~~ class Contract ( object ) : \n 
def __init__ ( self , idl_parsed ) : \n 
self . idl_parsed = idl_parsed \n 
self . interfaces = { } \n 
self . structs = { } \n 
self . enums = { } \n 
self . meta = { } \n 
for e in idl_parsed : \n 
~~~ if e [ "type" ] == "struct" : \n 
~~~ self . structs [ e [ "name" ] ] = Struct ( e , self ) \n 
~~ elif e [ "type" ] == "enum" : \n 
~~~ self . enums [ e [ "name" ] ] = Enum ( e ) \n 
~~ elif e [ "type" ] == "interface" : \n 
~~~ self . interfaces [ e [ "name" ] ] = Interface ( e , self ) \n 
~~ elif e [ "type" ] == "meta" : \n 
~~~ for k , v in list ( e . items ( ) ) : \n 
~~~ if k != "type" : \n 
~~~ self . meta [ k ] = v \n 
~~ ~~ ~~ ~~ ~~ def validate_request ( self , iface_name , func_name , params ) : \n 
self . interface ( iface_name ) . function ( func_name ) . validate_params ( params ) \n 
~~ def validate_response ( self , iface_name , func_name , resp ) : \n 
self . interface ( iface_name ) . function ( func_name ) . validate_response ( resp ) \n 
~~ def get ( self , name ) : \n 
if name in self . structs : \n 
~~~ return self . structs [ name ] \n 
~~ elif name in self . enums : \n 
~~~ return self . enums [ name ] \n 
~~ elif name in self . interfaces : \n 
~~~ return self . interfaces [ name ] \n 
~~ ~~ def struct ( self , struct_name ) : \n 
if struct_name in self . structs : \n 
~~~ return self . structs [ struct_name ] \n 
~~ ~~ def has_interface ( self , iface_name ) : \n 
return iface_name in self . interfaces \n 
~~ def interface ( self , iface_name ) : \n 
if self . has_interface ( iface_name ) : \n 
~~~ return self . interfaces [ iface_name ] \n 
~~ ~~ def validate ( self , expected_type , is_array , val ) : \n 
if val is None : \n 
~~~ if expected_type . optional : \n 
~~~ return True , None \n 
~~ ~~ elif is_array : \n 
~~~ if not isinstance ( val , list ) : \n 
~~~ return self . _type_err ( val , "list" ) \n 
~~~ for v in val : \n 
~~~ ok , msg = self . validate ( expected_type , False , v ) \n 
if not ok : \n 
~~~ return ok , msg \n 
~~ ~~ ~~ ~~ elif expected_type . type == "int" : \n 
~~~ if not isinstance ( val , int ) : \n 
~~~ return self . _type_err ( val , "int" ) \n 
~~ ~~ elif expected_type . type == "float" : \n 
~~~ if not isinstance ( val , ( float , int ) ) : \n 
~~~ return self . _type_err ( val , "float" ) \n 
~~ ~~ elif expected_type . type == "bool" : \n 
~~~ if not isinstance ( val , bool ) : \n 
~~~ return self . _type_err ( val , "bool" ) \n 
~~ ~~ elif expected_type . type == "string" : \n 
~~~ if not isinstance ( val , str ) : \n 
~~~ return self . _type_err ( val , "string" ) \n 
~~~ return self . get ( expected_type . type ) . validate ( val ) \n 
~~ return True , None \n 
~~ def _type_err ( self , val , expected ) : \n 
~~ ~~ class Interface ( object ) : \n 
def __init__ ( self , iface , contract ) : \n 
self . name = iface [ "name" ] \n 
self . functions = { } \n 
for f in iface [ "functions" ] : \n 
~~~ self . functions [ f [ "name" ] ] = Function ( self . name , f , contract ) \n 
~~ ~~ def function ( self , func_name ) : \n 
if func_name in self . functions : \n 
~~~ return self . functions [ func_name ] \n 
~~~ raise RpcException ( ERR_METHOD_NOT_FOUND , \n 
~~ ~~ ~~ class Enum ( object ) : \n 
def __init__ ( self , enum ) : \n 
self . name = enum [ "name" ] \n 
self . values = [ ] \n 
for v in enum [ "values" ] : \n 
~~~ self . values . append ( v [ "value" ] ) \n 
~~ ~~ def validate ( self , val ) : \n 
if val in self . values : \n 
~~ ~~ ~~ class Struct ( object ) : \n 
def __init__ ( self , s , contract ) : \n 
self . name = s [ "name" ] \n 
self . extends = s [ "extends" ] \n 
self . parent = None \n 
self . fields = { } \n 
for f in s [ "fields" ] : \n 
~~~ self . fields [ f [ "name" ] ] = Type ( f ) \n 
~~ ~~ def field ( self , name ) : \n 
if name in self . fields : \n 
~~~ return self . fields [ name ] \n 
~~ elif self . extends : \n 
~~~ if not self . parent : \n 
~~~ self . parent = self . contract . struct ( self . extends ) \n 
~~ return self . parent . field ( name ) \n 
if type ( val ) is not dict : \n 
~~ for k , v in list ( val . items ( ) ) : \n 
~~~ field = self . field ( k ) \n 
if field : \n 
~~~ ok , msg = self . contract . validate ( field , field . is_array , v ) \n 
~~ ~~ all_fields = self . get_all_fields ( [ ] ) \n 
for field in all_fields : \n 
~~~ if field . name not in val and not field . optional : \n 
~~ ~~ return True , None \n 
~~ def get_all_fields ( self , arr ) : \n 
for k , v in list ( self . fields . items ( ) ) : \n 
~~~ arr . append ( v ) \n 
~~ if self . extends : \n 
~~~ parent = self . contract . get ( self . extends ) \n 
if parent : \n 
~~~ return parent . get_all_fields ( arr ) \n 
~~ ~~ return arr \n 
~~ ~~ class Function ( object ) : \n 
def __init__ ( self , iface_name , f , contract ) : \n 
self . name = f [ "name" ] \n 
self . params = [ ] \n 
for p in f [ "params" ] : \n 
~~~ self . params . append ( Type ( p ) ) \n 
~~ self . returns = Type ( f [ "returns" ] ) if "returns" in f else None \n 
self . full_name = "%s.%s" % ( iface_name , self . name ) \n 
self . validate_structure ( ) \n 
~~ def validate_structure ( self ) : \n 
if self . name in [ None , ] : \n 
~~~ raise InvalidFunctionError ( \n 
~~ if self . returns is None : \n 
self . full_name \n 
~~ ~~ def validate_params ( self , params ) : \n 
if params is not None : \n 
~~~ if len ( self . params ) != len ( params ) : \n 
~~~ vals = ( self . full_name , len ( self . params ) , len ( params ) ) \n 
raise RpcException ( ERR_INVALID_PARAMS , msg ) \n 
~~ [ self . _validate_param ( x , y ) for ( x , y ) in zip ( self . params , params ) ] \n 
~~ ~~ def validate_response ( self , resp ) : \n 
ok , msg = self . contract . validate ( self . returns , \n 
self . returns . is_array , resp ) \n 
~~~ vals = ( self . full_name , str ( resp ) , msg ) \n 
raise RpcException ( ERR_INVALID_RESP , msg ) \n 
~~ ~~ def _validate_param ( self , expected , param ) : \n 
ok , msg = self . contract . validate ( expected , expected . is_array , param ) \n 
~~~ vals = ( self . full_name , expected . name , msg ) \n 
~~ ~~ ~~ class Type ( object ) : \n 
~~~ def __init__ ( self , type_dict ) : \n 
~~~ self . name = "" \n 
self . optional = False \n 
if "name" in type_dict : \n 
~~~ self . name = type_dict [ "name" ] \n 
~~ self . type = type_dict [ "type" ] \n 
self . is_array = type_dict [ "is_array" ] \n 
if "optional" in type_dict : \n 
~~~ self . optional = type_dict [ "optional" ] \n 
~~ ~~ ~~ import os \n 
class CheckProcs ( object ) : \n 
~~~ myPid = 0 \n 
state = "" \n 
name = "" \n 
pid = 0 \n 
allProcs = [ ] \n 
interestingProcs = [ ] \n 
procDir = "/proc" \n 
debug = False \n 
~~~ self . myPid = os . getpid ( ) \n 
~~ def setup ( self , debug = False , pidlist = False ) : \n 
~~~ self . debug = debug \n 
self . pidlist = pidlist \n 
if debug is True : \n 
~~ self . allProcs = [ procs for procs in os . listdir ( self . procDir ) if procs . isdigit ( ) and \n 
int ( procs ) != int ( self . myPid ) ] \n 
~~ def process ( self , criteria ) : \n 
~~~ for p in self . allProcs : \n 
~~~ fh = open ( self . procDir + "/" + p + "/stat" ) \n 
pInfo = fh . readline ( ) . split ( ) \n 
cmdfh = open ( self . procDir + "/" + p + "/cmdline" ) \n 
cmd = cmdfh . readline ( ) \n 
pInfo [ 1 ] = cmd \n 
~~~ cmdfh . close ( ) \n 
fh . close ( ) \n 
~~ if criteria == : \n 
~~~ if pInfo [ 2 ] == self . state : \n 
~~~ self . interestingProcs . append ( pInfo ) \n 
~~ ~~ elif criteria == : \n 
~~~ if re . search ( self . name , pInfo [ 1 ] ) : \n 
~~~ if pInfo [ 0 ] == self . pid : \n 
~~ ~~ ~~ ~~ def byState ( self , state ) : \n 
~~~ self . state = state \n 
self . process ( criteria = ) \n 
self . show ( ) \n 
~~ def byPid ( self , pid ) : \n 
~~~ self . pid = pid \n 
~~ def byName ( self , name ) : \n 
~~ def run ( self , foo , criteria ) : \n 
~~~ if foo == : \n 
~~~ self . byState ( criteria ) \n 
~~ elif foo == : \n 
~~~ self . byName ( criteria ) \n 
~~~ self . byPid ( criteria ) \n 
~~ ~~ def show ( self ) : \n 
~~~ prettyOut = { } \n 
if len ( self . interestingProcs ) > 0 : \n 
~~~ for proc in self . interestingProcs : \n 
~~~ prettyOut [ proc [ 0 ] ] = proc [ 1 ] \n 
~~ ~~ if self . pidlist is True : \n 
~~~ pidlist = . join ( prettyOut . keys ( ) ) \n 
sys . stderr . write ( pidlist ) \n 
~~ print ( json . dumps ( prettyOut ) ) \n 
~~~ if "pidlist" in sys . argv : \n 
~~~ pidlist = True \n 
~~~ pidlist = False \n 
~~ foo = CheckProcs ( ) \n 
foo . setup ( debug = False , pidlist = pidlist ) \n 
foo . run ( sys . argv [ 1 ] , sys . argv [ 2 ] ) \n 
~~ from oslo_config import cfg \n 
import st2common . config as common_config \n 
from st2common . constants . system import VERSION_STRING \n 
common_config . register_opts ( ) \n 
def parse_args ( args = None ) : \n 
~~~ CONF ( args = args , version = VERSION_STRING ) \n 
~~ def register_opts ( ) : \n 
~~~ _register_common_opts ( ) \n 
_register_notifier_opts ( ) \n 
~~ def get_logging_config_path ( ) : \n 
~~~ return cfg . CONF . notifier . logging \n 
~~ def _register_common_opts ( ) : \n 
~~~ common_config . register_opts ( ) \n 
~~ def _register_notifier_opts ( ) : \n 
~~~ notifier_opts = [ \n 
cfg . StrOpt ( , default = , \n 
CONF . register_opts ( notifier_opts , group = ) \n 
scheduler_opts = [ \n 
cfg . BoolOpt ( , default = True , help = ) , \n 
cfg . IntOpt ( , default = 600 , \n 
help = ) , cfg . IntOpt ( , default = 300 , \n 
CONF . register_opts ( scheduler_opts , group = ) \n 
~~ register_opts ( ) \n 
import abc \n 
from distutils . spawn import find_executable \n 
import six \n 
from st2actions . runners import ActionRunner \n 
WINEXE_EXISTS = find_executable ( ) is not None \n 
SMBCLIENT_EXISTS = find_executable ( ) is not None \n 
ERROR_CODE_TO_MESSAGE_MAP = { \n 
@ six . add_metaclass ( abc . ABCMeta ) \n 
class BaseWindowsRunner ( ActionRunner ) : \n 
~~~ def _verify_winexe_exists ( self ) : \n 
~~~ if not WINEXE_EXISTS : \n 
raise Exception ( msg ) \n 
~~ ~~ def _verify_smbclient_exists ( self ) : \n 
~~~ if not SMBCLIENT_EXISTS : \n 
~~ ~~ def _get_winexe_command_args ( self , host , username , password , command , domain = None ) : \n 
~~~ args = [ ] \n 
args += [ , ] \n 
if domain : \n 
~~~ args += [ , % ( domain , username , password ) ] \n 
~~~ args += [ , % ( username , password ) ] \n 
~~ args += [ % ( host ) ] \n 
args += [ command ] \n 
~~ def _get_smbclient_command_args ( self , host , username , password , command , share = , \n 
domain = None ) : \n 
values = { : domain , : username , : password } \n 
~~~ auth_string = % values \n 
~~ args += [ , auth_string ] \n 
args += [ % { : host , : share } ] \n 
args += [ , command ] \n 
~~ def _parse_winexe_error ( self , stdout , stderr ) : \n 
~~~ for code , message in ERROR_CODE_TO_MESSAGE_MAP . items ( ) : \n 
~~~ if code in stdout : \n 
~~~ return message \n 
from mistralclient . api . v2 import executions \n 
from mistralclient . api . v2 import tasks \n 
from mistralclient . api . v2 import workbooks \n 
from mistralclient . api . v2 import workflows \n 
import st2tests . config as tests_config \n 
tests_config . parse_args ( ) \n 
cfg . CONF . set_override ( , 100 , group = ) \n 
cfg . CONF . set_override ( , 200 , group = ) \n 
import st2common . bootstrap . runnersregistrar as runners_registrar \n 
from st2actions . runners . localrunner import LocalShellRunner \n 
from st2actions . runners . mistral . v2 import MistralRunner \n 
from st2common . constants import action as action_constants \n 
from st2common . models . api . action import ActionAPI \n 
from st2common . models . db . liveaction import LiveActionDB \n 
from st2common . persistence . action import Action \n 
from st2common . persistence . liveaction import LiveAction \n 
from st2common . services import action as action_service \n 
from st2common . transport . liveaction import LiveActionPublisher \n 
from st2common . transport . publishers import CUDPublisher \n 
from st2tests import DbTestCase \n 
from st2tests . fixturesloader import FixturesLoader \n 
from tests . unit . base import MockLiveActionPublisher \n 
TEST_FIXTURES = { \n 
PACK = \n 
LOADER = FixturesLoader ( ) \n 
FIXTURES = LOADER . load_fixtures ( fixtures_pack = PACK , fixtures_dict = TEST_FIXTURES ) \n 
WB1_YAML_FILE_NAME = TEST_FIXTURES [ ] [ 1 ] \n 
WB1_YAML_FILE_PATH = LOADER . get_fixture_file_path_abs ( PACK , , WB1_YAML_FILE_NAME ) \n 
WB1_SPEC = FIXTURES [ ] [ WB1_YAML_FILE_NAME ] \n 
WB1_YAML = yaml . safe_dump ( WB1_SPEC , default_flow_style = False ) \n 
WB1_NAME = % ( PACK , WB1_YAML_FILE_NAME . replace ( , ) ) \n 
WB1 = workbooks . Workbook ( None , { : WB1_NAME , : WB1_YAML } ) \n 
WB1_MAIN_EXEC = { : str ( uuid . uuid4 ( ) ) , : } \n 
WB1_MAIN_EXEC [ ] = WB1_NAME + \n 
WB1_MAIN_EXEC_ERRORED = copy . deepcopy ( WB1_MAIN_EXEC ) \n 
WB1_MAIN_EXEC_ERRORED [ ] = \n 
WB1_MAIN_TASK1 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WB1_MAIN_TASKS = [ tasks . Task ( None , WB1_MAIN_TASK1 ) ] \n 
WB1_MAIN_TASK_ID = WB1_MAIN_TASK1 [ ] \n 
WB1_SUB1_EXEC = { : str ( uuid . uuid4 ( ) ) , : , : WB1_MAIN_TASK_ID } WB1_SUB1_EXEC [ ] = WB1_NAME + \n 
WB1_SUB1_EXEC_ERRORED = copy . deepcopy ( WB1_SUB1_EXEC ) \n 
WB1_SUB1_EXEC_ERRORED [ ] = \n 
WB1_SUB1_TASK1 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WB1_SUB1_TASK2 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WB1_SUB1_TASKS = [ tasks . Task ( None , WB1_SUB1_TASK1 ) , tasks . Task ( None , WB1_SUB1_TASK2 ) ] \n 
WF1_YAML_FILE_NAME = TEST_FIXTURES [ ] [ 0 ] \n 
WF1_YAML_FILE_PATH = LOADER . get_fixture_file_path_abs ( PACK , , WF1_YAML_FILE_NAME ) \n 
WF1_SPEC = FIXTURES [ ] [ WF1_YAML_FILE_NAME ] \n 
WF1_YAML = yaml . safe_dump ( WF1_SPEC , default_flow_style = False ) \n 
WF1_NAME = % ( PACK , WF1_YAML_FILE_NAME . replace ( , ) ) \n 
WF1 = workflows . Workflow ( None , { : WF1_NAME , : WF1_YAML } ) \n 
WF1_EXEC = { : str ( uuid . uuid4 ( ) ) , : , : WF1_NAME } \n 
WF1_EXEC_NOT_RERUNABLE = copy . deepcopy ( WF1_EXEC ) \n 
WF1_EXEC_NOT_RERUNABLE [ ] = \n 
WF1_TASK1 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WF1_TASK2 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WF1_TASKS = [ tasks . Task ( None , WF1_TASK1 ) , tasks . Task ( None , WF1_TASK2 ) ] \n 
ACTION_PARAMS = { : } \n 
NON_EMPTY_RESULT = \n 
@ mock . patch . object ( LocalShellRunner , , mock . \n 
MagicMock ( return_value = ( action_constants . LIVEACTION_STATUS_SUCCEEDED , \n 
NON_EMPTY_RESULT , None ) ) ) \n 
@ mock . patch . object ( CUDPublisher , , mock . MagicMock ( return_value = None ) ) \n 
@ mock . patch . object ( CUDPublisher , , \n 
mock . MagicMock ( side_effect = MockLiveActionPublisher . publish_create ) ) \n 
@ mock . patch . object ( LiveActionPublisher , , \n 
mock . MagicMock ( side_effect = MockLiveActionPublisher . publish_state ) ) \n 
class MistralRunnerTest ( DbTestCase ) : \n 
def setUpClass ( cls ) : \n 
~~~ super ( MistralRunnerTest , cls ) . setUpClass ( ) \n 
runners_registrar . register_runner_types ( ) \n 
for _ , fixture in six . iteritems ( FIXTURES [ ] ) : \n 
~~~ instance = ActionAPI ( ** fixture ) \n 
Action . add_or_update ( ActionAPI . to_model ( instance ) ) \n 
~~ ~~ def setUp ( self ) : \n 
~~~ super ( MistralRunnerTest , self ) . setUp ( ) \n 
cfg . CONF . set_override ( , , group = ) \n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
mock . MagicMock ( return_value = [ WF1 ] ) ) \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC ) ) ) \n 
MistralRunner , , \n 
mock . MagicMock ( \n 
return_value = ( action_constants . LIVEACTION_STATUS_RUNNING , \n 
{ : [ ] } , \n 
{ : str ( uuid . uuid4 ( ) ) } ) \n 
def test_resume_option ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WF1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
self . assertFalse ( MistralRunner . resume . called ) \n 
: execution1 . id , \n 
: [ ] \n 
liveaction2 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_RUNNING ) \n 
task_specs = { \n 
MistralRunner . resume . assert_called_with ( ex_ref = execution1 , task_specs = task_specs ) \n 
def test_resume_option_reset_tasks ( self ) : \n 
: True \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC_NOT_RERUNABLE ) ) ) \n 
tasks . TaskManager , , \n 
mock . MagicMock ( return_value = WF1_TASKS ) ) \n 
def test_resume_workflow_not_in_rerunable_state ( self ) : \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_FAILED ) \n 
self . assertIn ( , liveaction2 . result . get ( ) ) \n 
mock . MagicMock ( return_value = [ executions . Execution ( None , WF1_EXEC ) ] ) ) \n 
def test_resume_tasks_not_in_rerunable_state ( self ) : \n 
def test_resume_unidentified_tasks ( self ) : \n 
workbooks . WorkbookManager , , \n 
mock . MagicMock ( return_value = WB1 ) ) \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC ) ) ) \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) ) ) \n 
return_value = [ \n 
executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) , \n 
executions . Execution ( None , WB1_SUB1_EXEC_ERRORED ) ] ) ) \n 
mock . MagicMock ( side_effect = [ WB1_MAIN_TASKS , WB1_SUB1_TASKS ] ) ) \n 
mock . MagicMock ( return_value = None ) ) \n 
def test_resume_subworkflow_task ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WB1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction2 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
expected_env = { \n 
: str ( liveaction2 . id ) , \n 
: str ( execution2 . id ) , \n 
: { } , \n 
: context [ ] , \n 
: str ( execution2 . id ) \n 
tasks . TaskManager . rerun . assert_called_with ( \n 
WB1_SUB1_TASK2 [ ] , \n 
reset = False , \n 
env = expected_env \n 
def test_resume_unidentified_subworkflow_task ( self ) : \n 
def test_resume_and_reset_subworkflow_task ( self ) : \n 
reset = True , \n 
from pecan import abort \n 
from mongoengine import ValidationError \n 
from st2api . controllers import resource \n 
from st2api . controllers . v1 . actionviews import ActionViewsController \n 
from st2common import log as logging \n 
from st2common . constants . triggers import ACTION_FILE_WRITTEN_TRIGGER \n 
from st2common . exceptions . action import InvalidActionParameterException \n 
from st2common . exceptions . apivalidation import ValueValidationException \n 
from st2common . models . api . base import jsexpose \n 
from st2common . models . api . action import ActionCreateAPI \n 
from st2common . persistence . pack import Pack \n 
from st2common . validators . api . misc import validate_not_part_of_system_pack \n 
from st2common . content . utils import get_pack_base_path \n 
from st2common . content . utils import get_pack_resource_file_abs_path \n 
from st2common . content . utils import get_relative_path_to_pack \n 
from st2common . transport . reactor import TriggerDispatcher \n 
from st2common . util . system_info import get_host_info \n 
import st2common . validators . api . action as action_validator \n 
from st2common . rbac . types import PermissionType \n 
from st2common . rbac . decorators import request_user_has_permission \n 
from st2common . rbac . decorators import request_user_has_resource_api_permission \n 
from st2common . rbac . decorators import request_user_has_resource_db_permission \n 
http_client = six . moves . http_client \n 
class ActionsController ( resource . ContentPackResourceController ) : \n 
views = ActionViewsController ( ) \n 
model = ActionAPI \n 
access = Action \n 
supported_filters = { \n 
query_options = { \n 
include_reference = True \n 
~~~ super ( ActionsController , self ) . __init__ ( * args , ** kwargs ) \n 
self . _trigger_dispatcher = TriggerDispatcher ( LOG ) \n 
~~ @ request_user_has_permission ( permission_type = PermissionType . ACTION_LIST ) \n 
@ jsexpose ( ) \n 
def get_all ( self , ** kwargs ) : \n 
~~~ return super ( ActionsController , self ) . _get_all ( ** kwargs ) \n 
~~ @ request_user_has_resource_db_permission ( permission_type = PermissionType . ACTION_VIEW ) \n 
@ jsexpose ( arg_types = [ str ] ) \n 
def get_one ( self , ref_or_id ) : \n 
~~~ return super ( ActionsController , self ) . _get_one ( ref_or_id ) \n 
~~ @ jsexpose ( body_cls = ActionCreateAPI , status_code = http_client . CREATED ) \n 
@ request_user_has_resource_api_permission ( permission_type = PermissionType . ACTION_CREATE ) \n 
def post ( self , action ) : \n 
~~~ validate_not_part_of_system_pack ( action ) \n 
action_validator . validate_action ( action ) \n 
~~ except ( ValidationError , ValueError , \n 
ValueValidationException , InvalidActionParameterException ) as e : \n 
~~~ LOG . exception ( , action ) \n 
abort ( http_client . BAD_REQUEST , str ( e ) ) \n 
~~ data_files = getattr ( action , , [ ] ) \n 
written_data_files = [ ] \n 
if data_files : \n 
~~~ written_data_files = self . _handle_data_files ( pack_name = action . pack , \n 
data_files = data_files ) \n 
~~ action_model = ActionAPI . to_model ( action ) \n 
LOG . debug ( , action ) \n 
action_db = Action . add_or_update ( action_model ) \n 
LOG . debug ( , action_db ) \n 
if written_data_files : \n 
~~~ self . _dispatch_trigger_for_written_data_files ( action_db = action_db , \n 
written_data_files = written_data_files ) \n 
~~ extra = { : action_db } \n 
LOG . audit ( % ( action_db . id ) , extra = extra ) \n 
action_api = ActionAPI . from_model ( action_db ) \n 
return action_api \n 
~~ @ request_user_has_resource_db_permission ( permission_type = PermissionType . ACTION_MODIFY ) \n 
@ jsexpose ( arg_types = [ str ] , body_cls = ActionCreateAPI ) \n 
def put ( self , action_ref_or_id , action ) : \n 
~~~ action_db = self . _get_by_ref_or_id ( ref_or_id = action_ref_or_id ) \n 
action_id = action_db . id \n 
if not getattr ( action , , None ) : \n 
~~~ action . pack = action_db . pack \n 
~~ validate_not_part_of_system_pack ( action ) \n 
data_files = getattr ( action , , [ ] ) \n 
~~~ action_db = ActionAPI . to_model ( action ) \n 
action_db . id = action_id \n 
action_db = Action . add_or_update ( action_db ) \n 
~~ except ( ValidationError , ValueError ) as e : \n 
~~ if written_data_files : \n 
~~ action_api = ActionAPI . from_model ( action_db ) \n 
LOG . debug ( , action_api ) \n 
~~ @ request_user_has_resource_db_permission ( permission_type = PermissionType . ACTION_DELETE ) \n 
@ jsexpose ( arg_types = [ str ] , status_code = http_client . NO_CONTENT ) \n 
def delete ( self , action_ref_or_id ) : \n 
action_db = self . _get_by_ref_or_id ( ref_or_id = action_ref_or_id ) \n 
~~~ validate_not_part_of_system_pack ( action_db ) \n 
~~ except ValueValidationException as e : \n 
~~~ abort ( http_client . BAD_REQUEST , str ( e ) ) \n 
~~ LOG . debug ( , \n 
action_ref_or_id , action_db ) \n 
~~~ Action . delete ( action_db ) \n 
, action_id , e ) \n 
abort ( http_client . INTERNAL_SERVER_ERROR , str ( e ) ) \n 
~~ def _handle_data_files ( self , pack_name , data_files ) : \n 
written_file_paths = self . _write_data_files_to_disk ( pack_name = pack_name , \n 
self . _update_pack_model ( pack_name = pack_name , data_files = data_files , \n 
written_file_paths = written_file_paths ) \n 
return written_file_paths \n 
~~ def _write_data_files_to_disk ( self , pack_name , data_files ) : \n 
written_file_paths = [ ] \n 
for data_file in data_files : \n 
~~~ file_path = data_file [ ] \n 
content = data_file [ ] \n 
file_path = get_pack_resource_file_abs_path ( pack_name = pack_name , \n 
resource_type = , \n 
file_path = file_path ) \n 
self . _write_data_file ( pack_name = pack_name , file_path = file_path , content = content ) \n 
written_file_paths . append ( file_path ) \n 
~~ return written_file_paths \n 
~~ def _update_pack_model ( self , pack_name , data_files , written_file_paths ) : \n 
for file_path in written_file_paths : \n 
~~~ file_path = get_relative_path_to_pack ( pack_name = pack_name , file_path = file_path ) \n 
file_paths . append ( file_path ) \n 
~~ pack_db = Pack . get_by_ref ( pack_name ) \n 
pack_db . files = set ( pack_db . files ) \n 
pack_db . files . update ( set ( file_paths ) ) \n 
pack_db . files = list ( pack_db . files ) \n 
pack_db = Pack . add_or_update ( pack_db ) \n 
return pack_db \n 
~~ def _write_data_file ( self , pack_name , file_path , content ) : \n 
pack_base_path = get_pack_base_path ( pack_name = pack_name ) \n 
if not os . path . isdir ( pack_base_path ) : \n 
~~ directory = os . path . dirname ( file_path ) \n 
if not os . path . isdir ( directory ) : \n 
~~~ os . makedirs ( directory ) \n 
~~ with open ( file_path , ) as fp : \n 
~~~ fp . write ( content ) \n 
~~ ~~ def _dispatch_trigger_for_written_data_files ( self , action_db , written_data_files ) : \n 
~~~ trigger = ACTION_FILE_WRITTEN_TRIGGER [ ] \n 
host_info = get_host_info ( ) \n 
for file_path in written_data_files : \n 
~~~ payload = { \n 
: action_db . ref , \n 
: file_path , \n 
: host_info \n 
self . _trigger_dispatcher . dispatch ( trigger = trigger , payload = payload ) \n 
~~ ~~ ~~ import httplib \n 
from st2common . rbac . types import ResourceType \n 
from st2common . persistence . auth import User \n 
from st2common . persistence . rbac import Role \n 
from st2common . persistence . rbac import UserRoleAssignment \n 
from st2common . persistence . rbac import PermissionGrant \n 
from st2common . models . db . auth import UserDB \n 
from st2common . models . db . rbac import RoleDB \n 
from st2common . models . db . rbac import UserRoleAssignmentDB \n 
from st2common . models . db . rbac import PermissionGrantDB \n 
from tests . base import APIControllerWithRBACTestCase \n 
FIXTURES_PACK = \n 
ACTION_2 = { \n 
: { : , : , : 0 } , \n 
: { : , : , : True } \n 
class ActionControllerRBACTestCase ( APIControllerWithRBACTestCase ) : \n 
~~~ fixtures_loader = FixturesLoader ( ) \n 
def setUp ( self ) : \n 
~~~ super ( ActionControllerRBACTestCase , self ) . setUp ( ) \n 
self . fixtures_loader . save_fixtures_to_db ( fixtures_pack = FIXTURES_PACK , \n 
fixtures_dict = TEST_FIXTURES ) \n 
file_name = \n 
ActionControllerRBACTestCase . ACTION_1 = self . fixtures_loader . load_fixtures ( \n 
fixtures_pack = FIXTURES_PACK , \n 
fixtures_dict = { : [ file_name ] } ) [ ] [ file_name ] \n 
self = self \n 
self . users = { } \n 
self . roles = { } \n 
user_1_db = UserDB ( name = ) \n 
user_1_db = User . add_or_update ( user_1_db ) \n 
self . users [ ] = user_1_db \n 
user_2_db = UserDB ( name = ) \n 
user_2_db = User . add_or_update ( user_2_db ) \n 
self . users [ ] = user_2_db \n 
grant_db = PermissionGrantDB ( resource_uid = , \n 
resource_type = ResourceType . PACK , \n 
permission_types = [ PermissionType . ACTION_CREATE ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_1_db = RoleDB ( name = , permission_grants = permission_grants ) \n 
role_1_db = Role . add_or_update ( role_1_db ) \n 
self . roles [ ] = role_1_db \n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
~~ def test_create_action_no_action_create_permission ( self ) : \n 
~~~ user_db = self . users [ ] \n 
self . use_user ( user_db ) \n 
resp = self . __do_post ( ActionControllerRBACTestCase . ACTION_1 ) \n 
self . assertEqual ( resp . status_code , httplib . FORBIDDEN ) \n 
self . assertEqual ( resp . json [ ] , expected_msg ) \n 
self . use_user ( { } ) \n 
~~ @ mock . patch . object ( action_validator , , mock . MagicMock ( \n 
return_value = True ) ) \n 
def test_create_action_success ( self ) : \n 
resp = self . __do_post ( ACTION_2 ) \n 
self . assertEqual ( resp . status_code , httplib . CREATED ) \n 
def __get_action_id ( resp ) : \n 
~~~ return resp . json [ ] \n 
~~ def __do_post ( self , rule ) : \n 
~~~ return self . app . post_json ( , rule , expect_errors = True ) \n 
~~ def __do_delete ( self , action_id , expect_errors = False ) : \n 
~~~ return self . app . delete ( % action_id , expect_errors = expect_errors ) \n 
~~ ~~ import eventlet \n 
from eventlet import wsgi \n 
from st2common . service_setup import setup as common_setup \n 
from st2common . service_setup import teardown as common_teardown \n 
from st2common . util . monkey_patch import monkey_patch \n 
from st2common . constants . auth import VALID_MODES \n 
from st2auth import config \n 
config . register_opts ( ) \n 
from st2auth import app \n 
monkey_patch ( ) \n 
def _setup ( ) : \n 
~~~ common_setup ( service = , config = config , setup_db = True , register_mq_exchanges = False , \n 
register_signal_handlers = True , register_internal_trigger_types = False , \n 
run_migrations = False ) \n 
if cfg . CONF . auth . mode not in VALID_MODES : \n 
~~~ raise ValueError ( % ( . join ( VALID_MODES ) ) ) \n 
~~ ~~ def _run_server ( ) : \n 
~~~ host = cfg . CONF . auth . host \n 
port = cfg . CONF . auth . port \n 
use_ssl = cfg . CONF . auth . use_ssl \n 
cert_file_path = os . path . realpath ( cfg . CONF . auth . cert ) \n 
key_file_path = os . path . realpath ( cfg . CONF . auth . key ) \n 
if use_ssl and not os . path . isfile ( cert_file_path ) : \n 
~~ if use_ssl and not os . path . isfile ( key_file_path ) : \n 
~~ socket = eventlet . listen ( ( host , port ) ) \n 
if use_ssl : \n 
~~~ socket = eventlet . wrap_ssl ( socket , \n 
certfile = cert_file_path , \n 
keyfile = key_file_path , \n 
server_side = True ) \n 
LOG . info ( , os . getpid ( ) , \n 
if use_ssl else , host , port ) \n 
wsgi . server ( socket , app . setup_app ( ) ) \n 
~~ def _teardown ( ) : \n 
~~~ common_teardown ( ) \n 
~~~ _setup ( ) \n 
return _run_server ( ) \n 
~~ except SystemExit as exit_code : \n 
~~~ sys . exit ( exit_code ) \n 
~~~ LOG . exception ( , os . getpid ( ) ) \n 
~~~ _teardown ( ) \n 
~~ ~~ __all__ = [ \n 
CONFIG = { } \n 
def get_config ( ) : \n 
global CONFIG \n 
return CONFIG \n 
~~ def set_config ( config ) : \n 
CONFIG = config \n 
return config \n 
ALLOWED_EXTS = [ , , , ] \n 
PARSER_FUNCS = { : json . load , : yaml . safe_load , : yaml . safe_load } \n 
def get_fixtures_base_path ( ) : \n 
~~~ return os . path . dirname ( __file__ ) \n 
~~ def load_content ( file_path ) : \n 
file_name , file_ext = os . path . splitext ( file_path ) \n 
if file_ext not in ALLOWED_EXTS : \n 
~~~ raise Exception ( % \n 
( file_ext , file_path , ALLOWED_EXTS ) ) \n 
~~ parser_func = PARSER_FUNCS . get ( file_ext , None ) \n 
with open ( file_path , ) as fd : \n 
~~~ return parser_func ( fd ) if parser_func else fd . read ( ) \n 
~~ ~~ def load_fixtures ( fixtures_dict = None ) : \n 
if fixtures_dict is None : \n 
~~~ fixtures_dict = { } \n 
~~ all_fixtures = { } \n 
fixtures_base_path = get_fixtures_base_path ( ) \n 
for fixture_type , fixtures in six . iteritems ( fixtures_dict ) : \n 
~~~ loaded_fixtures = { } \n 
for fixture in fixtures : \n 
~~~ fixture_path = fixtures_base_path + + fixture \n 
fixture_dict = load_content ( fixture_path ) \n 
loaded_fixtures [ fixture ] = fixture_dict \n 
~~ all_fixtures [ fixture_type ] = loaded_fixtures \n 
~~ return all_fixtures \n 
def do_register_opts ( opts , group = None , ignore_errors = False ) : \n 
~~~ cfg . CONF . register_opts ( opts , group = group ) \n 
~~~ if not ignore_errors : \n 
~~ ~~ ~~ def do_register_cli_opts ( opt , ignore_errors = False ) : \n 
~~~ if not isinstance ( opt , ( list , tuple ) ) : \n 
~~~ opts = [ opt ] \n 
~~~ opts = opt \n 
~~~ cfg . CONF . register_cli_opts ( opts ) \n 
~~ ~~ ~~ def register_opts ( ignore_errors = False ) : \n 
~~~ rbac_opts = [ \n 
cfg . BoolOpt ( , default = False , help = ) , \n 
do_register_opts ( rbac_opts , , ignore_errors ) \n 
system_user_opts = [ \n 
cfg . StrOpt ( , \n 
do_register_opts ( system_user_opts , , ignore_errors ) \n 
schema_opts = [ \n 
cfg . IntOpt ( , default = 4 , help = ) , \n 
do_register_opts ( schema_opts , , ignore_errors ) \n 
system_opts = [ \n 
do_register_opts ( system_opts , , ignore_errors ) \n 
system_packs_base_path = os . path . join ( cfg . CONF . system . base_path , ) \n 
content_opts = [ \n 
cfg . StrOpt ( , default = system_packs_base_path , \n 
cfg . StrOpt ( , default = None , \n 
do_register_opts ( content_opts , , ignore_errors ) \n 
db_opts = [ \n 
cfg . StrOpt ( , default = , help = ) , \n 
cfg . IntOpt ( , default = 27017 , help = ) , \n 
cfg . StrOpt ( , help = ) , \n 
cfg . IntOpt ( , help = , \n 
default = 3 ) , \n 
cfg . IntOpt ( , help = , default = 10 ) , \n 
default = 1 ) , \n 
cfg . BoolOpt ( , help = , default = False ) , \n 
help = , \n 
default = None ) , \n 
cfg . StrOpt ( , help = , \n 
cfg . StrOpt ( , choices = , \n 
cfg . BoolOpt ( , \n 
default = True ) \n 
do_register_opts ( db_opts , , ignore_errors ) \n 
messaging_opts = [ \n 
cfg . ListOpt ( , default = [ ] , \n 
do_register_opts ( messaging_opts , , ignore_errors ) \n 
syslog_opts = [ \n 
cfg . IntOpt ( , default = 514 , \n 
do_register_opts ( syslog_opts , , ignore_errors ) \n 
log_opts = [ \n 
cfg . ListOpt ( , default = , \n 
cfg . BoolOpt ( , default = False , \n 
cfg . BoolOpt ( , default = True , \n 
do_register_opts ( log_opts , , ignore_errors ) \n 
api_opts = [ \n 
cfg . IntOpt ( , default = 9101 , help = ) , \n 
do_register_opts ( api_opts , , ignore_errors ) \n 
keyvalue_opts = [ \n 
help = + \n 
+ \n 
do_register_opts ( keyvalue_opts , group = ) \n 
auth_opts = [ \n 
cfg . IntOpt ( , default = 86400 , help = ) \n 
do_register_opts ( auth_opts , , ignore_errors ) \n 
default_python_bin_path = sys . executable \n 
base_dir = os . path . dirname ( os . path . realpath ( default_python_bin_path ) ) \n 
default_virtualenv_bin_path = os . path . join ( base_dir , ) \n 
action_runner_opts = [ \n 
cfg . StrOpt ( , default = default_python_bin_path , \n 
cfg . StrOpt ( , default = default_virtualenv_bin_path , \n 
do_register_opts ( action_runner_opts , group = ) \n 
action_sensor_opts = [ \n 
do_register_opts ( action_sensor_opts , group = ) \n 
coord_opts = [ \n 
cfg . StrOpt ( , default = None , help = ) , \n 
cfg . IntOpt ( , default = 60 , help = ) \n 
do_register_opts ( coord_opts , , ignore_errors ) \n 
mistral_opts = [ \n 
cfg . StrOpt ( , default = , help = ) , cfg . IntOpt ( , default = 1000 , help = ) , \n 
cfg . IntOpt ( , default = 300000 , help = ) , \n 
cfg . IntOpt ( , default = 600000 , help = ) , \n 
cfg . StrOpt ( , default = None , help = ( \n 
do_register_opts ( mistral_opts , group = , ignore_errors = ignore_errors ) \n 
debug = cfg . BoolOpt ( , default = False , \n 
profile = cfg . BoolOpt ( , default = False , \n 
help = ( \n 
use_debugger = cfg . BoolOpt ( , default = True , \n 
cli_opts = [ debug , profile , use_debugger ] \n 
do_register_cli_opts ( cli_opts , ignore_errors = ignore_errors ) \n 
~~ def parse_args ( args = None ) : \n 
~~~ register_opts ( ) \n 
cfg . CONF ( args = args , version = VERSION_STRING ) \n 
~~ from st2common . exceptions import StackStormBaseException \n 
class InternalServerErrorException ( StackStormBaseException ) : \n 
~~ import six \n 
from st2common . util import isotime \n 
from st2common . models . api . base import BaseAPI \n 
from st2common . models . db . auth import UserDB , TokenDB , ApiKeyDB \n 
def get_system_username ( ) : \n 
~~~ return cfg . CONF . system_user . user \n 
~~ class UserAPI ( BaseAPI ) : \n 
~~~ model = UserDB \n 
schema = { \n 
"title" : "User" , \n 
"type" : "object" , \n 
"properties" : { \n 
"name" : { \n 
"type" : "string" , \n 
"required" : True \n 
"additionalProperties" : False \n 
def to_model ( cls , user ) : \n 
~~~ name = user . name \n 
model = cls . model ( name = name ) \n 
return model \n 
~~ ~~ class TokenAPI ( BaseAPI ) : \n 
~~~ model = TokenDB \n 
"title" : "Token" , \n 
"id" : { \n 
"type" : "string" \n 
"user" : { \n 
"type" : [ "string" , "null" ] \n 
"token" : { \n 
"ttl" : { \n 
"type" : "integer" , \n 
"minimum" : 1 \n 
"expiry" : { \n 
"type" : [ "string" , "null" ] , \n 
"pattern" : isotime . ISO8601_UTC_REGEX \n 
"metadata" : { \n 
"type" : [ "object" , "null" ] \n 
def from_model ( cls , model , mask_secrets = False ) : \n 
~~~ doc = super ( cls , cls ) . _from_model ( model , mask_secrets = mask_secrets ) \n 
doc [ ] = isotime . format ( model . expiry , offset = False ) if model . expiry else None \n 
return cls ( ** doc ) \n 
def to_model ( cls , instance ) : \n 
~~~ user = str ( instance . user ) if instance . user else None \n 
token = str ( instance . token ) if instance . token else None \n 
expiry = isotime . parse ( instance . expiry ) if instance . expiry else None \n 
model = cls . model ( user = user , token = token , expiry = expiry ) \n 
~~ ~~ class ApiKeyAPI ( BaseAPI ) : \n 
~~~ model = ApiKeyDB \n 
"title" : "ApiKey" , \n 
"uid" : { \n 
"default" : "" \n 
"key_hash" : { \n 
: isotime . ISO8601_UTC_REGEX \n 
"enabled" : { \n 
"type" : "boolean" , \n 
"default" : True \n 
doc [ ] = isotime . format ( model . created_at , offset = False ) if model . created_at else None \n 
key_hash = getattr ( instance , , None ) \n 
metadata = getattr ( instance , , { } ) \n 
enabled = bool ( getattr ( instance , , True ) ) \n 
model = cls . model ( user = user , key_hash = key_hash , metadata = metadata , enabled = enabled ) \n 
~~ ~~ class ApiKeyCreateResponseAPI ( BaseAPI ) : \n 
~~~ schema = { \n 
"title" : "APIKeyCreateResponse" , \n 
"key" : { \n 
~~~ doc = cls . _from_model ( model = model , mask_secrets = mask_secrets ) \n 
attrs = { attr : value for attr , value in six . iteritems ( doc ) if value is not None } \n 
attrs [ ] = isotime . format ( model . created_at , offset = False ) if model . created_at else None \n 
attrs . pop ( , None ) \n 
attrs [ ] = None \n 
return cls ( ** attrs ) \n 
~~ ~~ from st2common . models . db . rule import ( ActionExecutionSpecDB , RuleDB ) \n 
from st2common . models . db . sensor import SensorTypeDB \n 
from st2common . models . db . trigger import ( TriggerDB , TriggerTypeDB , TriggerInstanceDB ) \n 
MODELS = [ RuleDB , SensorTypeDB , TriggerDB , TriggerInstanceDB , \n 
TriggerTypeDB ] \n 
from st2common . models . db import MongoDBAccess \n 
from st2common . models . db . marker import MarkerDB \n 
from st2common . models . db . marker import DumperMarkerDB \n 
from st2common . persistence . base import Access \n 
class Marker ( Access ) : \n 
~~~ impl = MongoDBAccess ( MarkerDB ) \n 
publisher = None \n 
def _get_impl ( cls ) : \n 
~~~ return cls . impl \n 
~~ ~~ class DumperMarker ( Access ) : \n 
~~~ impl = MongoDBAccess ( DumperMarkerDB ) \n 
~~ ~~ from st2common . rbac . types import PermissionType \n 
from st2common . rbac . types import SystemRole \n 
def get_all_roles ( exclude_system = False ) : \n 
if exclude_system : \n 
~~~ result = Role . query ( system = False ) \n 
~~~ result = Role . get_all ( ) \n 
~~ def get_system_roles ( ) : \n 
result = Role . query ( system = True ) \n 
~~ def get_roles_for_user ( user_db ) : \n 
role_names = UserRoleAssignment . query ( user = user_db . name ) . only ( ) . scalar ( ) \n 
result = Role . query ( name__in = role_names ) \n 
~~ def get_role_assignments_for_user ( user_db ) : \n 
result = UserRoleAssignment . query ( user = user_db . name ) \n 
~~ def get_role_by_name ( name ) : \n 
result = Role . get ( name = name ) \n 
~~ def create_role ( name , description = None ) : \n 
if name in SystemRole . get_valid_values ( ) : \n 
~~ role_db = RoleDB ( name = name , description = description ) \n 
role_db = Role . add_or_update ( role_db ) \n 
return role_db \n 
~~ def delete_role ( name ) : \n 
~~~ raise ValueError ( ) \n 
~~ role_db = Role . get ( name = name ) \n 
result = Role . delete ( role_db ) \n 
~~ def assign_role_to_user ( role_db , user_db , description = None ) : \n 
role_assignment_db = UserRoleAssignmentDB ( user = user_db . name , role = role_db . name , \n 
description = description ) \n 
role_assignment_db = UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
return role_assignment_db \n 
~~ def revoke_role_from_user ( role_db , user_db ) : \n 
role_assignment_db = UserRoleAssignment . get ( user = user_db . name , role = role_db . name ) \n 
result = UserRoleAssignment . delete ( role_assignment_db ) \n 
~~ def get_all_permission_grants_for_user ( user_db , resource_uid = None , resource_types = None , \n 
permission_types = None ) : \n 
permission_grant_ids = Role . query ( name__in = role_names ) . scalar ( ) \n 
permission_grant_ids = sum ( permission_grant_ids , [ ] ) \n 
permission_grants_filters = { } \n 
permission_grants_filters [ ] = permission_grant_ids \n 
if resource_uid : \n 
~~~ permission_grants_filters [ ] = resource_uid \n 
~~ if resource_types : \n 
~~~ permission_grants_filters [ ] = resource_types \n 
~~ if permission_types : \n 
~~~ permission_grants_filters [ ] = permission_types \n 
~~ permission_grant_dbs = PermissionGrant . query ( ** permission_grants_filters ) \n 
return permission_grant_dbs \n 
~~ def create_permission_grant_for_resource_db ( role_db , resource_db , permission_types ) : \n 
permission_types = _validate_permission_types ( resource_db = resource_db , \n 
permission_types = permission_types ) \n 
resource_uid = resource_db . get_uid ( ) \n 
resource_type = resource_db . get_resource_type ( ) \n 
result = create_permission_grant ( role_db = role_db , resource_uid = resource_uid , \n 
resource_type = resource_type , \n 
~~ def create_permission_grant ( role_db , resource_uid , resource_type , permission_types ) : \n 
permission_grant_db = PermissionGrantDB ( resource_uid = resource_uid , \n 
permission_grant_db = PermissionGrant . add_or_update ( permission_grant_db ) \n 
role_db . update ( push__permission_grants = permission_grant_db . id ) \n 
return permission_grant_db \n 
~~ def remove_permission_grant_for_resource_db ( role_db , resource_db , permission_types ) : \n 
permission_grant_db = PermissionGrant . get ( resource_uid = resource_uid , \n 
role_db . update ( pull__permission_grants = permission_grant_db . id ) \n 
~~ def _validate_resource_type ( resource_db ) : \n 
valid_resource_types = ResourceType . get_valid_values ( ) \n 
if resource_type not in valid_resource_types : \n 
~~~ raise ValueError ( % \n 
( resource_type ) ) \n 
~~ return resource_db \n 
~~ def _validate_permission_types ( resource_db , permission_types ) : \n 
resource_db = _validate_resource_type ( resource_db = resource_db ) \n 
valid_permission_types = PermissionType . get_valid_permissions_for_resource_type ( resource_type ) \n 
for permission_type in permission_types : \n 
~~~ if permission_type not in valid_permission_types : \n 
~~~ raise ValueError ( % ( permission_type ) ) \n 
~~ ~~ return permission_types \n 
~~ import binascii \n 
def symmetric_encrypt ( encrypt_key , message ) : \n 
return binascii . hexlify ( encrypt_key . Encrypt ( message ) ) . upper ( ) \n 
~~ def symmetric_decrypt ( decrypt_key , crypto ) : \n 
return decrypt_key . Decrypt ( binascii . unhexlify ( crypto ) ) \n 
from st2common . models . db . stormbase import UIDFieldMixin \n 
def parse_uid ( uid ) : \n 
if UIDFieldMixin . UID_SEPARATOR not in uid : \n 
~~~ raise ValueError ( % ( uid ) ) \n 
~~ parsed = uid . split ( UIDFieldMixin . UID_SEPARATOR ) \n 
if len ( parsed ) < 2 : \n 
~~ resource_type = parsed [ 0 ] \n 
uid_remainder = parsed [ 1 : ] \n 
return ( resource_type , uid_remainder ) \n 
from st2common . bootstrap import aliasesregistrar \n 
from st2tests import DbTestCase , fixturesloader \n 
ALIASES_FIXTURE_PACK_PATH = os . path . join ( fixturesloader . get_fixtures_base_path ( ) , ) \n 
ALIASES_FIXTURE_PATH = os . path . join ( ALIASES_FIXTURE_PACK_PATH , ) \n 
class TestAliasRegistrar ( DbTestCase ) : \n 
~~~ def test_alias_registration ( self ) : \n 
~~~ count = aliasesregistrar . register_aliases ( pack_dir = ALIASES_FIXTURE_PACK_PATH ) \n 
self . assertEqual ( count , len ( os . listdir ( ALIASES_FIXTURE_PATH ) ) ) \n 
~~ ~~ from st2tests . base import CleanDbTestCase \n 
from st2common . models . db . keyvalue import KeyValuePairDB \n 
from st2common . persistence . keyvalue import KeyValuePair \n 
from st2common . services . keyvalues import KeyValueLookup \n 
class TestKeyValueLookup ( CleanDbTestCase ) : \n 
~~~ def test_non_hierarchical_lookup ( self ) : \n 
~~~ k1 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k2 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k3 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
lookup = KeyValueLookup ( ) \n 
self . assertEquals ( str ( lookup . k1 ) , k1 . value ) \n 
self . assertEquals ( str ( lookup . k2 ) , k2 . value ) \n 
self . assertEquals ( str ( lookup . k3 ) , k3 . value ) \n 
~~ def test_hierarchical_lookup_dotted ( self ) : \n 
self . assertEquals ( str ( lookup . a . b ) , k1 . value ) \n 
self . assertEquals ( str ( lookup . a . b . c ) , k2 . value ) \n 
self . assertEquals ( str ( lookup . b . c ) , k3 . value ) \n 
self . assertEquals ( str ( lookup . a ) , ) \n 
~~ def test_hierarchical_lookup_dict ( self ) : \n 
self . assertEquals ( str ( lookup [ ] [ ] ) , k1 . value ) \n 
self . assertEquals ( str ( lookup [ ] [ ] [ ] ) , k2 . value ) \n 
self . assertEquals ( str ( lookup [ ] [ ] ) , k3 . value ) \n 
self . assertEquals ( str ( lookup [ ] ) , ) \n 
~~ def test_missing_key_lookup ( self ) : \n 
~~~ lookup = KeyValueLookup ( ) \n 
self . assertEquals ( str ( lookup . missing_key ) , ) \n 
self . assertTrue ( lookup . missing_key , ) \n 
~~ def test_secret_lookup ( self ) : \n 
~~~ secret_value = + \n 
k1 = KeyValuePair . add_or_update ( KeyValuePairDB ( \n 
name = , value = secret_value , \n 
secret = True , encrypted = True ) \n 
import bson \n 
from st2common . triggers import register_internal_trigger_types \n 
from st2common . persistence . rule import Rule \n 
from st2common . persistence . rule_enforcement import RuleEnforcement \n 
from st2common . models . db . rule import RuleDB \n 
from st2common . models . db . rule_enforcement import RuleEnforcementDB \n 
from st2common . rbac . resolvers import RuleEnforcementPermissionsResolver \n 
from tests . unit . test_rbac_resolvers import BasePermissionsResolverTestCase \n 
class RuleEnforcementPermissionsResolverTestCase ( BasePermissionsResolverTestCase ) : \n 
~~~ super ( RuleEnforcementPermissionsResolverTestCase , self ) . setUp ( ) \n 
register_internal_trigger_types ( ) \n 
user_3_db = UserDB ( name = ) \n 
user_3_db = User . add_or_update ( user_3_db ) \n 
self . users [ ] = user_3_db \n 
user_4_db = UserDB ( name = ) \n 
user_4_db = User . add_or_update ( user_4_db ) \n 
self . users [ ] = user_4_db \n 
user_5_db = UserDB ( name = ) \n 
user_5_db = User . add_or_update ( user_5_db ) \n 
self . users [ ] = user_5_db \n 
user_6_db = UserDB ( name = ) \n 
user_6_db = User . add_or_update ( user_6_db ) \n 
self . users [ ] = user_6_db \n 
user_7_db = UserDB ( name = ) \n 
user_7_db = User . add_or_update ( user_7_db ) \n 
self . users [ ] = user_7_db \n 
user_8_db = UserDB ( name = ) \n 
user_8_db = User . add_or_update ( user_8_db ) \n 
self . users [ ] = user_8_db \n 
user_9_db = UserDB ( name = ) \n 
user_9_db = User . add_or_update ( user_9_db ) \n 
self . users [ ] = user_9_db \n 
user_10_db = UserDB ( name = ) \n 
user_10_db = User . add_or_update ( user_10_db ) \n 
self . users [ ] = user_10_db \n 
rule_1_db = RuleDB ( pack = , name = , action = { : } , \n 
trigger = ) \n 
rule_1_db = Rule . add_or_update ( rule_1_db ) \n 
self . resources [ ] = rule_1_db \n 
rule_enforcement_1_db = RuleEnforcementDB ( trigger_instance_id = str ( bson . ObjectId ( ) ) , \n 
execution_id = str ( bson . ObjectId ( ) ) , \n 
rule = { : rule_1_db . ref , \n 
: rule_1_db . uid , \n 
: str ( rule_1_db . id ) } ) \n 
rule_enforcement_1_db = RuleEnforcement . add_or_update ( rule_enforcement_1_db ) \n 
self . resources [ ] = rule_enforcement_1_db \n 
rule_2_db = RuleDB ( pack = , name = ) \n 
rule_2_db = Rule . add_or_update ( rule_2_db ) \n 
self . resources [ ] = rule_2_db \n 
rule_enforcement_2_db = RuleEnforcementDB ( trigger_instance_id = str ( bson . ObjectId ( ) ) , \n 
rule = { : rule_2_db . ref , \n 
: rule_2_db . uid , \n 
: str ( rule_2_db . id ) } ) \n 
rule_enforcement_2_db = RuleEnforcement . add_or_update ( rule_enforcement_2_db ) \n 
self . resources [ ] = rule_enforcement_2_db \n 
rule_3_db = RuleDB ( pack = , name = ) \n 
rule_3_db = Rule . add_or_update ( rule_3_db ) \n 
self . resources [ ] = rule_3_db \n 
rule_enforcement_3_db = RuleEnforcementDB ( trigger_instance_id = str ( bson . ObjectId ( ) ) , \n 
rule = { : rule_3_db . ref , \n 
: rule_3_db . uid , \n 
: str ( rule_3_db . id ) } ) \n 
rule_enforcement_3_db = RuleEnforcement . add_or_update ( rule_enforcement_3_db ) \n 
self . resources [ ] = rule_enforcement_3_db \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
permission_types = [ PermissionType . RULE_VIEW ] ) \n 
role_3_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_3_db = Role . add_or_update ( role_3_db ) \n 
self . roles [ ] = role_3_db \n 
resource_type = ResourceType . RULE , \n 
role_4_db = RoleDB ( name = , permission_grants = permission_grants ) \n 
role_4_db = Role . add_or_update ( role_4_db ) \n 
self . roles [ ] = role_4_db \n 
permission_types = [ PermissionType . RULE_ALL ] ) \n 
role_4_db = RoleDB ( name = , \n 
permission_types = [ PermissionType . RULE_MODIFY ] ) \n 
role_5_db = RoleDB ( name = , \n 
role_5_db = Role . add_or_update ( role_5_db ) \n 
self . roles [ ] = role_5_db \n 
permission_types = [ PermissionType . RULE_CREATE ] ) \n 
role_6_db = RoleDB ( name = , \n 
role_6_db = Role . add_or_update ( role_6_db ) \n 
self . roles [ ] = role_6_db \n 
role_7_db = RoleDB ( name = , \n 
role_7_db = Role . add_or_update ( role_7_db ) \n 
self . roles [ ] = role_7_db \n 
role_8_db = RoleDB ( name = , \n 
role_8_db = Role . add_or_update ( role_8_db ) \n 
self . roles [ ] = role_8_db \n 
role_9_db = RoleDB ( name = , \n 
role_9_db = Role . add_or_update ( role_9_db ) \n 
self . roles [ ] = role_9_db \n 
grant_db = PermissionGrantDB ( resource_uid = None , \n 
resource_type = None , \n 
permission_types = [ PermissionType . RULE_LIST ] ) \n 
role_10_db = RoleDB ( name = , \n 
role_10_db = Role . add_or_update ( role_10_db ) \n 
self . roles [ ] = role_10_db \n 
role_assignment_db = UserRoleAssignmentDB ( user = user_db . name , \n 
~~ def test_user_has_permission ( self ) : \n 
~~~ resolver = RuleEnforcementPermissionsResolver ( ) \n 
permission_type = PermissionType . RULE_ENFORCEMENT_LIST \n 
self . assertTrue ( resolver . user_has_permission ( user_db = user_db , \n 
permission_type = permission_type ) ) \n 
self . assertFalse ( resolver . user_has_permission ( user_db = user_db , \n 
~~ def test_user_has_resource_db_permission ( self ) : \n 
all_permission_types = PermissionType . get_valid_permissions_for_resource_type ( \n 
ResourceType . RULE_ENFORCEMENT ) \n 
resource_db = self . resources [ ] \n 
self . assertTrue ( self . _user_has_resource_db_permissions ( \n 
resolver = resolver , \n 
user_db = user_db , \n 
resource_db = resource_db , \n 
permission_types = all_permission_types ) ) \n 
self . assertTrue ( resolver . user_has_resource_db_permission ( \n 
resource_db = self . resources [ ] , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
self . assertFalse ( self . _user_has_resource_db_permissions ( \n 
self . assertFalse ( resolver . user_has_resource_db_permission ( \n 
import tarfile \n 
import httplib \n 
import gnupg \n 
import st2common \n 
from st2common . content . utils import get_packs_base_paths \n 
from st2common import __version__ as st2_version \n 
from st2common import config \n 
from st2common . util import date as date_utils \n 
from st2common . util . shell import run_command \n 
from st2debug . constants import GPG_KEY \n 
from st2debug . constants import GPG_KEY_FINGERPRINT \n 
from st2debug . constants import S3_BUCKET_URL \n 
from st2debug . constants import COMPANY_NAME \n 
from st2debug . constants import ARG_NAMES \n 
from st2debug . utils . fs import copy_files \n 
from st2debug . utils . fs import get_full_file_list \n 
from st2debug . utils . fs import get_dirs_in_path \n 
from st2debug . utils . fs import remove_file \n 
from st2debug . utils . system_info import get_cpu_info \n 
from st2debug . utils . system_info import get_memory_info \n 
from st2debug . utils . system_info import get_package_list \n 
from st2debug . utils . git_utils import get_repo_latest_revision_hash \n 
from st2debug . processors import process_st2_config \n 
from st2debug . processors import process_mistral_config \n 
from st2debug . processors import process_content_pack_dir \n 
GPG_INSTALLED = find_executable ( ) is not None \n 
LOG_FILE_PATHS = [ \n 
ST2_CONFIG_FILE_PATH = \n 
MISTRAL_CONFIG_FILE_PATH = \n 
SHELL_COMMANDS = [ ] \n 
DIRECTORY_STRUCTURE = [ \n 
OUTPUT_PATHS = { \n 
ST2_CONF_OPTIONS_TO_REMOVE = { \n 
REMOVE_VALUE_NAME = \n 
OUTPUT_FILENAME_TEMPLATE = \n 
DATE_FORMAT = \n 
~~~ config . parse_args ( args = [ ] ) \n 
~~ def setup_logging ( ) : \n 
~~~ root = LOG \n 
root . setLevel ( logging . INFO ) \n 
ch = logging . StreamHandler ( sys . stdout ) \n 
ch . setLevel ( logging . DEBUG ) \n 
ch . setFormatter ( formatter ) \n 
root . addHandler ( ch ) \n 
~~ class DebugInfoCollector ( object ) : \n 
~~~ def __init__ ( self , include_logs , include_configs , include_content , include_system_info , \n 
include_shell_commands = False , user_info = None , debug = False , config_file = None , \n 
output_path = None ) : \n 
self . include_logs = include_logs \n 
self . include_configs = include_configs \n 
self . include_content = include_content \n 
self . include_system_info = include_system_info \n 
self . include_shell_commands = include_shell_commands \n 
self . user_info = user_info \n 
self . debug = debug \n 
self . output_path = output_path \n 
config_file = config_file or { } \n 
self . st2_config_file_path = config_file . get ( , ST2_CONFIG_FILE_PATH ) \n 
self . mistral_config_file_path = config_file . get ( , \n 
MISTRAL_CONFIG_FILE_PATH ) \n 
self . log_file_paths = config_file . get ( , LOG_FILE_PATHS [ : ] ) \n 
self . gpg_key = config_file . get ( , GPG_KEY ) \n 
self . gpg_key_fingerprint = config_file . get ( , GPG_KEY_FINGERPRINT ) \n 
self . s3_bucket_url = config_file . get ( , S3_BUCKET_URL ) \n 
self . company_name = config_file . get ( , COMPANY_NAME ) \n 
self . shell_commands = config_file . get ( , SHELL_COMMANDS ) \n 
self . st2_config_file_name = os . path . basename ( self . st2_config_file_path ) \n 
self . mistral_config_file_name = os . path . basename ( self . mistral_config_file_path ) \n 
self . config_file_paths = [ \n 
self . st2_config_file_path , \n 
self . mistral_config_file_path \n 
~~ def run ( self , encrypt = False , upload = False , existing_file = None ) : \n 
temp_files = [ ] \n 
~~~ if existing_file : \n 
~~~ working_file = existing_file \n 
~~~ working_file = self . create_archive ( ) \n 
if not encrypt and not upload : \n 
~~~ LOG . info ( \n 
% working_file ) \n 
~~~ temp_files . append ( working_file ) \n 
~~ ~~ if encrypt : \n 
~~~ working_file = self . encrypt_archive ( archive_file_path = working_file ) \n 
if not upload : \n 
~~~ LOG . info ( % \n 
working_file ) \n 
~~ ~~ if upload : \n 
~~~ self . upload_archive ( archive_file_path = working_file ) \n 
tarball_name = os . path . basename ( working_file ) \n 
LOG . info ( % \n 
( self . company_name , tarball_name ) ) \n 
LOG . info ( \n 
% tarball_name ) \n 
~~~ for temp_file in temp_files : \n 
~~~ assert temp_file . startswith ( ) \n 
remove_file ( file_path = temp_file ) \n 
~~ ~~ ~~ def create_archive ( self ) : \n 
~~~ temp_dir_path = self . create_temp_directories ( ) \n 
output_paths = { } \n 
for key , path in OUTPUT_PATHS . iteritems ( ) : \n 
~~~ output_paths [ key ] = os . path . join ( temp_dir_path , path ) \n 
~~ LOG . info ( ) \n 
if self . include_logs : \n 
~~~ self . collect_logs ( output_paths [ ] ) \n 
~~ if self . include_configs : \n 
~~~ self . collect_config_files ( output_paths [ ] ) \n 
~~ if self . include_content : \n 
~~~ self . collect_pack_content ( output_paths [ ] ) \n 
~~ if self . include_system_info : \n 
~~~ self . add_system_information ( output_paths [ ] ) \n 
~~ if self . user_info : \n 
~~~ self . add_user_info ( output_paths [ ] ) \n 
~~ if self . include_shell_commands : \n 
~~~ self . add_shell_command_output ( output_paths [ ] ) \n 
~~ return self . create_tarball ( temp_dir_path ) \n 
~~~ LOG . exception ( , exc_info = True ) \n 
raise e \n 
~~ ~~ def encrypt_archive ( self , archive_file_path ) : \n 
~~~ assert archive_file_path . endswith ( ) \n 
LOG . info ( ) \n 
gpg = gnupg . GPG ( verbose = self . debug ) \n 
import_result = gpg . import_keys ( self . gpg_key ) \n 
assert import_result . count == 1 \n 
encrypted_archive_output_file_name = os . path . basename ( archive_file_path ) + \n 
encrypted_archive_output_file_path = os . path . join ( , \n 
encrypted_archive_output_file_name ) \n 
with open ( archive_file_path , ) as fp : \n 
~~~ gpg . encrypt_file ( file = fp , \n 
recipients = self . gpg_key_fingerprint , \n 
always_trust = True , \n 
output = encrypted_archive_output_file_path ) \n 
~~ return encrypted_archive_output_file_path \n 
~~ ~~ def upload_archive ( self , archive_file_path ) : \n 
LOG . debug ( ) \n 
file_name = os . path . basename ( archive_file_path ) \n 
url = self . s3_bucket_url + file_name \n 
assert url . startswith ( ) \n 
~~~ response = requests . put ( url = url , files = { : fp } ) \n 
~~ assert response . status_code == httplib . OK \n 
~~~ LOG . exception ( % self . company_name , exc_info = True ) \n 
~~ ~~ def collect_logs ( self , output_path ) : \n 
for file_path_glob in self . log_file_paths : \n 
~~~ log_file_list = get_full_file_list ( file_path_glob = file_path_glob ) \n 
copy_files ( file_paths = log_file_list , destination = output_path ) \n 
~~ ~~ def collect_config_files ( self , output_path ) : \n 
copy_files ( file_paths = self . config_file_paths , destination = output_path ) \n 
st2_config_path = os . path . join ( output_path , self . st2_config_file_name ) \n 
process_st2_config ( config_path = st2_config_path ) \n 
mistral_config_path = os . path . join ( output_path , self . mistral_config_file_name ) \n 
process_mistral_config ( config_path = mistral_config_path ) \n 
def collect_pack_content ( output_path ) : \n 
packs_base_paths = get_packs_base_paths ( ) \n 
for index , packs_base_path in enumerate ( packs_base_paths , 1 ) : \n 
~~~ dst = os . path . join ( output_path , % index ) \n 
~~~ shutil . copytree ( src = packs_base_path , dst = dst ) \n 
~~ ~~ base_pack_dirs = get_dirs_in_path ( file_path = output_path ) \n 
for base_pack_dir in base_pack_dirs : \n 
~~~ pack_dirs = get_dirs_in_path ( file_path = base_pack_dir ) \n 
for pack_dir in pack_dirs : \n 
~~~ process_content_pack_dir ( pack_dir = pack_dir ) \n 
~~ ~~ ~~ def add_system_information ( self , output_path ) : \n 
system_information = yaml . dump ( self . get_system_information ( ) , \n 
default_flow_style = False ) \n 
with open ( output_path , ) as fp : \n 
~~~ fp . write ( system_information ) \n 
~~ ~~ def add_user_info ( self , output_path ) : \n 
user_info = yaml . dump ( self . user_info , default_flow_style = False ) \n 
~~~ fp . write ( user_info ) \n 
~~ ~~ def add_shell_command_output ( self , output_path ) : \n 
for cmd in self . shell_commands : \n 
~~~ output_file = os . path . join ( output_path , % self . format_output_filename ( cmd ) ) \n 
exit_code , stdout , stderr = run_command ( cmd = cmd , shell = True ) \n 
with open ( output_file , ) as fp : \n 
~~~ fp . write ( ) \n 
fp . write ( stdout ) \n 
fp . write ( ) \n 
fp . write ( stderr ) \n 
~~ ~~ ~~ def create_tarball ( self , temp_dir_path ) : \n 
if self . output_path : \n 
~~~ output_file_path = self . output_path \n 
~~~ date = date_utils . get_datetime_utc_now ( ) . strftime ( DATE_FORMAT ) \n 
values = { : socket . gethostname ( ) , : date } \n 
output_file_name = OUTPUT_FILENAME_TEMPLATE % values \n 
output_file_path = os . path . join ( , output_file_name ) \n 
~~ with tarfile . open ( output_file_path , ) as tar : \n 
~~~ tar . add ( temp_dir_path , arcname = ) \n 
~~ return output_file_path \n 
def create_temp_directories ( ) : \n 
temp_dir_path = tempfile . mkdtemp ( ) \n 
for directory_name in DIRECTORY_STRUCTURE : \n 
~~~ full_path = os . path . join ( temp_dir_path , directory_name ) \n 
os . mkdir ( full_path ) \n 
~~ return temp_dir_path \n 
def format_output_filename ( cmd ) : \n 
def get_system_information ( ) : \n 
system_information = { \n 
: socket . gethostname ( ) , \n 
: { } \n 
system_information [ ] [ ] = platform . system ( ) \n 
system_information [ ] [ ] = platform . release ( ) \n 
system_information [ ] [ ] = platform . platform ( ) \n 
system_information [ ] [ ] = . join ( platform . architecture ( ) ) \n 
~~~ distribution = . join ( platform . linux_distribution ( ) ) \n 
system_information [ ] [ ] = distribution \n 
~~ system_information [ ] [ ] = sys . version . split ( ) [ 0 ] \n 
cpu_info = get_cpu_info ( ) \n 
if cpu_info : \n 
~~~ core_count = len ( cpu_info ) \n 
model = cpu_info [ 0 ] [ ] \n 
system_information [ ] [ ] = { \n 
: core_count , \n 
: model \n 
~~~ system_information [ ] [ ] = \n 
~~ memory_info = get_memory_info ( ) \n 
if memory_info : \n 
~~~ total = memory_info [ ] / 1024 \n 
free = memory_info [ ] / 1024 \n 
used = ( total - free ) \n 
: total , \n 
: used , \n 
: free \n 
~~ system_information [ ] [ ] = st2_version \n 
st2common_path = st2common . __file__ \n 
st2common_path = os . path . dirname ( st2common_path ) \n 
if in st2common_path : \n 
~~~ base_install_path = st2common_path . replace ( , ) \n 
revision_hash = get_repo_latest_revision_hash ( repo_path = base_install_path ) \n 
system_information [ ] [ ] = \n 
system_information [ ] [ ] = revision_hash \n 
~~~ package_list = get_package_list ( name_startswith = ) \n 
system_information [ ] [ ] = package_list \n 
~~ repo_path = \n 
revision_hash = get_repo_latest_revision_hash ( repo_path = repo_path ) \n 
return system_information \n 
~~~ parser = argparse . ArgumentParser ( description = ) \n 
parser . add_argument ( , action = , default = False , \n 
parser . add_argument ( , action = , default = None , \n 
setup_logging ( ) \n 
abort = True \n 
for arg_name in ARG_NAMES : \n 
~~~ abort &= getattr ( args , arg_name , False ) \n 
~~ if abort : \n 
sys . exit ( 2 ) \n 
~~ if args . config : \n 
~~~ with open ( args . config , ) as yaml_file : \n 
~~~ config_file = yaml . safe_load ( yaml_file ) \n 
~~~ LOG . error ( % e ) \n 
~~ if not isinstance ( config_file , dict ) : \n 
~~~ LOG . error ( ) \n 
~~~ config_file = { } \n 
~~ company_name = config_file . get ( , COMPANY_NAME ) \n 
encrypt = True \n 
upload = True \n 
if args . review : \n 
~~~ encrypt = False \n 
upload = False \n 
~~ if encrypt : \n 
~~~ if not GPG_INSTALLED : \n 
raise ValueError ( msg ) \n 
~~ ~~ if not args . yes and not args . existing_file and upload : \n 
~~~ submitted_content = [ name . replace ( , ) for name in ARG_NAMES if \n 
not getattr ( args , name , False ) ] \n 
submitted_content = . join ( submitted_content ) \n 
print ( % ( company_name , \n 
submitted_content ) ) \n 
value = six . moves . input ( ) \n 
if value . strip ( ) . lower ( ) not in [ , ] : \n 
~~ ~~ user_info = { } \n 
if not args . yes and not args . existing_file : \n 
~~~ print ( \n 
if value . strip ( ) . lower ( ) in [ , ] : \n 
~~~ user_info [ ] = six . moves . input ( ) \n 
user_info [ ] = six . moves . input ( ) \n 
~~ ~~ debug_collector = DebugInfoCollector ( include_logs = not args . exclude_logs , \n 
include_configs = not args . exclude_configs , \n 
include_content = not args . exclude_content , \n 
include_system_info = not args . exclude_system_info , \n 
include_shell_commands = not args . exclude_shell_commands , \n 
user_info = user_info , \n 
debug = args . debug , \n 
config_file = config_file , \n 
output_path = args . output ) \n 
debug_collector . run ( encrypt = encrypt , upload = upload , existing_file = args . existing_file ) \n 
from st2reactor . container . process_container import ProcessSensorContainer \n 
from st2common . services . sensor_watcher import SensorWatcher \n 
from st2common . models . system . common import ResourceReference \n 
class SensorContainerManager ( object ) : \n 
~~~ def __init__ ( self , sensors_partitioner ) : \n 
~~~ self . _sensor_container = None \n 
self . _sensors_watcher = SensorWatcher ( create_handler = self . _handle_create_sensor , \n 
update_handler = self . _handle_update_sensor , \n 
delete_handler = self . _handle_delete_sensor , \n 
queue_suffix = ) \n 
self . _container_thread = None \n 
if not sensors_partitioner : \n 
~~ self . _sensors_partitioner = sensors_partitioner \n 
~~ def run_sensors ( self ) : \n 
sensors = self . _sensors_partitioner . get_sensors ( ) \n 
if sensors : \n 
~~~ LOG . info ( , len ( sensors ) ) \n 
LOG . info ( , [ self . _get_sensor_ref ( sensor ) for sensor in sensors ] ) \n 
~~ sensors_to_run = [ ] \n 
for sensor in sensors : \n 
~~~ sensors_to_run . append ( self . _to_sensor_object ( sensor ) ) \n 
~~ LOG . info ( , os . getpid ( ) ) \n 
self . _setup_sigterm_handler ( ) \n 
self . _spin_container_and_wait ( sensors_to_run ) \n 
~~ def _spin_container_and_wait ( self , sensors ) : \n 
~~~ self . _sensor_container = ProcessSensorContainer ( sensors = sensors ) \n 
self . _container_thread = eventlet . spawn ( self . _sensor_container . run ) \n 
self . _sensors_watcher . start ( ) \n 
exit_code = self . _container_thread . wait ( ) \n 
LOG . error ( , exit_code ) \n 
LOG . error ( , os . getpid ( ) ) \n 
~~ except ( KeyboardInterrupt , SystemExit ) : \n 
~~~ self . _sensor_container . shutdown ( ) \n 
self . _sensors_watcher . stop ( ) \n 
sys . exc_info ( ) [ 0 ] . __name__ ) \n 
eventlet . kill ( self . _container_thread ) \n 
~~ ~~ def _setup_sigterm_handler ( self ) : \n 
~~~ def sigterm_handler ( signum = None , frame = None ) : \n 
~~ signal . signal ( signal . SIGTERM , sigterm_handler ) \n 
~~ def _to_sensor_object ( self , sensor_db ) : \n 
~~~ file_path = sensor_db . artifact_uri . replace ( , ) \n 
class_name = sensor_db . entry_point . split ( ) [ - 1 ] \n 
sensor_obj = { \n 
: sensor_db . pack , \n 
: class_name , \n 
: sensor_db . trigger_types , \n 
: sensor_db . poll_interval , \n 
: self . _get_sensor_ref ( sensor_db ) \n 
return sensor_obj \n 
################################################# \n 
~~ def _handle_create_sensor ( self , sensor ) : \n 
~~~ if not self . _sensors_partitioner . is_sensor_owner ( sensor ) : \n 
~~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
~~ if not sensor . enabled : \n 
~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
self . _sensor_container . add_sensor ( sensor = self . _to_sensor_object ( sensor ) ) \n 
~~ def _handle_update_sensor ( self , sensor ) : \n 
~~ sensor_ref = self . _get_sensor_ref ( sensor ) \n 
sensor_obj = self . _to_sensor_object ( sensor ) \n 
if not sensor . enabled : \n 
~~~ LOG . info ( , sensor_ref ) \n 
self . _sensor_container . remove_sensor ( sensor = sensor_obj ) \n 
~~ LOG . info ( , sensor_ref ) \n 
~~~ self . _sensor_container . remove_sensor ( sensor = sensor_obj ) \n 
~~~ LOG . exception ( , sensor_ref ) \n 
~~~ self . _sensor_container . add_sensor ( sensor = sensor_obj ) \n 
LOG . info ( , sensor_ref ) \n 
~~ ~~ def _handle_delete_sensor ( self , sensor ) : \n 
self . _sensor_container . remove_sensor ( sensor = self . _to_sensor_object ( sensor ) ) \n 
~~ def _get_sensor_ref ( self , sensor ) : \n 
~~~ return ResourceReference . to_string_reference ( pack = sensor . pack , name = sensor . name ) \n 
~~ ~~ import math \n 
from random_words import RandomWords \n 
from st2reactor . container . hash_partitioner import HashPartitioner , Range \n 
from st2tests import config \n 
FIXTURES_1 = { \n 
: [ , , ] \n 
class HashPartitionerTest ( DbTestCase ) : \n 
~~~ models = None \n 
~~~ super ( HashPartitionerTest , cls ) . setUpClass ( ) \n 
cls . models = FixturesLoader ( ) . save_fixtures_to_db ( \n 
fixtures_pack = PACK , fixtures_dict = FIXTURES_1 ) \n 
config . parse_args ( ) \n 
~~ def test_full_range_hash_partitioner ( self ) : \n 
~~~ partitioner = HashPartitioner ( , ) \n 
sensors = partitioner . get_sensors ( ) \n 
self . assertEqual ( len ( sensors ) , 3 , ) \n 
~~ def test_multi_range_hash_partitioner ( self ) : \n 
~~~ range_third = int ( Range . RANGE_MAX_VALUE / 3 ) \n 
range_two_third = range_third * 2 \n 
hash_ranges = . format ( \n 
range_third = range_third , range_two_third = range_two_third ) \n 
partitioner = HashPartitioner ( , hash_ranges ) \n 
~~ def test_split_range_hash_partitioner ( self ) : \n 
~~~ range_mid = int ( Range . RANGE_MAX_VALUE / 2 ) \n 
partitioner = HashPartitioner ( , % range_mid ) \n 
sensors1 = partitioner . get_sensors ( ) \n 
sensors2 = partitioner . get_sensors ( ) \n 
self . assertEqual ( len ( sensors1 ) + len ( sensors2 ) , 3 , ) \n 
~~ def test_hash_effectiveness ( self ) : \n 
partitioner1 = HashPartitioner ( , % range_third ) \n 
partitioner2 = HashPartitioner ( , % ( range_third , range_third + range_third ) ) partitioner3 = HashPartitioner ( , % ( range_third + range_third ) ) \n 
refs_count = 1000 \n 
refs = self . _generate_refs ( count = refs_count ) \n 
p1_count = 0 \n 
p2_count = 0 \n 
p3_count = 0 \n 
for ref in refs : \n 
~~~ if partitioner1 . _is_in_hash_range ( ref ) : \n 
~~~ p1_count += 1 \n 
~~ if partitioner2 . _is_in_hash_range ( ref ) : \n 
~~~ p2_count += 1 \n 
~~ if partitioner3 . _is_in_hash_range ( ref ) : \n 
~~~ p3_count += 1 \n 
~~ ~~ self . assertEqual ( p1_count + p2_count + p3_count , refs_count , \n 
mean = refs_count / 3 \n 
variance = float ( ( p1_count - mean ) ** 2 + ( p1_count - mean ) ** 2 + ( p3_count - mean ) ** 2 ) / 3 \n 
sd = math . sqrt ( variance ) \n 
self . assertTrue ( sd / mean <= 0.2 , ) \n 
~~ def _generate_refs ( self , count = 10 ) : \n 
~~~ random_word_count = int ( math . sqrt ( count ) ) + 1 \n 
words = RandomWords ( ) . random_words ( count = random_word_count ) \n 
x_index = 0 \n 
y_index = 0 \n 
while count > 0 : \n 
~~~ yield % ( words [ x_index ] , words [ y_index ] ) \n 
if y_index < len ( words ) - 1 : \n 
~~~ y_index += 1 \n 
~~~ x_index += 1 \n 
~~ count -= 1 \n 
~~ ~~ from unittest2 import TestCase \n 
from st2actions . runners . utils import get_action_class_instance \n 
from st2tests . mocks . action import MockActionWrapper \n 
from st2tests . mocks . action import MockActionService \n 
class BaseActionTestCase ( TestCase ) : \n 
action_cls = None \n 
~~~ super ( BaseActionTestCase , self ) . setUp ( ) \n 
class_name = self . action_cls . __name__ \n 
action_wrapper = MockActionWrapper ( pack = , class_name = class_name ) \n 
self . action_service = MockActionService ( action_wrapper = action_wrapper ) \n 
~~ def get_action_instance ( self , config = None ) : \n 
instance = get_action_class_instance ( action_cls = self . action_cls , \n 
config = config , \n 
action_service = self . action_service ) \n 
return instance \n 
import sets \n 
from st2common . service_setup import db_setup \n 
~~~ from graphviz import Digraph \n 
raise ImportError ( msg ) \n 
~~ def do_register_cli_opts ( opts , ignore_errors = False ) : \n 
~~~ for opt in opts : \n 
~~~ cfg . CONF . register_cli_opt ( opt ) \n 
~~ ~~ ~~ ~~ class RuleLink ( object ) : \n 
~~~ def __init__ ( self , source_action_ref , rule_ref , dest_action_ref ) : \n 
~~~ self . _source_action_ref = source_action_ref \n 
self . _rule_ref = rule_ref \n 
self . _dest_action_ref = dest_action_ref \n 
~~~ return % ( self . _source_action_ref , self . _rule_ref , self . _dest_action_ref ) \n 
~~ ~~ class LinksAnalyzer ( object ) : \n 
~~~ self . _rule_link_by_action_ref = { } \n 
self . _rules = { } \n 
~~ def analyze ( self , root_action_ref , link_tigger_ref ) : \n 
~~~ rules = Rule . query ( trigger = link_tigger_ref , enabled = True ) \n 
for rule in rules : \n 
~~~ source_action_ref = self . _get_source_action_ref ( rule ) \n 
if not source_action_ref : \n 
~~~ print % rule . ref \n 
~~ rule_links = self . _rules . get ( source_action_ref , None ) \n 
if rule_links is None : \n 
~~~ rule_links = [ ] \n 
self . _rules [ source_action_ref ] = rule_links \n 
~~ rule_links . append ( RuleLink ( source_action_ref = source_action_ref , rule_ref = rule . ref , \n 
dest_action_ref = rule . action . ref ) ) \n 
~~ analyzed = self . _do_analyze ( action_ref = root_action_ref ) \n 
for ( depth , rule_link ) in analyzed : \n 
~~~ print % ( * depth , rule_link ) \n 
~~ return analyzed \n 
~~ def _get_source_action_ref ( self , rule ) : \n 
~~~ criteria = rule . criteria \n 
source_action_ref = criteria . get ( , None ) \n 
~~~ source_action_ref = criteria . get ( , None ) \n 
~~ return source_action_ref [ ] if source_action_ref else None \n 
~~ def _do_analyze ( self , action_ref , rule_links = None , processed = None , depth = 0 ) : \n 
~~~ if processed is None : \n 
~~~ processed = sets . Set ( ) \n 
~~ if rule_links is None : \n 
~~ processed . add ( action_ref ) \n 
for rule_link in self . _rules . get ( action_ref , [ ] ) : \n 
~~~ rule_links . append ( ( depth , rule_link ) ) \n 
if rule_link . _dest_action_ref in processed : \n 
~~ self . _do_analyze ( rule_link . _dest_action_ref , rule_links = rule_links , \n 
processed = processed , depth = depth + 1 ) \n 
~~ return rule_links \n 
~~ ~~ class Grapher ( object ) : \n 
~~~ def generate_graph ( self , rule_links , out_file ) : \n 
~~~ graph_label = \n 
graph_attr = { \n 
: graph_label \n 
node_attr = { } \n 
dot = Digraph ( comment = , \n 
node_attr = node_attr , graph_attr = graph_attr , format = ) \n 
nodes = sets . Set ( ) \n 
for _ , rule_link in rule_links : \n 
~~~ print rule_link . _source_action_ref \n 
if rule_link . _source_action_ref not in nodes : \n 
~~~ nodes . add ( rule_link . _source_action_ref ) \n 
dot . node ( rule_link . _source_action_ref , rule_link . _source_action_ref ) \n 
~~ if rule_link . _dest_action_ref not in nodes : \n 
~~~ nodes . add ( rule_link . _dest_action_ref ) \n 
dot . node ( rule_link . _dest_action_ref , rule_link . _dest_action_ref ) \n 
~~ dot . edge ( rule_link . _source_action_ref , rule_link . _dest_action_ref , constraint = , \n 
label = rule_link . _rule_ref ) \n 
~~ output_path = os . path . join ( os . getcwd ( ) , out_file ) \n 
dot . format = \n 
dot . render ( output_path ) \n 
~~~ monkey_patch ( ) \n 
cli_opts = [ \n 
cfg . StrOpt ( , default = ) \n 
do_register_cli_opts ( cli_opts ) \n 
db_setup ( ) \n 
rule_links = LinksAnalyzer ( ) . analyze ( cfg . CONF . action_ref , cfg . CONF . link_trigger_ref ) \n 
Grapher ( ) . generate_graph ( rule_links , cfg . CONF . out_file ) \n 
~~ import boto \n 
class FieldLists ( ) : \n 
~~~ ADDRESS = [ \n 
BLOCK_DEVICE_TYPE = [ \n 
BUCKET = [ \n 
EC2ZONE = [ \n 
INSTANCE = [ \n 
RECORD = [ \n 
R53ZONE = [ \n 
R53STATUS = [ \n 
VOLUME = [ \n 
TAG = [ \n 
STACK = [ \n 
DBINSTANCE = [ \n 
~~ class ResultSets ( object ) : \n 
~~~ self . foo = \n 
~~ def selector ( self , output ) : \n 
~~~ if isinstance ( output , boto . ec2 . instance . Reservation ) : \n 
~~~ return self . parseReservation ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . instance . Instance ) : \n 
~~~ return self . parseInstance ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . volume . Volume ) : \n 
~~~ return self . parseVolume ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . blockdevicemapping . BlockDeviceType ) : \n 
~~~ return self . parseBlockDeviceType ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . zone . Zone ) : \n 
~~~ return self . parseEC2Zone ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . address . Address ) : \n 
~~~ return self . parseAddress ( output ) \n 
~~ elif isinstance ( output , boto . route53 . record . Record ) : \n 
~~~ return self . parseRecord ( output ) \n 
~~ elif isinstance ( output , boto . route53 . zone . Zone ) : \n 
~~~ return self . parseR53Zone ( output ) \n 
~~ elif isinstance ( output , boto . route53 . status . Status ) : \n 
~~~ return self . parseR53Status ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . tag . Tag ) : \n 
~~~ return self . parseTag ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . ec2object . EC2Object ) : \n 
~~~ return self . parseEC2Object ( output ) \n 
~~ elif isinstance ( output , boto . cloudformation . stack . Stack ) : \n 
~~~ return self . parseStackObject ( output ) \n 
~~ elif isinstance ( output , boto . rds . dbinstance . DBInstance ) : \n 
~~~ return self . parseDBInstanceObject ( output ) \n 
~~~ return output \n 
~~ ~~ def formatter ( self , output ) : \n 
~~~ if isinstance ( output , list ) : \n 
~~~ return [ self . formatter ( item ) for item in output ] \n 
~~ elif isinstance ( output , dict ) : \n 
~~~ return { key : self . formatter ( value ) for key , value in six . iteritems ( output ) } \n 
~~~ return self . selector ( output ) \n 
~~ ~~ def parseReservation ( self , output ) : \n 
~~~ instance_list = [ ] \n 
for instance in output . instances : \n 
~~~ instance_data = self . parseInstance ( instance ) \n 
instance_data [ ] = output . owner_id \n 
instance_list . append ( instance_data ) \n 
~~ return instance_list \n 
~~ def parseAddress ( self , output ) : \n 
~~~ instance_data = { field : getattr ( output , field ) for field in FieldLists . ADDRESS } \n 
return instance_data \n 
~~ def parseInstance ( self , output ) : \n 
~~~ instance_data = { field : getattr ( output , field ) for field in FieldLists . INSTANCE } \n 
~~ def parseVolume ( self , output ) : \n 
~~~ volume_data = { field : getattr ( output , field ) for field in FieldLists . VOLUME } \n 
return volume_data \n 
~~ def parseBlockDeviceType ( self , output ) : \n 
~~~ data = { field : getattr ( output , field ) for field in FieldLists . BLOCK_DEVICE_TYPE } \n 
~~ def parseEC2Zone ( self , output ) : \n 
~~~ zone_data = { field : getattr ( output , field ) for field in FieldLists . EC2ZONE } \n 
return zone_data \n 
~~ def parseRecord ( self , output ) : \n 
~~~ record_data = { field : getattr ( output , field ) for field in FieldLists . RECORD } \n 
return record_data \n 
~~ def parseR53Zone ( self , output ) : \n 
~~~ zone_data = { field : getattr ( output , field ) for field in FieldLists . R53ZONE } \n 
~~ def parseR53Status ( self , output ) : \n 
~~~ status_data = { field : getattr ( output , field ) for field in FieldLists . R53STATUS } \n 
return status_data \n 
~~ def parseBucket ( self , output ) : \n 
~~~ bucket_data = { field : getattr ( output , field ) for field in FieldLists . BUCKET } \n 
return bucket_data \n 
~~ def parseTag ( self , output ) : \n 
~~~ tag_data = { field : getattr ( output , field ) for field in FieldLists . TAG } \n 
return tag_data \n 
~~ def parseStackObject ( self , output ) : \n 
~~~ stack_data = { field : getattr ( output , field ) for field in FieldLists . STACK } \n 
return stack_data \n 
~~ def parseDBInstanceObject ( self , output ) : \n 
~~~ dbinstance_data = { field : getattr ( output , field ) for field in FieldLists . DBINSTANCE } \n 
return dbinstance_data \n 
~~ def parseEC2Object ( self , output ) : \n 
~~~ output = vars ( output ) \n 
del output [ ] \n 
region = output . get ( , None ) \n 
output [ ] = region . name if region else \n 
for k , v in six . iteritems ( output ) : \n 
~~~ if isinstance ( v , boto . ec2 . ec2object . EC2Object ) : \n 
~~~ output [ k ] = getattr ( v , , str ( v ) ) \n 
~~ if isinstance ( v , list ) : \n 
~~~ v_list = [ ] \n 
for item in v : \n 
~~~ if isinstance ( item , ( basestring , bool , int , long , float ) ) : \n 
~~~ v_list . append ( v ) \n 
~~~ v_list . append ( str ( item ) ) \n 
~~ ~~ output [ k ] = v_list \n 
~~ ~~ return output \n 
~~ ~~ from st2actions . runners . pythonrunner import Action \n 
from bitbucket . bitbucket import Bitbucket \n 
class BitBucketAction ( Action ) : \n 
~~~ def __init__ ( self , config ) : \n 
~~~ super ( BitBucketAction , self ) . __init__ ( config ) \n 
~~ def _get_client ( self , repo = None ) : \n 
~~~ if repo : \n 
~~~ bb = Bitbucket ( username = self . config [ ] , \n 
password = self . config [ ] , \n 
repo_name_or_slug = repo ) \n 
password = self . config [ ] ) \n 
~~ return bb \n 
from flask import Flask , request , abort \n 
from st2reactor . sensor . base import Sensor \n 
TRIGGER_REF = \n 
class CircleCIWebhookSensor ( Sensor ) : \n 
~~~ self . host = self . _config [ ] \n 
self . port = self . _config [ ] \n 
self . _endpoints = self . _config [ ] \n 
self . app = Flask ( __name__ ) \n 
self . trigger_ref = TRIGGER_REF \n 
self . log = self . _sensor_service . get_logger ( __name__ ) \n 
@ self . app . route ( ) \n 
def status ( ) : \n 
~~~ return json . dumps ( { : } ) \n 
~~ @ self . app . route ( , methods = [ ] ) \n 
def build_events ( endpoint ) : \n 
~~~ if endpoint not in self . _endpoints : \n 
~~~ self . log . error ( , endpoint ) \n 
abort ( 404 ) \n 
~~ webhook_body = request . get_json ( ) \n 
payload [ ] = self . _get_headers_as_dict ( request . headers ) \n 
payload [ ] = webhook_body \n 
response = self . _sensor_service . dispatch ( self . trigger_ref , payload ) \n 
self . log . debug ( json . dumps ( response ) ) \n 
return json . dumps ( { : } ) \n 
~~ ~~ def run ( self ) : \n 
~~~ self . app . run ( host = self . host , port = self . port , threaded = True ) \n 
~~ def cleanup ( self ) : \n 
~~ def _get_headers_as_dict ( self , headers ) : \n 
~~~ headers_dict = { } \n 
for key , value in headers : \n 
~~~ headers_dict [ key ] = value \n 
~~ return headers_dict \n 
~~ def add_trigger ( self , trigger ) : \n 
~~ def update_trigger ( self , trigger ) : \n 
~~ def remove_trigger ( self , trigger ) : \n 
~~ ~~ from libcloud . loadbalancer . base import Algorithm \n 
from lib . actions import BaseAction \n 
class CreateBalancerAction ( BaseAction ) : \n 
~~~ def run ( self , region , network_domain_id , name , port , protocol , \n 
algorithm = Algorithm . ROUND_ROBIN ) : \n 
~~~ driver = self . _get_lb_driver ( region ) \n 
_VALUE_TO_ALGORITHM_MAP = { \n 
: Algorithm . ROUND_ROBIN , \n 
: Algorithm . LEAST_CONNECTIONS , \n 
: Algorithm . SHORTEST_RESPONSE , \n 
: Algorithm . PERSISTENT_IP \n 
if algorithm is not Algorithm . ROUND_ROBIN : \n 
~~~ algorithm = _VALUE_TO_ALGORITHM_MAP [ algorithm ] \n 
~~ driver . network_domain_id = network_domain_id \n 
record = driver . create_balancer ( name = name , \n 
port = port , \n 
protocol = protocol , \n 
algorithm = algorithm , \n 
members = None ) \n 
return self . resultsets . formatter ( record ) \n 
~~ ~~ from lib . base import DockerBasePythonAction \n 
class DockerPullImageAction ( DockerBasePythonAction ) : \n 
~~~ def run ( self , repo , tag = None , insecure_registry = False , \n 
auth_username_override = None , auth_password_override = None ) : \n 
~~~ auth_override = ( auth_username_override and auth_password_override ) \n 
if auth_override : \n 
~~~ auth_config = { } \n 
auth_config [ ] = auth_username_override \n 
auth_config [ ] = auth_password_override \n 
return self . wrapper . pull ( repo = repo , tag = tag , insecure_registry = insecure_registry , \n 
auth_config = auth_config ) \n 
~~~ return self . wrapper . pull ( repo = repo , tag = tag , insecure_registry = insecure_registry ) \n 
~~ ~~ ~~ from lib . actions import BaseAction \n 
class ViewAXConfig ( BaseAction ) : \n 
~~~ def run ( self ) : \n 
~~~ response = self . _api_get ( ) \n 
~~ ~~ from lib import action \n 
class ColorTempKelvinAction ( action . BaseAction ) : \n 
~~~ def run ( self , light_id , temperature , transition_time ) : \n 
~~~ light = self . hue . lights . get ( light_id ) \n 
light . ct ( temperature , transition_time ) \n 
class BaseAction ( Action ) : \n 
~~~ super ( BaseAction , self ) . __init__ ( config ) \n 
~~ ~~ import libcloud . compute . base as compute_base \n 
import libcloud . dns . base as dns_base \n 
import libcloud . loadbalancer . base as lb_base \n 
import libcloud . container . base as container_base \n 
class FieldLists ( object ) : \n 
NODE = [ , , , , , , ] \n 
NODE_SIZE = [ , , , , , ] \n 
NODE_IMAGE = [ , ] \n 
LOCATION = [ , , ] \n 
NODE_KEY = [ ] \n 
NODE_PASSWORD = [ , ] \n 
STORAGE_VOLUME = [ , , , ] \n 
VOLUME_SNAPSHOT = [ , , ] \n 
ZONE = [ , , , ] \n 
RECORD = [ , , , , ] \n 
MEMBER = [ , , , ] \n 
BALANCER = [ , , , ] \n 
CONTAINER = [ , , ] \n 
CONTAINER_IMAGE = [ , , , ] \n 
CONTAINER_CLUSTER = [ , ] \n 
~~~ def selector ( self , output ) : \n 
~~~ if isinstance ( output , compute_base . Node ) : \n 
~~~ return self . parse ( output , FieldLists . NODE ) \n 
~~ elif isinstance ( output , compute_base . NodeSize ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_SIZE ) \n 
~~ elif isinstance ( output , compute_base . NodeImage ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_IMAGE ) \n 
~~ elif isinstance ( output , compute_base . NodeLocation ) : \n 
~~~ return self . parse ( output , FieldLists . LOCATION ) \n 
~~ elif isinstance ( output , compute_base . NodeAuthSSHKey ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_KEY ) \n 
~~ elif isinstance ( output , compute_base . NodeAuthPassword ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_PASSWORD ) \n 
~~ elif isinstance ( output , compute_base . StorageVolume ) : \n 
~~~ return self . parse ( output , FieldLists . STORAGE_VOLUME ) \n 
~~ elif isinstance ( output , compute_base . VolumeSnapshot ) : \n 
~~~ return self . parse ( output , FieldLists . VOLUME_SNAPSHOT ) \n 
~~ elif isinstance ( output , dns_base . Zone ) : \n 
~~~ return self . parse ( output , FieldLists . ZONE ) \n 
~~ elif isinstance ( output , dns_base . Record ) : \n 
~~~ return self . parse ( output , FieldLists . RECORD ) \n 
~~ elif isinstance ( output , lb_base . Member ) : \n 
~~~ return self . parse ( output , FieldLists . MEMBER ) \n 
~~ elif isinstance ( output , lb_base . LoadBalancer ) : \n 
~~~ return self . parse ( output , FieldLists . BALANCER ) \n 
~~ elif isinstance ( output , container_base . Container ) : \n 
~~~ return self . parse ( output , FieldLists . CONTAINER ) \n 
~~ elif isinstance ( output , container_base . ContainerImage ) : \n 
~~~ return self . parse ( output , FieldLists . CONTAINER_IMAGE ) \n 
~~ elif isinstance ( output , container_base . ContainerCluster ) : \n 
~~~ return self . parse ( output , FieldLists . CONTAINER_CLUSTER ) \n 
~~~ formatted = [ ] \n 
if isinstance ( output , list ) : \n 
~~~ for o in output : \n 
~~~ formatted . append ( self . selector ( o ) ) \n 
~~~ formatted = self . selector ( output ) \n 
~~ return formatted \n 
~~ def _getval ( self , obj , field ) : \n 
~~~ return self . selector ( getattr ( obj , field ) ) \n 
~~ def parse ( self , output , field_list ) : \n 
~~~ instance_data = { field : self . _getval ( output , field ) for field in field_list } \n 
~~ ~~ from lib . mmonit import MmonitBaseAction \n 
class MmonitActionHost ( MmonitBaseAction ) : \n 
~~~ def run ( self , host_id , action , service ) : \n 
~~~ self . login ( ) \n 
data = { "service" : service , "id" : host_id , "action" : action } \n 
self . session . post ( "{}/admin/hosts/action" . format ( self . url ) , data = data ) \n 
self . logout ( ) \n 
~~ ~~ from lib import actions \n 
class GetModeAction ( actions . BaseAction ) : \n 
~~~ def run ( self , structure = None , device = None ) : \n 
~~~ if structure and device : \n 
~~~ nest = self . _get_device ( structure , device ) \n 
~~~ nest = self . _get_default_device ( ) \n 
~~ return nest . mode \n 
~~ ~~ from lib . action import BaseAction \n 
class SendCommandAction ( BaseAction ) : \n 
~~~ def run ( self , item , command ) : \n 
~~~ self . _post ( item , command ) \n 
return { : } \n 
~~ ~~ import requests \n 
from lib . base import OpscenterAction \n 
class SetNodeConfAction ( OpscenterAction ) : \n 
~~~ def run ( self , node_ip , node_conf , cluster_id = None ) : \n 
~~~ if not cluster_id : \n 
~~~ cluster_id = self . cluster_id \n 
~~~ self . logger . error ( ) \n 
~~ url = self . _get_full_url ( [ cluster_id , , node_ip ] ) \n 
return requests . post ( url , data = node_conf ) . json ( ) \n 
~~ ~~ from lib . python_actions import PuppetBasePythonAction \n 
class PuppetCertCleanAction ( PuppetBasePythonAction ) : \n 
~~~ def run ( self , environment , host ) : \n 
~~~ success = self . client . cert_clean ( environment = environment , host = host ) \n 
return success \n 
~~ ~~ from lib . action import PyraxBaseAction \n 
class CreateLoadBalancerAction ( PyraxBaseAction ) : \n 
~~~ def run ( self , name , port , protocol ) : \n 
~~~ clb = self . pyrax . cloud_loadbalancers \n 
virtual_ipv4 = clb . VirtualIP ( type = "PUBLIC" , ipVersion = ) \n 
self . logger . info ( ) \n 
load_balancer = clb . create ( name , port = port , protocol = protocol , virtual_ips = [ virtual_ipv4 ] ) \n 
self . pyrax . utils . wait_until ( load_balancer , "status" , "ACTIVE" , interval = 1 , \n 
attempts = 30 ) \n 
self . logger . info ( % load_balancer ) \n 
: load_balancer . cluster , \n 
: load_balancer . algorithm , \n 
: load_balancer . id , \n 
: load_balancer . name , \n 
: load_balancer . port , \n 
: load_balancer . protocol , \n 
: load_balancer . sourceAddresses [ ] , \n 
: load_balancer . connectionLogging [ ] , \n 
: load_balancer . contentCaching [ ] , \n 
: load_balancer . httpsRedirect , \n 
: load_balancer . timeout , \n 
: load_balancer . status \n 
~~ ~~ from lib . actions import BaseAction \n 
class CreateMessage ( BaseAction ) : \n 
~~~ VISIBILITY = { \n 
def run ( self , slug , message , visibility = , \n 
suppress_notification = False ) : \n 
: message , \n 
: CreateMessage . VISIBILITY [ visibility ] \n 
if suppress_notification : \n 
~~~ payload [ ] = True \n 
~~ endpoint = . format ( slug ) \n 
response = self . _api_post ( endpoint , json = payload ) \n 
import servicenow_rest . api as sn \n 
self . client = self . _get_client ( ) \n 
~~ def _get_client ( self ) : \n 
~~~ instance_name = self . config [ ] \n 
username = self . config [ ] \n 
password = self . config [ ] \n 
return sn . Client ( instance_name , username , password ) \n 
from lib . action import St2BaseAction \n 
class St2KVPGetObjectAction ( St2BaseAction ) : \n 
~~~ def run ( self , key ) : \n 
~~~ _key = self . client . keys . get_by_name ( key ) \n 
if _key : \n 
~~~ deserialized_value = json . loads ( _key . value ) \n 
return deserialized_value \n 
~~ ~~ ~~ from lib . action import TravisCI \n 
class ListHooksAction ( TravisCI ) : \n 
path = \n 
response = self . _perform_request ( path , method = , requires_auth = True ) \n 
data = response . json ( ) \n 
hooks = { } \n 
for hook in data [ ] : \n 
~~~ hooks [ hook [ ] ] = [ hook [ ] , hook [ ] ] \n 
~~ return hooks \n 
class VaultIsInitializedAction ( action . VaultBaseAction ) : \n 
~~~ return self . vault . is_initialized ( ) \n 
from pyVmomi import vim \n 
from vmwarelib import inventory \n 
from vmwarelib import checkinputs \n 
from vmwarelib . actions import BaseAction \n 
class VMApplyPowerState ( BaseAction ) : \n 
~~~ def run ( self , vm_id , vm_name , power_onoff ) : \n 
vm = inventory . get_virtualmachine ( self . si_content , \n 
moid = vm_id , name = vm_name ) \n 
if not vm : \n 
~~ if power_onoff == "poweroff" : \n 
~~~ task = vm . PowerOffVM_Task ( ) \n 
~~ elif power_onoff == "poweron" : \n 
~~~ task = vm . PowerOnVM_Task ( ) \n 
~~ while task . info . state == vim . TaskInfo . State . running : \n 
~~~ eventlet . sleep ( 1 ) \n 
~~ return { : str ( task . info . state ) } \n 
from distutils . core import setup \n 
README = open ( os . path . join ( os . path . dirname ( __file__ ) , ) ) . read ( ) \n 
os . chdir ( os . path . normpath ( os . path . join ( os . path . abspath ( __file__ ) , os . pardir ) ) ) \n 
name = "SimpleIDML" , \n 
version = "0.92.4" , \n 
long_description = README , \n 
package_dir = { : } , \n 
packages = [ \n 
data_files = [ ] , \n 
scripts = [ \n 
import Queue \n 
import textwrap \n 
from thunderdome . exceptions import ThunderdomeException \n 
from thunderdome . spec import Spec \n 
class ThunderdomeConnectionError ( ThunderdomeException ) : \n 
~~ class ThunderdomeQueryError ( ThunderdomeException ) : \n 
def __init__ ( self , message , full_response = { } ) : \n 
super ( ThunderdomeQueryError , self ) . __init__ ( message ) \n 
self . _full_response = full_response \n 
def raw_response ( self ) : \n 
return self . _full_response \n 
~~ ~~ class ThunderdomeGraphMissingError ( ThunderdomeException ) : \n 
~~ Host = namedtuple ( , [ , ] ) \n 
_hosts = [ ] \n 
_host_idx = 0 \n 
_graph_name = None \n 
_username = None \n 
_password = None \n 
_index_all_fields = True \n 
_existing_indices = None \n 
_statsd = None \n 
def create_key_index ( name ) : \n 
global _existing_indices \n 
_existing_indices = _existing_indices or execute_query ( ) \n 
if name not in _existing_indices : \n 
~~~ execute_query ( \n 
{ : name } , transaction = False ) \n 
~~ ~~ def create_unique_index ( name , data_type ) : \n 
~~ ~~ def setup ( hosts , graph_name , username = None , password = None , index_all_fields = False , statsd = None ) : \n 
global _hosts \n 
global _graph_name \n 
global _username \n 
global _password \n 
global _index_all_fields \n 
global _statsd \n 
_graph_name = graph_name \n 
_username = username \n 
_password = password \n 
_index_all_fields = index_all_fields \n 
if statsd : \n 
~~~ sd = statsd \n 
import statsd \n 
tmp = sd . split ( ) \n 
if len ( tmp ) == 1 : \n 
~~~ tmp . append ( ) \n 
~~ _statsd = statsd . StatsClient ( tmp [ 0 ] , int ( tmp [ 1 ] ) , prefix = ) \n 
~~ ~~ for host in hosts : \n 
host = host . split ( ) \n 
if len ( host ) == 1 : \n 
~~~ _hosts . append ( Host ( host [ 0 ] , 8182 ) ) \n 
~~ elif len ( host ) == 2 : \n 
~~~ _hosts . append ( Host ( * host ) ) \n 
~~ ~~ if not _hosts : \n 
~~ random . shuffle ( _hosts ) \n 
create_unique_index ( , ) \n 
from thunderdome . models import vertex_types \n 
for klass in vertex_types . values ( ) : \n 
~~~ klass . _create_indices ( ) \n 
~~ ~~ def execute_query ( query , params = { } , transaction = True , context = "" ) : \n 
if transaction : \n 
~~~ query = "g.stopTransaction(FAILURE)\\n" + query \n 
~~ if len ( _hosts ) <= 0 : \n 
~~~ raise ThunderdomeConnectionError ( \n 
~~ host = _hosts [ 0 ] \n 
data = json . dumps ( { : query , : params } ) \n 
headers = { : , : , : import time \n 
~~~ start_time = time . time ( ) \n 
conn = httplib . HTTPConnection ( host . name , host . port ) \n 
conn . request ( "POST" , . format ( _graph_name ) , data , headers ) \n 
response = conn . getresponse ( ) \n 
content = response . read ( ) \n 
total_time = int ( ( time . time ( ) - start_time ) * 1000 ) \n 
if context and _statsd : \n 
~~~ _statsd . timing ( "{}.timer" . format ( context ) , total_time ) \n 
_statsd . incr ( "{}.counter" . format ( context ) ) \n 
~~ ~~ except socket . error as sock_err : \n 
~~~ if _statsd : \n 
~~~ total_time = int ( ( time . time ( ) - start_time ) * 1000 ) \n 
_statsd . incr ( "thunderdome.socket_error" . format ( context ) , total_time ) \n 
~~ raise ThunderdomeQueryError ( . format ( sock_err ) ) \n 
~~ logger . info ( json . dumps ( data ) ) \n 
logger . info ( content ) \n 
~~~ response_data = json . loads ( content ) \n 
~~ except ValueError as ve : \n 
~~ if response . status != 200 : \n 
~~~ if in response_data and len ( response_data [ ] ) > 0 : \n 
if re . search ( graph_missing_re , response_data [ ] ) : \n 
~~~ raise ThunderdomeGraphMissingError ( response_data [ ] ) \n 
~~~ raise ThunderdomeQueryError ( \n 
response_data [ ] , \n 
response_data \n 
~~~ _statsd . incr ( "{}.error" . format ( context ) ) \n 
~~ raise ThunderdomeQueryError ( \n 
~~ ~~ return response_data [ ] \n 
~~ def sync_spec ( filename , host , graph_name , dry_run = False ) : \n 
Spec ( filename ) . sync ( host , graph_name , dry_run = dry_run ) \n 
~~ from setuptools import setup , find_packages \n 
def read ( fname ) : \n 
~~~ return open ( os . path . join ( os . path . dirname ( __file__ ) , fname ) ) . read ( ) \n 
~~ README = read ( ) \n 
name = "django-pagination-plus" , \n 
version = "0.0.3" , \n 
author_email = "stefan@steeffie.net" , \n 
url = "https://github.com/SteefH/django-pagination-plus" , \n 
keywords = [ , ] , \n 
import base \n 
from brewery import dq \n 
from brewery . metadata import expand_record \n 
~~~ from pyes . es import ES \n 
~~~ from brewery . utils import MissingPackage \n 
~~ class ESDataSource ( base . DataSource ) : \n 
def __init__ ( self , document_type , database = None , host = None , port = None , \n 
expand = False , ** elasticsearch_args ) : \n 
self . document_type = document_type \n 
self . database_name = database \n 
self . elasticsearch_args = elasticsearch_args \n 
self . expand = expand \n 
self . connection = None \n 
self . _fields = None \n 
~~ def initialize ( self ) : \n 
args = self . elasticsearch_args . copy ( ) \n 
server = "" \n 
if self . host : \n 
~~~ server = self . host \n 
~~ if self . port : \n 
~~~ server += ":" + self . port \n 
~~ self . connection = ES ( server , ** args ) \n 
self . connection . default_indices = self . database_name \n 
self . connection . default_types = self . document_type \n 
~~ def read_fields ( self , limit = 0 ) : \n 
~~~ keys = [ ] \n 
probes = { } \n 
def probe_record ( record , parent = None ) : \n 
~~~ for key , value in record . items ( ) : \n 
~~~ if parent : \n 
~~~ full_key = parent + "." + key \n 
~~~ full_key = key \n 
~~ if self . expand and type ( value ) == dict : \n 
~~~ probe_record ( value , full_key ) \n 
~~ if not full_key in probes : \n 
~~~ probe = dq . FieldTypeProbe ( full_key ) \n 
probes [ full_key ] = probe \n 
keys . append ( full_key ) \n 
~~~ probe = probes [ full_key ] \n 
~~ probe . probe ( value ) \n 
~~ ~~ for record in self . document_type . find ( limit = limit ) : \n 
~~~ probe_record ( record ) \n 
~~ fields = [ ] \n 
for key in keys : \n 
~~~ probe = probes [ key ] \n 
field = base . Field ( probe . field ) \n 
storage_type = probe . unique_storage_type \n 
if not storage_type : \n 
~~~ field . storage_type = "unknown" \n 
~~ elif storage_type == "unicode" : \n 
~~~ field . storage_type = "string" \n 
field . concrete_storage_type = storage_type \n 
~~ fields . append ( field ) \n 
~~ self . fields = list ( fields ) \n 
return self . fields \n 
~~ def rows ( self ) : \n 
~~~ if not self . connection : \n 
~~ from pyes . query import MatchAllQuery \n 
fields = self . fields . names ( ) \n 
results = self . connection . search ( MatchAllQuery ( ) , search_type = "scan" , timeout = "5m" , size = "200" return ESRowIterator ( results , fields ) \n 
~~ def records ( self ) : \n 
results = self . connection . search ( MatchAllQuery ( ) , search_type = "scan" , timeout = "5m" , size = "200" return ESRecordIterator ( results , self . expand ) \n 
~~ ~~ class ESRowIterator ( object ) : \n 
def __init__ ( self , resultset , field_names ) : \n 
~~~ self . resultset = resultset \n 
self . field_names = field_names \n 
~~ def __getitem__ ( self , index ) : \n 
~~~ record = self . resultset . __getitem__ ( index ) \n 
array = [ ] \n 
for field in self . field_names : \n 
~~~ value = record \n 
for key in field . split ( ) : \n 
~~~ if key in value : \n 
~~~ value = value [ key ] \n 
~~ ~~ array . append ( value ) \n 
~~ return tuple ( array ) \n 
~~ ~~ class ESRecordIterator ( object ) : \n 
def __init__ ( self , resultset , expand = False ) : \n 
~~~ def expand_record ( record , parent = None ) : \n 
~~~ ret = { } \n 
for key , value in record . items ( ) : \n 
~~ if type ( value ) == dict : \n 
~~~ expanded = expand_record ( value , full_key ) \n 
ret . update ( expanded ) \n 
~~~ ret [ full_key ] = value \n 
~~ record = self . resultset . __getitem__ ( index ) \n 
if not self . expand : \n 
~~~ return record \n 
~~~ return expand_record ( record ) \n 
~~ ~~ ~~ class ESDataTarget ( base . DataTarget ) : \n 
def __init__ ( self , document_type , database = "test" , host = "127.0.0.1" , port = "9200" , \n 
truncate = False , expand = False , ** elasticsearch_args ) : \n 
self . truncate = truncate \n 
from pyes . es import ES \n 
from pyes . exceptions import IndexAlreadyExistsException \n 
~~ create = args . pop ( "create" , False ) \n 
replace = args . pop ( "replace" , False ) \n 
self . connection = ES ( server , ** args ) \n 
created = False \n 
if create : \n 
~~~ self . connection . create_index ( self . database_name ) \n 
self . connection . refresh ( self . database_name ) \n 
created = True \n 
~~ except IndexAlreadyExistsException : \n 
~~ ~~ if replace and not created : \n 
~~~ self . connection . delete_index_if_exists ( self . database_name ) \n 
time . sleep ( 2 ) \n 
self . connection . create_index ( self . database_name ) \n 
~~ if self . truncate : \n 
~~~ self . connection . delete_mapping ( self . database_name , self . document_type ) \n 
~~ ~~ def append ( self , obj ) : \n 
~~~ record = obj \n 
if not isinstance ( obj , dict ) : \n 
~~~ record = dict ( zip ( self . fields . names ( ) , obj ) ) \n 
~~ if self . expand : \n 
~~~ record = expand_record ( record ) \n 
~~ id = record . get ( ) or record . get ( ) \n 
self . connection . index ( record , self . database_name , self . document_type , id , bulk = True ) \n 
~~ def finalize ( self ) : \n 
~~~ self . connection . flush_bulk ( forced = True ) \n 
from brewery import ds \n 
import brewery . metadata \n 
from sqlalchemy import Table , Column , Integer , String , Text \n 
from sqlalchemy import create_engine , MetaData \n 
class SQLStreamsTestCase ( unittest . TestCase ) : \n 
~~~ self . engine = create_engine ( "sqlite://" ) \n 
self . metadata = MetaData ( ) \n 
self . fields = brewery . metadata . FieldList ( [ \n 
( "category" , "string" ) , \n 
( "category_label" , "string" ) , \n 
( "subcategory" , "string" ) , \n 
( "subcategory_label" , "string" ) , \n 
( "line_item" , "string" ) , \n 
( "year" , "integer" ) , \n 
( "amount" , "integer" ) ] ) \n 
self . example_row = [ "cat" , "Category" , "scat" , "Sub-category" , "foo" , 2012 , 100 ] \n 
~~ def test_table_fields ( self ) : \n 
~~~ table = Table ( , self . metadata , \n 
Column ( , Integer , primary_key = True ) , \n 
Column ( , String ( 32 ) ) , \n 
Column ( , String ( 255 ) ) , \n 
Column ( , Text ) \n 
self . metadata . create_all ( self . engine ) \n 
stream = ds . SQLDataSource ( connection = self . engine , table = str ( table ) ) \n 
fields = stream . fields \n 
self . assertEqual ( 4 , len ( fields ) ) \n 
~~ def test_target_no_existing_table ( self ) : \n 
~~~ stream = ds . SQLDataTarget ( connection = self . engine , table = "test" ) \n 
self . assertRaises ( Exception , stream . initialize ) \n 
~~ def test_target_create_table ( self ) : \n 
~~~ stream = ds . SQLDataTarget ( connection = self . engine , table = "test" , create = True ) \n 
stream . fields = self . fields \n 
stream . initialize ( ) \n 
cnames = [ str ( c ) for c in stream . table . columns ] \n 
fnames = [ "test." + f . name for f in self . fields ] \n 
self . assertEqual ( fnames , cnames ) \n 
stream . finalize ( ) \n 
~~ def test_target_replace_table ( self ) : \n 
stream = ds . SQLDataTarget ( connection = self . engine , table = "test" , \n 
create = True , replace = False ) \n 
create = True , replace = True ) \n 
~~ def test_target_concrete_type_map ( self ) : \n 
~~~ ctm = { "string" : String ( 123 ) } \n 
create = True , \n 
fields = self . fields , \n 
concrete_type_map = ctm ) \n 
c = stream . table . c [ "line_item" ] \n 
self . assertEqual ( 123 , c . type . length ) from . context import * \n 
~~ ~~ from . engine import * \n 
from . graph import * \n 
from . pipeline import * \n 
from bubbles import * \n 
class GraphTestCase ( unittest . TestCase ) : \n 
~~~ def test_basic ( self ) : \n 
~~~ g = Graph ( ) \n 
g . add ( Node ( "src" ) , "n1" ) \n 
g . add ( Node ( "distinct" ) , "n2" ) \n 
g . add ( Node ( "pretty_print" ) , "n3" ) \n 
self . assertEqual ( 3 , len ( g . nodes ) ) \n 
g . connect ( "n1" , "n2" ) \n 
sources = g . sources ( "n2" ) \n 
self . assertEqual ( 1 , len ( sources ) ) \n 
self . assertTrue ( isinstance ( sources [ "default" ] , Node ) ) \n 
self . assertEqual ( "src" , sources [ "default" ] . opname ) \n 
~~ def test_ports ( self ) : \n 
g . add ( Node ( "dim" ) , "dim" ) \n 
g . add ( Node ( "src" ) , "src" ) \n 
g . add ( Node ( "join_detail" ) , "j" ) \n 
g . connect ( "dim" , "j" , "master" ) \n 
with self . assertRaises ( GraphError ) : \n 
~~~ g . connect ( "src" , "j" , "master" ) \n 
~~ g . connect ( "src" , "j" , "detail" ) \n 
sources = g . sources ( "j" ) \n 
self . assertEqual ( 2 , len ( sources ) ) \n 
self . assertEqual ( [ "detail" , "master" ] , sorted ( sources . keys ( ) ) ) \n 
from upstream . shard import Shard \n 
from upstream . streamer import Streamer \n 
from upstream . exc import ConnectError , FileError , ShardError , ResponseError \n 
class TestStreamer ( unittest . TestCase ) : \n 
~~~ self . stream = Streamer ( "http://node1.metadisk.org" ) \n 
self . orig_hash = None \n 
self . uploadfile = "tests/1k.testfile" \n 
self . downloadfile = "download.testfile" \n 
self . shard = Shard ( \n 
"2032e4fd19d4ab49a74ead0984a5f672c26e60da6e992eaf51f05dc874e94bd7" , \n 
"1b1f463cef1807a127af668f3a4fdcc7977c647bf2f357d9fa125f13548b1d14" \n 
~~~ del self . stream \n 
del self . orig_hash \n 
del self . uploadfile \n 
~~~ os . remove ( self . downloadfile ) \n 
~~~ os . remove ( self . shard . filehash ) \n 
~~ del self . downloadfile \n 
del self . shard \n 
~~ def test_initialization ( self ) : \n 
~~~ self . assertEqual ( self . stream . server , "http://node1.metadisk.org" ) \n 
~~ def test_check_connectivity ( self ) : \n 
~~~ def _failing_connection ( ) : \n 
~~~ Streamer ( "http://does.not.exist" ) \n 
~~ self . assertRaises ( ConnectError , _failing_connection ) \n 
def test_upload_form_encoded ( self , post ) : \n 
def test_upload_sharded_encoded ( self , post ) : \n 
~~~ with self . assertRaises ( NotImplementedError ) : \n 
~~~ self . stream . _upload_sharded_encoded ( , ) \n 
~~ ~~ @ mock . patch ( ) \n 
def test_filestream ( self , post ) : \n 
~~~ self . stream . _filestream ( ) \n 
~~ ~~ def test_upload ( self ) : \n 
~~~ self . shard = self . stream . upload ( self . uploadfile ) \n 
self . shard . filehash , \n 
"2032e4fd19d4ab49a74ead0984a5f672" \n 
"c26e60da6e992eaf51f05dc874e94bd7" ) \n 
self . shard . decryptkey , \n 
"1b1f463cef1807a127af668f3a4fdcc7" \n 
"977c647bf2f357d9fa125f13548b1d14" ) \n 
def _failing_upload ( ) : \n 
~~~ self . stream . upload ( "not-a-real-file" ) \n 
~~ self . assertRaises ( FileError , _failing_upload ) \n 
~~ def test_upload_patched_404 ( self ) : \n 
~~~ self . stream . _upload_form_encoded = mock . MagicMock ( ) \n 
self . stream . _upload_form_encoded . return_value ( ) \n 
self . stream . _upload_form_encoded . return_value . status_code = 404 \n 
def _fourohfour ( ) : \n 
~~~ self . stream . upload ( self . uploadfile ) \n 
~~ with self . assertRaises ( ResponseError ) as ex : \n 
~~~ _fourohfour ( ) \n 
~~ ~~ def test_upload_patched_402 ( self ) : \n 
self . stream . _upload_form_encoded . return_value . status_code = 402 \n 
def _fourohtwo ( ) : \n 
~~ with self . assertRaises ( ResponseError ) : \n 
~~~ _fourohtwo ( ) \n 
~~ ~~ def test_upload_patched_500 ( self ) : \n 
self . stream . _upload_form_encoded . return_value . status_code = 500 \n 
def _fivehundred ( ) : \n 
~~~ _fivehundred ( ) \n 
~~ ~~ def test_upload_patched_501 ( self ) : \n 
self . stream . _upload_form_encoded . return_value . status_code = 501 \n 
def _fiveohone ( ) : \n 
~~~ _fiveohone ( ) \n 
self . assertEqual ( ex . message , \n 
~~ ~~ def test_upload_check_path ( self ) : \n 
~~~ homedir = os . path . expanduser ( self . uploadfile ) \n 
result = self . stream . check_path ( self . uploadfile ) \n 
self . assertEqual ( homedir , result ) \n 
with self . assertRaises ( FileError ) as ex : \n 
~~~ self . stream . check_path ( ) \n 
ex . message , ) \n 
~~ ~~ def test_download ( self ) : \n 
~~~ r = self . stream . download ( self . shard ) \n 
self . assertEquals ( r . status_code , 200 ) \n 
self . assertEqual ( len ( r . content ) , 1024 ) \n 
~~ def test_download_exception ( self ) : \n 
~~~ self . shard . filehash = self . shard . filehash [ : - 5 ] \n 
with self . assertRaises ( ResponseError ) as ex : \n 
~~~ self . stream . download ( self . shard ) \n 
~~ self . assertEqual ( ex . exception . response . status_code , 404 ) \n 
~~ def test_download_empty_shard ( self ) : \n 
~~~ shard = Shard ( ) \n 
with self . assertRaises ( ShardError ) as e : \n 
~~~ self . stream . download ( shard ) \n 
~~ ~~ import re \n 
from functools import partial \n 
from sublime_plugin import WindowCommand , TextCommand , EventListener \n 
from . util import find_view_by_settings , get_setting \n 
from . cmd import GitCmd \n 
from . helpers import GitDiffHelper , GitErrorHelper , GitStatusHelper \n 
RE_DIFF_HEAD = re . compile ( ) \n 
GIT_DIFF_TITLE = \n 
GIT_DIFF_TITLE_PREFIX = GIT_DIFF_TITLE + \n 
GIT_DIFF_CACHED_TITLE = \n 
GIT_DIFF_CACHED_TITLE_PREFIX = GIT_DIFF_CACHED_TITLE + \n 
GIT_DIFF_VIEW_SYNTAX = \n 
class GitDiffCommand ( WindowCommand , GitCmd ) : \n 
def run ( self , repo = None , path = None , cached = False ) : \n 
~~~ repo = repo or self . get_repo ( ) \n 
if not repo : \n 
~~ path = path or repo \n 
title = self . get_view_title ( path , cached ) \n 
git_view = % ( if cached else ) \n 
view = find_view_by_settings ( self . window , git_view = git_view , git_repo = repo , git_diff_path = path if not view : \n 
~~~ view = self . window . new_file ( ) \n 
view . set_name ( title ) \n 
view . set_syntax_file ( GIT_DIFF_VIEW_SYNTAX ) \n 
view . set_read_only ( True ) \n 
view . settings ( ) . set ( , git_view ) \n 
view . settings ( ) . set ( , repo ) \n 
view . settings ( ) . set ( , path ) \n 
view . settings ( ) . set ( , cached ) \n 
view . settings ( ) . set ( , 3 ) \n 
~~ self . window . focus_view ( view ) \n 
view . run_command ( , { : path , : cached , : True } ) \n 
~~ def get_view_title ( self , path = None , cached = False ) : \n 
~~~ if cached : \n 
~~~ return GIT_DIFF_CACHED_TITLE_PREFIX + path if path else GIT_DIFF_CACHED_TITLE \n 
~~~ return GIT_DIFF_TITLE_PREFIX + path if path else GIT_DIFF_TITLE \n 
~~ ~~ ~~ class GitDiffCachedCommand ( GitDiffCommand ) : \n 
def run ( self , path = None ) : \n 
~~~ super ( GitDiffCachedCommand , self ) . run ( path = path , cached = True ) \n 
~~ ~~ class GitDiffCurrentFileCommand ( GitCmd , GitStatusHelper , TextCommand ) : \n 
def run ( self , edit , cached = False ) : \n 
~~~ filename = self . view . file_name ( ) \n 
~~~ sublime . error_message ( ) \n 
~~ repo = self . get_repo ( ) \n 
~~ in_git = self . file_in_git ( repo , filename ) \n 
if not in_git : \n 
~~~ sublime . error_message ( % filename . replace ( repo , ) . return \n 
~~ self . view . window ( ) . run_command ( , { : repo , : filename , : cached \n 
~~ ~~ class GitDiffCachedCurrentFileCommand ( GitDiffCurrentFileCommand ) : \n 
def run ( self , edit ) : \n 
~~~ super ( GitDiffCachedCurrentFileCommand , self ) . run ( edit , cached = True ) \n 
~~ ~~ class GitDiffTextCmd ( GitCmd , GitDiffHelper ) : \n 
~~~ def move_to_point ( self , point ) : \n 
~~~ self . view . sel ( ) . clear ( ) \n 
self . view . sel ( ) . add ( sublime . Region ( point ) ) \n 
if not self . view . visible_region ( ) . contains ( point ) : \n 
~~~ view = self . view \n 
sublime . set_timeout ( partial ( view . show , point , True ) , 50 ) \n 
~~ ~~ def parse_diff ( self ) : \n 
~~~ sections = [ ] \n 
state = None \n 
prev_file = None \n 
current_file = { } \n 
current_hunks = [ ] \n 
prev_hunk = None \n 
current_hunk = None \n 
for line in self . view . lines ( sublime . Region ( 0 , self . view . size ( ) ) ) : \n 
~~~ linetext = self . view . substr ( line ) \n 
if linetext . startswith ( ) : \n 
~~~ state = \n 
if prev_file != line : \n 
~~~ if prev_file is not None : \n 
~~~ if current_hunk : \n 
~~~ current_hunks . append ( current_hunk ) \n 
~~ sections . append ( ( current_file , current_hunks ) ) \n 
~~ prev_file = line \n 
~~ current_file = line \n 
~~ elif state == and RE_DIFF_HEAD . match ( linetext ) : \n 
~~~ current_file = current_file . cover ( line ) \n 
~~ elif linetext . startswith ( ) : \n 
if prev_hunk != line : \n 
~~~ if prev_hunk is not None : \n 
~~ prev_hunk = line \n 
~~ current_hunk = line \n 
~~ elif state == and linetext [ 0 ] in ( , , ) : \n 
~~~ current_hunk = current_hunk . cover ( line ) \n 
~~ ~~ if current_file and current_hunk : \n 
sections . append ( ( current_file , current_hunks ) ) \n 
~~ return sections \n 
~~ def build_lookup ( self , parsed_diff ) : \n 
~~~ lookup = [ ] \n 
for header , hunks in parsed_diff : \n 
~~~ for h in hunks : \n 
~~~ lookup . append ( ( h , header ) ) \n 
~~ ~~ return lookup \n 
~~ def get_hunks_from_selection ( self , selection ) : \n 
~~~ if not selection : \n 
~~ diffspec = self . parse_diff ( ) \n 
lookup = self . build_lookup ( diffspec ) \n 
hunks = { } \n 
for s in selection : \n 
~~~ for hunk , header in lookup : \n 
~~~ if s . intersects ( hunk ) or hunk . contains ( s ) or ( s . begin ( ) == self . view . size ( ) and hunk ~~~ hunks . setdefault ( ( header . begin ( ) , header . end ( ) ) , [ ] ) . append ( hunk ) \n 
~~ ~~ ~~ return hunks \n 
~~ def create_patch ( self , selected_hunks ) : \n 
~~~ patch = [ ] \n 
for ( hstart , hend ) , hunks in selected_hunks . items ( ) : \n 
~~~ header = sublime . Region ( hstart , hend ) \n 
for head in self . view . lines ( header ) : \n 
~~~ headline = self . view . substr ( head ) \n 
if headline . startswith ( ) or headline . startswith ( ) : \n 
~~~ patch . append ( "%s\\n" % headline . strip ( ) ) \n 
~~~ patch . append ( "%s\\n" % headline ) \n 
~~ ~~ for h in hunks : \n 
~~~ patch . append ( self . view . substr ( self . view . full_line ( h ) ) ) \n 
~~ ~~ return "" . join ( patch ) \n 
~~ ~~ class GitDiffRefreshCommand ( TextCommand , GitDiffTextCmd ) : \n 
~~~ def is_visible ( self ) : \n 
~~ def run ( self , edit , path = None , cached = False , run_move = False ) : \n 
~~~ path = path if path else self . view . settings ( ) . get ( ) \n 
cached = cached if cached else self . view . settings ( ) . get ( ) \n 
unified = self . view . settings ( ) . get ( , 3 ) \n 
repo = self . view . settings ( ) . get ( ) \n 
if path is None or cached is None : \n 
~~ point = self . view . sel ( ) [ 0 ] . begin ( ) if self . view . sel ( ) else 0 \n 
row , col = self . view . rowcol ( point ) \n 
diff = self . get_diff ( repo , path , cached , unified = unified ) \n 
clean = False \n 
if not diff : \n 
~~~ diff = GIT_DIFF_CLEAN_CACHED if cached else GIT_DIFF_CLEAN \n 
clean = True \n 
~~ self . view . settings ( ) . set ( , clean ) \n 
self . view . set_read_only ( False ) \n 
self . view . replace ( edit , sublime . Region ( 0 , self . view . size ( ) ) , diff ) \n 
self . view . set_read_only ( True ) \n 
if run_move : \n 
~~~ self . view . run_command ( ) \n 
~~~ row_begin = self . view . text_point ( row , 0 ) \n 
line = self . view . line ( row_begin ) \n 
point = self . view . text_point ( row , min ( col , ( line . end ( ) - line . begin ( ) ) ) ) \n 
self . move_to_point ( point ) \n 
~~ ~~ ~~ class GitDiffEventListener ( EventListener ) : \n 
~~~ def on_activated ( self , view ) : \n 
~~~ if view . settings ( ) . get ( ) in ( , ) and get_setting ( ~~~ view . run_command ( ) \n 
~~ ~~ ~~ class GitDiffChangeHunkSizeCommand ( TextCommand ) : \n 
~~ def run ( self , edit , action = ) : \n 
~~~ unified = self . view . settings ( ) . get ( , 3 ) \n 
if action == : \n 
~~~ self . view . settings ( ) . set ( , unified + 1 ) \n 
~~~ self . view . settings ( ) . set ( , max ( 1 , unified - 1 ) ) \n 
~~ self . view . run_command ( ) \n 
~~ ~~ class GitDiffMoveCommand ( TextCommand , GitDiffTextCmd ) : \n 
~~ def run ( self , edit , item = , which = 0 , start = None ) : \n 
~~~ if self . view . settings ( ) . get ( ) is True : \n 
~~ if item not in ( , ) : \n 
~~~ which = int ( which ) \n 
~~~ if which not in ( , , , ) : \n 
~~ ~~ if start is not None : \n 
~~~ start = int ( start ) \n 
~~ elif self . view . sel ( ) : \n 
~~~ start = self . view . sel ( ) [ 0 ] . begin ( ) \n 
~~~ start = 0 \n 
~~ file_lookup = self . parse_diff ( ) \n 
hunk_lookup = self . build_lookup ( file_lookup ) \n 
if not hunk_lookup : \n 
~~ goto = None \n 
if which == : \n 
~~~ goto , _ = hunk_lookup [ 0 ] \n 
~~ elif which == : \n 
~~~ goto , _ = hunk_lookup [ - 1 ] \n 
~~~ if item == : \n 
~~~ next_hunks = [ ( h , f ) for h , f in hunk_lookup if h . begin ( ) > start ] \n 
goto , _ = next_hunks [ 0 ] if next_hunks else hunk_lookup [ - 1 ] \n 
~~~ next_files = [ ( f , h ) for f , h in file_lookup if f . begin ( ) > start ] \n 
goto , _ = next_files [ 0 ] if next_files else file_lookup [ - 1 ] \n 
~~ ~~ elif which == : \n 
~~~ prev_hunks = [ ( h , f ) for h , f in hunk_lookup if h . end ( ) < start ] \n 
goto , _ = prev_hunks [ - 1 ] if prev_hunks else hunk_lookup [ 0 ] \n 
~~~ prev_files = [ ( f , h ) for f , h in file_lookup if h [ - 1 ] . end ( ) < start ] \n 
goto , _ = prev_files [ - 1 ] if prev_files else file_lookup [ 0 ] \n 
~~~ goto , _ = hunk_lookup [ max ( 0 , which ) ] if which < len ( hunk_lookup ) else hunk_lookup [ - 1 ~~ else : \n 
~~~ goto , _ = file_lookup [ max ( 0 , which ) ] if which < len ( file_lookup ) else file_lookup [ - 1 \n 
~~ ~~ if goto : \n 
~~~ self . move_to_point ( goto . begin ( ) ) \n 
~~ ~~ ~~ class GitDiffStageUnstageHunkCommand ( GitDiffTextCmd , GitErrorHelper , TextCommand ) : \n 
~~ def run ( self , edit , reverse = False ) : \n 
~~~ repo = self . view . settings ( ) . get ( ) \n 
if self . view . settings ( ) . get ( ) is not reverse : \n 
~~~ if reverse : \n 
~~~ sublime . error_message ( GIT_DIFF_UNSTAGE_ERROR ) \n 
~~~ sublime . error_message ( GIT_DIFF_STAGE_ERROR ) \n 
~~ if self . view . settings ( ) . get ( ) is True : \n 
~~ hunks = self . get_hunks_from_selection ( self . view . sel ( ) ) \n 
if hunks : \n 
~~~ patch = self . create_patch ( hunks ) \n 
cmd = [ , , , if reverse else None , exit , stdout , stderr = self . git ( cmd , stdin = patch , cwd = repo ) \n 
if exit != 0 : \n 
~~~ sublime . error_message ( self . format_error_message ( stderr ) ) \n 
import sublime_plugin \n 
import subprocess \n 
from functools import reduce \n 
if int ( sublime . version ( ) ) < 3000 : \n 
~~~ import symbols \n 
from sublime_haskell_common import * \n 
~~~ import SublimeHaskell . symbols as symbols \n 
from SublimeHaskell . sublime_haskell_common import * \n 
~~ def concat_args ( args ) : \n 
~~~ def cat ( x , y ) : \n 
~~~ ( px , ex ) = x \n 
( py , ey ) = y \n 
return ( px or py , ( ex if px else [ ] ) + ( ey if py else [ ] ) ) \n 
~~ return reduce ( cat , args , ( True , [ ] ) ) [ 1 ] \n 
~~ def concat_opts ( opts ) : \n 
v = ( ex if px else { } ) . copy ( ) \n 
v . update ( ( ey if py else { } ) . copy ( ) ) \n 
return ( px or py , v ) \n 
~~ return reduce ( cat , opts , ( True , { } ) ) [ 1 ] \n 
~~ def flatten_opts ( opts ) : \n 
~~~ r = [ ] \n 
def to_opt ( x ) : \n 
~~~ return . format ( x ) \n 
~~ for k , v in opts . items ( ) : \n 
~~~ if v is None : \n 
~~~ r . append ( to_opt ( k ) ) \n 
~~ elif type ( v ) is list : \n 
~~~ for n in v : \n 
~~~ r . extend ( [ to_opt ( k ) , str ( n ) ] ) \n 
~~~ r . extend ( [ to_opt ( k ) , str ( v ) ] ) \n 
~~ ~~ return r \n 
~~ def hsdev_enabled ( ) : \n 
~~~ return get_setting_async ( ) == True \n 
~~ def hsdev_enable ( enable = True ) : \n 
~~~ set_setting_async ( , enable ) \n 
~~ def hsdev_version ( ) : \n 
~~~ ( exit_code , out , err ) = call_and_wait ( [ , ] ) \n 
if exit_code == 0 : \n 
~~~ m = re . match ( , out ) \n 
~~~ major = int ( m . group ( ) ) \n 
minor = int ( m . group ( ) ) \n 
revision = int ( m . group ( ) ) \n 
build = int ( m . group ( ) ) \n 
return [ major , minor , revision , build ] \n 
~~ ~~ ~~ except FileNotFoundError : \n 
~~ def show_version ( ver ) : \n 
~~~ return . join ( map ( lambda i : str ( i ) , ver ) ) \n 
~~ def check_version ( ver , minimal = [ 0 , 0 , 0 , 0 ] , maximal = None ) : \n 
~~~ if ver is None : \n 
~~ if ver < minimal : \n 
~~ if maximal and ver >= maximal : \n 
~~ def if_some ( x , lst ) : \n 
~~~ return lst if x is not None else [ ] \n 
~~ def cabal_path ( cabal ) : \n 
~~~ if not cabal : \n 
~~ return [ "--cabal" ] if cabal == else [ "--sandbox={0}" . format ( cabal ) ] \n 
~~ def hsinspect ( module = None , file = None , cabal = None , ghc_opts = [ ] ) : \n 
~~~ cmd = [ ] \n 
on_result = lambda s : s \n 
if module : \n 
~~~ cmd . extend ( [ module ] ) \n 
on_result = parse_module \n 
~~ elif file : \n 
~~~ cmd . extend ( [ file ] ) \n 
~~ elif cabal : \n 
~~~ cmd . extend ( [ cabal ] ) \n 
~~~ log ( , log_debug ) \n 
~~ for opt in ghc_opts : \n 
~~~ cmd . extend ( [ , opt ] ) \n 
~~ r = call_and_wait_tool ( cmd , , lambda s : json . loads ( s ) , file , None ) \n 
if r : \n 
~~~ if in r : \n 
~~~ log ( . format ( r [ ] ) , log_error ) \n 
~~~ return on_result ( r ) \n 
~~ def print_status ( s ) : \n 
~~~ print ( s [ ] ) \n 
~~ def parse_database ( s ) : \n 
~~ if s and in s and in s : \n 
~~~ return ( s [ ] , [ parse_module ( m ) for m in s [ ] ] ) \n 
~~ def parse_decls ( s ) : \n 
~~~ if s is None : \n 
~~ return [ parse_module_declaration ( decl ) for decl in s ] \n 
~~ def parse_modules_brief ( s ) : \n 
~~ return [ parse_module_id ( m ) for m in s ] \n 
~~ def get_value ( dc , ks , defval = None ) : \n 
~~~ if dc is None : \n 
~~ if type ( ks ) == list : \n 
~~~ cur = dc \n 
for k in ks : \n 
~~~ cur = cur . get ( k ) \n 
if cur is None : \n 
~~ ~~ return cur \n 
~~~ return dc . get ( ks , defval ) \n 
~~ ~~ def parse_package_db ( d , defval = None ) : \n 
~~~ if type ( d ) == dict : \n 
~~~ pdb = get_value ( d , ) \n 
return symbols . PackageDb ( package_db = pdb ) if pdb else defval \n 
~~ if d == : \n 
~~~ return symbols . PackageDb ( global_db = True ) \n 
~~~ return symbols . PackageDb ( user_db = True ) \n 
~~ return defval \n 
~~ def parse_position ( d ) : \n 
~~~ if not d : \n 
~~ line = get_value ( d , ) \n 
column = get_value ( d , ) \n 
if line is not None and column is not None : \n 
~~~ return symbols . Position ( line , column ) \n 
~~ def parse_location ( d ) : \n 
~~~ loc = symbols . Location ( \n 
get_value ( d , ) , \n 
get_value ( d , ) ) \n 
if not loc . is_null ( ) : \n 
~~~ return loc \n 
~~ loc = symbols . InstalledLocation ( \n 
symbols . parse_package ( get_value ( d , ) ) , \n 
parse_package_db ( get_value ( d , ) ) ) \n 
~~ loc = symbols . OtherLocation ( \n 
~~ def parse_import ( d ) : \n 
~~ return symbols . Import ( d [ ] , d [ ] , d . get ( ) , parse_position ( d . get ( ) ) ) \n 
~~ def parse_module_id ( d ) : \n 
~~~ if d is None : \n 
~~ return symbols . Module ( \n 
d [ ] , \n 
[ ] , [ ] , { } , \n 
parse_location ( d . get ( ) ) ) \n 
~~ def parse_declaration ( decl ) : \n 
~~~ what = decl [ ] [ ] \n 
docs = crlf2lf ( decl . get ( ) ) \n 
name = decl [ ] \n 
pos = parse_position ( decl . get ( ) ) \n 
imported = [ ] \n 
if in decl and decl [ ] : \n 
~~~ imported = [ parse_import ( d ) for d in decl [ ] ] \n 
~~ defined = None \n 
~~~ defined = parse_module_id ( decl [ ] ) \n 
~~ if what == : \n 
~~~ return symbols . Function ( name , decl [ ] . get ( ) , docs , imported , defined , pos ) \n 
~~ elif what == : \n 
~~~ return symbols . Type ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ elif what == : \n 
~~~ return symbols . Newtype ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ elif what == : \n 
~~~ return symbols . Data ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ elif what == : \n 
~~~ return symbols . Class ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ else : \n 
~~~ log ( . format ( e ) , log_error ) \n 
~~ ~~ def parse_declarations ( decls ) : \n 
~~~ if decls is None : \n 
~~ return [ parse_declaration ( d ) for d in decls ] \n 
~~ def parse_module_declaration ( d , parse_module_info = True ) : \n 
~~~ m = None \n 
if in d and parse_module_info : \n 
~~~ m = parse_module_id ( d [ ] ) \n 
~~ loc = parse_location ( d [ ] . get ( ) ) \n 
decl = parse_declaration ( d [ ] ) \n 
if not decl : \n 
~~ decl . module = m \n 
return decl \n 
~~ ~~ def parse_module ( d ) : \n 
d . get ( ) , \n 
[ parse_import ( i ) for i in d [ ] ] if in d else [ ] , \n 
dict ( ( decl [ ] , parse_declaration ( decl ) ) for decl in d [ ] ) if parse_location ( d . get ( ) ) ) \n 
~~ def parse_modules ( ds ) : \n 
~~~ if ds is None : \n 
~~ return [ parse_module ( d ) for d in ds ] \n 
~~ def parse_cabal_package ( d ) : \n 
~~ return symbols . CabalPackage ( \n 
d . get ( ) ) \n 
~~ def parse_corrections ( d ) : \n 
~~ return [ parse_correction ( c ) for c in d ] \n 
~~ def parse_correction ( d ) : \n 
~~~ return symbols . Correction ( \n 
d [ ] [ ] , \n 
parse_corrector ( d [ ] [ ] ) ) \n 
~~ def parse_corrector ( d ) : \n 
~~~ return symbols . Corrector ( \n 
parse_position ( d [ ] [ ] ) , \n 
d [ ] ) \n 
~~ def encode_corrections ( cs ) : \n 
~~~ return [ encode_correction ( c ) for c in cs ] \n 
~~ def encode_correction ( c ) : \n 
: c . file } , \n 
: c . level , \n 
: encode_corrector ( c . corrector ) , \n 
: c . message } , \n 
: encode_position ( c . corrector . start . from_zero_based ( ) ) , \n 
: encode_position ( c . corrector . end . from_zero_based ( ) ) } \n 
~~ def encode_corrector ( c ) : \n 
: encode_position ( c . start ) , \n 
: encode_position ( c . end ) } , \n 
: c . contents } \n 
~~ def encode_position ( p ) : \n 
: p . line , \n 
: p . column } \n 
~~ def encode_package_db ( db ) : \n 
~~~ if db . user_db : \n 
~~ if db . global_db : \n 
~~ if db . package_db : \n 
~~~ return { : db . package_db } \n 
~~ def reconnect_function ( fn ) : \n 
~~~ def wrapped ( self , * args , ** kwargs ) : \n 
~~~ autoconnect_ = kwargs . pop ( , False ) \n 
on_reconnect_ = kwargs . pop ( , None ) \n 
just_connect_ = kwargs . pop ( , False ) \n 
def run_fn ( ) : \n 
~~~ if not just_connect_ : \n 
~~~ self . autoconnect = autoconnect_ \n 
self . on_reconnect = on_reconnect_ \n 
~~ return fn ( self , * args , ** kwargs ) \n 
~~ if not just_connect_ : \n 
~~~ self . set_reconnect_function ( run_fn ) \n 
~~ return run_fn ( ) \n 
~~ return wrapped \n 
~~ class begin_connecting ( object ) : \n 
~~~ def __init__ ( self , agent ) : \n 
~~~ self . agent = agent \n 
~~~ self . agent . set_connecting ( ) \n 
~~ def __exit__ ( self , type , value , traceback ) : \n 
~~~ if type : \n 
~~~ self . agent . set_unconnected ( ) \n 
~~~ if self . agent . is_connecting ( ) : \n 
~~ ~~ ~~ ~~ def connect_function ( fn ) : \n 
~~~ if self . is_unconnected ( ) : \n 
~~~ with begin_connecting ( self ) : \n 
~~~ return fn ( self , * args , ** kwargs ) \n 
~~~ log ( , log_warning ) \n 
~~ ~~ return wrapped \n 
~~ def hsdev_command ( async = False , timeout = None , is_list = False ) : \n 
~~~ def wrap_function ( fn ) : \n 
~~~ wait_flag = kwargs . pop ( , not async ) \n 
timeout_arg = kwargs . pop ( , timeout ) \n 
on_resp = kwargs . pop ( , None ) \n 
on_not = kwargs . pop ( , None ) \n 
on_err = kwargs . pop ( , None ) \n 
on_res_part = kwargs . pop ( , None ) \n 
split_res = kwargs . pop ( , on_res_part is not None ) \n 
( name_ , opts_ , on_result_ ) = fn ( self , * args , ** kwargs ) \n 
if is_list and split_res : \n 
~~~ result = [ ] \n 
def on_notify ( n ) : \n 
~~~ if in n : \n 
~~~ rp = on_result_ ( [ n [ ] ] ) [ 0 ] \n 
call_callback ( on_res_part , rp ) \n 
result . append ( rp ) \n 
~~~ call_callback ( on_not , n ) \n 
~~ ~~ def on_response ( r ) : \n 
~~~ on_resp ( result ) \n 
r = self . call ( \n 
name_ , \n 
opts_ , \n 
on_response = on_response if on_resp else None , \n 
on_notify = on_notify , \n 
on_error = on_err , \n 
wait = wait_flag , \n 
timeout = timeout_arg ) \n 
if wait_flag : \n 
~~ return r \n 
~~~ def on_response ( r ) : \n 
~~~ on_resp ( on_result_ ( r ) ) \n 
~~ r = self . call ( \n 
on_notify = on_not , \n 
~~~ return on_result_ ( r ) \n 
~~ return wrap_function \n 
~~ def command ( fn ) : \n 
~~~ return hsdev_command ( async = False , timeout = 1 ) ( fn ) \n 
~~ def async_command ( fn ) : \n 
~~~ return hsdev_command ( async = True ) ( fn ) \n 
~~ def list_command ( fn ) : \n 
~~~ return hsdev_command ( async = False , timeout = 1 , is_list = True ) ( fn ) \n 
~~ def async_list_command ( fn ) : \n 
~~~ return hsdev_command ( async = True , is_list = True ) ( fn ) \n 
~~ def cmd ( name_ , opts_ = { } , on_result = lambda r : r ) : \n 
~~~ return ( name_ , opts_ , on_result ) \n 
~~ def call_callback ( fn , * args , ** kwargs ) : \n 
~~~ name = kwargs . get ( ) \n 
if name : \n 
~~~ del kwargs [ ] \n 
~~~ if fn is not None : \n 
~~~ fn ( * args , ** kwargs ) \n 
~~ ~~ def format_error_details ( ds ) : \n 
~~~ return . join ( [ . format ( k , v ) for k , v in ds . items ( ) ] ) \n 
~~ class HsDevCallbacks ( object ) : \n 
~~~ def __init__ ( self , id , command , on_response = None , on_notify = None , on_error = None ) : \n 
~~~ self . id = id \n 
self . command = command \n 
self . start_time = time . clock ( ) \n 
self . on_response = on_response \n 
self . on_notify = on_notify \n 
self . on_error = on_error \n 
~~~ return time . clock ( ) - self . start_time if self . start_time is not None else None \n 
~~ def log_time ( self ) : \n 
~~~ log ( . format ( self . command , self . time ( ) ) , log_trace ) \n 
~~ def call_response ( self , r ) : \n 
~~~ self . log_time ( ) \n 
call_callback ( self . on_response , r ) \n 
~~ def call_notify ( self , n ) : \n 
~~~ call_callback ( self . on_notify , n ) \n 
~~ def call_error ( self , e , ds ) : \n 
log ( . format ( self . command , e , format_error_details ( ds ) ) , log_error call_callback ( self . on_error , e , ds ) \n 
~~ ~~ class HsDev ( object ) : \n 
~~~ def __init__ ( self , port = 4567 ) : \n 
~~~ self . port = port \n 
self . connecting = threading . Event ( ) \n 
self . connected = threading . Event ( ) \n 
self . socket = None \n 
self . listener = None \n 
self . hsdev_address = None \n 
self . autoconnect = True \n 
self . map = LockedObject ( { } ) \n 
self . id = 1 \n 
self . connect_fun = None \n 
self . part = \n 
self . on_connected = None \n 
self . on_disconnected = None \n 
self . on_reconnect = None \n 
~~~ self . close ( ) \n 
~~ def set_reconnect_function ( self , f ) : \n 
~~~ if self . connect_fun is None : \n 
~~~ self . connect_fun = f \n 
~~ ~~ def reconnect ( self ) : \n 
~~~ if self . connect_fun is not None : \n 
~~~ log ( , log_info ) \n 
call_callback ( self . on_reconnect , name = ) \n 
self . connect_fun ( ) \n 
def run_server ( port = 4567 , cache = None , log_file = None , log_config = None ) : \n 
~~~ cmd = concat_args ( [ \n 
( True , [ "hsdev" , "run" ] ) , \n 
( port , [ "--port" , str ( port ) ] ) , \n 
( cache , [ "--cache" , cache ] ) , \n 
( log_file , [ "--log" , log_file ] ) , \n 
( log_config , [ "--log-config" , log_config ] ) ] ) \n 
log ( , log_info ) \n 
p = call_and_wait ( cmd , wait = False ) \n 
if not p : \n 
~~~ log ( , log_error ) \n 
~~~ output = crlf2lf ( decode_bytes ( p . stdout . readline ( ) ) ) \n 
m = re . match ( , output ) \n 
~~~ log ( . format ( m . group ( ) ) ) \n 
p . stdout . close ( ) \n 
p . stderr . close ( ) \n 
~~ ~~ ~~ @ staticmethod \n 
def start_server ( port = 4567 , cache = None , log_file = None , log_config = None ) : \n 
( True , [ "hsdev" , "start" ] ) , \n 
def parse_response ( s ) : \n 
~~~ return { } if s . isspace ( ) else json . loads ( s ) \n 
~~~ return { : , : s } \n 
~~ ~~ log ( , log_info ) \n 
ret = call_and_wait_tool ( cmd , , , None , None , None , check_enabled = False ) \n 
if ret is not None : \n 
~~~ return ret \n 
def client ( port = 4567 , cache = None , autoconnect = False ) : \n 
~~~ start_server ( port = port , cache = cache ) \n 
h = HsDev ( port = port ) \n 
h . connect ( autoconnect = autoconnect ) \n 
return h \n 
def client_async ( port = 4567 , cache = None , autoconnect = False ) : \n 
h . connect_async ( autoconnect = autoconnect ) \n 
~~ @ connect_function \n 
@ reconnect_function \n 
def connect ( self , tries = 10 , delay = 1.0 ) : \n 
~~~ self . socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
for n in range ( 0 , tries ) : \n 
~~~ log ( . format ( n ) , log_info ) \n 
self . socket . connect ( ( , self . port ) ) \n 
self . hsdev_socket = self . socket \n 
self . hsdev_address = \n 
self . set_connected ( ) \n 
self . listener = threading . Thread ( target = self . listen ) \n 
self . listener . start ( ) \n 
call_callback ( self . on_connected , name = ) \n 
~~~ log ( . format ( n ) , log_warning ) \n 
time . sleep ( delay ) \n 
~~ @ reconnect_function \n 
def connect_async ( self , tries = 10 , delay = 1.0 ) : \n 
~~~ thread = threading . Thread ( \n 
target = self . connect , \n 
kwargs = { : tries , : delay , : True } ) \n 
thread . start ( ) \n 
~~ def wait ( self , timeout = None ) : \n 
~~~ return self . connected . wait ( timeout ) \n 
~~ self . connected . clear ( ) \n 
if self . hsdev_socket : \n 
~~~ self . hsdev_socket . close ( ) \n 
self . hsdev_socket = None \n 
~~ self . socket . close ( ) \n 
~~ def is_connecting ( self ) : \n 
~~~ return self . connecting . is_set ( ) \n 
~~~ return self . connected . is_set ( ) \n 
~~ def is_unconnected ( self ) : \n 
~~~ return ( not self . is_connecting ( ) ) and ( not self . is_connected ( ) ) \n 
~~ def set_unconnected ( self ) : \n 
~~~ if self . connecting . is_set ( ) : \n 
~~~ self . connecting . clear ( ) \n 
~~ if self . connected . is_set ( ) : \n 
~~~ self . connected . clear ( ) \n 
~~ ~~ def set_connecting ( self ) : \n 
~~~ self . set_unconnected ( ) \n 
self . connecting . set ( ) \n 
~~ def set_connected ( self ) : \n 
~~~ if self . is_connecting ( ) : \n 
~~~ self . connected . set ( ) \n 
self . connecting . clear ( ) \n 
~~ ~~ def on_receive ( self , id , command , on_response = None , on_notify = None , on_error = None ) : \n 
~~~ with self . map as m : \n 
~~~ m [ id ] = HsDevCallbacks ( id , command , on_response , on_notify , on_error ) \n 
~~ ~~ def verify_connected ( self ) : \n 
~~~ self . connection_lost ( , ) \n 
return self . is_connected ( ) \n 
~~ ~~ def connection_lost ( self , fn , e ) : \n 
~~ self . close ( ) \n 
log ( . format ( fn , e ) , log_error ) \n 
call_callback ( self . on_disconnected , name = ) \n 
with self . map as m : \n 
~~~ for on_msg in m . values ( ) : \n 
~~~ on_msg . on_error ( ) \n 
~~ m . clear ( ) \n 
~~ self . id = 1 \n 
if self . autoconnect : \n 
~~~ self . reconnect ( ) \n 
~~~ args_cmd = . format ( command ) \n 
call_cmd = . format ( command , opts ) \n 
if not self . verify_connected ( ) : \n 
~~~ return None if wait else False \n 
~~~ wait_receive = threading . Event ( ) if wait else None \n 
x = { } \n 
def on_response_ ( r ) : \n 
~~~ x [ ] = r \n 
call_callback ( on_response , r ) \n 
if wait_receive : \n 
~~~ wait_receive . set ( ) \n 
~~ ~~ def on_error_ ( e , ds ) : \n 
~~~ call_callback ( on_error , e , ds ) \n 
~~ ~~ if wait or on_response or on_notify or on_error : \n 
~~~ if id is None : \n 
~~~ id = str ( self . id ) \n 
self . id = self . id + 1 \n 
~~ self . on_receive ( id , args_cmd , on_response_ , on_notify , on_error_ ) \n 
~~ opts . update ( { : True } ) \n 
opts . update ( { : id , : command } ) \n 
msg = json . dumps ( opts , separators = ( , ) ) \n 
self . hsdev_socket . sendall ( msg . encode ( ) ) \n 
self . hsdev_socket . sendall ( . encode ( ) ) \n 
log ( call_cmd , log_trace ) \n 
if wait : \n 
~~~ wait_receive . wait ( timeout ) \n 
return x . get ( ) \n 
~~~ log ( . format ( call_cmd , e ) , log_error ) \n 
self . connection_lost ( , e ) \n 
~~ ~~ def listen ( self ) : \n 
~~~ while self . verify_connected ( ) : \n 
~~~ resp = json . loads ( self . get_response ( ) ) \n 
if in resp : \n 
~~~ callbacks = None \n 
~~~ if resp [ ] in m : \n 
~~~ callbacks = m [ resp [ ] ] \n 
~~ ~~ if callbacks : \n 
~~~ if in resp : \n 
~~~ callbacks . call_notify ( resp [ ] ) \n 
~~ if in resp : \n 
~~~ err = resp . pop ( "error" ) \n 
callbacks . call_error ( err , resp ) \n 
~~~ m . pop ( resp [ ] ) \n 
~~ ~~ if in resp : \n 
~~~ callbacks . call_response ( resp [ ] ) \n 
~~ ~~ ~~ ~~ ~~ except Exception as e : \n 
~~~ self . connection_lost ( , e ) \n 
~~ ~~ ~~ def get_response ( self ) : \n 
~~~ while not in self . part : \n 
~~~ self . part = self . part + self . socket . recv ( 65536 ) . decode ( ) \n 
~~ ( r , _ , post ) = self . part . partition ( ) \n 
self . part = post \n 
return r \n 
~~ @ command \n 
def link ( self , hold = False , ** kwargs ) : \n 
~~~ return cmd ( , { \n 
: hold } ) \n 
def ping ( self ) : \n 
~~~ return cmd ( , { } , lambda r : r and ( in r ) and ( r [ ] == ) ) \n 
~~ @ async_command \n 
def scan ( self , cabal = False , sandboxes = [ ] , projects = [ ] , files = [ ] , paths = [ ] , ghc = [ ] , contents ~~~ return cmd ( , { \n 
: projects , \n 
: cabal , \n 
: sandboxes , \n 
: files , \n 
: paths , \n 
: [ { : f , : cts } for f , cts in contents . items ( ) ] , \n 
: ghc , \n 
: docs , \n 
: infer } ) \n 
def docs ( self , projects = [ ] , files = [ ] , modules = [ ] ) : \n 
: modules } ) \n 
def infer ( self , projects = [ ] , files = [ ] , modules = [ ] ) : \n 
~~ @ async_list_command \n 
def remove ( self , cabal = False , sandboxes = [ ] , projects = [ ] , files = [ ] , packages = [ ] ) : \n 
: packages } ) \n 
def remove_all ( self ) : \n 
~~~ return cmd ( , { } ) \n 
~~ @ list_command \n 
def list_modules ( self , project = None , file = None , module = None , deps = None , sandbox = None , ~~~ fs = [ ] \n 
if project : \n 
~~~ fs . append ( { : project } ) \n 
~~ if file : \n 
~~~ fs . append ( { : file } ) \n 
~~ if module : \n 
~~~ fs . append ( { : module } ) \n 
~~ if deps : \n 
~~~ fs . append ( { : deps } ) \n 
~~ if sandbox : \n 
~~~ fs . append ( { : { : sandbox } } ) \n 
~~ if cabal : \n 
~~~ fs . append ( { : } ) \n 
~~ if db : \n 
~~~ fs . append ( { : encode_package_db ( db ) } ) \n 
~~ if package : \n 
~~~ fs . append ( { : package } ) \n 
~~ if source : \n 
~~~ fs . append ( ) \n 
~~ if standalone : \n 
~~ return cmd ( , { : fs } , parse_modules_brief ) \n 
def list_packages ( self ) : \n 
def list_projects ( self ) : \n 
~~~ q = { : input , : search_type } \n 
fs = [ ] \n 
~~ return cmd ( , { : q , : fs , : locals } , parse_decls ) \n 
def module ( self , input = "" , search_type = , project = None , file = None , module = None , ~~~ q = { : input , : search_type } \n 
~~ return cmd ( , { : q , : fs } , parse_modules ) \n 
def resolve ( self , file , exports = False ) : \n 
~~~ return cmd ( , { : file , : exports } , parse_module ) \n 
def project ( self , project = None , path = None ) : \n 
~~~ return cmd ( , { : project } if project else { : path } ) \n 
def sandbox ( self , path ) : \n 
~~~ return cmd ( , { : path } ) \n 
def lookup ( self , name , file ) : \n 
~~~ return cmd ( , { : name , : file } , parse_decls ) \n 
def whois ( self , name , file ) : \n 
~~~ return cmd ( , { : name , : file } , parse_declarations ) \n 
def scope_modules ( self , file , input = , search_type = ) : \n 
~~~ return cmd ( , { : { : input , : search_type } , : file } , \n 
def scope ( self , file , input = , search_type = , global_scope = False ) : \n 
~~~ return cmd ( , { : { : input , : search_type } , : global_scope , \n 
def complete ( self , input , file , wide = False ) : \n 
~~~ return cmd ( , { : input , : wide , : file } , parse_declarations ) \n 
def hayoo ( self , query , page = None , pages = None ) : \n 
~~~ return cmd ( , { : query , : page or 0 , : pages or 1 } , parse_decls ) \n 
def cabal_list ( self , packages ) : \n 
~~~ cmd ( , { : packages } , lambda r : [ parse_cabal_package ( s ) for s in r ] if r \n 
def lint ( self , files = [ ] , contents = { } , hlint = [ ] ) : \n 
: hlint } ) \n 
def check ( self , files = [ ] , contents = { } , ghc = [ ] ) : \n 
: ghc } ) \n 
def check_lint ( self , files = [ ] , contents = { } , ghc = [ ] , hlint = [ ] ) : \n 
def types ( self , files = [ ] , contents = { } , ghc = [ ] ) : \n 
def ghcmod_lang ( self ) : \n 
~~~ return cmd ( ) \n 
def ghcmod_flags ( self ) : \n 
def ghcmod_type ( self , file , line , column = 1 , ghc = [ ] ) : \n 
: { : int ( line ) , : int ( column ) } , \n 
: file , \n 
def ghcmod_check ( self , files , ghc = [ ] ) : \n 
~~~ return cmd ( , { : files , : ghc } ) \n 
def ghcmod_lint ( self , files , hlint = [ ] ) : \n 
~~~ return cmd ( , { : files , : hlint } ) \n 
def ghcmod_check_lint ( self , files , ghc = [ ] , hlint = [ ] ) : \n 
~~~ return cmd ( , { : files , : ghc , : hlint } ) \n 
def autofix_show ( self , messages ) : \n 
~~~ return cmd ( , { : messages } , parse_corrections ) \n 
def autofix_fix ( self , messages , rest = [ ] , pure = False ) : \n 
~~~ return cmd ( , { : messages , : rest , : pure } , parse_corrections \n 
def ghc_eval ( self , exprs ) : \n 
~~~ return cmd ( , { : exprs } ) \n 
def exit ( self ) : \n 
~~ ~~ def wait_result ( fn , * args , ** kwargs ) : \n 
~~~ wait_receive = threading . Event ( ) \n 
x = { : None } \n 
on_resp = kwargs . get ( ) \n 
on_err = kwargs . get ( ) \n 
def wait_response ( r ) : \n 
if on_resp : \n 
~~~ on_resp ( r ) \n 
~~ wait_receive . set ( ) \n 
~~ def wait_error ( e , ds ) : \n 
~~~ log ( . format ( e , format_error_details ( ds ) ) ) \n 
if on_err : \n 
~~~ on_err ( e , ds ) \n 
~~ tm = kwargs . pop ( , 0.1 ) \n 
kwargs [ ] = wait_response \n 
kwargs [ ] = wait_error \n 
fn ( * args , ** kwargs ) \n 
wait_receive . wait ( tm ) \n 
return x [ ] \n 
~~ class HsDevProcess ( threading . Thread ) : \n 
~~~ def __init__ ( self , port = 4567 , cache = None , log_file = None , log_config = None ) : \n 
~~~ super ( HsDevProcess , self ) . __init__ ( ) \n 
self . process = None \n 
self . on_start = None \n 
self . on_exit = None \n 
self . stop_event = threading . Event ( ) \n 
self . create_event = threading . Event ( ) \n 
self . cache = cache \n 
self . log_file = log_file \n 
self . log_config = log_config \n 
~~~ self . create_event . wait ( ) \n 
self . create_event . clear ( ) \n 
while not self . stop_event . is_set ( ) : \n 
~~~ self . process = HsDev . run_server ( port = self . port , cache = self . cache , log_file = self if not self . process : \n 
self . stop_event . set ( ) \n 
~~~ call_callback ( self . on_start , name = ) \n 
~~ self . process . wait ( ) \n 
call_callback ( self . on_exit , name = ) \n 
~~ self . stop_event . clear ( ) \n 
~~ ~~ def active ( self ) : \n 
~~~ return self . process . poll ( ) is None \n 
~~ def inactive ( self ) : \n 
~~~ return self . process . poll ( ) is not None \n 
~~ def create ( self ) : \n 
~~~ self . create_event . set ( ) \n 
~~~ self . stop_event . set ( ) \n 
from SublimeLinter . lint import Linter \n 
class Phpcs ( Linter ) : \n 
syntax = ( , , ) \n 
regex = ( \n 
executable = \n 
defaults = { \n 
inline_overrides = ( ) \n 
tempfile_suffix = \n 
def cmd ( self ) : \n 
settings = Linter . get_view_settings ( self ) \n 
if in settings : \n 
~~~ command = [ settings . get ( ) ] \n 
~~~ command = [ self . executable_path ] \n 
~~ command . append ( ) \n 
return command \n 
~~ ~~ import sublime \n 
import re , os \n 
completions = [ ] \n 
SETTINGS = sublime . load_settings ( ) \n 
def add_methods ( cfc_file , hint_text ) : \n 
~~~ with open ( cfc_file , ) as f : \n 
~~~ read_data = f . read ( ) \n 
~~ methods = [ ] \n 
method_lines = re . findall ( , read_data ) \n 
for l in method_lines : \n 
s = re . search ( , l ) \n 
if s : \n 
~~~ methods . append ( s . group ( ) . strip ( ) ) \n 
~~ ~~ for c in methods : \n 
~~~ snippet = c \n 
params = re . sub ( "\\w+\\(" , "" , snippet , 1 ) [ : - 1 ] . split ( "," ) \n 
num = 1 \n 
if len ( params [ 0 ] ) : \n 
~~~ for p in params : \n 
~~~ snippet = snippet . replace ( p , + str ( num ) + + p + ) \n 
num = num + 1 \n 
~~ ~~ c = re . sub ( "\\(.*\\)" , "" , c ) \n 
~~ ~~ class MethodsAutoComplete ( sublime_plugin . EventListener ) : \n 
~~~ def on_query_completions ( self , view , prefix , locations ) : \n 
~~~ if not view . match_selector ( locations [ 0 ] , \n 
~~ if not SETTINGS . get ( "component_method_completions" ) : \n 
~~ _completions = [ ] \n 
~~~ cfc_region = view . find_by_selector ( "meta.component-operator.extends.value.cfscript" ) [ 0 ] \n 
~~~ cfc_region = "" \n 
~~ if len ( cfc_region ) : \n 
~~~ extendspath = view . substr ( cfc_region ) . replace ( "." , "/" ) \n 
this_file = view . file_name ( ) \n 
if not dir_len > 0 : \n 
~~ this_dir = this_file [ : ( dir_len + 1 ) ] \n 
cfc_file = this_dir + extendspath + ".cfc" \n 
if not os . path . isfile ( cfc_file ) : \n 
~~~ for folder in sublime . active_window ( ) . folders ( ) : \n 
~~~ if os . path . isfile ( folder + "/" + extendspath + ".cfc" ) : \n 
~~~ cfc_file = folder + "/" + extendspath + ".cfc" \n 
~~~ add_methods ( cfc_file , view . substr ( cfc_region ) . split ( "." ) [ - 1 ] ) \n 
~~ except UnboundLocalError : \n 
~~ ~~ add_methods ( view . file_name ( ) , "this" ) \n 
_completions . extend ( completions ) \n 
del completions [ : ] \n 
return _completions \n 
CMD_TARGET_APPLICATION = 0 \n 
CMD_TARGET_WINDOW = 1 \n 
CMD_TARGET_VIEW = 2 \n 
CMD_RUN = \n 
CMD_KEY = \n 
CMD_SET = \n 
def str_to_dict ( s ) : \n 
els = s . split ( ) \n 
for el in els : \n 
~~~ key , value = el . split ( ) \n 
~~~ d [ ] = eval ( value , { } , { } ) \n 
~~~ d [ ] = value \n 
~~ ~~ return d \n 
~~ def run_ ( cmd ) : \n 
~~~ target , predicate = cmd [ ] , cmd [ ] \n 
if cmd [ ] : \n 
~~~ if cmd [ ] : \n 
~~~ target . run_command ( ) \n 
~~ cmd_ , _ , args = predicate . partition ( ) \n 
if args : \n 
~~~ args = str_to_dict ( args ) \n 
~~~ args = { } \n 
~~ if not cmd [ ] : \n 
~~~ target . run_command ( str ( cmd_ ) , args ) \n 
~~ ~~ def set_ ( cmd ) : \n 
~~~ syntax = os . path . basename ( target . settings ( ) . get ( ) ) \n 
target . run_command ( , { \n 
: syntax , \n 
: predicate \n 
~~ if not target . settings ( ) . has ( predicate ) : \n 
sublime . status_message ( msg ) \n 
~~~ name , _ , value = predicate . partition ( ) \n 
target . settings ( ) . set ( name , eval ( value , { } , { } ) ) \n 
~~ except ValueError , e : \n 
~~ ~~ def key_ ( args ) : \n 
~~ from vex . parsers . g_cmd import GlobalLexer \n 
class TestGlobalLexer ( unittest . TestCase ) : \n 
~~~ self . lexer = GlobalLexer ( ) \n 
~~ def testCanMatchFullPattern ( self ) : \n 
~~~ actual = self . lexer . parse ( ) \n 
self . assertEqual ( actual , [ , ] ) \n 
~~ def testCanMatchEmtpySearch ( self ) : \n 
~~ def testCanEscapeCharactersInSearchPattern ( self ) : \n 
~~ def testCanEscapeBackSlashes ( self ) : \n 
from django . contrib import admin \n 
from . models import Product , Option \n 
admin . site . register ( Product ) \n 
admin . site . register ( Option ) \n 
dict_of = lambda o : { k : getattr ( o , k ) for k in dir ( o ) if not in k and not callable ( getattr ( o \n 
from django . db import connections \n 
def fetch ( query , params = [ ] , db = ) : \n 
~~~ cursor = connections [ db ] . cursor ( ) \n 
cursor . execute ( query , params ) \n 
return cursor . fetchall ( ) \n 
~~ from time import time \n 
from funcy . flow import * \n 
def test_silent ( ) : \n 
~~~ assert silent ( int ) ( 1 ) == 1 \n 
assert silent ( int ) ( ) == 1 \n 
assert silent ( int ) ( ) is None \n 
assert silent ( str . upper ) ( ) == \n 
~~ class MyError ( Exception ) : \n 
~~ def test_ignore ( ) : \n 
~~~ assert ignore ( Exception ) ( raiser ( Exception ) ) ( ) is None \n 
assert ignore ( Exception ) ( raiser ( MyError ) ) ( ) is None \n 
assert ignore ( ( TypeError , MyError ) ) ( raiser ( MyError ) ) ( ) is None \n 
with pytest . raises ( TypeError ) : \n 
~~~ ignore ( MyError ) ( raiser ( TypeError ) ) ( ) \n 
~~ assert ignore ( MyError , default = 42 ) ( raiser ( MyError ) ) ( ) == 42 \n 
~~ def test_raiser ( ) : \n 
~~~ with pytest . raises ( Exception ) as e : raiser ( ) ( ) \n 
assert e . type is Exception \n 
with pytest . raises ( MyError ) : raiser ( MyError ) ( ) \n 
with pytest . raises ( MyError ) as e : raiser ( MyError , ) ( ) \n 
assert e . value . args == ( , ) \n 
with pytest . raises ( MyError ) : raiser ( MyError ( ) ) ( ) \n 
with pytest . raises ( MyError ) : raiser ( MyError ) ( , keyword = ) \n 
~~ def test_suppress ( ) : \n 
~~~ with suppress ( Exception ) : \n 
~~~ raise Exception \n 
~~ with suppress ( Exception ) : \n 
~~~ raise MyError \n 
~~ with pytest . raises ( TypeError ) : \n 
~~~ with suppress ( MyError ) : \n 
~~~ raise TypeError \n 
~~ ~~ with suppress ( TypeError , MyError ) : \n 
~~ ~~ def test_retry ( ) : \n 
~~~ calls = [ ] \n 
def failing ( n = 1 ) : \n 
~~~ if len ( calls ) < n : \n 
~~~ calls . append ( 1 ) \n 
raise MyError \n 
~~ with pytest . raises ( MyError ) : failing ( ) \n 
calls = [ ] \n 
assert retry ( 2 , MyError ) ( failing ) ( ) == 1 \n 
with pytest . raises ( MyError ) : retry ( 2 , MyError ) ( failing ) ( 2 ) \n 
~~ def test_retry_timeout ( ) : \n 
~~~ def failing ( ) : \n 
~~ start_time = time ( ) \n 
with pytest . raises ( MyError ) : retry ( 11 , MyError , timeout = 0.01 ) ( failing ) ( ) \n 
assert 0.1 < time ( ) - start_time < 0.11 \n 
start_time = time ( ) \n 
with pytest . raises ( MyError ) : retry ( 4 , MyError , timeout = lambda a : 0.01 * 2 ** a ) ( failing ) ( ) \n 
d = time ( ) - start_time \n 
assert 0.07 < d < 0.08 \n 
~~ def test_retry_many_errors ( ) : \n 
~~ assert retry ( 2 , ( MyError , RuntimeError ) ) ( failing ) ( ) == 1 \n 
assert retry ( 2 , [ MyError , RuntimeError ] ) ( failing ) ( ) == 1 \n 
~~ def test_fallback ( ) : \n 
~~~ assert fallback ( raiser ( ) , lambda : 1 ) == 1 \n 
with pytest . raises ( Exception ) : fallback ( ( raiser ( ) , MyError ) , lambda : 1 ) \n 
assert fallback ( ( raiser ( MyError ) , MyError ) , lambda : 1 ) == 1 \n 
~~ def test_limit_error_rate ( ) : \n 
@ limit_error_rate ( 2 , 60 , MyError ) \n 
def limited ( x ) : \n 
~~~ calls . append ( x ) \n 
raise TypeError \n 
~~ with pytest . raises ( TypeError ) : limited ( 1 ) \n 
with pytest . raises ( TypeError ) : limited ( 2 ) \n 
with pytest . raises ( MyError ) : limited ( 3 ) \n 
assert calls == [ 1 , 2 ] \n 
~~ def test_post_processing ( ) : \n 
~~~ @ post_processing ( max ) \n 
def my_max ( l ) : \n 
~~~ return l \n 
~~ assert my_max ( [ 1 , 3 , 2 ] ) == 3 \n 
~~ def test_collecting ( ) : \n 
~~~ @ collecting \n 
def doubles ( l ) : \n 
~~~ for i in l : \n 
~~~ yield i * 2 \n 
~~ ~~ assert doubles ( [ 1 , 2 ] ) == [ 2 , 4 ] \n 
~~ def test_once ( ) : \n 
@ once \n 
def call ( n ) : \n 
~~~ calls . append ( n ) \n 
return n \n 
~~ call ( 1 ) \n 
call ( 2 ) \n 
assert calls == [ 1 ] \n 
~~ def test_once_per ( ) : \n 
@ once_per ( ) \n 
def call ( n , x = None ) : \n 
call ( 1 , 42 ) \n 
~~ def test_once_per_args ( ) : \n 
@ once_per_args \n 
assert calls == [ 1 , 2 , 1 ] \n 
call ( 1 ) \n 
long_description = open ( ) . read ( ) , \n 
packages = [ , , ] , \n 
import zerorpc \n 
import client_helper \n 
def run ( ) : \n 
args = client_helper . grab_server_args ( ) \n 
workbench = zerorpc . Client ( timeout = 300 , heartbeat = 60 ) \n 
workbench . connect ( + args [ ] + + args [ ] ) \n 
data_path = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , ) \n 
file_list = [ os . path . join ( data_path , child ) for child in os . listdir ( data_path ) ] [ : 2 ] \n 
file_list += [ os . path . join ( data_path , child ) for child in os . listdir ( data_path ) ] [ : 2 ] \n 
for filename in file_list : \n 
~~~ if in filename : continue \n 
with open ( filename , ) as f : \n 
~~~ base_name = os . path . basename ( filename ) \n 
md5 = workbench . store_sample ( f . read ( ) , base_name , ) \n 
results = workbench . work_request ( , md5 ) \n 
pprint . pprint ( results ) \n 
~~ ~~ ~~ def test ( ) : \n 
run ( ) \n 
from rekall_adapter . rekall_adapter import RekallAdapter \n 
class MemoryImageConnScan ( object ) : \n 
dependencies = [ ] \n 
self . plugin_name = \n 
self . current_table_name = \n 
self . output = { : collections . defaultdict ( list ) } \n 
self . column_map = { } \n 
~~ def execute ( self , input_data ) : \n 
adapter = RekallAdapter ( ) \n 
adapter . set_plugin_name ( self . plugin_name ) \n 
rekall_output = adapter . execute ( input_data ) \n 
for line in rekall_output : \n 
~~~ self . output [ ] = line [ ] \n 
~~~ self . current_table_name = line [ ] [ ] [ 1 ] \n 
~~~ row = RekallAdapter . process_row ( line [ ] , self . column_map ) \n 
self . output [ ] [ self . current_table_name ] . append ( row ) \n 
~~~ print % ( line [ ] , line [ ] ) \n 
~~ ~~ return self . output \n 
@ pytest . mark . xfail \n 
workbench . connect ( "tcp://127.0.0.1:4242" ) \n 
data_path = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , if not os . path . isfile ( data_path ) : \n 
urllib . urlretrieve ( \n 
~~ if not os . path . isfile ( data_path ) : \n 
exit ( 1 ) \n 
~~ if os . stat ( data_path ) . st_size < 100000 : \n 
~~~ data_path = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , with open ( data_path , ) as mem_file : \n 
~~~ print % mem_file . read ( ) [ : 500 ] \n 
~~ print \n 
~~ data_path = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , with open ( data_path , ) as mem_file : \n 
~~~ raw_bytes = mem_file . read ( ) \n 
md5 = hashlib . md5 ( raw_bytes ) . hexdigest ( ) \n 
if not workbench . has_sample ( md5 ) : \n 
~~~ md5 = workbench . store_sample ( open ( data_path , ) . read ( ) , , ) \n 
~~ ~~ worker = MemoryImageConnScan ( ) \n 
output = worker . execute ( { : { : raw_bytes } } ) \n 
print % output [ ] \n 
for name , table in output [ ] . iteritems ( ) : \n 
~~~ print % name \n 
pprint . pprint ( table ) \n 
~~ assert not in output \n 
output = workbench . work_request ( , md5 ) [ ] \n 
class View ( object ) : \n 
~~~ self . workbench = zerorpc . Client ( timeout = 300 , heartbeat = 60 ) \n 
self . workbench . connect ( "tcp://127.0.0.1:4242" ) \n 
~~~ md5 = input_data [ ] [ ] \n 
tag = input_data [ ] [ ] \n 
if tag == : \n 
~~~ result = self . workbench . work_request ( , md5 ) [ ] \n 
~~ elif tag == : \n 
~~~ result = input_data \n 
self . workbench . close ( ) \n 
~~ ~~ def test ( ) : \n 
data_path = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , \n 
md5 = workbench . store_sample ( open ( data_path , ) . read ( ) , , ) \n 
input_data = workbench . work_request ( , md5 ) \n 
worker = View ( ) \n 
output = worker . execute ( input_data ) \n 
pprint . pprint ( output ) \n 
~~ from __future__ import unicode_literals , print_function \n 
def get_requirements_file_path ( ) : \n 
directory = os . path . dirname ( __file__ ) \n 
requirements_file = \n 
return os . path . join ( directory , requirements_file ) \n 
~~~ requirements_file = get_requirements_file_path ( ) \n 
print ( % requirements_file ) \n 
os . system ( % requirements_file ) \n 
#coding:utf-8 \n 
~~ from __future__ import unicode_literals , print_function , division \n 
VERSION = "0.1.0" \n 
GENERATOR = "http://pypi.python.org/pypi/ezodf/%s$Python%s" % ( VERSION , sys . version ) \n 
MIMETYPES = { \n 
: "application/vnd.oasis.opendocument.text" , \n 
: "application/vnd.oasis.opendocument.text-template" , \n 
: "application/vnd.oasis.opendocument.graphics" , \n 
: "application/vnd.oasis.opendocument.graphics-template" , \n 
: "application/vnd.oasis.opendocument.presentation" , \n 
: "application/vnd.oasis.opendocument.presentation-template" , \n 
: "application/vnd.oasis.opendocument.spreadsheet" , \n 
: "application/vnd.oasis.opendocument.spreadsheet-template" , \n 
: "application/vnd.oasis.opendocument.chart" , \n 
: "application/vnd.oasis.opendocument.chart-template" , \n 
: "application/vnd.oasis.opendocument.image" , \n 
: "application/vnd.oasis.opendocument.image-template" , \n 
: "application/vnd.oasis.opendocument.formula" , \n 
: "application/vnd.oasis.opendocument.formula-template" , \n 
: "application/vnd.oasis.opendocument.text-master" , \n 
: "application/vnd.oasis.opendocument.text-web" , \n 
FILE_EXT_FOR_MIMETYPE = dict ( [ ( mimetype , ext ) for ext , mimetype in MIMETYPES . items ( ) ] ) \n 
ANIM_NS = "urn:oasis:names:tc:opendocument:xmlns:animation:1.0" \n 
DB_NS = "urn:oasis:names:tc:opendocument:xmlns:database:1.0" \n 
CHART_NS = "urn:oasis:names:tc:opendocument:xmlns:chart:1.0" \n 
CONFIG_NS = "urn:oasis:names:tc:opendocument:xmlns:config:1.0" \n 
CSS3T_NS = "http://www.w3.org/TR/css3-text/" \n 
DC_NS = "http://purl.org/dc/elements/1.1/" \n 
DOM_NS = "http://www.w3.org/2001/xml-events" \n 
DR3D_NS = "urn:oasis:names:tc:opendocument:xmlns:dr3d:1.0" \n 
DRAW_NS = "urn:oasis:names:tc:opendocument:xmlns:drawing:1.0" \n 
DRAWOOO_NS = "http://openoffice.org/2010/draw" \n 
FIELD_NS = "urn:openoffice:names:experimental:ooo-ms-interop:xmlns:field:1.0" \n 
FO_NS = "urn:oasis:names:tc:opendocument:xmlns:xsl-fo-compatible:1.0" \n 
FORM_NS = "urn:oasis:names:tc:opendocument:xmlns:form:1.0" \n 
FORMX_NS = "urn:openoffice:names:experimental:ooxml-odf-interop:xmlns:form:1.0" \n 
GRDDL_NS = "http://www.w3.org/2003/g/data-view#" \n 
KOFFICE_NS = "http://www.koffice.org/2005/" \n 
MANIFEST_NS = "urn:oasis:names:tc:opendocument:xmlns:manifest:1.0" \n 
MATH_NS = "http://www.w3.org/1998/Math/MathML" \n 
META_NS = "urn:oasis:names:tc:opendocument:xmlns:meta:1.0" \n 
NUMBERS_NS = "urn:oasis:names:tc:opendocument:xmlns:datastyle:1.0" \n 
OFFICE_NS = "urn:oasis:names:tc:opendocument:xmlns:office:1.0" \n 
OFFICEOOO_NS = "http://openoffice.org/2009/office" \n 
OF_NS = "urn:oasis:names:tc:opendocument:xmlns:of:1.2" \n 
OOO_NS = "http://openoffice.org/2004/office" \n 
OOOW_NS = "http://openoffice.org/2004/writer" \n 
OOOC_NS = "http://openoffice.org/2004/calc" \n 
PRESENTATION_NS = "urn:oasis:names:tc:opendocument:xmlns:presentation:1.0" \n 
RDFA_NS = "http://docs.oasis-open.org/opendocument/meta/rdfa#" \n 
RPT_NS = "http://openoffice.org/2005/report" \n 
SCRIPT_NS = "urn:oasis:names:tc:opendocument:xmlns:script:1.0" \n 
SMIL_NS = "urn:oasis:names:tc:opendocument:xmlns:smil-compatible:1.0" \n 
STYLE_NS = "urn:oasis:names:tc:opendocument:xmlns:style:1.0" \n 
SVG_NS = "urn:oasis:names:tc:opendocument:xmlns:svg-compatible:1.0" \n 
TABLE_NS = "urn:oasis:names:tc:opendocument:xmlns:table:1.0" \n 
TABLEOOO_NS = "http://openoffice.org/2009/table" \n 
TEXT_NS = "urn:oasis:names:tc:opendocument:xmlns:text:1.0" \n 
XFORMS_NS = "http://www.w3.org/2002/xforms" \n 
XHTML_NS = "http://www.w3.org/1999/xhtml" \n 
XLINKS_NS = "http://www.w3.org/1999/xlink" \n 
XML_NS = "http://www.w3.org/XML/1998/namespace" \n 
XSD_NS = "http://www.w3.org/2001/XMLSchema" \n 
XSI_NS = "http://www.w3.org/2001/XMLSchema-instance" \n 
META_NSMAP = { \n 
: OFFICE_NS , \n 
: XLINKS_NS , \n 
: DC_NS , \n 
: META_NS , \n 
: OOO_NS , \n 
: GRDDL_NS , \n 
MANIFEST_NSMAP = { \n 
: MANIFEST_NS , \n 
STYLES_NSMAP = { \n 
: STYLE_NS , \n 
: TEXT_NS , \n 
: TABLE_NS , \n 
: DRAW_NS , \n 
: FO_NS , \n 
: NUMBERS_NS , \n 
: SVG_NS , \n 
: CHART_NS , \n 
: DR3D_NS , \n 
: MATH_NS , \n 
: FORM_NS , \n 
: SCRIPT_NS , \n 
: OOOW_NS , \n 
: OOOC_NS , \n 
: DOM_NS , \n 
: RPT_NS , \n 
: OF_NS , \n 
: XHTML_NS , \n 
: TABLEOOO_NS , \n 
: CSS3T_NS , \n 
SETTINGS_NSMAP = { \n 
: CONFIG_NS , \n 
TEXT_NSMAP = { \n 
: PRESENTATION_NS , \n 
: XFORMS_NS , \n 
: XSD_NS , \n 
: XSI_NS , \n 
: FIELD_NS , \n 
: FORMX_NS , \n 
SPREADSHEET_NSMAP = { \n 
SPREADSHEET_NSMAP . update ( TEXT_NSMAP ) \n 
PRESENTATION_NSMAP = { \n 
: SMIL_NS , \n 
: ANIM_NS , \n 
: OFFICEOOO_NS , \n 
: DRAWOOO_NS , \n 
PRESENTATION_NSMAP . update ( SPREADSHEET_NSMAP ) \n 
GRAPHICS_NSMAP = PRESENTATION_NSMAP \n 
ALL_NSMAP = { } \n 
ALL_NSMAP . update ( META_NSMAP ) \n 
ALL_NSMAP . update ( MANIFEST_NSMAP ) \n 
ALL_NSMAP . update ( STYLES_NSMAP ) \n 
ALL_NSMAP . update ( SETTINGS_NSMAP ) \n 
ALL_NSMAP . update ( TEXT_NSMAP ) \n 
ALL_NSMAP . update ( SPREADSHEET_NSMAP ) \n 
ALL_NSMAP . update ( PRESENTATION_NSMAP ) \n 
ALL_NSMAP . update ( GRAPHICS_NSMAP ) \n 
MIMETYPE_NSMAP = { \n 
"application/vnd.oasis.opendocument.text" : TEXT_NSMAP , \n 
"application/vnd.oasis.opendocument.text-template" : TEXT_NSMAP , \n 
"application/vnd.oasis.opendocument.graphics" : GRAPHICS_NSMAP , \n 
"application/vnd.oasis.opendocument.graphics-template" : GRAPHICS_NSMAP , \n 
"application/vnd.oasis.opendocument.presentation" : PRESENTATION_NSMAP , \n 
"application/vnd.oasis.opendocument.presentation-template" : PRESENTATION_NSMAP , \n 
"application/vnd.oasis.opendocument.spreadsheet" : SPREADSHEET_NSMAP , \n 
"application/vnd.oasis.opendocument.spreadsheet-template" : SPREADSHEET_NSMAP , \n 
"application/vnd.oasis.opendocument.chart" : GRAPHICS_NSMAP , \n 
"application/vnd.oasis.opendocument.image" : GRAPHICS_NSMAP , \n 
"application/vnd.oasis.opendocument.formula" : GRAPHICS_NSMAP , \n 
MIMETYPE_BODYTAG_MAP = { \n 
"application/vnd.oasis.opendocument.text" : "office:text" , \n 
"application/vnd.oasis.opendocument.text-template" : "office:text" , \n 
"application/vnd.oasis.opendocument.graphics" : "office:drawing" , \n 
"application/vnd.oasis.opendocument.graphics-template" : "office:drawing" , \n 
"application/vnd.oasis.opendocument.presentation" : "office:presentation" , \n 
"application/vnd.oasis.opendocument.presentation-template" : "office:presentation" , \n 
"application/vnd.oasis.opendocument.spreadsheet" : "office:spreadsheet" , \n 
"application/vnd.oasis.opendocument.spreadsheet-template" : "office:spreadsheet" , \n 
"application/vnd.oasis.opendocument.chart" : "office:chart" , \n 
"application/vnd.oasis.opendocument.image" : "office:image" , \n 
DEFAULT_TABLE_EXPAND_STRATEGY = "all_less_maxcount" \n 
DEFAULT_MAXCOUNT = ( 32 , 32 ) \n 
from version import VERSION \n 
AUTHOR_NAME = \n 
AUTHOR_EMAIL = \n 
MAINTAINER_NAME = \n 
MAINTAINER_EMAIL = \n 
~~ ~~ setup ( name = , \n 
version = VERSION , \n 
author = AUTHOR_NAME , \n 
author_email = AUTHOR_EMAIL , \n 
maintainer = MAINTAINER_NAME , \n 
maintainer_email = MAINTAINER_EMAIL , \n 
packages = [ ] , \n 
provides = [ ] , \n 
requires = [ , ] , \n 
keywords = [ , , , ] , \n 
long_description = read ( ) + read ( ) , \n 
from __future__ import unicode_literals , print_function , division \n 
~~~ import unittest2 as unittest \n 
~~~ import unittest \n 
~~ from ezodf . xmlns import etree , CN \n 
from ezodf . base import GenericWrapper \n 
from ezodf . text import Paragraph , Span , Heading , NumberedParagraph , Hyperlink \n 
class TestSpan ( unittest . TestCase ) : \n 
~~~ def test_bare_init ( self ) : \n 
~~~ span = Span ( ) \n 
self . assertTrue ( isinstance ( span , GenericWrapper ) ) \n 
self . assertEqual ( span . xmlnode . tag , CN ( ) ) \n 
~~ def test_init_xmlroot ( self ) : \n 
~~~ node = etree . Element ( CN ( ) , test = "span" ) \n 
span = Span ( xmlnode = node ) \n 
self . assertEqual ( span . xmlnode . get ( ) , "span" ) \n 
~~ def test_init_XML ( self ) : \n 
~~~ node = etree . XML ( SPANDATA ) \n 
~~ def test_textlen ( self ) : \n 
~~~ span = Span ( xmlnode = etree . XML ( SPANDATA ) ) \n 
self . assertEqual ( span . textlen , 8 ) \n 
~~ def test_textlen_with_spaces ( self ) : \n 
~~~ span = Span ( xmlnode = etree . XML ( SPANDATA_SPC ) ) \n 
self . assertEqual ( span . textlen , 12 ) \n 
~~ def test_textlen_with_line_break ( self ) : \n 
~~~ span = Span ( xmlnode = etree . XML ( SPANDATA_BRK ) ) \n 
self . assertEqual ( span . textlen , 11 ) \n 
~~ def test_textlen_with_tab ( self ) : \n 
~~~ span = Span ( xmlnode = etree . XML ( SPANDATA_TAB ) ) \n 
~~ def test_textlen_with_all ( self ) : \n 
~~~ span = Span ( xmlnode = etree . XML ( SPANDATA_ALL ) ) \n 
self . assertEqual ( span . textlen , 23 ) \n 
~~ def test_plaintext ( self ) : \n 
self . assertEqual ( span . plaintext ( ) , ) \n 
~~ def test_plaintext_with_spaces ( self ) : \n 
~~ def test_plaintext_with_line_break ( self ) : \n 
~~ def test_plaintext_with_tab ( self ) : \n 
~~ def test_plaintext_with_all ( self ) : \n 
~~ def test_get_style_name ( self ) : \n 
self . assertEqual ( span . style_name , ) \n 
~~ def test_set_style_name ( self ) : \n 
span . style_name = "XXX" \n 
~~ def test_append_text ( self ) : \n 
~~~ txt = "TEXT" \n 
span = Span ( text = txt ) \n 
self . assertEqual ( span . text , txt ) \n 
self . assertEqual ( span . plaintext ( ) , txt ) \n 
~~ def test_append_text_2 ( self ) : \n 
self . assertEqual ( span [ 0 ] . TAG , CN ( ) ) \n 
self . assertEqual ( span [ 0 ] . tail , "TAIL" ) \n 
~~ def test_append_text_3 ( self ) : \n 
self . assertEqual ( span [ 1 ] . TAG , CN ( ) ) \n 
self . assertEqual ( span [ 2 ] . TAG , CN ( ) ) \n 
self . assertEqual ( span [ 2 ] . tail , None ) \n 
self . assertEqual ( span [ 3 ] . TAG , CN ( ) ) \n 
self . assertEqual ( span [ 4 ] . TAG , CN ( ) ) \n 
self . assertEqual ( span [ 4 ] . count , 3 ) \n 
~~ def test_append_text_4 ( self ) : \n 
~~~ span = Span ( text = "TEXT" ) \n 
class TestParagraph ( unittest . TestCase ) : \n 
~~~ p = Paragraph ( ) \n 
self . assertTrue ( isinstance ( p , GenericWrapper ) ) \n 
self . assertEqual ( p . xmlnode . tag , CN ( ) ) \n 
~~~ node = etree . Element ( CN ( ) , test = "paragraph" ) \n 
p = Paragraph ( xmlnode = node ) \n 
self . assertEqual ( p . xmlnode . get ( ) , "paragraph" ) \n 
~~ def test_cond_style_name ( self ) : \n 
p . cond_style_name = "CONDSTYLE" \n 
self . assertEqual ( p . cond_style_name , "CONDSTYLE" ) \n 
self . assertIsNotNone ( p . xmlnode . get ( CN ( ) ) ) \n 
~~ def test_ID ( self ) : \n 
p . ID = "ID001" \n 
self . assertEqual ( p . ID , "ID001" ) \n 
~~ ~~ class TestNumberedParagraph ( unittest . TestCase ) : \n 
~~~ def test_init_1 ( self ) : \n 
~~~ np = NumberedParagraph ( ) \n 
self . assertEqual ( np . xmlnode . tag , CN ( ) ) \n 
~~ def test_init_type_error ( self ) : \n 
~~~ with self . assertRaises ( TypeError ) : \n 
~~ ~~ def test_paragraph_content ( self ) : \n 
~~~ np = NumberedParagraph ( Paragraph ( ) ) \n 
res = np . content \n 
self . assertEqual ( res . plaintext ( ) , ) \n 
~~ def test_heading_content ( self ) : \n 
~~~ np = NumberedParagraph ( Heading ( ) ) \n 
~~ def test_no_content ( self ) : \n 
newp = np . content \n 
self . assertEqual ( newp . kind , ) \n 
newp . append_text ( ) \n 
self . assertEqual ( np . content . plaintext ( ) , ) \n 
~~ def test_level_0 ( self ) : \n 
np . level = 0 \n 
self . assertEqual ( np . level , 1 ) \n 
~~ def test_level_1 ( self ) : \n 
np . level = \n 
~~ def test_level_2 ( self ) : \n 
np . level = 2. \n 
self . assertEqual ( np . level , 2 ) \n 
~~ ~~ class TestNumberingMixin ( unittest . TestCase ) : \n 
~~~ def test_unset_start_value ( self ) : \n 
self . assertIsNone ( np . start_value ) \n 
~~ def test_start_value_0 ( self ) : \n 
np . start_value = 0 \n 
self . assertEqual ( np . start_value , 1 ) \n 
~~ def test_start_value_1 ( self ) : \n 
np . start_value = \n 
~~ def test_start_value_2 ( self ) : \n 
np . start_value = 2. \n 
self . assertEqual ( np . start_value , 2 ) \n 
~~ def test_unset_formatted_number ( self ) : \n 
self . assertIsNone ( np . formatted_number ) \n 
~~ def test_formatted_value ( self ) : \n 
np . formatted_number = \n 
self . assertEqual ( np . formatted_number , ) \n 
~~ ~~ class TestHeading ( unittest . TestCase ) : \n 
~~~ h = Heading ( ) \n 
self . assertTrue ( isinstance ( h , GenericWrapper ) ) \n 
self . assertEqual ( h . xmlnode . tag , CN ( ) ) \n 
~~~ node = etree . Element ( CN ( ) , test = "heading" ) \n 
h = Heading ( xmlnode = node ) \n 
self . assertEqual ( h . xmlnode . get ( ) , "heading" ) \n 
~~ def test_outline_level_0 ( self ) : \n 
~~~ h = Heading ( , outline_level = 0 ) \n 
self . assertEqual ( h . outline_level , 1 ) \n 
~~ def test_outline_level_1 ( self ) : \n 
~~ def test_outline_level_2 ( self ) : \n 
h . outline_level = 2. \n 
self . assertEqual ( h . outline_level , 2 ) \n 
~~ def test_unset_restart_numbering ( self ) : \n 
self . assertFalse ( h . restart_numbering ) \n 
~~ def test_restart_numbering_true ( self ) : \n 
h . restart_numbering = True \n 
self . assertTrue ( h . restart_numbering ) \n 
h . restart_numbering = False \n 
~~ def test_unset_suppress_numbering ( self ) : \n 
self . assertFalse ( h . suppress_numbering ) \n 
~~ def test_suppress_numbering_true ( self ) : \n 
h . suppress_numbering = True \n 
self . assertTrue ( h . suppress_numbering ) \n 
~~ def test_suppress_numbering_false ( self ) : \n 
h . suppress_numbering = False \n 
~~ ~~ class TestHyperlink ( unittest . TestCase ) : \n 
~~~ def test_init ( self ) : \n 
~~~ h = Hyperlink ( xmlnode = None ) \n 
self . assertIsNotNone ( h ) \n 
~~ def test_unset_name ( self ) : \n 
self . assertIsNone ( h . name ) \n 
~~ def test_name ( self ) : \n 
h . name = \n 
self . assertEqual ( h . name , ) \n 
~~ def test_unset_href ( self ) : \n 
self . assertIsNone ( h . href ) \n 
~~ def test_href ( self ) : \n 
h . href = \n 
self . assertEqual ( h . href , ) \n 
~~ def test_target_frame ( self ) : \n 
h . target_frame = \n 
self . assertEqual ( h . target_frame , ) \n 
self . assertEqual ( h . get_attr ( CN ( ) ) , ) \n 
~~ def test_target_frame_blank ( self ) : \n 
~~ from coapthon . utils import parse_blockwise \n 
from coapthon import defines \n 
from coapthon . messages . option import Option \n 
__version__ = "3.0" \n 
~~~ self . _type = None \n 
self . _mid = None \n 
self . _token = None \n 
self . _options = [ ] \n 
self . _payload = None \n 
self . _destination = None \n 
self . _source = None \n 
self . _code = None \n 
self . _acknowledged = None \n 
self . _rejected = None \n 
self . _timeouted = None \n 
self . _cancelled = None \n 
self . _duplicated = None \n 
self . _timestamp = None \n 
self . _version = 1 \n 
def version ( self ) : \n 
~~~ return self . _version \n 
~~ @ version . setter \n 
def version ( self , v ) : \n 
~~~ if not isinstance ( v , int ) or v != 1 : \n 
~~~ raise AttributeError \n 
~~ self . _version = v \n 
def type ( self ) : \n 
return self . _type \n 
~~ @ type . setter \n 
def type ( self , value ) : \n 
if value not in defines . Types . values ( ) : \n 
~~ self . _type = value \n 
def mid ( self ) : \n 
return self . _mid \n 
~~ @ mid . setter \n 
def mid ( self , value ) : \n 
if not isinstance ( value , int ) or value > 65536 : \n 
~~ self . _mid = value \n 
~~ @ mid . deleter \n 
~~~ self . _mid = None \n 
def token ( self ) : \n 
return self . _token \n 
~~ @ token . setter \n 
def token ( self , value ) : \n 
if not isinstance ( value , str ) : \n 
~~~ value = str ( value ) \n 
~~ if len ( value ) > 256 : \n 
~~ self . _token = value \n 
~~ @ token . deleter \n 
~~~ self . _token = None \n 
def options ( self ) : \n 
return self . _options \n 
~~ @ options . setter \n 
def options ( self , value ) : \n 
~~~ value = [ ] \n 
~~ assert isinstance ( value , list ) \n 
self . _options = value \n 
def payload ( self ) : \n 
return self . _payload \n 
~~ @ payload . setter \n 
def payload ( self , value ) : \n 
if isinstance ( value , tuple ) : \n 
~~~ content_type , payload = value \n 
self . content_type = content_type \n 
self . _payload = payload \n 
~~~ self . _payload = value \n 
def destination ( self ) : \n 
return self . _destination \n 
~~ @ destination . setter \n 
def destination ( self , value ) : \n 
if value is not None and ( not isinstance ( value , tuple ) or len ( value ) ) != 2 : \n 
~~ self . _destination = value \n 
return self . _source \n 
~~ @ source . setter \n 
def source ( self , value ) : \n 
if not isinstance ( value , tuple ) or len ( value ) != 2 : \n 
~~ self . _source = value \n 
def code ( self ) : \n 
return self . _code \n 
~~ @ code . setter \n 
def code ( self , value ) : \n 
if value not in defines . Codes . LIST . keys ( ) and value is not None : \n 
~~ self . _code = value \n 
def acknowledged ( self ) : \n 
return self . _acknowledged \n 
~~ @ acknowledged . setter \n 
def acknowledged ( self , value ) : \n 
assert ( isinstance ( value , bool ) ) \n 
self . _acknowledged = value \n 
~~~ self . _timeouted = False \n 
self . _rejected = False \n 
self . _cancelled = False \n 
def rejected ( self ) : \n 
return self . _rejected \n 
~~ @ rejected . setter \n 
def rejected ( self , value ) : \n 
self . _rejected = value \n 
self . _acknowledged = False \n 
self . _cancelled = True \n 
def timeouted ( self ) : \n 
return self . _timeouted \n 
~~ @ timeouted . setter \n 
def timeouted ( self , value ) : \n 
self . _timeouted = value \n 
~~~ self . _acknowledged = False \n 
def duplicated ( self ) : \n 
return self . _duplicated \n 
~~ @ duplicated . setter \n 
def duplicated ( self , value ) : \n 
self . _duplicated = value \n 
def timestamp ( self ) : \n 
return self . _timestamp \n 
~~ @ timestamp . setter \n 
def timestamp ( self , value ) : \n 
self . _timestamp = value \n 
~~ def _already_in ( self , option ) : \n 
for opt in self . _options : \n 
~~~ if option . number == opt . number : \n 
~~ def add_option ( self , option ) : \n 
assert isinstance ( option , Option ) \n 
repeatable = defines . OptionRegistry . LIST [ option . number ] . repeatable \n 
if not repeatable : \n 
~~~ ret = self . _already_in ( option ) \n 
if ret : \n 
~~~ self . _options . append ( option ) \n 
~~ ~~ def del_option ( self , option ) : \n 
while option in list ( self . _options ) : \n 
~~~ self . _options . remove ( option ) \n 
~~ ~~ def del_option_by_name ( self , name ) : \n 
for o in list ( self . _options ) : \n 
~~~ assert isinstance ( o , Option ) \n 
if o . name == name : \n 
~~~ self . _options . remove ( o ) \n 
~~ ~~ ~~ def del_option_by_number ( self , number ) : \n 
if o . number == number : \n 
~~ ~~ ~~ @ property \n 
def etag ( self ) : \n 
value = [ ] \n 
for option in self . options : \n 
~~~ if option . number == defines . OptionRegistry . ETAG . number : \n 
~~~ value . append ( option . value ) \n 
~~ ~~ return value \n 
~~ @ etag . setter \n 
def etag ( self , etag ) : \n 
if not isinstance ( etag , list ) : \n 
~~~ etag = [ etag ] \n 
~~ for e in etag : \n 
~~~ option = Option ( ) \n 
option . number = defines . OptionRegistry . ETAG . number \n 
option . value = e \n 
self . add_option ( option ) \n 
~~ ~~ @ etag . deleter \n 
self . del_option_by_number ( defines . OptionRegistry . ETAG . number ) \n 
def content_type ( self ) : \n 
value = 0 \n 
~~~ if option . number == defines . OptionRegistry . CONTENT_TYPE . number : \n 
~~~ value = int ( option . value ) \n 
~~ @ content_type . setter \n 
def content_type ( self , content_type ) : \n 
option = Option ( ) \n 
option . number = defines . OptionRegistry . CONTENT_TYPE . number \n 
option . value = int ( content_type ) \n 
~~ @ content_type . deleter \n 
~~~ self . del_option_by_number ( defines . OptionRegistry . CONTENT_TYPE . number ) \n 
def observe ( self ) : \n 
~~~ if option . number == defines . OptionRegistry . OBSERVE . number : \n 
~~~ if option . value is None : \n 
~~ return option . value \n 
~~ @ observe . setter \n 
def observe ( self , ob ) : \n 
option . number = defines . OptionRegistry . OBSERVE . number \n 
option . value = ob \n 
self . del_option_by_number ( defines . OptionRegistry . OBSERVE . number ) \n 
~~ @ observe . deleter \n 
~~~ self . del_option_by_number ( defines . OptionRegistry . OBSERVE . number ) \n 
def block1 ( self ) : \n 
value = None \n 
~~~ if option . number == defines . OptionRegistry . BLOCK1 . number : \n 
~~~ value = parse_blockwise ( option . value ) \n 
~~ @ block1 . setter \n 
def block1 ( self , value ) : \n 
option . number = defines . OptionRegistry . BLOCK1 . number \n 
num , m , size = value \n 
if size > 512 : \n 
~~~ szx = 6 \n 
~~ elif 256 < size <= 512 : \n 
~~~ szx = 5 \n 
~~ elif 128 < size <= 256 : \n 
~~~ szx = 4 \n 
~~ elif 64 < size <= 128 : \n 
~~~ szx = 3 \n 
~~ elif 32 < size <= 64 : \n 
~~~ szx = 2 \n 
~~ elif 16 < size <= 32 : \n 
~~~ szx = 1 \n 
~~~ szx = 0 \n 
~~ value = ( num << 4 ) \n 
value |= ( m << 3 ) \n 
value |= szx \n 
option . value = value \n 
~~ @ block1 . deleter \n 
~~~ self . del_option_by_number ( defines . OptionRegistry . BLOCK1 . number ) \n 
def block2 ( self ) : \n 
~~~ if option . number == defines . OptionRegistry . BLOCK2 . number : \n 
~~ @ block2 . setter \n 
def block2 ( self , value ) : \n 
option . number = defines . OptionRegistry . BLOCK2 . number \n 
~~ @ block2 . deleter \n 
~~~ self . del_option_by_number ( defines . OptionRegistry . BLOCK2 . number ) \n 
def line_print ( self ) : \n 
~~~ inv_types = { v : k for k , v in defines . Types . iteritems ( ) } \n 
if self . _code is None : \n 
~~~ self . _code = defines . Codes . EMPTY . number \n 
~~ msg += "]" \n 
if self . payload is not None : \n 
~~ return msg \n 
~~~ return self . line_print \n 
~~ def pretty_print ( self ) : \n 
inv_types = { v : k for k , v in defines . Types . iteritems ( ) } \n 
~~~ self . _code = 0 \n 
~~~ msg += str ( opt ) \n 
msg += str ( self . _payload ) + "\\n" \n 
return msg \n 
import decimal \n 
if sys . version_info [ 0 ] == 2 : \n 
~~ logger = logging . getLogger ( __name__ ) \n 
OBJECT = "OBJECT" \n 
ARRAY = "ARRAY" \n 
FIELD = "FIELD" \n 
STRING = "STRING" \n 
NUMBER = "NUMBER" \n 
BOOLEAN = "BOOLEAN" \n 
NULL = "null" \n 
TRUE = "true" \n 
FALSE = "false" \n 
START_OBJECT = "START_OBJECT" \n 
START_ARRAY = "START_ARRAY" \n 
FIELD_NAME = "FIELD_NAME" \n 
FIELD_VALUE = "FIELD_VALUE" \n 
ARRAY_VALUE = "ARRAY_VALUE" \n 
END_OBJECT = "END_OBJECT" \n 
END_ARRAY = "END_ARRAY" \n 
JSON_SYNTAX_ERROR = "JSON_SYNTAX_ERROR" \n 
JSON_INCOMPLETE_ERROR = "JSON_INCOMPLETE_ERROR" \n 
JSON_UNEXPECTED_ELEMENT_ERROR = "JSON_UNEXPECTED_ELEMENT_ERROR" \n 
class JSONPullParser ( object ) : \n 
~~~ def __init__ ( self , stream , size = 2 ** 16 ) : \n 
self . stream = stream \n 
self . size = size \n 
self . node = None \n 
self . value = "" \n 
self . valueType = None \n 
self . tokens = [ ] \n 
self . tokenIndex = 0 \n 
self . halfToken = "" \n 
self . pattern = re . compile ( \'([\\[\\]{}:\\\\\\\\",])\' ) \n 
~~ def expectObject ( self ) : \n 
event = self . nextEvent ( ) \n 
if event . type != START_OBJECT : \n 
~~~ raise JSONParseError ( \n 
JSON_UNEXPECTED_ELEMENT_ERROR , \n 
~~ ~~ def expectArray ( self ) : \n 
if event . type != START_ARRAY : \n 
~~ return JSONArrayIterator ( self ) \n 
~~ def expectField ( self , expectedName , expectedType = None , allowNull = False , \n 
readAll = False ) : \n 
if event . type != FIELD_NAME : \n 
~~ if event . value != expectedName : \n 
~~ return self . _expectValue ( FIELD_VALUE , expectedType , allowNull , readAll ) \n 
~~ def expectArrayValue ( self , expectedType = None , allowNull = False , \n 
return self . _expectValue ( ARRAY_VALUE , expectedType , allowNull , readAll ) \n 
~~ def _expectValue ( self , eventType , expectedType , allowNull , readAll ) : \n 
~~~ event = self . nextEvent ( ) \n 
if event . type == eventType : \n 
~~~ if allowNull and event . valueType == NULL : \n 
~~ elif expectedType is not None and event . valueType != expectedType : \n 
~~~ return event . value \n 
~~~ if eventType == ARRAY_VALUE : \n 
~~~ if event . node . parent is None or event . node . parent != ARRAY : \n 
~~ ~~ if event . type == START_OBJECT : \n 
~~~ if expectedType is not None and expectedType != OBJECT : \n 
~~ elif expectedType is None or readAll : \n 
~~~ return self . readObject ( event ) \n 
~~ ~~ elif event . type == START_ARRAY : \n 
~~~ if expectedType is not None and expectedType != ARRAY : \n 
~~ if expectedType is None or readAll : \n 
~~~ return self . readArray ( event ) \n 
~~~ return JSONArrayIterator ( self ) \n 
~~ ~~ ~~ def readObject ( self , event = None ) : \n 
if event is None : \n 
popRequired = False \n 
~~~ popRequired = True \n 
~~ if event is None : \n 
~~ if event . type != START_OBJECT : \n 
~~ obj = self . _load ( event ) \n 
if popRequired : \n 
~~~ self . _pop ( ) \n 
~~ return obj \n 
~~ def readArray ( self , event = None ) : \n 
~~ if event . type != START_ARRAY : \n 
~~ arr = self . _load ( event ) \n 
~~ return arr \n 
~~ def nextEvent ( self ) : \n 
~~~ return self . __next__ ( ) \n 
~~ ~~ def next ( self ) : \n 
return self . __next__ ( ) \n 
~~ def __next__ ( self ) : \n 
~~~ token = self . tokens [ self . tokenIndex ] \n 
self . tokenIndex += 1 \n 
if token == "" or token . isspace ( ) : \n 
~~ elif token == : \n 
~~~ return self . _push ( OBJECT ) \n 
~~~ if self . node . type == FIELD : \n 
~~~ self . tokenIndex -= 1 \n 
event = self . _pop ( ) \n 
if event is not None : \n 
~~~ return event \n 
~~ ~~ elif self . node . type == OBJECT : \n 
~~~ return self . _pop ( ) \n 
JSON_SYNTAX_ERROR , \n 
~~ ~~ elif token == : \n 
~~~ if self . node is not None and self . node . type == OBJECT : \n 
~~ return self . _push ( ARRAY ) \n 
~~~ if self . valueType is not None : \n 
event = self . _arrayValue ( ) \n 
~~ ~~ elif self . node . type == ARRAY : \n 
~~~ if self . node . lastIndex == self . node . arrayLength : \n 
~~~ self . node . arrayLength += 1 \n 
~~ return self . _pop ( ) \n 
~~~ if self . node . type == OBJECT : \n 
~~~ if self . value != "" and self . valueType == STRING : \n 
~~~ event = self . _push ( FIELD , self . value ) \n 
~~~ if self . node . type == ARRAY : \n 
~~~ event = self . _arrayValue ( ) \n 
self . node . arrayLength += 1 \n 
~~ elif self . node . type == FIELD : \n 
~~~ event = self . _pop ( ) \n 
~~ if event is not None : \n 
~~ elif self . node is None : \n 
~~ elif token == \'"\' : \n 
~~~ escape = False \n 
if token == "" : \n 
~~ elif escape : \n 
self . value += token \n 
~~~ escape = True \n 
~~~ self . value += token \n 
~~~ data = self . stream . read ( self . size ) \n 
if data == "" : \n 
JSON_INCOMPLETE_ERROR , \n 
~~ self . tokens = self . pattern . split ( data ) \n 
~~ ~~ self . valueType = STRING \n 
~~~ token = token . strip ( ) \n 
if self . tokenIndex == len ( self . tokens ) : \n 
~~~ self . halfToken = token \n 
raise IndexError \n 
~~ elif token [ 0 ] . isdigit ( ) or token [ 0 ] == : \n 
~~~ self . value = decimal . Decimal ( token ) \n 
self . valueType = NUMBER \n 
~~ elif token == "null" : \n 
~~~ self . value = None \n 
self . valueType = NULL \n 
~~ elif token == "true" : \n 
~~~ self . value = True \n 
self . valueType = BOOLEAN \n 
~~ elif token == "false" : \n 
~~~ self . value = False \n 
~~ ~~ ~~ ~~ except IndexError : \n 
~~~ if self . node is not None : \n 
~~~ raise StopIteration ( ) \n 
~~ logger . trace ( data ) \n 
self . tokens = self . pattern . split ( data ) \n 
if self . halfToken is not None : \n 
~~~ self . tokens [ 0 ] = self . halfToken + self . tokens [ 0 ] \n 
self . halfToken = None \n 
~~ ~~ ~~ ~~ def _load ( self , event ) : \n 
~~~ if event . type == START_OBJECT : \n 
~~~ value = start = "{" \n 
end = "}" \n 
~~ elif event . type == START_ARRAY : \n 
~~~ value = start = "[" \n 
end = "]" \n 
~~ count = 1 \n 
tokens = self . tokens \n 
tokenIndex = self . tokenIndex \n 
inString = False \n 
inEscape = False \n 
~~~ startIndex = tokenIndex \n 
for token in tokens [ startIndex : ] : \n 
~~~ tokenIndex += 1 \n 
~~ elif inString : \n 
~~~ if inEscape : \n 
~~~ inEscape = False \n 
~~~ inString = False \n 
~~~ inEscape = True \n 
~~ ~~ elif token == \'"\' : \n 
~~~ inString = True \n 
~~ elif token == start : \n 
~~ elif token == end : \n 
~~~ count -= 1 \n 
if count == 0 : \n 
~~~ value += "" . join ( tokens [ startIndex : tokenIndex ] ) \n 
raise StopIteration ( ) \n 
~~ ~~ ~~ value += "" . join ( tokens [ startIndex : ] ) \n 
data = self . stream . read ( self . size ) \n 
~~ tokens = self . pattern . split ( data ) \n 
tokenIndex = 0 \n 
~~ ~~ except StopIteration : \n 
~~ self . tokens = tokens \n 
self . tokenIndex = tokenIndex \n 
~~~ return json . loads ( value , parse_float = decimal . Decimal , \n 
parse_int = decimal . Decimal ) \n 
~~~ raise JSONParseError ( JSON_SYNTAX_ERROR , "" . join ( e . args ) ) \n 
~~ ~~ def _push ( self , nodeType , value = None ) : \n 
~~~ if self . node is not None and self . node . type == FIELD : \n 
~~~ self . node . valueType = nodeType \n 
~~ self . node = JSONNode ( self . node , nodeType , value ) \n 
if self . node . parent is not None and self . node . parent . type == ARRAY : \n 
~~~ self . node . arrayIndex = self . node . parent . arrayLength \n 
if self . node . parent . lastIndex == self . node . parent . arrayLength : \n 
~~ self . node . parent . lastIndex = self . node . parent . arrayLength \n 
~~ return self . node . startEvent ( ) \n 
~~ def _pop ( self ) : \n 
~~~ node = self . node \n 
self . node = self . node . parent \n 
if node . valueType is None : \n 
~~~ node . valueType = self . valueType \n 
node . value = self . value \n 
~~ self . value = "" \n 
if node . type == FIELD and node . valueType is None : \n 
~~ return node . endEvent ( ) \n 
~~ def _arrayValue ( self ) : \n 
~~~ endOfArray = self . node . lastIndex == self . node . arrayLength \n 
if self . valueType is None and endOfArray : \n 
~~ elif self . valueType is None : \n 
str ( self . node . arrayLength ) ) \n 
~~~ event = JSONEvent ( \n 
self . node , ARRAY_VALUE , self . value , self . valueType , \n 
self . node . arrayLength ) \n 
self . node . lastIndex = self . node . arrayLength \n 
~~ ~~ def __iter__ ( self ) : \n 
~~ ~~ class JSONParseError ( Exception ) : \n 
~~~ def __init__ ( self , code , msg ) : \n 
~~~ self . args = ( code , msg ) \n 
~~ ~~ class JSONNode ( object ) : \n 
~~~ def __init__ ( self , parent , nodeType , name = None , value = None , \n 
valueType = None ) : \n 
~~~ self . parent = parent \n 
self . type = nodeType \n 
self . name = name \n 
self . value = value \n 
self . valueType = valueType \n 
self . arrayIndex = None \n 
self . arrayLength = None \n 
self . lastIndex = - 1 \n 
if nodeType == ARRAY : \n 
~~~ self . arrayLength = 0 \n 
~~ ~~ def startEvent ( self ) : \n 
~~~ if self . type == ARRAY : \n 
~~~ return JSONEvent ( self , START_ARRAY , arrayIndex = self . arrayIndex ) \n 
~~ elif self . type == OBJECT : \n 
~~~ return JSONEvent ( self , START_OBJECT , arrayIndex = self . arrayIndex ) \n 
~~ elif self . type == FIELD : \n 
~~~ return JSONEvent ( self , FIELD_NAME , self . name ) \n 
~~ ~~ def endEvent ( self ) : \n 
~~~ return JSONEvent ( self , END_ARRAY , arrayIndex = self . arrayIndex , \n 
arrayLength = self . arrayLength ) \n 
~~~ return JSONEvent ( self , END_OBJECT , arrayIndex = self . arrayIndex ) \n 
~~ elif self . type == FIELD and self . valueType not in ( OBJECT , ARRAY ) : \n 
~~~ return JSONEvent ( self , FIELD_VALUE , self . value , self . valueType ) \n 
~~ ~~ ~~ class JSONEvent ( object ) : \n 
~~~ def __init__ ( self , node , eventType , value = None , valueType = None , \n 
arrayIndex = None , arrayLength = None ) : \n 
~~~ self . node = node \n 
self . type = eventType \n 
self . arrayIndex = arrayIndex \n 
self . arrayLength = arrayLength \n 
if self . value is not None : \n 
~~ if self . valueType is not None : \n 
~~ if self . arrayIndex is not None : \n 
~~ if self . arrayLength is not None : \n 
~~ text += ")" \n 
~~ ~~ class JSONArrayIterator ( object ) : \n 
~~~ def __init__ ( self , parser ) : \n 
~~~ self . parser = parser \n 
self . complete = False \n 
~~~ if self . complete : \n 
~~~ event = self . parser . nextEvent ( ) \n 
if event . type == START_OBJECT : \n 
~~~ return self . parser . readObject ( event ) \n 
~~~ return self . parser . readArray ( event ) \n 
~~ elif event . type == ARRAY_VALUE : \n 
~~ elif event . type == END_ARRAY : \n 
~~~ self . complete = True \n 
~~ ~~ ~~ def next ( self ) : \n 
from urllib . parse import urljoin \n 
from bs4 import BeautifulSoup \n 
LOGGER = logging . getLogger ( __name__ ) \n 
class Site ( object ) : \n 
def __init__ ( self , url_base , url_archive , paste_tag , target_patterns , paste ) : \n 
~~~ self . url_base = url_base \n 
self . url_archive = url_archive \n 
self . paste_tag = paste_tag \n 
self . target_patterns = target_patterns \n 
self . headers = { : } \n 
self . paste = paste \n 
req = requests . get ( self . url_base + self . url_archive , headers = self . headers ) \n 
LOGGER . debug ( ) \n 
while req . status_code != 200 : \n 
~~~ LOGGER . error ( ) \n 
req = requests . get ( self . url_base + self . url_archive ) \n 
~~ soup = BeautifulSoup ( req . text , ) \n 
links = soup . find_all ( self . paste_tag ) \n 
LOGGER . debug ( , len ( links ) ) \n 
return [ self . paste ( urljoin ( self . url_base , link . a . get ( ) ) ) for link in links ] \n 
~~ def get_paste ( self , paste ) : \n 
req = requests . get ( paste . url , headers = self . headers ) \n 
req = requests . get ( paste . url ) \n 
~~ found = [ ] \n 
for name , pattern in self . target_patterns : \n 
~~~ LOGGER . debug ( , pattern ) \n 
matches = re . findall ( pattern , req . text ) \n 
LOGGER . debug ( , len ( matches ) ) \n 
if matches : \n 
~~~ found . append ( ( name , len ( matches ) ) ) \n 
~~ ~~ return found \n 
~~ ~~ class Paste ( object ) : \n 
def __init__ ( self , url ) : \n 
~~~ _id = url . split ( ) [ - 1 ] \n 
self . _id = _id \n 
~~ ~~ class PastebinPaste ( Paste ) : \n 
~~~ super ( ) . __init__ ( url ) \n 
self . url = . format ( self . _id ) \n 
~~ ~~ class PastiePaste ( Paste ) : \n 
~~ ~~ class SlexyPaste ( Paste ) : \n 
~~ ~~ class Pastebin ( Site ) : \n 
def __init__ ( self , target_patterns ) : \n 
~~~ self . url_base = \n 
self . url_archive = \n 
self . paste_tag = lambda tag : tag . name == and tag . a and not in tag . a [ ] and tag . a [ ] [ 1 : ] \n 
super ( ) . __init__ ( self . url_base , self . url_archive , self . paste_tag , target_patterns , \n 
PastebinPaste ) \n 
~~ ~~ class Pastie ( Site ) : \n 
self . paste_tag = lambda tag : tag . name == and tag . a and self . url_base in tag . a [ ] \n 
PastiePaste ) \n 
~~ ~~ class Slexy ( Site ) : \n 
self . paste_tag = lambda tag : tag . name == and tag . a and in tag . a [ ] \n 
SlexyPaste ) \n 
from peewee import Proxy , TextField , FloatField , CharField , IntegerField , SqliteDatabase , Model , DateTimeField \n 
db = Proxy ( ) \n 
class Result ( Model ) : \n 
error = TextField ( null = True ) \n 
scriptrun_time = FloatField ( ) \n 
elapsed = FloatField ( ) \n 
epoch = FloatField ( ) \n 
custom_timers = TextField ( null = True ) \n 
turret_name = CharField ( default = ) \n 
def to_dict ( self ) : \n 
: self . error , \n 
: self . scriptrun_time , \n 
: self . elapsed , \n 
: self . epoch , \n 
: json . loads ( self . custom_timers ) , \n 
: self . turret_name \n 
~~~ database = db \n 
~~ ~~ class Turret ( Model ) : \n 
name = TextField ( ) \n 
uuid = TextField ( ) \n 
cannons = IntegerField ( ) \n 
script = TextField ( ) \n 
rampup = IntegerField ( ) \n 
status = TextField ( ) \n 
updated_at = DateTimeField ( default = datetime . datetime . now ( ) ) \n 
def save ( self , * args , ** kwargs ) : \n 
~~~ self . updated_at = datetime . datetime . now ( ) \n 
return super ( Turret , self ) . save ( * args , ** kwargs ) \n 
~~ def to_dict ( self ) : \n 
: self . name , \n 
: self . uuid , \n 
: self . cannons , \n 
: self . script , \n 
: self . rampup , \n 
: self . status , \n 
: self . updated_at \n 
~~ ~~ def set_database ( db_path , proxy , config ) : \n 
if in config and config [ ] is True : \n 
~~~ database = SqliteDatabase ( , check_same_thread = False ) \n 
~~~ database = SqliteDatabase ( db_path , check_same_thread = False ) \n 
~~ proxy . initialize ( database ) \n 
~~ from . . import ( ctan , error , local_info ) \n 
from . import ( install , uninstall ) \n 
def run ( package ) : \n 
~~~ if isinstance ( package , list ) : \n 
~~~ for pkg in package : \n 
~~~ run ( pkg ) \n 
~~ pkg = package . lower ( ) \n 
error . installed ( , pkg , fail = True ) \n 
installed_version = local_info . get_data ( pkg ) [ 1 ] \n 
version , _ = ctan . get_data ( pkg ) \n 
error . updated ( package , installed_version , version , fail = True ) \n 
uninstall . run ( package ) \n 
install . run ( package ) \n 
import gettor . twitter \n 
~~~ logging_level = \n 
logging_file = \n 
logging_format = \n 
logging . basicConfig ( \n 
format = logging_format , \n 
datefmt = date_format , \n 
filename = logging_file , \n 
level = logging_level \n 
~~~ bot = gettor . twitter . TwitterBot ( ) \n 
bot . start ( ) \n 
~~ except gettor . twitter . ConfigError as e : \n 
~~ except gettor . twitter . InternalError as e : \n 
~~ from __future__ import print_function , unicode_literals , with_statement , division \n 
~~~ from django . utils import simplejson as json \n 
~~ class JsonResponse ( HttpResponse ) : \n 
~~~ def __init__ ( self , data = None , errors = None , success = True ) : \n 
if not errors : \n 
~~~ errors = [ ] \n 
~~ if not data : \n 
~~~ data = { } \n 
~~ json_resp = json_response ( data = data , errors = errors , success = success ) \n 
super ( JsonResponse , self ) . __init__ ( json_resp , content_type = ) \n 
~~ ~~ class JsonpResponse ( HttpResponse ) : \n 
def __init__ ( self , request , data = None , errors = None , success = True ) : \n 
js = "{0}({1})" . format ( request . GET . get ( "jsonp" , "jsonp_callback" ) , json_resp ) \n 
super ( JsonpResponse , self ) . __init__ ( js , mimetype = ) \n 
~~ ~~ def json_response ( data = None , errors = None , success = True ) : \n 
~~~ if not errors : \n 
~~ data . update ( { \n 
: errors , \n 
: len ( errors ) == 0 and success , \n 
return json . dumps ( data ) \n 
~~ class XMLResponse ( HttpResponse ) : \n 
super ( XMLResponse , self ) . __init__ ( data , mimetype = ) \n 
~~ ~~ from django . db import models \n 
class CronJobLog ( models . Model ) : \n 
code = models . CharField ( max_length = 64 , db_index = True ) \n 
start_time = models . DateTimeField ( db_index = True ) \n 
end_time = models . DateTimeField ( db_index = True ) \n 
is_success = models . BooleanField ( default = False ) \n 
ran_at_time = models . TimeField ( null = True , blank = True , db_index = True , editable = False ) \n 
~~~ return % ( self . code , if self . is_success else ) \n 
~~~ index_together = [ \n 
( , , ) , \n 
app_label = \n 
~~ ~~ from django . core . urlresolvers import reverse \n 
from django . contrib . auth import get_user_model \n 
from django . test . utils import override_settings \n 
from django . utils . encoding import force_text \n 
from . test_base import BaseAPITestCase \n 
class APITestCase1 ( TestCase , BaseAPITestCase ) : \n 
urls = \n 
USERNAME = \n 
PASS = \n 
EMAIL = "person1@world.com" \n 
NEW_PASS = \n 
REGISTRATION_VIEW = \n 
REGISTRATION_DATA = { \n 
"username" : USERNAME , \n 
"password1" : PASS , \n 
"password2" : PASS \n 
REGISTRATION_DATA_WITH_EMAIL = REGISTRATION_DATA . copy ( ) \n 
REGISTRATION_DATA_WITH_EMAIL [ ] = EMAIL \n 
BASIC_USER_DATA = { \n 
: "John" , \n 
: EMAIL \n 
USER_DATA = BASIC_USER_DATA . copy ( ) \n 
USER_DATA [ ] = True \n 
~~~ self . init ( ) \n 
~~ def _generate_uid_and_token ( self , user ) : \n 
~~~ result = { } \n 
from django . utils . encoding import force_bytes \n 
from django . contrib . auth . tokens import default_token_generator \n 
from django . utils . http import urlsafe_base64_encode \n 
result [ ] = urlsafe_base64_encode ( force_bytes ( user . pk ) ) \n 
result [ ] = default_token_generator . make_token ( user ) \n 
~~ def test_login ( self ) : \n 
"username" : self . USERNAME , \n 
"password" : self . PASS \n 
self . post ( self . login_url , data = payload , status_code = 400 ) \n 
self . post ( self . password_change_url , status_code = 403 ) \n 
user = get_user_model ( ) . objects . create_user ( self . USERNAME , , self . PASS ) \n 
self . post ( self . login_url , data = payload , status_code = 200 ) \n 
self . assertEqual ( in self . response . json . keys ( ) , True ) \n 
self . token = self . response . json [ ] \n 
self . post ( self . password_change_url , status_code = 400 ) \n 
user . is_active = False \n 
"username" : self . USERNAME + , \n 
self . post ( self . login_url , data = { } , status_code = 400 ) \n 
~~ @ override_settings ( REST_USE_JWT = True ) \n 
def test_login_jwt ( self ) : \n 
get_user_model ( ) . objects . create_user ( self . USERNAME , , self . PASS ) \n 
~~ def test_login_by_email ( self ) : \n 
~~~ settings . INSTALLED_APPS . remove ( ) \n 
"email" : self . EMAIL . lower ( ) , \n 
user = get_user_model ( ) . objects . create_user ( self . USERNAME , self . EMAIL , self . PASS ) \n 
"email" : self . EMAIL . upper ( ) , \n 
"email" : + self . EMAIL , \n 
~~ def test_password_change ( self ) : \n 
~~~ login_payload = { \n 
self . post ( self . login_url , data = login_payload , status_code = 200 ) \n 
new_password_payload = { \n 
"new_password1" : "new_person" , \n 
"new_password2" : "new_person" \n 
self . post ( \n 
self . password_change_url , \n 
data = new_password_payload , \n 
status_code = 200 \n 
self . post ( self . login_url , data = login_payload , status_code = 400 ) \n 
login_payload [ ] = new_password_payload [ ] \n 
"new_password1" : "new_person1" , \n 
status_code = 400 \n 
self . post ( self . password_change_url , data = { } , status_code = 400 ) \n 
~~ @ override_settings ( OLD_PASSWORD_FIELD_ENABLED = True ) \n 
def test_password_change_with_old_password ( self ) : \n 
"old_password" : self . PASS , \n 
~~ def test_password_reset ( self ) : \n 
~~~ user = get_user_model ( ) . objects . create_user ( self . USERNAME , self . EMAIL , self . PASS ) \n 
mail_count = len ( mail . outbox ) \n 
payload = { : self . EMAIL } \n 
self . post ( self . password_reset_url , data = payload , status_code = 200 ) \n 
self . assertEqual ( len ( mail . outbox ) , mail_count + 1 ) \n 
url_kwargs = self . _generate_uid_and_token ( user ) \n 
url = reverse ( ) \n 
: self . NEW_PASS , \n 
: force_text ( url_kwargs [ ] ) , \n 
self . post ( url , data = data , status_code = 400 ) \n 
: url_kwargs [ ] \n 
self . post ( url , data = data , status_code = 200 ) \n 
"password" : self . NEW_PASS \n 
~~ def test_password_reset_with_email_in_different_case ( self ) : \n 
~~~ get_user_model ( ) . objects . create_user ( self . USERNAME , self . EMAIL . lower ( ) , self . PASS ) \n 
payload = { : self . EMAIL . upper ( ) } \n 
~~ def test_password_reset_with_invalid_email ( self ) : \n 
get_user_model ( ) . objects . create_user ( self . USERNAME , self . EMAIL , self . PASS ) \n 
payload = { : } \n 
self . assertEqual ( len ( mail . outbox ) , mail_count ) \n 
~~ def test_user_details ( self ) : \n 
self . get ( self . user_url , status_code = 200 ) \n 
self . patch ( self . user_url , data = self . BASIC_USER_DATA , status_code = 200 ) \n 
user = get_user_model ( ) . objects . get ( pk = user . pk ) \n 
self . assertEqual ( user . first_name , self . response . json [ ] ) \n 
self . assertEqual ( user . last_name , self . response . json [ ] ) \n 
self . assertEqual ( user . email , self . response . json [ ] ) \n 
def test_user_details_using_jwt ( self ) : \n 
~~ def test_registration ( self ) : \n 
~~~ user_count = get_user_model ( ) . objects . all ( ) . count ( ) \n 
self . post ( self . register_url , data = { } , status_code = 400 ) \n 
result = self . post ( self . register_url , data = self . REGISTRATION_DATA , status_code = 201 ) \n 
self . assertIn ( , result . data ) \n 
self . assertEqual ( get_user_model ( ) . objects . all ( ) . count ( ) , user_count + 1 ) \n 
new_user = get_user_model ( ) . objects . latest ( ) \n 
self . assertEqual ( new_user . username , self . REGISTRATION_DATA [ ] ) \n 
self . _login ( ) \n 
self . _logout ( ) \n 
def test_registration_with_jwt ( self ) : \n 
~~ def test_registration_with_invalid_password ( self ) : \n 
~~~ data = self . REGISTRATION_DATA . copy ( ) \n 
data [ ] = \n 
self . post ( self . register_url , data = data , status_code = 400 ) \n 
~~ @ override_settings ( \n 
ACCOUNT_EMAIL_VERIFICATION = , \n 
ACCOUNT_EMAIL_REQUIRED = True \n 
def test_registration_with_email_verification ( self ) : \n 
self . register_url , \n 
data = { } , \n 
status_code = status . HTTP_400_BAD_REQUEST \n 
result = self . post ( \n 
data = self . REGISTRATION_DATA_WITH_EMAIL , \n 
status_code = status . HTTP_201_CREATED \n 
self . assertNotIn ( , result . data ) \n 
self . login_url , \n 
data = payload , \n 
status = status . HTTP_400_BAD_REQUEST \n 
email_confirmation = new_user . emailaddress_set . get ( email = self . EMAIL ) . emailconfirmation_set . order_by ( ) [ 0 ] \n 
self . veirfy_email_url , \n 
data = { "key" : email_confirmation . key } , \n 
status_code = status . HTTP_200_OK \n 
~~ @ override_settings ( ACCOUNT_LOGOUT_ON_GET = True ) \n 
def test_logout_on_get ( self ) : \n 
self . get ( self . logout_url , status = status . HTTP_200_OK ) \n 
~~ @ override_settings ( ACCOUNT_LOGOUT_ON_GET = False ) \n 
def test_logout_on_post_only ( self ) : \n 
self . post ( self . login_url , data = payload , status_code = status . HTTP_200_OK ) \n 
self . get ( self . logout_url , status_code = status . HTTP_405_METHOD_NOT_ALLOWED ) \n 
os . environ . setdefault ( "DJANGO_SETTINGS_MODULE" , "examples.settings" ) \n 
from django . core . wsgi import get_wsgi_application \n 
application = get_wsgi_application ( ) \n 
from argparse import RawDescriptionHelpFormatter \n 
from gearbox . command import Command \n 
from gearbox . template import GearBoxTemplate \n 
from gearbox . utils . plugins import find_egg_info_dir \n 
class ScaffoldCommand ( Command ) : \n 
~~~ def get_description ( self ) : \n 
~~ def get_parser ( self , prog_name ) : \n 
~~~ parser = super ( ScaffoldCommand , self ) . get_parser ( prog_name ) \n 
parser . formatter_class = RawDescriptionHelpFormatter \n 
parser . add_argument ( , , \n 
~~ def _lookup ( self , template , where ) : \n 
~~~ template_filename = None \n 
for root , __ , files in os . walk ( where ) : \n 
~~~ for f in files : \n 
~~~ fname , fext = os . path . splitext ( f ) \n 
if fext == and os . path . splitext ( fname ) [ 0 ] == template : \n 
~~~ template_filename = os . path . join ( root , f ) \n 
~~ ~~ ~~ return template_filename \n 
~~ def _find_toplevel_packages ( self ) : \n 
~~~ egg_info_dir = find_egg_info_dir ( os . getcwd ( ) ) \n 
if egg_info_dir is None : \n 
~~~ print ( ) return [ ] \n 
~~ modules = os . path . join ( egg_info_dir , ) \n 
~~~ modules = open ( modules ) . readlines ( ) \n 
return [ ] \n 
~~ modules = map ( lambda x : x . strip ( ) . replace ( os . sep , ) , modules ) \n 
return list ( modules ) \n 
~~ def take_action ( self , opts ) : \n 
~~~ for template in opts . scaffold_name : \n 
template_relpath = None \n 
if template . endswith ( ) : \n 
~~~ template_filename = template \n 
template_relpath = \n 
~~~ template_filename = self . _lookup ( template , ) \n 
if template_filename : \n 
~~~ template_relpath = os . path . dirname ( template_filename ) \n 
~~ if template_filename is None and opts . lookup : \n 
~~~ template_filename = self . _lookup ( template , opts . lookup ) \n 
~~~ template_relpath = os . path . relpath ( os . path . dirname ( template_filename ) , opts . \n 
~~ ~~ ~~ if not template_filename or not os . path . exists ( template_filename ) : \n 
~~~ print ( % ( template ) ) \n 
~~ print ( % ( template_filename , opts . target ) ) \n 
template_with_ext , __ = os . path . splitext ( template_filename ) \n 
__ , output_ext = os . path . splitext ( template_with_ext ) \n 
output_dir = opts . path \n 
if not output_dir : \n 
~~~ output_dir = template_relpath \n 
~~ if opts . subdir : \n 
~~~ output_dir = os . path . join ( output_dir , opts . subdir ) \n 
~~ if not os . path . exists ( output_dir ) : \n 
~~~ os . makedirs ( output_dir ) \n 
if opts . subdir : \n 
~~~ package_init = os . path . join ( output_dir , ) \n 
if not os . path . exists ( package_init ) : \n 
~~~ with open ( package_init , ) as pif : \n 
~~~ pif . write ( ) \n 
~~ ~~ ~~ ~~ output_path = os . path . join ( output_dir , opts . target ) + output_ext \n 
print ( % output_path ) \n 
with open ( template_filename , ) as tf : \n 
~~~ subdir_as_package = opts . subdir . replace ( os . sep , ) if opts . subdir else \n 
text = GearBoxTemplate ( ) . template_renderer ( tf . read ( ) , { \n 
: opts . target , \n 
: opts . subdir , \n 
: subdir_as_package , \n 
: + subdir_as_package , \n 
: self . _find_toplevel_packages ( ) \n 
~~ except NameError as e : \n 
~~~ print ( % e ) \n 
~~ with open ( output_path , ) as of : \n 
~~~ of . write ( text ) \n 
~~ ~~ ~~ ~~ ~~ import unittest \n 
from models . appointment import Appointment \n 
class BaseTest ( unittest . TestCase ) : \n 
~~~ from reminders import app , db \n 
self . app = app \n 
self . db = db \n 
self . celery = app . celery ( ) \n 
self . test_client = app . flask_app . test_client ( ) \n 
self . app . flask_app . config [ ] = False \n 
~~~ self . db . session . query ( Appointment ) . delete ( ) \n 
self . db . session . commit ( ) \n 
self . celery . control . purge ( ) \n 
self . celery . conf . CELERY_ALWAYS_EAGER = False \n 
~~ ~~ \n 
from docpie import docpie \n 
~~~ print ( docpie ( __doc__ , name = ) ) \n 
~~ import zmq \n 
from collections import defaultdict \n 
from cloudasr import Poller \n 
from cloudasr . messages import HeartbeatMessage \n 
from cloudasr . messages . helpers import * \n 
def create_master ( worker_address , frontend_address , monitor_address ) : \n 
~~~ poller = create_poller ( worker_address , frontend_address ) \n 
context = zmq . Context ( ) \n 
monitor = context . socket ( zmq . PUSH ) \n 
monitor . connect ( monitor_address ) \n 
run_forever = lambda : True \n 
return Master ( poller , monitor , run_forever ) \n 
~~ def create_poller ( worker_address , frontend_address ) : \n 
~~~ context = zmq . Context ( ) \n 
worker_socket = context . socket ( zmq . PULL ) \n 
worker_socket . bind ( worker_address ) \n 
frontend_socket = context . socket ( zmq . REP ) \n 
frontend_socket . bind ( frontend_address ) \n 
sockets = { \n 
"worker" : { "socket" : worker_socket , "receive" : worker_socket . recv , "send" : worker_socket . send_json "frontend" : { "socket" : frontend_socket , "receive" : frontend_socket . recv , "send" : frontend_socket } \n 
time_func = time . time \n 
return Poller ( sockets , time_func ) \n 
~~ class Master : \n 
~~~ def __init__ ( self , poller , monitor , should_continue ) : \n 
~~~ self . poller = poller \n 
self . should_continue = should_continue \n 
self . workers = WorkerPool ( monitor ) \n 
self . time = 0 \n 
~~~ while self . should_continue ( ) : \n 
~~~ messages , self . time = self . poller . poll ( ) \n 
if "worker" in messages : \n 
~~~ self . handle_worker_request ( messages [ "worker" ] ) \n 
~~ if "frontend" in messages : \n 
~~~ self . handle_fronted_request ( messages [ "frontend" ] ) \n 
~~ ~~ ~~ def handle_fronted_request ( self , message ) : \n 
~~~ request = parseWorkerRequestMessage ( message ) \n 
model = request . model \n 
worker = self . workers . get_worker ( model , self . time ) \n 
message = createMasterResponseMessage ( "SUCCESS" , worker ) \n 
self . poller . send ( "frontend" , message . SerializeToString ( ) ) \n 
~~ except NoWorkerAvailableException : \n 
~~~ message = createMasterResponseMessage ( "ERROR" ) \n 
~~ ~~ def handle_worker_request ( self , message ) : \n 
~~~ statuses = { \n 
HeartbeatMessage . STARTED : "STARTED" , \n 
HeartbeatMessage . WAITING : "WAITING" , \n 
HeartbeatMessage . WORKING : "WORKING" , \n 
HeartbeatMessage . FINISHED : "FINISHED" \n 
heartbeat = parseHeartbeatMessage ( message ) \n 
address = heartbeat . address \n 
model = heartbeat . model \n 
status = statuses [ heartbeat . status ] \n 
self . workers . add_worker ( model , address , status , self . time ) \n 
~~ ~~ class WorkerPool : \n 
~~~ def __init__ ( self , monitor ) : \n 
~~~ self . workers_status = defaultdict ( lambda : { "status" : "STARTED" , "last_heartbeat" : 0 , "waiting_for_first_chunk_secs" self . available_workers = defaultdict ( list ) \n 
self . monitor = monitor \n 
~~ def get_worker ( self , model , time ) : \n 
~~~ worker = self . find_available_worker ( model , time ) \n 
if worker is None : \n 
~~~ raise NoWorkerAvailableException ( ) \n 
~~ self . update_worker_status ( model , worker , "WORKING" , time ) \n 
return worker \n 
~~ def find_available_worker ( self , model , time ) : \n 
~~~ while len ( self . available_workers [ model ] ) > 0 : \n 
~~~ worker = self . available_workers [ model ] . pop ( 0 ) \n 
if self . is_worker_available ( worker , time ) : \n 
~~~ return worker \n 
~~ def is_worker_available ( self , worker , time ) : \n 
~~~ status = self . workers_status [ worker ] \n 
return status [ "status" ] and status [ "last_heartbeat" ] > time - 10 \n 
~~ def add_worker ( self , model , address , status , time ) : \n 
~~~ worker_status = self . workers_status [ address ] [ "status" ] \n 
if worker_status == "WORKING" : \n 
~~~ if status == "FINISHED" or status == "STARTED" : \n 
~~~ self . available_workers [ model ] . append ( address ) \n 
self . update_worker_status ( model , address , "WAITING" , time ) \n 
~~ if status == "WORKING" : \n 
~~~ self . update_worker_status ( model , address , "WORKING" , time ) \n 
~~ if status == "WAITING" : \n 
~~~ self . workers_status [ address ] [ "waiting_for_first_chunk_secs" ] += 1 \n 
if self . workers_status [ address ] [ "waiting_for_first_chunk_secs" ] == 10 : \n 
~~ ~~ ~~ elif worker_status == "STARTED" : \n 
self . update_worker_status ( model , address , "STARTED" , time ) \n 
~~ elif worker_status == "WAITING" : \n 
~~~ self . update_worker_status ( model , address , "WAITING" , time ) \n 
~~ ~~ def update_worker_status ( self , model , worker , status , time ) : \n 
~~~ self . workers_status [ worker ] = { \n 
"status" : "WAITING" if status == "STARTED" else status , \n 
"last_heartbeat" : time , \n 
"waiting_for_first_chunk_secs" : 0 \n 
worker_status = createWorkerStatusMessage ( worker , model , status , int ( time ) ) \n 
self . monitor . send ( worker_status . SerializeToString ( ) ) \n 
~~ ~~ class NoWorkerAvailableException ( Exception ) : \n 
from lib import create_worker \n 
worker = create_worker ( os . environ [ ] , os . environ [ ] , os . environ [ ] , os . environ [ worker . run ( ) \n 
import dshell \n 
import util \n 
import netflowout \n 
class DshellDecoder ( dshell . TCPDecoder ) : \n 
self . sessions = { } \n 
self . alerts = False \n 
self . file = None \n 
dshell . TCPDecoder . __init__ ( self , \n 
optiondict = { \n 
: { : , : } , : { : , : : { : , : : { : , : : { : } } ) \n 
self . __decoder = dshell . IPDecoder ( ) \n 
self . out = netflowout . NetflowOutput ( ) \n 
self . chainable = True \n 
~~ def decode ( self , * args ) : \n 
~~~ if len ( args ) is 3 : \n 
~~~ ts , pktdata = args \n 
pktlen = len ( pktdata ) \n 
dshell . TCPDecoder . decode ( self , pktlen , pktdata , ts ) \n 
self . __decoder . decode ( pktlen , pktdata , ts , raw = pktdata ) \n 
~~ def __callback ( self , addr , pkt , ts , raw = None , ** kw ) : \n 
~~~ self . subDecoder . decode ( len ( raw ) , str ( raw ) , ts ) \n 
~~~ self . dump ( raw , ts ) \n 
~~ ~~ ~~ def connectionInitHandler ( self , conn ) : \n 
m = self . __countryTest ( conn ) \n 
~~~ self . sessions [ conn . addr ] = m \n 
~~ ~~ def __countryTest ( self , conn ) : \n 
~~~ if self . code == None or not len ( self . code ) : \n 
~~ if self . neither and conn . clientcountrycode != self . code and conn . servercountrycode != self . code ~~~ return + self . code \n 
~~ if self . both and conn . clientcountrycode == self . code and conn . servercountrycode == self . code ~~~ return + self . code \n 
~~ if self . notboth and ( conn . clientcountrycode != self . code or conn . servercountrycode != self . code ~~~ return + self . code \n 
~~ if conn . clientcountrycode == self . code : \n 
~~~ return + self . code \n 
~~ if conn . servercountrycode == self . code : \n 
~~ def connectionHandler ( self , conn ) : \n 
~~~ if conn . addr in self . sessions and self . alerts : \n 
~~~ self . alert ( self . sessions [ conn . addr ] , ** conn . info ( ) ) \n 
~~ ~~ def connectionCloseHandler ( self , conn ) : \n 
~~~ if conn . addr in self . sessions : \n 
~~~ del self . sessions [ conn . addr ] \n 
~~ ~~ ~~ dObj = DshellDecoder ( ) \n 
import output \n 
self . optiondict = { } \n 
self . filter = \n 
self . __super__ ( ) . __init__ ( ** kwargs ) \n 
~~ def packetHandler ( self , udp , data ) : \n 
~~ def connectionInitHandler ( self , conn ) : \n 
~~ def blobHandler ( self , conn , blob ) : \n 
~~ def connectionCloseHandler ( self , conn ) : \n 
~~ ~~ dObj = DshellDecoder ( ) \n 
import copy \n 
from mininext . mount import MountProperties \n 
from mininext . util import ParamContainer \n 
class Service ( ParamContainer ) : \n 
def __init__ ( self , name , ** kwargs ) : \n 
ParamContainer . __init__ ( self , name = name , ** kwargs ) \n 
~~ def nodeIsSubscribed ( self , node ) : \n 
return self . hasNodeParams ( node ) \n 
~~ def errIfNodeNotSubscribed ( self , node ) : \n 
if self . nodeIsSubscribed ( node ) is False : \n 
% ( self . name , node ) ) \n 
~~ ~~ def setupNode ( self , node , nodeServiceParams , includeDefaults = True ) : \n 
if self . nodeIsSubscribed ( node ) : \n 
~~ self . verifyNodeMeetsServiceRequirements ( node ) \n 
self . storeNodeParams ( node , nodeServiceParams , includeDefaults ) \n 
self . setupNodeMounts ( node ) \n 
self . setupNodeForService ( node ) \n 
~~ def verifyNodeMeetsServiceRequirements ( self , node ) : \n 
~~ def setupNodeForService ( self , node ) : \n 
~~ def setupNodeMounts ( self , node ) : \n 
nodeServiceMounts = self . getMountsForNode ( node ) \n 
for mountPoint in nodeServiceMounts : \n 
~~~ if mountPoint . source is not None and mountPoint . source . path is not None and mountPoint . target is not None : \n 
~~~ node . setupMountPoint ( mountPoint ) \n 
~~ ~~ ~~ def autoStart ( self , node ) : \n 
if self . getNodeParam ( node , , defaultValue = None ) is True : \n 
~~~ return self . start ( node ) \n 
~~ def autoStop ( self , node ) : \n 
~~~ return self . stop ( node ) \n 
~~ def start ( self , node ) : \n 
self . errIfNodeNotSubscribed ( node ) \n 
startCmd = self . getNodeParam ( node , , defaultValue = None ) \n 
if startCmd is None : \n 
% ( self . name ) ) \n 
~~ _ , err , ret = node . pexec ( startCmd ) \n 
if ret != 0 and self . getNodeParam ( \n 
) is True : \n 
~~ return { : err , : ret } \n 
~~ def stop ( self , node ) : \n 
stopCmd = self . getNodeParam ( node , , defaultValue = None ) \n 
if stopCmd is None : \n 
~~ _ , err , ret = node . pexec ( stopCmd ) \n 
return { : err , : ret } \n 
~~ def getDefaultGlobalParams ( self ) : \n 
~~ def getDefaultGlobalMounts ( self ) : \n 
mounts = [ ] \n 
mountConfigPairs = { } \n 
return mounts , mountConfigPairs \n 
~~ def getMountsForNode ( self , node ) : \n 
if self . hasNodeParam ( node , ) is True : \n 
~~~ return self . getNodeParam ( node , ) \n 
~~ nodeMounts = [ ] \n 
_ , mountConfigPairs = self . getDefaultGlobalMounts ( ) \n 
for mountName , mountProperties in mountConfigPairs . iteritems ( ) : \n 
~~~ mountProperties = copy . deepcopy ( mountProperties ) \n 
nodeMountOptions = self . getNodeParam ( node , mountName , \n 
defaultValue = None ) \n 
if nodeMountOptions is None : \n 
~~ if isinstance ( nodeMountOptions , basestring ) : \n 
~~~ mountProperties . source . path = nodeMountOptions \n 
~~ elif isinstance ( nodeMountOptions , MountProperties ) : \n 
~~~ mountProperties = nodeMountOptions \n 
~~ nodeMounts . append ( mountProperties ) \n 
~~ return nodeMounts \n 
~~~ return % ( self . __class__ . __name__ , self . name ) \n 
~~~ return self . name \n 
return hash ( self . name ) \n 
~~ ~~ from numpy . random . mtrand import uniform \n 
import netCDF4 \n 
from timeit import Timer \n 
import os , sys \n 
n1dim = 30 \n 
n2dim = 15 \n 
n3dim = 73 \n 
n4dim = 144 \n 
ntrials = 10 \n 
sys . stdout . write ( % ( n1dim , n2dim , n3dim , n4dim array = uniform ( size = ( n1dim , n2dim , n3dim , n4dim ) ) \n 
def write_netcdf ( filename , zlib = False , least_significant_digit = None , format = ) : \n 
~~~ file = netCDF4 . Dataset ( filename , , format = format ) \n 
file . createDimension ( , n1dim ) \n 
file . createDimension ( , n2dim ) \n 
file . createDimension ( , n3dim ) \n 
file . createDimension ( , n4dim ) \n 
foo = file . createVariable ( , , ( , , , ) , zlib = zlib , least_significant_digit = least_significant_digit foo [ : ] = array \n 
file . close ( ) \n 
~~ def read_netcdf ( filename ) : \n 
~~~ file = netCDF4 . Dataset ( filename ) \n 
data = file . variables [ ] [ : ] \n 
~~ for format in [ , , , ] : \n 
~~~ sys . stdout . write ( % format ) \n 
sys . stdout . write ( % repr ( sum ( t . repeat ( ntrials , 1 ) ) / ntrials ) ) \n 
sys . stdout . write ( % \n 
repr ( sum ( t . repeat ( ntrials , 1 ) ) / ntrials ) ) \n 
class test_filepath ( unittest . TestCase ) : \n 
~~~ self . netcdf_file = os . path . join ( os . getcwd ( ) , "netcdf_dummy_file.nc" ) \n 
self . nc = netCDF4 . Dataset ( self . netcdf_file ) \n 
~~ def test_filepath ( self ) : \n 
~~~ assert self . nc . filepath ( ) == str ( self . netcdf_file ) \n 
~~ from . _version import get_versions \n 
__version__ = get_versions ( ) [ ] \n 
del get_versions \n 
import xml . etree . ElementTree as ET \n 
from siphon . ncss_dataset import NCSSDataset , _Types \n 
from siphon . testing import get_recorder \n 
from siphon . http_util import urlopen \n 
log = logging . getLogger ( "siphon.ncss_dataset" ) \n 
log . setLevel ( logging . WARNING ) \n 
log . addHandler ( logging . StreamHandler ( ) ) \n 
recorder = get_recorder ( __file__ ) \n 
class TestSimpleTypes ( object ) : \n 
def setup_class ( cls ) : \n 
~~~ cls . types = _Types ( ) \n 
~~ def test_attribute_1 ( self ) : \n 
element = ET . fromstring ( xml ) \n 
actual = self . types . handle_attribute ( element ) \n 
assert expected == actual \n 
~~ def test_attribute_2 ( self ) : \n 
expected = { "missing_value" : [ float ( "NaN" ) ] } \n 
assert expected . keys ( ) == actual . keys ( ) \n 
assert ( math . isnan ( actual [ "missing_value" ] [ 0 ] ) ) \n 
assert ( math . isnan ( expected [ "missing_value" ] [ 0 ] ) ) \n 
~~ def test_attribute_3 ( self ) : \n 
expected = { "missing_value" : [ float ( - 999 ) ] } \n 
~~ def test_attribute_4 ( self ) : \n 
expected = { "missing_value" : [ - 999 ] } \n 
~~ def test_value_1 ( self ) : \n 
xml = \n 
expected = { "values" : [ "2.0" ] } \n 
actual = self . types . handle_values ( element ) \n 
~~ def test_value_2 ( self ) : \n 
expected = { "values" : [ "50000.0" , "70000.0" , "85000.0" ] } \n 
~~ def test_value_3 ( self ) : \n 
expected = { "values" : [ 50000.0 , 70000.0 , 85000.0 ] } \n 
actual = self . types . handle_values ( element , value_type = "float" ) \n 
~~ def test_value_4 ( self ) : \n 
expected = { "values" : [ 50000 , 70000 , 85000 ] } \n 
actual = self . types . handle_values ( element , value_type = "int" ) \n 
~~ def test_projection_box ( self ) : \n 
expected = { "projectionBox" : { "minx" : - 2959.1533203125 , \n 
"maxx" : 2932.8466796875 , \n 
"miny" : - 1827.929443359375 , \n 
"maxy" : 1808.070556640625 } } \n 
actual = self . types . handle_projectionBox ( element ) \n 
~~ def test_axis_ref ( self ) : \n 
expected = "time1" \n 
actual = self . types . handle_axisRef ( element ) \n 
~~ def test_coord_trans_ref ( self ) : \n 
expected = { "coordTransRef" : "LambertConformal_Projection" } \n 
actual = self . types . handle_coordTransRef ( element ) \n 
~~ def test_grid ( self ) : \n 
expected = { "name" : "Temperature_isobaric" , \n 
"attributes" : { "units" : "K" , \n 
"missing_value" : [ - 999.9 ] , \n 
"Grib2_Parameter" : [ 0 , 0 , 0 ] } } \n 
actual = self . types . handle_grid ( element ) \n 
assert expected [ "attributes" ] == actual [ "attributes" ] \n 
assert expected . pop ( "attributes" , None ) == actual . pop ( "attributes" , None ) \n 
~~ def test_parameter ( self ) : \n 
expected = { "earth_radius" : "6371229.0" } \n 
actual = self . types . handle_parameter ( element ) \n 
~~ def test_feature_type ( self ) : \n 
expected = { "type" : "station" , \n 
"url" : "/thredds/ncss/nws/metar/ncdecoded/" \n 
"Metar_Station_Data_fc.cdmr" } \n 
actual = self . types . handle_featureDataset ( element ) \n 
~~ def test_variable ( self ) : \n 
expected = { "name" : "precipitation_amount_hourly" , \n 
"type" : "float" , \n 
"standard_name" : "precipitation_amount" , \n 
"_FillValue" : [ - 99999.0 ] , \n 
actual = self . types . handle_variable ( element ) \n 
~~ ~~ def test_dataset_elements_axis ( ) : \n 
actual = NCSSDataset ( element ) . axes \n 
assert actual \n 
assert len ( actual ) == 1 \n 
assert actual [ "height_above_ground" ] \n 
assert len ( actual [ "height_above_ground" ] ) == 4 \n 
assert actual [ "height_above_ground" ] [ "attributes" ] \n 
assert len ( actual [ "height_above_ground" ] [ "attributes" ] ) == 8 \n 
~~ def test_dataset_elements_grid_set ( ) : \n 
actual = NCSSDataset ( element ) . gridsets \n 
assert gs [ "axisRef" ] \n 
assert len ( gs [ "axisRef" ] ) == 4 \n 
assert gs [ "coordTransRef" ] \n 
assert gs [ "projectionBox" ] \n 
assert len ( gs [ "projectionBox" ] ) == 4 \n 
assert gs [ "grid" ] \n 
assert len ( gs [ "grid" ] ) == 2 \n 
for grid in gs [ "grid" ] : \n 
~~~ assert len ( gs [ "grid" ] [ grid ] ) == 4 \n 
assert gs [ "grid" ] [ grid ] [ "desc" ] \n 
assert gs [ "grid" ] [ grid ] [ "shape" ] \n 
assert gs [ "grid" ] [ grid ] [ "type" ] \n 
assert gs [ "grid" ] [ grid ] [ "type" ] == "float" \n 
assert len ( gs [ "grid" ] [ grid ] [ "attributes" ] ) == 13 \n 
~~ ~~ def test_dataset_elements_coord_transform_valid ( ) : \n 
actual = NCSSDataset ( element ) . coordinate_transforms \n 
assert actual [ "LambertConformal_Projection" ] \n 
assert len ( actual [ "LambertConformal_Projection" ] ) == 2 \n 
assert actual [ "LambertConformal_Projection" ] [ "transformType" ] == "Projection" \n 
parameters = actual [ "LambertConformal_Projection" ] [ "parameters" ] \n 
assert len ( parameters ) == 5 \n 
expected = { "grid_mapping_name" : "lambert_conformal_conic" , \n 
"latitude_of_projection_origin" : "40.0" , \n 
"longitude_of_central_meridian" : "262.0" , \n 
"standard_parallel" : "40.0" , \n 
"earth_radius" : "6371229.0" } \n 
assert parameters == expected \n 
~~ def test_dataset_elements_lat_lon_box ( ) : \n 
expected = { "west" : - 140.1465 , \n 
"east" : - 56.1753 , \n 
"south" : 19.8791 , \n 
"north" : 49.9041 } \n 
actual = NCSSDataset ( element ) . lat_lon_box \n 
~~ def test_dataset_elements_time_span ( ) : \n 
expected = { "begin" : "2015-06-19T12:00:00Z" , \n 
"end" : "2015-06-23T18:00:00Z" } \n 
actual = NCSSDataset ( element ) . time_span \n 
~~ def test_dataset_elements_accept_list ( ) : \n 
expected = { "GridAsPoint" : [ "xml" , "xml_file" , \n 
"csv" , "csv_file" , \n 
"netcdf" , "netcdf4" ] , \n 
"Grid" : [ "netcdf" , "netcdf4" ] } \n 
actual = NCSSDataset ( element ) . accept_list \n 
~~ def test_dataset_elements_station_accept_list ( ) : \n 
expected = { "PointFeatureCollection" : [ "csv" , "text/csv" , \n 
"xml" , "text/xml" , \n 
"waterml2" , "netcdf" , "netcdf4" ] } \n 
~~ @ recorder . use_cassette ( ) \n 
def test_dataset_elements_full_ncss_station ( ) : \n 
url = ( \n 
element = ET . fromstring ( urlopen ( url ) . read ( ) ) \n 
parsed = NCSSDataset ( element ) \n 
assert parsed \n 
def test_dataset_elements_full_ncss_grid ( ) : \n 
from Cython . Build import cythonize \n 
PACKAGES = [ , , \n 
name = "Polyglot" , \n 
version = "0.0.1" , \n 
platforms = "any" , \n 
ext_modules = cythonize ( ) , \n 
packages = PACKAGES \n 
from requests . auth import AuthBase \n 
from ttrss . exceptions import raise_on_error \n 
class TTRAuth ( AuthBase ) : \n 
~~~ def __init__ ( self , user , password , http_auth ) : \n 
~~~ self . user = user \n 
self . http_auth = http_auth \n 
self . sid = None \n 
~~ def response_hook ( self , r , ** kwargs ) : \n 
~~~ j = json . loads ( r . text ) \n 
if int ( j [ ] ) == 0 : \n 
~~~ return r \n 
~~ self . sid = self . _get_sid ( r . request . url ) \n 
r . request . deregister_hook ( , self . response_hook ) \n 
j = json . loads ( r . request . body ) \n 
j . update ( { : self . sid } ) \n 
req = requests . Request ( , r . request . url , auth = self . http_auth ) \n 
req . data = json . dumps ( j ) \n 
_r = requests . Session ( ) . send ( req . prepare ( ) ) \n 
raise_on_error ( _r ) \n 
return _r \n 
~~ def __call__ ( self , r ) : \n 
~~~ r . register_hook ( , self . response_hook ) \n 
data = json . loads ( r . body ) \n 
if not in data : \n 
~~~ if self . sid is None : \n 
~~~ self . sid = self . _get_sid ( r . url ) \n 
~~ data . update ( { : self . sid } ) \n 
req = requests . Request ( , r . url , auth = self . http_auth ) \n 
req . data = json . dumps ( data ) \n 
return req . prepare ( ) \n 
~~~ self . sid = data [ ] \n 
~~ def _get_sid ( self , url ) : \n 
~~~ res = requests . post ( url , auth = self . http_auth , data = json . dumps ( { \n 
: self . user , \n 
: self . password \n 
raise_on_error ( res ) \n 
j = json . loads ( res . text ) \n 
return j [ ] [ ] \n 
from __future__ import division , print_function , unicode_literals \n 
from reprounzip . utils import join_root \n 
from reprounzip . unpackers . common . misc import UsageError , COMPAT_OK , COMPAT_NO , COMPAT_MAYBE , composite_action , target_must_exist , unique_names , make_unique_name , shell_escape , load_config , busybox_url , sudo_url , FileUploader , FileDownloader , get_runs , add_environment_options , fixup_environment , interruptible_call , metadata_read , metadata_write , metadata_initial_iofiles , metadata_update_run \n 
from reprounzip . unpackers . common . packages import THIS_DISTRIBUTION , PKG_NOT_INSTALLED , CantFindInstaller , select_installer \n 
__all__ = [ , , , \n 
from percept . utils . input import import_from_string \n 
from percept . conf . base import settings \n 
import pickle \n 
class BaseStore ( object ) : \n 
~~~ self . data_path = settings . DATA_PATH \n 
~~ def save ( self , obj , id_code ) : \n 
filestream = open ( . format ( self . data_path , id_code ) , ) \n 
pickle . dump ( obj , filestream ) \n 
filestream . close ( ) \n 
~~ def load ( self , id_code ) : \n 
workflow = pickle . load ( filestream ) \n 
return workflow \n 
~~ ~~ class FileStore ( BaseStore ) : \n 
from . agent import NeuralAgent \n 
class ALEAgent ( NeuralAgent ) : \n 
~~~ def _chooseAction ( self ) : \n 
~~~ if self . _mode != - 1 : \n 
~~~ if self . _randomState . rand ( ) < 0.05 : \n 
~~~ action = self . _randomState . randint ( 0 , self . _environment . nActions ( ) ) \n 
V = 0 \n 
~~~ action , V = self . bestAction ( ) \n 
~~~ if self . _dataSet . nElems ( ) > self . _replayMemoryStartSize : \n 
~~~ if self . _randomState . rand ( ) < self . _epsilon : \n 
~~ ~~ for c in self . _controllers : c . OnActionChosen ( self , action ) \n 
return action , V \n 
import deer \n 
NAME = \n 
AUTHOR_EMAIL = "v.francois@ulg.ac.be" \n 
URL = \n 
DESCRIPTION = \n 
~~~ LONG_DESCRIPTION = f . read ( ) \n 
~~ CLASSIFIERS = [ \n 
~~~ setup ( name = NAME , \n 
author = AUTHOR , \n 
url = URL , \n 
description = DESCRIPTION , \n 
long_description = LONG_DESCRIPTION , \n 
classifiers = CLASSIFIERS , \n 
from sqlalchemy . engine import create_engine \n 
from sqlalchemy . orm . session import sessionmaker \n 
from geeknote import storage \n 
def hacked_init ( self ) : \n 
engine = create_engine ( , echo = False ) \n 
storage . Base . metadata . create_all ( engine ) \n 
Session = sessionmaker ( bind = engine ) \n 
self . session = Session ( ) \n 
~~ class storageTest ( unittest . TestCase ) : \n 
~~~ stor = storage . Storage \n 
stor . __init__ = hacked_init \n 
self . storage = stor ( ) \n 
self . otoken = \n 
self . userinfo = { : } \n 
self . tags = { : 1 , : 2 , : } \n 
self . notebooks = { : } \n 
self . storage . createUser ( self . otoken , \n 
self . userinfo ) \n 
~~ def test_create_user_without_token_fail ( self ) : \n 
~~~ self . assertFalse ( self . storage . createUser ( None , self . userinfo ) ) \n 
~~ def test_create_user_without_info_fail ( self ) : \n 
~~~ self . assertFalse ( self . storage . createUser ( self . otoken , None ) ) \n 
~~ def test_remove_user_success ( self ) : \n 
~~~ self . assertTrue ( self . storage . removeUser ( ) ) \n 
~~ def test_get_user_token_success ( self ) : \n 
~~~ self . assertEquals ( self . storage . getUserToken ( ) , self . otoken ) \n 
~~ def test_get_user_info_success ( self ) : \n 
~~~ self . assertEquals ( self . storage . getUserInfo ( ) , self . userinfo ) \n 
~~ def test_get_user_props_success ( self ) : \n 
~~~ props = [ { : } , \n 
{ : { : } } ] \n 
self . assertEquals ( self . storage . getUserprops ( ) , props ) \n 
~~ def test_get_user_props_exists_success ( self ) : \n 
~~~ self . assertEquals ( self . storage . getUserprop ( ) , \n 
~~ def test_get_user_prop_not_exists ( self ) : \n 
~~~ self . assertFalse ( self . storage . getUserprop ( ) ) \n 
~~ def test_set_new_user_prop ( self ) : \n 
self . assertTrue ( self . storage . setUserprop ( , ) ) \n 
self . assertEquals ( self . storage . getUserprop ( ) , ) \n 
~~ def test_set_exists_user_prop ( self ) : \n 
~~~ newmail = { : } \n 
self . assertEquals ( self . storage . getUserprop ( ) , self . userinfo ) \n 
self . assertTrue ( self . storage . setUserprop ( , newmail ) , newmail ) \n 
self . assertEquals ( self . storage . getUserprop ( ) , newmail ) \n 
~~ def test_get_empty_settings ( self ) : \n 
~~~ self . assertEquals ( self . storage . getSettings ( ) , { } ) \n 
~~ def test_set_settings_success ( self ) : \n 
~~~ self . storage . setSettings ( { : } ) \n 
self . assertEquals ( self . storage . getSettings ( ) , \n 
{ : u"S\'vim\'\\np0\\n." } ) \n 
~~ def test_set_setting_error_type_fail ( self ) : \n 
~~~ self . assertFalse ( self . storage . setSettings ( ) ) \n 
~~ def test_set_setting_none_value_fail ( self ) : \n 
~~~ self . assertFalse ( self . storage . setSettings ( { : None } ) ) \n 
~~ def test_update_settings_fail ( self ) : \n 
self . assertTrue ( self . storage . setSettings ( { : } ) ) \n 
{ : u"S\'nano\'\\np0\\n." } ) \n 
~~ def test_get_setting_exist_success ( self ) : \n 
editor = self . storage . getSetting ( ) \n 
self . assertEquals ( pickle . loads ( editor ) , ) \n 
~~ def test_set_setting_true ( self ) : \n 
~~~ editor = \n 
self . assertTrue ( self . storage . setSetting ( , editor ) ) \n 
self . assertEquals ( self . storage . getSetting ( ) , editor ) \n 
~~ def test_get_setting_not_exist_fail ( self ) : \n 
~~~ self . assertFalse ( self . storage . getSetting ( ) ) \n 
~~ def test_set_tags_success ( self ) : \n 
~~~ self . assertTrue ( self . storage . setTags ( self . tags ) ) \n 
~~ def test_set_tags_error_type_fail ( self ) : \n 
~~~ self . assertFalse ( self . storage . setTags ( ) ) \n 
~~ def test_set_tags_none_value_fail ( self ) : \n 
~~~ self . assertFalse ( self . storage . setTags ( { : None } ) ) \n 
~~ def test_get_tags_success ( self ) : \n 
~~~ tags = { : , : , : } \n 
self . assertTrue ( self . storage . setTags ( self . tags ) ) \n 
self . assertEquals ( self . storage . getTags ( ) , tags ) \n 
~~ def test_replace_tags_success ( self ) : \n 
self . tags [ ] = 3 \n 
~~ def test_set_notebooks_success ( self ) : \n 
~~~ self . assertEquals ( self . storage . getNotebooks ( ) , { } ) \n 
self . storage . setNotebooks ( self . notebooks ) \n 
self . assertEquals ( self . storage . getNotebooks ( ) , self . notebooks ) \n 
~~ def test_replace_notebooks_success ( self ) : \n 
~~~ newnotebooks = { : } \n 
self . storage . setNotebooks ( newnotebooks ) \n 
self . assertEquals ( self . storage . getNotebooks ( ) , newnotebooks ) \n 
~~ def test_get_empty_search_success ( self ) : \n 
~~~ self . assertFalse ( self . storage . getSearch ( ) ) \n 
~~ def test_get_search_exists_success ( self ) : \n 
~~~ query = \n 
self . assertTrue ( self . storage . setSearch ( query ) ) \n 
self . assertEquals ( self . storage . getSearch ( ) , query ) \n 
~~ def test_set_notebooks_error_type_fail ( self ) : \n 
~~~ self . assertFalse ( self . storage . setNotebooks ( ) ) \n 
~~ def test_set_notebooks_none_value_fail ( self ) : \n 
~~~ self . assertFalse ( self . storage . setNotebooks ( { : None } ) ) \n 
~~ def test_set_search_true ( self ) : \n 
~~~ self . assertTrue ( self . storage . setSearch ( ) ) \n 
~~ ~~ class modelsTest ( unittest . TestCase ) : \n 
~~~ def test_rept_userprop ( self ) : \n 
~~~ userprop = storage . Userprop ( key = , \n 
value = ) \n 
self . assertEquals ( userprop . __repr__ ( ) , \n 
"<Userprop(\'test\',\'value)>" ) \n 
~~ def test_repr_setting ( self ) : \n 
~~~ setting = storage . Setting ( key = , \n 
self . assertEquals ( setting . __repr__ ( ) , \n 
"<Setting(\'test\',\'value)>" ) \n 
~~ def test_repr_notebook ( self ) : \n 
~~~ notebook = storage . Notebook ( name = , \n 
guid = ) \n 
self . assertEquals ( notebook . __repr__ ( ) , \n 
"<Notebook(\'notebook\')>" ) \n 
~~ def test_repr_tag ( self ) : \n 
~~~ tag = storage . Tag ( tag = , \n 
self . assertEquals ( tag . __repr__ ( ) , "<Tag(\'testtag\')>" ) \n 
~~ def test_repr_search ( self ) : \n 
~~~ search = storage . Search ( search_obj = ) \n 
self . assertEquals ( search . __repr__ ( ) , \n 
"<Search(\'%s\')>" % search . timestamp ) \n 
import wdr \n 
from wdr . app import * #noqa \n 
from wdr . config import * #noqa \n 
from wdr . control import * #noqa \n 
from wdr . manifest import * #noqa \n 
from wdr . util import * #noqa \n 
from wdrtest . topology import topology \n 
AdminApp , AdminConfig , AdminControl , AdminTask , Help \n 
) = wdr . WsadminObjects ( ) . getObjects ( ) \n 
class AbstractConfigTest ( unittest . TestCase ) : \n 
~~~ def tearDown ( self ) : \n 
~~~ reset ( ) \n 
~~ def assertTrue ( self , value , msg = None ) : \n 
~~~ self . assertNotEqual ( 0 , value , msg ) \n 
~~ def assertFalse ( self , value , msg = None ) : \n 
~~~ self . assertEqual ( 0 , value , msg ) \n 
~~ def assertNone ( self , value , msg = None ) : \n 
~~~ if value is not None : \n 
~~~ raise AssertionError ( msg or % value ) \n 
~~ ~~ def assertNotNone ( self , value , msg = None ) : \n 
~~ ~~ ~~ class VariableSubstitutionTest ( unittest . TestCase ) : \n 
~~~ def testSimple ( self ) : \n 
~~~ self . assertEquals ( \n 
substituteVariables ( , { : } ) \n 
~~ def testWithMixedCase ( self ) : \n 
~~ def testWithDigits ( self ) : \n 
~~ def testWithUnderscores ( self ) : \n 
~~ def testWithUnderscorePrefix ( self ) : \n 
~~ def testWithUnderscoreOnly ( self ) : \n 
~~ def testWithNestedDictionary ( self ) : \n 
substituteVariables ( \n 
, { : { : } } \n 
~~ def testWithDoubleNestedDictionary ( self ) : \n 
~~ def testNone ( self ) : \n 
substituteVariables ( , { : None } ) \n 
substituteVariables ( , { : 123 } ) \n 
~~ def testList ( self ) : \n 
substituteVariables ( , { : [ 123 , 456 ] } ) \n 
~~ def testTuple ( self ) : \n 
substituteVariables ( , { : ( 123 , 456 ) } ) \n 
~~ def testCallable ( self ) : \n 
{ : lambda expression , variables : } \n 
~~ def testCallableReturningNone ( self ) : \n 
{ : lambda expression , variables : None } \n 
~~ def testWithExtraSpaces ( self ) : \n 
{ : { : } } \n 
~~ ~~ class VariableSubstitutionWithFilteringTest ( unittest . TestCase ) : \n 
~~~ def testUpper ( self ) : \n 
{ : , : string . upper } \n 
~~ def testListProcessing ( self ) : \n 
{ : [ , ] , : string . join } \n 
~~ def testListWithLambdaOnList ( self ) : \n 
[ \n 
[ , ] , \n 
lambda targetList : ( . join ( \n 
+ . join ( \n 
[ . join ( attList ) for attList in target ] \n 
for target in targetList \n 
~~ def testListWithLambdaOnDict ( self ) : \n 
lambda targetList : ( \n 
. join ( \n 
% t \n 
for t in filter ( \n 
lambda e : e . get ( ) , targetList \n 
lambda e : e . get ( ) and e . get ( ) , \n 
targetList \n 
~~ ~~ class BasicManifestImportTest ( AbstractConfigTest ) : \n 
~~~ def testUpdateStrings ( self ) : \n 
srv = getid1 ( \n 
% topology \n 
importConfigurationManifest ( \n 
, topology \n 
self . assertEquals ( srv . changeUserAfterStartup , ) \n 
self . assertEquals ( srv . changeGroupAfterStartup , ) \n 
~~ def testUpdateBooleans ( self ) : \n 
self . assertFalse ( srv . developmentMode ) \n 
self . assertTrue ( srv . parallelStartEnabled ) \n 
self . assertTrue ( srv . developmentMode ) \n 
self . assertFalse ( srv . parallelStartEnabled ) \n 
~~ def testUpdateBooleanWithInvalidValue ( self ) : \n 
~~ def testUpdateInteger ( self ) : \n 
jvm = getid1 ( \n 
self . assertEquals ( jvm . initialHeapSize , 0 ) \n 
self . assertEquals ( jvm . maximumHeapSize , 0 ) \n 
self . assertEquals ( jvm . initialHeapSize , 12345 ) \n 
self . assertEquals ( jvm . maximumHeapSize , 67890 ) \n 
~~ def testUpdateEnum ( self ) : \n 
cell = getid1 ( \n 
self . assertEquals ( cell . cellDiscoveryProtocol , ) \n 
~~ def testUpdateEnumWithInvalidValue ( self ) : \n 
failure = 0 \n 
~~~ importConfigurationManifest ( \n 
failure = 1 \n 
~~ self . assertFalse ( failure , ) \n 
~~ def testFailOnInvalidType ( self ) : \n 
~~ def testFailOnInvalidKey ( self ) : \n 
~~ def testFailOnInvalidAttribute ( self ) : \n 
topology \n 
~~ def testCommentsValid ( self ) : \n 
~~ def testCommentIndented ( self ) : \n 
self . assertFalse ( hasChanges ( ) ) \n 
~~ def testNoChanges ( self ) : \n 
~~ ~~ class HierarchiesManifestImportTest ( AbstractConfigTest ) : \n 
~~~ def testOneChild ( self ) : \n 
~~~ providers = getid ( \n 
self . assertEquals ( providers , [ ] ) \n 
providers = getid ( \n 
self . assertEquals ( len ( providers ) , 1 ) \n 
db2provider = providers [ 0 ] \n 
self . assertEquals ( db2provider . name , ) \n 
self . assertEquals ( db2provider . classpath , [ , , ] ) \n 
self . assertTrue ( db2provider . xa ) \n 
~~ def testMultipleChildren ( self ) : \n 
self . assertEquals ( len ( providers ) , 2 ) \n 
self . assertFalse ( db2provider . xa ) \n 
db2provider = providers [ 1 ] \n 
~~ def testMultipleChildrenInList ( self ) : \n 
~~~ cellVariables = getid1 ( \n 
variableCount = len ( cellVariables . entries ) \n 
self . assertEquals ( len ( cellVariables . entries ) , variableCount + 2 ) \n 
~~ def testMultipleChildrenInAttribute ( self ) : \n 
self . assertEquals ( len ( db2provider . propertySet . resourceProperties ) , 2 ) \n 
~~ def testRepeatedObjects ( self ) : \n 
server = getid1 ( \n 
referenceList = server . lookup1 ( \n 
~~ ~~ class IncludesAndImportsTest ( AbstractConfigTest ) : \n 
~~~ def testImportValid ( self ) : \n 
~~ def testIncludeWithPath ( self ) : \n 
self . assertEquals ( len ( cellVariables . entries ) , variableCount + 3 ) \n 
~~ def testIncludeWithoutPath ( self ) : \n 
~~ ~~ class VariablesAndFiltersTest ( AbstractConfigTest ) : \n 
~~~ def testCallables ( self ) : \n 
~~~ d = { } \n 
d . update ( topology ) \n 
d [ ] = [ , , , ] \n 
cellVariables = getid1 ( \n 
d \n 
self . assertEquals ( cellVariables . entries [ - 2 ] . symbolicName , ) \n 
self . assertEquals ( cellVariables . entries [ - 2 ] . value , ) \n 
self . assertEquals ( cellVariables . entries [ - 1 ] . symbolicName , ) \n 
self . assertEquals ( cellVariables . entries [ - 1 ] . value , ) \n 
~~ ~~ class ReferencesTest ( AbstractConfigTest ) : \n 
~~~ def testMailProtocolProvider ( self ) : \n 
smtpProtocol = getid1 ( \n 
) . lookup1 ( \n 
mailSession = getid1 ( \n 
self . assertEquals ( \n 
str ( mailSession . mailTransportProtocol ) , str ( smtpProtocol ) \n 
self . assertEquals ( mailSession . name , ) \n 
self . assertEquals ( mailSession . jndiName , ) \n 
self . assertEquals ( mailSession . mailTransportHost , ) \n 
~~ def testDependentService ( self ) : \n 
) . prerequisiteServices \n 
self . assertEquals ( len ( referenceList ) , 3 ) \n 
self . assertEquals ( referenceList [ 0 ] . displayName , ) \n 
self . assertEquals ( referenceList [ 1 ] . displayName , ) \n 
self . assertEquals ( referenceList [ 2 ] . displayName , ) \n 
~~ def testDependentServiceExtension ( self ) : \n 
self . assertEquals ( len ( referenceList ) , 5 ) \n 
self . assertEquals ( referenceList [ 3 ] . displayName , ) \n 
self . assertEquals ( referenceList [ 4 ] . displayName , ) \n 
~~ ~~ class RemovalTest ( AbstractConfigTest ) : \n 
~~~ def testJdbcProvider ( self ) : \n 
self . assertEquals ( len ( providers ) , 0 ) \n 
~~ def testJvmProperty ( self ) : \n 
prop = getid ( \n 
self . assertEquals ( len ( prop ) , 1 ) \n 
self . assertEquals ( len ( prop ) , 0 ) \n 
~~ def testMailTransportProtocol ( self ) : \n 
self . assertNone ( mailSession . mailTransportProtocol ) \n 
self . assertEquals ( len ( referenceList ) , 2 ) \n 
~~ ~~ class CustomizeTest ( AbstractConfigTest ) : \n 
~~~ def testJvmProperty ( self ) : \n 
self . assertEquals ( prop [ 0 ] . value , ) \n 
~~ def testMissingJvmProperty ( self ) : \n 
prop [ 0 ] . remove ( ) \n 
~~ def testExistingDataSource ( self ) : \n 
dataSource = getid1 ( \n 
self . assertEquals ( dataSource . jndiName , ) \n 
~~ def testNonexistentDataSource ( self ) : \n 
provider = getid1 ( \n 
provider . description , \n 
dataSource . remove ( ) \n 
dataSources = getid ( \n 
self . assertEquals ( len ( dataSources ) , 1 ) \n 
self . assertEquals ( dataSources [ 0 ] . jndiName , ) \n 
~~ def testNonexistentProvider ( self ) : \n 
provider . remove ( ) \n 
self . assertEquals ( dataSources , [ ] ) \n 
~~ ~~ class TemplatesTest ( AbstractConfigTest ) : \n 
~~~ def test_db2_ibm_jcc_xa ( self ) : \n 
db2provider . name , \n 
db2provider . description , \n 
db2provider . providerType , \n 
~~ def test_db2_ibm_jcc_xa_using_full_id ( self ) : \n 
~~ def test_non_unique_template ( self ) : \n 
~~ self . fail ( ) \n 
~~ def test_web_server ( self ) : \n 
~~~ self . assertEqual ( 0 , len ( getid ( ) ) ) \n 
self . assertEqual ( 1 , len ( getid ( ) ) ) \n 
~~ def test_non_existent_template ( self ) : \n 
~~ ~~ from . settings_base import * \n 
INSTALLED_APPS += [ "tests" ] \n 
VERSATILEIMAGEFIELD_SETTINGS = { \n 
: 2592000 , \n 
: 70 , \n 
: os . path . join ( \n 
PROJECT_DIR , \n 
~~~ op . create_table ( , \n 
sa . Column ( , sa . Integer ( ) , nullable = False ) , \n 
sa . Column ( , sa . DateTime ( ) , nullable = True ) , \n 
sa . Column ( , sa . String ( ) , nullable = True ) , \n 
sa . PrimaryKeyConstraint ( ) \n 
~~~ op . drop_table ( ) \n 
~~~ op . add_column ( , sa . Column ( , sa . Integer ( ) , nullable = True ) ) \n 
op . create_foreign_key ( None , , , [ ] , [ ] ) \n 
~~~ op . drop_constraint ( None , , type_ = ) \n 
op . drop_column ( , ) \n 
~~ from wpc . models import Subscriber , Streamer , YoutubeChannel , YoutubeStream \n 
from wpc . flask_utils import get_or_create \n 
from utils import youtube_video_id \n 
from flask_wtf import Form \n 
from wtforms import StringField , SubmitField , validators , TextAreaField \n 
from wtforms . validators import ValidationError \n 
from flask . ext . login import current_user \n 
def validate_email_unique ( form , field ) : \n 
~~~ email = field . data \n 
if Subscriber . query . filter_by ( email = email ) . first ( ) is not None : \n 
~~ ~~ class SubscribeForm ( Form ) : \n 
~~ class GLMSubscribeForm ( Form ) : \n 
submit_button = SubmitField ( ) \n 
~~ class DashboardEmailForm ( Form ) : \n 
def prepopulate ( self , streamer ) : \n 
~~~ if streamer . as_subscriber : \n 
~~~ self . email . data = streamer . as_subscriber . email \n 
~~ ~~ ~~ class DashboardAddVideoForm ( Form ) : \n 
def validate_link ( form , field ) : \n 
~~~ ytid = youtube_video_id ( field . data ) \n 
if not ytid : \n 
~~ existing_stream = YoutubeStream . query . filter_by ( ytid = ytid ) . first ( ) \n 
if existing_stream and existing_stream . streamer : \n 
~~ ~~ ~~ class IdeaForm ( Form ) : \n 
~~ class EditStreamTitleForm ( Form ) : \n 
~~~ title = StringField ( "Title" , [ validators . Length ( max = 200 ) ] ) \n 
~~ class RtmpRedirectForm ( Form ) : \n 
~~~ for rid in xrange ( 1 , 4 ) : \n 
~~~ attrname = . format ( rid ) \n 
getattr ( self , attrname ) . data = getattr ( streamer , attrname ) \n 
~~ ~~ ~~ class EditStreamerInfoForm ( Form ) : \n 
info = TextAreaField ( "Info" , [ validators . Length ( max = 5000 ) ] ) \n 
def twitch_channel_extract ( self ) : \n 
string = self . twitch_channel . data . strip ( ) \n 
position = string . find ( ) \n 
if position != - 1 : \n 
~~~ path = urlparse ( string [ position : ] ) . path . split ( ) \n 
if len ( path ) < 2 : \n 
~~ string = path [ 1 ] \n 
~~ return string if len ( string ) <= 25 and re . match ( , string ) else None \n 
~~ def youtube_channel_extract ( self ) : \n 
string = self . youtube_channel . data . strip ( ) \n 
if len ( path ) < 3 or path [ 1 ] != "channel" : \n 
~~~ string = path [ 2 ] \n 
~~ ~~ return string if len ( string ) == 24 and re . match ( , string ) or string == else None \n 
~~ def validate_youtube_channel ( form , field ) : \n 
~~~ yc = form . youtube_channel_extract ( ) \n 
if yc is None : \n 
~~ streamer = get_or_create ( YoutubeChannel , channel_id = yc ) . streamer \n 
if streamer and streamer . checked and streamer != current_user : \n 
~~ ~~ def validate_twith_channel ( form , field ) : \n 
~~~ tc = form . twitch_channel_extract ( ) \n 
if tc is None : \n 
~~ streamer = Streamer . query . filter_by ( twitch_channel = tc ) . first ( ) \n 
~~ ~~ ~~ class SearchForm ( Form ) : \n 
~~~ query = StringField ( "Search" ) \n 
search_button = SubmitField ( ) \n 
~~ import struct \n 
from handshake import * \n 
class ServerNameExtension ( TLSExtension ) : \n 
~~~ HostName = 0 \n 
~~~ TLSExtension . __init__ ( self ) \n 
def create ( cls , hostname , hostnames = [ ] , name_type = HostName ) : \n 
~~~ if len ( hostnames ) == 0 : \n 
~~~ hostnames = [ hostname ] \n 
~~ name_list = \n 
for hostname in hostnames : \n 
~~~ name = struct . pack ( % ( len ( hostname ) ) , \n 
name_type , \n 
len ( hostname ) , \n 
hostname ) \n 
name_list += name \n 
~~ data = struct . pack ( % ( len ( name_list ) ) , \n 
len ( name_list ) + 2 , \n 
len ( name_list ) , \n 
name_list ) \n 
#hexdump(data) \n 
return TLSExtension . create ( TLSExtension . ServerName , data ) \n 
from . import compat \n 
class Client ( object ) : \n 
connection = None \n 
def __init__ ( self , name , connection = None ) : \n 
~~~ self . name = self . _get_name ( name ) \n 
if not connection : \n 
~~~ connection = statsd . Connection ( ) \n 
~~ self . connection = connection \n 
self . logger = logging . getLogger ( \n 
% ( __name__ , self . __class__ . __name__ ) ) \n 
def _get_name ( cls , * name_parts ) : \n 
~~~ name_parts = [ compat . to_str ( x ) for x in name_parts if x ] \n 
return . join ( name_parts ) \n 
~~ def get_client ( self , name = None , class_ = None ) : \n 
name = self . _get_name ( self . name , name ) \n 
if not class_ : \n 
~~~ class_ = self . __class__ \n 
~~ return class_ ( \n 
connection = self . connection , \n 
~~ def get_average ( self , name = None ) : \n 
return self . get_client ( name = name , class_ = statsd . Average ) \n 
~~ def get_counter ( self , name = None ) : \n 
return self . get_client ( name = name , class_ = statsd . Counter ) \n 
~~ def get_gauge ( self , name = None ) : \n 
return self . get_client ( name = name , class_ = statsd . Gauge ) \n 
~~ def get_raw ( self , name = None ) : \n 
return self . get_client ( name = name , class_ = statsd . Raw ) \n 
~~ def get_timer ( self , name = None ) : \n 
return self . get_client ( name = name , class_ = statsd . Timer ) \n 
self . name , \n 
~~ def _send ( self , data ) : \n 
~~~ return self . connection . send ( data ) \n 
~~ ~~ version_info = ( 1 , 5 , 0 ) \n 
version = . join ( map ( str , version_info ) ) \n 
class TestConfigurationLoading ( unittest . TestCase ) : \n 
~~~ super ( TestConfigurationLoading , self ) . setUp ( ) \n 
self . _reset_config ( ) \n 
~~~ super ( TestConfigurationLoading , self ) . tearDown ( ) \n 
~~ def _reset_config ( self ) : \n 
from furious . config import get_config \n 
from furious . config import default_config \n 
config = get_config ( ) \n 
get_config ( ) . clear ( ) \n 
config . update ( default_config ( ) ) \n 
~~ def test_completion_config ( self ) : \n 
~~~ from furious . config import get_completion_cleanup_delay \n 
from furious . config import get_completion_cleanup_queue \n 
from furious . config import get_completion_default_queue \n 
queue = get_completion_cleanup_queue ( ) \n 
self . assertEqual ( queue , expected ) \n 
queue = get_completion_default_queue ( ) \n 
expected = 7600 \n 
delay = get_completion_cleanup_delay ( ) \n 
self . assertEqual ( delay , expected ) \n 
~~ def test_load_yaml_config ( self ) : \n 
from furious . config import _load_yaml_config \n 
contents = _load_yaml_config ( os . path . join ( , ) ) \n 
e = \n 
self . assertEqual ( contents , e ) \n 
~~ @ patch ( , autospec = True ) \n 
def test_not_find_yaml ( self , mock_exists ) : \n 
mock_exists . return_value = False \n 
from furious . config import find_furious_yaml \n 
config_yaml_path = find_furious_yaml ( ) \n 
self . assertIsNone ( config_yaml_path ) \n 
~~ def test_get_config ( self ) : \n 
from furious . config import _parse_yaml_config \n 
my_config = _parse_yaml_config ( example_yaml ) \n 
self . assertEqual ( my_config , { : , \n 
: 7600 , \n 
~~ def test_get_configured_persistence_exists ( self ) : \n 
from furious import config \n 
config . _config = my_config \n 
persistence_module = config . get_default_persistence_engine ( \n 
self . assertEqual ( persistence_module , config ) \n 
~~ def test_get_config_invalid_yaml ( self ) : \n 
from furious . config import InvalidYamlFile \n 
example_yaml = str ( \'secret_key:"blah"\\n\' \n 
self . assertRaises ( InvalidYamlFile , _parse_yaml_config , example_yaml ) \n 
~~ def test_get_configured_module_by_path ( self ) : \n 
from furious . config import _get_configured_module \n 
config . get_config ( ) [ ] = \n 
module = _get_configured_module ( ) \n 
self . assertEqual ( module , config ) \n 
~~ def test_get_configured_module_by_name ( self ) : \n 
from furious import async \n 
known_modules = { : } \n 
module = _get_configured_module ( , known_modules ) \n 
self . assertEqual ( module , async ) \n 
~~ def test_get_config_empty_yaml ( self ) : \n 
example_yaml = str ( ) \n 
self . assertEqual ( my_config , default_config ( ) ) \n 
migrations . AlterModelOptions ( \n 
options = { : ( ( , ) , ( ) , \n 
__licence__ = \n 
from django . utils . html import strip_tags \n 
from django import forms \n 
import bleach \n 
def sanitize ( t , allow_html = True , full_html = False ) : \n 
if not t : \n 
~~~ return t \n 
~~ if not allow_html : \n 
~~~ return strip_tags ( t ) \n 
~~ allowed_tags = bleach . ALLOWED_TAGS + [ , , , , , , , , , , , , , , , , , , , , , , ] \n 
if full_html : \n 
~~~ allowed_tags += [ ] \n 
~~ allowed_attributes = { \n 
: [ , , , , ] , \n 
: [ , , , ] , \n 
: [ , , ] , \n 
allowed_styles = [ , , , , , ] \n 
return bleach . clean ( t , tags = allowed_tags , attributes = allowed_attributes , styles = allowed_styles ) \n 
~~ class ModelSanitizeForm ( forms . ModelForm ) : \n 
def clean ( self ) : \n 
cleaned_data = super ( ModelSanitizeForm , self ) . clean ( ) \n 
for field in self . Meta . sanitize : \n 
~~~ if field in cleaned_data : \n 
~~~ cleaned_data [ field ] = sanitize ( cleaned_data [ field ] , allow_html = False ) \n 
~~ ~~ return cleaned_data \n 
~~ ~~ import argparse \n 
import service_config \n 
import urlparse \n 
import deploy_utils \n 
from log import Log \n 
ALL_JOBS = [ "chronos" ] \n 
def _get_chronos_service_config ( args ) : \n 
~~~ args . chronos_config = deploy_utils . get_service_config ( args ) \n 
~~ def generate_zk_jaas_config ( args ) : \n 
~~~ if not deploy_utils . is_security_enabled ( args ) : \n 
~~ config_dict = args . chronos_config . configuration . generated_files [ "jaas.conf" ] \n 
for key , value in config_dict . items ( ) [ 1 : ] : \n 
~~~ if value != "true" and value != "false" and value . find ( "\\"" ) == - 1 : \n 
~~~ config_dict [ key ] = "\\"" + value + "\\"" \n 
~~ ~~ header_line = config_dict [ "headerLine" ] \n 
for ( key , value ) in config_dict . iteritems ( ) if key != config_dict . keys ( ) [ 0 ] ] ) ) \n 
~~ def generate_configs ( args , job_name , host_id , instance_id ) : \n 
~~~ chronos_cfg_dict = args . chronos_config . configuration . generated_files [ "chronos.cfg" ] \n 
hosts = args . chronos_config . jobs [ job_name ] . hosts \n 
chronos_cfg = deploy_utils . generate_properties_file ( args , chronos_cfg_dict ) \n 
config_files = { \n 
"chronos.cfg" : chronos_cfg , \n 
"jaas.conf" : generate_zk_jaas_config ( args ) , \n 
config_files . update ( args . chronos_config . configuration . raw_files ) \n 
return config_files \n 
~~ def generate_run_scripts_params ( args , host , job_name , host_id , instance_id ) : \n 
~~~ job = args . chronos_config . jobs [ job_name ] \n 
supervisor_client = deploy_utils . get_supervisor_client ( host , \n 
"chronos" , args . chronos_config . cluster . name , job_name , instance_id = instance_id ) \n 
artifact_and_version = "chronos-" + args . chronos_config . cluster . version \n 
jar_dirs = "$package_dir/lib/*" \n 
log_level = deploy_utils . get_service_log_level ( args , args . chronos_config ) \n 
params = job . get_arguments ( args , args . chronos_config . cluster , args . chronos_config . jobs , \n 
args . chronos_config . arguments_dict , job_name , host_id , instance_id ) \n 
script_dict = { \n 
"artifact" : artifact_and_version , \n 
"job_name" : job_name , \n 
"jar_dirs" : jar_dirs , \n 
"run_dir" : supervisor_client . get_run_dir ( ) , \n 
"params" : params , \n 
return script_dict \n 
~~ def generate_start_script ( args , host , job_name , host_id , instance_id ) : \n 
~~~ script_params = generate_run_scripts_params ( args , host , job_name , host_id , instance_id ) \n 
return deploy_utils . create_run_script ( \n 
"%s/start.sh.tmpl" % deploy_utils . get_template_dir ( ) , \n 
script_params ) \n 
~~ def install ( args ) : \n 
~~~ _get_chronos_service_config ( args ) \n 
deploy_utils . install_service ( args , "chronos" , args . chronos_config , "chronos" ) \n 
~~ def cleanup ( args ) : \n 
cleanup_token = deploy_utils . confirm_cleanup ( args , \n 
"chronos" , args . chronos_config ) \n 
for job_name in args . job or ALL_JOBS : \n 
~~~ hosts = args . chronos_config . jobs [ job_name ] . hosts \n 
args . task_map = deploy_utils . parse_args_host_and_task ( args , hosts ) \n 
for host_id in args . task_map . keys ( ) or hosts . keys ( ) : \n 
~~~ for instance_id in args . task_map . get ( host_id ) or range ( hosts [ host_id ] . instance_num ) : \n 
~~~ instance_id = - 1 if not deploy_utils . is_multiple_instances ( host_id , hosts ) else instance_id \n 
deploy_utils . cleanup_job ( "chronos" , args . chronos_config , \n 
hosts [ host_id ] . ip , job_name , instance_id , cleanup_token ) \n 
~~ ~~ ~~ ~~ def bootstrap_job ( args , host , job_name , host_id , instance_id , cleanup_token ) : \n 
~~~ args . chronos_config . parse_generated_config_files ( args , job_name , host_id , instance_id ) \n 
deploy_utils . bootstrap_job ( args , "chronos" , "chronos" , \n 
args . chronos_config , host , job_name , instance_id , cleanup_token , ) \n 
start_job ( args , host , job_name , host_id , instance_id ) \n 
~~ def bootstrap ( args ) : \n 
cleanup_token = deploy_utils . confirm_bootstrap ( "chronos" , args . chronos_config ) \n 
bootstrap_job ( args , hosts [ host_id ] . ip , job_name , host_id , instance_id , cleanup_token ) \n 
~~ ~~ ~~ ~~ def start_job ( args , host , job_name , host_id , instance_id ) : \n 
config_files = generate_configs ( args , job_name , host_id , instance_id ) \n 
start_script = generate_start_script ( args , host , job_name , host_id , instance_id ) \n 
http_url = deploy_utils . get_http_service_uri ( host , \n 
args . chronos_config . jobs [ job_name ] . base_port , instance_id ) \n 
deploy_utils . start_job ( args , "chronos" , "chronos" , args . chronos_config , \n 
host , job_name , instance_id , start_script , http_url , ** config_files ) \n 
~~ def start ( args ) : \n 
~~~ if not args . skip_confirm : \n 
~~~ deploy_utils . confirm_start ( args ) \n 
~~ _get_chronos_service_config ( args ) \n 
start_job ( args , hosts [ host_id ] . ip , job_name , host_id , instance_id ) \n 
~~ ~~ ~~ ~~ def stop_job ( args , host , job_name , instance_id ) : \n 
~~~ deploy_utils . stop_job ( "chronos" , args . chronos_config , host , job_name , instance_id ) \n 
~~ def stop ( args ) : \n 
~~~ deploy_utils . confirm_stop ( args ) \n 
stop_job ( args , hosts [ host_id ] . ip , job_name , instance_id ) \n 
~~ ~~ ~~ ~~ def restart ( args ) : \n 
~~~ deploy_utils . confirm_restart ( args ) \n 
~~ ~~ ~~ for job_name in args . job or ALL_JOBS : \n 
deploy_utils . wait_for_job_stopping ( "chronos" , \n 
args . chronos_config . cluster . name , job_name , hosts [ host_id ] . ip , instance_id ) \n 
~~ ~~ ~~ ~~ def show ( args ) : \n 
deploy_utils . show_job ( "chronos" , args . chronos_config , \n 
hosts [ host_id ] . ip , job_name , instance_id ) \n 
~~ ~~ ~~ ~~ def run_shell ( args ) : \n 
~~ def pack ( args ) : \n 
~~ def rolling_update ( args ) : \n 
~~~ if not args . job : \n 
job_name = args . job [ 0 ] \n 
if not args . skip_confirm : \n 
~~~ deploy_utils . confirm_action ( args , "rolling_update" ) \n 
wait_time = 0 \n 
for host_id in args . task_map . keys ( ) or hosts . iterkeys ( ) : \n 
deploy_utils . confirm_rolling_update ( host_id , instance_id , wait_time ) \n 
deploy_utils . wait_for_job_starting ( "chronos" , \n 
wait_time = args . time_interval \n 
~~ from django . conf . urls import patterns , url \n 
import views \n 
urlpatterns = patterns ( \n 
url ( , views . index ) , \n 
url ( , views . show_online ) , \n 
url ( , views . show_business ) , \n 
import utils . quota_util \n 
def param_group ( graph_config ) : \n 
~~~ return . join ( [ group for group , key in graph_config ] ) \n 
~~ @ register . filter ( name = ) \n 
def param_key ( graph_config ) : \n 
~~~ return . join ( [ . join ( ( group , key ) ) for group , key , unit in graph_config ] ) \n 
def param_multikey_for_view ( view_config ) : \n 
~~~ return . join ( [ param_key ( graph_config ) for graph_config in view_config ] ) \n 
def param_height ( view_config ) : \n 
~~~ graph_per_row = 3 \n 
height_per_row = 295 \n 
return ( len ( view_config ) + ( graph_per_row - 1 ) ) / graph_per_row * height_per_row \n 
def pic_width ( span ) : \n 
~~~ return span * 100 \n 
def pic_heigth ( metrics ) : \n 
~~~ return len ( metrics ) * 10 + 450 \n 
def format_bigint ( value ) : \n 
~~~ value = int ( value ) \n 
~~ except ( TypeError , ValueError ) : \n 
~~ if value < 1024 * 1024 : \n 
~~ K = 1024 \n 
formaters = ( \n 
( 2 , ) , \n 
( 3 , ) , \n 
( 4 , ) , \n 
( 5 , ) , \n 
for exponent , formater in formaters : \n 
~~~ larger_num = K ** exponent \n 
if value < larger_num * K : \n 
~~~ return formater % ( value / float ( larger_num ) ) \n 
~~ ~~ ~~ @ register . filter ( name = ) \n 
def is_space_quota_healthy ( total , used ) : \n 
~~~ return utils . quota_util . is_space_quota_healthy ( total , used ) \n 
def is_name_quota_healthy ( total , used ) : \n 
~~~ return utils . quota_util . is_name_quota_healthy ( total , used ) \n 
class CrashMailTests ( unittest . TestCase ) : \n 
~~~ def _getTargetClass ( self ) : \n 
~~~ from superlance . crashmail import CrashMail \n 
return CrashMail \n 
~~ def _makeOne ( self , * opts ) : \n 
~~~ return self . _getTargetClass ( ) ( * opts ) \n 
~~ def setUp ( self ) : \n 
~~~ import tempfile \n 
self . tempdir = tempfile . mkdtemp ( ) \n 
~~~ import shutil \n 
shutil . rmtree ( self . tempdir ) \n 
~~ def _makeOnePopulated ( self , programs , any , response = None ) : \n 
~~~ import os \n 
sendmail = % os . path . join ( self . tempdir , ) \n 
email = \n 
header = \n 
prog = self . _makeOne ( programs , any , email , sendmail , header ) \n 
prog . stdin = StringIO ( ) \n 
prog . stdout = StringIO ( ) \n 
prog . stderr = StringIO ( ) \n 
return prog \n 
~~ def test_runforever_not_process_state_exited ( self ) : \n 
~~~ programs = { : 0 , : 0 , : 0 } \n 
groups = { } \n 
any = None \n 
prog = self . _makeOnePopulated ( programs , any ) \n 
prog . stdin . write ( ) \n 
prog . stdin . seek ( 0 ) \n 
prog . runforever ( test = True ) \n 
self . assertEqual ( prog . stderr . getvalue ( ) , ) \n 
~~ def test_runforever_expected_exit ( self ) : \n 
~~~ programs = [ ] \n 
payload = ( \n 
prog . stdin . write ( \n 
% len ( payload ) ) \n 
prog . stdin . write ( payload ) \n 
~~ def test_runforever_unexpected_exit ( self ) : \n 
output = prog . stderr . getvalue ( ) \n 
lines = output . split ( ) \n 
self . assertEqual ( lines [ 0 ] , ) \n 
self . assertEqual ( lines [ 1 ] , ) \n 
self . assertEqual ( lines [ 2 ] , ) \n 
self . assertEqual ( lines [ 3 ] , ) \n 
self . failUnless ( in lines [ 4 ] ) \n 
self . assertEqual ( lines [ 5 ] , ) \n 
self . failUnless ( \n 
in lines [ 6 ] ) \n 
mail = open ( os . path . join ( self . tempdir , ) , ) . read ( ) \n 
in mail ) \n 
import cmd \n 
import getpass \n 
import xmlrpclib \n 
from supervisor . medusa import asyncore_25 as asyncore \n 
from supervisor . options import ClientOptions \n 
from supervisor . options import split_namespec \n 
from supervisor import xmlrpc \n 
from supervisor import states \n 
class fgthread ( threading . Thread ) : \n 
def __init__ ( self , program , ctl ) : \n 
import http_client \n 
self . killed = False \n 
self . program = program \n 
self . ctl = ctl \n 
self . listener = http_client . Listener ( ) \n 
self . output_handler = http_client . HTTPHandler ( self . listener , \n 
self . ctl . options . username , \n 
self . ctl . options . password ) \n 
self . error_handler = http_client . HTTPHandler ( self . listener , \n 
~~ def start ( self ) : \n 
~~~ self . __run_backup = self . run \n 
self . run = self . __run \n 
threading . Thread . start ( self ) \n 
~~~ self . output_handler . get ( self . ctl . options . serverurl , \n 
% self . program ) \n 
self . error_handler . get ( self . ctl . options . serverurl , \n 
asyncore . loop ( ) \n 
~~ def __run ( self ) : \n 
~~~ sys . settrace ( self . globaltrace ) \n 
self . __run_backup ( ) \n 
self . run = self . __run_backup \n 
~~ def globaltrace ( self , frame , why , arg ) : \n 
~~~ if why == : \n 
~~~ return self . localtrace \n 
~~ ~~ def localtrace ( self , frame , why , arg ) : \n 
~~~ if self . killed : \n 
~~~ raise SystemExit ( ) \n 
~~ ~~ return self . localtrace \n 
~~~ self . output_handler . close ( ) \n 
self . error_handler . close ( ) \n 
self . killed = True \n 
~~ ~~ class Controller ( cmd . Cmd ) : \n 
~~~ def __init__ ( self , options , completekey = , stdin = None , \n 
stdout = None ) : \n 
~~~ self . options = options \n 
self . prompt = self . options . prompt + \n 
self . options . plugins = [ ] \n 
self . vocab = [ , , , , , \n 
, , , , , \n 
, , , , , , \n 
cmd . Cmd . __init__ ( self , completekey , stdin , stdout ) \n 
for name , factory , kwargs in self . options . plugin_factories : \n 
~~~ plugin = factory ( self , ** kwargs ) \n 
self . options . plugins . append ( plugin ) \n 
plugin . name = name \n 
~~ ~~ def emptyline ( self ) : \n 
~~ def onecmd ( self , line ) : \n 
origline = line \n 
lines = line . split ( ) \n 
line = lines . pop ( 0 ) \n 
self . cmdqueue . extend ( lines ) \n 
cmd , arg , line = self . parseline ( line ) \n 
if not line : \n 
~~~ return self . emptyline ( ) \n 
~~ if cmd is None : \n 
~~~ return self . default ( line ) \n 
~~ self . lastcmd = line \n 
~~~ do_func = self . _get_do_func ( cmd ) \n 
if do_func is None : \n 
~~~ return do_func ( arg ) \n 
~~ except xmlrpclib . ProtocolError , e : \n 
~~~ if e . errcode == 401 : \n 
~~~ if self . options . interactive : \n 
~~~ self . output ( ) \n 
username = raw_input ( ) \n 
password = getpass . getpass ( prompt = ) \n 
self . output ( ) \n 
self . options . username = username \n 
self . options . password = password \n 
return self . onecmd ( origline ) \n 
~~~ self . options . usage ( ) \n 
~~ ~~ do_func ( arg ) \n 
~~ except SystemExit : \n 
~~~ ( file , fun , line ) , t , v , tbinfo = asyncore . compact_traceback ( ) \n 
error = % ( t , v , file , line ) \n 
self . output ( error ) \n 
if not self . options . interactive : \n 
~~~ sys . exit ( 2 ) \n 
~~ ~~ ~~ ~~ def _get_do_func ( self , cmd ) : \n 
~~~ func_name = + cmd \n 
func = getattr ( self , func_name , None ) \n 
if not func : \n 
~~~ for plugin in self . options . plugins : \n 
~~~ func = getattr ( plugin , func_name , None ) \n 
if func is not None : \n 
~~ ~~ ~~ return func \n 
~~ def output ( self , stuff ) : \n 
~~~ if stuff is not None : \n 
~~~ if isinstance ( stuff , unicode ) : \n 
~~~ stuff = stuff . encode ( ) \n 
~~ self . stdout . write ( stuff + ) \n 
~~ ~~ def get_supervisor ( self ) : \n 
~~~ return self . get_server_proxy ( ) \n 
~~ def get_server_proxy ( self , namespace = None ) : \n 
~~~ proxy = self . options . getServerProxy ( ) \n 
if namespace is None : \n 
~~~ return proxy \n 
~~~ return getattr ( proxy , namespace ) \n 
~~ ~~ def upcheck ( self ) : \n 
~~~ supervisor = self . get_supervisor ( ) \n 
from supervisor import rpcinterface \n 
if api != rpcinterface . API_VERSION : \n 
~~~ self . output ( \n 
% ( rpcinterface . API_VERSION , api ) ) \n 
~~ ~~ except xmlrpclib . Fault , e : \n 
~~~ if e . faultCode == xmlrpc . Faults . UNKNOWN_METHOD : \n 
~~ except socket . error , why : \n 
~~~ if why [ 0 ] == errno . ECONNREFUSED : \n 
~~~ self . output ( % self . options . serverurl ) \n 
~~ elif why [ 0 ] == errno . ENOENT : \n 
~~ def completionmatches ( self , text , line , flag = 0 ) : \n 
~~~ groups = [ ] \n 
programs = [ ] \n 
groupwiseprograms = { } \n 
info = self . get_supervisor ( ) . getAllProcessInfo ( ) \n 
for i in info : \n 
~~~ programs . append ( i [ ] ) \n 
if i [ ] not in groups : \n 
~~~ groups . append ( i [ ] ) \n 
groupwiseprograms [ i [ ] ] = [ ] \n 
~~ groupwiseprograms [ i [ ] ] . append ( i [ ] ) \n 
~~ total = [ ] \n 
for i in groups : \n 
~~~ if i in programs : \n 
~~~ total . append ( i + ) \n 
~~~ for n in groupwiseprograms [ i ] : \n 
~~~ total . append ( i + + n + ) \n 
~~ ~~ ~~ if flag : \n 
~~~ return [ i + for i in groups if i . startswith ( text ) ] \n 
~~ if len ( line . split ( ) ) == 1 : \n 
~~~ return total \n 
~~~ current = line . split ( ) [ - 1 ] \n 
if line . endswith ( ) and len ( line . split ( ) ) > 1 : \n 
~~~ results = [ i for i in total if i . startswith ( text ) ] \n 
~~ if in current : \n 
~~~ g = current . split ( ) [ 0 ] \n 
results = [ i + for i in groupwiseprograms [ g ] \n 
if i . startswith ( text ) ] \n 
~~ results = [ i for i in total if i . startswith ( text ) ] \n 
~~ ~~ def complete ( self , text , state ) : \n 
~~~ import readline \n 
~~ line = readline . get_line_buffer ( ) \n 
if line == : \n 
~~~ results = [ i + for i in self . vocab if i . startswith ( text ) ] + [ None ] \n 
return results [ state ] \n 
~~~ exp = line . split ( ) [ 0 ] \n 
if exp in [ , , , , , , , ] : \n 
~~~ if not line . endswith ( ) and len ( line . split ( ) ) == 1 : \n 
~~~ return [ text + , None ] [ state ] \n 
~~ if exp == : \n 
~~~ if line . endswith ( ) and len ( line . split ( ) ) > 1 : \n 
~~ ~~ results = self . completionmatches ( text , line ) + [ None ] \n 
~~ elif exp in [ , , , , , , \n 
, , ] : \n 
~~ elif exp == : \n 
~~ results = [ i + for i in self . vocab if i . startswith ( text ) ] + [ None ] \n 
~~ elif exp in [ , ] : \n 
~~~ results = self . completionmatches ( text , line , flag = 1 ) + [ None ] \n 
~~ ~~ ~~ def do_help ( self , arg ) : \n 
~~~ plugin . do_help ( arg ) \n 
~~ ~~ def help_help ( self ) : \n 
~~ def do_EOF ( self , arg ) : \n 
~~ def help_EOF ( self ) : \n 
~~ ~~ def get_names ( inst ) : \n 
~~~ names = [ ] \n 
classes = [ inst . __class__ ] \n 
while classes : \n 
~~~ aclass = classes . pop ( 0 ) \n 
if aclass . __bases__ : \n 
~~~ classes = classes + list ( aclass . __bases__ ) \n 
~~ names = names + dir ( aclass ) \n 
~~ return names \n 
~~ class ControllerPluginBase : \n 
def __init__ ( self , controller ) : \n 
~~~ self . ctl = controller \n 
~~ def _doc_header ( self ) : \n 
~~ doc_header = property ( _doc_header ) \n 
def do_help ( self , arg ) : \n 
~~~ if arg : \n 
~~~ func = getattr ( self , + arg ) \n 
~~~ doc = getattr ( self , + arg ) . __doc__ \n 
if doc : \n 
~~~ self . ctl . stdout . write ( "%s\\n" % str ( doc ) ) \n 
~~ ~~ except AttributeError : \n 
~~ self . ctl . stdout . write ( "%s\\n" % str ( self . ctl . nohelp % ( arg , ) ) ) \n 
~~ func ( ) \n 
~~~ names = get_names ( self ) \n 
cmds_doc = [ ] \n 
cmds_undoc = [ ] \n 
help = { } \n 
for name in names : \n 
~~~ if name [ : 5 ] == : \n 
~~~ help [ name [ 5 : ] ] = 1 \n 
~~ ~~ names . sort ( ) \n 
prevname = \n 
~~~ if name [ : 3 ] == : \n 
~~~ if name == prevname : \n 
~~ prevname = name \n 
cmd = name [ 3 : ] \n 
if cmd in help : \n 
~~~ cmds_doc . append ( cmd ) \n 
del help [ cmd ] \n 
~~ elif getattr ( self , name ) . __doc__ : \n 
~~~ cmds_undoc . append ( cmd ) \n 
~~ ~~ ~~ self . ctl . stdout . write ( "\\n" ) \n 
self . ctl . print_topics ( self . doc_header , cmds_doc , 15 , 80 ) \n 
~~ ~~ ~~ class DefaultControllerPlugin ( ControllerPluginBase ) : \n 
def _tailf ( self , path ) : \n 
~~~ if not self . ctl . upcheck ( ) : \n 
~~ self . ctl . output ( ) \n 
username = self . ctl . options . username \n 
password = self . ctl . options . password \n 
~~~ import http_client \n 
if self . listener is None : \n 
~~~ listener = http_client . Listener ( ) \n 
~~ handler = http_client . HTTPHandler ( listener , username , password ) \n 
handler . get ( self . ctl . options . serverurl , path ) \n 
~~ except KeyboardInterrupt : \n 
~~~ handler . close ( ) \n 
self . ctl . output ( ) \n 
~~ ~~ def do_tail ( self , arg ) : \n 
~~ args = arg . strip ( ) . split ( ) \n 
if len ( args ) < 1 : \n 
~~~ self . ctl . output ( ) \n 
self . help_tail ( ) \n 
~~ elif len ( args ) > 3 : \n 
~~ modifier = None \n 
if args [ 0 ] . startswith ( ) : \n 
~~~ modifier = args . pop ( 0 ) \n 
~~~ name = args [ - 1 ] \n 
channel = \n 
~~~ name = args [ 0 ] \n 
channel = args [ - 1 ] . lower ( ) \n 
if channel not in ( , ) : \n 
~~~ self . ctl . output ( % channel ) \n 
~~ ~~ bytes = 1600 \n 
if modifier is not None : \n 
~~~ what = modifier [ 1 : ] \n 
if what == : \n 
~~~ bytes = None \n 
~~~ bytes = int ( what ) \n 
~~~ self . ctl . output ( % modifier ) \n 
~~ ~~ ~~ supervisor = self . ctl . get_supervisor ( ) \n 
if bytes is None : \n 
~~~ return self . _tailf ( % ( name , channel ) ) \n 
~~~ if channel is : \n 
~~~ output = supervisor . readProcessStdoutLog ( name , \n 
- bytes , 0 ) \n 
~~~ output = supervisor . readProcessStderrLog ( name , \n 
~~~ template = \n 
if e . faultCode == xmlrpc . Faults . NO_FILE : \n 
~~~ self . ctl . output ( template % ( name , ) ) \n 
~~ elif e . faultCode == xmlrpc . Faults . FAILED : \n 
~~~ self . ctl . output ( template % ( name , \n 
~~ elif e . faultCode == xmlrpc . Faults . BAD_NAME : \n 
~~~ self . ctl . output ( output ) \n 
~~ ~~ ~~ def help_tail ( self ) : \n 
~~~ self . ctl . output ( \n 
"Ex:\\n" \n 
~~ def do_maintail ( self , arg ) : \n 
if len ( args ) > 1 : \n 
self . help_maintail ( ) \n 
~~ elif len ( args ) == 1 : \n 
~~~ if args [ 0 ] . startswith ( ) : \n 
~~~ what = args [ 0 ] [ 1 : ] \n 
~~~ path = \n 
return self . _tailf ( path ) \n 
~~~ what = int ( what ) \n 
~~~ self . ctl . output ( % args [ 0 ] ) \n 
~~~ bytes = what \n 
~~~ bytes = 1600 \n 
~~ supervisor = self . ctl . get_supervisor ( ) \n 
~~~ output = supervisor . readLog ( - bytes , 0 ) \n 
~~ except xmlrpclib . Fault , e : \n 
~~~ self . ctl . output ( template % ( , ) ) \n 
~~~ self . ctl . output ( template % ( , \n 
~~ ~~ def help_maintail ( self ) : \n 
~~ def do_quit ( self , arg ) : \n 
~~ def help_quit ( self ) : \n 
~~ do_exit = do_quit \n 
def help_exit ( self ) : \n 
~~ def _procrepr ( self , info ) : \n 
if info [ ] == info [ ] : \n 
~~~ name = info [ ] \n 
~~~ name = % ( info [ ] , info [ ] ) \n 
~~ return template % { : name , : info [ ] , \n 
: info [ ] } \n 
~~ def do_status ( self , arg ) : \n 
names = arg . strip ( ) . split ( ) \n 
if names : \n 
~~~ info = supervisor . getProcessInfo ( name ) \n 
~~~ if e . faultCode == xmlrpc . Faults . BAD_NAME : \n 
~~~ self . ctl . output ( % name ) \n 
~~ self . ctl . output ( self . _procrepr ( info ) ) \n 
~~~ for info in supervisor . getAllProcessInfo ( ) : \n 
~~~ self . ctl . output ( self . _procrepr ( info ) ) \n 
~~ ~~ ~~ def help_status ( self ) : \n 
self . ctl . output ( \n 
"processes." ) \n 
~~ def do_pid ( self , arg ) : \n 
~~~ supervisor = self . ctl . get_supervisor ( ) \n 
if not self . ctl . upcheck ( ) : \n 
~~ names = arg . strip ( ) . split ( ) \n 
if not names : \n 
~~~ pid = supervisor . getPID ( ) \n 
self . ctl . output ( str ( pid ) ) \n 
~~ elif in names : \n 
~~~ self . ctl . output ( str ( info [ ] ) ) \n 
~~ self . ctl . output ( str ( info [ ] ) ) \n 
~~ ~~ ~~ def help_pid ( self ) : \n 
~~ def _startresult ( self , result ) : \n 
~~~ name = result [ ] \n 
code = result [ ] \n 
template = \n 
if code == xmlrpc . Faults . BAD_NAME : \n 
~~~ return template % ( name , ) \n 
~~ elif code == xmlrpc . Faults . NO_FILE : \n 
~~ elif code == xmlrpc . Faults . NOT_EXECUTABLE : \n 
~~ elif code == xmlrpc . Faults . ALREADY_STARTED : \n 
~~ elif code == xmlrpc . Faults . SPAWN_ERROR : \n 
~~ elif code == xmlrpc . Faults . ABNORMAL_TERMINATION : \n 
~~ elif code == xmlrpc . Faults . SUCCESS : \n 
~~~ return % name \n 
~~ raise ValueError ( % ( code , name ) ) \n 
~~ def do_start ( self , arg ) : \n 
supervisor = self . ctl . get_supervisor ( ) \n 
self . help_start ( ) \n 
~~ if in names : \n 
~~~ results = supervisor . startAllProcesses ( ) \n 
for result in results : \n 
~~~ result = self . _startresult ( result ) \n 
self . ctl . output ( result ) \n 
~~~ group_name , process_name = split_namespec ( name ) \n 
if process_name is None : \n 
~~~ results = supervisor . startProcessGroup ( group_name ) \n 
~~~ result = supervisor . startProcess ( name ) \n 
~~~ error = self . _startresult ( { : e . faultCode , \n 
: name , \n 
: e . faultString } ) \n 
self . ctl . output ( error ) \n 
~~ ~~ ~~ ~~ ~~ def help_start ( self ) : \n 
~~ def _stopresult ( self , result ) : \n 
fault_string = result [ ] \n 
~~ elif code == xmlrpc . Faults . NOT_RUNNING : \n 
~~ elif code == xmlrpc . Faults . FAILED : \n 
~~~ return fault_string \n 
~~ def do_stop ( self , arg ) : \n 
self . help_stop ( ) \n 
~~~ results = supervisor . stopAllProcesses ( ) \n 
~~~ result = self . _stopresult ( result ) \n 
~~~ results = supervisor . stopProcessGroup ( group_name ) \n 
~~~ result = supervisor . stopProcess ( name ) \n 
~~~ error = self . _stopresult ( { : e . faultCode , \n 
~~ ~~ ~~ ~~ ~~ def help_stop ( self ) : \n 
~~ def do_restart ( self , arg ) : \n 
self . help_restart ( ) \n 
~~ self . do_stop ( arg ) \n 
self . do_start ( arg ) \n 
~~ def help_restart ( self ) : \n 
"groups" ) \n 
~~ def do_shutdown ( self , arg ) : \n 
~~~ if self . ctl . options . interactive : \n 
~~~ yesno = raw_input ( \n 
really = yesno . lower ( ) . startswith ( ) \n 
~~~ really = 1 \n 
~~ if really : \n 
~~~ supervisor . shutdown ( ) \n 
~~~ if e . faultCode == xmlrpc . Faults . SHUTDOWN_STATE : \n 
~~ ~~ except socket . error , e : \n 
~~~ if e [ 0 ] == errno . ECONNREFUSED : \n 
~~~ msg = \n 
self . ctl . output ( msg % self . ctl . options . serverurl ) \n 
~~ elif e [ 0 ] == errno . ENOENT : \n 
~~ ~~ ~~ def help_shutdown ( self ) : \n 
~~ def do_reload ( self , arg ) : \n 
~~~ supervisor . restart ( ) \n 
~~ ~~ ~~ def help_reload ( self ) : \n 
~~ def _formatChanges ( self , ( added , changed , dropped ) ) : \n 
~~~ changedict = { } \n 
for n , t in [ ( added , ) , \n 
( changed , ) , \n 
( dropped , ) ] : \n 
~~~ changedict . update ( dict ( zip ( n , [ t ] * len ( n ) ) ) ) \n 
~~ if changedict : \n 
~~~ names = changedict . keys ( ) \n 
names . sort ( ) \n 
~~ ~~ def _formatConfigInfo ( self , configinfo ) : \n 
~~~ if configinfo [ ] == configinfo [ ] : \n 
~~~ name = configinfo [ ] \n 
~~~ name = "%s:%s" % ( configinfo [ ] , configinfo [ ] ) \n 
~~ formatted = { : name } \n 
if configinfo [ ] : \n 
~~~ formatted [ ] = \n 
~~ if configinfo [ ] : \n 
~~ formatted [ ] = "%s:%s" % ( configinfo [ ] , \n 
configinfo [ ] ) \n 
return template % formatted \n 
~~ def do_avail ( self , arg ) : \n 
~~~ configinfo = supervisor . getAllConfigInfo ( ) \n 
~~~ for pinfo in configinfo : \n 
~~~ self . ctl . output ( self . _formatConfigInfo ( pinfo ) ) \n 
~~ ~~ ~~ def help_avail ( self ) : \n 
~~ def do_reread ( self , arg ) : \n 
~~~ result = supervisor . reloadConfig ( ) \n 
~~ elif e . faultCode == xmlrpc . Faults . CANT_REREAD : \n 
~~~ self . ctl . output ( % e . faultString ) \n 
~~~ self . _formatChanges ( result [ 0 ] ) \n 
~~ ~~ def help_reread ( self ) : \n 
~~ def do_add ( self , arg ) : \n 
~~~ names = arg . strip ( ) . split ( ) \n 
~~~ supervisor . addProcessGroup ( name ) \n 
~~ elif e . faultCode == xmlrpc . Faults . ALREADY_ADDED : \n 
~~ ~~ ~~ def help_add ( self ) : \n 
~~ def do_remove ( self , arg ) : \n 
~~~ result = supervisor . removeProcessGroup ( name ) \n 
~~~ if e . faultCode == xmlrpc . Faults . STILL_RUNNING : \n 
~~ ~~ ~~ def help_remove ( self ) : \n 
~~ def do_update ( self , arg ) : \n 
~~~ def log ( name , message ) : \n 
~~ ~~ added , changed , removed = result [ 0 ] \n 
for gname in removed : \n 
~~~ results = supervisor . stopProcessGroup ( gname ) \n 
log ( gname , "stopped" ) \n 
fails = [ res for res in results \n 
if res [ ] == xmlrpc . Faults . FAILED ] \n 
if fails : \n 
~~ supervisor . removeProcessGroup ( gname ) \n 
~~ for gname in changed : \n 
supervisor . removeProcessGroup ( gname ) \n 
supervisor . addProcessGroup ( gname ) \n 
~~ for gname in added : \n 
~~~ supervisor . addProcessGroup ( gname ) \n 
~~ ~~ def help_update ( self ) : \n 
~~ def _clearresult ( self , result ) : \n 
~~ def do_clear ( self , arg ) : \n 
self . help_clear ( ) \n 
if in names : \n 
~~~ results = supervisor . clearAllProcessLogs ( ) \n 
~~~ result = self . _clearresult ( result ) \n 
~~~ result = supervisor . clearProcessLogs ( name ) \n 
~~~ error = self . _clearresult ( { : e . faultCode , \n 
~~ ~~ ~~ ~~ def help_clear ( self ) : \n 
~~ def do_open ( self , arg ) : \n 
~~~ url = arg . strip ( ) \n 
parts = urlparse . urlparse ( url ) \n 
if parts [ 0 ] not in ( , ) : \n 
~~ self . ctl . options . serverurl = url \n 
self . do_status ( ) \n 
~~ def help_open ( self ) : \n 
~~ def do_version ( self , arg ) : \n 
self . ctl . output ( supervisor . getSupervisorVersion ( ) ) \n 
~~ def help_version ( self ) : \n 
"process" ) \n 
~~ def do_fg ( self , args = None ) : \n 
~~ if not args : \n 
self . help_fg ( ) \n 
~~ args = args . split ( ) \n 
~~ program = args [ 0 ] \n 
~~~ info = supervisor . getProcessInfo ( program ) \n 
~~ except xmlrpclib . Fault , msg : \n 
~~~ if msg . faultCode == xmlrpc . Faults . BAD_NAME : \n 
~~ self . ctl . output ( str ( msg ) ) \n 
~~ if not info [ ] == states . ProcessStates . RUNNING : \n 
~~~ a = fgthread ( program , self . ctl ) \n 
a . start ( ) \n 
~~~ inp = raw_input ( ) + \n 
~~~ supervisor . sendProcessStdin ( program , inp ) \n 
~~~ if msg . faultCode == xmlrpc . Faults . NOT_RUNNING : \n 
a . kill ( ) \n 
~~ ~~ info = supervisor . getProcessInfo ( program ) \n 
if not info [ ] == states . ProcessStates . RUNNING : \n 
~~ ~~ except ( KeyboardInterrupt , EOFError ) : \n 
~~~ a . kill ( ) \n 
~~ def help_fg ( self , args = None ) : \n 
~~ ~~ def main ( args = None , options = None ) : \n 
~~~ if options is None : \n 
~~~ options = ClientOptions ( ) \n 
~~ options . realize ( args , doc = __doc__ ) \n 
c = Controller ( options ) \n 
if options . args : \n 
~~ if options . interactive : \n 
if options . history_file : \n 
~~~ readline . read_history_file ( options . history_file ) \n 
~~ def save ( ) : \n 
~~~ readline . write_history_file ( options . history_file ) \n 
~~ ~~ import atexit \n 
atexit . register ( save ) \n 
~~ ~~ except ImportError : \n 
~~~ c . cmdqueue . append ( ) \n 
c . cmdloop ( ) \n 
~~~ c . output ( ) \n 
~~ ~~ ~~ if __name__ == "__main__" : \n 
__all__ = [ , ] \n 
import webbrowser \n 
from email . Utils import encode_rfc2231 \n 
_controllers = { } \n 
_open = None \n 
class BaseController ( object ) : \n 
def __init__ ( self , name ) : \n 
~~ def open ( self , filename ) : \n 
~~ ~~ _is_windows = sys . platform . startswith ( ) \n 
_is_linux = sys . platform . startswith ( ) \n 
_is_osx = sys . platform == \n 
class Controller ( BaseController ) : \n 
def __init__ ( self , * args ) : \n 
~~~ super ( Controller , self ) . __init__ ( os . path . basename ( args [ 0 ] ) ) \n 
self . args = list ( args ) \n 
~~ def _invoke ( self , cmdline ) : \n 
~~~ if _is_windows : \n 
~~~ closefds = False \n 
startupinfo = subprocess . STARTUPINFO ( ) \n 
startupinfo . dwFlags |= subprocess . STARTF_USESHOWWINDOW \n 
~~~ closefds = True \n 
startupinfo = None \n 
~~ if os . environ . get ( ) or _is_windows or _is_osx : \n 
~~~ inout = file ( os . devnull , ) \n 
~~~ inout = None \n 
~~ setsid = getattr ( os , , None ) \n 
if not setsid : \n 
~~~ setsid = getattr ( os , , None ) \n 
~~ pipe = subprocess . Popen ( cmdline , stdin = inout , stdout = inout , \n 
stderr = inout , close_fds = closefds , \n 
preexec_fn = setsid , startupinfo = startupinfo ) \n 
returncode = pipe . wait ( ) \n 
~~~ returncode = self . fixreturncode ( returncode ) \n 
~~ return not returncode \n 
~~~ if isinstance ( filename , basestring ) : \n 
~~~ cmdline = self . args + [ filename ] \n 
~~~ cmdline = self . args + filename \n 
~~~ return self . _invoke ( cmdline ) \n 
~~ ~~ ~~ if _is_windows : \n 
~~~ class Start ( BaseController ) : \n 
def open ( self , filename ) : \n 
~~~ os . startfile ( filename ) \n 
~~ except WindowsError : \n 
~~ ~~ ~~ _controllers [ ] = Start ( ) \n 
_open = _controllers [ ] . open \n 
~~ elif _is_osx : \n 
~~~ _controllers [ ] = Controller ( ) \n 
~~~ import commands \n 
from webbrowser import _iscommand \n 
class KfmClient ( Controller ) : \n 
def __init__ ( self , kfmclient = ) : \n 
~~~ super ( KfmClient , self ) . __init__ ( kfmclient , ) \n 
self . kde_version = self . detect_kde_version ( ) \n 
~~ def detect_kde_version ( self ) : \n 
~~~ kde_version = None \n 
~~~ info = commands . getoutput ( ) \n 
for line in info . splitlines ( ) : \n 
~~~ if line . startswith ( ) : \n 
~~~ kde_version = line . split ( ) [ - 1 ] . strip ( ) \n 
~~ ~~ ~~ except ( OSError , RuntimeError ) : \n 
~~ return kde_version \n 
~~ def fixreturncode ( self , returncode ) : \n 
~~~ if returncode is not None and self . kde_version > : \n 
~~~ return returncode \n 
~~~ return os . EX_OK \n 
~~ ~~ ~~ def detect_desktop_environment ( ) : \n 
desktop_environment = \n 
if os . environ . get ( ) == : \n 
~~~ desktop_environment = \n 
~~ elif os . environ . get ( ) : \n 
~~ ~~ except ( OSError , RuntimeError ) : \n 
~~ ~~ return desktop_environment \n 
~~ def register_X_controllers ( ) : \n 
~~~ if _iscommand ( ) : \n 
~~~ _controllers [ ] = KfmClient ( ) \n 
~~ for command in ( , , ) : \n 
~~~ if _iscommand ( command ) : \n 
~~~ _controllers [ command ] = Controller ( command ) \n 
~~ ~~ ~~ def get ( ) : \n 
~~~ controllers_map = { \n 
desktop_environment = detect_desktop_environment ( ) \n 
~~~ controller_name = controllers_map [ desktop_environment ] \n 
return _controllers [ controller_name ] . open \n 
~~~ if _controllers . has_key ( ) : \n 
~~~ return _controllers [ ] . open \n 
~~~ return webbrowser . open \n 
~~ ~~ ~~ if os . environ . get ( "DISPLAY" ) : \n 
~~~ register_X_controllers ( ) \n 
~~ _open = get ( ) \n 
~~ def open ( filename ) : \n 
return _open ( filename ) \n 
~~ def _fix_addersses ( ** kwargs ) : \n 
~~~ for headername in ( , , , ) : \n 
~~~ headervalue = kwargs [ headername ] \n 
if not headervalue : \n 
~~~ del kwargs [ headername ] \n 
~~ elif not isinstance ( headervalue , basestring ) : \n 
~~~ headervalue = . join ( headervalue ) \n 
% ( headername , \n 
type ( headervalue ) . __name__ ) ) \n 
~~~ translation_map = { : , : , : } \n 
for char , replacement in translation_map . items ( ) : \n 
~~~ headervalue = headervalue . replace ( char , replacement ) \n 
~~ kwargs [ headername ] = headervalue \n 
~~ ~~ return kwargs \n 
~~ def mailto_format ( ** kwargs ) : \n 
~~~ kwargs = _fix_addersses ( ** kwargs ) \n 
parts = [ ] \n 
for headername in ( , , , , , ) : \n 
~~~ if kwargs . has_key ( headername ) : \n 
~~ if headername in ( , , , ) : \n 
~~~ parts . append ( % ( headername , headervalue ) ) \n 
parts . append ( % ( headername , headervalue ) ) \n 
~~ ~~ ~~ mailto_string = % kwargs . get ( , ) \n 
if parts : \n 
~~~ mailto_string = % ( mailto_string , . join ( parts ) ) \n 
~~ return mailto_string \n 
~~ def mailto ( address , to = None , cc = None , bcc = None , subject = None , body = None , \n 
attach = None ) : \n 
mailto_string = mailto_format ( ** locals ( ) ) \n 
return open ( mailto_string ) \n 
def loadDataSet ( fileName ) : \n 
~~~ dataMat = [ ] ; \n 
labelMat = [ ] \n 
fr = open ( fileName ) \n 
for line in fr . readlines ( ) : \n 
~~~ lineArr = line . strip ( ) . split ( ) \n 
dataMat . append ( [ float ( lineArr [ 0 ] ) , float ( lineArr [ 1 ] ) ] ) \n 
labelMat . append ( float ( lineArr [ 2 ] ) ) \n 
~~ return dataMat , labelMat \n 
~~ def selectJrand ( i , m ) : \n 
while ( j == i ) : \n 
~~~ j = int ( random . uniform ( 0 , m ) ) \n 
~~ return j \n 
~~ def clipAlpha ( aj , H , L ) : \n 
~~~ if aj > H : \n 
~~~ aj = H \n 
~~ if L > aj : \n 
~~~ aj = L \n 
~~ return aj \n 
~~ def smoSimple ( dataMatIn , classLabels , C , toler , maxIter ) : \n 
~~~ dataMatrix = mat ( dataMatIn ) ; \n 
labelMat = mat ( classLabels ) . transpose ( ) \n 
b = 0 ; \n 
m , n = shape ( dataMatrix ) \n 
alphas = mat ( zeros ( ( m , 1 ) ) ) \n 
iter = 0 \n 
while ( iter < maxIter ) : \n 
~~~ alphaPairsChanged = 0 \n 
for i in range ( m ) : \n 
~~~ fXi = float ( multiply ( alphas , labelMat ) . T * ( dataMatrix * dataMatrix [ i , : ] . T ) ) + b \n 
if ( ( labelMat [ i ] * Ei < - toler ) and ( alphas [ i ] < C ) ) or ( ( labelMat [ i ] * Ei > toler ) and ~~~ j = selectJrand ( i , m ) \n 
fXj = float ( multiply ( alphas , labelMat ) . T * ( dataMatrix * dataMatrix [ j , : ] . T ) ) + b \n 
Ej = fXj - float ( labelMat [ j ] ) \n 
alphaIold = alphas [ i ] . copy ( ) ; \n 
alphaJold = alphas [ j ] . copy ( ) ; \n 
if ( labelMat [ i ] != labelMat [ j ] ) : \n 
~~~ L = max ( 0 , alphas [ j ] - alphas [ i ] ) \n 
H = min ( C , C + alphas [ j ] - alphas [ i ] ) \n 
~~~ L = max ( 0 , alphas [ j ] + alphas [ i ] - C ) \n 
H = min ( C , alphas [ j ] + alphas [ i ] ) \n 
~~ if L == H : print "L==H" ; continue \n 
eta = 2.0 * dataMatrix [ i , : ] * dataMatrix [ j , : ] . T - dataMatrix [ i , : ] * dataMatrix [ i , dataMatrix [ j , : ] * dataMatrix [ j , : ] . T \n 
if eta >= 0 : print "eta>=0" ; continue \n 
alphas [ j ] -= labelMat [ j ] * ( Ei - Ej ) / eta \n 
alphas [ j ] = clipAlpha ( alphas [ j ] , H , L ) \n 
b1 = b - Ei - labelMat [ i ] * ( alphas [ i ] - alphaIold ) * dataMatrix [ i , : ] * dataMatrix [ labelMat [ j ] * ( alphas [ j ] - alphaJold ) * dataMatrix [ i , : ] * dataMatrix [ j , : ] . T \n 
b2 = b - Ej - labelMat [ i ] * ( alphas [ i ] - alphaIold ) * dataMatrix [ i , : ] * dataMatrix [ labelMat [ j ] * ( alphas [ j ] - alphaJold ) * dataMatrix [ j , : ] * dataMatrix [ j , : ] . T \n 
if ( 0 < alphas [ i ] ) and ( C > alphas [ i ] ) : \n 
~~~ b = b1 \n 
~~ elif ( 0 < alphas [ j ] ) and ( C > alphas [ j ] ) : \n 
~~~ b = b2 \n 
~~~ b = ( b1 + b2 ) / 2.0 \n 
~~ alphaPairsChanged += 1 \n 
~~ ~~ if ( alphaPairsChanged == 0 ) : \n 
~~~ iter += 1 \n 
~~~ iter = 0 \n 
~~ return b , alphas \n 
~~~ m , n = shape ( X ) \n 
K = mat ( zeros ( ( m , 1 ) ) ) \n 
if kTup [ 0 ] == : \n 
~~ elif kTup [ 0 ] == : \n 
~~~ for j in range ( m ) : \n 
~~~ deltaRow = X [ j , : ] - A \n 
K [ j ] = deltaRow * deltaRow . T \n 
~~~ raise NameError ( ) \n 
~~ return K \n 
~~ class optStruct : \n 
self . labelMat = classLabels \n 
self . C = C \n 
self . tol = toler \n 
self . m = shape ( dataMatIn ) [ 0 ] \n 
self . alphas = mat ( zeros ( ( self . m , 1 ) ) ) \n 
self . b = 0 \n 
self . K = mat ( zeros ( ( self . m , self . m ) ) ) \n 
for i in range ( self . m ) : \n 
~~~ self . K [ : , i ] = kernelTrans ( self . X , self . X [ i , : ] , kTup ) \n 
~~ ~~ ~~ def calcEk ( oS , k ) : \n 
~~~ fXk = float ( multiply ( oS . alphas , oS . labelMat ) . T * oS . K [ : , k ] + oS . b ) \n 
Ek = fXk - float ( oS . labelMat [ k ] ) \n 
return Ek \n 
~~~ maxK = - 1 ; \n 
maxDeltaE = 0 ; \n 
Ej = 0 \n 
validEcacheList = nonzero ( oS . eCache [ : , 0 ] . A ) [ 0 ] \n 
if ( len ( validEcacheList ) ) > 1 : \n 
Ek = calcEk ( oS , k ) \n 
deltaE = abs ( Ei - Ek ) \n 
if ( deltaE > maxDeltaE ) : \n 
~~~ maxK = k ; \n 
maxDeltaE = deltaE ; \n 
Ej = Ek \n 
~~ ~~ return maxK , Ej \n 
~~~ j = selectJrand ( i , oS . m ) \n 
Ej = calcEk ( oS , j ) \n 
~~ return j , Ej \n 
~~~ Ek = calcEk ( oS , k ) \n 
oS . eCache [ k ] = [ 1 , Ek ] \n 
~~ def innerL ( i , oS ) : \n 
~~~ Ei = calcEk ( oS , i ) \n 
if ( ( oS . labelMat [ i ] * Ei < - oS . tol ) and ( oS . alphas [ i ] < oS . C ) ) or ( \n 
( oS . labelMat [ i ] * Ei > oS . tol ) and ( oS . alphas [ i ] > 0 ) ) : \n 
alphaIold = oS . alphas [ i ] . copy ( ) ; \n 
alphaJold = oS . alphas [ j ] . copy ( ) ; \n 
if ( oS . labelMat [ i ] != oS . labelMat [ j ] ) : \n 
~~~ L = max ( 0 , oS . alphas [ j ] - oS . alphas [ i ] ) \n 
H = min ( oS . C , oS . C + oS . alphas [ j ] - oS . alphas [ i ] ) \n 
~~~ L = max ( 0 , oS . alphas [ j ] + oS . alphas [ i ] - oS . C ) \n 
H = min ( oS . C , oS . alphas [ j ] + oS . alphas [ i ] ) \n 
~~ if L == H : print "L==H" ; return 0 \n 
if eta >= 0 : print "eta>=0" ; return 0 \n 
oS . alphas [ j ] -= oS . labelMat [ j ] * ( Ei - Ej ) / eta \n 
oS . alphas [ j ] = clipAlpha ( oS . alphas [ j ] , H , L ) \n 
b2 = oS . b - Ej - oS . labelMat [ i ] * ( oS . alphas [ i ] - alphaIold ) * oS . K [ i , j ] - oS . labelMat [ j ] * oS . alphas [ j ] - alphaJold ) * oS . K [ j , j ] \n 
if ( 0 < oS . alphas [ i ] ) and ( oS . C > oS . alphas [ i ] ) : \n 
~~~ oS . b = b1 \n 
~~ elif ( 0 < oS . alphas [ j ] ) and ( oS . C > oS . alphas [ j ] ) : \n 
~~~ oS . b = b2 \n 
~~~ oS . b = ( b1 + b2 ) / 2.0 \n 
~~~ oS = optStruct ( mat ( dataMatIn ) , mat ( classLabels ) . transpose ( ) , C , toler , kTup ) \n 
entireSet = True ; \n 
alphaPairsChanged = 0 \n 
while ( iter < maxIter ) and ( ( alphaPairsChanged > 0 ) or ( entireSet ) ) : \n 
~~~ for i in range ( oS . m ) : \n 
~~~ alphaPairsChanged += innerL ( i , oS ) \n 
~~ iter += 1 \n 
~~~ nonBoundIs = nonzero ( ( oS . alphas . A > 0 ) * ( oS . alphas . A < C ) ) [ 0 ] \n 
for i in nonBoundIs : \n 
~~ if entireSet : \n 
~~ elif ( alphaPairsChanged == 0 ) : \n 
~~~ entireSet = True \n 
~~ return oS . b , oS . alphas \n 
~~ def calcWs ( alphas , dataArr , classLabels ) : \n 
~~~ X = mat ( dataArr ) ; \n 
m , n = shape ( X ) \n 
w = zeros ( ( n , 1 ) ) \n 
~~~ w += multiply ( alphas [ i ] * labelMat [ i ] , X [ i , : ] . T ) \n 
~~ return w \n 
~~ def testRbf ( k1 = 1.3 ) : \n 
~~~ dataArr , labelArr = loadDataSet ( ) \n 
datMat = mat ( dataArr ) ; \n 
labelMat = mat ( labelArr ) . transpose ( ) \n 
svInd = nonzero ( alphas . A > 0 ) [ 0 ] \n 
labelSV = labelMat [ svInd ] ; \n 
m , n = shape ( datMat ) \n 
errorCount = 0 \n 
~~~ kernelEval = kernelTrans ( sVs , datMat [ i , : ] , ( , k1 ) ) \n 
predict = kernelEval . T * multiply ( labelSV , alphas [ svInd ] ) + b \n 
if sign ( predict ) != sign ( labelArr [ i ] ) : errorCount += 1 \n 
dataArr , labelArr = loadDataSet ( ) \n 
~~ def img2vector ( filename ) : \n 
~~~ returnVect = zeros ( ( 1 , 1024 ) ) \n 
fr = open ( filename ) \n 
for i in range ( 32 ) : \n 
~~~ lineStr = fr . readline ( ) \n 
for j in range ( 32 ) : \n 
~~~ returnVect [ 0 , 32 * i + j ] = int ( lineStr [ j ] ) \n 
~~ ~~ return returnVect \n 
~~ def loadImages ( dirName ) : \n 
~~~ from os import listdir \n 
hwLabels = [ ] \n 
m = len ( trainingFileList ) \n 
trainingMat = zeros ( ( m , 1024 ) ) \n 
~~~ fileNameStr = trainingFileList [ i ] \n 
classNumStr = int ( fileStr . split ( ) [ 0 ] ) \n 
if classNumStr == 9 : \n 
~~~ hwLabels . append ( - 1 ) \n 
~~~ hwLabels . append ( 1 ) \n 
~~ trainingMat [ i , : ] = img2vector ( % ( dirName , fileNameStr ) ) \n 
~~ return trainingMat , hwLabels \n 
~~ def testDigits ( kTup = ( , 10 ) ) : \n 
~~~ dataArr , labelArr = loadImages ( ) \n 
b , alphas = smoP ( dataArr , labelArr , 200 , 0.0001 , 10000 , kTup ) \n 
sVs = datMat [ svInd ] \n 
~~~ kernelEval = kernelTrans ( sVs , datMat [ i , : ] , kTup ) \n 
dataArr , labelArr = loadImages ( ) \n 
~~ def listDirInMac ( path ) : \n 
~~~ os_list = os . listdir ( path ) \n 
for item in os_list : \n 
~~~ if item . startswith ( ) and os . path . isfile ( os . path . join ( path , item ) ) : \n 
~~~ os_list . remove ( item ) \n 
~~ ~~ return os_list \n 
class optStructK : \n 
~~ ~~ def calcEkK ( oS , k ) : \n 
~~~ fXk = float ( multiply ( oS . alphas , oS . labelMat ) . T * ( oS . X * oS . X [ k , : ] . T ) ) + oS . b \n 
~~ def innerLK ( i , oS ) : \n 
eta = 2.0 * oS . X [ i , : ] * oS . X [ j , : ] . T - oS . X [ i , : ] * oS . X [ i , : ] . T - oS . X [ j , : ] * oS . X [ j , : ] . if eta >= 0 : print "eta>=0" ; return 0 \n 
b2 = oS . b - Ej - oS . labelMat [ i ] * ( oS . alphas [ i ] - alphaIold ) * oS . X [ i , : ] * oS . X [ j , : ] . T - oS oS . alphas [ j ] - alphaJold ) * oS . X [ j , : ] * oS . X [ j , : ] . T \n 
~~~ oS = optStruct ( mat ( dataMatIn ) , mat ( classLabels ) . transpose ( ) , C , toler ) \n 
~~~ testDigits ( ( , 20 ) ) \n 
~~ import testify as T \n 
from testify . contrib . doctestcase import DocTestCase \n 
class TurtleTestCase ( T . TestCase ) : \n 
~~~ @ T . setup \n 
def build_turtle ( self ) : \n 
~~~ self . leonardo = T . turtle . Turtle ( ) \n 
~~ def test_call ( self ) : \n 
ret = self . leonardo ( ) \n 
assert ret \n 
T . assert_length ( self . leonardo . returns , 1 ) \n 
T . assert_call ( self . leonardo , 0 ) \n 
T . assert_equal ( ret , self . leonardo . returns [ 0 ] ) \n 
~~ def test_attribute ( self ) : \n 
assert self . leonardo . is_awesome ( ) . and_can_chain ( ) . whatever_he_wants ( ) \n 
~~ def test_call_record ( self ) : \n 
self . leonardo ( 1 , 2 , 3 , quatro = 4 ) \n 
T . assert_length ( self . leonardo . calls , 1 ) \n 
T . assert_call ( self . leonardo , 0 , 1 , 2 , 3 , quatro = 4 ) \n 
self . leonardo ( 5 , six = 6 ) \n 
T . assert_call ( self . leonardo , 1 , 5 , six = 6 ) \n 
~~ def test_attribute_setting ( self ) : \n 
self . leonardo . color = "blue" \n 
T . assert_equal ( self . leonardo . color , "blue" ) \n 
~~ def test_attribute_persistence ( self ) : \n 
weapon = self . leonardo . weapon \n 
T . assert_equal ( weapon , self . leonardo . weapon ) \n 
assert weapon is self . leonardo . weapon \n 
~~ ~~ class DocTest ( DocTestCase ) : \n 
~~~ module = T . turtle \n 
~~ from testify import TestCase , run \n 
class ExampleTestCase ( TestCase ) : \n 
~~~ def test_one ( self ) : \n 
~~ def test_two ( self ) : \n 
~~ ~~ class SecondTestCase ( TestCase ) : \n 
import httpretty \n 
from six . moves import cStringIO \n 
from six . moves . urllib import parse as urlparse \n 
from bravado . client import SwaggerClient \n 
from tests . functional . conftest import register_spec , API_DOCS_URL , register_get \n 
def test_form_params_in_request ( httprettified , swagger_dict ) : \n 
~~~ param1_spec = { \n 
"in" : "formData" , \n 
"name" : "param_id" , \n 
"type" : "integer" \n 
param2_spec = { \n 
"name" : "param_name" , \n 
path_spec = swagger_dict [ ] [ ] \n 
path_spec [ ] = path_spec . pop ( ) \n 
path_spec [ ] [ ] = [ param1_spec , param2_spec ] \n 
register_spec ( swagger_dict ) \n 
httpretty . register_uri ( httpretty . POST , "http://localhost/test_http?" ) \n 
resource = SwaggerClient . from_url ( API_DOCS_URL ) . api_test \n 
resource . testHTTP ( param_id = 42 , param_name = ) . result ( ) \n 
content_type = httpretty . last_request ( ) . headers [ ] \n 
assert == content_type \n 
body = urlparse . parse_qs ( httpretty . last_request ( ) . body ) \n 
assert { : [ ] , : [ ] } == body \n 
~~ def test_file_upload_in_request ( httprettified , swagger_dict ) : \n 
"name" : "file_name" , \n 
"type" : "file" \n 
path_spec [ ] [ ] = [ ] \n 
resource . testHTTP ( param_id = 42 , file_name = cStringIO ( ) ) . result ( ) \n 
assert content_type . startswith ( ) \n 
assert b"42" in httpretty . last_request ( ) . body \n 
assert b"boo" in httpretty . last_request ( ) . body \n 
~~ def test_parameter_in_path_of_request ( httprettified , swagger_dict ) : \n 
~~~ path_param_spec = { \n 
"in" : "path" , \n 
paths_spec = swagger_dict [ ] \n 
paths_spec [ ] = paths_spec . pop ( ) \n 
paths_spec [ ] [ ] [ ] . append ( \n 
path_param_spec ) \n 
register_get ( ) \n 
assert resource . testHTTP ( test_param = "foo" , param_id = "42" ) . result ( ) is None \n 
~~ def test_default_value_not_in_request ( httprettified , swagger_dict ) : \n 
~~~ swagger_dict [ ] [ ] [ ] [ ] [ 0 ] [ ] = \n 
register_get ( "http://localhost/test_http?" ) \n 
resource . testHTTP ( ) . result ( ) \n 
assert not in httpretty . last_request ( ) . querystring \n 
~~ def test_array_with_collection_format_in_path_of_request ( \n 
httprettified , swagger_dict ) : \n 
swagger_dict [ ] [ ] = swagger_dict [ ] . pop ( ) \n 
swagger_dict [ ] [ ] [ ] [ ] = [ path_param_spec ] \n 
assert resource . testHTTP ( param_ids = [ 40 , 41 , 42 ] ) . result ( ) is None \n 
def test_200_success ( petstore ) : \n 
~~~ User = petstore . get_model ( ) \n 
user = User ( \n 
id = 1 , \n 
username = , \n 
firstName = , \n 
lastName = , \n 
email = , \n 
password = , \n 
phone = , \n 
userStatus = 2 , \n 
result = petstore . user . updateUser ( username = , body = user ) . result ( ) \n 
assert result is None \n 
~~ @ pytest . mark . xfail ( reason = ) \n 
def test_404_user_not_found ( petstore ) : \n 
userStatus = 3 , \n 
result = petstore . user . updateUser ( \n 
username = , body = user ) . result ( ) \n 
def test_400_invalid_username ( petstore ) : \n 
~~~ assert False \n 
~~ EOF = \n 
def ttyflags ( fd ) : \n 
import termios as T \n 
attrs = T . tcgetattr ( fd ) \n 
attrs [ 1 ] &= ~ T . OPOST \n 
attrs [ 3 ] &= ~ T . ECHO \n 
T . tcsetattr ( fd , T . TCSANOW , attrs ) \n 
~~ def readall ( fd ) : \n 
from os import read \n 
result = \n 
~~~ chunk = read ( fd , 1 << 10 ) \n 
~~ except OSError as error : \n 
~~ ~~ if chunk == : \n 
~~~ result += chunk \n 
~~ ~~ ~~ def _test ( fd ) : \n 
ttyflags ( fd ) \n 
from os import write \n 
assert write ( fd , ) == 6 \n 
assert write ( fd , EOF * 2 ) == 2 \n 
output = readall ( fd ) \n 
assert output == , repr ( output ) \n 
~~ def test_tty ( debug_disabled ) : \n 
import pty \n 
pid , fd = pty . fork ( ) \n 
if pid == 0 : \n 
~~~ from os import execvp \n 
execvp ( , ( , ) ) \n 
~~~ _test ( fd ) \n 
import diamond . collector \n 
from diamond . collector import str_to_bool \n 
~~~ import psutil \n 
~~~ psutil = None \n 
~~ class CPUCollector ( diamond . collector . Collector ) : \n 
~~~ PROC = \n 
INTERVAL = 1 \n 
MAX_VALUES = { \n 
: diamond . collector . MAX_COUNTER , \n 
def get_default_config_help ( self ) : \n 
~~~ config_help = super ( CPUCollector , self ) . get_default_config_help ( ) \n 
config_help . update ( { \n 
return config_help \n 
~~ def get_default_config ( self ) : \n 
config = super ( CPUCollector , self ) . get_default_config ( ) \n 
config . update ( { \n 
~~ def collect ( self ) : \n 
def cpu_time_list ( ) : \n 
statFile = open ( self . PROC , "r" ) \n 
for i in range ( len ( timeList ) ) : \n 
~~~ timeList [ i ] = int ( timeList [ i ] ) \n 
~~ statFile . close ( ) \n 
return timeList \n 
~~ def cpu_delta_time ( interval ) : \n 
pre_check = cpu_time_list ( ) \n 
time . sleep ( interval ) \n 
post_check = cpu_time_list ( ) \n 
for i in range ( len ( pre_check ) ) : \n 
~~~ post_check [ i ] -= pre_check [ i ] \n 
~~ return post_check \n 
~~ if os . access ( self . PROC , os . R_OK ) : \n 
~~~ if str_to_bool ( self . config [ ] ) : \n 
~~~ dt = cpu_delta_time ( self . INTERVAL ) \n 
cpuPct = 100 - ( dt [ len ( dt ) - 1 ] * 100.00 / sum ( dt ) ) \n 
self . publish ( , str ( % cpuPct ) ) \n 
~~ results = { } \n 
file = open ( self . PROC ) \n 
ncpus = - 1 \n 
~~~ if not line . startswith ( ) : \n 
~~ ncpus += 1 \n 
elements = line . split ( ) \n 
cpu = elements [ 0 ] \n 
if cpu == : \n 
~~~ cpu = \n 
~~ elif not str_to_bool ( self . config [ ] ) : \n 
~~ results [ cpu ] = { } \n 
if len ( elements ) >= 2 : \n 
~~~ results [ cpu ] [ ] = elements [ 1 ] \n 
~~ if len ( elements ) >= 3 : \n 
~~~ results [ cpu ] [ ] = elements [ 2 ] \n 
~~ if len ( elements ) >= 4 : \n 
~~~ results [ cpu ] [ ] = elements [ 3 ] \n 
~~ if len ( elements ) >= 5 : \n 
~~~ results [ cpu ] [ ] = elements [ 4 ] \n 
~~ if len ( elements ) >= 6 : \n 
~~~ results [ cpu ] [ ] = elements [ 5 ] \n 
~~ if len ( elements ) >= 7 : \n 
~~~ results [ cpu ] [ ] = elements [ 6 ] \n 
~~ if len ( elements ) >= 8 : \n 
~~~ results [ cpu ] [ ] = elements [ 7 ] \n 
~~ if len ( elements ) >= 9 : \n 
~~~ results [ cpu ] [ ] = elements [ 8 ] \n 
~~ if len ( elements ) >= 10 : \n 
~~~ results [ cpu ] [ ] = elements [ 9 ] \n 
~~ if len ( elements ) >= 11 : \n 
~~~ results [ cpu ] [ ] = elements [ 10 ] \n 
~~ ~~ file . close ( ) \n 
metrics = { } \n 
for cpu in results . keys ( ) : \n 
~~~ stats = results [ cpu ] \n 
for s in stats . keys ( ) : \n 
~~~ metric_name = . join ( [ cpu , s ] ) \n 
if ( str_to_bool ( self . config [ ] ) \n 
and cpu == and ncpus > 0 ) : \n 
~~~ metrics [ metric_name ] = long ( stats [ s ] ) / ncpus \n 
~~~ metrics [ metric_name ] = long ( stats [ s ] ) \n 
~~ ~~ ~~ if self . config [ ] is None or self . config [ ] is True : \n 
~~~ if os . path . isdir ( ) : \n 
~~~ total = 0 \n 
for metric_name in metrics . keys ( ) : \n 
~~~ if in metric_name : \n 
~~~ total += int ( metrics [ metric_name ] ) \n 
~~ ~~ if total > 110 : \n 
~~~ self . config [ ] = True \n 
for mname in metrics . keys ( ) : \n 
~~~ if in mname : \n 
~~~ metrics [ mname ] = float ( metrics [ mname ] ) / 2 \n 
~~ ~~ ~~ elif total > 0 : \n 
~~~ self . config [ ] = False \n 
~~ ~~ for metric_name in metrics . keys ( ) : \n 
~~~ metric_value = metrics [ metric_name ] \n 
if not in metric_name : \n 
~~~ metric_name , stat = metric_name . split ( ) \n 
core = metric_name [ 3 : ] \n 
metric_name = . join ( [ , stat ] ) \n 
self . dimensions = { \n 
: str ( core ) , \n 
~~ self . publish_cumulative_counter ( metric_name , metric_value ) \n 
~~~ if not psutil : \n 
~~~ self . log . error ( ) \n 
self . log . error ( ) \n 
~~ cpu_time = psutil . cpu_times ( True ) \n 
cpu_count = len ( cpu_time ) \n 
total_time = psutil . cpu_times ( ) \n 
for i in range ( 0 , len ( cpu_time ) ) : \n 
~~~ metric_name = \n 
: str ( i ) , \n 
self . publish_cumulative_counter ( metric_name + , \n 
cpu_time [ i ] . user ) \n 
if hasattr ( cpu_time [ i ] , ) : \n 
~~~ self . dimensions = { \n 
cpu_time [ i ] . nice ) \n 
~~ self . dimensions = { \n 
cpu_time [ i ] . system ) \n 
cpu_time [ i ] . idle ) \n 
~~ metric_name = \n 
total_time . user / cpu_count ) \n 
if hasattr ( total_time , ) : \n 
~~~ self . publish_cumulative_counter ( metric_name + , \n 
total_time . nice / cpu_count ) \n 
~~ self . publish_cumulative_counter ( metric_name + , \n 
total_time . system / cpu_count ) \n 
total_time . idle / cpu_count ) \n 
~~ ~~ from test import CollectorTestCase \n 
from test import get_collector_config \n 
from test import unittest \n 
from mock import Mock \n 
from diamond . collector import Collector \n 
from kafka_jolokia import KafkaJolokiaCollector \n 
def find_metric ( metric_list , metric_name ) : \n 
~~~ return filter ( lambda metric : metric [ "name" ] . find ( metric_name ) > - 1 , metric_list ) \n 
~~ def find_by_dimension ( metric_list , key , val ) : \n 
~~~ return filter ( lambda metric : metric [ "dimensions" ] [ key ] == val , metric_list ) [ 0 ] \n 
~~ def list_request ( ) : \n 
~~~ return { : { : } , : 200 } \n 
~~ class TestKafkaJolokiaCollector ( CollectorTestCase ) : \n 
~~~ config = get_collector_config ( , { } ) \n 
self . collector = KafkaJolokiaCollector ( config , None ) \n 
self . collector . list_request = list_request \n 
~~ def test_import ( self ) : \n 
~~~ self . assertTrue ( KafkaJolokiaCollector ) \n 
~~ @ patch . object ( Collector , ) \n 
def test_should_create_type ( self , publish_mock ) : \n 
~~~ def se ( url ) : \n 
~~~ return self . getFixture ( "kafka_server.json" ) \n 
~~ patch_urlopen = patch ( , Mock ( side_effect = se ) ) \n 
with patch_urlopen : \n 
~~~ self . collector . collect ( ) \n 
self . assertEquals ( len ( self . collector . payload ) , 24 ) \n 
~~ metrics = find_metric ( self . collector . payload , "kafka.server.BrokerTopicMetrics.MessagesInPerSec.count" self . assertNotEqual ( len ( metrics ) , 0 ) \n 
metric = find_by_dimension ( metrics , "topic" , "foobar" ) \n 
self . assertEquals ( metric [ "type" ] , "CUMCOUNTER" ) \n 
metrics_dots = find_metric ( self . collector . payload , \n 
"kafka.server.KafkaServer4.2.BrokerState.value" ) \n 
self . assertNotEqual ( len ( metrics_dots ) , 0 ) \n 
def test_blacklisting ( self , publish_mock ) : \n 
~~ metrics = find_metric ( self . collector . payload , "kafka.server.BrokerTopicMetrics.MessagesInPerSec.meanrate" self . assertEquals ( len ( metrics ) , 0 ) \n 
def test_total_topic ( self , publish_mock ) : \n 
~~ metrics = find_metric ( self . collector . payload , "kafka.server.BrokerTopicMetrics.BytesRejectedPerSec.count" self . assertNotEqual ( len ( metrics ) , 0 ) \n 
metric = find_by_dimension ( metrics , "topic" , "_TOTAL_" ) \n 
~~ def get_port_stats ( port ) : \n 
cnts = defaultdict ( int ) \n 
for c in psutil . net_connections ( ) : \n 
~~~ c_port = c . laddr [ 1 ] \n 
if c_port != port : \n 
~~ status = c . status . lower ( ) \n 
cnts [ status ] += 1 \n 
~~ return cnts \n 
~~ class PortStatCollector ( diamond . collector . Collector ) : \n 
~~~ def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( PortStatCollector , self ) . __init__ ( * args , ** kwargs ) \n 
self . ports = { } \n 
for port_name , cfg in self . config [ ] . items ( ) : \n 
~~~ port_cfg = { } \n 
for key in ( , ) : \n 
~~~ port_cfg [ key ] = cfg . get ( key , [ ] ) \n 
~~ self . ports [ port_name ] = port_cfg \n 
~~ ~~ def get_default_config_help ( self ) : \n 
~~~ config_help = super ( PortStatCollector , self ) . get_default_config_help ( ) \n 
~~~ config = super ( PortStatCollector , self ) . get_default_config ( ) \n 
if psutil is None : \n 
~~ for port_name , port_cfg in self . ports . iteritems ( ) : \n 
~~~ port = int ( port_cfg [ ] ) \n 
stats = get_port_stats ( port ) \n 
for stat_name , stat_value in stats . iteritems ( ) : \n 
~~~ metric_name = % ( port_name , stat_name ) \n 
self . publish ( metric_name , stat_value ) \n 
~~ ~~ ~~ ~~ import time \n 
from error import DiamondException \n 
class Metric ( object ) : \n 
~~~ _METRIC_TYPES = [ , , ] \n 
def __init__ ( self , path , value , raw_value = None , timestamp = None , precision = 0 , \n 
metric_type = , ttl = None , host = "ignored" , dimensions = None ) : \n 
if ( None in [ path , value ] or metric_type not in self . _METRIC_TYPES ) : \n 
% ( path , value , metric_type ) ) \n 
~~ if timestamp is None : \n 
~~~ timestamp = int ( time . time ( ) ) \n 
~~~ if not isinstance ( timestamp , int ) : \n 
~~~ timestamp = int ( timestamp ) \n 
% ( path , e ) ) \n 
~~ ~~ ~~ if not isinstance ( value , ( int , float ) ) : \n 
~~~ if precision == 0 : \n 
~~~ value = round ( float ( value ) ) \n 
~~~ value = float ( value ) \n 
~~ ~~ if dimensions is not None : \n 
~~~ if not isinstance ( dimensions , dict ) : \n 
% ( path , dimensions ) ) \n 
~~~ dimensions = dict ( \n 
( k , str ( v ) ) for k , v in dimensions . iteritems ( ) \n 
if v is not None and isinstance ( v , ( int , float , str , unicode ) ) and k is not None ) \n 
~~ ~~ self . dimensions = dimensions \n 
self . raw_value = raw_value \n 
self . timestamp = timestamp \n 
self . precision = precision \n 
self . metric_type = metric_type \n 
self . ttl = ttl \n 
if not isinstance ( self . precision , ( int , long ) ) : \n 
~~~ log = logging . getLogger ( ) \n 
log . warn ( , self . path ) \n 
self . precision = 0 \n 
return fstring % ( self . path , self . value , self . timestamp ) \n 
def parse ( cls , string ) : \n 
match = re . match ( \n 
+ , \n 
string ) \n 
~~~ groups = match . groupdict ( ) \n 
return Metric ( groups [ ] , \n 
groups [ ] , \n 
float ( groups [ ] ) ) \n 
~~~ raise DiamondException ( \n 
~~ ~~ def getPathPrefix ( self ) : \n 
return self . path . split ( ) [ 0 ] \n 
~~ def getCollectorPath ( self ) : \n 
return self . path . split ( ) [ 1 ] \n 
~~ def getMetricPath ( self ) : \n 
path = self . path . split ( ) [ 2 : ] \n 
return . join ( path ) \n 
~~ ~~ from __future__ import absolute_import \n 
from git_code_debt . util . iter import chunk_iter \n 
from git_code_debt . util . subprocess import cmd_output \n 
Commit = collections . namedtuple ( , [ , ] ) \n 
COMMIT_FORMAT = \n 
class RepoParser ( object ) : \n 
~~~ def __init__ ( self , git_repo ) : \n 
~~~ self . git_repo = git_repo \n 
self . tempdir = None \n 
~~ @ contextlib . contextmanager \n 
def repo_checked_out ( self ) : \n 
~~~ assert not self . tempdir \n 
self . tempdir = tempfile . mkdtemp ( suffix = ) \n 
~~~ subprocess . check_call ( \n 
self . git_repo , self . tempdir , \n 
stdout = None , \n 
~~~ shutil . rmtree ( self . tempdir ) \n 
~~ ~~ def get_commit ( self , sha ) : \n 
~~~ output = cmd_output ( \n 
, , COMMIT_FORMAT , sha , \n 
cwd = self . tempdir , \n 
sha , date = output . splitlines ( ) [ : 2 ] \n 
return Commit ( sha , int ( date ) ) \n 
~~ def get_commits ( self , since_sha = None ) : \n 
assert self . tempdir \n 
cmd = [ , , , , COMMIT_FORMAT ] \n 
if since_sha : \n 
~~~ commits = [ self . get_commit ( since_sha ) ] \n 
cmd . append ( . format ( since_sha ) ) \n 
~~~ commits = [ ] \n 
cmd . append ( ) \n 
~~ output = cmd_output ( * cmd , cwd = self . tempdir ) \n 
for sha , date in chunk_iter ( output . splitlines ( ) , 2 ) : \n 
~~~ commits . append ( Commit ( sha , int ( date ) ) ) \n 
~~ return commits \n 
~~ def get_original_commit ( self , sha ) : \n 
~~~ assert self . tempdir \n 
output = cmd_output ( \n 
, , sha , cwd = self . tempdir , encoding = None , \n 
return output \n 
~~ def get_commit_diff ( self , previous_sha , sha ) : \n 
, , previous_sha , sha , cwd = self . tempdir , encoding = None , \n 
import io \n 
from git_code_debt . create_tables import create_schema \n 
from git_code_debt . create_tables import populate_metric_ids \n 
from git_code_debt . repo_parser import Commit \n 
from git_code_debt . repo_parser import COMMIT_FORMAT \n 
from testing . utilities . auto_namedtuple import auto_namedtuple \n 
from testing . utilities . cwd import cwd \n 
@ pytest . yield_fixture \n 
def tempdir_factory ( tmpdir ) : \n 
~~~ class TmpdirFactory ( object ) : \n 
~~~ self . tmpdir_count = 0 \n 
~~~ path = tmpdir . join ( six . text_type ( self . tmpdir_count ) ) . strpath \n 
self . tmpdir_count += 1 \n 
os . mkdir ( path ) \n 
return path \n 
~~ ~~ yield TmpdirFactory ( ) \n 
~~ class Sandbox ( collections . namedtuple ( , [ ] ) ) : \n 
def db_path ( self ) : \n 
~~~ return os . path . join ( self . directory , ) \n 
def db ( self ) : \n 
~~~ with sqlite3 . connect ( self . db_path ) as db : \n 
~~~ yield db \n 
~~ ~~ ~~ @ pytest . yield_fixture \n 
def sandbox ( tempdir_factory ) : \n 
~~~ ret = Sandbox ( tempdir_factory . get ( ) ) \n 
with ret . db ( ) as db : \n 
~~~ create_schema ( db ) \n 
populate_metric_ids ( db , tuple ( ) , False ) \n 
~~ yield ret \n 
~~ @ pytest . yield_fixture \n 
def cloneable ( tempdir_factory ) : \n 
~~~ repo_path = tempdir_factory . get ( ) \n 
with cwd ( repo_path ) : \n 
~~~ subprocess . check_call ( ( , , ) ) \n 
subprocess . check_call ( ( , , , , ) ) \n 
~~ yield repo_path \n 
def cloneable_with_commits ( cloneable ) : \n 
def append_commit ( ) : \n 
~~~ output = cmd_output ( , , COMMIT_FORMAT ) \n 
commits . append ( Commit ( sha , int ( date ) ) ) \n 
~~ def make_commit ( filename , contents ) : \n 
~~~ with io . open ( filename , ) as file_obj : \n 
~~~ file_obj . write ( contents ) \n 
~~ subprocess . check_call ( ( , , filename ) ) \n 
subprocess . check_call ( ( \n 
, , , . format ( filename ) , \n 
append_commit ( ) \n 
~~ with cwd ( cloneable ) : \n 
~~~ append_commit ( ) \n 
make_commit ( , ) \n 
~~ yield auto_namedtuple ( path = cloneable , commits = commits ) \n 
import flask \n 
def test_healthcheck ( server ) : \n 
~~~ server . client . get ( flask . url_for ( ) ) \n 
from mrjob . job import MRJob \n 
from mrjob . step import MRStep \n 
WORD_RE = re . compile ( r"[\\w\']+" ) \n 
class MRNextWordStats ( MRJob ) : \n 
~~~ SORT_VALUES = True \n 
def steps ( self ) : \n 
~~~ return [ MRStep ( mapper = self . m_find_words , \n 
combiner = self . c_combine_counts , \n 
reducer = self . r_sum_counts ) , \n 
MRStep ( reducer = self . r_compute_stats ) ] \n 
~~ def m_find_words ( self , _ , line ) : \n 
prev_word = None \n 
for word in WORD_RE . findall ( line ) : \n 
~~~ word = word . lower ( ) \n 
if prev_word is not None : \n 
~~~ yield ( prev_word , ) , 1 \n 
yield ( prev_word , word ) , 1 \n 
~~ prev_word = word \n 
~~ ~~ def c_combine_counts ( self , key , counts ) : \n 
yield key , sum ( counts ) \n 
~~ def r_sum_counts ( self , key , counts ) : \n 
count = sum ( counts ) \n 
prev_word , word = key \n 
if word == : \n 
~~~ yield prev_word , ( , count ) \n 
~~~ yield prev_word , ( , ( word , count ) ) \n 
~~ ~~ def r_compute_stats ( self , prev_word , value ) : \n 
total = None \n 
for value_type , data in value : \n 
~~~ if value_type == : \n 
~~~ total = data \n 
~~~ assert value_type == \n 
word , count = data \n 
percent = 100.0 * count / total \n 
yield ( prev_word , word ) , ( total , count , percent ) \n 
~~~ MRNextWordStats . run ( ) \n 
import posixpath \n 
from mrjob . util import file_ext \n 
from . ids import _add_implied_task_id \n 
from . ids import _to_job_id \n 
from . log4j import _parse_hadoop_log4j_records \n 
from . wrap import _cat_log \n 
from . wrap import _ls_logs \n 
_JAVA_TRACEBACK_RE = re . compile ( \n 
re . MULTILINE ) \n 
_OPENING_FOR_READING_RE = re . compile ( \n 
_PRE_YARN_TASK_SYSLOG_PATH_RE = re . compile ( \n 
_TASK_STDERR_IGNORE_RE = re . compile ( ) \n 
_YARN_INPUT_SPLIT_RE = re . compile ( \n 
_YARN_TASK_SYSLOG_PATH_RE = re . compile ( \n 
def _ls_task_syslogs ( fs , log_dir_stream , application_id = None , job_id = None ) : \n 
return _ls_logs ( fs , log_dir_stream , _match_task_syslog_path , \n 
application_id = application_id , \n 
job_id = job_id ) \n 
~~ def _match_task_syslog_path ( path , application_id = None , job_id = None ) : \n 
m = _PRE_YARN_TASK_SYSLOG_PATH_RE . match ( path ) \n 
~~~ if job_id and job_id != _to_job_id ( m . group ( ) ) : \n 
~~ return dict ( \n 
attempt_id = m . group ( ) ) \n 
~~ m = _YARN_TASK_SYSLOG_PATH_RE . match ( path ) \n 
~~~ if application_id and application_id != m . group ( ) : \n 
application_id = m . group ( ) , \n 
container_id = m . group ( ) ) \n 
~~ def _interpret_task_logs ( fs , matches , partial = True , stderr_callback = None ) : \n 
for match in matches : \n 
~~~ syslog_path = match [ ] \n 
error = _parse_task_syslog ( _cat_log ( fs , syslog_path ) ) \n 
if not error . get ( ) : \n 
~~ error [ ] [ ] = syslog_path \n 
for id_key in , , : \n 
~~~ if id_key in match : \n 
~~~ error [ id_key ] = match [ id_key ] \n 
~~ ~~ _add_implied_task_id ( error ) \n 
stderr_path = _syslog_to_stderr_path ( syslog_path ) \n 
if fs . exists ( stderr_path ) : \n 
~~~ if stderr_callback : \n 
~~~ stderr_callback ( stderr_path ) \n 
~~ task_error = _parse_task_stderr ( _cat_log ( fs , stderr_path ) ) \n 
if task_error : \n 
~~~ task_error [ ] = stderr_path \n 
error [ ] = task_error \n 
~~ ~~ result . setdefault ( , [ ] ) \n 
result [ ] . append ( error ) \n 
if partial : \n 
~~~ result [ ] = True \n 
~~ def _syslog_to_stderr_path ( path ) : \n 
stem , filename = posixpath . split ( path ) \n 
return posixpath . join ( stem , + file_ext ( filename ) ) \n 
~~ def _parse_task_syslog ( lines ) : \n 
for record in _parse_hadoop_log4j_records ( lines ) : \n 
~~~ message = record [ ] \n 
m = _OPENING_FOR_READING_RE . match ( message ) \n 
~~~ result [ ] = dict ( path = m . group ( ) ) \n 
~~ m = _YARN_INPUT_SPLIT_RE . match ( message ) \n 
~~~ result [ ] = dict ( \n 
path = m . group ( ) , \n 
start_line = int ( m . group ( ) ) , \n 
num_lines = int ( m . group ( ) ) ) \n 
~~ m = _JAVA_TRACEBACK_RE . search ( message ) \n 
message = message , \n 
num_lines = record [ ] , \n 
start_line = record [ ] , \n 
~~ def _parse_task_stderr ( lines ) : \n 
task_error = None \n 
for line_num , line in enumerate ( lines ) : \n 
~~~ line = line . rstrip ( ) \n 
if _TASK_STDERR_IGNORE_RE . match ( line ) : \n 
~~~ if task_error and not in task_error : \n 
~~~ task_error [ ] = line_num - task_error [ ] \n 
~~ elif not task_error or line . startswith ( ) : \n 
~~~ task_error = dict ( \n 
message = line , \n 
start_line = line_num ) \n 
~~~ task_error [ ] += + line \n 
~~ ~~ if task_error : \n 
~~~ if not in task_error : \n 
~~~ task_error [ ] = line_num + 1 - task_error [ ] \n 
~~ return task_error \n 
~~ ~~ from io import BytesIO \n 
from tests . py2 import mock_stdout_or_stderr \n 
from tests . sandbox import SandboxedTestCase \n 
class MockSubprocessTestCase ( SandboxedTestCase ) : \n 
~~~ def mock_popen ( self , module , main_func , env ) : \n 
PopenClass = self . _make_popen_class ( main_func , env ) \n 
original_popen = module . Popen \n 
module . Popen = PopenClass \n 
self . addCleanup ( setattr , module , , original_popen ) \n 
~~ def _make_popen_class ( outer , func , env ) : \n 
~~~ class MockPopen ( object ) : \n 
~~~ def __init__ ( self , args , stdin = None , stdout = None , stderr = None ) : \n 
~~~ self . args = args \n 
self . stdin = BytesIO ( ) \n 
self . stdout = BytesIO ( ) \n 
self . stderr = BytesIO ( ) \n 
self . _run ( ) \n 
~~ def _run ( self ) : \n 
~~~ stdout = mock_stdout_or_stderr ( ) \n 
stderr = mock_stdout_or_stderr ( ) \n 
self . returncode = func ( \n 
self . stdin , stdout , stderr , self . args , env ) \n 
self . stdout = BytesIO ( stdout . getvalue ( ) ) \n 
self . stderr = BytesIO ( stderr . getvalue ( ) ) \n 
~~ def communicate ( self , input = None ) : \n 
~~~ return self . stdout . getvalue ( ) , self . stderr . getvalue ( ) \n 
~~ def wait ( self ) : \n 
~~~ return self . returncode \n 
~~ ~~ return MockPopen \n 
class MRSortValues ( MRJob ) : \n 
def mapper_init ( self ) : \n 
~~~ MRSortValues . run ( ) \n 
~~ from mrjob . retry import RetryGoRound \n 
from mrjob . retry import RetryWrapper \n 
from tests . py2 import Mock \n 
from tests . py2 import TestCase \n 
class RetryGoRoundTestCase ( TestCase ) : \n 
~~~ def test_empty ( self ) : \n 
~~~ self . assertRaises ( \n 
ValueError , RetryGoRound , [ ] , lambda ex : isinstance ( ex , IOError ) ) \n 
~~ def test_success ( self ) : \n 
~~~ a1 = Mock ( ) \n 
a1 . f = Mock ( __name__ = , return_value = 1 ) \n 
a2 = Mock ( ) \n 
a2 . f = Mock ( __name__ = , return_value = 2 ) \n 
a = RetryGoRound ( [ a1 , a2 ] , lambda ex : isinstance ( ex , IOError ) ) \n 
self . assertEqual ( a . f ( ) , 1 ) \n 
self . assertEqual ( a1 . f . call_count , 1 ) \n 
self . assertEqual ( a2 . f . call_count , 0 ) \n 
~~ def test_one_failure ( self ) : \n 
a1 . f = Mock ( __name__ = , side_effect = IOError ) \n 
a1 . x = 100 \n 
a2 . x = 200 \n 
self . assertEqual ( a . x , 100 ) \n 
self . assertEqual ( a . f ( ) , 2 ) \n 
self . assertEqual ( a . x , 200 ) \n 
self . assertEqual ( a2 . f . call_count , 2 ) \n 
~~ def test_all_fail ( self ) : \n 
a2 . f = Mock ( __name__ = , side_effect = IOError ) \n 
self . assertRaises ( IOError , a . f ) \n 
self . assertEqual ( a1 . f . call_count , 2 ) \n 
~~ def test_unrecoverable_error ( self ) : \n 
a1 . f = Mock ( __name__ = , side_effect = ValueError ) \n 
self . assertRaises ( ValueError , a . f ) \n 
~~ def test_can_wrap_around ( self ) : \n 
a1 . f = Mock ( __name__ = , side_effect = [ IOError , 1 ] ) \n 
a2 . f = Mock ( __name__ = , side_effect = [ 2 , IOError ] ) \n 
~~ def test_wrapping ( self ) : \n 
self . assertEqual ( a . f ( , bar = ) , 2 ) \n 
a1 . f . assert_called_once_with ( , bar = ) \n 
a2 . f . assert_called_once_with ( , bar = ) \n 
self . assertEqual ( a . f . __name__ , ) \n 
~~ ~~ class RetryWrapperTestCase ( TestCase ) : \n 
~~~ def test_success ( self ) : \n 
a1 . f = Mock ( __name__ = , side_effect = None ) \n 
a = RetryWrapper ( \n 
a1 , \n 
retry_if = lambda x : True , \n 
backoff = 0.0001 , \n 
max_tries = 2 \n 
a . f ( ) \n 
a1 . f . assert_called_once_with ( ) \n 
~~ def test_failure ( self ) : \n 
~~ def test_failure_raises_if_all_tries_fail ( self ) : \n 
a1 . f = Mock ( __name__ = , side_effect = [ IOError , IOError ] ) \n 
with self . assertRaises ( IOError ) : \n 
~~~ a . f ( ) \n 
~~ self . assertEqual ( a1 . f . call_count , 2 ) \n 
~~ def test_try_till_success ( self ) : \n 
a1 . f = Mock ( __name__ = , side_effect = [ IOError , IOError , None ] ) \n 
max_tries = 0 \n 
self . assertEqual ( a1 . f . call_count , 3 ) \n 
~~ ~~ from behave import then \n 
from behave import when \n 
from paasta_tools import chronos_tools \n 
def create_trivial_chronos_job ( context , job_name ) : \n 
~~~ job_config = { \n 
context . jobs [ job_name ] = job_config \n 
context . chronos_client . add ( job_config ) \n 
context . chronos_job_name = job_config [ ] \n 
~~ @ when ( def create_chronos_job_config_object_from_configs ( context , service , instance , job_name ) : \n 
~~~ job_config = chronos_tools . create_complete_config ( \n 
service = service , \n 
job_name = instance , \n 
soa_dir = context . soa_dir , \n 
chronos_tools . service_configuration_lib . _yaml_cache = { } \n 
~~ @ when ( ) \n 
def send_job_to_chronos ( context ) : \n 
~~~ context . chronos_client . add ( context . chronos_job_config ) \n 
def chronos_job_is_ready ( context , job_name ) : \n 
chronos_tools . wait_for_job ( context . chronos_client , context . jobs [ job_name ] [ ] ) \n 
def list_chronos_jobs_has_job ( context , should_or_not ) : \n 
~~~ jobs = context . chronos_client . list ( ) \n 
job_names = [ job [ ] for job in jobs ] \n 
~~~ assert context . chronos_job_name not in job_names \n 
~~~ assert context . chronos_job_name in job_names \n 
def chronos_check_running_tasks ( context , old_or_new_job , has_or_not ) : \n 
~~~ assert True \n 
def chronos_check_job_state ( context , field , job_name , value ) : \n 
~~~ job_id = context . jobs [ job_name ] [ ] \n 
service , instance = chronos_tools . decompose_job_id ( job_id ) \n 
jobs = chronos_tools . lookup_chronos_jobs ( \n 
instance = instance , \n 
client = context . chronos_client , \n 
include_disabled = True \n 
assert len ( jobs ) == 1 \n 
assert str ( jobs [ 0 ] [ field ] ) == value \n 
def job_is_disabled ( context , job_name , disabled ) : \n 
~~~ is_disabled = True if disabled == "disabled" else False \n 
full_job_name = context . jobs [ job_name ] [ "name" ] \n 
all_jobs = context . chronos_client . list ( ) \n 
filtered_jobs = [ job for job in all_jobs if job [ "name" ] == full_job_name ] \n 
assert filtered_jobs [ 0 ] [ "disabled" ] is is_disabled \n 
def job_exists ( context , job_name ) : \n 
~~~ all_jobs = context . chronos_client . list ( ) \n 
assert any ( [ job [ ] == job_name for job in all_jobs ] ) \n 
~~ from paasta_tools . cli . utils import execute_paasta_serviceinit_on_remote_master \n 
from paasta_tools . cli . utils import figure_out_service_name \n 
from paasta_tools . cli . utils import lazy_choices_completer \n 
from paasta_tools . cli . utils import list_instances \n 
from paasta_tools . cli . utils import list_services \n 
from paasta_tools . utils import compose_job_id \n 
from paasta_tools . utils import DEFAULT_SOA_DIR \n 
from paasta_tools . utils import list_clusters \n 
def add_subparser ( subparsers ) : \n 
~~~ status_parser = subparsers . add_parser ( \n 
description = ( \n 
epilog = ( \n 
status_parser . add_argument ( \n 
) . completer = lazy_choices_completer ( list_services ) \n 
) . completer = lazy_choices_completer ( list_instances ) \n 
) . completer = lazy_choices_completer ( list_clusters ) \n 
default = DEFAULT_SOA_DIR , \n 
status_parser . set_defaults ( command = paasta_emergency_scale ) \n 
~~ def paasta_emergency_scale ( args ) : \n 
service = figure_out_service_name ( args , soa_dir = args . yelpsoa_config_root ) \n 
output = execute_paasta_serviceinit_on_remote_master ( , args . cluster , service , args . instance app_id = args . appid , delta = args . delta ) \n 
print "%s" % "\\n" . join ( paasta_emergency_scale . __doc__ . splitlines ( ) [ - 7 : ] ) \n 
~~ import argparse \n 
from dateutil import tz \n 
from pytimeparse import timeparse \n 
from paasta_tools import marathon_tools \n 
def parse_args ( ) : \n 
parser . add_argument ( , , dest = , type = timedelta_type , default = , \n 
parser . add_argument ( , , action = "store_true" , \n 
parser . add_argument ( , , action = ) \n 
options = parser . parse_args ( ) \n 
return options \n 
~~ def timedelta_type ( value ) : \n 
~~ return datetime_seconds_ago ( timeparse . timeparse ( value ) ) \n 
~~ def datetime_seconds_ago ( seconds ) : \n 
~~~ return now ( ) - datetime . timedelta ( seconds = seconds ) \n 
~~ def now ( ) : \n 
~~~ return datetime . datetime . now ( tz . tzutc ( ) ) \n 
~~ def delete_deployment_if_too_old ( client , deployment , max_date , dry_run ) : \n 
~~~ started_at = dateutil . parser . parse ( deployment . version ) \n 
age = now ( ) - started_at \n 
if started_at < max_date : \n 
~~~ if dry_run is True : \n 
~~~ args = parse_args ( ) \n 
config = marathon_tools . load_marathon_config ( ) \n 
client = marathon_tools . get_marathon_client ( config . get_url ( ) , config . get_username ( ) , config . get_password \n 
for deployment in client . list_deployments ( ) : \n 
~~~ delete_deployment_if_too_old ( \n 
client = client , \n 
deployment = deployment , \n 
max_date = args . age , \n 
dry_run = args . dry_run , \n 
~~ from contextlib import nested \n 
from pytest import raises \n 
from paasta_tools . cli . fsm import autosuggest \n 
class TestGetSmartstackProxyPortFromFile : \n 
~~~ def test_multiple_stanzas_per_file ( self ) : \n 
~~~ with nested ( \n 
mock . patch ( "__builtin__.open" , autospec = True ) , \n 
mock . patch ( "paasta_tools.cli.fsm.autosuggest.yaml" , autospec = True ) , \n 
) as ( \n 
mock_open , \n 
mock_yaml , \n 
~~~ mock_yaml . load . return_value = { \n 
"main" : { \n 
"proxy_port" : 1 , \n 
"foo" : { \n 
"proxy_port" : 2 , \n 
actual = autosuggest . _get_smartstack_proxy_port_from_file ( \n 
"fake_root" , \n 
"smartstack.yaml" , \n 
assert actual == 2 \n 
~~ ~~ ~~ class TestSuggestSmartstackProxyPort : \n 
~~~ def test_suggest_smartstack_proxy_port ( self ) : \n 
~~~ yelpsoa_config_root = "fake_yelpsoa_config_root" \n 
walk_return = [ \n 
( "fake_root1" , "fake_dir1" , [ "service.yaml" ] ) , \n 
( "fake_root2" , "fake_dir2" , [ "smartstack.yaml" ] ) , \n 
( "fake_root3" , "fake_dir3" , [ "service.yaml" ] ) , \n 
mock_walk = mock . Mock ( return_value = walk_return ) \n 
20001 , \n 
20002 , \n 
def get_smarstack_proxy_port_from_file_side_effect ( * args ) : \n 
~~~ return get_smartstack_proxy_port_from_file_returns . pop ( 0 ) \n 
~~ mock_get_smartstack_proxy_port_from_file = mock . Mock ( side_effect = get_smarstack_proxy_port_from_file_side_effect with nested ( \n 
mock . patch ( "os.walk" , mock_walk ) , \n 
mock . patch ( "paasta_tools.cli.fsm.autosuggest._get_smartstack_proxy_port_from_file" , \n 
mock_get_smartstack_proxy_port_from_file ) , \n 
~~ assert mock_get_smartstack_proxy_port_from_file . call_count == 3 \n 
~~ def test_suggest_smartstack_proxy_port_too_many_services ( self ) : \n 
yelpsoa_config_root = "fake_yelpsoa_config_root" \n 
~~~ with raises ( Exception ) as exc : \n 
~~~ autosuggest . suggest_smartstack_proxy_port ( yelpsoa_config_root , range_min = 20001 , \n 
range_max = 20002 ) \n 
from paasta_tools . monitoring . replication_utils import backend_is_up \n 
from paasta_tools . monitoring . replication_utils import get_registered_marathon_tasks \n 
from paasta_tools . monitoring . replication_utils import get_replication_for_services \n 
from paasta_tools . monitoring . replication_utils import ip_port_hostname_from_svname \n 
from paasta_tools . monitoring . replication_utils import match_backends_and_tasks \n 
from paasta_tools . utils import DEFAULT_SYNAPSE_HAPROXY_URL_FORMAT \n 
def test_get_replication_for_service ( ) : \n 
~~~ testdir = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
testdata = os . path . join ( testdir , ) \n 
with open ( testdata , ) as fd : \n 
~~~ mock_haproxy_data = fd . read ( ) \n 
~~ mock_response = mock . Mock ( ) \n 
mock_response . text = mock_haproxy_data \n 
mock_get = mock . Mock ( return_value = ( mock_response ) ) \n 
with mock . patch . object ( requests . Session , , mock_get ) : \n 
~~~ replication_result = get_replication_for_services ( \n 
6666 , \n 
DEFAULT_SYNAPSE_HAPROXY_URL_FORMAT , \n 
[ , , , ] \n 
expected = { \n 
: 18 , \n 
: 19 , \n 
: 3 \n 
assert expected == replication_result \n 
~~ ~~ def test_get_registered_marathon_tasks ( ) : \n 
~~~ backends = [ \n 
{ "pxname" : "servicename.main" , "svname" : "10.50.2.4:31000_box4" , "status" : "UP" } , \n 
{ "pxname" : "servicename.main" , "svname" : "10.50.2.5:31001_box5" , "status" : "UP" } , \n 
{ "pxname" : "servicename.main" , "svname" : "10.50.2.6:31001_box6" , "status" : "UP" } , \n 
{ "pxname" : "servicename.main" , "svname" : "10.50.2.6:31002_box7" , "status" : "UP" } , \n 
{ "pxname" : "servicename.main" , "svname" : "10.50.2.8:31000_box8" , "status" : "UP" } , \n 
hostnames = { \n 
good_task1 = mock . Mock ( host = , ports = [ 31000 ] ) \n 
good_task2 = mock . Mock ( host = , ports = [ 31001 ] ) \n 
bad_task = mock . Mock ( host = , ports = [ 31000 ] ) \n 
marathon_tasks = [ \n 
good_task1 , \n 
good_task2 , \n 
bad_task , \n 
with mock . patch ( \n 
return_value = backends \n 
) as mock_get_multiple_backends : \n 
~~~ with mock . patch ( \n 
side_effect = lambda x : hostnames [ x ] , \n 
~~~ actual = get_registered_marathon_tasks ( \n 
marathon_tasks , \n 
expected = [ good_task1 , good_task2 ] \n 
assert actual == expected \n 
mock_get_multiple_backends . assert_called_once_with ( \n 
[ ] , \n 
synapse_host = , \n 
synapse_port = 6666 , \n 
synapse_haproxy_url_format = DEFAULT_SYNAPSE_HAPROXY_URL_FORMAT , \n 
~~ ~~ ~~ def test_backend_is_up ( ) : \n 
~~~ assert True is backend_is_up ( { "status" : "UP" } ) \n 
assert False is backend_is_up ( { "status" : "DOWN" } ) \n 
assert False is backend_is_up ( { "status" : "MAINT" } ) \n 
~~ def test_ip_port_hostname_from_svname ( ) : \n 
~~~ assert ( "1.2.3.4" , 5 , "six" ) == ip_port_hostname_from_svname ( "1.2.3.4:5_six" ) \n 
~~ def test_match_backends_and_tasks ( ) : \n 
tasks = [ good_task1 , good_task2 , bad_task ] \n 
~~~ expected = [ \n 
( backends [ 0 ] , good_task1 ) , \n 
( backends [ 1 ] , good_task2 ) , \n 
( None , bad_task ) , \n 
( backends [ 2 ] , None ) , \n 
( backends [ 3 ] , None ) , \n 
( backends [ 4 ] , None ) , \n 
actual = match_backends_and_tasks ( backends , tasks ) \n 
assert sorted ( actual ) == sorted ( expected ) \n 
from apparent_temperature . measure_generator import MeasureGeneratorSpout \n 
HumidityMeasure = namedtuple ( \n 
"HumidityMeasure" , \n 
class HumiditySpout ( MeasureGeneratorSpout ) : \n 
~~~ OUTPUT_FIELDS = HumidityMeasure \n 
SENSORS = { \n 
1042 : ( 56 , 17 ) , \n 
1077 : ( 47 , 22 ) , \n 
1078 : ( 22 , 19 ) , \n 
1079 : ( 12 , 15 ) , \n 
1082 : ( 67 , 15 ) , \n 
1126 : ( 70 , 12 ) , \n 
1156 : ( 51 , 19 ) , \n 
1178 : ( 43 , 14 ) , \n 
1201 : ( 57 , 11 ) , \n 
1234 : ( 55 , 7 ) , \n 
1312 : ( 12 , 9 ) , \n 
1448 : ( 56 , 22 ) , \n 
2089 : ( 32 , 30 ) , \n 
def measure ( self , * args ) : \n 
~~~ return min ( 100 , random . normalvariate ( * args ) ) \n 
~~ def log ( self , measure ) : \n 
. format ( * measure ) ) \n 
~~~ logging . basicConfig ( \n 
level = logging . DEBUG , \n 
filemode = , \n 
HumiditySpout ( ) . run ( ) \n 
from pyleus import __version__ \n 
from pyleus . cli . topology_spec import TopologySpec \n 
from pyleus . cli . virtualenv_proxy import VirtualenvProxy \n 
from pyleus . compat import StringIO \n 
from pyleus . storm . component import DESCRIBE_OPT \n 
from pyleus . exception import InvalidTopologyError \n 
from pyleus . exception import JarError \n 
from pyleus . utils import expand_path \n 
RESOURCES_PATH = "resources" \n 
YAML_FILENAME = "pyleus_topology.yaml" \n 
DEFAULT_REQUIREMENTS_FILENAME = "requirements.txt" \n 
VIRTUALENV_NAME = "pyleus_venv" \n 
def _open_jar ( base_jar ) : \n 
if not os . path . exists ( base_jar ) : \n 
~~ if not zipfile . is_zipfile ( base_jar ) : \n 
~~ zip_file = zipfile . ZipFile ( base_jar , "r" ) \n 
return zip_file \n 
~~ def _zip_dir ( src , arc ) : \n 
src_re = re . compile ( src + "/*" ) \n 
for root , dirs , files in os . walk ( src ) : \n 
~~~ prefix = re . sub ( src_re , "" , root ) \n 
~~~ arc . write ( os . path . join ( root , f ) , os . path . join ( prefix , f ) , \n 
zipfile . ZIP_DEFLATED ) \n 
~~ ~~ ~~ def _pack_jar ( tmp_dir , output_jar ) : \n 
zf = zipfile . ZipFile ( output_jar , "w" ) \n 
~~~ _zip_dir ( tmp_dir , zf ) \n 
~~~ zf . close ( ) \n 
~~ ~~ def _validate_venv ( topology_dir , venv ) : \n 
if os . path . exists ( venv ) : \n 
~~ ~~ def _path_contained_by ( containing_path , path ) : \n 
real_containing_path = os . path . join ( os . path . realpath ( containing_path ) , ) \n 
real_path = os . path . realpath ( path ) \n 
common_prefix = os . path . commonprefix ( [ real_containing_path , real_path ] ) \n 
return common_prefix == real_containing_path \n 
~~ def _remove_pyleus_base_jar ( venv ) : \n 
base_jar_path = venv . execute_module ( "pyleus._base_jar" , \n 
cwd = venv . path ) . strip ( ) \n 
if _path_contained_by ( venv . path , base_jar_path ) : \n 
~~~ os . remove ( base_jar_path ) \n 
~~ ~~ def _set_up_virtualenv ( venv_name , tmp_dir , req , \n 
include_packages , system_site_packages , \n 
pypi_index_url , python_interpreter , verbose ) : \n 
venv = VirtualenvProxy ( \n 
os . path . join ( tmp_dir , venv_name ) , \n 
system_site_packages = system_site_packages , \n 
pypi_index_url = pypi_index_url , \n 
python_interpreter = python_interpreter , \n 
verbose = verbose \n 
packages = [ "pyleus=={0}" . format ( __version__ ) ] \n 
if include_packages is not None : \n 
~~~ packages += include_packages \n 
~~ for package in packages : \n 
~~~ venv . install_package ( package ) \n 
~~ if req is not None : \n 
~~~ venv . install_from_requirements ( req ) \n 
~~ _remove_pyleus_base_jar ( venv ) \n 
return venv \n 
~~ def _assemble_full_topology_yaml ( spec , venv , resources_dir ) : \n 
for component in spec . topology : \n 
~~~ if component . type == "python" : \n 
~~~ log . debug ( . format ( component . module ) ) \n 
description = venv . execute_module ( module = component . module , \n 
args = [ DESCRIBE_OPT ] , \n 
cwd = resources_dir ) \n 
module_spec = yaml . load ( description ) \n 
component . update_from_module ( module_spec ) \n 
~~ ~~ spec . verify_groupings ( ) \n 
new_yaml = StringIO ( ) \n 
yaml . dump ( spec . asdict ( ) , new_yaml ) \n 
return new_yaml . getvalue ( ) \n 
~~ def _content_to_copy ( src , exclude ) : \n 
content = set ( glob . glob ( os . path . join ( src , "*" ) ) ) \n 
content -= set ( exclude ) \n 
return content \n 
~~ def _copy_dir_content ( src , dst , exclude ) : \n 
content = _content_to_copy ( src , exclude ) \n 
for t in content : \n 
~~~ if os . path . isdir ( t ) : \n 
~~~ shutil . copytree ( t , os . path . join ( dst , os . path . basename ( t ) ) , \n 
symlinks = True ) \n 
~~~ shutil . copy2 ( t , dst ) \n 
~~ ~~ ~~ def _create_pyleus_jar ( original_topology_spec , topology_dir , base_jar , \n 
output_jar , zip_file , tmp_dir , include_packages , \n 
system_site_packages , pypi_index_url , verbose ) : \n 
requirements_filename = original_topology_spec . requirements_filename \n 
if not requirements_filename : \n 
~~~ requirements_filename = DEFAULT_REQUIREMENTS_FILENAME \n 
~~ python_interpreter = original_topology_spec . python_interpreter \n 
venv = os . path . join ( topology_dir , VIRTUALENV_NAME ) \n 
req = os . path . join ( topology_dir , requirements_filename ) \n 
if not os . path . isfile ( req ) : \n 
~~~ req = None \n 
~~ _validate_venv ( topology_dir , venv ) \n 
zip_file . extractall ( tmp_dir ) \n 
resources_dir = os . path . join ( tmp_dir , RESOURCES_PATH ) \n 
os . mkdir ( resources_dir ) \n 
_copy_dir_content ( \n 
src = topology_dir , \n 
dst = resources_dir , \n 
exclude = [ venv , req , output_jar ] , \n 
venv = _set_up_virtualenv ( \n 
venv_name = VIRTUALENV_NAME , \n 
tmp_dir = resources_dir , \n 
req = req , \n 
include_packages = include_packages , \n 
verbose = verbose ) \n 
new_yaml = _assemble_full_topology_yaml ( \n 
spec = original_topology_spec , \n 
venv = venv , \n 
resources_dir = resources_dir ) \n 
jar_yaml = os . path . join ( resources_dir , YAML_FILENAME ) \n 
with open ( jar_yaml , ) as f : \n 
~~~ f . write ( new_yaml ) \n 
~~ _pack_jar ( tmp_dir , output_jar ) \n 
~~ def _build_output_path ( output_arg , topology_name ) : \n 
if output_arg is not None : \n 
~~~ return expand_path ( output_arg ) \n 
~~~ return expand_path ( topology_name + ".jar" ) \n 
~~ ~~ def parse_original_topology ( topology_path ) : \n 
~~~ with open ( topology_path ) as f : \n 
~~~ yaml_spec = yaml . load ( f ) \n 
~~ return TopologySpec ( yaml_spec ) \n 
~~ def build_topology_jar ( configs ) : \n 
topology_path = expand_path ( configs . topology_path ) \n 
topology_dir = expand_path ( os . path . dirname ( topology_path ) ) \n 
base_jar = expand_path ( configs . base_jar ) \n 
original_topology_spec = parse_original_topology ( topology_path ) \n 
output_jar = _build_output_path ( configs . output_jar , \n 
original_topology_spec . name ) \n 
include_packages = None \n 
if configs . include_packages is not None : \n 
~~ zip_file = _open_jar ( base_jar ) \n 
~~~ tmp_dir = tempfile . mkdtemp ( ) \n 
~~~ _create_pyleus_jar ( \n 
original_topology_spec = original_topology_spec , \n 
topology_dir = topology_dir , \n 
base_jar = base_jar , \n 
output_jar = output_jar , \n 
zip_file = zip_file , \n 
tmp_dir = tmp_dir , \n 
system_site_packages = configs . system_site_packages , \n 
pypi_index_url = configs . pypi_index_url , \n 
verbose = configs . verbose , \n 
~~~ shutil . rmtree ( tmp_dir ) \n 
~~~ zip_file . close ( ) \n 
~~ ~~ import glob \n 
from pyleus import exception \n 
from pyleus . cli import build \n 
from pyleus . testing import mock \n 
class TestBuild ( object ) : \n 
~~~ @ mock . patch . object ( os . path , , autospec = True ) \n 
def test__open_jar_jarfile_not_found ( self , mock_exists ) : \n 
~~~ mock_exists . return_value = False \n 
with pytest . raises ( exception . JarError ) : \n 
~~~ build . _open_jar ( "foo" ) \n 
~~ mock_exists . assert_called_once_with ( "foo" ) \n 
~~ @ mock . patch . object ( os . path , , autospec = True ) \n 
@ mock . patch . object ( zipfile , , autospec = True ) \n 
def test__open_jar_not_jarfile ( self , mock_is_zipfile , mock_exists ) : \n 
~~~ mock_exists . return_value = True \n 
mock_is_zipfile . return_value = False \n 
~~ mock_is_zipfile . assert_called_once_with ( "foo" ) \n 
~~ @ mock . patch . object ( os , , autospec = True ) \n 
def test__zip_dir ( self , mock_walk ) : \n 
~~~ mock_arc = mock . Mock ( autospec = True ) \n 
mock_walk . return_value = [ \n 
( "foo" , [ "bar" ] , [ "baz" ] ) , \n 
( "foo/bar" , [ ] , [ "qux" ] ) \n 
build . _zip_dir ( "foo" , mock_arc ) \n 
mock_walk . assert_any_call ( "foo" ) \n 
mock . call ( "foo/baz" , "baz" , zipfile . ZIP_DEFLATED ) , \n 
mock . call ( "foo/bar/qux" , "bar/qux" , zipfile . ZIP_DEFLATED ) , \n 
mock_arc . write . assert_has_calls ( expected ) \n 
~~ @ mock . patch . object ( zipfile , , autospec = True ) \n 
@ mock . patch . object ( build , , autospec = True ) \n 
def test__pack_jar ( self , mock_zip_dir , mock_zipfile ) : \n 
~~~ build . _pack_jar ( "foo" , "bar" ) \n 
mock_zipfile . assert_called_once_with ( "bar" , "w" ) \n 
mock_zip_dir . assert_called_once_with ( "foo" , mock_zipfile . return_value ) \n 
def test__validate_venv_dir_contains_venv ( self , mock_exists ) : \n 
with pytest . raises ( exception . InvalidTopologyError ) : \n 
~~~ build . _validate_venv ( "foo" , "foo/bar_venv" ) \n 
~~ mock_exists . assert_called_once_with ( "foo/bar_venv" ) \n 
~~ @ mock . patch . object ( build , , autospec = True ) \n 
def test__set_up_virtualenv_with_requirements ( self , mock_venv , \n 
mock_remove_base_jar ) : \n 
~~~ venv = mock_venv . return_value \n 
build . _set_up_virtualenv ( \n 
venv_name = "foo" , \n 
tmp_dir = "bar" , \n 
req = "baz.txt" , \n 
include_packages = [ "fruit" , "ninja==7.7.7" ] , \n 
system_site_packages = True , \n 
pypi_index_url = "http://pypi-ninja.ninjacorp.com/simple" , \n 
python_interpreter = "python2.7" , \n 
verbose = False ) \n 
expected_install = [ \n 
mock . call ( "pyleus=={0}" . format ( __version__ ) ) , \n 
mock . call ( "fruit" ) , \n 
mock . call ( "ninja==7.7.7" ) \n 
venv . install_package . assert_has_calls ( expected_install ) \n 
venv . install_from_requirements . assert_called_once_with ( "baz.txt" ) \n 
mock_remove_base_jar . assert_called_once_with ( venv ) \n 
def test__set_up_virtualenv_without_requirements ( self , mock_venv , \n 
req = None , \n 
assert venv . install_from_requirements . call_count == 0 \n 
~~ @ mock . patch . object ( glob , , autospec = True ) \n 
def test__content_to_copy ( self , mock_glob ) : \n 
~~~ mock_glob . return_value = [ "foo/good1.mkv" , "foo/good2.bat" , \n 
"foo/bad1.txt" , "foo/bad2.jar" ] \n 
content = build . _content_to_copy ( "foo" , [ "foo/bad1.txt" , \n 
"foo/bad2.jar" ] ) \n 
mock_glob . assert_called_once_with ( "foo/*" ) \n 
assert content == set ( [ "foo/good1.mkv" , "foo/good2.bat" ] ) \n 
@ mock . patch . object ( os . path , , autospec = True ) \n 
@ mock . patch . object ( shutil , , autospec = True ) \n 
def test__copy_dir_content ( \n 
self , mock_copy2 , mock_copytree , mock_isdir , mock_cont_to_copy ) : \n 
~~~ mock_cont_to_copy . return_value = [ "foo/ham" , "foo/honey" ] \n 
mock_isdir . side_effect = iter ( [ True , False ] ) \n 
build . _copy_dir_content ( src = "foo" , dst = "bar" , exclude = [ ] ) \n 
mock_cont_to_copy . assert_called_once_with ( "foo" , [ ] ) \n 
expected = [ mock . call ( "foo/ham" ) , mock . call ( "foo/honey" ) ] \n 
mock_isdir . assert_has_calls ( expected ) \n 
mock_copytree . assert_called_once_with ( \n 
"foo/ham" , "bar/ham" , symlinks = True ) \n 
mock_copy2 . assert_called_once_with ( "foo/honey" , "bar" ) \n 
def test__build_otuput_path ( self , mock_ex_path ) : \n 
~~~ build . _build_output_path ( "foo" , "bar" ) \n 
mock_ex_path . assert_called_with ( "foo" ) \n 
build . _build_output_path ( None , "bar" ) \n 
mock_ex_path . assert_called_with ( "bar.jar" ) \n 
~~ def test__path_contained_by ( self ) : \n 
~~~ p1 = \n 
p2 = \n 
p3 = \n 
assert not build . _path_contained_by ( p1 , p2 ) \n 
assert build . _path_contained_by ( p1 , p3 ) \n 
~~ def test__remove_pyleus_base_jar ( self ) : \n 
mock_venv_path = "/path/to/venv" \n 
def mock_execute_module ( module , cwd ) : \n 
~~~ return "/path/to/venv/inside.jar" \n 
~~ mock_venv = mock . Mock ( \n 
path = mock_venv_path , \n 
execute_module = mock_execute_module , \n 
with mock . patch . object ( os , ) as mock_remove : \n 
~~~ build . _remove_pyleus_base_jar ( mock_venv ) \n 
~~ mock_remove . assert_called_once_with ( "/path/to/venv/inside.jar" ) \n 
~~ def test__remove_pyleus_base_jar_no_remove ( self ) : \n 
~~~ return "/foo/bar/outside.jar" \n 
~~ assert not mock_remove . called \n 
~~ ~~ import io \n 
from tests . testing import resource_filename \n 
from yelp . obj . deal import Deal \n 
def test_init_deal ( ) : \n 
~~~ with io . open ( resource_filename ( ) ) as biz : \n 
~~~ response = json . load ( biz ) [ ] [ 0 ] \n 
deal = Deal ( response ) \n 
assert deal . url == response [ ] \n 
~~ ~~ from yelp . obj . response_object import ResponseObject \n 
class Span ( ResponseObject ) : \n 
~~~ _fields = [ \n 
import sqlalchemy as SA \n 
import pushmanager . core . db as db \n 
import pushmanager . core . util \n 
from pushmanager . core . mail import MailQueue \n 
from pushmanager . core . rb import RBQueue \n 
from pushmanager . core . requesthandler import RequestHandler \n 
class LivePushServlet ( RequestHandler ) : \n 
~~~ def _arg ( self , key ) : \n 
~~~ return pushmanager . core . util . get_str_arg ( self . request , key , ) \n 
~~ def post ( self ) : \n 
~~~ if not self . current_user : \n 
~~~ return self . send_error ( 403 ) \n 
~~ self . pushid = pushmanager . core . util . get_int_arg ( self . request , ) \n 
push_query = db . push_pushes . update ( ) . where ( db . push_pushes . c . id == self . pushid ) . values ( { \n 
: time . time ( ) , \n 
request_query = db . push_requests . update ( ) . where ( SA . and_ ( \n 
db . push_requests . c . state == , \n 
SA . exists ( \n 
[ 1 ] , \n 
SA . and_ ( \n 
db . push_pushcontents . c . push == self . pushid , \n 
db . push_pushcontents . c . request == db . push_requests . c . id , \n 
) ) ) . values ( { \n 
reset_query = db . push_requests . update ( ) . where ( \n 
) ) . values ( { \n 
delete_query = db . push_pushcontents . delete ( ) . where ( \n 
SA . exists ( [ 1 ] , SA . and_ ( \n 
) ) ) \n 
live_query = db . push_requests . select ( ) . where ( \n 
SA . and_ ( db . push_requests . c . state == , \n 
db . push_pushcontents . c . request == db . push_requests . c . id ) \n 
db . execute_transaction_cb ( \n 
[ push_query , request_query , reset_query , delete_query , live_query ] , \n 
self . on_db_complete , \n 
~~ def on_db_complete ( self , success , db_results ) : \n 
~~~ self . check_db_results ( success , db_results ) \n 
_ , _ , _ , _ , live_requests = db_results \n 
for req in live_requests : \n 
~~~ if req [ ] : \n 
~~~ review_id = int ( req [ ] ) \n 
RBQueue . enqueue_review ( review_id ) \n 
~~ if req [ ] : \n 
~~~ user_string = % ( req [ ] , req [ ] ) \n 
users = [ req [ ] ] + req [ ] . split ( ) \n 
~~~ user_string = req [ ] \n 
users = [ req [ ] ] \n 
~~ msg = ( \n 
) % pushmanager . core . util . EscapedDict ( { \n 
: self . current_user , \n 
: user_string , \n 
: req [ ] , \n 
MailQueue . enqueue_user_email ( users , msg , subject ) \n 
~~ ~~ ~~ import mock \n 
import testify as T \n 
import tornado . httpserver \n 
from pushmanager . core . requesthandler import get_base_url \n 
from pushmanager . core . settings import Settings \n 
from pushmanager . testing . mocksettings import MockedSettings \n 
class RequestHandlerTest ( T . TestCase ) : \n 
~~~ def test_get_api_page ( self ) : \n 
~~~ MockedSettings [ ] = { : 8043 , : } \n 
with mock . patch . dict ( Settings , MockedSettings ) : \n 
~~~ T . assert_equal ( \n 
RequestHandler . get_api_page ( "pushes" ) , \n 
"http://push.test.com:8043/api/pushes" \n 
~~ ~~ def test_get_base_url_empty_headers ( self ) : \n 
~~~ MockedSettings [ ] = { : 1111 , : } \n 
request = tornado . httpserver . HTTPRequest ( , ) \n 
request . protocol = \n 
get_base_url ( request ) , \n 
Settings [ ] [ ] = 443 \n 
T . assert_equal ( \n 
~~ ~~ def test_get_base_url_proto_header ( self ) : \n 
request . headers [ ] = \n 
Settings [ ] [ ] = 80 \n 
~~ ~~ def test_get_base_url_port_header ( self ) : \n 
request . headers [ ] = 443 \n 
~~ ~~ def test_RequestHandler_get_base_url ( self ) : \n 
class FakeRequest ( object ) : \n 
~~ ~~ with mock . patch . dict ( Settings , MockedSettings ) : \n 
~~~ fake_requesthandler = FakeRequest ( ) \n 
RequestHandler . get_base_url . __func__ ( fake_requesthandler ) , \n 
~~ ~~ ~~ import testify as T \n 
from pushmanager . ui_methods import authorized_to_manage_request \n 
from pushmanager . ui_methods import sort_pickmes \n 
class UIMethodTest ( T . TestCase ) : \n 
~~~ def test_authorized_to_manage_request_random_user ( self ) : \n 
~~~ request = { : , : None } \n 
T . assert_equal ( False , authorized_to_manage_request ( None , request , ) ) \n 
~~ def test_authorized_to_manage_request_request_user ( self ) : \n 
T . assert_equal ( True , authorized_to_manage_request ( None , request , ) ) \n 
~~ def test_authorized_to_manage_request_pushmaster ( self ) : \n 
T . assert_equal ( True , authorized_to_manage_request ( None , request , , True ) ) \n 
~~ def test_authorized_to_manage_request_watcher ( self ) : \n 
~~~ request = { : , : } \n 
~~ def test_sort_pickmes_regular_case ( self ) : \n 
~~~ requests = [ \n 
sorted_requests = sort_pickmes ( None , requests , [ , , ] ) \n 
T . assert_equal ( len ( sorted_requests ) , 8 ) \n 
T . assert_equal ( sorted_requests [ 0 ] , { : , : } ) \n 
T . assert_equal ( sorted_requests [ 1 ] , { : , : } ) \n 
T . assert_equal ( sorted_requests [ 2 ] , { : , : } ) \n 
T . assert_equal ( sorted_requests [ 3 ] , { : , : } ) \n 
T . assert_equal ( sorted_requests [ 4 ] , { : , : } ) \n 
T . assert_equal ( sorted_requests [ 5 ] , { : , : } ) \n 
T . assert_equal ( sorted_requests [ 6 ] , { : , : } ) \n 
T . assert_equal ( sorted_requests [ 7 ] , { : , : } ) \n 
~~ def test_sort_pickmes_all_tags_in_ordering ( self ) : \n 
T . assert_equal ( len ( sorted_requests ) , 9 ) \n 
T . assert_equal ( sorted_requests [ 8 ] , { : , : } ) \n 
~~ def test_sort_pickmes_no_tags_order ( self ) : \n 
sorted_requests = sort_pickmes ( None , requests , [ ] ) \n 
T . assert_equal ( len ( sorted_requests ) , 3 ) \n 
~~~ T . run ( ) \n 
~~ import collections \n 
from gearman . client import GearmanClient \n 
from gearman . client_handler import GearmanClientCommandHandler \n 
from gearman . constants import PRIORITY_NONE , PRIORITY_HIGH , PRIORITY_LOW , JOB_UNKNOWN , JOB_PENDING , from gearman . errors import ExceededConnectionAttempts , ServerUnavailable , InvalidClientState \n 
from gearman . protocol import submit_cmd_for_background_priority , GEARMAN_COMMAND_STATUS_RES , GEARMAN_COMMAND_GET_STATUS GEARMAN_COMMAND_WORK_STATUS , GEARMAN_COMMAND_WORK_FAIL , GEARMAN_COMMAND_WORK_COMPLETE , GEARMAN_COMMAND_WORK_DATA \n 
from tests . _core_testing import _GearmanAbstractTest , MockGearmanConnectionManager , MockGearmanConnection \n 
class MockGearmanClient ( GearmanClient , MockGearmanConnectionManager ) : \n 
~~ class ClientTest ( _GearmanAbstractTest ) : \n 
connection_manager_class = MockGearmanClient \n 
command_handler_class = GearmanClientCommandHandler \n 
~~~ super ( ClientTest , self ) . setUp ( ) \n 
self . original_handle_connection_activity = self . connection_manager . handle_connection_activity \n 
~~~ super ( ClientTest , self ) . tearDown ( ) \n 
self . connection_manager . handle_connection_activity = self . original_handle_connection_activity \n 
~~ def generate_job_request ( self , submitted = True , accepted = True ) : \n 
~~~ current_request = super ( ClientTest , self ) . generate_job_request ( ) \n 
if submitted or accepted : \n 
~~~ self . connection_manager . establish_request_connection ( current_request ) \n 
self . command_handler . send_job_request ( current_request ) \n 
~~ if submitted and accepted : \n 
~~~ self . command_handler . recv_command ( GEARMAN_COMMAND_JOB_CREATED , job_handle = current_request self . assert_ ( current_request . job . handle in self . command_handler . handle_to_request_map ) \n 
~~ return current_request \n 
~~ def test_establish_request_connection_complex ( self ) : \n 
~~~ failed_connection = MockGearmanConnection ( ) \n 
failed_connection . _fail_on_bind = True \n 
failed_then_retried_connection = MockGearmanConnection ( ) \n 
failed_then_retried_connection . _fail_on_bind = True \n 
good_connection = MockGearmanConnection ( ) \n 
good_connection . connect ( ) \n 
self . connection_manager . connection_list = [ failed_connection , failed_then_retried_connection \n 
current_request = self . generate_job_request ( submitted = False , accepted = False ) \n 
self . failIf ( current_request in self . connection_manager . request_to_rotating_connection_queue ) \n 
chosen_connection = self . connection_manager . establish_request_connection ( current_request ) \n 
self . assertEqual ( chosen_connection , good_connection ) \n 
self . assertFalse ( failed_connection . connected ) \n 
self . assertFalse ( failed_then_retried_connection . connected ) \n 
self . assertTrue ( good_connection . connected ) \n 
good_connection . _reset_connection ( ) \n 
good_connection . _fail_on_bind = True \n 
failed_then_retried_connection . _fail_on_bind = False \n 
failed_then_retried_connection . connect ( ) \n 
self . assertEqual ( chosen_connection , failed_then_retried_connection ) \n 
self . assertTrue ( failed_then_retried_connection . connected ) \n 
self . assertFalse ( good_connection . connected ) \n 
~~ def test_establish_request_connection_dead ( self ) : \n 
~~~ self . connection_manager . connection_list = [ ] \n 
self . connection_manager . command_handlers = { } \n 
self . assertRaises ( ServerUnavailable , self . connection_manager . establish_request_connection , current_request \n 
failed_connection = MockGearmanConnection ( ) \n 
self . connection_manager . connection_list . append ( failed_connection ) \n 
~~ def test_auto_retry_behavior ( self ) : \n 
~~~ current_request = self . generate_job_request ( submitted = False , accepted = False ) \n 
def fail_then_create_jobs ( rx_conns , wr_conns , ex_conns ) : \n 
~~~ if self . connection_manager . current_failures < self . connection_manager . expected_failures : ~~~ self . connection_manager . current_failures += 1 \n 
self . assertTrue ( self . connection . connected ) \n 
self . connection_manager . handle_error ( self . connection ) \n 
self . assertFalse ( self . connection . connected ) \n 
~~~ self . assertEquals ( current_request . state , JOB_PENDING ) \n 
self . command_handler . recv_command ( GEARMAN_COMMAND_JOB_CREATED , job_handle = current_request \n 
~~ return rx_conns , wr_conns , ex_conns \n 
~~ self . connection_manager . handle_connection_activity = fail_then_create_jobs \n 
self . connection_manager . expected_failures = 5 \n 
self . connection_manager . current_failures = current_request . connection_attempts = 0 \n 
current_request . max_connection_attempts = self . connection_manager . expected_failures + 1 \n 
current_request . state = JOB_UNKNOWN \n 
accepted_jobs = self . connection_manager . wait_until_jobs_accepted ( [ current_request ] ) \n 
self . assertEquals ( current_request . state , JOB_CREATED ) \n 
self . assertEquals ( current_request . connection_attempts , current_request . max_connection_attempts \n 
current_request . max_connection_attempts = self . connection_manager . expected_failures \n 
self . assertRaises ( ExceededConnectionAttempts , self . connection_manager . wait_until_jobs_accepted self . assertEquals ( current_request . state , JOB_UNKNOWN ) \n 
~~ def test_multiple_fg_job_submission ( self ) : \n 
~~~ submitted_job_count = 5 \n 
expected_job_list = [ self . generate_job ( ) for _ in xrange ( submitted_job_count ) ] \n 
def mark_jobs_created ( rx_conns , wr_conns , ex_conns ) : \n 
~~~ for current_job in expected_job_list : \n 
~~~ self . command_handler . recv_command ( GEARMAN_COMMAND_JOB_CREATED , job_handle = current_job \n 
~~ self . connection_manager . handle_connection_activity = mark_jobs_created \n 
job_dictionaries = [ current_job . to_dict ( ) for current_job in expected_job_list ] \n 
job_requests = self . connection_manager . submit_multiple_jobs ( job_dictionaries , wait_until_complete for current_request , expected_job in zip ( job_requests , expected_job_list ) : \n 
~~~ current_job = current_request . job \n 
self . assert_jobs_equal ( current_job , expected_job ) \n 
self . assertEqual ( current_request . priority , PRIORITY_NONE ) \n 
self . assertEqual ( current_request . background , False ) \n 
self . assertEqual ( current_request . state , JOB_CREATED ) \n 
self . assertFalse ( current_request . complete ) \n 
~~ ~~ def test_single_bg_job_submission ( self ) : \n 
~~~ expected_job = self . generate_job ( ) \n 
def mark_job_created ( rx_conns , wr_conns , ex_conns ) : \n 
~~~ self . command_handler . recv_command ( GEARMAN_COMMAND_JOB_CREATED , job_handle = expected_job . handle return rx_conns , wr_conns , ex_conns \n 
~~ self . connection_manager . handle_connection_activity = mark_job_created \n 
job_request = self . connection_manager . submit_job ( expected_job . task , expected_job . data , unique \n 
current_job = job_request . job \n 
self . assertEqual ( job_request . priority , PRIORITY_LOW ) \n 
self . assertEqual ( job_request . background , True ) \n 
self . assertEqual ( job_request . state , JOB_CREATED ) \n 
self . assertTrue ( job_request . complete ) \n 
~~ def test_single_fg_job_submission_timeout ( self ) : \n 
def job_failed_submission ( rx_conns , wr_conns , ex_conns ) : \n 
~~~ return rx_conns , wr_conns , ex_conns \n 
~~ self . connection_manager . handle_connection_activity = job_failed_submission \n 
self . assertEqual ( job_request . priority , PRIORITY_HIGH ) \n 
self . assertEqual ( job_request . background , False ) \n 
self . assertEqual ( job_request . state , JOB_PENDING ) \n 
self . assertFalse ( job_request . complete ) \n 
self . assertTrue ( job_request . timed_out ) \n 
~~ def test_wait_for_multiple_jobs_to_complete_or_timeout ( self ) : \n 
~~~ completed_request = self . generate_job_request ( ) \n 
failed_request = self . generate_job_request ( ) \n 
timeout_request = self . generate_job_request ( ) \n 
self . update_requests = True \n 
def multiple_job_updates ( rx_conns , wr_conns , ex_conns ) : \n 
~~~ if self . update_requests : \n 
~~~ self . command_handler . recv_command ( GEARMAN_COMMAND_WORK_COMPLETE , job_handle = completed_request self . command_handler . recv_command ( GEARMAN_COMMAND_WORK_FAIL , job_handle = failed_request self . update_requests = False \n 
~~ self . connection_manager . handle_connection_activity = multiple_job_updates \n 
finished_requests = self . connection_manager . wait_until_jobs_completed ( [ completed_request , failed_request del self . update_requests \n 
finished_completed_request , finished_failed_request , finished_timeout_request = finished_requests \n 
self . assert_jobs_equal ( finished_completed_request . job , completed_request . job ) \n 
self . assertEqual ( finished_completed_request . state , JOB_COMPLETE ) \n 
self . assertEqual ( finished_completed_request . result , ) \n 
self . assertFalse ( finished_completed_request . timed_out ) \n 
self . assert_jobs_equal ( finished_failed_request . job , failed_request . job ) \n 
self . assertEqual ( finished_failed_request . state , JOB_FAILED ) \n 
self . assertEqual ( finished_failed_request . result , None ) \n 
self . assertFalse ( finished_failed_request . timed_out ) \n 
self . assertEqual ( finished_timeout_request . state , JOB_CREATED ) \n 
self . assertEqual ( finished_timeout_request . result , None ) \n 
self . assertTrue ( finished_timeout_request . timed_out ) \n 
self . assert_ ( finished_timeout_request . job . handle in self . command_handler . handle_to_request_map \n 
~~ def test_get_job_status ( self ) : \n 
~~~ single_request = self . generate_job_request ( ) \n 
def retrieve_status ( rx_conns , wr_conns , ex_conns ) : \n 
~~~ self . command_handler . recv_command ( GEARMAN_COMMAND_STATUS_RES , job_handle = single_request . return rx_conns , wr_conns , ex_conns \n 
~~ self . connection_manager . handle_connection_activity = retrieve_status \n 
job_request = self . connection_manager . get_job_status ( single_request ) \n 
request_status = job_request . status \n 
self . failUnless ( request_status ) \n 
self . assertTrue ( request_status [ ] ) \n 
self . assertFalse ( request_status [ ] ) \n 
self . assertEqual ( request_status [ ] , 0 ) \n 
self . assertEqual ( request_status [ ] , 1 ) \n 
self . assertFalse ( job_request . timed_out ) \n 
~~ def test_get_job_status_unknown ( self ) : \n 
current_handle = single_request . job . handle \n 
self . command_handler . recv_command ( GEARMAN_COMMAND_WORK_FAIL , job_handle = current_handle ) \n 
~~~ self . command_handler . recv_command ( GEARMAN_COMMAND_STATUS_RES , job_handle = current_handle , return rx_conns , wr_conns , ex_conns \n 
~~ def test_get_job_status_timeout ( self ) : \n 
def retrieve_status_timeout ( rx_conns , wr_conns , ex_conns ) : \n 
~~ self . connection_manager . handle_connection_activity = retrieve_status_timeout \n 
job_request = self . connection_manager . get_job_status ( single_request , poll_timeout = 0.01 ) \n 
~~ ~~ class ClientCommandHandlerInterfaceTest ( _GearmanAbstractTest ) : \n 
def test_send_job_request ( self ) : \n 
~~~ current_request = self . generate_job_request ( ) \n 
gearman_job = current_request . job \n 
for priority in ( PRIORITY_NONE , PRIORITY_HIGH , PRIORITY_LOW ) : \n 
~~~ for background in ( False , True ) : \n 
~~~ current_request . reset ( ) \n 
current_request . priority = priority \n 
current_request . background = background \n 
queued_request = self . command_handler . requests_awaiting_handles . popleft ( ) \n 
self . assertEqual ( queued_request , current_request ) \n 
expected_cmd_type = submit_cmd_for_background_priority ( background , priority ) \n 
self . assert_sent_command ( expected_cmd_type , task = gearman_job . task , data = gearman_job . \n 
~~ ~~ ~~ def test_get_status_of_job ( self ) : \n 
self . command_handler . send_get_status_of_job ( current_request ) \n 
self . assert_sent_command ( GEARMAN_COMMAND_GET_STATUS , job_handle = current_request . job . handle ) \n 
~~ ~~ class ClientCommandHandlerStateMachineTest ( _GearmanAbstractTest ) : \n 
def generate_job_request ( self , submitted = True , accepted = True ) : \n 
~~~ current_request = super ( ClientCommandHandlerStateMachineTest , self ) . generate_job_request ( ) \n 
~~~ self . command_handler . requests_awaiting_handles . append ( current_request ) \n 
current_request . state = JOB_PENDING \n 
~~~ self . command_handler . recv_command ( GEARMAN_COMMAND_JOB_CREATED , job_handle = current_request \n 
~~ def test_received_job_created ( self ) : \n 
~~~ current_request = self . generate_job_request ( accepted = False ) \n 
new_handle = str ( random . random ( ) ) \n 
self . command_handler . recv_command ( GEARMAN_COMMAND_JOB_CREATED , job_handle = new_handle ) \n 
self . assertEqual ( current_request . job . handle , new_handle ) \n 
self . assertEqual ( self . command_handler . handle_to_request_map [ new_handle ] , current_request ) \n 
~~ def test_received_job_created_out_of_order ( self ) : \n 
~~~ self . assertEqual ( self . command_handler . requests_awaiting_handles , collections . deque ( ) ) \n 
self . assertRaises ( InvalidClientState , self . command_handler . recv_command , GEARMAN_COMMAND_JOB_CREATED \n 
~~ def test_required_state_pending ( self ) : \n 
invalid_states = [ JOB_UNKNOWN , JOB_CREATED , JOB_COMPLETE , JOB_FAILED ] \n 
for bad_state in invalid_states : \n 
~~~ current_request . state = bad_state \n 
self . command_handler . requests_awaiting_handles . append ( current_request ) \n 
~~ ~~ def test_required_state_queued ( self ) : \n 
job_handle = current_request . job . handle \n 
new_data = str ( random . random ( ) ) \n 
invalid_states = [ JOB_UNKNOWN , JOB_PENDING , JOB_COMPLETE , JOB_FAILED ] \n 
self . assertRaises ( InvalidClientState , self . command_handler . recv_command , GEARMAN_COMMAND_WORK_DATA \n 
self . assertRaises ( InvalidClientState , self . command_handler . recv_command , GEARMAN_COMMAND_WORK_WARNING \n 
self . assertRaises ( InvalidClientState , self . command_handler . recv_command , GEARMAN_COMMAND_WORK_STATUS \n 
self . assertRaises ( InvalidClientState , self . command_handler . recv_command , GEARMAN_COMMAND_WORK_COMPLETE \n 
self . assertRaises ( InvalidClientState , self . command_handler . recv_command , GEARMAN_COMMAND_WORK_FAIL \n 
~~ ~~ def test_in_flight_work_updates ( self ) : \n 
self . command_handler . recv_command ( GEARMAN_COMMAND_WORK_DATA , job_handle = job_handle , data = new_data self . assertEqual ( current_request . data_updates . popleft ( ) , new_data ) \n 
self . command_handler . recv_command ( GEARMAN_COMMAND_WORK_WARNING , job_handle = job_handle , data = self . assertEqual ( current_request . warning_updates . popleft ( ) , new_data ) \n 
self . command_handler . recv_command ( GEARMAN_COMMAND_WORK_STATUS , job_handle = job_handle , numerator \n 
self . assertEqual ( current_request . status_updates . popleft ( ) , ( 0 , 1 ) ) \n 
~~ def test_work_complete ( self ) : \n 
self . command_handler . recv_command ( GEARMAN_COMMAND_WORK_COMPLETE , job_handle = job_handle , data \n 
self . assertEqual ( current_request . result , new_data ) \n 
self . assertEqual ( current_request . state , JOB_COMPLETE ) \n 
~~ def test_work_fail ( self ) : \n 
self . command_handler . recv_command ( GEARMAN_COMMAND_WORK_FAIL , job_handle = job_handle ) \n 
self . assertEqual ( current_request . state , JOB_FAILED ) \n 
~~ def test_status_request ( self ) : \n 
self . assertEqual ( current_request . status , { } ) \n 
self . command_handler . recv_command ( GEARMAN_COMMAND_STATUS_RES , job_handle = job_handle , known = \n 
self . assertEqual ( current_request . status [ ] , job_handle ) \n 
self . assertTrue ( current_request . status [ ] ) \n 
self . assertEqual ( current_request . status [ ] , 0 ) \n 
self . assertEqual ( current_request . status [ ] , 1 ) \n 
~~ from __future__ import unicode_literals , absolute_import \n 
class Type ( dict ) : \n 
~~~ def __init__ ( self , type_name ) : \n 
~~~ super ( Type , self ) . __init__ ( ) \n 
self . type_name = type_name \n 
self [ "type" ] = self . _build_dict ( ) \n 
~~ def _build_dict ( self ) : \n 
"value" : self . type_name \n 
from pyeqs import Filter \n 
from pyeqs . dsl import Term \n 
from tests . helpers import homogeneous \n 
def test_create_filter ( ) : \n 
t = Filter ( ) \n 
results = { \n 
"and" : [ ] \n 
homogeneous ( t , results ) \n 
~~ def test_add_filter ( ) : \n 
t . filter ( Term ( "foo" , "bar" ) ) \n 
"and" : [ \n 
"term" : { \n 
"foo" : "bar" \n 
~~ def test_filter_with_or ( ) : \n 
t = Filter ( "or" ) \n 
"or" : [ ] \n 
~~ def test_filter_with_and ( ) : \n 
t = Filter ( "and" ) \n 
import pyhsm . exception \n 
import pyhsm . defines \n 
class YHSM_Cmd ( ) : \n 
response_status = None \n 
executed = False \n 
def __init__ ( self , stick , command , payload = ) : \n 
self . stick = stick \n 
self . payload = payload \n 
~~ def execute ( self , read_response = True ) : \n 
if self . command != pyhsm . defines . YSM_NULL : \n 
~~~ cmd_buf = struct . pack ( , len ( self . payload ) + 1 , self . command ) \n 
~~~ cmd_buf = chr ( self . command ) \n 
~~ cmd_buf += self . payload \n 
debug_info = None \n 
unlock = self . stick . acquire ( ) \n 
~~~ if self . stick . debug : \n 
~~ self . stick . write ( cmd_buf , debug_info ) \n 
if not read_response : \n 
~~ return self . _read_response ( ) \n 
~~~ unlock ( ) \n 
~~ ~~ def _read_response ( self ) : \n 
res = self . stick . read ( 2 , ) \n 
if len ( res ) != 2 : \n 
~~~ self . _handle_invalid_read_response ( res , 2 ) \n 
~~ response_len , response_status = struct . unpack ( , res ) \n 
if response_status & pyhsm . defines . YSM_RESPONSE : \n 
~~ res = self . stick . read ( response_len , debug_info ) \n 
if res : \n 
~~~ if response_status == self . command | pyhsm . defines . YSM_RESPONSE : \n 
~~~ self . executed = True \n 
self . response_status = response_status \n 
return self . parse_result ( res ) \n 
~~~ reset ( self . stick ) \n 
raise pyhsm . exception . YHSM_Error ( ) \n 
~~~ raise pyhsm . exception . YHSM_Error ( ) \n 
~~ ~~ def _handle_invalid_read_response ( self , res , expected_len ) : \n 
if not res : \n 
raise pyhsm . exception . YHSM_Error ( % ( pyhsm . defines . cmd2str ( self . command ) ) ) \n 
~~ self . stick . write ( , ) \n 
lines = res2 . split ( ) \n 
for this in lines : \n 
~~~ if re . match ( , this ) : \n 
~~ def parse_result ( self , data ) : \n 
~~ ~~ def reset ( stick ) : \n 
nulls = ( pyhsm . defines . YSM_MAX_PKT_SIZE - 1 ) * \n 
res = YHSM_Cmd ( stick , pyhsm . defines . YSM_NULL , payload = nulls ) . execute ( read_response = False ) \n 
unlock = stick . acquire ( ) \n 
~~~ stick . drain ( ) \n 
stick . flush ( ) \n 
~~ return res == 0 \n 
import pyhsm \n 
import test_common \n 
class TestSoftHSM ( test_common . YHSM_TestCase ) : \n 
~~~ test_common . YHSM_TestCase . setUp ( self ) \n 
self . nonce = "4d4d4d4d4d4d" . decode ( ) \n 
self . key = "A" * 16 \n 
~~ def test_aes_CCM_encrypt_decrypt ( self ) : \n 
key = chr ( 0x09 ) * 16 \n 
key_handle = 1 \n 
plaintext = "foo" . ljust ( 16 , chr ( 0x0 ) ) \n 
ct = pyhsm . soft_hsm . aesCCM ( key , key_handle , self . nonce , plaintext , decrypt = False ) \n 
pt = pyhsm . soft_hsm . aesCCM ( key , key_handle , self . nonce , ct , decrypt = True ) \n 
self . assertEquals ( plaintext , pt ) \n 
~~ def test_aes_CCM_wrong_key ( self ) : \n 
key = chr ( 0x08 ) * 16 \n 
self . assertRaises ( pyhsm . exception . YHSM_Error , pyhsm . soft_hsm . aesCCM , \n 
key , key_handle , self . nonce , ct , decrypt = True ) \n 
~~ def test_aes_CCM_wrong_key_handle ( self ) : \n 
key_handle = 2 \n 
~~ def test_soft_simple_aead_generation ( self ) : \n 
key_handle = 0x2000 \n 
plaintext = . ljust ( 16 , chr ( 0x0 ) ) \n 
key = str ( "2000" * 16 ) . decode ( ) \n 
aead = self . hsm . generate_aead_simple ( self . nonce , key_handle , plaintext ) \n 
self . assertEquals ( aead . data , ct ) \n 
~~ def test_soft_generate_long_aead ( self ) : \n 
plaintext = * 64 \n 
~~ def test_soft_generate_yubikey_secrets_aead ( self ) : \n 
plaintext = * 22 \n 
import imp \n 
from u2fval import default_settings \n 
import logging . config \n 
SETTINGS_FILE = os . getenv ( , os . path . join ( \n 
LOG_CONFIG_FILE = os . path . join ( os . path . dirname ( os . path . abspath ( SETTINGS_FILE ) ) , \n 
VALUES = { \n 
def parse ( conf , settings = { } ) : \n 
~~~ for confkey , settingskey in VALUES . items ( ) : \n 
~~~ settings [ settingskey ] = conf . __getattribute__ ( confkey ) \n 
~~ ~~ return settings \n 
~~ settings = parse ( default_settings ) \n 
dont_write_bytecode = sys . dont_write_bytecode \n 
~~~ sys . dont_write_bytecode = True \n 
user_settings = imp . load_source ( , SETTINGS_FILE ) \n 
settings = parse ( user_settings , settings ) \n 
~~ except IOError as e : \n 
~~~ if e . errno not in [ errno . ENOENT , errno . EACCES ] : \n 
~~~ sys . dont_write_bytecode = dont_write_bytecode \n 
~~~ logging . config . fileConfig ( LOG_CONFIG_FILE ) \n 
~~~ logging . basicConfig ( level = logging . INFO ) \n 
from PySide import QtGui , QtCore \n 
from neoman import messages as m \n 
from neoman . storage import settings \n 
from neoman . exc import ModeSwitchError \n 
from neoman . model . neo import YubiKeyNeo \n 
from neoman . model . applet import Applet \n 
from neoman . model . modes import MODE \n 
from neoman . view . tabs import TabWidgetWithAbout \n 
U2F_URL = "http://www.yubico.com/products/yubikey-hardware/yubikey-neo/" + "yubikey-neo-u2f/" \n 
def get_text ( * args , ** kwargs ) : \n 
~~~ flags = ( \n 
QtCore . Qt . WindowTitleHint | \n 
QtCore . Qt . WindowSystemMenuHint \n 
kwargs [ ] = flags \n 
return QtGui . QInputDialog . getText ( * args , ** kwargs ) \n 
~~ class NeoPage ( TabWidgetWithAbout ) : \n 
~~~ _neo = QtCore . Signal ( YubiKeyNeo ) \n 
applet = QtCore . Signal ( Applet ) \n 
~~~ super ( NeoPage , self ) . __init__ ( ) \n 
self . _tabs = [ ] \n 
self . _supported = True \n 
self . _unsupported_tab = UnsupportedTab ( ) \n 
settings_tab = SettingsTab ( ) \n 
self . _neo . connect ( settings_tab . set_neo ) \n 
self . addTab ( settings_tab , m . settings ) \n 
if QtCore . QCoreApplication . instance ( ) . devmode : \n 
~~~ apps = AppsTab ( self , 1 ) \n 
self . _neo . connect ( apps . set_neo ) \n 
apps . applet . connect ( self . _set_applet ) \n 
self . addTab ( apps , m . installed_apps ) \n 
~~ ~~ def addTab ( self , tab , title ) : \n 
~~~ self . _tabs . append ( ( tab , title ) ) \n 
if self . _supported : \n 
~~~ super ( NeoPage , self ) . addTab ( tab , title ) \n 
~~ ~~ @ QtCore . Slot ( YubiKeyNeo ) \n 
def setNeo ( self , neo ) : \n 
~~~ self . _supported = neo and neo . supported \n 
self . clear ( ) \n 
~~~ for ( tab , title ) in self . _tabs : \n 
~~~ super ( NeoPage , self ) . addTab ( self . _unsupported_tab , m . settings ) \n 
~~ self . _neo . emit ( neo ) \n 
~~ @ QtCore . Slot ( Applet ) \n 
def _set_applet ( self , applet ) : \n 
~~~ self . applet . emit ( applet ) \n 
~~ ~~ class UnsupportedTab ( QtGui . QWidget ) : \n 
~~~ super ( UnsupportedTab , self ) . __init__ ( ) \n 
layout = QtGui . QVBoxLayout ( ) \n 
layout . addWidget ( QtGui . QLabel ( m . unsupported_device ) ) \n 
self . setLayout ( layout ) \n 
~~ ~~ class SettingsTab ( QtGui . QWidget ) : \n 
~~~ super ( SettingsTab , self ) . __init__ ( ) \n 
self . _neo = None \n 
self . _name = QtGui . QLabel ( ) \n 
self . _serial = QtGui . QLabel ( ) \n 
self . _firmware = QtGui . QLabel ( ) \n 
self . _u2f = QtGui . QLabel ( ) \n 
self . _u2f . setOpenExternalLinks ( True ) \n 
name_row = QtGui . QHBoxLayout ( ) \n 
name_row . addWidget ( self . _name ) \n 
self . _name_btn = QtGui . QPushButton ( m . change_name ) \n 
self . _name_btn . clicked . connect ( self . change_name ) \n 
name_row . addWidget ( self . _name_btn ) \n 
details_row = QtGui . QHBoxLayout ( ) \n 
details_row . addWidget ( self . _serial ) \n 
details_row . addWidget ( self . _firmware ) \n 
self . _u2f_row = QtGui . QHBoxLayout ( ) \n 
self . _u2f_row . addWidget ( QtGui . QLabel ( ) ) \n 
self . _u2f_row . addWidget ( self . _u2f ) \n 
layout . addLayout ( name_row ) \n 
layout . addLayout ( details_row ) \n 
layout . addLayout ( self . _u2f_row ) \n 
button = QtGui . QPushButton ( m . manage_keys ) \n 
button . clicked . connect ( self . manage_keys ) \n 
self . _mode_btn = QtGui . QPushButton ( m . change_mode ) \n 
self . _mode_btn . clicked . connect ( self . change_mode ) \n 
layout . addWidget ( self . _mode_btn ) \n 
self . _mode_note = QtGui . QLabel ( m . note_1 % m . mode_note ) \n 
self . _mode_note . setWordWrap ( True ) \n 
layout . addWidget ( self . _mode_note ) \n 
layout . addStretch ( ) \n 
~~ @ QtCore . Slot ( YubiKeyNeo ) \n 
def set_neo ( self , neo ) : \n 
~~~ self . _neo = neo \n 
if not neo : \n 
~~ self . _name_btn . setDisabled ( neo . serial is None ) \n 
self . _name . setText ( m . name_1 % neo . name ) \n 
self . _serial . setText ( m . serial_1 % neo . serial ) \n 
show_firmware = neo . version != ( 0 , 0 , 0 ) \n 
self . _u2f_row . setDirection ( \n 
QtGui . QBoxLayout . LeftToRight if show_firmware else \n 
QtGui . QBoxLayout . RightToLeft ) \n 
self . _firmware . setVisible ( show_firmware ) \n 
self . _firmware . setText ( m . firmware_1 % . join ( map ( str , neo . version ) ) ) \n 
if neo . allowed_modes [ 2 ] : \n 
~~~ self . _u2f . setText ( m . u2f_1 % m . u2f_supported ) \n 
~~~ self . _u2f . setText ( m . u2f_1 % m . u2f_not_supported_1 % U2F_URL ) \n 
~~ self . _mode_btn . setText ( m . change_mode_1 % MODE . name_for_mode ( neo . mode ) ) \n 
self . _mode_note . setVisible ( neo . version < ( 4 , 1 , 0 ) ) \n 
~~ def change_name ( self ) : \n 
~~~ name , ok = get_text ( self , m . name , m . change_name_desc , \n 
text = self . _neo . name ) \n 
if ok : \n 
~~~ self . _neo . name = name \n 
self . _name . setText ( m . name_1 % name ) \n 
~~ ~~ def manage_keys ( self ) : \n 
~~~ print m . manage_keys \n 
~~ def change_mode ( self ) : \n 
~~~ mode = ModeDialog . change_mode ( self . _neo , self ) \n 
if mode is not None : \n 
~~~ self . _neo . set_mode ( mode ) \n 
~~ except ModeSwitchError : \n 
~~~ QtGui . QMessageBox . critical ( self , m . mode_error , \n 
m . mode_error_desc ) \n 
~~ self . _mode_btn . setText ( m . change_mode_1 % MODE . name_for_mode ( mode ) ) \n 
remove_dialog = QtGui . QMessageBox ( self ) \n 
remove_dialog . setWindowTitle ( m . change_mode ) \n 
remove_dialog . setIcon ( QtGui . QMessageBox . Information ) \n 
remove_dialog . setText ( m . remove_device ) \n 
remove_dialog . setStandardButtons ( QtGui . QMessageBox . NoButton ) \n 
self . _neo . removed . connect ( remove_dialog . accept ) \n 
remove_dialog . exec_ ( ) \n 
~~ ~~ ~~ class ModeDialog ( QtGui . QDialog ) : \n 
~~~ def __init__ ( self , neo , parent = None ) : \n 
~~~ super ( ModeDialog , self ) . __init__ ( parent ) \n 
self . setWindowFlags ( self . windowFlags ( ) \n 
^ QtCore . Qt . WindowContextHelpButtonHint ) \n 
layout . addWidget ( QtGui . QLabel ( m . change_mode_desc ) ) \n 
boxes = QtGui . QHBoxLayout ( ) \n 
self . _otp = QtGui . QCheckBox ( m . otp ) \n 
self . _otp . clicked . connect ( self . _state_changed ) \n 
boxes . addWidget ( self . _otp ) \n 
self . _ccid = QtGui . QCheckBox ( m . ccid ) \n 
self . _ccid . clicked . connect ( self . _state_changed ) \n 
boxes . addWidget ( self . _ccid ) \n 
self . _u2f = QtGui . QCheckBox ( m . u2f ) \n 
self . _u2f . clicked . connect ( self . _state_changed ) \n 
boxes . addWidget ( self . _u2f ) \n 
layout . addLayout ( boxes ) \n 
buttons = QtGui . QDialogButtonBox ( QtGui . QDialogButtonBox . Ok | \n 
QtGui . QDialogButtonBox . Cancel ) \n 
buttons . accepted . connect ( self . accept ) \n 
buttons . rejected . connect ( self . reject ) \n 
self . _ok = buttons . button ( QtGui . QDialogButtonBox . Ok ) \n 
layout . addWidget ( buttons ) \n 
self . setWindowTitle ( m . change_mode ) \n 
allowed = neo . allowed_modes \n 
self . _otp . setEnabled ( allowed [ 0 ] ) \n 
self . _otp . setVisible ( allowed [ 0 ] ) \n 
self . _ccid . setEnabled ( allowed [ 1 ] ) \n 
self . _ccid . setVisible ( allowed [ 1 ] ) \n 
self . _u2f . setEnabled ( allowed [ 2 ] ) \n 
self . _u2f . setVisible ( allowed [ 2 ] ) \n 
self . mode = neo . mode \n 
~~ def _state_changed ( self ) : \n 
~~~ self . _ok . setDisabled ( not any ( self . flags ) ) \n 
def flags ( self ) : \n 
~~~ return self . _otp . isChecked ( ) , self . _ccid . isChecked ( ) , self . _u2f . isChecked ( ) \n 
def mode ( self ) : \n 
~~~ return MODE . mode_for_flags ( * self . flags ) \n 
~~ @ mode . setter \n 
def mode ( self , value ) : \n 
~~~ otp , ccid , u2f , touch_eject = MODE . flags_for_mode ( value ) \n 
self . _otp . setChecked ( otp and self . _otp . isEnabled ( ) ) \n 
self . _ccid . setChecked ( ccid and self . _ccid . isEnabled ( ) ) \n 
self . _u2f . setChecked ( u2f and self . _u2f . isEnabled ( ) ) \n 
def change_mode ( cls , neo , parent = None ) : \n 
~~~ dialog = cls ( neo , parent ) \n 
if dialog . exec_ ( ) : \n 
~~~ mode = dialog . mode \n 
legacy = neo . version < ( 3 , 3 , 0 ) \n 
~~~ mode = 0x82 \n 
~~ return mode \n 
~~ ~~ ~~ class AppsTab ( QtGui . QWidget ) : \n 
~~~ applet = QtCore . Signal ( Applet ) \n 
def __init__ ( self , parent , index ) : \n 
~~~ super ( AppsTab , self ) . __init__ ( ) \n 
self . index = index \n 
self . _apps = [ ] \n 
self . _apps_list = QtGui . QListView ( ) \n 
self . _apps_list . setModel ( QtGui . QStringListModel ( [ ] ) ) \n 
self . _apps_list . setEditTriggers ( QtGui . QListView . NoEditTriggers ) \n 
self . _apps_list . doubleClicked . connect ( self . open_app ) \n 
layout . addWidget ( self . _apps_list ) \n 
self . _install_cap_btn = QtGui . QPushButton ( m . install_cap ) \n 
self . _install_cap_btn . clicked . connect ( self . install_cap ) \n 
layout . addWidget ( self . _install_cap_btn ) \n 
parent . currentChanged . connect ( self . tab_changed ) \n 
~~ def install_cap ( self ) : \n 
~~~ path = settings . value ( , None ) \n 
( cap , _ ) = QtGui . QFileDialog . getOpenFileName ( self , m . select_cap , \n 
path , "*.cap" ) \n 
if not cap : \n 
~~ settings . setValue ( , os . path . dirname ( cap ) ) \n 
worker = QtCore . QCoreApplication . instance ( ) . worker \n 
self . _cap = os . path . basename ( cap ) \n 
worker . post ( m . installing , partial ( self . _neo . install_app , cap ) , \n 
self . install_done ) \n 
~~ @ QtCore . Slot ( object ) \n 
def install_done ( self , status ) : \n 
~~~ if status : \n 
~~~ print status \n 
QtGui . QMessageBox . warning ( self , m . error_installing , \n 
m . error_installing_1 % self . _cap ) \n 
~~ self . set_neo ( self . _neo ) \n 
~~ def open_app ( self , index ) : \n 
~~~ readable = index . data ( ) \n 
aid = readable [ readable . rindex ( ) + 1 : readable . rindex ( ) ] \n 
appletmanager = QtCore . QCoreApplication . instance ( ) . appletmanager \n 
self . applet . emit ( appletmanager . get_applet ( aid ) ) \n 
~~ def tab_changed ( self , index ) : \n 
~~~ if index != self . index : \n 
~~~ while self . _neo . locked : \n 
~~~ self . _neo . unlock ( ) \n 
~~~ del self . _neo . key \n 
pw , ok = get_text ( self , m . key_required , m . key_required_desc ) \n 
~~~ self . parent . setCurrentIndex ( 0 ) \n 
~~ self . _neo . key = pw \n 
~~ ~~ appletmanager = QtCore . QCoreApplication . instance ( ) . appletmanager \n 
self . _apps = filter ( None , map ( appletmanager . get_applet , \n 
self . _neo . list_apps ( ) ) ) \n 
self . _apps_list . model ( ) . setStringList ( \n 
if not neo or not neo . has_ccid : \n 
~~~ self . parent . setTabEnabled ( self . index , False ) \n 
self . parent . setTabToolTip ( self . index , m . requires_ccid ) \n 
~~ self . parent . setTabEnabled ( self . index , True ) \n 
self . parent . setTabToolTip ( self . index , None ) \n 
if neo . locked : \n 
~~~ neo . unlock ( ) \n 
neo . list_apps ( ) ) ) \n 
~~ ~~ from . properties import * \n 
from . strategy import * \n 
from nose . plugins . attrib import attr \n 
from tornado . testing import gen_test \n 
from . base_tests import GraphPropertyBaseClassTestCase , create_key \n 
from goblin import connection \n 
from goblin . properties . properties import Integer , Short , PositiveInteger , Long , PositiveLong \n 
from goblin . models import Vertex \n 
from goblin . _compat import print_ , long_ \n 
@ attr ( , , ) \n 
class IntegerPropertyTestCase ( GraphPropertyBaseClassTestCase ) : \n 
~~~ klass = Integer \n 
good_cases = ( 1 , 0 , None ) \n 
bad_cases = ( , , 1.1 , [ ] , [ 1 ] , { } , { : 1 } ) \n 
@ gen_test \n 
def test_manual_schema ( self ) : \n 
~~~ key = IntegerTestVertex . get_property_by_name ( ) \n 
label = IntegerTestVertex . get_label ( ) \n 
yield create_key ( key , ) \n 
stream = yield connection . execute_query ( \n 
bindings = { : label , : key , : 101 } ) \n 
resp = yield stream . read ( ) \n 
resp . data [ 0 ] [ ] [ key ] [ 0 ] [ ] , 101 ) \n 
~~ @ gen_test \n 
def test_violate_manual_schema_int ( self ) : \n 
with self . assertRaises ( RuntimeError ) : \n 
~~~ stream = yield connection . execute_query ( \n 
bindings = { : label , : key , : } ) \n 
~~ ~~ @ gen_test \n 
def test_violate_manual_schema_long ( self ) : \n 
def test_violate_manual_schema_short ( self ) : \n 
~~ ~~ ~~ class IntegerTestVertex ( Vertex ) : \n 
~~~ element_type = \n 
test_val1 = Integer ( ) \n 
test_val2 = Integer ( ) \n 
test_val3 = Integer ( ) \n 
~~ @ attr ( , , ) \n 
class IntegerVertexTestCase ( GraphPropertyBaseClassTestCase ) : \n 
~~~ @ gen_test \n 
def test_integer_io ( self ) : \n 
key = IntegerTestVertex . get_property_by_name ( ) \n 
dt = yield IntegerTestVertex . create ( test_val1 = 1 ) \n 
dt2 = yield IntegerTestVertex . get ( dt . _id ) \n 
self . assertEqual ( dt2 . test_val1 , dt . test_val1 ) \n 
yield dt2 . delete ( ) \n 
dt = yield IntegerTestVertex . create ( test_val1 = 2 ) \n 
self . assertEqual ( dt2 . test_val1 , 2 ) \n 
~~ ~~ @ attr ( , , ) \n 
class LongPropertyTestCase ( GraphPropertyBaseClassTestCase ) : \n 
~~~ klass = Long \n 
good_cases = ( long_ ( 1 ) , long_ ( 0 ) , None ) \n 
~~ class LongTestVertex ( Vertex ) : \n 
test_val = Long ( ) \n 
class LongVertexTestCase ( GraphPropertyBaseClassTestCase ) : \n 
def test_long_io ( self ) : \n 
key = LongTestVertex . get_property_by_name ( ) \n 
dt = yield LongTestVertex . create ( test_val = 1 ) \n 
dt2 = yield LongTestVertex . get ( dt . _id ) \n 
self . assertEqual ( dt2 . test_val , dt . test_val ) \n 
dt = yield LongTestVertex . create ( test_val = 2 ) \n 
self . assertEqual ( dt2 . test_val , 2 ) \n 
class ShortPropertyTestCase ( IntegerPropertyTestCase ) : \n 
~~~ klass = Short \n 
~~ class ShortTestVertex ( Vertex ) : \n 
test_val = Short ( ) \n 
class ShortVertexTestCase ( GraphPropertyBaseClassTestCase ) : \n 
def test_short_io ( self ) : \n 
key = ShortTestVertex . get_property_by_name ( ) \n 
dt = yield ShortTestVertex . create ( test_val = 1 ) \n 
dt2 = yield ShortTestVertex . get ( dt . _id ) \n 
dt = yield ShortTestVertex . create ( test_val = 2 ) \n 
class PositiveIntegerPropertyTestCase ( IntegerPropertyTestCase ) : \n 
~~~ klass = PositiveInteger \n 
~~ class PositiveIntegerTestVertex ( Vertex ) : \n 
test_val = PositiveInteger ( ) \n 
class PositiveIntegerVertexTestCase ( GraphPropertyBaseClassTestCase ) : \n 
def test_positive_integer_io ( self ) : \n 
key = PositiveIntegerTestVertex . get_property_by_name ( ) \n 
dt = yield PositiveIntegerTestVertex . create ( test_val = 1 ) \n 
dt2 = yield PositiveIntegerTestVertex . get ( dt . _id ) \n 
dt = yield PositiveIntegerTestVertex . create ( test_val = 2 ) \n 
class PositiveLongPropertyTestCase ( LongPropertyTestCase ) : \n 
~~~ klass = PositiveLong \n 
~~ class PositiveLongTestVertex ( Vertex ) : \n 
test_val = PositiveLong ( ) \n 
class PositiveLongVertexTestCase ( GraphPropertyBaseClassTestCase ) : \n 
def test_positive_long_io ( self ) : \n 
key = PositiveLongTestVertex . get_property_by_name ( ) \n 
dt = yield PositiveLongTestVertex . create ( test_val = 1 ) \n 
dt2 = yield PositiveLongTestVertex . get ( dt . _id ) \n 
dt = yield PositiveLongTestVertex . create ( test_val = 2 ) \n 
from . . modules import session \n 
def _objectify ( s ) : \n 
~~~ if isinstance ( s , str ) : \n 
~~~ return json . loads ( s ) \n 
~~ class SessionDecoder ( json . JSONDecoder ) : \n 
~~~ def decode ( self , s ) : \n 
~~~ o = _objectify ( s ) \n 
~~~ name = o [ "name" ] \n 
windows = [ \n 
WindowDecoder . decode ( self , w ) for w in o [ "windows" ] \n 
~~~ return session . Session ( name , windows ) \n 
~~ return json . JSONDecoder . decode ( self , o ) \n 
~~ ~~ class WindowDecoder ( json . JSONDecoder ) : \n 
~~~ project = o [ "project" ] \n 
project_path = o [ "project_path" ] \n 
views = [ \n 
ViewDecoder . decode ( self , view ) for view in o [ "views" ] \n 
~~~ return session . Window ( project , project_path , views ) \n 
~~ ~~ class ViewDecoder ( json . JSONDecoder ) : \n 
~~~ file_path = o [ "file_path" ] \n 
active = o [ "active" ] \n 
sel_regions = [ \n 
RegionDecoder . decode ( self , region ) for region in o [ "sel_regions" ] \n 
visible_region = RegionDecoder . decode ( self , o [ "visible_region" ] ) \n 
~~~ return session . View ( file_path , active , sel_regions , visible_region ) \n 
~~ ~~ class RegionDecoder ( json . JSONDecoder ) : \n 
~~~ a = o [ 0 ] \n 
b = o [ 1 ] \n 
~~~ return sublime . Region ( a , b ) \n 
from xml . dom import minidom \n 
from pythonzimbra . tools . xmlserializer import dom_to_dict \n 
from . response import Response \n 
class ResponseXml ( Response ) : \n 
~~~ response_doc = None \n 
response_type = "xml" \n 
~~~ super ( ResponseXml , self ) . clean ( ) \n 
self . response_doc = None \n 
~~ def set_response ( self , response_text ) : \n 
~~~ if not isinstance ( response_text , str ) : \n 
~~~ response_text = response_text . encode ( "utf-8" ) \n 
~~ self . response_doc = minidom . parseString ( response_text ) \n 
~~ def get_body ( self ) : \n 
~~~ return self . _filter_response ( \n 
dom_to_dict ( \n 
self . response_doc . getElementsByTagNameNS ( \n 
"*" , "Body" \n 
) . item ( 0 ) . firstChild \n 
~~ def get_header ( self ) : \n 
"*" , "Header" \n 
~~ def is_batch ( self ) : \n 
~~~ if self . response_doc . getElementsByTagName ( "BatchResponse" ) . length > 0 : \n 
~~ def get_batch ( self ) : \n 
~~~ if not self . is_batch ( ) : \n 
~~ ret_dict = { \n 
has_fault = False \n 
search_node = self . response_doc . getElementsByTagNameNS ( \n 
"*" , "BatchResponse" \n 
) . item ( 0 ) \n 
for child in search_node . childNodes : \n 
~~~ tag = child . tagName \n 
if ":" in tag : \n 
~~~ tag = tag . split ( ":" ) [ 1 ] \n 
~~ if tag == : \n 
~~~ has_fault = True \n 
~~ request_id = child . getAttribute ( "requestId" ) \n 
ret_dict [ ] [ request_id ] = tag \n 
if tag not in list ( ret_dict [ ] . keys ( ) ) : \n 
~~~ ret_dict [ ] [ tag ] = [ ] \n 
~~ ret_dict [ ] [ tag ] . append ( request_id ) \n 
~~ ret_dict [ ] = has_fault \n 
return ret_dict \n 
~~ def get_response ( self , request_id = 0 ) : \n 
~~~ if self . is_batch ( ) : \n 
~~~ search_node = self . response_doc . getElementsByTagNameNS ( \n 
~~~ if int ( child . getAttribute ( "requestId" ) ) == request_id : \n 
~~~ return self . _filter_response ( dom_to_dict ( child ) ) \n 
return self . _filter_response ( dom_to_dict ( search_node . firstChild ) ) \n 
~~ ~~ def get_fault_code ( self ) : \n 
~~~ ret_dict = { } \n 
~~~ ret_dict [ child . getAttribute ( "requestId" ) ] = self . get_response ( \n 
int ( child . getAttribute ( "requestId" ) ) \n 
) [ "Fault" ] [ "Detail" ] [ "Error" ] [ "Code" ] \n 
~~ return ret_dict \n 
~~~ return self . get_response ( ) [ "Fault" ] [ "Detail" ] [ "Error" ] [ \n 
"Code" ] \n 
~~ ~~ def get_fault_message ( self ) : \n 
) [ "Fault" ] [ "Reason" ] [ "Text" ] \n 
~~~ return self . get_response ( ) [ "Fault" ] [ "Reason" ] [ "Text" ] \n 
~~ ~~ ~~ import sys \n 
from game . settings import * \n 
from game import color as clr \n 
class Piece ( object ) : \n 
def __init__ ( self , x , y , colour ) : \n 
self . state = \n 
self . flipped = False \n 
self . colour = colour \n 
self . drawing = { \n 
"WHITE" : self . draw_white , \n 
"BLACK" : self . draw_black , \n 
"BOARD" : self . draw_board , \n 
"MOVE" : self . draw_move } \n 
~~ def draw ( self ) : \n 
if self . state in self . drawing : \n 
~~~ result = self . drawing [ self . state ] ( ) \n 
~~ def draw_white ( self ) : \n 
if self . flipped : \n 
~~~ if self . colour : \n 
~~~ return clr . format_color ( , fg = clr . rgb ( 4 , 4 , 4 ) , bg = clr . rgb ( 5 , 5 , 5 ) ) \n 
~~~ return clr . format_color ( , bg = clr . rgb ( 5 , 5 , 5 ) ) \n 
~~ ~~ def draw_black ( self ) : \n 
~~~ return clr . format_color ( , fg = clr . rgb ( 2 , 2 , 2 ) , bg = clr . rgb ( 1 , 1 , 1 ) ) \n 
~~~ return clr . format_color ( , bg = clr . rgb ( 1 , 1 , 1 ) ) \n 
~~ ~~ def draw_board ( self ) : \n 
if self . colour : \n 
~~~ return clr . format_color ( , bg = clr . rgb ( 0 , 3 , 0 ) ) \n 
~~ ~~ def draw_move ( self ) : \n 
~~~ return clr . format_color ( , fg = clr . rgb ( 5 , 0 , 0 ) , bg = clr . rgb ( 0 , 3 , 0 ) ) \n 
~~ def set_black ( self ) : \n 
~~ def set_white ( self ) : \n 
~~ def set_move ( self ) : \n 
self . state = MOVE \n 
~~ def set_board ( self ) : \n 
self . state = BOARD \n 
~~ def get_state ( self ) : \n 
return self . state \n 
~~ def flip ( self ) : \n 
if self . state == BLACK : \n 
~~~ self . state = WHITE \n 
~~ elif self . state == WHITE : \n 
~~~ self . state = BLACK \n 
~~~ raise ValueError \n 
~~ self . flipped = True \n 
~~ def set_flipped ( self ) : \n 
self . flipped = True \n 
~~ def reset_flipped ( self ) : \n 
~~ def is_flipped ( self ) : \n 
return self . flipped \n 
~~ def get_position ( self ) : \n 
return self . x , self . y \n 
~~~ import numpy as np \n 
~~~ np = None \n 
~~ if np : \n 
~~~ import pandas as pd \n 
~~~ pd = None \n 
~~ from . import Converter , Options \n 
class NumpyArrayConverter ( Converter ) : \n 
~~~ writes_types = np . ndarray \n 
def base_reader ( cls , options ) : \n 
~~~ return ( \n 
super ( NumpyArrayConverter , cls ) . base_reader ( \n 
Options ( options ) \n 
. defaults ( empty = np . nan ) \n 
def read_value ( cls , value , options ) : \n 
~~~ dtype = options . get ( , None ) \n 
copy = options . get ( , True ) \n 
order = options . get ( , None ) \n 
ndim = options . get ( , None ) or 0 \n 
return np . array ( value , dtype = dtype , copy = copy , order = order , ndmin = ndim ) \n 
def write_value ( cls , value , options ) : \n 
~~~ value = np . where ( np . isnan ( value ) , None , value ) \n 
value = value . tolist ( ) \n 
~~~ if pd : \n 
~~~ value [ pd . isnull ( value ) ] = None \n 
~~ ~~ NumpyArrayConverter . register ( np . array , np . ndarray ) \n 
class Settings ( object ) : \n 
API = "rgIYSFIeGBxVXOPy22QzA" \n 
API_SECRET = "VX7ohOHpJm1mXlGX6XS08JcT4Vp8j83QhRNo1SVRevb" \n 
AUTH_FILE = os . path . expanduser ( "~/.pyweet" ) \n 
urls = { \n 
#start_imports \n 
from random import Random \n 
from math import cos \n 
from math import pi \n 
from inspyred import ec \n 
from inspyred . ec import terminators \n 
#end_imports \n 
def generate_rastrigin ( random , args ) : \n 
~~~ size = args . get ( , 10 ) \n 
return [ random . uniform ( - 5.12 , 5.12 ) for i in range ( size ) ] \n 
~~ def evaluate_rastrigin ( candidates , args ) : \n 
~~~ fitness = [ ] \n 
for cs in candidates : \n 
~~~ fit = 10 * len ( cs ) + sum ( [ ( ( x - 1 ) ** 2 - 10 * cos ( 2 * pi * ( x - 1 ) ) ) for x in cs ] ) \n 
fitness . append ( fit ) \n 
~~ return fitness \n 
#start_main \n 
~~ rand = Random ( ) \n 
rand . seed ( int ( time ( ) ) ) \n 
es = ec . ES ( rand ) \n 
es . terminator = terminators . evaluation_termination \n 
final_pop = es . evolve ( generator = generate_rastrigin , \n 
evaluator = evaluate_rastrigin , \n 
pop_size = 100 , \n 
maximize = False , \n 
bounder = ec . Bounder ( - 5.12 , 5.12 ) , \n 
max_evaluations = 20000 , \n 
mutation_rate = 0.25 , \n 
num_inputs = 3 ) \n 
final_pop . sort ( reverse = True ) \n 
print ( final_pop [ 0 ] ) \n 
#end_main \n 
import inspyred \n 
def main ( prng = None , display = False ) : \n 
~~~ if prng is None : \n 
~~~ prng = Random ( ) \n 
prng . seed ( time ( ) ) \n 
~~ problem = inspyred . benchmarks . Sphere ( 2 ) \n 
ea = inspyred . ec . SA ( prng ) \n 
ea . terminator = inspyred . ec . terminators . evaluation_termination \n 
final_pop = ea . evolve ( evaluator = problem . evaluator , \n 
generator = problem . generator , \n 
maximize = problem . maximize , \n 
bounder = problem . bounder , \n 
max_evaluations = 30000 ) \n 
if display : \n 
~~~ best = max ( final_pop ) \n 
print ( . format ( str ( best ) ) ) \n 
~~ return ea \n 
~~~ main ( display = True ) \n 
import SocketServer \n 
class NetworkMigrator ( SocketServer . ThreadingMixIn , SocketServer . TCPServer ) : \n 
def __init__ ( self , server_address , client_addresses , max_migrants = 1 ) : \n 
~~~ self . _lock = threading . Lock ( ) \n 
SocketServer . TCPServer . __init__ ( self , server_address , None ) \n 
self . client_addresses = client_addresses \n 
self . migrants = collections . deque ( maxlen = max_migrants ) \n 
t = threading . Thread ( target = self . serve_forever ) \n 
t . setDaemon ( True ) \n 
t . start ( ) \n 
self . __name__ = self . __class__ . __name__ \n 
~~ def finish_request ( self , request , client_address ) : \n 
~~~ rbufsize = - 1 \n 
wbufsize = 0 \n 
rfile = request . makefile ( , rbufsize ) \n 
wfile = request . makefile ( , wbufsize ) \n 
pickle_data = rfile . readline ( ) . strip ( ) \n 
migrant = pickle . loads ( pickle_data ) \n 
with self . _lock : \n 
~~~ self . migrants . append ( migrant ) \n 
~~ if not wfile . closed : \n 
~~~ wfile . flush ( ) \n 
~~ wfile . close ( ) \n 
rfile . close ( ) \n 
~~~ sys . exc_traceback = None \n 
~~ ~~ def __call__ ( self , random , population , args ) : \n 
evaluate_migrant = args . setdefault ( , False ) \n 
client_address = random . choice ( self . client_addresses ) \n 
migrant_index = random . randint ( 0 , len ( population ) - 1 ) \n 
pickle_data = pickle . dumps ( population [ migrant_index ] ) \n 
sock = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
~~~ sock . connect ( client_address ) \n 
sock . send ( pickle_data + ) \n 
~~~ sock . close ( ) \n 
~~ migrant = None \n 
~~~ if len ( self . migrants ) > 0 : \n 
~~~ migrant = self . migrants . popleft ( ) \n 
~~ ~~ if migrant is not None : \n 
~~~ if evaluate_migrant : \n 
~~~ fit = args . _ec . evaluator ( [ migrant ] , args ) \n 
migrant . fitness = fit [ 0 ] \n 
args . _ec . num_evaluations += 1 \n 
~~ population [ migrant_index ] = migrant \n 
~~ return population \n 
~~~ return str ( self . migrants ) \n 
import setuptools \n 
os . putenv ( , ) \n 
README = os . path . join ( os . path . dirname ( __file__ ) , ) \n 
if not os . path . exists ( README ) : \n 
~~~ os . symlink ( README + , README ) \n 
~~ description = ( \n 
with open ( README ) as f : \n 
~~~ long_description = . join ( f . read ( ) . split ( ) [ 1 : ] ) \n 
~~ setuptools . setup ( \n 
description = description , \n 
from passlib . hash import pbkdf2_sha256 \n 
from passlib . utils import generate_password \n 
~~~ plaintext = sys . argv [ 1 ] \n 
~~~ plaintext = generate_password ( ) \n 
~~ print plaintext , pbkdf2_sha256 . encrypt ( plaintext , rounds = 200000 , salt_size = 16 ) \n 
from __future__ import absolute_import , unicode_literals \n 
from braces . views import LoginRequiredMixin \n 
from django . views . generic import TemplateView \n 
class HomeView ( LoginRequiredMixin , TemplateView ) : \n 
~~~ template_name = \n 
from pebl import prior , config , evaluator , result , network \n 
from pebl . learner . base import Learner \n 
from pebl . taskcontroller . base import Task \n 
class ListLearner ( Learner ) : \n 
~~~ _params = ( \n 
config . StringParameter ( \n 
default = \n 
def __init__ ( self , data_ = None , prior_ = None , networks = None ) : \n 
super ( ListLearner , self ) . __init__ ( data_ , prior_ ) \n 
self . networks = networks \n 
if not networks : \n 
~~~ variables = self . data . variables \n 
_net = lambda netstr : network . Network ( variables , netstr ) \n 
netstrings = config . get ( ) . splitlines ( ) \n 
self . networks = ( _net ( s ) for s in netstrings if s ) \n 
~~~ self . result = result . LearnerResult ( self ) \n 
self . evaluator = evaluator . fromconfig ( self . data , prior_ = self . prior ) \n 
self . result . start_run ( ) \n 
for net in self . networks : \n 
~~~ self . result . add_network ( net , self . evaluator . score_network ( net ) ) \n 
~~ self . result . stop_run ( ) \n 
~~ def split ( self , count ) : \n 
nets = list ( self . networks ) \n 
numnets = len ( nets ) \n 
netspertask = numnets / count \n 
indices = [ [ i , i + netspertask ] for i in xrange ( 0 , numnets , netspertask ) ] \n 
if len ( indices ) > count : \n 
~~~ indices . pop ( - 1 ) \n 
indices [ - 1 ] [ 1 ] = numnets - 1 \n 
~~ return [ ListLearner ( self . data , self . prior , nets [ i : j ] ) for i , j in indices ] \n 
~~~ d = self . __dict__ \n 
d [ ] = list ( d [ ] ) \n 
import numpy as N \n 
from pebl import network , data , config \n 
class TestEdgeSet : \n 
~~~ self . edges = network . EdgeSet ( num_nodes = 6 ) \n 
self . tuplelist = [ ( 0 , 2 ) , ( 0 , 5 ) , ( 1 , 2 ) ] \n 
for edge in self . tuplelist : \n 
~~~ self . edges . add ( edge ) \n 
~~ ~~ def test_add ( self ) : \n 
~~~ self . edges . add ( ( 5 , 1 ) ) \n 
assert set ( self . edges ) == set ( self . tuplelist + [ ( 5 , 1 ) ] ) \n 
~~ def test_add_many ( self ) : \n 
~~~ self . edges . add_many ( [ ( 5 , 1 ) , ( 5 , 2 ) ] ) \n 
assert set ( self . edges ) == set ( self . tuplelist + [ ( 5 , 1 ) , ( 5 , 2 ) ] ) \n 
~~ def test_remove ( self ) : \n 
~~~ self . edges . remove ( ( 0 , 2 ) ) \n 
assert set ( self . edges ) == set ( [ ( 0 , 5 ) , ( 1 , 2 ) ] ) \n 
~~ def test_remove_many ( self ) : \n 
~~~ self . edges . remove_many ( [ ( 0 , 2 ) , ( 0 , 5 ) ] ) \n 
assert set ( self . edges ) == set ( [ ( 1 , 2 ) ] ) \n 
~~ def test_edgeiter ( self ) : \n 
~~ def test_len ( self ) : \n 
~~ def test_addedges1 ( self ) : \n 
~~~ self . edges . add ( ( 0 , 3 ) ) \n 
~~ def test_incoming ( self ) : \n 
~~ def test_outgoing ( self ) : \n 
~~ def test_parents ( self ) : \n 
~~ def test_children ( self ) : \n 
~~ def test_contains1 ( self ) : \n 
~~ def test_contains2 ( self ) : \n 
~~ def test_clear ( self ) : \n 
~~~ self . edges . clear ( ) \n 
~~ def test_adjacency_matrix ( self ) : \n 
[ 0 , 0 , 1 , 0 , 0 , 1 ] , \n 
[ 0 , 0 , 1 , 0 , 0 , 0 ] , \n 
[ 0 , 0 , 0 , 0 , 0 , 0 ] , \n 
~~ ~~ class TestIsAcyclic : \n 
~~~ self . net = network . Network ( [ data . DiscreteVariable ( i , 3 ) for i in xrange ( 6 ) ] ) \n 
for edge in [ ( 0 , 1 ) , ( 0 , 3 ) , ( 1 , 2 ) ] : \n 
~~~ self . net . edges . add ( edge ) \n 
~~ ~~ def test_loopchecking ( self ) : \n 
~~ def test_loopchecking2 ( self ) : \n 
~~~ self . net . edges . add ( ( 2 , 0 ) ) \n 
~~ ~~ class TestNetwork : \n 
~~ ~~ def test_as_pydot ( self ) : \n 
~~ def test_as_image ( self ) : \n 
~~~ filename = "testnet.png" \n 
self . net . as_image ( filename = filename ) \n 
file_exists = filename in os . listdir ( "." ) \n 
if file_exists : \n 
~~~ os . remove ( "./" + filename ) \n 
~~ def test_as_dotstring ( self ) : \n 
~~ def test_as_dotfile ( self ) : \n 
~~~ self . net . as_dotfile ( ) \n 
~~ def test_as_string ( self ) : \n 
~~ def test_layout ( self ) : \n 
~~~ self . net . layout ( ) \n 
~~ ~~ class TestNetworkFromListOfEdges : \n 
~~~ self . net = network . Network ( \n 
[ data . DiscreteVariable ( str ( i ) , 3 ) for i in xrange ( 6 ) ] , \n 
[ ( 0 , 1 ) , ( 4 , 5 ) , ( 2 , 3 ) ] \n 
~~ def test_number_of_edges ( self ) : \n 
~~~ assert len ( list ( self . net . edges ) ) == 3 \n 
~~ def test_edges_exist ( self ) : \n 
~~~ assert ( 0 , 1 ) in self . net . edges and ( 4 , 5 ) in self . net . edges and ( 2 , 3 ) in self . net . edges \n 
~~ ~~ class TestNetworkFromString ( TestNetworkFromListOfEdges ) : \n 
"0,1;4,5;2,3" \n 
~~ ~~ class TestRandomNetwork : \n 
~~~ self . nodes = [ data . DiscreteVariable ( str ( i ) , 3 ) for i in xrange ( 6 ) ] \n 
~~ def test_acyclic ( self ) : \n 
~~~ net = network . random_network ( self . nodes ) \n 
~~ def test_required_edges ( self ) : \n 
~~~ net = network . random_network ( self . nodes , required_edges = [ ( 0 , 1 ) , ( 3 , 0 ) ] ) \n 
assert net . is_acyclic ( ) == True and ( 0 , 1 ) in net . edges and ( 3 , 0 ) in net . edges \n 
~~ def test_prohibited_edges ( self ) : \n 
~~~ net = network . random_network ( self . nodes , prohibited_edges = [ ( 0 , 1 ) , ( 3 , 0 ) ] ) \n 
assert net . is_acyclic ( ) == True and ( 0 , 1 ) not in net . edges and ( 3 , 0 ) not in net . edges \n 
~~ def test_required_and_prohibited_edges ( self ) : \n 
~~~ net = network . random_network ( self . nodes , required_edges = [ ( 0 , 1 ) , ( 3 , 0 ) ] , \n 
prohibited_edges = [ ( 2 , 3 ) , ( 1 , 4 ) ] ) \n 
assert net . is_acyclic ( ) == True and ( 0 , 1 ) in net . edges and ( 3 , 0 ) in net . edges and ( 2 , 3 ) not in net . edges and ( 1 , 4 ) not in net . edges \n 
import django . core . exceptions \n 
~~~ from django . conf import settings \n 
~~ except django . core . exceptions . ImproperlyConfigured : \n 
logger = logging . getLogger ( "aboutyou.middleware" ) \n 
class AboutyouMiddleware ( object ) : \n 
def process_request ( self , request ) : \n 
~~~ user = None \n 
if not request . user . is_authenticated ( ) : \n 
~~~ access_token = None \n 
if "HTTP_AUTHORIZATION" in request . META : \n 
~~~ access_token = request . META [ "HTTP_AUTHORIZATION" ] . split ( ) [ 1 ] \n 
logger . debug ( , access_token ) \n 
~~~ code = request . GET . get ( ) \n 
state = request . GET . get ( ) \n 
if code and state : \n 
~~~ redirect_uri = request . build_absolut_uri ( settings . AUTH_REDIRECT_PATH ) \n 
access_token = settings . AUTH . access_token ( code , redirect_uri ) [ \n 
~~ ~~ if access_token : \n 
~~~ user = authenticate ( access_token = access_token ) \n 
if user is not None and not user . is_anonymous ( ) : \n 
~~ ~~ ~~ ~~ except Exception : \n 
~~~ logger . exception ( ) \n 
~~ ~~ ~~ from authorize_net import AuthorizeNet \n 
from innovative_gw import InnovativeGW \n 
from firstdata_legacy import FirstDataLegacy \n 
from plugnpay import PlugnPay \n 
from stripe_com import Stripe \n 
from samurai_ff import Samurai \n 
from firstdata import FirstData \n 
from abusehelper . core import bot \n 
from . import host_or_ip , resolve_level , split_description , AbuseCHFeedBot \n 
class ZeusCcBot ( AbuseCHFeedBot ) : \n 
~~~ feed_malware = "zeus" \n 
feed_type = "c&c" \n 
feeds = bot . ListParam ( default = [ "https://zeustracker.abuse.ch/rss.php" ] ) \n 
def parse_title ( self , title ) : \n 
~~~ pieces = title . split ( None , 1 ) \n 
yield host_or_ip ( pieces [ 0 ] ) \n 
if len ( pieces ) > 1 : \n 
~~~ date = pieces [ 1 ] \n 
date = re . sub ( "[()]" , "" , date ) \n 
~~ ~~ def parse_description ( self , description ) : \n 
~~~ for key , value in split_description ( description ) : \n 
~~~ if key == "status" : \n 
~~~ yield key , value \n 
~~ elif key == "level" : \n 
~~~ yield "description" , resolve_level ( value ) \n 
~~~ yield "ip" , value \n 
~~ ~~ ~~ ~~ if __name__ == "__main__" : \n 
~~~ ZeusCcBot . from_command_line ( ) . execute ( ) \n 
import idiokit \n 
from abusehelper . core import bot , events \n 
class StressBot ( bot . FeedBot ) : \n 
@ idiokit . stream \n 
def feed ( self ) : \n 
~~~ event = events . Event . from_unicode ( self . data . decode ( "utf-8" ) ) \n 
~~~ yield idiokit . send ( event ) \n 
~~~ StressBot . from_command_line ( ) . execute ( ) \n 
from . . atoms import String , RegExp , IP , DomainName \n 
from . . rules import And , Or , No , Match , NonMatch , Fuzzy \n 
from ... events import Event \n 
class TestRules ( unittest . TestCase ) : \n 
~~~ def test_results_should_get_cached ( self ) : \n 
~~~ rule = Match ( "a" , "a" ) \n 
cache = { } \n 
rule . match ( Event ( ) , cache ) \n 
self . assertTrue ( rule in cache ) \n 
self . assertFalse ( cache [ rule ] ) \n 
~~ def test_cached_results_should_get_used ( self ) : \n 
cache = { rule : True } \n 
self . assertTrue ( rule . match ( Event ( ) , cache ) ) \n 
~~ ~~ class TestAnd ( unittest . TestCase ) : \n 
~~~ def test_can_not_be_initialized_with_zero_arguments ( self ) : \n 
~~~ self . assertRaises ( TypeError , And ) \n 
~~ def test_commutativity ( self ) : \n 
~~~ a = Match ( "a" , "a" ) \n 
b = Match ( "b" , "b" ) \n 
self . assertEqual ( And ( a , b ) , And ( b , a ) ) \n 
~~ def test_redundant_arguments_get_deduplicated ( self ) : \n 
self . assertEqual ( And ( a , a ) , And ( a ) ) \n 
~~ _options = [ \n 
And ( Match ( "a" ) , Match ( "b" ) ) \n 
def test_pickling_and_unpickling ( self ) : \n 
~~~ for option in self . _options : \n 
~~~ self . assertEqual ( option , pickle . loads ( pickle . dumps ( option ) ) ) \n 
~~ ~~ def test_repr ( self ) : \n 
~~~ self . assertEqual ( option , eval ( repr ( option ) ) ) \n 
~~ ~~ ~~ class TestOr ( unittest . TestCase ) : \n 
~~~ self . assertRaises ( TypeError , Or ) \n 
self . assertEqual ( Or ( a , b ) , Or ( b , a ) ) \n 
self . assertEqual ( Or ( a , a ) , Or ( a ) ) \n 
Or ( Match ( "a" ) , Match ( "b" ) ) \n 
~~ ~~ ~~ class TestNo ( unittest . TestCase ) : \n 
~~~ _options = [ \n 
No ( Match ( "a" ) ) \n 
~~ ~~ ~~ class TestMatch ( unittest . TestCase ) : \n 
~~~ def test_init_conversions ( self ) : \n 
~~~ self . assertEqual ( \n 
Match ( "a" , "b" ) , \n 
Match ( "a" , String ( "b" ) ) ) \n 
Match ( "a" , re . compile ( "b" ) ) , \n 
Match ( "a" , RegExp ( "b" ) ) ) \n 
~~ def test_domainname ( self ) : \n 
~~~ matcher = Match ( "a" , DomainName ( "*.example" ) ) \n 
self . assertTrue ( matcher . match ( Event ( { "a" : "domain.example" } ) ) ) \n 
self . assertTrue ( matcher . match ( Event ( { "a" : "sub.domain.example" } ) ) ) \n 
self . assertFalse ( matcher . match ( Event ( { "a" : "*.example" } ) ) ) \n 
self . assertFalse ( matcher . match ( Event ( { "a" : "*.domain.example" } ) ) ) \n 
self . assertFalse ( matcher . match ( Event ( { "a" : "domain.test" } ) ) ) \n 
Match ( ) , \n 
Match ( "a" , String ( "b" ) ) , \n 
Match ( "a" , RegExp ( "b" ) ) , \n 
Match ( "a" , IP ( "192.0.2.0" ) ) , \n 
Match ( "a" , DomainName ( "*.example" ) ) \n 
~~ ~~ ~~ class TestNonMatch ( unittest . TestCase ) : \n 
NonMatch ( "a" , "b" ) , \n 
NonMatch ( "a" , String ( "b" ) ) ) \n 
NonMatch ( "a" , re . compile ( "b" ) ) , \n 
NonMatch ( "a" , RegExp ( "b" ) ) ) \n 
~~~ matcher = NonMatch ( "a" , DomainName ( "*.example" ) ) \n 
self . assertFalse ( matcher . match ( Event ( { "a" : "domain.example" } ) ) ) \n 
self . assertFalse ( matcher . match ( Event ( { "a" : "sub.domain.example" } ) ) ) \n 
self . assertTrue ( matcher . match ( Event ( { "a" : "*.example" } ) ) ) \n 
self . assertTrue ( matcher . match ( Event ( { "a" : "*.domain.example" } ) ) ) \n 
self . assertTrue ( matcher . match ( Event ( { "a" : "domain.test" } ) ) ) \n 
NonMatch ( ) , \n 
NonMatch ( "a" , String ( "b" ) ) , \n 
NonMatch ( "a" , RegExp ( "b" ) ) , \n 
NonMatch ( "a" , IP ( "192.0.2.0" ) ) , \n 
NonMatch ( "a" , DomainName ( "*.example" ) ) \n 
~~ ~~ ~~ class TestFuzzy ( unittest . TestCase ) : \n 
~~~ def test_base ( self ) : \n 
~~~ rule = Fuzzy ( String ( "a" ) ) \n 
self . assertTrue ( rule . match ( Event ( { "a" : "xy" } ) ) ) \n 
self . assertTrue ( rule . match ( Event ( { "xy" : "a" } ) ) ) \n 
self . assertTrue ( rule . match ( Event ( { "ba" : "xy" } ) ) ) \n 
self . assertTrue ( rule . match ( Event ( { "xy" : "ba" } ) ) ) \n 
self . assertFalse ( rule . match ( Event ( { "xy" : "xy" } ) ) ) \n 
self . assertTrue ( rule . match ( Event ( { "A" : "xy" } ) ) ) \n 
self . assertTrue ( rule . match ( Event ( { "xy" : "A" } ) ) ) \n 
rule = Fuzzy ( RegExp ( "a" ) ) \n 
self . assertFalse ( rule . match ( Event ( { "A" : "xy" } ) ) ) \n 
self . assertFalse ( rule . match ( Event ( { "xy" : "A" } ) ) ) \n 
Fuzzy ( String ( "a" ) ) , \n 
Fuzzy ( RegExp ( "a" ) ) , \n 
Fuzzy ( IP ( "192.0.2.0" ) ) , \n 
Fuzzy ( DomainName ( "*.example" ) ) \n 
from . mltools import * \n 
import optunity \n 
import ast \n 
if sys . version_info < ( 3 , 0 ) : \n 
~~~ import ConfigParser as configparser \n 
~~~ import configparser \n 
~~ from pkg_resources import Requirement , resource_filename \n 
_ELMK_CONFIG = resource_filename ( Requirement . parse ( "elm" ) , "elm/elmk.cfg" ) \n 
class ELMKernel ( MLTools ) : \n 
def __init__ ( self , params = [ ] ) : \n 
super ( self . __class__ , self ) . __init__ ( ) \n 
self . regressor_name = "elmk" \n 
self . available_kernel_functions = [ "rbf" , "linear" , "poly" ] \n 
self . default_param_kernel_function = "rbf" \n 
self . default_param_c = 9 \n 
self . default_param_kernel_params = [ - 15 ] \n 
self . output_weight = [ ] \n 
self . training_patterns = [ ] \n 
if not params : \n 
~~~ self . param_kernel_function = self . default_param_kernel_function \n 
self . param_c = self . default_param_c \n 
self . param_kernel_params = self . default_param_kernel_params \n 
~~~ self . param_kernel_function = params [ 0 ] \n 
self . param_c = params [ 1 ] \n 
self . param_kernel_params = params [ 2 ] \n 
~~ ~~ def _kernel_matrix ( self , training_patterns , kernel_type , kernel_param , \n 
test_patterns = None ) : \n 
number_training_patterns = training_patterns . shape [ 0 ] \n 
if kernel_type == "rbf" : \n 
~~~ if test_patterns is None : \n 
~~~ temp_omega = np . dot ( \n 
np . sum ( training_patterns ** 2 , axis = 1 ) . reshape ( - 1 , 1 ) , \n 
np . ones ( ( 1 , number_training_patterns ) ) ) \n 
temp_omega = temp_omega + temp_omega . conj ( ) . T \n 
omega = np . exp ( \n 
- ( 2 ** kernel_param [ 0 ] ) * ( temp_omega - 2 * ( np . dot ( \n 
training_patterns , training_patterns . conj ( ) . T ) ) ) ) \n 
~~~ number_test_patterns = test_patterns . shape [ 0 ] \n 
temp1 = np . dot ( \n 
np . ones ( ( 1 , number_test_patterns ) ) ) \n 
temp2 = np . dot ( \n 
np . sum ( test_patterns ** 2 , axis = 1 ) . reshape ( - 1 , 1 ) , \n 
temp_omega = temp1 + temp2 . conj ( ) . T \n 
omega = np . exp ( - ( 2 ** kernel_param [ 0 ] ) * \n 
( temp_omega - 2 * np . dot ( training_patterns , \n 
test_patterns . conj ( ) . T ) ) ) \n 
~~ ~~ elif kernel_type == "linear" : \n 
~~~ omega = np . dot ( training_patterns , training_patterns . conj ( ) . T ) \n 
~~~ omega = np . dot ( training_patterns , test_patterns . conj ( ) . T ) \n 
~~ ~~ elif kernel_type == "poly" : \n 
~~~ kernel_param [ 1 ] = round ( kernel_param [ 1 ] ) \n 
if test_patterns is None : \n 
~~~ temp = np . dot ( training_patterns , training_patterns . conj ( ) . T ) + kernel_param [ 0 ] \n 
omega = temp ** kernel_param [ 1 ] \n 
~~~ temp = np . dot ( training_patterns , test_patterns . conj ( ) . T ) + kernel_param [ 0 ] \n 
~~ return omega \n 
~~ def _local_train ( self , training_patterns , training_expected_targets , \n 
params ) : \n 
~~~ if not params : \n 
~~ self . training_patterns = training_patterns \n 
number_training_patterns = self . training_patterns . shape [ 0 ] \n 
omega_train = self . _kernel_matrix ( self . training_patterns , \n 
self . param_kernel_function , \n 
self . param_kernel_params ) \n 
self . output_weight = np . linalg . solve ( \n 
( omega_train + np . eye ( number_training_patterns ) / \n 
( 2 ** self . param_c ) ) , \n 
training_expected_targets ) . reshape ( - 1 , 1 ) \n 
training_predicted_targets = np . dot ( omega_train , self . output_weight ) \n 
return training_predicted_targets \n 
~~ def _local_test ( self , testing_patterns , testing_expected_targets , \n 
predicting ) : \n 
~~~ omega_test = self . _kernel_matrix ( self . training_patterns , \n 
self . param_kernel_params , \n 
testing_patterns ) \n 
testing_predicted_targets = np . dot ( omega_test . conj ( ) . T , \n 
self . output_weight ) \n 
return testing_predicted_targets \n 
~~ def get_available_kernel_functions ( self ) : \n 
return self . available_kernel_functions \n 
~~ def print_parameters ( self ) : \n 
~~ def search_param ( self , database , dataprocess = None , path_filename = ( "" , "" ) , \n 
save = False , cv = "ts" , of = "rmse" , kf = None , eval = 50 ) : \n 
if kf is None : \n 
~~~ search_kernel_functions = self . available_kernel_functions \n 
~~ elif type ( kf ) is list : \n 
~~~ search_kernel_functions = kf \n 
~~ print ( self . regressor_name ) \n 
config = configparser . ConfigParser ( ) \n 
~~~ config . readfp ( open ( _ELMK_CONFIG ) ) \n 
~~~ config . read_file ( open ( _ELMK_CONFIG ) ) \n 
~~ best_function_error = 99999.9 \n 
temp_error = best_function_error \n 
best_param_c = 0 \n 
best_param_kernel_function = "" \n 
best_param_kernel_param = [ ] \n 
for kernel_function in search_kernel_functions : \n 
~~~ if sys . version_info < ( 3 , 0 ) : \n 
~~~ elmk_c_range = ast . literal_eval ( config . get ( "DEFAULT" , \n 
"elmk_c_range" ) ) \n 
n_parameters = config . getint ( kernel_function , "kernel_n_param" ) \n 
kernel_p_range = ast . literal_eval ( config . get ( kernel_function , \n 
"kernel_params_range" ) ) \n 
~~~ kernel_config = config [ kernel_function ] \n 
elmk_c_range = ast . literal_eval ( kernel_config [ "elmk_c_range" ] ) \n 
n_parameters = int ( kernel_config [ "kernel_n_param" ] ) \n 
kernel_p_range = ast . literal_eval ( kernel_config [ "kernel_params_range" ] ) \n 
~~ param_ranges = [ [ elmk_c_range [ 0 ] [ 0 ] , elmk_c_range [ 0 ] [ 1 ] ] ] \n 
for param in range ( n_parameters ) : \n 
~~~ param_ranges . append ( [ kernel_p_range [ param ] [ 0 ] , \n 
kernel_p_range [ param ] [ 1 ] ] ) \n 
~~ def wrapper_0param ( param_c ) : \n 
if cv == "ts" : \n 
~~~ cv_tr_error , cv_te_error = time_series_cross_validation ( self , database , \n 
[ kernel_function , \n 
param_c , \n 
list ( [ ] ) ] , \n 
number_folds = 10 , \n 
dataprocess = dataprocess ) \n 
~~ elif cv == "kfold" : \n 
~~~ cv_tr_error , cv_te_error = kfold_cross_validation ( self , database , \n 
~~ if of == "accuracy" : \n 
~~~ util = 1 / cv_te_error . get_accuracy ( ) \n 
~~~ util = cv_te_error . get ( of ) \n 
~~ return util \n 
~~ def wrapper_1param ( param_c , param_kernel ) : \n 
list ( [ param_kernel ] ) ] , \n 
~~ def wrapper_2param ( param_c , param_kernel1 , param_kernel2 ) : \n 
list ( [ param_kernel1 , \n 
param_kernel2 ] ) ] , \n 
~~ if kernel_function == "linear" : \n 
~~~ optimal_parameters , details , _ = optunity . minimize ( wrapper_0param , \n 
solver_name = "cma-es" , \n 
num_evals = eval , \n 
param_c = param_ranges [ 0 ] ) \n 
~~ elif kernel_function == "rbf" : \n 
~~~ optimal_parameters , details , _ = optunity . minimize ( wrapper_1param , \n 
param_c = param_ranges [ 0 ] , \n 
param_kernel = param_ranges [ 1 ] ) \n 
~~ elif kernel_function == "poly" : \n 
~~~ optimal_parameters , details , _ = optunity . minimize ( wrapper_2param , \n 
param_kernel1 = param_ranges [ 1 ] , \n 
param_kernel2 = param_ranges [ 2 ] ) \n 
~~ if details [ 0 ] < temp_error : \n 
~~~ temp_error = details [ 0 ] \n 
if of == "accuracy" : \n 
~~~ best_function_error = 1 / temp_error \n 
~~~ best_function_error = temp_error \n 
~~ best_param_kernel_function = kernel_function \n 
best_param_c = optimal_parameters [ "param_c" ] \n 
if best_param_kernel_function == "linear" : \n 
~~~ best_param_kernel_param = [ ] \n 
~~ elif best_param_kernel_function == "rbf" : \n 
~~~ best_param_kernel_param = [ optimal_parameters [ "param_kernel" ] ] \n 
~~ elif best_param_kernel_function == "poly" : \n 
~~~ best_param_kernel_param = [ optimal_parameters [ "param_kernel1" ] , \n 
optimal_parameters [ "param_kernel2" ] ] \n 
~~ ~~ if of == "accuracy" : \n 
~~ ~~ self . cv_best_rmse = best_function_error \n 
self . param_c = best_param_c \n 
self . param_kernel_function = best_param_kernel_function \n 
self . param_kernel_params = best_param_kernel_param \n 
self . print_parameters ( ) \n 
~~ def train ( self , training_matrix , params = [ ] ) : \n 
return self . _ml_train ( training_matrix , params ) \n 
~~ def test ( self , testing_matrix , predicting = False ) : \n 
return self . _ml_test ( testing_matrix , predicting ) \n 
~~ @ copy_doc_of ( MLTools . _ml_predict ) \n 
def predict ( self , horizon = 1 ) : \n 
~~~ return self . _ml_predict ( horizon ) \n 
~~ @ copy_doc_of ( MLTools . _ml_train_iterative ) \n 
def train_iterative ( self , database_matrix , params = [ ] , \n 
sliding_window = 168 , k = 1 ) : \n 
~~~ return self . _ml_train_iterative ( database_matrix , params , \n 
sliding_window , k ) \n 
~~ ~~ __all__ = ( "get_git_version" ) \n 
from subprocess import Popen , PIPE \n 
def call_git_describe ( abbrev = 4 ) : \n 
~~~ dot_git = os . path . join ( \n 
os . path . dirname ( os . path . abspath ( __file__ ) ) , \n 
if not os . path . exists ( dot_git ) : \n 
~~~ return None , None \n 
~~ line = None \n 
p = None \n 
~~~ p = Popen ( [ , , % abbrev ] , \n 
stdout = PIPE , stderr = PIPE , \n 
cwd = os . path . dirname ( os . path . abspath ( __file__ ) ) ) \n 
describe_line = p . stdout . readlines ( ) [ 0 ] . strip ( ) \n 
p = Popen ( [ , , ] , \n 
stdout = PIPE , stderr = PIPE ) \n 
source_hash = p . stdout . readlines ( ) [ 0 ] . strip ( ) \n 
source_hash = source_hash [ : abbrev ] \n 
parts = describe_line . split ( ) \n 
if len ( parts ) == 1 : \n 
~~~ version = parts [ 0 ] \n 
~~~ ver , rel , source_hash = parts \n 
version_parts = ver . split ( ) \n 
lasti = len ( version_parts ) - 1 \n 
version_parts [ lasti ] = str ( int ( version_parts [ lasti ] ) + 1 ) \n 
version = . format ( . join ( version_parts ) , rel ) \n 
~~ return version , source_hash \n 
~~ except Exception , exc : \n 
~~~ sys . stderr . write ( % line ) \n 
sys . stderr . write ( traceback . format_exc ( exc ) ) \n 
~~~ sys . stderr . write ( % p . stderr . read ( ) ) \n 
~~~ sys . stderr . write ( traceback . format_exc ( exc ) ) \n 
~~~ sys . stderr . write ( % os . getcwd ( ) ) \n 
~~ return None , None \n 
~~ ~~ def read_release_version ( ) : \n 
~~~ f = open ( "RELEASE-VERSION" , "r" ) \n 
~~~ version = f . readlines ( ) [ 0 ] \n 
return version . strip ( ) . split ( ) \n 
~~~ f . close ( ) \n 
~~ ~~ def write_release_version ( version , source_hash ) : \n 
~~~ f = open ( "RELEASE-VERSION" , "w" ) \n 
f . write ( "%s,%s\\n" % ( version , source_hash ) ) \n 
~~ def get_git_version ( abbrev = 4 ) : \n 
~~~ release_version , release_source_hash = read_release_version ( ) \n 
version , source_hash = call_git_describe ( abbrev ) \n 
if version is None : \n 
~~~ version = release_version \n 
source_hash = release_source_hash \n 
~~ if version is None : \n 
~~~ version = \n 
source_hash = \n 
~~ if version != release_version or source_hash != release_source_hash : \n 
~~~ write_release_version ( version , source_hash ) \n 
~~~ print get_git_version ( ) \n 
~~ from . csvc import * \n 
from . kml import * \n 
class StructuresParameters ( object ) : \n 
~~~ INF = 100000 \n 
"""Structures""" \n 
structureTypes = OrderedDict ( ) \n 
slicePlane = "Axial" \n 
structureTypes [ + slicePlane ] = ( 1 , 34 , 0 , , 0.75 , 0.05 , 0 , 900 structureTypes [ + slicePlane ] = ( 2 , 35 , 0 , , 0.95 , 0.23 , 0.10 structureTypes [ + slicePlane ] = ( 3 , 37 , 0 , , 0.48 , 0.66 , 0.7 , 900 structureTypes [ + slicePlane ] = ( 4 , 38 , 0 , , 0.68 , 0.86 , 0.9 structureTypes [ + slicePlane ] = ( 5 , 56 , 0 , , 0.65 , 0.24 , structureTypes [ + slicePlane ] = ( 6 , 57 , 0 , , 0.85 , 0.44 structureTypes [ + slicePlane ] = ( 7 , 46 , 0 , , 0.05 , 0.8 structureTypes [ + slicePlane ] = ( 8 , 18 , 0 , , 0.1 , 0.2 , structureTypes [ + slicePlane ] = ( 9 , 50 , 0 , , structureTypes [ + slicePlane ] = ( 10 , 16 , 0 , , 0.9 , 0.75 , 0.1 , 900 structureTypes [ + slicePlane ] = ( 11 , 25 , 0 , , 0 , 0.5 , 0.5 , 900 , 50 , ) \n 
structureTypes [ + slicePlane ] = ( 12 , 26 , 0 , , 0.4 , 0.3 , 0.9 , 900 , 50 , ) structureTypes [ + slicePlane ] = ( 13 , 43 , 0 , , 0.05 , 0.7 , 0.2 , 900 structureTypes [ + slicePlane ] = ( 14 , 44 , 0 , , 0.9 , 0.2 , 0.4 , 900 \n 
slicePlane = "Sagittal" \n 
structureTypes [ + slicePlane ] = ( 31 , 46 , 0 , , 0.3 , structureTypes [ + slicePlane ] = ( 32 , 45 , 0 , , 0.8 , 0.8 structureTypes [ + slicePlane ] = ( 33 , 18 , 0 , , 0.2 , 0.32 structureTypes [ + slicePlane ] = ( 34 , 16 , 0 , , 0.1 , 0.3 , 0.57 , structureTypes [ + slicePlane ] = ( 35 , 32 , 0 , , 0.4 , 0.2 , 0.2 , 900 , 50 , structureTypes [ + slicePlane ] = ( 36 , 58 , 0 , , 0.1 , 0.5 , 0.9 , 900 , 50 structureTypes [ + slicePlane ] = ( 37 , 51 , 0 , , 0.2 , 0.8 , 0.8 , 900 , 50 , structureTypes [ + slicePlane ] = ( 38 , 25 , 0 , , 0 , 1 , 1 , 900 , 50 , ) \n 
structureTypes [ + slicePlane ] = ( 39 , 40 , 0 , , 0.7 , 0.3 , 0.6 , 900 structureTypes [ + slicePlane ] = ( 40 , 41 , 0 , , 0.5 , 0.1 , 0.4 , 900 structureTypes [ + slicePlane ] = ( 41 , 52 , 0 , , 0.2 , 0.1 , \n 
slicePlane = "Coronal" \n 
structureTypes [ + slicePlane ] = ( 51 , 47 , 0 , , 0.4 , 0.2 structureTypes [ + slicePlane ] = ( 52 , 58 , 0 , , 0.9 , 0.4 , 0.4 , 900 , 50 , structureTypes [ + slicePlane ] = ( 53 , 45 , 0 , , 0.9 , 0.9 , structureTypes [ + slicePlane ] = ( 54 , 25 , 0 , , 0.1 , 0.15 , 0.15 , 900 , 50 , structureTypes [ + slicePlane ] = ( 55 , 52 , 0 , , 0.1 , 0.2 , 0.6 structureTypes [ + slicePlane ] = ( 56 , 64 , 0 , , 0.1 , 0.8 , 0.4 structureTypes [ + slicePlane ] = ( 57 , 62 , 0 , , 0.8 , 0.3 , structureTypes [ + slicePlane ] = ( 58 , 63 , 0 , , 0.6 , 0.1 structureTypes [ + slicePlane ] = ( 59 , 48 , 0 , , 0.6 structureTypes [ + slicePlane ] = ( 60 , 51 , 0 , , 0.2 , 0.7 , 0.2 , 900 , 50 , ) structureTypes [ + slicePlane ] = ( 61 , 66 , 81 , , 0.2 , 0.5 , \n 
def getItem ( self , structureId ) : \n 
~~~ return self . structureTypes [ structureId ] \n 
~~ def getIntCodeItem ( self , structureId ) : \n 
return self . getItem ( structureId ) [ self . STRUCTURE_ID ] \n 
~~ def getRegionIdItem ( self , structureId ) : \n 
return self . getItem ( structureId ) [ self . CHEST_REGION_ID ] \n 
~~ def getTypeIdItem ( self , structureId ) : \n 
return self . getItem ( structureId ) [ self . CHEST_TYPE_ID ] \n 
~~ def getDescriptionItem ( self , structureId ) : \n 
return self . getItem ( structureId ) [ self . DESCRIPTION ] \n 
~~ def getRedItem ( self , structureId ) : \n 
return self . getItem ( structureId ) [ self . RED ] \n 
~~ def getGreenItem ( self , structureId ) : \n 
return self . getItem ( structureId ) [ self . GREEN ] \n 
~~ def getBlueItem ( self , structureId ) : \n 
return self . getItem ( structureId ) [ self . BLUE ] \n 
~~ def getWindowRange ( self , structureId ) : \n 
if not item : \n 
~~ width = item [ self . WINDOW_WIDTH ] \n 
level = item [ self . WINDOW_LEVEL ] \n 
if width == self . INF or level == self . INF : \n 
~~ return ( width , level ) \n 
~~ def getPlaneItem ( self , structureId ) : \n 
~~~ return self . getItem ( structureId ) [ self . PLANE ] \n 
from __main__ import vtk , qt , ctk , slicer \n 
from slicer . ScriptedLoadableModule import * \n 
~~~ from CIP . logic . SlicerUtil import SlicerUtil \n 
~~~ currentpath = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
path = os . path . normpath ( currentpath + ) \n 
if not os . path . exists ( path ) : \n 
~~~ path = os . path . normpath ( currentpath + ) \n 
~~ sys . path . append ( path ) \n 
from CIP . logic . SlicerUtil import SlicerUtil \n 
~~ from CIP . logic import Util \n 
from CIP . ui import CaseReportsWidget \n 
class CIP_PAARatio ( ScriptedLoadableModule ) : \n 
def __init__ ( self , parent ) : \n 
~~~ ScriptedLoadableModule . __init__ ( self , parent ) \n 
self . parent . categories = SlicerUtil . CIP_ModulesCategory \n 
self . parent . dependencies = [ SlicerUtil . CIP_ModuleName ] \n 
self . parent . acknowledgementText = SlicerUtil . ACIL_AcknowledgementText \n 
~~ ~~ class CIP_PAARatioWidget ( ScriptedLoadableModuleWidget ) : \n 
def moduleName ( self ) : \n 
~~~ return "CIP_PAARatio" \n 
~~ def __init__ ( self , parent ) : \n 
~~~ ScriptedLoadableModuleWidget . __init__ ( self , parent ) \n 
def __onNodeAddedObserver__ ( self , caller , eventId , callData ) : \n 
if callData . GetClassName ( ) == and slicer . util . mainWindow ( ) . moduleSelector ( ) . selectedModule == self . moduleName : ~~~ self . volumeSelector . setCurrentNode ( callData ) \n 
~~ ~~ self . __onNodeAddedObserver__ = partial ( __onNodeAddedObserver__ , self ) \n 
self . __onNodeAddedObserver__ . CallDataType = vtk . VTK_OBJECT \n 
~~ def setup ( self ) : \n 
ScriptedLoadableModuleWidget . setup ( self ) \n 
self . logic = CIP_PAARatioLogic ( ) \n 
mainAreaCollapsibleButton = ctk . ctkCollapsibleButton ( ) \n 
self . layout . addWidget ( mainAreaCollapsibleButton ) \n 
self . mainAreaLayout = qt . QGridLayout ( mainAreaCollapsibleButton ) \n 
self . mainAreaLayout . addWidget ( self . label , 0 , 0 ) \n 
self . volumeSelector = slicer . qMRMLNodeComboBox ( ) \n 
self . volumeSelector . nodeTypes = ( "vtkMRMLScalarVolumeNode" , "" ) \n 
self . volumeSelector . autoFillBackground = True \n 
self . volumeSelector . addEnabled = True \n 
self . volumeSelector . noneEnabled = False \n 
self . volumeSelector . removeEnabled = False \n 
self . volumeSelector . showHidden = False \n 
self . volumeSelector . showChildNodeTypes = False \n 
self . volumeSelector . setMRMLScene ( slicer . mrmlScene ) \n 
self . mainAreaLayout . addWidget ( self . volumeSelector , 0 , 1 ) \n 
self . placeDefaultRulersButton = ctk . ctkPushButton ( ) \n 
self . mainAreaLayout . addWidget ( self . placeDefaultRulersButton , 1 , 1 ) \n 
self . groupboxLayout = qt . QVBoxLayout ( ) \n 
self . structuresGroupbox . setLayout ( self . groupboxLayout ) \n 
self . mainAreaLayout . addWidget ( self . structuresGroupbox , 2 , 0 ) \n 
self . structuresButtonGroup = qt . QButtonGroup ( ) \n 
btn = qt . QRadioButton ( "Both" ) \n 
btn . checked = True \n 
self . structuresButtonGroup . addButton ( btn , 0 ) \n 
self . groupboxLayout . addWidget ( btn ) \n 
self . structuresButtonGroup . addButton ( btn , 1 ) \n 
btn = qt . QRadioButton ( "Aorta" ) \n 
self . structuresButtonGroup . addButton ( btn , 2 ) \n 
self . buttonsToolboxFrame = qt . QFrame ( ) \n 
self . buttonsToolboxLayout = qt . QGridLayout ( ) \n 
self . buttonsToolboxFrame . setLayout ( self . buttonsToolboxLayout ) \n 
self . mainAreaLayout . addWidget ( self . buttonsToolboxFrame , 2 , 1 ) \n 
self . placeRulersButton = ctk . ctkPushButton ( ) \n 
self . placeRulersButton . setIconSize ( qt . QSize ( 20 , 20 ) ) \n 
self . placeRulersButton . setFixedWidth ( 105 ) \n 
self . placeRulersButton . setStyleSheet ( "font-weight:bold" ) \n 
self . buttonsToolboxLayout . addWidget ( self . placeRulersButton , 0 , 0 ) \n 
self . moveUpButton = ctk . ctkPushButton ( ) \n 
self . moveUpButton . setIcon ( qt . QIcon ( "{0}/move_up.png" . format ( SlicerUtil . CIP_ICON_DIR ) ) ) \n 
self . moveUpButton . setIconSize ( qt . QSize ( 20 , 20 ) ) \n 
self . moveUpButton . setFixedWidth ( 95 ) \n 
self . buttonsToolboxLayout . addWidget ( self . moveUpButton , 0 , 1 ) \n 
self . moveDownButton = ctk . ctkPushButton ( ) \n 
self . moveDownButton . setIcon ( qt . QIcon ( "{0}/move_down.png" . format ( SlicerUtil . CIP_ICON_DIR ) ) ) \n 
self . moveDownButton . setIconSize ( qt . QSize ( 20 , 20 ) ) \n 
self . moveDownButton . setFixedWidth ( 95 ) \n 
self . buttonsToolboxLayout . addWidget ( self . moveDownButton , 0 , 2 ) \n 
self . removeButton = ctk . ctkPushButton ( ) \n 
self . removeButton . setIcon ( qt . QIcon ( "{0}/delete.png" . format ( SlicerUtil . CIP_ICON_DIR ) ) ) \n 
self . removeButton . setIconSize ( qt . QSize ( 20 , 20 ) ) \n 
self . buttonsToolboxLayout . addWidget ( self . removeButton , 1 , 1 , 1 , 2 , 2 ) \n 
self . textboxesFrame = qt . QFrame ( ) \n 
self . textboxesLayout = qt . QFormLayout ( ) \n 
self . textboxesFrame . setLayout ( self . textboxesLayout ) \n 
self . textboxesFrame . setFixedWidth ( 190 ) \n 
self . mainAreaLayout . addWidget ( self . textboxesFrame , 3 , 0 ) \n 
self . paTextBox = qt . QLineEdit ( ) \n 
self . paTextBox . setReadOnly ( True ) \n 
self . aortaTextBox = qt . QLineEdit ( ) \n 
self . aortaTextBox . setReadOnly ( True ) \n 
self . ratioTextBox = qt . QLineEdit ( ) \n 
self . ratioTextBox . setReadOnly ( True ) \n 
self . reportsCollapsibleButton = ctk . ctkCollapsibleButton ( ) \n 
self . reportsCollapsibleButton . text = "Reporting" \n 
self . layout . addWidget ( self . reportsCollapsibleButton ) \n 
self . reportsLayout = qt . QHBoxLayout ( self . reportsCollapsibleButton ) \n 
self . storedColumnNames = [ "caseId" , "paDiameter_mm" , "aortaDiameter_mm" , \n 
"pa1r" , "pa1a" , "pa1s" , "pa2r" , "pa2a" , "pa2s" , \n 
"a1r" , "a1a" , "a1s" , "a2r" , "a2a" , "a2s" ] \n 
self . reportsWidget = CaseReportsWidget ( "CIP_PAARatio" , columnNames = self . storedColumnNames , parentWidget self . reportsWidget . setup ( ) \n 
self . switchToRedView ( ) \n 
##### \n 
if SlicerUtil . isSlicerACILLoaded ( ) : \n 
~~~ caseNavigatorAreaCollapsibleButton = ctk . ctkCollapsibleButton ( ) \n 
self . layout . addWidget ( caseNavigatorAreaCollapsibleButton , 0x0020 ) \n 
from ACIL . ui import CaseNavigatorWidget \n 
self . caseNavigatorWidget = CaseNavigatorWidget ( self . moduleName , caseNavigatorAreaCollapsibleButton self . caseNavigatorWidget . setup ( ) \n 
~~ self . layout . addStretch ( ) \n 
self . observers = [ ] \n 
self . __addSceneObservables__ ( ) \n 
self . volumeSelector . connect ( , self . onVolumeSelectorChanged self . placeDefaultRulersButton . connect ( , self . oPlaceDefaultRulersClicked ) \n 
self . placeRulersButton . connect ( , self . onPlaceRulersClicked ) \n 
self . moveUpButton . connect ( , self . onMoveUpRulerClicked ) \n 
self . moveDownButton . connect ( , self . onMoveDownRulerClicked ) \n 
self . removeButton . connect ( , self . onRemoveRulerClicked ) \n 
self . reportsWidget . addObservable ( self . reportsWidget . EVENT_SAVE_BUTTON_CLICKED , self . onSaveReport \n 
~~ def enter ( self ) : \n 
volumeId = self . volumeSelector . currentNodeId \n 
if volumeId : \n 
~~~ SlicerUtil . setActiveVolumeId ( volumeId ) \n 
~~ ~~ def exit ( self ) : \n 
~~ def jumpToTemptativeSlice ( self , volumeId ) : \n 
aorta1 , aorta2 , pa1 , pa2 = self . logic . getDefaultCoords ( volumeId ) \n 
self . moveRedWindowToSlice ( aorta1 [ 2 ] ) \n 
~~ def placeDefaultRulers ( self , volumeId ) : \n 
if not volumeId : \n 
~~ self . logic . hideAllRulers ( ) \n 
self . logic . removeRulers ( volumeId ) \n 
self . logic . createDefaultRulers ( volumeId , self . onRulerUpdated ) \n 
self . structuresButtonGroup . buttons ( ) [ 0 ] . setChecked ( True ) \n 
self . jumpToTemptativeSlice ( volumeId ) \n 
self . placeRuler ( ) \n 
self . logic . currentVolumesLoaded . add ( volumeId ) \n 
redSliceNode = slicer . util . getFirstNodeByClassByName ( "vtkMRMLSliceNode" , "Red" ) \n 
factor = 0.5 \n 
newFOVx = redSliceNode . GetFieldOfView ( ) [ 0 ] * factor \n 
newFOVy = redSliceNode . GetFieldOfView ( ) [ 1 ] * factor \n 
newFOVz = redSliceNode . GetFieldOfView ( ) [ 2 ] \n 
redSliceNode . SetFieldOfView ( newFOVx , newFOVy , newFOVz ) \n 
redSliceNode . SetXYZOrigin ( 0 , 50 , 0 ) \n 
redSliceNode . UpdateMatrices ( ) \n 
~~ def placeRuler ( self ) : \n 
if volumeId == : \n 
~~~ self . showUnselectedVolumeWarningMessage ( ) \n 
~~ selectedStructure = self . getCurrentSelectedStructure ( ) \n 
if selectedStructure == self . logic . NONE : \n 
~~~ qt . QMessageBox . warning ( slicer . util . mainWindow ( ) , , \n 
~~ currentSlice = self . getCurrentRedWindowSlice ( ) \n 
if selectedStructure == self . logic . BOTH : \n 
~~~ structures = [ self . logic . PA , self . logic . AORTA ] \n 
~~~ structures = [ selectedStructure ] \n 
~~ for structure in structures : \n 
~~~ self . logic . placeRulerInSlice ( volumeId , structure , currentSlice , self . onRulerUpdated ) \n 
~~ self . refreshTextboxes ( ) \n 
~~ def getCurrentSelectedStructure ( self ) : \n 
selectedStructureText = self . structuresButtonGroup . checkedButton ( ) . text \n 
if selectedStructureText == "Aorta" : return self . logic . AORTA \n 
elif selectedStructureText == "Both" : return self . logic . BOTH \n 
return self . logic . NONE \n 
~~ def stepSlice ( self , offset ) : \n 
~~~ self . showUnselectedStructureWarningMessage ( ) \n 
~~ if selectedStructure == self . logic . BOTH : \n 
~~~ self . logic . stepSlice ( volumeId , self . logic . AORTA , offset ) \n 
newSlice = self . logic . stepSlice ( volumeId , self . logic . PA , offset ) \n 
~~~ newSlice = self . logic . stepSlice ( volumeId , selectedStructure , offset ) \n 
~~ self . moveRedWindowToSlice ( newSlice ) \n 
~~ def removeRulers ( self ) : \n 
self . logic . removeRulers ( self . volumeSelector . currentNodeId ) \n 
self . refreshTextboxes ( reset = True ) \n 
~~ def getCurrentRedWindowSlice ( self ) : \n 
redNodeSliceNode = slicer . app . layoutManager ( ) . sliceWidget ( ) . sliceLogic ( ) . GetSliceNode ( ) return redNodeSliceNode . GetSliceOffset ( ) \n 
~~ def moveRedWindowToSlice ( self , newSlice ) : \n 
redNodeSliceNode = slicer . app . layoutManager ( ) . sliceWidget ( ) . sliceLogic ( ) . GetSliceNode ( ) redNodeSliceNode . JumpSlice ( 0 , 0 , newSlice ) \n 
~~ def refreshTextboxes ( self , reset = False ) : \n 
self . aortaTextBox . setText ( "0" ) \n 
self . paTextBox . setText ( "0" ) \n 
self . ratioTextBox . setText ( "0" ) \n 
if volumeId not in self . logic . currentVolumesLoaded : \n 
~~ if volumeId : \n 
~~~ self . logic . changeColor ( volumeId , self . logic . defaultColor ) \n 
~~ aorta = None \n 
pa = None \n 
if not reset : \n 
~~~ rulerAorta , newAorta = self . logic . getRulerNodeForVolumeAndStructure ( self . volumeSelector . self . logic . AORTA , createIfNotExist = False ) \n 
rulerPA , newPA = self . logic . getRulerNodeForVolumeAndStructure ( self . volumeSelector . currentNodeId self . logic . PA , createIfNotExist = False ) \n 
if rulerAorta : \n 
~~~ aorta = rulerAorta . GetDistanceMeasurement ( ) \n 
self . aortaTextBox . setText ( str ( aorta ) ) \n 
~~ if rulerPA : \n 
~~~ pa = rulerPA . GetDistanceMeasurement ( ) \n 
self . paTextBox . setText ( str ( pa ) ) \n 
~~ if aorta is not None and aorta != 0 : \n 
~~~ ratio = pa / aorta \n 
self . ratioTextBox . setText ( str ( ratio ) ) \n 
if ratio > 1 : \n 
~~~ Util . print_last_exception ( ) \n 
~~ ~~ ~~ ~~ def showUnselectedVolumeWarningMessage ( self ) : \n 
~~ def showUnselectedStructureWarningMessage ( self ) : \n 
~~ def switchToRedView ( self ) : \n 
layoutManager = slicer . app . layoutManager ( ) \n 
layoutManager . setLayout ( 6 ) \n 
~~ def __addSceneObservables__ ( self ) : \n 
~~~ self . observers . append ( slicer . mrmlScene . AddObserver ( slicer . vtkMRMLScene . NodeAddedEvent , self . self . observers . append ( slicer . mrmlScene . AddObserver ( slicer . vtkMRMLScene . EndCloseEvent , self . __onSceneClosed__ \n 
~~ def __removeSceneObservables ( self ) : \n 
~~~ for observer in self . observers : \n 
~~~ slicer . mrmlScene . RemoveObserver ( observer ) \n 
self . observers . remove ( observer ) \n 
######### \n 
~~ ~~ def onVolumeSelectorChanged ( self , node ) : \n 
~~~ self . refreshTextboxes ( ) \n 
~~ def onStructureClicked ( self , button ) : \n 
~~~ fiducialsNode = self . getFiducialsNode ( self . volumeSelector . currentNodeId ) \n 
if fiducialsNode is not None : \n 
~~~ self . __addRuler__ ( button . text , self . volumeSelector . currentNodeId ) \n 
markupsLogic = slicer . modules . markups . logic ( ) \n 
markupsLogic . SetActiveListID ( fiducialsNode ) \n 
applicationLogic = slicer . app . applicationLogic ( ) \n 
selectionNode = applicationLogic . GetSelectionNode ( ) \n 
selectionNode . SetReferenceActivePlaceNodeClassName ( "vtkMRMLAnnotationRulerNode" ) \n 
interactionNode = applicationLogic . GetInteractionNode ( ) \n 
interactionNode . SwitchToSinglePlaceMode ( ) \n 
~~ ~~ def oPlaceDefaultRulersClicked ( self ) : \n 
~~~ volumeId = self . volumeSelector . currentNodeId \n 
~~ self . placeDefaultRulers ( volumeId ) \n 
~~ def onRulerUpdated ( self , node , event ) : \n 
~~ def onPlaceRulersClicked ( self ) : \n 
~~~ self . placeRuler ( ) \n 
~~ def onMoveUpRulerClicked ( self ) : \n 
~~~ self . stepSlice ( 1 ) \n 
~~ def onMoveDownRulerClicked ( self ) : \n 
~~~ self . stepSlice ( - 1 ) \n 
~~ def onRemoveRulerClicked ( self ) : \n 
~~~ if ( qt . QMessageBox . question ( slicer . util . mainWindow ( ) , , \n 
qt . QMessageBox . Yes | qt . QMessageBox . No ) ) == qt . QMessageBox . Yes : \n 
~~~ self . logic . removeRulers ( self . volumeSelector . currentNodeId ) \n 
self . refreshTextboxes ( ) \n 
~~ ~~ def onSaveReport ( self ) : \n 
~~~ caseName = slicer . mrmlScene . GetNodeByID ( volumeId ) . GetName ( ) \n 
coords = [ 0 , 0 , 0 , 0 ] \n 
pa1 = pa2 = a1 = a2 = None \n 
rulerNode , newNode = self . logic . getRulerNodeForVolumeAndStructure ( volumeId , self . logic . PA if rulerNode : \n 
~~~ rulerNode . GetPositionWorldCoordinates1 ( coords ) \n 
pa1 = list ( coords ) \n 
rulerNode . GetPositionWorldCoordinates2 ( coords ) \n 
pa2 = list ( coords ) \n 
~~ rulerNode , newNode = self . logic . getRulerNodeForVolumeAndStructure ( volumeId , self . logic . AORTA if rulerNode : \n 
a1 = list ( coords ) \n 
a2 = list ( coords ) \n 
~~ self . reportsWidget . saveCurrentValues ( \n 
caseId = caseName , \n 
paDiameter_mm = self . paTextBox . text , \n 
aortaDiameter_mm = self . aortaTextBox . text , \n 
pa1r = pa1 [ 0 ] if pa1 is not None else , \n 
pa1a = pa1 [ 1 ] if pa1 is not None else , \n 
pa1s = pa1 [ 2 ] if pa1 is not None else , \n 
pa2r = pa2 [ 0 ] if pa2 is not None else , \n 
pa2a = pa2 [ 1 ] if pa2 is not None else , \n 
pa2s = pa2 [ 2 ] if pa2 is not None else , \n 
a1r = a1 [ 0 ] if a1 is not None else , \n 
a1a = a1 [ 1 ] if a1 is not None else , \n 
a1s = a1 [ 2 ] if a1 is not None else , \n 
a2r = a2 [ 0 ] if a2 is not None else , \n 
a2a = a2 [ 1 ] if a2 is not None else , \n 
a2s = a2 [ 2 ] if a2 is not None else \n 
qt . QMessageBox . information ( slicer . util . mainWindow ( ) , , \n 
~~ ~~ def __onSceneClosed__ ( self , arg1 , arg2 ) : \n 
self . logic . currentVolumesLoaded . clear ( ) \n 
~~ ~~ class CIP_PAARatioLogic ( ScriptedLoadableModuleLogic ) : \n 
NONE = 0 \n 
AORTA = 1 \n 
PA = 2 \n 
BOTH = 3 \n 
SLICEFACTOR = 0.6 \n 
defaultAorta2 = [ 275 , 175 , 0 ] \n 
defaultPA1 = [ 280 , 175 , 0 ] \n 
defaultPA2 = [ 320 , 190 , 0 ] \n 
defaultColor = [ 0.5 , 0.5 , 1 ] \n 
defaultWarningColor = [ 1 , 0 , 0 ] \n 
~~~ self . currentVolumesLoaded = set ( ) \n 
~~ def getRootAnnotationsNode ( self ) : \n 
return SlicerUtil . getRootAnnotationsNode ( ) \n 
~~ def getRulersListNode ( self , volumeId , createIfNotExist = True ) : \n 
nodeName = volumeId + \n 
rulersNode = slicer . util . getNode ( nodeName ) \n 
if rulersNode is None and createIfNotExist : \n 
~~~ annotationsLogic = slicer . modules . annotations . logic ( ) \n 
rootHierarchyNode = self . getRootAnnotationsNode ( ) \n 
annotationsLogic . SetActiveHierarchyNodeID ( rootHierarchyNode . GetID ( ) ) \n 
annotationsLogic . AddHierarchy ( ) \n 
n = rootHierarchyNode . GetNumberOfChildrenNodes ( ) \n 
rulersNode = rootHierarchyNode . GetNthChildNode ( n - 1 ) \n 
rulersNode . SetName ( nodeName ) \n 
~~ return rulersNode \n 
isNewNode = False \n 
~~~ return None , isNewNode \n 
~~~ nodeName = "A" \n 
~~ elif structureId == self . PA : \n 
~~~ nodeName = "PA" \n 
~~ rulersListNode = self . getRulersListNode ( volumeId , createIfNotExist = createIfNotExist ) \n 
node = None \n 
if rulersListNode : \n 
~~~ for i in range ( rulersListNode . GetNumberOfChildrenNodes ( ) ) : \n 
~~~ nodeWrapper = rulersListNode . GetNthChildNode ( i ) \n 
nodeWrapper . GetChildrenDisplayableNodes ( col ) \n 
rulerNode = col . GetItemAsObject ( 0 ) \n 
if rulerNode . GetName ( ) == nodeName : \n 
~~~ node = rulerNode \n 
~~ ~~ if node is None and createIfNotExist : \n 
annotationsLogic . SetActiveHierarchyNodeID ( rulersListNode . GetID ( ) ) \n 
node = slicer . mrmlScene . CreateNodeByClass ( ) \n 
node . SetName ( nodeName ) \n 
self . __changeColor__ ( node , self . defaultColor ) \n 
slicer . mrmlScene . AddNode ( node ) \n 
isNewNode = True \n 
node . AddObserver ( vtk . vtkCommand . ModifiedEvent , callbackWhenRulerModified ) \n 
~~ ~~ return node , isNewNode \n 
~~ def hideAllRulers ( self ) : \n 
for volume in self . currentVolumesLoaded : \n 
~~~ rulersNode = self . getRulersListNode ( volume , False ) \n 
if rulersNode : \n 
~~ ~~ ~~ def __changeColor__ ( self , node , color ) : \n 
~~~ for i in range ( 3 ) : \n 
~~~ n = node . GetNthDisplayNode ( i ) \n 
if n : \n 
~~~ n . SetColor ( color ) \n 
~~ ~~ slicer . app . layoutManager ( ) . sliceWidget ( "Red" ) . sliceView ( ) . mrmlSliceNode ( ) . Modified ( ) \n 
~~ def changeColor ( self , volumeId , color ) : \n 
for structureId in [ self . PA , self . AORTA ] : \n 
~~~ node , new = self . getRulerNodeForVolumeAndStructure ( volumeId , structureId , createIfNotExist if node : \n 
~~~ self . __changeColor__ ( node , color ) \n 
~~ ~~ ~~ def createDefaultRulers ( self , volumeId , callbackWhenRulerModified ) : \n 
aorta1 , aorta2 , pa1 , pa2 = self . getDefaultCoords ( volumeId ) \n 
rulerNodeAorta , newNodeAorta = self . getRulerNodeForVolumeAndStructure ( volumeId , self . AORTA , \n 
createIfNotExist = True , callbackWhenRulerModified = callbackWhenRulerModified rulerNodeAorta . SetPositionWorldCoordinates1 ( aorta1 ) \n 
rulerNodeAorta . SetPositionWorldCoordinates2 ( aorta2 ) \n 
rulerNodePA , newNodePA = self . getRulerNodeForVolumeAndStructure ( volumeId , self . PA , \n 
createIfNotExist = True , callbackWhenRulerModified = callbackWhenRulerModified rulerNodePA . SetPositionWorldCoordinates1 ( pa1 ) \n 
rulerNodePA . SetPositionWorldCoordinates2 ( pa2 ) \n 
return rulerNodeAorta , newNodeAorta , rulerNodePA , newNodePA \n 
~~ def stepSlice ( self , volumeId , structureId , sliceStep ) : \n 
rulerNode , newNode = self . getRulerNodeForVolumeAndStructure ( volumeId , structureId , createIfNotExist if not rulerNode : \n 
~~ coords = [ 0 , 0 , 0 , 0 ] \n 
rulerNode . GetPositionWorldCoordinates1 ( coords ) \n 
rastoijk = vtk . vtkMatrix4x4 ( ) \n 
ijktoras = vtk . vtkMatrix4x4 ( ) \n 
scalarVolumeNode = slicer . mrmlScene . GetNodeByID ( volumeId ) \n 
scalarVolumeNode . GetRASToIJKMatrix ( rastoijk ) \n 
scalarVolumeNode . GetIJKToRASMatrix ( ijktoras ) \n 
ijkCoords = list ( rastoijk . MultiplyPoint ( coords ) ) \n 
ijkCoords [ 2 ] += sliceStep \n 
newSlice = ijktoras . MultiplyPoint ( ijkCoords ) [ 2 ] \n 
self . _placeRulerInSlice_ ( rulerNode , structureId , volumeId , newSlice ) \n 
return newSlice \n 
~~ def placeRulerInSlice ( self , volumeId , structureId , newSlice , callbackWhenUpdated = None ) : \n 
rulerNode , newNode = self . getRulerNodeForVolumeAndStructure ( volumeId , structureId , createIfNotExist \n 
self . currentVolumesLoaded . add ( volumeId ) \n 
~~ def _placeRulerInSlice_ ( self , rulerNode , structureId , volumeId , newSlice ) : \n 
coords1 = [ 0 , 0 , 0 , 0 ] \n 
coords2 = [ 0 , 0 , 0 , 0 ] \n 
rulerNode . GetPositionWorldCoordinates1 ( coords1 ) \n 
rulerNode . GetPositionWorldCoordinates2 ( coords2 ) \n 
coords1 [ 2 ] = coords2 [ 2 ] = newSlice \n 
if coords1 [ 0 ] == 0 and coords1 [ 1 ] == 0 : \n 
~~~ defaultCoords = self . getDefaultCoords ( volumeId ) \n 
if structureId == self . AORTA : \n 
~~~ coords1 [ 0 ] = defaultCoords [ 0 ] [ 0 ] \n 
coords1 [ 1 ] = defaultCoords [ 0 ] [ 1 ] \n 
coords2 [ 0 ] = defaultCoords [ 1 ] [ 0 ] \n 
coords2 [ 1 ] = defaultCoords [ 1 ] [ 1 ] \n 
~~~ coords1 [ 0 ] = defaultCoords [ 2 ] [ 0 ] \n 
coords1 [ 1 ] = defaultCoords [ 2 ] [ 1 ] \n 
coords2 [ 0 ] = defaultCoords [ 3 ] [ 0 ] \n 
coords2 [ 1 ] = defaultCoords [ 3 ] [ 1 ] \n 
~~ ~~ rulerNode . SetPositionWorldCoordinates1 ( coords1 ) \n 
rulerNode . SetPositionWorldCoordinates2 ( coords2 ) \n 
~~ def getDefaultCoords ( self , volumeId ) : \n 
volume = slicer . mrmlScene . GetNodeByID ( volumeId ) \n 
rasBounds = [ 0 , 0 , 0 , 0 , 0 , 0 ] \n 
volume . GetRASBounds ( rasBounds ) \n 
ijk = self . RAStoIJK ( volume , [ 0 , 0 , rasBounds [ 5 ] ] ) \n 
aorta1 = list ( self . defaultAorta1 ) \n 
aorta1 [ 2 ] = slice \n 
aorta1 = self . IJKtoRAS ( volume , aorta1 ) \n 
aorta2 = list ( self . defaultAorta2 ) \n 
aorta2 [ 2 ] = slice \n 
aorta2 = self . IJKtoRAS ( volume , aorta2 ) \n 
pa1 = list ( self . defaultPA1 ) \n 
pa1 [ 2 ] = slice \n 
pa1 = self . IJKtoRAS ( volume , pa1 ) \n 
pa2 = list ( self . defaultPA2 ) \n 
pa2 [ 2 ] = slice \n 
pa2 = self . IJKtoRAS ( volume , pa2 ) \n 
return aorta1 , aorta2 , pa1 , pa2 \n 
~~ def removeRulers ( self , volumeId ) : \n 
rulersListNode = self . getRulersListNode ( volumeId , createIfNotExist = False ) \n 
~~~ rulersListNode . RemoveAllChildrenNodes ( ) \n 
slicer . mrmlScene . RemoveNode ( rulersListNode ) \n 
~~ ~~ def RAStoIJK ( self , volumeNode , rasCoords ) : \n 
volumeNode . GetRASToIJKMatrix ( rastoijk ) \n 
rasCoords . append ( 1 ) \n 
return list ( rastoijk . MultiplyPoint ( rasCoords ) ) \n 
~~ def IJKtoRAS ( self , volumeNode , ijkCoords ) : \n 
volumeNode . GetIJKToRASMatrix ( ijktoras ) \n 
ijkCoords . append ( 1 ) \n 
return list ( ijktoras . MultiplyPoint ( ijkCoords ) ) \n 
~~ ~~ class CIP_PAARatioTest ( ScriptedLoadableModuleTest ) : \n 
slicer . mrmlScene . Clear ( 0 ) \n 
~~ def runTest ( self ) : \n 
self . setUp ( ) \n 
self . test_CIP_PAARatio_PrintMessage ( ) \n 
~~ def test_CIP_PAARatio_PrintMessage ( self ) : \n 
logic = CIP_PAARatioLogic ( ) \n 
volumeId = volume [ 1 ] . GetID ( ) \n 
logic . createDefaultRulers ( volumeId ) \n 
ruler = logic . getRulerNodeForVolumeAndStructure ( volumeId , logic . AORTA , createIfNotExist = False self . assertFalse ( ruler [ 0 ] is None ) \n 
self . delayDisplay ( ) \n 
from google . appengine . ext import db \n 
from google . appengine . api import memcache \n 
class Token ( db . Model ) : \n 
t = db . BlobProperty ( ) \n 
~~ def get_token ( unique_key ) : \n 
token_string = memcache . get ( unique_key ) \n 
if token_string is None : \n 
~~~ token = Token . get_by_key_name ( unique_key ) \n 
if token is None : \n 
~~ return token . t \n 
~~ return token_string \n 
~~ def set_token ( unique_key , token_str ) : \n 
result = memcache . set ( unique_key , token_str ) \n 
~~~ result = memcache . delete ( unique_key ) \n 
if result == 0 : \n 
~~ ~~ if Token ( key_name = unique_key , t = token_str ) . put ( ) : \n 
~~ def delete_token ( unique_key ) : \n 
~~~ memcache . delete ( unique_key ) \n 
Token ( key_name = unique_key ) . delete ( ) \n 
import gdata . client \n 
import gdata . gauth \n 
import gdata . blogger . data \n 
import atom . data \n 
import atom . http_core \n 
BLOGS_URL = \n 
BLOG_POST_URL = \n 
BLOG_PAGE_URL = \n 
BLOG_POST_COMMENTS_URL = \n 
BLOG_COMMENTS_URL = \n 
BLOG_ARCHIVE_URL = \n 
class BloggerClient ( gdata . client . GDClient ) : \n 
~~~ api_version = \n 
auth_service = \n 
auth_scopes = gdata . gauth . AUTH_SCOPES [ ] \n 
def get_blogs ( self , user_id = , auth_token = None , \n 
desired_class = gdata . blogger . data . BlogFeed , ** kwargs ) : \n 
~~~ return self . get_feed ( BLOGS_URL % user_id , auth_token = auth_token , \n 
desired_class = desired_class , ** kwargs ) \n 
~~ GetBlogs = get_blogs \n 
def get_posts ( self , blog_id , auth_token = None , \n 
desired_class = gdata . blogger . data . BlogPostFeed , query = None , \n 
~~~ return self . get_feed ( BLOG_POST_URL % blog_id , auth_token = auth_token , \n 
desired_class = desired_class , query = query , ** kwargs ) \n 
~~ GetPosts = get_posts \n 
def get_pages ( self , blog_id , auth_token = None , \n 
desired_class = gdata . blogger . data . BlogPageFeed , query = None , \n 
~~~ return self . get_feed ( BLOG_PAGE_URL % blog_id , auth_token = auth_token , \n 
~~ GetPages = get_pages \n 
def get_post_comments ( self , blog_id , post_id , auth_token = None , \n 
desired_class = gdata . blogger . data . CommentFeed , \n 
query = None , ** kwargs ) : \n 
~~~ return self . get_feed ( BLOG_POST_COMMENTS_URL % ( blog_id , post_id ) , \n 
auth_token = auth_token , desired_class = desired_class , \n 
query = query , ** kwargs ) \n 
~~ GetPostComments = get_post_comments \n 
def get_blog_comments ( self , blog_id , auth_token = None , \n 
~~~ return self . get_feed ( BLOG_COMMENTS_URL % blog_id , auth_token = auth_token , \n 
~~ GetBlogComments = get_blog_comments \n 
def get_blog_archive ( self , blog_id , auth_token = None , ** kwargs ) : \n 
~~~ return self . get_feed ( BLOG_ARCHIVE_URL % blog_id , auth_token = auth_token , \n 
~~ GetBlogArchive = get_blog_archive \n 
def add_post ( self , blog_id , title , body , labels = None , draft = False , \n 
auth_token = None , title_type = , body_type = , ** kwargs ) : \n 
~~~ new_entry = gdata . blogger . data . BlogPost ( \n 
title = atom . data . Title ( text = title , type = title_type ) , \n 
content = atom . data . Content ( text = body , type = body_type ) ) \n 
if labels : \n 
~~~ for label in labels : \n 
~~~ new_entry . add_label ( label ) \n 
~~ ~~ if draft : \n 
~~~ new_entry . control = atom . data . Control ( draft = atom . data . Draft ( text = ) ) \n 
~~ return self . post ( new_entry , BLOG_POST_URL % blog_id , auth_token = auth_token , ** kwargs ) \n 
~~ AddPost = add_post \n 
def add_page ( self , blog_id , title , body , draft = False , auth_token = None , \n 
title_type = , body_type = , ** kwargs ) : \n 
~~~ new_entry = gdata . blogger . data . BlogPage ( \n 
if draft : \n 
~~ return self . post ( new_entry , BLOG_PAGE_URL % blog_id , auth_token = auth_token , ** kwargs ) \n 
~~ AddPage = add_page \n 
def add_comment ( self , blog_id , post_id , body , auth_token = None , \n 
~~~ new_entry = gdata . blogger . data . Comment ( \n 
return self . post ( new_entry , BLOG_POST_COMMENTS_URL % ( blog_id , post_id ) , \n 
auth_token = auth_token , ** kwargs ) \n 
~~ AddComment = add_comment \n 
def update ( self , entry , auth_token = None , ** kwargs ) : \n 
~~~ old_etag = entry . etag \n 
entry . etag = None \n 
response = gdata . client . GDClient . update ( self , entry , \n 
entry . etag = old_etag \n 
~~ Update = update \n 
def delete ( self , entry_or_uri , auth_token = None , ** kwargs ) : \n 
~~~ if isinstance ( entry_or_uri , ( str , unicode , atom . http_core . Uri ) ) : \n 
~~~ return gdata . client . GDClient . delete ( self , entry_or_uri , \n 
~~ old_etag = entry_or_uri . etag \n 
entry_or_uri . etag = None \n 
response = gdata . client . GDClient . delete ( self , entry_or_uri , \n 
entry_or_uri . etag = old_etag \n 
~~ Delete = delete \n 
~~ class Query ( gdata . client . Query ) : \n 
~~~ def __init__ ( self , order_by = None , ** kwargs ) : \n 
~~~ gdata . client . Query . __init__ ( self , ** kwargs ) \n 
self . order_by = order_by \n 
~~ def modify_request ( self , http_request ) : \n 
~~~ gdata . client . _add_query_param ( , self . order_by , http_request ) \n 
gdata . client . Query . modify_request ( self , http_request ) \n 
~~ ModifyRequest = modify_request \n 
import atom . core \n 
GEORSS_TEMPLATE = \n 
GML_TEMPLATE = \n 
GEO_TEMPLATE = \n 
class GeoLat ( atom . core . XmlElement ) : \n 
_qname = GEO_TEMPLATE % \n 
~~ class GeoLong ( atom . core . XmlElement ) : \n 
~~ class GeoRssBox ( atom . core . XmlElement ) : \n 
_qname = GEORSS_TEMPLATE % \n 
~~ class GeoRssPoint ( atom . core . XmlElement ) : \n 
~~ class GmlLowerCorner ( atom . core . XmlElement ) : \n 
_qname = GML_TEMPLATE % \n 
~~ class GmlPos ( atom . core . XmlElement ) : \n 
~~ class GmlPoint ( atom . core . XmlElement ) : \n 
pos = GmlPos \n 
~~ class GmlUpperCorner ( atom . core . XmlElement ) : \n 
~~ class GmlEnvelope ( atom . core . XmlElement ) : \n 
lower_corner = GmlLowerCorner \n 
upper_corner = GmlUpperCorner \n 
~~ class GeoRssWhere ( atom . core . XmlElement ) : \n 
Point = GmlPoint \n 
Envelope = GmlEnvelope \n 
~~ class W3CPoint ( atom . core . XmlElement ) : \n 
long = GeoLong \n 
lat = GeoLat \n 
from . cryptomath import * \n 
class RSAKey : \n 
def __init__ ( self , n = 0 , e = 0 ) : \n 
return numBits ( self . n ) \n 
~~ def hasPrivateKey ( self ) : \n 
~~ def hash ( self ) : \n 
~~ def getSigningAlgorithm ( self ) : \n 
return "pkcs1-sha1" \n 
~~ def hashAndSign ( self , bytes ) : \n 
if not isinstance ( bytes , type ( "" ) ) : \n 
~~~ bytes = bytesToString ( bytes ) \n 
~~ hashBytes = stringToBytes ( sha1 ( bytes ) . digest ( ) ) \n 
prefixedHashBytes = self . _addPKCS1SHA1Prefix ( hashBytes ) \n 
sigBytes = self . sign ( prefixedHashBytes ) \n 
return sigBytes \n 
~~ def hashAndVerify ( self , sigBytes , bytes ) : \n 
return self . verify ( sigBytes , prefixedHashBytes ) \n 
~~ def sign ( self , bytes ) : \n 
if not self . hasPrivateKey ( ) : \n 
~~ paddedBytes = self . _addPKCS1Padding ( bytes , 1 ) \n 
m = bytesToNumber ( paddedBytes ) \n 
if m >= self . n : \n 
~~ c = self . _rawPrivateKeyOp ( m ) \n 
sigBytes = numberToBytes ( c ) \n 
~~ def verify ( self , sigBytes , bytes ) : \n 
paddedBytes = self . _addPKCS1Padding ( bytes , 1 ) \n 
c = bytesToNumber ( sigBytes ) \n 
if c >= self . n : \n 
~~ m = self . _rawPublicKeyOp ( c ) \n 
checkBytes = numberToBytes ( m ) \n 
return checkBytes == paddedBytes \n 
~~ def encrypt ( self , bytes ) : \n 
paddedBytes = self . _addPKCS1Padding ( bytes , 2 ) \n 
~~ c = self . _rawPublicKeyOp ( m ) \n 
encBytes = numberToBytes ( c ) \n 
return encBytes \n 
~~ def decrypt ( self , encBytes ) : \n 
~~ c = bytesToNumber ( encBytes ) \n 
~~ m = self . _rawPrivateKeyOp ( c ) \n 
decBytes = numberToBytes ( m ) \n 
~~~ if decBytes [ x ] == 0 : \n 
~~ def _rawPrivateKeyOp ( self , m ) : \n 
~~ def _rawPublicKeyOp ( self , c ) : \n 
~~ def acceptsPassword ( self ) : \n 
~~ def write ( self , password = None ) : \n 
~~ def writeXMLPublicKey ( self , indent = ) : \n 
return Python_RSAKey ( self . n , self . e ) . write ( indent ) \n 
~~ def generate ( bits ) : \n 
~~ generate = staticmethod ( generate ) \n 
def _addPKCS1SHA1Prefix ( self , bytes ) : \n 
~~~ prefixBytes = createByteArraySequence ( [ 48 , 33 , 48 , 9 , 6 , 5 , 43 , 14 , 3 , 2 , 26 , 5 , 0 , 4 , 20 ] ) \n 
prefixedBytes = prefixBytes + bytes \n 
return prefixedBytes \n 
~~ def _addPKCS1Padding ( self , bytes , blockType ) : \n 
~~~ padLength = ( numBytes ( self . n ) - ( len ( bytes ) + 3 ) ) \n 
~~~ pad = [ 0xFF ] * padLength \n 
~~~ pad = createByteArraySequence ( [ ] ) \n 
while len ( pad ) < padLength : \n 
~~~ padBytes = getRandomBytes ( padLength * 2 ) \n 
pad = [ b for b in padBytes if b != 0 ] \n 
pad = pad [ : padLength ] \n 
~~ padding = createByteArraySequence ( [ blockType ] + pad + [ 0 ] ) \n 
paddedBytes = padding + bytes \n 
return paddedBytes \n 
~~ ~~ import i3ipc \n 
from argparse import ArgumentParser \n 
~~~ parser = ArgumentParser ( description = ) \n 
parser . parse_args ( ) \n 
i3 = i3ipc . Connection ( ) \n 
workspaces = i3 . get_workspaces ( ) \n 
numbered_workspaces = filter ( lambda w : w . name [ 0 ] . isdigit ( ) , workspaces ) \n 
numbers = list ( map ( lambda w : int ( re . search ( , w . name ) . group ( 0 ) ) , numbered_workspaces ) \n 
new = 0 \n 
for i in range ( 1 , max ( numbers ) + 2 ) : \n 
~~~ if i not in numbers : \n 
~~~ new = i \n 
~~ import uuid \n 
import dbus \n 
from . . interfaces import GattService , GattCharacteristic , GattDescriptor \n 
from . . platform import get_provider \n 
_SERVICE_INTERFACE = \n 
_CHARACTERISTIC_INTERFACE = \n 
_DESCRIPTOR_INTERFACE = \n 
class BluezGattService ( GattService ) : \n 
def __init__ ( self , dbus_obj ) : \n 
self . _props = dbus . Interface ( dbus_obj , ) \n 
def uuid ( self ) : \n 
return uuid . UUID ( str ( self . _props . Get ( _SERVICE_INTERFACE , ) ) ) \n 
~~ def list_characteristics ( self ) : \n 
paths = self . _props . Get ( _SERVICE_INTERFACE , ) \n 
return map ( BluezGattCharacteristic , \n 
get_provider ( ) . _get_objects_by_path ( paths ) ) \n 
~~ ~~ class BluezGattCharacteristic ( GattCharacteristic ) : \n 
self . _characteristic = dbus . Interface ( dbus_obj , _CHARACTERISTIC_INTERFACE ) \n 
return uuid . UUID ( str ( self . _props . Get ( _CHARACTERISTIC_INTERFACE , ) ) ) \n 
~~ def read_value ( self ) : \n 
return self . _characteristic . ReadValue ( ) \n 
~~ def write_value ( self , value ) : \n 
self . _characteristic . WriteValue ( value ) \n 
~~ def start_notify ( self , on_change ) : \n 
def characteristic_changed ( iface , changed_props , invalidated_props ) : \n 
~~~ if iface != _CHARACTERISTIC_INTERFACE : \n 
~~ if not in changed_props : \n 
~~ on_change ( . join ( map ( chr , changed_props [ ] ) ) ) \n 
~~ self . _props . connect_to_signal ( , characteristic_changed ) \n 
self . _characteristic . StartNotify ( ) \n 
~~ def stop_notify ( self ) : \n 
self . _characteristic . StopNotify ( ) \n 
~~ def list_descriptors ( self ) : \n 
paths = self . _props . Get ( _CHARACTERISTIC_INTERFACE , ) \n 
return map ( BluezGattDescriptor , \n 
~~ ~~ class BluezGattDescriptor ( GattDescriptor ) : \n 
self . _descriptor = dbus . Interface ( dbus_obj , _DESCRIPTOR_INTERFACE ) \n 
return uuid . UUID ( str ( self . _props . Get ( _DESCRIPTOR_INTERFACE , ) ) ) \n 
return self . _descriptor . ReadValue ( ) \n 
import Adafruit_CharLCD as LCD \n 
lcd_rs = 27 \n 
lcd_en = 22 \n 
lcd_d4 = 25 \n 
lcd_d5 = 24 \n 
lcd_d6 = 23 \n 
lcd_d7 = 18 \n 
lcd_backlight = 4 \n 
lcd_columns = 16 \n 
lcd_rows = 2 \n 
lcd = LCD . Adafruit_CharLCD ( lcd_rs , lcd_en , lcd_d4 , lcd_d5 , lcd_d6 , lcd_d7 , \n 
lcd_columns , lcd_rows , lcd_backlight ) \n 
lcd . message ( ) \n 
time . sleep ( 5.0 ) \n 
lcd . clear ( ) \n 
lcd . show_cursor ( True ) \n 
lcd . blink ( True ) \n 
lcd . show_cursor ( False ) \n 
lcd . blink ( False ) \n 
message = \n 
lcd . message ( message ) \n 
for i in range ( lcd_columns - len ( message ) ) : \n 
~~~ time . sleep ( 0.5 ) \n 
lcd . move_right ( ) \n 
~~ for i in range ( lcd_columns - len ( message ) ) : \n 
lcd . move_left ( ) \n 
~~ lcd . clear ( ) \n 
lcd . set_backlight ( 0 ) \n 
time . sleep ( 2.0 ) \n 
lcd . set_backlight ( 1 ) \n 
from ez_setup import use_setuptools \n 
use_setuptools ( ) \n 
classifiers = [ , \n 
setup ( name = , \n 
description = license = , \n 
classifiers = classifiers , \n 
dependency_links = [ install_requires = [ ] , \n 
packages = find_packages ( ) ) \n 
class GCConnectionStatus : \n 
~~~ GCConnectionStatus_HAVE_SESSION = 999 \n 
GCConnectionStatus_GC_GOING_DOWN = 1 \n 
GCConnectionStatus_NO_SESSION = 2 \n 
GCConnectionStatus_NO_SESSION_IN_LOGON_QUEUE = 3 \n 
GCConnectionStatus_NO_STEAM = 4 \n 
~~ class EGCSystemMsg : \n 
~~~ k_EGCMsgInvalid = 0 \n 
k_EGCMsgMulti = 1 \n 
k_EGCMsgGenericReply = 10 \n 
k_EGCMsgSystemBase = 50 \n 
k_EGCMsgAchievementAwarded = 51 \n 
k_EGCMsgConCommand = 52 \n 
k_EGCMsgStartPlaying = 53 \n 
k_EGCMsgStopPlaying = 54 \n 
k_EGCMsgStartGameserver = 55 \n 
k_EGCMsgStopGameserver = 56 \n 
k_EGCMsgWGRequest = 57 \n 
k_EGCMsgWGResponse = 58 \n 
k_EGCMsgGetUserGameStatsSchema = 59 \n 
k_EGCMsgGetUserGameStatsSchemaResponse = 60 \n 
k_EGCMsgGetUserStatsDEPRECATED = 61 \n 
k_EGCMsgGetUserStatsResponse = 62 \n 
k_EGCMsgAppInfoUpdated = 63 \n 
k_EGCMsgValidateSession = 64 \n 
k_EGCMsgValidateSessionResponse = 65 \n 
k_EGCMsgLookupAccountFromInput = 66 \n 
k_EGCMsgSendHTTPRequest = 67 \n 
k_EGCMsgSendHTTPRequestResponse = 68 \n 
k_EGCMsgPreTestSetup = 69 \n 
k_EGCMsgRecordSupportAction = 70 \n 
k_EGCMsgGetAccountDetails_DEPRECATED = 71 \n 
k_EGCMsgReceiveInterAppMessage = 73 \n 
k_EGCMsgFindAccounts = 74 \n 
k_EGCMsgPostAlert = 75 \n 
k_EGCMsgGetLicenses = 76 \n 
k_EGCMsgGetUserStats = 77 \n 
k_EGCMsgGetCommands = 78 \n 
k_EGCMsgGetCommandsResponse = 79 \n 
k_EGCMsgAddFreeLicense = 80 \n 
k_EGCMsgAddFreeLicenseResponse = 81 \n 
k_EGCMsgGetIPLocation = 82 \n 
k_EGCMsgGetIPLocationResponse = 83 \n 
k_EGCMsgSystemStatsSchema = 84 \n 
k_EGCMsgGetSystemStats = 85 \n 
k_EGCMsgGetSystemStatsResponse = 86 \n 
k_EGCMsgSendEmail = 87 \n 
k_EGCMsgSendEmailResponse = 88 \n 
k_EGCMsgGetEmailTemplate = 89 \n 
k_EGCMsgGetEmailTemplateResponse = 90 \n 
k_EGCMsgGrantGuestPass = 91 \n 
k_EGCMsgGrantGuestPassResponse = 92 \n 
k_EGCMsgGetAccountDetails = 93 \n 
k_EGCMsgGetAccountDetailsResponse = 94 \n 
k_EGCMsgGetPersonaNames = 95 \n 
k_EGCMsgGetPersonaNamesResponse = 96 \n 
k_EGCMsgMultiplexMsg = 97 \n 
k_EGCMsgWebAPIRegisterInterfaces = 101 \n 
k_EGCMsgWebAPIJobRequest = 102 \n 
k_EGCMsgWebAPIJobRequestHttpResponse = 104 \n 
k_EGCMsgWebAPIJobRequestForwardResponse = 105 \n 
k_EGCMsgMemCachedGet = 200 \n 
k_EGCMsgMemCachedGetResponse = 201 \n 
k_EGCMsgMemCachedSet = 202 \n 
k_EGCMsgMemCachedDelete = 203 \n 
k_EGCMsgMemCachedStats = 204 \n 
k_EGCMsgMemCachedStatsResponse = 205 \n 
k_EGCMsgMasterSetDirectory = 220 \n 
k_EGCMsgMasterSetDirectoryResponse = 221 \n 
k_EGCMsgMasterSetWebAPIRouting = 222 \n 
k_EGCMsgMasterSetWebAPIRoutingResponse = 223 \n 
k_EGCMsgMasterSetClientMsgRouting = 224 \n 
k_EGCMsgMasterSetClientMsgRoutingResponse = 225 \n 
k_EGCMsgSetOptions = 226 \n 
k_EGCMsgSetOptionsResponse = 227 \n 
k_EGCMsgSystemBase2 = 500 \n 
k_EGCMsgGetPurchaseTrustStatus = 501 \n 
k_EGCMsgGetPurchaseTrustStatusResponse = 502 \n 
k_EGCMsgUpdateSession = 503 \n 
k_EGCMsgGCAccountVacStatusChange = 504 \n 
k_EGCMsgCheckFriendship = 505 \n 
k_EGCMsgCheckFriendshipResponse = 506 \n 
~~ class ESOMsg : \n 
~~~ k_ESOMsg_Create = 21 \n 
k_ESOMsg_Update = 22 \n 
k_ESOMsg_Destroy = 23 \n 
k_ESOMsg_CacheSubscribed = 24 \n 
k_ESOMsg_CacheUnsubscribed = 25 \n 
k_ESOMsg_UpdateMultiple = 26 \n 
k_ESOMsg_CacheSubscriptionCheck = 27 \n 
k_ESOMsg_CacheSubscriptionRefresh = 28 \n 
~~ class EGCBaseClientMsg : \n 
~~~ k_EMsgGCClientWelcome = 4004 \n 
k_EMsgGCServerWelcome = 4005 \n 
k_EMsgGCClientHello = 4006 \n 
k_EMsgGCServerHello = 4007 \n 
k_EMsgGCClientConnectionStatus = 4009 \n 
k_EMsgGCServerConnectionStatus = 4010 \n 
~~ class EGCToGCMsg : \n 
~~~ k_EGCToGCMsgMasterAck = 150 \n 
k_EGCToGCMsgMasterAckResponse = 151 \n 
k_EGCToGCMsgRouted = 152 \n 
k_EGCToGCMsgRoutedReply = 153 \n 
k_EMsgUpdateSessionIP = 154 \n 
k_EMsgRequestSessionIP = 155 \n 
k_EMsgRequestSessionIPResponse = 156 \n 
k_EGCToGCMsgMasterStartupComplete = 157 \n 
~~ class ECSGOCMsg : \n 
~~~ k_EMsgGCCStrike15_v2_Base = 9100 \n 
k_EMsgGCCStrike15_v2_MatchmakingStart = 9101 \n 
k_EMsgGCCStrike15_v2_MatchmakingStop = 9102 \n 
k_EMsgGCCStrike15_v2_MatchmakingClient2ServerPing = 9103 \n 
k_EMsgGCCStrike15_v2_MatchmakingGC2ClientUpdate = 9104 \n 
k_EMsgGCCStrike15_v2_MatchmakingGC2ServerReserve = 9105 \n 
k_EMsgGCCStrike15_v2_MatchmakingServerReservationResponse = 9106 \n 
k_EMsgGCCStrike15_v2_MatchmakingGC2ClientReserve = 9107 \n 
k_EMsgGCCStrike15_v2_MatchmakingServerRoundStats = 9108 \n 
k_EMsgGCCStrike15_v2_MatchmakingClient2GCHello = 9109 \n 
k_EMsgGCCStrike15_v2_MatchmakingGC2ClientHello = 9110 \n 
k_EMsgGCCStrike15_v2_MatchmakingServerMatchEnd = 9111 \n 
k_EMsgGCCStrike15_v2_MatchmakingGC2ClientAbandon = 9112 \n 
k_EMsgGCCStrike15_v2_MatchmakingServer2GCKick = 9113 \n 
k_EMsgGCCStrike15_v2_MatchmakingGC2ServerConfirm = 9114 \n 
k_EMsgGCCStrike15_v2_MatchmakingGCOperationalStats = 9115 \n 
k_EMsgGCCStrike15_v2_MatchmakingGC2ServerRankUpdate = 9116 \n 
k_EMsgGCCStrike15_v2_MatchmakingOperator2GCBlogUpdate = 9117 \n 
k_EMsgGCCStrike15_v2_ServerNotificationForUserPenalty = 9118 \n 
k_EMsgGCCStrike15_v2_ClientReportPlayer = 9119 \n 
k_EMsgGCCStrike15_v2_ClientReportServer = 9120 \n 
k_EMsgGCCStrike15_v2_ClientCommendPlayer = 9121 \n 
k_EMsgGCCStrike15_v2_ClientReportResponse = 9122 \n 
k_EMsgGCCStrike15_v2_ClientCommendPlayerQuery = 9123 \n 
k_EMsgGCCStrike15_v2_ClientCommendPlayerQueryResponse = 9124 \n 
k_EMsgGCCStrike15_v2_WatchInfoUsers = 9126 \n 
k_EMsgGCCStrike15_v2_ClientRequestPlayersProfile = 9127 \n 
k_EMsgGCCStrike15_v2_PlayersProfile = 9128 \n 
k_EMsgGCCStrike15_v2_SetMyMedalsInfo = 9129 \n 
k_EMsgGCCStrike15_v2_PlayerEarnedRewardNotification = 9130 \n 
k_EMsgGCCStrike15_v2_PlayerOverwatchCaseUpdate = 9131 \n 
k_EMsgGCCStrike15_v2_PlayerOverwatchCaseAssignment = 9132 \n 
k_EMsgGCCStrike15_v2_PlayerOverwatchCaseStatus = 9133 \n 
k_EMsgGCCStrike15_v2_GC2ClientTextMsg = 9134 \n 
k_EMsgGCCStrike15_v2_Client2GCTextMsg = 9135 \n 
k_EMsgGCCStrike15_v2_MatchEndRunRewardDrops = 9136 \n 
k_EMsgGCCStrike15_v2_MatchEndRewardDropsNotification = 9137 \n 
k_EMsgGCCStrike15_v2_ClientRequestWatchInfoFriends2 = 9138 \n 
k_EMsgGCCStrike15_v2_MatchList = 9139 \n 
k_EMsgGCCStrike15_v2_MatchListRequestCurrentLiveGames = 9140 \n 
k_EMsgGCCStrike15_v2_MatchListRequestRecentUserGames = 9141 \n 
k_EMsgGCCStrike15_v2_GC2ServerReservationUpdate = 9142 \n 
k_EMsgGCCStrike15_v2_ClientVarValueNotificationInfo = 9144 \n 
k_EMsgGCCStrike15_v2_TournamentMatchRewardDropsNotification = 9145 \n 
k_EMsgGCCStrike15_v2_MatchListRequestTournamentGames = 9146 \n 
k_EMsgGCCStrike15_v2_MatchListRequestFullGameInfo = 9147 \n 
k_EMsgGCCStrike15_v2_GiftsLeaderboardRequest = 9148 \n 
k_EMsgGCCStrike15_v2_GiftsLeaderboardResponse = 9149 \n 
k_EMsgGCCStrike15_v2_ServerVarValueNotificationInfo = 9150 \n 
k_EMsgGCToGCReloadVersions = 9151 \n 
k_EMsgGCCStrike15_v2_ClientSubmitSurveyVote = 9152 \n 
k_EMsgGCCStrike15_v2_Server2GCClientValidate = 9153 \n 
k_EMsgGCCStrike15_v2_MatchListRequestLiveGameForUser = 9154 \n 
k_EMsgGCCStrike15_v2_Server2GCPureServerValidationFailure = 9155 \n 
k_EMsgGCCStrike15_v2_Client2GCEconPreviewDataBlockRequest = 9156 \n 
k_EMsgGCCStrike15_v2_Client2GCEconPreviewDataBlockResponse = 9157 \n 
k_EMsgGCCStrike15_v2_AccountPrivacySettings = 9158 \n 
k_EMsgGCCStrike15_v2_SetMyActivityInfo = 9159 \n 
k_EMsgGCCStrike15_v2_MatchListRequestTournamentPredictions = 9160 \n 
k_EMsgGCCStrike15_v2_MatchListUploadTournamentPredictions = 9161 \n 
k_EMsgGCCStrike15_v2_DraftSummary = 9162 \n 
k_EMsgGCCStrike15_v2_ClientRequestJoinFriendData = 9163 \n 
k_EMsgGCCStrike15_v2_ClientRequestJoinServerData = 9164 \n 
k_EMsgGCCStrike15_v2_ClientRequestNewMission = 9165 \n 
~~ from . base import Base \n 
class Pep0257Formatter ( Base ) : \n 
def decorators ( self , attributes ) : \n 
~~ def extends ( self , attributes ) : \n 
~~ def arguments ( self , attributes ) : \n 
~~~ section = \n 
for attr in attributes [ ] : \n 
~~~ section += template . format ( \n 
name = self . _generate_field ( , attr [ ] ) , \n 
description = self . _generate_field ( ) , \n 
~~ if len ( attributes [ ] ) == 0 : \n 
~~ section += self . keyword_arguments ( attributes [ ] ) \n 
return section \n 
~~ def keyword_arguments ( self , attributes ) : \n 
if len ( attributes ) == 0 : \n 
~~ for attr in attributes : \n 
default = self . _generate_field ( , attr [ ] ) , \n 
~~ return section \n 
~~ def returns ( self , attribute ) : \n 
~~ def yields ( self , attribute ) : \n 
~~ def raises ( self , attributes ) : \n 
attribute = attr , \n 
tab_index_1 = next ( self . tab_index ) \n 
~~ def variables ( self , attributes ) : \n 
Bit1BooleanField , NullBit1BooleanField \n 
ListCharField , ListTextField \n 
SetCharField , SetTextField \n 
SizedBinaryField , SizedTextField \n 
from django . db import migrations \n 
from django_mysql . models import EnumField \n 
migrations . AlterField ( \n 
field = EnumField ( choices = [ \n 
( , ) , ( , ) , \n 
~~ from django . db import connections \n 
class IsMariaDBTests ( TestCase ) : \n 
~~~ def test_connections ( self ) : \n 
~~~ for alias in connections : \n 
~~~ connection = connections [ alias ] \n 
if not hasattr ( connection , ) : \n 
~~ with connection . cursor ( ) as cursor : \n 
version = cursor . fetchone ( ) [ 0 ] \n 
~~ is_mariadb = ( in version ) \n 
assert connection . is_mariadb == is_mariadb \n 
assert in connection . __dict__ \n 
~~ ~~ ~~ from django . test import TestCase \n 
~~~ from django . contrib . auth import get_user_model \n 
User = get_user_model ( ) \n 
~~~ from django . contrib . auth . models import User \n 
~~ class TestSuBackend ( TestCase ) : \n 
~~~ super ( TestSuBackend , self ) . setUp ( ) \n 
from django_su . backends import SuBackend \n 
self . user = User . objects . create ( username = ) \n 
self . backend = SuBackend ( ) \n 
~~ def test_authenticate_do_it ( self ) : \n 
self . backend . authenticate ( su = True , user_id = self . user . pk ) , \n 
self . user \n 
~~ def test_authenticate_dont_do_it ( self ) : \n 
self . backend . authenticate ( su = False , user_id = self . user . pk ) , \n 
~~ def test_authenticate_id_none ( self ) : \n 
self . backend . authenticate ( su = True , user_id = None ) , \n 
~~ def test_authenticate_id_non_existent ( self ) : \n 
self . backend . authenticate ( su = True , user_id = 999 ) , \n 
~~ def test_authenticate_id_invalid ( self ) : \n 
self . backend . authenticate ( su = True , user_id = ) , \n 
~~ def test_get_user_exists ( self ) : \n 
self . backend . get_user ( user_id = self . user . pk ) , \n 
~~ def test_get_user_does_not_exist ( self ) : \n 
self . backend . get_user ( user_id = 999 ) , \n 
from sarge import capture_stdout , run \n 
from dokku_client . command_tools import global_opts_doc \n 
class BaseCommand ( object ) : \n 
~~~ check_config = True \n 
sort_order = 10 \n 
~~~ self . _name = name \n 
self . args = { } \n 
def doc ( self ) : \n 
doc = self . __doc__ \n 
doc = re . sub ( , , doc , flags = re . MULTILINE ) \n 
doc = re . sub ( , "\\n\\n%s" % global_opts_doc ( ) , doc ) \n 
return doc \n 
def description ( self ) : \n 
~~~ return self . __doc__ . strip ( ) . split ( "\\n" ) [ 0 ] \n 
def name ( self ) : \n 
~~~ return self . _name \n 
~~ def main ( self , global_args , command_args ) : \n 
~~ def run_remote ( self , cmd , input = None ) : \n 
~~~ host = self . args [ ] \n 
return capture_stdout ( % ( host , cmd ) , input = input ) \n 
~~ def restart_container ( self ) : \n 
~~~ app = self . args [ ] \n 
self . run_remote ( % app ) \n 
import pysam \n 
from collections import OrderedDict as od \n 
logging . basicConfig ( format = FORMAT ) \n 
logger . setLevel ( logging . INFO ) \n 
~~~ assert os . path . exists ( args . fasta + ) , % args . fasta \n 
fa = pysam . FastaFile ( args . fasta ) \n 
bam = pysam . AlignmentFile ( args . bam , ) \n 
bam_reflen = od ( [ ( ref , length ) for ref , length in zip ( bam . references , bam . lengths ) ] ) \n 
with open ( args . outfa , ) as outfa : \n 
~~~ for ref in bam . references : \n 
~~~ assert ref in fa . references , % ref \n 
assert fa . get_reference_length ( ref ) == bam_reflen [ ref ] , \n 
logger . info ( % ( ref , args . outfa , bam_reflen [ ref ] ) ) \n 
outfa . write ( % ( ref , fa . fetch ( ref ) ) ) \n 
parser . add_argument ( , , default = , help = args = parser . parse_args ( ) \n 
main ( args ) \n 
from __future__ import print_function , division \n 
import fnmatch \n 
EXCLUSIONS = ( "agg-kicad.lib" , "conn.lib" , "power.lib" , "switch.lib" ) \n 
def checkdefs ( contents , libf , errs ) : \n 
~~~ n_defs = re_defs . findall ( contents ) \n 
if len ( n_defs ) > 1 : \n 
~~ elif len ( n_defs ) == 0 : \n 
~~ partname = n_defs [ 0 ] [ 0 ] \n 
designator = n_defs [ 0 ] [ 1 ] \n 
libname = os . path . split ( libf ) [ - 1 ] . split ( "." ) [ 0 ] \n 
if partname . lower ( ) != libname : \n 
. format ( partname , libname ) ) \n 
~~ return partname , designator \n 
~~ def checkpins ( contents , designator , errs ) : \n 
~~~ pins = re_pins . findall ( contents ) \n 
nums = [ ] \n 
for name , num , x , y , length , numsize , namesize in pins : \n 
~~~ if int ( x ) % 100 != 0 or int ( y ) % 100 != 0 : \n 
~~ if designator in ( "IC" , "U" ) and int ( length ) not in ( 100 , 150 ) : \n 
. format ( name ) ) \n 
~~ if int ( namesize ) != 50 or ( int ( numsize ) != 50 and num . isdigit ( ) ) : \n 
~~ if num . isdigit ( ) : \n 
~~~ nums . append ( int ( num ) ) \n 
~~ ~~ if nums : \n 
~~~ expected = set ( range ( min ( nums ) , max ( nums ) + 1 ) ) \n 
if set ( nums ) != expected : \n 
~~~ missing = [ str ( x ) for x in set ( expected ) - set ( nums ) ] \n 
~~ duplicates = set ( [ str ( x ) for x in nums if nums . count ( x ) > 1 ] ) \n 
if duplicates : \n 
~~ ~~ ~~ def checkboxes ( contents , designator , errs ) : \n 
~~~ if designator == "IC" : \n 
~~~ boxes = re_poly . findall ( contents ) \n 
if "f" not in boxes : \n 
~~ ~~ ~~ def checkfields ( contents , errs , prettypath ) : \n 
~~~ refn_f = re_refn . findall ( contents ) \n 
name_f = re_name . findall ( contents ) \n 
foot_f = re_fp . findall ( contents ) \n 
data_f = re_ds . findall ( contents ) \n 
code_f = re_oc . findall ( contents ) \n 
fields = ( ( refn_f , "reference" ) , ( name_f , "name" ) , ( foot_f , "footprint" ) , \n 
for field , fn in fields : \n 
~~~ for value , x , y , size , orient , visible , hjust , vjust in field : \n 
~~~ if fn in ( "reference" , "name" ) : \n 
~~~ if visible != "V" : \n 
~~~ if "#invisible{}" . format ( fn ) not in contents : \n 
~~~ if visible != "I" : \n 
~~ ~~ if orient != "H" : \n 
~~ if size != "50" : \n 
~~ ~~ ~~ refn_y = int ( refn_f [ 0 ] [ 2 ] ) \n 
name_y = int ( name_f [ 0 ] [ 2 ] ) \n 
if refn_y <= name_y : \n 
~~ fp = foot_f [ 0 ] [ 0 ] [ 1 : - 1 ] \n 
if fp . startswith ( "agg:" ) : \n 
~~~ fp = fp . split ( ":" ) [ 1 ] + ".kicad_mod" \n 
path = os . path . join ( prettypath , fp ) \n 
. format ( fp ) ) \n 
~~ ~~ elif len ( fp ) > 0 and ":" not in fp : \n 
~~ ~~ def checklib ( libf , prettypath ) : \n 
~~~ errs = [ ] \n 
dcmpath = "." . join ( libf . split ( "." ) [ : - 1 ] ) + ".dcm" \n 
if not os . path . isfile ( dcmpath ) : \n 
~~ with open ( libf ) as f : \n 
~~~ contents = f . read ( ) \n 
~~ partname , designator = checkdefs ( contents , libf , errs ) \n 
checkpins ( contents , designator , errs ) \n 
checkboxes ( contents , designator , errs ) \n 
checkfields ( contents , errs , prettypath ) \n 
if len ( errs ) == 0 : \n 
for err in errs : \n 
~~ print ( "" , file = sys . stderr ) \n 
~~ ~~ def main ( libpath , prettypath ) : \n 
~~~ ok = True \n 
for dirpath , dirnames , files in os . walk ( libpath ) : \n 
~~~ dirnames . sort ( ) \n 
files . sort ( ) \n 
for f in fnmatch . filter ( files , "*.lib" ) : \n 
~~~ path = os . path . join ( dirpath , f ) \n 
if f not in EXCLUSIONS : \n 
~~~ result = checklib ( path , prettypath ) \n 
~~~ ok = False \n 
~~ ~~ ~~ return ok \n 
~~~ if len ( sys . argv ) != 3 : \n 
~~~ libpath = sys . argv [ 1 ] \n 
prettypath = sys . argv [ 2 ] \n 
success = main ( libpath , prettypath ) \n 
if success : \n 
~~~ sys . exit ( 1 ) \n 
from abc import ABCMeta , abstractmethod \n 
from _config import AttrDict \n 
__all__ = [ , , , , , \n 
, , ] \n 
def multikey_getter_gen ( parser , keys , is_indices = False , delimiter = "\\t" ) : \n 
if is_indices : \n 
~~~ keys = map ( int , keys ) \n 
~~ def multikey_getter ( line , parser , keyset ) : \n 
~~~ data = parser ( line . strip ( ) ) \n 
return delimiter . join ( ( unicode ( data [ k ] ) for k in keyset ) ) \n 
~~ def multiindex_getter ( line , parser , keyset ) : \n 
return delimiter . join ( ( unicode ( data . by_index ( idx - 1 , raw = True ) ) for idx in keys ) ) \n 
~~ if is_indices is True : \n 
~~~ return partial ( multiindex_getter , parser = parser , keyset = keys ) \n 
~~~ return partial ( multikey_getter , parser = parser , keyset = keys ) \n 
~~ ~~ def unescape_json ( s ) : \n 
return s . replace ( "\\\\/" , ) . replace ( \'\\\\"\' , \'"\' ) . decode ( ) \n 
~~ class LogParser ( object ) : \n 
__metaclass__ = ABCMeta \n 
def __call__ ( self , line ) : \n 
return self . parse ( line ) \n 
~~ @ abstractmethod \n 
def parse ( self , line ) : \n 
~~ def set_format ( self , format ) : \n 
~~ ~~ class LogLine ( dict ) : \n 
def __init__ ( self , fieldnames = None ) : \n 
self . _fieldnames = None \n 
if fieldnames : \n 
~~~ self . fieldnames = fieldnames \n 
def fieldnames ( self ) : \n 
return self . _fieldnames \n 
~~ @ fieldnames . setter \n 
def fieldnames ( self , fieldnames ) : \n 
self . _fieldnames = dict ( enumerate ( fieldnames ) ) \n 
~~ def by_index ( self , i , raw = False ) : \n 
~~~ return self . by_key ( self . _fieldnames [ i ] , raw = raw ) \n 
~~ def by_key ( self , key , raw = False ) : \n 
val = None \n 
if raw is True : \n 
~~~ return self [ key ] \n 
~~ if key == : \n 
~~~ val = datetime . strptime ( self [ key ] [ 1 : - 7 ] , ) \n 
~~~ val = self [ key ] \n 
~~ return val \n 
~~ ~~ class JSONParser ( LogParser ) : \n 
~~~ LogParser . __init__ ( self ) \n 
self . _logline_wrapper = LogLine ( ) \n 
~~ def parse ( self , line ) : \n 
parsed_row = json . loads ( line ) \n 
data = self . _logline_wrapper \n 
self . _logline_wrapper . fieldnames = parsed_row . keys ( ) \n 
data . clear ( ) \n 
for k , v in parsed_row . iteritems ( ) : \n 
~~ ~~ class AccessLog ( LogParser ) : \n 
def __init__ ( self , format = None ) : \n 
self . fieldnames = None \n 
self . fieldselector = None \n 
self . _logline_wrapper = None \n 
if format : \n 
~~~ self . fieldselector = self . _parse_log_format ( format ) \n 
self . _logline_wrapper = LogLine ( self . fieldnames ) \n 
~~ ~~ def set_format ( self , format ) : \n 
self . fieldselector = self . _parse_log_format ( format ) \n 
~~ def parse ( self , logline ) : \n 
~~~ match = self . fieldselector . match ( logline ) \n 
~~ except AttributeError , exc : \n 
~~ if match : \n 
~~~ data = self . _logline_wrapper \n 
for k , v in zip ( self . fieldnames , match . groups ( ) ) : \n 
~~ ~~ def _parse_log_format ( self , format ) : \n 
format = format . strip ( ) \n 
format = re . sub ( , , format ) \n 
subpatterns = [ ] \n 
findquotes = re . compile ( r\'^"\' ) \n 
findreferreragent = re . compile ( ) \n 
findpercent = re . compile ( ) \n 
lstripquotes = re . compile ( r\'^"\' ) \n 
rstripquotes = re . compile ( r\'"$\' ) \n 
self . fieldnames = [ ] \n 
for element in format . split ( ) : \n 
~~~ hasquotes = 0 \n 
if findquotes . match ( element ) : \n 
~~~ hasquotes = 1 \n 
~~ if hasquotes : \n 
~~~ element = lstripquotes . sub ( , element ) \n 
element = rstripquotes . sub ( , element ) \n 
~~ self . fieldnames . append ( element ) \n 
subpattern = \n 
if hasquotes : \n 
~~~ if element == or findreferreragent . search ( element ) : \n 
~~~ subpattern = r\'\\"([^"\\\\]*(?:\\\\.[^"\\\\]*)*)\\"\' \n 
~~~ subpattern = r\'\\"([^\\"]*)\\"\' \n 
~~ ~~ elif findpercent . search ( element ) : \n 
~~~ subpattern = \n 
~~ elif element == : \n 
~~ subpatterns . append ( subpattern ) \n 
~~ _pattern = + . join ( subpatterns ) + \n 
_regex = re . compile ( _pattern ) \n 
return _regex \n 
~~ ~~ class CommonLogFormat ( AccessLog ) : \n 
~~ ~~ class uWSGIParser ( LogParser ) : \n 
self . _re = re . compile ( self . fieldnames = ( , , , , , ) \n 
match = self . _re . match ( logline ) \n 
~~ ~~ ~~ import textwrap \n 
import numpy \n 
from OpenGL import GL \n 
from pygly . shader import Shader , VertexShader , FragmentShader , ShaderProgram \n 
from pygly . vertex_buffer import VertexBuffer , BufferAttributes , GenericAttribute , VertexAttribute , TextureCoordAttribute from pygly . vertex_array import VertexArray \n 
from pyrr import geometry \n 
vertices , indices = geometry . create_quad ( scale = ( 5.0 , 5.0 ) , st = True , dtype = ) \n 
vertices = vertices [ indices ] \n 
vertices . dtype = [ \n 
( , , ( 3 , ) ) , \n 
( , , ( 2 , ) ) , \n 
def create ( core_profile = True ) : \n 
~~~ if core_profile : \n 
~~~ return CoreQuad ( ) \n 
~~~ return LegacyQuad ( ) \n 
~~ ~~ class CoreQuad ( object ) : \n 
~~~ super ( CoreQuad , self ) . __init__ ( ) \n 
global vertices \n 
self . shader = ShaderProgram ( \n 
VertexShader ( self . vertex_shader ) , \n 
FragmentShader ( self . fragment_shader ) \n 
self . buffer = VertexBuffer ( \n 
GL . GL_ARRAY_BUFFER , \n 
GL . GL_STATIC_DRAW , \n 
data = vertices , \n 
self . buffer_attributes = BufferAttributes ( ) \n 
self . buffer_attributes [ ] = GenericAttribute . from_dtype ( \n 
self . buffer , \n 
vertices . dtype , \n 
location = self . shader . attributes [ ] \n 
self . vao = VertexArray ( ) \n 
self . vao . bind ( ) \n 
self . buffer . bind ( ) \n 
self . buffer_attributes . set ( ) \n 
self . buffer . unbind ( ) \n 
self . vao . unbind ( ) \n 
~~ def draw ( self , projection , model_view ) : \n 
~~~ global vertices \n 
self . shader . bind ( ) \n 
self . shader . uniforms [ ] . value = projection \n 
self . shader . uniforms [ ] . value = model_view \n 
self . shader . uniforms [ ] . value = 0 \n 
GL . glDrawArrays ( GL . GL_TRIANGLES , 0 , len ( vertices ) ) \n 
self . shader . unbind ( ) \n 
~~ ~~ class LegacyQuad ( object ) : \n 
~~~ super ( LegacyQuad , self ) . __init__ ( ) \n 
self . use_shaders = True \n 
GL . glEnable ( GL . GL_TEXTURE_2D ) \n 
self . buffer_attributes [ ] = VertexAttribute . from_dtype ( \n 
self . buffer_attributes [ ] = TextureCoordAttribute . from_dtype ( \n 
if self . use_shaders : \n 
~~~ self . shader . bind ( ) \n 
~~ self . buffer_attributes . push_attributes ( ) \n 
self . buffer_attributes . pop_attributes ( ) \n 
~~~ self . shader . unbind ( ) \n 
from OpenGL . GL . ARB import texture_rg \n 
from pyrr . utils import parameters_as_numpy_arrays \n 
import numpy_utils \n 
import gl \n 
def active_unit ( ) : \n 
~~~ return GL . glGetInteger ( GL . GL_ACTIVE_TEXTURE ) \n 
~~ def active_texture ( target , unit ) : \n 
unit_ = active_unit ( ) \n 
if unit_ != unit : \n 
~~~ GL . glActiveTexture ( GL . GL_TEXTURE0 + unit ) \n 
~~ query_enums = { \n 
GL . GL_TEXTURE_1D : GL . GL_TEXTURE_BINDING_1D , \n 
GL . GL_TEXTURE_2D : GL . GL_TEXTURE_BINDING_2D , \n 
GL . GL_TEXTURE_3D : GL . GL_TEXTURE_BINDING_3D , \n 
enum = query_enums [ target ] \n 
handle = GL . glGetInteger ( enum ) \n 
~~~ GL . glActiveTexture ( GL . GL_TEXTURE0 + unit_ ) \n 
~~ return handle \n 
~~ def format ( format ) : \n 
~~~ if isinstance ( format , basestring ) : \n 
~~~ return format \n 
~~ ~~ class Texture ( object ) : \n 
inferred_targets = { \n 
1 : GL . GL_TEXTURE_1D , \n 
2 : GL . GL_TEXTURE_2D , \n 
3 : GL . GL_TEXTURE_3D , \n 
target_ndims = { \n 
GL . GL_TEXTURE_1D : 1 , \n 
GL . GL_TEXTURE_2D : 2 , \n 
GL . GL_TEXTURE_3D : 3 , \n 
inferred_formats = { \n 
1 : GL . GL_RED , \n 
2 : texture_rg . GL_RG , \n 
3 : GL . GL_RGB , \n 
4 : GL . GL_RGBA , \n 
funcs = { \n 
1 : ( GL . glTexImage1D , GL . glTexSubImage1D ) , \n 
2 : ( GL . glTexImage2D , GL . glTexSubImage2D ) , \n 
3 : ( GL . glTexImage3D , GL . glTexSubImage3D ) , \n 
channels = { \n 
: GL . GL_RED , \n 
: GL . GL_GREEN , \n 
: GL . GL_BLUE , \n 
: GL . GL_ONE , \n 
: GL . GL_ZERO , \n 
def from_file ( cls , filename , ** kwargs ) : \n 
~~~ from PIL import Image \n 
image = Image . open ( filename ) \n 
return cls . from_image ( image , ** kwargs ) \n 
def from_image ( cls , image , ** kwargs ) : \n 
~~~ texture = cls ( ** kwargs ) \n 
texture . bind ( ) \n 
texture . set_pil_image ( image ) \n 
texture . unbind ( ) \n 
return texture \n 
~~ @ parameters_as_numpy_arrays ( ) \n 
def __init__ ( \n 
data = None , \n 
target = None , \n 
internal_format = None , \n 
min_filter = GL . GL_NEAREST , \n 
mag_filter = GL . GL_NEAREST , \n 
wrap_s = GL . GL_REPEAT , \n 
wrap_t = GL . GL_REPEAT , \n 
shape = None , \n 
format = None , \n 
data_type = None , \n 
border = False , \n 
super ( Texture , self ) . __init__ ( ) \n 
if data != None : \n 
~~~ if not target : \n 
~~~ target = Texture . inferred_targets [ data . ndims - 1 ] \n 
~~ if not shape : \n 
~~~ shape = data . shape [ : - 1 ] \n 
~~ if not data_type : \n 
~~~ data_type = numpy_utils . dtype_gl_enum ( data . dtype ) \n 
~~ if not internal_format : \n 
~~~ internal_format = Texture . inferred_formats [ data . shape [ - 1 ] ] \n 
~~ ~~ self . _target = target \n 
self . _internal_format = internal_format \n 
self . _handle = GL . glGenTextures ( 1 ) \n 
self . _border = border \n 
self . _shape = shape \n 
self . bind ( ) \n 
self . min_filter = min_filter \n 
self . mag_filter = mag_filter \n 
self . wrap_s = wrap_s \n 
self . wrap_t = wrap_t \n 
if shape and data_type and internal_format : \n 
~~~ self . allocate ( shape , data_type , self . _internal_format , ** kwargs ) \n 
~~~ self . set_data ( data , format = format , shape = shape , data_type = data_type , ** kwargs ) \n 
~~ ~~ self . unbind ( ) \n 
def handle ( self ) : \n 
~~~ return self . _handle \n 
def target ( self ) : \n 
~~~ return self . _target \n 
def internal_format ( self ) : \n 
~~~ return self . _internal_format \n 
def border ( self ) : \n 
~~~ return self . _border \n 
def shape ( self ) : \n 
~~~ return self . _shape \n 
def min_filter ( self ) : \n 
~~~ return self . _mag_filter \n 
~~ @ min_filter . setter \n 
def min_filter ( self , mode ) : \n 
~~~ if mode not in [ GL . GL_NEAREST , GL . GL_LINEAR , GL . GL_NEAREST_MIPMAP_NEAREST , GL . GL_LINEAR_MIPMAP_NEAREST ~~~ raise ValueError ( ) \n 
~~ self . _min_filter = mode \n 
GL . glTexParameteri ( self . _target , GL . GL_TEXTURE_MIN_FILTER , mode ) \n 
def mag_filter ( self ) : \n 
~~ @ mag_filter . setter \n 
def mag_filter ( self , mode ) : \n 
~~ self . _mag_filter = mode \n 
GL . glTexParameteri ( self . _target , GL . GL_TEXTURE_MAG_FILTER , mode ) \n 
def wrap_s ( self ) : \n 
~~~ return self . _wrap_s \n 
~~ @ wrap_s . setter \n 
def wrap_s ( self , mode ) : \n 
~~~ if mode not in [ GL . GL_CLAMP_TO_EDGE , GL . GL_CLAMP_TO_BORDER , GL . GL_MIRRORED_REPEAT , GL . GL_REPEAT ~~~ raise ValueError ( ) \n 
~~ self . _wrap_s = mode \n 
GL . glTexParameteri ( self . _target , GL . GL_TEXTURE_WRAP_S , mode ) \n 
def wrap_t ( self ) : \n 
~~~ return self . _wrap_t \n 
~~ @ wrap_t . setter \n 
def wrap_t ( self , mode ) : \n 
~~ self . _wrap_t = mode \n 
GL . glTexParameteri ( self . _target , GL . GL_TEXTURE_WRAP_T , mode ) \n 
~~ def bind ( self ) : \n 
~~~ GL . glBindTexture ( self . _target , self . _handle ) \n 
~~ def unbind ( self ) : \n 
~~~ GL . glBindTexture ( self . _target , 0 ) \n 
~~ def allocate ( self , shape , data_type , internal_format = None , ** kwargs ) : \n 
~~~ if not internal_format : \n 
~~~ internal_format = self . _internal_format \n 
~~~ self . _internal_format = internal_format \n 
~~ ndims = Texture . target_ndims [ self . _target ] \n 
func , _ = Texture . funcs [ ndims ] \n 
border = 1 if self . _border else 0 \n 
self . _shape = tuple ( shape ) \n 
if not data_type : \n 
~~~ if isinstance ( data_type , numpy . dtype ) : \n 
~~~ data_type = numpy_utils . dtype_gl_enum ( data_type ) \n 
~~ elif isinstance ( data_type , basestring ) : \n 
~~~ data_type = numpy_utils . dtype_gl_enum ( numpy . dtype ( data_type ) ) \n 
~~ ~~ args = [ \n 
0 , \n 
internal_format , ] + list ( shape ) + [ \n 
border , \n 
data_type , \n 
None , \n 
func ( * args ) \n 
def set_data ( self , data , format = None , shape = None , offset = None , data_type = None , level = 0 , swizzle = ~~~ def set_swizzle ( swizzle ) : \n 
~~~ if isinstance ( swizzle , ( list , tuple ) ) : \n 
~~~ swizzle = ( GL . GLint * len ( swizzle ) ) ( * swizzle ) \n 
~~ elif isinstance ( swizzle , basestring ) : \n 
~~~ result = [ \n 
Texture . channels [ channel ] \n 
for channel in swizzle . lower ( ) \n 
swizzle = ( GL . GLint * len ( result ) ) ( * result ) \n 
~~ GL . glTexParameteriv ( self . _target , GL . GL_TEXTURE_SWIZZLE_RGBA , swizzle ) \n 
_ , func = Texture . funcs [ ndims ] \n 
if not shape : \n 
~~~ if data . ndims != ( ndims + 1 ) : \n 
~~ shape = data . shape [ : - 1 ] \n 
~~ if not offset : \n 
~~~ offset = [ 0 ] * ndims \n 
~~ if not format : \n 
~~~ num_channels = data . shape [ - 1 ] \n 
format = Texture . inferred_formats [ num_channels ] \n 
~~ if gl . is_core ( ) : \n 
~~~ if swizzle : \n 
~~~ set_swizzle ( swizzle ) \n 
level , ] + list ( offset ) + list ( shape ) + [ \n 
format , \n 
data , \n 
~~ def set_pil_image ( self , image , level = 0 , flip = True , ** kwargs ) : \n 
~~~ def legacy_format ( format ) : \n 
: GL . GL_RGB , \n 
: GL . GL_RGBA , \n 
: GL . GL_LUMINANCE , \n 
: GL . GL_LUMINANCE_ALPHA , \n 
} [ format ] \n 
~~ def core_format ( format ) : \n 
: texture_rg . GL_RG , \n 
~~ types = { \n 
: ( , GL . GL_RGB8 ) , \n 
: ( , GL . GL_RGBA8 ) , \n 
: ( , GL . GL_RGBA32F ) , \n 
: ( , GL . GL_RGBA32I ) , \n 
from PIL import Image \n 
if self . _target != GL . GL_TEXTURE_2D : \n 
~~ if image . format == : \n 
~~~ image = image . convert ( ) \n 
~~ if image . mode in [ , , ] : \n 
~~ dtype , internal_format = types [ image . mode ] \n 
swizzle = None \n 
if gl . is_legacy ( ) : \n 
~~~ format = legacy_format ( image . mode ) \n 
~~~ format = core_format ( image . mode ) \n 
swizzles = { \n 
if image . mode in swizzles : \n 
~~~ swizzle = swizzles [ image . mode ] \n 
~~ ~~ if flip : \n 
~~~ image = image . transpose ( Image . FLIP_TOP_BOTTOM ) \n 
~~ data = numpy . array ( image . getdata ( ) , dtype = dtype ) \n 
data . shape = ( image . size [ 0 ] , image . size [ 1 ] , - 1 ) \n 
self . allocate ( image . size , dtype , internal_format ) \n 
self . set_data ( \n 
data = data , \n 
shape = image . size , \n 
format = format , \n 
level = level , \n 
swizzle = swizzle , \n 
~~ ~~ class Texture1D ( Texture ) : \n 
~~~ super ( Texture1D , self ) . __init__ ( target = GL . GL_TEXTURE_1D , * args , ** kwargs ) \n 
def width ( self ) : \n 
~~~ return self . _shape [ 0 ] \n 
~~ ~~ class Texture2D ( Texture ) : \n 
~~~ super ( Texture2D , self ) . __init__ ( target = GL . GL_TEXTURE_2D , * args , ** kwargs ) \n 
def height ( self ) : \n 
~~~ return self . _shape [ 1 ] \n 
~~ ~~ class Texture3D ( Texture ) : \n 
~~~ super ( Texture3D , self ) . __init__ ( target = GL . GL_TEXTURE_3D , * args , ** kwargs ) \n 
def depth ( self ) : \n 
~~~ return self . _shape [ 2 ] \n 
from multipledispatch import dispatch \n 
from . base import BaseObject , BaseQuaternion , BaseMatrix , BaseVector , NpProxy \n 
from . . import quaternion \n 
class Quaternion ( BaseQuaternion ) : \n 
~~~ _module = quaternion \n 
_shape = ( 4 , ) \n 
x = NpProxy ( 0 ) \n 
y = NpProxy ( 1 ) \n 
z = NpProxy ( 2 ) \n 
w = NpProxy ( 3 ) \n 
xy = NpProxy ( [ 0 , 1 ] ) \n 
xyz = NpProxy ( [ 0 , 1 , 2 ] ) \n 
xyzw = NpProxy ( [ 0 , 1 , 2 , 3 ] ) \n 
xz = NpProxy ( [ 0 , 2 ] ) \n 
xzw = NpProxy ( [ 0 , 2 , 3 ] ) \n 
xyw = NpProxy ( [ 0 , 1 , 3 ] ) \n 
xw = NpProxy ( [ 0 , 3 ] ) \n 
######################## \n 
def from_x_rotation ( cls , theta , dtype = None ) : \n 
return cls ( quaternion . create_from_x_rotation ( theta , dtype ) ) \n 
def from_y_rotation ( cls , theta , dtype = None ) : \n 
return cls ( quaternion . create_from_y_rotation ( theta , dtype ) ) \n 
def from_z_rotation ( cls , theta , dtype = None ) : \n 
return cls ( quaternion . create_from_z_rotation ( theta , dtype ) ) \n 
def from_axis_rotation ( cls , axis , theta , dtype = None ) : \n 
return cls ( quaternion . create_from_axis_rotation ( axis , theta , dtype ) ) \n 
def from_matrix ( cls , matrix , dtype = None ) : \n 
return cls ( quaternion . create_from_matrix ( matrix , dtype ) ) \n 
def from_eulers ( cls , eulers , dtype = None ) : \n 
return cls ( quaternion . create_from_eulers ( eulers , dtype ) ) \n 
def from_inverse_of_eulers ( cls , eulers , dtype = None ) : \n 
return cls ( quaternion . create_from_inverse_of_eulers ( eulers , dtype ) ) \n 
~~ def __new__ ( cls , value = None , dtype = None ) : \n 
~~~ obj = value \n 
if not isinstance ( value , np . ndarray ) : \n 
~~~ obj = np . array ( value , dtype = dtype ) \n 
~~ if obj . shape in ( ( 4 , 4 , ) , ( 3 , 3 , ) ) or isinstance ( obj , ( Matrix33 , Matrix44 ) ) : \n 
~~~ obj = quaternion . create_from_matrix ( obj , dtype = dtype ) \n 
~~~ obj = quaternion . create ( dtype = dtype ) \n 
~~ obj = obj . view ( cls ) \n 
return super ( Quaternion , cls ) . __new__ ( cls , obj ) \n 
~~ @ dispatch ( BaseObject ) \n 
def __add__ ( self , other ) : \n 
~~~ self . _unsupported_type ( , other ) \n 
def __sub__ ( self , other ) : \n 
def __mul__ ( self , other ) : \n 
def __truediv__ ( self , other ) : \n 
def __div__ ( self , other ) : \n 
~~ @ dispatch ( ( BaseQuaternion , np . ndarray , list ) ) \n 
~~~ return Quaternion ( super ( Quaternion , self ) . __sub__ ( other ) ) \n 
~~ @ dispatch ( ( BaseQuaternion , list ) ) \n 
~~~ return self . cross ( other ) \n 
def __or__ ( self , other ) : \n 
~~~ return self . dot ( other ) \n 
~~ def __invert__ ( self ) : \n 
~~~ return self . conjugate \n 
~~ @ dispatch ( BaseMatrix ) \n 
~~~ return self * Quaternion ( other ) \n 
~~ @ dispatch ( BaseVector ) \n 
~~~ return type ( other ) ( quaternion . apply_to_vector ( self , other ) ) \n 
def length ( self ) : \n 
return quaternion . length ( self ) \n 
~~ def normalise ( self ) : \n 
self [ : ] = quaternion . normalise ( self ) \n 
def normalised ( self ) : \n 
return Quaternion ( quaternion . normalise ( self ) ) \n 
def angle ( self ) : \n 
return quaternion . rotation_angle ( self ) \n 
def axis ( self ) : \n 
return Vector3 ( quaternion . rotation_axis ( self ) ) \n 
~~ def cross ( self , other ) : \n 
return Quaternion ( quaternion . cross ( self , other ) ) \n 
~~ def dot ( self , other ) : \n 
return quaternion . dot ( self , other ) \n 
def conjugate ( self ) : \n 
return Quaternion ( quaternion . conjugate ( self ) ) \n 
def inverse ( self ) : \n 
return Quaternion ( quaternion . inverse ( self ) ) \n 
~~ def power ( self , exponent ) : \n 
return Quaternion ( quaternion . power ( self , exponent ) ) \n 
def negative ( self ) : \n 
return Quaternion ( quaternion . negate ( self ) ) \n 
def is_identity ( self ) : \n 
return quaternion . is_identity ( self ) \n 
def matrix44 ( self ) : \n 
return Matrix44 . from_quaternion ( self ) \n 
def matrix33 ( self ) : \n 
return Matrix33 . from_quaternion ( self ) \n 
~~ ~~ from . vector3 import Vector3 \n 
from . matrix33 import Matrix33 \n 
from . matrix44 import Matrix44 \n 
~~ import numpy as np \n 
from pyrr import quaternion \n 
class test_quaternion ( unittest . TestCase ) : \n 
~~~ def test_import ( self ) : \n 
~~~ import pyrr \n 
pyrr . quaternion \n 
~~ def test_create ( self ) : \n 
~~~ result = quaternion . create ( ) \n 
np . testing . assert_almost_equal ( result , [ 0. , 0. , 0. , 1. ] , decimal = 5 ) \n 
self . assertTrue ( result . dtype == np . float ) \n 
~~ def test_create_parameters ( self ) : \n 
~~~ result = quaternion . create ( 1.0 , 2.0 , 3.0 , 4.0 ) \n 
np . testing . assert_almost_equal ( result , [ 1.0 , 2.0 , 3.0 , 4.0 ] , decimal = 5 ) \n 
~~ def test_create_from_x_rotation ( self ) : \n 
~~~ q = quaternion . create_from_x_rotation ( np . pi ) \n 
self . assertTrue ( np . allclose ( q , [ 1. , 0. , 0. , 0. ] ) ) \n 
q = quaternion . create_from_x_rotation ( np . pi / 2. ) \n 
self . assertTrue ( np . allclose ( q , [ np . sqrt ( 0.5 ) , 0. , 0. , np . sqrt ( 0.5 ) ] ) ) \n 
q = quaternion . create_from_x_rotation ( - np . pi / 2. ) \n 
self . assertTrue ( np . allclose ( q , [ - np . sqrt ( 0.5 ) , 0. , 0. , np . sqrt ( 0.5 ) ] ) ) \n 
~~ def test_create_from_y_rotation ( self ) : \n 
~~~ q = quaternion . create_from_y_rotation ( np . pi ) \n 
self . assertTrue ( np . allclose ( q , [ 0. , 1. , 0. , 0. ] ) ) \n 
q = quaternion . create_from_y_rotation ( np . pi / 2. ) \n 
self . assertTrue ( np . allclose ( q , [ 0. , np . sqrt ( 0.5 ) , 0. , np . sqrt ( 0.5 ) ] ) ) \n 
q = quaternion . create_from_y_rotation ( - np . pi / 2. ) \n 
~~ def test_create_from_z_rotation ( self ) : \n 
~~~ q = quaternion . create_from_z_rotation ( np . pi ) \n 
self . assertTrue ( np . allclose ( q , [ 0. , 0. , 1. , 0. ] ) ) \n 
q = quaternion . create_from_z_rotation ( np . pi / 2. ) \n 
self . assertTrue ( np . allclose ( q , [ 0. , 0. , np . sqrt ( 0.5 ) , np . sqrt ( 0.5 ) ] ) ) \n 
q = quaternion . create_from_z_rotation ( - np . pi / 2. ) \n 
~~ def test_create_from_axis_rotation ( self ) : \n 
~~~ result = quaternion . create_from_axis_rotation ( [ 0.57735 , 0.57735 , 0.57735 ] , np . pi ) \n 
np . testing . assert_almost_equal ( result , [ 5.77350000e-01 , 5.77350000e-01 , 5.77350000e-01 , 6.12323400e-17 self . assertTrue ( result . dtype == np . float ) \n 
~~ def test_create_from_axis_rotation_non_normalised ( self ) : \n 
~~~ result = quaternion . create_from_axis_rotation ( [ 1. , 1. , 1. ] , np . pi ) \n 
~~ def test_create_from_matrix_unit ( self ) : \n 
~~~ result = quaternion . create_from_matrix ( np . eye ( 3 ) ) \n 
~~ def test_create_from_matrix_x ( self ) : \n 
~~~ result = quaternion . create_from_matrix ( [ \n 
[ 1. , 0. , 0. ] , \n 
[ 0. , - 1. , 0. ] , \n 
[ 0. , 0. , - 1. ] , \n 
np . testing . assert_almost_equal ( result , [ 1. , 0. , 0. , 0. ] , decimal = 5 ) \n 
~~ def test_create_from_matrix_y ( self ) : \n 
[ - 1. , 0. , 0. ] , \n 
[ 0. , 1. , 0. ] , \n 
np . testing . assert_almost_equal ( result , [ 0. , 1. , 0. , 0. ] , decimal = 5 ) \n 
~~ def test_create_from_matrix_z ( self ) : \n 
[ 0. , 0. , 1. ] , \n 
np . testing . assert_almost_equal ( result , [ 0. , 0. , 1. , 0. ] , decimal = 5 ) \n 
~~ @ unittest . skip ( ) \n 
def test_create_from_eulers ( self ) : \n 
def test_create_from_inverse_of_eulers ( self ) : \n 
~~ def test_cross ( self ) : \n 
~~~ q1 = quaternion . create_from_x_rotation ( np . pi / 2.0 ) \n 
q2 = quaternion . create_from_x_rotation ( - np . pi / 2.0 ) \n 
result = quaternion . cross ( q1 , q2 ) \n 
np . testing . assert_almost_equal ( result , quaternion . create ( ) , decimal = 5 ) \n 
~~ def test_is_zero_length ( self ) : \n 
~~~ result = quaternion . is_zero_length ( [ 1. , 0. , 0. , 0. ] ) \n 
self . assertFalse ( result ) \n 
~~ def test_is_zero_length_zero ( self ) : \n 
~~~ result = quaternion . is_zero_length ( [ 0. , 0. , 0. , 0. ] ) \n 
self . assertTrue ( result ) \n 
~~ def test_is_non_zero_length ( self ) : \n 
~~~ result = quaternion . is_non_zero_length ( [ 1. , 0. , 0. , 0. ] ) \n 
~~ def test_is_non_zero_length_zero ( self ) : \n 
~~~ result = quaternion . is_non_zero_length ( [ 0. , 0. , 0. , 0. ] ) \n 
~~ def test_squared_length_identity ( self ) : \n 
~~~ result = quaternion . squared_length ( [ 0. , 0. , 0. , 1. ] ) \n 
np . testing . assert_almost_equal ( result , 1. , decimal = 5 ) \n 
~~ def test_squared_length ( self ) : \n 
~~~ result = quaternion . squared_length ( [ 1. , 1. , 1. , 1. ] ) \n 
np . testing . assert_almost_equal ( result , 4. , decimal = 5 ) \n 
~~ def test_squared_length_batch ( self ) : \n 
~~~ result = quaternion . squared_length ( [ \n 
[ 0. , 0. , 0. , 1. ] , \n 
[ 1. , 1. , 1. , 1. ] , \n 
np . testing . assert_almost_equal ( result , [ 1. , 4. ] , decimal = 5 ) \n 
~~ def test_length_identity ( self ) : \n 
~~~ result = quaternion . length ( [ 0. , 0. , 0. , 1. ] ) \n 
~~ def test_length ( self ) : \n 
~~~ result = quaternion . length ( [ 1. , 1. , 1. , 1. ] ) \n 
np . testing . assert_almost_equal ( result , 2. , decimal = 5 ) \n 
~~ def test_length_batch ( self ) : \n 
~~~ result = quaternion . length ( [ \n 
np . testing . assert_almost_equal ( result , [ 1. , 2. ] , decimal = 5 ) \n 
~~ def test_normalise_identity ( self ) : \n 
~~~ result = quaternion . normalise ( [ 0. , 0. , 0. , 1. ] ) \n 
~~ def test_normalise_non_identity ( self ) : \n 
~~~ result = quaternion . normalise ( [ 1. , 2. , 3. , 4. ] ) \n 
np . testing . assert_almost_equal ( result , [ 1. / np . sqrt ( 30. ) , np . sqrt ( 2. / 15. ) , np . sqrt ( 3. / 10. \n 
~~ def test_normalise_batch ( self ) : \n 
~~~ result = quaternion . normalise ( [ \n 
[ 1. , 2. , 3. , 4. ] , \n 
[ 1. / np . sqrt ( 30. ) , np . sqrt ( 2. / 15. ) , np . sqrt ( 3. / 10. ) , 2. * np . sqrt ( 2. / 15. ) ] , \n 
np . testing . assert_almost_equal ( result , expected , decimal = 5 ) \n 
~~ def test_rotation_angle ( self ) : \n 
~~~ result = quaternion . rotation_angle ( [ 5.77350000e-01 , 5.77350000e-01 , 5.77350000e-01 , 6.12323400e-17 np . testing . assert_almost_equal ( result , np . pi , decimal = 5 ) \n 
~~ def test_rotation_axis ( self ) : \n 
~~~ result = quaternion . rotation_axis ( [ 5.77350000e-01 , 5.77350000e-01 , 5.77350000e-01 , 6.12323400e-17 np . testing . assert_almost_equal ( result , [ 0.57735 , 0.57735 , 0.57735 ] , decimal = 5 ) \n 
~~ def test_dot_adjacent ( self ) : \n 
~~~ result = quaternion . dot ( [ 1. , 0. , 0. , 0. ] , [ 0. , 1. , 0. , 0. ] ) \n 
np . testing . assert_almost_equal ( result , 0.0 , decimal = 5 ) \n 
~~ def test_dot_parallel ( self ) : \n 
~~~ result = quaternion . dot ( [ 0. , 1. , 0. , 0. ] , [ 0. , 1. , 0. , 0. ] ) \n 
np . testing . assert_almost_equal ( result , 1.0 , decimal = 5 ) \n 
~~ def test_dot_angle ( self ) : \n 
~~~ result = quaternion . dot ( [ .2 , .2 , 0. , 0. ] , [ 2. , - .2 , 0. , 0. ] ) \n 
np . testing . assert_almost_equal ( result , 0.36 , decimal = 5 ) \n 
~~ def test_dot_batch ( self ) : \n 
~~~ result = quaternion . dot ( [ \n 
[ 1. , 0. , 0. , 0. ] , \n 
[ 0. , 1. , 0. , 0. ] , \n 
[ .2 , .2 , 0. , 0. ] \n 
] , [ \n 
[ 2. , - .2 , 0. , 0. ] \n 
expected = [ 0. , 1. , 0.36 ] \n 
~~ def test_conjugate ( self ) : \n 
~~ def test_conjugate_rotation ( self ) : \n 
~~~ result = quaternion . conjugate ( [ 5.77350000e-01 , 5.77350000e-01 , 5.77350000e-01 , 6.12323400e-17 np . testing . assert_almost_equal ( result , [ - 0.57735 , - 0.57735 , - 0.57735 , 6.12323e-17 ] , decimal = \n 
def test_power ( self ) : \n 
~~ def test_inverse ( self ) : \n 
~~~ result = quaternion . inverse ( [ 0. , 0. , 0. , 1. ] ) \n 
~~ def test_inverse_rotation ( self ) : \n 
~~~ result = quaternion . inverse ( [ 5.77350000e-01 , 5.77350000e-01 , 5.77350000e-01 , 6.12323400e-17 ] np . testing . assert_almost_equal ( result , [ - 0.577351 , - 0.577351 , - 0.577351 , 6.12324e-17 ] , decimal \n 
~~ def test_inverse_non_unit ( self ) : \n 
~~~ q = [ 1 , 2 , 3 , 4 ] \n 
result = quaternion . inverse ( q ) \n 
expected = quaternion . conjugate ( q ) / quaternion . length ( q ) \n 
~~ def test_negate_unit ( self ) : \n 
~~~ result = quaternion . negate ( [ 0. , 0. , 0. , 1. ] ) \n 
np . testing . assert_almost_equal ( result , [ 0. , 0. , 0. , - 1. ] , decimal = 5 ) \n 
~~ def test_negate ( self ) : \n 
~~~ result = quaternion . negate ( [ 1. , 2. , 3. , 4. ] ) \n 
np . testing . assert_almost_equal ( result , [ - 1. , - 2. , - 3. , - 4. ] , decimal = 5 ) \n 
~~ def test_apply_to_vector_unit_x ( self ) : \n 
~~~ result = quaternion . apply_to_vector ( [ 0. , 0. , 0. , 1. ] , [ 1. , 0. , 0. ] ) \n 
np . testing . assert_almost_equal ( result , [ 1. , 0. , 0. ] , decimal = 5 ) \n 
~~ def test_apply_to_vector_x ( self ) : \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 1. , 0. , 0. ] ) , [ 1. , 0. , 0. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 1. , 0. ] ) , [ 0. , - 1. , 0. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 0. , 1. ] ) , [ 0. , 0. , - 1. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 1. , 0. ] ) , [ 0. , 0. , 1. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 0. , 1. ] ) , [ 0. , - 1. , 0. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 1. , 0. ] ) , [ 0. , 0. , - 1. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 0. , 1. ] ) , [ 0. , 1. , 0. ] ) ) \n 
~~ def test_apply_to_vector_y ( self ) : \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 1. , 0. , 0. ] ) , [ - 1. , 0. , 0. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 1. , 0. ] ) , [ 0. , 1. , 0. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 1. , 0. , 0. ] ) , [ 0. , 0. , - 1. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 0. , 1. ] ) , [ 1. , 0. , 0. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 1. , 0. , 0. ] ) , [ 0. , 0. , 1. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 0. , 1. ] ) , [ - 1. , 0. , 0. ] ) ) \n 
~~ def test_apply_to_vector_z ( self ) : \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 0. , 1. ] ) , [ 0. , 0. , 1. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 1. , 0. , 0. ] ) , [ 0. , 1. , 0. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 1. , 0. ] ) , [ - 1. , 0. , 0. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 1. , 0. , 0. ] ) , [ 0. , - 1. , 0. ] ) ) \n 
self . assertTrue ( np . allclose ( quaternion . apply_to_vector ( q , [ 0. , 1. , 0. ] ) , [ 1. , 0. , 0. ] ) ) \n 
~~ def test_identity ( self ) : \n 
~~~ i = quaternion . create ( 1. , 0. , 0. , 0. ) \n 
j = quaternion . create ( 0. , 1. , 0. , 0. ) \n 
k = quaternion . create ( 0. , 0. , 1. , 0. ) \n 
one = quaternion . create ( 0. , 0. , 0. , 1. ) \n 
i1 = quaternion . cross ( i , one ) \n 
j1 = quaternion . cross ( j , one ) \n 
k1 = quaternion . cross ( k , one ) \n 
_1i = quaternion . cross ( one , i ) \n 
_1j = quaternion . cross ( one , j ) \n 
_1k = quaternion . cross ( one , k ) \n 
self . assertTrue ( np . allclose ( i1 , _1i , i ) ) \n 
self . assertTrue ( np . allclose ( j1 , _1j , j ) ) \n 
self . assertTrue ( np . allclose ( k1 , _1k , k ) ) \n 
ii = quaternion . cross ( i , i ) \n 
kk = quaternion . cross ( k , k ) \n 
jj = quaternion . cross ( j , j ) \n 
ijk = quaternion . cross ( quaternion . cross ( i , j ) , k ) \n 
self . assertTrue ( np . allclose ( ii , - one ) ) \n 
self . assertTrue ( np . allclose ( jj , - one ) ) \n 
self . assertTrue ( np . allclose ( kk , - one ) ) \n 
self . assertTrue ( np . allclose ( ijk , - one ) ) \n 
ij = quaternion . cross ( i , j ) \n 
ji = quaternion . cross ( j , i ) \n 
jk = quaternion . cross ( j , k ) \n 
kj = quaternion . cross ( k , j ) \n 
ki = quaternion . cross ( k , i ) \n 
ik = quaternion . cross ( i , k ) \n 
self . assertTrue ( np . allclose ( ij , k ) ) \n 
self . assertTrue ( np . allclose ( ji , - k ) ) \n 
self . assertTrue ( np . allclose ( jk , i ) ) \n 
self . assertTrue ( np . allclose ( kj , - i ) ) \n 
self . assertTrue ( np . allclose ( ki , j ) ) \n 
self . assertTrue ( np . allclose ( ik , - j ) ) \n 
ijkk = quaternion . cross ( quaternion . cross ( ij , k ) , k ) \n 
ijk2 = quaternion . cross ( ij , quaternion . cross ( k , k ) ) \n 
ij_m1 = quaternion . cross ( ij , - one ) \n 
self . assertTrue ( np . allclose ( ijkk , ijk2 ) ) \n 
self . assertTrue ( np . allclose ( ijk2 , ij_m1 ) ) \n 
PLUGIN_VERSION = "2.0.0" \n 
class ViewInBrowserCommand ( sublime_plugin . TextCommand ) : \n 
~~~ _pythonVersion = sys . version_info [ 0 ] \n 
def expandWindowsUserShellFolder ( self , command ) : \n 
~~~ browserCommand = "" \n 
windowsFolders = self . getWindowsUserShellFolders ( ) \n 
specialFolder = re . sub ( r"%([A-Za-z\\s]+)%.*" , "\\\\1" , command ) \n 
if specialFolder != command : \n 
~~~ expandedFolder = windowsFolders [ specialFolder ] . replace ( "\\\\" , "\\\\\\\\" ) \n 
browserCommand = re . sub ( r"%[A-Za-z\\s]+%(.*)" , "%s\\\\1" % expandedFolder , command ) \n 
~~~ browserCommand = command \n 
~~ return browserCommand . encode ( "ascii" , "ignore" ) if self . _pythonVersion < 3 else browserCommand \n 
~~ def getBaseCommand ( self , command , osName ) : \n 
~~~ baseCommand = command \n 
if osName == "nt" : \n 
~~~ baseCommand = self . expandWindowsUserShellFolder ( baseCommand ) \n 
~~ return baseCommand \n 
~~ def getOsName ( self ) : \n 
~~~ return os . name \n 
~~ def getPlatform ( self ) : \n 
~~~ return sys . platform \n 
~~ def getWindowsUserShellFolders ( self ) : \n 
~~~ import _winreg \n 
~~~ import winreg as _winreg \n 
~~ return_dict = { } \n 
~~~ Hive = _winreg . ConnectRegistry ( None , _winreg . HKEY_CURRENT_USER ) \n 
return return_dict \n 
~~~ for i in range ( 0 , _winreg . QueryInfoKey ( Key ) [ 1 ] ) : \n 
~~~ name , value , val_type = _winreg . EnumValue ( Key , i ) \n 
return_dict [ name ] = value . encode ( "ascii" ) \n 
~~~ _winreg . CloseKey ( Key ) \n 
_winreg . CloseKey ( Hive ) \n 
~~ ~~ def giveFileAProjectPath ( self , fileToOpen , basePath , baseUrl ) : \n 
~~ def loadPluginSettings ( self , defaultBrowser ) : \n 
~~~ result = { \n 
"browser" : "" , \n 
"baseCommand" : "" \n 
if not defaultBrowser : \n 
~~~ result [ "browser" ] = settings . get ( "browser" ) \n 
~~~ result [ "browser" ] = defaultBrowser \n 
~~ osName = self . getOsName ( ) \n 
platform = self . getPlatform ( ) \n 
selectedOs = settings . get ( osName ) \n 
result [ "baseCommand" ] = self . getBaseCommand ( selectedOs [ platform ] [ result [ "browser" ] ] , osName ) \n 
~~ def loadProjectSettings ( self , view ) : \n 
~~~ return view . settings ( ) . get ( "sublime-view-in-browser" ) \n 
~~ def normalizePath ( self , fileToOpen ) : \n 
~~~ fileToOpen = fileToOpen . replace ( "\\\\" , "/" ) \n 
return fileToOpen \n 
~~ def openBrowser ( self , command , osName ) : \n 
~~~ useShell = False if osName != "posix" else True \n 
subprocess . Popen ( command , shell = useShell ) \n 
~~ def run ( self , edit , browser = None ) : \n 
pluginSettings = self . loadPluginSettings ( browser ) \n 
projectSettings = self . loadProjectSettings ( self . view ) \n 
fileToOpen = self . view . file_name ( ) \n 
if projectSettings : \n 
~~~ fileToOpen = self . giveFileAProjectPath ( fileToOpen , projectSettings [ "basePath" ] , projectSettings [ "baseUrl" \n 
~~ if fileToOpen == None : \n 
~~~ fileToOpen = self . normalizePath ( self . saveCurrentViewInTempFile ( self . view ) ) \n 
~~~ if self . view . is_dirty ( ) : \n 
self . view . window ( ) . run_command ( "save" ) \n 
~~ if not projectSettings : \n 
~~~ fileToOpen = self . normalizePath ( fileToOpen ) \n 
~~ ~~ if pluginSettings [ "baseCommand" ] : \n 
print ( command ) \n 
self . openBrowser ( command , self . getOsName ( ) ) \n 
~~~ if self . _pythonVersion < 3 : \n 
~~~ webbrowser . open_new_tab ( fileToOpen . encode ( sys . getfilesystemencoding ( ) ) ) \n 
~~~ webbrowser . open_new_tab ( fileToOpen ) \n 
~~ ~~ ~~ def saveCurrentViewInTempFile ( self , view ) : \n 
~~~ tempFile = tempfile . NamedTemporaryFile ( suffix = ".htm" , delete = False ) \n 
tempFile . write ( text . encode ( "utf-8" ) ) \n 
tempFile . close ( ) \n 
return tempFile . name \n 
~~ ~~ from sellmo . core import chaining \n 
from sellmo . core . http . query import QueryString \n 
from sellmo . apps . product . routines import ( list_products_from_request , \n 
product_filters_from_request ) \n 
from sellmo . contrib . category . views import category \n 
from django . shortcuts import render \n 
from django . template . response import TemplateResponse \n 
from product . utils import paginate \n 
@ chaining . link ( category ) \n 
def _category ( request , category , ** kwargs ) : \n 
~~~ query = QueryString ( request ) \n 
products = list_products_from_request ( request , category = category , query = query ) \n 
if not category . is_leaf_node ( ) : \n 
~~~ root = category \n 
~~ elif category . is_child_node ( ) : \n 
~~~ root = category . parent \n 
~~ products = paginate ( request , products ) \n 
product_filters = product_filters_from_request ( request , products . object_list . facets ( ) , category = \n 
yield TemplateResponse ( request , , { \n 
: products , \n 
: product_filters , \n 
: root , \n 
: category , \n 
: query \n 
~~ from django . contrib import admin \n 
from sellmo . contrib . mailing . models import MailMessage \n 
class MailMessageAdmin ( admin . ModelAdmin ) : \n 
~~~ list_display = [ , , , \n 
list_display_links = [ ] \n 
~~ admin . site . register ( MailMessage , MailMessageAdmin ) \n 
from sellmo . contrib . variation . models import Variation \n 
class VariationAdmin ( admin . ModelAdmin ) : \n 
~~~ search_fields = [ ] \n 
list_display = [ , ] \n 
list_editable = [ ] \n 
def has_add_permission ( self , request ) : \n 
~~ ~~ admin . site . register ( Variation , VariationAdmin ) \n 
from sellmo . core import chaining \n 
from sellmo . apps . customer . routines import customer_from_request \n 
from . routines import completed_order_from_request \n 
@ chaining . link ( customer_from_request , takes_result = True ) \n 
def _customer_from_request ( customer , request , ** kwargs ) : \n 
~~~ order = completed_order_from_request ( request ) \n 
if order is not None : \n 
~~~ if customer is None : \n 
~~~ customer = Contactable . clone ( order , cls = Customer ) \n 
for address in order . addresses : \n 
~~~ if address . address_type not in customer . addresses : \n 
~~~ customer . addresses [ address . address_type ] = address . clone ( ) \n 
~~ ~~ yield customer \n 
customer = ( yield chaining . wait ) [ ] \n 
~~ customer . last_completed_order = order \n 
~~ ~~ from sellmo . core . registry import Module \n 
ModelsModule = Module . imports ( % __name__ ) \n 
IndexesModule = Module . imports ( % __name__ ) \n 
models = ModelsModule ( % __name__ ) \n 
indexes = IndexesModule ( % __name__ ) \n 
default_app_config = % __name__ \n 
from sellmo . core . apps import SellmoAppConfig \n 
class DefaultConfig ( SellmoAppConfig ) : \n 
~~ from sellmo . utils . forms import FormFactory \n 
from django . utils . translation import ugettext_lazy as _ \n 
from . models import Attribute \n 
class ProductAttributeFormMixin ( object ) : \n 
~~~ initial = { } \n 
if in kwargs : \n 
~~~ initial = kwargs [ ] \n 
~~ instance = None \n 
~~~ instance = kwargs [ ] \n 
~~~ for attribute in self . __attributes : \n 
~~~ if attribute in instance . attributes : \n 
~~~ field_name = self . __attribute_field_names [ attribute . key ] \n 
field = self . __attribute_fields [ attribute . key ] \n 
value = instance . attributes [ attribute ] \n 
initial [ field_name ] = value \n 
~~ ~~ ~~ kwargs [ ] = initial \n 
super ( ProductAttributeFormMixin , self ) . __init__ ( * args , ** kwargs ) \n 
~~ def save ( self , commit = True ) : \n 
~~~ instance = super ( ProductAttributeFormMixin , self ) . save ( commit = False ) \n 
for attribute in self . __attributes : \n 
value = self . cleaned_data . get ( field_name ) \n 
instance . attributes [ attribute . key ] = value \n 
~~ if commit : \n 
~~~ instance . save ( ) \n 
self . save_m2m ( ) \n 
~~ ~~ class ProductAttributeFormFactory ( FormFactory ) : \n 
~~~ def __init__ ( \n 
form = None , \n 
mixin = None , \n 
prefix = None , \n 
attribute_set = None , \n 
formfield_callback = None \n 
~~~ self . form = form or forms . ModelForm \n 
self . mixin = mixin or ProductAttributeFormMixin \n 
self . prefix = prefix \n 
self . attribute_set = attribute_set \n 
self . formfield_callback = formfield_callback \n 
~~ def get_attributes ( self ) : \n 
~~~ attributes = Attribute . objects . all ( ) \n 
if self . attribute_set is not None : \n 
~~~ attributes = ( \n 
attributes . for_attribute_set ( self . attribute_set ) \n 
| attributes . filter ( sets = None ) \n 
~~~ attributes = attributes . filter ( sets = None ) \n 
~~ return attributes \n 
~~ def get_attribute_formfield_names ( self ) : \n 
~~~ keys = self . get_attributes ( ) . values_list ( , flat = True ) \n 
names = [ ] \n 
~~~ if self . prefix : \n 
~~~ names . append ( % ( self . prefix , key ) ) \n 
~~~ names . append ( key ) \n 
~~ ~~ return names \n 
~~ def get_attribute_formfield_name ( self , attribute ) : \n 
~~~ return % ( self . prefix , attribute . key ) \n 
~~~ return attribute . key \n 
~~ ~~ def get_attribute_formfield ( self , attribute ) : \n 
~~~ typ = attribute . get_type ( ) \n 
choices = typ . get_choices ( attribute ) \n 
if choices is not None and not attribute . required : \n 
~~~ choices = [ ( typ . get_empty_value ( ) , ) ] + choices \n 
~~ field_cls , args , kwargs = typ . get_formfield ( \n 
label = attribute . name , \n 
required = attribute . required , \n 
choices = choices \n 
formfield = field_cls ( * args , ** kwargs ) \n 
return formfield \n 
~~ def factory ( self ) : \n 
~~~ attributes = self . get_attributes ( ) \n 
names = { } \n 
fields = { } \n 
attr_dict = { \n 
. format ( self . mixin . __name__ ) : attributes , \n 
. format ( self . mixin . __name__ ) : names , \n 
. format ( self . mixin . __name__ ) : fields , \n 
for attribute in attributes : \n 
~~~ formfield = self . get_attribute_formfield ( attribute ) \n 
if self . formfield_callback : \n 
~~~ formfield = self . formfield_callback ( attribute , formfield ) \n 
~~ name = self . get_attribute_formfield_name ( attribute ) \n 
names [ attribute . key ] = name \n 
fields [ attribute . key ] = formfield \n 
attr_dict [ name ] = formfield \n 
~~ return type ( , ( self . mixin , self . form ) , attr_dict ) \n 
from sellmo . core . loading import load \n 
import sellmo . contrib . checkout . mailing as _checkout_mailing \n 
@ load ( action = ) \n 
def finalize_model ( ) : \n 
~~~ class OrderMailMessage ( _checkout_mailing . models . OrderMailMessage ) : \n 
~~~ objects = ( \n 
_checkout_mailing . models . OrderMailMessageManager . from_queryset ( \n 
_checkout_mailing . models . OrderMailMessageQuerySet \n 
) ( ) \n 
class Meta ( _checkout_mailing . models . OrderMailMessage . Meta ) : \n 
~~~ app_label = \n 
~~ ~~ _checkout_mailing . models . OrderMailMessage = OrderMailMessage \n 
~~ class OrderMailMessageQuerySet ( models . QuerySet ) : \n 
~~ class OrderMailMessageManager ( models . Manager ) : \n 
~~ class OrderMailMessage ( models . Model ) : \n 
~~~ order = models . ForeignKey ( ) \n 
mail_message = models . ForeignKey ( ) \n 
~~~ return unicode ( self . mail_message ) \n 
~~ ~~ from django . utils . translation import ugettext_lazy as _ \n 
from . price import DiscountPriceComponent \n 
DISCOUNT = DiscountPriceComponent ( ) \n 
APPLIES_TO_PRODUCT_PRICE = \n 
APPLIES_TO_SHIPPING_COSTS = \n 
APPLIES_TO_TOTAL = \n 
APPLIES_TO_CHOICES = ( \n 
NO_MUTATION = object ( ) \n 
import sellmo . apps . checkout as _checkout \n 
@ load ( before = ) \n 
def load_model ( ) : \n 
~~~ class Order ( _checkout . models . Order ) : \n 
~~~ _mutated_payment = NO_MUTATION \n 
def invalidate ( self ) : \n 
~~~ super ( Order , self ) . invalidate ( ) \n 
self . payment_method = None \n 
def payment ( self ) : \n 
~~~ if self . _mutated_payment is not NO_MUTATION : \n 
~~~ return self . _mutated_payment \n 
~~ return getattr ( self , , None ) \n 
~~ def get_payment_method ( self ) : \n 
~~~ if self . payment is not None : \n 
~~~ from . method import PaymentMethod \n 
return PaymentMethod . from_payment ( self . payment ) \n 
~~ ~~ def set_payment_method ( self , value ) : \n 
~~~ payment = None \n 
if value is not None : \n 
~~~ payment = value . make_payment ( self ) \n 
~~ self . _mutated_payment = payment \n 
~~ payment_method = property ( get_payment_method , set_payment_method ) \n 
~~~ super ( Order , self ) . save ( * args , ** kwargs ) \n 
if self . _mutated_payment is not NO_MUTATION : \n 
~~~ if hasattr ( self , ) : \n 
~~~ self . order_payment . delete ( ) \n 
self . order_payment . order = None \n 
~~ if self . _mutated_payment is not None : \n 
~~~ self . _mutated_payment . order = self \n 
self . _mutated_payment . save ( ) \n 
~~ self . _mutated_payment = NO_MUTATION \n 
~~ ~~ class Meta ( _checkout . models . Order . Meta ) : \n 
~~ ~~ _checkout . models . Order = Order \n 
~~ from sellmo . core . apps import SellmoAppConfig \n 
~~ from weasyprint import HTML \n 
from sellmo . contrib . reporting . generators import AbstractReportGenerator \n 
logger . handlers = [ ] \n 
class WeasyPrintReportGenerator ( AbstractReportGenerator ) : \n 
def input_formats ( self ) : \n 
def output_formats ( self ) : \n 
~~ def get_data ( self , writer , frmt ) : \n 
~~~ html = super ( WeasyPrintReportGenerator , self ) . get_data ( writer , frmt ) \n 
return HTML ( string = html ) . write_pdf ( ) \n 
~~ def get_extension ( self , frmt ) : \n 
~~~ return + frmt \n 
~~ def get_mimetype ( self , frmt ) : \n 
~~~ if frmt == : \n 
~~ ~~ ~~ from django . db import models \n 
from sellmo . contrib . settings import settings_manager \n 
from . constants import MAX_TIER_ATTRIBUTES \n 
for i in range ( MAX_TIER_ATTRIBUTES ) : \n 
~~~ settings_manager . add_setting ( \n 
. format ( i + 1 ) , \n 
models . ForeignKey ( \n 
null = True , \n 
related_name = \n 
group \n 
~~ from sellmo . utils . text import underscore_concat \n 
from sellmo . contrib . attribute . helpers import ( \n 
ProductAttributeHelper as _ProductAttributeHelper \n 
from sellmo . contrib . attribute . helpers import AttributeHelper \n 
from django . core . exceptions import ValidationError \n 
from django . utils . functional import cached_property \n 
from django . utils . encoding import smart_text \n 
from django . utils . text import capfirst \n 
import sellmo . contrib . attribute as _attribute \n 
def differs_field_name ( field_name ) : \n 
~~~ return underscore_concat ( field_name , ) \n 
~~ class ProductAttributeHelper ( _ProductAttributeHelper ) : \n 
~~~ def get_values_queryset ( self ) : \n 
~~~ values = super ( ProductAttributeHelper , self ) . get_values_queryset ( ) \n 
values = values . filter ( variates = False ) \n 
return values \n 
~~ def get_new_value ( self ) : \n 
~~~ value = super ( ProductAttributeHelper , self ) . get_new_value ( ) \n 
value . variates = False \n 
return value \n 
~~ ~~ class VariantAttributeHelper ( ProductAttributeHelper ) : \n 
~~~ def get_values ( self ) : \n 
~~~ variant_values = { \n 
value . attribute . key : value \n 
for value in super ( VariantAttributeHelper , self ) . get_values ( ) \n 
product_values = { \n 
for value in self . _product . product . attributes . get_values ( ) \n 
return list ( six . itervalues ( dict ( product_values , ** variant_values ) ) ) \n 
~~ def get_value ( self , key ) : \n 
~~~ value = super ( VariantAttributeHelper , self ) . get_value ( key ) \n 
~~~ value = self . _product . product . attributes . get_value ( key ) \n 
~~ def set_value_value ( self , key , value_value ) : \n 
~~~ attribute = self . get_attribute ( key ) \n 
product_value = self . _product . product . attributes . get_value ( key ) \n 
variant_value = self . get_own_value ( key ) \n 
if ( \n 
not attribute . variates and product_value is not None and \n 
product_value . value == value_value \n 
~~~ if variant_value : \n 
~~~ variant_value . value = None \n 
~~~ super ( VariantAttributeHelper , self ) . set_value_value ( \n 
key , value_value \n 
~~ ~~ ~~ class VariationAttributeHelper ( AttributeHelper ) : \n 
~~~ def __init__ ( self , variation ) : \n 
~~~ super ( VariationAttributeHelper , self ) . __init__ ( ) \n 
self . _variation = variation \n 
~~ @ cached_property \n 
def _product ( self ) : \n 
~~~ if hasattr ( self . _variation , ) : \n 
~~~ return self . _variation . product . downcast ( ) \n 
~~ ~~ def get_values ( self ) : \n 
~~~ variation_values = { \n 
for value in self . _variation . values . all ( ) \n 
variant_values = { } \n 
if self . _product : \n 
for value in self . _product . attributes . get_values ( ) \n 
~~ return list ( six . itervalues ( dict ( variant_values , ** variation_values ) ) ) \n 
~~~ value = self . _variation . values . get ( attribute = attribute ) \n 
~~ except _attribute . models . Value . DoesNotExist : \n 
~~~ if self . _product : \n 
~~~ value = self . _product . attributes . get_value ( key ) \n 
~~ ~~ from sellmo . core . indexing . adapters import AbstractIndexAdapter \n 
from . tasks import sync_index , build_index \n 
class CeleryIndexAdapterWrapper ( AbstractIndexAdapter ) : \n 
~~~ def sync_index ( self , index , documents , full = False ) : \n 
sync_index . apply_async ( ( index , documents , full ) ) \n 
~~ def build_index ( \n 
index , \n 
added_fields = None , \n 
deleted_fields = None , \n 
changed_fields = None , \n 
exists = False \n 
~~~ build_index . apply_async ( ( index , ) ) \n 
def wrap ( cls , adapter ) : \n 
~~~ wrapped = type ( \n 
% adapter . __name__ , ( cls , adapter ) , { } \n 
return wrapped \n 
from types import ModuleType \n 
from importlib import import_module \n 
from django . apps import apps \n 
from . exceptions import RegistryError \n 
allowed = None \n 
def should_fix_type ( typ ) : \n 
~~~ global allowed \n 
if allowed is None : \n 
~~~ allowed = [ ] \n 
for app in apps . get_app_configs ( ) : \n 
~~~ allowed . append ( app . name ) \n 
~~ ~~ return any ( typ . __module__ . startswith ( name ) for name in allowed ) \n 
~~ class ModuleAttribute ( object ) : \n 
~~~ def __init__ ( self , module_name , name ) : \n 
~~~ self . module_name = module_name \n 
self . _accessed = False \n 
self . _access_traceback = None \n 
self . _injection_handlers = [ ] \n 
~~ def inject ( self , handler ) : \n 
~~~ self . _injection_handlers . insert ( 0 , handler ) \n 
def is_accesed ( self ) : \n 
~~~ return self . _accessed \n 
def is_assigned ( self ) : \n 
~~~ return hasattr ( self , ) \n 
~~ def access ( self ) : \n 
~~~ if not self . _accessed : \n 
~~~ if getattr ( settings , , False ) : \n 
~~~ self . _access_traceback = traceback . extract_stack ( ) \n 
~~ value = self . _value \n 
for handler in self . _injection_handlers : \n 
~~~ value = handler ( value ) \n 
~~~ raise RegistryError , ex , sys . exc_info ( ) [ 2 ] \n 
~~ ~~ if isinstance ( value , type ) and should_fix_type ( value ) : \n 
~~~ value . __module__ = self . module_name \n 
~~ self . _accessed = True \n 
self . _value = value \n 
~~ return self . _value \n 
~~ def assign ( self , value ) : \n 
~~~ if self . is_accesed : \n 
~~~ filename , line , name , text = , , , \n 
if self . _access_traceback is not None : \n 
~~~ filename , line , name , text = self . _access_traceback [ - 3 ] \n 
~~ raise RegistryError ( \n 
filename , \n 
line , \n 
text ) ) \n 
~~ self . _value = value \n 
~~ ~~ class BaseModule ( ModuleType ) : \n 
~~~ _imports = None \n 
_imported_attrs = None \n 
_modules = { } \n 
def __new__ ( cls , fullname ) : \n 
~~~ if fullname in cls . _modules : \n 
~~ module = super ( BaseModule , cls ) . __new__ ( cls , fullname ) \n 
cls . _modules [ fullname ] = module \n 
return module \n 
~~ def __init__ ( self , fullname ) : \n 
~~~ super ( BaseModule , self ) . __init__ ( fullname ) \n 
self . _attrs = { } \n 
~~ def _import ( self ) : \n 
~~~ if self . _imported_attrs is None : \n 
~~~ self . _imported_attrs = { } \n 
if self . _imports : \n 
~~~ module = import_module ( self . _imports ) \n 
self . _imported_attrs . update ( \n 
** { \n 
name : value \n 
for name , value in six . iteritems ( vars ( module ) ) \n 
if not name . startswith ( ) \n 
~~ ~~ ~~ def __getattribute__ ( self , name ) : \n 
~~~ if name . startswith ( ) : \n 
~~~ return super ( BaseModule , self ) . __getattribute__ ( name ) \n 
~~ ~~ elif not self [ name ] . is_assigned : \n 
~~~ value = super ( BaseModule , self ) . __getattribute__ ( name ) \n 
~~~ self . _import ( ) \n 
if name in self . _imported_attrs : \n 
~~~ self [ name ] . assign ( self . _imported_attrs [ name ] ) \n 
~~~ self [ name ] . assign ( value ) \n 
~~ ~~ if self [ name ] . is_assigned : \n 
~~~ return self [ name ] . access ( ) \n 
~~ raise AttributeError ( name ) \n 
def __all__ ( self ) : \n 
~~~ names = set ( ) \n 
for name in six . iterkeys ( self . _attrs ) : \n 
~~~ names . add ( name ) \n 
~~ for name in six . iterkeys ( self . _base_attrs ) : \n 
~~ return list ( names ) \n 
~~ def __setattr__ ( self , name , value ) : \n 
~~~ self . __dict__ [ name ] = value \n 
~~ ~~ def __getitem__ ( self , name ) : \n 
~~~ if name not in self . _attrs : \n 
~~~ self . _attrs [ name ] = ModuleAttribute ( self . __name__ , name ) \n 
~~ return self . _attrs [ name ] \n 
def imports ( cls , module ) : \n 
~~~ return type ( , ( cls , ) , { : module } ) \n 
def find_module ( cls , fullname , path = None ) : \n 
~~~ return cls \n 
~~ ~~ @ classmethod \n 
def load_module ( cls , fullname ) : \n 
~~~ module = cls . _modules [ fullname ] \n 
module . __loader__ = cls \n 
module . __file__ = "<%s>" % cls . __name__ \n 
module . __package__ = fullname . rpartition ( ) [ 0 ] \n 
module . _import ( ) \n 
sys . modules . setdefault ( fullname , module ) \n 
~~ from sellmo import modules \n 
class ColorAdmin ( admin . ModelAdmin ) : \n 
~~~ list_display = [ , ] \n 
~~ admin . site . register ( modules . color . Color , ColorAdmin ) \n 
BUILDBOT_DIR = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
TRUNK_DIR = os . path . dirname ( BUILDBOT_DIR ) \n 
ROOT_DIR = os . path . dirname ( TRUNK_DIR ) \n 
CMAKE_DIR = os . path . join ( ROOT_DIR , ) \n 
CMAKE_BIN_DIR = os . path . join ( CMAKE_DIR , ) \n 
OUT_DIR = os . path . join ( TRUNK_DIR , ) \n 
def CallSubProcess ( * args , ** kwargs ) : \n 
with open ( os . devnull ) as devnull_fd : \n 
~~~ retcode = subprocess . call ( stdin = devnull_fd , * args , ** kwargs ) \n 
~~ if retcode != 0 : \n 
~~ ~~ def PrepareCmake ( ) : \n 
if os . environ [ ] == : \n 
shutil . rmtree ( CMAKE_DIR ) \n 
~~ if os . path . isdir ( CMAKE_DIR ) : \n 
os . mkdir ( CMAKE_DIR ) \n 
CallSubProcess ( \n 
[ , , \n 
CMAKE_DIR ] , \n 
cwd = CMAKE_DIR ) \n 
[ , , % CMAKE_DIR ] , \n 
CallSubProcess ( [ , ] , cwd = CMAKE_DIR ) \n 
~~ def GypTestFormat ( title , format = None , msvs_version = None , tests = [ ] ) : \n 
if not format : \n 
~~~ format = title \n 
~~ print + title + \n 
env = os . environ . copy ( ) \n 
if msvs_version : \n 
~~~ env [ ] = msvs_version \n 
~~ command = . join ( \n 
[ sys . executable , , \n 
, format , \n 
, CMAKE_BIN_DIR , \n 
, ] + tests ) \n 
retcode = subprocess . call ( command , cwd = ROOT_DIR , env = env , shell = True ) \n 
if retcode : \n 
~~ def GypBuild ( ) : \n 
print % OUT_DIR \n 
shutil . rmtree ( OUT_DIR , ignore_errors = True ) \n 
retcode = 0 \n 
if sys . platform . startswith ( ) : \n 
~~~ retcode += GypTestFormat ( ) \n 
retcode += GypTestFormat ( ) \n 
PrepareCmake ( ) \n 
~~ elif sys . platform == : \n 
~~~ retcode += GypTestFormat ( , format = , \n 
msvs_version = , \n 
tests = [ \n 
retcode += GypTestFormat ( , format = , msvs_version = ) \n 
~~ if retcode : \n 
sys . exit ( retcode ) \n 
~~~ GypBuild ( ) \n 
~~ import filecmp \n 
import gyp . common \n 
import gyp . xcodeproj_file \n 
import gyp . xcode_ninja \n 
_intermediate_var = \n 
_shared_intermediate_var = \n 
_library_search_paths_var = \n 
generator_default_variables = { \n 
: % _intermediate_var , \n 
: % _shared_intermediate_var , \n 
generator_additional_path_sections = [ \n 
generator_additional_non_configuration_keys = [ \n 
generator_extra_sources_for_rules = [ \n 
xcode_standard_library_dirs = frozenset ( [ \n 
def CreateXCConfigurationList ( configuration_names ) : \n 
~~~ xccl = gyp . xcodeproj_file . XCConfigurationList ( { : [ ] } ) \n 
if len ( configuration_names ) == 0 : \n 
~~~ configuration_names = [ ] \n 
~~ for configuration_name in configuration_names : \n 
~~~ xcbc = gyp . xcodeproj_file . XCBuildConfiguration ( { \n 
: configuration_name } ) \n 
xccl . AppendProperty ( , xcbc ) \n 
~~ xccl . SetProperty ( , configuration_names [ 0 ] ) \n 
return xccl \n 
~~ class XcodeProject ( object ) : \n 
~~~ def __init__ ( self , gyp_path , path , build_file_dict ) : \n 
~~~ self . gyp_path = gyp_path \n 
self . project = gyp . xcodeproj_file . PBXProject ( path = path ) \n 
projectDirPath = gyp . common . RelativePath ( \n 
os . path . dirname ( os . path . abspath ( self . gyp_path ) ) , \n 
os . path . dirname ( path ) or ) \n 
self . project . SetProperty ( , projectDirPath ) \n 
self . project_file = gyp . xcodeproj_file . XCProjectFile ( { : self . project } ) \n 
self . build_file_dict = build_file_dict \n 
self . created_dir = False \n 
~~~ os . makedirs ( self . path ) \n 
self . created_dir = True \n 
~~~ if e . errno != errno . EEXIST : \n 
~~ ~~ ~~ def Finalize1 ( self , xcode_targets , serialize_all_tests ) : \n 
~~~ configurations = [ ] \n 
for xct in self . project . GetProperty ( ) : \n 
~~~ xccl = xct . GetProperty ( ) \n 
xcbcs = xccl . GetProperty ( ) \n 
for xcbc in xcbcs : \n 
~~~ name = xcbc . GetProperty ( ) \n 
if name not in configurations : \n 
~~~ configurations . append ( name ) \n 
~~~ xccl = CreateXCConfigurationList ( configurations ) \n 
self . project . SetProperty ( , xccl ) \n 
~~ xccl . SetBuildSetting ( _intermediate_var , \n 
xccl . SetBuildSetting ( _shared_intermediate_var , \n 
for xck , xcv in self . build_file_dict . get ( , { } ) . iteritems ( ) : \n 
~~~ xccl . SetBuildSetting ( xck , xcv ) \n 
~~ if in self . build_file_dict : \n 
~~~ config_ref = self . project . AddOrGetFileInRootGroup ( \n 
self . build_file_dict [ ] ) \n 
xccl . SetBaseConfiguration ( config_ref ) \n 
~~ build_file_configurations = self . build_file_dict . get ( , { } ) \n 
if build_file_configurations : \n 
~~~ for config_name in configurations : \n 
~~~ build_file_configuration_named = build_file_configurations . get ( config_name , { } ) \n 
if build_file_configuration_named : \n 
~~~ xcc = xccl . ConfigurationNamed ( config_name ) \n 
for xck , xcv in build_file_configuration_named . get ( , \n 
{ } ) . iteritems ( ) : \n 
~~~ xcc . SetBuildSetting ( xck , xcv ) \n 
~~ if in build_file_configuration_named : \n 
build_file_configurations [ config_name ] [ ] ) \n 
xcc . SetBaseConfiguration ( config_ref ) \n 
~~ ~~ ~~ ~~ ordinary_targets = [ ] \n 
run_test_targets = [ ] \n 
support_targets = [ ] \n 
targets = [ ] \n 
has_custom_all = False \n 
targets_for_all = [ ] \n 
for target in self . build_file_dict [ ] : \n 
~~~ target_name = target [ ] \n 
toolset = target [ ] \n 
qualified_target = gyp . common . QualifiedTarget ( self . gyp_path , target_name , \n 
toolset ) \n 
xcode_target = xcode_targets [ qualified_target ] \n 
assert xcode_target in self . project . _properties [ ] \n 
targets . append ( xcode_target ) \n 
ordinary_targets . append ( xcode_target ) \n 
if xcode_target . support_target : \n 
~~~ support_targets . append ( xcode_target . support_target ) \n 
targets . append ( xcode_target . support_target ) \n 
~~ if not int ( target . get ( , False ) ) : \n 
~~~ targets_for_all . append ( xcode_target ) \n 
~~ if target_name . lower ( ) == : \n 
~~~ has_custom_all = True ; \n 
~~ if target . get ( ) : \n 
run_target = gyp . xcodeproj_file . PBXAggregateTarget ( { \n 
: + target_name , \n 
: xcode_target . GetProperty ( ) , \n 
: xccl , \n 
parent = self . project ) \n 
run_target . AddDependency ( xcode_target ) \n 
command = target [ ] \n 
script = \n 
if command . get ( ) : \n 
command . get ( ) ) \n 
~~ if command . get ( ) : \n 
~~~ script = script + "\\n" . join ( \n 
( key , gyp . xcodeproj_file . ConvertVariablesToShellSyntax ( val ) ) \n 
for ( key , val ) in command . get ( ) . iteritems ( ) ] ) + "\\n" \n 
~~ command_prefix = \n 
if serialize_all_tests : \n 
~~ script = script + + command_prefix + % gyp . xcodeproj_file . ConvertVariablesToShellSyntax ( \n 
gyp . common . EncodePOSIXShellList ( command . get ( ) ) ) \n 
ssbp = gyp . xcodeproj_file . PBXShellScriptBuildPhase ( { \n 
: script , \n 
run_target . AppendProperty ( , ssbp ) \n 
targets . append ( run_target ) \n 
run_test_targets . append ( run_target ) \n 
xcode_target . test_runner = run_target \n 
~~ ~~ assert len ( self . project . _properties [ ] ) == len ( ordinary_targets ) + len ( support_targets ) \n 
self . project . _properties [ ] = targets \n 
self . project . RootGroupsTakeOverOnlyChildren ( True ) \n 
self . project . SortGroups ( ) \n 
if len ( targets_for_all ) > 1 and not has_custom_all : \n 
all_target = gyp . xcodeproj_file . PBXAggregateTarget ( \n 
for target in targets_for_all : \n 
~~~ all_target . AddDependency ( target ) \n 
~~ self . project . _properties [ ] . insert ( 0 , all_target ) \n 
~~ if len ( run_test_targets ) > 1 : \n 
run_all_tests_target = gyp . xcodeproj_file . PBXAggregateTarget ( \n 
for run_test_target in run_test_targets : \n 
~~~ run_all_tests_target . AddDependency ( run_test_target ) \n 
~~ self . project . _properties [ ] . insert ( 1 , run_all_tests_target ) \n 
~~ ~~ def Finalize2 ( self , xcode_targets , xcode_target_to_target_dict ) : \n 
~~~ for bf_tgt in self . build_file_dict [ ] : \n 
~~~ if int ( bf_tgt . get ( , 0 ) ) : \n 
~~~ tgt_name = bf_tgt [ ] \n 
toolset = bf_tgt [ ] \n 
qualified_target = gyp . common . QualifiedTarget ( self . gyp_path , \n 
tgt_name , toolset ) \n 
if isinstance ( xcode_target , gyp . xcodeproj_file . PBXAggregateTarget ) : \n 
~~~ all_run_tests = [ ] \n 
pbxtds = xcode_target . GetProperty ( ) \n 
for pbxtd in pbxtds : \n 
~~~ pbxcip = pbxtd . GetProperty ( ) \n 
dependency_xct = pbxcip . GetProperty ( ) \n 
if hasattr ( dependency_xct , ) : \n 
~~~ all_run_tests . append ( dependency_xct . test_runner ) \n 
~~ ~~ if len ( all_run_tests ) > 0 : \n 
~~~ run_all_target = gyp . xcodeproj_file . PBXAggregateTarget ( { \n 
: % tgt_name , \n 
: tgt_name , \n 
for run_test_target in all_run_tests : \n 
~~~ run_all_target . AddDependency ( run_test_target ) \n 
~~ idx = self . project . _properties [ ] . index ( xcode_target ) \n 
self . project . _properties [ ] . insert ( idx + 1 , run_all_target ) \n 
~~ ~~ ~~ ~~ for other_pbxproject in self . project . _other_pbxprojects . keys ( ) : \n 
~~~ self . project . AddOrGetProjectReference ( other_pbxproject ) \n 
~~ self . project . SortRemoteProductReferences ( ) \n 
self . project_file . ComputeIDs ( ) \n 
self . project_file . EnsureNoIDCollisions ( ) \n 
~~ def Write ( self ) : \n 
~~~ ( output_fd , new_pbxproj_path ) = tempfile . mkstemp ( suffix = , prefix = , \n 
dir = self . path ) \n 
~~~ output_file = os . fdopen ( output_fd , ) \n 
self . project_file . Print ( output_file ) \n 
output_file . close ( ) \n 
pbxproj_path = os . path . join ( self . path , ) \n 
same = False \n 
~~~ same = filecmp . cmp ( pbxproj_path , new_pbxproj_path , False ) \n 
~~~ if e . errno != errno . ENOENT : \n 
~~ ~~ if same : \n 
~~~ os . unlink ( new_pbxproj_path ) \n 
~~~ umask = os . umask ( 0 77 ) \n 
os . umask ( umask ) \n 
os . chmod ( new_pbxproj_path , 0 666 & ~ umask ) \n 
os . rename ( new_pbxproj_path , pbxproj_path ) \n 
if self . created_dir : \n 
~~~ shutil . rmtree ( self . path , True ) \n 
~~ ~~ ~~ def AddSourceToTarget ( source , type , pbxp , xct ) : \n 
~~~ source_extensions = [ , , , , , , , ] \n 
library_extensions = [ , , , ] \n 
basename = posixpath . basename ( source ) \n 
( root , ext ) = posixpath . splitext ( basename ) \n 
if ext : \n 
~~~ ext = ext [ 1 : ] . lower ( ) \n 
~~ if ext in source_extensions and type != : \n 
~~~ xct . SourcesPhase ( ) . AddFile ( source ) \n 
~~ elif ext in library_extensions and type != : \n 
~~~ xct . FrameworksPhase ( ) . AddFile ( source ) \n 
~~~ pbxp . AddOrGetFileInRootGroup ( source ) \n 
~~ ~~ def AddResourceToTarget ( resource , pbxp , xct ) : \n 
~~~ xct . ResourcesPhase ( ) . AddFile ( resource ) \n 
~~ def AddHeaderToTarget ( header , pbxp , xct , is_public ) : \n 
~~~ settings = % ( , ) [ is_public ] \n 
xct . HeadersPhase ( ) . AddFile ( header , settings ) \n 
~~ _xcode_variable_re = re . compile ( ) \n 
def ExpandXcodeVariables ( string , expansions ) : \n 
matches = _xcode_variable_re . findall ( string ) \n 
if matches == None : \n 
~~~ return string \n 
~~ matches . reverse ( ) \n 
~~~ ( to_replace , variable ) = match \n 
if not variable in expansions : \n 
~~ replacement = expansions [ variable ] \n 
string = re . sub ( re . escape ( to_replace ) , replacement , string ) \n 
~~ return string \n 
def EscapeXcodeDefine ( s ) : \n 
return re . sub ( _xcode_define_re , , s ) \n 
~~ def PerformBuild ( data , configurations , params ) : \n 
~~~ options = params [ ] \n 
for build_file , build_file_dict in data . iteritems ( ) : \n 
~~~ ( build_file_root , build_file_ext ) = os . path . splitext ( build_file ) \n 
if build_file_ext != : \n 
~~ xcodeproj_path = build_file_root + options . suffix + \n 
if options . generator_output : \n 
~~~ xcodeproj_path = os . path . join ( options . generator_output , xcodeproj_path ) \n 
~~ ~~ for config in configurations : \n 
~~~ arguments = [ , , xcodeproj_path ] \n 
arguments += [ , config ] \n 
subprocess . check_call ( arguments ) \n 
~~ ~~ def GenerateOutput ( target_list , target_dicts , data , params ) : \n 
~~~ ninja_wrapper = params . get ( ) == \n 
if ninja_wrapper : \n 
~~~ ( target_list , target_dicts , data ) = gyp . xcode_ninja . CreateWrapper ( target_list , target_dicts , data , params ) \n 
~~ options = params [ ] \n 
generator_flags = params . get ( , { } ) \n 
parallel_builds = generator_flags . get ( , True ) \n 
serialize_all_tests = generator_flags . get ( , True ) \n 
upgrade_check_project_version = generator_flags . get ( , None ) \n 
if upgrade_check_project_version : \n 
~~~ upgrade_check_project_version = str ( upgrade_check_project_version ) \n 
while len ( upgrade_check_project_version ) < 4 : \n 
~~~ upgrade_check_project_version = + upgrade_check_project_version \n 
~~ ~~ skip_excluded_files = not generator_flags . get ( , True ) \n 
xcode_projects = { } \n 
~~ xcp = XcodeProject ( build_file , xcodeproj_path , build_file_dict ) \n 
xcode_projects [ build_file ] = xcp \n 
pbxp = xcp . project \n 
project_attributes = { } ; \n 
if parallel_builds : \n 
~~~ project_attributes [ ] = \n 
~~ if upgrade_check_project_version : \n 
~~~ project_attributes [ ] = upgrade_check_project_version \n 
project_attributes [ ] = upgrade_check_project_version \n 
~~ pbxp . SetProperty ( , project_attributes ) \n 
if not generator_flags . get ( ) : \n 
~~~ main_group = pbxp . GetProperty ( ) \n 
build_group = gyp . xcodeproj_file . PBXGroup ( { : } ) \n 
main_group . AppendChild ( build_group ) \n 
for included_file in build_file_dict [ ] : \n 
~~~ build_group . AddOrGetFileByPath ( included_file , False ) \n 
~~ ~~ ~~ xcode_targets = { } \n 
xcode_target_to_target_dict = { } \n 
for qualified_target in target_list : \n 
~~~ [ build_file , target_name , toolset ] = gyp . common . ParseQualifiedTarget ( qualified_target ) \n 
spec = target_dicts [ qualified_target ] \n 
if spec [ ] != : \n 
~~~ raise Exception ( \n 
qualified_target ) \n 
~~ configuration_names = [ spec [ ] ] \n 
for configuration_name in sorted ( spec [ ] . keys ( ) ) : \n 
~~~ if configuration_name not in configuration_names : \n 
~~~ configuration_names . append ( configuration_name ) \n 
~~ ~~ xcp = xcode_projects [ build_file ] \n 
xccl = CreateXCConfigurationList ( configuration_names ) \n 
_types = { \n 
target_properties = { \n 
: target_name , \n 
type = spec [ ] \n 
is_xctest = int ( spec . get ( , 0 ) ) \n 
is_bundle = int ( spec . get ( , 0 ) ) or is_xctest \n 
is_app_extension = int ( spec . get ( , 0 ) ) \n 
is_watchkit_extension = int ( spec . get ( , 0 ) ) \n 
is_watch_app = int ( spec . get ( , 0 ) ) \n 
if type != : \n 
~~~ type_bundle_key = type \n 
if is_xctest : \n 
~~~ type_bundle_key += \n 
assert type == , ( \n 
% target_name ) \n 
~~ elif is_app_extension : \n 
~~~ assert is_bundle , ( \n 
type_bundle_key += \n 
~~ elif is_watchkit_extension : \n 
~~ elif is_watch_app : \n 
~~ elif is_bundle : \n 
~~ xctarget_type = gyp . xcodeproj_file . PBXNativeTarget \n 
~~~ target_properties [ ] = _types [ type_bundle_key ] \n 
~~~ xctarget_type = gyp . xcodeproj_file . PBXAggregateTarget \n 
assert not is_bundle , ( \n 
target_name ) \n 
assert not is_xctest , ( \n 
~~ target_product_name = spec . get ( ) \n 
if target_product_name is not None : \n 
~~~ target_properties [ ] = target_product_name \n 
~~ xct = xctarget_type ( target_properties , parent = pbxp , \n 
force_outdir = spec . get ( ) , \n 
force_prefix = spec . get ( ) , \n 
force_extension = spec . get ( ) ) \n 
pbxp . AppendProperty ( , xct ) \n 
xcode_targets [ qualified_target ] = xct \n 
xcode_target_to_target_dict [ xct ] = spec \n 
spec_actions = spec . get ( , [ ] ) \n 
spec_rules = spec . get ( , [ ] ) \n 
support_xct = None \n 
if type != and ( spec_actions or spec_rules ) and not ninja_wrapper : \n 
~~~ support_xccl = CreateXCConfigurationList ( configuration_names ) ; \n 
support_target_suffix = generator_flags . get ( \n 
support_target_properties = { \n 
: support_xccl , \n 
: target_name + support_target_suffix , \n 
if target_product_name : \n 
~~~ support_target_properties [ ] = target_product_name + \n 
~~ support_xct = gyp . xcodeproj_file . PBXAggregateTarget ( support_target_properties , \n 
parent = pbxp ) \n 
pbxp . AppendProperty ( , support_xct ) \n 
xct . AddDependency ( support_xct ) \n 
~~ xct . support_target = support_xct \n 
prebuild_index = 0 \n 
for action in spec_actions : \n 
~~~ message = action . get ( ) \n 
~~~ message = + gyp . common . EncodePOSIXShellArgument ( message ) \n 
~~ action_string = gyp . common . EncodePOSIXShellList ( action [ ] ) \n 
message_sh = gyp . xcodeproj_file . ConvertVariablesToShellSyntax ( message ) \n 
action_string_sh = gyp . xcodeproj_file . ConvertVariablesToShellSyntax ( \n 
action_string ) \n 
if message_sh : \n 
~~~ script += message_sh + \n 
~~ script += + action_string_sh + \n 
: action [ ] , \n 
if support_xct : \n 
~~~ support_xct . AppendProperty ( , ssbp ) \n 
~~~ xct . _properties [ ] . insert ( prebuild_index , ssbp ) \n 
prebuild_index = prebuild_index + 1 \n 
~~ if int ( action . get ( , False ) ) : \n 
~~~ for output in action [ ] : \n 
~~~ AddSourceToTarget ( output , type , pbxp , xct ) \n 
~~ ~~ if int ( action . get ( , False ) ) : \n 
~~~ AddResourceToTarget ( output , pbxp , xct ) \n 
~~ ~~ ~~ if is_bundle : \n 
~~~ tgt_mac_bundle_resources = spec . get ( , [ ] ) \n 
~~~ tgt_mac_bundle_resources = [ ] \n 
~~ rules_by_ext = { } \n 
for rule in spec_rules : \n 
~~~ rules_by_ext [ rule [ ] ] = rule \n 
concrete_outputs_by_rule_source = [ ] \n 
concrete_outputs_all = [ ] \n 
messages = [ ] \n 
actions = [ ] \n 
for rule_source in rule . get ( , [ ] ) : \n 
~~~ rule_source_dirname , rule_source_basename = posixpath . split ( rule_source ) \n 
( rule_source_root , rule_source_ext ) = posixpath . splitext ( rule_source_basename ) \n 
rule_input_dict = { \n 
: rule_source_root , \n 
: rule_source_ext , \n 
: rule_source_basename , \n 
: rule_source , \n 
: rule_source_dirname , \n 
concrete_outputs_for_this_rule_source = [ ] \n 
for output in rule . get ( , [ ] ) : \n 
~~~ concrete_output = ExpandXcodeVariables ( output , rule_input_dict ) \n 
concrete_outputs_for_this_rule_source . append ( concrete_output ) \n 
pbxp . AddOrGetFileInRootGroup ( concrete_output ) \n 
~~ concrete_outputs_by_rule_source . append ( concrete_outputs_for_this_rule_source ) \n 
concrete_outputs_all . extend ( concrete_outputs_for_this_rule_source ) \n 
if int ( rule . get ( , False ) ) : \n 
~~~ for output in concrete_outputs_for_this_rule_source : \n 
~~ ~~ was_mac_bundle_resource = rule_source in tgt_mac_bundle_resources \n 
if was_mac_bundle_resource or int ( rule . get ( , False ) ) : \n 
~~ ~~ message = rule . get ( ) \n 
~~~ message = gyp . common . EncodePOSIXShellArgument ( message ) \n 
message = ExpandXcodeVariables ( message , rule_input_dict ) \n 
~~ messages . append ( message ) \n 
action_string = gyp . common . EncodePOSIXShellList ( rule [ ] ) \n 
action = ExpandXcodeVariables ( action_string , rule_input_dict ) \n 
actions . append ( action ) \n 
~~ if len ( concrete_outputs_all ) > 0 : \n 
~~~ makefile_name = % re . sub ( \n 
, , % ( target_name , rule [ ] ) ) \n 
makefile_path = os . path . join ( xcode_projects [ build_file ] . path , \n 
makefile_name ) \n 
makefile = open ( makefile_path , ) \n 
makefile . write ( ) \n 
for concrete_output_index in xrange ( 0 , len ( concrete_outputs_by_rule_source ) ) : \n 
~~~ concrete_output = concrete_outputs_by_rule_source [ concrete_output_index ] [ 0 ] \n 
if concrete_output_index == len ( concrete_outputs_by_rule_source ) - 1 : \n 
~~~ eol = \n 
~~ makefile . write ( % ( concrete_output , eol ) ) \n 
~~ for ( rule_source , concrete_outputs , message , action ) in zip ( rule [ ] , concrete_outputs_by_rule_source , \n 
messages , actions ) : \n 
~~~ makefile . write ( ) \n 
concrete_output_dirs = [ ] \n 
for concrete_output_index in xrange ( 0 , len ( concrete_outputs ) ) : \n 
~~~ concrete_output = concrete_outputs [ concrete_output_index ] \n 
if concrete_output_index == 0 : \n 
~~~ bol = \n 
~~ makefile . write ( % ( bol , concrete_output ) ) \n 
concrete_output_dir = posixpath . dirname ( concrete_output ) \n 
if ( concrete_output_dir and \n 
concrete_output_dir not in concrete_output_dirs ) : \n 
~~~ concrete_output_dirs . append ( concrete_output_dir ) \n 
~~ ~~ makefile . write ( ) \n 
prerequisites = [ rule_source ] \n 
prerequisites . extend ( rule . get ( , [ ] ) ) \n 
for prerequisite_index in xrange ( 0 , len ( prerequisites ) ) : \n 
~~~ prerequisite = prerequisites [ prerequisite_index ] \n 
if prerequisite_index == len ( prerequisites ) - 1 : \n 
~~ makefile . write ( % ( prerequisite , eol ) ) \n 
~~ if len ( concrete_output_dirs ) > 0 : \n 
~~ if message : \n 
~~~ makefile . write ( % message ) \n 
~~ makefile . write ( % action ) \n 
~~ makefile . close ( ) \n 
~~ ~~ groups = [ , ] \n 
if skip_excluded_files : \n 
~~~ groups = [ x for x in groups if not x . endswith ( ) ] \n 
~~ for group in groups : \n 
~~~ for item in rule . get ( group , [ ] ) : \n 
~~~ pbxp . AddOrGetFileInRootGroup ( item ) \n 
~~ ~~ ~~ for source in spec . get ( , [ ] ) : \n 
~~~ ( source_root , source_extension ) = posixpath . splitext ( source ) \n 
if source_extension [ 1 : ] not in rules_by_ext : \n 
~~~ AddSourceToTarget ( source , type , pbxp , xct ) \n 
~~ ~~ if is_bundle : \n 
~~~ for resource in tgt_mac_bundle_resources : \n 
~~~ ( resource_root , resource_extension ) = posixpath . splitext ( resource ) \n 
if resource_extension [ 1 : ] not in rules_by_ext : \n 
~~~ AddResourceToTarget ( resource , pbxp , xct ) \n 
~~~ pbxp . AddOrGetFileInRootGroup ( resource ) \n 
~~ ~~ for header in spec . get ( , [ ] ) : \n 
~~~ AddHeaderToTarget ( header , pbxp , xct , False ) \n 
~~ ~~ if is_bundle or type == : \n 
~~~ for header in spec . get ( , [ ] ) : \n 
~~~ AddHeaderToTarget ( header , pbxp , xct , True ) \n 
~~ ~~ pbxcp_dict = { } \n 
for copy_group in spec . get ( , [ ] ) : \n 
~~~ dest = copy_group [ ] \n 
if dest [ 0 ] not in ( , ) : \n 
~~~ dest = + dest \n 
~~ code_sign = int ( copy_group . get ( , 0 ) ) \n 
settings = ( None , ) [ code_sign ] ; \n 
pbxcp = pbxcp_dict . get ( dest , None ) \n 
if pbxcp is None : \n 
~~~ pbxcp = gyp . xcodeproj_file . PBXCopyFilesBuildPhase ( { \n 
: + copy_group [ ] \n 
parent = xct ) \n 
pbxcp . SetDestination ( dest ) \n 
xct . _properties [ ] . insert ( prebuild_index , pbxcp ) \n 
pbxcp_dict [ dest ] = pbxcp \n 
~~ for file in copy_group [ ] : \n 
~~~ pbxcp . AddFile ( file , settings ) \n 
~~ ~~ if not skip_excluded_files : \n 
~~~ for key in [ , , , \n 
] : \n 
~~~ excluded_key = key + \n 
for item in spec . get ( excluded_key , [ ] ) : \n 
~~ ~~ ~~ groups = [ , , , ] \n 
~~ for action in spec . get ( , [ ] ) : \n 
~~~ for group in groups : \n 
~~~ for item in action . get ( group , [ ] ) : \n 
~~~ if not item . startswith ( ) : \n 
~~ ~~ ~~ ~~ for postbuild in spec . get ( , [ ] ) : \n 
~~~ action_string_sh = gyp . common . EncodePOSIXShellList ( postbuild [ ] ) \n 
script = + action_string_sh + \n 
xct . AppendProperty ( , ssbp ) \n 
~~ if in spec : \n 
~~~ for dependency in spec [ ] : \n 
~~~ xct . AddDependency ( xcode_targets [ dependency ] ) \n 
~~~ support_xct . AddDependency ( xcode_targets [ dependency ] ) \n 
~~ ~~ ~~ if in spec : \n 
~~~ for library in spec [ ] : \n 
~~~ xct . FrameworksPhase ( ) . AddFile ( library ) \n 
library_dir = posixpath . dirname ( library ) \n 
if library_dir not in xcode_standard_library_dirs and ( \n 
not xct . HasBuildSetting ( _library_search_paths_var ) or \n 
library_dir not in xct . GetBuildSetting ( _library_search_paths_var ) ) : \n 
~~~ xct . AppendBuildSetting ( _library_search_paths_var , library_dir ) \n 
~~ ~~ ~~ for configuration_name in configuration_names : \n 
~~~ configuration = spec [ ] [ configuration_name ] \n 
xcbc = xct . ConfigurationNamed ( configuration_name ) \n 
for include_dir in configuration . get ( , [ ] ) : \n 
~~~ xcbc . AppendBuildSetting ( , include_dir ) \n 
~~ for include_dir in configuration . get ( , [ ] ) : \n 
~~ for library_dir in configuration . get ( , [ ] ) : \n 
~~~ if library_dir not in xcode_standard_library_dirs and ( \n 
not xcbc . HasBuildSetting ( _library_search_paths_var ) or \n 
library_dir not in xcbc . GetBuildSetting ( _library_search_paths_var ) ) : \n 
~~~ xcbc . AppendBuildSetting ( _library_search_paths_var , library_dir ) \n 
~~ ~~ if in configuration : \n 
~~~ for define in configuration [ ] : \n 
~~~ set_define = EscapeXcodeDefine ( define ) \n 
xcbc . AppendBuildSetting ( , set_define ) \n 
~~~ for xck , xcv in configuration [ ] . iteritems ( ) : \n 
~~~ xcbc . SetBuildSetting ( xck , xcv ) \n 
~~~ config_ref = pbxp . AddOrGetFileInRootGroup ( \n 
configuration [ ] ) \n 
xcbc . SetBaseConfiguration ( config_ref ) \n 
~~ ~~ ~~ build_files = [ ] \n 
~~~ if build_file . endswith ( ) : \n 
~~~ build_files . append ( build_file ) \n 
~~ ~~ for build_file in build_files : \n 
~~~ xcode_projects [ build_file ] . Finalize1 ( xcode_targets , serialize_all_tests ) \n 
~~ for build_file in build_files : \n 
~~~ xcode_projects [ build_file ] . Finalize2 ( xcode_targets , \n 
xcode_target_to_target_dict ) \n 
~~~ xcode_projects [ build_file ] . Write ( ) \n 
~~ ~~ 