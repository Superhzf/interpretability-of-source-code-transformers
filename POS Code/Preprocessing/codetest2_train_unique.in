\n 
# \n 
from horizon import tabs \n 
class NetworkProfileTab ( tabs . Tab ) : \n 
slug = "network_profile" \n 
template_name = \n 
def get_context_data ( self , request ) : \n 
~~~ return None \n 
~~ ~~ class PolicyProfileTab ( tabs . Tab ) : \n 
slug = "policy_profile" \n 
preload = False \n 
~~ class IndexTabs ( tabs . TabGroup ) : \n 
~~~ slug = "indextabs" \n 
tabs = ( NetworkProfileTab , PolicyProfileTab ) \n 
import weakref \n 
from eventlet import corolocal \n 
class WeakLocal ( corolocal . local ) : \n 
~~~ def __getattribute__ ( self , attr ) : \n 
~~~ rval = corolocal . local . __getattribute__ ( self , attr ) \n 
if rval : \n 
~~~ rval = rval ( ) \n 
~~ return rval \n 
~~ def __setattr__ ( self , attr , value ) : \n 
~~~ value = weakref . ref ( value ) \n 
return corolocal . local . __setattr__ ( self , attr , value ) \n 
~~ ~~ store = WeakLocal ( ) \n 
weak_store = WeakLocal ( ) \n 
strong_store = corolocal . local \n 
import eventlet \n 
eventlet . monkey_patch ( ) \n 
import contextlib \n 
import sys \n 
from oslo . config import cfg \n 
from openstack_dashboard . openstack . common import log as logging \n 
from openstack_dashboard . openstack . common import rpc \n 
from openstack_dashboard . openstack . common . rpc import impl_zmq \n 
CONF = cfg . CONF \n 
CONF . register_opts ( rpc . rpc_opts ) \n 
CONF . register_opts ( impl_zmq . zmq_opts ) \n 
def main ( ) : \n 
~~~ CONF ( sys . argv [ 1 : ] , project = ) \n 
logging . setup ( "oslo" ) \n 
with contextlib . closing ( impl_zmq . ZmqProxy ( CONF ) ) as reactor : \n 
~~~ reactor . consume_in_thread ( ) \n 
reactor . wait ( ) \n 
~~ ~~ from enum import IntEnum \n 
from . component import Component \n 
from . object import field \n 
class ReflectionProbeUsage ( IntEnum ) : \n 
~~~ Off = 0 \n 
BlendProbes = 1 \n 
BlendProbesAndSkybox = 2 \n 
Simple = 3 \n 
~~ class ShadowCastingMode ( IntEnum ) : \n 
On = 1 \n 
TwoSided = 2 \n 
ShadowsOnly = 3 \n 
~~ class Renderer ( Component ) : \n 
~~~ enabled = field ( "m_Enabled" , bool ) \n 
lightmap_index = field ( "m_LightmapIndex" ) \n 
materials = field ( "m_Materials" ) \n 
probe_anchor = field ( "m_ProbeAnchor" ) \n 
receive_shadows = field ( "m_ReceiveShadows" , bool ) \n 
reflection_probe_usage = field ( "m_ReflectionProbeUsage" , ReflectionProbeUsage ) \n 
shadow_casting_mode = field ( "m_CastShadows" , ShadowCastingMode ) \n 
sorting_layer_id = field ( "m_SortingLayerID" ) \n 
sorting_order = field ( "m_SortingOrder" ) \n 
use_light_probes = field ( "m_UseLightProbes" , bool ) \n 
lightmap_index_dynamic = field ( "m_LightmapIndexDynamic" ) \n 
lightmap_tiling_offset = field ( "m_LightmapTilingOffset" ) \n 
lightmap_tiling_offset_dynamic = field ( "m_LightmapTilingOffsetDynamic" ) \n 
static_batch_root = field ( "m_StaticBatchRoot" ) \n 
subset_indices = field ( "m_SubsetIndices" ) \n 
@ property \n 
def material ( self ) : \n 
~~~ return self . materials [ 0 ] \n 
~~ ~~ class ParticleSystemRenderMode ( IntEnum ) : \n 
~~~ Billboard = 0 \n 
Stretch = 1 \n 
HorizontalBillboard = 2 \n 
VerticalBillboard = 3 \n 
Mesh = 4 \n 
~~ class ParticleSystemSortMode ( IntEnum ) : \n 
~~~ None_ = 0 \n 
Distance = 1 \n 
OldestInFront = 2 \n 
YoungestInFront = 3 \n 
~~ class MeshRenderer ( Component ) : \n 
~~~ pass \n 
~~ class ParticleRenderer ( Renderer ) : \n 
~~~ camera_velocity_scale = field ( "m_CameraVelocityScale" ) \n 
length_scale = field ( "m_LengthScale" ) \n 
max_particle_size = field ( "m_MaxParticleSize" ) \n 
velocity_scale = field ( "m_VelocityScale" ) \n 
stretch_particles = field ( "m_StretchParticles" ) \n 
~~ class ParticleSystemRenderer ( Renderer ) : \n 
mesh = field ( "m_Mesh" ) \n 
mesh1 = field ( "m_Mesh1" ) \n 
mesh2 = field ( "m_Mesh2" ) \n 
mesh3 = field ( "m_Mesh3" ) \n 
normal_direction = field ( "m_NormalDirection" ) \n 
render_mode = field ( "m_RenderMode" , ParticleSystemRenderMode ) \n 
sort_mode = field ( "m_SortMode" , ParticleSystemSortMode ) \n 
sorting_fudge = field ( "m_SortingFudge" ) \n 
~~ from ConfigParser import * \n 
from StringIO import * \n 
from Log import Log \n 
import datetime \n 
class Config : \n 
~~~ @ staticmethod \n 
def LoadConfig ( ) : \n 
~~~ Config . parser = ConfigParser ( ) \n 
try : \n 
~~~ sconff = open ( CONFIG_FILE , "r" ) \n 
~~ except : \n 
return \n 
~~ sconf = StringIO ( ) \n 
sconf . write ( "[sysconf]\\n" ) \n 
sconf . write ( sconff . read ( ) ) \n 
sconf . seek ( 0 ) \n 
Config . parser . readfp ( sconf ) \n 
sconff . close ( ) \n 
sconf . close ( ) \n 
~~ @ staticmethod \n 
def GetBoardsFile ( ) : \n 
~~~ return BOARDS_FILE \n 
def GetInt ( name , defval ) : \n 
~~~ if ( Config . parser . has_option ( , name ) ) : \n 
~~~ return Config . parser . getint ( , name ) \n 
~~ else : \n 
~~~ return defval \n 
~~ ~~ @ staticmethod \n 
def GetString ( name , defval ) : \n 
~~~ val = Config . parser . get ( , name ) \n 
if ( val [ 0 ] == \'"\' and val . endswith ( \'"\' ) ) : \n 
~~~ val = val [ 1 : - 1 ] \n 
~~ return val . decode ( ) \n 
~~ ~~ ~~ BBS_ROOT = \n 
BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n 
BBS_XMPP_KEY_FILE = BBS_ROOT + "xmpp.key" \n 
BOARDS_FILE = BBS_ROOT + \n 
STRLEN = 80 \n 
ARTICLE_TITLE_LEN = 60 \n 
BM_LEN = 60 \n 
MAXBOARD = 400 \n 
CONFIG_FILE = BBS_ROOT + \n 
FILENAME_LEN = 20 \n 
OWNER_LEN = 30 \n 
SESSIONID_LEN = 32 \n 
REFRESH_TOKEN_LEN = 128 \n 
NAMELEN = 40 \n 
IDLEN = 12 \n 
MD5PASSLEN = 16 \n 
OLDPASSLEN = 14 \n 
MOBILE_NUMBER_LEN = 17 \n 
MAXCLUB = 128 \n 
MAXUSERS = 20000 \n 
MAX_MSG_SIZE = 1024 \n 
MAXFRIENDS = 400 \n 
MAXMESSAGE = 5 \n 
MAXSIGLINES = 6 \n 
IPLEN = 16 \n 
DEFAULTBOARD = "sysop" \n 
BLESS_BOARD = "happy_birthday" \n 
QUOTED_LINES = 10 \n 
MAXACTIVE = 8000 \n 
USHM_SIZE = MAXACTIVE + 10 \n 
UTMP_HASHSIZE = USHM_SIZE * 4 \n 
UCACHE_SEMLOCK = 0 \n 
LEN_FRIEND_EXP = 15 \n 
SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n 
SESSION_TIMEOUT_SECONDS = 86400 * 30 \n 
XMPP_IDLE_TIME = 300 \n 
XMPP_LONG_IDLE_TIME = 1800 \n 
XMPP_UPDATE_TIME_INTERVAL = 10 \n 
XMPP_PING_TIME_INTERVAL = 60 \n 
PUBLIC_SHMKEY = 3700 \n 
MAX_ATTACHSIZE = 20 * 1024 * 1024 \n 
BMDEL_DECREASE = True \n 
SYSMAIL_BOARD = "sysmail" \n 
ADD_EDITMARK = True \n 
SEARCH_COUNT_LIMIT = 20 \n 
MAIL_SIZE_LIMIT = - 1 \n 
SEC_DELETED_OLDHOME = 3600 * 24 * 3 \n 
SELF_INTRO_MAX_LEN = 800 \n 
import re \n 
import os \n 
import stat \n 
import json \n 
import struct \n 
import time \n 
import Config \n 
import Board \n 
import Post \n 
import BoardManager \n 
from Util import Util \n 
from errors import * \n 
DEFAULT_DIGEST_LIST_COUNT = 20 \n 
class DigestItem : \n 
~~~ def __init__ ( self , basepath ) : \n 
~~~ self . basepath = basepath \n 
self . title = \n 
self . host = \n 
self . port = 0 \n 
self . attachpos = 0 \n 
self . fname = \n 
self . mtitle = \n 
self . items = [ ] \n 
self . update_time = 0 \n 
self . id = 0 \n 
self . sysop_only = 0 \n 
self . bms_only = 0 \n 
self . zixia_only = 0 \n 
~~ def IsDir ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
return stat . S_ISDIR ( st . st_mode ) \n 
~~~ return False \n 
~~ ~~ def IsFile ( self ) : \n 
return stat . S_ISREG ( st . st_mode ) \n 
~~ ~~ def GetModTime ( self ) : \n 
mtime = st . st_mtime \n 
~~~ mtime = time . time ( ) \n 
~~ return mtime \n 
~~ def names_path ( self ) : \n 
~~~ return "%s/.Names" % self . realpath ( ) \n 
~~ def realpath ( self ) : \n 
~~~ return "%s/%s" % ( Config . BBS_ROOT , self . path ( ) ) \n 
~~ def path ( self ) : \n 
~~~ if ( self . fname ) : \n 
~~~ return "%s/%s" % ( self . basepath , self . fname ) \n 
~~~ return self . basepath \n 
~~ ~~ def CheckUpdate ( self ) : \n 
~~~ stat = os . stat ( self . names_path ( ) ) \n 
if ( stat . st_mtime > self . update_time ) : \n 
~~~ self . LoadNames ( ) \n 
~~ ~~ except : \n 
~~ return True \n 
~~ def LoadNames ( self ) : \n 
~~~ f = open ( self . names_path ( ) , "r" ) \n 
~~ except IOError : \n 
~~~ return 0 \n 
~~ stat = os . fstat ( f . fileno ( ) ) \n 
self . update_time = stat . st_mtime \n 
item = DigestItem ( self . path ( ) ) \n 
hostname = \n 
_id = 0 \n 
bms_only = 0 \n 
sysop_only = 0 \n 
zixia_only = 0 \n 
while ( True ) : \n 
~~~ line = f . readline ( ) \n 
if ( line == "" ) : break \n 
npos = line . find ( "\\n" ) \n 
if ( npos != - 1 ) : line = line [ : npos ] \n 
if ( line [ : 1 ] == ) : \n 
~~~ if ( not self . mtitle ) : \n 
~~~ self . mtitle = line [ 8 : ] \n 
~~ ~~ ~~ result = re . match ( , line ) \n 
if ( result ) : \n 
~~~ key = result . group ( 1 ) \n 
value = result . group ( 2 ) \n 
if ( key == "Name" ) : \n 
~~~ item . title = value \n 
item . attachpos = 0 \n 
~~ elif ( key == "Path" ) : \n 
~~~ if ( value [ : 2 ] == "~/" ) : \n 
~~~ item . fname = value [ 2 : ] \n 
~~~ item . fname = value \n 
~~ if ( item . fname . find ( ".." ) != - 1 ) : \n 
~~~ continue \n 
~~~ bms_only += 1 \n 
~~~ sysop_only += 1 \n 
~~~ zixia_only += 1 \n 
~~ if ( item . fname . find ( "!@#$%" ) != - 1 ) : \n 
~~~ parts = re . split ( , item . fname ) \n 
newparts = [ ] \n 
for part in parts : \n 
~~~ if ( part ) : \n 
~~~ newparts += [ part ] \n 
~~ ~~ hostname = newparts [ 0 ] \n 
item . fname = newparts [ 1 ] \n 
~~~ item . port = int ( newparts [ 2 ] ) \n 
~~~ item . port = 0 \n 
~~ ~~ item . id = _id \n 
_id += 1 \n 
item . bms_only = bms_only \n 
item . sysop_only = sysop_only \n 
item . zixia_only = zixia_only \n 
item . host = hostname \n 
self . items += [ item ] \n 
~~ elif ( key == "Host" ) : \n 
~~~ hostname = value \n 
~~ elif ( key == "Port" ) : \n 
~~~ item . port = int ( value ) \n 
~~ ~~ elif ( key == "Attach" ) : \n 
~~~ item . attachpos = int ( value ) \n 
~~~ item . attachpos = 0 \n 
~~ ~~ ~~ ~~ f . close ( ) \n 
return 1 \n 
~~ def GetItem ( self , user , route , has_perm = False , need_perm = False ) : \n 
~~~ self . CheckUpdate ( ) \n 
if ( self . mtitle . find ( "(BM:" ) != - 1 ) : \n 
~~~ if ( Board . Board . IsBM ( user , self . mtitle [ 4 : ] , ) or user . IsSysop ( ) ) : \n 
~~~ has_perm = True \n 
~~ elif ( need_perm and not has_perm ) : \n 
~~ if ( len ( route ) == 0 ) : \n 
~~~ return self \n 
~~ target = route [ 0 ] - 1 \n 
_id = target \n 
if ( _id >= len ( self . items ) ) : \n 
~~ while ( self . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ _id += 1 \n 
~~ ~~ item = self . items [ _id ] \n 
item . mtitle = item . title \n 
if ( len ( route ) == 1 ) : \n 
~~~ return item \n 
~~~ if ( item . IsDir ( ) ) : \n 
~~~ if ( not item . CheckUpdate ( ) ) : \n 
~~ return item . GetItem ( user , route [ 1 : ] , has_perm , need_perm ) \n 
~~ ~~ ~~ def GetRange ( self , user , route , start , end , has_perm = False , need_perm = False ) : \n 
firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n 
if ( not firstitem ) : \n 
~~~ return [ ] \n 
~~ parent = self . GetItem ( user , route , has_perm , need_perm ) \n 
if ( not parent ) : \n 
~~ if ( not parent . IsDir ( ) ) : \n 
~~ result = [ ] \n 
_id = start - 1 \n 
for i in range ( start , end + 1 ) : \n 
~~~ target = i - 1 \n 
if ( _id >= len ( parent . items ) ) : \n 
~~ while ( parent . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ return result \n 
~~ ~~ item = parent . items [ _id ] \n 
result += [ item ] \n 
~~ return result \n 
~~ def EffectiveId ( self , user ) : \n 
~~~ _id = self . id \n 
if ( user . IsSysop ( ) ) : \n 
~~~ return _id \n 
~~ if ( not user . IsSysop ( ) ) : \n 
~~~ _id -= self . sysop_only \n 
~~ if ( not user . IsBM ( ) ) : \n 
~~~ _id -= self . bms_only \n 
~~ if ( not user . IsSECANC ( ) ) : \n 
~~~ _id -= self . zixia_only \n 
~~ return _id \n 
~~ def GetInfo ( self ) : \n 
~~~ info = { } \n 
info [ ] = Util . gbkDec ( self . mtitle ) \n 
info [ ] = Util . gbkDec ( self . title ) \n 
info [ ] = self . attachpos \n 
if ( self . host != ) : \n 
~~~ info [ ] = self . host \n 
info [ ] = self . port \n 
info [ ] = \n 
~~ elif ( self . IsDir ( ) ) : \n 
~~~ info [ ] = \n 
~~ elif ( self . IsFile ( ) ) : \n 
~~ info [ ] = int ( self . GetModTime ( ) ) \n 
return info \n 
~~ def GetInfoForUser ( self , user ) : \n 
~~~ info = self . GetInfo ( ) \n 
info [ ] = self . EffectiveId ( user ) + 1 \n 
~~ def GetAttachLink ( self , session ) : \n 
filename = \n 
for i in range ( 2 ) : \n 
~~~ filename += "%0x" % struct . unpack ( , _hash [ i * 4 : ( i + 1 ) * 4 ] ) \n 
~~ link = "http://%s/bbscon.php?b=xattach&f=%s" % ( session . GetMirror ( Config . Config . GetInt ( , 80 ) ) , filename ) \n 
linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n 
target = "../../%s" % self . path ( ) \n 
~~~ os . symlink ( target , linkfile ) \n 
~~ return link \n 
~~ ~~ class Digest : \n 
~~~ root = DigestItem ( "0Announce" ) \n 
def __init__ ( self , board , path ) : \n 
~~~ self . board = board \n 
self . path = path \n 
self . root = DigestItem ( self . path ) \n 
def GET ( svc , session , params , action ) : \n 
~~~ if ( session is None ) : raise Unauthorized ( ) \n 
user = session . GetUser ( ) \n 
boardname = svc . get_str ( params , , ) \n 
if ( boardname ) : \n 
~~~ board = BoardManager . BoardManager . GetBoard ( boardname ) \n 
if ( board is None ) : raise NotFound ( % boardname ) \n 
if ( not board . CheckReadPerm ( user ) ) : \n 
~~~ raise NoPerm ( ) \n 
~~ basenode = board . digest . root \n 
has_perm = user . IsDigestMgr ( ) or user . IsSysop ( ) or user . IsSuperBM ( ) \n 
~~~ basenode = Digest . root \n 
has_perm = user . IsDigestMgr ( ) \n 
~~ if ( action == "list" ) : \n 
~~~ route = svc . get_str ( params , ) \n 
start = svc . get_int ( params , , 1 ) \n 
end = svc . get_int ( params , , start + DEFAULT_DIGEST_LIST_COUNT - 1 ) \n 
Digest . List ( svc , basenode , route , start , end , session , has_perm ) \n 
~~ elif ( action == "view" ) : \n 
start = svc . get_int ( params , , 0 ) \n 
count = svc . get_int ( params , , 0 ) \n 
Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n 
~~~ raise WrongArgs ( % action ) \n 
def ParseRoute ( route ) : \n 
~~~ ret = [ ] \n 
items = re . split ( , route ) \n 
items = items [ 1 : ] \n 
for item in items : \n 
~~~ ret += [ int ( item ) ] \n 
~~~ raise WrongArgs ( % item ) \n 
~~ ~~ return ret \n 
def List ( svc , basenode , route , start , end , session , has_perm ) : \n 
~~~ route_array = Digest . ParseRoute ( route ) \n 
parent = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
~~~ raise WrongArgs ( % route ) \n 
~~ items = basenode . GetRange ( session . GetUser ( ) , route_array , start , end , has_perm ) \n 
result = { } \n 
result [ ] = parent . GetInfoForUser ( session . GetUser ( ) ) \n 
result [ ] = len ( items ) \n 
result_list = [ ] \n 
~~~ result_list += [ item . GetInfoForUser ( session . GetUser ( ) ) ] \n 
~~ result [ ] = result_list \n 
svc . writedata ( json . dumps ( result ) ) \n 
def View ( svc , basenode , route , session , has_perm , start , count ) : \n 
item = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
if ( not item ) : \n 
~~ if ( not item . IsFile ( ) ) : \n 
~~ result = { } \n 
result [ ] = item . GetInfoForUser ( session . GetUser ( ) ) \n 
postinfo = Post . Post ( item . realpath ( ) , None ) \n 
( result [ ] , result [ ] ) = postinfo . GetContent ( start , count ) \n 
attachlist = postinfo . GetAttachListByType ( ) \n 
result [ ] = attachlist [ 0 ] \n 
result [ ] = attachlist [ 1 ] \n 
if ( attachlist [ 0 ] or attachlist [ 1 ] ) : \n 
~~~ result [ ] = item . GetAttachLink ( session ) \n 
~~ svc . writedata ( json . dumps ( result ) ) \n 
~~ ~~ import time \n 
import UserManager \n 
import UserInfo \n 
from Session import Session \n 
import UCache \n 
import MsgBox \n 
import xmpp \n 
import modes \n 
import Util \n 
import traceback \n 
from xmpp . features import NoRoute \n 
__disco_info_ns__ = \n 
__disco_items_ns__ = \n 
__vcard_ns__ = \n 
STEAL_AFTER_SEEN = 3 \n 
def elem_to_str ( elem ) : \n 
~~ class XMPPServer ( xmpp . Plugin ) : \n 
def __init__ ( self , rosters , host ) : \n 
~~~ self . probed = False \n 
self . _closed = False \n 
self . rosters = rosters \n 
self . _session = None \n 
self . rosters . set_resources ( self . get_resources ( ) ) \n 
self . _fixedjid = UCache . UCache . formalize_jid ( unicode ( self . authJID ) ) \n 
self . _userid = self . _fixedjid . partition ( ) [ 0 ] . encode ( "gbk" ) \n 
if ( not self . rosters . allow_login ( self . authJID . bare ) ) : \n 
self . stream_error ( , ) \n 
if self . authJID . resource [ : - 8 ] != "Resource" and len ( self . authJID . resource ) > 8 : \n 
~~~ routes = self . routes ( self . authJID . bare ) \n 
for route in routes : \n 
~~~ jid = route [ 0 ] \n 
if jid . resource [ : - 8 ] == self . authJID . resource [ : - 8 ] : \n 
~~~ if jid . resource != self . authJID . resource : \n 
route [ 1 ] . stream_error ( , ) \n 
~~ ~~ else : \n 
~~ ~~ ~~ except NoRoute : \n 
~~ self . _user = UserManager . UserManager . LoadUser ( self . _userid ) \n 
if ( self . _user == None ) : \n 
~~ self . _peer_addr = self . getpeername ( ) \n 
self . _session = Session ( self . _user , self . _peer_addr [ 0 ] ) \n 
self . _session . RecordLogin ( ) \n 
self . _userinfo = self . _session . Register ( ) \n 
self . _loginid = self . _session . utmpent \n 
self . _hostname = host \n 
self . bind ( xmpp . ReceivedCloseStream , self . recv_close ) \n 
self . bind ( xmpp . StreamClosed , self . stream_closed ) \n 
self . bind ( xmpp . SentCloseStream , self . sent_close ) \n 
self . rosters . register_conn ( self ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
if self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) is None : \n 
~~~ self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msgbox . GetMsgCount ( all = False ) - msgbox . GetUnreadCount ( ) ) \n 
~~ self . check_msg ( ) \n 
~~ def get_loginid ( self ) : \n 
~~~ return self . _loginid \n 
~~ def recv_close ( self ) : \n 
return self . close ( ) \n 
~~ def stream_closed ( self ) : \n 
~~ def sent_close ( self ) : \n 
~~ def close ( self ) : \n 
~~~ if ( self . _closed ) : \n 
~~ self . _closed = True \n 
if ( self . _session ) : \n 
~~~ self . _session . Unregister ( ) \n 
~~ self . unbind_res ( ) \n 
self . rosters . unregister_conn ( self ) \n 
~~ @ xmpp . iq ( ) \n 
def ping ( self , iq ) : \n 
self . refresh ( ) \n 
return self . iq ( , iq ) \n 
~~ @ xmpp . stanza ( ) \n 
def message ( self , elem ) : \n 
to_jid = elem . get ( ) \n 
from_jid = elem . get ( ) \n 
if ( from_jid == None ) : \n 
~~~ return \n 
~~ text_body = None \n 
for child in elem : \n 
~~~ if ( child . tag . endswith ( ) ) : \n 
~~~ text_body = child . text \n 
~~ ~~ if ( text_body == None ) : \n 
~~ ret = self . rosters . send_msg ( from_jid , to_jid , text_body ) \n 
if ( ret <= 0 ) : \n 
errors = { \n 
if ( ret in errors ) : \n 
~~~ elem = self . E . message ( { : to_jid , \n 
: from_jid , \n 
: } , \n 
self . E . body ( errors [ ret ] ) ) \n 
self . recv ( from_jid , elem ) \n 
~~ ~~ ~~ def make_jid ( self , userid ) : \n 
~~~ return "%s@%s" % ( userid , self . _hostname ) \n 
~~ def refresh ( self ) : \n 
~~~ self . _userinfo . freshtime = int ( time . time ( ) ) \n 
self . _userinfo . save ( ) \n 
~~ def ping_result ( self , iq ) : \n 
~~~ self . refresh ( ) \n 
~~ def ping_client ( self ) : \n 
~~~ pingelem = self . E . ping ( xmlns = ) \n 
return self . iq ( , self . ping_result , pingelem ) \n 
~~ except Exception as e : \n 
Log . debug ( traceback . format_exc ( ) ) \n 
return False \n 
~~ ~~ def get_uid ( self ) : \n 
~~~ return self . _user . GetUID ( ) \n 
~~ def recv_msg ( self , from_ , msgtext ) : \n 
~~~ elem = self . E . message ( { : from_ , : unicode ( self . authJID ) } , \n 
self . E . body ( msgtext ) ) \n 
self . recv ( unicode ( self . authJID ) , elem ) \n 
~~ def check_msg ( self ) : \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
my_pid = os . getpid ( ) \n 
xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n 
if xmpp_read > msg_count : \n 
~~~ xmpp_read = 0 \n 
self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msg_count ) \n 
if xmpp_read < msg_count : \n 
~~~ return xmpp_read \n 
~~~ return - 1 \n 
~~ ~~ def deliver_msg ( self , start ) : \n 
for i in range ( start , msg_count ) : \n 
~~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if msghead . topid == my_pid : \n 
~~~ msgtext = msgbox . LoadMsgText ( msghead ) \n 
self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n 
~~ ~~ ~~ def steal_msg ( self ) : \n 
msg_unread = msgbox . GetUnreadCount ( ) \n 
read_count = msg_count - msg_unread \n 
term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n 
term_stealed = self . rosters . get_term_stealed ( self . get_uid ( ) ) \n 
all_xmpp = True \n 
new_unread = { } \n 
for i in range ( read_count - 1 , msg_count ) : \n 
~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if i >= read_count and all_xmpp : \n 
~~~ if msghead . topid == my_pid : \n 
~~~ msgbox . GetUnreadMsg ( ) \n 
~~~ all_xmpp = False \n 
~~ ~~ if msghead . topid == my_pid : \n 
~~~ session = self . rosters . find_session ( self . authJID . bare , msghead . topid ) \n 
if session is None or session . get_mode ( ) != modes . MSG : \n 
~~ if msghead . topid not in new_unread : \n 
new_unread [ msghead . topid ] = i \n 
~~ ~~ final_unread = { } \n 
to_steal = { } \n 
to_steal_begin = msg_count \n 
for pid in term_read : \n 
~~~ if pid in new_unread : \n 
~~~ if new_unread [ pid ] == term_read [ pid ] [ 0 ] : \n 
~~~ final_unread [ pid ] = ( term_read [ pid ] [ 0 ] , term_read [ pid ] [ 1 ] + 1 ) \n 
if final_unread [ pid ] [ 1 ] > STEAL_AFTER_SEEN : \n 
~~~ to_steal [ pid ] = final_unread [ pid ] \n 
if pid in term_stealed : \n 
~~~ steal_begin = max ( final_unread [ pid ] [ 0 ] , term_stealed [ pid ] + 1 ) \n 
~~~ steal_begin = final_unread [ pid ] [ 0 ] \n 
~~ if steal_begin < to_steal_begin : \n 
~~~ to_steal_begin = steal_begin \n 
~~ ~~ ~~ else : \n 
~~~ final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
pass \n 
~~ ~~ for pid in new_unread : \n 
~~~ if pid not in term_read : \n 
final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
~~ ~~ if to_steal : \n 
for i in range ( to_steal_begin , msg_count ) : \n 
msgbox . GetUnreadMsg ( ) \n 
~~ elif msghead . topid in to_steal : \n 
~~~ if msghead . topid not in term_stealed or i > term_stealed [ msghead . topid ] : \n 
msgtext = msgbox . LoadMsgText ( msghead ) \n 
term_stealed [ msghead . topid ] = i \n 
~~ ~~ ~~ ~~ self . rosters . set_term_read ( self . get_uid ( ) , final_unread ) \n 
def presence ( self , elem ) : \n 
if self . authJID == elem . get ( ) : \n 
~~~ if ( elem . get ( ) == None or ( not self . authJID . match_bare ( elem . get ( ) ) ) ) : \n 
~~~ return self . send_presence ( elem ) \n 
~~ ~~ self . recv_presence ( elem ) \n 
~~ def send_presence ( self , elem ) : \n 
direct = elem . get ( ) \n 
if not direct : \n 
~~~ self . rosters . broadcast ( self , elem ) \n 
if elem . get ( ) != : \n 
~~~ self . recv_presence ( elem ) \n 
~~ if not self . probed : \n 
~~~ self . probed = True \n 
self . rosters . probe ( self ) \n 
~~ ~~ elif not self . rosters . send ( self , direct , elem ) : \n 
~~~ self . send ( direct , elem ) \n 
~~ ~~ def recv_presence ( self , elem ) : \n 
if not self . rosters . recv ( self , elem ) : \n 
self . write ( elem ) \n 
~~ ~~ @ xmpp . iq ( ) \n 
def roster ( self , iq ) : \n 
roster = self . rosters . get ( self ) \n 
method = getattr ( self , % iq . get ( ) ) \n 
return method and method ( iq , roster ) \n 
~~ def get_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( { : } ) \n 
for item in roster . items ( ) : \n 
~~~ query . append ( item ) \n 
~~ return self . iq ( , iq , query ) \n 
~~ def set_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( xmlns = ) \n 
for item in iq [ 0 ] : \n 
~~~ result = roster . set ( item ) \n 
if result is not None : \n 
~~~ query . append ( result ) \n 
~~ ~~ if len ( query ) > 0 : \n 
~~~ self . push ( roster , query ) \n 
~~ return self . iq ( , iq ) \n 
~~ def push ( self , roster , query ) : \n 
for jid in roster . requests ( ) : \n 
~~~ for ( to , route ) in self . routes ( jid ) : \n 
~~~ route . iq ( , self . ignore , query ) \n 
~~ ~~ ~~ def ignore ( self , iq ) : \n 
def vcard ( self , iq ) : \n 
if iq . get ( ) == : \n 
~~~ if ( iq . get ( ) == None ) : \n 
~~~ target = iq . get ( ) \n 
~~ form_target = UCache . UCache . formalize_jid ( target ) \n 
name = form_target . partition ( ) [ 0 ] \n 
user = UserManager . UserManager . LoadUser ( name ) \n 
info = user . GetInfo ( ) \n 
desc = % ( info [ ] , info [ ] , info [ ] , \n 
info [ ] , info [ ] , info [ ] , info [ ] ) \n 
if ( in info ) : \n 
~~~ desc += "Plan:\\r%s" % ( info [ ] . replace ( , ) ) \n 
~~ vcard = self . E . vCard ( { : } , \n 
self . E ( , name ) , \n 
self . E ( , Util . Util . RemoveTags ( info [ ] ) ) , \n 
self . E ( , Util . Util . RemoveTags ( desc ) ) ) \n 
if ( iq . get ( ) == None ) : \n 
~~~ return self . iq ( , iq , vcard ) \n 
~~~ return self . iq ( , iq , vcard , { : iq . get ( ) } ) \n 
~~ ~~ ~~ @ xmpp . iq ( % __disco_info_ns__ ) \n 
def disco_info ( self , iq ) : \n 
target = iq . get ( ) \n 
if ( target . find ( ) < 0 ) : \n 
~~~ query = self . E . query ( { : __disco_info_ns__ } , \n 
self . E . identity ( { : , \n 
: , \n 
: Config . Config . GetString ( , ) , \n 
} ) ) \n 
features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n 
for feature in features : \n 
~~~ query . append ( self . E . feature ( { : feature } ) ) \n 
~~ ~~ return self . iq ( , iq , query , { : target } ) \n 
~~ @ xmpp . iq ( % __disco_items_ns__ ) \n 
def disco_items ( self , iq ) : \n 
~~~ query = self . E . query ( { : __disco_items_ns__ } ) \n 
~~ return self . iq ( , iq , query , { : target } ) \n 
### \n 
~~ ~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
from hpOneView . common import uri \n 
import hpOneView . profile as profile \n 
def acceptEULA ( con ) : \n 
~~~ con . get_eula_status ( ) \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
print ( e ) \n 
~~ ~~ def login ( con , credential ) : \n 
~~~ con . login ( credential ) \n 
~~ ~~ def get_eg_uri_from_arg ( srv , name ) : \n 
~~~ if srv and name : \n 
~~~ if name . startswith ( ) and uri [ ] in name : \n 
~~~ return name \n 
~~~ egs = srv . get_enclosure_groups ( ) \n 
for eg in egs : \n 
~~~ if eg [ ] == name : \n 
~~~ return eg [ ] \n 
~~ ~~ ~~ ~~ return None \n 
~~ def get_sht_from_arg ( srv , name ) : \n 
~~~ shts = srv . get_server_hardware_types ( ) \n 
for sht in shts : \n 
~~~ if sht [ ] == name : \n 
~~~ return sht \n 
~~ def define_profile_template ( \n 
srv , \n 
name , \n 
desc , \n 
sp_desc , \n 
server_hwt , \n 
enc_group , \n 
affinity , \n 
hide_flexnics , \n 
conn_list , \n 
fw_settings , \n 
boot , \n 
bootmode ) : \n 
~~~ if conn_list : \n 
~~~ conn = json . loads ( open ( conn_list ) . read ( ) ) \n 
~~~ conn = [ ] \n 
~~ profile_template = srv . create_server_profile_template ( \n 
name = name , \n 
description = desc , \n 
serverProfileDescription = sp_desc , \n 
serverHardwareTypeUri = server_hwt , \n 
enclosureGroupUri = enc_group , \n 
affinity = affinity , \n 
hideUnusedFlexNics = hide_flexnics , \n 
profileConnectionV4 = conn , \n 
firmwareSettingsV3 = fw_settings , \n 
bootSettings = boot , \n 
bootModeSetting = bootmode ) \n 
if in profile_template : \n 
~~~ print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( ) \n 
for connection in profile_template [ ] : \n 
~~~ print ( , connection [ ] ) \n 
print ( , connection [ ] ) \n 
~~ print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] , ) \n 
~~~ pprint ( profile_template ) \n 
~~ ~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
parser . add_argument ( , dest = , \n 
required = True , \n 
required = False , \n 
required = False , choices = [ , ] , \n 
action = , \n 
nargs = , \n 
choices = [ , , ] , \n 
choices = [ , , , \n 
, ] , \n 
args = parser . parse_args ( ) \n 
credential = { : args . user , : args . passwd } \n 
con = hpov . connection ( args . host ) \n 
srv = hpov . servers ( con ) \n 
sts = hpov . settings ( con ) \n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
eg_uri = get_eg_uri_from_arg ( srv , args . enc_group ) \n 
sht = get_sht_from_arg ( srv , args . server_hwt ) \n 
fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n 
boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n 
args . boot_order , args . boot_mode , args . pxe ) \n 
define_profile_template ( srv , \n 
args . name , \n 
args . desc , \n 
args . sp_desc , \n 
sht [ ] , \n 
eg_uri , \n 
args . affinity , \n 
args . hide_flexnics , \n 
args . conn_list , \n 
bootmode ) \n 
~~ if __name__ == : \n 
~~~ import argparse \n 
sys . exit ( main ( ) ) \n 
~~ from __future__ import print_function \n 
~~ ~~ def get_address_pools ( con , srv , types ) : \n 
~~~ if types == or types == : \n 
~~~ vmac = srv . get_vmac_pool ( ) \n 
for key in sorted ( vmac ) : \n 
~~~ print ( . format ( key , vmac [ key ] ) ) \n 
~~ if in vmac : \n 
~~~ for uri in vmac [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
~~ ~~ ~~ if types == or types == : \n 
~~~ vwwn = srv . get_vwwn_pool ( ) \n 
for key in sorted ( vwwn ) : \n 
~~~ print ( . format ( key , vwwn [ key ] ) ) \n 
~~ if in vwwn : \n 
~~~ for uri in vwwn [ ] : \n 
~~~ vsn = srv . get_vsn_pool ( ) \n 
for key in sorted ( vsn ) : \n 
~~~ print ( . format ( key , vsn [ key ] ) ) \n 
~~ if in vsn : \n 
~~~ for uri in vsn [ ] : \n 
~~ ~~ ~~ ~~ def main ( ) : \n 
choices = [ , , , ] , default = , \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
get_address_pools ( con , srv , args . types ) \n 
~~~ import sys \n 
import argparse \n 
~~ ~~ def get_managed_sans ( fcs ) : \n 
~~~ sans = fcs . get_managed_sans ( ) \n 
pprint ( sans ) \n 
~~ def main ( ) : \n 
fcs = hpov . fcsans ( con ) \n 
get_managed_sans ( fcs ) \n 
~~ ~~ def getpolicy ( sts ) : \n 
~~~ policy = sts . get_storage_vol_template_policy ( ) \n 
print ( policy [ ] ) \n 
getpolicy ( sts ) \n 
from __future__ import print_function \n 
__title__ = \n 
__version__ = \n 
__copyright__ = \n 
__license__ = \n 
__status__ = \n 
from hpOneView . common import * \n 
from hpOneView . connection import * \n 
from hpOneView . activity import * \n 
from hpOneView . exceptions import * \n 
class servers ( object ) : \n 
~~~ def __init__ ( self , con ) : \n 
~~~ self . _con = con \n 
self . _activity = activity ( con ) \n 
########################################################################### \n 
~~ def get_connections ( self , filter = ) : \n 
return get_members ( self . _con . get ( uri [ ] + filter ) ) \n 
~~ def get_connection ( self , server ) : \n 
body = self . _con . get ( server [ ] ) \n 
return body \n 
~~ def get_server_by_bay ( self , baynum ) : \n 
~~~ servers = get_members ( self . _con . get ( uri [ ] ) ) \n 
for server in servers : \n 
~~~ if server [ ] == baynum : \n 
~~~ return server \n 
~~ ~~ ~~ def get_server_by_name ( self , name ) : \n 
~~~ if server [ ] == name : \n 
~~ ~~ ~~ def get_available_servers ( self , server_hardware_type = None , \n 
enclosure_group = None , server_profile = None ) : \n 
~~~ filters = [ ] \n 
if server_hardware_type : \n 
~~~ filters . append ( + server_hardware_type [ ] ) \n 
~~ if enclosure_group : \n 
~~~ filters . append ( + enclosure_group [ ] ) \n 
~~ if server_profile : \n 
~~~ filters . append ( + server_profile [ ] ) \n 
~~ query_string = \n 
if filters : \n 
~~~ query_string = + . join ( filters ) \n 
~~ return self . _con . get ( uri [ ] + query_string ) \n 
~~ def get_servers ( self ) : \n 
~~~ return get_members ( self . _con . get ( uri [ ] ) ) \n 
~~ def get_utilization ( self , server ) : \n 
body = self . _con . get ( server [ ] + ) \n 
~~ def get_env_conf ( self , server ) : \n 
~~ def set_server_powerstate ( self , server , state , force = False , blocking = True , \n 
verbose = False ) : \n 
~~~ if state == and force is True : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == and force is False : \n 
~~ elif state == : \n 
~~ task , body = self . _con . put ( server [ ] + , powerRequest ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 60 , verbose = verbose ) \n 
~~ return task \n 
~~ def delete_server ( self , server , force = False , blocking = True , verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( server [ ] + ) \n 
~~~ task , body = self . _con . delete ( server [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ def update_server ( self , server ) : \n 
~~~ task , body = self . _con . put ( server [ ] , server ) \n 
~~ def add_server ( self , server , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , server ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
server = self . _con . get ( entity [ ] ) \n 
return server \n 
~~ ~~ return task \n 
~~ def get_server_schema ( self ) : \n 
return self . _con . get ( uri [ ] + ) \n 
~~ def get_bios ( self , server ) : \n 
return self . _con . get ( server [ ] + ) \n 
~~ def get_ilo_sso_url ( self , server ) : \n 
~~ def get_java_remote_console_url ( self , server ) : \n 
~~ def get_remote_console_url ( self , server ) : \n 
~~ def get_server_hardware_types ( self ) : \n 
body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
~~ def remove_server_hardware_type ( self , server_hardware_type , force = False , blocking = True , verbose = False ) : \n 
if force : \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] + ) \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] ) \n 
~~ def get_server_type_schema ( self ) : \n 
~~ def get_server_hardware_type ( self , server_type ) : \n 
return self . _con . get ( server_type [ ] ) \n 
~~ def set_server_hardware_type ( self , server_hardware_type , name , description ) : \n 
request = make_server_type_dict ( name , description ) \n 
task , body = self . _con . put ( server_hardware_type [ ] , request ) \n 
return task \n 
~~ def create_server_profile ( self , \n 
affinity = , \n 
biosSettings = None , \n 
bootSettings = None , \n 
bootModeSetting = None , \n 
profileConnectionV4 = None , \n 
description = None , \n 
firmwareSettingsV3 = None , \n 
hideUnusedFlexNics = True , \n 
localStorageSettingsV3 = None , \n 
macType = , \n 
name = None , \n 
sanStorageV3 = None , \n 
serialNumber = None , \n 
serialNumberType = , \n 
serverHardwareTypeUri = None , \n 
serverHardwareUri = None , \n 
serverProfileTemplateUri = None , \n 
uuid = None , \n 
wwnType = , \n 
blocking = True , verbose = False ) : \n 
profile = make_ServerProfileV5 ( affinity , biosSettings , bootSettings , \n 
bootModeSetting , profileConnectionV4 , \n 
description , firmwareSettingsV3 , \n 
hideUnusedFlexNics , \n 
localStorageSettingsV3 , macType , name , \n 
sanStorageV3 , serialNumber , \n 
serialNumberType , serverHardwareTypeUri , \n 
serverHardwareUri , \n 
serverProfileTemplateUri , uuid , wwnType ) \n 
task , body = self . _con . post ( uri [ ] , profile ) \n 
if profile [ ] is None : \n 
~~~ tout = 600 \n 
~~~ tout = 3600 \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
profile = self . _con . get ( entity [ ] ) \n 
return profile \n 
~~ def post_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~ def remove_server_profile ( self , profile , force = False , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile [ ] + ) \n 
~~~ task , body = self . _con . delete ( profile [ ] ) \n 
~~ def get_server_profiles ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
~~ def update_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile [ ] , profile ) \n 
~~~ if profile [ ] [ ] is None : \n 
~~ ~~ except Exception : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileResource [ ] ) \n 
~~ def update_server_profile_from_template ( self , profile , blocking = True , verbose = False ) : \n 
~~~ patch_request = [ { : , : , : } ] \n 
task , body = self . _con . patch ( profile [ ] , patch_request ) \n 
~~ ~~ def get_server_profile_by_name ( self , name ) : \n 
~~~ body = self . _con . get_entity_byfield ( uri [ ] , , name ) \n 
~~ def get_profile_message ( self , profile ) : \n 
message = self . _con . get ( profile [ ] + ) \n 
return message \n 
~~ def get_profile_compliance_preview ( self , profile ) : \n 
return self . _con . get ( profile [ ] + ) \n 
~~ def create_server_profile_template ( \n 
self , \n 
serverProfileDescription = None , \n 
enclosureGroupUri = None , \n 
affinity = None , \n 
hideUnusedFlexNics = None , \n 
blocking = True , \n 
profile_template = make_ServerProfileTemplateV1 ( name , \n 
description , \n 
serverProfileDescription , \n 
serverHardwareTypeUri , \n 
enclosureGroupUri , \n 
profileConnectionV4 , \n 
firmwareSettingsV3 , \n 
bootSettings , \n 
bootModeSetting ) \n 
task , body = self . _con . post ( uri [ ] , profile_template ) \n 
tout = 600 \n 
profile_template = self . _con . get ( entity [ ] ) \n 
return profile_template \n 
~~ def remove_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile_template [ ] ) \n 
~~ return body \n 
~~ def get_server_profile_templates ( self ) : \n 
~~ def get_server_profile_template_by_name ( self , name ) : \n 
~~ def update_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile_template [ ] , profile_template ) \n 
~~ profileTemplateResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileTemplateResource [ ] ) \n 
~~ def get_server_profile_from_template ( self , profile_template ) : \n 
~~~ profile = self . _con . get ( profile_template [ ] + ) \n 
~~ def get_enclosures ( self ) : \n 
~~ def add_enclosure ( self , enclosure , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , enclosure ) \n 
if enclosure [ ] is : \n 
~~ elif enclosure [ ] is None : \n 
enclosure = self . _con . get ( entity [ ] ) \n 
return enclosure \n 
~~ def remove_enclosure ( self , enclosure , force = False , blocking = True , \n 
~~~ task , body = self . _con . delete ( enclosure [ ] + ) \n 
~~~ task , body = self . _con . delete ( enclosure [ ] ) \n 
~~ def create_enclosure_group ( self , associatedLIGs , name , \n 
powerMode = ) : \n 
egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n 
task , body = self . _con . post ( uri [ ] , egroup ) \n 
~~ def delete_enclosure_group ( self , egroup ) : \n 
~~~ self . _con . delete ( egroup [ ] ) \n 
~~ def get_enclosure_groups ( self ) : \n 
~~ def update_enclosure_group ( self , enclosuregroup ) : \n 
~~~ task , body = self . _con . put ( enclosuregroup [ ] , enclosuregroup ) \n 
~~ def get_pool ( self , pooltype ) : \n 
~~~ body = self . _con . get ( uri [ ] + + pooltype ) \n 
~~ def get_vmac_pool ( self ) : \n 
~~ def get_vwwn_pool ( self ) : \n 
~~ def get_vsn_pool ( self ) : \n 
~~ def get_profile_networks ( self ) : \n 
~~ def get_profile_schema ( self ) : \n 
~~~ return self . _con . get ( uri [ ] ) \n 
~~ def get_profile_available_servers ( self ) : \n 
~~ def get_profile_available_storage_systems ( self ) : \n 
~~ def get_profile_ports ( self ) : \n 
~~ def allocate_pool_ids ( self , url , count ) : \n 
~~~ allocatorUrl = % url \n 
allocatorBody = { : count } \n 
task , body = self . _con . put ( allocatorUrl , allocatorBody ) \n 
~~ def release_pool_ids ( self , url , idList ) : \n 
~~~ collectorUrl = % url \n 
collectorBody = { : idList } \n 
task , body = self . _con . put ( collectorUrl , collectorBody ) \n 
~~ def allocate_range_ids ( self , allocatorUrl , count ) : \n 
~~~ task , body = self . _con . put ( allocatorUrl , { : count } ) \n 
~~ def release_range_ids ( self , collectorUrl , idList ) : \n 
~~~ task , body = self . _con . put ( collectorUrl , { : idList } ) \n 
~~ def enable_range ( self , url ) : \n 
~~~ prange = self . _con . get ( url ) \n 
prange [ ] = True \n 
task , body = self . _con . put ( url , prange ) \n 
~~ def disable_range ( self , url ) : \n 
prange [ ] = False \n 
~~ ~~ from . constants import MILLI_MICROS , SECOND_MICROS , MINUTE_MICROS \n 
import calendar \n 
from datetime import datetime \n 
from dateutil import parser \n 
from dateutil . tz import tzlocal \n 
from . error import TimeConstructionError \n 
from . sanedelta import SaneDelta \n 
import pytz \n 
MICROS_TRANSLATIONS = ( \n 
( ( , , , , ) , MINUTE_MICROS ) , \n 
( ( , , , , ) , SECOND_MICROS ) , \n 
( ( , , , , ) , MILLI_MICROS ) , \n 
( ( , , , , ) , 1 ) ) \n 
MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n 
class SaneTime ( object ) : \n 
def __init__ ( self , * args , ** kwargs ) : \n 
super ( time , self ) . __init__ ( ) \n 
uss = set ( ) \n 
tzs = set ( ) \n 
naive_dt = None \n 
avoid_localize = False \n 
for k , v in kwargs . iteritems ( ) : \n 
~~~ if k in ( , ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( v ) ) \n 
~~ elif k in MICROS_TRANSLATION_HASH : \n 
~~~ uss . add ( MICROS_TRANSLATION_HASH [ k ] * v ) \n 
~~ ~~ args = list ( args ) \n 
if len ( args ) > 2 and len ( args ) <= 8 : \n 
~~~ args = [ datetime ( * args ) ] \n 
~~ if len ( args ) == 2 : \n 
~~~ tzs . add ( SaneTime . to_timezone ( args . pop ( ) ) ) \n 
~~ if len ( args ) == 1 : \n 
~~~ arg = args . pop ( ) \n 
if hasattr ( arg , ) : \n 
~~~ uss . add ( int ( arg ) ) \n 
if hasattr ( arg , ) : tzs . add ( arg . tz ) \n 
~~ elif isinstance ( arg , basestring ) : \n 
~~~ parts = arg . strip ( ) . split ( ) \n 
if len ( parts ) > 1 and parts [ - 1 ] . startswith ( ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( parts [ - 1 ] [ 1 : ] ) ) \n 
arg = . join ( parts [ : - 1 ] ) \n 
~~ except : pass \n 
~~ utc = arg . endswith ( ) or arg . endswith ( ) \n 
arg = parser . parse ( arg ) \n 
~~~ if utc : \n 
~~~ tzs . add ( pytz . utc ) \n 
arg = arg . replace ( tzinfo = None ) \n 
~~~ arg = arg . replace ( tzinfo = None ) \n 
~~~ avoid_localize = True \n 
arg = arg . astimezone ( pytz . utc ) . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if type ( arg ) == datetime : \n 
~~~ naive_dt = arg \n 
if naive_dt . tzinfo : \n 
~~~ tzs . add ( SaneTime . to_timezone ( str ( naive_dt . tzinfo ) ) ) \n 
naive_dt = naive_dt . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if len ( tzs ) > 1 : \n 
~~ self . tz = len ( tzs ) and tzs . pop ( ) or pytz . utc \n 
if naive_dt : \n 
~~~ if avoid_localize : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( naive_dt ) ) \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( self . tz . localize ( naive_dt ) . astimezone ( pytz . utc ) ) ) \n 
~~ ~~ if len ( uss ) == 0 : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( datetime . utcnow ( ) ) ) \n 
~~ if len ( uss ) > 1 : \n 
~~ self . us = uss . pop ( ) \n 
if len ( args ) > 0 : \n 
~~ ~~ @ property \n 
def ms ( self ) : return self . us / MILLI_MICROS \n 
epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n 
def s ( self ) : return self . us / SECOND_MICROS \n 
epoch_seconds = epoch_secs = seconds = secs = s \n 
def m ( self ) : return self . us / MINUTE_MICROS \n 
epoch_minutes = epoch_mins = minutes = mins = m \n 
def micros ( self ) : return self . us \n 
epoch_microseconds = epoch_micros = microseconds = micros \n 
def tz_name ( self ) : return self . tz . zone \n 
def tz_abbr ( self ) : return self . tz . _tzname \n 
def set_tz ( self , tz ) : \n 
~~~ self . tz = self . __class__ . to_timezone ( tz ) ; return self \n 
~~ def with_tz ( self , tz ) : \n 
~~~ return self . __class__ ( self . us , tz ) \n 
~~ @ property \n 
def _tuple ( self ) : return ( self . us , self . tz ) \n 
def strftime ( self , * args , ** kwargs ) : return self . datetime . strftime ( * args , ** kwargs ) \n 
def __cmp__ ( self , other ) : \n 
~~~ if not hasattr ( other , ) : other = SaneTime ( other ) \n 
return cmp ( self . us , int ( other ) ) \n 
~~ def __hash__ ( self ) : return self . us . __hash__ ( ) \n 
def __add__ ( self , operand ) : \n 
~~~ if not hasattr ( operand , ) : operand = SaneTime ( operand ) \n 
return self . __class__ ( self . us + int ( operand ) , tz = self . tz ) \n 
~~ def __sub__ ( self , operand ) : \n 
if isinstance ( operand , SaneTime ) : return SaneDelta ( self . us - int ( operand ) ) \n 
return self . __add__ ( - int ( operand ) ) \n 
~~ def __mul__ ( self , operand ) : \n 
~~~ return self . us * int ( operand ) \n 
~~ def __div__ ( self , operand ) : \n 
~~~ return self . us / int ( operand ) \n 
~~ def __int__ ( self ) : return int ( self . us ) \n 
def __long__ ( self ) : return long ( self . us ) \n 
def __repr__ ( self ) : return u"SaneTime(%s,%s)" % ( self . us , repr ( self . tz ) ) \n 
def __str__ ( self ) : return unicode ( self ) . encode ( ) \n 
def __unicode__ ( self ) : \n 
~~~ dt = self . datetime \n 
micros = u".%06d" % dt . microsecond if dt . microsecond else \n 
~~ def clone ( self ) : \n 
return self . __class__ ( self . us , self . tz ) \n 
def ny_str ( self ) : \n 
return self . ny_ndt . strftime ( ) \n 
def utc_datetime ( self ) : return SaneTime . us_to_utc_datetime ( self . us ) \n 
utc_dt = utc_datetime \n 
def utc_naive_datetime ( self ) : return self . utc_datetime . replace ( tzinfo = None ) \n 
utc_ndt = utc_naive_datetime \n 
def to_timezoned_datetime ( self , tz ) : return self . utc_datetime . astimezone ( SaneTime . to_timezone ( tz ) ) \n 
def to_timezoned_naive_datetime ( self , tz ) : return self . to_timezoned_datetime ( tz ) . replace ( tzinfo = None ) \n 
def datetime ( self ) : return self . to_timezoned_datetime ( self . tz ) \n 
dt = datetime \n 
def naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( self . tz ) \n 
ndt = naive_datetime \n 
def ny_datetime ( self ) : return self . to_timezoned_datetime ( ) \n 
ny_dt = ny_datetime \n 
def ny_naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( ) \n 
ny_ndt = ny_naive_datetime \n 
def year ( self ) : return self . dt . year \n 
def month ( self ) : return self . dt . month \n 
def day ( self ) : return self . dt . day \n 
def hour ( self ) : return self . dt . hour \n 
def minute ( self ) : return self . dt . minute \n 
def second ( self ) : return self . dt . second \n 
def microsecond ( self ) : return self . dt . microsecond \n 
@ classmethod \n 
def utc_datetime_to_us ( kls , dt ) : \n 
~~~ return calendar . timegm ( dt . timetuple ( ) ) * 1000 ** 2 + dt . microsecond \n 
~~ @ classmethod \n 
def us_to_utc_datetime ( kls , us ) : \n 
~~~ return pytz . utc . localize ( datetime . utcfromtimestamp ( us / 10 ** 6 ) ) . replace ( microsecond = us % 10 ** 6 ) \n 
def to_timezone ( kls , tz ) : \n 
~~~ if not isinstance ( tz , basestring ) : return tz \n 
return pytz . timezone ( tz ) \n 
~~ ~~ def ntime ( * args , ** kwargs ) : \n 
~~~ if args : \n 
~~~ if args [ 0 ] is None : return None \n 
~~ elif kwargs : \n 
~~~ if None in [ v for k , v in kwargs . iteritems ( ) if k != ] : return None \n 
~~ return SaneTime ( * args , ** kwargs ) \n 
~~ time = sanetime = SaneTime \n 
nsanetime = ntime \n 
from tastypie . authorization import Authorization \n 
from openpds . authentication import OAuth2Authentication \n 
from openpds . core . models import Profile , AuditEntry \n 
import settings \n 
import pdb \n 
class PDSAuthorization ( Authorization ) : \n 
~~~ audit_enabled = True \n 
scope = "" \n 
requester_uuid = "" \n 
def requester ( self ) : \n 
~~~ return self . requester_uuid \n 
~~ def trustWrapper ( self , datastore_owner ) : \n 
~~ def is_authorized ( self , request , object = None ) : \n 
~~~ authenticator = OAuth2Authentication ( self . scope ) \n 
if "datastore_owner__uuid" in request . REQUEST : \n 
~~~ authorized = True \n 
token = request . REQUEST [ "bearer_token" ] if "bearer_token" in request . REQUEST else request . META [ "HTTP_BEARER_TOKEN" ] \n 
datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n 
datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n 
self . requester_uuid = authenticator . get_userinfo_from_token ( token , self . scope ) \n 
if self . requester_uuid is False or self . requester_uuid is None or len ( self . requester_uuid ) == 0 : \n 
~~~ self . requester_uuid = "not-specified" \n 
authorized = False \n 
~~ self . trustWrapper ( datastore_owner ) \n 
~~~ if ( self . audit_enabled ) : \n 
#pdb.set_trace() \n 
~~~ audit_entry = AuditEntry ( token = token ) \n 
audit_entry . method = request . method \n 
audit_entry . scope = self . scope \n 
audit_entry . purpose = request . REQUEST [ "purpose" ] if "purpose" in request . REQUEST else "" \n 
audit_entry . system_entity_toggle = request . REQUEST [ "system_entity" ] if "system_entity" in request . REQUEST else False \n 
audit_entry . datastore_owner = datastore_owner \n 
audit_entry . requester , created = Profile . objects . get_or_create ( uuid = self . requester_uuid ) \n 
audit_entry . script = request . path \n 
audit_entry . save ( ) \n 
~~~ print e \n 
~~ return authorized \n 
~~ return False \n 
~~ def __init__ ( self , scope , audit_enabled = True ) : \n 
~~~ self . scope = scope \n 
self . audit_enabled = audit_enabled \n 
from django import template \n 
register = template . Library ( ) \n 
class VerbatimNode ( template . Node ) : \n 
~~~ def __init__ ( self , text ) : \n 
~~~ self . text = text \n 
~~ def render ( self , context ) : \n 
~~~ return self . text \n 
~~ ~~ @ register . tag \n 
def verbatim ( parser , token ) : \n 
~~~ text = [ ] \n 
while 1 : \n 
~~~ token = parser . tokens . pop ( 0 ) \n 
if token . contents == : \n 
~~~ break \n 
~~ if token . token_type == template . TOKEN_VAR : \n 
~~~ text . append ( ) \n 
~~ elif token . token_type == template . TOKEN_BLOCK : \n 
~~ text . append ( token . contents ) \n 
if token . token_type == template . TOKEN_VAR : \n 
~~ ~~ return VerbatimNode ( . join ( text ) ) \n 
~~ from django . shortcuts import render_to_response \n 
from django . template import RequestContext \n 
from gevent import monkey ; monkey . patch_all ( ) \n 
import gevent \n 
from ws4py . client . geventclient import WebSocketClient \n 
if __name__ == : \n 
~~~ ws = WebSocketClient ( , protocols = [ , ] ) \n 
ws . connect ( ) \n 
print ( ( ws . receive ( ) , ) ) \n 
def incoming ( ) : \n 
~~~ while True : \n 
~~~ m = ws . receive ( ) \n 
if m is not None : \n 
~~~ m = str ( m ) \n 
print ( ( m , len ( m ) ) ) \n 
if len ( m ) == 35 : \n 
~~~ ws . close ( ) \n 
break \n 
~~ def outgoing ( ) : \n 
~~~ for i in range ( 0 , 40 , 5 ) : \n 
~~~ ws . send ( "*" * i ) \n 
~~ ws . send ( "Foobar" ) \n 
~~ greenlets = [ \n 
gevent . spawn ( incoming ) , \n 
gevent . spawn ( outgoing ) , \n 
] \n 
gevent . joinall ( greenlets ) \n 
~~ import os \n 
from ws4py . framing import Frame , OPCODE_CONTINUATION , OPCODE_TEXT , OPCODE_BINARY , OPCODE_CLOSE , OPCODE_PING , OPCODE_PONG \n 
from ws4py . compat import unicode , py3k \n 
__all__ = [ , , , , \n 
, ] \n 
class Message ( object ) : \n 
~~~ def __init__ ( self , opcode , data = , encoding = ) : \n 
self . opcode = opcode \n 
self . _completed = False \n 
self . encoding = encoding \n 
if isinstance ( data , unicode ) : \n 
~~~ if not encoding : \n 
~~ data = data . encode ( encoding ) \n 
~~ elif isinstance ( data , bytearray ) : \n 
~~~ data = bytes ( data ) \n 
~~ elif not isinstance ( data , bytes ) : \n 
~~ self . data = data \n 
~~ def single ( self , mask = False ) : \n 
mask = os . urandom ( 4 ) if mask else None \n 
return Frame ( body = self . data , opcode = self . opcode , \n 
masking_key = mask , fin = 1 ) . build ( ) \n 
~~ def fragment ( self , first = False , last = False , mask = False ) : \n 
fin = 1 if last is True else 0 \n 
opcode = self . opcode if first is True else OPCODE_CONTINUATION \n 
return Frame ( body = self . data , \n 
opcode = opcode , masking_key = mask , \n 
fin = fin ) . build ( ) \n 
def completed ( self ) : \n 
return self . _completed \n 
~~ @ completed . setter \n 
def completed ( self , state ) : \n 
self . _completed = state \n 
~~ def extend ( self , data ) : \n 
if isinstance ( data , bytes ) : \n 
~~~ self . data += data \n 
~~~ self . data += bytes ( data ) \n 
~~ elif isinstance ( data , unicode ) : \n 
~~~ self . data += data . encode ( self . encoding ) \n 
~~ ~~ def __len__ ( self ) : \n 
~~~ return len ( self . __unicode__ ( ) ) \n 
~~ def __str__ ( self ) : \n 
~~~ if py3k : \n 
~~~ return self . data . decode ( self . encoding ) \n 
~~ return self . data \n 
~~ def __unicode__ ( self ) : \n 
~~ ~~ class TextMessage ( Message ) : \n 
~~~ def __init__ ( self , text = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_TEXT , text ) \n 
def is_binary ( self ) : \n 
def is_text ( self ) : \n 
~~~ return True \n 
~~ ~~ class BinaryMessage ( Message ) : \n 
~~~ def __init__ ( self , bytes = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_BINARY , bytes , encoding = None ) \n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . data ) \n 
~~ ~~ class CloseControlMessage ( Message ) : \n 
~~~ def __init__ ( self , code = 1000 , reason = ) : \n 
~~~ data = b"" \n 
if code : \n 
~~~ data += struct . pack ( "!H" , code ) \n 
~~ if reason is not None : \n 
~~~ if isinstance ( reason , unicode ) : \n 
~~~ reason = reason . encode ( ) \n 
~~ data += reason \n 
~~ Message . __init__ ( self , OPCODE_CLOSE , data , ) \n 
self . code = code \n 
self . reason = reason \n 
~~~ return self . reason . decode ( ) \n 
~~ return self . reason \n 
~~~ return self . reason . decode ( self . encoding ) \n 
~~ ~~ class PingControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_PING , data ) \n 
~~ ~~ class PongControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data ) : \n 
~~~ Message . __init__ ( self , OPCODE_PONG , data ) \n 
~~ ~~ import sys \n 
import base64 \n 
import urllib \n 
from struct import unpack \n 
from threading import Lock \n 
from binascii import hexlify \n 
from urlparse import urlparse \n 
from mod_python import apache \n 
from PyAuthenNTLM2 . ntlm_dc_proxy import NTLM_DC_Proxy \n 
from PyAuthenNTLM2 . ntlm_ad_proxy import NTLM_AD_Proxy \n 
use_basic_auth = True \n 
~~~ from PyAuthenNTLM2 . ntlm_client import NTLM_Client \n 
~~ except ImportError : \n 
~~~ use_basic_auth = False \n 
~~ class CacheConnections : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . _mutex = Lock ( ) \n 
self . _cache = { } \n 
~~~ return len ( self . _cache ) \n 
~~ def remove ( self , id ) : \n 
~~~ self . _mutex . acquire ( ) \n 
( proxy , ts ) = self . _cache . get ( id , ( None , None ) ) \n 
if proxy : \n 
~~~ proxy . close ( ) \n 
del self . _cache [ id ] \n 
~~ self . _mutex . release ( ) \n 
~~ def add ( self , id , proxy ) : \n 
self . _cache [ id ] = ( proxy , int ( time . time ( ) ) ) \n 
self . _mutex . release ( ) \n 
~~ def clean ( self ) : \n 
~~~ now = int ( time . time ( ) ) \n 
self . _mutex . acquire ( ) \n 
for id , conn in self . _cache . items ( ) : \n 
~~~ if conn [ 1 ] + 60 < now : \n 
~~~ conn [ 0 ] . close ( ) \n 
~~ ~~ self . _mutex . release ( ) \n 
~~ def has_key ( self , id ) : \n 
~~~ return self . _cache . has_key ( id ) \n 
~~ def get_proxy ( self , id ) : \n 
proxy = self . _cache [ id ] [ 0 ] \n 
return proxy \n 
~~ ~~ class CacheGroups : \n 
~~ def add ( self , group , user ) : \n 
if not self . _cache . has_key ( group ) : \n 
~~~ self . _cache [ group ] = { } \n 
~~ self . _cache [ group ] [ user ] = int ( time . time ( ) ) \n 
old = [ ] \n 
for group , members in self . _cache . items ( ) : \n 
~~~ for user in members : \n 
~~~ if members [ user ] + 3 * 60 * 60 < now : \n 
~~~ old . append ( ( group , user ) ) \n 
~~ ~~ ~~ for group , user in old : \n 
~~~ del self . _cache [ group ] [ user ] \n 
~~ def has ( self , group , user ) : \n 
~~~ if not self . _cache . has_key ( group ) : \n 
~~ return self . _cache [ group ] . has_key ( user ) \n 
~~ ~~ cache = CacheConnections ( ) \n 
cacheGroups = CacheGroups ( ) \n 
def ntlm_message_type ( msg ) : \n 
~~~ if not msg . startswith ( ) or len ( msg ) < 12 : \n 
~~ msg_type = unpack ( , msg [ 8 : 8 + 4 ] ) [ 0 ] \n 
if msg_type not in ( 1 , 2 , 3 ) : \n 
~~ return msg_type \n 
~~ def parse_ntlm_authenticate ( msg ) : \n 
~~~ \n 
NTLMSSP_NEGOTIATE_UNICODE = 0x00000001 \n 
idx = 28 \n 
length , offset = unpack ( , msg [ idx : idx + 8 ] ) \n 
domain = msg [ offset : offset + length ] \n 
idx += 8 \n 
username = msg [ offset : offset + length ] \n 
idx += 24 \n 
flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n 
if flags & NTLMSSP_NEGOTIATE_UNICODE : \n 
~~~ domain = str ( domain . decode ( ) ) \n 
username = str ( username . decode ( ) ) \n 
~~ return username , domain \n 
~~ def set_remote_user ( req , username , domain ) : \n 
~~~ format = req . get_options ( ) . get ( , ) . lower ( ) \n 
if format == : \n 
~~~ req . user = domain + + username \n 
~~~ req . user = username \n 
~~ ~~ def decode_http_authorization_header ( auth ) : \n 
ah = auth . split ( ) \n 
if len ( ah ) == 2 : \n 
~~~ b64 = base64 . b64decode ( ah [ 1 ] ) \n 
if ah [ 0 ] == : \n 
~~~ return ( , b64 ) \n 
~~ elif ah [ 0 ] == and use_basic_auth : \n 
~~~ ( user , password ) = b64 . split ( ) \n 
return ( , user , password ) \n 
~~ ~~ return False \n 
~~ def handle_unauthorized ( req ) : \n 
req . err_headers_out . add ( , ) \n 
if use_basic_auth : \n 
~~ req . err_headers_out . add ( , ) \n 
return apache . HTTP_UNAUTHORIZED \n 
~~ def connect_to_proxy ( req , type1 ) : \n 
~~~ domain = req . get_options ( ) [ ] \n 
pdc = req . get_options ( ) [ ] \n 
bdc = req . get_options ( ) . get ( , False ) \n 
~~ except KeyError , e : \n 
~~~ req . log_error ( % str ( e ) , apache . APLOG_CRIT ) \n 
raise \n 
~~ ntlm_challenge = None \n 
for server in ( pdc , bdc ) : \n 
~~~ if not server : continue \n 
~~~ if server . startswith ( ) : \n 
~~~ url = urlparse ( server ) \n 
decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n 
( url . netloc , domain , decoded_path ) , apache . APLOG_INFO ) \n 
proxy = NTLM_AD_Proxy ( url . netloc , domain , base = decoded_path ) \n 
~~~ req . log_error ( % \n 
( server , domain ) , apache . APLOG_INFO ) \n 
proxy = NTLM_DC_Proxy ( server , domain ) \n 
~~ ntlm_challenge = proxy . negotiate ( type1 ) \n 
~~ except Exception , e : \n 
~~~ req . log_error ( % ( server , str ( e ) ) , apache . APLOG_CRIT ) \n 
~~ if ntlm_challenge : break \n 
proxy . close ( ) \n 
~~ return ( proxy , ntlm_challenge ) \n 
~~ def handle_type1 ( req , ntlm_message ) : \n 
cache . remove ( req . connection . id ) \n 
cache . clean ( ) \n 
~~~ ( proxy , ntlm_challenge ) = connect_to_proxy ( req , ntlm_message ) \n 
~~~ return apache . HTTP_INTERNAL_SERVER_ERROR \n 
~~ cache . add ( req . connection . id , proxy ) \n 
~~ def check_authorization ( req , username , proxy ) : \n 
rules = . join ( req . requires ( ) ) . strip ( ) \n 
if rules == or cacheGroups . has ( rules , username ) : \n 
~~ groups = [ ] \n 
for r in req . requires ( ) : \n 
~~~ users = [ u . strip ( ) for u in r [ 5 : ] . split ( "," ) ] \n 
if username in users : \n 
( username , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
return True \n 
~~~ groups += [ g . strip ( ) for g in r [ 6 : ] . split ( "," ) ] \n 
~~ ~~ if groups : \n 
~~~ res = proxy . check_membership ( username , groups ) \n 
~~~ req . log_error ( % ( username , str ( groups ) , req . unparsed_uri , str ( e ) ) ) \n 
~~ if res : \n 
~~~ cacheGroups . add ( rules , username ) \n 
req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
~~ req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) ) \n 
( username , req . unparsed_uri ) ) \n 
~~ def handle_type3 ( req , ntlm_message ) : \n 
proxy = cache . get_proxy ( req . connection . id ) \n 
~~~ user , domain = parse_ntlm_authenticate ( ntlm_message ) \n 
if not domain : \n 
~~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
~~ result = proxy . authenticate ( ntlm_message ) \n 
user , domain = , \n 
result = False \n 
~~ if not result : \n 
~~~ cache . remove ( req . connection . id ) \n 
req . log_error ( % ( \n 
domain , user , req . unparsed_uri ) ) \n 
return handle_unauthorized ( req ) \n 
~~ req . log_error ( % ( user , domain , req . unparsed_uri ) , apache . APLOG_NOTICE ) \n 
set_remote_user ( req , user , domain ) \n 
result = check_authorization ( req , user , proxy ) \n 
if not result : \n 
~~~ return apache . HTTP_FORBIDDEN \n 
~~ req . connection . notes . add ( , req . user ) \n 
return apache . OK \n 
~~ def handle_basic ( req , user , password ) : \n 
req . log_error ( % ( req . unparsed_uri ) ) \n 
domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
client = NTLM_Client ( user , domain , password ) \n 
type1 = client . make_ntlm_negotiate ( ) \n 
~~~ ( proxy , type2 ) = connect_to_proxy ( req , type1 ) \n 
~~ client . parse_ntlm_challenge ( type2 ) \n 
type3 = client . make_ntlm_authenticate ( ) \n 
if not proxy . authenticate ( type3 ) : \n 
user , domain , req . unparsed_uri ) ) \n 
~~ req . connection . notes . add ( , user + password ) \n 
~~ def authenhandler ( req ) : \n 
req . connection . id , req . method , req . unparsed_uri , len ( cache ) ) , apache . APLOG_INFO ) \n 
auth_headers = req . headers_in . get ( , [ ] ) \n 
if not isinstance ( auth_headers , list ) : \n 
~~~ auth_headers = [ auth_headers ] \n 
~~ user = req . connection . notes . get ( , None ) \n 
if user : \n 
~~~ req . user = user \n 
if auth_headers : \n 
~~~ req . log_error ( % ( \n 
req . connection . id , req . method , req . clength , auth_headers ) , apache . APLOG_INFO ) \n 
if req . method != or req . clength > 0 : \n 
~~~ return apache . OK \n 
~~ ~~ if not auth_headers : \n 
~~~ return handle_unauthorized ( req ) \n 
~~ try : \n 
~~~ for ah in auth_headers : \n 
~~~ ah_data = decode_http_authorization_header ( ah ) \n 
if ah_data : \n 
~~ ~~ ~~ except : \n 
~~~ ah_data = False \n 
~~ if not ah_data : \n 
~~~ req . log_error ( % req . unparsed_uri , apache . APLOG_ERR ) \n 
return apache . HTTP_BAD_REQUEST \n 
~~ if ah_data [ 0 ] == : \n 
~~~ userpwd = req . connection . notes . get ( , None ) \n 
if userpwd : \n 
~~~ if userpwd != ah_data [ 1 ] + ah_data [ 2 ] : \n 
~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
set_remote_user ( req , ah_data [ 1 ] , domain ) \n 
~~ return handle_basic ( req , ah_data [ 1 ] , ah_data [ 2 ] ) \n 
~~~ ntlm_version = ntlm_message_type ( ah_data [ 1 ] ) \n 
if ntlm_version == 1 : \n 
~~~ return handle_type1 ( req , ah_data [ 1 ] ) \n 
~~ if ntlm_version == 3 : \n 
~~~ if cache . has_key ( req . connection . id ) : \n 
~~~ return handle_type3 ( req , ah_data [ 1 ] ) \n 
( req . unparsed_uri ) , apache . APLOG_INFO ) \n 
~~ error = \n 
~~~ error = str ( e ) \n 
( req . unparsed_uri , error ) , apache . APLOG_ERR ) \n 
~~ from celery import Celery \n 
def create_celery_app ( app ) : \n 
~~~ if app . config . get ( ) : \n 
~~~ app . celery = Celery ( __name__ , broker = app . config [ ] ) \n 
app . celery . conf . update ( app . config ) \n 
taskbase = app . celery . Task \n 
class ContextTask ( taskbase ) : \n 
~~~ abstract = True \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ with app . app_context ( ) : \n 
~~~ return taskbase . __call__ ( self , * args , ** kwargs ) \n 
~~ ~~ ~~ app . celery . Task = ContextTask \n 
~~ ~~ import unittest \n 
sys . path . append ( os . path . dirname ( os . path . realpath ( __file__ ) . rsplit ( , 2 ) [ 0 ] ) ) \n 
from app import create_app \n 
app = create_app ( ) \n 
class TestUsers ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . app = app . test_client ( ) \n 
~~ def test_01_add ( self ) : \n 
~~~ rv = self . app . post ( , data = add_data , \n 
content_type = "application/json" ) \n 
assert rv . status_code == 201 \n 
~~ def test_02_read_update ( self ) : \n 
~~~ request = self . app . get ( ) \n 
dict = json . loads ( request . data . decode ( ) ) \n 
id = dict [ ] [ 0 ] [ ] \n 
rv = self . app . patch ( . format ( id ) , \n 
data = update_data , content_type = "application/json" ) \n 
assert rv . status_code == 200 \n 
~~ def test_03_delete ( self ) : \n 
rv = self . app . delete ( . format ( id ) ) \n 
assert rv . status_code == 204 \n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ from django . core . management . base import BaseCommand \n 
from chronam . core . management . commands import configure_logging \n 
from chronam . core . index import index_pages \n 
configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n 
class Command ( BaseCommand ) : \n 
~~~ def handle ( self , ** options ) : \n 
~~~ index_pages ( ) \n 
~~ ~~ import os \n 
from django . conf import settings \n 
from django . http import HttpResponse \n 
class HttpResponseServiceUnavailable ( HttpResponse ) : \n 
~~~ status_code = 503 \n 
~~ class TooBusyMiddleware ( object ) : \n 
~~~ def process_request ( self , request ) : \n 
~~~ one , five , fifteen = os . getloadavg ( ) \n 
if one > settings . TOO_BUSY_LOAD_AVERAGE : \n 
~~ return None \n 
~~ ~~ from os . path import dirname , join \n 
from django . test import TestCase \n 
from chronam . core . ocr_extractor import ocr_extractor \n 
class OcrExtractorTests ( TestCase ) : \n 
~~~ def test_extractor ( self ) : \n 
~~~ dir = join ( dirname ( dirname ( __file__ ) ) , ) \n 
ocr_file = join ( dir , ) \n 
text , coord_info = ocr_extractor ( ocr_file ) \n 
coords = coord_info [ "coords" ] \n 
expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n 
self . assertEqual ( text , expected_text ) \n 
self . assertEqual ( len ( coords . keys ( ) ) , 2150 ) \n 
self . assertEqual ( len ( coords [ ] ) , 3 ) \n 
self . assertTrue ( coords . has_key ( ) ) \n 
self . assertTrue ( not coords . has_key ( ) ) \n 
import logging \n 
from . tests import * \n 
logger = logging . getLogger ( "human_curl" ) \n 
logger . setLevel ( logging . DEBUG ) \n 
handler = logging . StreamHandler ( ) \n 
handler . setFormatter ( formatter ) \n 
logger . addHandler ( handler ) \n 
import asyncio \n 
from zeroservices import ZeroMQMedium , ResourceService \n 
from zeroservices . services import get_http_interface \n 
from zeroservices . discovery import UdpDiscoveryMedium \n 
~~~ loop = asyncio . get_event_loop ( ) \n 
medium = ZeroMQMedium ( loop , UdpDiscoveryMedium ) \n 
service = ResourceService ( , medium ) \n 
application = get_http_interface ( service , loop , port = 5001 , allowed_origins = "*" ) \n 
application = loop . run_until_complete ( application ) \n 
loop . run_until_complete ( service . start ( ) ) \n 
loop . run_forever ( ) \n 
from trello import TrelloClient \n 
from slugify import slugify \n 
from matterllo . utils import config \n 
from matterllo . utils import logger \n 
SETTINGS = config ( ) \n 
LOGGING = logger ( ) \n 
parser . add_argument ( , dest = , action = , help = ) \n 
if not args . cleanup and not args . update and not args . init : \n 
~~~ print parser . print_help ( ) \n 
sys . exit ( 0 ) \n 
~~ client = TrelloClient ( api_key = SETTINGS [ ] , token = SETTINGS [ ] ) \n 
trello_boards = client . list_boards ( ) \n 
boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n 
if args . cleanup or args . init : \n 
~~~ result = [ h . delete ( ) for h in client . list_hooks ( ) ] \n 
LOGGING . info ( . format ( len ( result ) ) ) \n 
~~ if args . update or args . init : \n 
~~~ for board in trello_boards : \n 
~~~ board_name = slugify ( board . name ) \n 
if board_name not in boards_name : \n 
~~ LOGGING . info ( . format ( board_name ) ) \n 
url = SETTINGS [ ] + \n 
result = client . create_hook ( url , board . id ) \n 
LOGGING . info ( . format ( board_name , result ) ) \n 
~~ ~~ ~~ except Exception as e : \n 
~~~ LOGGING . error ( . format ( e ) ) \n 
sys . exit ( 1 ) \n 
~~~ main ( ) \n 
from contextlib import contextmanager \n 
import zlib \n 
~~~ from . import ssl_compat \n 
~~~ ssl_compat = None \n 
~~ _ver = sys . version_info \n 
is_py2 = _ver [ 0 ] == 2 \n 
is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n 
is_py3 = _ver [ 0 ] == 3 \n 
is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n 
@ contextmanager \n 
def ignore_missing ( ) : \n 
~~~ yield \n 
~~ ~~ if is_py2 : \n 
~~~ if is_py2_7_9_or_later : \n 
~~~ import ssl \n 
~~~ ssl = ssl_compat \n 
~~ from urllib import urlencode \n 
from urlparse import urlparse , urlsplit \n 
from itertools import imap \n 
def to_byte ( char ) : \n 
~~~ return ord ( char ) \n 
~~ def decode_hex ( b ) : \n 
~~~ return b . decode ( ) \n 
~~ def write_to_stdout ( data ) : \n 
~~~ sys . stdout . write ( data + ) \n 
sys . stdout . flush ( ) \n 
~~ def zlib_compressobj ( level = 6 , method = zlib . DEFLATED , wbits = 15 , memlevel = 8 , \n 
strategy = zlib . Z_DEFAULT_STRATEGY ) : \n 
~~~ return zlib . compressobj ( level , method , wbits , memlevel , strategy ) \n 
~~ unicode = unicode \n 
bytes = str \n 
~~ elif is_py3 : \n 
~~~ from urllib . parse import urlencode , urlparse , urlsplit \n 
imap = map \n 
~~~ return char \n 
~~~ return bytes . fromhex ( b ) \n 
~~~ sys . stdout . buffer . write ( data + ) \n 
sys . stdout . buffer . flush ( ) \n 
~~ zlib_compressobj = zlib . compressobj \n 
if is_py3_3 : \n 
~~ unicode = str \n 
bytes = bytes \n 
import threading \n 
import socket \n 
from hyper import HTTP20Connection \n 
from hyper . compat import ssl \n 
from hyper . http11 . connection import HTTP11Connection \n 
from hpack . hpack import Encoder \n 
from hpack . huffman import HuffmanEncoder \n 
from hpack . huffman_constants import ( \n 
REQUEST_CODES , REQUEST_CODES_LENGTH \n 
) \n 
from hyper . tls import NPN_PROTOCOL \n 
class SocketServerThread ( threading . Thread ) : \n 
def __init__ ( self , \n 
socket_handler , \n 
host = , \n 
ready_event = None , \n 
h2 = True , \n 
secure = True ) : \n 
~~~ threading . Thread . __init__ ( self ) \n 
self . socket_handler = socket_handler \n 
self . host = host \n 
self . secure = secure \n 
self . ready_event = ready_event \n 
self . daemon = True \n 
if self . secure : \n 
~~~ self . cxt = ssl . SSLContext ( ssl . PROTOCOL_SSLv23 ) \n 
if ssl . HAS_NPN and h2 : \n 
~~~ self . cxt . set_npn_protocols ( [ NPN_PROTOCOL ] ) \n 
~~ self . cxt . load_cert_chain ( certfile = , \n 
keyfile = ) \n 
~~ ~~ def _start_server ( self ) : \n 
~~~ sock = socket . socket ( ) \n 
if sys . platform != : \n 
~~~ sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n 
~~ if self . secure : \n 
~~~ sock = self . cxt . wrap_socket ( sock , server_side = True ) \n 
~~ sock . bind ( ( self . host , 0 ) ) \n 
self . port = sock . getsockname ( ) [ 1 ] \n 
sock . listen ( 1 ) \n 
if self . ready_event : \n 
~~~ self . ready_event . set ( ) \n 
~~ self . socket_handler ( sock ) \n 
sock . close ( ) \n 
~~ def _wrap_socket ( self , sock ) : \n 
~~~ raise NotImplementedError ( ) \n 
~~ def run ( self ) : \n 
~~~ self . server = self . _start_server ( ) \n 
~~ ~~ class SocketLevelTest ( object ) : \n 
def set_up ( self , secure = True , proxy = False ) : \n 
~~~ self . host = None \n 
self . port = None \n 
self . secure = secure if not proxy else False \n 
self . proxy = proxy \n 
self . server_thread = None \n 
~~ def _start_server ( self , socket_handler ) : \n 
ready_event = threading . Event ( ) \n 
self . server_thread = SocketServerThread ( \n 
socket_handler = socket_handler , \n 
ready_event = ready_event , \n 
h2 = self . h2 , \n 
secure = self . secure \n 
self . server_thread . start ( ) \n 
ready_event . wait ( ) \n 
self . host = self . server_thread . host \n 
self . port = self . server_thread . port \n 
self . secure = self . server_thread . secure \n 
~~ def get_connection ( self ) : \n 
~~~ if self . h2 : \n 
~~~ if not self . proxy : \n 
~~~ return HTTP20Connection ( self . host , self . port , self . secure ) \n 
~~~ return HTTP20Connection ( , secure = self . secure , \n 
proxy_host = self . host , \n 
proxy_port = self . port ) \n 
~~~ return HTTP11Connection ( self . host , self . port , self . secure ) \n 
~~~ return HTTP11Connection ( , secure = self . secure , \n 
~~ ~~ ~~ def get_encoder ( self ) : \n 
e = Encoder ( ) \n 
e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n 
return e \n 
~~ def tear_down ( self ) : \n 
self . server_thread . join ( 0.1 ) \n 
import unittest \n 
from SystemConfiguration import * \n 
class SCPreferences ( object ) : \n 
session = None \n 
def __init__ ( self ) : \n 
~~~ super ( SCPreferences , self ) . __init__ ( ) \n 
self . session = SCPreferencesCreate ( None , "set-proxy" , None ) \n 
~~ def save ( self ) : \n 
~~~ if not self . session : \n 
~~ if not SCPreferencesCommitChanges ( self . session ) : \n 
~~ if not SCPreferencesApplyChanges ( self . session ) : \n 
~~ ~~ def set_proxy ( self , enable = True , protocol = "HTTP" , server = "localhost" , port = 3128 ) : \n 
~~~ new_settings = SCPreferencesPathGetValue ( self . session , ) \n 
for interface in new_settings : \n 
~~~ new_settings [ interface ] [ ] [ "%sEnable" % protocol ] = 1 if enable else 0 \n 
if enable : \n 
~~~ new_settings [ interface ] [ ] [ % protocol ] = int ( port ) \n 
new_settings [ interface ] [ ] [ % protocol ] = server \n 
~~ ~~ SCPreferencesPathSetValue ( self . session , , new_settings ) \n 
~~ ~~ class SCPreferencesTests ( unittest . TestCase ) : \n 
~~ import sublime , sublime_plugin , os , re \n 
class StyleSheetSetup : \n 
~~~ def __init__ ( self , extensions , regex , partials = None , index = None ) : \n 
~~~ if partials is None : \n 
~~~ self . partials = False \n 
~~~ self . partials = partials \n 
~~ if index is None : \n 
~~~ self . index = False \n 
~~~ self . index = index \n 
~~ self . extensions = extensions \n 
self . regex = regex \n 
~~ ~~ class ListStylesheetVariables ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit ) : \n 
~~~ settings = sublime . load_settings ( ) \n 
handle_imports = settings . get ( "readImported" ) \n 
read_all_views = settings . get ( "readAllViews" ) \n 
setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n 
chosen_setup = None \n 
self . edit = edit \n 
fn = self . view . file_name ( ) . encode ( "utf_8" ) \n 
for setup in setups : \n 
~~~ for ext in setup . extensions : \n 
~~~ if fn . endswith ( ext ) : \n 
~~~ chosen_setup = setup \n 
~~ ~~ ~~ if chosen_setup == None : \n 
~~ imports = [ ] \n 
imported_vars = [ ] \n 
compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n 
if handle_imports : \n 
file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n 
for i , filename in enumerate ( imports ) : \n 
~~~ has_extension = False \n 
for ext in chosen_setup . extensions : \n 
~~~ if filename . endswith ( ext . decode ( "utf-8" ) ) : \n 
~~~ has_extension = True \n 
~~ ~~ if has_extension == False : \n 
~~~ for ext in chosen_setup . extensions : \n 
~~~ ext = ext . decode ( "utf-8" ) \n 
if os . path . isfile ( os . path . normpath ( file_dir + + filename + ext ) ) : \n 
~~~ filename += ext \n 
~~ if chosen_setup . partials : \n 
~~~ fn_split = os . path . split ( filename ) \n 
partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n 
if os . path . isfile ( os . path . normpath ( file_dir + partial_filename + ext ) ) : \n 
~~~ filename = "_" + filename + ext \n 
~~ ~~ if chosen_setup . index and os . path . isfile ( os . path . normpath ( file_dir + "/" + filename + "/index" + ext ) ) : \n 
~~~ filename += "/index" + ext \n 
~~ ~~ ~~ try : \n 
~~~ f = open ( os . path . normpath ( file_dir + + filename ) , ) \n 
contents = f . read ( ) \n 
f . close ( ) \n 
m = re . findall ( compiled_regex , contents ) \n 
imported_vars = imported_vars + m \n 
~~~ print ( + filename ) \n 
~~ ~~ imported_vars = [ list ( item ) for item in imported_vars ] \n 
~~ self . variables = [ ] \n 
vars_from_views = [ ] \n 
if read_all_views : \n 
~~~ for view in self . view . window ( ) . views ( ) : \n 
~~~ viewfn = self . view . file_name ( ) . encode ( "utf-8" ) \n 
compatible_view = False \n 
~~~ if viewfn . endswith ( ext ) : \n 
~~~ viewvars = [ ] \n 
view . find_all ( chosen_setup . regex , 0 , "$1|$2" , viewvars ) \n 
vars_from_views += viewvars \n 
break ; \n 
~~ ~~ ~~ ~~ else : \n 
~~~ self . view . find_all ( chosen_setup . regex , 0 , "$1|$2" , self . variables ) \n 
~~ self . variables += vars_from_views \n 
self . variables = list ( set ( self . variables ) ) \n 
for i , val in enumerate ( self . variables ) : \n 
~~~ self . variables [ i ] = val . split ( "|" ) \n 
~~ self . variables = imported_vars + self . variables \n 
self . variables . sort ( ) \n 
self . view . window ( ) . show_quick_panel ( self . variables , self . insert_variable , sublime . MONOSPACE_FONT ) \n 
~~ def insert_variable ( self , choice ) : \n 
~~~ if choice == - 1 : \n 
~~ self . view . run_command ( , { : self . variables [ choice ] [ 0 ] } ) \n 
~~ ~~ class InsertText ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit , string = ) : \n 
~~~ for selection in self . view . sel ( ) : \n 
~~~ self . view . insert ( edit , selection . begin ( ) , string ) \n 
~~ ~~ ~~ from driver_base import DriverBase \n 
os . sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import log \n 
class CMDTYPE : \n 
PIXEL_DATA = 2 \n 
BRIGHTNESS = 3 \n 
~~ class RETURN_CODES : \n 
~~ class DriverNetwork ( DriverBase ) : \n 
def __init__ ( self , num = 0 , width = 0 , height = 0 , host = "localhost" , port = 3142 ) : \n 
~~~ super ( DriverNetwork , self ) . __init__ ( num , width , height ) \n 
self . _host = host \n 
self . _port = port \n 
~~ def _generateHeader ( self , cmd , size ) : \n 
~~~ packet = bytearray ( ) \n 
packet . append ( cmd ) \n 
packet . append ( size & 0xFF ) \n 
packet . append ( size >> 8 ) \n 
return packet \n 
~~ def _connect ( self ) : \n 
~~~ s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
s . connect ( ( self . _host , self . _port ) ) \n 
return s \n 
~~ except socket . gaierror : \n 
self . _host ) \n 
log . error ( error ) \n 
raise IOError ( error ) \n 
~~ ~~ def update ( self , data ) : \n 
~~~ s = self . _connect ( ) \n 
count = self . bufByteCount \n 
packet = self . _generateHeader ( CMDTYPE . PIXEL_DATA , count ) \n 
packet . extend ( data ) \n 
s . sendall ( packet ) \n 
resp = ord ( s . recv ( 1 ) ) \n 
s . close ( ) \n 
if resp != RETURN_CODES . SUCCESS : \n 
~~~ log . exception ( e ) \n 
~~ ~~ def setMasterBrightness ( self , brightness ) : \n 
~~~ packet = self . _generateHeader ( CMDTYPE . BRIGHTNESS , 1 ) \n 
packet . append ( brightness ) \n 
s = self . _connect ( ) \n 
~~ ~~ ~~ MANIFEST = [ \n 
{ \n 
"id" : "network" , \n 
"class" : DriverNetwork , \n 
"type" : "driver" , \n 
"display" : "Network" , \n 
"params" : [ { \n 
"id" : "num" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
} , { \n 
"id" : "width" , \n 
"label" : "Width" , \n 
"id" : "height" , \n 
"label" : "Height" , \n 
"id" : "host" , \n 
"type" : "str" , \n 
"default" : "localhost" , \n 
"id" : "port" , \n 
"label" : "Port" , \n 
"default" : 3142 , \n 
} ] \n 
} \n 
_ID = \n 
API = \n 
API_CALL_TIMEOUT = \n 
API_VERSION = \n 
API_VERSION_MAXIMUM = \n 
API_VERSION_MINIMUM = \n 
AREA = \n 
AREA_MAX = \n 
BBOX = \n 
BOUNDS = \n 
CFGSLAB = \n 
CFGVERSION = 1 \n 
CHANGESET = \n 
CHANGESETS = \n 
CHANGESETS_INLINE_SIZE = \n 
CHANGESETS_PER_SLAB = \n 
CHANGESETS_MAX = \n 
CONFIGURATION_SCHEMA_VERSION = \n 
CONTENT_TYPE = \n 
COUCHDB = \n 
DATASTORE = \n 
DATASTORE_BACKEND = \n 
DATASTORE_CONFIG = \n 
DATASTORE_ENCODING = \n 
DBHOST = \n 
DBJOB_ADDELEM = \n 
DBJOB_QUIT = \n 
DBNAME = \n 
DBPORT = \n 
DBURL = \n 
DEFAULT = \n 
ELEMENT = \n 
FRONT_END = \n 
GENERATOR = \n 
GEODOC = \n 
GEODOC_LRU_SIZE = \n 
GEODOC_LRU_THREADS = \n 
GEOHASH_LENGTH = \n 
ID = \n 
JSON = \n 
K = \n 
LAT = \n 
LAT_MAX = + 90.0 \n 
LAT_MIN = - 90.0 \n 
LON = \n 
LON_MAX = + 180.0 \n 
LON_MIN = - 180.0 \n 
MAXIMUM = \n 
MAXIMUM_ELEMENTS = \n 
MAXGHLAT = 89.999999999999992 \n 
MAXLAT = \n 
MAXLON = \n 
MEMBASE = \n 
MEMBASE_MAX_VALUE_LENGTH = 20 * 1024 * 1024 \n 
MEMBER = \n 
MEMBERS = \n 
MINIMUM = \n 
MINLAT = \n 
MINLON = \n 
ND = \n 
NODE = \n 
NODES = \n 
NODES_INLINE_SIZE = \n 
NODES_PER_SLAB = \n 
OSM = \n 
PER_PAGE = \n 
PORT = \n 
PROJECT_DOC = \n 
PROTOBUF = \n 
REF = \n 
REFERENCES = \n 
RELATION = \n 
RELATIONS = \n 
RELATIONS_INLINE_SIZE = \n 
RELATIONS_PER_SLAB = \n 
ROLE = \n 
SCALE_FACTOR = \n 
SECONDS = \n 
SERVER_NAME = \n 
SERVER_VERSION = \n 
SLAB_LRU_SIZE = \n 
SLAB_LRU_THREADS = \n 
SOURCE_REPOSITORY = \n 
STATUS = \n 
TAG = \n 
TAGS = \n 
TEXT_XML = \n 
TIMEOUT = \n 
TRACEPOINTS = \n 
TRACEPOINTS_PER_PAGE = \n 
TYPE = \n 
UTF8 = \n 
V = \n 
VERSION = \n 
WAY = \n 
WAYS = \n 
WAYS_INLINE_SIZE = \n 
WAYS_PER_SLAB = \n 
WAYNODES = \n 
WAYNODES_MAX = \n 
import webapp2 \n 
from urllib import urlencode \n 
import json , urllib2 \n 
from secret import client_id , client_secret \n 
import config \n 
class AuthRedirector ( webapp2 . RequestHandler ) : \n 
~~~ def get ( self ) : \n 
~~~ args = self . request . GET \n 
args [ "client_id" ] = client_id \n 
args [ "redirect_uri" ] = config . auth_redir_uri \n 
url = "https://accounts.google.com/o/oauth2/auth?" + urlencode ( args ) \n 
self . response . location = url \n 
self . response . status_int = 302 \n 
~~ ~~ def query_json ( url , data ) : \n 
if not ( data is str ) : \n 
~~~ data = urlencode ( data ) \n 
~~~ return json . loads ( urllib2 . urlopen ( url , data ) . read ( ) ) \n 
~~~ return json . loads ( e . read ( ) ) \n 
~~ ~~ def json_compactify ( data ) : \n 
~~ class AuthCallback ( webapp2 . RequestHandler ) : \n 
def get ( self ) : \n 
~~~ state = self . request . get ( "state" ) \n 
code = self . request . get ( "code" ) \n 
error = self . request . get ( "error" ) \n 
q = { \n 
"code" : code , \n 
"client_id" : client_id , \n 
"client_secret" : client_secret , \n 
"redirect_uri" : config . auth_redir_uri , \n 
"grant_type" : "authorization_code" , \n 
result = query_json ( "https://accounts.google.com/o/oauth2/token" , q ) \n 
url = ( config . auth_success_page if "access_token" in result \n 
else config . auth_failure_page ) + "#" + urlencode ( result ) \n 
~~ ~~ class AuthRefresh ( webapp2 . RequestHandler ) : \n 
~~~ refresh_token = self . request . get ( "refresh_token" ) \n 
if not refresh_token : \n 
~~~ self . response . status_int = 400 \n 
~~ q = { \n 
"refresh_token" : refresh_token , \n 
"grant_type" : "refresh_token" , \n 
self . response . write ( json_compactify ( result ) ) \n 
~~ ~~ application = webapp2 . WSGIApplication ( [ \n 
( , AuthRedirector ) , \n 
( , AuthCallback ) , \n 
( , AuthRefresh ) , \n 
] , debug = True ) \n 
from django . db import migrations , models \n 
class Migration ( migrations . Migration ) : \n 
~~~ dependencies = [ \n 
( , ) , \n 
operations = [ \n 
migrations . AddField ( \n 
model_name = , \n 
name = , \n 
field = models . EmailField ( help_text = , max_length = 75 , null = True , verbose_name = , blank = True ) , \n 
preserve_default = True , \n 
) , \n 
import string \n 
from jsbeautifier . unpackers import UnpackingError \n 
PRIORITY = 1 \n 
def detect ( source ) : \n 
return source . replace ( , ) . startswith ( ) \n 
~~ def unpack ( source ) : \n 
payload , symtab , radix , count = _filterargs ( source ) \n 
if count != len ( symtab ) : \n 
~~~ raise UnpackingError ( ) \n 
~~~ unbase = Unbaser ( radix ) \n 
~~ except TypeError : \n 
~~ def lookup ( match ) : \n 
word = match . group ( 0 ) \n 
return symtab [ unbase ( word ) ] or word \n 
~~ source = re . sub ( , lookup , payload ) \n 
return _replacestrings ( source ) \n 
~~ def _filterargs ( source ) : \n 
args = re . search ( argsregex , source , re . DOTALL ) . groups ( ) \n 
~~~ return args [ 0 ] , args [ 3 ] . split ( ) , int ( args [ 1 ] ) , int ( args [ 2 ] ) \n 
~~ except ValueError : \n 
~~ ~~ def _replacestrings ( source ) : \n 
if match : \n 
~~~ varname , strings = match . groups ( ) \n 
startpoint = len ( match . group ( 0 ) ) \n 
lookup = strings . split ( \'","\' ) \n 
variable = % varname \n 
for index , value in enumerate ( lookup ) : \n 
~~~ source = source . replace ( variable % index , \'"%s"\' % value ) \n 
~~ return source [ startpoint : ] \n 
~~ return source \n 
~~ class Unbaser ( object ) : \n 
ALPHABET = { \n 
62 : , \n 
def __init__ ( self , base ) : \n 
~~~ self . base = base \n 
if 2 <= base <= 36 : \n 
~~~ self . unbase = lambda string : int ( string , base ) \n 
~~~ self . dictionary = dict ( ( cipher , index ) for \n 
index , cipher in enumerate ( self . ALPHABET [ base ] ) ) \n 
~~ except KeyError : \n 
~~~ raise TypeError ( ) \n 
~~ self . unbase = self . _dictunbaser \n 
~~ ~~ def __call__ ( self , string ) : \n 
~~~ return self . unbase ( string ) \n 
~~ def _dictunbaser ( self , string ) : \n 
ret = 0 \n 
for index , cipher in enumerate ( string [ : : - 1 ] ) : \n 
~~~ ret += ( self . base ** index ) * self . dictionary [ cipher ] \n 
~~ return ret \n 
~~ ~~ import os , sys \n 
parentdir = os . path . dirname ( __file__ ) \n 
sys . path . insert ( 0 , parentdir ) \n 
import executemechanize \n 
class redirection : \n 
~~~ def createarray ( self ) : \n 
~~~ setattr ( self , "redirection_list" , [ ] ) \n 
~~ def appendurl ( self , url ) : \n 
~~~ url = str ( url ) \n 
if not url . endswith ( ".js" ) or url . endswith ( ".json" ) : \n 
~~~ self . redirection_list . append ( url ) ; \n 
self . passarray ( ) \n 
~~ ~~ def passarray ( self ) : \n 
~~~ executemechanize . set_redirection_list ( self . redirection_list ) \n 
~~ ~~ SQL_PORT = 15000 \n 
ZMQ_RPC_PORT = 15598 \n 
HTTP_PORT = 15597 \n 
HTTPS_PORT = 443 \n 
ZMQ_PUBSUB_PORT = 15596 \n 
__author__ = \n 
__maintainer__ = \n 
__email__ = \n 
from . . config import PATHS \n 
from . . entity import Entity \n 
class StrategyConcept ( Entity ) : \n 
collection = \n 
resource = \n 
_relations = { \n 
, \n 
_pull = { \n 
: int , \n 
: Entity . _strpt , \n 
: Entity . _int_to_bool , \n 
_push = _pull . copy ( ) \n 
_push . update ( { \n 
} ) \n 
_readonly = Entity . _readonly | { , } \n 
def __init__ ( self , session , properties = None , ** kwargs ) : \n 
~~~ super ( StrategyConcept , self ) . __init__ ( session , properties , ** kwargs ) \n 
~~ def remove ( self ) : \n 
url = . join ( [ self . collection , \n 
str ( self . id ) , \n 
] ) \n 
self . _post ( PATHS [ ] , rest = url , data = { : self . version } ) \n 
for item in list ( self . properties . keys ( ) ) : \n 
~~~ del self . properties [ item ] \n 
~~ ~~ ~~ from __future__ import print_function \n 
import responses \n 
import requests \n 
from . requests_patch import patched_extract_cookies_to_jar \n 
from terminalone import T1 \n 
mock_credentials = { \n 
API_BASE = \n 
requests . sessions . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
requests . adapters . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
class TestPermissions ( unittest . TestCase ) : \n 
~~~ def setup ( self ) : \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . POST , , \n 
body = fixture , \n 
adding_headers = { \n 
} , \n 
content_type = ) \n 
self . t1 = T1 ( auth_method = , \n 
api_base = API_BASE , \n 
** mock_credentials ) \n 
~~ @ responses . activate \n 
def test_get_permissions ( self ) : \n 
~~~ self . setup ( ) \n 
~~ responses . add ( responses . GET , \n 
content_type = , \n 
match_querystring = True ) \n 
p = self . t1 . get ( , 10000 , child = ) \n 
assert p . _type == , . format ( p . _type ) \n 
assert p . parent_id == 10000 , . format ( p . parent_id ) \n 
def test_remove_advertiser ( self ) : \n 
remove_id = 6 \n 
assert remove_id in p . advertiser . keys ( ) , . format ( remove_id ) \n 
p . remove ( , 6 ) \n 
assert remove_id not in p . advertiser . keys ( ) , . format ( remove_id ) \n 
def test_it_should_remove_child_advertisers_when_removing_agency ( self ) : \n 
remove_ids = [ 6 , 7 ] \n 
for ad_id in remove_ids : \n 
~~~ assert ad_id in p . advertiser . keys ( ) , . format ( ad_id ) \n 
~~ p . remove ( , 3 ) \n 
~~~ assert ad_id not in p . advertiser . keys ( ) , . format ( ad_id ) \n 
~~ ~~ @ responses . activate \n 
def test_it_should_remove_child_agencies_and_advertisers_when_removing_organization ( self ) : \n 
remove_advertiser_ids = [ 8 , 9 , 10 ] \n 
remove_agency_ids = [ 4 , 5 ] \n 
for advertiser_id in remove_advertiser_ids : \n 
~~~ assert advertiser_id in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
~~ for agency_id in remove_agency_ids : \n 
~~~ assert agency_id in p . agency . keys ( ) , . format ( agency_id ) \n 
~~ p . remove ( , 2 ) \n 
~~~ assert advertiser_id not in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
~~~ assert agency_id not in p . agency . keys ( ) , . format ( agency_id ) \n 
def test_it_should_add_entity_ids_on_save ( self ) : \n 
p . add ( , 10 ) \n 
data = p . _generate_save_data ( ) \n 
assert sorted ( data [ ] ) == [ 1 , 2 , 10 ] , data [ ] \n 
def test_it_should_add_access_to_empty_permissions ( self ) : \n 
assert sorted ( data [ ] ) == [ 10 ] , data [ ] \n 
~~ ~~ VERSION = ( 0 , 1 , 9 ) \n 
__version__ = "0.1.9" \n 
import sys as _sys \n 
from operator import itemgetter as _itemgetter \n 
from keyword import iskeyword as _iskeyword \n 
from collections import OrderedDict \n 
################################################################################ \n 
class tagtuple ( tuple ) : \n 
__slots__ = ( ) \n 
def __new__ ( cls , * args ) : \n 
return super ( tagtuple , cls ) . __new__ ( cls , args ) \n 
~~ def __repr__ ( self ) : \n 
return type ( self ) . __name__ + super ( tagtuple , self ) . __repr__ ( ) \n 
~~ def __getnewargs__ ( self ) : \n 
return tuple ( self ) \n 
~~ def __eq__ ( self , other ) : \n 
~~~ return type ( self ) is type ( other ) and super ( tagtuple , self ) . __eq__ ( other ) \n 
~~ def __ne__ ( self , other ) : \n 
~~~ return not self . __eq__ ( other ) \n 
~~ def __getslice__ ( self , i , j ) : \n 
~~~ return type ( self ) ( * super ( tagtuple , self ) . __getslice__ ( i , j ) ) \n 
~~ __add__ = property ( ) \n 
__mul__ = property ( ) \n 
__rmul__ = property ( ) \n 
count = property ( ) \n 
index = property ( ) \n 
~~ _class_template = \n 
_repr_template = \n 
_field_template = \n 
def rectuple ( typename , field_names , verbose = False , rename = False ) : \n 
if isinstance ( field_names , basestring ) : \n 
~~~ field_names = field_names . replace ( , ) . split ( ) \n 
~~ field_names = map ( str , field_names ) \n 
if rename : \n 
~~~ seen = set ( ) \n 
for index , name in enumerate ( field_names ) : \n 
~~~ if ( not all ( c . isalnum ( ) or c == for c in name ) \n 
or _iskeyword ( name ) \n 
or not name \n 
or name [ 0 ] . isdigit ( ) \n 
or name . startswith ( ) \n 
or name in seen ) : \n 
~~~ field_names [ index ] = % index \n 
~~ seen . add ( name ) \n 
~~ ~~ for name in [ typename ] + field_names : \n 
~~~ if not all ( c . isalnum ( ) or c == for c in name ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if _iskeyword ( name ) : \n 
~~ if name [ 0 ] . isdigit ( ) : \n 
~~ ~~ seen = set ( ) \n 
for name in field_names : \n 
~~~ if name . startswith ( ) and not rename : \n 
~~ if name in seen : \n 
~~~ raise ValueError ( % name ) \n 
~~ class_definition = _class_template . format ( \n 
typename = typename , \n 
field_names = tuple ( field_names ) , \n 
num_fields = len ( field_names ) , \n 
arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n 
repr_fmt = . join ( _repr_template . format ( name = name ) \n 
for name in field_names ) , \n 
field_defs = . join ( _field_template . format ( index = index , name = name ) \n 
for index , name in enumerate ( field_names ) ) \n 
if verbose : \n 
~~~ print class_definition \n 
~~ namespace = dict ( _itemgetter = _itemgetter , __name__ = % typename , \n 
OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n 
~~~ exec class_definition in namespace \n 
~~ except SyntaxError as e : \n 
~~~ raise SyntaxError ( e . message + + class_definition ) \n 
~~ result = namespace [ typename ] \n 
~~~ result . __module__ = _sys . _getframe ( 1 ) . f_globals . get ( , ) \n 
~~ except ( AttributeError , ValueError ) : \n 
~~~ import pickle \n 
from itertools import chain , product \n 
print \n 
class A ( tagtuple ) : \n 
~~~ __slots__ = ( ) \n 
~~ class B ( tagtuple ) : \n 
~~ a = A ( 1 , 2 , 3 ) \n 
b = B ( 1 , 2 , 3 ) \n 
t = ( 1 , 2 , 3 ) \n 
d = { } \n 
d [ a ] = 1 \n 
d [ b ] = 2 \n 
d [ t ] = 3 \n 
s = set ( ) \n 
s . add ( a ) \n 
s . add ( b ) \n 
s . add ( t ) \n 
a0 = pickle . loads ( pickle . dumps ( a , 0 ) ) \n 
a1 = pickle . loads ( pickle . dumps ( a , 1 ) ) \n 
a2 = pickle . loads ( pickle . dumps ( a , 2 ) ) \n 
A = rectuple ( , , verbose = True ) \n 
B = rectuple ( , , verbose = True ) \n 
a = A ( 1 , 2 ) \n 
b = B ( 1 , 2 ) \n 
t = ( 1 , 2 ) \n 
~~ import math \n 
def distance ( pa , pb ) : \n 
~~~ ax , ay = pa \n 
bx , by = pb \n 
return math . sqrt ( ( ax - bx ) ** 2 + ( ay - by ) ** 2 ) \n 
~~ def index_of_nearest ( p , hot_points , distance_f = distance ) : \n 
min_dist = None \n 
nearest_hp_i = None \n 
for i , hp in enumerate ( hot_points ) : \n 
~~~ dist = distance_f ( p , hp ) \n 
if min_dist is None or dist < min_dist : \n 
~~~ min_dist = dist \n 
nearest_hp_i = i \n 
~~ ~~ return nearest_hp_i \n 
~~ from fabric import main as fab_main \n 
from cloudferry import fabfile \n 
~~~ fab = fabfile . __file__ \n 
if fab . endswith ( ) : \n 
~~~ fab = fab [ : - 1 ] \n 
~~ fab_main . main ( [ fab ] ) \n 
~~ from cloudferry . lib . base . action import action \n 
DEFAULT = 0 \n 
PATH_ONE = 1 \n 
PATH_TWO = 2 \n 
class IsOption ( action . Action ) : \n 
~~~ def __init__ ( self , init , option_name ) : \n 
~~~ self . option_name = option_name \n 
super ( IsOption , self ) . __init__ ( init ) \n 
~~ def run ( self , ** kwargs ) : \n 
option_value = self . cfg . migrate [ self . option_name ] \n 
if option_value : \n 
~~~ self . set_next_path ( PATH_ONE ) \n 
~~~ self . set_next_path ( PATH_TWO ) \n 
~~ return { } \n 
~~ ~~ from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import log \n 
from cloudferry . lib . utils import utils as utl \n 
LOG = log . getLogger ( __name__ ) \n 
class CheckConfigQuotaNeutron ( action . Action ) : \n 
def run ( self , ** kwargs ) : \n 
~~~ src_cloud = self . src_cloud \n 
dst_cloud = self . dst_cloud \n 
network_src = src_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
identity_dst = dst_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
network_dst = dst_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
search_opts_tenant = kwargs . get ( , { } ) \n 
tenants_src = self . get_src_tenants ( search_opts_tenant ) \n 
list_quotas = network_src . list_quotas ( ) \n 
tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n 
list_quotas ) \n 
if not tenants_without_quotas : \n 
quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n 
quot_default_dst = network_dst . show_quota ( dst_temp_tenant . id ) \n 
is_configs_different = False \n 
identity_dst . delete_tenant ( dst_temp_tenant ) \n 
for item_quot , val_quot in quot . iteritems ( ) : \n 
~~~ if val_quot != quot_default_dst [ item_quot ] : \n 
~~~ is_configs_different = True \n 
quot_default_dst [ item_quot ] ) \n 
~~ ~~ if not is_configs_different : \n 
def get_tenants_without_quotas ( tenants_src , list_quotas ) : \n 
~~~ tenants_ids = tenants_src . keys ( ) \n 
quotas_ids_tenants = [ quota [ "tenant_id" ] for quota in list_quotas ] \n 
return list ( set ( tenants_ids ) - set ( quotas_ids_tenants ) ) \n 
~~ def get_src_tenants ( self , search_opts ) : \n 
~~~ identity_src = self . src_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
if search_opts . get ( ) : \n 
~~~ filter_tenants_ids_list = search_opts [ ] \n 
tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n 
tnt_id in filter_tenants_ids_list ] \n 
~~~ tenants = identity_src . get_tenants_list ( ) \n 
~~ tenants_dict = { tenant . id : tenant . name for tenant in tenants } \n 
return tenants_dict \n 
~~ ~~ import copy \n 
from oslo_config import cfg \n 
from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import utils \n 
LOG = logging . getLogger ( __name__ ) \n 
class DetachVolumesCompute ( action . Action ) : \n 
~~~ def run ( self , info , ** kwargs ) : \n 
~~~ info = copy . deepcopy ( info ) \n 
compute_resource = self . cloud . resources [ utils . COMPUTE_RESOURCE ] \n 
storage_resource = self . cloud . resources [ utils . STORAGE_RESOURCE ] \n 
for instance in info [ utils . INSTANCES_TYPE ] . itervalues ( ) : \n 
instance [ ] [ ] , instance [ ] [ ] ) \n 
if not instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~ for vol in instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~~ volume_status = storage_resource . get_status ( vol [ ] ) \n 
vol [ ] , volume_status ) \n 
if volume_status == : \n 
~~~ compute_resource . detach_volume ( instance [ ] [ ] , \n 
vol [ ] ) \n 
timeout = CONF . migrate . storage_backend_timeout \n 
storage_resource . wait_for_status ( \n 
vol [ ] , storage_resource . get_status , , \n 
timeout = timeout ) \n 
~~ ~~ ~~ return { } \n 
from cloudferry . lib . utils . ssh_util import SshUtil \n 
class RemoteExecution ( action . Action ) : \n 
~~~ def __init__ ( self , cloud , host = None , int_host = None , config_migrate = None ) : \n 
~~~ self . cloud = cloud \n 
self . int_host = int_host \n 
self . config_migrate = config_migrate \n 
self . remote_exec_obj = SshUtil ( self . cloud , \n 
self . config_migrate , \n 
self . host ) \n 
super ( RemoteExecution , self ) . __init__ ( { } ) \n 
~~ def run ( self , command , ** kwargs ) : \n 
~~~ self . remote_exec_obj . execute ( command , self . int_host ) \n 
return { } \n 
~~ ~~ import json \n 
from xml . etree import ElementTree \n 
nova_instances_path = "/var/lib/nova/instances/" \n 
def instance_path ( instance_id ) : \n 
~~~ return os . path . join ( nova_instances_path , instance_id ) \n 
~~ def instance_image_path ( instance_id ) : \n 
~~~ return os . path . join ( instance_path ( instance_id ) , "disk" ) \n 
~~ def _qemu_img_rebase ( src , dst ) : \n 
~~ class QemuBackingFileMover ( object ) : \n 
~~~ def __init__ ( self , runner , src , instance_id ) : \n 
~~~ self . runner = runner \n 
self . src = src \n 
self . dst = instance_image_path ( instance_id ) \n 
~~ def __enter__ ( self ) : \n 
~~~ cmd = _qemu_img_rebase ( self . src , self . dst ) \n 
self . runner . run ( cmd ) \n 
return self \n 
~~ def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n 
~~~ cmd = _qemu_img_rebase ( self . dst , self . src ) \n 
self . runner . run_ignoring_errors ( cmd ) \n 
~~ ~~ class DestNovaInstanceDestroyer ( object ) : \n 
def __init__ ( self , dest_libvirt , dest_nova , libvirt_name , nova_vm_id ) : \n 
~~~ self . dest_libvirt = dest_libvirt \n 
self . dest_nova = dest_nova \n 
self . libvirt_name = libvirt_name \n 
self . nova_vm_id = nova_vm_id \n 
~~~ self . do ( ) \n 
~~~ self . undo ( ) \n 
~~ def do ( self ) : \n 
~~~ self . dest_libvirt . destroy_vm ( self . libvirt_name ) \n 
~~ def undo ( self ) : \n 
self . dest_nova . reset_state ( self . nova_vm_id ) \n 
self . dest_nova . delete_vm_by_id ( self . nova_vm_id ) \n 
~~ except RuntimeError : \n 
~~ ~~ ~~ class Libvirt ( object ) : \n 
~~~ def __init__ ( self , remote_runner ) : \n 
self . runner = remote_runner \n 
~~ def get_backing_file ( self , instance_id ) : \n 
image_path = instance_image_path ( instance_id ) ) ) \n 
~~~ image_info = json . loads ( self . runner . run ( cmd ) ) \n 
return image_info [ ] \n 
~~ except ( ValueError , TypeError ) as e : \n 
instance_id ) \n 
~~ ~~ def get_xml ( self , libvirt_instance_name ) : \n 
inst_name = libvirt_instance_name ) ) \n 
return LibvirtXml ( self . runner . run ( cmd ) ) \n 
~~ def destroy_vm ( self , libvirt_instance_name ) : \n 
~~~ cmds = [ \n 
for cmd in cmds : \n 
~~~ self . runner . run ( cmd ) \n 
~~ ~~ def move_backing_file ( self , source_file , instance_id ) : \n 
~~~ cmd = _qemu_img_rebase ( src = source_file , \n 
dst = instance_image_path ( instance_id ) ) \n 
~~ def live_migrate ( self , libvirt_instance_name , dest_host , migration_xml ) : \n 
dst_host = dest_host , \n 
migration_xml = migration_xml ) ) \n 
~~ ~~ class LibvirtDeviceInterfaceHwAddress ( object ) : \n 
~~~ def __init__ ( self , element ) : \n 
~~~ self . type = element . get ( ) \n 
self . domain = element . get ( ) \n 
self . bus = element . get ( ) \n 
self . slot = element . get ( ) \n 
self . function = element . get ( ) \n 
~~~ return ( isinstance ( other , self . __class__ ) and \n 
self . type == other . type and \n 
self . domain == other . domain and \n 
self . bus == other . bus and \n 
self . slot == other . slot and \n 
self . function == other . function ) \n 
~~ ~~ class LibvirtDeviceInterface ( object ) : \n 
~~~ def __init__ ( self , interface ) : \n 
self . _xml_element = interface \n 
self . mac = interface . find ( ) . get ( ) \n 
self . source_iface = interface . find ( ) . get ( ) \n 
self . target_iface = interface . find ( ) . get ( ) \n 
self . hw_address = LibvirtDeviceInterfaceHwAddress ( \n 
interface . find ( ) ) \n 
self . source_iface == other . source_iface and \n 
self . target_iface == other . target_iface and \n 
self . hw_address == other . hw_address ) \n 
mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n 
def _replace_attr ( cls , element , attr , value ) : \n 
~~~ if element . get ( attr ) != value : \n 
~~~ element . clear ( ) \n 
element . attrib = { attr : value } \n 
~~ ~~ def element ( self ) : \n 
~~~ source = self . _xml_element . find ( ) \n 
target = self . _xml_element . find ( ) \n 
self . _replace_attr ( source , , self . source_iface ) \n 
self . _replace_attr ( target , , self . target_iface ) \n 
return self . _xml_element \n 
~~ ~~ class LibvirtXml ( object ) : \n 
~~~ def __init__ ( self , contents ) : \n 
self . _xml = ElementTree . fromstring ( contents ) \n 
self . _interfaces = [ LibvirtDeviceInterface ( i ) \n 
for i in self . _xml . findall ( ) ] \n 
self . disk_file = self . _get ( , ) \n 
self . serial_file = self . _get ( , ) \n 
self . console_file = self . _get ( , ) \n 
~~ def _get ( self , element , attribute ) : \n 
~~~ el = self . _xml . find ( element ) \n 
if el is not None : \n 
~~~ return el . get ( attribute ) \n 
~~ ~~ def _set ( self , element , attribute , value ) : \n 
~~~ el . set ( attribute , value ) \n 
def interfaces ( self ) : \n 
~~~ return self . _interfaces \n 
~~ @ interfaces . setter \n 
def interfaces ( self , other ) : \n 
if len ( self . interfaces ) != len ( other ) : \n 
~~ for other_iface in other : \n 
~~~ for this_iface in self . interfaces : \n 
~~~ identical = ( this_iface . mac == other_iface . mac ) \n 
if identical : \n 
~~~ this_iface . source_iface = other_iface . source_iface \n 
this_iface . target_iface = other_iface . target_iface \n 
~~ ~~ ~~ ~~ def dump ( self ) : \n 
~~~ self . _set ( , , self . disk_file ) \n 
self . _set ( , , self . serial_file ) \n 
self . _set ( , , self . console_file ) \n 
xml_devices = self . _xml . find ( ) \n 
xml_interfaces = self . _xml . findall ( ) \n 
for iface in xml_interfaces : \n 
~~~ xml_devices . remove ( iface ) \n 
~~ for iface in self . _interfaces : \n 
~~~ xml_devices . append ( iface . element ( ) ) \n 
~~ return ElementTree . tostring ( self . _xml ) \n 
~~ ~~ import abc \n 
from cloudferry . lib . utils import files \n 
from cloudferry . lib . utils import remote_runner \n 
from cloudferry . lib . copy_engines import base \n 
class CopyFailed ( RuntimeError ) : \n 
~~ class CopyMechanism ( object ) : \n 
~~~ __metaclass__ = abc . ABCMeta \n 
@ abc . abstractmethod \n 
def copy ( self , context , source_object , destination_object ) : \n 
~~ ~~ class CopyObject ( object ) : \n 
~~~ def __init__ ( self , host = None , path = None ) : \n 
~~~ self . host = host \n 
~~~ return "{host}:{path}" . format ( host = self . host , path = self . path ) \n 
~~ ~~ class RemoteFileCopy ( CopyMechanism ) : \n 
~~~ data = { \n 
: source_object . host , \n 
: source_object . path , \n 
: destination_object . host , \n 
: destination_object . path \n 
~~~ copier = base . get_copier ( context . src_cloud , \n 
context . dst_cloud , \n 
data ) \n 
copier . transfer ( data ) \n 
~~ except ( base . FileCopyError , \n 
base . CopierCannotBeUsed , \n 
base . CopierNotFound ) as e : \n 
src_host = source_object . host , \n 
src_file = source_object . path , \n 
dst_host = destination_object . host , \n 
dst_file = destination_object . path , \n 
err = e . message ) \n 
raise CopyFailed ( msg ) \n 
~~ ~~ ~~ class CopyRegularFileToBlockDevice ( CopyMechanism ) : \n 
~~~ src_user = context . cfg . src . ssh_user \n 
dst_user = context . cfg . dst . ssh_user \n 
src_host = source_object . host \n 
dst_host = destination_object . host \n 
rr = remote_runner . RemoteRunner ( src_host , src_user ) \n 
ssh_opts = ( \n 
~~~ progress_view = "" \n 
if files . is_installed ( rr , "pv" ) : \n 
~~~ src_file_size = files . remote_file_size ( rr , source_object . path ) \n 
size = src_file_size ) \n 
rr . run ( copy . format ( src_file = source_object . path , \n 
dst_user = dst_user , \n 
dst_host = dst_host , \n 
ssh_opts = ssh_opts , \n 
dst_device = destination_object . path , \n 
progress_view = progress_view ) ) \n 
~~ except remote_runner . RemoteExecutionError as e : \n 
msg = msg . format ( src_object = source_object , \n 
dst_object = destination_object , \n 
error = e . message ) \n 
~~ ~~ ~~ import datetime \n 
from logging import config \n 
from logging import handlers \n 
from fabric import api \n 
import yaml \n 
from cloudferry . lib . utils import sizeof_format \n 
getLogger = logging . getLogger \n 
class StdoutLogger ( object ) : \n 
def __init__ ( self , name = None ) : \n 
~~~ self . log = logging . getLogger ( name or ) \n 
~~ def write ( self , message ) : \n 
~~~ message = message . strip ( ) \n 
if message : \n 
~~~ self . log . info ( message ) \n 
~~ ~~ def flush ( self ) : \n 
~~ ~~ def configure_logging ( log_config = None , debug = None , forward_stdout = None ) : \n 
if log_config is None : \n 
~~~ log_config = CONF . migrate . log_config \n 
~~ if debug is None : \n 
~~~ debug = CONF . migrate . debug \n 
~~ if forward_stdout is None : \n 
~~~ forward_stdout = CONF . migrate . forward_stdout \n 
~~ with open ( log_config , ) as f : \n 
~~~ config . dictConfig ( yaml . load ( f ) ) \n 
~~ if debug : \n 
~~~ logger = logging . getLogger ( ) \n 
for handler in logger . handlers : \n 
~~~ if handler . name == : \n 
~~~ handler . setLevel ( logging . DEBUG ) \n 
~~ ~~ ~~ if forward_stdout : \n 
~~~ sys . stdout = StdoutLogger ( ) \n 
~~ ~~ class RunRotatingFileHandler ( handlers . RotatingFileHandler ) : \n 
filename = , \n 
date_format = , \n 
** kwargs ) : \n 
~~~ self . date_format = date_format \n 
max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n 
super ( RunRotatingFileHandler , self ) . __init__ ( \n 
filename = self . get_filename ( filename ) , \n 
maxBytes = max_bytes , \n 
** kwargs ) \n 
~~ def get_filename ( self , filename ) : \n 
if hasattr ( CONF , ) and hasattr ( CONF . migrate , ) : \n 
~~~ scenario_filename = os . path . basename ( CONF . migrate . scenario ) \n 
scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n 
~~~ scenario = \n 
~~ dt = datetime . datetime . now ( ) . strftime ( self . date_format ) \n 
return filename % { \n 
: scenario , \n 
: dt \n 
~~ ~~ class CurrentTaskFilter ( logging . Filter ) : \n 
def __init__ ( self , name_format = , ** kwargs ) : \n 
~~~ super ( CurrentTaskFilter , self ) . __init__ ( ** kwargs ) \n 
self . name_format = name_format \n 
~~ def filter ( self , record ) : \n 
~~~ current_task = self . name_format % { \n 
: api . env . current_task or , \n 
record . current_task = current_task \n 
import cloudferry_devlab . tests . config as config \n 
from cloudferry_devlab . tests . data_collector import DataCollector \n 
from cloudferry_devlab . tests import functional_test \n 
import cloudferry_devlab . tests . utils as utils \n 
class RollbackVerification ( functional_test . FunctionalTest ) : \n 
~~~ data_collector = DataCollector ( config = config ) \n 
self . data_after = utils . convert ( data_collector . data_collector ( ) ) \n 
file_name = config . rollback_params [ ] [ ] \n 
pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n 
with open ( pre_file_path , "r" ) as f : \n 
~~~ self . pre_data = yaml . load ( f ) \n 
~~ ~~ def test_verify_rollback ( self ) : \n 
self . maxDiff = None \n 
for cloud in self . data_after : \n 
~~~ for service in self . data_after [ cloud ] : \n 
~~~ for resource in self . data_after [ cloud ] [ service ] : \n 
~~~ print ( msg . format ( service . lower ( ) , resource . lower ( ) ) ) \n 
self . assertEqual ( self . data_after [ cloud ] [ service ] [ resource ] , \n 
self . pre_data [ cloud ] [ service ] [ resource ] ) \n 
~~ ~~ ~~ ~~ ~~ import mock \n 
from cloudferry . lib . os . actions import convert_volume_to_image \n 
from tests import test \n 
class ConverterVolumeToImageTest ( test . TestCase ) : \n 
~~~ super ( ConverterVolumeToImageTest , self ) . setUp ( ) \n 
self . fake_src_cloud = mock . Mock ( ) \n 
self . fake_storage = mock . Mock ( ) \n 
self . fake_storage . deploy = mock . Mock ( ) \n 
self . fake_storage . upload_volume_to_image . return_value = ( \n 
, ) \n 
self . fake_storage . get_backend . return_value = \n 
self . fake_image = mock . Mock ( ) \n 
self . fake_image . wait_for_status = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted . return_value = { \n 
: { \n 
: { : , : { } } } } \n 
self . fake_image . patch_image = mock . Mock ( ) \n 
self . fake_src_cloud . resources = { : self . fake_storage , \n 
: self . fake_image } \n 
self . fake_volumes_info = { \n 
} } , \n 
self . fake_dst_cloud = mock . Mock ( ) \n 
self . fake_config = utils . ext_dict ( migrate = utils . ext_dict ( \n 
{ : , \n 
: } ) ) \n 
self . fake_init = { \n 
: self . fake_src_cloud , \n 
: self . fake_dst_cloud , \n 
: self . fake_config \n 
~~ def test_action ( self ) : \n 
~~~ fake_action = convert_volume_to_image . ConvertVolumeToImage ( \n 
self . fake_init , \n 
cloud = ) \n 
res = fake_action . run ( self . fake_volumes_info ) \n 
self . assertEqual ( , \n 
res [ ] [ ] [ ] [ ] ) \n 
res [ ] [ ] [ ] [ ] [ \n 
] [ ] ) \n 
~~ ~~ from cloudferry . lib . utils . cache import Memoized , Cached \n 
class MemoizationTestCase ( test . TestCase ) : \n 
~~~ def test_treats_self_as_separate_objects ( self ) : \n 
~~~ class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
~~ @ Memoized \n 
def get_i ( self ) : \n 
~~~ return self . i \n 
~~ ~~ o1 = C ( 1 ) \n 
o2 = C ( 2 ) \n 
self . assertNotEqual ( o1 . get_i ( ) , o2 . get_i ( ) ) \n 
self . assertEqual ( o1 . get_i ( ) , 1 ) \n 
self . assertEqual ( o2 . get_i ( ) , 2 ) \n 
~~ def test_takes_value_from_cache ( self ) : \n 
~~ def set_i ( self , i ) : \n 
~~ ~~ original = 1 \n 
o = C ( original ) \n 
self . assertEqual ( o . get_i ( ) , original ) \n 
o . set_i ( 10 ) \n 
~~ ~~ class CacheTestCase ( test . TestCase ) : \n 
~~~ def test_resets_cache_when_modifier_called ( self ) : \n 
~~~ @ Cached ( getter = , modifier = ) \n 
class C ( object ) : \n 
~~ def get_i ( self ) : \n 
~~ ~~ o = C ( 1 ) \n 
self . assertEqual ( o . get_i ( ) , 1 ) \n 
o . set_i ( 100 ) \n 
self . assertEqual ( o . get_i ( ) , 100 ) \n 
~~ ~~ from django . http import HttpResponse , HttpResponseRedirect , HttpResponseNotFound \n 
from django . template import Context , loader \n 
from django . core . urlresolvers import reverse \n 
from django . shortcuts import get_object_or_404 , render_to_response \n 
from django . core . exceptions import ObjectDoesNotExist \n 
from tagging . models import Tag , TaggedItem \n 
from django . views . decorators . csrf import csrf_exempt \n 
from django . contrib . auth . models import User \n 
from django . contrib . auth . decorators import login_required \n 
from django . contrib . auth import authenticate , login \n 
from django . core . mail import send_mail \n 
from django . http import Http404 \n 
from django . db . models import Q \n 
from openwatch . recordings . models import Recording \n 
from openwatch import recording_tags \n 
@ login_required \n 
def moderate ( request ) : \n 
response_values = { } \n 
org_tag = request . user . get_profile ( ) . org_tag \n 
if not request . user . is_superuser and ( not request . user . get_profile ( ) . can_moderate or org_tag == ) : \n 
~~~ raise Http404 \n 
~~ if recording_tags . ACLU_NJ in org_tag : \n 
~~~ location = { } \n 
location [ ] = 40.167274 \n 
location [ ] = - 74.616338 \n 
response_values [ ] = location \n 
~~ response_values [ ] = \n 
return render_to_response ( , response_values , context_instance = RequestContext ( request ) ) \n 
~~ def map ( request ) : \n 
~~~ total = "lots!" \n 
return render_to_response ( , { : total } , context_instance = RequestContext ( request ) ) \n 
~~ def size ( request ) : \n 
~~~ featureset = Recording . objects . filter ( ~ Q ( lat = None ) , ~ Q ( lon = None ) , ~ Q ( jtype = ) ) . exclude ( location__exact = ) . exclude ( location__exact = ) . order_by ( ) \n 
total = len ( featureset ) \n 
~~ def redir ( self ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
~~ def map_json ( request ) : \n 
~~~ featureset = Recording . objects . all ( ) . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) [ : 750 ] \n 
resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
~~ @ login_required \n 
def map_json_moderate ( request ) : \n 
~~~ org_tag = request . user . get_profile ( ) . org_tag \n 
if org_tag != : \n 
~~~ featureset = Recording . objects . filter ( org_approved = False , org_flagged = False , tags__contains = org_tag ) \n 
~~~ featureset = Recording . objects . all ( ) \n 
~~ featureset = featureset . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) \n 
~~ def map_location_json ( request , ne_lat = 0 , ne_lon = 0 , sw_lat = 0 , sw_lon = 0 ) : \n 
~~~ ne_lat = float ( ne_lat ) \n 
ne_lon = float ( ne_lon ) \n 
sw_lat = float ( sw_lat ) \n 
sw_lon = float ( sw_lon ) \n 
featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon ) . order_by ( ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) . exclude ( location__exact = ) . exclude ( location__exact = ) [ : 750 ] \n 
if len ( featureset ) < 1 : \n 
~~~ return HttpResponse ( "{\\"objects\\":[]}" , mimetype = "application/json" ) \n 
~~ resp = encode_queryset ( featureset ) \n 
~~ def encode_queryset ( featureset ) : \n 
~~~ resp = \'{"objects":[\' \n 
for obj in featureset : \n 
~~~ resp = resp + json . dumps ( obj . to_dict ( ) ) + \n 
~~ resp = resp [ : - 1 ] + \n 
~~ from django . template import loader , RequestContext \n 
from django . http import HttpResponse , Http404 \n 
from django . http import HttpResponseRedirect , HttpResponsePermanentRedirect \n 
from django . db . models . base import ModelBase \n 
from django . db . models . manager import Manager \n 
from django . db . models . query import QuerySet \n 
from django . core import urlresolvers \n 
from django . utils import six \n 
~~~ import json \n 
def render_to_easy_api_response ( * args , ** kwargs ) : \n 
httpresponse_kwargs = { : kwargs . pop ( , None ) } \n 
context = kwargs . pop ( ) \n 
processors = context . context_processors \n 
request = processors [ ] [ ] \n 
if request . GET . has_key ( ) : \n 
~~~ api_type = request . GET [ ] \n 
for arg in args : \n 
~~~ passed = arg \n 
~~ dump_me = { } \n 
for key in passed . keys ( ) : \n 
~~~ value = passed [ key ] \n 
dump_me [ key ] = dump_object ( value ) \n 
~~ if api_type == : \n 
~~~ def replace_spaces ( dump_me ) : \n 
~~~ new = { } \n 
for k , v in dump_me . iteritems ( ) : \n 
~~~ if isinstance ( v , dict ) : \n 
~~~ v = replace_spaces ( v ) \n 
~~ new [ k . replace ( , ) ] = v \n 
~~ return new \n 
~~ new = replace_spaces ( dump_me ) \n 
dump_me = dict2xml ( new ) \n 
pretty = dom . toprettyxml ( ) \n 
return HttpResponse ( pretty , content_type = ) \n 
~~~ yml = yaml . safe_dump ( dump_me ) \n 
return HttpResponse ( yml , content_type = ) \n 
return HttpResponse ( dump_me , content_type = ) \n 
~~ ~~ return HttpResponse ( loader . render_to_string ( * args , ** kwargs ) , ** httpresponse_kwargs ) \n 
~~ def render_to_response ( * args , ** kwargs ) : \n 
return render_to_easy_api_response ( * args , ** kwargs ) \n 
## \n 
~~ def dump_object ( queryset ) : \n 
~~~ d = DataDumper ( ) \n 
ret = d . dump ( queryset ) \n 
return ret \n 
~~~ modelName = queryset [ 0 ] . __class__ . __name__ \n 
modelNameData = [ ] \n 
fields = get_fields ( queryset [ 0 ] ) \n 
for obj in queryset : \n 
~~~ temp_dict = dict ( ) \n 
for field in fields : \n 
~~~ attribute = getattr ( obj , str ( field ) ) \n 
temp_dict [ field ] = attribute \n 
~~ ~~ modelNameData . append ( temp_dict ) \n 
~~ dthandler = lambda obj : obj . isoformat ( ) if isinstance ( obj , datetime . datetime ) or isinstance ( obj , datetime . date ) else None \n 
return json . loads ( json . dumps ( modelNameData , default = dthandler ) ) \n 
~~ ~~ def get_fields ( model ) : \n 
~~~ if hasattr ( model , "easy_api_fields" ) : \n 
~~~ fields = model . easy_api_fields ( ) \n 
~~~ fields = model . to_dict ( ) . keys ( ) \n 
~~~ fields = model . _meta . get_all_field_names ( ) \n 
~~ ~~ return fields \n 
~~ ~~ class SimpleEngagementCalculator ( object ) : \n 
~~~ def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n 
~~ ~~ ROOT_URLCONF = None \n 
DATABASE_ENGINE = \n 
DATABASE_NAME = \n 
DATABASE_SUPPORTS_TRANSACTIONS = False \n 
INSTALLED_APPS = [ \n 
TEMPLATE_CONTEXT_PROCESSORS = ( \n 
"django.core.context_processors.auth" , \n 
"django.core.context_processors.debug" , \n 
"django.core.context_processors.i18n" , \n 
"django.core.context_processors.media" , \n 
"django.core.context_processors.request" ) \n 
if __name__ == "__main__" : \n 
~~~ os . environ . setdefault ( "DJANGO_SETTINGS_MODULE" , "test_settings" ) \n 
from django . core . management import execute_from_command_line \n 
is_testing = in sys . argv \n 
if is_testing : \n 
~~~ import coverage \n 
cov = coverage . coverage ( include = "django_zappa/*" , omit = [ ] ) \n 
cov . erase ( ) \n 
cov . start ( ) \n 
~~ execute_from_command_line ( sys . argv ) \n 
~~~ cov . stop ( ) \n 
cov . save ( ) \n 
cov . report ( ) \n 
import simplexml , time , sys \n 
from protocol import * \n 
from client import PlugIn \n 
DefaultTimeout = 25 \n 
ID = 0 \n 
class Dispatcher ( PlugIn ) : \n 
~~~ PlugIn . __init__ ( self ) \n 
DBG_LINE = \n 
self . handlers = { } \n 
self . _expected = { } \n 
self . _defaultHandler = None \n 
self . _pendingExceptions = [ ] \n 
self . _eventHandler = None \n 
self . _cycleHandlers = [ ] \n 
self . _exported_methods = [ self . Process , self . RegisterHandler , self . RegisterDefaultHandler , self . RegisterEventHandler , self . UnregisterCycleHandler , self . RegisterCycleHandler , self . RegisterHandlerOnce , self . UnregisterHandler , self . RegisterProtocol , self . WaitForResponse , self . SendAndWaitForResponse , self . send , self . disconnect , self . SendAndCallForResponse , ] \n 
~~ def dumpHandlers ( self ) : \n 
return self . handlers \n 
~~ def restoreHandlers ( self , handlers ) : \n 
self . handlers = handlers \n 
~~ def _init ( self ) : \n 
self . RegisterNamespace ( ) \n 
self . RegisterNamespace ( NS_STREAMS ) \n 
self . RegisterNamespace ( self . _owner . defaultNamespace ) \n 
self . RegisterProtocol ( , Iq ) \n 
self . RegisterProtocol ( , Presence ) \n 
self . RegisterProtocol ( , Message ) \n 
self . RegisterDefaultHandler ( self . returnStanzaHandler ) \n 
self . RegisterHandler ( , self . streamErrorHandler , xmlns = NS_STREAMS ) \n 
~~ def plugin ( self , owner ) : \n 
self . _init ( ) \n 
for method in self . _old_owners_methods : \n 
~~~ if method . __name__ == : self . _owner_send = method ; break \n 
~~ self . _owner . lastErrNode = None \n 
self . _owner . lastErr = None \n 
self . _owner . lastErrCode = None \n 
self . StreamInit ( ) \n 
~~ def plugout ( self ) : \n 
self . Stream . dispatch = None \n 
self . Stream . DEBUG = None \n 
self . Stream . features = None \n 
self . Stream . destroy ( ) \n 
~~ def StreamInit ( self ) : \n 
self . Stream = simplexml . NodeBuilder ( ) \n 
self . Stream . _dispatch_depth = 2 \n 
self . Stream . dispatch = self . dispatch \n 
self . Stream . stream_header_received = self . _check_stream_start \n 
self . _owner . debug_flags . append ( simplexml . DBG_NODEBUILDER ) \n 
self . Stream . DEBUG = self . _owner . DEBUG \n 
self . _metastream = Node ( ) \n 
self . _metastream . setNamespace ( self . _owner . Namespace ) \n 
self . _metastream . setAttr ( , ) \n 
self . _metastream . setAttr ( , NS_STREAMS ) \n 
self . _metastream . setAttr ( , self . _owner . Server ) \n 
~~ def _check_stream_start ( self , ns , tag , attrs ) : \n 
~~~ if ns < > NS_STREAMS or tag < > : \n 
~~~ raise ValueError ( % ( tag , ns ) ) \n 
~~ ~~ def Process ( self , timeout = 0 ) : \n 
for handler in self . _cycleHandlers : handler ( self ) \n 
if len ( self . _pendingExceptions ) > 0 : \n 
~~~ _pendingException = self . _pendingExceptions . pop ( ) \n 
raise _pendingException [ 0 ] , _pendingException [ 1 ] , _pendingException [ 2 ] \n 
~~ if self . _owner . Connection . pending_data ( timeout ) : \n 
~~~ try : data = self . _owner . Connection . receive ( ) \n 
except IOError : return \n 
self . Stream . Parse ( data ) \n 
~~ if data : return len ( data ) \n 
~~ def RegisterNamespace ( self , xmlns , order = ) : \n 
self . handlers [ xmlns ] = { } \n 
self . RegisterProtocol ( , Protocol , xmlns = xmlns ) \n 
~~ def RegisterProtocol ( self , tag_name , Proto , xmlns = None , order = ) : \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . handlers [ xmlns ] [ tag_name ] = { type : Proto , : [ ] } \n 
~~ def RegisterNamespaceHandler ( self , xmlns , handler , typ = , ns = , makefirst = 0 , system = 0 ) : \n 
self . RegisterHandler ( , handler , typ , ns , xmlns , makefirst , system ) \n 
~~ def RegisterHandler ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
if not typ and not ns : typ = \n 
if not self . handlers . has_key ( xmlns ) : self . RegisterNamespace ( xmlns , ) \n 
if not self . handlers [ xmlns ] . has_key ( name ) : self . RegisterProtocol ( name , Protocol , xmlns , ) \n 
if not self . handlers [ xmlns ] [ name ] . has_key ( typ + ns ) : self . handlers [ xmlns ] [ name ] [ typ + ns ] = [ ] \n 
if makefirst : self . handlers [ xmlns ] [ name ] [ typ + ns ] . insert ( 0 , { : handler , : system } ) \n 
else : self . handlers [ xmlns ] [ name ] [ typ + ns ] . append ( { : handler , : system } ) \n 
~~ def RegisterHandlerOnce ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
self . RegisterHandler ( name , handler , typ , ns , xmlns , makefirst , system ) \n 
~~ def UnregisterHandler ( self , name , handler , typ = , ns = , xmlns = None ) : \n 
if not self . handlers . has_key ( xmlns ) : return \n 
for pack in self . handlers [ xmlns ] [ name ] [ typ + ns ] : \n 
~~~ if handler == pack [ ] : break \n 
~~ else : pack = None \n 
try : self . handlers [ xmlns ] [ name ] [ typ + ns ] . remove ( pack ) \n 
except ValueError : pass \n 
~~ def RegisterDefaultHandler ( self , handler ) : \n 
self . _defaultHandler = handler \n 
~~ def RegisterEventHandler ( self , handler ) : \n 
self . _eventHandler = handler \n 
~~ def returnStanzaHandler ( self , conn , stanza ) : \n 
if stanza . getType ( ) in [ , ] : \n 
~~~ conn . send ( Error ( stanza , ERR_FEATURE_NOT_IMPLEMENTED ) ) \n 
~~ ~~ def streamErrorHandler ( self , conn , error ) : \n 
~~~ name , text = , error . getData ( ) \n 
for tag in error . getChildren ( ) : \n 
~~~ if tag . getNamespace ( ) == NS_XMPP_STREAMS : \n 
~~~ if tag . getName ( ) == : text = tag . getData ( ) \n 
else : name = tag . getName ( ) \n 
~~ ~~ if name in stream_exceptions . keys ( ) : exc = stream_exceptions [ name ] \n 
else : exc = StreamError \n 
raise exc ( ( name , text ) ) \n 
~~ def RegisterCycleHandler ( self , handler ) : \n 
if handler not in self . _cycleHandlers : self . _cycleHandlers . append ( handler ) \n 
~~ def UnregisterCycleHandler ( self , handler ) : \n 
if handler in self . _cycleHandlers : self . _cycleHandlers . remove ( handler ) \n 
~~ def Event ( self , realm , event , data ) : \n 
if self . _eventHandler : self . _eventHandler ( realm , event , data ) \n 
~~ def dispatch ( self , stanza , session = None , direct = 0 ) : \n 
if not session : session = self \n 
session . Stream . _mini_dom = None \n 
name = stanza . getName ( ) \n 
if not direct and self . _owner . _route : \n 
~~~ if name == : \n 
~~~ if stanza . getAttr ( ) == None : \n 
~~~ if len ( stanza . getChildren ( ) ) == 1 : \n 
~~~ stanza = stanza . getChildren ( ) [ 0 ] \n 
~~~ for each in stanza . getChildren ( ) : \n 
~~~ self . dispatch ( each , session , direct = 1 ) \n 
~~ return \n 
~~ ~~ ~~ elif name == : \n 
~~ elif name in ( , ) : \n 
~~~ raise UnsupportedStanzaType ( name ) \n 
~~ ~~ if name == : session . Stream . features = stanza \n 
xmlns = stanza . getNamespace ( ) \n 
if not self . handlers . has_key ( xmlns ) : \n 
xmlns = \n 
~~ if not self . handlers [ xmlns ] . has_key ( name ) : \n 
name = \n 
~~ if stanza . __class__ . __name__ == : stanza = self . handlers [ xmlns ] [ name ] [ type ] ( node = stanza ) \n 
typ = stanza . getType ( ) \n 
if not typ : typ = \n 
stanza . props = stanza . getProperties ( ) \n 
ID = stanza . getID ( ) \n 
for prop in stanza . props : \n 
~~~ if self . handlers [ xmlns ] [ name ] . has_key ( prop ) : list . append ( prop ) \n 
~~ chain = self . handlers [ xmlns ] [ ] [ ] \n 
for key in list : \n 
~~~ if key : chain = chain + self . handlers [ xmlns ] [ name ] [ key ] \n 
~~ output = \n 
if session . _expected . has_key ( ID ) : \n 
~~~ user = 0 \n 
if type ( session . _expected [ ID ] ) == type ( ( ) ) : \n 
~~~ cb , args = session . _expected [ ID ] \n 
try : cb ( session , stanza , ** args ) \n 
except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : raise \n 
session . _expected [ ID ] = stanza \n 
~~ ~~ else : user = 1 \n 
for handler in chain : \n 
~~~ if user or handler [ ] : \n 
~~~ handler [ ] ( session , stanza ) \n 
~~ except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : \n 
~~~ self . _pendingExceptions . insert ( 0 , sys . exc_info ( ) ) \n 
~~ user = 0 \n 
~~ ~~ ~~ if user and self . _defaultHandler : self . _defaultHandler ( session , stanza ) \n 
~~ def WaitForResponse ( self , ID , timeout = DefaultTimeout ) : \n 
self . _expected [ ID ] = None \n 
has_timed_out = 0 \n 
abort_time = time . time ( ) + timeout \n 
while not self . _expected [ ID ] : \n 
~~~ if not self . Process ( 0.04 ) : \n 
~~~ self . _owner . lastErr = "Disconnect" \n 
return None \n 
~~ if time . time ( ) > abort_time : \n 
~~~ self . _owner . lastErr = "Timeout" \n 
~~ ~~ response = self . _expected [ ID ] \n 
del self . _expected [ ID ] \n 
if response . getErrorCode ( ) : \n 
~~~ self . _owner . lastErrNode = response \n 
self . _owner . lastErr = response . getError ( ) \n 
self . _owner . lastErrCode = response . getErrorCode ( ) \n 
~~ return response \n 
~~ def SendAndWaitForResponse ( self , stanza , timeout = DefaultTimeout ) : \n 
return self . WaitForResponse ( self . send ( stanza ) , timeout ) \n 
~~ def SendAndCallForResponse ( self , stanza , func , args = { } ) : \n 
self . _expected [ self . send ( stanza ) ] = ( func , args ) \n 
~~ def send ( self , stanza ) : \n 
if type ( stanza ) in [ type ( ) , type ( ) ] : return self . _owner_send ( stanza ) \n 
if not isinstance ( stanza , Protocol ) : _ID = None \n 
elif not stanza . getID ( ) : \n 
~~~ global ID \n 
ID += 1 \n 
_ID = ` ID ` \n 
stanza . setID ( _ID ) \n 
~~ else : _ID = stanza . getID ( ) \n 
if self . _owner . _registered_name and not stanza . getAttr ( ) : stanza . setAttr ( , self . _owner . _registered_name ) \n 
if self . _owner . _route and stanza . getName ( ) != : \n 
~~~ to = self . _owner . Server \n 
if stanza . getTo ( ) and stanza . getTo ( ) . getDomain ( ) : \n 
~~~ to = stanza . getTo ( ) . getDomain ( ) \n 
~~ frm = stanza . getFrom ( ) \n 
if frm . getDomain ( ) : \n 
~~~ frm = frm . getDomain ( ) \n 
~~ route = Protocol ( , to = to , frm = frm , payload = [ stanza ] ) \n 
stanza = route \n 
~~ stanza . setNamespace ( self . _owner . Namespace ) \n 
stanza . setParent ( self . _metastream ) \n 
self . _owner_send ( stanza ) \n 
return _ID \n 
~~ def disconnect ( self ) : \n 
self . _owner_send ( ) \n 
while self . Process ( 1 ) : pass \n 
~~ ~~ from . gl_utils import * \n 
from . texture import VideoTexture \n 
from . widget import Widget , BGUI_DEFAULT , WeakMethod \n 
from . image import Image \n 
class Video ( Image ) : \n 
def __init__ ( self , parent , vid , name = None , play_audio = False , repeat = 0 , aspect = None , size = [ 1 , 1 ] , pos = [ 0 , 0 ] , \n 
sub_theme = , options = BGUI_DEFAULT ) : \n 
Image . __init__ ( self , parent , name , None , aspect , size , pos , sub_theme = sub_theme , options = options ) \n 
self . _texture = VideoTexture ( vid , GL_LINEAR , repeat , play_audio ) \n 
self . _on_finish = None \n 
self . _on_finish_called = False \n 
~~ def play ( self , start , end , use_frames = True , fps = None ) : \n 
~~~ self . _texture . play ( start , end , use_frames , fps ) \n 
def on_finish ( self ) : \n 
return self . _on_finish \n 
~~ @ on_finish . setter \n 
def on_finish ( self , value ) : \n 
~~~ self . _on_finish = WeakMethod ( value ) \n 
~~ def _draw ( self ) : \n 
self . _texture . update ( ) \n 
Image . _draw ( self ) \n 
if self . _texture . video . status == 3 : \n 
~~~ if self . _on_finish and not self . _on_finish_called : \n 
~~~ self . on_finish ( self ) \n 
self . _on_finish_called = Truefrom django import template \n 
~~ ~~ ~~ ~~ from django . conf import settings \n 
class CheckGrappelli ( template . Node ) : \n 
~~~ def __init__ ( self , var_name ) : \n 
~~~ self . var_name = var_name \n 
~~~ context [ self . var_name ] = in settings . INSTALLED_APPS \n 
~~ ~~ def check_grappelli ( parser , token ) : \n 
bits = token . contents . split ( ) \n 
if len ( bits ) != 3 : \n 
~~ if bits [ 1 ] != : \n 
~~ varname = bits [ 2 ] \n 
return CheckGrappelli ( varname ) \n 
~~ register . tag ( check_grappelli ) \n 
from setuptools import setup , find_packages \n 
import sys , os \n 
__description__ = , \n 
__author__ = , \n 
__email__ = , \n 
sys . path . insert ( 0 , os . path . dirname ( __file__ ) ) \n 
REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n 
setup ( \n 
version = __version__ , \n 
url = , \n 
download_url = , \n 
license = __license__ , \n 
author = __author__ , \n 
author_email = __email__ , \n 
description = __description__ , \n 
long_description = __doc__ , \n 
test_suite = , \n 
zip_safe = False , \n 
platforms = , \n 
install_requires = REQUIRES , \n 
packages = find_packages ( exclude = ( , , ) ) , \n 
include_package_data = True , \n 
setup_requires = [ , ] , \n 
classifiers = [ \n 
from mongoengine . base import BaseField \n 
__all__ = ( ) \n 
class WtfBaseField ( BaseField ) : \n 
def __init__ ( self , validators = None , filters = None , ** kwargs ) : \n 
~~~ self . validators = self . _ensure_callable_or_list ( validators , ) \n 
self . filters = self . _ensure_callable_or_list ( filters , ) \n 
BaseField . __init__ ( self , ** kwargs ) \n 
~~ def _ensure_callable_or_list ( self , field , msg_flag ) : \n 
if field is not None : \n 
~~~ if callable ( field ) : \n 
~~~ field = [ field ] \n 
if not isinstance ( field , list ) : \n 
~~~ raise TypeError ( msg ) \n 
~~ ~~ ~~ return field \n 
~~ ~~ from bson import DBRef , SON \n 
from mongoengine . python_support import txt_type \n 
from base import ( \n 
BaseDict , BaseList , EmbeddedDocumentList , \n 
TopLevelDocumentMetaclass , get_document \n 
from fields import ( ReferenceField , ListField , DictField , MapField ) \n 
from connection import get_db \n 
from queryset import QuerySet \n 
from document import Document , EmbeddedDocument \n 
class DeReference ( object ) : \n 
~~~ def __call__ ( self , items , max_depth = 1 , instance = None , name = None ) : \n 
if items is None or isinstance ( items , basestring ) : \n 
~~~ return items \n 
~~ if isinstance ( items , QuerySet ) : \n 
~~~ items = [ i for i in items ] \n 
~~ self . max_depth = max_depth \n 
doc_type = None \n 
if instance and isinstance ( instance , ( Document , EmbeddedDocument , \n 
TopLevelDocumentMetaclass ) ) : \n 
~~~ doc_type = instance . _fields . get ( name ) \n 
while hasattr ( doc_type , ) : \n 
~~~ doc_type = doc_type . field \n 
~~ if isinstance ( doc_type , ReferenceField ) : \n 
~~~ field = doc_type \n 
doc_type = doc_type . document_type \n 
is_list = not hasattr ( items , ) \n 
if is_list and all ( [ i . __class__ == doc_type for i in items ] ) : \n 
~~ elif not is_list and all ( \n 
[ i . __class__ == doc_type for i in items . values ( ) ] ) : \n 
~~ elif not field . dbref : \n 
~~~ if not hasattr ( items , ) : \n 
~~~ def _get_items ( items ) : \n 
~~~ new_items = [ ] \n 
for v in items : \n 
~~~ if isinstance ( v , list ) : \n 
~~~ new_items . append ( _get_items ( v ) ) \n 
~~ elif not isinstance ( v , ( DBRef , Document ) ) : \n 
~~~ new_items . append ( field . to_python ( v ) ) \n 
~~~ new_items . append ( v ) \n 
~~ ~~ return new_items \n 
~~ items = _get_items ( items ) \n 
~~~ items = dict ( [ \n 
( k , field . to_python ( v ) ) \n 
if not isinstance ( v , ( DBRef , Document ) ) else ( k , v ) \n 
for k , v in items . iteritems ( ) ] \n 
~~ ~~ ~~ ~~ self . reference_map = self . _find_references ( items ) \n 
self . object_map = self . _fetch_objects ( doc_type = doc_type ) \n 
return self . _attach_objects ( items , 0 , instance , name ) \n 
~~ def _find_references ( self , items , depth = 0 ) : \n 
reference_map = { } \n 
if not items or depth >= self . max_depth : \n 
~~~ return reference_map \n 
~~ if not hasattr ( items , ) : \n 
~~~ iterator = enumerate ( items ) \n 
~~~ iterator = items . iteritems ( ) \n 
~~ depth += 1 \n 
for k , item in iterator : \n 
~~~ if isinstance ( item , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in item . _fields . iteritems ( ) : \n 
~~~ v = item . _data . get ( field_name , None ) \n 
if isinstance ( v , DBRef ) : \n 
~~~ reference_map . setdefault ( field . document_type , set ( ) ) . add ( v . id ) \n 
~~ elif isinstance ( v , ( dict , SON ) ) and in v : \n 
~~~ reference_map . setdefault ( get_document ( v [ ] ) , set ( ) ) . add ( v [ ] . id ) \n 
~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ field_cls = getattr ( getattr ( field , , None ) , , None ) \n 
references = self . _find_references ( v , depth ) \n 
for key , refs in references . iteritems ( ) : \n 
~~~ if isinstance ( field_cls , ( Document , TopLevelDocumentMetaclass ) ) : \n 
~~~ key = field_cls \n 
~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ ~~ elif isinstance ( item , DBRef ) : \n 
~~~ reference_map . setdefault ( item . collection , set ( ) ) . add ( item . id ) \n 
~~ elif isinstance ( item , ( dict , SON ) ) and in item : \n 
~~~ reference_map . setdefault ( get_document ( item [ ] ) , set ( ) ) . add ( item [ ] . id ) \n 
~~ elif isinstance ( item , ( dict , list , tuple ) ) and depth - 1 <= self . max_depth : \n 
~~~ references = self . _find_references ( item , depth - 1 ) \n 
~~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ return reference_map \n 
~~ def _fetch_objects ( self , doc_type = None ) : \n 
object_map = { } \n 
for collection , dbrefs in self . reference_map . iteritems ( ) : \n 
~~~ col_name = collection . _get_collection_name ( ) \n 
refs = [ dbref for dbref in dbrefs \n 
if ( col_name , dbref ) not in object_map ] \n 
references = collection . objects . in_bulk ( refs ) \n 
for key , doc in references . iteritems ( ) : \n 
~~~ object_map [ ( col_name , key ) ] = doc \n 
~~~ if isinstance ( doc_type , ( ListField , DictField , MapField , ) ) : \n 
~~ refs = [ dbref for dbref in dbrefs \n 
if ( collection , dbref ) not in object_map ] \n 
if doc_type : \n 
~~~ references = doc_type . _get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
for ref in references : \n 
~~~ doc = doc_type . _from_son ( ref ) \n 
object_map [ ( collection , doc . id ) ] = doc \n 
~~~ references = get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
~~~ if in ref : \n 
~~~ doc = get_document ( ref [ "_cls" ] ) . _from_son ( ref ) \n 
~~ elif doc_type is None : \n 
~~~ doc = get_document ( \n 
. join ( x . capitalize ( ) \n 
for x in collection . split ( ) ) ) . _from_son ( ref ) \n 
~~ object_map [ ( collection , doc . id ) ] = doc \n 
~~ ~~ ~~ ~~ return object_map \n 
~~ def _attach_objects ( self , items , depth = 0 , instance = None , name = None ) : \n 
if not items : \n 
~~~ if isinstance ( items , ( BaseDict , BaseList ) ) : \n 
~~ if instance : \n 
~~~ if isinstance ( items , dict ) : \n 
~~~ return BaseDict ( items , instance , name ) \n 
~~~ return BaseList ( items , instance , name ) \n 
~~ ~~ ~~ if isinstance ( items , ( dict , SON ) ) : \n 
~~~ if in items : \n 
~~~ return self . object_map . get ( \n 
( items [ ] . collection , items [ ] . id ) , items ) \n 
~~ elif in items : \n 
~~~ doc = get_document ( items [ ] ) . _from_son ( items ) \n 
_cls = doc . _data . pop ( , None ) \n 
del items [ ] \n 
doc . _data = self . _attach_objects ( doc . _data , depth , doc , None ) \n 
if _cls is not None : \n 
~~~ doc . _data [ ] = _cls \n 
~~ return doc \n 
~~ ~~ if not hasattr ( items , ) : \n 
~~~ is_list = True \n 
list_type = BaseList \n 
if isinstance ( items , EmbeddedDocumentList ) : \n 
~~~ list_type = EmbeddedDocumentList \n 
~~ as_tuple = isinstance ( items , tuple ) \n 
iterator = enumerate ( items ) \n 
data = [ ] \n 
~~~ is_list = False \n 
iterator = items . iteritems ( ) \n 
data = { } \n 
for k , v in iterator : \n 
~~~ if is_list : \n 
~~~ data . append ( v ) \n 
~~~ data [ k ] = v \n 
~~ if k in self . object_map and not is_list : \n 
~~~ data [ k ] = self . object_map [ k ] \n 
~~ elif isinstance ( v , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in v . _fields . iteritems ( ) : \n 
~~~ v = data [ k ] . _data . get ( field_name , None ) \n 
~~~ data [ k ] . _data [ field_name ] = self . object_map . get ( \n 
( v . collection , v . id ) , v ) \n 
( v [ ] . collection , v [ ] . id ) , v ) \n 
~~~ item_name = txt_type ( "{0}.{1}.{2}" ) . format ( name , k , field_name ) \n 
data [ k ] . _data [ field_name ] = self . _attach_objects ( v , depth , instance = instance , name = item_name ) \n 
~~ ~~ ~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ item_name = % ( name , k ) if name else name \n 
data [ k ] = self . _attach_objects ( v , depth - 1 , instance = instance , name = item_name ) \n 
~~ elif hasattr ( v , ) : \n 
~~~ data [ k ] = self . object_map . get ( ( v . collection , v . id ) , v ) \n 
~~ ~~ if instance and name : \n 
~~~ return tuple ( data ) if as_tuple else list_type ( data , instance , name ) \n 
~~ return BaseDict ( data , instance , name ) \n 
return data \n 
sys . path [ 0 : 0 ] = [ "" ] \n 
from mongoengine import * \n 
from mongoengine . connection import get_db \n 
__all__ = ( "GeoFieldTest" , ) \n 
class GeoFieldTest ( unittest . TestCase ) : \n 
~~~ connect ( db = ) \n 
self . db = get_db ( ) \n 
~~ def _test_for_expected_error ( self , Cls , loc , expected ) : \n 
~~~ Cls ( loc = loc ) . validate ( ) \n 
self . fail ( . format ( loc ) ) \n 
~~ except ValidationError as e : \n 
~~~ self . assertEqual ( expected , e . to_dict ( ) [ ] ) \n 
~~ ~~ def test_geopoint_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = GeoPointField ( ) \n 
~~ invalid_coords = [ { "x" : 1 , "y" : 2 } , 5 , "a" ] \n 
expected = \n 
for coord in invalid_coords : \n 
~~~ self . _test_for_expected_error ( Location , coord , expected ) \n 
~~ invalid_coords = [ [ ] , [ 1 ] , [ 1 , 2 , 3 ] ] \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
~~ invalid_coords = [ [ { } , { } ] , ( "a" , "b" ) ] \n 
~~ ~~ def test_point_validation ( self ) : \n 
~~~ loc = PointField ( ) \n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ ] } \n 
invalid_coords = { "type" : "Point" , "coordinates" : [ 1 , 2 , 3 ] } \n 
invalid_coords = [ 5 , "a" ] \n 
~~ Location ( loc = [ 1 , 2 ] ) . validate ( ) \n 
Location ( loc = { \n 
"type" : "Point" , \n 
"coordinates" : [ \n 
81.4471435546875 , \n 
23.61432859499169 \n 
] } ) . validate ( ) \n 
~~ def test_linestring_validation ( self ) : \n 
~~~ loc = LineStringField ( ) \n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
invalid_coords = { "type" : "LineString" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
invalid_coords = [ [ 1 ] ] \n 
invalid_coords = [ [ 1 , 2 , 3 ] ] \n 
invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
~~ Location ( loc = [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ) . validate ( ) \n 
~~ def test_polygon_validation ( self ) : \n 
~~~ loc = PolygonField ( ) \n 
invalid_coords = { "type" : "Polygon" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
invalid_coords = [ [ [ 5 , "a" ] ] ] \n 
invalid_coords = [ [ [ ] ] ] \n 
invalid_coords = [ [ [ 1 , 2 , 3 ] ] ] \n 
invalid_coords = [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] \n 
Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
~~ def test_multipoint_validation ( self ) : \n 
~~~ loc = MultiPointField ( ) \n 
invalid_coords = { "type" : "MultiPoint" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
invalid_coords = [ [ ] ] \n 
invalid_coords = [ [ [ 1 ] ] , [ [ 1 , 2 , 3 ] ] ] \n 
~~ invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
~~ Location ( loc = [ [ 1 , 2 ] ] ) . validate ( ) \n 
"type" : "MultiPoint" , \n 
[ 1 , 2 ] , \n 
[ 81.4471435546875 , 23.61432859499169 ] \n 
~~ def test_multilinestring_validation ( self ) : \n 
~~~ loc = MultiLineStringField ( ) \n 
invalid_coords = { "type" : "MultiLineString" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
invalid_coords = [ [ [ 1 ] ] ] \n 
invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( "a" , "b" ) ] ] ] \n 
~~ Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
~~ def test_multipolygon_validation ( self ) : \n 
~~~ loc = MultiPolygonField ( ) \n 
invalid_coords = { "type" : "MultiPolygon" , "coordinates" : [ [ [ [ 1 , 2 , 3 ] ] ] ] } \n 
invalid_coords = [ [ [ [ 5 , "a" ] ] ] ] \n 
invalid_coords = [ [ [ [ ] ] ] ] \n 
invalid_coords = [ [ [ [ 1 , 2 , 3 ] ] ] ] \n 
invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n 
Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n 
~~ def test_indexes_geopoint ( self ) : \n 
class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
location = GeoPointField ( ) \n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertEqual ( geo_indicies , [ { : [ ( , ) ] } ] ) \n 
~~ def test_geopoint_embedded_indexes ( self ) : \n 
class Venue ( EmbeddedDocument ) : \n 
~~~ location = GeoPointField ( ) \n 
name = StringField ( ) \n 
~~ class Event ( Document ) : \n 
venue = EmbeddedDocumentField ( Venue ) \n 
~~ def test_indexes_2dsphere ( self ) : \n 
point = PointField ( ) \n 
line = LineStringField ( ) \n 
polygon = PolygonField ( ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
~~ def test_indexes_2dsphere_embedded ( self ) : \n 
~~~ name = StringField ( ) \n 
~~ def test_geo_indexes_recursion ( self ) : \n 
~~ class Parent ( Document ) : \n 
location = ReferenceField ( Location ) \n 
~~ Location . drop_collection ( ) \n 
Parent . drop_collection ( ) \n 
Parent ( name = ) . save ( ) \n 
info = Parent . _get_collection ( ) . index_information ( ) \n 
self . assertFalse ( in info ) \n 
info = Location . _get_collection ( ) . index_information ( ) \n 
self . assertTrue ( in info ) \n 
self . assertEqual ( len ( Parent . _geo_indices ( ) ) , 0 ) \n 
self . assertEqual ( len ( Location . _geo_indices ( ) ) , 1 ) \n 
~~ def test_geo_indexes_auto_index ( self ) : \n 
~~~ class Log ( Document ) : \n 
~~~ location = PointField ( auto_index = False ) \n 
datetime = DateTimeField ( ) \n 
meta = { \n 
: [ [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] ] \n 
~~ self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n 
Log . drop_collection ( ) \n 
Log . ensure_indexes ( ) \n 
info = Log . _get_collection ( ) . index_information ( ) \n 
self . assertEqual ( info [ "location_2dsphere_datetime_1" ] [ "key" ] , \n 
[ ( , ) , ( , 1 ) ] ) \n 
class Log ( Document ) : \n 
: [ \n 
{ : [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] } \n 
~~ from south . db import db \n 
from django . db import models \n 
from django_lean . experiments . models import * \n 
class Migration : \n 
~~~ def forwards ( self , orm ) : \n 
~~~ db . create_table ( , ( \n 
( , orm [ ] ) , \n 
) ) \n 
db . send_create_signal ( , [ ] ) \n 
db . create_table ( , ( \n 
db . create_unique ( , [ , ] ) \n 
~~ def backwards ( self , orm ) : \n 
~~~ db . delete_table ( ) \n 
db . delete_table ( ) \n 
db . delete_unique ( , [ , ] ) \n 
~~ models = { \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" , : } ) \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" } ) , \n 
: ( , [ ] , { : } ) \n 
: ( , [ ] , { : "orm[\'auth.Group\']" , : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" , : } ) , \n 
: ( , [ ] , { : , : } ) \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'experiments.Experiment\']" } ) , \n 
: ( , [ ] , { } ) \n 
: ( , [ ] , { : "orm[\'auth.User\']" } ) \n 
complete_apps = [ ] \n 
~~ class SimpleEngagementCalculator ( object ) : \n 
from django . contrib . sites . models import Site \n 
from django . core import mail \n 
from django . db import transaction \n 
from django . utils . functional import LazyObject \n 
def get_current_site ( ) : \n 
~~~ if Site . _meta . installed : \n 
~~~ return Site . objects . get_current ( ) \n 
~~ def in_transaction ( test_ignore = True ) : \n 
~~~ result = transaction . is_managed ( ) \n 
if test_ignore : \n 
~~~ result = result and not hasattr ( mail , ) \n 
~~ @ contextmanager \n 
def patch ( namespace , name , function ) : \n 
if isinstance ( namespace , LazyObject ) : \n 
~~~ if namespace . _wrapped is None : \n 
~~~ namespace . _setup ( ) \n 
~~ namespace = namespace . _wrapped \n 
~~~ original = getattr ( namespace , name ) \n 
~~ except AttributeError : \n 
~~~ original = NotImplemented \n 
~~~ setattr ( namespace , name , function ) \n 
yield \n 
~~ finally : \n 
~~~ if original is NotImplemented : \n 
~~~ delattr ( namespace , name ) \n 
~~~ setattr ( namespace , name , original ) \n 
from construct import * \n 
from ipv4 import IpAddress \n 
echo_payload = Struct ( "echo_payload" , \n 
UBInt16 ( "identifier" ) , \n 
UBInt16 ( "sequence" ) , \n 
dest_unreachable_payload = Struct ( "dest_unreachable_payload" , \n 
Padding ( 2 ) , \n 
UBInt16 ( "next_hop_mtu" ) , \n 
IpAddress ( "host" ) , \n 
Bytes ( "echo" , 8 ) , \n 
dest_unreachable_code = Enum ( Byte ( "code" ) , \n 
Network_unreachable_error = 0 , \n 
Host_unreachable_error = 1 , \n 
Protocol_unreachable_error = 2 , \n 
Port_unreachable_error = 3 , \n 
The_datagram_is_too_big = 4 , \n 
Source_route_failed_error = 5 , \n 
Destination_network_unknown_error = 6 , \n 
Destination_host_unknown_error = 7 , \n 
Source_host_isolated_error = 8 , \n 
Desination_administratively_prohibited = 9 , \n 
Host_administratively_prohibited2 = 10 , \n 
Network_TOS_unreachable = 11 , \n 
Host_TOS_unreachable = 12 , \n 
icmp_header = Struct ( "icmp_header" , \n 
Enum ( Byte ( "type" ) , \n 
Echo_reply = 0 , \n 
Destination_unreachable = 3 , \n 
Source_quench = 4 , \n 
Redirect = 5 , \n 
Alternate_host_address = 6 , \n 
Echo_request = 8 , \n 
Router_advertisement = 9 , \n 
Router_solicitation = 10 , \n 
Time_exceeded = 11 , \n 
Parameter_problem = 12 , \n 
Timestamp_request = 13 , \n 
Timestamp_reply = 14 , \n 
Information_request = 15 , \n 
Information_reply = 16 , \n 
Address_mask_request = 17 , \n 
Address_mask_reply = 18 , \n 
_default_ = Pass , \n 
Switch ( "code" , lambda ctx : ctx . type , \n 
"Destination_unreachable" : dest_unreachable_code , \n 
default = Byte ( "code" ) , \n 
UBInt16 ( "crc" ) , \n 
Switch ( "payload" , lambda ctx : ctx . type , \n 
"Echo_reply" : echo_payload , \n 
"Echo_request" : echo_payload , \n 
"Destination_unreachable" : dest_unreachable_payload , \n 
default = Pass \n 
~~~ cap1 = ( "0800305c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
"63646566676869" ) . decode ( "hex" ) \n 
cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n 
print icmp_header . parse ( cap1 ) \n 
print icmp_header . parse ( cap2 ) \n 
print icmp_header . parse ( cap3 ) \n 
~~ from construct . core import Container \n 
from construct . adapters import Adapter \n 
class AstNode ( Container ) : \n 
~~~ def __init__ ( self , nodetype , ** kw ) : \n 
~~~ Container . __init__ ( self ) \n 
self . nodetype = nodetype \n 
for k , v in sorted ( kw . iteritems ( ) ) : \n 
~~~ setattr ( self , k , v ) \n 
~~ ~~ def accept ( self , visitor ) : \n 
~~~ return getattr ( visitor , "visit_%s" % ( self . nodetype , ) ) ( self ) \n 
~~ ~~ class AstTransformator ( Adapter ) : \n 
~~~ def _decode ( self , obj , context ) : \n 
~~~ return self . to_ast ( obj , context ) \n 
~~ def _encode ( self , obj , context ) : \n 
~~~ return self . to_cst ( obj , context ) \n 
~~ ~~ def pytest_funcarg__setupopts ( request ) : \n 
~~~ return OptsSetup ( request ) \n 
~~ def pytest_addoption ( parser ) : \n 
~~~ parser . addoption ( "--uri-file" , dest = "urifile" , \n 
type = str , default = None , \n 
parser . addoption ( "--use-ns" , dest = "use_ns" , \n 
action = "store_true" , \n 
parser . addoption ( "--create-graph" , dest = "create_graph" , \n 
parser . addoption ( "--num-executors" , dest = "num_exec" , \n 
type = int , default = 0 , \n 
parser . addoption ( "--time" , dest = "time" , \n 
type = str , default = "2:00:00:00" , \n 
parser . addoption ( "--proc" , dest = "proc" , \n 
type = int , default = 8 , \n 
parser . addoption ( "--mem" , dest = "mem" , \n 
type = float , default = 16 , \n 
parser . addoption ( "--ppn" , dest = "ppn" , \n 
parser . addoption ( "--queue" , dest = "queue" , \n 
parser . addoption ( "--restart" , dest = "restart" , \n 
parser . addoption ( "--backup-dir" , dest = "backup_directory" , \n 
type = str , default = ".pipeline-backup" , \n 
~~ class OptsSetup ( ) : \n 
~~~ def __init__ ( self , request ) : \n 
~~~ self . config = request . config \n 
~~ def returnAllOptions ( self ) : \n 
~~~ return self . config . option \n 
~~ def getNumExecutors ( self ) : \n 
~~~ return self . config . option . num_exec \n 
~~ def getTime ( self ) : \n 
~~~ return self . config . option . time \n 
~~ def getProc ( self ) : \n 
~~~ return self . config . option . proc \n 
~~ def getMem ( self ) : \n 
~~~ return self . config . option . mem \n 
~~ def getQueue ( self ) : \n 
~~~ return self . config . option . queue \n 
~~ def getPpn ( self ) : \n 
~~~ return self . config . option . ppn \n 
~~ def getRestart ( self ) : \n 
~~~ return self . config . option . restart \n 
~~ def getBackupDir ( self ) : \n 
~~~ return self . config . option . backup_directory \n 
~~ def returnSampleArgs ( self ) : \n 
~~~ sampleArgArray = [ "TestProgName.py" , "img_A.mnc" , "img_B.mnc" ] \n 
return sampleArgArray \n 
import random \n 
from twisted . python import log \n 
from twisted . web . error import Error as WebError \n 
from opennsa import constants as cnt , config \n 
from opennsa . backends . common import genericbackend \n 
from opennsa . protocols . shared import httpclient \n 
#</service> \n 
LOG_SYSTEM = \n 
class NCSVPNTarget ( object ) : \n 
~~~ def __init__ ( self , router , interface , vlan = None ) : \n 
~~~ self . router = router \n 
self . interface = interface \n 
self . vlan = vlan \n 
~~~ if self . vlan : \n 
~~~ return % ( self . router , self . interface , self . vlan ) \n 
~~~ return % ( self . router , self . interface ) \n 
~~ ~~ ~~ def createVPNPayload ( service_name , source_target , dest_target ) : \n 
~~~ intps = { \n 
: service_name , \n 
: source_target . router , \n 
: source_target . interface , \n 
: dest_target . router , \n 
: dest_target . interface \n 
if source_target . vlan and dest_target . vlan : \n 
~~~ if source_target . vlan == dest_target . vlan : \n 
~~~ intps [ ] = source_target . vlan \n 
payload = ETHERNET_VLAN_VPN_PAYLOAD_BASE % intps \n 
intps [ ] = dest_target . vlan \n 
payload = ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE % intps \n 
~~~ payload = ETHERNET_VPN_PAYLOAD_BASE % intps \n 
~~ return payload \n 
~~ def _extractErrorMessage ( failure ) : \n 
~~~ if isinstance ( failure . value , WebError ) : \n 
~~~ return failure . value . response \n 
~~~ return failure . getErrorMessage ( ) \n 
~~ ~~ class NCSVPNConnectionManager : \n 
~~~ def __init__ ( self , ncs_services_url , user , password , port_map , log_system ) : \n 
~~~ self . ncs_services_url = ncs_services_url \n 
self . user = user \n 
self . password = password \n 
self . port_map = port_map \n 
self . log_system = log_system \n 
~~ def getResource ( self , port , label_type , label_value ) : \n 
~~~ assert label_type in ( None , cnt . ETHERNET_VLAN ) , \n 
~~ def getTarget ( self , port , label_type , label_value ) : \n 
if label_type == cnt . ETHERNET_VLAN : \n 
~~~ vlan = int ( label_value ) \n 
assert 1 <= vlan <= 4095 , % label_value \n 
~~ ri = self . port_map [ port ] \n 
router , interface = ri . split ( ) \n 
return NCSVPNTarget ( router , interface , vlan ) \n 
~~ def createConnectionId ( self , source_target , dest_target ) : \n 
~~~ return + str ( random . randint ( 100000 , 999999 ) ) \n 
~~ def canSwapLabel ( self , label_type ) : \n 
~~~ return label_type == cnt . ETHERNET_VLAN \n 
~~ def _createAuthzHeader ( self ) : \n 
~~~ return + base64 . b64encode ( self . user + + self . password ) \n 
~~ def _createHeaders ( self ) : \n 
~~~ headers = { } \n 
headers [ ] = \n 
headers [ ] = self . _createAuthzHeader ( ) \n 
return headers \n 
~~ def setupLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + NO_OUT_OF_SYNC_CHECK \n 
payload = createVPNPayload ( connection_id , source_target , dest_target ) \n 
headers = self . _createHeaders ( ) \n 
def linkUp ( _ ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
~~ def error ( failure ) : \n 
log . msg ( % _extractErrorMessage ( failure ) , system = self . log_system ) \n 
return failure \n 
~~ d = httpclient . httpRequest ( service_url , payload , headers , method = , timeout = NCS_TIMEOUT ) \n 
d . addCallbacks ( linkUp , error ) \n 
return d \n 
~~ def teardownLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + connection_id + + NO_OUT_OF_SYNC_CHECK \n 
def linkDown ( _ ) : \n 
~~ d = httpclient . httpRequest ( service_url , None , headers , method = , timeout = NCS_TIMEOUT ) \n 
d . addCallbacks ( linkDown , error ) \n 
~~ ~~ def NCSVPNBackend ( network_name , nrm_ports , parent_requester , cfg ) : \n 
~~~ name = % network_name \n 
user = cfg [ config . NCS_USER ] \n 
password = cfg [ config . NCS_PASSWORD ] \n 
cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n 
return genericbackend . GenericBackend ( network_name , nrm_map , cm , parent_requester , name ) \n 
from twisted . python import log , failure \n 
from opennsa import nsa , error \n 
from opennsa . shared import xmlhelper \n 
from opennsa . protocols . shared import minisoap , soapresource \n 
from opennsa . protocols . nsi2 import helper , queryhelper \n 
from opennsa . protocols . nsi2 . bindings import actions , nsiconnection , p2pservices \n 
class ProviderService : \n 
~~~ def __init__ ( self , soap_resource , provider ) : \n 
~~~ self . provider = provider \n 
soap_resource . registerDecoder ( actions . RESERVE , self . reserve ) \n 
soap_resource . registerDecoder ( actions . RESERVE_COMMIT , self . reserveCommit ) \n 
soap_resource . registerDecoder ( actions . RESERVE_ABORT , self . reserveAbort ) \n 
soap_resource . registerDecoder ( actions . PROVISION , self . provision ) \n 
soap_resource . registerDecoder ( actions . RELEASE , self . release ) \n 
soap_resource . registerDecoder ( actions . TERMINATE , self . terminate ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY , self . querySummary ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY_SYNC , self . querySummarySync ) \n 
soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n 
~~ def _createSOAPFault ( self , err , provider_nsa , connection_id = None , service_type = None ) : \n 
~~~ log . msg ( % err . getErrorMessage ( ) , system = LOG_SYSTEM ) \n 
se = helper . createServiceException ( err , provider_nsa , connection_id ) \n 
ex_element = se . xml ( nsiconnection . serviceException ) \n 
soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n 
return soap_fault \n 
~~ def reserve ( self , soap_data , request_info ) : \n 
~~~ t_start = time . time ( ) \n 
header , reservation = helper . parseRequest ( soap_data ) \n 
criteria = reservation . criteria \n 
p2ps = criteria . serviceDefinition \n 
if type ( p2ps ) is not p2pservices . P2PServiceBaseType : \n 
~~~ err = failure . Failure ( error . PayloadError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa , service_type = service_type ) \n 
~~ if p2ps . directionality in ( None , ) : \n 
~~~ err = failure . Failure ( error . MissingParameterError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa ) \n 
~~ start_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . startTime ) if criteria . schedule . startTime is not None else None \n 
end_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . endTime ) if criteria . schedule . endTime is not None else None \n 
schedule = nsa . Schedule ( start_time , end_time ) \n 
src_stp = helper . createSTP ( p2ps . sourceSTP ) \n 
dst_stp = helper . createSTP ( p2ps . destSTP ) \n 
if p2ps . ero : \n 
~~ params = [ ( p . type_ , p . value ) for p in p2ps . parameter ] if p2ps . parameter else None \n 
symmetric = p2ps . symmetricPath or False \n 
sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , None , params ) \n 
crt = nsa . Criteria ( criteria . version , schedule , sd ) \n 
t_delta = time . time ( ) - t_start \n 
log . msg ( % round ( t_delta , 3 ) , profile = True , system = LOG_SYSTEM ) \n 
d = self . provider . reserve ( header , reservation . connectionId , reservation . globalReservationId , reservation . description , crt , request_info ) \n 
def createReserveAcknowledgement ( connection_id ) : \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , None , header . correlation_id ) \n 
reserve_response = nsiconnection . ReserveResponseType ( connection_id ) \n 
reserve_response_element = reserve_response . xml ( nsiconnection . reserveResponse ) \n 
payload = minisoap . createSoapPayload ( reserve_response_element , soap_header_element ) \n 
return payload \n 
~~ d . addCallbacks ( createReserveAcknowledgement , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n 
~~ def reserveCommit ( self , soap_data , request_info ) : \n 
~~~ header , confirm = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveCommit ( header , confirm . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , confirm . connectionId ) ) \n 
~~ def reserveAbort ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveAbort ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n 
~~ def provision ( self , soap_data , request_info ) : \n 
d = self . provider . provision ( header , request . connectionId , request_info ) \n 
~~ def release ( self , soap_data , request_info ) : \n 
d = self . provider . release ( header , request . connectionId , request_info ) \n 
~~ def terminate ( self , soap_data , request_info ) : \n 
d = self . provider . terminate ( header , request . connectionId , request_info ) \n 
~~ def querySummary ( self , soap_data , request_info ) : \n 
~~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummary ( header , query . connectionId , query . globalReservationId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n 
~~ def querySummarySync ( self , soap_data , request_info ) : \n 
~~~ def gotReservations ( reservations , header ) : \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , correlation_id = header . correlation_id ) \n 
qs_reservations = queryhelper . buildQuerySummaryResultType ( reservations ) \n 
qsct = nsiconnection . QuerySummaryConfirmedType ( qs_reservations ) \n 
payload = minisoap . createSoapPayload ( qsct . xml ( nsiconnection . querySummarySyncConfirmed ) , soap_header_element ) \n 
~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummarySync ( header , query . connectionId , query . globalReservationId , request_info ) \n 
d . addCallbacks ( gotReservations , self . _createSOAPFault , callbackArgs = ( header , ) , errbackArgs = ( header . provider_nsa , ) ) \n 
~~ def queryRecursive ( self , soap_data , request_info ) : \n 
d = self . provider . queryRecursive ( header , query . connectionId , query . globalReservationId , request_info ) \n 
~~ ~~ import os , datetime , json \n 
from twisted . trial import unittest \n 
from twisted . internet import defer , task \n 
from opennsa import config , nsa , database \n 
from opennsa . topology import nml \n 
from opennsa . backends import ncsvpn \n 
from . import common \n 
class NCSVPNBackendTest ( unittest . TestCase ) : \n 
~~~ self . clock = task . Clock ( ) \n 
tcf = os . path . expanduser ( ) \n 
tc = json . load ( open ( tcf ) ) \n 
ncs_config = { \n 
config . NCS_SERVICES_URL : tc [ ] , \n 
config . NCS_USER : tc [ ] , \n 
config . NCS_PASSWORD : tc [ ] \n 
self . requester = common . DUDRequester ( ) \n 
self . backend = ncsvpn . NCSVPNBackend ( , self . sr , self . requester , ncs_config ) \n 
self . backend . scheduler . clock = self . clock \n 
self . backend . startService ( ) \n 
database . setupDatabase ( tc [ ] , tc [ ] , tc [ ] ) \n 
self . requester_nsa = nsa . NetworkServiceAgent ( , ) \n 
self . provider_nsa = nsa . NetworkServiceAgent ( , ) \n 
source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , ) ] ) \n 
dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , ) ] ) \n 
start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n 
end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n 
bandwidth = 200 \n 
self . service_params = nsa . ServiceParameters ( start_time , end_time , source_stp , dest_stp , bandwidth ) \n 
~~ @ defer . inlineCallbacks \n 
def tearDown ( self ) : \n 
~~~ from opennsa . backends . common import simplebackend \n 
yield simplebackend . Simplebackendconnection . deleteAll ( ) \n 
yield self . backend . stopService ( ) \n 
def testActivation ( self ) : \n 
~~~ _ , _ , cid , sp = yield self . reserve ( self . requester_nsa , self . provider_nsa , None , None , None , None , self . service_params ) \n 
yield self . backend . reserveCommit ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
yield self . backend . provision ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
self . clock . advance ( 3 ) \n 
connection_id , active , version_consistent , version , timestamp = yield d_up \n 
self . failUnlessEqual ( cid , connection_id ) \n 
self . failUnlessEqual ( active , True ) \n 
self . failUnlessEqual ( version_consistent , True ) \n 
yield self . backend . terminate ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
connection_id , active , version_consistent , version , timestamp = yield d_down \n 
self . failUnlessEqual ( active , False ) \n 
~~ testActivation . skip = \n 
#-- \n 
revision = \n 
down_revision = \n 
from alembic import op \n 
import sqlalchemy as sa \n 
def upgrade ( ) : \n 
~~~ op . add_column ( , sa . Column ( , sa . Integer ) ) \n 
~~ def downgrade ( ) : \n 
~~~ op . drop_column ( , ) \n 
~~ from . import view \n 
from . comp import Card , NewCard \n 
from nagare . i18n import _ \n 
from nagare import presentation , ajax , security , component \n 
from . comp import Gallery , Asset , AssetCropper \n 
def render_image ( self , h , comp , size , randomize = False , ** kw ) : \n 
~~~ metadata = self . assets_manager . get_metadata ( self . filename ) \n 
src = self . assets_manager . get_image_url ( self . filename , size ) \n 
if randomize : \n 
~~~ src += + h . generate_id ( ) \n 
~~ return h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = src , ** kw ) \n 
~~ def render_file ( self , h , comp , size , ** kw ) : \n 
~~~ kw [ ] += \n 
metadata = self . assets_manager . get_metadata ( self . filename ) \n 
res = [ h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = "img/file-icon.jpg" , ** kw ) ] \n 
if size == : \n 
~~~ res . append ( h . span ( metadata [ ] ) ) \n 
~~ return res \n 
~~ CONTENT_TYPES = { : render_image , \n 
: render_image , \n 
: render_image } \n 
@ presentation . render_for ( Gallery ) \n 
def render ( self , h , comp , * args ) : \n 
~~~ with h . div ( id = + self . comp_id ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ h << comp . render ( h , model = ) \n 
~~ with h . div ( id = "card-gallery" ) : \n 
~~~ h << comp . render ( h , self . model ) \n 
~~ ~~ return h . root \n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_Gallery_view ( self , h , comp , model ) : \n 
~~~ model = if security . has_permissions ( , self ) else \n 
for asset in self . assets : \n 
~~~ h << asset . render ( h , model ) \n 
~~ return h . root \n 
def render_Gallery_crop ( self , h , comp , model ) : \n 
~~~ return self . cropper . on_answer ( self . action ) \n 
def render_cover ( self , h , comp , model ) : \n 
~~~ cover = self . get_cover ( ) \n 
if cover : \n 
~~~ h << h . p ( component . Component ( self . get_cover ( ) , model = ) , class_ = ) \n 
~~ @ presentation . render_for ( Gallery , "action" ) \n 
def render_download ( self , h , comp , * args ) : \n 
~~~ if security . has_permissions ( , self ) : \n 
~~~ submit_id = h . generate_id ( "attach_submit" ) \n 
input_id = h . generate_id ( "attach_input" ) \n 
h << h . label ( ( h . i ( class_ = ) , \n 
with h . form : \n 
~~~ h << h . script ( \n 
% \n 
: ajax . py2js ( self . assets_manager . max_size ) , \n 
: ajax . py2js ( input_id ) , \n 
: ajax . py2js ( submit_id ) , \n 
: ajax . py2js ( \n 
_ ( ) \n 
) . decode ( ) \n 
submit_action = ajax . Update ( \n 
render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( ) ) , \n 
component_to_update = + self . comp_id , \n 
h << h . input ( id = input_id , class_ = , type = "file" , name = "file" , multiple = "multiple" , maxlength = "100" , ) . action ( self . add_assets ) \n 
h << h . input ( class_ = , id = submit_id , type = "submit" ) . action ( submit_action ) \n 
~~ @ presentation . render_for ( Gallery , model = ) \n 
def render_gallery_badge ( self , h , * args ) : \n 
if self . assets : \n 
~~~ with h . span ( class_ = ) : \n 
~~~ h << h . span ( h . i ( class_ = ) , , len ( self . assets ) , class_ = ) \n 
~~ @ presentation . render_for ( Asset ) \n 
@ presentation . render_for ( Asset , model = ) \n 
def render_asset ( self , h , comp , model , * args ) : \n 
~~~ res = [ ] \n 
kw = { : True } if model == else { } \n 
kw [ ] = model \n 
if self . is_cover : \n 
~~~ res . append ( h . span ( class_ = ) ) \n 
~~ meth = CONTENT_TYPES . get ( metadata [ ] , render_file ) \n 
res . append ( meth ( self , h , comp , model , ** kw ) ) \n 
return res \n 
~~ @ presentation . render_for ( Asset , model = ) \n 
def render_Asset_thumb ( self , h , comp , model , * args ) : \n 
~~~ action = h . a . action ( lambda : comp . answer ( ( , self ) ) ) . get ( ) \n 
onclick = _ ( ) \n 
with h . a ( class_ = , title = _ ( ) , href = , onclick = onclick ) : \n 
~~~ h << h . i ( class_ = ) \n 
~~ if self . is_image ( ) : \n 
~~~ with h . a ( class_ = , title = _ ( ) ) . action ( lambda : comp . answer ( ( , self ) ) ) : \n 
~~~ if self . is_cover : \n 
~~~ h << { : } \n 
~~ h << h . i ( class_ = ) \n 
~~ ~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , ) \n 
~~ @ presentation . render_for ( Asset , model = "anonymous" ) \n 
def render_asset_anonymous ( self , h , comp , model , * args ) : \n 
~~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , model = "thumb" ) \n 
~~ @ presentation . render_for ( AssetCropper ) \n 
def render_gallery_cropper ( self , h , comp , * args ) : \n 
~~~ h << h . p ( _ ( ) ) \n 
form_id = h . generate_id ( ) \n 
img_id = h . generate_id ( ) \n 
~~~ for crop_name in , , , : \n 
~~~ h << h . input ( type = , id = form_id + + crop_name ) . action ( getattr ( self , crop_name ) ) \n 
~~ h << h . p ( render_image ( self . asset , h , comp , , id = img_id ) ) \n 
h << h . script ( \n 
"YAHOO.util.Event.onContentReady(%s," \n 
ajax . py2js ( img_id ) , \n 
ajax . py2js ( form_id ) , \n 
ajax . py2js ( self . crop_width ( ) ) , \n 
ajax . py2js ( self . crop_height ( ) ) \n 
with h . div ( class_ = ) : \n 
~~~ h << h . button ( _ ( ) , class_ = ) . action ( self . commit , comp ) \n 
if self . asset . is_cover : \n 
~~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . remove_cover , comp ) \n 
~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . cancel , comp ) \n 
~~ class EventHandlerMixIn ( object ) : \n 
def emit_event ( self , comp , kind , data = None ) : \n 
~~~ event = kind ( data , source = [ self ] ) \n 
return comp . answer ( event ) \n 
~~ def handle_event ( self , comp , event ) : \n 
~~~ local_res = None \n 
local_handler = getattr ( self , , None ) \n 
if local_handler : \n 
~~~ local_res = local_handler ( comp , event ) \n 
~~ event . append ( self ) \n 
upper_res = comp . answer ( event ) \n 
return local_res or upper_res \n 
~~ ~~ class Event ( object ) : \n 
def __init__ ( self , data , source = [ ] ) : \n 
self . _source = source \n 
self . data = data \n 
def source ( self ) : \n 
~~~ return self . _source . copy ( ) \n 
def emitter ( self ) : \n 
~~~ return self . _source [ 0 ] \n 
def last_relay ( self ) : \n 
~~~ return self . _source [ - 1 ] \n 
~~ def is_ ( self , kind ) : \n 
~~~ return type ( self ) is kind \n 
~~ def is_kind_of ( self , kind ) : \n 
~~~ return isinstance ( self , kind ) \n 
~~ def append ( self , relay ) : \n 
~~~ self . _source . append ( relay ) \n 
~~ def cast_as ( self , sub_kind ) : \n 
~~~ return sub_kind ( self . data , self . _source ) \n 
~~ ~~ class ColumnDeleted ( Event ) : \n 
~~ class CardClicked ( Event ) : \n 
~~ class PopinClosed ( Event ) : \n 
~~ class CardEditorClosed ( PopinClosed ) : \n 
~~ class CardArchived ( Event ) : \n 
~~ class SearchIndexUpdated ( Event ) : \n 
~~ class CardDisplayed ( Event ) : \n 
~~ class BoardAccessChanged ( Event ) : \n 
~~ class BoardDeleted ( BoardAccessChanged ) : \n 
~~ class BoardArchived ( BoardAccessChanged ) : \n 
~~ class BoardRestored ( BoardAccessChanged ) : \n 
~~ class BoardLeft ( BoardAccessChanged ) : \n 
~~ class ParentTitleNeeded ( Event ) : \n 
~~ class NewTemplateRequested ( Event ) : \n 
~~ from . comp import EditableTitle \n 
from . import view \n 
#!/usr/bin/python2.7 \n 
####################################################################################################################### \n 
import genie2 . client . wrapper \n 
import genie2 . model . ClusterCriteria \n 
import genie2 . model . Job \n 
import genie2 . model . FileAttachment \n 
genie = genie2 . client . wrapper . Genie2 ( "http://localhost:8080/genie" , \n 
genie2 . client . wrapper . RetryPolicy ( \n 
tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) \n 
job = genie2 . model . Job . Job ( ) \n 
job . name = "GenieDockerExamplePigJob2" \n 
job . user = "root" \n 
job . version = "0.14.0" \n 
job . clusterCriterias = list ( ) \n 
cluster_criteria = genie2 . model . ClusterCriteria . ClusterCriteria ( ) \n 
criteria = set ( ) \n 
criteria . add ( "sched:adhoc" ) \n 
criteria . add ( "type:yarn" ) \n 
cluster_criteria . tags = criteria \n 
job . clusterCriterias . append ( cluster_criteria ) \n 
command_criteria = set ( ) \n 
command_criteria . add ( "type:pig" ) \n 
job . commandCriteria = command_criteria \n 
job . fileDependencies = "file:///apps/genie/pig/0.14.0/tutorial/script2-hadoop.pig,file:///apps/genie/pig/0.14.0/tutorial/tutorial.jar" \n 
job . commandArgs = "script2-hadoop.pig" \n 
job = genie . submitJob ( job ) \n 
while job . status != "SUCCEEDED" and job . status != "KILLED" and job . status != "FAILED" : \n 
time . sleep ( 10 ) \n 
job = genie . getJob ( job . id ) \n 
ON = 1 \n 
DISCONNECTED = 20 \n 
CONNECTED = 30 \n 
DEFAULT_EVENT_VERSION = 1 \n 
LOG_LEVEL = "DEBUG" \n 
LOG_FILE = "/var/log/security_monkey/security_monkey-deploy.log" \n 
SQLALCHEMY_DATABASE_URI = \n 
SQLALCHEMY_POOL_SIZE = 50 \n 
SQLALCHEMY_MAX_OVERFLOW = 15 \n 
ENVIRONMENT = \n 
USE_ROUTE53 = False \n 
FQDN = \n 
API_PORT = \n 
WEB_PORT = \n 
WEB_PATH = \n 
FRONTED_BY_NGINX = True \n 
NGINX_PORT = \n 
BASE_URL = . format ( FQDN ) \n 
SECRET_KEY = \n 
MAIL_DEFAULT_SENDER = \n 
SECURITY_REGISTERABLE = True \n 
SECURITY_CONFIRMABLE = False \n 
SECURITY_RECOVERABLE = False \n 
SECURITY_PASSWORD_HASH = \n 
SECURITY_PASSWORD_SALT = \n 
SECURITY_TRACKABLE = True \n 
SECURITY_POST_LOGIN_VIEW = BASE_URL \n 
SECURITY_POST_REGISTER_VIEW = BASE_URL \n 
SECURITY_POST_CONFIRM_VIEW = BASE_URL \n 
SECURITY_POST_RESET_VIEW = BASE_URL \n 
SECURITY_POST_CHANGE_VIEW = BASE_URL \n 
SECURITY_TEAM_EMAIL = [ ] \n 
SES_REGION = \n 
MAIL_SERVER = \n 
MAIL_PORT = 465 \n 
MAIL_USE_SSL = True \n 
MAIL_USERNAME = \n 
MAIL_PASSWORD = \n 
WTF_CSRF_ENABLED = True \n 
WTF_CSRF_METHODS = [ , , , ] \n 
SECURITYGROUP_INSTANCE_DETAIL = \n 
CORE_THREADS = 25 \n 
MAX_THREADS = 30 \n 
from security_monkey . auditor import Auditor \n 
from security_monkey . watchers . rds_security_group import RDSSecurityGroup \n 
from security_monkey . datastore import NetworkWhitelistEntry \n 
from security_monkey . auditors . security_group import _check_rfc_1918 \n 
import ipaddr \n 
class RDSSecurityGroupAuditor ( Auditor ) : \n 
~~~ index = RDSSecurityGroup . index \n 
i_am_singular = RDSSecurityGroup . i_am_singular \n 
i_am_plural = RDSSecurityGroup . i_am_plural \n 
network_whitelist = [ ] \n 
def __init__ ( self , accounts = None , debug = False ) : \n 
~~~ super ( RDSSecurityGroupAuditor , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
~~ def prep_for_audit ( self ) : \n 
~~~ self . network_whitelist = NetworkWhitelistEntry . query . all ( ) \n 
~~ def _check_inclusion_in_network_whitelist ( self , cidr ) : \n 
~~~ for entry in self . network_whitelist : \n 
~~~ if ipaddr . IPNetwork ( cidr ) in ipaddr . IPNetwork ( str ( entry . cidr ) ) : \n 
~~ def check_rds_ec2_rfc1918 ( self , sg_item ) : \n 
severity = 8 \n 
if sg_item . config . get ( "vpc_id" , None ) : \n 
~~ for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and _check_rfc_1918 ( cidr ) : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
~~ ~~ ~~ def check_securitygroup_large_subnet ( self , sg_item ) : \n 
severity = 3 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
if cidr and not self . _check_inclusion_in_network_whitelist ( cidr ) : \n 
~~~ if in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
~~~ mask = int ( cidr . split ( ) [ 1 ] ) \n 
if mask < 24 and mask > 0 : \n 
~~ ~~ ~~ ~~ ~~ def check_securitygroup_zero_subnet ( self , sg_item ) : \n 
severity = 10 \n 
if cidr and in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
if mask == 0 : \n 
~~ ~~ ~~ ~~ def check_securitygroup_any ( self , sg_item ) : \n 
severity = 5 \n 
~~~ cidr = ipr . get ( "cidr_ip" ) \n 
if "0.0.0.0/0" == cidr : \n 
~~ ~~ ~~ def check_securitygroup_10net ( self , sg_item ) : \n 
if "10.0.0.0/8" == cidr : \n 
from security_monkey . datastore import NetworkWhitelistEntry , Account \n 
from security_monkey . tests import SecurityMonkeyTestCase \n 
from security_monkey import db \n 
from security_monkey . watchers . elasticsearch_service import ElasticSearchServiceItem \n 
CONFIG_ONE = { \n 
"name" : "es_test" , \n 
CONFIG_TWO = { \n 
"name" : "es_test_2" , \n 
CONFIG_THREE = { \n 
"name" : "es_test_3" , \n 
CONFIG_FOUR = { \n 
"name" : "es_test_4" , \n 
CONFIG_FIVE = { \n 
"name" : "es_test_5" , \n 
CONFIG_SIX = { \n 
"name" : "es_test_6" , \n 
CONFIG_SEVEN = { \n 
"name" : "es_test_7" , \n 
CONFIG_EIGHT = { \n 
"name" : "es_test_8" , \n 
CONFIG_NINE = { \n 
"name" : "es_test_9" , \n 
WHITELIST_CIDRS = [ \n 
class ElasticSearchServiceTestCase ( SecurityMonkeyTestCase ) : \n 
~~~ self . es_items = [ \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config = CONFIG_ONE ) , \n 
ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config = CONFIG_TWO ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config = CONFIG_THREE ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config = CONFIG_FOUR ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config = CONFIG_FIVE ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config = CONFIG_SIX ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config = CONFIG_SEVEN ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config = CONFIG_EIGHT ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config = CONFIG_NINE ) , \n 
test_account = Account ( ) \n 
test_account . name = "TEST_ACCOUNT" \n 
test_account . s3_name = "TEST_ACCOUNT" \n 
test_account . number = "012345678910" \n 
test_account . role_name = "TEST_ACCOUNT" \n 
db . session . add ( test_account ) \n 
db . session . commit ( ) \n 
~~ def tearDown ( self ) : \n 
~~~ test_account = Account . query . filter ( Account . number == "012345678910" ) . first ( ) \n 
if test_account is not None : \n 
~~~ db . session . delete ( test_account ) \n 
~~ ~~ def test_es_auditor ( self ) : \n 
~~~ from security_monkey . auditors . elasticsearch_service import ElasticSearchServiceAuditor \n 
es_auditor = ElasticSearchServiceAuditor ( accounts = [ "012345678910" ] ) \n 
es_auditor . network_whitelist = [ ] \n 
for cidr in WHITELIST_CIDRS : \n 
~~~ whitelist_cidr = NetworkWhitelistEntry ( ) \n 
whitelist_cidr . cidr = cidr [ 1 ] \n 
whitelist_cidr . name = cidr [ 0 ] \n 
es_auditor . network_whitelist . append ( whitelist_cidr ) \n 
~~ for es_domain in self . es_items : \n 
~~~ es_auditor . check_es_access_policy ( es_domain ) \n 
~~ self . assertEquals ( len ( self . es_items [ 0 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 0 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 1 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 1 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 2 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 1 ] . score , 7 ) \n 
self . assertEquals ( len ( self . es_items [ 3 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 3 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 4 ] . audit_issues ) , 0 ) \n 
self . assertEquals ( len ( self . es_items [ 5 ] . audit_issues ) , 0 ) \n 
self . assertEquals ( len ( self . es_items [ 6 ] . audit_issues ) , 3 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 1 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 2 ] . score , 7 ) \n 
self . assertEquals ( len ( self . es_items [ 7 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 7 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 8 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 0 ] . score , 6 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 1 ] . score , 10 ) \n 
from security_monkey . watcher import Watcher \n 
from security_monkey . watcher import ChangeItem \n 
from security_monkey . constants import TROUBLE_REGIONS \n 
from security_monkey . exceptions import BotoConnectionIssue \n 
from security_monkey import app \n 
from boto . redshift import regions \n 
class Redshift ( Watcher ) : \n 
~~~ index = \n 
i_am_singular = \n 
i_am_plural = \n 
~~~ super ( Redshift , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
~~ def slurp ( self ) : \n 
self . prep_for_slurp ( ) \n 
from security_monkey . common . sts_connect import connect \n 
item_list = [ ] \n 
exception_map = { } \n 
for account in self . accounts : \n 
~~~ for region in regions ( ) : \n 
~~~ redshift = connect ( account , , region = region ) \n 
all_clusters = [ ] \n 
marker = None \n 
while True : \n 
~~~ response = self . wrap_aws_rate_limited_call ( \n 
redshift . describe_clusters , \n 
marker = marker \n 
all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n 
~~~ if region . name not in TROUBLE_REGIONS : \n 
~~~ exc = BotoConnectionIssue ( str ( e ) , , account , region . name ) \n 
self . slurp_exception ( ( self . index , account , region . name ) , exc , exception_map ) ~~ continue \n 
for cluster in all_clusters : \n 
~~~ cluster_id = cluster [ ] \n 
if self . check_ignore_list ( cluster_id ) : \n 
~~ item = RedshiftCluster ( region = region . name , account = account , name = cluster_id , config item_list . append ( item ) \n 
~~ ~~ ~~ return item_list , exception_map \n 
~~ ~~ class RedshiftCluster ( ChangeItem ) : \n 
~~~ def __init__ ( self , region = None , account = None , name = None , config = { } ) : \n 
~~~ super ( RedshiftCluster , self ) . __init__ ( \n 
index = Redshift . index , \n 
region = region , \n 
account = account , \n 
new_config = config ) \n 
from ndscheduler import settings \n 
from ndscheduler . core . datastore . providers import base \n 
class DatastorePostgresql ( base . DatastoreBase ) : \n 
~~~ @ classmethod \n 
def get_db_url ( cls ) : \n 
return % ( \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] ) \n 
from ndscheduler import job \n 
logger = logging . getLogger ( __name__ ) \n 
class CurlJob ( job . JobBase ) : \n 
~~~ TIMEOUT = 10 \n 
def meta_info ( cls ) : \n 
~~~ return { \n 
: % ( cls . __module__ , cls . __name__ ) , \n 
{ : , : } , \n 
{ : , : \n 
] , \n 
~~ def run ( self , url , request_type , * args , ** kwargs ) : \n 
~~~ print ( % ( url ) ) \n 
session = requests . Session ( ) \n 
result = session . request ( request_type , \n 
url , \n 
timeout = self . TIMEOUT , \n 
headers = None , \n 
data = None ) \n 
print ( result . text ) \n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ job = CurlJob . create_test_instance ( ) \n 
job . run ( ) \n 
~~ import unittest \n 
from . mock import MagicMock , Mock \n 
from . util import TrelloElementMock , CommandMock , OperationMock \n 
from operations import * \n 
class BaseOperationTests ( unittest . TestCase ) : \n 
~~~ self . base_operation , self . trello_element = OperationMock . create ( BaseOperation ) \n 
self . class_mock , self . instance_mock = OperationMock . instance ( self . base_operation ) \n 
self . collection = TrelloElementMock . collection ( ) \n 
self . base_operation . collection = TrelloCollection ( self . collection ) \n 
~~ def test_items_sets_the_collection ( self ) : \n 
~~~ self . base_operation . set_collection = MagicMock ( ) \n 
self . base_operation . items ( ) \n 
self . base_operation . set_collection . assert_called_with ( ) \n 
~~ def test_items_returns_every_name_from_the_collection_with_the_added_options ( self ) : \n 
~~ def test_callback_uses_find_to_instantiate_the_operation_if_the_index_is_in_the_collection ( self ) ~~~ self . base_operation . callback ( 3 ) \n 
self . class_mock . assert_called_with ( self . collection [ 0 ] , self . base_operation ) \n 
~~ def test_callback_calls_execute_on_the_operation ( self ) : \n 
~~~ self . base_operation . callback ( 3 ) \n 
self . instance_mock . execute . assert_called_with ( self . base_operation . command ) \n 
~~ def test_callback_doesnt_call_find_if_the_index_is_bigger_than_the_collection_length ( self ) : \n 
~~~ big_index = 55 \n 
self . base_operation . callback ( big_index ) \n 
assert not self . class_mock . called \n 
~~ def test_callback_calls_execute_on_the_previous_operation_if_index_is_0 ( self ) : \n 
~~~ self . base_operation . callback ( 0 ) \n 
self . base_operation . previous_operation . execute . assert_called_with ( ) \n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . base_operation . command . input = MagicMock ( ) \n 
self . base_operation . callback ( 2 ) \n 
self . base_operation . command . input . assert_called_with ( "Name" , self . base_operation . deferred_add \n 
~~ def test_base_add_calls_add_with_the_text_and_cleans_the_cache_for_the_element ( self ) : \n 
~~~ text = "Text" \n 
self . base_operation . add = MagicMock ( ) \n 
self . base_operation . trello_element . reload = MagicMock ( ) \n 
self . base_operation . base_add ( text ) \n 
self . base_operation . add . assert_called_with ( text ) \n 
self . trello_element . reload . assert_called_with ( ) \n 
~~ def test_base_add_calls_add_and_execute_if_renavigate_is_true ( self ) : \n 
self . base_operation . command . renavigate = True \n 
self . base_operation . execute = MagicMock ( ) \n 
self . base_operation . execute . assert_called_with ( ) \n 
~~ ~~ class BoardOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( BoardOperation ) \n 
self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n 
~~ def test_items_returns_every_name_from_the_collection_without_goback ( self ) : \n 
~~~ self . operation . set_collection = MagicMock ( ) \n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "boards" ) \n 
~~ def test_callback_calls_execute_command_with_the_index ( self ) : \n 
~~~ self . operation . execute_command = MagicMock ( ) \n 
self . operation . callback ( 5 ) \n 
self . operation . execute_command . assert_called_with ( 3 ) \n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . operation . command . input = MagicMock ( ) \n 
self . operation . callback ( 1 ) \n 
self . operation . command . input . assert_called_with ( "Name" , self . operation . deferred_add ) \n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , ListOperation ) \n 
~~ def test_add_creates_a_board_with_the_text ( self ) : \n 
self . trello_element . add_board = MagicMock ( ) \n 
self . operation . add ( text ) \n 
self . trello_element . add_board . assert_called_with ( text ) \n 
~~ ~~ class ListOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( ListOperation ) \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "lists" ) \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOperation ) \n 
~~ def test_add_creates_a_list_with_the_text ( self ) : \n 
self . trello_element . add_list = MagicMock ( ) \n 
self . trello_element . add_list . assert_called_with ( text ) \n 
~~ ~~ class CardOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( CardOperation ) \n 
~~ def test_items_returns_every_name_from_the_collection_with_custom_actions ( self ) : \n 
self . assertEqual ( self . operation . items ( ) , [ , , \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "cards" ) \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOptions ) \n 
~~ def test_add_creates_a_card_with_the_text_and_description ( self ) : \n 
self . trello_element . add_card = MagicMock ( ) \n 
self . operation . add ( name , desc ) \n 
self . trello_element . add_card . assert_called_with ( name , desc ) \n 
~~ def test_split_card_contents_returns_the_name_and_description_splitted_by_new_lines ( self ) : \n 
~~~ content = "Name!!\\n\\nDescription\\nYeah!" \n 
name , desc = self . operation . split_card_contents ( content ) \n 
self . assertEqual ( name , "Name!!" ) \n 
self . assertEqual ( desc , "Description\\nYeah!" ) \n 
~~ \n 
DEBUG = env . bool ( , default = True ) \n 
TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n 
SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n 
EMAIL_HOST = \n 
EMAIL_PORT = 1025 \n 
EMAIL_BACKEND = env ( , \n 
default = ) \n 
CACHES = { \n 
: \n 
MIDDLEWARE_CLASSES += ( , ) \n 
INSTALLED_APPS += ( , ) \n 
INTERNAL_IPS = ( , , ) \n 
DEBUG_TOOLBAR_CONFIG = { \n 
: True , \n 
TEST_RUNNER = \n 
from django . contrib import messages \n 
from django . contrib . auth import logout , login , authenticate \n 
from django . http import HttpResponseBadRequest , Http404 \n 
from django . shortcuts import render , redirect , get_object_or_404 \n 
from reddit . forms import UserForm , ProfileForm \n 
from reddit . utils . helpers import post_only \n 
from users . models import RedditUser \n 
def user_profile ( request , username ) : \n 
~~~ user = get_object_or_404 ( User , username = username ) \n 
profile = RedditUser . objects . get ( user = user ) \n 
return render ( request , , { : profile } ) \n 
def edit_profile ( request ) : \n 
~~~ user = RedditUser . objects . get ( user = request . user ) \n 
if request . method == : \n 
~~~ profile_form = ProfileForm ( instance = user ) \n 
~~ elif request . method == : \n 
~~~ profile_form = ProfileForm ( request . POST , instance = user ) \n 
if profile_form . is_valid ( ) : \n 
~~~ profile = profile_form . save ( commit = False ) \n 
profile . update_profile_data ( ) \n 
profile . save ( ) \n 
~~ return render ( request , , { : profile_form } ) \n 
~~ def user_login ( request ) : \n 
if request . user . is_authenticated ( ) : \n 
return render ( request , ) \n 
~~ if request . method == "POST" : \n 
~~~ username = request . POST . get ( ) \n 
password = request . POST . get ( ) \n 
if not username or not password : \n 
~~~ return HttpResponseBadRequest ( ) \n 
~~ user = authenticate ( username = username , \n 
password = password ) \n 
~~~ if user . is_active : \n 
~~~ login ( request , user ) \n 
redirect_url = request . POST . get ( ) or \n 
return redirect ( redirect_url ) \n 
~~~ return render ( request , , \n 
~~ ~~ return render ( request , ) \n 
~~ @ post_only \n 
def user_logout ( request ) : \n 
~~~ redirect_page = request . POST . get ( , ) \n 
logout ( request ) \n 
messages . success ( request , ) \n 
return redirect ( redirect_page ) \n 
~~ return redirect ( ) \n 
~~ def register ( request ) : \n 
user_form = UserForm ( ) \n 
~~~ messages . warning ( request , \n 
return render ( request , , { : user_form } ) \n 
~~~ user_form = UserForm ( request . POST ) \n 
if user_form . is_valid ( ) : \n 
~~~ user = user_form . save ( ) \n 
user . set_password ( user . password ) \n 
user . save ( ) \n 
reddit_user = RedditUser ( ) \n 
reddit_user . user = user \n 
reddit_user . save ( ) \n 
user = authenticate ( username = request . POST [ ] , \n 
password = request . POST [ ] ) \n 
login ( request , user ) \n 
return redirect ( ) \n 
~~ ~~ return render ( request , , { : user_form } ) \n 
~~ import unittest2 \n 
from pymysql . tests import base \n 
from pymysql import util \n 
class TestNextset ( base . PyMySQLTestCase ) : \n 
~~~ super ( TestNextset , self ) . setUp ( ) \n 
self . con = self . connections [ 0 ] \n 
~~ def test_nextset ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
r = cur . nextset ( ) \n 
self . assertTrue ( r ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur ) ) \n 
self . assertIsNone ( cur . nextset ( ) ) \n 
~~ def test_skip_nextset ( self ) : \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur ) ) \n 
~~ def test_ok_and_next ( self ) : \n 
self . assertTrue ( cur . nextset ( ) ) \n 
self . assertFalse ( bool ( cur . nextset ( ) ) ) \n 
~~ @ unittest2 . expectedFailure \n 
def test_multi_cursor ( self ) : \n 
~~~ cur1 = self . con . cursor ( ) \n 
cur2 = self . con . cursor ( ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur1 ) ) \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur2 ) ) \n 
r = cur1 . nextset ( ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur1 ) ) \n 
self . assertIsNone ( cur1 . nextset ( ) ) \n 
~~ def test_multi_statement_warnings ( self ) : \n 
~~~ cursor = self . con . cursor ( ) \n 
~~~ cursor . execute ( \n 
~~~ self . fail ( ) \n 
~~ ~~ ~~ from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import message as _message \n 
from google . protobuf import reflection as _reflection \n 
from google . protobuf import descriptor_pb2 \n 
DESCRIPTOR = _descriptor . FileDescriptor ( \n 
package = , \n 
_PUSHNOTIFICATION = _descriptor . Descriptor ( \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 3 , cpp_type = 2 , label = 1 , \n 
has_default_value = False , default_value = 0 , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
name = , full_name = , index = 1 , \n 
number = 2 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
name = , full_name = , index = 2 , \n 
number = 3 , type = 9 , cpp_type = 9 , label = 1 , \n 
name = , full_name = , index = 3 , \n 
number = 4 , type = 9 , cpp_type = 9 , label = 1 , \n 
extensions = [ \n 
nested_types = [ ] , \n 
enum_types = [ \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
serialized_start = 33 , \n 
serialized_end = 117 , \n 
_BATCHNOTIFICATIONREQUEST = _descriptor . Descriptor ( \n 
number = 1 , type = 11 , cpp_type = 10 , label = 3 , \n 
has_default_value = False , default_value = [ ] , \n 
serialized_start = 119 , \n 
serialized_end = 187 , \n 
_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _BATCHNOTIFICATIONREQUEST \n 
class PushNotification ( _message . Message ) : \n 
~~~ __metaclass__ = _reflection . GeneratedProtocolMessageType \n 
DESCRIPTOR = _PUSHNOTIFICATION \n 
~~ class BatchNotificationRequest ( _message . Message ) : \n 
DESCRIPTOR = _BATCHNOTIFICATIONREQUEST \n 
~~ DESCRIPTOR . has_options = True \n 
import pytest \n 
from pushkin import pushkin_cli \n 
import tornado . web \n 
from pushkin import context \n 
from pushkin . database import database \n 
from pushkin . request . request_processor import RequestProcessor \n 
from pushkin . requesthandlers . events import JsonEventHandler \n 
from pushkin . requesthandlers . notifications import JsonNotificationHandler \n 
from pushkin import test_config_ini_path \n 
from pushkin import config \n 
@ pytest . fixture \n 
def setup_database ( ) : \n 
~~~ database . create_database ( ) \n 
~~ @ pytest . fixture \n 
def mock_processor ( mocker ) : \n 
mocker . patch ( ) \n 
def app ( ) : \n 
~~~ pushkin_cli . CONFIGURATION_FILENAME = test_config_ini_path \n 
pushkin_cli . init ( ) \n 
return pushkin_cli . create_app ( ) \n 
def notification_batch_json ( ) : \n 
def post_notification_url ( base_url ) : \n 
~~~ return base_url + config . json_notification_handler_url \n 
def event_batch_json ( ) : \n 
def post_event_url ( base_url ) : \n 
~~~ return base_url + config . json_event_handler_url \n 
~~ @ pytest . mark . gen_test \n 
@ pytest . mark . parametrize ( "input" , [ \n 
( ) , \n 
def test_post_notification_empty_request ( setup_database , mock_processor , http_client , post_notification_url ~~~ request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = input ) \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
~~ assert not context . request_processor . submit . called \n 
def test_post_notification ( setup_database , mock_processor , http_client , post_notification_url , \n 
notification_batch_json ) : \n 
request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = notification_batch_json response = yield http_client . fetch ( request ) \n 
assert response . code == 200 \n 
assert context . request_processor . submit . called \n 
def test_post_event_empty_request ( setup_database , mock_processor , http_client , post_event_url , input ~~~ \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = input ) \n 
def test_post_event ( setup_database , mock_processor , http_client , post_event_url , event_batch_json ) : \n 
context . request_processor . submit . return_value = True \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = event_batch_json ) \n 
response = yield http_client . fetch ( request ) \n 
def test_post_event_service_unavailable ( setup_database , mock_processor , http_client , post_event_url , app ) : \n 
context . request_processor . submit . return_value = False \n 
RequestProcessor . submit . return_value = False \n 
from pywechat . excepts import WechatError \n 
class Basic ( object ) : \n 
def __init__ ( self , app_id , app_secret ) : \n 
self . __app_id = app_id \n 
self . __app_secret = app_secret \n 
self . __access_token = self . access_token \n 
self . __token_expires_at = None \n 
def access_token ( self ) : \n 
if self . __access_token and self . __token_expires_at : \n 
~~~ if self . __token_expires_at - time . time ( ) > 60 : \n 
~~~ return self . __access_token \n 
~~ ~~ self . _grant_access_token ( ) \n 
return self . __access_token \n 
~~ def _send_request ( self , method , url , ** kwargs ) : \n 
if not kwargs . get ( ) : \n 
~~~ kwargs [ ] = { \n 
"access_token" : self . access_token \n 
~~ if kwargs . get ( ) : \n 
~~~ data = json . dumps ( kwargs [ ] ) . encode ( ) \n 
kwargs [ "data" ] = data \n 
~~ request = requests . request ( \n 
method = method , \n 
url = url , \n 
** kwargs \n 
request . raise_for_status ( ) \n 
json_data = request . json ( ) \n 
self . _check_wechat_error ( json_data ) \n 
return json_data \n 
def _check_wechat_error ( cls , json_data ) : \n 
errcode = json_data . get ( ) \n 
if errcode and errcode != 0 : \n 
~~~ raise WechatError ( errcode , json_data . get ( ) ) \n 
~~ ~~ def _grant_access_token ( self ) : \n 
url = \n 
params = { \n 
"grant_type" : "client_credential" , \n 
"appid" : self . __app_id , \n 
"secret" : self . __app_secret \n 
json_data = self . _send_request ( , url , params = params ) \n 
self . __access_token = json_data . get ( ) \n 
self . __token_expires_at = int ( \n 
time . time ( ) ) + json_data . get ( ) \n 
~~ def _get_wechat_server_ips ( self ) : \n 
url = "https://api.weixin.qq.com/cgi-bin/getcallbackip" \n 
from . . model . hashes import Hashes \n 
from . . one_drive_object_base import OneDriveObjectBase \n 
class File ( OneDriveObjectBase ) : \n 
~~~ def __init__ ( self , prop_dict = { } ) : \n 
~~~ self . _prop_dict = prop_dict \n 
def hashes ( self ) : \n 
if "hashes" in self . _prop_dict : \n 
~~~ if isinstance ( self . _prop_dict [ "hashes" ] , OneDriveObjectBase ) : \n 
~~~ return self . _prop_dict [ "hashes" ] \n 
~~~ self . _prop_dict [ "hashes" ] = Hashes ( self . _prop_dict [ "hashes" ] ) \n 
return self . _prop_dict [ "hashes" ] \n 
~~ ~~ return None \n 
~~ @ hashes . setter \n 
def hashes ( self , val ) : \n 
~~~ self . _prop_dict [ "hashes" ] = val \n 
def mime_type ( self ) : \n 
if "mimeType" in self . _prop_dict : \n 
~~~ return self . _prop_dict [ "mimeType" ] \n 
~~ ~~ @ mime_type . setter \n 
def mime_type ( self , val ) : \n 
~~~ self . _prop_dict [ "mimeType" ] = val \n 
class RequestBuilderBase ( object ) : \n 
~~~ def __init__ ( self , request_url , client ) : \n 
self . _request_url = request_url \n 
self . _client = client \n 
~~ def append_to_request_url ( self , url_segment ) : \n 
return self . _request_url + "/" + url_segment \n 
from . . collection_base import CollectionRequestBase , CollectionResponseBase , CollectionPageBase \n 
from . . request_builder_base import RequestBuilderBase \n 
from . . model . item import Item \n 
class SharedCollectionRequest ( CollectionRequestBase ) : \n 
~~~ def __init__ ( self , request_url , client , options ) : \n 
super ( SharedCollectionRequest , self ) . __init__ ( request_url , client , options ) \n 
~~ def get ( self ) : \n 
self . method = "GET" \n 
collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n 
return self . _page_from_response ( collection_response ) \n 
~~ ~~ class SharedCollectionRequestBuilder ( RequestBuilderBase ) : \n 
~~~ def __getitem__ ( self , key ) : \n 
return ItemRequestBuilder ( self . append_to_request_url ( str ( key ) ) , self . _client ) \n 
~~ def request ( self , expand = None , select = None , top = None , order_by = None , options = None ) : \n 
req = SharedCollectionRequest ( self . _request_url , self . _client , options ) \n 
req . _set_query_options ( expand = expand , select = select , top = top , order_by = order_by ) \n 
return req \n 
return self . request ( ) . get ( ) \n 
~~ ~~ class SharedCollectionResponse ( CollectionResponseBase ) : \n 
~~~ @ property \n 
def collection_page ( self ) : \n 
if self . _collection_page : \n 
~~~ self . _collection_page . _prop_list = self . _prop_dict [ "value" ] \n 
~~~ self . _collection_page = SharedCollectionPage ( self . _prop_dict [ "value" ] ) \n 
~~ return self . _collection_page \n 
~~ ~~ class SharedCollectionPage ( CollectionPageBase ) : \n 
~~~ def __getitem__ ( self , index ) : \n 
return Item ( self . _prop_list [ index ] ) \n 
~~ def shared ( self ) : \n 
for item in self . _prop_list : \n 
~~~ yield Item ( item ) \n 
~~ ~~ def _init_next_page_request ( self , next_page_link , client , options ) : \n 
self . _next_page_request = SharedCollectionRequest ( next_page_link , client , options ) \n 
~~ ~~ from . . request . item_request_builder import ItemRequestBuilder \n 
from __future__ import absolute_import , division , print_function \n 
import glob \n 
import os . path \n 
from cffi import FFI \n 
from cffi . verifier import Verifier \n 
__all__ = [ "ffi" ] \n 
HEADERS = glob . glob ( \n 
os . path . join ( os . path . abspath ( os . path . dirname ( __file__ ) ) , "*.h" ) \n 
ffi = FFI ( ) \n 
for header in sorted ( HEADERS ) : \n 
~~~ with open ( header , "r" ) as hfile : \n 
~~~ ffi . cdef ( hfile . read ( ) ) \n 
~~ ~~ ffi . verifier = Verifier ( \n 
ffi , \n 
libraries = [ "sodium" ] , \n 
ext_package = "nacl._lib" , \n 
class Library ( object ) : \n 
~~~ def __init__ ( self , ffi ) : \n 
~~~ self . ffi = ffi \n 
self . _lib = None \n 
def _compile_module ( * args , ** kwargs ) : \n 
~~ self . ffi . verifier . compile_module = _compile_module \n 
~~ def __getattr__ ( self , name ) : \n 
~~~ if self . _lib is None : \n 
~~~ self . _lib = self . ffi . verifier . load_library ( ) \n 
~~ return getattr ( self . _lib , name ) \n 
~~ ~~ lib = Library ( ffi ) \n 
import sqlite3 \n 
def migrate ( database_path ) : \n 
conn = sqlite3 . connect ( database_path ) \n 
conn . text_factory = str \n 
cursor = conn . cursor ( ) \n 
cursor . execute ( ) \n 
notifications = cursor . fetchall ( ) \n 
for n in notifications : \n 
~~~ cursor . execute ( , ( n [ 0 ] , n [ 1 ] , n [ 2 ] , n [ 3 ] , n [ 4 ] , n [ 5 ] , n [ 6 ] , n [ 7 ] , \n 
~~ cursor . execute ( ) \n 
conn . commit ( ) \n 
conn . close ( ) \n 
DEBUG = 5 \n 
WARNING = 4 \n 
INFO = 3 \n 
ERROR = 2 \n 
CRITICAL = 1 \n 
levels = { "debug" : 5 , "warning" : 4 , "info" : 3 , "error" : 2 , "critical" : 1 } \n 
class FileLogObserver ( log . FileLogObserver ) : \n 
~~~ def __init__ ( self , f = None , level = "info" , default = DEBUG ) : \n 
~~~ log . FileLogObserver . __init__ ( self , f or sys . stdout ) \n 
self . level = levels [ level ] \n 
self . default = default \n 
~~ def emit ( self , eventDict ) : \n 
~~~ ll = eventDict . get ( , self . default ) \n 
if eventDict [ ] or in eventDict or self . level >= ll : \n 
~~~ log . FileLogObserver . emit ( self , eventDict ) \n 
~~ ~~ ~~ class Logger ( object ) : \n 
~~~ def __init__ ( self , ** kwargs ) : \n 
~~~ self . kwargs = kwargs \n 
~~ def msg ( self , message , ** kw ) : \n 
~~~ kw . update ( self . kwargs ) \n 
if in kw and not isinstance ( kw [ ] , str ) : \n 
~~~ kw [ ] = kw [ ] . __class__ . __name__ \n 
~~ log . msg ( message , ** kw ) \n 
~~ def info ( self , message , ** kw ) : \n 
~~~ kw [ ] = INFO \n 
~~ def debug ( self , message , ** kw ) : \n 
~~~ kw [ ] = DEBUG \n 
~~ def warning ( self , message , ** kw ) : \n 
~~~ kw [ ] = WARNING \n 
~~ def error ( self , message , ** kw ) : \n 
~~~ kw [ ] = ERROR \n 
~~ def critical ( self , message , ** kw ) : \n 
~~~ kw [ ] = CRITICAL \n 
~~ ~~ try : \n 
~~~ theLogger \n 
~~ except NameError : \n 
~~~ theLogger = Logger ( ) \n 
msg = theLogger . msg \n 
info = theLogger . info \n 
debug = theLogger . debug \n 
warning = theLogger . warning \n 
error = theLogger . error \n 
critical = theLogger . critical \n 
~~ import sys \n 
_b = sys . version_info [ 0 ] < 3 and ( lambda x : x ) or ( lambda x : x . encode ( ) ) \n 
from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import symbol_database as _symbol_database \n 
_sym_db = _symbol_database . Default ( ) \n 
_sym_db . RegisterFileDescriptor ( DESCRIPTOR ) \n 
_PEERSEEDS = _descriptor . Descriptor ( \n 
number = 1 , type = 12 , cpp_type = 9 , label = 3 , \n 
number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n 
has_default_value = False , default_value = _b ( "" ) , \n 
oneofs = [ \n 
serialized_start = 15 , \n 
serialized_end = 69 , \n 
DESCRIPTOR . message_types_by_name [ ] = _PEERSEEDS \n 
PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n 
DESCRIPTOR = _PEERSEEDS , \n 
__module__ = \n 
_sym_db . RegisterMessage ( PeerSeeds ) \n 
@ register . filter ( name = ) \n 
def get_item ( dictionary , key ) : \n 
~~~ return getattr ( dictionary , key ) \n 
~~ default_app_config = \n 
from . . utils . access_permissions import BaseAccessPermissions \n 
class MediafileAccessPermissions ( BaseAccessPermissions ) : \n 
def can_retrieve ( self , user ) : \n 
return user . has_perm ( ) \n 
~~ def get_serializer_class ( self , user = None ) : \n 
from . serializers import MediafileSerializer \n 
return MediafileSerializer \n 
~~ ~~ from __future__ import unicode_literals \n 
import openslides . utils . models \n 
~~~ initial = True \n 
dependencies = [ \n 
migrations . CreateModel ( \n 
( , models . AutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name ( , models . CharField ( max_length = 128 , verbose_name = ) ) , \n 
( , models . DateTimeField ( blank = True , null = True , verbose_name = ( , models . BooleanField ( \n 
default = False , \n 
help_text = verbose_name = ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 , unique = True ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 50 ) ) , \n 
( , models . TextField ( blank = True , default = ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 100 ) ) , \n 
( , models . BooleanField ( default = True ) ) , \n 
( , models . BooleanField ( default = False ) ) , \n 
( , models . ManyToManyField ( \n 
blank = True , \n 
help_text = related_name = , \n 
related_query_name = , \n 
to = , \n 
verbose_name = ) ) , \n 
help_text = , \n 
related_name = , \n 
options = { \n 
: ( \n 
( , ) ( , ) ) , \n 
: ( ) , \n 
: ( , , ) , \n 
bases = ( openslides . utils . models . RESTModelMixin , models . Model ) , \n 
~~ import json \n 
from django . dispatch import receiver \n 
from rest_framework import status \n 
from rest_framework . test import APIClient \n 
from openslides import __version__ as version \n 
from openslides . core . config import ConfigVariable , config \n 
from openslides . core . models import CustomSlide , Projector \n 
from openslides . core . signals import config_signal \n 
from openslides . utils . rest_api import ValidationError \n 
from openslides . utils . test import TestCase \n 
class ProjectorAPI ( TestCase ) : \n 
def test_slide_on_default_projector ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n 
default_projector . config = { \n 
: { : , : customslide . id } } \n 
default_projector . save ( ) \n 
response = self . client . get ( reverse ( , args = [ ] ) ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
: 1 , \n 
{ : customslide . id , \n 
: } } , \n 
: 0 , \n 
: 0 } ) \n 
~~ def test_invalid_slide_on_default_projector ( self ) : \n 
default_projector = Projector . objects . get ( pk = 1 ) \n 
: { : } } \n 
~~ ~~ class VersionView ( TestCase ) : \n 
def test_get ( self ) : \n 
response = self . client . get ( reverse ( ) ) \n 
: version , \n 
: } ] } ) \n 
~~ ~~ class ConfigViewSet ( TestCase ) : \n 
def test_retrieve ( self ) : \n 
config [ ] = \n 
response = self . client . get ( reverse ( , args = [ ] ) ) self . assertEqual ( \n 
response . data , \n 
: } ) \n 
~~ def test_update ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( config [ ] , ) \n 
~~ def test_update_wrong_datatype ( self ) : \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
~~ def test_update_wrong_datatype_that_can_be_converted ( self ) : \n 
self . client = APIClient ( ) \n 
self . assertEqual ( response . status_code , 200 ) \n 
~~ def test_update_good_choice ( self ) : \n 
~~ def test_update_bad_choice ( self ) : \n 
self . assertEqual ( response . data , { : } ) \n 
~~ def test_update_validator_ok ( self ) : \n 
~~ def test_update_validator_invalid ( self ) : \n 
~~ def test_update_only_with_key ( self ) : \n 
reverse ( , args = [ ] ) ) \n 
~~ def test_metadata_with_hidden ( self ) : \n 
response = self . client . options ( reverse ( ) ) \n 
filter_obj = filter ( \n 
lambda item : item [ ] == , \n 
response . data [ ] [ 0 ] [ ] [ 0 ] [ ] ) \n 
self . assertEqual ( len ( list ( filter_obj ) ) , 0 ) \n 
~~ ~~ def validator_for_testing ( value ) : \n 
if value == : \n 
~~~ raise ValidationError ( { : } ) \n 
~~ ~~ @ receiver ( config_signal , dispatch_uid = ) \n 
def set_simple_config_view_integration_config_test ( sender , ** kwargs ) : \n 
yield ConfigVariable ( \n 
default_value = None , \n 
label = ) \n 
default_value = ) \n 
default_value = 0 , \n 
input_type = ) \n 
default_value = , \n 
input_type = , \n 
choices = ( \n 
{ : , : } , { : , : } ) \n 
validators = ( validator_for_testing , ) ) \n 
label = , \n 
hidden = True ) \n 
~~ from unittest import TestCase \n 
from unittest . mock import MagicMock , patch \n 
from openslides . users . serializers import UserFullSerializer \n 
class UserCreateUpdateSerializerTest ( TestCase ) : \n 
~~~ def test_validate_no_data ( self ) : \n 
serializer = UserFullSerializer ( ) \n 
with self . assertRaises ( ValidationError ) : \n 
~~~ serializer . validate ( data ) \n 
~~ ~~ @ patch ( ) \n 
def test_validate_no_username ( self , generate_username ) : \n 
generate_username . return_value = \n 
data = { : } \n 
new_data = serializer . validate ( data ) \n 
self . assertEqual ( new_data [ ] , ) \n 
~~ def test_validate_no_username_in_patch_request ( self ) : \n 
view = MagicMock ( action = ) \n 
serializer = UserFullSerializer ( context = { : view } ) \n 
self . assertIsNone ( new_data . get ( ) ) \n 
#domain... #localhost... \n 
, re . IGNORECASE ) \n 
USERNAME_REGEX = re . compile ( , re . I ) \n 
FULLNAME_REGEX = re . compile ( , re . U ) \n 
EMAIL_REGEX = re . compile ( , re . IGNORECASE ) \n 
class CheckValue ( object ) : \n 
~~ def length ( self , data , minimum = - 1 , maximum = - 1 ) : \n 
len_input = len ( data ) \n 
if len_input < minimum or maximum != - 1 and len_input > maximum : \n 
~~ def regexp ( self , data , regex , flags = 0 ) : \n 
regex = re . compile ( regex , flags ) \n 
if regex . match ( data ) : \n 
~~ def username ( self , data ) : \n 
if USERNAME_REGEX . match ( data ) : \n 
~~ def full_name ( self , data ) : \n 
if FULLNAME_REGEX . match ( data ) : \n 
~~ def email ( self , data ) : \n 
if EMAIL_REGEX . match ( data ) : \n 
~~ def url ( self , data ) : \n 
if URL_REGEX . match ( data ) : \n 
~~ def url_two ( self , data ) : \n 
regex = re . compile ( , re . IGNORECASE ) \n 
~~ def is_integer ( self , data ) : \n 
~~~ tmp = int ( data ) \n 
~~ ~~ def float ( self , data ) : \n 
~~~ tmp = float ( data ) \n 
~~ ~~ ~~ import logging \n 
from bagpipe . bgp . common import utils \n 
from bagpipe . bgp . common import logDecorator \n 
from bagpipe . bgp . vpn . vpn_instance import VPNInstance \n 
from bagpipe . bgp . engine import RouteEvent \n 
from bagpipe . bgp . vpn . dataplane_drivers import DummyDataplaneDriver as _DummyDataplaneDriver \n 
from bagpipe . bgp . common . looking_glass import LookingGlass , LGMap \n 
from bagpipe . exabgp . structure . vpn import RouteDistinguisher , VPNLabelledPrefix \n 
from bagpipe . exabgp . structure . mpls import LabelStackEntry \n 
from bagpipe . exabgp . structure . address import AFI , SAFI \n 
from bagpipe . exabgp . structure . ip import Inet , Prefix \n 
from bagpipe . exabgp . message . update . route import Route \n 
from bagpipe . exabgp . message . update . attribute . nexthop import NextHop \n 
from bagpipe . exabgp . message . update . attribute . communities import ECommunities \n 
class DummyDataplaneDriver ( _DummyDataplaneDriver ) : \n 
~~ class VRF ( VPNInstance , LookingGlass ) : \n 
~~~ type = "ipvpn" \n 
afi = AFI ( AFI . ipv4 ) \n 
safi = SAFI ( SAFI . mpls_vpn ) \n 
@ logDecorator . log \n 
~~~ VPNInstance . __init__ ( self , * args , ** kwargs ) \n 
self . readvertised = set ( ) \n 
~~ def _routeFrom ( self , prefix , label , rd ) : \n 
~~~ return Route ( VPNLabelledPrefix ( self . afi , self . safi , prefix , rd , \n 
[ LabelStackEntry ( label , True ) ] \n 
~~ def generateVifBGPRoute ( self , macAdress , ipPrefix , prefixLen , label ) : \n 
~~~ route = self . _routeFrom ( Prefix ( self . afi , ipPrefix , prefixLen ) , label , \n 
RouteDistinguisher ( \n 
RouteDistinguisher . TYPE_IP_LOC , None , \n 
self . bgpManager . getLocalAddress ( ) , \n 
self . instanceId ) \n 
return self . _newRouteEntry ( self . afi , self . safi , self . exportRTs , \n 
route . nlri , route . attributes ) \n 
~~ def _getLocalLabels ( self ) : \n 
~~~ for portData in self . macAddress2LocalPortData . itervalues ( ) : \n 
~~~ yield portData [ ] \n 
~~ ~~ def _getRDFromLabel ( self , label ) : \n 
~~~ return RouteDistinguisher ( RouteDistinguisher . TYPE_IP_LOC , None , \n 
10000 + label ) \n 
~~ def _routeForReAdvertisement ( self , prefix , label ) : \n 
~~~ route = self . _routeFrom ( prefix , label , \n 
self . _getRDFromLabel ( label ) ) \n 
nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n 
self . dataplane . driver . getLocalAddress ( ) ) ) \n 
route . attributes . add ( NextHop ( nh ) ) \n 
route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n 
routeEntry = self . _newRouteEntry ( self . afi , self . safi , \n 
self . readvertiseToRTs , \n 
return routeEntry \n 
~~ @ logDecorator . log \n 
def _readvertise ( self , nlri ) : \n 
for label in self . _getLocalLabels ( ) : \n 
nlri . prefix , label ) \n 
routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n 
~~ self . readvertised . add ( nlri . prefix ) \n 
def _readvertiseStop ( self , nlri ) : \n 
self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n 
~~ self . readvertised . remove ( nlri . prefix ) \n 
~~ def vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) : \n 
~~~ VPNInstance . vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) \n 
label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
for prefix in self . readvertised : \n 
prefix ) \n 
routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n 
~~ ~~ def vifUnplugged ( self , macAddress , ipAddressPrefix , advertiseSubnet ) : \n 
~~~ label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
~~ VPNInstance . vifUnplugged ( self , macAddress , ipAddressPrefix , \n 
~~ def _route2trackedEntry ( self , route ) : \n 
~~~ if isinstance ( route . nlri , VPNLabelledPrefix ) : \n 
~~~ return route . nlri . prefix \n 
type ( route . nlri ) ) \n 
~~ ~~ def _toReadvertise ( self , route ) : \n 
~~~ return ( len ( set ( route . routeTargets ) . intersection ( \n 
set ( self . readvertiseFromRTs ) ) ) > 0 ) \n 
~~ def _imported ( self , route ) : \n 
set ( self . importRTs ) ) ) > 0 ) \n 
~~ @ utils . synchronized \n 
def _newBestRoute ( self , entry , newRoute ) : \n 
~~~ prefix = entry \n 
if self . readvertise : \n 
if self . _toReadvertise ( newRoute ) : \n 
self . _readvertise ( newRoute . nlri ) \n 
if not self . _imported ( newRoute ) : \n 
~~ ~~ ~~ encaps = self . _checkEncaps ( newRoute ) \n 
if not encaps : \n 
~~ self . dataplane . setupDataplaneForRemoteEndpoint ( \n 
prefix , newRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n 
def _bestRouteRemoved ( self , entry , oldRoute , last ) : \n 
if self . readvertise and last : \n 
~~~ if self . _toReadvertise ( oldRoute ) : \n 
self . _readvertiseStop ( oldRoute . nlri ) \n 
if not self . _imported ( oldRoute ) : \n 
~~ ~~ ~~ if self . _skipRouteRemoval ( last ) : \n 
~~ self . dataplane . removeDataplaneForRemoteEndpoint ( \n 
prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n 
~~ def getLGMap ( self ) : \n 
"readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n 
self . readvertised ] ) \n 
from bagpipe . exabgp . message . update . attribute import AttributeID , Flag , Attribute \n 
class Origin ( Attribute ) : \n 
~~~ ID = AttributeID . ORIGIN \n 
FLAG = Flag . TRANSITIVE \n 
MULTIPLE = False \n 
IGP = 0x00 \n 
EGP = 0x01 \n 
INCOMPLETE = 0x02 \n 
def __init__ ( self , origin ) : \n 
~~~ self . origin = origin \n 
~~ def pack ( self ) : \n 
~~~ return self . _attribute ( chr ( self . origin ) ) \n 
~~~ return len ( self . pack ( ) ) \n 
~~~ if self . origin == 0x00 : return \n 
if self . origin == 0x01 : return \n 
if self . origin == 0x02 : return \n 
~~~ return str ( self ) \n 
~~ def __cmp__ ( self , other ) : \n 
~~~ if ( not isinstance ( other , Origin ) \n 
or ( self . origin != other . origin ) \n 
) : \n 
import inspect \n 
import signal \n 
from multiprocessing import Process \n 
import tasa \n 
from tasa . worker import BaseWorker \n 
logging . basicConfig ( level = logging . INFO ) \n 
def signal_handler ( signal , frame ) : \n 
~~~ sys . exit ( 0 ) \n 
~~ def _get_argparser ( ) : \n 
~~~ parser = argparse . ArgumentParser ( ) \n 
parser . add_argument ( \n 
, , action = , \n 
version = % ( \n 
tasa . __version__ , sys . version ) ) \n 
return parser \n 
~~ def run ( ) : \n 
~~~ sys . path . insert ( 0 , ) \n 
parser = _get_argparser ( ) \n 
parser . description = \n 
parser . add_argument ( , \n 
type = lambda w : w . partition ( ) [ : : 2 ] , \n 
help = \n 
worker_class_name = args . worker [ 1 ] or \n 
worker_module = __import__ ( args . worker [ 0 ] , globals ( ) , locals ( ) , \n 
[ worker_class_name ] ) \n 
~~~ WorkerClass = getattr ( worker_module , worker_class_name ) \n 
potential_workers = inspect . getmembers ( \n 
worker_module , \n 
lambda x : type ( x ) == type and issubclass ( x , BaseWorker ) ) \n 
if potential_workers : \n 
for name , value in potential_workers : \n 
~~~ print . join ( [ args . worker [ 0 ] , name ] ) \n 
~~ ~~ exit ( 1 ) \n 
~~ worker = WorkerClass ( ) \n 
print % ( args . worker [ 0 ] , \n 
worker . __class__ . __name__ ) \n 
~~~ for job in worker : \n 
~~~ if job : \n 
worker . __class__ . __name__ , \n 
str ( job ) [ : 50 ] ) \n 
~~~ time . sleep ( .3 ) \n 
~~ ~~ ~~ except KeyboardInterrupt : \n 
~~~ print \n 
~~ ~~ def runm ( ) : \n 
signal . signal ( signal . SIGINT , signal_handler ) \n 
count = int ( sys . argv . pop ( 1 ) ) \n 
processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n 
~~~ for p in processes : \n 
~~~ p . start ( ) \n 
~~ ~~ except KeyError : \n 
~~~ p . join ( ) \n 
~~ ~~ ~~ def log ( ) : \n 
~~~ parser = _get_argparser ( ) \n 
raise NotImplemented ( ) \n 
~~~ cmd = if len ( sys . argv ) < 2 else sys . argv . pop ( 1 ) \n 
if cmd == : \n 
~~~ run ( ) \n 
~~ elif cmd == : \n 
~~~ log ( ) \n 
from time import sleep \n 
from flask import Flask \n 
from flask_tut . models import ( \n 
db , \n 
User , \n 
Address , \n 
app = Flask ( __name__ ) \n 
with app . app_context ( ) : \n 
~~~ db . create_all ( ) \n 
~~ i = 0 \n 
while i < 30 : \n 
~~~ address = Address ( description = + str ( i ) . rjust ( 2 , "0" ) ) \n 
db . session . add ( address ) \n 
user = User ( name = + str ( i ) . rjust ( 2 , "0" ) ) \n 
user . address = address \n 
db . session . add ( user ) \n 
sleep ( 1 ) \n 
i += 1 \n 
~~ db . session . commit ( ) \n 
import urllib2 \n 
import google \n 
import pyprind \n 
class Crawler ( object ) : \n 
~~~ version = "1.2.3" \n 
outputDir = "output" \n 
languageDir = "languages" \n 
basicString = "/get.php?username=%s&password=%s&type=m3u&output=mpegts" \n 
def __init__ ( self , language = "it" ) : \n 
self . language = language . lower ( ) \n 
self . parsedUrls = [ ] \n 
self . foundedAccounts = 0 \n 
~~ def change_language ( self , language = "it" ) : \n 
if os . path . isfile ( self . languageDir + "/" + language + ".txt" ) : \n 
~~~ self . language = language \n 
~~ ~~ def search_links ( self ) : \n 
for url in google . search ( self . searchString , num = 30 , stop = 1 ) : \n 
~~~ parsed = urlparse ( url ) \n 
self . parsedUrls . append ( parsed . scheme + "://" + parsed . netloc ) \n 
~~ ~~ def search_accounts ( self , url = None ) : \n 
if not self . parsedUrls : \n 
~~~ if not url : \n 
~~~ url = random . choice ( self . parsedUrls ) \n 
~~ fileName = self . languageDir + "/" + self . language + ".txt" \n 
fileLength = self . file_length ( fileName ) \n 
with open ( fileName ) as f : \n 
~~~ rows = f . readlines ( ) \n 
~~ for row in rows : \n 
~~~ opener = urllib2 . build_opener ( ) \n 
opener . addheaders = [ ( , ) ] \n 
response = opener . open ( url + self . basicString % ( row . rstrip ( ) . lstrip ( ) , row . rstrip ( ) fetched = response . read ( ) \n 
fileLength = fileLength - 1 \n 
progressBar . update ( ) \n 
if len ( fetched ) > 0 : \n 
~~~ newPath = self . outputDir + "/" + url . replace ( "http://" , "" ) \n 
self . create_file ( row , newPath , fetched ) \n 
~~ ~~ self . parsedUrls . remove ( url ) \n 
if self . foundedAccounts != 0 : \n 
~~ ~~ except IOError : \n 
~~ except urllib2 . HTTPError , e : \n 
~~ except urllib2 . URLError , e : \n 
~~ except Exception : \n 
~~ ~~ def create_file ( self , row , newPath , fetched ) : \n 
if os . path . exists ( newPath ) is False : \n 
~~~ os . makedirs ( newPath ) \n 
~~ outputFile = open ( str ( newPath ) + "/tv_channels_%s.m3u" % row . rstrip ( ) . lstrip ( ) , "w" ) \n 
outputFile . write ( fetched ) \n 
self . foundedAccounts = self . foundedAccounts + 1 \n 
outputFile . close ( ) \n 
~~ def file_length ( self , fileName ) : \n 
~~~ for i , l in enumerate ( f ) : \n 
~~ ~~ return i + 1 \n 
~~ ~~ from cStringIO import StringIO \n 
from binascii import b2a_hex \n 
from urllib import quote \n 
import Connecter \n 
~~~ True \n 
~~~ True = 1 \n 
False = 0 \n 
~~ DEBUG = False \n 
protocol_name = \n 
option_pattern = chr ( 0 ) * 8 \n 
def toint ( s ) : \n 
~~~ return long ( b2a_hex ( s ) , 16 ) \n 
~~ def tohex ( s ) : \n 
~~~ return b2a_hex ( s ) . upper ( ) \n 
~~ def make_readable ( s ) : \n 
~~~ if not s : \n 
~~ if quote ( s ) . find ( ) >= 0 : \n 
~~~ return tohex ( s ) \n 
~~ return \'"\' + s + \'"\' \n 
~~ streamno = 0 \n 
class StreamCheck : \n 
~~~ global streamno \n 
self . no = streamno \n 
streamno += 1 \n 
self . buffer = StringIO ( ) \n 
self . next_len , self . next_func = 1 , self . read_header_len \n 
~~ def read_header_len ( self , s ) : \n 
~~~ if ord ( s ) != len ( protocol_name ) : \n 
~~~ print self . no , \n 
~~ return len ( protocol_name ) , self . read_header \n 
~~ def read_header ( self , s ) : \n 
~~~ if s != protocol_name : \n 
~~ return 8 , self . read_reserved \n 
~~ def read_reserved ( self , s ) : \n 
~~~ return 20 , self . read_download_id \n 
~~ def read_download_id ( self , s ) : \n 
~~~ if DEBUG : \n 
~~~ print self . no , + tohex ( s ) \n 
~~ return 20 , self . read_peer_id \n 
~~ def read_peer_id ( self , s ) : \n 
~~~ print self . no , + make_readable ( s ) \n 
~~ return 4 , self . read_len \n 
~~ def read_len ( self , s ) : \n 
~~~ l = toint ( s ) \n 
if l > 2 ** 23 : \n 
~~~ print self . no , + str ( l ) + + s + \n 
~~ return l , self . read_message \n 
~~ def read_message ( self , s ) : \n 
~~~ return 4 , self . read_len \n 
~~ m = s [ 0 ] \n 
if ord ( m ) > 8 : \n 
~~~ print self . no , + str ( ord ( m ) ) \n 
~~ if m == Connecter . REQUEST : \n 
~~~ if len ( s ) != 13 : \n 
~~~ print self . no , + str ( len ( s ) ) \n 
return 4 , self . read_len \n 
~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = toint ( s [ 9 : ] ) \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ elif m == Connecter . CANCEL : \n 
~~ elif m == Connecter . PIECE : \n 
~~~ index = toint ( s [ 1 : 5 ] ) \n 
length = len ( s ) - 9 \n 
~~~ print self . no , + str ( ord ( m ) ) + + str ( len ( s ) ) + \n 
~~ def write ( self , s ) : \n 
~~~ while 1 : \n 
~~~ i = self . next_len - self . buffer . tell ( ) \n 
if i > len ( s ) : \n 
~~~ self . buffer . write ( s ) \n 
~~ self . buffer . write ( s [ : i ] ) \n 
s = s [ i : ] \n 
m = self . buffer . getvalue ( ) \n 
self . buffer . reset ( ) \n 
self . buffer . truncate ( ) \n 
x = self . next_func ( m ) \n 
self . next_len , self . next_func = x \n 
~~ ~~ ~~ from types import * \n 
from cStringIO import StringIO \n 
def splitLine ( line , COLS = 80 , indent = 10 ) : \n 
width = COLS - ( len ( indent ) + 1 ) \n 
if indent and width < 15 : \n 
~~~ width = COLS - 2 \n 
~~ s = StringIO ( ) \n 
i = 0 \n 
for word in line . split ( ) : \n 
~~~ if i == 0 : \n 
~~~ s . write ( indent + word ) \n 
i = len ( word ) \n 
continue \n 
~~ if i + len ( word ) >= width : \n 
~~~ s . write ( + indent + word ) \n 
~~ s . write ( + word ) \n 
i += len ( word ) + 1 \n 
~~ return s . getvalue ( ) \n 
~~ def formatDefinitions ( options , COLS , presets = { } ) : \n 
~~~ s = StringIO ( ) \n 
for ( longname , default , doc ) in options : \n 
~~~ s . write ( + longname + ) \n 
default = presets . get ( longname , default ) \n 
if type ( default ) in ( IntType , LongType ) : \n 
~~~ default = int ( default ) \n 
~~ ~~ if default is not None : \n 
~~~ doc += + repr ( default ) + \n 
~~ s . write ( splitLine ( doc , COLS , 10 ) ) \n 
s . write ( ) \n 
~~ def usage ( string ) : \n 
~~~ raise ValueError ( string ) \n 
~~ def defaultargs ( options ) : \n 
~~~ l = { } \n 
~~~ if default is not None : \n 
~~~ l [ longname ] = default \n 
~~ ~~ return l \n 
~~ def parseargs ( argv , options , minargs = None , maxargs = None , presets = { } ) : \n 
~~~ config = { } \n 
longkeyed = { } \n 
for option in options : \n 
~~~ longname , default , doc = option \n 
longkeyed [ longname ] = option \n 
config [ longname ] = default \n 
~~~ config [ longname ] = presets [ longname ] \n 
~~ options = [ ] \n 
args = [ ] \n 
pos = 0 \n 
while pos < len ( argv ) : \n 
~~~ if argv [ pos ] [ : 2 ] != : \n 
~~~ args . append ( argv [ pos ] ) \n 
pos += 1 \n 
~~~ if pos == len ( argv ) - 1 : \n 
~~~ usage ( ) \n 
~~ key , value = argv [ pos ] [ 2 : ] , argv [ pos + 1 ] \n 
pos += 2 \n 
if not longkeyed . has_key ( key ) : \n 
~~~ usage ( + key ) \n 
~~ longname , default , doc = longkeyed [ key ] \n 
~~~ t = type ( config [ longname ] ) \n 
if t is NoneType or t is StringType : \n 
~~~ config [ longname ] = value \n 
~~ elif t in ( IntType , LongType ) : \n 
~~~ config [ longname ] = long ( value ) \n 
~~ elif t is FloatType : \n 
~~~ config [ longname ] = float ( value ) \n 
~~~ assert 0 \n 
~~ ~~ except ValueError , e : \n 
~~~ usage ( % ( key , str ( e ) ) ) \n 
~~ ~~ ~~ for key , value in config . items ( ) : \n 
~~~ if value is None : \n 
~~ ~~ if minargs is not None and len ( args ) < minargs : \n 
~~ if maxargs is not None and len ( args ) > maxargs : \n 
~~ return ( config , args ) \n 
~~ def test_parseargs ( ) : \n 
~~~ assert parseargs ( ( , , , , , , , , ) , ( ( , , ) , ( , assert parseargs ( [ ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
assert parseargs ( [ , , , ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
~~~ parseargs ( [ ] , [ ( , , ) ] ) \n 
~~~ parseargs ( [ , ] , [ ] ) \n 
~~~ parseargs ( [ ] , [ ] , 1 , 2 ) \n 
~~ assert parseargs ( [ ] , [ ] , 1 , 2 ) == ( { } , [ ] ) \n 
assert parseargs ( [ , ] , [ ] , 1 , 2 ) == ( { } , [ , ] ) \n 
~~~ parseargs ( [ , , ] , [ ] , 1 , 2 ) \n 
~~~ parseargs ( [ , ] , [ ( , 3 , ) ] ) \n 
~~~ parseargs ( [ , ] , [ ( , 2.1 , ) ] ) \n 
~~ ~~ import datetime \n 
from south . db import db \n 
from south . v2 import SchemaMigration \n 
class Migration ( SchemaMigration ) : \n 
~~~ db . add_column ( , , \n 
self . gf ( ) ( to = orm [ keep_default = False ) \n 
~~~ db . delete_column ( , ) \n 
: { : } , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'auth.Permission\']" } , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.Group\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" : ( , [ ] , { : , : } , \n 
: ( , [ ] , { : , : } , \n 
: ( , [ ] , { : "\'taggit_taggeditem_tagged_items\'" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "\'taggit_taggeditem_items\'" } , \n 
: ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.User\']" , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : } \n 
from collections import namedtuple \n 
from shutil import rmtree \n 
from stat import S_IFDIR , S_IFREG , S_IFLNK \n 
from pygit2 import ( clone_repository , Signature , GIT_SORT_TOPOLOGICAL , \n 
GIT_FILEMODE_TREE , GIT_STATUS_CURRENT , \n 
GIT_FILEMODE_LINK , GIT_FILEMODE_BLOB , GIT_BRANCH_REMOTE , \n 
GIT_BRANCH_LOCAL , GIT_FILEMODE_BLOB_EXECUTABLE ) \n 
from six import iteritems \n 
from gitfs . cache import CommitCache \n 
from gitfs . log import log \n 
from gitfs . utils . path import split_path_into_components \n 
from gitfs . utils . commits import CommitsList \n 
DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n 
"first_commits" , "second_commits" ] ) \n 
class Repository ( object ) : \n 
~~~ def __init__ ( self , repository , commits = None ) : \n 
~~~ self . _repo = repository \n 
self . commits = commits or CommitCache ( self ) \n 
self . behind = False \n 
~~ def __getitem__ ( self , item ) : \n 
return self . _repo [ item ] \n 
~~ def __getattr__ ( self , attr ) : \n 
if attr not in self . __dict__ : \n 
~~~ return getattr ( self . _repo , attr ) \n 
~~~ return self . __dict__ [ attr ] \n 
~~ ~~ def ahead ( self , upstream , branch ) : \n 
~~~ ahead , _ = self . diverge ( upstream , branch ) \n 
return ahead \n 
~~ def diverge ( self , upstream , branch ) : \n 
~~~ reference = "{}/{}" . format ( upstream , branch ) \n 
remote_branch = self . lookup_branch ( reference , GIT_BRANCH_REMOTE ) \n 
local_branch = self . lookup_branch ( branch , GIT_BRANCH_LOCAL ) \n 
if remote_branch . target == local_branch . target : \n 
~~~ return False , False \n 
~~ diverge_commits = self . find_diverge_commits ( local_branch , \n 
remote_branch ) \n 
behind = len ( diverge_commits . second_commits ) > 0 \n 
ahead = len ( diverge_commits . first_commits ) > 0 \n 
return ahead , behind \n 
~~ def checkout ( self , ref , * args , ** kwargs ) : \n 
~~~ result = self . _repo . checkout ( ref , * args , ** kwargs ) \n 
self . ignore . update ( ) \n 
status = self . _repo . status ( ) \n 
for path , status in iteritems ( status ) : \n 
~~~ if status == GIT_STATUS_CURRENT : \n 
~~ full_path = self . _full_path ( path ) \n 
if path not in self . _repo . index : \n 
~~~ if path not in self . ignore : \n 
~~~ os . unlink ( full_path ) \n 
~~ except OSError : \n 
~~~ rmtree ( \n 
full_path , \n 
onerror = lambda function , fpath , excinfo : log . info ( \n 
~~ ~~ continue \n 
~~ stats = self . get_git_object_default_stats ( ref , path ) \n 
current_stat = os . lstat ( full_path ) \n 
if stats [ ] != current_stat . st_mode : \n 
~~~ os . chmod ( full_path , current_stat . st_mode ) \n 
self . _repo . index . add ( self . _sanitize ( path ) ) \n 
~~ ~~ return result \n 
~~ def _sanitize ( self , path ) : \n 
~~~ if path is not None and path . startswith ( "/" ) : \n 
~~~ path = path [ 1 : ] \n 
~~ return path \n 
~~ def push ( self , upstream , branch , credentials ) : \n 
remote = self . get_remote ( upstream ) \n 
remote . push ( [ "refs/heads/%s" % ( branch ) ] , callbacks = credentials ) \n 
~~ def fetch ( self , upstream , branch_name , credentials ) : \n 
remote . fetch ( callbacks = credentials ) \n 
_ , behind = self . diverge ( upstream , branch_name ) \n 
self . behind = behind \n 
return behind \n 
~~ def commit ( self , message , author , commiter , parents = None , ref = "HEAD" ) : \n 
if status == { } : \n 
~~ author = Signature ( author [ 0 ] , author [ 1 ] ) \n 
commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n 
tree = self . _repo . index . write_tree ( ) \n 
self . _repo . index . write ( ) \n 
if parents is None : \n 
~~~ parents = [ self . _repo . revparse_single ( ref ) . id ] \n 
~~ return self . _repo . create_commit ( ref , author , commiter , message , \n 
tree , parents ) \n 
def clone ( cls , remote_url , path , branch = None , credentials = None ) : \n 
repo = clone_repository ( remote_url , path , checkout_branch = branch , \n 
callbacks = credentials ) \n 
repo . checkout_head ( ) \n 
return cls ( repo ) \n 
~~ def _is_searched_entry ( self , entry_name , searched_entry , path_components ) : \n 
return ( entry_name == searched_entry and \n 
len ( path_components ) == 1 and \n 
entry_name == path_components [ 0 ] ) \n 
~~ def _get_git_object ( self , tree , obj_name , path_components , modifier ) : \n 
git_obj = None \n 
for entry in tree : \n 
~~~ if self . _is_searched_entry ( entry . name , obj_name , path_components ) : \n 
~~~ return modifier ( entry ) \n 
~~ elif entry . filemode == GIT_FILEMODE_TREE : \n 
~~~ git_obj = self . _get_git_object ( self . _repo [ entry . id ] , obj_name , \n 
path_components [ 1 : ] , modifier ) \n 
if git_obj : \n 
~~~ return git_obj \n 
~~ ~~ ~~ return git_obj \n 
~~ def get_git_object_type ( self , tree , path ) : \n 
path_components = split_path_into_components ( path ) \n 
~~~ return self . _get_git_object ( tree , path_components [ - 1 ] , \n 
path_components , \n 
lambda entry : entry . filemode ) \n 
~~~ return GIT_FILEMODE_TREE \n 
~~ ~~ def get_git_object ( self , tree , path ) : \n 
return self . _get_git_object ( tree , path_components [ - 1 ] , path_components , \n 
lambda entry : self . _repo [ entry . id ] ) \n 
~~ def get_git_object_default_stats ( self , ref , path ) : \n 
~~~ types = { \n 
GIT_FILEMODE_LINK : { \n 
: S_IFLNK | 0o444 , \n 
} , GIT_FILEMODE_TREE : { \n 
: S_IFDIR | 0o555 , \n 
: 2 \n 
} , GIT_FILEMODE_BLOB : { \n 
: S_IFREG | 0o444 , \n 
} , GIT_FILEMODE_BLOB_EXECUTABLE : { \n 
: S_IFREG | 0o555 , \n 
if path == "/" : \n 
~~~ return types [ GIT_FILEMODE_TREE ] \n 
~~ obj_type = self . get_git_object_type ( ref , path ) \n 
if obj_type is None : \n 
~~~ return obj_type \n 
~~ stats = types [ obj_type ] \n 
if obj_type in [ GIT_FILEMODE_BLOB , GIT_FILEMODE_BLOB_EXECUTABLE ] : \n 
~~~ stats [ ] = self . get_blob_size ( ref , path ) \n 
~~ return stats \n 
~~ def get_blob_size ( self , tree , path ) : \n 
return self . get_git_object ( tree , path ) . size \n 
~~ def get_blob_data ( self , tree , path ) : \n 
return self . get_git_object ( tree , path ) . data \n 
~~ def get_commit_dates ( self ) : \n 
return list ( self . commits . keys ( ) ) \n 
~~ def get_commits_by_date ( self , date ) : \n 
return list ( map ( str , self . commits [ date ] ) ) \n 
~~ def walk_branches ( self , sort , * branches ) : \n 
iterators = [ self . _repo . walk ( branch . target , sort ) \n 
for branch in branches ] \n 
stop_iteration = [ False for branch in branches ] \n 
commits = [ ] \n 
for iterator in iterators : \n 
~~~ commit = next ( iterator ) \n 
~~ except StopIteration : \n 
~~~ commit = None \n 
~~ commits . append ( commit ) \n 
~~ yield ( commit for commit in commits ) \n 
while not all ( stop_iteration ) : \n 
~~~ for index , iterator in enumerate ( iterators ) : \n 
commits [ index ] = commit \n 
~~~ stop_iteration [ index ] = True \n 
~~ ~~ if not all ( stop_iteration ) : \n 
~~~ yield ( commit for commit in commits ) \n 
~~ ~~ ~~ def remote_head ( self , upstream , branch ) : \n 
~~~ ref = "%s/%s" % ( upstream , branch ) \n 
remote = self . _repo . lookup_branch ( ref , GIT_BRANCH_REMOTE ) \n 
return remote . get_object ( ) \n 
~~ def get_remote ( self , name ) : \n 
remote = [ remote for remote in self . _repo . remotes \n 
if remote . name == name ] \n 
if not remote : \n 
~~ return remote [ 0 ] \n 
~~ def _full_path ( self , partial ) : \n 
~~~ if partial . startswith ( "/" ) : \n 
~~~ partial = partial [ 1 : ] \n 
~~ return os . path . join ( self . _repo . workdir , partial ) \n 
~~ def find_diverge_commits ( self , first_branch , second_branch ) : \n 
common_parent = None \n 
first_commits = CommitsList ( ) \n 
second_commits = CommitsList ( ) \n 
walker = self . walk_branches ( GIT_SORT_TOPOLOGICAL , \n 
first_branch , second_branch ) \n 
for first_commit , second_commit in walker : \n 
~~~ if ( first_commit in second_commits or \n 
second_commit in first_commits ) : \n 
~~ if first_commit not in first_commits : \n 
~~~ first_commits . append ( first_commit ) \n 
~~ if second_commit not in second_commits : \n 
~~~ second_commits . append ( second_commit ) \n 
~~ if second_commit . hex == first_commit . hex : \n 
~~~ index = second_commits . index ( first_commit ) \n 
~~~ second_commits = second_commits [ : index ] \n 
common_parent = first_commit \n 
~~~ index = first_commits . index ( second_commit ) \n 
~~~ first_commits = first_commits [ : index ] \n 
common_parent = second_commit \n 
~~ return DivergeCommits ( common_parent , first_commits , second_commits ) \n 
~~ ~~ from datetime import datetime \n 
from mock import MagicMock , call \n 
from pygit2 import GIT_SORT_TIME \n 
from gitfs . cache . commits import Commit , CommitCache \n 
class TestCommit ( object ) : \n 
~~~ def test_commit ( self ) : \n 
~~~ commit = Commit ( 1 , 1 , 1 ) \n 
new_commit = Commit ( 2 , 2 , "21111111111" ) \n 
assert new_commit > commit \n 
assert repr ( new_commit ) == "2-2111111111" \n 
~~ ~~ class TestCommitCache ( object ) : \n 
~~~ def test_cache ( self ) : \n 
~~~ mocked_repo = MagicMock ( ) \n 
mocked_commit = MagicMock ( ) \n 
mocked_repo . lookup_reference ( ) . resolve ( ) . target = "head" \n 
mocked_repo . walk . return_value = [ mocked_commit ] \n 
mocked_commit . commit_time = 1411135000 \n 
mocked_commit . hex = \n 
cache = CommitCache ( mocked_repo ) \n 
cache . update ( ) \n 
cache [ ] = Commit ( 1 , 1 , "1111111111" ) \n 
assert sorted ( cache . keys ( ) ) == [ , ] \n 
asserted_time = datetime . fromtimestamp ( mocked_commit . commit_time ) \n 
asserted_time = "{}-{}-{}" . format ( asserted_time . hour , asserted_time . minute , \n 
asserted_time . second ) \n 
assert repr ( cache [ ] ) == % asserted_time \n 
del cache [ ] \n 
for commit_date in cache : \n 
~~~ assert commit_date == \n 
~~ mocked_repo . lookup_reference . has_calls ( [ call ( "HEAD" ) ] ) \n 
mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n 
assert mocked_repo . lookup_reference ( ) . resolve . call_count == 2 \n 
~~ ~~ import pytest \n 
import datetime as dt \n 
from mock import MagicMock \n 
from gitfs . utils . strptime import TimeParser \n 
from gitfs . utils import strptime \n 
class TestDateTimeUtils ( object ) : \n 
~~~ def test_strptime ( self ) : \n 
~~~ date = dt . date ( 2014 , 8 , 21 ) \n 
datetime = dt . datetime ( 2014 , 8 , 21 , 1 , 2 , 3 ) \n 
to_datetime = True ) == datetime \n 
date = dt . date ( 2014 , 8 , 30 ) \n 
datetime = dt . datetime ( 2014 , 8 , 30 , 1 , 2 , 3 ) \n 
date = dt . date ( 1970 , 1 , 1 ) \n 
datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n 
with pytest . raises ( ValueError ) : \n 
~~ ~~ def test_time_parser_match_with_value_error ( self ) : \n 
~~~ mocked_pattern = MagicMock ( ) \n 
mocked_pattern . match . return_value = False \n 
parser . pattern = mocked_pattern \n 
~~~ parser . match ( "daytime" ) \n 
~~ mocked_pattern . match . assert_called_once_with ( "daytime" ) \n 
~~ ~~ from zipa import api_github_com as github \n 
repos = github . orgs . django . repos \n 
for repo in repos [ { : , : } ] : \n 
~~~ print repo . name \n 
~~~ from django . conf . urls import patterns , url \n 
~~ urlpatterns = patterns ( , \n 
url ( , , name = ) , \n 
url ( , , name = ) , ) \n 
from django . http import * \n 
from django . core import serializers \n 
from django . core . exceptions import ValidationError , SuspiciousOperation , ObjectDoesNotExist \n 
from django . db import IntegrityError , connection , transaction \n 
from django . shortcuts import render_to_response \n 
from django . core . context_processors import csrf \n 
from django . contrib . comments . models import Comment \n 
from django . contrib . comments . forms import CommentForm \n 
from django . contrib . contenttypes . models import ContentType \n 
from django . contrib . auth . decorators import login_required , user_passes_test \n 
from django . contrib . sessions . models import Session \n 
from django . contrib . sessions . backends . db import SessionStore \n 
from django . contrib . gis . geos . collections import MultiPolygon \n 
from django . contrib . gis . geos import GEOSGeometry \n 
from django . contrib . gis . gdal import * \n 
from django . contrib . gis . gdal . libgdal import lgdal \n 
from django . contrib import humanize \n 
from django . template import loader , Context as DjangoContext , RequestContext \n 
from django . utils import simplejson as json , translation \n 
from django . utils . translation import ugettext as _ , ungettext as _n \n 
from django . template . defaultfilters import slugify , force_escape \n 
from tagging . utils import parse_tag_input \n 
from datetime import datetime , time , timedelta \n 
from decimal import * \n 
from functools import wraps \n 
from redistricting . calculators import * \n 
from redistricting . models import * \n 
from redistricting . tasks import * \n 
import random , string , math , types , copy , time , threading , traceback , os \n 
import commands , sys , tempfile , csv , hashlib , inflect , logging \n 
import ModestMaps \n 
from PIL import Image , ImageChops , ImageMath \n 
import urllib , urllib2 \n 
from xhtml2pdf . pisa import CreatePDF \n 
import StringIO \n 
UNASSIGNED_DISTRICT_ID = 0 \n 
def using_unique_session ( u ) : \n 
if u . is_anonymous ( ) or u . is_superuser : \n 
~~ sessions = Session . objects . all ( ) \n 
count = 0 \n 
for session in sessions : \n 
~~~ decoded = session . get_decoded ( ) \n 
if in decoded and decoded [ ] == u . id : \n 
~~~ if in decoded and decoded [ ] < datetime . now ( ) : \n 
~~~ Session . objects . filter ( session_key = session . session_key ) . delete ( ) \n 
~~~ count += 1 \n 
~~ ~~ ~~ except SuspiciousOperation : \n 
~~ ~~ for session in sessions : \n 
~~~ websession = SessionStore ( session_key = session . session_key ) \n 
websession [ ] = count \n 
websession . save ( ) \n 
~~ ~~ except SuspiciousOperation : \n 
~~ ~~ return ( count <= 1 ) \n 
~~ def unique_session_or_json_redirect ( function ) : \n 
def decorator ( request , * args , ** kwargs ) : \n 
~~~ def return_nonunique_session_result ( ) : \n 
~~~ status = { : False } \n 
status [ ] = _ ( \n 
status [ ] = \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ if not using_unique_session ( request . user ) : \n 
~~~ return return_nonunique_session_result ( ) \n 
~~~ return function ( request , * args , ** kwargs ) \n 
~~ ~~ return wraps ( function ) ( decorator ) \n 
~~ def is_session_available ( req ) : \n 
if req . user . is_superuser or req . user . is_staff : \n 
~~ sessions = Session . objects . filter ( expire_date__gt = datetime . now ( ) ) \n 
if ( not req . user . is_anonymous ( ) ) and in decoded and decoded [ ~~~ count += 1 \n 
~~ ~~ avail = count < settings . CONCURRENT_SESSIONS \n 
req . session [ ] = avail \n 
return avail \n 
~~ def note_session_activity ( req ) : \n 
window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n 
req . session [ ] = datetime . now ( ) + window \n 
def unloadplan ( request , planid ) : \n 
note_session_activity ( request ) \n 
status = { : False } \n 
ps = Plan . objects . filter ( pk = planid ) \n 
if len ( ps ) > 0 : \n 
~~~ p = ps [ 0 ] \n 
if not can_copy ( request . user , p ) : \n 
~~ if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
~~ ~~ status [ ] = True \n 
@ unique_session_or_json_redirect \n 
def copyplan ( request , planid ) : \n 
if not is_plan_ready ( planid ) : \n 
~~ status = { : False } \n 
p = Plan . objects . get ( pk = planid ) \n 
if ( request . method == "POST" ) : \n 
~~~ newname = request . POST [ "name" ] [ 0 : 200 ] \n 
shared = request . POST . get ( "shared" , False ) \n 
~~ plan_copy = Plan . objects . filter ( name = newname , owner = request . user , legislative_body = p . legislative_body \n 
if len ( plan_copy ) > 0 : \n 
~~ plan_copy = Plan ( name = newname , owner = request . user , is_shared = shared , legislative_body = p . legislative_body plan_copy . create_unassigned = False \n 
plan_copy . save ( ) \n 
districts = p . get_districts_at_version ( p . version , include_geom = True ) \n 
for district in districts : \n 
~~~ district_copy = copy . copy ( district ) \n 
district_copy . id = None \n 
district_copy . version = 0 \n 
district_copy . is_locked = False \n 
district_copy . plan = plan_copy \n 
~~~ district_copy . save ( ) \n 
~~ except Exception as inst : \n 
status [ "exception" ] = inst . message \n 
~~ district_copy . clone_relations_from ( district ) \n 
~~ data = serializers . serialize ( "json" , [ plan_copy ] ) \n 
return HttpResponse ( data , mimetype = ) \n 
def scoreplan ( request , planid ) : \n 
plan = Plan . objects . get ( pk = planid ) \n 
criterion = ValidationCriteria . objects . filter ( legislative_body = plan . legislative_body ) \n 
status [ ] = True \n 
for criteria in criterion : \n 
~~~ score = ComputedPlanScore . compute ( criteria . function , plan ) \n 
~~~ logger . debug ( traceback . format_exc ( ) ) \n 
~~ if not score or not score [ ] : \n 
~~~ status [ ] = False \n 
status [ ] = % ( criteria . get_short_label ( ) , criteria . get_long_description break \n 
~~ ~~ if status [ ] : \n 
~~~ status [ ] = True \n 
plan . is_valid = True \n 
plan . save ( ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ def get_user_info ( user ) : \n 
if user . is_anonymous ( ) : \n 
~~ profile = user . get_profile ( ) \n 
return { \n 
: user . username , \n 
: user . email , \n 
: profile . pass_hint , \n 
: user . first_name , \n 
: user . last_name , \n 
: profile . organization , \n 
: user . id \n 
~~ def commonplan ( request , planid ) : \n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
plan . edited = getutc ( plan . edited ) \n 
levels = plan . legislative_body . get_geolevels ( ) \n 
districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
editable = can_edit ( request . user , plan ) \n 
default_demo = plan . legislative_body . get_default_subject ( ) \n 
max_dists = plan . legislative_body . max_districts \n 
body_member_short_label = plan . legislative_body . get_short_label ( ) \n 
body_member_long_label = plan . legislative_body . get_label ( ) \n 
body_members = plan . legislative_body . get_members_label ( ) \n 
reporting_template = % plan . legislative_body . name if not plan . is_community ( ) \n 
index = body_member_short_label . find ( ) \n 
if index >= 0 : \n 
~~~ body_member_short_label = body_member_short_label [ 0 : index ] \n 
~~ index = body_member_long_label . find ( ) \n 
~~~ body_member_long_label = body_member_long_label [ 0 : index ] \n 
~~ if not editable and not can_view ( request . user , plan ) : \n 
~~~ plan = { } \n 
tags = [ ] \n 
calculator_reports = [ ] \n 
~~~ tags = Tag . objects . filter ( name__startswith = ) . order_by ( ) . values_list ( , flat tags = map ( lambda x : x [ 5 : ] , tags ) \n 
if settings . REPORTS_ENABLED == : \n 
~~~ report_displays = ScoreDisplay . objects . filter ( name = "%s_reports" % plan . legislative_body if len ( report_displays ) > 0 : \n 
~~~ calculator_reports = map ( lambda p : { \n 
: p . __unicode__ ( ) , \n 
: map ( lambda f : { \n 
: f . get_label ( ) , \n 
: f . id \n 
} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n 
levels = list ( ) \n 
districts = { } \n 
editable = False \n 
default_demo = None \n 
max_dists = 0 \n 
body_member_short_label = \n 
body_member_long_label = _ ( ) + \n 
body_members = _n ( , , 2 ) \n 
reporting_template = None \n 
~~ demos = Subject . objects . all ( ) . order_by ( ) [ 0 : 3 ] \n 
layers = [ ] \n 
snaplayers = [ ] \n 
if len ( levels ) > 0 : \n 
~~~ study_area_extent = list ( levels [ 0 ] . geounit_set . extent ( field_name = ) ) \n 
~~~ for lb in LegislativeBody . objects . all ( ) : \n 
~~~ biglevel = lb . get_geolevels ( ) [ 0 ] \n 
if biglevel . geounit_set . count ( ) > 0 : \n 
~~~ study_area_extent = biglevel . geounit_set . extent ( field_name = ) \n 
~~ ~~ ~~ for level in levels : \n 
~~~ snaplayers . append ( { \n 
: level . id , \n 
: level . name , \n 
: + level . name , \n 
: level . get_long_description ( ) , \n 
: level . min_zoom \n 
~~ default_selected = False \n 
for demo in demos : \n 
~~~ isdefault = str ( ( not default_demo is None ) and ( demo . id == default_demo . id ) ) . lower ( ) \n 
if isdefault == : \n 
~~~ default_selected = True \n 
~~ layers . append ( { \n 
: demo . id , \n 
: demo . get_short_label ( ) , \n 
: demo . name , \n 
: isdefault , \n 
: str ( demo . is_displayed ) . lower ( ) \n 
~~ if default_demo and not default_selected : \n 
~~~ layers . insert ( 0 , { \n 
: default_demo . id , \n 
: default_demo . get_short_label ( ) , \n 
: default_demo . name , \n 
: str ( True ) . lower ( ) , \n 
: str ( default_demo . is_displayed ) . lower ( ) \n 
~~ if in settings . __members__ : \n 
~~~ mapserver_protocol = settings . MAP_SERVER_PROTOCOL \n 
~~~ mapserver_protocol = \n 
~~ short_label = body_member_short_label . strip ( ) . lower ( ) \n 
long_label = body_member_long_label . strip ( ) . lower ( ) \n 
has_regions = Region . objects . all ( ) . count ( ) > 1 \n 
bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n 
l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n 
~~~ loader . get_template ( reporting_template ) \n 
~~~ reporting_template = None \n 
~~ return RequestContext ( request , { \n 
: bodies , \n 
: has_regions , \n 
: l_bodies , \n 
: plan , \n 
: districts , \n 
: settings . MAP_SERVER , \n 
: mapserver_protocol , \n 
: settings . BASE_MAPS , \n 
: settings . MAP_SERVER_NS , \n 
: settings . MAP_SERVER_NSHREF , \n 
: settings . FEATURE_LIMIT , \n 
: settings . ADJACENCY , \n 
: settings . CONVEX_CHOROPLETH , \n 
: layers , \n 
: snaplayers , \n 
: UNASSIGNED_DISTRICT_ID , \n 
: request . user . username != and request . user . username != , \n 
: settings . DEBUG and request . user . is_staff , \n 
: get_user_info ( request . user ) , \n 
: editable , \n 
: max_dists + 1 , \n 
: settings . GA_ACCOUNT , \n 
: settings . GA_DOMAIN , \n 
: short_label , \n 
: long_label , \n 
: body_members , \n 
: reporting_template , \n 
: study_area_extent , \n 
: len ( ScoreDisplay . objects . filter ( is_page = True ) ) > 0 , \n 
: json . dumps ( calculator_reports ) , \n 
: ( in settings . __members__ ) , \n 
: tags , \n 
: Site . objects . get_current ( ) , \n 
: translation . get_language ( ) , \n 
~~ def is_plan_ready ( planid ) : \n 
planid = int ( planid ) \n 
return planid == 0 or len ( Plan . objects . filter ( id = planid , processing_state = ProcessingState . READY ) \n 
~~ @ user_passes_test ( using_unique_session ) \n 
def viewplan ( request , planid ) : \n 
if not is_session_available ( request ) or not is_plan_ready ( planid ) : \n 
~~ if not request . user . is_anonymous ( ) and ( int ( planid ) == 0 ) and ( settings . MAX_UNDOS_AFTER_EDIT > 0 ~~~ for p in Plan . objects . filter ( owner = request . user ) : \n 
~~ ~~ return render_to_response ( , commonplan ( request , planid ) ) \n 
def editplan ( request , planid ) : \n 
if request . user . is_anonymous ( ) or not is_session_available ( request ) or not is_plan_ready ( planid ) ~~~ return HttpResponseRedirect ( ) \n 
~~ cfg = commonplan ( request , planid ) \n 
if cfg [ ] == False : \n 
~~~ return HttpResponseRedirect ( % planid ) \n 
~~ plan = Plan . objects . get ( id = planid , owner = request . user ) \n 
cfg [ ] = len ( cfg [ ] ) > plan . legislative_body . max_districts \n 
cfg [ ] = plan . get_available_districts ( ) \n 
if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ plan . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
~~ return render_to_response ( , cfg ) \n 
def printplan ( request , planid ) : \n 
if not is_session_available ( request ) : \n 
sha = hashlib . sha1 ( ) \n 
sha . update ( str ( planid ) + str ( datetime . now ( ) ) ) \n 
cfg [ ] = % sha . hexdigest ( ) \n 
cfg [ ] = % request . META [ ] \n 
~~~ if not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST : \n 
~~ height = 500 * 2 \n 
if in request . REQUEST : \n 
~~~ height = int ( request . REQUEST [ ] ) * 2 \n 
~~ width = 1024 * 2 \n 
~~~ width = int ( request . REQUEST [ ] ) * 2 \n 
~~ opacity = 0.8 \n 
~~~ opacity = float ( request . REQUEST [ ] ) \n 
~~ full_legend = json . loads ( request . REQUEST [ ] ) \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = Plan . objects . get ( id = int ( request . REQUEST [ ] ) ) \n 
cfg [ ] = datetime . now ( ) \n 
bbox = request . REQUEST [ ] . split ( ) \n 
pt1 = Point ( float ( bbox [ 0 ] ) , float ( bbox [ 1 ] ) , srid = 3785 ) \n 
pt1 . transform ( SpatialReference ( ) ) \n 
ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n 
pt2 = Point ( float ( bbox [ 2 ] ) , float ( bbox [ 3 ] ) , srid = 3785 ) \n 
pt2 . transform ( SpatialReference ( ) ) \n 
ur = ModestMaps . Geo . Location ( pt2 . y , pt2 . x ) \n 
dims = ModestMaps . Core . Point ( width , height ) \n 
provider = ModestMaps . OpenStreetMap . Provider ( ) \n 
basemap = ModestMaps . mapByExtent ( provider , ll , ur , dims ) \n 
fullImg = basemap . draw ( ) \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: 512 , \n 
: 512 \n 
overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n 
maskImg = ImageChops . invert ( overlayImg ) \n 
: request . REQUEST [ ] , \n 
overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n 
fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n 
fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n 
t = loader . get_template ( ) \n 
page = t . render ( DjangoContext ( cfg ) ) \n 
result = StringIO . StringIO ( ) \n 
CreatePDF ( page , result , show_error_as_pdf = True ) \n 
response = HttpResponse ( result . getvalue ( ) , mimetype = ) \n 
response [ ] = \n 
return response \n 
~~ ~~ @ login_required \n 
def createplan ( request ) : \n 
if request . method == "POST" : \n 
~~~ name = request . POST [ ] [ 0 : 200 ] \n 
body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n 
plan = Plan ( name = name , owner = request . user , legislative_body = body , processing_state = ProcessingState try : \n 
~~~ plan . save ( ) \n 
status = serializers . serialize ( "json" , [ plan ] ) \n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ @ unique_session_or_json_redirect \n 
def uploadfile ( request ) : \n 
if request . user . is_anonymous ( ) : \n 
~~ status = commonplan ( request , 0 ) \n 
index_file = request . FILES . get ( , False ) \n 
if not index_file : \n 
return render_to_response ( , status ) \n 
~~~ filename = index_file . name \n 
~~ if index_file . size > settings . MAX_UPLOAD_SIZE : \n 
~~~ logger . error ( ) \n 
status [ ] = False \n 
~~ if not filename . endswith ( ( , ) ) : \n 
~~ elif request . POST [ ] == : \n 
~~~ dest = tempfile . NamedTemporaryFile ( mode = , delete = False ) \n 
for chunk in request . FILES [ ] . chunks ( ) : \n 
~~~ dest . write ( chunk ) \n 
~~ dest . close ( ) \n 
if request . FILES [ ] . name . endswith ( ) : \n 
~~~ os . rename ( dest . name , % ( dest . name , ) ) \n 
filename = % ( dest . name , ) \n 
~~~ filename = dest . name \n 
~~ ~~ except Exception as ex : \n 
logger . error ( , ex ) \n 
~~ DistrictIndexFile . index2plan . delay ( request . POST [ ] , request . POST [ \n 
~~ return render_to_response ( , status ) \n 
~~ def generate_report_hash ( qdict ) : \n 
params = qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) \n 
sha . update ( params ) \n 
return sha . hexdigest ( ) \n 
def getreport ( request , planid ) : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~~ status [ ] = _ ( ) \n 
~~ if not can_view ( request . user , plan ) : \n 
~~ if not settings . REPORTS_ENABLED is None : \n 
~~ if request . method != : \n 
~~ stamp = request . POST . get ( , generate_report_hash ( request . POST ) ) \n 
rptstatus = PlanReport . checkreport ( planid , stamp ) \n 
if rptstatus == : \n 
~~~ status = { \n 
: PlanReport . getreport ( planid , stamp ) , \n 
: _ ( ) , \n 
: stamp \n 
~~ elif rptstatus == : \n 
: reverse ( getreport , args = [ planid ] ) , \n 
: 10 , \n 
req = { \n 
: request . POST . get ( , ) , \n 
: request . POST . getlist ( ) , \n 
: request . POST . get ( , ) \n 
PlanReport . markpending ( planid , stamp ) \n 
PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n 
~~~ status [ ] = _ ( \n 
def getcalculatorreport ( request , planid ) : \n 
~~ function_ids = request . POST . get ( , ) \n 
sha . update ( function_ids ) \n 
stamp = request . POST . get ( , sha . hexdigest ( ) ) \n 
rptstatus = CalculatorReport . checkreport ( planid , stamp ) \n 
: CalculatorReport . getreport ( planid , stamp ) , \n 
: reverse ( getcalculatorreport , args = [ planid ] ) , \n 
: 5 , \n 
req = { : function_ids } \n 
CalculatorReport . markpending ( planid , stamp ) \n 
CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n 
def newdistrict ( request , planid ) : \n 
if len ( request . REQUEST . items ( ) ) >= 3 : \n 
~~~ plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
~~~ geolevel = request . REQUEST [ ] \n 
~~~ geolevel = None \n 
~~ if in request . REQUEST : \n 
~~~ geounit_ids = string . split ( request . REQUEST [ ] , ) \n 
~~~ geounit_ids = None \n 
~~~ district_id = int ( request . REQUEST [ ] ) \n 
~~~ district_id = None \n 
~~~ district_short = request . REQUEST [ ] [ 0 : 10 ] \n 
~~ elif not district_id is None : \n 
~~~ district_short = plan . legislative_body . get_short_label ( ) % { : district_id } \n 
~~~ district_short = None \n 
~~~ district_long = request . REQUEST [ ] [ 0 : 256 ] \n 
~~~ district_long = plan . legislative_body . get_label ( ) % { : district_id } \n 
~~~ district_long = None \n 
~~~ version = request . REQUEST [ ] \n 
~~~ version = plan . version \n 
~~ if geolevel and geounit_ids and district_id : \n 
~~~ fixed = plan . add_geounits ( ( district_id , district_short , district_long , ) , geounit_ids \n 
district = plan . district_set . filter ( district_id = district_id , short_label = district_short if plan . legislative_body . multi_members_allowed : \n 
~~~ district . num_members = plan . legislative_body . min_multi_district_members \n 
district . save ( ) \n 
~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
if in request . POST and request . POST [ ] != : \n 
~~~ comment = Comment ( \n 
object_pk = district . id , \n 
content_type = ct , \n 
site_id = Site . objects . get_current ( ) . id , \n 
user_name = request . user . username , \n 
user_email = request . user . email , \n 
comment = request . POST [ ] ) \n 
comment . save ( ) \n 
~~ if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~~ strtags = request . REQUEST . getlist ( ) \n 
for strtag in strtags : \n 
~~~ if strtag == : \n 
~~ if strtag . count ( ) > 0 : \n 
~~~ strtag = \'"type=%s"\' % strtag \n 
~~~ strtag = % strtag \n 
~~ Tag . objects . add_tag ( district , strtag ) \n 
status [ ] = _ ( ) \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
status [ ] = getutc ( plan . edited ) . isoformat ( ) \n 
status [ ] = district_id \n 
status [ ] = plan . version \n 
~~ except ValidationError : \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
@ transaction . commit_manually \n 
def add_districts_to_plan ( request , planid ) : \n 
~~ if not can_edit ( request . user , plan ) : \n 
~~ district_list = request . POST . getlist ( ) \n 
if len ( district_list ) == 0 : \n 
~~~ districts = District . objects . filter ( id__in = district_list ) \n 
version = int ( request . POST . get ( , None ) ) \n 
status [ ] = _ ( ) % { : len ( districts ) } \n 
~~ allowed_districts = plan . get_available_districts ( version = version ) \n 
if len ( districts ) > allowed_districts : \n 
~~~ status [ ] = _ ( ) % { : allowed_districts } \n 
~~~ results = plan . paste_districts ( districts , version = version ) \n 
transaction . commit ( ) \n 
status [ ] = _ ( ) % { status [ ] = plan . version \n 
~~ except Exception as ex : \n 
~~~ transaction . rollback ( ) \n 
status [ ] = str ( ex ) \n 
status [ ] = traceback . format_exc ( ) \n 
def assign_district_members ( request , planid ) : \n 
~~ leg_bod = plan . legislative_body \n 
if ( not leg_bod . multi_members_allowed ) : \n 
~~ districts = request . POST . getlist ( ) \n 
counts = request . POST . getlist ( ) \n 
~~~ changed = 0 \n 
for i in range ( 0 , len ( districts ) ) : \n 
~~~ id = int ( districts [ i ] ) \n 
count = int ( counts [ i ] ) \n 
district = District . objects . filter ( plan = plan , district_id = id , version__lte = version ) . order_by \n 
if district . num_members != count : \n 
~~~ if ( changed == 0 ) : \n 
~~~ if version != plan . version : \n 
~~~ plan . purge ( after = version ) \n 
~~ plan . version = plan . version + 1 \n 
~~ plan . update_num_members ( district , count ) \n 
changed += 1 \n 
~~ ~~ transaction . commit ( ) \n 
status [ ] = changed \n 
) % { : changed } \n 
logger . warn ( ) \n 
def combine_districts ( request , planid ) : \n 
~~ version = int ( request . POST . get ( , plan . version ) ) \n 
from_id = int ( request . POST . get ( , - 1 ) ) \n 
to_id = int ( request . POST . get ( , None ) ) \n 
~~~ all_districts = plan . get_districts_at_version ( version , include_geom = True ) \n 
from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n 
locked = to_district . is_locked \n 
for district in from_districts : \n 
~~~ if district . is_locked : \n 
~~~ locked = True \n 
~~ ~~ if locked : \n 
~~ result = plan . combine_districts ( to_district , from_districts , version = version ) \n 
if result [ 0 ] == True : \n 
status [ ] = result [ 1 ] \n 
~~ ~~ except Exception , ex : \n 
def fix_unassigned ( request , planid ) : \n 
~~~ version = int ( request . POST . get ( , plan . version ) ) \n 
result = plan . fix_unassigned ( version ) \n 
status [ ] = result [ 0 ] \n 
def get_splits ( request , planid , otherid , othertype ) : \n 
otherid = int ( otherid ) \n 
~~ version = int ( request . REQUEST [ ] if in request . REQUEST else plan . version ) \n 
~~~ if othertype == : \n 
~~~ otherplan = Plan . objects . get ( pk = otherid ) \n 
~~ if not can_view ( request . user , otherplan ) : \n 
~~ otherversion = int ( request . REQUEST [ ] if in request . REQUEST splits = plan . find_plan_splits ( otherplan , version , otherversion ) \n 
~~ elif othertype == : \n 
~~~ splits = plan . find_geolevel_splits ( otherid , version ) \n 
~~~ status [ ] = _ ( ) % { : othertype } \n 
~~ split_word = _ ( ) if len ( splits ) == 1 else inflect . engine ( ) . plural ( _ ( ) ) \n 
status [ ] = _ ( ) % { : len ( splits ) , : split_word } \n 
status [ ] = splits \n 
status [ ] = list ( set ( [ i [ 0 ] for i in splits ] ) ) \n 
status [ ] = list ( set ( [ i [ 1 ] for i in splits ] ) ) \n 
~~ def get_processing_status ( request ) : \n 
plan_ids = request . REQUEST . getlist ( ) \n 
if len ( plan_ids ) == 0 : \n 
~~~ statuses = { } \n 
for p in Plan . objects . filter ( id__in = plan_ids ) : \n 
~~~ statuses [ str ( p . id ) ] = p . get_processing_state_display ( ) \n 
~~ status [ ] = True \n 
status [ ] = statuses \n 
~~ def get_splits_report ( request , planid ) : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
~~ if not using_unique_session ( request . user ) or not can_view ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
inverse = request . REQUEST [ ] == if in request . REQUEST else False \n 
extended = request . REQUEST [ ] == if in request . REQUEST else False \n 
layers = request . REQUEST . getlist ( ) \n 
if len ( layers ) == 0 : \n 
~~~ report = loader . get_template ( ) \n 
html = \n 
for layer in layers : \n 
~~~ my_context = { : extended } \n 
my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n 
community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n 
~~~ my_context . update ( community_info ) \n 
~~ calc_context = DjangoContext ( my_context ) \n 
html += report . render ( calc_context ) \n 
if not last_item : \n 
~~~ html += \n 
~~ ~~ return HttpResponse ( html , mimetype = ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
def addtodistrict ( request , planid , districtid ) : \n 
if len ( request . REQUEST . items ( ) ) >= 2 : \n 
~~~ geolevel = request . REQUEST [ "geolevel" ] \n 
geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n 
~~~ status [ ] = traceback . format_exc ( ) \n 
~~~ fixed = plan . add_geounits ( districtid , geounit_ids , geolevel , version ) \n 
status [ ] = True ; \n 
status [ ] = _ ( ) % { : fixed } \n 
status [ ] = fixed \n 
def setdistrictlock ( request , planid , district_id ) : \n 
if request . method != : \n 
~~ lock = request . POST . get ( ) . lower ( ) == \n 
version = request . POST . get ( ) \n 
if lock == None : \n 
~~ elif version == None : \n 
district = plan . district_set . filter ( district_id = district_id , version__lte = version ) . order_by ( ~~ except ObjectDoesNotExist : \n 
~~ if plan . owner != request . user : \n 
~~ district . is_locked = lock \n 
status [ ] = _ ( ) % { : _ ( ) if lock else _ ( ) } \n 
def getdistricts ( request , planid ) : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
~~ districts = plan . get_districts_at_version ( version , include_geom = False ) \n 
status [ ] = [ ] \n 
status [ ] = plan . legislative_body . max_districts - len ( districts ) + 1 \n 
max_version = max ( [ d . version for d in districts ] ) \n 
can_undo = max_version > plan . min_version \n 
~~~ status [ ] . append ( { \n 
: district . district_id , \n 
: . join ( map ( _ , district . short_label . split ( ) ) ) , \n 
: . join ( map ( _ , district . long_label . split ( ) ) ) , \n 
: district . version \n 
~~ status [ ] = can_undo \n 
~~ def simple_district_versioned ( request , planid , district_ids = None ) : \n 
status = { : } \n 
~~ subject_id = None \n 
~~~ subject_id = request . REQUEST [ ] \n 
~~ elif plan . legislative_body . get_default_subject ( ) : \n 
~~~ subject_id = plan . legislative_body . get_default_subject ( ) . id \n 
~~ geolevel = plan . legislative_body . get_geolevels ( ) [ 0 ] . id \n 
~~~ geolevel = int ( request . REQUEST [ ] ) \n 
~~~ district_ids = request . REQUEST [ ] \n 
if len ( district_ids ) > 0 : \n 
~~~ district_ids = district_ids . split ( ) \n 
~~~ district_ids = [ ] \n 
~~ ~~ if subject_id : \n 
~~~ bbox = None \n 
~~~ bbox = request . REQUEST [ ] \n 
bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n 
~~~ bbox = plan . district_set . all ( ) . extent ( field_name = ) \n 
~~ status [ ] = plan . get_wfs_districts ( version , subject_id , bbox , geolevel , district_ids ~~ else : \n 
~~~ status [ ] = [ ] \n 
~~ def get_unlocked_simple_geometries ( request , planid ) : \n 
version = request . POST . get ( , plan . version ) \n 
geolevel = request . POST . get ( , plan . legislative_body . get_geolevels ( ) [ 0 ] . id ) \n 
geom = request . POST . get ( , None ) \n 
if geom is not None : \n 
~~~ wkt = request . POST . get ( , None ) \n 
geom = GEOSGeometry ( wkt ) \n 
~~ except GEOSException : \n 
~~~ wkt = request . REQUEST [ ] . replace ( , ) \n 
wkt = wkt . replace ( , ) . replace ( , ) \n 
~~~ geom = GEOSGeometry ( wkt ) \n 
~~~ geom = None \n 
~~ ~~ selection = Q ( geom__intersects = geom ) \n 
districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n 
locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n 
filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n 
features = [ ] \n 
for feature in filtered : \n 
~~~ geom = feature . simple \n 
if locked and geom . intersects ( locked_buffered ) : \n 
~~~ if feature . geom . within ( locked ) : \n 
~~ if feature . geom . overlaps ( locked ) : \n 
~~ ~~ features . append ( { \n 
: % feature . id , \n 
: json . loads ( geom . json ) , \n 
: feature . name , \n 
: geolevel , \n 
: feature . id \n 
~~ status [ ] = features \n 
def get_statistics ( request , planid ) : \n 
~~~ note_session_activity ( request ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = , status = 500 ) \n 
~~~ display = ScoreDisplay . objects . get ( legislative_body = plan . legislative_body , name = "%s_sidebar_demo" ~~ except : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . POST [ ] ) \n 
logger . warn ( str ( request . POST ) ) \n 
~~~ html = display . render ( plan , request , version = version ) \n 
return HttpResponse ( html , mimetype = ) \n 
~~ ~~ def getutc ( t ) : \n 
t_tuple = t . timetuple ( ) \n 
t_seconds = time . mktime ( t_tuple ) \n 
return t . utcfromtimestamp ( t_seconds ) \n 
def getdistrictfilestatus ( request , planid ) : \n 
if not can_copy ( request . user , plan ) : \n 
~~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n 
status [ ] = file_status \n 
status [ ] = ex \n 
def getdistrictfile ( request , planid ) : \n 
~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
if file_status == : \n 
~~~ if is_shape : \n 
~~~ archive = DistrictShapeFile . plan2shape ( plan ) \n 
~~~ archive = DistrictIndexFile . plan2index ( plan ) \n 
~~ response = HttpResponse ( open ( archive . name ) . read ( ) , content_type = ) \n 
~~~ DistrictShapeFile . plan2shape . delay ( plan ) \n 
~~~ DistrictIndexFile . plan2index . delay ( plan ) \n 
~~ response = HttpResponse ( _ ( \n 
def emaildistrictindexfile ( request , planid ) : \n 
~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ DistrictIndexFile . emailfile . delay ( plan , request . user , request . POST , translation . get_language ( ) ) \n 
return HttpResponse ( json . dumps ( { \n 
: _ ( ) } ) , \n 
mimetype = ) \n 
~~ def getvalidplans ( leg_body , owner = None ) : \n 
pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n 
if owner is not None : \n 
~~~ pfilter = pfilter & Q ( owner = owner ) \n 
~~ return list ( Plan . objects . filter ( pfilter ) ) \n 
~~ def getleaderboarddisplay ( leg_body , owner_filter ) : \n 
~~~ return ScoreDisplay . objects . get ( name = "%s_leader_%s" % ( leg_body . name , owner_filter ) ) \n 
~~ ~~ def getleaderboard ( request ) : \n 
if not using_unique_session ( request . user ) : \n 
~~ owner_filter = request . REQUEST [ ] \n 
body_pk = int ( request . REQUEST [ ] ) ; \n 
leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n 
display = getleaderboarddisplay ( leg_body , owner_filter ) \n 
if display is None : \n 
~~ plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
~~~ html = display . render ( plans , request ) \n 
~~ ~~ def getleaderboardcsv ( request ) : \n 
plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
panels = display . scorepanel_set . all ( ) . order_by ( ) \n 
~~~ response = HttpResponse ( mimetype = ) \n 
writer = csv . writer ( response ) \n 
writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n 
for plan in plans : \n 
~~~ row = [ plan . id , plan . name , plan . owner . username ] \n 
for panel in panels : \n 
~~~ function = panel . score_functions . all ( ) [ 0 ] \n 
score = ComputedPlanScore . compute ( function , plan ) \n 
row . append ( score [ ] ) \n 
~~ writer . writerow ( row ) \n 
~~ ~~ def getplans ( request ) : \n 
~~ if request . method == : \n 
~~~ page = int ( request . POST . get ( , 1 ) ) \n 
rows = int ( request . POST . get ( , 10 ) ) \n 
sidx = request . POST . get ( , ) \n 
sord = request . POST . get ( , ) \n 
owner_filter = request . POST . get ( ) ; \n 
body_pk = request . POST . get ( ) ; \n 
body_pk = int ( body_pk ) if body_pk else body_pk ; \n 
search = request . POST . get ( , False ) ; \n 
search_string = request . POST . get ( , ) ; \n 
is_community = request . POST . get ( , False ) == ; \n 
~~ end = page * rows \n 
start = end - rows \n 
if owner_filter == : \n 
~~~ available = Q ( is_template = True ) \n 
~~ elif owner_filter == : \n 
~~~ available = Q ( is_shared = True ) \n 
~~~ if request . user . is_anonymous ( ) : \n 
~~~ available = Q ( owner__exact = request . user ) \n 
~~ ~~ elif owner_filter == : \n 
~~~ available = Q ( is_template = True ) | Q ( is_shared = True ) \n 
if not request . user . is_anonymous ( ) : \n 
~~~ available = available | Q ( owner__exact = request . user ) \n 
~~ not_creating = ~ Q ( processing_state = ProcessingState . CREATING ) & ~ Q ( processing_state = ProcessingState \n 
if sidx . startswith ( ) : \n 
~~~ sidx = sidx [ len ( ) : ] \n 
~~ if sidx == : \n 
~~~ sidx = \n 
~~ if sord == : \n 
~~~ sidx = + sidx \n 
~~ if search : \n 
~~~ search_filter = Q ( name__icontains = search_string ) | Q ( description__icontains = search_string ~~ else : \n 
~~~ search_filter = None \n 
~~ if body_pk : \n 
~~~ body_filter = Q ( legislative_body = body_pk ) \n 
all_plans = Plan . objects . filter ( available , not_creating , body_filter , search_filter ) . order_by ~~ else : \n 
~~~ community_filter = Q ( legislative_body__is_community = is_community ) \n 
all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n 
~~ if all_plans . count ( ) > 0 : \n 
~~~ total_pages = math . ceil ( all_plans . count ( ) / float ( rows ) ) \n 
~~~ total_pages = 1 \n 
~~ plans = all_plans [ start : end ] \n 
plans_list = list ( ) \n 
~~~ plans_list . append ( { \n 
: plan . id , \n 
: plan . name , \n 
: plan . description , \n 
: time . mktime ( plan . edited . timetuple ( ) ) , \n 
: plan . is_template , \n 
: plan . is_shared , \n 
: plan . owner . username , \n 
: can_edit ( request . user , plan ) , \n 
: plan . legislative_body . get_long_description ( ) , \n 
: plan . get_processing_state_display ( ) \n 
~~ def get_shared_districts ( request , planid ) : \n 
~~ all_districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
~~~ plan = None \n 
all_districts = ( ) \n 
~~ if len ( all_districts ) > 0 : \n 
~~~ total_pages = math . ceil ( len ( all_districts ) / float ( rows ) ) \n 
~~ districts = all_districts [ start : end ] \n 
districts_list = list ( ) \n 
~~~ if not district . is_unassigned : \n 
~~~ districts_list . append ( { \n 
: district . id , \n 
: district . short_label , \n 
: district . long_label , \n 
def editplanattributes ( request , planid ) : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
~~ new_name = request . POST . get ( , None ) \n 
new_description = request . POST . get ( , ) \n 
if not planid or not ( new_name or new_description ) : \n 
~~~ return HttpResponseBadRequest ( \n 
_ ( ) ) \n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if not new_name is None : \n 
~~~ plan . name = new_name \n 
~~ plan . description = new_description \n 
def deleteplan ( request , planid ) : \n 
~~ if not planid : \n 
~~~ return HttpResponseBadRequest ( _ ( ) ) \n 
~~~ plan . delete ( ) \n 
def reaggregateplan ( request , planid ) : \n 
~~~ reaggregate_plan . delay ( plan . id ) \n 
plan . processing_state = ProcessingState . REAGGREGATING \n 
~~ def get_health ( request ) : \n 
~~~ def num_users ( minutes ) : \n 
~~~ users = 0 \n 
for session in Session . objects . all ( ) : \n 
~~~ session . delete ( ) \n 
~~ if in decoded : \n 
~~~ activity_delta = decoded [ ] - timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT if activity_delta > ( datetime . now ( ) - timedelta ( 0 , 0 , 0 , 0 , minutes ) ) : \n 
~~~ users += 1 \n 
~~ ~~ ~~ return users \n 
~~~ result = _ ( ) % { : datetime . now ( ) } \n 
result += _ ( ) % { : Plan . objects . all ( ) . count ( ) } \n 
result += _ ( ) % { : Session . objects . all ( ) . count ( ) , \n 
: settings . CONCURRENT_SESSIONS } \n 
result += _ ( ) % { : num_users ( 10 ) } \n 
space = os . statvfs ( ) \n 
result += _ ( ) % { : ( ( space . f_bsize * space . f_bavail ) / ( 1024 * 1024 ) ) } \n 
result += _ ( ) % { : commands . getoutput ( ) } \n 
return HttpResponse ( result , mimetype = ) \n 
~~ ~~ def statistics_sets ( request , planid ) : \n 
~~~ result = { : False } \n 
if plan . count ( ) == 0 : \n 
~~~ result [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~~ sets = [ ] \n 
scorefunctions = [ ] \n 
user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n 
~~~ if not in f . name . lower ( ) and not in f . name . lower ( ) : \n 
~~~ scorefunctions . append ( { : f . id , : force_escape ( f . get_label ( ) ) } ) \n 
~~ ~~ result [ ] = scorefunctions \n 
admin_display_names = [ \n 
"%s_sidebar_demo" % plan . legislative_body . name , \n 
if plan . legislative_body . is_community : \n 
~~~ admin_display_names . append ( "%s_sidebar_comments" % \n 
plan . legislative_body . name ) \n 
~~~ admin_display_names . append ( "%s_sidebar_basic" % \n 
~~ admin_displays = ScoreDisplay . objects . filter ( \n 
owner__is_superuser = True , \n 
legislative_body = plan . legislative_body , \n 
name__in = admin_display_names \n 
for admin_display in admin_displays : \n 
~~~ sets . append ( { \n 
: admin_display . id , \n 
: force_escape ( admin_display . get_label ( ) ) , \n 
: [ ] , \n 
: False \n 
~~~ user_displays = ScoreDisplay . objects . filter ( \n 
owner = request . user , \n 
is_page = False ) . order_by ( ) \n 
result [ ] = len ( user_displays ) \n 
for display in user_displays : \n 
~~~ functions = [ ] \n 
for panel in display . scorepanel_set . all ( ) : \n 
~~~ if panel . type == : \n 
~~~ functions = map ( lambda x : x . id , panel . score_functions . all ( ) ) \n 
if len ( functions ) == 0 : \n 
~~ ~~ ~~ sets . append ( { : display . id , : force_escape ( display . __unicode__ ( ) ) , ~~ ~~ except Exception , ex : \n 
~~~ result [ ] = _ ( ) % { : request . user } \n 
~~ result [ ] = sets \n 
result [ ] = True \n 
~~ elif request . method == and in request . POST : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . REQUEST . get ( , - 1 ) ) \n 
result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id } \n 
qset = display . scorepanel_set . all ( ) \n 
for panel in qset : \n 
~~~ if panel . displays . count ( ) == 1 : \n 
~~~ panel . delete ( ) \n 
~~ ~~ display . delete ( ) \n 
result [ ] = traceback . format_exc ( ) \n 
~~ ~~ elif request . method == : \n 
~~~ def validate_num ( user , limit = 3 ) : \n 
~~~ return ScoreDisplay . objects . filter ( owner = user , legislative_body = plan . legislative_body , is_page \n 
~~ if in request . POST : \n 
~~~ functions = request . POST . getlist ( ) \n 
functions = map ( lambda x : int ( x ) , functions ) \n 
~~~ display = ScoreDisplay . objects . get ( title = request . POST . get ( ) , owner = request . user display = display . copy_from ( display = display , functions = functions ) \n 
~~~ limit = 3 \n 
if validate_num ( request . user , limit ) : \n 
~~~ demo = ScoreDisplay . objects . filter ( \n 
is_page = False , \n 
title = "Demographics" \n 
for disp in demo : \n 
~~~ has_comments = False \n 
for pnl in disp . scorepanel_set . all ( ) : \n 
~~~ for fn in pnl . score_functions . all ( ) : \n 
~~~ has_comments = has_comments or fn . calculator . endswith ( ) \n 
~~ ~~ if not has_comments : \n 
~~~ demo = disp \n 
~~ ~~ display = ScoreDisplay ( ) \n 
display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n 
~~~ result [ ] = _ ( \n 
) % { : limit } \n 
result [ ] = \n 
~~ ~~ result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id , result [ ] = True \n 
~~ ~~ return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~ def purge_plan_clear_cache ( district , version ) : \n 
district . plan . purge ( after = version ) \n 
district . plan . version = version \n 
district . plan . save ( ) \n 
cache = district . computeddistrictscore_set . filter ( function__calculator__endswith = ) \n 
cache . delete ( ) \n 
def district_info ( request , planid , district_id ) : \n 
version = plan . version \n 
version = min ( plan . version , int ( version ) ) \n 
~~ ~~ district_id = int ( district_id ) \n 
district = plan . get_districts_at_version ( version , include_geom = False ) \n 
district = filter ( lambda d : d . district_id == district_id , district ) \n 
~~~ district = plan . district_set . get ( id = request . POST [ ] ) \n 
district . short_label = request . POST [ ] [ 0 : 10 ] \n 
district . long_label = request . POST [ ] [ 0 : 256 ] \n 
if district . version < version : \n 
district_copy . version = version \n 
district_copy . save ( ) \n 
district_copy . clone_relations_from ( district ) \n 
district = district_copy \n 
~~~ district . save ( ) \n 
~~ has_comment = in request . POST and request . POST [ ] != \n 
if has_comment : \n 
~~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n 
comment = Comment ( \n 
~~ tset = Tag . objects . get_for_object ( district ) . filter ( name__startswith = ) \n 
TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n 
purge_plan_clear_cache ( district , version ) \n 
if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~ ~~ status [ ] = version \n 
~~ def plan_feed ( request ) : \n 
~~~ feed = loader . get_template ( ) \n 
plans = Plan . objects . all ( ) . order_by ( ) [ 0 : 10 ] \n 
geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
extent = geolevel . geounit_set . collect ( ) . extent \n 
if extent [ 2 ] - extent [ 0 ] > extent [ 3 ] - extent [ 1 ] : \n 
~~~ width = 500 \n 
height = int ( 500 * ( extent [ 3 ] - extent [ 1 ] ) / ( extent [ 2 ] - extent [ 0 ] ) ) \n 
~~~ width = int ( 500 * ( extent [ 2 ] - extent [ 0 ] ) / ( extent [ 3 ] - extent [ 1 ] ) ) \n 
height = 500 \n 
~~ mapserver = settings . MAP_SERVER if settings . MAP_SERVER != else request . META [ ] \n 
context = { \n 
: plans , \n 
: mapserver , \n 
: extent , \n 
: width , \n 
: height \n 
xml = feed . render ( DjangoContext ( context ) ) \n 
return HttpResponse ( xml , mimetype = ) \n 
~~ def share_feed ( request ) : \n 
plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n 
if plans . count ( ) < 0 : \n 
~~~ geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
~~~ extent = ( 0 , 0 , 0 , 0 , ) \n 
width = 1 \n 
height = 1 \n 
#------------------------------------------------------------------------------- \n 
~~ DSIZE = 4 \n 
a_offset = 1 * 1024 * 1024 \n 
b_offset = 2 * 1024 * 1024 \n 
iochannel = CoramIoChannel ( idx = 0 , datawidth = 32 ) \n 
channel = CoramChannel ( idx = 0 , datawidth = 8 * DSIZE ) \n 
def st_set_mesh_size ( mesh_size ) : \n 
~~~ channel . write ( mesh_size ) \n 
~~ def st_step ( mesh_size , read_start , write_start ) : \n 
~~~ read_page = 3 \n 
write_page = 0 \n 
read_addr = read_start \n 
mem0 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
mem1 . write ( 0 , read_addr , mesh_size ) \n 
mem2 . write ( 0 , read_addr , mesh_size ) \n 
write_addr = write_start + mesh_size * DSIZE + DSIZE \n 
for i in range ( mesh_size - 2 ) : \n 
~~~ hot_spot = 1 if i == 0 else 0 \n 
pos = ( ( hot_spot << 6 ) | \n 
( ( 0x1 << write_page ) << 4 ) | \n 
( 0x1 << read_page ) ) \n 
mem0 . wait ( ) \n 
mem1 . wait ( ) \n 
mem2 . wait ( ) \n 
mem3 . wait ( ) \n 
channel . write ( pos ) \n 
if read_page == 0 : \n 
~~~ mem0 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 1 : \n 
~~~ mem1 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 2 : \n 
~~~ mem2 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 3 : \n 
~~~ mem3 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ read_page = 0 if read_page == 3 else read_page + 1 \n 
channel . read ( ) \n 
mem_d0 . wait ( ) \n 
mem_d1 . wait ( ) \n 
if write_page == 0 : \n 
~~~ mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ elif write_page == 1 : \n 
~~~ mem_d1 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ write_addr += mesh_size * DSIZE \n 
write_page = 0 if write_page == 1 else write_page + 1 \n 
~~ mem_d0 . wait ( ) \n 
~~ def st_computation ( num_iter , mesh_size ) : \n 
~~~ for i in range ( num_iter / 2 ) : \n 
~~~ st_step ( mesh_size , a_offset , b_offset ) \n 
st_step ( mesh_size , b_offset , a_offset ) \n 
~~ ~~ def st_sum ( mesh_size ) : \n 
~~~ check_sum = 0 \n 
read_addr = a_offset \n 
for i in range ( mesh_size ) : \n 
~~~ mem0 . write ( 0 , read_addr , mesh_size ) \n 
init_sum = 1 if i == 0 else 0 \n 
calc_sum = 1 \n 
pos = ( init_sum << 8 ) | ( calc_sum << 7 ) \n 
check_sum = channel . read ( ) \n 
return check_sum \n 
~~ def st_main ( ) : \n 
~~~ global a_offset \n 
global b_offset \n 
mesh_size = iochannel . read ( ) \n 
num_iter = iochannel . read ( ) \n 
a_offset = iochannel . read ( ) \n 
b_offset = iochannel . read ( ) \n 
st_set_mesh_size ( mesh_size ) \n 
st_computation ( num_iter , mesh_size ) \n 
check_sum = st_sum ( mesh_size ) \n 
iochannel . write ( check_sum ) \n 
~~ while True : \n 
~~~ st_main ( ) \n 
~~~ read_page = 0 \n 
pos = hot_spot \n 
~~ read_page = 0 if read_page == 2 else read_page + 1 \n 
mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
write_addr += mesh_size * DSIZE \n 
~~ ~~ def st_computation ( num_iter , mesh_size ) : \n 
pos = ( init_sum << 2 ) | ( calc_sum << 1 ) \n 
~~ from __future__ import absolute_import \n 
def getRamId ( oid , sid ) : \n 
~~~ if 0 <= sid and sid <= 31 : \n 
~~ if 32 <= sid and sid <= 63 : \n 
~~~ return 1 \n 
~~ if 64 <= sid and sid <= 95 : \n 
~~~ return 2 \n 
~~ if 96 <= sid and sid <= 127 : \n 
~~~ return 3 \n 
~~ ~~ def getRamSubId ( oid , sid ) : \n 
~~~ return sid \n 
~~~ return sid - 32 \n 
~~~ return sid - 64 \n 
~~~ return sid - 96 \n 
~~ ~~ def getChannelId ( oid , sid ) : \n 
~~~ return oid \n 
~~ def getChannelSubId ( oid , sid ) : \n 
~~ def getRegisterId ( oid , sid ) : \n 
~~ def getRegisterSubId ( oid , sid ) : \n 
~~~ f = open ( sys . argv [ 1 ] , ) \n 
lines = f . readlines ( ) \n 
output = [ ] \n 
p_thread = re . compile ( ) \n 
p_thread_id = re . compile ( ) \n 
p_object_id = re . compile ( ) \n 
p_width = re . compile ( ) \n 
p_depth = re . compile ( ) \n 
p_indexwidth = re . compile ( ) \n 
p_logdepth = re . compile ( ) \n 
p_sub_id = re . compile ( ) \n 
module_name = None \n 
thread_name = None \n 
thread_id = None \n 
object_id = None \n 
sub_id = None \n 
width = None \n 
indexwidth = None \n 
depth = None \n 
mode = False \n 
sub_id_num = None \n 
sub_id_base = None \n 
buffer = [ ] \n 
for line in lines : \n 
~~~ if not mode : \n 
~~~ m = p_thread . match ( line ) \n 
if m : \n 
~~~ thread_name = re . match ( \'.*(".*").*\' , m . group ( 2 ) ) . group ( 1 ) \n 
module_name = re . search ( , line ) . group ( 1 ) \n 
mode = True \n 
buffer . append ( line ) \n 
~~~ m = p_thread_id . match ( line ) \n 
~~~ tid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
thread_id = re . match ( , tid_str ) . group ( 2 ) \n 
~~ m = p_object_id . match ( line ) \n 
~~~ oid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
object_id = re . match ( , oid_str ) . group ( 2 ) \n 
~~ m = p_width . match ( line ) \n 
~~~ width_str = m . group ( 2 ) \n 
width = re . match ( , width_str ) . group ( 1 ) \n 
~~ m = p_depth . match ( line ) \n 
~~~ depth_str = m . group ( 2 ) \n 
depth = re . match ( , depth_str ) . group ( 1 ) \n 
~~ m = p_indexwidth . match ( line ) \n 
~~~ indexwidth_str = m . group ( 2 ) \n 
indexwidth = re . match ( , indexwidth_str ) . group ( 1 ) \n 
~~ m = p_logdepth . match ( line ) \n 
~~~ logdepth_str = m . group ( 2 ) \n 
logdepth = re . match ( , logdepth_str ) . group ( 1 ) \n 
~~ m = p_sub_id . match ( line ) \n 
~~~ sid_str = m . group ( 2 ) \n 
sub_id_m = re . search ( , sid_str ) \n 
sub_id = sub_id_m . group ( 0 ) \n 
sub_id_num = sub_id_m . group ( 2 ) \n 
sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n 
16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n 
2 if sub_id_m . group ( 1 ) . count ( "\'b" ) > 0 else \n 
10 ) \n 
~~ ~~ if mode : \n 
if module_name . count ( ) > 0 : \n 
~~ if module_name . count ( ) > 0 : \n 
print ( . join ( buffer [ 1 : ] ) ) \n 
~~ mode = False \n 
print ( line , end = ) \n 
~~ ~~ main ( ) \n 
from optparse import OptionParser \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import pyverilog . utils . version \n 
from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n 
VERSION = pyverilog . utils . version . VERSION \n 
def showVersion ( ) : \n 
~~~ print ( INFO ) \n 
print ( VERSION ) \n 
print ( USAGE ) \n 
sys . exit ( ) \n 
~~ optparser = OptionParser ( ) \n 
optparser . add_option ( "-v" , "--version" , action = "store_true" , dest = "showversion" , \n 
optparser . add_option ( "-I" , "--include" , dest = "include" , action = "append" , \n 
optparser . add_option ( "-D" , dest = "define" , action = "append" , \n 
optparser . add_option ( "-t" , "--top" , dest = "topmodule" , \n 
optparser . add_option ( "--nobind" , action = "store_true" , dest = "nobind" , \n 
optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n 
( options , args ) = optparser . parse_args ( ) \n 
filelist = args \n 
if options . showversion : \n 
~~~ showVersion ( ) \n 
~~ for f in filelist : \n 
~~ if len ( filelist ) == 0 : \n 
~~ analyzer = VerilogDataflowAnalyzer ( filelist , options . topmodule , \n 
noreorder = options . noreorder , \n 
nobind = options . nobind , \n 
preprocess_include = options . include , \n 
preprocess_define = options . define ) \n 
analyzer . generate ( ) \n 
directives = analyzer . get_directives ( ) \n 
for dr in sorted ( directives , key = lambda x : str ( x ) ) : \n 
~~~ print ( dr ) \n 
~~ instances = analyzer . getInstances ( ) \n 
for module , instname in sorted ( instances , key = lambda x : str ( x [ 1 ] ) ) : \n 
~~~ print ( ( module , instname ) ) \n 
~~ if options . nobind : \n 
signals = analyzer . getSignals ( ) \n 
for sig in signals : \n 
~~~ print ( sig ) \n 
consts = analyzer . getConsts ( ) \n 
for con in consts : \n 
~~~ print ( con ) \n 
~~~ terms = analyzer . getTerms ( ) \n 
for tk , tv in sorted ( terms . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ print ( tv . tostr ( ) ) \n 
~~ binddict = analyzer . getBinddict ( ) \n 
for bk , bv in sorted ( binddict . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ for bvi in bv : \n 
~~~ print ( bvi . tostr ( ) ) \n 
~~ ~~ ~~ ~~ if __name__ == : \n 
from pyverilog . dataflow . dataflow import * \n 
def replaceUndefined ( tree , termname ) : \n 
~~~ if tree is None : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFUndefined ) : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFConstant ) : return tree \n 
if isinstance ( tree , DFEvalValue ) : return tree \n 
if isinstance ( tree , DFTerminal ) : return tree \n 
if isinstance ( tree , DFBranch ) : \n 
~~~ condnode = replaceUndefined ( tree . condnode , termname ) \n 
truenode = replaceUndefined ( tree . truenode , termname ) \n 
falsenode = replaceUndefined ( tree . falsenode , termname ) \n 
return DFBranch ( condnode , truenode , falsenode ) \n 
~~ if isinstance ( tree , DFOperator ) : \n 
~~~ nextnodes = [ ] \n 
for n in tree . nextnodes : \n 
~~~ nextnodes . append ( replaceUndefined ( n , termname ) ) \n 
~~ return DFOperator ( tuple ( nextnodes ) , tree . operator ) \n 
~~ if isinstance ( tree , DFPartselect ) : \n 
~~~ msb = replaceUndefined ( tree . msb , termname ) \n 
lsb = replaceUndefined ( tree . lsb , termname ) \n 
var = replaceUndefined ( tree . var , termname ) \n 
return DFPartselect ( var , msb , lsb ) \n 
~~ if isinstance ( tree , DFPointer ) : \n 
~~~ ptr = replaceUndefined ( tree . ptr , termname ) \n 
return DFPointer ( var , ptr ) \n 
~~ if isinstance ( tree , DFConcat ) : \n 
~~ return DFConcat ( tuple ( nextnodes ) ) \n 
~~ raise DefinitionError ( % ( str ( type ( tree ) ) , str ( tree ) ) ) \n 
from pyverilog . dataflow . optimizer import VerilogDataflowOptimizer \n 
from pyverilog . controlflow . controlflow_analyzer import VerilogControlflowAnalyzer \n 
codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n 
def test ( ) : \n 
~~~ filelist = [ codedir + ] \n 
topmodule = \n 
noreorder = False \n 
nobind = False \n 
include = None \n 
define = None \n 
analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n 
noreorder = noreorder , \n 
nobind = nobind , \n 
preprocess_include = include , \n 
preprocess_define = define ) \n 
instances = analyzer . getInstances ( ) \n 
terms = analyzer . getTerms ( ) \n 
binddict = analyzer . getBinddict ( ) \n 
optimizer = VerilogDataflowOptimizer ( terms , binddict ) \n 
optimizer . resolveConstant ( ) \n 
c_analyzer = VerilogControlflowAnalyzer ( topmodule , terms , \n 
binddict , \n 
resolved_terms = optimizer . getResolvedTerms ( ) , \n 
resolved_binddict = optimizer . getResolvedBinddict ( ) , \n 
constlist = optimizer . getConstlist ( ) \n 
for tk in sorted ( c_analyzer . resolved_terms . keys ( ) , key = lambda x : str ( x ) ) : \n 
~~~ tree = c_analyzer . makeTree ( tk ) \n 
output . append ( str ( tk ) + + tree . tocode ( ) ) \n 
~~ rslt = . join ( output ) + \n 
print ( rslt ) \n 
assert ( expected == rslt ) \n 
~~~ test ( ) \n 
import dataflow_example \n 
~~~ test_module = dataflow_example . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
assert ( expected_code == code ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) \n 
from veriloggen import * \n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
interval = m . Parameter ( , 16 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , 8 , initval = 0 ) \n 
count = m . Reg ( , 32 , initval = 0 ) \n 
seq = Seq ( m , , clk , rst ) \n 
seq . add ( Systask ( , , led , count ) ) \n 
seq . add ( count ( count + 1 ) , cond = count < interval - 1 ) \n 
seq . add ( count ( 0 ) , cond = count == interval - 1 ) \n 
seq . add ( led ( led + 1 ) , cond = count == interval - 1 ) \n 
seq . make_always ( ) \n 
return m \n 
~~ def mkTest ( ) : \n 
led = mkLed ( ) \n 
params = m . copy_params ( led ) \n 
ports = m . copy_sim_ports ( led ) \n 
clk = ports [ ] \n 
rst = ports [ ] \n 
uut = m . Instance ( led , , \n 
params = m . connect_params ( led ) , \n 
ports = m . connect_ports ( led ) ) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = 100 ) \n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ \n 
width = m . Parameter ( , 8 ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Reg ( , 32 ) \n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( Cond ( count == 1023 , 0 , count + 1 ) ) \n 
led ( 0 ) \n 
led ( Cond ( count == 1024 - 1 , led + 1 , led ) ) \n 
~~~ led = mkLed ( ) \n 
verilog = led . to_verilog ( ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
def mkSub ( ) : \n 
count = m . OutputReg ( , 32 ) \n 
If ( count == 1023 ) ( \n 
count ( count + 1 ) \n 
~~ def mkLed ( ) : \n 
count = m . Wire ( , 32 ) \n 
sub = mkSub ( ) \n 
m . Instance ( sub , , m . connect_params ( sub ) , m . connect_ports ( sub ) ) \n 
led ( led + 1 ) \n 
inst_sub = m . Reg ( , 32 ) \n 
~~ except ValueError as e : \n 
~~~ print ( e . args [ 0 ] ) \n 
#print(verilog) \n 
If ( count == 1024 - 1 ) ( \n 
led ( led + 1 ) , \n 
SingleStatement ( SystemTask ( , , led ) ) \n 
import veriloggen . dataflow as dataflow \n 
def mkMain ( ) : \n 
~~~ x = dataflow . Variable ( , valid = , ready = , point = 8 ) \n 
y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n 
z = x * y \n 
z . output ( , valid = , ready = ) \n 
df = dataflow . Dataflow ( z ) \n 
m = df . to_module ( ) \n 
main = mkMain ( ) \n 
params = m . copy_params ( main ) \n 
ports = m . copy_sim_ports ( main ) \n 
xdata = ports [ ] \n 
xvalid = ports [ ] \n 
xready = ports [ ] \n 
ydata = ports [ ] \n 
yvalid = ports [ ] \n 
yready = ports [ ] \n 
zdata = ports [ ] \n 
zvalid = ports [ ] \n 
zready = ports [ ] \n 
xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
ydata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
zdata_orig = m . WireLike ( ports [ ] , name = ) \n 
m . Always ( ) ( xdata ( fixed . to_fixed ( xdata_orig , 8 ) ) ) \n 
m . Always ( ) ( ydata ( fixed . to_fixed ( ydata_orig , 4 ) ) ) \n 
m . Assign ( zdata_orig ( fixed . fixed_to_int ( zdata , 8 ) ) ) \n 
uut = m . Instance ( main , , \n 
params = m . connect_params ( main ) , \n 
ports = m . connect_ports ( main ) ) \n 
reset_done = m . Reg ( , initval = 0 ) \n 
reset_stmt = [ ] \n 
reset_stmt . append ( reset_done ( 0 ) ) \n 
reset_stmt . append ( xdata ( 0 ) ) \n 
reset_stmt . append ( xvalid ( 0 ) ) \n 
reset_stmt . append ( ydata ( 0 ) ) \n 
reset_stmt . append ( yvalid ( 0 ) ) \n 
reset_stmt . append ( zready ( 0 ) ) \n 
reset_stmt . append ( xdata_orig ( 0 ) ) \n 
reset_stmt . append ( ydata_orig ( 0 ) ) \n 
simulation . setup_waveform ( m , uut , xdata_orig , ydata_orig , zdata_orig ) \n 
init = simulation . setup_reset ( m , rst , reset_stmt , period = 100 ) \n 
nclk = simulation . next_clock \n 
reset_done ( 1 ) , \n 
nclk ( clk ) , \n 
Delay ( 10000 ) , \n 
def send ( name , data , valid , ready , step = 1 , waitnum = 10 ) : \n 
~~~ fsm = FSM ( m , name + , clk , rst ) \n 
count = m . TmpReg ( 32 , initval = 0 ) \n 
fsm . add ( valid ( 0 ) ) \n 
fsm . goto_next ( cond = reset_done ) \n 
for _ in range ( waitnum ) : \n 
~~~ fsm . goto_next ( ) \n 
~~ fsm . add ( valid ( 1 ) ) \n 
fsm . goto_next ( ) \n 
fsm . add ( data ( data + step ) , cond = ready ) \n 
fsm . add ( count . inc ( ) , cond = ready ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 5 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 5 , ready ) ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 10 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 10 , ready ) ) \n 
fsm . make_always ( ) \n 
~~ def receive ( name , data , valid , ready , waitnum = 10 ) : \n 
fsm . add ( ready ( 0 ) ) \n 
yinit = fsm . current ( ) \n 
fsm . add ( ready ( 1 ) , cond = valid ) \n 
fsm . goto_next ( cond = valid ) \n 
for i in range ( waitnum ) : \n 
~~~ fsm . add ( ready ( 0 ) ) \n 
~~ fsm . goto ( yinit ) \n 
~~ send ( , xdata_orig , xvalid , xready , step = 1 , waitnum = 10 ) \n 
send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n 
receive ( , zdata , zvalid , zready , waitnum = 50 ) \n 
If ( reset_done ) ( \n 
If ( AndList ( xvalid , xready ) ) ( \n 
Systask ( , , xdata_orig ) \n 
If ( AndList ( yvalid , yready ) ) ( \n 
Systask ( , , ydata_orig ) \n 
If ( AndList ( zvalid , zready ) ) ( \n 
Systask ( , , zdata_orig ) \n 
sim = simulation . Simulator ( test ) \n 
#sim.view_waveform(background=True) \n 
import dataflow_mul \n 
~~~ test_module = dataflow_mul . mkTest ( ) \n 
valid = m . OutputReg ( , initval = 0 ) \n 
count = m . Reg ( , width = 32 , initval = 0 ) \n 
up = m . Wire ( ) \n 
down = m . Wire ( ) \n 
m . Assign ( up ( 1 ) ) \n 
m . Assign ( down ( 0 ) ) \n 
fsm = FSM ( m , , clk , rst ) \n 
for i in range ( 4 ) : \n 
~~ c = count >= 16 \n 
fsm . add ( valid ( up ) , cond = c , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . goto_next ( cond = c ) \n 
for i in range ( 8 ) : \n 
~~ c = count >= 32 \n 
~~~ fsm . add ( valid ( up ) , cond = c , delay = 1 , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n 
~~ fsm . make_always ( reset = [ count . reset ( ) ] , body = [ count ( count + 1 ) ] ) \n 
clk = m . Reg ( ) \n 
rst = m . Reg ( ) \n 
valid = m . Wire ( ) \n 
uut = m . Instance ( mkLed ( ) , , \n 
ports = ( ( , clk ) , ( , rst ) , ( , valid ) ) ) \n 
simulation . setup_waveform ( m , uut ) \n 
init = simulation . setup_reset ( m , rst , period = 100 ) \n 
import pipeline_draw_graph \n 
~~~ test_module = pipeline_draw_graph . mkTest ( ) \n 
import read_verilog_module_str \n 
~~~ test_module = read_verilog_module_str . mkTop ( ) \n 
import veriloggen . core . vtypes as vtypes \n 
import veriloggen . core . module as module \n 
def mkMultiplierCore ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ retwidth = lwidth + rwidth \n 
m = module . Module ( % index ) \n 
update = m . Input ( ) \n 
a = m . Input ( , lwidth ) \n 
b = m . Input ( , rwidth ) \n 
c = m . Output ( , retwidth ) \n 
_a = m . Reg ( , lwidth , signed = lsigned ) \n 
_b = m . Reg ( , rwidth , signed = rsigned ) \n 
tmpval = [ m . Reg ( % i , retwidth , signed = True ) for i in range ( depth - 1 ) ] \n 
rslt = m . Wire ( , retwidth , signed = True ) \n 
__a = _a \n 
__b = _b \n 
if not lsigned : \n 
~~~ __a = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _a ) ) \n 
~~ if not rsigned : \n 
~~~ __b = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _b ) ) \n 
~~ m . Assign ( rslt ( __a * __b ) ) \n 
m . Assign ( c ( tmpval [ depth - 2 ] ) ) \n 
m . Always ( vtypes . Posedge ( clk ) ) ( \n 
vtypes . If ( update ) ( \n 
_a ( a ) , \n 
_b ( b ) , \n 
tmpval [ 0 ] ( rslt ) , \n 
[ tmpval [ i ] ( tmpval [ i - 1 ] ) for i in range ( 1 , depth - 1 ) ] \n 
~~ def mkMultiplier ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
retwidth = lwidth + rwidth \n 
mult = mkMultiplierCore ( index , lwidth , rwidth , lsigned , rsigned , depth ) \n 
enable = m . Input ( ) \n 
valid = m . Output ( ) \n 
valid_reg = [ m . Reg ( % i ) for i in range ( depth ) ] \n 
m . Assign ( valid ( valid_reg [ depth - 1 ] ) ) \n 
vtypes . If ( rst ) ( \n 
[ valid_reg [ i ] ( 0 ) for i in range ( depth ) ] \n 
valid_reg [ 0 ] ( enable ) , \n 
[ valid_reg [ i ] ( valid_reg [ i - 1 ] ) for i in range ( 1 , depth ) ] \n 
ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n 
m . Instance ( mult , , ports = ports ) \n 
~~ index_count = 0 \n 
def get_mul ( lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ global index_count \n 
mul = mkMultiplier ( index_count , lwidth , rwidth , lsigned , rsigned , depth ) \n 
index_count += 1 \n 
return mul \n 
~~ def reset ( ) : \n 
index_count = 0 \n 
from pyramid import testing \n 
import mock \n 
class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
~~~ testing . tearDown ( ) \n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_modified \n 
return acl_modified ( event ) \n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ from substanced . audit import AuditLog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
auditlog = AuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
context . __oid__ = 5 \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
event . old_acl = \n 
event . new_acl = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 5 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
: entry [ 2 ] . timestamp , \n 
: { : 1 , : } , \n 
def test_it_nolog ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
~~ ~~ _marker = object ( ) \n 
class Test_content_added_moved_or_duplicated ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_added_moved_or_duplicated \n 
return content_added_moved_or_duplicated ( event ) \n 
def test_it_added ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
event = _makeEvent ( ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
: 5 \n 
def test_it_added_noscribe ( self , mock_get_auditlog ) : \n 
def test_it_moved ( self , mock_get_auditlog ) : \n 
event . moving = True \n 
event . duplicating = None \n 
def test_it_duplicated ( self , mock_get_auditlog ) : \n 
event . moving = None \n 
event . duplicating = True \n 
~~ ~~ class Test_content_removed ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_removed \n 
return content_removed ( event ) \n 
~~ def test_it_moving ( self ) : \n 
~~~ event = Dummy ( ) \n 
~~ ~~ class Test_content_modified ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_modified \n 
return content_modified ( event ) \n 
def test_it_noscribe ( self , mock_get_auditlog ) : \n 
~~ ~~ class Test_logged_in ( unittest . TestCase ) : \n 
~~~ from . . subscribers import logged_in \n 
return logged_in ( event ) \n 
event . request = Dummy ( ) \n 
event . request . context = context \n 
def test_it_user_has_oid ( self , mock_get_auditlog ) : \n 
user = Dummy ( ) \n 
user . __oid__ = 5 \n 
event . user = user \n 
event . login = \n 
self . assertEqual ( entry [ 2 ] . oid , None ) \n 
def test_it_user_has_no_oid ( self , mock_get_auditlog ) : \n 
: None , \n 
~~ ~~ class Test_root_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import root_added \n 
return root_added ( event ) \n 
def test_it ( self , mock_set_auditlog ) : \n 
root = Dummy ( ) \n 
def is_set ( _root ) : \n 
~~~ self . assertEqual ( _root , root ) \n 
~~ mock_set_auditlog . side_effect = is_set \n 
event . object = root \n 
~~ ~~ class Dummy ( object ) : \n 
~~~ def __init__ ( self , kw = None ) : \n 
~~~ if kw : \n 
~~~ self . __dict__ . update ( kw ) \n 
~~ ~~ ~~ class DummyContentRegistry ( object ) : \n 
~~~ def typeof ( self , content ) : \n 
~~ ~~ def _makeAuditLog ( ) : \n 
return auditlog \n 
~~ def _makeRegistry ( ) : \n 
~~~ registry = Dummy ( ) \n 
registry . content = DummyContentRegistry ( ) \n 
return registry \n 
~~ def _makeEvent ( ) : \n 
event . parent = testing . DummyResource ( ) \n 
event . parent . __oid__ = 10 \n 
event . name = \n 
context . __parent__ = event . parent \n 
return event \n 
class Test_root_factory ( unittest . TestCase ) : \n 
~~~ self . config = testing . setUp ( ) \n 
~~ def _callFUT ( self , request , transaction , get_connection , evolve_packages ) : \n 
~~~ from . . import root_factory \n 
return root_factory ( request , transaction , get_connection , \n 
evolve_packages ) \n 
~~ def _makeRequest ( self , app_root = None ) : \n 
~~~ request = Dummy ( ) \n 
request . registry = DummyRegistry ( ) \n 
request . registry . content = Dummy ( ) \n 
request . registry . content . create = lambda * arg : app_root \n 
return request \n 
~~ def test_without_app_root ( self ) : \n 
~~~ txn = DummyTransaction ( ) \n 
root = { } \n 
gc = Dummy_get_connection ( root ) \n 
ep = DummyFunction ( True ) \n 
app_root = object ( ) \n 
request = self . _makeRequest ( app_root ) \n 
result = self . _callFUT ( request , txn , gc , ep ) \n 
self . assertEqual ( result , app_root ) \n 
self . assertTrue ( txn . committed ) \n 
self . assertTrue ( txn . savepointed ) \n 
self . assertTrue ( ep . called ) \n 
~~ def test_with_app_root ( self ) : \n 
root = { : app_root } \n 
request = testing . DummyRequest ( ) \n 
self . assertFalse ( txn . committed ) \n 
~~ ~~ class Test_includeme ( unittest . TestCase ) : \n 
~~~ def test_it ( self ) : \n 
~~~ from . . import ( \n 
includeme , \n 
connection_opened , \n 
connection_will_close , \n 
ZODBConnectionOpened , \n 
ZODBConnectionWillClose , \n 
config = DummyConfig ( ) \n 
includeme ( config ) \n 
config . subscriptions , \n 
[ ( connection_opened , ZODBConnectionOpened ) , \n 
( connection_will_close , ZODBConnectionWillClose ) , \n 
~~ ~~ class Test_connection_opened ( unittest . TestCase ) : \n 
~~~ from . . import connection_opened \n 
event = DummyEvent ( ) \n 
connection_opened ( event ) \n 
self . assertEqual ( event . request . _zodb_tx_counts , ( 0 , 0 ) ) \n 
~~ ~~ class Test_connection_will_close ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event , statsd_incr ) : \n 
~~~ from . . import connection_will_close \n 
return connection_will_close ( event , statsd_incr ) \n 
~~ def test_no_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( ) \n 
result = self . _callFUT ( event , None ) \n 
~~ def test_with_postitive_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 5 , 5 ) \n 
event . request . _zodb_tx_counts = ( 1 , 1 ) \n 
L = [ ] \n 
def statsd_incr ( name , num , registry = None ) : \n 
~~~ L . append ( ( name , num ) ) \n 
~~ self . _callFUT ( event , statsd_incr ) \n 
L , \n 
[ ( , 4 ) , ( , 4 ) ] \n 
~~ def test_with_zero_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 1 , 1 ) \n 
self . _callFUT ( event , None ) \n 
[ ] \n 
~~ ~~ class DummyTransaction ( object ) : \n 
~~~ committed = False \n 
savepointed = False \n 
def commit ( self ) : \n 
~~~ self . committed = True \n 
~~ def savepoint ( self ) : \n 
~~~ self . savepointed = True \n 
~~ ~~ class Dummy_get_connection ( object ) : \n 
~~~ def __init__ ( self , root ) : \n 
~~~ self . _root = root \n 
~~ def root ( self ) : \n 
~~~ return self . _root \n 
~~ def __call__ ( self , request ) : \n 
~~ ~~ class DummyFunction ( object ) : \n 
~~~ called = False \n 
def __init__ ( self , result ) : \n 
~~~ self . result = result \n 
~~ def __call__ ( self , * args , ** kw ) : \n 
~~~ self . called = True \n 
self . args = args \n 
self . kw = kw \n 
return self . result \n 
~~ class DummyRegistry ( object ) : \n 
~~~ def notify ( self , event ) : \n 
~~~ self . event = event \n 
~~ ~~ class DummyConfig ( object ) : \n 
~~~ self . subscriptions = [ ] \n 
~~ def add_subscriber ( self , fn , event_type ) : \n 
~~~ self . subscriptions . append ( ( fn , event_type ) ) \n 
~~ ~~ class DummyConnection ( object ) : \n 
~~~ def __init__ ( self , loads , stores ) : \n 
~~~ self . loads = loads \n 
self . stores = stores \n 
~~ def getTransferCounts ( self ) : \n 
~~~ return ( self . loads , self . stores ) \n 
~~ ~~ class DummyEvent ( object ) : \n 
~~~ def __init__ ( self , loads = 0 , stores = 0 ) : \n 
self . conn = DummyConnection ( loads , stores ) \n 
~~ ~~ import pkg_resources \n 
import mimetypes \n 
import colander \n 
import deform . schema \n 
from pyramid . httpexceptions import HTTPFound \n 
from pyramid . response import Response \n 
from pyramid . security import NO_PERMISSION_REQUIRED \n 
from . . form import FormView \n 
from . . file import ( \n 
FilePropertiesSchema , \n 
FileUploadTempStore , \n 
file_upload_widget , \n 
file_name_node , \n 
USE_MAGIC , \n 
from . . interfaces import ( \n 
IFile , \n 
IFolder , \n 
from . . sdi import mgmt_view \n 
@ mgmt_view ( \n 
context = IFile , \n 
permission = , \n 
tab_condition = False , \n 
http_cache = 0 , \n 
def view_file ( context , request ) : \n 
~~~ return context . get_response ( request = request ) \n 
~~ @ mgmt_view ( \n 
tab_title = , \n 
permission = \n 
def view_tab ( context , request ) : \n 
~~~ return HTTPFound ( location = request . sdiapi . mgmt_path ( context ) ) \n 
~~ class AddFileSchema ( FilePropertiesSchema ) : \n 
~~~ file = colander . SchemaNode ( \n 
deform . schema . FileData ( ) , \n 
widget = file_upload_widget , \n 
missing = colander . null , \n 
~~ @ colander . deferred \n 
def name_or_file ( node , kw ) : \n 
~~~ def _name_or_file ( node , struct ) : \n 
~~~ if not struct [ ] and not struct [ ] : \n 
~~~ raise colander . Invalid ( node , ) \n 
~~ if not struct [ ] : \n 
~~~ filename = struct [ ] . get ( ) \n 
if filename : \n 
~~~ name_node = file_name_node . bind ( \n 
context = kw [ ] , request = kw [ ] \n 
name_node . validator ( node [ ] , filename ) \n 
~~~ raise colander . Invalid ( \n 
node , \n 
~~ ~~ ~~ return _name_or_file \n 
context = IFolder , \n 
renderer = , \n 
addable_content = , \n 
tab_condition = False \n 
class AddFileView ( FormView ) : \n 
~~~ title = \n 
schema = AddFileSchema ( validator = name_or_file ) . clone ( ) \n 
schema [ ] . missing = colander . null \n 
buttons = ( , ) \n 
def _makeob ( self , stream , title , mimetype ) : \n 
~~~ return self . request . registry . content . create ( \n 
stream = stream , \n 
mimetype = mimetype , \n 
title = title , \n 
~~ def add_success ( self , appstruct ) : \n 
~~~ name = appstruct [ ] \n 
title = appstruct [ ] or None \n 
filedata = appstruct [ ] \n 
mimetype = appstruct [ ] or USE_MAGIC \n 
stream = None \n 
filename = None \n 
if filedata : \n 
~~~ filename = filedata [ ] \n 
stream = filedata [ ] \n 
if stream : \n 
~~~ stream . seek ( 0 ) \n 
~~~ stream = None \n 
~~ ~~ name = name or filename \n 
fileob = self . _makeob ( stream , title , mimetype ) \n 
self . context [ name ] = fileob \n 
tmpstore = FileUploadTempStore ( self . request ) \n 
tmpstore . clear ( ) \n 
return HTTPFound ( self . request . sdiapi . mgmt_path ( self . context ) ) \n 
~~ ~~ onepixel = pkg_resources . resource_filename ( \n 
permission = NO_PERMISSION_REQUIRED \n 
def preview_image_upload ( request ) : \n 
~~~ uid = request . subpath [ 0 ] \n 
tempstore = FileUploadTempStore ( request ) \n 
filedata = tempstore . get ( uid , { } ) \n 
fp = filedata . get ( ) \n 
if fp is not None : \n 
~~~ fp . seek ( 0 ) \n 
filename = filedata [ ] \n 
~~ mimetype = mimetypes . guess_type ( filename , strict = False ) [ 0 ] \n 
if not mimetype or not mimetype . startswith ( ) : \n 
~~~ mimetype = \n 
fp = open ( onepixel , ) \n 
~~ response = Response ( content_type = mimetype , app_iter = fp ) \n 
class Test_principal_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import principal_added \n 
return principal_added ( event ) \n 
~~ def test_event_wo_loading_attr ( self ) : \n 
~~~ event = testing . DummyResource ( ) \n 
event . object = testing . DummyResource ( ) \n 
self . assertRaises ( AttributeError , self . _callFUT , event ) \n 
~~ def test_event_w_loading_True ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
~~ def test_wo_principals_service ( self ) : \n 
~~~ from zope . interface import directlyProvides \n 
from ... interfaces import IFolder \n 
event = testing . DummyResource ( loading = False ) \n 
root = testing . DummyResource ( ) \n 
directlyProvides ( root , IFolder ) \n 
event . object = root [ ] = testing . DummyResource ( ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
~~ def test_user_not_in_groups ( self ) : \n 
~~~ from ... testing import make_site \n 
from ... interfaces import IUser \n 
site = make_site ( ) \n 
user = testing . DummyResource ( __provides__ = IUser ) \n 
site [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
~~ def test_user_in_groups ( self ) : \n 
groups = site [ ] [ ] \n 
groups [ ] = testing . DummyResource ( ) \n 
~~ def test_group_not_in_users ( self ) : \n 
group = testing . DummyResource ( ) \n 
site [ ] = group \n 
event = testing . DummyResource ( object = group , loading = False ) \n 
~~ def test_group_in_users ( self ) : \n 
users = site [ ] [ ] \n 
users [ ] = testing . DummyResource ( ) \n 
~~ ~~ class Test_user_will_be_removed ( unittest . TestCase ) : \n 
~~~ from . . subscribers import user_will_be_removed \n 
return user_will_be_removed ( event ) \n 
~~ def test_loading ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True , moving = None ) \n 
~~ def test_moving ( self ) : \n 
~~~ event = testing . DummyResource ( loading = False , moving = True ) \n 
~~ def test_it ( self ) : \n 
~~~ from ... interfaces import IFolder \n 
parent = testing . DummyResource ( __provides__ = IFolder ) \n 
user = testing . DummyResource ( ) \n 
reset = testing . DummyResource ( ) \n 
def commit_suicide ( ) : \n 
~~~ reset . committed = True \n 
~~ reset . commit_suicide = commit_suicide \n 
objectmap = DummyObjectMap ( ( reset , ) ) \n 
parent . __objectmap__ = objectmap \n 
parent [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False , moving = None ) \n 
self . assertTrue ( reset . committed ) \n 
~~~ event = testing . DummyResource ( object = None , loading = False ) \n 
~~ ~~ class Test_user_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import user_added \n 
return user_added ( event ) \n 
~~ def test_it_user_has_no_oid ( self ) : \n 
~~~ user = testing . DummyResource ( ) \n 
event . registry = DummyRegistry ( ) \n 
~~~ from pyramid . security import Allow \n 
user . __oid__ = 1 \n 
user . __acl__ , \n 
[ ( Allow , 1 , ( , \n 
) ) ] ) \n 
~~ ~~ class Test_acl_maybe_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import acl_maybe_added \n 
return acl_maybe_added ( event ) \n 
~~~ event = DummyEvent ( moving = True , loading = False ) \n 
self . assertEqual ( self . _callFUT ( event ) , False ) \n 
~~~ event = DummyEvent ( moving = None , loading = True ) \n 
~~ def test_objectmap_is_None ( self ) : \n 
~~~ event = DummyEvent ( moving = None , object = None , loading = False ) \n 
~~ def test_no_acls ( self ) : \n 
~~~ from substanced . interfaces import IFolder \n 
resource1 = testing . DummyResource ( __provides__ = IFolder ) \n 
resource2 = testing . DummyResource ( ) \n 
resource1 [ ] = resource2 \n 
objectmap = DummyObjectMap ( ) \n 
resource1 . __objectmap__ = objectmap \n 
event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n 
self . assertEqual ( objectmap . connections , [ ] ) \n 
~~ def test_with_acls ( self ) : \n 
~~~ from ... interfaces import PrincipalToACLBearing \n 
from substanced . interfaces import IFolder \n 
resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n 
resource2 . __acl__ = [ ( None , , None ) , ( None , 2 , None ) ] \n 
objectmap . connections , \n 
[ ( 2 , resource2 , PrincipalToACLBearing ) , \n 
( 1 , resource1 , PrincipalToACLBearing ) ] \n 
~~ ~~ class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ event = DummyEvent ( object = None ) \n 
~~ def test_gardenpath ( self ) : \n 
resource = testing . DummyResource ( ) \n 
resource . __objectmap__ = objectmap \n 
event = DummyEvent ( \n 
object = resource , \n 
new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n 
old_acl = [ ( None , , None ) , ( None , 2 , None ) ] , \n 
[ ( 1 , resource , PrincipalToACLBearing ) ] \n 
objectmap . disconnections , \n 
[ ( 2 , resource , PrincipalToACLBearing ) ] \n 
~~ ~~ class DummyObjectMap ( object ) : \n 
~~~ def __init__ ( self , result = ( ) ) : \n 
self . connections = [ ] \n 
self . disconnections = [ ] \n 
~~ def targets ( self , object , reftype ) : \n 
~~~ return self . result \n 
~~ def connect ( self , source , target , reftype ) : \n 
~~~ self . connections . append ( ( source , target , reftype ) ) \n 
~~ def disconnect ( self , source , target , reftype ) : \n 
~~~ self . disconnections . append ( ( source , target , reftype ) ) \n 
~~~ def __init__ ( self , ** kw ) : \n 
~~ ~~ class DummyRegistry ( object ) : \n 
~~~ def subscribers ( self , * arg ) : \n 
~~ ~~ from pyramid . httpexceptions import ( \n 
HTTPForbidden , \n 
HTTPFound \n 
from pyramid . renderers import get_renderer \n 
from pyramid . session import check_csrf_token \n 
from pyramid . security import ( \n 
remember , \n 
forget , \n 
Authenticated , \n 
NO_PERMISSION_REQUIRED , \n 
from ... util import get_oid \n 
from . . import mgmt_view \n 
from substanced . interfaces import IUserLocator \n 
from substanced . principal import DefaultUserLocator \n 
from substanced . event import LoggedIn \n 
context = HTTPForbidden , \n 
permission = NO_PERMISSION_REQUIRED , \n 
effective_principals = Authenticated , \n 
def login ( context , request ) : \n 
~~~ login_url = request . sdiapi . mgmt_path ( request . context , ) \n 
referrer = request . url \n 
if in referrer : \n 
~~~ return HTTPForbidden ( ) \n 
~~ if login_url in referrer : \n 
~~~ referrer = request . sdiapi . mgmt_path ( request . virtual_root ) \n 
~~ came_from = request . session . setdefault ( , referrer ) \n 
login = \n 
password = \n 
if in request . params : \n 
~~~ check_csrf_token ( request ) \n 
~~~ request . sdiapi . flash ( , ) \n 
~~~ login = request . params [ ] \n 
password = request . params [ ] \n 
adapter = request . registry . queryMultiAdapter ( \n 
( context , request ) , \n 
IUserLocator \n 
if adapter is None : \n 
~~~ adapter = DefaultUserLocator ( context , request ) \n 
~~ user = adapter . get_user_by_login ( login ) \n 
if user is not None and user . check_password ( password ) : \n 
~~~ request . session . pop ( , None ) \n 
headers = remember ( request , get_oid ( user ) ) \n 
request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n 
return HTTPFound ( location = came_from , headers = headers ) \n 
~~ request . sdiapi . flash ( , ) \n 
~~ ~~ template = get_renderer ( \n 
) . implementation ( ) \n 
return dict ( \n 
url = request . sdiapi . mgmt_path ( request . virtual_root , ) , \n 
came_from = came_from , \n 
login = login , \n 
password = password , \n 
login_template = template , \n 
def logout ( request ) : \n 
~~~ headers = forget ( request ) \n 
return HTTPFound ( location = request . sdiapi . mgmt_path ( request . context ) , \n 
headers = headers ) \n 
~~ 