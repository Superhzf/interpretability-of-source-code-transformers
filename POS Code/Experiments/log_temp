Loading model: bert-base-uncased
Reading input corpus
Preparing output file
Extracting representations from model
Sentence         : "for i in range ( 5 ) : \n"
Original    (009): ['for', 'i', 'in', 'range', '(', '5', ')', ':', '\\n']
Tokenized   (012): ['[CLS]', 'for', 'i', 'in', 'range', '(', '5', ')', ':', '\\', 'n', '[SEP]']
Filtered   (010): ['for', 'i', 'in', 'range', '(', '5', ')', ':', '\\', 'n']
Detokenized (009): ['for', 'i', 'in', 'range', '(', '5', ')', ':', '\\n']
Counter: 10
===================================================================
Hidden states:  (13, 9, 768)
# Extracted words:  9
Sentence         : "if i > 10 : \n"
Original    (006): ['if', 'i', '>', '10', ':', '\\n']
Tokenized   (009): ['[CLS]', 'if', 'i', '>', '10', ':', '\\', 'n', '[SEP]']
Filtered   (007): ['if', 'i', '>', '10', ':', '\\', 'n']
Detokenized (006): ['if', 'i', '>', '10', ':', '\\n']
Counter: 7
===================================================================
Hidden states:  (13, 6, 768)
# Extracted words:  6
Loading json activations from bert_activations_temp.json...
2 13.0
The length of activations 2
The activation of the first sentence, length: (9, 9984)
[[ 0.12586598  0.47281384 -0.78951544 ...  0.18306012 -0.4550504
   0.37854514]
 [-0.3318887   0.48600858 -0.15778467 ... -0.8457081  -0.49218208
   1.3379644 ]
 [-0.37223202  0.22232907 -0.12656714 ...  0.07646084 -0.78977495
   0.58231616]
 ...
 [ 0.2701397   0.10105477  0.7140784  ...  0.03576119 -0.8111805
  -0.16551374]
 [-0.5039458   0.17337863 -0.80271304 ... -0.64230627 -0.27011454
   1.0999715 ]
 [ 0.39944354 -0.04736179 -0.2535036  ... -0.28667185 -0.72748697
   1.0034733 ]]
The activation of the first sentence, length: (6, 9984)
[[-0.3406073   0.70248497 -0.6482753  ... -0.4028326  -0.5237236
   0.7751946 ]
 [-0.3318887   0.48600858 -0.15778467 ... -0.5155879  -0.46797884
   1.238392  ]
 [ 1.1385635  -0.13703825 -0.42685252 ... -0.33585    -0.5462433
   1.0122095 ]
 [ 0.2842071   0.08648323  0.5679555  ... -0.4887393  -0.28946784
   0.33928448]
 [-0.5925337   0.09750148 -0.87041426 ... -0.60065776 -0.22100395
   0.9925645 ]
 [ 0.48485294 -0.26946315 -0.25521272 ... -0.2366854  -0.84667706
   1.031491  ]]
The difference of the activations between i
0.0
