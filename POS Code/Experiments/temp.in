\n 
# \n 
template_name = \n 
slug = "policy_profile" \n 
preload = False \n 
tabs = ( NetworkProfileTab , PolicyProfileTab ) \n 
weak_store = WeakLocal ( ) \n 
strong_store = corolocal . local \n 
eventlet . monkey_patch ( ) \n 
CONF . register_opts ( impl_zmq . zmq_opts ) \n 
vpnservices_dict = { : self . api_vpnservices . list ( ) } \n 
vpnservice [ ] [ ] ) \n 
form_data } ) . AndReturn ( ipsecsiteconnection ) \n 
ipsecsiteconnections_dict ) \n 
ipsecsiteconnections [ ] ) : \n 
neutronclient . show_ipsec_site_connection ( \n 
ret_val = api . vpn . ipsecsiteconnection_get ( self . request , \n 
response_kwargs . setdefault ( "filename" , "usage.csv" ) \n 
BlendProbes = 1 \n 
lightmap_index = field ( "m_LightmapIndex" ) \n 
receive_shadows = field ( "m_ReceiveShadows" , bool ) \n 
Config . parser . readfp ( sconf ) \n 
BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n 
BOARDS_FILE = BBS_ROOT + \n 
USHM_SIZE = MAXACTIVE + 10 \n 
UTMP_HASHSIZE = USHM_SIZE * 4 \n 
SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n 
SESSION_TIMEOUT_SECONDS = 86400 * 30 \n 
MAX_ATTACHSIZE = 20 * 1024 * 1024 \n 
MAIL_SIZE_LIMIT = - 1 \n 
newparts = [ ] \n 
firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n 
_id = start - 1 \n 
linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n 
boardname = svc . get_str ( params , , ) \n 
has_perm = user . IsDigestMgr ( ) \n 
Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n 
svc . writedata ( json . dumps ( result ) ) \n 
postinfo = Post . Post ( item . realpath ( ) , None ) \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n 
read_count = msg_count - msg_unread \n 
term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n 
new_unread [ msghead . topid ] = i \n 
to_steal = { } \n 
to_steal_begin = msg_count \n 
pass \n 
final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
msgtext = msgbox . LoadMsgText ( msghead ) \n 
roster = self . rosters . get ( self ) \n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
sp_desc , \n 
con = hpov . connection ( args . host ) \n 
acceptEULA ( con ) \n 
fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n 
boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n 
define_profile_template ( srv , \n 
sht [ ] , \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
get_address_pools ( con , srv , args . types ) \n 
enclosure_group = None , server_profile = None ) : \n 
biosSettings = None , \n 
macType = , \n 
localStorageSettingsV3 , macType , name , \n 
sanStorageV3 , serialNumber , \n 
serverProfileTemplateUri , uuid , wwnType ) \n 
profile_template = self . _con . get ( entity [ ] ) \n 
powerMode = ) : \n 
egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n 
allocatorBody = { : count } \n 
prange [ ] = False \n 
tempstr = "hp-rest-classes-bios-" + romfamily + "-" + biosversion \n 
monolith = None ) : \n 
pathjoinstr ) ) : \n 
newclass . set_root ( root ) \n 
folderentries = data [ "links" ] \n 
datareturn . append ( self . load_file ( fqpath , root = root , biossection = True , registries = True , datareturn = True ) ) \n 
currdict = currdict , monolith = monolith , \n 
newarg = newarg , checkall = checkall ) \n 
attrreg = self . find_bios_registry ( regname = regname ) \n 
schlink = schlink [ len ( schlink ) - 2 ] \n 
schname . lower ( ) ) : \n 
xref = os . path . normpath ( currloc . Uri . extref ) . lstrip ( os . path . sep ) \n 
fqpath = os . path . join ( root , xref ) \n 
langcode = list ( locale . getdefaultlocale ( ) ) \n 
locationlanguage = locationlanguage . replace ( "-" , "_" ) \n 
currtype = currtype . split ( ) [ 0 ] + \n 
insttype = instance . resp . dict [ "title" ] . split ( ) [ : 1 ] \n 
nextarg = newarg [ newarg . index ( arg ) + 1 ] \n 
regcopy [ nextarg ] = patterninfo \n 
validictory . validate ( tdict , jsonsch ) \n 
wrapper . subsequent_indent = * 4 \n 
RegistryValidationError ( \n 
regentry = self \n 
"\'%(ValueExpression)s\'" % ( self ) , regentry = self ) ) \n 
intval = int ( newval ) \n 
MICROS_TRANSLATIONS = ( \n 
MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n 
epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n 
epoch_microseconds = epoch_micros = microseconds = micros \n 
micros = u".%06d" % dt . microsecond if dt . microsecond else \n 
datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n 
datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n 
port = db . Column ( db . Integer , nullable = False ) \n 
eru_container_id = db . Column ( db . String ( 64 ) , index = True ) \n 
suppress_alert = db . Column ( db . Integer , nullable = False , default = 1 ) \n 
__table_args__ = ( db . Index ( , , , unique = True ) , ) \n 
cluster_id = cluster_id ) \n 
Proxy . id . desc ( ) ) . offset ( offset ) . limit ( limit ) . all ( ) \n 
iou_as_issuer . issue_funds ( amount_issued , ) \n 
github_info_json = urllib2 . urlopen ( latest ) . read ( ) \n 
drawing_tool . set_window_title ( update_notifier , watching_player = twitch_username , updates_queued = len ( new_states_queue ) , read_delay = opt . read_delay ) \n 
put_url = opt . trackerserver_url + "/tracker/api/update/" + opt . trackerserver_authkey \n 
json_string = json . dumps ( state , cls = TrackerStateEncoder , sort_keys = True ) \n 
new_states_queue . pop ( 0 ) \n 
framecount += 1 \n 
decay = decay , \n 
expected_kwargs = { , } \n 
outputs [ ] = in_shapes [ ] \n 
buf = BufferStructure ( self . in_shapes [ ] . feature_shape [ - 1 ] ) \n 
internals [ ] = self . in_shapes [ ] \n 
sigma_b , centered , x_hat = buffers . internals \n 
dgamma = buffers . gradients . gamma \n 
outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n 
_h . add_tt ( term4 , indeltas , indeltas ) \n 
targets_name = , mask_name = ) : \n 
mask_name = , name = None ) : \n 
true_labels ) . astype ( np . float ) \n 
epochs = [ 0 ] * 4 + [ 1 ] * 4 + [ 2 ] * 4 \n 
on_rtd = os . environ . get ( , None ) == \n 
html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n 
latex_elements = { \n 
latex_documents = [ \n 
ignored_fallbacks = ( ) ) : \n 
"b" : 2.0 , \n 
"c" : True , \n 
"d" : , \n 
"e" : [ 1 , 2 , 3 ] , \n 
"f" : { : , : } , \n 
"answer" : 42 \n 
p_error = self . kp * current_error \n 
d_error = self . kd * ( current_error - self . previous_error ) / timestep \n 
current_error + self . previous_error ) / 2 + self . integral_error \n 
i_error = self . ki * self . integral_error \n 
total_error = p_error + d_error + i_error \n 
cmd_match_names = cmd . Cmd . completenames ( self , text , * ignored ) \n 
obj_names = self . ctrl_client . objects . keys ( ) \n 
api_match_names = [ x for x in obj_names if x . startswith ( text ) ] \n 
reply_time = self . ctrl_client . ping ( ) \n 
sub_addr = sys . argv [ 2 ] \n 
CLI ( ctrl_addr , sub_addr ) . cmdloop ( ) \n 
duty = int ( cur_pwm [ "duty_ns" ] ) \n 
read_pos = int ( round ( ( ( duty - 580000 ) / 2320000. ) * 180 ) ) \n 
create_login_url , create_logout_url \n 
create_logout_url ( request . url ) \n 
_value = _options_header_vkw ( _value , kw ) \n 
value_type == value_subtype == ) or \n 
value_subtype == or \n 
item_subtype == value_subtype ) ) \n 
no_cache = cache_property ( , , None ) \n 
no_store = cache_property ( , None , bool ) \n 
max_age = cache_property ( , - 1 , int ) \n 
no_transform = cache_property ( , None , None ) \n 
max_stale = cache_property ( , , int ) \n 
etag , weak = unquote_etag ( etag ) \n 
uri = property ( lambda x : x . get ( ) , doc = ) \n 
_require_quoting = frozenset ( [ , , , ] ) \n 
auth_type = d . pop ( , None ) or \n 
allow_token = key not in self . _require_quoting ) ) \n 
realm = auth_property ( , doc = ) \n 
rshell , shell , clear_datastore , create_user , \n 
Rule ( , endpoint = , \n 
data_field = db . StringProperty ( required = True , \n 
dst_name = path . join ( dst_path , filename ) \n 
modifiable_problem_fields = [ "description" ] \n 
problem = api . problem . get_problem ( pid = pid ) \n 
build = get_generator ( pid ) . generate ( random , pid , api . autogen_tools , n ) \n 
autogen_instance_path = get_instance_path ( pid , n = n ) \n 
"resource_files" : { \n 
instance_path = path . join ( path . dirname ( generator_path ) , "instances" , name , str ( n ) ) \n 
"correct" : correct , \n 
"points" : problem [ "score" ] , \n 
"message" : message \n 
k = str ( random . randint ( 0 , 1000 ) ) \n 
"public" : [ ( "/tmp/key" , "public_static" ) ] , \n 
"private" : [ ( "/tmp/key" , "private_static" ) ] \n 
layout = eval ( scriptWindow . setLayout ( layout ) \n 
scriptWindow . _Widget__qtWidget . resize ( 995 , 500 ) \n 
"inputSequence" , \n 
defaultValue = "" , \n 
dc . write ( "-" + objectName , "bound" , b ) \n 
additionalTerminalPlugTypes = ( GafferScene . ScenePlug , ) \n 
replace = context . get ( "textWriter:replace" , IECore . StringVectorData ( ) ) \n 
inMetadata = r [ "out" ] [ "metadata" ] . getValue ( ) \n 
negFileName = os . path . expandvars ( "$GAFFER_ROOT/python/GafferImageTest/images/checkerWithNegativeDataWindow.200x150.exr" \n 
GafferImage . Display , \n 
"port" : [ \n 
updateCountPlug . setValue ( updateCountPlug . getValue ( ) + 1 ) \n 
__import__ ( "IECore" ) . loadConfig ( "GAFFER_STARTUP_PATHS" , { } , subdirectory = "GafferImageUI" ) \n 
GafferRenderMan . RenderManShader . shaderLoader ( ) . clear ( ) \n 
coshader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshader.sl" ) \n 
nn [ "outString" ] = Gaffer . StringPlug ( direction = Gaffer . Plug . Direction . Out ) \n 
shader2 = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/version2.sl" , shaderName = "unversioned" \n 
assignment [ "shader" ] . setInput ( shaderNode [ "out" ] ) \n 
dirtiedNames = [ x [ 0 ] . fullName ( ) for x in cs ] \n 
"dynamicFloatArray" : IECore . FloatVectorData ( [ ] ) , \n 
"fixedFloatArray" : IECore . FloatVectorData ( [ 1 , 2 , 3 , 4 ] ) , \n 
"dynamicStringArray" : IECore . StringVectorData ( [ "dynamic" , "arrays" , "can" , "still" , "have" , "defaults" "fixedStringArray" : IECore . StringVectorData ( [ "hello" , "goodbye" ] ) , \n 
"dynamicColorArray" : IECore . Color3fVectorData ( [ IECore . Color3f ( 1 ) , IECore . Color3f ( 2 ) ] ) , \n 
"dynamicVectorArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Vector ) , \n 
"fixedVectorArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData "dynamicPointArray" : IECore . V3fVectorData ( [ ] , IECore . GeometricData . Interpretation . Point ) , \n 
"fixedNormalArray" : IECore . V3fVectorData ( [ IECore . V3f ( x ) for x in range ( 1 , 6 ) ] , IECore . GeometricData } \n 
arrayShader = self . compileShader ( os . path . dirname ( __file__ ) + "/shaders/coshaderArrayParameters.sl" n4 = GafferRenderMan . RenderManShader ( ) \n 
coshaderNode [ "enabled" ] . setValue ( False ) \n 
floatValue = IECore . Splineff ( \n 
S [ "parameters" ] [ "coshaderParameter" ] . setInput ( D2 [ "out" ] ) \n 
coshaderNode0 [ "parameters" ] [ "floatParameter" ] . setValue ( 0 ) \n 
sn1 = GafferRenderMan . RenderManShader ( "Shader1" ) \n 
script [ "assignment" ] [ "shader" ] . setInput ( script [ "shader" ] [ "out" ] ) \n 
traverseConnection = Gaffer . ScopedConnection ( GafferSceneTest . connectTraverseSceneToPlugDirtiedSignal script [ "shader" ] . loadShader ( "matte" ) \n 
current = s [ "render" ] . hash ( c ) \n 
"layout:section" , "Transform" , \n 
"toolbarLayout:index" , 2 , \n 
"toolbarLayout:divider" , True , \n 
currentName = self . getPlug ( ) . getValue ( ) \n 
menuButton = GafferUI . MenuButton ( menu = menu , image = "grid.png" , hasFrame = False ) \n 
p3 = Gaffer . IntPlug ( "sum" , Gaffer . Plug . Direction . Out ) \n 
childrenStrings = [ str ( c ) for c in children ] \n 
c2 = [ str ( p ) for p in path2 . children ( ) ] \n 
horizontalAlignment = GafferUI . Label . HorizontalAlignment . Right , \n 
nameWidget . textWidget ( ) . _qtWidget ( ) . setFixedWidth ( GafferUI . PlugWidget . labelWidth ( ) ) \n 
childPlug [ "enabled" ] , \n 
memberPlug = memberPlug if memberPlug is not None else plug . ancestor ( Gaffer . CompoundDataPlug . MemberPlug if memberPlug is None : \n 
menuDefinition . append ( "/Delete" , { "command" : IECore . curry ( __deletePlug , memberPlug ) , "active" \n 
includeSequences = Gaffer . Metadata . plugValue ( self . getPlug ( ) , "fileSystemPathPlugValueWidget:includeSequences" \n 
reuse = reuseUntil is not None \n 
_MultiLineStringMetadataWidget ( key = "description" ) \n 
"active" : isinstance ( node , Gaffer . Box ) or nodeEditor . nodeUI ( ) . plugValueWidget ( node [ "user" ] ) } \n 
dialogue = GafferUI . ColorChooserDialogue ( color = color , useDisplayTransform = False ) \n 
editor . plugEditor ( ) . reveal ( ) \n 
_MetadataWidget . __init__ ( self , self . __menuButton , key , target , parenting = parenting ) \n 
"checkBox" : value == self . __currentValue \n 
child . __parent = None \n 
columns = ( GafferUI . PathListingWidget . defaultNameColumn , ) , \n 
definition = Gaffer . WeakMethod ( self . __addMenuDefinition ) \n 
newIndex = 0 if event . line . p0 . y < 1 else len ( newParent ) \n 
newParent . insert ( newIndex , self . __dragItem ) \n 
selection [ : ] = self . __dragItem . fullName ( ) . split ( "." ) \n 
_registerMetadata ( plug , "nodule:type" , "" ) \n 
parentItem \n 
existingSectionNames = set ( c . name ( ) for c in rootItem if isinstance ( c , _SectionLayoutItem ) ) \n 
Gaffer . Metadata . plugValue ( self . getPlug ( ) , "preset:" + selectedPaths [ 0 ] [ 0 ] ) \n 
srcPath = self . __pathListing . getPath ( ) . copy ( ) . setFromString ( event . data [ 0 ] ) \n 
srcIndex = d . keys ( ) . index ( srcPath [ 0 ] ) \n 
targetPath = self . __pathListing . pathAt ( event . line . p0 ) \n 
item = items [ srcIndex ] \n 
selectedPreset = self . __pathListing . getSelectedPaths ( ) [ 0 ] [ 0 ] \n 
selectedIndex = [ p [ 0 ] for p in paths ] . index ( selectedPreset ) \n 
preset = selectedPaths [ 0 ] [ 0 ] \n 
scrolledContainer . setChild ( GafferUI . ListContainer ( spacing = 4 ) ) \n 
menu = GafferUI . Menu ( Gaffer . WeakMethod ( self . __gadgetMenuDefinition ) ) \n 
"/" + g . label , \n 
"command" : functools . partial ( Gaffer . WeakMethod ( self . __registerOrDeregisterMetadata ) , key = "checkBox" : metadata == g . metadata , \n 
__WidgetDefinition ( "None" , Gaffer . Plug , "" ) , \n 
__MetadataDefinition = collections . namedtuple ( "MetadataDefinition" , ( "key" , "label" , "metadataWidgetType" __metadataDefinitions = ( \n 
newSectionPath [ - 1 ] = nameWidget . getText ( ) . replace ( "." , "" ) \n 
_metadata ( self . getPlugParent ( ) , name ) \n 
_deregisterMetadata ( self . getPlugParent ( ) , name ) \n 
wr2 = weakref . ref ( w . _qtWidget ( ) ) \n 
WidgetTest . signalsEmitted = 0 \n 
QtGui . QApplication . instance ( ) . sendEvent ( w . _qtWidget ( ) , event ) \n 
GafferUI . BoxUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition ) \n 
GafferSceneUI . FilteredSceneProcessorUI . appendNodeEditorToolMenuDefinitions ( nodeEditor , node , menuDefinition \n 
yappi . print_stats ( sort_type = yappi . SORTTYPE_TTOT , limit = 30 , thread_stats_on = False ) \n 
SAMPLE_EXTRACT_METRICS_PAGE = os . path . join ( datadir , "monthly_download" ) \n 
SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH = os . path . join ( datadir , "monthly_download_different_month" \n 
testitem_aliases = ( "pmid" , TEST_PMID ) \n 
sample_data_dump = open ( SAMPLE_EXTRACT_METRICS_PAGE , "r" ) . read ( ) \n 
sample_data_dump_different_month = open ( SAMPLE_EXTRACT_METRICS_PAGE_DIFFERENT_MONTH , "r" ) . read \n 
"max_event_date" : "2012-01-31T07:34:01.126892" \n 
"_id" : "abc123" , \n 
"raw" : "max_event_date" : "2012-10-31T07:34:01.126892" , \n 
"23110252" \n 
cache_client = redis . from_url ( os . getenv ( "REDIS_URL" ) , REDIS_CACHE_DATABASE_NUMBER ) \n 
MAX_CACHE_SIZE_BYTES = 100 * 1000 * 1000 #100mb \n 
set_response = mc . set ( hash_key , json . dumps ( data ) ) \n 
metrics_url_template = "http://alm.plos.org/api/v3/articles?ids=%s&source=citations,counter&api_key=" provenance_url_template = "http://dx.doi.org/%s" \n 
relevant = ( ( "doi" == namespace ) and ( "10.1371/" in nid ) ) \n 
this_article = json_response [ 0 ] [ "sources" ] [ 0 ] [ "metrics" ] \n 
redis_url = os . environ . get ( , "redis://127.0.0.1:6379/" ) \n 
Queue ( , routing_key = ) \n 
CELERY_ACCEPT_CONTENT = [ , ] \n 
CELERY_IMPORTS = ( "core_tasks" , ) \n 
sampledir = os . path . join ( os . path . split ( __file__ ) [ 0 ] , "../../../extras/sample_provider_pages/" ) \n 
TEST_XML = open ( os . path . join ( sampledir , "facebook" , "metrics" ) ) . read ( ) \n 
provider_names = [ provider . __class__ . __name__ for provider in providers ] \n 
assert_equals ( md [ "pubmed" ] [ ] , ) \n 
tiid = db . Column ( db . Text , db . ForeignKey ( ) , primary_key = True ) \n 
tweet_ids_with_response = [ tweet [ "id_str" ] for tweet in data ] \n 
tweet_ids_without_response = [ tweet for tweet in tweet_ids if tweet not in tweet_ids_with_response flag_deleted_tweets ( tweet_ids_without_response ) \n 
access_token = os . getenv ( "TWITTER_ACCESS_TOKEN" ) \n 
num = len ( tweets ) ) ) \n 
list_of_groups = [ tweets [ i : i + group_size ] for i in range ( 0 , len ( tweets ) , group_size ) ] \n 
handle_all_tweets ( response . data , tweet_subset ) \n 
tweets = Tweet . query . filter ( Tweet . profile_id == profile_id ) \n 
tweet_dict = dict ( [ ( ( tweet . tweet_id , tweet . tiid ) , tweet ) for tweet in tweets ] ) \n 
tweet . profile_id = profile_id \n 
tweet_ids = [ tweet . tweet_id for tweet in tweets_to_hydrate_from_twitter ] \n 
display_url = url_info [ "display_url" ] \n 
tweet_id = self . tweet_id , \n 
file_loc = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
urllib . urlretrieve ( url + fname , fname ) \n 
loaded = np . fromstring ( fd . read ( ) , dtype = np . uint8 ) \n 
fd = gzip . open ( os . path . join ( data_dir , ) ) \n 
trY = loaded [ 8 : ] . reshape ( ( 60000 ) ) \n 
trX = trX . reshape ( - 1 , 28 , 28 ) \n 
dirpath = os . path . join ( self . repo . path , "unused_directory" ) \n 
subpath = os . path . join ( self . repo . path , "a" , "b" , "c" ) \n 
handle = self . profile . username , \n 
id_key = ) \n 
message_thread = model . MessageThread ( okc_id = self . thread . id , \n 
new_messages = [ message for message in self . thread . messages \n 
new_message_model = model . Message ( okc_id = new_message . id , \n 
time_sent = new_message . time_sent ) \n 
mailbox . Sync ( user ) . all ( ) \n 
user_model . upsert_model ( id_key = ) \n 
response_dict = user . photo . upload_and_confirm ( user . quickmatch ( ) . photo_infos [ 0 ] ) \n 
vcr_live_sleep ( 2 ) \n 
b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b1 = shared_zeros ( ( n_h * gates ) ) \n 
b2 = shared_zeros ( ( n_h * gates , ) ) \n 
i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n 
f_on = T . nnet . sigmoid ( g_on [ : , n_h : 2 * n_h ] ) \n 
o_on = T . nnet . sigmoid ( g_on [ : , 2 * n_h : 3 * n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 1 * n_h : 2 * n_h ] ) + T . dot ( h , U [ : , 1 * n_h : 2 * n_h ] ) + b [ 1 * n_h : 2 * n_h ] ) \n 
te_cost , te_h_updates = model ( self . X , self . params , 0. ) \n 
csvWriter = csv . writer ( sys . stdout , delimiter = separator , quotechar = quote , \n 
IColumnProvider_Methods = IPersist_Methods + [ "Initialize" , "GetColumnInfo" , "GetItemData" ] \n 
_com_interfaces_ = [ pythoncom . IID_IPersist , \n 
20 , #cChars \n 
fmt_id == self . _reg_clsid_ \n 
"Folder\\\\ShellEx\\\\ColumnHandlers\\\\" + str ( ColumnProvider . _reg_clsid_ ) ) \n 
_winreg . SetValueEx ( key , None , 0 , _winreg . REG_SZ , ColumnProvider . _reg_desc_ ) \n 
register . UseCommandLine ( ColumnProvider , \n 
aliases = MultipleValueField ( required = False ) \n 
StoredQueryParameter = namedtuple ( "StoredQueryParameter" , ( , , , , \n 
fts = list ( self . models . keys ( ) ) \n 
sort_by = parms . cleaned_data [ ] \n 
geometry_field = self . geometries [ type_names [ 0 ] ] \n 
mxy = mxy ) \n 
query_set = query_set . order_by ( * sort_by ) \n 
output_format = root . get ( , ) \n 
type_names . append ( ( namespace , name ) ) \n 
"schema" : feature_type . schema , \n 
"ns_name" : feature_type . ns_name \n 
db_params = settings . DATABASES [ response . db ] \n 
parameters = tuple ( [ adapt ( p ) for p in parameters ] ) \n 
connection_string = "PG:dbname=\'{db}\'" . format ( db = db_params [ ] ) \n 
etree . SubElement ( p , ) . text = parameter . abstractS \n 
"isPrivate" : parameter . query_expression . private == True , \n 
"language" : parameter . query_expression . language , \n 
"returnFeatureTypes" : . join ( parameter . query_expression . return_feature_types } ) . text = parameter . query_expression . text \n 
"endpoint" : request . build_absolute_uri ( ) . split ( ) [ 0 ] , \n 
"output_formats" : [ ogr . GetDriver ( drv ) . GetName ( ) for drv in range ( ogr . GetDriverCount ( ) ) "addr_street" : self . addr_street , \n 
"feature_versioning" : self . adapter . supports_feature_versioning ( ) , \n 
"date" : datetime . now ( ) , \n 
matchItem . setText ( 3 , unicode ( self . data [ "Matches" ] ) ) \n 
roundItem . setText ( 3 , unicode ( opponent [ 2 ] ) ) \n 
opponent [ 3 ] = roundItem \n 
M = np . matrix ( [ [ 2 , 3 , 4 ] , \n 
matrix = Matrix ( M , mtype = ) \n 
math = Math ( data = [ , vec_name , , Matrix ( M * a ) ] ) \n 
q2 = Quantity ( v , format_cb = lambda x : str ( int ( x ) ) ) \n 
q3 = Quantity ( v , options = { : } ) \n 
test_dimensionality_to_siunitx ( ) \n 
ph = put_handler . put_handler ( fs , ) \n 
hs = http_server . http_server ( ip = , port = 8080 ) \n 
num_trans = num_requests * num_conns \n 
trans_per_sec = num_trans / total_time \n 
map ( str , ( num_conns , num_requests , request_size , throughput , trans_per_sec ) \n 
queue . add_task ( task , 3 ) \n 
futures . append ( queue . yield_task ( task , 3 ) ) \n 
task_results [ : ] = res \n 
shuffle ( self . __queued_servers ) \n 
event_name = event [ ] \n 
event_data = zlib . compress ( pickle . dumps ( event ) ) \n 
path_only , query = self . _split_path ( path ) \n 
break ; \n 
u . email = user [ 2 ] \n 
trac_components = list ( [ ] ) \n 
component . owner = self . _get_user_login ( component . owner ) \n 
networks [ pkt . pduSource ] . append ( pkt . wirtnNetwork ) \n 
filterSource = Address ( sys . argv [ i + 1 ] ) \n 
net_count . sort ( lambda x , y : cmp ( y [ 1 ] , x [ 1 ] ) ) \n 
strm = StringIO ( self . pickleBuffer ) \n 
pdu . pduSource = self . peer \n 
connect_task . install_task ( _time ( ) + self . reconnect [ actor . peer ] ) \n 
asyncore . dispatcher . __init__ ( self , sock ) \n 
TCPServerDirector . _warning ( , err ) \n 
buff = packet [ 1 ] \n 
fileIdentifier = ( obj_type , obj_inst ) , \n 
record_data = list ( args [ 4 : ] ) \n 
accessMethod = AtomicWriteFileRequestAccessMethodChoice ( \n 
objectIdentifier = int ( args . ini . objectidentifier ) , \n 
this_application = TestApplication ( this_device , args . ini . address ) \n 
_log . debug ( "running" ) \n 
Status . FAILED_TO_ADD_TO_CLIENT : % ( COLOR_FAILED_TO_ADD_TO_CLIENT , Color . ENDC ) , \n 
end_size += f [ ] \n 
files_to_check += self . db . find_hash_varying_size ( f [ ] ) \n 
match_start , match_end = pieces . match_file ( db_file , start_size , end_size ) \n 
path_files [ os . path . join ( * path ) ] . append ( { \n 
files_sorted [ . join ( orig_path ) ] = i \n 
found_size , missing_size = 0 , 0 \n 
output_fp . write ( * write_bytes ) \n 
bytes_written += read_bytes \n 
missing_percent = ( missing_size / ( found_size + missing_size ) ) * 100 \n 
found_percent = 100 - missing_percent \n 
would_not_add = missing_size and missing_percent > self . add_limit_percent or missing_size > \n 
LEGO_PALETTE = ( , , , , , , ) \n 
Draft4Validator , RefResolver , create , extend , validator_for , validate , \n 
u"enum" : [ [ "a" , "b" , "c" ] , [ "d" , "e" , "f" ] ] , \n 
got = ( e . message for e in self . validator . iter_errors ( instance , schema ) ) \n 
checker . checks ( u"thing" ) ( check_fn ) \n 
deque ( [ "type" , 1 , "properties" , "foo" , "enum" ] ) , \n 
"baz" : { "minItems" : 2 } , \n 
"required" : [ "root" ] , \n 
e2 . absolute_schema_path , deque ( \n 
"additionalProperties" : { "type" : "integer" , "minimum" : 5 } \n 
"bar" : { "type" : "string" } , \n 
"foo" : { "minimum" : 5 } \n 
"items" : [ { } ] , \n 
validate ( instance = instance , schema = { my_property : my_value } ) \n 
chk_schema . assert_called_once_with ( { } ) \n 
stored_schema = { "stored" : "schema" } \n 
"ports" : \n 
l2Report . generateReport ( pod . id , True , False ) \n 
_YAML_ = splitext ( __file__ ) [ 0 ] + \n 
globals ( ) . update ( loadyaml ( _YAML_ ) ) \n 
gather_facts = False ) \n 
DEFAULT_API_URLS = ( , \n 
BAD_STATUS_CODES = [ , , , \n 
translate_otp = True , api_urls = DEFAULT_API_URLS , \n 
rand_str = b ( os . urandom ( 30 ) ) \n 
nonce = base64 . b64encode ( rand_str , b ( ) ) [ : 25 ] . decode ( ) \n 
otp . otp , nonce , \n 
pairs_string = . join ( [ . join ( pair ) for pair in pairs_sorted ] ) \n 
digest = hmac . new ( self . key , b ( pairs_string ) , hashlib . sha1 ) . digest ( ) \n 
signature = ( [ unquote ( v ) for k , v in pairs if k == ] or [ None ] ) [ 0 ] \n 
query_string = . join ( [ k + + v for k , v in pairs if k != ] ) \n 
pairs = ( x . split ( , 1 ) for x in query_string . split ( ) ) \n 
py_modules = [ ] , \n 
submitter , msg = result [ 0 ] \n 
contact = self . line_interface . _get_contact_by_id ( me . id ) \n 
ok_ ( me_display_name == me . name ) \n 
transport . get_extra_info . return_value = None \n 
ShortenerSettings = namedtuple ( , [ \n 
right_to_left = [ , ] , \n 
shortener = { } , \n 
workers_pool = 10 , \n 
cms_service_host = "http://localhost:5001" \n 
subparsers = args . add_subparsers ( help = , dest = ) \n 
template_parser . add_argument ( , \n 
config_parser . add_argument ( , help = ) \n 
gui_parser . add_argument ( , , type = str , help = ) \n 
changePwdResult = conn . extend . microsoft . modify_password ( user_dn , newpassword ) \n 
cap_path = os . path . join ( caps_directory , ) \n 
cap . eventloop . stop ( ) \n 
flush = Service ( name = , \n 
sourceIds = [ d [ ] for d in response . json ] \n 
"folder." ) \n 
minerva_metadata [ ] = { \n 
Description ( ) \n 
matches = re . findall ( "(\'|\\")(\\S+)(\'|\\")" , text ) \n 
package_data = { : [ ] } , \n 
list_permissions = [ , , , ] \n 
option_list = BaseCommand . option_list + ( \n 
make_option ( , \n 
confirm_token = Column ( Unicode ( 100 ) ) \n 
creation_date = Column ( DateTime ( ) , nullable = False ) \n 
last_login_date = Column ( DateTime ( ) ) \n 
SHARING_ROLES = [ , , ] \n 
USER_MANAGEMENT_ROLES = SHARING_ROLES + [ ] \n 
_DEFAULT_SHARING_ROLES = SHARING_ROLES [ : ] \n 
principal = get_principals ( ) . get ( name ) \n 
lg for lg in context . local_groups \n 
LocalGroup ( context , name , unicode ( group_name ) ) \n 
filters . append ( func . lower ( col ) . like ( value ) ) \n 
bcrypt . hashpw ( password . encode ( ) , hashed . encode ( ) ) ) \n 
browser . open ( . format ( BASE_URL ) ) \n 
AUTHORS = open ( os . path . join ( here , ) ) . read ( ) \n 
fixed_boxes ) : \n 
resolve_percentages ( box , ( containing_block . width , containing_block . height ) ) \n 
box , _ , _ , _ , _ = block_container_layout ( \n 
skip_stack = None , device_size = device_size , page_is_empty = False , \n 
list_marker_layout ( context , box ) \n 
hypothetical_position = box . position_y + collapsed_margin \n 
box_width = box . margin_width ( ) if outer else box . border_width ( ) \n 
max_right_bound -= box . margin_right \n 
shape . position_y + shape . margin_height ( ) \n 
urlpatterns = patterns ( , \n 
obj1 , obj2 = qs \n 
n1 = Normal . objects . language ( ) . get ( pk = self . normal_id [ 1 ] ) \n 
shared_field = NEW_SHARED , translated_field = NEW_TRANSLATED \n 
NORMAL [ 2 ] . shared_field ] ) \n 
ja = Normal . objects . language ( ) . get ( pk = en . pk ) \n 
AggregateModel . objects . language ( "en" ) . create ( number = 0 , translated_number = 0 ) \n 
shared_contains_two = Q ( shared_field__contains = ) \n 
normal_one = Q ( normal_field = STANDARD [ 1 ] . normal_field ) \n 
shared_one = Q ( normal__shared_field = NORMAL [ STANDARD [ 1 ] . normal ] . shared_field ) \n 
translated_one_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 1 ] . normal ] . translated_field [ translated_two_en = Q ( normal__translated_field = NORMAL [ STANDARD [ 2 ] . normal ] . translated_field [ \n 
qs = manager . filter ( shared_one & ~ translated_two_en ) \n 
Normal . objects . language ( ) . complex_filter , \n 
analytics . track ( user_id , "Activate" , { \n 
sublime_plugin . WindowCommand . __init__ ( self , * args , ** kwargs ) \n 
"show_response" , { "title" : title , "text" : text } ) \n 
ensure_ascii = False \n 
syntax = "Packages/JavaScript/JSON.tmLanguage" ) \n 
scroll = self . settings . scroll_size \n 
"show_panel" , { "panel" : "output.elasticsearch" } ) \n 
panel . set_read_only ( True ) \n 
400 : RequestError , \n 
TestConfigFileSource . ConcreteConfigFileSource ) \n 
ReferenceReachabilityTester . TwoWayScopeReferenceAttacher . attach ( self . _scope_tree ) \n 
declaring_id_node [ REFERECED_FLAG ] = True \n 
node_type = NodeType ( node [ ] ) \n 
is_set_cmd = excmd_node [ ] [ ] . get ( ) in SetCommandFamily \n 
option_name = re . match ( , option_expr ) . group ( 0 ) \n 
is_valid = option_name not in AbbreviationsIncludingInvertPrefix \n 
stderr . setFormatter ( logging . Formatter ( \n 
level = level if level else os . environ . get ( , ) \n 
g_s = g0 * g_c * g_R * g_D * g_T * g_M \n 
g_T = ( ( TK - TL ) * ( TH - TK ) ** alpha_T ) / ( ( T0 - TL ) * ( TH - T0 ) ** alpha_T ) \n 
r_a = AeroReist ( um , zm , z0 , d ) \n 
r_s = SurfResist ( g0 , S , D , Tc , SM , SM0 ) \n 
LE = ( delta * Rn + ( rho_a * cP * D ) / r_a ) / ( delta + gamma * ( 1.0 + r_s / r_a ) ) \n 
glClearColor ( * background_color ) \n 
glScissor ( x , y , width , height ) \n 
glOrtho ( x , x + width , y , y + height , - 1 , 1 ) \n 
glNormal3f ( 0 , 1. , 0 ) \n 
glVertex ( n , n , p ) \n 
companies = [ path for path in paths \n 
and os . path . exists ( os . path . join ( folder , path , ) ) ] \n 
folder = os . path . join ( root_folder , , , ) \n 
scripts = [ , \n 
pro2 . predict ( ) from collections import OrderedDict \n 
"dimensions." % ( self . __class__ . __name__ , shape ) ) \n 
only = set ( tag for tag , value in tags . items ( ) if value ) \n 
crop1 = [ None , , , ] \n 
outs = [ o . eval ( ) for o in outs ] \n 
crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n 
cropping = [ ] * 2 ) \n 
desired_result_0 = numpy . concatenate ( [ x0 [ : , : 2 ] , x1 [ : , : 2 ] ] , axis = 0 ) \n 
desired_result_1 = numpy . concatenate ( [ x0 [ : 4 , : ] , x1 [ : 4 , : ] ] , axis = 1 ) \n 
inputs = [ theano . shared ( a ) , \n 
theano . shared ( b ) ] \n 
mask = os . urandom ( 4 ) if mask else None \n 
masking_key = mask , fin = 1 ) . build ( ) \n 
fin = fin ) . build ( ) \n 
flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n 
pdc = req . get_options ( ) [ ] \n 
bdc = req . get_options ( ) . get ( , False ) \n 
decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n 
rules = . join ( req . requires ( ) ) . strip ( ) \n 
domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
auth_headers = req . headers_in . get ( , [ ] ) \n 
set_remote_user ( req , ah_data [ 1 ] , domain ) \n 
dict = json . loads ( request . data . decode ( ) ) \n 
rv = self . app . delete ( . format ( id ) ) \n 
configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n 
ocr_file = join ( dir , ) \n 
expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n 
tuples . append ( ( os . path . join ( root , pair [ 0 ] ) , np . int32 ( pair [ 1 ] ) ) ) \n 
val_list = load_image_list ( args . val , args . root ) \n 
mean_image = pickle . load ( open ( args . mean , ) ) \n 
cropwidth = 256 - model . insize \n 
left = random . randint ( 0 , cropwidth - 1 ) \n 
image /= 255 \n 
val_batch_pool = [ None ] * args . val_batchsize \n 
perm = np . random . permutation ( len ( train_list ) ) \n 
batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n 
read_image , ( path , True , False ) ) \n 
val_count = val_loss = val_accuracy = 0 \n 
duration = time . time ( ) - val_begin_at \n 
mean_error = 1 - val_accuracy * args . val_batchsize / 50000 \n 
serializers . save_hdf5 ( args . outstate , optimizer ) \n 
boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n 
LOGGING . info ( . format ( board_name , result ) ) \n 
is_py2 = _ver [ 0 ] == 2 \n 
is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n 
is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n 
strategy = zlib . Z_DEFAULT_STRATEGY ) : \n 
secure = self . secure \n 
e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n 
train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = 15 , max_nr_sent = 1000 ) \n 
pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( , ) ) \n 
total = np . zeros ( self . features . n_feats ) \n 
scores = self . features . compute_scores ( feats , self . weights ) \n 
t0 = 1.0 / ( sigma * eta0 ) \n 
marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n 
score_corr += scores [ h , m ] \n 
features = self . add_final_features ( sequence , sequence . y [ - 1 ] , features ) \n 
node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n 
edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n 
y_name = self . dataset . y_dict . get_label_name ( y ) \n 
feat_name = "prev_tag:%s::%s" % ( y_prev_name , y_name ) \n 
point_geom . AddPoint ( point [ 0 ] , point [ 1 ] ) \n 
longitudes = [ 100 , 110 , 120 , 130 , 140 ] \n 
point_1 . AddPoint ( longitudes [ 0 ] , latitudes [ 0 ] , elevation ) \n 
di = np . array ( [ x for x in range ( projected_X . shape [ 1 ] ) ] ) \n 
projected_X = np . c_ [ ids , projected_X ] \n 
clusterer . fit ( inverse_x [ : , 1 : ] ) \n 
complex [ "meta" ] = self . projection \n 
json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex ~~ k2e [ k ] = e \n 
width_js = "%s" % width_html \n 
new_settings [ interface ] [ ] [ % protocol ] = server \n 
setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n 
fn = self . view . file_name ( ) . encode ( "utf_8" ) \n 
compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n 
file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n 
partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n 
imported_vars = imported_vars + m \n 
"params" : [ { \n 
LAT_MAX = + 90.0 \n 
"purple" , "teal" , "lightgray" ] \n 
numrange = None , default = None , max_width = 72 ) : \n 
found_letter . lower ( ) == default . lower ( ) ) ) : \n 
option [ : index ] + show_letter + option [ index + 1 : ] \n 
display_letters . append ( found_letter . upper ( ) ) \n 
default_name = self . colorize ( , default_name ) \n 
prompt_parts . append ( tmpl % default_name ) \n 
matcher = SequenceMatcher ( lambda x : False , a , b ) \n 
b_out . append ( self . colorize ( color , b [ b_start : b_end ] ) ) \n 
variable = % varname \n 
62 : , \n 
_push . update ( { \n 
_readonly = Entity . _readonly | { , } \n 
remove_ids = [ 6 , 7 ] \n 
remove_advertiser_ids = [ 8 , 9 , 10 ] \n 
num_users , num_items = dataset . shape \n 
async_job = view . map_async ( process , tasks , retries = 2 ) \n 
remaining = len ( tasks ) - len ( done ) \n 
num_items , type ( model ) . __name__ , simsfile ) \n 
before = text [ : len ( text ) - len ( like ) ] \n 
Version = namedtuple ( , ) \n 
_defaults = collections . OrderedDict ( [ \n 
rootDirectory = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , , ) \n 
httpd . handle_request ( ) from . import TestEnable # \n 
field_names = tuple ( field_names ) , \n 
arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n 
repr_fmt = . join ( _repr_template . format ( name = name ) \n 
field_defs = . join ( _field_template . format ( index = index , name = name ) \n 
OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n 
xx = Xdf [ ] . values \n 
X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ , ] ) \n 
cd33 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ , , ] ) \n 
X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ ] ) \n 
mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = 0 ) \n 
mouse_data = pandas . read_excel ( data_file , sheetname = 1 , index_col = [ 0 , 1 ] ) \n 
data_efficient [ ] = 1. \n 
exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( \n 
aggregated [ ] = aggregated [ [ , ] ] . mean ( axis = 1 ) \n 
known_pairs = { : [ , , , ] , \n 
drugs_to_genes [ ] . extend ( [ , , , , , \n 
Xtmp [ ] = drug \n 
y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n 
experiments [ ] = [ , , , ] \n 
data_tmp [ "variance" ] = np . var ( data_tmp . values , axis = 1 ) \n 
gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n 
annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n 
Y_cols_to_keep = np . unique ( [ , , , \n 
gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n 
onedupind = np . where ( Y . index . duplicated ( ) ) [ 0 ] [ 0 ] \n 
alldupind = np . where ( Y . index . get_level_values ( 0 ) . values == Y . index [ onedupind ] [ 0 ] ) [ 0 ] \n 
newindex [ onedupind ] = ( newindex [ onedupind ] [ 0 ] , newindex [ onedupind ] [ 1 ] , "nodrug2" ) \n 
Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
mouse_genes = Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
all_genes = get_V3_genes ( None , None ) return np . setdiff1d ( all_genes , mouse_genes ) \n 
option_value = self . cfg . migrate [ self . option_name ] \n 
search_opts_tenant = kwargs . get ( , { } ) \n 
tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n 
quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n 
quot_default_dst [ item_quot ] ) \n 
tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n 
tnt_id in filter_tenants_ids_list ] \n 
instance [ ] [ ] , instance [ ] [ ] ) \n 
vol [ ] , storage_resource . get_status , , \n 
inst_name = libvirt_instance_name ) ) \n 
dst = instance_image_path ( instance_id ) ) \n 
interface . find ( ) ) \n 
mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n 
element . attrib = { attr : value } \n 
rr . run ( copy . format ( src_file = source_object . path , \n 
max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n 
scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n 
file_name = config . rollback_params [ ] [ ] \n 
pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n 
o2 = C ( 2 ) \n 
org_tag = request . user . get_profile ( ) . org_tag \n 
featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon \n 
httpresponse_kwargs = { : kwargs . pop ( , None ) } \n 
is_testing = in sys . argv \n 
up_time = end_time - self . start_time \n 
remaining_time = self . count_down_total - datetime . timedelta ( seconds = ( int ( up_time ) ) ) \n 
abort_time = time . time ( ) + timeout \n 
elif not stanza . getID ( ) : \n 
_ID = ` ID ` \n 
__description__ = , \n 
REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n 
BaseField . __init__ ( self , ** kwargs ) \n 
is_list = not hasattr ( items , ) \n 
object_map [ ( collection , doc . id ) ] = doc \n 
_cls = doc . _data . pop ( , None ) \n 
81.4471435546875 , \n 
23.61432859499169 \n 
invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n 
Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n 
Parent ( name = ) . save ( ) \n 
echo_payload = Struct ( "echo_payload" , \n 
Padding ( 2 ) , \n 
IpAddress ( "host" ) , \n 
Bytes ( "echo" , 8 ) , \n 
dest_unreachable_code = Enum ( Byte ( "code" ) , \n 
Enum ( Byte ( "type" ) , \n 
Switch ( "payload" , lambda ctx : ctx . type , \n 
"63646566676869" ) . decode ( "hex" ) \n 
cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n 
intps [ ] = dest_target . vlan \n 
router , interface = ri . split ( ) \n 
cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n 
soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n 
soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n 
symmetric = p2ps . symmetricPath or False sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , \n 
crt = nsa . Criteria ( criteria . version , schedule , sd ) \n 
tc = json . load ( open ( tcf ) ) \n 
source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n 
end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n 
connection_id , active , version_consistent , version , timestamp = yield d_down \n 
scheduler = digits . scheduler . Scheduler ( config_value ( ) , True ) \n 
row_index = int ( params [ ] [ 0 ] ) \n 
char_index = int ( params [ ] [ 0 ] ) - 1 \n 
comparator = comparators . index ( params [ ] [ 0 ] ) - 1 \n 
truth = ( cmp ( ord ( current_character ) , test_char ) == comparator ) \n 
start_response ( , [ ( , ) ] ) \n 
CHARSET = [ chr ( x ) for x in xrange ( 32 , 127 ) ] \n 
obj_struct [ ] = [ int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) ] \n 
mpre = np . concatenate ( ( [ 0. ] , prec , [ 0. ] ) ) \n 
difficult = np . array ( [ x [ ] for x in R ] ) . astype ( np . bool ) \n 
det = [ False ] * len ( R ) \n 
npos = npos + sum ( ~ difficult ) \n 
class_recs [ imagename ] = { : bbox , \n 
confidence = np . array ( [ float ( x [ 1 ] ) for x in splitlines ] ) \n 
sorted_ind = np . argsort ( - confidence ) \n 
BB = BB [ sorted_ind , : ] \n 
image_ids = [ image_ids [ x ] for x in sorted_ind ] \n 
bb = BB [ d , : ] . astype ( float ) \n 
BBGT = R [ ] . astype ( float ) \n 
iymin = np . maximum ( BBGT [ : , 1 ] , bb [ 1 ] ) \n 
iw = np . maximum ( ixmax - ixmin + 1. , 0. ) \n 
uni = ( ( bb [ 2 ] - bb [ 0 ] + 1. ) * ( bb [ 3 ] - bb [ 1 ] + 1. ) + \n 
rec = tp / float ( npos ) \n 
prec = tp / ( tp + fp + 1e-10 ) \n 
scale = strip_mantissa ( maxval ) / float ( 1 << ( bits - sign - 1 ) ) \n 
ary = np . around ( ary * ( 1.0 / scale ) ) . astype ( np . int64 ) \n 
f2 -= dif \n 
slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - 1 , K ) ) \n 
K , P , Q , N = E . shape \n 
qSlice = [ fconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
slicedE = E [ : , p , q , : ] \n 
rcp3 = 1.0 / 3.0 \n 
t3 = I [ 1 , : ] + I [ 3 , : ] * 4.0 \n 
T1 = np . empty ( ( 3 , 3 ) ) \n 
Fw = np . empty ( ( D , D , C , K ) ) \n 
sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n 
O [ k , p0 : p1 , q0 : q1 , n ] = Out [ 0 : plen , 0 : qlen ] \n 
start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n 
trans = ( 2 , 2 ) \n 
E = np . random . uniform ( - 1.0 , 1.0 , dimO ) \n 
xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n 
xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n 
updat_direct ( I , E , Ud , padding , strides ) \n 
updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n 
BranchNode , SkipNode , LRN , ColorNoise ) \n 
img_set_options = dict ( repo_dir = args . data_dir , \n 
subset_pct = 0.09990891117239205 ) \n 
do_transforms = False , ** img_set_options ) \n 
test = ImageLoader ( set_name = , scale_range = ( 256 , 384 ) , shuffle = False , \n 
Pooling ( 3 , strides = 2 ) , \n 
activation = Rectlin ( ) , padding = 1 ) , \n 
Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n 
Dropout ( keep = 1.0 ) , \n 
Affine ( nout = 1000 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( - 7 ) , activation = Softmax ( ) ) ] \n 
weight_sched = Schedule ( [ 22 , 44 , 65 ] , ( 1 / 250. ) ** ( 1 / 3. ) ) \n 
opt_gdm = GradientDescentMomentum ( 0.01 / 10 , 0.9 , wdecay = 0.0005 , schedule = weight_sched , \n 
opt_biases = GradientDescentMomentum ( 0.02 / 10 , 0.9 , schedule = Schedule ( [ 44 ] , 0.1 ) , \n 
valmetric = TopKMisclassification ( k = 5 ) \n 
nifm_rng = [ 8 ] \n 
fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n 
bsz = inp . shape [ - 1 ] \n 
check_inds = check_inds [ 0 : ncheck ] \n 
inpa = inp . get ( ) . reshape ( inp_lshape ) \n 
outshape = ( inp_lshape [ 0 ] , \n 
be . output_dim ( inp_lshape [ 2 ] , fshape [ 1 ] , padding , strides [ 1 ] , pooling = True ) , \n 
inp_lshape [ - 1 ] ) \n 
inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , 0 : , 0 : , : ] \n 
out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n 
NervanaObject . be . bsz = batch_size \n 
inshape = ( nifm , in_sz , in_sz ) \n 
src = "img/file-icon.jpg" , ** kw ) ] \n 
render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( component_to_update = + self . comp_id , \n 
ajax . py2js ( self . crop_height ( ) ) \n 
local_handler = getattr ( self , , None ) \n 
genie2 . client . wrapper . RetryPolicy ( \n 
tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) ) \n 
tagging . add_argument ( , , dest = , action = conf_action ( context . ami ) , help = \n 
BASE_URL = . format ( FQDN ) \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config ] \n 
test_account . role_name = "TEST_ACCOUNT" \n 
all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n 
_container_child_objects = ( , ) \n 
_recommended_attrs = ( ( ( , np . ndarray , 1 , np . dtype ( ) ) , \n 
errstr = . format ( errno , pszMsgBuffer . value ) \n 
supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n 
readable_objects = [ Segment ] \n 
read_params = { Segment : [ ] } \n 
labels . append ( str ( pData . value ) ) \n 
ea . labels = np . array ( labels , dtype = ) \n 
dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes total_read += pdwContCount . value \n 
anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n 
pData = np . zeros ( ( dwDataBufferSize ) , dtype = ) \n 
waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo left_sweep = nsample / 2. / float ( pdwSegmentInfo . dSampleRate ) * pq sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n 
ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n 
neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n 
dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n 
times = pData * pq . s \n 
clone_object , TEST_ANNOTATIONS ) \n 
range ( len ( TEST_ANNOTATIONS ) ) ] ) \n 
file_datetime = get_fake_value ( , datetime , seed = 0 ) \n 
file_origin = get_fake_value ( , str ) \n 
attrs1 = { : file_datetime , \n 
res21 = get_fake_values ( Segment , annotate = True , seed = 0 ) \n 
res22 = get_fake_values ( , annotate = True , seed = 0 ) \n 
targ0 = get_fake_value ( , datetime , seed = seed + 0 ) \n 
targ4 = get_fake_value ( , str , \n 
seed = seed + 4 , obj = Segment ) \n 
childobjs = ( , , \n 
children = ( self . sigs1a + self . sigarrs1a + \n 
"analogsignals" : self . nchildren ** 2 , \n 
"epocharrays" : self . nchildren , "eventarrays" : self . nchildren , \n 
"analogsignalarrays" : self . nchildren } \n 
targdict = { : 5 } ) \n 
res6 = filterdata ( data , name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = filterdata ( data , { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res9 = filterdata ( data , { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res11 = filterdata ( data , name = self . epcs2 [ 1 ] . name , targdict = { : 5 } ) \n 
targ = [ self . epcs1a [ 1 ] ] \n 
res3 = filterdata ( data , [ { : 1 } , { : 2 } ] ) \n 
res4 = filterdata ( data , { : 1 } , i = 2 ) \n 
res5 = filterdata ( data , [ { : 1 } ] , i = 2 ) \n 
ann = pretty ( ann ) . replace ( , ) \n 
unit_with_sig = np . array ( [ 0 , 2 , 5 ] ) \n 
rcgs = [ RecordingChannelGroup ( name = , \n 
RecordingChannelGroup ( name = , \n 
t_start = 0. , t_stop = 10 ) \n 
st . unit = all_unit [ j ] \n 
sampling_rate = 1000. * pq . Hz , \n 
newseg = seg . construct_subsegment_by_unit ( all_unit [ : 4 ] ) \n 
ind2 = self . unit2 . channel_indexes [ 0 ] \n 
result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n 
targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n 
result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n 
json_content = json_content . decode ( "utf-8" ) . replace ( , ) \n 
Image1 = StatisticMap ( name = , collection = self . Collection1 , file = , Image1 . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image1 . save ( ) \n 
fname = os . path . basename ( os . path . join ( self . test_path , ) ) file_dict = { : SimpleUploadedFile ( fname , zip_file . read ( ) ) } \n 
Image2ss = StatisticMap ( name = , collection = self . Collection3 , file = Image2ss . file = SimpleUploadedFile ( , file ( os . path . join ( self . test_path , Image2ss . save ( ) \n 
acc_new = rho * acc + ( 1 - rho ) * g ** 2 \n 
gradient_scaling = T . sqrt ( acc_new + epsilon ) \n 
py_x = softmax ( T . dot ( h2 , w_o ) ) \n 
w_h = init_weights ( ( 784 , 625 ) ) \n 
noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , 0.2 , 0.5 ) \n 
y_x = T . argmax ( py_x , axis = 1 ) \n 
cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n 
updates = RMSprop ( cost , params , lr = 0.001 ) \n 
train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n 
predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n 
settings . DATABASE_CONFIG_DICT [ ] ) \n 
TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n 
SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n 
MIDDLEWARE_CLASSES += ( , ) \n 
INTERNAL_IPS = ( , , ) \n 
redirect_url = request . POST . get ( ) or \n 
icon = self . get_plugin_icon ( ) , \n 
memoryprofiler_act . setEnabled ( is_memoryprofiler_installed ( ) ) \n 
wdir , args = None , None \n 
use_colors = self . get_option ( , True ) ) \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n 
expected_zip_content = [ "manifest.json" , "sd_bl.bin" , "sd_bl.dat" ] \n 
sd_req = [ 0x1000 , 0xffff ] , \n 
pkg_name = os . path . join ( self . work_directory , "mypackage.zip" ) \n 
manifest = self . p . unpack_package ( os . path . join ( self . work_directory , pkg_name ) , unpacked_dir ) \n 
style = wx . richtext . RE_MULTILINE , value = ) \n 
fgSizer1 . Add ( self . lblChangelt , 0 , wx . ALL , 5 ) \n 
sbThreshold . Add ( fgSizer1 , 0 , wx . EXPAND , 5 ) \n 
bsValueThresh . Add ( sbThreshold , 1 , 0 , 5 ) \n 
cbGapTimeChoices = [ u"second" , u"minute" , u"hour" , u"day" ] \n 
fmt24hr = True , spinButton = self . sbBefore , oob_color = ) \n 
bsButtons . Add ( self . btnOK , 1 , wx . ALL | wx . EXPAND , 5 ) \n 
mat ( op2 . INC , ( elem_node [ op2 . i [ 0 ] ] , \n 
elem_node [ op2 . i [ 1 ] ] ) ) , \n 
partition_size = NUM_ELE / 2 , \n 
statusBarAx . barh ( [ 0 ] , [ 100.0 * expNumber / len ( data . getPaths ( ) ) ] , \n 
fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n 
meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n 
lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n 
json_data = self . _send_request ( , url , params = params ) \n 
mkdir ( env . hosts_data . log_path ( ) ) \n 
StringIO ( env . hosts_data . celery_supervisor_config ( ) ) , \n 
env . hosts_data . celery_supervisor_config_path ( ) , \n 
rmdir ( env . hosts_data . celery_supervisor_config_path ( ) , sudo_access = True ) \n 
sudo ( . format ( env . hosts_data . application_name ( ) ) ) \n 
collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n 
libraries = [ "sodium" ] , \n 
number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n 
has_default_value = False , default_value = _b ( "" ) , \n 
PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n 
__module__ = \n 
tstream = BytearrayStream ( istream . read ( self . length ) ) \n 
opts , args = parser . parse_args ( sys . argv [ 1 : ] ) \n 
"{0}" . format ( uid ) ) \n 
"located." . format ( path ) \n 
discover_versions . DiscoverVersionsResponsePayload , ** kwargs ) \n 
sqltypes . Base . metadata . create_all ( self . engine ) \n 
enums . OpaqueDataType . NONE , \n 
binascii . hexlify ( self . bytes_a ) , enums . OpaqueDataType . NONE ) \n 
Session = sessionmaker ( bind = self . engine ) \n 
get_obj = session . query ( OpaqueObject ) . filter ( \n 
ManagedObject . unique_identifier == obj . unique_identifier \n 
0 ) ) \n 
expected_mo_names . append ( sqltypes . ManagedObjectName ( expected_names [ 1 ] , \n 
expected_names = [ first_name , added_name ] \n 
] } , \n 
types . MethodType ( _lib_dir_option , None , MSVCCompiler ) ) \n 
setup ( ** kwds ) \n 
intersphinx_mapping = { : None } \n 
new_w = int ( width * wrat ) \n 
im . getbbox ( ) , Image . BICUBIC ) \n 
resize_image ( os . path . abspath ( os . path . join ( , dest + ) ) ) \n 
f_x = Float ( 0.0 , iotype = "out" ) \n 
doe_c = [ 0.1 , 0.2 , 0.3 , 0.5 , 0.7 , 0.8 , 0.9 ] + doe_e \n 
responses = ( , ) , nfi = self . nfi ) ) \n 
sigma_cok = np . array ( [ d . sigma for d in sim_cok . mm_checker . case_outputs . meta_model . f_x ] ) \n 
actual = sim_k . mm_checker . case_outputs . model . f_x \n 
predicted_cok - 2 * sigma_cok , facecolor = , alpha = 0.2 ) \n 
newsetupfile = os . path . join ( os . path . dirname ( setupfile ) , \n 
srcdir = os . path . abspath ( os . path . expanduser ( srcdir ) ) . replace ( , ) \n 
cmd . extend ( [ , destdir ] ) \n 
list ( newfiles ) ) \n 
destdir = os . path . abspath ( os . path . expanduser ( options . destdir ) ) \n 
all_names . extend ( [ prefix + name \n 
lnames = [ prefix + rec for rec in driver [ ] ] \n 
driver_grp = self . _inp [ ] [ driver_name ] \n 
iteration_grp = self . _inp [ ] [ driver_name ] [ iteration_case_name ] \n 
info = self . read_iteration_case_from_hdf5 ( self . _inp , driver_name , iteration_case_name ) yield info \n 
sleep_time = Float ( 0.0 , iotype = , desc = ) \n 
accuracy = Float ( 1.0e-6 , iotype = , \n 
iprint = Enum ( 0 , [ 0 , 1 , 2 , 3 ] , iotype = , \n 
output_filename = Str ( , iotype = , \n 
la = max ( m , 1 ) \n 
gg = zeros ( [ la ] , ) \n 
dg = zeros ( [ la , n + 1 ] , ) \n 
mineq = m - meq + 2 * ( n + 1 ) \n 
lsq = ( n + 1 ) * ( ( n + 1 ) + 1 ) + meq * ( ( n + 1 ) + 1 ) + mineq * ( ( n + 1 ) + 1 ) \n 
lsi = ( ( n + 1 ) - meq + 1 ) * ( mineq + 2 ) + 2 * mineq \n 
lsei = ( ( n + 1 ) + mineq ) * ( ( n + 1 ) - meq ) + 2 * meq + ( n + 1 ) \n 
slsqpb = ( n + 1 ) * ( n / 2 ) + 2 * m + 3 * n + 3 * ( n + 1 ) + 1 \n 
lw = lsq + lsi + lsei + slsqpb + n + m \n 
ljw = max ( mineq , ( n + 1 ) - meq ) \n 
_iodict = { : , : } \n 
state [ ] = { } \n 
key = ( addr_type , addr , proxy . _authkey ) \n 
address = ( ip_addr , 0 ) \n 
access = addr if addr_type == else addr_type \n 
manager = ObjectManager ( self , address , authkey = proxy . _authkey , \n 
match_dict = self . _alltraits ( ** metadata ) \n 
childname , _ , restofpath = traitpath . partition ( ) \n 
mdict . setdefault ( , t . __class__ . __name__ ) \n 
expr = compile ( assign , assign , mode = ) \n 
tstamp = % ( now . year , now . month , now . day , now . hour , now . minute ) \n 
entry_pts = [ ( self , name , _get_entry_group ( self ) ) ] \n 
root_start = root_start + 1 if root_start >= 0 else 0 \n 
root_pathname += \n 
Container . _bases ( type ( obj ) , names ) \n 
names . append ( % ( cls . __module__ , cls . __name__ ) ) \n 
_get_entry_group . group_map = [ \n 
pprint . pprint ( dict ( [ ( n , str ( v ) ) \n 
** metadata ) ] ) , \n 
io_attr [ ] = \n 
_redirect_streams ( ofile . fileno ( ) ) \n 
leftover = arr_size % num_divisions \n 
sizes [ : leftover ] += 1 \n 
offsets [ 1 : ] = numpy . cumsum ( sizes ) [ : - 1 ] \n 
z1 = Float ( 0. , iotype = ) \n 
z_store = Array ( [ 0. , 0. ] , iotype = ) \n 
ssa_F = Array ( [ 0.0 ] , iotype = ) \n 
ssa_dG = Array ( [ [ 0.0 , 0.0 ] , [ 0.0 , 0.0 ] ] , iotype = ) \n 
arr_out = Array ( [ 1. , 2. , 3. ] , iotype = , units = ) \n 
arg [ ] = np . array ( [ 3.1 ] ) \n 
jacs [ ] = np . array ( [ [ 100.0 , 101 , 102 , 103 ] , \n 
assert_rel_error ( self , J [ 3 ] [ 0 ] , 3.0 , 1e-5 ) \n 
newval = _getformat ( val ) % val \n 
newline = re . sub ( self . reg , sub . replace_array , line ) \n 
fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n 
j2 = self . current_row + rowend + 1 \n 
ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n 
num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n 
num_float = ToFloat ( Combine ( Optional ( sign ) + \n 
Optional ( ee + Optional ( sign ) + digits ) \n 
mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n 
string_text ) ) ) \n 
J [ , ] = - 1.0 \n 
top [ ] = - 7.0 \n 
lhs , op , rhs = _parse_constraint ( expr ) \n 
first , second = ( rhs , lhs ) if op . startswith ( ) else ( lhs , rhs ) \n 
input_graph . add_edges_from ( ( ( start , p ) for p in plist [ 1 : ] ) , \n 
src_idxs = { src : None } \n 
units = [ params_dict [ n ] . get ( ) for n in connected_inputs ] \n 
vals = [ params_dict [ n ] [ ] for n in connected_inputs ] \n 
tname , t = connected_inputs [ i ] , u \n 
correct_src = params_dict [ connected_inputs [ 0 ] ] [ ] \n 
sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n 
full_order = { s . pathname : i for i , s in \n 
enumerate ( self . root . subsystems ( recurse = True ) ) } \n 
ssys = srcs [ 0 ] . rsplit ( , 1 ) [ 0 ] \n 
params_dict , unknowns_dict = self . root . _setup_variables ( ) \n 
is not Component . setup_distrib ) ) : \n 
alloc_derivs = not self . root . fd_options [ ] \n 
dangling_params = sorted ( set ( [ \n 
nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n 
local = True ) \n 
recorders . extend ( grp . ln_solver . recorders ) \n 
conn_comps . update ( [ s . rsplit ( , 1 ) [ 0 ] \n 
noconn_comps = sorted ( [ c . pathname \n 
strong = [ s for s in nx . strongly_connected_components ( graph ) \n 
subs = [ s for s in grp . _subsystems ] \n 
tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - 1 ] ] ) \n 
relstrong [ - 1 ] = [ t [ 1 ] for t in tups ] \n 
nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n 
pbos = [ var for var in vec if vec . metadata ( var ) . get ( ) ] \n 
iteritems ( self . root . _params_dict ) ) : \n 
dv_scale = None , cn_scale = None , sparsity = None ) : \n 
fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n 
usize += len ( idx ) \n 
fwd = mode == \n 
poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = cn_scale , dv_scale \n 
duvec = self . root . dumat [ vkey ] \n 
rhs [ vkey ] [ : ] = 0.0 \n 
isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n 
unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( ) ] \n 
p_size = np . size ( dinputs [ p_name ] ) \n 
dresids . _dat [ u_name ] . val [ idx ] = 1.0 \n 
dunknowns , dresids , ) \n 
jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n 
c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n 
param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( ) ] \n 
to_prom_name [ p ] for p , idxs in param_srcs \n 
unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n 
_assemble_deriv_data ( indep_list , unknown_list , data , \n 
_both_names ( tmeta , to_prom_name ) ) \n 
abs_unames = self . root . _sysdata . to_abs_uname \n 
out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n 
magfor , magrev , magfd , abs1 , abs2 , \n 
rel1 , rel2 ) ) \n 
_pad_name ( , 12 , quotes = False ) \n 
magfd , magfd2 , abs4 , rel4 ) ) \n 
out_stream . write ( str ( Jsub_fd2 ) ) \n 
sqlite_dict_args . setdefault ( , ) \n 
ll_1 = ll_0 + n_samples - k - 1 \n 
D = self . D [ lvl ] \n 
initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n 
y_best = y [ nlevel - 1 ] \n 
+ str ( theta ) ) \n 
Yt = solve_triangular ( C , y , lower = True ) \n 
err2 = np . dot ( err . T , err ) [ 0 , 0 ] \n 
sigma2 = err2 / ( n_samples - p - q ) \n 
detR = ( ( np . diag ( C ) ) ** ( 2. / n_samples ) ) . prod ( ) \n 
rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n 
log10t [ i ] - np . log10 ( thetaL [ 0 ] [ i ] ) } ) \n 
np . log10 ( thetaU [ 0 ] [ i ] ) - log10t [ i ] } ) \n 
sol = minimize ( rlf_transform , x0 , method = , \n 
optimal_theta = 10. ** log10_optimal_x \n 
r_t = solve_triangular ( C , r_ . T , lower = True ) \n 
dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n 
r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n 
yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n 
mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n 
u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n 
sigma2_rho = ( sigma2_rho * g ) . sum ( axis = 1 ) \n 
MSE [ : , i ] = sigma2_rho * MSE [ : , i - 1 ] + Q_ / ( 2 * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) ) + self . sigma2 [ i ] * ( u_ ** 2 ) . sum ( axis = 0 ) \n 
n_features = np . zeros ( nlevel , dtype = int ) \n 
n_samples_y [ i ] = y [ i ] . shape [ 0 ] \n 
Y_pred , MSE = self . model . predict ( [ new_x ] ) \n 
X , Y = self . _fit_adapter ( X , Y ) \n 
Y = [ np . array ( y ) for y in reversed ( Y ) ] \n 
newdata = np . array ( parsed [ : ] ) \n 
icc . DB_USER ) , shell = True ) \n 
instance_db_name , shell = True ) \n 
customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n 
default_projector = Projector . objects . get ( pk = 1 ) \n 
reverse ( , args = [ ] ) ) \n 
yield ConfigVariable ( \n 
{ : , : } , { : , : } ) \n 
validators = ( validator_for_testing , ) ) \n 
generate_username . return_value = \n 
serializer = UserFullSerializer ( context = { : view } ) \n 
#domain... #localhost... \n 
USERNAME_REGEX = re . compile ( , re . I ) \n 
RouteDistinguisher . TYPE_IP_LOC , None , \n 
10000 + label ) \n 
nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n 
route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n 
nlri . prefix , label ) \n 
set ( self . importRTs ) ) ) > 0 ) \n 
newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n 
prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n 
"readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n 
REACTORNAME = DEFAULT_REACTORS . get ( platform . system ( ) , ) \n 
set_reactor = lambda : reactor \n 
SIGNALS = dict ( ( k , v ) for v , k in signal . __dict__ . iteritems ( ) if v . startswith ( ) and not v . startswith ( ) ) \n 
pargs = ( self . name , self . label , self . reactor ) \n 
* pargs , ** pkwargs \n 
logdir = env . pop ( , os . path . join ( os . path . sep , ) ) \n 
masksignals = bool ( env . pop ( , True ) ) \n 
usetty = bool ( env . pop ( , ) ) \n 
maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 1 ] \n 
hasattr ( os , "devnull" ) and os . devnull or "/dev/null" , \n 
droned . logging . logToDir ( \n 
conversation . say ( contextSummary , useHTML = False ) \n 
moduleProvides ( IDroneDService ) #requirement \n 
hour = property ( lambda foo : 3600 ) \n 
watchDict = property ( lambda s : SERVICECONFIG . wrapped . get ( , { } ) ) \n 
model = gm . throat_surface_area . cylinder ) \n 
pores = network . find_connected_pores ( throats , flatten = False ) \n 
C0 = network [ ] [ pores , 0 ] \n 
P = phase [ pore_P ] / 100000 \n 
a1 = - 1 / b \n 
a2 = ( R * T + b * P ) / ( a * b ) \n 
a3 = - P / ( a * b ) \n 
coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n 
density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n 
comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
OpenPNM . Phases . GenericPhase ( network = self . net , \n 
components = [ comp1 , comp2 ] ) \n 
phase . set_component ( comp2 , mode = ) \n 
best_seq = fd [ x ] . sequence \n 
best_id , best_seq , best_qual = rep_info [ pb_id ] \n 
_id_ = "{0}|{1}|{2}" . format ( pb_id , coords [ best_id ] , best_id ) \n 
iter = BioReaders . GMAPSAMReader ( gmap_sam_filename , True , query_len_dict = transfrag_len_dict ) \n 
TmpRec = namedtuple ( , [ , , , , , , ] ) \n 
compressed_records_pointer_dict = defaultdict ( lambda : [ ] ) \n 
check_ids_unique ( fa_or_fq_filename , is_fq = is_fq ) \n 
fusion_candidates = find_fusion_candidates ( sam_filename , bs . transfrag_len_dict , min_locus_coverage \n 
pbid1 , groups1 = line . strip ( ) . split ( ) \n 
pbid2 , groups2 = f . readline ( ) . strip ( ) . split ( ) \n 
f_group . write ( "{0}\\t{1}\\n" . format ( pbid1 [ : pbid1 . rfind ( ) ] , "," . join ( group ) ) ) \n 
group_info [ pbid1 [ : pbid1 . rfind ( ) ] ] = list ( group ) \n 
d1 . update ( d [ ] ) \n 
fusion_main ( args . input , args . sam , args . prefix , \n 
is_fq = args . fq , allow_extra_5_exons = args . allow_extra_5exon , \n 
skip_5_exon_alt = False , prefix_dict_pickle_filename = args . prefix_dict_pickle_filename , min_locus_coverage = args . min_locus_coverage , min_locus_coverage_bp = args . min_locus_coverage_bp min_total_coverage = args . min_total_coverage , \n 
raw = self . f . readline ( ) . strip ( ) . split ( ) \n 
iden = float ( raw [ 3 ] ) \n 
_qStart , qAln = self . f . readline ( ) . strip ( ) . split ( ) \n 
_sStart , sAln = self . f . readline ( ) . strip ( ) . split ( ) [ : 2 ] \n 
missed_q = missed_q * 1. / r . qLength , \n 
ece_penalty , ece_min_len ) : \n 
heading = % ( current_indent , , self . heading ) \n 
section = self . _Section ( self , self . _current_section , heading ) \n 
invocations = [ get_invocation ( action ) ] \n 
action_usage = format ( optionals + positionals , groups ) \n 
text_width = self . _width - self . _current_indent \n 
line_len += len ( part ) + 1 \n 
part = % ( option_string , args_string ) \n 
indent = * self . _current_indent \n 
help_width = self . _width - help_position \n 
action_width = help_position - self . _current_indent - 2 \n 
sup . __init__ ( option_strings = [ ] , dest = name , help = help ) \n 
arg_strings = values [ 1 : ] \n 
args_str = . join ( [ repr ( arg ) for arg in args if arg is not None ] ) \n 
type_func = self . _registry_get ( , action . type , action . type ) \n 
conflict_string = . join ( [ option_string \n 
in conflicting_actions ] ) \n 
super_init ( description = description , ** kwargs ) \n 
"""instead""" , DeprecationWarning ) \n 
superinit ( description = description , \n 
default_prefix + , default_prefix * 2 + , \n 
conflicts . extend ( group_actions [ i + 1 : ] ) \n 
action , option_string , explicit_arg = option_tuple \n 
option_string = char + explicit_arg [ 0 ] \n 
new_explicit_arg = explicit_arg [ 1 : ] or None \n 
action_tuples . append ( ( action , args , option_string ) ) \n 
selected_patterns = arg_strings_pattern [ start : ] \n 
extras . extend ( arg_strings [ stop_index : ] ) \n 
OPTIONAL : _ ( ) , \n 
pattern = . join ( [ self . _get_nargs_pattern ( action ) \n 
short_option_prefix = option_string [ : 2 ] \n 
tup = action , option_string , short_explicit_arg \n 
not action . option_strings ) : \n 
vulnerability = obj [ ] [ ] [ ] [ ] [ ] \n 
apikey = common . apikey ( sessionKey , args [ 0 ] , debug ) \n 
device = pandevice . base . PanDevice ( args [ 0 ] , api_key = apikey ) \n 
rebalance_backoff_ms = 2 * 1000 , \n 
uuid = uuid4 ( ) \n 
"" . join ( traceback . format_tb ( tb ) ) ) \n 
kazoo_kwargs = { : timeout / 1000 } \n 
p_to_str = lambda p : . join ( [ str ( p . topic . name ) , str ( p . leader . id ) , str ( p . id ) ] ) \n 
idx = participants . index ( consumer_id or self . _consumer_id ) \n 
parts_per_consumer = len ( all_parts ) // len ( participants ) \n 
remainder_ppc = len ( all_parts ) % len ( participants ) \n 
num_parts = parts_per_consumer + ( 0 if ( idx + 1 > remainder_ppc ) else 1 ) \n 
log . debug ( , [ p_to_str ( p ) for p in new_partitions ] ) \n 
id_ = get_string ( self . _consumer_id ) \n 
path = self . _topic_path , slug = partition_slug ) ) \n 
HZCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 ) \n 
ISO2022CNCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n 
ISO2022JPCharLenTable = ( 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ) \n 
logging . basicConfig ( level = logging . INFO ) \n 
tasa . __version__ , sys . version ) ) \n 
type = lambda w : w . partition ( ) [ : : 2 ] , \n 
worker_class_name = args . worker [ 1 ] or \n 
str ( job ) [ : 50 ] ) \n 
processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n 
color = models . CharField ( max_length = 6 , validators = [ color_regex ] , help_text = \n 
first_name = models . CharField ( max_length = 64 ) \n 
role = models . CharField ( max_length = 17 , choices = ROLE_CHOICES ) \n 
phone_work = models . CharField ( max_length = 15 , validators = [ phone_regex ] , blank = True ) \n 
job_title = models . CharField ( max_length = 128 , blank = True ) \n 
category = models . CharField ( max_length = 21 , choices = CATEGORY_CHOICES , help_text = description = models . CharField ( max_length = 256 , blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n 
acronym = models . CharField ( max_length = 20 , unique = True , help_text = category = models . CharField ( max_length = 9 , choices = CATEGORY_CHOICES , help_text = jurisdiction = models . CharField ( max_length = 64 , help_text = description = models . TextField ( blank = True , help_text = reference = models . URLField ( blank = True , help_text = ) \n 
description = models . CharField ( max_length = 256 , blank = True , help_text = \n 
business_criticality = models . CharField ( max_length = 9 , choices = BUSINESS_CRITICALITY_CHOICES , blank platform = models . CharField ( max_length = 11 , choices = PLATFORM_CHOICES , blank = True , null = True ) \n 
lifecycle = models . CharField ( max_length = 8 , choices = LIFECYCLE_CHOICES , blank = True , null = True ) \n 
user_records = models . PositiveIntegerField ( blank = True , null = True , help_text = revenue = models . DecimalField ( max_digits = 15 , decimal_places = 2 , blank = True , null = True , help_text = external_audience = models . BooleanField ( default = False , help_text = internet_accessible = models . BooleanField ( default = False , help_text = requestable = models . NullBooleanField ( default = True , help_text = _ ( \n 
override_dcl = models . IntegerField ( choices = DATA_CLASSIFICATION_CHOICES , blank = True , null = True , help_text override_reason = models . TextField ( blank = True , help_text = \n 
threadfix = models . ForeignKey ( ThreadFix , blank = True , null = True , help_text = threadfix_team_id = models . PositiveIntegerField ( blank = True , null = True , help_text = threadfix_application_id = models . PositiveIntegerField ( blank = True , null = True , help_text = \n 
asvs_level = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = asvs_level_percent_achieved = models . PositiveIntegerField ( blank = True , null = True , help_text = asvs_doc_url = models . URLField ( blank = True , help_text = ) \n 
asvs_level_target = models . IntegerField ( choices = ASVS_CHOICES , blank = True , null = True , help_text = \n 
organization = models . ForeignKey ( Organization , help_text = people = models . ManyToManyField ( Person , through = , blank = True ) \n 
delta = self . created_date - timezone . now ( ) \n 
person = models . ForeignKey ( Person , help_text = ) \n 
environment_type = models . CharField ( max_length = 4 , choices = ENVIRONMENT_CHOICES , help_text = description = models . TextField ( blank = True , help_text = testing_approved = models . BooleanField ( default = False , help_text = \n 
location = models . URLField ( help_text = notes = models . TextField ( blank = True , help_text = \n 
role_description = models . CharField ( max_length = 128 , blank = True , help_text = notes = models . TextField ( blank = True , help_text = \n 
start_date = models . DateField ( help_text = ) \n 
open_date = models . DateTimeField ( blank = True , null = True , help_text = close_date = models . DateTimeField ( blank = True , null = True , help_text = duration = models . DurationField ( blank = True , null = True ) \n 
metrics = managers . ActivityTypeMetrics . from_queryset ( managers . ActivityTypeQuerySet ) ( ) \n 
token = models . UUIDField ( default = uuid . uuid4 , editable = False ) \n 
activities = models . ManyToManyField ( ActivityType , limit_choices_to = { : True } ) \n 
opener . addheaders = [ ( , ) ] \n 
False = 0 \n 
option_pattern = chr ( 0 ) * 8 \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = len ( s ) - 9 \n 
DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n 
"first_commits" , "second_commits" ] ) \n 
behind = len ( diverge_commits . second_commits ) > 0 \n 
onerror = lambda function , fpath , excinfo : log . info ( \n 
commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n 
len ( path_components ) == 1 and \n 
entry_name == path_components [ 0 ] ) \n 
lambda entry : self . _repo [ entry . id ] ) \n 
GIT_FILEMODE_LINK : { \n 
iterators = [ self . _repo . walk ( branch . target , sort ) \n 
stop_iteration = [ False for branch in branches ] \n 
second_commit in first_commits ) : \n 
new_commit = Commit ( 2 , 2 , "21111111111" ) \n 
mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n 
to_datetime = True ) == datetime \n 
date = dt . date ( 1970 , 1 , 1 ) \n 
datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n 
internationalizeDocstring = lambda x : x \n 
conf . supybot . drivers . maxReconnectWait ( ) ) \n 
inst . conn . _sock . __class__ is socket . _closedsocket ) : \n 
network_config = getattr ( conf . supybot . networks , self . irc . network ) \n 
vhost = conf . supybot . protocols . irc . vhost ( ) , \n 
trusted_fingerprints = network_config . ssl . serverFingerprints ( ) , \n 
while tb : \n 
frame . f_lineno ) ) \n 
window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n 
shared = request . POST . get ( "shared" , False ) \n 
} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n 
body_member_long_label = _ ( ) + \n 
body_members = _n ( , , 2 ) \n 
has_regions = Region . objects . all ( ) . count ( ) > 1 \n 
bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n 
l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n 
cfg [ ] = datetime . now ( ) \n 
ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n 
fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n 
CreatePDF ( page , result , show_error_as_pdf = True ) \n 
body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n 
PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n 
stamp = request . POST . get ( , sha . hexdigest ( ) ) \n 
CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n 
site_id = Site . objects . get_current ( ) . id , \n 
from_id = int ( request . POST . get ( , - 1 ) ) \n 
to_id = int ( request . POST . get ( , None ) ) \n 
from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n 
inverse = request . REQUEST [ ] == if in request . REQUEST else False \n 
my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n 
community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n 
html += report . render ( calc_context ) \n 
geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n 
max_version = max ( [ d . version for d in districts ] ) \n 
can_undo = max_version > plan . min_version \n 
bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n 
wkt = wkt . replace ( , ) . replace ( , ) \n 
districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n 
locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n 
filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n 
pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n 
panels = display . scorepanel_set . all ( ) . order_by ( ) \n 
writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n 
rows = int ( request . POST . get ( , 10 ) ) \n 
sidx = request . POST . get ( , ) \n 
owner_filter = request . POST . get ( ) ; \n 
body_pk = int ( body_pk ) if body_pk else body_pk ; \n 
search = request . POST . get ( , False ) ; \n 
search_string = request . POST . get ( , ) ; \n 
is_community = request . POST . get ( , False ) == ; \n 
all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n 
all_districts = ( ) \n 
_ ( ) ) \n 
user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n 
"%s_sidebar_demo" % plan . legislative_body . name , \n 
plan . legislative_body . name ) \n 
functions = map ( lambda x : int ( x ) , functions ) \n 
display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n 
version = min ( plan . version , int ( version ) ) \n 
Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n 
TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n 
geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n 
write_page = 0 if write_page == 1 else write_page + 1 \n 
init_sum = 1 if i == 0 else 0 \n 
mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
write_addr += mesh_size * DSIZE \n 
sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n 
16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n 
10 ) \n 
optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n 
truenode = replaceUndefined ( tree . truenode , termname ) \n 
codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n 
analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n 
constlist = optimizer . getConstlist ( ) \n 
interval = m . Parameter ( , 16 ) \n 
led ( led + 1 ) , \n 
SingleStatement ( SystemTask ( , , led ) ) \n 
y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n 
z . output ( , valid = , ready = ) \n 
xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
zdata_orig = m . WireLike ( ports [ ] , name = ) \n 
params = m . connect_params ( main ) , \n 
reset_stmt . append ( ydata_orig ( 0 ) ) \n 
nclk ( clk ) , \n 
send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n 
receive ( , zdata , zvalid , zready , waitnum = 50 ) \n 
If ( AndList ( zvalid , zready ) ) ( \n 
Systask ( , , zdata_orig ) \n 
count = m . Reg ( , width = 32 , initval = 0 ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n 
uut = m . Instance ( mkLed ( ) , , \n 
rslt = m . Wire ( , retwidth , signed = True ) \n 
tmpval [ 0 ] ( rslt ) , \n 
vtypes . If ( rst ) ( \n 
ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n 
m . Instance ( mult , , ports = ports ) \n 
stdout = subprocess . PIPE ) . stdout \n 
line = [ l for l in sout ] [ 0 ] \n 
round ( wmean , prec ) , "+-" , round ( wstd , prec ) ) \n 
size = stop - start ) \n 
rndbase = numpy . random . randint ( self . nrows , size = niter ) \n 
rng = [ - 1000 , - 1000 ] \n 
benchtime , stones = prof . run ( \n 
db . rng = [ - rng / 2 , rng / 2 ] \n 
MB_ = 1024 * KB_ \n 
markers = [ , , , , , , , , , ] \n 
memcpyw = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyr" ] . append ( memcpyr ) \n 
ratio = float ( line . split ( ) [ - 1 ] ) \n 
xlim ( 0 , xmax ) \n 
ylim ( 0 , None ) \n 
grid ( True ) \n 
legend ( [ p [ 0 ] for p in plots \n 
savefig ( outfile , dpi = 64 ) \n 
help = , ) \n 
parser . add_option ( , , action = , \n 
show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n 
options . xmax else None ) \n 
hdfarray . attrs . object = { "a" : 32.1 , "b" : 1 , "c" : [ 1 , 2 ] } \n 
addr = hex ( id ( self ) ) \n 
node_manager . registry . pop ( pathname , None ) \n 
oldpathname , self . _v_pathname ) \n 
recursive = False , _log = False , ** kwargs ) \n 
% node . _v_pathname ) \n 
or pathname . startswith ( mypathname + ) ) : \n 
newarr = self . h5file . create_array ( , , [ 1 ] ) \n 
with self . assertRaises ( tables . UndoRedoError ) : \n 
"/othergroup1/othergroup2/othergroup3" not in self . h5file ) \n 
var2 = BoolCol ( dflt = 0 , pos = 2 ) \n 
None , nrows ) \n 
populateTable ( self . h5file . root , ) \n 
new_node = self . h5file . copy_children ( \n 
, , recursive = 1 ) \n 
setattr ( attrs , , 11 ) \n 
delattr ( attrs , ) \n 
arr . _v_attrs . foo = \n 
assert response . header ( ) == \n 
CHANGES = open ( os . path . join ( here , ) , encoding = "utf-8" ) . read ( ) \n 
tests_require = requires + [ ] , \n 
user_id = Column ( Integer , primary_key = True ) \n 
username = Column ( Unicode ( 20 ) , unique = True ) \n 
hits = Column ( Integer , default = 0 ) \n 
_password = Column ( , Unicode ( 60 ) ) \n 
Column ( , Integer , ForeignKey ( ) ) \n 
target_id = Column ( Integer , ForeignKey ( ) ) \n 
comments = relation ( , cascade = "delete" , \n 
author = relation ( User , cascade = "delete" , backref = ) \n 
tags = relation ( Tag , secondary = ideas_tags , backref = ) \n 
voted_users = relation ( User , secondary = voted_users , lazy = , \n 
total_votes = column_property ( ( hits + misses ) . label ( ) ) \n 
query = query . filter ( cls . target == None ) . order_by ( order_by ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
json . loads ( entry [ 2 ] . payload ) , \n 
: 5 \n 
registry . content = DummyContentRegistry ( ) \n 
ep = DummyFunction ( True ) \n 
name_node . validator ( node [ ] , filename ) \n 
schema [ ] . missing = colander . null \n 
title = appstruct [ ] or None \n 
mimetype = appstruct [ ] or USE_MAGIC \n 
filedata = tempstore . get ( uid , { } ) \n 
resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n 
new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n 
request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n 
dirname , filename = os . path . split ( context . path ) \n 
response . content_type = mt or \n 
getattr ( SkipCase ( ) , ) ) \n 
iterator . __class__ . __name__ ) ) \n 
parts = super ( newbytes , self ) . splitlines ( keepends ) \n 
pos = self . rfind ( sub , * args ) \n 
replaced_builtins = . split ( ) \n 
expression = . join ( [ "name=\'{0}\'" . format ( name ) for name in replaced_builtins ] ) \n 
touch_import_top ( , name . value , node ) \n 
retcode = main ( [ self . textfilename ] ) \n 
v = self . visit ( node . values [ i ] ) \n 
props . update ( self . _class_props [ p ] ) \n 
kwargs_init = [ % ( x . split ( ) [ 0 ] , x . split ( ) [ 0 ] ) for x in kwargs ] \n 
else : nargs = node . args . args \n 
kwargs . append ( % ( a , default_value ) ) \n 
offset = len ( node . args . args ) - len ( node . args . defaults ) \n 
varargs = [ % n for n in range ( 16 ) ] \n 
buffer += % self . indent ( ) \n 
arg_name = args = None \n 
comp . append ( self . visit ( node . comparators [ i ] ) ) \n 
testtime = time ( ) - starttime \n 
primes_per_sec = len ( seq ) * ( 1.0 / testtime ) \n 
b = range ( 1 , 10 ) \n 
w1 = threading . start_webworker ( worker , ( seq , , ) ) \n 
TestError ( in seq ) \n 
del self . face_groups [ : ] \n 
mtllib_path = os . path . join ( model_path , data [ 0 ] ) \n 
vertex = ( float ( x ) , float ( y ) , float ( z ) ) \n 
tex_coord = ( float ( s ) , float ( t ) ) \n 
indices = ( int ( vi ) - 1 , int ( ti ) - 1 , int ( ni ) - 1 ) \n 
glBindTexture ( GL_TEXTURE_2D , material . texture_id ) \n 
glPixelStorei ( GL_UNPACK_ALIGNMENT , 1 ) \n 
glNormal3fv ( normals [ ni ] ) \n 
picture = pygame . image . load ( picture_file ) . convert ( ) \n 
screen . blit ( picture , ( - picture_pos . x , picture_pos . y ) ) \n 
time_passed_seconds = time_passed / 1000.0 \n 
picture_pos += scroll_direction * scroll_speed * time_passed_seconds \n 
VERSION = open ( "version.txt" ) . readline ( ) . strip ( ) \n 
DOWNLOAD_URL = DOWNLOAD_BASEURL + "dubbo-client-%s-py2.7.egg" % VERSION \n 
long_description = open ( "README.md" ) . read ( ) , \n 
install_requires = [ "kazoo>=2.0" , "python-jsonrpc>=0.7.3" ] , \n 
field = models . BooleanField ( default = False ) , \n 
args = [ in_path , user_out_path ] , env = [ "PATH=" + os . environ . get ( "PATH" , "" ) use_sandbox = True , use_nobody = True ) \n 
print_skip = 5 , * args , ** kwargs ) : \n 
error = np . max ( np . abs ( new_v - v ) ) \n 
ac = ( a_0 - c ) / 2.0 \n 
R = - R \n 
B = np . array ( [ [ 0. ] , \n 
Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n 
Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = 1e-12 ) \n 
Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n 
group = h5f . createGroup ( "/" , ) \n 
global ctr \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NYSE/" ) \n 
filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , filtered_names ) \n 
stock_data = np . loadtxt ( path + stock + ".csv" , np . float , None , "," , None , 1 , use_cols ) \n 
pkl . dump ( stock_data , f , - 1 ) \n 
startday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
t = map ( int , sys . argv [ 2 ] . split ( ) ) \n 
historic = dataobj . get_data ( timestamps , symbols , "close" ) \n 
alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols ~~ alloc [ ] = 1 - alloc [ symbols [ 0 ] ] \n 
output = open ( sys . argv [ 3 ] , "wb" ) \n 
stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , stocksAtThisPath \n 
@ memoize_default ( None , evaluator_is_first_arg = True ) \n 
param_str = _search_param_in_docstr ( func . raw_doc , str ( param . get_name ( ) ) ) \n 
patterns = [ re . compile ( p % re . escape ( param_str ) ) \n 
it = ( evaluator . execute ( d ) for d in definitions ) \n 
tok = parsed . module . subscopes [ 0 ] . statements [ 0 ] . _token_list [ 2 ] \n 
__slots__ = ( "graphVariable" , \n 
( None , \n 
def __init__ ( self , patterns = [ ] , prolog = None ) : \n 
return term . n3 ( ) \n 
. join ( [ + . join ( [ \n 
[ ( "a" , "?b" , 24 ) , ( "?r" , "?c" , 12345 ) , ( v1 , "?c" , 3333 ) , ( u1 , "?c" , 9999 ) ] ) \n 
unittest . TextTestRunner ( verbosity = 3 ) . run ( suite ) \n 
) . parse ( "http://www.w3.org/People/Berners-Lee/card.rdf" ) \n 
graph . get_context ( URIRef ( ) \n 
bob . set ( FOAF . name , Literal ( "Bob" ) ) \n 
print g . serialize ( format = ) \n 
context = self . uriref ( ) or self . nodeid ( ) or self . sink . identifier \n 
from rdflib . query import Result , ResultSerializer , ResultParser \n 
class CSVResultParser ( ResultParser ) : \n 
r . bindings = [ ] \n 
if result . type != "SELECT" : \n 
stream = codecs . getwriter ( encoding ) ( stream ) \n 
vs = [ self . serializeTerm ( v , encoding ) for v in self . result . vars ] \n 
for row in self . result . bindings : \n 
row . get ( v ) , encoding ) for v in self . result . vars ] ) \n 
try : import nose \n 
~~~ argv += DEFAULT_DIRS \n 
nose . run_exit ( argv = finalArgs ) \n 
