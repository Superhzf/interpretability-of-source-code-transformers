\n 
# \n 
from horizon import tabs \n 
class NetworkProfileTab ( tabs . Tab ) : \n 
slug = "network_profile" \n 
template_name = \n 
def get_context_data ( self , request ) : \n 
~~~ return None \n 
~~ ~~ class PolicyProfileTab ( tabs . Tab ) : \n 
slug = "policy_profile" \n 
preload = False \n 
~~ class IndexTabs ( tabs . TabGroup ) : \n 
~~~ slug = "indextabs" \n 
tabs = ( NetworkProfileTab , PolicyProfileTab ) \n 
import weakref \n 
from eventlet import corolocal \n 
class WeakLocal ( corolocal . local ) : \n 
~~~ def __getattribute__ ( self , attr ) : \n 
~~~ rval = corolocal . local . __getattribute__ ( self , attr ) \n 
if rval : \n 
~~~ rval = rval ( ) \n 
~~ return rval \n 
~~ def __setattr__ ( self , attr , value ) : \n 
~~~ value = weakref . ref ( value ) \n 
return corolocal . local . __setattr__ ( self , attr , value ) \n 
~~ ~~ store = WeakLocal ( ) \n 
weak_store = WeakLocal ( ) \n 
strong_store = corolocal . local \n 
import eventlet \n 
eventlet . monkey_patch ( ) \n 
import contextlib \n 
import sys \n 
from oslo . config import cfg \n 
from openstack_dashboard . openstack . common import log as logging \n 
from openstack_dashboard . openstack . common import rpc \n 
from openstack_dashboard . openstack . common . rpc import impl_zmq \n 
CONF = cfg . CONF \n 
CONF . register_opts ( rpc . rpc_opts ) \n 
CONF . register_opts ( impl_zmq . zmq_opts ) \n 
def main ( ) : \n 
~~~ CONF ( sys . argv [ 1 : ] , project = ) \n 
logging . setup ( "oslo" ) \n 
with contextlib . closing ( impl_zmq . ZmqProxy ( CONF ) ) as reactor : \n 
~~~ reactor . consume_in_thread ( ) \n 
reactor . wait ( ) \n 
~~ ~~ from enum import IntEnum \n 
from . component import Component \n 
from . object import field \n 
class ReflectionProbeUsage ( IntEnum ) : \n 
~~~ Off = 0 \n 
BlendProbes = 1 \n 
BlendProbesAndSkybox = 2 \n 
Simple = 3 \n 
~~ class ShadowCastingMode ( IntEnum ) : \n 
On = 1 \n 
TwoSided = 2 \n 
ShadowsOnly = 3 \n 
~~ class Renderer ( Component ) : \n 
~~~ enabled = field ( "m_Enabled" , bool ) \n 
lightmap_index = field ( "m_LightmapIndex" ) \n 
materials = field ( "m_Materials" ) \n 
probe_anchor = field ( "m_ProbeAnchor" ) \n 
receive_shadows = field ( "m_ReceiveShadows" , bool ) \n 
reflection_probe_usage = field ( "m_ReflectionProbeUsage" , ReflectionProbeUsage ) \n 
shadow_casting_mode = field ( "m_CastShadows" , ShadowCastingMode ) \n 
sorting_layer_id = field ( "m_SortingLayerID" ) \n 
sorting_order = field ( "m_SortingOrder" ) \n 
use_light_probes = field ( "m_UseLightProbes" , bool ) \n 
lightmap_index_dynamic = field ( "m_LightmapIndexDynamic" ) \n 
lightmap_tiling_offset = field ( "m_LightmapTilingOffset" ) \n 
lightmap_tiling_offset_dynamic = field ( "m_LightmapTilingOffsetDynamic" ) \n 
static_batch_root = field ( "m_StaticBatchRoot" ) \n 
subset_indices = field ( "m_SubsetIndices" ) \n 
@ property \n 
def material ( self ) : \n 
~~~ return self . materials [ 0 ] \n 
~~ ~~ class ParticleSystemRenderMode ( IntEnum ) : \n 
~~~ Billboard = 0 \n 
Stretch = 1 \n 
HorizontalBillboard = 2 \n 
VerticalBillboard = 3 \n 
Mesh = 4 \n 
~~ class ParticleSystemSortMode ( IntEnum ) : \n 
~~~ None_ = 0 \n 
Distance = 1 \n 
OldestInFront = 2 \n 
YoungestInFront = 3 \n 
~~ class MeshRenderer ( Component ) : \n 
~~~ pass \n 
~~ class ParticleRenderer ( Renderer ) : \n 
~~~ camera_velocity_scale = field ( "m_CameraVelocityScale" ) \n 
length_scale = field ( "m_LengthScale" ) \n 
max_particle_size = field ( "m_MaxParticleSize" ) \n 
velocity_scale = field ( "m_VelocityScale" ) \n 
stretch_particles = field ( "m_StretchParticles" ) \n 
~~ class ParticleSystemRenderer ( Renderer ) : \n 
mesh = field ( "m_Mesh" ) \n 
mesh1 = field ( "m_Mesh1" ) \n 
mesh2 = field ( "m_Mesh2" ) \n 
mesh3 = field ( "m_Mesh3" ) \n 
normal_direction = field ( "m_NormalDirection" ) \n 
render_mode = field ( "m_RenderMode" , ParticleSystemRenderMode ) \n 
sort_mode = field ( "m_SortMode" , ParticleSystemSortMode ) \n 
sorting_fudge = field ( "m_SortingFudge" ) \n 
~~ from ConfigParser import * \n 
from StringIO import * \n 
from Log import Log \n 
import datetime \n 
class Config : \n 
~~~ @ staticmethod \n 
def LoadConfig ( ) : \n 
~~~ Config . parser = ConfigParser ( ) \n 
try : \n 
~~~ sconff = open ( CONFIG_FILE , "r" ) \n 
~~ except : \n 
return \n 
~~ sconf = StringIO ( ) \n 
sconf . write ( "[sysconf]\\n" ) \n 
sconf . write ( sconff . read ( ) ) \n 
sconf . seek ( 0 ) \n 
Config . parser . readfp ( sconf ) \n 
sconff . close ( ) \n 
sconf . close ( ) \n 
~~ @ staticmethod \n 
def GetBoardsFile ( ) : \n 
~~~ return BOARDS_FILE \n 
def GetInt ( name , defval ) : \n 
~~~ if ( Config . parser . has_option ( , name ) ) : \n 
~~~ return Config . parser . getint ( , name ) \n 
~~ else : \n 
~~~ return defval \n 
~~ ~~ @ staticmethod \n 
def GetString ( name , defval ) : \n 
~~~ val = Config . parser . get ( , name ) \n 
if ( val [ 0 ] == \'"\' and val . endswith ( \'"\' ) ) : \n 
~~~ val = val [ 1 : - 1 ] \n 
~~ return val . decode ( ) \n 
~~ ~~ ~~ BBS_ROOT = \n 
BBS_XMPP_CERT_FILE = BBS_ROOT + "xmpp.crt" \n 
BBS_XMPP_KEY_FILE = BBS_ROOT + "xmpp.key" \n 
BOARDS_FILE = BBS_ROOT + \n 
STRLEN = 80 \n 
ARTICLE_TITLE_LEN = 60 \n 
BM_LEN = 60 \n 
MAXBOARD = 400 \n 
CONFIG_FILE = BBS_ROOT + \n 
FILENAME_LEN = 20 \n 
OWNER_LEN = 30 \n 
SESSIONID_LEN = 32 \n 
REFRESH_TOKEN_LEN = 128 \n 
NAMELEN = 40 \n 
IDLEN = 12 \n 
MD5PASSLEN = 16 \n 
OLDPASSLEN = 14 \n 
MOBILE_NUMBER_LEN = 17 \n 
MAXCLUB = 128 \n 
MAXUSERS = 20000 \n 
MAX_MSG_SIZE = 1024 \n 
MAXFRIENDS = 400 \n 
MAXMESSAGE = 5 \n 
MAXSIGLINES = 6 \n 
IPLEN = 16 \n 
DEFAULTBOARD = "sysop" \n 
BLESS_BOARD = "happy_birthday" \n 
QUOTED_LINES = 10 \n 
MAXACTIVE = 8000 \n 
USHM_SIZE = MAXACTIVE + 10 \n 
UTMP_HASHSIZE = USHM_SIZE * 4 \n 
UCACHE_SEMLOCK = 0 \n 
LEN_FRIEND_EXP = 15 \n 
SESSION_TIMEOUT = datetime . timedelta ( 30 ) \n 
SESSION_TIMEOUT_SECONDS = 86400 * 30 \n 
XMPP_IDLE_TIME = 300 \n 
XMPP_LONG_IDLE_TIME = 1800 \n 
XMPP_UPDATE_TIME_INTERVAL = 10 \n 
XMPP_PING_TIME_INTERVAL = 60 \n 
PUBLIC_SHMKEY = 3700 \n 
MAX_ATTACHSIZE = 20 * 1024 * 1024 \n 
BMDEL_DECREASE = True \n 
SYSMAIL_BOARD = "sysmail" \n 
ADD_EDITMARK = True \n 
SEARCH_COUNT_LIMIT = 20 \n 
MAIL_SIZE_LIMIT = - 1 \n 
SEC_DELETED_OLDHOME = 3600 * 24 * 3 \n 
SELF_INTRO_MAX_LEN = 800 \n 
import re \n 
import os \n 
import stat \n 
import json \n 
import struct \n 
import time \n 
import Config \n 
import Board \n 
import Post \n 
import BoardManager \n 
from Util import Util \n 
from errors import * \n 
DEFAULT_DIGEST_LIST_COUNT = 20 \n 
class DigestItem : \n 
~~~ def __init__ ( self , basepath ) : \n 
~~~ self . basepath = basepath \n 
self . title = \n 
self . host = \n 
self . port = 0 \n 
self . attachpos = 0 \n 
self . fname = \n 
self . mtitle = \n 
self . items = [ ] \n 
self . update_time = 0 \n 
self . id = 0 \n 
self . sysop_only = 0 \n 
self . bms_only = 0 \n 
self . zixia_only = 0 \n 
~~ def IsDir ( self ) : \n 
~~~ try : \n 
~~~ st = os . stat ( self . realpath ( ) ) \n 
return stat . S_ISDIR ( st . st_mode ) \n 
~~~ return False \n 
~~ ~~ def IsFile ( self ) : \n 
return stat . S_ISREG ( st . st_mode ) \n 
~~ ~~ def GetModTime ( self ) : \n 
mtime = st . st_mtime \n 
~~~ mtime = time . time ( ) \n 
~~ return mtime \n 
~~ def names_path ( self ) : \n 
~~~ return "%s/.Names" % self . realpath ( ) \n 
~~ def realpath ( self ) : \n 
~~~ return "%s/%s" % ( Config . BBS_ROOT , self . path ( ) ) \n 
~~ def path ( self ) : \n 
~~~ if ( self . fname ) : \n 
~~~ return "%s/%s" % ( self . basepath , self . fname ) \n 
~~~ return self . basepath \n 
~~ ~~ def CheckUpdate ( self ) : \n 
~~~ stat = os . stat ( self . names_path ( ) ) \n 
if ( stat . st_mtime > self . update_time ) : \n 
~~~ self . LoadNames ( ) \n 
~~ ~~ except : \n 
~~ return True \n 
~~ def LoadNames ( self ) : \n 
~~~ f = open ( self . names_path ( ) , "r" ) \n 
~~ except IOError : \n 
~~~ return 0 \n 
~~ stat = os . fstat ( f . fileno ( ) ) \n 
self . update_time = stat . st_mtime \n 
item = DigestItem ( self . path ( ) ) \n 
hostname = \n 
_id = 0 \n 
bms_only = 0 \n 
sysop_only = 0 \n 
zixia_only = 0 \n 
while ( True ) : \n 
~~~ line = f . readline ( ) \n 
if ( line == "" ) : break \n 
npos = line . find ( "\\n" ) \n 
if ( npos != - 1 ) : line = line [ : npos ] \n 
if ( line [ : 1 ] == ) : \n 
~~~ if ( not self . mtitle ) : \n 
~~~ self . mtitle = line [ 8 : ] \n 
~~ ~~ ~~ result = re . match ( , line ) \n 
if ( result ) : \n 
~~~ key = result . group ( 1 ) \n 
value = result . group ( 2 ) \n 
if ( key == "Name" ) : \n 
~~~ item . title = value \n 
item . attachpos = 0 \n 
~~ elif ( key == "Path" ) : \n 
~~~ if ( value [ : 2 ] == "~/" ) : \n 
~~~ item . fname = value [ 2 : ] \n 
~~~ item . fname = value \n 
~~ if ( item . fname . find ( ".." ) != - 1 ) : \n 
~~~ continue \n 
~~~ bms_only += 1 \n 
~~~ sysop_only += 1 \n 
~~~ zixia_only += 1 \n 
~~ if ( item . fname . find ( "!@#$%" ) != - 1 ) : \n 
~~~ parts = re . split ( , item . fname ) \n 
newparts = [ ] \n 
for part in parts : \n 
~~~ if ( part ) : \n 
~~~ newparts += [ part ] \n 
~~ ~~ hostname = newparts [ 0 ] \n 
item . fname = newparts [ 1 ] \n 
~~~ item . port = int ( newparts [ 2 ] ) \n 
~~~ item . port = 0 \n 
~~ ~~ item . id = _id \n 
_id += 1 \n 
item . bms_only = bms_only \n 
item . sysop_only = sysop_only \n 
item . zixia_only = zixia_only \n 
item . host = hostname \n 
self . items += [ item ] \n 
~~ elif ( key == "Host" ) : \n 
~~~ hostname = value \n 
~~ elif ( key == "Port" ) : \n 
~~~ item . port = int ( value ) \n 
~~ ~~ elif ( key == "Attach" ) : \n 
~~~ item . attachpos = int ( value ) \n 
~~~ item . attachpos = 0 \n 
~~ ~~ ~~ ~~ f . close ( ) \n 
return 1 \n 
~~ def GetItem ( self , user , route , has_perm = False , need_perm = False ) : \n 
~~~ self . CheckUpdate ( ) \n 
if ( self . mtitle . find ( "(BM:" ) != - 1 ) : \n 
~~~ if ( Board . Board . IsBM ( user , self . mtitle [ 4 : ] , ) or user . IsSysop ( ) ) : \n 
~~~ has_perm = True \n 
~~ elif ( need_perm and not has_perm ) : \n 
~~ if ( len ( route ) == 0 ) : \n 
~~~ return self \n 
~~ target = route [ 0 ] - 1 \n 
_id = target \n 
if ( _id >= len ( self . items ) ) : \n 
~~ while ( self . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ _id += 1 \n 
~~ ~~ item = self . items [ _id ] \n 
item . mtitle = item . title \n 
if ( len ( route ) == 1 ) : \n 
~~~ return item \n 
~~~ if ( item . IsDir ( ) ) : \n 
~~~ if ( not item . CheckUpdate ( ) ) : \n 
~~ return item . GetItem ( user , route [ 1 : ] , has_perm , need_perm ) \n 
~~ ~~ ~~ def GetRange ( self , user , route , start , end , has_perm = False , need_perm = False ) : \n 
firstitem = self . GetItem ( user , route + [ start ] , has_perm , need_perm ) \n 
if ( not firstitem ) : \n 
~~~ return [ ] \n 
~~ parent = self . GetItem ( user , route , has_perm , need_perm ) \n 
if ( not parent ) : \n 
~~ if ( not parent . IsDir ( ) ) : \n 
~~ result = [ ] \n 
_id = start - 1 \n 
for i in range ( start , end + 1 ) : \n 
~~~ target = i - 1 \n 
if ( _id >= len ( parent . items ) ) : \n 
~~ while ( parent . items [ _id ] . EffectiveId ( user ) < target ) : \n 
~~~ return result \n 
~~ ~~ item = parent . items [ _id ] \n 
result += [ item ] \n 
~~ return result \n 
~~ def EffectiveId ( self , user ) : \n 
~~~ _id = self . id \n 
if ( user . IsSysop ( ) ) : \n 
~~~ return _id \n 
~~ if ( not user . IsSysop ( ) ) : \n 
~~~ _id -= self . sysop_only \n 
~~ if ( not user . IsBM ( ) ) : \n 
~~~ _id -= self . bms_only \n 
~~ if ( not user . IsSECANC ( ) ) : \n 
~~~ _id -= self . zixia_only \n 
~~ return _id \n 
~~ def GetInfo ( self ) : \n 
~~~ info = { } \n 
info [ ] = Util . gbkDec ( self . mtitle ) \n 
info [ ] = Util . gbkDec ( self . title ) \n 
info [ ] = self . attachpos \n 
if ( self . host != ) : \n 
~~~ info [ ] = self . host \n 
info [ ] = self . port \n 
info [ ] = \n 
~~ elif ( self . IsDir ( ) ) : \n 
~~~ info [ ] = \n 
~~ elif ( self . IsFile ( ) ) : \n 
~~ info [ ] = int ( self . GetModTime ( ) ) \n 
return info \n 
~~ def GetInfoForUser ( self , user ) : \n 
~~~ info = self . GetInfo ( ) \n 
info [ ] = self . EffectiveId ( user ) + 1 \n 
~~ def GetAttachLink ( self , session ) : \n 
filename = \n 
for i in range ( 2 ) : \n 
~~~ filename += "%0x" % struct . unpack ( , _hash [ i * 4 : ( i + 1 ) * 4 ] ) \n 
~~ link = "http://%s/bbscon.php?b=xattach&f=%s" % ( session . GetMirror ( Config . Config . GetInt ( , 80 ) ) , filename ) \n 
linkfile = "%s/boards/xattach/%s" % ( Config . BBS_ROOT , filename ) \n 
target = "../../%s" % self . path ( ) \n 
~~~ os . symlink ( target , linkfile ) \n 
~~ return link \n 
~~ ~~ class Digest : \n 
~~~ root = DigestItem ( "0Announce" ) \n 
def __init__ ( self , board , path ) : \n 
~~~ self . board = board \n 
self . path = path \n 
self . root = DigestItem ( self . path ) \n 
def GET ( svc , session , params , action ) : \n 
~~~ if ( session is None ) : raise Unauthorized ( ) \n 
user = session . GetUser ( ) \n 
boardname = svc . get_str ( params , , ) \n 
if ( boardname ) : \n 
~~~ board = BoardManager . BoardManager . GetBoard ( boardname ) \n 
if ( board is None ) : raise NotFound ( % boardname ) \n 
if ( not board . CheckReadPerm ( user ) ) : \n 
~~~ raise NoPerm ( ) \n 
~~ basenode = board . digest . root \n 
has_perm = user . IsDigestMgr ( ) or user . IsSysop ( ) or user . IsSuperBM ( ) \n 
~~~ basenode = Digest . root \n 
has_perm = user . IsDigestMgr ( ) \n 
~~ if ( action == "list" ) : \n 
~~~ route = svc . get_str ( params , ) \n 
start = svc . get_int ( params , , 1 ) \n 
end = svc . get_int ( params , , start + DEFAULT_DIGEST_LIST_COUNT - 1 ) \n 
Digest . List ( svc , basenode , route , start , end , session , has_perm ) \n 
~~ elif ( action == "view" ) : \n 
start = svc . get_int ( params , , 0 ) \n 
count = svc . get_int ( params , , 0 ) \n 
Digest . View ( svc , basenode , route , session , has_perm , start , count ) \n 
~~~ raise WrongArgs ( % action ) \n 
def ParseRoute ( route ) : \n 
~~~ ret = [ ] \n 
items = re . split ( , route ) \n 
items = items [ 1 : ] \n 
for item in items : \n 
~~~ ret += [ int ( item ) ] \n 
~~~ raise WrongArgs ( % item ) \n 
~~ ~~ return ret \n 
def List ( svc , basenode , route , start , end , session , has_perm ) : \n 
~~~ route_array = Digest . ParseRoute ( route ) \n 
parent = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
~~~ raise WrongArgs ( % route ) \n 
~~ items = basenode . GetRange ( session . GetUser ( ) , route_array , start , end , has_perm ) \n 
result = { } \n 
result [ ] = parent . GetInfoForUser ( session . GetUser ( ) ) \n 
result [ ] = len ( items ) \n 
result_list = [ ] \n 
~~~ result_list += [ item . GetInfoForUser ( session . GetUser ( ) ) ] \n 
~~ result [ ] = result_list \n 
svc . writedata ( json . dumps ( result ) ) \n 
def View ( svc , basenode , route , session , has_perm , start , count ) : \n 
item = basenode . GetItem ( session . GetUser ( ) , route_array , has_perm ) \n 
if ( not item ) : \n 
~~ if ( not item . IsFile ( ) ) : \n 
~~ result = { } \n 
result [ ] = item . GetInfoForUser ( session . GetUser ( ) ) \n 
postinfo = Post . Post ( item . realpath ( ) , None ) \n 
( result [ ] , result [ ] ) = postinfo . GetContent ( start , count ) \n 
attachlist = postinfo . GetAttachListByType ( ) \n 
result [ ] = attachlist [ 0 ] \n 
result [ ] = attachlist [ 1 ] \n 
if ( attachlist [ 0 ] or attachlist [ 1 ] ) : \n 
~~~ result [ ] = item . GetAttachLink ( session ) \n 
~~ svc . writedata ( json . dumps ( result ) ) \n 
~~ ~~ import time \n 
import UserManager \n 
import UserInfo \n 
from Session import Session \n 
import UCache \n 
import MsgBox \n 
import xmpp \n 
import modes \n 
import Util \n 
import traceback \n 
from xmpp . features import NoRoute \n 
__disco_info_ns__ = \n 
__disco_items_ns__ = \n 
__vcard_ns__ = \n 
STEAL_AFTER_SEEN = 3 \n 
def elem_to_str ( elem ) : \n 
~~ class XMPPServer ( xmpp . Plugin ) : \n 
def __init__ ( self , rosters , host ) : \n 
~~~ self . probed = False \n 
self . _closed = False \n 
self . rosters = rosters \n 
self . _session = None \n 
self . rosters . set_resources ( self . get_resources ( ) ) \n 
self . _fixedjid = UCache . UCache . formalize_jid ( unicode ( self . authJID ) ) \n 
self . _userid = self . _fixedjid . partition ( ) [ 0 ] . encode ( "gbk" ) \n 
if ( not self . rosters . allow_login ( self . authJID . bare ) ) : \n 
self . stream_error ( , ) \n 
if self . authJID . resource [ : - 8 ] != "Resource" and len ( self . authJID . resource ) > 8 : \n 
~~~ routes = self . routes ( self . authJID . bare ) \n 
for route in routes : \n 
~~~ jid = route [ 0 ] \n 
if jid . resource [ : - 8 ] == self . authJID . resource [ : - 8 ] : \n 
~~~ if jid . resource != self . authJID . resource : \n 
route [ 1 ] . stream_error ( , ) \n 
~~ ~~ else : \n 
~~ ~~ ~~ except NoRoute : \n 
~~ self . _user = UserManager . UserManager . LoadUser ( self . _userid ) \n 
if ( self . _user == None ) : \n 
~~ self . _peer_addr = self . getpeername ( ) \n 
self . _session = Session ( self . _user , self . _peer_addr [ 0 ] ) \n 
self . _session . RecordLogin ( ) \n 
self . _userinfo = self . _session . Register ( ) \n 
self . _loginid = self . _session . utmpent \n 
self . _hostname = host \n 
self . bind ( xmpp . ReceivedCloseStream , self . recv_close ) \n 
self . bind ( xmpp . StreamClosed , self . stream_closed ) \n 
self . bind ( xmpp . SentCloseStream , self . sent_close ) \n 
self . rosters . register_conn ( self ) \n 
msgbox = MsgBox . MsgBox ( self . _userid ) \n 
if self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) is None : \n 
~~~ self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msgbox . GetMsgCount ( all = False ) - msgbox . GetUnreadCount ( ) ) \n 
~~ self . check_msg ( ) \n 
~~ def get_loginid ( self ) : \n 
~~~ return self . _loginid \n 
~~ def recv_close ( self ) : \n 
return self . close ( ) \n 
~~ def stream_closed ( self ) : \n 
~~ def sent_close ( self ) : \n 
~~ def close ( self ) : \n 
~~~ if ( self . _closed ) : \n 
~~ self . _closed = True \n 
if ( self . _session ) : \n 
~~~ self . _session . Unregister ( ) \n 
~~ self . unbind_res ( ) \n 
self . rosters . unregister_conn ( self ) \n 
~~ @ xmpp . iq ( ) \n 
def ping ( self , iq ) : \n 
self . refresh ( ) \n 
return self . iq ( , iq ) \n 
~~ @ xmpp . stanza ( ) \n 
def message ( self , elem ) : \n 
to_jid = elem . get ( ) \n 
from_jid = elem . get ( ) \n 
if ( from_jid == None ) : \n 
~~~ return \n 
~~ text_body = None \n 
for child in elem : \n 
~~~ if ( child . tag . endswith ( ) ) : \n 
~~~ text_body = child . text \n 
~~ ~~ if ( text_body == None ) : \n 
~~ ret = self . rosters . send_msg ( from_jid , to_jid , text_body ) \n 
if ( ret <= 0 ) : \n 
errors = { \n 
if ( ret in errors ) : \n 
~~~ elem = self . E . message ( { : to_jid , \n 
: from_jid , \n 
: } , \n 
self . E . body ( errors [ ret ] ) ) \n 
self . recv ( from_jid , elem ) \n 
~~ ~~ ~~ def make_jid ( self , userid ) : \n 
~~~ return "%s@%s" % ( userid , self . _hostname ) \n 
~~ def refresh ( self ) : \n 
~~~ self . _userinfo . freshtime = int ( time . time ( ) ) \n 
self . _userinfo . save ( ) \n 
~~ def ping_result ( self , iq ) : \n 
~~~ self . refresh ( ) \n 
~~ def ping_client ( self ) : \n 
~~~ pingelem = self . E . ping ( xmlns = ) \n 
return self . iq ( , self . ping_result , pingelem ) \n 
~~ except Exception as e : \n 
Log . debug ( traceback . format_exc ( ) ) \n 
return False \n 
~~ ~~ def get_uid ( self ) : \n 
~~~ return self . _user . GetUID ( ) \n 
~~ def recv_msg ( self , from_ , msgtext ) : \n 
~~~ elem = self . E . message ( { : from_ , : unicode ( self . authJID ) } , \n 
self . E . body ( msgtext ) ) \n 
self . recv ( unicode ( self . authJID ) , elem ) \n 
~~ def check_msg ( self ) : \n 
msg_count = msgbox . GetMsgCount ( all = False ) \n 
my_pid = os . getpid ( ) \n 
xmpp_read = self . rosters . get_xmpp_read ( self . _user . GetUID ( ) ) \n 
if xmpp_read > msg_count : \n 
~~~ xmpp_read = 0 \n 
self . rosters . set_xmpp_read ( self . _user . GetUID ( ) , msg_count ) \n 
if xmpp_read < msg_count : \n 
~~~ return xmpp_read \n 
~~~ return - 1 \n 
~~ ~~ def deliver_msg ( self , start ) : \n 
for i in range ( start , msg_count ) : \n 
~~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if msghead . topid == my_pid : \n 
~~~ msgtext = msgbox . LoadMsgText ( msghead ) \n 
self . recv_msg ( self . make_jid ( msghead . id ) , msgtext ) \n 
~~ ~~ ~~ def steal_msg ( self ) : \n 
msg_unread = msgbox . GetUnreadCount ( ) \n 
read_count = msg_count - msg_unread \n 
term_read = self . rosters . get_term_read ( self . get_uid ( ) ) \n 
term_stealed = self . rosters . get_term_stealed ( self . get_uid ( ) ) \n 
all_xmpp = True \n 
new_unread = { } \n 
for i in range ( read_count - 1 , msg_count ) : \n 
~~ msghead = msgbox . LoadMsgHead ( i , all = False ) \n 
if i >= read_count and all_xmpp : \n 
~~~ if msghead . topid == my_pid : \n 
~~~ msgbox . GetUnreadMsg ( ) \n 
~~~ all_xmpp = False \n 
~~ ~~ if msghead . topid == my_pid : \n 
~~~ session = self . rosters . find_session ( self . authJID . bare , msghead . topid ) \n 
if session is None or session . get_mode ( ) != modes . MSG : \n 
~~ if msghead . topid not in new_unread : \n 
new_unread [ msghead . topid ] = i \n 
~~ ~~ final_unread = { } \n 
to_steal = { } \n 
to_steal_begin = msg_count \n 
for pid in term_read : \n 
~~~ if pid in new_unread : \n 
~~~ if new_unread [ pid ] == term_read [ pid ] [ 0 ] : \n 
~~~ final_unread [ pid ] = ( term_read [ pid ] [ 0 ] , term_read [ pid ] [ 1 ] + 1 ) \n 
if final_unread [ pid ] [ 1 ] > STEAL_AFTER_SEEN : \n 
~~~ to_steal [ pid ] = final_unread [ pid ] \n 
if pid in term_stealed : \n 
~~~ steal_begin = max ( final_unread [ pid ] [ 0 ] , term_stealed [ pid ] + 1 ) \n 
~~~ steal_begin = final_unread [ pid ] [ 0 ] \n 
~~ if steal_begin < to_steal_begin : \n 
~~~ to_steal_begin = steal_begin \n 
~~ ~~ ~~ else : \n 
~~~ final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
pass \n 
~~ ~~ for pid in new_unread : \n 
~~~ if pid not in term_read : \n 
final_unread [ pid ] = ( new_unread [ pid ] , 1 ) \n 
~~ ~~ if to_steal : \n 
for i in range ( to_steal_begin , msg_count ) : \n 
msgbox . GetUnreadMsg ( ) \n 
~~ elif msghead . topid in to_steal : \n 
~~~ if msghead . topid not in term_stealed or i > term_stealed [ msghead . topid ] : \n 
msgtext = msgbox . LoadMsgText ( msghead ) \n 
term_stealed [ msghead . topid ] = i \n 
~~ ~~ ~~ ~~ self . rosters . set_term_read ( self . get_uid ( ) , final_unread ) \n 
def presence ( self , elem ) : \n 
if self . authJID == elem . get ( ) : \n 
~~~ if ( elem . get ( ) == None or ( not self . authJID . match_bare ( elem . get ( ) ) ) ) : \n 
~~~ return self . send_presence ( elem ) \n 
~~ ~~ self . recv_presence ( elem ) \n 
~~ def send_presence ( self , elem ) : \n 
direct = elem . get ( ) \n 
if not direct : \n 
~~~ self . rosters . broadcast ( self , elem ) \n 
if elem . get ( ) != : \n 
~~~ self . recv_presence ( elem ) \n 
~~ if not self . probed : \n 
~~~ self . probed = True \n 
self . rosters . probe ( self ) \n 
~~ ~~ elif not self . rosters . send ( self , direct , elem ) : \n 
~~~ self . send ( direct , elem ) \n 
~~ ~~ def recv_presence ( self , elem ) : \n 
if not self . rosters . recv ( self , elem ) : \n 
self . write ( elem ) \n 
~~ ~~ @ xmpp . iq ( ) \n 
def roster ( self , iq ) : \n 
roster = self . rosters . get ( self ) \n 
method = getattr ( self , % iq . get ( ) ) \n 
return method and method ( iq , roster ) \n 
~~ def get_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( { : } ) \n 
for item in roster . items ( ) : \n 
~~~ query . append ( item ) \n 
~~ return self . iq ( , iq , query ) \n 
~~ def set_roster ( self , iq , roster ) : \n 
~~~ query = self . E . query ( xmlns = ) \n 
for item in iq [ 0 ] : \n 
~~~ result = roster . set ( item ) \n 
if result is not None : \n 
~~~ query . append ( result ) \n 
~~ ~~ if len ( query ) > 0 : \n 
~~~ self . push ( roster , query ) \n 
~~ return self . iq ( , iq ) \n 
~~ def push ( self , roster , query ) : \n 
for jid in roster . requests ( ) : \n 
~~~ for ( to , route ) in self . routes ( jid ) : \n 
~~~ route . iq ( , self . ignore , query ) \n 
~~ ~~ ~~ def ignore ( self , iq ) : \n 
def vcard ( self , iq ) : \n 
if iq . get ( ) == : \n 
~~~ if ( iq . get ( ) == None ) : \n 
~~~ target = iq . get ( ) \n 
~~ form_target = UCache . UCache . formalize_jid ( target ) \n 
name = form_target . partition ( ) [ 0 ] \n 
user = UserManager . UserManager . LoadUser ( name ) \n 
info = user . GetInfo ( ) \n 
desc = % ( info [ ] , info [ ] , info [ ] , \n 
info [ ] , info [ ] , info [ ] , info [ ] ) \n 
if ( in info ) : \n 
~~~ desc += "Plan:\\r%s" % ( info [ ] . replace ( , ) ) \n 
~~ vcard = self . E . vCard ( { : } , \n 
self . E ( , name ) , \n 
self . E ( , Util . Util . RemoveTags ( info [ ] ) ) , \n 
self . E ( , Util . Util . RemoveTags ( desc ) ) ) \n 
if ( iq . get ( ) == None ) : \n 
~~~ return self . iq ( , iq , vcard ) \n 
~~~ return self . iq ( , iq , vcard , { : iq . get ( ) } ) \n 
~~ ~~ ~~ @ xmpp . iq ( % __disco_info_ns__ ) \n 
def disco_info ( self , iq ) : \n 
target = iq . get ( ) \n 
if ( target . find ( ) < 0 ) : \n 
~~~ query = self . E . query ( { : __disco_info_ns__ } , \n 
self . E . identity ( { : , \n 
: , \n 
: Config . Config . GetString ( , ) , \n 
} ) ) \n 
features = [ __disco_info_ns__ , __disco_items_ns__ , __vcard_ns__ ] \n 
for feature in features : \n 
~~~ query . append ( self . E . feature ( { : feature } ) ) \n 
~~ ~~ return self . iq ( , iq , query , { : target } ) \n 
~~ @ xmpp . iq ( % __disco_items_ns__ ) \n 
def disco_items ( self , iq ) : \n 
~~~ query = self . E . query ( { : __disco_items_ns__ } ) \n 
~~ return self . iq ( , iq , query , { : target } ) \n 
### \n 
~~ ~~ from __future__ import print_function \n 
from __future__ import unicode_literals \n 
from __future__ import division \n 
from __future__ import absolute_import \n 
from builtins import range \n 
from future import standard_library \n 
standard_library . install_aliases ( ) \n 
PYTHON_VERSION = sys . version_info [ : 3 ] \n 
PY2 = ( PYTHON_VERSION [ 0 ] == 2 ) \n 
if PY2 : \n 
~~~ if PYTHON_VERSION < ( 2 , 7 , 9 ) : \n 
~~~ raise Exception ( ) \n 
~~ ~~ elif PYTHON_VERSION < ( 3 , 4 ) : \n 
~~ import hpOneView as hpov \n 
from pprint import pprint \n 
from hpOneView . common import uri \n 
import hpOneView . profile as profile \n 
def acceptEULA ( con ) : \n 
~~~ con . get_eula_status ( ) \n 
~~~ if con . get_eula_status ( ) is True : \n 
~~~ print ( ) \n 
con . set_eula ( ) \n 
~~ ~~ except Exception as e : \n 
print ( e ) \n 
~~ ~~ def login ( con , credential ) : \n 
~~~ con . login ( credential ) \n 
~~ ~~ def get_eg_uri_from_arg ( srv , name ) : \n 
~~~ if srv and name : \n 
~~~ if name . startswith ( ) and uri [ ] in name : \n 
~~~ return name \n 
~~~ egs = srv . get_enclosure_groups ( ) \n 
for eg in egs : \n 
~~~ if eg [ ] == name : \n 
~~~ return eg [ ] \n 
~~ ~~ ~~ ~~ return None \n 
~~ def get_sht_from_arg ( srv , name ) : \n 
~~~ shts = srv . get_server_hardware_types ( ) \n 
for sht in shts : \n 
~~~ if sht [ ] == name : \n 
~~~ return sht \n 
~~ def define_profile_template ( \n 
srv , \n 
name , \n 
desc , \n 
sp_desc , \n 
server_hwt , \n 
enc_group , \n 
affinity , \n 
hide_flexnics , \n 
conn_list , \n 
fw_settings , \n 
boot , \n 
bootmode ) : \n 
~~~ if conn_list : \n 
~~~ conn = json . loads ( open ( conn_list ) . read ( ) ) \n 
~~~ conn = [ ] \n 
~~ profile_template = srv . create_server_profile_template ( \n 
name = name , \n 
description = desc , \n 
serverProfileDescription = sp_desc , \n 
serverHardwareTypeUri = server_hwt , \n 
enclosureGroupUri = enc_group , \n 
affinity = affinity , \n 
hideUnusedFlexNics = hide_flexnics , \n 
profileConnectionV4 = conn , \n 
firmwareSettingsV3 = fw_settings , \n 
bootSettings = boot , \n 
bootModeSetting = bootmode ) \n 
if in profile_template : \n 
~~~ print ( , profile_template [ ] ) \n 
print ( , profile_template [ ] ) \n 
print ( ) \n 
for connection in profile_template [ ] : \n 
~~~ print ( , connection [ ] ) \n 
print ( , connection [ ] ) \n 
~~ print ( ) \n 
print ( , profile_template [ ] [ ] ) \n 
print ( , profile_template [ ] [ ] , ) \n 
~~~ pprint ( profile_template ) \n 
~~ ~~ def main ( ) : \n 
~~~ parser = argparse . ArgumentParser ( add_help = True , \n 
formatter_class = argparse . RawTextHelpFormatter , \n 
description = ) \n 
parser . add_argument ( , dest = , required = True , \n 
help = ) \n 
parser . add_argument ( , dest = , required = False , \n 
default = , \n 
parser . add_argument ( , dest = , \n 
required = True , \n 
required = False , \n 
required = False , choices = [ , ] , \n 
action = , \n 
nargs = , \n 
choices = [ , , ] , \n 
choices = [ , , , \n 
, ] , \n 
args = parser . parse_args ( ) \n 
credential = { : args . user , : args . passwd } \n 
con = hpov . connection ( args . host ) \n 
srv = hpov . servers ( con ) \n 
sts = hpov . settings ( con ) \n 
if args . proxy : \n 
~~~ con . set_proxy ( args . proxy . split ( ) [ 0 ] , args . proxy . split ( ) [ 1 ] ) \n 
~~ if args . cert : \n 
~~~ con . set_trusted_ssl_bundle ( args . cert ) \n 
~~ login ( con , credential ) \n 
acceptEULA ( con ) \n 
eg_uri = get_eg_uri_from_arg ( srv , args . enc_group ) \n 
sht = get_sht_from_arg ( srv , args . server_hwt ) \n 
fw_settings = profile . make_firmware_dict ( sts , args . baseline ) \n 
boot , bootmode = profile . make_boot_settings_dict ( srv , sht , args . disable_manage_boot , \n 
args . boot_order , args . boot_mode , args . pxe ) \n 
define_profile_template ( srv , \n 
args . name , \n 
args . desc , \n 
args . sp_desc , \n 
sht [ ] , \n 
eg_uri , \n 
args . affinity , \n 
args . hide_flexnics , \n 
args . conn_list , \n 
bootmode ) \n 
~~ if __name__ == : \n 
~~~ import argparse \n 
sys . exit ( main ( ) ) \n 
~~ from __future__ import print_function \n 
~~ ~~ def get_address_pools ( con , srv , types ) : \n 
~~~ if types == or types == : \n 
~~~ vmac = srv . get_vmac_pool ( ) \n 
for key in sorted ( vmac ) : \n 
~~~ print ( . format ( key , vmac [ key ] ) ) \n 
~~ if in vmac : \n 
~~~ for uri in vmac [ ] : \n 
~~~ ranges = con . get ( uri ) \n 
print ( , ranges [ ] ) \n 
~~ ~~ ~~ if types == or types == : \n 
~~~ vwwn = srv . get_vwwn_pool ( ) \n 
for key in sorted ( vwwn ) : \n 
~~~ print ( . format ( key , vwwn [ key ] ) ) \n 
~~ if in vwwn : \n 
~~~ for uri in vwwn [ ] : \n 
~~~ vsn = srv . get_vsn_pool ( ) \n 
for key in sorted ( vsn ) : \n 
~~~ print ( . format ( key , vsn [ key ] ) ) \n 
~~ if in vsn : \n 
~~~ for uri in vsn [ ] : \n 
~~ ~~ ~~ ~~ def main ( ) : \n 
choices = [ , , , ] , default = , \n 
credential = { : args . domain . upper ( ) , : args . user , : args . passwd } \n 
get_address_pools ( con , srv , args . types ) \n 
~~~ import sys \n 
import argparse \n 
~~ ~~ def get_managed_sans ( fcs ) : \n 
~~~ sans = fcs . get_managed_sans ( ) \n 
pprint ( sans ) \n 
~~ def main ( ) : \n 
fcs = hpov . fcsans ( con ) \n 
get_managed_sans ( fcs ) \n 
~~ ~~ def getpolicy ( sts ) : \n 
~~~ policy = sts . get_storage_vol_template_policy ( ) \n 
print ( policy [ ] ) \n 
getpolicy ( sts ) \n 
from __future__ import print_function \n 
__title__ = \n 
__version__ = \n 
__copyright__ = \n 
__license__ = \n 
__status__ = \n 
from hpOneView . common import * \n 
from hpOneView . connection import * \n 
from hpOneView . activity import * \n 
from hpOneView . exceptions import * \n 
class servers ( object ) : \n 
~~~ def __init__ ( self , con ) : \n 
~~~ self . _con = con \n 
self . _activity = activity ( con ) \n 
########################################################################### \n 
~~ def get_connections ( self , filter = ) : \n 
return get_members ( self . _con . get ( uri [ ] + filter ) ) \n 
~~ def get_connection ( self , server ) : \n 
body = self . _con . get ( server [ ] ) \n 
return body \n 
~~ def get_server_by_bay ( self , baynum ) : \n 
~~~ servers = get_members ( self . _con . get ( uri [ ] ) ) \n 
for server in servers : \n 
~~~ if server [ ] == baynum : \n 
~~~ return server \n 
~~ ~~ ~~ def get_server_by_name ( self , name ) : \n 
~~~ if server [ ] == name : \n 
~~ ~~ ~~ def get_available_servers ( self , server_hardware_type = None , \n 
enclosure_group = None , server_profile = None ) : \n 
~~~ filters = [ ] \n 
if server_hardware_type : \n 
~~~ filters . append ( + server_hardware_type [ ] ) \n 
~~ if enclosure_group : \n 
~~~ filters . append ( + enclosure_group [ ] ) \n 
~~ if server_profile : \n 
~~~ filters . append ( + server_profile [ ] ) \n 
~~ query_string = \n 
if filters : \n 
~~~ query_string = + . join ( filters ) \n 
~~ return self . _con . get ( uri [ ] + query_string ) \n 
~~ def get_servers ( self ) : \n 
~~~ return get_members ( self . _con . get ( uri [ ] ) ) \n 
~~ def get_utilization ( self , server ) : \n 
body = self . _con . get ( server [ ] + ) \n 
~~ def get_env_conf ( self , server ) : \n 
~~ def set_server_powerstate ( self , server , state , force = False , blocking = True , \n 
verbose = False ) : \n 
~~~ if state == and force is True : \n 
~~~ powerRequest = make_powerstate_dict ( , ) \n 
~~ elif state == and force is False : \n 
~~ elif state == : \n 
~~ task , body = self . _con . put ( server [ ] + , powerRequest ) \n 
if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 60 , verbose = verbose ) \n 
~~ return task \n 
~~ def delete_server ( self , server , force = False , blocking = True , verbose = False ) : \n 
~~~ if force : \n 
~~~ task , body = self . _con . delete ( server [ ] + ) \n 
~~~ task , body = self . _con . delete ( server [ ] ) \n 
~~ if blocking is True : \n 
~~~ task = self . _activity . wait4task ( task , tout = 600 , verbose = verbose ) \n 
~~ def update_server ( self , server ) : \n 
~~~ task , body = self . _con . put ( server [ ] , server ) \n 
~~ def add_server ( self , server , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , server ) \n 
if in task and task [ ] . startswith ( ) : \n 
~~~ entity = self . _activity . get_task_associated_resource ( task ) \n 
server = self . _con . get ( entity [ ] ) \n 
return server \n 
~~ ~~ return task \n 
~~ def get_server_schema ( self ) : \n 
return self . _con . get ( uri [ ] + ) \n 
~~ def get_bios ( self , server ) : \n 
return self . _con . get ( server [ ] + ) \n 
~~ def get_ilo_sso_url ( self , server ) : \n 
~~ def get_java_remote_console_url ( self , server ) : \n 
~~ def get_remote_console_url ( self , server ) : \n 
~~ def get_server_hardware_types ( self ) : \n 
body = self . _con . get ( uri [ ] ) \n 
return get_members ( body ) \n 
~~ def remove_server_hardware_type ( self , server_hardware_type , force = False , blocking = True , verbose = False ) : \n 
if force : \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] + ) \n 
~~~ task , body = self . _con . delete ( server_hardware_type [ ] ) \n 
~~ def get_server_type_schema ( self ) : \n 
~~ def get_server_hardware_type ( self , server_type ) : \n 
return self . _con . get ( server_type [ ] ) \n 
~~ def set_server_hardware_type ( self , server_hardware_type , name , description ) : \n 
request = make_server_type_dict ( name , description ) \n 
task , body = self . _con . put ( server_hardware_type [ ] , request ) \n 
return task \n 
~~ def create_server_profile ( self , \n 
affinity = , \n 
biosSettings = None , \n 
bootSettings = None , \n 
bootModeSetting = None , \n 
profileConnectionV4 = None , \n 
description = None , \n 
firmwareSettingsV3 = None , \n 
hideUnusedFlexNics = True , \n 
localStorageSettingsV3 = None , \n 
macType = , \n 
name = None , \n 
sanStorageV3 = None , \n 
serialNumber = None , \n 
serialNumberType = , \n 
serverHardwareTypeUri = None , \n 
serverHardwareUri = None , \n 
serverProfileTemplateUri = None , \n 
uuid = None , \n 
wwnType = , \n 
blocking = True , verbose = False ) : \n 
profile = make_ServerProfileV5 ( affinity , biosSettings , bootSettings , \n 
bootModeSetting , profileConnectionV4 , \n 
description , firmwareSettingsV3 , \n 
hideUnusedFlexNics , \n 
localStorageSettingsV3 , macType , name , \n 
sanStorageV3 , serialNumber , \n 
serialNumberType , serverHardwareTypeUri , \n 
serverHardwareUri , \n 
serverProfileTemplateUri , uuid , wwnType ) \n 
task , body = self . _con . post ( uri [ ] , profile ) \n 
if profile [ ] is None : \n 
~~~ tout = 600 \n 
~~~ tout = 3600 \n 
~~~ task = self . _activity . wait4task ( task , tout , verbose = verbose ) \n 
profile = self . _con . get ( entity [ ] ) \n 
return profile \n 
~~ def post_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~ def remove_server_profile ( self , profile , force = False , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile [ ] + ) \n 
~~~ task , body = self . _con . delete ( profile [ ] ) \n 
~~ def get_server_profiles ( self ) : \n 
~~~ body = self . _con . get ( uri [ ] ) \n 
~~ def update_server_profile ( self , profile , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile [ ] , profile ) \n 
~~~ if profile [ ] [ ] is None : \n 
~~ ~~ except Exception : \n 
~~~ task = self . _activity . wait4task ( task , tout = tout , verbose = verbose ) \n 
~~ profileResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileResource [ ] ) \n 
~~ def update_server_profile_from_template ( self , profile , blocking = True , verbose = False ) : \n 
~~~ patch_request = [ { : , : , : } ] \n 
task , body = self . _con . patch ( profile [ ] , patch_request ) \n 
~~ ~~ def get_server_profile_by_name ( self , name ) : \n 
~~~ body = self . _con . get_entity_byfield ( uri [ ] , , name ) \n 
~~ def get_profile_message ( self , profile ) : \n 
message = self . _con . get ( profile [ ] + ) \n 
return message \n 
~~ def get_profile_compliance_preview ( self , profile ) : \n 
return self . _con . get ( profile [ ] + ) \n 
~~ def create_server_profile_template ( \n 
self , \n 
serverProfileDescription = None , \n 
enclosureGroupUri = None , \n 
affinity = None , \n 
hideUnusedFlexNics = None , \n 
blocking = True , \n 
profile_template = make_ServerProfileTemplateV1 ( name , \n 
description , \n 
serverProfileDescription , \n 
serverHardwareTypeUri , \n 
enclosureGroupUri , \n 
profileConnectionV4 , \n 
firmwareSettingsV3 , \n 
bootSettings , \n 
bootModeSetting ) \n 
task , body = self . _con . post ( uri [ ] , profile_template ) \n 
tout = 600 \n 
profile_template = self . _con . get ( entity [ ] ) \n 
return profile_template \n 
~~ def remove_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . delete ( profile_template [ ] ) \n 
~~ return body \n 
~~ def get_server_profile_templates ( self ) : \n 
~~ def get_server_profile_template_by_name ( self , name ) : \n 
~~ def update_server_profile_template ( self , profile_template , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . put ( profile_template [ ] , profile_template ) \n 
~~ profileTemplateResource = self . _activity . get_task_associated_resource ( task ) \n 
profile = self . _con . get ( profileTemplateResource [ ] ) \n 
~~ def get_server_profile_from_template ( self , profile_template ) : \n 
~~~ profile = self . _con . get ( profile_template [ ] + ) \n 
~~ def get_enclosures ( self ) : \n 
~~ def add_enclosure ( self , enclosure , blocking = True , verbose = False ) : \n 
~~~ task , body = self . _con . post ( uri [ ] , enclosure ) \n 
if enclosure [ ] is : \n 
~~ elif enclosure [ ] is None : \n 
enclosure = self . _con . get ( entity [ ] ) \n 
return enclosure \n 
~~ def remove_enclosure ( self , enclosure , force = False , blocking = True , \n 
~~~ task , body = self . _con . delete ( enclosure [ ] + ) \n 
~~~ task , body = self . _con . delete ( enclosure [ ] ) \n 
~~ def create_enclosure_group ( self , associatedLIGs , name , \n 
powerMode = ) : \n 
egroup = make_EnclosureGroupV200 ( associatedLIGs , name , powerMode ) \n 
task , body = self . _con . post ( uri [ ] , egroup ) \n 
~~ def delete_enclosure_group ( self , egroup ) : \n 
~~~ self . _con . delete ( egroup [ ] ) \n 
~~ def get_enclosure_groups ( self ) : \n 
~~ def update_enclosure_group ( self , enclosuregroup ) : \n 
~~~ task , body = self . _con . put ( enclosuregroup [ ] , enclosuregroup ) \n 
~~ def get_pool ( self , pooltype ) : \n 
~~~ body = self . _con . get ( uri [ ] + + pooltype ) \n 
~~ def get_vmac_pool ( self ) : \n 
~~ def get_vwwn_pool ( self ) : \n 
~~ def get_vsn_pool ( self ) : \n 
~~ def get_profile_networks ( self ) : \n 
~~ def get_profile_schema ( self ) : \n 
~~~ return self . _con . get ( uri [ ] ) \n 
~~ def get_profile_available_servers ( self ) : \n 
~~ def get_profile_available_storage_systems ( self ) : \n 
~~ def get_profile_ports ( self ) : \n 
~~ def allocate_pool_ids ( self , url , count ) : \n 
~~~ allocatorUrl = % url \n 
allocatorBody = { : count } \n 
task , body = self . _con . put ( allocatorUrl , allocatorBody ) \n 
~~ def release_pool_ids ( self , url , idList ) : \n 
~~~ collectorUrl = % url \n 
collectorBody = { : idList } \n 
task , body = self . _con . put ( collectorUrl , collectorBody ) \n 
~~ def allocate_range_ids ( self , allocatorUrl , count ) : \n 
~~~ task , body = self . _con . put ( allocatorUrl , { : count } ) \n 
~~ def release_range_ids ( self , collectorUrl , idList ) : \n 
~~~ task , body = self . _con . put ( collectorUrl , { : idList } ) \n 
~~ def enable_range ( self , url ) : \n 
~~~ prange = self . _con . get ( url ) \n 
prange [ ] = True \n 
task , body = self . _con . put ( url , prange ) \n 
~~ def disable_range ( self , url ) : \n 
prange [ ] = False \n 
~~ ~~ from . constants import MILLI_MICROS , SECOND_MICROS , MINUTE_MICROS \n 
import calendar \n 
from datetime import datetime \n 
from dateutil import parser \n 
from dateutil . tz import tzlocal \n 
from . error import TimeConstructionError \n 
from . sanedelta import SaneDelta \n 
import pytz \n 
MICROS_TRANSLATIONS = ( \n 
( ( , , , , ) , MINUTE_MICROS ) , \n 
( ( , , , , ) , SECOND_MICROS ) , \n 
( ( , , , , ) , MILLI_MICROS ) , \n 
( ( , , , , ) , 1 ) ) \n 
MICROS_TRANSLATION_HASH = dict ( ( alt , v ) for k , v in MICROS_TRANSLATIONS for alt in k ) \n 
class SaneTime ( object ) : \n 
def __init__ ( self , * args , ** kwargs ) : \n 
super ( time , self ) . __init__ ( ) \n 
uss = set ( ) \n 
tzs = set ( ) \n 
naive_dt = None \n 
avoid_localize = False \n 
for k , v in kwargs . iteritems ( ) : \n 
~~~ if k in ( , ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( v ) ) \n 
~~ elif k in MICROS_TRANSLATION_HASH : \n 
~~~ uss . add ( MICROS_TRANSLATION_HASH [ k ] * v ) \n 
~~ ~~ args = list ( args ) \n 
if len ( args ) > 2 and len ( args ) <= 8 : \n 
~~~ args = [ datetime ( * args ) ] \n 
~~ if len ( args ) == 2 : \n 
~~~ tzs . add ( SaneTime . to_timezone ( args . pop ( ) ) ) \n 
~~ if len ( args ) == 1 : \n 
~~~ arg = args . pop ( ) \n 
if hasattr ( arg , ) : \n 
~~~ uss . add ( int ( arg ) ) \n 
if hasattr ( arg , ) : tzs . add ( arg . tz ) \n 
~~ elif isinstance ( arg , basestring ) : \n 
~~~ parts = arg . strip ( ) . split ( ) \n 
if len ( parts ) > 1 and parts [ - 1 ] . startswith ( ) : \n 
~~~ tzs . add ( SaneTime . to_timezone ( parts [ - 1 ] [ 1 : ] ) ) \n 
arg = . join ( parts [ : - 1 ] ) \n 
~~ except : pass \n 
~~ utc = arg . endswith ( ) or arg . endswith ( ) \n 
arg = parser . parse ( arg ) \n 
~~~ if utc : \n 
~~~ tzs . add ( pytz . utc ) \n 
arg = arg . replace ( tzinfo = None ) \n 
~~~ arg = arg . replace ( tzinfo = None ) \n 
~~~ avoid_localize = True \n 
arg = arg . astimezone ( pytz . utc ) . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if type ( arg ) == datetime : \n 
~~~ naive_dt = arg \n 
if naive_dt . tzinfo : \n 
~~~ tzs . add ( SaneTime . to_timezone ( str ( naive_dt . tzinfo ) ) ) \n 
naive_dt = naive_dt . replace ( tzinfo = None ) \n 
~~ ~~ ~~ if len ( tzs ) > 1 : \n 
~~ self . tz = len ( tzs ) and tzs . pop ( ) or pytz . utc \n 
if naive_dt : \n 
~~~ if avoid_localize : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( naive_dt ) ) \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( self . tz . localize ( naive_dt ) . astimezone ( pytz . utc ) ) ) \n 
~~ ~~ if len ( uss ) == 0 : \n 
~~~ uss . add ( SaneTime . utc_datetime_to_us ( datetime . utcnow ( ) ) ) \n 
~~ if len ( uss ) > 1 : \n 
~~ self . us = uss . pop ( ) \n 
if len ( args ) > 0 : \n 
~~ ~~ @ property \n 
def ms ( self ) : return self . us / MILLI_MICROS \n 
epoch_milliseconds = epoch_millis = milliseconds = millis = ms \n 
def s ( self ) : return self . us / SECOND_MICROS \n 
epoch_seconds = epoch_secs = seconds = secs = s \n 
def m ( self ) : return self . us / MINUTE_MICROS \n 
epoch_minutes = epoch_mins = minutes = mins = m \n 
def micros ( self ) : return self . us \n 
epoch_microseconds = epoch_micros = microseconds = micros \n 
def tz_name ( self ) : return self . tz . zone \n 
def tz_abbr ( self ) : return self . tz . _tzname \n 
def set_tz ( self , tz ) : \n 
~~~ self . tz = self . __class__ . to_timezone ( tz ) ; return self \n 
~~ def with_tz ( self , tz ) : \n 
~~~ return self . __class__ ( self . us , tz ) \n 
~~ @ property \n 
def _tuple ( self ) : return ( self . us , self . tz ) \n 
def strftime ( self , * args , ** kwargs ) : return self . datetime . strftime ( * args , ** kwargs ) \n 
def __cmp__ ( self , other ) : \n 
~~~ if not hasattr ( other , ) : other = SaneTime ( other ) \n 
return cmp ( self . us , int ( other ) ) \n 
~~ def __hash__ ( self ) : return self . us . __hash__ ( ) \n 
def __add__ ( self , operand ) : \n 
~~~ if not hasattr ( operand , ) : operand = SaneTime ( operand ) \n 
return self . __class__ ( self . us + int ( operand ) , tz = self . tz ) \n 
~~ def __sub__ ( self , operand ) : \n 
if isinstance ( operand , SaneTime ) : return SaneDelta ( self . us - int ( operand ) ) \n 
return self . __add__ ( - int ( operand ) ) \n 
~~ def __mul__ ( self , operand ) : \n 
~~~ return self . us * int ( operand ) \n 
~~ def __div__ ( self , operand ) : \n 
~~~ return self . us / int ( operand ) \n 
~~ def __int__ ( self ) : return int ( self . us ) \n 
def __long__ ( self ) : return long ( self . us ) \n 
def __repr__ ( self ) : return u"SaneTime(%s,%s)" % ( self . us , repr ( self . tz ) ) \n 
def __str__ ( self ) : return unicode ( self ) . encode ( ) \n 
def __unicode__ ( self ) : \n 
~~~ dt = self . datetime \n 
micros = u".%06d" % dt . microsecond if dt . microsecond else \n 
~~ def clone ( self ) : \n 
return self . __class__ ( self . us , self . tz ) \n 
def ny_str ( self ) : \n 
return self . ny_ndt . strftime ( ) \n 
def utc_datetime ( self ) : return SaneTime . us_to_utc_datetime ( self . us ) \n 
utc_dt = utc_datetime \n 
def utc_naive_datetime ( self ) : return self . utc_datetime . replace ( tzinfo = None ) \n 
utc_ndt = utc_naive_datetime \n 
def to_timezoned_datetime ( self , tz ) : return self . utc_datetime . astimezone ( SaneTime . to_timezone ( tz ) ) \n 
def to_timezoned_naive_datetime ( self , tz ) : return self . to_timezoned_datetime ( tz ) . replace ( tzinfo = None ) \n 
def datetime ( self ) : return self . to_timezoned_datetime ( self . tz ) \n 
dt = datetime \n 
def naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( self . tz ) \n 
ndt = naive_datetime \n 
def ny_datetime ( self ) : return self . to_timezoned_datetime ( ) \n 
ny_dt = ny_datetime \n 
def ny_naive_datetime ( self ) : return self . to_timezoned_naive_datetime ( ) \n 
ny_ndt = ny_naive_datetime \n 
def year ( self ) : return self . dt . year \n 
def month ( self ) : return self . dt . month \n 
def day ( self ) : return self . dt . day \n 
def hour ( self ) : return self . dt . hour \n 
def minute ( self ) : return self . dt . minute \n 
def second ( self ) : return self . dt . second \n 
def microsecond ( self ) : return self . dt . microsecond \n 
@ classmethod \n 
def utc_datetime_to_us ( kls , dt ) : \n 
~~~ return calendar . timegm ( dt . timetuple ( ) ) * 1000 ** 2 + dt . microsecond \n 
~~ @ classmethod \n 
def us_to_utc_datetime ( kls , us ) : \n 
~~~ return pytz . utc . localize ( datetime . utcfromtimestamp ( us / 10 ** 6 ) ) . replace ( microsecond = us % 10 ** 6 ) \n 
def to_timezone ( kls , tz ) : \n 
~~~ if not isinstance ( tz , basestring ) : return tz \n 
return pytz . timezone ( tz ) \n 
~~ ~~ def ntime ( * args , ** kwargs ) : \n 
~~~ if args : \n 
~~~ if args [ 0 ] is None : return None \n 
~~ elif kwargs : \n 
~~~ if None in [ v for k , v in kwargs . iteritems ( ) if k != ] : return None \n 
~~ return SaneTime ( * args , ** kwargs ) \n 
~~ time = sanetime = SaneTime \n 
nsanetime = ntime \n 
from tastypie . authorization import Authorization \n 
from openpds . authentication import OAuth2Authentication \n 
from openpds . core . models import Profile , AuditEntry \n 
import settings \n 
import pdb \n 
class PDSAuthorization ( Authorization ) : \n 
~~~ audit_enabled = True \n 
scope = "" \n 
requester_uuid = "" \n 
def requester ( self ) : \n 
~~~ return self . requester_uuid \n 
~~ def trustWrapper ( self , datastore_owner ) : \n 
~~ def is_authorized ( self , request , object = None ) : \n 
~~~ authenticator = OAuth2Authentication ( self . scope ) \n 
if "datastore_owner__uuid" in request . REQUEST : \n 
~~~ authorized = True \n 
token = request . REQUEST [ "bearer_token" ] if "bearer_token" in request . REQUEST else request . META [ "HTTP_BEARER_TOKEN" ] \n 
datastore_owner_uuid = request . REQUEST [ "datastore_owner__uuid" ] \n 
datastore_owner , ds_owner_created = Profile . objects . get_or_create ( uuid = datastore_owner_uuid ) \n 
self . requester_uuid = authenticator . get_userinfo_from_token ( token , self . scope ) \n 
if self . requester_uuid is False or self . requester_uuid is None or len ( self . requester_uuid ) == 0 : \n 
~~~ self . requester_uuid = "not-specified" \n 
authorized = False \n 
~~ self . trustWrapper ( datastore_owner ) \n 
~~~ if ( self . audit_enabled ) : \n 
#pdb.set_trace() \n 
~~~ audit_entry = AuditEntry ( token = token ) \n 
audit_entry . method = request . method \n 
audit_entry . scope = self . scope \n 
audit_entry . purpose = request . REQUEST [ "purpose" ] if "purpose" in request . REQUEST else "" \n 
audit_entry . system_entity_toggle = request . REQUEST [ "system_entity" ] if "system_entity" in request . REQUEST else False \n 
audit_entry . datastore_owner = datastore_owner \n 
audit_entry . requester , created = Profile . objects . get_or_create ( uuid = self . requester_uuid ) \n 
audit_entry . script = request . path \n 
audit_entry . save ( ) \n 
~~~ print e \n 
~~ return authorized \n 
~~ return False \n 
~~ def __init__ ( self , scope , audit_enabled = True ) : \n 
~~~ self . scope = scope \n 
self . audit_enabled = audit_enabled \n 
from django import template \n 
register = template . Library ( ) \n 
class VerbatimNode ( template . Node ) : \n 
~~~ def __init__ ( self , text ) : \n 
~~~ self . text = text \n 
~~ def render ( self , context ) : \n 
~~~ return self . text \n 
~~ ~~ @ register . tag \n 
def verbatim ( parser , token ) : \n 
~~~ text = [ ] \n 
while 1 : \n 
~~~ token = parser . tokens . pop ( 0 ) \n 
if token . contents == : \n 
~~~ break \n 
~~ if token . token_type == template . TOKEN_VAR : \n 
~~~ text . append ( ) \n 
~~ elif token . token_type == template . TOKEN_BLOCK : \n 
~~ text . append ( token . contents ) \n 
if token . token_type == template . TOKEN_VAR : \n 
~~ ~~ return VerbatimNode ( . join ( text ) ) \n 
~~ from django . shortcuts import render_to_response \n 
from django . template import RequestContext \n 
from gevent import monkey ; monkey . patch_all ( ) \n 
import gevent \n 
from ws4py . client . geventclient import WebSocketClient \n 
if __name__ == : \n 
~~~ ws = WebSocketClient ( , protocols = [ , ] ) \n 
ws . connect ( ) \n 
print ( ( ws . receive ( ) , ) ) \n 
def incoming ( ) : \n 
~~~ while True : \n 
~~~ m = ws . receive ( ) \n 
if m is not None : \n 
~~~ m = str ( m ) \n 
print ( ( m , len ( m ) ) ) \n 
if len ( m ) == 35 : \n 
~~~ ws . close ( ) \n 
break \n 
~~ def outgoing ( ) : \n 
~~~ for i in range ( 0 , 40 , 5 ) : \n 
~~~ ws . send ( "*" * i ) \n 
~~ ws . send ( "Foobar" ) \n 
~~ greenlets = [ \n 
gevent . spawn ( incoming ) , \n 
gevent . spawn ( outgoing ) , \n 
] \n 
gevent . joinall ( greenlets ) \n 
~~ import os \n 
from ws4py . framing import Frame , OPCODE_CONTINUATION , OPCODE_TEXT , OPCODE_BINARY , OPCODE_CLOSE , OPCODE_PING , OPCODE_PONG \n 
from ws4py . compat import unicode , py3k \n 
__all__ = [ , , , , \n 
, ] \n 
class Message ( object ) : \n 
~~~ def __init__ ( self , opcode , data = , encoding = ) : \n 
self . opcode = opcode \n 
self . _completed = False \n 
self . encoding = encoding \n 
if isinstance ( data , unicode ) : \n 
~~~ if not encoding : \n 
~~ data = data . encode ( encoding ) \n 
~~ elif isinstance ( data , bytearray ) : \n 
~~~ data = bytes ( data ) \n 
~~ elif not isinstance ( data , bytes ) : \n 
~~ self . data = data \n 
~~ def single ( self , mask = False ) : \n 
mask = os . urandom ( 4 ) if mask else None \n 
return Frame ( body = self . data , opcode = self . opcode , \n 
masking_key = mask , fin = 1 ) . build ( ) \n 
~~ def fragment ( self , first = False , last = False , mask = False ) : \n 
fin = 1 if last is True else 0 \n 
opcode = self . opcode if first is True else OPCODE_CONTINUATION \n 
return Frame ( body = self . data , \n 
opcode = opcode , masking_key = mask , \n 
fin = fin ) . build ( ) \n 
def completed ( self ) : \n 
return self . _completed \n 
~~ @ completed . setter \n 
def completed ( self , state ) : \n 
self . _completed = state \n 
~~ def extend ( self , data ) : \n 
if isinstance ( data , bytes ) : \n 
~~~ self . data += data \n 
~~~ self . data += bytes ( data ) \n 
~~ elif isinstance ( data , unicode ) : \n 
~~~ self . data += data . encode ( self . encoding ) \n 
~~ ~~ def __len__ ( self ) : \n 
~~~ return len ( self . __unicode__ ( ) ) \n 
~~ def __str__ ( self ) : \n 
~~~ if py3k : \n 
~~~ return self . data . decode ( self . encoding ) \n 
~~ return self . data \n 
~~ def __unicode__ ( self ) : \n 
~~ ~~ class TextMessage ( Message ) : \n 
~~~ def __init__ ( self , text = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_TEXT , text ) \n 
def is_binary ( self ) : \n 
def is_text ( self ) : \n 
~~~ return True \n 
~~ ~~ class BinaryMessage ( Message ) : \n 
~~~ def __init__ ( self , bytes = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_BINARY , bytes , encoding = None ) \n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . data ) \n 
~~ ~~ class CloseControlMessage ( Message ) : \n 
~~~ def __init__ ( self , code = 1000 , reason = ) : \n 
~~~ data = b"" \n 
if code : \n 
~~~ data += struct . pack ( "!H" , code ) \n 
~~ if reason is not None : \n 
~~~ if isinstance ( reason , unicode ) : \n 
~~~ reason = reason . encode ( ) \n 
~~ data += reason \n 
~~ Message . __init__ ( self , OPCODE_CLOSE , data , ) \n 
self . code = code \n 
self . reason = reason \n 
~~~ return self . reason . decode ( ) \n 
~~ return self . reason \n 
~~~ return self . reason . decode ( self . encoding ) \n 
~~ ~~ class PingControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data = None ) : \n 
~~~ Message . __init__ ( self , OPCODE_PING , data ) \n 
~~ ~~ class PongControlMessage ( Message ) : \n 
~~~ def __init__ ( self , data ) : \n 
~~~ Message . __init__ ( self , OPCODE_PONG , data ) \n 
~~ ~~ import sys \n 
import base64 \n 
import urllib \n 
from struct import unpack \n 
from threading import Lock \n 
from binascii import hexlify \n 
from urlparse import urlparse \n 
from mod_python import apache \n 
from PyAuthenNTLM2 . ntlm_dc_proxy import NTLM_DC_Proxy \n 
from PyAuthenNTLM2 . ntlm_ad_proxy import NTLM_AD_Proxy \n 
use_basic_auth = True \n 
~~~ from PyAuthenNTLM2 . ntlm_client import NTLM_Client \n 
~~ except ImportError : \n 
~~~ use_basic_auth = False \n 
~~ class CacheConnections : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . _mutex = Lock ( ) \n 
self . _cache = { } \n 
~~~ return len ( self . _cache ) \n 
~~ def remove ( self , id ) : \n 
~~~ self . _mutex . acquire ( ) \n 
( proxy , ts ) = self . _cache . get ( id , ( None , None ) ) \n 
if proxy : \n 
~~~ proxy . close ( ) \n 
del self . _cache [ id ] \n 
~~ self . _mutex . release ( ) \n 
~~ def add ( self , id , proxy ) : \n 
self . _cache [ id ] = ( proxy , int ( time . time ( ) ) ) \n 
self . _mutex . release ( ) \n 
~~ def clean ( self ) : \n 
~~~ now = int ( time . time ( ) ) \n 
self . _mutex . acquire ( ) \n 
for id , conn in self . _cache . items ( ) : \n 
~~~ if conn [ 1 ] + 60 < now : \n 
~~~ conn [ 0 ] . close ( ) \n 
~~ ~~ self . _mutex . release ( ) \n 
~~ def has_key ( self , id ) : \n 
~~~ return self . _cache . has_key ( id ) \n 
~~ def get_proxy ( self , id ) : \n 
proxy = self . _cache [ id ] [ 0 ] \n 
return proxy \n 
~~ ~~ class CacheGroups : \n 
~~ def add ( self , group , user ) : \n 
if not self . _cache . has_key ( group ) : \n 
~~~ self . _cache [ group ] = { } \n 
~~ self . _cache [ group ] [ user ] = int ( time . time ( ) ) \n 
old = [ ] \n 
for group , members in self . _cache . items ( ) : \n 
~~~ for user in members : \n 
~~~ if members [ user ] + 3 * 60 * 60 < now : \n 
~~~ old . append ( ( group , user ) ) \n 
~~ ~~ ~~ for group , user in old : \n 
~~~ del self . _cache [ group ] [ user ] \n 
~~ def has ( self , group , user ) : \n 
~~~ if not self . _cache . has_key ( group ) : \n 
~~ return self . _cache [ group ] . has_key ( user ) \n 
~~ ~~ cache = CacheConnections ( ) \n 
cacheGroups = CacheGroups ( ) \n 
def ntlm_message_type ( msg ) : \n 
~~~ if not msg . startswith ( ) or len ( msg ) < 12 : \n 
~~ msg_type = unpack ( , msg [ 8 : 8 + 4 ] ) [ 0 ] \n 
if msg_type not in ( 1 , 2 , 3 ) : \n 
~~ return msg_type \n 
~~ def parse_ntlm_authenticate ( msg ) : \n 
~~~ \n 
NTLMSSP_NEGOTIATE_UNICODE = 0x00000001 \n 
idx = 28 \n 
length , offset = unpack ( , msg [ idx : idx + 8 ] ) \n 
domain = msg [ offset : offset + length ] \n 
idx += 8 \n 
username = msg [ offset : offset + length ] \n 
idx += 24 \n 
flags = unpack ( , msg [ idx : idx + 4 ] ) [ 0 ] \n 
if flags & NTLMSSP_NEGOTIATE_UNICODE : \n 
~~~ domain = str ( domain . decode ( ) ) \n 
username = str ( username . decode ( ) ) \n 
~~ return username , domain \n 
~~ def set_remote_user ( req , username , domain ) : \n 
~~~ format = req . get_options ( ) . get ( , ) . lower ( ) \n 
if format == : \n 
~~~ req . user = domain + + username \n 
~~~ req . user = username \n 
~~ ~~ def decode_http_authorization_header ( auth ) : \n 
ah = auth . split ( ) \n 
if len ( ah ) == 2 : \n 
~~~ b64 = base64 . b64decode ( ah [ 1 ] ) \n 
if ah [ 0 ] == : \n 
~~~ return ( , b64 ) \n 
~~ elif ah [ 0 ] == and use_basic_auth : \n 
~~~ ( user , password ) = b64 . split ( ) \n 
return ( , user , password ) \n 
~~ ~~ return False \n 
~~ def handle_unauthorized ( req ) : \n 
req . err_headers_out . add ( , ) \n 
if use_basic_auth : \n 
~~ req . err_headers_out . add ( , ) \n 
return apache . HTTP_UNAUTHORIZED \n 
~~ def connect_to_proxy ( req , type1 ) : \n 
~~~ domain = req . get_options ( ) [ ] \n 
pdc = req . get_options ( ) [ ] \n 
bdc = req . get_options ( ) . get ( , False ) \n 
~~ except KeyError , e : \n 
~~~ req . log_error ( % str ( e ) , apache . APLOG_CRIT ) \n 
raise \n 
~~ ntlm_challenge = None \n 
for server in ( pdc , bdc ) : \n 
~~~ if not server : continue \n 
~~~ if server . startswith ( ) : \n 
~~~ url = urlparse ( server ) \n 
decoded_path = urllib . unquote ( url . path ) [ 1 : ] \n 
( url . netloc , domain , decoded_path ) , apache . APLOG_INFO ) \n 
proxy = NTLM_AD_Proxy ( url . netloc , domain , base = decoded_path ) \n 
~~~ req . log_error ( % \n 
( server , domain ) , apache . APLOG_INFO ) \n 
proxy = NTLM_DC_Proxy ( server , domain ) \n 
~~ ntlm_challenge = proxy . negotiate ( type1 ) \n 
~~ except Exception , e : \n 
~~~ req . log_error ( % ( server , str ( e ) ) , apache . APLOG_CRIT ) \n 
~~ if ntlm_challenge : break \n 
proxy . close ( ) \n 
~~ return ( proxy , ntlm_challenge ) \n 
~~ def handle_type1 ( req , ntlm_message ) : \n 
cache . remove ( req . connection . id ) \n 
cache . clean ( ) \n 
~~~ ( proxy , ntlm_challenge ) = connect_to_proxy ( req , ntlm_message ) \n 
~~~ return apache . HTTP_INTERNAL_SERVER_ERROR \n 
~~ cache . add ( req . connection . id , proxy ) \n 
~~ def check_authorization ( req , username , proxy ) : \n 
rules = . join ( req . requires ( ) ) . strip ( ) \n 
if rules == or cacheGroups . has ( rules , username ) : \n 
~~ groups = [ ] \n 
for r in req . requires ( ) : \n 
~~~ users = [ u . strip ( ) for u in r [ 5 : ] . split ( "," ) ] \n 
if username in users : \n 
( username , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
return True \n 
~~~ groups += [ g . strip ( ) for g in r [ 6 : ] . split ( "," ) ] \n 
~~ ~~ if groups : \n 
~~~ res = proxy . check_membership ( username , groups ) \n 
~~~ req . log_error ( % ( username , str ( groups ) , req . unparsed_uri , str ( e ) ) ) \n 
~~ if res : \n 
~~~ cacheGroups . add ( rules , username ) \n 
req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) , apache . APLOG_INFO ) \n 
~~ req . log_error ( % \n 
( username , str ( groups ) , req . unparsed_uri ) ) \n 
( username , req . unparsed_uri ) ) \n 
~~ def handle_type3 ( req , ntlm_message ) : \n 
proxy = cache . get_proxy ( req . connection . id ) \n 
~~~ user , domain = parse_ntlm_authenticate ( ntlm_message ) \n 
if not domain : \n 
~~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
~~ result = proxy . authenticate ( ntlm_message ) \n 
user , domain = , \n 
result = False \n 
~~ if not result : \n 
~~~ cache . remove ( req . connection . id ) \n 
req . log_error ( % ( \n 
domain , user , req . unparsed_uri ) ) \n 
return handle_unauthorized ( req ) \n 
~~ req . log_error ( % ( user , domain , req . unparsed_uri ) , apache . APLOG_NOTICE ) \n 
set_remote_user ( req , user , domain ) \n 
result = check_authorization ( req , user , proxy ) \n 
if not result : \n 
~~~ return apache . HTTP_FORBIDDEN \n 
~~ req . connection . notes . add ( , req . user ) \n 
return apache . OK \n 
~~ def handle_basic ( req , user , password ) : \n 
req . log_error ( % ( req . unparsed_uri ) ) \n 
domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
client = NTLM_Client ( user , domain , password ) \n 
type1 = client . make_ntlm_negotiate ( ) \n 
~~~ ( proxy , type2 ) = connect_to_proxy ( req , type1 ) \n 
~~ client . parse_ntlm_challenge ( type2 ) \n 
type3 = client . make_ntlm_authenticate ( ) \n 
if not proxy . authenticate ( type3 ) : \n 
user , domain , req . unparsed_uri ) ) \n 
~~ req . connection . notes . add ( , user + password ) \n 
~~ def authenhandler ( req ) : \n 
req . connection . id , req . method , req . unparsed_uri , len ( cache ) ) , apache . APLOG_INFO ) \n 
auth_headers = req . headers_in . get ( , [ ] ) \n 
if not isinstance ( auth_headers , list ) : \n 
~~~ auth_headers = [ auth_headers ] \n 
~~ user = req . connection . notes . get ( , None ) \n 
if user : \n 
~~~ req . user = user \n 
if auth_headers : \n 
~~~ req . log_error ( % ( \n 
req . connection . id , req . method , req . clength , auth_headers ) , apache . APLOG_INFO ) \n 
if req . method != or req . clength > 0 : \n 
~~~ return apache . OK \n 
~~ ~~ if not auth_headers : \n 
~~~ return handle_unauthorized ( req ) \n 
~~ try : \n 
~~~ for ah in auth_headers : \n 
~~~ ah_data = decode_http_authorization_header ( ah ) \n 
if ah_data : \n 
~~ ~~ ~~ except : \n 
~~~ ah_data = False \n 
~~ if not ah_data : \n 
~~~ req . log_error ( % req . unparsed_uri , apache . APLOG_ERR ) \n 
return apache . HTTP_BAD_REQUEST \n 
~~ if ah_data [ 0 ] == : \n 
~~~ userpwd = req . connection . notes . get ( , None ) \n 
if userpwd : \n 
~~~ if userpwd != ah_data [ 1 ] + ah_data [ 2 ] : \n 
~~ domain = req . get_options ( ) . get ( , req . auth_name ( ) ) \n 
set_remote_user ( req , ah_data [ 1 ] , domain ) \n 
~~ return handle_basic ( req , ah_data [ 1 ] , ah_data [ 2 ] ) \n 
~~~ ntlm_version = ntlm_message_type ( ah_data [ 1 ] ) \n 
if ntlm_version == 1 : \n 
~~~ return handle_type1 ( req , ah_data [ 1 ] ) \n 
~~ if ntlm_version == 3 : \n 
~~~ if cache . has_key ( req . connection . id ) : \n 
~~~ return handle_type3 ( req , ah_data [ 1 ] ) \n 
( req . unparsed_uri ) , apache . APLOG_INFO ) \n 
~~ error = \n 
~~~ error = str ( e ) \n 
( req . unparsed_uri , error ) , apache . APLOG_ERR ) \n 
~~ from celery import Celery \n 
def create_celery_app ( app ) : \n 
~~~ if app . config . get ( ) : \n 
~~~ app . celery = Celery ( __name__ , broker = app . config [ ] ) \n 
app . celery . conf . update ( app . config ) \n 
taskbase = app . celery . Task \n 
class ContextTask ( taskbase ) : \n 
~~~ abstract = True \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ with app . app_context ( ) : \n 
~~~ return taskbase . __call__ ( self , * args , ** kwargs ) \n 
~~ ~~ ~~ app . celery . Task = ContextTask \n 
~~ ~~ import unittest \n 
sys . path . append ( os . path . dirname ( os . path . realpath ( __file__ ) . rsplit ( , 2 ) [ 0 ] ) ) \n 
from app import create_app \n 
app = create_app ( ) \n 
class TestUsers ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ self . app = app . test_client ( ) \n 
~~ def test_01_add ( self ) : \n 
~~~ rv = self . app . post ( , data = add_data , \n 
content_type = "application/json" ) \n 
assert rv . status_code == 201 \n 
~~ def test_02_read_update ( self ) : \n 
~~~ request = self . app . get ( ) \n 
dict = json . loads ( request . data . decode ( ) ) \n 
id = dict [ ] [ 0 ] [ ] \n 
rv = self . app . patch ( . format ( id ) , \n 
data = update_data , content_type = "application/json" ) \n 
assert rv . status_code == 200 \n 
~~ def test_03_delete ( self ) : \n 
rv = self . app . delete ( . format ( id ) ) \n 
assert rv . status_code == 204 \n 
~~ ~~ if __name__ == : \n 
~~~ unittest . main ( ) \n 
~~ from django . core . management . base import BaseCommand \n 
from chronam . core . management . commands import configure_logging \n 
from chronam . core . index import index_pages \n 
configure_logging ( "index_pages_logging.config" , "index_pages.log" ) \n 
class Command ( BaseCommand ) : \n 
~~~ def handle ( self , ** options ) : \n 
~~~ index_pages ( ) \n 
~~ ~~ import os \n 
from django . conf import settings \n 
from django . http import HttpResponse \n 
class HttpResponseServiceUnavailable ( HttpResponse ) : \n 
~~~ status_code = 503 \n 
~~ class TooBusyMiddleware ( object ) : \n 
~~~ def process_request ( self , request ) : \n 
~~~ one , five , fifteen = os . getloadavg ( ) \n 
if one > settings . TOO_BUSY_LOAD_AVERAGE : \n 
~~ return None \n 
~~ ~~ from os . path import dirname , join \n 
from django . test import TestCase \n 
from chronam . core . ocr_extractor import ocr_extractor \n 
class OcrExtractorTests ( TestCase ) : \n 
~~~ def test_extractor ( self ) : \n 
~~~ dir = join ( dirname ( dirname ( __file__ ) ) , ) \n 
ocr_file = join ( dir , ) \n 
text , coord_info = ocr_extractor ( ocr_file ) \n 
coords = coord_info [ "coords" ] \n 
expected_text = { "eng" : file ( join ( dir , ) ) . read ( ) . decode ( ) } \n 
self . assertEqual ( text , expected_text ) \n 
self . assertEqual ( len ( coords . keys ( ) ) , 2150 ) \n 
self . assertEqual ( len ( coords [ ] ) , 3 ) \n 
self . assertTrue ( coords . has_key ( ) ) \n 
self . assertTrue ( not coords . has_key ( ) ) \n 
import logging \n 
from . tests import * \n 
logger = logging . getLogger ( "human_curl" ) \n 
logger . setLevel ( logging . DEBUG ) \n 
handler = logging . StreamHandler ( ) \n 
handler . setFormatter ( formatter ) \n 
logger . addHandler ( handler ) \n 
import asyncio \n 
from zeroservices import ZeroMQMedium , ResourceService \n 
from zeroservices . services import get_http_interface \n 
from zeroservices . discovery import UdpDiscoveryMedium \n 
~~~ loop = asyncio . get_event_loop ( ) \n 
medium = ZeroMQMedium ( loop , UdpDiscoveryMedium ) \n 
service = ResourceService ( , medium ) \n 
application = get_http_interface ( service , loop , port = 5001 , allowed_origins = "*" ) \n 
application = loop . run_until_complete ( application ) \n 
loop . run_until_complete ( service . start ( ) ) \n 
loop . run_forever ( ) \n 
from trello import TrelloClient \n 
from slugify import slugify \n 
from matterllo . utils import config \n 
from matterllo . utils import logger \n 
SETTINGS = config ( ) \n 
LOGGING = logger ( ) \n 
parser . add_argument ( , dest = , action = , help = ) \n 
if not args . cleanup and not args . update and not args . init : \n 
~~~ print parser . print_help ( ) \n 
sys . exit ( 0 ) \n 
~~ client = TrelloClient ( api_key = SETTINGS [ ] , token = SETTINGS [ ] ) \n 
trello_boards = client . list_boards ( ) \n 
boards_name = [ slugify ( b [ ] ) for b in SETTINGS . get ( , { } ) . values ( ) ] \n 
if args . cleanup or args . init : \n 
~~~ result = [ h . delete ( ) for h in client . list_hooks ( ) ] \n 
LOGGING . info ( . format ( len ( result ) ) ) \n 
~~ if args . update or args . init : \n 
~~~ for board in trello_boards : \n 
~~~ board_name = slugify ( board . name ) \n 
if board_name not in boards_name : \n 
~~ LOGGING . info ( . format ( board_name ) ) \n 
url = SETTINGS [ ] + \n 
result = client . create_hook ( url , board . id ) \n 
LOGGING . info ( . format ( board_name , result ) ) \n 
~~ ~~ ~~ except Exception as e : \n 
~~~ LOGGING . error ( . format ( e ) ) \n 
sys . exit ( 1 ) \n 
~~~ main ( ) \n 
from contextlib import contextmanager \n 
import zlib \n 
~~~ from . import ssl_compat \n 
~~~ ssl_compat = None \n 
~~ _ver = sys . version_info \n 
is_py2 = _ver [ 0 ] == 2 \n 
is_py2_7_9_or_later = _ver [ 0 ] >= 2 and _ver [ 1 ] >= 7 and _ver [ 2 ] >= 9 \n 
is_py3 = _ver [ 0 ] == 3 \n 
is_py3_3 = is_py3 and _ver [ 1 ] == 3 \n 
@ contextmanager \n 
def ignore_missing ( ) : \n 
~~~ yield \n 
~~ ~~ if is_py2 : \n 
~~~ if is_py2_7_9_or_later : \n 
~~~ import ssl \n 
~~~ ssl = ssl_compat \n 
~~ from urllib import urlencode \n 
from urlparse import urlparse , urlsplit \n 
from itertools import imap \n 
def to_byte ( char ) : \n 
~~~ return ord ( char ) \n 
~~ def decode_hex ( b ) : \n 
~~~ return b . decode ( ) \n 
~~ def write_to_stdout ( data ) : \n 
~~~ sys . stdout . write ( data + ) \n 
sys . stdout . flush ( ) \n 
~~ def zlib_compressobj ( level = 6 , method = zlib . DEFLATED , wbits = 15 , memlevel = 8 , \n 
strategy = zlib . Z_DEFAULT_STRATEGY ) : \n 
~~~ return zlib . compressobj ( level , method , wbits , memlevel , strategy ) \n 
~~ unicode = unicode \n 
bytes = str \n 
~~ elif is_py3 : \n 
~~~ from urllib . parse import urlencode , urlparse , urlsplit \n 
imap = map \n 
~~~ return char \n 
~~~ return bytes . fromhex ( b ) \n 
~~~ sys . stdout . buffer . write ( data + ) \n 
sys . stdout . buffer . flush ( ) \n 
~~ zlib_compressobj = zlib . compressobj \n 
if is_py3_3 : \n 
~~ unicode = str \n 
bytes = bytes \n 
import threading \n 
import socket \n 
from hyper import HTTP20Connection \n 
from hyper . compat import ssl \n 
from hyper . http11 . connection import HTTP11Connection \n 
from hpack . hpack import Encoder \n 
from hpack . huffman import HuffmanEncoder \n 
from hpack . huffman_constants import ( \n 
REQUEST_CODES , REQUEST_CODES_LENGTH \n 
) \n 
from hyper . tls import NPN_PROTOCOL \n 
class SocketServerThread ( threading . Thread ) : \n 
def __init__ ( self , \n 
socket_handler , \n 
host = , \n 
ready_event = None , \n 
h2 = True , \n 
secure = True ) : \n 
~~~ threading . Thread . __init__ ( self ) \n 
self . socket_handler = socket_handler \n 
self . host = host \n 
self . secure = secure \n 
self . ready_event = ready_event \n 
self . daemon = True \n 
if self . secure : \n 
~~~ self . cxt = ssl . SSLContext ( ssl . PROTOCOL_SSLv23 ) \n 
if ssl . HAS_NPN and h2 : \n 
~~~ self . cxt . set_npn_protocols ( [ NPN_PROTOCOL ] ) \n 
~~ self . cxt . load_cert_chain ( certfile = , \n 
keyfile = ) \n 
~~ ~~ def _start_server ( self ) : \n 
~~~ sock = socket . socket ( ) \n 
if sys . platform != : \n 
~~~ sock . setsockopt ( socket . SOL_SOCKET , socket . SO_REUSEADDR , 1 ) \n 
~~ if self . secure : \n 
~~~ sock = self . cxt . wrap_socket ( sock , server_side = True ) \n 
~~ sock . bind ( ( self . host , 0 ) ) \n 
self . port = sock . getsockname ( ) [ 1 ] \n 
sock . listen ( 1 ) \n 
if self . ready_event : \n 
~~~ self . ready_event . set ( ) \n 
~~ self . socket_handler ( sock ) \n 
sock . close ( ) \n 
~~ def _wrap_socket ( self , sock ) : \n 
~~~ raise NotImplementedError ( ) \n 
~~ def run ( self ) : \n 
~~~ self . server = self . _start_server ( ) \n 
~~ ~~ class SocketLevelTest ( object ) : \n 
def set_up ( self , secure = True , proxy = False ) : \n 
~~~ self . host = None \n 
self . port = None \n 
self . secure = secure if not proxy else False \n 
self . proxy = proxy \n 
self . server_thread = None \n 
~~ def _start_server ( self , socket_handler ) : \n 
ready_event = threading . Event ( ) \n 
self . server_thread = SocketServerThread ( \n 
socket_handler = socket_handler , \n 
ready_event = ready_event , \n 
h2 = self . h2 , \n 
secure = self . secure \n 
self . server_thread . start ( ) \n 
ready_event . wait ( ) \n 
self . host = self . server_thread . host \n 
self . port = self . server_thread . port \n 
self . secure = self . server_thread . secure \n 
~~ def get_connection ( self ) : \n 
~~~ if self . h2 : \n 
~~~ if not self . proxy : \n 
~~~ return HTTP20Connection ( self . host , self . port , self . secure ) \n 
~~~ return HTTP20Connection ( , secure = self . secure , \n 
proxy_host = self . host , \n 
proxy_port = self . port ) \n 
~~~ return HTTP11Connection ( self . host , self . port , self . secure ) \n 
~~~ return HTTP11Connection ( , secure = self . secure , \n 
~~ ~~ ~~ def get_encoder ( self ) : \n 
e = Encoder ( ) \n 
e . huffman_coder = HuffmanEncoder ( REQUEST_CODES , REQUEST_CODES_LENGTH ) \n 
return e \n 
~~ def tear_down ( self ) : \n 
self . server_thread . join ( 0.1 ) \n 
import unittest \n 
from SystemConfiguration import * \n 
class SCPreferences ( object ) : \n 
session = None \n 
def __init__ ( self ) : \n 
~~~ super ( SCPreferences , self ) . __init__ ( ) \n 
self . session = SCPreferencesCreate ( None , "set-proxy" , None ) \n 
~~ def save ( self ) : \n 
~~~ if not self . session : \n 
~~ if not SCPreferencesCommitChanges ( self . session ) : \n 
~~ if not SCPreferencesApplyChanges ( self . session ) : \n 
~~ ~~ def set_proxy ( self , enable = True , protocol = "HTTP" , server = "localhost" , port = 3128 ) : \n 
~~~ new_settings = SCPreferencesPathGetValue ( self . session , ) \n 
for interface in new_settings : \n 
~~~ new_settings [ interface ] [ ] [ "%sEnable" % protocol ] = 1 if enable else 0 \n 
if enable : \n 
~~~ new_settings [ interface ] [ ] [ % protocol ] = int ( port ) \n 
new_settings [ interface ] [ ] [ % protocol ] = server \n 
~~ ~~ SCPreferencesPathSetValue ( self . session , , new_settings ) \n 
~~ ~~ class SCPreferencesTests ( unittest . TestCase ) : \n 
~~ import sublime , sublime_plugin , os , re \n 
class StyleSheetSetup : \n 
~~~ def __init__ ( self , extensions , regex , partials = None , index = None ) : \n 
~~~ if partials is None : \n 
~~~ self . partials = False \n 
~~~ self . partials = partials \n 
~~ if index is None : \n 
~~~ self . index = False \n 
~~~ self . index = index \n 
~~ self . extensions = extensions \n 
self . regex = regex \n 
~~ ~~ class ListStylesheetVariables ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit ) : \n 
~~~ settings = sublime . load_settings ( ) \n 
handle_imports = settings . get ( "readImported" ) \n 
read_all_views = settings . get ( "readAllViews" ) \n 
setups = ( less_setup , sass_setup , stylus_setup , sass_erb_setup ) \n 
chosen_setup = None \n 
self . edit = edit \n 
fn = self . view . file_name ( ) . encode ( "utf_8" ) \n 
for setup in setups : \n 
~~~ for ext in setup . extensions : \n 
~~~ if fn . endswith ( ext ) : \n 
~~~ chosen_setup = setup \n 
~~ ~~ ~~ if chosen_setup == None : \n 
~~ imports = [ ] \n 
imported_vars = [ ] \n 
compiled_regex = re . compile ( chosen_setup . regex , re . MULTILINE ) \n 
if handle_imports : \n 
file_dir = os . path . dirname ( fn ) . decode ( "utf-8" ) \n 
for i , filename in enumerate ( imports ) : \n 
~~~ has_extension = False \n 
for ext in chosen_setup . extensions : \n 
~~~ if filename . endswith ( ext . decode ( "utf-8" ) ) : \n 
~~~ has_extension = True \n 
~~ ~~ if has_extension == False : \n 
~~~ for ext in chosen_setup . extensions : \n 
~~~ ext = ext . decode ( "utf-8" ) \n 
if os . path . isfile ( os . path . normpath ( file_dir + + filename + ext ) ) : \n 
~~~ filename += ext \n 
~~ if chosen_setup . partials : \n 
~~~ fn_split = os . path . split ( filename ) \n 
partial_filename = fn_split [ 0 ] + "/_" + fn_split [ 1 ] \n 
if os . path . isfile ( os . path . normpath ( file_dir + partial_filename + ext ) ) : \n 
~~~ filename = "_" + filename + ext \n 
~~ ~~ if chosen_setup . index and os . path . isfile ( os . path . normpath ( file_dir + "/" + filename + "/index" + ext ) ) : \n 
~~~ filename += "/index" + ext \n 
~~ ~~ ~~ try : \n 
~~~ f = open ( os . path . normpath ( file_dir + + filename ) , ) \n 
contents = f . read ( ) \n 
f . close ( ) \n 
m = re . findall ( compiled_regex , contents ) \n 
imported_vars = imported_vars + m \n 
~~~ print ( + filename ) \n 
~~ ~~ imported_vars = [ list ( item ) for item in imported_vars ] \n 
~~ self . variables = [ ] \n 
vars_from_views = [ ] \n 
if read_all_views : \n 
~~~ for view in self . view . window ( ) . views ( ) : \n 
~~~ viewfn = self . view . file_name ( ) . encode ( "utf-8" ) \n 
compatible_view = False \n 
~~~ if viewfn . endswith ( ext ) : \n 
~~~ viewvars = [ ] \n 
view . find_all ( chosen_setup . regex , 0 , "$1|$2" , viewvars ) \n 
vars_from_views += viewvars \n 
break ; \n 
~~ ~~ ~~ ~~ else : \n 
~~~ self . view . find_all ( chosen_setup . regex , 0 , "$1|$2" , self . variables ) \n 
~~ self . variables += vars_from_views \n 
self . variables = list ( set ( self . variables ) ) \n 
for i , val in enumerate ( self . variables ) : \n 
~~~ self . variables [ i ] = val . split ( "|" ) \n 
~~ self . variables = imported_vars + self . variables \n 
self . variables . sort ( ) \n 
self . view . window ( ) . show_quick_panel ( self . variables , self . insert_variable , sublime . MONOSPACE_FONT ) \n 
~~ def insert_variable ( self , choice ) : \n 
~~~ if choice == - 1 : \n 
~~ self . view . run_command ( , { : self . variables [ choice ] [ 0 ] } ) \n 
~~ ~~ class InsertText ( sublime_plugin . TextCommand ) : \n 
~~~ def run ( self , edit , string = ) : \n 
~~~ for selection in self . view . sel ( ) : \n 
~~~ self . view . insert ( edit , selection . begin ( ) , string ) \n 
~~ ~~ ~~ from driver_base import DriverBase \n 
os . sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import log \n 
class CMDTYPE : \n 
PIXEL_DATA = 2 \n 
BRIGHTNESS = 3 \n 
~~ class RETURN_CODES : \n 
~~ class DriverNetwork ( DriverBase ) : \n 
def __init__ ( self , num = 0 , width = 0 , height = 0 , host = "localhost" , port = 3142 ) : \n 
~~~ super ( DriverNetwork , self ) . __init__ ( num , width , height ) \n 
self . _host = host \n 
self . _port = port \n 
~~ def _generateHeader ( self , cmd , size ) : \n 
~~~ packet = bytearray ( ) \n 
packet . append ( cmd ) \n 
packet . append ( size & 0xFF ) \n 
packet . append ( size >> 8 ) \n 
return packet \n 
~~ def _connect ( self ) : \n 
~~~ s = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
s . connect ( ( self . _host , self . _port ) ) \n 
return s \n 
~~ except socket . gaierror : \n 
self . _host ) \n 
log . error ( error ) \n 
raise IOError ( error ) \n 
~~ ~~ def update ( self , data ) : \n 
~~~ s = self . _connect ( ) \n 
count = self . bufByteCount \n 
packet = self . _generateHeader ( CMDTYPE . PIXEL_DATA , count ) \n 
packet . extend ( data ) \n 
s . sendall ( packet ) \n 
resp = ord ( s . recv ( 1 ) ) \n 
s . close ( ) \n 
if resp != RETURN_CODES . SUCCESS : \n 
~~~ log . exception ( e ) \n 
~~ ~~ def setMasterBrightness ( self , brightness ) : \n 
~~~ packet = self . _generateHeader ( CMDTYPE . BRIGHTNESS , 1 ) \n 
packet . append ( brightness ) \n 
s = self . _connect ( ) \n 
~~ ~~ ~~ MANIFEST = [ \n 
{ \n 
"id" : "network" , \n 
"class" : DriverNetwork , \n 
"type" : "driver" , \n 
"display" : "Network" , \n 
"params" : [ { \n 
"id" : "num" , \n 
"type" : "int" , \n 
"default" : 0 , \n 
"min" : 0 , \n 
} , { \n 
"id" : "width" , \n 
"label" : "Width" , \n 
"id" : "height" , \n 
"label" : "Height" , \n 
"id" : "host" , \n 
"type" : "str" , \n 
"default" : "localhost" , \n 
"id" : "port" , \n 
"label" : "Port" , \n 
"default" : 3142 , \n 
} ] \n 
} \n 
_ID = \n 
API = \n 
API_CALL_TIMEOUT = \n 
API_VERSION = \n 
API_VERSION_MAXIMUM = \n 
API_VERSION_MINIMUM = \n 
AREA = \n 
AREA_MAX = \n 
BBOX = \n 
BOUNDS = \n 
CFGSLAB = \n 
CFGVERSION = 1 \n 
CHANGESET = \n 
CHANGESETS = \n 
CHANGESETS_INLINE_SIZE = \n 
CHANGESETS_PER_SLAB = \n 
CHANGESETS_MAX = \n 
CONFIGURATION_SCHEMA_VERSION = \n 
CONTENT_TYPE = \n 
COUCHDB = \n 
DATASTORE = \n 
DATASTORE_BACKEND = \n 
DATASTORE_CONFIG = \n 
DATASTORE_ENCODING = \n 
DBHOST = \n 
DBJOB_ADDELEM = \n 
DBJOB_QUIT = \n 
DBNAME = \n 
DBPORT = \n 
DBURL = \n 
DEFAULT = \n 
ELEMENT = \n 
FRONT_END = \n 
GENERATOR = \n 
GEODOC = \n 
GEODOC_LRU_SIZE = \n 
GEODOC_LRU_THREADS = \n 
GEOHASH_LENGTH = \n 
ID = \n 
JSON = \n 
K = \n 
LAT = \n 
LAT_MAX = + 90.0 \n 
LAT_MIN = - 90.0 \n 
LON = \n 
LON_MAX = + 180.0 \n 
LON_MIN = - 180.0 \n 
MAXIMUM = \n 
MAXIMUM_ELEMENTS = \n 
MAXGHLAT = 89.999999999999992 \n 
MAXLAT = \n 
MAXLON = \n 
MEMBASE = \n 
MEMBASE_MAX_VALUE_LENGTH = 20 * 1024 * 1024 \n 
MEMBER = \n 
MEMBERS = \n 
MINIMUM = \n 
MINLAT = \n 
MINLON = \n 
ND = \n 
NODE = \n 
NODES = \n 
NODES_INLINE_SIZE = \n 
NODES_PER_SLAB = \n 
OSM = \n 
PER_PAGE = \n 
PORT = \n 
PROJECT_DOC = \n 
PROTOBUF = \n 
REF = \n 
REFERENCES = \n 
RELATION = \n 
RELATIONS = \n 
RELATIONS_INLINE_SIZE = \n 
RELATIONS_PER_SLAB = \n 
ROLE = \n 
SCALE_FACTOR = \n 
SECONDS = \n 
SERVER_NAME = \n 
SERVER_VERSION = \n 
SLAB_LRU_SIZE = \n 
SLAB_LRU_THREADS = \n 
SOURCE_REPOSITORY = \n 
STATUS = \n 
TAG = \n 
TAGS = \n 
TEXT_XML = \n 
TIMEOUT = \n 
TRACEPOINTS = \n 
TRACEPOINTS_PER_PAGE = \n 
TYPE = \n 
UTF8 = \n 
V = \n 
VERSION = \n 
WAY = \n 
WAYS = \n 
WAYS_INLINE_SIZE = \n 
WAYS_PER_SLAB = \n 
WAYNODES = \n 
WAYNODES_MAX = \n 
import webapp2 \n 
from urllib import urlencode \n 
import json , urllib2 \n 
from secret import client_id , client_secret \n 
import config \n 
class AuthRedirector ( webapp2 . RequestHandler ) : \n 
~~~ def get ( self ) : \n 
~~~ args = self . request . GET \n 
args [ "client_id" ] = client_id \n 
args [ "redirect_uri" ] = config . auth_redir_uri \n 
url = "https://accounts.google.com/o/oauth2/auth?" + urlencode ( args ) \n 
self . response . location = url \n 
self . response . status_int = 302 \n 
~~ ~~ def query_json ( url , data ) : \n 
if not ( data is str ) : \n 
~~~ data = urlencode ( data ) \n 
~~~ return json . loads ( urllib2 . urlopen ( url , data ) . read ( ) ) \n 
~~~ return json . loads ( e . read ( ) ) \n 
~~ ~~ def json_compactify ( data ) : \n 
~~ class AuthCallback ( webapp2 . RequestHandler ) : \n 
def get ( self ) : \n 
~~~ state = self . request . get ( "state" ) \n 
code = self . request . get ( "code" ) \n 
error = self . request . get ( "error" ) \n 
q = { \n 
"code" : code , \n 
"client_id" : client_id , \n 
"client_secret" : client_secret , \n 
"redirect_uri" : config . auth_redir_uri , \n 
"grant_type" : "authorization_code" , \n 
result = query_json ( "https://accounts.google.com/o/oauth2/token" , q ) \n 
url = ( config . auth_success_page if "access_token" in result \n 
else config . auth_failure_page ) + "#" + urlencode ( result ) \n 
~~ ~~ class AuthRefresh ( webapp2 . RequestHandler ) : \n 
~~~ refresh_token = self . request . get ( "refresh_token" ) \n 
if not refresh_token : \n 
~~~ self . response . status_int = 400 \n 
~~ q = { \n 
"refresh_token" : refresh_token , \n 
"grant_type" : "refresh_token" , \n 
self . response . write ( json_compactify ( result ) ) \n 
~~ ~~ application = webapp2 . WSGIApplication ( [ \n 
( , AuthRedirector ) , \n 
( , AuthCallback ) , \n 
( , AuthRefresh ) , \n 
] , debug = True ) \n 
from django . db import migrations , models \n 
class Migration ( migrations . Migration ) : \n 
~~~ dependencies = [ \n 
( , ) , \n 
operations = [ \n 
migrations . AddField ( \n 
model_name = , \n 
name = , \n 
field = models . EmailField ( help_text = , max_length = 75 , null = True , verbose_name = , blank = True ) , \n 
preserve_default = True , \n 
) , \n 
import string \n 
from jsbeautifier . unpackers import UnpackingError \n 
PRIORITY = 1 \n 
def detect ( source ) : \n 
return source . replace ( , ) . startswith ( ) \n 
~~ def unpack ( source ) : \n 
payload , symtab , radix , count = _filterargs ( source ) \n 
if count != len ( symtab ) : \n 
~~~ raise UnpackingError ( ) \n 
~~~ unbase = Unbaser ( radix ) \n 
~~ except TypeError : \n 
~~ def lookup ( match ) : \n 
word = match . group ( 0 ) \n 
return symtab [ unbase ( word ) ] or word \n 
~~ source = re . sub ( , lookup , payload ) \n 
return _replacestrings ( source ) \n 
~~ def _filterargs ( source ) : \n 
args = re . search ( argsregex , source , re . DOTALL ) . groups ( ) \n 
~~~ return args [ 0 ] , args [ 3 ] . split ( ) , int ( args [ 1 ] ) , int ( args [ 2 ] ) \n 
~~ except ValueError : \n 
~~ ~~ def _replacestrings ( source ) : \n 
if match : \n 
~~~ varname , strings = match . groups ( ) \n 
startpoint = len ( match . group ( 0 ) ) \n 
lookup = strings . split ( \'","\' ) \n 
variable = % varname \n 
for index , value in enumerate ( lookup ) : \n 
~~~ source = source . replace ( variable % index , \'"%s"\' % value ) \n 
~~ return source [ startpoint : ] \n 
~~ return source \n 
~~ class Unbaser ( object ) : \n 
ALPHABET = { \n 
62 : , \n 
def __init__ ( self , base ) : \n 
~~~ self . base = base \n 
if 2 <= base <= 36 : \n 
~~~ self . unbase = lambda string : int ( string , base ) \n 
~~~ self . dictionary = dict ( ( cipher , index ) for \n 
index , cipher in enumerate ( self . ALPHABET [ base ] ) ) \n 
~~ except KeyError : \n 
~~~ raise TypeError ( ) \n 
~~ self . unbase = self . _dictunbaser \n 
~~ ~~ def __call__ ( self , string ) : \n 
~~~ return self . unbase ( string ) \n 
~~ def _dictunbaser ( self , string ) : \n 
ret = 0 \n 
for index , cipher in enumerate ( string [ : : - 1 ] ) : \n 
~~~ ret += ( self . base ** index ) * self . dictionary [ cipher ] \n 
~~ return ret \n 
~~ ~~ import os , sys \n 
parentdir = os . path . dirname ( __file__ ) \n 
sys . path . insert ( 0 , parentdir ) \n 
import executemechanize \n 
class redirection : \n 
~~~ def createarray ( self ) : \n 
~~~ setattr ( self , "redirection_list" , [ ] ) \n 
~~ def appendurl ( self , url ) : \n 
~~~ url = str ( url ) \n 
if not url . endswith ( ".js" ) or url . endswith ( ".json" ) : \n 
~~~ self . redirection_list . append ( url ) ; \n 
self . passarray ( ) \n 
~~ ~~ def passarray ( self ) : \n 
~~~ executemechanize . set_redirection_list ( self . redirection_list ) \n 
~~ ~~ SQL_PORT = 15000 \n 
ZMQ_RPC_PORT = 15598 \n 
HTTP_PORT = 15597 \n 
HTTPS_PORT = 443 \n 
ZMQ_PUBSUB_PORT = 15596 \n 
__author__ = \n 
__maintainer__ = \n 
__email__ = \n 
from . . config import PATHS \n 
from . . entity import Entity \n 
class StrategyConcept ( Entity ) : \n 
collection = \n 
resource = \n 
_relations = { \n 
, \n 
_pull = { \n 
: int , \n 
: Entity . _strpt , \n 
: Entity . _int_to_bool , \n 
_push = _pull . copy ( ) \n 
_push . update ( { \n 
} ) \n 
_readonly = Entity . _readonly | { , } \n 
def __init__ ( self , session , properties = None , ** kwargs ) : \n 
~~~ super ( StrategyConcept , self ) . __init__ ( session , properties , ** kwargs ) \n 
~~ def remove ( self ) : \n 
url = . join ( [ self . collection , \n 
str ( self . id ) , \n 
] ) \n 
self . _post ( PATHS [ ] , rest = url , data = { : self . version } ) \n 
for item in list ( self . properties . keys ( ) ) : \n 
~~~ del self . properties [ item ] \n 
~~ ~~ ~~ from __future__ import print_function \n 
import responses \n 
import requests \n 
from . requests_patch import patched_extract_cookies_to_jar \n 
from terminalone import T1 \n 
mock_credentials = { \n 
API_BASE = \n 
requests . sessions . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
requests . adapters . extract_cookies_to_jar = patched_extract_cookies_to_jar \n 
class TestPermissions ( unittest . TestCase ) : \n 
~~~ def setup ( self ) : \n 
with open ( ) as f : \n 
~~~ fixture = f . read ( ) \n 
~~ responses . add ( responses . POST , , \n 
body = fixture , \n 
adding_headers = { \n 
} , \n 
content_type = ) \n 
self . t1 = T1 ( auth_method = , \n 
api_base = API_BASE , \n 
** mock_credentials ) \n 
~~ @ responses . activate \n 
def test_get_permissions ( self ) : \n 
~~~ self . setup ( ) \n 
~~ responses . add ( responses . GET , \n 
content_type = , \n 
match_querystring = True ) \n 
p = self . t1 . get ( , 10000 , child = ) \n 
assert p . _type == , . format ( p . _type ) \n 
assert p . parent_id == 10000 , . format ( p . parent_id ) \n 
def test_remove_advertiser ( self ) : \n 
remove_id = 6 \n 
assert remove_id in p . advertiser . keys ( ) , . format ( remove_id ) \n 
p . remove ( , 6 ) \n 
assert remove_id not in p . advertiser . keys ( ) , . format ( remove_id ) \n 
def test_it_should_remove_child_advertisers_when_removing_agency ( self ) : \n 
remove_ids = [ 6 , 7 ] \n 
for ad_id in remove_ids : \n 
~~~ assert ad_id in p . advertiser . keys ( ) , . format ( ad_id ) \n 
~~ p . remove ( , 3 ) \n 
~~~ assert ad_id not in p . advertiser . keys ( ) , . format ( ad_id ) \n 
~~ ~~ @ responses . activate \n 
def test_it_should_remove_child_agencies_and_advertisers_when_removing_organization ( self ) : \n 
remove_advertiser_ids = [ 8 , 9 , 10 ] \n 
remove_agency_ids = [ 4 , 5 ] \n 
for advertiser_id in remove_advertiser_ids : \n 
~~~ assert advertiser_id in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
~~ for agency_id in remove_agency_ids : \n 
~~~ assert agency_id in p . agency . keys ( ) , . format ( agency_id ) \n 
~~ p . remove ( , 2 ) \n 
~~~ assert advertiser_id not in p . advertiser . keys ( ) , . format ( advertiser_id ) \n 
~~~ assert agency_id not in p . agency . keys ( ) , . format ( agency_id ) \n 
def test_it_should_add_entity_ids_on_save ( self ) : \n 
p . add ( , 10 ) \n 
data = p . _generate_save_data ( ) \n 
assert sorted ( data [ ] ) == [ 1 , 2 , 10 ] , data [ ] \n 
def test_it_should_add_access_to_empty_permissions ( self ) : \n 
assert sorted ( data [ ] ) == [ 10 ] , data [ ] \n 
~~ ~~ VERSION = ( 0 , 1 , 9 ) \n 
__version__ = "0.1.9" \n 
import sys as _sys \n 
from operator import itemgetter as _itemgetter \n 
from keyword import iskeyword as _iskeyword \n 
from collections import OrderedDict \n 
################################################################################ \n 
class tagtuple ( tuple ) : \n 
__slots__ = ( ) \n 
def __new__ ( cls , * args ) : \n 
return super ( tagtuple , cls ) . __new__ ( cls , args ) \n 
~~ def __repr__ ( self ) : \n 
return type ( self ) . __name__ + super ( tagtuple , self ) . __repr__ ( ) \n 
~~ def __getnewargs__ ( self ) : \n 
return tuple ( self ) \n 
~~ def __eq__ ( self , other ) : \n 
~~~ return type ( self ) is type ( other ) and super ( tagtuple , self ) . __eq__ ( other ) \n 
~~ def __ne__ ( self , other ) : \n 
~~~ return not self . __eq__ ( other ) \n 
~~ def __getslice__ ( self , i , j ) : \n 
~~~ return type ( self ) ( * super ( tagtuple , self ) . __getslice__ ( i , j ) ) \n 
~~ __add__ = property ( ) \n 
__mul__ = property ( ) \n 
__rmul__ = property ( ) \n 
count = property ( ) \n 
index = property ( ) \n 
~~ _class_template = \n 
_repr_template = \n 
_field_template = \n 
def rectuple ( typename , field_names , verbose = False , rename = False ) : \n 
if isinstance ( field_names , basestring ) : \n 
~~~ field_names = field_names . replace ( , ) . split ( ) \n 
~~ field_names = map ( str , field_names ) \n 
if rename : \n 
~~~ seen = set ( ) \n 
for index , name in enumerate ( field_names ) : \n 
~~~ if ( not all ( c . isalnum ( ) or c == for c in name ) \n 
or _iskeyword ( name ) \n 
or not name \n 
or name [ 0 ] . isdigit ( ) \n 
or name . startswith ( ) \n 
or name in seen ) : \n 
~~~ field_names [ index ] = % index \n 
~~ seen . add ( name ) \n 
~~ ~~ for name in [ typename ] + field_names : \n 
~~~ if not all ( c . isalnum ( ) or c == for c in name ) : \n 
~~~ raise ValueError ( \n 
% name ) \n 
~~ if _iskeyword ( name ) : \n 
~~ if name [ 0 ] . isdigit ( ) : \n 
~~ ~~ seen = set ( ) \n 
for name in field_names : \n 
~~~ if name . startswith ( ) and not rename : \n 
~~ if name in seen : \n 
~~~ raise ValueError ( % name ) \n 
~~ class_definition = _class_template . format ( \n 
typename = typename , \n 
field_names = tuple ( field_names ) , \n 
num_fields = len ( field_names ) , \n 
arg_list = repr ( tuple ( field_names ) ) . replace ( "\'" , "" ) [ 1 : - 1 ] , \n 
repr_fmt = . join ( _repr_template . format ( name = name ) \n 
for name in field_names ) , \n 
field_defs = . join ( _field_template . format ( index = index , name = name ) \n 
for index , name in enumerate ( field_names ) ) \n 
if verbose : \n 
~~~ print class_definition \n 
~~ namespace = dict ( _itemgetter = _itemgetter , __name__ = % typename , \n 
OrderedDict = OrderedDict , _property = property , _tuple = tuple ) \n 
~~~ exec class_definition in namespace \n 
~~ except SyntaxError as e : \n 
~~~ raise SyntaxError ( e . message + + class_definition ) \n 
~~ result = namespace [ typename ] \n 
~~~ result . __module__ = _sys . _getframe ( 1 ) . f_globals . get ( , ) \n 
~~ except ( AttributeError , ValueError ) : \n 
~~~ import pickle \n 
from itertools import chain , product \n 
print \n 
class A ( tagtuple ) : \n 
~~~ __slots__ = ( ) \n 
~~ class B ( tagtuple ) : \n 
~~ a = A ( 1 , 2 , 3 ) \n 
b = B ( 1 , 2 , 3 ) \n 
t = ( 1 , 2 , 3 ) \n 
d = { } \n 
d [ a ] = 1 \n 
d [ b ] = 2 \n 
d [ t ] = 3 \n 
s = set ( ) \n 
s . add ( a ) \n 
s . add ( b ) \n 
s . add ( t ) \n 
a0 = pickle . loads ( pickle . dumps ( a , 0 ) ) \n 
a1 = pickle . loads ( pickle . dumps ( a , 1 ) ) \n 
a2 = pickle . loads ( pickle . dumps ( a , 2 ) ) \n 
A = rectuple ( , , verbose = True ) \n 
B = rectuple ( , , verbose = True ) \n 
a = A ( 1 , 2 ) \n 
b = B ( 1 , 2 ) \n 
t = ( 1 , 2 ) \n 
~~ import math \n 
def distance ( pa , pb ) : \n 
~~~ ax , ay = pa \n 
bx , by = pb \n 
return math . sqrt ( ( ax - bx ) ** 2 + ( ay - by ) ** 2 ) \n 
~~ def index_of_nearest ( p , hot_points , distance_f = distance ) : \n 
min_dist = None \n 
nearest_hp_i = None \n 
for i , hp in enumerate ( hot_points ) : \n 
~~~ dist = distance_f ( p , hp ) \n 
if min_dist is None or dist < min_dist : \n 
~~~ min_dist = dist \n 
nearest_hp_i = i \n 
~~ ~~ return nearest_hp_i \n 
~~ from fabric import main as fab_main \n 
from cloudferry import fabfile \n 
~~~ fab = fabfile . __file__ \n 
if fab . endswith ( ) : \n 
~~~ fab = fab [ : - 1 ] \n 
~~ fab_main . main ( [ fab ] ) \n 
~~ from cloudferry . lib . base . action import action \n 
DEFAULT = 0 \n 
PATH_ONE = 1 \n 
PATH_TWO = 2 \n 
class IsOption ( action . Action ) : \n 
~~~ def __init__ ( self , init , option_name ) : \n 
~~~ self . option_name = option_name \n 
super ( IsOption , self ) . __init__ ( init ) \n 
~~ def run ( self , ** kwargs ) : \n 
option_value = self . cfg . migrate [ self . option_name ] \n 
if option_value : \n 
~~~ self . set_next_path ( PATH_ONE ) \n 
~~~ self . set_next_path ( PATH_TWO ) \n 
~~ return { } \n 
~~ ~~ from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import log \n 
from cloudferry . lib . utils import utils as utl \n 
LOG = log . getLogger ( __name__ ) \n 
class CheckConfigQuotaNeutron ( action . Action ) : \n 
def run ( self , ** kwargs ) : \n 
~~~ src_cloud = self . src_cloud \n 
dst_cloud = self . dst_cloud \n 
network_src = src_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
identity_dst = dst_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
network_dst = dst_cloud . resources [ utl . NETWORK_RESOURCE ] \n 
search_opts_tenant = kwargs . get ( , { } ) \n 
tenants_src = self . get_src_tenants ( search_opts_tenant ) \n 
list_quotas = network_src . list_quotas ( ) \n 
tenants_without_quotas = self . get_tenants_without_quotas ( tenants_src , \n 
list_quotas ) \n 
if not tenants_without_quotas : \n 
quot = network_src . show_quota ( tenants_without_quotas [ 0 ] ) \n 
quot_default_dst = network_dst . show_quota ( dst_temp_tenant . id ) \n 
is_configs_different = False \n 
identity_dst . delete_tenant ( dst_temp_tenant ) \n 
for item_quot , val_quot in quot . iteritems ( ) : \n 
~~~ if val_quot != quot_default_dst [ item_quot ] : \n 
~~~ is_configs_different = True \n 
quot_default_dst [ item_quot ] ) \n 
~~ ~~ if not is_configs_different : \n 
def get_tenants_without_quotas ( tenants_src , list_quotas ) : \n 
~~~ tenants_ids = tenants_src . keys ( ) \n 
quotas_ids_tenants = [ quota [ "tenant_id" ] for quota in list_quotas ] \n 
return list ( set ( tenants_ids ) - set ( quotas_ids_tenants ) ) \n 
~~ def get_src_tenants ( self , search_opts ) : \n 
~~~ identity_src = self . src_cloud . resources [ utl . IDENTITY_RESOURCE ] \n 
if search_opts . get ( ) : \n 
~~~ filter_tenants_ids_list = search_opts [ ] \n 
tenants = [ identity_src . keystone_client . tenants . find ( id = tnt_id ) for \n 
tnt_id in filter_tenants_ids_list ] \n 
~~~ tenants = identity_src . get_tenants_list ( ) \n 
~~ tenants_dict = { tenant . id : tenant . name for tenant in tenants } \n 
return tenants_dict \n 
~~ ~~ import copy \n 
from oslo_config import cfg \n 
from cloudferry . lib . base . action import action \n 
from cloudferry . lib . utils import utils \n 
LOG = logging . getLogger ( __name__ ) \n 
class DetachVolumesCompute ( action . Action ) : \n 
~~~ def run ( self , info , ** kwargs ) : \n 
~~~ info = copy . deepcopy ( info ) \n 
compute_resource = self . cloud . resources [ utils . COMPUTE_RESOURCE ] \n 
storage_resource = self . cloud . resources [ utils . STORAGE_RESOURCE ] \n 
for instance in info [ utils . INSTANCES_TYPE ] . itervalues ( ) : \n 
instance [ ] [ ] , instance [ ] [ ] ) \n 
if not instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~ for vol in instance [ ] [ utils . VOLUMES_TYPE ] : \n 
~~~ volume_status = storage_resource . get_status ( vol [ ] ) \n 
vol [ ] , volume_status ) \n 
if volume_status == : \n 
~~~ compute_resource . detach_volume ( instance [ ] [ ] , \n 
vol [ ] ) \n 
timeout = CONF . migrate . storage_backend_timeout \n 
storage_resource . wait_for_status ( \n 
vol [ ] , storage_resource . get_status , , \n 
timeout = timeout ) \n 
~~ ~~ ~~ return { } \n 
from cloudferry . lib . utils . ssh_util import SshUtil \n 
class RemoteExecution ( action . Action ) : \n 
~~~ def __init__ ( self , cloud , host = None , int_host = None , config_migrate = None ) : \n 
~~~ self . cloud = cloud \n 
self . int_host = int_host \n 
self . config_migrate = config_migrate \n 
self . remote_exec_obj = SshUtil ( self . cloud , \n 
self . config_migrate , \n 
self . host ) \n 
super ( RemoteExecution , self ) . __init__ ( { } ) \n 
~~ def run ( self , command , ** kwargs ) : \n 
~~~ self . remote_exec_obj . execute ( command , self . int_host ) \n 
return { } \n 
~~ ~~ import json \n 
from xml . etree import ElementTree \n 
nova_instances_path = "/var/lib/nova/instances/" \n 
def instance_path ( instance_id ) : \n 
~~~ return os . path . join ( nova_instances_path , instance_id ) \n 
~~ def instance_image_path ( instance_id ) : \n 
~~~ return os . path . join ( instance_path ( instance_id ) , "disk" ) \n 
~~ def _qemu_img_rebase ( src , dst ) : \n 
~~ class QemuBackingFileMover ( object ) : \n 
~~~ def __init__ ( self , runner , src , instance_id ) : \n 
~~~ self . runner = runner \n 
self . src = src \n 
self . dst = instance_image_path ( instance_id ) \n 
~~ def __enter__ ( self ) : \n 
~~~ cmd = _qemu_img_rebase ( self . src , self . dst ) \n 
self . runner . run ( cmd ) \n 
return self \n 
~~ def __exit__ ( self , exc_type , exc_val , exc_tb ) : \n 
~~~ cmd = _qemu_img_rebase ( self . dst , self . src ) \n 
self . runner . run_ignoring_errors ( cmd ) \n 
~~ ~~ class DestNovaInstanceDestroyer ( object ) : \n 
def __init__ ( self , dest_libvirt , dest_nova , libvirt_name , nova_vm_id ) : \n 
~~~ self . dest_libvirt = dest_libvirt \n 
self . dest_nova = dest_nova \n 
self . libvirt_name = libvirt_name \n 
self . nova_vm_id = nova_vm_id \n 
~~~ self . do ( ) \n 
~~~ self . undo ( ) \n 
~~ def do ( self ) : \n 
~~~ self . dest_libvirt . destroy_vm ( self . libvirt_name ) \n 
~~ def undo ( self ) : \n 
self . dest_nova . reset_state ( self . nova_vm_id ) \n 
self . dest_nova . delete_vm_by_id ( self . nova_vm_id ) \n 
~~ except RuntimeError : \n 
~~ ~~ ~~ class Libvirt ( object ) : \n 
~~~ def __init__ ( self , remote_runner ) : \n 
self . runner = remote_runner \n 
~~ def get_backing_file ( self , instance_id ) : \n 
image_path = instance_image_path ( instance_id ) ) ) \n 
~~~ image_info = json . loads ( self . runner . run ( cmd ) ) \n 
return image_info [ ] \n 
~~ except ( ValueError , TypeError ) as e : \n 
instance_id ) \n 
~~ ~~ def get_xml ( self , libvirt_instance_name ) : \n 
inst_name = libvirt_instance_name ) ) \n 
return LibvirtXml ( self . runner . run ( cmd ) ) \n 
~~ def destroy_vm ( self , libvirt_instance_name ) : \n 
~~~ cmds = [ \n 
for cmd in cmds : \n 
~~~ self . runner . run ( cmd ) \n 
~~ ~~ def move_backing_file ( self , source_file , instance_id ) : \n 
~~~ cmd = _qemu_img_rebase ( src = source_file , \n 
dst = instance_image_path ( instance_id ) ) \n 
~~ def live_migrate ( self , libvirt_instance_name , dest_host , migration_xml ) : \n 
dst_host = dest_host , \n 
migration_xml = migration_xml ) ) \n 
~~ ~~ class LibvirtDeviceInterfaceHwAddress ( object ) : \n 
~~~ def __init__ ( self , element ) : \n 
~~~ self . type = element . get ( ) \n 
self . domain = element . get ( ) \n 
self . bus = element . get ( ) \n 
self . slot = element . get ( ) \n 
self . function = element . get ( ) \n 
~~~ return ( isinstance ( other , self . __class__ ) and \n 
self . type == other . type and \n 
self . domain == other . domain and \n 
self . bus == other . bus and \n 
self . slot == other . slot and \n 
self . function == other . function ) \n 
~~ ~~ class LibvirtDeviceInterface ( object ) : \n 
~~~ def __init__ ( self , interface ) : \n 
self . _xml_element = interface \n 
self . mac = interface . find ( ) . get ( ) \n 
self . source_iface = interface . find ( ) . get ( ) \n 
self . target_iface = interface . find ( ) . get ( ) \n 
self . hw_address = LibvirtDeviceInterfaceHwAddress ( \n 
interface . find ( ) ) \n 
self . source_iface == other . source_iface and \n 
self . target_iface == other . target_iface and \n 
self . hw_address == other . hw_address ) \n 
mac = self . mac , src = self . source_iface , dst = self . target_iface ) \n 
def _replace_attr ( cls , element , attr , value ) : \n 
~~~ if element . get ( attr ) != value : \n 
~~~ element . clear ( ) \n 
element . attrib = { attr : value } \n 
~~ ~~ def element ( self ) : \n 
~~~ source = self . _xml_element . find ( ) \n 
target = self . _xml_element . find ( ) \n 
self . _replace_attr ( source , , self . source_iface ) \n 
self . _replace_attr ( target , , self . target_iface ) \n 
return self . _xml_element \n 
~~ ~~ class LibvirtXml ( object ) : \n 
~~~ def __init__ ( self , contents ) : \n 
self . _xml = ElementTree . fromstring ( contents ) \n 
self . _interfaces = [ LibvirtDeviceInterface ( i ) \n 
for i in self . _xml . findall ( ) ] \n 
self . disk_file = self . _get ( , ) \n 
self . serial_file = self . _get ( , ) \n 
self . console_file = self . _get ( , ) \n 
~~ def _get ( self , element , attribute ) : \n 
~~~ el = self . _xml . find ( element ) \n 
if el is not None : \n 
~~~ return el . get ( attribute ) \n 
~~ ~~ def _set ( self , element , attribute , value ) : \n 
~~~ el . set ( attribute , value ) \n 
def interfaces ( self ) : \n 
~~~ return self . _interfaces \n 
~~ @ interfaces . setter \n 
def interfaces ( self , other ) : \n 
if len ( self . interfaces ) != len ( other ) : \n 
~~ for other_iface in other : \n 
~~~ for this_iface in self . interfaces : \n 
~~~ identical = ( this_iface . mac == other_iface . mac ) \n 
if identical : \n 
~~~ this_iface . source_iface = other_iface . source_iface \n 
this_iface . target_iface = other_iface . target_iface \n 
~~ ~~ ~~ ~~ def dump ( self ) : \n 
~~~ self . _set ( , , self . disk_file ) \n 
self . _set ( , , self . serial_file ) \n 
self . _set ( , , self . console_file ) \n 
xml_devices = self . _xml . find ( ) \n 
xml_interfaces = self . _xml . findall ( ) \n 
for iface in xml_interfaces : \n 
~~~ xml_devices . remove ( iface ) \n 
~~ for iface in self . _interfaces : \n 
~~~ xml_devices . append ( iface . element ( ) ) \n 
~~ return ElementTree . tostring ( self . _xml ) \n 
~~ ~~ import abc \n 
from cloudferry . lib . utils import files \n 
from cloudferry . lib . utils import remote_runner \n 
from cloudferry . lib . copy_engines import base \n 
class CopyFailed ( RuntimeError ) : \n 
~~ class CopyMechanism ( object ) : \n 
~~~ __metaclass__ = abc . ABCMeta \n 
@ abc . abstractmethod \n 
def copy ( self , context , source_object , destination_object ) : \n 
~~ ~~ class CopyObject ( object ) : \n 
~~~ def __init__ ( self , host = None , path = None ) : \n 
~~~ self . host = host \n 
~~~ return "{host}:{path}" . format ( host = self . host , path = self . path ) \n 
~~ ~~ class RemoteFileCopy ( CopyMechanism ) : \n 
~~~ data = { \n 
: source_object . host , \n 
: source_object . path , \n 
: destination_object . host , \n 
: destination_object . path \n 
~~~ copier = base . get_copier ( context . src_cloud , \n 
context . dst_cloud , \n 
data ) \n 
copier . transfer ( data ) \n 
~~ except ( base . FileCopyError , \n 
base . CopierCannotBeUsed , \n 
base . CopierNotFound ) as e : \n 
src_host = source_object . host , \n 
src_file = source_object . path , \n 
dst_host = destination_object . host , \n 
dst_file = destination_object . path , \n 
err = e . message ) \n 
raise CopyFailed ( msg ) \n 
~~ ~~ ~~ class CopyRegularFileToBlockDevice ( CopyMechanism ) : \n 
~~~ src_user = context . cfg . src . ssh_user \n 
dst_user = context . cfg . dst . ssh_user \n 
src_host = source_object . host \n 
dst_host = destination_object . host \n 
rr = remote_runner . RemoteRunner ( src_host , src_user ) \n 
ssh_opts = ( \n 
~~~ progress_view = "" \n 
if files . is_installed ( rr , "pv" ) : \n 
~~~ src_file_size = files . remote_file_size ( rr , source_object . path ) \n 
size = src_file_size ) \n 
rr . run ( copy . format ( src_file = source_object . path , \n 
dst_user = dst_user , \n 
dst_host = dst_host , \n 
ssh_opts = ssh_opts , \n 
dst_device = destination_object . path , \n 
progress_view = progress_view ) ) \n 
~~ except remote_runner . RemoteExecutionError as e : \n 
msg = msg . format ( src_object = source_object , \n 
dst_object = destination_object , \n 
error = e . message ) \n 
~~ ~~ ~~ import datetime \n 
from logging import config \n 
from logging import handlers \n 
from fabric import api \n 
import yaml \n 
from cloudferry . lib . utils import sizeof_format \n 
getLogger = logging . getLogger \n 
class StdoutLogger ( object ) : \n 
def __init__ ( self , name = None ) : \n 
~~~ self . log = logging . getLogger ( name or ) \n 
~~ def write ( self , message ) : \n 
~~~ message = message . strip ( ) \n 
if message : \n 
~~~ self . log . info ( message ) \n 
~~ ~~ def flush ( self ) : \n 
~~ ~~ def configure_logging ( log_config = None , debug = None , forward_stdout = None ) : \n 
if log_config is None : \n 
~~~ log_config = CONF . migrate . log_config \n 
~~ if debug is None : \n 
~~~ debug = CONF . migrate . debug \n 
~~ if forward_stdout is None : \n 
~~~ forward_stdout = CONF . migrate . forward_stdout \n 
~~ with open ( log_config , ) as f : \n 
~~~ config . dictConfig ( yaml . load ( f ) ) \n 
~~ if debug : \n 
~~~ logger = logging . getLogger ( ) \n 
for handler in logger . handlers : \n 
~~~ if handler . name == : \n 
~~~ handler . setLevel ( logging . DEBUG ) \n 
~~ ~~ ~~ if forward_stdout : \n 
~~~ sys . stdout = StdoutLogger ( ) \n 
~~ ~~ class RunRotatingFileHandler ( handlers . RotatingFileHandler ) : \n 
filename = , \n 
date_format = , \n 
** kwargs ) : \n 
~~~ self . date_format = date_format \n 
max_bytes = sizeof_format . parse_size ( kwargs . pop ( , 0 ) ) \n 
super ( RunRotatingFileHandler , self ) . __init__ ( \n 
filename = self . get_filename ( filename ) , \n 
maxBytes = max_bytes , \n 
** kwargs ) \n 
~~ def get_filename ( self , filename ) : \n 
if hasattr ( CONF , ) and hasattr ( CONF . migrate , ) : \n 
~~~ scenario_filename = os . path . basename ( CONF . migrate . scenario ) \n 
scenario = os . path . splitext ( scenario_filename ) [ 0 ] \n 
~~~ scenario = \n 
~~ dt = datetime . datetime . now ( ) . strftime ( self . date_format ) \n 
return filename % { \n 
: scenario , \n 
: dt \n 
~~ ~~ class CurrentTaskFilter ( logging . Filter ) : \n 
def __init__ ( self , name_format = , ** kwargs ) : \n 
~~~ super ( CurrentTaskFilter , self ) . __init__ ( ** kwargs ) \n 
self . name_format = name_format \n 
~~ def filter ( self , record ) : \n 
~~~ current_task = self . name_format % { \n 
: api . env . current_task or , \n 
record . current_task = current_task \n 
import cloudferry_devlab . tests . config as config \n 
from cloudferry_devlab . tests . data_collector import DataCollector \n 
from cloudferry_devlab . tests import functional_test \n 
import cloudferry_devlab . tests . utils as utils \n 
class RollbackVerification ( functional_test . FunctionalTest ) : \n 
~~~ data_collector = DataCollector ( config = config ) \n 
self . data_after = utils . convert ( data_collector . data_collector ( ) ) \n 
file_name = config . rollback_params [ ] [ ] \n 
pre_file_path = os . path . join ( self . cloudferry_dir , file_name ) \n 
with open ( pre_file_path , "r" ) as f : \n 
~~~ self . pre_data = yaml . load ( f ) \n 
~~ ~~ def test_verify_rollback ( self ) : \n 
self . maxDiff = None \n 
for cloud in self . data_after : \n 
~~~ for service in self . data_after [ cloud ] : \n 
~~~ for resource in self . data_after [ cloud ] [ service ] : \n 
~~~ print ( msg . format ( service . lower ( ) , resource . lower ( ) ) ) \n 
self . assertEqual ( self . data_after [ cloud ] [ service ] [ resource ] , \n 
self . pre_data [ cloud ] [ service ] [ resource ] ) \n 
~~ ~~ ~~ ~~ ~~ import mock \n 
from cloudferry . lib . os . actions import convert_volume_to_image \n 
from tests import test \n 
class ConverterVolumeToImageTest ( test . TestCase ) : \n 
~~~ super ( ConverterVolumeToImageTest , self ) . setUp ( ) \n 
self . fake_src_cloud = mock . Mock ( ) \n 
self . fake_storage = mock . Mock ( ) \n 
self . fake_storage . deploy = mock . Mock ( ) \n 
self . fake_storage . upload_volume_to_image . return_value = ( \n 
, ) \n 
self . fake_storage . get_backend . return_value = \n 
self . fake_image = mock . Mock ( ) \n 
self . fake_image . wait_for_status = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted = mock . Mock ( ) \n 
self . fake_image . get_image_by_id_converted . return_value = { \n 
: { \n 
: { : , : { } } } } \n 
self . fake_image . patch_image = mock . Mock ( ) \n 
self . fake_src_cloud . resources = { : self . fake_storage , \n 
: self . fake_image } \n 
self . fake_volumes_info = { \n 
} } , \n 
self . fake_dst_cloud = mock . Mock ( ) \n 
self . fake_config = utils . ext_dict ( migrate = utils . ext_dict ( \n 
{ : , \n 
: } ) ) \n 
self . fake_init = { \n 
: self . fake_src_cloud , \n 
: self . fake_dst_cloud , \n 
: self . fake_config \n 
~~ def test_action ( self ) : \n 
~~~ fake_action = convert_volume_to_image . ConvertVolumeToImage ( \n 
self . fake_init , \n 
cloud = ) \n 
res = fake_action . run ( self . fake_volumes_info ) \n 
self . assertEqual ( , \n 
res [ ] [ ] [ ] [ ] ) \n 
res [ ] [ ] [ ] [ ] [ \n 
] [ ] ) \n 
~~ ~~ from cloudferry . lib . utils . cache import Memoized , Cached \n 
class MemoizationTestCase ( test . TestCase ) : \n 
~~~ def test_treats_self_as_separate_objects ( self ) : \n 
~~~ class C ( object ) : \n 
~~~ def __init__ ( self , i ) : \n 
~~~ self . i = i \n 
~~ @ Memoized \n 
def get_i ( self ) : \n 
~~~ return self . i \n 
~~ ~~ o1 = C ( 1 ) \n 
o2 = C ( 2 ) \n 
self . assertNotEqual ( o1 . get_i ( ) , o2 . get_i ( ) ) \n 
self . assertEqual ( o1 . get_i ( ) , 1 ) \n 
self . assertEqual ( o2 . get_i ( ) , 2 ) \n 
~~ def test_takes_value_from_cache ( self ) : \n 
~~ def set_i ( self , i ) : \n 
~~ ~~ original = 1 \n 
o = C ( original ) \n 
self . assertEqual ( o . get_i ( ) , original ) \n 
o . set_i ( 10 ) \n 
~~ ~~ class CacheTestCase ( test . TestCase ) : \n 
~~~ def test_resets_cache_when_modifier_called ( self ) : \n 
~~~ @ Cached ( getter = , modifier = ) \n 
class C ( object ) : \n 
~~ def get_i ( self ) : \n 
~~ ~~ o = C ( 1 ) \n 
self . assertEqual ( o . get_i ( ) , 1 ) \n 
o . set_i ( 100 ) \n 
self . assertEqual ( o . get_i ( ) , 100 ) \n 
~~ ~~ from django . http import HttpResponse , HttpResponseRedirect , HttpResponseNotFound \n 
from django . template import Context , loader \n 
from django . core . urlresolvers import reverse \n 
from django . shortcuts import get_object_or_404 , render_to_response \n 
from django . core . exceptions import ObjectDoesNotExist \n 
from tagging . models import Tag , TaggedItem \n 
from django . views . decorators . csrf import csrf_exempt \n 
from django . contrib . auth . models import User \n 
from django . contrib . auth . decorators import login_required \n 
from django . contrib . auth import authenticate , login \n 
from django . core . mail import send_mail \n 
from django . http import Http404 \n 
from django . db . models import Q \n 
from openwatch . recordings . models import Recording \n 
from openwatch import recording_tags \n 
@ login_required \n 
def moderate ( request ) : \n 
response_values = { } \n 
org_tag = request . user . get_profile ( ) . org_tag \n 
if not request . user . is_superuser and ( not request . user . get_profile ( ) . can_moderate or org_tag == ) : \n 
~~~ raise Http404 \n 
~~ if recording_tags . ACLU_NJ in org_tag : \n 
~~~ location = { } \n 
location [ ] = 40.167274 \n 
location [ ] = - 74.616338 \n 
response_values [ ] = location \n 
~~ response_values [ ] = \n 
return render_to_response ( , response_values , context_instance = RequestContext ( request ) ) \n 
~~ def map ( request ) : \n 
~~~ total = "lots!" \n 
return render_to_response ( , { : total } , context_instance = RequestContext ( request ) ) \n 
~~ def size ( request ) : \n 
~~~ featureset = Recording . objects . filter ( ~ Q ( lat = None ) , ~ Q ( lon = None ) , ~ Q ( jtype = ) ) . exclude ( location__exact = ) . exclude ( location__exact = ) . order_by ( ) \n 
total = len ( featureset ) \n 
~~ def redir ( self ) : \n 
~~~ return HttpResponseRedirect ( ) \n 
~~ def map_json ( request ) : \n 
~~~ featureset = Recording . objects . all ( ) . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) [ : 750 ] \n 
resp = encode_queryset ( featureset ) \n 
return HttpResponse ( resp , mimetype = "application/json" ) \n 
~~ @ login_required \n 
def map_json_moderate ( request ) : \n 
~~~ org_tag = request . user . get_profile ( ) . org_tag \n 
if org_tag != : \n 
~~~ featureset = Recording . objects . filter ( org_approved = False , org_flagged = False , tags__contains = org_tag ) \n 
~~~ featureset = Recording . objects . all ( ) \n 
~~ featureset = featureset . order_by ( ) . filter ( ~ Q ( location = ) ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) \n 
~~ def map_location_json ( request , ne_lat = 0 , ne_lon = 0 , sw_lat = 0 , sw_lon = 0 ) : \n 
~~~ ne_lat = float ( ne_lat ) \n 
ne_lon = float ( ne_lon ) \n 
sw_lat = float ( sw_lat ) \n 
sw_lon = float ( sw_lon ) \n 
featureset = Recording . objects . filter ( lat__lt = ne_lat , lat__gt = sw_lat , lon__lt = ne_lon , lon__gt = sw_lon ) . order_by ( ) . exclude ( location__isnull = True ) . exclude ( location__exact = ) . exclude ( location__exact = ) . exclude ( location__exact = ) [ : 750 ] \n 
if len ( featureset ) < 1 : \n 
~~~ return HttpResponse ( "{\\"objects\\":[]}" , mimetype = "application/json" ) \n 
~~ resp = encode_queryset ( featureset ) \n 
~~ def encode_queryset ( featureset ) : \n 
~~~ resp = \'{"objects":[\' \n 
for obj in featureset : \n 
~~~ resp = resp + json . dumps ( obj . to_dict ( ) ) + \n 
~~ resp = resp [ : - 1 ] + \n 
~~ from django . template import loader , RequestContext \n 
from django . http import HttpResponse , Http404 \n 
from django . http import HttpResponseRedirect , HttpResponsePermanentRedirect \n 
from django . db . models . base import ModelBase \n 
from django . db . models . manager import Manager \n 
from django . db . models . query import QuerySet \n 
from django . core import urlresolvers \n 
from django . utils import six \n 
~~~ import json \n 
def render_to_easy_api_response ( * args , ** kwargs ) : \n 
httpresponse_kwargs = { : kwargs . pop ( , None ) } \n 
context = kwargs . pop ( ) \n 
processors = context . context_processors \n 
request = processors [ ] [ ] \n 
if request . GET . has_key ( ) : \n 
~~~ api_type = request . GET [ ] \n 
for arg in args : \n 
~~~ passed = arg \n 
~~ dump_me = { } \n 
for key in passed . keys ( ) : \n 
~~~ value = passed [ key ] \n 
dump_me [ key ] = dump_object ( value ) \n 
~~ if api_type == : \n 
~~~ def replace_spaces ( dump_me ) : \n 
~~~ new = { } \n 
for k , v in dump_me . iteritems ( ) : \n 
~~~ if isinstance ( v , dict ) : \n 
~~~ v = replace_spaces ( v ) \n 
~~ new [ k . replace ( , ) ] = v \n 
~~ return new \n 
~~ new = replace_spaces ( dump_me ) \n 
dump_me = dict2xml ( new ) \n 
pretty = dom . toprettyxml ( ) \n 
return HttpResponse ( pretty , content_type = ) \n 
~~~ yml = yaml . safe_dump ( dump_me ) \n 
return HttpResponse ( yml , content_type = ) \n 
return HttpResponse ( dump_me , content_type = ) \n 
~~ ~~ return HttpResponse ( loader . render_to_string ( * args , ** kwargs ) , ** httpresponse_kwargs ) \n 
~~ def render_to_response ( * args , ** kwargs ) : \n 
return render_to_easy_api_response ( * args , ** kwargs ) \n 
## \n 
~~ def dump_object ( queryset ) : \n 
~~~ d = DataDumper ( ) \n 
ret = d . dump ( queryset ) \n 
return ret \n 
~~~ modelName = queryset [ 0 ] . __class__ . __name__ \n 
modelNameData = [ ] \n 
fields = get_fields ( queryset [ 0 ] ) \n 
for obj in queryset : \n 
~~~ temp_dict = dict ( ) \n 
for field in fields : \n 
~~~ attribute = getattr ( obj , str ( field ) ) \n 
temp_dict [ field ] = attribute \n 
~~ ~~ modelNameData . append ( temp_dict ) \n 
~~ dthandler = lambda obj : obj . isoformat ( ) if isinstance ( obj , datetime . datetime ) or isinstance ( obj , datetime . date ) else None \n 
return json . loads ( json . dumps ( modelNameData , default = dthandler ) ) \n 
~~ ~~ def get_fields ( model ) : \n 
~~~ if hasattr ( model , "easy_api_fields" ) : \n 
~~~ fields = model . easy_api_fields ( ) \n 
~~~ fields = model . to_dict ( ) . keys ( ) \n 
~~~ fields = model . _meta . get_all_field_names ( ) \n 
~~ ~~ return fields \n 
~~ ~~ class SimpleEngagementCalculator ( object ) : \n 
~~~ def calculate_user_engagement_score ( self , user , start_date , end_date ) : \n 
~~ ~~ ROOT_URLCONF = None \n 
DATABASE_ENGINE = \n 
DATABASE_NAME = \n 
DATABASE_SUPPORTS_TRANSACTIONS = False \n 
INSTALLED_APPS = [ \n 
TEMPLATE_CONTEXT_PROCESSORS = ( \n 
"django.core.context_processors.auth" , \n 
"django.core.context_processors.debug" , \n 
"django.core.context_processors.i18n" , \n 
"django.core.context_processors.media" , \n 
"django.core.context_processors.request" ) \n 
if __name__ == "__main__" : \n 
~~~ os . environ . setdefault ( "DJANGO_SETTINGS_MODULE" , "test_settings" ) \n 
from django . core . management import execute_from_command_line \n 
is_testing = in sys . argv \n 
if is_testing : \n 
~~~ import coverage \n 
cov = coverage . coverage ( include = "django_zappa/*" , omit = [ ] ) \n 
cov . erase ( ) \n 
cov . start ( ) \n 
~~ execute_from_command_line ( sys . argv ) \n 
~~~ cov . stop ( ) \n 
cov . save ( ) \n 
cov . report ( ) \n 
import simplexml , time , sys \n 
from protocol import * \n 
from client import PlugIn \n 
DefaultTimeout = 25 \n 
ID = 0 \n 
class Dispatcher ( PlugIn ) : \n 
~~~ PlugIn . __init__ ( self ) \n 
DBG_LINE = \n 
self . handlers = { } \n 
self . _expected = { } \n 
self . _defaultHandler = None \n 
self . _pendingExceptions = [ ] \n 
self . _eventHandler = None \n 
self . _cycleHandlers = [ ] \n 
self . _exported_methods = [ self . Process , self . RegisterHandler , self . RegisterDefaultHandler , self . RegisterEventHandler , self . UnregisterCycleHandler , self . RegisterCycleHandler , self . RegisterHandlerOnce , self . UnregisterHandler , self . RegisterProtocol , self . WaitForResponse , self . SendAndWaitForResponse , self . send , self . disconnect , self . SendAndCallForResponse , ] \n 
~~ def dumpHandlers ( self ) : \n 
return self . handlers \n 
~~ def restoreHandlers ( self , handlers ) : \n 
self . handlers = handlers \n 
~~ def _init ( self ) : \n 
self . RegisterNamespace ( ) \n 
self . RegisterNamespace ( NS_STREAMS ) \n 
self . RegisterNamespace ( self . _owner . defaultNamespace ) \n 
self . RegisterProtocol ( , Iq ) \n 
self . RegisterProtocol ( , Presence ) \n 
self . RegisterProtocol ( , Message ) \n 
self . RegisterDefaultHandler ( self . returnStanzaHandler ) \n 
self . RegisterHandler ( , self . streamErrorHandler , xmlns = NS_STREAMS ) \n 
~~ def plugin ( self , owner ) : \n 
self . _init ( ) \n 
for method in self . _old_owners_methods : \n 
~~~ if method . __name__ == : self . _owner_send = method ; break \n 
~~ self . _owner . lastErrNode = None \n 
self . _owner . lastErr = None \n 
self . _owner . lastErrCode = None \n 
self . StreamInit ( ) \n 
~~ def plugout ( self ) : \n 
self . Stream . dispatch = None \n 
self . Stream . DEBUG = None \n 
self . Stream . features = None \n 
self . Stream . destroy ( ) \n 
~~ def StreamInit ( self ) : \n 
self . Stream = simplexml . NodeBuilder ( ) \n 
self . Stream . _dispatch_depth = 2 \n 
self . Stream . dispatch = self . dispatch \n 
self . Stream . stream_header_received = self . _check_stream_start \n 
self . _owner . debug_flags . append ( simplexml . DBG_NODEBUILDER ) \n 
self . Stream . DEBUG = self . _owner . DEBUG \n 
self . _metastream = Node ( ) \n 
self . _metastream . setNamespace ( self . _owner . Namespace ) \n 
self . _metastream . setAttr ( , ) \n 
self . _metastream . setAttr ( , NS_STREAMS ) \n 
self . _metastream . setAttr ( , self . _owner . Server ) \n 
~~ def _check_stream_start ( self , ns , tag , attrs ) : \n 
~~~ if ns < > NS_STREAMS or tag < > : \n 
~~~ raise ValueError ( % ( tag , ns ) ) \n 
~~ ~~ def Process ( self , timeout = 0 ) : \n 
for handler in self . _cycleHandlers : handler ( self ) \n 
if len ( self . _pendingExceptions ) > 0 : \n 
~~~ _pendingException = self . _pendingExceptions . pop ( ) \n 
raise _pendingException [ 0 ] , _pendingException [ 1 ] , _pendingException [ 2 ] \n 
~~ if self . _owner . Connection . pending_data ( timeout ) : \n 
~~~ try : data = self . _owner . Connection . receive ( ) \n 
except IOError : return \n 
self . Stream . Parse ( data ) \n 
~~ if data : return len ( data ) \n 
~~ def RegisterNamespace ( self , xmlns , order = ) : \n 
self . handlers [ xmlns ] = { } \n 
self . RegisterProtocol ( , Protocol , xmlns = xmlns ) \n 
~~ def RegisterProtocol ( self , tag_name , Proto , xmlns = None , order = ) : \n 
if not xmlns : xmlns = self . _owner . defaultNamespace \n 
self . handlers [ xmlns ] [ tag_name ] = { type : Proto , : [ ] } \n 
~~ def RegisterNamespaceHandler ( self , xmlns , handler , typ = , ns = , makefirst = 0 , system = 0 ) : \n 
self . RegisterHandler ( , handler , typ , ns , xmlns , makefirst , system ) \n 
~~ def RegisterHandler ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
if not typ and not ns : typ = \n 
if not self . handlers . has_key ( xmlns ) : self . RegisterNamespace ( xmlns , ) \n 
if not self . handlers [ xmlns ] . has_key ( name ) : self . RegisterProtocol ( name , Protocol , xmlns , ) \n 
if not self . handlers [ xmlns ] [ name ] . has_key ( typ + ns ) : self . handlers [ xmlns ] [ name ] [ typ + ns ] = [ ] \n 
if makefirst : self . handlers [ xmlns ] [ name ] [ typ + ns ] . insert ( 0 , { : handler , : system } ) \n 
else : self . handlers [ xmlns ] [ name ] [ typ + ns ] . append ( { : handler , : system } ) \n 
~~ def RegisterHandlerOnce ( self , name , handler , typ = , ns = , xmlns = None , makefirst = 0 , system = 0 ) : \n 
self . RegisterHandler ( name , handler , typ , ns , xmlns , makefirst , system ) \n 
~~ def UnregisterHandler ( self , name , handler , typ = , ns = , xmlns = None ) : \n 
if not self . handlers . has_key ( xmlns ) : return \n 
for pack in self . handlers [ xmlns ] [ name ] [ typ + ns ] : \n 
~~~ if handler == pack [ ] : break \n 
~~ else : pack = None \n 
try : self . handlers [ xmlns ] [ name ] [ typ + ns ] . remove ( pack ) \n 
except ValueError : pass \n 
~~ def RegisterDefaultHandler ( self , handler ) : \n 
self . _defaultHandler = handler \n 
~~ def RegisterEventHandler ( self , handler ) : \n 
self . _eventHandler = handler \n 
~~ def returnStanzaHandler ( self , conn , stanza ) : \n 
if stanza . getType ( ) in [ , ] : \n 
~~~ conn . send ( Error ( stanza , ERR_FEATURE_NOT_IMPLEMENTED ) ) \n 
~~ ~~ def streamErrorHandler ( self , conn , error ) : \n 
~~~ name , text = , error . getData ( ) \n 
for tag in error . getChildren ( ) : \n 
~~~ if tag . getNamespace ( ) == NS_XMPP_STREAMS : \n 
~~~ if tag . getName ( ) == : text = tag . getData ( ) \n 
else : name = tag . getName ( ) \n 
~~ ~~ if name in stream_exceptions . keys ( ) : exc = stream_exceptions [ name ] \n 
else : exc = StreamError \n 
raise exc ( ( name , text ) ) \n 
~~ def RegisterCycleHandler ( self , handler ) : \n 
if handler not in self . _cycleHandlers : self . _cycleHandlers . append ( handler ) \n 
~~ def UnregisterCycleHandler ( self , handler ) : \n 
if handler in self . _cycleHandlers : self . _cycleHandlers . remove ( handler ) \n 
~~ def Event ( self , realm , event , data ) : \n 
if self . _eventHandler : self . _eventHandler ( realm , event , data ) \n 
~~ def dispatch ( self , stanza , session = None , direct = 0 ) : \n 
if not session : session = self \n 
session . Stream . _mini_dom = None \n 
name = stanza . getName ( ) \n 
if not direct and self . _owner . _route : \n 
~~~ if name == : \n 
~~~ if stanza . getAttr ( ) == None : \n 
~~~ if len ( stanza . getChildren ( ) ) == 1 : \n 
~~~ stanza = stanza . getChildren ( ) [ 0 ] \n 
~~~ for each in stanza . getChildren ( ) : \n 
~~~ self . dispatch ( each , session , direct = 1 ) \n 
~~ return \n 
~~ ~~ ~~ elif name == : \n 
~~ elif name in ( , ) : \n 
~~~ raise UnsupportedStanzaType ( name ) \n 
~~ ~~ if name == : session . Stream . features = stanza \n 
xmlns = stanza . getNamespace ( ) \n 
if not self . handlers . has_key ( xmlns ) : \n 
xmlns = \n 
~~ if not self . handlers [ xmlns ] . has_key ( name ) : \n 
name = \n 
~~ if stanza . __class__ . __name__ == : stanza = self . handlers [ xmlns ] [ name ] [ type ] ( node = stanza ) \n 
typ = stanza . getType ( ) \n 
if not typ : typ = \n 
stanza . props = stanza . getProperties ( ) \n 
ID = stanza . getID ( ) \n 
for prop in stanza . props : \n 
~~~ if self . handlers [ xmlns ] [ name ] . has_key ( prop ) : list . append ( prop ) \n 
~~ chain = self . handlers [ xmlns ] [ ] [ ] \n 
for key in list : \n 
~~~ if key : chain = chain + self . handlers [ xmlns ] [ name ] [ key ] \n 
~~ output = \n 
if session . _expected . has_key ( ID ) : \n 
~~~ user = 0 \n 
if type ( session . _expected [ ID ] ) == type ( ( ) ) : \n 
~~~ cb , args = session . _expected [ ID ] \n 
try : cb ( session , stanza , ** args ) \n 
except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : raise \n 
session . _expected [ ID ] = stanza \n 
~~ ~~ else : user = 1 \n 
for handler in chain : \n 
~~~ if user or handler [ ] : \n 
~~~ handler [ ] ( session , stanza ) \n 
~~ except Exception , typ : \n 
~~~ if typ . __class__ . __name__ < > : \n 
~~~ self . _pendingExceptions . insert ( 0 , sys . exc_info ( ) ) \n 
~~ user = 0 \n 
~~ ~~ ~~ if user and self . _defaultHandler : self . _defaultHandler ( session , stanza ) \n 
~~ def WaitForResponse ( self , ID , timeout = DefaultTimeout ) : \n 
self . _expected [ ID ] = None \n 
has_timed_out = 0 \n 
abort_time = time . time ( ) + timeout \n 
while not self . _expected [ ID ] : \n 
~~~ if not self . Process ( 0.04 ) : \n 
~~~ self . _owner . lastErr = "Disconnect" \n 
return None \n 
~~ if time . time ( ) > abort_time : \n 
~~~ self . _owner . lastErr = "Timeout" \n 
~~ ~~ response = self . _expected [ ID ] \n 
del self . _expected [ ID ] \n 
if response . getErrorCode ( ) : \n 
~~~ self . _owner . lastErrNode = response \n 
self . _owner . lastErr = response . getError ( ) \n 
self . _owner . lastErrCode = response . getErrorCode ( ) \n 
~~ return response \n 
~~ def SendAndWaitForResponse ( self , stanza , timeout = DefaultTimeout ) : \n 
return self . WaitForResponse ( self . send ( stanza ) , timeout ) \n 
~~ def SendAndCallForResponse ( self , stanza , func , args = { } ) : \n 
self . _expected [ self . send ( stanza ) ] = ( func , args ) \n 
~~ def send ( self , stanza ) : \n 
if type ( stanza ) in [ type ( ) , type ( ) ] : return self . _owner_send ( stanza ) \n 
if not isinstance ( stanza , Protocol ) : _ID = None \n 
elif not stanza . getID ( ) : \n 
~~~ global ID \n 
ID += 1 \n 
_ID = ` ID ` \n 
stanza . setID ( _ID ) \n 
~~ else : _ID = stanza . getID ( ) \n 
if self . _owner . _registered_name and not stanza . getAttr ( ) : stanza . setAttr ( , self . _owner . _registered_name ) \n 
if self . _owner . _route and stanza . getName ( ) != : \n 
~~~ to = self . _owner . Server \n 
if stanza . getTo ( ) and stanza . getTo ( ) . getDomain ( ) : \n 
~~~ to = stanza . getTo ( ) . getDomain ( ) \n 
~~ frm = stanza . getFrom ( ) \n 
if frm . getDomain ( ) : \n 
~~~ frm = frm . getDomain ( ) \n 
~~ route = Protocol ( , to = to , frm = frm , payload = [ stanza ] ) \n 
stanza = route \n 
~~ stanza . setNamespace ( self . _owner . Namespace ) \n 
stanza . setParent ( self . _metastream ) \n 
self . _owner_send ( stanza ) \n 
return _ID \n 
~~ def disconnect ( self ) : \n 
self . _owner_send ( ) \n 
while self . Process ( 1 ) : pass \n 
~~ ~~ from . gl_utils import * \n 
from . texture import VideoTexture \n 
from . widget import Widget , BGUI_DEFAULT , WeakMethod \n 
from . image import Image \n 
class Video ( Image ) : \n 
def __init__ ( self , parent , vid , name = None , play_audio = False , repeat = 0 , aspect = None , size = [ 1 , 1 ] , pos = [ 0 , 0 ] , \n 
sub_theme = , options = BGUI_DEFAULT ) : \n 
Image . __init__ ( self , parent , name , None , aspect , size , pos , sub_theme = sub_theme , options = options ) \n 
self . _texture = VideoTexture ( vid , GL_LINEAR , repeat , play_audio ) \n 
self . _on_finish = None \n 
self . _on_finish_called = False \n 
~~ def play ( self , start , end , use_frames = True , fps = None ) : \n 
~~~ self . _texture . play ( start , end , use_frames , fps ) \n 
def on_finish ( self ) : \n 
return self . _on_finish \n 
~~ @ on_finish . setter \n 
def on_finish ( self , value ) : \n 
~~~ self . _on_finish = WeakMethod ( value ) \n 
~~ def _draw ( self ) : \n 
self . _texture . update ( ) \n 
Image . _draw ( self ) \n 
if self . _texture . video . status == 3 : \n 
~~~ if self . _on_finish and not self . _on_finish_called : \n 
~~~ self . on_finish ( self ) \n 
self . _on_finish_called = Truefrom django import template \n 
~~ ~~ ~~ ~~ from django . conf import settings \n 
class CheckGrappelli ( template . Node ) : \n 
~~~ def __init__ ( self , var_name ) : \n 
~~~ self . var_name = var_name \n 
~~~ context [ self . var_name ] = in settings . INSTALLED_APPS \n 
~~ ~~ def check_grappelli ( parser , token ) : \n 
bits = token . contents . split ( ) \n 
if len ( bits ) != 3 : \n 
~~ if bits [ 1 ] != : \n 
~~ varname = bits [ 2 ] \n 
return CheckGrappelli ( varname ) \n 
~~ register . tag ( check_grappelli ) \n 
from setuptools import setup , find_packages \n 
import sys , os \n 
__description__ = , \n 
__author__ = , \n 
__email__ = , \n 
sys . path . insert ( 0 , os . path . dirname ( __file__ ) ) \n 
REQUIRES = [ i . strip ( ) for i in open ( "requirements.txt" ) . readlines ( ) ] \n 
setup ( \n 
version = __version__ , \n 
url = , \n 
download_url = , \n 
license = __license__ , \n 
author = __author__ , \n 
author_email = __email__ , \n 
description = __description__ , \n 
long_description = __doc__ , \n 
test_suite = , \n 
zip_safe = False , \n 
platforms = , \n 
install_requires = REQUIRES , \n 
packages = find_packages ( exclude = ( , , ) ) , \n 
include_package_data = True , \n 
setup_requires = [ , ] , \n 
classifiers = [ \n 
from mongoengine . base import BaseField \n 
__all__ = ( ) \n 
class WtfBaseField ( BaseField ) : \n 
def __init__ ( self , validators = None , filters = None , ** kwargs ) : \n 
~~~ self . validators = self . _ensure_callable_or_list ( validators , ) \n 
self . filters = self . _ensure_callable_or_list ( filters , ) \n 
BaseField . __init__ ( self , ** kwargs ) \n 
~~ def _ensure_callable_or_list ( self , field , msg_flag ) : \n 
if field is not None : \n 
~~~ if callable ( field ) : \n 
~~~ field = [ field ] \n 
if not isinstance ( field , list ) : \n 
~~~ raise TypeError ( msg ) \n 
~~ ~~ ~~ return field \n 
~~ ~~ from bson import DBRef , SON \n 
from mongoengine . python_support import txt_type \n 
from base import ( \n 
BaseDict , BaseList , EmbeddedDocumentList , \n 
TopLevelDocumentMetaclass , get_document \n 
from fields import ( ReferenceField , ListField , DictField , MapField ) \n 
from connection import get_db \n 
from queryset import QuerySet \n 
from document import Document , EmbeddedDocument \n 
class DeReference ( object ) : \n 
~~~ def __call__ ( self , items , max_depth = 1 , instance = None , name = None ) : \n 
if items is None or isinstance ( items , basestring ) : \n 
~~~ return items \n 
~~ if isinstance ( items , QuerySet ) : \n 
~~~ items = [ i for i in items ] \n 
~~ self . max_depth = max_depth \n 
doc_type = None \n 
if instance and isinstance ( instance , ( Document , EmbeddedDocument , \n 
TopLevelDocumentMetaclass ) ) : \n 
~~~ doc_type = instance . _fields . get ( name ) \n 
while hasattr ( doc_type , ) : \n 
~~~ doc_type = doc_type . field \n 
~~ if isinstance ( doc_type , ReferenceField ) : \n 
~~~ field = doc_type \n 
doc_type = doc_type . document_type \n 
is_list = not hasattr ( items , ) \n 
if is_list and all ( [ i . __class__ == doc_type for i in items ] ) : \n 
~~ elif not is_list and all ( \n 
[ i . __class__ == doc_type for i in items . values ( ) ] ) : \n 
~~ elif not field . dbref : \n 
~~~ if not hasattr ( items , ) : \n 
~~~ def _get_items ( items ) : \n 
~~~ new_items = [ ] \n 
for v in items : \n 
~~~ if isinstance ( v , list ) : \n 
~~~ new_items . append ( _get_items ( v ) ) \n 
~~ elif not isinstance ( v , ( DBRef , Document ) ) : \n 
~~~ new_items . append ( field . to_python ( v ) ) \n 
~~~ new_items . append ( v ) \n 
~~ ~~ return new_items \n 
~~ items = _get_items ( items ) \n 
~~~ items = dict ( [ \n 
( k , field . to_python ( v ) ) \n 
if not isinstance ( v , ( DBRef , Document ) ) else ( k , v ) \n 
for k , v in items . iteritems ( ) ] \n 
~~ ~~ ~~ ~~ self . reference_map = self . _find_references ( items ) \n 
self . object_map = self . _fetch_objects ( doc_type = doc_type ) \n 
return self . _attach_objects ( items , 0 , instance , name ) \n 
~~ def _find_references ( self , items , depth = 0 ) : \n 
reference_map = { } \n 
if not items or depth >= self . max_depth : \n 
~~~ return reference_map \n 
~~ if not hasattr ( items , ) : \n 
~~~ iterator = enumerate ( items ) \n 
~~~ iterator = items . iteritems ( ) \n 
~~ depth += 1 \n 
for k , item in iterator : \n 
~~~ if isinstance ( item , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in item . _fields . iteritems ( ) : \n 
~~~ v = item . _data . get ( field_name , None ) \n 
if isinstance ( v , DBRef ) : \n 
~~~ reference_map . setdefault ( field . document_type , set ( ) ) . add ( v . id ) \n 
~~ elif isinstance ( v , ( dict , SON ) ) and in v : \n 
~~~ reference_map . setdefault ( get_document ( v [ ] ) , set ( ) ) . add ( v [ ] . id ) \n 
~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ field_cls = getattr ( getattr ( field , , None ) , , None ) \n 
references = self . _find_references ( v , depth ) \n 
for key , refs in references . iteritems ( ) : \n 
~~~ if isinstance ( field_cls , ( Document , TopLevelDocumentMetaclass ) ) : \n 
~~~ key = field_cls \n 
~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ ~~ elif isinstance ( item , DBRef ) : \n 
~~~ reference_map . setdefault ( item . collection , set ( ) ) . add ( item . id ) \n 
~~ elif isinstance ( item , ( dict , SON ) ) and in item : \n 
~~~ reference_map . setdefault ( get_document ( item [ ] ) , set ( ) ) . add ( item [ ] . id ) \n 
~~ elif isinstance ( item , ( dict , list , tuple ) ) and depth - 1 <= self . max_depth : \n 
~~~ references = self . _find_references ( item , depth - 1 ) \n 
~~~ reference_map . setdefault ( key , set ( ) ) . update ( refs ) \n 
~~ ~~ ~~ return reference_map \n 
~~ def _fetch_objects ( self , doc_type = None ) : \n 
object_map = { } \n 
for collection , dbrefs in self . reference_map . iteritems ( ) : \n 
~~~ col_name = collection . _get_collection_name ( ) \n 
refs = [ dbref for dbref in dbrefs \n 
if ( col_name , dbref ) not in object_map ] \n 
references = collection . objects . in_bulk ( refs ) \n 
for key , doc in references . iteritems ( ) : \n 
~~~ object_map [ ( col_name , key ) ] = doc \n 
~~~ if isinstance ( doc_type , ( ListField , DictField , MapField , ) ) : \n 
~~ refs = [ dbref for dbref in dbrefs \n 
if ( collection , dbref ) not in object_map ] \n 
if doc_type : \n 
~~~ references = doc_type . _get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
for ref in references : \n 
~~~ doc = doc_type . _from_son ( ref ) \n 
object_map [ ( collection , doc . id ) ] = doc \n 
~~~ references = get_db ( ) [ collection ] . find ( { : { : refs } } ) \n 
~~~ if in ref : \n 
~~~ doc = get_document ( ref [ "_cls" ] ) . _from_son ( ref ) \n 
~~ elif doc_type is None : \n 
~~~ doc = get_document ( \n 
. join ( x . capitalize ( ) \n 
for x in collection . split ( ) ) ) . _from_son ( ref ) \n 
~~ object_map [ ( collection , doc . id ) ] = doc \n 
~~ ~~ ~~ ~~ return object_map \n 
~~ def _attach_objects ( self , items , depth = 0 , instance = None , name = None ) : \n 
if not items : \n 
~~~ if isinstance ( items , ( BaseDict , BaseList ) ) : \n 
~~ if instance : \n 
~~~ if isinstance ( items , dict ) : \n 
~~~ return BaseDict ( items , instance , name ) \n 
~~~ return BaseList ( items , instance , name ) \n 
~~ ~~ ~~ if isinstance ( items , ( dict , SON ) ) : \n 
~~~ if in items : \n 
~~~ return self . object_map . get ( \n 
( items [ ] . collection , items [ ] . id ) , items ) \n 
~~ elif in items : \n 
~~~ doc = get_document ( items [ ] ) . _from_son ( items ) \n 
_cls = doc . _data . pop ( , None ) \n 
del items [ ] \n 
doc . _data = self . _attach_objects ( doc . _data , depth , doc , None ) \n 
if _cls is not None : \n 
~~~ doc . _data [ ] = _cls \n 
~~ return doc \n 
~~ ~~ if not hasattr ( items , ) : \n 
~~~ is_list = True \n 
list_type = BaseList \n 
if isinstance ( items , EmbeddedDocumentList ) : \n 
~~~ list_type = EmbeddedDocumentList \n 
~~ as_tuple = isinstance ( items , tuple ) \n 
iterator = enumerate ( items ) \n 
data = [ ] \n 
~~~ is_list = False \n 
iterator = items . iteritems ( ) \n 
data = { } \n 
for k , v in iterator : \n 
~~~ if is_list : \n 
~~~ data . append ( v ) \n 
~~~ data [ k ] = v \n 
~~ if k in self . object_map and not is_list : \n 
~~~ data [ k ] = self . object_map [ k ] \n 
~~ elif isinstance ( v , ( Document , EmbeddedDocument ) ) : \n 
~~~ for field_name , field in v . _fields . iteritems ( ) : \n 
~~~ v = data [ k ] . _data . get ( field_name , None ) \n 
~~~ data [ k ] . _data [ field_name ] = self . object_map . get ( \n 
( v . collection , v . id ) , v ) \n 
( v [ ] . collection , v [ ] . id ) , v ) \n 
~~~ item_name = txt_type ( "{0}.{1}.{2}" ) . format ( name , k , field_name ) \n 
data [ k ] . _data [ field_name ] = self . _attach_objects ( v , depth , instance = instance , name = item_name ) \n 
~~ ~~ ~~ elif isinstance ( v , ( dict , list , tuple ) ) and depth <= self . max_depth : \n 
~~~ item_name = % ( name , k ) if name else name \n 
data [ k ] = self . _attach_objects ( v , depth - 1 , instance = instance , name = item_name ) \n 
~~ elif hasattr ( v , ) : \n 
~~~ data [ k ] = self . object_map . get ( ( v . collection , v . id ) , v ) \n 
~~ ~~ if instance and name : \n 
~~~ return tuple ( data ) if as_tuple else list_type ( data , instance , name ) \n 
~~ return BaseDict ( data , instance , name ) \n 
return data \n 
sys . path [ 0 : 0 ] = [ "" ] \n 
from mongoengine import * \n 
from mongoengine . connection import get_db \n 
__all__ = ( "GeoFieldTest" , ) \n 
class GeoFieldTest ( unittest . TestCase ) : \n 
~~~ connect ( db = ) \n 
self . db = get_db ( ) \n 
~~ def _test_for_expected_error ( self , Cls , loc , expected ) : \n 
~~~ Cls ( loc = loc ) . validate ( ) \n 
self . fail ( . format ( loc ) ) \n 
~~ except ValidationError as e : \n 
~~~ self . assertEqual ( expected , e . to_dict ( ) [ ] ) \n 
~~ ~~ def test_geopoint_validation ( self ) : \n 
~~~ class Location ( Document ) : \n 
~~~ loc = GeoPointField ( ) \n 
~~ invalid_coords = [ { "x" : 1 , "y" : 2 } , 5 , "a" ] \n 
expected = \n 
for coord in invalid_coords : \n 
~~~ self . _test_for_expected_error ( Location , coord , expected ) \n 
~~ invalid_coords = [ [ ] , [ 1 ] , [ 1 , 2 , 3 ] ] \n 
self . _test_for_expected_error ( Location , coord , expected ) \n 
~~ invalid_coords = [ [ { } , { } ] , ( "a" , "b" ) ] \n 
~~ ~~ def test_point_validation ( self ) : \n 
~~~ loc = PointField ( ) \n 
~~ invalid_coords = { "x" : 1 , "y" : 2 } \n 
self . _test_for_expected_error ( Location , invalid_coords , expected ) \n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ ] } \n 
invalid_coords = { "type" : "Point" , "coordinates" : [ 1 , 2 , 3 ] } \n 
invalid_coords = [ 5 , "a" ] \n 
~~ Location ( loc = [ 1 , 2 ] ) . validate ( ) \n 
Location ( loc = { \n 
"type" : "Point" , \n 
"coordinates" : [ \n 
81.4471435546875 , \n 
23.61432859499169 \n 
] } ) . validate ( ) \n 
~~ def test_linestring_validation ( self ) : \n 
~~~ loc = LineStringField ( ) \n 
invalid_coords = { "type" : "MadeUp" , "coordinates" : [ [ ] ] } \n 
invalid_coords = { "type" : "LineString" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
invalid_coords = [ [ 1 ] ] \n 
invalid_coords = [ [ 1 , 2 , 3 ] ] \n 
invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
~~ Location ( loc = [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ) . validate ( ) \n 
~~ def test_polygon_validation ( self ) : \n 
~~~ loc = PolygonField ( ) \n 
invalid_coords = { "type" : "Polygon" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
invalid_coords = [ [ [ 5 , "a" ] ] ] \n 
invalid_coords = [ [ [ ] ] ] \n 
invalid_coords = [ [ [ 1 , 2 , 3 ] ] ] \n 
invalid_coords = [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] \n 
Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
~~ def test_multipoint_validation ( self ) : \n 
~~~ loc = MultiPointField ( ) \n 
invalid_coords = { "type" : "MultiPoint" , "coordinates" : [ [ 1 , 2 , 3 ] ] } \n 
invalid_coords = [ [ ] ] \n 
invalid_coords = [ [ [ 1 ] ] , [ [ 1 , 2 , 3 ] ] ] \n 
~~ invalid_coords = [ [ [ { } , { } ] ] , [ ( "a" , "b" ) ] ] \n 
~~ Location ( loc = [ [ 1 , 2 ] ] ) . validate ( ) \n 
"type" : "MultiPoint" , \n 
[ 1 , 2 ] , \n 
[ 81.4471435546875 , 23.61432859499169 ] \n 
~~ def test_multilinestring_validation ( self ) : \n 
~~~ loc = MultiLineStringField ( ) \n 
invalid_coords = { "type" : "MultiLineString" , "coordinates" : [ [ [ 1 , 2 , 3 ] ] ] } \n 
invalid_coords = [ [ [ 1 ] ] ] \n 
invalid_coords = [ [ [ [ { } , { } ] ] ] , [ [ ( "a" , "b" ) ] ] ] \n 
~~ Location ( loc = [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ) . validate ( ) \n 
~~ def test_multipolygon_validation ( self ) : \n 
~~~ loc = MultiPolygonField ( ) \n 
invalid_coords = { "type" : "MultiPolygon" , "coordinates" : [ [ [ [ 1 , 2 , 3 ] ] ] ] } \n 
invalid_coords = [ [ [ [ 5 , "a" ] ] ] ] \n 
invalid_coords = [ [ [ [ ] ] ] ] \n 
invalid_coords = [ [ [ [ 1 , 2 , 3 ] ] ] ] \n 
invalid_coords = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] ] ] ] \n 
Location ( loc = [ [ [ [ 1 , 2 ] , [ 3 , 4 ] , [ 5 , 6 ] , [ 1 , 2 ] ] ] ] ) . validate ( ) \n 
~~ def test_indexes_geopoint ( self ) : \n 
class Event ( Document ) : \n 
~~~ title = StringField ( ) \n 
location = GeoPointField ( ) \n 
~~ geo_indicies = Event . _geo_indices ( ) \n 
self . assertEqual ( geo_indicies , [ { : [ ( , ) ] } ] ) \n 
~~ def test_geopoint_embedded_indexes ( self ) : \n 
class Venue ( EmbeddedDocument ) : \n 
~~~ location = GeoPointField ( ) \n 
name = StringField ( ) \n 
~~ class Event ( Document ) : \n 
venue = EmbeddedDocumentField ( Venue ) \n 
~~ def test_indexes_2dsphere ( self ) : \n 
point = PointField ( ) \n 
line = LineStringField ( ) \n 
polygon = PolygonField ( ) \n 
self . assertTrue ( { : [ ( , ) ] } in geo_indicies ) \n 
~~ def test_indexes_2dsphere_embedded ( self ) : \n 
~~~ name = StringField ( ) \n 
~~ def test_geo_indexes_recursion ( self ) : \n 
~~ class Parent ( Document ) : \n 
location = ReferenceField ( Location ) \n 
~~ Location . drop_collection ( ) \n 
Parent . drop_collection ( ) \n 
Parent ( name = ) . save ( ) \n 
info = Parent . _get_collection ( ) . index_information ( ) \n 
self . assertFalse ( in info ) \n 
info = Location . _get_collection ( ) . index_information ( ) \n 
self . assertTrue ( in info ) \n 
self . assertEqual ( len ( Parent . _geo_indices ( ) ) , 0 ) \n 
self . assertEqual ( len ( Location . _geo_indices ( ) ) , 1 ) \n 
~~ def test_geo_indexes_auto_index ( self ) : \n 
~~~ class Log ( Document ) : \n 
~~~ location = PointField ( auto_index = False ) \n 
datetime = DateTimeField ( ) \n 
meta = { \n 
: [ [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] ] \n 
~~ self . assertEqual ( [ ] , Log . _geo_indices ( ) ) \n 
Log . drop_collection ( ) \n 
Log . ensure_indexes ( ) \n 
info = Log . _get_collection ( ) . index_information ( ) \n 
self . assertEqual ( info [ "location_2dsphere_datetime_1" ] [ "key" ] , \n 
[ ( , ) , ( , 1 ) ] ) \n 
class Log ( Document ) : \n 
: [ \n 
{ : [ ( "location" , "2dsphere" ) , ( "datetime" , 1 ) ] } \n 
~~ from south . db import db \n 
from django . db import models \n 
from django_lean . experiments . models import * \n 
class Migration : \n 
~~~ def forwards ( self , orm ) : \n 
~~~ db . create_table ( , ( \n 
( , orm [ ] ) , \n 
) ) \n 
db . send_create_signal ( , [ ] ) \n 
db . create_table ( , ( \n 
db . create_unique ( , [ , ] ) \n 
~~ def backwards ( self , orm ) : \n 
~~~ db . delete_table ( ) \n 
db . delete_table ( ) \n 
db . delete_unique ( , [ , ] ) \n 
~~ models = { \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" , : } ) \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" } ) , \n 
: ( , [ ] , { : } ) \n 
: ( , [ ] , { : "orm[\'auth.Group\']" , : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" , : } ) , \n 
: ( , [ ] , { : , : } ) \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'experiments.Experiment\']" } ) , \n 
: ( , [ ] , { } ) \n 
: ( , [ ] , { : "orm[\'auth.User\']" } ) \n 
complete_apps = [ ] \n 
~~ class SimpleEngagementCalculator ( object ) : \n 
from django . contrib . sites . models import Site \n 
from django . core import mail \n 
from django . db import transaction \n 
from django . utils . functional import LazyObject \n 
def get_current_site ( ) : \n 
~~~ if Site . _meta . installed : \n 
~~~ return Site . objects . get_current ( ) \n 
~~ def in_transaction ( test_ignore = True ) : \n 
~~~ result = transaction . is_managed ( ) \n 
if test_ignore : \n 
~~~ result = result and not hasattr ( mail , ) \n 
~~ @ contextmanager \n 
def patch ( namespace , name , function ) : \n 
if isinstance ( namespace , LazyObject ) : \n 
~~~ if namespace . _wrapped is None : \n 
~~~ namespace . _setup ( ) \n 
~~ namespace = namespace . _wrapped \n 
~~~ original = getattr ( namespace , name ) \n 
~~ except AttributeError : \n 
~~~ original = NotImplemented \n 
~~~ setattr ( namespace , name , function ) \n 
yield \n 
~~ finally : \n 
~~~ if original is NotImplemented : \n 
~~~ delattr ( namespace , name ) \n 
~~~ setattr ( namespace , name , original ) \n 
from construct import * \n 
from ipv4 import IpAddress \n 
echo_payload = Struct ( "echo_payload" , \n 
UBInt16 ( "identifier" ) , \n 
UBInt16 ( "sequence" ) , \n 
dest_unreachable_payload = Struct ( "dest_unreachable_payload" , \n 
Padding ( 2 ) , \n 
UBInt16 ( "next_hop_mtu" ) , \n 
IpAddress ( "host" ) , \n 
Bytes ( "echo" , 8 ) , \n 
dest_unreachable_code = Enum ( Byte ( "code" ) , \n 
Network_unreachable_error = 0 , \n 
Host_unreachable_error = 1 , \n 
Protocol_unreachable_error = 2 , \n 
Port_unreachable_error = 3 , \n 
The_datagram_is_too_big = 4 , \n 
Source_route_failed_error = 5 , \n 
Destination_network_unknown_error = 6 , \n 
Destination_host_unknown_error = 7 , \n 
Source_host_isolated_error = 8 , \n 
Desination_administratively_prohibited = 9 , \n 
Host_administratively_prohibited2 = 10 , \n 
Network_TOS_unreachable = 11 , \n 
Host_TOS_unreachable = 12 , \n 
icmp_header = Struct ( "icmp_header" , \n 
Enum ( Byte ( "type" ) , \n 
Echo_reply = 0 , \n 
Destination_unreachable = 3 , \n 
Source_quench = 4 , \n 
Redirect = 5 , \n 
Alternate_host_address = 6 , \n 
Echo_request = 8 , \n 
Router_advertisement = 9 , \n 
Router_solicitation = 10 , \n 
Time_exceeded = 11 , \n 
Parameter_problem = 12 , \n 
Timestamp_request = 13 , \n 
Timestamp_reply = 14 , \n 
Information_request = 15 , \n 
Information_reply = 16 , \n 
Address_mask_request = 17 , \n 
Address_mask_reply = 18 , \n 
_default_ = Pass , \n 
Switch ( "code" , lambda ctx : ctx . type , \n 
"Destination_unreachable" : dest_unreachable_code , \n 
default = Byte ( "code" ) , \n 
UBInt16 ( "crc" ) , \n 
Switch ( "payload" , lambda ctx : ctx . type , \n 
"Echo_reply" : echo_payload , \n 
"Echo_request" : echo_payload , \n 
"Destination_unreachable" : dest_unreachable_payload , \n 
default = Pass \n 
~~~ cap1 = ( "0800305c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
"63646566676869" ) . decode ( "hex" ) \n 
cap2 = ( "0000385c02001b006162636465666768696a6b6c6d6e6f70717273747576776162" \n 
cap3 = ( "0301000000001122aabbccdd0102030405060708" ) . decode ( "hex" ) \n 
print icmp_header . parse ( cap1 ) \n 
print icmp_header . parse ( cap2 ) \n 
print icmp_header . parse ( cap3 ) \n 
~~ from construct . core import Container \n 
from construct . adapters import Adapter \n 
class AstNode ( Container ) : \n 
~~~ def __init__ ( self , nodetype , ** kw ) : \n 
~~~ Container . __init__ ( self ) \n 
self . nodetype = nodetype \n 
for k , v in sorted ( kw . iteritems ( ) ) : \n 
~~~ setattr ( self , k , v ) \n 
~~ ~~ def accept ( self , visitor ) : \n 
~~~ return getattr ( visitor , "visit_%s" % ( self . nodetype , ) ) ( self ) \n 
~~ ~~ class AstTransformator ( Adapter ) : \n 
~~~ def _decode ( self , obj , context ) : \n 
~~~ return self . to_ast ( obj , context ) \n 
~~ def _encode ( self , obj , context ) : \n 
~~~ return self . to_cst ( obj , context ) \n 
~~ ~~ def pytest_funcarg__setupopts ( request ) : \n 
~~~ return OptsSetup ( request ) \n 
~~ def pytest_addoption ( parser ) : \n 
~~~ parser . addoption ( "--uri-file" , dest = "urifile" , \n 
type = str , default = None , \n 
parser . addoption ( "--use-ns" , dest = "use_ns" , \n 
action = "store_true" , \n 
parser . addoption ( "--create-graph" , dest = "create_graph" , \n 
parser . addoption ( "--num-executors" , dest = "num_exec" , \n 
type = int , default = 0 , \n 
parser . addoption ( "--time" , dest = "time" , \n 
type = str , default = "2:00:00:00" , \n 
parser . addoption ( "--proc" , dest = "proc" , \n 
type = int , default = 8 , \n 
parser . addoption ( "--mem" , dest = "mem" , \n 
type = float , default = 16 , \n 
parser . addoption ( "--ppn" , dest = "ppn" , \n 
parser . addoption ( "--queue" , dest = "queue" , \n 
parser . addoption ( "--restart" , dest = "restart" , \n 
parser . addoption ( "--backup-dir" , dest = "backup_directory" , \n 
type = str , default = ".pipeline-backup" , \n 
~~ class OptsSetup ( ) : \n 
~~~ def __init__ ( self , request ) : \n 
~~~ self . config = request . config \n 
~~ def returnAllOptions ( self ) : \n 
~~~ return self . config . option \n 
~~ def getNumExecutors ( self ) : \n 
~~~ return self . config . option . num_exec \n 
~~ def getTime ( self ) : \n 
~~~ return self . config . option . time \n 
~~ def getProc ( self ) : \n 
~~~ return self . config . option . proc \n 
~~ def getMem ( self ) : \n 
~~~ return self . config . option . mem \n 
~~ def getQueue ( self ) : \n 
~~~ return self . config . option . queue \n 
~~ def getPpn ( self ) : \n 
~~~ return self . config . option . ppn \n 
~~ def getRestart ( self ) : \n 
~~~ return self . config . option . restart \n 
~~ def getBackupDir ( self ) : \n 
~~~ return self . config . option . backup_directory \n 
~~ def returnSampleArgs ( self ) : \n 
~~~ sampleArgArray = [ "TestProgName.py" , "img_A.mnc" , "img_B.mnc" ] \n 
return sampleArgArray \n 
import random \n 
from twisted . python import log \n 
from twisted . web . error import Error as WebError \n 
from opennsa import constants as cnt , config \n 
from opennsa . backends . common import genericbackend \n 
from opennsa . protocols . shared import httpclient \n 
#</service> \n 
LOG_SYSTEM = \n 
class NCSVPNTarget ( object ) : \n 
~~~ def __init__ ( self , router , interface , vlan = None ) : \n 
~~~ self . router = router \n 
self . interface = interface \n 
self . vlan = vlan \n 
~~~ if self . vlan : \n 
~~~ return % ( self . router , self . interface , self . vlan ) \n 
~~~ return % ( self . router , self . interface ) \n 
~~ ~~ ~~ def createVPNPayload ( service_name , source_target , dest_target ) : \n 
~~~ intps = { \n 
: service_name , \n 
: source_target . router , \n 
: source_target . interface , \n 
: dest_target . router , \n 
: dest_target . interface \n 
if source_target . vlan and dest_target . vlan : \n 
~~~ if source_target . vlan == dest_target . vlan : \n 
~~~ intps [ ] = source_target . vlan \n 
payload = ETHERNET_VLAN_VPN_PAYLOAD_BASE % intps \n 
intps [ ] = dest_target . vlan \n 
payload = ETHERNET_VLAN_REWRITE_VPN_PAYLOAD_BASE % intps \n 
~~~ payload = ETHERNET_VPN_PAYLOAD_BASE % intps \n 
~~ return payload \n 
~~ def _extractErrorMessage ( failure ) : \n 
~~~ if isinstance ( failure . value , WebError ) : \n 
~~~ return failure . value . response \n 
~~~ return failure . getErrorMessage ( ) \n 
~~ ~~ class NCSVPNConnectionManager : \n 
~~~ def __init__ ( self , ncs_services_url , user , password , port_map , log_system ) : \n 
~~~ self . ncs_services_url = ncs_services_url \n 
self . user = user \n 
self . password = password \n 
self . port_map = port_map \n 
self . log_system = log_system \n 
~~ def getResource ( self , port , label_type , label_value ) : \n 
~~~ assert label_type in ( None , cnt . ETHERNET_VLAN ) , \n 
~~ def getTarget ( self , port , label_type , label_value ) : \n 
if label_type == cnt . ETHERNET_VLAN : \n 
~~~ vlan = int ( label_value ) \n 
assert 1 <= vlan <= 4095 , % label_value \n 
~~ ri = self . port_map [ port ] \n 
router , interface = ri . split ( ) \n 
return NCSVPNTarget ( router , interface , vlan ) \n 
~~ def createConnectionId ( self , source_target , dest_target ) : \n 
~~~ return + str ( random . randint ( 100000 , 999999 ) ) \n 
~~ def canSwapLabel ( self , label_type ) : \n 
~~~ return label_type == cnt . ETHERNET_VLAN \n 
~~ def _createAuthzHeader ( self ) : \n 
~~~ return + base64 . b64encode ( self . user + + self . password ) \n 
~~ def _createHeaders ( self ) : \n 
~~~ headers = { } \n 
headers [ ] = \n 
headers [ ] = self . _createAuthzHeader ( ) \n 
return headers \n 
~~ def setupLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + NO_OUT_OF_SYNC_CHECK \n 
payload = createVPNPayload ( connection_id , source_target , dest_target ) \n 
headers = self . _createHeaders ( ) \n 
def linkUp ( _ ) : \n 
~~~ log . msg ( % ( source_target , dest_target ) , system = self . log_system ) \n 
~~ def error ( failure ) : \n 
log . msg ( % _extractErrorMessage ( failure ) , system = self . log_system ) \n 
return failure \n 
~~ d = httpclient . httpRequest ( service_url , payload , headers , method = , timeout = NCS_TIMEOUT ) \n 
d . addCallbacks ( linkUp , error ) \n 
return d \n 
~~ def teardownLink ( self , connection_id , source_target , dest_target , bandwidth ) : \n 
~~~ service_url = self . ncs_services_url + + connection_id + + NO_OUT_OF_SYNC_CHECK \n 
def linkDown ( _ ) : \n 
~~ d = httpclient . httpRequest ( service_url , None , headers , method = , timeout = NCS_TIMEOUT ) \n 
d . addCallbacks ( linkDown , error ) \n 
~~ ~~ def NCSVPNBackend ( network_name , nrm_ports , parent_requester , cfg ) : \n 
~~~ name = % network_name \n 
user = cfg [ config . NCS_USER ] \n 
password = cfg [ config . NCS_PASSWORD ] \n 
cm = NCSVPNConnectionManager ( ncs_services_url , user , password , port_map , name ) \n 
return genericbackend . GenericBackend ( network_name , nrm_map , cm , parent_requester , name ) \n 
from twisted . python import log , failure \n 
from opennsa import nsa , error \n 
from opennsa . shared import xmlhelper \n 
from opennsa . protocols . shared import minisoap , soapresource \n 
from opennsa . protocols . nsi2 import helper , queryhelper \n 
from opennsa . protocols . nsi2 . bindings import actions , nsiconnection , p2pservices \n 
class ProviderService : \n 
~~~ def __init__ ( self , soap_resource , provider ) : \n 
~~~ self . provider = provider \n 
soap_resource . registerDecoder ( actions . RESERVE , self . reserve ) \n 
soap_resource . registerDecoder ( actions . RESERVE_COMMIT , self . reserveCommit ) \n 
soap_resource . registerDecoder ( actions . RESERVE_ABORT , self . reserveAbort ) \n 
soap_resource . registerDecoder ( actions . PROVISION , self . provision ) \n 
soap_resource . registerDecoder ( actions . RELEASE , self . release ) \n 
soap_resource . registerDecoder ( actions . TERMINATE , self . terminate ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY , self . querySummary ) \n 
soap_resource . registerDecoder ( actions . QUERY_SUMMARY_SYNC , self . querySummarySync ) \n 
soap_resource . registerDecoder ( actions . QUERY_RECURSIVE , self . queryRecursive ) \n 
~~ def _createSOAPFault ( self , err , provider_nsa , connection_id = None , service_type = None ) : \n 
~~~ log . msg ( % err . getErrorMessage ( ) , system = LOG_SYSTEM ) \n 
se = helper . createServiceException ( err , provider_nsa , connection_id ) \n 
ex_element = se . xml ( nsiconnection . serviceException ) \n 
soap_fault = soapresource . SOAPFault ( err . getErrorMessage ( ) , ex_element ) \n 
return soap_fault \n 
~~ def reserve ( self , soap_data , request_info ) : \n 
~~~ t_start = time . time ( ) \n 
header , reservation = helper . parseRequest ( soap_data ) \n 
criteria = reservation . criteria \n 
p2ps = criteria . serviceDefinition \n 
if type ( p2ps ) is not p2pservices . P2PServiceBaseType : \n 
~~~ err = failure . Failure ( error . PayloadError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa , service_type = service_type ) \n 
~~ if p2ps . directionality in ( None , ) : \n 
~~~ err = failure . Failure ( error . MissingParameterError ( ) ) \n 
return self . _createSOAPFault ( err , header . provider_nsa ) \n 
~~ start_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . startTime ) if criteria . schedule . startTime is not None else None \n 
end_time = xmlhelper . parseXMLTimestamp ( criteria . schedule . endTime ) if criteria . schedule . endTime is not None else None \n 
schedule = nsa . Schedule ( start_time , end_time ) \n 
src_stp = helper . createSTP ( p2ps . sourceSTP ) \n 
dst_stp = helper . createSTP ( p2ps . destSTP ) \n 
if p2ps . ero : \n 
~~ params = [ ( p . type_ , p . value ) for p in p2ps . parameter ] if p2ps . parameter else None \n 
symmetric = p2ps . symmetricPath or False \n 
sd = nsa . Point2PointService ( src_stp , dst_stp , p2ps . capacity , p2ps . directionality , symmetric , None , params ) \n 
crt = nsa . Criteria ( criteria . version , schedule , sd ) \n 
t_delta = time . time ( ) - t_start \n 
log . msg ( % round ( t_delta , 3 ) , profile = True , system = LOG_SYSTEM ) \n 
d = self . provider . reserve ( header , reservation . connectionId , reservation . globalReservationId , reservation . description , crt , request_info ) \n 
def createReserveAcknowledgement ( connection_id ) : \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , None , header . correlation_id ) \n 
reserve_response = nsiconnection . ReserveResponseType ( connection_id ) \n 
reserve_response_element = reserve_response . xml ( nsiconnection . reserveResponse ) \n 
payload = minisoap . createSoapPayload ( reserve_response_element , soap_header_element ) \n 
return payload \n 
~~ d . addCallbacks ( createReserveAcknowledgement , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n 
~~ def reserveCommit ( self , soap_data , request_info ) : \n 
~~~ header , confirm = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveCommit ( header , confirm . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , confirm . connectionId ) ) \n 
~~ def reserveAbort ( self , soap_data , request_info ) : \n 
~~~ header , request = helper . parseRequest ( soap_data ) \n 
d = self . provider . reserveAbort ( header , request . connectionId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , request . connectionId ) ) \n 
~~ def provision ( self , soap_data , request_info ) : \n 
d = self . provider . provision ( header , request . connectionId , request_info ) \n 
~~ def release ( self , soap_data , request_info ) : \n 
d = self . provider . release ( header , request . connectionId , request_info ) \n 
~~ def terminate ( self , soap_data , request_info ) : \n 
d = self . provider . terminate ( header , request . connectionId , request_info ) \n 
~~ def querySummary ( self , soap_data , request_info ) : \n 
~~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummary ( header , query . connectionId , query . globalReservationId , request_info ) \n 
d . addCallbacks ( lambda _ : helper . createGenericProviderAcknowledgement ( header ) , self . _createSOAPFault , errbackArgs = ( header . provider_nsa , ) ) \n 
~~ def querySummarySync ( self , soap_data , request_info ) : \n 
~~~ def gotReservations ( reservations , header ) : \n 
~~~ soap_header_element = helper . createProviderHeader ( header . requester_nsa , header . provider_nsa , correlation_id = header . correlation_id ) \n 
qs_reservations = queryhelper . buildQuerySummaryResultType ( reservations ) \n 
qsct = nsiconnection . QuerySummaryConfirmedType ( qs_reservations ) \n 
payload = minisoap . createSoapPayload ( qsct . xml ( nsiconnection . querySummarySyncConfirmed ) , soap_header_element ) \n 
~~ header , query = helper . parseRequest ( soap_data ) \n 
d = self . provider . querySummarySync ( header , query . connectionId , query . globalReservationId , request_info ) \n 
d . addCallbacks ( gotReservations , self . _createSOAPFault , callbackArgs = ( header , ) , errbackArgs = ( header . provider_nsa , ) ) \n 
~~ def queryRecursive ( self , soap_data , request_info ) : \n 
d = self . provider . queryRecursive ( header , query . connectionId , query . globalReservationId , request_info ) \n 
~~ ~~ import os , datetime , json \n 
from twisted . trial import unittest \n 
from twisted . internet import defer , task \n 
from opennsa import config , nsa , database \n 
from opennsa . topology import nml \n 
from opennsa . backends import ncsvpn \n 
from . import common \n 
class NCSVPNBackendTest ( unittest . TestCase ) : \n 
~~~ self . clock = task . Clock ( ) \n 
tcf = os . path . expanduser ( ) \n 
tc = json . load ( open ( tcf ) ) \n 
ncs_config = { \n 
config . NCS_SERVICES_URL : tc [ ] , \n 
config . NCS_USER : tc [ ] , \n 
config . NCS_PASSWORD : tc [ ] \n 
self . requester = common . DUDRequester ( ) \n 
self . backend = ncsvpn . NCSVPNBackend ( , self . sr , self . requester , ncs_config ) \n 
self . backend . scheduler . clock = self . clock \n 
self . backend . startService ( ) \n 
database . setupDatabase ( tc [ ] , tc [ ] , tc [ ] ) \n 
self . requester_nsa = nsa . NetworkServiceAgent ( , ) \n 
self . provider_nsa = nsa . NetworkServiceAgent ( , ) \n 
source_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , ) ] ) \n 
dest_stp = nsa . STP ( , , labels = [ nsa . Label ( nml . ETHERNET_VLAN , ) ] ) \n 
start_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 2 ) \n 
end_time = datetime . datetime . utcnow ( ) + datetime . timedelta ( seconds = 30 ) \n 
bandwidth = 200 \n 
self . service_params = nsa . ServiceParameters ( start_time , end_time , source_stp , dest_stp , bandwidth ) \n 
~~ @ defer . inlineCallbacks \n 
def tearDown ( self ) : \n 
~~~ from opennsa . backends . common import simplebackend \n 
yield simplebackend . Simplebackendconnection . deleteAll ( ) \n 
yield self . backend . stopService ( ) \n 
def testActivation ( self ) : \n 
~~~ _ , _ , cid , sp = yield self . reserve ( self . requester_nsa , self . provider_nsa , None , None , None , None , self . service_params ) \n 
yield self . backend . reserveCommit ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
yield self . backend . provision ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
self . clock . advance ( 3 ) \n 
connection_id , active , version_consistent , version , timestamp = yield d_up \n 
self . failUnlessEqual ( cid , connection_id ) \n 
self . failUnlessEqual ( active , True ) \n 
self . failUnlessEqual ( version_consistent , True ) \n 
yield self . backend . terminate ( self . requester_nsa , self . provider_nsa , None , cid ) \n 
connection_id , active , version_consistent , version , timestamp = yield d_down \n 
self . failUnlessEqual ( active , False ) \n 
~~ testActivation . skip = \n 
#-- \n 
revision = \n 
down_revision = \n 
from alembic import op \n 
import sqlalchemy as sa \n 
def upgrade ( ) : \n 
~~~ op . add_column ( , sa . Column ( , sa . Integer ) ) \n 
~~ def downgrade ( ) : \n 
~~~ op . drop_column ( , ) \n 
~~ from . import view \n 
from . comp import Card , NewCard \n 
from nagare . i18n import _ \n 
from nagare import presentation , ajax , security , component \n 
from . comp import Gallery , Asset , AssetCropper \n 
def render_image ( self , h , comp , size , randomize = False , ** kw ) : \n 
~~~ metadata = self . assets_manager . get_metadata ( self . filename ) \n 
src = self . assets_manager . get_image_url ( self . filename , size ) \n 
if randomize : \n 
~~~ src += + h . generate_id ( ) \n 
~~ return h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = src , ** kw ) \n 
~~ def render_file ( self , h , comp , size , ** kw ) : \n 
~~~ kw [ ] += \n 
metadata = self . assets_manager . get_metadata ( self . filename ) \n 
res = [ h . img ( title = metadata [ ] , alt = metadata [ ] , \n 
src = "img/file-icon.jpg" , ** kw ) ] \n 
if size == : \n 
~~~ res . append ( h . span ( metadata [ ] ) ) \n 
~~ return res \n 
~~ CONTENT_TYPES = { : render_image , \n 
: render_image , \n 
: render_image } \n 
@ presentation . render_for ( Gallery ) \n 
def render ( self , h , comp , * args ) : \n 
~~~ with h . div ( id = + self . comp_id ) : \n 
~~~ with h . div ( class_ = ) : \n 
~~~ h << comp . render ( h , model = ) \n 
~~ with h . div ( id = "card-gallery" ) : \n 
~~~ h << comp . render ( h , self . model ) \n 
~~ ~~ return h . root \n 
~~ @ presentation . render_for ( Gallery , ) \n 
def render_Gallery_view ( self , h , comp , model ) : \n 
~~~ model = if security . has_permissions ( , self ) else \n 
for asset in self . assets : \n 
~~~ h << asset . render ( h , model ) \n 
~~ return h . root \n 
def render_Gallery_crop ( self , h , comp , model ) : \n 
~~~ return self . cropper . on_answer ( self . action ) \n 
def render_cover ( self , h , comp , model ) : \n 
~~~ cover = self . get_cover ( ) \n 
if cover : \n 
~~~ h << h . p ( component . Component ( self . get_cover ( ) , model = ) , class_ = ) \n 
~~ @ presentation . render_for ( Gallery , "action" ) \n 
def render_download ( self , h , comp , * args ) : \n 
~~~ if security . has_permissions ( , self ) : \n 
~~~ submit_id = h . generate_id ( "attach_submit" ) \n 
input_id = h . generate_id ( "attach_input" ) \n 
h << h . label ( ( h . i ( class_ = ) , \n 
with h . form : \n 
~~~ h << h . script ( \n 
% \n 
: ajax . py2js ( self . assets_manager . max_size ) , \n 
: ajax . py2js ( input_id ) , \n 
: ajax . py2js ( submit_id ) , \n 
: ajax . py2js ( \n 
_ ( ) \n 
) . decode ( ) \n 
submit_action = ajax . Update ( \n 
render = lambda r : r . div ( comp . render ( r , model = None ) , r . script ( ) ) , \n 
component_to_update = + self . comp_id , \n 
h << h . input ( id = input_id , class_ = , type = "file" , name = "file" , multiple = "multiple" , maxlength = "100" , ) . action ( self . add_assets ) \n 
h << h . input ( class_ = , id = submit_id , type = "submit" ) . action ( submit_action ) \n 
~~ @ presentation . render_for ( Gallery , model = ) \n 
def render_gallery_badge ( self , h , * args ) : \n 
if self . assets : \n 
~~~ with h . span ( class_ = ) : \n 
~~~ h << h . span ( h . i ( class_ = ) , , len ( self . assets ) , class_ = ) \n 
~~ @ presentation . render_for ( Asset ) \n 
@ presentation . render_for ( Asset , model = ) \n 
def render_asset ( self , h , comp , model , * args ) : \n 
~~~ res = [ ] \n 
kw = { : True } if model == else { } \n 
kw [ ] = model \n 
if self . is_cover : \n 
~~~ res . append ( h . span ( class_ = ) ) \n 
~~ meth = CONTENT_TYPES . get ( metadata [ ] , render_file ) \n 
res . append ( meth ( self , h , comp , model , ** kw ) ) \n 
return res \n 
~~ @ presentation . render_for ( Asset , model = ) \n 
def render_Asset_thumb ( self , h , comp , model , * args ) : \n 
~~~ action = h . a . action ( lambda : comp . answer ( ( , self ) ) ) . get ( ) \n 
onclick = _ ( ) \n 
with h . a ( class_ = , title = _ ( ) , href = , onclick = onclick ) : \n 
~~~ h << h . i ( class_ = ) \n 
~~ if self . is_image ( ) : \n 
~~~ with h . a ( class_ = , title = _ ( ) ) . action ( lambda : comp . answer ( ( , self ) ) ) : \n 
~~~ if self . is_cover : \n 
~~~ h << { : } \n 
~~ h << h . i ( class_ = ) \n 
~~ ~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , ) \n 
~~ @ presentation . render_for ( Asset , model = "anonymous" ) \n 
def render_asset_anonymous ( self , h , comp , model , * args ) : \n 
~~~ with h . a ( href = self . assets_manager . get_image_url ( self . filename ) , target = ) : \n 
~~~ h << comp . render ( h , model = "thumb" ) \n 
~~ @ presentation . render_for ( AssetCropper ) \n 
def render_gallery_cropper ( self , h , comp , * args ) : \n 
~~~ h << h . p ( _ ( ) ) \n 
form_id = h . generate_id ( ) \n 
img_id = h . generate_id ( ) \n 
~~~ for crop_name in , , , : \n 
~~~ h << h . input ( type = , id = form_id + + crop_name ) . action ( getattr ( self , crop_name ) ) \n 
~~ h << h . p ( render_image ( self . asset , h , comp , , id = img_id ) ) \n 
h << h . script ( \n 
"YAHOO.util.Event.onContentReady(%s," \n 
ajax . py2js ( img_id ) , \n 
ajax . py2js ( form_id ) , \n 
ajax . py2js ( self . crop_width ( ) ) , \n 
ajax . py2js ( self . crop_height ( ) ) \n 
with h . div ( class_ = ) : \n 
~~~ h << h . button ( _ ( ) , class_ = ) . action ( self . commit , comp ) \n 
if self . asset . is_cover : \n 
~~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . remove_cover , comp ) \n 
~~ h << \n 
h << h . button ( _ ( ) , class_ = ) . action ( self . cancel , comp ) \n 
~~ class EventHandlerMixIn ( object ) : \n 
def emit_event ( self , comp , kind , data = None ) : \n 
~~~ event = kind ( data , source = [ self ] ) \n 
return comp . answer ( event ) \n 
~~ def handle_event ( self , comp , event ) : \n 
~~~ local_res = None \n 
local_handler = getattr ( self , , None ) \n 
if local_handler : \n 
~~~ local_res = local_handler ( comp , event ) \n 
~~ event . append ( self ) \n 
upper_res = comp . answer ( event ) \n 
return local_res or upper_res \n 
~~ ~~ class Event ( object ) : \n 
def __init__ ( self , data , source = [ ] ) : \n 
self . _source = source \n 
self . data = data \n 
def source ( self ) : \n 
~~~ return self . _source . copy ( ) \n 
def emitter ( self ) : \n 
~~~ return self . _source [ 0 ] \n 
def last_relay ( self ) : \n 
~~~ return self . _source [ - 1 ] \n 
~~ def is_ ( self , kind ) : \n 
~~~ return type ( self ) is kind \n 
~~ def is_kind_of ( self , kind ) : \n 
~~~ return isinstance ( self , kind ) \n 
~~ def append ( self , relay ) : \n 
~~~ self . _source . append ( relay ) \n 
~~ def cast_as ( self , sub_kind ) : \n 
~~~ return sub_kind ( self . data , self . _source ) \n 
~~ ~~ class ColumnDeleted ( Event ) : \n 
~~ class CardClicked ( Event ) : \n 
~~ class PopinClosed ( Event ) : \n 
~~ class CardEditorClosed ( PopinClosed ) : \n 
~~ class CardArchived ( Event ) : \n 
~~ class SearchIndexUpdated ( Event ) : \n 
~~ class CardDisplayed ( Event ) : \n 
~~ class BoardAccessChanged ( Event ) : \n 
~~ class BoardDeleted ( BoardAccessChanged ) : \n 
~~ class BoardArchived ( BoardAccessChanged ) : \n 
~~ class BoardRestored ( BoardAccessChanged ) : \n 
~~ class BoardLeft ( BoardAccessChanged ) : \n 
~~ class ParentTitleNeeded ( Event ) : \n 
~~ class NewTemplateRequested ( Event ) : \n 
~~ from . comp import EditableTitle \n 
from . import view \n 
#!/usr/bin/python2.7 \n 
####################################################################################################################### \n 
import genie2 . client . wrapper \n 
import genie2 . model . ClusterCriteria \n 
import genie2 . model . Job \n 
import genie2 . model . FileAttachment \n 
genie = genie2 . client . wrapper . Genie2 ( "http://localhost:8080/genie" , \n 
genie2 . client . wrapper . RetryPolicy ( \n 
tries = 8 , none_on_404 = True , no_retry_http_codes = range ( 400 , 500 ) \n 
job = genie2 . model . Job . Job ( ) \n 
job . name = "GenieDockerExamplePigJob2" \n 
job . user = "root" \n 
job . version = "0.14.0" \n 
job . clusterCriterias = list ( ) \n 
cluster_criteria = genie2 . model . ClusterCriteria . ClusterCriteria ( ) \n 
criteria = set ( ) \n 
criteria . add ( "sched:adhoc" ) \n 
criteria . add ( "type:yarn" ) \n 
cluster_criteria . tags = criteria \n 
job . clusterCriterias . append ( cluster_criteria ) \n 
command_criteria = set ( ) \n 
command_criteria . add ( "type:pig" ) \n 
job . commandCriteria = command_criteria \n 
job . fileDependencies = "file:///apps/genie/pig/0.14.0/tutorial/script2-hadoop.pig,file:///apps/genie/pig/0.14.0/tutorial/tutorial.jar" \n 
job . commandArgs = "script2-hadoop.pig" \n 
job = genie . submitJob ( job ) \n 
while job . status != "SUCCEEDED" and job . status != "KILLED" and job . status != "FAILED" : \n 
time . sleep ( 10 ) \n 
job = genie . getJob ( job . id ) \n 
ON = 1 \n 
DISCONNECTED = 20 \n 
CONNECTED = 30 \n 
DEFAULT_EVENT_VERSION = 1 \n 
LOG_LEVEL = "DEBUG" \n 
LOG_FILE = "/var/log/security_monkey/security_monkey-deploy.log" \n 
SQLALCHEMY_DATABASE_URI = \n 
SQLALCHEMY_POOL_SIZE = 50 \n 
SQLALCHEMY_MAX_OVERFLOW = 15 \n 
ENVIRONMENT = \n 
USE_ROUTE53 = False \n 
FQDN = \n 
API_PORT = \n 
WEB_PORT = \n 
WEB_PATH = \n 
FRONTED_BY_NGINX = True \n 
NGINX_PORT = \n 
BASE_URL = . format ( FQDN ) \n 
SECRET_KEY = \n 
MAIL_DEFAULT_SENDER = \n 
SECURITY_REGISTERABLE = True \n 
SECURITY_CONFIRMABLE = False \n 
SECURITY_RECOVERABLE = False \n 
SECURITY_PASSWORD_HASH = \n 
SECURITY_PASSWORD_SALT = \n 
SECURITY_TRACKABLE = True \n 
SECURITY_POST_LOGIN_VIEW = BASE_URL \n 
SECURITY_POST_REGISTER_VIEW = BASE_URL \n 
SECURITY_POST_CONFIRM_VIEW = BASE_URL \n 
SECURITY_POST_RESET_VIEW = BASE_URL \n 
SECURITY_POST_CHANGE_VIEW = BASE_URL \n 
SECURITY_TEAM_EMAIL = [ ] \n 
SES_REGION = \n 
MAIL_SERVER = \n 
MAIL_PORT = 465 \n 
MAIL_USE_SSL = True \n 
MAIL_USERNAME = \n 
MAIL_PASSWORD = \n 
WTF_CSRF_ENABLED = True \n 
WTF_CSRF_METHODS = [ , , , ] \n 
SECURITYGROUP_INSTANCE_DETAIL = \n 
CORE_THREADS = 25 \n 
MAX_THREADS = 30 \n 
from security_monkey . auditor import Auditor \n 
from security_monkey . watchers . rds_security_group import RDSSecurityGroup \n 
from security_monkey . datastore import NetworkWhitelistEntry \n 
from security_monkey . auditors . security_group import _check_rfc_1918 \n 
import ipaddr \n 
class RDSSecurityGroupAuditor ( Auditor ) : \n 
~~~ index = RDSSecurityGroup . index \n 
i_am_singular = RDSSecurityGroup . i_am_singular \n 
i_am_plural = RDSSecurityGroup . i_am_plural \n 
network_whitelist = [ ] \n 
def __init__ ( self , accounts = None , debug = False ) : \n 
~~~ super ( RDSSecurityGroupAuditor , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
~~ def prep_for_audit ( self ) : \n 
~~~ self . network_whitelist = NetworkWhitelistEntry . query . all ( ) \n 
~~ def _check_inclusion_in_network_whitelist ( self , cidr ) : \n 
~~~ for entry in self . network_whitelist : \n 
~~~ if ipaddr . IPNetwork ( cidr ) in ipaddr . IPNetwork ( str ( entry . cidr ) ) : \n 
~~ def check_rds_ec2_rfc1918 ( self , sg_item ) : \n 
severity = 8 \n 
if sg_item . config . get ( "vpc_id" , None ) : \n 
~~ for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
~~~ cidr = ipr . get ( "cidr_ip" , None ) \n 
if cidr and _check_rfc_1918 ( cidr ) : \n 
~~~ self . add_issue ( severity , tag , sg_item , notes = cidr ) \n 
~~ ~~ ~~ def check_securitygroup_large_subnet ( self , sg_item ) : \n 
severity = 3 \n 
for ipr in sg_item . config . get ( "ip_ranges" , [ ] ) : \n 
if cidr and not self . _check_inclusion_in_network_whitelist ( cidr ) : \n 
~~~ if in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
~~~ mask = int ( cidr . split ( ) [ 1 ] ) \n 
if mask < 24 and mask > 0 : \n 
~~ ~~ ~~ ~~ ~~ def check_securitygroup_zero_subnet ( self , sg_item ) : \n 
severity = 10 \n 
if cidr and in cidr and not cidr == "0.0.0.0/0" and not cidr == "10.0.0.0/8" : \n 
if mask == 0 : \n 
~~ ~~ ~~ ~~ def check_securitygroup_any ( self , sg_item ) : \n 
severity = 5 \n 
~~~ cidr = ipr . get ( "cidr_ip" ) \n 
if "0.0.0.0/0" == cidr : \n 
~~ ~~ ~~ def check_securitygroup_10net ( self , sg_item ) : \n 
if "10.0.0.0/8" == cidr : \n 
from security_monkey . datastore import NetworkWhitelistEntry , Account \n 
from security_monkey . tests import SecurityMonkeyTestCase \n 
from security_monkey import db \n 
from security_monkey . watchers . elasticsearch_service import ElasticSearchServiceItem \n 
CONFIG_ONE = { \n 
"name" : "es_test" , \n 
CONFIG_TWO = { \n 
"name" : "es_test_2" , \n 
CONFIG_THREE = { \n 
"name" : "es_test_3" , \n 
CONFIG_FOUR = { \n 
"name" : "es_test_4" , \n 
CONFIG_FIVE = { \n 
"name" : "es_test_5" , \n 
CONFIG_SIX = { \n 
"name" : "es_test_6" , \n 
CONFIG_SEVEN = { \n 
"name" : "es_test_7" , \n 
CONFIG_EIGHT = { \n 
"name" : "es_test_8" , \n 
CONFIG_NINE = { \n 
"name" : "es_test_9" , \n 
WHITELIST_CIDRS = [ \n 
class ElasticSearchServiceTestCase ( SecurityMonkeyTestCase ) : \n 
~~~ self . es_items = [ \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test" , config = CONFIG_ONE ) , \n 
ElasticSearchServiceItem ( region = "us-west-2" , account = "TEST_ACCOUNT" , name = "es_test_2" , config = CONFIG_TWO ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_3" , config = CONFIG_THREE ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_4" , config = CONFIG_FOUR ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_5" , config = CONFIG_FIVE ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_6" , config = CONFIG_SIX ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_7" , config = CONFIG_SEVEN ) , \n 
ElasticSearchServiceItem ( region = "eu-west-1" , account = "TEST_ACCOUNT" , name = "es_test_8" , config = CONFIG_EIGHT ) , \n 
ElasticSearchServiceItem ( region = "us-east-1" , account = "TEST_ACCOUNT" , name = "es_test_9" , config = CONFIG_NINE ) , \n 
test_account = Account ( ) \n 
test_account . name = "TEST_ACCOUNT" \n 
test_account . s3_name = "TEST_ACCOUNT" \n 
test_account . number = "012345678910" \n 
test_account . role_name = "TEST_ACCOUNT" \n 
db . session . add ( test_account ) \n 
db . session . commit ( ) \n 
~~ def tearDown ( self ) : \n 
~~~ test_account = Account . query . filter ( Account . number == "012345678910" ) . first ( ) \n 
if test_account is not None : \n 
~~~ db . session . delete ( test_account ) \n 
~~ ~~ def test_es_auditor ( self ) : \n 
~~~ from security_monkey . auditors . elasticsearch_service import ElasticSearchServiceAuditor \n 
es_auditor = ElasticSearchServiceAuditor ( accounts = [ "012345678910" ] ) \n 
es_auditor . network_whitelist = [ ] \n 
for cidr in WHITELIST_CIDRS : \n 
~~~ whitelist_cidr = NetworkWhitelistEntry ( ) \n 
whitelist_cidr . cidr = cidr [ 1 ] \n 
whitelist_cidr . name = cidr [ 0 ] \n 
es_auditor . network_whitelist . append ( whitelist_cidr ) \n 
~~ for es_domain in self . es_items : \n 
~~~ es_auditor . check_es_access_policy ( es_domain ) \n 
~~ self . assertEquals ( len ( self . es_items [ 0 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 0 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 1 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 1 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 2 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 2 ] . audit_issues [ 1 ] . score , 7 ) \n 
self . assertEquals ( len ( self . es_items [ 3 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 3 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 4 ] . audit_issues ) , 0 ) \n 
self . assertEquals ( len ( self . es_items [ 5 ] . audit_issues ) , 0 ) \n 
self . assertEquals ( len ( self . es_items [ 6 ] . audit_issues ) , 3 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 0 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 1 ] . score , 5 ) \n 
self . assertEquals ( self . es_items [ 6 ] . audit_issues [ 2 ] . score , 7 ) \n 
self . assertEquals ( len ( self . es_items [ 7 ] . audit_issues ) , 1 ) \n 
self . assertEquals ( self . es_items [ 7 ] . audit_issues [ 0 ] . score , 20 ) \n 
self . assertEquals ( len ( self . es_items [ 8 ] . audit_issues ) , 2 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 0 ] . score , 6 ) \n 
self . assertEquals ( self . es_items [ 8 ] . audit_issues [ 1 ] . score , 10 ) \n 
from security_monkey . watcher import Watcher \n 
from security_monkey . watcher import ChangeItem \n 
from security_monkey . constants import TROUBLE_REGIONS \n 
from security_monkey . exceptions import BotoConnectionIssue \n 
from security_monkey import app \n 
from boto . redshift import regions \n 
class Redshift ( Watcher ) : \n 
~~~ index = \n 
i_am_singular = \n 
i_am_plural = \n 
~~~ super ( Redshift , self ) . __init__ ( accounts = accounts , debug = debug ) \n 
~~ def slurp ( self ) : \n 
self . prep_for_slurp ( ) \n 
from security_monkey . common . sts_connect import connect \n 
item_list = [ ] \n 
exception_map = { } \n 
for account in self . accounts : \n 
~~~ for region in regions ( ) : \n 
~~~ redshift = connect ( account , , region = region ) \n 
all_clusters = [ ] \n 
marker = None \n 
while True : \n 
~~~ response = self . wrap_aws_rate_limited_call ( \n 
redshift . describe_clusters , \n 
marker = marker \n 
all_clusters . extend ( response [ ] [ if response [ ] [ ] [ ] ~~~ marker = response [ ] [ ] [ ~~ else : \n 
~~~ if region . name not in TROUBLE_REGIONS : \n 
~~~ exc = BotoConnectionIssue ( str ( e ) , , account , region . name ) \n 
self . slurp_exception ( ( self . index , account , region . name ) , exc , exception_map ) ~~ continue \n 
for cluster in all_clusters : \n 
~~~ cluster_id = cluster [ ] \n 
if self . check_ignore_list ( cluster_id ) : \n 
~~ item = RedshiftCluster ( region = region . name , account = account , name = cluster_id , config item_list . append ( item ) \n 
~~ ~~ ~~ return item_list , exception_map \n 
~~ ~~ class RedshiftCluster ( ChangeItem ) : \n 
~~~ def __init__ ( self , region = None , account = None , name = None , config = { } ) : \n 
~~~ super ( RedshiftCluster , self ) . __init__ ( \n 
index = Redshift . index , \n 
region = region , \n 
account = account , \n 
new_config = config ) \n 
from ndscheduler import settings \n 
from ndscheduler . core . datastore . providers import base \n 
class DatastorePostgresql ( base . DatastoreBase ) : \n 
~~~ @ classmethod \n 
def get_db_url ( cls ) : \n 
return % ( \n 
settings . DATABASE_CONFIG_DICT [ ] , \n 
settings . DATABASE_CONFIG_DICT [ ] ) \n 
from ndscheduler import job \n 
logger = logging . getLogger ( __name__ ) \n 
class CurlJob ( job . JobBase ) : \n 
~~~ TIMEOUT = 10 \n 
def meta_info ( cls ) : \n 
~~~ return { \n 
: % ( cls . __module__ , cls . __name__ ) , \n 
{ : , : } , \n 
{ : , : \n 
] , \n 
~~ def run ( self , url , request_type , * args , ** kwargs ) : \n 
~~~ print ( % ( url ) ) \n 
session = requests . Session ( ) \n 
result = session . request ( request_type , \n 
url , \n 
timeout = self . TIMEOUT , \n 
headers = None , \n 
data = None ) \n 
print ( result . text ) \n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ job = CurlJob . create_test_instance ( ) \n 
job . run ( ) \n 
~~ import unittest \n 
from . mock import MagicMock , Mock \n 
from . util import TrelloElementMock , CommandMock , OperationMock \n 
from operations import * \n 
class BaseOperationTests ( unittest . TestCase ) : \n 
~~~ self . base_operation , self . trello_element = OperationMock . create ( BaseOperation ) \n 
self . class_mock , self . instance_mock = OperationMock . instance ( self . base_operation ) \n 
self . collection = TrelloElementMock . collection ( ) \n 
self . base_operation . collection = TrelloCollection ( self . collection ) \n 
~~ def test_items_sets_the_collection ( self ) : \n 
~~~ self . base_operation . set_collection = MagicMock ( ) \n 
self . base_operation . items ( ) \n 
self . base_operation . set_collection . assert_called_with ( ) \n 
~~ def test_items_returns_every_name_from_the_collection_with_the_added_options ( self ) : \n 
~~ def test_callback_uses_find_to_instantiate_the_operation_if_the_index_is_in_the_collection ( self ) ~~~ self . base_operation . callback ( 3 ) \n 
self . class_mock . assert_called_with ( self . collection [ 0 ] , self . base_operation ) \n 
~~ def test_callback_calls_execute_on_the_operation ( self ) : \n 
~~~ self . base_operation . callback ( 3 ) \n 
self . instance_mock . execute . assert_called_with ( self . base_operation . command ) \n 
~~ def test_callback_doesnt_call_find_if_the_index_is_bigger_than_the_collection_length ( self ) : \n 
~~~ big_index = 55 \n 
self . base_operation . callback ( big_index ) \n 
assert not self . class_mock . called \n 
~~ def test_callback_calls_execute_on_the_previous_operation_if_index_is_0 ( self ) : \n 
~~~ self . base_operation . callback ( 0 ) \n 
self . base_operation . previous_operation . execute . assert_called_with ( ) \n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . base_operation . command . input = MagicMock ( ) \n 
self . base_operation . callback ( 2 ) \n 
self . base_operation . command . input . assert_called_with ( "Name" , self . base_operation . deferred_add \n 
~~ def test_base_add_calls_add_with_the_text_and_cleans_the_cache_for_the_element ( self ) : \n 
~~~ text = "Text" \n 
self . base_operation . add = MagicMock ( ) \n 
self . base_operation . trello_element . reload = MagicMock ( ) \n 
self . base_operation . base_add ( text ) \n 
self . base_operation . add . assert_called_with ( text ) \n 
self . trello_element . reload . assert_called_with ( ) \n 
~~ def test_base_add_calls_add_and_execute_if_renavigate_is_true ( self ) : \n 
self . base_operation . command . renavigate = True \n 
self . base_operation . execute = MagicMock ( ) \n 
self . base_operation . execute . assert_called_with ( ) \n 
~~ ~~ class BoardOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( BoardOperation ) \n 
self . operation . collection = TrelloCollection ( TrelloElementMock . collection ( ) ) \n 
~~ def test_items_returns_every_name_from_the_collection_without_goback ( self ) : \n 
~~~ self . operation . set_collection = MagicMock ( ) \n 
~~ def test_trello_element_property ( self ) : \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "boards" ) \n 
~~ def test_callback_calls_execute_command_with_the_index ( self ) : \n 
~~~ self . operation . execute_command = MagicMock ( ) \n 
self . operation . callback ( 5 ) \n 
self . operation . execute_command . assert_called_with ( 3 ) \n 
~~ def test_callback_calls_the_input_method_on_the_command_with_deferred_add_as_callback_if_index_is_1 ~~~ self . operation . command . input = MagicMock ( ) \n 
self . operation . callback ( 1 ) \n 
self . operation . command . input . assert_called_with ( "Name" , self . operation . deferred_add ) \n 
~~ def test_next_operation_class ( self ) : \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , ListOperation ) \n 
~~ def test_add_creates_a_board_with_the_text ( self ) : \n 
self . trello_element . add_board = MagicMock ( ) \n 
self . operation . add ( text ) \n 
self . trello_element . add_board . assert_called_with ( text ) \n 
~~ ~~ class ListOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( ListOperation ) \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "lists" ) \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOperation ) \n 
~~ def test_add_creates_a_list_with_the_text ( self ) : \n 
self . trello_element . add_list = MagicMock ( ) \n 
self . trello_element . add_list . assert_called_with ( text ) \n 
~~ ~~ class CardOperationTests ( unittest . TestCase ) : \n 
~~~ self . operation , self . trello_element = OperationMock . create ( CardOperation ) \n 
~~ def test_items_returns_every_name_from_the_collection_with_custom_actions ( self ) : \n 
self . assertEqual ( self . operation . items ( ) , [ , , \n 
~~~ self . assertEqual ( self . operation . trello_element_property ( ) , "cards" ) \n 
~~~ self . assertEqual ( self . operation . next_operation_class ( ) , CardOptions ) \n 
~~ def test_add_creates_a_card_with_the_text_and_description ( self ) : \n 
self . trello_element . add_card = MagicMock ( ) \n 
self . operation . add ( name , desc ) \n 
self . trello_element . add_card . assert_called_with ( name , desc ) \n 
~~ def test_split_card_contents_returns_the_name_and_description_splitted_by_new_lines ( self ) : \n 
~~~ content = "Name!!\\n\\nDescription\\nYeah!" \n 
name , desc = self . operation . split_card_contents ( content ) \n 
self . assertEqual ( name , "Name!!" ) \n 
self . assertEqual ( desc , "Description\\nYeah!" ) \n 
~~ \n 
DEBUG = env . bool ( , default = True ) \n 
TEMPLATES [ 0 ] [ ] [ ] = DEBUG \n 
SECRET_KEY = env ( "DJANGO_SECRET_KEY" , default = ) \n 
EMAIL_HOST = \n 
EMAIL_PORT = 1025 \n 
EMAIL_BACKEND = env ( , \n 
default = ) \n 
CACHES = { \n 
: \n 
MIDDLEWARE_CLASSES += ( , ) \n 
INSTALLED_APPS += ( , ) \n 
INTERNAL_IPS = ( , , ) \n 
DEBUG_TOOLBAR_CONFIG = { \n 
: True , \n 
TEST_RUNNER = \n 
from django . contrib import messages \n 
from django . contrib . auth import logout , login , authenticate \n 
from django . http import HttpResponseBadRequest , Http404 \n 
from django . shortcuts import render , redirect , get_object_or_404 \n 
from reddit . forms import UserForm , ProfileForm \n 
from reddit . utils . helpers import post_only \n 
from users . models import RedditUser \n 
def user_profile ( request , username ) : \n 
~~~ user = get_object_or_404 ( User , username = username ) \n 
profile = RedditUser . objects . get ( user = user ) \n 
return render ( request , , { : profile } ) \n 
def edit_profile ( request ) : \n 
~~~ user = RedditUser . objects . get ( user = request . user ) \n 
if request . method == : \n 
~~~ profile_form = ProfileForm ( instance = user ) \n 
~~ elif request . method == : \n 
~~~ profile_form = ProfileForm ( request . POST , instance = user ) \n 
if profile_form . is_valid ( ) : \n 
~~~ profile = profile_form . save ( commit = False ) \n 
profile . update_profile_data ( ) \n 
profile . save ( ) \n 
~~ return render ( request , , { : profile_form } ) \n 
~~ def user_login ( request ) : \n 
if request . user . is_authenticated ( ) : \n 
return render ( request , ) \n 
~~ if request . method == "POST" : \n 
~~~ username = request . POST . get ( ) \n 
password = request . POST . get ( ) \n 
if not username or not password : \n 
~~~ return HttpResponseBadRequest ( ) \n 
~~ user = authenticate ( username = username , \n 
password = password ) \n 
~~~ if user . is_active : \n 
~~~ login ( request , user ) \n 
redirect_url = request . POST . get ( ) or \n 
return redirect ( redirect_url ) \n 
~~~ return render ( request , , \n 
~~ ~~ return render ( request , ) \n 
~~ @ post_only \n 
def user_logout ( request ) : \n 
~~~ redirect_page = request . POST . get ( , ) \n 
logout ( request ) \n 
messages . success ( request , ) \n 
return redirect ( redirect_page ) \n 
~~ return redirect ( ) \n 
~~ def register ( request ) : \n 
user_form = UserForm ( ) \n 
~~~ messages . warning ( request , \n 
return render ( request , , { : user_form } ) \n 
~~~ user_form = UserForm ( request . POST ) \n 
if user_form . is_valid ( ) : \n 
~~~ user = user_form . save ( ) \n 
user . set_password ( user . password ) \n 
user . save ( ) \n 
reddit_user = RedditUser ( ) \n 
reddit_user . user = user \n 
reddit_user . save ( ) \n 
user = authenticate ( username = request . POST [ ] , \n 
password = request . POST [ ] ) \n 
login ( request , user ) \n 
return redirect ( ) \n 
~~ ~~ return render ( request , , { : user_form } ) \n 
~~ import unittest2 \n 
from pymysql . tests import base \n 
from pymysql import util \n 
class TestNextset ( base . PyMySQLTestCase ) : \n 
~~~ super ( TestNextset , self ) . setUp ( ) \n 
self . con = self . connections [ 0 ] \n 
~~ def test_nextset ( self ) : \n 
~~~ cur = self . con . cursor ( ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur ) ) \n 
r = cur . nextset ( ) \n 
self . assertTrue ( r ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur ) ) \n 
self . assertIsNone ( cur . nextset ( ) ) \n 
~~ def test_skip_nextset ( self ) : \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur ) ) \n 
~~ def test_ok_and_next ( self ) : \n 
self . assertTrue ( cur . nextset ( ) ) \n 
self . assertFalse ( bool ( cur . nextset ( ) ) ) \n 
~~ @ unittest2 . expectedFailure \n 
def test_multi_cursor ( self ) : \n 
~~~ cur1 = self . con . cursor ( ) \n 
cur2 = self . con . cursor ( ) \n 
self . assertEqual ( [ ( 1 , ) ] , list ( cur1 ) ) \n 
self . assertEqual ( [ ( 42 , ) ] , list ( cur2 ) ) \n 
r = cur1 . nextset ( ) \n 
self . assertEqual ( [ ( 2 , ) ] , list ( cur1 ) ) \n 
self . assertIsNone ( cur1 . nextset ( ) ) \n 
~~ def test_multi_statement_warnings ( self ) : \n 
~~~ cursor = self . con . cursor ( ) \n 
~~~ cursor . execute ( \n 
~~~ self . fail ( ) \n 
~~ ~~ ~~ from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import message as _message \n 
from google . protobuf import reflection as _reflection \n 
from google . protobuf import descriptor_pb2 \n 
DESCRIPTOR = _descriptor . FileDescriptor ( \n 
package = , \n 
_PUSHNOTIFICATION = _descriptor . Descriptor ( \n 
full_name = , \n 
filename = None , \n 
file = DESCRIPTOR , \n 
containing_type = None , \n 
fields = [ \n 
_descriptor . FieldDescriptor ( \n 
name = , full_name = , index = 0 , \n 
number = 1 , type = 3 , cpp_type = 2 , label = 1 , \n 
has_default_value = False , default_value = 0 , \n 
message_type = None , enum_type = None , containing_type = None , \n 
is_extension = False , extension_scope = None , \n 
options = None ) , \n 
name = , full_name = , index = 1 , \n 
number = 2 , type = 9 , cpp_type = 9 , label = 1 , \n 
has_default_value = False , default_value = unicode ( "" , "utf-8" ) , \n 
name = , full_name = , index = 2 , \n 
number = 3 , type = 9 , cpp_type = 9 , label = 1 , \n 
name = , full_name = , index = 3 , \n 
number = 4 , type = 9 , cpp_type = 9 , label = 1 , \n 
extensions = [ \n 
nested_types = [ ] , \n 
enum_types = [ \n 
options = None , \n 
is_extendable = False , \n 
extension_ranges = [ ] , \n 
serialized_start = 33 , \n 
serialized_end = 117 , \n 
_BATCHNOTIFICATIONREQUEST = _descriptor . Descriptor ( \n 
number = 1 , type = 11 , cpp_type = 10 , label = 3 , \n 
has_default_value = False , default_value = [ ] , \n 
serialized_start = 119 , \n 
serialized_end = 187 , \n 
_BATCHNOTIFICATIONREQUEST . fields_by_name [ ] . message_type = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _PUSHNOTIFICATION \n 
DESCRIPTOR . message_types_by_name [ ] = _BATCHNOTIFICATIONREQUEST \n 
class PushNotification ( _message . Message ) : \n 
~~~ __metaclass__ = _reflection . GeneratedProtocolMessageType \n 
DESCRIPTOR = _PUSHNOTIFICATION \n 
~~ class BatchNotificationRequest ( _message . Message ) : \n 
DESCRIPTOR = _BATCHNOTIFICATIONREQUEST \n 
~~ DESCRIPTOR . has_options = True \n 
import pytest \n 
from pushkin import pushkin_cli \n 
import tornado . web \n 
from pushkin import context \n 
from pushkin . database import database \n 
from pushkin . request . request_processor import RequestProcessor \n 
from pushkin . requesthandlers . events import JsonEventHandler \n 
from pushkin . requesthandlers . notifications import JsonNotificationHandler \n 
from pushkin import test_config_ini_path \n 
from pushkin import config \n 
@ pytest . fixture \n 
def setup_database ( ) : \n 
~~~ database . create_database ( ) \n 
~~ @ pytest . fixture \n 
def mock_processor ( mocker ) : \n 
mocker . patch ( ) \n 
def app ( ) : \n 
~~~ pushkin_cli . CONFIGURATION_FILENAME = test_config_ini_path \n 
pushkin_cli . init ( ) \n 
return pushkin_cli . create_app ( ) \n 
def notification_batch_json ( ) : \n 
def post_notification_url ( base_url ) : \n 
~~~ return base_url + config . json_notification_handler_url \n 
def event_batch_json ( ) : \n 
def post_event_url ( base_url ) : \n 
~~~ return base_url + config . json_event_handler_url \n 
~~ @ pytest . mark . gen_test \n 
@ pytest . mark . parametrize ( "input" , [ \n 
( ) , \n 
def test_post_notification_empty_request ( setup_database , mock_processor , http_client , post_notification_url ~~~ request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = input ) \n 
with pytest . raises ( tornado . httpclient . HTTPError ) : \n 
~~~ yield http_client . fetch ( request ) \n 
~~ assert not context . request_processor . submit . called \n 
def test_post_notification ( setup_database , mock_processor , http_client , post_notification_url , \n 
notification_batch_json ) : \n 
request = tornado . httpclient . HTTPRequest ( post_notification_url , method = , body = notification_batch_json response = yield http_client . fetch ( request ) \n 
assert response . code == 200 \n 
assert context . request_processor . submit . called \n 
def test_post_event_empty_request ( setup_database , mock_processor , http_client , post_event_url , input ~~~ \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = input ) \n 
def test_post_event ( setup_database , mock_processor , http_client , post_event_url , event_batch_json ) : \n 
context . request_processor . submit . return_value = True \n 
request = tornado . httpclient . HTTPRequest ( post_event_url , method = , body = event_batch_json ) \n 
response = yield http_client . fetch ( request ) \n 
def test_post_event_service_unavailable ( setup_database , mock_processor , http_client , post_event_url , app ) : \n 
context . request_processor . submit . return_value = False \n 
RequestProcessor . submit . return_value = False \n 
from pywechat . excepts import WechatError \n 
class Basic ( object ) : \n 
def __init__ ( self , app_id , app_secret ) : \n 
self . __app_id = app_id \n 
self . __app_secret = app_secret \n 
self . __access_token = self . access_token \n 
self . __token_expires_at = None \n 
def access_token ( self ) : \n 
if self . __access_token and self . __token_expires_at : \n 
~~~ if self . __token_expires_at - time . time ( ) > 60 : \n 
~~~ return self . __access_token \n 
~~ ~~ self . _grant_access_token ( ) \n 
return self . __access_token \n 
~~ def _send_request ( self , method , url , ** kwargs ) : \n 
if not kwargs . get ( ) : \n 
~~~ kwargs [ ] = { \n 
"access_token" : self . access_token \n 
~~ if kwargs . get ( ) : \n 
~~~ data = json . dumps ( kwargs [ ] ) . encode ( ) \n 
kwargs [ "data" ] = data \n 
~~ request = requests . request ( \n 
method = method , \n 
url = url , \n 
** kwargs \n 
request . raise_for_status ( ) \n 
json_data = request . json ( ) \n 
self . _check_wechat_error ( json_data ) \n 
return json_data \n 
def _check_wechat_error ( cls , json_data ) : \n 
errcode = json_data . get ( ) \n 
if errcode and errcode != 0 : \n 
~~~ raise WechatError ( errcode , json_data . get ( ) ) \n 
~~ ~~ def _grant_access_token ( self ) : \n 
url = \n 
params = { \n 
"grant_type" : "client_credential" , \n 
"appid" : self . __app_id , \n 
"secret" : self . __app_secret \n 
json_data = self . _send_request ( , url , params = params ) \n 
self . __access_token = json_data . get ( ) \n 
self . __token_expires_at = int ( \n 
time . time ( ) ) + json_data . get ( ) \n 
~~ def _get_wechat_server_ips ( self ) : \n 
url = "https://api.weixin.qq.com/cgi-bin/getcallbackip" \n 
from . . model . hashes import Hashes \n 
from . . one_drive_object_base import OneDriveObjectBase \n 
class File ( OneDriveObjectBase ) : \n 
~~~ def __init__ ( self , prop_dict = { } ) : \n 
~~~ self . _prop_dict = prop_dict \n 
def hashes ( self ) : \n 
if "hashes" in self . _prop_dict : \n 
~~~ if isinstance ( self . _prop_dict [ "hashes" ] , OneDriveObjectBase ) : \n 
~~~ return self . _prop_dict [ "hashes" ] \n 
~~~ self . _prop_dict [ "hashes" ] = Hashes ( self . _prop_dict [ "hashes" ] ) \n 
return self . _prop_dict [ "hashes" ] \n 
~~ ~~ return None \n 
~~ @ hashes . setter \n 
def hashes ( self , val ) : \n 
~~~ self . _prop_dict [ "hashes" ] = val \n 
def mime_type ( self ) : \n 
if "mimeType" in self . _prop_dict : \n 
~~~ return self . _prop_dict [ "mimeType" ] \n 
~~ ~~ @ mime_type . setter \n 
def mime_type ( self , val ) : \n 
~~~ self . _prop_dict [ "mimeType" ] = val \n 
class RequestBuilderBase ( object ) : \n 
~~~ def __init__ ( self , request_url , client ) : \n 
self . _request_url = request_url \n 
self . _client = client \n 
~~ def append_to_request_url ( self , url_segment ) : \n 
return self . _request_url + "/" + url_segment \n 
from . . collection_base import CollectionRequestBase , CollectionResponseBase , CollectionPageBase \n 
from . . request_builder_base import RequestBuilderBase \n 
from . . model . item import Item \n 
class SharedCollectionRequest ( CollectionRequestBase ) : \n 
~~~ def __init__ ( self , request_url , client , options ) : \n 
super ( SharedCollectionRequest , self ) . __init__ ( request_url , client , options ) \n 
~~ def get ( self ) : \n 
self . method = "GET" \n 
collection_response = SharedCollectionResponse ( json . loads ( self . send ( ) . content ) ) \n 
return self . _page_from_response ( collection_response ) \n 
~~ ~~ class SharedCollectionRequestBuilder ( RequestBuilderBase ) : \n 
~~~ def __getitem__ ( self , key ) : \n 
return ItemRequestBuilder ( self . append_to_request_url ( str ( key ) ) , self . _client ) \n 
~~ def request ( self , expand = None , select = None , top = None , order_by = None , options = None ) : \n 
req = SharedCollectionRequest ( self . _request_url , self . _client , options ) \n 
req . _set_query_options ( expand = expand , select = select , top = top , order_by = order_by ) \n 
return req \n 
return self . request ( ) . get ( ) \n 
~~ ~~ class SharedCollectionResponse ( CollectionResponseBase ) : \n 
~~~ @ property \n 
def collection_page ( self ) : \n 
if self . _collection_page : \n 
~~~ self . _collection_page . _prop_list = self . _prop_dict [ "value" ] \n 
~~~ self . _collection_page = SharedCollectionPage ( self . _prop_dict [ "value" ] ) \n 
~~ return self . _collection_page \n 
~~ ~~ class SharedCollectionPage ( CollectionPageBase ) : \n 
~~~ def __getitem__ ( self , index ) : \n 
return Item ( self . _prop_list [ index ] ) \n 
~~ def shared ( self ) : \n 
for item in self . _prop_list : \n 
~~~ yield Item ( item ) \n 
~~ ~~ def _init_next_page_request ( self , next_page_link , client , options ) : \n 
self . _next_page_request = SharedCollectionRequest ( next_page_link , client , options ) \n 
~~ ~~ from . . request . item_request_builder import ItemRequestBuilder \n 
from __future__ import absolute_import , division , print_function \n 
import glob \n 
import os . path \n 
from cffi import FFI \n 
from cffi . verifier import Verifier \n 
__all__ = [ "ffi" ] \n 
HEADERS = glob . glob ( \n 
os . path . join ( os . path . abspath ( os . path . dirname ( __file__ ) ) , "*.h" ) \n 
ffi = FFI ( ) \n 
for header in sorted ( HEADERS ) : \n 
~~~ with open ( header , "r" ) as hfile : \n 
~~~ ffi . cdef ( hfile . read ( ) ) \n 
~~ ~~ ffi . verifier = Verifier ( \n 
ffi , \n 
libraries = [ "sodium" ] , \n 
ext_package = "nacl._lib" , \n 
class Library ( object ) : \n 
~~~ def __init__ ( self , ffi ) : \n 
~~~ self . ffi = ffi \n 
self . _lib = None \n 
def _compile_module ( * args , ** kwargs ) : \n 
~~ self . ffi . verifier . compile_module = _compile_module \n 
~~ def __getattr__ ( self , name ) : \n 
~~~ if self . _lib is None : \n 
~~~ self . _lib = self . ffi . verifier . load_library ( ) \n 
~~ return getattr ( self . _lib , name ) \n 
~~ ~~ lib = Library ( ffi ) \n 
import sqlite3 \n 
def migrate ( database_path ) : \n 
conn = sqlite3 . connect ( database_path ) \n 
conn . text_factory = str \n 
cursor = conn . cursor ( ) \n 
cursor . execute ( ) \n 
notifications = cursor . fetchall ( ) \n 
for n in notifications : \n 
~~~ cursor . execute ( , ( n [ 0 ] , n [ 1 ] , n [ 2 ] , n [ 3 ] , n [ 4 ] , n [ 5 ] , n [ 6 ] , n [ 7 ] , \n 
~~ cursor . execute ( ) \n 
conn . commit ( ) \n 
conn . close ( ) \n 
DEBUG = 5 \n 
WARNING = 4 \n 
INFO = 3 \n 
ERROR = 2 \n 
CRITICAL = 1 \n 
levels = { "debug" : 5 , "warning" : 4 , "info" : 3 , "error" : 2 , "critical" : 1 } \n 
class FileLogObserver ( log . FileLogObserver ) : \n 
~~~ def __init__ ( self , f = None , level = "info" , default = DEBUG ) : \n 
~~~ log . FileLogObserver . __init__ ( self , f or sys . stdout ) \n 
self . level = levels [ level ] \n 
self . default = default \n 
~~ def emit ( self , eventDict ) : \n 
~~~ ll = eventDict . get ( , self . default ) \n 
if eventDict [ ] or in eventDict or self . level >= ll : \n 
~~~ log . FileLogObserver . emit ( self , eventDict ) \n 
~~ ~~ ~~ class Logger ( object ) : \n 
~~~ def __init__ ( self , ** kwargs ) : \n 
~~~ self . kwargs = kwargs \n 
~~ def msg ( self , message , ** kw ) : \n 
~~~ kw . update ( self . kwargs ) \n 
if in kw and not isinstance ( kw [ ] , str ) : \n 
~~~ kw [ ] = kw [ ] . __class__ . __name__ \n 
~~ log . msg ( message , ** kw ) \n 
~~ def info ( self , message , ** kw ) : \n 
~~~ kw [ ] = INFO \n 
~~ def debug ( self , message , ** kw ) : \n 
~~~ kw [ ] = DEBUG \n 
~~ def warning ( self , message , ** kw ) : \n 
~~~ kw [ ] = WARNING \n 
~~ def error ( self , message , ** kw ) : \n 
~~~ kw [ ] = ERROR \n 
~~ def critical ( self , message , ** kw ) : \n 
~~~ kw [ ] = CRITICAL \n 
~~ ~~ try : \n 
~~~ theLogger \n 
~~ except NameError : \n 
~~~ theLogger = Logger ( ) \n 
msg = theLogger . msg \n 
info = theLogger . info \n 
debug = theLogger . debug \n 
warning = theLogger . warning \n 
error = theLogger . error \n 
critical = theLogger . critical \n 
~~ import sys \n 
_b = sys . version_info [ 0 ] < 3 and ( lambda x : x ) or ( lambda x : x . encode ( ) ) \n 
from google . protobuf import descriptor as _descriptor \n 
from google . protobuf import symbol_database as _symbol_database \n 
_sym_db = _symbol_database . Default ( ) \n 
_sym_db . RegisterFileDescriptor ( DESCRIPTOR ) \n 
_PEERSEEDS = _descriptor . Descriptor ( \n 
number = 1 , type = 12 , cpp_type = 9 , label = 3 , \n 
number = 2 , type = 12 , cpp_type = 9 , label = 2 , \n 
has_default_value = False , default_value = _b ( "" ) , \n 
oneofs = [ \n 
serialized_start = 15 , \n 
serialized_end = 69 , \n 
DESCRIPTOR . message_types_by_name [ ] = _PEERSEEDS \n 
PeerSeeds = _reflection . GeneratedProtocolMessageType ( , ( _message . Message , ) , dict ( \n 
DESCRIPTOR = _PEERSEEDS , \n 
__module__ = \n 
_sym_db . RegisterMessage ( PeerSeeds ) \n 
@ register . filter ( name = ) \n 
def get_item ( dictionary , key ) : \n 
~~~ return getattr ( dictionary , key ) \n 
~~ default_app_config = \n 
from . . utils . access_permissions import BaseAccessPermissions \n 
class MediafileAccessPermissions ( BaseAccessPermissions ) : \n 
def can_retrieve ( self , user ) : \n 
return user . has_perm ( ) \n 
~~ def get_serializer_class ( self , user = None ) : \n 
from . serializers import MediafileSerializer \n 
return MediafileSerializer \n 
~~ ~~ from __future__ import unicode_literals \n 
import openslides . utils . models \n 
~~~ initial = True \n 
dependencies = [ \n 
migrations . CreateModel ( \n 
( , models . AutoField ( auto_created = True , primary_key = True , serialize = False , verbose_name ( , models . CharField ( max_length = 128 , verbose_name = ) ) , \n 
( , models . DateTimeField ( blank = True , null = True , verbose_name = ( , models . BooleanField ( \n 
default = False , \n 
help_text = verbose_name = ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 , unique = True ) ) , \n 
( , models . CharField ( blank = True , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 255 ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 50 ) ) , \n 
( , models . TextField ( blank = True , default = ) ) , \n 
( , models . CharField ( blank = True , default = , max_length = 100 ) ) , \n 
( , models . BooleanField ( default = True ) ) , \n 
( , models . BooleanField ( default = False ) ) , \n 
( , models . ManyToManyField ( \n 
blank = True , \n 
help_text = related_name = , \n 
related_query_name = , \n 
to = , \n 
verbose_name = ) ) , \n 
help_text = , \n 
related_name = , \n 
options = { \n 
: ( \n 
( , ) ( , ) ) , \n 
: ( ) , \n 
: ( , , ) , \n 
bases = ( openslides . utils . models . RESTModelMixin , models . Model ) , \n 
~~ import json \n 
from django . dispatch import receiver \n 
from rest_framework import status \n 
from rest_framework . test import APIClient \n 
from openslides import __version__ as version \n 
from openslides . core . config import ConfigVariable , config \n 
from openslides . core . models import CustomSlide , Projector \n 
from openslides . core . signals import config_signal \n 
from openslides . utils . rest_api import ValidationError \n 
from openslides . utils . test import TestCase \n 
class ProjectorAPI ( TestCase ) : \n 
def test_slide_on_default_projector ( self ) : \n 
~~~ self . client . login ( username = , password = ) \n 
customslide = CustomSlide . objects . create ( title = , text = default_projector = Projector . objects . get ( pk = 1 ) \n 
default_projector . config = { \n 
: { : , : customslide . id } } \n 
default_projector . save ( ) \n 
response = self . client . get ( reverse ( , args = [ ] ) ) \n 
self . assertEqual ( response . status_code , status . HTTP_200_OK ) \n 
self . assertEqual ( json . loads ( response . content . decode ( ) ) , { \n 
: 1 , \n 
{ : customslide . id , \n 
: } } , \n 
: 0 , \n 
: 0 } ) \n 
~~ def test_invalid_slide_on_default_projector ( self ) : \n 
default_projector = Projector . objects . get ( pk = 1 ) \n 
: { : } } \n 
~~ ~~ class VersionView ( TestCase ) : \n 
def test_get ( self ) : \n 
response = self . client . get ( reverse ( ) ) \n 
: version , \n 
: } ] } ) \n 
~~ ~~ class ConfigViewSet ( TestCase ) : \n 
def test_retrieve ( self ) : \n 
config [ ] = \n 
response = self . client . get ( reverse ( , args = [ ] ) ) self . assertEqual ( \n 
response . data , \n 
: } ) \n 
~~ def test_update ( self ) : \n 
~~~ self . client = APIClient ( ) \n 
self . client . login ( username = , password = ) \n 
response = self . client . put ( \n 
reverse ( , args = [ ] ) , \n 
{ : } ) \n 
self . assertEqual ( config [ ] , ) \n 
~~ def test_update_wrong_datatype ( self ) : \n 
self . assertEqual ( response . status_code , status . HTTP_400_BAD_REQUEST ) \n 
~~ def test_update_wrong_datatype_that_can_be_converted ( self ) : \n 
self . client = APIClient ( ) \n 
self . assertEqual ( response . status_code , 200 ) \n 
~~ def test_update_good_choice ( self ) : \n 
~~ def test_update_bad_choice ( self ) : \n 
self . assertEqual ( response . data , { : } ) \n 
~~ def test_update_validator_ok ( self ) : \n 
~~ def test_update_validator_invalid ( self ) : \n 
~~ def test_update_only_with_key ( self ) : \n 
reverse ( , args = [ ] ) ) \n 
~~ def test_metadata_with_hidden ( self ) : \n 
response = self . client . options ( reverse ( ) ) \n 
filter_obj = filter ( \n 
lambda item : item [ ] == , \n 
response . data [ ] [ 0 ] [ ] [ 0 ] [ ] ) \n 
self . assertEqual ( len ( list ( filter_obj ) ) , 0 ) \n 
~~ ~~ def validator_for_testing ( value ) : \n 
if value == : \n 
~~~ raise ValidationError ( { : } ) \n 
~~ ~~ @ receiver ( config_signal , dispatch_uid = ) \n 
def set_simple_config_view_integration_config_test ( sender , ** kwargs ) : \n 
yield ConfigVariable ( \n 
default_value = None , \n 
label = ) \n 
default_value = ) \n 
default_value = 0 , \n 
input_type = ) \n 
default_value = , \n 
input_type = , \n 
choices = ( \n 
{ : , : } , { : , : } ) \n 
validators = ( validator_for_testing , ) ) \n 
label = , \n 
hidden = True ) \n 
~~ from unittest import TestCase \n 
from unittest . mock import MagicMock , patch \n 
from openslides . users . serializers import UserFullSerializer \n 
class UserCreateUpdateSerializerTest ( TestCase ) : \n 
~~~ def test_validate_no_data ( self ) : \n 
serializer = UserFullSerializer ( ) \n 
with self . assertRaises ( ValidationError ) : \n 
~~~ serializer . validate ( data ) \n 
~~ ~~ @ patch ( ) \n 
def test_validate_no_username ( self , generate_username ) : \n 
generate_username . return_value = \n 
data = { : } \n 
new_data = serializer . validate ( data ) \n 
self . assertEqual ( new_data [ ] , ) \n 
~~ def test_validate_no_username_in_patch_request ( self ) : \n 
view = MagicMock ( action = ) \n 
serializer = UserFullSerializer ( context = { : view } ) \n 
self . assertIsNone ( new_data . get ( ) ) \n 
#domain... #localhost... \n 
, re . IGNORECASE ) \n 
USERNAME_REGEX = re . compile ( , re . I ) \n 
FULLNAME_REGEX = re . compile ( , re . U ) \n 
EMAIL_REGEX = re . compile ( , re . IGNORECASE ) \n 
class CheckValue ( object ) : \n 
~~ def length ( self , data , minimum = - 1 , maximum = - 1 ) : \n 
len_input = len ( data ) \n 
if len_input < minimum or maximum != - 1 and len_input > maximum : \n 
~~ def regexp ( self , data , regex , flags = 0 ) : \n 
regex = re . compile ( regex , flags ) \n 
if regex . match ( data ) : \n 
~~ def username ( self , data ) : \n 
if USERNAME_REGEX . match ( data ) : \n 
~~ def full_name ( self , data ) : \n 
if FULLNAME_REGEX . match ( data ) : \n 
~~ def email ( self , data ) : \n 
if EMAIL_REGEX . match ( data ) : \n 
~~ def url ( self , data ) : \n 
if URL_REGEX . match ( data ) : \n 
~~ def url_two ( self , data ) : \n 
regex = re . compile ( , re . IGNORECASE ) \n 
~~ def is_integer ( self , data ) : \n 
~~~ tmp = int ( data ) \n 
~~ ~~ def float ( self , data ) : \n 
~~~ tmp = float ( data ) \n 
~~ ~~ ~~ import logging \n 
from bagpipe . bgp . common import utils \n 
from bagpipe . bgp . common import logDecorator \n 
from bagpipe . bgp . vpn . vpn_instance import VPNInstance \n 
from bagpipe . bgp . engine import RouteEvent \n 
from bagpipe . bgp . vpn . dataplane_drivers import DummyDataplaneDriver as _DummyDataplaneDriver \n 
from bagpipe . bgp . common . looking_glass import LookingGlass , LGMap \n 
from bagpipe . exabgp . structure . vpn import RouteDistinguisher , VPNLabelledPrefix \n 
from bagpipe . exabgp . structure . mpls import LabelStackEntry \n 
from bagpipe . exabgp . structure . address import AFI , SAFI \n 
from bagpipe . exabgp . structure . ip import Inet , Prefix \n 
from bagpipe . exabgp . message . update . route import Route \n 
from bagpipe . exabgp . message . update . attribute . nexthop import NextHop \n 
from bagpipe . exabgp . message . update . attribute . communities import ECommunities \n 
class DummyDataplaneDriver ( _DummyDataplaneDriver ) : \n 
~~ class VRF ( VPNInstance , LookingGlass ) : \n 
~~~ type = "ipvpn" \n 
afi = AFI ( AFI . ipv4 ) \n 
safi = SAFI ( SAFI . mpls_vpn ) \n 
@ logDecorator . log \n 
~~~ VPNInstance . __init__ ( self , * args , ** kwargs ) \n 
self . readvertised = set ( ) \n 
~~ def _routeFrom ( self , prefix , label , rd ) : \n 
~~~ return Route ( VPNLabelledPrefix ( self . afi , self . safi , prefix , rd , \n 
[ LabelStackEntry ( label , True ) ] \n 
~~ def generateVifBGPRoute ( self , macAdress , ipPrefix , prefixLen , label ) : \n 
~~~ route = self . _routeFrom ( Prefix ( self . afi , ipPrefix , prefixLen ) , label , \n 
RouteDistinguisher ( \n 
RouteDistinguisher . TYPE_IP_LOC , None , \n 
self . bgpManager . getLocalAddress ( ) , \n 
self . instanceId ) \n 
return self . _newRouteEntry ( self . afi , self . safi , self . exportRTs , \n 
route . nlri , route . attributes ) \n 
~~ def _getLocalLabels ( self ) : \n 
~~~ for portData in self . macAddress2LocalPortData . itervalues ( ) : \n 
~~~ yield portData [ ] \n 
~~ ~~ def _getRDFromLabel ( self , label ) : \n 
~~~ return RouteDistinguisher ( RouteDistinguisher . TYPE_IP_LOC , None , \n 
10000 + label ) \n 
~~ def _routeForReAdvertisement ( self , prefix , label ) : \n 
~~~ route = self . _routeFrom ( prefix , label , \n 
self . _getRDFromLabel ( label ) ) \n 
nh = Inet ( 1 , socket . inet_pton ( socket . AF_INET , \n 
self . dataplane . driver . getLocalAddress ( ) ) ) \n 
route . attributes . add ( NextHop ( nh ) ) \n 
route . attributes . add ( ECommunities ( self . readvertiseToRTs ) ) \n 
routeEntry = self . _newRouteEntry ( self . afi , self . safi , \n 
self . readvertiseToRTs , \n 
return routeEntry \n 
~~ @ logDecorator . log \n 
def _readvertise ( self , nlri ) : \n 
for label in self . _getLocalLabels ( ) : \n 
nlri . prefix , label ) \n 
routeEntry = self . _routeForReAdvertisement ( nlri . prefix , label ) \n 
self . _pushEvent ( RouteEvent ( RouteEvent . ADVERTISE , routeEntry ) ) \n 
~~ self . readvertised . add ( nlri . prefix ) \n 
def _readvertiseStop ( self , nlri ) : \n 
self . _pushEvent ( RouteEvent ( RouteEvent . WITHDRAW , routeEntry ) ) \n 
~~ self . readvertised . remove ( nlri . prefix ) \n 
~~ def vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) : \n 
~~~ VPNInstance . vifPlugged ( self , macAddress , ipAddressPrefix , localPort , \n 
advertiseSubnet ) \n 
label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
for prefix in self . readvertised : \n 
prefix ) \n 
routeEntry = self . _routeForReAdvertisement ( prefix , label ) \n 
~~ ~~ def vifUnplugged ( self , macAddress , ipAddressPrefix , advertiseSubnet ) : \n 
~~~ label = self . macAddress2LocalPortData [ macAddress ] [ ] \n 
~~ VPNInstance . vifUnplugged ( self , macAddress , ipAddressPrefix , \n 
~~ def _route2trackedEntry ( self , route ) : \n 
~~~ if isinstance ( route . nlri , VPNLabelledPrefix ) : \n 
~~~ return route . nlri . prefix \n 
type ( route . nlri ) ) \n 
~~ ~~ def _toReadvertise ( self , route ) : \n 
~~~ return ( len ( set ( route . routeTargets ) . intersection ( \n 
set ( self . readvertiseFromRTs ) ) ) > 0 ) \n 
~~ def _imported ( self , route ) : \n 
set ( self . importRTs ) ) ) > 0 ) \n 
~~ @ utils . synchronized \n 
def _newBestRoute ( self , entry , newRoute ) : \n 
~~~ prefix = entry \n 
if self . readvertise : \n 
if self . _toReadvertise ( newRoute ) : \n 
self . _readvertise ( newRoute . nlri ) \n 
if not self . _imported ( newRoute ) : \n 
~~ ~~ ~~ encaps = self . _checkEncaps ( newRoute ) \n 
if not encaps : \n 
~~ self . dataplane . setupDataplaneForRemoteEndpoint ( \n 
prefix , newRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
newRoute . nlri . labelStack [ 0 ] . labelValue , newRoute . nlri , encaps ) \n 
def _bestRouteRemoved ( self , entry , oldRoute , last ) : \n 
if self . readvertise and last : \n 
~~~ if self . _toReadvertise ( oldRoute ) : \n 
self . _readvertiseStop ( oldRoute . nlri ) \n 
if not self . _imported ( oldRoute ) : \n 
~~ ~~ ~~ if self . _skipRouteRemoval ( last ) : \n 
~~ self . dataplane . removeDataplaneForRemoteEndpoint ( \n 
prefix , oldRoute . attributes . get ( NextHop . ID ) . next_hop , \n 
oldRoute . nlri . labelStack [ 0 ] . labelValue , oldRoute . nlri ) \n 
~~ def getLGMap ( self ) : \n 
"readvertised" : ( LGMap . VALUE , [ repr ( prefix ) for prefix in \n 
self . readvertised ] ) \n 
from bagpipe . exabgp . message . update . attribute import AttributeID , Flag , Attribute \n 
class Origin ( Attribute ) : \n 
~~~ ID = AttributeID . ORIGIN \n 
FLAG = Flag . TRANSITIVE \n 
MULTIPLE = False \n 
IGP = 0x00 \n 
EGP = 0x01 \n 
INCOMPLETE = 0x02 \n 
def __init__ ( self , origin ) : \n 
~~~ self . origin = origin \n 
~~ def pack ( self ) : \n 
~~~ return self . _attribute ( chr ( self . origin ) ) \n 
~~~ return len ( self . pack ( ) ) \n 
~~~ if self . origin == 0x00 : return \n 
if self . origin == 0x01 : return \n 
if self . origin == 0x02 : return \n 
~~~ return str ( self ) \n 
~~ def __cmp__ ( self , other ) : \n 
~~~ if ( not isinstance ( other , Origin ) \n 
or ( self . origin != other . origin ) \n 
) : \n 
import inspect \n 
import signal \n 
from multiprocessing import Process \n 
import tasa \n 
from tasa . worker import BaseWorker \n 
logging . basicConfig ( level = logging . INFO ) \n 
def signal_handler ( signal , frame ) : \n 
~~~ sys . exit ( 0 ) \n 
~~ def _get_argparser ( ) : \n 
~~~ parser = argparse . ArgumentParser ( ) \n 
parser . add_argument ( \n 
, , action = , \n 
version = % ( \n 
tasa . __version__ , sys . version ) ) \n 
return parser \n 
~~ def run ( ) : \n 
~~~ sys . path . insert ( 0 , ) \n 
parser = _get_argparser ( ) \n 
parser . description = \n 
parser . add_argument ( , \n 
type = lambda w : w . partition ( ) [ : : 2 ] , \n 
help = \n 
worker_class_name = args . worker [ 1 ] or \n 
worker_module = __import__ ( args . worker [ 0 ] , globals ( ) , locals ( ) , \n 
[ worker_class_name ] ) \n 
~~~ WorkerClass = getattr ( worker_module , worker_class_name ) \n 
potential_workers = inspect . getmembers ( \n 
worker_module , \n 
lambda x : type ( x ) == type and issubclass ( x , BaseWorker ) ) \n 
if potential_workers : \n 
for name , value in potential_workers : \n 
~~~ print . join ( [ args . worker [ 0 ] , name ] ) \n 
~~ ~~ exit ( 1 ) \n 
~~ worker = WorkerClass ( ) \n 
print % ( args . worker [ 0 ] , \n 
worker . __class__ . __name__ ) \n 
~~~ for job in worker : \n 
~~~ if job : \n 
worker . __class__ . __name__ , \n 
str ( job ) [ : 50 ] ) \n 
~~~ time . sleep ( .3 ) \n 
~~ ~~ ~~ except KeyboardInterrupt : \n 
~~~ print \n 
~~ ~~ def runm ( ) : \n 
signal . signal ( signal . SIGINT , signal_handler ) \n 
count = int ( sys . argv . pop ( 1 ) ) \n 
processes = [ Process ( target = run , args = ( ) ) for x in range ( count ) ] \n 
~~~ for p in processes : \n 
~~~ p . start ( ) \n 
~~ ~~ except KeyError : \n 
~~~ p . join ( ) \n 
~~ ~~ ~~ def log ( ) : \n 
~~~ parser = _get_argparser ( ) \n 
raise NotImplemented ( ) \n 
~~~ cmd = if len ( sys . argv ) < 2 else sys . argv . pop ( 1 ) \n 
if cmd == : \n 
~~~ run ( ) \n 
~~ elif cmd == : \n 
~~~ log ( ) \n 
from time import sleep \n 
from flask import Flask \n 
from flask_tut . models import ( \n 
db , \n 
User , \n 
Address , \n 
app = Flask ( __name__ ) \n 
with app . app_context ( ) : \n 
~~~ db . create_all ( ) \n 
~~ i = 0 \n 
while i < 30 : \n 
~~~ address = Address ( description = + str ( i ) . rjust ( 2 , "0" ) ) \n 
db . session . add ( address ) \n 
user = User ( name = + str ( i ) . rjust ( 2 , "0" ) ) \n 
user . address = address \n 
db . session . add ( user ) \n 
sleep ( 1 ) \n 
i += 1 \n 
~~ db . session . commit ( ) \n 
import urllib2 \n 
import google \n 
import pyprind \n 
class Crawler ( object ) : \n 
~~~ version = "1.2.3" \n 
outputDir = "output" \n 
languageDir = "languages" \n 
basicString = "/get.php?username=%s&password=%s&type=m3u&output=mpegts" \n 
def __init__ ( self , language = "it" ) : \n 
self . language = language . lower ( ) \n 
self . parsedUrls = [ ] \n 
self . foundedAccounts = 0 \n 
~~ def change_language ( self , language = "it" ) : \n 
if os . path . isfile ( self . languageDir + "/" + language + ".txt" ) : \n 
~~~ self . language = language \n 
~~ ~~ def search_links ( self ) : \n 
for url in google . search ( self . searchString , num = 30 , stop = 1 ) : \n 
~~~ parsed = urlparse ( url ) \n 
self . parsedUrls . append ( parsed . scheme + "://" + parsed . netloc ) \n 
~~ ~~ def search_accounts ( self , url = None ) : \n 
if not self . parsedUrls : \n 
~~~ if not url : \n 
~~~ url = random . choice ( self . parsedUrls ) \n 
~~ fileName = self . languageDir + "/" + self . language + ".txt" \n 
fileLength = self . file_length ( fileName ) \n 
with open ( fileName ) as f : \n 
~~~ rows = f . readlines ( ) \n 
~~ for row in rows : \n 
~~~ opener = urllib2 . build_opener ( ) \n 
opener . addheaders = [ ( , ) ] \n 
response = opener . open ( url + self . basicString % ( row . rstrip ( ) . lstrip ( ) , row . rstrip ( ) fetched = response . read ( ) \n 
fileLength = fileLength - 1 \n 
progressBar . update ( ) \n 
if len ( fetched ) > 0 : \n 
~~~ newPath = self . outputDir + "/" + url . replace ( "http://" , "" ) \n 
self . create_file ( row , newPath , fetched ) \n 
~~ ~~ self . parsedUrls . remove ( url ) \n 
if self . foundedAccounts != 0 : \n 
~~ ~~ except IOError : \n 
~~ except urllib2 . HTTPError , e : \n 
~~ except urllib2 . URLError , e : \n 
~~ except Exception : \n 
~~ ~~ def create_file ( self , row , newPath , fetched ) : \n 
if os . path . exists ( newPath ) is False : \n 
~~~ os . makedirs ( newPath ) \n 
~~ outputFile = open ( str ( newPath ) + "/tv_channels_%s.m3u" % row . rstrip ( ) . lstrip ( ) , "w" ) \n 
outputFile . write ( fetched ) \n 
self . foundedAccounts = self . foundedAccounts + 1 \n 
outputFile . close ( ) \n 
~~ def file_length ( self , fileName ) : \n 
~~~ for i , l in enumerate ( f ) : \n 
~~ ~~ return i + 1 \n 
~~ ~~ from cStringIO import StringIO \n 
from binascii import b2a_hex \n 
from urllib import quote \n 
import Connecter \n 
~~~ True \n 
~~~ True = 1 \n 
False = 0 \n 
~~ DEBUG = False \n 
protocol_name = \n 
option_pattern = chr ( 0 ) * 8 \n 
def toint ( s ) : \n 
~~~ return long ( b2a_hex ( s ) , 16 ) \n 
~~ def tohex ( s ) : \n 
~~~ return b2a_hex ( s ) . upper ( ) \n 
~~ def make_readable ( s ) : \n 
~~~ if not s : \n 
~~ if quote ( s ) . find ( ) >= 0 : \n 
~~~ return tohex ( s ) \n 
~~ return \'"\' + s + \'"\' \n 
~~ streamno = 0 \n 
class StreamCheck : \n 
~~~ global streamno \n 
self . no = streamno \n 
streamno += 1 \n 
self . buffer = StringIO ( ) \n 
self . next_len , self . next_func = 1 , self . read_header_len \n 
~~ def read_header_len ( self , s ) : \n 
~~~ if ord ( s ) != len ( protocol_name ) : \n 
~~~ print self . no , \n 
~~ return len ( protocol_name ) , self . read_header \n 
~~ def read_header ( self , s ) : \n 
~~~ if s != protocol_name : \n 
~~ return 8 , self . read_reserved \n 
~~ def read_reserved ( self , s ) : \n 
~~~ return 20 , self . read_download_id \n 
~~ def read_download_id ( self , s ) : \n 
~~~ if DEBUG : \n 
~~~ print self . no , + tohex ( s ) \n 
~~ return 20 , self . read_peer_id \n 
~~ def read_peer_id ( self , s ) : \n 
~~~ print self . no , + make_readable ( s ) \n 
~~ return 4 , self . read_len \n 
~~ def read_len ( self , s ) : \n 
~~~ l = toint ( s ) \n 
if l > 2 ** 23 : \n 
~~~ print self . no , + str ( l ) + + s + \n 
~~ return l , self . read_message \n 
~~ def read_message ( self , s ) : \n 
~~~ return 4 , self . read_len \n 
~~ m = s [ 0 ] \n 
if ord ( m ) > 8 : \n 
~~~ print self . no , + str ( ord ( m ) ) \n 
~~ if m == Connecter . REQUEST : \n 
~~~ if len ( s ) != 13 : \n 
~~~ print self . no , + str ( len ( s ) ) \n 
return 4 , self . read_len \n 
~~ index = toint ( s [ 1 : 5 ] ) \n 
begin = toint ( s [ 5 : 9 ] ) \n 
length = toint ( s [ 9 : ] ) \n 
print self . no , + str ( index ) + + str ( begin ) + + str ( begin ) + + str ( length ) \n 
~~ elif m == Connecter . CANCEL : \n 
~~ elif m == Connecter . PIECE : \n 
~~~ index = toint ( s [ 1 : 5 ] ) \n 
length = len ( s ) - 9 \n 
~~~ print self . no , + str ( ord ( m ) ) + + str ( len ( s ) ) + \n 
~~ def write ( self , s ) : \n 
~~~ while 1 : \n 
~~~ i = self . next_len - self . buffer . tell ( ) \n 
if i > len ( s ) : \n 
~~~ self . buffer . write ( s ) \n 
~~ self . buffer . write ( s [ : i ] ) \n 
s = s [ i : ] \n 
m = self . buffer . getvalue ( ) \n 
self . buffer . reset ( ) \n 
self . buffer . truncate ( ) \n 
x = self . next_func ( m ) \n 
self . next_len , self . next_func = x \n 
~~ ~~ ~~ from types import * \n 
from cStringIO import StringIO \n 
def splitLine ( line , COLS = 80 , indent = 10 ) : \n 
width = COLS - ( len ( indent ) + 1 ) \n 
if indent and width < 15 : \n 
~~~ width = COLS - 2 \n 
~~ s = StringIO ( ) \n 
i = 0 \n 
for word in line . split ( ) : \n 
~~~ if i == 0 : \n 
~~~ s . write ( indent + word ) \n 
i = len ( word ) \n 
continue \n 
~~ if i + len ( word ) >= width : \n 
~~~ s . write ( + indent + word ) \n 
~~ s . write ( + word ) \n 
i += len ( word ) + 1 \n 
~~ return s . getvalue ( ) \n 
~~ def formatDefinitions ( options , COLS , presets = { } ) : \n 
~~~ s = StringIO ( ) \n 
for ( longname , default , doc ) in options : \n 
~~~ s . write ( + longname + ) \n 
default = presets . get ( longname , default ) \n 
if type ( default ) in ( IntType , LongType ) : \n 
~~~ default = int ( default ) \n 
~~ ~~ if default is not None : \n 
~~~ doc += + repr ( default ) + \n 
~~ s . write ( splitLine ( doc , COLS , 10 ) ) \n 
s . write ( ) \n 
~~ def usage ( string ) : \n 
~~~ raise ValueError ( string ) \n 
~~ def defaultargs ( options ) : \n 
~~~ l = { } \n 
~~~ if default is not None : \n 
~~~ l [ longname ] = default \n 
~~ ~~ return l \n 
~~ def parseargs ( argv , options , minargs = None , maxargs = None , presets = { } ) : \n 
~~~ config = { } \n 
longkeyed = { } \n 
for option in options : \n 
~~~ longname , default , doc = option \n 
longkeyed [ longname ] = option \n 
config [ longname ] = default \n 
~~~ config [ longname ] = presets [ longname ] \n 
~~ options = [ ] \n 
args = [ ] \n 
pos = 0 \n 
while pos < len ( argv ) : \n 
~~~ if argv [ pos ] [ : 2 ] != : \n 
~~~ args . append ( argv [ pos ] ) \n 
pos += 1 \n 
~~~ if pos == len ( argv ) - 1 : \n 
~~~ usage ( ) \n 
~~ key , value = argv [ pos ] [ 2 : ] , argv [ pos + 1 ] \n 
pos += 2 \n 
if not longkeyed . has_key ( key ) : \n 
~~~ usage ( + key ) \n 
~~ longname , default , doc = longkeyed [ key ] \n 
~~~ t = type ( config [ longname ] ) \n 
if t is NoneType or t is StringType : \n 
~~~ config [ longname ] = value \n 
~~ elif t in ( IntType , LongType ) : \n 
~~~ config [ longname ] = long ( value ) \n 
~~ elif t is FloatType : \n 
~~~ config [ longname ] = float ( value ) \n 
~~~ assert 0 \n 
~~ ~~ except ValueError , e : \n 
~~~ usage ( % ( key , str ( e ) ) ) \n 
~~ ~~ ~~ for key , value in config . items ( ) : \n 
~~~ if value is None : \n 
~~ ~~ if minargs is not None and len ( args ) < minargs : \n 
~~ if maxargs is not None and len ( args ) > maxargs : \n 
~~ return ( config , args ) \n 
~~ def test_parseargs ( ) : \n 
~~~ assert parseargs ( ( , , , , , , , , ) , ( ( , , ) , ( , assert parseargs ( [ ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
assert parseargs ( [ , , , ] , [ ( , , ) ] ) == ( { : } , [ ] ) \n 
~~~ parseargs ( [ ] , [ ( , , ) ] ) \n 
~~~ parseargs ( [ , ] , [ ] ) \n 
~~~ parseargs ( [ ] , [ ] , 1 , 2 ) \n 
~~ assert parseargs ( [ ] , [ ] , 1 , 2 ) == ( { } , [ ] ) \n 
assert parseargs ( [ , ] , [ ] , 1 , 2 ) == ( { } , [ , ] ) \n 
~~~ parseargs ( [ , , ] , [ ] , 1 , 2 ) \n 
~~~ parseargs ( [ , ] , [ ( , 3 , ) ] ) \n 
~~~ parseargs ( [ , ] , [ ( , 2.1 , ) ] ) \n 
~~ ~~ import datetime \n 
from south . db import db \n 
from south . v2 import SchemaMigration \n 
class Migration ( SchemaMigration ) : \n 
~~~ db . add_column ( , , \n 
self . gf ( ) ( to = orm [ keep_default = False ) \n 
~~~ db . delete_column ( , ) \n 
: { : } , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'auth.Permission\']" } , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.Group\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" : ( , [ ] , { : , : } , \n 
: ( , [ ] , { : , : } , \n 
: ( , [ ] , { : "\'taggit_taggeditem_tagged_items\'" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "\'taggit_taggeditem_items\'" } , \n 
: ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'videoportal.Video\']" } , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'videoportal.Channel\']" : ( , [ ] , { : , : ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.User\']" , : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : } \n 
from collections import namedtuple \n 
from shutil import rmtree \n 
from stat import S_IFDIR , S_IFREG , S_IFLNK \n 
from pygit2 import ( clone_repository , Signature , GIT_SORT_TOPOLOGICAL , \n 
GIT_FILEMODE_TREE , GIT_STATUS_CURRENT , \n 
GIT_FILEMODE_LINK , GIT_FILEMODE_BLOB , GIT_BRANCH_REMOTE , \n 
GIT_BRANCH_LOCAL , GIT_FILEMODE_BLOB_EXECUTABLE ) \n 
from six import iteritems \n 
from gitfs . cache import CommitCache \n 
from gitfs . log import log \n 
from gitfs . utils . path import split_path_into_components \n 
from gitfs . utils . commits import CommitsList \n 
DivergeCommits = namedtuple ( "DivergeCommits" , [ "common_parent" , \n 
"first_commits" , "second_commits" ] ) \n 
class Repository ( object ) : \n 
~~~ def __init__ ( self , repository , commits = None ) : \n 
~~~ self . _repo = repository \n 
self . commits = commits or CommitCache ( self ) \n 
self . behind = False \n 
~~ def __getitem__ ( self , item ) : \n 
return self . _repo [ item ] \n 
~~ def __getattr__ ( self , attr ) : \n 
if attr not in self . __dict__ : \n 
~~~ return getattr ( self . _repo , attr ) \n 
~~~ return self . __dict__ [ attr ] \n 
~~ ~~ def ahead ( self , upstream , branch ) : \n 
~~~ ahead , _ = self . diverge ( upstream , branch ) \n 
return ahead \n 
~~ def diverge ( self , upstream , branch ) : \n 
~~~ reference = "{}/{}" . format ( upstream , branch ) \n 
remote_branch = self . lookup_branch ( reference , GIT_BRANCH_REMOTE ) \n 
local_branch = self . lookup_branch ( branch , GIT_BRANCH_LOCAL ) \n 
if remote_branch . target == local_branch . target : \n 
~~~ return False , False \n 
~~ diverge_commits = self . find_diverge_commits ( local_branch , \n 
remote_branch ) \n 
behind = len ( diverge_commits . second_commits ) > 0 \n 
ahead = len ( diverge_commits . first_commits ) > 0 \n 
return ahead , behind \n 
~~ def checkout ( self , ref , * args , ** kwargs ) : \n 
~~~ result = self . _repo . checkout ( ref , * args , ** kwargs ) \n 
self . ignore . update ( ) \n 
status = self . _repo . status ( ) \n 
for path , status in iteritems ( status ) : \n 
~~~ if status == GIT_STATUS_CURRENT : \n 
~~ full_path = self . _full_path ( path ) \n 
if path not in self . _repo . index : \n 
~~~ if path not in self . ignore : \n 
~~~ os . unlink ( full_path ) \n 
~~ except OSError : \n 
~~~ rmtree ( \n 
full_path , \n 
onerror = lambda function , fpath , excinfo : log . info ( \n 
~~ ~~ continue \n 
~~ stats = self . get_git_object_default_stats ( ref , path ) \n 
current_stat = os . lstat ( full_path ) \n 
if stats [ ] != current_stat . st_mode : \n 
~~~ os . chmod ( full_path , current_stat . st_mode ) \n 
self . _repo . index . add ( self . _sanitize ( path ) ) \n 
~~ ~~ return result \n 
~~ def _sanitize ( self , path ) : \n 
~~~ if path is not None and path . startswith ( "/" ) : \n 
~~~ path = path [ 1 : ] \n 
~~ return path \n 
~~ def push ( self , upstream , branch , credentials ) : \n 
remote = self . get_remote ( upstream ) \n 
remote . push ( [ "refs/heads/%s" % ( branch ) ] , callbacks = credentials ) \n 
~~ def fetch ( self , upstream , branch_name , credentials ) : \n 
remote . fetch ( callbacks = credentials ) \n 
_ , behind = self . diverge ( upstream , branch_name ) \n 
self . behind = behind \n 
return behind \n 
~~ def commit ( self , message , author , commiter , parents = None , ref = "HEAD" ) : \n 
if status == { } : \n 
~~ author = Signature ( author [ 0 ] , author [ 1 ] ) \n 
commiter = Signature ( commiter [ 0 ] , commiter [ 1 ] ) \n 
tree = self . _repo . index . write_tree ( ) \n 
self . _repo . index . write ( ) \n 
if parents is None : \n 
~~~ parents = [ self . _repo . revparse_single ( ref ) . id ] \n 
~~ return self . _repo . create_commit ( ref , author , commiter , message , \n 
tree , parents ) \n 
def clone ( cls , remote_url , path , branch = None , credentials = None ) : \n 
repo = clone_repository ( remote_url , path , checkout_branch = branch , \n 
callbacks = credentials ) \n 
repo . checkout_head ( ) \n 
return cls ( repo ) \n 
~~ def _is_searched_entry ( self , entry_name , searched_entry , path_components ) : \n 
return ( entry_name == searched_entry and \n 
len ( path_components ) == 1 and \n 
entry_name == path_components [ 0 ] ) \n 
~~ def _get_git_object ( self , tree , obj_name , path_components , modifier ) : \n 
git_obj = None \n 
for entry in tree : \n 
~~~ if self . _is_searched_entry ( entry . name , obj_name , path_components ) : \n 
~~~ return modifier ( entry ) \n 
~~ elif entry . filemode == GIT_FILEMODE_TREE : \n 
~~~ git_obj = self . _get_git_object ( self . _repo [ entry . id ] , obj_name , \n 
path_components [ 1 : ] , modifier ) \n 
if git_obj : \n 
~~~ return git_obj \n 
~~ ~~ ~~ return git_obj \n 
~~ def get_git_object_type ( self , tree , path ) : \n 
path_components = split_path_into_components ( path ) \n 
~~~ return self . _get_git_object ( tree , path_components [ - 1 ] , \n 
path_components , \n 
lambda entry : entry . filemode ) \n 
~~~ return GIT_FILEMODE_TREE \n 
~~ ~~ def get_git_object ( self , tree , path ) : \n 
return self . _get_git_object ( tree , path_components [ - 1 ] , path_components , \n 
lambda entry : self . _repo [ entry . id ] ) \n 
~~ def get_git_object_default_stats ( self , ref , path ) : \n 
~~~ types = { \n 
GIT_FILEMODE_LINK : { \n 
: S_IFLNK | 0o444 , \n 
} , GIT_FILEMODE_TREE : { \n 
: S_IFDIR | 0o555 , \n 
: 2 \n 
} , GIT_FILEMODE_BLOB : { \n 
: S_IFREG | 0o444 , \n 
} , GIT_FILEMODE_BLOB_EXECUTABLE : { \n 
: S_IFREG | 0o555 , \n 
if path == "/" : \n 
~~~ return types [ GIT_FILEMODE_TREE ] \n 
~~ obj_type = self . get_git_object_type ( ref , path ) \n 
if obj_type is None : \n 
~~~ return obj_type \n 
~~ stats = types [ obj_type ] \n 
if obj_type in [ GIT_FILEMODE_BLOB , GIT_FILEMODE_BLOB_EXECUTABLE ] : \n 
~~~ stats [ ] = self . get_blob_size ( ref , path ) \n 
~~ return stats \n 
~~ def get_blob_size ( self , tree , path ) : \n 
return self . get_git_object ( tree , path ) . size \n 
~~ def get_blob_data ( self , tree , path ) : \n 
return self . get_git_object ( tree , path ) . data \n 
~~ def get_commit_dates ( self ) : \n 
return list ( self . commits . keys ( ) ) \n 
~~ def get_commits_by_date ( self , date ) : \n 
return list ( map ( str , self . commits [ date ] ) ) \n 
~~ def walk_branches ( self , sort , * branches ) : \n 
iterators = [ self . _repo . walk ( branch . target , sort ) \n 
for branch in branches ] \n 
stop_iteration = [ False for branch in branches ] \n 
commits = [ ] \n 
for iterator in iterators : \n 
~~~ commit = next ( iterator ) \n 
~~ except StopIteration : \n 
~~~ commit = None \n 
~~ commits . append ( commit ) \n 
~~ yield ( commit for commit in commits ) \n 
while not all ( stop_iteration ) : \n 
~~~ for index , iterator in enumerate ( iterators ) : \n 
commits [ index ] = commit \n 
~~~ stop_iteration [ index ] = True \n 
~~ ~~ if not all ( stop_iteration ) : \n 
~~~ yield ( commit for commit in commits ) \n 
~~ ~~ ~~ def remote_head ( self , upstream , branch ) : \n 
~~~ ref = "%s/%s" % ( upstream , branch ) \n 
remote = self . _repo . lookup_branch ( ref , GIT_BRANCH_REMOTE ) \n 
return remote . get_object ( ) \n 
~~ def get_remote ( self , name ) : \n 
remote = [ remote for remote in self . _repo . remotes \n 
if remote . name == name ] \n 
if not remote : \n 
~~ return remote [ 0 ] \n 
~~ def _full_path ( self , partial ) : \n 
~~~ if partial . startswith ( "/" ) : \n 
~~~ partial = partial [ 1 : ] \n 
~~ return os . path . join ( self . _repo . workdir , partial ) \n 
~~ def find_diverge_commits ( self , first_branch , second_branch ) : \n 
common_parent = None \n 
first_commits = CommitsList ( ) \n 
second_commits = CommitsList ( ) \n 
walker = self . walk_branches ( GIT_SORT_TOPOLOGICAL , \n 
first_branch , second_branch ) \n 
for first_commit , second_commit in walker : \n 
~~~ if ( first_commit in second_commits or \n 
second_commit in first_commits ) : \n 
~~ if first_commit not in first_commits : \n 
~~~ first_commits . append ( first_commit ) \n 
~~ if second_commit not in second_commits : \n 
~~~ second_commits . append ( second_commit ) \n 
~~ if second_commit . hex == first_commit . hex : \n 
~~~ index = second_commits . index ( first_commit ) \n 
~~~ second_commits = second_commits [ : index ] \n 
common_parent = first_commit \n 
~~~ index = first_commits . index ( second_commit ) \n 
~~~ first_commits = first_commits [ : index ] \n 
common_parent = second_commit \n 
~~ return DivergeCommits ( common_parent , first_commits , second_commits ) \n 
~~ ~~ from datetime import datetime \n 
from mock import MagicMock , call \n 
from pygit2 import GIT_SORT_TIME \n 
from gitfs . cache . commits import Commit , CommitCache \n 
class TestCommit ( object ) : \n 
~~~ def test_commit ( self ) : \n 
~~~ commit = Commit ( 1 , 1 , 1 ) \n 
new_commit = Commit ( 2 , 2 , "21111111111" ) \n 
assert new_commit > commit \n 
assert repr ( new_commit ) == "2-2111111111" \n 
~~ ~~ class TestCommitCache ( object ) : \n 
~~~ def test_cache ( self ) : \n 
~~~ mocked_repo = MagicMock ( ) \n 
mocked_commit = MagicMock ( ) \n 
mocked_repo . lookup_reference ( ) . resolve ( ) . target = "head" \n 
mocked_repo . walk . return_value = [ mocked_commit ] \n 
mocked_commit . commit_time = 1411135000 \n 
mocked_commit . hex = \n 
cache = CommitCache ( mocked_repo ) \n 
cache . update ( ) \n 
cache [ ] = Commit ( 1 , 1 , "1111111111" ) \n 
assert sorted ( cache . keys ( ) ) == [ , ] \n 
asserted_time = datetime . fromtimestamp ( mocked_commit . commit_time ) \n 
asserted_time = "{}-{}-{}" . format ( asserted_time . hour , asserted_time . minute , \n 
asserted_time . second ) \n 
assert repr ( cache [ ] ) == % asserted_time \n 
del cache [ ] \n 
for commit_date in cache : \n 
~~~ assert commit_date == \n 
~~ mocked_repo . lookup_reference . has_calls ( [ call ( "HEAD" ) ] ) \n 
mocked_repo . walk . assert_called_once_with ( "head" , GIT_SORT_TIME ) \n 
assert mocked_repo . lookup_reference ( ) . resolve . call_count == 2 \n 
~~ ~~ import pytest \n 
import datetime as dt \n 
from mock import MagicMock \n 
from gitfs . utils . strptime import TimeParser \n 
from gitfs . utils import strptime \n 
class TestDateTimeUtils ( object ) : \n 
~~~ def test_strptime ( self ) : \n 
~~~ date = dt . date ( 2014 , 8 , 21 ) \n 
datetime = dt . datetime ( 2014 , 8 , 21 , 1 , 2 , 3 ) \n 
to_datetime = True ) == datetime \n 
date = dt . date ( 2014 , 8 , 30 ) \n 
datetime = dt . datetime ( 2014 , 8 , 30 , 1 , 2 , 3 ) \n 
date = dt . date ( 1970 , 1 , 1 ) \n 
datetime = dt . datetime ( 1970 , 1 , 1 , 13 , 30 ) \n 
with pytest . raises ( ValueError ) : \n 
~~ ~~ def test_time_parser_match_with_value_error ( self ) : \n 
~~~ mocked_pattern = MagicMock ( ) \n 
mocked_pattern . match . return_value = False \n 
parser . pattern = mocked_pattern \n 
~~~ parser . match ( "daytime" ) \n 
~~ mocked_pattern . match . assert_called_once_with ( "daytime" ) \n 
~~ ~~ from zipa import api_github_com as github \n 
repos = github . orgs . django . repos \n 
for repo in repos [ { : , : } ] : \n 
~~~ print repo . name \n 
~~~ from django . conf . urls import patterns , url \n 
~~ urlpatterns = patterns ( , \n 
url ( , , name = ) , \n 
url ( , , name = ) , ) \n 
from django . http import * \n 
from django . core import serializers \n 
from django . core . exceptions import ValidationError , SuspiciousOperation , ObjectDoesNotExist \n 
from django . db import IntegrityError , connection , transaction \n 
from django . shortcuts import render_to_response \n 
from django . core . context_processors import csrf \n 
from django . contrib . comments . models import Comment \n 
from django . contrib . comments . forms import CommentForm \n 
from django . contrib . contenttypes . models import ContentType \n 
from django . contrib . auth . decorators import login_required , user_passes_test \n 
from django . contrib . sessions . models import Session \n 
from django . contrib . sessions . backends . db import SessionStore \n 
from django . contrib . gis . geos . collections import MultiPolygon \n 
from django . contrib . gis . geos import GEOSGeometry \n 
from django . contrib . gis . gdal import * \n 
from django . contrib . gis . gdal . libgdal import lgdal \n 
from django . contrib import humanize \n 
from django . template import loader , Context as DjangoContext , RequestContext \n 
from django . utils import simplejson as json , translation \n 
from django . utils . translation import ugettext as _ , ungettext as _n \n 
from django . template . defaultfilters import slugify , force_escape \n 
from tagging . utils import parse_tag_input \n 
from datetime import datetime , time , timedelta \n 
from decimal import * \n 
from functools import wraps \n 
from redistricting . calculators import * \n 
from redistricting . models import * \n 
from redistricting . tasks import * \n 
import random , string , math , types , copy , time , threading , traceback , os \n 
import commands , sys , tempfile , csv , hashlib , inflect , logging \n 
import ModestMaps \n 
from PIL import Image , ImageChops , ImageMath \n 
import urllib , urllib2 \n 
from xhtml2pdf . pisa import CreatePDF \n 
import StringIO \n 
UNASSIGNED_DISTRICT_ID = 0 \n 
def using_unique_session ( u ) : \n 
if u . is_anonymous ( ) or u . is_superuser : \n 
~~ sessions = Session . objects . all ( ) \n 
count = 0 \n 
for session in sessions : \n 
~~~ decoded = session . get_decoded ( ) \n 
if in decoded and decoded [ ] == u . id : \n 
~~~ if in decoded and decoded [ ] < datetime . now ( ) : \n 
~~~ Session . objects . filter ( session_key = session . session_key ) . delete ( ) \n 
~~~ count += 1 \n 
~~ ~~ ~~ except SuspiciousOperation : \n 
~~ ~~ for session in sessions : \n 
~~~ websession = SessionStore ( session_key = session . session_key ) \n 
websession [ ] = count \n 
websession . save ( ) \n 
~~ ~~ except SuspiciousOperation : \n 
~~ ~~ return ( count <= 1 ) \n 
~~ def unique_session_or_json_redirect ( function ) : \n 
def decorator ( request , * args , ** kwargs ) : \n 
~~~ def return_nonunique_session_result ( ) : \n 
~~~ status = { : False } \n 
status [ ] = _ ( \n 
status [ ] = \n 
return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ if not using_unique_session ( request . user ) : \n 
~~~ return return_nonunique_session_result ( ) \n 
~~~ return function ( request , * args , ** kwargs ) \n 
~~ ~~ return wraps ( function ) ( decorator ) \n 
~~ def is_session_available ( req ) : \n 
if req . user . is_superuser or req . user . is_staff : \n 
~~ sessions = Session . objects . filter ( expire_date__gt = datetime . now ( ) ) \n 
if ( not req . user . is_anonymous ( ) ) and in decoded and decoded [ ~~~ count += 1 \n 
~~ ~~ avail = count < settings . CONCURRENT_SESSIONS \n 
req . session [ ] = avail \n 
return avail \n 
~~ def note_session_activity ( req ) : \n 
window = timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT ) \n 
req . session [ ] = datetime . now ( ) + window \n 
def unloadplan ( request , planid ) : \n 
note_session_activity ( request ) \n 
status = { : False } \n 
ps = Plan . objects . filter ( pk = planid ) \n 
if len ( ps ) > 0 : \n 
~~~ p = ps [ 0 ] \n 
if not can_copy ( request . user , p ) : \n 
~~ if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ p . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
~~ ~~ status [ ] = True \n 
@ unique_session_or_json_redirect \n 
def copyplan ( request , planid ) : \n 
if not is_plan_ready ( planid ) : \n 
~~ status = { : False } \n 
p = Plan . objects . get ( pk = planid ) \n 
if ( request . method == "POST" ) : \n 
~~~ newname = request . POST [ "name" ] [ 0 : 200 ] \n 
shared = request . POST . get ( "shared" , False ) \n 
~~ plan_copy = Plan . objects . filter ( name = newname , owner = request . user , legislative_body = p . legislative_body \n 
if len ( plan_copy ) > 0 : \n 
~~ plan_copy = Plan ( name = newname , owner = request . user , is_shared = shared , legislative_body = p . legislative_body plan_copy . create_unassigned = False \n 
plan_copy . save ( ) \n 
districts = p . get_districts_at_version ( p . version , include_geom = True ) \n 
for district in districts : \n 
~~~ district_copy = copy . copy ( district ) \n 
district_copy . id = None \n 
district_copy . version = 0 \n 
district_copy . is_locked = False \n 
district_copy . plan = plan_copy \n 
~~~ district_copy . save ( ) \n 
~~ except Exception as inst : \n 
status [ "exception" ] = inst . message \n 
~~ district_copy . clone_relations_from ( district ) \n 
~~ data = serializers . serialize ( "json" , [ plan_copy ] ) \n 
return HttpResponse ( data , mimetype = ) \n 
def scoreplan ( request , planid ) : \n 
plan = Plan . objects . get ( pk = planid ) \n 
criterion = ValidationCriteria . objects . filter ( legislative_body = plan . legislative_body ) \n 
status [ ] = True \n 
for criteria in criterion : \n 
~~~ score = ComputedPlanScore . compute ( criteria . function , plan ) \n 
~~~ logger . debug ( traceback . format_exc ( ) ) \n 
~~ if not score or not score [ ] : \n 
~~~ status [ ] = False \n 
status [ ] = % ( criteria . get_short_label ( ) , criteria . get_long_description break \n 
~~ ~~ if status [ ] : \n 
~~~ status [ ] = True \n 
plan . is_valid = True \n 
plan . save ( ) \n 
~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ def get_user_info ( user ) : \n 
if user . is_anonymous ( ) : \n 
~~ profile = user . get_profile ( ) \n 
return { \n 
: user . username , \n 
: user . email , \n 
: profile . pass_hint , \n 
: user . first_name , \n 
: user . last_name , \n 
: profile . organization , \n 
: user . id \n 
~~ def commonplan ( request , planid ) : \n 
plan = Plan . objects . filter ( id = planid ) \n 
if plan . count ( ) == 1 : \n 
~~~ plan = plan [ 0 ] \n 
plan . edited = getutc ( plan . edited ) \n 
levels = plan . legislative_body . get_geolevels ( ) \n 
districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
editable = can_edit ( request . user , plan ) \n 
default_demo = plan . legislative_body . get_default_subject ( ) \n 
max_dists = plan . legislative_body . max_districts \n 
body_member_short_label = plan . legislative_body . get_short_label ( ) \n 
body_member_long_label = plan . legislative_body . get_label ( ) \n 
body_members = plan . legislative_body . get_members_label ( ) \n 
reporting_template = % plan . legislative_body . name if not plan . is_community ( ) \n 
index = body_member_short_label . find ( ) \n 
if index >= 0 : \n 
~~~ body_member_short_label = body_member_short_label [ 0 : index ] \n 
~~ index = body_member_long_label . find ( ) \n 
~~~ body_member_long_label = body_member_long_label [ 0 : index ] \n 
~~ if not editable and not can_view ( request . user , plan ) : \n 
~~~ plan = { } \n 
tags = [ ] \n 
calculator_reports = [ ] \n 
~~~ tags = Tag . objects . filter ( name__startswith = ) . order_by ( ) . values_list ( , flat tags = map ( lambda x : x [ 5 : ] , tags ) \n 
if settings . REPORTS_ENABLED == : \n 
~~~ report_displays = ScoreDisplay . objects . filter ( name = "%s_reports" % plan . legislative_body if len ( report_displays ) > 0 : \n 
~~~ calculator_reports = map ( lambda p : { \n 
: p . __unicode__ ( ) , \n 
: map ( lambda f : { \n 
: f . get_label ( ) , \n 
: f . id \n 
} , p . score_functions . all ( ) . filter ( selectable_bodies = plan . legislative_body } , report_displays [ 0 ] . scorepanel_set . all ( ) . order_by ( ) ) \n 
levels = list ( ) \n 
districts = { } \n 
editable = False \n 
default_demo = None \n 
max_dists = 0 \n 
body_member_short_label = \n 
body_member_long_label = _ ( ) + \n 
body_members = _n ( , , 2 ) \n 
reporting_template = None \n 
~~ demos = Subject . objects . all ( ) . order_by ( ) [ 0 : 3 ] \n 
layers = [ ] \n 
snaplayers = [ ] \n 
if len ( levels ) > 0 : \n 
~~~ study_area_extent = list ( levels [ 0 ] . geounit_set . extent ( field_name = ) ) \n 
~~~ for lb in LegislativeBody . objects . all ( ) : \n 
~~~ biglevel = lb . get_geolevels ( ) [ 0 ] \n 
if biglevel . geounit_set . count ( ) > 0 : \n 
~~~ study_area_extent = biglevel . geounit_set . extent ( field_name = ) \n 
~~ ~~ ~~ for level in levels : \n 
~~~ snaplayers . append ( { \n 
: level . id , \n 
: level . name , \n 
: + level . name , \n 
: level . get_long_description ( ) , \n 
: level . min_zoom \n 
~~ default_selected = False \n 
for demo in demos : \n 
~~~ isdefault = str ( ( not default_demo is None ) and ( demo . id == default_demo . id ) ) . lower ( ) \n 
if isdefault == : \n 
~~~ default_selected = True \n 
~~ layers . append ( { \n 
: demo . id , \n 
: demo . get_short_label ( ) , \n 
: demo . name , \n 
: isdefault , \n 
: str ( demo . is_displayed ) . lower ( ) \n 
~~ if default_demo and not default_selected : \n 
~~~ layers . insert ( 0 , { \n 
: default_demo . id , \n 
: default_demo . get_short_label ( ) , \n 
: default_demo . name , \n 
: str ( True ) . lower ( ) , \n 
: str ( default_demo . is_displayed ) . lower ( ) \n 
~~ if in settings . __members__ : \n 
~~~ mapserver_protocol = settings . MAP_SERVER_PROTOCOL \n 
~~~ mapserver_protocol = \n 
~~ short_label = body_member_short_label . strip ( ) . lower ( ) \n 
long_label = body_member_long_label . strip ( ) . lower ( ) \n 
has_regions = Region . objects . all ( ) . count ( ) > 1 \n 
bodies = LegislativeBody . objects . all ( ) . order_by ( , ) \n 
l_bodies = [ b for b in bodies if b in [ sd . legislative_body for sd in ScoreDisplay . objects . filter \n 
~~~ loader . get_template ( reporting_template ) \n 
~~~ reporting_template = None \n 
~~ return RequestContext ( request , { \n 
: bodies , \n 
: has_regions , \n 
: l_bodies , \n 
: plan , \n 
: districts , \n 
: settings . MAP_SERVER , \n 
: mapserver_protocol , \n 
: settings . BASE_MAPS , \n 
: settings . MAP_SERVER_NS , \n 
: settings . MAP_SERVER_NSHREF , \n 
: settings . FEATURE_LIMIT , \n 
: settings . ADJACENCY , \n 
: settings . CONVEX_CHOROPLETH , \n 
: layers , \n 
: snaplayers , \n 
: UNASSIGNED_DISTRICT_ID , \n 
: request . user . username != and request . user . username != , \n 
: settings . DEBUG and request . user . is_staff , \n 
: get_user_info ( request . user ) , \n 
: editable , \n 
: max_dists + 1 , \n 
: settings . GA_ACCOUNT , \n 
: settings . GA_DOMAIN , \n 
: short_label , \n 
: long_label , \n 
: body_members , \n 
: reporting_template , \n 
: study_area_extent , \n 
: len ( ScoreDisplay . objects . filter ( is_page = True ) ) > 0 , \n 
: json . dumps ( calculator_reports ) , \n 
: ( in settings . __members__ ) , \n 
: tags , \n 
: Site . objects . get_current ( ) , \n 
: translation . get_language ( ) , \n 
~~ def is_plan_ready ( planid ) : \n 
planid = int ( planid ) \n 
return planid == 0 or len ( Plan . objects . filter ( id = planid , processing_state = ProcessingState . READY ) \n 
~~ @ user_passes_test ( using_unique_session ) \n 
def viewplan ( request , planid ) : \n 
if not is_session_available ( request ) or not is_plan_ready ( planid ) : \n 
~~ if not request . user . is_anonymous ( ) and ( int ( planid ) == 0 ) and ( settings . MAX_UNDOS_AFTER_EDIT > 0 ~~~ for p in Plan . objects . filter ( owner = request . user ) : \n 
~~ ~~ return render_to_response ( , commonplan ( request , planid ) ) \n 
def editplan ( request , planid ) : \n 
if request . user . is_anonymous ( ) or not is_session_available ( request ) or not is_plan_ready ( planid ) ~~~ return HttpResponseRedirect ( ) \n 
~~ cfg = commonplan ( request , planid ) \n 
if cfg [ ] == False : \n 
~~~ return HttpResponseRedirect ( % planid ) \n 
~~ plan = Plan . objects . get ( id = planid , owner = request . user ) \n 
cfg [ ] = len ( cfg [ ] ) > plan . legislative_body . max_districts \n 
cfg [ ] = plan . get_available_districts ( ) \n 
if settings . MAX_UNDOS_AFTER_EDIT > 0 : \n 
~~~ plan . purge_beyond_nth_step ( settings . MAX_UNDOS_AFTER_EDIT ) \n 
~~ return render_to_response ( , cfg ) \n 
def printplan ( request , planid ) : \n 
if not is_session_available ( request ) : \n 
sha = hashlib . sha1 ( ) \n 
sha . update ( str ( planid ) + str ( datetime . now ( ) ) ) \n 
cfg [ ] = % sha . hexdigest ( ) \n 
cfg [ ] = % request . META [ ] \n 
~~~ if not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST or not in request . REQUEST : \n 
~~ height = 500 * 2 \n 
if in request . REQUEST : \n 
~~~ height = int ( request . REQUEST [ ] ) * 2 \n 
~~ width = 1024 * 2 \n 
~~~ width = int ( request . REQUEST [ ] ) * 2 \n 
~~ opacity = 0.8 \n 
~~~ opacity = float ( request . REQUEST [ ] ) \n 
~~ full_legend = json . loads ( request . REQUEST [ ] ) \n 
cfg [ ] = request . REQUEST [ ] \n 
cfg [ ] = full_legend [ ] \n 
cfg [ ] = Plan . objects . get ( id = int ( request . REQUEST [ ] ) ) \n 
cfg [ ] = datetime . now ( ) \n 
bbox = request . REQUEST [ ] . split ( ) \n 
pt1 = Point ( float ( bbox [ 0 ] ) , float ( bbox [ 1 ] ) , srid = 3785 ) \n 
pt1 . transform ( SpatialReference ( ) ) \n 
ll = ModestMaps . Geo . Location ( pt1 . y , pt1 . x ) \n 
pt2 = Point ( float ( bbox [ 2 ] ) , float ( bbox [ 3 ] ) , srid = 3785 ) \n 
pt2 . transform ( SpatialReference ( ) ) \n 
ur = ModestMaps . Geo . Location ( pt2 . y , pt2 . x ) \n 
dims = ModestMaps . Core . Point ( width , height ) \n 
provider = ModestMaps . OpenStreetMap . Provider ( ) \n 
basemap = ModestMaps . mapByExtent ( provider , ll , ur , dims ) \n 
fullImg = basemap . draw ( ) \n 
provider = ModestMaps . WMS . Provider ( cfg [ ] , { \n 
: cfg [ ] , \n 
: 512 , \n 
: 512 \n 
overlayImg = ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) \n 
maskImg = ImageChops . invert ( overlayImg ) \n 
: request . REQUEST [ ] , \n 
overlayImg = Image . blend ( overlayImg , ModestMaps . mapByExtent ( provider , ll , ur , dims ) . draw ( ) , \n 
fullImg = Image . composite ( fullImg , Image . blend ( fullImg , overlayImg , opacity ) , maskImg ) \n 
fullImg . save ( settings . WEB_TEMP + ( % sha . hexdigest ( ) ) , , quality = 100 ) \n 
t = loader . get_template ( ) \n 
page = t . render ( DjangoContext ( cfg ) ) \n 
result = StringIO . StringIO ( ) \n 
CreatePDF ( page , result , show_error_as_pdf = True ) \n 
response = HttpResponse ( result . getvalue ( ) , mimetype = ) \n 
response [ ] = \n 
return response \n 
~~ ~~ @ login_required \n 
def createplan ( request ) : \n 
if request . method == "POST" : \n 
~~~ name = request . POST [ ] [ 0 : 200 ] \n 
body = LegislativeBody . objects . get ( id = int ( request . POST [ ] ) ) \n 
plan = Plan ( name = name , owner = request . user , legislative_body = body , processing_state = ProcessingState try : \n 
~~~ plan . save ( ) \n 
status = serializers . serialize ( "json" , [ plan ] ) \n 
~~ ~~ return HttpResponse ( json . dumps ( status ) , mimetype = ) \n 
~~ @ unique_session_or_json_redirect \n 
def uploadfile ( request ) : \n 
if request . user . is_anonymous ( ) : \n 
~~ status = commonplan ( request , 0 ) \n 
index_file = request . FILES . get ( , False ) \n 
if not index_file : \n 
return render_to_response ( , status ) \n 
~~~ filename = index_file . name \n 
~~ if index_file . size > settings . MAX_UPLOAD_SIZE : \n 
~~~ logger . error ( ) \n 
status [ ] = False \n 
~~ if not filename . endswith ( ( , ) ) : \n 
~~ elif request . POST [ ] == : \n 
~~~ dest = tempfile . NamedTemporaryFile ( mode = , delete = False ) \n 
for chunk in request . FILES [ ] . chunks ( ) : \n 
~~~ dest . write ( chunk ) \n 
~~ dest . close ( ) \n 
if request . FILES [ ] . name . endswith ( ) : \n 
~~~ os . rename ( dest . name , % ( dest . name , ) ) \n 
filename = % ( dest . name , ) \n 
~~~ filename = dest . name \n 
~~ ~~ except Exception as ex : \n 
logger . error ( , ex ) \n 
~~ DistrictIndexFile . index2plan . delay ( request . POST [ ] , request . POST [ \n 
~~ return render_to_response ( , status ) \n 
~~ def generate_report_hash ( qdict ) : \n 
params = qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) + qdict . get ( , ) \n 
sha . update ( params ) \n 
return sha . hexdigest ( ) \n 
def getreport ( request , planid ) : \n 
~~~ plan = Plan . objects . get ( pk = planid ) \n 
~~~ status [ ] = _ ( ) \n 
~~ if not can_view ( request . user , plan ) : \n 
~~ if not settings . REPORTS_ENABLED is None : \n 
~~ if request . method != : \n 
~~ stamp = request . POST . get ( , generate_report_hash ( request . POST ) ) \n 
rptstatus = PlanReport . checkreport ( planid , stamp ) \n 
if rptstatus == : \n 
~~~ status = { \n 
: PlanReport . getreport ( planid , stamp ) , \n 
: _ ( ) , \n 
: stamp \n 
~~ elif rptstatus == : \n 
: reverse ( getreport , args = [ planid ] ) , \n 
: 10 , \n 
req = { \n 
: request . POST . get ( , ) , \n 
: request . POST . getlist ( ) , \n 
: request . POST . get ( , ) \n 
PlanReport . markpending ( planid , stamp ) \n 
PlanReport . createreport . delay ( planid , stamp , req , language = translation . get_language ( ) ) \n 
~~~ status [ ] = _ ( \n 
def getcalculatorreport ( request , planid ) : \n 
~~ function_ids = request . POST . get ( , ) \n 
sha . update ( function_ids ) \n 
stamp = request . POST . get ( , sha . hexdigest ( ) ) \n 
rptstatus = CalculatorReport . checkreport ( planid , stamp ) \n 
: CalculatorReport . getreport ( planid , stamp ) , \n 
: reverse ( getcalculatorreport , args = [ planid ] ) , \n 
: 5 , \n 
req = { : function_ids } \n 
CalculatorReport . markpending ( planid , stamp ) \n 
CalculatorReport . createcalculatorreport . delay ( planid , stamp , req , language = translation . get_language ~~ else : \n 
def newdistrict ( request , planid ) : \n 
if len ( request . REQUEST . items ( ) ) >= 3 : \n 
~~~ plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
~~~ geolevel = request . REQUEST [ ] \n 
~~~ geolevel = None \n 
~~ if in request . REQUEST : \n 
~~~ geounit_ids = string . split ( request . REQUEST [ ] , ) \n 
~~~ geounit_ids = None \n 
~~~ district_id = int ( request . REQUEST [ ] ) \n 
~~~ district_id = None \n 
~~~ district_short = request . REQUEST [ ] [ 0 : 10 ] \n 
~~ elif not district_id is None : \n 
~~~ district_short = plan . legislative_body . get_short_label ( ) % { : district_id } \n 
~~~ district_short = None \n 
~~~ district_long = request . REQUEST [ ] [ 0 : 256 ] \n 
~~~ district_long = plan . legislative_body . get_label ( ) % { : district_id } \n 
~~~ district_long = None \n 
~~~ version = request . REQUEST [ ] \n 
~~~ version = plan . version \n 
~~ if geolevel and geounit_ids and district_id : \n 
~~~ fixed = plan . add_geounits ( ( district_id , district_short , district_long , ) , geounit_ids \n 
district = plan . district_set . filter ( district_id = district_id , short_label = district_short if plan . legislative_body . multi_members_allowed : \n 
~~~ district . num_members = plan . legislative_body . min_multi_district_members \n 
district . save ( ) \n 
~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
if in request . POST and request . POST [ ] != : \n 
~~~ comment = Comment ( \n 
object_pk = district . id , \n 
content_type = ct , \n 
site_id = Site . objects . get_current ( ) . id , \n 
user_name = request . user . username , \n 
user_email = request . user . email , \n 
comment = request . POST [ ] ) \n 
comment . save ( ) \n 
~~ if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~~ strtags = request . REQUEST . getlist ( ) \n 
for strtag in strtags : \n 
~~~ if strtag == : \n 
~~ if strtag . count ( ) > 0 : \n 
~~~ strtag = \'"type=%s"\' % strtag \n 
~~~ strtag = % strtag \n 
~~ Tag . objects . add_tag ( district , strtag ) \n 
status [ ] = _ ( ) \n 
plan = Plan . objects . get ( pk = planid , owner = request . user ) \n 
status [ ] = getutc ( plan . edited ) . isoformat ( ) \n 
status [ ] = district_id \n 
status [ ] = plan . version \n 
~~ except ValidationError : \n 
~~ except Exception , ex : \n 
~~~ logger . warn ( ) \n 
logger . debug ( , ex ) \n 
@ transaction . commit_manually \n 
def add_districts_to_plan ( request , planid ) : \n 
~~ if not can_edit ( request . user , plan ) : \n 
~~ district_list = request . POST . getlist ( ) \n 
if len ( district_list ) == 0 : \n 
~~~ districts = District . objects . filter ( id__in = district_list ) \n 
version = int ( request . POST . get ( , None ) ) \n 
status [ ] = _ ( ) % { : len ( districts ) } \n 
~~ allowed_districts = plan . get_available_districts ( version = version ) \n 
if len ( districts ) > allowed_districts : \n 
~~~ status [ ] = _ ( ) % { : allowed_districts } \n 
~~~ results = plan . paste_districts ( districts , version = version ) \n 
transaction . commit ( ) \n 
status [ ] = _ ( ) % { status [ ] = plan . version \n 
~~ except Exception as ex : \n 
~~~ transaction . rollback ( ) \n 
status [ ] = str ( ex ) \n 
status [ ] = traceback . format_exc ( ) \n 
def assign_district_members ( request , planid ) : \n 
~~ leg_bod = plan . legislative_body \n 
if ( not leg_bod . multi_members_allowed ) : \n 
~~ districts = request . POST . getlist ( ) \n 
counts = request . POST . getlist ( ) \n 
~~~ changed = 0 \n 
for i in range ( 0 , len ( districts ) ) : \n 
~~~ id = int ( districts [ i ] ) \n 
count = int ( counts [ i ] ) \n 
district = District . objects . filter ( plan = plan , district_id = id , version__lte = version ) . order_by \n 
if district . num_members != count : \n 
~~~ if ( changed == 0 ) : \n 
~~~ if version != plan . version : \n 
~~~ plan . purge ( after = version ) \n 
~~ plan . version = plan . version + 1 \n 
~~ plan . update_num_members ( district , count ) \n 
changed += 1 \n 
~~ ~~ transaction . commit ( ) \n 
status [ ] = changed \n 
) % { : changed } \n 
logger . warn ( ) \n 
def combine_districts ( request , planid ) : \n 
~~ version = int ( request . POST . get ( , plan . version ) ) \n 
from_id = int ( request . POST . get ( , - 1 ) ) \n 
to_id = int ( request . POST . get ( , None ) ) \n 
~~~ all_districts = plan . get_districts_at_version ( version , include_geom = True ) \n 
from_districts = filter ( lambda d : True if d . district_id == from_id else False , all_districts to_district = filter ( lambda d : True if d . district_id == to_id else False , all_districts ) [ 0 ] \n 
locked = to_district . is_locked \n 
for district in from_districts : \n 
~~~ if district . is_locked : \n 
~~~ locked = True \n 
~~ ~~ if locked : \n 
~~ result = plan . combine_districts ( to_district , from_districts , version = version ) \n 
if result [ 0 ] == True : \n 
status [ ] = result [ 1 ] \n 
~~ ~~ except Exception , ex : \n 
def fix_unassigned ( request , planid ) : \n 
~~~ version = int ( request . POST . get ( , plan . version ) ) \n 
result = plan . fix_unassigned ( version ) \n 
status [ ] = result [ 0 ] \n 
def get_splits ( request , planid , otherid , othertype ) : \n 
otherid = int ( otherid ) \n 
~~ version = int ( request . REQUEST [ ] if in request . REQUEST else plan . version ) \n 
~~~ if othertype == : \n 
~~~ otherplan = Plan . objects . get ( pk = otherid ) \n 
~~ if not can_view ( request . user , otherplan ) : \n 
~~ otherversion = int ( request . REQUEST [ ] if in request . REQUEST splits = plan . find_plan_splits ( otherplan , version , otherversion ) \n 
~~ elif othertype == : \n 
~~~ splits = plan . find_geolevel_splits ( otherid , version ) \n 
~~~ status [ ] = _ ( ) % { : othertype } \n 
~~ split_word = _ ( ) if len ( splits ) == 1 else inflect . engine ( ) . plural ( _ ( ) ) \n 
status [ ] = _ ( ) % { : len ( splits ) , : split_word } \n 
status [ ] = splits \n 
status [ ] = list ( set ( [ i [ 0 ] for i in splits ] ) ) \n 
status [ ] = list ( set ( [ i [ 1 ] for i in splits ] ) ) \n 
~~ def get_processing_status ( request ) : \n 
plan_ids = request . REQUEST . getlist ( ) \n 
if len ( plan_ids ) == 0 : \n 
~~~ statuses = { } \n 
for p in Plan . objects . filter ( id__in = plan_ids ) : \n 
~~~ statuses [ str ( p . id ) ] = p . get_processing_state_display ( ) \n 
~~ status [ ] = True \n 
status [ ] = statuses \n 
~~ def get_splits_report ( request , planid ) : \n 
~~~ return HttpResponse ( _ ( ) , mimetype = ) \n 
~~ if not using_unique_session ( request . user ) or not can_view ( request . user , plan ) : \n 
~~~ return HttpResponseForbidden ( ) \n 
inverse = request . REQUEST [ ] == if in request . REQUEST else False \n 
extended = request . REQUEST [ ] == if in request . REQUEST else False \n 
layers = request . REQUEST . getlist ( ) \n 
if len ( layers ) == 0 : \n 
~~~ report = loader . get_template ( ) \n 
html = \n 
for layer in layers : \n 
~~~ my_context = { : extended } \n 
my_context . update ( plan . compute_splits ( layer , version = version , inverse = inverse , extended last_item = layer is layers [ - 1 ] \n 
community_info = plan . get_community_type_info ( layer , version = version , inverse = inverse if community_info is not None : \n 
~~~ my_context . update ( community_info ) \n 
~~ calc_context = DjangoContext ( my_context ) \n 
html += report . render ( calc_context ) \n 
if not last_item : \n 
~~~ html += \n 
~~ ~~ return HttpResponse ( html , mimetype = ) \n 
return HttpResponse ( str ( ex ) , mimetype = ) \n 
def addtodistrict ( request , planid , districtid ) : \n 
if len ( request . REQUEST . items ( ) ) >= 2 : \n 
~~~ geolevel = request . REQUEST [ "geolevel" ] \n 
geounit_ids = string . split ( request . REQUEST [ "geounits" ] , "|" ) \n 
~~~ status [ ] = traceback . format_exc ( ) \n 
~~~ fixed = plan . add_geounits ( districtid , geounit_ids , geolevel , version ) \n 
status [ ] = True ; \n 
status [ ] = _ ( ) % { : fixed } \n 
status [ ] = fixed \n 
def setdistrictlock ( request , planid , district_id ) : \n 
if request . method != : \n 
~~ lock = request . POST . get ( ) . lower ( ) == \n 
version = request . POST . get ( ) \n 
if lock == None : \n 
~~ elif version == None : \n 
district = plan . district_set . filter ( district_id = district_id , version__lte = version ) . order_by ( ~~ except ObjectDoesNotExist : \n 
~~ if plan . owner != request . user : \n 
~~ district . is_locked = lock \n 
status [ ] = _ ( ) % { : _ ( ) if lock else _ ( ) } \n 
def getdistricts ( request , planid ) : \n 
~~~ version = int ( request . REQUEST [ ] ) \n 
~~ districts = plan . get_districts_at_version ( version , include_geom = False ) \n 
status [ ] = [ ] \n 
status [ ] = plan . legislative_body . max_districts - len ( districts ) + 1 \n 
max_version = max ( [ d . version for d in districts ] ) \n 
can_undo = max_version > plan . min_version \n 
~~~ status [ ] . append ( { \n 
: district . district_id , \n 
: . join ( map ( _ , district . short_label . split ( ) ) ) , \n 
: . join ( map ( _ , district . long_label . split ( ) ) ) , \n 
: district . version \n 
~~ status [ ] = can_undo \n 
~~ def simple_district_versioned ( request , planid , district_ids = None ) : \n 
status = { : } \n 
~~ subject_id = None \n 
~~~ subject_id = request . REQUEST [ ] \n 
~~ elif plan . legislative_body . get_default_subject ( ) : \n 
~~~ subject_id = plan . legislative_body . get_default_subject ( ) . id \n 
~~ geolevel = plan . legislative_body . get_geolevels ( ) [ 0 ] . id \n 
~~~ geolevel = int ( request . REQUEST [ ] ) \n 
~~~ district_ids = request . REQUEST [ ] \n 
if len ( district_ids ) > 0 : \n 
~~~ district_ids = district_ids . split ( ) \n 
~~~ district_ids = [ ] \n 
~~ ~~ if subject_id : \n 
~~~ bbox = None \n 
~~~ bbox = request . REQUEST [ ] \n 
bbox = tuple ( map ( lambda x : float ( x ) , bbox . split ( ) ) ) \n 
~~~ bbox = plan . district_set . all ( ) . extent ( field_name = ) \n 
~~ status [ ] = plan . get_wfs_districts ( version , subject_id , bbox , geolevel , district_ids ~~ else : \n 
~~~ status [ ] = [ ] \n 
~~ def get_unlocked_simple_geometries ( request , planid ) : \n 
version = request . POST . get ( , plan . version ) \n 
geolevel = request . POST . get ( , plan . legislative_body . get_geolevels ( ) [ 0 ] . id ) \n 
geom = request . POST . get ( , None ) \n 
if geom is not None : \n 
~~~ wkt = request . POST . get ( , None ) \n 
geom = GEOSGeometry ( wkt ) \n 
~~ except GEOSException : \n 
~~~ wkt = request . REQUEST [ ] . replace ( , ) \n 
wkt = wkt . replace ( , ) . replace ( , ) \n 
~~~ geom = GEOSGeometry ( wkt ) \n 
~~~ geom = None \n 
~~ ~~ selection = Q ( geom__intersects = geom ) \n 
districts = [ d . id for d in plan . get_districts_at_version ( version , include_geom = True ) if locked = District . objects . filter ( id__in = districts ) . collect ( ) \n 
locked_buffered = locked . simplify ( 100 , True ) . buffer ( 100 ) if locked else None \n 
filtered = Geolevel . objects . get ( id = geolevel ) . geounit_set . filter ( selection ) \n 
features = [ ] \n 
for feature in filtered : \n 
~~~ geom = feature . simple \n 
if locked and geom . intersects ( locked_buffered ) : \n 
~~~ if feature . geom . within ( locked ) : \n 
~~ if feature . geom . overlaps ( locked ) : \n 
~~ ~~ features . append ( { \n 
: % feature . id , \n 
: json . loads ( geom . json ) , \n 
: feature . name , \n 
: geolevel , \n 
: feature . id \n 
~~ status [ ] = features \n 
def get_statistics ( request , planid ) : \n 
~~~ note_session_activity ( request ) \n 
return HttpResponse ( json . dumps ( status ) , mimetype = , status = 500 ) \n 
~~~ display = ScoreDisplay . objects . get ( legislative_body = plan . legislative_body , name = "%s_sidebar_demo" ~~ except : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . POST [ ] ) \n 
logger . warn ( str ( request . POST ) ) \n 
~~~ html = display . render ( plan , request , version = version ) \n 
return HttpResponse ( html , mimetype = ) \n 
~~ ~~ def getutc ( t ) : \n 
t_tuple = t . timetuple ( ) \n 
t_seconds = time . mktime ( t_tuple ) \n 
return t . utcfromtimestamp ( t_seconds ) \n 
def getdistrictfilestatus ( request , planid ) : \n 
if not can_copy ( request . user , plan ) : \n 
~~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
file_status = DistrictFile . get_file_status ( plan , shape = is_shape ) \n 
status [ ] = file_status \n 
status [ ] = ex \n 
def getdistrictfile ( request , planid ) : \n 
~~ is_shape = in request . REQUEST and request . REQUEST [ ] == \n 
if file_status == : \n 
~~~ if is_shape : \n 
~~~ archive = DistrictShapeFile . plan2shape ( plan ) \n 
~~~ archive = DistrictIndexFile . plan2index ( plan ) \n 
~~ response = HttpResponse ( open ( archive . name ) . read ( ) , content_type = ) \n 
~~~ DistrictShapeFile . plan2shape . delay ( plan ) \n 
~~~ DistrictIndexFile . plan2index . delay ( plan ) \n 
~~ response = HttpResponse ( _ ( \n 
def emaildistrictindexfile ( request , planid ) : \n 
~~ plan = Plan . objects . get ( pk = planid ) \n 
~~ DistrictIndexFile . emailfile . delay ( plan , request . user , request . POST , translation . get_language ( ) ) \n 
return HttpResponse ( json . dumps ( { \n 
: _ ( ) } ) , \n 
mimetype = ) \n 
~~ def getvalidplans ( leg_body , owner = None ) : \n 
pfilter = Q ( legislative_body = leg_body ) & Q ( is_valid = True ) \n 
if owner is not None : \n 
~~~ pfilter = pfilter & Q ( owner = owner ) \n 
~~ return list ( Plan . objects . filter ( pfilter ) ) \n 
~~ def getleaderboarddisplay ( leg_body , owner_filter ) : \n 
~~~ return ScoreDisplay . objects . get ( name = "%s_leader_%s" % ( leg_body . name , owner_filter ) ) \n 
~~ ~~ def getleaderboard ( request ) : \n 
if not using_unique_session ( request . user ) : \n 
~~ owner_filter = request . REQUEST [ ] \n 
body_pk = int ( request . REQUEST [ ] ) ; \n 
leg_body = LegislativeBody . objects . get ( pk = body_pk ) \n 
display = getleaderboarddisplay ( leg_body , owner_filter ) \n 
if display is None : \n 
~~ plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
~~~ html = display . render ( plans , request ) \n 
~~ ~~ def getleaderboardcsv ( request ) : \n 
plans = getvalidplans ( leg_body , request . user if owner_filter == else None ) \n 
panels = display . scorepanel_set . all ( ) . order_by ( ) \n 
~~~ response = HttpResponse ( mimetype = ) \n 
writer = csv . writer ( response ) \n 
writer . writerow ( [ , , ] + [ p . __unicode__ ( ) for p in panels ] ) \n 
for plan in plans : \n 
~~~ row = [ plan . id , plan . name , plan . owner . username ] \n 
for panel in panels : \n 
~~~ function = panel . score_functions . all ( ) [ 0 ] \n 
score = ComputedPlanScore . compute ( function , plan ) \n 
row . append ( score [ ] ) \n 
~~ writer . writerow ( row ) \n 
~~ ~~ def getplans ( request ) : \n 
~~ if request . method == : \n 
~~~ page = int ( request . POST . get ( , 1 ) ) \n 
rows = int ( request . POST . get ( , 10 ) ) \n 
sidx = request . POST . get ( , ) \n 
sord = request . POST . get ( , ) \n 
owner_filter = request . POST . get ( ) ; \n 
body_pk = request . POST . get ( ) ; \n 
body_pk = int ( body_pk ) if body_pk else body_pk ; \n 
search = request . POST . get ( , False ) ; \n 
search_string = request . POST . get ( , ) ; \n 
is_community = request . POST . get ( , False ) == ; \n 
~~ end = page * rows \n 
start = end - rows \n 
if owner_filter == : \n 
~~~ available = Q ( is_template = True ) \n 
~~ elif owner_filter == : \n 
~~~ available = Q ( is_shared = True ) \n 
~~~ if request . user . is_anonymous ( ) : \n 
~~~ available = Q ( owner__exact = request . user ) \n 
~~ ~~ elif owner_filter == : \n 
~~~ available = Q ( is_template = True ) | Q ( is_shared = True ) \n 
if not request . user . is_anonymous ( ) : \n 
~~~ available = available | Q ( owner__exact = request . user ) \n 
~~ not_creating = ~ Q ( processing_state = ProcessingState . CREATING ) & ~ Q ( processing_state = ProcessingState \n 
if sidx . startswith ( ) : \n 
~~~ sidx = sidx [ len ( ) : ] \n 
~~ if sidx == : \n 
~~~ sidx = \n 
~~ if sord == : \n 
~~~ sidx = + sidx \n 
~~ if search : \n 
~~~ search_filter = Q ( name__icontains = search_string ) | Q ( description__icontains = search_string ~~ else : \n 
~~~ search_filter = None \n 
~~ if body_pk : \n 
~~~ body_filter = Q ( legislative_body = body_pk ) \n 
all_plans = Plan . objects . filter ( available , not_creating , body_filter , search_filter ) . order_by ~~ else : \n 
~~~ community_filter = Q ( legislative_body__is_community = is_community ) \n 
all_plans = Plan . objects . filter ( available , not_creating , search_filter , community_filter ) . order_by \n 
~~ if all_plans . count ( ) > 0 : \n 
~~~ total_pages = math . ceil ( all_plans . count ( ) / float ( rows ) ) \n 
~~~ total_pages = 1 \n 
~~ plans = all_plans [ start : end ] \n 
plans_list = list ( ) \n 
~~~ plans_list . append ( { \n 
: plan . id , \n 
: plan . name , \n 
: plan . description , \n 
: time . mktime ( plan . edited . timetuple ( ) ) , \n 
: plan . is_template , \n 
: plan . is_shared , \n 
: plan . owner . username , \n 
: can_edit ( request . user , plan ) , \n 
: plan . legislative_body . get_long_description ( ) , \n 
: plan . get_processing_state_display ( ) \n 
~~ def get_shared_districts ( request , planid ) : \n 
~~ all_districts = plan . get_districts_at_version ( plan . version , include_geom = False ) \n 
~~~ plan = None \n 
all_districts = ( ) \n 
~~ if len ( all_districts ) > 0 : \n 
~~~ total_pages = math . ceil ( len ( all_districts ) / float ( rows ) ) \n 
~~ districts = all_districts [ start : end ] \n 
districts_list = list ( ) \n 
~~~ if not district . is_unassigned : \n 
~~~ districts_list . append ( { \n 
: district . id , \n 
: district . short_label , \n 
: district . long_label , \n 
def editplanattributes ( request , planid ) : \n 
~~~ return HttpResponseNotAllowed ( [ ] ) \n 
~~ new_name = request . POST . get ( , None ) \n 
new_description = request . POST . get ( , ) \n 
if not planid or not ( new_name or new_description ) : \n 
~~~ return HttpResponseBadRequest ( \n 
_ ( ) ) \n 
~~ plan = Plan . objects . filter ( pk = planid , owner = request . user ) \n 
if not new_name is None : \n 
~~~ plan . name = new_name \n 
~~ plan . description = new_description \n 
def deleteplan ( request , planid ) : \n 
~~ if not planid : \n 
~~~ return HttpResponseBadRequest ( _ ( ) ) \n 
~~~ plan . delete ( ) \n 
def reaggregateplan ( request , planid ) : \n 
~~~ reaggregate_plan . delay ( plan . id ) \n 
plan . processing_state = ProcessingState . REAGGREGATING \n 
~~ def get_health ( request ) : \n 
~~~ def num_users ( minutes ) : \n 
~~~ users = 0 \n 
for session in Session . objects . all ( ) : \n 
~~~ session . delete ( ) \n 
~~ if in decoded : \n 
~~~ activity_delta = decoded [ ] - timedelta ( 0 , 0 , 0 , 0 , settings . SESSION_TIMEOUT if activity_delta > ( datetime . now ( ) - timedelta ( 0 , 0 , 0 , 0 , minutes ) ) : \n 
~~~ users += 1 \n 
~~ ~~ ~~ return users \n 
~~~ result = _ ( ) % { : datetime . now ( ) } \n 
result += _ ( ) % { : Plan . objects . all ( ) . count ( ) } \n 
result += _ ( ) % { : Session . objects . all ( ) . count ( ) , \n 
: settings . CONCURRENT_SESSIONS } \n 
result += _ ( ) % { : num_users ( 10 ) } \n 
space = os . statvfs ( ) \n 
result += _ ( ) % { : ( ( space . f_bsize * space . f_bavail ) / ( 1024 * 1024 ) ) } \n 
result += _ ( ) % { : commands . getoutput ( ) } \n 
return HttpResponse ( result , mimetype = ) \n 
~~ ~~ def statistics_sets ( request , planid ) : \n 
~~~ result = { : False } \n 
if plan . count ( ) == 0 : \n 
~~~ result [ ] = _ ( ) \n 
return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~~ sets = [ ] \n 
scorefunctions = [ ] \n 
user_functions = ScoreFunction . objects . filter ( selectable_bodies = plan . legislative_body ) . order_by for f in user_functions : \n 
~~~ if not in f . name . lower ( ) and not in f . name . lower ( ) : \n 
~~~ scorefunctions . append ( { : f . id , : force_escape ( f . get_label ( ) ) } ) \n 
~~ ~~ result [ ] = scorefunctions \n 
admin_display_names = [ \n 
"%s_sidebar_demo" % plan . legislative_body . name , \n 
if plan . legislative_body . is_community : \n 
~~~ admin_display_names . append ( "%s_sidebar_comments" % \n 
plan . legislative_body . name ) \n 
~~~ admin_display_names . append ( "%s_sidebar_basic" % \n 
~~ admin_displays = ScoreDisplay . objects . filter ( \n 
owner__is_superuser = True , \n 
legislative_body = plan . legislative_body , \n 
name__in = admin_display_names \n 
for admin_display in admin_displays : \n 
~~~ sets . append ( { \n 
: admin_display . id , \n 
: force_escape ( admin_display . get_label ( ) ) , \n 
: [ ] , \n 
: False \n 
~~~ user_displays = ScoreDisplay . objects . filter ( \n 
owner = request . user , \n 
is_page = False ) . order_by ( ) \n 
result [ ] = len ( user_displays ) \n 
for display in user_displays : \n 
~~~ functions = [ ] \n 
for panel in display . scorepanel_set . all ( ) : \n 
~~~ if panel . type == : \n 
~~~ functions = map ( lambda x : x . id , panel . score_functions . all ( ) ) \n 
if len ( functions ) == 0 : \n 
~~ ~~ ~~ sets . append ( { : display . id , : force_escape ( display . __unicode__ ( ) ) , ~~ ~~ except Exception , ex : \n 
~~~ result [ ] = _ ( ) % { : request . user } \n 
~~ result [ ] = sets \n 
result [ ] = True \n 
~~ elif request . method == and in request . POST : \n 
~~~ display = ScoreDisplay . objects . get ( pk = request . REQUEST . get ( , - 1 ) ) \n 
result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id } \n 
qset = display . scorepanel_set . all ( ) \n 
for panel in qset : \n 
~~~ if panel . displays . count ( ) == 1 : \n 
~~~ panel . delete ( ) \n 
~~ ~~ display . delete ( ) \n 
result [ ] = traceback . format_exc ( ) \n 
~~ ~~ elif request . method == : \n 
~~~ def validate_num ( user , limit = 3 ) : \n 
~~~ return ScoreDisplay . objects . filter ( owner = user , legislative_body = plan . legislative_body , is_page \n 
~~ if in request . POST : \n 
~~~ functions = request . POST . getlist ( ) \n 
functions = map ( lambda x : int ( x ) , functions ) \n 
~~~ display = ScoreDisplay . objects . get ( title = request . POST . get ( ) , owner = request . user display = display . copy_from ( display = display , functions = functions ) \n 
~~~ limit = 3 \n 
if validate_num ( request . user , limit ) : \n 
~~~ demo = ScoreDisplay . objects . filter ( \n 
is_page = False , \n 
title = "Demographics" \n 
for disp in demo : \n 
~~~ has_comments = False \n 
for pnl in disp . scorepanel_set . all ( ) : \n 
~~~ for fn in pnl . score_functions . all ( ) : \n 
~~~ has_comments = has_comments or fn . calculator . endswith ( ) \n 
~~ ~~ if not has_comments : \n 
~~~ demo = disp \n 
~~ ~~ display = ScoreDisplay ( ) \n 
display = display . copy_from ( display = demo , title = request . POST . get ( ) , owner = result [ ] = True \n 
~~~ result [ ] = _ ( \n 
) % { : limit } \n 
result [ ] = \n 
~~ ~~ result [ ] = { : force_escape ( display . __unicode__ ( ) ) , : display . id , result [ ] = True \n 
~~ ~~ return HttpResponse ( json . dumps ( result ) , mimetype = ) \n 
~~ def purge_plan_clear_cache ( district , version ) : \n 
district . plan . purge ( after = version ) \n 
district . plan . version = version \n 
district . plan . save ( ) \n 
cache = district . computeddistrictscore_set . filter ( function__calculator__endswith = ) \n 
cache . delete ( ) \n 
def district_info ( request , planid , district_id ) : \n 
version = plan . version \n 
version = min ( plan . version , int ( version ) ) \n 
~~ ~~ district_id = int ( district_id ) \n 
district = plan . get_districts_at_version ( version , include_geom = False ) \n 
district = filter ( lambda d : d . district_id == district_id , district ) \n 
~~~ district = plan . district_set . get ( id = request . POST [ ] ) \n 
district . short_label = request . POST [ ] [ 0 : 10 ] \n 
district . long_label = request . POST [ ] [ 0 : 256 ] \n 
if district . version < version : \n 
district_copy . version = version \n 
district_copy . save ( ) \n 
district_copy . clone_relations_from ( district ) \n 
district = district_copy \n 
~~~ district . save ( ) \n 
~~ has_comment = in request . POST and request . POST [ ] != \n 
if has_comment : \n 
~~~ ct = ContentType . objects . get ( app_label = , model = ) \n 
Comment . objects . filter ( object_pk = district . id , content_type = ct ) . delete ( ) \n 
comment = Comment ( \n 
~~ tset = Tag . objects . get_for_object ( district ) . filter ( name__startswith = ) \n 
TaggedItem . objects . filter ( tag__in = tset , object_id = district . id ) . delete ( ) \n 
purge_plan_clear_cache ( district , version ) \n 
if len ( request . REQUEST . getlist ( ) ) > 0 : \n 
~~ ~~ status [ ] = version \n 
~~ def plan_feed ( request ) : \n 
~~~ feed = loader . get_template ( ) \n 
plans = Plan . objects . all ( ) . order_by ( ) [ 0 : 10 ] \n 
geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
extent = geolevel . geounit_set . collect ( ) . extent \n 
if extent [ 2 ] - extent [ 0 ] > extent [ 3 ] - extent [ 1 ] : \n 
~~~ width = 500 \n 
height = int ( 500 * ( extent [ 3 ] - extent [ 1 ] ) / ( extent [ 2 ] - extent [ 0 ] ) ) \n 
~~~ width = int ( 500 * ( extent [ 2 ] - extent [ 0 ] ) / ( extent [ 3 ] - extent [ 1 ] ) ) \n 
height = 500 \n 
~~ mapserver = settings . MAP_SERVER if settings . MAP_SERVER != else request . META [ ] \n 
context = { \n 
: plans , \n 
: mapserver , \n 
: extent , \n 
: width , \n 
: height \n 
xml = feed . render ( DjangoContext ( context ) ) \n 
return HttpResponse ( xml , mimetype = ) \n 
~~ def share_feed ( request ) : \n 
plans = Plan . objects . filter ( is_shared = True ) . order_by ( ) [ 0 : 10 ] \n 
if plans . count ( ) < 0 : \n 
~~~ geolevel = plans [ 0 ] . legislative_body . get_geolevels ( ) [ 0 ] \n 
~~~ extent = ( 0 , 0 , 0 , 0 , ) \n 
width = 1 \n 
height = 1 \n 
#------------------------------------------------------------------------------- \n 
~~ DSIZE = 4 \n 
a_offset = 1 * 1024 * 1024 \n 
b_offset = 2 * 1024 * 1024 \n 
iochannel = CoramIoChannel ( idx = 0 , datawidth = 32 ) \n 
channel = CoramChannel ( idx = 0 , datawidth = 8 * DSIZE ) \n 
def st_set_mesh_size ( mesh_size ) : \n 
~~~ channel . write ( mesh_size ) \n 
~~ def st_step ( mesh_size , read_start , write_start ) : \n 
~~~ read_page = 3 \n 
write_page = 0 \n 
read_addr = read_start \n 
mem0 . write ( 0 , read_addr , mesh_size ) \n 
read_addr += mesh_size * DSIZE \n 
mem1 . write ( 0 , read_addr , mesh_size ) \n 
mem2 . write ( 0 , read_addr , mesh_size ) \n 
write_addr = write_start + mesh_size * DSIZE + DSIZE \n 
for i in range ( mesh_size - 2 ) : \n 
~~~ hot_spot = 1 if i == 0 else 0 \n 
pos = ( ( hot_spot << 6 ) | \n 
( ( 0x1 << write_page ) << 4 ) | \n 
( 0x1 << read_page ) ) \n 
mem0 . wait ( ) \n 
mem1 . wait ( ) \n 
mem2 . wait ( ) \n 
mem3 . wait ( ) \n 
channel . write ( pos ) \n 
if read_page == 0 : \n 
~~~ mem0 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 1 : \n 
~~~ mem1 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 2 : \n 
~~~ mem2 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ elif read_page == 3 : \n 
~~~ mem3 . write_nonblocking ( 0 , read_addr , mesh_size ) \n 
~~ read_page = 0 if read_page == 3 else read_page + 1 \n 
channel . read ( ) \n 
mem_d0 . wait ( ) \n 
mem_d1 . wait ( ) \n 
if write_page == 0 : \n 
~~~ mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ elif write_page == 1 : \n 
~~~ mem_d1 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
~~ write_addr += mesh_size * DSIZE \n 
write_page = 0 if write_page == 1 else write_page + 1 \n 
~~ mem_d0 . wait ( ) \n 
~~ def st_computation ( num_iter , mesh_size ) : \n 
~~~ for i in range ( num_iter / 2 ) : \n 
~~~ st_step ( mesh_size , a_offset , b_offset ) \n 
st_step ( mesh_size , b_offset , a_offset ) \n 
~~ ~~ def st_sum ( mesh_size ) : \n 
~~~ check_sum = 0 \n 
read_addr = a_offset \n 
for i in range ( mesh_size ) : \n 
~~~ mem0 . write ( 0 , read_addr , mesh_size ) \n 
init_sum = 1 if i == 0 else 0 \n 
calc_sum = 1 \n 
pos = ( init_sum << 8 ) | ( calc_sum << 7 ) \n 
check_sum = channel . read ( ) \n 
return check_sum \n 
~~ def st_main ( ) : \n 
~~~ global a_offset \n 
global b_offset \n 
mesh_size = iochannel . read ( ) \n 
num_iter = iochannel . read ( ) \n 
a_offset = iochannel . read ( ) \n 
b_offset = iochannel . read ( ) \n 
st_set_mesh_size ( mesh_size ) \n 
st_computation ( num_iter , mesh_size ) \n 
check_sum = st_sum ( mesh_size ) \n 
iochannel . write ( check_sum ) \n 
~~ while True : \n 
~~~ st_main ( ) \n 
~~~ read_page = 0 \n 
pos = hot_spot \n 
~~ read_page = 0 if read_page == 2 else read_page + 1 \n 
mem_d0 . read_nonblocking ( 1 , write_addr , mesh_size - 2 ) \n 
write_addr += mesh_size * DSIZE \n 
~~ ~~ def st_computation ( num_iter , mesh_size ) : \n 
pos = ( init_sum << 2 ) | ( calc_sum << 1 ) \n 
~~ from __future__ import absolute_import \n 
def getRamId ( oid , sid ) : \n 
~~~ if 0 <= sid and sid <= 31 : \n 
~~ if 32 <= sid and sid <= 63 : \n 
~~~ return 1 \n 
~~ if 64 <= sid and sid <= 95 : \n 
~~~ return 2 \n 
~~ if 96 <= sid and sid <= 127 : \n 
~~~ return 3 \n 
~~ ~~ def getRamSubId ( oid , sid ) : \n 
~~~ return sid \n 
~~~ return sid - 32 \n 
~~~ return sid - 64 \n 
~~~ return sid - 96 \n 
~~ ~~ def getChannelId ( oid , sid ) : \n 
~~~ return oid \n 
~~ def getChannelSubId ( oid , sid ) : \n 
~~ def getRegisterId ( oid , sid ) : \n 
~~ def getRegisterSubId ( oid , sid ) : \n 
~~~ f = open ( sys . argv [ 1 ] , ) \n 
lines = f . readlines ( ) \n 
output = [ ] \n 
p_thread = re . compile ( ) \n 
p_thread_id = re . compile ( ) \n 
p_object_id = re . compile ( ) \n 
p_width = re . compile ( ) \n 
p_depth = re . compile ( ) \n 
p_indexwidth = re . compile ( ) \n 
p_logdepth = re . compile ( ) \n 
p_sub_id = re . compile ( ) \n 
module_name = None \n 
thread_name = None \n 
thread_id = None \n 
object_id = None \n 
sub_id = None \n 
width = None \n 
indexwidth = None \n 
depth = None \n 
mode = False \n 
sub_id_num = None \n 
sub_id_base = None \n 
buffer = [ ] \n 
for line in lines : \n 
~~~ if not mode : \n 
~~~ m = p_thread . match ( line ) \n 
if m : \n 
~~~ thread_name = re . match ( \'.*(".*").*\' , m . group ( 2 ) ) . group ( 1 ) \n 
module_name = re . search ( , line ) . group ( 1 ) \n 
mode = True \n 
buffer . append ( line ) \n 
~~~ m = p_thread_id . match ( line ) \n 
~~~ tid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
thread_id = re . match ( , tid_str ) . group ( 2 ) \n 
~~ m = p_object_id . match ( line ) \n 
~~~ oid_str = m . group ( 2 ) [ 1 : - 1 ] \n 
object_id = re . match ( , oid_str ) . group ( 2 ) \n 
~~ m = p_width . match ( line ) \n 
~~~ width_str = m . group ( 2 ) \n 
width = re . match ( , width_str ) . group ( 1 ) \n 
~~ m = p_depth . match ( line ) \n 
~~~ depth_str = m . group ( 2 ) \n 
depth = re . match ( , depth_str ) . group ( 1 ) \n 
~~ m = p_indexwidth . match ( line ) \n 
~~~ indexwidth_str = m . group ( 2 ) \n 
indexwidth = re . match ( , indexwidth_str ) . group ( 1 ) \n 
~~ m = p_logdepth . match ( line ) \n 
~~~ logdepth_str = m . group ( 2 ) \n 
logdepth = re . match ( , logdepth_str ) . group ( 1 ) \n 
~~ m = p_sub_id . match ( line ) \n 
~~~ sid_str = m . group ( 2 ) \n 
sub_id_m = re . search ( , sid_str ) \n 
sub_id = sub_id_m . group ( 0 ) \n 
sub_id_num = sub_id_m . group ( 2 ) \n 
sub_id_base = ( 10 if sub_id_m . group ( 1 ) . count ( "\'d" ) > 0 else \n 
16 if sub_id_m . group ( 1 ) . count ( "\'h" ) > 0 else \n 
2 if sub_id_m . group ( 1 ) . count ( "\'b" ) > 0 else \n 
10 ) \n 
~~ ~~ if mode : \n 
if module_name . count ( ) > 0 : \n 
~~ if module_name . count ( ) > 0 : \n 
print ( . join ( buffer [ 1 : ] ) ) \n 
~~ mode = False \n 
print ( line , end = ) \n 
~~ ~~ main ( ) \n 
from optparse import OptionParser \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) \n 
import pyverilog . utils . version \n 
from pyverilog . dataflow . dataflow_analyzer import VerilogDataflowAnalyzer \n 
VERSION = pyverilog . utils . version . VERSION \n 
def showVersion ( ) : \n 
~~~ print ( INFO ) \n 
print ( VERSION ) \n 
print ( USAGE ) \n 
sys . exit ( ) \n 
~~ optparser = OptionParser ( ) \n 
optparser . add_option ( "-v" , "--version" , action = "store_true" , dest = "showversion" , \n 
optparser . add_option ( "-I" , "--include" , dest = "include" , action = "append" , \n 
optparser . add_option ( "-D" , dest = "define" , action = "append" , \n 
optparser . add_option ( "-t" , "--top" , dest = "topmodule" , \n 
optparser . add_option ( "--nobind" , action = "store_true" , dest = "nobind" , \n 
optparser . add_option ( "--noreorder" , action = "store_true" , dest = "noreorder" , \n 
( options , args ) = optparser . parse_args ( ) \n 
filelist = args \n 
if options . showversion : \n 
~~~ showVersion ( ) \n 
~~ for f in filelist : \n 
~~ if len ( filelist ) == 0 : \n 
~~ analyzer = VerilogDataflowAnalyzer ( filelist , options . topmodule , \n 
noreorder = options . noreorder , \n 
nobind = options . nobind , \n 
preprocess_include = options . include , \n 
preprocess_define = options . define ) \n 
analyzer . generate ( ) \n 
directives = analyzer . get_directives ( ) \n 
for dr in sorted ( directives , key = lambda x : str ( x ) ) : \n 
~~~ print ( dr ) \n 
~~ instances = analyzer . getInstances ( ) \n 
for module , instname in sorted ( instances , key = lambda x : str ( x [ 1 ] ) ) : \n 
~~~ print ( ( module , instname ) ) \n 
~~ if options . nobind : \n 
signals = analyzer . getSignals ( ) \n 
for sig in signals : \n 
~~~ print ( sig ) \n 
consts = analyzer . getConsts ( ) \n 
for con in consts : \n 
~~~ print ( con ) \n 
~~~ terms = analyzer . getTerms ( ) \n 
for tk , tv in sorted ( terms . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ print ( tv . tostr ( ) ) \n 
~~ binddict = analyzer . getBinddict ( ) \n 
for bk , bv in sorted ( binddict . items ( ) , key = lambda x : str ( x [ 0 ] ) ) : \n 
~~~ for bvi in bv : \n 
~~~ print ( bvi . tostr ( ) ) \n 
~~ ~~ ~~ ~~ if __name__ == : \n 
from pyverilog . dataflow . dataflow import * \n 
def replaceUndefined ( tree , termname ) : \n 
~~~ if tree is None : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFUndefined ) : return DFTerminal ( termname ) \n 
if isinstance ( tree , DFConstant ) : return tree \n 
if isinstance ( tree , DFEvalValue ) : return tree \n 
if isinstance ( tree , DFTerminal ) : return tree \n 
if isinstance ( tree , DFBranch ) : \n 
~~~ condnode = replaceUndefined ( tree . condnode , termname ) \n 
truenode = replaceUndefined ( tree . truenode , termname ) \n 
falsenode = replaceUndefined ( tree . falsenode , termname ) \n 
return DFBranch ( condnode , truenode , falsenode ) \n 
~~ if isinstance ( tree , DFOperator ) : \n 
~~~ nextnodes = [ ] \n 
for n in tree . nextnodes : \n 
~~~ nextnodes . append ( replaceUndefined ( n , termname ) ) \n 
~~ return DFOperator ( tuple ( nextnodes ) , tree . operator ) \n 
~~ if isinstance ( tree , DFPartselect ) : \n 
~~~ msb = replaceUndefined ( tree . msb , termname ) \n 
lsb = replaceUndefined ( tree . lsb , termname ) \n 
var = replaceUndefined ( tree . var , termname ) \n 
return DFPartselect ( var , msb , lsb ) \n 
~~ if isinstance ( tree , DFPointer ) : \n 
~~~ ptr = replaceUndefined ( tree . ptr , termname ) \n 
return DFPointer ( var , ptr ) \n 
~~ if isinstance ( tree , DFConcat ) : \n 
~~ return DFConcat ( tuple ( nextnodes ) ) \n 
~~ raise DefinitionError ( % ( str ( type ( tree ) ) , str ( tree ) ) ) \n 
from pyverilog . dataflow . optimizer import VerilogDataflowOptimizer \n 
from pyverilog . controlflow . controlflow_analyzer import VerilogControlflowAnalyzer \n 
codedir = os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) + \n 
def test ( ) : \n 
~~~ filelist = [ codedir + ] \n 
topmodule = \n 
noreorder = False \n 
nobind = False \n 
include = None \n 
define = None \n 
analyzer = VerilogDataflowAnalyzer ( filelist , topmodule , \n 
noreorder = noreorder , \n 
nobind = nobind , \n 
preprocess_include = include , \n 
preprocess_define = define ) \n 
instances = analyzer . getInstances ( ) \n 
terms = analyzer . getTerms ( ) \n 
binddict = analyzer . getBinddict ( ) \n 
optimizer = VerilogDataflowOptimizer ( terms , binddict ) \n 
optimizer . resolveConstant ( ) \n 
c_analyzer = VerilogControlflowAnalyzer ( topmodule , terms , \n 
binddict , \n 
resolved_terms = optimizer . getResolvedTerms ( ) , \n 
resolved_binddict = optimizer . getResolvedBinddict ( ) , \n 
constlist = optimizer . getConstlist ( ) \n 
for tk in sorted ( c_analyzer . resolved_terms . keys ( ) , key = lambda x : str ( x ) ) : \n 
~~~ tree = c_analyzer . makeTree ( tk ) \n 
output . append ( str ( tk ) + + tree . tocode ( ) ) \n 
~~ rslt = . join ( output ) + \n 
print ( rslt ) \n 
assert ( expected == rslt ) \n 
~~~ test ( ) \n 
import dataflow_example \n 
~~~ test_module = dataflow_example . mkTest ( ) \n 
code = test_module . to_verilog ( ) \n 
from pyverilog . vparser . parser import VerilogParser \n 
from pyverilog . ast_code_generator . codegen import ASTCodeGenerator \n 
parser = VerilogParser ( ) \n 
expected_ast = parser . parse ( expected_verilog ) \n 
codegen = ASTCodeGenerator ( ) \n 
expected_code = codegen . visit ( expected_ast ) \n 
assert ( expected_code == code ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ ) ) ) ) ) \n 
from veriloggen import * \n 
def mkLed ( ) : \n 
~~~ m = Module ( ) \n 
interval = m . Parameter ( , 16 ) \n 
clk = m . Input ( ) \n 
rst = m . Input ( ) \n 
led = m . OutputReg ( , 8 , initval = 0 ) \n 
count = m . Reg ( , 32 , initval = 0 ) \n 
seq = Seq ( m , , clk , rst ) \n 
seq . add ( Systask ( , , led , count ) ) \n 
seq . add ( count ( count + 1 ) , cond = count < interval - 1 ) \n 
seq . add ( count ( 0 ) , cond = count == interval - 1 ) \n 
seq . add ( led ( led + 1 ) , cond = count == interval - 1 ) \n 
seq . make_always ( ) \n 
return m \n 
~~ def mkTest ( ) : \n 
led = mkLed ( ) \n 
params = m . copy_params ( led ) \n 
ports = m . copy_sim_ports ( led ) \n 
clk = ports [ ] \n 
rst = ports [ ] \n 
uut = m . Instance ( led , , \n 
params = m . connect_params ( led ) , \n 
ports = m . connect_ports ( led ) ) \n 
simulation . setup_clock ( m , clk , hperiod = 5 ) \n 
init = simulation . setup_reset ( m , rst , m . make_reset ( ) , period = 100 ) \n 
init . add ( \n 
Delay ( 1000 ) , \n 
Systask ( ) , \n 
~~~ test = mkTest ( ) \n 
verilog = test . to_verilog ( ) \n 
print ( verilog ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . abspath ( __file__ \n 
width = m . Parameter ( , 8 ) \n 
led = m . OutputReg ( , width ) \n 
count = m . Reg ( , 32 ) \n 
m . Always ( Posedge ( clk ) ) ( \n 
If ( rst ) ( \n 
count ( 0 ) \n 
) . Else ( \n 
count ( Cond ( count == 1023 , 0 , count + 1 ) ) \n 
led ( 0 ) \n 
led ( Cond ( count == 1024 - 1 , led + 1 , led ) ) \n 
~~~ led = mkLed ( ) \n 
verilog = led . to_verilog ( ) \n 
sys . path . insert ( 0 , os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os . path . dirname ( os \n 
def mkSub ( ) : \n 
count = m . OutputReg ( , 32 ) \n 
If ( count == 1023 ) ( \n 
count ( count + 1 ) \n 
~~ def mkLed ( ) : \n 
count = m . Wire ( , 32 ) \n 
sub = mkSub ( ) \n 
m . Instance ( sub , , m . connect_params ( sub ) , m . connect_ports ( sub ) ) \n 
led ( led + 1 ) \n 
inst_sub = m . Reg ( , 32 ) \n 
~~ except ValueError as e : \n 
~~~ print ( e . args [ 0 ] ) \n 
#print(verilog) \n 
If ( count == 1024 - 1 ) ( \n 
led ( led + 1 ) , \n 
SingleStatement ( SystemTask ( , , led ) ) \n 
import veriloggen . dataflow as dataflow \n 
def mkMain ( ) : \n 
~~~ x = dataflow . Variable ( , valid = , ready = , point = 8 ) \n 
y = dataflow . Variable ( , valid = , ready = , point = 4 ) \n 
z = x * y \n 
z . output ( , valid = , ready = ) \n 
df = dataflow . Dataflow ( z ) \n 
m = df . to_module ( ) \n 
main = mkMain ( ) \n 
params = m . copy_params ( main ) \n 
ports = m . copy_sim_ports ( main ) \n 
xdata = ports [ ] \n 
xvalid = ports [ ] \n 
xready = ports [ ] \n 
ydata = ports [ ] \n 
yvalid = ports [ ] \n 
yready = ports [ ] \n 
zdata = ports [ ] \n 
zvalid = ports [ ] \n 
zready = ports [ ] \n 
xdata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
ydata_orig = m . RegLike ( ports [ ] , name = , initval = 0 ) \n 
zdata_orig = m . WireLike ( ports [ ] , name = ) \n 
m . Always ( ) ( xdata ( fixed . to_fixed ( xdata_orig , 8 ) ) ) \n 
m . Always ( ) ( ydata ( fixed . to_fixed ( ydata_orig , 4 ) ) ) \n 
m . Assign ( zdata_orig ( fixed . fixed_to_int ( zdata , 8 ) ) ) \n 
uut = m . Instance ( main , , \n 
params = m . connect_params ( main ) , \n 
ports = m . connect_ports ( main ) ) \n 
reset_done = m . Reg ( , initval = 0 ) \n 
reset_stmt = [ ] \n 
reset_stmt . append ( reset_done ( 0 ) ) \n 
reset_stmt . append ( xdata ( 0 ) ) \n 
reset_stmt . append ( xvalid ( 0 ) ) \n 
reset_stmt . append ( ydata ( 0 ) ) \n 
reset_stmt . append ( yvalid ( 0 ) ) \n 
reset_stmt . append ( zready ( 0 ) ) \n 
reset_stmt . append ( xdata_orig ( 0 ) ) \n 
reset_stmt . append ( ydata_orig ( 0 ) ) \n 
simulation . setup_waveform ( m , uut , xdata_orig , ydata_orig , zdata_orig ) \n 
init = simulation . setup_reset ( m , rst , reset_stmt , period = 100 ) \n 
nclk = simulation . next_clock \n 
reset_done ( 1 ) , \n 
nclk ( clk ) , \n 
Delay ( 10000 ) , \n 
def send ( name , data , valid , ready , step = 1 , waitnum = 10 ) : \n 
~~~ fsm = FSM ( m , name + , clk , rst ) \n 
count = m . TmpReg ( 32 , initval = 0 ) \n 
fsm . add ( valid ( 0 ) ) \n 
fsm . goto_next ( cond = reset_done ) \n 
for _ in range ( waitnum ) : \n 
~~~ fsm . goto_next ( ) \n 
~~ fsm . add ( valid ( 1 ) ) \n 
fsm . goto_next ( ) \n 
fsm . add ( data ( data + step ) , cond = ready ) \n 
fsm . add ( count . inc ( ) , cond = ready ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 5 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 5 , ready ) ) \n 
fsm . add ( valid ( 0 ) , cond = AndList ( count == 10 , ready ) ) \n 
fsm . goto_next ( cond = AndList ( count == 10 , ready ) ) \n 
fsm . make_always ( ) \n 
~~ def receive ( name , data , valid , ready , waitnum = 10 ) : \n 
fsm . add ( ready ( 0 ) ) \n 
yinit = fsm . current ( ) \n 
fsm . add ( ready ( 1 ) , cond = valid ) \n 
fsm . goto_next ( cond = valid ) \n 
for i in range ( waitnum ) : \n 
~~~ fsm . add ( ready ( 0 ) ) \n 
~~ fsm . goto ( yinit ) \n 
~~ send ( , xdata_orig , xvalid , xready , step = 1 , waitnum = 10 ) \n 
send ( , ydata_orig , yvalid , yready , step = 1 , waitnum = 20 ) \n 
receive ( , zdata , zvalid , zready , waitnum = 50 ) \n 
If ( reset_done ) ( \n 
If ( AndList ( xvalid , xready ) ) ( \n 
Systask ( , , xdata_orig ) \n 
If ( AndList ( yvalid , yready ) ) ( \n 
Systask ( , , ydata_orig ) \n 
If ( AndList ( zvalid , zready ) ) ( \n 
Systask ( , , zdata_orig ) \n 
sim = simulation . Simulator ( test ) \n 
#sim.view_waveform(background=True) \n 
import dataflow_mul \n 
~~~ test_module = dataflow_mul . mkTest ( ) \n 
valid = m . OutputReg ( , initval = 0 ) \n 
count = m . Reg ( , width = 32 , initval = 0 ) \n 
up = m . Wire ( ) \n 
down = m . Wire ( ) \n 
m . Assign ( up ( 1 ) ) \n 
m . Assign ( down ( 0 ) ) \n 
fsm = FSM ( m , , clk , rst ) \n 
for i in range ( 4 ) : \n 
~~ c = count >= 16 \n 
fsm . add ( valid ( up ) , cond = c , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . goto_next ( cond = c ) \n 
for i in range ( 8 ) : \n 
~~ c = count >= 32 \n 
~~~ fsm . add ( valid ( up ) , cond = c , delay = 1 , keep = 3 , eager_val = True , lazy_cond = True ) \n 
fsm . add ( valid ( down ) , cond = c , delay = 4 , eager_val = True , lazy_cond = True ) \n 
~~ fsm . make_always ( reset = [ count . reset ( ) ] , body = [ count ( count + 1 ) ] ) \n 
clk = m . Reg ( ) \n 
rst = m . Reg ( ) \n 
valid = m . Wire ( ) \n 
uut = m . Instance ( mkLed ( ) , , \n 
ports = ( ( , clk ) , ( , rst ) , ( , valid ) ) ) \n 
simulation . setup_waveform ( m , uut ) \n 
init = simulation . setup_reset ( m , rst , period = 100 ) \n 
import pipeline_draw_graph \n 
~~~ test_module = pipeline_draw_graph . mkTest ( ) \n 
import read_verilog_module_str \n 
~~~ test_module = read_verilog_module_str . mkTop ( ) \n 
import veriloggen . core . vtypes as vtypes \n 
import veriloggen . core . module as module \n 
def mkMultiplierCore ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ retwidth = lwidth + rwidth \n 
m = module . Module ( % index ) \n 
update = m . Input ( ) \n 
a = m . Input ( , lwidth ) \n 
b = m . Input ( , rwidth ) \n 
c = m . Output ( , retwidth ) \n 
_a = m . Reg ( , lwidth , signed = lsigned ) \n 
_b = m . Reg ( , rwidth , signed = rsigned ) \n 
tmpval = [ m . Reg ( % i , retwidth , signed = True ) for i in range ( depth - 1 ) ] \n 
rslt = m . Wire ( , retwidth , signed = True ) \n 
__a = _a \n 
__b = _b \n 
if not lsigned : \n 
~~~ __a = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _a ) ) \n 
~~ if not rsigned : \n 
~~~ __b = vtypes . SystemTask ( , vtypes . Cat ( vtypes . Int ( 0 , width = 1 ) , _b ) ) \n 
~~ m . Assign ( rslt ( __a * __b ) ) \n 
m . Assign ( c ( tmpval [ depth - 2 ] ) ) \n 
m . Always ( vtypes . Posedge ( clk ) ) ( \n 
vtypes . If ( update ) ( \n 
_a ( a ) , \n 
_b ( b ) , \n 
tmpval [ 0 ] ( rslt ) , \n 
[ tmpval [ i ] ( tmpval [ i - 1 ] ) for i in range ( 1 , depth - 1 ) ] \n 
~~ def mkMultiplier ( index , lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
retwidth = lwidth + rwidth \n 
mult = mkMultiplierCore ( index , lwidth , rwidth , lsigned , rsigned , depth ) \n 
enable = m . Input ( ) \n 
valid = m . Output ( ) \n 
valid_reg = [ m . Reg ( % i ) for i in range ( depth ) ] \n 
m . Assign ( valid ( valid_reg [ depth - 1 ] ) ) \n 
vtypes . If ( rst ) ( \n 
[ valid_reg [ i ] ( 0 ) for i in range ( depth ) ] \n 
valid_reg [ 0 ] ( enable ) , \n 
[ valid_reg [ i ] ( valid_reg [ i - 1 ] ) for i in range ( 1 , depth ) ] \n 
ports = [ ( , clk ) , ( , update ) , ( , a ) , ( , b ) , ( , c ) ] \n 
m . Instance ( mult , , ports = ports ) \n 
~~ index_count = 0 \n 
def get_mul ( lwidth = 32 , rwidth = 32 , lsigned = True , rsigned = True , depth = 6 ) : \n 
~~~ global index_count \n 
mul = mkMultiplier ( index_count , lwidth , rwidth , lsigned , rsigned , depth ) \n 
index_count += 1 \n 
return mul \n 
~~ def reset ( ) : \n 
index_count = 0 \n 
from pyramid import testing \n 
import mock \n 
class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ self . request = testing . DummyRequest ( ) \n 
self . config = testing . setUp ( request = self . request ) \n 
~~~ testing . tearDown ( ) \n 
~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import acl_modified \n 
return acl_modified ( event ) \n 
~~ @ mock . patch ( ) \n 
def test_it ( self , mock_get_auditlog ) : \n 
~~~ from substanced . audit import AuditLog \n 
self . request . user = Dummy ( { : 1 , : } ) \n 
event = Dummy ( ) \n 
context = testing . DummyResource ( ) \n 
auditlog = AuditLog ( ) \n 
mock_get_auditlog . side_effect = lambda c : auditlog \n 
context . __oid__ = 5 \n 
event . registry = _makeRegistry ( ) \n 
event . object = context \n 
event . old_acl = \n 
event . new_acl = \n 
self . _callFUT ( event ) \n 
self . assertEqual ( len ( auditlog ) , 1 ) \n 
entries = list ( auditlog . entries ) \n 
entry = entries [ 0 ] \n 
self . assertEqual ( entry [ 0 ] , 0 ) \n 
self . assertEqual ( entry [ 1 ] , 0 ) \n 
self . assertEqual ( entry [ 2 ] . name , ) \n 
self . assertEqual ( entry [ 2 ] . oid , 5 ) \n 
self . assertEqual ( \n 
json . loads ( entry [ 2 ] . payload ) , \n 
: entry [ 2 ] . timestamp , \n 
: { : 1 , : } , \n 
def test_it_nolog ( self , mock_get_auditlog ) : \n 
~~~ mock_get_auditlog . side_effect = lambda c : None \n 
self . assertEqual ( self . _callFUT ( event ) , None ) \n 
~~ ~~ _marker = object ( ) \n 
class Test_content_added_moved_or_duplicated ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_added_moved_or_duplicated \n 
return content_added_moved_or_duplicated ( event ) \n 
def test_it_added ( self , mock_get_auditlog ) : \n 
~~~ auditlog = _makeAuditLog ( ) \n 
event = _makeEvent ( ) \n 
self . assertEqual ( entry [ 2 ] . oid , 10 ) \n 
: 5 \n 
def test_it_added_noscribe ( self , mock_get_auditlog ) : \n 
def test_it_moved ( self , mock_get_auditlog ) : \n 
event . moving = True \n 
event . duplicating = None \n 
def test_it_duplicated ( self , mock_get_auditlog ) : \n 
event . moving = None \n 
event . duplicating = True \n 
~~ ~~ class Test_content_removed ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_removed \n 
return content_removed ( event ) \n 
~~ def test_it_moving ( self ) : \n 
~~~ event = Dummy ( ) \n 
~~ ~~ class Test_content_modified ( unittest . TestCase ) : \n 
~~~ from . . subscribers import content_modified \n 
return content_modified ( event ) \n 
def test_it_noscribe ( self , mock_get_auditlog ) : \n 
~~ ~~ class Test_logged_in ( unittest . TestCase ) : \n 
~~~ from . . subscribers import logged_in \n 
return logged_in ( event ) \n 
event . request = Dummy ( ) \n 
event . request . context = context \n 
def test_it_user_has_oid ( self , mock_get_auditlog ) : \n 
user = Dummy ( ) \n 
user . __oid__ = 5 \n 
event . user = user \n 
event . login = \n 
self . assertEqual ( entry [ 2 ] . oid , None ) \n 
def test_it_user_has_no_oid ( self , mock_get_auditlog ) : \n 
: None , \n 
~~ ~~ class Test_root_added ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event ) : \n 
~~~ from . . subscribers import root_added \n 
return root_added ( event ) \n 
def test_it ( self , mock_set_auditlog ) : \n 
root = Dummy ( ) \n 
def is_set ( _root ) : \n 
~~~ self . assertEqual ( _root , root ) \n 
~~ mock_set_auditlog . side_effect = is_set \n 
event . object = root \n 
~~ ~~ class Dummy ( object ) : \n 
~~~ def __init__ ( self , kw = None ) : \n 
~~~ if kw : \n 
~~~ self . __dict__ . update ( kw ) \n 
~~ ~~ ~~ class DummyContentRegistry ( object ) : \n 
~~~ def typeof ( self , content ) : \n 
~~ ~~ def _makeAuditLog ( ) : \n 
return auditlog \n 
~~ def _makeRegistry ( ) : \n 
~~~ registry = Dummy ( ) \n 
registry . content = DummyContentRegistry ( ) \n 
return registry \n 
~~ def _makeEvent ( ) : \n 
event . parent = testing . DummyResource ( ) \n 
event . parent . __oid__ = 10 \n 
event . name = \n 
context . __parent__ = event . parent \n 
return event \n 
class Test_root_factory ( unittest . TestCase ) : \n 
~~~ self . config = testing . setUp ( ) \n 
~~ def _callFUT ( self , request , transaction , get_connection , evolve_packages ) : \n 
~~~ from . . import root_factory \n 
return root_factory ( request , transaction , get_connection , \n 
evolve_packages ) \n 
~~ def _makeRequest ( self , app_root = None ) : \n 
~~~ request = Dummy ( ) \n 
request . registry = DummyRegistry ( ) \n 
request . registry . content = Dummy ( ) \n 
request . registry . content . create = lambda * arg : app_root \n 
return request \n 
~~ def test_without_app_root ( self ) : \n 
~~~ txn = DummyTransaction ( ) \n 
root = { } \n 
gc = Dummy_get_connection ( root ) \n 
ep = DummyFunction ( True ) \n 
app_root = object ( ) \n 
request = self . _makeRequest ( app_root ) \n 
result = self . _callFUT ( request , txn , gc , ep ) \n 
self . assertEqual ( result , app_root ) \n 
self . assertTrue ( txn . committed ) \n 
self . assertTrue ( txn . savepointed ) \n 
self . assertTrue ( ep . called ) \n 
~~ def test_with_app_root ( self ) : \n 
root = { : app_root } \n 
request = testing . DummyRequest ( ) \n 
self . assertFalse ( txn . committed ) \n 
~~ ~~ class Test_includeme ( unittest . TestCase ) : \n 
~~~ def test_it ( self ) : \n 
~~~ from . . import ( \n 
includeme , \n 
connection_opened , \n 
connection_will_close , \n 
ZODBConnectionOpened , \n 
ZODBConnectionWillClose , \n 
config = DummyConfig ( ) \n 
includeme ( config ) \n 
config . subscriptions , \n 
[ ( connection_opened , ZODBConnectionOpened ) , \n 
( connection_will_close , ZODBConnectionWillClose ) , \n 
~~ ~~ class Test_connection_opened ( unittest . TestCase ) : \n 
~~~ from . . import connection_opened \n 
event = DummyEvent ( ) \n 
connection_opened ( event ) \n 
self . assertEqual ( event . request . _zodb_tx_counts , ( 0 , 0 ) ) \n 
~~ ~~ class Test_connection_will_close ( unittest . TestCase ) : \n 
~~~ def _callFUT ( self , event , statsd_incr ) : \n 
~~~ from . . import connection_will_close \n 
return connection_will_close ( event , statsd_incr ) \n 
~~ def test_no_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( ) \n 
result = self . _callFUT ( event , None ) \n 
~~ def test_with_postitive_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 5 , 5 ) \n 
event . request . _zodb_tx_counts = ( 1 , 1 ) \n 
L = [ ] \n 
def statsd_incr ( name , num , registry = None ) : \n 
~~~ L . append ( ( name , num ) ) \n 
~~ self . _callFUT ( event , statsd_incr ) \n 
L , \n 
[ ( , 4 ) , ( , 4 ) ] \n 
~~ def test_with_zero_tx_counts ( self ) : \n 
~~~ event = DummyEvent ( 1 , 1 ) \n 
self . _callFUT ( event , None ) \n 
[ ] \n 
~~ ~~ class DummyTransaction ( object ) : \n 
~~~ committed = False \n 
savepointed = False \n 
def commit ( self ) : \n 
~~~ self . committed = True \n 
~~ def savepoint ( self ) : \n 
~~~ self . savepointed = True \n 
~~ ~~ class Dummy_get_connection ( object ) : \n 
~~~ def __init__ ( self , root ) : \n 
~~~ self . _root = root \n 
~~ def root ( self ) : \n 
~~~ return self . _root \n 
~~ def __call__ ( self , request ) : \n 
~~ ~~ class DummyFunction ( object ) : \n 
~~~ called = False \n 
def __init__ ( self , result ) : \n 
~~~ self . result = result \n 
~~ def __call__ ( self , * args , ** kw ) : \n 
~~~ self . called = True \n 
self . args = args \n 
self . kw = kw \n 
return self . result \n 
~~ class DummyRegistry ( object ) : \n 
~~~ def notify ( self , event ) : \n 
~~~ self . event = event \n 
~~ ~~ class DummyConfig ( object ) : \n 
~~~ self . subscriptions = [ ] \n 
~~ def add_subscriber ( self , fn , event_type ) : \n 
~~~ self . subscriptions . append ( ( fn , event_type ) ) \n 
~~ ~~ class DummyConnection ( object ) : \n 
~~~ def __init__ ( self , loads , stores ) : \n 
~~~ self . loads = loads \n 
self . stores = stores \n 
~~ def getTransferCounts ( self ) : \n 
~~~ return ( self . loads , self . stores ) \n 
~~ ~~ class DummyEvent ( object ) : \n 
~~~ def __init__ ( self , loads = 0 , stores = 0 ) : \n 
self . conn = DummyConnection ( loads , stores ) \n 
~~ ~~ import pkg_resources \n 
import mimetypes \n 
import colander \n 
import deform . schema \n 
from pyramid . httpexceptions import HTTPFound \n 
from pyramid . response import Response \n 
from pyramid . security import NO_PERMISSION_REQUIRED \n 
from . . form import FormView \n 
from . . file import ( \n 
FilePropertiesSchema , \n 
FileUploadTempStore , \n 
file_upload_widget , \n 
file_name_node , \n 
USE_MAGIC , \n 
from . . interfaces import ( \n 
IFile , \n 
IFolder , \n 
from . . sdi import mgmt_view \n 
@ mgmt_view ( \n 
context = IFile , \n 
permission = , \n 
tab_condition = False , \n 
http_cache = 0 , \n 
def view_file ( context , request ) : \n 
~~~ return context . get_response ( request = request ) \n 
~~ @ mgmt_view ( \n 
tab_title = , \n 
permission = \n 
def view_tab ( context , request ) : \n 
~~~ return HTTPFound ( location = request . sdiapi . mgmt_path ( context ) ) \n 
~~ class AddFileSchema ( FilePropertiesSchema ) : \n 
~~~ file = colander . SchemaNode ( \n 
deform . schema . FileData ( ) , \n 
widget = file_upload_widget , \n 
missing = colander . null , \n 
~~ @ colander . deferred \n 
def name_or_file ( node , kw ) : \n 
~~~ def _name_or_file ( node , struct ) : \n 
~~~ if not struct [ ] and not struct [ ] : \n 
~~~ raise colander . Invalid ( node , ) \n 
~~ if not struct [ ] : \n 
~~~ filename = struct [ ] . get ( ) \n 
if filename : \n 
~~~ name_node = file_name_node . bind ( \n 
context = kw [ ] , request = kw [ ] \n 
name_node . validator ( node [ ] , filename ) \n 
~~~ raise colander . Invalid ( \n 
node , \n 
~~ ~~ ~~ return _name_or_file \n 
context = IFolder , \n 
renderer = , \n 
addable_content = , \n 
tab_condition = False \n 
class AddFileView ( FormView ) : \n 
~~~ title = \n 
schema = AddFileSchema ( validator = name_or_file ) . clone ( ) \n 
schema [ ] . missing = colander . null \n 
buttons = ( , ) \n 
def _makeob ( self , stream , title , mimetype ) : \n 
~~~ return self . request . registry . content . create ( \n 
stream = stream , \n 
mimetype = mimetype , \n 
title = title , \n 
~~ def add_success ( self , appstruct ) : \n 
~~~ name = appstruct [ ] \n 
title = appstruct [ ] or None \n 
filedata = appstruct [ ] \n 
mimetype = appstruct [ ] or USE_MAGIC \n 
stream = None \n 
filename = None \n 
if filedata : \n 
~~~ filename = filedata [ ] \n 
stream = filedata [ ] \n 
if stream : \n 
~~~ stream . seek ( 0 ) \n 
~~~ stream = None \n 
~~ ~~ name = name or filename \n 
fileob = self . _makeob ( stream , title , mimetype ) \n 
self . context [ name ] = fileob \n 
tmpstore = FileUploadTempStore ( self . request ) \n 
tmpstore . clear ( ) \n 
return HTTPFound ( self . request . sdiapi . mgmt_path ( self . context ) ) \n 
~~ ~~ onepixel = pkg_resources . resource_filename ( \n 
permission = NO_PERMISSION_REQUIRED \n 
def preview_image_upload ( request ) : \n 
~~~ uid = request . subpath [ 0 ] \n 
tempstore = FileUploadTempStore ( request ) \n 
filedata = tempstore . get ( uid , { } ) \n 
fp = filedata . get ( ) \n 
if fp is not None : \n 
~~~ fp . seek ( 0 ) \n 
filename = filedata [ ] \n 
~~ mimetype = mimetypes . guess_type ( filename , strict = False ) [ 0 ] \n 
if not mimetype or not mimetype . startswith ( ) : \n 
~~~ mimetype = \n 
fp = open ( onepixel , ) \n 
~~ response = Response ( content_type = mimetype , app_iter = fp ) \n 
class Test_principal_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import principal_added \n 
return principal_added ( event ) \n 
~~ def test_event_wo_loading_attr ( self ) : \n 
~~~ event = testing . DummyResource ( ) \n 
event . object = testing . DummyResource ( ) \n 
self . assertRaises ( AttributeError , self . _callFUT , event ) \n 
~~ def test_event_w_loading_True ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True ) \n 
result = self . _callFUT ( event ) \n 
self . assertEqual ( result , None ) \n 
~~ def test_wo_principals_service ( self ) : \n 
~~~ from zope . interface import directlyProvides \n 
from ... interfaces import IFolder \n 
event = testing . DummyResource ( loading = False ) \n 
root = testing . DummyResource ( ) \n 
directlyProvides ( root , IFolder ) \n 
event . object = root [ ] = testing . DummyResource ( ) \n 
self . assertRaises ( ValueError , self . _callFUT , event ) \n 
~~ def test_user_not_in_groups ( self ) : \n 
~~~ from ... testing import make_site \n 
from ... interfaces import IUser \n 
site = make_site ( ) \n 
user = testing . DummyResource ( __provides__ = IUser ) \n 
site [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False ) \n 
~~ def test_user_in_groups ( self ) : \n 
groups = site [ ] [ ] \n 
groups [ ] = testing . DummyResource ( ) \n 
~~ def test_group_not_in_users ( self ) : \n 
group = testing . DummyResource ( ) \n 
site [ ] = group \n 
event = testing . DummyResource ( object = group , loading = False ) \n 
~~ def test_group_in_users ( self ) : \n 
users = site [ ] [ ] \n 
users [ ] = testing . DummyResource ( ) \n 
~~ ~~ class Test_user_will_be_removed ( unittest . TestCase ) : \n 
~~~ from . . subscribers import user_will_be_removed \n 
return user_will_be_removed ( event ) \n 
~~ def test_loading ( self ) : \n 
~~~ event = testing . DummyResource ( loading = True , moving = None ) \n 
~~ def test_moving ( self ) : \n 
~~~ event = testing . DummyResource ( loading = False , moving = True ) \n 
~~ def test_it ( self ) : \n 
~~~ from ... interfaces import IFolder \n 
parent = testing . DummyResource ( __provides__ = IFolder ) \n 
user = testing . DummyResource ( ) \n 
reset = testing . DummyResource ( ) \n 
def commit_suicide ( ) : \n 
~~~ reset . committed = True \n 
~~ reset . commit_suicide = commit_suicide \n 
objectmap = DummyObjectMap ( ( reset , ) ) \n 
parent . __objectmap__ = objectmap \n 
parent [ ] = user \n 
event = testing . DummyResource ( object = user , loading = False , moving = None ) \n 
self . assertTrue ( reset . committed ) \n 
~~~ event = testing . DummyResource ( object = None , loading = False ) \n 
~~ ~~ class Test_user_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import user_added \n 
return user_added ( event ) \n 
~~ def test_it_user_has_no_oid ( self ) : \n 
~~~ user = testing . DummyResource ( ) \n 
event . registry = DummyRegistry ( ) \n 
~~~ from pyramid . security import Allow \n 
user . __oid__ = 1 \n 
user . __acl__ , \n 
[ ( Allow , 1 , ( , \n 
) ) ] ) \n 
~~ ~~ class Test_acl_maybe_added ( unittest . TestCase ) : \n 
~~~ from . . subscribers import acl_maybe_added \n 
return acl_maybe_added ( event ) \n 
~~~ event = DummyEvent ( moving = True , loading = False ) \n 
self . assertEqual ( self . _callFUT ( event ) , False ) \n 
~~~ event = DummyEvent ( moving = None , loading = True ) \n 
~~ def test_objectmap_is_None ( self ) : \n 
~~~ event = DummyEvent ( moving = None , object = None , loading = False ) \n 
~~ def test_no_acls ( self ) : \n 
~~~ from substanced . interfaces import IFolder \n 
resource1 = testing . DummyResource ( __provides__ = IFolder ) \n 
resource2 = testing . DummyResource ( ) \n 
resource1 [ ] = resource2 \n 
objectmap = DummyObjectMap ( ) \n 
resource1 . __objectmap__ = objectmap \n 
event = DummyEvent ( moving = None , object = resource1 , loading = False ) \n 
self . assertEqual ( objectmap . connections , [ ] ) \n 
~~ def test_with_acls ( self ) : \n 
~~~ from ... interfaces import PrincipalToACLBearing \n 
from substanced . interfaces import IFolder \n 
resource1 . __acl__ = [ ( None , , None ) , ( None , 1 , None ) ] \n 
resource2 . __acl__ = [ ( None , , None ) , ( None , 2 , None ) ] \n 
objectmap . connections , \n 
[ ( 2 , resource2 , PrincipalToACLBearing ) , \n 
( 1 , resource1 , PrincipalToACLBearing ) ] \n 
~~ ~~ class Test_acl_modified ( unittest . TestCase ) : \n 
~~~ event = DummyEvent ( object = None ) \n 
~~ def test_gardenpath ( self ) : \n 
resource = testing . DummyResource ( ) \n 
resource . __objectmap__ = objectmap \n 
event = DummyEvent ( \n 
object = resource , \n 
new_acl = [ ( None , , None ) , ( None , 1 , None ) ] , \n 
old_acl = [ ( None , , None ) , ( None , 2 , None ) ] , \n 
[ ( 1 , resource , PrincipalToACLBearing ) ] \n 
objectmap . disconnections , \n 
[ ( 2 , resource , PrincipalToACLBearing ) ] \n 
~~ ~~ class DummyObjectMap ( object ) : \n 
~~~ def __init__ ( self , result = ( ) ) : \n 
self . connections = [ ] \n 
self . disconnections = [ ] \n 
~~ def targets ( self , object , reftype ) : \n 
~~~ return self . result \n 
~~ def connect ( self , source , target , reftype ) : \n 
~~~ self . connections . append ( ( source , target , reftype ) ) \n 
~~ def disconnect ( self , source , target , reftype ) : \n 
~~~ self . disconnections . append ( ( source , target , reftype ) ) \n 
~~~ def __init__ ( self , ** kw ) : \n 
~~ ~~ class DummyRegistry ( object ) : \n 
~~~ def subscribers ( self , * arg ) : \n 
~~ ~~ from pyramid . httpexceptions import ( \n 
HTTPForbidden , \n 
HTTPFound \n 
from pyramid . renderers import get_renderer \n 
from pyramid . session import check_csrf_token \n 
from pyramid . security import ( \n 
remember , \n 
forget , \n 
Authenticated , \n 
NO_PERMISSION_REQUIRED , \n 
from ... util import get_oid \n 
from . . import mgmt_view \n 
from substanced . interfaces import IUserLocator \n 
from substanced . principal import DefaultUserLocator \n 
from substanced . event import LoggedIn \n 
context = HTTPForbidden , \n 
permission = NO_PERMISSION_REQUIRED , \n 
effective_principals = Authenticated , \n 
def login ( context , request ) : \n 
~~~ login_url = request . sdiapi . mgmt_path ( request . context , ) \n 
referrer = request . url \n 
if in referrer : \n 
~~~ return HTTPForbidden ( ) \n 
~~ if login_url in referrer : \n 
~~~ referrer = request . sdiapi . mgmt_path ( request . virtual_root ) \n 
~~ came_from = request . session . setdefault ( , referrer ) \n 
login = \n 
password = \n 
if in request . params : \n 
~~~ check_csrf_token ( request ) \n 
~~~ request . sdiapi . flash ( , ) \n 
~~~ login = request . params [ ] \n 
password = request . params [ ] \n 
adapter = request . registry . queryMultiAdapter ( \n 
( context , request ) , \n 
IUserLocator \n 
if adapter is None : \n 
~~~ adapter = DefaultUserLocator ( context , request ) \n 
~~ user = adapter . get_user_by_login ( login ) \n 
if user is not None and user . check_password ( password ) : \n 
~~~ request . session . pop ( , None ) \n 
headers = remember ( request , get_oid ( user ) ) \n 
request . registry . notify ( LoggedIn ( login , user , context , request ) ) \n 
return HTTPFound ( location = came_from , headers = headers ) \n 
~~ request . sdiapi . flash ( , ) \n 
~~ ~~ template = get_renderer ( \n 
) . implementation ( ) \n 
return dict ( \n 
url = request . sdiapi . mgmt_path ( request . virtual_root , ) , \n 
came_from = came_from , \n 
login = login , \n 
password = password , \n 
login_template = template , \n 
def logout ( request ) : \n 
~~~ headers = forget ( request ) \n 
return HTTPFound ( location = request . sdiapi . mgmt_path ( request . context ) , \n 
headers = headers ) \n 
~~ import fnmatch \n 
~~~ import simplejson as json \n 
~~ def dump_sorted_json_string ( input , ** kwargs ) : \n 
return json . dumps ( input , sort_keys = True , separators = ( , ) , ** kwargs ) \n 
~~ def load_extra_plugins ( pathspec ) : \n 
loaded_plugins = [ ] \n 
paths = pathspec . split ( ) \n 
for path in paths : \n 
~~~ loaded_plugins . extend ( _load_plugins_from_dir ( path ) ) \n 
~~ return loaded_plugins \n 
~~ def _load_plugins_from_dir ( path ) : \n 
plugins = [ ] \n 
full_path = os . path . expanduser ( path ) \n 
~~~ filtered = fnmatch . filter ( os . listdir ( path ) , ) \n 
plugins . extend ( filtered ) \n 
~~~ sys . path . insert ( 1 , path ) \n 
~~ for plugin in plugins : \n 
~~~ match = re . search ( , plugin ) \n 
~~~ __import__ ( match . group ( ) , globals ( ) , locals ( ) , [ ] , - 1 ) \n 
~~~ loaded_plugins . append ( match . group ( ) ) \n 
~~ ~~ ~~ return loaded_plugins \n 
~~ __author__ = \n 
import websocket \n 
class Session ( object ) : \n 
~~~ self . running = False \n 
self . thread = None \n 
self . ws = None \n 
self . port = 443 \n 
self . inbound = [ ] \n 
~~ def connect ( self , host , port ) : \n 
~~~ if not self . is_connected ( ) : \n 
~~~ if type ( port ) == int : \n 
~~~ port = str ( port ) \n 
~~ url = + host + + port + \n 
~~~ self . ws = websocket . WebSocket ( ) \n 
self . ws . connect ( url , origin = ) \n 
self . running = True \n 
self . thread = threading . Thread ( name = , target = self . run ) \n 
self . thread . start ( ) \n 
~~~ print ( + url ) \n 
~~~ if self . is_connected ( ) : \n 
~~~ if self . ws . connected : \n 
~~~ self . ws . close ( ) \n 
~~ def is_connected ( self ) : \n 
~~~ return self . running and self . ws . connected \n 
~~~ while self . is_connected ( ) and self . thread == threading . current_thread ( ) : \n 
~~~ data = self . ws . recv ( ) \n 
self . inbound . append ( data ) \n 
~~~ print ( + str ( self . ws . connected ) + + str ( ex ) ) \n 
~~ ~~ ~~ def read ( self ) : \n 
~~~ if len ( self . inbound ) > 0 : \n 
~~~ data = self . inbound [ 0 ] \n 
self . inbound = self . inbound [ 1 : ] \n 
~~ def write ( self , data ) : \n 
~~~ if type ( data ) == bytearray : \n 
~~ if len ( data ) > 0 : \n 
~~~ self . ws . send ( data ) \n 
~~~ print ( + str ( ex ) ) \n 
~~ ~~ ~~ return False \n 
~~~ ses = Session ( ) \n 
################################################################################################# \n 
from pyral import Rally , rallyWorkset \n 
errout = sys . stderr . write \n 
def main ( args ) : \n 
~~~ options = [ opt for opt in args if opt . startswith ( ) ] \n 
args = [ arg for arg in args if arg not in options ] \n 
if not args : \n 
~~ query = "" \n 
target = args [ 0 ] \n 
if target in [ , , ] : \n 
~~~ target = "HierarchicalRequirement" \n 
~~ if in target : \n 
~~~ parent , entity = target . split ( , 1 ) \n 
target = entity \n 
server , username , password , apikey , workspace , project = rallyWorkset ( options ) \n 
~~~ if apikey : \n 
~~~ rally = Rally ( server , apikey = apikey , workspace = workspace , project = project ) \n 
~~~ rally = Rally ( server , user = username , password = password , workspace = workspace , project = project ~~ ~~ except Exception as ex : \n 
~~~ errout ( str ( ex . args [ 0 ] ) ) \n 
~~ typedef = rally . typedef ( target ) \n 
showAttributes ( typedef . Attributes ) \n 
print "" \n 
print "-" * 64 \n 
for ix , ancestor in enumerate ( typedef . inheritanceChain ( ) ) : \n 
~~ ~~ def showAttributes ( attributes ) : \n 
~~~ required = [ ] \n 
optional = [ ] \n 
av_limit = 20 \n 
for attr in attributes : \n 
~~~ name = % attr . ElementName \n 
a_type = % attr . AttributeType \n 
s_type = % attr . SchemaType \n 
s_type = s_type . replace ( , ) \n 
if s_type . upper ( ) == a_type : \n 
~~~ s_type = \n 
~~ reqd = if attr . Required else \n 
rdonly = if attr . ReadOnly else \n 
custom = if attr . Custom else \n 
hidden = if attr . Hidden else \n 
allowedValues = attr . AllowedValues \n 
tank = required if reqd == else optional \n 
num_allowed_values = "" \n 
if len ( allowedValues ) > 0 : \n 
if len ( allowedValues ) == 1 : \n 
~~~ num_allowed_values = num_allowed_values [ : - 1 ] \n 
tank . append ( info ) \n 
if num_allowed_values : \n 
~~ for av in allowedValues [ : av_limit ] : \n 
tank . append ( av_info ) \n 
~~ if len ( allowedValues ) > av_limit : \n 
~~ ~~ for item in required + optional : \n 
~~~ print item . encode ( ) \n 
~~~ main ( sys . argv [ 1 : ] ) \n 
import binascii \n 
import yubi_goog \n 
class TestYubiGoog ( unittest . TestCase ) : \n 
self . test_secret = binascii . hexlify ( . encode ( ) ) \n 
self . test_vectors = [ { : 1111111111 , : } , \n 
{ : 1234567890 , : } , \n 
{ : 2000000000 , : } ] \n 
~~ def test_decode_secret ( self ) : \n 
~~~ decoded = yubi_goog . decode_secret ( self . google_secret ) . upper ( ) \n 
self . assertEqual ( decoded , "6DE9FAF2507F9A99193D" . encode ( ) ) \n 
~~ def test_totp ( self ) : \n 
~~~ for pair in self . test_vectors : \n 
~~~ time = pair [ ] \n 
real_otp = pair [ ] \n 
tm = int ( int ( time ) / 30 ) \n 
tm = struct . pack ( , tm ) \n 
otp = yubi_goog . totp ( self . test_secret , tm ) \n 
self . assertEqual ( otp , real_otp ) \n 
~~ ~~ ~~ if __name__ == : \n 
import itertools \n 
from . core import GroupMixin , Command \n 
from . errors import CommandError \n 
class HelpFormatter : \n 
def __init__ ( self , show_hidden = False , show_check_failure = False , width = 80 ) : \n 
~~~ self . width = width \n 
self . show_hidden = show_hidden \n 
self . show_check_failure = show_check_failure \n 
~~ def has_subcommands ( self ) : \n 
return isinstance ( self . command , GroupMixin ) \n 
~~ def is_bot ( self ) : \n 
return self . command is self . context . bot \n 
~~ def is_cog ( self ) : \n 
return not self . is_bot ( ) and not isinstance ( self . command , Command ) \n 
~~ def shorten ( self , text ) : \n 
if len ( text ) > self . width : \n 
~~~ return text [ : self . width - 3 ] + \n 
~~ return text \n 
def max_name_size ( self ) : \n 
~~~ commands = self . command . commands if not self . is_cog ( ) else self . context . bot . commands \n 
if commands : \n 
~~~ return max ( map ( lambda c : len ( c . name ) , commands . values ( ) ) ) \n 
~~ return 0 \n 
~~~ return len ( self . command . name ) \n 
def clean_prefix ( self ) : \n 
user = self . context . bot . user \n 
return self . context . prefix . replace ( user . mention , + user . name ) \n 
~~ def get_qualified_command_name ( self ) : \n 
entries = [ ] \n 
command = self . command \n 
while command . parent is not None : \n 
~~~ command = command . parent \n 
entries . append ( command . name ) \n 
~~ return . join ( reversed ( entries ) ) \n 
~~ def get_command_signature ( self ) : \n 
result = [ ] \n 
prefix = self . clean_prefix \n 
qualified = self . get_qualified_command_name ( ) \n 
cmd = self . command \n 
if len ( cmd . aliases ) > 0 : \n 
~~~ aliases = . join ( cmd . aliases ) \n 
fmt = \n 
if qualified : \n 
~~~ fmt = \n 
~~ result . append ( fmt . format ( prefix , cmd , aliases , qualified ) ) \n 
~~~ name = prefix + cmd . name if not qualified else prefix + qualified + + cmd . name \n 
result . append ( name ) \n 
~~ params = cmd . clean_params \n 
if len ( params ) > 0 : \n 
~~~ for name , param in params . items ( ) : \n 
~~~ if param . default is not param . empty : \n 
~~~ should_print = param . default if isinstance ( param . default , str ) else param . default if should_print : \n 
~~~ result . append ( . format ( name , param . default ) ) \n 
~~~ result . append ( . format ( name ) ) \n 
~~ ~~ elif param . kind == param . VAR_POSITIONAL : \n 
~~ ~~ ~~ return . join ( result ) \n 
~~ def get_ending_note ( self ) : \n 
~~~ command_name = self . context . invoked_with \n 
~~ def filter_command_list ( self ) : \n 
def predicate ( tuple ) : \n 
~~~ cmd = tuple [ 1 ] \n 
if self . is_cog ( ) : \n 
~~~ if cmd . instance is not self . command : \n 
~~ ~~ if cmd . hidden and not self . show_hidden : \n 
~~ if self . show_check_failure : \n 
~~~ return cmd . can_run ( self . context ) \n 
~~ except CommandError : \n 
~~ ~~ iterator = self . command . commands . items ( ) if not self . is_cog ( ) else self . context . bot . commands return filter ( predicate , iterator ) \n 
~~ def _check_new_page ( self ) : \n 
~~~ if self . _count + len ( self . _current_page ) >= 1980 : \n 
~~~ self . _current_page . append ( ) \n 
self . _pages . append ( . join ( self . _current_page ) ) \n 
self . _current_page = [ ] \n 
self . _count = 4 \n 
~~ def _add_subcommands_to_page ( self , max_width , commands ) : \n 
~~~ for name , command in commands : \n 
~~~ if name in command . aliases : \n 
~~ entry = . format ( name , command . short_doc , width = max_width ) \n 
shortened = self . shorten ( entry ) \n 
self . _count += len ( shortened ) \n 
if self . _check_new_page ( ) : \n 
~~~ self . _count += len ( shortened ) \n 
~~ self . _current_page . append ( shortened ) \n 
~~ ~~ def format_help_for ( self , context , command_or_bot ) : \n 
self . context = context \n 
self . command = command_or_bot \n 
return self . format ( ) \n 
~~ def format ( self ) : \n 
self . _pages = [ ] \n 
description = self . command . description if not self . is_cog ( ) else inspect . getdoc ( self . command \n 
if description : \n 
~~~ self . _current_page . append ( description ) \n 
self . _current_page . append ( ) \n 
self . _count += len ( description ) \n 
~~ if isinstance ( self . command , Command ) : \n 
~~~ signature = self . get_command_signature ( ) \n 
self . _count += 2 + len ( signature ) \n 
self . _current_page . append ( signature ) \n 
if self . command . help : \n 
~~~ self . _count += 2 + len ( self . command . help ) \n 
self . _current_page . append ( self . command . help ) \n 
self . _check_new_page ( ) \n 
~~ if not self . has_subcommands ( ) : \n 
return self . _pages \n 
~~ ~~ max_width = self . max_name_size \n 
def category ( tup ) : \n 
~~~ cog = tup [ 1 ] . cog_name \n 
return cog + if cog is not None else \n 
~~ if self . is_bot ( ) : \n 
~~~ data = sorted ( self . filter_command_list ( ) , key = category ) \n 
for category , commands in itertools . groupby ( data , key = category ) : \n 
~~~ commands = list ( commands ) \n 
if len ( commands ) > 0 : \n 
~~~ self . _current_page . append ( category ) \n 
self . _count += len ( category ) \n 
~~ self . _add_subcommands_to_page ( max_width , commands ) \n 
self . _count += 1 + len ( self . _current_page [ - 1 ] ) \n 
self . _add_subcommands_to_page ( max_width , self . filter_command_list ( ) ) \n 
~~ self . _current_page . append ( ) \n 
ending_note = self . get_ending_note ( ) \n 
self . _count += len ( ending_note ) \n 
self . _current_page . append ( ending_note ) \n 
if len ( self . _current_page ) > 1 : \n 
~~ return self . _pages \n 
~~ ~~ from rx . disposables import Disposable , SingleAssignmentDisposable \n 
from . scheduler import Scheduler \n 
class CatchScheduler ( Scheduler ) : \n 
~~~ def __init__ ( self , scheduler , handler ) : \n 
~~~ self . _scheduler = scheduler \n 
self . _handler = handler \n 
self . _recursive_original = None \n 
self . _recursive_wrapper = None \n 
super ( CatchScheduler , self ) . __init__ ( ) \n 
~~ def local_now ( self ) : \n 
~~~ return self . _scheduler . now ( ) \n 
~~ def schedule_now ( self , state , action ) : \n 
return self . _scheduler . scheduleWithState ( state , self . _wrap ( action ) ) \n 
~~ def schedule_relative ( self , duetime , action , state = None ) : \n 
return self . _scheduler . schedule_relative ( duetime , self . _wrap ( action ) , \n 
state = state ) \n 
~~ def schedule_absolute ( self , duetime , action , state = None ) : \n 
return self . _scheduler . schedule_absolute ( duetime , self . _wrap ( action ) , \n 
~~ def _clone ( self , scheduler ) : \n 
~~~ return CatchScheduler ( scheduler , self . _handler ) \n 
~~ def _wrap ( self , action ) : \n 
~~~ parent = self \n 
def wrapped_action ( self , state ) : \n 
~~~ return action ( parent . _get_recursive_wrapper ( self ) , state ) \n 
~~~ if not parent . _handler ( ex ) : \n 
~~~ raise Exception ( ex ) \n 
~~ return Disposable . empty ( ) \n 
~~ ~~ return wrapped_action \n 
~~ def _get_recursive_wrapper ( self , scheduler ) : \n 
~~~ if self . _recursive_original != scheduler : \n 
~~~ self . _recursive_original = scheduler \n 
wrapper = self . _clone ( scheduler ) \n 
wrapper . _recursive_original = scheduler \n 
wrapper . _recursive_wrapper = wrapper \n 
self . _recursive_wrapper = wrapper \n 
~~ return self . _recursive_wrapper \n 
~~ def schedule_periodic ( self , period , action , state = None ) : \n 
~~~ d = SingleAssignmentDisposable ( ) \n 
failed = [ False ] \n 
def periodic_action ( periodic_state ) : \n 
~~~ if failed [ 0 ] : \n 
~~~ return action ( periodic_state ) \n 
~~~ failed [ 0 ] = True \n 
if not self . _handler ( ex ) : \n 
~~ d . dispose ( ) \n 
~~ ~~ d . disposable = self . _scheduler . schedule_periodic ( periodic_action , \n 
period , state ) \n 
~~ ~~ from . booleandisposable import BooleanDisposable \n 
class SingleAssignmentDisposable ( BooleanDisposable ) : \n 
~~~ super ( SingleAssignmentDisposable , self ) . __init__ ( True ) \n 
~~ ~~ import threading \n 
from rx . blockingobservable import BlockingObservable \n 
from rx . internal import extensionmethod \n 
from rx . internal . enumerator import Enumerator \n 
@ extensionmethod ( BlockingObservable ) \n 
def to_iterable ( self ) : \n 
condition = threading . Condition ( ) \n 
notifications = [ ] \n 
def on_next ( value ) : \n 
condition . acquire ( ) \n 
notifications . append ( value ) \n 
condition . release ( ) \n 
~~ self . observable . materialize ( ) . subscribe ( on_next ) \n 
def gen ( ) : \n 
~~~ condition . acquire ( ) \n 
while not len ( notifications ) : \n 
~~~ condition . wait ( ) \n 
~~ notification = notifications . pop ( 0 ) \n 
if notification . kind == "E" : \n 
~~~ raise notification . exception \n 
~~ if notification . kind == "C" : \n 
~~ condition . release ( ) \n 
yield notification . value \n 
~~ ~~ return Enumerator ( gen ( ) ) \n 
~~ @ extensionmethod ( BlockingObservable ) \n 
def __iter__ ( self ) : \n 
return self . to_iterable ( ) \n 
~~ from rx . observable import Observable \n 
from rx . anonymousobservable import AnonymousObservable \n 
def find_value ( source , predicate , yield_index ) : \n 
~~~ def subscribe ( observer ) : \n 
~~~ i = [ 0 ] \n 
def on_next ( x ) : \n 
~~~ should_run = False \n 
~~~ should_run = predicate ( x , i , source ) \n 
~~~ observer . on_error ( ex ) \n 
~~ if should_run : \n 
~~~ observer . on_next ( i [ 0 ] if yield_index else x ) \n 
observer . on_completed ( ) \n 
~~~ i [ 0 ] += 1 \n 
~~ ~~ def on_completed ( ) : \n 
~~~ observer . on_next ( - 1 if yield_index else None ) \n 
~~ return source . subscribe ( on_next , observer . on_error , on_completed ) \n 
~~ return AnonymousObservable ( subscribe ) \n 
~~ @ extensionmethod ( Observable ) \n 
def find ( self , predicate ) : \n 
return find_value ( self , predicate , False ) \n 
~~ from rx import Observable , AnonymousObservable \n 
from rx . linq . connectableobservable import ConnectableObservable \n 
from rx . disposables import CompositeDisposable \n 
@ extensionmethod ( Observable ) \n 
def multicast ( self , subject = None , subject_selector = None , selector = None ) : \n 
source = self \n 
if subject_selector : \n 
~~~ connectable = source . multicast ( subject = subject_selector ( ) ) \n 
return CompositeDisposable ( selector ( connectable ) . subscribe ( observer ) , connectable . connect \n 
~~~ return ConnectableObservable ( source , subject ) \n 
from rx . observable import Observable \n 
def skip_until_with_time ( self , start_time , scheduler ) : \n 
scheduler = scheduler or timeout_scheduler \n 
if isinstance ( start_time , datetime ) : \n 
~~~ scheduler_method = \n 
~~ def subscribe ( observer ) : \n 
~~~ open = [ False ] \n 
~~~ if open [ 0 ] : \n 
~~~ observer . on_next ( x ) \n 
~~ ~~ subscription = source . subscribe ( on_next , observer . on_error , \n 
observer . on_completed ) \n 
def action ( scheduler , state ) : \n 
~~~ open [ 0 ] = True \n 
~~ disposable = getattr ( scheduler , scheduler_method ) ( start_time , action ) \n 
return CompositeDisposable ( disposable , subscription ) \n 
from rx . concurrency import timeout_scheduler \n 
from rx . subjects import AsyncSubject \n 
from rx . internal import extensionclassmethod \n 
@ extensionclassmethod ( Observable ) \n 
def to_async ( cls , func , scheduler = None ) : \n 
def wrapper ( * args ) : \n 
~~~ subject = AsyncSubject ( ) \n 
~~~ result = func ( * args ) \n 
~~~ subject . on_error ( ex ) \n 
~~ subject . on_next ( result ) \n 
subject . on_completed ( ) \n 
~~ scheduler . schedule ( action ) \n 
return subject . as_observable ( ) \n 
~~ return wrapper \n 
~~ from rx import Lock \n 
class InnerSubscription ( object ) : \n 
~~~ def __init__ ( self , subject , observer ) : \n 
~~~ self . subject = subject \n 
self . observer = observer \n 
self . lock = Lock ( ) \n 
~~ def dispose ( self ) : \n 
~~~ with self . lock : \n 
~~~ if not self . subject . is_disposed and self . observer : \n 
~~~ if self . observer in self . subject . observers : \n 
~~~ self . subject . observers . remove ( self . observer ) \n 
~~ self . observer = None \n 
~~ ~~ ~~ ~~ import unittest \n 
from datetime import datetime , timedelta \n 
from rx . concurrency import NewThreadScheduler \n 
class TestNewThreadScheduler ( unittest . TestCase ) : \n 
~~~ def test_new_thread_now ( self ) : \n 
~~~ scheduler = NewThreadScheduler ( ) \n 
res = scheduler . now ( ) - datetime . utcnow ( ) \n 
assert res < timedelta ( microseconds = 1000 ) \n 
~~ def test_new_thread_schedule_action ( self ) : \n 
ran = [ False ] \n 
~~~ ran [ 0 ] = True \n 
sleep ( 0.1 ) \n 
assert ( ran [ 0 ] == True ) \n 
~~ def test_new_thread_schedule_action_due ( self ) : \n 
starttime = datetime . utcnow ( ) \n 
endtime = [ None ] \n 
~~~ endtime [ 0 ] = datetime . utcnow ( ) \n 
~~ scheduler . schedule_relative ( timedelta ( milliseconds = 200 ) , action ) \n 
sleep ( 0.3 ) \n 
diff = endtime [ 0 ] - starttime \n 
assert ( diff > timedelta ( milliseconds = 180 ) ) \n 
~~ def test_new_thread_schedule_action_cancel ( self ) : \n 
~~~ ran = [ False ] \n 
scheduler = NewThreadScheduler ( ) \n 
~~ d = scheduler . schedule_relative ( timedelta ( milliseconds = 1 ) , action ) \n 
d . dispose ( ) \n 
assert ( not ran [ 0 ] ) \n 
from rx import Observable \n 
from rx . testing import TestScheduler , ReactiveTest , is_prime , MockDisposable \n 
from rx . disposables import Disposable , SerialDisposable \n 
on_next = ReactiveTest . on_next \n 
on_completed = ReactiveTest . on_completed \n 
on_error = ReactiveTest . on_error \n 
subscribe = ReactiveTest . subscribe \n 
subscribed = ReactiveTest . subscribed \n 
disposed = ReactiveTest . disposed \n 
created = ReactiveTest . created \n 
class TestCount ( unittest . TestCase ) : \n 
~~~ def test_count_empty ( self ) : \n 
~~~ scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_completed ( 250 ) ) \n 
res = scheduler . start ( create = lambda : xs . count ( ) ) . messages \n 
res . assert_equal ( on_next ( 250 , 0 ) , on_completed ( 250 ) ) \n 
~~ def test_count_empty_ii ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_completed ( 250 ) ) \n 
def create ( ) : \n 
~~~ return xs . count ( ) \n 
~~ res = scheduler . start ( create = create ) . messages \n 
res . assert_equal ( on_next ( 250 , 1 ) , on_completed ( 250 ) ) \n 
~~ def test_count_some ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next res = scheduler . start ( create = lambda : xs . count ( ) ) . messages \n 
res . assert_equal ( on_next ( 250 , 3 ) , on_completed ( 250 ) ) \n 
~~ def test_count_throw ( self ) : \n 
~~~ ex = \n 
scheduler = TestScheduler ( ) \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_error ( 210 , ex ) ) \n 
res . assert_equal ( on_error ( 210 , ex ) ) \n 
~~ def test_count_never ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) ) \n 
res . assert_equal ( ) \n 
~~ def test_count_predicate_empty_true ( self ) : \n 
~~~ return xs . count ( lambda _ : True ) \n 
~~ res = scheduler . start ( create = create ) \n 
res . messages . assert_equal ( on_next ( 250 , 0 ) , on_completed ( 250 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 250 ) ) \n 
~~ def test_count_predicate_empty_false ( self ) : \n 
~~~ return xs . count ( lambda _ : False ) \n 
~~ def test_count_predicate_return_true ( self ) : \n 
res . messages . assert_equal ( on_next ( 250 , 1 ) , on_completed ( 250 ) ) \n 
~~ def test_count_predicate_return_false ( self ) : \n 
~~ def test_count_predicate_some_all ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next \n 
~~~ return xs . count ( lambda x : x < 10 ) \n 
res . messages . assert_equal ( on_next ( 250 , 3 ) , on_completed ( 250 ) ) \n 
~~ def test_count_predicate_some_none ( self ) : \n 
~~~ return xs . count ( lambda x : x > 10 ) \n 
~~ def test_count_predicate_some_even ( self ) : \n 
~~~ return xs . count ( lambda x : x % 2 == 0 ) \n 
res . messages . assert_equal ( on_next ( 250 , 2 ) , on_completed ( 250 ) ) \n 
~~ def test_count_predicate_throw_true ( self ) : \n 
res . messages . assert_equal ( on_error ( 210 , ex ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 210 ) ) \n 
~~ def test_count_predicate_throw_false ( self ) : \n 
~~ def test_count_predicate_never ( self ) : \n 
res . messages . assert_equal ( ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 1000 ) ) \n 
~~ def test_count_predicate_predicate_throws ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 230 , 3 ) , on_completed \n 
~~~ def predicate ( x ) : \n 
~~~ if x == 3 : \n 
~~ ~~ return xs . count ( predicate ) \n 
res . messages . assert_equal ( on_error ( 230 , ex ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 200 , 230 ) ) \n 
from rx . subjects import Subject \n 
class RxException ( Exception ) : \n 
~~ def _raise ( ex ) : \n 
~~~ raise RxException ( ex ) \n 
~~ class TestTimeInterval ( unittest . TestCase ) : \n 
~~~ def test_interval_timespan_basic ( self ) : \n 
~~~ return Observable . interval ( 100 , scheduler = scheduler ) \n 
~~ results = scheduler . start ( create ) \n 
results . messages . assert_equal ( on_next ( 300 , 0 ) , on_next ( 400 , 1 ) , on_next ( 500 , 2 ) , on_next ( 600 \n 
~~ def test_interval_timespan_zero ( self ) : \n 
~~~ return Observable . interval ( 0 , scheduler = scheduler ) \n 
~~ results = scheduler . start ( create , disposed = 210 ) \n 
results . messages . assert_equal ( on_next ( 201 , 0 ) , on_next ( 202 , 1 ) , on_next ( 203 , 2 ) , on_next ( 204 \n 
~~ def test_interval_timespan_negative ( self ) : \n 
~~~ return Observable . interval ( - 1 , scheduler = scheduler ) \n 
~~ def test_interval_timespan_disposed ( self ) : \n 
~~~ return Observable . interval ( 1000 , scheduler = scheduler ) \n 
results . messages . assert_equal ( ) \n 
~~ def test_interval_timespan_observer_throws ( self ) : \n 
xs = Observable . interval ( 1 , scheduler = scheduler ) \n 
xs . subscribe ( lambda x : _raise ( "ex" ) ) \n 
with self . assertRaises ( RxException ) : \n 
~~~ scheduler . start ( ) \n 
~~ ~~ ~~ import unittest \n 
from rx . testing import TestScheduler , ReactiveTest \n 
class TestReplay ( unittest . TestCase ) : \n 
~~~ def test_replay_count_basic ( self ) : \n 
~~~ connection = [ None ] \n 
subscription = [ None ] \n 
ys = [ None ] \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next results = scheduler . create_observer ( ) \n 
def action0 ( scheduler , state ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , 3 , None , scheduler ) \n 
~~ scheduler . schedule_absolute ( created , action0 ) \n 
def action1 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] = ys [ 0 ] . subscribe ( results ) \n 
~~ scheduler . schedule_absolute ( 450 , action1 ) \n 
def action2 ( scheduler , state ) : \n 
~~~ subscription [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( disposed , action2 ) \n 
def action3 ( scheduler , state ) : \n 
~~~ connection [ 0 ] = ys [ 0 ] . connect ( ) \n 
~~ scheduler . schedule_absolute ( 300 , action3 ) \n 
def action4 ( scheduler , state ) : \n 
~~~ connection [ 0 ] . dispose ( ) \n 
~~ scheduler . schedule_absolute ( 400 , action4 ) \n 
def action5 ( scheduler , state ) : \n 
~~ scheduler . schedule_absolute ( 500 , action5 ) \n 
def action6 ( scheduler , state ) : \n 
~~ scheduler . schedule_absolute ( 550 , action6 ) \n 
def action7 ( scheduler , state ) : \n 
~~ scheduler . schedule_absolute ( 650 , action7 ) \n 
def action8 ( scheduler , state ) : \n 
~~ scheduler . schedule_absolute ( 800 , action8 ) \n 
scheduler . start ( ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) , on_next ( 521 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_count_error ( self ) : \n 
ex = \n 
~~ scheduler . schedule_absolute ( 800 , action6 ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) , on_next ( 521 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
~~ def test_replay_count_complete ( self ) : \n 
def action1 ( scehduler , state ) : \n 
~~ scheduler . schedule_absolute ( 800 , action ) \n 
~~ def test_replay_count_dispose ( self ) : \n 
~~ scheduler . schedule_absolute ( 475 , action2 ) \n 
results . messages . assert_equal ( on_next ( 451 , 5 ) , on_next ( 452 , 6 ) , on_next ( 453 , 7 ) ) \n 
xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_count_multiple_connections ( self ) : \n 
~~~ xs = Observable . never ( ) \n 
ys = xs . replay ( None , 3 ) \n 
connection1 = ys . connect ( ) \n 
connection2 = ys . connect ( ) \n 
assert ( connection1 == connection2 ) \n 
connection1 . dispose ( ) \n 
connection2 . dispose ( ) \n 
connection3 = ys . connect ( ) \n 
assert ( connection1 != connection3 ) \n 
~~ def test_replay_count_lambda_zip_complete ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next \n 
def action ( ) : \n 
~~~ def selector ( _xs ) : \n 
~~~ return _xs . take ( 6 ) . repeat ( ) \n 
~~ return xs . replay ( selector , 3 , None , scheduler ) \n 
~~ results = scheduler . start ( action , disposed = 610 ) \n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 600 ) ) \n 
~~ def test_replay_count_lambda_zip_error ( self ) : \n 
~~ def test_replay_count_lambda_zip_dispose ( self ) : \n 
~~ results = scheduler . start ( create , disposed = 470 ) \n 
results . messages . assert_equal ( on_next ( 221 , 3 ) , on_next ( 281 , 4 ) , on_next ( 291 , 1 ) , on_next ( 341 xs . subscriptions . assert_equal ( subscribe ( 200 , 470 ) ) \n 
~~ def test_replay_time_basic ( self ) : \n 
~~~ subscription = [ None ] \n 
connection = [ None ] \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 150 , scheduler ) \n 
results . messages . assert_equal ( on_next ( 451 , 8 ) , on_next ( 452 , 5 ) , on_next ( 453 , 6 ) , on_next ( 454 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 550 ) , subscribe ( 650 , 800 ) ) \n 
~~ def test_replay_time_error ( self ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 75 , scheduler ) \n 
results . messages . assert_equal ( on_next ( 451 , 7 ) , on_next ( 521 , 11 ) , on_next ( 561 , 20 ) , on_error ( xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
~~ def test_replay_time_complete ( self ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 85 , scheduler ) \n 
results . messages . assert_equal ( on_next ( 451 , 6 ) , on_next ( 452 , 7 ) , on_next ( 521 , 11 ) , on_next ( 561 xs . subscriptions . assert_equal ( subscribe ( 300 , 400 ) , subscribe ( 500 , 600 ) ) \n 
~~ def test_replay_time_dispose ( self ) : \n 
~~~ ys [ 0 ] = xs . replay ( None , None , 100 , scheduler ) \n 
~~ def test_replay_time_multiple_connections ( self ) : \n 
ys = xs . replay ( None , None , 100 ) \n 
~~ def test_replay_time_lambda_zip_complete ( self ) : \n 
~~ return xs . replay ( selector , None , 50 , scheduler ) \n 
~~ results = scheduler . start ( create , disposed = 610 ) \n 
~~ def test_replay_time_lambda_zip_error ( self ) : \n 
~~ def test_replay_time_lambda_zip_dispose ( self ) : \n 
xs = scheduler . create_hot_observable ( on_next ( 110 , 7 ) , on_next ( 220 , 3 ) , on_next ( 280 , 4 ) , on_next def create ( ) : \n 
~~ class TestTakeUntil ( unittest . TestCase ) : \n 
~~~ def test_take_until_preempt_somedata_next ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 r_msgs = [ on_next ( 150 , 1 ) , on_next ( 225 , 99 ) , on_completed ( 230 ) ] \n 
l = scheduler . create_hot_observable ( l_msgs ) \n 
r = scheduler . create_hot_observable ( r_msgs ) \n 
~~~ return l . take_until ( r ) \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_completed ( 225 ) ) \n 
~~ def test_take_until_preempt_somedata_error ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 r_msgs = [ on_next ( 150 , 1 ) , on_error ( 225 , ex ) ] \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_error ( 225 , ex ) ) \n 
~~ def test_take_until_nopreempt_somedata_empty ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 r_msgs = [ on_next ( 150 , 1 ) , on_completed ( 225 ) ] \n 
results . messages . assert_equal ( on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 \n 
~~ def test_take_until_nopreempt_somedata_never ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_next ( 220 , 3 ) , on_next ( 230 , 4 ) , on_next ( 240 , 5 l = scheduler . create_hot_observable ( l_msgs ) \n 
r = Observable . never ( ) \n 
~~ def test_take_until_preempt_never_next ( self ) : \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 225 , 2 ) , on_completed ( 250 ) ] \n 
l = Observable . never ( ) \n 
results . messages . assert_equal ( on_completed ( 225 ) ) \n 
~~ def test_take_until_preempt_never_error ( self ) : \n 
r_msgs = [ on_next ( 150 , 1 ) , on_error ( 225 , ex ) ] \n 
results . messages . assert_equal ( on_error ( 225 , ex ) ) \n 
~~ def test_take_until_nopreempt_never_empty ( self ) : \n 
r_msgs = [ on_next ( 150 , 1 ) , on_completed ( 225 ) ] \n 
~~ def test_take_until_nopreempt_never_never ( self ) : \n 
~~ def test_take_until_preempt_beforefirstproduced ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_next ( 230 , 2 ) , on_completed ( 240 ) ] \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 210 , 2 ) , on_completed ( 220 ) ] \n 
results . messages . assert_equal ( on_completed ( 210 ) ) \n 
~~ def test_take_until_preempt_beforefirstproduced_remain_silent_and_proper_disposed ( self ) : \n 
l_msgs = [ on_next ( 150 , 1 ) , on_error ( 215 , ) , on_completed ( 240 ) ] \n 
source_not_disposed = [ False ] \n 
~~~ source_not_disposed [ 0 ] = True \n 
~~ l = scheduler . create_hot_observable ( l_msgs ) . do_action ( on_next = action ) \n 
assert ( not source_not_disposed [ 0 ] ) \n 
~~ def test_take_until_nopreempt_afterlastproduced_proper_disposed_signal ( self ) : \n 
r_msgs = [ on_next ( 150 , 1 ) , on_next ( 250 , 2 ) , on_completed ( 260 ) ] \n 
signal_not_disposed = [ False ] \n 
~~~ signal_not_disposed [ 0 ] = True \n 
~~ r = scheduler . create_hot_observable ( r_msgs ) . do_action ( on_next = action ) \n 
results . messages . assert_equal ( on_next ( 230 , 2 ) , on_completed ( 240 ) ) \n 
assert ( not signal_not_disposed [ 0 ] ) \n 
~~ ~~ import logging \n 
FORMAT = \n 
logging . basicConfig ( filename = , format = FORMAT , level = logging . DEBUG ) \n 
log = logging . getLogger ( ) \n 
~~~ test_buffer_with_time_or_count_basic ( ) \n 
~~ def prune_old_authorization_codes ( ) : \n 
from . compat import now \n 
from . models import AuthorizationCode \n 
AuthorizationCode . objects . with_expiration_before ( now ( ) ) . delete ( ) \n 
~~ def get_handler ( handler_name ) : \n 
from . conf import options \n 
handlers = options . handlers \n 
for handler in handlers : \n 
~~~ handler_path = handler . split ( "." ) \n 
name = handler_path [ - 2 ] \n 
if handler_name == name : \n 
~~~ handler_module = __import__ ( "." . join ( handler_path [ : - 1 ] ) , { } , { } , str ( handler_path [ - 1 ] ) ) \n 
return getattr ( handler_module , handler_path [ - 1 ] ) ( ) \n 
~~ def request_error_header ( exception ) : \n 
if hasattr ( exception , "error" ) : \n 
~~ if hasattr ( exception , "reason" ) : \n 
~~ return header \n 
~~ def total_seconds ( delta ) : \n 
return delta . days * 86400 + delta . seconds \n 
__all__ = ( \n 
, , , \n 
, , \n 
, , , , \n 
WS_NORMAL = 1000 \n 
WS_GOING_AWAY = 1001 \n 
WS_PROTOCOL_ERROR = 1002 \n 
WS_DATA_CANNOT_ACCEPT = 1003 \n 
WS_RESERVED = 1004 \n 
WS_NO_STATUS_CODE = 1005 \n 
WS_CLOSED_ABNORMALLY = 1006 \n 
WS_MESSAGE_NOT_CONSISTENT = 1007 \n 
WS_MESSAGE_VIOLATE_POLICY = 1008 \n 
WS_MESSAGE_TOO_BIG = 1009 \n 
WS_SERVER_DIDNT_RETURN_EXTENSIONS = 1010 \n 
WS_UNEXPECTED_CONDITION = 1011 \n 
WS_FAILURE_TLS = 1015 \n 
def is_not_used ( code ) : \n 
return 0 <= code <= 999 \n 
~~ def is_reserved ( code ) : \n 
return 1000 <= code <= 2999 \n 
~~ def is_library ( code ) : \n 
return 3000 <= code <= 3999 \n 
~~ def is_private ( code ) : \n 
return 4000 <= code <= 4999 \n 
~~ DATABASES = { \n 
import fileinput \n 
ALPHABET = ( "A" , "C" , "G" , "T" ) \n 
def sequence_count ( string , wordlength ) : \n 
wc = { } \n 
while i < len ( string ) - wordlength + 1 : \n 
~~~ word = string [ i : i + wordlength ] \n 
if word in wc : \n 
~~~ wc [ word ] += 1 \n 
~~~ wc [ word ] = 1 \n 
~~ i += 1 \n 
~~ return wc \n 
~~ def pretty_print ( wordcount_dict , wordlength ) : \n 
output = "" \n 
e = 0 \n 
for w in itertools . product ( ALPHABET , repeat = wordlength ) : \n 
~~~ w = . join ( w ) \n 
~~~ output += "%3.10s%13s\\n" % ( e , wordcount_dict [ w ] ) \n 
~~ except IndexError : \n 
~~~ output += "%s\\t0\\n" \n 
~~ e += 1 \n 
~~ return output \n 
~~ def histogram ( filename , wordlength ) : \n 
for line in open ( filename ) . readlines ( ) : \n 
~~~ wcnew = sequence_count ( line , wordlength ) \n 
wc = { i : wc . get ( i , 0 ) + wcnew . get ( i , 0 ) for i in set ( wc ) | set ( wcnew ) } \n 
~~ return pretty_print ( wc , wordlength ) \n 
~~ if __name__ == "__main__" : \n 
~~~ print histogram ( sys . argv [ 1 ] , 3 ) \n 
~~ from setuptools import setup \n 
version = , \n 
py_modules = [ ] , \n 
install_requires = [ \n 
entry_points = , \n 
"""\nDocuments\n""" \n 
import shutil \n 
import sublime \n 
import zipfile \n 
from . import pyarduino \n 
from . import st_base \n 
from . import st_menu \n 
from . import st_console \n 
def set_pyarduino ( ) : \n 
~~~ user_path = st_base . get_stino_user_path ( ) \n 
package_path = st_base . get_plugin_path ( ) \n 
stino_path = os . path . join ( package_path , ) \n 
pyarduino_path = os . path . join ( stino_path , ) \n 
settings_path = os . path . join ( pyarduino_path , ) \n 
settings = pyarduino . base . settings . Settings ( settings_path ) \n 
settings . set ( , package_path ) \n 
settings . set ( , user_path ) \n 
~~ def load_keywords ( ) : \n 
~~~ arduino_info = st_base . get_arduino_info ( ) \n 
ide_dir = arduino_info . get_ide_dir ( ) \n 
keywords = ide_dir . get_keywords ( ) \n 
for root in arduino_info . get_root_dirs ( ) : \n 
~~~ libraries = root . get_libraries ( ) \n 
for library in libraries : \n 
~~~ keywords += library . get_keywords ( ) \n 
~~ for package in root . get_packages ( ) : \n 
~~~ for platform in package . get_platforms ( ) : \n 
~~~ libraries = platform . get_libraries ( ) \n 
~~ ~~ ~~ ~~ return keywords \n 
~~ def create_completions ( ) : \n 
file_path = os . path . join ( user_path , ) \n 
completions_file = pyarduino . base . json_file . JSONFile ( file_path ) \n 
cpp_keywords = [ , , , , ] \n 
cpp_keywords += [ , , , , ] \n 
keywords = load_keywords ( ) \n 
keyword_ids = [ k . get_id ( ) for k in keywords ] \n 
keyword_ids += cpp_keywords \n 
completions_dict = { : } \n 
completions_dict [ ] = keyword_ids \n 
completions_file . set_data ( completions_dict ) \n 
~~ def create_syntax_file ( ) : \n 
~~~ keywords = load_keywords ( ) \n 
LITERAL1s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
KEYWORD1s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
KEYWORD2s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
KEYWORD3s = [ k . get_id ( ) for k in keywords if k . get_type ( ) == ] \n 
LITERAL1_text = . join ( LITERAL1s ) \n 
KEYWORD1_text = . join ( KEYWORD1s ) \n 
KEYWORD2_text = . join ( KEYWORD2s ) \n 
KEYWORD3_text = . join ( KEYWORD3s ) \n 
preset_path = st_base . get_preset_path ( ) \n 
file_path = os . path . join ( preset_path , ) \n 
template_file = pyarduino . base . abs_file . File ( file_path ) \n 
text = template_file . read ( ) \n 
text = text . replace ( , LITERAL1_text ) \n 
text = text . replace ( , KEYWORD1_text ) \n 
text = text . replace ( , KEYWORD2_text ) \n 
text = text . replace ( , KEYWORD3_text ) \n 
user_path = st_base . get_stino_user_path ( ) \n 
syntax_file = pyarduino . base . abs_file . File ( file_path ) \n 
syntax_file . write ( text ) \n 
~~ def create_sub_menus ( ) : \n 
st_menu . create_sketchbook_menu ( arduino_info ) \n 
st_menu . create_examples_menu ( arduino_info ) \n 
st_menu . create_libraries_menu ( arduino_info ) \n 
st_menu . create_boards_menu ( arduino_info ) \n 
st_menu . create_board_options_menu ( arduino_info ) \n 
st_menu . create_programmers_menu ( arduino_info ) \n 
st_menu . create_serials_menu ( ) \n 
st_menu . create_languages_menu ( ) \n 
~~ def create_menus ( ) : \n 
~~~ st_menu . create_main_menu ( ) \n 
settings = st_base . get_settings ( ) \n 
show_arduino_menu = settings . get ( , True ) \n 
if show_arduino_menu : \n 
~~~ st_menu . create_arduino_menu ( ) \n 
create_sub_menus ( ) \n 
~~~ user_menu_path = st_base . get_user_menu_path ( ) \n 
if os . path . isdir ( user_menu_path ) : \n 
~~~ shutil . rmtree ( user_menu_path ) \n 
~~ ~~ ~~ def update_menu ( ) : \n 
arduino_info . reload ( ) \n 
arduino_info . update ( ) \n 
i18n = st_base . get_i18n ( ) \n 
i18n . load ( ) \n 
create_menus ( ) \n 
create_completions ( ) \n 
create_syntax_file ( ) \n 
~~ def create_sketch ( sketch_name ) : \n 
sketchbook_dir = arduino_info . get_sketchbook_dir ( ) \n 
sketchbook_path = sketchbook_dir . get_path ( ) \n 
sketch_path = os . path . join ( sketchbook_path , sketch_name ) \n 
index = 0 \n 
org_name = sketch_name \n 
while os . path . exists ( sketch_path ) : \n 
~~~ sketch_name = % ( org_name , index ) \n 
index += 1 \n 
~~ os . makedirs ( sketch_path ) \n 
bare_gcc = settings . get ( , False ) \n 
if bare_gcc : \n 
~~~ ext = \n 
~~ template_file_name = + ext \n 
template_file_path = os . path . join ( preset_path , template_file_name ) \n 
template_file = pyarduino . base . abs_file . File ( template_file_path ) \n 
src_code = template_file . read ( ) \n 
src_file_name = sketch_name + ext \n 
src_file_path = os . path . join ( sketch_path , src_file_name ) \n 
src_file = pyarduino . base . abs_file . File ( src_file_path ) \n 
src_code = src_code . replace ( , src_file_name ) \n 
src_file . write ( src_code ) \n 
return sketch_path \n 
~~ def new_sketch ( window , sketch_name ) : \n 
~~~ sketch_path = create_sketch ( sketch_name ) \n 
open_sketch ( window , sketch_path ) \n 
~~ def open_sketch ( window , sketch_path ) : \n 
~~~ project = pyarduino . arduino_project . Project ( sketch_path ) \n 
ino_files = project . list_ino_files ( ) \n 
cpp_files = project . list_cpp_files ( ) \n 
h_files = project . list_h_files ( ) \n 
files = ino_files + cpp_files + h_files \n 
views = [ ] \n 
for f in files : \n 
~~~ view = window . open_file ( f . get_path ( ) ) \n 
views . append ( view ) \n 
~~ if views : \n 
~~~ window . focus_view ( views [ 0 ] ) \n 
~~ project_params = window . project_data ( ) \n 
if project_params is None : \n 
~~~ project_params = { } \n 
~~ folders = project_params . setdefault ( , [ ] ) \n 
folders . append ( { : True , : sketch_path } ) \n 
project_params [ ] = folders \n 
window . set_project_data ( project_params ) \n 
~~ def import_library ( view , edit , library_path ) : \n 
target_arch = arduino_info . get_target_board_info ( ) . get_target_arch ( ) \n 
library = pyarduino . arduino_library . Library ( library_path ) \n 
h_files = library . list_h_files ( target_arch ) \n 
region = sublime . Region ( 0 , view . size ( ) ) \n 
src_text = view . substr ( region ) \n 
headers = pyarduino . arduino_src . list_headers_from_src ( src_text ) \n 
h_files = [ f for f in h_files if not f . get_name ( ) in headers ] \n 
includes = [ % f . get_name ( ) for f in h_files ] \n 
text = . join ( includes ) \n 
if text : \n 
~~~ text += \n 
~~ view . insert ( edit , 0 , text ) \n 
~~ def handle_sketch ( view , func , using_programmer = False ) : \n 
~~~ window = view . window ( ) \n 
views = window . views ( ) \n 
if view not in views : \n 
~~~ view = window . active_view ( ) \n 
~~ if view . file_name ( ) is None : \n 
~~~ tmp_path = pyarduino . base . sys_path . get_tmp_path ( ) \n 
tmp_path = os . path . join ( tmp_path , ) \n 
name = str ( time . time ( ) ) . split ( ) [ 1 ] \n 
sketch_path = os . path . join ( tmp_path , name ) \n 
os . makedirs ( sketch_path ) \n 
~~ src_file_name = name + ext \n 
text = view . substr ( region ) \n 
src_file . write ( text ) \n 
view . set_scratch ( True ) \n 
window = view . window ( ) \n 
window . run_command ( ) \n 
view = window . open_file ( src_file_path ) \n 
~~ if view . is_dirty ( ) : \n 
~~~ view . run_command ( ) \n 
~~ file_path = view . file_name ( ) \n 
sketch_path = os . path . dirname ( file_path ) \n 
func ( view , sketch_path , using_programmer ) \n 
~~ def build_sketch ( view , sketch_path , using_programmer = False ) : \n 
console_name = + sketch_path + + str ( time . time ( ) ) \n 
console = st_console . Console ( window , name = console_name ) \n 
compiler = pyarduino . arduino_compiler . Compiler ( sketch_path , console ) \n 
compiler . build ( ) \n 
~~ def upload_sketch ( view , sketch_path , using_programmer ) : \n 
uploader = pyarduino . arduino_uploader . Uploader ( sketch_path , console ) \n 
uploader . upload ( using_programmer ) \n 
~~ def burn_bootloader ( window ) : \n 
~~~ console_name = + str ( time . time ( ) ) \n 
bootloader = pyarduino . arduino_bootloader . Bootloader ( console ) \n 
bootloader . burn ( ) \n 
~~ def change_board ( window , board_id ) : \n 
arduino_info . change_board ( board_id ) \n 
view = window . active_view ( ) \n 
set_status ( view ) \n 
~~ def change_sub_board ( window , option_index , sub_board_id ) : \n 
arduino_info . change_sub_board ( option_index , sub_board_id ) \n 
~~ def change_programmer ( programmer_id ) : \n 
arduino_info . change_programmer ( programmer_id ) \n 
~~ def archive_sketch ( window , sketch_path ) : \n 
~~~ sketch_name = os . path . basename ( sketch_path ) \n 
console = st_console . Console ( window , str ( time . time ( ) ) ) \n 
message_queue = pyarduino . base . message_queue . MessageQueue ( console ) \n 
message_queue . put ( , sketch_name ) \n 
zip_file_name = sketch_name + \n 
document_path = pyarduino . base . sys_path . get_document_path ( ) \n 
zip_file_path = os . path . join ( document_path , zip_file_name ) \n 
os . chdir ( sketch_path ) \n 
sketch_dir = pyarduino . base . abs_file . Dir ( sketch_path ) \n 
files = sketch_dir . list_files ( ) \n 
file_names = [ f . get_name ( ) for f in files ] \n 
~~~ opened_zipfile = zipfile . ZipFile ( zip_file_path , \n 
, zipfile . ZIP_DEFLATED ) \n 
~~~ text = \n 
message_queue . put ( text ) \n 
~~~ for file_name in file_names : \n 
~~~ opened_zipfile . write ( file_name ) \n 
~~ opened_zipfile . close ( ) \n 
message_queue . put ( , zip_file_path ) \n 
~~ message_queue . print_screen ( one_time = True ) \n 
~~ def get_url ( url ) : \n 
ide_path = ide_dir . get_path ( ) \n 
file_name = url + \n 
reference_folder = os . path . join ( ide_path , ) \n 
reference_file = os . path . join ( reference_folder , file_name ) \n 
if os . path . isfile ( reference_file ) : \n 
~~~ reference_file = reference_file . replace ( os . path . sep , ) \n 
url = + reference_file \n 
~~~ url = \n 
~~ return url \n 
~~ def find_in_ref ( view ) : \n 
id_keyword_dict = ide_dir . get_id_keyword_dict ( ) \n 
ref_list = [ ] \n 
selected_text = get_selected_text_from_view ( view ) \n 
words = get_word_list_from_text ( selected_text ) \n 
for word in words : \n 
~~~ if word in id_keyword_dict : \n 
~~~ keyword = id_keyword_dict . get ( word ) \n 
ref = keyword . get_ref ( ) \n 
if ref and not ref in ref_list : \n 
~~~ ref_list . append ( ref ) \n 
~~ ~~ ~~ for ref in ref_list : \n 
~~~ url = get_url ( ref ) \n 
sublime . run_command ( , { : url } ) \n 
~~ ~~ def get_selected_text_from_view ( view ) : \n 
~~~ selected_text = \n 
region_list = view . sel ( ) \n 
for region in region_list : \n 
~~~ selected_region = view . word ( region ) \n 
selected_text += view . substr ( selected_region ) \n 
selected_text += \n 
~~ return selected_text \n 
~~ def get_word_list_from_text ( text ) : \n 
~~~ pattern_text = \n 
word_list = re . findall ( pattern_text , text ) \n 
return word_list \n 
~~ def is_arduino_ide_path ( dir_path ) : \n 
~~~ path = pyarduino . arduino_root . update_ide_path ( dir_path ) \n 
return pyarduino . arduino_root . is_arduino_ide_path ( path ) \n 
~~ def set_arduino_ide_path ( window , dir_path ) : \n 
~~~ if is_arduino_ide_path ( dir_path ) : \n 
arduino_info . change_ide_path ( dir_path ) \n 
version_name = ide_dir . get_version_name ( ) \n 
text = \n 
message_queue . put ( text , version_name , dir_path ) \n 
message_queue . print_screen ( one_time = True ) \n 
return 0 \n 
~~ ~~ def set_sketchbook_path ( window , dir_path ) : \n 
arduino_info . change_sketchbook_path ( dir_path ) \n 
message_queue . put ( text , dir_path ) \n 
~~ def set_build_path ( window , dir_path ) : \n 
~~~ settings = st_base . get_settings ( ) \n 
settings . set ( , dir_path ) \n 
~~ def select_arduino_dir ( window ) : \n 
~~~ select_dir ( window , func = set_arduino_ide_path , \n 
condition_func = is_arduino_ide_path ) \n 
~~ def change_sketchbook_dir ( window ) : \n 
~~~ select_dir ( window , func = set_sketchbook_path , is_user = True ) \n 
~~ def change_build_dir ( window ) : \n 
~~~ select_dir ( window , func = set_build_path , is_user = True ) \n 
~~ def select_dir ( window , index = - 2 , level = 0 , paths = None , \n 
func = None , condition_func = None , is_user = False ) : \n 
~~~ if index == - 1 : \n 
~~ if level > 0 and index == 0 : \n 
~~~ sel_path = paths [ 0 ] . split ( ) [ 1 ] [ : - 1 ] \n 
if func : \n 
~~~ return_code = func ( window , sel_path ) \n 
if return_code == 0 : \n 
~~~ if index == 1 : \n 
~~~ level -= 1 \n 
~~ elif index > 1 : \n 
~~~ level += 1 \n 
~~ if level <= 0 : \n 
~~~ level = 0 \n 
dir_path = \n 
parent_path = \n 
if is_user : \n 
~~~ paths = pyarduino . base . sys_path . list_user_root_path ( ) \n 
~~~ paths = pyarduino . base . sys_path . list_os_root_path ( ) \n 
~~~ sel_path = paths [ index ] \n 
if sel_path == pyarduino . base . sys_path . ROOT_PATH : \n 
~~~ sel_path = \n 
~~ dir_path = os . path . abspath ( sel_path ) \n 
if condition_func and condition_func ( dir_path ) : \n 
~~~ func ( window , dir_path ) \n 
~~ parent_path = os . path . join ( dir_path , ) \n 
cur_dir = pyarduino . base . abs_file . Dir ( dir_path ) \n 
sub_dirs = cur_dir . list_dirs ( ) \n 
paths = [ d . get_path ( ) for d in sub_dirs ] \n 
~~ paths . insert ( 0 , parent_path ) \n 
paths . insert ( 0 , % dir_path ) \n 
~~ sublime . set_timeout ( lambda : window . show_quick_panel ( \n 
paths , lambda index : select_dir ( window , index , level , paths , \n 
func , condition_func , is_user ) ) , 5 ) \n 
~~ def update_serial_info ( ) : \n 
~~~ st_menu . create_serials_menu ( ) \n 
window = sublime . active_window ( ) \n 
~~ def get_serial_listener ( ) : \n 
~~~ serial_listener = pyarduino . base . serial_listener . SerialListener ( \n 
func = update_serial_info ) \n 
return serial_listener \n 
~~ def toggle_serial_monitor ( window ) : \n 
~~~ monitor_module = pyarduino . base . serial_monitor \n 
serial_monitor = None \n 
serial_port = settings . get ( , ) \n 
serial_ports = pyarduino . base . serial_port . list_serial_ports ( ) \n 
if serial_port in serial_ports : \n 
~~~ if serial_port in monitor_module . serials_in_use : \n 
~~~ serial_monitor = monitor_module . serial_monitor_dict . get ( \n 
serial_port , None ) \n 
~~ if not serial_monitor : \n 
~~~ monitor_view = st_console . MonitorView ( window , serial_port ) \n 
serial_monitor = pyarduino . base . serial_monitor . SerialMonitor ( \n 
serial_port , monitor_view ) \n 
~~ if not serial_monitor . is_running ( ) : \n 
~~~ serial_monitor . start ( ) \n 
if not serial_port in monitor_module . serials_in_use : \n 
~~~ monitor_module . serials_in_use . append ( serial_port ) \n 
~~ monitor_module . serial_monitor_dict [ serial_port ] = serial_monitor \n 
~~~ serial_monitor . stop ( ) \n 
monitor_module . serials_in_use . remove ( serial_port ) \n 
~~ ~~ ~~ def send_serial_message ( text ) : \n 
if serial_port in monitor_module . serials_in_use : \n 
if serial_monitor and serial_monitor . is_running ( ) : \n 
~~~ serial_monitor . send ( text ) \n 
~~ ~~ ~~ def set_status ( view ) : \n 
~~~ infos = [ ] \n 
exts = [ , , , , ] \n 
file_name = view . file_name ( ) \n 
if file_name and file_name . split ( ) [ - 1 ] in exts : \n 
version_name = arduino_info . get_ide_dir ( ) . get_version_name ( ) \n 
version_text = % version_name \n 
infos . append ( version_text ) \n 
target_board_info = arduino_info . get_target_board_info ( ) \n 
target_board = target_board_info . get_target_board ( ) \n 
if target_board : \n 
~~~ target_board_caption = target_board . get_caption ( ) \n 
infos . append ( target_board_caption ) \n 
if target_board . has_options ( ) : \n 
~~~ target_sub_boards = target_board_info . get_target_sub_boards ( ) \n 
for index , target_sub_board in enumerate ( target_sub_boards ) : \n 
~~~ caption_text = target_sub_board . get_caption ( ) \n 
if index == 0 : \n 
~~~ caption_text = + caption_text \n 
~~ if index == len ( target_sub_boards ) - 1 : \n 
~~~ caption_text += \n 
~~ infos . append ( caption_text ) \n 
~~~ target_board_caption = \n 
~~ settings = st_base . get_settings ( ) \n 
target_serial_port = settings . get ( , ) \n 
if not target_serial_port in serial_ports : \n 
~~~ target_serial_port = \n 
~~ serial_text = % target_serial_port \n 
infos . append ( serial_text ) \n 
text = . join ( infos ) \n 
view . set_status ( , text ) \n 
~~ ~~ def show_items_panel ( window , item_type ) : \n 
~~~ sublime . set_timeout ( lambda : window . show_quick_panel ( [ , ] , ppp ) ) \n 
~~ def ppp ( index ) : \n 
~~~ print ( index ) \n 
~~ VERSION = \n 
if os . name == : \n 
~~~ from . serialwin32 import * \n 
~~ elif os . name == : \n 
~~~ from . serialposix import * \n 
~~ protocol_handler_packages = [ \n 
def serial_for_url ( url , * args , ** kwargs ) : \n 
do_open = not in kwargs or not kwargs [ ] \n 
if in kwargs : del kwargs [ ] \n 
klass = Serial \n 
~~~ url_nocase = url . lower ( ) \n 
~~~ if in url_nocase : \n 
~~~ protocol = url_nocase . split ( , 1 ) [ 0 ] \n 
for package_name in protocol_handler_packages : \n 
~~~ module_name = % ( package_name , protocol , ) \n 
~~~ handler_module = __import__ ( module_name ) \n 
~~~ klass = sys . modules [ module_name ] . Serial \n 
~~~ raise ValueError ( % ( protocol , ) ) \n 
~~~ klass = Serial \n 
~~ ~~ instance = klass ( None , * args , ** kwargs ) \n 
instance . port = url \n 
if do_open : \n 
~~~ instance . open ( ) \n 
~~ return instance \n 
~~ import director \n 
import director . objectmodel as om \n 
from director import visualization as vis \n 
from director . visualization import PolyDataItem \n 
from director import filterUtils \n 
from director import ioUtils \n 
from director import meshmanager \n 
from director import transformUtils \n 
from director . uuidutil import newUUID \n 
from director . debugVis import DebugData \n 
from director import vtkAll as vtk \n 
import numpy as np \n 
import uuid \n 
class AffordanceItem ( PolyDataItem ) : \n 
LOCAL_PROPERTY_NAMES = ( ) \n 
def __init__ ( self , name , polyData , view ) : \n 
~~~ PolyDataItem . __init__ ( self , name , polyData , view ) \n 
self . params = { } \n 
self . addProperty ( , newUUID ( ) , attributes = om . PropertyAttributes ( hidden = True ) ) \n 
self . addProperty ( , True ) \n 
self . addProperty ( , [ 0.0 , 0.0 , 0.0 , 1.0 , 0.0 , 0.0 , 0.0 ] , attributes = om . PropertyAttributes self . addProperty ( , False ) \n 
self . properties . setPropertyIndex ( , 0 ) \n 
self . setProperty ( , om . Icons . Hammer ) \n 
~~ def getPose ( self ) : \n 
~~~ childFrame = self . getChildFrame ( ) \n 
t = childFrame . transform if childFrame else vtk . vtkTransform ( ) \n 
return transformUtils . poseFromTransform ( t ) \n 
~~ def getDescription ( self ) : \n 
~~~ d = OrderedDict ( ) \n 
d [ ] = type ( self ) . __name__ \n 
d . update ( self . properties . _properties ) \n 
d [ ] = self . getPose ( ) \n 
~~ def _onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ PolyDataItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
if propertyName == : \n 
~~~ self . updateGeometryFromProperties ( ) \n 
~~ ~~ def updateGeometryFromProperties ( ) : \n 
~~ def setPolyData ( self , polyData ) : \n 
~~~ if polyData . GetNumberOfPoints ( ) : \n 
~~~ originPose = self . getProperty ( ) \n 
pos , quat = originPose [ : 3 ] , originPose [ 3 : ] \n 
t = transformUtils . transformFromPose ( pos , quat ) \n 
polyData = filterUtils . transformPolyData ( polyData , t . GetLinearInverse ( ) ) \n 
~~ PolyDataItem . setPolyData ( self , polyData ) \n 
~~ def repositionFromDescription ( self , desc ) : \n 
~~~ position , quat = desc [ ] \n 
t = transformUtils . transformFromPose ( position , quat ) \n 
self . getChildFrame ( ) . copyFrame ( t ) \n 
~~ def loadDescription ( self , desc , copyMode = COPY_MODE_ALL ) : \n 
~~~ self . syncProperties ( desc , copyMode ) \n 
self . repositionFromDescription ( desc ) \n 
self . _renderAllViews ( ) \n 
~~ def syncProperties ( self , desc , copyMode = COPY_MODE_ALL ) : \n 
~~~ for propertyName , propertyValue in desc . iteritems ( ) : \n 
~~~ if copyMode == self . COPY_MODE_SKIP_LOCAL : \n 
~~~ if propertyName in self . LOCAL_PROPERTY_NAMES : \n 
~~ ~~ if self . hasProperty ( propertyName ) and ( self . getProperty ( propertyName ) != propertyValue ) : \n 
~~~ self . setProperty ( propertyName , propertyValue ) \n 
~~ ~~ ~~ def onRemoveFromObjectModel ( self ) : \n 
~~~ PolyDataItem . onRemoveFromObjectModel ( self ) \n 
~~ ~~ class BoxAffordanceItem ( AffordanceItem ) : \n 
~~~ def __init__ ( self , name , view ) : \n 
~~~ AffordanceItem . __init__ ( self , name , vtk . vtkPolyData ( ) , view ) \n 
self . addProperty ( , [ 0.25 , 0.25 , 0.25 ] , attributes = om . PropertyAttributes ( decimals self . addProperty ( , 0 , attributes = om . PropertyAttributes ( minimum = 0 , maximum = 1000 self . properties . setPropertyIndex ( , 0 ) \n 
self . properties . setPropertyIndex ( , 1 ) \n 
self . updateGeometryFromProperties ( ) \n 
~~ def updateGeometryFromProperties ( self ) : \n 
~~~ d = DebugData ( ) \n 
d . addCube ( self . getProperty ( ) , ( 0 , 0 , 0 ) , subdivisions = self . getProperty ( self . setPolyData ( d . getPolyData ( ) ) \n 
~~~ AffordanceItem . _onPropertyChanged ( self , propertySet , propertyName ) \n 
if propertyName in ( , ) : \n 
~~ ~~ ~~ class SphereAffordanceItem ( AffordanceItem ) : \n 
self . addProperty ( , 0.15 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . properties . setPropertyIndex ( , 0 ) \n 
d . addSphere ( ( 0 , 0 , 0 ) , self . getProperty ( ) ) \n 
self . setPolyData ( d . getPolyData ( ) ) \n 
~~ ~~ ~~ class CylinderAffordanceItem ( AffordanceItem ) : \n 
self . addProperty ( , 0.03 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . addProperty ( , 0.5 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . properties . setPropertyIndex ( , 0 ) \n 
length = self . getProperty ( ) \n 
d . addCylinder ( center = ( 0 , 0 , 0 ) , axis = ( 0 , 0 , 1 ) , length = self . getProperty ( ) , radius = self . getProperty self . setPolyData ( d . getPolyData ( ) ) \n 
~~ ~~ ~~ class CapsuleAffordanceItem ( AffordanceItem ) : \n 
d . addCapsule ( center = ( 0 , 0 , 0 ) , axis = ( 0 , 0 , 1 ) , length = self . getProperty ( ) , radius = self . getProperty self . setPolyData ( d . getPolyData ( ) ) \n 
~~ ~~ ~~ class CapsuleRingAffordanceItem ( AffordanceItem ) : \n 
self . setProperty ( , False ) \n 
self . addProperty ( , 0.15 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 0.01 self . addProperty ( , 0.02 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep self . addProperty ( , 8 , attributes = om . PropertyAttributes ( decimals = 3 , singleStep = 1 , minimum \n 
self . properties . setPropertyIndex ( , 2 ) \n 
~~~ radius = self . getProperty ( ) \n 
circlePoints = np . linspace ( 0 , 2 * np . pi , self . getProperty ( ) + 1 ) \n 
spokes = [ ( 0.0 , np . sin ( x ) , np . cos ( x ) ) for x in circlePoints ] \n 
spokes = [ radius * np . array ( x ) / np . linalg . norm ( x ) for x in spokes ] \n 
d = DebugData ( ) \n 
for a , b in zip ( spokes , spokes [ 1 : ] ) : \n 
~~~ d . addCapsule ( center = ( a + b ) / 2.0 , axis = ( b - a ) , length = np . linalg . norm ( b - a ) , radius = self . getProperty ~~ self . setPolyData ( d . getPolyData ( ) ) \n 
if propertyName in ( , , ) : \n 
~~ ~~ ~~ class MeshAffordanceItem ( AffordanceItem ) : \n 
~~~ _meshManager = None \n 
def __init__ ( self , name , view ) : \n 
self . addProperty ( , ) \n 
if self . getProperty ( ) and not self . polyData . GetNumberOfPoints ( ) : \n 
~~ ~~ def updateGeometryFromProperties ( self ) : \n 
~~~ filename = self . getProperty ( ) \n 
if not filename : \n 
~~~ polyData = vtk . vtkPolyData ( ) \n 
~~~ polyData = self . getMeshManager ( ) . get ( filename ) \n 
~~ if not polyData : \n 
~~~ if not os . path . isabs ( filename ) : \n 
~~~ filename = os . path . join ( director . getDRCBaseDir ( ) , filename ) \n 
~~ if os . path . isfile ( filename ) : \n 
~~~ polyData = ioUtils . readPolyData ( filename ) \n 
d . addFrame ( vtk . vtkTransform ( ) , scale = 0.1 , tubeRadius = 0.005 ) \n 
polyData = d . getPolyData ( ) \n 
~~ ~~ self . setPolyData ( polyData ) \n 
def getMeshManager ( cls ) : \n 
~~~ if cls . _meshManager is None : \n 
~~~ cls . _meshManager = meshmanager . MeshManager ( ) \n 
~~ return cls . _meshManager \n 
def promotePolyDataItem ( cls , obj ) : \n 
~~~ parent = obj . parent ( ) \n 
view = obj . views [ 0 ] \n 
name = obj . getProperty ( ) \n 
polyData = obj . polyData \n 
props = obj . properties . _properties \n 
childFrame = obj . getChildFrame ( ) \n 
if childFrame : \n 
~~~ t = transformUtils . copyFrame ( childFrame . transform ) \n 
~~~ t = vtk . vtkTransform ( ) \n 
t . PostMultiply ( ) \n 
t . Translate ( filterUtils . computeCentroid ( polyData ) ) \n 
~~ children = [ c for c in obj . children ( ) if c is not childFrame ] \n 
meshId = cls . getMeshManager ( ) . add ( polyData ) \n 
om . removeFromObjectModel ( obj ) \n 
obj = MeshAffordanceItem ( name , view ) \n 
obj . setProperty ( , meshId ) \n 
om . addToObjectModel ( obj , parentObj = parent ) \n 
frame = vis . addChildFrame ( obj ) \n 
frame . copyFrame ( t ) \n 
for child in children : \n 
~~~ om . addToObjectModel ( child , parentObj = obj ) \n 
~~ obj . syncProperties ( props ) \n 
return obj \n 
~~ ~~ ~~ class FrameAffordanceItem ( AffordanceItem ) : \n 
~~~ def setAffordanceParams ( self , params ) : \n 
~~~ self . params = params \n 
~~ def updateParamsFromActorTransform ( self ) : \n 
~~~ t = self . actor . GetUserTransform ( ) \n 
xaxis = np . array ( t . TransformVector ( [ 1 , 0 , 0 ] ) ) \n 
yaxis = np . array ( t . TransformVector ( [ 0 , 1 , 0 ] ) ) \n 
zaxis = np . array ( t . TransformVector ( [ 0 , 0 , 1 ] ) ) \n 
self . params [ ] = xaxis \n 
self . params [ ] = yaxis \n 
self . params [ ] = zaxis \n 
self . params [ ] = t . GetPosition ( ) \n 
import vtkAll as vtk \n 
import math \n 
import types \n 
import functools \n 
from director import lcmUtils \n 
from director . timercallback import TimerCallback \n 
from director . asynctaskqueue import AsyncTaskQueue \n 
from director . fieldcontainer import FieldContainer \n 
from director import objectmodel as om \n 
from director import applogic as app \n 
from director import ik \n 
from director . ikparameters import IkParameters \n 
from director import ikplanner \n 
from director import affordanceitems \n 
from director . simpletimer import SimpleTimer \n 
from director . utime import getUtime \n 
from director import robotstate \n 
from director import robotplanlistener \n 
from director import segmentation \n 
from director import planplayback \n 
from director . footstepsdriver import FootstepRequestGenerator \n 
from director . tasks . taskuserpanel import TaskUserPanel \n 
from director . tasks . taskuserpanel import ImageBasedAffordanceFit \n 
import director . tasks . robottasks as rt \n 
import director . tasks . taskmanagerwidget as tmw \n 
import drc as lcmdrc \n 
from PythonQt import QtCore , QtGui \n 
class DoorDemo ( object ) : \n 
~~~ def __init__ ( self , robotModel , footstepPlanner , manipPlanner , ikPlanner , lhandDriver , rhandDriver ~~~ self . robotModel = robotModel \n 
self . footstepPlanner = footstepPlanner \n 
self . manipPlanner = manipPlanner \n 
self . ikPlanner = ikPlanner \n 
self . lhandDriver = lhandDriver \n 
self . rhandDriver = rhandDriver \n 
self . atlasDriver = atlasDriver \n 
self . multisenseDriver = multisenseDriver \n 
self . affordanceFitFunction = affordanceFitFunction \n 
self . sensorJointController = sensorJointController \n 
self . planPlaybackFunction = planPlaybackFunction \n 
self . showPoseFunction = showPoseFunction \n 
self . graspingHand = \n 
self . endPose = None \n 
self . planFromCurrentRobotState = True \n 
self . visOnly = False \n 
self . useFootstepPlanner = False \n 
self . userPromptEnabled = True \n 
self . constraintSet = None \n 
self . plans = [ ] \n 
self . usePinchGrasp = False \n 
self . pinchDistance = 0.1 \n 
self . doorHandleFrame = None \n 
self . doorHandleGraspFrame = None \n 
self . doorHingeFrame = None \n 
self . handleTouchHeight = 0.0 \n 
self . handleTouchDepth = - 0.08 \n 
self . handleTouchWidth = 0.06 \n 
self . handleReachAngle = 20 \n 
self . handleTurnHeight = - 0.08 \n 
self . handleTurnWidth = 0.01 \n 
self . handleTurnAngle = 60 \n 
self . handleLiftHeight = 0.12 \n 
self . handlePushDepth = 0.0 \n 
self . handlePushAngle = 2 \n 
self . handleOpenDepth = 0.1 \n 
self . handleOpenWidth = 0.4 \n 
self . speedHigh = 60 \n 
self . speedLow = 15 \n 
self . setFootstepThroughDoorParameters ( ) \n 
self . setChopParametersToDefaults ( ) \n 
~~ def setChopParametersToDefaults ( self ) : \n 
~~~ self . preChopDepth = - 0.06 \n 
self . preChopWidth = - 0.08 \n 
self . preChopHeight = 0.10 \n 
self . chopDistance = - 0.15 \n 
self . chopSidewaysDistance = 0.03 \n 
~~ def addPlan ( self , plan ) : \n 
~~~ self . plans . append ( plan ) \n 
~~ def computeGraspOrientation ( self ) : \n 
~~~ return [ 180 + self . handleReachAngle , 0 , 90 ] \n 
~~ def computeGroundFrame ( self , robotModel ) : \n 
t1 = robotModel . getLinkFrame ( ) \n 
t2 = robotModel . getLinkFrame ( ) \n 
pelvisT = robotModel . getLinkFrame ( ) \n 
xaxis = [ 1.0 , 0.0 , 0.0 ] \n 
pelvisT . TransformVector ( xaxis , xaxis ) \n 
xaxis = np . array ( xaxis ) \n 
zaxis = np . array ( [ 0.0 , 0.0 , 1.0 ] ) \n 
yaxis = np . cross ( zaxis , xaxis ) \n 
yaxis /= np . linalg . norm ( yaxis ) \n 
xaxis = np . cross ( yaxis , zaxis ) \n 
stancePosition = ( np . array ( t2 . GetPosition ( ) ) + np . array ( t1 . GetPosition ( ) ) ) / 2.0 \n 
footHeight = 0.0811 \n 
t = transformUtils . getTransformFromAxes ( xaxis , yaxis , zaxis ) \n 
t . Translate ( stancePosition ) \n 
t . Translate ( [ 0.0 , 0.0 , - footHeight ] ) \n 
return t \n 
~~ def computeDoorHandleGraspFrame ( self ) : \n 
~~~ doorSide = 1 if self . graspingHand == else - 1 \n 
graspOrientation = self . computeGraspOrientation ( ) \n 
self . doorHandleAxisFrame = self . computeDoorHandleAxisFrame ( ) \n 
def makeFrame ( name , offset , turnAngle = 0 ) : \n 
~~~ t = transformUtils . frameFromPositionAndRPY ( offset , graspOrientation ) \n 
t . Concatenate ( transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , [ - turnAngle , 0 , 0 ] t . Concatenate ( transformUtils . copyFrame ( self . doorHandleAxisFrame . transform ) ) \n 
return vis . updateFrame ( t , name , parent = self . doorHandleAffordance , visible = False , scale = 0.2 \n 
~~ def makeFrameNew ( name , transforms ) : \n 
~~~ t = transformUtils . concatenateTransforms ( transforms ) \n 
~~ graspToAxisTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
graspOrientation ) \n 
self . doorHandleGraspFrame = makeFrameNew ( , \n 
[ graspToAxisTransform , \n 
self . doorHandleAxisFrame . transform ] ) \n 
if self . usePinchGrasp : \n 
~~~ reachToGraspTransform = transformUtils . frameFromPositionAndRPY ( [ - doorSide * self . handleTouchWidth self . handleTouchDepth , \n 
- self . handleTouchHeight ] [ 0.0 , 0.0 , 0.0 ] ) \n 
self . doorHandleReachFrame = makeFrameNew ( , [ reachToGraspTransform self . doorHandleGraspFrame \n 
handleTurnTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , [ - self . handleTurnAngle self . doorHandleTurnFrame = makeFrameNew ( , [ reachToGraspTransform graspToAxisTransform , \n 
handleTurnTransform , \n 
self . doorHandleAxisFrame \n 
handlePushTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
[ 0 , 0 , self . handlePushAngle ] self . doorHandlePushFrame = makeFrameNew ( , \n 
[ self . doorHandleTurnFrame . transform , \n 
self . doorHingeFrame . transform . GetInverse ( ) , \n 
handlePushTransform , \n 
self . doorHingeFrame . transform ] ) \n 
self . doorHandlePushLiftFrame = makeFrameNew ( , \n 
[ self . doorHandleReachFrame . transform , \n 
self . doorHandlePushLiftAxisFrame = makeFrameNew ( , \n 
[ self . doorHandleAxisFrame . transform , \n 
self . doorHandlePushOpenFrame = makeFrame ( , [ self . handleOpenDepth \n 
t = vtk . vtkTransform ( ) \n 
t . RotateX ( 25 ) \n 
t . Concatenate ( self . doorHandlePushOpenFrame . transform ) \n 
self . doorHandlePushOpenFrame . copyFrame ( t ) \n 
~~~ reachToAxisTransform = transformUtils . frameFromPositionAndRPY ( [ self . preChopDepth , \n 
doorSide * self . preChopWidth , \n 
self . preChopHeight ] , \n 
[ 0 , 90 , - 90 ] ) \n 
obj = om . findObjectByName ( ) \n 
self . doorHandleReachFrame = makeFrameNew ( , \n 
[ reachToAxisTransform , self . doorHandleAxisFrame \n 
if not obj : \n 
~~~ obj = self . doorHandleReachFrame \n 
obj . setProperty ( , True ) \n 
rep = obj . widget . GetRepresentation ( ) \n 
rep . SetRotateAxisEnabled ( 0 , False ) \n 
rep . SetRotateAxisEnabled ( 1 , False ) \n 
rep . SetRotateAxisEnabled ( 2 , False ) \n 
obj . widget . HandleRotationEnabledOff ( ) \n 
obj . setProperty ( , False ) \n 
~~ preChopToReachTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , \n 
- 0.15 , \n 
0.0 ] , \n 
[ 0 , 0 , 0 ] ) \n 
self . doorHandlePreChopFrame = makeFrameNew ( , \n 
[ preChopToReachTransform , self . doorHandleReachFrame \n 
~~ self . doorHandleFrame . frameSync = vis . FrameSync ( ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleFrame ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleAxisFrame ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleGraspFrame , ignoreIncoming = True ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandleReachFrame , ignoreIncoming = True ) \n 
~~~ self . doorHandleFrame . frameSync . addFrame ( self . doorHandleTurnFrame , ignoreIncoming = True ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushFrame , ignoreIncoming = True ) \n 
self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushLiftFrame , ignoreIncoming = True self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushLiftAxisFrame , ignoreIncoming self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePushOpenFrame , ignoreIncoming = True ~~ else : \n 
~~~ self . doorHandleFrame . frameSync . addFrame ( self . doorHandlePreChopFrame , ignoreIncoming = True \n 
~~ ~~ def computeDoorHandleAxisFrame ( self ) : \n 
~~~ handleLength = self . doorHandleAffordance . getProperty ( ) [ 1 ] \n 
doorSide = 1 if self . graspingHand == else - 1 \n 
t = transformUtils . frameFromPositionAndRPY ( [ 0.0 , doorSide * handleLength / 2.0 , 0.0 ] , [ 0 , 0 , 0 ] ) t . PostMultiply ( ) \n 
t . Concatenate ( transformUtils . copyFrame ( self . doorHandleFrame . transform ) ) \n 
return vis . updateFrame ( t , , parent = self . doorHandleAffordance , \n 
visible = False , scale = 0.2 ) \n 
~~ def computeDoorHingeFrame ( self ) : \n 
doorAffordance = om . findObjectByName ( ) \n 
doorDimensions = doorAffordance . getProperty ( ) \n 
doorDepth = doorDimensions [ 0 ] \n 
doorWidth = doorDimensions [ 1 ] \n 
t = transformUtils . frameFromPositionAndRPY ( [ doorDepth / 2 , - doorSide * doorWidth / 2.0 , 0.0 ] , [ 0 , t . PostMultiply ( ) \n 
t . Concatenate ( transformUtils . copyFrame ( doorAffordance . getChildFrame ( ) . transform ) ) \n 
self . doorHingeFrame = vis . updateFrame ( t , , parent = doorAffordance , \n 
return self . doorHingeFrame \n 
~~ def computeDoorHandleStanceFrame ( self ) : \n 
~~~ graspFrame = self . doorHandleFrame . transform \n 
groundFrame = self . computeGroundFrame ( self . robotModel ) \n 
groundHeight = groundFrame . GetPosition ( ) [ 2 ] \n 
graspPosition = np . array ( graspFrame . GetPosition ( ) ) \n 
yaxis = [ 0.0 , 1.0 , 0.0 ] \n 
zaxis = [ 0 , 0 , 1 ] \n 
graspFrame . TransformVector ( xaxis , xaxis ) \n 
graspFrame . TransformVector ( yaxis , yaxis ) \n 
graspGroundFrame = transformUtils . getTransformFromAxes ( xaxis , yaxis , zaxis ) \n 
graspGroundFrame . PostMultiply ( ) \n 
graspGroundFrame . Translate ( graspPosition [ 0 ] , graspPosition [ 1 ] , groundHeight ) \n 
position = [ - 0.77 , 0.4 , 0.0 ] \n 
rpy = [ 0 , 0 , - 20 ] \n 
t = transformUtils . frameFromPositionAndRPY ( position , rpy ) \n 
t . Concatenate ( graspGroundFrame ) \n 
self . doorHandleStanceFrame = vis . updateFrame ( t , , parent = self . doorHandleAffordance #self.frameSync.addFrame(self.doorHandleStanceFrame) \n 
~~ def moveRobotToStanceFrame ( self ) : \n 
~~~ frame = self . doorHandleStanceFrame . transform \n 
self . sensorJointController . setPose ( ) \n 
stancePosition = frame . GetPosition ( ) \n 
stanceOrientation = frame . GetOrientation ( ) \n 
self . sensorJointController . q [ : 2 ] = [ stancePosition [ 0 ] , stancePosition [ 1 ] ] \n 
self . sensorJointController . q [ 5 ] = math . radians ( stanceOrientation [ 2 ] ) \n 
self . sensorJointController . push ( ) \n 
~~ def planNominal ( self ) : \n 
~~~ startPose = self . getPlanningStartPose ( ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , ) \n 
endPose , info = self . ikPlanner . computeStandPose ( endPose ) \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose ) \n 
self . addPlan ( newPlan ) \n 
~~ def planPreReach ( self ) : \n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedHigh ) \n 
nonGraspingHand = if self . graspingHand == else \n 
startPose = self . getPlanningStartPose ( ) \n 
~~~ endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , \n 
side = self . graspingHand ) \n 
~~ endPose = self . ikPlanner . getMergedPostureFromDatabase ( endPose , , \n 
side = nonGraspingHand ) \n 
endPose , info = self . ikPlanner . computeStandPose ( endPose , ikParameters = ikParameters ) \n 
newPlan = self . ikPlanner . computePostureGoal ( startPose , endPose , ikParameters = ikParameters ) \n 
~~ def planUnReach ( self ) : \n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedLow ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , \n 
~~ def planTuckArms ( self ) : \n 
otherSide = if self . graspingHand == else \n 
standPose , info = self . ikPlanner . computeStandPose ( startPose , ikParameters = ikParameters ) \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side = otherSide a = 0.25 \n 
q2 = ( 1.0 - a ) * np . array ( standPose ) + a * q2 \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( q2 , , , side = self . graspingHand a = 0.75 \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side endPose = self . ikPlanner . getMergedPostureFromDatabase ( endPose , , , side = \n 
newPlan = self . ikPlanner . computeMultiPostureGoal ( [ startPose , q2 , endPose ] , ikParameters = ikParameters self . addPlan ( newPlan ) \n 
~~ def planTuckArmsPrePush ( self ) : \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( standPose , , , side = self a = 0.25 \n 
q2 = self . ikPlanner . getMergedPostureFromDatabase ( q2 , , , side = otherSide ) a = 0.75 \n 
~~ def planChop ( self , deltaZ = None , deltaY = None , deltaX = None ) : \n 
if deltaZ is None : \n 
~~~ deltaZ = self . chopDistance \n 
~~ if deltaY is None : \n 
~~~ deltaY = self . chopSidewaysDistance \n 
~~ if deltaX is None : \n 
~~~ deltaX = 0.0 \n 
~~ linkOffsetFrame = self . ikPlanner . getPalmToHandLink ( self . graspingHand ) \n 
handLinkName = self . ikPlanner . getHandLink ( self . graspingHand ) \n 
startFrame = self . ikPlanner . getLinkFrameAtPose ( handLinkName , startPose ) \n 
endToStartTransform = transformUtils . frameFromPositionAndRPY ( [ deltaZ , - deltaX , - deltaY ] , \n 
endFrame = transformUtils . concatenateTransforms ( [ endToStartTransform , startFrame ] ) ; \n 
vis . updateFrame ( endFrame , , parent = self . doorHandleAffordance , visible = False , scale palmToWorld1 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , startFrame ] ) \n 
palmToWorld2 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , endFrame ] ) \n 
constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , palmToWorld2 constraintSet . nominalPoseName = \n 
constraintSet . ikParameters = IkParameters ( usePointwise = False , \n 
maxDegreesPerSecond = self . speedLow , \n 
numberOfAddedKnots = 2 ) \n 
endPose , info = constraintSet . runIk ( ) \n 
motionVector = np . array ( palmToWorld2 . GetPosition ( ) ) - np . array ( palmToWorld1 . GetPosition ( ) ) \n 
motionTargetFrame = transformUtils . getTransformFromOriginAndNormal ( np . array ( palmToWorld2 . GetPosition \n 
p = self . ikPlanner . createLinePositionConstraint ( handLinkName , linkOffsetFrame , motionTargetFrame constraintSet . constraints . append ( p ) \n 
plan = constraintSet . runIkTraj ( ) \n 
self . addPlan ( plan ) \n 
~~ def stopPushing ( self ) : \n 
~~~ startPose = self . getPlanningStartPose \n 
plan = self . robotSystem . ikPlanner . computePostureGoal ( startPose , startPose ) \n 
self . commitManipPlan ( ) \n 
~~ def planReach ( self , reachTargetFrame = None , jointSpeedLimit = None ) : \n 
~~~ if reachTargetFrame is None : \n 
~~~ reachTargetFrame = self . doorHandleReachFrame \n 
~~ if jointSpeedLimit is None : \n 
~~~ jointSpeedLimit = self . speedLow \n 
~~ startPose = self . getPlanningStartPose ( ) \n 
constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , reachTargetFrame constraintSet . nominalPoseName = \n 
linkOffsetFrame = self . ikPlanner . getPalmToHandLink ( self . graspingHand ) \n 
handToWorld1 = self . ikPlanner . getLinkFrameAtPose ( handLinkName , startPose ) \n 
handToWorld2 = self . ikPlanner . getLinkFrameAtPose ( handLinkName , endPose ) \n 
palmToWorld1 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , handToWorld1 ] ) \n 
palmToWorld2 = transformUtils . concatenateTransforms ( [ linkOffsetFrame , handToWorld2 ] ) \n 
~~ def planPreChop ( self ) : \n 
~~~ self . planReach ( self . doorHandlePreChopFrame , self . speedHigh ) \n 
~~ def createHingeConstraint ( self , referenceFrame , axis , linkName , startPose , tspan = [ 0 , 1 ] ) : \n 
~~~ constraints = [ ] \n 
linkFrame = self . ikPlanner . getLinkFrameAtPose ( linkName , startPose ) \n 
#turnTransform, \n 
#referenceFrame.transform]) \n 
def addPivotPoint ( constraints , pivotPoint ) : \n 
~~~ constraints . append ( ik . PositionConstraint ( ) ) \n 
constraints [ - 1 ] . linkName = linkName \n 
constraints [ - 1 ] . referenceFrame = referenceFrame . transform \n 
constraints [ - 1 ] . lowerBound = np . array ( pivotPoint ) \n 
constraints [ - 1 ] . upperBound = np . array ( pivotPoint ) \n 
pivotPointInWorld = referenceFrame . transform . TransformDoublePoint ( pivotPoint ) \n 
constraints [ - 1 ] . pointInLink = linkFrame . GetInverse ( ) . TransformDoublePoint ( pivotPointInWorld constraints [ - 1 ] . tspan = tspan \n 
~~ addPivotPoint ( constraints , [ 0.0 , 0.0 , 0.0 ] ) \n 
addPivotPoint ( constraints , axis ) \n 
return constraints \n 
~~ def planHandleTurn ( self , turnAngle = None ) : \n 
if turnAngle is None : \n 
~~~ turnAngle = self . handleTurnAngle \n 
linkFrame = self . ikPlanner . getLinkFrameAtPose ( self . ikPlanner . getHandLink ( ) , startPose ) \n 
finalGraspToReferenceTransfrom = transformUtils . concatenateTransforms ( \n 
[ self . ikPlanner . getPalmToHandLink ( self . graspingHand ) , linkFrame , \n 
self . doorHandleAxisFrame . transform . GetInverse ( ) ] ) \n 
handleTurnTransform = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , \n 
[ doorSide * turnAngle , 0 , 0 ] ) \n 
doorHandleTurnFrame = transformUtils . concatenateTransforms ( [ finalGraspToReferenceTransfrom , \n 
self . doorHandleAxisFrame . transform \n 
vis . updateFrame ( doorHandleTurnFrame , , parent = self . doorHandleAffordance , visible constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , \n 
doorHandleTurnFrame ) \n 
constraintSet . nominalPoseName = \n 
constraints = constraintSet . constraints \n 
constraints . extend ( self . createHingeConstraint ( self . doorHandleAxisFrame , [ 1.0 , 0.0 , 0.0 ] , \n 
self . ikPlanner . getHandLink ( ) , \n 
constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedBasePostureConstraint ( constraintSet . startPoseName constraints . append ( self . ikPlanner . createLockedBackPostureConstraint ( constraintSet . startPoseName constraints . extend ( self . ikPlanner . createFixedFootConstraints ( constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedArmPostureConstraint ( constraintSet . startPoseName \n 
~~ def planDoorPushOpen ( self ) : \n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = 15 ) \n 
endPose = self . ikPlanner . getMergedPostureFromDatabase ( startPose , , , side = nonGraspingHand \n 
~~ def planDoorPushOpenTwist ( self ) : \n 
~~~ ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = 60 ) \n 
~~ def planHandlePush ( self ) : \n 
self . doorHingeFrame . transform . GetInverse ( ) ] ) \n 
[ 0 , 0 , - doorSide * self . handlePushAngle doorHandlePushFrame = transformUtils . concatenateTransforms ( [ finalGraspToReferenceTransfrom , \n 
vis . updateFrame ( doorHandlePushFrame , , parent = self . doorHandleAffordance , visible constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , \n 
doorHandlePushFrame ) \n 
constraintSet . ikParameters = IkParameters ( usePointwise = False , maxDegreesPerSecond = self . speedLow constraintSet . nominalPoseName = \n 
constraints . extend ( self . createHingeConstraint ( self . doorHingeFrame , [ 0.0 , 0.0 , 1.0 ] , \n 
self . ikPlanner . getHandLink ( side = self . graspingHand constraintSet . startPoseName ) ) \n 
constraints . append ( self . ikPlanner . createLockedBasePostureConstraint ( constraintSet . startPoseName #constraints.append(self.ikPlanner.createLockedBackPostureConstraint(constraintSet.startPoseName)) constraints . extend ( self . ikPlanner . createFixedFootConstraints ( constraintSet . startPoseName ) ) \n 
~~ def planHandlePushLift ( self ) : \n 
~~~ self . planHandleTurn ( - self . handleTurnAngle ) \n 
#self.addPlan(plan) \n 
~~ def planDoorTouch ( self ) : \n 
~~ def planHandlePushOpen ( self ) : \n 
constraintSet = self . ikPlanner . planEndEffectorGoal ( startPose , self . graspingHand , self . doorHandlePushOpenFrame constraintSet . ikParameters = IkParameters ( usePointwise = False , \n 
~~ def planFootstepsToDoor ( self ) : \n 
goalFrame = self . doorHandleStanceFrame . transform \n 
request = self . footstepPlanner . constructFootstepPlanRequest ( startPose , goalFrame ) \n 
self . footstepPlan = self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True \n 
~~ def planFootstepsThroughDoor ( self ) : \n 
goalFrame = self . doorWalkFrame . transform \n 
request . params . nom_step_width = 0.21 \n 
self . footstepPlan = self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True rt . _addPlanItem ( self . footstepPlan , , rt . FootstepPlanItem ) \n 
~~ def setFootstepThroughDoorParameters ( self ) : \n 
~~~ bias = - 0.02 \n 
self . doorFootstepParams = FieldContainer ( \n 
leadingFoot = , \n 
preEntryFootWidth = - 0.12 + bias , \n 
preEntryFootDistance = - 0.6 , \n 
entryFootWidth = 0.07 + bias , \n 
entryFootDistance = - 0.26 , \n 
exitFootWidth = - 0.08 + bias , \n 
exitFootDistance = 0.12 , \n 
exitStepDistance = 0.3 , \n 
endStanceWidth = 0.26 , \n 
numberOfExitSteps = 1 , \n 
centerStepDistance = 0.26 , \n 
centerStanceWidth = 0.20 , \n 
centerLeadingFoot = \n 
~~ def setTestingFootstepThroughDoorParameters ( self ) : \n 
~~~ self . doorFootstepParams = FieldContainer ( \n 
entryFootWidth = 0.12 , \n 
exitFootWidth = - 0.12 , \n 
preEntryFootDistance = - 0.55 , \n 
preEntryFootWidth = - 0.12 \n 
~~ def getRelativeFootstepsThroughDoorWithSway ( self ) : \n 
~~~ p = self . doorFootstepParams \n 
stepFrames = [ \n 
[ p . preEntryFootDistance , p . preEntryFootWidth , 0.0 ] , \n 
[ p . entryFootDistance , p . entryFootWidth , 0.0 ] , \n 
[ p . exitFootDistance , p . exitFootWidth , 0.0 ] , \n 
for i in xrange ( p . numberOfExitSteps ) : \n 
~~~ sign = - 1 if ( i % 2 ) else 1 \n 
stepFrames . append ( [ p . exitFootDistance + ( i + 1 ) * p . exitStepDistance , sign * p . endStanceWidth / \n 
~~ lastStep = list ( stepFrames [ - 1 ] ) \n 
lastStep [ 1 ] *= - 1 \n 
stepFrames . append ( lastStep ) \n 
return FootstepRequestGenerator . makeStepFrames ( stepFrames , relativeFrame = self . doorGroundFrame \n 
~~ def getRelativeFootstepsThroughDoorCentered ( self ) : \n 
stepDistance = p . centerStepDistance \n 
stanceWidth = p . centerStanceWidth \n 
leadingFoot = p . centerLeadingFoot \n 
stepFrames = [ ] \n 
for i in xrange ( 30 ) : \n 
~~~ sign = - 1 if leadingFoot is else 1 \n 
if i % 2 : \n 
~~~ sign = - sign \n 
~~ stepX = ( i + 1 ) * stepDistance \n 
if stepX > 1.5 : \n 
~~~ stepX = 1.5 \n 
~~ stepFrames . append ( [ stepX , sign * stanceWidth / 2.0 , 0.0 ] ) \n 
if stepX == 1.5 : \n 
~~ ~~ lastStep = list ( stepFrames [ - 1 ] ) \n 
stepFrames [ - 1 ] [ 1 ] = np . sign ( stepFrames [ - 1 ] [ 1 ] ) * ( p . endStanceWidth / 2.0 ) \n 
stepFrames [ - 2 ] [ 1 ] = np . sign ( stepFrames [ - 2 ] [ 1 ] ) * ( p . endStanceWidth / 2.0 ) \n 
return FootstepRequestGenerator . makeStepFrames ( stepFrames , relativeFrame = self . doorHandleStanceFrame \n 
~~ def planManualFootstepsTest ( self , stepDistance = 0.26 , stanceWidth = 0.26 , numberOfSteps = 4 , leadingFoot \n 
~~~ stepFrames = [ ] \n 
for i in xrange ( numberOfSteps ) : \n 
~~ stepFrames . append ( [ ( i + 1 ) * stepDistance , sign * stanceWidth / 2.0 , 0.0 ] ) \n 
stanceFrame = FootstepRequestGenerator . getRobotStanceFrame ( self . robotModel ) \n 
stepFrames = FootstepRequestGenerator . makeStepFrames ( stepFrames , relativeFrame = stanceFrame ) \n 
helper = FootstepRequestGenerator ( self . footstepPlanner ) \n 
request = helper . makeFootstepRequest ( startPose , stepFrames , leadingFoot ) \n 
self . footstepPlanner . sendFootstepPlanRequest ( request , waitForResponse = True ) \n 
~~ def planFootstepsThroughDoorManual ( self ) : \n 
stepFrames , leadingFoot = self . getRelativeFootstepsThroughDoorCentered ( ) \n 
request = helper . makeFootstepRequest ( startPose , stepFrames , leadingFoot , numberOfFillSteps = 2 \n 
~~ def computeWalkingPlan ( self ) : \n 
self . walkingPlan = self . footstepPlanner . sendWalkingPlanRequest ( self . footstepPlan , startPose , self . addPlan ( self . walkingPlan ) \n 
~~ def commitManipPlan ( self ) : \n 
~~~ self . manipPlanner . commitManipPlan ( self . plans [ - 1 ] ) \n 
~~ def fitDoor ( self , doorGroundFrame ) : \n 
~~~ om . removeFromObjectModel ( om . findObjectByName ( ) ) \n 
self . spawnDoorAffordance ( ) \n 
affordanceFrame = om . findObjectByName ( ) \n 
assert affordanceFrame is not None \n 
affordanceFrame . copyFrame ( doorGroundFrame ) \n 
om . findObjectByName ( ) . setProperty ( , False ) \n 
~~ def showDoorHandlePoints ( self , polyData ) : \n 
~~~ doorHandle = om . findObjectByName ( ) \n 
door = om . findObjectByName ( ) \n 
doorWidth = door . getProperty ( ) [ 1 ] \n 
doorAxes = transformUtils . getAxesFromTransform ( door . getChildFrame ( ) . transform ) \n 
doorOrigin = np . array ( door . getChildFrame ( ) . transform . GetPosition ( ) ) \n 
handleAxes = transformUtils . getAxesFromTransform ( doorHandle . getChildFrame ( ) . transform ) \n 
handleOrigin = np . array ( doorHandle . getChildFrame ( ) . transform . GetPosition ( ) ) \n 
polyData = segmentation . cropToLineSegment ( polyData , doorOrigin - doorAxes [ 0 ] * 0.02 , doorOrigin polyData = segmentation . cropToLineSegment ( polyData , doorOrigin , doorOrigin + doorAxes [ 1 ] * ( doorWidth polyData = segmentation . cropToLineSegment ( polyData , handleOrigin - handleAxes [ 2 ] * 0.1 , handleOrigin \n 
pointsName = \n 
existed = om . findObjectByName ( pointsName ) is not None \n 
obj = vis . updatePolyData ( polyData , pointsName , parent = doorHandle , color = [ 1 , 0 , 0 ] ) \n 
if not existed : \n 
~~~ obj . setProperty ( , 10 ) \n 
~~ ~~ def spawnDoorAffordance ( self ) : \n 
~~~ groundFrame = self . computeGroundFrame ( self . robotModel ) \n 
doorOffsetX = 0.7 \n 
doorOffsetY = 0.0 \n 
doorGroundFrame = transformUtils . frameFromPositionAndRPY ( [ doorOffsetX , 0.0 , 0.0 ] , [ 0.0 , 0.0 , doorGroundFrame . PostMultiply ( ) \n 
doorGroundFrame . Concatenate ( groundFrame ) \n 
stanceFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , 0.0 ] , [ 0.0 , 0.0 , 0.0 ] ) \n 
stanceFrame . PostMultiply ( ) \n 
stanceFrame . Concatenate ( groundFrame ) \n 
doorWalkFrame = transformUtils . frameFromPositionAndRPY ( [ doorOffsetX + 0.6 , 0.0 , 0.0 ] , [ 0.0 , doorWalkFrame . PostMultiply ( ) \n 
doorWalkFrame . Concatenate ( groundFrame ) \n 
doorWidth = 36 * 0.0254 \n 
doorHeight = 81 * 0.0254 \n 
doorDepth = 0.5 * 0.0254 \n 
handleHeightFromGround = 35 * 0.0254 \n 
handleDistanceFromEdge = 1.625 * 0.0254 \n 
handleDistanceFromDoor = 1.75 * 0.0254 \n 
handleLength = 4.125 * 0.0254 \n 
handleDepth = 0.25 * 0.0254 \n 
doorJamWidth = 0.5 \n 
doorJamDepth = 4.5 * 0.0254 \n 
handleFrame = transformUtils . frameFromPositionAndRPY ( [ - handleDistanceFromDoor - doorDepth / 2.0 handleFrame . PostMultiply ( ) \n 
handleFrame . Concatenate ( doorGroundFrame ) \n 
doorFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , 0.0 , doorHeight / 2.0 ] , [ 0.0 , 0.0 , 0.0 doorFrame . PostMultiply ( ) \n 
doorFrame . Concatenate ( doorGroundFrame ) \n 
leftDoorJamFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , ( doorWidth / 2.0 + doorJamWidth leftDoorJamFrame . PostMultiply ( ) \n 
leftDoorJamFrame . Concatenate ( doorGroundFrame ) \n 
rightDoorJamFrame = transformUtils . frameFromPositionAndRPY ( [ 0.0 , - ( doorWidth / 2.0 + doorJamWidth rightDoorJamFrame . PostMultiply ( ) \n 
rightDoorJamFrame . Concatenate ( doorGroundFrame ) \n 
desc = dict ( classname = , Name = , \n 
pose = transformUtils . poseFromTransform ( handleFrame ) , Dimensions = [ handleDepth , handleLength handleAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
pose = transformUtils . poseFromTransform ( doorFrame ) , Dimensions = [ doorDepth , doorWidth , doorHeight doorAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
pose = transformUtils . poseFromTransform ( leftDoorJamFrame ) , Dimensions = [ doorJamDepth , doorJamWidth leftDoorJamAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
pose = transformUtils . poseFromTransform ( rightDoorJamFrame ) , Dimensions = [ doorJamDepth , doorJamWidth rightDoorJamAffordance = segmentation . affordanceManager . newAffordanceFromDescription ( desc ) \n 
doorGroundFrame = vis . showFrame ( doorGroundFrame , , parent = doorAffordance ) stanceFrame = vis . showFrame ( stanceFrame , , parent = doorAffordance ) \n 
doorWalkFrame = vis . showFrame ( doorWalkFrame , , visible = False , parent = doorAffordance \n 
doorFrame = doorAffordance . getChildFrame ( ) \n 
handleFrame = handleAffordance . getChildFrame ( ) \n 
leftDoorJamFrame = leftDoorJamAffordance . getChildFrame ( ) \n 
rightDoorJamFrame = rightDoorJamAffordance . getChildFrame ( ) \n 
self . doorFrameSync = vis . FrameSync ( ) \n 
self . doorFrameSync . addFrame ( doorGroundFrame ) \n 
self . doorFrameSync . addFrame ( stanceFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( doorWalkFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( doorFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( leftDoorJamFrame , ignoreIncoming = True ) \n 
self . doorFrameSync . addFrame ( rightDoorJamFrame , ignoreIncoming = True ) \n 
self . doorHandleFrameSync = vis . FrameSync ( ) \n 
self . doorHandleFrameSync . addFrame ( doorFrame ) \n 
self . doorHandleFrameSync . addFrame ( handleFrame , ignoreIncoming = True ) \n 
self . findDoorHandleAffordance ( ) \n 
self . doorGroundFrame = doorGroundFrame \n 
self . doorHandleStanceFrame = stanceFrame \n 
self . doorWalkFrame = doorWalkFrame \n 
~~ def findDoorHandleAffordance ( self ) : \n 
~~~ self . doorHandleAffordance = om . findObjectByName ( ) \n 
self . doorHandleFrame = self . doorHandleAffordance . getChildFrame ( ) \n 
self . computeDoorHingeFrame ( ) \n 
self . computeDoorHandleGraspFrame ( ) \n 
#self.computeDoorHandleStanceFrame() \n 
~~ def getEstimatedRobotStatePose ( self ) : \n 
~~~ return np . array ( self . sensorJointController . getPose ( ) ) \n 
~~ def getPlanningStartPose ( self ) : \n 
~~~ if self . planFromCurrentRobotState : \n 
~~~ return self . getEstimatedRobotStatePose ( ) \n 
~~~ if self . plans : \n 
~~~ return robotstate . convertStateMessageToDrakePose ( self . plans [ - 1 ] . plan [ - 1 ] ) \n 
~~ ~~ ~~ ~~ class DoorImageFitter ( ImageBasedAffordanceFit ) : \n 
~~~ def __init__ ( self , doorDemo ) : \n 
~~~ ImageBasedAffordanceFit . __init__ ( self , numberOfPoints = 1 ) \n 
self . doorDemo = doorDemo \n 
~~ def fit ( self , polyData , points ) : \n 
~~~ stanceFrame = FootstepRequestGenerator . getRobotStanceFrame ( self . doorDemo . robotModel ) \n 
doorGroundFrame = segmentation . segmentDoorPlane ( polyData , points [ 0 ] , stanceFrame ) \n 
self . doorDemo . fitDoor ( doorGroundFrame ) \n 
self . doorDemo . showDoorHandlePoints ( polyData ) \n 
~~ ~~ class DoorTaskPanel ( TaskUserPanel ) : \n 
~~~ TaskUserPanel . __init__ ( self , windowTitle = ) \n 
self . fitter = DoorImageFitter ( self . doorDemo ) \n 
self . initImageView ( self . fitter . imageView ) \n 
self . addDefaultProperties ( ) \n 
self . addButtons ( ) \n 
self . addTasks ( ) \n 
~~ def addButtons ( self ) : \n 
~~~ self . addManualButton ( , self . doorDemo . spawnDoorAffordance ) \n 
self . addManualSpacer ( ) \n 
self . addManualButton ( , self . doorDemo . planFootstepsToDoor ) \n 
self . addManualButton ( , self . doorDemo . planFootstepsThroughDoor ) \n 
self . addManualButton ( , self . doorDemo . planPreReach ) \n 
self . addManualButton ( , self . doorDemo . planTuckArmsPrePush ) \n 
self . addManualButton ( , self . doorDemo . planTuckArms ) \n 
self . addManualButton ( , self . openPinch ) \n 
self . addManualButton ( , self . closePinch ) \n 
self . addManualButton ( , self . doorDemo . planReach ) \n 
self . addManualButton ( , self . doorDemo . planUnReach ) \n 
self . addManualButton ( , self . doorDemo . planPreChop ) \n 
self . addManualButton ( , self . doorDemo . planChop ) \n 
self . addManualButton ( , functools . partial ( self . doorDemo . planChop , deltaX = - 0.1 , deltaY self . addManualSpacer ( ) \n 
self . addManualButton ( , functools . partial ( self . doorDemo . planHandleTurn , 10 ) ) \n 
self . addManualButton ( , functools . partial ( self . doorDemo . planHandleTurn , - 10 ) ) \n 
self . addManualButton ( , self . doorDemo . planDoorPushOpenTwist ) \n 
self . addManualButton ( , self . doorDemo . commitManipPlan ) \n 
self . addManualButton ( , self . doorDemo . stopPushing ) \n 
~~ def getSide ( self ) : \n 
~~~ return self . params . getPropertyEnumValue ( ) . lower ( ) \n 
~~ def openPinch ( self ) : \n 
~~~ rt . OpenHand ( side = self . getSide ( ) . capitalize ( ) , mode = ) . run ( ) \n 
~~ def closePinch ( self ) : \n 
~~~ rt . CloseHand ( side = self . getSide ( ) . capitalize ( ) , mode = ) . run ( ) \n 
~~ def addDefaultProperties ( self ) : \n 
~~~ self . params . addProperty ( , 0 , attributes = om . PropertyAttributes ( enumNames = [ , self . params . addProperty ( , self . doorDemo . preChopWidth , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . preChopDepth , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . preChopHeight , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . chopDistance , attributes = om . PropertyAttributes self . params . addProperty ( , self . doorDemo . chopSidewaysDistance , attributes self . _syncProperties ( ) \n 
~~ def onPropertyChanged ( self , propertySet , propertyName ) : \n 
~~~ if propertyName == : \n 
~~~ self . taskTree . removeAllTasks ( ) \n 
~~ self . doorDemo . findDoorHandleAffordance ( ) \n 
self . _syncProperties ( ) \n 
~~ def _syncProperties ( self ) : \n 
~~~ self . doorDemo . graspingHand = self . params . getPropertyEnumValue ( ) . lower ( ) \n 
self . doorDemo . ikPlanner . reachingSide = self . doorDemo . graspingHand \n 
if hasattr ( self . doorDemo , ) : \n 
~~~ self . doorDemo . computeDoorHandleGraspFrame ( ) \n 
~~ self . doorDemo . chopDistance = self . params . getProperty ( ) \n 
self . doorDemo . chopSidewaysDistance = self . params . getProperty ( ) \n 
self . doorDemo . preChopWidth = self . params . getProperty ( ) \n 
self . doorDemo . preChopDepth = self . params . getProperty ( ) \n 
self . doorDemo . preChopHeight = self . params . getProperty ( ) \n 
~~ def addTasks ( self ) : \n 
~~~ self . folder = None \n 
def addTask ( task , parent = None ) : \n 
~~~ parent = parent or self . folder \n 
self . taskTree . onAddTask ( task , copy = False , parent = parent ) \n 
~~ def addFunc ( func , name , parent = None ) : \n 
~~~ addTask ( rt . CallbackTask ( callback = func , name = name ) , parent = parent ) \n 
~~ def addFolder ( name , parent = None ) : \n 
~~~ self . folder = self . taskTree . addGroup ( name , parent = parent ) \n 
return self . folder \n 
~~ d = self . doorDemo \n 
self . taskTree . removeAllTasks ( ) \n 
side = self . params . getPropertyEnumValue ( ) \n 
############### \n 
folder = addFolder ( ) \n 
addTask ( rt . CloseHand ( name = , side = ) ) \n 
addTask ( rt . UserPromptTask ( name = , message = addTask ( rt . FindAffordance ( name = , affordanceName = ) ) \n 
addTask ( rt . SetNeckPitch ( name = , angle = 35 ) ) \n 
addTask ( rt . RequestFootstepPlan ( name = , stanceFrameName = addTask ( rt . UserPromptTask ( name = , message = ) addTask ( rt . CommitFootstepPlan ( name = , planName = addTask ( rt . WaitForWalkExecution ( name = ) ) \n 
addTask ( rt . UserPromptTask ( name = , message = \n 
addTask ( rt . OpenHand ( name = , side = side , mode = ) ) \n 
def addManipTask ( name , planFunc , userPrompt = False ) : \n 
~~~ folder = addFolder ( name ) \n 
addFunc ( planFunc , name = ) \n 
if not userPrompt : \n 
~~~ addTask ( rt . CheckPlanInfo ( name = ) ) \n 
~~~ addTask ( rt . UserPromptTask ( name = , message = ~~ addFunc ( d . commitManipPlan , name = ) \n 
addTask ( rt . WaitForManipulationPlanExecution ( name = ) ) \n 
~~ addManipTask ( , d . planPreReach , userPrompt = False ) \n 
addManipTask ( , d . planDoorTouch , userPrompt = False ) \n 
if d . usePinchGrasp : \n 
~~~ addManipTask ( , d . planReach , userPrompt = True ) \n 
addFunc ( self . closePinch , name = ) \n 
addTask ( rt . UserPromptTask ( name = , \n 
message = ) ) \n 
addManipTask ( , d . planHandleTurn , userPrompt = False ) \n 
~~~ addFunc ( self . doorDemo . setChopParametersToDefaults , name = ) \n 
addManipTask ( , d . planReach , userPrompt = True ) \n 
addManipTask ( , d . planChop , userPrompt = True ) \n 
~~ addManipTask ( , d . planHandlePush , userPrompt = False ) \n 
addManipTask ( , d . planHandlePush , userPrompt = False ) \n 
~~~ addManipTask ( , d . planHandlePushLift , userPrompt = False ) \n 
addTask ( rt . CloseHand ( name = , side = side , mode = , amount = 0 ) ) \n 
~~ addManipTask ( , d . planDoorPushOpen , userPrompt = False ) \n 
addTask ( rt . CloseHand ( name = , side = side ) ) \n 
addManipTask ( , d . planTuckArms , userPrompt = False ) \n 
addFunc ( d . planFootstepsThroughDoorManual , name = ) \n 
addTask ( rt . UserPromptTask ( name = , message = ) addTask ( rt . CommitFootstepPlan ( name = , planName = ) addTask ( rt . WaitForWalkExecution ( name = ) ) \n 
addTask ( rt . PlanPostureGoal ( name = , postureGroup = , postureName = addTask ( rt . UserPromptTask ( name = , message = ) ) \n 
addTask ( rt . CommitManipulationPlan ( name = , planName = addTask ( rt . WaitForManipulationPlanExecution ( name = ) ) \n 
~~ ~~ import PythonQt \n 
from PythonQt import QtCore , QtGui , QtUiTools \n 
from time import time \n 
from copy import deepcopy \n 
def addWidgetsToDict ( widgets , d ) : \n 
~~~ for widget in widgets : \n 
~~~ if widget . objectName : \n 
~~~ d [ str ( widget . objectName ) ] = widget \n 
~~ addWidgetsToDict ( widget . children ( ) , d ) \n 
~~ ~~ class WidgetDict ( object ) : \n 
~~~ def __init__ ( self , widgets ) : \n 
~~~ addWidgetsToDict ( widgets , self . __dict__ ) \n 
~~ ~~ class SpindleSpinChecker ( object ) : \n 
~~~ def __init__ ( self , spindleMonitor ) : \n 
~~~ self . spindleMonitor = spindleMonitor \n 
self . timer = TimerCallback ( targetFps = 3 ) \n 
self . timer . callback = self . update \n 
self . warningButton = None \n 
self . action = None \n 
~~ def update ( self ) : \n 
~~~ if abs ( self . spindleMonitor . getAverageSpindleVelocity ( ) ) < 0.2 : \n 
~~~ self . notifyUserStatusBar ( ) \n 
~~~ self . clearStatusBarWarning ( ) \n 
~~ ~~ def start ( self ) : \n 
~~~ self . action . checked = True \n 
self . timer . start ( ) \n 
~~ def stop ( self ) : \n 
~~~ self . action . checked = False \n 
self . timer . stop ( ) \n 
~~ def setupMenuAction ( self ) : \n 
~~~ self . action = app . addMenuAction ( , ) \n 
self . action . setCheckable ( True ) \n 
self . action . checked = self . timer . isActive ( ) \n 
self . action . connect ( , self . onActionChanged ) \n 
~~ def onActionChanged ( self ) : \n 
~~~ if self . action . checked : \n 
~~~ self . start ( ) \n 
~~~ self . stop ( ) \n 
~~ ~~ def clearStatusBarWarning ( self ) : \n 
~~~ if self . warningButton : \n 
~~~ self . warningButton . deleteLater ( ) \n 
~~ ~~ def notifyUserStatusBar ( self ) : \n 
~~ self . warningButton = QtGui . QPushButton ( ) \n 
self . warningButton . setStyleSheet ( "background-color:red" ) \n 
app . getMainWindow ( ) . statusBar ( ) . insertPermanentWidget ( 0 , self . warningButton ) \n 
~~ ~~ class MultisensePanel ( object ) : \n 
~~~ def __init__ ( self , multisenseDriver ) : \n 
~~~ self . multisenseDriver = multisenseDriver \n 
self . multisenseChanged = False \n 
loader = QtUiTools . QUiLoader ( ) \n 
uifile = QtCore . QFile ( ) \n 
assert uifile . open ( uifile . ReadOnly ) \n 
self . widget = loader . load ( uifile ) \n 
self . ui = WidgetDict ( self . widget . children ( ) ) \n 
self . updateTimer = TimerCallback ( targetFps = 2 ) \n 
self . updateTimer . callback = self . updatePanel \n 
self . updateTimer . start ( ) \n 
self . widget . headCamGainSpinner . setEnabled ( False ) \n 
self . widget . headCamExposureSpinner . setEnabled ( False ) \n 
self . widget . spinRateSpinner . valueChanged . connect ( self . spinRateChange ) \n 
self . widget . scanDurationSpinner . valueChanged . connect ( self . scanDurationChange ) \n 
self . widget . headCamFpsSpinner . valueChanged . connect ( self . headCamFpsChange ) \n 
self . widget . headCamGainSpinner . valueChanged . connect ( self . headCamGainChange ) \n 
self . widget . headCamExposureSpinner . valueChanged . connect ( self . headCamExposureChange ) \n 
self . widget . headAutoGainCheck . clicked . connect ( self . headCamAutoGainChange ) \n 
self . widget . ledOnCheck . clicked . connect ( self . ledOnCheckChange ) \n 
self . widget . ledBrightnessSpinner . valueChanged . connect ( self . ledBrightnessChange ) \n 
self . widget . sendButton . clicked . connect ( self . sendButtonClicked ) \n 
self . updatePanel ( ) \n 
~~ def getCameraFps ( self ) : \n 
~~~ return self . widget . headCamFpsSpinner . value \n 
~~ def getCameraGain ( self ) : \n 
~~~ return self . widget . headCamGainSpinner . value \n 
~~ def getCameraExposure ( self ) : \n 
~~~ return self . widget . headCamExposureSpinner . value \n 
~~ def getCameraLedOn ( self ) : \n 
~~~ return self . widget . ledOnCheck . isChecked ( ) \n 
~~ def getCameraLedBrightness ( self ) : \n 
~~~ return self . widget . ledBrightnessSpinner . value \n 
~~ def getCameraAutoGain ( self ) : \n 
~~~ return self . widget . headAutoGainCheck . isChecked ( ) \n 
~~ def getSpinRate ( self ) : \n 
~~~ return self . widget . spinRateSpinner . value \n 
~~ def getScanDuration ( self ) : \n 
~~~ return self . widget . scanDurationSpinner . value \n 
~~ def ledBrightnessChange ( self , event ) : \n 
~~~ self . multisenseChanged = True \n 
~~ def ledOnCheckChange ( self , event ) : \n 
~~ def headCamExposureChange ( self , event ) : \n 
~~ def headCamAutoGainChange ( self , event ) : \n 
self . widget . headCamGainSpinner . setEnabled ( not self . getCameraAutoGain ( ) ) \n 
self . widget . headCamExposureSpinner . setEnabled ( not self . getCameraAutoGain ( ) ) \n 
~~ def headCamFpsChange ( self , event ) : \n 
~~ def headCamGainChange ( self , event ) : \n 
~~ def spinRateChange ( self , event ) : \n 
spinRate = self . getSpinRate ( ) \n 
if spinRate == 0.0 : \n 
~~~ scanDuration = 240.0 \n 
~~~ scanDuration = abs ( 60.0 / ( spinRate * 2 ) ) \n 
~~ if scanDuration > 240.0 : \n 
~~ self . widget . scanDurationSpinner . blockSignals ( True ) \n 
self . widget . scanDurationSpinner . value = scanDuration \n 
self . widget . scanDurationSpinner . blockSignals ( False ) \n 
~~ def scanDurationChange ( self , event ) : \n 
scanDuration = self . getScanDuration ( ) \n 
spinRate = abs ( 60.0 / ( scanDuration * 2 ) ) \n 
self . widget . spinRateSpinner . blockSignals ( True ) \n 
self . widget . spinRateSpinner . value = spinRate \n 
self . widget . spinRateSpinner . blockSignals ( False ) \n 
~~ def sendButtonClicked ( self , event ) : \n 
~~~ self . publishCommand ( ) \n 
~~ def updatePanel ( self ) : \n 
~~~ if not self . widget . isVisible ( ) : \n 
~~ ~~ def publishCommand ( self ) : \n 
~~~ fps = self . getCameraFps ( ) \n 
camGain = self . getCameraGain ( ) \n 
exposure = 1000 * self . getCameraExposure ( ) \n 
ledFlash = self . getCameraLedOn ( ) \n 
ledDuty = self . getCameraLedBrightness ( ) \n 
autoGain = 1 if self . getCameraAutoGain ( ) else 0 \n 
self . multisenseDriver . sendMultisenseCommand ( fps , camGain , exposure , autoGain , spinRate , ledFlash \n 
~~ ~~ def _getAction ( ) : \n 
~~~ return app . getToolBarActions ( ) [ ] \n 
~~ def init ( driver ) : \n 
~~~ global panel \n 
global dock \n 
panel = MultisensePanel ( driver ) \n 
dock = app . addWidgetToDock ( panel . widget , action = _getAction ( ) ) \n 
dock . hide ( ) \n 
return panel \n 
~~ def deepCopy ( dataOb ) : \n 
~~~ newData = dataObject . NewInstance ( ) \n 
newData . DeepCopy ( dataObj ) \n 
return newData \n 
~~ def shallowCopy ( dataObj ) : \n 
~~~ newData = dataObj . NewInstance ( ) \n 
newData . ShallowCopy ( dataObj ) \n 
#!/usr/bin/python \n 
~~ from __future__ import division \n 
from numpy import * \n 
def minBoundingRect ( hull_points_2d ) : \n 
for i in range ( len ( edges ) ) : \n 
~~~ edge_x = hull_points_2d [ i + 1 , 0 ] - hull_points_2d [ i , 0 ] \n 
edge_y = hull_points_2d [ i + 1 , 1 ] - hull_points_2d [ i , 1 ] \n 
edges [ i ] = [ edge_x , edge_y ] \n 
for i in range ( len ( edge_angles ) ) : \n 
~~~ edge_angles [ i ] = math . atan2 ( edges [ i , 1 ] , edges [ i , 0 ] ) \n 
~~ for i in range ( len ( edge_angles ) ) : \n 
~~ edge_angles = unique ( edge_angles ) \n 
min_x = nanmin ( rot_points [ 0 ] , axis = 0 ) \n 
max_x = nanmax ( rot_points [ 0 ] , axis = 0 ) \n 
min_y = nanmin ( rot_points [ 1 ] , axis = 0 ) \n 
max_y = nanmax ( rot_points [ 1 ] , axis = 0 ) \n 
width = max_x - min_x \n 
height = max_y - min_y \n 
area = width * height \n 
~~~ min_bbox = ( edge_angles [ i ] , area , width , height , min_x , max_x , min_y , max_y ) \n 
~~ ~~ angle = min_bbox [ 0 ] \n 
min_x = min_bbox [ 4 ] \n 
max_x = min_bbox [ 5 ] \n 
min_y = min_bbox [ 6 ] \n 
max_y = min_bbox [ 7 ] \n 
center_x = ( min_x + max_x ) / 2 \n 
center_y = ( min_y + max_y ) / 2 \n 
center_point = dot ( [ center_x , center_y ] , R ) \n 
corner_points [ 0 ] = dot ( [ max_x , min_y ] , R ) \n 
corner_points [ 1 ] = dot ( [ min_x , min_y ] , R ) \n 
corner_points [ 2 ] = dot ( [ min_x , max_y ] , R ) \n 
corner_points [ 3 ] = dot ( [ max_x , max_y ] , R ) \n 
~~ from director import atlasdriver \n 
from director import consoleapp \n 
atlasDriver = atlasdriver . init ( ) \n 
w = QtGui . QWidget ( ) \n 
l = QtGui . QVBoxLayout ( w ) \n 
Button = namedtuple ( , [ , , ] ) ; \n 
buttons = [ \n 
Button ( , atlasDriver . sendRecoveryTriggerOn , None ) \n 
for button in buttons : \n 
~~~ qb = QtGui . QPushButton ( button . name ) \n 
qb . connect ( , button . callback ) \n 
qb . setSizePolicy ( QtGui . QSizePolicy . Expanding , QtGui . QSizePolicy . Expanding ) \n 
s = qb . styleSheet \n 
if button . color : \n 
~~ qb . setStyleSheet ( s ) \n 
l . addWidget ( qb ) \n 
~~ w . setWindowTitle ( ) \n 
w . show ( ) \n 
w . resize ( 500 , 600 ) \n 
consoleapp . ConsoleApp . start ( ) \n 
from director . consoleapp import ConsoleApp \n 
from director import roboturdf \n 
from director import jointcontrol \n 
def getArgs ( ) : \n 
parser . add_argument ( , type = str , default = None , help = ) \n 
args , unknown = parser . parse_known_args ( ) \n 
return args \n 
~~ app = ConsoleApp ( ) \n 
view = app . createView ( ) \n 
args = getArgs ( ) \n 
if args . urdf : \n 
~~~ robotModel = roboturdf . openUrdf ( args . urdf , view ) \n 
jointNames = robotModel . model . getJointNames ( ) \n 
jointController = jointcontrol . JointController ( [ robotModel ] , jointNames = jointNames ) \n 
~~~ robotModel , jointController = roboturdf . loadRobotModel ( , view ) \n 
~~ print , robotModel . getProperty ( ) \n 
for joint in robotModel . model . getJointNames ( ) : \n 
~~~ print , joint \n 
~~ for link in robotModel . model . getLinkNames ( ) : \n 
~~~ print , link \n 
robotModel . getLinkFrame ( link ) \n 
~~ if app . getTestingInteractiveEnabled ( ) : \n 
~~~ view . show ( ) \n 
app . start ( ) \n 
~~ from distutils . core import setup \n 
from catkin_pkg . python_setup import generate_distutils_setup \n 
d = generate_distutils_setup ( \n 
packages = [ , , , package_dir = { : } , \n 
setup ( ** d ) \n 
from random import randint \n 
from rosbridge_library . util import json \n 
send_fragment_size = 1000 \n 
receive_fragment_size = 10 \n 
receive_message_intervall = 0.0 \n 
def calculate_service_response ( request ) : \n 
message = { "data" : { "data" : 42.0 } } \n 
response_object = { "op" : "service_response" , \n 
"id" : request_object [ "id" ] , \n 
response_message = json . dumps ( response_object ) \n 
return response_message \n 
~~ buffer = "" \n 
def connect_tcp_socket ( ) : \n 
tcp_sock . connect ( ( rosbridge_ip , rosbridge_port ) ) \n 
return tcp_sock \n 
~~~ advertise_message_object = { "op" : "advertise_service" , \n 
"type" : service_type , \n 
"service" : service_name , \n 
"fragment_size" : receive_fragment_size , \n 
"message_intervall" : receive_message_intervall \n 
advertise_message = json . dumps ( advertise_message_object ) \n 
tcp_socket . send ( str ( advertise_message ) ) \n 
"service" : service_name \n 
unadvertise_message = json . dumps ( unadvertise_message_object ) \n 
tcp_socket . send ( str ( unadvertise_message ) ) \n 
global buffer \n 
~~~ done = False \n 
while not done : \n 
if data_object [ "op" ] == "call_service" : \n 
~~~ data = buffer \n 
done = True \n 
for fragment in result_string : \n 
~~~ if fragment [ 0 ] != "{" : \n 
~~~ fragment = "{" + fragment \n 
~~ if fragment [ len ( fragment ) - 1 ] != "}" : \n 
~~~ fragment = fragment + "}" \n 
~~ result . append ( json . loads ( fragment ) ) \n 
announced = int ( result [ 0 ] [ "total" ] ) \n 
if fragment_count == announced : \n 
~~~ reconstructed = "" \n 
unsorted_result = [ ] \n 
for fragment in result : \n 
~~~ unsorted_result . append ( fragment ) \n 
sorted_result [ int ( fragment [ "num" ] ) ] = fragment \n 
return reconstructed \n 
~~ ~~ except Exception , e : \n 
print e \n 
~~~ print "defrag_error:" , buffer \n 
~~ ~~ ~~ except Exception , e : \n 
~~ return data \n 
while cursor < len ( full_message ) : \n 
~~~ fragment_begin = cursor \n 
if len ( full_message ) < cursor + fragment_size : \n 
~~~ fragment_end = len ( full_message ) \n 
cursor = len ( full_message ) \n 
~~~ fragment_end = cursor + fragment_size \n 
cursor += fragment_size \n 
~~ fragment = full_message [ fragment_begin : fragment_end ] \n 
fragments . append ( fragment ) \n 
"data" : str ( fragment ) , \n 
"num" : count , \n 
"total" : len ( fragments ) \n 
~~ return fragmented_messages_list \n 
for fragment in fragment_list : \n 
from __future__ import absolute_import , division , print_function , with_statement \n 
import collections \n 
import pycurl \n 
from tornado import httputil \n 
from tornado import ioloop \n 
from tornado . log import gen_log \n 
from tornado import stack_context \n 
from tornado . escape import utf8 , native_str \n 
from tornado . httpclient import HTTPResponse , HTTPError , AsyncHTTPClient , main \n 
from tornado . util import bytes_type \n 
~~ class CurlAsyncHTTPClient ( AsyncHTTPClient ) : \n 
~~~ def initialize ( self , io_loop , max_clients = 10 , defaults = None ) : \n 
~~~ super ( CurlAsyncHTTPClient , self ) . initialize ( io_loop , defaults = defaults ) \n 
self . _multi = pycurl . CurlMulti ( ) \n 
self . _multi . setopt ( pycurl . M_TIMERFUNCTION , self . _set_timeout ) \n 
self . _multi . setopt ( pycurl . M_SOCKETFUNCTION , self . _handle_socket ) \n 
self . _curls = [ _curl_create ( ) for i in range ( max_clients ) ] \n 
self . _free_list = self . _curls [ : ] \n 
self . _requests = collections . deque ( ) \n 
self . _fds = { } \n 
self . _timeout = None \n 
self . _force_timeout_callback = ioloop . PeriodicCallback ( \n 
self . _handle_force_timeout , 1000 , io_loop = io_loop ) \n 
self . _force_timeout_callback . start ( ) \n 
dummy_curl_handle = pycurl . Curl ( ) \n 
self . _multi . add_handle ( dummy_curl_handle ) \n 
self . _multi . remove_handle ( dummy_curl_handle ) \n 
~~~ self . _force_timeout_callback . stop ( ) \n 
if self . _timeout is not None : \n 
~~~ self . io_loop . remove_timeout ( self . _timeout ) \n 
~~ for curl in self . _curls : \n 
~~~ curl . close ( ) \n 
~~ self . _multi . close ( ) \n 
super ( CurlAsyncHTTPClient , self ) . close ( ) \n 
~~ def fetch_impl ( self , request , callback ) : \n 
~~~ self . _requests . append ( ( request , callback ) ) \n 
self . _process_queue ( ) \n 
self . _set_timeout ( 0 ) \n 
~~ def _handle_socket ( self , event , fd , multi , data ) : \n 
event_map = { \n 
pycurl . POLL_NONE : ioloop . IOLoop . NONE , \n 
pycurl . POLL_IN : ioloop . IOLoop . READ , \n 
pycurl . POLL_OUT : ioloop . IOLoop . WRITE , \n 
pycurl . POLL_INOUT : ioloop . IOLoop . READ | ioloop . IOLoop . WRITE \n 
if event == pycurl . POLL_REMOVE : \n 
~~~ if fd in self . _fds : \n 
~~~ self . io_loop . remove_handler ( fd ) \n 
del self . _fds [ fd ] \n 
~~~ ioloop_event = event_map [ event ] \n 
if fd in self . _fds : \n 
~~ self . io_loop . add_handler ( fd , self . _handle_events , \n 
ioloop_event ) \n 
self . _fds [ fd ] = ioloop_event \n 
~~ ~~ def _set_timeout ( self , msecs ) : \n 
~~ self . _timeout = self . io_loop . add_timeout ( \n 
self . io_loop . time ( ) + msecs / 1000.0 , self . _handle_timeout ) \n 
~~ def _handle_events ( self , fd , events ) : \n 
action = 0 \n 
if events & ioloop . IOLoop . READ : \n 
~~~ action |= pycurl . CSELECT_IN \n 
~~ if events & ioloop . IOLoop . WRITE : \n 
~~~ action |= pycurl . CSELECT_OUT \n 
~~~ ret , num_handles = self . _multi . socket_action ( fd , action ) \n 
~~ except pycurl . error as e : \n 
~~~ ret = e . args [ 0 ] \n 
~~ if ret != pycurl . E_CALL_MULTI_PERFORM : \n 
~~ ~~ self . _finish_pending_requests ( ) \n 
~~ def _handle_timeout ( self ) : \n 
with stack_context . NullContext ( ) : \n 
~~~ self . _timeout = None \n 
~~~ ret , num_handles = self . _multi . socket_action ( \n 
pycurl . SOCKET_TIMEOUT , 0 ) \n 
~~ new_timeout = self . _multi . timeout ( ) \n 
if new_timeout >= 0 : \n 
~~~ self . _set_timeout ( new_timeout ) \n 
~~ ~~ def _handle_force_timeout ( self ) : \n 
~~~ ret , num_handles = self . _multi . socket_all ( ) \n 
~~ ~~ def _finish_pending_requests ( self ) : \n 
~~~ num_q , ok_list , err_list = self . _multi . info_read ( ) \n 
for curl in ok_list : \n 
~~~ self . _finish ( curl ) \n 
~~ for curl , errnum , errmsg in err_list : \n 
~~~ self . _finish ( curl , errnum , errmsg ) \n 
~~ if num_q == 0 : \n 
~~ ~~ self . _process_queue ( ) \n 
~~ def _process_queue ( self ) : \n 
~~~ with stack_context . NullContext ( ) : \n 
~~~ started = 0 \n 
while self . _free_list and self . _requests : \n 
~~~ started += 1 \n 
curl = self . _free_list . pop ( ) \n 
( request , callback ) = self . _requests . popleft ( ) \n 
curl . info = { \n 
"headers" : httputil . HTTPHeaders ( ) , \n 
"buffer" : BytesIO ( ) , \n 
"request" : request , \n 
"callback" : callback , \n 
"curl_start_time" : time . time ( ) , \n 
_curl_setup_request ( curl , request , curl . info [ "buffer" ] , \n 
curl . info [ "headers" ] ) \n 
self . _multi . add_handle ( curl ) \n 
~~ if not started : \n 
~~ ~~ ~~ ~~ def _finish ( self , curl , curl_error = None , curl_message = None ) : \n 
~~~ info = curl . info \n 
curl . info = None \n 
self . _multi . remove_handle ( curl ) \n 
self . _free_list . append ( curl ) \n 
buffer = info [ "buffer" ] \n 
if curl_error : \n 
~~~ error = CurlError ( curl_error , curl_message ) \n 
code = error . code \n 
effective_url = None \n 
buffer . close ( ) \n 
buffer = None \n 
~~~ error = None \n 
code = curl . getinfo ( pycurl . HTTP_CODE ) \n 
effective_url = curl . getinfo ( pycurl . EFFECTIVE_URL ) \n 
buffer . seek ( 0 ) \n 
~~ time_info = dict ( \n 
queue = info [ "curl_start_time" ] - info [ "request" ] . start_time , \n 
namelookup = curl . getinfo ( pycurl . NAMELOOKUP_TIME ) , \n 
connect = curl . getinfo ( pycurl . CONNECT_TIME ) , \n 
pretransfer = curl . getinfo ( pycurl . PRETRANSFER_TIME ) , \n 
starttransfer = curl . getinfo ( pycurl . STARTTRANSFER_TIME ) , \n 
total = curl . getinfo ( pycurl . TOTAL_TIME ) , \n 
redirect = curl . getinfo ( pycurl . REDIRECT_TIME ) , \n 
~~~ info [ "callback" ] ( HTTPResponse ( \n 
request = info [ "request" ] , code = code , headers = info [ "headers" ] , \n 
buffer = buffer , effective_url = effective_url , error = error , \n 
reason = info [ ] . get ( "X-Http-Reason" , None ) , \n 
request_time = time . time ( ) - info [ "curl_start_time" ] , \n 
time_info = time_info ) ) \n 
~~~ self . handle_callback_exception ( info [ "callback" ] ) \n 
~~ ~~ def handle_callback_exception ( self , callback ) : \n 
~~~ self . io_loop . handle_callback_exception ( callback ) \n 
~~ ~~ class CurlError ( HTTPError ) : \n 
~~~ def __init__ ( self , errno , message ) : \n 
~~~ HTTPError . __init__ ( self , 599 , message ) \n 
self . errno = errno \n 
~~ ~~ def _curl_create ( ) : \n 
~~~ curl = pycurl . Curl ( ) \n 
if gen_log . isEnabledFor ( logging . DEBUG ) : \n 
~~~ curl . setopt ( pycurl . VERBOSE , 1 ) \n 
curl . setopt ( pycurl . DEBUGFUNCTION , _curl_debug ) \n 
~~ return curl \n 
~~ def _curl_setup_request ( curl , request , buffer , headers ) : \n 
~~~ curl . setopt ( pycurl . URL , native_str ( request . url ) ) \n 
if "Expect" not in request . headers : \n 
~~~ request . headers [ "Expect" ] = "" \n 
~~ if "Pragma" not in request . headers : \n 
~~~ request . headers [ "Pragma" ] = "" \n 
~~ if isinstance ( request . headers , httputil . HTTPHeaders ) : \n 
~~~ curl . setopt ( pycurl . HTTPHEADER , \n 
~~ if request . header_callback : \n 
~~~ curl . setopt ( pycurl . HEADERFUNCTION , \n 
lambda line : request . header_callback ( native_str ( line ) ) ) \n 
lambda line : _curl_header_callback ( headers , \n 
native_str ( line ) ) ) \n 
~~ if request . streaming_callback : \n 
~~~ write_function = request . streaming_callback \n 
~~~ write_function = buffer . write \n 
~~~ curl . setopt ( pycurl . WRITEFUNCTION , write_function ) \n 
~~~ curl . setopt ( pycurl . WRITEFUNCTION , lambda s : write_function ( utf8 ( s ) ) ) \n 
~~ curl . setopt ( pycurl . FOLLOWLOCATION , request . follow_redirects ) \n 
curl . setopt ( pycurl . MAXREDIRS , request . max_redirects ) \n 
curl . setopt ( pycurl . CONNECTTIMEOUT_MS , int ( 1000 * request . connect_timeout ) ) \n 
curl . setopt ( pycurl . TIMEOUT_MS , int ( 1000 * request . request_timeout ) ) \n 
if request . user_agent : \n 
~~~ curl . setopt ( pycurl . USERAGENT , native_str ( request . user_agent ) ) \n 
~~ if request . network_interface : \n 
~~~ curl . setopt ( pycurl . INTERFACE , request . network_interface ) \n 
~~ if request . decompress_response : \n 
~~~ curl . setopt ( pycurl . ENCODING , "gzip,deflate" ) \n 
~~~ curl . setopt ( pycurl . ENCODING , "none" ) \n 
~~ if request . proxy_host and request . proxy_port : \n 
~~~ curl . setopt ( pycurl . PROXY , request . proxy_host ) \n 
curl . setopt ( pycurl . PROXYPORT , request . proxy_port ) \n 
if request . proxy_username : \n 
~~~ credentials = % ( request . proxy_username , \n 
request . proxy_password ) \n 
curl . setopt ( pycurl . PROXYUSERPWD , credentials ) \n 
~~~ curl . setopt ( pycurl . PROXY , ) \n 
curl . unsetopt ( pycurl . PROXYUSERPWD ) \n 
~~ if request . validate_cert : \n 
~~~ curl . setopt ( pycurl . SSL_VERIFYPEER , 1 ) \n 
curl . setopt ( pycurl . SSL_VERIFYHOST , 2 ) \n 
~~~ curl . setopt ( pycurl . SSL_VERIFYPEER , 0 ) \n 
curl . setopt ( pycurl . SSL_VERIFYHOST , 0 ) \n 
~~ if request . ca_certs is not None : \n 
~~~ curl . setopt ( pycurl . CAINFO , request . ca_certs ) \n 
~~ if request . allow_ipv6 is False : \n 
~~~ curl . setopt ( pycurl . IPRESOLVE , pycurl . IPRESOLVE_V4 ) \n 
~~~ curl . setopt ( pycurl . IPRESOLVE , pycurl . IPRESOLVE_WHATEVER ) \n 
~~ curl_options = { \n 
"GET" : pycurl . HTTPGET , \n 
"POST" : pycurl . POST , \n 
"PUT" : pycurl . UPLOAD , \n 
"HEAD" : pycurl . NOBODY , \n 
custom_methods = set ( [ "DELETE" , "OPTIONS" , "PATCH" ] ) \n 
for o in curl_options . values ( ) : \n 
~~~ curl . setopt ( o , False ) \n 
~~ if request . method in curl_options : \n 
~~~ curl . unsetopt ( pycurl . CUSTOMREQUEST ) \n 
curl . setopt ( curl_options [ request . method ] , True ) \n 
~~ elif request . allow_nonstandard_methods or request . method in custom_methods : \n 
~~~ curl . setopt ( pycurl . CUSTOMREQUEST , request . method ) \n 
~~~ raise KeyError ( + request . method ) \n 
~~ if request . method in ( "POST" , "PUT" ) : \n 
~~~ if request . body is None : \n 
~~~ raise AssertionError ( \n 
% request . method ) \n 
~~ request_buffer = BytesIO ( utf8 ( request . body ) ) \n 
curl . setopt ( pycurl . READFUNCTION , request_buffer . read ) \n 
~~~ def ioctl ( cmd ) : \n 
~~~ if cmd == curl . IOCMD_RESTARTREAD : \n 
~~~ request_buffer . seek ( 0 ) \n 
~~ ~~ curl . setopt ( pycurl . IOCTLFUNCTION , ioctl ) \n 
curl . setopt ( pycurl . POSTFIELDSIZE , len ( request . body ) ) \n 
~~~ curl . setopt ( pycurl . INFILESIZE , len ( request . body ) ) \n 
~~ ~~ elif request . method == "GET" : \n 
~~~ if request . body is not None : \n 
~~~ raise AssertionError ( ) \n 
~~ ~~ if request . auth_username is not None : \n 
~~~ userpwd = "%s:%s" % ( request . auth_username , request . auth_password or ) \n 
if request . auth_mode is None or request . auth_mode == "basic" : \n 
~~~ curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_BASIC ) \n 
~~ elif request . auth_mode == "digest" : \n 
~~~ curl . setopt ( pycurl . HTTPAUTH , pycurl . HTTPAUTH_DIGEST ) \n 
~~ curl . setopt ( pycurl . USERPWD , native_str ( userpwd ) ) \n 
request . auth_username ) \n 
~~~ curl . unsetopt ( pycurl . USERPWD ) \n 
~~ if request . client_cert is not None : \n 
~~~ curl . setopt ( pycurl . SSLCERT , request . client_cert ) \n 
~~ if request . client_key is not None : \n 
~~~ curl . setopt ( pycurl . SSLKEY , request . client_key ) \n 
~~ if threading . activeCount ( ) > 1 : \n 
~~~ curl . setopt ( pycurl . NOSIGNAL , 1 ) \n 
~~ if request . prepare_curl_callback is not None : \n 
~~~ request . prepare_curl_callback ( curl ) \n 
~~ ~~ def _curl_header_callback ( headers , header_line ) : \n 
~~~ header_line = header_line . strip ( ) \n 
if header_line . startswith ( "HTTP/" ) : \n 
~~~ headers . clear ( ) \n 
~~~ ( __ , __ , reason ) = httputil . parse_response_start_line ( header_line ) \n 
~~ except httputil . HTTPInputError : \n 
~~ ~~ if not header_line : \n 
~~ headers . parse_line ( header_line ) \n 
~~ def _curl_debug ( debug_type , debug_msg ) : \n 
~~~ debug_types = ( , , , , ) \n 
if debug_type == 0 : \n 
~~~ gen_log . debug ( , debug_msg . strip ( ) ) \n 
~~ elif debug_type in ( 1 , 2 ) : \n 
~~~ for line in debug_msg . splitlines ( ) : \n 
~~~ gen_log . debug ( , debug_types [ debug_type ] , line ) \n 
~~ ~~ elif debug_type == 4 : \n 
~~~ gen_log . debug ( , debug_types [ debug_type ] , debug_msg ) \n 
~~~ AsyncHTTPClient . configure ( CurlAsyncHTTPClient ) \n 
main ( ) \n 
import errno \n 
from tornado . log import app_log \n 
from tornado . ioloop import IOLoop \n 
from tornado . iostream import IOStream , SSLIOStream \n 
from tornado . netutil import bind_sockets , add_accept_handler , ssl_wrap_socket \n 
from tornado import process \n 
from tornado . util import errno_from_exception \n 
~~~ ssl = None \n 
~~ class TCPServer ( object ) : \n 
def __init__ ( self , io_loop = None , ssl_options = None , max_buffer_size = None , \n 
read_chunk_size = None ) : \n 
~~~ self . io_loop = io_loop \n 
self . ssl_options = ssl_options \n 
self . _pending_sockets = [ ] \n 
self . _started = False \n 
self . max_buffer_size = max_buffer_size \n 
self . read_chunk_size = None \n 
if self . ssl_options is not None and isinstance ( self . ssl_options , dict ) : \n 
~~~ if not in self . ssl_options : \n 
~~ if not os . path . exists ( self . ssl_options [ ] ) : \n 
self . ssl_options [ ] ) \n 
~~ if ( in self . ssl_options and \n 
not os . path . exists ( self . ssl_options [ ] ) ) : \n 
~~ ~~ ~~ def listen ( self , port , address = "" ) : \n 
sockets = bind_sockets ( port , address = address ) \n 
self . add_sockets ( sockets ) \n 
~~ def add_sockets ( self , sockets ) : \n 
if self . io_loop is None : \n 
~~~ self . io_loop = IOLoop . current ( ) \n 
~~ for sock in sockets : \n 
~~~ self . _sockets [ sock . fileno ( ) ] = sock \n 
add_accept_handler ( sock , self . _handle_connection , \n 
io_loop = self . io_loop ) \n 
~~ ~~ def add_socket ( self , socket ) : \n 
self . add_sockets ( [ socket ] ) \n 
~~ def bind ( self , port , address = None , family = socket . AF_UNSPEC , backlog = 128 ) : \n 
sockets = bind_sockets ( port , address = address , family = family , \n 
backlog = backlog ) \n 
if self . _started : \n 
~~~ self . add_sockets ( sockets ) \n 
~~~ self . _pending_sockets . extend ( sockets ) \n 
~~ ~~ def start ( self , num_processes = 1 ) : \n 
assert not self . _started \n 
self . _started = True \n 
if num_processes != 1 : \n 
~~~ process . fork_processes ( num_processes ) \n 
~~ sockets = self . _pending_sockets \n 
for fd , sock in self . _sockets . items ( ) : \n 
~~ ~~ def handle_stream ( self , stream , address ) : \n 
raise NotImplementedError ( ) \n 
~~ def _handle_connection ( self , connection , address ) : \n 
~~~ if self . ssl_options is not None : \n 
~~~ connection = ssl_wrap_socket ( connection , \n 
self . ssl_options , \n 
server_side = True , \n 
do_handshake_on_connect = False ) \n 
~~ except ssl . SSLError as err : \n 
~~~ if err . args [ 0 ] == ssl . SSL_ERROR_EOF : \n 
~~~ return connection . close ( ) \n 
~~~ raise \n 
~~ ~~ except socket . error as err : \n 
~~~ if errno_from_exception ( err ) in ( errno . ECONNABORTED , errno . EINVAL ) : \n 
~~~ stream = SSLIOStream ( connection , io_loop = self . io_loop , \n 
max_buffer_size = self . max_buffer_size , \n 
read_chunk_size = self . read_chunk_size ) \n 
~~~ stream = IOStream ( connection , io_loop = self . io_loop , \n 
~~ self . handle_stream ( stream , address ) \n 
~~ ~~ ~~ from textwrap import dedent \n 
from flask import abort , redirect , render_template , request , url_for \n 
from pypi_portal . core import flash \n 
from pypi_portal . blueprints import examples_alerts \n 
@ examples_alerts . route ( ) \n 
def index ( ) : \n 
~~~ return render_template ( ) \n 
~~ @ examples_alerts . route ( ) \n 
def modal ( ) : \n 
message_size = request . args . get ( ) \n 
flash_count = request . args . get ( ) \n 
flash_type = request . args . get ( ) \n 
available_types = [ k for k , v in flash . __dict__ . items ( ) if callable ( v ) ] \n 
if flash_type not in available_types : \n 
~~~ abort ( 400 ) \n 
~~ if not str ( flash_count ) . isdigit ( ) or not ( 1 <= int ( flash_count ) <= 10 ) : \n 
~~ if message_size == : \n 
~~ elif message_size == : \n 
~~~ message = \n 
~~ func = getattr ( flash , flash_type ) \n 
for i in range ( int ( flash_count ) ) : \n 
~~~ func ( message ) \n 
~~ return redirect ( url_for ( ) ) \n 
from colorclass . codes import ANSICodeMapping , BASE_CODES \n 
CODE_GROUPS = ( \n 
RE_ANSI = re . compile ( ) \n 
RE_COMBINE = re . compile ( ) \n 
RE_SPLIT = re . compile ( ) \n 
def prune_overridden ( ansi_string ) : \n 
for escape , codes in multi_seqs : \n 
~~~ r_codes = list ( reversed ( codes . split ( ) ) ) \n 
~~~ r_codes = r_codes [ : r_codes . index ( ) + 1 ] \n 
~~ for group in CODE_GROUPS : \n 
~~~ for pos in reversed ( [ i for i , n in enumerate ( r_codes ) if n in group ] [ 1 : ] ) : \n 
~~~ r_codes . pop ( pos ) \n 
~~ ~~ reduced_codes = . join ( sorted ( r_codes , key = int ) ) \n 
if codes != reduced_codes : \n 
~~~ ansi_string = ansi_string . replace ( escape , + reduced_codes + ) \n 
~~ ~~ return ansi_string \n 
~~ def parse_input ( tagged_string , disable_colors ) : \n 
codes = ANSICodeMapping ( tagged_string ) \n 
output_colors = getattr ( tagged_string , , tagged_string ) \n 
for tag , replacement in ( ( + k + , if v is None else % v ) for k , v in codes . ~~~ output_colors = output_colors . replace ( tag , replacement ) \n 
~~ output_no_colors = RE_ANSI . sub ( , output_colors ) \n 
if disable_colors : \n 
~~~ return output_no_colors , output_no_colors \n 
~~~ simplified = RE_COMBINE . sub ( , output_colors ) \n 
if simplified == output_colors : \n 
~~ output_colors = simplified \n 
~~ output_colors = prune_overridden ( output_colors ) \n 
previous_escape = None \n 
segments = list ( ) \n 
for item in ( i for i in RE_SPLIT . split ( output_colors ) if i ) : \n 
~~~ if RE_SPLIT . match ( item ) : \n 
~~~ if item != previous_escape : \n 
~~~ segments . append ( item ) \n 
previous_escape = item \n 
~~ ~~ output_colors = . join ( segments ) \n 
return output_colors , output_no_colors \n 
from terminaltables . tables import AsciiTable , UnixTable \n 
@ pytest . mark . parametrize ( , [ AsciiTable , UnixTable ] ) \n 
def test_empty ( cls ) : \n 
table = cls ( [ ] ) \n 
assert table . padded_table_data == [ ] \n 
table = cls ( [ [ ] ] ) \n 
assert table . padded_table_data == [ [ ] ] \n 
~~ @ pytest . mark . parametrize ( , [ AsciiTable , UnixTable ] ) \n 
def test_simple ( cls ) : \n 
table_data = [ \n 
[ , , ] , \n 
table = cls ( table_data ) \n 
expected = [ \n 
assert table . padded_table_data == expected \n 
table_data . append ( [ , ] ) \n 
table_data . append ( [ ] ) \n 
def test_attributes ( cls ) : \n 
[ , ] \n 
table . justify_columns [ 0 ] = \n 
[ , , ] \n 
table . justify_columns [ 2 ] = \n 
def test_multi_line ( cls ) : \n 
table . justify_columns = { 1 : , 2 : } \n 
~~ from flask . ext . wtf import Form \n 
from wtforms import StringField , TextAreaField , BooleanField , SelectField , SubmitField \n 
from wtforms . validators import Required , Length , Email , Regexp \n 
from wtforms import ValidationError \n 
from flask . ext . pagedown . fields import PageDownField \n 
from . . models import Role , User \n 
class NameForm ( Form ) : \n 
~~~ name = StringField ( , validators = [ Required ( ) ] ) \n 
submit = SubmitField ( ) \n 
~~ class EditProfileForm ( Form ) : \n 
~~~ name = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
location = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
about_me = TextAreaField ( ) \n 
~~ class EditProfileAdminForm ( Form ) : \n 
~~~ email = StringField ( , validators = [ Required ( ) , Length ( 1 , 64 ) , \n 
Email ( ) ] ) \n 
username = StringField ( , validators = [ \n 
Required ( ) , Length ( 1 , 64 ) , Regexp ( , 0 , \n 
) ] ) \n 
confirmed = BooleanField ( ) \n 
role = SelectField ( , coerce = int ) \n 
name = StringField ( , validators = [ Length ( 0 , 64 ) ] ) \n 
def __init__ ( self , user , * args , ** kwargs ) : \n 
~~~ super ( EditProfileAdminForm , self ) . __init__ ( * args , ** kwargs ) \n 
self . role . choices = [ ( role . id , role . name ) \n 
for role in Role . query . order_by ( Role . name ) . all ( ) ] \n 
~~ def validate_email ( self , field ) : \n 
~~~ if field . data != self . user . email and User . query . filter_by ( email = field . data ) . first ( ) : \n 
~~~ raise ValidationError ( ) \n 
~~ ~~ def validate_username ( self , field ) : \n 
~~~ if field . data != self . user . username and User . query . filter_by ( username = field . data ) . first ( ) : \n 
~~ ~~ ~~ class PostForm ( Form ) : \n 
~~ class CommentForm ( Form ) : \n 
~~~ body = StringField ( , validators = [ Required ( ) ] ) \n 
__all__ = [ "filter" , "fnmatch" , "fnmatchcase" , "translate" ] \n 
_cache = { } \n 
_MAXCACHE = 100 \n 
def _purge ( ) : \n 
_cache . clear ( ) \n 
~~ def fnmatch ( name , pat ) : \n 
name = os . path . normcase ( name ) \n 
pat = os . path . normcase ( pat ) \n 
return fnmatchcase ( name , pat ) \n 
~~ def filter ( names , pat ) : \n 
import os , posixpath \n 
~~~ re_pat = _cache [ pat ] \n 
~~~ res = translate ( pat ) \n 
if len ( _cache ) >= _MAXCACHE : \n 
~~~ _cache . clear ( ) \n 
~~ _cache [ pat ] = re_pat = re . compile ( res ) \n 
~~ match = re_pat . match \n 
if os . path is posixpath : \n 
~~~ for name in names : \n 
~~~ if match ( name ) : \n 
~~~ result . append ( name ) \n 
~~~ if match ( os . path . normcase ( name ) ) : \n 
~~ ~~ ~~ return result \n 
~~ def fnmatchcase ( name , pat ) : \n 
~~ return re_pat . match ( name ) is not None \n 
~~ def translate ( pat ) : \n 
i , n = 0 , len ( pat ) \n 
res = \n 
while i < n : \n 
~~~ c = pat [ i ] \n 
i = i + 1 \n 
if c == : \n 
~~~ res = res + \n 
~~ elif c == : \n 
~~~ j = i \n 
if j < n and pat [ j ] == : \n 
~~~ j = j + 1 \n 
~~ if j < n and pat [ j ] == : \n 
~~ while j < n and pat [ j ] != : \n 
~~ if j >= n : \n 
~~~ stuff = pat [ i : j ] . replace ( , ) \n 
i = j + 1 \n 
if stuff [ 0 ] == : \n 
~~~ stuff = + stuff [ 1 : ] \n 
~~ elif stuff [ 0 ] == : \n 
~~~ stuff = + stuff \n 
~~ res = % ( res , stuff ) \n 
~~~ res = res + re . escape ( c ) \n 
~~ ~~ return res + \n 
~~ from contextlib import contextmanager \n 
from sqlalchemy . types import NULLTYPE , Integer \n 
from sqlalchemy import schema as sa_schema \n 
from . import util \n 
from . compat import string_types \n 
from . ddl import impl \n 
__all__ = ( , ) \n 
class Operations ( object ) : \n 
def __init__ ( self , migration_context ) : \n 
self . migration_context = migration_context \n 
self . impl = migration_context . impl \n 
def context ( cls , migration_context ) : \n 
~~~ from . op import _install_proxy , _remove_proxy \n 
op = Operations ( migration_context ) \n 
_install_proxy ( op ) \n 
yield op \n 
_remove_proxy ( ) \n 
~~ def _primary_key_constraint ( self , name , table_name , cols , schema = None ) : \n 
~~~ m = sa_schema . MetaData ( ) \n 
columns = [ sa_schema . Column ( n , NULLTYPE ) for n in cols ] \n 
t1 = sa_schema . Table ( table_name , m , \n 
* columns , \n 
schema = schema ) \n 
p = sa_schema . PrimaryKeyConstraint ( * columns , name = name ) \n 
t1 . append_constraint ( p ) \n 
return p \n 
~~ def _foreign_key_constraint ( self , name , source , referent , \n 
local_cols , remote_cols , \n 
onupdate = None , ondelete = None , \n 
deferrable = None , source_schema = None , \n 
referent_schema = None ) : \n 
if source == referent : \n 
~~~ t1_cols = local_cols + remote_cols \n 
~~~ t1_cols = local_cols \n 
sa_schema . Table ( referent , m , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in remote_cols ] , \n 
schema = referent_schema ) \n 
~~ t1 = sa_schema . Table ( source , m , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in t1_cols ] , \n 
schema = source_schema ) \n 
tname = "%s.%s" % ( referent_schema , referent ) if referent_schema else referent \n 
f = sa_schema . ForeignKeyConstraint ( local_cols , \n 
[ "%s.%s" % ( tname , n ) \n 
for n in remote_cols ] , \n 
onupdate = onupdate , \n 
ondelete = ondelete , \n 
deferrable = deferrable \n 
t1 . append_constraint ( f ) \n 
return f \n 
~~ def _unique_constraint ( self , name , source , local_cols , schema = None , ** kw ) : \n 
~~~ t = sa_schema . Table ( source , sa_schema . MetaData ( ) , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in local_cols ] , \n 
kw [ ] = name \n 
uq = sa_schema . UniqueConstraint ( * [ t . c [ n ] for n in local_cols ] , ** kw ) \n 
t . append_constraint ( uq ) \n 
return uq \n 
~~ def _check_constraint ( self , name , source , condition , schema = None , ** kw ) : \n 
sa_schema . Column ( , Integer ) , schema = schema ) \n 
ck = sa_schema . CheckConstraint ( condition , name = name , ** kw ) \n 
t . append_constraint ( ck ) \n 
return ck \n 
~~ def _table ( self , name , * columns , ** kw ) : \n 
t = sa_schema . Table ( name , m , * columns , ** kw ) \n 
for f in t . foreign_keys : \n 
~~~ self . _ensure_table_for_fk ( m , f ) \n 
~~ return t \n 
~~ def _column ( self , name , type_ , ** kw ) : \n 
~~~ return sa_schema . Column ( name , type_ , ** kw ) \n 
~~ def _index ( self , name , tablename , columns , schema = None , ** kw ) : \n 
~~~ t = sa_schema . Table ( tablename or , sa_schema . MetaData ( ) , \n 
* [ sa_schema . Column ( n , NULLTYPE ) for n in columns ] , \n 
schema = schema \n 
return sa_schema . Index ( name , * [ t . c [ n ] for n in columns ] , ** kw ) \n 
~~ def _parse_table_key ( self , table_key ) : \n 
~~~ if in table_key : \n 
~~~ tokens = table_key . split ( ) \n 
sname = "." . join ( tokens [ 0 : - 1 ] ) \n 
tname = tokens [ - 1 ] \n 
~~~ tname = table_key \n 
sname = None \n 
~~ return ( sname , tname ) \n 
~~ def _ensure_table_for_fk ( self , metadata , fk ) : \n 
if isinstance ( fk . _colspec , string_types ) : \n 
~~~ table_key , cname = fk . _colspec . rsplit ( , 1 ) \n 
sname , tname = self . _parse_table_key ( table_key ) \n 
if table_key not in metadata . tables : \n 
~~~ rel_t = sa_schema . Table ( tname , metadata , schema = sname ) \n 
~~~ rel_t = metadata . tables [ table_key ] \n 
~~ if cname not in rel_t . c : \n 
~~~ rel_t . append_column ( sa_schema . Column ( cname , NULLTYPE ) ) \n 
~~ ~~ ~~ def get_context ( self ) : \n 
return self . migration_context \n 
~~ def rename_table ( self , old_table_name , new_table_name , schema = None ) : \n 
self . impl . rename_table ( \n 
old_table_name , \n 
new_table_name , \n 
~~ @ util . _with_legacy_names ( [ ( , ) ] ) \n 
def alter_column ( self , table_name , column_name , \n 
nullable = None , \n 
server_default = False , \n 
new_column_name = None , \n 
type_ = None , \n 
autoincrement = None , \n 
existing_type = None , \n 
existing_server_default = False , \n 
existing_nullable = None , \n 
existing_autoincrement = None , \n 
schema = None \n 
compiler = self . impl . dialect . statement_compiler ( \n 
self . impl . dialect , \n 
None \n 
def _count_constraint ( constraint ) : \n 
~~~ return not isinstance ( constraint , sa_schema . PrimaryKeyConstraint ) and ( not constraint . _create_rule or \n 
constraint . _create_rule ( compiler ) ) \n 
~~ if existing_type and type_ : \n 
~~~ t = self . _table ( table_name , \n 
sa_schema . Column ( column_name , existing_type ) , \n 
for constraint in t . constraints : \n 
~~~ if _count_constraint ( constraint ) : \n 
~~~ self . impl . drop_constraint ( constraint ) \n 
~~ ~~ ~~ self . impl . alter_column ( table_name , column_name , \n 
nullable = nullable , \n 
server_default = server_default , \n 
name = new_column_name , \n 
type_ = type_ , \n 
schema = schema , \n 
autoincrement = autoincrement , \n 
existing_type = existing_type , \n 
existing_server_default = existing_server_default , \n 
existing_nullable = existing_nullable , \n 
existing_autoincrement = existing_autoincrement \n 
if type_ : \n 
sa_schema . Column ( column_name , type_ ) , \n 
~~~ self . impl . add_constraint ( constraint ) \n 
~~ ~~ ~~ ~~ def add_column ( self , table_name , column , schema = None ) : \n 
t = self . _table ( table_name , column , schema = schema ) \n 
self . impl . add_column ( \n 
table_name , \n 
column , \n 
~~~ if not isinstance ( constraint , sa_schema . PrimaryKeyConstraint ) : \n 
~~ ~~ ~~ def drop_column ( self , table_name , column_name , ** kw ) : \n 
self . impl . drop_column ( \n 
self . _column ( column_name , NULLTYPE ) , \n 
** kw \n 
~~ def create_primary_key ( self , name , table_name , cols , schema = None ) : \n 
self . impl . add_constraint ( \n 
self . _primary_key_constraint ( name , table_name , cols , \n 
schema ) \n 
~~ def create_foreign_key ( self , name , source , referent , local_cols , \n 
remote_cols , onupdate = None , ondelete = None , \n 
self . _foreign_key_constraint ( name , source , referent , \n 
onupdate = onupdate , ondelete = ondelete , \n 
deferrable = deferrable , source_schema = source_schema , \n 
referent_schema = referent_schema ) \n 
~~ def create_unique_constraint ( self , name , source , local_cols , \n 
schema = None , ** kw ) : \n 
self . _unique_constraint ( name , source , local_cols , \n 
schema = schema , ** kw ) \n 
~~ def create_check_constraint ( self , name , source , condition , \n 
self . _check_constraint ( name , source , condition , schema = schema , ** kw ) \n 
~~ def create_table ( self , name , * columns , ** kw ) : \n 
self . impl . create_table ( \n 
self . _table ( name , * columns , ** kw ) \n 
~~ def drop_table ( self , name , ** kw ) : \n 
self . impl . drop_table ( \n 
self . _table ( name , ** kw ) \n 
~~ def create_index ( self , name , table_name , columns , schema = None , ** kw ) : \n 
self . impl . create_index ( \n 
self . _index ( name , table_name , columns , schema = schema , ** kw ) \n 
def drop_index ( self , name , table_name = None , schema = None ) : \n 
self . impl . drop_index ( \n 
self . _index ( name , table_name , [ ] , schema = schema ) \n 
~~ @ util . _with_legacy_names ( [ ( "type" , "type_" ) ] ) \n 
def drop_constraint ( self , name , table_name , type_ = None , schema = None ) : \n 
t = self . _table ( table_name , schema = schema ) \n 
types = { \n 
: lambda name : sa_schema . ForeignKeyConstraint ( \n 
[ ] , [ ] , name = name ) , \n 
: sa_schema . PrimaryKeyConstraint , \n 
: sa_schema . UniqueConstraint , \n 
: lambda name : sa_schema . CheckConstraint ( "" , name = name ) , \n 
None : sa_schema . Constraint \n 
~~~ const = types [ type_ ] \n 
~~ const = const ( name = name ) \n 
t . append_constraint ( const ) \n 
self . impl . drop_constraint ( const ) \n 
~~ def bulk_insert ( self , table , rows ) : \n 
self . impl . bulk_insert ( table , rows ) \n 
~~ def inline_literal ( self , value , type_ = None ) : \n 
return impl . _literal_bindparam ( None , value , type_ = type_ ) \n 
~~ def execute ( self , sql , execution_options = None ) : \n 
self . migration_context . impl . execute ( sql , \n 
execution_options = execution_options ) \n 
~~ def get_bind ( self ) : \n 
return self . migration_context . impl . bind \n 
import os , re , sys \n 
~~~ from sets import Set as set \n 
~~~ sorted = sorted \n 
~~~ def sorted ( iterable ) : \n 
lst = list ( iterable ) \n 
lst . sort ( ) \n 
return lst \n 
~~~ reversed = reversed \n 
~~~ def reversed ( iterable ) : \n 
return lst [ : : - 1 ] \n 
~~~ "" . rpartition \n 
~~~ def rpartition ( s , sep ) : \n 
i = s . rfind ( sep ) \n 
if i == - 1 : \n 
~~~ return ( , , s ) \n 
~~~ return ( s [ : i ] , sep , s [ i + len ( sep ) : ] ) \n 
return s . rpartition ( sep ) \n 
~~~ from cStringIO import StringIO \n 
BytesIO = StringIO \n 
~~~ from io import StringIO , BytesIO \n 
~~~ string_class = basestring \n 
~~~ string_class = str \n 
~~~ import cPickle as pickle \n 
~~~ range = xrange \n 
~~~ range = range \n 
~~~ { } . iteritems \n 
~~~ def iitems ( d ) : \n 
return d . items ( ) \n 
return d . iteritems ( ) \n 
~~ ~~ if sys . version_info >= ( 3 , 0 ) : \n 
~~~ def exec_code_object ( code , global_map ) : \n 
exec ( code , global_map ) \n 
~~~ eval ( \n 
compile ( \n 
"<exec_function>" , "exec" \n 
~~ if sys . version_info >= ( 3 , 0 ) : \n 
~~~ import tokenize \n 
~~~ from io import TextIOWrapper \n 
def open_source ( fname ) : \n 
buffer = open ( fname , ) \n 
encoding , _ = detect_encoding ( buffer . readline ) \n 
text = TextIOWrapper ( buffer , encoding , line_buffering = True ) \n 
text . mode = \n 
return text \n 
~~~ def open_source ( fname ) : \n 
return open ( fname , "rU" ) \n 
~~~ def to_bytes ( s ) : \n 
return s . encode ( ) \n 
~~ def to_string ( b ) : \n 
return b . decode ( ) \n 
~~ def binary_bytes ( byte_values ) : \n 
return bytes ( byte_values ) \n 
~~ def byte_to_int ( byte_value ) : \n 
return byte_value \n 
~~ def bytes_to_ints ( bytes_value ) : \n 
return bytes_value \n 
return b \n 
return "" . join ( [ chr ( b ) for b in byte_values ] ) \n 
return ord ( byte_value ) \n 
for byte in bytes_value : \n 
~~~ yield ord ( byte ) \n 
~~~ import hashlib \n 
md5 = hashlib . md5 \n 
~~~ import md5 \n 
md5 = md5 . new \n 
~~ from wtforms . fields import TextAreaField \n 
from . widgets import PageDown \n 
class PageDownField ( TextAreaField ) : \n 
~~~ widget = PageDown ( ) \n 
from . . dictionaries_loader import get_dictionary \n 
__all__ = [ \n 
def first_name ( ) : \n 
_dict = get_dictionary ( ) \n 
_dict += get_dictionary ( ) \n 
return random . choice ( _dict ) . strip ( ) \n 
~~ def last_name ( ) : \n 
return random . choice ( get_dictionary ( ) ) . strip ( ) \n 
~~ def full_name ( ) : \n 
return first_name ( ) + + last_name ( ) \n 
~~ def male_first_name ( ) : \n 
~~ def female_first_name ( ) : \n 
~~ def company_name ( ) : \n 
~~ def job_title ( ) : \n 
result = random . choice ( get_dictionary ( ) ) . strip ( ) \n 
result = result . replace ( , job_title_suffix ( ) ) \n 
return result \n 
~~ def job_title_suffix ( ) : \n 
~~ def title ( ) : \n 
~~ def suffix ( ) : \n 
~~ def location ( ) : \n 
~~ def industry ( ) : \n 
~~ import errno \n 
from gunicorn import util \n 
from gunicorn . six import string_types \n 
SD_LISTEN_FDS_START = 3 \n 
class BaseSocket ( object ) : \n 
~~~ def __init__ ( self , address , conf , log , fd = None ) : \n 
~~~ self . log = log \n 
self . conf = conf \n 
self . cfg_addr = address \n 
if fd is None : \n 
~~~ sock = socket . socket ( self . FAMILY , socket . SOCK_STREAM ) \n 
~~~ sock = socket . fromfd ( fd , self . FAMILY , socket . SOCK_STREAM ) \n 
~~ self . sock = self . set_options ( sock , bound = ( fd is not None ) ) \n 
~~ def __str__ ( self , name ) : \n 
~~~ return getattr ( self . sock , name ) \n 
~~ def set_options ( self , sock , bound = False ) : \n 
if not bound : \n 
~~~ self . bind ( sock ) \n 
~~ sock . setblocking ( 0 ) \n 
if hasattr ( sock , "set_inheritable" ) : \n 
~~~ sock . set_inheritable ( True ) \n 
~~ sock . listen ( self . conf . backlog ) \n 
return sock \n 
~~ def bind ( self , sock ) : \n 
~~~ sock . bind ( self . cfg_addr ) \n 
~~~ self . sock . close ( ) \n 
~~ except socket . error as e : \n 
~~ del self . sock \n 
~~ ~~ class TCPSocket ( BaseSocket ) : \n 
~~~ FAMILY = socket . AF_INET \n 
def __str__ ( self ) : \n 
~~~ if self . conf . is_ssl : \n 
~~~ scheme = "https" \n 
~~~ scheme = "http" \n 
~~ addr = self . sock . getsockname ( ) \n 
return "%s://%s:%d" % ( scheme , addr [ 0 ] , addr [ 1 ] ) \n 
~~~ sock . setsockopt ( socket . IPPROTO_TCP , socket . TCP_NODELAY , 1 ) \n 
return super ( TCPSocket , self ) . set_options ( sock , bound = bound ) \n 
~~ ~~ class TCP6Socket ( TCPSocket ) : \n 
~~~ FAMILY = socket . AF_INET6 \n 
~~~ ( host , port , fl , sc ) = self . sock . getsockname ( ) \n 
return "http://[%s]:%d" % ( host , port ) \n 
~~ ~~ class UnixSocket ( BaseSocket ) : \n 
~~~ FAMILY = socket . AF_UNIX \n 
def __init__ ( self , addr , conf , log , fd = None ) : \n 
~~~ if fd is None : \n 
~~~ st = os . stat ( addr ) \n 
~~ except OSError as e : \n 
~~~ if e . args [ 0 ] != errno . ENOENT : \n 
~~~ if stat . S_ISSOCK ( st . st_mode ) : \n 
~~~ os . remove ( addr ) \n 
~~ ~~ ~~ self . parent = os . getpid ( ) \n 
super ( UnixSocket , self ) . __init__ ( addr , conf , log , fd = fd ) \n 
~~~ return "unix:%s" % self . cfg_addr \n 
~~~ old_umask = os . umask ( self . conf . umask ) \n 
sock . bind ( self . cfg_addr ) \n 
util . chown ( self . cfg_addr , self . conf . uid , self . conf . gid ) \n 
os . umask ( old_umask ) \n 
~~~ super ( UnixSocket , self ) . close ( ) \n 
if self . parent == os . getpid ( ) : \n 
~~~ os . unlink ( self . cfg_addr ) \n 
~~ ~~ ~~ def _sock_type ( addr ) : \n 
~~~ if isinstance ( addr , tuple ) : \n 
~~~ if util . is_ipv6 ( addr [ 0 ] ) : \n 
~~~ sock_type = TCP6Socket \n 
~~~ sock_type = TCPSocket \n 
~~ ~~ elif isinstance ( addr , string_types ) : \n 
~~~ sock_type = UnixSocket \n 
~~ return sock_type \n 
~~ def create_sockets ( conf , log ) : \n 
listeners = [ ] \n 
if ( in os . environ \n 
and int ( os . environ . get ( ) ) == os . getpid ( ) ) : \n 
~~~ for i in range ( int ( os . environ . get ( , 0 ) ) ) : \n 
~~~ fd = i + SD_LISTEN_FDS_START \n 
~~~ sock = socket . fromfd ( fd , socket . AF_UNIX , socket . SOCK_STREAM ) \n 
sockname = sock . getsockname ( ) \n 
if isinstance ( sockname , str ) and sockname . startswith ( ) : \n 
~~~ listeners . append ( UnixSocket ( sockname , conf , log , fd = fd ) ) \n 
~~ elif len ( sockname ) == 2 and in sockname [ 0 ] : \n 
~~~ listeners . append ( TCPSocket ( "%s:%s" % sockname , conf , log , \n 
fd = fd ) ) \n 
~~ elif len ( sockname ) == 4 and in sockname [ 0 ] : \n 
~~~ listeners . append ( TCP6Socket ( "[%s]:%s" % sockname [ : 2 ] , conf , \n 
log , fd = fd ) ) \n 
~~ ~~ except socket . error : \n 
~~ ~~ del os . environ [ ] , os . environ [ ] \n 
if listeners : \n 
~~~ log . debug ( , \n 
"," . join ( [ str ( l ) for l in listeners ] ) ) \n 
return listeners \n 
~~ ~~ laddr = conf . address \n 
if conf . certfile and not os . path . exists ( conf . certfile ) : \n 
~~ if conf . keyfile and not os . path . exists ( conf . keyfile ) : \n 
~~ if in os . environ : \n 
~~~ fds = os . environ . pop ( ) . split ( ) \n 
for i , fd in enumerate ( fds ) : \n 
~~~ fd = int ( fd ) \n 
addr = laddr [ i ] \n 
sock_type = _sock_type ( addr ) \n 
~~~ listeners . append ( sock_type ( addr , conf , log , fd = fd ) ) \n 
~~~ if e . args [ 0 ] == errno . ENOTCONN : \n 
~~ ~~ ~~ return listeners \n 
~~ for addr in laddr : \n 
~~~ sock_type = _sock_type ( addr ) \n 
sock = None \n 
for i in range ( 5 ) : \n 
~~~ sock = sock_type ( addr , conf , log ) \n 
~~~ if e . args [ 0 ] == errno . EADDRINUSE : \n 
~~ if e . args [ 0 ] == errno . EADDRNOTAVAIL : \n 
~~ if i < 5 : \n 
time . sleep ( 1 ) \n 
~~ ~~ if sock is None : \n 
~~ listeners . append ( sock ) \n 
~~ return listeners \n 
from requests . compat import ( \n 
is_windows , \n 
bytes , \n 
str , \n 
is_py3 , \n 
is_py26 , \n 
~~~ from urllib . parse import urlsplit \n 
~~~ from urlparse import urlsplit \n 
from . import Extension \n 
from . . blockprocessors import OListProcessor , UListProcessor \n 
class SaneOListProcessor ( OListProcessor ) : \n 
~~~ CHILD_RE = re . compile ( ) \n 
SIBLING_TAGS = [ ] \n 
~~ class SaneUListProcessor ( UListProcessor ) : \n 
~~ class SaneListExtension ( Extension ) : \n 
def extendMarkdown ( self , md , md_globals ) : \n 
md . parser . blockprocessors [ ] = SaneOListProcessor ( md . parser ) \n 
md . parser . blockprocessors [ ] = SaneUListProcessor ( md . parser ) \n 
~~ ~~ def makeExtension ( configs = { } ) : \n 
~~~ return SaneListExtension ( configs = configs ) \n 
~~ from __future__ import absolute_import , division , unicode_literals \n 
from pip . _vendor . six import text_type \n 
import gettext \n 
_ = gettext . gettext \n 
from . . constants import voidElements , spaceCharacters \n 
spaceCharacters = "" . join ( spaceCharacters ) \n 
class TreeWalker ( object ) : \n 
~~~ def __init__ ( self , tree ) : \n 
~~~ self . tree = tree \n 
~~ def __iter__ ( self ) : \n 
~~~ raise NotImplementedError \n 
~~ def error ( self , msg ) : \n 
~~~ return { "type" : "SerializeError" , "data" : msg } \n 
~~ def emptyTag ( self , namespace , name , attrs , hasChildren = False ) : \n 
~~~ assert namespace is None or isinstance ( namespace , text_type ) , type ( namespace ) \n 
assert isinstance ( name , text_type ) , type ( name ) \n 
assert all ( ( namespace is None or isinstance ( namespace , text_type ) ) and \n 
isinstance ( name , text_type ) and \n 
isinstance ( value , text_type ) \n 
for ( namespace , name ) , value in attrs . items ( ) ) \n 
yield { "type" : "EmptyTag" , "name" : name , \n 
"namespace" : namespace , \n 
"data" : attrs } \n 
if hasChildren : \n 
~~ ~~ def startTag ( self , namespace , name , attrs ) : \n 
return { "type" : "StartTag" , \n 
"name" : name , \n 
~~ def endTag ( self , namespace , name ) : \n 
assert isinstance ( name , text_type ) , type ( namespace ) \n 
return { "type" : "EndTag" , \n 
"data" : { } } \n 
~~ def text ( self , data ) : \n 
~~~ assert isinstance ( data , text_type ) , type ( data ) \n 
data = data \n 
middle = data . lstrip ( spaceCharacters ) \n 
left = data [ : len ( data ) - len ( middle ) ] \n 
if left : \n 
~~~ yield { "type" : "SpaceCharacters" , "data" : left } \n 
~~ data = middle \n 
middle = data . rstrip ( spaceCharacters ) \n 
right = data [ len ( middle ) : ] \n 
if middle : \n 
~~~ yield { "type" : "Characters" , "data" : middle } \n 
~~ if right : \n 
~~~ yield { "type" : "SpaceCharacters" , "data" : right } \n 
~~ ~~ def comment ( self , data ) : \n 
return { "type" : "Comment" , "data" : data } \n 
~~ def doctype ( self , name , publicId = None , systemId = None , correct = True ) : \n 
~~~ assert name is None or isinstance ( name , text_type ) , type ( name ) \n 
assert publicId is None or isinstance ( publicId , text_type ) , type ( publicId ) \n 
assert systemId is None or isinstance ( systemId , text_type ) , type ( systemId ) \n 
return { "type" : "Doctype" , \n 
"name" : name if name is not None else "" , \n 
"publicId" : publicId , \n 
"systemId" : systemId , \n 
"correct" : correct } \n 
~~ def entity ( self , name ) : \n 
~~~ assert isinstance ( name , text_type ) , type ( name ) \n 
return { "type" : "Entity" , "name" : name } \n 
~~ def unknown ( self , nodeType ) : \n 
~~ ~~ class RecursiveTreeWalker ( TreeWalker ) : \n 
~~~ def walkChildren ( self , node ) : \n 
~~ def element ( self , node , namespace , name , attrs , hasChildren ) : \n 
~~~ if name in voidElements : \n 
~~~ for token in self . emptyTag ( namespace , name , attrs , hasChildren ) : \n 
~~~ yield token \n 
~~~ yield self . startTag ( name , attrs ) \n 
~~~ for token in self . walkChildren ( node ) : \n 
~~ ~~ yield self . endTag ( name ) \n 
~~ ~~ ~~ from xml . dom import Node \n 
DOCUMENT = Node . DOCUMENT_NODE \n 
DOCTYPE = Node . DOCUMENT_TYPE_NODE \n 
TEXT = Node . TEXT_NODE \n 
ELEMENT = Node . ELEMENT_NODE \n 
COMMENT = Node . COMMENT_NODE \n 
ENTITY = Node . ENTITY_NODE \n 
UNKNOWN = "<#UNKNOWN#>" \n 
class NonRecursiveTreeWalker ( TreeWalker ) : \n 
~~~ def getNodeDetails ( self , node ) : \n 
~~ def getFirstChild ( self , node ) : \n 
~~ def getNextSibling ( self , node ) : \n 
~~ def getParentNode ( self , node ) : \n 
~~~ currentNode = self . tree \n 
while currentNode is not None : \n 
~~~ details = self . getNodeDetails ( currentNode ) \n 
type , details = details [ 0 ] , details [ 1 : ] \n 
hasChildren = False \n 
if type == DOCTYPE : \n 
~~~ yield self . doctype ( * details ) \n 
~~ elif type == TEXT : \n 
~~~ for token in self . text ( * details ) : \n 
~~ ~~ elif type == ELEMENT : \n 
~~~ namespace , name , attributes , hasChildren = details \n 
if name in voidElements : \n 
~~~ for token in self . emptyTag ( namespace , name , attributes , \n 
hasChildren ) : \n 
~~ hasChildren = False \n 
~~~ yield self . startTag ( namespace , name , attributes ) \n 
~~ ~~ elif type == COMMENT : \n 
~~~ yield self . comment ( details [ 0 ] ) \n 
~~ elif type == ENTITY : \n 
~~~ yield self . entity ( details [ 0 ] ) \n 
~~ elif type == DOCUMENT : \n 
~~~ hasChildren = True \n 
~~~ yield self . unknown ( details [ 0 ] ) \n 
~~ if hasChildren : \n 
~~~ firstChild = self . getFirstChild ( currentNode ) \n 
~~~ firstChild = None \n 
~~ if firstChild is not None : \n 
~~~ currentNode = firstChild \n 
~~~ while currentNode is not None : \n 
if type == ELEMENT : \n 
if name not in voidElements : \n 
~~~ yield self . endTag ( namespace , name ) \n 
~~ ~~ if self . tree is currentNode : \n 
~~~ currentNode = None \n 
~~ nextSibling = self . getNextSibling ( currentNode ) \n 
if nextSibling is not None : \n 
~~~ currentNode = nextSibling \n 
~~~ currentNode = self . getParentNode ( currentNode ) \n 
esc = "\\x1b[" \n 
codes = { } \n 
codes [ "" ] = "" \n 
codes [ "reset" ] = esc + "39;49;00m" \n 
codes [ "bold" ] = esc + "01m" \n 
codes [ "faint" ] = esc + "02m" \n 
codes [ "standout" ] = esc + "03m" \n 
codes [ "underline" ] = esc + "04m" \n 
codes [ "blink" ] = esc + "05m" \n 
codes [ "overline" ] = esc + "06m" \n 
dark_colors = [ "black" , "darkred" , "darkgreen" , "brown" , "darkblue" , \n 
"purple" , "teal" , "lightgray" ] \n 
light_colors = [ "darkgray" , "red" , "green" , "yellow" , "blue" , \n 
"fuchsia" , "turquoise" , "white" ] \n 
x = 30 \n 
for d , l in zip ( dark_colors , light_colors ) : \n 
~~~ codes [ d ] = esc + "%im" % x \n 
codes [ l ] = esc + "%i;01m" % x \n 
x += 1 \n 
~~ del d , l , x \n 
codes [ "darkteal" ] = codes [ "turquoise" ] \n 
codes [ "darkyellow" ] = codes [ "brown" ] \n 
codes [ "fuscia" ] = codes [ "fuchsia" ] \n 
codes [ "white" ] = codes [ "bold" ] \n 
def reset_color ( ) : \n 
~~~ return codes [ "reset" ] \n 
~~ def colorize ( color_key , text ) : \n 
~~~ return codes [ color_key ] + text + codes [ "reset" ] \n 
~~ def ansiformat ( attr , text ) : \n 
if attr [ : 1 ] == attr [ - 1 : ] == : \n 
~~~ result . append ( codes [ ] ) \n 
attr = attr [ 1 : - 1 ] \n 
~~ if attr [ : 1 ] == attr [ - 1 : ] == : \n 
~~ result . append ( codes [ attr ] ) \n 
result . append ( text ) \n 
result . append ( codes [ ] ) \n 
return . join ( result ) \n 
CONSTANTS = [ , \n 
FUNCTIONS = [ , \n 
DISTRIBUTIONS = [ , \n 
from pygments . style import Style \n 
from pygments . token import Keyword , Name , Comment , String , Error , Number , Operator , Generic , Whitespace \n 
class DefaultStyle ( Style ) : \n 
background_color = "#f8f8f8" \n 
default_style = "" \n 
styles = { \n 
Whitespace : "#bbbbbb" , \n 
Keyword . Pseudo : "nobold" , \n 
Operator : "#666666" , \n 
Name . Builtin : "#008000" , \n 
Name . Function : "#0000FF" , \n 
Name . Variable : "#19177C" , \n 
Name . Constant : "#880000" , \n 
Name . Label : "#A0A000" , \n 
Name . Attribute : "#7D9029" , \n 
Name . Decorator : "#AA22FF" , \n 
String : "#BA2121" , \n 
String . Doc : "italic" , \n 
String . Regex : "#BB6688" , \n 
String . Symbol : "#19177C" , \n 
String . Other : "#008000" , \n 
Number : "#666666" , \n 
Generic . Deleted : "#A00000" , \n 
Generic . Inserted : "#00A000" , \n 
Generic . Error : "#FF0000" , \n 
Generic . Emph : "italic" , \n 
Generic . Strong : "bold" , \n 
Generic . Output : "#888" , \n 
Generic . Traceback : "#04D" , \n 
Error : "border:#FF0000" \n 
from socket import error as SocketError , timeout as SocketTimeout \n 
~~~ from queue import LifoQueue , Empty , Full \n 
~~~ from Queue import LifoQueue , Empty , Full \n 
~~ from . exceptions import ( \n 
ClosedPoolError , \n 
ConnectTimeoutError , \n 
EmptyPoolError , \n 
HostChangedError , \n 
MaxRetryError , \n 
SSLError , \n 
TimeoutError , \n 
ReadTimeoutError , \n 
ProxyError , \n 
from . packages . ssl_match_hostname import CertificateError \n 
from . packages import six \n 
from . connection import ( \n 
DummyConnection , \n 
HTTPConnection , HTTPSConnection , VerifiedHTTPSConnection , \n 
HTTPException , BaseSSLError , \n 
from . request import RequestMethods \n 
from . response import HTTPResponse \n 
from . util import ( \n 
assert_fingerprint , \n 
get_host , \n 
is_connection_dropped , \n 
Timeout , \n 
xrange = six . moves . xrange \n 
log = logging . getLogger ( __name__ ) \n 
_Default = object ( ) \n 
port_by_scheme = { \n 
: 80 , \n 
: 443 , \n 
class ConnectionPool ( object ) : \n 
scheme = None \n 
QueueCls = LifoQueue \n 
def __init__ ( self , host , port = None ) : \n 
~~~ host = host . strip ( ) \n 
self . port = port \n 
~~~ return % ( type ( self ) . __name__ , \n 
self . host , self . port ) \n 
~~ ~~ _blocking_errnos = set ( [ errno . EAGAIN , errno . EWOULDBLOCK ] ) \n 
class HTTPConnectionPool ( ConnectionPool , RequestMethods ) : \n 
scheme = \n 
ConnectionCls = HTTPConnection \n 
def __init__ ( self , host , port = None , strict = False , \n 
timeout = Timeout . DEFAULT_TIMEOUT , maxsize = 1 , block = False , \n 
headers = None , _proxy = None , _proxy_headers = None ) : \n 
~~~ ConnectionPool . __init__ ( self , host , port ) \n 
RequestMethods . __init__ ( self , headers ) \n 
self . strict = strict \n 
if not isinstance ( timeout , Timeout ) : \n 
~~~ timeout = Timeout . from_float ( timeout ) \n 
~~ self . timeout = timeout \n 
self . pool = self . QueueCls ( maxsize ) \n 
self . block = block \n 
self . proxy = _proxy \n 
self . proxy_headers = _proxy_headers or { } \n 
for _ in xrange ( maxsize ) : \n 
~~~ self . pool . put ( None ) \n 
~~ self . num_connections = 0 \n 
self . num_requests = 0 \n 
~~ def _new_conn ( self ) : \n 
self . num_connections += 1 \n 
( self . num_connections , self . host ) ) \n 
extra_params = { } \n 
~~~ extra_params [ ] = self . strict \n 
~~ return self . ConnectionCls ( host = self . host , port = self . port , \n 
timeout = self . timeout . connect_timeout , \n 
** extra_params ) \n 
~~ def _get_conn ( self , timeout = None ) : \n 
conn = None \n 
~~~ conn = self . pool . get ( block = self . block , timeout = timeout ) \n 
~~ except Empty : \n 
~~~ if self . block : \n 
~~~ raise EmptyPoolError ( self , \n 
~~ pass \n 
~~ if conn and is_connection_dropped ( conn ) : \n 
~~ return conn or self . _new_conn ( ) \n 
~~ def _put_conn ( self , conn ) : \n 
~~~ self . pool . put ( conn , block = False ) \n 
~~ except Full : \n 
% self . host ) \n 
~~ if conn : \n 
~~~ conn . close ( ) \n 
~~ ~~ def _get_timeout ( self , timeout ) : \n 
if timeout is _Default : \n 
~~~ return self . timeout . clone ( ) \n 
~~ if isinstance ( timeout , Timeout ) : \n 
~~~ return timeout . clone ( ) \n 
~~~ return Timeout . from_float ( timeout ) \n 
~~ ~~ def _make_request ( self , conn , method , url , timeout = _Default , \n 
** httplib_request_kw ) : \n 
self . num_requests += 1 \n 
timeout_obj = self . _get_timeout ( timeout ) \n 
~~~ timeout_obj . start_connect ( ) \n 
conn . timeout = timeout_obj . connect_timeout \n 
conn . request ( method , url , ** httplib_request_kw ) \n 
~~ except SocketTimeout : \n 
~~~ raise ConnectTimeoutError ( \n 
( self . host , timeout_obj . connect_timeout ) ) \n 
~~ read_timeout = timeout_obj . read_timeout \n 
if hasattr ( conn , ) : \n 
~~~ if read_timeout == 0 : \n 
~~~ raise ReadTimeoutError ( \n 
self , url , \n 
~~ if read_timeout is Timeout . DEFAULT_TIMEOUT : \n 
~~~ conn . sock . settimeout ( socket . getdefaulttimeout ( ) ) \n 
~~~ conn . sock . settimeout ( read_timeout ) \n 
~~~ httplib_response = conn . getresponse ( buffering = True ) \n 
~~~ httplib_response = conn . getresponse ( ) \n 
~~ ~~ except SocketTimeout : \n 
~~ except BaseSSLError as e : \n 
~~ raise \n 
~~~ if e . errno in _blocking_errnos : \n 
~~ http_version = getattr ( conn , , ) \n 
httplib_response . status , \n 
httplib_response . length ) ) \n 
return httplib_response \n 
old_pool , self . pool = self . pool , None \n 
~~~ conn = old_pool . get ( block = False ) \n 
if conn : \n 
~~ ~~ ~~ except Empty : \n 
~~ ~~ def is_same_host ( self , url ) : \n 
if url . startswith ( ) : \n 
~~ scheme , host , port = get_host ( url ) \n 
if self . port and not port : \n 
~~~ port = port_by_scheme . get ( scheme ) \n 
~~ return ( scheme , host , port ) == ( self . scheme , self . host , self . port ) \n 
~~ def urlopen ( self , method , url , body = None , headers = None , retries = 3 , \n 
redirect = True , assert_same_host = True , timeout = _Default , \n 
pool_timeout = None , release_conn = None , ** response_kw ) : \n 
if headers is None : \n 
~~~ headers = self . headers \n 
~~ if retries < 0 : \n 
~~~ raise MaxRetryError ( self , url ) \n 
~~ if release_conn is None : \n 
~~~ release_conn = response_kw . get ( , True ) \n 
~~ if assert_same_host and not self . is_same_host ( url ) : \n 
~~~ raise HostChangedError ( self , url , retries - 1 ) \n 
~~ conn = None \n 
if self . scheme == : \n 
~~~ headers = headers . copy ( ) \n 
headers . update ( self . proxy_headers ) \n 
~~~ conn = self . _get_conn ( timeout = pool_timeout ) \n 
httplib_response = self . _make_request ( conn , method , url , \n 
timeout = timeout , \n 
body = body , headers = headers ) \n 
response_conn = not release_conn and conn \n 
response = HTTPResponse . from_httplib ( httplib_response , \n 
pool = self , \n 
connection = response_conn , \n 
** response_kw ) \n 
~~~ raise SSLError ( e ) \n 
~~ except CertificateError as e : \n 
~~ except TimeoutError as e : \n 
~~~ conn = None \n 
err = e \n 
if retries == 0 : \n 
~~ ~~ except ( HTTPException , SocketError ) as e : \n 
~~~ if isinstance ( e , SocketError ) and self . proxy is not None : \n 
~~~ raise ProxyError ( \n 
% e ) \n 
~~~ raise MaxRetryError ( self , url , e ) \n 
~~ ~~ finally : \n 
~~~ if release_conn : \n 
~~~ self . _put_conn ( conn ) \n 
~~ ~~ if not conn : \n 
return self . urlopen ( method , url , body , headers , retries - 1 , \n 
redirect , assert_same_host , \n 
timeout = timeout , pool_timeout = pool_timeout , \n 
release_conn = release_conn , ** response_kw ) \n 
~~ redirect_location = redirect and response . get_redirect_location ( ) \n 
if redirect_location : \n 
~~~ if response . status == 303 : \n 
~~~ method = \n 
return self . urlopen ( method , redirect_location , body , headers , \n 
retries - 1 , redirect , assert_same_host , \n 
~~ ~~ class HTTPSConnectionPool ( HTTPConnectionPool ) : \n 
ConnectionCls = HTTPSConnection \n 
def __init__ ( self , host , port = None , \n 
strict = False , timeout = None , maxsize = 1 , \n 
block = False , headers = None , \n 
_proxy = None , _proxy_headers = None , \n 
key_file = None , cert_file = None , cert_reqs = None , \n 
ca_certs = None , ssl_version = None , \n 
assert_hostname = None , assert_fingerprint = None ) : \n 
~~~ HTTPConnectionPool . __init__ ( self , host , port , strict , timeout , maxsize , \n 
block , headers , _proxy , _proxy_headers ) \n 
self . key_file = key_file \n 
self . cert_file = cert_file \n 
self . cert_reqs = cert_reqs \n 
self . ca_certs = ca_certs \n 
self . ssl_version = ssl_version \n 
self . assert_hostname = assert_hostname \n 
self . assert_fingerprint = assert_fingerprint \n 
~~ def _prepare_conn ( self , conn ) : \n 
if isinstance ( conn , VerifiedHTTPSConnection ) : \n 
~~~ conn . set_cert ( key_file = self . key_file , \n 
cert_file = self . cert_file , \n 
cert_reqs = self . cert_reqs , \n 
ca_certs = self . ca_certs , \n 
assert_hostname = self . assert_hostname , \n 
assert_fingerprint = self . assert_fingerprint ) \n 
conn . ssl_version = self . ssl_version \n 
~~ if self . proxy is not None : \n 
~~~ set_tunnel = conn . set_tunnel \n 
~~~ set_tunnel = conn . _set_tunnel \n 
~~ set_tunnel ( self . host , self . port , self . proxy_headers ) \n 
conn . connect ( ) \n 
~~ return conn \n 
% ( self . num_connections , self . host ) ) \n 
if not self . ConnectionCls or self . ConnectionCls is DummyConnection : \n 
~~ actual_host = self . host \n 
actual_port = self . port \n 
if self . proxy is not None : \n 
~~~ actual_host = self . proxy . host \n 
actual_port = self . proxy . port \n 
~~ extra_params = { } \n 
~~ conn = self . ConnectionCls ( host = actual_host , port = actual_port , \n 
return self . _prepare_conn ( conn ) \n 
~~ ~~ def connection_from_url ( url , ** kw ) : \n 
scheme , host , port = get_host ( url ) \n 
if scheme == : \n 
~~~ return HTTPSConnectionPool ( host , port = port , ** kw ) \n 
~~~ return HTTPConnectionPool ( host , port = port , ** kw ) \n 
import platform \n 
from subprocess import Popen , STDOUT \n 
from selenium . common . exceptions import WebDriverException \n 
from selenium . webdriver . common import utils \n 
class FirefoxBinary ( object ) : \n 
~~~ NO_FOCUS_LIBRARY_NAME = "x_ignore_nofocus.so" \n 
def __init__ ( self , firefox_path = None , log_file = None ) : \n 
self . _start_cmd = firefox_path \n 
self . _log_file = log_file or open ( os . devnull , "wb" ) \n 
self . command_line = None \n 
if self . _start_cmd is None : \n 
~~~ self . _start_cmd = self . _get_firefox_start_cmd ( ) \n 
~~ if not self . _start_cmd . strip ( ) : \n 
~~ self . _firefox_env = os . environ . copy ( ) \n 
self . _firefox_env [ "MOZ_CRASHREPORTER_DISABLE" ] = "1" \n 
self . _firefox_env [ "MOZ_NO_REMOTE" ] = "1" \n 
self . _firefox_env [ "NO_EM_RESTART" ] = "1" \n 
~~ def add_command_line_options ( self , * args ) : \n 
~~~ self . command_line = args \n 
~~ def launch_browser ( self , profile ) : \n 
self . profile = profile \n 
self . _start_from_profile_path ( self . profile . path ) \n 
self . _wait_until_connectable ( ) \n 
~~ def kill ( self ) : \n 
if self . process : \n 
~~~ self . process . kill ( ) \n 
self . process . wait ( ) \n 
~~ ~~ def _start_from_profile_path ( self , path ) : \n 
~~~ self . _firefox_env [ "XRE_PROFILE_PATH" ] = path \n 
if platform . system ( ) . lower ( ) == : \n 
~~~ self . _modify_link_library_path ( ) \n 
~~ command = [ self . _start_cmd , "-silent" ] \n 
if self . command_line is not None : \n 
~~~ for cli in self . command_line : \n 
~~~ command . append ( cli ) \n 
~~ ~~ Popen ( command , stdout = self . _log_file , stderr = STDOUT , \n 
env = self . _firefox_env ) . communicate ( ) \n 
command [ 1 ] = \n 
self . process = Popen ( \n 
command , stdout = self . _log_file , stderr = STDOUT , \n 
env = self . _firefox_env ) \n 
~~ def _wait_until_connectable ( self ) : \n 
while not utils . is_connectable ( self . profile . port ) : \n 
~~~ if self . process . poll ( ) is not None : \n 
~~ if count == 30 : \n 
~~~ self . kill ( ) \n 
~~ count += 1 \n 
~~ def _find_exe_in_registry ( self ) : \n 
~~~ from _winreg import OpenKey , QueryValue , HKEY_LOCAL_MACHINE , HKEY_CURRENT_USER \n 
~~~ from winreg import OpenKey , QueryValue , HKEY_LOCAL_MACHINE , HKEY_CURRENT_USER \n 
~~ import shlex \n 
keys = ( \n 
r"SOFTWARE\\Classes\\FirefoxHTML\\shell\\open\\command" , \n 
r"SOFTWARE\\Classes\\Applications\\firefox.exe\\shell\\open\\command" \n 
command = "" \n 
for path in keys : \n 
~~~ key = OpenKey ( HKEY_LOCAL_MACHINE , path ) \n 
command = QueryValue ( key , "" ) \n 
~~~ key = OpenKey ( HKEY_CURRENT_USER , path ) \n 
~~~ return "" \n 
~~ if not command : \n 
~~ return shlex . split ( command ) [ 0 ] \n 
~~ def _get_firefox_start_cmd ( self ) : \n 
start_cmd = "" \n 
if platform . system ( ) == "Darwin" : \n 
~~~ start_cmd = ( "/Applications/Firefox.app/Contents/MacOS/firefox-bin" ) \n 
~~ elif platform . system ( ) == "Windows" : \n 
~~~ start_cmd = ( self . _find_exe_in_registry ( ) or \n 
self . _default_windows_location ( ) ) \n 
~~ elif platform . system ( ) == and os . _name == : \n 
~~~ start_cmd = self . _default_windows_location ( ) \n 
~~~ for ffname in [ "firefox" , "iceweasel" ] : \n 
~~~ start_cmd = self . which ( ffname ) \n 
if start_cmd is not None : \n 
~~ ~~ return start_cmd \n 
~~ def _default_windows_location ( self ) : \n 
for path in program_files : \n 
if os . access ( binary_path , os . X_OK ) : \n 
~~~ return binary_path \n 
~~ ~~ return "" \n 
~~ def _modify_link_library_path ( self ) : \n 
~~~ existing_ld_lib_path = os . environ . get ( , ) \n 
new_ld_lib_path = self . _extract_and_check ( \n 
self . profile , self . NO_FOCUS_LIBRARY_NAME , "x86" , "amd64" ) \n 
new_ld_lib_path += existing_ld_lib_path \n 
self . _firefox_env [ "LD_LIBRARY_PATH" ] = new_ld_lib_path \n 
self . _firefox_env [ ] = self . NO_FOCUS_LIBRARY_NAME \n 
~~ def _extract_and_check ( self , profile , no_focus_so_name , x86 , amd64 ) : \n 
~~~ paths = [ x86 , amd64 ] \n 
built_path = "" \n 
~~~ library_path = os . path . join ( profile . path , path ) \n 
os . makedirs ( library_path ) \n 
shutil . copy ( os . path . join ( os . path . dirname ( __file__ ) , path , \n 
self . NO_FOCUS_LIBRARY_NAME ) , \n 
library_path ) \n 
built_path += library_path + ":" \n 
~~ return built_path \n 
~~ def which ( self , fname ) : \n 
for pe in os . environ [ ] . split ( os . pathsep ) : \n 
~~~ checkname = os . path . join ( pe , fname ) \n 
if os . access ( checkname , os . X_OK ) and not os . path . isdir ( checkname ) : \n 
~~~ return checkname \n 
~~ ~~ from selenium . common . exceptions import NoSuchElementException \n 
from selenium . common . exceptions import NoSuchFrameException \n 
from selenium . common . exceptions import StaleElementReferenceException \n 
from selenium . common . exceptions import NoAlertPresentException \n 
class title_is ( object ) : \n 
def __init__ ( self , title ) : \n 
~~~ self . title = title \n 
~~ def __call__ ( self , driver ) : \n 
~~~ return self . title == driver . title \n 
~~ ~~ class title_contains ( object ) : \n 
~~~ return self . title in driver . title \n 
~~ ~~ class presence_of_element_located ( object ) : \n 
def __init__ ( self , locator ) : \n 
~~~ self . locator = locator \n 
~~~ return _find_element ( driver , self . locator ) \n 
~~ ~~ class visibility_of_element_located ( object ) : \n 
~~~ return _element_if_visible ( _find_element ( driver , self . locator ) ) \n 
~~ except StaleElementReferenceException : \n 
~~ ~~ ~~ class visibility_of ( object ) : \n 
def __init__ ( self , element ) : \n 
~~~ self . element = element \n 
~~ def __call__ ( self , ignored ) : \n 
~~~ return _element_if_visible ( self . element ) \n 
~~ ~~ def _element_if_visible ( element ) : \n 
~~~ return element if element . is_displayed ( ) else False \n 
~~ class presence_of_all_elements_located ( object ) : \n 
~~~ return _find_elements ( driver , self . locator ) \n 
~~ ~~ class text_to_be_present_in_element ( object ) : \n 
def __init__ ( self , locator , text_ ) : \n 
self . text = text_ \n 
~~~ element_text = _find_element ( driver , self . locator ) . text \n 
return self . text in element_text \n 
~~ ~~ ~~ class text_to_be_present_in_element_value ( object ) : \n 
~~~ element_text = _find_element ( driver , \n 
self . locator ) . get_attribute ( "value" ) \n 
if element_text : \n 
~~~ return self . text in element_text \n 
~~ ~~ except StaleElementReferenceException : \n 
~~ ~~ ~~ class frame_to_be_available_and_switch_to_it ( object ) : \n 
~~~ self . frame_locator = locator \n 
~~~ if isinstance ( self . frame_locator , tuple ) : \n 
~~~ driver . switch_to . frame ( _find_element ( driver , \n 
self . frame_locator ) ) \n 
~~~ driver . switch_to . frame ( self . frame_locator ) \n 
~~ except NoSuchFrameException : \n 
~~ ~~ ~~ class invisibility_of_element_located ( object ) : \n 
~~~ return not _find_element ( driver , self . locator ) . is_displayed ( ) \n 
~~ except ( NoSuchElementException , StaleElementReferenceException ) : \n 
~~ ~~ ~~ class element_to_be_clickable ( object ) : \n 
~~~ element = visibility_of_element_located ( self . locator ) ( driver ) \n 
if element and element . is_enabled ( ) : \n 
~~~ return element \n 
~~ ~~ ~~ class staleness_of ( object ) : \n 
~~~ self . element . is_enabled ( ) \n 
~~ except StaleElementReferenceException as expected : \n 
~~ ~~ ~~ class element_to_be_selected ( object ) : \n 
~~~ return self . element . is_selected ( ) \n 
~~ ~~ class element_located_to_be_selected ( object ) : \n 
~~~ return _find_element ( driver , self . locator ) . is_selected ( ) \n 
~~ ~~ class element_selection_state_to_be ( object ) : \n 
def __init__ ( self , element , is_selected ) : \n 
self . is_selected = is_selected \n 
~~~ return self . element . is_selected ( ) == self . is_selected \n 
~~ ~~ class element_located_selection_state_to_be ( object ) : \n 
def __init__ ( self , locator , is_selected ) : \n 
~~~ element = _find_element ( driver , self . locator ) \n 
return element . is_selected ( ) == self . is_selected \n 
~~ ~~ ~~ class alert_is_present ( object ) : \n 
~~~ alert = driver . switch_to . alert \n 
alert . text \n 
return alert \n 
~~ except NoAlertPresentException : \n 
~~ ~~ ~~ def _find_element ( driver , by ) : \n 
~~~ return driver . find_element ( * by ) \n 
~~ except NoSuchElementException as e : \n 
~~~ raise e \n 
~~ except WebDriverException as e : \n 
~~ ~~ def _find_elements ( driver , by ) : \n 
~~~ return driver . find_elements ( * by ) \n 
~~ ~~ from . base import ischema_names \n 
from ... import types as sqltypes \n 
__all__ = ( , , ) \n 
class RangeOperators ( object ) : \n 
class comparator_factory ( sqltypes . Concatenable . Comparator ) : \n 
def __ne__ ( self , other ) : \n 
return self . expr . op ( ) ( other ) \n 
~~ def contains ( self , other , ** kw ) : \n 
~~ def contained_by ( self , other ) : \n 
~~ def overlaps ( self , other ) : \n 
~~ def strictly_left_of ( self , other ) : \n 
~~ __lshift__ = strictly_left_of \n 
def strictly_right_of ( self , other ) : \n 
~~ __rshift__ = strictly_right_of \n 
def not_extend_right_of ( self , other ) : \n 
~~ def not_extend_left_of ( self , other ) : \n 
~~ def adjacent_to ( self , other ) : \n 
~~ def __add__ ( self , other ) : \n 
~~ ~~ ~~ class INT4RANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
__visit_name__ = \n 
~~ ischema_names [ ] = INT4RANGE \n 
class INT8RANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = INT8RANGE \n 
class NUMRANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = NUMRANGE \n 
class DATERANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = DATERANGE \n 
class TSRANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = TSRANGE \n 
class TSTZRANGE ( RangeOperators , sqltypes . TypeEngine ) : \n 
~~ ischema_names [ ] = TSTZRANGE \n 
from . . import exc , util \n 
_key_to_collection = collections . defaultdict ( dict ) \n 
_collection_to_key = collections . defaultdict ( dict ) \n 
def _collection_gced ( ref ) : \n 
~~~ if not _collection_to_key or ref not in _collection_to_key : \n 
~~ listener_to_key = _collection_to_key . pop ( ref ) \n 
for key in listener_to_key . values ( ) : \n 
~~~ if key in _key_to_collection : \n 
~~~ dispatch_reg = _key_to_collection [ key ] \n 
dispatch_reg . pop ( ref ) \n 
if not dispatch_reg : \n 
~~~ _key_to_collection . pop ( key ) \n 
~~ ~~ ~~ ~~ def _stored_in_collection ( event_key , owner ) : \n 
~~~ key = event_key . _key \n 
dispatch_reg = _key_to_collection [ key ] \n 
owner_ref = owner . ref \n 
listen_ref = weakref . ref ( event_key . _listen_fn ) \n 
if owner_ref in dispatch_reg : \n 
~~ dispatch_reg [ owner_ref ] = listen_ref \n 
listener_to_key = _collection_to_key [ owner_ref ] \n 
listener_to_key [ listen_ref ] = key \n 
~~ def _removed_from_collection ( event_key , owner ) : \n 
dispatch_reg . pop ( owner_ref , None ) \n 
~~~ del _key_to_collection [ key ] \n 
~~ if owner_ref in _collection_to_key : \n 
~~~ listener_to_key = _collection_to_key [ owner_ref ] \n 
listener_to_key . pop ( listen_ref ) \n 
~~ ~~ def _stored_in_collection_multi ( newowner , oldowner , elements ) : \n 
~~~ if not elements : \n 
~~ oldowner = oldowner . ref \n 
newowner = newowner . ref \n 
old_listener_to_key = _collection_to_key [ oldowner ] \n 
new_listener_to_key = _collection_to_key [ newowner ] \n 
for listen_fn in elements : \n 
~~~ listen_ref = weakref . ref ( listen_fn ) \n 
key = old_listener_to_key [ listen_ref ] \n 
if newowner in dispatch_reg : \n 
~~~ assert dispatch_reg [ newowner ] == listen_ref \n 
~~~ dispatch_reg [ newowner ] = listen_ref \n 
~~ new_listener_to_key [ listen_ref ] = key \n 
~~ ~~ def _clear ( owner , elements ) : \n 
~~ owner = owner . ref \n 
listener_to_key = _collection_to_key [ owner ] \n 
key = listener_to_key [ listen_ref ] \n 
dispatch_reg . pop ( owner , None ) \n 
~~ ~~ ~~ class _EventKey ( object ) : \n 
def __init__ ( self , target , identifier , \n 
fn , dispatch_target , _fn_wrap = None ) : \n 
~~~ self . target = target \n 
self . identifier = identifier \n 
self . fn = fn \n 
if isinstance ( fn , types . MethodType ) : \n 
~~~ self . fn_key = id ( fn . __func__ ) , id ( fn . __self__ ) \n 
~~~ self . fn_key = id ( fn ) \n 
~~ self . fn_wrap = _fn_wrap \n 
self . dispatch_target = dispatch_target \n 
def _key ( self ) : \n 
~~~ return ( id ( self . target ) , self . identifier , self . fn_key ) \n 
~~ def with_wrapper ( self , fn_wrap ) : \n 
~~~ if fn_wrap is self . _listen_fn : \n 
~~~ return _EventKey ( \n 
self . target , \n 
self . identifier , \n 
self . fn , \n 
self . dispatch_target , \n 
_fn_wrap = fn_wrap \n 
~~ ~~ def with_dispatch_target ( self , dispatch_target ) : \n 
~~~ if dispatch_target is self . dispatch_target : \n 
dispatch_target , \n 
_fn_wrap = self . fn_wrap \n 
~~ ~~ def listen ( self , * args , ** kw ) : \n 
~~~ once = kw . pop ( "once" , False ) \n 
named = kw . pop ( "named" , False ) \n 
target , identifier , fn = self . dispatch_target , self . identifier , self . _listen_fn \n 
dispatch_descriptor = getattr ( target . dispatch , identifier ) \n 
adjusted_fn = dispatch_descriptor . _adjust_fn_spec ( fn , named ) \n 
self = self . with_wrapper ( adjusted_fn ) \n 
if once : \n 
~~~ self . with_wrapper ( \n 
util . only_once ( self . _listen_fn ) ) . listen ( * args , ** kw ) \n 
~~~ self . dispatch_target . dispatch . _listen ( self , * args , ** kw ) \n 
~~ ~~ def remove ( self ) : \n 
~~~ key = self . _key \n 
if key not in _key_to_collection : \n 
~~~ raise exc . InvalidRequestError ( \n 
( self . target , self . identifier , self . fn ) \n 
~~ dispatch_reg = _key_to_collection . pop ( key ) \n 
for collection_ref , listener_ref in dispatch_reg . items ( ) : \n 
~~~ collection = collection_ref ( ) \n 
listener_fn = listener_ref ( ) \n 
if collection is not None and listener_fn is not None : \n 
~~~ collection . remove ( self . with_wrapper ( listener_fn ) ) \n 
~~ ~~ ~~ def contains ( self ) : \n 
return self . _key in _key_to_collection \n 
~~ def base_listen ( self , propagate = False , insert = False , \n 
named = False ) : \n 
~~~ target , identifier , fn = self . dispatch_target , self . identifier , self . _listen_fn \n 
if insert : \n 
~~~ dispatch_descriptor . for_modify ( target . dispatch ) . insert ( self , propagate ) \n 
~~~ dispatch_descriptor . for_modify ( target . dispatch ) . append ( self , propagate ) \n 
def _listen_fn ( self ) : \n 
~~~ return self . fn_wrap or self . fn \n 
~~ def append_to_list ( self , owner , list_ ) : \n 
~~~ if _stored_in_collection ( self , owner ) : \n 
~~~ list_ . append ( self . _listen_fn ) \n 
~~ ~~ def remove_from_list ( self , owner , list_ ) : \n 
~~~ _removed_from_collection ( self , owner ) \n 
list_ . remove ( self . _listen_fn ) \n 
~~ def prepend_to_list ( self , owner , list_ ) : \n 
~~~ list_ . insert ( 0 , self . _listen_fn ) \n 
~~ ~~ ~~ import operator \n 
from . . sql import operators \n 
from . . import util \n 
class UnevaluatableError ( Exception ) : \n 
~~ _straight_ops = set ( getattr ( operators , op ) \n 
for op in ( , , , \n 
, , , , , ) ) \n 
_notimplemented_ops = set ( getattr ( operators , op ) \n 
, , ) ) \n 
class EvaluatorCompiler ( object ) : \n 
~~~ def __init__ ( self , target_cls = None ) : \n 
~~~ self . target_cls = target_cls \n 
~~ def process ( self , clause ) : \n 
~~~ meth = getattr ( self , "visit_%s" % clause . __visit_name__ , None ) \n 
if not meth : \n 
~~~ raise UnevaluatableError ( \n 
~~ return meth ( clause ) \n 
~~ def visit_grouping ( self , clause ) : \n 
~~~ return self . process ( clause . element ) \n 
~~ def visit_null ( self , clause ) : \n 
~~~ return lambda obj : None \n 
~~ def visit_false ( self , clause ) : \n 
~~~ return lambda obj : False \n 
~~ def visit_true ( self , clause ) : \n 
~~~ return lambda obj : True \n 
~~ def visit_column ( self , clause ) : \n 
~~~ if in clause . _annotations : \n 
~~~ parentmapper = clause . _annotations [ ] \n 
if self . target_cls and not issubclass ( \n 
self . target_cls , parentmapper . class_ ) : \n 
~~~ util . warn ( \n 
~~ key = parentmapper . _columntoproperty [ clause ] . key \n 
~~~ key = clause . key \n 
~~ get_corresponding_attr = operator . attrgetter ( key ) \n 
return lambda obj : get_corresponding_attr ( obj ) \n 
~~ def visit_clauselist ( self , clause ) : \n 
~~~ evaluators = list ( map ( self . process , clause . clauses ) ) \n 
if clause . operator is operators . or_ : \n 
~~~ def evaluate ( obj ) : \n 
~~~ has_null = False \n 
for sub_evaluate in evaluators : \n 
~~~ value = sub_evaluate ( obj ) \n 
if value : \n 
~~ has_null = has_null or value is None \n 
~~ if has_null : \n 
~~ ~~ elif clause . operator is operators . and_ : \n 
~~~ for sub_evaluate in evaluators : \n 
if not value : \n 
~~ ~~ return True \n 
clause . operator ) \n 
~~ return evaluate \n 
~~ def visit_binary ( self , clause ) : \n 
~~~ eval_left , eval_right = list ( map ( self . process , \n 
[ clause . left , clause . right ] ) ) \n 
operator = clause . operator \n 
if operator is operators . is_ : \n 
~~~ return eval_left ( obj ) == eval_right ( obj ) \n 
~~ ~~ elif operator is operators . isnot : \n 
~~~ return eval_left ( obj ) != eval_right ( obj ) \n 
~~ ~~ elif operator in _straight_ops : \n 
~~~ left_val = eval_left ( obj ) \n 
right_val = eval_right ( obj ) \n 
if left_val is None or right_val is None : \n 
~~ return operator ( eval_left ( obj ) , eval_right ( obj ) ) \n 
( type ( clause ) . __name__ , clause . operator ) ) \n 
~~ def visit_unary ( self , clause ) : \n 
~~~ eval_inner = self . process ( clause . element ) \n 
if clause . operator is operators . inv : \n 
~~~ value = eval_inner ( obj ) \n 
if value is None : \n 
~~ return not value \n 
~~ raise UnevaluatableError ( \n 
~~ def visit_bindparam ( self , clause ) : \n 
~~~ val = clause . value \n 
return lambda obj : val \n 
from . elements import ClauseElement \n 
from . visitors import traverse \n 
from . base import Executable , _generative , SchemaVisitor , _bind_or_error \n 
from . . util import topological \n 
from . . import event \n 
from . . import exc \n 
class _DDLCompiles ( ClauseElement ) : \n 
~~~ def _compiler ( self , dialect , ** kw ) : \n 
return dialect . ddl_compiler ( dialect , self , ** kw ) \n 
~~ ~~ class DDLElement ( Executable , _DDLCompiles ) : \n 
_execution_options = Executable . _execution_options . union ( { : True } ) \n 
target = None \n 
on = None \n 
dialect = None \n 
callable_ = None \n 
def _execute_on_connection ( self , connection , multiparams , params ) : \n 
~~~ return connection . _execute_ddl ( self , multiparams , params ) \n 
~~ def execute ( self , bind = None , target = None ) : \n 
if bind is None : \n 
~~~ bind = _bind_or_error ( self ) \n 
~~ if self . _should_execute ( target , bind ) : \n 
~~~ return bind . execute ( self . against ( target ) ) \n 
~~~ bind . engine . logger . info ( \n 
":meth:`.DDLElement.execute_if`." ) \n 
def execute_at ( self , event_name , target ) : \n 
def call_event ( target , connection , ** kw ) : \n 
~~~ if self . _should_execute_deprecated ( event_name , \n 
target , connection , ** kw ) : \n 
~~~ return connection . execute ( self . against ( target ) ) \n 
~~ ~~ event . listen ( target , "" + event_name . replace ( , ) , call_event ) \n 
~~ @ _generative \n 
def against ( self , target ) : \n 
self . target = target \n 
def execute_if ( self , dialect = None , callable_ = None , state = None ) : \n 
self . dialect = dialect \n 
self . callable_ = callable_ \n 
self . state = state \n 
~~ def _should_execute ( self , target , bind , ** kw ) : \n 
~~~ if self . on is not None and not self . _should_execute_deprecated ( None , target , bind , ** kw ) : \n 
~~ if isinstance ( self . dialect , util . string_types ) : \n 
~~~ if self . dialect != bind . engine . name : \n 
~~ ~~ elif isinstance ( self . dialect , ( tuple , list , set ) ) : \n 
~~~ if bind . engine . name not in self . dialect : \n 
~~ ~~ if ( self . callable_ is not None and \n 
not self . callable_ ( self , target , bind , \n 
state = self . state , ** kw ) ) : \n 
~~ def _should_execute_deprecated ( self , event , target , bind , ** kw ) : \n 
~~~ if self . on is None : \n 
~~ elif isinstance ( self . on , util . string_types ) : \n 
~~~ return self . on == bind . engine . name \n 
~~ elif isinstance ( self . on , ( tuple , list , set ) ) : \n 
~~~ return bind . engine . name in self . on \n 
~~~ return self . on ( self , event , target , bind , ** kw ) \n 
~~ ~~ def __call__ ( self , target , bind , ** kw ) : \n 
if self . _should_execute ( target , bind , ** kw ) : \n 
~~ ~~ def _check_ddl_on ( self , on ) : \n 
~~~ if ( on is not None and \n 
( not isinstance ( on , util . string_types + ( tuple , list , set ) ) and \n 
not util . callable ( on ) ) ) : \n 
~~~ raise exc . ArgumentError ( \n 
~~ ~~ def bind ( self ) : \n 
~~~ if self . _bind : \n 
~~~ return self . _bind \n 
~~ ~~ def _set_bind ( self , bind ) : \n 
~~~ self . _bind = bind \n 
~~ bind = property ( bind , _set_bind ) \n 
def _generate ( self ) : \n 
~~~ s = self . __class__ . __new__ ( self . __class__ ) \n 
s . __dict__ = self . __dict__ . copy ( ) \n 
~~ ~~ class DDL ( DDLElement ) : \n 
__visit_name__ = "ddl" \n 
def __init__ ( self , statement , on = None , context = None , bind = None ) : \n 
if not isinstance ( statement , util . string_types ) : \n 
statement ) \n 
~~ self . statement = statement \n 
self . context = context or { } \n 
self . _check_ddl_on ( on ) \n 
self . on = on \n 
self . _bind = bind \n 
~~~ return % ( \n 
type ( self ) . __name__ , id ( self ) , \n 
. join ( [ repr ( self . statement ) ] + \n 
[ % ( key , getattr ( self , key ) ) \n 
for key in ( , ) \n 
if getattr ( self , key ) ] ) ) \n 
~~ ~~ class _CreateDropBase ( DDLElement ) : \n 
def __init__ ( self , element , on = None , bind = None ) : \n 
self . bind = bind \n 
~~ def _create_rule_disable ( self , compiler ) : \n 
~~ ~~ class CreateSchema ( _CreateDropBase ) : \n 
__visit_name__ = "create_schema" \n 
def __init__ ( self , name , quote = None , ** kw ) : \n 
self . quote = quote \n 
super ( CreateSchema , self ) . __init__ ( name , ** kw ) \n 
~~ ~~ class DropSchema ( _CreateDropBase ) : \n 
__visit_name__ = "drop_schema" \n 
def __init__ ( self , name , quote = None , cascade = False , ** kw ) : \n 
self . cascade = cascade \n 
super ( DropSchema , self ) . __init__ ( name , ** kw ) \n 
~~ ~~ class CreateTable ( _CreateDropBase ) : \n 
__visit_name__ = "create_table" \n 
super ( CreateTable , self ) . __init__ ( element , on = on , bind = bind ) \n 
self . columns = [ CreateColumn ( column ) \n 
for column in element . columns \n 
~~ ~~ class _DropView ( _CreateDropBase ) : \n 
__visit_name__ = "drop_view" \n 
~~ class CreateColumn ( _DDLCompiles ) : \n 
~~ ~~ class DropTable ( _CreateDropBase ) : \n 
__visit_name__ = "drop_table" \n 
~~ class CreateSequence ( _CreateDropBase ) : \n 
__visit_name__ = "create_sequence" \n 
~~ class DropSequence ( _CreateDropBase ) : \n 
__visit_name__ = "drop_sequence" \n 
~~ class CreateIndex ( _CreateDropBase ) : \n 
__visit_name__ = "create_index" \n 
~~ class DropIndex ( _CreateDropBase ) : \n 
__visit_name__ = "drop_index" \n 
~~ class AddConstraint ( _CreateDropBase ) : \n 
__visit_name__ = "add_constraint" \n 
def __init__ ( self , element , * args , ** kw ) : \n 
~~~ super ( AddConstraint , self ) . __init__ ( element , * args , ** kw ) \n 
element . _create_rule = util . portable_instancemethod ( \n 
self . _create_rule_disable ) \n 
~~ ~~ class DropConstraint ( _CreateDropBase ) : \n 
__visit_name__ = "drop_constraint" \n 
def __init__ ( self , element , cascade = False , ** kw ) : \n 
~~~ self . cascade = cascade \n 
super ( DropConstraint , self ) . __init__ ( element , ** kw ) \n 
~~ ~~ class DDLBase ( SchemaVisitor ) : \n 
~~~ def __init__ ( self , connection ) : \n 
~~~ self . connection = connection \n 
~~ ~~ class SchemaGenerator ( DDLBase ) : \n 
~~~ def __init__ ( self , dialect , connection , checkfirst = False , \n 
tables = None , ** kwargs ) : \n 
~~~ super ( SchemaGenerator , self ) . __init__ ( connection , ** kwargs ) \n 
self . checkfirst = checkfirst \n 
self . tables = tables \n 
self . preparer = dialect . identifier_preparer \n 
self . memo = { } \n 
~~ def _can_create_table ( self , table ) : \n 
~~~ self . dialect . validate_identifier ( table . name ) \n 
if table . schema : \n 
~~~ self . dialect . validate_identifier ( table . schema ) \n 
~~ return not self . checkfirst or not self . dialect . has_table ( self . connection , \n 
table . name , schema = table . schema ) \n 
~~ def _can_create_sequence ( self , sequence ) : \n 
~~~ return self . dialect . supports_sequences and ( \n 
( not self . dialect . sequences_optional or \n 
not sequence . optional ) and \n 
( \n 
not self . checkfirst or \n 
not self . dialect . has_sequence ( \n 
self . connection , \n 
sequence . name , \n 
schema = sequence . schema ) \n 
~~ def visit_metadata ( self , metadata ) : \n 
~~~ if self . tables is not None : \n 
~~~ tables = self . tables \n 
~~~ tables = list ( metadata . tables . values ( ) ) \n 
~~ collection = [ t for t in sort_tables ( tables ) \n 
if self . _can_create_table ( t ) ] \n 
seq_coll = [ s for s in metadata . _sequences . values ( ) \n 
if s . column is None and self . _can_create_sequence ( s ) ] \n 
metadata . dispatch . before_create ( metadata , self . connection , \n 
tables = collection , \n 
checkfirst = self . checkfirst , \n 
_ddl_runner = self ) \n 
for seq in seq_coll : \n 
~~~ self . traverse_single ( seq , create_ok = True ) \n 
~~ for table in collection : \n 
~~~ self . traverse_single ( table , create_ok = True ) \n 
~~ metadata . dispatch . after_create ( metadata , self . connection , \n 
~~ def visit_table ( self , table , create_ok = False ) : \n 
~~~ if not create_ok and not self . _can_create_table ( table ) : \n 
~~ table . dispatch . before_create ( table , self . connection , \n 
for column in table . columns : \n 
~~~ if column . default is not None : \n 
~~~ self . traverse_single ( column . default ) \n 
~~ ~~ self . connection . execute ( CreateTable ( table ) ) \n 
if hasattr ( table , ) : \n 
~~~ for index in table . indexes : \n 
~~~ self . traverse_single ( index ) \n 
~~ ~~ table . dispatch . after_create ( table , self . connection , \n 
~~ def visit_sequence ( self , sequence , create_ok = False ) : \n 
~~~ if not create_ok and not self . _can_create_sequence ( sequence ) : \n 
~~ self . connection . execute ( CreateSequence ( sequence ) ) \n 
~~ def visit_index ( self , index ) : \n 
~~~ self . connection . execute ( CreateIndex ( index ) ) \n 
~~ ~~ class SchemaDropper ( DDLBase ) : \n 
~~~ super ( SchemaDropper , self ) . __init__ ( connection , ** kwargs ) \n 
~~ collection = [ \n 
t \n 
for t in reversed ( sort_tables ( tables ) ) \n 
if self . _can_drop_table ( t ) \n 
seq_coll = [ \n 
s \n 
for s in metadata . _sequences . values ( ) \n 
if s . column is None and self . _can_drop_sequence ( s ) \n 
metadata . dispatch . before_drop ( \n 
metadata , self . connection , tables = collection , \n 
checkfirst = self . checkfirst , _ddl_runner = self ) \n 
for table in collection : \n 
~~~ self . traverse_single ( table , drop_ok = True ) \n 
~~ for seq in seq_coll : \n 
~~~ self . traverse_single ( seq , drop_ok = True ) \n 
~~ metadata . dispatch . after_drop ( \n 
~~ def _can_drop_table ( self , table ) : \n 
~~ return not self . checkfirst or self . dialect . has_table ( \n 
self . connection , table . name , schema = table . schema ) \n 
~~ def _can_drop_sequence ( self , sequence ) : \n 
~~~ return self . dialect . supports_sequences and ( ( not self . dialect . sequences_optional or \n 
( not self . checkfirst or \n 
self . dialect . has_sequence ( \n 
schema = sequence . schema ) ) \n 
~~~ self . connection . execute ( DropIndex ( index ) ) \n 
~~ def visit_table ( self , table , drop_ok = False ) : \n 
~~~ if not drop_ok and not self . _can_drop_table ( table ) : \n 
~~ table . dispatch . before_drop ( table , self . connection , \n 
~~ ~~ self . connection . execute ( DropTable ( table ) ) \n 
table . dispatch . after_drop ( table , self . connection , \n 
~~ def visit_sequence ( self , sequence , drop_ok = False ) : \n 
~~~ if not drop_ok and not self . _can_drop_sequence ( sequence ) : \n 
~~ self . connection . execute ( DropSequence ( sequence ) ) \n 
~~ ~~ def sort_tables ( tables , skip_fn = None , extra_dependencies = None ) : \n 
tables = list ( tables ) \n 
tuples = [ ] \n 
if extra_dependencies is not None : \n 
~~~ tuples . extend ( extra_dependencies ) \n 
~~ def visit_foreign_key ( fkey ) : \n 
~~~ if fkey . use_alter : \n 
~~ elif skip_fn and skip_fn ( fkey ) : \n 
~~ parent_table = fkey . column . table \n 
if parent_table in tables : \n 
~~~ child_table = fkey . parent . table \n 
if parent_table is not child_table : \n 
~~~ tuples . append ( ( parent_table , child_table ) ) \n 
~~ ~~ ~~ for table in tables : \n 
~~~ traverse ( table , \n 
{ : True } , \n 
{ : visit_foreign_key } ) \n 
tuples . extend ( \n 
[ parent , table ] for parent in table . _extra_dependencies \n 
~~ return list ( topological . sort ( tuples , tables ) ) \n 
~~ from __future__ import unicode_literals \n 
import warnings \n 
from sqlalchemy . orm . exc import NoResultFound \n 
class Unique ( object ) : \n 
field_flags = ( , ) \n 
def __init__ ( self , get_session , model , column , message = None ) : \n 
~~~ warnings . warn ( , DeprecationWarning ) \n 
self . get_session = get_session \n 
self . model = model \n 
self . column = column \n 
self . message = message \n 
~~ def __call__ ( self , form , field ) : \n 
~~~ obj = self . get_session ( ) . query ( self . model ) . filter ( self . column == field . data ) . one ( ) \n 
if not hasattr ( form , ) or not form . _obj == obj : \n 
~~~ if self . message is None : \n 
~~~ self . message = field . gettext ( ) \n 
~~ raise ValidationError ( self . message ) \n 
~~ ~~ except NoResultFound : \n 
~~ ~~ ~~ from pygments . lexers . web import HtmlLexer , XmlLexer , JavascriptLexer , CssLexer \n 
from pygments . lexers . agile import PythonLexer , Python3Lexer \n 
from pygments . lexer import DelegatingLexer , RegexLexer , bygroups , include , using \n 
from pygments . token import Text , Comment , Operator , Keyword , Name , String , Other \n 
from pygments . formatters . html import HtmlFormatter \n 
from pygments import highlight \n 
from mako import compat \n 
class MakoLexer ( RegexLexer ) : \n 
~~~ name = \n 
aliases = [ ] \n 
filenames = [ ] \n 
tokens = { \n 
( , \n 
bygroups ( Text , Comment . Preproc , Keyword , Other ) ) , \n 
bygroups ( Text , Comment . Preproc , using ( PythonLexer ) , Other ) ) , \n 
bygroups ( Text , Comment . Preproc , Other ) ) , \n 
( , Comment . Preproc ) , \n 
bygroups ( Comment . Preproc , Name . Builtin ) , ) , \n 
bygroups ( Comment . Preproc , Name . Builtin , Comment . Preproc ) ) , \n 
( , Comment . Preproc , ) , \n 
bygroups ( Comment . Preproc , using ( PythonLexer ) , Comment . Preproc ) ) , \n 
( , bygroups ( Other , Operator ) ) , \n 
( , Text ) , \n 
( , Name . Builtin ) , \n 
include ( ) , \n 
( r\'((?:\\w+)\\s*=)\\s*(".*?")\' , \n 
bygroups ( Name . Attribute , String ) ) , \n 
( \'".*?"\' , String , ) , \n 
( "\'.*?\'" , String , ) , \n 
( , String , ) , \n 
~~ class MakoHtmlLexer ( DelegatingLexer ) : \n 
def __init__ ( self , ** options ) : \n 
~~~ super ( MakoHtmlLexer , self ) . __init__ ( HtmlLexer , MakoLexer , \n 
** options ) \n 
~~ ~~ class MakoXmlLexer ( DelegatingLexer ) : \n 
~~~ super ( MakoXmlLexer , self ) . __init__ ( XmlLexer , MakoLexer , \n 
~~ ~~ class MakoJavascriptLexer ( DelegatingLexer ) : \n 
aliases = [ , ] \n 
~~~ super ( MakoJavascriptLexer , self ) . __init__ ( JavascriptLexer , \n 
MakoLexer , ** options ) \n 
~~ ~~ class MakoCssLexer ( DelegatingLexer ) : \n 
~~~ super ( MakoCssLexer , self ) . __init__ ( CssLexer , MakoLexer , \n 
~~ ~~ pygments_html_formatter = HtmlFormatter ( cssclass = , \n 
linenos = True ) \n 
def syntax_highlight ( filename = , language = None ) : \n 
~~~ mako_lexer = MakoLexer ( ) \n 
if compat . py3k : \n 
~~~ python_lexer = Python3Lexer ( ) \n 
~~~ python_lexer = PythonLexer ( ) \n 
~~ if filename . startswith ( ) or language == : \n 
~~~ return lambda string : highlight ( string , mako_lexer , \n 
pygments_html_formatter ) \n 
~~ return lambda string : highlight ( string , python_lexer , \n 
~~ from ... import Table , MetaData , Column \n 
from ... types import String , Unicode , UnicodeText , Integer , TypeDecorator \n 
from ... import cast \n 
from ... import util \n 
from ... sql import expression \n 
from ... ext . compiler import compiles \n 
ischema = MetaData ( ) \n 
class CoerceUnicode ( TypeDecorator ) : \n 
~~~ impl = Unicode \n 
def process_bind_param ( self , value , dialect ) : \n 
~~~ if util . py2k and isinstance ( value , util . binary_type ) : \n 
~~~ value = value . decode ( dialect . encoding ) \n 
~~ return value \n 
~~ def bind_expression ( self , bindvalue ) : \n 
~~~ return _cast_on_2005 ( bindvalue ) \n 
~~ ~~ class _cast_on_2005 ( expression . ColumnElement ) : \n 
~~~ def __init__ ( self , bindvalue ) : \n 
~~~ self . bindvalue = bindvalue \n 
~~ ~~ @ compiles ( _cast_on_2005 ) \n 
def _compile ( element , compiler , ** kw ) : \n 
~~~ from . import base \n 
if compiler . dialect . server_version_info < base . MS_2005_VERSION : \n 
~~~ return compiler . process ( element . bindvalue , ** kw ) \n 
~~~ return compiler . process ( cast ( element . bindvalue , Unicode ) , ** kw ) \n 
~~ ~~ schemata = Table ( "SCHEMATA" , ischema , \n 
Column ( "CATALOG_NAME" , CoerceUnicode , key = "catalog_name" ) , \n 
Column ( "SCHEMA_NAME" , CoerceUnicode , key = "schema_name" ) , \n 
Column ( "SCHEMA_OWNER" , CoerceUnicode , key = "schema_owner" ) , \n 
schema = "INFORMATION_SCHEMA" ) \n 
tables = Table ( "TABLES" , ischema , \n 
Column ( "TABLE_CATALOG" , CoerceUnicode , key = "table_catalog" ) , \n 
Column ( "TABLE_SCHEMA" , CoerceUnicode , key = "table_schema" ) , \n 
Column ( "TABLE_NAME" , CoerceUnicode , key = "table_name" ) , \n 
Column ( "TABLE_TYPE" , String ( convert_unicode = True ) , key = "table_type" ) , \n 
columns = Table ( "COLUMNS" , ischema , \n 
Column ( "COLUMN_NAME" , CoerceUnicode , key = "column_name" ) , \n 
Column ( "IS_NULLABLE" , Integer , key = "is_nullable" ) , \n 
Column ( "DATA_TYPE" , String , key = "data_type" ) , \n 
Column ( "ORDINAL_POSITION" , Integer , key = "ordinal_position" ) , \n 
Column ( "CHARACTER_MAXIMUM_LENGTH" , Integer , key = "character_maximum_length" ) , \n 
Column ( "NUMERIC_PRECISION" , Integer , key = "numeric_precision" ) , \n 
Column ( "NUMERIC_SCALE" , Integer , key = "numeric_scale" ) , \n 
Column ( "COLUMN_DEFAULT" , Integer , key = "column_default" ) , \n 
Column ( "COLLATION_NAME" , String , key = "collation_name" ) , \n 
constraints = Table ( "TABLE_CONSTRAINTS" , ischema , \n 
Column ( "CONSTRAINT_NAME" , CoerceUnicode , key = "constraint_name" ) , \n 
Column ( "CONSTRAINT_TYPE" , String ( convert_unicode = True ) , key = "constraint_type" ) , \n 
column_constraints = Table ( "CONSTRAINT_COLUMN_USAGE" , ischema , \n 
key_constraints = Table ( "KEY_COLUMN_USAGE" , ischema , \n 
ref_constraints = Table ( "REFERENTIAL_CONSTRAINTS" , ischema , \n 
Column ( "CONSTRAINT_CATALOG" , CoerceUnicode , key = "constraint_catalog" ) , \n 
Column ( "CONSTRAINT_SCHEMA" , CoerceUnicode , key = "constraint_schema" ) , \n 
Column ( "UNIQUE_CONSTRAINT_CATLOG" , CoerceUnicode , \n 
key = "unique_constraint_catalog" ) , \n 
Column ( "UNIQUE_CONSTRAINT_SCHEMA" , CoerceUnicode , \n 
key = "unique_constraint_schema" ) , \n 
Column ( "UNIQUE_CONSTRAINT_NAME" , CoerceUnicode , \n 
key = "unique_constraint_name" ) , \n 
Column ( "MATCH_OPTION" , String , key = "match_option" ) , \n 
Column ( "UPDATE_RULE" , String , key = "update_rule" ) , \n 
Column ( "DELETE_RULE" , String , key = "delete_rule" ) , \n 
views = Table ( "VIEWS" , ischema , \n 
Column ( "VIEW_DEFINITION" , CoerceUnicode , key = "view_definition" ) , \n 
Column ( "CHECK_OPTION" , String , key = "check_option" ) , \n 
Column ( "IS_UPDATABLE" , String , key = "is_updatable" ) , \n 
from . import fixtures \n 
class User ( fixtures . ComparableEntity ) : \n 
~~ class Order ( fixtures . ComparableEntity ) : \n 
~~ class Dingaling ( fixtures . ComparableEntity ) : \n 
~~ class EmailUser ( User ) : \n 
~~ class Address ( fixtures . ComparableEntity ) : \n 
~~ class Child1 ( fixtures . ComparableEntity ) : \n 
~~ class Child2 ( fixtures . ComparableEntity ) : \n 
~~ class Parent ( fixtures . ComparableEntity ) : \n 
~~ class Screen ( object ) : \n 
~~~ def __init__ ( self , obj , parent = None ) : \n 
~~~ self . obj = obj \n 
self . parent = parent \n 
~~ ~~ class Foo ( object ) : \n 
~~~ def __init__ ( self , moredata ) : \n 
~~~ self . data = \n 
self . stuff = \n 
self . moredata = moredata \n 
~~ __hash__ = object . __hash__ \n 
def __eq__ ( self , other ) : \n 
~~~ return other . data == self . data and other . stuff == self . stuff and other . moredata == self . moredata \n 
~~ ~~ class Bar ( object ) : \n 
~~~ def __init__ ( self , x , y ) : \n 
~~~ self . x = x \n 
self . y = y \n 
~~~ return other . __class__ is self . __class__ and other . x == self . x and other . y == self . y \n 
~~ ~~ class OldSchool : \n 
~~ ~~ class OldSchoolWithoutCompare : \n 
~~ ~~ class BarWithoutCompare ( object ) : \n 
~~ ~~ class NotComparable ( object ) : \n 
~~~ self . data = data \n 
~~ def __hash__ ( self ) : \n 
~~~ return id ( self ) \n 
~~~ return NotImplemented \n 
~~ ~~ class BrokenComparable ( object ) : \n 
from . . exc import CircularDependencyError \n 
__all__ = [ , , ] \n 
def sort_as_subsets ( tuples , allitems ) : \n 
~~~ edges = util . defaultdict ( set ) \n 
for parent , child in tuples : \n 
~~~ edges [ child ] . add ( parent ) \n 
~~ todo = set ( allitems ) \n 
while todo : \n 
~~~ output = set ( ) \n 
for node in list ( todo ) : \n 
~~~ if not todo . intersection ( edges [ node ] ) : \n 
~~~ output . add ( node ) \n 
~~ ~~ if not output : \n 
~~~ raise CircularDependencyError ( \n 
find_cycles ( tuples , allitems ) , \n 
_gen_edges ( edges ) \n 
~~ todo . difference_update ( output ) \n 
yield output \n 
~~ ~~ def sort ( tuples , allitems ) : \n 
for set_ in sort_as_subsets ( tuples , allitems ) : \n 
~~~ for s in set_ : \n 
~~~ yield s \n 
~~ ~~ ~~ def find_cycles ( tuples , allitems ) : \n 
~~~ edges [ parent ] . add ( child ) \n 
~~ nodes_to_test = set ( edges ) \n 
output = set ( ) \n 
for node in nodes_to_test : \n 
~~~ stack = [ node ] \n 
todo = nodes_to_test . difference ( stack ) \n 
while stack : \n 
~~~ top = stack [ - 1 ] \n 
for node in edges [ top ] : \n 
~~~ if node in stack : \n 
~~~ cyc = stack [ stack . index ( node ) : ] \n 
todo . difference_update ( cyc ) \n 
output . update ( cyc ) \n 
~~ if node in todo : \n 
~~~ stack . append ( node ) \n 
todo . remove ( node ) \n 
~~~ node = stack . pop ( ) \n 
~~ ~~ ~~ return output \n 
~~ def _gen_edges ( edges ) : \n 
~~~ return set ( [ \n 
( right , left ) \n 
for left in edges \n 
for right in edges [ left ] \n 
from kombu import Exchange , Queue \n 
DEBUG = True \n 
TEMPLATE_DEBUG = DEBUG \n 
SESSION_COOKIE_SECURE = False \n 
AWS_ACCESS_KEY_ID = os . environ . get ( , ) \n 
AWS_UPLOAD_CLIENT_KEY = AWS_ACCESS_KEY_ID \n 
AWS_SECRET_ACCESS_KEY = os . environ . get ( , ) \n 
AWS_UPLOAD_CLIENT_SECRET_KEY = AWS_SECRET_ACCESS_KEY \n 
AWS_BUCKET_NAME = os . environ . get ( "AWS_BUCKET_NAME" , "be-dev-uploads" ) \n 
AWS_STORAGE_BUCKET_NAME = AWS_BUCKET_NAME \n 
DATABASES = { \n 
: "127.0.0.1" , \n 
if "test" in sys . argv or "harvest" in sys . argv : \n 
~~~ CACHES = { \n 
: "127.0.0.1:6379" , \n 
: { : 1 } , \n 
: 300 \n 
LOGGING = { \n 
~~ BROKER_URL = \n 
CELERY_RESULT_BACKEND = BROKER_URL \n 
CELERY_DEFAULT_QUEUE = \n 
CELERY_QUEUES = ( \n 
Queue ( \n 
CELERY_DEFAULT_QUEUE , \n 
Exchange ( CELERY_DEFAULT_QUEUE ) , \n 
routing_key = CELERY_DEFAULT_QUEUE \n 
REQUIRE_UNIQUE_EMAIL = False \n 
COMPRESS_ENABLED = False \n 
if "COMPRESS_ENABLED" not in locals ( ) or not COMPRESS_ENABLED : \n 
~~~ COMPRESS_PRECOMPILERS = ( ) \n 
COMPRESS_CSS_FILTERS = [ ] \n 
COMPRESS_JS_FILTERS = [ ] \n 
~~ ALLOWED_HOSTS = [ ] \n 
~~~ import imp \n 
import config . settings \n 
local_untracked_exists = imp . find_module ( \n 
, config . settings . __path__ \n 
~~ if in locals ( ) : \n 
import celery \n 
import raven \n 
from raven . contrib . celery import register_signal , register_logger_signal \n 
os . environ . setdefault ( , ) \n 
class Celery ( celery . Celery ) : \n 
~~~ def on_configure ( self ) : \n 
~~~ client = raven . Client ( settings . RAVEN_CONFIG [ ] ) \n 
register_logger_signal ( client ) \n 
register_signal ( client ) \n 
~~ ~~ ~~ app = Celery ( ) \n 
app . config_from_object ( ) \n 
app . autodiscover_tasks ( lambda : settings . SEED_CORE_APPS ) \n 
~~~ app . start ( ) \n 
from salad . steps . everything import ImportRecord , world , django_url , time , Project \n 
from lettuce import step \n 
@ step ( ) \n 
def i_visit_the_home_page ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:home" ) ) ) \n 
~~ @ step ( ) \n 
def given_i_go_to_the_jasmine_unit_tests_for_the_SEED ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:angular_js_tests" ) ) ) \n 
def then_i_should_see_that_the_tests_passed ( step ) : \n 
~~~ time . sleep ( 2 ) \n 
~~~ assert world . browser . is_element_present_by_css ( ".passingAlert.bar" ) \n 
~~~ time . sleep ( 50 ) \n 
assert len ( world . browser . find_by_css ( ".passingAlert.bar" ) ) > 0 \n 
~~ ~~ @ step ( ) \n 
def when_i_visit_the_projects_page ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:home" ) ) + "#/projects" ) \n 
def then_i_should_see_my_projects ( step ) : \n 
~~~ assert world . browser . is_text_present ( ) \n 
assert world . browser . is_text_present ( ) \n 
def and_i_have_a_project ( step ) : \n 
~~~ Project . objects . create ( \n 
super_organization_id = world . org . id , \n 
owner = world . user \n 
def and_i_have_a_dataset ( step ) : \n 
~~~ ImportRecord . objects . create ( \n 
super_organization = world . org , \n 
def when_i_visit_the_dataset_page ( step ) : \n 
~~~ world . browser . visit ( django_url ( reverse ( "seed:home" ) ) + "#/data" ) \n 
def and_i_delete_a_dataset ( step ) : \n 
~~~ delete_icon = world . browser . find_by_css ( ) \n 
delete_icon . click ( ) \n 
alert = world . browser . get_alert ( ) \n 
alert . accept ( ) \n 
def then_i_should_see_no_datasets ( step ) : \n 
~~~ number_of_datasets = len ( world . browser . find_by_css ( ) ) \n 
number_of_datasets = len ( world . browser . find_by_css ( ) ) \n 
assert number_of_datasets == 0 \n 
MAP = { \n 
from django . db import models , migrations \n 
( , models . ForeignKey ( primary_key = True , serialize = False , to = ] , \n 
bases = ( models . Model , ) , \n 
import pprint \n 
import csv \n 
from calendar import timegm \n 
def upload_file ( upload_header , upload_filepath , main_url , upload_dataset_id , upload_datatype ) : \n 
def _upload_file_to_aws ( aws_upload_details ) : \n 
sig_uri = aws_upload_details [ ] \n 
now = dt . datetime . utcnow ( ) \n 
expires = now + dt . timedelta ( hours = 1 ) \n 
now_ts = timegm ( now . timetuple ( ) ) \n 
key = % ( filename , now_ts ) \n 
payload = { } \n 
payload [ ] = expires . isoformat ( ) + \n 
payload [ ] = [ \n 
{ : aws_upload_details [ ] } , \n 
{ : } , \n 
{ : key } \n 
sig_result = requests . post ( main_url + sig_uri , \n 
headers = upload_header , \n 
data = json . dumps ( payload ) ) \n 
if sig_result . status_code != 200 : \n 
raise RuntimeError ( msg ) \n 
~~~ sig_result = sig_result . json ( ) \n 
~~ upload_url = "http://%s.s3.amazonaws.com/" % ( aws_upload_details [ ] ) \n 
s3_payload = [ \n 
( , key ) , \n 
( , aws_upload_details [ ] ) , \n 
( , sig_result [ ] ) , \n 
( , ( filename , open ( upload_filepath , ) ) ) \n 
result = requests . post ( upload_url , \n 
files = s3_payload ) \n 
if result . status_code != 200 : \n 
~~ completion_uri = aws_upload_details [ ] \n 
completion_payload = { \n 
: upload_dataset_id , \n 
: key , \n 
: upload_datatype \n 
return requests . get ( main_url + completion_uri , \n 
params = completion_payload ) \n 
~~ def _upload_file_to_file_system ( upload_details ) : \n 
upload_url = "%s%s" % ( main_url , upload_details [ ] ) \n 
fsysparams = { : upload_filepath , \n 
: upload_datatype } \n 
return requests . post ( upload_url , \n 
params = fsysparams , \n 
files = { : open ( upload_filepath , ) } , \n 
headers = upload_header ) \n 
~~ upload_details = requests . get ( main_url + , headers = upload_header ) \n 
upload_details = upload_details . json ( ) \n 
filename = os . path . basename ( upload_filepath ) \n 
if upload_details [ ] == : \n 
~~~ return _upload_file_to_aws ( upload_details ) \n 
~~ elif upload_details [ ] == : \n 
~~~ return _upload_file_to_file_system ( upload_details ) \n 
upload_details [ ] ) \n 
~~ ~~ def check_status ( resultOut , partmsg , log , PIIDflag = None ) : \n 
if resultOut . status_code in [ 200 , 403 , 401 ] : \n 
~~~ if PIIDflag == : \n 
~~~ msg = pprint . pformat ( resultOut . json ( ) , indent = 2 , width = 70 ) \n 
~~~ if in resultOut . json ( ) . keys ( ) and resultOut . json ( ) [ ] == : \n 
~~~ msg = resultOut . json ( ) [ ] \n 
log . error ( partmsg + ) \n 
log . debug ( msg ) \n 
raise RuntimeError \n 
~~ elif in resultOut . json ( ) . keys ( ) and not resultOut . json ( ) [ ] : \n 
~~~ msg = resultOut . json ( ) \n 
~~~ msg = + str ( len ( resultOut . json ( ) [ ~~ elif PIIDflag == : \n 
~~~ msg = + str ( len ( resultOut . json ( ) [ ] [ 0 ] ) ) \n 
~~ elif PIIDflag == : \n 
~~~ msg = pprint . pformat ( resultOut . json ( ) [ ] , indent = 2 ~~ elif PIIDflag == : \n 
~~~ log . error ( partmsg , ) \n 
log . debug ( ) \n 
~~ ~~ log . info ( partmsg + ) \n 
~~~ msg = resultOut . reason \n 
~~ def check_progress ( mainURL , Header , progress_key ) : \n 
time . sleep ( 5 ) \n 
progressResult = requests . get ( mainURL + , \n 
headers = Header , \n 
data = json . dumps ( { : progress_key } ) ) \n 
if progressResult . json ( ) [ ] == 100 : \n 
~~~ return ( progressResult ) \n 
~~~ progressResult = check_progress ( mainURL , Header , progress_key ) \n 
~~ ~~ def read_map_file ( mapfilePath ) : \n 
mapReader = csv . reader ( open ( mapfilePath , ) ) \n 
maplist = list ( ) \n 
for rowitem in mapReader : \n 
~~~ maplist . append ( rowitem ) \n 
~~ return maplist \n 
~~ def setup_logger ( filename ) : \n 
logging . getLogger ( "requests" ) . setLevel ( logging . WARNING ) \n 
logger = logging . getLogger ( ) \n 
formatter = logging . Formatter ( ) \n 
formatter_console = logging . Formatter ( ) \n 
fh = logging . FileHandler ( filename , mode = ) \n 
fh . setLevel ( logging . DEBUG ) \n 
fh . setFormatter ( formatter ) \n 
logger . addHandler ( fh ) \n 
ch = logging . StreamHandler ( ) \n 
ch . setLevel ( logging . INFO ) \n 
ch . setFormatter ( formatter_console ) \n 
logger . addHandler ( ch ) \n 
return logger \n 
from seed . utils . generic import split_model_fields \n 
class DummyClass ( object ) : \n 
field_one = "field_one" \n 
field_two = "field_two" \n 
~~ class TestGenericUtils ( TestCase ) : \n 
~~~ def test_split_model_fields ( self ) : \n 
f1 = \n 
f2 = \n 
f3 = \n 
f4 = \n 
obj = DummyClass ( ) \n 
fields_to_split = [ f1 , f2 , f3 , f4 ] \n 
obj_fields , non_obj_fields = split_model_fields ( obj , fields_to_split ) \n 
self . assertEqual ( obj_fields , [ f1 , f2 ] ) \n 
self . assertEqual ( non_obj_fields , [ f3 , f4 ] ) \n 
fields_to_split = [ f1 ] \n 
self . assertEqual ( obj_fields , [ f1 ] ) \n 
self . assertEqual ( non_obj_fields , [ ] ) \n 
fields_to_split = [ f4 ] \n 
self . assertEqual ( obj_fields , [ ] ) \n 
self . assertEqual ( non_obj_fields , [ f4 ] ) \n 
~~ ~~ from setuptools import setup , find_packages \n 
packages = find_packages ( ) , \n 
license = , \n 
author = , \n 
author_email = , \n 
description = scripts = [ ] \n 
import time as t \n 
from os . path import join as pjoin \n 
from StringIO import StringIO \n 
import tempfile \n 
from nose . tools import assert_true , assert_equal \n 
from numpy . testing import assert_array_equal \n 
import smartdispatch \n 
from smartdispatch import utils \n 
def test_generate_name_from_command ( ) : \n 
~~~ date_length = 20 \n 
expected = "_" . join ( command . split ( ) ) \n 
assert_equal ( smartdispatch . generate_name_from_command ( command ) [ date_length : ] , expected ) \n 
max_length_arg = 7 \n 
long_arg = "veryverylongarg1" \n 
expected = command . split ( ) \n 
expected [ 1 ] = long_arg [ - max_length_arg : ] \n 
expected = "_" . join ( expected ) \n 
assert_equal ( smartdispatch . generate_name_from_command ( command , max_length_arg ) [ date_length : ] , expected \n 
max_length = 23 \n 
assert_equal ( smartdispatch . generate_name_from_command ( command , max_length = max_length + date_length \n 
expected = "command_pathnumberone_pathnumbertwo" \n 
~~ def test_get_commands_from_file ( ) : \n 
"command2" , \n 
fileobj = StringIO ( "\\n" . join ( commands ) ) \n 
assert_array_equal ( smartdispatch . get_commands_from_file ( fileobj ) , commands ) \n 
fileobj = StringIO ( "\\n" . join ( commands ) + "\\n" ) \n 
~~ def test_unfold_command ( ) : \n 
~~~ cmd = "ls" \n 
assert_equal ( smartdispatch . unfold_command ( cmd ) , [ "ls" ] ) \n 
~~ def test_replace_uid_tag ( ) : \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( [ command ] ) , [ command ] ) \n 
uid = utils . generate_uid_from_string ( command ) \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( [ command ] ) , [ command . replace ( "{UID}" , uid ) ] ) \n 
uid = utils . generate_uid_from_string ( commands [ 0 ] ) \n 
assert_array_equal ( smartdispatch . replace_uid_tag ( commands ) , [ commands [ 0 ] . replace ( "{UID}" , uid ) ] \n 
~~ def test_get_available_queues ( ) : \n 
~~~ assert_equal ( smartdispatch . get_available_queues ( cluster_name = None ) , { } ) \n 
assert_equal ( smartdispatch . get_available_queues ( cluster_name = "unknown" ) , { } ) \n 
queues_infos = smartdispatch . get_available_queues ( cluster_name = "guillimin" ) \n 
assert_true ( len ( queues_infos ) > 0 ) \n 
queues_infos = smartdispatch . get_available_queues ( cluster_name = "mammouth" ) \n 
~~ def test_get_job_folders ( ) : \n 
~~~ temp_dir = tempfile . mkdtemp ( ) \n 
jobname = "this_is_the_name_of_my_job" \n 
job_folders_paths = smartdispatch . get_job_folders ( temp_dir , jobname ) \n 
path_job , path_job_logs , path_job_commands = job_folders_paths \n 
assert_true ( jobname in path_job ) \n 
assert_true ( os . path . isdir ( path_job ) ) \n 
assert_equal ( os . path . basename ( path_job ) , jobname ) \n 
assert_true ( jobname in path_job_logs ) \n 
assert_true ( os . path . isdir ( path_job_logs ) ) \n 
assert_true ( os . path . isdir ( pjoin ( path_job_logs , ) ) ) \n 
assert_equal ( os . path . basename ( path_job_logs ) , "logs" ) \n 
assert_true ( jobname in path_job_commands ) \n 
assert_true ( os . path . isdir ( path_job_commands ) ) \n 
assert_equal ( os . path . basename ( path_job_commands ) , "commands" ) \n 
jobname += "2" \n 
os . rename ( path_job , path_job + "2" ) \n 
shutil . rmtree ( temp_dir ) \n 
~~ def test_log_command_line ( ) : \n 
command_line_log_file = pjoin ( temp_dir , "command_line.log" ) \n 
smartdispatch . log_command_line ( temp_dir , command_1 ) \n 
assert_true ( os . path . isfile ( command_line_log_file ) ) \n 
lines = open ( command_line_log_file ) . read ( ) . strip ( ) . split ( "\\n" ) \n 
assert_equal ( lines [ 1 ] , command_1 ) \n 
smartdispatch . log_command_line ( temp_dir , command_2 ) \n 
assert_equal ( len ( lines ) , 5 ) \n 
assert_equal ( lines [ 4 ] , command_2 . replace ( \'"\' , r\'\\"\' ) ) \n 
smartdispatch . log_command_line ( temp_dir , command_3 ) \n 
assert_equal ( len ( lines ) , 8 ) \n 
assert_equal ( lines [ 7 ] , re . sub ( , r\'"\\1\\2\\3"\' , command_3 ) ) \n 
~~ from . sgd import SGD \n 
from . adagrad import AdaGrad \n 
from . adam import Adam \n 
from . rmsprop import RMSProp \n 
from . adadelta import Adadelta \n 
from zope . interface import Attribute \n 
from zope . interface import Interface \n 
from zope . interface import implements \n 
class IIndexEvent ( Interface ) : \n 
~~ class IIndexUpdate ( Interface ) : \n 
~~ class IPackageEvent ( IIndexEvent ) : \n 
path = Attribute ( ) \n 
~~ class IPackageAdded ( IPackageEvent ) : \n 
~~ class IPackageRemoved ( IPackageEvent ) : \n 
~~ class IndexEvent ( object ) : \n 
~~~ implements ( IIndexEvent ) \n 
def __init__ ( self , datafile , index ) : \n 
self . datafile = datafile \n 
~~ ~~ class IndexUpdate ( IndexEvent ) : \n 
~~~ implements ( IIndexUpdate ) \n 
~~ class PackageEvent ( object ) : \n 
implements ( IPackageEvent ) \n 
def __init__ ( self , index_manager , path = None , name = None , version = None ) : \n 
~~~ self . name = name \n 
self . version = version \n 
self . im = index_manager \n 
if self . name is None and self . path : \n 
~~~ info = self . im . pkginfo_from_file ( path , self . im . move_on_error ) \n 
self . name = info . name \n 
self . version = info . version \n 
~~ ~~ ~~ class PackageAdded ( PackageEvent ) : \n 
~~~ implements ( IPackageAdded ) \n 
~~ class PackageRemoved ( PackageEvent ) : \n 
~~~ implements ( IPackageRemoved ) \n 
import simplejson as json \n 
import codecs \n 
class Filter ( ) : \n 
~~~ FILTERED_RESOURCES = { } \n 
FILTERED_EXTENSIONS = [ ] \n 
def __init__ ( self , json_repo_path , json_filtered_repo_path , filtered_resources_path , filtered_extensions_path ~~~ self . JSON_REPO_PATH = json_repo_path \n 
self . JSON_REPO_FILTERED_PATH = json_filtered_repo_path \n 
self . FILTERED_RESOURCES_PATH = filtered_resources_path \n 
self . FILTERED_EXTENSIONS_PATH = filtered_extensions_path \n 
self . type = type \n 
self . logger = logger \n 
~~ def get_filtered_files_to_dict ( self ) : \n 
~~~ filtered_file = codecs . open ( self . FILTERED_RESOURCES_PATH , , ) \n 
for line in filtered_file : \n 
~~~ splitted_line = line . split ( ) \n 
ref = splitted_line [ 0 ] . strip ( ) \n 
dir = splitted_line [ 1 ] . strip ( ) \n 
file = splitted_line [ 2 ] . strip ( ) \n 
if Filter . FILTERED_RESOURCES . get ( ref ) : \n 
~~~ dir2file_dict = Filter . FILTERED_RESOURCES . get ( ref ) \n 
if dir == "*" : \n 
~~~ dir2file_dict . update ( { : } ) \n 
Filter . FILTERED_RESOURCES . update ( { ref : dir2file_dict } ) \n 
~~ elif dir2file_dict . get ( dir ) : \n 
~~~ files = dir2file_dict . get ( dir ) \n 
if file == "*" : \n 
~~~ dir2file_dict . update ( { dir : [ ] } ) \n 
~~~ files . append ( file ) \n 
dir2file_dict . update ( { dir : files } ) \n 
~~~ if file == "*" : \n 
~~~ dir2file_dict . update ( { dir : [ file ] } ) \n 
~~~ if dir == "*" : \n 
~~~ Filter . FILTERED_RESOURCES . update ( { ref : { : } } ) \n 
~~~ Filter . FILTERED_RESOURCES . update ( { ref : { dir : [ ] } } ) \n 
~~~ Filter . FILTERED_RESOURCES . update ( { ref : { dir : [ file ] } } ) \n 
~~ ~~ ~~ ~~ filtered_file . close ( ) \n 
~~ def get_filtered_extensions_to_list ( self ) : \n 
~~~ file = codecs . open ( self . FILTERED_EXTENSIONS_PATH , , ) \n 
for line in file : \n 
~~~ ext = line . strip ( ) \n 
Filter . FILTERED_EXTENSIONS . append ( ext ) \n 
~~ file . close ( ) \n 
~~ def is_filtered ( self , ext , ref , dir , file ) : \n 
~~~ found = False \n 
if ext in Filter . FILTERED_EXTENSIONS : \n 
~~~ found = True \n 
~~~ if Filter . FILTERED_RESOURCES . get ( ref ) : \n 
~~~ filtered_dirs_in_ref = Filter . FILTERED_RESOURCES . get ( ref ) \n 
if filtered_dirs_in_ref . get ( ) : \n 
~~~ if filtered_dirs_in_ref . get ( dir ) : \n 
~~~ filtered_files_in_dir = filtered_dirs_in_ref . get ( dir ) \n 
if in filtered_files_in_dir : \n 
~~~ if file in filtered_files_in_dir : \n 
~~ ~~ ~~ ~~ ~~ ~~ return found \n 
~~ def get_dirs ( self , dirs ) : \n 
~~~ if not dirs : \n 
~~~ dirs . append ( ) \n 
~~ return dirs \n 
~~ def select_files ( self ) : \n 
~~~ repo_json = codecs . open ( self . JSON_REPO_PATH , , ) \n 
filtered_repo_json = codecs . open ( self . JSON_REPO_FILTERED_PATH , , ) \n 
for json_line in repo_json : \n 
~~~ json_entry = json . loads ( json_line ) \n 
ref = json_entry . get ( ) \n 
dirs = self . get_dirs ( json_entry . get ( ) ) \n 
ext = json_entry . get ( ) \n 
file = json_entry . get ( ) \n 
filtered = True \n 
for dir in dirs : \n 
~~~ if self . is_filtered ( ext , ref , dir , file ) : \n 
~~~ filtered = False \n 
~~ ~~ if not filtered : \n 
~~~ filtered_repo_json . write ( json . dumps ( json_entry ) + ) \n 
~~ ~~ repo_json . close ( ) \n 
filtered_repo_json . close ( ) \n 
~~ def reject_files ( self ) : \n 
filtered = False \n 
~~~ filtered = True \n 
~~ def filter ( self ) : \n 
~~~ start_time = datetime . now ( ) \n 
if self . FILTERED_EXTENSIONS_PATH : \n 
~~~ self . get_filtered_extensions_to_list ( ) \n 
~~ if self . FILTERED_RESOURCES_PATH : \n 
~~~ self . get_filtered_files_to_dict ( ) \n 
~~ if self . type == "in" : \n 
~~~ self . select_files ( ) \n 
~~ elif self . type == "out" : \n 
~~~ self . reject_files ( ) \n 
~~ end_time = datetime . now ( ) \n 
minutes_and_seconds = divmod ( ( end_time - start_time ) . total_seconds ( ) , 60 ) \n 
from cybox . common import Hash \n 
from cybox . objects . file_object import File \n 
from stix . core import STIXPackage , STIXHeader \n 
~~~ shv = Hash ( ) \n 
shv . simple_hash_value = "4EC0027BEF4D7E1786A04D021FA8A67F" \n 
f = File ( ) \n 
h = Hash ( shv , Hash . TYPE_MD5 ) \n 
f . add_hash ( h ) \n 
stix_package = STIXPackage ( ) \n 
stix_header = STIXHeader ( ) \n 
stix_package . stix_header = stix_header \n 
stix_package . add ( f ) \n 
print ( stix_package . to_xml ( ) ) \n 
from mixbox . binding_utils import * \n 
from stix . bindings import register_extension \n 
import stix . bindings . exploit_target as exploit_target_binding \n 
XML_NS = "http://stix.mitre.org/extensions/Vulnerability#CVRF-1" \n 
@ register_extension \n 
class CVRF1_1InstanceType ( exploit_target_binding . VulnerabilityType ) : \n 
subclass = None \n 
superclass = exploit_target_binding . VulnerabilityType \n 
xmlns = XML_NS \n 
xmlns_prefix = "cvrfVuln" \n 
xml_type = "CVRF1.1InstanceType" \n 
def __init__ ( self , Description = None , CVE_ID = None , OSVDB_ID = None , CVSS_Score = None , cvrfdoc = None ) : ~~~ super ( CVRF1_1InstanceType , self ) . __init__ ( Description = Description , CVE_ID = CVE_ID , OSVDB_ID = OSVDB_ID self . cvrfdoc = cvrfdoc \n 
~~ def factory ( * args_ , ** kwargs_ ) : \n 
~~~ if CVRF1_1InstanceType . subclass : \n 
~~~ return CVRF1_1InstanceType . subclass ( * args_ , ** kwargs_ ) \n 
~~~ return CVRF1_1InstanceType ( * args_ , ** kwargs_ ) \n 
~~ ~~ factory = staticmethod ( factory ) \n 
def get_cvrfdoc ( self ) : return self . cvrfdoc \n 
def set_cvrfdoc ( self , cvrfdoc ) : self . cvrfdoc = cvrfdoc \n 
def hasContent_ ( self ) : \n 
~~~ if ( \n 
self . cvrfdoc is not None or \n 
super ( CVRF1_1InstanceType , self ) . hasContent_ ( ) \n 
~~ ~~ def export ( self , lwrite , level , nsmap , namespace_ = XML_NS , name_ = , namespacedef_ ~~~ if pretty_print : \n 
~~~ eol_ = \n 
~~ showIndent ( lwrite , level , pretty_print ) \n 
lwrite ( % ( nsmap [ namespace_ ] , name_ , namespacedef_ and + namespacedef_ or , already_processed = set ( ) \n 
self . exportAttributes ( lwrite , level , already_processed , namespace_ , name_ = if self . hasContent_ ( ) : \n 
~~~ lwrite ( % ( eol_ , ) ) \n 
self . exportChildren ( lwrite , level + 1 , nsmap , XML_NS , name_ , pretty_print = pretty_print ) \n 
showIndent ( lwrite , level , pretty_print ) \n 
lwrite ( % ( nsmap [ namespace_ ] , name_ , eol_ ) ) \n 
~~ ~~ def exportAttributes ( self , lwrite , level , already_processed , namespace_ = , name_ = ~~~ super ( CVRF1_1InstanceType , self ) . exportAttributes ( lwrite , level , already_processed , namespace_ if not in already_processed : \n 
~~~ already_processed . add ( ) \n 
lwrite ( xmlns ) \n 
~~ if not in already_processed : \n 
lwrite ( xsi_type ) \n 
~~ ~~ def exportChildren ( self , lwrite , level , nsmap , namespace_ = XML_NS , name_ = , fromsubclass_ ~~~ super ( CVRF1_1InstanceType , self ) . exportChildren ( lwrite , level , nsmap , namespace_ , name_ , True if pretty_print : \n 
~~ if self . cvrfdoc is not None : \n 
~~~ showIndent ( lwrite , level , pretty_print ) \n 
lwrite ( etree_ . tostring ( self . cvrfdoc , pretty_print = pretty_print ) ) \n 
~~ ~~ def build ( self , node ) : \n 
~~~ already_processed = set ( ) \n 
self . buildAttributes ( node , node . attrib , already_processed ) \n 
for child in node : \n 
~~~ nodeName_ = Tag_pattern_ . match ( child . tag ) . groups ( ) [ - 1 ] \n 
self . buildChildren ( child , node , nodeName_ ) \n 
~~ ~~ def buildAttributes ( self , node , attrs , already_processed ) : \n 
~~~ super ( CVRF1_1InstanceType , self ) . buildAttributes ( node , attrs , already_processed ) \n 
~~ def buildChildren ( self , child_ , node , nodeName_ , fromsubclass_ = False ) : \n 
~~~ if nodeName_ == : \n 
~~~ self . set_cvrfdoc ( child_ ) \n 
~~ super ( CVRF1_1InstanceType , self ) . buildChildren ( child_ , node , nodeName_ , True ) \n 
~~ ~~ GDSClassesMapping = { } \n 
def usage ( ) : \n 
~~~ print USAGE_TEXT \n 
~~ def get_root_tag ( node ) : \n 
~~~ tag = Tag_pattern_ . match ( node . tag ) . groups ( ) [ - 1 ] \n 
rootClass = GDSClassesMapping . get ( tag ) \n 
if rootClass is None : \n 
~~~ rootClass = globals ( ) . get ( tag ) \n 
~~ return tag , rootClass \n 
~~ def parse ( inFileName ) : \n 
~~~ doc = parsexml_ ( inFileName ) \n 
rootNode = doc . getroot ( ) \n 
rootTag , rootClass = get_root_tag ( rootNode ) \n 
~~~ rootTag = \n 
rootClass = CVRF1_1InstanceType \n 
~~ rootObj = rootClass . factory ( ) \n 
rootObj . build ( rootNode ) \n 
doc = None \n 
rootObj . export ( sys . stdout , 0 , name_ = rootTag , \n 
namespacedef_ = , \n 
pretty_print = True ) \n 
return rootObj \n 
~~ def parseEtree ( inFileName ) : \n 
rootElement = rootObj . to_etree ( None , name_ = rootTag ) \n 
content = etree_ . tostring ( rootElement , pretty_print = True , \n 
xml_declaration = True , encoding = "utf-8" ) \n 
sys . stdout . write ( content ) \n 
sys . stdout . write ( ) \n 
return rootObj , rootElement \n 
~~ def parseString ( inString ) : \n 
~~~ from StringIO import StringIO \n 
doc = parsexml_ ( StringIO ( inString ) ) \n 
rootObj . export ( sys . stdout , 0 , name_ = "CVRF1.1InstanceType" , \n 
namespacedef_ = ) \n 
~~~ args = sys . argv [ 1 : ] \n 
if len ( args ) == 1 : \n 
~~~ parse ( args [ 0 ] ) \n 
~~ __all__ = [ \n 
"CVRF1_1InstanceType" \n 
import stix \n 
from stix . utils . deprecated import idref_deprecated \n 
from stix . campaign import Campaign \n 
from stix . coa import CourseOfAction \n 
from stix . exploit_target import ExploitTarget \n 
from stix . indicator import Indicator \n 
from stix . incident import Incident \n 
from stix . report import Report \n 
from stix . threat_actor import ThreatActor \n 
from stix . bindings import stix_core as stix_core_binding \n 
from stix . bindings import stix_common as stix_common_binding \n 
class Campaigns ( stix . EntityList ) : \n 
~~~ _binding = stix_core_binding \n 
_namespace = \n 
_binding_class = _binding . CampaignsType \n 
_contained_type = Campaign \n 
_binding_var = "Campaign" \n 
_inner_name = "campaigns" \n 
_dict_as_list = True \n 
def _is_valid ( self , value ) : \n 
~~~ idref_deprecated ( value ) \n 
return stix . EntityList . _is_valid ( self , value ) \n 
~~ ~~ class CoursesOfAction ( stix . EntityList ) : \n 
_binding_class = _binding . CoursesOfActionType \n 
_contained_type = CourseOfAction \n 
_binding_var = "Course_Of_Action" \n 
_inner_name = "courses_of_action" \n 
~~ ~~ class ExploitTargets ( stix . EntityList ) : \n 
~~~ _binding = stix_common_binding \n 
_binding_class = _binding . ExploitTargetsType \n 
_contained_type = ExploitTarget \n 
_binding_var = "Exploit_Target" \n 
_inner_name = "exploit_targets" \n 
~~ ~~ class Incidents ( stix . EntityList ) : \n 
_binding_class = _binding . IncidentsType \n 
_contained_type = Incident \n 
_binding_var = "Incident" \n 
_inner_name = "incidents" \n 
~~ ~~ class Indicators ( stix . EntityList ) : \n 
_binding_class = _binding . IndicatorsType \n 
_contained_type = Indicator \n 
_binding_var = "Indicator" \n 
_inner_name = "indicators" \n 
~~ ~~ class ThreatActors ( stix . EntityList ) : \n 
_binding_class = _binding . ThreatActorsType \n 
_contained_type = ThreatActor \n 
_binding_var = "Threat_Actor" \n 
_inner_name = "threat_actors" \n 
~~ ~~ class Reports ( stix . EntityList ) : \n 
_binding_class = _binding . ReportsType \n 
_contained_type = Report \n 
_binding_var = "Report" \n 
_inner_name = "reports" \n 
from cybox . common import Contributor \n 
import stix . utils \n 
import stix . bindings . incident as incident_binding \n 
class Contributors ( stix . EntityList ) : \n 
~~~ _namespace = "http://stix.mitre.org/Incident-1" \n 
_binding = incident_binding \n 
_binding_class = _binding . ContributorsType \n 
_contained_type = Contributor \n 
_binding_var = "Contributor" \n 
_inner_name = "contributors" \n 
from stix . test import EntityTestCase \n 
from stix . test . common import structured_text_tests \n 
from stix . common import InformationSource \n 
class InformationSourceTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = InformationSource \n 
_full_dict = { \n 
: "Spiderman" , \n 
: "Batman" , \n 
: "Superman" , \n 
: "2010-11-12T01:02:03" , \n 
: "2013-12-11T03:02:01" , \n 
: "Web" , \n 
: "Superwebs" , \n 
: "Tubes" , \n 
: "Supertubes" , \n 
~~ class InformationSourceMultiDescTests ( EntityTestCase , unittest . TestCase ) : \n 
: structured_text_tests . StructuredTextListTests . _full_dict \n 
from stix . test import EntityTestCase , assert_warnings \n 
from stix . test import data_marking_test \n 
from stix . test . common import related_test , identity_test , kill_chains_test \n 
from stix . core import STIXPackage \n 
import stix . ttp as ttp \n 
from stix . ttp import ( \n 
resource , infrastructure , exploit_targets , malware_instance , exploit , \n 
attack_pattern , behavior \n 
class ExploitTargetsTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = exploit_targets . ExploitTargets \n 
related_test . RelatedExploitTargetTests . _full_dict \n 
~~ class PersonasTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = resource . Personas \n 
_full_dict = [ \n 
identity_test . IdentityTests . _full_dict \n 
~~ class InfrastructureTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = infrastructure . Infrastructure \n 
: [ , ] , \n 
: 2 , \n 
: "example:Observable-1" \n 
~~ class ResourcesTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = ttp . Resource \n 
: PersonasTests . _full_dict , \n 
: "Tool" \n 
: InfrastructureTests . _full_dict \n 
~~ class MalwareInstanceTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = malware_instance . MalwareInstance \n 
_full_dict = _full_dict = { \n 
: [ , ] \n 
~~ class MalwareInstancesTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . MalwareInstances \n 
MalwareInstanceTests . _full_dict \n 
~~ class ExploitTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = exploit . Exploit \n 
~~ class ExploitsTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . Exploits \n 
ExploitTests . _full_dict \n 
~~ class AttackPatternTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = attack_pattern . AttackPattern \n 
def idref_test ( self ) : \n 
~~~ ap = attack_pattern . AttackPattern ( ) \n 
ap . id_ = \n 
self . assertEqual ( ap . id_ , ) \n 
ap . idref = \n 
self . assertEqual ( ap . idref , ) \n 
self . assertEqual ( ap . id_ , None ) \n 
~~ ~~ class AttackPatternsTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . AttackPatterns \n 
AttackPatternTests . _full_dict \n 
~~ class BehaviorTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = behavior . Behavior \n 
: MalwareInstancesTests . _full_dict , \n 
: ExploitsTests . _full_dict , \n 
: AttackPatternsTests . _full_dict \n 
~~ class TTPTests ( EntityTestCase , unittest . TestCase ) : \n 
~~~ klass = ttp . TTP \n 
: "TTP1" , \n 
: ResourcesTests . _full_dict , \n 
: data_marking_test . MarkingTests . _full_dict , \n 
: ExploitTargetsTests . _full_dict , \n 
: BehaviorTests . _full_dict , \n 
: related_test . RelatedPackageRefsTests . _full_dict , \n 
: kill_chains_test . KillChainPhasesReferenceTests . _full_dict \n 
def test_add_description ( self ) : \n 
~~~ o1 = self . klass ( ) \n 
o2 = self . klass ( ) \n 
o1 . add_description ( "Test" ) \n 
o2 . descriptions . add ( "Test" ) \n 
o1 . descriptions . to_dict ( ) , \n 
o2 . descriptions . to_dict ( ) \n 
~~ def test_add_short_description ( self ) : \n 
o1 . add_short_description ( "Test" ) \n 
o2 . short_descriptions . add ( "Test" ) \n 
o1 . short_descriptions . to_dict ( ) , \n 
o2 . short_descriptions . to_dict ( ) \n 
~~ @ assert_warnings \n 
def test_deprecated_related_packages ( self ) : \n 
~~~ t = ttp . TTP ( ) \n 
t . related_packages . append ( STIXPackage ( ) ) \n 
self . assertEqual ( len ( t . related_packages ) , 1 ) \n 
~~ class AzureError ( Exception ) : \n 
def __init__ ( self , message ) : \n 
~~~ self . message = message \n 
~~~ return repr ( self . message ) \n 
~~ ~~ class AzureAccountConfigurationError ( AzureError ) : \n 
~~ class AzureAccountDefaultSectionNotFound ( AzureError ) : \n 
~~ class AzureAccountLoadFailed ( AzureError ) : \n 
~~ class AzureBlobServicePropertyError ( AzureError ) : \n 
~~ class AzureCannotInit ( AzureError ) : \n 
~~ class AzureCloudServiceAddCertificateError ( AzureError ) : \n 
~~ class AzureCloudServiceAddressError ( AzureError ) : \n 
~~ class AzureCloudServiceCreateError ( AzureError ) : \n 
~~ class AzureCloudServiceDeleteError ( AzureError ) : \n 
~~ class AzureCloudServiceOpenSSLError ( AzureError ) : \n 
~~ class AzureCommandNotLoaded ( AzureError ) : \n 
~~ class AzureConfigDefaultLinkError ( AzureError ) : \n 
~~ class AzureConfigAccountFileNotFound ( AzureError ) : \n 
~~ class AzureConfigAccountNotFound ( AzureError ) : \n 
~~ class AzureConfigAddAccountSectionError ( AzureError ) : \n 
~~ class AzureConfigAddRegionSectionError ( AzureError ) : \n 
~~ class AzureConfigParseError ( AzureError ) : \n 
~~ class AzureConfigPublishSettingsError ( AzureError ) : \n 
~~ class AzureConfigRegionNotFound ( AzureError ) : \n 
~~ class AzureConfigSectionNotFound ( AzureError ) : \n 
~~ class AzureConfigVariableNotFound ( AzureError ) : \n 
~~ class AzureConfigWriteError ( AzureError ) : \n 
~~ class AzureContainerCreateError ( AzureError ) : \n 
~~ class AzureContainerDeleteError ( AzureError ) : \n 
~~ class AzureContainerListContentError ( AzureError ) : \n 
~~ class AzureContainerListError ( AzureError ) : \n 
~~ class AzureDataDiskCreateError ( AzureError ) : \n 
~~ class AzureDataDiskDeleteError ( AzureError ) : \n 
~~ class AzureDataDiskNoAvailableLun ( AzureError ) : \n 
~~ class AzureDataDiskShowError ( AzureError ) : \n 
~~ class AzureDomainLookupError ( AzureError ) : \n 
~~ class AzureEndpointCreateError ( AzureError ) : \n 
~~ class AzureEndpointDeleteError ( AzureError ) : \n 
~~ class AzureEndpointListError ( AzureError ) : \n 
~~ class AzureEndpointShowError ( AzureError ) : \n 
~~ class AzureFileShareCreateError ( AzureError ) : \n 
~~ class AzureFileShareDeleteError ( AzureError ) : \n 
~~ class AzureFileShareListError ( AzureError ) : \n 
~~ class AzureHelpNoCommandGiven ( AzureError ) : \n 
~~ class AzureImageNotReachableByCloudServiceError ( AzureError ) : \n 
~~ class AzureInvalidCommand ( AzureError ) : \n 
~~ class AzureLoadCommandUndefined ( AzureError ) : \n 
~~ class AzureManagementCertificateNotFound ( AzureError ) : \n 
~~ class AzureOsImageCreateError ( AzureError ) : \n 
~~ class AzureOsImageDeleteError ( AzureError ) : \n 
~~ class AzureOsImageDetailsShowError ( AzureError ) : \n 
~~ class AzureOsImageListError ( AzureError ) : \n 
~~ class AzureOsImagePublishError ( AzureError ) : \n 
~~ class AzureOsImageReplicateError ( AzureError ) : \n 
~~ class AzureOsImageShowError ( AzureError ) : \n 
~~ class AzureOsImageUnReplicateError ( AzureError ) : \n 
~~ class AzureOsImageUpdateError ( AzureError ) : \n 
~~ class AzurePageBlobAlignmentViolation ( AzureError ) : \n 
~~ class AzurePageBlobSetupError ( AzureError ) : \n 
~~ class AzurePageBlobUpdateError ( AzureError ) : \n 
~~ class AzurePageBlobZeroPageError ( AzureError ) : \n 
~~ class AzureRequestError ( AzureError ) : \n 
~~ class AzureRequestStatusError ( AzureError ) : \n 
~~ class AzureRequestTimeout ( AzureError ) : \n 
~~ class AzureReservedIpCreateError ( AzureError ) : \n 
~~ class AzureReservedIpDeleteError ( AzureError ) : \n 
~~ class AzureReservedIpListError ( AzureError ) : \n 
~~ class AzureReservedIpShowError ( AzureError ) : \n 
~~ class AzureSSHKeyFileNotFound ( AzureError ) : \n 
~~ class AzureServiceManagementError ( AzureError ) : \n 
~~ class AzureServiceManagementUrlNotFound ( AzureError ) : \n 
~~ class AzureStorageAccountCreateError ( AzureError ) : \n 
~~ class AzureStorageAccountDeleteError ( AzureError ) : \n 
~~ class AzureStorageAccountListError ( AzureError ) : \n 
~~ class AzureStorageAccountShowError ( AzureError ) : \n 
~~ class AzureStorageAccountUpdateError ( AzureError ) : \n 
~~ class AzureStorageDeleteError ( AzureError ) : \n 
~~ class AzureStorageFileNotFound ( AzureError ) : \n 
~~ class AzureStorageListError ( AzureError ) : \n 
~~ class AzureStorageNotReachableByCloudServiceError ( AzureError ) : \n 
~~ class AzureStorageStreamError ( AzureError ) : \n 
~~ class AzureStorageUploadError ( AzureError ) : \n 
~~ class AzureSubscriptionCertificateDecodeError ( AzureError ) : \n 
~~ class AzureSubscriptionIdNotFound ( AzureError ) : \n 
~~ class AzureSubscriptionPKCS12DecodeError ( AzureError ) : \n 
~~ class AzureSubscriptionParseError ( AzureError ) : \n 
~~ class AzureSubscriptionPrivateKeyDecodeError ( AzureError ) : \n 
~~ class AzureUnknownCommand ( AzureError ) : \n 
~~ class AzureUnknownServiceName ( AzureError ) : \n 
~~ class AzureUnrecognizedManagementUrl ( AzureError ) : \n 
~~ class AzureVmCreateError ( AzureError ) : \n 
~~ class AzureVmDeleteError ( AzureError ) : \n 
~~ class AzureXZError ( AzureError ) : \n 
~~ import time \n 
from . . azurectl_exceptions import ( \n 
AzureRequestStatusError , \n 
AzureRequestTimeout , \n 
AzureRequestError \n 
class RequestResult ( object ) : \n 
def __init__ ( self , request_id ) : \n 
~~~ self . request_id = request_id \n 
self . request_timeout_count = 60 \n 
self . request_timeout = 5 \n 
~~ def status ( self , service ) : \n 
~~~ return service . get_operation_status ( self . request_id ) \n 
~~~ raise AzureRequestStatusError ( \n 
% ( type ( e ) . __name__ , format ( e ) ) \n 
~~ ~~ def wait_for_request_completion ( self , service ) : \n 
result = self . status ( service ) \n 
while result . status == : \n 
~~~ count = count + 1 \n 
if count > self . request_timeout_count : \n 
~~~ raise AzureRequestTimeout ( \n 
% self . request_id \n 
~~ time . sleep ( self . request_timeout ) \n 
~~ if result . status != : \n 
~~~ raise AzureRequestError ( \n 
% ( \n 
self . request_id , \n 
format ( result . error . message ) , \n 
format ( result . error . code ) \n 
~~ ~~ ~~ import dateutil . parser \n 
from mock import patch \n 
from test_helper import * \n 
import azurectl \n 
from azurectl . azurectl_exceptions import * \n 
from azurectl . commands . storage_container import StorageContainerTask \n 
class TestStorageContainerTask : \n 
~~~ sys . argv = [ \n 
sys . argv [ 0 ] , , , \n 
azurectl . commands . storage_container . AzureAccount . storage_names = mock . Mock ( \n 
return_value = mock . Mock ( ) \n 
self . storage = mock . Mock ( ) \n 
self . storage . upload = mock . Mock ( ) \n 
azurectl . commands . storage_container . Container = mock . Mock ( \n 
azurectl . commands . storage_container . Help = mock . Mock ( \n 
self . task = StorageContainerTask ( ) \n 
self . __init_command_args ( ) \n 
~~ def __init_command_args ( self ) : \n 
~~~ self . task . command_args = { } \n 
self . task . command_args [ ] = False \n 
self . task . command_args [ ] = \n 
~~ def test_process_storage_container_delete ( self ) : \n 
~~~ self . __init_command_args ( ) \n 
self . task . command_args [ ] = True \n 
self . task . process ( ) \n 
self . task . container . delete . assert_called_once_with ( \n 
self . task . command_args [ ] \n 
~~ def test_process_storage_container_create ( self ) : \n 
self . task . container . create . assert_called_once_with ( \n 
~~ @ patch ( ) \n 
def test_process_storage_container_show ( self , mock_out ) : \n 
self . task . container . content . assert_called_once_with ( \n 
~~ @ raises ( AzureInvalidCommand ) \n 
def test_start_date_validation ( self ) : \n 
def test_end_date_validation ( self ) : \n 
def test_permissions_validation ( self ) : \n 
def test_process_storage_container_sas ( self , mock_out ) : \n 
start = dateutil . parser . parse ( \n 
expiry = dateutil . parser . parse ( \n 
self . task . container . sas . assert_called_once_with ( \n 
self . task . command_args [ ] , start , expiry , \n 
def test_process_storage_container_sas_now ( self , mock_out ) : \n 
self . task . command_args [ ] , mock . ANY , expiry , \n 
def test_process_storage_container_sas_expire ( self , mock_out ) : \n 
expiry = start + datetime . timedelta ( days = 30 ) \n 
def test_process_storage_container_list ( self , mock_out ) : \n 
self . task . container . list . assert_called_once_with ( ) \n 
def test_process_storage_container_from_cfg_list ( self , mock_out ) : \n 
~~ def test_process_storage_container_help ( self ) : \n 
self . task . manual . show . assert_called_once_with ( \n 
from google . appengine . ext import db , webapp \n 
from google . appengine . ext . webapp import util \n 
import member \n 
class CleanUp ( webapp . RequestHandler ) : \n 
~~~ if self . request . headers . get ( ) == : \n 
~~~ now = datetime . datetime . now ( ) \n 
thenMem = now - deltaMem \n 
query = db . Query ( member . Member ) . filter ( , thenMem ) \n 
mems = [ ] \n 
for m in query : \n 
~~~ mems . append ( m ) \n 
~~ db . delete ( mems ) \n 
~~ ~~ ~~ def main ( ) : \n 
~~~ application = webapp . WSGIApplication ( [ ( , CleanUp ) ] , \n 
debug = True ) \n 
util . run_wsgi_app ( application ) \n 
__all__ = [ ] \n 
from . import api_utils \n 
~~~ from google . appengine . api import app_identity \n 
from google . appengine . ext import ndb \n 
~~ def _make_sync_method ( name ) : \n 
def sync_wrapper ( self , * args , ** kwds ) : \n 
~~~ method = getattr ( self , name ) \n 
future = method ( * args , ** kwds ) \n 
return future . get_result ( ) \n 
~~ return sync_wrapper \n 
~~ def add_sync_methods ( cls ) : \n 
for name in cls . __dict__ . keys ( ) : \n 
~~~ if name . endswith ( ) : \n 
~~~ sync_name = name [ : - 6 ] \n 
if not hasattr ( cls , sync_name ) : \n 
~~~ setattr ( cls , sync_name , _make_sync_method ( name ) ) \n 
~~ ~~ ~~ return cls \n 
~~ class _AE_TokenStorage_ ( ndb . Model ) : \n 
token = ndb . StringProperty ( ) \n 
expires = ndb . FloatProperty ( ) \n 
~~ @ ndb . tasklet \n 
def _make_token_async ( scopes , service_account_id ) : \n 
rpc = app_identity . create_rpc ( ) \n 
app_identity . make_get_access_token_call ( rpc , scopes , service_account_id ) \n 
token , expires_at = yield rpc \n 
raise ndb . Return ( ( token , expires_at ) ) \n 
~~ class _RestApi ( object ) : \n 
def __init__ ( self , scopes , service_account_id = None , token_maker = None , \n 
retry_params = None ) : \n 
if isinstance ( scopes , basestring ) : \n 
~~~ scopes = [ scopes ] \n 
~~ self . scopes = scopes \n 
self . service_account_id = service_account_id \n 
self . make_token_async = token_maker or _make_token_async \n 
if not retry_params : \n 
~~~ retry_params = api_utils . _get_default_retry_params ( ) \n 
~~ self . retry_params = retry_params \n 
self . user_agent = { : retry_params . _user_agent } \n 
self . expiration_headroom = random . randint ( 60 , 240 ) \n 
~~ def __getstate__ ( self ) : \n 
return { : self . scopes , \n 
: self . service_account_id , \n 
: ( None if self . make_token_async == _make_token_async \n 
else self . make_token_async ) , \n 
: self . retry_params , \n 
: self . expiration_headroom } \n 
~~ def __setstate__ ( self , state ) : \n 
self . __init__ ( state [ ] , \n 
service_account_id = state [ ] , \n 
token_maker = state [ ] , \n 
retry_params = state [ ] ) \n 
self . expiration_headroom = state [ ] \n 
def do_request_async ( self , url , method = , headers = None , payload = None , \n 
deadline = None , callback = None ) : \n 
retry_wrapper = api_utils . _RetryWrapper ( \n 
self . retry_params , \n 
retriable_exceptions = api_utils . _RETRIABLE_EXCEPTIONS , \n 
should_retry = api_utils . _should_retry ) \n 
resp = yield retry_wrapper . run ( \n 
self . urlfetch_async , \n 
headers = headers , \n 
payload = payload , \n 
deadline = deadline , \n 
callback = callback , \n 
follow_redirects = False ) \n 
raise ndb . Return ( ( resp . status_code , resp . headers , resp . content ) ) \n 
def get_token_async ( self , refresh = False ) : \n 
key = % ( self . service_account_id , . join ( self . scopes ) ) \n 
ts = yield _AE_TokenStorage_ . get_by_id_async ( \n 
key , use_cache = True , use_memcache = True , \n 
use_datastore = self . retry_params . save_access_token ) \n 
if refresh or ts is None or ts . expires < ( \n 
time . time ( ) + self . expiration_headroom ) : \n 
~~~ token , expires_at = yield self . make_token_async ( \n 
self . scopes , self . service_account_id ) \n 
timeout = int ( expires_at - time . time ( ) ) \n 
ts = _AE_TokenStorage_ ( id = key , token = token , expires = expires_at ) \n 
if timeout > 0 : \n 
~~~ yield ts . put_async ( memcache_timeout = timeout , \n 
use_datastore = self . retry_params . save_access_token , \n 
use_cache = True , use_memcache = True ) \n 
~~ ~~ raise ndb . Return ( ts . token ) \n 
def urlfetch_async ( self , url , method = , headers = None , \n 
payload = None , deadline = None , callback = None , \n 
follow_redirects = False ) : \n 
headers = { } if headers is None else dict ( headers ) \n 
headers . update ( self . user_agent ) \n 
~~~ self . token = yield self . get_token_async ( ) \n 
~~ except app_identity . InternalError , e : \n 
~~~ if os . environ . get ( , ) . endswith ( ) : \n 
~~~ self . token = None \n 
logging . warning ( \n 
~~ ~~ if self . token : \n 
~~~ headers [ ] = + self . token \n 
~~ deadline = deadline or self . retry_params . urlfetch_timeout \n 
ctx = ndb . get_context ( ) \n 
resp = yield ctx . urlfetch ( \n 
url , payload = payload , method = method , \n 
headers = headers , follow_redirects = follow_redirects , \n 
deadline = deadline , callback = callback ) \n 
raise ndb . Return ( resp ) \n 
~~ ~~ _RestApi = add_sync_methods ( _RestApi ) \n 
from . dict_object import DictObject \n 
class UserProfile ( DictObject ) : \n 
~~~ super ( UserProfile , self ) . __init__ ( kwargs ) \n 
~~ ~~ class UserGroupHeader ( DictObject ) : \n 
~~~ super ( UserGroupHeader , self ) . __init__ ( kwargs ) \n 
~~ ~~ class Team ( DictObject ) : \n 
def __init__ ( self , ** kwargs ) : \n 
~~~ super ( Team , self ) . __init__ ( kwargs ) \n 
def getURI ( cls , id ) : \n 
~~~ return % id \n 
~~ def postURI ( self ) : \n 
~~ def putURI ( self ) : \n 
~~ def deleteURI ( self ) : \n 
~~~ return % self . id \n 
~~ def getACLURI ( self ) : \n 
~~ def putACLURI ( self ) : \n 
~~ ~~ class TeamMember ( DictObject ) : \n 
~~~ if in kwargs : \n 
~~~ kwargs [ ] = UserGroupHeader ( ** kwargs [ ] ) \n 
~~ super ( TeamMember , self ) . __init__ ( kwargs ) \n 
~~ ~~ from nose . tools import assert_raises \n 
from synapseclient . evaluation import Evaluation , Submission \n 
def test_Evaluation ( ) : \n 
assert_raises ( ValueError , Evaluation , name = , description = , status = ) \n 
assert_raises ( ValueError , Evaluation , name = , description = , status = , contentSource \n 
ev = Evaluation ( name = , description = , status = , contentSource = ) \n 
assert ( ev [ ] == ev . name ) \n 
assert ( ev [ ] == ev . description ) \n 
assert ( ev [ ] == ev . status ) \n 
~~ def test_Submission ( ) : \n 
assert_raises ( KeyError , Submission , foo = ) \n 
import ssl \n 
__status__ = "Prototype" \n 
class JiraAPI ( object ) : \n 
~~~ def __init__ ( self , server , credentials ) : \n 
~~~ super ( JiraAPI , self ) . __init__ ( ) \n 
self . server = server \n 
self . credentials = credentials \n 
self . verify = None \n 
__location__ = os . path . realpath ( os . path . join ( os . getcwd ( ) , os . path . dirname ( __file__ ) ) ) \n 
if ( self . server == "jira.exacttarget.com:8443" ) : \n 
~~~ self . verify = os . path . join ( __location__ , ) \n 
~~ ~~ def fetchCommitDetails ( self , url ) : \n 
~~~ r = requests . get ( url , auth = self . auth ( ) , verify = self . verify ) ; \n 
if r . headers [ ] : \n 
~~~ remaining_requests = int ( r . headers [ ] ) \n 
if ( remaining_requests == 0 ) : \n 
~~~ self . _no_more_requests_until = datetime . datetime . fromtimestamp ( float ( r . headers [ \n 
~~ ~~ if ( r . ok ) : \n 
~~~ return r . json ( ) \n 
~~ def jql ( self , query , offset = None ) : \n 
~~~ verbose = False \n 
resource_name = "search" \n 
url = "https://%s/rest/api/latest/%s" % ( self . server , urllib . quote ( resource_name ) ) \n 
params = { "jql" : query } \n 
if offset is not None : \n 
~~~ params [ "startAt" ] = offset \n 
~~ r = requests . get ( url , params = params , headers = { "Authorization" : self . credentials . authorizationHeaderValue if ( r . ok ) : \n 
~~~ results = r . json ( ) \n 
return results \n 
print r . text \n 
~~~ usercredentials_jsonfile = "bugsystems-Jira-usercreds.json" \n 
user_creds_data = open ( usercredentials_jsonfile ) \n 
user_creds = json . load ( user_creds_data ) \n 
user = user_creds [ "user" ] \n 
password = user_creds [ "token" ] \n 
server_url = \n 
jira = JiraAPI ( server_url , user , password ) \n 
import dateutil . parser \n 
from Empire . cloudservices . github import GithubOrg , GithubRepo , GithubCommit \n 
from repos . base import RepoSource , RepoCommit , RepoPatch \n 
from repos . diffparser import DiffParser \n 
owner = \n 
repo = \n 
class GithubSource ( RepoSource ) : \n 
~~~ def __init__ ( self , creds = None , host = , owner = , repo = ) : \n 
~~~ self . _last_date = None \n 
self . _last_identifier = None \n 
self . _no_more_requests_until = None \n 
github_org = GithubOrg ( host , owner , creds ) \n 
self . _github_repo = GithubRepo ( github_org , repo ) \n 
~~ def processSinceIdentifier ( self , identifier , commit_started_callback , patch_callback , commit_finished_callback ~~~ since_datetime = datetime . datetime . utcnow ( ) \n 
since_datetime = since_datetime . replace ( tzinfo = pytz . UTC , hour = 0 , minute = 0 , second = 0 ) \n 
if identifier : \n 
~~~ since_datetime = dateutil . parser . parse ( identifier ) \n 
~~ ~~ commits = self . _github_repo . commits ( since_datetime , path = path ) \n 
self . processCommits ( commits , \n 
commit_started_callback = commit_started_callback , \n 
patch_callback = patch_callback , \n 
commit_finished_callback = commit_finished_callback ) ; \n 
if self . _last_date : \n 
~~~ return self . _last_date . isoformat ( ) \n 
~~ if since_datetime : \n 
~~~ return since_datetime . isoformat ( ) \n 
~~ def processCommits ( self , commits , commit_started_callback , patch_callback , commit_finished_callback ~~~ if commits is None : \n 
~~ commits = commits [ : : - 1 ] \n 
for github_commit in commits : \n 
if github_commit . sha : \n 
~~~ github_commit = self . _github_repo . commit ( github_commit . sha ) \n 
repo_commit = RepoCommit ( ) ; \n 
repo_commit . url = github_commit . html_url \n 
repo_commit . repo_source = self \n 
if github_commit . date is not None : \n 
~~~ self . _last_date = dateutil . parser . parse ( github_commit . date ) \n 
repo_commit . date = self . _last_date \n 
self . _last_date += datetime . timedelta ( seconds = 1 ) \n 
repo_commit . identifier = self . _last_date . isoformat ( ) \n 
repo_commit . committer_email = github_commit . committer_email \n 
repo_commit . committer_name = github_commit . committer_name \n 
repo_commit . username = github_commit . committer_login \n 
repo_commit . message = github_commit . message \n 
repo_commit . sha = github_commit . sha \n 
commit_started_callback ( repo_commit ) \n 
if github_commit . files : \n 
~~~ for file_info in github_commit . files : \n 
~~~ if file_info . get ( ) : \n 
~~~ filename = committer_username = None \n 
diff = DiffParser ( file_info [ ] ) \n 
repo_patch = RepoPatch ( repo_commit = repo_commit ) \n 
repo_patch . diff = diff \n 
repo_patch . filename = file_info . get ( "filename" ) \n 
patch_callback ( repo_patch ) \n 
~~ ~~ ~~ commit_finished_callback ( repo_commit ) \n 
~~ ~~ ~~ logger . debug ( "done" ) \n 
~~~ from Empire . creds import CredentialManager \n 
credentials_file = "credentials.json" \n 
credential_key = os . environ . get ( ) \n 
if credential_key is None : \n 
~~~ credential_key = getpass . getpass ( ) \n 
~~ credential_manager = CredentialManager ( credentials_file , credential_key ) \n 
creds = credential_manager . get_or_create_credentials_for ( "github-mfeldmansf" , "password" ) \n 
test = GithubSource ( creds ) ; \n 
def cstart ( commit ) : \n 
~~~ print commit \n 
~~ def pstart ( patch ) : \n 
~~ def cend ( commit ) : \n 
~~ test . processSinceIdentifier ( "2014-11-12T00:00:00Z" , cstart , pstart , cend ) ; \n 
def upload_test_results ( ) : \n 
~~~ APEXTESTSDB_BASE_URL = os . environ . get ( ) \n 
APEXTESTSDB_USER_ID = os . environ . get ( ) \n 
APEXTESTSDB_TOKEN = os . environ . get ( ) \n 
PACKAGE = os . environ . get ( ) \n 
REPOSITORY_URL = os . environ . get ( ) \n 
BRANCH_NAME = os . environ . get ( ) \n 
COMMIT_SHA = os . environ . get ( ) \n 
EXECUTION_NAME = os . environ . get ( ) \n 
EXECUTION_URL = os . environ . get ( ) \n 
RESULTS_FILE_URL = os . environ . get ( ) \n 
ENVIRONMENT_NAME = os . environ . get ( ) \n 
payload = { \n 
: PACKAGE , \n 
: REPOSITORY_URL , \n 
: BRANCH_NAME , \n 
: COMMIT_SHA , \n 
: EXECUTION_NAME , \n 
: EXECUTION_URL , \n 
: ENVIRONMENT_NAME , \n 
: APEXTESTSDB_USER_ID , \n 
: APEXTESTSDB_TOKEN , \n 
: RESULTS_FILE_URL , \n 
response = requests . post ( APEXTESTSDB_BASE_URL + , data = payload ) \n 
data = json . loads ( response . content ) \n 
return % ( APEXTESTSDB_BASE_URL , data [ ] ) \n 
~~~ execution_detail_url = upload_test_results ( ) \n 
print execution_detail_url \n 
~~~ import traceback \n 
exc_type , exc_value , exc_traceback = sys . exc_info ( ) \n 
print * 60 \n 
traceback . print_exception ( exc_type , exc_value , exc_traceback , file = sys . stdout ) \n 
~~ ~~ from flask import request , session , render_template , redirect , url_for \n 
import glass \n 
import foursquare \n 
app = glass . Application ( \n 
client_id = config . GOOGLE_CLIENT_ID , \n 
client_secret = config . GOOGLE_CLIENT_SECRET , \n 
scopes = config . GOOGLE_SCOPES , \n 
template_folder = "templates" , \n 
static_url_path = , \n 
static_folder = ) \n 
app . web . secret_key = \n 
FOURSQUARE_TOKENS = { } \n 
def foursquare_client ( ) : \n 
~~~ return foursquare . Foursquare ( client_id = config . FOURSQUARE_CLIENT_ID , client_secret = config . FOURSQUARE_CLIENT_SECRET \n 
~~ @ app . web . route ( "/" ) \n 
~~~ return render_template ( "index.html" , auth = False ) \n 
~~ @ app . subscriptions . login \n 
def login ( user ) : \n 
session [ ] = user . token \n 
return redirect ( "/foursquare/authorize" ) \n 
~~ @ app . subscriptions . location \n 
def change_location ( user ) : \n 
~~~ location = user . location ( ) \n 
llat = location . get ( ) \n 
llong = location . get ( ) \n 
client = foursquare . Foursquare ( access_token = FOURSQUARE_TOKENS [ user . token ] ) \n 
venues = client . venues . search ( params = { : llat + + llong , : location . get ( ) } ) \n 
if len ( venues [ ] ) > 0 : \n 
~~~ user . timeline . post_template ( "venue.html" , venue = venues [ ] [ 0 ] , llat = llat , llong = llong ) \n 
~~ ~~ @ app . web . route ( "/foursquare/authorize" ) \n 
def foursquare_authorize ( ) : \n 
~~~ client = foursquare_client ( ) \n 
return redirect ( client . oauth . auth_url ( ) ) \n 
~~ @ app . web . route ( "/foursquare/callback" ) \n 
def foursquare_callback ( ) : \n 
~~~ code = request . args . get ( , None ) \n 
client = foursquare_client ( ) \n 
if code is None or not in session : \n 
~~ access_token = client . oauth . get_token ( code ) \n 
FOURSQUARE_TOKENS [ session [ ] ] = access_token \n 
client . set_access_token ( access_token ) \n 
user = client . users ( ) \n 
username = user [ ] [ ] \n 
userglass = glass . User ( app = app , token = session [ ] ) \n 
return render_template ( "index.html" , auth = True ) \n 
app . run ( port = config . PORT , host = config . HOST ) \n 
~~ import logging \n 
from hashlib import sha1 \n 
from redis . client import StrictRedis \n 
from redis . exceptions import ConnectionError \n 
class Node ( object ) : \n 
redis_client_class = StrictRedis \n 
def __init__ ( self , name , host , port ) : \n 
self . connection = self . redis_client_class ( host , int ( port ) ) \n 
~~ ~~ def executeOnNode ( func ) : \n 
@ wraps ( func ) \n 
def wrapper ( self , key , * args , ** kwargs ) : \n 
~~~ node = self . get_node_for_key ( key ) \n 
nodeFunc = getattr ( node . connection , func . __name__ ) \n 
~~~ return nodeFunc ( key , * args , ** kwargs ) \n 
~~ except ConnectionError : \n 
~~~ node = self . get_master ( node ) \n 
return nodeFunc ( key , * args , ** kwargs ) \n 
~~ ~~ return wrapper \n 
~~ class DisredisClient ( object ) : \n 
sentinel = None \n 
nodes = None \n 
def __init__ ( self , sentinel_addresses ) : \n 
~~~ self . sentinel_addresses = sentinel_addresses \n 
self . _get_nodes ( ) \n 
~~~ address = self . sentinel_addresses . pop ( 0 ) \n 
host , port = address . split ( ":" ) \n 
self . sentinel = self . redis_client_class ( host , int ( port ) ) \n 
self . sentinel_addresses . append ( address ) \n 
~~~ if not self . sentinel_addresses : \n 
~~ ~~ except IndexError : \n 
~~ ~~ ~~ def _execute_sentinel_command ( self , * args , ** kwargs ) : \n 
~~~ if self . sentinel is None : \n 
~~~ self . _connect ( ) \n 
~~ return self . sentinel . execute_command ( "SENTINEL" , * args , \n 
~~~ self . sentinel = None \n 
if self . sentinel_addresses : \n 
~~ ~~ ~~ ~~ def _get_nodes ( self ) : \n 
masterList = self . _execute_sentinel_command ( "MASTERS" ) \n 
self . nodes = [ ] \n 
for master in masterList : \n 
~~~ info = dict ( zip ( master [ : : 2 ] , master [ 1 : : 2 ] ) ) \n 
self . nodes . append ( Node ( info [ "name" ] , info [ "ip" ] , info [ "port" ] ) ) \n 
~~ ~~ def get_master ( self , node ) : \n 
host , port = self . _execute_sentinel_command ( "get-master-addr-by-name" , \n 
node . name ) \n 
if host == node . host and port == node . port : \n 
~~~ return node \n 
~~ newNode = Node ( node . name , host , port ) \n 
self . nodes [ self . nodes . index ( node ) ] = newNode \n 
return newNode \n 
~~ def get_node_for_key ( self , key ) : \n 
if "{" in key and "}" in key : \n 
~~~ key = key [ key . index ( "{" ) + 1 : key . index ( "}" ) ] \n 
~~ return self . nodes [ int ( sha1 ( key ) . hexdigest ( ) , 16 ) % len ( self . nodes ) ] \n 
~~ def set_response_callback ( self , command , callback ) : \n 
~~ def pipeline ( self , transaction = True , shard_hint = None ) : \n 
~~ def transaction ( self , func , * watches , ** kwargs ) : \n 
~~ @ executeOnNode \n 
def lock ( self , name , timeout = None , sleep = 0.1 ) : \n 
def pubsub ( self , shard_hint = None ) : \n 
~~ def bgrewriteaof ( self ) : \n 
~~ def bgsave ( self ) : \n 
~~ def client_kill ( self , address ) : \n 
~~ def client_list ( self ) : \n 
~~ def client_getname ( self ) : \n 
~~ def client_setname ( self , name ) : \n 
~~ def config_get ( self , pattern = "*" ) : \n 
~~ def config_set ( self , name , value ) : \n 
~~ def dbsize ( self ) : \n 
~~ def time ( self ) : \n 
def debug_object ( self , key ) : \n 
~~ def delete ( self , * names ) : \n 
return self . execute_command ( , * names ) \n 
~~ __delitem__ = delete \n 
def echo ( self , value ) : \n 
~~ def flushall ( self ) : \n 
~~ def flushdb ( self ) : \n 
~~ def info ( self , section = None ) : \n 
~~ def lastsave ( self ) : \n 
~~ def object ( self , infotype , key ) : \n 
~~ def ping ( self ) : \n 
~~ def shutdown ( self ) : \n 
~~ def slaveof ( self , host = None , port = None ) : \n 
def append ( self , key , value ) : \n 
def getrange ( self , key , start , end ) : \n 
def bitcount ( self , key , start = None , end = None ) : \n 
~~ def bitop ( self , operation , dest , * keys ) : \n 
def decr ( self , name , amount = 1 ) : \n 
def exists ( self , name ) : \n 
~~ __contains__ = exists \n 
@ executeOnNode \n 
def expire ( self , name , time ) : \n 
def expireat ( self , name , when ) : \n 
def get ( self , name ) : \n 
~~ def __getitem__ ( self , name ) : \n 
value = self . get ( name ) \n 
~~~ return value \n 
~~ raise KeyError ( name ) \n 
def getbit ( self , name , offset ) : \n 
def getset ( self , name , value ) : \n 
def incr ( self , name , amount = 1 ) : \n 
~~ def incrby ( self , name , amount = 1 ) : \n 
return self . incr ( name , amount ) \n 
def incrbyfloat ( self , name , amount = 1.0 ) : \n 
~~ def keys ( self , pattern = ) : \n 
~~ def mget ( self , keys , * args ) : \n 
~~ def mset ( self , mapping ) : \n 
~~ def msetnx ( self , mapping ) : \n 
def move ( self , name , db ) : \n 
def persist ( self , name ) : \n 
def pexpire ( self , name , time ) : \n 
def pexpireat ( self , name , when ) : \n 
def psetex ( self , name , time_ms , value ) : \n 
def pttl ( self , name ) : \n 
~~ def randomkey ( self ) : \n 
~~ def rename ( self , src , dst ) : \n 
~~ def renamenx ( self , src , dst ) : \n 
def set ( self , name , value , ex = None , px = None , nx = False , xx = False ) : \n 
~~ __setitem__ = set \n 
def setbit ( self , name , offset , value ) : \n 
def setex ( self , name , time , value ) : \n 
def setnx ( self , name , value ) : \n 
def setrange ( self , name , offset , value ) : \n 
def strlen ( self , name ) : \n 
def substr ( self , name , start , end = - 1 ) : \n 
def ttl ( self , name ) : \n 
def type ( self , name ) : \n 
~~ def blpop ( self , keys , timeout = 0 ) : \n 
~~ def brpop ( self , keys , timeout = 0 ) : \n 
~~ def brpoplpush ( self , src , dst , timeout = 0 ) : \n 
def lindex ( self , name , index ) : \n 
def linsert ( self , name , where , refvalue , value ) : \n 
def llen ( self , name ) : \n 
def lpop ( self , name ) : \n 
def lpush ( self , name , * values ) : \n 
def lpushx ( self , name , value ) : \n 
def lrange ( self , name , start , end ) : \n 
def lrem ( self , name , count , value ) : \n 
def lset ( self , name , index , value ) : \n 
def ltrim ( self , name , start , end ) : \n 
def rpop ( self , name ) : \n 
~~ def rpoplpush ( self , src , dst ) : \n 
def rpush ( self , name , * values ) : \n 
def rpushx ( self , name , value ) : \n 
def sort ( self , name , start = None , num = None , by = None , get = None , \n 
desc = False , alpha = False , store = None , groups = False ) : \n 
def sadd ( self , name , * values ) : \n 
def scard ( self , name ) : \n 
~~ def sdiff ( self , keys , * args ) : \n 
~~ def sdiffstore ( self , dest , keys , * args ) : \n 
~~ def sinter ( self , keys , * args ) : \n 
~~ def sinterstore ( self , dest , keys , * args ) : \n 
def sismember ( self , name , value ) : \n 
def smembers ( self , name ) : \n 
~~ def smove ( self , src , dst , value ) : \n 
return self . execute_command ( , src , dst , value ) \n 
def spop ( self , name ) : \n 
def srandmember ( self , name , number = None ) : \n 
def srem ( self , name , * values ) : \n 
~~ def sunion ( self , keys , * args ) : \n 
~~ def sunionstore ( self , dest , keys , * args ) : \n 
def zadd ( self , name , * args , ** kwargs ) : \n 
def zcard ( self , name ) : \n 
def zcount ( self , name , min , max ) : \n 
def zincrby ( self , name , value , amount = 1 ) : \n 
~~ def zinterstore ( self , dest , keys , aggregate = None ) : \n 
def zrange ( self , name , start , end , desc = False , withscores = False , \n 
score_cast_func = float ) : \n 
def zrangebyscore ( self , name , min , max , start = None , num = None , \n 
withscores = False , score_cast_func = float ) : \n 
def zrank ( self , name , value ) : \n 
def zrem ( self , name , * values ) : \n 
def zremrangebyrank ( self , name , min , max ) : \n 
def zremrangebyscore ( self , name , min , max ) : \n 
def zrevrange ( self , name , start , num , withscores = False , \n 
def zrevrangebyscore ( self , name , max , min , start = None , num = None , \n 
def zrevrank ( self , name , value ) : \n 
def zscore ( self , name , value ) : \n 
~~ def zunionstore ( self , dest , keys , aggregate = None ) : \n 
def hdel ( self , name , * keys ) : \n 
def hexists ( self , name , key ) : \n 
def hget ( self , name , key ) : \n 
def hgetall ( self , name ) : \n 
def hincrby ( self , name , key , amount = 1 ) : \n 
def hincrbyfloat ( self , name , key , amount = 1.0 ) : \n 
def hkeys ( self , name ) : \n 
def hlen ( self , name ) : \n 
def hset ( self , name , key , value ) : \n 
def hsetnx ( self , name , key , value ) : \n 
def hmset ( self , name , mapping ) : \n 
def hmget ( self , name , keys , * args ) : \n 
def hvals ( self , name ) : \n 
~~ def publish ( self , channel , message ) : \n 
~~ def eval ( self , script , numkeys , * keys_and_args ) : \n 
~~ def evalsha ( self , sha , numkeys , * keys_and_args ) : \n 
~~ def script_exists ( self , * args ) : \n 
~~ def script_flush ( self ) : \n 
~~ def script_kill ( self ) : \n 
~~ def script_load ( self , script ) : \n 
~~ def register_script ( self , script ) : \n 
from service import upload \n 
from utils import base_url \n 
import Image \n 
import ImageOps \n 
import cStringIO \n 
__all__ = [ "handle_item" , "CONTACTS" , "WELCOMES" ] \n 
CONTACTS = [ \n 
"acceptTypes" : "image/*" , \n 
"id" : "instaglass_sepia" , \n 
"displayName" : "Sepia" , \n 
"imageUrls" : [ base_url + "/images/sepia.jpg" ] \n 
WELCOMES = [ \n 
"</article>" ) \n 
def _make_linear_ramp ( white ) : \n 
ramp = [ ] \n 
r , g , b = white \n 
for i in range ( 255 ) : \n 
~~~ ramp . extend ( ( r * i / 255 , g * i / 255 , b * i / 255 ) ) \n 
~~ return ramp \n 
~~ def _apply_sepia_filter ( image ) : \n 
sepia = _make_linear_ramp ( ( 255 , 240 , 192 ) ) \n 
orig_mode = image . mode \n 
if orig_mode != "L" : \n 
~~~ image = image . convert ( "L" ) \n 
~~ image = ImageOps . autocontrast ( image ) \n 
image . putpalette ( sepia ) \n 
~~~ image = image . convert ( orig_mode ) \n 
~~ return image \n 
~~ def handle_item ( item , notification , service , test ) : \n 
if "userActions" in notification : \n 
~~~ for action in notification [ "userActions" ] : \n 
~~~ if "type" in action and action [ "type" ] == "SHARE" : \n 
~~ if "recipients" in item : \n 
~~~ for rec in item [ "recipients" ] : \n 
~~~ if rec [ "id" ] == "instaglass_sepia" : \n 
~~ imageId = None \n 
if "attachments" in item : \n 
~~~ for att in item [ "attachments" ] : \n 
~~~ if att [ "contentType" ] . startswith ( "image/" ) : \n 
~~~ imageId = att [ "id" ] \n 
~~ ~~ ~~ if imageId is None : \n 
~~ attachment_metadata = service . timeline ( ) . attachments ( ) . get ( \n 
itemId = item [ "id" ] , attachmentId = imageId ) . execute ( ) \n 
content_url = attachment_metadata . get ( "contentUrl" ) \n 
resp , content = service . _http . request ( content_url ) \n 
if resp . status != 200 : \n 
~~ tempimg = cStringIO . StringIO ( content ) \n 
im = Image . open ( tempimg ) \n 
new_im = _apply_sepia_filter ( im ) \n 
f = cStringIO . StringIO ( ) \n 
new_im . save ( f , "JPEG" ) \n 
content = f . getvalue ( ) \n 
new_item = { } \n 
new_item [ "menuItems" ] = [ { "action" : "SHARE" } ] \n 
result = upload . multipart_insert ( new_item , content , "image/jpeg" , service , test ) \n 
logging . info ( result ) \n 
from setuptools import setup \n 
"argparse>=1.2.1" , \n 
"requests>=2.4.3" \n 
description = , \n 
packages = ( , ) , \n 
scripts = ( \n 
install_requires = install_requires , \n 
Handler_mapping = { } \n 
def handler ( cmdid ) : \n 
def _module_dec ( cls ) : \n 
~~~ Handler_mapping [ cmdid ] = cls \n 
return cls \n 
import hashlib \n 
if not ( hasattr ( __builtins__ , "bytes" ) ) or str is bytes : \n 
~~~ def bytes ( var , * args ) : \n 
~~~ return . join ( map ( chr , var ) ) \n 
~~~ return map ( ord , var ) \n 
~~ ~~ ~~ __all__ = [ "rollingchecksum" , "weakchecksum" , "patchstream" , "rsyncdelta" , \n 
"blockchecksums" ] \n 
def rsyncdelta ( datastream , remotesignatures , blocksize = 4096 ) : \n 
remote_weak , remote_strong = remotesignatures \n 
match = True \n 
matchblock = - 1 \n 
deltaqueue = collections . deque ( ) \n 
~~~ if match and datastream is not None : \n 
~~~ window = collections . deque ( bytes ( datastream . read ( blocksize ) ) ) \n 
checksum , a , b = weakchecksum ( window ) \n 
~~~ matchblock = remote_weak . index ( checksum , matchblock + 1 ) \n 
stronghash = hashlib . sha256 ( bytes ( window ) ) . hexdigest ( ) \n 
matchblock = remote_strong . index ( stronghash , matchblock ) \n 
deltaqueue . append ( matchblock ) \n 
if datastream . closed : \n 
~~ continue \n 
~~~ match = False \n 
~~~ if datastream : \n 
~~~ newbyte = ord ( datastream . read ( 1 ) ) \n 
window . append ( newbyte ) \n 
~~ ~~ except TypeError : \n 
~~~ newbyte = 0 \n 
tailsize = datastream . tell ( ) % blocksize \n 
datastream = None \n 
~~ if datastream is None and len ( window ) <= tailsize : \n 
~~~ deltaqueue . append ( window ) \n 
~~ oldbyte = window . popleft ( ) \n 
checksum , a , b = rollingchecksum ( oldbyte , newbyte , a , b , blocksize ) \n 
~~~ deltaqueue [ - 1 ] . append ( oldbyte ) \n 
~~ except ( AttributeError , IndexError ) : \n 
~~~ deltaqueue . append ( [ oldbyte ] ) \n 
~~ ~~ ~~ deltastructure = [ blocksize ] \n 
for element in deltaqueue : \n 
~~~ if isinstance ( element , int ) : \n 
~~~ deltastructure . append ( element ) \n 
~~ elif element : \n 
~~~ deltastructure . append ( bytes ( element ) ) \n 
~~ ~~ return deltastructure \n 
~~ def blockchecksums ( instream , blocksize = 4096 ) : \n 
weakhashes = list ( ) \n 
stronghashes = list ( ) \n 
read = instream . read ( blocksize ) \n 
while read : \n 
~~~ weakhashes . append ( weakchecksum ( bytes ( read ) ) [ 0 ] ) \n 
stronghashes . append ( hashlib . sha256 ( read ) . hexdigest ( ) ) \n 
~~ return weakhashes , stronghashes \n 
~~ def patchstream ( instream , outstream , delta ) : \n 
blocksize = delta [ 0 ] \n 
for element in delta [ 1 : ] : \n 
~~~ if isinstance ( element , int ) and blocksize : \n 
~~~ instream . seek ( element * blocksize ) \n 
element = instream . read ( blocksize ) \n 
~~ outstream . write ( element ) \n 
~~ ~~ def rollingchecksum ( removed , new , a , b , blocksize = 4096 ) : \n 
a -= removed - new \n 
b -= removed * blocksize - a \n 
return ( b << 16 ) | a , a , b \n 
~~ def weakchecksum ( data ) : \n 
a = b = 0 \n 
l = len ( data ) \n 
for i in range ( l ) : \n 
~~~ a += data [ i ] \n 
b += ( l - i ) * data [ i ] \n 
~~ return ( b << 16 ) | a , a , b \n 
from shopify_settings import * \n 
SITE_ROOT = os . path . dirname ( os . path . realpath ( __file__ ) ) \n 
~~~ from djangoappengine . settings_base import * \n 
USING_APP_ENGINE = True \n 
~~~ USING_APP_ENGINE = False \n 
: os . path . join ( SITE_ROOT , ) , \n 
SITE_ID = 1 \n 
~~ USE_I18N = True \n 
USE_L10N = True \n 
MEDIA_ROOT = \n 
MEDIA_URL = \n 
STATIC_ROOT = \n 
STATIC_URL = \n 
ADMIN_MEDIA_PREFIX = \n 
STATICFILES_DIRS = ( \n 
os . path . join ( SITE_ROOT , ) , \n 
TEMPLATE_LOADERS = ( \n 
if not USING_APP_ENGINE : \n 
~~~ TEMPLATE_CONTEXT_PROCESSORS += ( \n 
~~ MIDDLEWARE_CLASSES = ( \n 
ROOT_URLCONF = \n 
TEMPLATE_DIRS = ( \n 
INSTALLED_APPS = ( \n 
if USING_APP_ENGINE : \n 
~~~ INSTALLED_APPS += ( \n 
~~ LOGGING = { \n 
: False , \n 
from . customer_saved_search import CustomerSavedSearch \n 
class CustomerGroup ( CustomerSavedSearch ) : \n 
~~ from . . base import ShopifyResource \n 
class ShippingZone ( ShopifyResource ) : \n 
~~ import shopify \n 
from test . test_helper import TestCase \n 
from pyactiveresource . activeresource import ActiveResource \n 
from pyactiveresource . util import xml_to_dict \n 
class OrderTest ( TestCase ) : \n 
~~~ def test_should_be_loaded_correctly_from_order_xml ( self ) : \n 
order = shopify . Order ( xml_to_dict ( order_xml ) [ "order" ] ) \n 
self . assertEqual ( 1 , len ( order . note_attributes ) ) \n 
note_attribute = order . note_attributes [ 0 ] \n 
self . assertEqual ( "size" , note_attribute . name ) \n 
self . assertEqual ( "large" , note_attribute . value ) \n 
~~ def test_should_be_able_to_add_note_attributes_to_an_order ( self ) : \n 
~~~ order = shopify . Order ( ) \n 
order . note_attributes = [ ] \n 
order . note_attributes . append ( shopify . NoteAttribute ( { : "color" , : "blue" } ) ) \n 
order_xml = xml_to_dict ( order . to_xml ( ) ) \n 
note_attributes = order_xml [ "order" ] [ "note_attributes" ] \n 
self . assertTrue ( isinstance ( note_attributes , list ) ) \n 
attribute = note_attributes [ 0 ] \n 
self . assertEqual ( "color" , attribute [ "name" ] ) \n 
self . assertEqual ( "blue" , attribute [ "value" ] ) \n 
~~ def test_get_order ( self ) : \n 
~~~ self . fake ( , method = , body = self . load_fixture ( ) ) \n 
order = shopify . Order . find ( 450789469 ) \n 
self . assertEqual ( , order . email ) \n 
~~ def test_get_order_transaction ( self ) : \n 
self . fake ( , method = , body = self . load_fixture ( transactions = order . transactions ( ) \n 
self . assertEqual ( "409.94" , transactions [ 0 ] . amount ) \n 
from os . path import exists , dirname \n 
from . base import Base \n 
from deoplete . util import set_default , get_simple_buffer_config \n 
class Source ( Base ) : \n 
~~~ def __init__ ( self , vim ) : \n 
~~~ Base . __init__ ( self , vim ) \n 
self . name = \n 
self . mark = \n 
self . min_pattern_length = 0 \n 
self . rank = 150 \n 
set_default ( self . vim , , 0 ) \n 
~~ def get_complete_position ( self , context ) : \n 
~~~ pos = context [ ] . rfind ( ) \n 
return pos if pos < 0 else pos + 1 \n 
~~ def gather_candidates ( self , context ) : \n 
~~~ p = self . __longest_path_that_exists ( context [ ] ) \n 
if p in ( None , [ ] ) or p == or re . search ( , p ) : \n 
~~ complete_str = self . __substitute_path ( dirname ( p ) + ) \n 
if not os . path . isdir ( complete_str ) : \n 
~~ hidden = context [ ] . find ( ) == 0 \n 
dirs = [ x for x in os . listdir ( complete_str ) \n 
if os . path . isdir ( complete_str + x ) and \n 
( hidden or x [ 0 ] != ) ] \n 
files = [ x for x in os . listdir ( complete_str ) \n 
if not os . path . isdir ( complete_str + x ) and \n 
return [ { : x , : x + } for x in sorted ( dirs ) \n 
] + [ { : x } for x in sorted ( files ) ] \n 
~~ def __longest_path_that_exists ( self , input_str ) : \n 
~~~ data = re . split ( self . vim . call ( \n 
self . vim . options [ ] ) , input_str ) \n 
existing_paths = list ( filter ( lambda x : exists ( \n 
dirname ( self . __substitute_path ( x ) ) ) , pos ) ) \n 
if existing_paths and len ( existing_paths ) > 0 : \n 
~~~ return sorted ( existing_paths ) [ - 1 ] \n 
~~ def __substitute_path ( self , path ) : \n 
~~~ buffer_path = get_simple_buffer_config ( \n 
self . vim , \n 
m = re . match ( , path ) \n 
~~~ h = self . vim . funcs . repeat ( , len ( m . group ( 1 ) ) ) \n 
return re . sub ( , \n 
self . vim . funcs . fnamemodify ( \n 
( self . vim . funcs . bufname ( ) \n 
if buffer_path \n 
else self . vim . funcs . getcwd ( ) ) , + h ) , \n 
path ) \n 
~~ m = re . match ( , path ) \n 
if m and os . environ . get ( ) : \n 
~~~ return re . sub ( , os . environ . get ( ) , path ) \n 
if m and os . environ . get ( m . group ( 1 ) ) : \n 
~~~ return re . sub ( , os . environ . get ( m . group ( 1 ) ) , path ) \n 
IS_PY3 = sys . version_info [ 0 ] == 3 \n 
def itervalues ( obj , ** kwargs ) : \n 
return iter ( obj . values ( ** kwargs ) ) if IS_PY3 else obj . itervalues ( ** kwargs ) \n 
def assert_errors ( errors , expected_errors ) : \n 
assert len ( errors ) == len ( expected_errors ) \n 
for error , expected in zip ( errors , expected_errors ) : \n 
~~~ assert expected in str ( error ) \n 
from os import path \n 
from hcpsdk . version import _Version \n 
#try: \n 
here = path . abspath ( path . dirname ( __file__ ) ) \n 
with open ( path . normpath ( path . join ( here , ) ) , encoding = ) as f : \n 
~~~ long_description = f . read ( ) \n 
~~ setup ( \n 
version = str ( _Version ( ) ) , \n 
long_description = long_description , \n 
keywords = , \n 
packages = find_packages ( exclude = [ ] ) , \n 
install_requires = [ ] , \n 
class PipelineDefinitionError ( Exception ) : \n 
~~~ def __init__ ( self , msg , definition ) : \n 
~~~ full_msg = ( \n 
super ( PipelineDefinitionError , self ) . __init__ ( full_msg ) \n 
self . msg = msg \n 
self . definition = definition \n 
~~ ~~ def api_to_definition ( definition ) : \n 
~~~ if in definition : \n 
~~~ definition [ ] = _api_to_objects_definition ( \n 
definition . pop ( ) ) \n 
~~ if in definition : \n 
~~~ definition [ ] = _api_to_parameters_definition ( \n 
~~~ definition [ ] = _api_to_values_definition ( \n 
~~ return definition \n 
~~ def definition_to_api_objects ( definition ) : \n 
~~~ if not in definition : \n 
~~ api_elements = [ ] \n 
for element in definition [ ] : \n 
~~~ element_id = element . pop ( ) \n 
json . dumps ( element ) , definition ) \n 
~~ api_object = { : element_id } \n 
name = element . pop ( , element_id ) \n 
api_object [ ] = name \n 
fields = [ ] \n 
for key , value in sorted ( element . items ( ) ) : \n 
~~~ fields . extend ( _parse_each_field ( key , value ) ) \n 
~~ api_object [ ] = fields \n 
api_elements . append ( api_object ) \n 
~~ return api_elements \n 
~~ def definition_to_api_parameters ( definition ) : \n 
~~ parameter_objects = [ ] \n 
~~~ parameter_id = element . pop ( ) \n 
~~ parameter_object = { : parameter_id } \n 
attributes = [ ] \n 
~~~ attributes . extend ( _parse_each_field ( key , value ) ) \n 
~~ parameter_object [ ] = attributes \n 
parameter_objects . append ( parameter_object ) \n 
~~ return parameter_objects \n 
~~ def definition_to_parameter_values ( definition ) : \n 
~~ parameter_values = [ ] \n 
for key in definition [ ] : \n 
~~~ parameter_values . extend ( \n 
_convert_single_parameter_value ( key , definition [ ] [ key ] ) ) \n 
~~ return parameter_values \n 
~~ def _parse_each_field ( key , value ) : \n 
~~~ values = [ ] \n 
if isinstance ( value , list ) : \n 
~~~ for item in value : \n 
~~~ values . append ( _convert_single_field ( key , item ) ) \n 
~~~ values . append ( _convert_single_field ( key , value ) ) \n 
~~ return values \n 
~~ def _convert_single_field ( key , value ) : \n 
~~~ field = { : key } \n 
if isinstance ( value , dict ) and list ( value . keys ( ) ) == [ ] : \n 
~~~ field [ ] = value [ ] \n 
~~~ field [ ] = value \n 
~~ return field \n 
~~ def _convert_single_parameter_value ( key , values ) : \n 
~~~ parameter_values = [ ] \n 
if isinstance ( values , list ) : \n 
~~~ for each_value in values : \n 
~~~ parameter_value = { : key , : each_value } \n 
parameter_values . append ( parameter_value ) \n 
~~~ parameter_value = { : key , : values } \n 
~~ def _api_to_objects_definition ( api_response ) : \n 
~~~ pipeline_objects = [ ] \n 
for element in api_response : \n 
~~~ current = { \n 
: element [ ] , \n 
: element [ ] \n 
for field in element [ ] : \n 
~~~ key = field [ ] \n 
if in field : \n 
~~~ value = field [ ] \n 
~~~ value = { : field [ ] } \n 
~~ _add_value ( key , value , current ) \n 
~~ pipeline_objects . append ( current ) \n 
~~ return pipeline_objects \n 
~~ def _api_to_parameters_definition ( api_response ) : \n 
~~~ parameter_objects = [ ] \n 
for attribute in element [ ] : \n 
~~~ _add_value ( attribute [ ] , attribute [ ] , current ) \n 
~~ parameter_objects . append ( current ) \n 
~~ def _api_to_values_definition ( api_response ) : \n 
~~~ pipeline_values = { } \n 
~~~ _add_value ( element [ ] , element [ ] , pipeline_values ) \n 
~~ return pipeline_values \n 
~~ def _add_value ( key , value , current_map ) : \n 
~~~ if key not in current_map : \n 
~~~ current_map [ key ] = value \n 
~~ elif isinstance ( current_map [ key ] , list ) : \n 
~~~ current_map [ key ] . append ( value ) \n 
~~~ converted_list = [ current_map [ key ] , value ] \n 
current_map [ key ] = converted_list \n 
__docformat__ = \n 
from api import * \n 
from enums import * \n 
from utils import * \n 
from conversion import * \n 
from client import * \n 
from user import * \n 
from call import * \n 
from profile import * \n 
from settings import * \n 
from chat import * \n 
from application import * \n 
from voicemail import * \n 
from sms import * \n 
from filetransfer import * \n 
class APINotifier ( SkypeAPINotifier ) : \n 
~~~ def __init__ ( self , skype ) : \n 
~~~ self . skype = weakref . proxy ( skype ) \n 
~~ def attachment_changed ( self , status ) : \n 
~~~ self . skype . _CallEventHandler ( , status ) \n 
if status == apiAttachRefused : \n 
~~~ raise SkypeAPIError ( ) \n 
~~ ~~ except weakref . ReferenceError : \n 
~~ ~~ def notification_received ( self , notification ) : \n 
~~~ skype = self . skype \n 
skype . _CallEventHandler ( , notification ) \n 
a , b = chop ( notification ) \n 
object_type = None \n 
if a in ( , , , , , , , ~~~ object_type , object_id , prop_name , value = [ a ] + chop ( b , 2 ) \n 
skype . _CacheDict [ str ( object_type ) , str ( object_id ) , str ( prop_name ) ] = value \n 
if object_type == : \n 
~~~ o = User ( skype , object_id ) \n 
if prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , str ( value ) ) \n 
~~ elif prop_name == or prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , value ) \n 
~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o ) \n 
~~ ~~ elif object_type == : \n 
~~~ o = Call ( skype , object_id ) \n 
~~~ skype . _CallEventHandler ( , o , ( value == ) ) \n 
~~~ o = Chat ( skype , object_id ) \n 
~~~ skype . _CallEventHandler ( , o , UserCollection ( skype , split ~~ if prop_name in ( , ) : \n 
~~~ skype . _CallEventHandler ( , o , ( prop_name == ) ) \n 
~~~ o = ChatMember ( skype , object_id ) \n 
~~~ o = ChatMessage ( skype , object_id ) \n 
~~~ o = Application ( skype , object_id ) \n 
~~~ skype . _CallEventHandler ( , o , UserCollection ( skype , split ~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ApplicationStreamCollection ~~ elif prop_name == : \n 
~~~ handle , text = chop ( value ) \n 
skype . _CallEventHandler ( , o , ApplicationStream ( o , handle ~~ elif prop_name == : \n 
~~~ skype . _CallEventHandler ( , o , ApplicationStreamCollection ~~ ~~ elif object_type == : \n 
~~~ o = Group ( skype , object_id ) \n 
~~~ skype . _CallEventHandler ( , o , int ( value ) ) \n 
~~~ o = SmsMessage ( skype , object_id ) \n 
~~~ for t in split ( value , ) : \n 
~~~ number , status = t . split ( ) \n 
skype . _CallEventHandler ( , SmsTarget ( o , number ) , ~~ ~~ ~~ elif object_type == : \n 
~~~ o = FileTransfer ( skype , object_id ) \n 
~~~ o = Voicemail ( skype , object_id ) \n 
~~ ~~ ~~ elif a in ( , ) : \n 
~~~ object_type , object_id , prop_name , value = [ a , ] + chop ( b ) \n 
~~ elif a in ( , , , , ~~~ object_type , object_id , prop_name , value = [ a , , , b ] \n 
~~~ skype . _CallEventHandler ( , value == ) \n 
~~ elif object_type == : \n 
~~~ skype . _CallEventHandler ( , str ( value ) ) \n 
~~~ skype . _CallEventHandler ( , ( value == ) ) \n 
~~ ~~ elif a == : \n 
~~~ skype . _CallEventHandler ( ) \n 
~~ elif a == : \n 
~~~ prop_name , value = chop ( b ) \n 
~~~ skype . _CallEventHandler ( , int ( value ) ) \n 
~~~ object_id , prop_name , value = chop ( b , 2 ) \n 
~~~ skype . _CallEventHandler ( , PluginEvent ( skype , object_id ) ) \n 
~~~ i = value . rfind ( ) \n 
if i >= 0 : \n 
~~~ context = chop ( value [ i + 8 : ] ) [ 0 ] \n 
users = ( ) \n 
context_id = \n 
if context in ( pluginContextContact , pluginContextCall , pluginContextChat ) : \n 
~~~ users = UserCollection ( skype , split ( value [ : i - 1 ] , ) ) \n 
~~ if context in ( pluginContextCall , pluginContextChat ) : \n 
~~~ j = value . rfind ( ) \n 
if j >= 0 : \n 
~~~ context_id = str ( chop ( value [ j + 11 : ] ) [ 0 ] ) \n 
if context == pluginContextCall : \n 
~~~ context_id = int ( context_id ) \n 
~~ ~~ ~~ skype . _CallEventHandler ( , PluginMenuItem ( skype , object_id ~~ ~~ ~~ elif a == : \n 
~~~ skype . _CallEventHandler ( , unicode2path ( b ) ) \n 
~~ ~~ def sending_command ( self , command ) : \n 
~~~ self . skype . _CallEventHandler ( , command ) \n 
~~ except weakref . ReferenceError : \n 
~~ ~~ def reply_received ( self , command ) : \n 
~~ ~~ ~~ class Skype ( EventHandlingBase ) : \n 
def __init__ ( self , Events = None , ** Options ) : \n 
self . _Logger = logging . getLogger ( ) \n 
self . _Logger . info ( ) \n 
EventHandlingBase . __init__ ( self ) \n 
if Events : \n 
~~~ self . _SetEventHandlerObject ( Events ) \n 
~~~ self . _Api = Options . pop ( ) \n 
if Options : \n 
~~~ self . _Api = SkypeAPI ( Options ) \n 
~~ self . _Api . set_notifier ( APINotifier ( self ) ) \n 
Cached . _CreateOwner ( self ) \n 
self . _Cache = True \n 
self . ResetCache ( ) \n 
from api import DEFAULT_TIMEOUT \n 
self . _Timeout = DEFAULT_TIMEOUT \n 
self . _Convert = Conversion ( self ) \n 
self . _Client = Client ( self ) \n 
self . _Settings = Settings ( self ) \n 
self . _Profile = Profile ( self ) \n 
~~ def __del__ ( self ) : \n 
if hasattr ( self , ) : \n 
~~~ self . _Api . close ( ) \n 
~~ self . _Logger . info ( ) \n 
~~ def _DoCommand ( self , Cmd , ExpectedReply = ) : \n 
~~~ command = Command ( Cmd , ExpectedReply , True , self . Timeout ) \n 
self . SendCommand ( command ) \n 
a , b = chop ( command . Reply ) \n 
if a == : \n 
~~~ errnum , errstr = chop ( b ) \n 
self . _CallEventHandler ( , command , int ( errnum ) , errstr ) \n 
raise SkypeError ( int ( errnum ) , errstr ) \n 
~~ if not command . Reply . startswith ( command . Expected ) : \n 
~~~ raise SkypeError ( 0 , % ( command . Reply , command . Expected ) ) \n 
~~ return command . Reply \n 
~~ def _Property ( self , ObjectType , ObjectId , PropName , Set = None , Cache = True ) : \n 
~~~ h = ( str ( ObjectType ) , str ( ObjectId ) , str ( PropName ) ) \n 
arg = ( % h ) . split ( ) \n 
while in arg : \n 
~~~ arg . remove ( ) \n 
~~ jarg = . join ( arg ) \n 
~~~ if Cache and self . _Cache and h in self . _CacheDict : \n 
~~~ return self . _CacheDict [ h ] \n 
~~ value = self . _DoCommand ( % jarg , jarg ) \n 
while arg : \n 
~~~ a , b = chop ( value ) \n 
~~ if a . lower ( ) != arg [ 0 ] . lower ( ) : \n 
~~ del arg [ 0 ] \n 
value = b \n 
~~ if Cache and self . _Cache : \n 
~~~ self . _CacheDict [ h ] = value \n 
~~~ value = unicode ( Set ) \n 
self . _DoCommand ( % ( jarg , value ) , jarg ) \n 
if Cache and self . _Cache : \n 
~~ ~~ ~~ def _Alter ( self , ObjectType , ObjectId , AlterName , Args = None , Reply = None ) : \n 
~~~ cmd = % ( str ( ObjectType ) , str ( ObjectId ) , str ( AlterName ) ) \n 
if Reply is None : \n 
~~~ Reply = cmd \n 
~~ if Args is not None : \n 
~~~ cmd = % ( cmd , tounicode ( Args ) ) \n 
~~ reply = self . _DoCommand ( cmd , Reply ) \n 
arg = cmd . split ( ) \n 
~~~ a , b = chop ( reply ) \n 
reply = b \n 
~~ return reply \n 
~~ def _Search ( self , ObjectType , Args = None ) : \n 
~~~ cmd = % ObjectType \n 
if Args is not None : \n 
~~~ cmd = % ( cmd , Args ) \n 
~~ return split ( chop ( str ( self . _DoCommand ( cmd ) ) ) [ - 1 ] , ) \n 
~~ def ApiSecurityContextEnabled ( self , Context ) : \n 
self . _Api . security_context_enabled ( Context ) \n 
~~ def Application ( self , Name ) : \n 
return Application ( self , Name ) \n 
~~ def _AsyncSearchUsersReplyHandler ( self , Command ) : \n 
~~~ if Command in self . _AsyncSearchUsersCommands : \n 
~~~ self . _AsyncSearchUsersCommands . remove ( Command ) \n 
self . _CallEventHandler ( , Command . Id , \n 
UserCollection ( self , split ( chop ( Command . Reply ) [ - 1 ] , ) ) ) \n 
if len ( self . _AsyncSearchUsersCommands ) == 0 : \n 
~~~ self . UnregisterEventHandler ( , self . _AsyncSearchUsersReplyHandler ) \n 
del self . _AsyncSearchUsersCommands \n 
~~ ~~ ~~ def AsyncSearchUsers ( self , Target ) : \n 
if not hasattr ( self , ) : \n 
~~~ self . _AsyncSearchUsersCommands = [ ] \n 
self . RegisterEventHandler ( , self . _AsyncSearchUsersReplyHandler ) \n 
~~ command = Command ( % tounicode ( Target ) , , False , self . Timeout ) \n 
self . _AsyncSearchUsersCommands . append ( command ) \n 
return command . Id \n 
~~ def Attach ( self , Protocol = 5 , Wait = True ) : \n 
~~~ self . _Api . protocol = Protocol \n 
self . _Api . attach ( self . Timeout , Wait ) \n 
~~ except SkypeAPIError : \n 
~~~ self . ResetCache ( ) \n 
~~ ~~ def Call ( self , Id = 0 ) : \n 
o = Call ( self , Id ) \n 
return o \n 
~~ def Calls ( self , Target = ) : \n 
return CallCollection ( self , self . _Search ( , Target ) ) \n 
~~ def _ChangeUserStatus_UserStatus ( self , Status ) : \n 
~~~ if Status . upper ( ) == self . _ChangeUserStatus_Status : \n 
~~~ self . _ChangeUserStatus_Event . set ( ) \n 
~~ ~~ def ChangeUserStatus ( self , Status ) : \n 
if self . CurrentUserStatus . upper ( ) == Status . upper ( ) : \n 
~~ self . _ChangeUserStatus_Event = threading . Event ( ) \n 
self . _ChangeUserStatus_Status = Status . upper ( ) \n 
self . RegisterEventHandler ( , self . _ChangeUserStatus_UserStatus ) \n 
self . CurrentUserStatus = Status \n 
self . _ChangeUserStatus_Event . wait ( ) \n 
self . UnregisterEventHandler ( , self . _ChangeUserStatus_UserStatus ) \n 
del self . _ChangeUserStatus_Event , self . _ChangeUserStatus_Status \n 
~~ def Chat ( self , Name = ) : \n 
o = Chat ( self , Name ) \n 
~~ def ClearCallHistory ( self , Username = , Type = chsAllCalls ) : \n 
cmd = % ( str ( Type ) , Username ) \n 
self . _DoCommand ( cmd , cmd ) \n 
~~ def ClearChatHistory ( self ) : \n 
cmd = \n 
~~ def ClearVoicemailHistory ( self ) : \n 
self . _DoCommand ( ) \n 
~~ def Command ( self , Command , Reply = , Block = False , Timeout = 30000 , Id = - 1 ) : \n 
from api import Command as CommandClass \n 
return CommandClass ( Command , Reply , Block , Timeout , Id ) \n 
~~ def Conference ( self , Id = 0 ) : \n 
o = Conference ( self , Id ) \n 
if Id <= 0 or not o . Calls : \n 
~~~ raise SkypeError ( 0 , ) \n 
~~ return o \n 
~~ def CreateChatUsingBlob ( self , Blob ) : \n 
return Chat ( self , chop ( self . _DoCommand ( % Blob ) , 2 ) [ 1 ] ) \n 
~~ def CreateChatWith ( self , * Usernames ) : \n 
return Chat ( self , chop ( self . _DoCommand ( % . join ( Usernames ) ) , 2 ) [ 1 ] ) \n 
~~ def CreateGroup ( self , GroupName ) : \n 
groups = self . CustomGroups \n 
self . _DoCommand ( % tounicode ( GroupName ) ) \n 
for g in self . CustomGroups : \n 
~~~ if g not in groups and g . DisplayName == GroupName : \n 
~~~ return g \n 
~~ ~~ raise SkypeError ( 0 , ) \n 
~~ def CreateSms ( self , MessageType , * TargetNumbers ) : \n 
return SmsMessage ( self , chop ( self . _DoCommand ( % ( MessageType , . join ( TargetNumbers \n 
~~ def DeleteGroup ( self , GroupId ) : \n 
self . _DoCommand ( % GroupId ) \n 
~~ def EnableApiSecurityContext ( self , Context ) : \n 
self . _Api . enable_security_context ( Context ) \n 
~~ def FindChatUsingBlob ( self , Blob ) : \n 
~~ def Greeting ( self , Username = ) : \n 
for v in self . Voicemails : \n 
~~~ if Username and v . PartnerHandle != Username : \n 
~~ if v . Type in ( vmtDefaultGreeting , vmtCustomGreeting ) : \n 
~~~ return v \n 
~~ ~~ ~~ def Message ( self , Id = 0 ) : \n 
o = ChatMessage ( self , Id ) \n 
~~ def Messages ( self , Target = ) : \n 
return ChatMessageCollection ( self , self . _Search ( , Target ) ) \n 
~~ def PlaceCall ( self , * Targets ) : \n 
calls = self . ActiveCalls \n 
reply = self . _DoCommand ( % . join ( Targets ) ) \n 
if reply . startswith ( ) : \n 
~~~ return Call ( self , chop ( reply , 2 ) [ 1 ] ) \n 
~~ for c in self . ActiveCalls : \n 
~~~ if c not in calls : \n 
~~~ return c \n 
~~ def Privilege ( self , Name ) : \n 
return ( self . _Property ( , , Name . upper ( ) ) == ) \n 
~~ def Profile ( self , Property , Set = None ) : \n 
return self . _Property ( , , Property , Set ) \n 
~~ def Property ( self , ObjectType , ObjectId , PropName , Set = None ) : \n 
return self . _Property ( ObjectType , ObjectId , PropName , Set ) \n 
~~ def ResetCache ( self ) : \n 
self . _CacheDict = { } \n 
~~ def SearchForUsers ( self , Target ) : \n 
return UserCollection ( self , self . _Search ( , tounicode ( Target ) ) ) \n 
~~ def SendCommand ( self , Command ) : \n 
~~~ self . _Api . send_command ( Command ) \n 
~~ ~~ def SendMessage ( self , Username , Text ) : \n 
return self . CreateChatWith ( Username ) . SendMessage ( Text ) \n 
~~ def SendSms ( self , * TargetNumbers , ** Properties ) : \n 
sms = self . CreateSms ( smsMessageTypeOutgoing , * TargetNumbers ) \n 
for name , value in Properties . items ( ) : \n 
~~~ if isinstance ( getattr ( sms . __class__ , name , None ) , property ) : \n 
~~~ setattr ( sms , name , value ) \n 
~~~ raise TypeError ( % prop ) \n 
~~ ~~ sms . Send ( ) \n 
return sms \n 
~~ def SendVoicemail ( self , Username ) : \n 
if self . _Api . protocol >= 6 : \n 
~~~ self . _DoCommand ( % Username ) \n 
~~ ~~ def User ( self , Username = ) : \n 
if not Username : \n 
~~~ Username = self . CurrentUserHandle \n 
~~ o = User ( self , Username ) \n 
~~ def Variable ( self , Name , Set = None ) : \n 
return self . _Property ( Name , , , Set ) \n 
~~ def Voicemail ( self , Id ) : \n 
o = Voicemail ( self , Id ) \n 
~~ def _GetActiveCalls ( self ) : \n 
~~~ return CallCollection ( self , self . _Search ( ) ) \n 
~~ ActiveCalls = property ( _GetActiveCalls , \n 
def _GetActiveChats ( self ) : \n 
~~~ return ChatCollection ( self , self . _Search ( ) ) \n 
~~ ActiveChats = property ( _GetActiveChats , \n 
def _GetActiveFileTransfers ( self ) : \n 
~~~ return FileTransferCollection ( self , self . _Search ( ) ) \n 
~~ ActiveFileTransfers = property ( _GetActiveFileTransfers , \n 
def _GetApiWrapperVersion ( self ) : \n 
~~~ import pkg_resources \n 
return pkg_resources . get_distribution ( "Skype4Py" ) . version \n 
~~ ApiWrapperVersion = property ( _GetApiWrapperVersion , \n 
def _GetAttachmentStatus ( self ) : \n 
~~~ return self . _Api . attachment_status \n 
~~ AttachmentStatus = property ( _GetAttachmentStatus , \n 
def _GetBookmarkedChats ( self ) : \n 
~~ BookmarkedChats = property ( _GetBookmarkedChats , \n 
def _GetCache ( self ) : \n 
~~~ return self . _Cache \n 
~~ def _SetCache ( self , Value ) : \n 
~~~ self . _Cache = bool ( Value ) \n 
~~ Cache = property ( _GetCache , _SetCache , \n 
def _GetChats ( self ) : \n 
~~ Chats = property ( _GetChats , \n 
def _GetClient ( self ) : \n 
~~~ return self . _Client \n 
~~ Client = property ( _GetClient , \n 
def _GetCommandId ( self ) : \n 
~~ def _SetCommandId ( self , Value ) : \n 
~~~ if not Value : \n 
~~ ~~ CommandId = property ( _GetCommandId , _SetCommandId , \n 
def _GetConferences ( self ) : \n 
~~~ cids = [ ] \n 
for c in self . Calls ( ) : \n 
~~~ cid = c . ConferenceId \n 
if cid > 0 and cid not in cids : \n 
~~~ cids . append ( cid ) \n 
~~ ~~ return ConferenceCollection ( self , cids ) \n 
~~ Conferences = property ( _GetConferences , \n 
def _GetConnectionStatus ( self ) : \n 
~~~ return str ( self . Variable ( ) ) \n 
~~ ConnectionStatus = property ( _GetConnectionStatus , \n 
def _GetConvert ( self ) : \n 
~~~ return self . _Convert \n 
~~ Convert = property ( _GetConvert , \n 
def _GetCurrentUser ( self ) : \n 
~~~ return User ( self , self . CurrentUserHandle ) \n 
~~ CurrentUser = property ( _GetCurrentUser , \n 
def _GetCurrentUserHandle ( self ) : \n 
~~ CurrentUserHandle = property ( _GetCurrentUserHandle , \n 
def _GetCurrentUserProfile ( self ) : \n 
~~~ return self . _Profile \n 
~~ CurrentUserProfile = property ( _GetCurrentUserProfile , \n 
def _GetCurrentUserStatus ( self ) : \n 
~~ def _SetCurrentUserStatus ( self , Value ) : \n 
~~~ self . Variable ( , str ( Value ) ) \n 
~~ CurrentUserStatus = property ( _GetCurrentUserStatus , _SetCurrentUserStatus , \n 
def _GetCustomGroups ( self ) : \n 
~~~ return GroupCollection ( self , self . _Search ( , ) ) \n 
~~ CustomGroups = property ( _GetCustomGroups , \n 
def _GetFileTransfers ( self ) : \n 
~~ FileTransfers = property ( _GetFileTransfers , \n 
def _GetFocusedContacts ( self ) : \n 
~~~ return UserCollection ( self , split ( chop ( self . _DoCommand ( , \n 
~~ FocusedContacts = property ( _GetFocusedContacts , \n 
def _GetFriendlyName ( self ) : \n 
~~~ return self . _Api . friendly_name \n 
~~ def _SetFriendlyName ( self , Value ) : \n 
~~~ self . _Api . set_friendly_name ( tounicode ( Value ) ) \n 
~~ FriendlyName = property ( _GetFriendlyName , _SetFriendlyName , \n 
def _GetFriends ( self ) : \n 
~~~ return UserCollection ( self , self . _Search ( ) ) \n 
~~ Friends = property ( _GetFriends , \n 
def _GetGroups ( self ) : \n 
~~ Groups = property ( _GetGroups , \n 
def _GetHardwiredGroups ( self ) : \n 
~~ HardwiredGroups = property ( _GetHardwiredGroups , \n 
def _GetMissedCalls ( self ) : \n 
~~ MissedCalls = property ( _GetMissedCalls , \n 
def _GetMissedChats ( self ) : \n 
~~ MissedChats = property ( _GetMissedChats , \n 
def _GetMissedMessages ( self ) : \n 
~~~ return ChatMessageCollection ( self , self . _Search ( ) ) \n 
~~ MissedMessages = property ( _GetMissedMessages , \n 
def _GetMissedSmss ( self ) : \n 
~~~ return SmsMessageCollection ( self , self . _Search ( ) ) \n 
~~ MissedSmss = property ( _GetMissedSmss , \n 
def _GetMissedVoicemails ( self ) : \n 
~~~ return VoicemailCollection ( self , self . _Search ( ) ) \n 
~~ MissedVoicemails = property ( _GetMissedVoicemails , \n 
def _GetMute ( self ) : \n 
~~~ return self . Variable ( ) == \n 
~~ def _SetMute ( self , Value ) : \n 
~~~ self . Variable ( , cndexp ( Value , , ) ) \n 
~~ Mute = property ( _GetMute , _SetMute , \n 
def _GetPredictiveDialerCountry ( self ) : \n 
~~ PredictiveDialerCountry = property ( _GetPredictiveDialerCountry , \n 
def _GetProtocol ( self ) : \n 
~~~ return self . _Api . protocol \n 
~~ def _SetProtocol ( self , Value ) : \n 
~~~ self . _DoCommand ( % Value ) \n 
self . _Api . protocol = int ( Value ) \n 
~~ Protocol = property ( _GetProtocol , _SetProtocol , \n 
def _GetRecentChats ( self ) : \n 
~~ RecentChats = property ( _GetRecentChats , \n 
def _GetSettings ( self ) : \n 
~~~ return self . _Settings \n 
~~ Settings = property ( _GetSettings , \n 
def _GetSilentMode ( self ) : \n 
~~~ return self . _Property ( , , , Cache = False ) == \n 
~~ def _SetSilentMode ( self , Value ) : \n 
~~~ self . _Property ( , , , cndexp ( Value , , ) , Cache = False ) \n 
~~ SilentMode = property ( _GetSilentMode , _SetSilentMode , \n 
def _GetSmss ( self ) : \n 
~~ Smss = property ( _GetSmss , \n 
def _GetTimeout ( self ) : \n 
~~~ return self . _Timeout \n 
~~ def _SetTimeout ( self , Value ) : \n 
~~~ if not isinstance ( Value , ( int , long , float ) ) : \n 
~~~ raise TypeError ( % repr ( type ( Value ) ) ) \n 
~~ self . _Timeout = Value \n 
~~ Timeout = property ( _GetTimeout , _SetTimeout , \n 
def _GetUsersWaitingAuthorization ( self ) : \n 
~~ UsersWaitingAuthorization = property ( _GetUsersWaitingAuthorization , \n 
def _GetVersion ( self ) : \n 
~~ Version = property ( _GetVersion , \n 
def _GetVoicemails ( self ) : \n 
~~ Voicemails = property ( _GetVoicemails , \n 
~~ class SkypeEvents ( object ) : \n 
def ApplicationConnecting ( self , App , Users ) : \n 
~~ def ApplicationDatagram ( self , App , Stream , Text ) : \n 
~~ def ApplicationReceiving ( self , App , Streams ) : \n 
~~ def ApplicationSending ( self , App , Streams ) : \n 
~~ def ApplicationStreams ( self , App , Streams ) : \n 
~~ def AsyncSearchUsersFinished ( self , Cookie , Users ) : \n 
~~ def AttachmentStatus ( self , Status ) : \n 
~~ def AutoAway ( self , Automatic ) : \n 
~~ def CallDtmfReceived ( self , Call , Code ) : \n 
~~ def CallHistory ( self ) : \n 
~~ def CallInputStatusChanged ( self , Call , Active ) : \n 
~~ def CallSeenStatusChanged ( self , Call , Seen ) : \n 
~~ def CallStatus ( self , Call , Status ) : \n 
~~ def CallTransferStatusChanged ( self , Call , Status ) : \n 
~~ def CallVideoReceiveStatusChanged ( self , Call , Status ) : \n 
~~ def CallVideoSendStatusChanged ( self , Call , Status ) : \n 
~~ def CallVideoStatusChanged ( self , Call , Status ) : \n 
~~ def ChatMemberRoleChanged ( self , Member , Role ) : \n 
~~ def ChatMembersChanged ( self , Chat , Members ) : \n 
~~ def ChatWindowState ( self , Chat , State ) : \n 
~~ def ClientWindowState ( self , State ) : \n 
~~ def Command ( self , command ) : \n 
~~ def ConnectionStatus ( self , Status ) : \n 
~~ def ContactsFocused ( self , Username ) : \n 
~~ def Error ( self , command , Number , Description ) : \n 
~~ def FileTransferStatusChanged ( self , Transfer , Status ) : \n 
~~ def GroupDeleted ( self , GroupId ) : \n 
~~ def GroupExpanded ( self , Group , Expanded ) : \n 
~~ def GroupUsers ( self , Group , Count ) : \n 
~~ def GroupVisible ( self , Group , Visible ) : \n 
~~ def MessageHistory ( self , Username ) : \n 
~~ def MessageStatus ( self , Message , Status ) : \n 
~~ def Mute ( self , Mute ) : \n 
~~ def Notify ( self , Notification ) : \n 
~~ def OnlineStatus ( self , User , Status ) : \n 
~~ def PluginEventClicked ( self , Event ) : \n 
~~ def PluginMenuItemClicked ( self , MenuItem , Users , PluginContext , ContextId ) : \n 
~~ def Reply ( self , command ) : \n 
~~ def SilentModeStatusChanged ( self , Silent ) : \n 
~~ def SmsMessageStatusChanged ( self , Message , Status ) : \n 
~~ def SmsTargetStatusChanged ( self , Target , Status ) : \n 
~~ def UserAuthorizationRequestReceived ( self , User ) : \n 
~~ def UserMood ( self , User , MoodText ) : \n 
~~ def UserStatus ( self , Status ) : \n 
~~ def VoicemailStatus ( self , Mail , Status ) : \n 
~~ def WallpaperChanged ( self , Path ) : \n 
~~ ~~ Skype . _AddEvents ( SkypeEvents ) \n 
import skype4pytest \n 
from Skype4Py . sms import * \n 
class SmsMessageTest ( skype4pytest . TestCase ) : \n 
~~~ def setUpObject ( self ) : \n 
~~~ self . obj = SmsMessage ( self . skype , ) \n 
~~ def testDelete ( self ) : \n 
~~~ self . api . enqueue ( ) \n 
self . obj . Delete ( ) \n 
self . failUnless ( self . api . is_empty ( ) ) \n 
~~ def testMarkAsSeen ( self ) : \n 
~~~ self . api . enqueue ( , \n 
self . obj . MarkAsSeen ( ) \n 
~~ def testSend ( self ) : \n 
self . obj . Send ( ) \n 
~~ def testBody ( self ) : \n 
t = self . obj . Body \n 
self . assertInstance ( t , unicode ) \n 
self . assertEqual ( t , ) \n 
self . api . enqueue ( , \n 
self . obj . Body = \n 
~~ def testChunks ( self ) : \n 
t = self . obj . Chunks \n 
self . assertInstance ( t , SmsChunkCollection ) \n 
self . assertEqual ( len ( t ) , 2 ) \n 
~~ def testDatetime ( self ) : \n 
~~~ from datetime import datetime \n 
now = time ( ) \n 
% now ) \n 
t = self . obj . Datetime \n 
self . assertInstance ( t , datetime ) \n 
self . assertEqual ( t , datetime . fromtimestamp ( now ) ) \n 
~~ def testFailureReason ( self ) : \n 
t = self . obj . FailureReason \n 
self . assertInstance ( t , str ) \n 
~~ def testId ( self ) : \n 
~~~ t = self . obj . Id \n 
self . assertInstance ( t , int ) \n 
self . assertEqual ( t , 1234 ) \n 
~~ def testIsFailedUnseen ( self ) : \n 
t = self . obj . IsFailedUnseen \n 
self . assertInstance ( t , bool ) \n 
self . assertEqual ( t , True ) \n 
~~ def testPrice ( self ) : \n 
t = self . obj . Price \n 
self . assertEqual ( t , 123 ) \n 
~~ def testPriceCurrency ( self ) : \n 
t = self . obj . PriceCurrency \n 
~~ def testPricePrecision ( self ) : \n 
t = self . obj . PricePrecision \n 
self . assertEqual ( t , 3 ) \n 
~~ def testPriceToText ( self ) : \n 
t = self . obj . PriceToText \n 
~~ def testPriceValue ( self ) : \n 
t = self . obj . PriceValue \n 
self . assertInstance ( t , float ) \n 
self . assertEqual ( t , 0.123 ) \n 
~~ def testReplyToNumber ( self ) : \n 
t = self . obj . ReplyToNumber \n 
self . obj . ReplyToNumber = \n 
~~ def testSeen ( self ) : \n 
~~~ from warnings import simplefilter \n 
simplefilter ( ) \n 
~~~ self . obj . Seen = True \n 
~~~ simplefilter ( ) \n 
~~ self . failUnless ( self . api . is_empty ( ) ) \n 
~~ def testStatus ( self ) : \n 
t = self . obj . Status \n 
~~ def testTargetNumbers ( self ) : \n 
t = self . obj . TargetNumbers \n 
self . assertInstance ( t , tuple ) \n 
self . obj . TargetNumbers = ( , ) \n 
~~ def testTargets ( self ) : \n 
t = self . obj . Targets \n 
self . assertInstance ( t , SmsTargetCollection ) \n 
~~ def testTimestamp ( self ) : \n 
t = self . obj . Timestamp \n 
self . assertEqual ( t , 123.4 ) \n 
~~ def testType ( self ) : \n 
t = self . obj . Type \n 
~~ ~~ class SmsChunkTest ( skype4pytest . TestCase ) : \n 
~~~ self . obj = SmsChunk ( SmsMessage ( self . skype , ) , 1 ) \n 
~~ def testCharactersLeft ( self ) : \n 
t = self . obj . CharactersLeft \n 
self . assertEqual ( t , 30 ) \n 
self . assertEqual ( t , 1 ) \n 
~~ def testMessage ( self ) : \n 
~~~ t = self . obj . Message \n 
self . assertInstance ( t , SmsMessage ) \n 
self . assertEqual ( t . Id , 1234 ) \n 
~~ def testText ( self ) : \n 
t = self . obj . Text \n 
~~ ~~ class SmsTargetTest ( skype4pytest . TestCase ) : \n 
~~~ self . obj = SmsTarget ( SmsMessage ( self . skype , ) , ) \n 
~~ def testNumber ( self ) : \n 
~~~ t = self . obj . Number \n 
~~ ~~ def suite ( ) : \n 
~~~ return unittest . TestSuite ( [ \n 
unittest . defaultTestLoader . loadTestsFromTestCase ( SmsMessageTest ) , \n 
unittest . defaultTestLoader . loadTestsFromTestCase ( SmsChunkTest ) , \n 
unittest . defaultTestLoader . loadTestsFromTestCase ( SmsTargetTest ) , \n 
~~ from . import convs , widgets , fields \n 
from iktomi . utils . i18n import N_ \n 
class PasswordConv ( convs . Char ) : \n 
~~~ error_mismatch = N_ ( ) \n 
error_required = N_ ( ) \n 
def from_python ( self , value ) : \n 
~~~ return dict ( [ ( field . name , None ) for field in self . field . fields ] ) \n 
~~ def get_initial ( self ) : \n 
~~ def to_python ( self , value ) : \n 
~~~ etalon = value [ list ( value ) [ 0 ] ] \n 
for field in self . field . fields : \n 
~~~ self . assert_ ( value [ field . name ] == etalon , \n 
self . error_mismatch ) \n 
~~ if self . required : \n 
~~~ self . assert_ ( etalon not in ( None , ) , self . error_required ) \n 
~~ elif etalon in ( None , ) : \n 
~~ return etalon \n 
~~ ~~ def PasswordSet ( name = , \n 
min_length = 3 , max_length = 200 , required = False , \n 
password_label = None , confirm_label = , filters = ( ) , \n 
~~~ char = convs . Char ( convs . length ( min_length , max_length ) , * filters , \n 
** dict ( required = required ) ) \n 
items = ( ( , password_label ) , ( , confirm_label ) ) \n 
kwargs [ ] = [ fields . Field ( subfieldname , \n 
conv = char , \n 
label = label , \n 
widget = widgets . PasswordInput ) \n 
for subfieldname , label in items ] \n 
kwargs . setdefault ( , PasswordConv ( required = required ) ) \n 
kwargs . setdefault ( , widgets . FieldSetWidget ( \n 
template = ) ) \n 
return fields . FieldSet ( name , get_initial = lambda : , ** kwargs ) \n 
~~ __all__ = [ , ] \n 
from iktomi . storage import LocalMemStorage , MemcachedStorage \n 
class LocalMemStorageTest ( unittest . TestCase ) : \n 
~~~ def test_set ( self ) : \n 
s = LocalMemStorage ( ) \n 
s . set ( , ) \n 
self . assertEqual ( s . storage [ ] , ) \n 
~~ def test_set_rewrite ( self ) : \n 
~~ def test_get ( self ) : \n 
self . assertEqual ( s . get ( ) , None ) \n 
self . assertEqual ( s . get ( , ) , ) \n 
self . assertEqual ( s . get ( ) , ) \n 
~~ def test_delete ( self ) : \n 
self . assertEqual ( s . delete ( ) , True ) \n 
~~ ~~ class MemcachedStorageTest ( unittest . TestCase ) : \n 
~~~ self . storage = MemcachedStorage ( ) \n 
if not self . storage . storage . set ( , ) : \n 
~~ ~~ def tearDown ( self ) : \n 
~~~ memcached = self . storage . storage \n 
memcached . delete ( ) \n 
memcached . disconnect_all ( ) \n 
~~ def test_set ( self ) : \n 
self . assertEqual ( self . storage . set ( , ) , True ) \n 
self . assertEqual ( self . storage . get ( ) , None ) \n 
self . assertEqual ( self . storage . get ( , ) , ) \n 
self . storage . set ( , ) \n 
self . assertEqual ( self . storage . get ( ) , ) \n 
self . assertEqual ( self . storage . delete ( ) , True ) \n 
name = "SmartlingApiSdk" , \n 
version = "1.2.5" , \n 
author_email = "aartamonov@smartling.com" , \n 
url = "https://docs.smartling.com/display/docs/Files+API" , \n 
packages = [ , "simplejson24" , "example" , "test" ] , \n 
package_data = { \n 
import gc \n 
from datetime import datetime , date , timedelta \n 
from optparse import make_option \n 
from django . core . files . storage import get_storage_class \n 
from django . core . management . base import BaseCommand \n 
from easy_thumbnails . conf import settings \n 
from easy_thumbnails . models import Source \n 
class ThumbnailCollectionCleaner ( object ) : \n 
sources = 0 \n 
thumbnails = 0 \n 
thumbnails_deleted = 0 \n 
source_refs_deleted = 0 \n 
execution_time = 0 \n 
def _get_absolute_path ( self , path ) : \n 
~~~ return os . path . join ( settings . MEDIA_ROOT , path ) \n 
~~ def _get_relative_path ( self , path ) : \n 
~~~ return os . path . relpath ( path , settings . MEDIA_ROOT ) \n 
~~ def _check_if_exists ( self , storage , path ) : \n 
~~~ return storage . exists ( path ) \n 
print ( str ( e ) ) \n 
~~ ~~ def _delete_sources_by_id ( self , ids ) : \n 
~~~ Source . objects . all ( ) . filter ( id__in = ids ) . delete ( ) \n 
~~ def clean_up ( self , dry_run = False , verbosity = 1 , last_n_days = 0 , \n 
cleanup_path = None , storage = None ) : \n 
if dry_run : \n 
~~ if not storage : \n 
~~~ storage = get_storage_class ( settings . THUMBNAIL_DEFAULT_STORAGE ) ( ) \n 
~~ sources_to_delete = [ ] \n 
time_start = time . time ( ) \n 
query = Source . objects . all ( ) \n 
if last_n_days > 0 : \n 
~~~ today = date . today ( ) \n 
query = query . filter ( \n 
modified__range = ( today - timedelta ( days = last_n_days ) , today ) ) \n 
~~ if cleanup_path : \n 
~~~ query = query . filter ( name__startswith = cleanup_path ) \n 
~~ for source in queryset_iterator ( query ) : \n 
~~~ self . sources += 1 \n 
abs_source_path = self . _get_absolute_path ( source . name ) \n 
if not self . _check_if_exists ( storage , abs_source_path ) : \n 
~~~ if verbosity > 0 : \n 
~~ self . source_refs_deleted += 1 \n 
sources_to_delete . append ( source . id ) \n 
for thumb in source . thumbnails . all ( ) : \n 
~~~ self . thumbnails_deleted += 1 \n 
abs_thumbnail_path = self . _get_absolute_path ( thumb . name ) \n 
if self . _check_if_exists ( storage , abs_thumbnail_path ) : \n 
~~~ if not dry_run : \n 
~~~ storage . delete ( abs_thumbnail_path ) \n 
~~ if verbosity > 0 : \n 
~~ ~~ ~~ ~~ if len ( sources_to_delete ) >= 1000 and not dry_run : \n 
~~~ self . _delete_sources_by_id ( sources_to_delete ) \n 
sources_to_delete = [ ] \n 
~~ ~~ if not dry_run : \n 
~~ self . execution_time = round ( time . time ( ) - time_start ) \n 
~~ def print_stats ( self ) : \n 
print ( \n 
"{0:-<48}" . format ( str ( datetime . now ( ) . strftime ( ) ) ) ) \n 
self . thumbnails_deleted ) ) \n 
~~ ~~ def queryset_iterator ( queryset , chunksize = 1000 ) : \n 
primary_key = 0 \n 
last_pk = queryset . order_by ( ) [ 0 ] . pk \n 
queryset = queryset . order_by ( ) \n 
while primary_key < last_pk : \n 
~~~ for row in queryset . filter ( pk__gt = primary_key ) [ : chunksize ] : \n 
~~~ primary_key = row . pk \n 
yield row \n 
~~ gc . collect ( ) \n 
~~ ~~ class Command ( BaseCommand ) : \n 
option_list = BaseCommand . option_list + ( \n 
make_option ( \n 
dest = , \n 
help = ) , \n 
default = 0 , \n 
type = , \n 
def handle ( self , * args , ** options ) : \n 
~~~ tcc = ThumbnailCollectionCleaner ( ) \n 
tcc . clean_up ( \n 
dry_run = options . get ( , False ) , \n 
verbosity = int ( options . get ( , 1 ) ) , \n 
last_n_days = int ( options . get ( , 0 ) ) , \n 
cleanup_path = options . get ( ) ) \n 
tcc . print_stats ( ) \n 
~~~ db . delete_unique ( , [ , ] ) \n 
db . create_unique ( , [ , , ] ) \n 
~~~ db . delete_unique ( , [ , , ] ) \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , } , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : "\'thumbnails\'" : ( , [ ] , { : , } \n 
__credits__ = [ ] \n 
] from django . contrib . auth . models import AbstractBaseUser , PermissionsMixin \n 
from django . contrib . auth . models import UserManager \n 
from django . contrib . flatpages . models import FlatPage \n 
from django . db . models import signals \n 
from django . templatetags . static import static \n 
from django . utils import timezone \n 
from allauth . account . models import EmailAddress \n 
from django_countries . fields import CountryField \n 
from skills . models import Skill , TrainingBit \n 
class CustomUserManager ( UserManager ) : \n 
~~~ def create_superuser ( self , username , password , ** kwargs ) : \n 
~~~ user = self . model ( username = username , is_staff = True , is_superuser = True , ** kwargs ) \n 
user . set_password ( password ) \n 
return user \n 
~~ ~~ class User ( AbstractBaseUser , PermissionsMixin ) : \n 
~~~ USERNAME_FIELD = \n 
datetime_joined = models . DateTimeField ( default = timezone . now ) \n 
username = models . CharField ( max_length = 40 , unique = True , error_messages = { \n 
email = models . CharField ( max_length = 100 , blank = True ) \n 
image = models . ImageField ( upload_to = , null = True , blank = True ) \n 
description = models . TextField ( blank = True ) \n 
skills_in_progress = models . ManyToManyField ( Skill , blank = True , related_name = ) skills_completed = models . ManyToManyField ( Skill , blank = True , related_name = ) \n 
trainingbits_in_progress = models . ManyToManyField ( TrainingBit , blank = True , related_name = trainingbits_completed = models . ManyToManyField ( TrainingBit , blank = True , related_name = \n 
is_active = models . BooleanField ( default = True ) \n 
has_been_welcomed = models . BooleanField ( default = False ) \n 
def is_taking_skill ( self , skill ) : \n 
~~~ return skill in self . skills_in_progress . all ( ) \n 
~~ def complete_skill ( self , completed_skill ) : \n 
~~~ skills = self . complete_skills ( [ complete_skill ] ) \n 
return skills [ 0 ] \n 
~~ def complete_skills ( self , completed_skills ) : \n 
~~~ self . skills_in_progress . remove ( * completed_skills ) \n 
self . skills_completed . add ( * completed_skills ) \n 
return completed_skills \n 
~~ def has_completed_skill ( self , skill ) : \n 
~~~ return skill in self . skills_completed . all ( ) \n 
~~ def is_taking_trainingbit ( self , trainingbit ) : \n 
~~~ return trainingbit in self . trainingbits_in_progress . all ( ) \n 
~~ def complete_trainingbit ( self , completed_trainingbit ) : \n 
~~~ tbs = self . complete_trainingbits ( [ completed_trainingbit ] ) \n 
return tbs [ 0 ] \n 
~~ def complete_trainingbits ( self , completed_trainingbits ) : \n 
~~~ self . trainingbits_in_progress . remove ( * completed_trainingbits ) \n 
self . trainingbits_completed . add ( * completed_trainingbits ) \n 
return completed_trainingbits \n 
~~ def has_completed_trainingbit ( self , trainingbit ) : \n 
~~~ return trainingbit in self . trainingbits_completed . all ( ) \n 
~~ def get_full_name ( self ) : \n 
~~~ return self . email \n 
~~ def get_short_name ( self ) : \n 
~~ objects = CustomUserManager ( ) \n 
def account_verified ( self ) : \n 
~~~ if self . user . is_authenticated : \n 
~~~ result = EmailAddress . objects . get_primary ( self . user ) \n 
if len ( result ) : \n 
~~~ return result [ 0 ] . verified \n 
def is_trainer ( self ) : \n 
~~~ if self . is_admin : \n 
~~ return self . groups . filter ( name = ) \n 
def is_admin ( self ) : \n 
~~~ return self . groups . filter ( name = ) or self . is_superuser \n 
def is_staff ( self ) : \n 
return self . is_admin or self . is_trainer \n 
~~ def get_absolute_url ( self ) : \n 
~~~ return reverse ( , args = [ self . id ] ) \n 
~~ def getImage ( self ) : \n 
~~~ if self . image : \n 
~~~ return self . image . url \n 
~~~ return static ( ) \n 
~~ ~~ ~~ class UserInfo ( models . Model ) : \n 
~~~ SEXES = [ \n 
ORGANISATION_TYPES = [ \n 
sex = models . CharField ( max_length = 20 , choices = SEXES , blank = False ) \n 
country = CountryField ( null = True ) \n 
birthdate = models . DateField ( null = True ) \n 
organization = models . CharField ( max_length = 15 , choices = ORGANISATION_TYPES , blank = False ) \n 
user = models . OneToOneField ( User , null = True , blank = True ) \n 
~~ def get_or_create_userinfo ( user ) : \n 
userinfo , c = UserInfo . objects . get_or_create ( user = user ) \n 
return userinfo \n 
~~ User . userinfo = property ( get_or_create_userinfo ) \n 
class GCLFlatPage ( FlatPage ) : \n 
~~~ show_in_footer = models . BooleanField ( default = False ) \n 
class Meta : \n 
~~~ return reverse ( , args = [ self . url ] ) \n 
~~ ~~ from solo . models import SingletonModel \n 
class SiteConfiguration ( SingletonModel ) : \n 
~~~ analytics_code = models . TextField ( help_text = \n 
~~ class Meta : \n 
~~ ~~ from south . utils import datetime_utils as datetime \n 
from django . utils . text import slugify \n 
~~~ if not db . dry_run : \n 
~~~ for _class in [ orm . Skill , orm . Project , orm . TrainingBit ] : \n 
~~~ for obj in _class . objects . all ( ) : \n 
~~~ objs_with_slug = _class . objects . filter ( slug__exact = obj . slug ) \n 
if obj . slug == : \n 
~~~ obj . slug = slugify ( obj . name ) \n 
~~~ obj . slug = \n 
~~ ~~ if objs_with_slug . count ( ) > 1 : \n 
~~~ existing_slugs = _class . objects . filter ( slug__regex = + obj . slug + ) . values_list ( , flat = True ) \n 
if len ( existing_slugs ) > 0 : \n 
~~~ last_existing_slug = sorted ( existing_slugs ) [ - 1 ] \n 
m = re . match ( , last_existing_slug ) \n 
id_counter = int ( m . group ( 1 ) ) + 1 \n 
~~~ id_counter = 1 \n 
~~ obj . slug = % ( obj . slug , id_counter ) \n 
~~ obj . save ( ) \n 
~~ ~~ ~~ db . create_unique ( , [ ] ) \n 
db . create_unique ( , [ ] ) \n 
~~~ db . delete_unique ( , [ ] ) \n 
db . delete_unique ( , [ ] ) \n 
: ( , [ ] , { : , : } : ( , [ ] , { : , } , \n 
: { : "\'django_content_type\'" , : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : : ( , [ ] , { : ( , [ ] , { : ( , [ ] , { : : ( , [ ] , { : , : } , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : "orm[\'skills.Comment\']" : ( , [ ] , { : "orm[\'skills.Project\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : ( , [ ] , { } ) \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'skills.TrainingBit\']" : ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'global_change_lab.User\']" : ( , [ ] , { : : ( , [ ] , { } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : , : ( , [ ] , { } ) \n 
: ( , [ ] , { : , : : ( , [ ] , { : , : ( , [ ] , { : , : : ( , [ ] , { : : ( , [ ] , { } ) \n 
: { : , : "[\'-created_at\']" } , \n 
: ( , [ ] , { : \'\\\'{"learn":[],"act":[],"share":[]}\\\'\' : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { } ) \n 
~~ from sparkpost import SparkPost \n 
sp = SparkPost ( ) \n 
result = sp . suppression_list . update ( { \n 
print ( result ) \n 
from . utils import wrap_future \n 
from . . transmissions import Transmissions as SyncTransmissions \n 
class Transmissions ( SyncTransmissions ) : \n 
~~~ def get ( self , transmission_id ) : \n 
~~~ results = self . _fetch_get ( transmission_id ) \n 
return wrap_future ( results , lambda f : f [ "transmission" ] ) \n 
from minecraft_data . v1_8 import windows as windows_by_id \n 
from minecraft_data . v1_8 import windows_list \n 
from spockbot . mcdata import constants , get_item_or_block \n 
from spockbot . mcdata . blocks import Block \n 
from spockbot . mcdata . items import Item \n 
from spockbot . mcdata . utils import camel_case , snake_case \n 
def make_slot_check ( wanted ) : \n 
if isinstance ( wanted , types . FunctionType ) : \n 
~~ if isinstance ( wanted , int ) : \n 
~~~ item , meta = wanted , None \n 
~~ elif isinstance ( wanted , Slot ) : \n 
~~ elif isinstance ( wanted , ( Item , Block ) ) : \n 
~~~ item , meta = wanted . id , wanted . metadata \n 
~~ elif isinstance ( wanted , str ) : \n 
~~~ item_or_block = get_item_or_block ( wanted , init = True ) \n 
item , meta = item_or_block . id , item_or_block . metadata \n 
~~~ item , meta = wanted \n 
~~~ raise ValueError ( % wanted ) \n 
~~ ~~ return lambda slot : item == slot . item_id and meta in ( None , slot . damage ) \n 
~~ class Slot ( object ) : \n 
~~~ def __init__ ( self , window , slot_nr , id = constants . INV_ITEMID_EMPTY , \n 
damage = 0 , amount = 0 , enchants = None ) : \n 
~~~ self . window = window \n 
self . slot_nr = slot_nr \n 
self . item_id = id \n 
self . damage = damage \n 
self . amount = amount \n 
self . nbt = enchants \n 
self . item = get_item_or_block ( self . item_id , self . damage ) or Item ( ) \n 
~~ def move_to_window ( self , window , slot_nr ) : \n 
~~~ self . window , self . slot_nr = window , slot_nr \n 
def is_empty ( self ) : \n 
~~~ return self . amount <= 0 \n 
~~ def matches ( self , other ) : \n 
~~~ return make_slot_check ( other ) ( self ) \n 
~~ def stacks_with ( self , other ) : \n 
~~~ if self . item_id != other . item_id : \n 
~~ if self . damage != other . damage : \n 
~~ return self . item . stack_size != 1 \n 
~~ def get_dict ( self ) : \n 
data = { : self . item_id } \n 
if self . item_id != constants . INV_ITEMID_EMPTY : \n 
~~~ data [ ] = self . damage \n 
data [ ] = self . amount \n 
if self . nbt is not None : \n 
~~~ data [ ] = self . nbt \n 
~~ ~~ return data \n 
~~ def copy ( self ) : \n 
~~~ return Slot ( self . window , self . slot_nr , self . item_id , \n 
self . damage , self . amount , self . nbt ) \n 
~~ def __bool__ ( self ) : \n 
~~~ return not self . is_empty \n 
~~~ if self . is_empty : \n 
~~~ s = \n 
~~~ item = self . item \n 
s = % ( self . amount , item . stack_size , str ( item ) ) \n 
~~ if self . slot_nr != - 1 : \n 
~~~ s += % self . slot_nr \n 
~~ if self . window : \n 
~~~ s += % self . window \n 
~~ return % s \n 
~~ ~~ class SlotCursor ( Slot ) : \n 
~~~ def __init__ ( self , id = constants . INV_ITEMID_EMPTY , damage = 0 , amount = 0 , \n 
enchants = None ) : \n 
~~~ window_id = constants . INV_WINID_CURSOR \n 
def __repr__ ( self ) : \n 
~~ ~~ super ( SlotCursor , self ) . __init__ ( \n 
CursorWindow ( ) , constants . INV_SLOT_NR_CURSOR , \n 
id , damage , amount , enchants ) \n 
~~ ~~ class BaseClick ( object ) : \n 
~~~ def get_packet ( self , inv_plugin ) : \n 
~~ def apply ( self , inv_plugin ) : \n 
~~ def on_success ( self , inv_plugin , emit_set_slot ) : \n 
self . dirty = set ( ) \n 
self . apply ( inv_plugin ) \n 
for changed_slot in self . dirty : \n 
~~~ emit_set_slot ( changed_slot ) \n 
~~ ~~ def copy_slot_type ( self , slot_from , slot_to ) : \n 
~~~ slot_to . item_id , slot_to . damage = slot_from . item_id , slot_from . damage \n 
slot_to . nbt = slot_from . nbt \n 
self . mark_dirty ( slot_to ) \n 
~~ def swap_slots ( self , slot_a , slot_b ) : \n 
~~~ slot_a . item_id , slot_b . item_id = slot_b . item_id , slot_a . item_id \n 
slot_a . damage , slot_b . damage = slot_b . damage , slot_a . damage \n 
slot_a . amount , slot_b . amount = slot_b . amount , slot_a . amount \n 
slot_a . nbt , slot_b . nbt = slot_b . nbt , slot_a . nbt \n 
self . mark_dirty ( slot_a ) \n 
self . mark_dirty ( slot_b ) \n 
~~ def transfer ( self , from_slot , to_slot , max_amount ) : \n 
~~~ transfer_amount = min ( max_amount , from_slot . amount , \n 
to_slot . item . stack_size - to_slot . amount ) \n 
if transfer_amount <= 0 : \n 
~~ self . copy_slot_type ( from_slot , to_slot ) \n 
to_slot . amount += transfer_amount \n 
from_slot . amount -= transfer_amount \n 
self . cleanup_if_empty ( from_slot ) \n 
~~ def cleanup_if_empty ( self , slot ) : \n 
~~~ if slot . is_empty : \n 
~~~ empty_slot_at_same_position = Slot ( slot . window , slot . slot_nr ) \n 
self . copy_slot_type ( empty_slot_at_same_position , slot ) \n 
~~ self . mark_dirty ( slot ) \n 
~~ def mark_dirty ( self , slot ) : \n 
~~~ self . dirty . add ( slot ) \n 
~~ ~~ class SingleClick ( BaseClick ) : \n 
~~~ def __init__ ( self , slot , button = constants . INV_BUTTON_LEFT ) : \n 
~~~ self . slot = slot \n 
self . button = button \n 
if button not in ( constants . INV_BUTTON_LEFT , \n 
constants . INV_BUTTON_RIGHT ) : \n 
~~~ raise NotImplementedError ( \n 
% button ) \n 
~~ ~~ def get_packet ( self , inv_plugin ) : \n 
~~~ slot_nr = self . slot . slot_nr \n 
if self . slot == inv_plugin . cursor_slot : \n 
~~~ slot_nr = constants . INV_OUTSIDE_WINDOW \n 
~~ return { \n 
: slot_nr , \n 
: self . button , \n 
: self . slot . get_dict ( ) , \n 
~~~ clicked = self . slot \n 
cursor = inv_plugin . cursor_slot \n 
if clicked == cursor : \n 
~~~ if self . button == constants . INV_BUTTON_LEFT : \n 
~~~ clicked . amount = 0 \n 
~~ elif self . button == constants . INV_BUTTON_RIGHT : \n 
~~~ clicked . amount -= 1 \n 
~~ self . cleanup_if_empty ( clicked ) \n 
~~ elif self . button == constants . INV_BUTTON_LEFT : \n 
~~~ if clicked . stacks_with ( cursor ) : \n 
~~~ self . transfer ( cursor , clicked , cursor . amount ) \n 
~~~ self . swap_slots ( cursor , clicked ) \n 
~~ ~~ elif self . button == constants . INV_BUTTON_RIGHT : \n 
~~~ if cursor . is_empty : \n 
~~~ self . transfer ( clicked , cursor , ( clicked . amount + 1 ) // 2 ) \n 
~~ elif clicked . is_empty or clicked . stacks_with ( cursor ) : \n 
~~~ self . transfer ( cursor , clicked , 1 ) \n 
% self . button ) \n 
~~ ~~ ~~ class DropClick ( BaseClick ) : \n 
~~~ def __init__ ( self , slot , drop_stack = False ) : \n 
self . drop_stack = drop_stack \n 
~~ def get_packet ( self , inv_plugin ) : \n 
~~~ if self . slot == inv_plugin . cursor_slot : \n 
~~ if not inv_plugin . cursor_slot . is_empty : \n 
: self . slot . slot_nr , \n 
: 1 if self . drop_stack else 0 , \n 
: 4 , \n 
: inv_plugin . cursor_slot . get_dict ( ) , \n 
~~~ if self . drop_stack : \n 
~~~ self . slot . amount = 0 \n 
~~~ self . slot . amount -= 1 \n 
~~ self . cleanup_if_empty ( self . slot ) \n 
~~ ~~ class Window ( object ) : \n 
name = None \n 
inv_type = None \n 
inv_data = { } \n 
def __init__ ( self , window_id , title , slot_count , \n 
inv_type = None , persistent_slots = None , eid = None ) : \n 
~~~ assert not inv_type or inv_type == self . inv_type , % ( inv_type , self . inv_type ) \n 
~~~ window_dict = windows_by_id [ inv_type ] \n 
if in window_dict : \n 
~~~ slot_count = max ( slot [ ] + slot . get ( , 1 ) \n 
for slot in window_dict [ ] ) \n 
~~ ~~ self . window_id = window_id \n 
self . title = title \n 
self . slots = [ Slot ( self , slot_nr ) for slot_nr in range ( slot_count ) ] \n 
if persistent_slots is None : \n 
~~~ for slot_nr in range ( constants . INV_SLOTS_PERSISTENT ) : \n 
~~~ self . slots . append ( Slot ( self , slot_nr + slot_count ) ) \n 
~~~ moved_slots = persistent_slots [ - constants . INV_SLOTS_PERSISTENT : ] \n 
for slot_nr , moved_slot in enumerate ( moved_slots ) : \n 
~~~ moved_slot . move_to_window ( self , slot_nr + slot_count ) \n 
self . slots . append ( moved_slot ) \n 
~~ ~~ self . properties = { } \n 
self . __class__ . __name__ , \n 
self . window_id , self . title , len ( self . slots ) ) \n 
def persistent_slots ( self ) : \n 
~~~ return self . slots [ - constants . INV_SLOTS_PERSISTENT : ] \n 
def inventory_slots ( self ) : \n 
~~~ return self . slots [ \n 
- constants . INV_SLOTS_PERSISTENT : - constants . INV_SLOTS_HOTBAR ] \n 
def hotbar_slots ( self ) : \n 
~~~ return self . slots [ - constants . INV_SLOTS_HOTBAR : ] \n 
def window_slots ( self ) : \n 
return self . slots [ : - constants . INV_SLOTS_PERSISTENT ] \n 
~~ ~~ def _make_window ( window_dict ) : \n 
cls_name = % camel_case ( str ( window_dict [ ] ) ) \n 
bases = ( Window , ) \n 
attrs = { \n 
: sys . modules [ __name__ ] , \n 
: str ( window_dict [ ] ) , \n 
: window_dict , \n 
def make_slot_method ( index , size = 1 ) : \n 
~~~ if size == 1 : \n 
~~~ return lambda self : self . slots [ index ] \n 
~~~ return lambda self : self . slots [ index : ( index + size ) ] \n 
~~ ~~ for slots in window_dict . get ( , [ ] ) : \n 
~~~ index = slots [ ] \n 
size = slots . get ( , 1 ) \n 
attr_name = snake_case ( str ( slots [ ] ) ) \n 
attr_name += if size == 1 else \n 
slots_method = make_slot_method ( index , size ) \n 
slots_method . __name__ = attr_name \n 
attrs [ attr_name ] = property ( slots_method ) \n 
~~ for i , prop_name in enumerate ( window_dict . get ( , [ ] ) ) : \n 
~~~ def make_prop_method ( i ) : \n 
~~~ return lambda self : self . properties [ i ] \n 
~~ prop_method = make_prop_method ( i ) \n 
prop_name = snake_case ( str ( prop_name ) ) \n 
prop_method . __name__ = prop_name \n 
attrs [ prop_name ] = property ( prop_method ) \n 
~~ cls = type ( cls_name , bases , attrs ) \n 
setattr ( sys . modules [ __name__ ] , cls_name , cls ) \n 
~~ inv_types = { } \n 
def _create_windows ( ) : \n 
~~~ for window in windows_list : \n 
~~~ cls = _make_window ( window ) \n 
inv_types [ cls . inv_type ] = cls \n 
~~ ~~ _create_windows ( ) \n 
_player_window = sys . modules [ __name__ ] . PlayerWindow \n 
def _player_init ( self , * args , ** kwargs ) : \n 
~~~ super ( _player_window , self ) . __init__ ( \n 
constants . INV_WINID_PLAYER , self . name , constants . INV_SLOTS_PLAYER , \n 
* args , ** kwargs ) \n 
~~ setattr ( _player_window , , _player_init ) \n 
from spockbot . mcdata import blocks , constants as const \n 
from spockbot . mcdata . utils import BoundingBox \n 
from spockbot . plugins . base import PluginBase , pl_announce \n 
from spockbot . plugins . tools import collision \n 
from spockbot . vector import Vector3 \n 
FP_MAGIC = 1e-4 \n 
class PhysicsCore ( object ) : \n 
~~~ def __init__ ( self , pos , vec , abilities ) : \n 
~~~ self . pos = pos \n 
self . vec = vec \n 
self . sprinting = False \n 
self . move_accel = abilities . walking_speed \n 
self . abilities = abilities \n 
self . direction = Vector3 ( ) \n 
~~ def jump ( self ) : \n 
~~~ if self . pos . on_ground : \n 
~~~ if self . sprinting : \n 
~~~ ground_speed = Vector3 ( self . vec . x , 0 , self . vec . z ) \n 
if ground_speed : \n 
~~~ self . vec += ground_speed . norm ( ) * const . PHY_JMP_MUL \n 
~~ ~~ self . vec . y = const . PHY_JMP_ABS \n 
~~ ~~ def walk ( self ) : \n 
~~~ self . sprinting = False \n 
self . move_accel = self . abilities . walking_speed \n 
~~ def sprint ( self ) : \n 
~~~ self . sprinting = True \n 
self . move_accel = self . abilities . walking_speed * const . PHY_SPR_MUL \n 
~~ def move_target ( self , vector ) : \n 
~~~ self . direction = vector - self . pos \n 
self . direction . y = 0 \n 
if self . direction <= Vector3 ( self . vec . x , 0 , self . vec . z ) : \n 
~~ ~~ def move_vector ( self , vector ) : \n 
~~~ vector . y = 0 \n 
self . direction = vector \n 
~~ def move_angle ( self , angle , radians = False ) : \n 
~~~ angle = angle if radians else math . radians ( angle ) \n 
self . direction = Vector3 ( math . sin ( angle ) , 0 , math . cos ( angle ) ) \n 
~~ ~~ @ pl_announce ( ) \n 
class PhysicsPlugin ( PluginBase ) : \n 
~~~ requires = ( , , , ) \n 
events = { \n 
def __init__ ( self , ploader , settings ) : \n 
~~~ super ( PhysicsPlugin , self ) . __init__ ( ploader , settings ) \n 
self . vec = Vector3 ( 0.0 , 0.0 , 0.0 ) \n 
self . col = collision . MTVTest ( \n 
self . world , BoundingBox ( const . PLAYER_WIDTH , const . PLAYER_HEIGHT ) \n 
self . pos = self . clientinfo . position \n 
self . skip_tick = False \n 
self . pc = PhysicsCore ( self . pos , self . vec , self . clientinfo . abilities ) \n 
ploader . provides ( , self . pc ) \n 
~~ def skip_physics ( self , _ = None , __ = None ) : \n 
~~~ self . vec . zero ( ) \n 
self . skip_tick = True \n 
~~ def suspend_physics ( self , _ = None , __ = None ) : \n 
self . event . unreg_event_handler ( , self . physics_tick ) \n 
~~ def resume_physics ( self , _ = None , __ = None ) : \n 
~~~ self . event . reg_event_handler ( , self . physics_tick ) \n 
~~ def client_tick ( self , name , data ) : \n 
~~~ self . net . push_packet ( , \n 
self . clientinfo . position . get_dict ( ) ) \n 
~~ def physics_tick ( self , _ , __ ) : \n 
~~~ if self . skip_tick : \n 
~~~ self . skip_tick = False \n 
~~ self . apply_accel ( ) \n 
mtv = self . get_mtv ( ) \n 
self . apply_vector ( mtv ) \n 
self . pos . on_ground = mtv . y > 0 \n 
self . vec -= Vector3 ( 0 , const . PHY_GAV_ACC , 0 ) \n 
self . apply_drag ( ) \n 
self . pc . direction = Vector3 ( ) \n 
~~ def get_block_slip ( self ) : \n 
~~~ bpos = self . pos . floor ( ) \n 
return blocks . get_block ( * self . world . get_block ( * bpos ) ) . slipperiness \n 
~~ return 1 \n 
~~ def apply_accel ( self ) : \n 
~~~ if not self . pc . direction : \n 
~~ if self . pos . on_ground : \n 
~~~ block_slip = self . get_block_slip ( ) \n 
accel_mod = const . BASE_GND_SLIP ** 3 / block_slip ** 3 \n 
accel = self . pc . move_accel * accel_mod * const . PHY_BASE_DRG \n 
~~~ accel = const . PHY_JMP_ACC \n 
~~ self . vec += self . pc . direction . norm ( ) * accel \n 
~~ def apply_vector ( self , mtv ) : \n 
~~~ self . pos += ( self . vec + mtv ) \n 
self . vec . x = 0 if mtv . x else self . vec . x \n 
self . vec . y = 0 if mtv . y else self . vec . y \n 
self . vec . z = 0 if mtv . z else self . vec . z \n 
~~ def apply_drag ( self ) : \n 
~~~ drag = self . get_block_slip ( ) * const . PHY_DRG_MUL \n 
self . vec . x *= drag \n 
self . vec . z *= drag \n 
self . vec . y *= const . PHY_BASE_DRG \n 
~~ def get_mtv ( self ) : \n 
~~~ pos = self . pos + self . vec \n 
pos = collision . uncenter_position ( pos , self . col . bbox ) \n 
q = collections . deque ( ( Vector3 ( ) , ) ) \n 
while q : \n 
~~~ current_vector = q . popleft ( ) \n 
transform_vectors = self . col . check_collision ( pos , current_vector ) \n 
if not all ( transform_vectors ) : \n 
~~ for vector in transform_vectors : \n 
~~~ test_vec = self . vec + current_vector + vector \n 
if test_vec . dist_sq ( ) <= self . vec . dist_sq ( ) + FP_MAGIC : \n 
~~~ q . append ( current_vector + vector ) \n 
~~~ logger . debug ( ) \n 
self . vec . zero ( ) \n 
return Vector3 ( ) \n 
~~ possible_mtv = [ current_vector ] \n 
~~~ possible_mtv . append ( current_vector ) \n 
~~ ~~ return min ( possible_mtv ) \n 
from beaker . middleware import SessionMiddleware \n 
from paste . cascade import Cascade \n 
from paste . registry import RegistryManager \n 
from paste . urlparser import StaticURLParser \n 
from paste . deploy . converters import asbool \n 
from pylons . middleware import ErrorHandler , StatusCodeRedirect \n 
from pylons . wsgiapp import PylonsApp \n 
from routes . middleware import RoutesMiddleware \n 
from nipapwww . config . environment import load_environment \n 
def make_app ( global_conf , full_stack = True , static_files = True , ** app_conf ) : \n 
config = load_environment ( global_conf , app_conf ) \n 
app = PylonsApp ( config = config ) \n 
app = RoutesMiddleware ( app , config [ ] , singleton = False ) \n 
app = SessionMiddleware ( app , config ) \n 
if asbool ( full_stack ) : \n 
~~~ app = ErrorHandler ( app , global_conf , ** config [ ] ) \n 
if asbool ( config [ ] ) : \n 
~~~ app = StatusCodeRedirect ( app ) \n 
~~~ app = StatusCodeRedirect ( app , [ 400 , 401 , 403 , 404 , 500 ] ) \n 
~~ ~~ app = RegistryManager ( app ) \n 
if asbool ( static_files ) : \n 
~~~ static_app = StaticURLParser ( config [ ] [ ] ) \n 
app = Cascade ( [ static_app , app ] ) \n 
~~ app . config = config \n 
return app \n 
~~ class NipapError ( Exception ) : \n 
error_code = 1000 \n 
~~ class NipapInputError ( NipapError ) : \n 
error_code = 1100 \n 
~~ class NipapMissingInputError ( NipapInputError ) : \n 
error_code = 1110 \n 
~~ class NipapExtraneousInputError ( NipapInputError ) : \n 
error_code = 1120 \n 
~~ class NipapNoSuchOperatorError ( NipapInputError ) : \n 
error_code = 1130 \n 
~~ class NipapValueError ( NipapError ) : \n 
error_code = 1200 \n 
~~ class NipapNonExistentError ( NipapError ) : \n 
error_code = 1300 \n 
~~ class NipapDuplicateError ( NipapError ) : \n 
error_code = 1400 \n 
__version__ = "0.28.4" \n 
__license__ = "MIT" \n 
__status__ = "Development" \n 
__url__ = "http://SpriteLink.github.com/NIPAP" \n 
UMASK = 0 \n 
WORKDIR = "/" \n 
MAXFD = 1024 \n 
if ( hasattr ( os , "devnull" ) ) : \n 
~~~ REDIRECT_TO = os . devnull \n 
~~~ REDIRECT_TO = "/dev/null" \n 
~~ def createDaemon ( ) : \n 
~~~ pid = os . fork ( ) \n 
~~ except OSError , e : \n 
~~~ os . setsid ( ) \n 
~~~ os . chdir ( WORKDIR ) \n 
os . umask ( UMASK ) \n 
maxfd = resource . getrlimit ( resource . RLIMIT_NOFILE ) [ 1 ] \n 
if ( maxfd == resource . RLIM_INFINITY ) : \n 
~~~ maxfd = MAXFD \n 
return ( 0 ) \n 
~~ def drop_privileges ( uid_name = , gid_name = ) : \n 
~~~ if os . getuid ( ) != 0 : \n 
~~ import pwd , grp \n 
uid = pwd . getpwnam ( uid_name ) . pw_uid \n 
gid = grp . getgrnam ( gid_name ) . gr_gid \n 
os . setgroups ( [ ] ) \n 
os . setgid ( gid ) \n 
os . setuid ( uid ) \n 
old_umask = os . umask ( 0 77 ) \n 
import urllib . request \n 
import urllib . error \n 
import urllib . parse \n 
from . exceptions import InvalidFunctionError \n 
ERR_PARSE = - 32700 \n 
ERR_INVALID_REQ = - 32600 \n 
ERR_METHOD_NOT_FOUND = - 32601 \n 
ERR_INVALID_PARAMS = - 32602 \n 
ERR_INTERNAL = - 32603 \n 
ERR_UNKNOWN = - 32000 \n 
ERR_INVALID_RESP = - 32001 \n 
def contract_from_file ( fname ) : \n 
f = open ( fname ) \n 
j = f . read ( ) \n 
return Contract ( json . loads ( j ) ) \n 
~~ def unpack_method ( method ) : \n 
pos = method . find ( "." ) \n 
if pos == - 1 : \n 
~~ iface_name = method [ : pos ] \n 
func_name = method [ pos + 1 : ] \n 
return iface_name , func_name \n 
~~ def idgen_uuid ( ) : \n 
return uuid . uuid4 ( ) . hex \n 
~~ idgen_seq_counter = itertools . count ( ) \n 
def idgen_seq ( ) : \n 
return str ( next ( idgen_seq_counter ) ) \n 
~~ def err_response ( reqid , code , msg , data = None ) : \n 
err = { "code" : code , "message" : msg } \n 
if data : \n 
~~~ err [ "data" ] = data \n 
~~ return { "jsonrpc" : "2.0" , "id" : reqid , "error" : err } \n 
~~ def safe_get ( d , key , def_val = None ) : \n 
if key in d : \n 
~~~ return d [ key ] \n 
~~~ return def_val \n 
~~ ~~ class RpcException ( Exception , json . JSONEncoder ) : \n 
def __init__ ( self , code , msg = "" , data = None ) : \n 
if self . data : \n 
~~ return s \n 
~~ ~~ class RequestContext ( object ) : \n 
def __init__ ( self , props , req ) : \n 
self . props = props \n 
self . request = req \n 
self . response = None \n 
self . error = None \n 
~~ def func_name ( self ) : \n 
~~~ return unpack_method ( self . request [ "method" ] ) [ 1 ] \n 
~~ def get_prop ( self , key , default_val = None ) : \n 
if key in self . props : \n 
~~~ return self . props [ key ] \n 
~~~ return default_val \n 
~~ ~~ def set_error ( self , code , msg , data = None ) : \n 
self . error = err_response ( self . request [ "id" ] , code , msg , data ) \n 
~~ ~~ class Filter ( object ) : \n 
def pre ( self , context ) : \n 
~~ def post ( self , context ) : \n 
~~ ~~ class Server ( object ) : \n 
def __init__ ( self , contract , validate_request = True , validate_response = True ) : \n 
logging . basicConfig ( ) \n 
self . log = logging . getLogger ( "common.barrister" ) \n 
self . validate_req = validate_request \n 
self . validate_resp = validate_response \n 
self . contract = contract \n 
self . filters = None \n 
~~ def add_handler ( self , iface_name , handler ) : \n 
if self . contract . has_interface ( iface_name ) : \n 
~~~ self . handlers [ iface_name ] = handler \n 
~~ ~~ def set_filters ( self , filters ) : \n 
if filters is None or isinstance ( filters , ( tuple , list ) ) : \n 
~~~ self . filters = filters \n 
~~~ self . filters = [ filters ] \n 
~~ ~~ def call_json ( self , req_json , props = None ) : \n 
~~~ req = json . loads ( req_json ) \n 
return json . dumps ( err_response ( None , - 32700 , msg ) ) \n 
~~ return json . dumps ( self . call ( req , props ) ) \n 
~~ def call ( self , req , props = None ) : \n 
resp = None \n 
if self . log . isEnabledFor ( logging . DEBUG ) : \n 
~~ if isinstance ( req , list ) : \n 
~~~ if len ( req ) < 1 : \n 
~~~ resp = [ self . _call_and_format ( r , props ) for r in req ] \n 
~~~ resp = self . _call_and_format ( req , props ) \n 
~~ if self . log . isEnabledFor ( logging . DEBUG ) : \n 
~~ return resp \n 
~~ def _call_and_format ( self , req , props = None ) : \n 
if not isinstance ( req , dict ) : \n 
~~~ return err_response ( None , ERR_INVALID_REQ , \n 
~~ reqid = None \n 
if "id" in req : \n 
~~~ reqid = req [ "id" ] \n 
~~ if props is None : \n 
~~~ props = { } \n 
~~ context = RequestContext ( props , req ) \n 
if self . filters : \n 
~~~ for f in self . filters : \n 
~~~ f . pre ( context ) \n 
~~ ~~ if context . error : \n 
~~~ return context . error \n 
~~ resp = None \n 
~~~ result = self . _call ( context ) \n 
resp = { "jsonrpc" : "2.0" , "id" : reqid , "result" : result } \n 
~~ except RpcException as e : \n 
~~~ resp = err_response ( reqid , e . code , e . msg , e . data ) \n 
data = { \n 
: str ( e ) \n 
~~ if self . filters : \n 
~~~ context . response = resp \n 
for f in self . filters : \n 
~~~ f . post ( context ) \n 
~~ ~~ return resp \n 
~~ def _call ( self , context ) : \n 
req = context . request \n 
if "method" not in req : \n 
~~ method = req [ "method" ] \n 
if method == "common.barrister-idl" or method == "getIdl" : \n 
~~~ return self . contract . idl_parsed \n 
~~ iface_name , func_name = unpack_method ( method ) \n 
if iface_name in self . handlers : \n 
~~~ iface_impl = self . handlers [ iface_name ] \n 
func = getattr ( iface_impl , func_name ) \n 
~~~ if "params" in req : \n 
~~~ params = req [ "params" ] \n 
~~~ params = [ ] \n 
~~ if self . validate_req : \n 
~~~ self . contract . validate_request ( iface_name , func_name , params ) \n 
~~ if hasattr ( iface_impl , "barrister_pre" ) : \n 
~~~ pre_hook = getattr ( iface_impl , "barrister_pre" ) \n 
pre_hook ( context , params ) \n 
~~ if params : \n 
~~~ result = func ( * params ) \n 
~~~ result = func ( ) \n 
~~ if self . validate_resp : \n 
~~~ self . contract . validate_response ( iface_name , func_name , result ) \n 
raise RpcException ( ERR_METHOD_NOT_FOUND , msg ) \n 
~~ ~~ ~~ class HttpTransport ( object ) : \n 
def __init__ ( self , url , handlers = None , headers = None ) : \n 
if not headers : \n 
~~ headers [ ] = \n 
self . url = url \n 
self . headers = headers \n 
if handlers : \n 
~~~ self . opener = urllib . request . build_opener ( * handlers ) \n 
~~~ self . opener = urllib . request . build_opener ( ) \n 
~~ ~~ def request ( self , req ) : \n 
data = json . dumps ( req ) \n 
req = urllib . request . Request ( self . url , data , self . headers ) \n 
f = self . opener . open ( req ) \n 
resp = f . read ( ) \n 
return json . loads ( resp ) \n 
~~ ~~ class InProcTransport ( object ) : \n 
def __init__ ( self , server ) : \n 
~~ def request ( self , req ) : \n 
return self . server . call ( req ) \n 
~~ ~~ class Client ( object ) : \n 
def __init__ ( self , transport , validate_request = True , validate_response = True , \n 
id_gen = idgen_uuid ) : \n 
self . transport = transport \n 
self . id_gen = id_gen \n 
req = { "jsonrpc" : "2.0" , "method" : "common.barrister-idl" , "id" : "1" } \n 
resp = transport . request ( req ) \n 
self . contract = Contract ( resp [ "result" ] ) \n 
for k , v in list ( self . contract . interfaces . items ( ) ) : \n 
~~~ setattr ( self , k , InterfaceClientProxy ( self , v ) ) \n 
~~ ~~ def get_meta ( self ) : \n 
return self . contract . meta \n 
~~ def call ( self , iface_name , func_name , params ) : \n 
req = self . to_request ( iface_name , func_name , params ) \n 
~~ resp = self . transport . request ( req ) \n 
~~ return self . to_result ( iface_name , func_name , resp ) \n 
~~ def to_request ( self , iface_name , func_name , params ) : \n 
if self . validate_req : \n 
~~ method = "%s.%s" % ( iface_name , func_name ) \n 
reqid = self . id_gen ( ) \n 
return { "jsonrpc" : "2.0" , "id" : reqid , "method" : method , "params" : params } \n 
~~ def to_result ( self , iface_name , func_name , resp ) : \n 
if "error" in resp : \n 
~~~ e = resp [ "error" ] \n 
data = None \n 
if "data" in e : \n 
~~~ data = e [ "data" ] \n 
~~ raise RpcException ( e [ "code" ] , e [ "message" ] , data ) \n 
~~ result = resp [ "result" ] \n 
if self . validate_resp : \n 
~~ def start_batch ( self ) : \n 
return Batch ( self ) \n 
~~ ~~ class InterfaceClientProxy ( object ) : \n 
def __init__ ( self , client , iface ) : \n 
self . client = client \n 
iface_name = iface . name \n 
for func_name , func in list ( iface . functions . items ( ) ) : \n 
~~~ setattr ( self , func_name , self . _caller ( iface_name , func_name ) ) \n 
~~ ~~ def _caller ( self , iface_name , func_name ) : \n 
def caller ( * params ) : \n 
~~~ return self . client . call ( iface_name , func_name , params ) \n 
~~ return caller \n 
~~ ~~ class Batch ( object ) : \n 
def __init__ ( self , client ) : \n 
self . req_list = [ ] \n 
self . sent = False \n 
for k , v in list ( client . contract . interfaces . items ( ) ) : \n 
~~ ~~ def call ( self , iface_name , func_name , params ) : \n 
if self . sent : \n 
~~~ req = self . client . to_request ( iface_name , func_name , params ) \n 
self . req_list . append ( req ) \n 
~~ ~~ def send ( self ) : \n 
~~~ self . sent = True \n 
results = self . client . transport . request ( self . req_list ) \n 
id_to_method = { } \n 
by_id = { } \n 
for res in results : \n 
~~~ reqid = res [ "id" ] \n 
by_id [ reqid ] = res \n 
~~ in_req_order = [ ] \n 
for req in self . req_list : \n 
result = None \n 
error = None \n 
resp = safe_get ( by_id , reqid ) \n 
if resp is None : \n 
error = RpcException ( ERR_INVALID_RESP , msg ) \n 
~~~ r_err = safe_get ( resp , "error" ) \n 
if r_err is None : \n 
~~~ result = resp [ "result" ] \n 
~~~ error = RpcException ( r_err [ "code" ] , r_err [ "message" ] , safe_get ( r_err , "data" ~~ ~~ in_req_order . append ( RpcResponse ( req , result , error ) ) \n 
~~ return in_req_order \n 
~~ ~~ ~~ class RpcResponse ( object ) : \n 
def __init__ ( self , request , result , error ) : \n 
~~~ self . request = request \n 
self . result = result \n 
self . error = error \n 
~~ ~~ class Contract ( object ) : \n 
def __init__ ( self , idl_parsed ) : \n 
self . idl_parsed = idl_parsed \n 
self . interfaces = { } \n 
self . structs = { } \n 
self . enums = { } \n 
self . meta = { } \n 
for e in idl_parsed : \n 
~~~ if e [ "type" ] == "struct" : \n 
~~~ self . structs [ e [ "name" ] ] = Struct ( e , self ) \n 
~~ elif e [ "type" ] == "enum" : \n 
~~~ self . enums [ e [ "name" ] ] = Enum ( e ) \n 
~~ elif e [ "type" ] == "interface" : \n 
~~~ self . interfaces [ e [ "name" ] ] = Interface ( e , self ) \n 
~~ elif e [ "type" ] == "meta" : \n 
~~~ for k , v in list ( e . items ( ) ) : \n 
~~~ if k != "type" : \n 
~~~ self . meta [ k ] = v \n 
~~ ~~ ~~ ~~ ~~ def validate_request ( self , iface_name , func_name , params ) : \n 
self . interface ( iface_name ) . function ( func_name ) . validate_params ( params ) \n 
~~ def validate_response ( self , iface_name , func_name , resp ) : \n 
self . interface ( iface_name ) . function ( func_name ) . validate_response ( resp ) \n 
~~ def get ( self , name ) : \n 
if name in self . structs : \n 
~~~ return self . structs [ name ] \n 
~~ elif name in self . enums : \n 
~~~ return self . enums [ name ] \n 
~~ elif name in self . interfaces : \n 
~~~ return self . interfaces [ name ] \n 
~~ ~~ def struct ( self , struct_name ) : \n 
if struct_name in self . structs : \n 
~~~ return self . structs [ struct_name ] \n 
~~ ~~ def has_interface ( self , iface_name ) : \n 
return iface_name in self . interfaces \n 
~~ def interface ( self , iface_name ) : \n 
if self . has_interface ( iface_name ) : \n 
~~~ return self . interfaces [ iface_name ] \n 
~~ ~~ def validate ( self , expected_type , is_array , val ) : \n 
if val is None : \n 
~~~ if expected_type . optional : \n 
~~~ return True , None \n 
~~ ~~ elif is_array : \n 
~~~ if not isinstance ( val , list ) : \n 
~~~ return self . _type_err ( val , "list" ) \n 
~~~ for v in val : \n 
~~~ ok , msg = self . validate ( expected_type , False , v ) \n 
if not ok : \n 
~~~ return ok , msg \n 
~~ ~~ ~~ ~~ elif expected_type . type == "int" : \n 
~~~ if not isinstance ( val , int ) : \n 
~~~ return self . _type_err ( val , "int" ) \n 
~~ ~~ elif expected_type . type == "float" : \n 
~~~ if not isinstance ( val , ( float , int ) ) : \n 
~~~ return self . _type_err ( val , "float" ) \n 
~~ ~~ elif expected_type . type == "bool" : \n 
~~~ if not isinstance ( val , bool ) : \n 
~~~ return self . _type_err ( val , "bool" ) \n 
~~ ~~ elif expected_type . type == "string" : \n 
~~~ if not isinstance ( val , str ) : \n 
~~~ return self . _type_err ( val , "string" ) \n 
~~~ return self . get ( expected_type . type ) . validate ( val ) \n 
~~ return True , None \n 
~~ def _type_err ( self , val , expected ) : \n 
~~ ~~ class Interface ( object ) : \n 
def __init__ ( self , iface , contract ) : \n 
self . name = iface [ "name" ] \n 
self . functions = { } \n 
for f in iface [ "functions" ] : \n 
~~~ self . functions [ f [ "name" ] ] = Function ( self . name , f , contract ) \n 
~~ ~~ def function ( self , func_name ) : \n 
if func_name in self . functions : \n 
~~~ return self . functions [ func_name ] \n 
~~~ raise RpcException ( ERR_METHOD_NOT_FOUND , \n 
~~ ~~ ~~ class Enum ( object ) : \n 
def __init__ ( self , enum ) : \n 
self . name = enum [ "name" ] \n 
self . values = [ ] \n 
for v in enum [ "values" ] : \n 
~~~ self . values . append ( v [ "value" ] ) \n 
~~ ~~ def validate ( self , val ) : \n 
if val in self . values : \n 
~~ ~~ ~~ class Struct ( object ) : \n 
def __init__ ( self , s , contract ) : \n 
self . name = s [ "name" ] \n 
self . extends = s [ "extends" ] \n 
self . parent = None \n 
self . fields = { } \n 
for f in s [ "fields" ] : \n 
~~~ self . fields [ f [ "name" ] ] = Type ( f ) \n 
~~ ~~ def field ( self , name ) : \n 
if name in self . fields : \n 
~~~ return self . fields [ name ] \n 
~~ elif self . extends : \n 
~~~ if not self . parent : \n 
~~~ self . parent = self . contract . struct ( self . extends ) \n 
~~ return self . parent . field ( name ) \n 
if type ( val ) is not dict : \n 
~~ for k , v in list ( val . items ( ) ) : \n 
~~~ field = self . field ( k ) \n 
if field : \n 
~~~ ok , msg = self . contract . validate ( field , field . is_array , v ) \n 
~~ ~~ all_fields = self . get_all_fields ( [ ] ) \n 
for field in all_fields : \n 
~~~ if field . name not in val and not field . optional : \n 
~~ ~~ return True , None \n 
~~ def get_all_fields ( self , arr ) : \n 
for k , v in list ( self . fields . items ( ) ) : \n 
~~~ arr . append ( v ) \n 
~~ if self . extends : \n 
~~~ parent = self . contract . get ( self . extends ) \n 
if parent : \n 
~~~ return parent . get_all_fields ( arr ) \n 
~~ ~~ return arr \n 
~~ ~~ class Function ( object ) : \n 
def __init__ ( self , iface_name , f , contract ) : \n 
self . name = f [ "name" ] \n 
self . params = [ ] \n 
for p in f [ "params" ] : \n 
~~~ self . params . append ( Type ( p ) ) \n 
~~ self . returns = Type ( f [ "returns" ] ) if "returns" in f else None \n 
self . full_name = "%s.%s" % ( iface_name , self . name ) \n 
self . validate_structure ( ) \n 
~~ def validate_structure ( self ) : \n 
if self . name in [ None , ] : \n 
~~~ raise InvalidFunctionError ( \n 
~~ if self . returns is None : \n 
self . full_name \n 
~~ ~~ def validate_params ( self , params ) : \n 
if params is not None : \n 
~~~ if len ( self . params ) != len ( params ) : \n 
~~~ vals = ( self . full_name , len ( self . params ) , len ( params ) ) \n 
raise RpcException ( ERR_INVALID_PARAMS , msg ) \n 
~~ [ self . _validate_param ( x , y ) for ( x , y ) in zip ( self . params , params ) ] \n 
~~ ~~ def validate_response ( self , resp ) : \n 
ok , msg = self . contract . validate ( self . returns , \n 
self . returns . is_array , resp ) \n 
~~~ vals = ( self . full_name , str ( resp ) , msg ) \n 
raise RpcException ( ERR_INVALID_RESP , msg ) \n 
~~ ~~ def _validate_param ( self , expected , param ) : \n 
ok , msg = self . contract . validate ( expected , expected . is_array , param ) \n 
~~~ vals = ( self . full_name , expected . name , msg ) \n 
~~ ~~ ~~ class Type ( object ) : \n 
~~~ def __init__ ( self , type_dict ) : \n 
~~~ self . name = "" \n 
self . optional = False \n 
if "name" in type_dict : \n 
~~~ self . name = type_dict [ "name" ] \n 
~~ self . type = type_dict [ "type" ] \n 
self . is_array = type_dict [ "is_array" ] \n 
if "optional" in type_dict : \n 
~~~ self . optional = type_dict [ "optional" ] \n 
~~ ~~ ~~ import os \n 
class CheckProcs ( object ) : \n 
~~~ myPid = 0 \n 
state = "" \n 
name = "" \n 
pid = 0 \n 
allProcs = [ ] \n 
interestingProcs = [ ] \n 
procDir = "/proc" \n 
debug = False \n 
~~~ self . myPid = os . getpid ( ) \n 
~~ def setup ( self , debug = False , pidlist = False ) : \n 
~~~ self . debug = debug \n 
self . pidlist = pidlist \n 
if debug is True : \n 
~~ self . allProcs = [ procs for procs in os . listdir ( self . procDir ) if procs . isdigit ( ) and \n 
int ( procs ) != int ( self . myPid ) ] \n 
~~ def process ( self , criteria ) : \n 
~~~ for p in self . allProcs : \n 
~~~ fh = open ( self . procDir + "/" + p + "/stat" ) \n 
pInfo = fh . readline ( ) . split ( ) \n 
cmdfh = open ( self . procDir + "/" + p + "/cmdline" ) \n 
cmd = cmdfh . readline ( ) \n 
pInfo [ 1 ] = cmd \n 
~~~ cmdfh . close ( ) \n 
fh . close ( ) \n 
~~ if criteria == : \n 
~~~ if pInfo [ 2 ] == self . state : \n 
~~~ self . interestingProcs . append ( pInfo ) \n 
~~ ~~ elif criteria == : \n 
~~~ if re . search ( self . name , pInfo [ 1 ] ) : \n 
~~~ if pInfo [ 0 ] == self . pid : \n 
~~ ~~ ~~ ~~ def byState ( self , state ) : \n 
~~~ self . state = state \n 
self . process ( criteria = ) \n 
self . show ( ) \n 
~~ def byPid ( self , pid ) : \n 
~~~ self . pid = pid \n 
~~ def byName ( self , name ) : \n 
~~ def run ( self , foo , criteria ) : \n 
~~~ if foo == : \n 
~~~ self . byState ( criteria ) \n 
~~ elif foo == : \n 
~~~ self . byName ( criteria ) \n 
~~~ self . byPid ( criteria ) \n 
~~ ~~ def show ( self ) : \n 
~~~ prettyOut = { } \n 
if len ( self . interestingProcs ) > 0 : \n 
~~~ for proc in self . interestingProcs : \n 
~~~ prettyOut [ proc [ 0 ] ] = proc [ 1 ] \n 
~~ ~~ if self . pidlist is True : \n 
~~~ pidlist = . join ( prettyOut . keys ( ) ) \n 
sys . stderr . write ( pidlist ) \n 
~~ print ( json . dumps ( prettyOut ) ) \n 
~~~ if "pidlist" in sys . argv : \n 
~~~ pidlist = True \n 
~~~ pidlist = False \n 
~~ foo = CheckProcs ( ) \n 
foo . setup ( debug = False , pidlist = pidlist ) \n 
foo . run ( sys . argv [ 1 ] , sys . argv [ 2 ] ) \n 
~~ from oslo_config import cfg \n 
import st2common . config as common_config \n 
from st2common . constants . system import VERSION_STRING \n 
common_config . register_opts ( ) \n 
def parse_args ( args = None ) : \n 
~~~ CONF ( args = args , version = VERSION_STRING ) \n 
~~ def register_opts ( ) : \n 
~~~ _register_common_opts ( ) \n 
_register_notifier_opts ( ) \n 
~~ def get_logging_config_path ( ) : \n 
~~~ return cfg . CONF . notifier . logging \n 
~~ def _register_common_opts ( ) : \n 
~~~ common_config . register_opts ( ) \n 
~~ def _register_notifier_opts ( ) : \n 
~~~ notifier_opts = [ \n 
cfg . StrOpt ( , default = , \n 
CONF . register_opts ( notifier_opts , group = ) \n 
scheduler_opts = [ \n 
cfg . BoolOpt ( , default = True , help = ) , \n 
cfg . IntOpt ( , default = 600 , \n 
help = ) , cfg . IntOpt ( , default = 300 , \n 
CONF . register_opts ( scheduler_opts , group = ) \n 
~~ register_opts ( ) \n 
import abc \n 
from distutils . spawn import find_executable \n 
import six \n 
from st2actions . runners import ActionRunner \n 
WINEXE_EXISTS = find_executable ( ) is not None \n 
SMBCLIENT_EXISTS = find_executable ( ) is not None \n 
ERROR_CODE_TO_MESSAGE_MAP = { \n 
@ six . add_metaclass ( abc . ABCMeta ) \n 
class BaseWindowsRunner ( ActionRunner ) : \n 
~~~ def _verify_winexe_exists ( self ) : \n 
~~~ if not WINEXE_EXISTS : \n 
raise Exception ( msg ) \n 
~~ ~~ def _verify_smbclient_exists ( self ) : \n 
~~~ if not SMBCLIENT_EXISTS : \n 
~~ ~~ def _get_winexe_command_args ( self , host , username , password , command , domain = None ) : \n 
~~~ args = [ ] \n 
args += [ , ] \n 
if domain : \n 
~~~ args += [ , % ( domain , username , password ) ] \n 
~~~ args += [ , % ( username , password ) ] \n 
~~ args += [ % ( host ) ] \n 
args += [ command ] \n 
~~ def _get_smbclient_command_args ( self , host , username , password , command , share = , \n 
domain = None ) : \n 
values = { : domain , : username , : password } \n 
~~~ auth_string = % values \n 
~~ args += [ , auth_string ] \n 
args += [ % { : host , : share } ] \n 
args += [ , command ] \n 
~~ def _parse_winexe_error ( self , stdout , stderr ) : \n 
~~~ for code , message in ERROR_CODE_TO_MESSAGE_MAP . items ( ) : \n 
~~~ if code in stdout : \n 
~~~ return message \n 
from mistralclient . api . v2 import executions \n 
from mistralclient . api . v2 import tasks \n 
from mistralclient . api . v2 import workbooks \n 
from mistralclient . api . v2 import workflows \n 
import st2tests . config as tests_config \n 
tests_config . parse_args ( ) \n 
cfg . CONF . set_override ( , 100 , group = ) \n 
cfg . CONF . set_override ( , 200 , group = ) \n 
import st2common . bootstrap . runnersregistrar as runners_registrar \n 
from st2actions . runners . localrunner import LocalShellRunner \n 
from st2actions . runners . mistral . v2 import MistralRunner \n 
from st2common . constants import action as action_constants \n 
from st2common . models . api . action import ActionAPI \n 
from st2common . models . db . liveaction import LiveActionDB \n 
from st2common . persistence . action import Action \n 
from st2common . persistence . liveaction import LiveAction \n 
from st2common . services import action as action_service \n 
from st2common . transport . liveaction import LiveActionPublisher \n 
from st2common . transport . publishers import CUDPublisher \n 
from st2tests import DbTestCase \n 
from st2tests . fixturesloader import FixturesLoader \n 
from tests . unit . base import MockLiveActionPublisher \n 
TEST_FIXTURES = { \n 
PACK = \n 
LOADER = FixturesLoader ( ) \n 
FIXTURES = LOADER . load_fixtures ( fixtures_pack = PACK , fixtures_dict = TEST_FIXTURES ) \n 
WB1_YAML_FILE_NAME = TEST_FIXTURES [ ] [ 1 ] \n 
WB1_YAML_FILE_PATH = LOADER . get_fixture_file_path_abs ( PACK , , WB1_YAML_FILE_NAME ) \n 
WB1_SPEC = FIXTURES [ ] [ WB1_YAML_FILE_NAME ] \n 
WB1_YAML = yaml . safe_dump ( WB1_SPEC , default_flow_style = False ) \n 
WB1_NAME = % ( PACK , WB1_YAML_FILE_NAME . replace ( , ) ) \n 
WB1 = workbooks . Workbook ( None , { : WB1_NAME , : WB1_YAML } ) \n 
WB1_MAIN_EXEC = { : str ( uuid . uuid4 ( ) ) , : } \n 
WB1_MAIN_EXEC [ ] = WB1_NAME + \n 
WB1_MAIN_EXEC_ERRORED = copy . deepcopy ( WB1_MAIN_EXEC ) \n 
WB1_MAIN_EXEC_ERRORED [ ] = \n 
WB1_MAIN_TASK1 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WB1_MAIN_TASKS = [ tasks . Task ( None , WB1_MAIN_TASK1 ) ] \n 
WB1_MAIN_TASK_ID = WB1_MAIN_TASK1 [ ] \n 
WB1_SUB1_EXEC = { : str ( uuid . uuid4 ( ) ) , : , : WB1_MAIN_TASK_ID } WB1_SUB1_EXEC [ ] = WB1_NAME + \n 
WB1_SUB1_EXEC_ERRORED = copy . deepcopy ( WB1_SUB1_EXEC ) \n 
WB1_SUB1_EXEC_ERRORED [ ] = \n 
WB1_SUB1_TASK1 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WB1_SUB1_TASK2 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WB1_SUB1_TASKS = [ tasks . Task ( None , WB1_SUB1_TASK1 ) , tasks . Task ( None , WB1_SUB1_TASK2 ) ] \n 
WF1_YAML_FILE_NAME = TEST_FIXTURES [ ] [ 0 ] \n 
WF1_YAML_FILE_PATH = LOADER . get_fixture_file_path_abs ( PACK , , WF1_YAML_FILE_NAME ) \n 
WF1_SPEC = FIXTURES [ ] [ WF1_YAML_FILE_NAME ] \n 
WF1_YAML = yaml . safe_dump ( WF1_SPEC , default_flow_style = False ) \n 
WF1_NAME = % ( PACK , WF1_YAML_FILE_NAME . replace ( , ) ) \n 
WF1 = workflows . Workflow ( None , { : WF1_NAME , : WF1_YAML } ) \n 
WF1_EXEC = { : str ( uuid . uuid4 ( ) ) , : , : WF1_NAME } \n 
WF1_EXEC_NOT_RERUNABLE = copy . deepcopy ( WF1_EXEC ) \n 
WF1_EXEC_NOT_RERUNABLE [ ] = \n 
WF1_TASK1 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WF1_TASK2 = { : str ( uuid . uuid4 ( ) ) , : , : } \n 
WF1_TASKS = [ tasks . Task ( None , WF1_TASK1 ) , tasks . Task ( None , WF1_TASK2 ) ] \n 
ACTION_PARAMS = { : } \n 
NON_EMPTY_RESULT = \n 
@ mock . patch . object ( LocalShellRunner , , mock . \n 
MagicMock ( return_value = ( action_constants . LIVEACTION_STATUS_SUCCEEDED , \n 
NON_EMPTY_RESULT , None ) ) ) \n 
@ mock . patch . object ( CUDPublisher , , mock . MagicMock ( return_value = None ) ) \n 
@ mock . patch . object ( CUDPublisher , , \n 
mock . MagicMock ( side_effect = MockLiveActionPublisher . publish_create ) ) \n 
@ mock . patch . object ( LiveActionPublisher , , \n 
mock . MagicMock ( side_effect = MockLiveActionPublisher . publish_state ) ) \n 
class MistralRunnerTest ( DbTestCase ) : \n 
def setUpClass ( cls ) : \n 
~~~ super ( MistralRunnerTest , cls ) . setUpClass ( ) \n 
runners_registrar . register_runner_types ( ) \n 
for _ , fixture in six . iteritems ( FIXTURES [ ] ) : \n 
~~~ instance = ActionAPI ( ** fixture ) \n 
Action . add_or_update ( ActionAPI . to_model ( instance ) ) \n 
~~ ~~ def setUp ( self ) : \n 
~~~ super ( MistralRunnerTest , self ) . setUp ( ) \n 
cfg . CONF . set_override ( , , group = ) \n 
~~ @ mock . patch . object ( \n 
workflows . WorkflowManager , , \n 
mock . MagicMock ( return_value = [ ] ) ) \n 
@ mock . patch . object ( \n 
mock . MagicMock ( return_value = WF1 ) ) \n 
mock . MagicMock ( return_value = [ WF1 ] ) ) \n 
executions . ExecutionManager , , \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC ) ) ) \n 
MistralRunner , , \n 
mock . MagicMock ( \n 
return_value = ( action_constants . LIVEACTION_STATUS_RUNNING , \n 
{ : [ ] } , \n 
{ : str ( uuid . uuid4 ( ) ) } ) \n 
def test_resume_option ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WF1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction1 , execution1 = action_service . request ( liveaction1 ) \n 
self . assertFalse ( MistralRunner . resume . called ) \n 
: execution1 . id , \n 
: [ ] \n 
liveaction2 = LiveActionDB ( action = WF1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
liveaction2 , execution2 = action_service . request ( liveaction2 ) \n 
liveaction2 = LiveAction . get_by_id ( str ( liveaction2 . id ) ) \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_RUNNING ) \n 
task_specs = { \n 
MistralRunner . resume . assert_called_with ( ex_ref = execution1 , task_specs = task_specs ) \n 
def test_resume_option_reset_tasks ( self ) : \n 
: True \n 
mock . MagicMock ( return_value = executions . Execution ( None , WF1_EXEC_NOT_RERUNABLE ) ) ) \n 
tasks . TaskManager , , \n 
mock . MagicMock ( return_value = WF1_TASKS ) ) \n 
def test_resume_workflow_not_in_rerunable_state ( self ) : \n 
self . assertEqual ( liveaction2 . status , action_constants . LIVEACTION_STATUS_FAILED ) \n 
self . assertIn ( , liveaction2 . result . get ( ) ) \n 
mock . MagicMock ( return_value = [ executions . Execution ( None , WF1_EXEC ) ] ) ) \n 
def test_resume_tasks_not_in_rerunable_state ( self ) : \n 
def test_resume_unidentified_tasks ( self ) : \n 
workbooks . WorkbookManager , , \n 
mock . MagicMock ( return_value = WB1 ) ) \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC ) ) ) \n 
mock . MagicMock ( return_value = executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) ) ) \n 
return_value = [ \n 
executions . Execution ( None , WB1_MAIN_EXEC_ERRORED ) , \n 
executions . Execution ( None , WB1_SUB1_EXEC_ERRORED ) ] ) ) \n 
mock . MagicMock ( side_effect = [ WB1_MAIN_TASKS , WB1_SUB1_TASKS ] ) ) \n 
mock . MagicMock ( return_value = None ) ) \n 
def test_resume_subworkflow_task ( self ) : \n 
~~~ MistralRunner . entry_point = mock . PropertyMock ( return_value = WB1_YAML_FILE_PATH ) \n 
liveaction1 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS ) \n 
liveaction2 = LiveActionDB ( action = WB1_NAME , parameters = ACTION_PARAMS , context = context ) \n 
expected_env = { \n 
: str ( liveaction2 . id ) , \n 
: str ( execution2 . id ) , \n 
: { } , \n 
: context [ ] , \n 
: str ( execution2 . id ) \n 
tasks . TaskManager . rerun . assert_called_with ( \n 
WB1_SUB1_TASK2 [ ] , \n 
reset = False , \n 
env = expected_env \n 
def test_resume_unidentified_subworkflow_task ( self ) : \n 
def test_resume_and_reset_subworkflow_task ( self ) : \n 
reset = True , \n 
from pecan import abort \n 
from mongoengine import ValidationError \n 
from st2api . controllers import resource \n 
from st2api . controllers . v1 . actionviews import ActionViewsController \n 
from st2common import log as logging \n 
from st2common . constants . triggers import ACTION_FILE_WRITTEN_TRIGGER \n 
from st2common . exceptions . action import InvalidActionParameterException \n 
from st2common . exceptions . apivalidation import ValueValidationException \n 
from st2common . models . api . base import jsexpose \n 
from st2common . models . api . action import ActionCreateAPI \n 
from st2common . persistence . pack import Pack \n 
from st2common . validators . api . misc import validate_not_part_of_system_pack \n 
from st2common . content . utils import get_pack_base_path \n 
from st2common . content . utils import get_pack_resource_file_abs_path \n 
from st2common . content . utils import get_relative_path_to_pack \n 
from st2common . transport . reactor import TriggerDispatcher \n 
from st2common . util . system_info import get_host_info \n 
import st2common . validators . api . action as action_validator \n 
from st2common . rbac . types import PermissionType \n 
from st2common . rbac . decorators import request_user_has_permission \n 
from st2common . rbac . decorators import request_user_has_resource_api_permission \n 
from st2common . rbac . decorators import request_user_has_resource_db_permission \n 
http_client = six . moves . http_client \n 
class ActionsController ( resource . ContentPackResourceController ) : \n 
views = ActionViewsController ( ) \n 
model = ActionAPI \n 
access = Action \n 
supported_filters = { \n 
query_options = { \n 
include_reference = True \n 
~~~ super ( ActionsController , self ) . __init__ ( * args , ** kwargs ) \n 
self . _trigger_dispatcher = TriggerDispatcher ( LOG ) \n 
~~ @ request_user_has_permission ( permission_type = PermissionType . ACTION_LIST ) \n 
@ jsexpose ( ) \n 
def get_all ( self , ** kwargs ) : \n 
~~~ return super ( ActionsController , self ) . _get_all ( ** kwargs ) \n 
~~ @ request_user_has_resource_db_permission ( permission_type = PermissionType . ACTION_VIEW ) \n 
@ jsexpose ( arg_types = [ str ] ) \n 
def get_one ( self , ref_or_id ) : \n 
~~~ return super ( ActionsController , self ) . _get_one ( ref_or_id ) \n 
~~ @ jsexpose ( body_cls = ActionCreateAPI , status_code = http_client . CREATED ) \n 
@ request_user_has_resource_api_permission ( permission_type = PermissionType . ACTION_CREATE ) \n 
def post ( self , action ) : \n 
~~~ validate_not_part_of_system_pack ( action ) \n 
action_validator . validate_action ( action ) \n 
~~ except ( ValidationError , ValueError , \n 
ValueValidationException , InvalidActionParameterException ) as e : \n 
~~~ LOG . exception ( , action ) \n 
abort ( http_client . BAD_REQUEST , str ( e ) ) \n 
~~ data_files = getattr ( action , , [ ] ) \n 
written_data_files = [ ] \n 
if data_files : \n 
~~~ written_data_files = self . _handle_data_files ( pack_name = action . pack , \n 
data_files = data_files ) \n 
~~ action_model = ActionAPI . to_model ( action ) \n 
LOG . debug ( , action ) \n 
action_db = Action . add_or_update ( action_model ) \n 
LOG . debug ( , action_db ) \n 
if written_data_files : \n 
~~~ self . _dispatch_trigger_for_written_data_files ( action_db = action_db , \n 
written_data_files = written_data_files ) \n 
~~ extra = { : action_db } \n 
LOG . audit ( % ( action_db . id ) , extra = extra ) \n 
action_api = ActionAPI . from_model ( action_db ) \n 
return action_api \n 
~~ @ request_user_has_resource_db_permission ( permission_type = PermissionType . ACTION_MODIFY ) \n 
@ jsexpose ( arg_types = [ str ] , body_cls = ActionCreateAPI ) \n 
def put ( self , action_ref_or_id , action ) : \n 
~~~ action_db = self . _get_by_ref_or_id ( ref_or_id = action_ref_or_id ) \n 
action_id = action_db . id \n 
if not getattr ( action , , None ) : \n 
~~~ action . pack = action_db . pack \n 
~~ validate_not_part_of_system_pack ( action ) \n 
data_files = getattr ( action , , [ ] ) \n 
~~~ action_db = ActionAPI . to_model ( action ) \n 
action_db . id = action_id \n 
action_db = Action . add_or_update ( action_db ) \n 
~~ except ( ValidationError , ValueError ) as e : \n 
~~ if written_data_files : \n 
~~ action_api = ActionAPI . from_model ( action_db ) \n 
LOG . debug ( , action_api ) \n 
~~ @ request_user_has_resource_db_permission ( permission_type = PermissionType . ACTION_DELETE ) \n 
@ jsexpose ( arg_types = [ str ] , status_code = http_client . NO_CONTENT ) \n 
def delete ( self , action_ref_or_id ) : \n 
action_db = self . _get_by_ref_or_id ( ref_or_id = action_ref_or_id ) \n 
~~~ validate_not_part_of_system_pack ( action_db ) \n 
~~ except ValueValidationException as e : \n 
~~~ abort ( http_client . BAD_REQUEST , str ( e ) ) \n 
~~ LOG . debug ( , \n 
action_ref_or_id , action_db ) \n 
~~~ Action . delete ( action_db ) \n 
, action_id , e ) \n 
abort ( http_client . INTERNAL_SERVER_ERROR , str ( e ) ) \n 
~~ def _handle_data_files ( self , pack_name , data_files ) : \n 
written_file_paths = self . _write_data_files_to_disk ( pack_name = pack_name , \n 
self . _update_pack_model ( pack_name = pack_name , data_files = data_files , \n 
written_file_paths = written_file_paths ) \n 
return written_file_paths \n 
~~ def _write_data_files_to_disk ( self , pack_name , data_files ) : \n 
written_file_paths = [ ] \n 
for data_file in data_files : \n 
~~~ file_path = data_file [ ] \n 
content = data_file [ ] \n 
file_path = get_pack_resource_file_abs_path ( pack_name = pack_name , \n 
resource_type = , \n 
file_path = file_path ) \n 
self . _write_data_file ( pack_name = pack_name , file_path = file_path , content = content ) \n 
written_file_paths . append ( file_path ) \n 
~~ return written_file_paths \n 
~~ def _update_pack_model ( self , pack_name , data_files , written_file_paths ) : \n 
for file_path in written_file_paths : \n 
~~~ file_path = get_relative_path_to_pack ( pack_name = pack_name , file_path = file_path ) \n 
file_paths . append ( file_path ) \n 
~~ pack_db = Pack . get_by_ref ( pack_name ) \n 
pack_db . files = set ( pack_db . files ) \n 
pack_db . files . update ( set ( file_paths ) ) \n 
pack_db . files = list ( pack_db . files ) \n 
pack_db = Pack . add_or_update ( pack_db ) \n 
return pack_db \n 
~~ def _write_data_file ( self , pack_name , file_path , content ) : \n 
pack_base_path = get_pack_base_path ( pack_name = pack_name ) \n 
if not os . path . isdir ( pack_base_path ) : \n 
~~ directory = os . path . dirname ( file_path ) \n 
if not os . path . isdir ( directory ) : \n 
~~~ os . makedirs ( directory ) \n 
~~ with open ( file_path , ) as fp : \n 
~~~ fp . write ( content ) \n 
~~ ~~ def _dispatch_trigger_for_written_data_files ( self , action_db , written_data_files ) : \n 
~~~ trigger = ACTION_FILE_WRITTEN_TRIGGER [ ] \n 
host_info = get_host_info ( ) \n 
for file_path in written_data_files : \n 
~~~ payload = { \n 
: action_db . ref , \n 
: file_path , \n 
: host_info \n 
self . _trigger_dispatcher . dispatch ( trigger = trigger , payload = payload ) \n 
~~ ~~ ~~ import httplib \n 
from st2common . rbac . types import ResourceType \n 
from st2common . persistence . auth import User \n 
from st2common . persistence . rbac import Role \n 
from st2common . persistence . rbac import UserRoleAssignment \n 
from st2common . persistence . rbac import PermissionGrant \n 
from st2common . models . db . auth import UserDB \n 
from st2common . models . db . rbac import RoleDB \n 
from st2common . models . db . rbac import UserRoleAssignmentDB \n 
from st2common . models . db . rbac import PermissionGrantDB \n 
from tests . base import APIControllerWithRBACTestCase \n 
FIXTURES_PACK = \n 
ACTION_2 = { \n 
: { : , : , : 0 } , \n 
: { : , : , : True } \n 
class ActionControllerRBACTestCase ( APIControllerWithRBACTestCase ) : \n 
~~~ fixtures_loader = FixturesLoader ( ) \n 
def setUp ( self ) : \n 
~~~ super ( ActionControllerRBACTestCase , self ) . setUp ( ) \n 
self . fixtures_loader . save_fixtures_to_db ( fixtures_pack = FIXTURES_PACK , \n 
fixtures_dict = TEST_FIXTURES ) \n 
file_name = \n 
ActionControllerRBACTestCase . ACTION_1 = self . fixtures_loader . load_fixtures ( \n 
fixtures_pack = FIXTURES_PACK , \n 
fixtures_dict = { : [ file_name ] } ) [ ] [ file_name ] \n 
self = self \n 
self . users = { } \n 
self . roles = { } \n 
user_1_db = UserDB ( name = ) \n 
user_1_db = User . add_or_update ( user_1_db ) \n 
self . users [ ] = user_1_db \n 
user_2_db = UserDB ( name = ) \n 
user_2_db = User . add_or_update ( user_2_db ) \n 
self . users [ ] = user_2_db \n 
grant_db = PermissionGrantDB ( resource_uid = , \n 
resource_type = ResourceType . PACK , \n 
permission_types = [ PermissionType . ACTION_CREATE ] ) \n 
grant_db = PermissionGrant . add_or_update ( grant_db ) \n 
permission_grants = [ str ( grant_db . id ) ] \n 
role_1_db = RoleDB ( name = , permission_grants = permission_grants ) \n 
role_1_db = Role . add_or_update ( role_1_db ) \n 
self . roles [ ] = role_1_db \n 
user_db = self . users [ ] \n 
role_assignment_db = UserRoleAssignmentDB ( \n 
user = user_db . name , \n 
role = self . roles [ ] . name ) \n 
UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
~~ def test_create_action_no_action_create_permission ( self ) : \n 
~~~ user_db = self . users [ ] \n 
self . use_user ( user_db ) \n 
resp = self . __do_post ( ActionControllerRBACTestCase . ACTION_1 ) \n 
self . assertEqual ( resp . status_code , httplib . FORBIDDEN ) \n 
self . assertEqual ( resp . json [ ] , expected_msg ) \n 
self . use_user ( { } ) \n 
~~ @ mock . patch . object ( action_validator , , mock . MagicMock ( \n 
return_value = True ) ) \n 
def test_create_action_success ( self ) : \n 
resp = self . __do_post ( ACTION_2 ) \n 
self . assertEqual ( resp . status_code , httplib . CREATED ) \n 
def __get_action_id ( resp ) : \n 
~~~ return resp . json [ ] \n 
~~ def __do_post ( self , rule ) : \n 
~~~ return self . app . post_json ( , rule , expect_errors = True ) \n 
~~ def __do_delete ( self , action_id , expect_errors = False ) : \n 
~~~ return self . app . delete ( % action_id , expect_errors = expect_errors ) \n 
~~ ~~ import eventlet \n 
from eventlet import wsgi \n 
from st2common . service_setup import setup as common_setup \n 
from st2common . service_setup import teardown as common_teardown \n 
from st2common . util . monkey_patch import monkey_patch \n 
from st2common . constants . auth import VALID_MODES \n 
from st2auth import config \n 
config . register_opts ( ) \n 
from st2auth import app \n 
monkey_patch ( ) \n 
def _setup ( ) : \n 
~~~ common_setup ( service = , config = config , setup_db = True , register_mq_exchanges = False , \n 
register_signal_handlers = True , register_internal_trigger_types = False , \n 
run_migrations = False ) \n 
if cfg . CONF . auth . mode not in VALID_MODES : \n 
~~~ raise ValueError ( % ( . join ( VALID_MODES ) ) ) \n 
~~ ~~ def _run_server ( ) : \n 
~~~ host = cfg . CONF . auth . host \n 
port = cfg . CONF . auth . port \n 
use_ssl = cfg . CONF . auth . use_ssl \n 
cert_file_path = os . path . realpath ( cfg . CONF . auth . cert ) \n 
key_file_path = os . path . realpath ( cfg . CONF . auth . key ) \n 
if use_ssl and not os . path . isfile ( cert_file_path ) : \n 
~~ if use_ssl and not os . path . isfile ( key_file_path ) : \n 
~~ socket = eventlet . listen ( ( host , port ) ) \n 
if use_ssl : \n 
~~~ socket = eventlet . wrap_ssl ( socket , \n 
certfile = cert_file_path , \n 
keyfile = key_file_path , \n 
server_side = True ) \n 
LOG . info ( , os . getpid ( ) , \n 
if use_ssl else , host , port ) \n 
wsgi . server ( socket , app . setup_app ( ) ) \n 
~~ def _teardown ( ) : \n 
~~~ common_teardown ( ) \n 
~~~ _setup ( ) \n 
return _run_server ( ) \n 
~~ except SystemExit as exit_code : \n 
~~~ sys . exit ( exit_code ) \n 
~~~ LOG . exception ( , os . getpid ( ) ) \n 
~~~ _teardown ( ) \n 
~~ ~~ __all__ = [ \n 
CONFIG = { } \n 
def get_config ( ) : \n 
global CONFIG \n 
return CONFIG \n 
~~ def set_config ( config ) : \n 
CONFIG = config \n 
return config \n 
ALLOWED_EXTS = [ , , , ] \n 
PARSER_FUNCS = { : json . load , : yaml . safe_load , : yaml . safe_load } \n 
def get_fixtures_base_path ( ) : \n 
~~~ return os . path . dirname ( __file__ ) \n 
~~ def load_content ( file_path ) : \n 
file_name , file_ext = os . path . splitext ( file_path ) \n 
if file_ext not in ALLOWED_EXTS : \n 
~~~ raise Exception ( % \n 
( file_ext , file_path , ALLOWED_EXTS ) ) \n 
~~ parser_func = PARSER_FUNCS . get ( file_ext , None ) \n 
with open ( file_path , ) as fd : \n 
~~~ return parser_func ( fd ) if parser_func else fd . read ( ) \n 
~~ ~~ def load_fixtures ( fixtures_dict = None ) : \n 
if fixtures_dict is None : \n 
~~~ fixtures_dict = { } \n 
~~ all_fixtures = { } \n 
fixtures_base_path = get_fixtures_base_path ( ) \n 
for fixture_type , fixtures in six . iteritems ( fixtures_dict ) : \n 
~~~ loaded_fixtures = { } \n 
for fixture in fixtures : \n 
~~~ fixture_path = fixtures_base_path + + fixture \n 
fixture_dict = load_content ( fixture_path ) \n 
loaded_fixtures [ fixture ] = fixture_dict \n 
~~ all_fixtures [ fixture_type ] = loaded_fixtures \n 
~~ return all_fixtures \n 
def do_register_opts ( opts , group = None , ignore_errors = False ) : \n 
~~~ cfg . CONF . register_opts ( opts , group = group ) \n 
~~~ if not ignore_errors : \n 
~~ ~~ ~~ def do_register_cli_opts ( opt , ignore_errors = False ) : \n 
~~~ if not isinstance ( opt , ( list , tuple ) ) : \n 
~~~ opts = [ opt ] \n 
~~~ opts = opt \n 
~~~ cfg . CONF . register_cli_opts ( opts ) \n 
~~ ~~ ~~ def register_opts ( ignore_errors = False ) : \n 
~~~ rbac_opts = [ \n 
cfg . BoolOpt ( , default = False , help = ) , \n 
do_register_opts ( rbac_opts , , ignore_errors ) \n 
system_user_opts = [ \n 
cfg . StrOpt ( , \n 
do_register_opts ( system_user_opts , , ignore_errors ) \n 
schema_opts = [ \n 
cfg . IntOpt ( , default = 4 , help = ) , \n 
do_register_opts ( schema_opts , , ignore_errors ) \n 
system_opts = [ \n 
do_register_opts ( system_opts , , ignore_errors ) \n 
system_packs_base_path = os . path . join ( cfg . CONF . system . base_path , ) \n 
content_opts = [ \n 
cfg . StrOpt ( , default = system_packs_base_path , \n 
cfg . StrOpt ( , default = None , \n 
do_register_opts ( content_opts , , ignore_errors ) \n 
db_opts = [ \n 
cfg . StrOpt ( , default = , help = ) , \n 
cfg . IntOpt ( , default = 27017 , help = ) , \n 
cfg . StrOpt ( , help = ) , \n 
cfg . IntOpt ( , help = , \n 
default = 3 ) , \n 
cfg . IntOpt ( , help = , default = 10 ) , \n 
default = 1 ) , \n 
cfg . BoolOpt ( , help = , default = False ) , \n 
help = , \n 
default = None ) , \n 
cfg . StrOpt ( , help = , \n 
cfg . StrOpt ( , choices = , \n 
cfg . BoolOpt ( , \n 
default = True ) \n 
do_register_opts ( db_opts , , ignore_errors ) \n 
messaging_opts = [ \n 
cfg . ListOpt ( , default = [ ] , \n 
do_register_opts ( messaging_opts , , ignore_errors ) \n 
syslog_opts = [ \n 
cfg . IntOpt ( , default = 514 , \n 
do_register_opts ( syslog_opts , , ignore_errors ) \n 
log_opts = [ \n 
cfg . ListOpt ( , default = , \n 
cfg . BoolOpt ( , default = False , \n 
cfg . BoolOpt ( , default = True , \n 
do_register_opts ( log_opts , , ignore_errors ) \n 
api_opts = [ \n 
cfg . IntOpt ( , default = 9101 , help = ) , \n 
do_register_opts ( api_opts , , ignore_errors ) \n 
keyvalue_opts = [ \n 
help = + \n 
+ \n 
do_register_opts ( keyvalue_opts , group = ) \n 
auth_opts = [ \n 
cfg . IntOpt ( , default = 86400 , help = ) \n 
do_register_opts ( auth_opts , , ignore_errors ) \n 
default_python_bin_path = sys . executable \n 
base_dir = os . path . dirname ( os . path . realpath ( default_python_bin_path ) ) \n 
default_virtualenv_bin_path = os . path . join ( base_dir , ) \n 
action_runner_opts = [ \n 
cfg . StrOpt ( , default = default_python_bin_path , \n 
cfg . StrOpt ( , default = default_virtualenv_bin_path , \n 
do_register_opts ( action_runner_opts , group = ) \n 
action_sensor_opts = [ \n 
do_register_opts ( action_sensor_opts , group = ) \n 
coord_opts = [ \n 
cfg . StrOpt ( , default = None , help = ) , \n 
cfg . IntOpt ( , default = 60 , help = ) \n 
do_register_opts ( coord_opts , , ignore_errors ) \n 
mistral_opts = [ \n 
cfg . StrOpt ( , default = , help = ) , cfg . IntOpt ( , default = 1000 , help = ) , \n 
cfg . IntOpt ( , default = 300000 , help = ) , \n 
cfg . IntOpt ( , default = 600000 , help = ) , \n 
cfg . StrOpt ( , default = None , help = ( \n 
do_register_opts ( mistral_opts , group = , ignore_errors = ignore_errors ) \n 
debug = cfg . BoolOpt ( , default = False , \n 
profile = cfg . BoolOpt ( , default = False , \n 
help = ( \n 
use_debugger = cfg . BoolOpt ( , default = True , \n 
cli_opts = [ debug , profile , use_debugger ] \n 
do_register_cli_opts ( cli_opts , ignore_errors = ignore_errors ) \n 
~~ def parse_args ( args = None ) : \n 
~~~ register_opts ( ) \n 
cfg . CONF ( args = args , version = VERSION_STRING ) \n 
~~ from st2common . exceptions import StackStormBaseException \n 
class InternalServerErrorException ( StackStormBaseException ) : \n 
~~ import six \n 
from st2common . util import isotime \n 
from st2common . models . api . base import BaseAPI \n 
from st2common . models . db . auth import UserDB , TokenDB , ApiKeyDB \n 
def get_system_username ( ) : \n 
~~~ return cfg . CONF . system_user . user \n 
~~ class UserAPI ( BaseAPI ) : \n 
~~~ model = UserDB \n 
schema = { \n 
"title" : "User" , \n 
"type" : "object" , \n 
"properties" : { \n 
"name" : { \n 
"type" : "string" , \n 
"required" : True \n 
"additionalProperties" : False \n 
def to_model ( cls , user ) : \n 
~~~ name = user . name \n 
model = cls . model ( name = name ) \n 
return model \n 
~~ ~~ class TokenAPI ( BaseAPI ) : \n 
~~~ model = TokenDB \n 
"title" : "Token" , \n 
"id" : { \n 
"type" : "string" \n 
"user" : { \n 
"type" : [ "string" , "null" ] \n 
"token" : { \n 
"ttl" : { \n 
"type" : "integer" , \n 
"minimum" : 1 \n 
"expiry" : { \n 
"type" : [ "string" , "null" ] , \n 
"pattern" : isotime . ISO8601_UTC_REGEX \n 
"metadata" : { \n 
"type" : [ "object" , "null" ] \n 
def from_model ( cls , model , mask_secrets = False ) : \n 
~~~ doc = super ( cls , cls ) . _from_model ( model , mask_secrets = mask_secrets ) \n 
doc [ ] = isotime . format ( model . expiry , offset = False ) if model . expiry else None \n 
return cls ( ** doc ) \n 
def to_model ( cls , instance ) : \n 
~~~ user = str ( instance . user ) if instance . user else None \n 
token = str ( instance . token ) if instance . token else None \n 
expiry = isotime . parse ( instance . expiry ) if instance . expiry else None \n 
model = cls . model ( user = user , token = token , expiry = expiry ) \n 
~~ ~~ class ApiKeyAPI ( BaseAPI ) : \n 
~~~ model = ApiKeyDB \n 
"title" : "ApiKey" , \n 
"uid" : { \n 
"default" : "" \n 
"key_hash" : { \n 
: isotime . ISO8601_UTC_REGEX \n 
"enabled" : { \n 
"type" : "boolean" , \n 
"default" : True \n 
doc [ ] = isotime . format ( model . created_at , offset = False ) if model . created_at else None \n 
key_hash = getattr ( instance , , None ) \n 
metadata = getattr ( instance , , { } ) \n 
enabled = bool ( getattr ( instance , , True ) ) \n 
model = cls . model ( user = user , key_hash = key_hash , metadata = metadata , enabled = enabled ) \n 
~~ ~~ class ApiKeyCreateResponseAPI ( BaseAPI ) : \n 
~~~ schema = { \n 
"title" : "APIKeyCreateResponse" , \n 
"key" : { \n 
~~~ doc = cls . _from_model ( model = model , mask_secrets = mask_secrets ) \n 
attrs = { attr : value for attr , value in six . iteritems ( doc ) if value is not None } \n 
attrs [ ] = isotime . format ( model . created_at , offset = False ) if model . created_at else None \n 
attrs . pop ( , None ) \n 
attrs [ ] = None \n 
return cls ( ** attrs ) \n 
~~ ~~ from st2common . models . db . rule import ( ActionExecutionSpecDB , RuleDB ) \n 
from st2common . models . db . sensor import SensorTypeDB \n 
from st2common . models . db . trigger import ( TriggerDB , TriggerTypeDB , TriggerInstanceDB ) \n 
MODELS = [ RuleDB , SensorTypeDB , TriggerDB , TriggerInstanceDB , \n 
TriggerTypeDB ] \n 
from st2common . models . db import MongoDBAccess \n 
from st2common . models . db . marker import MarkerDB \n 
from st2common . models . db . marker import DumperMarkerDB \n 
from st2common . persistence . base import Access \n 
class Marker ( Access ) : \n 
~~~ impl = MongoDBAccess ( MarkerDB ) \n 
publisher = None \n 
def _get_impl ( cls ) : \n 
~~~ return cls . impl \n 
~~ ~~ class DumperMarker ( Access ) : \n 
~~~ impl = MongoDBAccess ( DumperMarkerDB ) \n 
~~ ~~ from st2common . rbac . types import PermissionType \n 
from st2common . rbac . types import SystemRole \n 
def get_all_roles ( exclude_system = False ) : \n 
if exclude_system : \n 
~~~ result = Role . query ( system = False ) \n 
~~~ result = Role . get_all ( ) \n 
~~ def get_system_roles ( ) : \n 
result = Role . query ( system = True ) \n 
~~ def get_roles_for_user ( user_db ) : \n 
role_names = UserRoleAssignment . query ( user = user_db . name ) . only ( ) . scalar ( ) \n 
result = Role . query ( name__in = role_names ) \n 
~~ def get_role_assignments_for_user ( user_db ) : \n 
result = UserRoleAssignment . query ( user = user_db . name ) \n 
~~ def get_role_by_name ( name ) : \n 
result = Role . get ( name = name ) \n 
~~ def create_role ( name , description = None ) : \n 
if name in SystemRole . get_valid_values ( ) : \n 
~~ role_db = RoleDB ( name = name , description = description ) \n 
role_db = Role . add_or_update ( role_db ) \n 
return role_db \n 
~~ def delete_role ( name ) : \n 
~~~ raise ValueError ( ) \n 
~~ role_db = Role . get ( name = name ) \n 
result = Role . delete ( role_db ) \n 
~~ def assign_role_to_user ( role_db , user_db , description = None ) : \n 
role_assignment_db = UserRoleAssignmentDB ( user = user_db . name , role = role_db . name , \n 
description = description ) \n 
role_assignment_db = UserRoleAssignment . add_or_update ( role_assignment_db ) \n 
return role_assignment_db \n 
~~ def revoke_role_from_user ( role_db , user_db ) : \n 
role_assignment_db = UserRoleAssignment . get ( user = user_db . name , role = role_db . name ) \n 
result = UserRoleAssignment . delete ( role_assignment_db ) \n 
~~ def get_all_permission_grants_for_user ( user_db , resource_uid = None , resource_types = None , \n 
permission_types = None ) : \n 
permission_grant_ids = Role . query ( name__in = role_names ) . scalar ( ) \n 
permission_grant_ids = sum ( permission_grant_ids , [ ] ) \n 
permission_grants_filters = { } \n 
permission_grants_filters [ ] = permission_grant_ids \n 
if resource_uid : \n 
~~~ permission_grants_filters [ ] = resource_uid \n 
~~ if resource_types : \n 
~~~ permission_grants_filters [ ] = resource_types \n 
~~ if permission_types : \n 
~~~ permission_grants_filters [ ] = permission_types \n 
~~ permission_grant_dbs = PermissionGrant . query ( ** permission_grants_filters ) \n 
return permission_grant_dbs \n 
~~ def create_permission_grant_for_resource_db ( role_db , resource_db , permission_types ) : \n 
permission_types = _validate_permission_types ( resource_db = resource_db , \n 
permission_types = permission_types ) \n 
resource_uid = resource_db . get_uid ( ) \n 
resource_type = resource_db . get_resource_type ( ) \n 
result = create_permission_grant ( role_db = role_db , resource_uid = resource_uid , \n 
resource_type = resource_type , \n 
~~ def create_permission_grant ( role_db , resource_uid , resource_type , permission_types ) : \n 
permission_grant_db = PermissionGrantDB ( resource_uid = resource_uid , \n 
permission_grant_db = PermissionGrant . add_or_update ( permission_grant_db ) \n 
role_db . update ( push__permission_grants = permission_grant_db . id ) \n 
return permission_grant_db \n 
~~ def remove_permission_grant_for_resource_db ( role_db , resource_db , permission_types ) : \n 
permission_grant_db = PermissionGrant . get ( resource_uid = resource_uid , \n 
role_db . update ( pull__permission_grants = permission_grant_db . id ) \n 
~~ def _validate_resource_type ( resource_db ) : \n 
valid_resource_types = ResourceType . get_valid_values ( ) \n 
if resource_type not in valid_resource_types : \n 
~~~ raise ValueError ( % \n 
( resource_type ) ) \n 
~~ return resource_db \n 
~~ def _validate_permission_types ( resource_db , permission_types ) : \n 
resource_db = _validate_resource_type ( resource_db = resource_db ) \n 
valid_permission_types = PermissionType . get_valid_permissions_for_resource_type ( resource_type ) \n 
for permission_type in permission_types : \n 
~~~ if permission_type not in valid_permission_types : \n 
~~~ raise ValueError ( % ( permission_type ) ) \n 
~~ ~~ return permission_types \n 
~~ import binascii \n 
def symmetric_encrypt ( encrypt_key , message ) : \n 
return binascii . hexlify ( encrypt_key . Encrypt ( message ) ) . upper ( ) \n 
~~ def symmetric_decrypt ( decrypt_key , crypto ) : \n 
return decrypt_key . Decrypt ( binascii . unhexlify ( crypto ) ) \n 
from st2common . models . db . stormbase import UIDFieldMixin \n 
def parse_uid ( uid ) : \n 
if UIDFieldMixin . UID_SEPARATOR not in uid : \n 
~~~ raise ValueError ( % ( uid ) ) \n 
~~ parsed = uid . split ( UIDFieldMixin . UID_SEPARATOR ) \n 
if len ( parsed ) < 2 : \n 
~~ resource_type = parsed [ 0 ] \n 
uid_remainder = parsed [ 1 : ] \n 
return ( resource_type , uid_remainder ) \n 
from st2common . bootstrap import aliasesregistrar \n 
from st2tests import DbTestCase , fixturesloader \n 
ALIASES_FIXTURE_PACK_PATH = os . path . join ( fixturesloader . get_fixtures_base_path ( ) , ) \n 
ALIASES_FIXTURE_PATH = os . path . join ( ALIASES_FIXTURE_PACK_PATH , ) \n 
class TestAliasRegistrar ( DbTestCase ) : \n 
~~~ def test_alias_registration ( self ) : \n 
~~~ count = aliasesregistrar . register_aliases ( pack_dir = ALIASES_FIXTURE_PACK_PATH ) \n 
self . assertEqual ( count , len ( os . listdir ( ALIASES_FIXTURE_PATH ) ) ) \n 
~~ ~~ from st2tests . base import CleanDbTestCase \n 
from st2common . models . db . keyvalue import KeyValuePairDB \n 
from st2common . persistence . keyvalue import KeyValuePair \n 
from st2common . services . keyvalues import KeyValueLookup \n 
class TestKeyValueLookup ( CleanDbTestCase ) : \n 
~~~ def test_non_hierarchical_lookup ( self ) : \n 
~~~ k1 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k2 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
k3 = KeyValuePair . add_or_update ( KeyValuePairDB ( name = , value = ) ) \n 
lookup = KeyValueLookup ( ) \n 
self . assertEquals ( str ( lookup . k1 ) , k1 . value ) \n 
self . assertEquals ( str ( lookup . k2 ) , k2 . value ) \n 
self . assertEquals ( str ( lookup . k3 ) , k3 . value ) \n 
~~ def test_hierarchical_lookup_dotted ( self ) : \n 
self . assertEquals ( str ( lookup . a . b ) , k1 . value ) \n 
self . assertEquals ( str ( lookup . a . b . c ) , k2 . value ) \n 
self . assertEquals ( str ( lookup . b . c ) , k3 . value ) \n 
self . assertEquals ( str ( lookup . a ) , ) \n 
~~ def test_hierarchical_lookup_dict ( self ) : \n 
self . assertEquals ( str ( lookup [ ] [ ] ) , k1 . value ) \n 
self . assertEquals ( str ( lookup [ ] [ ] [ ] ) , k2 . value ) \n 
self . assertEquals ( str ( lookup [ ] [ ] ) , k3 . value ) \n 
self . assertEquals ( str ( lookup [ ] ) , ) \n 
~~ def test_missing_key_lookup ( self ) : \n 
~~~ lookup = KeyValueLookup ( ) \n 
self . assertEquals ( str ( lookup . missing_key ) , ) \n 
self . assertTrue ( lookup . missing_key , ) \n 
~~ def test_secret_lookup ( self ) : \n 
~~~ secret_value = + \n 
k1 = KeyValuePair . add_or_update ( KeyValuePairDB ( \n 
name = , value = secret_value , \n 
secret = True , encrypted = True ) \n 
import bson \n 
from st2common . triggers import register_internal_trigger_types \n 
from st2common . persistence . rule import Rule \n 
from st2common . persistence . rule_enforcement import RuleEnforcement \n 
from st2common . models . db . rule import RuleDB \n 
from st2common . models . db . rule_enforcement import RuleEnforcementDB \n 
from st2common . rbac . resolvers import RuleEnforcementPermissionsResolver \n 
from tests . unit . test_rbac_resolvers import BasePermissionsResolverTestCase \n 
class RuleEnforcementPermissionsResolverTestCase ( BasePermissionsResolverTestCase ) : \n 
~~~ super ( RuleEnforcementPermissionsResolverTestCase , self ) . setUp ( ) \n 
register_internal_trigger_types ( ) \n 
user_3_db = UserDB ( name = ) \n 
user_3_db = User . add_or_update ( user_3_db ) \n 
self . users [ ] = user_3_db \n 
user_4_db = UserDB ( name = ) \n 
user_4_db = User . add_or_update ( user_4_db ) \n 
self . users [ ] = user_4_db \n 
user_5_db = UserDB ( name = ) \n 
user_5_db = User . add_or_update ( user_5_db ) \n 
self . users [ ] = user_5_db \n 
user_6_db = UserDB ( name = ) \n 
user_6_db = User . add_or_update ( user_6_db ) \n 
self . users [ ] = user_6_db \n 
user_7_db = UserDB ( name = ) \n 
user_7_db = User . add_or_update ( user_7_db ) \n 
self . users [ ] = user_7_db \n 
user_8_db = UserDB ( name = ) \n 
user_8_db = User . add_or_update ( user_8_db ) \n 
self . users [ ] = user_8_db \n 
user_9_db = UserDB ( name = ) \n 
user_9_db = User . add_or_update ( user_9_db ) \n 
self . users [ ] = user_9_db \n 
user_10_db = UserDB ( name = ) \n 
user_10_db = User . add_or_update ( user_10_db ) \n 
self . users [ ] = user_10_db \n 
rule_1_db = RuleDB ( pack = , name = , action = { : } , \n 
trigger = ) \n 
rule_1_db = Rule . add_or_update ( rule_1_db ) \n 
self . resources [ ] = rule_1_db \n 
rule_enforcement_1_db = RuleEnforcementDB ( trigger_instance_id = str ( bson . ObjectId ( ) ) , \n 
execution_id = str ( bson . ObjectId ( ) ) , \n 
rule = { : rule_1_db . ref , \n 
: rule_1_db . uid , \n 
: str ( rule_1_db . id ) } ) \n 
rule_enforcement_1_db = RuleEnforcement . add_or_update ( rule_enforcement_1_db ) \n 
self . resources [ ] = rule_enforcement_1_db \n 
rule_2_db = RuleDB ( pack = , name = ) \n 
rule_2_db = Rule . add_or_update ( rule_2_db ) \n 
self . resources [ ] = rule_2_db \n 
rule_enforcement_2_db = RuleEnforcementDB ( trigger_instance_id = str ( bson . ObjectId ( ) ) , \n 
rule = { : rule_2_db . ref , \n 
: rule_2_db . uid , \n 
: str ( rule_2_db . id ) } ) \n 
rule_enforcement_2_db = RuleEnforcement . add_or_update ( rule_enforcement_2_db ) \n 
self . resources [ ] = rule_enforcement_2_db \n 
rule_3_db = RuleDB ( pack = , name = ) \n 
rule_3_db = Rule . add_or_update ( rule_3_db ) \n 
self . resources [ ] = rule_3_db \n 
rule_enforcement_3_db = RuleEnforcementDB ( trigger_instance_id = str ( bson . ObjectId ( ) ) , \n 
rule = { : rule_3_db . ref , \n 
: rule_3_db . uid , \n 
: str ( rule_3_db . id ) } ) \n 
rule_enforcement_3_db = RuleEnforcement . add_or_update ( rule_enforcement_3_db ) \n 
self . resources [ ] = rule_enforcement_3_db \n 
grant_db = PermissionGrantDB ( resource_uid = self . resources [ ] . get_uid ( ) , \n 
permission_types = [ PermissionType . RULE_VIEW ] ) \n 
role_3_db = RoleDB ( name = , \n 
permission_grants = permission_grants ) \n 
role_3_db = Role . add_or_update ( role_3_db ) \n 
self . roles [ ] = role_3_db \n 
resource_type = ResourceType . RULE , \n 
role_4_db = RoleDB ( name = , permission_grants = permission_grants ) \n 
role_4_db = Role . add_or_update ( role_4_db ) \n 
self . roles [ ] = role_4_db \n 
permission_types = [ PermissionType . RULE_ALL ] ) \n 
role_4_db = RoleDB ( name = , \n 
permission_types = [ PermissionType . RULE_MODIFY ] ) \n 
role_5_db = RoleDB ( name = , \n 
role_5_db = Role . add_or_update ( role_5_db ) \n 
self . roles [ ] = role_5_db \n 
permission_types = [ PermissionType . RULE_CREATE ] ) \n 
role_6_db = RoleDB ( name = , \n 
role_6_db = Role . add_or_update ( role_6_db ) \n 
self . roles [ ] = role_6_db \n 
role_7_db = RoleDB ( name = , \n 
role_7_db = Role . add_or_update ( role_7_db ) \n 
self . roles [ ] = role_7_db \n 
role_8_db = RoleDB ( name = , \n 
role_8_db = Role . add_or_update ( role_8_db ) \n 
self . roles [ ] = role_8_db \n 
role_9_db = RoleDB ( name = , \n 
role_9_db = Role . add_or_update ( role_9_db ) \n 
self . roles [ ] = role_9_db \n 
grant_db = PermissionGrantDB ( resource_uid = None , \n 
resource_type = None , \n 
permission_types = [ PermissionType . RULE_LIST ] ) \n 
role_10_db = RoleDB ( name = , \n 
role_10_db = Role . add_or_update ( role_10_db ) \n 
self . roles [ ] = role_10_db \n 
role_assignment_db = UserRoleAssignmentDB ( user = user_db . name , \n 
~~ def test_user_has_permission ( self ) : \n 
~~~ resolver = RuleEnforcementPermissionsResolver ( ) \n 
permission_type = PermissionType . RULE_ENFORCEMENT_LIST \n 
self . assertTrue ( resolver . user_has_permission ( user_db = user_db , \n 
permission_type = permission_type ) ) \n 
self . assertFalse ( resolver . user_has_permission ( user_db = user_db , \n 
~~ def test_user_has_resource_db_permission ( self ) : \n 
all_permission_types = PermissionType . get_valid_permissions_for_resource_type ( \n 
ResourceType . RULE_ENFORCEMENT ) \n 
resource_db = self . resources [ ] \n 
self . assertTrue ( self . _user_has_resource_db_permissions ( \n 
resolver = resolver , \n 
user_db = user_db , \n 
resource_db = resource_db , \n 
permission_types = all_permission_types ) ) \n 
self . assertTrue ( resolver . user_has_resource_db_permission ( \n 
resource_db = self . resources [ ] , \n 
permission_type = PermissionType . RULE_ENFORCEMENT_VIEW ) ) \n 
self . assertFalse ( self . _user_has_resource_db_permissions ( \n 
self . assertFalse ( resolver . user_has_resource_db_permission ( \n 
import tarfile \n 
import httplib \n 
import gnupg \n 
import st2common \n 
from st2common . content . utils import get_packs_base_paths \n 
from st2common import __version__ as st2_version \n 
from st2common import config \n 
from st2common . util import date as date_utils \n 
from st2common . util . shell import run_command \n 
from st2debug . constants import GPG_KEY \n 
from st2debug . constants import GPG_KEY_FINGERPRINT \n 
from st2debug . constants import S3_BUCKET_URL \n 
from st2debug . constants import COMPANY_NAME \n 
from st2debug . constants import ARG_NAMES \n 
from st2debug . utils . fs import copy_files \n 
from st2debug . utils . fs import get_full_file_list \n 
from st2debug . utils . fs import get_dirs_in_path \n 
from st2debug . utils . fs import remove_file \n 
from st2debug . utils . system_info import get_cpu_info \n 
from st2debug . utils . system_info import get_memory_info \n 
from st2debug . utils . system_info import get_package_list \n 
from st2debug . utils . git_utils import get_repo_latest_revision_hash \n 
from st2debug . processors import process_st2_config \n 
from st2debug . processors import process_mistral_config \n 
from st2debug . processors import process_content_pack_dir \n 
GPG_INSTALLED = find_executable ( ) is not None \n 
LOG_FILE_PATHS = [ \n 
ST2_CONFIG_FILE_PATH = \n 
MISTRAL_CONFIG_FILE_PATH = \n 
SHELL_COMMANDS = [ ] \n 
DIRECTORY_STRUCTURE = [ \n 
OUTPUT_PATHS = { \n 
ST2_CONF_OPTIONS_TO_REMOVE = { \n 
REMOVE_VALUE_NAME = \n 
OUTPUT_FILENAME_TEMPLATE = \n 
DATE_FORMAT = \n 
~~~ config . parse_args ( args = [ ] ) \n 
~~ def setup_logging ( ) : \n 
~~~ root = LOG \n 
root . setLevel ( logging . INFO ) \n 
ch = logging . StreamHandler ( sys . stdout ) \n 
ch . setLevel ( logging . DEBUG ) \n 
ch . setFormatter ( formatter ) \n 
root . addHandler ( ch ) \n 
~~ class DebugInfoCollector ( object ) : \n 
~~~ def __init__ ( self , include_logs , include_configs , include_content , include_system_info , \n 
include_shell_commands = False , user_info = None , debug = False , config_file = None , \n 
output_path = None ) : \n 
self . include_logs = include_logs \n 
self . include_configs = include_configs \n 
self . include_content = include_content \n 
self . include_system_info = include_system_info \n 
self . include_shell_commands = include_shell_commands \n 
self . user_info = user_info \n 
self . debug = debug \n 
self . output_path = output_path \n 
config_file = config_file or { } \n 
self . st2_config_file_path = config_file . get ( , ST2_CONFIG_FILE_PATH ) \n 
self . mistral_config_file_path = config_file . get ( , \n 
MISTRAL_CONFIG_FILE_PATH ) \n 
self . log_file_paths = config_file . get ( , LOG_FILE_PATHS [ : ] ) \n 
self . gpg_key = config_file . get ( , GPG_KEY ) \n 
self . gpg_key_fingerprint = config_file . get ( , GPG_KEY_FINGERPRINT ) \n 
self . s3_bucket_url = config_file . get ( , S3_BUCKET_URL ) \n 
self . company_name = config_file . get ( , COMPANY_NAME ) \n 
self . shell_commands = config_file . get ( , SHELL_COMMANDS ) \n 
self . st2_config_file_name = os . path . basename ( self . st2_config_file_path ) \n 
self . mistral_config_file_name = os . path . basename ( self . mistral_config_file_path ) \n 
self . config_file_paths = [ \n 
self . st2_config_file_path , \n 
self . mistral_config_file_path \n 
~~ def run ( self , encrypt = False , upload = False , existing_file = None ) : \n 
temp_files = [ ] \n 
~~~ if existing_file : \n 
~~~ working_file = existing_file \n 
~~~ working_file = self . create_archive ( ) \n 
if not encrypt and not upload : \n 
~~~ LOG . info ( \n 
% working_file ) \n 
~~~ temp_files . append ( working_file ) \n 
~~ ~~ if encrypt : \n 
~~~ working_file = self . encrypt_archive ( archive_file_path = working_file ) \n 
if not upload : \n 
~~~ LOG . info ( % \n 
working_file ) \n 
~~ ~~ if upload : \n 
~~~ self . upload_archive ( archive_file_path = working_file ) \n 
tarball_name = os . path . basename ( working_file ) \n 
LOG . info ( % \n 
( self . company_name , tarball_name ) ) \n 
LOG . info ( \n 
% tarball_name ) \n 
~~~ for temp_file in temp_files : \n 
~~~ assert temp_file . startswith ( ) \n 
remove_file ( file_path = temp_file ) \n 
~~ ~~ ~~ def create_archive ( self ) : \n 
~~~ temp_dir_path = self . create_temp_directories ( ) \n 
output_paths = { } \n 
for key , path in OUTPUT_PATHS . iteritems ( ) : \n 
~~~ output_paths [ key ] = os . path . join ( temp_dir_path , path ) \n 
~~ LOG . info ( ) \n 
if self . include_logs : \n 
~~~ self . collect_logs ( output_paths [ ] ) \n 
~~ if self . include_configs : \n 
~~~ self . collect_config_files ( output_paths [ ] ) \n 
~~ if self . include_content : \n 
~~~ self . collect_pack_content ( output_paths [ ] ) \n 
~~ if self . include_system_info : \n 
~~~ self . add_system_information ( output_paths [ ] ) \n 
~~ if self . user_info : \n 
~~~ self . add_user_info ( output_paths [ ] ) \n 
~~ if self . include_shell_commands : \n 
~~~ self . add_shell_command_output ( output_paths [ ] ) \n 
~~ return self . create_tarball ( temp_dir_path ) \n 
~~~ LOG . exception ( , exc_info = True ) \n 
raise e \n 
~~ ~~ def encrypt_archive ( self , archive_file_path ) : \n 
~~~ assert archive_file_path . endswith ( ) \n 
LOG . info ( ) \n 
gpg = gnupg . GPG ( verbose = self . debug ) \n 
import_result = gpg . import_keys ( self . gpg_key ) \n 
assert import_result . count == 1 \n 
encrypted_archive_output_file_name = os . path . basename ( archive_file_path ) + \n 
encrypted_archive_output_file_path = os . path . join ( , \n 
encrypted_archive_output_file_name ) \n 
with open ( archive_file_path , ) as fp : \n 
~~~ gpg . encrypt_file ( file = fp , \n 
recipients = self . gpg_key_fingerprint , \n 
always_trust = True , \n 
output = encrypted_archive_output_file_path ) \n 
~~ return encrypted_archive_output_file_path \n 
~~ ~~ def upload_archive ( self , archive_file_path ) : \n 
LOG . debug ( ) \n 
file_name = os . path . basename ( archive_file_path ) \n 
url = self . s3_bucket_url + file_name \n 
assert url . startswith ( ) \n 
~~~ response = requests . put ( url = url , files = { : fp } ) \n 
~~ assert response . status_code == httplib . OK \n 
~~~ LOG . exception ( % self . company_name , exc_info = True ) \n 
~~ ~~ def collect_logs ( self , output_path ) : \n 
for file_path_glob in self . log_file_paths : \n 
~~~ log_file_list = get_full_file_list ( file_path_glob = file_path_glob ) \n 
copy_files ( file_paths = log_file_list , destination = output_path ) \n 
~~ ~~ def collect_config_files ( self , output_path ) : \n 
copy_files ( file_paths = self . config_file_paths , destination = output_path ) \n 
st2_config_path = os . path . join ( output_path , self . st2_config_file_name ) \n 
process_st2_config ( config_path = st2_config_path ) \n 
mistral_config_path = os . path . join ( output_path , self . mistral_config_file_name ) \n 
process_mistral_config ( config_path = mistral_config_path ) \n 
def collect_pack_content ( output_path ) : \n 
packs_base_paths = get_packs_base_paths ( ) \n 
for index , packs_base_path in enumerate ( packs_base_paths , 1 ) : \n 
~~~ dst = os . path . join ( output_path , % index ) \n 
~~~ shutil . copytree ( src = packs_base_path , dst = dst ) \n 
~~ ~~ base_pack_dirs = get_dirs_in_path ( file_path = output_path ) \n 
for base_pack_dir in base_pack_dirs : \n 
~~~ pack_dirs = get_dirs_in_path ( file_path = base_pack_dir ) \n 
for pack_dir in pack_dirs : \n 
~~~ process_content_pack_dir ( pack_dir = pack_dir ) \n 
~~ ~~ ~~ def add_system_information ( self , output_path ) : \n 
system_information = yaml . dump ( self . get_system_information ( ) , \n 
default_flow_style = False ) \n 
with open ( output_path , ) as fp : \n 
~~~ fp . write ( system_information ) \n 
~~ ~~ def add_user_info ( self , output_path ) : \n 
user_info = yaml . dump ( self . user_info , default_flow_style = False ) \n 
~~~ fp . write ( user_info ) \n 
~~ ~~ def add_shell_command_output ( self , output_path ) : \n 
for cmd in self . shell_commands : \n 
~~~ output_file = os . path . join ( output_path , % self . format_output_filename ( cmd ) ) \n 
exit_code , stdout , stderr = run_command ( cmd = cmd , shell = True ) \n 
with open ( output_file , ) as fp : \n 
~~~ fp . write ( ) \n 
fp . write ( stdout ) \n 
fp . write ( ) \n 
fp . write ( stderr ) \n 
~~ ~~ ~~ def create_tarball ( self , temp_dir_path ) : \n 
if self . output_path : \n 
~~~ output_file_path = self . output_path \n 
~~~ date = date_utils . get_datetime_utc_now ( ) . strftime ( DATE_FORMAT ) \n 
values = { : socket . gethostname ( ) , : date } \n 
output_file_name = OUTPUT_FILENAME_TEMPLATE % values \n 
output_file_path = os . path . join ( , output_file_name ) \n 
~~ with tarfile . open ( output_file_path , ) as tar : \n 
~~~ tar . add ( temp_dir_path , arcname = ) \n 
~~ return output_file_path \n 
def create_temp_directories ( ) : \n 
temp_dir_path = tempfile . mkdtemp ( ) \n 
for directory_name in DIRECTORY_STRUCTURE : \n 
~~~ full_path = os . path . join ( temp_dir_path , directory_name ) \n 
os . mkdir ( full_path ) \n 
~~ return temp_dir_path \n 
def format_output_filename ( cmd ) : \n 
def get_system_information ( ) : \n 
system_information = { \n 
: socket . gethostname ( ) , \n 
: { } \n 
system_information [ ] [ ] = platform . system ( ) \n 
system_information [ ] [ ] = platform . release ( ) \n 
system_information [ ] [ ] = platform . platform ( ) \n 
system_information [ ] [ ] = . join ( platform . architecture ( ) ) \n 
~~~ distribution = . join ( platform . linux_distribution ( ) ) \n 
system_information [ ] [ ] = distribution \n 
~~ system_information [ ] [ ] = sys . version . split ( ) [ 0 ] \n 
cpu_info = get_cpu_info ( ) \n 
if cpu_info : \n 
~~~ core_count = len ( cpu_info ) \n 
model = cpu_info [ 0 ] [ ] \n 
system_information [ ] [ ] = { \n 
: core_count , \n 
: model \n 
~~~ system_information [ ] [ ] = \n 
~~ memory_info = get_memory_info ( ) \n 
if memory_info : \n 
~~~ total = memory_info [ ] / 1024 \n 
free = memory_info [ ] / 1024 \n 
used = ( total - free ) \n 
: total , \n 
: used , \n 
: free \n 
~~ system_information [ ] [ ] = st2_version \n 
st2common_path = st2common . __file__ \n 
st2common_path = os . path . dirname ( st2common_path ) \n 
if in st2common_path : \n 
~~~ base_install_path = st2common_path . replace ( , ) \n 
revision_hash = get_repo_latest_revision_hash ( repo_path = base_install_path ) \n 
system_information [ ] [ ] = \n 
system_information [ ] [ ] = revision_hash \n 
~~~ package_list = get_package_list ( name_startswith = ) \n 
system_information [ ] [ ] = package_list \n 
~~ repo_path = \n 
revision_hash = get_repo_latest_revision_hash ( repo_path = repo_path ) \n 
return system_information \n 
~~~ parser = argparse . ArgumentParser ( description = ) \n 
parser . add_argument ( , action = , default = False , \n 
parser . add_argument ( , action = , default = None , \n 
setup_logging ( ) \n 
abort = True \n 
for arg_name in ARG_NAMES : \n 
~~~ abort &= getattr ( args , arg_name , False ) \n 
~~ if abort : \n 
sys . exit ( 2 ) \n 
~~ if args . config : \n 
~~~ with open ( args . config , ) as yaml_file : \n 
~~~ config_file = yaml . safe_load ( yaml_file ) \n 
~~~ LOG . error ( % e ) \n 
~~ if not isinstance ( config_file , dict ) : \n 
~~~ LOG . error ( ) \n 
~~~ config_file = { } \n 
~~ company_name = config_file . get ( , COMPANY_NAME ) \n 
encrypt = True \n 
upload = True \n 
if args . review : \n 
~~~ encrypt = False \n 
upload = False \n 
~~ if encrypt : \n 
~~~ if not GPG_INSTALLED : \n 
raise ValueError ( msg ) \n 
~~ ~~ if not args . yes and not args . existing_file and upload : \n 
~~~ submitted_content = [ name . replace ( , ) for name in ARG_NAMES if \n 
not getattr ( args , name , False ) ] \n 
submitted_content = . join ( submitted_content ) \n 
print ( % ( company_name , \n 
submitted_content ) ) \n 
value = six . moves . input ( ) \n 
if value . strip ( ) . lower ( ) not in [ , ] : \n 
~~ ~~ user_info = { } \n 
if not args . yes and not args . existing_file : \n 
~~~ print ( \n 
if value . strip ( ) . lower ( ) in [ , ] : \n 
~~~ user_info [ ] = six . moves . input ( ) \n 
user_info [ ] = six . moves . input ( ) \n 
~~ ~~ debug_collector = DebugInfoCollector ( include_logs = not args . exclude_logs , \n 
include_configs = not args . exclude_configs , \n 
include_content = not args . exclude_content , \n 
include_system_info = not args . exclude_system_info , \n 
include_shell_commands = not args . exclude_shell_commands , \n 
user_info = user_info , \n 
debug = args . debug , \n 
config_file = config_file , \n 
output_path = args . output ) \n 
debug_collector . run ( encrypt = encrypt , upload = upload , existing_file = args . existing_file ) \n 
from st2reactor . container . process_container import ProcessSensorContainer \n 
from st2common . services . sensor_watcher import SensorWatcher \n 
from st2common . models . system . common import ResourceReference \n 
class SensorContainerManager ( object ) : \n 
~~~ def __init__ ( self , sensors_partitioner ) : \n 
~~~ self . _sensor_container = None \n 
self . _sensors_watcher = SensorWatcher ( create_handler = self . _handle_create_sensor , \n 
update_handler = self . _handle_update_sensor , \n 
delete_handler = self . _handle_delete_sensor , \n 
queue_suffix = ) \n 
self . _container_thread = None \n 
if not sensors_partitioner : \n 
~~ self . _sensors_partitioner = sensors_partitioner \n 
~~ def run_sensors ( self ) : \n 
sensors = self . _sensors_partitioner . get_sensors ( ) \n 
if sensors : \n 
~~~ LOG . info ( , len ( sensors ) ) \n 
LOG . info ( , [ self . _get_sensor_ref ( sensor ) for sensor in sensors ] ) \n 
~~ sensors_to_run = [ ] \n 
for sensor in sensors : \n 
~~~ sensors_to_run . append ( self . _to_sensor_object ( sensor ) ) \n 
~~ LOG . info ( , os . getpid ( ) ) \n 
self . _setup_sigterm_handler ( ) \n 
self . _spin_container_and_wait ( sensors_to_run ) \n 
~~ def _spin_container_and_wait ( self , sensors ) : \n 
~~~ self . _sensor_container = ProcessSensorContainer ( sensors = sensors ) \n 
self . _container_thread = eventlet . spawn ( self . _sensor_container . run ) \n 
self . _sensors_watcher . start ( ) \n 
exit_code = self . _container_thread . wait ( ) \n 
LOG . error ( , exit_code ) \n 
LOG . error ( , os . getpid ( ) ) \n 
~~ except ( KeyboardInterrupt , SystemExit ) : \n 
~~~ self . _sensor_container . shutdown ( ) \n 
self . _sensors_watcher . stop ( ) \n 
sys . exc_info ( ) [ 0 ] . __name__ ) \n 
eventlet . kill ( self . _container_thread ) \n 
~~ ~~ def _setup_sigterm_handler ( self ) : \n 
~~~ def sigterm_handler ( signum = None , frame = None ) : \n 
~~ signal . signal ( signal . SIGTERM , sigterm_handler ) \n 
~~ def _to_sensor_object ( self , sensor_db ) : \n 
~~~ file_path = sensor_db . artifact_uri . replace ( , ) \n 
class_name = sensor_db . entry_point . split ( ) [ - 1 ] \n 
sensor_obj = { \n 
: sensor_db . pack , \n 
: class_name , \n 
: sensor_db . trigger_types , \n 
: sensor_db . poll_interval , \n 
: self . _get_sensor_ref ( sensor_db ) \n 
return sensor_obj \n 
################################################# \n 
~~ def _handle_create_sensor ( self , sensor ) : \n 
~~~ if not self . _sensors_partitioner . is_sensor_owner ( sensor ) : \n 
~~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
~~ if not sensor . enabled : \n 
~~ LOG . info ( , self . _get_sensor_ref ( sensor ) ) \n 
self . _sensor_container . add_sensor ( sensor = self . _to_sensor_object ( sensor ) ) \n 
~~ def _handle_update_sensor ( self , sensor ) : \n 
~~ sensor_ref = self . _get_sensor_ref ( sensor ) \n 
sensor_obj = self . _to_sensor_object ( sensor ) \n 
if not sensor . enabled : \n 
~~~ LOG . info ( , sensor_ref ) \n 
self . _sensor_container . remove_sensor ( sensor = sensor_obj ) \n 
~~ LOG . info ( , sensor_ref ) \n 
~~~ self . _sensor_container . remove_sensor ( sensor = sensor_obj ) \n 
~~~ LOG . exception ( , sensor_ref ) \n 
~~~ self . _sensor_container . add_sensor ( sensor = sensor_obj ) \n 
LOG . info ( , sensor_ref ) \n 
~~ ~~ def _handle_delete_sensor ( self , sensor ) : \n 
self . _sensor_container . remove_sensor ( sensor = self . _to_sensor_object ( sensor ) ) \n 
~~ def _get_sensor_ref ( self , sensor ) : \n 
~~~ return ResourceReference . to_string_reference ( pack = sensor . pack , name = sensor . name ) \n 
~~ ~~ import math \n 
from random_words import RandomWords \n 
from st2reactor . container . hash_partitioner import HashPartitioner , Range \n 
from st2tests import config \n 
FIXTURES_1 = { \n 
: [ , , ] \n 
class HashPartitionerTest ( DbTestCase ) : \n 
~~~ models = None \n 
~~~ super ( HashPartitionerTest , cls ) . setUpClass ( ) \n 
cls . models = FixturesLoader ( ) . save_fixtures_to_db ( \n 
fixtures_pack = PACK , fixtures_dict = FIXTURES_1 ) \n 
config . parse_args ( ) \n 
~~ def test_full_range_hash_partitioner ( self ) : \n 
~~~ partitioner = HashPartitioner ( , ) \n 
sensors = partitioner . get_sensors ( ) \n 
self . assertEqual ( len ( sensors ) , 3 , ) \n 
~~ def test_multi_range_hash_partitioner ( self ) : \n 
~~~ range_third = int ( Range . RANGE_MAX_VALUE / 3 ) \n 
range_two_third = range_third * 2 \n 
hash_ranges = . format ( \n 
range_third = range_third , range_two_third = range_two_third ) \n 
partitioner = HashPartitioner ( , hash_ranges ) \n 
~~ def test_split_range_hash_partitioner ( self ) : \n 
~~~ range_mid = int ( Range . RANGE_MAX_VALUE / 2 ) \n 
partitioner = HashPartitioner ( , % range_mid ) \n 
sensors1 = partitioner . get_sensors ( ) \n 
sensors2 = partitioner . get_sensors ( ) \n 
self . assertEqual ( len ( sensors1 ) + len ( sensors2 ) , 3 , ) \n 
~~ def test_hash_effectiveness ( self ) : \n 
partitioner1 = HashPartitioner ( , % range_third ) \n 
partitioner2 = HashPartitioner ( , % ( range_third , range_third + range_third ) ) partitioner3 = HashPartitioner ( , % ( range_third + range_third ) ) \n 
refs_count = 1000 \n 
refs = self . _generate_refs ( count = refs_count ) \n 
p1_count = 0 \n 
p2_count = 0 \n 
p3_count = 0 \n 
for ref in refs : \n 
~~~ if partitioner1 . _is_in_hash_range ( ref ) : \n 
~~~ p1_count += 1 \n 
~~ if partitioner2 . _is_in_hash_range ( ref ) : \n 
~~~ p2_count += 1 \n 
~~ if partitioner3 . _is_in_hash_range ( ref ) : \n 
~~~ p3_count += 1 \n 
~~ ~~ self . assertEqual ( p1_count + p2_count + p3_count , refs_count , \n 
mean = refs_count / 3 \n 
variance = float ( ( p1_count - mean ) ** 2 + ( p1_count - mean ) ** 2 + ( p3_count - mean ) ** 2 ) / 3 \n 
sd = math . sqrt ( variance ) \n 
self . assertTrue ( sd / mean <= 0.2 , ) \n 
~~ def _generate_refs ( self , count = 10 ) : \n 
~~~ random_word_count = int ( math . sqrt ( count ) ) + 1 \n 
words = RandomWords ( ) . random_words ( count = random_word_count ) \n 
x_index = 0 \n 
y_index = 0 \n 
while count > 0 : \n 
~~~ yield % ( words [ x_index ] , words [ y_index ] ) \n 
if y_index < len ( words ) - 1 : \n 
~~~ y_index += 1 \n 
~~~ x_index += 1 \n 
~~ count -= 1 \n 
~~ ~~ from unittest2 import TestCase \n 
from st2actions . runners . utils import get_action_class_instance \n 
from st2tests . mocks . action import MockActionWrapper \n 
from st2tests . mocks . action import MockActionService \n 
class BaseActionTestCase ( TestCase ) : \n 
action_cls = None \n 
~~~ super ( BaseActionTestCase , self ) . setUp ( ) \n 
class_name = self . action_cls . __name__ \n 
action_wrapper = MockActionWrapper ( pack = , class_name = class_name ) \n 
self . action_service = MockActionService ( action_wrapper = action_wrapper ) \n 
~~ def get_action_instance ( self , config = None ) : \n 
instance = get_action_class_instance ( action_cls = self . action_cls , \n 
config = config , \n 
action_service = self . action_service ) \n 
return instance \n 
import sets \n 
from st2common . service_setup import db_setup \n 
~~~ from graphviz import Digraph \n 
raise ImportError ( msg ) \n 
~~ def do_register_cli_opts ( opts , ignore_errors = False ) : \n 
~~~ for opt in opts : \n 
~~~ cfg . CONF . register_cli_opt ( opt ) \n 
~~ ~~ ~~ ~~ class RuleLink ( object ) : \n 
~~~ def __init__ ( self , source_action_ref , rule_ref , dest_action_ref ) : \n 
~~~ self . _source_action_ref = source_action_ref \n 
self . _rule_ref = rule_ref \n 
self . _dest_action_ref = dest_action_ref \n 
~~~ return % ( self . _source_action_ref , self . _rule_ref , self . _dest_action_ref ) \n 
~~ ~~ class LinksAnalyzer ( object ) : \n 
~~~ self . _rule_link_by_action_ref = { } \n 
self . _rules = { } \n 
~~ def analyze ( self , root_action_ref , link_tigger_ref ) : \n 
~~~ rules = Rule . query ( trigger = link_tigger_ref , enabled = True ) \n 
for rule in rules : \n 
~~~ source_action_ref = self . _get_source_action_ref ( rule ) \n 
if not source_action_ref : \n 
~~~ print % rule . ref \n 
~~ rule_links = self . _rules . get ( source_action_ref , None ) \n 
if rule_links is None : \n 
~~~ rule_links = [ ] \n 
self . _rules [ source_action_ref ] = rule_links \n 
~~ rule_links . append ( RuleLink ( source_action_ref = source_action_ref , rule_ref = rule . ref , \n 
dest_action_ref = rule . action . ref ) ) \n 
~~ analyzed = self . _do_analyze ( action_ref = root_action_ref ) \n 
for ( depth , rule_link ) in analyzed : \n 
~~~ print % ( * depth , rule_link ) \n 
~~ return analyzed \n 
~~ def _get_source_action_ref ( self , rule ) : \n 
~~~ criteria = rule . criteria \n 
source_action_ref = criteria . get ( , None ) \n 
~~~ source_action_ref = criteria . get ( , None ) \n 
~~ return source_action_ref [ ] if source_action_ref else None \n 
~~ def _do_analyze ( self , action_ref , rule_links = None , processed = None , depth = 0 ) : \n 
~~~ if processed is None : \n 
~~~ processed = sets . Set ( ) \n 
~~ if rule_links is None : \n 
~~ processed . add ( action_ref ) \n 
for rule_link in self . _rules . get ( action_ref , [ ] ) : \n 
~~~ rule_links . append ( ( depth , rule_link ) ) \n 
if rule_link . _dest_action_ref in processed : \n 
~~ self . _do_analyze ( rule_link . _dest_action_ref , rule_links = rule_links , \n 
processed = processed , depth = depth + 1 ) \n 
~~ return rule_links \n 
~~ ~~ class Grapher ( object ) : \n 
~~~ def generate_graph ( self , rule_links , out_file ) : \n 
~~~ graph_label = \n 
graph_attr = { \n 
: graph_label \n 
node_attr = { } \n 
dot = Digraph ( comment = , \n 
node_attr = node_attr , graph_attr = graph_attr , format = ) \n 
nodes = sets . Set ( ) \n 
for _ , rule_link in rule_links : \n 
~~~ print rule_link . _source_action_ref \n 
if rule_link . _source_action_ref not in nodes : \n 
~~~ nodes . add ( rule_link . _source_action_ref ) \n 
dot . node ( rule_link . _source_action_ref , rule_link . _source_action_ref ) \n 
~~ if rule_link . _dest_action_ref not in nodes : \n 
~~~ nodes . add ( rule_link . _dest_action_ref ) \n 
dot . node ( rule_link . _dest_action_ref , rule_link . _dest_action_ref ) \n 
~~ dot . edge ( rule_link . _source_action_ref , rule_link . _dest_action_ref , constraint = , \n 
label = rule_link . _rule_ref ) \n 
~~ output_path = os . path . join ( os . getcwd ( ) , out_file ) \n 
dot . format = \n 
dot . render ( output_path ) \n 
~~~ monkey_patch ( ) \n 
cli_opts = [ \n 
cfg . StrOpt ( , default = ) \n 
do_register_cli_opts ( cli_opts ) \n 
db_setup ( ) \n 
rule_links = LinksAnalyzer ( ) . analyze ( cfg . CONF . action_ref , cfg . CONF . link_trigger_ref ) \n 
Grapher ( ) . generate_graph ( rule_links , cfg . CONF . out_file ) \n 
~~ import boto \n 
class FieldLists ( ) : \n 
~~~ ADDRESS = [ \n 
BLOCK_DEVICE_TYPE = [ \n 
BUCKET = [ \n 
EC2ZONE = [ \n 
INSTANCE = [ \n 
RECORD = [ \n 
R53ZONE = [ \n 
R53STATUS = [ \n 
VOLUME = [ \n 
TAG = [ \n 
STACK = [ \n 
DBINSTANCE = [ \n 
~~ class ResultSets ( object ) : \n 
~~~ self . foo = \n 
~~ def selector ( self , output ) : \n 
~~~ if isinstance ( output , boto . ec2 . instance . Reservation ) : \n 
~~~ return self . parseReservation ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . instance . Instance ) : \n 
~~~ return self . parseInstance ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . volume . Volume ) : \n 
~~~ return self . parseVolume ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . blockdevicemapping . BlockDeviceType ) : \n 
~~~ return self . parseBlockDeviceType ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . zone . Zone ) : \n 
~~~ return self . parseEC2Zone ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . address . Address ) : \n 
~~~ return self . parseAddress ( output ) \n 
~~ elif isinstance ( output , boto . route53 . record . Record ) : \n 
~~~ return self . parseRecord ( output ) \n 
~~ elif isinstance ( output , boto . route53 . zone . Zone ) : \n 
~~~ return self . parseR53Zone ( output ) \n 
~~ elif isinstance ( output , boto . route53 . status . Status ) : \n 
~~~ return self . parseR53Status ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . tag . Tag ) : \n 
~~~ return self . parseTag ( output ) \n 
~~ elif isinstance ( output , boto . ec2 . ec2object . EC2Object ) : \n 
~~~ return self . parseEC2Object ( output ) \n 
~~ elif isinstance ( output , boto . cloudformation . stack . Stack ) : \n 
~~~ return self . parseStackObject ( output ) \n 
~~ elif isinstance ( output , boto . rds . dbinstance . DBInstance ) : \n 
~~~ return self . parseDBInstanceObject ( output ) \n 
~~~ return output \n 
~~ ~~ def formatter ( self , output ) : \n 
~~~ if isinstance ( output , list ) : \n 
~~~ return [ self . formatter ( item ) for item in output ] \n 
~~ elif isinstance ( output , dict ) : \n 
~~~ return { key : self . formatter ( value ) for key , value in six . iteritems ( output ) } \n 
~~~ return self . selector ( output ) \n 
~~ ~~ def parseReservation ( self , output ) : \n 
~~~ instance_list = [ ] \n 
for instance in output . instances : \n 
~~~ instance_data = self . parseInstance ( instance ) \n 
instance_data [ ] = output . owner_id \n 
instance_list . append ( instance_data ) \n 
~~ return instance_list \n 
~~ def parseAddress ( self , output ) : \n 
~~~ instance_data = { field : getattr ( output , field ) for field in FieldLists . ADDRESS } \n 
return instance_data \n 
~~ def parseInstance ( self , output ) : \n 
~~~ instance_data = { field : getattr ( output , field ) for field in FieldLists . INSTANCE } \n 
~~ def parseVolume ( self , output ) : \n 
~~~ volume_data = { field : getattr ( output , field ) for field in FieldLists . VOLUME } \n 
return volume_data \n 
~~ def parseBlockDeviceType ( self , output ) : \n 
~~~ data = { field : getattr ( output , field ) for field in FieldLists . BLOCK_DEVICE_TYPE } \n 
~~ def parseEC2Zone ( self , output ) : \n 
~~~ zone_data = { field : getattr ( output , field ) for field in FieldLists . EC2ZONE } \n 
return zone_data \n 
~~ def parseRecord ( self , output ) : \n 
~~~ record_data = { field : getattr ( output , field ) for field in FieldLists . RECORD } \n 
return record_data \n 
~~ def parseR53Zone ( self , output ) : \n 
~~~ zone_data = { field : getattr ( output , field ) for field in FieldLists . R53ZONE } \n 
~~ def parseR53Status ( self , output ) : \n 
~~~ status_data = { field : getattr ( output , field ) for field in FieldLists . R53STATUS } \n 
return status_data \n 
~~ def parseBucket ( self , output ) : \n 
~~~ bucket_data = { field : getattr ( output , field ) for field in FieldLists . BUCKET } \n 
return bucket_data \n 
~~ def parseTag ( self , output ) : \n 
~~~ tag_data = { field : getattr ( output , field ) for field in FieldLists . TAG } \n 
return tag_data \n 
~~ def parseStackObject ( self , output ) : \n 
~~~ stack_data = { field : getattr ( output , field ) for field in FieldLists . STACK } \n 
return stack_data \n 
~~ def parseDBInstanceObject ( self , output ) : \n 
~~~ dbinstance_data = { field : getattr ( output , field ) for field in FieldLists . DBINSTANCE } \n 
return dbinstance_data \n 
~~ def parseEC2Object ( self , output ) : \n 
~~~ output = vars ( output ) \n 
del output [ ] \n 
region = output . get ( , None ) \n 
output [ ] = region . name if region else \n 
for k , v in six . iteritems ( output ) : \n 
~~~ if isinstance ( v , boto . ec2 . ec2object . EC2Object ) : \n 
~~~ output [ k ] = getattr ( v , , str ( v ) ) \n 
~~ if isinstance ( v , list ) : \n 
~~~ v_list = [ ] \n 
for item in v : \n 
~~~ if isinstance ( item , ( basestring , bool , int , long , float ) ) : \n 
~~~ v_list . append ( v ) \n 
~~~ v_list . append ( str ( item ) ) \n 
~~ ~~ output [ k ] = v_list \n 
~~ ~~ return output \n 
~~ ~~ from st2actions . runners . pythonrunner import Action \n 
from bitbucket . bitbucket import Bitbucket \n 
class BitBucketAction ( Action ) : \n 
~~~ def __init__ ( self , config ) : \n 
~~~ super ( BitBucketAction , self ) . __init__ ( config ) \n 
~~ def _get_client ( self , repo = None ) : \n 
~~~ if repo : \n 
~~~ bb = Bitbucket ( username = self . config [ ] , \n 
password = self . config [ ] , \n 
repo_name_or_slug = repo ) \n 
password = self . config [ ] ) \n 
~~ return bb \n 
from flask import Flask , request , abort \n 
from st2reactor . sensor . base import Sensor \n 
TRIGGER_REF = \n 
class CircleCIWebhookSensor ( Sensor ) : \n 
~~~ self . host = self . _config [ ] \n 
self . port = self . _config [ ] \n 
self . _endpoints = self . _config [ ] \n 
self . app = Flask ( __name__ ) \n 
self . trigger_ref = TRIGGER_REF \n 
self . log = self . _sensor_service . get_logger ( __name__ ) \n 
@ self . app . route ( ) \n 
def status ( ) : \n 
~~~ return json . dumps ( { : } ) \n 
~~ @ self . app . route ( , methods = [ ] ) \n 
def build_events ( endpoint ) : \n 
~~~ if endpoint not in self . _endpoints : \n 
~~~ self . log . error ( , endpoint ) \n 
abort ( 404 ) \n 
~~ webhook_body = request . get_json ( ) \n 
payload [ ] = self . _get_headers_as_dict ( request . headers ) \n 
payload [ ] = webhook_body \n 
response = self . _sensor_service . dispatch ( self . trigger_ref , payload ) \n 
self . log . debug ( json . dumps ( response ) ) \n 
return json . dumps ( { : } ) \n 
~~ ~~ def run ( self ) : \n 
~~~ self . app . run ( host = self . host , port = self . port , threaded = True ) \n 
~~ def cleanup ( self ) : \n 
~~ def _get_headers_as_dict ( self , headers ) : \n 
~~~ headers_dict = { } \n 
for key , value in headers : \n 
~~~ headers_dict [ key ] = value \n 
~~ return headers_dict \n 
~~ def add_trigger ( self , trigger ) : \n 
~~ def update_trigger ( self , trigger ) : \n 
~~ def remove_trigger ( self , trigger ) : \n 
~~ ~~ from libcloud . loadbalancer . base import Algorithm \n 
from lib . actions import BaseAction \n 
class CreateBalancerAction ( BaseAction ) : \n 
~~~ def run ( self , region , network_domain_id , name , port , protocol , \n 
algorithm = Algorithm . ROUND_ROBIN ) : \n 
~~~ driver = self . _get_lb_driver ( region ) \n 
_VALUE_TO_ALGORITHM_MAP = { \n 
: Algorithm . ROUND_ROBIN , \n 
: Algorithm . LEAST_CONNECTIONS , \n 
: Algorithm . SHORTEST_RESPONSE , \n 
: Algorithm . PERSISTENT_IP \n 
if algorithm is not Algorithm . ROUND_ROBIN : \n 
~~~ algorithm = _VALUE_TO_ALGORITHM_MAP [ algorithm ] \n 
~~ driver . network_domain_id = network_domain_id \n 
record = driver . create_balancer ( name = name , \n 
port = port , \n 
protocol = protocol , \n 
algorithm = algorithm , \n 
members = None ) \n 
return self . resultsets . formatter ( record ) \n 
~~ ~~ from lib . base import DockerBasePythonAction \n 
class DockerPullImageAction ( DockerBasePythonAction ) : \n 
~~~ def run ( self , repo , tag = None , insecure_registry = False , \n 
auth_username_override = None , auth_password_override = None ) : \n 
~~~ auth_override = ( auth_username_override and auth_password_override ) \n 
if auth_override : \n 
~~~ auth_config = { } \n 
auth_config [ ] = auth_username_override \n 
auth_config [ ] = auth_password_override \n 
return self . wrapper . pull ( repo = repo , tag = tag , insecure_registry = insecure_registry , \n 
auth_config = auth_config ) \n 
~~~ return self . wrapper . pull ( repo = repo , tag = tag , insecure_registry = insecure_registry ) \n 
~~ ~~ ~~ from lib . actions import BaseAction \n 
class ViewAXConfig ( BaseAction ) : \n 
~~~ def run ( self ) : \n 
~~~ response = self . _api_get ( ) \n 
~~ ~~ from lib import action \n 
class ColorTempKelvinAction ( action . BaseAction ) : \n 
~~~ def run ( self , light_id , temperature , transition_time ) : \n 
~~~ light = self . hue . lights . get ( light_id ) \n 
light . ct ( temperature , transition_time ) \n 
class BaseAction ( Action ) : \n 
~~~ super ( BaseAction , self ) . __init__ ( config ) \n 
~~ ~~ import libcloud . compute . base as compute_base \n 
import libcloud . dns . base as dns_base \n 
import libcloud . loadbalancer . base as lb_base \n 
import libcloud . container . base as container_base \n 
class FieldLists ( object ) : \n 
NODE = [ , , , , , , ] \n 
NODE_SIZE = [ , , , , , ] \n 
NODE_IMAGE = [ , ] \n 
LOCATION = [ , , ] \n 
NODE_KEY = [ ] \n 
NODE_PASSWORD = [ , ] \n 
STORAGE_VOLUME = [ , , , ] \n 
VOLUME_SNAPSHOT = [ , , ] \n 
ZONE = [ , , , ] \n 
RECORD = [ , , , , ] \n 
MEMBER = [ , , , ] \n 
BALANCER = [ , , , ] \n 
CONTAINER = [ , , ] \n 
CONTAINER_IMAGE = [ , , , ] \n 
CONTAINER_CLUSTER = [ , ] \n 
~~~ def selector ( self , output ) : \n 
~~~ if isinstance ( output , compute_base . Node ) : \n 
~~~ return self . parse ( output , FieldLists . NODE ) \n 
~~ elif isinstance ( output , compute_base . NodeSize ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_SIZE ) \n 
~~ elif isinstance ( output , compute_base . NodeImage ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_IMAGE ) \n 
~~ elif isinstance ( output , compute_base . NodeLocation ) : \n 
~~~ return self . parse ( output , FieldLists . LOCATION ) \n 
~~ elif isinstance ( output , compute_base . NodeAuthSSHKey ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_KEY ) \n 
~~ elif isinstance ( output , compute_base . NodeAuthPassword ) : \n 
~~~ return self . parse ( output , FieldLists . NODE_PASSWORD ) \n 
~~ elif isinstance ( output , compute_base . StorageVolume ) : \n 
~~~ return self . parse ( output , FieldLists . STORAGE_VOLUME ) \n 
~~ elif isinstance ( output , compute_base . VolumeSnapshot ) : \n 
~~~ return self . parse ( output , FieldLists . VOLUME_SNAPSHOT ) \n 
~~ elif isinstance ( output , dns_base . Zone ) : \n 
~~~ return self . parse ( output , FieldLists . ZONE ) \n 
~~ elif isinstance ( output , dns_base . Record ) : \n 
~~~ return self . parse ( output , FieldLists . RECORD ) \n 
~~ elif isinstance ( output , lb_base . Member ) : \n 
~~~ return self . parse ( output , FieldLists . MEMBER ) \n 
~~ elif isinstance ( output , lb_base . LoadBalancer ) : \n 
~~~ return self . parse ( output , FieldLists . BALANCER ) \n 
~~ elif isinstance ( output , container_base . Container ) : \n 
~~~ return self . parse ( output , FieldLists . CONTAINER ) \n 
~~ elif isinstance ( output , container_base . ContainerImage ) : \n 
~~~ return self . parse ( output , FieldLists . CONTAINER_IMAGE ) \n 
~~ elif isinstance ( output , container_base . ContainerCluster ) : \n 
~~~ return self . parse ( output , FieldLists . CONTAINER_CLUSTER ) \n 
~~~ formatted = [ ] \n 
if isinstance ( output , list ) : \n 
~~~ for o in output : \n 
~~~ formatted . append ( self . selector ( o ) ) \n 
~~~ formatted = self . selector ( output ) \n 
~~ return formatted \n 
~~ def _getval ( self , obj , field ) : \n 
~~~ return self . selector ( getattr ( obj , field ) ) \n 
~~ def parse ( self , output , field_list ) : \n 
~~~ instance_data = { field : self . _getval ( output , field ) for field in field_list } \n 
~~ ~~ from lib . mmonit import MmonitBaseAction \n 
class MmonitActionHost ( MmonitBaseAction ) : \n 
~~~ def run ( self , host_id , action , service ) : \n 
~~~ self . login ( ) \n 
data = { "service" : service , "id" : host_id , "action" : action } \n 
self . session . post ( "{}/admin/hosts/action" . format ( self . url ) , data = data ) \n 
self . logout ( ) \n 
~~ ~~ from lib import actions \n 
class GetModeAction ( actions . BaseAction ) : \n 
~~~ def run ( self , structure = None , device = None ) : \n 
~~~ if structure and device : \n 
~~~ nest = self . _get_device ( structure , device ) \n 
~~~ nest = self . _get_default_device ( ) \n 
~~ return nest . mode \n 
~~ ~~ from lib . action import BaseAction \n 
class SendCommandAction ( BaseAction ) : \n 
~~~ def run ( self , item , command ) : \n 
~~~ self . _post ( item , command ) \n 
return { : } \n 
~~ ~~ import requests \n 
from lib . base import OpscenterAction \n 
class SetNodeConfAction ( OpscenterAction ) : \n 
~~~ def run ( self , node_ip , node_conf , cluster_id = None ) : \n 
~~~ if not cluster_id : \n 
~~~ cluster_id = self . cluster_id \n 
~~~ self . logger . error ( ) \n 
~~ url = self . _get_full_url ( [ cluster_id , , node_ip ] ) \n 
return requests . post ( url , data = node_conf ) . json ( ) \n 
~~ ~~ from lib . python_actions import PuppetBasePythonAction \n 
class PuppetCertCleanAction ( PuppetBasePythonAction ) : \n 
~~~ def run ( self , environment , host ) : \n 
~~~ success = self . client . cert_clean ( environment = environment , host = host ) \n 
return success \n 
~~ ~~ from lib . action import PyraxBaseAction \n 
class CreateLoadBalancerAction ( PyraxBaseAction ) : \n 
~~~ def run ( self , name , port , protocol ) : \n 
~~~ clb = self . pyrax . cloud_loadbalancers \n 
virtual_ipv4 = clb . VirtualIP ( type = "PUBLIC" , ipVersion = ) \n 
self . logger . info ( ) \n 
load_balancer = clb . create ( name , port = port , protocol = protocol , virtual_ips = [ virtual_ipv4 ] ) \n 
self . pyrax . utils . wait_until ( load_balancer , "status" , "ACTIVE" , interval = 1 , \n 
attempts = 30 ) \n 
self . logger . info ( % load_balancer ) \n 
: load_balancer . cluster , \n 
: load_balancer . algorithm , \n 
: load_balancer . id , \n 
: load_balancer . name , \n 
: load_balancer . port , \n 
: load_balancer . protocol , \n 
: load_balancer . sourceAddresses [ ] , \n 
: load_balancer . connectionLogging [ ] , \n 
: load_balancer . contentCaching [ ] , \n 
: load_balancer . httpsRedirect , \n 
: load_balancer . timeout , \n 
: load_balancer . status \n 
~~ ~~ from lib . actions import BaseAction \n 
class CreateMessage ( BaseAction ) : \n 
~~~ VISIBILITY = { \n 
def run ( self , slug , message , visibility = , \n 
suppress_notification = False ) : \n 
: message , \n 
: CreateMessage . VISIBILITY [ visibility ] \n 
if suppress_notification : \n 
~~~ payload [ ] = True \n 
~~ endpoint = . format ( slug ) \n 
response = self . _api_post ( endpoint , json = payload ) \n 
import servicenow_rest . api as sn \n 
self . client = self . _get_client ( ) \n 
~~ def _get_client ( self ) : \n 
~~~ instance_name = self . config [ ] \n 
username = self . config [ ] \n 
password = self . config [ ] \n 
return sn . Client ( instance_name , username , password ) \n 
from lib . action import St2BaseAction \n 
class St2KVPGetObjectAction ( St2BaseAction ) : \n 
~~~ def run ( self , key ) : \n 
~~~ _key = self . client . keys . get_by_name ( key ) \n 
if _key : \n 
~~~ deserialized_value = json . loads ( _key . value ) \n 
return deserialized_value \n 
~~ ~~ ~~ from lib . action import TravisCI \n 
class ListHooksAction ( TravisCI ) : \n 
path = \n 
response = self . _perform_request ( path , method = , requires_auth = True ) \n 
data = response . json ( ) \n 
hooks = { } \n 
for hook in data [ ] : \n 
~~~ hooks [ hook [ ] ] = [ hook [ ] , hook [ ] ] \n 
~~ return hooks \n 
class VaultIsInitializedAction ( action . VaultBaseAction ) : \n 
~~~ return self . vault . is_initialized ( ) \n 
from pyVmomi import vim \n 
from vmwarelib import inventory \n 
from vmwarelib import checkinputs \n 
from vmwarelib . actions import BaseAction \n 
class VMApplyPowerState ( BaseAction ) : \n 
~~~ def run ( self , vm_id , vm_name , power_onoff ) : \n 
vm = inventory . get_virtualmachine ( self . si_content , \n 
moid = vm_id , name = vm_name ) \n 
if not vm : \n 
~~ if power_onoff == "poweroff" : \n 
~~~ task = vm . PowerOffVM_Task ( ) \n 
~~ elif power_onoff == "poweron" : \n 
~~~ task = vm . PowerOnVM_Task ( ) \n 
~~ while task . info . state == vim . TaskInfo . State . running : \n 
~~~ eventlet . sleep ( 1 ) \n 
~~ return { : str ( task . info . state ) } \n 
from distutils . core import setup \n 
README = open ( os . path . join ( os . path . dirname ( __file__ ) , ) ) . read ( ) \n 
os . chdir ( os . path . normpath ( os . path . join ( os . path . abspath ( __file__ ) , os . pardir ) ) ) \n 
name = "SimpleIDML" , \n 
version = "0.92.4" , \n 
long_description = README , \n 
package_dir = { : } , \n 
packages = [ \n 
data_files = [ ] , \n 
scripts = [ \n 
import Queue \n 
import textwrap \n 
from thunderdome . exceptions import ThunderdomeException \n 
from thunderdome . spec import Spec \n 
class ThunderdomeConnectionError ( ThunderdomeException ) : \n 
~~ class ThunderdomeQueryError ( ThunderdomeException ) : \n 
def __init__ ( self , message , full_response = { } ) : \n 
super ( ThunderdomeQueryError , self ) . __init__ ( message ) \n 
self . _full_response = full_response \n 
def raw_response ( self ) : \n 
return self . _full_response \n 
~~ ~~ class ThunderdomeGraphMissingError ( ThunderdomeException ) : \n 
~~ Host = namedtuple ( , [ , ] ) \n 
_hosts = [ ] \n 
_host_idx = 0 \n 
_graph_name = None \n 
_username = None \n 
_password = None \n 
_index_all_fields = True \n 
_existing_indices = None \n 
_statsd = None \n 
def create_key_index ( name ) : \n 
global _existing_indices \n 
_existing_indices = _existing_indices or execute_query ( ) \n 
if name not in _existing_indices : \n 
~~~ execute_query ( \n 
{ : name } , transaction = False ) \n 
~~ ~~ def create_unique_index ( name , data_type ) : \n 
~~ ~~ def setup ( hosts , graph_name , username = None , password = None , index_all_fields = False , statsd = None ) : \n 
global _hosts \n 
global _graph_name \n 
global _username \n 
global _password \n 
global _index_all_fields \n 
global _statsd \n 
_graph_name = graph_name \n 
_username = username \n 
_password = password \n 
_index_all_fields = index_all_fields \n 
if statsd : \n 
~~~ sd = statsd \n 
import statsd \n 
tmp = sd . split ( ) \n 
if len ( tmp ) == 1 : \n 
~~~ tmp . append ( ) \n 
~~ _statsd = statsd . StatsClient ( tmp [ 0 ] , int ( tmp [ 1 ] ) , prefix = ) \n 
~~ ~~ for host in hosts : \n 
host = host . split ( ) \n 
if len ( host ) == 1 : \n 
~~~ _hosts . append ( Host ( host [ 0 ] , 8182 ) ) \n 
~~ elif len ( host ) == 2 : \n 
~~~ _hosts . append ( Host ( * host ) ) \n 
~~ ~~ if not _hosts : \n 
~~ random . shuffle ( _hosts ) \n 
create_unique_index ( , ) \n 
from thunderdome . models import vertex_types \n 
for klass in vertex_types . values ( ) : \n 
~~~ klass . _create_indices ( ) \n 
~~ ~~ def execute_query ( query , params = { } , transaction = True , context = "" ) : \n 
if transaction : \n 
~~~ query = "g.stopTransaction(FAILURE)\\n" + query \n 
~~ if len ( _hosts ) <= 0 : \n 
~~~ raise ThunderdomeConnectionError ( \n 
~~ host = _hosts [ 0 ] \n 
data = json . dumps ( { : query , : params } ) \n 
headers = { : , : , : import time \n 
~~~ start_time = time . time ( ) \n 
conn = httplib . HTTPConnection ( host . name , host . port ) \n 
conn . request ( "POST" , . format ( _graph_name ) , data , headers ) \n 
response = conn . getresponse ( ) \n 
content = response . read ( ) \n 
total_time = int ( ( time . time ( ) - start_time ) * 1000 ) \n 
if context and _statsd : \n 
~~~ _statsd . timing ( "{}.timer" . format ( context ) , total_time ) \n 
_statsd . incr ( "{}.counter" . format ( context ) ) \n 
~~ ~~ except socket . error as sock_err : \n 
~~~ if _statsd : \n 
~~~ total_time = int ( ( time . time ( ) - start_time ) * 1000 ) \n 
_statsd . incr ( "thunderdome.socket_error" . format ( context ) , total_time ) \n 
~~ raise ThunderdomeQueryError ( . format ( sock_err ) ) \n 
~~ logger . info ( json . dumps ( data ) ) \n 
logger . info ( content ) \n 
~~~ response_data = json . loads ( content ) \n 
~~ except ValueError as ve : \n 
~~ if response . status != 200 : \n 
~~~ if in response_data and len ( response_data [ ] ) > 0 : \n 
if re . search ( graph_missing_re , response_data [ ] ) : \n 
~~~ raise ThunderdomeGraphMissingError ( response_data [ ] ) \n 
~~~ raise ThunderdomeQueryError ( \n 
response_data [ ] , \n 
response_data \n 
~~~ _statsd . incr ( "{}.error" . format ( context ) ) \n 
~~ raise ThunderdomeQueryError ( \n 
~~ ~~ return response_data [ ] \n 
~~ def sync_spec ( filename , host , graph_name , dry_run = False ) : \n 
Spec ( filename ) . sync ( host , graph_name , dry_run = dry_run ) \n 
~~ from setuptools import setup , find_packages \n 
def read ( fname ) : \n 
~~~ return open ( os . path . join ( os . path . dirname ( __file__ ) , fname ) ) . read ( ) \n 
~~ README = read ( ) \n 
name = "django-pagination-plus" , \n 
version = "0.0.3" , \n 
author_email = "stefan@steeffie.net" , \n 
url = "https://github.com/SteefH/django-pagination-plus" , \n 
keywords = [ , ] , \n 
import base \n 
from brewery import dq \n 
from brewery . metadata import expand_record \n 
~~~ from pyes . es import ES \n 
~~~ from brewery . utils import MissingPackage \n 
~~ class ESDataSource ( base . DataSource ) : \n 
def __init__ ( self , document_type , database = None , host = None , port = None , \n 
expand = False , ** elasticsearch_args ) : \n 
self . document_type = document_type \n 
self . database_name = database \n 
self . elasticsearch_args = elasticsearch_args \n 
self . expand = expand \n 
self . connection = None \n 
self . _fields = None \n 
~~ def initialize ( self ) : \n 
args = self . elasticsearch_args . copy ( ) \n 
server = "" \n 
if self . host : \n 
~~~ server = self . host \n 
~~ if self . port : \n 
~~~ server += ":" + self . port \n 
~~ self . connection = ES ( server , ** args ) \n 
self . connection . default_indices = self . database_name \n 
self . connection . default_types = self . document_type \n 
~~ def read_fields ( self , limit = 0 ) : \n 
~~~ keys = [ ] \n 
probes = { } \n 
def probe_record ( record , parent = None ) : \n 
~~~ for key , value in record . items ( ) : \n 
~~~ if parent : \n 
~~~ full_key = parent + "." + key \n 
~~~ full_key = key \n 
~~ if self . expand and type ( value ) == dict : \n 
~~~ probe_record ( value , full_key ) \n 
~~ if not full_key in probes : \n 
~~~ probe = dq . FieldTypeProbe ( full_key ) \n 
probes [ full_key ] = probe \n 
keys . append ( full_key ) \n 
~~~ probe = probes [ full_key ] \n 
~~ probe . probe ( value ) \n 
~~ ~~ for record in self . document_type . find ( limit = limit ) : \n 
~~~ probe_record ( record ) \n 
~~ fields = [ ] \n 
for key in keys : \n 
~~~ probe = probes [ key ] \n 
field = base . Field ( probe . field ) \n 
storage_type = probe . unique_storage_type \n 
if not storage_type : \n 
~~~ field . storage_type = "unknown" \n 
~~ elif storage_type == "unicode" : \n 
~~~ field . storage_type = "string" \n 
field . concrete_storage_type = storage_type \n 
~~ fields . append ( field ) \n 
~~ self . fields = list ( fields ) \n 
return self . fields \n 
~~ def rows ( self ) : \n 
~~~ if not self . connection : \n 
~~ from pyes . query import MatchAllQuery \n 
fields = self . fields . names ( ) \n 
results = self . connection . search ( MatchAllQuery ( ) , search_type = "scan" , timeout = "5m" , size = "200" return ESRowIterator ( results , fields ) \n 
~~ def records ( self ) : \n 
results = self . connection . search ( MatchAllQuery ( ) , search_type = "scan" , timeout = "5m" , size = "200" return ESRecordIterator ( results , self . expand ) \n 
~~ ~~ class ESRowIterator ( object ) : \n 
def __init__ ( self , resultset , field_names ) : \n 
~~~ self . resultset = resultset \n 
self . field_names = field_names \n 
~~ def __getitem__ ( self , index ) : \n 
~~~ record = self . resultset . __getitem__ ( index ) \n 
array = [ ] \n 
for field in self . field_names : \n 
~~~ value = record \n 
for key in field . split ( ) : \n 
~~~ if key in value : \n 
~~~ value = value [ key ] \n 
~~ ~~ array . append ( value ) \n 
~~ return tuple ( array ) \n 
~~ ~~ class ESRecordIterator ( object ) : \n 
def __init__ ( self , resultset , expand = False ) : \n 
~~~ def expand_record ( record , parent = None ) : \n 
~~~ ret = { } \n 
for key , value in record . items ( ) : \n 
~~ if type ( value ) == dict : \n 
~~~ expanded = expand_record ( value , full_key ) \n 
ret . update ( expanded ) \n 
~~~ ret [ full_key ] = value \n 
~~ record = self . resultset . __getitem__ ( index ) \n 
if not self . expand : \n 
~~~ return record \n 
~~~ return expand_record ( record ) \n 
~~ ~~ ~~ class ESDataTarget ( base . DataTarget ) : \n 
def __init__ ( self , document_type , database = "test" , host = "127.0.0.1" , port = "9200" , \n 
truncate = False , expand = False , ** elasticsearch_args ) : \n 
self . truncate = truncate \n 
from pyes . es import ES \n 
from pyes . exceptions import IndexAlreadyExistsException \n 
~~ create = args . pop ( "create" , False ) \n 
replace = args . pop ( "replace" , False ) \n 
self . connection = ES ( server , ** args ) \n 
created = False \n 
if create : \n 
~~~ self . connection . create_index ( self . database_name ) \n 
self . connection . refresh ( self . database_name ) \n 
created = True \n 
~~ except IndexAlreadyExistsException : \n 
~~ ~~ if replace and not created : \n 
~~~ self . connection . delete_index_if_exists ( self . database_name ) \n 
time . sleep ( 2 ) \n 
self . connection . create_index ( self . database_name ) \n 
~~ if self . truncate : \n 
~~~ self . connection . delete_mapping ( self . database_name , self . document_type ) \n 
~~ ~~ def append ( self , obj ) : \n 
~~~ record = obj \n 
if not isinstance ( obj , dict ) : \n 
~~~ record = dict ( zip ( self . fields . names ( ) , obj ) ) \n 
~~ if self . expand : \n 
~~~ record = expand_record ( record ) \n 
~~ id = record . get ( ) or record . get ( ) \n 
self . connection . index ( record , self . database_name , self . document_type , id , bulk = True ) \n 
~~ def finalize ( self ) : \n 
~~~ self . connection . flush_bulk ( forced = True ) \n 
from brewery import ds \n 
import brewery . metadata \n 
from sqlalchemy import Table , Column , Integer , String , Text \n 
from sqlalchemy import create_engine , MetaData \n 
class SQLStreamsTestCase ( unittest . TestCase ) : \n 
~~~ self . engine = create_engine ( "sqlite://" ) \n 
self . metadata = MetaData ( ) \n 
self . fields = brewery . metadata . FieldList ( [ \n 
( "category" , "string" ) , \n 
( "category_label" , "string" ) , \n 
( "subcategory" , "string" ) , \n 
( "subcategory_label" , "string" ) , \n 
( "line_item" , "string" ) , \n 
( "year" , "integer" ) , \n 
( "amount" , "integer" ) ] ) \n 
self . example_row = [ "cat" , "Category" , "scat" , "Sub-category" , "foo" , 2012 , 100 ] \n 
~~ def test_table_fields ( self ) : \n 
~~~ table = Table ( , self . metadata , \n 
Column ( , Integer , primary_key = True ) , \n 
Column ( , String ( 32 ) ) , \n 
Column ( , String ( 255 ) ) , \n 
Column ( , Text ) \n 
self . metadata . create_all ( self . engine ) \n 
stream = ds . SQLDataSource ( connection = self . engine , table = str ( table ) ) \n 
fields = stream . fields \n 
self . assertEqual ( 4 , len ( fields ) ) \n 
~~ def test_target_no_existing_table ( self ) : \n 
~~~ stream = ds . SQLDataTarget ( connection = self . engine , table = "test" ) \n 
self . assertRaises ( Exception , stream . initialize ) \n 
~~ def test_target_create_table ( self ) : \n 
~~~ stream = ds . SQLDataTarget ( connection = self . engine , table = "test" , create = True ) \n 
stream . fields = self . fields \n 
stream . initialize ( ) \n 
cnames = [ str ( c ) for c in stream . table . columns ] \n 
fnames = [ "test." + f . name for f in self . fields ] \n 
self . assertEqual ( fnames , cnames ) \n 
stream . finalize ( ) \n 
~~ def test_target_replace_table ( self ) : \n 
stream = ds . SQLDataTarget ( connection = self . engine , table = "test" , \n 
create = True , replace = False ) \n 
create = True , replace = True ) \n 
~~ def test_target_concrete_type_map ( self ) : \n 
~~~ ctm = { "string" : String ( 123 ) } \n 
create = True , \n 
fields = self . fields , \n 
concrete_type_map = ctm ) \n 
c = stream . table . c [ "line_item" ] \n 
self . assertEqual ( 123 , c . type . length ) from . context import * \n 
~~ ~~ from . engine import * \n 
from . graph import * \n 
from . pipeline import * \n 
from bubbles import * \n 
class GraphTestCase ( unittest . TestCase ) : \n 
~~~ def test_basic ( self ) : \n 
~~~ g = Graph ( ) \n 
g . add ( Node ( "src" ) , "n1" ) \n 
g . add ( Node ( "distinct" ) , "n2" ) \n 
g . add ( Node ( "pretty_print" ) , "n3" ) \n 
self . assertEqual ( 3 , len ( g . nodes ) ) \n 
g . connect ( "n1" , "n2" ) \n 
sources = g . sources ( "n2" ) \n 
self . assertEqual ( 1 , len ( sources ) ) \n 
self . assertTrue ( isinstance ( sources [ "default" ] , Node ) ) \n 
self . assertEqual ( "src" , sources [ "default" ] . opname ) \n 
~~ def test_ports ( self ) : \n 
g . add ( Node ( "dim" ) , "dim" ) \n 
g . add ( Node ( "src" ) , "src" ) \n 
g . add ( Node ( "join_detail" ) , "j" ) \n 
g . connect ( "dim" , "j" , "master" ) \n 
with self . assertRaises ( GraphError ) : \n 
~~~ g . connect ( "src" , "j" , "master" ) \n 
~~ g . connect ( "src" , "j" , "detail" ) \n 
sources = g . sources ( "j" ) \n 
self . assertEqual ( 2 , len ( sources ) ) \n 
self . assertEqual ( [ "detail" , "master" ] , sorted ( sources . keys ( ) ) ) \n 
from upstream . shard import Shard \n 
from upstream . streamer import Streamer \n 
from upstream . exc import ConnectError , FileError , ShardError , ResponseError \n 
class TestStreamer ( unittest . TestCase ) : \n 
~~~ self . stream = Streamer ( "http://node1.metadisk.org" ) \n 
self . orig_hash = None \n 
self . uploadfile = "tests/1k.testfile" \n 
self . downloadfile = "download.testfile" \n 
self . shard = Shard ( \n 
"2032e4fd19d4ab49a74ead0984a5f672c26e60da6e992eaf51f05dc874e94bd7" , \n 
"1b1f463cef1807a127af668f3a4fdcc7977c647bf2f357d9fa125f13548b1d14" \n 
~~~ del self . stream \n 
del self . orig_hash \n 
del self . uploadfile \n 
~~~ os . remove ( self . downloadfile ) \n 
~~~ os . remove ( self . shard . filehash ) \n 
~~ del self . downloadfile \n 
del self . shard \n 
~~ def test_initialization ( self ) : \n 
~~~ self . assertEqual ( self . stream . server , "http://node1.metadisk.org" ) \n 
~~ def test_check_connectivity ( self ) : \n 
~~~ def _failing_connection ( ) : \n 
~~~ Streamer ( "http://does.not.exist" ) \n 
~~ self . assertRaises ( ConnectError , _failing_connection ) \n 
def test_upload_form_encoded ( self , post ) : \n 
def test_upload_sharded_encoded ( self , post ) : \n 
~~~ with self . assertRaises ( NotImplementedError ) : \n 
~~~ self . stream . _upload_sharded_encoded ( , ) \n 
~~ ~~ @ mock . patch ( ) \n 
def test_filestream ( self , post ) : \n 
~~~ self . stream . _filestream ( ) \n 
~~ ~~ def test_upload ( self ) : \n 
~~~ self . shard = self . stream . upload ( self . uploadfile ) \n 
self . shard . filehash , \n 
"2032e4fd19d4ab49a74ead0984a5f672" \n 
"c26e60da6e992eaf51f05dc874e94bd7" ) \n 
self . shard . decryptkey , \n 
"1b1f463cef1807a127af668f3a4fdcc7" \n 
"977c647bf2f357d9fa125f13548b1d14" ) \n 
def _failing_upload ( ) : \n 
~~~ self . stream . upload ( "not-a-real-file" ) \n 
~~ self . assertRaises ( FileError , _failing_upload ) \n 
~~ def test_upload_patched_404 ( self ) : \n 
~~~ self . stream . _upload_form_encoded = mock . MagicMock ( ) \n 
self . stream . _upload_form_encoded . return_value ( ) \n 
self . stream . _upload_form_encoded . return_value . status_code = 404 \n 
def _fourohfour ( ) : \n 
~~~ self . stream . upload ( self . uploadfile ) \n 
~~ with self . assertRaises ( ResponseError ) as ex : \n 
~~~ _fourohfour ( ) \n 
~~ ~~ def test_upload_patched_402 ( self ) : \n 
self . stream . _upload_form_encoded . return_value . status_code = 402 \n 
def _fourohtwo ( ) : \n 
~~ with self . assertRaises ( ResponseError ) : \n 
~~~ _fourohtwo ( ) \n 
~~ ~~ def test_upload_patched_500 ( self ) : \n 
self . stream . _upload_form_encoded . return_value . status_code = 500 \n 
def _fivehundred ( ) : \n 
~~~ _fivehundred ( ) \n 
~~ ~~ def test_upload_patched_501 ( self ) : \n 
self . stream . _upload_form_encoded . return_value . status_code = 501 \n 
def _fiveohone ( ) : \n 
~~~ _fiveohone ( ) \n 
self . assertEqual ( ex . message , \n 
~~ ~~ def test_upload_check_path ( self ) : \n 
~~~ homedir = os . path . expanduser ( self . uploadfile ) \n 
result = self . stream . check_path ( self . uploadfile ) \n 
self . assertEqual ( homedir , result ) \n 
with self . assertRaises ( FileError ) as ex : \n 
~~~ self . stream . check_path ( ) \n 
ex . message , ) \n 
~~ ~~ def test_download ( self ) : \n 
~~~ r = self . stream . download ( self . shard ) \n 
self . assertEquals ( r . status_code , 200 ) \n 
self . assertEqual ( len ( r . content ) , 1024 ) \n 
~~ def test_download_exception ( self ) : \n 
~~~ self . shard . filehash = self . shard . filehash [ : - 5 ] \n 
with self . assertRaises ( ResponseError ) as ex : \n 
~~~ self . stream . download ( self . shard ) \n 
~~ self . assertEqual ( ex . exception . response . status_code , 404 ) \n 
~~ def test_download_empty_shard ( self ) : \n 
~~~ shard = Shard ( ) \n 
with self . assertRaises ( ShardError ) as e : \n 
~~~ self . stream . download ( shard ) \n 
~~ ~~ import re \n 
from functools import partial \n 
from sublime_plugin import WindowCommand , TextCommand , EventListener \n 
from . util import find_view_by_settings , get_setting \n 
from . cmd import GitCmd \n 
from . helpers import GitDiffHelper , GitErrorHelper , GitStatusHelper \n 
RE_DIFF_HEAD = re . compile ( ) \n 
GIT_DIFF_TITLE = \n 
GIT_DIFF_TITLE_PREFIX = GIT_DIFF_TITLE + \n 
GIT_DIFF_CACHED_TITLE = \n 
GIT_DIFF_CACHED_TITLE_PREFIX = GIT_DIFF_CACHED_TITLE + \n 
GIT_DIFF_VIEW_SYNTAX = \n 
class GitDiffCommand ( WindowCommand , GitCmd ) : \n 
def run ( self , repo = None , path = None , cached = False ) : \n 
~~~ repo = repo or self . get_repo ( ) \n 
if not repo : \n 
~~ path = path or repo \n 
title = self . get_view_title ( path , cached ) \n 
git_view = % ( if cached else ) \n 
view = find_view_by_settings ( self . window , git_view = git_view , git_repo = repo , git_diff_path = path if not view : \n 
~~~ view = self . window . new_file ( ) \n 
view . set_name ( title ) \n 
view . set_syntax_file ( GIT_DIFF_VIEW_SYNTAX ) \n 
view . set_read_only ( True ) \n 
view . settings ( ) . set ( , git_view ) \n 
view . settings ( ) . set ( , repo ) \n 
view . settings ( ) . set ( , path ) \n 
view . settings ( ) . set ( , cached ) \n 
view . settings ( ) . set ( , 3 ) \n 
~~ self . window . focus_view ( view ) \n 
view . run_command ( , { : path , : cached , : True } ) \n 
~~ def get_view_title ( self , path = None , cached = False ) : \n 
~~~ if cached : \n 
~~~ return GIT_DIFF_CACHED_TITLE_PREFIX + path if path else GIT_DIFF_CACHED_TITLE \n 
~~~ return GIT_DIFF_TITLE_PREFIX + path if path else GIT_DIFF_TITLE \n 
~~ ~~ ~~ class GitDiffCachedCommand ( GitDiffCommand ) : \n 
def run ( self , path = None ) : \n 
~~~ super ( GitDiffCachedCommand , self ) . run ( path = path , cached = True ) \n 
~~ ~~ class GitDiffCurrentFileCommand ( GitCmd , GitStatusHelper , TextCommand ) : \n 
def run ( self , edit , cached = False ) : \n 
~~~ filename = self . view . file_name ( ) \n 
~~~ sublime . error_message ( ) \n 
~~ repo = self . get_repo ( ) \n 
~~ in_git = self . file_in_git ( repo , filename ) \n 
if not in_git : \n 
~~~ sublime . error_message ( % filename . replace ( repo , ) . return \n 
~~ self . view . window ( ) . run_command ( , { : repo , : filename , : cached \n 
~~ ~~ class GitDiffCachedCurrentFileCommand ( GitDiffCurrentFileCommand ) : \n 
def run ( self , edit ) : \n 
~~~ super ( GitDiffCachedCurrentFileCommand , self ) . run ( edit , cached = True ) \n 
~~ ~~ class GitDiffTextCmd ( GitCmd , GitDiffHelper ) : \n 
~~~ def move_to_point ( self , point ) : \n 
~~~ self . view . sel ( ) . clear ( ) \n 
self . view . sel ( ) . add ( sublime . Region ( point ) ) \n 
if not self . view . visible_region ( ) . contains ( point ) : \n 
~~~ view = self . view \n 
sublime . set_timeout ( partial ( view . show , point , True ) , 50 ) \n 
~~ ~~ def parse_diff ( self ) : \n 
~~~ sections = [ ] \n 
state = None \n 
prev_file = None \n 
current_file = { } \n 
current_hunks = [ ] \n 
prev_hunk = None \n 
current_hunk = None \n 
for line in self . view . lines ( sublime . Region ( 0 , self . view . size ( ) ) ) : \n 
~~~ linetext = self . view . substr ( line ) \n 
if linetext . startswith ( ) : \n 
~~~ state = \n 
if prev_file != line : \n 
~~~ if prev_file is not None : \n 
~~~ if current_hunk : \n 
~~~ current_hunks . append ( current_hunk ) \n 
~~ sections . append ( ( current_file , current_hunks ) ) \n 
~~ prev_file = line \n 
~~ current_file = line \n 
~~ elif state == and RE_DIFF_HEAD . match ( linetext ) : \n 
~~~ current_file = current_file . cover ( line ) \n 
~~ elif linetext . startswith ( ) : \n 
if prev_hunk != line : \n 
~~~ if prev_hunk is not None : \n 
~~ prev_hunk = line \n 
~~ current_hunk = line \n 
~~ elif state == and linetext [ 0 ] in ( , , ) : \n 
~~~ current_hunk = current_hunk . cover ( line ) \n 
~~ ~~ if current_file and current_hunk : \n 
sections . append ( ( current_file , current_hunks ) ) \n 
~~ return sections \n 
~~ def build_lookup ( self , parsed_diff ) : \n 
~~~ lookup = [ ] \n 
for header , hunks in parsed_diff : \n 
~~~ for h in hunks : \n 
~~~ lookup . append ( ( h , header ) ) \n 
~~ ~~ return lookup \n 
~~ def get_hunks_from_selection ( self , selection ) : \n 
~~~ if not selection : \n 
~~ diffspec = self . parse_diff ( ) \n 
lookup = self . build_lookup ( diffspec ) \n 
hunks = { } \n 
for s in selection : \n 
~~~ for hunk , header in lookup : \n 
~~~ if s . intersects ( hunk ) or hunk . contains ( s ) or ( s . begin ( ) == self . view . size ( ) and hunk ~~~ hunks . setdefault ( ( header . begin ( ) , header . end ( ) ) , [ ] ) . append ( hunk ) \n 
~~ ~~ ~~ return hunks \n 
~~ def create_patch ( self , selected_hunks ) : \n 
~~~ patch = [ ] \n 
for ( hstart , hend ) , hunks in selected_hunks . items ( ) : \n 
~~~ header = sublime . Region ( hstart , hend ) \n 
for head in self . view . lines ( header ) : \n 
~~~ headline = self . view . substr ( head ) \n 
if headline . startswith ( ) or headline . startswith ( ) : \n 
~~~ patch . append ( "%s\\n" % headline . strip ( ) ) \n 
~~~ patch . append ( "%s\\n" % headline ) \n 
~~ ~~ for h in hunks : \n 
~~~ patch . append ( self . view . substr ( self . view . full_line ( h ) ) ) \n 
~~ ~~ return "" . join ( patch ) \n 
~~ ~~ class GitDiffRefreshCommand ( TextCommand , GitDiffTextCmd ) : \n 
~~~ def is_visible ( self ) : \n 
~~ def run ( self , edit , path = None , cached = False , run_move = False ) : \n 
~~~ path = path if path else self . view . settings ( ) . get ( ) \n 
cached = cached if cached else self . view . settings ( ) . get ( ) \n 
unified = self . view . settings ( ) . get ( , 3 ) \n 
repo = self . view . settings ( ) . get ( ) \n 
if path is None or cached is None : \n 
~~ point = self . view . sel ( ) [ 0 ] . begin ( ) if self . view . sel ( ) else 0 \n 
row , col = self . view . rowcol ( point ) \n 
diff = self . get_diff ( repo , path , cached , unified = unified ) \n 
clean = False \n 
if not diff : \n 
~~~ diff = GIT_DIFF_CLEAN_CACHED if cached else GIT_DIFF_CLEAN \n 
clean = True \n 
~~ self . view . settings ( ) . set ( , clean ) \n 
self . view . set_read_only ( False ) \n 
self . view . replace ( edit , sublime . Region ( 0 , self . view . size ( ) ) , diff ) \n 
self . view . set_read_only ( True ) \n 
if run_move : \n 
~~~ self . view . run_command ( ) \n 
~~~ row_begin = self . view . text_point ( row , 0 ) \n 
line = self . view . line ( row_begin ) \n 
point = self . view . text_point ( row , min ( col , ( line . end ( ) - line . begin ( ) ) ) ) \n 
self . move_to_point ( point ) \n 
~~ ~~ ~~ class GitDiffEventListener ( EventListener ) : \n 
~~~ def on_activated ( self , view ) : \n 
~~~ if view . settings ( ) . get ( ) in ( , ) and get_setting ( ~~~ view . run_command ( ) \n 
~~ ~~ ~~ class GitDiffChangeHunkSizeCommand ( TextCommand ) : \n 
~~ def run ( self , edit , action = ) : \n 
~~~ unified = self . view . settings ( ) . get ( , 3 ) \n 
if action == : \n 
~~~ self . view . settings ( ) . set ( , unified + 1 ) \n 
~~~ self . view . settings ( ) . set ( , max ( 1 , unified - 1 ) ) \n 
~~ self . view . run_command ( ) \n 
~~ ~~ class GitDiffMoveCommand ( TextCommand , GitDiffTextCmd ) : \n 
~~ def run ( self , edit , item = , which = 0 , start = None ) : \n 
~~~ if self . view . settings ( ) . get ( ) is True : \n 
~~ if item not in ( , ) : \n 
~~~ which = int ( which ) \n 
~~~ if which not in ( , , , ) : \n 
~~ ~~ if start is not None : \n 
~~~ start = int ( start ) \n 
~~ elif self . view . sel ( ) : \n 
~~~ start = self . view . sel ( ) [ 0 ] . begin ( ) \n 
~~~ start = 0 \n 
~~ file_lookup = self . parse_diff ( ) \n 
hunk_lookup = self . build_lookup ( file_lookup ) \n 
if not hunk_lookup : \n 
~~ goto = None \n 
if which == : \n 
~~~ goto , _ = hunk_lookup [ 0 ] \n 
~~ elif which == : \n 
~~~ goto , _ = hunk_lookup [ - 1 ] \n 
~~~ if item == : \n 
~~~ next_hunks = [ ( h , f ) for h , f in hunk_lookup if h . begin ( ) > start ] \n 
goto , _ = next_hunks [ 0 ] if next_hunks else hunk_lookup [ - 1 ] \n 
~~~ next_files = [ ( f , h ) for f , h in file_lookup if f . begin ( ) > start ] \n 
goto , _ = next_files [ 0 ] if next_files else file_lookup [ - 1 ] \n 
~~ ~~ elif which == : \n 
~~~ prev_hunks = [ ( h , f ) for h , f in hunk_lookup if h . end ( ) < start ] \n 
goto , _ = prev_hunks [ - 1 ] if prev_hunks else hunk_lookup [ 0 ] \n 
~~~ prev_files = [ ( f , h ) for f , h in file_lookup if h [ - 1 ] . end ( ) < start ] \n 
goto , _ = prev_files [ - 1 ] if prev_files else file_lookup [ 0 ] \n 
~~~ goto , _ = hunk_lookup [ max ( 0 , which ) ] if which < len ( hunk_lookup ) else hunk_lookup [ - 1 ~~ else : \n 
~~~ goto , _ = file_lookup [ max ( 0 , which ) ] if which < len ( file_lookup ) else file_lookup [ - 1 \n 
~~ ~~ if goto : \n 
~~~ self . move_to_point ( goto . begin ( ) ) \n 
~~ ~~ ~~ class GitDiffStageUnstageHunkCommand ( GitDiffTextCmd , GitErrorHelper , TextCommand ) : \n 
~~ def run ( self , edit , reverse = False ) : \n 
~~~ repo = self . view . settings ( ) . get ( ) \n 
if self . view . settings ( ) . get ( ) is not reverse : \n 
~~~ if reverse : \n 
~~~ sublime . error_message ( GIT_DIFF_UNSTAGE_ERROR ) \n 
~~~ sublime . error_message ( GIT_DIFF_STAGE_ERROR ) \n 
~~ if self . view . settings ( ) . get ( ) is True : \n 
~~ hunks = self . get_hunks_from_selection ( self . view . sel ( ) ) \n 
if hunks : \n 
~~~ patch = self . create_patch ( hunks ) \n 
cmd = [ , , , if reverse else None , exit , stdout , stderr = self . git ( cmd , stdin = patch , cwd = repo ) \n 
if exit != 0 : \n 
~~~ sublime . error_message ( self . format_error_message ( stderr ) ) \n 
import sublime_plugin \n 
import subprocess \n 
from functools import reduce \n 
if int ( sublime . version ( ) ) < 3000 : \n 
~~~ import symbols \n 
from sublime_haskell_common import * \n 
~~~ import SublimeHaskell . symbols as symbols \n 
from SublimeHaskell . sublime_haskell_common import * \n 
~~ def concat_args ( args ) : \n 
~~~ def cat ( x , y ) : \n 
~~~ ( px , ex ) = x \n 
( py , ey ) = y \n 
return ( px or py , ( ex if px else [ ] ) + ( ey if py else [ ] ) ) \n 
~~ return reduce ( cat , args , ( True , [ ] ) ) [ 1 ] \n 
~~ def concat_opts ( opts ) : \n 
v = ( ex if px else { } ) . copy ( ) \n 
v . update ( ( ey if py else { } ) . copy ( ) ) \n 
return ( px or py , v ) \n 
~~ return reduce ( cat , opts , ( True , { } ) ) [ 1 ] \n 
~~ def flatten_opts ( opts ) : \n 
~~~ r = [ ] \n 
def to_opt ( x ) : \n 
~~~ return . format ( x ) \n 
~~ for k , v in opts . items ( ) : \n 
~~~ if v is None : \n 
~~~ r . append ( to_opt ( k ) ) \n 
~~ elif type ( v ) is list : \n 
~~~ for n in v : \n 
~~~ r . extend ( [ to_opt ( k ) , str ( n ) ] ) \n 
~~~ r . extend ( [ to_opt ( k ) , str ( v ) ] ) \n 
~~ ~~ return r \n 
~~ def hsdev_enabled ( ) : \n 
~~~ return get_setting_async ( ) == True \n 
~~ def hsdev_enable ( enable = True ) : \n 
~~~ set_setting_async ( , enable ) \n 
~~ def hsdev_version ( ) : \n 
~~~ ( exit_code , out , err ) = call_and_wait ( [ , ] ) \n 
if exit_code == 0 : \n 
~~~ m = re . match ( , out ) \n 
~~~ major = int ( m . group ( ) ) \n 
minor = int ( m . group ( ) ) \n 
revision = int ( m . group ( ) ) \n 
build = int ( m . group ( ) ) \n 
return [ major , minor , revision , build ] \n 
~~ ~~ ~~ except FileNotFoundError : \n 
~~ def show_version ( ver ) : \n 
~~~ return . join ( map ( lambda i : str ( i ) , ver ) ) \n 
~~ def check_version ( ver , minimal = [ 0 , 0 , 0 , 0 ] , maximal = None ) : \n 
~~~ if ver is None : \n 
~~ if ver < minimal : \n 
~~ if maximal and ver >= maximal : \n 
~~ def if_some ( x , lst ) : \n 
~~~ return lst if x is not None else [ ] \n 
~~ def cabal_path ( cabal ) : \n 
~~~ if not cabal : \n 
~~ return [ "--cabal" ] if cabal == else [ "--sandbox={0}" . format ( cabal ) ] \n 
~~ def hsinspect ( module = None , file = None , cabal = None , ghc_opts = [ ] ) : \n 
~~~ cmd = [ ] \n 
on_result = lambda s : s \n 
if module : \n 
~~~ cmd . extend ( [ module ] ) \n 
on_result = parse_module \n 
~~ elif file : \n 
~~~ cmd . extend ( [ file ] ) \n 
~~ elif cabal : \n 
~~~ cmd . extend ( [ cabal ] ) \n 
~~~ log ( , log_debug ) \n 
~~ for opt in ghc_opts : \n 
~~~ cmd . extend ( [ , opt ] ) \n 
~~ r = call_and_wait_tool ( cmd , , lambda s : json . loads ( s ) , file , None ) \n 
if r : \n 
~~~ if in r : \n 
~~~ log ( . format ( r [ ] ) , log_error ) \n 
~~~ return on_result ( r ) \n 
~~ def print_status ( s ) : \n 
~~~ print ( s [ ] ) \n 
~~ def parse_database ( s ) : \n 
~~ if s and in s and in s : \n 
~~~ return ( s [ ] , [ parse_module ( m ) for m in s [ ] ] ) \n 
~~ def parse_decls ( s ) : \n 
~~~ if s is None : \n 
~~ return [ parse_module_declaration ( decl ) for decl in s ] \n 
~~ def parse_modules_brief ( s ) : \n 
~~ return [ parse_module_id ( m ) for m in s ] \n 
~~ def get_value ( dc , ks , defval = None ) : \n 
~~~ if dc is None : \n 
~~ if type ( ks ) == list : \n 
~~~ cur = dc \n 
for k in ks : \n 
~~~ cur = cur . get ( k ) \n 
if cur is None : \n 
~~ ~~ return cur \n 
~~~ return dc . get ( ks , defval ) \n 
~~ ~~ def parse_package_db ( d , defval = None ) : \n 
~~~ if type ( d ) == dict : \n 
~~~ pdb = get_value ( d , ) \n 
return symbols . PackageDb ( package_db = pdb ) if pdb else defval \n 
~~ if d == : \n 
~~~ return symbols . PackageDb ( global_db = True ) \n 
~~~ return symbols . PackageDb ( user_db = True ) \n 
~~ return defval \n 
~~ def parse_position ( d ) : \n 
~~~ if not d : \n 
~~ line = get_value ( d , ) \n 
column = get_value ( d , ) \n 
if line is not None and column is not None : \n 
~~~ return symbols . Position ( line , column ) \n 
~~ def parse_location ( d ) : \n 
~~~ loc = symbols . Location ( \n 
get_value ( d , ) , \n 
get_value ( d , ) ) \n 
if not loc . is_null ( ) : \n 
~~~ return loc \n 
~~ loc = symbols . InstalledLocation ( \n 
symbols . parse_package ( get_value ( d , ) ) , \n 
parse_package_db ( get_value ( d , ) ) ) \n 
~~ loc = symbols . OtherLocation ( \n 
~~ def parse_import ( d ) : \n 
~~ return symbols . Import ( d [ ] , d [ ] , d . get ( ) , parse_position ( d . get ( ) ) ) \n 
~~ def parse_module_id ( d ) : \n 
~~~ if d is None : \n 
~~ return symbols . Module ( \n 
d [ ] , \n 
[ ] , [ ] , { } , \n 
parse_location ( d . get ( ) ) ) \n 
~~ def parse_declaration ( decl ) : \n 
~~~ what = decl [ ] [ ] \n 
docs = crlf2lf ( decl . get ( ) ) \n 
name = decl [ ] \n 
pos = parse_position ( decl . get ( ) ) \n 
imported = [ ] \n 
if in decl and decl [ ] : \n 
~~~ imported = [ parse_import ( d ) for d in decl [ ] ] \n 
~~ defined = None \n 
~~~ defined = parse_module_id ( decl [ ] ) \n 
~~ if what == : \n 
~~~ return symbols . Function ( name , decl [ ] . get ( ) , docs , imported , defined , pos ) \n 
~~ elif what == : \n 
~~~ return symbols . Type ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ elif what == : \n 
~~~ return symbols . Newtype ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ elif what == : \n 
~~~ return symbols . Data ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ elif what == : \n 
~~~ return symbols . Class ( name , decl [ ] [ ] . get ( ) , decl [ ] [ ] . get ( ~~ else : \n 
~~~ log ( . format ( e ) , log_error ) \n 
~~ ~~ def parse_declarations ( decls ) : \n 
~~~ if decls is None : \n 
~~ return [ parse_declaration ( d ) for d in decls ] \n 
~~ def parse_module_declaration ( d , parse_module_info = True ) : \n 
~~~ m = None \n 
if in d and parse_module_info : \n 
~~~ m = parse_module_id ( d [ ] ) \n 
~~ loc = parse_location ( d [ ] . get ( ) ) \n 
decl = parse_declaration ( d [ ] ) \n 
if not decl : \n 
~~ decl . module = m \n 
return decl \n 
~~ ~~ def parse_module ( d ) : \n 
d . get ( ) , \n 
[ parse_import ( i ) for i in d [ ] ] if in d else [ ] , \n 
dict ( ( decl [ ] , parse_declaration ( decl ) ) for decl in d [ ] ) if parse_location ( d . get ( ) ) ) \n 
~~ def parse_modules ( ds ) : \n 
~~~ if ds is None : \n 
~~ return [ parse_module ( d ) for d in ds ] \n 
~~ def parse_cabal_package ( d ) : \n 
~~ return symbols . CabalPackage ( \n 
d . get ( ) ) \n 
~~ def parse_corrections ( d ) : \n 
~~ return [ parse_correction ( c ) for c in d ] \n 
~~ def parse_correction ( d ) : \n 
~~~ return symbols . Correction ( \n 
d [ ] [ ] , \n 
parse_corrector ( d [ ] [ ] ) ) \n 
~~ def parse_corrector ( d ) : \n 
~~~ return symbols . Corrector ( \n 
parse_position ( d [ ] [ ] ) , \n 
d [ ] ) \n 
~~ def encode_corrections ( cs ) : \n 
~~~ return [ encode_correction ( c ) for c in cs ] \n 
~~ def encode_correction ( c ) : \n 
: c . file } , \n 
: c . level , \n 
: encode_corrector ( c . corrector ) , \n 
: c . message } , \n 
: encode_position ( c . corrector . start . from_zero_based ( ) ) , \n 
: encode_position ( c . corrector . end . from_zero_based ( ) ) } \n 
~~ def encode_corrector ( c ) : \n 
: encode_position ( c . start ) , \n 
: encode_position ( c . end ) } , \n 
: c . contents } \n 
~~ def encode_position ( p ) : \n 
: p . line , \n 
: p . column } \n 
~~ def encode_package_db ( db ) : \n 
~~~ if db . user_db : \n 
~~ if db . global_db : \n 
~~ if db . package_db : \n 
~~~ return { : db . package_db } \n 
~~ def reconnect_function ( fn ) : \n 
~~~ def wrapped ( self , * args , ** kwargs ) : \n 
~~~ autoconnect_ = kwargs . pop ( , False ) \n 
on_reconnect_ = kwargs . pop ( , None ) \n 
just_connect_ = kwargs . pop ( , False ) \n 
def run_fn ( ) : \n 
~~~ if not just_connect_ : \n 
~~~ self . autoconnect = autoconnect_ \n 
self . on_reconnect = on_reconnect_ \n 
~~ return fn ( self , * args , ** kwargs ) \n 
~~ if not just_connect_ : \n 
~~~ self . set_reconnect_function ( run_fn ) \n 
~~ return run_fn ( ) \n 
~~ return wrapped \n 
~~ class begin_connecting ( object ) : \n 
~~~ def __init__ ( self , agent ) : \n 
~~~ self . agent = agent \n 
~~~ self . agent . set_connecting ( ) \n 
~~ def __exit__ ( self , type , value , traceback ) : \n 
~~~ if type : \n 
~~~ self . agent . set_unconnected ( ) \n 
~~~ if self . agent . is_connecting ( ) : \n 
~~ ~~ ~~ ~~ def connect_function ( fn ) : \n 
~~~ if self . is_unconnected ( ) : \n 
~~~ with begin_connecting ( self ) : \n 
~~~ return fn ( self , * args , ** kwargs ) \n 
~~~ log ( , log_warning ) \n 
~~ ~~ return wrapped \n 
~~ def hsdev_command ( async = False , timeout = None , is_list = False ) : \n 
~~~ def wrap_function ( fn ) : \n 
~~~ wait_flag = kwargs . pop ( , not async ) \n 
timeout_arg = kwargs . pop ( , timeout ) \n 
on_resp = kwargs . pop ( , None ) \n 
on_not = kwargs . pop ( , None ) \n 
on_err = kwargs . pop ( , None ) \n 
on_res_part = kwargs . pop ( , None ) \n 
split_res = kwargs . pop ( , on_res_part is not None ) \n 
( name_ , opts_ , on_result_ ) = fn ( self , * args , ** kwargs ) \n 
if is_list and split_res : \n 
~~~ result = [ ] \n 
def on_notify ( n ) : \n 
~~~ if in n : \n 
~~~ rp = on_result_ ( [ n [ ] ] ) [ 0 ] \n 
call_callback ( on_res_part , rp ) \n 
result . append ( rp ) \n 
~~~ call_callback ( on_not , n ) \n 
~~ ~~ def on_response ( r ) : \n 
~~~ on_resp ( result ) \n 
r = self . call ( \n 
name_ , \n 
opts_ , \n 
on_response = on_response if on_resp else None , \n 
on_notify = on_notify , \n 
on_error = on_err , \n 
wait = wait_flag , \n 
timeout = timeout_arg ) \n 
if wait_flag : \n 
~~ return r \n 
~~~ def on_response ( r ) : \n 
~~~ on_resp ( on_result_ ( r ) ) \n 
~~ r = self . call ( \n 
on_notify = on_not , \n 
~~~ return on_result_ ( r ) \n 
~~ return wrap_function \n 
~~ def command ( fn ) : \n 
~~~ return hsdev_command ( async = False , timeout = 1 ) ( fn ) \n 
~~ def async_command ( fn ) : \n 
~~~ return hsdev_command ( async = True ) ( fn ) \n 
~~ def list_command ( fn ) : \n 
~~~ return hsdev_command ( async = False , timeout = 1 , is_list = True ) ( fn ) \n 
~~ def async_list_command ( fn ) : \n 
~~~ return hsdev_command ( async = True , is_list = True ) ( fn ) \n 
~~ def cmd ( name_ , opts_ = { } , on_result = lambda r : r ) : \n 
~~~ return ( name_ , opts_ , on_result ) \n 
~~ def call_callback ( fn , * args , ** kwargs ) : \n 
~~~ name = kwargs . get ( ) \n 
if name : \n 
~~~ del kwargs [ ] \n 
~~~ if fn is not None : \n 
~~~ fn ( * args , ** kwargs ) \n 
~~ ~~ def format_error_details ( ds ) : \n 
~~~ return . join ( [ . format ( k , v ) for k , v in ds . items ( ) ] ) \n 
~~ class HsDevCallbacks ( object ) : \n 
~~~ def __init__ ( self , id , command , on_response = None , on_notify = None , on_error = None ) : \n 
~~~ self . id = id \n 
self . command = command \n 
self . start_time = time . clock ( ) \n 
self . on_response = on_response \n 
self . on_notify = on_notify \n 
self . on_error = on_error \n 
~~~ return time . clock ( ) - self . start_time if self . start_time is not None else None \n 
~~ def log_time ( self ) : \n 
~~~ log ( . format ( self . command , self . time ( ) ) , log_trace ) \n 
~~ def call_response ( self , r ) : \n 
~~~ self . log_time ( ) \n 
call_callback ( self . on_response , r ) \n 
~~ def call_notify ( self , n ) : \n 
~~~ call_callback ( self . on_notify , n ) \n 
~~ def call_error ( self , e , ds ) : \n 
log ( . format ( self . command , e , format_error_details ( ds ) ) , log_error call_callback ( self . on_error , e , ds ) \n 
~~ ~~ class HsDev ( object ) : \n 
~~~ def __init__ ( self , port = 4567 ) : \n 
~~~ self . port = port \n 
self . connecting = threading . Event ( ) \n 
self . connected = threading . Event ( ) \n 
self . socket = None \n 
self . listener = None \n 
self . hsdev_address = None \n 
self . autoconnect = True \n 
self . map = LockedObject ( { } ) \n 
self . id = 1 \n 
self . connect_fun = None \n 
self . part = \n 
self . on_connected = None \n 
self . on_disconnected = None \n 
self . on_reconnect = None \n 
~~~ self . close ( ) \n 
~~ def set_reconnect_function ( self , f ) : \n 
~~~ if self . connect_fun is None : \n 
~~~ self . connect_fun = f \n 
~~ ~~ def reconnect ( self ) : \n 
~~~ if self . connect_fun is not None : \n 
~~~ log ( , log_info ) \n 
call_callback ( self . on_reconnect , name = ) \n 
self . connect_fun ( ) \n 
def run_server ( port = 4567 , cache = None , log_file = None , log_config = None ) : \n 
~~~ cmd = concat_args ( [ \n 
( True , [ "hsdev" , "run" ] ) , \n 
( port , [ "--port" , str ( port ) ] ) , \n 
( cache , [ "--cache" , cache ] ) , \n 
( log_file , [ "--log" , log_file ] ) , \n 
( log_config , [ "--log-config" , log_config ] ) ] ) \n 
log ( , log_info ) \n 
p = call_and_wait ( cmd , wait = False ) \n 
if not p : \n 
~~~ log ( , log_error ) \n 
~~~ output = crlf2lf ( decode_bytes ( p . stdout . readline ( ) ) ) \n 
m = re . match ( , output ) \n 
~~~ log ( . format ( m . group ( ) ) ) \n 
p . stdout . close ( ) \n 
p . stderr . close ( ) \n 
~~ ~~ ~~ @ staticmethod \n 
def start_server ( port = 4567 , cache = None , log_file = None , log_config = None ) : \n 
( True , [ "hsdev" , "start" ] ) , \n 
def parse_response ( s ) : \n 
~~~ return { } if s . isspace ( ) else json . loads ( s ) \n 
~~~ return { : , : s } \n 
~~ ~~ log ( , log_info ) \n 
ret = call_and_wait_tool ( cmd , , , None , None , None , check_enabled = False ) \n 
if ret is not None : \n 
~~~ return ret \n 
def client ( port = 4567 , cache = None , autoconnect = False ) : \n 
~~~ start_server ( port = port , cache = cache ) \n 
h = HsDev ( port = port ) \n 
h . connect ( autoconnect = autoconnect ) \n 
return h \n 
def client_async ( port = 4567 , cache = None , autoconnect = False ) : \n 
h . connect_async ( autoconnect = autoconnect ) \n 
~~ @ connect_function \n 
@ reconnect_function \n 
def connect ( self , tries = 10 , delay = 1.0 ) : \n 
~~~ self . socket = socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) \n 
for n in range ( 0 , tries ) : \n 
~~~ log ( . format ( n ) , log_info ) \n 
self . socket . connect ( ( , self . port ) ) \n 
self . hsdev_socket = self . socket \n 
self . hsdev_address = \n 
self . set_connected ( ) \n 
self . listener = threading . Thread ( target = self . listen ) \n 
self . listener . start ( ) \n 
call_callback ( self . on_connected , name = ) \n 
~~~ log ( . format ( n ) , log_warning ) \n 
time . sleep ( delay ) \n 
~~ @ reconnect_function \n 
def connect_async ( self , tries = 10 , delay = 1.0 ) : \n 
~~~ thread = threading . Thread ( \n 
target = self . connect , \n 
kwargs = { : tries , : delay , : True } ) \n 
thread . start ( ) \n 
~~ def wait ( self , timeout = None ) : \n 
~~~ return self . connected . wait ( timeout ) \n 
~~ self . connected . clear ( ) \n 
if self . hsdev_socket : \n 
~~~ self . hsdev_socket . close ( ) \n 
self . hsdev_socket = None \n 
~~ self . socket . close ( ) \n 
~~ def is_connecting ( self ) : \n 
~~~ return self . connecting . is_set ( ) \n 
~~~ return self . connected . is_set ( ) \n 
~~ def is_unconnected ( self ) : \n 
~~~ return ( not self . is_connecting ( ) ) and ( not self . is_connected ( ) ) \n 
~~ def set_unconnected ( self ) : \n 
~~~ if self . connecting . is_set ( ) : \n 
~~~ self . connecting . clear ( ) \n 
~~ if self . connected . is_set ( ) : \n 
~~~ self . connected . clear ( ) \n 
~~ ~~ def set_connecting ( self ) : \n 
~~~ self . set_unconnected ( ) \n 
self . connecting . set ( ) \n 
~~ def set_connected ( self ) : \n 
~~~ if self . is_connecting ( ) : \n 
~~~ self . connected . set ( ) \n 
self . connecting . clear ( ) \n 
~~ ~~ def on_receive ( self , id , command , on_response = None , on_notify = None , on_error = None ) : \n 
~~~ with self . map as m : \n 
~~~ m [ id ] = HsDevCallbacks ( id , command , on_response , on_notify , on_error ) \n 
~~ ~~ def verify_connected ( self ) : \n 
~~~ self . connection_lost ( , ) \n 
return self . is_connected ( ) \n 
~~ ~~ def connection_lost ( self , fn , e ) : \n 
~~ self . close ( ) \n 
log ( . format ( fn , e ) , log_error ) \n 
call_callback ( self . on_disconnected , name = ) \n 
with self . map as m : \n 
~~~ for on_msg in m . values ( ) : \n 
~~~ on_msg . on_error ( ) \n 
~~ m . clear ( ) \n 
~~ self . id = 1 \n 
if self . autoconnect : \n 
~~~ self . reconnect ( ) \n 
~~~ args_cmd = . format ( command ) \n 
call_cmd = . format ( command , opts ) \n 
if not self . verify_connected ( ) : \n 
~~~ return None if wait else False \n 
~~~ wait_receive = threading . Event ( ) if wait else None \n 
x = { } \n 
def on_response_ ( r ) : \n 
~~~ x [ ] = r \n 
call_callback ( on_response , r ) \n 
if wait_receive : \n 
~~~ wait_receive . set ( ) \n 
~~ ~~ def on_error_ ( e , ds ) : \n 
~~~ call_callback ( on_error , e , ds ) \n 
~~ ~~ if wait or on_response or on_notify or on_error : \n 
~~~ if id is None : \n 
~~~ id = str ( self . id ) \n 
self . id = self . id + 1 \n 
~~ self . on_receive ( id , args_cmd , on_response_ , on_notify , on_error_ ) \n 
~~ opts . update ( { : True } ) \n 
opts . update ( { : id , : command } ) \n 
msg = json . dumps ( opts , separators = ( , ) ) \n 
self . hsdev_socket . sendall ( msg . encode ( ) ) \n 
self . hsdev_socket . sendall ( . encode ( ) ) \n 
log ( call_cmd , log_trace ) \n 
if wait : \n 
~~~ wait_receive . wait ( timeout ) \n 
return x . get ( ) \n 
~~~ log ( . format ( call_cmd , e ) , log_error ) \n 
self . connection_lost ( , e ) \n 
~~ ~~ def listen ( self ) : \n 
~~~ while self . verify_connected ( ) : \n 
~~~ resp = json . loads ( self . get_response ( ) ) \n 
if in resp : \n 
~~~ callbacks = None \n 
~~~ if resp [ ] in m : \n 
~~~ callbacks = m [ resp [ ] ] \n 
~~ ~~ if callbacks : \n 
~~~ if in resp : \n 
~~~ callbacks . call_notify ( resp [ ] ) \n 
~~ if in resp : \n 
~~~ err = resp . pop ( "error" ) \n 
callbacks . call_error ( err , resp ) \n 
~~~ m . pop ( resp [ ] ) \n 
~~ ~~ if in resp : \n 
~~~ callbacks . call_response ( resp [ ] ) \n 
~~ ~~ ~~ ~~ ~~ except Exception as e : \n 
~~~ self . connection_lost ( , e ) \n 
~~ ~~ ~~ def get_response ( self ) : \n 
~~~ while not in self . part : \n 
~~~ self . part = self . part + self . socket . recv ( 65536 ) . decode ( ) \n 
~~ ( r , _ , post ) = self . part . partition ( ) \n 
self . part = post \n 
return r \n 
~~ @ command \n 
def link ( self , hold = False , ** kwargs ) : \n 
~~~ return cmd ( , { \n 
: hold } ) \n 
def ping ( self ) : \n 
~~~ return cmd ( , { } , lambda r : r and ( in r ) and ( r [ ] == ) ) \n 
~~ @ async_command \n 
def scan ( self , cabal = False , sandboxes = [ ] , projects = [ ] , files = [ ] , paths = [ ] , ghc = [ ] , contents ~~~ return cmd ( , { \n 
: projects , \n 
: cabal , \n 
: sandboxes , \n 
: files , \n 
: paths , \n 
: [ { : f , : cts } for f , cts in contents . items ( ) ] , \n 
: ghc , \n 
: docs , \n 
: infer } ) \n 
def docs ( self , projects = [ ] , files = [ ] , modules = [ ] ) : \n 
: modules } ) \n 
def infer ( self , projects = [ ] , files = [ ] , modules = [ ] ) : \n 
~~ @ async_list_command \n 
def remove ( self , cabal = False , sandboxes = [ ] , projects = [ ] , files = [ ] , packages = [ ] ) : \n 
: packages } ) \n 
def remove_all ( self ) : \n 
~~~ return cmd ( , { } ) \n 
~~ @ list_command \n 
def list_modules ( self , project = None , file = None , module = None , deps = None , sandbox = None , ~~~ fs = [ ] \n 
if project : \n 
~~~ fs . append ( { : project } ) \n 
~~ if file : \n 
~~~ fs . append ( { : file } ) \n 
~~ if module : \n 
~~~ fs . append ( { : module } ) \n 
~~ if deps : \n 
~~~ fs . append ( { : deps } ) \n 
~~ if sandbox : \n 
~~~ fs . append ( { : { : sandbox } } ) \n 
~~ if cabal : \n 
~~~ fs . append ( { : } ) \n 
~~ if db : \n 
~~~ fs . append ( { : encode_package_db ( db ) } ) \n 
~~ if package : \n 
~~~ fs . append ( { : package } ) \n 
~~ if source : \n 
~~~ fs . append ( ) \n 
~~ if standalone : \n 
~~ return cmd ( , { : fs } , parse_modules_brief ) \n 
def list_packages ( self ) : \n 
def list_projects ( self ) : \n 
~~~ q = { : input , : search_type } \n 
fs = [ ] \n 
~~ return cmd ( , { : q , : fs , : locals } , parse_decls ) \n 
def module ( self , input = "" , search_type = , project = None , file = None , module = None , ~~~ q = { : input , : search_type } \n 
~~ return cmd ( , { : q , : fs } , parse_modules ) \n 
def resolve ( self , file , exports = False ) : \n 
~~~ return cmd ( , { : file , : exports } , parse_module ) \n 
def project ( self , project = None , path = None ) : \n 
~~~ return cmd ( , { : project } if project else { : path } ) \n 
def sandbox ( self , path ) : \n 
~~~ return cmd ( , { : path } ) \n 
def lookup ( self , name , file ) : \n 
~~~ return cmd ( , { : name , : file } , parse_decls ) \n 
def whois ( self , name , file ) : \n 
~~~ return cmd ( , { : name , : file } , parse_declarations ) \n 
def scope_modules ( self , file , input = , search_type = ) : \n 
~~~ return cmd ( , { : { : input , : search_type } , : file } , \n 
def scope ( self , file , input = , search_type = , global_scope = False ) : \n 
~~~ return cmd ( , { : { : input , : search_type } , : global_scope , \n 
def complete ( self , input , file , wide = False ) : \n 
~~~ return cmd ( , { : input , : wide , : file } , parse_declarations ) \n 
def hayoo ( self , query , page = None , pages = None ) : \n 
~~~ return cmd ( , { : query , : page or 0 , : pages or 1 } , parse_decls ) \n 
def cabal_list ( self , packages ) : \n 
~~~ cmd ( , { : packages } , lambda r : [ parse_cabal_package ( s ) for s in r ] if r \n 
def lint ( self , files = [ ] , contents = { } , hlint = [ ] ) : \n 
: hlint } ) \n 
def check ( self , files = [ ] , contents = { } , ghc = [ ] ) : \n 
: ghc } ) \n 
def check_lint ( self , files = [ ] , contents = { } , ghc = [ ] , hlint = [ ] ) : \n 
def types ( self , files = [ ] , contents = { } , ghc = [ ] ) : \n 
def ghcmod_lang ( self ) : \n 
~~~ return cmd ( ) \n 
def ghcmod_flags ( self ) : \n 
def ghcmod_type ( self , file , line , column = 1 , ghc = [ ] ) : \n 
: { : int ( line ) , : int ( column ) } , \n 
: file , \n 
def ghcmod_check ( self , files , ghc = [ ] ) : \n 
~~~ return cmd ( , { : files , : ghc } ) \n 
def ghcmod_lint ( self , files , hlint = [ ] ) : \n 
~~~ return cmd ( , { : files , : hlint } ) \n 
def ghcmod_check_lint ( self , files , ghc = [ ] , hlint = [ ] ) : \n 
~~~ return cmd ( , { : files , : ghc , : hlint } ) \n 
def autofix_show ( self , messages ) : \n 
~~~ return cmd ( , { : messages } , parse_corrections ) \n 
def autofix_fix ( self , messages , rest = [ ] , pure = False ) : \n 
~~~ return cmd ( , { : messages , : rest , : pure } , parse_corrections \n 
def ghc_eval ( self , exprs ) : \n 
~~~ return cmd ( , { : exprs } ) \n 
def exit ( self ) : \n 
~~ ~~ def wait_result ( fn , * args , ** kwargs ) : \n 
~~~ wait_receive = threading . Event ( ) \n 
x = { : None } \n 
on_resp = kwargs . get ( ) \n 
on_err = kwargs . get ( ) \n 
def wait_response ( r ) : \n 
if on_resp : \n 
~~~ on_resp ( r ) \n 
~~ wait_receive . set ( ) \n 
~~ def wait_error ( e , ds ) : \n 
~~~ log ( . format ( e , format_error_details ( ds ) ) ) \n 
if on_err : \n 
~~~ on_err ( e , ds ) \n 
~~ tm = kwargs . pop ( , 0.1 ) \n 
kwargs [ ] = wait_response \n 
kwargs [ ] = wait_error \n 
fn ( * args , ** kwargs ) \n 
wait_receive . wait ( tm ) \n 
return x [ ] \n 
~~ class HsDevProcess ( threading . Thread ) : \n 
~~~ def __init__ ( self , port = 4567 , cache = None , log_file = None , log_config = None ) : \n 
~~~ super ( HsDevProcess , self ) . __init__ ( ) \n 
self . process = None \n 
self . on_start = None \n 
self . on_exit = None \n 
self . stop_event = threading . Event ( ) \n 
self . create_event = threading . Event ( ) \n 
self . cache = cache \n 
self . log_file = log_file \n 
self . log_config = log_config \n 
~~~ self . create_event . wait ( ) \n 
self . create_event . clear ( ) \n 
while not self . stop_event . is_set ( ) : \n 
~~~ self . process = HsDev . run_server ( port = self . port , cache = self . cache , log_file = self if not self . process : \n 
self . stop_event . set ( ) \n 
~~~ call_callback ( self . on_start , name = ) \n 
~~ self . process . wait ( ) \n 
call_callback ( self . on_exit , name = ) \n 
~~ self . stop_event . clear ( ) \n 
~~ ~~ def active ( self ) : \n 
~~~ return self . process . poll ( ) is None \n 
~~ def inactive ( self ) : \n 
~~~ return self . process . poll ( ) is not None \n 
~~ def create ( self ) : \n 
~~~ self . create_event . set ( ) \n 
~~~ self . stop_event . set ( ) \n 
from SublimeLinter . lint import Linter \n 
class Phpcs ( Linter ) : \n 
syntax = ( , , ) \n 
regex = ( \n 
executable = \n 
defaults = { \n 
inline_overrides = ( ) \n 
tempfile_suffix = \n 
def cmd ( self ) : \n 
settings = Linter . get_view_settings ( self ) \n 
if in settings : \n 
~~~ command = [ settings . get ( ) ] \n 
~~~ command = [ self . executable_path ] \n 
~~ command . append ( ) \n 
return command \n 
~~ ~~ import sublime \n 
import re , os \n 
completions = [ ] \n 
SETTINGS = sublime . load_settings ( ) \n 
def add_methods ( cfc_file , hint_text ) : \n 
~~~ with open ( cfc_file , ) as f : \n 
~~~ read_data = f . read ( ) \n 
~~ methods = [ ] \n 
method_lines = re . findall ( , read_data ) \n 
for l in method_lines : \n 
s = re . search ( , l ) \n 
if s : \n 
~~~ methods . append ( s . group ( ) . strip ( ) ) \n 
~~ ~~ for c in methods : \n 
~~~ snippet = c \n 
params = re . sub ( "\\w+\\(" , "" , snippet , 1 ) [ : - 1 ] . split ( "," ) \n 
num = 1 \n 
if len ( params [ 0 ] ) : \n 
~~~ for p in params : \n 
~~~ snippet = snippet . replace ( p , + str ( num ) + + p + ) \n 
num = num + 1 \n 
~~ ~~ c = re . sub ( "\\(.*\\)" , "" , c ) \n 
~~ ~~ class MethodsAutoComplete ( sublime_plugin . EventListener ) : \n 
~~~ def on_query_completions ( self , view , prefix , locations ) : \n 
~~~ if not view . match_selector ( locations [ 0 ] , \n 
~~ if not SETTINGS . get ( "component_method_completions" ) : \n 
~~ _completions = [ ] \n 
~~~ cfc_region = view . find_by_selector ( "meta.component-operator.extends.value.cfscript" ) [ 0 ] \n 
~~~ cfc_region = "" \n 
~~ if len ( cfc_region ) : \n 
~~~ extendspath = view . substr ( cfc_region ) . replace ( "." , "/" ) \n 
this_file = view . file_name ( ) \n 
if not dir_len > 0 : \n 
~~ this_dir = this_file [ : ( dir_len + 1 ) ] \n 
cfc_file = this_dir + extendspath + ".cfc" \n 
if not os . path . isfile ( cfc_file ) : \n 
~~~ for folder in sublime . active_window ( ) . folders ( ) : \n 
~~~ if os . path . isfile ( folder + "/" + extendspath + ".cfc" ) : \n 
~~~ cfc_file = folder + "/" + extendspath + ".cfc" \n 
~~~ add_methods ( cfc_file , view . substr ( cfc_region ) . split ( "." ) [ - 1 ] ) \n 
~~ except UnboundLocalError : \n 
~~ ~~ add_methods ( view . file_name ( ) , "this" ) \n 
_completions . extend ( completions ) \n 
del completions [ : ] \n 
return _completions \n 
CMD_TARGET_APPLICATION = 0 \n 
CMD_TARGET_WINDOW = 1 \n 
CMD_TARGET_VIEW = 2 \n 
CMD_RUN = \n 
CMD_KEY = \n 
CMD_SET = \n 
def str_to_dict ( s ) : \n 
els = s . split ( ) \n 
for el in els : \n 
~~~ key , value = el . split ( ) \n 
~~~ d [ ] = eval ( value , { } , { } ) \n 
~~~ d [ ] = value \n 
~~ ~~ return d \n 
~~ def run_ ( cmd ) : \n 
~~~ target , predicate = cmd [ ] , cmd [ ] \n 
if cmd [ ] : \n 
~~~ if cmd [ ] : \n 
~~~ target . run_command ( ) \n 
~~ cmd_ , _ , args = predicate . partition ( ) \n 
if args : \n 
~~~ args = str_to_dict ( args ) \n 
~~~ args = { } \n 
~~ if not cmd [ ] : \n 
~~~ target . run_command ( str ( cmd_ ) , args ) \n 
~~ ~~ def set_ ( cmd ) : \n 
~~~ syntax = os . path . basename ( target . settings ( ) . get ( ) ) \n 
target . run_command ( , { \n 
: syntax , \n 
: predicate \n 
~~ if not target . settings ( ) . has ( predicate ) : \n 
sublime . status_message ( msg ) \n 
~~~ name , _ , value = predicate . partition ( ) \n 
target . settings ( ) . set ( name , eval ( value , { } , { } ) ) \n 
~~ except ValueError , e : \n 
~~ ~~ def key_ ( args ) : \n 
~~ from vex . parsers . g_cmd import GlobalLexer \n 
class TestGlobalLexer ( unittest . TestCase ) : \n 
~~~ self . lexer = GlobalLexer ( ) \n 
~~ def testCanMatchFullPattern ( self ) : \n 
~~~ actual = self . lexer . parse ( ) \n 
self . assertEqual ( actual , [ , ] ) \n 
~~ def testCanMatchEmtpySearch ( self ) : \n 
~~ def testCanEscapeCharactersInSearchPattern ( self ) : \n 
~~ def testCanEscapeBackSlashes ( self ) : \n 
from django . contrib import admin \n 
from . models import Product , Option \n 
admin . site . register ( Product ) \n 
admin . site . register ( Option ) \n 
dict_of = lambda o : { k : getattr ( o , k ) for k in dir ( o ) if not in k and not callable ( getattr ( o \n 
from django . db import connections \n 
def fetch ( query , params = [ ] , db = ) : \n 
~~~ cursor = connections [ db ] . cursor ( ) \n 
cursor . execute ( query , params ) \n 
return cursor . fetchall ( ) \n 
~~ from time import time \n 
from funcy . flow import * \n 
def test_silent ( ) : \n 
~~~ assert silent ( int ) ( 1 ) == 1 \n 
assert silent ( int ) ( ) == 1 \n 
assert silent ( int ) ( ) is None \n 
assert silent ( str . upper ) ( ) == \n 
~~ class MyError ( Exception ) : \n 
~~ def test_ignore ( ) : \n 
~~~ assert ignore ( Exception ) ( raiser ( Exception ) ) ( ) is None \n 
assert ignore ( Exception ) ( raiser ( MyError ) ) ( ) is None \n 
assert ignore ( ( TypeError , MyError ) ) ( raiser ( MyError ) ) ( ) is None \n 
with pytest . raises ( TypeError ) : \n 
~~~ ignore ( MyError ) ( raiser ( TypeError ) ) ( ) \n 
~~ assert ignore ( MyError , default = 42 ) ( raiser ( MyError ) ) ( ) == 42 \n 
~~ def test_raiser ( ) : \n 
~~~ with pytest . raises ( Exception ) as e : raiser ( ) ( ) \n 
assert e . type is Exception \n 
with pytest . raises ( MyError ) : raiser ( MyError ) ( ) \n 
with pytest . raises ( MyError ) as e : raiser ( MyError , ) ( ) \n 
assert e . value . args == ( , ) \n 
with pytest . raises ( MyError ) : raiser ( MyError ( ) ) ( ) \n 
with pytest . raises ( MyError ) : raiser ( MyError ) ( , keyword = ) \n 
~~ def test_suppress ( ) : \n 
~~~ with suppress ( Exception ) : \n 
~~~ raise Exception \n 
~~ with suppress ( Exception ) : \n 
~~~ raise MyError \n 
~~ with pytest . raises ( TypeError ) : \n 
~~~ with suppress ( MyError ) : \n 
~~~ raise TypeError \n 
~~ ~~ with suppress ( TypeError , MyError ) : \n 
~~ ~~ def test_retry ( ) : \n 
~~~ calls = [ ] \n 
def failing ( n = 1 ) : \n 
~~~ if len ( calls ) < n : \n 
~~~ calls . append ( 1 ) \n 
raise MyError \n 
~~ with pytest . raises ( MyError ) : failing ( ) \n 
calls = [ ] \n 
assert retry ( 2 , MyError ) ( failing ) ( ) == 1 \n 
with pytest . raises ( MyError ) : retry ( 2 , MyError ) ( failing ) ( 2 ) \n 
~~ def test_retry_timeout ( ) : \n 
~~~ def failing ( ) : \n 
~~ start_time = time ( ) \n 
with pytest . raises ( MyError ) : retry ( 11 , MyError , timeout = 0.01 ) ( failing ) ( ) \n 
assert 0.1 < time ( ) - start_time < 0.11 \n 
start_time = time ( ) \n 
with pytest . raises ( MyError ) : retry ( 4 , MyError , timeout = lambda a : 0.01 * 2 ** a ) ( failing ) ( ) \n 
d = time ( ) - start_time \n 
assert 0.07 < d < 0.08 \n 
~~ def test_retry_many_errors ( ) : \n 
~~ assert retry ( 2 , ( MyError , RuntimeError ) ) ( failing ) ( ) == 1 \n 
assert retry ( 2 , [ MyError , RuntimeError ] ) ( failing ) ( ) == 1 \n 
~~ def test_fallback ( ) : \n 
~~~ assert fallback ( raiser ( ) , lambda : 1 ) == 1 \n 
with pytest . raises ( Exception ) : fallback ( ( raiser ( ) , MyError ) , lambda : 1 ) \n 
assert fallback ( ( raiser ( MyError ) , MyError ) , lambda : 1 ) == 1 \n 
~~ def test_limit_error_rate ( ) : \n 
@ limit_error_rate ( 2 , 60 , MyError ) \n 
def limited ( x ) : \n 
~~~ calls . append ( x ) \n 
raise TypeError \n 
~~ with pytest . raises ( TypeError ) : limited ( 1 ) \n 
with pytest . raises ( TypeError ) : limited ( 2 ) \n 
with pytest . raises ( MyError ) : limited ( 3 ) \n 
assert calls == [ 1 , 2 ] \n 
~~ def test_post_processing ( ) : \n 
~~~ @ post_processing ( max ) \n 
def my_max ( l ) : \n 
~~~ return l \n 
~~ assert my_max ( [ 1 , 3 , 2 ] ) == 3 \n 
~~ def test_collecting ( ) : \n 
~~~ @ collecting \n 
def doubles ( l ) : \n 
~~~ for i in l : \n 
~~~ yield i * 2 \n 
~~ ~~ assert doubles ( [ 1 , 2 ] ) == [ 2 , 4 ] \n 
~~ def test_once ( ) : \n 
@ once \n 
def call ( n ) : \n 
~~~ calls . append ( n ) \n 
return n \n 
~~ call ( 1 ) \n 
call ( 2 ) \n 
assert calls == [ 1 ] \n 
~~ def test_once_per ( ) : \n 
@ once_per ( ) \n 
def call ( n , x = None ) : \n 
call ( 1 , 42 ) \n 
~~ def test_once_per_args ( ) : \n 
@ once_per_args \n 
assert calls == [ 1 , 2 , 1 ] \n 
call ( 1 ) \n 
long_description = open ( ) . read ( ) , \n 
packages = [ , , ] , \n 
