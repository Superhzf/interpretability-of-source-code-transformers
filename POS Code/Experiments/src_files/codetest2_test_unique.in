from __future__ import division , print_function , unicode_literals \n 
\n 
from collections import OrderedDict \n 
from brainstorm . layers . base_layer import Layer \n 
from brainstorm . structure . buffer_structure import ( BufferStructure , \n 
StructureTemplate ) \n 
from brainstorm . structure . construction import ConstructionWrapper \n 
from brainstorm . utils import flatten_all_but_last \n 
def BatchNorm ( name = None , decay = 0.9 , epsilon = 1.0e-5 ) : \n 
return ConstructionWrapper . create ( BatchNormLayerImpl , \n 
name = name , \n 
decay = decay , \n 
epsilon = epsilon ) \n 
~~ class BatchNormLayerImpl ( Layer ) : \n 
~~~ expected_inputs = { : StructureTemplate ( , , ) } \n 
expected_kwargs = { , } \n 
def setup ( self , kwargs , in_shapes ) : \n 
~~~ self . epsilon = kwargs . get ( , 1.0e-5 ) \n 
self . decay = kwargs . get ( , 0.9 ) \n 
outputs = OrderedDict ( ) \n 
outputs [ ] = in_shapes [ ] \n 
parameters = OrderedDict ( ) \n 
buf = BufferStructure ( self . in_shapes [ ] . feature_shape [ - 1 ] ) \n 
parameters [ ] = buf \n 
internals = OrderedDict ( ) \n 
internals [ ] = buf \n 
internals [ ] = self . in_shapes [ ] \n 
return outputs , parameters , internals \n 
~~ def forward_pass ( self , buffers , training_pass = True ) : \n 
~~~ _h = self . handler \n 
sigma_b , centered , x_hat = buffers . internals \n 
gamma , beta , mu , sigma = buffers . parameters \n 
inputs = flatten_all_but_last ( buffers . inputs . default ) \n 
centered = flatten_all_but_last ( centered ) \n 
x_hat = flatten_all_but_last ( x_hat ) \n 
out = flatten_all_but_last ( buffers . outputs . default ) \n 
m = inputs . shape [ 0 ] \n 
if training_pass : \n 
_h . sum_t ( inputs , 0 , mu_b ) \n 
_h . mult_st ( - 1.0 / m , mu_b , mu_b ) \n 
_h . mult_st ( self . decay , mu , mu ) \n 
_h . mult_add_st ( 1.0 - self . decay , mu_b , mu ) \n 
mu = mu_b \n 
~~ _h . add_mv ( inputs , mu . reshape ( ( 1 , mu . size ) ) , centered ) \n 
_h . mult_tt ( centered , centered , centered2 ) \n 
_h . sum_t ( centered2 , 0 , sigma2 ) \n 
_h . sqrt_t ( sigma2 , sigma_b ) \n 
_h . mult_st ( self . decay , sigma , sigma ) \n 
_h . mult_add_st ( 1.0 - self . decay , sigma_b , sigma ) \n 
sigma = sigma_b \n 
~~ _h . divide_mv ( centered , sigma . reshape ( ( 1 , sigma . size ) ) , x_hat ) \n 
_h . mult_mv ( x_hat , gamma . reshape ( ( 1 , gamma . size ) ) , out ) \n 
_h . add_mv ( out , beta . reshape ( ( 1 , beta . size ) ) , out ) \n 
~~ def backward_pass ( self , buffers ) : \n 
gamma = buffers . parameters . gamma \n 
dgamma = buffers . gradients . gamma \n 
dbeta = buffers . gradients . beta \n 
outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n 
indeltas = flatten_all_but_last ( buffers . input_deltas . default ) \n 
m = outdeltas . shape [ 0 ] \n 
tmp = big_tmp \n 
dgamma_tmp = small_tmp \n 
_h . mult_tt ( outdeltas , x_hat , tmp ) \n 
_h . sum_t ( tmp , axis = 0 , out = dgamma_tmp ) \n 
_h . add_tt ( dgamma_tmp , dgamma , dgamma ) \n 
_h . mult_st ( 1 / m , dgamma_tmp , dgamma_tmp ) \n 
term1 = big_tmp \n 
_h . mult_mv ( x_hat , dgamma_tmp . reshape ( ( 1 , gamma . size ) ) , term1 ) \n 
dbeta_tmp = small_tmp \n 
_h . sum_t ( outdeltas , axis = 0 , out = dbeta_tmp ) \n 
_h . add_tt ( dbeta_tmp , dbeta , dbeta ) \n 
_h . mult_st ( 1 / m , dbeta_tmp , dbeta_tmp ) \n 
term2 = big_tmp \n 
term3 = big_tmp \n 
_h . subtract_tt ( outdeltas , term1 , term2 ) \n 
_h . subtract_mv ( term2 , dbeta_tmp . reshape ( ( 1 , dbeta . size ) ) , term3 ) \n 
coeff = small_tmp \n 
_h . divide_tt ( gamma , sigma_b , coeff ) \n 
term4 = big_tmp \n 
_h . mult_mv ( term3 , coeff . reshape ( ( 1 , coeff . size ) ) , term4 ) \n 
_h . add_tt ( term4 , indeltas , indeltas ) \n 
~~ ~~ from __future__ import division , print_function , unicode_literals \n 
import numpy as np \n 
from brainstorm . describable import Describable \n 
class Scorer ( Describable ) : \n 
~~~ def __init__ ( self , out_name = , targets_name = , mask_name = , \n 
name = None ) : \n 
~~~ self . out_name = out_name \n 
self . targets_name = targets_name \n 
self . mask_name = mask_name \n 
self . __name__ = name if name is not None else self . __class__ . __name__ \n 
~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ pass \n 
~~ @ staticmethod \n 
def aggregate ( errors ) : \n 
~~~ errors = np . array ( errors ) \n 
assert errors . ndim == 2 and errors . shape [ 1 ] == 2 \n 
return np . sum ( errors [ : , 1 ] ) / np . sum ( errors [ : , 0 ] ) \n 
~~ ~~ def gather_losses_and_scores ( net , scorers , scores , out_name = , \n 
targets_name = , mask_name = ) : \n 
~~~ ls = net . get_loss_values ( ) \n 
for name , loss in ls . items ( ) : \n 
~~~ scores [ name ] . append ( ( net . _buffer_manager . batch_size , loss ) ) \n 
~~ for sc in scorers : \n 
~~~ name = sc . __name__ \n 
predicted = net . get ( sc . out_name or out_name or net . output_name ) \n 
true_labels = net . get_input ( sc . targets_name ) if sc . targets_name else net . get_input ( targets_name ) \n 
mask = net . get_input ( sc . mask_name ) if sc . mask_name else ( net . get_input ( mask_name ) if mask_name else None ) \n 
predicted = _flatten_all_but_last ( predicted ) \n 
true_labels = _flatten_all_but_last ( true_labels ) \n 
mask = _flatten_all_but_last ( mask ) \n 
weight = mask . sum ( ) if mask is not None else predicted . shape [ 0 ] \n 
scores [ name ] . append ( ( weight , sc ( true_labels , predicted , mask ) ) ) \n 
~~ ~~ def aggregate_losses_and_scores ( scores , net , scorers ) : \n 
~~~ results = OrderedDict ( ) \n 
for name in net . get_loss_values ( ) : \n 
~~~ results [ name ] = _weighted_average ( scores [ name ] ) \n 
~~~ results [ sc . __name__ ] = sc . aggregate ( scores [ sc . __name__ ] ) \n 
~~ return results \n 
~~ class Accuracy ( Scorer ) : \n 
~~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ if predicted . shape [ 1 ] > 1 : \n 
~~~ predicted = predicted . argmax ( 1 ) . reshape ( - 1 , 1 ) \n 
~~ correct = ( predicted == true_labels ) . astype ( np . float ) \n 
if mask is not None : \n 
~~~ correct *= mask \n 
~~ return np . sum ( correct ) \n 
~~ ~~ class Hamming ( Scorer ) : \n 
~~~ def __init__ ( self , threshold = 0.5 , out_name = , targets_name = , \n 
mask_name = , name = None ) : \n 
~~~ super ( Hamming , self ) . __init__ ( out_name , targets_name , mask_name , name ) \n 
self . threshold = threshold \n 
~~~ correct = np . logical_xor ( predicted < self . threshold , \n 
true_labels ) . astype ( np . float ) \n 
~~ return np . sum ( correct ) / true_labels . shape [ 1 ] \n 
~~ ~~ class MeanSquaredError ( Scorer ) : \n 
~~~ errors = ( true_labels - predicted ) ** 2 \n 
~~~ errors *= mask \n 
~~ return 0.5 * np . sum ( errors ) \n 
~~ ~~ def _flatten_all_but_last ( a ) : \n 
~~~ if a is None : \n 
~~~ return None \n 
~~ return a . reshape ( - 1 , a . shape [ - 1 ] ) \n 
~~ def _weighted_average ( errors ) : \n 
return np . sum ( errors [ : , 1 ] * errors [ : , 0 ] / np . sum ( errors [ : , 0 ] ) ) \n 
~~ from __future__ import division , print_function , unicode_literals \n 
import pytest \n 
import six \n 
from brainstorm . training . schedules import Exponential , Linear , MultiStep \n 
def test_linear ( ) : \n 
~~~ sch = Linear ( initial_value = 1.0 , final_value = 0.5 , num_changes = 5 ) \n 
epochs = [ 0 ] * 2 + [ 1 ] * 2 + [ 2 ] * 2 + [ 3 ] * 2 + [ 4 ] * 2 \n 
updates = range ( 10 ) \n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 , 1.0 , 0.9 , 0.9 , 0.8 , 0.8 , 0.7 , 0.7 , 0.6 , 0.6 ] \n 
assert values == [ 1.0 , 0.9 , 0.8 , 0.7 , 0.6 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ] \n 
values = [ sch ( epoch , update , , 3 , None , None , None ) \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.9 , 0.9 , 0.9 , 0.8 , 0.8 , 0.8 , 0.7 ] \n 
~~ def test_exponential ( ) : \n 
~~~ sch = Exponential ( initial_value = 1.0 , factor = 0.99 , minimum = 0.97 ) \n 
epochs = [ 0 ] * 4 + [ 1 ] * 4 + [ 2 ] * 4 \n 
updates = range ( 12 ) \n 
assert values == [ 1.0 ] * 4 + [ 0.99 ] * 4 + [ 0.99 * 0.99 ] * 4 \n 
assert values == [ 1.0 * ( 0.99 ** x ) for x in range ( 4 ) ] + [ 0.97 ] * 8 \n 
assert values == [ 1.0 ] * 3 + [ 0.99 ] * 3 + [ 0.9801 ] * 3 + [ 0.99 ** 3 ] * 3 \n 
~~ def test_multistep ( ) : \n 
~~~ sch = MultiStep ( initial_value = 1.0 , steps = [ 3 , 5 , 8 ] , \n 
values = [ 0.1 , 0.01 , 0.001 ] ) \n 
assert values == [ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.1 , 0.1 ] \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.01 , 0.01 , 0.01 , 0.001 , 0.001 ] \n 
with pytest . raises ( AssertionError ) : \n 
~~~ _ = sch ( 0 , 0 , , 3 , None , None , None ) \n 
# \n 
~~ ~~ import os \n 
import sys \n 
try : \n 
~~~ from unittest . mock import MagicMock \n 
~~ except ImportError : \n 
~~~ from mock import Mock as MagicMock \n 
~~ class Mock ( MagicMock ) : \n 
~~~ @ classmethod \n 
def __getattr__ ( cls , name ) : \n 
~~~ return Mock ( ) \n 
~~ ~~ MOCK_MODULES = [ , ] \n 
sys . modules . update ( ( mod_name , Mock ( ) ) for mod_name in MOCK_MODULES ) \n 
cwd = os . getcwd ( ) \n 
parent = os . path . dirname ( cwd ) \n 
sys . path . insert ( 0 , parent ) \n 
import brainstorm \n 
extensions = [ , , \n 
] \n 
templates_path = [ ] \n 
source_suffix = \n 
master_doc = \n 
project = \n 
copyright = \n 
version = brainstorm . __version__ \n 
release = brainstorm . __version__ \n 
exclude_patterns = [ ] \n 
pygments_style = \n 
on_rtd = os . environ . get ( , None ) == \n 
if not on_rtd : \n 
~~~ try : \n 
~~~ import sphinx_rtd_theme \n 
html_theme = \n 
html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n 
~~~ html_theme = \n 
~~ ~~ html_static_path = [ ] \n 
htmlhelp_basename = \n 
latex_elements = { \n 
} \n 
latex_documents = [ \n 
( , , , \n 
, ) , \n 
man_pages = [ \n 
[ ] , 1 ) \n 
texinfo_documents = [ \n 
, , , \n 
) , \n 
import theano . tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams \n 
from theano . tensor . nnet . conv import conv2d \n 
from theano . tensor . signal . downsample import max_pool_2d \n 
from theano . tensor . shared_randomstreams import RandomStreams \n 
from toolbox import * \n 
from modelbase import * \n 
class LM_gru ( ModelLMBase ) : \n 
~~~ def __init__ ( self , data , hp ) : \n 
~~~ super ( LM_gru , self ) . __init__ ( self . __class__ . __name__ , data , hp ) \n 
self . n_h = 256 \n 
self . dropout = 0.5 \n 
self . params = Parameters ( ) \n 
self . hiddenstates = Parameters ( ) \n 
n_tokens = self . data [ ] \n 
n_h = self . n_h \n 
scale = hp . init_scale \n 
gates = 3 \n 
with self . hiddenstates : \n 
~~~ b1_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
~~ if hp . load_model and os . path . isfile ( self . filename ) : \n 
~~~ self . params . load ( self . filename ) \n 
~~ else : \n 
~~~ with self . params : \n 
~~~ W_emb = shared_normal ( ( n_tokens , n_h ) , scale = scale ) \n 
W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b1 = shared_zeros ( ( n_h * gates ) ) \n 
W2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b2 = shared_zeros ( ( n_h * gates , ) ) \n 
~~ ~~ def lstm ( X , h , c , W , U , b ) : \n 
~~~ g_on = T . dot ( X , W ) + T . dot ( h , U ) + b \n 
i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n 
f_on = T . nnet . sigmoid ( g_on [ : , n_h : 2 * n_h ] ) \n 
o_on = T . nnet . sigmoid ( g_on [ : , 2 * n_h : 3 * n_h ] ) \n 
c = f_on * c + i_on * T . tanh ( g_on [ : , 3 * n_h : ] ) \n 
h = o_on * T . tanh ( c ) \n 
return h , c \n 
~~ def gru ( X , h , W , U , b ) : \n 
~~~ z_t = T . nnet . sigmoid ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
r_t = T . nnet . sigmoid ( T . dot ( X , W [ : , n_h : 2 * n_h ] ) + T . dot ( h , U [ : , n_h : 2 * n_h ] ) + b [ n_h : 2 * n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 2 * n_h : 3 * n_h ] ) + r_t * T . dot ( h , U [ : , 2 * n_h : 3 * n_h ] ) + b [ 2 * n_h : 3 * n_h ] ) \n 
return ( 1 - z_t ) * h + z_t * h_t \n 
~~ def sgru ( X , h , W , U , b ) : \n 
~~~ z_t = T . tanh ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 1 * n_h : 2 * n_h ] ) + T . dot ( h , U [ : , 1 * n_h : 2 * n_h ] ) + b [ 1 * n_h : 2 * n_h ] ) \n 
return z_t * h_t \n 
~~ def model ( x , p , p_dropout ) : \n 
~~~ input_size = x . shape [ 1 ] \n 
h0 = dropout ( h0 , p_dropout ) \n 
cost , h1 , h2 = [ 0. , b1_h , b2_h ] \n 
for t in xrange ( 0 , self . hp . seq_size ) : \n 
~~~ if t >= self . hp . warmup_size : \n 
~~~ pyx = softmax ( T . dot ( dropout ( h2 , p_dropout ) , T . transpose ( p . W_emb ) ) ) \n 
cost += T . sum ( T . nnet . categorical_crossentropy ( pyx , theano_one_hot ( x [ t ] , n_tokens ) ) ) \n 
~~ h1 = gru ( h0 [ t ] , h1 , p . W1 , p . V1 , p . b1 ) \n 
h2 = gru ( dropout ( h1 , p_dropout ) , h2 , p . W2 , p . V2 , p . b2 ) \n 
~~ h_updates = [ ( b1_h , h1 ) , ( b2_h , h2 ) ] \n 
return cost , h_updates \n 
~~ cost , h_updates = model ( self . X , self . params , self . dropout ) \n 
te_cost , te_h_updates = model ( self . X , self . params , 0. ) \n 
self . compile ( cost , te_cost , h_updates , te_h_updates ) \n 
~~ ~~ from collections import OrderedDict \n 
from . . import utils \n 
__all__ = [ \n 
"Layer" , \n 
"MergeLayer" , \n 
class Layer ( object ) : \n 
def __init__ ( self , incoming , name = None ) : \n 
~~~ if isinstance ( incoming , tuple ) : \n 
~~~ self . input_shape = incoming \n 
self . input_layer = None \n 
~~~ self . input_shape = incoming . output_shape \n 
self . input_layer = incoming \n 
~~ self . name = name \n 
self . params = OrderedDict ( ) \n 
self . get_output_kwargs = [ ] \n 
if any ( d is not None and d <= 0 for d in self . input_shape ) : \n 
~~~ raise ValueError ( ( \n 
self . input_shape , self . name ) ) \n 
~~ ~~ @ property \n 
def output_shape ( self ) : \n 
~~~ shape = self . get_output_shape_for ( self . input_shape ) \n 
if any ( isinstance ( s , T . Variable ) for s in shape ) : \n 
"dimensions." % ( self . __class__ . __name__ , shape ) ) \n 
~~ return shape \n 
~~ def get_params ( self , ** tags ) : \n 
result = list ( self . params . keys ( ) ) \n 
only = set ( tag for tag , value in tags . items ( ) if value ) \n 
if only : \n 
~~~ result = [ param for param in result \n 
if not ( only - self . params [ param ] ) ] \n 
~~ exclude = set ( tag for tag , value in tags . items ( ) if not value ) \n 
if exclude : \n 
if not ( self . params [ param ] & exclude ) ] \n 
~~ return utils . collect_shared_vars ( result ) \n 
~~ def get_output_shape_for ( self , input_shape ) : \n 
return input_shape \n 
~~ def get_output_for ( self , input , ** kwargs ) : \n 
raise NotImplementedError \n 
~~ def add_param ( self , spec , shape , name = None , ** tags ) : \n 
if name is not None : \n 
~~~ if self . name is not None : \n 
~~~ name = "%s.%s" % ( self . name , name ) \n 
~~ ~~ param = utils . create_param ( spec , shape , name ) \n 
tags [ ] = tags . get ( , True ) \n 
self . params [ param ] = set ( tag for tag , value in tags . items ( ) if value ) \n 
return param \n 
~~ ~~ class MergeLayer ( Layer ) : \n 
def __init__ ( self , incomings , name = None ) : \n 
~~~ self . input_shapes = [ incoming if isinstance ( incoming , tuple ) \n 
else incoming . output_shape \n 
for incoming in incomings ] \n 
self . input_layers = [ None if isinstance ( incoming , tuple ) \n 
else incoming \n 
self . name = name \n 
~~ @ Layer . output_shape . getter \n 
~~~ shape = self . get_output_shape_for ( self . input_shapes ) \n 
~~ def get_output_shape_for ( self , input_shapes ) : \n 
~~ def get_output_for ( self , inputs , ** kwargs ) : \n 
~~ ~~ from mock import Mock \n 
import numpy \n 
import theano \n 
class TestAutocrop : \n 
~~~ def test_autocrop_array_shapes ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop_array_shapes \n 
crop0 = None \n 
crop1 = [ None , , , ] \n 
crop2 = [ , ] \n 
crop_bad = [ , , , ] \n 
assert autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop0 ) == [ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop1 ) == [ ( 1 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) ] \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop2 ) == [ ( 1 , 2 , 3 , 4 ) , ( 1 , 2 , 7 , 8 ) , ( 1 , 2 , 3 , 2 ) ] \n 
with pytest . raises ( ValueError ) : \n 
~~~ autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop_bad ) \n 
~~ with pytest . raises ( ValueError ) : \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 ) , ( 5 , 4 , 3 , 2 , 10 ) ] , crop1 ) \n 
~~ ~~ def test_crop_inputs ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop \n 
from numpy . testing import assert_array_equal \n 
crop_0 = None \n 
crop_1 = [ None , , , ] \n 
crop_l = [ , , , ] \n 
crop_c = [ , , , ] \n 
crop_u = [ , , , ] \n 
crop_x = [ , ] \n 
x0 = numpy . random . random ( ( 2 , 3 , 5 , 7 ) ) \n 
x1 = numpy . random . random ( ( 1 , 2 , 3 , 4 ) ) \n 
x2 = numpy . random . random ( ( 6 , 3 , 4 , 2 ) ) \n 
def crop_test ( cropping , inputs , expected ) : \n 
~~~ inputs = [ theano . shared ( x ) for x in inputs ] \n 
outs = autocrop ( inputs , cropping ) \n 
outs = [ o . eval ( ) for o in outs ] \n 
assert len ( outs ) == len ( expected ) \n 
for o , e in zip ( outs , expected ) : \n 
~~~ assert_array_equal ( o , e ) \n 
~~ ~~ crop_test ( crop_0 , [ x0 , x1 ] , \n 
[ x0 , x1 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 4 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 1 : 5 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_0 , [ x0 , x2 ] , \n 
[ x0 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 5 : ] , x2 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , : 2 ] , x2 [ : 2 , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 2 : 4 ] , x2 [ 2 : 4 , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x2 ] , \n 
[ x0 [ : , : , 1 : , 5 : ] , x2 [ 4 : , : , : , : ] ] ) \n 
crop_test ( crop_0 , [ x0 , x1 , x2 ] , \n 
[ x0 , x1 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 , x2 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ : , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 2 ] , x1 [ : , : , : , : 2 ] , x2 [ : 1 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 2 : 4 ] , x1 [ : , : , : , 1 : 3 ] , x2 [ 2 : 3 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 , x2 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ 5 : , 1 : , 1 : , : ] ] ) \n 
crop_test ( crop_x , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] , \n 
x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
~~~ crop_test ( crop_bad , [ x0 , x1 , x2 ] , \n 
~~~ crop_test ( crop_bad , [ x0 [ : , : , : , 0 ] , x1 , x2 [ : , : , : , : , None ] ] , \n 
~~ ~~ ~~ class TestConcatLayer : \n 
~~~ @ pytest . fixture \n 
def layer ( self ) : \n 
~~~ from lasagne . layers . merge import ConcatLayer \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 ) \n 
~~ @ pytest . fixture \n 
def crop_layer_0 ( self ) : \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 0 , \n 
cropping = [ ] * 2 ) \n 
def crop_layer_1 ( self ) : \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 , \n 
~~ def test_get_output_shape_for ( self , layer ) : \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , None ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 5 ) ] ) == ( None , 7 ) \n 
~~~ layer . get_output_shape_for ( [ ( 4 , None ) , ( 3 , 5 ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 4 , None ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) , ( 4 , 5 ) ] ) \n 
~~ ~~ def test_get_output_shape_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ input_shapes = [ ( 3 , 2 ) , ( 4 , 5 ) ] \n 
result_0 = crop_layer_0 . get_output_shape_for ( input_shapes ) \n 
result_1 = crop_layer_1 . get_output_shape_for ( input_shapes ) \n 
assert result_0 == ( 7 , 2 ) \n 
assert result_1 == ( 3 , 7 ) \n 
~~ def test_get_output_for ( self , layer ) : \n 
~~~ inputs = [ theano . shared ( numpy . ones ( ( 3 , 3 ) ) ) , \n 
theano . shared ( numpy . ones ( ( 3 , 2 ) ) ) ] \n 
result = layer . get_output_for ( inputs ) \n 
result_eval = result . eval ( ) \n 
desired_result = numpy . hstack ( [ input . get_value ( ) for input in inputs ] ) \n 
assert ( result_eval == desired_result ) . all ( ) \n 
~~ def test_get_output_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
x1 = numpy . random . random ( ( 4 , 2 ) ) \n 
inputs = [ theano . shared ( x0 ) , \n 
theano . shared ( x1 ) ] \n 
result_0 = crop_layer_0 . get_output_for ( inputs ) . eval ( ) \n 
result_1 = crop_layer_1 . get_output_for ( inputs ) . eval ( ) \n 
desired_result_0 = numpy . concatenate ( [ x0 [ : , : 2 ] , x1 [ : , : 2 ] ] , axis = 0 ) \n 
desired_result_1 = numpy . concatenate ( [ x0 [ : 4 , : ] , x1 [ : 4 , : ] ] , axis = 1 ) \n 
assert ( result_0 == desired_result_0 ) . all ( ) \n 
assert ( result_1 == desired_result_1 ) . all ( ) \n 
~~ ~~ class TestElemwiseSumLayer : \n 
~~~ from lasagne . layers . merge import ElemwiseSumLayer \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] ) \n 
def crop_layer ( self ) : \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] , \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 2 ) ] ) == ( None , 2 ) \n 
~~~ layer . get_output_shape_for ( [ ( 3 , None ) , ( 4 , 2 ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) , ( 4 , 2 ) ] ) \n 
~~ ~~ def test_get_output_for ( self , layer ) : \n 
~~~ a = numpy . array ( [ [ 0 , 1 ] , [ 2 , 3 ] ] ) \n 
b = numpy . array ( [ [ 1 , 2 ] , [ 4 , 5 ] ] ) \n 
inputs = [ theano . shared ( a ) , \n 
theano . shared ( b ) ] \n 
desired_result = 2 * a - b \n 
~~ def test_get_output_for_cropped ( self , crop_layer ) : \n 
~~~ from numpy . testing import assert_array_almost_equal as aeq \n 
x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
result = crop_layer . get_output_for ( inputs ) . eval ( ) \n 
desired_result = 2 * x0 [ : 4 , : 2 ] - x1 [ : 4 , : 2 ] \n 
aeq ( result , desired_result ) \n 
~~ def test_bad_coeffs_fails ( self , layer ) : \n 
~~~ ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , 3 , - 1 ] ) \n 
~~ ~~ ~~ class TestElemwiseMergeLayerMul : \n 
~~~ import theano . tensor as T \n 
from lasagne . layers . merge import ElemwiseMergeLayer \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . mul ) \n 
desired_result = a * b \n 
~~ ~~ class TestElemwiseMergeLayerMaximum : \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . maximum ) \n 
desired_result = numpy . maximum ( a , b ) \n 
def tokenProgressFunc ( state = "update" , action = None , text = None , tick = 0 ) : \n 
~~ def build ( \n 
documentPath , \n 
outputUFOFormatVersion = 2 , \n 
roundGeometry = True , \n 
verbose = True , \n 
logPath = None , \n 
progressFunc = None , \n 
) : \n 
from mutatorMath . ufo . document import DesignSpaceDocumentReader \n 
import os , glob \n 
if os . path . isdir ( documentPath ) : \n 
~~~ todo = glob . glob ( os . path . join ( documentPath , "*.designspace" ) ) \n 
~~~ todo = [ documentPath ] \n 
~~ results = [ ] \n 
for path in todo : \n 
~~~ reader = DesignSpaceDocumentReader ( \n 
path , \n 
ufoVersion = outputUFOFormatVersion , \n 
roundGeometry = roundGeometry , \n 
verbose = verbose , \n 
logPath = logPath , \n 
progressFunc = progressFunc \n 
) \n 
reader . process ( ) \n 
results . append ( reader . results ) \n 
~~ reader = None \n 
return results \n 
from __future__ import print_function \n 
import argparse \n 
import datetime \n 
import json \n 
import multiprocessing \n 
import os \n 
import random \n 
import threading \n 
import time \n 
from PIL import Image \n 
import six . moves . cPickle as pickle \n 
from six . moves import queue \n 
import chainer \n 
from chainer import computational_graph \n 
from chainer import cuda \n 
from chainer import optimizers \n 
from chainer import serializers \n 
parser = argparse . ArgumentParser ( \n 
description = ) \n 
parser . add_argument ( , help = ) \n 
parser . add_argument ( , , default = , \n 
help = ) \n 
parser . add_argument ( , , type = int , default = 32 , \n 
parser . add_argument ( , , type = int , default = 250 , \n 
parser . add_argument ( , , default = 10 , type = int , \n 
parser . add_argument ( , , default = - 1 , type = int , \n 
parser . add_argument ( , , default = 20 , type = int , \n 
parser . add_argument ( , default = , \n 
args = parser . parse_args ( ) \n 
if args . gpu >= 0 : \n 
~~~ cuda . check_cuda_available ( ) \n 
~~ xp = cuda . cupy if args . gpu >= 0 else np \n 
assert 50000 % args . val_batchsize == 0 \n 
def load_image_list ( path , root ) : \n 
~~~ tuples = [ ] \n 
for line in open ( path ) : \n 
~~~ pair = line . strip ( ) . split ( ) \n 
tuples . append ( ( os . path . join ( root , pair [ 0 ] ) , np . int32 ( pair [ 1 ] ) ) ) \n 
~~ return tuples \n 
~~ train_list = load_image_list ( args . train , args . root ) \n 
val_list = load_image_list ( args . val , args . root ) \n 
mean_image = pickle . load ( open ( args . mean , ) ) \n 
if args . arch == : \n 
~~~ import nin \n 
model = nin . NIN ( ) \n 
~~ elif args . arch == : \n 
~~~ import alex \n 
model = alex . Alex ( ) \n 
~~~ import alexbn \n 
model = alexbn . AlexBN ( ) \n 
~~~ import googlenet \n 
model = googlenet . GoogLeNet ( ) \n 
~~~ import googlenetbn \n 
model = googlenetbn . GoogLeNetBN ( ) \n 
~~~ raise ValueError ( ) \n 
~~ if args . gpu >= 0 : \n 
~~~ cuda . get_device ( args . gpu ) . use ( ) \n 
model . to_gpu ( ) \n 
~~ optimizer = optimizers . MomentumSGD ( lr = 0.01 , momentum = 0.9 ) \n 
optimizer . setup ( model ) \n 
if args . initmodel : \n 
~~~ print ( , args . initmodel ) \n 
serializers . load_hdf5 ( args . initmodel , model ) \n 
~~ if args . resume : \n 
~~~ print ( , args . resume ) \n 
serializers . load_hdf5 ( args . resume , optimizer ) \n 
~~ data_q = queue . Queue ( maxsize = 1 ) \n 
res_q = queue . Queue ( ) \n 
cropwidth = 256 - model . insize \n 
def read_image ( path , center = False , flip = False ) : \n 
~~~ image = np . asarray ( Image . open ( path ) ) . transpose ( 2 , 0 , 1 ) \n 
if center : \n 
~~~ top = left = cropwidth / 2 \n 
~~~ top = random . randint ( 0 , cropwidth - 1 ) \n 
left = random . randint ( 0 , cropwidth - 1 ) \n 
~~ bottom = model . insize + top \n 
right = model . insize + left \n 
image = image [ : , top : bottom , left : right ] . astype ( np . float32 ) \n 
image -= mean_image [ : , top : bottom , left : right ] \n 
image /= 255 \n 
if flip and random . randint ( 0 , 1 ) == 0 : \n 
~~~ return image [ : , : , : : - 1 ] \n 
~~~ return image \n 
~~ ~~ def feed_data ( ) : \n 
~~~ i = 0 \n 
count = 0 \n 
x_batch = np . ndarray ( \n 
( args . batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
y_batch = np . ndarray ( ( args . batchsize , ) , dtype = np . int32 ) \n 
val_x_batch = np . ndarray ( \n 
( args . val_batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
val_y_batch = np . ndarray ( ( args . val_batchsize , ) , dtype = np . int32 ) \n 
batch_pool = [ None ] * args . batchsize \n 
val_batch_pool = [ None ] * args . val_batchsize \n 
pool = multiprocessing . Pool ( args . loaderjob ) \n 
data_q . put ( ) \n 
for epoch in six . moves . range ( 1 , 1 + args . epoch ) : \n 
~~~ print ( , epoch , file = sys . stderr ) \n 
print ( , optimizer . lr , file = sys . stderr ) \n 
perm = np . random . permutation ( len ( train_list ) ) \n 
for idx in perm : \n 
~~~ path , label = train_list [ idx ] \n 
batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n 
y_batch [ i ] = label \n 
i += 1 \n 
if i == args . batchsize : \n 
~~~ for j , x in enumerate ( batch_pool ) : \n 
~~~ x_batch [ j ] = x . get ( ) \n 
~~ data_q . put ( ( x_batch . copy ( ) , y_batch . copy ( ) ) ) \n 
i = 0 \n 
~~ count += 1 \n 
if count % 1000 == 0 : \n 
~~~ data_q . put ( ) \n 
j = 0 \n 
for path , label in val_list : \n 
~~~ val_batch_pool [ j ] = pool . apply_async ( \n 
read_image , ( path , True , False ) ) \n 
val_y_batch [ j ] = label \n 
j += 1 \n 
if j == args . val_batchsize : \n 
~~~ for k , x in enumerate ( val_batch_pool ) : \n 
~~~ val_x_batch [ k ] = x . get ( ) \n 
~~ data_q . put ( ( val_x_batch . copy ( ) , val_y_batch . copy ( ) ) ) \n 
~~ ~~ data_q . put ( ) \n 
~~ ~~ optimizer . lr *= 0.97 \n 
~~ pool . close ( ) \n 
pool . join ( ) \n 
~~ def log_result ( ) : \n 
~~~ train_count = 0 \n 
train_cur_loss = 0 \n 
train_cur_accuracy = 0 \n 
begin_at = time . time ( ) \n 
val_begin_at = None \n 
while True : \n 
~~~ result = res_q . get ( ) \n 
if result == : \n 
~~~ print ( file = sys . stderr ) \n 
break \n 
~~ elif result == : \n 
train = True \n 
if val_begin_at is not None : \n 
~~~ begin_at += time . time ( ) - val_begin_at \n 
~~ continue \n 
train = False \n 
val_count = val_loss = val_accuracy = 0 \n 
val_begin_at = time . time ( ) \n 
continue \n 
~~ loss , accuracy = result \n 
if train : \n 
~~~ train_count += 1 \n 
duration = time . time ( ) - begin_at \n 
throughput = train_count * args . batchsize / duration \n 
sys . stderr . write ( \n 
. format ( train_count , train_count * args . batchsize , \n 
datetime . timedelta ( seconds = duration ) , throughput ) ) \n 
train_cur_loss += loss \n 
train_cur_accuracy += accuracy \n 
if train_count % 1000 == 0 : \n 
~~~ mean_loss = train_cur_loss / 1000 \n 
mean_error = 1 - train_cur_accuracy / 1000 \n 
print ( file = sys . stderr ) \n 
print ( json . dumps ( { : , : train_count , \n 
: mean_error , : mean_loss } ) ) \n 
sys . stdout . flush ( ) \n 
~~ ~~ else : \n 
~~~ val_count += args . val_batchsize \n 
duration = time . time ( ) - val_begin_at \n 
throughput = val_count / duration \n 
. format ( val_count / args . val_batchsize , val_count , \n 
val_loss += loss \n 
val_accuracy += accuracy \n 
if val_count == 50000 : \n 
~~~ mean_loss = val_loss * args . val_batchsize / 50000 \n 
mean_error = 1 - val_accuracy * args . val_batchsize / 50000 \n 
~~ ~~ ~~ ~~ def train_loop ( ) : \n 
~~~ graph_generated = False \n 
~~~ while data_q . empty ( ) : \n 
~~~ time . sleep ( 0.1 ) \n 
~~ inp = data_q . get ( ) \n 
~~~ res_q . put ( ) \n 
model . train = True \n 
serializers . save_hdf5 ( args . out , model ) \n 
serializers . save_hdf5 ( args . outstate , optimizer ) \n 
model . train = False \n 
~~ volatile = if model . train else \n 
x = chainer . Variable ( xp . asarray ( inp [ 0 ] ) , volatile = volatile ) \n 
t = chainer . Variable ( xp . asarray ( inp [ 1 ] ) , volatile = volatile ) \n 
if model . train : \n 
~~~ optimizer . update ( model , x , t ) \n 
if not graph_generated : \n 
~~~ with open ( , ) as o : \n 
~~~ o . write ( computational_graph . build_computational_graph ( \n 
( model . loss , ) ) . dump ( ) ) \n 
~~ print ( , file = sys . stderr ) \n 
graph_generated = True \n 
~~~ model ( x , t ) \n 
~~ res_q . put ( ( float ( model . loss . data ) , float ( model . accuracy . data ) ) ) \n 
del x , t \n 
~~ ~~ feeder = threading . Thread ( target = feed_data ) \n 
feeder . daemon = True \n 
feeder . start ( ) \n 
logger = threading . Thread ( target = log_result ) \n 
logger . daemon = True \n 
logger . start ( ) \n 
train_loop ( ) \n 
feeder . join ( ) \n 
logger . join ( ) \n 
import lxmls . readers . pos_corpus as pcc \n 
from os import path \n 
import pickle \n 
corpus = pcc . PostagCorpus ( ) \n 
input_data = path . join ( \n 
path . dirname ( __file__ ) , \n 
"../../data/train-02-21.conll" ) \n 
train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = 15 , max_nr_sent = 1000 ) \n 
pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( , ) ) \n 
with open ( , ) as output : \n 
~~~ for seq in train_seq : \n 
~~~ words = [ corpus . word_dict . get_label_name ( seq . x [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
tags = [ corpus . tag_dict . get_label_name ( seq . y [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
s = . join ( [ . join ( [ word , tag ] ) for word , tag in zip ( words , tags ) ] ) \n 
output . write ( s + ) \n 
~~ ~~ import sys \n 
from lxmls . parsing . dependency_reader import * \n 
from lxmls . parsing . dependency_writer import * \n 
from lxmls . parsing . dependency_features import * \n 
from lxmls . parsing . dependency_decoder import * \n 
from lxmls . util . my_math_utils import * \n 
class DependencyParser ( ) : \n 
~~~ \n 
def __init__ ( self ) : \n 
~~~ self . trained = False \n 
self . projective = False \n 
self . language = "" \n 
self . weights = [ ] \n 
self . decoder = DependencyDecoder ( ) \n 
self . reader = DependencyReader ( ) \n 
self . writer = DependencyWriter ( ) \n 
self . features = DependencyFeatures ( ) \n 
~~ def read_data ( self , language ) : \n 
~~~ self . language = language \n 
self . reader . load ( language ) \n 
self . features . create_dictionary ( self . reader . train_instances ) \n 
~~ def train_perceptron ( self , n_epochs ) : \n 
self . weights = np . zeros ( self . features . n_feats ) \n 
total = np . zeros ( self . features . n_feats ) \n 
for epoch in range ( n_epochs ) : \n 
n_mistakes = 0 \n 
n_tokens = 0 \n 
n_instances = 0 \n 
for instance in self . reader . train_instances : \n 
~~~ feats = self . features . create_features ( instance ) \n 
scores = self . features . compute_scores ( feats , self . weights ) \n 
if self . projective : \n 
~~~ heads_pred = self . decoder . parse_proj ( scores ) \n 
~~~ heads_pred = self . decoder . parse_nonproj ( scores ) \n 
~~ for m in range ( np . size ( heads_pred ) ) : \n 
~~~ for f in feats [ instance . heads [ m ] ] [ m ] : \n 
~~~ if f < 0 : \n 
~~~ continue \n 
~~ self . weights [ f ] += 1.0 \n 
~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~ self . weights [ f ] -= 1.0 \n 
~~ n_mistakes += 1 \n 
~~ n_tokens += 1 \n 
~~ n_instances += 1 \n 
total += self . weights \n 
~~ self . weights = total / np . double ( n_epochs ) \n 
~~ def train_crf_sgd ( self , n_epochs , sigma , eta0 = 0.001 ) : \n 
t = 0 \n 
t0 = 1.0 / ( sigma * eta0 ) \n 
objective = 0.0 \n 
~~~ eta = 1.0 / ( sigma * ( t + t0 ) ) \n 
feats = self . features . create_features ( instance ) \n 
marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n 
for h in range ( np . size ( marginals , 0 ) ) : \n 
~~~ for m in range ( 1 , np . size ( marginals , 1 ) ) : \n 
~~~ if feats [ h ] [ m ] == None : \n 
~~ for f in feats [ h ] [ m ] : \n 
~~ self . weights [ f ] -= eta * marginals [ h , m ] \n 
~~ ~~ ~~ score_corr = 0.0 \n 
for m in range ( 1 , np . size ( instance . heads ) ) : \n 
~~~ h = instance . heads [ m ] \n 
score_corr += scores [ h , m ] \n 
for f in feats [ h ] [ m ] : \n 
~~ self . weights [ f ] += eta \n 
~~ ~~ objective += 0.5 * sigma * np . dot ( self . weights , self . weights ) - score_corr + logZ \n 
n_instances += 1 \n 
t += 1 \n 
~~ ~~ def test ( self ) : \n 
~~~ n_mistakes = 0 \n 
arr_heads_pred = [ ] ; \n 
for instance in self . reader . test_instances : \n 
~~ ~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~ ~~ n_mistakes += 1 \n 
arr_heads_pred . append ( heads_pred ) \n 
self . writer . save ( self . language , arr_heads_pred ) \n 
~~ ~~ from lxmls . sequences . label_dictionary import * \n 
import pdb \n 
################# \n 
class IDFeatures : \n 
def __init__ ( self , dataset ) : \n 
self . feature_dict = LabelDictionary ( ) \n 
self . feature_list = [ ] \n 
self . add_features = False \n 
self . dataset = dataset \n 
self . node_feature_cache = { } \n 
self . initial_state_feature_cache = { } \n 
self . final_state_feature_cache = { } \n 
self . edge_feature_cache = { } \n 
~~ def get_num_features ( self ) : \n 
~~~ return len ( self . feature_dict ) \n 
~~ def build_features ( self ) : \n 
self . add_features = True \n 
for sequence in self . dataset . seq_list : \n 
~~~ initial_features , transition_features , final_features , emission_features = self . get_sequence_features ( sequence ) \n 
self . feature_list . append ( [ initial_features , transition_features , final_features , emission_features ] ) \n 
~~ self . add_features = False \n 
~~ def get_sequence_features ( self , sequence ) : \n 
emission_features = [ ] \n 
initial_features = [ ] \n 
transition_features = [ ] \n 
final_features = [ ] \n 
features = [ ] \n 
features = self . add_initial_features ( sequence , sequence . y [ 0 ] , features ) \n 
initial_features . append ( features ) \n 
for pos , tag in enumerate ( sequence . y ) : \n 
~~~ features = [ ] \n 
features = self . add_emission_features ( sequence , pos , sequence . y [ pos ] , features ) \n 
emission_features . append ( features ) \n 
if pos > 0 : \n 
~~~ prev_tag = sequence . y [ pos - 1 ] \n 
features = self . add_transition_features ( sequence , pos - 1 , tag , prev_tag , features ) \n 
transition_features . append ( features ) \n 
~~ ~~ features = [ ] \n 
features = self . add_final_features ( sequence , sequence . y [ - 1 ] , features ) \n 
final_features . append ( features ) \n 
return initial_features , transition_features , final_features , emission_features \n 
#f(t,y_t,X) \n 
~~ def get_emission_features ( self , sequence , pos , y ) : \n 
~~~ all_feat = [ ] \n 
x = sequence . x [ pos ] \n 
if ( x not in self . node_feature_cache ) : \n 
~~~ self . node_feature_cache [ x ] = { } \n 
~~ if ( y not in self . node_feature_cache [ x ] ) : \n 
~~~ node_idx = [ ] \n 
node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n 
self . node_feature_cache [ x ] [ y ] = node_idx \n 
~~ idx = self . node_feature_cache [ x ] [ y ] \n 
all_feat = idx [ : ] \n 
return all_feat \n 
#f(t,y_t,y_(t-1),X) \n 
~~ def get_transition_features ( self , sequence , pos , y , y_prev ) : \n 
~~~ assert ( pos >= 0 and pos < len ( sequence . x ) ) , pdb . set_trace ( ) \n 
if ( y not in self . edge_feature_cache ) : \n 
~~~ self . edge_feature_cache [ y ] = { } \n 
~~ if ( y_prev not in self . edge_feature_cache [ y ] ) : \n 
~~~ edge_idx = [ ] \n 
edge_idx = self . add_transition_features ( sequence , pos , y , y_prev , edge_idx ) \n 
self . edge_feature_cache [ y ] [ y_prev ] = edge_idx \n 
~~ return self . edge_feature_cache [ y ] [ y_prev ] \n 
~~ def get_initial_features ( self , sequence , y ) : \n 
~~~ if ( y not in self . initial_state_feature_cache ) : \n 
edge_idx = self . add_initial_features ( sequence , y , edge_idx ) \n 
self . initial_state_feature_cache [ y ] = edge_idx \n 
~~ return self . initial_state_feature_cache [ y ] \n 
~~ def get_final_features ( self , sequence , y_prev ) : \n 
~~~ if ( y_prev not in self . final_state_feature_cache ) : \n 
edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n 
self . final_state_feature_cache [ y_prev ] = edge_idx \n 
~~ return self . final_state_feature_cache [ y_prev ] \n 
~~ def add_initial_features ( self , sequence , y , features ) : \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y ) \n 
feat_name = "init_tag:%s" % ( y_name ) \n 
feat_id = self . add_feature ( feat_name ) \n 
if ( feat_id != - 1 ) : \n 
~~~ features . append ( feat_id ) \n 
~~ return features \n 
~~ def add_final_features ( self , sequence , y_prev , features ) : \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
feat_name = "final_prev_tag:%s" % ( y_name ) \n 
~~ def add_emission_features ( self , sequence , pos , y , features ) : \n 
y_name = self . dataset . y_dict . get_label_name ( y ) \n 
x_name = self . dataset . x_dict . get_label_name ( x ) \n 
feat_name = "id:%s::%s" % ( x_name , y_name ) \n 
if feat_id != - 1 : \n 
~~ def add_transition_features ( self , sequence , pos , y , y_prev , features ) : \n 
assert pos < len ( sequence . x ) - 1 , pdb . set_trace ( ) \n 
y_prev_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
feat_name = "prev_tag:%s::%s" % ( y_prev_name , y_name ) \n 
~~ def add_feature ( self , feat_name ) : \n 
if ( feat_name in self . feature_dict ) : \n 
~~~ return self . feature_dict [ feat_name ] \n 
~~ if not self . add_features : \n 
~~~ return - 1 \n 
~~ return self . feature_dict . add ( feat_name ) \n 
~~ ~~ from __future__ import division \n 
from collections import defaultdict \n 
import itertools \n 
from sklearn import cluster , preprocessing , manifold \n 
from datetime import datetime \n 
class KeplerMapper ( object ) : \n 
~~~ def __init__ ( self , verbose = 2 ) : \n 
~~~ self . verbose = verbose \n 
self . chunk_dist = [ ] \n 
self . overlap_dist = [ ] \n 
self . d = [ ] \n 
self . nr_cubes = 0 \n 
self . overlap_perc = 0 \n 
self . clusterer = False \n 
~~ def fit_transform ( self , X , projection = "sum" , scaler = preprocessing . MinMaxScaler ( ) ) : \n 
~~~ self . scaler = scaler \n 
self . projection = str ( projection ) \n 
~~~ reducer = projection \n 
if self . verbose > 0 : \n 
~~~ projection . set_params ( ** { "verbose" : self . verbose } ) \n 
~~ except : \n 
~~ X = reducer . fit_transform ( X ) \n 
~~ if isinstance ( projection , str ) : \n 
~~~ if self . verbose > 0 : \n 
~~~ X = np . sum ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . mean ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . median ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . max ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . min ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . std ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X_mean = np . mean ( X , axis = 0 ) \n 
X = np . sum ( np . sqrt ( ( X - X_mean ) ** 2 ) , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~ ~~ if isinstance ( projection , list ) : \n 
~~ X = X [ : , np . array ( projection ) ] \n 
~~ if scaler is not None : \n 
~~ X = scaler . fit_transform ( X ) \n 
~~ return X \n 
~~ def map ( self , projected_X , inverse_X = None , clusterer = cluster . DBSCAN ( eps = 0.5 , min_samples = 3 ) , nr_cubes = 10 , overlap_perc = 0.1 ) : \n 
~~~ start = datetime . now ( ) \n 
def cube_coordinates_all ( nr_cubes , nr_dimensions ) : \n 
~~~ l = [ ] \n 
for x in range ( nr_cubes ) : \n 
~~~ l += [ x ] * nr_dimensions \n 
~~ return [ np . array ( list ( f ) ) for f in sorted ( set ( itertools . permutations ( l , nr_dimensions ) ) ) ] \n 
~~ nodes = defaultdict ( list ) \n 
links = defaultdict ( list ) \n 
complex = { } \n 
self . nr_cubes = nr_cubes \n 
self . clusterer = clusterer \n 
self . overlap_perc = overlap_perc \n 
~~ if inverse_X is None : \n 
~~~ inverse_X = projected_X \n 
~~ self . chunk_dist = ( np . max ( projected_X , axis = 0 ) - np . min ( projected_X , axis = 0 ) ) / nr_cubes \n 
self . overlap_dist = self . overlap_perc * self . chunk_dist \n 
self . d = np . min ( projected_X , axis = 0 ) \n 
di = np . array ( [ x for x in range ( projected_X . shape [ 1 ] ) ] ) \n 
ids = np . array ( [ x for x in range ( projected_X . shape [ 0 ] ) ] ) \n 
projected_X = np . c_ [ ids , projected_X ] \n 
inverse_X = np . c_ [ ids , inverse_X ] \n 
~~~ total_cubes = len ( cube_coordinates_all ( nr_cubes , projected_X . shape [ 1 ] ) ) \n 
~~ for i , coor in enumerate ( cube_coordinates_all ( nr_cubes , di . shape [ 0 ] ) ) : \n 
~~~ hypercube = projected_X [ np . invert ( np . any ( ( projected_X [ : , di + 1 ] >= self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) & \n 
( projected_X [ : , di + 1 ] < self . d [ di ] + ( coor * self . chunk_dist [ di ] ) + self . chunk_dist [ di ] + self . overlap_dist [ di ] ) == False , axis = 1 ) ) ] \n 
if self . verbose > 1 : \n 
( hypercube . shape [ 0 ] , i , total_cubes , self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) ) \n 
~~ if hypercube . shape [ 0 ] > 0 : \n 
~~~ inverse_x = inverse_X [ [ int ( nn ) for nn in hypercube [ : , 0 ] ] ] \n 
clusterer . fit ( inverse_x [ : , 1 : ] ) \n 
~~ for a in np . c_ [ hypercube [ : , 0 ] , clusterer . labels_ ] : \n 
nodes [ cluster_id ] . append ( int ( a [ 0 ] ) ) \n 
~~ ~~ ~~ else : \n 
~~~ if self . verbose > 1 : \n 
~~ ~~ ~~ candidates = itertools . combinations ( nodes . keys ( ) , 2 ) \n 
for candidate in candidates : \n 
~~~ if len ( nodes [ candidate [ 0 ] ] + nodes [ candidate [ 1 ] ] ) != len ( set ( nodes [ candidate [ 0 ] ] + nodes [ candidate [ 1 ] ] ) ) : \n 
~~~ links [ candidate [ 0 ] ] . append ( candidate [ 1 ] ) \n 
~~ ~~ if self . verbose > 0 : \n 
~~~ nr_links = 0 \n 
for k in links : \n 
~~~ nr_links += len ( links [ k ] ) \n 
~~ complex [ "nodes" ] = nodes \n 
complex [ "links" ] = links \n 
complex [ "meta" ] = self . projection \n 
return complex \n 
graph_link_distance = 30 , graph_gravity = 0.1 , graph_charge = - 120 , custom_tooltips = None , width_html = 0 , \n 
height_html = 0 , show_tooltips = True , show_title = True , show_meta = True ) : \n 
~~~ json_s = { } \n 
json_s [ "nodes" ] = [ ] \n 
json_s [ "links" ] = [ ] \n 
k2e = { } \n 
for e , k in enumerate ( complex [ "nodes" ] ) : \n 
~~~ if custom_tooltips is not None : \n 
if color_function == "average_signal_cluster" : \n 
~~~ tooltip_i = int ( ( ( sum ( [ f for f in custom_tooltips [ complex [ "nodes" ] [ k ] ] ] ) / len ( custom_tooltips [ complex [ "nodes" ] [ k ] ] ) ) * 30 ) ) \n 
json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex [ "nodes" ] [ k ] ) ) ) , "color" : str ( tooltip_i ) } ) \n 
~~~ json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex [ "nodes" ] [ k ] ) ) ) , "color" : str ( k . split ( "_" ) [ 0 ] ) } ) \n 
json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex [ "nodes" ] [ k ] ) ) ) , "color" : str ( k . split ( "_" ) [ 0 ] ) } ) \n 
~~ k2e [ k ] = e \n 
~~ for k in complex [ "links" ] : \n 
~~~ for link in complex [ "links" ] [ k ] : \n 
~~~ json_s [ "links" ] . append ( { "source" : k2e [ k ] , "target" : k2e [ link ] , "value" : 1 } ) \n 
~~ ~~ if width_html == 0 : \n 
~~~ width_css = "100%" \n 
width_js = \'document.getElementById("holder").offsetWidth-20\' \n 
~~~ width_css = "%spx" % width_html \n 
width_js = "%s" % width_html \n 
~~ if height_html == 0 : \n 
~~~ height_css = "100%" \n 
height_js = \'document.getElementById("holder").offsetHeight-20\' \n 
~~~ height_css = "%spx" % height_html \n 
height_js = "%s" % height_html \n 
~~ if show_tooltips == False : \n 
~~~ tooltips_display = "" \n 
~~ if show_meta == False : \n 
~~~ meta_display = "" \n 
~~ if show_title == False : \n 
~~~ title_display = "" \n 
~~ with open ( path_html , "wb" ) as outfile : \n 
outfile . write ( html . encode ( "utf-8" ) ) \n 
~~ if self . verbose > 0 : \n 
~~ ~~ ~~ import sys , os \n 
sys . path . insert ( 0 , os . path . abspath ( ) ) \n 
needs_sphinx = \n 
extensions = [ , , , ] \n 
import pkg_resources \n 
~~~ release = pkg_resources . get_distribution ( ) . version \n 
~~ except pkg_resources . DistributionNotFound : \n 
~~~ print \n 
print \n 
sys . exit ( 1 ) \n 
~~ del pkg_resources \n 
version = . join ( release . split ( ) [ : 2 ] ) \n 
html_static_path = [ ] \n 
html_use_smartypants = True \n 
import math \n 
import glob \n 
import re \n 
import subprocess \n 
from shutil import rmtree \n 
import logging \n 
from mrec import load_sparse_matrix , save_recommender \n 
class ItemSimilarityRunner ( object ) : \n 
~~~ def run ( self , view , model , input_format , trainfile , num_engines , simsdir , overwrite , max_sims , simsfile , modelfile ) : \n 
~~~ logging . info ( ) \n 
dataset = load_sparse_matrix ( input_format , trainfile ) \n 
num_users , num_items = dataset . shape \n 
del dataset \n 
logging . info ( , num_users , num_items ) \n 
logging . info ( . format ( simsdir ) ) \n 
subprocess . check_call ( [ , , simsdir ] ) \n 
done = [ ] \n 
if not overwrite : \n 
done . extend ( self . find_done ( simsdir ) ) \n 
if done : \n 
~~~ logging . info ( . format ( len ( done ) ) ) \n 
~~ ~~ logging . info ( ) \n 
tasks = self . create_tasks ( model , input_format , trainfile , simsdir , num_items , num_engines , max_sims , done ) \n 
if num_engines > 0 : \n 
~~~ logging . info ( \n 
, len ( tasks ) ) \n 
async_job = view . map_async ( process , tasks , retries = 2 ) \n 
results = async_job . get ( ) \n 
results = [ process ( task ) for task in tasks ] \n 
~~ logging . info ( ) \n 
done = self . find_done ( simsdir ) \n 
remaining = len ( tasks ) - len ( done ) \n 
if remaining == 0 : \n 
logging . info ( . format ( len ( done ) ) ) \n 
paths = [ os . path . join ( simsdir , . format ( start , end ) ) for start , end in done ] \n 
cmd = [ ] + paths \n 
subprocess . check_call ( cmd , stdout = open ( simsfile , ) ) \n 
logging . info ( ) \n 
rmtree ( simsdir ) \n 
logging . info ( , \n 
num_items , type ( model ) . __name__ , simsfile ) \n 
model . load_similarity_matrix ( simsfile , num_items ) \n 
save_recommender ( model , modelfile ) \n 
~~~ logging . error ( . format ( remaining , len ( tasks ) ) ) \n 
logging . error ( ) \n 
~~ ~~ def find_done ( self , outdir ) : \n 
~~~ success_files = glob . glob ( os . path . join ( outdir , ) ) \n 
r = re . compile ( ) \n 
for path in success_files : \n 
~~~ m = r . match ( path ) \n 
start = int ( m . group ( 1 ) ) \n 
end = int ( m . group ( 2 ) ) \n 
done . append ( ( start , end ) ) \n 
~~ return done \n 
~~ def create_tasks ( self , model , input_format , trainfile , outdir , num_items , num_engines , max_similar_items , done ) : \n 
~~~ if num_engines == 0 : \n 
~~~ num_engines = 1 \n 
~~ items_per_engine = int ( math . ceil ( float ( num_items ) / num_engines ) ) \n 
tasks = [ ] \n 
for start in xrange ( 0 , num_items , items_per_engine ) : \n 
~~~ end = min ( num_items , start + items_per_engine ) \n 
if ( start , end ) not in done : \n 
~~~ tasks . append ( ( model , input_format , trainfile , outdir , start , end , max_similar_items ) ) \n 
~~ ~~ return tasks \n 
~~ ~~ def process ( task ) : \n 
from mrec import load_fast_sparse_matrix \n 
model , input_format , trainfile , outdir , start , end , max_similar_items = task \n 
dataset = load_fast_sparse_matrix ( input_format , trainfile ) \n 
if hasattr ( model , ) : \n 
~~~ model . similarity_matrix = None \n 
~~ outfile = os . path . join ( outdir , . format ( start , end ) ) \n 
out = open ( outfile , ) \n 
for j in xrange ( start , end ) : \n 
~~~ w = model . get_similar_items ( j , max_similar_items = max_similar_items , dataset = dataset ) \n 
for k , v in w : \n 
~~ ~~ out . close ( ) \n 
cmd = [ , os . path . join ( outdir , . format ( start , end ) ) ] \n 
subprocess . check_call ( cmd ) \n 
return start , end \n 
import inspect \n 
NO_DEFAULT = object ( ) \n 
def memoize_default ( default = NO_DEFAULT , evaluator_is_first_arg = False , second_arg_is_evaluator = False ) : \n 
def func ( function ) : \n 
~~~ def wrapper ( obj , * args , ** kwargs ) : \n 
~~~ if evaluator_is_first_arg : \n 
~~~ cache = obj . memoize_cache \n 
~~~ cache = args [ 0 ] . memoize_cache \n 
~~~ cache = obj . _evaluator . memoize_cache \n 
~~ try : \n 
~~~ memo = cache [ function ] \n 
~~ except KeyError : \n 
~~~ memo = { } \n 
cache [ function ] = memo \n 
~~ key = ( obj , args , frozenset ( kwargs . items ( ) ) ) \n 
if key in memo : \n 
~~~ return memo [ key ] \n 
~~~ if default is not NO_DEFAULT : \n 
~~~ memo [ key ] = default \n 
~~ rv = function ( obj , * args , ** kwargs ) \n 
if inspect . isgenerator ( rv ) : \n 
~~~ rv = list ( rv ) \n 
~~ memo [ key ] = rv \n 
return rv \n 
~~ ~~ return wrapper \n 
~~ return func \n 
~~ class CachedMetaClass ( type ) : \n 
@ memoize_default ( None , second_arg_is_evaluator = True ) \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ return super ( CachedMetaClass , self ) . __call__ ( * args , ** kwargs ) \n 
from __future__ import absolute_import \n 
import __main__ \n 
from collections import namedtuple \n 
from jedi import Interpreter \n 
from jedi . api . helpers import completion_parts \n 
from jedi . parser . user_context import UserContext \n 
def setup_readline ( namespace_module = __main__ ) : \n 
class JediRL ( object ) : \n 
~~~ def complete ( self , text , state ) : \n 
if state == 0 : \n 
~~~ sys . path . insert ( 0 , os . getcwd ( ) ) \n 
~~~ interpreter = Interpreter ( text , [ namespace_module . __dict__ ] ) \n 
path = UserContext ( text , ( 1 , len ( text ) ) ) . get_path_until_cursor ( ) \n 
path , dot , like = completion_parts ( path ) \n 
before = text [ : len ( text ) - len ( like ) ] \n 
completions = interpreter . completions ( ) \n 
~~ finally : \n 
~~~ sys . path . pop ( 0 ) \n 
~~ self . matches = [ before + c . name_with_symbols for c in completions ] \n 
~~~ return self . matches [ state ] \n 
~~ except IndexError : \n 
~~ ~~ ~~ try : \n 
~~~ import readline \n 
~~~ readline . set_completer ( JediRL ( ) . complete ) \n 
readline . set_completer_delims ( ) \n 
~~ ~~ def version_info ( ) : \n 
Version = namedtuple ( , ) \n 
from jedi import __version__ \n 
tupl = re . findall ( , __version__ ) \n 
return Version ( * [ x if i == 3 else int ( x ) for i , x in enumerate ( tupl ) ] ) \n 
~~ import pandas \n 
import util \n 
import matplotlib . pyplot as plt \n 
import scipy as sp \n 
import scipy . stats \n 
cur_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
def from_custom_file ( data_file , learn_options ) : \n 
data = pandas . read_csv ( data_file ) \n 
mandatory_columns = [ , , , ] \n 
for col in mandatory_columns : \n 
~~ Xdf = pandas . DataFrame ( data ) \n 
Xdf [ ] = Xdf [ ] \n 
Xdf = Xdf . set_index ( [ , ] ) \n 
Xdf . index . names = [ , ] \n 
Xdf [ ] = [ % i for i in range ( Xdf . shape [ 0 ] ) ] \n 
Xdf = Xdf . set_index ( , append = True ) \n 
Y = None \n 
gene_position = Xdf [ [ , ] ] \n 
target_genes = np . unique ( Xdf . index . levels [ 1 ] ) \n 
learn_options = set_V2_target_names ( learn_options ) \n 
return Xdf , Y , gene_position , target_genes \n 
~~ def from_file ( data_file , learn_options , data_file2 = None , data_file3 = None ) : \n 
annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options ) \n 
learn_options [ ] = \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , learn_options ) \n 
xx = Xdf [ ] . values \n 
yy = Y [ ] . values \n 
rr , pp = sp . stats . pearsonr ( xx , yy ) \n 
~~~ learn_options [ ] = \n 
learn_options [ ] = None \n 
Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
Xdf , Y , gene_position , target_genes = merge_all ( data_file , data_file2 , data_file3 , learn_options ) \n 
~~ elif learn_options [ ] == 5 : \n 
gene_position , target_genes , Xdf , Y = read_xu_et_al ( data_file3 ) \n 
~~ Xdf [ "30mer" ] = Xdf [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
~~ def set_V2_target_names ( learn_options ) : \n 
~~~ if not in learn_options . keys ( ) : \n 
~~ if not in learn_options . keys ( ) : \n 
~~ learn_options [ ] = \n 
return learn_options \n 
~~ def combine_organisms ( human_data , mouse_data ) : \n 
~~~ cd13 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ , ] ) \n 
cd33 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ , , ] ) \n 
cd15 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ ] ) \n 
mouse_X = pandas . DataFrame ( ) \n 
mouse_Y = pandas . DataFrame ( ) \n 
for k in mouse_data . index . levels [ 1 ] : \n 
mouse_X = pandas . concat ( [ mouse_X , X ] , axis = 0 ) \n 
mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = 0 ) \n 
~~ X = pandas . concat ( [ X_CD13 , X_CD15 , X_CD33 , mouse_X ] , axis = 0 ) \n 
Y = pandas . concat ( [ Y_CD13 , Y_CD15 , Y_CD33 , mouse_Y ] , axis = 0 ) \n 
return X , Y \n 
~~ def read_V1_data ( data_file , learn_options , AML_file = cur_dir + "/data/V1_suppl_data.txt" ) : \n 
~~~ if data_file is None : \n 
~~~ data_file = cur_dir + "/data/V1_data.xlsx" \n 
~~ human_data = pandas . read_excel ( data_file , sheetname = 0 , index_col = [ 0 , 1 ] ) \n 
mouse_data = pandas . read_excel ( data_file , sheetname = 1 , index_col = [ 0 , 1 ] ) \n 
Xdf , Y = combine_organisms ( human_data , mouse_data ) \n 
annotations = pandas . read_csv ( AML_file , delimiter = , index_col = [ 0 , 4 ] ) \n 
annotations . index . names = Xdf . index . names \n 
gene_position = pandas . merge ( Xdf , annotations , how = "inner" , left_index = True , right_index = True ) \n 
gene_position = util . impute_gene_position ( gene_position ) \n 
gene_position = gene_position [ [ , , ] ] \n 
Y = Y . loc [ gene_position . index ] \n 
Xdf = Xdf . loc [ gene_position . index ] \n 
target_genes = Y [ ] . unique ( ) \n 
Y . index . names = [ , ] \n 
if learn_options is not None and learn_options [ "flipV1target" ] : \n 
~~~ print "************************************************************************" \n 
print "************************************************************************" \n 
import ipdb \n 
ipdb . set_trace ( ) \n 
~~ return annotations , gene_position , target_genes , Xdf , Y \n 
~~ def rank_transform ( x ) : \n 
~~~ return 1.0 - sp . stats . mstats . rankdata ( x ) / sp . stats . mstats . rankdata ( x ) . max ( ) \n 
~~ def read_xu_et_al ( data_file , learn_options = None , verbose = True , subsetting = ) : \n 
~~~ data_file = \n 
~~ datasets = [ , , ] \n 
aggregated = None \n 
for d in datasets : \n 
~~~ data_efficient = pandas . read_excel ( data_file , sheetname = % d , skiprows = 2 ) \n 
data_inefficient = pandas . read_excel ( data_file , sheetname = % d , skiprows = 2 ) \n 
data_efficient [ ] = 1. \n 
data_inefficient [ ] = 0. \n 
exp_data = pandas . concat ( ( data_efficient , data_inefficient ) ) \n 
exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( rank_transform ) \n 
if aggregated is None : \n 
~~~ aggregated = exp_data \n 
~~~ aggregated = pandas . concat ( ( aggregated , exp_data ) ) \n 
~~ ~~ if subsetting == : \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x [ 6 : - 4 ] ) \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x [ 10 : ] ) \n 
~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x . upper ( ) ) \n 
aggregated . rename ( columns = { "sequence(target+3\'+5\')" : , : , : } , inplace = True ) \n 
aggregated [ ] . loc [ aggregated [ ] == ] = \n 
aggregated [ ] = aggregated [ [ , ] ] . mean ( axis = 1 ) \n 
df = aggregated \n 
df = df . rename ( columns = { : , : } ) \n 
df [ ] = \n 
df [ ] = 1 \n 
df = df . set_index ( [ , , ] ) \n 
df [ ] = df . index . get_level_values ( 0 ) \n 
df [ ] = df . index . get_level_values ( 1 ) \n 
df [ ] = df [ ] \n 
df [ ] = 0 \n 
target_genes = np . unique ( df [ ] . values ) \n 
return df [ [ , , ] ] , target_genes , df [ [ , ] ] , df [ [ , , , ] ] \n 
~~ def read_V2_data ( data_file , learn_options = None , verbose = True ) : \n 
~~~ data_file = cur_dir + "/data/V2_data.xlsx" \n 
~~ data = pandas . read_excel ( data_file , sheetname = "ResultsFiltered" , skiprows = range ( 0 , 6 + 1 ) , index_col = [ 0 , 4 ] ) \n 
Xdf = pandas . DataFrame ( ) \n 
known_pairs = { : [ , , , ] , \n 
: [ ] , \n 
: [ , , , ] } \n 
drugs_to_genes = { : [ , , , ] , \n 
if learn_options is not None : \n 
if learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , ] ) \n 
~~ elif learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , , , , ] ) \n 
~~ ~~ count = 0 \n 
for drug in drugs_to_genes . keys ( ) : \n 
~~~ genes = drugs_to_genes [ drug ] \n 
for g in genes : \n 
~~~ Xtmp = data . copy ( ) . xs ( g , level = , drop_level = False ) \n 
Xtmp [ ] = drug \n 
if g in known_pairs [ drug ] : \n 
~~~ Xtmp [ ] = 1. \n 
~~~ Xtmp [ ] = 0. \n 
~~ count = count + Xtmp . shape [ 0 ] \n 
Xdf = pandas . concat ( [ Xdf , Xtmp ] , axis = 0 ) \n 
if verbose : \n 
~~ ~~ ~~ Xdf = Xdf . set_index ( , append = True ) \n 
Y = pandas . DataFrame ( Xdf . pop ( "score" ) ) \n 
Y . columns . names = [ "score" ] \n 
test_gene = pandas . DataFrame ( Xdf . pop ( ) ) \n 
Y = pandas . concat ( ( Y , target , test_gene ) , axis = 1 ) \n 
y_rank = pandas . DataFrame ( ) \n 
y_threshold = pandas . DataFrame ( ) \n 
y_quant = pandas . DataFrame ( ) \n 
~~~ gene_list = drugs_to_genes [ drug ] \n 
for gene in gene_list : \n 
y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = 0.8 , prefix = "score_drug_gene" , flip = False ) \n 
y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n 
y_threshold = pandas . concat ( ( y_threshold , y_thresholdtmp ) , axis = 0 ) \n 
y_quant = pandas . concat ( ( y_quant , y_quanttmp ) , axis = 0 ) \n 
~~ ~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
Y = pandas . merge ( Y , yall , how = , left_index = True , right_index = True ) \n 
~~~ ytmp = pandas . DataFrame ( Y . xs ( drug , level = "drug" , drop_level = False ) [ ] ) \n 
y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = 0.8 , prefix = "score_drug" , flip = False ) \n 
~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
PLOT = False \n 
if PLOT : \n 
~~~ labels = [ "score" , "score_drug_gene_rank" , "score_drug_rank" , "score_drug_gene_threshold" , "score_drug_threshold" ] \n 
for label in labels : \n 
~~~ plt . figure ( ) \n 
plt . plot ( Xdf [ ] . values , Y [ label ] . values , ) \n 
r , pearp = sp . stats . pearsonr ( Xdf [ ] . values . flatten ( ) , Y [ label ] . values . flatten ( ) ) \n 
plt . title ( label + % ( r , pearp ) ) \n 
plt . ylabel ( label ) \n 
~~ ~~ gene_position = util . impute_gene_position ( gene_position ) \n 
if learn_options is not None and learn_options [ "weighted" ] == "variance" : \n 
data = pandas . read_excel ( data_file , sheetname = "Normalized" , skiprows = range ( 0 , 6 + 1 ) , index_col = [ 0 , 4 ] ) \n 
experiments = { } \n 
experiments [ ] = [ , , , ] \n 
variance = None \n 
~~~ data_tmp = data . iloc [ data . index . get_level_values ( ) . isin ( drugs_to_genes [ drug ] ) ] [ experiments [ drug ] ] \n 
data_tmp [ "drug" ] = drug \n 
data_tmp = data_tmp . set_index ( , append = True ) \n 
data_tmp [ "variance" ] = np . var ( data_tmp . values , axis = 1 ) \n 
if variance is None : \n 
~~~ variance = data_tmp [ "variance" ] . copy ( ) \n 
~~~ variance = pandas . concat ( ( variance , data_tmp [ "variance" ] ) , axis = 0 ) \n 
~~ ~~ orig_index = Y . index . copy ( ) \n 
Y = pandas . merge ( Y , pandas . DataFrame ( variance ) , how = "inner" , left_index = True , right_index = True ) \n 
Y = Y . ix [ orig_index ] \n 
print "done." \n 
return Xdf , drugs_to_genes , target_genes , Y , gene_position \n 
~~ def merge_all ( data_file = None , data_file2 = None , data_file3 = None , learn_options = None ) : \n 
~~~ Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n 
Xdf = pandas . concat ( ( Xdf , Xdf_xu ) ) \n 
Y = pandas . concat ( ( Y , Y_xu ) ) \n 
gene_position = pandas . concat ( ( gene_position , gene_position_xu ) ) \n 
target_genes = np . concatenate ( ( target_genes , target_genes_xu ) ) \n 
~~ def mergeV1_V2 ( data_file , data_file2 , learn_options ) : \n 
annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n 
Xdf2 , drugs_to_genes , target_genes2 , Y2 , gene_position2 = read_V2_data ( data_file2 ) \n 
Y1 [ "drug" ] = [ "nodrug" for x in range ( Y1 . shape [ 0 ] ) ] \n 
Y1 = Y1 . set_index ( , append = True ) \n 
Y1 . index . names = [ , , ] \n 
Y_cols_to_keep = np . unique ( [ , , , ] ) \n 
Y1 = Y1 [ Y_cols_to_keep ] \n 
Y2 = Y2 [ Y_cols_to_keep ] \n 
Xdf1 [ "drug" ] = [ "nodrug" for x in range ( Xdf1 . shape [ 0 ] ) ] \n 
Xdf1 = Xdf1 . set_index ( , append = True ) \n 
X_cols_to_keep = [ , ] \n 
Xdf1 = Xdf1 [ X_cols_to_keep ] \n 
Xdf2 = Xdf2 [ X_cols_to_keep ] \n 
gene_position1 [ "drug" ] = [ "nodrug" for x in range ( gene_position1 . shape [ 0 ] ) ] \n 
gene_position1 = gene_position1 . set_index ( , append = True ) \n 
gene_position1 . index . names = [ , , ] \n 
cols_to_keep = [ , ] \n 
gene_position1 = gene_position1 [ cols_to_keep ] \n 
gene_position2 = gene_position2 [ cols_to_keep ] \n 
Y = pandas . concat ( ( Y1 , Y2 ) , axis = 0 ) \n 
Xdf = pandas . concat ( ( Xdf1 , Xdf2 ) , axis = 0 ) \n 
gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n 
target_genes = np . concatenate ( ( target_genes1 , target_genes2 ) ) \n 
save_to_file = False \n 
if save_to_file : \n 
~~~ Y . index . names = [ , , ] \n 
onedupind = np . where ( Y . index . duplicated ( ) ) [ 0 ] [ 0 ] \n 
alldupind = np . where ( Y . index . get_level_values ( 0 ) . values == Y . index [ onedupind ] [ 0 ] ) [ 0 ] \n 
newindex = Y . index . tolist ( ) \n 
newindex [ onedupind ] = ( newindex [ onedupind ] [ 0 ] , newindex [ onedupind ] [ 1 ] , "nodrug2" ) \n 
Y . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
XandY = pandas . merge ( Xdf , Y , how = "inner" , left_index = True , right_index = True ) \n 
gene_position_tmp = gene_position . copy ( ) \n 
gene_position_tmp . index . names = [ , , ] \n 
gene_position_tmp . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
XandY = pandas . merge ( XandY , gene_position_tmp , how = "inner" , left_index = True , right_index = True ) \n 
XandY [ "30mer" ] = XandY [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
XandY . to_csv ( ) \n 
~~ return Xdf , Y , gene_position , target_genes \n 
~~ def get_V1_genes ( data_file = None ) : \n 
~~~ annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n 
return target_genes \n 
~~ def get_V2_genes ( data_file = None ) : \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , verbose = False ) \n 
~~ def get_V3_genes ( data_fileV1 = None , data_fileV2 = None ) : \n 
~~~ target_genes = np . concatenate ( ( get_V1_genes ( data_fileV1 ) , get_V2_genes ( data_fileV2 ) ) ) \n 
~~ def get_xu_genes ( data_file = None ) : \n 
~~~ return read_xu_et_al ( data_file ) [ 1 ] \n 
~~ def get_mouse_genes ( data_file = None ) : \n 
return Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
~~ def get_human_genes ( data_file = None ) : \n 
mouse_genes = Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
all_genes = get_V3_genes ( None , None ) \n 
return np . setdiff1d ( all_genes , mouse_genes ) \n 
~~ from __future__ import absolute_import \n 
from . classification import * \n 
from . generic import * \n 
from . job import ImageDatasetJob \n 
from . images import * \n 
from . job import InferenceJob \n 
from . caffe_train import CaffeTrainTask \n 
from . torch_train import TorchTrainTask \n 
from . train import TrainTask \n 
import flask \n 
from flask . ext . socketio import SocketIO \n 
from gevent import monkey ; monkey . patch_all ( ) \n 
from . config import config_value \n 
from digits import utils \n 
import digits . scheduler \n 
app = flask . Flask ( __name__ ) \n 
app . config [ ] = True \n 
app . config [ ] = False \n 
app . config [ ] = config_value ( ) \n 
app . url_map . redirect_defaults = False \n 
socketio = SocketIO ( app ) \n 
scheduler = digits . scheduler . Scheduler ( config_value ( ) , True ) \n 
app . jinja_env . globals [ ] = config_value ( ) \n 
app . jinja_env . globals [ ] = digits . __version__ \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_diff \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_since \n 
app . jinja_env . filters [ ] = utils . sizeof_fmt \n 
app . jinja_env . filters [ ] = utils . auth . has_permission \n 
app . jinja_env . trim_blocks = True \n 
app . jinja_env . lstrip_blocks = True \n 
import digits . views \n 
app . register_blueprint ( digits . views . blueprint ) \n 
import digits . dataset . views \n 
app . register_blueprint ( digits . dataset . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . views \n 
app . register_blueprint ( digits . dataset . images . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . classification . views \n 
app . register_blueprint ( digits . dataset . images . classification . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . generic . views \n 
app . register_blueprint ( digits . dataset . images . generic . views . blueprint , url_prefix = ) \n 
import digits . model . views \n 
app . register_blueprint ( digits . model . views . blueprint , url_prefix = ) \n 
import digits . model . images . views \n 
app . register_blueprint ( digits . model . images . views . blueprint , url_prefix = ) \n 
import digits . model . images . classification . views \n 
app . register_blueprint ( digits . model . images . classification . views . blueprint , url_prefix = ) \n 
import digits . model . images . generic . views \n 
app . register_blueprint ( digits . model . images . generic . views . blueprint , url_prefix = ) \n 
def username_decorator ( f ) : \n 
~~~ from functools import wraps \n 
@ wraps ( f ) \n 
def decorated ( * args , ** kwargs ) : \n 
~~~ this_username = flask . request . cookies . get ( , None ) \n 
app . jinja_env . globals [ ] = this_username \n 
return f ( * args , ** kwargs ) \n 
~~ return decorated \n 
~~ for endpoint , function in app . view_functions . iteritems ( ) : \n 
~~~ app . view_functions [ endpoint ] = username_decorator ( function ) \n 
~~ scheduler . load_past_jobs ( ) \n 
import xml . etree . ElementTree as ET \n 
import cPickle \n 
def parse_rec ( filename ) : \n 
tree = ET . parse ( filename ) \n 
objects = [ ] \n 
for obj in tree . findall ( ) : \n 
~~~ obj_struct = { } \n 
obj_struct [ ] = obj . find ( ) . text \n 
obj_struct [ ] = int ( obj . find ( ) . text ) \n 
bbox = obj . find ( ) \n 
obj_struct [ ] = [ int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) ] \n 
objects . append ( obj_struct ) \n 
~~ return objects \n 
~~ def voc_ap ( rec , prec , use_07_metric = False ) : \n 
if use_07_metric : \n 
~~~ ap = 0. \n 
for t in np . arange ( 0. , 1.1 , 0.1 ) : \n 
~~~ if np . sum ( rec >= t ) == 0 : \n 
~~~ p = 0 \n 
~~~ p = np . max ( prec [ rec >= t ] ) \n 
~~ ap = ap + p / 11. \n 
~~~ mrec = np . concatenate ( ( [ 0. ] , rec , [ 1. ] ) ) \n 
mpre = np . concatenate ( ( [ 0. ] , prec , [ 0. ] ) ) \n 
for i in range ( mpre . size - 1 , 0 , - 1 ) : \n 
~~~ mpre [ i - 1 ] = np . maximum ( mpre [ i - 1 ] , mpre [ i ] ) \n 
~~ i = np . where ( mrec [ 1 : ] != mrec [ : - 1 ] ) [ 0 ] \n 
ap = np . sum ( ( mrec [ i + 1 ] - mrec [ i ] ) * mpre [ i + 1 ] ) \n 
~~ return ap \n 
~~ def voc_eval ( detpath , \n 
annopath , \n 
imagesetfile , \n 
classname , \n 
cachedir , \n 
ovthresh = 0.5 , \n 
use_07_metric = False ) : \n 
if not os . path . isdir ( cachedir ) : \n 
~~~ os . mkdir ( cachedir ) \n 
~~ cachefile = os . path . join ( cachedir , ) \n 
with open ( imagesetfile , ) as f : \n 
~~~ lines = f . readlines ( ) \n 
~~ imagenames = [ x . strip ( ) for x in lines ] \n 
if not os . path . isfile ( cachefile ) : \n 
~~~ recs = { } \n 
for i , imagename in enumerate ( imagenames ) : \n 
~~~ recs [ imagename ] = parse_rec ( annopath . format ( imagename ) ) \n 
if i % 100 == 0 : \n 
~~~ print . format ( \n 
i + 1 , len ( imagenames ) ) \n 
~~ ~~ print . format ( cachefile ) \n 
with open ( cachefile , ) as f : \n 
~~~ cPickle . dump ( recs , f ) \n 
~~~ with open ( cachefile , ) as f : \n 
~~~ recs = cPickle . load ( f ) \n 
~~ ~~ class_recs = { } \n 
npos = 0 \n 
for imagename in imagenames : \n 
~~~ R = [ obj for obj in recs [ imagename ] if obj [ ] == classname ] \n 
bbox = np . array ( [ x [ ] for x in R ] ) \n 
difficult = np . array ( [ x [ ] for x in R ] ) . astype ( np . bool ) \n 
det = [ False ] * len ( R ) \n 
npos = npos + sum ( ~ difficult ) \n 
class_recs [ imagename ] = { : bbox , \n 
: difficult , \n 
: det } \n 
~~ detfile = detpath . format ( classname ) \n 
with open ( detfile , ) as f : \n 
~~ splitlines = [ x . strip ( ) . split ( ) for x in lines ] \n 
image_ids = [ x [ 0 ] for x in splitlines ] \n 
confidence = np . array ( [ float ( x [ 1 ] ) for x in splitlines ] ) \n 
BB = np . array ( [ [ float ( z ) for z in x [ 2 : ] ] for x in splitlines ] ) \n 
sorted_ind = np . argsort ( - confidence ) \n 
BB = BB [ sorted_ind , : ] \n 
image_ids = [ image_ids [ x ] for x in sorted_ind ] \n 
nd = len ( image_ids ) \n 
tp = np . zeros ( nd ) \n 
fp = np . zeros ( nd ) \n 
for d in range ( nd ) : \n 
~~~ R = class_recs [ image_ids [ d ] ] \n 
bb = BB [ d , : ] . astype ( float ) \n 
ovmax = - np . inf \n 
BBGT = R [ ] . astype ( float ) \n 
if BBGT . size > 0 : \n 
~~~ ixmin = np . maximum ( BBGT [ : , 0 ] , bb [ 0 ] ) \n 
iymin = np . maximum ( BBGT [ : , 1 ] , bb [ 1 ] ) \n 
ixmax = np . minimum ( BBGT [ : , 2 ] , bb [ 2 ] ) \n 
iymax = np . minimum ( BBGT [ : , 3 ] , bb [ 3 ] ) \n 
iw = np . maximum ( ixmax - ixmin + 1. , 0. ) \n 
ih = np . maximum ( iymax - iymin + 1. , 0. ) \n 
inters = iw * ih \n 
uni = ( ( bb [ 2 ] - bb [ 0 ] + 1. ) * ( bb [ 3 ] - bb [ 1 ] + 1. ) + \n 
( BBGT [ : , 2 ] - BBGT [ : , 0 ] + 1. ) * \n 
( BBGT [ : , 3 ] - BBGT [ : , 1 ] + 1. ) - inters ) \n 
overlaps = inters / uni \n 
ovmax = np . max ( overlaps ) \n 
jmax = np . argmax ( overlaps ) \n 
~~ if ovmax > ovthresh : \n 
~~~ if not R [ ] [ jmax ] : \n 
~~~ tp [ d ] = 1. \n 
R [ ] [ jmax ] = 1 \n 
~~~ fp [ d ] = 1. \n 
~~ ~~ fp = np . cumsum ( fp ) \n 
tp = np . cumsum ( tp ) \n 
rec = tp / float ( npos ) \n 
prec = tp / ( tp + fp + 1e-10 ) \n 
ap = voc_ap ( rec , prec , use_07_metric ) \n 
return rec , prec , ap \n 
#!/usr/bin/python \n 
~~ import numpy as np \n 
from ipdb import set_trace \n 
from struct import pack , unpack \n 
def ceil_div ( x , y ) : \n 
~~~ return - ( - x // y ) \n 
~~ def out_dim ( S , X , padding , strides ) : \n 
~~~ return ceil_div ( X - S + 1 + 2 * padding , strides ) \n 
~~ def strip_mantissa ( val ) : \n 
~~~ i = unpack ( , pack ( , val ) ) [ 0 ] & 0x7f800000 \n 
f = unpack ( , pack ( , i ) ) [ 0 ] \n 
return f \n 
~~ def quantize ( ary , bits , sign = 1 ) : \n 
~~~ maxval = float ( np . max ( np . absolute ( ary ) ) ) \n 
scale = strip_mantissa ( maxval ) / float ( 1 << ( bits - sign - 1 ) ) \n 
ary = np . around ( ary * ( 1.0 / scale ) ) . astype ( np . int64 ) \n 
return ary , np . float64 ( scale ) \n 
~~ def fconv_slice ( q , S , X , padding , strides ) : \n 
~~~ f1 = 0 \n 
f2 = S - 1 \n 
x1 = q * strides - padding \n 
x2 = x1 + f2 \n 
if x1 < 0 : \n 
~~~ f1 = - x1 \n 
x1 = 0 \n 
~~ if x2 >= X : \n 
~~~ dif = x2 - X + 1 \n 
f2 -= dif \n 
x2 -= dif \n 
~~ return ( slice ( f1 , f2 + 1 ) , slice ( x1 , x2 + 1 ) , f2 - f1 + 1 ) \n 
~~ def bconv_slice ( x , S , Q , padding , strides ) : \n 
~~~ qs = x - ( S - padding - 1 ) \n 
firstF = None \n 
~~~ q = qs + s \n 
if q % strides == 0 : \n 
~~~ q strides \n 
if q >= 0 and q < Q : \n 
~~~ if firstF is None : \n 
~~~ firstF = s \n 
firstE = q \n 
~~ lastF = s \n 
lastE = q \n 
~~ ~~ ~~ return ( slice ( firstF , lastF + 1 , strides ) , slice ( firstE , lastE + 1 , strides ) , 0 ) \n 
~~ def xprop_direct ( I , F , O , padding , strides , backward = False ) : \n 
~~~ if all ( x == 1 for x in F . shape [ 1 : 3 ] ) : \n 
~~~ C = F . shape [ 0 ] \n 
K = F . shape [ 4 ] \n 
if backward : \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) , I . reshape ( ( K , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) . T , I . reshape ( ( C , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~ return \n 
~~ if backward : \n 
~~~ F = np . transpose ( F [ : , : : - 1 , : : - 1 , : ] , ( 3 , 1 , 2 , 0 ) ) . copy ( ) \n 
xconv_slice = bconv_slice \n 
~~~ xconv_slice = fconv_slice \n 
~~ C , Y , X , N = I . shape \n 
C , R , S , K = F . shape \n 
K , P , Q , N = O . shape \n 
qSlice = [ xconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
for p in range ( P ) : \n 
~~~ sliceR , sliceY , _ = xconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
for q in range ( Q ) : \n 
~~~ sliceS , sliceX , _ = qSlice [ q ] \n 
slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - 1 , K ) ) \n 
slicedI = I [ : , sliceY , sliceX , : ] . reshape ( ( - 1 , N ) ) \n 
O [ : , p , q , : ] = np . dot ( slicedF . T , slicedI ) \n 
~~ ~~ ~~ def updat_direct ( I , E , U , padding , strides ) : \n 
~~~ C , Y , X , N = I . shape \n 
K , P , Q , N = E . shape \n 
C , R , S , K = U . shape \n 
if all ( x == 1 for x in ( R , S ) ) : \n 
~~~ U [ : ] = np . dot ( I . reshape ( ( C , - 1 ) ) , E . reshape ( ( K , - 1 ) ) . T ) . reshape ( ( U . shape ) ) \n 
return \n 
~~ U . fill ( 0.0 ) \n 
qSlice = [ fconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
~~~ sliceR , sliceY , rlen = fconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
~~~ sliceS , sliceX , slen = qSlice [ q ] \n 
slicedE = E [ : , p , q , : ] \n 
U [ : , sliceR , sliceS , : ] += np . dot ( slicedI , slicedE . T ) . reshape ( ( C , rlen , slen , K ) ) \n 
~~ ~~ ~~ I_4x4_3x3 = ( \n 
np . array ( [ \n 
[ 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 4.0 , - 4.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , - 4.0 , - 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 , - 1.0 , 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 2.0 , - 1.0 , - 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 ] ] ) , \n 
[ 1.0 , 0.0 , - 5.0 / 4.0 , 0.0 , 1.0 / 4.0 , 0.0 ] , \n 
[ 0.0 , 2.0 / 3.0 , 2.0 / 3.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 / 3.0 , 2.0 / 3.0 , 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 12. , - 1.0 / 24. , 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , 1.0 / 12. , - 1.0 / 24. , - 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
F_4x4_3x3 = ( \n 
[ 1.0 / 4.0 , 0.0 , 0.0 ] , \n 
[ - 1.0 / 6.0 , - 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ - 1.0 / 6.0 , 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , - 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 0.0 , 0.0 , 1.0 ] ] ) , \n 
[ 1.0 , 0.0 , 0.0 ] , \n 
[ 1.0 , 1.0 , 1.0 ] , \n 
[ 1.0 , - 1.0 , 1.0 ] , \n 
[ 1.0 , 2.0 , 4.0 ] , \n 
[ 1.0 , - 2.0 , 4.0 ] , \n 
O_4x4_3x3 = ( \n 
[ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 2.0 , - 2.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , 1.0 , 4.0 , 4.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 8.0 , - 8.0 , 1.0 ] ] ) , \n 
[ 1.0 / 4.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 24. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 12. , - 1.0 / 12. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 3.0 , - 1.0 / 3.0 , 1.0 ] ] ) , \n 
rcp3 = 1.0 / 3.0 \n 
rcp4 = 1.0 / 4.0 \n 
rcp6 = 1.0 / 6.0 \n 
rcp12 = 1.0 / 12.0 \n 
rcp24 = 1.0 / 24.0 \n 
def trans_I_4x4_3x3 ( Iw , I , minimal = False , trans = False ) : \n 
~~~ if minimal : \n 
~~~ T0 = np . empty ( ( 6 , 6 ) ) \n 
T1 = np . empty ( ( 6 , 6 ) ) \n 
for O , I in ( ( T0 , I ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = ( I [ 2 , : ] * 4.0 - I [ 4 , : ] ) * rcp6 \n 
t1 = ( I [ 1 , : ] * 4.0 - I [ 3 , : ] ) * rcp6 \n 
t2 = ( I [ 4 , : ] - I [ 2 , : ] ) * rcp24 \n 
t3 = ( I [ 3 , : ] - I [ 1 , : ] ) * rcp12 \n 
O [ 0 , : ] = I [ 0 , : ] + ( I [ 2 , : ] * - 5.0 + I [ 4 , : ] ) * rcp4 \n 
O [ 1 , : ] = t0 + t1 \n 
O [ 2 , : ] = t0 - t1 \n 
O [ 3 , : ] = t2 + t3 \n 
O [ 4 , : ] = t2 - t3 \n 
O [ 5 , : ] = I [ 1 , : ] * 4.0 - I [ 3 , : ] * 5.0 + I [ 5 , : ] \n 
~~ Iw [ : ] = T1 . T \n 
~~~ Iw [ : ] = np . dot ( np . dot ( I_4x4_3x3 [ trans [ 0 ] ] , I ) , I_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_F_4x4_3x3 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 6 , 3 ) ) \n 
for O , I in ( ( T0 , F ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = I [ 0 , : ] + I [ 2 , : ] \n 
t1 = I [ 0 , : ] + I [ 2 , : ] * 4.0 \n 
O [ 0 , : ] = I [ 0 , : ] \n 
O [ 1 , : ] = t0 + I [ 1 , : ] \n 
O [ 2 , : ] = t0 - I [ 1 , : ] \n 
O [ 3 , : ] = t1 + I [ 1 , : ] * 2.0 \n 
O [ 4 , : ] = t1 - I [ 1 , : ] * 2.0 \n 
O [ 5 , : ] = I [ 2 , : ] \n 
~~ Fw [ : ] = T1 . T \n 
~~~ Fw [ : ] = np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] , F ) , F_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_O_4x4_3x3 ( Mw , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 4 , 6 ) ) \n 
T1 = np . empty ( ( 4 , 4 ) ) \n 
for O , I in ( ( T0 , Mw ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = I [ 1 , : ] + I [ 2 , : ] \n 
t1 = I [ 3 , : ] + I [ 4 , : ] \n 
t2 = I [ 1 , : ] - I [ 2 , : ] \n 
t3 = I [ 3 , : ] - I [ 4 , : ] \n 
O [ 0 , : ] = t0 + t1 + I [ 0 , : ] \n 
O [ 1 , : ] = t2 + t3 * 2.0 \n 
O [ 2 , : ] = t0 + t1 * 4.0 \n 
O [ 3 , : ] = t2 + t3 * 8.0 + I [ 5 , : ] \n 
~~ return T1 . T \n 
~~~ return np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] , Mw ) , O_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_F_3x3_4x4 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 6 , 4 ) ) \n 
t2 = I [ 1 , : ] + I [ 3 , : ] \n 
t3 = I [ 1 , : ] + I [ 3 , : ] * 4.0 \n 
O [ 1 , : ] = t0 + t2 \n 
O [ 2 , : ] = t0 - t2 \n 
O [ 3 , : ] = t1 + t3 * 2.0 \n 
O [ 4 , : ] = t1 - t3 * 2.0 \n 
O [ 5 , : ] = I [ 3 , : ] \n 
~~~ Fw [ : ] = np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] . T , F ) , O_4x4_3x3 [ trans [ 1 ] ] ) \n 
~~ ~~ def trans_O_3x3_4x4 ( Mw , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 3 , 6 ) ) \n 
T1 = np . empty ( ( 3 , 3 ) ) \n 
O [ 0 , : ] = I [ 0 , : ] + t0 + t1 \n 
O [ 1 , : ] = I [ 1 , : ] - I [ 2 , : ] + 2 * ( I [ 3 , : ] - I [ 4 , : ] ) \n 
O [ 2 , : ] = t0 + 4 * t1 + I [ 5 , : ] \n 
~~~ return np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] . T , Mw ) , F_4x4_3x3 [ trans [ 1 ] ] ) \n 
~~ ~~ def image_slice ( x , X , B , D , pad = 0 ) : \n 
~~~ start = x * B - pad \n 
stop = start + D \n 
pad = [ 0 , 0 ] \n 
if start < 0 : \n 
~~~ pad [ 0 ] = - start \n 
start = 0 \n 
~~ if stop - 1 >= X : \n 
~~~ pad [ 1 ] = stop - X \n 
~~ return start , stop , pad \n 
~~ def output_slice ( p , P , B ) : \n 
~~~ p0 = p * B \n 
p1 = p0 + B \n 
if p1 > P : \n 
~~~ p1 = P \n 
~~ return p0 , p1 , p1 - p0 \n 
~~ def xprop_winograd ( I , F , O , padding , minimal = False , trans = False , backward = False ) : \n 
~~~ if backward : \n 
padding = [ 2 - p for p in padding ] \n 
B = 4 \n 
D = B + 2 \n 
Yw = ceil_div ( P , B ) \n 
Xw = ceil_div ( Q , B ) \n 
Fw = np . empty ( ( D , D , C , K ) ) \n 
Iw = np . empty ( ( D , D , C , Yw , Xw , N ) ) \n 
for c in range ( C ) : \n 
~~~ for k in range ( K ) : \n 
~~~ trans_F_4x4_3x3 ( Fw [ : , : , c , k ] , F [ c , : , : , k ] , minimal , trans ) \n 
~~ ~~ for y in range ( Yw ) : \n 
~~~ start_y , stop_y , pad_y = image_slice ( y , Y , B , D , padding [ 0 ] ) \n 
for x in range ( Xw ) : \n 
~~~ start_x , stop_x , pad_x = image_slice ( x , X , B , D , padding [ 1 ] ) \n 
sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n 
if any ( pad_y ) or any ( pad_x ) : \n 
~~~ sliceI = np . pad ( sliceI , ( ( 0 , 0 ) , pad_y , pad_x , ( 0 , 0 ) ) , ) \n 
~~ for c in range ( C ) : \n 
~~~ for n in range ( N ) : \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , c , y , x , n ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ for s in range ( D ) : \n 
~~~ for t in range ( D ) : \n 
~~~ Mw [ s , t ] = np . dot ( Fw [ s , t ] . T , Iw [ s , t ] . reshape ( C , - 1 ) ) . reshape ( ( K , Yw , Xw , N ) ) \n 
~~~ p0 , p1 , plen = output_slice ( y , P , B ) \n 
~~~ q0 , q1 , qlen = output_slice ( x , Q , B ) \n 
for k in range ( K ) : \n 
~~~ Out = trans_O_4x4_3x3 ( Mw [ : , : , k , y , x , n ] , minimal , trans ) \n 
O [ k , p0 : p1 , q0 : q1 , n ] = Out [ 0 : plen , 0 : qlen ] \n 
~~ ~~ ~~ ~~ ~~ def updat_winograd ( I , E , U , padding , minimal = False , trans = False , inner = True ) : \n 
Iw = np . empty ( ( D , D , N , C ) ) \n 
Ew = np . empty ( ( D , D , N , K ) ) \n 
if inner : \n 
~~~ Mw = np . empty ( ( D , D , C , K ) ) \n 
U . fill ( 0.0 ) \n 
~~~ Mw = np . zeros ( ( D , D , C , K ) ) \n 
~~ for y in range ( Yw ) : \n 
start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n 
start_q , stop_q , pad_q = image_slice ( x , Q , B , B ) \n 
sliceE = E [ : , start_p : stop_p , start_q : stop_q , : ] \n 
~~ if any ( pad_p ) or any ( pad_q ) : \n 
~~~ sliceE = np . pad ( sliceE , ( ( 0 , 0 ) , pad_p , pad_q , ( 0 , 0 ) ) , ) \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , n , c ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
~~ ~~ for k in range ( K ) : \n 
~~~ trans_F_3x3_4x4 ( Ew [ : , : , n , k ] , sliceE [ k , : , : , n ] , minimal , trans ) \n 
~~ ~~ for s in range ( D ) : \n 
~~~ if inner : \n 
~~~ Mw [ s , t ] = np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
~~~ Mw [ s , t ] += np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
~~ ~~ ~~ if inner : \n 
~~~ for c in range ( C ) : \n 
~~~ U [ c , : , : , k ] += trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ ~~ if not inner : \n 
~~~ U [ c , : , : , k ] = trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ np . set_printoptions ( threshold = 8192 * 4 , linewidth = 600 , formatter = { : lambda x : "%6.3f" % x } ) \n 
minimal = 1 \n 
trans = ( 2 , 2 ) \n 
ones = 0 \n 
N = 32 \n 
C , K = 32 , 32 \n 
Y , X = 6 , 6 \n 
P = out_dim ( R , Y , padding [ 0 ] , strides [ 0 ] ) \n 
Q = out_dim ( S , X , padding [ 1 ] , strides [ 1 ] ) \n 
print P , Q \n 
dimI = ( C , Y , X , N ) \n 
dimF = ( C , R , S , K ) \n 
dimO = ( K , P , Q , N ) \n 
if ones : \n 
~~~ I = np . zeros ( dimI ) \n 
F = np . ones ( dimF ) \n 
E = np . zeros ( dimO ) \n 
for p , q in np . ndindex ( ( Y , X ) ) : \n 
~~~ I [ : , p , q , : ] = np . identity ( N ) \n 
~~ for p , q in np . ndindex ( ( P , Q ) ) : \n 
~~~ E [ : , p , q , n ] = range ( K ) \n 
~~~ I = np . random . uniform ( - 1.0 , 1.0 , dimI ) \n 
F = np . random . uniform ( - 1.0 , 1.0 , dimF ) \n 
E = np . random . uniform ( - 1.0 , 1.0 , dimO ) \n 
~~ Od = np . empty ( dimO ) \n 
Bd = np . empty ( dimI ) \n 
Ud = np . empty ( dimF ) \n 
Uw = np . empty ( dimF ) \n 
xprop_direct ( I , F , Od , padding , strides ) \n 
xprop_winograd ( I , F , Ow , padding , minimal = minimal , trans = trans ) \n 
xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n 
xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n 
updat_direct ( I , E , Ud , padding , strides ) \n 
updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n 
difO = Od - Ow \n 
difB = Bd - Bw \n 
difU = Ud - Uw \n 
print abs ( difO ) . max ( ) / Od . max ( ) \n 
print abs ( difB ) . max ( ) / Bd . max ( ) \n 
print abs ( difU ) . max ( ) / Ud . max ( ) \n 
from neon . layers . layer import ( Linear , Bias , Affine , Conv , Convolution , GeneralizedCost , Dropout , \n 
Pooling , Activation , DataTransform , BatchNorm , BatchNormAutodiff , \n 
Deconv , Deconvolution , GeneralizedCostMask , LookupTable , \n 
BranchNode , SkipNode , LRN , ColorNoise ) \n 
from neon . layers . recurrent import ( Recurrent , LSTM , GRU , RecurrentSum , RecurrentMean , RecurrentLast , \n 
BiRNN , BiLSTM , DeepBiRNN , DeepBiLSTM ) \n 
from neon . layers . container import ( Tree , Sequential , MergeMultistream , MergeBroadcast , Multicost , \n 
RoiPooling , MergeSum , SingleOutputTree ) \n 
from neon . util . argparser import NeonArgparser \n 
from neon . initializers import Constant , Gaussian \n 
from neon . layers import Conv , Dropout , Pooling , GeneralizedCost , Affine \n 
from neon . optimizers import GradientDescentMomentum , MultiOptimizer , Schedule \n 
from neon . transforms import Rectlin , Softmax , CrossEntropyMulti , TopKMisclassification \n 
from neon . models import Model \n 
from neon . data import ImageLoader \n 
from neon . callbacks . callbacks import Callbacks \n 
parser = NeonArgparser ( __doc__ ) \n 
img_set_options = dict ( repo_dir = args . data_dir , \n 
inner_size = 224 , \n 
subset_pct = 0.09990891117239205 ) \n 
train = ImageLoader ( set_name = , scale_range = ( 256 , 256 ) , shuffle = False , \n 
do_transforms = False , ** img_set_options ) \n 
test = ImageLoader ( set_name = , scale_range = ( 256 , 384 ) , shuffle = False , \n 
layers = [ Conv ( ( 11 , 11 , 64 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 3 , strides = 4 ) , \n 
Pooling ( 3 , strides = 2 ) , \n 
Conv ( ( 5 , 5 , 192 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , \n 
activation = Rectlin ( ) , padding = 2 ) , \n 
Conv ( ( 3 , 3 , 384 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 1 ) , \n 
Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n 
Affine ( nout = 4096 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , activation = Rectlin ( ) ) , \n 
Dropout ( keep = 1.0 ) , \n 
Affine ( nout = 1000 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( - 7 ) , activation = Softmax ( ) ) ] \n 
model = Model ( layers = layers ) \n 
weight_sched = Schedule ( [ 22 , 44 , 65 ] , ( 1 / 250. ) ** ( 1 / 3. ) ) \n 
opt_gdm = GradientDescentMomentum ( 0.01 / 10 , 0.9 , wdecay = 0.0005 , schedule = weight_sched , \n 
stochastic_round = args . rounding ) \n 
opt_biases = GradientDescentMomentum ( 0.02 / 10 , 0.9 , schedule = Schedule ( [ 44 ] , 0.1 ) , \n 
opt = MultiOptimizer ( { : opt_gdm , : opt_biases } ) \n 
valmetric = TopKMisclassification ( k = 5 ) \n 
callbacks = Callbacks ( model , eval_set = test , metric = valmetric , ** args . callback_args ) \n 
cost = GeneralizedCost ( costfunc = CrossEntropyMulti ( ) ) \n 
model . fit ( train , optimizer = opt , num_epochs = args . epochs , cost = cost , callbacks = callbacks ) \n 
import itertools as itt \n 
from neon import NervanaObject \n 
from neon . layers . layer import Pooling \n 
from tests . utils import allclose_with_out \n 
def pytest_generate_tests ( metafunc ) : \n 
~~~ np . random . seed ( 1 ) \n 
if metafunc . config . option . all : \n 
~~~ bsz_rng = [ 32 , 64 ] \n 
~~~ bsz_rng = [ 128 ] \n 
~~ if in metafunc . fixturenames : \n 
~~~ fargs = [ ] \n 
~~~ fs_rng = [ 2 , 3 , 5 ] \n 
pad_rng = [ 0 , 1 ] \n 
nifm_rng = [ 16 , 32 ] \n 
in_sz_rng = [ 8 , 16 ] \n 
~~~ fs_rng = [ 2 , 4 ] \n 
nifm_rng = [ 8 ] \n 
in_sz_rng = [ 8 ] \n 
~~ fargs_ = [ ] \n 
for fs in fs_rng : \n 
~~~ stride_rng = set ( [ 1 , fs / 2 , fs ] ) \n 
fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n 
~~ fargs = itt . chain ( * fargs_ ) \n 
metafunc . parametrize ( , fargs ) \n 
~~ ~~ def ref_pooling ( inp , inp_shape , fshape , padding , strides , be , ncheck = None ) : \n 
~~~ inp_lshape = list ( inp_shape ) \n 
bsz = inp . shape [ - 1 ] \n 
if ncheck is None : \n 
~~~ check_inds = np . arange ( bsz ) \n 
~~ elif type ( ncheck ) is int : \n 
~~~ check_inds = np . random . permutation ( bsz ) \n 
check_inds = check_inds [ 0 : ncheck ] \n 
~~~ check_inds = ncheck \n 
~~ check_inds = np . sort ( check_inds ) \n 
inp_lshape . append ( bsz ) \n 
inpa = inp . get ( ) . reshape ( inp_lshape ) \n 
outshape = ( inp_lshape [ 0 ] , \n 
be . output_dim ( inp_lshape [ 1 ] , fshape [ 0 ] , padding , strides [ 0 ] , pooling = True ) , \n 
be . output_dim ( inp_lshape [ 2 ] , fshape [ 1 ] , padding , strides [ 1 ] , pooling = True ) , \n 
len ( check_inds ) ) \n 
if padding > 0 : \n 
~~~ padded_shape = ( inp_lshape [ 0 ] , \n 
inp_lshape [ 1 ] + 2 * padding , \n 
inp_lshape [ 2 ] + 2 * padding , \n 
inp_lshape [ - 1 ] ) \n 
inp_pad = np . zeros ( padded_shape ) \n 
inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , 0 : , 0 : , : ] \n 
~~~ inp_pad = inpa \n 
~~ out_exp = np . zeros ( outshape ) \n 
for indC in range ( outshape [ 0 ] ) : \n 
~~~ for indh in range ( outshape [ 1 ] ) : \n 
~~~ hrng = ( indh * strides [ 0 ] , indh * strides [ 0 ] + fshape [ 0 ] ) \n 
for indw in range ( outshape [ 2 ] ) : \n 
~~~ wrng = ( indw * strides [ 1 ] , indw * strides [ 1 ] + fshape [ 1 ] ) \n 
for cnt , indb in enumerate ( check_inds ) : \n 
~~~ inp_check = inp_pad [ indC , hrng [ 0 ] : hrng [ 1 ] , wrng [ 0 ] : wrng [ 1 ] , indb ] \n 
out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n 
~~ ~~ ~~ ~~ return ( out_exp , check_inds ) \n 
~~ def test_padding ( backend_default , poolargs ) : \n 
~~~ fshape , nifm , padding , stride , in_sz , batch_size = poolargs \n 
NervanaObject . be . bsz = batch_size \n 
inshape = ( nifm , in_sz , in_sz ) \n 
insize = np . prod ( inshape ) \n 
neon_layer = Pooling ( fshape = fshape , strides = stride , padding = padding ) \n 
inp = neon_layer . be . array ( np . random . random ( ( insize , batch_size ) ) ) \n 
inp . lshape = inshape \n 
neon_layer . configure ( inshape ) \n 
neon_layer . prev_layer = True \n 
neon_layer . allocate ( ) \n 
neon_layer . set_deltas ( [ neon_layer . be . iobuf ( inshape ) ] ) \n 
out = neon_layer . fprop ( inp ) . get ( ) \n 
ncheck = [ 0 , batch_size / 2 , batch_size - 1 ] \n 
( out_exp , check_inds ) = ref_pooling ( inp , inp . lshape , \n 
( fshape , fshape ) , \n 
padding , \n 
( stride , stride ) , \n 
neon_layer . be , \n 
ncheck = ncheck ) \n 
out_shape = list ( out_exp . shape [ 0 : 3 ] ) \n 
out_shape . append ( batch_size ) \n 
outa = out . reshape ( out_shape ) \n 
assert allclose_with_out ( out_exp , outa [ : , : , : , check_inds ] , atol = 0.0 , rtol = 0.0 ) \n 
~~ \n 
from __future__ import absolute_import , division , print_function \n 
from neo . core . container import Container \n 
class RecordingChannelGroup ( Container ) : \n 
_container_child_objects = ( , ) \n 
_data_child_objects = ( , ) \n 
_multi_child_objects = ( , ) \n 
_single_parent_objects = ( , ) \n 
_recommended_attrs = ( ( ( , np . ndarray , 1 , np . dtype ( ) ) , \n 
( , np . ndarray , 1 , np . dtype ( ) ) ) + \n 
Container . _recommended_attrs ) \n 
def __init__ ( self , channel_names = None , channel_indexes = None , name = None , \n 
description = None , file_origin = None , ** annotations ) : \n 
super ( RecordingChannelGroup , self ) . __init__ ( name = name , \n 
description = description , \n 
file_origin = file_origin , \n 
** annotations ) \n 
if channel_indexes is None : \n 
~~~ channel_indexes = np . array ( [ ] , dtype = np . int ) \n 
~~ if channel_names is None : \n 
~~~ channel_names = np . array ( [ ] , dtype = ) \n 
~~ self . channel_names = channel_names \n 
self . channel_indexes = channel_indexes \n 
import ctypes \n 
~~~ file \n 
~~ except NameError : \n 
~~~ import io \n 
file = io . BufferedReader \n 
import quantities as pq \n 
from neo . io . baseio import BaseIO \n 
from neo . core import Segment , AnalogSignal , SpikeTrain , EventArray \n 
class NeuroshareError ( Exception ) : \n 
~~~ def __init__ ( self , lib , errno ) : \n 
~~~ self . lib = lib \n 
self . errno = errno \n 
pszMsgBuffer = ctypes . create_string_buffer ( 256 ) \n 
self . lib . ns_GetLastErrorMsg ( pszMsgBuffer , ctypes . c_uint32 ( 256 ) ) \n 
errstr = . format ( errno , pszMsgBuffer . value ) \n 
Exception . __init__ ( self , errstr ) \n 
~~ ~~ class DllWithError ( ) : \n 
~~~ def __init__ ( self , lib ) : \n 
~~ def __getattr__ ( self , attr ) : \n 
~~~ f = getattr ( self . lib , attr ) \n 
return self . decorate_with_error ( f ) \n 
~~ def decorate_with_error ( self , f ) : \n 
~~~ def func_with_error ( * args ) : \n 
~~~ errno = f ( * args ) \n 
if errno != ns_OK : \n 
~~~ raise NeuroshareError ( self . lib , errno ) \n 
~~ return errno \n 
~~ return func_with_error \n 
~~ ~~ class NeurosharectypesIO ( BaseIO ) : \n 
is_readable = True \n 
is_writable = False \n 
supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n 
readable_objects = [ Segment ] \n 
writeable_objects = [ ] \n 
has_header = False \n 
is_streameable = False \n 
read_params = { Segment : [ ] } \n 
write_params = None \n 
name = \n 
extensions = [ ] \n 
mode = \n 
def __init__ ( self , filename = , dllname = ) : \n 
BaseIO . __init__ ( self ) \n 
self . dllname = dllname \n 
self . filename = filename \n 
~~ def read_segment ( self , import_neuroshare_segment = True , \n 
lazy = False , cascade = True ) : \n 
seg = Segment ( file_origin = os . path . basename ( self . filename ) , ) \n 
if sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . windll . LoadLibrary ( self . dllname ) \n 
~~ elif sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . cdll . LoadLibrary ( self . dllname ) \n 
~~ neuroshare = DllWithError ( neuroshare ) \n 
info = ns_LIBRARYINFO ( ) \n 
neuroshare . ns_GetLibraryInfo ( ctypes . byref ( info ) , ctypes . sizeof ( info ) ) \n 
seg . annotate ( neuroshare_version = str ( info . dwAPIVersionMaj ) + + str ( info . dwAPIVersionMin ) ) \n 
if not cascade : \n 
~~~ return seg \n 
~~ hFile = ctypes . c_uint32 ( 0 ) \n 
neuroshare . ns_OpenFile ( ctypes . c_char_p ( self . filename ) , ctypes . byref ( hFile ) ) \n 
fileinfo = ns_FILEINFO ( ) \n 
neuroshare . ns_GetFileInfo ( hFile , ctypes . byref ( fileinfo ) , ctypes . sizeof ( fileinfo ) ) \n 
for dwEntityID in range ( fileinfo . dwEntityCount ) : \n 
~~~ entityInfo = ns_ENTITYINFO ( ) \n 
neuroshare . ns_GetEntityInfo ( hFile , dwEntityID , ctypes . byref ( entityInfo ) , ctypes . sizeof ( entityInfo ) ) \n 
if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pEventInfo = ns_EVENTINFO ( ) \n 
neuroshare . ns_GetEventInfo ( hFile , dwEntityID , ctypes . byref ( pEventInfo ) , ctypes . sizeof ( pEventInfo ) ) \n 
if pEventInfo . dwEventType == 0 : #TEXT \n 
~~~ pData = ctypes . create_string_buffer ( pEventInfo . dwMaxDataLength ) \n 
~~ elif pEventInfo . dwEventType == 1 : #CVS \n 
~~~ pData = ctypes . c_byte ( 0 ) \n 
~~~ pData = ctypes . c_int16 ( 0 ) \n 
~~~ pData = ctypes . c_int32 ( 0 ) \n 
~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
pdwDataRetSize = ctypes . c_uint32 ( 0 ) \n 
ea = EventArray ( name = str ( entityInfo . szEntityLabel ) , ) \n 
if not lazy : \n 
~~~ times = [ ] \n 
labels = [ ] \n 
for dwIndex in range ( entityInfo . dwItemCount ) : \n 
~~~ neuroshare . ns_GetEventData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , ctypes . byref ( pData ) , \n 
ctypes . sizeof ( pData ) , ctypes . byref ( pdwDataRetSize ) ) \n 
times . append ( pdTimeStamp . value ) \n 
labels . append ( str ( pData . value ) ) \n 
~~ ea . times = times * pq . s \n 
ea . labels = np . array ( labels , dtype = ) \n 
~~~ ea . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . eventarrays . append ( ea ) \n 
~~ if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pAnalogInfo = ns_ANALOGINFO ( ) \n 
neuroshare . ns_GetAnalogInfo ( hFile , dwEntityID , ctypes . byref ( pAnalogInfo ) , ctypes . sizeof ( pAnalogInfo ) ) \n 
dwIndexCount = entityInfo . dwItemCount \n 
if lazy : \n 
~~~ signal = [ ] * pq . Quantity ( 1 , pAnalogInfo . szUnits ) \n 
~~~ pdwContCount = ctypes . c_uint32 ( 0 ) \n 
pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
total_read = 0 \n 
while total_read < entityInfo . dwItemCount : \n 
~~~ dwStartIndex = ctypes . c_uint32 ( total_read ) \n 
dwStopIndex = ctypes . c_uint32 ( entityInfo . dwItemCount - total_read ) \n 
neuroshare . ns_GetAnalogData ( hFile , dwEntityID , dwStartIndex , \n 
dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n 
total_read += pdwContCount . value \n 
~~ signal = pq . Quantity ( pData , units = pAnalogInfo . szUnits , copy = False ) \n 
#t_start \n 
~~ dwIndex = 0 \n 
pdTime = ctypes . c_double ( 0 ) \n 
neuroshare . ns_GetTimeByIndex ( hFile , dwEntityID , dwIndex , ctypes . byref ( pdTime ) ) \n 
anaSig = AnalogSignal ( signal , \n 
sampling_rate = pAnalogInfo . dSampleRate * pq . Hz , \n 
t_start = pdTime . value * pq . s , \n 
name = str ( entityInfo . szEntityLabel ) , \n 
anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n 
~~~ anaSig . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . analogsignals . append ( anaSig ) \n 
#segment \n 
~~ if entity_types [ entityInfo . dwEntityType ] == and import_neuroshare_segment : \n 
~~~ pdwSegmentInfo = ns_SEGMENTINFO ( ) \n 
if not str ( entityInfo . szEntityLabel ) . startswith ( ) : \n 
~~ neuroshare . ns_GetSegmentInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pdwSegmentInfo ) , ctypes . sizeof ( pdwSegmentInfo ) ) \n 
nsource = pdwSegmentInfo . dwSourceCount \n 
neuroshare . ns_GetLastErrorMsg ( ctypes . byref ( pszMsgBuffer ) , 256 ) \n 
for dwSourceID in range ( pdwSegmentInfo . dwSourceCount ) : \n 
~~~ pSourceInfo = ns_SEGSOURCEINFO ( ) \n 
neuroshare . ns_GetSegmentSourceInfo ( hFile , dwEntityID , dwSourceID , \n 
ctypes . byref ( pSourceInfo ) , ctypes . sizeof ( pSourceInfo ) ) \n 
~~ if lazy : \n 
~~~ sptr = SpikeTrain ( times , name = str ( entityInfo . szEntityLabel ) , t_stop = 0. * pq . s ) \n 
sptr . lazy_shape = entityInfo . dwItemCount \n 
~~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
dwDataBufferSize = pdwSegmentInfo . dwMaxSampleCount * pdwSegmentInfo . dwSourceCount \n 
pData = np . zeros ( ( dwDataBufferSize ) , dtype = ) \n 
pdwSampleCount = ctypes . c_uint32 ( 0 ) \n 
pdwUnitID = ctypes . c_uint32 ( 0 ) \n 
nsample = int ( dwDataBufferSize ) \n 
times = np . empty ( ( entityInfo . dwItemCount ) , dtype = ) \n 
waveforms = np . empty ( ( entityInfo . dwItemCount , nsource , nsample ) , dtype = ) \n 
~~~ neuroshare . ns_GetSegmentData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) , \n 
dwDataBufferSize * 8 , ctypes . byref ( pdwSampleCount ) , \n 
ctypes . byref ( pdwUnitID ) ) \n 
times [ dwIndex ] = pdTimeStamp . value \n 
waveforms [ dwIndex , : , : ] = pData [ : nsample * nsource ] . reshape ( nsample , nsource ) . transpose ( ) \n 
~~ sptr = SpikeTrain ( times = pq . Quantity ( times , units = , copy = False ) , \n 
t_stop = times . max ( ) , \n 
waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo . szUnits ) , copy = False ) , \n 
left_sweep = nsample / 2. / float ( pdwSegmentInfo . dSampleRate ) * pq . s , \n 
sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n 
~~ seg . spiketrains . append ( sptr ) \n 
~~~ pNeuralInfo = ns_NEURALINFO ( ) \n 
neuroshare . ns_GetNeuralInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n 
~~~ times = [ ] * pq . s \n 
t_stop = 0 * pq . s \n 
~~~ pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
dwStartIndex = 0 \n 
neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n 
dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n 
times = pData * pq . s \n 
t_stop = times . max ( ) \n 
~~ sptr = SpikeTrain ( times , t_stop = t_stop , \n 
name = str ( entityInfo . szEntityLabel ) , ) \n 
~~~ sptr . lazy_shape = entityInfo . dwItemCount \n 
~~ ~~ neuroshare . ns_CloseFile ( hFile ) \n 
seg . create_many_to_one_relationship ( ) \n 
return seg \n 
~~ ~~ class ns_FILEDESC ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_char * 32 ) , \n 
( , ctypes . c_char * 8 ) , \n 
( , ctypes . c_char * 16 ) , \n 
~~ class ns_LIBRARYINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 64 ) , \n 
( , ns_FILEDESC * 16 ) , \n 
~~ class ns_FILEINFO ( ctypes . Structure ) : \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_char * 256 ) , \n 
~~ class ns_ENTITYINFO ( ctypes . Structure ) : \n 
~~ entity_types = { 0 : , \n 
1 : , \n 
2 : , \n 
3 : , \n 
4 : , \n 
class ns_EVENTINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ \n 
( , ctypes . c_char * 128 ) , \n 
~~ class ns_ANALOGINFO ( ctypes . Structure ) : \n 
~~ class ns_SEGMENTINFO ( ctypes . Structure ) : \n 
( , ctypes . c_char * 32 ) , \n 
~~ class ns_SEGSOURCEINFO ( ctypes . Structure ) : \n 
~~ class ns_NEURALINFO ( ctypes . Structure ) : \n 
~~~ import unittest2 as unittest \n 
~~~ import unittest \n 
~~~ from IPython . lib . pretty import pretty \n 
~~ except ImportError as err : \n 
~~~ HAVE_IPYTHON = False \n 
~~~ HAVE_IPYTHON = True \n 
~~ from neo . core . segment import Segment \n 
from neo . core import ( AnalogSignalArray , Block , \n 
Epoch , EpochArray , \n 
RecordingChannelGroup , SpikeTrain , Unit ) \n 
from neo . core . container import filterdata \n 
from neo . test . tools import ( assert_neo_object_is_compliant , \n 
assert_same_sub_schema ) \n 
from neo . test . generate_datasets import ( fake_neo , get_fake_value , \n 
get_fake_values , get_annotations , \n 
clone_object , TEST_ANNOTATIONS ) \n 
class Test__generate_datasets ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ np . random . seed ( 0 ) \n 
self . annotations = dict ( [ ( str ( x ) , TEST_ANNOTATIONS [ x ] ) for x in \n 
range ( len ( TEST_ANNOTATIONS ) ) ] ) \n 
~~ def test__get_fake_values ( self ) : \n 
~~~ self . annotations [ ] = 0 \n 
file_datetime = get_fake_value ( , datetime , seed = 0 ) \n 
rec_datetime = get_fake_value ( , datetime , seed = 1 ) \n 
index = get_fake_value ( , int , seed = 2 ) \n 
name = get_fake_value ( , str , seed = 3 , obj = Segment ) \n 
description = get_fake_value ( , str , seed = 4 , obj = ) \n 
file_origin = get_fake_value ( , str ) \n 
attrs1 = { : file_datetime , \n 
: rec_datetime , \n 
: index , \n 
: name , \n 
: description , \n 
: file_origin } \n 
attrs2 = attrs1 . copy ( ) \n 
attrs2 . update ( self . annotations ) \n 
res11 = get_fake_values ( Segment , annotate = False , seed = 0 ) \n 
res12 = get_fake_values ( , annotate = False , seed = 0 ) \n 
res21 = get_fake_values ( Segment , annotate = True , seed = 0 ) \n 
res22 = get_fake_values ( , annotate = True , seed = 0 ) \n 
self . assertEqual ( res11 , attrs1 ) \n 
self . assertEqual ( res12 , attrs1 ) \n 
self . assertEqual ( res21 , attrs2 ) \n 
self . assertEqual ( res22 , attrs2 ) \n 
~~ def test__fake_neo__cascade ( self ) : \n 
~~~ self . annotations [ ] = None \n 
obj_type = Segment \n 
cascade = True \n 
res = fake_neo ( obj_type = obj_type , cascade = cascade ) \n 
self . assertTrue ( isinstance ( res , Segment ) ) \n 
assert_neo_object_is_compliant ( res ) \n 
self . assertEqual ( res . annotations , self . annotations ) \n 
self . assertEqual ( len ( res . analogsignalarrays ) , 1 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 1 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 1 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 1 ) \n 
self . assertEqual ( len ( res . spikes ) , 1 ) \n 
self . assertEqual ( len ( res . events ) , 1 ) \n 
self . assertEqual ( len ( res . epochs ) , 1 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 1 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 1 ) \n 
for child in res . children : \n 
~~~ del child . annotations [ ] \n 
del child . annotations [ ] \n 
~~ self . assertEqual ( res . analogsignalarrays [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . analogsignals [ 0 ] . annotations , \n 
self . assertEqual ( res . irregularlysampledsignals [ 0 ] . annotations , \n 
self . assertEqual ( res . spiketrains [ 0 ] . annotations , \n 
self . assertEqual ( res . spikes [ 0 ] . annotations , \n 
self . assertEqual ( res . events [ 0 ] . annotations , \n 
self . assertEqual ( res . epochs [ 0 ] . annotations , \n 
self . assertEqual ( res . eventarrays [ 0 ] . annotations , \n 
self . assertEqual ( res . epocharrays [ 0 ] . annotations , \n 
~~ def test__fake_neo__nocascade ( self ) : \n 
obj_type = \n 
cascade = False \n 
self . assertEqual ( len ( res . analogsignalarrays ) , 0 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 0 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 0 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 0 ) \n 
self . assertEqual ( len ( res . spikes ) , 0 ) \n 
self . assertEqual ( len ( res . events ) , 0 ) \n 
self . assertEqual ( len ( res . epochs ) , 0 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 0 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 0 ) \n 
~~ ~~ class TestSegment ( unittest . TestCase ) : \n 
~~~ self . nchildren = 2 \n 
blk = fake_neo ( Block , seed = 0 , n = self . nchildren ) \n 
self . unit1 , self . unit2 , self . unit3 , self . unit4 = blk . list_units \n 
self . seg1 , self . seg2 = blk . segments \n 
self . targobj = self . seg1 \n 
self . seed1 = self . seg1 . annotations [ ] \n 
self . seed2 = self . seg2 . annotations [ ] \n 
del self . seg1 . annotations [ ] \n 
del self . seg2 . annotations [ ] \n 
self . sigs1 = self . seg1 . analogsignals \n 
self . sigs2 = self . seg2 . analogsignals \n 
self . sigarrs1 = self . seg1 . analogsignalarrays \n 
self . sigarrs2 = self . seg2 . analogsignalarrays \n 
self . irsigs1 = self . seg1 . irregularlysampledsignals \n 
self . irsigs2 = self . seg2 . irregularlysampledsignals \n 
self . spikes1 = self . seg1 . spikes \n 
self . spikes2 = self . seg2 . spikes \n 
self . trains1 = self . seg1 . spiketrains \n 
self . trains2 = self . seg2 . spiketrains \n 
self . epcs1 = self . seg1 . epochs \n 
self . epcs2 = self . seg2 . epochs \n 
self . epcas1 = self . seg1 . epocharrays \n 
self . epcas2 = self . seg2 . epocharrays \n 
self . evts1 = self . seg1 . events \n 
self . evts2 = self . seg2 . events \n 
self . evtas1 = self . seg1 . eventarrays \n 
self . evtas2 = self . seg2 . eventarrays \n 
self . sigs1a = clone_object ( self . sigs1 ) \n 
self . sigarrs1a = clone_object ( self . sigarrs1 , n = 2 ) \n 
self . irsigs1a = clone_object ( self . irsigs1 ) \n 
self . spikes1a = clone_object ( self . spikes1 ) \n 
self . trains1a = clone_object ( self . trains1 ) \n 
self . epcs1a = clone_object ( self . epcs1 ) \n 
self . epcas1a = clone_object ( self . epcas1 ) \n 
self . evts1a = clone_object ( self . evts1 ) \n 
self . evtas1a = clone_object ( self . evtas1 ) \n 
for obj , obja in zip ( self . sigs1 + self . sigarrs1 , \n 
self . sigs1a + self . sigarrs1a ) : \n 
~~~ obja . channel_index = obj . channel_index \n 
~~ ~~ def test_init ( self ) : \n 
~~~ seg = Segment ( name = , index = 3 ) \n 
assert_neo_object_is_compliant ( seg ) \n 
self . assertEqual ( seg . name , ) \n 
self . assertEqual ( seg . file_origin , None ) \n 
self . assertEqual ( seg . index , 3 ) \n 
~~ def check_creation ( self , seg ) : \n 
~~~ assert_neo_object_is_compliant ( seg ) \n 
seed = seg . annotations [ ] \n 
targ0 = get_fake_value ( , datetime , seed = seed + 0 ) \n 
self . assertEqual ( seg . file_datetime , targ0 ) \n 
targ1 = get_fake_value ( , datetime , seed = seed + 1 ) \n 
self . assertEqual ( seg . rec_datetime , targ1 ) \n 
targ2 = get_fake_value ( , int , seed = seed + 2 ) \n 
self . assertEqual ( seg . index , targ2 ) \n 
targ3 = get_fake_value ( , str , seed = seed + 3 , obj = Segment ) \n 
self . assertEqual ( seg . name , targ3 ) \n 
targ4 = get_fake_value ( , str , \n 
seed = seed + 4 , obj = Segment ) \n 
self . assertEqual ( seg . description , targ4 ) \n 
targ5 = get_fake_value ( , str ) \n 
self . assertEqual ( seg . file_origin , targ5 ) \n 
targ6 = get_annotations ( ) \n 
targ6 [ ] = seed \n 
self . assertEqual ( seg . annotations , targ6 ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertEqual ( len ( seg . analogsignals ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . analogsignalarrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . irregularlysampledsignals ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . epochs ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . epocharrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . events ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . eventarrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . spikes ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . spiketrains ) , self . nchildren ** 2 ) \n 
~~ def test__creation ( self ) : \n 
~~~ self . check_creation ( self . seg1 ) \n 
self . check_creation ( self . seg2 ) \n 
~~ def test__merge ( self ) : \n 
~~~ seg1a = fake_neo ( Block , seed = self . seed1 , n = self . nchildren ) . segments [ 0 ] \n 
assert_same_sub_schema ( self . seg1 , seg1a ) \n 
seg1a . spikes . append ( self . spikes2 [ 0 ] ) \n 
seg1a . epocharrays . append ( self . epcas2 [ 0 ] ) \n 
seg1a . annotate ( seed = self . seed2 ) \n 
seg1a . merge ( self . seg2 ) \n 
assert_same_sub_schema ( self . sigs1a + self . sigs2 , seg1a . analogsignals ) \n 
assert_same_sub_schema ( self . sigarrs1a + self . sigarrs2 , \n 
seg1a . analogsignalarrays ) \n 
assert_same_sub_schema ( self . irsigs1a + self . irsigs2 , \n 
seg1a . irregularlysampledsignals ) \n 
assert_same_sub_schema ( self . epcs1 + self . epcs2 , seg1a . epochs ) \n 
assert_same_sub_schema ( self . epcas1 + self . epcas2 , seg1a . epocharrays ) \n 
assert_same_sub_schema ( self . evts1 + self . evts2 , seg1a . events ) \n 
assert_same_sub_schema ( self . evtas1 + self . evtas2 , seg1a . eventarrays ) \n 
assert_same_sub_schema ( self . spikes1 + self . spikes2 , seg1a . spikes ) \n 
assert_same_sub_schema ( self . trains1 + self . trains2 , seg1a . spiketrains ) \n 
~~ def test__children ( self ) : \n 
~~~ blk = Block ( name = ) \n 
blk . segments = [ self . seg1 ] \n 
blk . create_many_to_one_relationship ( force = True ) \n 
assert_neo_object_is_compliant ( self . seg1 ) \n 
assert_neo_object_is_compliant ( blk ) \n 
childobjs = ( , , \n 
, , \n 
, \n 
, ) \n 
childconts = ( , , \n 
self . assertEqual ( self . seg1 . _container_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _single_parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _child_properties , ( ) ) \n 
self . assertEqual ( self . seg1 . _single_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _container_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_parent_containers , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _parent_containers , ( , ) ) \n 
self . assertEqual ( len ( self . seg1 . _single_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children_recur ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children_recur ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . _multi_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children_recur ) , 0 ) \n 
children = ( self . sigs1a + self . sigarrs1a + \n 
self . epcs1a + self . epcas1a + \n 
self . evts1a + self . evtas1a + \n 
self . irsigs1a + \n 
self . spikes1a + self . trains1a ) \n 
assert_same_sub_schema ( list ( self . seg1 . _single_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children_recur ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children_recur ) , children ) \n 
self . assertEqual ( len ( self . seg1 . parents ) , 1 ) \n 
self . assertEqual ( self . seg1 . parents [ 0 ] . name , ) \n 
~~ def test__size ( self ) : \n 
~~~ targ1 = { "epochs" : self . nchildren , "events" : self . nchildren , \n 
"analogsignals" : self . nchildren ** 2 , \n 
"irregularlysampledsignals" : self . nchildren ** 2 , \n 
"spikes" : self . nchildren ** 2 , \n 
"spiketrains" : self . nchildren ** 2 , \n 
"epocharrays" : self . nchildren , "eventarrays" : self . nchildren , \n 
"analogsignalarrays" : self . nchildren } \n 
self . assertEqual ( self . targobj . size , targ1 ) \n 
~~ def test__filter_none ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( ) \n 
res1 = self . targobj . filter ( { } ) \n 
res2 = self . targobj . filter ( [ ] ) \n 
res3 = self . targobj . filter ( [ { } ] ) \n 
res4 = self . targobj . filter ( [ { } , { } ] ) \n 
res5 = self . targobj . filter ( [ { } , { } ] ) \n 
res6 = self . targobj . filter ( targdict = { } ) \n 
res7 = self . targobj . filter ( targdict = [ ] ) \n 
res8 = self . targobj . filter ( targdict = [ { } ] ) \n 
res9 = self . targobj . filter ( targdict = [ { } , { } ] ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
assert_same_sub_schema ( res5 , targ ) \n 
assert_same_sub_schema ( res6 , targ ) \n 
assert_same_sub_schema ( res7 , targ ) \n 
assert_same_sub_schema ( res8 , targ ) \n 
assert_same_sub_schema ( res9 , targ ) \n 
~~ def test__filter_annotation_single ( self ) : \n 
~~~ targ = ( self . sigs1a + self . sigarrs1a + \n 
[ self . epcs1a [ 0 ] , self . epcas1a [ 0 ] ] + \n 
[ self . evts1a [ 0 ] , self . evtas1a [ 0 ] ] + \n 
res0 = self . targobj . filter ( j = 0 ) \n 
res1 = self . targobj . filter ( { : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : 0 } ) \n 
res3 = self . targobj . filter ( [ { : 0 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 0 } ] ) \n 
~~ def test__filter_single_annotation_nores ( self ) : \n 
res0 = self . targobj . filter ( j = 5 ) \n 
res1 = self . targobj . filter ( { : 5 } ) \n 
res2 = self . targobj . filter ( targdict = { : 5 } ) \n 
res3 = self . targobj . filter ( [ { : 5 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 5 } ] ) \n 
~~ def test__filter_attribute_single ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } ) \n 
~~ def test__filter_attribute_single_nores ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs2 [ 0 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs2 [ 0 ] . name } ) \n 
~~ def test__filter_multi ( self ) : \n 
self . spikes1a + self . trains1a + \n 
[ self . epcs1a [ 1 ] ] ) \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name , \n 
: 0 } ) \n 
~~ def test__filter_multi_nores ( self ) : \n 
res0 = self . targobj . filter ( [ { : 0 } , { } ] ) \n 
res1 = self . targobj . filter ( { } , ttype = 0 ) \n 
res2 = self . targobj . filter ( [ { } ] , ttype = 0 ) \n 
res3 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
j = 0 ) \n 
res5 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
targdict = { : 0 } ) \n 
res6 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name , \n 
: 5 } ) \n 
res9 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name } , \n 
j = 5 ) \n 
res11 = self . targobj . filter ( name = self . epcs2 [ 1 ] . name , \n 
targdict = { : 5 } ) \n 
res12 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
res14 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
assert_same_sub_schema ( res10 , targ ) \n 
assert_same_sub_schema ( res11 , targ ) \n 
assert_same_sub_schema ( res12 , targ ) \n 
assert_same_sub_schema ( res13 , targ ) \n 
assert_same_sub_schema ( res14 , targ ) \n 
~~ def test__filter_multi_partres ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res3 = self . targobj . filter ( [ { : 1 } , { : 2 } ] ) \n 
res4 = self . targobj . filter ( { : 1 } , i = 2 ) \n 
res5 = self . targobj . filter ( [ { : 1 } ] , i = 2 ) \n 
~~ def test__filter_single_annotation_obj_single ( self ) : \n 
res0 = self . targobj . filter ( j = 1 , objects = ) \n 
res1 = self . targobj . filter ( j = 1 , objects = Epoch ) \n 
res2 = self . targobj . filter ( j = 1 , objects = [ ] ) \n 
res3 = self . targobj . filter ( j = 1 , objects = [ Epoch ] ) \n 
res4 = self . targobj . filter ( j = 1 , objects = [ Epoch , \n 
RecordingChannelGroup ] ) \n 
~~ def test__filter_single_annotation_obj_multi ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , objects = [ , EpochArray ] ) \n 
~~ def test__filter_single_annotation_obj_none ( self ) : \n 
res0 = self . targobj . filter ( j = 1 , objects = RecordingChannelGroup ) \n 
res1 = self . targobj . filter ( j = 1 , objects = ) \n 
~~ def test__filter_single_annotation_norecur ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] , \n 
self . evts1a [ 1 ] , self . evtas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , \n 
recursive = False ) \n 
~~ def test__filter_single_attribute_norecur ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
~~ def test__filter_single_annotation_nodata ( self ) : \n 
res0 = self . targobj . filter ( j = 0 , \n 
data = False ) \n 
~~ def test__filter_single_attribute_nodata ( self ) : \n 
~~ def test__filter_single_annotation_nodata_norecur ( self ) : \n 
data = False , recursive = False ) \n 
~~ def test__filter_single_attribute_nodata_norecur ( self ) : \n 
~~ def test__filter_single_annotation_container ( self ) : \n 
container = True ) \n 
~~ def test__filter_single_attribute_container ( self ) : \n 
~~ def test__filter_single_annotation_container_norecur ( self ) : \n 
container = True , recursive = False ) \n 
~~ def test__filter_single_attribute_container_norecur ( self ) : \n 
~~ def test__filter_single_annotation_nodata_container ( self ) : \n 
data = False , container = True ) \n 
~~ def test__filter_single_attribute_nodata_container ( self ) : \n 
~~ def test__filter_single_annotation_nodata_container_norecur ( self ) : \n 
data = False , container = True , \n 
~~ def test__filter_single_attribute_nodata_container_norecur ( self ) : \n 
data = self . targobj . children_recur \n 
targ = ( self . sigs1a + self . sigarrs1a + \n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
~~ def test__filterdata_multi_nores ( self ) : \n 
~~~ data = self . targobj . children_recur \n 
targ = [ ] \n 
res0 = filterdata ( data , [ { : 0 } , { } ] ) \n 
res1 = filterdata ( data , { } , ttype = 0 ) \n 
res2 = filterdata ( data , [ { } ] , ttype = 0 ) \n 
res3 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res5 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 0 } ) \n 
res6 = filterdata ( data , name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = filterdata ( data , { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res9 = filterdata ( data , { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res11 = filterdata ( data , name = self . epcs2 [ 1 ] . name , targdict = { : 5 } ) \n 
res12 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res14 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 5 } ) \n 
~~ def test__filterdata_multi_partres ( self ) : \n 
targ = [ self . epcs1a [ 1 ] ] \n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res3 = filterdata ( data , [ { : 1 } , { : 2 } ] ) \n 
res4 = filterdata ( data , { : 1 } , i = 2 ) \n 
res5 = filterdata ( data , [ { : 1 } ] , i = 2 ) \n 
def test__pretty ( self ) : \n 
~~~ ann = get_annotations ( ) \n 
ann [ ] = self . seed1 \n 
ann = pretty ( ann ) . replace ( , ) \n 
res = pretty ( self . seg1 ) \n 
sig0 = pretty ( self . sigs1 [ 0 ] ) \n 
sig1 = pretty ( self . sigs1 [ 1 ] ) \n 
sig2 = pretty ( self . sigs1 [ 2 ] ) \n 
sig3 = pretty ( self . sigs1 [ 3 ] ) \n 
sig0 = sig0 . replace ( , ) \n 
sig1 = sig1 . replace ( , ) \n 
sig2 = sig2 . replace ( , ) \n 
sig3 = sig3 . replace ( , ) \n 
sigarr0 = pretty ( self . sigarrs1 [ 0 ] ) \n 
sigarr1 = pretty ( self . sigarrs1 [ 1 ] ) \n 
sigarr0 = sigarr0 . replace ( , ) \n 
sigarr1 = sigarr1 . replace ( , ) \n 
( len ( self . sigs1a ) , len ( self . sigarrs1a ) ) ) + \n 
( len ( self . epcs1a ) , len ( self . epcas1a ) ) ) + \n 
( len ( self . evts1a ) , len ( self . evtas1a ) ) ) + \n 
len ( self . irsigs1a ) ) + \n 
( len ( self . spikes1a ) , len ( self . trains1a ) ) ) + \n 
( self . seg1 . name , self . seg1 . description ) \n 
) + \n 
( % ( 0 , sig0 ) ) + \n 
( % ( 1 , sig1 ) ) + \n 
( % ( 2 , sig2 ) ) + \n 
( % ( 3 , sig3 ) ) + \n 
( % ( 0 , sigarr0 ) ) + \n 
( % ( 1 , sigarr1 ) ) ) \n 
self . assertEqual ( res , targ ) \n 
~~ def test__construct_subsegment_by_unit ( self ) : \n 
~~~ nb_seg = 3 \n 
nb_unit = 7 \n 
unit_with_sig = np . array ( [ 0 , 2 , 5 ] ) \n 
signal_types = [ , ] \n 
sig_len = 100 \n 
#recordingchannelgroups \n 
rcgs = [ RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) , \n 
RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) ] \n 
all_unit = [ ] \n 
for u in range ( nb_unit ) : \n 
~~~ un = Unit ( name = % u , channel_indexes = np . array ( [ u ] ) ) \n 
assert_neo_object_is_compliant ( un ) \n 
all_unit . append ( un ) \n 
~~ blk = Block ( ) \n 
blk . recordingchannelgroups = rcgs \n 
for s in range ( nb_seg ) : \n 
~~~ seg = Segment ( name = % s ) \n 
for j in range ( nb_unit ) : \n 
~~~ st = SpikeTrain ( [ 1 , 2 ] , units = , \n 
t_start = 0. , t_stop = 10 ) \n 
st . unit = all_unit [ j ] \n 
~~ for t in signal_types : \n 
~~~ anasigarr = AnalogSignalArray ( np . zeros ( ( sig_len , \n 
len ( unit_with_sig ) ) ) , \n 
units = , \n 
sampling_rate = 1000. * pq . Hz , \n 
channel_indexes = unit_with_sig ) \n 
seg . analogsignalarrays . append ( anasigarr ) \n 
~~ ~~ blk . create_many_to_one_relationship ( ) \n 
for unit in all_unit : \n 
~~~ assert_neo_object_is_compliant ( unit ) \n 
~~ for rcg in rcgs : \n 
~~~ assert_neo_object_is_compliant ( rcg ) \n 
~~ assert_neo_object_is_compliant ( blk ) \n 
newseg = seg . construct_subsegment_by_unit ( all_unit [ : 4 ] ) \n 
assert_neo_object_is_compliant ( newseg ) \n 
~~ def test_segment_take_spikes_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spikes_by_unit ( ) \n 
result21 = self . seg1 . take_spikes_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spikes_by_unit ( [ self . unit2 ] ) \n 
self . assertEqual ( result1 , [ ] ) \n 
assert_same_sub_schema ( result21 , [ self . spikes1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . spikes1a [ 1 ] ] ) \n 
~~ def test_segment_take_spiketrains_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spiketrains_by_unit ( ) \n 
result21 = self . seg1 . take_spiketrains_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spiketrains_by_unit ( [ self . unit2 ] ) \n 
assert_same_sub_schema ( result21 , [ self . trains1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . trains1a [ 1 ] ] ) \n 
~~ def test_segment_take_analogsignal_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_analogsignal_by_unit ( ) \n 
result21 = self . seg1 . take_analogsignal_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_unit ( [ self . unit2 ] ) \n 
assert_same_sub_schema ( result21 , [ self . sigs1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . sigs1a [ 1 ] ] ) \n 
~~ def test_segment_take_analogsignal_by_channelindex ( self ) : \n 
~~~ ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind2 = self . unit2 . channel_indexes [ 0 ] \n 
result1 = self . seg1 . take_analogsignal_by_channelindex ( ) \n 
result21 = self . seg1 . take_analogsignal_by_channelindex ( [ ind1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n 
~~ def test_seg_take_slice_of_analogsignalarray_by_unit ( self ) : \n 
~~~ seg = self . seg1 \n 
result1 = seg . take_slice_of_analogsignalarray_by_unit ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit3 ] ) \n 
targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ False ] ) ] ] \n 
targ3 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ False ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ True ] ) ] ] \n 
assert_same_sub_schema ( result21 , targ1 ) \n 
assert_same_sub_schema ( result23 , targ3 ) \n 
~~ def test_seg_take_slice_of_analogsignalarray_by_channelindex ( self ) : \n 
ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind3 = self . unit3 . channel_indexes [ 0 ] \n 
result1 = seg . take_slice_of_analogsignalarray_by_channelindex ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind3 ] ) \n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
from __future__ import absolute_import , division \n 
~~ from neo . io import NeuroScopeIO \n 
from neo . test . iotest . common_io_test import BaseTestIO \n 
class TestNeuroScopeIO ( BaseTestIO , unittest . TestCase , ) : \n 
~~~ ioclass = NeuroScopeIO \n 
files_to_test = [ ] \n 
files_to_download = [ , \n 
~~ if __name__ == "__main__" : \n 
~~ import theano \n 
from theano import tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams as RandomStreams \n 
from load import mnist \n 
srng = RandomStreams ( ) \n 
def floatX ( X ) : \n 
~~~ return np . asarray ( X , dtype = theano . config . floatX ) \n 
~~ def init_weights ( shape ) : \n 
~~~ return theano . shared ( floatX ( np . random . randn ( * shape ) * 0.01 ) ) \n 
~~ def rectify ( X ) : \n 
~~~ return T . maximum ( X , 0. ) \n 
~~ def softmax ( X ) : \n 
~~~ e_x = T . exp ( X - X . max ( axis = 1 ) . dimshuffle ( 0 , ) ) \n 
return e_x / e_x . sum ( axis = 1 ) . dimshuffle ( 0 , ) \n 
~~ def RMSprop ( cost , params , lr = 0.001 , rho = 0.9 , epsilon = 1e-6 ) : \n 
~~~ grads = T . grad ( cost = cost , wrt = params ) \n 
updates = [ ] \n 
for p , g in zip ( params , grads ) : \n 
~~~ acc = theano . shared ( p . get_value ( ) * 0. ) \n 
acc_new = rho * acc + ( 1 - rho ) * g ** 2 \n 
gradient_scaling = T . sqrt ( acc_new + epsilon ) \n 
g = g / gradient_scaling \n 
updates . append ( ( acc , acc_new ) ) \n 
updates . append ( ( p , p - lr * g ) ) \n 
~~ return updates \n 
~~ def dropout ( X , p = 0. ) : \n 
~~~ if p > 0 : \n 
~~~ retain_prob = 1 - p \n 
X *= srng . binomial ( X . shape , p = retain_prob , dtype = theano . config . floatX ) \n 
X /= retain_prob \n 
~~ def model ( X , w_h , w_h2 , w_o , p_drop_input , p_drop_hidden ) : \n 
~~~ X = dropout ( X , p_drop_input ) \n 
h = rectify ( T . dot ( X , w_h ) ) \n 
h = dropout ( h , p_drop_hidden ) \n 
h2 = rectify ( T . dot ( h , w_h2 ) ) \n 
h2 = dropout ( h2 , p_drop_hidden ) \n 
py_x = softmax ( T . dot ( h2 , w_o ) ) \n 
return h , h2 , py_x \n 
~~ trX , teX , trY , teY = mnist ( onehot = True ) \n 
X = T . fmatrix ( ) \n 
Y = T . fmatrix ( ) \n 
w_h = init_weights ( ( 784 , 625 ) ) \n 
w_h2 = init_weights ( ( 625 , 625 ) ) \n 
w_o = init_weights ( ( 625 , 10 ) ) \n 
noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , 0.2 , 0.5 ) \n 
h , h2 , py_x = model ( X , w_h , w_h2 , w_o , 0. , 0. ) \n 
y_x = T . argmax ( py_x , axis = 1 ) \n 
cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n 
params = [ w_h , w_h2 , w_o ] \n 
updates = RMSprop ( cost , params , lr = 0.001 ) \n 
train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n 
predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n 
for i in range ( 100 ) : \n 
~~~ for start , end in zip ( range ( 0 , len ( trX ) , 128 ) , range ( 128 , len ( trX ) , 128 ) ) : \n 
~~~ cost = train ( trX [ start : end ] , trY [ start : end ] ) \n 
~~ print np . mean ( np . argmax ( teY , axis = 1 ) == predict ( teX ) ) \n 
from matplotlib import pyplot as plt \n 
import oscaar \n 
import astrometry \n 
import photometry \n 
import dataBank \n 
import systematics \n 
import IO \n 
import pyfits \n 
plt . ion ( ) \n 
data = dataBank . dataBank ( ) \n 
allStars = data . getDict ( ) \n 
outputPath = data . outputPath \n 
N_exposures = len ( data . getPaths ( ) ) \n 
meanDarkFrame = data . getMeanDarkFrame ( ) \n 
masterFlat = data . masterFlat \n 
plottingThings , statusBarFig , statusBarAx = IO . plottingSettings ( data . trackPlots , data . photPlots ) \n 
for expNumber in xrange ( N_exposures ) : \n 
~~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
~~~ plt . cla ( ) \n 
statusBarAx . set_title ( ) \n 
statusBarAx . set_xlim ( [ 0 , 100 ] ) \n 
statusBarAx . set_xlabel ( ) \n 
statusBarAx . get_yaxis ( ) . set_ticks ( [ ] ) \n 
statusBarAx . barh ( [ 0 ] , [ 100.0 * expNumber / len ( data . getPaths ( ) ) ] , \n 
[ 1 ] , color = ) \n 
~~ image = ( pyfits . getdata ( data . getPaths ( ) [ expNumber ] ) - meanDarkFrame ) / masterFlat \n 
data . storeTime ( expNumber ) \n 
for star in allStars : \n 
~~~ est_x , est_y = data . centroidInitialGuess ( expNumber , star ) \n 
x , y , radius , trackFlag = astrometry . trackSmooth ( image , est_x , est_y , \n 
data . smoothConst , \n 
plottingThings , \n 
zoom = data . trackingZoom , \n 
plots = data . trackPlots ) \n 
data . storeCentroid ( star , expNumber , x , y ) \n 
fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n 
data . apertureRadii , \n 
ccdGain = data . ccdGain , \n 
plots = data . photPlots ) \n 
photFlag = any ( photFlags ) \n 
data . storeFluxes ( star , expNumber , fluxes , errors ) \n 
if trackFlag or photFlag and not data . getFlag ( ) : \n 
~~~ data . setFlag ( star , False ) \n 
~~ if data . trackPlots or data . photPlots : \n 
~~~ plt . draw ( ) \n 
~~ ~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
~~ ~~ plt . close ( ) \n 
data . scaleFluxes_multirad ( ) \n 
meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n 
lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n 
meanComparisonStarErrors ) \n 
oscaar . IO . save ( data , outputPath ) \n 
data . plotLightCurve_multirad ( ) \n 
import warnings \n 
from openmdao . components . exec_comp import ExecComp \n 
class ConstraintComp ( ExecComp ) : \n 
def __init__ ( self , expr , out = ) : \n 
~~~ warnings . simplefilter ( , DeprecationWarning ) \n 
DeprecationWarning , stacklevel = 2 ) \n 
warnings . simplefilter ( , DeprecationWarning ) \n 
newexpr = _combined_expr ( expr ) \n 
~~ ~~ def _combined_expr ( expr ) : \n 
lhs , op , rhs = _parse_constraint ( expr ) \n 
first , second = ( rhs , lhs ) if op . startswith ( ) else ( lhs , rhs ) \n 
~~~ if float ( first ) == 0 : \n 
~~~ return "-(%s)" % second \n 
~~ ~~ except Exception : \n 
~~~ if float ( second ) == 0. : \n 
~~~ return first \n 
~~ return % ( first , second ) \n 
~~ def _parse_constraint ( expr_string ) : \n 
for comparator in [ , , , , , ] : \n 
~~~ parts = expr_string . split ( comparator ) \n 
if len ( parts ) == 2 : \n 
~~~ if comparator == : \n 
~~~ break \n 
~~ return ( parts [ 0 ] . strip ( ) , comparator , parts [ 1 ] . strip ( ) ) \n 
raise ValueError ( msg ) \n 
import traceback \n 
from itertools import chain \n 
from six import iteritems , itervalues \n 
from six . moves import cStringIO \n 
import networkx as nx \n 
from openmdao . core . system import System \n 
from openmdao . core . group import Group \n 
from openmdao . core . component import Component \n 
from openmdao . core . parallel_group import ParallelGroup \n 
from openmdao . core . parallel_fd_group import ParallelFDGroup \n 
from openmdao . core . basic_impl import BasicImpl \n 
from openmdao . core . _checks import check_connections , _both_names \n 
from openmdao . core . driver import Driver \n 
from openmdao . core . mpi_wrap import MPI , under_mpirun , debug \n 
from openmdao . core . relevance import Relevance \n 
from openmdao . components . indep_var_comp import IndepVarComp \n 
from openmdao . solvers . scipy_gmres import ScipyGMRES \n 
from openmdao . solvers . ln_direct import DirectSolver \n 
from openmdao . solvers . ln_gauss_seidel import LinearGaussSeidel \n 
from openmdao . units . units import get_conversion_tuple \n 
from openmdao . util . string_util import get_common_ancestor , nearest_child , name_relative_to \n 
from openmdao . util . graph import plain_bfs \n 
from openmdao . util . options import OptionsDictionary \n 
force_check = os . environ . get ( ) \n 
trace = os . environ . get ( ) \n 
class _ProbData ( object ) : \n 
~~~ self . top_lin_gs = False \n 
self . in_complex_step = False \n 
~~ ~~ class Problem ( object ) : \n 
def __init__ ( self , root = None , driver = None , impl = None , comm = None ) : \n 
~~~ super ( Problem , self ) . __init__ ( ) \n 
self . root = root \n 
self . _probdata = _ProbData ( ) \n 
if MPI : \n 
~~~ from openmdao . core . petsc_impl import PetscImpl \n 
if impl != PetscImpl : \n 
~~ ~~ if impl is None : \n 
~~~ self . _impl = BasicImpl \n 
~~~ self . _impl = impl \n 
~~ self . comm = comm \n 
if driver is None : \n 
~~~ self . driver = Driver ( ) \n 
~~~ self . driver = driver \n 
~~ self . pathname = \n 
~~ def __getitem__ ( self , name ) : \n 
if name in self . root . unknowns : \n 
~~~ return self . root . unknowns [ name ] \n 
~~ elif name in self . root . params : \n 
~~~ return self . root . params [ name ] \n 
~~ elif name in self . root . _sysdata . to_abs_pnames : \n 
~~~ for p in self . root . _sysdata . to_abs_pnames [ name ] : \n 
~~~ return self . _rec_get_param ( p ) \n 
~~ ~~ elif name in self . _dangling : \n 
~~~ for p in self . _dangling [ name ] : \n 
~~ ~~ def _rec_get_param ( self , absname ) : \n 
~~~ parts = absname . rsplit ( , 1 ) \n 
if len ( parts ) == 1 : \n 
~~~ return self . root . params [ absname ] \n 
~~~ grp = self . root . _subsystem ( parts [ 0 ] ) \n 
return grp . params [ parts [ 1 ] ] \n 
~~ ~~ def __setitem__ ( self , name , val ) : \n 
~~~ self . root . unknowns [ name ] = val \n 
~~ elif name in self . _dangling : \n 
~~~ parts = p . rsplit ( , 1 ) \n 
~~~ self . root . params [ p ] = val \n 
grp . params [ parts [ 1 ] ] = val \n 
~~ ~~ def _setup_connections ( self , params_dict , unknowns_dict ) : \n 
to_prom_name = self . _probdata . to_prom_name \n 
connections = self . root . _get_explicit_connections ( ) \n 
prom_noconns = self . _add_implicit_connections ( connections ) \n 
input_graph = nx . DiGraph ( ) \n 
self . _dangling = { } \n 
to_abs_pnames = self . root . _sysdata . to_abs_pnames \n 
usrcs = set ( ) \n 
for tgt , srcs in iteritems ( connections ) : \n 
~~~ for src , idxs in srcs : \n 
~~~ input_graph . add_edge ( src , tgt , idxs = idxs ) \n 
if src in unknowns_dict : \n 
~~~ usrcs . add ( src ) \n 
~~ ~~ ~~ for prom , plist in iteritems ( to_abs_pnames ) : \n 
~~~ input_graph . add_nodes_from ( plist ) \n 
if prom in prom_noconns : \n 
~~~ start = plist [ 0 ] \n 
input_graph . add_edges_from ( ( ( start , p ) for p in plist [ 1 : ] ) , \n 
idxs = None ) \n 
~~ ~~ newconns = { } \n 
for src in usrcs : \n 
~~~ newconns [ src ] = None \n 
src_idxs = { src : None } \n 
for s , t in nx . dfs_edges ( input_graph , src ) : \n 
~~~ tidxs = input_graph [ s ] [ t ] [ ] \n 
sidxs = src_idxs [ s ] \n 
if tidxs is None : \n 
~~~ tidxs = sidxs \n 
~~ elif sidxs is not None : \n 
~~~ tidxs = np . array ( sidxs ) [ tidxs ] \n 
~~ src_idxs [ t ] = tidxs \n 
if t in newconns : \n 
~~~ newconns [ t ] . append ( ( src , tidxs ) ) \n 
~~~ newconns [ t ] = [ ( src , tidxs ) ] \n 
~~ ~~ ~~ self . _input_inputs = { } \n 
for node in input_graph . nodes_iter ( ) : \n 
~~~ if node not in newconns and len ( input_graph . pred [ node ] ) == 0 : \n 
~~~ nosrc = [ node ] \n 
for s , t in nx . dfs_edges ( input_graph , node ) : \n 
~~~ src = newconns [ t ] [ 0 ] [ 0 ] \n 
for n in nosrc : \n 
~~~ newconns [ n ] = [ ( src , None ) ] \n 
~~ break \n 
~~~ nosrc . append ( t ) \n 
~~~ set_nosrc = set ( nosrc ) \n 
~~~ self . _dangling [ to_prom_name [ n ] ] = set_nosrc \n 
self . _input_inputs [ n ] = nosrc \n 
~~ ~~ ~~ ~~ connections = OrderedDict ( ) \n 
for tgt , srcs in sorted ( newconns . items ( ) ) : \n 
~~~ if srcs is not None : \n 
~~~ if len ( srcs ) > 1 : \n 
~~~ src_names = ( n for n , idx in srcs ) \n 
( tgt , sorted ( src_names ) ) ) \n 
~~ connections [ tgt ] = srcs [ 0 ] \n 
~~ ~~ return connections \n 
~~ def _check_input_diffs ( self , connections , params_dict , unknowns_dict ) : \n 
for tgt , connected_inputs in iteritems ( self . _input_inputs ) : \n 
~~~ tgt_idx = connected_inputs . index ( tgt ) \n 
units = [ params_dict [ n ] . get ( ) for n in connected_inputs ] \n 
vals = [ params_dict [ n ] [ ] for n in connected_inputs ] \n 
diff_units = [ ] \n 
for i , u in enumerate ( units ) : \n 
~~~ if i != tgt_idx and u != units [ tgt_idx ] : \n 
~~~ if units [ tgt_idx ] is None : \n 
~~~ sname , s = connected_inputs [ i ] , u \n 
tname , t = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
~~~ sname , s = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
tname , t = connected_inputs [ i ] , u \n 
~~ diff_units . append ( ( connected_inputs [ i ] , u ) ) \n 
~~ ~~ if isinstance ( vals [ tgt_idx ] , np . ndarray ) : \n 
~~~ diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if not \n 
( isinstance ( v , np . ndarray ) and \n 
v . shape == vals [ tgt_idx ] . shape and \n 
( v == vals [ tgt_idx ] ) . all ( ) ) ] \n 
~~~ vtype = type ( vals [ tgt_idx ] ) \n 
diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if vtype != type ( v ) or \n 
v != vals [ tgt_idx ] ] \n 
~~ if diff_units : \n 
~~~ filt = set ( [ u for n , u in diff_units ] ) \n 
if None in filt : \n 
~~~ filt . remove ( None ) \n 
~~ if filt : \n 
~~~ proms = set ( [ params_dict [ item ] [ ] for item in connected_inputs ] ) \n 
if len ( proms ) == 1 : \n 
correct_src = params_dict [ connected_inputs [ 0 ] ] [ ] \n 
self . _setup_errors . append ( msg ) \n 
~~ ~~ if diff_vals : \n 
( sorted ( [ ( tgt , params_dict [ tgt ] [ ] ) ] + \n 
diff_vals ) ) ) \n 
~~ ~~ for promname , absnames in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ if len ( absnames ) > 1 : \n 
~~~ step_sizes , step_types , forms = { } , { } , { } \n 
for name in absnames : \n 
~~~ meta = self . root . _params_dict [ name ] \n 
ss = meta . get ( ) \n 
if ss is not None : \n 
~~~ step_sizes [ ss ] = name \n 
~~ st = meta . get ( ) \n 
if st is not None : \n 
~~~ step_types [ st ] = name \n 
~~ f = meta . get ( ) \n 
if f is not None : \n 
~~~ forms [ f ] = name \n 
~~ ~~ if len ( step_sizes ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in step_sizes . items ( ) ] ) ) ) \n 
~~ if len ( step_types ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in step_types . items ( ) ] ) ) ) \n 
~~ if len ( forms ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n 
~~ ~~ ~~ ~~ def _get_ubc_vars ( self , connections ) : \n 
full_order = { s . pathname : i for i , s in \n 
enumerate ( self . root . subsystems ( recurse = True ) ) } \n 
ubcs = [ ] \n 
~~~ tsys = tgt . rsplit ( , 1 ) [ 0 ] \n 
ssys = srcs [ 0 ] . rsplit ( , 1 ) [ 0 ] \n 
if full_order [ ssys ] > full_order [ tsys ] : \n 
~~~ ubcs . append ( tgt ) \n 
~~ ~~ return ubcs \n 
~~ def setup ( self , check = True , out_stream = sys . stdout ) : \n 
self . _setup_errors = [ ] \n 
tree_changed = False \n 
meta_changed = False \n 
if isinstance ( self . root . ln_solver , LinearGaussSeidel ) : \n 
~~~ self . _probdata . top_lin_gs = True \n 
~~ self . driver . root = self . root \n 
self . root . _init_sys_data ( self . pathname , self . _probdata ) \n 
self . _setup_communicators ( ) \n 
params_dict , unknowns_dict = self . root . _setup_variables ( ) \n 
self . _probdata . params_dict = params_dict \n 
self . _probdata . unknowns_dict = unknowns_dict \n 
self . _probdata . to_prom_name = self . root . _sysdata . to_prom_name \n 
connections = self . _setup_connections ( params_dict , unknowns_dict ) \n 
self . _probdata . connections = connections \n 
for tgt , ( src , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ tgt ] \n 
if not in tmeta or not tmeta [ ] : \n 
~~~ if tmeta [ ] == ( ) : \n 
~~~ smeta = unknowns_dict [ src ] \n 
if idxs is not None : \n 
~~~ size = len ( idxs ) \n 
tmeta [ ] = ( size , ) \n 
tmeta [ ] = size \n 
tmeta [ ] = smeta [ ] [ np . array ( idxs ) ] \n 
~~~ tmeta [ ] = smeta [ ] \n 
tmeta [ ] = smeta [ ] \n 
~~ ~~ if idxs is not None : \n 
~~~ if isinstance ( idxs , np . ndarray ) : \n 
~~~ tmeta [ ] = idxs \n 
~~~ tmeta [ ] = np . array ( idxs , \n 
dtype = self . _impl . idx_arr_type ) \n 
~~ ~~ ~~ ~~ if MPI : \n 
~~~ for s in self . root . components ( recurse = True ) : \n 
~~~ if hasattr ( s , ) or ( \n 
hasattr ( s , ) and ( s . setup_distrib \n 
is not Component . setup_distrib ) ) : \n 
~~~ meta_changed = True \n 
~~ ~~ ~~ if tree_changed : \n 
~~~ return self . setup ( check = check , out_stream = out_stream ) \n 
~~ elif meta_changed : \n 
~~~ params_dict , unknowns_dict = self . root . _setup_variables ( compute_indices = True ) \n 
~~ self . _setup_errors . extend ( check_connections ( connections , params_dict , \n 
unknowns_dict , \n 
self . root . _sysdata . to_prom_name ) ) \n 
self . _setup_units ( connections , params_dict , unknowns_dict ) \n 
to_prom_name = self . root . _sysdata . to_prom_name \n 
self . _probdata . to_prom_name = to_prom_name \n 
for path , meta in iteritems ( params_dict ) : \n 
~~~ meta [ ] = to_prom_name [ path ] \n 
if path not in connections : \n 
~~~ if not in meta or not meta [ ] : \n 
~~~ if meta [ ] == ( ) : \n 
~~ ~~ ~~ ~~ for path , meta in iteritems ( unknowns_dict ) : \n 
~~ param_owners = _assign_parameters ( connections ) \n 
pois = self . driver . desvars_of_interest ( ) \n 
oois = self . driver . outputs_of_interest ( ) \n 
self . _driver_vois = set ( ) \n 
for tup in chain ( pois , oois ) : \n 
~~~ self . _driver_vois . update ( tup ) \n 
~~ promoted_unknowns = self . root . _sysdata . to_abs_uname \n 
parallel_p = False \n 
for vnames in pois : \n 
~~~ if len ( vnames ) > 1 : \n 
~~~ parallel_p = True \n 
~~ for v in vnames : \n 
~~~ if v not in promoted_unknowns : \n 
~~ ~~ ~~ parallel_u = False \n 
for vnames in oois : \n 
~~~ parallel_u = True \n 
~~ ~~ ~~ mode = self . _check_for_parallel_derivs ( pois , oois , parallel_u , parallel_p ) \n 
self . _probdata . relevance = Relevance ( self . root , params_dict , \n 
unknowns_dict , connections , \n 
pois , oois , mode ) \n 
for s in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if not s . _order_set : \n 
~~~ order = None \n 
broken_edges = None \n 
if self . comm . rank == 0 : \n 
~~~ order , broken_edges = s . list_auto_order ( ) \n 
~~ if MPI : \n 
~~~ if trace : \n 
~~ order , broken_edges = self . comm . bcast ( ( order , broken_edges ) , root = 0 ) \n 
if trace : \n 
~~ ~~ s . set_order ( order ) \n 
for edge in broken_edges : \n 
~~~ cname = edge [ 1 ] \n 
head_sys = self . root \n 
for name in cname . split ( ) : \n 
~~~ head_sys = getattr ( head_sys , name ) \n 
~~ head_sys . _run_apply = True \n 
~~ ~~ ~~ self . _check_input_diffs ( connections , params_dict , unknowns_dict ) \n 
alloc_derivs = not self . root . fd_options [ ] \n 
for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ alloc_derivs = alloc_derivs or sub . nl_solver . supports [ ] \n 
~~ self . root . _setup_vectors ( param_owners , impl = self . _impl , alloc_derivs = alloc_derivs ) \n 
self . driver . _setup ( ) \n 
self . _poi_indices , self . _qoi_indices = self . driver . _map_voi_indices ( ) \n 
~~~ sub . nl_solver . setup ( sub ) \n 
sub . ln_solver . setup ( sub ) \n 
~~ self . _check_solvers ( ) \n 
self . _start_recorders ( ) \n 
if self . _setup_errors : \n 
~~~ stream = cStringIO ( ) \n 
for err in self . _setup_errors : \n 
~~~ stream . write ( "%s\\n" % err ) \n 
~~ raise RuntimeError ( stream . getvalue ( ) ) \n 
~~ OptionsDictionary . locked = True \n 
if check or force_check : \n 
~~~ return self . check_setup ( out_stream ) \n 
~~ return { } \n 
~~ def cleanup ( self ) : \n 
self . driver . cleanup ( ) \n 
self . root . cleanup ( ) \n 
~~ def _check_solvers ( self ) : \n 
iterated_states = set ( ) \n 
group_states = [ ] \n 
has_iter_solver = { } \n 
for group in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ has_iter_solver [ group . pathname ] = ( group . ln_solver . options [ ] > 1 ) \n 
~~~ if isinstance ( group . ln_solver , DirectSolver ) : \n 
~~~ has_iter_solver [ group . pathname ] = ( True ) \n 
~~ ~~ opt = group . fd_options \n 
if opt [ ] == True and opt [ ] == : \n 
~~~ if group . name != : \n 
~~ for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if hasattr ( sub . nl_solver , ) : \n 
self . _setup_errors . append ( msg . format ( sub . name ) ) \n 
~~ ~~ ~~ parts = group . pathname . split ( ) \n 
for i in range ( len ( parts ) ) : \n 
~~~ if has_iter_solver [ . join ( parts [ : i ] ) ] : \n 
~~~ is_iterated_somewhere = True \n 
~~~ is_iterated_somewhere = False \n 
~~ if is_iterated_somewhere : \n 
~~ if isinstance ( group . ln_solver , LinearGaussSeidel ) and group . ln_solver . options [ ] == 1 : \n 
~~~ graph = group . _get_sys_graph ( ) \n 
strong = [ sorted ( s ) for s in nx . strongly_connected_components ( graph ) \n 
if len ( s ) > 1 ] \n 
if strong : \n 
"recommended)." \n 
% ( group . pathname , strong ) ) \n 
~~ ~~ states = [ n for n , m in iteritems ( group . _unknowns_dict ) if m . get ( ) ] \n 
if states : \n 
~~~ group_states . append ( ( group , states ) ) \n 
if isinstance ( group . ln_solver , DirectSolver ) or group . ln_solver . options [ ] > 1 : \n 
~~~ iterated_states . update ( states ) \n 
~~~ for s in states : \n 
~~~ if s not in iterated_states : \n 
~~~ cname = s . rsplit ( , 1 ) [ 0 ] \n 
comp = self . root \n 
~~~ comp = getattr ( comp , name ) \n 
~~ if not _needs_iteration ( comp ) : \n 
~~~ iterated_states . add ( s ) \n 
~~ ~~ ~~ ~~ ~~ ~~ for group , states in group_states : \n 
~~~ uniterated_states = [ s for s in states if s not in iterated_states ] \n 
if uniterated_states : \n 
( group . pathname , uniterated_states ) ) \n 
~~ ~~ ~~ def _check_dangling_params ( self , out_stream = sys . stdout ) : \n 
dangling_params = sorted ( set ( [ \n 
to_prom_name [ p ] for p , m in iteritems ( self . root . _params_dict ) \n 
if p not in self . root . connections \n 
] ) ) \n 
if dangling_params : \n 
file = out_stream ) \n 
for d in dangling_params : \n 
~~~ print ( d , file = out_stream ) \n 
~~ ~~ return dangling_params \n 
~~ def _check_mode ( self , out_stream = sys . stdout ) : \n 
if self . _calculated_mode != self . root . _probdata . relevance . mode : \n 
self . _calculated_mode , \n 
self . _p_length , \n 
self . _u_length ) , \n 
~~ return ( self . root . _probdata . relevance . mode , self . _calculated_mode ) \n 
~~ def _check_no_unknown_comps ( self , out_stream = sys . stdout ) : \n 
nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n 
local = True ) \n 
if len ( c . unknowns ) == 0 ] ) \n 
if nocomps : \n 
for n in nocomps : \n 
~~~ print ( n , file = out_stream ) \n 
~~ ~~ return nocomps \n 
~~ def _check_no_recorders ( self , out_stream = sys . stdout ) : \n 
recorders = [ ] \n 
recorders . extend ( self . driver . recorders ) \n 
for grp in self . root . subgroups ( recurse = True , local = True , \n 
include_self = True ) : \n 
~~~ recorders . extend ( grp . nl_solver . recorders ) \n 
recorders . extend ( grp . ln_solver . recorders ) \n 
~~ if not recorders : \n 
~~ return recorders \n 
~~ def _check_no_connect_comps ( self , out_stream = sys . stdout ) : \n 
conn_comps = set ( [ t . rsplit ( , 1 ) [ 0 ] \n 
for t in self . root . connections ] ) \n 
conn_comps . update ( [ s . rsplit ( , 1 ) [ 0 ] \n 
for s , i in itervalues ( self . root . connections ) ] ) \n 
noconn_comps = sorted ( [ c . pathname \n 
for c in self . root . components ( recurse = True , local = True ) \n 
if c . pathname not in conn_comps ] ) \n 
if noconn_comps : \n 
for comp in noconn_comps : \n 
~~~ print ( comp , file = out_stream ) \n 
~~ ~~ return noconn_comps \n 
~~ def _check_mpi ( self , out_stream = sys . stdout ) : \n 
if under_mpirun ( ) : \n 
~~~ parr = True \n 
~~~ for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if ( isinstance ( grp , ParallelGroup ) or \n 
isinstance ( grp , ParallelFDGroup ) ) : \n 
~~~ parr = False \n 
~~ mincpu , maxcpu = self . root . get_req_procs ( ) \n 
if maxcpu is not None and self . comm . size > maxcpu : \n 
( self . comm . size , maxcpu ) ) \n 
~~ return ( self . comm . size , maxcpu , parr ) \n 
~~~ pargrps = [ ] \n 
for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if isinstance ( grp , ParallelGroup ) : \n 
grp . pathname , file = out_stream ) \n 
pargrps . append ( grp . pathname ) \n 
~~ ~~ return sorted ( pargrps ) \n 
~~ ~~ def _check_graph ( self , out_stream = sys . stdout ) : \n 
cycles = [ ] \n 
ooo = [ ] \n 
~~~ graph = grp . _get_sys_graph ( ) \n 
strong = [ s for s in nx . strongly_connected_components ( graph ) \n 
~~~ relstrong = [ ] \n 
for slist in strong : \n 
~~~ relstrong . append ( [ ] ) \n 
for s in slist : \n 
~~~ relstrong [ - 1 ] . append ( nearest_child ( grp . pathname , s ) ) \n 
subs = [ s for s in grp . _subsystems ] \n 
tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - 1 ] ] ) \n 
relstrong [ - 1 ] = [ t [ 1 ] for t in tups ] \n 
( grp . pathname , relstrong ) , file = out_stream ) \n 
cycles . append ( relstrong ) \n 
~~ graph , _ = grp . _break_cycles ( grp . list_order ( ) , graph ) \n 
visited = set ( ) \n 
out_of_order = { } \n 
for sub in itervalues ( grp . _subsystems ) : \n 
~~~ visited . add ( sub . pathname ) \n 
for u , v in nx . dfs_edges ( graph , sub . pathname ) : \n 
~~~ if v in visited : \n 
~~~ out_of_order . setdefault ( nearest_child ( grp . pathname , v ) , \n 
set ( ) ) . add ( sub . pathname ) \n 
~~ ~~ ~~ if out_of_order : \n 
~~~ for name in out_of_order : \n 
~~~ out_of_order [ name ] = sorted ( [ \n 
nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n 
] ) \n 
for n , subs in iteritems ( out_of_order ) : \n 
~~ ooo . append ( ( grp . pathname , list ( iteritems ( out_of_order ) ) ) ) \n 
~~ ~~ return ( cycles , sorted ( ooo ) ) \n 
~~ def _check_gmres_under_mpi ( self , out_stream = sys . stdout ) : \n 
~~~ has_parallel = False \n 
~~~ if isinstance ( s , ParallelGroup ) : \n 
~~~ has_parallel = True \n 
~~ ~~ if has_parallel and isinstance ( self . root . ln_solver , ScipyGMRES ) : \n 
~~ ~~ ~~ def _check_ubcs ( self , out_stream = sys . stdout ) : \n 
~~~ ubcs = self . _get_ubc_vars ( self . root . connections ) \n 
if ubcs : \n 
~~ return ubcs \n 
~~ def _check_unmarked_pbos ( self , out_stream = sys . stdout ) : \n 
~~~ pbos = [ ] \n 
for comp in self . root . components ( recurse = True , include_self = True ) : \n 
~~~ if comp . _pbo_warns : \n 
~~~ pbos . append ( ( comp . pathname , comp . _pbo_warns ) ) \n 
~~ ~~ if pbos : \n 
for cname , pbo_warns in sorted ( pbos , key = lambda x : x [ 0 ] ) : \n 
~~~ for vname , val in pbo_warns : \n 
type ( val ) . __name__ ) , file = out_stream ) \n 
~~ ~~ ~~ return pbos \n 
~~ def _check_relevant_pbos ( self , out_stream = sys . stdout ) : \n 
if self . driver . __class__ is Driver or self . driver . supports [ ] is False or self . root . fd_options [ ] : \n 
~~~ return [ ] \n 
~~ vec = self . root . unknowns \n 
pbos = [ var for var in vec if vec . metadata ( var ) . get ( ) ] \n 
rels = set ( ) \n 
for key , rel in iteritems ( self . _probdata . relevance . relevant ) : \n 
~~~ rels . update ( rel ) \n 
~~ rel_pbos = rels . intersection ( pbos ) \n 
if rel_pbos : \n 
~~~ rel_conns = [ ] \n 
for src in rel_pbos : \n 
~~~ for tgt , src_tuple in iteritems ( self . root . connections ) : \n 
~~~ if src_tuple [ 0 ] == src and tgt in rels : \n 
~~~ rel_conns . append ( ( src , tgt ) ) \n 
~~ ~~ ~~ if rel_conns : \n 
for src , tgt in rel_conns : \n 
~~~ val = vec [ src ] \n 
~~ return list ( rel_pbos ) \n 
~~ def check_setup ( self , out_stream = sys . stdout ) : \n 
print ( "##############################################" , file = out_stream ) \n 
results [ ] = self . _check_no_recorders ( out_stream ) \n 
results [ ] = self . _check_mpi ( out_stream ) \n 
results [ ] = self . _check_dangling_params ( out_stream ) \n 
results [ ] = self . _check_mode ( out_stream ) \n 
results [ ] = self . _check_no_unknown_comps ( out_stream ) \n 
results [ ] = self . _check_no_connect_comps ( out_stream ) \n 
results [ ] , results [ ] = self . _check_graph ( out_stream ) \n 
results [ ] = self . _check_ubcs ( out_stream ) \n 
results [ ] = self . _check_gmres_under_mpi ( out_stream ) \n 
results [ ] = self . _check_unmarked_pbos ( out_stream ) \n 
results [ ] = self . _check_relevant_pbos ( out_stream ) \n 
for s in self . root . subsystems ( recurse = True , local = True , include_self = True ) : \n 
s . check_setup ( out_stream = stream ) \n 
content = stream . getvalue ( ) \n 
if content : \n 
~~~ print ( "%s:\\n%s\\n" % ( s . pathname , content ) , file = out_stream ) \n 
results [ "@%s" % s . pathname ] = content \n 
print ( "##############################################\\n" , file = out_stream ) \n 
~~ def pre_run_check ( self ) : \n 
if not self . root . fd_options . locked : \n 
raise RuntimeError ( msg ) \n 
~~ ~~ def run ( self ) : \n 
self . pre_run_check ( ) \n 
if self . root . is_active ( ) : \n 
~~~ self . driver . run ( self ) \n 
self . root . comm . barrier ( ) \n 
~~ ~~ ~~ def run_once ( self ) : \n 
root = self . root \n 
driver = self . driver \n 
if root . is_active ( ) : \n 
~~~ driver . run_once ( self ) \n 
with root . _dircontext : \n 
~~~ root . apply_nonlinear ( root . params , root . unknowns , root . resids , \n 
metadata = driver . metadata ) \n 
root . comm . barrier ( ) \n 
~~ ~~ ~~ def _mode ( self , mode , indep_list , unknown_list ) : \n 
self . _p_length = 0 \n 
self . _u_length = 0 \n 
uset = set ( ) \n 
for unames in unknown_list : \n 
~~~ if isinstance ( unames , tuple ) : \n 
~~~ uset . update ( unames ) \n 
~~~ uset . add ( unames ) \n 
~~ ~~ pset = set ( ) \n 
for pnames in indep_list : \n 
~~~ if isinstance ( pnames , tuple ) : \n 
~~~ pset . update ( pnames ) \n 
~~~ pset . add ( pnames ) \n 
~~ ~~ to_prom_name = self . root . _sysdata . to_prom_name \n 
for path , meta in chain ( iteritems ( self . root . _unknowns_dict ) , \n 
iteritems ( self . root . _params_dict ) ) : \n 
~~~ prom_name = to_prom_name [ path ] \n 
if prom_name in uset : \n 
~~~ self . _u_length += meta [ ] \n 
uset . remove ( prom_name ) \n 
~~ if prom_name in pset : \n 
~~~ self . _p_length += meta [ ] \n 
pset . remove ( prom_name ) \n 
~~ ~~ if uset : \n 
~~ if pset : \n 
~~ if self . _p_length > self . _u_length : \n 
~~~ self . _calculated_mode = \n 
~~ if mode == : \n 
~~~ mode = self . root . ln_solver . options [ ] \n 
if mode == : \n 
~~~ mode = self . _calculated_mode \n 
~~ ~~ return mode \n 
~~ def calc_gradient ( self , indep_list , unknown_list , mode = , \n 
return_format = , dv_scale = None , cn_scale = None , \n 
sparsity = None ) : \n 
if mode not in [ , , , ] : \n 
~~ if return_format not in [ , ] : \n 
~~ with self . root . _dircontext : \n 
~~~ if mode == or self . root . fd_options [ ] : \n 
~~~ return self . _calc_gradient_fd ( indep_list , unknown_list , \n 
return_format , dv_scale = dv_scale , \n 
cn_scale = cn_scale , sparsity = sparsity ) \n 
~~~ return self . _calc_gradient_ln_solver ( indep_list , unknown_list , \n 
return_format , mode , \n 
dv_scale = dv_scale , \n 
cn_scale = cn_scale , \n 
sparsity = sparsity ) \n 
~~ ~~ ~~ def _calc_gradient_fd ( self , indep_list , unknown_list , return_format , \n 
dv_scale = None , cn_scale = None , sparsity = None ) : \n 
unknowns = root . unknowns \n 
params = root . params \n 
to_prom_name = root . _sysdata . to_prom_name \n 
to_abs_pnames = root . _sysdata . to_abs_pnames \n 
to_abs_uname = root . _sysdata . to_abs_uname \n 
if dv_scale is None : \n 
~~~ dv_scale = { } \n 
~~ if cn_scale is None : \n 
~~~ cn_scale = { } \n 
~~ abs_params = [ ] \n 
fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n 
pass_unknowns = [ var for var in unknown_list if var in indep_list ] \n 
for name in indep_list : \n 
~~~ if name in unknowns : \n 
~~~ name = to_abs_uname [ name ] \n 
~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if name == src : \n 
~~~ name = tgt \n 
~~ ~~ abs_params . append ( name ) \n 
~~ Jfd = root . fd_jacobian ( params , unknowns , root . resids , total_derivs = True , \n 
fd_params = abs_params , fd_unknowns = fd_unknowns , \n 
pass_unknowns = pass_unknowns , \n 
poi_indices = self . _poi_indices , \n 
qoi_indices = self . _qoi_indices ) \n 
def get_fd_ikey ( ikey ) : \n 
~~~ if isinstance ( ikey , tuple ) : \n 
~~~ ikey = ikey [ 0 ] \n 
~~ fd_ikey = ikey \n 
if fd_ikey not in params : \n 
~~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if src == ikey : \n 
~~~ fd_ikey = tgt \n 
~~ ~~ if fd_ikey not in params : \n 
~~~ for key , meta in iteritems ( params ) : \n 
~~~ if to_prom_name [ key ] == fd_ikey : \n 
~~~ fd_ikey = meta [ ] \n 
~~ ~~ ~~ ~~ return fd_ikey \n 
~~ if return_format == : \n 
~~~ J = OrderedDict ( ) \n 
for okey in unknown_list : \n 
~~~ J [ okey ] = OrderedDict ( ) \n 
for j , ikey in enumerate ( indep_list ) : \n 
~~~ if sparsity is not None : \n 
~~~ if ikey not in sparsity [ okey ] : \n 
~~ ~~ abs_ikey = abs_params [ j ] \n 
fd_ikey = get_fd_ikey ( abs_ikey ) \n 
if ( okey , fd_ikey ) not in Jfd : \n 
~~~ fd_ikey = to_abs_pnames [ fd_ikey ] [ 0 ] \n 
~~ J [ okey ] [ ikey ] = Jfd [ ( okey , fd_ikey ) ] \n 
if ikey in dv_scale : \n 
~~~ J [ okey ] [ ikey ] *= dv_scale [ ikey ] \n 
~~ if okey in cn_scale : \n 
~~~ J [ okey ] [ ikey ] *= cn_scale [ okey ] \n 
~~ ~~ ~~ ~~ else : \n 
~~~ usize = 0 \n 
psize = 0 \n 
for u in unknown_list : \n 
~~~ if u in self . _qoi_indices : \n 
~~~ idx = self . _qoi_indices [ u ] \n 
usize += len ( idx ) \n 
~~~ usize += self . root . unknowns . metadata ( u ) [ ] \n 
~~ ~~ for p in indep_list : \n 
~~~ if p in self . _poi_indices : \n 
~~~ idx = self . _poi_indices [ p ] \n 
psize += len ( idx ) \n 
~~~ psize += self . root . unknowns . metadata ( p ) [ ] \n 
~~ ~~ J = np . zeros ( ( usize , psize ) ) \n 
ui = 0 \n 
~~~ pi = 0 \n 
for j , p in enumerate ( indep_list ) : \n 
~~~ abs_ikey = abs_params [ j ] \n 
if ( u , fd_ikey ) not in Jfd : \n 
~~ pd = Jfd [ u , fd_ikey ] \n 
rows , cols = pd . shape \n 
for row in range ( 0 , rows ) : \n 
~~~ for col in range ( 0 , cols ) : \n 
~~~ J [ ui + row ] [ pi + col ] = pd [ row ] [ col ] \n 
if p in dv_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= dv_scale [ p ] \n 
~~ if u in cn_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= cn_scale [ u ] \n 
~~ ~~ ~~ pi += cols \n 
~~ ui += rows \n 
~~ ~~ return J \n 
~~ def _calc_gradient_ln_solver ( self , indep_list , unknown_list , return_format , mode , \n 
relevance = root . _probdata . relevance \n 
unknowns_dict = root . _unknowns_dict \n 
comm = root . comm \n 
iproc = comm . rank \n 
nproc = comm . size \n 
owned = root . _owning_ranks \n 
~~ mode = self . _mode ( mode , indep_list , unknown_list ) \n 
fwd = mode == \n 
root . clear_dparams ( ) \n 
for names in root . _probdata . relevance . vars_of_interest ( mode ) : \n 
~~~ for name in names : \n 
~~~ if name in root . dumat : \n 
~~~ root . dumat [ name ] . vec [ : ] = 0.0 \n 
root . drmat [ name ] . vec [ : ] = 0.0 \n 
~~ ~~ ~~ root . dumat [ None ] . vec [ : ] = 0.0 \n 
root . drmat [ None ] . vec [ : ] = 0.0 \n 
root . _sys_linearize ( root . params , unknowns , root . resids ) \n 
if return_format == : \n 
for okeys in unknown_list : \n 
~~~ if isinstance ( okeys , str ) : \n 
~~~ okeys = ( okeys , ) \n 
~~ for okey in okeys : \n 
for ikeys in indep_list : \n 
~~~ if isinstance ( ikeys , str ) : \n 
~~~ ikeys = ( ikeys , ) \n 
~~ for ikey in ikeys : \n 
~~ ~~ J [ okey ] [ ikey ] = None \n 
~~ ~~ ~~ ~~ ~~ else : \n 
Jslices = OrderedDict ( ) \n 
~~~ start = usize \n 
if u in self . _qoi_indices : \n 
~~ Jslices [ u ] = slice ( start , usize ) \n 
~~ for p in indep_list : \n 
~~~ start = psize \n 
if p in self . _poi_indices : \n 
~~~ psize += unknowns . metadata ( p ) [ ] \n 
~~ Jslices [ p ] = slice ( start , psize ) \n 
~~ J = np . zeros ( ( usize , psize ) ) \n 
~~ if fwd : \n 
~~~ input_list , output_list = indep_list , unknown_list \n 
poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = dv_scale , cn_scale \n 
~~~ input_list , output_list = unknown_list , indep_list \n 
qoi_indices , poi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = cn_scale , dv_scale \n 
~~ all_vois = self . root . _probdata . relevance . vars_of_interest ( mode ) \n 
input_set = set ( ) \n 
for inp in input_list : \n 
~~~ if isinstance ( inp , str ) : \n 
~~~ input_set . add ( inp ) \n 
~~~ input_set . update ( inp ) \n 
~~ ~~ voi_sets = [ ] \n 
for voi_set in all_vois : \n 
~~~ for voi in voi_set : \n 
~~~ if voi in input_set : \n 
~~~ voi_sets . append ( voi_set ) \n 
~~ ~~ ~~ flat_voi = [ item for sublist in all_vois for item in sublist ] \n 
for items in input_list : \n 
~~~ if isinstance ( items , str ) : \n 
~~~ items = ( items , ) \n 
~~ for item in items : \n 
~~~ if item not in flat_voi : \n 
~~~ voi_sets . append ( ( item , ) ) \n 
~~ ~~ ~~ voi_srcs = { } \n 
for params in voi_sets : \n 
~~~ rhs = OrderedDict ( ) \n 
voi_idxs = { } \n 
old_size = None \n 
for voi in params : \n 
~~~ vkey = self . _get_voi_key ( voi , params ) \n 
duvec = self . root . dumat [ vkey ] \n 
rhs [ vkey ] = np . empty ( ( len ( duvec . vec ) , ) ) \n 
voi_srcs [ vkey ] = voi \n 
if voi in duvec : \n 
~~~ in_idxs = duvec . _get_local_idxs ( voi , poi_indices ) \n 
~~~ in_idxs = [ ] \n 
~~ if len ( in_idxs ) == 0 : \n 
~~~ if voi in poi_indices : \n 
~~~ in_idxs = duvec . to_idx_array ( poi_indices [ voi ] ) \n 
~~~ in_idxs = np . arange ( 0 , unknowns_dict [ to_abs_uname [ voi ] ] [ ] , dtype = int ) \n 
~~ ~~ if old_size is None : \n 
~~~ old_size = len ( in_idxs ) \n 
~~ elif old_size != len ( in_idxs ) : \n 
~~ voi_idxs [ vkey ] = in_idxs \n 
~~ for i in range ( len ( in_idxs ) ) : \n 
~~~ for voi in params : \n 
rhs [ vkey ] [ : ] = 0.0 \n 
if self . root . _owning_ranks [ voi_srcs [ vkey ] ] == iproc : \n 
~~~ rhs [ vkey ] [ voi_idxs [ vkey ] [ i ] ] = - 1.0 \n 
~~ ~~ dx_mat = root . ln_solver . solve ( rhs , root , mode ) \n 
for param , dx in iteritems ( dx_mat ) : \n 
~~~ vkey = self . _get_voi_key ( param , params ) \n 
if param is None : \n 
~~~ param = params [ 0 ] \n 
~~ for item in output_list : \n 
~~~ if fwd and param not in sparsity [ item ] : \n 
~~ elif not fwd and item not in sparsity [ param ] : \n 
~~ ~~ if relevance . is_relevant ( vkey , item ) : \n 
~~~ if fwd or owned [ item ] == iproc : \n 
~~~ out_idxs = self . root . dumat [ vkey ] . _get_local_idxs ( item , \n 
qoi_indices , \n 
get_slice = True ) \n 
dxval = dx [ out_idxs ] \n 
if dxval . size == 0 : \n 
~~~ dxval = None \n 
~~ if nproc > 1 : \n 
( dxval , owned [ item ] , param , item ) ) \n 
~~ dxval = comm . bcast ( dxval , root = owned [ item ] ) \n 
~~~ if item in qoi_indices : \n 
~~~ zsize = len ( qoi_indices [ item ] ) \n 
~~~ zsize = unknowns . metadata ( item ) [ ] \n 
~~ dxval = np . zeros ( zsize ) \n 
~~ if dxval is not None : \n 
~~~ nk = len ( dxval ) \n 
~~~ if fwd : \n 
~~~ if J [ item ] [ param ] is None : \n 
~~~ J [ item ] [ param ] = np . zeros ( ( nk , len ( in_idxs ) ) ) \n 
~~ J [ item ] [ param ] [ : , i ] = dxval \n 
if param in in_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= in_scale [ param ] \n 
~~ if item in un_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= un_scale [ item ] \n 
~~~ if J [ param ] [ item ] is None : \n 
~~~ J [ param ] [ item ] = np . zeros ( ( len ( in_idxs ) , nk ) ) \n 
~~ J [ param ] [ item ] [ i , : ] = dxval \n 
~~~ J [ param ] [ item ] [ i , : ] *= in_scale [ param ] \n 
~~~ J [ param ] [ item ] [ i , : ] *= un_scale [ item ] \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] = dxval \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= in_scale [ param ] \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= un_scale [ item ] \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] = dxval \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= in_scale [ param ] \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= un_scale [ item ] \n 
~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ root . clear_dparams ( ) \n 
return J \n 
~~ def _get_voi_key ( self , voi , grp ) : \n 
if ( voi in self . _driver_vois and \n 
isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n 
~~~ if ( len ( grp ) > 1 or \n 
self . root . ln_solver . options [ ] ) : \n 
~~~ return voi \n 
~~ ~~ return None \n 
~~ def check_partial_derivatives ( self , out_stream = sys . stdout , comps = None , \n 
compact_print = False ) : \n 
if self . driver . iter_count < 1 : \n 
~~~ out_stream . write ( ) \n 
self . run_once ( ) \n 
~~ root . _sys_linearize ( root . params , root . unknowns , root . resids ) \n 
if out_stream is not None : \n 
~~ data = { } \n 
voi = None \n 
allcomps = root . components ( recurse = True ) \n 
if comps is None : \n 
~~~ comps = allcomps \n 
~~~ allcompnames = set ( [ c . pathname for c in allcomps ] ) \n 
requested = set ( comps ) \n 
diff = requested . difference ( allcompnames ) \n 
if diff : \n 
~~~ sorted_diff = list ( diff ) \n 
sorted_diff . sort ( ) \n 
msg += str ( sorted_diff ) \n 
~~ comps = [ root . _subsystem ( c_name ) for c_name in comps ] \n 
~~ for comp in comps : \n 
~~~ cname = comp . pathname \n 
opt = comp . fd_options \n 
fwd_rev = True \n 
if opt [ ] : \n 
~~~ f_d_2 = True \n 
fd_desc = opt [ ] \n 
fd_desc2 = opt [ ] \n 
~~~ f_d_2 = False \n 
fd_desc = None \n 
fd_desc2 = None \n 
~~ if opt [ ] : \n 
~~~ if not f_d_2 : \n 
~~ fwd_rev = False \n 
~~ if isinstance ( comp , IndepVarComp ) : \n 
~~ data [ cname ] = { } \n 
jac_fwd = OrderedDict ( ) \n 
jac_rev = OrderedDict ( ) \n 
jac_fd = OrderedDict ( ) \n 
jac_fd2 = OrderedDict ( ) \n 
params = comp . params \n 
unknowns = comp . unknowns \n 
resids = comp . resids \n 
dparams = comp . dpmat [ voi ] \n 
dunknowns = comp . dumat [ voi ] \n 
dresids = comp . drmat [ voi ] \n 
states = comp . states \n 
if len ( dparams ) == 0 : \n 
~~ param_list = [ item for item in dparams if not dparams . metadata ( item ) . get ( ) ] \n 
param_list . extend ( states ) \n 
unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( ) ] \n 
~~~ out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
~~ for p_name in param_list : \n 
~~~ if not fwd_rev : \n 
~~ dinputs = dunknowns if p_name in states else dparams \n 
p_size = np . size ( dinputs [ p_name ] ) \n 
for u_name in unkn_list : \n 
~~~ u_size = np . size ( dunknowns [ u_name ] ) \n 
if comp . _jacobian_cache : \n 
~~~ if ( u_name , p_name ) in comp . _jacobian_cache : \n 
~~~ user = comp . _jacobian_cache [ ( u_name , p_name ) ] . shape \n 
if len ( user ) < 2 : \n 
~~~ user = ( user [ 0 ] , 1 ) \n 
~~ if user [ 0 ] != u_size or user [ 1 ] != p_size : \n 
msg = msg . format ( cname , u_name , p_name , ( u_size , p_size ) , user ) \n 
~~ ~~ ~~ jac_fwd [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
jac_rev [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
~~ ~~ if fwd_rev : \n 
~~~ for u_name in unkn_list : \n 
for idx in range ( u_size ) : \n 
~~~ dresids . vec [ : ] = 0.0 \n 
dunknowns . vec [ : ] = 0.0 \n 
dresids . _dat [ u_name ] . val [ idx ] = 1.0 \n 
~~~ comp . apply_linear ( params , unknowns , dparams , \n 
dunknowns , dresids , ) \n 
~~~ dparams . _apply_unit_derivatives ( ) \n 
~~~ dinputs = dunknowns if p_name in states else dparams \n 
jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n 
~~ ~~ ~~ ~~ if fwd_rev : \n 
~~~ for p_name in param_list : \n 
for idx in range ( p_size ) : \n 
dinputs . _dat [ p_name ] . val [ idx ] = 1.0 \n 
dparams . _apply_unit_derivatives ( ) \n 
comp . apply_linear ( params , unknowns , dparams , \n 
for u_name , u_val in dresids . vec_val_iter ( ) : \n 
~~~ jac_fwd [ ( u_name , p_name ) ] [ : , idx ] = u_val \n 
~~ ~~ ~~ ~~ dresids . vec [ : ] = 0.0 \n 
if opt [ ] == : \n 
~~~ fd_func = comp . complex_step_jacobian \n 
~~~ fd_func = comp . fd_jacobian \n 
~~ jac_fd = fd_func ( params , unknowns , resids ) \n 
if f_d_2 : \n 
~~ save_form = opt [ ] \n 
OptionsDictionary . locked = False \n 
opt [ ] = opt [ ] \n 
jac_fd2 = fd_func ( params , unknowns , resids ) \n 
opt [ ] = save_form \n 
OptionsDictionary . locked = True \n 
~~ _assemble_deriv_data ( chain ( dparams , states ) , resids , data [ cname ] , \n 
jac_fwd , jac_rev , jac_fd , out_stream , \n 
c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n 
fd_desc2 = fd_desc2 , compact_print = compact_print ) \n 
~~ return data \n 
~~ def check_total_derivatives ( self , out_stream = sys . stdout ) : \n 
if driver . iter_count < 1 : \n 
~~ if out_stream is not None : \n 
~~ if len ( driver . _desvars ) > 0 : \n 
~~~ param_srcs = list ( driver . _desvars . keys ( ) ) \n 
to_abs_name = root . _sysdata . to_abs_uname \n 
indep_list = [ p for p in param_srcs if not root . _unknowns_dict [ to_abs_name [ p ] ] . get ( ) ] \n 
~~~ abs_indep_list = root . _get_fd_params ( ) \n 
param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( ) ] \n 
indep_list = [ \n 
to_prom_name [ p ] for p , idxs in param_srcs \n 
~~ if len ( driver . _objs ) > 0 or len ( driver . _cons ) > 0 : \n 
~~~ unknown_list = list ( driver . _objs . keys ( ) ) \n 
unknown_list . extend ( list ( driver . _cons . keys ( ) ) ) \n 
unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n 
~~~ unknown_list = root . _get_fd_unknowns ( ) \n 
~~ if root . ln_solver . options . get ( ) : \n 
~~~ mode = self . _mode ( , indep_list , unknown_list ) \n 
~~~ fwd , rev = True , False \n 
Jrev = None \n 
out_stream . write ( ) \n 
~~~ fwd , rev = False , True \n 
Jfor = None \n 
~~~ fwd = rev = True \n 
~~~ Jfor = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
return_format = ) \n 
Jfor = _jac_to_flat_dict ( Jfor ) \n 
~~ if rev : \n 
~~~ Jrev = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
Jrev = _jac_to_flat_dict ( Jrev ) \n 
~~ Jfd = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
Jfd = _jac_to_flat_dict ( Jfd ) \n 
data = { } \n 
_assemble_deriv_data ( indep_list , unknown_list , data , \n 
Jfor , Jrev , Jfd , out_stream ) \n 
return data \n 
~~ def _start_recorders ( self ) : \n 
self . driver . recorders . startup ( self . root ) \n 
self . driver . recorders . record_metadata ( self . root ) \n 
~~~ for solver in ( group . nl_solver , group . ln_solver ) : \n 
~~~ solver . recorders . startup ( group ) \n 
solver . recorders . record_metadata ( self . root ) \n 
~~ ~~ ~~ def _check_for_parallel_derivs ( self , params , unknowns , par_u , par_p ) : \n 
mode = self . _mode ( , params , unknowns ) \n 
~~~ has_parallel_derivs = par_p \n 
~~~ has_parallel_derivs = par_u \n 
~~ if ( isinstance ( self . root . ln_solver , LinearGaussSeidel ) and \n 
self . root . ln_solver . options [ ] ) and has_parallel_derivs : \n 
~~~ for sub in self . root . subgroups ( recurse = True ) : \n 
~~~ sub_mode = sub . ln_solver . options [ ] \n 
if isinstance ( sub . ln_solver , LinearGaussSeidel ) and sub_mode not in ( mode , ) : \n 
msg = msg . format ( name = sub . name , submode = sub_mode , rootmode = mode ) \n 
~~ ~~ ~~ return mode \n 
~~ def _json_system_tree ( self ) : \n 
def _tree_dict ( system ) : \n 
~~~ dct = OrderedDict ( ) \n 
for s in system . subsystems ( recurse = True ) : \n 
~~~ if isinstance ( s , Group ) : \n 
~~~ dct [ s . name ] = _tree_dict ( s ) \n 
~~~ dct [ s . name ] = OrderedDict ( ) \n 
for vname , meta in iteritems ( s . unknowns ) : \n 
~~~ dct [ s . name ] [ vname ] = m = meta . copy ( ) \n 
for mname in m : \n 
~~~ if isinstance ( m [ mname ] , np . ndarray ) : \n 
~~~ m [ mname ] = m [ mname ] . tolist ( ) \n 
~~ ~~ ~~ ~~ ~~ return dct \n 
~~ tree = OrderedDict ( ) \n 
tree [ ] = _tree_dict ( self . root ) \n 
return json . dumps ( tree ) \n 
~~ def _setup_communicators ( self ) : \n 
~~~ if self . comm is None : \n 
~~~ self . comm = self . _impl . world_comm ( ) \n 
~~ minproc , maxproc = self . driver . get_req_procs ( ) \n 
~~~ if not ( maxproc is None or maxproc >= self . comm . size ) : \n 
( self . comm . size , minproc , maxproc ) ) \n 
~~ elif self . comm . size < minproc : \n 
~~~ if maxproc is None : \n 
~~~ maxproc = \n 
~~ ~~ self . driver . _setup_communicators ( self . comm , os . getcwd ( ) ) \n 
~~ def _setup_units ( self , connections , params_dict , unknowns_dict ) : \n 
for target , ( source , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ target ] \n 
smeta = unknowns_dict [ source ] \n 
if not in tmeta or not in smeta : \n 
~~ src_unit = smeta [ ] \n 
tgt_unit = tmeta [ ] \n 
~~~ scale , offset = get_conversion_tuple ( src_unit , tgt_unit ) \n 
~~ except TypeError as err : \n 
_both_names ( smeta , to_prom_name ) , \n 
tgt_unit , \n 
_both_names ( tmeta , to_prom_name ) ) \n 
~~~ raise \n 
~~ ~~ if scale != 1.0 or offset != 0.0 : \n 
~~~ tmeta [ ] = ( scale , offset ) \n 
~~ ~~ ~~ def _add_implicit_connections ( self , connections ) : \n 
dangling = set ( ) \n 
abs_unames = self . root . _sysdata . to_abs_uname \n 
for prom_name , pabs_list in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ for pabs in pabs_list : \n 
~~~ connections . setdefault ( pabs , [ ] ) . append ( ( abs_unames [ prom_name ] , None ) ) \n 
~~~ dangling . add ( prom_name ) \n 
~~ ~~ return dangling \n 
~~ def print_all_convergence ( self ) : \n 
root . ln_solver . print_all_convergence ( ) \n 
root . nl_solver . print_all_convergence ( ) \n 
for grp in root . subgroups ( recurse = True ) : \n 
~~~ grp . ln_solver . print_all_convergence ( ) \n 
grp . nl_solver . print_all_convergence ( ) \n 
~~ ~~ ~~ def _assign_parameters ( connections ) : \n 
param_owners = { } \n 
for par , ( unk , idxs ) in iteritems ( connections ) : \n 
~~~ param_owners . setdefault ( get_common_ancestor ( par , unk ) , set ( ) ) . add ( par ) \n 
~~ return param_owners \n 
~~ def _jac_to_flat_dict ( jac ) : \n 
new_jac = OrderedDict ( ) \n 
for key1 , val1 in iteritems ( jac ) : \n 
~~~ for key2 , val2 in iteritems ( val1 ) : \n 
~~~ new_jac [ ( key1 , key2 ) ] = val2 \n 
~~ ~~ return new_jac \n 
~~ def _pad_name ( name , pad_num = 13 , quotes = True ) : \n 
l_name = len ( name ) \n 
if l_name < pad_num : \n 
~~~ pad = pad_num - l_name \n 
if quotes : \n 
~~~ pad_str = "\'{name}\'{sep:<{pad}}" \n 
~~~ pad_str = "{name}{sep:<{pad}}" \n 
~~ pad_name = pad_str . format ( name = name , sep = , pad = pad ) \n 
return pad_name \n 
~~~ return . format ( name ) \n 
~~ ~~ def _assemble_deriv_data ( params , resids , cdata , jac_fwd , jac_rev , jac_fd , \n 
out_stream , c_name = , jac_fd2 = None , fd_desc = None , \n 
fd_desc2 = None , compact_print = False ) : \n 
started = False \n 
for p_name in params : \n 
~~~ for u_name in resids : \n 
~~~ key = ( u_name , p_name ) \n 
if key not in jac_fd : \n 
~~ ldata = cdata [ key ] = { } \n 
Jsub_fd = jac_fd [ key ] \n 
ldata [ ] = Jsub_fd \n 
magfd = np . linalg . norm ( Jsub_fd ) \n 
if jac_fwd : \n 
~~~ Jsub_for = jac_fwd [ key ] \n 
ldata [ ] = Jsub_for \n 
magfor = np . linalg . norm ( Jsub_for ) \n 
~~~ magfor = None \n 
~~ if jac_rev : \n 
~~~ Jsub_rev = jac_rev [ key ] \n 
ldata [ ] = Jsub_rev \n 
magrev = np . linalg . norm ( Jsub_rev ) \n 
~~~ magrev = None \n 
~~ if jac_fd2 : \n 
~~~ Jsub_fd2 = jac_fd2 [ key ] \n 
ldata [ ] = Jsub_fd2 \n 
magfd2 = np . linalg . norm ( Jsub_fd2 ) \n 
~~~ magfd2 = None \n 
~~ ldata [ ] = ( magfor , magrev , magfd ) \n 
~~~ abs1 = np . linalg . norm ( Jsub_for - Jsub_fd ) \n 
~~~ abs1 = None \n 
~~~ abs2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) \n 
~~~ abs2 = None \n 
~~ if jac_fwd and jac_rev : \n 
~~~ abs3 = np . linalg . norm ( Jsub_for - Jsub_rev ) \n 
~~~ abs3 = None \n 
~~~ abs4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) \n 
~~~ abs4 = None \n 
~~ ldata [ ] = ( abs1 , abs2 , abs3 ) \n 
if magfd == 0.0 : \n 
~~~ rel1 = rel2 = rel3 = rel4 = float ( ) \n 
~~~ if jac_fwd : \n 
~~~ rel1 = np . linalg . norm ( Jsub_for - Jsub_fd ) / magfd \n 
~~~ rel1 = None \n 
~~~ rel2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) / magfd \n 
~~~ rel2 = None \n 
~~~ rel3 = np . linalg . norm ( Jsub_for - Jsub_rev ) / magfd \n 
~~~ rel3 = None \n 
~~~ rel4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) / magfd \n 
~~~ rel4 = None \n 
~~ ~~ ldata [ ] = ( rel1 , rel2 , rel3 ) \n 
if out_stream is None : \n 
~~ if compact_print : \n 
~~~ if jac_fwd and jac_rev : \n 
~~~ if not started : \n 
out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) \n 
out_stream . write ( out_str ) \n 
out_stream . write ( * len ( out_str ) + ) \n 
started = True \n 
out_stream . write ( tmp1 . format ( _pad_name ( u_name ) , _pad_name ( p_name ) , \n 
magfor , magrev , magfd , abs1 , abs2 , \n 
rel1 , rel2 ) ) \n 
~~ elif jac_fd and jac_fd2 : \n 
_pad_name ( , 13 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) \n 
magfd , magfd2 , abs4 , rel4 ) ) \n 
~~~ if started : \n 
~~~ out_stream . write ( * 30 + ) \n 
~~~ started = True \n 
~~~ out_stream . write ( % magfor ) \n 
~~~ out_stream . write ( % magrev ) \n 
~~ if not jac_fwd and not jac_rev : \n 
~~ if jac_fd : \n 
~~~ out_stream . write ( % magfd ) \n 
if fd_desc : \n 
~~~ out_stream . write ( % fd_desc ) \n 
~~ out_stream . write ( ) \n 
~~~ out_stream . write ( % magfd2 ) \n 
if fd_desc2 : \n 
~~~ out_stream . write ( % fd_desc2 ) \n 
~~~ out_stream . write ( % abs1 ) \n 
~~~ out_stream . write ( % abs2 ) \n 
~~~ out_stream . write ( % abs3 ) \n 
~~~ out_stream . write ( % abs4 ) \n 
~~~ out_stream . write ( % rel1 ) \n 
~~~ out_stream . write ( % rel2 ) \n 
~~~ out_stream . write ( % rel3 ) \n 
~~~ out_stream . write ( % rel4 ) \n 
out_stream . write ( str ( Jsub_for ) ) \n 
out_stream . write ( str ( Jsub_rev ) ) \n 
out_stream . write ( str ( Jsub_fd ) ) \n 
if jac_fd2 : \n 
out_stream . write ( str ( Jsub_fd2 ) ) \n 
~~ ~~ ~~ ~~ ~~ def _needs_iteration ( comp ) : \n 
if isinstance ( comp , Component ) and comp . is_active ( ) and comp . states : \n 
~~~ for klass in comp . __class__ . __mro__ : \n 
~~~ if klass is Component : \n 
~~ if in klass . __dict__ : \n 
~~~ return False \n 
~~ ~~ return True \n 
~~ return False \n 
~~ def _get_gmres_name ( ) : \n 
~~~ if MPI : \n 
~~~ return \n 
from sqlitedict import SqliteDict \n 
from openmdao . recorders . base_recorder import BaseRecorder \n 
from openmdao . util . record_util import format_iteration_coordinate \n 
from openmdao . core . mpi_wrap import MPI \n 
class SqliteRecorder ( BaseRecorder ) : \n 
def __init__ ( self , out , ** sqlite_dict_args ) : \n 
~~~ super ( SqliteRecorder , self ) . __init__ ( ) \n 
if MPI and MPI . COMM_WORLD . rank > 0 : \n 
~~~ self . _open_close_sqlitedict = False \n 
~~~ self . _open_close_sqlitedict = True \n 
~~ if self . _open_close_sqlitedict : \n 
~~~ sqlite_dict_args . setdefault ( , True ) \n 
sqlite_dict_args . setdefault ( , ) \n 
self . out = SqliteDict ( filename = out , flag = , ** sqlite_dict_args ) \n 
~~~ self . out = None \n 
~~ ~~ def record_metadata ( self , group ) : \n 
params = group . params . iteritems ( ) \n 
resids = group . resids . iteritems ( ) \n 
unknowns = group . unknowns . iteritems ( ) \n 
data = OrderedDict ( [ ( , dict ( params ) ) , \n 
( , dict ( unknowns ) ) , \n 
self . out [ ] = data \n 
~~ def record_iteration ( self , params , unknowns , resids , metadata ) : \n 
data = OrderedDict ( ) \n 
iteration_coordinate = metadata [ ] \n 
timestamp = metadata [ ] \n 
group_name = format_iteration_coordinate ( iteration_coordinate ) \n 
data [ ] = timestamp \n 
data [ ] = metadata [ ] \n 
if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( params , , iteration_coordinate ) \n 
~~ if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( unknowns , , iteration_coordinate ) \n 
~~~ data [ ] = self . _filter_vector ( resids , , iteration_coordinate ) \n 
~~ self . out [ group_name ] = data \n 
~~ def record_derivatives ( self , derivs , metadata ) : \n 
group_name = % group_name \n 
data [ ] = derivs \n 
self . out [ group_name ] = data \n 
~~ def close ( self ) : \n 
if self . _open_close_sqlitedict : \n 
~~~ if self . out is not None : \n 
~~~ self . out . close ( ) \n 
self . out = None \n 
from numpy import atleast_2d as array2d \n 
from scipy import linalg \n 
from scipy . optimize import minimize \n 
from scipy . spatial . distance import squareform \n 
from openmdao . surrogate_models . surrogate_model import MultiFiSurrogateModel \n 
_logger = logging . getLogger ( ) \n 
THETA0_DEFAULT = 0.5 \n 
THETAL_DEFAULT = 1e-5 \n 
THETAU_DEFAULT = 50 \n 
if hasattr ( linalg , ) : \n 
~~~ solve_triangular = linalg . solve_triangular \n 
~~~ def solve_triangular ( x , y , lower = True ) : \n 
~~~ return linalg . solve ( x , y ) \n 
~~ ~~ def constant_regression ( x ) : \n 
x = np . asarray ( x , dtype = np . float ) \n 
n_eval = x . shape [ 0 ] \n 
f = np . ones ( [ n_eval , 1 ] ) \n 
~~ def linear_regression ( x ) : \n 
f = np . hstack ( [ np . ones ( [ n_eval , 1 ] ) , x ] ) \n 
~~ def squared_exponential_correlation ( theta , d ) : \n 
theta = np . asarray ( theta , dtype = np . float ) \n 
d = np . asarray ( d , dtype = np . float ) \n 
if d . ndim > 1 : \n 
~~~ n_features = d . shape [ 1 ] \n 
~~~ n_features = 1 \n 
~~ if theta . size == 1 : \n 
~~~ return np . exp ( - theta [ 0 ] * np . sum ( d ** 2 , axis = 1 ) ) \n 
~~ elif theta . size != n_features : \n 
~~~ return np . exp ( - np . sum ( theta . reshape ( 1 , n_features ) * d ** 2 , axis = 1 ) ) \n 
~~ ~~ def l1_cross_distances ( X , Y = None ) : \n 
if Y is None : \n 
~~~ X = array2d ( X ) \n 
n_samples , n_features = X . shape \n 
n_nonzero_cross_dist = n_samples * ( n_samples - 1 ) // 2 \n 
D = np . zeros ( ( n_nonzero_cross_dist , n_features ) ) \n 
ll_1 = 0 \n 
for k in range ( n_samples - 1 ) : \n 
~~~ ll_0 = ll_1 \n 
ll_1 = ll_0 + n_samples - k - 1 \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - X [ ( k + 1 ) : ] ) \n 
~~ return D \n 
Y = array2d ( Y ) \n 
n_samples_X , n_features_X = X . shape \n 
n_samples_Y , n_features_Y = Y . shape \n 
if n_features_X != n_features_Y : \n 
~~ n_features = n_features_X \n 
n_nonzero_cross_dist = n_samples_X * n_samples_Y \n 
for k in range ( n_samples_X ) : \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - Y ) \n 
~~ ~~ class MultiFiCoKriging ( object ) : \n 
_regression_types = { \n 
: constant_regression , \n 
: linear_regression } \n 
def __init__ ( self , regr = , rho_regr = , \n 
theta = None , theta0 = None , thetaL = None , thetaU = None ) : \n 
~~~ self . corr = squared_exponential_correlation \n 
self . regr = regr \n 
self . rho_regr = rho_regr \n 
self . theta = theta \n 
self . theta0 = theta0 \n 
self . thetaL = thetaL \n 
self . thetaU = thetaU \n 
self . _nfev = 0 \n 
~~ def _build_R ( self , lvl , theta ) : \n 
D = self . D [ lvl ] \n 
n_samples = self . n_samples [ lvl ] \n 
R = np . eye ( n_samples ) * ( 1. + NUGGET ) \n 
corr = squareform ( self . corr ( theta , D ) ) \n 
R = R + corr \n 
return R \n 
~~ def fit ( self , X , y , \n 
initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n 
self . _check_list_structure ( X , y ) \n 
self . _check_params ( ) \n 
X = self . X \n 
y = self . y \n 
nlevel = self . nlevel \n 
n_samples = self . n_samples \n 
self . beta = nlevel * [ 0 ] \n 
self . beta_rho = nlevel * [ None ] \n 
self . beta_regr = nlevel * [ None ] \n 
self . C = nlevel * [ 0 ] \n 
self . D = nlevel * [ 0 ] \n 
self . F = nlevel * [ 0 ] \n 
self . p = nlevel * [ 0 ] \n 
self . q = nlevel * [ 0 ] \n 
self . G = nlevel * [ 0 ] \n 
self . sigma2 = nlevel * [ 0 ] \n 
self . _R_adj = nlevel * [ None ] \n 
y_best = y [ nlevel - 1 ] \n 
for i in range ( nlevel - 1 ) [ : : - 1 ] : \n 
~~~ y_best = np . concatenate ( ( y [ i ] [ : - n_samples [ i + 1 ] ] , y_best ) ) \n 
~~ self . y_best = y_best \n 
self . y_mean = np . zeros ( 1 ) \n 
self . y_std = np . ones ( 1 ) \n 
self . X_mean = np . zeros ( 1 ) \n 
self . X_std = np . ones ( 1 ) \n 
for lvl in range ( nlevel ) : \n 
~~~ self . D [ lvl ] = l1_cross_distances ( X [ lvl ] ) \n 
if ( np . min ( np . sum ( self . D [ lvl ] , axis = 1 ) ) == 0. ) : \n 
~~ self . F [ lvl ] = self . regr ( X [ lvl ] ) \n 
self . p [ lvl ] = self . F [ lvl ] . shape [ 1 ] \n 
if lvl > 0 : \n 
~~~ F_rho = self . rho_regr ( X [ lvl ] ) \n 
self . q [ lvl ] = F_rho . shape [ 1 ] \n 
self . F [ lvl ] = np . hstack ( ( F_rho * np . dot ( ( self . y [ lvl - 1 ] ) [ - n_samples [ lvl ] : ] , \n 
np . ones ( ( 1 , self . q [ lvl ] ) ) ) , self . F [ lvl ] ) ) \n 
~~~ self . q [ lvl ] = 0 \n 
~~ n_samples_F_i = self . F [ lvl ] . shape [ 0 ] \n 
if n_samples_F_i != n_samples [ lvl ] : \n 
~~ if int ( self . p [ lvl ] + self . q [ lvl ] ) >= n_samples_F_i : \n 
% ( n_samples [ i ] , self . p [ lvl ] + self . q [ lvl ] ) ) \n 
~~ ~~ self . X = X \n 
self . y = y \n 
self . rlf_value = np . zeros ( nlevel ) \n 
~~~ if self . theta [ lvl ] is None : \n 
~~~ sol = self . _max_rlf ( lvl = lvl , initial_range = initial_range , tol = tol ) \n 
self . theta [ lvl ] = sol [ ] \n 
self . rlf_value [ lvl ] = sol [ ] \n 
if np . isinf ( self . rlf_value [ lvl ] ) : \n 
~~~ self . rlf_value [ lvl ] = self . rlf ( lvl = lvl ) \n 
~~ ~~ ~~ return \n 
~~ def rlf ( self , lvl , theta = None ) : \n 
if theta is None : \n 
~~~ theta = self . theta [ lvl ] \n 
~~ rlf_value = 1e20 \n 
y = self . y [ lvl ] \n 
F = self . F [ lvl ] \n 
p = self . p [ lvl ] \n 
q = self . q [ lvl ] \n 
R = self . _build_R ( lvl , theta ) \n 
~~~ C = linalg . cholesky ( R , lower = True ) \n 
~~ except linalg . LinAlgError : \n 
~~~ _logger . warning ( ( % lvl ) + \n 
+ str ( theta ) ) \n 
return rlf_value \n 
~~ Ft = solve_triangular ( C , F , lower = True ) \n 
Yt = solve_triangular ( C , y , lower = True ) \n 
~~~ Q , G = linalg . qr ( Ft , econ = True ) \n 
~~~ Q , G = linalg . qr ( Ft , mode = ) \n 
pass \n 
~~ beta = solve_triangular ( G , np . dot ( Q . T , Yt ) ) \n 
err = Yt - np . dot ( Ft , beta ) \n 
err2 = np . dot ( err . T , err ) [ 0 , 0 ] \n 
self . _err = err \n 
sigma2 = err2 / ( n_samples - p - q ) \n 
detR = ( ( np . diag ( C ) ) ** ( 2. / n_samples ) ) . prod ( ) \n 
rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n 
self . beta_rho [ lvl ] = beta [ : q ] \n 
self . beta_regr [ lvl ] = beta [ q : ] \n 
self . beta [ lvl ] = beta \n 
self . sigma2 [ lvl ] = sigma2 \n 
self . C [ lvl ] = C \n 
self . G [ lvl ] = G \n 
~~ def _max_rlf ( self , lvl , initial_range , tol ) : \n 
thetaL = self . thetaL [ lvl ] \n 
thetaU = self . thetaU [ lvl ] \n 
def rlf_transform ( x ) : \n 
~~~ return self . rlf ( theta = 10. ** x , lvl = lvl ) \n 
~~ theta0 = self . theta0 [ lvl ] \n 
x0 = np . log10 ( theta0 [ 0 ] ) \n 
constraints = [ ] \n 
for i in range ( theta0 . size ) : \n 
~~~ constraints . append ( { : , : lambda log10t , i = i : \n 
log10t [ i ] - np . log10 ( thetaL [ 0 ] [ i ] ) } ) \n 
constraints . append ( { : , : lambda log10t , i = i : \n 
np . log10 ( thetaU [ 0 ] [ i ] ) - log10t [ i ] } ) \n 
~~ constraints = tuple ( constraints ) \n 
sol = minimize ( rlf_transform , x0 , method = , \n 
constraints = constraints , \n 
options = { : initial_range , \n 
: tol , : 0 } ) \n 
log10_optimal_x = sol [ ] \n 
optimal_rlf_value = sol [ ] \n 
self . _nfev += sol [ ] \n 
optimal_theta = 10. ** log10_optimal_x \n 
res = { } \n 
res [ ] = optimal_theta \n 
res [ ] = optimal_rlf_value \n 
return res \n 
~~ def predict ( self , X , eval_MSE = True ) : \n 
X = array2d ( X ) \n 
n_eval , n_features_X = X . shape \n 
mu = np . zeros ( ( n_eval , nlevel ) ) \n 
f = self . regr ( X ) \n 
f0 = self . regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ 0 ] ) \n 
F = self . F [ 0 ] \n 
C = self . C [ 0 ] \n 
beta = self . beta [ 0 ] \n 
Ft = solve_triangular ( C , F , lower = True ) \n 
yt = solve_triangular ( C , self . y [ 0 ] , lower = True ) \n 
r_ = self . corr ( self . theta [ 0 ] , dx ) . reshape ( n_eval , self . n_samples [ 0 ] ) \n 
gamma = solve_triangular ( C . T , yt - np . dot ( Ft , beta ) , lower = False ) \n 
mu [ : , 0 ] = ( np . dot ( f , beta ) + np . dot ( r_ , gamma ) ) . ravel ( ) \n 
if eval_MSE : \n 
~~~ self . sigma2_rho = nlevel * [ None ] \n 
MSE = np . zeros ( ( n_eval , nlevel ) ) \n 
r_t = solve_triangular ( C , r_ . T , lower = True ) \n 
G = self . G [ 0 ] \n 
u_ = solve_triangular ( G . T , f . T - np . dot ( Ft . T , r_t ) , lower = True ) \n 
MSE [ : , 0 ] = self . sigma2 [ 0 ] * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) + ( u_ ** 2 ) . sum ( axis = 0 ) ) \n 
~~ for i in range ( 1 , nlevel ) : \n 
~~~ C = self . C [ i ] \n 
F = self . F [ i ] \n 
g = self . rho_regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n 
r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n 
f = np . vstack ( ( g . T * mu [ : , i - 1 ] , f0 . T ) ) \n 
yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n 
G = self . G [ i ] \n 
beta = self . beta [ i ] \n 
mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n 
~~~ Q_ = ( np . dot ( ( yt - np . dot ( Ft , beta ) ) . T , yt - np . dot ( Ft , beta ) ) ) [ 0 , 0 ] \n 
u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n 
sigma2_rho = np . dot ( g , self . sigma2 [ i ] * linalg . inv ( np . dot ( G . T , G ) ) [ : self . q [ i ] , : self . q [ i ] ] + np . dot ( beta [ : self . q [ i ] ] , beta [ : self . q [ i ] ] . T ) ) \n 
sigma2_rho = ( sigma2_rho * g ) . sum ( axis = 1 ) \n 
MSE [ : , i ] = sigma2_rho * MSE [ : , i - 1 ] + Q_ / ( 2 * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) ) + self . sigma2 [ i ] * ( u_ ** 2 ) . sum ( axis = 0 ) \n 
~~~ mu [ : , i ] = self . y_mean + self . y_std * mu [ : , i ] \n 
~~~ MSE [ : , i ] = self . y_std ** 2 * MSE [ : , i ] \n 
~~ ~~ if eval_MSE : \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) , MSE [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
~~ ~~ def _check_list_structure ( self , X , y ) : \n 
~~~ if type ( X ) is not list : \n 
~~~ nlevel = 1 \n 
X = [ X ] \n 
~~~ nlevel = len ( X ) \n 
~~ if type ( y ) is not list : \n 
~~~ y = [ y ] \n 
~~ if len ( X ) != len ( y ) : \n 
~~ n_samples = np . zeros ( nlevel , dtype = int ) \n 
n_features = np . zeros ( nlevel , dtype = int ) \n 
n_samples_y = np . zeros ( nlevel , dtype = int ) \n 
for i in range ( nlevel ) : \n 
~~~ n_samples [ i ] , n_features [ i ] = X [ i ] . shape \n 
if i > 1 and n_features [ i ] != n_features [ i - 1 ] : \n 
~~ y [ i ] = np . asarray ( y [ i ] ) . ravel ( ) [ : , np . newaxis ] \n 
n_samples_y [ i ] = y [ i ] . shape [ 0 ] \n 
if n_samples [ i ] != n_samples_y [ i ] : \n 
~~ ~~ self . n_features = n_features [ 0 ] \n 
if type ( self . theta ) is not list : \n 
~~~ self . theta = nlevel * [ self . theta ] \n 
~~ elif len ( self . theta ) != nlevel : \n 
~~ if type ( self . theta0 ) is not list : \n 
~~~ self . theta0 = nlevel * [ self . theta0 ] \n 
~~ elif len ( self . theta0 ) != nlevel : \n 
~~ if type ( self . thetaL ) is not list : \n 
~~~ self . thetaL = nlevel * [ self . thetaL ] \n 
~~ elif len ( self . thetaL ) != nlevel : \n 
~~ if type ( self . thetaU ) is not list : \n 
~~~ self . thetaU = nlevel * [ self . thetaU ] \n 
~~ elif len ( self . thetaU ) != nlevel : \n 
~~ self . nlevel = nlevel \n 
self . X = X [ : ] \n 
self . y = y [ : ] \n 
self . n_samples = n_samples \n 
~~ def _check_params ( self ) : \n 
~~~ if not callable ( self . regr ) : \n 
~~~ if self . regr in self . _regression_types : \n 
~~~ self . regr = self . _regression_types [ self . regr ] \n 
% ( self . _regression_types . keys ( ) , self . regr ) ) \n 
~~ ~~ if not callable ( self . rho_regr ) : \n 
~~~ if self . rho_regr in self . _regression_types : \n 
~~~ self . rho_regr = self . _regression_types [ self . rho_regr ] \n 
% ( self . _regression_types . keys ( ) , self . rho_regr ) ) \n 
~~ ~~ for i in range ( self . nlevel ) : \n 
~~~ if self . theta [ i ] is not None : \n 
~~~ self . theta [ i ] = array2d ( self . theta [ i ] ) \n 
if np . any ( self . theta [ i ] <= 0 ) : \n 
~~ ~~ if self . theta0 [ i ] is not None : \n 
~~~ self . theta0 [ i ] = array2d ( self . theta0 [ i ] ) \n 
if np . any ( self . theta0 [ i ] <= 0 ) : \n 
~~~ self . theta0 [ i ] = array2d ( self . n_features * [ THETA0_DEFAULT ] ) \n 
~~ lth = self . theta0 [ i ] . size \n 
if self . thetaL [ i ] is not None : \n 
~~~ self . thetaL [ i ] = array2d ( self . thetaL [ i ] ) \n 
if self . thetaL [ i ] . size != lth : \n 
~~~ self . thetaL [ i ] = array2d ( self . n_features * [ THETAL_DEFAULT ] ) \n 
~~ if self . thetaU [ i ] is not None : \n 
~~~ self . thetaU [ i ] = array2d ( self . thetaU [ i ] ) \n 
if self . thetaU [ i ] . size != lth : \n 
~~~ self . thetaU [ i ] = array2d ( self . n_features * [ THETAU_DEFAULT ] ) \n 
~~ if np . any ( self . thetaL [ i ] <= 0 ) or np . any ( self . thetaU [ i ] < self . thetaL [ i ] ) : \n 
"thetaU." ) \n 
~~ ~~ return \n 
~~ ~~ class MultiFiCoKrigingSurrogate ( MultiFiSurrogateModel ) : \n 
theta = None , theta0 = None , thetaL = None , thetaU = None , \n 
tolerance = TOLERANCE_DEFAULT , initial_range = INITIAL_RANGE_DEFAULT ) : \n 
~~~ super ( MultiFiCoKrigingSurrogate , self ) . __init__ ( ) \n 
self . tolerance = tolerance \n 
self . initial_range = initial_range \n 
self . model = MultiFiCoKriging ( regr = regr , rho_regr = rho_regr , theta = theta , \n 
theta0 = theta0 , thetaL = thetaL , thetaU = thetaU ) \n 
~~ def predict ( self , new_x ) : \n 
Y_pred , MSE = self . model . predict ( [ new_x ] ) \n 
return Y_pred , np . sqrt ( np . abs ( MSE ) ) \n 
~~ def train_multifi ( self , X , Y ) : \n 
X , Y = self . _fit_adapter ( X , Y ) \n 
self . model . fit ( X , Y , tol = self . tolerance , initial_range = self . initial_range ) \n 
~~ def _fit_adapter ( self , X , Y ) : \n 
~~~ if len ( np . shape ( np . array ( X [ 0 ] ) ) ) == 1 : \n 
~~~ X = [ X ] \n 
Y = [ Y ] \n 
~~ X = [ np . array ( x ) for x in reversed ( X ) ] \n 
Y = [ np . array ( y ) for y in reversed ( Y ) ] \n 
return ( X , Y ) \n 
~~ ~~ class FloatMultiFiCoKrigingSurrogate ( MultiFiCoKrigingSurrogate ) : \n 
def predict ( self , new_x ) : \n 
~~~ dist = super ( FloatMultiFiCoKrigingSurrogate , self ) . predict ( new_x ) \n 
return dist . mu \n 
~~~ import doctest \n 
doctest . testmod ( ) \n 
from six . moves import range \n 
from pyparsing import CaselessLiteral , Combine , OneOrMore , Optional , TokenConverter , Word , nums , oneOf , printables , ParserElement , alphanums \n 
__all__ = [ , ] \n 
def _getformat ( val ) : \n 
~~~ if int ( val ) == val : \n 
~~~ return "%.1f" \n 
~~~ return "%.16g" \n 
~~ ~~ class _SubHelper ( object ) : \n 
~~~ self . newtext = "" \n 
self . replace_location = 0 \n 
self . current_location = 0 \n 
self . counter = 0 \n 
self . start_location = 0 \n 
self . end_location = 0 \n 
~~ def set ( self , newtext , location ) : \n 
self . newtext = newtext \n 
self . replace_location = location \n 
~~ def set_array ( self , newtext , start_location , end_location ) : \n 
self . start_location = start_location \n 
self . end_location = end_location \n 
~~ def replace ( self , text ) : \n 
self . current_location += 1 \n 
if self . current_location == self . replace_location : \n 
~~~ if isinstance ( self . newtext , float ) : \n 
~~~ return _getformat ( self . newtext ) % self . newtext \n 
~~~ return str ( self . newtext ) \n 
~~~ return text . group ( ) \n 
~~ ~~ def replace_array ( self , text ) : \n 
end = len ( self . newtext ) \n 
if self . current_location >= self . start_location and self . current_location <= self . end_location and self . counter < end : \n 
~~~ if isinstance ( self . newtext [ self . counter ] , float ) : \n 
~~~ val = self . newtext [ self . counter ] \n 
newval = _getformat ( val ) % val \n 
~~~ newval = str ( self . newtext [ self . counter ] ) \n 
~~ self . counter += 1 \n 
return newval \n 
~~ ~~ ~~ class ToInteger ( TokenConverter ) : \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
return int ( tokenlist [ 0 ] ) \n 
~~ ~~ class ToFloat ( TokenConverter ) : \n 
return float ( tokenlist [ 0 ] . replace ( , ) ) \n 
~~ ~~ class ToNan ( TokenConverter ) : \n 
return float ( ) \n 
~~ ~~ class ToInf ( TokenConverter ) : \n 
~~ ~~ class InputFileGenerator ( object ) : \n 
~~~ self . template_filename = [ ] \n 
self . output_filename = [ ] \n 
self . reg = re . compile ( ) \n 
self . data = [ ] \n 
self . current_row = 0 \n 
self . anchored = False \n 
~~ def set_template_file ( self , filename ) : \n 
self . template_filename = filename \n 
templatefile = open ( filename , ) \n 
self . data = templatefile . readlines ( ) \n 
templatefile . close ( ) \n 
~~ def set_generated_file ( self , filename ) : \n 
self . output_filename = filename \n 
~~ def set_delimiters ( self , delimiter ) : \n 
self . delimiter = delimiter \n 
self . reg = re . compile ( + delimiter + ) \n 
~~ def mark_anchor ( self , anchor , occurrence = 1 ) : \n 
if not isinstance ( occurrence , int ) : \n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ count = 0 \n 
max_lines = len ( self . data ) \n 
for index in range ( self . current_row , max_lines ) : \n 
~~~ line = self . data [ index ] \n 
if count == 0 and self . anchored : \n 
~~~ line = line . split ( anchor ) [ - 1 ] \n 
~~ if line . find ( anchor ) > - 1 : \n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ self . current_row += count \n 
self . anchored = True \n 
~~ ~~ count += 1 \n 
~~ ~~ elif occurrence < 0 : \n 
~~~ max_lines = len ( self . data ) - 1 \n 
count = max_lines \n 
for index in range ( max_lines , - 1 , - 1 ) : \n 
if count == max_lines and self . anchored : \n 
~~~ line = line . split ( anchor ) [ 0 ] \n 
~~~ instance += - 1 \n 
~~~ self . current_row = count \n 
~~ ~~ count -= 1 \n 
~~ def reset_anchor ( self ) : \n 
~~ def transfer_var ( self , value , row , field ) : \n 
j = self . current_row + row \n 
line = self . data [ j ] \n 
sub = _SubHelper ( ) \n 
sub . set ( value , field ) \n 
newline = re . sub ( self . reg , sub . replace , line ) \n 
self . data [ j ] = newline \n 
~~ def transfer_array ( self , value , row_start , field_start , field_end , \n 
if row_end is None : \n 
~~~ row_end = row_start \n 
~~ sub = _SubHelper ( ) \n 
for row in range ( row_start , row_end + 1 ) : \n 
~~~ j = self . current_row + row \n 
if row == row_end : \n 
~~~ f_end = field_end \n 
~~~ f_end = 99999 \n 
~~ sub . set_array ( value , field_start , f_end ) \n 
field_start = 0 \n 
newline = re . sub ( self . reg , sub . replace_array , line ) \n 
~~ if sub . counter < len ( value ) : \n 
~~~ for val in value [ sub . counter : ] : \n 
~~~ newline = newline . rstrip ( ) + sep + str ( val ) \n 
~~ self . data [ j ] = newline \n 
~~ elif sub . counter > len ( value ) : \n 
~~ self . data [ j ] += "\\n" \n 
~~ def transfer_2Darray ( self , value , row_start , row_end , field_start , \n 
field_end ) : \n 
sub . set_array ( value [ i , : ] , field_start , field_end ) \n 
sub . current_location = 0 \n 
sub . counter = 0 \n 
~~ ~~ def clearline ( self , row ) : \n 
self . data [ self . current_row + row ] = "\\n" \n 
~~ def generate ( self ) : \n 
infile = open ( self . output_filename , ) \n 
infile . writelines ( self . data ) \n 
infile . close ( ) \n 
~~ ~~ class FileParser ( object ) : \n 
def __init__ ( self , end_of_line_comment_char = None , full_line_comment_char = None ) : \n 
~~~ self . filename = [ ] \n 
self . end_of_line_comment_char = end_of_line_comment_char \n 
self . full_line_comment_char = full_line_comment_char \n 
self . set_delimiters ( self . delimiter ) \n 
~~ def set_file ( self , filename ) : \n 
inputfile = open ( filename , ) \n 
if not self . end_of_line_comment_char and not self . full_line_comment_char : \n 
~~~ self . data = inputfile . readlines ( ) \n 
~~~ self . data = [ ] \n 
for line in inputfile : \n 
~~~ if line [ 0 ] == self . full_line_comment_char : \n 
~~ self . data . append ( line . split ( self . end_of_line_comment_char ) [ 0 ] ) \n 
~~ ~~ inputfile . close ( ) \n 
if delimiter != "columns" : \n 
~~~ ParserElement . setDefaultWhitespaceChars ( str ( delimiter ) ) \n 
~~ self . _reset_tokens ( ) \n 
~~ if anchor in line : \n 
~~ def transfer_line ( self , row ) : \n 
return self . data [ self . current_row + row ] . rstrip ( ) \n 
~~ def transfer_var ( self , row , field , fieldend = None ) : \n 
if self . delimiter == "columns" : \n 
~~~ if not fieldend : \n 
~~~ line = line [ ( field - 1 ) : ] \n 
~~~ line = line [ ( field - 1 ) : ( fieldend ) ] \n 
~~ data = self . _parse_line ( ) . parseString ( line ) \n 
if len ( data ) > 1 : \n 
~~~ return line \n 
~~~ return data [ 0 ] \n 
~~~ data = self . _parse_line ( ) . parseString ( line ) \n 
return data [ field - 1 ] \n 
~~ ~~ def transfer_keyvar ( self , key , field , occurrence = 1 , rowoffset = 0 ) : \n 
if not isinstance ( occurrence , int ) or occurrence == 0 : \n 
~~~ row = 0 \n 
for line in self . data [ self . current_row : ] : \n 
~~~ if line . find ( key ) > - 1 : \n 
~~ ~~ row += 1 \n 
~~~ row = - 1 \n 
for line in reversed ( self . data [ self . current_row : ] ) : \n 
~~ ~~ row -= 1 \n 
~~ ~~ j = self . current_row + row + rowoffset \n 
fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n 
return fields [ field ] \n 
~~ def transfer_array ( self , rowstart , fieldstart , rowend = None , fieldend = None ) : \n 
j1 = self . current_row + rowstart \n 
if rowend is None : \n 
~~~ j2 = j1 + 1 \n 
~~~ j2 = self . current_row + rowend + 1 \n 
~~ if not fieldend : \n 
~~ lines = self . data [ j1 : j2 ] \n 
data = np . zeros ( shape = ( 0 , 0 ) ) \n 
for i , line in enumerate ( lines ) : \n 
~~~ if self . delimiter == "columns" : \n 
~~~ line = line [ ( fieldstart - 1 ) : fieldend ] \n 
line = line . strip ( ) \n 
parsed = self . _parse_line ( ) . parseString ( line ) \n 
newdata = np . array ( parsed [ : ] ) \n 
if newdata . dtype . type is np . str_ : \n 
~~~ newdata = np . array ( line ) \n 
~~ data = np . append ( data , newdata ) \n 
~~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
if i == j2 - j1 - 1 : \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) ) \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : ] ) ) \n 
~~ fieldstart = 1 \n 
~~ ~~ return data \n 
~~ def transfer_2Darray ( self , rowstart , fieldstart , rowend , fieldend = None ) : \n 
if fieldend and ( fieldstart > fieldend ) : \n 
~~ if rowstart > rowend : \n 
~~ j1 = self . current_row + rowstart \n 
j2 = self . current_row + rowend + 1 \n 
lines = list ( self . data [ j1 : j2 ] ) \n 
~~~ if fieldend : \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : fieldend ] \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : ] \n 
~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
row = np . array ( parsed [ : ] ) \n 
data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
data [ 0 , : ] = row \n 
for i , line in enumerate ( list ( lines [ 1 : ] ) ) : \n 
~~~ line = line [ ( fieldstart - 1 ) : ] \n 
data [ i + 1 , : ] = np . array ( parsed [ : ] ) \n 
~~~ parsed = self . _parse_line ( ) . parseString ( lines [ 0 ] ) \n 
if fieldend : \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ print ( data ) \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ ~~ ~~ return data \n 
~~ def _parse_line ( self ) : \n 
return self . line_parse_token \n 
~~ def _reset_tokens ( self ) : \n 
if self . delimiter . isspace ( ) : \n 
~~~ textchars = printables \n 
~~~ textchars = alphanums \n 
symbols = [ , , , , , , , , , , \n 
, , , , , , , , , , \n 
, , , , , , ] \n 
for symbol in symbols : \n 
~~~ if symbol not in self . delimiter : \n 
~~~ textchars = textchars + symbol \n 
~~ ~~ ~~ digits = Word ( nums ) \n 
dot = "." \n 
ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n 
num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n 
num_float = ToFloat ( Combine ( Optional ( sign ) + \n 
( ( digits + dot + Optional ( digits ) ) | \n 
( dot + digits ) ) + \n 
Optional ( ee + Optional ( sign ) + digits ) \n 
) ) \n 
mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n 
string_text = Word ( textchars ) \n 
self . line_parse_token = ( OneOrMore ( ( nan | num_float | mixed_exp | num_int | \n 
string_text ) ) ) \n 
from OpenPNM . Geometry import models as gm \n 
from OpenPNM . Geometry import GenericGeometry \n 
class SGL10 ( GenericGeometry ) : \n 
def __init__ ( self , ** kwargs ) : \n 
~~~ super ( ) . __init__ ( ** kwargs ) \n 
self . _generate ( ) \n 
~~ def _generate ( self ) : \n 
~~~ self . models . add ( propname = , \n 
model = gm . pore_misc . random , \n 
num_range = [ 0 , 0.8834 ] , \n 
regen_mode = ) \n 
self . models . add ( propname = , \n 
model = gm . throat_misc . neighbor , \n 
pore_prop = , \n 
mode = ) \n 
model = gm . pore_diameter . sphere , \n 
psd_name = , \n 
psd_shape = 3.07 , \n 
psd_loc = 1.97e-6 , \n 
psd_scale = 1.6e-5 , \n 
psd_offset = 18e-6 ) \n 
model = gm . pore_area . spherical ) \n 
model = gm . pore_volume . sphere ) \n 
model = gm . throat_diameter . cylinder , \n 
tsd_name = , \n 
tsd_shape = 3.07 , \n 
tsd_loc = 1.97e-6 , \n 
tsd_scale = 1.6e-5 , \n 
tsd_offset = 18e-6 ) \n 
model = gm . throat_length . straight ) \n 
model = gm . throat_volume . cylinder ) \n 
model = gm . throat_area . cylinder ) \n 
model = gm . throat_surface_area . cylinder ) \n 
import scipy as _sp \n 
def pore_to_pore ( geometry , ** kwargs ) : \n 
network = geometry . _net \n 
throats = network . throats ( geometry . name ) \n 
pores = network . find_connected_pores ( throats , flatten = False ) \n 
C0 = network [ ] [ pores , 0 ] \n 
C1 = network [ ] [ pores , 1 ] \n 
V = C1 - C0 \n 
L = _sp . array ( _sp . sqrt ( _sp . sum ( V [ : , : ] ** 2 , axis = 1 ) ) , ndmin = 1 ) \n 
value = V / _sp . array ( L , ndmin = 2 ) . T \n 
return value \n 
def standard ( phase , \n 
pore_MW = , \n 
pore_density = , \n 
** kwargs ) : \n 
MW = phase [ pore_MW ] \n 
rho = phase [ pore_density ] \n 
value = rho / MW \n 
~~ def ideal_gas ( phase , \n 
pore_pressure = , \n 
pore_temperature = , \n 
R = 8.31447 \n 
P = phase [ pore_pressure ] \n 
T = phase [ pore_temperature ] \n 
value = P / ( R * T ) \n 
~~ def vanderwaals ( phase , \n 
pore_P = , \n 
pore_T = , \n 
pore_Pc = , \n 
pore_Tc = , \n 
P = phase [ pore_P ] / 100000 \n 
T = phase [ pore_T ] \n 
Tc = phase [ pore_Tc ] \n 
R = 83.1447 \n 
a = 27 * ( R ** 2 ) * ( Tc ** 2 ) / ( 64 * Pc ) \n 
b = R * Tc / ( 8 * Pc ) \n 
a1 = - 1 / b \n 
a2 = ( R * T + b * P ) / ( a * b ) \n 
a3 = - P / ( a * b ) \n 
a0 = sp . ones ( sp . shape ( a1 ) ) \n 
coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n 
density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n 
~~ import os \n 
from distutils . util import convert_path \n 
~~~ from setuptools import setup \n 
~~~ from distutils . core import setup \n 
~~ sys . path . append ( os . getcwd ( ) ) \n 
main_ = { } \n 
ver_path = convert_path ( ) \n 
with open ( ver_path ) as f : \n 
~~~ for line in f : \n 
~~~ if line . startswith ( ) : \n 
~~~ exec ( line , main_ ) \n 
~~ ~~ ~~ setup ( \n 
name = , \n 
description = version = main_ [ ] , \n 
classifiers = [ \n 
] , \n 
packages = [ \n 
install_requires = [ \n 
author = , \n 
author_email = , \n 
download_url = , \n 
url = \n 
class PoreCentroidTest : \n 
~~~ def test_voronoi ( self ) : \n 
~~ ~~ import pytest \n 
import OpenPNM \n 
class GenericPhaseTest : \n 
~~~ def setup_class ( self ) : \n 
~~~ self . net = OpenPNM . Network . Cubic ( shape = [ 5 , 5 , 5 ] ) \n 
~~ def test_init_w_no_network ( self ) : \n 
~~~ OpenPNM . Phases . GenericPhase ( ) \n 
~~ def test_init_w_components ( self ) : \n 
~~~ comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
OpenPNM . Phases . GenericPhase ( network = self . net , \n 
components = [ comp1 , comp2 ] ) \n 
~~ def test_set_component_add ( self ) : \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase . set_component ( comp1 ) \n 
phase . set_component ( comp2 ) \n 
~~ def test_set_component_add_twice ( self ) : \n 
with pytest . raises ( Exception ) : \n 
~~~ phase . set_components ( comp1 ) \n 
~~ ~~ def test_set_component_remove ( self ) : \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net , \n 
phase . set_component ( comp1 , mode = ) \n 
phase . set_component ( comp2 , mode = ) \n 
~~ def test_set_component_remove_twice ( self ) : \n 
~~~ phase . set_component ( comp1 , mode = ) \n 
~~ ~~ ~~ from __future__ import print_function \n 
from time import time \n 
MROW = 1000 * 1000. \n 
COLDCACHE = 5 \n 
WARMCACHE = 5 \n 
rdm_cod = [ , ] \n 
def get_nrows ( nrows_str ) : \n 
~~~ if nrows_str . endswith ( "k" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 ) \n 
~~ elif nrows_str . endswith ( "m" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 ) \n 
~~ elif nrows_str . endswith ( "g" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 * 1000 ) \n 
~~~ raise ValueError ( \n 
~~ ~~ class DB ( object ) : \n 
~~~ def __init__ ( self , nrows , rng , userandom ) : \n 
~~~ global step , scale \n 
self . step = STEP \n 
self . scale = SCALE \n 
self . rng = rng \n 
self . userandom = userandom \n 
self . filename = . join ( [ rdm_cod [ userandom ] , nrows ] ) \n 
self . nrows = get_nrows ( nrows ) \n 
~~ def get_db_size ( self ) : \n 
stdout = subprocess . PIPE ) . stdout \n 
line = [ l for l in sout ] [ 0 ] \n 
return int ( line . split ( ) [ 0 ] ) \n 
~~ def print_mtime ( self , t1 , explain ) : \n 
~~~ mtime = time ( ) - t1 \n 
print ( "%s:" % explain , round ( mtime , 6 ) ) \n 
print ( "Krows/s:" , round ( ( self . nrows / 1000. ) / mtime , 6 ) ) \n 
~~ def print_qtime ( self , colname , ltimes ) : \n 
print ( "Mrows/s:" , round ( ( self . nrows / ( MROW ) ) / qtime1 , 6 ) ) \n 
~~ def norm_times ( self , ltimes ) : \n 
lmean = ltimes . mean ( ) \n 
lstd = ltimes . std ( ) \n 
ntimes = ltimes [ ltimes < lmean + lstd ] \n 
nmean = ntimes . mean ( ) \n 
nstd = ntimes . std ( ) \n 
return nmean , nstd \n 
~~ def print_qtime_idx ( self , colname , ltimes , repeated , verbose ) : \n 
~~~ if repeated : \n 
~~ ltimes = numpy . array ( ltimes ) \n 
ntimes = len ( ltimes ) \n 
ctimes = ltimes [ 1 : COLDCACHE ] \n 
cmean , cstd = self . norm_times ( ctimes ) \n 
wtimes = ltimes [ WARMCACHE : ] \n 
wmean , wstd = self . norm_times ( wtimes ) \n 
numpy . histogram ( wtimes ) ) \n 
round ( qtime1 , prec ) ) \n 
round ( cmean , prec ) , "+-" , round ( cstd , prec ) ) \n 
round ( wmean , prec ) , "+-" , round ( wstd , prec ) ) \n 
~~ def print_db_sizes ( self , init , filled , indexed ) : \n 
~~~ table_size = ( filled - init ) / 1024. \n 
indexes_size = ( indexed - filled ) / 1024. \n 
~~ def fill_arrays ( self , start , stop ) : \n 
~~~ arr_f8 = numpy . arange ( start , stop , dtype = ) \n 
arr_i4 = numpy . arange ( start , stop , dtype = ) \n 
if self . userandom : \n 
~~~ arr_f8 += numpy . random . normal ( 0 , stop * self . scale , \n 
size = stop - start ) \n 
arr_i4 = numpy . array ( arr_f8 , dtype = ) \n 
~~ return arr_i4 , arr_f8 \n 
~~ def create_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ self . con = self . open_db ( remove = 1 ) \n 
self . create_table ( self . con ) \n 
init_size = self . get_db_size ( ) \n 
t1 = time ( ) \n 
self . fill_table ( self . con ) \n 
table_size = self . get_db_size ( ) \n 
self . print_mtime ( t1 , ) \n 
self . index_db ( dtype , kind , optlevel , verbose ) \n 
indexes_size = self . get_db_size ( ) \n 
self . print_db_sizes ( init_size , table_size , indexes_size ) \n 
self . close_db ( self . con ) \n 
~~ def index_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ if dtype == "int" : \n 
~~~ idx_cols = [ ] \n 
~~ elif dtype == "float" : \n 
~~~ idx_cols = [ , ] \n 
~~ for colname in idx_cols : \n 
~~~ t1 = time ( ) \n 
self . index_col ( self . con , colname , kind , optlevel , verbose ) \n 
self . print_mtime ( t1 , % colname ) \n 
~~ ~~ def query_db ( self , niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) : \n 
~~~ self . con = self . open_db ( ) \n 
if dtype == "int" : \n 
~~~ reg_cols = [ ] \n 
idx_cols = [ ] \n 
~~~ reg_cols = [ , ] \n 
idx_cols = [ , ] \n 
~~ if avoidfscache : \n 
~~~ rseed = int ( numpy . random . randint ( self . nrows ) ) \n 
~~~ rseed = 19 \n 
~~ numpy . random . seed ( rseed ) \n 
base = numpy . random . randint ( self . nrows ) \n 
if not onlyidxquery : \n 
~~~ for colname in reg_cols : \n 
~~~ ltimes = [ ] \n 
random . seed ( rseed ) \n 
for i in range ( NI_NTIMES ) : \n 
results = self . do_query ( self . con , colname , base , inkernel ) \n 
ltimes . append ( time ( ) - t1 ) \n 
~~ if verbose : \n 
~~ self . print_qtime ( colname , ltimes ) \n 
~~ self . close_db ( self . con ) \n 
self . con = self . open_db ( ) \n 
~~ if not onlynonidxquery : \n 
~~~ for colname in idx_cols : \n 
numpy . random . seed ( rseed ) \n 
rndbase = numpy . random . randint ( self . nrows , size = niter ) \n 
for i in range ( niter ) : \n 
~~~ base = rndbase [ i ] \n 
~~ self . print_qtime_idx ( colname , ltimes , False , verbose ) \n 
ltimes = [ ] \n 
~~ ~~ self . close_db ( self . con ) \n 
~~ def close_db ( self , con ) : \n 
~~~ con . close ( ) \n 
~~~ import sys \n 
import getopt \n 
~~~ import psyco \n 
psyco_imported = 1 \n 
~~~ psyco_imported = 0 \n 
~~~ opts , pargs = getopt . getopt ( \n 
sys . argv [ 1 : ] , ) \n 
~~~ sys . stderr . write ( usage ) \n 
~~ usepytables = 0 \n 
usepostgres = 0 \n 
verbose = 0 \n 
doprofile = 0 \n 
dokprofile = 0 \n 
usepsyco = 0 \n 
userandom = 0 \n 
docreate = 0 \n 
optlevel = 0 \n 
kind = "medium" \n 
docompress = 0 \n 
complib = "zlib" \n 
doquery = False \n 
onlyidxquery = False \n 
onlynonidxquery = False \n 
inkernel = True \n 
avoidfscache = 0 \n 
rng = [ - 1000 , - 1000 ] \n 
repeatquery = 0 \n 
repeatvalue = 0 \n 
krows = \n 
niter = READ_TIMES \n 
dtype = "all" \n 
datadir = "data.nobackup" \n 
for option in opts : \n 
~~~ if option [ 0 ] == : \n 
~~~ usepytables = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ usepostgres = 1 \n 
~~~ verbose = 1 \n 
~~~ doprofile = 1 \n 
~~~ dokprofile = 1 \n 
~~~ usepsyco = 1 \n 
~~~ userandom = 1 \n 
~~~ docreate = 1 \n 
~~~ doquery = True \n 
onlyidxquery = True \n 
onlynonidxquery = True \n 
inkernel = False \n 
~~~ avoidfscache = 1 \n 
~~~ docompress = int ( option [ 1 ] ) \n 
~~~ complib = option [ 1 ] \n 
~~~ rng = [ int ( i ) for i in option [ 1 ] . split ( "," ) ] \n 
~~~ niter = int ( option [ 1 ] ) \n 
~~~ krows = option [ 1 ] \n 
~~~ datadir = option [ 1 ] \n 
~~~ optlevel = int ( option [ 1 ] ) \n 
~~~ if option [ 1 ] in ( , , , ) : \n 
~~~ kind = option [ 1 ] \n 
"\'ultralight\'" ) \n 
~~ ~~ elif option [ 0 ] == : \n 
~~~ if option [ 1 ] in ( , ) : \n 
~~~ dtype = option [ 1 ] \n 
~~~ repeatquery = 1 \n 
repeatvalue = int ( option [ 1 ] ) \n 
~~ ~~ if not usepytables and not usepostgres : \n 
~~ if usepytables : \n 
~~~ from pytables_backend import PyTables_DB \n 
db = PyTables_DB ( krows , rng , userandom , datadir , \n 
docompress , complib , kind , optlevel ) \n 
~~ elif usepostgres : \n 
~~~ from postgres_backend import Postgres_DB \n 
db = Postgres_DB ( krows , rng , userandom ) \n 
~~ if not avoidfscache : \n 
~~~ numpy . random . seed ( 20 ) \n 
~~~ if userandom : \n 
~~ if onlyidxquery : \n 
~~ ~~ if psyco_imported and usepsyco : \n 
~~~ psyco . bind ( db . create_db ) \n 
psyco . bind ( db . query_db ) \n 
~~ if docreate : \n 
~~~ if verbose : \n 
~~ db . create_db ( dtype , kind , optlevel , verbose ) \n 
~~ if doquery : \n 
if doprofile : \n 
~~~ import pstats \n 
import cProfile as prof \n 
prof . run ( \n 
stats = pstats . Stats ( ) \n 
stats . strip_dirs ( ) \n 
stats . sort_stats ( , ) \n 
~~~ stats . print_stats ( ) \n 
~~~ stats . print_stats ( 20 ) \n 
~~ ~~ elif dokprofile : \n 
~~~ from cProfile import Profile \n 
import lsprofcalltree \n 
prof = Profile ( ) \n 
kcg = lsprofcalltree . KCacheGrind ( prof ) \n 
ofile = open ( , ) \n 
kcg . output ( ofile ) \n 
ofile . close ( ) \n 
~~ elif doprofile : \n 
~~~ import hotshot \n 
import hotshot . stats \n 
prof = hotshot . Profile ( "indexed_search.prof" ) \n 
benchtime , stones = prof . run ( \n 
prof . close ( ) \n 
stats = hotshot . stats . load ( "indexed_search.prof" ) \n 
stats . print_stats ( 20 ) \n 
~~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) \n 
~~ ~~ if repeatquery : \n 
~~~ db . rng = [ 1 , 1 ] \n 
~~~ print ( "range:" , db . rng ) \n 
~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
for i in range ( repeatvalue ) : \n 
~~~ for j in ( 1 , 2 , 5 ) : \n 
~~~ rng = j * 10 ** i \n 
db . rng = [ - rng / 2 , rng / 2 ] \n 
import matplotlib as mpl \n 
from pylab import * \n 
KB_ = 1024 \n 
MB_ = 1024 * KB_ \n 
GB_ = 1024 * MB_ \n 
linewidth = 2 \n 
markers = [ , , , , , , , , , ] \n 
markersize = 8 \n 
def get_values ( filename ) : \n 
~~~ f = open ( filename ) \n 
values = { "memcpyw" : [ ] , "memcpyr" : [ ] } \n 
for line in f : \n 
~~~ tmp = line . split ( ) [ 1 ] \n 
nthreads , size , elsize , sbits , codec , shuffle = [ i for i in tmp . split ( ) ] \n 
nthreads , size , elsize , sbits = map ( int , ( nthreads , size , elsize , sbits ) ) \n 
values [ "size" ] = size * NCHUNKS / MB_ ; \n 
values [ "elsize" ] = elsize ; \n 
values [ "sbits" ] = sbits ; \n 
values [ "codec" ] = codec \n 
values [ "shuffle" ] = shuffle \n 
( ratios , speedsw , speedsr ) = ( [ ] , [ ] , [ ] ) \n 
values [ nthreads ] = ( ratios , speedsw , speedsr ) \n 
~~ elif line . startswith ( ) : \n 
memcpyw = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyw" ] . append ( memcpyw ) \n 
memcpyr = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyr" ] . append ( memcpyr ) \n 
speedw = float ( tmp . split ( ) [ 1 ] ) \n 
ratio = float ( line . split ( ) [ - 1 ] ) \n 
speedsw . append ( speedw ) \n 
ratios . append ( ratio ) \n 
speedr = float ( tmp . split ( ) [ 1 ] ) \n 
speedsr . append ( speedr ) \n 
if "OK" not in line : \n 
~~ ~~ ~~ f . close ( ) \n 
return nthreads , values \n 
~~ def show_plot ( plots , yaxis , legends , gtitle , xmax = None ) : \n 
~~~ xlabel ( ) \n 
ylabel ( ) \n 
title ( gtitle ) \n 
xlim ( 0 , xmax ) \n 
ylim ( 0 , None ) \n 
grid ( True ) \n 
legend ( [ p [ 0 ] for p in plots \n 
if not isinstance ( p , mpl . lines . Line2D ) ] , \n 
legends , loc = "best" ) \n 
if outfile : \n 
savefig ( outfile , dpi = 64 ) \n 
~~~ show ( ) \n 
~~ ~~ if __name__ == : \n 
~~~ from optparse import OptionParser \n 
compress_title = \n 
decompress_title = \n 
yaxis = \n 
parser = OptionParser ( usage = usage ) \n 
parser . add_option ( , \n 
dest = , \n 
help = ( \n 
help = , ) \n 
help = , \n 
default = None ) \n 
parser . add_option ( , , action = , \n 
default = False ) \n 
( options , args ) = parser . parse_args ( ) \n 
if len ( args ) == 0 : \n 
~~ elif len ( args ) > 1 : \n 
~~ if options . report and options . outfile : \n 
~~ if options . dspeed and options . cspeed : \n 
~~ elif options . cspeed : \n 
~~~ options . dspeed = False \n 
plot_title = compress_title \n 
~~~ options . dspeed = True \n 
plot_title = decompress_title \n 
~~ filename = args [ 0 ] \n 
cspeed = options . cspeed \n 
dspeed = options . dspeed \n 
if options . outfile : \n 
~~~ outfile = options . outfile \n 
~~ elif options . report : \n 
~~~ if cspeed : \n 
~~~ outfile = filename [ : filename . rindex ( ) ] + \n 
~~~ outfile = None \n 
~~ plots = [ ] \n 
legends = [ ] \n 
nthreads , values = get_values ( filename ) \n 
if options . limit : \n 
~~~ thread_range = eval ( options . limit ) \n 
~~~ thread_range = range ( 1 , nthreads + 1 ) \n 
~~ if options . title : \n 
~~~ plot_title = options . title \n 
~~ gtitle = plot_title \n 
for nt in thread_range : \n 
~~~ ( ratios , speedw , speedr ) = values [ nt ] \n 
if cspeed : \n 
~~~ speed = speedw \n 
~~~ speed = speedr \n 
~~ plot_ = plot ( ratios , speed , linewidth = 2 ) \n 
plots . append ( plot_ ) \n 
nmarker = nt \n 
if nt >= len ( markers ) : \n 
~~~ nmarker = nt % len ( markers ) \n 
~~ setp ( plot_ , marker = markers [ nmarker ] , markersize = markersize , \n 
linewidth = linewidth ) \n 
~~ if cspeed : \n 
~~~ mean = np . mean ( values [ "memcpyw" ] ) \n 
~~~ mean = np . mean ( values [ "memcpyr" ] ) \n 
~~ plot_ = axhline ( mean , linewidth = 3 , linestyle = , color = ) \n 
text ( 1.0 , mean + 50 , message ) \n 
show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n 
options . xmax else None ) \n 
import tables \n 
fileh = tables . open_file ( "attributes1.h5" , mode = "w" , \n 
root = fileh . root \n 
a = np . array ( [ 1 , 2 , 4 ] , np . int32 ) \n 
hdfarray . attrs . char = "1" \n 
hdfarray . attrs . int = 12 \n 
hdfarray . attrs . float = 12.32 \n 
hdfarray . attrs . object = { "a" : 32.1 , "b" : 1 , "c" : [ 1 , 2 ] } \n 
fileh . close ( ) \n 
def setUp ( filename ) : \n 
fileh . enable_undo ( ) \n 
return fileh \n 
~~ def tearDown ( fileh ) : \n 
~~~ fileh . disable_undo ( ) \n 
~~ def demo_6times3marks ( ) : \n 
fileh = setUp ( "undo-redo-6times3marks.h5" ) \n 
fileh . mark ( ) \n 
fileh . undo ( ) \n 
assert "/otherarray1" in fileh \n 
assert "/otherarray2" in fileh \n 
assert "/otherarray3" in fileh \n 
assert "/otherarray4" in fileh \n 
assert "/otherarray5" not in fileh \n 
assert "/otherarray6" not in fileh \n 
assert "/otherarray3" not in fileh \n 
assert "/otherarray4" not in fileh \n 
assert "/otherarray1" not in fileh \n 
assert "/otherarray2" not in fileh \n 
fileh . redo ( ) \n 
assert "/otherarray5" in fileh \n 
assert "/otherarray6" in fileh \n 
tearDown ( fileh ) \n 
~~ def demo_manyops ( ) : \n 
fileh = setUp ( "undo-redo-manyops.h5" ) \n 
new_node = fileh . copy_node ( , ) \n 
new_node = fileh . copy_children ( , , recursive = 1 ) \n 
fileh . rename_node ( , ) \n 
fileh . remove_node ( ) \n 
assert not in fileh \n 
assert in fileh \n 
assert fileh . root . agroup . anarray3 is new_node \n 
~~ if __name__ == : \n 
~~~ demo_6times3marks ( ) \n 
demo_manyops ( ) \n 
######################################################################## \n 
import functools \n 
from . registry import class_name_dict , class_id_dict \n 
from . exceptions import ( ClosedNodeError , NodeError , UndoRedoWarning , \n 
PerformanceWarning ) \n 
from . path import join_path , split_path , isvisiblepath \n 
from . utils import lazyattr \n 
from . undoredo import move_to_shadow \n 
from . attributeset import AttributeSet , NotLoggedAttributeSet \n 
__docformat__ = \n 
def _closedrepr ( oldmethod ) : \n 
@ functools . wraps ( oldmethod ) \n 
def newmethod ( self ) : \n 
~~~ if not self . _v_isopen : \n 
~~~ cmod = self . __class__ . __module__ \n 
cname = self . __class__ . __name__ \n 
addr = hex ( id ( self ) ) \n 
return % ( cmod , cname , addr ) \n 
~~ return oldmethod ( self ) \n 
~~ return newmethod \n 
~~ class MetaNode ( type ) : \n 
def __new__ ( class_ , name , bases , dict_ ) : \n 
~~~ for mname in [ , ] : \n 
~~~ if mname in dict_ : \n 
~~~ dict_ [ mname ] = _closedrepr ( dict_ [ mname ] ) \n 
~~ ~~ return type . __new__ ( class_ , name , bases , dict_ ) \n 
~~ def __init__ ( class_ , name , bases , dict_ ) : \n 
~~~ super ( MetaNode , class_ ) . __init__ ( name , bases , dict_ ) \n 
class_name_dict [ class_ . __name__ ] = class_ \n 
cid = getattr ( class_ , , None ) \n 
if cid is not None : \n 
~~~ for base in bases : \n 
~~~ pcid = getattr ( base , , None ) \n 
if pcid == cid : \n 
~~~ class_id_dict [ cid ] = class_ \n 
~~ ~~ ~~ ~~ class Node ( six . with_metaclass ( MetaNode , object ) ) : \n 
_AttributeSet = AttributeSet \n 
def _g_getparent ( self ) : \n 
( parentpath , nodename ) = split_path ( self . _v_pathname ) \n 
return self . _v_file . _get_node ( parentpath ) \n 
~~ _v_parent = property ( _g_getparent ) \n 
@ lazyattr \n 
def _v_attrs ( self ) : \n 
return self . _AttributeSet ( self ) \n 
~~ def _g_gettitle ( self ) : \n 
if hasattr ( self . _v_attrs , ) : \n 
~~~ return self . _v_attrs . TITLE \n 
~~ ~~ def _g_settitle ( self , title ) : \n 
~~~ self . _v_attrs . TITLE = title \n 
~~ _v_title = property ( _g_gettitle , _g_settitle ) \n 
_v_isopen = False \n 
def __init__ ( self , parentnode , name , _log = True ) : \n 
~~~ if isinstance ( parentnode , class_name_dict [ ] ) : \n 
~~~ parentnode = parentnode . dereference ( ) \n 
~~ self . _v_file = None \n 
self . _v_isopen = False \n 
self . _v_pathname = None \n 
self . _v_name = None \n 
self . _v_depth = None \n 
self . _v_maxtreedepth = parentnode . _v_file . params [ ] \n 
self . _v__deleting = False \n 
self . _v_objectid = None \n 
self . _g_check_group ( parentnode ) \n 
parentnode . _g_check_open ( ) \n 
file_ = parentnode . _v_file \n 
if new : \n 
~~~ file_ . _check_writable ( ) \n 
~~ if new : \n 
~~~ parentnode . _g_refnode ( self , name , validate ) \n 
~~ self . _g_set_location ( parentnode , name ) \n 
~~~ self . _g_new ( parentnode , name , init = True ) \n 
~~~ self . _v_objectid = self . _g_create ( ) \n 
~~~ self . _v_objectid = self . _g_open ( ) \n 
~~ if new and _log and file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_log_create ( ) \n 
~~ self . _g_post_init_hook ( ) \n 
~~~ self . _f_close ( ) \n 
raise \n 
~~ ~~ def _g_log_create ( self ) : \n 
~~~ self . _v_file . _log ( , self . _v_pathname ) \n 
~~ def __del__ ( self ) : \n 
~~ self . _v__deleting = True \n 
~~~ node_manager = self . _v_file . _node_manager \n 
node_manager . drop_node ( self , check_unregistered = False ) \n 
~~~ if self . _v_isopen : \n 
~~~ self . _v__deleting = True \n 
self . _f_close ( ) \n 
~~ ~~ ~~ def _g_pre_kill_hook ( self ) : \n 
~~ def _g_create ( self ) : \n 
~~ def _g_open ( self ) : \n 
~~ def _g_check_open ( self ) : \n 
if not self . _v_isopen : \n 
~~ def _g_set_location ( self , parentnode , name ) : \n 
parentdepth = parentnode . _v_depth \n 
self . _v_file = file_ \n 
self . _v_isopen = True \n 
root_uep = file_ . root_uep \n 
if name . startswith ( root_uep ) : \n 
~~~ assert parentdepth == 0 \n 
if root_uep == "/" : \n 
~~~ self . _v_pathname = name \n 
~~~ self . _v_pathname = name [ len ( root_uep ) : ] \n 
~~ _ , self . _v_name = split_path ( name ) \n 
self . _v_depth = name . count ( "/" ) - root_uep . count ( "/" ) + 1 \n 
~~~ self . _v_name = name \n 
self . _v_pathname = join_path ( parentnode . _v_pathname , name ) \n 
self . _v_depth = parentdepth + 1 \n 
~~ if parentdepth >= self . _v_maxtreedepth : \n 
% ( self . _v_pathname , self . _v_maxtreedepth ) , \n 
~~ if self . _v_pathname != : \n 
~~~ file_ . _node_manager . cache_node ( self , self . _v_pathname ) \n 
~~ ~~ def _g_update_location ( self , newparentpath ) : \n 
oldpath = self . _v_pathname \n 
newpath = join_path ( newparentpath , self . _v_name ) \n 
newdepth = newpath . count ( ) \n 
self . _v_pathname = newpath \n 
self . _v_depth = newdepth \n 
if newdepth > self . _v_maxtreedepth : \n 
% ( self . _v_maxtreedepth , ) , PerformanceWarning ) \n 
~~ node_manager = self . _v_file . _node_manager \n 
node_manager . rename_node ( oldpath , newpath ) \n 
self . _g_update_dependent ( ) \n 
~~ def _g_del_location ( self ) : \n 
node_manager = self . _v_file . _node_manager \n 
pathname = self . _v_pathname \n 
if not self . _v__deleting : \n 
~~~ node_manager . drop_from_cache ( pathname ) \n 
node_manager . registry . pop ( pathname , None ) \n 
~~ def _g_post_init_hook ( self ) : \n 
~~ def _g_update_dependent ( self ) : \n 
if in self . __dict__ : \n 
~~~ self . _v_attrs . _g_update_node_location ( self ) \n 
~~ ~~ def _f_close ( self ) : \n 
~~ myDict = self . __dict__ \n 
if in myDict : \n 
~~~ self . _v_attrs . _g_close ( ) \n 
~~ self . _g_del_location ( ) \n 
myDict . clear ( ) \n 
~~ def _g_remove ( self , recursive , force ) : \n 
parent = self . _v_parent \n 
parent . _g_unrefnode ( self . _v_name ) \n 
self . _g_delete ( parent ) \n 
~~ def _f_remove ( self , recursive = False , force = False ) : \n 
self . _g_check_open ( ) \n 
file_ = self . _v_file \n 
file_ . _check_writable ( ) \n 
if file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_remove_and_log ( recursive , force ) \n 
~~~ self . _g_remove ( recursive , force ) \n 
~~ ~~ def _g_remove_and_log ( self , recursive , force ) : \n 
~~~ file_ = self . _v_file \n 
oldpathname = self . _v_pathname \n 
file_ . _log ( , oldpathname ) \n 
move_to_shadow ( file_ , oldpathname ) \n 
~~ def _g_move ( self , newparent , newname ) : \n 
oldparent = self . _v_parent \n 
oldname = self . _v_name \n 
newparent . _g_refnode ( self , newname ) \n 
oldparent . _g_unrefnode ( oldname ) \n 
self . _g_del_location ( ) \n 
self . _g_set_location ( newparent , newname ) \n 
self . _g_new ( newparent , self . _v_name , init = False ) \n 
self . _v_parent . _g_move_node ( oldparent . _v_objectid , oldname , \n 
newparent . _v_objectid , newname , \n 
oldpathname , self . _v_pathname ) \n 
~~ def _f_rename ( self , newname , overwrite = False ) : \n 
self . _f_move ( newname = newname , overwrite = overwrite ) \n 
~~ def _f_move ( self , newparent = None , newname = None , \n 
overwrite = False , createparents = False ) : \n 
if newparent is None and newname is None : \n 
~~ if newparent is None : \n 
~~~ newparent = oldparent \n 
~~ if newname is None : \n 
~~~ newname = oldname \n 
~~~ newfile = newparent . _v_file \n 
newpath = newparent . _v_pathname \n 
~~~ newfile = file_ \n 
newpath = newparent \n 
% ( newparent , ) ) \n 
~~ if newfile is not file_ : \n 
~~ file_ . _check_writable ( ) \n 
oldpath = oldparent . _v_pathname \n 
if newpath == oldpath and newname == oldname : \n 
~~ self . _g_check_not_contains ( newpath ) \n 
newparent = file_ . _get_or_create_path ( newparent , createparents ) \n 
self . _g_maybe_remove ( newparent , newname , overwrite ) \n 
self . _g_move ( newparent , newname ) \n 
~~~ self . _g_log_move ( oldpathname ) \n 
~~ ~~ def _g_log_move ( self , oldpathname ) : \n 
~~~ self . _v_file . _log ( , oldpathname , self . _v_pathname ) \n 
~~ def _g_copy ( self , newparent , newname , recursive , _log = True , ** kwargs ) : \n 
~~ def _g_copy_as_child ( self , newparent , ** kwargs ) : \n 
return self . _g_copy ( newparent , self . _v_name , \n 
recursive = False , _log = False , ** kwargs ) \n 
~~ def _f_copy ( self , newparent = None , newname = None , \n 
overwrite = False , recursive = False , createparents = False , \n 
srcfile = self . _v_file \n 
srcparent = self . _v_parent \n 
srcname = self . _v_name \n 
dstparent = newparent \n 
dstname = newname \n 
if dstparent is None and dstname is None : \n 
~~ if dstparent is None : \n 
~~~ dstparent = srcparent \n 
~~ if dstname is None : \n 
~~~ dstname = srcname \n 
~~~ dstfile = dstparent . _v_file \n 
dstpath = dstparent . _v_pathname \n 
~~~ dstfile = srcfile \n 
dstpath = dstparent \n 
% ( dstparent , ) ) \n 
~~ if dstfile is srcfile : \n 
~~~ srcpath = srcparent . _v_pathname \n 
if dstpath == srcpath and dstname == srcname : \n 
~~~ raise NodeError ( \n 
% self . _v_pathname ) \n 
~~ if recursive : \n 
~~~ self . _g_check_not_contains ( dstpath ) \n 
~~ ~~ dstparent = srcfile . _get_or_create_path ( dstparent , createparents ) \n 
if dstfile is not srcfile and srcfile . is_undo_enabled ( ) : \n 
UndoRedoWarning ) \n 
~~ self . _g_maybe_remove ( dstparent , dstname , overwrite ) \n 
return self . _g_copy ( dstparent , dstname , recursive , ** kwargs ) \n 
~~ def _f_isvisible ( self ) : \n 
return isvisiblepath ( self . _v_pathname ) \n 
~~ def _g_check_group ( self , node ) : \n 
~~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
% node . _v_pathname ) \n 
~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
~~ ~~ def _g_check_not_contains ( self , pathname ) : \n 
~~~ mypathname = self . _v_pathname \n 
or pathname == mypathname \n 
or pathname . startswith ( mypathname + ) ) : \n 
~~ ~~ def _g_maybe_remove ( self , parent , name , overwrite ) : \n 
~~~ if name in parent : \n 
~~~ if not overwrite : \n 
~~ parent . _f_get_child ( name ) . _f_remove ( True ) \n 
~~ ~~ def _g_check_name ( self , name ) : \n 
if name . startswith ( ) : \n 
~~ ~~ def _f_getattr ( self , name ) : \n 
return getattr ( self . _v_attrs , name ) \n 
~~ def _f_setattr ( self , name , value ) : \n 
setattr ( self . _v_attrs , name , value ) \n 
~~ def _f_delattr ( self , name ) : \n 
delattr ( self . _v_attrs , name ) \n 
~~ ~~ class NotLoggedMixin : \n 
~~~ _AttributeSet = NotLoggedAttributeSet \n 
def _g_log_create ( self ) : \n 
~~ def _g_log_move ( self , oldpathname ) : \n 
~~ def _g_remove_and_log ( self , recursive , force ) : \n 
~~ ~~ from __future__ import print_function \n 
from tables import IsDescription , StringCol , BoolCol , IntCol , FloatCol \n 
from tables . node import NotLoggedMixin \n 
from tables . path import join_path \n 
from tables . tests import common \n 
from tables . tests . common import unittest \n 
from tables . tests . common import PyTablesTestCase as TestCase \n 
class BasicTestCase ( common . TempFileMixin , TestCase ) : \n 
_reopen_flag = False \n 
def _do_reopen ( self ) : \n 
~~~ if self . _reopen_flag : \n 
~~~ self . _reopen ( ) \n 
~~ ~~ def setUp ( self ) : \n 
~~~ super ( BasicTestCase , self ) . setUp ( ) \n 
h5file = self . h5file \n 
root = h5file . root \n 
~~ def test00_simple ( self ) : \n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
~~ self . h5file . enable_undo ( ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray" not in self . h5file ) \n 
self . assertEqual ( self . h5file . _curaction , 0 ) \n 
self . assertEqual ( self . h5file . _curmark , 0 ) \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( ) \n 
~~ self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . _curaction , 1 ) \n 
~~ def test01_twice ( self ) : \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . _curaction , 2 ) \n 
~~ def test02_twice2 ( self ) : \n 
self . h5file . mark ( ) \n 
self . assertEqual ( self . h5file . _curaction , 3 ) \n 
self . assertEqual ( self . h5file . _curmark , 1 ) \n 
~~ def test03_6times3marks ( self ) : \n 
self . __class__ . __name__ ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" in self . h5file ) \n 
self . assertTrue ( "/otherarray6" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ 6 , 7 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray5 . read ( ) , [ 7 , 8 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray6 . read ( ) , [ 8 , 9 ] ) \n 
~~ def test04_6times3marksro ( self ) : \n 
~~ self . h5file . mark ( ) \n 
~~ def test05_destructive ( self ) : \n 
~~ def test05b_destructive ( self ) : \n 
~~ def test05c_destructive ( self ) : \n 
~~ def test05d_destructive ( self ) : \n 
self . h5file . undo ( 0 ) \n 
~~ def test05e_destructive ( self ) : \n 
~~ def test05f_destructive ( self ) : \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
self . assertTrue ( not in self . h5file ) \n 
newarr = self . h5file . create_array ( , , [ 1 ] ) \n 
self . assertTrue ( in self . h5file ) \n 
if not self . _reopen_flag : \n 
~~~ self . assertTrue ( self . h5file . root . newarray is newarr ) \n 
~~ ~~ def test06_totalunwind ( self ) : \n 
~~ def test07_totalrewind ( self ) : \n 
self . h5file . redo ( - 1 ) \n 
~~ def test08_marknames ( self ) : \n 
self . h5file . mark ( "first" ) \n 
self . h5file . mark ( "second" ) \n 
self . h5file . mark ( "third" ) \n 
self . h5file . undo ( "first" ) \n 
self . h5file . redo ( "third" ) \n 
self . h5file . undo ( "second" ) \n 
~~ def test08_initialmark ( self ) : \n 
initmid = self . h5file . get_current_mark ( ) \n 
self . h5file . undo ( initmid ) \n 
~~ def test09_marknames ( self ) : \n 
with self . assertRaises ( tables . UndoRedoError ) : \n 
~~~ self . h5file . undo ( "third" ) \n 
~~ self . h5file . redo ( "third" ) \n 
~~~ self . h5file . redo ( "second" ) \n 
~~ self . assertTrue ( "/otherarray1" in self . h5file ) \n 
~~ def test10_goto ( self ) : \n 
self . h5file . goto ( "first" ) \n 
self . h5file . goto ( "third" ) \n 
self . h5file . goto ( "second" ) \n 
self . h5file . goto ( - 1 ) \n 
~~ def test10_gotoint ( self ) : \n 
self . h5file . goto ( 1 ) \n 
self . h5file . goto ( 0 ) \n 
self . h5file . goto ( 3 ) \n 
self . h5file . goto ( 2 ) \n 
~~ def test11_contiguous ( self ) : \n 
m1 = self . h5file . mark ( ) \n 
m2 = self . h5file . mark ( ) \n 
self . assertNotEqual ( m1 , m2 ) \n 
self . h5file . undo ( m1 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m1 ) \n 
self . h5file . redo ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n 
self . h5file . goto ( m1 ) \n 
self . h5file . goto ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , 0 ) \n 
~~ def test12_keepMark ( self ) : \n 
mid = self . h5file . mark ( ) \n 
self . assertTrue ( mid is not None ) \n 
~~ def test13_severalEnableDisable ( self ) : \n 
self . h5file . disable_undo ( ) \n 
self . h5file . enable_undo ( ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , mid ) \n 
~~ ~~ class PersistenceTestCase ( BasicTestCase ) : \n 
_reopen_flag = True \n 
~~ class CreateArrayTestCase ( common . TempFileMixin , TestCase ) : \n 
def setUp ( self ) : \n 
~~~ super ( CreateArrayTestCase , self ) . setUp ( ) \n 
~~ def test00 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 1 , 2 ] ) \n 
~~ def test01 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
~~ def test02 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 3 , 4 ] ) \n 
~~ def test03 ( self ) : \n 
self . h5file . create_array ( , , \n 
self . assertTrue ( "/agroup/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/agroup/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . title , \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . title , \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . read ( ) , \n 
[ 3 , 4 ] ) \n 
~~ ~~ class CreateGroupTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CreateGroupTestCase , self ) . setUp ( ) \n 
self . assertTrue ( "/othergroup1" not in self . h5file ) \n 
self . assertTrue ( "/othergroup1" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n 
self . assertTrue ( "/othergroup2" not in self . h5file ) \n 
self . assertTrue ( "/othergroup2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup2 . _v_title , \n 
self . assertTrue ( "/othergroup3" not in self . h5file ) \n 
self . assertTrue ( "/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup3 . _v_title , \n 
self . h5file . create_group ( \n 
self . assertTrue ( "/othergroup1/othergroup2" not in self . h5file ) \n 
self . assertTrue ( \n 
"/othergroup1/othergroup2/othergroup3" not in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2" in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . othergroup2 . _v_title , \n 
self . assertEqual ( \n 
self . h5file . root . othergroup1 . othergroup2 . othergroup3 . _v_title , \n 
~~ ~~ minRowIndex = 10 \n 
def populateTable ( where , name ) : \n 
class Indexed ( IsDescription ) : \n 
~~~ var1 = StringCol ( itemsize = 4 , dflt = b"" , pos = 1 ) \n 
var2 = BoolCol ( dflt = 0 , pos = 2 ) \n 
var3 = IntCol ( dflt = 0 , pos = 3 ) \n 
var4 = FloatCol ( dflt = 0 , pos = 4 ) \n 
~~ nrows = minRowIndex \n 
table = where . _v_file . create_table ( where , name , Indexed , "Indexed" , \n 
None , nrows ) \n 
for i in range ( nrows ) : \n 
~~~ table . row [ ] = str ( i ) \n 
table . row [ ] = i % 2 \n 
table . row [ ] = i \n 
table . row [ ] = float ( nrows - i - 1 ) \n 
table . row . append ( ) \n 
~~ table . flush ( ) \n 
indexrows = table . cols . var1 . create_index ( ) \n 
indexrows = table . cols . var2 . create_index ( ) \n 
indexrows = table . cols . var3 . create_index ( ) \n 
~~ ~~ class RenameNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( RenameNodeTestCase , self ) . setUp ( ) \n 
populateTable ( self . h5file . root , ) \n 
self . h5file . rename_node ( , ) \n 
self . assertTrue ( "/agroup2" in self . h5file ) \n 
self . assertTrue ( "/agroup3" not in self . h5file ) \n 
self . assertTrue ( "/agroup2" not in self . h5file ) \n 
self . assertTrue ( "/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup3/agroup3" in self . h5file ) \n 
~~ def test01b ( self ) : \n 
self . assertTrue ( "/agroup4" not in self . h5file ) \n 
self . assertTrue ( "/agroup4" in self . h5file ) \n 
self . assertTrue ( "/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup4/agroup3" in self . h5file ) \n 
self . assertTrue ( "/anarray" in self . h5file ) \n 
self . assertTrue ( "/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/anarray" not in self . h5file ) \n 
self . assertTrue ( "/anarray2" in self . h5file ) \n 
self . assertTrue ( "/table" in self . h5file ) \n 
table = self . h5file . root . table \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertTrue ( "/table2" not in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table . title , "Indexed" ) \n 
self . assertTrue ( "/table" not in self . h5file ) \n 
self . assertTrue ( "/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table2 . title , "Indexed" ) \n 
table = self . h5file . root . table2 \n 
~~ ~~ class MoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( MoveNodeTestCase , self ) . setUp ( ) \n 
self . h5file . move_node ( , ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . anarray . title , \n 
self . h5file . move_node ( , , ) \n 
self . assertTrue ( "/agroup2/agroup3" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup3 . _v_title , \n 
self . assertTrue ( "/agroup2/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup4 . _v_title , \n 
self . assertTrue ( "/agroup2/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . table2 . title , "Indexed" ) \n 
table = self . h5file . root . agroup2 . table2 \n 
~~ ~~ class RemoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( RemoveNodeTestCase , self ) . setUp ( ) \n 
self . h5file . remove_node ( ) \n 
~~ def test00b ( self ) : \n 
self . assertTrue ( "/agroup/anarray2" not in self . h5file ) \n 
~~ def test00c ( self ) : \n 
self . h5file . remove_node ( , recursive = 1 ) \n 
self . assertTrue ( "/agroup/anarray1" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" not in self . h5file ) \n 
~~ ~~ class CopyNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CopyNodeTestCase , self ) . setUp ( ) \n 
~~ def test00_copyLeaf ( self ) : \n 
new_node = self . h5file . copy_node ( , ) \n 
self . assertTrue ( self . h5file . root . agroup . agroup3 . anarray is new_node ) \n 
~~ def test00b_copyTable ( self ) : \n 
warnings . filterwarnings ( "ignore" , category = UserWarning ) \n 
table = self . h5file . copy_node ( \n 
, , propindexes = True ) \n 
warnings . filterwarnings ( "default" , category = UserWarning ) \n 
self . assertTrue ( "/agroup/agroup3/table" in self . h5file ) \n 
table = self . h5file . root . agroup . agroup3 . table \n 
self . assertEqual ( table . title , "Indexed" ) \n 
self . assertTrue ( "/agroup/agroup3/table" not in self . h5file ) \n 
~~ def test01_copyGroup ( self ) : \n 
new_node = self . h5file . copy_node ( \n 
, newname = , recursive = True ) \n 
self . assertTrue ( self . h5file . root . acopy is new_node ) \n 
~~ def test02_copyLeafOverwrite ( self ) : \n 
oldNode = self . h5file . root . agroup \n 
, newname = , overwrite = True ) \n 
self . assertTrue ( self . h5file . root . agroup is oldNode ) \n 
self . assertTrue ( self . h5file . root . agroup is new_node ) \n 
~~ def test03_copyChildren ( self ) : \n 
self . h5file . copy_children ( , , recursive = True ) \n 
~~ ~~ class ComplexTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( ComplexTestCase , self ) . setUp ( ) \n 
self . h5file . create_array ( self . h5file . root , , \n 
new_node = self . h5file . copy_children ( \n 
, , recursive = 1 ) \n 
self . assertTrue ( self . h5file . root . agroup . anarray3 is new_node ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 1 ] ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 4 ] ) \n 
self . h5file . create_group ( self . h5file . root . agroup2 , , \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup5 . _v_title , \n 
self . h5file . create_group ( self . h5file . root . agroup , , \n 
~~ def test03b ( self ) : \n 
self . h5file . create_group ( self . h5file . root . agroup3 , , \n 
~~ ~~ class AttributesTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( AttributesTestCase , self ) . setUp ( ) \n 
array = self . h5file . create_array ( , , [ 1 , 2 ] ) \n 
attrs = array . attrs \n 
attrs . attr_1 = 10 \n 
attrs . attr_2 = 20 \n 
attrs . attr_3 = 30 \n 
~~ def test00_setAttr ( self ) : \n 
~~ array = self . h5file . root . array \n 
setattr ( attrs , , 0 ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_0 , 0 ) \n 
self . assertTrue ( not in attrs ) \n 
~~ def test01_setAttrExisting ( self ) : \n 
setattr ( attrs , , 11 ) \n 
self . assertEqual ( attrs . attr_1 , 11 ) \n 
self . assertEqual ( attrs . attr_1 , 10 ) \n 
~~ def test02_delAttr ( self ) : \n 
delattr ( attrs , ) \n 
~~ def test03_copyNodeAttrs ( self ) : \n 
~~ rattrs = self . h5file . root . _v_attrs \n 
rattrs . attr_0 = 0 \n 
rattrs . attr_1 = 100 \n 
array = self . h5file . root . array \n 
attrs . _f_copy ( self . h5file . root ) \n 
self . assertEqual ( rattrs . attr_0 , 0 ) \n 
self . assertEqual ( rattrs . attr_1 , 10 ) \n 
self . assertEqual ( rattrs . attr_2 , 20 ) \n 
self . assertEqual ( rattrs . attr_3 , 30 ) \n 
self . assertEqual ( rattrs . attr_1 , 100 ) \n 
self . assertTrue ( not in rattrs ) \n 
~~ def test04_replaceNode ( self ) : \n 
attrs . attr_1 = 11 \n 
arr = self . h5file . create_array ( , , [ 1 ] ) \n 
arr . attrs . attr_1 = 12 \n 
self . assertTrue ( in self . h5file . root . array . attrs ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 10 ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 12 ) \n 
~~ ~~ class NotLoggedTestCase ( common . TempFileMixin , TestCase ) : \n 
class NotLoggedArray ( NotLoggedMixin , tables . Array ) : \n 
~~ def test00_hierarchy ( self ) : \n 
self . h5file . create_group ( , ) \n 
arr = self . NotLoggedArray ( self . h5file . root , , \n 
[ 1 ] , self . _getMethodName ( ) ) \n 
arr . move ( ) \n 
arr . remove ( ) \n 
~~ def test01_attributes ( self ) : \n 
arr . _v_attrs . foo = \n 
self . assertEqual ( arr . _v_attrs . foo , ) \n 
del arr . _v_attrs . foo \n 
self . assertRaises ( AttributeError , getattr , arr . _v_attrs , ) \n 
~~ ~~ class CreateParentsTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CreateParentsTestCase , self ) . setUp ( ) \n 
g1 = self . h5file . create_group ( , ) \n 
self . h5file . create_group ( g1 , ) \n 
~~ def existing ( self , paths ) : \n 
return frozenset ( path for path in paths if path in self . h5file ) \n 
~~ def basetest ( self , doit , pre , post ) : \n 
~~~ pre ( ) \n 
paths = [ , , , ] \n 
for newpath in paths : \n 
~~~ before = self . existing ( paths ) \n 
doit ( newpath ) \n 
after = self . existing ( paths ) \n 
self . assertTrue ( after . issuperset ( before ) ) \n 
post ( newpath ) \n 
self . assertEqual ( after , before ) \n 
~~ ~~ def test00_create ( self ) : \n 
def pre ( ) : \n 
~~ def doit ( newpath ) : \n 
~~~ self . h5file . create_array ( newpath , , [ 1 ] , createparents = True ) \n 
self . assertTrue ( join_path ( newpath , ) in self . h5file ) \n 
~~ def post ( newpath ) : \n 
~~~ self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ self . basetest ( doit , pre , post ) \n 
~~ def test01_move ( self ) : \n 
~~~ self . h5file . create_array ( , , [ 1 ] ) \n 
~~~ self . h5file . move_node ( , newpath , createparents = True ) \n 
~~~ self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ def test02_copy ( self ) : \n 
~~~ self . h5file . copy_node ( , newpath , createparents = True ) \n 
~~~ g = self . h5file . create_group ( , ) \n 
self . h5file . create_array ( g , , [ 1 ] ) \n 
~~~ self . h5file . copy_children ( , newpath , createparents = True ) \n 
~~ ~~ def suite ( ) : \n 
~~~ theSuite = unittest . TestSuite ( ) \n 
niter = 1 \n 
for n in range ( niter ) : \n 
~~~ theSuite . addTest ( unittest . makeSuite ( BasicTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( PersistenceTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateArrayTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateGroupTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RenameNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( MoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RemoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CopyNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( AttributesTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( ComplexTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( NotLoggedTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateParentsTestCase ) ) \n 
~~ if common . heavy : \n 
~~ return theSuite \n 
common . parse_argv ( sys . argv ) \n 
common . print_versions ( ) \n 
unittest . main ( defaultTest = ) \n 
def _print_after_skip ( skip , it = None , dist = None , etime = None ) : \n 
~~~ if it is None : \n 
~~~ msg = "{i:<13}{d:<15}{t:<17}" . format ( i = "Iteration" , \n 
d = "Distance" , \n 
print ( msg ) \n 
print ( "-" * len ( msg ) ) \n 
~~ if it % skip == 0 : \n 
~~~ if etime is None : \n 
~~~ msg = "{i:<13}{d:<15.3e}{t:<18.3e}" \n 
print ( msg . format ( i = it , d = dist , t = etime ) ) \n 
~~ def compute_fixed_point ( T , v , error_tol = 1e-3 , max_iter = 50 , verbose = 1 , \n 
print_skip = 5 , * args , ** kwargs ) : \n 
iterate = 0 \n 
error = error_tol + 1 \n 
~~~ start_time = time . time ( ) \n 
_print_after_skip ( print_skip , it = None ) \n 
~~ while iterate < max_iter and error > error_tol : \n 
~~~ new_v = T ( v , * args , ** kwargs ) \n 
iterate += 1 \n 
error = np . max ( np . abs ( new_v - v ) ) \n 
~~~ etime = time . time ( ) - start_time \n 
_print_after_skip ( print_skip , iterate , error , etime ) \n 
~~~ v [ : ] = new_v \n 
~~ except TypeError : \n 
~~~ v = new_v \n 
~~ ~~ return v \n 
import unittest \n 
from scipy . linalg import LinAlgError \n 
from numpy . testing import assert_allclose \n 
from quantecon . lqcontrol import LQ \n 
from quantecon . robustlq import RBLQ \n 
class TestRBLQControl ( unittest . TestCase ) : \n 
~~~ a_0 = 100 \n 
a_1 = 0.5 \n 
rho = 0.9 \n 
sigma_d = 0.05 \n 
beta = 0.95 \n 
c = 2 \n 
gamma = 50.0 \n 
theta = 0.002 \n 
ac = ( a_0 - c ) / 2.0 \n 
R = np . array ( [ [ 0 , ac , 0 ] , \n 
[ ac , - a_1 , 0.5 ] , \n 
[ 0. , 0.5 , 0 ] ] ) \n 
R = - R \n 
Q = gamma / 2 \n 
A = np . array ( [ [ 1. , 0. , 0. ] , \n 
[ 0. , 1. , 0. ] , \n 
[ 0. , 0. , rho ] ] ) \n 
B = np . array ( [ [ 0. ] , \n 
[ 1. ] , \n 
[ 0. ] ] ) \n 
C = np . array ( [ [ 0. ] , \n 
[ 0. ] , \n 
[ sigma_d ] ] ) \n 
self . rblq_test = RBLQ ( Q , R , A , B , C , beta , theta ) \n 
self . lq_test = LQ ( Q , R , A , B , C , beta ) \n 
self . Fr , self . Kr , self . Pr = self . rblq_test . robust_rule ( ) \n 
~~ def tearDown ( self ) : \n 
~~~ del self . rblq_test \n 
~~ def test_robust_rule_vs_simple ( self ) : \n 
~~~ rblq = self . rblq_test \n 
Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n 
Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = 1e-12 ) \n 
assert_allclose ( Fr , Fs , rtol = 1e-4 ) \n 
assert_allclose ( Kr , Ks , rtol = 1e-4 ) \n 
assert_allclose ( Pr , Ps , rtol = 1e-4 ) \n 
~~ def test_f2k_and_k2f ( self ) : \n 
K_f2k , P_f2k = rblq . F_to_K ( Fr ) \n 
F_k2f , P_k2f = rblq . K_to_F ( Kr ) \n 
assert_allclose ( K_f2k , Kr , rtol = 1e-4 ) \n 
assert_allclose ( F_k2f , Fr , rtol = 1e-4 ) \n 
assert_allclose ( P_f2k , P_k2f , rtol = 1e-4 ) \n 
~~ def test_evaluate_F ( self ) : \n 
Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n 
assert_allclose ( Pf , Pr ) \n 
assert_allclose ( Kf , Kr ) \n 
~~~ suite = unittest . TestLoader ( ) . loadTestsFromTestCase ( TestRBLQControl ) \n 
unittest . TextTestRunner ( verbosity = 2 , stream = sys . stderr ) . run ( suite ) \n 
import tables as pt \n 
fileName = "defaultAlphaFileName.h5" \n 
h5f = [ ] \n 
group = [ ] \n 
table = [ ] \n 
opened = False \n 
ctr = float ( 0.0 ) \n 
class AlphaDataModelClass ( pt . IsDescription ) : \n 
~~~ symbol = pt . StringCol ( 30 ) \n 
exchange = pt . StringCol ( 10 ) \n 
alphaValue = pt . Float32Col ( ) \n 
timestamp = pt . Time64Col ( ) \n 
~~ ~~ def openFile ( newFileName ) : \n 
global fileName , h5f , group , table , opened , ctr \n 
if newFileName is None : \n 
~~~ if ( len ( newFileName ) > 0 ) : \n 
~~~ fileName = str ( newFileName ) \n 
~~ ~~ if not opened : \n 
~~~ h5f = pt . openFile ( str ( fileName ) , mode = "w" ) \n 
group = h5f . createGroup ( "/" , ) \n 
table = h5f . createTable ( group , , AlphaDataModelClass ) \n 
opened = True \n 
~~ ~~ def addRow ( currSymbol , currExchange , currAlphaVal , currTS ) : \n 
global ctr \n 
if opened : \n 
~~~ ctr = ctr + 1 \n 
row = table . row \n 
row [ ] = currSymbol \n 
row [ ] = currExchange \n 
row [ ] = currAlphaVal \n 
row [ ] = currTS \n 
row . append ( ) \n 
~~~ ctr = 0 \n 
raise IOError \n 
~~ ~~ def closeFile ( ) : \n 
table . flush ( ) \n 
h5f . close ( ) \n 
import pickle as pkl \n 
import qstkutil . utils as utils \n 
import dircache \n 
def main ( ) : \n 
~~~ print "Starting..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
~~~ rootdir = os . environ [ ] \n 
~~ fileExtensionToRemove = ".csv" \n 
listOfInputPaths = list ( ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NASDAQ/" ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NYSE/" ) \n 
listOfOutputPaths = list ( ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/AMEX/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NASDAQ/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NYSE/" ) \n 
for path in listOfOutputPaths : \n 
~~~ if not ( os . access ( path , os . F_OK ) ) : \n 
if ( len ( listOfInputPaths ) != len ( listOfOutputPaths ) ) : \n 
sys . exit ( "FAILURE" ) \n 
~~ path_ctr = - 1 ; \n 
for path in listOfInputPaths : \n 
~~~ path_ctr = path_ctr + 1 ; \n 
stocks_at_this_path = dircache . listdir ( str ( path ) ) \n 
filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , filtered_names ) \n 
stock_ctr = - 1 \n 
for stock in filtered_names : \n 
~~~ stock_ctr = stock_ctr + 1 \n 
stock_data = np . loadtxt ( path + stock + ".csv" , np . float , None , "," , None , 1 , use_cols ) \n 
stock_data_shape = stock_data . shape \n 
f = open ( listOfOutputPaths [ path_ctr ] + filtered_names [ stock_ctr ] + ".pkl" , "wb" ) \n 
pkl . dump ( stock_data , f , - 1 ) \n 
f . close ( ) \n 
~~ ~~ print "Finished..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
~~~ main ( ) \n 
~~ import cPickle \n 
from pandas import DataMatrix \n 
import datetime as dt \n 
import qstkutil . DataAccess as da \n 
import qstkutil . qsdateutil as du \n 
if __name__ == "__main__" : \n 
symbols = list ( [ ] ) \n 
t = map ( int , sys . argv [ 1 ] . split ( ) ) \n 
startday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
t = map ( int , sys . argv [ 2 ] . split ( ) ) \n 
endday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
timeofday = dt . timedelta ( hours = 16 ) \n 
timestamps = du . getNYSEdays ( startday , endday , timeofday ) \n 
dataobj = da . DataAccess ( ) \n 
historic = dataobj . get_data ( timestamps , symbols , "close" ) \n 
alloc_val = random . random ( ) \n 
alloc = DataMatrix ( index = [ historic . index [ 0 ] ] , data = [ alloc_val ] , columns = symbols ) \n 
for date in range ( 1 , len ( historic . index ) ) : \n 
~~~ alloc_val = 1 #random.random() \n 
alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols ~~ alloc [ ] = 1 - alloc [ symbols [ 0 ] ] \n 
output = open ( sys . argv [ 3 ] , "wb" ) \n 
cPickle . dump ( alloc , output ) \n 
import QSTK . qstkutil . DataAccess as da \n 
from itertools import izip \n 
def getStocks ( listOfPaths ) : \n 
~~~ listOfStocks = list ( ) \n 
fileExtensionToRemove = ".h5" \n 
for path in listOfPaths : \n 
~~~ stocksAtThisPath = list ( ) \n 
stocksAtThisPath = dircache . listdir ( str ( path ) ) \n 
stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , stocksAtThisPath \n 
for stock in stocksAtThisPath : \n 
~~~ listOfStocks . append ( stock ) \n 
~~ return listOfStocks \n 
~~~ print "Starting..." \n 
dataItemsList = [ ] \n 
dataItemsList . append ( ) \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NASDAQ/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/Delisted_US_Recent/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/OTC/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_AMEX/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_Delisted/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NYSE/") \n 
listOfStocks = list ( ) \n 
#listOfStocks.append("AAPL") \n 
#listOfStocks.append("YHOO") \n 
#listOfStocks.append("AMZN") \n 
listOfPaths = list ( ) \n 
listOfPaths . append ( "C:\\\\test\\\\temp\\\\" ) \n 
#listOfPaths.append("C:\\\\test\\\\hdf\\\\") \n 
listOfStocks = getStocks ( listOfPaths ) \n 
tslist = list ( alpha . getTimestampArray ( ) ) \n 
listOfTS = alpha . getTimestampArray ( ) \n 
for stock in [ "AAPL" ] : \n 
~~~ alphaList = alpha . getStockDataList ( stock , ) \n 
ctr = 0 \n 
for val in alphaList : \n 
ctr += 1 \n 
~~ ~~ print "DONE!" \n 
~~ 