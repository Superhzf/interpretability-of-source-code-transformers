from __future__ import division , print_function , unicode_literals \n 
\n 
from collections import OrderedDict \n 
from brainstorm . layers . base_layer import Layer \n 
from brainstorm . structure . buffer_structure import ( BufferStructure , \n 
StructureTemplate ) \n 
from brainstorm . structure . construction import ConstructionWrapper \n 
from brainstorm . utils import flatten_all_but_last \n 
def BatchNorm ( name = None , decay = 0.9 , epsilon = 1.0e-5 ) : \n 
return ConstructionWrapper . create ( BatchNormLayerImpl , \n 
name = name , \n 
decay = decay , \n 
epsilon = epsilon ) \n 
~~ class BatchNormLayerImpl ( Layer ) : \n 
~~~ expected_inputs = { : StructureTemplate ( , , ) } \n 
expected_kwargs = { , } \n 
def setup ( self , kwargs , in_shapes ) : \n 
~~~ self . epsilon = kwargs . get ( , 1.0e-5 ) \n 
self . decay = kwargs . get ( , 0.9 ) \n 
outputs = OrderedDict ( ) \n 
outputs [ ] = in_shapes [ ] \n 
parameters = OrderedDict ( ) \n 
buf = BufferStructure ( self . in_shapes [ ] . feature_shape [ - 1 ] ) \n 
parameters [ ] = buf \n 
internals = OrderedDict ( ) \n 
internals [ ] = buf \n 
internals [ ] = self . in_shapes [ ] \n 
return outputs , parameters , internals \n 
~~ def forward_pass ( self , buffers , training_pass = True ) : \n 
~~~ _h = self . handler \n 
sigma_b , centered , x_hat = buffers . internals \n 
gamma , beta , mu , sigma = buffers . parameters \n 
inputs = flatten_all_but_last ( buffers . inputs . default ) \n 
centered = flatten_all_but_last ( centered ) \n 
x_hat = flatten_all_but_last ( x_hat ) \n 
out = flatten_all_but_last ( buffers . outputs . default ) \n 
m = inputs . shape [ 0 ] \n 
if training_pass : \n 
_h . sum_t ( inputs , 0 , mu_b ) \n 
_h . mult_st ( - 1.0 / m , mu_b , mu_b ) \n 
_h . mult_st ( self . decay , mu , mu ) \n 
_h . mult_add_st ( 1.0 - self . decay , mu_b , mu ) \n 
mu = mu_b \n 
~~ _h . add_mv ( inputs , mu . reshape ( ( 1 , mu . size ) ) , centered ) \n 
_h . mult_tt ( centered , centered , centered2 ) \n 
_h . sum_t ( centered2 , 0 , sigma2 ) \n 
_h . sqrt_t ( sigma2 , sigma_b ) \n 
_h . mult_st ( self . decay , sigma , sigma ) \n 
_h . mult_add_st ( 1.0 - self . decay , sigma_b , sigma ) \n 
sigma = sigma_b \n 
~~ _h . divide_mv ( centered , sigma . reshape ( ( 1 , sigma . size ) ) , x_hat ) \n 
_h . mult_mv ( x_hat , gamma . reshape ( ( 1 , gamma . size ) ) , out ) \n 
_h . add_mv ( out , beta . reshape ( ( 1 , beta . size ) ) , out ) \n 
~~ def backward_pass ( self , buffers ) : \n 
gamma = buffers . parameters . gamma \n 
dgamma = buffers . gradients . gamma \n 
dbeta = buffers . gradients . beta \n 
outdeltas = flatten_all_but_last ( buffers . output_deltas . default ) \n 
indeltas = flatten_all_but_last ( buffers . input_deltas . default ) \n 
m = outdeltas . shape [ 0 ] \n 
tmp = big_tmp \n 
dgamma_tmp = small_tmp \n 
_h . mult_tt ( outdeltas , x_hat , tmp ) \n 
_h . sum_t ( tmp , axis = 0 , out = dgamma_tmp ) \n 
_h . add_tt ( dgamma_tmp , dgamma , dgamma ) \n 
_h . mult_st ( 1 / m , dgamma_tmp , dgamma_tmp ) \n 
term1 = big_tmp \n 
_h . mult_mv ( x_hat , dgamma_tmp . reshape ( ( 1 , gamma . size ) ) , term1 ) \n 
dbeta_tmp = small_tmp \n 
_h . sum_t ( outdeltas , axis = 0 , out = dbeta_tmp ) \n 
_h . add_tt ( dbeta_tmp , dbeta , dbeta ) \n 
_h . mult_st ( 1 / m , dbeta_tmp , dbeta_tmp ) \n 
term2 = big_tmp \n 
term3 = big_tmp \n 
_h . subtract_tt ( outdeltas , term1 , term2 ) \n 
_h . subtract_mv ( term2 , dbeta_tmp . reshape ( ( 1 , dbeta . size ) ) , term3 ) \n 
coeff = small_tmp \n 
_h . divide_tt ( gamma , sigma_b , coeff ) \n 
term4 = big_tmp \n 
_h . mult_mv ( term3 , coeff . reshape ( ( 1 , coeff . size ) ) , term4 ) \n 
_h . add_tt ( term4 , indeltas , indeltas ) \n 
~~ ~~ from __future__ import division , print_function , unicode_literals \n 
import numpy as np \n 
from brainstorm . describable import Describable \n 
class Scorer ( Describable ) : \n 
~~~ def __init__ ( self , out_name = , targets_name = , mask_name = , \n 
name = None ) : \n 
~~~ self . out_name = out_name \n 
self . targets_name = targets_name \n 
self . mask_name = mask_name \n 
self . __name__ = name if name is not None else self . __class__ . __name__ \n 
~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ pass \n 
~~ @ staticmethod \n 
def aggregate ( errors ) : \n 
~~~ errors = np . array ( errors ) \n 
assert errors . ndim == 2 and errors . shape [ 1 ] == 2 \n 
return np . sum ( errors [ : , 1 ] ) / np . sum ( errors [ : , 0 ] ) \n 
~~ ~~ def gather_losses_and_scores ( net , scorers , scores , out_name = , \n 
targets_name = , mask_name = ) : \n 
~~~ ls = net . get_loss_values ( ) \n 
for name , loss in ls . items ( ) : \n 
~~~ scores [ name ] . append ( ( net . _buffer_manager . batch_size , loss ) ) \n 
~~ for sc in scorers : \n 
~~~ name = sc . __name__ \n 
predicted = net . get ( sc . out_name or out_name or net . output_name ) \n 
true_labels = net . get_input ( sc . targets_name ) if sc . targets_name else net . get_input ( targets_name ) \n 
mask = net . get_input ( sc . mask_name ) if sc . mask_name else ( net . get_input ( mask_name ) if mask_name else None ) \n 
predicted = _flatten_all_but_last ( predicted ) \n 
true_labels = _flatten_all_but_last ( true_labels ) \n 
mask = _flatten_all_but_last ( mask ) \n 
weight = mask . sum ( ) if mask is not None else predicted . shape [ 0 ] \n 
scores [ name ] . append ( ( weight , sc ( true_labels , predicted , mask ) ) ) \n 
~~ ~~ def aggregate_losses_and_scores ( scores , net , scorers ) : \n 
~~~ results = OrderedDict ( ) \n 
for name in net . get_loss_values ( ) : \n 
~~~ results [ name ] = _weighted_average ( scores [ name ] ) \n 
~~~ results [ sc . __name__ ] = sc . aggregate ( scores [ sc . __name__ ] ) \n 
~~ return results \n 
~~ class Accuracy ( Scorer ) : \n 
~~~ def __call__ ( self , true_labels , predicted , mask = None ) : \n 
~~~ if predicted . shape [ 1 ] > 1 : \n 
~~~ predicted = predicted . argmax ( 1 ) . reshape ( - 1 , 1 ) \n 
~~ correct = ( predicted == true_labels ) . astype ( np . float ) \n 
if mask is not None : \n 
~~~ correct *= mask \n 
~~ return np . sum ( correct ) \n 
~~ ~~ class Hamming ( Scorer ) : \n 
~~~ def __init__ ( self , threshold = 0.5 , out_name = , targets_name = , \n 
mask_name = , name = None ) : \n 
~~~ super ( Hamming , self ) . __init__ ( out_name , targets_name , mask_name , name ) \n 
self . threshold = threshold \n 
~~~ correct = np . logical_xor ( predicted < self . threshold , \n 
true_labels ) . astype ( np . float ) \n 
~~ return np . sum ( correct ) / true_labels . shape [ 1 ] \n 
~~ ~~ class MeanSquaredError ( Scorer ) : \n 
~~~ errors = ( true_labels - predicted ) ** 2 \n 
~~~ errors *= mask \n 
~~ return 0.5 * np . sum ( errors ) \n 
~~ ~~ def _flatten_all_but_last ( a ) : \n 
~~~ if a is None : \n 
~~~ return None \n 
~~ return a . reshape ( - 1 , a . shape [ - 1 ] ) \n 
~~ def _weighted_average ( errors ) : \n 
return np . sum ( errors [ : , 1 ] * errors [ : , 0 ] / np . sum ( errors [ : , 0 ] ) ) \n 
~~ from __future__ import division , print_function , unicode_literals \n 
import pytest \n 
import six \n 
from brainstorm . training . schedules import Exponential , Linear , MultiStep \n 
def test_linear ( ) : \n 
~~~ sch = Linear ( initial_value = 1.0 , final_value = 0.5 , num_changes = 5 ) \n 
epochs = [ 0 ] * 2 + [ 1 ] * 2 + [ 2 ] * 2 + [ 3 ] * 2 + [ 4 ] * 2 \n 
updates = range ( 10 ) \n 
values = [ sch ( epoch , update , , 1 , None , None , None ) \n 
for epoch , update in six . moves . zip ( epochs , updates ) ] \n 
assert values == [ 1.0 , 1.0 , 0.9 , 0.9 , 0.8 , 0.8 , 0.7 , 0.7 , 0.6 , 0.6 ] \n 
assert values == [ 1.0 , 0.9 , 0.8 , 0.7 , 0.6 , 0.5 , 0.5 , 0.5 , 0.5 , 0.5 ] \n 
values = [ sch ( epoch , update , , 3 , None , None , None ) \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.9 , 0.9 , 0.9 , 0.8 , 0.8 , 0.8 , 0.7 ] \n 
~~ def test_exponential ( ) : \n 
~~~ sch = Exponential ( initial_value = 1.0 , factor = 0.99 , minimum = 0.97 ) \n 
epochs = [ 0 ] * 4 + [ 1 ] * 4 + [ 2 ] * 4 \n 
updates = range ( 12 ) \n 
assert values == [ 1.0 ] * 4 + [ 0.99 ] * 4 + [ 0.99 * 0.99 ] * 4 \n 
assert values == [ 1.0 * ( 0.99 ** x ) for x in range ( 4 ) ] + [ 0.97 ] * 8 \n 
assert values == [ 1.0 ] * 3 + [ 0.99 ] * 3 + [ 0.9801 ] * 3 + [ 0.99 ** 3 ] * 3 \n 
~~ def test_multistep ( ) : \n 
~~~ sch = MultiStep ( initial_value = 1.0 , steps = [ 3 , 5 , 8 ] , \n 
values = [ 0.1 , 0.01 , 0.001 ] ) \n 
assert values == [ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.1 , 0.1 ] \n 
assert values == [ 1.0 , 1.0 , 1.0 , 0.1 , 0.1 , 0.01 , 0.01 , 0.01 , 0.001 , 0.001 ] \n 
with pytest . raises ( AssertionError ) : \n 
~~~ _ = sch ( 0 , 0 , , 3 , None , None , None ) \n 
# \n 
~~ ~~ import os \n 
import sys \n 
try : \n 
~~~ from unittest . mock import MagicMock \n 
~~ except ImportError : \n 
~~~ from mock import Mock as MagicMock \n 
~~ class Mock ( MagicMock ) : \n 
~~~ @ classmethod \n 
def __getattr__ ( cls , name ) : \n 
~~~ return Mock ( ) \n 
~~ ~~ MOCK_MODULES = [ , ] \n 
sys . modules . update ( ( mod_name , Mock ( ) ) for mod_name in MOCK_MODULES ) \n 
cwd = os . getcwd ( ) \n 
parent = os . path . dirname ( cwd ) \n 
sys . path . insert ( 0 , parent ) \n 
import brainstorm \n 
extensions = [ , , \n 
] \n 
templates_path = [ ] \n 
source_suffix = \n 
master_doc = \n 
project = \n 
copyright = \n 
version = brainstorm . __version__ \n 
release = brainstorm . __version__ \n 
exclude_patterns = [ ] \n 
pygments_style = \n 
on_rtd = os . environ . get ( , None ) == \n 
if not on_rtd : \n 
~~~ try : \n 
~~~ import sphinx_rtd_theme \n 
html_theme = \n 
html_theme_path = [ sphinx_rtd_theme . get_html_theme_path ( ) ] \n 
~~~ html_theme = \n 
~~ ~~ html_static_path = [ ] \n 
htmlhelp_basename = \n 
latex_elements = { \n 
} \n 
latex_documents = [ \n 
( , , , \n 
, ) , \n 
man_pages = [ \n 
[ ] , 1 ) \n 
texinfo_documents = [ \n 
, , , \n 
) , \n 
import theano . tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams \n 
from theano . tensor . nnet . conv import conv2d \n 
from theano . tensor . signal . downsample import max_pool_2d \n 
from theano . tensor . shared_randomstreams import RandomStreams \n 
from toolbox import * \n 
from modelbase import * \n 
class LM_gru ( ModelLMBase ) : \n 
~~~ def __init__ ( self , data , hp ) : \n 
~~~ super ( LM_gru , self ) . __init__ ( self . __class__ . __name__ , data , hp ) \n 
self . n_h = 256 \n 
self . dropout = 0.5 \n 
self . params = Parameters ( ) \n 
self . hiddenstates = Parameters ( ) \n 
n_tokens = self . data [ ] \n 
n_h = self . n_h \n 
scale = hp . init_scale \n 
gates = 3 \n 
with self . hiddenstates : \n 
~~~ b1_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
b2_h = shared_zeros ( ( self . hp . batch_size , n_h ) ) \n 
~~ if hp . load_model and os . path . isfile ( self . filename ) : \n 
~~~ self . params . load ( self . filename ) \n 
~~ else : \n 
~~~ with self . params : \n 
~~~ W_emb = shared_normal ( ( n_tokens , n_h ) , scale = scale ) \n 
W1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V1 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b1 = shared_zeros ( ( n_h * gates ) ) \n 
W2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
V2 = shared_normal ( ( n_h , n_h * gates ) , scale = scale * 1.5 ) \n 
b2 = shared_zeros ( ( n_h * gates , ) ) \n 
~~ ~~ def lstm ( X , h , c , W , U , b ) : \n 
~~~ g_on = T . dot ( X , W ) + T . dot ( h , U ) + b \n 
i_on = T . nnet . sigmoid ( g_on [ : , : n_h ] ) \n 
f_on = T . nnet . sigmoid ( g_on [ : , n_h : 2 * n_h ] ) \n 
o_on = T . nnet . sigmoid ( g_on [ : , 2 * n_h : 3 * n_h ] ) \n 
c = f_on * c + i_on * T . tanh ( g_on [ : , 3 * n_h : ] ) \n 
h = o_on * T . tanh ( c ) \n 
return h , c \n 
~~ def gru ( X , h , W , U , b ) : \n 
~~~ z_t = T . nnet . sigmoid ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
r_t = T . nnet . sigmoid ( T . dot ( X , W [ : , n_h : 2 * n_h ] ) + T . dot ( h , U [ : , n_h : 2 * n_h ] ) + b [ n_h : 2 * n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 2 * n_h : 3 * n_h ] ) + r_t * T . dot ( h , U [ : , 2 * n_h : 3 * n_h ] ) + b [ 2 * n_h : 3 * n_h ] ) \n 
return ( 1 - z_t ) * h + z_t * h_t \n 
~~ def sgru ( X , h , W , U , b ) : \n 
~~~ z_t = T . tanh ( T . dot ( X , W [ : , : n_h ] ) + T . dot ( h , U [ : , : n_h ] ) + b [ : n_h ] ) \n 
h_t = T . tanh ( T . dot ( X , W [ : , 1 * n_h : 2 * n_h ] ) + T . dot ( h , U [ : , 1 * n_h : 2 * n_h ] ) + b [ 1 * n_h : 2 * n_h ] ) \n 
return z_t * h_t \n 
~~ def model ( x , p , p_dropout ) : \n 
~~~ input_size = x . shape [ 1 ] \n 
h0 = dropout ( h0 , p_dropout ) \n 
cost , h1 , h2 = [ 0. , b1_h , b2_h ] \n 
for t in xrange ( 0 , self . hp . seq_size ) : \n 
~~~ if t >= self . hp . warmup_size : \n 
~~~ pyx = softmax ( T . dot ( dropout ( h2 , p_dropout ) , T . transpose ( p . W_emb ) ) ) \n 
cost += T . sum ( T . nnet . categorical_crossentropy ( pyx , theano_one_hot ( x [ t ] , n_tokens ) ) ) \n 
~~ h1 = gru ( h0 [ t ] , h1 , p . W1 , p . V1 , p . b1 ) \n 
h2 = gru ( dropout ( h1 , p_dropout ) , h2 , p . W2 , p . V2 , p . b2 ) \n 
~~ h_updates = [ ( b1_h , h1 ) , ( b2_h , h2 ) ] \n 
return cost , h_updates \n 
~~ cost , h_updates = model ( self . X , self . params , self . dropout ) \n 
te_cost , te_h_updates = model ( self . X , self . params , 0. ) \n 
self . compile ( cost , te_cost , h_updates , te_h_updates ) \n 
~~ ~~ from collections import OrderedDict \n 
from . . import utils \n 
__all__ = [ \n 
"Layer" , \n 
"MergeLayer" , \n 
class Layer ( object ) : \n 
def __init__ ( self , incoming , name = None ) : \n 
~~~ if isinstance ( incoming , tuple ) : \n 
~~~ self . input_shape = incoming \n 
self . input_layer = None \n 
~~~ self . input_shape = incoming . output_shape \n 
self . input_layer = incoming \n 
~~ self . name = name \n 
self . params = OrderedDict ( ) \n 
self . get_output_kwargs = [ ] \n 
if any ( d is not None and d <= 0 for d in self . input_shape ) : \n 
~~~ raise ValueError ( ( \n 
self . input_shape , self . name ) ) \n 
~~ ~~ @ property \n 
def output_shape ( self ) : \n 
~~~ shape = self . get_output_shape_for ( self . input_shape ) \n 
if any ( isinstance ( s , T . Variable ) for s in shape ) : \n 
"dimensions." % ( self . __class__ . __name__ , shape ) ) \n 
~~ return shape \n 
~~ def get_params ( self , ** tags ) : \n 
result = list ( self . params . keys ( ) ) \n 
only = set ( tag for tag , value in tags . items ( ) if value ) \n 
if only : \n 
~~~ result = [ param for param in result \n 
if not ( only - self . params [ param ] ) ] \n 
~~ exclude = set ( tag for tag , value in tags . items ( ) if not value ) \n 
if exclude : \n 
if not ( self . params [ param ] & exclude ) ] \n 
~~ return utils . collect_shared_vars ( result ) \n 
~~ def get_output_shape_for ( self , input_shape ) : \n 
return input_shape \n 
~~ def get_output_for ( self , input , ** kwargs ) : \n 
raise NotImplementedError \n 
~~ def add_param ( self , spec , shape , name = None , ** tags ) : \n 
if name is not None : \n 
~~~ if self . name is not None : \n 
~~~ name = "%s.%s" % ( self . name , name ) \n 
~~ ~~ param = utils . create_param ( spec , shape , name ) \n 
tags [ ] = tags . get ( , True ) \n 
self . params [ param ] = set ( tag for tag , value in tags . items ( ) if value ) \n 
return param \n 
~~ ~~ class MergeLayer ( Layer ) : \n 
def __init__ ( self , incomings , name = None ) : \n 
~~~ self . input_shapes = [ incoming if isinstance ( incoming , tuple ) \n 
else incoming . output_shape \n 
for incoming in incomings ] \n 
self . input_layers = [ None if isinstance ( incoming , tuple ) \n 
else incoming \n 
self . name = name \n 
~~ @ Layer . output_shape . getter \n 
~~~ shape = self . get_output_shape_for ( self . input_shapes ) \n 
~~ def get_output_shape_for ( self , input_shapes ) : \n 
~~ def get_output_for ( self , inputs , ** kwargs ) : \n 
~~ ~~ from mock import Mock \n 
import numpy \n 
import theano \n 
class TestAutocrop : \n 
~~~ def test_autocrop_array_shapes ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop_array_shapes \n 
crop0 = None \n 
crop1 = [ None , , , ] \n 
crop2 = [ , ] \n 
crop_bad = [ , , , ] \n 
assert autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop0 ) == [ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop1 ) == [ ( 1 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) , ( 5 , 2 , 3 , 2 ) ] \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop2 ) == [ ( 1 , 2 , 3 , 4 ) , ( 1 , 2 , 7 , 8 ) , ( 1 , 2 , 3 , 2 ) ] \n 
with pytest . raises ( ValueError ) : \n 
~~~ autocrop_array_shapes ( \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 , 8 ) , ( 5 , 4 , 3 , 2 ) ] , crop_bad ) \n 
~~ with pytest . raises ( ValueError ) : \n 
[ ( 1 , 2 , 3 , 4 ) , ( 5 , 6 , 7 ) , ( 5 , 4 , 3 , 2 , 10 ) ] , crop1 ) \n 
~~ ~~ def test_crop_inputs ( self ) : \n 
~~~ from lasagne . layers . merge import autocrop \n 
from numpy . testing import assert_array_equal \n 
crop_0 = None \n 
crop_1 = [ None , , , ] \n 
crop_l = [ , , , ] \n 
crop_c = [ , , , ] \n 
crop_u = [ , , , ] \n 
crop_x = [ , ] \n 
x0 = numpy . random . random ( ( 2 , 3 , 5 , 7 ) ) \n 
x1 = numpy . random . random ( ( 1 , 2 , 3 , 4 ) ) \n 
x2 = numpy . random . random ( ( 6 , 3 , 4 , 2 ) ) \n 
def crop_test ( cropping , inputs , expected ) : \n 
~~~ inputs = [ theano . shared ( x ) for x in inputs ] \n 
outs = autocrop ( inputs , cropping ) \n 
outs = [ o . eval ( ) for o in outs ] \n 
assert len ( outs ) == len ( expected ) \n 
for o , e in zip ( outs , expected ) : \n 
~~~ assert_array_equal ( o , e ) \n 
~~ ~~ crop_test ( crop_0 , [ x0 , x1 ] , \n 
[ x0 , x1 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 4 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 1 : 5 ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 3 : ] , x1 [ : , : , : , : ] ] ) \n 
crop_test ( crop_0 , [ x0 , x2 ] , \n 
[ x0 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 5 : ] , x2 [ : , : , : , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , : 2 ] , x2 [ : 2 , : , : , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x2 ] , \n 
[ x0 [ : , : , : 4 , 2 : 4 ] , x2 [ 2 : 4 , : , : , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x2 ] , \n 
[ x0 [ : , : , 1 : , 5 : ] , x2 [ 4 : , : , : , : ] ] ) \n 
crop_test ( crop_0 , [ x0 , x1 , x2 ] , \n 
[ x0 , x1 , x2 ] ) \n 
crop_test ( crop_1 , [ x0 , x1 , x2 ] , \n 
[ x0 [ : , : 2 , 1 : 4 , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ : , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_l , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : 3 , : 2 ] , x1 [ : , : , : , : 2 ] , x2 [ : 1 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_c , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , 1 : 4 , 2 : 4 ] , x1 [ : , : , : , 1 : 3 ] , x2 [ 2 : 3 , : 2 , : 3 , : ] ] ) \n 
crop_test ( crop_u , [ x0 , x1 , x2 ] , \n 
[ x0 [ 1 : , 1 : , 2 : , 5 : ] , x1 [ : , : , : , 2 : ] , x2 [ 5 : , 1 : , 1 : , : ] ] ) \n 
crop_test ( crop_x , [ x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
crop_test ( crop_x , [ x0 , x1 , x2 , x0 , x1 , x2 ] , \n 
[ x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] , \n 
x0 [ : 1 , : 2 , : , : ] , x1 [ : 1 , : 2 , : , : ] , x2 [ : 1 , : 2 , : , : ] ] ) \n 
~~~ crop_test ( crop_bad , [ x0 , x1 , x2 ] , \n 
~~~ crop_test ( crop_bad , [ x0 [ : , : , : , 0 ] , x1 , x2 [ : , : , : , : , None ] ] , \n 
~~ ~~ ~~ class TestConcatLayer : \n 
~~~ @ pytest . fixture \n 
def layer ( self ) : \n 
~~~ from lasagne . layers . merge import ConcatLayer \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 ) \n 
~~ @ pytest . fixture \n 
def crop_layer_0 ( self ) : \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 0 , \n 
cropping = [ ] * 2 ) \n 
def crop_layer_1 ( self ) : \n 
return ConcatLayer ( [ Mock ( ) , Mock ( ) ] , axis = 1 , \n 
~~ def test_get_output_shape_for ( self , layer ) : \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , None ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) ] ) == ( 3 , 7 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 5 ) ] ) == ( None , 7 ) \n 
~~~ layer . get_output_shape_for ( [ ( 4 , None ) , ( 3 , 5 ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 4 , None ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 5 ) , ( 4 , 5 ) ] ) \n 
~~ ~~ def test_get_output_shape_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ input_shapes = [ ( 3 , 2 ) , ( 4 , 5 ) ] \n 
result_0 = crop_layer_0 . get_output_shape_for ( input_shapes ) \n 
result_1 = crop_layer_1 . get_output_shape_for ( input_shapes ) \n 
assert result_0 == ( 7 , 2 ) \n 
assert result_1 == ( 3 , 7 ) \n 
~~ def test_get_output_for ( self , layer ) : \n 
~~~ inputs = [ theano . shared ( numpy . ones ( ( 3 , 3 ) ) ) , \n 
theano . shared ( numpy . ones ( ( 3 , 2 ) ) ) ] \n 
result = layer . get_output_for ( inputs ) \n 
result_eval = result . eval ( ) \n 
desired_result = numpy . hstack ( [ input . get_value ( ) for input in inputs ] ) \n 
assert ( result_eval == desired_result ) . all ( ) \n 
~~ def test_get_output_for_cropped ( self , crop_layer_0 , crop_layer_1 ) : \n 
~~~ x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
x1 = numpy . random . random ( ( 4 , 2 ) ) \n 
inputs = [ theano . shared ( x0 ) , \n 
theano . shared ( x1 ) ] \n 
result_0 = crop_layer_0 . get_output_for ( inputs ) . eval ( ) \n 
result_1 = crop_layer_1 . get_output_for ( inputs ) . eval ( ) \n 
desired_result_0 = numpy . concatenate ( [ x0 [ : , : 2 ] , x1 [ : , : 2 ] ] , axis = 0 ) \n 
desired_result_1 = numpy . concatenate ( [ x0 [ : 4 , : ] , x1 [ : 4 , : ] ] , axis = 1 ) \n 
assert ( result_0 == desired_result_0 ) . all ( ) \n 
assert ( result_1 == desired_result_1 ) . all ( ) \n 
~~ ~~ class TestElemwiseSumLayer : \n 
~~~ from lasagne . layers . merge import ElemwiseSumLayer \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] ) \n 
def crop_layer ( self ) : \n 
return ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , - 1 ] , \n 
~~~ assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( 3 , 2 ) , ( 3 , None ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) ] ) == ( 3 , 2 ) \n 
assert layer . get_output_shape_for ( [ ( None , 2 ) , ( None , 2 ) ] ) == ( None , 2 ) \n 
~~~ layer . get_output_shape_for ( [ ( 3 , None ) , ( 4 , 2 ) ] ) \n 
~~~ layer . get_output_shape_for ( [ ( None , 2 ) , ( 3 , 2 ) , ( 4 , 2 ) ] ) \n 
~~ ~~ def test_get_output_for ( self , layer ) : \n 
~~~ a = numpy . array ( [ [ 0 , 1 ] , [ 2 , 3 ] ] ) \n 
b = numpy . array ( [ [ 1 , 2 ] , [ 4 , 5 ] ] ) \n 
inputs = [ theano . shared ( a ) , \n 
theano . shared ( b ) ] \n 
desired_result = 2 * a - b \n 
~~ def test_get_output_for_cropped ( self , crop_layer ) : \n 
~~~ from numpy . testing import assert_array_almost_equal as aeq \n 
x0 = numpy . random . random ( ( 5 , 3 ) ) \n 
result = crop_layer . get_output_for ( inputs ) . eval ( ) \n 
desired_result = 2 * x0 [ : 4 , : 2 ] - x1 [ : 4 , : 2 ] \n 
aeq ( result , desired_result ) \n 
~~ def test_bad_coeffs_fails ( self , layer ) : \n 
~~~ ElemwiseSumLayer ( [ Mock ( ) , Mock ( ) ] , coeffs = [ 2 , 3 , - 1 ] ) \n 
~~ ~~ ~~ class TestElemwiseMergeLayerMul : \n 
~~~ import theano . tensor as T \n 
from lasagne . layers . merge import ElemwiseMergeLayer \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . mul ) \n 
desired_result = a * b \n 
~~ ~~ class TestElemwiseMergeLayerMaximum : \n 
return ElemwiseMergeLayer ( [ Mock ( ) , Mock ( ) ] , merge_function = T . maximum ) \n 
desired_result = numpy . maximum ( a , b ) \n 
def tokenProgressFunc ( state = "update" , action = None , text = None , tick = 0 ) : \n 
~~ def build ( \n 
documentPath , \n 
outputUFOFormatVersion = 2 , \n 
roundGeometry = True , \n 
verbose = True , \n 
logPath = None , \n 
progressFunc = None , \n 
) : \n 
from mutatorMath . ufo . document import DesignSpaceDocumentReader \n 
import os , glob \n 
if os . path . isdir ( documentPath ) : \n 
~~~ todo = glob . glob ( os . path . join ( documentPath , "*.designspace" ) ) \n 
~~~ todo = [ documentPath ] \n 
~~ results = [ ] \n 
for path in todo : \n 
~~~ reader = DesignSpaceDocumentReader ( \n 
path , \n 
ufoVersion = outputUFOFormatVersion , \n 
roundGeometry = roundGeometry , \n 
verbose = verbose , \n 
logPath = logPath , \n 
progressFunc = progressFunc \n 
) \n 
reader . process ( ) \n 
results . append ( reader . results ) \n 
~~ reader = None \n 
return results \n 
from __future__ import print_function \n 
import argparse \n 
import datetime \n 
import json \n 
import multiprocessing \n 
import os \n 
import random \n 
import threading \n 
import time \n 
from PIL import Image \n 
import six . moves . cPickle as pickle \n 
from six . moves import queue \n 
import chainer \n 
from chainer import computational_graph \n 
from chainer import cuda \n 
from chainer import optimizers \n 
from chainer import serializers \n 
parser = argparse . ArgumentParser ( \n 
description = ) \n 
parser . add_argument ( , help = ) \n 
parser . add_argument ( , , default = , \n 
help = ) \n 
parser . add_argument ( , , type = int , default = 32 , \n 
parser . add_argument ( , , type = int , default = 250 , \n 
parser . add_argument ( , , default = 10 , type = int , \n 
parser . add_argument ( , , default = - 1 , type = int , \n 
parser . add_argument ( , , default = 20 , type = int , \n 
parser . add_argument ( , default = , \n 
args = parser . parse_args ( ) \n 
if args . gpu >= 0 : \n 
~~~ cuda . check_cuda_available ( ) \n 
~~ xp = cuda . cupy if args . gpu >= 0 else np \n 
assert 50000 % args . val_batchsize == 0 \n 
def load_image_list ( path , root ) : \n 
~~~ tuples = [ ] \n 
for line in open ( path ) : \n 
~~~ pair = line . strip ( ) . split ( ) \n 
tuples . append ( ( os . path . join ( root , pair [ 0 ] ) , np . int32 ( pair [ 1 ] ) ) ) \n 
~~ return tuples \n 
~~ train_list = load_image_list ( args . train , args . root ) \n 
val_list = load_image_list ( args . val , args . root ) \n 
mean_image = pickle . load ( open ( args . mean , ) ) \n 
if args . arch == : \n 
~~~ import nin \n 
model = nin . NIN ( ) \n 
~~ elif args . arch == : \n 
~~~ import alex \n 
model = alex . Alex ( ) \n 
~~~ import alexbn \n 
model = alexbn . AlexBN ( ) \n 
~~~ import googlenet \n 
model = googlenet . GoogLeNet ( ) \n 
~~~ import googlenetbn \n 
model = googlenetbn . GoogLeNetBN ( ) \n 
~~~ raise ValueError ( ) \n 
~~ if args . gpu >= 0 : \n 
~~~ cuda . get_device ( args . gpu ) . use ( ) \n 
model . to_gpu ( ) \n 
~~ optimizer = optimizers . MomentumSGD ( lr = 0.01 , momentum = 0.9 ) \n 
optimizer . setup ( model ) \n 
if args . initmodel : \n 
~~~ print ( , args . initmodel ) \n 
serializers . load_hdf5 ( args . initmodel , model ) \n 
~~ if args . resume : \n 
~~~ print ( , args . resume ) \n 
serializers . load_hdf5 ( args . resume , optimizer ) \n 
~~ data_q = queue . Queue ( maxsize = 1 ) \n 
res_q = queue . Queue ( ) \n 
cropwidth = 256 - model . insize \n 
def read_image ( path , center = False , flip = False ) : \n 
~~~ image = np . asarray ( Image . open ( path ) ) . transpose ( 2 , 0 , 1 ) \n 
if center : \n 
~~~ top = left = cropwidth / 2 \n 
~~~ top = random . randint ( 0 , cropwidth - 1 ) \n 
left = random . randint ( 0 , cropwidth - 1 ) \n 
~~ bottom = model . insize + top \n 
right = model . insize + left \n 
image = image [ : , top : bottom , left : right ] . astype ( np . float32 ) \n 
image -= mean_image [ : , top : bottom , left : right ] \n 
image /= 255 \n 
if flip and random . randint ( 0 , 1 ) == 0 : \n 
~~~ return image [ : , : , : : - 1 ] \n 
~~~ return image \n 
~~ ~~ def feed_data ( ) : \n 
~~~ i = 0 \n 
count = 0 \n 
x_batch = np . ndarray ( \n 
( args . batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
y_batch = np . ndarray ( ( args . batchsize , ) , dtype = np . int32 ) \n 
val_x_batch = np . ndarray ( \n 
( args . val_batchsize , 3 , model . insize , model . insize ) , dtype = np . float32 ) \n 
val_y_batch = np . ndarray ( ( args . val_batchsize , ) , dtype = np . int32 ) \n 
batch_pool = [ None ] * args . batchsize \n 
val_batch_pool = [ None ] * args . val_batchsize \n 
pool = multiprocessing . Pool ( args . loaderjob ) \n 
data_q . put ( ) \n 
for epoch in six . moves . range ( 1 , 1 + args . epoch ) : \n 
~~~ print ( , epoch , file = sys . stderr ) \n 
print ( , optimizer . lr , file = sys . stderr ) \n 
perm = np . random . permutation ( len ( train_list ) ) \n 
for idx in perm : \n 
~~~ path , label = train_list [ idx ] \n 
batch_pool [ i ] = pool . apply_async ( read_image , ( path , False , True ) ) \n 
y_batch [ i ] = label \n 
i += 1 \n 
if i == args . batchsize : \n 
~~~ for j , x in enumerate ( batch_pool ) : \n 
~~~ x_batch [ j ] = x . get ( ) \n 
~~ data_q . put ( ( x_batch . copy ( ) , y_batch . copy ( ) ) ) \n 
i = 0 \n 
~~ count += 1 \n 
if count % 1000 == 0 : \n 
~~~ data_q . put ( ) \n 
j = 0 \n 
for path , label in val_list : \n 
~~~ val_batch_pool [ j ] = pool . apply_async ( \n 
read_image , ( path , True , False ) ) \n 
val_y_batch [ j ] = label \n 
j += 1 \n 
if j == args . val_batchsize : \n 
~~~ for k , x in enumerate ( val_batch_pool ) : \n 
~~~ val_x_batch [ k ] = x . get ( ) \n 
~~ data_q . put ( ( val_x_batch . copy ( ) , val_y_batch . copy ( ) ) ) \n 
~~ ~~ data_q . put ( ) \n 
~~ ~~ optimizer . lr *= 0.97 \n 
~~ pool . close ( ) \n 
pool . join ( ) \n 
~~ def log_result ( ) : \n 
~~~ train_count = 0 \n 
train_cur_loss = 0 \n 
train_cur_accuracy = 0 \n 
begin_at = time . time ( ) \n 
val_begin_at = None \n 
while True : \n 
~~~ result = res_q . get ( ) \n 
if result == : \n 
~~~ print ( file = sys . stderr ) \n 
break \n 
~~ elif result == : \n 
train = True \n 
if val_begin_at is not None : \n 
~~~ begin_at += time . time ( ) - val_begin_at \n 
~~ continue \n 
train = False \n 
val_count = val_loss = val_accuracy = 0 \n 
val_begin_at = time . time ( ) \n 
continue \n 
~~ loss , accuracy = result \n 
if train : \n 
~~~ train_count += 1 \n 
duration = time . time ( ) - begin_at \n 
throughput = train_count * args . batchsize / duration \n 
sys . stderr . write ( \n 
. format ( train_count , train_count * args . batchsize , \n 
datetime . timedelta ( seconds = duration ) , throughput ) ) \n 
train_cur_loss += loss \n 
train_cur_accuracy += accuracy \n 
if train_count % 1000 == 0 : \n 
~~~ mean_loss = train_cur_loss / 1000 \n 
mean_error = 1 - train_cur_accuracy / 1000 \n 
print ( file = sys . stderr ) \n 
print ( json . dumps ( { : , : train_count , \n 
: mean_error , : mean_loss } ) ) \n 
sys . stdout . flush ( ) \n 
~~ ~~ else : \n 
~~~ val_count += args . val_batchsize \n 
duration = time . time ( ) - val_begin_at \n 
throughput = val_count / duration \n 
. format ( val_count / args . val_batchsize , val_count , \n 
val_loss += loss \n 
val_accuracy += accuracy \n 
if val_count == 50000 : \n 
~~~ mean_loss = val_loss * args . val_batchsize / 50000 \n 
mean_error = 1 - val_accuracy * args . val_batchsize / 50000 \n 
~~ ~~ ~~ ~~ def train_loop ( ) : \n 
~~~ graph_generated = False \n 
~~~ while data_q . empty ( ) : \n 
~~~ time . sleep ( 0.1 ) \n 
~~ inp = data_q . get ( ) \n 
~~~ res_q . put ( ) \n 
model . train = True \n 
serializers . save_hdf5 ( args . out , model ) \n 
serializers . save_hdf5 ( args . outstate , optimizer ) \n 
model . train = False \n 
~~ volatile = if model . train else \n 
x = chainer . Variable ( xp . asarray ( inp [ 0 ] ) , volatile = volatile ) \n 
t = chainer . Variable ( xp . asarray ( inp [ 1 ] ) , volatile = volatile ) \n 
if model . train : \n 
~~~ optimizer . update ( model , x , t ) \n 
if not graph_generated : \n 
~~~ with open ( , ) as o : \n 
~~~ o . write ( computational_graph . build_computational_graph ( \n 
( model . loss , ) ) . dump ( ) ) \n 
~~ print ( , file = sys . stderr ) \n 
graph_generated = True \n 
~~~ model ( x , t ) \n 
~~ res_q . put ( ( float ( model . loss . data ) , float ( model . accuracy . data ) ) ) \n 
del x , t \n 
~~ ~~ feeder = threading . Thread ( target = feed_data ) \n 
feeder . daemon = True \n 
feeder . start ( ) \n 
logger = threading . Thread ( target = log_result ) \n 
logger . daemon = True \n 
logger . start ( ) \n 
train_loop ( ) \n 
feeder . join ( ) \n 
logger . join ( ) \n 
import lxmls . readers . pos_corpus as pcc \n 
from os import path \n 
import pickle \n 
corpus = pcc . PostagCorpus ( ) \n 
input_data = path . join ( \n 
path . dirname ( __file__ ) , \n 
"../../data/train-02-21.conll" ) \n 
train_seq = corpus . read_sequence_list_conll ( input_data , max_sent_len = 15 , max_nr_sent = 1000 ) \n 
pickle . dump ( ( corpus . word_dict , corpus . tag_dict ) , open ( , ) ) \n 
with open ( , ) as output : \n 
~~~ for seq in train_seq : \n 
~~~ words = [ corpus . word_dict . get_label_name ( seq . x [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
tags = [ corpus . tag_dict . get_label_name ( seq . y [ i ] ) for i in xrange ( len ( seq ) ) ] \n 
s = . join ( [ . join ( [ word , tag ] ) for word , tag in zip ( words , tags ) ] ) \n 
output . write ( s + ) \n 
~~ ~~ import sys \n 
from lxmls . parsing . dependency_reader import * \n 
from lxmls . parsing . dependency_writer import * \n 
from lxmls . parsing . dependency_features import * \n 
from lxmls . parsing . dependency_decoder import * \n 
from lxmls . util . my_math_utils import * \n 
class DependencyParser ( ) : \n 
~~~ \n 
def __init__ ( self ) : \n 
~~~ self . trained = False \n 
self . projective = False \n 
self . language = "" \n 
self . weights = [ ] \n 
self . decoder = DependencyDecoder ( ) \n 
self . reader = DependencyReader ( ) \n 
self . writer = DependencyWriter ( ) \n 
self . features = DependencyFeatures ( ) \n 
~~ def read_data ( self , language ) : \n 
~~~ self . language = language \n 
self . reader . load ( language ) \n 
self . features . create_dictionary ( self . reader . train_instances ) \n 
~~ def train_perceptron ( self , n_epochs ) : \n 
self . weights = np . zeros ( self . features . n_feats ) \n 
total = np . zeros ( self . features . n_feats ) \n 
for epoch in range ( n_epochs ) : \n 
n_mistakes = 0 \n 
n_tokens = 0 \n 
n_instances = 0 \n 
for instance in self . reader . train_instances : \n 
~~~ feats = self . features . create_features ( instance ) \n 
scores = self . features . compute_scores ( feats , self . weights ) \n 
if self . projective : \n 
~~~ heads_pred = self . decoder . parse_proj ( scores ) \n 
~~~ heads_pred = self . decoder . parse_nonproj ( scores ) \n 
~~ for m in range ( np . size ( heads_pred ) ) : \n 
~~~ for f in feats [ instance . heads [ m ] ] [ m ] : \n 
~~~ if f < 0 : \n 
~~~ continue \n 
~~ self . weights [ f ] += 1.0 \n 
~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~ self . weights [ f ] -= 1.0 \n 
~~ n_mistakes += 1 \n 
~~ n_tokens += 1 \n 
~~ n_instances += 1 \n 
total += self . weights \n 
~~ self . weights = total / np . double ( n_epochs ) \n 
~~ def train_crf_sgd ( self , n_epochs , sigma , eta0 = 0.001 ) : \n 
t = 0 \n 
t0 = 1.0 / ( sigma * eta0 ) \n 
objective = 0.0 \n 
~~~ eta = 1.0 / ( sigma * ( t + t0 ) ) \n 
feats = self . features . create_features ( instance ) \n 
marginals , logZ = self . decoder . parse_marginals_nonproj ( scores ) \n 
for h in range ( np . size ( marginals , 0 ) ) : \n 
~~~ for m in range ( 1 , np . size ( marginals , 1 ) ) : \n 
~~~ if feats [ h ] [ m ] == None : \n 
~~ for f in feats [ h ] [ m ] : \n 
~~ self . weights [ f ] -= eta * marginals [ h , m ] \n 
~~ ~~ ~~ score_corr = 0.0 \n 
for m in range ( 1 , np . size ( instance . heads ) ) : \n 
~~~ h = instance . heads [ m ] \n 
score_corr += scores [ h , m ] \n 
for f in feats [ h ] [ m ] : \n 
~~ self . weights [ f ] += eta \n 
~~ ~~ objective += 0.5 * sigma * np . dot ( self . weights , self . weights ) - score_corr + logZ \n 
n_instances += 1 \n 
t += 1 \n 
~~ ~~ def test ( self ) : \n 
~~~ n_mistakes = 0 \n 
arr_heads_pred = [ ] ; \n 
for instance in self . reader . test_instances : \n 
~~ ~~ for f in feats [ heads_pred [ m ] ] [ m ] : \n 
~~ ~~ n_mistakes += 1 \n 
arr_heads_pred . append ( heads_pred ) \n 
self . writer . save ( self . language , arr_heads_pred ) \n 
~~ ~~ from lxmls . sequences . label_dictionary import * \n 
import pdb \n 
################# \n 
class IDFeatures : \n 
def __init__ ( self , dataset ) : \n 
self . feature_dict = LabelDictionary ( ) \n 
self . feature_list = [ ] \n 
self . add_features = False \n 
self . dataset = dataset \n 
self . node_feature_cache = { } \n 
self . initial_state_feature_cache = { } \n 
self . final_state_feature_cache = { } \n 
self . edge_feature_cache = { } \n 
~~ def get_num_features ( self ) : \n 
~~~ return len ( self . feature_dict ) \n 
~~ def build_features ( self ) : \n 
self . add_features = True \n 
for sequence in self . dataset . seq_list : \n 
~~~ initial_features , transition_features , final_features , emission_features = self . get_sequence_features ( sequence ) \n 
self . feature_list . append ( [ initial_features , transition_features , final_features , emission_features ] ) \n 
~~ self . add_features = False \n 
~~ def get_sequence_features ( self , sequence ) : \n 
emission_features = [ ] \n 
initial_features = [ ] \n 
transition_features = [ ] \n 
final_features = [ ] \n 
features = [ ] \n 
features = self . add_initial_features ( sequence , sequence . y [ 0 ] , features ) \n 
initial_features . append ( features ) \n 
for pos , tag in enumerate ( sequence . y ) : \n 
~~~ features = [ ] \n 
features = self . add_emission_features ( sequence , pos , sequence . y [ pos ] , features ) \n 
emission_features . append ( features ) \n 
if pos > 0 : \n 
~~~ prev_tag = sequence . y [ pos - 1 ] \n 
features = self . add_transition_features ( sequence , pos - 1 , tag , prev_tag , features ) \n 
transition_features . append ( features ) \n 
~~ ~~ features = [ ] \n 
features = self . add_final_features ( sequence , sequence . y [ - 1 ] , features ) \n 
final_features . append ( features ) \n 
return initial_features , transition_features , final_features , emission_features \n 
#f(t,y_t,X) \n 
~~ def get_emission_features ( self , sequence , pos , y ) : \n 
~~~ all_feat = [ ] \n 
x = sequence . x [ pos ] \n 
if ( x not in self . node_feature_cache ) : \n 
~~~ self . node_feature_cache [ x ] = { } \n 
~~ if ( y not in self . node_feature_cache [ x ] ) : \n 
~~~ node_idx = [ ] \n 
node_idx = self . add_emission_features ( sequence , pos , y , node_idx ) \n 
self . node_feature_cache [ x ] [ y ] = node_idx \n 
~~ idx = self . node_feature_cache [ x ] [ y ] \n 
all_feat = idx [ : ] \n 
return all_feat \n 
#f(t,y_t,y_(t-1),X) \n 
~~ def get_transition_features ( self , sequence , pos , y , y_prev ) : \n 
~~~ assert ( pos >= 0 and pos < len ( sequence . x ) ) , pdb . set_trace ( ) \n 
if ( y not in self . edge_feature_cache ) : \n 
~~~ self . edge_feature_cache [ y ] = { } \n 
~~ if ( y_prev not in self . edge_feature_cache [ y ] ) : \n 
~~~ edge_idx = [ ] \n 
edge_idx = self . add_transition_features ( sequence , pos , y , y_prev , edge_idx ) \n 
self . edge_feature_cache [ y ] [ y_prev ] = edge_idx \n 
~~ return self . edge_feature_cache [ y ] [ y_prev ] \n 
~~ def get_initial_features ( self , sequence , y ) : \n 
~~~ if ( y not in self . initial_state_feature_cache ) : \n 
edge_idx = self . add_initial_features ( sequence , y , edge_idx ) \n 
self . initial_state_feature_cache [ y ] = edge_idx \n 
~~ return self . initial_state_feature_cache [ y ] \n 
~~ def get_final_features ( self , sequence , y_prev ) : \n 
~~~ if ( y_prev not in self . final_state_feature_cache ) : \n 
edge_idx = self . add_final_features ( sequence , y_prev , edge_idx ) \n 
self . final_state_feature_cache [ y_prev ] = edge_idx \n 
~~ return self . final_state_feature_cache [ y_prev ] \n 
~~ def add_initial_features ( self , sequence , y , features ) : \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y ) \n 
feat_name = "init_tag:%s" % ( y_name ) \n 
feat_id = self . add_feature ( feat_name ) \n 
if ( feat_id != - 1 ) : \n 
~~~ features . append ( feat_id ) \n 
~~ return features \n 
~~ def add_final_features ( self , sequence , y_prev , features ) : \n 
~~~ y_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
feat_name = "final_prev_tag:%s" % ( y_name ) \n 
~~ def add_emission_features ( self , sequence , pos , y , features ) : \n 
y_name = self . dataset . y_dict . get_label_name ( y ) \n 
x_name = self . dataset . x_dict . get_label_name ( x ) \n 
feat_name = "id:%s::%s" % ( x_name , y_name ) \n 
if feat_id != - 1 : \n 
~~ def add_transition_features ( self , sequence , pos , y , y_prev , features ) : \n 
assert pos < len ( sequence . x ) - 1 , pdb . set_trace ( ) \n 
y_prev_name = self . dataset . y_dict . get_label_name ( y_prev ) \n 
feat_name = "prev_tag:%s::%s" % ( y_prev_name , y_name ) \n 
~~ def add_feature ( self , feat_name ) : \n 
if ( feat_name in self . feature_dict ) : \n 
~~~ return self . feature_dict [ feat_name ] \n 
~~ if not self . add_features : \n 
~~~ return - 1 \n 
~~ return self . feature_dict . add ( feat_name ) \n 
~~ ~~ from __future__ import division \n 
from collections import defaultdict \n 
import itertools \n 
from sklearn import cluster , preprocessing , manifold \n 
from datetime import datetime \n 
class KeplerMapper ( object ) : \n 
~~~ def __init__ ( self , verbose = 2 ) : \n 
~~~ self . verbose = verbose \n 
self . chunk_dist = [ ] \n 
self . overlap_dist = [ ] \n 
self . d = [ ] \n 
self . nr_cubes = 0 \n 
self . overlap_perc = 0 \n 
self . clusterer = False \n 
~~ def fit_transform ( self , X , projection = "sum" , scaler = preprocessing . MinMaxScaler ( ) ) : \n 
~~~ self . scaler = scaler \n 
self . projection = str ( projection ) \n 
~~~ reducer = projection \n 
if self . verbose > 0 : \n 
~~~ projection . set_params ( ** { "verbose" : self . verbose } ) \n 
~~ except : \n 
~~ X = reducer . fit_transform ( X ) \n 
~~ if isinstance ( projection , str ) : \n 
~~~ if self . verbose > 0 : \n 
~~~ X = np . sum ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . mean ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . median ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . max ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . min ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X = np . std ( X , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~~ X_mean = np . mean ( X , axis = 0 ) \n 
X = np . sum ( np . sqrt ( ( X - X_mean ) ** 2 ) , axis = 1 ) . reshape ( ( X . shape [ 0 ] , 1 ) ) \n 
~~ ~~ if isinstance ( projection , list ) : \n 
~~ X = X [ : , np . array ( projection ) ] \n 
~~ if scaler is not None : \n 
~~ X = scaler . fit_transform ( X ) \n 
~~ return X \n 
~~ def map ( self , projected_X , inverse_X = None , clusterer = cluster . DBSCAN ( eps = 0.5 , min_samples = 3 ) , nr_cubes = 10 , overlap_perc = 0.1 ) : \n 
~~~ start = datetime . now ( ) \n 
def cube_coordinates_all ( nr_cubes , nr_dimensions ) : \n 
~~~ l = [ ] \n 
for x in range ( nr_cubes ) : \n 
~~~ l += [ x ] * nr_dimensions \n 
~~ return [ np . array ( list ( f ) ) for f in sorted ( set ( itertools . permutations ( l , nr_dimensions ) ) ) ] \n 
~~ nodes = defaultdict ( list ) \n 
links = defaultdict ( list ) \n 
complex = { } \n 
self . nr_cubes = nr_cubes \n 
self . clusterer = clusterer \n 
self . overlap_perc = overlap_perc \n 
~~ if inverse_X is None : \n 
~~~ inverse_X = projected_X \n 
~~ self . chunk_dist = ( np . max ( projected_X , axis = 0 ) - np . min ( projected_X , axis = 0 ) ) / nr_cubes \n 
self . overlap_dist = self . overlap_perc * self . chunk_dist \n 
self . d = np . min ( projected_X , axis = 0 ) \n 
di = np . array ( [ x for x in range ( projected_X . shape [ 1 ] ) ] ) \n 
ids = np . array ( [ x for x in range ( projected_X . shape [ 0 ] ) ] ) \n 
projected_X = np . c_ [ ids , projected_X ] \n 
inverse_X = np . c_ [ ids , inverse_X ] \n 
~~~ total_cubes = len ( cube_coordinates_all ( nr_cubes , projected_X . shape [ 1 ] ) ) \n 
~~ for i , coor in enumerate ( cube_coordinates_all ( nr_cubes , di . shape [ 0 ] ) ) : \n 
~~~ hypercube = projected_X [ np . invert ( np . any ( ( projected_X [ : , di + 1 ] >= self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) & \n 
( projected_X [ : , di + 1 ] < self . d [ di ] + ( coor * self . chunk_dist [ di ] ) + self . chunk_dist [ di ] + self . overlap_dist [ di ] ) == False , axis = 1 ) ) ] \n 
if self . verbose > 1 : \n 
( hypercube . shape [ 0 ] , i , total_cubes , self . d [ di ] + ( coor * self . chunk_dist [ di ] ) ) ) \n 
~~ if hypercube . shape [ 0 ] > 0 : \n 
~~~ inverse_x = inverse_X [ [ int ( nn ) for nn in hypercube [ : , 0 ] ] ] \n 
clusterer . fit ( inverse_x [ : , 1 : ] ) \n 
~~ for a in np . c_ [ hypercube [ : , 0 ] , clusterer . labels_ ] : \n 
nodes [ cluster_id ] . append ( int ( a [ 0 ] ) ) \n 
~~ ~~ ~~ else : \n 
~~~ if self . verbose > 1 : \n 
~~ ~~ ~~ candidates = itertools . combinations ( nodes . keys ( ) , 2 ) \n 
for candidate in candidates : \n 
~~~ if len ( nodes [ candidate [ 0 ] ] + nodes [ candidate [ 1 ] ] ) != len ( set ( nodes [ candidate [ 0 ] ] + nodes [ candidate [ 1 ] ] ) ) : \n 
~~~ links [ candidate [ 0 ] ] . append ( candidate [ 1 ] ) \n 
~~ ~~ if self . verbose > 0 : \n 
~~~ nr_links = 0 \n 
for k in links : \n 
~~~ nr_links += len ( links [ k ] ) \n 
~~ complex [ "nodes" ] = nodes \n 
complex [ "links" ] = links \n 
complex [ "meta" ] = self . projection \n 
return complex \n 
graph_link_distance = 30 , graph_gravity = 0.1 , graph_charge = - 120 , custom_tooltips = None , width_html = 0 , \n 
height_html = 0 , show_tooltips = True , show_title = True , show_meta = True ) : \n 
~~~ json_s = { } \n 
json_s [ "nodes" ] = [ ] \n 
json_s [ "links" ] = [ ] \n 
k2e = { } \n 
for e , k in enumerate ( complex [ "nodes" ] ) : \n 
~~~ if custom_tooltips is not None : \n 
if color_function == "average_signal_cluster" : \n 
~~~ tooltip_i = int ( ( ( sum ( [ f for f in custom_tooltips [ complex [ "nodes" ] [ k ] ] ] ) / len ( custom_tooltips [ complex [ "nodes" ] [ k ] ] ) ) * 30 ) ) \n 
json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex [ "nodes" ] [ k ] ) ) ) , "color" : str ( tooltip_i ) } ) \n 
~~~ json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex [ "nodes" ] [ k ] ) ) ) , "color" : str ( k . split ( "_" ) [ 0 ] ) } ) \n 
json_s [ "nodes" ] . append ( { "name" : str ( k ) , "tooltip" : tooltip_s , "group" : 2 * int ( np . log ( len ( complex [ "nodes" ] [ k ] ) ) ) , "color" : str ( k . split ( "_" ) [ 0 ] ) } ) \n 
~~ k2e [ k ] = e \n 
~~ for k in complex [ "links" ] : \n 
~~~ for link in complex [ "links" ] [ k ] : \n 
~~~ json_s [ "links" ] . append ( { "source" : k2e [ k ] , "target" : k2e [ link ] , "value" : 1 } ) \n 
~~ ~~ if width_html == 0 : \n 
~~~ width_css = "100%" \n 
width_js = \'document.getElementById("holder").offsetWidth-20\' \n 
~~~ width_css = "%spx" % width_html \n 
width_js = "%s" % width_html \n 
~~ if height_html == 0 : \n 
~~~ height_css = "100%" \n 
height_js = \'document.getElementById("holder").offsetHeight-20\' \n 
~~~ height_css = "%spx" % height_html \n 
height_js = "%s" % height_html \n 
~~ if show_tooltips == False : \n 
~~~ tooltips_display = "" \n 
~~ if show_meta == False : \n 
~~~ meta_display = "" \n 
~~ if show_title == False : \n 
~~~ title_display = "" \n 
~~ with open ( path_html , "wb" ) as outfile : \n 
outfile . write ( html . encode ( "utf-8" ) ) \n 
~~ if self . verbose > 0 : \n 
~~ ~~ ~~ import sys , os \n 
sys . path . insert ( 0 , os . path . abspath ( ) ) \n 
needs_sphinx = \n 
extensions = [ , , , ] \n 
import pkg_resources \n 
~~~ release = pkg_resources . get_distribution ( ) . version \n 
~~ except pkg_resources . DistributionNotFound : \n 
~~~ print \n 
print \n 
sys . exit ( 1 ) \n 
~~ del pkg_resources \n 
version = . join ( release . split ( ) [ : 2 ] ) \n 
html_static_path = [ ] \n 
html_use_smartypants = True \n 
import math \n 
import glob \n 
import re \n 
import subprocess \n 
from shutil import rmtree \n 
import logging \n 
from mrec import load_sparse_matrix , save_recommender \n 
class ItemSimilarityRunner ( object ) : \n 
~~~ def run ( self , view , model , input_format , trainfile , num_engines , simsdir , overwrite , max_sims , simsfile , modelfile ) : \n 
~~~ logging . info ( ) \n 
dataset = load_sparse_matrix ( input_format , trainfile ) \n 
num_users , num_items = dataset . shape \n 
del dataset \n 
logging . info ( , num_users , num_items ) \n 
logging . info ( . format ( simsdir ) ) \n 
subprocess . check_call ( [ , , simsdir ] ) \n 
done = [ ] \n 
if not overwrite : \n 
done . extend ( self . find_done ( simsdir ) ) \n 
if done : \n 
~~~ logging . info ( . format ( len ( done ) ) ) \n 
~~ ~~ logging . info ( ) \n 
tasks = self . create_tasks ( model , input_format , trainfile , simsdir , num_items , num_engines , max_sims , done ) \n 
if num_engines > 0 : \n 
~~~ logging . info ( \n 
, len ( tasks ) ) \n 
async_job = view . map_async ( process , tasks , retries = 2 ) \n 
results = async_job . get ( ) \n 
results = [ process ( task ) for task in tasks ] \n 
~~ logging . info ( ) \n 
done = self . find_done ( simsdir ) \n 
remaining = len ( tasks ) - len ( done ) \n 
if remaining == 0 : \n 
logging . info ( . format ( len ( done ) ) ) \n 
paths = [ os . path . join ( simsdir , . format ( start , end ) ) for start , end in done ] \n 
cmd = [ ] + paths \n 
subprocess . check_call ( cmd , stdout = open ( simsfile , ) ) \n 
logging . info ( ) \n 
rmtree ( simsdir ) \n 
logging . info ( , \n 
num_items , type ( model ) . __name__ , simsfile ) \n 
model . load_similarity_matrix ( simsfile , num_items ) \n 
save_recommender ( model , modelfile ) \n 
~~~ logging . error ( . format ( remaining , len ( tasks ) ) ) \n 
logging . error ( ) \n 
~~ ~~ def find_done ( self , outdir ) : \n 
~~~ success_files = glob . glob ( os . path . join ( outdir , ) ) \n 
r = re . compile ( ) \n 
for path in success_files : \n 
~~~ m = r . match ( path ) \n 
start = int ( m . group ( 1 ) ) \n 
end = int ( m . group ( 2 ) ) \n 
done . append ( ( start , end ) ) \n 
~~ return done \n 
~~ def create_tasks ( self , model , input_format , trainfile , outdir , num_items , num_engines , max_similar_items , done ) : \n 
~~~ if num_engines == 0 : \n 
~~~ num_engines = 1 \n 
~~ items_per_engine = int ( math . ceil ( float ( num_items ) / num_engines ) ) \n 
tasks = [ ] \n 
for start in xrange ( 0 , num_items , items_per_engine ) : \n 
~~~ end = min ( num_items , start + items_per_engine ) \n 
if ( start , end ) not in done : \n 
~~~ tasks . append ( ( model , input_format , trainfile , outdir , start , end , max_similar_items ) ) \n 
~~ ~~ return tasks \n 
~~ ~~ def process ( task ) : \n 
from mrec import load_fast_sparse_matrix \n 
model , input_format , trainfile , outdir , start , end , max_similar_items = task \n 
dataset = load_fast_sparse_matrix ( input_format , trainfile ) \n 
if hasattr ( model , ) : \n 
~~~ model . similarity_matrix = None \n 
~~ outfile = os . path . join ( outdir , . format ( start , end ) ) \n 
out = open ( outfile , ) \n 
for j in xrange ( start , end ) : \n 
~~~ w = model . get_similar_items ( j , max_similar_items = max_similar_items , dataset = dataset ) \n 
for k , v in w : \n 
~~ ~~ out . close ( ) \n 
cmd = [ , os . path . join ( outdir , . format ( start , end ) ) ] \n 
subprocess . check_call ( cmd ) \n 
return start , end \n 
import inspect \n 
NO_DEFAULT = object ( ) \n 
def memoize_default ( default = NO_DEFAULT , evaluator_is_first_arg = False , second_arg_is_evaluator = False ) : \n 
def func ( function ) : \n 
~~~ def wrapper ( obj , * args , ** kwargs ) : \n 
~~~ if evaluator_is_first_arg : \n 
~~~ cache = obj . memoize_cache \n 
~~~ cache = args [ 0 ] . memoize_cache \n 
~~~ cache = obj . _evaluator . memoize_cache \n 
~~ try : \n 
~~~ memo = cache [ function ] \n 
~~ except KeyError : \n 
~~~ memo = { } \n 
cache [ function ] = memo \n 
~~ key = ( obj , args , frozenset ( kwargs . items ( ) ) ) \n 
if key in memo : \n 
~~~ return memo [ key ] \n 
~~~ if default is not NO_DEFAULT : \n 
~~~ memo [ key ] = default \n 
~~ rv = function ( obj , * args , ** kwargs ) \n 
if inspect . isgenerator ( rv ) : \n 
~~~ rv = list ( rv ) \n 
~~ memo [ key ] = rv \n 
return rv \n 
~~ ~~ return wrapper \n 
~~ return func \n 
~~ class CachedMetaClass ( type ) : \n 
@ memoize_default ( None , second_arg_is_evaluator = True ) \n 
def __call__ ( self , * args , ** kwargs ) : \n 
~~~ return super ( CachedMetaClass , self ) . __call__ ( * args , ** kwargs ) \n 
from __future__ import absolute_import \n 
import __main__ \n 
from collections import namedtuple \n 
from jedi import Interpreter \n 
from jedi . api . helpers import completion_parts \n 
from jedi . parser . user_context import UserContext \n 
def setup_readline ( namespace_module = __main__ ) : \n 
class JediRL ( object ) : \n 
~~~ def complete ( self , text , state ) : \n 
if state == 0 : \n 
~~~ sys . path . insert ( 0 , os . getcwd ( ) ) \n 
~~~ interpreter = Interpreter ( text , [ namespace_module . __dict__ ] ) \n 
path = UserContext ( text , ( 1 , len ( text ) ) ) . get_path_until_cursor ( ) \n 
path , dot , like = completion_parts ( path ) \n 
before = text [ : len ( text ) - len ( like ) ] \n 
completions = interpreter . completions ( ) \n 
~~ finally : \n 
~~~ sys . path . pop ( 0 ) \n 
~~ self . matches = [ before + c . name_with_symbols for c in completions ] \n 
~~~ return self . matches [ state ] \n 
~~ except IndexError : \n 
~~ ~~ ~~ try : \n 
~~~ import readline \n 
~~~ readline . set_completer ( JediRL ( ) . complete ) \n 
readline . set_completer_delims ( ) \n 
~~ ~~ def version_info ( ) : \n 
Version = namedtuple ( , ) \n 
from jedi import __version__ \n 
tupl = re . findall ( , __version__ ) \n 
return Version ( * [ x if i == 3 else int ( x ) for i , x in enumerate ( tupl ) ] ) \n 
~~ import pandas \n 
import util \n 
import matplotlib . pyplot as plt \n 
import scipy as sp \n 
import scipy . stats \n 
cur_dir = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
def from_custom_file ( data_file , learn_options ) : \n 
data = pandas . read_csv ( data_file ) \n 
mandatory_columns = [ , , , ] \n 
for col in mandatory_columns : \n 
~~ Xdf = pandas . DataFrame ( data ) \n 
Xdf [ ] = Xdf [ ] \n 
Xdf = Xdf . set_index ( [ , ] ) \n 
Xdf . index . names = [ , ] \n 
Xdf [ ] = [ % i for i in range ( Xdf . shape [ 0 ] ) ] \n 
Xdf = Xdf . set_index ( , append = True ) \n 
Y = None \n 
gene_position = Xdf [ [ , ] ] \n 
target_genes = np . unique ( Xdf . index . levels [ 1 ] ) \n 
learn_options = set_V2_target_names ( learn_options ) \n 
return Xdf , Y , gene_position , target_genes \n 
~~ def from_file ( data_file , learn_options , data_file2 = None , data_file3 = None ) : \n 
annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options ) \n 
learn_options [ ] = \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , learn_options ) \n 
xx = Xdf [ ] . values \n 
yy = Y [ ] . values \n 
rr , pp = sp . stats . pearsonr ( xx , yy ) \n 
~~~ learn_options [ ] = \n 
learn_options [ ] = None \n 
Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
Xdf , Y , gene_position , target_genes = merge_all ( data_file , data_file2 , data_file3 , learn_options ) \n 
~~ elif learn_options [ ] == 5 : \n 
gene_position , target_genes , Xdf , Y = read_xu_et_al ( data_file3 ) \n 
~~ Xdf [ "30mer" ] = Xdf [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
~~ def set_V2_target_names ( learn_options ) : \n 
~~~ if not in learn_options . keys ( ) : \n 
~~ if not in learn_options . keys ( ) : \n 
~~ learn_options [ ] = \n 
return learn_options \n 
~~ def combine_organisms ( human_data , mouse_data ) : \n 
~~~ cd13 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD13 , Y_CD13 = util . get_data ( cd13 , y_names = [ , ] ) \n 
cd33 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD33 , Y_CD33 = util . get_data ( cd33 , y_names = [ , , ] ) \n 
cd15 = human_data . xs ( , level = , drop_level = False ) \n 
X_CD15 , Y_CD15 = util . get_data ( cd15 , y_names = [ ] ) \n 
mouse_X = pandas . DataFrame ( ) \n 
mouse_Y = pandas . DataFrame ( ) \n 
for k in mouse_data . index . levels [ 1 ] : \n 
mouse_X = pandas . concat ( [ mouse_X , X ] , axis = 0 ) \n 
mouse_Y = pandas . concat ( [ mouse_Y , Y ] , axis = 0 ) \n 
~~ X = pandas . concat ( [ X_CD13 , X_CD15 , X_CD33 , mouse_X ] , axis = 0 ) \n 
Y = pandas . concat ( [ Y_CD13 , Y_CD15 , Y_CD33 , mouse_Y ] , axis = 0 ) \n 
return X , Y \n 
~~ def read_V1_data ( data_file , learn_options , AML_file = cur_dir + "/data/V1_suppl_data.txt" ) : \n 
~~~ if data_file is None : \n 
~~~ data_file = cur_dir + "/data/V1_data.xlsx" \n 
~~ human_data = pandas . read_excel ( data_file , sheetname = 0 , index_col = [ 0 , 1 ] ) \n 
mouse_data = pandas . read_excel ( data_file , sheetname = 1 , index_col = [ 0 , 1 ] ) \n 
Xdf , Y = combine_organisms ( human_data , mouse_data ) \n 
annotations = pandas . read_csv ( AML_file , delimiter = , index_col = [ 0 , 4 ] ) \n 
annotations . index . names = Xdf . index . names \n 
gene_position = pandas . merge ( Xdf , annotations , how = "inner" , left_index = True , right_index = True ) \n 
gene_position = util . impute_gene_position ( gene_position ) \n 
gene_position = gene_position [ [ , , ] ] \n 
Y = Y . loc [ gene_position . index ] \n 
Xdf = Xdf . loc [ gene_position . index ] \n 
target_genes = Y [ ] . unique ( ) \n 
Y . index . names = [ , ] \n 
if learn_options is not None and learn_options [ "flipV1target" ] : \n 
~~~ print "************************************************************************" \n 
print "************************************************************************" \n 
import ipdb \n 
ipdb . set_trace ( ) \n 
~~ return annotations , gene_position , target_genes , Xdf , Y \n 
~~ def rank_transform ( x ) : \n 
~~~ return 1.0 - sp . stats . mstats . rankdata ( x ) / sp . stats . mstats . rankdata ( x ) . max ( ) \n 
~~ def read_xu_et_al ( data_file , learn_options = None , verbose = True , subsetting = ) : \n 
~~~ data_file = \n 
~~ datasets = [ , , ] \n 
aggregated = None \n 
for d in datasets : \n 
~~~ data_efficient = pandas . read_excel ( data_file , sheetname = % d , skiprows = 2 ) \n 
data_inefficient = pandas . read_excel ( data_file , sheetname = % d , skiprows = 2 ) \n 
data_efficient [ ] = 1. \n 
data_inefficient [ ] = 0. \n 
exp_data = pandas . concat ( ( data_efficient , data_inefficient ) ) \n 
exp_data [ ] = exp_data . groupby ( ) [ ] . transform ( rank_transform ) \n 
if aggregated is None : \n 
~~~ aggregated = exp_data \n 
~~~ aggregated = pandas . concat ( ( aggregated , exp_data ) ) \n 
~~ ~~ if subsetting == : \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x [ 6 : - 4 ] ) \n 
~~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x [ 10 : ] ) \n 
~~ aggregated [ "sequence(target+3\'+5\')" ] = aggregated [ "sequence(target+3\'+5\')" ] . apply ( lambda x : x . upper ( ) ) \n 
aggregated . rename ( columns = { "sequence(target+3\'+5\')" : , : , : } , inplace = True ) \n 
aggregated [ ] . loc [ aggregated [ ] == ] = \n 
aggregated [ ] = aggregated [ [ , ] ] . mean ( axis = 1 ) \n 
df = aggregated \n 
df = df . rename ( columns = { : , : } ) \n 
df [ ] = \n 
df [ ] = 1 \n 
df = df . set_index ( [ , , ] ) \n 
df [ ] = df . index . get_level_values ( 0 ) \n 
df [ ] = df . index . get_level_values ( 1 ) \n 
df [ ] = df [ ] \n 
df [ ] = 0 \n 
target_genes = np . unique ( df [ ] . values ) \n 
return df [ [ , , ] ] , target_genes , df [ [ , ] ] , df [ [ , , , ] ] \n 
~~ def read_V2_data ( data_file , learn_options = None , verbose = True ) : \n 
~~~ data_file = cur_dir + "/data/V2_data.xlsx" \n 
~~ data = pandas . read_excel ( data_file , sheetname = "ResultsFiltered" , skiprows = range ( 0 , 6 + 1 ) , index_col = [ 0 , 4 ] ) \n 
Xdf = pandas . DataFrame ( ) \n 
known_pairs = { : [ , , , ] , \n 
: [ ] , \n 
: [ , , , ] } \n 
drugs_to_genes = { : [ , , , ] , \n 
if learn_options is not None : \n 
if learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , ] ) \n 
~~ elif learn_options [ ] : \n 
~~~ drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , ] ) \n 
drugs_to_genes [ ] . extend ( [ , , , , , , ] ) \n 
~~ ~~ count = 0 \n 
for drug in drugs_to_genes . keys ( ) : \n 
~~~ genes = drugs_to_genes [ drug ] \n 
for g in genes : \n 
~~~ Xtmp = data . copy ( ) . xs ( g , level = , drop_level = False ) \n 
Xtmp [ ] = drug \n 
if g in known_pairs [ drug ] : \n 
~~~ Xtmp [ ] = 1. \n 
~~~ Xtmp [ ] = 0. \n 
~~ count = count + Xtmp . shape [ 0 ] \n 
Xdf = pandas . concat ( [ Xdf , Xtmp ] , axis = 0 ) \n 
if verbose : \n 
~~ ~~ ~~ Xdf = Xdf . set_index ( , append = True ) \n 
Y = pandas . DataFrame ( Xdf . pop ( "score" ) ) \n 
Y . columns . names = [ "score" ] \n 
test_gene = pandas . DataFrame ( Xdf . pop ( ) ) \n 
Y = pandas . concat ( ( Y , target , test_gene ) , axis = 1 ) \n 
y_rank = pandas . DataFrame ( ) \n 
y_threshold = pandas . DataFrame ( ) \n 
y_quant = pandas . DataFrame ( ) \n 
~~~ gene_list = drugs_to_genes [ drug ] \n 
for gene in gene_list : \n 
y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = 0.8 , prefix = "score_drug_gene" , flip = False ) \n 
y_rank = pandas . concat ( ( y_rank , y_ranktmp ) , axis = 0 ) \n 
y_threshold = pandas . concat ( ( y_threshold , y_thresholdtmp ) , axis = 0 ) \n 
y_quant = pandas . concat ( ( y_quant , y_quanttmp ) , axis = 0 ) \n 
~~ ~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
Y = pandas . merge ( Y , yall , how = , left_index = True , right_index = True ) \n 
~~~ ytmp = pandas . DataFrame ( Y . xs ( drug , level = "drug" , drop_level = False ) [ ] ) \n 
y_ranktmp , y_rank_raw , y_thresholdtmp , y_quanttmp = util . get_ranks ( ytmp , thresh = 0.8 , prefix = "score_drug" , flip = False ) \n 
~~ yall = pandas . concat ( ( y_rank , y_threshold , y_quant ) , axis = 1 ) \n 
PLOT = False \n 
if PLOT : \n 
~~~ labels = [ "score" , "score_drug_gene_rank" , "score_drug_rank" , "score_drug_gene_threshold" , "score_drug_threshold" ] \n 
for label in labels : \n 
~~~ plt . figure ( ) \n 
plt . plot ( Xdf [ ] . values , Y [ label ] . values , ) \n 
r , pearp = sp . stats . pearsonr ( Xdf [ ] . values . flatten ( ) , Y [ label ] . values . flatten ( ) ) \n 
plt . title ( label + % ( r , pearp ) ) \n 
plt . ylabel ( label ) \n 
~~ ~~ gene_position = util . impute_gene_position ( gene_position ) \n 
if learn_options is not None and learn_options [ "weighted" ] == "variance" : \n 
data = pandas . read_excel ( data_file , sheetname = "Normalized" , skiprows = range ( 0 , 6 + 1 ) , index_col = [ 0 , 4 ] ) \n 
experiments = { } \n 
experiments [ ] = [ , , , ] \n 
variance = None \n 
~~~ data_tmp = data . iloc [ data . index . get_level_values ( ) . isin ( drugs_to_genes [ drug ] ) ] [ experiments [ drug ] ] \n 
data_tmp [ "drug" ] = drug \n 
data_tmp = data_tmp . set_index ( , append = True ) \n 
data_tmp [ "variance" ] = np . var ( data_tmp . values , axis = 1 ) \n 
if variance is None : \n 
~~~ variance = data_tmp [ "variance" ] . copy ( ) \n 
~~~ variance = pandas . concat ( ( variance , data_tmp [ "variance" ] ) , axis = 0 ) \n 
~~ ~~ orig_index = Y . index . copy ( ) \n 
Y = pandas . merge ( Y , pandas . DataFrame ( variance ) , how = "inner" , left_index = True , right_index = True ) \n 
Y = Y . ix [ orig_index ] \n 
print "done." \n 
return Xdf , drugs_to_genes , target_genes , Y , gene_position \n 
~~ def merge_all ( data_file = None , data_file2 = None , data_file3 = None , learn_options = None ) : \n 
~~~ Xdf , Y , gene_position , target_genes = mergeV1_V2 ( data_file , data_file2 , learn_options ) \n 
gene_position_xu , target_genes_xu , Xdf_xu , Y_xu = read_xu_et_al ( data_file3 , learn_options ) \n 
Xdf = pandas . concat ( ( Xdf , Xdf_xu ) ) \n 
Y = pandas . concat ( ( Y , Y_xu ) ) \n 
gene_position = pandas . concat ( ( gene_position , gene_position_xu ) ) \n 
target_genes = np . concatenate ( ( target_genes , target_genes_xu ) ) \n 
~~ def mergeV1_V2 ( data_file , data_file2 , learn_options ) : \n 
annotations , gene_position1 , target_genes1 , Xdf1 , Y1 = read_V1_data ( data_file , learn_options ) \n 
Xdf2 , drugs_to_genes , target_genes2 , Y2 , gene_position2 = read_V2_data ( data_file2 ) \n 
Y1 [ "drug" ] = [ "nodrug" for x in range ( Y1 . shape [ 0 ] ) ] \n 
Y1 = Y1 . set_index ( , append = True ) \n 
Y1 . index . names = [ , , ] \n 
Y_cols_to_keep = np . unique ( [ , , , ] ) \n 
Y1 = Y1 [ Y_cols_to_keep ] \n 
Y2 = Y2 [ Y_cols_to_keep ] \n 
Xdf1 [ "drug" ] = [ "nodrug" for x in range ( Xdf1 . shape [ 0 ] ) ] \n 
Xdf1 = Xdf1 . set_index ( , append = True ) \n 
X_cols_to_keep = [ , ] \n 
Xdf1 = Xdf1 [ X_cols_to_keep ] \n 
Xdf2 = Xdf2 [ X_cols_to_keep ] \n 
gene_position1 [ "drug" ] = [ "nodrug" for x in range ( gene_position1 . shape [ 0 ] ) ] \n 
gene_position1 = gene_position1 . set_index ( , append = True ) \n 
gene_position1 . index . names = [ , , ] \n 
cols_to_keep = [ , ] \n 
gene_position1 = gene_position1 [ cols_to_keep ] \n 
gene_position2 = gene_position2 [ cols_to_keep ] \n 
Y = pandas . concat ( ( Y1 , Y2 ) , axis = 0 ) \n 
Xdf = pandas . concat ( ( Xdf1 , Xdf2 ) , axis = 0 ) \n 
gene_position = pandas . concat ( ( gene_position1 , gene_position2 ) ) \n 
target_genes = np . concatenate ( ( target_genes1 , target_genes2 ) ) \n 
save_to_file = False \n 
if save_to_file : \n 
~~~ Y . index . names = [ , , ] \n 
onedupind = np . where ( Y . index . duplicated ( ) ) [ 0 ] [ 0 ] \n 
alldupind = np . where ( Y . index . get_level_values ( 0 ) . values == Y . index [ onedupind ] [ 0 ] ) [ 0 ] \n 
newindex = Y . index . tolist ( ) \n 
newindex [ onedupind ] = ( newindex [ onedupind ] [ 0 ] , newindex [ onedupind ] [ 1 ] , "nodrug2" ) \n 
Y . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
Xdf . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
XandY = pandas . merge ( Xdf , Y , how = "inner" , left_index = True , right_index = True ) \n 
gene_position_tmp = gene_position . copy ( ) \n 
gene_position_tmp . index . names = [ , , ] \n 
gene_position_tmp . index = pandas . MultiIndex . from_tuples ( newindex , names = Y . index . names ) \n 
XandY = pandas . merge ( XandY , gene_position_tmp , how = "inner" , left_index = True , right_index = True ) \n 
XandY [ "30mer" ] = XandY [ "30mer" ] . apply ( lambda x : x [ 0 : 30 ] ) \n 
XandY . to_csv ( ) \n 
~~ return Xdf , Y , gene_position , target_genes \n 
~~ def get_V1_genes ( data_file = None ) : \n 
~~~ annotations , gene_position , target_genes , Xdf , Y = read_V1_data ( data_file , learn_options = None ) \n 
return target_genes \n 
~~ def get_V2_genes ( data_file = None ) : \n 
~~~ Xdf , drugs_to_genes , target_genes , Y , gene_position = read_V2_data ( data_file , verbose = False ) \n 
~~ def get_V3_genes ( data_fileV1 = None , data_fileV2 = None ) : \n 
~~~ target_genes = np . concatenate ( ( get_V1_genes ( data_fileV1 ) , get_V2_genes ( data_fileV2 ) ) ) \n 
~~ def get_xu_genes ( data_file = None ) : \n 
~~~ return read_xu_et_al ( data_file ) [ 1 ] \n 
~~ def get_mouse_genes ( data_file = None ) : \n 
return Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
~~ def get_human_genes ( data_file = None ) : \n 
mouse_genes = Xdf [ Xdf [ ] == ] [ ] . unique ( ) \n 
all_genes = get_V3_genes ( None , None ) \n 
return np . setdiff1d ( all_genes , mouse_genes ) \n 
~~ from __future__ import absolute_import \n 
from . classification import * \n 
from . generic import * \n 
from . job import ImageDatasetJob \n 
from . images import * \n 
from . job import InferenceJob \n 
from . caffe_train import CaffeTrainTask \n 
from . torch_train import TorchTrainTask \n 
from . train import TrainTask \n 
import flask \n 
from flask . ext . socketio import SocketIO \n 
from gevent import monkey ; monkey . patch_all ( ) \n 
from . config import config_value \n 
from digits import utils \n 
import digits . scheduler \n 
app = flask . Flask ( __name__ ) \n 
app . config [ ] = True \n 
app . config [ ] = False \n 
app . config [ ] = config_value ( ) \n 
app . url_map . redirect_defaults = False \n 
socketio = SocketIO ( app ) \n 
scheduler = digits . scheduler . Scheduler ( config_value ( ) , True ) \n 
app . jinja_env . globals [ ] = config_value ( ) \n 
app . jinja_env . globals [ ] = digits . __version__ \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_diff \n 
app . jinja_env . filters [ ] = utils . time_filters . print_time_since \n 
app . jinja_env . filters [ ] = utils . sizeof_fmt \n 
app . jinja_env . filters [ ] = utils . auth . has_permission \n 
app . jinja_env . trim_blocks = True \n 
app . jinja_env . lstrip_blocks = True \n 
import digits . views \n 
app . register_blueprint ( digits . views . blueprint ) \n 
import digits . dataset . views \n 
app . register_blueprint ( digits . dataset . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . views \n 
app . register_blueprint ( digits . dataset . images . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . classification . views \n 
app . register_blueprint ( digits . dataset . images . classification . views . blueprint , url_prefix = ) \n 
import digits . dataset . images . generic . views \n 
app . register_blueprint ( digits . dataset . images . generic . views . blueprint , url_prefix = ) \n 
import digits . model . views \n 
app . register_blueprint ( digits . model . views . blueprint , url_prefix = ) \n 
import digits . model . images . views \n 
app . register_blueprint ( digits . model . images . views . blueprint , url_prefix = ) \n 
import digits . model . images . classification . views \n 
app . register_blueprint ( digits . model . images . classification . views . blueprint , url_prefix = ) \n 
import digits . model . images . generic . views \n 
app . register_blueprint ( digits . model . images . generic . views . blueprint , url_prefix = ) \n 
def username_decorator ( f ) : \n 
~~~ from functools import wraps \n 
@ wraps ( f ) \n 
def decorated ( * args , ** kwargs ) : \n 
~~~ this_username = flask . request . cookies . get ( , None ) \n 
app . jinja_env . globals [ ] = this_username \n 
return f ( * args , ** kwargs ) \n 
~~ return decorated \n 
~~ for endpoint , function in app . view_functions . iteritems ( ) : \n 
~~~ app . view_functions [ endpoint ] = username_decorator ( function ) \n 
~~ scheduler . load_past_jobs ( ) \n 
import xml . etree . ElementTree as ET \n 
import cPickle \n 
def parse_rec ( filename ) : \n 
tree = ET . parse ( filename ) \n 
objects = [ ] \n 
for obj in tree . findall ( ) : \n 
~~~ obj_struct = { } \n 
obj_struct [ ] = obj . find ( ) . text \n 
obj_struct [ ] = int ( obj . find ( ) . text ) \n 
bbox = obj . find ( ) \n 
obj_struct [ ] = [ int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) , \n 
int ( bbox . find ( ) . text ) ] \n 
objects . append ( obj_struct ) \n 
~~ return objects \n 
~~ def voc_ap ( rec , prec , use_07_metric = False ) : \n 
if use_07_metric : \n 
~~~ ap = 0. \n 
for t in np . arange ( 0. , 1.1 , 0.1 ) : \n 
~~~ if np . sum ( rec >= t ) == 0 : \n 
~~~ p = 0 \n 
~~~ p = np . max ( prec [ rec >= t ] ) \n 
~~ ap = ap + p / 11. \n 
~~~ mrec = np . concatenate ( ( [ 0. ] , rec , [ 1. ] ) ) \n 
mpre = np . concatenate ( ( [ 0. ] , prec , [ 0. ] ) ) \n 
for i in range ( mpre . size - 1 , 0 , - 1 ) : \n 
~~~ mpre [ i - 1 ] = np . maximum ( mpre [ i - 1 ] , mpre [ i ] ) \n 
~~ i = np . where ( mrec [ 1 : ] != mrec [ : - 1 ] ) [ 0 ] \n 
ap = np . sum ( ( mrec [ i + 1 ] - mrec [ i ] ) * mpre [ i + 1 ] ) \n 
~~ return ap \n 
~~ def voc_eval ( detpath , \n 
annopath , \n 
imagesetfile , \n 
classname , \n 
cachedir , \n 
ovthresh = 0.5 , \n 
use_07_metric = False ) : \n 
if not os . path . isdir ( cachedir ) : \n 
~~~ os . mkdir ( cachedir ) \n 
~~ cachefile = os . path . join ( cachedir , ) \n 
with open ( imagesetfile , ) as f : \n 
~~~ lines = f . readlines ( ) \n 
~~ imagenames = [ x . strip ( ) for x in lines ] \n 
if not os . path . isfile ( cachefile ) : \n 
~~~ recs = { } \n 
for i , imagename in enumerate ( imagenames ) : \n 
~~~ recs [ imagename ] = parse_rec ( annopath . format ( imagename ) ) \n 
if i % 100 == 0 : \n 
~~~ print . format ( \n 
i + 1 , len ( imagenames ) ) \n 
~~ ~~ print . format ( cachefile ) \n 
with open ( cachefile , ) as f : \n 
~~~ cPickle . dump ( recs , f ) \n 
~~~ with open ( cachefile , ) as f : \n 
~~~ recs = cPickle . load ( f ) \n 
~~ ~~ class_recs = { } \n 
npos = 0 \n 
for imagename in imagenames : \n 
~~~ R = [ obj for obj in recs [ imagename ] if obj [ ] == classname ] \n 
bbox = np . array ( [ x [ ] for x in R ] ) \n 
difficult = np . array ( [ x [ ] for x in R ] ) . astype ( np . bool ) \n 
det = [ False ] * len ( R ) \n 
npos = npos + sum ( ~ difficult ) \n 
class_recs [ imagename ] = { : bbox , \n 
: difficult , \n 
: det } \n 
~~ detfile = detpath . format ( classname ) \n 
with open ( detfile , ) as f : \n 
~~ splitlines = [ x . strip ( ) . split ( ) for x in lines ] \n 
image_ids = [ x [ 0 ] for x in splitlines ] \n 
confidence = np . array ( [ float ( x [ 1 ] ) for x in splitlines ] ) \n 
BB = np . array ( [ [ float ( z ) for z in x [ 2 : ] ] for x in splitlines ] ) \n 
sorted_ind = np . argsort ( - confidence ) \n 
BB = BB [ sorted_ind , : ] \n 
image_ids = [ image_ids [ x ] for x in sorted_ind ] \n 
nd = len ( image_ids ) \n 
tp = np . zeros ( nd ) \n 
fp = np . zeros ( nd ) \n 
for d in range ( nd ) : \n 
~~~ R = class_recs [ image_ids [ d ] ] \n 
bb = BB [ d , : ] . astype ( float ) \n 
ovmax = - np . inf \n 
BBGT = R [ ] . astype ( float ) \n 
if BBGT . size > 0 : \n 
~~~ ixmin = np . maximum ( BBGT [ : , 0 ] , bb [ 0 ] ) \n 
iymin = np . maximum ( BBGT [ : , 1 ] , bb [ 1 ] ) \n 
ixmax = np . minimum ( BBGT [ : , 2 ] , bb [ 2 ] ) \n 
iymax = np . minimum ( BBGT [ : , 3 ] , bb [ 3 ] ) \n 
iw = np . maximum ( ixmax - ixmin + 1. , 0. ) \n 
ih = np . maximum ( iymax - iymin + 1. , 0. ) \n 
inters = iw * ih \n 
uni = ( ( bb [ 2 ] - bb [ 0 ] + 1. ) * ( bb [ 3 ] - bb [ 1 ] + 1. ) + \n 
( BBGT [ : , 2 ] - BBGT [ : , 0 ] + 1. ) * \n 
( BBGT [ : , 3 ] - BBGT [ : , 1 ] + 1. ) - inters ) \n 
overlaps = inters / uni \n 
ovmax = np . max ( overlaps ) \n 
jmax = np . argmax ( overlaps ) \n 
~~ if ovmax > ovthresh : \n 
~~~ if not R [ ] [ jmax ] : \n 
~~~ tp [ d ] = 1. \n 
R [ ] [ jmax ] = 1 \n 
~~~ fp [ d ] = 1. \n 
~~ ~~ fp = np . cumsum ( fp ) \n 
tp = np . cumsum ( tp ) \n 
rec = tp / float ( npos ) \n 
prec = tp / ( tp + fp + 1e-10 ) \n 
ap = voc_ap ( rec , prec , use_07_metric ) \n 
return rec , prec , ap \n 
#!/usr/bin/python \n 
~~ import numpy as np \n 
from ipdb import set_trace \n 
from struct import pack , unpack \n 
def ceil_div ( x , y ) : \n 
~~~ return - ( - x // y ) \n 
~~ def out_dim ( S , X , padding , strides ) : \n 
~~~ return ceil_div ( X - S + 1 + 2 * padding , strides ) \n 
~~ def strip_mantissa ( val ) : \n 
~~~ i = unpack ( , pack ( , val ) ) [ 0 ] & 0x7f800000 \n 
f = unpack ( , pack ( , i ) ) [ 0 ] \n 
return f \n 
~~ def quantize ( ary , bits , sign = 1 ) : \n 
~~~ maxval = float ( np . max ( np . absolute ( ary ) ) ) \n 
scale = strip_mantissa ( maxval ) / float ( 1 << ( bits - sign - 1 ) ) \n 
ary = np . around ( ary * ( 1.0 / scale ) ) . astype ( np . int64 ) \n 
return ary , np . float64 ( scale ) \n 
~~ def fconv_slice ( q , S , X , padding , strides ) : \n 
~~~ f1 = 0 \n 
f2 = S - 1 \n 
x1 = q * strides - padding \n 
x2 = x1 + f2 \n 
if x1 < 0 : \n 
~~~ f1 = - x1 \n 
x1 = 0 \n 
~~ if x2 >= X : \n 
~~~ dif = x2 - X + 1 \n 
f2 -= dif \n 
x2 -= dif \n 
~~ return ( slice ( f1 , f2 + 1 ) , slice ( x1 , x2 + 1 ) , f2 - f1 + 1 ) \n 
~~ def bconv_slice ( x , S , Q , padding , strides ) : \n 
~~~ qs = x - ( S - padding - 1 ) \n 
firstF = None \n 
~~~ q = qs + s \n 
if q % strides == 0 : \n 
~~~ q strides \n 
if q >= 0 and q < Q : \n 
~~~ if firstF is None : \n 
~~~ firstF = s \n 
firstE = q \n 
~~ lastF = s \n 
lastE = q \n 
~~ ~~ ~~ return ( slice ( firstF , lastF + 1 , strides ) , slice ( firstE , lastE + 1 , strides ) , 0 ) \n 
~~ def xprop_direct ( I , F , O , padding , strides , backward = False ) : \n 
~~~ if all ( x == 1 for x in F . shape [ 1 : 3 ] ) : \n 
~~~ C = F . shape [ 0 ] \n 
K = F . shape [ 4 ] \n 
if backward : \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) , I . reshape ( ( K , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~~ O [ : ] = np . dot ( F . reshape ( ( C , - 1 ) ) . T , I . reshape ( ( C , - 1 ) ) ) . reshape ( ( O . shape ) ) \n 
~~ return \n 
~~ if backward : \n 
~~~ F = np . transpose ( F [ : , : : - 1 , : : - 1 , : ] , ( 3 , 1 , 2 , 0 ) ) . copy ( ) \n 
xconv_slice = bconv_slice \n 
~~~ xconv_slice = fconv_slice \n 
~~ C , Y , X , N = I . shape \n 
C , R , S , K = F . shape \n 
K , P , Q , N = O . shape \n 
qSlice = [ xconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
for p in range ( P ) : \n 
~~~ sliceR , sliceY , _ = xconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
for q in range ( Q ) : \n 
~~~ sliceS , sliceX , _ = qSlice [ q ] \n 
slicedF = F [ : , sliceR , sliceS , : ] . reshape ( ( - 1 , K ) ) \n 
slicedI = I [ : , sliceY , sliceX , : ] . reshape ( ( - 1 , N ) ) \n 
O [ : , p , q , : ] = np . dot ( slicedF . T , slicedI ) \n 
~~ ~~ ~~ def updat_direct ( I , E , U , padding , strides ) : \n 
~~~ C , Y , X , N = I . shape \n 
K , P , Q , N = E . shape \n 
C , R , S , K = U . shape \n 
if all ( x == 1 for x in ( R , S ) ) : \n 
~~~ U [ : ] = np . dot ( I . reshape ( ( C , - 1 ) ) , E . reshape ( ( K , - 1 ) ) . T ) . reshape ( ( U . shape ) ) \n 
return \n 
~~ U . fill ( 0.0 ) \n 
qSlice = [ fconv_slice ( q , S , X , padding [ 0 ] , strides [ 0 ] ) for q in range ( Q ) ] \n 
~~~ sliceR , sliceY , rlen = fconv_slice ( p , R , Y , padding [ 1 ] , strides [ 1 ] ) \n 
~~~ sliceS , sliceX , slen = qSlice [ q ] \n 
slicedE = E [ : , p , q , : ] \n 
U [ : , sliceR , sliceS , : ] += np . dot ( slicedI , slicedE . T ) . reshape ( ( C , rlen , slen , K ) ) \n 
~~ ~~ ~~ I_4x4_3x3 = ( \n 
np . array ( [ \n 
[ 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 4.0 , - 4.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , - 4.0 , - 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 , - 1.0 , 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 2.0 , - 1.0 , - 2.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 4.0 , 0.0 , - 5.0 , 0.0 , 1.0 ] ] ) , \n 
[ 1.0 , 0.0 , - 5.0 / 4.0 , 0.0 , 1.0 / 4.0 , 0.0 ] , \n 
[ 0.0 , 2.0 / 3.0 , 2.0 / 3.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 2.0 / 3.0 , 2.0 / 3.0 , 1.0 / 6.0 , - 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 12. , - 1.0 / 24. , 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , 1.0 / 12. , - 1.0 / 24. , - 1.0 / 12. , 1.0 / 24. , 0.0 ] , \n 
F_4x4_3x3 = ( \n 
[ 1.0 / 4.0 , 0.0 , 0.0 ] , \n 
[ - 1.0 / 6.0 , - 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ - 1.0 / 6.0 , 1.0 / 6.0 , - 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 1.0 / 24. , - 1.0 / 12. , 1.0 / 6.0 ] , \n 
[ 0.0 , 0.0 , 1.0 ] ] ) , \n 
[ 1.0 , 0.0 , 0.0 ] , \n 
[ 1.0 , 1.0 , 1.0 ] , \n 
[ 1.0 , - 1.0 , 1.0 ] , \n 
[ 1.0 , 2.0 , 4.0 ] , \n 
[ 1.0 , - 2.0 , 4.0 ] , \n 
O_4x4_3x3 = ( \n 
[ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 2.0 , - 2.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , 1.0 , 4.0 , 4.0 , 0.0 ] , \n 
[ 0.0 , 1.0 , - 1.0 , 8.0 , - 8.0 , 1.0 ] ] ) , \n 
[ 1.0 / 4.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 24. , 1.0 / 24. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 12. , - 1.0 / 12. , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 6.0 , 0.0 ] , \n 
[ 0.0 , - 1.0 / 6.0 , 1.0 / 6.0 , 1.0 / 3.0 , - 1.0 / 3.0 , 1.0 ] ] ) , \n 
rcp3 = 1.0 / 3.0 \n 
rcp4 = 1.0 / 4.0 \n 
rcp6 = 1.0 / 6.0 \n 
rcp12 = 1.0 / 12.0 \n 
rcp24 = 1.0 / 24.0 \n 
def trans_I_4x4_3x3 ( Iw , I , minimal = False , trans = False ) : \n 
~~~ if minimal : \n 
~~~ T0 = np . empty ( ( 6 , 6 ) ) \n 
T1 = np . empty ( ( 6 , 6 ) ) \n 
for O , I in ( ( T0 , I ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = ( I [ 2 , : ] * 4.0 - I [ 4 , : ] ) * rcp6 \n 
t1 = ( I [ 1 , : ] * 4.0 - I [ 3 , : ] ) * rcp6 \n 
t2 = ( I [ 4 , : ] - I [ 2 , : ] ) * rcp24 \n 
t3 = ( I [ 3 , : ] - I [ 1 , : ] ) * rcp12 \n 
O [ 0 , : ] = I [ 0 , : ] + ( I [ 2 , : ] * - 5.0 + I [ 4 , : ] ) * rcp4 \n 
O [ 1 , : ] = t0 + t1 \n 
O [ 2 , : ] = t0 - t1 \n 
O [ 3 , : ] = t2 + t3 \n 
O [ 4 , : ] = t2 - t3 \n 
O [ 5 , : ] = I [ 1 , : ] * 4.0 - I [ 3 , : ] * 5.0 + I [ 5 , : ] \n 
~~ Iw [ : ] = T1 . T \n 
~~~ Iw [ : ] = np . dot ( np . dot ( I_4x4_3x3 [ trans [ 0 ] ] , I ) , I_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_F_4x4_3x3 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 6 , 3 ) ) \n 
for O , I in ( ( T0 , F ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = I [ 0 , : ] + I [ 2 , : ] \n 
t1 = I [ 0 , : ] + I [ 2 , : ] * 4.0 \n 
O [ 0 , : ] = I [ 0 , : ] \n 
O [ 1 , : ] = t0 + I [ 1 , : ] \n 
O [ 2 , : ] = t0 - I [ 1 , : ] \n 
O [ 3 , : ] = t1 + I [ 1 , : ] * 2.0 \n 
O [ 4 , : ] = t1 - I [ 1 , : ] * 2.0 \n 
O [ 5 , : ] = I [ 2 , : ] \n 
~~ Fw [ : ] = T1 . T \n 
~~~ Fw [ : ] = np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] , F ) , F_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_O_4x4_3x3 ( Mw , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 4 , 6 ) ) \n 
T1 = np . empty ( ( 4 , 4 ) ) \n 
for O , I in ( ( T0 , Mw ) , ( T1 , T0 . T ) ) : \n 
~~~ t0 = I [ 1 , : ] + I [ 2 , : ] \n 
t1 = I [ 3 , : ] + I [ 4 , : ] \n 
t2 = I [ 1 , : ] - I [ 2 , : ] \n 
t3 = I [ 3 , : ] - I [ 4 , : ] \n 
O [ 0 , : ] = t0 + t1 + I [ 0 , : ] \n 
O [ 1 , : ] = t2 + t3 * 2.0 \n 
O [ 2 , : ] = t0 + t1 * 4.0 \n 
O [ 3 , : ] = t2 + t3 * 8.0 + I [ 5 , : ] \n 
~~ return T1 . T \n 
~~~ return np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] , Mw ) , O_4x4_3x3 [ trans [ 1 ] ] . T ) \n 
~~ ~~ def trans_F_3x3_4x4 ( Fw , F , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 6 , 4 ) ) \n 
t2 = I [ 1 , : ] + I [ 3 , : ] \n 
t3 = I [ 1 , : ] + I [ 3 , : ] * 4.0 \n 
O [ 1 , : ] = t0 + t2 \n 
O [ 2 , : ] = t0 - t2 \n 
O [ 3 , : ] = t1 + t3 * 2.0 \n 
O [ 4 , : ] = t1 - t3 * 2.0 \n 
O [ 5 , : ] = I [ 3 , : ] \n 
~~~ Fw [ : ] = np . dot ( np . dot ( O_4x4_3x3 [ trans [ 0 ] ] . T , F ) , O_4x4_3x3 [ trans [ 1 ] ] ) \n 
~~ ~~ def trans_O_3x3_4x4 ( Mw , minimal = False , trans = False ) : \n 
~~~ T0 = np . empty ( ( 3 , 6 ) ) \n 
T1 = np . empty ( ( 3 , 3 ) ) \n 
O [ 0 , : ] = I [ 0 , : ] + t0 + t1 \n 
O [ 1 , : ] = I [ 1 , : ] - I [ 2 , : ] + 2 * ( I [ 3 , : ] - I [ 4 , : ] ) \n 
O [ 2 , : ] = t0 + 4 * t1 + I [ 5 , : ] \n 
~~~ return np . dot ( np . dot ( F_4x4_3x3 [ trans [ 0 ] ] . T , Mw ) , F_4x4_3x3 [ trans [ 1 ] ] ) \n 
~~ ~~ def image_slice ( x , X , B , D , pad = 0 ) : \n 
~~~ start = x * B - pad \n 
stop = start + D \n 
pad = [ 0 , 0 ] \n 
if start < 0 : \n 
~~~ pad [ 0 ] = - start \n 
start = 0 \n 
~~ if stop - 1 >= X : \n 
~~~ pad [ 1 ] = stop - X \n 
~~ return start , stop , pad \n 
~~ def output_slice ( p , P , B ) : \n 
~~~ p0 = p * B \n 
p1 = p0 + B \n 
if p1 > P : \n 
~~~ p1 = P \n 
~~ return p0 , p1 , p1 - p0 \n 
~~ def xprop_winograd ( I , F , O , padding , minimal = False , trans = False , backward = False ) : \n 
~~~ if backward : \n 
padding = [ 2 - p for p in padding ] \n 
B = 4 \n 
D = B + 2 \n 
Yw = ceil_div ( P , B ) \n 
Xw = ceil_div ( Q , B ) \n 
Fw = np . empty ( ( D , D , C , K ) ) \n 
Iw = np . empty ( ( D , D , C , Yw , Xw , N ) ) \n 
for c in range ( C ) : \n 
~~~ for k in range ( K ) : \n 
~~~ trans_F_4x4_3x3 ( Fw [ : , : , c , k ] , F [ c , : , : , k ] , minimal , trans ) \n 
~~ ~~ for y in range ( Yw ) : \n 
~~~ start_y , stop_y , pad_y = image_slice ( y , Y , B , D , padding [ 0 ] ) \n 
for x in range ( Xw ) : \n 
~~~ start_x , stop_x , pad_x = image_slice ( x , X , B , D , padding [ 1 ] ) \n 
sliceI = I [ : , start_y : stop_y , start_x : stop_x , : ] \n 
if any ( pad_y ) or any ( pad_x ) : \n 
~~~ sliceI = np . pad ( sliceI , ( ( 0 , 0 ) , pad_y , pad_x , ( 0 , 0 ) ) , ) \n 
~~ for c in range ( C ) : \n 
~~~ for n in range ( N ) : \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , c , y , x , n ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ for s in range ( D ) : \n 
~~~ for t in range ( D ) : \n 
~~~ Mw [ s , t ] = np . dot ( Fw [ s , t ] . T , Iw [ s , t ] . reshape ( C , - 1 ) ) . reshape ( ( K , Yw , Xw , N ) ) \n 
~~~ p0 , p1 , plen = output_slice ( y , P , B ) \n 
~~~ q0 , q1 , qlen = output_slice ( x , Q , B ) \n 
for k in range ( K ) : \n 
~~~ Out = trans_O_4x4_3x3 ( Mw [ : , : , k , y , x , n ] , minimal , trans ) \n 
O [ k , p0 : p1 , q0 : q1 , n ] = Out [ 0 : plen , 0 : qlen ] \n 
~~ ~~ ~~ ~~ ~~ def updat_winograd ( I , E , U , padding , minimal = False , trans = False , inner = True ) : \n 
Iw = np . empty ( ( D , D , N , C ) ) \n 
Ew = np . empty ( ( D , D , N , K ) ) \n 
if inner : \n 
~~~ Mw = np . empty ( ( D , D , C , K ) ) \n 
U . fill ( 0.0 ) \n 
~~~ Mw = np . zeros ( ( D , D , C , K ) ) \n 
~~ for y in range ( Yw ) : \n 
start_p , stop_p , pad_p = image_slice ( y , P , B , B ) \n 
start_q , stop_q , pad_q = image_slice ( x , Q , B , B ) \n 
sliceE = E [ : , start_p : stop_p , start_q : stop_q , : ] \n 
~~ if any ( pad_p ) or any ( pad_q ) : \n 
~~~ sliceE = np . pad ( sliceE , ( ( 0 , 0 ) , pad_p , pad_q , ( 0 , 0 ) ) , ) \n 
~~~ trans_I_4x4_3x3 ( Iw [ : , : , n , c ] , sliceI [ c , : , : , n ] , minimal , trans ) \n 
~~ ~~ for k in range ( K ) : \n 
~~~ trans_F_3x3_4x4 ( Ew [ : , : , n , k ] , sliceE [ k , : , : , n ] , minimal , trans ) \n 
~~ ~~ for s in range ( D ) : \n 
~~~ if inner : \n 
~~~ Mw [ s , t ] = np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
~~~ Mw [ s , t ] += np . dot ( Iw [ s , t ] . T , Ew [ s , t ] ) \n 
~~ ~~ ~~ if inner : \n 
~~~ for c in range ( C ) : \n 
~~~ U [ c , : , : , k ] += trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ ~~ if not inner : \n 
~~~ U [ c , : , : , k ] = trans_O_3x3_4x4 ( Mw [ : , : , c , k ] , minimal , trans ) \n 
~~ ~~ ~~ ~~ np . set_printoptions ( threshold = 8192 * 4 , linewidth = 600 , formatter = { : lambda x : "%6.3f" % x } ) \n 
minimal = 1 \n 
trans = ( 2 , 2 ) \n 
ones = 0 \n 
N = 32 \n 
C , K = 32 , 32 \n 
Y , X = 6 , 6 \n 
P = out_dim ( R , Y , padding [ 0 ] , strides [ 0 ] ) \n 
Q = out_dim ( S , X , padding [ 1 ] , strides [ 1 ] ) \n 
print P , Q \n 
dimI = ( C , Y , X , N ) \n 
dimF = ( C , R , S , K ) \n 
dimO = ( K , P , Q , N ) \n 
if ones : \n 
~~~ I = np . zeros ( dimI ) \n 
F = np . ones ( dimF ) \n 
E = np . zeros ( dimO ) \n 
for p , q in np . ndindex ( ( Y , X ) ) : \n 
~~~ I [ : , p , q , : ] = np . identity ( N ) \n 
~~ for p , q in np . ndindex ( ( P , Q ) ) : \n 
~~~ E [ : , p , q , n ] = range ( K ) \n 
~~~ I = np . random . uniform ( - 1.0 , 1.0 , dimI ) \n 
F = np . random . uniform ( - 1.0 , 1.0 , dimF ) \n 
E = np . random . uniform ( - 1.0 , 1.0 , dimO ) \n 
~~ Od = np . empty ( dimO ) \n 
Bd = np . empty ( dimI ) \n 
Ud = np . empty ( dimF ) \n 
Uw = np . empty ( dimF ) \n 
xprop_direct ( I , F , Od , padding , strides ) \n 
xprop_winograd ( I , F , Ow , padding , minimal = minimal , trans = trans ) \n 
xprop_direct ( E , F , Bd , padding , strides , backward = True ) \n 
xprop_winograd ( E , F , Bw , padding , minimal = minimal , trans = trans , backward = True ) \n 
updat_direct ( I , E , Ud , padding , strides ) \n 
updat_winograd ( I , E , Uw , padding , minimal = minimal , trans = trans ) \n 
difO = Od - Ow \n 
difB = Bd - Bw \n 
difU = Ud - Uw \n 
print abs ( difO ) . max ( ) / Od . max ( ) \n 
print abs ( difB ) . max ( ) / Bd . max ( ) \n 
print abs ( difU ) . max ( ) / Ud . max ( ) \n 
from neon . layers . layer import ( Linear , Bias , Affine , Conv , Convolution , GeneralizedCost , Dropout , \n 
Pooling , Activation , DataTransform , BatchNorm , BatchNormAutodiff , \n 
Deconv , Deconvolution , GeneralizedCostMask , LookupTable , \n 
BranchNode , SkipNode , LRN , ColorNoise ) \n 
from neon . layers . recurrent import ( Recurrent , LSTM , GRU , RecurrentSum , RecurrentMean , RecurrentLast , \n 
BiRNN , BiLSTM , DeepBiRNN , DeepBiLSTM ) \n 
from neon . layers . container import ( Tree , Sequential , MergeMultistream , MergeBroadcast , Multicost , \n 
RoiPooling , MergeSum , SingleOutputTree ) \n 
from neon . util . argparser import NeonArgparser \n 
from neon . initializers import Constant , Gaussian \n 
from neon . layers import Conv , Dropout , Pooling , GeneralizedCost , Affine \n 
from neon . optimizers import GradientDescentMomentum , MultiOptimizer , Schedule \n 
from neon . transforms import Rectlin , Softmax , CrossEntropyMulti , TopKMisclassification \n 
from neon . models import Model \n 
from neon . data import ImageLoader \n 
from neon . callbacks . callbacks import Callbacks \n 
parser = NeonArgparser ( __doc__ ) \n 
img_set_options = dict ( repo_dir = args . data_dir , \n 
inner_size = 224 , \n 
subset_pct = 0.09990891117239205 ) \n 
train = ImageLoader ( set_name = , scale_range = ( 256 , 256 ) , shuffle = False , \n 
do_transforms = False , ** img_set_options ) \n 
test = ImageLoader ( set_name = , scale_range = ( 256 , 384 ) , shuffle = False , \n 
layers = [ Conv ( ( 11 , 11 , 64 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 3 , strides = 4 ) , \n 
Pooling ( 3 , strides = 2 ) , \n 
Conv ( ( 5 , 5 , 192 ) , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , \n 
activation = Rectlin ( ) , padding = 2 ) , \n 
Conv ( ( 3 , 3 , 384 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 0 ) , \n 
activation = Rectlin ( ) , padding = 1 ) , \n 
Conv ( ( 3 , 3 , 256 ) , init = Gaussian ( scale = 0.03 ) , bias = Constant ( 1 ) , \n 
Affine ( nout = 4096 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( 1 ) , activation = Rectlin ( ) ) , \n 
Dropout ( keep = 1.0 ) , \n 
Affine ( nout = 1000 , init = Gaussian ( scale = 0.01 ) , bias = Constant ( - 7 ) , activation = Softmax ( ) ) ] \n 
model = Model ( layers = layers ) \n 
weight_sched = Schedule ( [ 22 , 44 , 65 ] , ( 1 / 250. ) ** ( 1 / 3. ) ) \n 
opt_gdm = GradientDescentMomentum ( 0.01 / 10 , 0.9 , wdecay = 0.0005 , schedule = weight_sched , \n 
stochastic_round = args . rounding ) \n 
opt_biases = GradientDescentMomentum ( 0.02 / 10 , 0.9 , schedule = Schedule ( [ 44 ] , 0.1 ) , \n 
opt = MultiOptimizer ( { : opt_gdm , : opt_biases } ) \n 
valmetric = TopKMisclassification ( k = 5 ) \n 
callbacks = Callbacks ( model , eval_set = test , metric = valmetric , ** args . callback_args ) \n 
cost = GeneralizedCost ( costfunc = CrossEntropyMulti ( ) ) \n 
model . fit ( train , optimizer = opt , num_epochs = args . epochs , cost = cost , callbacks = callbacks ) \n 
import itertools as itt \n 
from neon import NervanaObject \n 
from neon . layers . layer import Pooling \n 
from tests . utils import allclose_with_out \n 
def pytest_generate_tests ( metafunc ) : \n 
~~~ np . random . seed ( 1 ) \n 
if metafunc . config . option . all : \n 
~~~ bsz_rng = [ 32 , 64 ] \n 
~~~ bsz_rng = [ 128 ] \n 
~~ if in metafunc . fixturenames : \n 
~~~ fargs = [ ] \n 
~~~ fs_rng = [ 2 , 3 , 5 ] \n 
pad_rng = [ 0 , 1 ] \n 
nifm_rng = [ 16 , 32 ] \n 
in_sz_rng = [ 8 , 16 ] \n 
~~~ fs_rng = [ 2 , 4 ] \n 
nifm_rng = [ 8 ] \n 
in_sz_rng = [ 8 ] \n 
~~ fargs_ = [ ] \n 
for fs in fs_rng : \n 
~~~ stride_rng = set ( [ 1 , fs / 2 , fs ] ) \n 
fargs_ . append ( itt . product ( fs_rng , nifm_rng , pad_rng , stride_rng , in_sz_rng , bsz_rng ) ) \n 
~~ fargs = itt . chain ( * fargs_ ) \n 
metafunc . parametrize ( , fargs ) \n 
~~ ~~ def ref_pooling ( inp , inp_shape , fshape , padding , strides , be , ncheck = None ) : \n 
~~~ inp_lshape = list ( inp_shape ) \n 
bsz = inp . shape [ - 1 ] \n 
if ncheck is None : \n 
~~~ check_inds = np . arange ( bsz ) \n 
~~ elif type ( ncheck ) is int : \n 
~~~ check_inds = np . random . permutation ( bsz ) \n 
check_inds = check_inds [ 0 : ncheck ] \n 
~~~ check_inds = ncheck \n 
~~ check_inds = np . sort ( check_inds ) \n 
inp_lshape . append ( bsz ) \n 
inpa = inp . get ( ) . reshape ( inp_lshape ) \n 
outshape = ( inp_lshape [ 0 ] , \n 
be . output_dim ( inp_lshape [ 1 ] , fshape [ 0 ] , padding , strides [ 0 ] , pooling = True ) , \n 
be . output_dim ( inp_lshape [ 2 ] , fshape [ 1 ] , padding , strides [ 1 ] , pooling = True ) , \n 
len ( check_inds ) ) \n 
if padding > 0 : \n 
~~~ padded_shape = ( inp_lshape [ 0 ] , \n 
inp_lshape [ 1 ] + 2 * padding , \n 
inp_lshape [ 2 ] + 2 * padding , \n 
inp_lshape [ - 1 ] ) \n 
inp_pad = np . zeros ( padded_shape ) \n 
inp_pad [ : , padding : - padding , padding : - padding , : ] = inpa [ : , 0 : , 0 : , : ] \n 
~~~ inp_pad = inpa \n 
~~ out_exp = np . zeros ( outshape ) \n 
for indC in range ( outshape [ 0 ] ) : \n 
~~~ for indh in range ( outshape [ 1 ] ) : \n 
~~~ hrng = ( indh * strides [ 0 ] , indh * strides [ 0 ] + fshape [ 0 ] ) \n 
for indw in range ( outshape [ 2 ] ) : \n 
~~~ wrng = ( indw * strides [ 1 ] , indw * strides [ 1 ] + fshape [ 1 ] ) \n 
for cnt , indb in enumerate ( check_inds ) : \n 
~~~ inp_check = inp_pad [ indC , hrng [ 0 ] : hrng [ 1 ] , wrng [ 0 ] : wrng [ 1 ] , indb ] \n 
out_exp [ indC , indh , indw , cnt ] = np . max ( inp_check ) \n 
~~ ~~ ~~ ~~ return ( out_exp , check_inds ) \n 
~~ def test_padding ( backend_default , poolargs ) : \n 
~~~ fshape , nifm , padding , stride , in_sz , batch_size = poolargs \n 
NervanaObject . be . bsz = batch_size \n 
inshape = ( nifm , in_sz , in_sz ) \n 
insize = np . prod ( inshape ) \n 
neon_layer = Pooling ( fshape = fshape , strides = stride , padding = padding ) \n 
inp = neon_layer . be . array ( np . random . random ( ( insize , batch_size ) ) ) \n 
inp . lshape = inshape \n 
neon_layer . configure ( inshape ) \n 
neon_layer . prev_layer = True \n 
neon_layer . allocate ( ) \n 
neon_layer . set_deltas ( [ neon_layer . be . iobuf ( inshape ) ] ) \n 
out = neon_layer . fprop ( inp ) . get ( ) \n 
ncheck = [ 0 , batch_size / 2 , batch_size - 1 ] \n 
( out_exp , check_inds ) = ref_pooling ( inp , inp . lshape , \n 
( fshape , fshape ) , \n 
padding , \n 
( stride , stride ) , \n 
neon_layer . be , \n 
ncheck = ncheck ) \n 
out_shape = list ( out_exp . shape [ 0 : 3 ] ) \n 
out_shape . append ( batch_size ) \n 
outa = out . reshape ( out_shape ) \n 
assert allclose_with_out ( out_exp , outa [ : , : , : , check_inds ] , atol = 0.0 , rtol = 0.0 ) \n 
~~ \n 
from __future__ import absolute_import , division , print_function \n 
from neo . core . container import Container \n 
class RecordingChannelGroup ( Container ) : \n 
_container_child_objects = ( , ) \n 
_data_child_objects = ( , ) \n 
_multi_child_objects = ( , ) \n 
_single_parent_objects = ( , ) \n 
_recommended_attrs = ( ( ( , np . ndarray , 1 , np . dtype ( ) ) , \n 
( , np . ndarray , 1 , np . dtype ( ) ) ) + \n 
Container . _recommended_attrs ) \n 
def __init__ ( self , channel_names = None , channel_indexes = None , name = None , \n 
description = None , file_origin = None , ** annotations ) : \n 
super ( RecordingChannelGroup , self ) . __init__ ( name = name , \n 
description = description , \n 
file_origin = file_origin , \n 
** annotations ) \n 
if channel_indexes is None : \n 
~~~ channel_indexes = np . array ( [ ] , dtype = np . int ) \n 
~~ if channel_names is None : \n 
~~~ channel_names = np . array ( [ ] , dtype = ) \n 
~~ self . channel_names = channel_names \n 
self . channel_indexes = channel_indexes \n 
import ctypes \n 
~~~ file \n 
~~ except NameError : \n 
~~~ import io \n 
file = io . BufferedReader \n 
import quantities as pq \n 
from neo . io . baseio import BaseIO \n 
from neo . core import Segment , AnalogSignal , SpikeTrain , EventArray \n 
class NeuroshareError ( Exception ) : \n 
~~~ def __init__ ( self , lib , errno ) : \n 
~~~ self . lib = lib \n 
self . errno = errno \n 
pszMsgBuffer = ctypes . create_string_buffer ( 256 ) \n 
self . lib . ns_GetLastErrorMsg ( pszMsgBuffer , ctypes . c_uint32 ( 256 ) ) \n 
errstr = . format ( errno , pszMsgBuffer . value ) \n 
Exception . __init__ ( self , errstr ) \n 
~~ ~~ class DllWithError ( ) : \n 
~~~ def __init__ ( self , lib ) : \n 
~~ def __getattr__ ( self , attr ) : \n 
~~~ f = getattr ( self . lib , attr ) \n 
return self . decorate_with_error ( f ) \n 
~~ def decorate_with_error ( self , f ) : \n 
~~~ def func_with_error ( * args ) : \n 
~~~ errno = f ( * args ) \n 
if errno != ns_OK : \n 
~~~ raise NeuroshareError ( self . lib , errno ) \n 
~~ return errno \n 
~~ return func_with_error \n 
~~ ~~ class NeurosharectypesIO ( BaseIO ) : \n 
is_readable = True \n 
is_writable = False \n 
supported_objects = [ Segment , AnalogSignal , EventArray , SpikeTrain ] \n 
readable_objects = [ Segment ] \n 
writeable_objects = [ ] \n 
has_header = False \n 
is_streameable = False \n 
read_params = { Segment : [ ] } \n 
write_params = None \n 
name = \n 
extensions = [ ] \n 
mode = \n 
def __init__ ( self , filename = , dllname = ) : \n 
BaseIO . __init__ ( self ) \n 
self . dllname = dllname \n 
self . filename = filename \n 
~~ def read_segment ( self , import_neuroshare_segment = True , \n 
lazy = False , cascade = True ) : \n 
seg = Segment ( file_origin = os . path . basename ( self . filename ) , ) \n 
if sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . windll . LoadLibrary ( self . dllname ) \n 
~~ elif sys . platform . startswith ( ) : \n 
~~~ neuroshare = ctypes . cdll . LoadLibrary ( self . dllname ) \n 
~~ neuroshare = DllWithError ( neuroshare ) \n 
info = ns_LIBRARYINFO ( ) \n 
neuroshare . ns_GetLibraryInfo ( ctypes . byref ( info ) , ctypes . sizeof ( info ) ) \n 
seg . annotate ( neuroshare_version = str ( info . dwAPIVersionMaj ) + + str ( info . dwAPIVersionMin ) ) \n 
if not cascade : \n 
~~~ return seg \n 
~~ hFile = ctypes . c_uint32 ( 0 ) \n 
neuroshare . ns_OpenFile ( ctypes . c_char_p ( self . filename ) , ctypes . byref ( hFile ) ) \n 
fileinfo = ns_FILEINFO ( ) \n 
neuroshare . ns_GetFileInfo ( hFile , ctypes . byref ( fileinfo ) , ctypes . sizeof ( fileinfo ) ) \n 
for dwEntityID in range ( fileinfo . dwEntityCount ) : \n 
~~~ entityInfo = ns_ENTITYINFO ( ) \n 
neuroshare . ns_GetEntityInfo ( hFile , dwEntityID , ctypes . byref ( entityInfo ) , ctypes . sizeof ( entityInfo ) ) \n 
if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pEventInfo = ns_EVENTINFO ( ) \n 
neuroshare . ns_GetEventInfo ( hFile , dwEntityID , ctypes . byref ( pEventInfo ) , ctypes . sizeof ( pEventInfo ) ) \n 
if pEventInfo . dwEventType == 0 : #TEXT \n 
~~~ pData = ctypes . create_string_buffer ( pEventInfo . dwMaxDataLength ) \n 
~~ elif pEventInfo . dwEventType == 1 : #CVS \n 
~~~ pData = ctypes . c_byte ( 0 ) \n 
~~~ pData = ctypes . c_int16 ( 0 ) \n 
~~~ pData = ctypes . c_int32 ( 0 ) \n 
~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
pdwDataRetSize = ctypes . c_uint32 ( 0 ) \n 
ea = EventArray ( name = str ( entityInfo . szEntityLabel ) , ) \n 
if not lazy : \n 
~~~ times = [ ] \n 
labels = [ ] \n 
for dwIndex in range ( entityInfo . dwItemCount ) : \n 
~~~ neuroshare . ns_GetEventData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , ctypes . byref ( pData ) , \n 
ctypes . sizeof ( pData ) , ctypes . byref ( pdwDataRetSize ) ) \n 
times . append ( pdTimeStamp . value ) \n 
labels . append ( str ( pData . value ) ) \n 
~~ ea . times = times * pq . s \n 
ea . labels = np . array ( labels , dtype = ) \n 
~~~ ea . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . eventarrays . append ( ea ) \n 
~~ if entity_types [ entityInfo . dwEntityType ] == : \n 
~~~ pAnalogInfo = ns_ANALOGINFO ( ) \n 
neuroshare . ns_GetAnalogInfo ( hFile , dwEntityID , ctypes . byref ( pAnalogInfo ) , ctypes . sizeof ( pAnalogInfo ) ) \n 
dwIndexCount = entityInfo . dwItemCount \n 
if lazy : \n 
~~~ signal = [ ] * pq . Quantity ( 1 , pAnalogInfo . szUnits ) \n 
~~~ pdwContCount = ctypes . c_uint32 ( 0 ) \n 
pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
total_read = 0 \n 
while total_read < entityInfo . dwItemCount : \n 
~~~ dwStartIndex = ctypes . c_uint32 ( total_read ) \n 
dwStopIndex = ctypes . c_uint32 ( entityInfo . dwItemCount - total_read ) \n 
neuroshare . ns_GetAnalogData ( hFile , dwEntityID , dwStartIndex , \n 
dwStopIndex , ctypes . byref ( pdwContCount ) , pData [ total_read : ] . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n 
total_read += pdwContCount . value \n 
~~ signal = pq . Quantity ( pData , units = pAnalogInfo . szUnits , copy = False ) \n 
#t_start \n 
~~ dwIndex = 0 \n 
pdTime = ctypes . c_double ( 0 ) \n 
neuroshare . ns_GetTimeByIndex ( hFile , dwEntityID , dwIndex , ctypes . byref ( pdTime ) ) \n 
anaSig = AnalogSignal ( signal , \n 
sampling_rate = pAnalogInfo . dSampleRate * pq . Hz , \n 
t_start = pdTime . value * pq . s , \n 
name = str ( entityInfo . szEntityLabel ) , \n 
anaSig . annotate ( probe_info = str ( pAnalogInfo . szProbeInfo ) ) \n 
~~~ anaSig . lazy_shape = entityInfo . dwItemCount \n 
~~ seg . analogsignals . append ( anaSig ) \n 
#segment \n 
~~ if entity_types [ entityInfo . dwEntityType ] == and import_neuroshare_segment : \n 
~~~ pdwSegmentInfo = ns_SEGMENTINFO ( ) \n 
if not str ( entityInfo . szEntityLabel ) . startswith ( ) : \n 
~~ neuroshare . ns_GetSegmentInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pdwSegmentInfo ) , ctypes . sizeof ( pdwSegmentInfo ) ) \n 
nsource = pdwSegmentInfo . dwSourceCount \n 
neuroshare . ns_GetLastErrorMsg ( ctypes . byref ( pszMsgBuffer ) , 256 ) \n 
for dwSourceID in range ( pdwSegmentInfo . dwSourceCount ) : \n 
~~~ pSourceInfo = ns_SEGSOURCEINFO ( ) \n 
neuroshare . ns_GetSegmentSourceInfo ( hFile , dwEntityID , dwSourceID , \n 
ctypes . byref ( pSourceInfo ) , ctypes . sizeof ( pSourceInfo ) ) \n 
~~ if lazy : \n 
~~~ sptr = SpikeTrain ( times , name = str ( entityInfo . szEntityLabel ) , t_stop = 0. * pq . s ) \n 
sptr . lazy_shape = entityInfo . dwItemCount \n 
~~~ pdTimeStamp = ctypes . c_double ( 0. ) \n 
dwDataBufferSize = pdwSegmentInfo . dwMaxSampleCount * pdwSegmentInfo . dwSourceCount \n 
pData = np . zeros ( ( dwDataBufferSize ) , dtype = ) \n 
pdwSampleCount = ctypes . c_uint32 ( 0 ) \n 
pdwUnitID = ctypes . c_uint32 ( 0 ) \n 
nsample = int ( dwDataBufferSize ) \n 
times = np . empty ( ( entityInfo . dwItemCount ) , dtype = ) \n 
waveforms = np . empty ( ( entityInfo . dwItemCount , nsource , nsample ) , dtype = ) \n 
~~~ neuroshare . ns_GetSegmentData ( hFile , dwEntityID , dwIndex , \n 
ctypes . byref ( pdTimeStamp ) , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) , \n 
dwDataBufferSize * 8 , ctypes . byref ( pdwSampleCount ) , \n 
ctypes . byref ( pdwUnitID ) ) \n 
times [ dwIndex ] = pdTimeStamp . value \n 
waveforms [ dwIndex , : , : ] = pData [ : nsample * nsource ] . reshape ( nsample , nsource ) . transpose ( ) \n 
~~ sptr = SpikeTrain ( times = pq . Quantity ( times , units = , copy = False ) , \n 
t_stop = times . max ( ) , \n 
waveforms = pq . Quantity ( waveforms , units = str ( pdwSegmentInfo . szUnits ) , copy = False ) , \n 
left_sweep = nsample / 2. / float ( pdwSegmentInfo . dSampleRate ) * pq . s , \n 
sampling_rate = float ( pdwSegmentInfo . dSampleRate ) * pq . Hz , \n 
~~ seg . spiketrains . append ( sptr ) \n 
~~~ pNeuralInfo = ns_NEURALINFO ( ) \n 
neuroshare . ns_GetNeuralInfo ( hFile , dwEntityID , \n 
ctypes . byref ( pNeuralInfo ) , ctypes . sizeof ( pNeuralInfo ) ) \n 
~~~ times = [ ] * pq . s \n 
t_stop = 0 * pq . s \n 
~~~ pData = np . zeros ( ( entityInfo . dwItemCount , ) , dtype = ) \n 
dwStartIndex = 0 \n 
neuroshare . ns_GetNeuralData ( hFile , dwEntityID , dwStartIndex , \n 
dwIndexCount , pData . ctypes . data_as ( ctypes . POINTER ( ctypes . c_double ) ) ) \n 
times = pData * pq . s \n 
t_stop = times . max ( ) \n 
~~ sptr = SpikeTrain ( times , t_stop = t_stop , \n 
name = str ( entityInfo . szEntityLabel ) , ) \n 
~~~ sptr . lazy_shape = entityInfo . dwItemCount \n 
~~ ~~ neuroshare . ns_CloseFile ( hFile ) \n 
seg . create_many_to_one_relationship ( ) \n 
return seg \n 
~~ ~~ class ns_FILEDESC ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_char * 32 ) , \n 
( , ctypes . c_char * 8 ) , \n 
( , ctypes . c_char * 16 ) , \n 
~~ class ns_LIBRARYINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ ( , ctypes . c_uint32 ) , \n 
( , ctypes . c_uint32 ) , \n 
( , ctypes . c_char * 64 ) , \n 
( , ns_FILEDESC * 16 ) , \n 
~~ class ns_FILEINFO ( ctypes . Structure ) : \n 
( , ctypes . c_double ) , \n 
( , ctypes . c_char * 256 ) , \n 
~~ class ns_ENTITYINFO ( ctypes . Structure ) : \n 
~~ entity_types = { 0 : , \n 
1 : , \n 
2 : , \n 
3 : , \n 
4 : , \n 
class ns_EVENTINFO ( ctypes . Structure ) : \n 
~~~ _fields_ = [ \n 
( , ctypes . c_char * 128 ) , \n 
~~ class ns_ANALOGINFO ( ctypes . Structure ) : \n 
~~ class ns_SEGMENTINFO ( ctypes . Structure ) : \n 
( , ctypes . c_char * 32 ) , \n 
~~ class ns_SEGSOURCEINFO ( ctypes . Structure ) : \n 
~~ class ns_NEURALINFO ( ctypes . Structure ) : \n 
~~~ import unittest2 as unittest \n 
~~~ import unittest \n 
~~~ from IPython . lib . pretty import pretty \n 
~~ except ImportError as err : \n 
~~~ HAVE_IPYTHON = False \n 
~~~ HAVE_IPYTHON = True \n 
~~ from neo . core . segment import Segment \n 
from neo . core import ( AnalogSignalArray , Block , \n 
Epoch , EpochArray , \n 
RecordingChannelGroup , SpikeTrain , Unit ) \n 
from neo . core . container import filterdata \n 
from neo . test . tools import ( assert_neo_object_is_compliant , \n 
assert_same_sub_schema ) \n 
from neo . test . generate_datasets import ( fake_neo , get_fake_value , \n 
get_fake_values , get_annotations , \n 
clone_object , TEST_ANNOTATIONS ) \n 
class Test__generate_datasets ( unittest . TestCase ) : \n 
~~~ def setUp ( self ) : \n 
~~~ np . random . seed ( 0 ) \n 
self . annotations = dict ( [ ( str ( x ) , TEST_ANNOTATIONS [ x ] ) for x in \n 
range ( len ( TEST_ANNOTATIONS ) ) ] ) \n 
~~ def test__get_fake_values ( self ) : \n 
~~~ self . annotations [ ] = 0 \n 
file_datetime = get_fake_value ( , datetime , seed = 0 ) \n 
rec_datetime = get_fake_value ( , datetime , seed = 1 ) \n 
index = get_fake_value ( , int , seed = 2 ) \n 
name = get_fake_value ( , str , seed = 3 , obj = Segment ) \n 
description = get_fake_value ( , str , seed = 4 , obj = ) \n 
file_origin = get_fake_value ( , str ) \n 
attrs1 = { : file_datetime , \n 
: rec_datetime , \n 
: index , \n 
: name , \n 
: description , \n 
: file_origin } \n 
attrs2 = attrs1 . copy ( ) \n 
attrs2 . update ( self . annotations ) \n 
res11 = get_fake_values ( Segment , annotate = False , seed = 0 ) \n 
res12 = get_fake_values ( , annotate = False , seed = 0 ) \n 
res21 = get_fake_values ( Segment , annotate = True , seed = 0 ) \n 
res22 = get_fake_values ( , annotate = True , seed = 0 ) \n 
self . assertEqual ( res11 , attrs1 ) \n 
self . assertEqual ( res12 , attrs1 ) \n 
self . assertEqual ( res21 , attrs2 ) \n 
self . assertEqual ( res22 , attrs2 ) \n 
~~ def test__fake_neo__cascade ( self ) : \n 
~~~ self . annotations [ ] = None \n 
obj_type = Segment \n 
cascade = True \n 
res = fake_neo ( obj_type = obj_type , cascade = cascade ) \n 
self . assertTrue ( isinstance ( res , Segment ) ) \n 
assert_neo_object_is_compliant ( res ) \n 
self . assertEqual ( res . annotations , self . annotations ) \n 
self . assertEqual ( len ( res . analogsignalarrays ) , 1 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 1 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 1 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 1 ) \n 
self . assertEqual ( len ( res . spikes ) , 1 ) \n 
self . assertEqual ( len ( res . events ) , 1 ) \n 
self . assertEqual ( len ( res . epochs ) , 1 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 1 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 1 ) \n 
for child in res . children : \n 
~~~ del child . annotations [ ] \n 
del child . annotations [ ] \n 
~~ self . assertEqual ( res . analogsignalarrays [ 0 ] . annotations , \n 
self . annotations ) \n 
self . assertEqual ( res . analogsignals [ 0 ] . annotations , \n 
self . assertEqual ( res . irregularlysampledsignals [ 0 ] . annotations , \n 
self . assertEqual ( res . spiketrains [ 0 ] . annotations , \n 
self . assertEqual ( res . spikes [ 0 ] . annotations , \n 
self . assertEqual ( res . events [ 0 ] . annotations , \n 
self . assertEqual ( res . epochs [ 0 ] . annotations , \n 
self . assertEqual ( res . eventarrays [ 0 ] . annotations , \n 
self . assertEqual ( res . epocharrays [ 0 ] . annotations , \n 
~~ def test__fake_neo__nocascade ( self ) : \n 
obj_type = \n 
cascade = False \n 
self . assertEqual ( len ( res . analogsignalarrays ) , 0 ) \n 
self . assertEqual ( len ( res . analogsignals ) , 0 ) \n 
self . assertEqual ( len ( res . irregularlysampledsignals ) , 0 ) \n 
self . assertEqual ( len ( res . spiketrains ) , 0 ) \n 
self . assertEqual ( len ( res . spikes ) , 0 ) \n 
self . assertEqual ( len ( res . events ) , 0 ) \n 
self . assertEqual ( len ( res . epochs ) , 0 ) \n 
self . assertEqual ( len ( res . eventarrays ) , 0 ) \n 
self . assertEqual ( len ( res . epocharrays ) , 0 ) \n 
~~ ~~ class TestSegment ( unittest . TestCase ) : \n 
~~~ self . nchildren = 2 \n 
blk = fake_neo ( Block , seed = 0 , n = self . nchildren ) \n 
self . unit1 , self . unit2 , self . unit3 , self . unit4 = blk . list_units \n 
self . seg1 , self . seg2 = blk . segments \n 
self . targobj = self . seg1 \n 
self . seed1 = self . seg1 . annotations [ ] \n 
self . seed2 = self . seg2 . annotations [ ] \n 
del self . seg1 . annotations [ ] \n 
del self . seg2 . annotations [ ] \n 
self . sigs1 = self . seg1 . analogsignals \n 
self . sigs2 = self . seg2 . analogsignals \n 
self . sigarrs1 = self . seg1 . analogsignalarrays \n 
self . sigarrs2 = self . seg2 . analogsignalarrays \n 
self . irsigs1 = self . seg1 . irregularlysampledsignals \n 
self . irsigs2 = self . seg2 . irregularlysampledsignals \n 
self . spikes1 = self . seg1 . spikes \n 
self . spikes2 = self . seg2 . spikes \n 
self . trains1 = self . seg1 . spiketrains \n 
self . trains2 = self . seg2 . spiketrains \n 
self . epcs1 = self . seg1 . epochs \n 
self . epcs2 = self . seg2 . epochs \n 
self . epcas1 = self . seg1 . epocharrays \n 
self . epcas2 = self . seg2 . epocharrays \n 
self . evts1 = self . seg1 . events \n 
self . evts2 = self . seg2 . events \n 
self . evtas1 = self . seg1 . eventarrays \n 
self . evtas2 = self . seg2 . eventarrays \n 
self . sigs1a = clone_object ( self . sigs1 ) \n 
self . sigarrs1a = clone_object ( self . sigarrs1 , n = 2 ) \n 
self . irsigs1a = clone_object ( self . irsigs1 ) \n 
self . spikes1a = clone_object ( self . spikes1 ) \n 
self . trains1a = clone_object ( self . trains1 ) \n 
self . epcs1a = clone_object ( self . epcs1 ) \n 
self . epcas1a = clone_object ( self . epcas1 ) \n 
self . evts1a = clone_object ( self . evts1 ) \n 
self . evtas1a = clone_object ( self . evtas1 ) \n 
for obj , obja in zip ( self . sigs1 + self . sigarrs1 , \n 
self . sigs1a + self . sigarrs1a ) : \n 
~~~ obja . channel_index = obj . channel_index \n 
~~ ~~ def test_init ( self ) : \n 
~~~ seg = Segment ( name = , index = 3 ) \n 
assert_neo_object_is_compliant ( seg ) \n 
self . assertEqual ( seg . name , ) \n 
self . assertEqual ( seg . file_origin , None ) \n 
self . assertEqual ( seg . index , 3 ) \n 
~~ def check_creation ( self , seg ) : \n 
~~~ assert_neo_object_is_compliant ( seg ) \n 
seed = seg . annotations [ ] \n 
targ0 = get_fake_value ( , datetime , seed = seed + 0 ) \n 
self . assertEqual ( seg . file_datetime , targ0 ) \n 
targ1 = get_fake_value ( , datetime , seed = seed + 1 ) \n 
self . assertEqual ( seg . rec_datetime , targ1 ) \n 
targ2 = get_fake_value ( , int , seed = seed + 2 ) \n 
self . assertEqual ( seg . index , targ2 ) \n 
targ3 = get_fake_value ( , str , seed = seed + 3 , obj = Segment ) \n 
self . assertEqual ( seg . name , targ3 ) \n 
targ4 = get_fake_value ( , str , \n 
seed = seed + 4 , obj = Segment ) \n 
self . assertEqual ( seg . description , targ4 ) \n 
targ5 = get_fake_value ( , str ) \n 
self . assertEqual ( seg . file_origin , targ5 ) \n 
targ6 = get_annotations ( ) \n 
targ6 [ ] = seed \n 
self . assertEqual ( seg . annotations , targ6 ) \n 
self . assertTrue ( hasattr ( seg , ) ) \n 
self . assertEqual ( len ( seg . analogsignals ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . analogsignalarrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . irregularlysampledsignals ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . epochs ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . epocharrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . events ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . eventarrays ) , self . nchildren ) \n 
self . assertEqual ( len ( seg . spikes ) , self . nchildren ** 2 ) \n 
self . assertEqual ( len ( seg . spiketrains ) , self . nchildren ** 2 ) \n 
~~ def test__creation ( self ) : \n 
~~~ self . check_creation ( self . seg1 ) \n 
self . check_creation ( self . seg2 ) \n 
~~ def test__merge ( self ) : \n 
~~~ seg1a = fake_neo ( Block , seed = self . seed1 , n = self . nchildren ) . segments [ 0 ] \n 
assert_same_sub_schema ( self . seg1 , seg1a ) \n 
seg1a . spikes . append ( self . spikes2 [ 0 ] ) \n 
seg1a . epocharrays . append ( self . epcas2 [ 0 ] ) \n 
seg1a . annotate ( seed = self . seed2 ) \n 
seg1a . merge ( self . seg2 ) \n 
assert_same_sub_schema ( self . sigs1a + self . sigs2 , seg1a . analogsignals ) \n 
assert_same_sub_schema ( self . sigarrs1a + self . sigarrs2 , \n 
seg1a . analogsignalarrays ) \n 
assert_same_sub_schema ( self . irsigs1a + self . irsigs2 , \n 
seg1a . irregularlysampledsignals ) \n 
assert_same_sub_schema ( self . epcs1 + self . epcs2 , seg1a . epochs ) \n 
assert_same_sub_schema ( self . epcas1 + self . epcas2 , seg1a . epocharrays ) \n 
assert_same_sub_schema ( self . evts1 + self . evts2 , seg1a . events ) \n 
assert_same_sub_schema ( self . evtas1 + self . evtas2 , seg1a . eventarrays ) \n 
assert_same_sub_schema ( self . spikes1 + self . spikes2 , seg1a . spikes ) \n 
assert_same_sub_schema ( self . trains1 + self . trains2 , seg1a . spiketrains ) \n 
~~ def test__children ( self ) : \n 
~~~ blk = Block ( name = ) \n 
blk . segments = [ self . seg1 ] \n 
blk . create_many_to_one_relationship ( force = True ) \n 
assert_neo_object_is_compliant ( self . seg1 ) \n 
assert_neo_object_is_compliant ( blk ) \n 
childobjs = ( , , \n 
, , \n 
, \n 
, ) \n 
childconts = ( , , \n 
self . assertEqual ( self . seg1 . _container_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _single_parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_objects , ( ) ) \n 
self . assertEqual ( self . seg1 . _child_properties , ( ) ) \n 
self . assertEqual ( self . seg1 . _single_child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _container_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _data_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _single_parent_containers , ( , ) ) \n 
self . assertEqual ( self . seg1 . _multi_child_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _multi_parent_containers , ( ) ) \n 
self . assertEqual ( self . seg1 . _child_objects , childobjs ) \n 
self . assertEqual ( self . seg1 . _child_containers , childconts ) \n 
self . assertEqual ( self . seg1 . _parent_objects , ( , ) ) \n 
self . assertEqual ( self . seg1 . _parent_containers , ( , ) ) \n 
self . assertEqual ( len ( self . seg1 . _single_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . data_children_recur ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . children_recur ) , totchildren ) \n 
self . assertEqual ( len ( self . seg1 . _multi_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children ) , 0 ) \n 
self . assertEqual ( len ( self . seg1 . container_children_recur ) , 0 ) \n 
children = ( self . sigs1a + self . sigarrs1a + \n 
self . epcs1a + self . epcas1a + \n 
self . evts1a + self . evtas1a + \n 
self . irsigs1a + \n 
self . spikes1a + self . trains1a ) \n 
assert_same_sub_schema ( list ( self . seg1 . _single_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . data_children_recur ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children ) , children ) \n 
assert_same_sub_schema ( list ( self . seg1 . children_recur ) , children ) \n 
self . assertEqual ( len ( self . seg1 . parents ) , 1 ) \n 
self . assertEqual ( self . seg1 . parents [ 0 ] . name , ) \n 
~~ def test__size ( self ) : \n 
~~~ targ1 = { "epochs" : self . nchildren , "events" : self . nchildren , \n 
"analogsignals" : self . nchildren ** 2 , \n 
"irregularlysampledsignals" : self . nchildren ** 2 , \n 
"spikes" : self . nchildren ** 2 , \n 
"spiketrains" : self . nchildren ** 2 , \n 
"epocharrays" : self . nchildren , "eventarrays" : self . nchildren , \n 
"analogsignalarrays" : self . nchildren } \n 
self . assertEqual ( self . targobj . size , targ1 ) \n 
~~ def test__filter_none ( self ) : \n 
~~~ targ = [ ] \n 
res0 = self . targobj . filter ( ) \n 
res1 = self . targobj . filter ( { } ) \n 
res2 = self . targobj . filter ( [ ] ) \n 
res3 = self . targobj . filter ( [ { } ] ) \n 
res4 = self . targobj . filter ( [ { } , { } ] ) \n 
res5 = self . targobj . filter ( [ { } , { } ] ) \n 
res6 = self . targobj . filter ( targdict = { } ) \n 
res7 = self . targobj . filter ( targdict = [ ] ) \n 
res8 = self . targobj . filter ( targdict = [ { } ] ) \n 
res9 = self . targobj . filter ( targdict = [ { } , { } ] ) \n 
assert_same_sub_schema ( res0 , targ ) \n 
assert_same_sub_schema ( res1 , targ ) \n 
assert_same_sub_schema ( res2 , targ ) \n 
assert_same_sub_schema ( res3 , targ ) \n 
assert_same_sub_schema ( res4 , targ ) \n 
assert_same_sub_schema ( res5 , targ ) \n 
assert_same_sub_schema ( res6 , targ ) \n 
assert_same_sub_schema ( res7 , targ ) \n 
assert_same_sub_schema ( res8 , targ ) \n 
assert_same_sub_schema ( res9 , targ ) \n 
~~ def test__filter_annotation_single ( self ) : \n 
~~~ targ = ( self . sigs1a + self . sigarrs1a + \n 
[ self . epcs1a [ 0 ] , self . epcas1a [ 0 ] ] + \n 
[ self . evts1a [ 0 ] , self . evtas1a [ 0 ] ] + \n 
res0 = self . targobj . filter ( j = 0 ) \n 
res1 = self . targobj . filter ( { : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : 0 } ) \n 
res3 = self . targobj . filter ( [ { : 0 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 0 } ] ) \n 
~~ def test__filter_single_annotation_nores ( self ) : \n 
res0 = self . targobj . filter ( j = 5 ) \n 
res1 = self . targobj . filter ( { : 5 } ) \n 
res2 = self . targobj . filter ( targdict = { : 5 } ) \n 
res3 = self . targobj . filter ( [ { : 5 } ] ) \n 
res4 = self . targobj . filter ( targdict = [ { : 5 } ] ) \n 
~~ def test__filter_attribute_single ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] ] \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } ) \n 
~~ def test__filter_attribute_single_nores ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name ) \n 
res1 = self . targobj . filter ( { : self . epcs2 [ 0 ] . name } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs2 [ 0 ] . name } ) \n 
~~ def test__filter_multi ( self ) : \n 
self . spikes1a + self . trains1a + \n 
[ self . epcs1a [ 1 ] ] ) \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name , \n 
: 0 } ) \n 
~~ def test__filter_multi_nores ( self ) : \n 
res0 = self . targobj . filter ( [ { : 0 } , { } ] ) \n 
res1 = self . targobj . filter ( { } , ttype = 0 ) \n 
res2 = self . targobj . filter ( [ { } ] , ttype = 0 ) \n 
res3 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
j = 0 ) \n 
res5 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
targdict = { : 0 } ) \n 
res6 = self . targobj . filter ( name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name , \n 
: 5 } ) \n 
res9 = self . targobj . filter ( { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = self . targobj . filter ( targdict = { : self . epcs2 [ 1 ] . name } , \n 
j = 5 ) \n 
res11 = self . targobj . filter ( name = self . epcs2 [ 1 ] . name , \n 
targdict = { : 5 } ) \n 
res12 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = self . targobj . filter ( targdict = { : self . epcs1a [ 1 ] . name } , \n 
res14 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
assert_same_sub_schema ( res10 , targ ) \n 
assert_same_sub_schema ( res11 , targ ) \n 
assert_same_sub_schema ( res12 , targ ) \n 
assert_same_sub_schema ( res13 , targ ) \n 
assert_same_sub_schema ( res14 , targ ) \n 
~~ def test__filter_multi_partres ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = self . targobj . filter ( { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res3 = self . targobj . filter ( [ { : 1 } , { : 2 } ] ) \n 
res4 = self . targobj . filter ( { : 1 } , i = 2 ) \n 
res5 = self . targobj . filter ( [ { : 1 } ] , i = 2 ) \n 
~~ def test__filter_single_annotation_obj_single ( self ) : \n 
res0 = self . targobj . filter ( j = 1 , objects = ) \n 
res1 = self . targobj . filter ( j = 1 , objects = Epoch ) \n 
res2 = self . targobj . filter ( j = 1 , objects = [ ] ) \n 
res3 = self . targobj . filter ( j = 1 , objects = [ Epoch ] ) \n 
res4 = self . targobj . filter ( j = 1 , objects = [ Epoch , \n 
RecordingChannelGroup ] ) \n 
~~ def test__filter_single_annotation_obj_multi ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , objects = [ , EpochArray ] ) \n 
~~ def test__filter_single_annotation_obj_none ( self ) : \n 
res0 = self . targobj . filter ( j = 1 , objects = RecordingChannelGroup ) \n 
res1 = self . targobj . filter ( j = 1 , objects = ) \n 
~~ def test__filter_single_annotation_norecur ( self ) : \n 
~~~ targ = [ self . epcs1a [ 1 ] , self . epcas1a [ 1 ] , \n 
self . evts1a [ 1 ] , self . evtas1a [ 1 ] ] \n 
res0 = self . targobj . filter ( j = 1 , \n 
recursive = False ) \n 
~~ def test__filter_single_attribute_norecur ( self ) : \n 
res0 = self . targobj . filter ( name = self . epcs1a [ 1 ] . name , \n 
~~ def test__filter_single_annotation_nodata ( self ) : \n 
res0 = self . targobj . filter ( j = 0 , \n 
data = False ) \n 
~~ def test__filter_single_attribute_nodata ( self ) : \n 
~~ def test__filter_single_annotation_nodata_norecur ( self ) : \n 
data = False , recursive = False ) \n 
~~ def test__filter_single_attribute_nodata_norecur ( self ) : \n 
~~ def test__filter_single_annotation_container ( self ) : \n 
container = True ) \n 
~~ def test__filter_single_attribute_container ( self ) : \n 
~~ def test__filter_single_annotation_container_norecur ( self ) : \n 
container = True , recursive = False ) \n 
~~ def test__filter_single_attribute_container_norecur ( self ) : \n 
~~ def test__filter_single_annotation_nodata_container ( self ) : \n 
data = False , container = True ) \n 
~~ def test__filter_single_attribute_nodata_container ( self ) : \n 
~~ def test__filter_single_annotation_nodata_container_norecur ( self ) : \n 
data = False , container = True , \n 
~~ def test__filter_single_attribute_nodata_container_norecur ( self ) : \n 
data = self . targobj . children_recur \n 
targ = ( self . sigs1a + self . sigarrs1a + \n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 0 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 0 } ) \n 
~~ def test__filterdata_multi_nores ( self ) : \n 
~~~ data = self . targobj . children_recur \n 
targ = [ ] \n 
res0 = filterdata ( data , [ { : 0 } , { } ] ) \n 
res1 = filterdata ( data , { } , ttype = 0 ) \n 
res2 = filterdata ( data , [ { } ] , ttype = 0 ) \n 
res3 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res4 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 0 ) \n 
res5 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 0 } ) \n 
res6 = filterdata ( data , name = self . epcs2 [ 0 ] . name , j = 5 ) \n 
res7 = filterdata ( data , { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res8 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name , : 5 } ) \n 
res9 = filterdata ( data , { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res10 = filterdata ( data , targdict = { : self . epcs2 [ 1 ] . name } , j = 5 ) \n 
res11 = filterdata ( data , name = self . epcs2 [ 1 ] . name , targdict = { : 5 } ) \n 
res12 = filterdata ( data , { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res13 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name } , j = 5 ) \n 
res14 = filterdata ( data , name = self . epcs1a [ 1 ] . name , targdict = { : 5 } ) \n 
~~ def test__filterdata_multi_partres ( self ) : \n 
targ = [ self . epcs1a [ 1 ] ] \n 
res0 = filterdata ( data , name = self . epcs1a [ 1 ] . name , j = 5 ) \n 
res1 = filterdata ( data , { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res2 = filterdata ( data , targdict = { : self . epcs1a [ 1 ] . name , : 5 } ) \n 
res3 = filterdata ( data , [ { : 1 } , { : 2 } ] ) \n 
res4 = filterdata ( data , { : 1 } , i = 2 ) \n 
res5 = filterdata ( data , [ { : 1 } ] , i = 2 ) \n 
def test__pretty ( self ) : \n 
~~~ ann = get_annotations ( ) \n 
ann [ ] = self . seed1 \n 
ann = pretty ( ann ) . replace ( , ) \n 
res = pretty ( self . seg1 ) \n 
sig0 = pretty ( self . sigs1 [ 0 ] ) \n 
sig1 = pretty ( self . sigs1 [ 1 ] ) \n 
sig2 = pretty ( self . sigs1 [ 2 ] ) \n 
sig3 = pretty ( self . sigs1 [ 3 ] ) \n 
sig0 = sig0 . replace ( , ) \n 
sig1 = sig1 . replace ( , ) \n 
sig2 = sig2 . replace ( , ) \n 
sig3 = sig3 . replace ( , ) \n 
sigarr0 = pretty ( self . sigarrs1 [ 0 ] ) \n 
sigarr1 = pretty ( self . sigarrs1 [ 1 ] ) \n 
sigarr0 = sigarr0 . replace ( , ) \n 
sigarr1 = sigarr1 . replace ( , ) \n 
( len ( self . sigs1a ) , len ( self . sigarrs1a ) ) ) + \n 
( len ( self . epcs1a ) , len ( self . epcas1a ) ) ) + \n 
( len ( self . evts1a ) , len ( self . evtas1a ) ) ) + \n 
len ( self . irsigs1a ) ) + \n 
( len ( self . spikes1a ) , len ( self . trains1a ) ) ) + \n 
( self . seg1 . name , self . seg1 . description ) \n 
) + \n 
( % ( 0 , sig0 ) ) + \n 
( % ( 1 , sig1 ) ) + \n 
( % ( 2 , sig2 ) ) + \n 
( % ( 3 , sig3 ) ) + \n 
( % ( 0 , sigarr0 ) ) + \n 
( % ( 1 , sigarr1 ) ) ) \n 
self . assertEqual ( res , targ ) \n 
~~ def test__construct_subsegment_by_unit ( self ) : \n 
~~~ nb_seg = 3 \n 
nb_unit = 7 \n 
unit_with_sig = np . array ( [ 0 , 2 , 5 ] ) \n 
signal_types = [ , ] \n 
sig_len = 100 \n 
#recordingchannelgroups \n 
rcgs = [ RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) , \n 
RecordingChannelGroup ( name = , \n 
channel_indexes = unit_with_sig ) ] \n 
all_unit = [ ] \n 
for u in range ( nb_unit ) : \n 
~~~ un = Unit ( name = % u , channel_indexes = np . array ( [ u ] ) ) \n 
assert_neo_object_is_compliant ( un ) \n 
all_unit . append ( un ) \n 
~~ blk = Block ( ) \n 
blk . recordingchannelgroups = rcgs \n 
for s in range ( nb_seg ) : \n 
~~~ seg = Segment ( name = % s ) \n 
for j in range ( nb_unit ) : \n 
~~~ st = SpikeTrain ( [ 1 , 2 ] , units = , \n 
t_start = 0. , t_stop = 10 ) \n 
st . unit = all_unit [ j ] \n 
~~ for t in signal_types : \n 
~~~ anasigarr = AnalogSignalArray ( np . zeros ( ( sig_len , \n 
len ( unit_with_sig ) ) ) , \n 
units = , \n 
sampling_rate = 1000. * pq . Hz , \n 
channel_indexes = unit_with_sig ) \n 
seg . analogsignalarrays . append ( anasigarr ) \n 
~~ ~~ blk . create_many_to_one_relationship ( ) \n 
for unit in all_unit : \n 
~~~ assert_neo_object_is_compliant ( unit ) \n 
~~ for rcg in rcgs : \n 
~~~ assert_neo_object_is_compliant ( rcg ) \n 
~~ assert_neo_object_is_compliant ( blk ) \n 
newseg = seg . construct_subsegment_by_unit ( all_unit [ : 4 ] ) \n 
assert_neo_object_is_compliant ( newseg ) \n 
~~ def test_segment_take_spikes_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spikes_by_unit ( ) \n 
result21 = self . seg1 . take_spikes_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spikes_by_unit ( [ self . unit2 ] ) \n 
self . assertEqual ( result1 , [ ] ) \n 
assert_same_sub_schema ( result21 , [ self . spikes1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . spikes1a [ 1 ] ] ) \n 
~~ def test_segment_take_spiketrains_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_spiketrains_by_unit ( ) \n 
result21 = self . seg1 . take_spiketrains_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_spiketrains_by_unit ( [ self . unit2 ] ) \n 
assert_same_sub_schema ( result21 , [ self . trains1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . trains1a [ 1 ] ] ) \n 
~~ def test_segment_take_analogsignal_by_unit ( self ) : \n 
~~~ result1 = self . seg1 . take_analogsignal_by_unit ( ) \n 
result21 = self . seg1 . take_analogsignal_by_unit ( [ self . unit1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_unit ( [ self . unit2 ] ) \n 
assert_same_sub_schema ( result21 , [ self . sigs1a [ 0 ] ] ) \n 
assert_same_sub_schema ( result22 , [ self . sigs1a [ 1 ] ] ) \n 
~~ def test_segment_take_analogsignal_by_channelindex ( self ) : \n 
~~~ ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind2 = self . unit2 . channel_indexes [ 0 ] \n 
result1 = self . seg1 . take_analogsignal_by_channelindex ( ) \n 
result21 = self . seg1 . take_analogsignal_by_channelindex ( [ ind1 ] ) \n 
result22 = self . seg1 . take_analogsignal_by_channelindex ( [ ind2 ] ) \n 
~~ def test_seg_take_slice_of_analogsignalarray_by_unit ( self ) : \n 
~~~ seg = self . seg1 \n 
result1 = seg . take_slice_of_analogsignalarray_by_unit ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_unit ( [ self . unit3 ] ) \n 
targ1 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ True ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ False ] ) ] ] \n 
targ3 = [ self . sigarrs1a [ 0 ] [ : , np . array ( [ False ] ) ] , \n 
self . sigarrs1a [ 1 ] [ : , np . array ( [ True ] ) ] ] \n 
assert_same_sub_schema ( result21 , targ1 ) \n 
assert_same_sub_schema ( result23 , targ3 ) \n 
~~ def test_seg_take_slice_of_analogsignalarray_by_channelindex ( self ) : \n 
ind1 = self . unit1 . channel_indexes [ 0 ] \n 
ind3 = self . unit3 . channel_indexes [ 0 ] \n 
result1 = seg . take_slice_of_analogsignalarray_by_channelindex ( ) \n 
result21 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind1 ] ) \n 
result23 = seg . take_slice_of_analogsignalarray_by_channelindex ( [ ind3 ] ) \n 
~~ ~~ if __name__ == "__main__" : \n 
~~~ unittest . main ( ) \n 
from __future__ import absolute_import , division \n 
~~ from neo . io import NeuroScopeIO \n 
from neo . test . iotest . common_io_test import BaseTestIO \n 
class TestNeuroScopeIO ( BaseTestIO , unittest . TestCase , ) : \n 
~~~ ioclass = NeuroScopeIO \n 
files_to_test = [ ] \n 
files_to_download = [ , \n 
~~ if __name__ == "__main__" : \n 
~~ import theano \n 
from theano import tensor as T \n 
from theano . sandbox . rng_mrg import MRG_RandomStreams as RandomStreams \n 
from load import mnist \n 
srng = RandomStreams ( ) \n 
def floatX ( X ) : \n 
~~~ return np . asarray ( X , dtype = theano . config . floatX ) \n 
~~ def init_weights ( shape ) : \n 
~~~ return theano . shared ( floatX ( np . random . randn ( * shape ) * 0.01 ) ) \n 
~~ def rectify ( X ) : \n 
~~~ return T . maximum ( X , 0. ) \n 
~~ def softmax ( X ) : \n 
~~~ e_x = T . exp ( X - X . max ( axis = 1 ) . dimshuffle ( 0 , ) ) \n 
return e_x / e_x . sum ( axis = 1 ) . dimshuffle ( 0 , ) \n 
~~ def RMSprop ( cost , params , lr = 0.001 , rho = 0.9 , epsilon = 1e-6 ) : \n 
~~~ grads = T . grad ( cost = cost , wrt = params ) \n 
updates = [ ] \n 
for p , g in zip ( params , grads ) : \n 
~~~ acc = theano . shared ( p . get_value ( ) * 0. ) \n 
acc_new = rho * acc + ( 1 - rho ) * g ** 2 \n 
gradient_scaling = T . sqrt ( acc_new + epsilon ) \n 
g = g / gradient_scaling \n 
updates . append ( ( acc , acc_new ) ) \n 
updates . append ( ( p , p - lr * g ) ) \n 
~~ return updates \n 
~~ def dropout ( X , p = 0. ) : \n 
~~~ if p > 0 : \n 
~~~ retain_prob = 1 - p \n 
X *= srng . binomial ( X . shape , p = retain_prob , dtype = theano . config . floatX ) \n 
X /= retain_prob \n 
~~ def model ( X , w_h , w_h2 , w_o , p_drop_input , p_drop_hidden ) : \n 
~~~ X = dropout ( X , p_drop_input ) \n 
h = rectify ( T . dot ( X , w_h ) ) \n 
h = dropout ( h , p_drop_hidden ) \n 
h2 = rectify ( T . dot ( h , w_h2 ) ) \n 
h2 = dropout ( h2 , p_drop_hidden ) \n 
py_x = softmax ( T . dot ( h2 , w_o ) ) \n 
return h , h2 , py_x \n 
~~ trX , teX , trY , teY = mnist ( onehot = True ) \n 
X = T . fmatrix ( ) \n 
Y = T . fmatrix ( ) \n 
w_h = init_weights ( ( 784 , 625 ) ) \n 
w_h2 = init_weights ( ( 625 , 625 ) ) \n 
w_o = init_weights ( ( 625 , 10 ) ) \n 
noise_h , noise_h2 , noise_py_x = model ( X , w_h , w_h2 , w_o , 0.2 , 0.5 ) \n 
h , h2 , py_x = model ( X , w_h , w_h2 , w_o , 0. , 0. ) \n 
y_x = T . argmax ( py_x , axis = 1 ) \n 
cost = T . mean ( T . nnet . categorical_crossentropy ( noise_py_x , Y ) ) \n 
params = [ w_h , w_h2 , w_o ] \n 
updates = RMSprop ( cost , params , lr = 0.001 ) \n 
train = theano . function ( inputs = [ X , Y ] , outputs = cost , updates = updates , allow_input_downcast = True ) \n 
predict = theano . function ( inputs = [ X ] , outputs = y_x , allow_input_downcast = True ) \n 
for i in range ( 100 ) : \n 
~~~ for start , end in zip ( range ( 0 , len ( trX ) , 128 ) , range ( 128 , len ( trX ) , 128 ) ) : \n 
~~~ cost = train ( trX [ start : end ] , trY [ start : end ] ) \n 
~~ print np . mean ( np . argmax ( teY , axis = 1 ) == predict ( teX ) ) \n 
from matplotlib import pyplot as plt \n 
import oscaar \n 
import astrometry \n 
import photometry \n 
import dataBank \n 
import systematics \n 
import IO \n 
import pyfits \n 
plt . ion ( ) \n 
data = dataBank . dataBank ( ) \n 
allStars = data . getDict ( ) \n 
outputPath = data . outputPath \n 
N_exposures = len ( data . getPaths ( ) ) \n 
meanDarkFrame = data . getMeanDarkFrame ( ) \n 
masterFlat = data . masterFlat \n 
plottingThings , statusBarFig , statusBarAx = IO . plottingSettings ( data . trackPlots , data . photPlots ) \n 
for expNumber in xrange ( N_exposures ) : \n 
~~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
~~~ plt . cla ( ) \n 
statusBarAx . set_title ( ) \n 
statusBarAx . set_xlim ( [ 0 , 100 ] ) \n 
statusBarAx . set_xlabel ( ) \n 
statusBarAx . get_yaxis ( ) . set_ticks ( [ ] ) \n 
statusBarAx . barh ( [ 0 ] , [ 100.0 * expNumber / len ( data . getPaths ( ) ) ] , \n 
[ 1 ] , color = ) \n 
~~ image = ( pyfits . getdata ( data . getPaths ( ) [ expNumber ] ) - meanDarkFrame ) / masterFlat \n 
data . storeTime ( expNumber ) \n 
for star in allStars : \n 
~~~ est_x , est_y = data . centroidInitialGuess ( expNumber , star ) \n 
x , y , radius , trackFlag = astrometry . trackSmooth ( image , est_x , est_y , \n 
data . smoothConst , \n 
plottingThings , \n 
zoom = data . trackingZoom , \n 
plots = data . trackPlots ) \n 
data . storeCentroid ( star , expNumber , x , y ) \n 
fluxes , errors , photFlags = photometry . multirad ( image , x , y , \n 
data . apertureRadii , \n 
ccdGain = data . ccdGain , \n 
plots = data . photPlots ) \n 
photFlag = any ( photFlags ) \n 
data . storeFluxes ( star , expNumber , fluxes , errors ) \n 
if trackFlag or photFlag and not data . getFlag ( ) : \n 
~~~ data . setFlag ( star , False ) \n 
~~ if data . trackPlots or data . photPlots : \n 
~~~ plt . draw ( ) \n 
~~ ~~ if statusBarAx is not None and expNumber % 15 == 0 : \n 
~~ ~~ plt . close ( ) \n 
data . scaleFluxes_multirad ( ) \n 
meanComparisonStars , meanComparisonStarErrors = data . calcMeanComparison_multirad ( ccdGain = data . ccdGain ) \n 
lightCurves , lightCurveErrors = data . computeLightCurve_multirad ( meanComparisonStars , \n 
meanComparisonStarErrors ) \n 
oscaar . IO . save ( data , outputPath ) \n 
data . plotLightCurve_multirad ( ) \n 
import warnings \n 
from openmdao . components . exec_comp import ExecComp \n 
class ConstraintComp ( ExecComp ) : \n 
def __init__ ( self , expr , out = ) : \n 
~~~ warnings . simplefilter ( , DeprecationWarning ) \n 
DeprecationWarning , stacklevel = 2 ) \n 
warnings . simplefilter ( , DeprecationWarning ) \n 
newexpr = _combined_expr ( expr ) \n 
~~ ~~ def _combined_expr ( expr ) : \n 
lhs , op , rhs = _parse_constraint ( expr ) \n 
first , second = ( rhs , lhs ) if op . startswith ( ) else ( lhs , rhs ) \n 
~~~ if float ( first ) == 0 : \n 
~~~ return "-(%s)" % second \n 
~~ ~~ except Exception : \n 
~~~ if float ( second ) == 0. : \n 
~~~ return first \n 
~~ return % ( first , second ) \n 
~~ def _parse_constraint ( expr_string ) : \n 
for comparator in [ , , , , , ] : \n 
~~~ parts = expr_string . split ( comparator ) \n 
if len ( parts ) == 2 : \n 
~~~ if comparator == : \n 
~~~ break \n 
~~ return ( parts [ 0 ] . strip ( ) , comparator , parts [ 1 ] . strip ( ) ) \n 
raise ValueError ( msg ) \n 
import traceback \n 
from itertools import chain \n 
from six import iteritems , itervalues \n 
from six . moves import cStringIO \n 
import networkx as nx \n 
from openmdao . core . system import System \n 
from openmdao . core . group import Group \n 
from openmdao . core . component import Component \n 
from openmdao . core . parallel_group import ParallelGroup \n 
from openmdao . core . parallel_fd_group import ParallelFDGroup \n 
from openmdao . core . basic_impl import BasicImpl \n 
from openmdao . core . _checks import check_connections , _both_names \n 
from openmdao . core . driver import Driver \n 
from openmdao . core . mpi_wrap import MPI , under_mpirun , debug \n 
from openmdao . core . relevance import Relevance \n 
from openmdao . components . indep_var_comp import IndepVarComp \n 
from openmdao . solvers . scipy_gmres import ScipyGMRES \n 
from openmdao . solvers . ln_direct import DirectSolver \n 
from openmdao . solvers . ln_gauss_seidel import LinearGaussSeidel \n 
from openmdao . units . units import get_conversion_tuple \n 
from openmdao . util . string_util import get_common_ancestor , nearest_child , name_relative_to \n 
from openmdao . util . graph import plain_bfs \n 
from openmdao . util . options import OptionsDictionary \n 
force_check = os . environ . get ( ) \n 
trace = os . environ . get ( ) \n 
class _ProbData ( object ) : \n 
~~~ self . top_lin_gs = False \n 
self . in_complex_step = False \n 
~~ ~~ class Problem ( object ) : \n 
def __init__ ( self , root = None , driver = None , impl = None , comm = None ) : \n 
~~~ super ( Problem , self ) . __init__ ( ) \n 
self . root = root \n 
self . _probdata = _ProbData ( ) \n 
if MPI : \n 
~~~ from openmdao . core . petsc_impl import PetscImpl \n 
if impl != PetscImpl : \n 
~~ ~~ if impl is None : \n 
~~~ self . _impl = BasicImpl \n 
~~~ self . _impl = impl \n 
~~ self . comm = comm \n 
if driver is None : \n 
~~~ self . driver = Driver ( ) \n 
~~~ self . driver = driver \n 
~~ self . pathname = \n 
~~ def __getitem__ ( self , name ) : \n 
if name in self . root . unknowns : \n 
~~~ return self . root . unknowns [ name ] \n 
~~ elif name in self . root . params : \n 
~~~ return self . root . params [ name ] \n 
~~ elif name in self . root . _sysdata . to_abs_pnames : \n 
~~~ for p in self . root . _sysdata . to_abs_pnames [ name ] : \n 
~~~ return self . _rec_get_param ( p ) \n 
~~ ~~ elif name in self . _dangling : \n 
~~~ for p in self . _dangling [ name ] : \n 
~~ ~~ def _rec_get_param ( self , absname ) : \n 
~~~ parts = absname . rsplit ( , 1 ) \n 
if len ( parts ) == 1 : \n 
~~~ return self . root . params [ absname ] \n 
~~~ grp = self . root . _subsystem ( parts [ 0 ] ) \n 
return grp . params [ parts [ 1 ] ] \n 
~~ ~~ def __setitem__ ( self , name , val ) : \n 
~~~ self . root . unknowns [ name ] = val \n 
~~ elif name in self . _dangling : \n 
~~~ parts = p . rsplit ( , 1 ) \n 
~~~ self . root . params [ p ] = val \n 
grp . params [ parts [ 1 ] ] = val \n 
~~ ~~ def _setup_connections ( self , params_dict , unknowns_dict ) : \n 
to_prom_name = self . _probdata . to_prom_name \n 
connections = self . root . _get_explicit_connections ( ) \n 
prom_noconns = self . _add_implicit_connections ( connections ) \n 
input_graph = nx . DiGraph ( ) \n 
self . _dangling = { } \n 
to_abs_pnames = self . root . _sysdata . to_abs_pnames \n 
usrcs = set ( ) \n 
for tgt , srcs in iteritems ( connections ) : \n 
~~~ for src , idxs in srcs : \n 
~~~ input_graph . add_edge ( src , tgt , idxs = idxs ) \n 
if src in unknowns_dict : \n 
~~~ usrcs . add ( src ) \n 
~~ ~~ ~~ for prom , plist in iteritems ( to_abs_pnames ) : \n 
~~~ input_graph . add_nodes_from ( plist ) \n 
if prom in prom_noconns : \n 
~~~ start = plist [ 0 ] \n 
input_graph . add_edges_from ( ( ( start , p ) for p in plist [ 1 : ] ) , \n 
idxs = None ) \n 
~~ ~~ newconns = { } \n 
for src in usrcs : \n 
~~~ newconns [ src ] = None \n 
src_idxs = { src : None } \n 
for s , t in nx . dfs_edges ( input_graph , src ) : \n 
~~~ tidxs = input_graph [ s ] [ t ] [ ] \n 
sidxs = src_idxs [ s ] \n 
if tidxs is None : \n 
~~~ tidxs = sidxs \n 
~~ elif sidxs is not None : \n 
~~~ tidxs = np . array ( sidxs ) [ tidxs ] \n 
~~ src_idxs [ t ] = tidxs \n 
if t in newconns : \n 
~~~ newconns [ t ] . append ( ( src , tidxs ) ) \n 
~~~ newconns [ t ] = [ ( src , tidxs ) ] \n 
~~ ~~ ~~ self . _input_inputs = { } \n 
for node in input_graph . nodes_iter ( ) : \n 
~~~ if node not in newconns and len ( input_graph . pred [ node ] ) == 0 : \n 
~~~ nosrc = [ node ] \n 
for s , t in nx . dfs_edges ( input_graph , node ) : \n 
~~~ src = newconns [ t ] [ 0 ] [ 0 ] \n 
for n in nosrc : \n 
~~~ newconns [ n ] = [ ( src , None ) ] \n 
~~ break \n 
~~~ nosrc . append ( t ) \n 
~~~ set_nosrc = set ( nosrc ) \n 
~~~ self . _dangling [ to_prom_name [ n ] ] = set_nosrc \n 
self . _input_inputs [ n ] = nosrc \n 
~~ ~~ ~~ ~~ connections = OrderedDict ( ) \n 
for tgt , srcs in sorted ( newconns . items ( ) ) : \n 
~~~ if srcs is not None : \n 
~~~ if len ( srcs ) > 1 : \n 
~~~ src_names = ( n for n , idx in srcs ) \n 
( tgt , sorted ( src_names ) ) ) \n 
~~ connections [ tgt ] = srcs [ 0 ] \n 
~~ ~~ return connections \n 
~~ def _check_input_diffs ( self , connections , params_dict , unknowns_dict ) : \n 
for tgt , connected_inputs in iteritems ( self . _input_inputs ) : \n 
~~~ tgt_idx = connected_inputs . index ( tgt ) \n 
units = [ params_dict [ n ] . get ( ) for n in connected_inputs ] \n 
vals = [ params_dict [ n ] [ ] for n in connected_inputs ] \n 
diff_units = [ ] \n 
for i , u in enumerate ( units ) : \n 
~~~ if i != tgt_idx and u != units [ tgt_idx ] : \n 
~~~ if units [ tgt_idx ] is None : \n 
~~~ sname , s = connected_inputs [ i ] , u \n 
tname , t = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
~~~ sname , s = connected_inputs [ tgt_idx ] , units [ tgt_idx ] \n 
tname , t = connected_inputs [ i ] , u \n 
~~ diff_units . append ( ( connected_inputs [ i ] , u ) ) \n 
~~ ~~ if isinstance ( vals [ tgt_idx ] , np . ndarray ) : \n 
~~~ diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if not \n 
( isinstance ( v , np . ndarray ) and \n 
v . shape == vals [ tgt_idx ] . shape and \n 
( v == vals [ tgt_idx ] ) . all ( ) ) ] \n 
~~~ vtype = type ( vals [ tgt_idx ] ) \n 
diff_vals = [ ( connected_inputs [ i ] , v ) for i , v in \n 
enumerate ( vals ) if vtype != type ( v ) or \n 
v != vals [ tgt_idx ] ] \n 
~~ if diff_units : \n 
~~~ filt = set ( [ u for n , u in diff_units ] ) \n 
if None in filt : \n 
~~~ filt . remove ( None ) \n 
~~ if filt : \n 
~~~ proms = set ( [ params_dict [ item ] [ ] for item in connected_inputs ] ) \n 
if len ( proms ) == 1 : \n 
correct_src = params_dict [ connected_inputs [ 0 ] ] [ ] \n 
self . _setup_errors . append ( msg ) \n 
~~ ~~ if diff_vals : \n 
( sorted ( [ ( tgt , params_dict [ tgt ] [ ] ) ] + \n 
diff_vals ) ) ) \n 
~~ ~~ for promname , absnames in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ if len ( absnames ) > 1 : \n 
~~~ step_sizes , step_types , forms = { } , { } , { } \n 
for name in absnames : \n 
~~~ meta = self . root . _params_dict [ name ] \n 
ss = meta . get ( ) \n 
if ss is not None : \n 
~~~ step_sizes [ ss ] = name \n 
~~ st = meta . get ( ) \n 
if st is not None : \n 
~~~ step_types [ st ] = name \n 
~~ f = meta . get ( ) \n 
if f is not None : \n 
~~~ forms [ f ] = name \n 
~~ ~~ if len ( step_sizes ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in step_sizes . items ( ) ] ) ) ) \n 
~~ if len ( step_types ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in step_types . items ( ) ] ) ) ) \n 
~~ if len ( forms ) > 1 : \n 
sorted ( [ ( v , k ) for k , v in forms . items ( ) ] ) ) ) \n 
~~ ~~ ~~ ~~ def _get_ubc_vars ( self , connections ) : \n 
full_order = { s . pathname : i for i , s in \n 
enumerate ( self . root . subsystems ( recurse = True ) ) } \n 
ubcs = [ ] \n 
~~~ tsys = tgt . rsplit ( , 1 ) [ 0 ] \n 
ssys = srcs [ 0 ] . rsplit ( , 1 ) [ 0 ] \n 
if full_order [ ssys ] > full_order [ tsys ] : \n 
~~~ ubcs . append ( tgt ) \n 
~~ ~~ return ubcs \n 
~~ def setup ( self , check = True , out_stream = sys . stdout ) : \n 
self . _setup_errors = [ ] \n 
tree_changed = False \n 
meta_changed = False \n 
if isinstance ( self . root . ln_solver , LinearGaussSeidel ) : \n 
~~~ self . _probdata . top_lin_gs = True \n 
~~ self . driver . root = self . root \n 
self . root . _init_sys_data ( self . pathname , self . _probdata ) \n 
self . _setup_communicators ( ) \n 
params_dict , unknowns_dict = self . root . _setup_variables ( ) \n 
self . _probdata . params_dict = params_dict \n 
self . _probdata . unknowns_dict = unknowns_dict \n 
self . _probdata . to_prom_name = self . root . _sysdata . to_prom_name \n 
connections = self . _setup_connections ( params_dict , unknowns_dict ) \n 
self . _probdata . connections = connections \n 
for tgt , ( src , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ tgt ] \n 
if not in tmeta or not tmeta [ ] : \n 
~~~ if tmeta [ ] == ( ) : \n 
~~~ smeta = unknowns_dict [ src ] \n 
if idxs is not None : \n 
~~~ size = len ( idxs ) \n 
tmeta [ ] = ( size , ) \n 
tmeta [ ] = size \n 
tmeta [ ] = smeta [ ] [ np . array ( idxs ) ] \n 
~~~ tmeta [ ] = smeta [ ] \n 
tmeta [ ] = smeta [ ] \n 
~~ ~~ if idxs is not None : \n 
~~~ if isinstance ( idxs , np . ndarray ) : \n 
~~~ tmeta [ ] = idxs \n 
~~~ tmeta [ ] = np . array ( idxs , \n 
dtype = self . _impl . idx_arr_type ) \n 
~~ ~~ ~~ ~~ if MPI : \n 
~~~ for s in self . root . components ( recurse = True ) : \n 
~~~ if hasattr ( s , ) or ( \n 
hasattr ( s , ) and ( s . setup_distrib \n 
is not Component . setup_distrib ) ) : \n 
~~~ meta_changed = True \n 
~~ ~~ ~~ if tree_changed : \n 
~~~ return self . setup ( check = check , out_stream = out_stream ) \n 
~~ elif meta_changed : \n 
~~~ params_dict , unknowns_dict = self . root . _setup_variables ( compute_indices = True ) \n 
~~ self . _setup_errors . extend ( check_connections ( connections , params_dict , \n 
unknowns_dict , \n 
self . root . _sysdata . to_prom_name ) ) \n 
self . _setup_units ( connections , params_dict , unknowns_dict ) \n 
to_prom_name = self . root . _sysdata . to_prom_name \n 
self . _probdata . to_prom_name = to_prom_name \n 
for path , meta in iteritems ( params_dict ) : \n 
~~~ meta [ ] = to_prom_name [ path ] \n 
if path not in connections : \n 
~~~ if not in meta or not meta [ ] : \n 
~~~ if meta [ ] == ( ) : \n 
~~ ~~ ~~ ~~ for path , meta in iteritems ( unknowns_dict ) : \n 
~~ param_owners = _assign_parameters ( connections ) \n 
pois = self . driver . desvars_of_interest ( ) \n 
oois = self . driver . outputs_of_interest ( ) \n 
self . _driver_vois = set ( ) \n 
for tup in chain ( pois , oois ) : \n 
~~~ self . _driver_vois . update ( tup ) \n 
~~ promoted_unknowns = self . root . _sysdata . to_abs_uname \n 
parallel_p = False \n 
for vnames in pois : \n 
~~~ if len ( vnames ) > 1 : \n 
~~~ parallel_p = True \n 
~~ for v in vnames : \n 
~~~ if v not in promoted_unknowns : \n 
~~ ~~ ~~ parallel_u = False \n 
for vnames in oois : \n 
~~~ parallel_u = True \n 
~~ ~~ ~~ mode = self . _check_for_parallel_derivs ( pois , oois , parallel_u , parallel_p ) \n 
self . _probdata . relevance = Relevance ( self . root , params_dict , \n 
unknowns_dict , connections , \n 
pois , oois , mode ) \n 
for s in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if not s . _order_set : \n 
~~~ order = None \n 
broken_edges = None \n 
if self . comm . rank == 0 : \n 
~~~ order , broken_edges = s . list_auto_order ( ) \n 
~~ if MPI : \n 
~~~ if trace : \n 
~~ order , broken_edges = self . comm . bcast ( ( order , broken_edges ) , root = 0 ) \n 
if trace : \n 
~~ ~~ s . set_order ( order ) \n 
for edge in broken_edges : \n 
~~~ cname = edge [ 1 ] \n 
head_sys = self . root \n 
for name in cname . split ( ) : \n 
~~~ head_sys = getattr ( head_sys , name ) \n 
~~ head_sys . _run_apply = True \n 
~~ ~~ ~~ self . _check_input_diffs ( connections , params_dict , unknowns_dict ) \n 
alloc_derivs = not self . root . fd_options [ ] \n 
for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ alloc_derivs = alloc_derivs or sub . nl_solver . supports [ ] \n 
~~ self . root . _setup_vectors ( param_owners , impl = self . _impl , alloc_derivs = alloc_derivs ) \n 
self . driver . _setup ( ) \n 
self . _poi_indices , self . _qoi_indices = self . driver . _map_voi_indices ( ) \n 
~~~ sub . nl_solver . setup ( sub ) \n 
sub . ln_solver . setup ( sub ) \n 
~~ self . _check_solvers ( ) \n 
self . _start_recorders ( ) \n 
if self . _setup_errors : \n 
~~~ stream = cStringIO ( ) \n 
for err in self . _setup_errors : \n 
~~~ stream . write ( "%s\\n" % err ) \n 
~~ raise RuntimeError ( stream . getvalue ( ) ) \n 
~~ OptionsDictionary . locked = True \n 
if check or force_check : \n 
~~~ return self . check_setup ( out_stream ) \n 
~~ return { } \n 
~~ def cleanup ( self ) : \n 
self . driver . cleanup ( ) \n 
self . root . cleanup ( ) \n 
~~ def _check_solvers ( self ) : \n 
iterated_states = set ( ) \n 
group_states = [ ] \n 
has_iter_solver = { } \n 
for group in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ has_iter_solver [ group . pathname ] = ( group . ln_solver . options [ ] > 1 ) \n 
~~~ if isinstance ( group . ln_solver , DirectSolver ) : \n 
~~~ has_iter_solver [ group . pathname ] = ( True ) \n 
~~ ~~ opt = group . fd_options \n 
if opt [ ] == True and opt [ ] == : \n 
~~~ if group . name != : \n 
~~ for sub in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if hasattr ( sub . nl_solver , ) : \n 
self . _setup_errors . append ( msg . format ( sub . name ) ) \n 
~~ ~~ ~~ parts = group . pathname . split ( ) \n 
for i in range ( len ( parts ) ) : \n 
~~~ if has_iter_solver [ . join ( parts [ : i ] ) ] : \n 
~~~ is_iterated_somewhere = True \n 
~~~ is_iterated_somewhere = False \n 
~~ if is_iterated_somewhere : \n 
~~ if isinstance ( group . ln_solver , LinearGaussSeidel ) and group . ln_solver . options [ ] == 1 : \n 
~~~ graph = group . _get_sys_graph ( ) \n 
strong = [ sorted ( s ) for s in nx . strongly_connected_components ( graph ) \n 
if len ( s ) > 1 ] \n 
if strong : \n 
"recommended)." \n 
% ( group . pathname , strong ) ) \n 
~~ ~~ states = [ n for n , m in iteritems ( group . _unknowns_dict ) if m . get ( ) ] \n 
if states : \n 
~~~ group_states . append ( ( group , states ) ) \n 
if isinstance ( group . ln_solver , DirectSolver ) or group . ln_solver . options [ ] > 1 : \n 
~~~ iterated_states . update ( states ) \n 
~~~ for s in states : \n 
~~~ if s not in iterated_states : \n 
~~~ cname = s . rsplit ( , 1 ) [ 0 ] \n 
comp = self . root \n 
~~~ comp = getattr ( comp , name ) \n 
~~ if not _needs_iteration ( comp ) : \n 
~~~ iterated_states . add ( s ) \n 
~~ ~~ ~~ ~~ ~~ ~~ for group , states in group_states : \n 
~~~ uniterated_states = [ s for s in states if s not in iterated_states ] \n 
if uniterated_states : \n 
( group . pathname , uniterated_states ) ) \n 
~~ ~~ ~~ def _check_dangling_params ( self , out_stream = sys . stdout ) : \n 
dangling_params = sorted ( set ( [ \n 
to_prom_name [ p ] for p , m in iteritems ( self . root . _params_dict ) \n 
if p not in self . root . connections \n 
] ) ) \n 
if dangling_params : \n 
file = out_stream ) \n 
for d in dangling_params : \n 
~~~ print ( d , file = out_stream ) \n 
~~ ~~ return dangling_params \n 
~~ def _check_mode ( self , out_stream = sys . stdout ) : \n 
if self . _calculated_mode != self . root . _probdata . relevance . mode : \n 
self . _calculated_mode , \n 
self . _p_length , \n 
self . _u_length ) , \n 
~~ return ( self . root . _probdata . relevance . mode , self . _calculated_mode ) \n 
~~ def _check_no_unknown_comps ( self , out_stream = sys . stdout ) : \n 
nocomps = sorted ( [ c . pathname for c in self . root . components ( recurse = True , \n 
local = True ) \n 
if len ( c . unknowns ) == 0 ] ) \n 
if nocomps : \n 
for n in nocomps : \n 
~~~ print ( n , file = out_stream ) \n 
~~ ~~ return nocomps \n 
~~ def _check_no_recorders ( self , out_stream = sys . stdout ) : \n 
recorders = [ ] \n 
recorders . extend ( self . driver . recorders ) \n 
for grp in self . root . subgroups ( recurse = True , local = True , \n 
include_self = True ) : \n 
~~~ recorders . extend ( grp . nl_solver . recorders ) \n 
recorders . extend ( grp . ln_solver . recorders ) \n 
~~ if not recorders : \n 
~~ return recorders \n 
~~ def _check_no_connect_comps ( self , out_stream = sys . stdout ) : \n 
conn_comps = set ( [ t . rsplit ( , 1 ) [ 0 ] \n 
for t in self . root . connections ] ) \n 
conn_comps . update ( [ s . rsplit ( , 1 ) [ 0 ] \n 
for s , i in itervalues ( self . root . connections ) ] ) \n 
noconn_comps = sorted ( [ c . pathname \n 
for c in self . root . components ( recurse = True , local = True ) \n 
if c . pathname not in conn_comps ] ) \n 
if noconn_comps : \n 
for comp in noconn_comps : \n 
~~~ print ( comp , file = out_stream ) \n 
~~ ~~ return noconn_comps \n 
~~ def _check_mpi ( self , out_stream = sys . stdout ) : \n 
if under_mpirun ( ) : \n 
~~~ parr = True \n 
~~~ for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if ( isinstance ( grp , ParallelGroup ) or \n 
isinstance ( grp , ParallelFDGroup ) ) : \n 
~~~ parr = False \n 
~~ mincpu , maxcpu = self . root . get_req_procs ( ) \n 
if maxcpu is not None and self . comm . size > maxcpu : \n 
( self . comm . size , maxcpu ) ) \n 
~~ return ( self . comm . size , maxcpu , parr ) \n 
~~~ pargrps = [ ] \n 
for grp in self . root . subgroups ( recurse = True , include_self = True ) : \n 
~~~ if isinstance ( grp , ParallelGroup ) : \n 
grp . pathname , file = out_stream ) \n 
pargrps . append ( grp . pathname ) \n 
~~ ~~ return sorted ( pargrps ) \n 
~~ ~~ def _check_graph ( self , out_stream = sys . stdout ) : \n 
cycles = [ ] \n 
ooo = [ ] \n 
~~~ graph = grp . _get_sys_graph ( ) \n 
strong = [ s for s in nx . strongly_connected_components ( graph ) \n 
~~~ relstrong = [ ] \n 
for slist in strong : \n 
~~~ relstrong . append ( [ ] ) \n 
for s in slist : \n 
~~~ relstrong [ - 1 ] . append ( nearest_child ( grp . pathname , s ) ) \n 
subs = [ s for s in grp . _subsystems ] \n 
tups = sorted ( [ ( subs . index ( s ) , s ) for s in relstrong [ - 1 ] ] ) \n 
relstrong [ - 1 ] = [ t [ 1 ] for t in tups ] \n 
( grp . pathname , relstrong ) , file = out_stream ) \n 
cycles . append ( relstrong ) \n 
~~ graph , _ = grp . _break_cycles ( grp . list_order ( ) , graph ) \n 
visited = set ( ) \n 
out_of_order = { } \n 
for sub in itervalues ( grp . _subsystems ) : \n 
~~~ visited . add ( sub . pathname ) \n 
for u , v in nx . dfs_edges ( graph , sub . pathname ) : \n 
~~~ if v in visited : \n 
~~~ out_of_order . setdefault ( nearest_child ( grp . pathname , v ) , \n 
set ( ) ) . add ( sub . pathname ) \n 
~~ ~~ ~~ if out_of_order : \n 
~~~ for name in out_of_order : \n 
~~~ out_of_order [ name ] = sorted ( [ \n 
nearest_child ( grp . pathname , n ) for n in out_of_order [ name ] \n 
] ) \n 
for n , subs in iteritems ( out_of_order ) : \n 
~~ ooo . append ( ( grp . pathname , list ( iteritems ( out_of_order ) ) ) ) \n 
~~ ~~ return ( cycles , sorted ( ooo ) ) \n 
~~ def _check_gmres_under_mpi ( self , out_stream = sys . stdout ) : \n 
~~~ has_parallel = False \n 
~~~ if isinstance ( s , ParallelGroup ) : \n 
~~~ has_parallel = True \n 
~~ ~~ if has_parallel and isinstance ( self . root . ln_solver , ScipyGMRES ) : \n 
~~ ~~ ~~ def _check_ubcs ( self , out_stream = sys . stdout ) : \n 
~~~ ubcs = self . _get_ubc_vars ( self . root . connections ) \n 
if ubcs : \n 
~~ return ubcs \n 
~~ def _check_unmarked_pbos ( self , out_stream = sys . stdout ) : \n 
~~~ pbos = [ ] \n 
for comp in self . root . components ( recurse = True , include_self = True ) : \n 
~~~ if comp . _pbo_warns : \n 
~~~ pbos . append ( ( comp . pathname , comp . _pbo_warns ) ) \n 
~~ ~~ if pbos : \n 
for cname , pbo_warns in sorted ( pbos , key = lambda x : x [ 0 ] ) : \n 
~~~ for vname , val in pbo_warns : \n 
type ( val ) . __name__ ) , file = out_stream ) \n 
~~ ~~ ~~ return pbos \n 
~~ def _check_relevant_pbos ( self , out_stream = sys . stdout ) : \n 
if self . driver . __class__ is Driver or self . driver . supports [ ] is False or self . root . fd_options [ ] : \n 
~~~ return [ ] \n 
~~ vec = self . root . unknowns \n 
pbos = [ var for var in vec if vec . metadata ( var ) . get ( ) ] \n 
rels = set ( ) \n 
for key , rel in iteritems ( self . _probdata . relevance . relevant ) : \n 
~~~ rels . update ( rel ) \n 
~~ rel_pbos = rels . intersection ( pbos ) \n 
if rel_pbos : \n 
~~~ rel_conns = [ ] \n 
for src in rel_pbos : \n 
~~~ for tgt , src_tuple in iteritems ( self . root . connections ) : \n 
~~~ if src_tuple [ 0 ] == src and tgt in rels : \n 
~~~ rel_conns . append ( ( src , tgt ) ) \n 
~~ ~~ ~~ if rel_conns : \n 
for src , tgt in rel_conns : \n 
~~~ val = vec [ src ] \n 
~~ return list ( rel_pbos ) \n 
~~ def check_setup ( self , out_stream = sys . stdout ) : \n 
print ( "##############################################" , file = out_stream ) \n 
results [ ] = self . _check_no_recorders ( out_stream ) \n 
results [ ] = self . _check_mpi ( out_stream ) \n 
results [ ] = self . _check_dangling_params ( out_stream ) \n 
results [ ] = self . _check_mode ( out_stream ) \n 
results [ ] = self . _check_no_unknown_comps ( out_stream ) \n 
results [ ] = self . _check_no_connect_comps ( out_stream ) \n 
results [ ] , results [ ] = self . _check_graph ( out_stream ) \n 
results [ ] = self . _check_ubcs ( out_stream ) \n 
results [ ] = self . _check_gmres_under_mpi ( out_stream ) \n 
results [ ] = self . _check_unmarked_pbos ( out_stream ) \n 
results [ ] = self . _check_relevant_pbos ( out_stream ) \n 
for s in self . root . subsystems ( recurse = True , local = True , include_self = True ) : \n 
s . check_setup ( out_stream = stream ) \n 
content = stream . getvalue ( ) \n 
if content : \n 
~~~ print ( "%s:\\n%s\\n" % ( s . pathname , content ) , file = out_stream ) \n 
results [ "@%s" % s . pathname ] = content \n 
print ( "##############################################\\n" , file = out_stream ) \n 
~~ def pre_run_check ( self ) : \n 
if not self . root . fd_options . locked : \n 
raise RuntimeError ( msg ) \n 
~~ ~~ def run ( self ) : \n 
self . pre_run_check ( ) \n 
if self . root . is_active ( ) : \n 
~~~ self . driver . run ( self ) \n 
self . root . comm . barrier ( ) \n 
~~ ~~ ~~ def run_once ( self ) : \n 
root = self . root \n 
driver = self . driver \n 
if root . is_active ( ) : \n 
~~~ driver . run_once ( self ) \n 
with root . _dircontext : \n 
~~~ root . apply_nonlinear ( root . params , root . unknowns , root . resids , \n 
metadata = driver . metadata ) \n 
root . comm . barrier ( ) \n 
~~ ~~ ~~ def _mode ( self , mode , indep_list , unknown_list ) : \n 
self . _p_length = 0 \n 
self . _u_length = 0 \n 
uset = set ( ) \n 
for unames in unknown_list : \n 
~~~ if isinstance ( unames , tuple ) : \n 
~~~ uset . update ( unames ) \n 
~~~ uset . add ( unames ) \n 
~~ ~~ pset = set ( ) \n 
for pnames in indep_list : \n 
~~~ if isinstance ( pnames , tuple ) : \n 
~~~ pset . update ( pnames ) \n 
~~~ pset . add ( pnames ) \n 
~~ ~~ to_prom_name = self . root . _sysdata . to_prom_name \n 
for path , meta in chain ( iteritems ( self . root . _unknowns_dict ) , \n 
iteritems ( self . root . _params_dict ) ) : \n 
~~~ prom_name = to_prom_name [ path ] \n 
if prom_name in uset : \n 
~~~ self . _u_length += meta [ ] \n 
uset . remove ( prom_name ) \n 
~~ if prom_name in pset : \n 
~~~ self . _p_length += meta [ ] \n 
pset . remove ( prom_name ) \n 
~~ ~~ if uset : \n 
~~ if pset : \n 
~~ if self . _p_length > self . _u_length : \n 
~~~ self . _calculated_mode = \n 
~~ if mode == : \n 
~~~ mode = self . root . ln_solver . options [ ] \n 
if mode == : \n 
~~~ mode = self . _calculated_mode \n 
~~ ~~ return mode \n 
~~ def calc_gradient ( self , indep_list , unknown_list , mode = , \n 
return_format = , dv_scale = None , cn_scale = None , \n 
sparsity = None ) : \n 
if mode not in [ , , , ] : \n 
~~ if return_format not in [ , ] : \n 
~~ with self . root . _dircontext : \n 
~~~ if mode == or self . root . fd_options [ ] : \n 
~~~ return self . _calc_gradient_fd ( indep_list , unknown_list , \n 
return_format , dv_scale = dv_scale , \n 
cn_scale = cn_scale , sparsity = sparsity ) \n 
~~~ return self . _calc_gradient_ln_solver ( indep_list , unknown_list , \n 
return_format , mode , \n 
dv_scale = dv_scale , \n 
cn_scale = cn_scale , \n 
sparsity = sparsity ) \n 
~~ ~~ ~~ def _calc_gradient_fd ( self , indep_list , unknown_list , return_format , \n 
dv_scale = None , cn_scale = None , sparsity = None ) : \n 
unknowns = root . unknowns \n 
params = root . params \n 
to_prom_name = root . _sysdata . to_prom_name \n 
to_abs_pnames = root . _sysdata . to_abs_pnames \n 
to_abs_uname = root . _sysdata . to_abs_uname \n 
if dv_scale is None : \n 
~~~ dv_scale = { } \n 
~~ if cn_scale is None : \n 
~~~ cn_scale = { } \n 
~~ abs_params = [ ] \n 
fd_unknowns = [ var for var in unknown_list if var not in indep_list ] \n 
pass_unknowns = [ var for var in unknown_list if var in indep_list ] \n 
for name in indep_list : \n 
~~~ if name in unknowns : \n 
~~~ name = to_abs_uname [ name ] \n 
~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if name == src : \n 
~~~ name = tgt \n 
~~ ~~ abs_params . append ( name ) \n 
~~ Jfd = root . fd_jacobian ( params , unknowns , root . resids , total_derivs = True , \n 
fd_params = abs_params , fd_unknowns = fd_unknowns , \n 
pass_unknowns = pass_unknowns , \n 
poi_indices = self . _poi_indices , \n 
qoi_indices = self . _qoi_indices ) \n 
def get_fd_ikey ( ikey ) : \n 
~~~ if isinstance ( ikey , tuple ) : \n 
~~~ ikey = ikey [ 0 ] \n 
~~ fd_ikey = ikey \n 
if fd_ikey not in params : \n 
~~~ for tgt , ( src , idxs ) in iteritems ( root . connections ) : \n 
~~~ if src == ikey : \n 
~~~ fd_ikey = tgt \n 
~~ ~~ if fd_ikey not in params : \n 
~~~ for key , meta in iteritems ( params ) : \n 
~~~ if to_prom_name [ key ] == fd_ikey : \n 
~~~ fd_ikey = meta [ ] \n 
~~ ~~ ~~ ~~ return fd_ikey \n 
~~ if return_format == : \n 
~~~ J = OrderedDict ( ) \n 
for okey in unknown_list : \n 
~~~ J [ okey ] = OrderedDict ( ) \n 
for j , ikey in enumerate ( indep_list ) : \n 
~~~ if sparsity is not None : \n 
~~~ if ikey not in sparsity [ okey ] : \n 
~~ ~~ abs_ikey = abs_params [ j ] \n 
fd_ikey = get_fd_ikey ( abs_ikey ) \n 
if ( okey , fd_ikey ) not in Jfd : \n 
~~~ fd_ikey = to_abs_pnames [ fd_ikey ] [ 0 ] \n 
~~ J [ okey ] [ ikey ] = Jfd [ ( okey , fd_ikey ) ] \n 
if ikey in dv_scale : \n 
~~~ J [ okey ] [ ikey ] *= dv_scale [ ikey ] \n 
~~ if okey in cn_scale : \n 
~~~ J [ okey ] [ ikey ] *= cn_scale [ okey ] \n 
~~ ~~ ~~ ~~ else : \n 
~~~ usize = 0 \n 
psize = 0 \n 
for u in unknown_list : \n 
~~~ if u in self . _qoi_indices : \n 
~~~ idx = self . _qoi_indices [ u ] \n 
usize += len ( idx ) \n 
~~~ usize += self . root . unknowns . metadata ( u ) [ ] \n 
~~ ~~ for p in indep_list : \n 
~~~ if p in self . _poi_indices : \n 
~~~ idx = self . _poi_indices [ p ] \n 
psize += len ( idx ) \n 
~~~ psize += self . root . unknowns . metadata ( p ) [ ] \n 
~~ ~~ J = np . zeros ( ( usize , psize ) ) \n 
ui = 0 \n 
~~~ pi = 0 \n 
for j , p in enumerate ( indep_list ) : \n 
~~~ abs_ikey = abs_params [ j ] \n 
if ( u , fd_ikey ) not in Jfd : \n 
~~ pd = Jfd [ u , fd_ikey ] \n 
rows , cols = pd . shape \n 
for row in range ( 0 , rows ) : \n 
~~~ for col in range ( 0 , cols ) : \n 
~~~ J [ ui + row ] [ pi + col ] = pd [ row ] [ col ] \n 
if p in dv_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= dv_scale [ p ] \n 
~~ if u in cn_scale : \n 
~~~ J [ ui + row ] [ pi + col ] *= cn_scale [ u ] \n 
~~ ~~ ~~ pi += cols \n 
~~ ui += rows \n 
~~ ~~ return J \n 
~~ def _calc_gradient_ln_solver ( self , indep_list , unknown_list , return_format , mode , \n 
relevance = root . _probdata . relevance \n 
unknowns_dict = root . _unknowns_dict \n 
comm = root . comm \n 
iproc = comm . rank \n 
nproc = comm . size \n 
owned = root . _owning_ranks \n 
~~ mode = self . _mode ( mode , indep_list , unknown_list ) \n 
fwd = mode == \n 
root . clear_dparams ( ) \n 
for names in root . _probdata . relevance . vars_of_interest ( mode ) : \n 
~~~ for name in names : \n 
~~~ if name in root . dumat : \n 
~~~ root . dumat [ name ] . vec [ : ] = 0.0 \n 
root . drmat [ name ] . vec [ : ] = 0.0 \n 
~~ ~~ ~~ root . dumat [ None ] . vec [ : ] = 0.0 \n 
root . drmat [ None ] . vec [ : ] = 0.0 \n 
root . _sys_linearize ( root . params , unknowns , root . resids ) \n 
if return_format == : \n 
for okeys in unknown_list : \n 
~~~ if isinstance ( okeys , str ) : \n 
~~~ okeys = ( okeys , ) \n 
~~ for okey in okeys : \n 
for ikeys in indep_list : \n 
~~~ if isinstance ( ikeys , str ) : \n 
~~~ ikeys = ( ikeys , ) \n 
~~ for ikey in ikeys : \n 
~~ ~~ J [ okey ] [ ikey ] = None \n 
~~ ~~ ~~ ~~ ~~ else : \n 
Jslices = OrderedDict ( ) \n 
~~~ start = usize \n 
if u in self . _qoi_indices : \n 
~~ Jslices [ u ] = slice ( start , usize ) \n 
~~ for p in indep_list : \n 
~~~ start = psize \n 
if p in self . _poi_indices : \n 
~~~ psize += unknowns . metadata ( p ) [ ] \n 
~~ Jslices [ p ] = slice ( start , psize ) \n 
~~ J = np . zeros ( ( usize , psize ) ) \n 
~~ if fwd : \n 
~~~ input_list , output_list = indep_list , unknown_list \n 
poi_indices , qoi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = dv_scale , cn_scale \n 
~~~ input_list , output_list = unknown_list , indep_list \n 
qoi_indices , poi_indices = self . _poi_indices , self . _qoi_indices \n 
in_scale , un_scale = cn_scale , dv_scale \n 
~~ all_vois = self . root . _probdata . relevance . vars_of_interest ( mode ) \n 
input_set = set ( ) \n 
for inp in input_list : \n 
~~~ if isinstance ( inp , str ) : \n 
~~~ input_set . add ( inp ) \n 
~~~ input_set . update ( inp ) \n 
~~ ~~ voi_sets = [ ] \n 
for voi_set in all_vois : \n 
~~~ for voi in voi_set : \n 
~~~ if voi in input_set : \n 
~~~ voi_sets . append ( voi_set ) \n 
~~ ~~ ~~ flat_voi = [ item for sublist in all_vois for item in sublist ] \n 
for items in input_list : \n 
~~~ if isinstance ( items , str ) : \n 
~~~ items = ( items , ) \n 
~~ for item in items : \n 
~~~ if item not in flat_voi : \n 
~~~ voi_sets . append ( ( item , ) ) \n 
~~ ~~ ~~ voi_srcs = { } \n 
for params in voi_sets : \n 
~~~ rhs = OrderedDict ( ) \n 
voi_idxs = { } \n 
old_size = None \n 
for voi in params : \n 
~~~ vkey = self . _get_voi_key ( voi , params ) \n 
duvec = self . root . dumat [ vkey ] \n 
rhs [ vkey ] = np . empty ( ( len ( duvec . vec ) , ) ) \n 
voi_srcs [ vkey ] = voi \n 
if voi in duvec : \n 
~~~ in_idxs = duvec . _get_local_idxs ( voi , poi_indices ) \n 
~~~ in_idxs = [ ] \n 
~~ if len ( in_idxs ) == 0 : \n 
~~~ if voi in poi_indices : \n 
~~~ in_idxs = duvec . to_idx_array ( poi_indices [ voi ] ) \n 
~~~ in_idxs = np . arange ( 0 , unknowns_dict [ to_abs_uname [ voi ] ] [ ] , dtype = int ) \n 
~~ ~~ if old_size is None : \n 
~~~ old_size = len ( in_idxs ) \n 
~~ elif old_size != len ( in_idxs ) : \n 
~~ voi_idxs [ vkey ] = in_idxs \n 
~~ for i in range ( len ( in_idxs ) ) : \n 
~~~ for voi in params : \n 
rhs [ vkey ] [ : ] = 0.0 \n 
if self . root . _owning_ranks [ voi_srcs [ vkey ] ] == iproc : \n 
~~~ rhs [ vkey ] [ voi_idxs [ vkey ] [ i ] ] = - 1.0 \n 
~~ ~~ dx_mat = root . ln_solver . solve ( rhs , root , mode ) \n 
for param , dx in iteritems ( dx_mat ) : \n 
~~~ vkey = self . _get_voi_key ( param , params ) \n 
if param is None : \n 
~~~ param = params [ 0 ] \n 
~~ for item in output_list : \n 
~~~ if fwd and param not in sparsity [ item ] : \n 
~~ elif not fwd and item not in sparsity [ param ] : \n 
~~ ~~ if relevance . is_relevant ( vkey , item ) : \n 
~~~ if fwd or owned [ item ] == iproc : \n 
~~~ out_idxs = self . root . dumat [ vkey ] . _get_local_idxs ( item , \n 
qoi_indices , \n 
get_slice = True ) \n 
dxval = dx [ out_idxs ] \n 
if dxval . size == 0 : \n 
~~~ dxval = None \n 
~~ if nproc > 1 : \n 
( dxval , owned [ item ] , param , item ) ) \n 
~~ dxval = comm . bcast ( dxval , root = owned [ item ] ) \n 
~~~ if item in qoi_indices : \n 
~~~ zsize = len ( qoi_indices [ item ] ) \n 
~~~ zsize = unknowns . metadata ( item ) [ ] \n 
~~ dxval = np . zeros ( zsize ) \n 
~~ if dxval is not None : \n 
~~~ nk = len ( dxval ) \n 
~~~ if fwd : \n 
~~~ if J [ item ] [ param ] is None : \n 
~~~ J [ item ] [ param ] = np . zeros ( ( nk , len ( in_idxs ) ) ) \n 
~~ J [ item ] [ param ] [ : , i ] = dxval \n 
if param in in_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= in_scale [ param ] \n 
~~ if item in un_scale : \n 
~~~ J [ item ] [ param ] [ : , i ] *= un_scale [ item ] \n 
~~~ if J [ param ] [ item ] is None : \n 
~~~ J [ param ] [ item ] = np . zeros ( ( len ( in_idxs ) , nk ) ) \n 
~~ J [ param ] [ item ] [ i , : ] = dxval \n 
~~~ J [ param ] [ item ] [ i , : ] *= in_scale [ param ] \n 
~~~ J [ param ] [ item ] [ i , : ] *= un_scale [ item ] \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] = dxval \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= in_scale [ param ] \n 
~~~ J [ Jslices [ item ] , Jslices [ param ] . start + i ] *= un_scale [ item ] \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] = dxval \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= in_scale [ param ] \n 
~~~ J [ Jslices [ param ] . start + i , Jslices [ item ] ] *= un_scale [ item ] \n 
~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ root . clear_dparams ( ) \n 
return J \n 
~~ def _get_voi_key ( self , voi , grp ) : \n 
if ( voi in self . _driver_vois and \n 
isinstance ( self . root . ln_solver , LinearGaussSeidel ) ) : \n 
~~~ if ( len ( grp ) > 1 or \n 
self . root . ln_solver . options [ ] ) : \n 
~~~ return voi \n 
~~ ~~ return None \n 
~~ def check_partial_derivatives ( self , out_stream = sys . stdout , comps = None , \n 
compact_print = False ) : \n 
if self . driver . iter_count < 1 : \n 
~~~ out_stream . write ( ) \n 
self . run_once ( ) \n 
~~ root . _sys_linearize ( root . params , root . unknowns , root . resids ) \n 
if out_stream is not None : \n 
~~ data = { } \n 
voi = None \n 
allcomps = root . components ( recurse = True ) \n 
if comps is None : \n 
~~~ comps = allcomps \n 
~~~ allcompnames = set ( [ c . pathname for c in allcomps ] ) \n 
requested = set ( comps ) \n 
diff = requested . difference ( allcompnames ) \n 
if diff : \n 
~~~ sorted_diff = list ( diff ) \n 
sorted_diff . sort ( ) \n 
msg += str ( sorted_diff ) \n 
~~ comps = [ root . _subsystem ( c_name ) for c_name in comps ] \n 
~~ for comp in comps : \n 
~~~ cname = comp . pathname \n 
opt = comp . fd_options \n 
fwd_rev = True \n 
if opt [ ] : \n 
~~~ f_d_2 = True \n 
fd_desc = opt [ ] \n 
fd_desc2 = opt [ ] \n 
~~~ f_d_2 = False \n 
fd_desc = None \n 
fd_desc2 = None \n 
~~ if opt [ ] : \n 
~~~ if not f_d_2 : \n 
~~ fwd_rev = False \n 
~~ if isinstance ( comp , IndepVarComp ) : \n 
~~ data [ cname ] = { } \n 
jac_fwd = OrderedDict ( ) \n 
jac_rev = OrderedDict ( ) \n 
jac_fd = OrderedDict ( ) \n 
jac_fd2 = OrderedDict ( ) \n 
params = comp . params \n 
unknowns = comp . unknowns \n 
resids = comp . resids \n 
dparams = comp . dpmat [ voi ] \n 
dunknowns = comp . dumat [ voi ] \n 
dresids = comp . drmat [ voi ] \n 
states = comp . states \n 
if len ( dparams ) == 0 : \n 
~~ param_list = [ item for item in dparams if not dparams . metadata ( item ) . get ( ) ] \n 
param_list . extend ( states ) \n 
unkn_list = [ item for item in dunknowns if not dunknowns . metadata ( item ) . get ( ) ] \n 
~~~ out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
out_stream . write ( * ( len ( cname ) + 15 ) + ) \n 
~~ for p_name in param_list : \n 
~~~ if not fwd_rev : \n 
~~ dinputs = dunknowns if p_name in states else dparams \n 
p_size = np . size ( dinputs [ p_name ] ) \n 
for u_name in unkn_list : \n 
~~~ u_size = np . size ( dunknowns [ u_name ] ) \n 
if comp . _jacobian_cache : \n 
~~~ if ( u_name , p_name ) in comp . _jacobian_cache : \n 
~~~ user = comp . _jacobian_cache [ ( u_name , p_name ) ] . shape \n 
if len ( user ) < 2 : \n 
~~~ user = ( user [ 0 ] , 1 ) \n 
~~ if user [ 0 ] != u_size or user [ 1 ] != p_size : \n 
msg = msg . format ( cname , u_name , p_name , ( u_size , p_size ) , user ) \n 
~~ ~~ ~~ jac_fwd [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
jac_rev [ ( u_name , p_name ) ] = np . zeros ( ( u_size , p_size ) ) \n 
~~ ~~ if fwd_rev : \n 
~~~ for u_name in unkn_list : \n 
for idx in range ( u_size ) : \n 
~~~ dresids . vec [ : ] = 0.0 \n 
dunknowns . vec [ : ] = 0.0 \n 
dresids . _dat [ u_name ] . val [ idx ] = 1.0 \n 
~~~ comp . apply_linear ( params , unknowns , dparams , \n 
dunknowns , dresids , ) \n 
~~~ dparams . _apply_unit_derivatives ( ) \n 
~~~ dinputs = dunknowns if p_name in states else dparams \n 
jac_rev [ ( u_name , p_name ) ] [ idx , : ] = dinputs . _dat [ p_name ] . val \n 
~~ ~~ ~~ ~~ if fwd_rev : \n 
~~~ for p_name in param_list : \n 
for idx in range ( p_size ) : \n 
dinputs . _dat [ p_name ] . val [ idx ] = 1.0 \n 
dparams . _apply_unit_derivatives ( ) \n 
comp . apply_linear ( params , unknowns , dparams , \n 
for u_name , u_val in dresids . vec_val_iter ( ) : \n 
~~~ jac_fwd [ ( u_name , p_name ) ] [ : , idx ] = u_val \n 
~~ ~~ ~~ ~~ dresids . vec [ : ] = 0.0 \n 
if opt [ ] == : \n 
~~~ fd_func = comp . complex_step_jacobian \n 
~~~ fd_func = comp . fd_jacobian \n 
~~ jac_fd = fd_func ( params , unknowns , resids ) \n 
if f_d_2 : \n 
~~ save_form = opt [ ] \n 
OptionsDictionary . locked = False \n 
opt [ ] = opt [ ] \n 
jac_fd2 = fd_func ( params , unknowns , resids ) \n 
opt [ ] = save_form \n 
OptionsDictionary . locked = True \n 
~~ _assemble_deriv_data ( chain ( dparams , states ) , resids , data [ cname ] , \n 
jac_fwd , jac_rev , jac_fd , out_stream , \n 
c_name = cname , jac_fd2 = jac_fd2 , fd_desc = fd_desc , \n 
fd_desc2 = fd_desc2 , compact_print = compact_print ) \n 
~~ return data \n 
~~ def check_total_derivatives ( self , out_stream = sys . stdout ) : \n 
if driver . iter_count < 1 : \n 
~~ if out_stream is not None : \n 
~~ if len ( driver . _desvars ) > 0 : \n 
~~~ param_srcs = list ( driver . _desvars . keys ( ) ) \n 
to_abs_name = root . _sysdata . to_abs_uname \n 
indep_list = [ p for p in param_srcs if not root . _unknowns_dict [ to_abs_name [ p ] ] . get ( ) ] \n 
~~~ abs_indep_list = root . _get_fd_params ( ) \n 
param_srcs = [ root . connections [ p ] for p in abs_indep_list if not root . _params_dict [ p ] . get ( ) ] \n 
indep_list = [ \n 
to_prom_name [ p ] for p , idxs in param_srcs \n 
~~ if len ( driver . _objs ) > 0 or len ( driver . _cons ) > 0 : \n 
~~~ unknown_list = list ( driver . _objs . keys ( ) ) \n 
unknown_list . extend ( list ( driver . _cons . keys ( ) ) ) \n 
unknown_list = [ item for item in unknown_list if not root . unknowns . metadata ( item ) . get ( ) ] \n 
~~~ unknown_list = root . _get_fd_unknowns ( ) \n 
~~ if root . ln_solver . options . get ( ) : \n 
~~~ mode = self . _mode ( , indep_list , unknown_list ) \n 
~~~ fwd , rev = True , False \n 
Jrev = None \n 
out_stream . write ( ) \n 
~~~ fwd , rev = False , True \n 
Jfor = None \n 
~~~ fwd = rev = True \n 
~~~ Jfor = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
return_format = ) \n 
Jfor = _jac_to_flat_dict ( Jfor ) \n 
~~ if rev : \n 
~~~ Jrev = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
Jrev = _jac_to_flat_dict ( Jrev ) \n 
~~ Jfd = self . calc_gradient ( indep_list , unknown_list , mode = , \n 
Jfd = _jac_to_flat_dict ( Jfd ) \n 
data = { } \n 
_assemble_deriv_data ( indep_list , unknown_list , data , \n 
Jfor , Jrev , Jfd , out_stream ) \n 
return data \n 
~~ def _start_recorders ( self ) : \n 
self . driver . recorders . startup ( self . root ) \n 
self . driver . recorders . record_metadata ( self . root ) \n 
~~~ for solver in ( group . nl_solver , group . ln_solver ) : \n 
~~~ solver . recorders . startup ( group ) \n 
solver . recorders . record_metadata ( self . root ) \n 
~~ ~~ ~~ def _check_for_parallel_derivs ( self , params , unknowns , par_u , par_p ) : \n 
mode = self . _mode ( , params , unknowns ) \n 
~~~ has_parallel_derivs = par_p \n 
~~~ has_parallel_derivs = par_u \n 
~~ if ( isinstance ( self . root . ln_solver , LinearGaussSeidel ) and \n 
self . root . ln_solver . options [ ] ) and has_parallel_derivs : \n 
~~~ for sub in self . root . subgroups ( recurse = True ) : \n 
~~~ sub_mode = sub . ln_solver . options [ ] \n 
if isinstance ( sub . ln_solver , LinearGaussSeidel ) and sub_mode not in ( mode , ) : \n 
msg = msg . format ( name = sub . name , submode = sub_mode , rootmode = mode ) \n 
~~ ~~ ~~ return mode \n 
~~ def _json_system_tree ( self ) : \n 
def _tree_dict ( system ) : \n 
~~~ dct = OrderedDict ( ) \n 
for s in system . subsystems ( recurse = True ) : \n 
~~~ if isinstance ( s , Group ) : \n 
~~~ dct [ s . name ] = _tree_dict ( s ) \n 
~~~ dct [ s . name ] = OrderedDict ( ) \n 
for vname , meta in iteritems ( s . unknowns ) : \n 
~~~ dct [ s . name ] [ vname ] = m = meta . copy ( ) \n 
for mname in m : \n 
~~~ if isinstance ( m [ mname ] , np . ndarray ) : \n 
~~~ m [ mname ] = m [ mname ] . tolist ( ) \n 
~~ ~~ ~~ ~~ ~~ return dct \n 
~~ tree = OrderedDict ( ) \n 
tree [ ] = _tree_dict ( self . root ) \n 
return json . dumps ( tree ) \n 
~~ def _setup_communicators ( self ) : \n 
~~~ if self . comm is None : \n 
~~~ self . comm = self . _impl . world_comm ( ) \n 
~~ minproc , maxproc = self . driver . get_req_procs ( ) \n 
~~~ if not ( maxproc is None or maxproc >= self . comm . size ) : \n 
( self . comm . size , minproc , maxproc ) ) \n 
~~ elif self . comm . size < minproc : \n 
~~~ if maxproc is None : \n 
~~~ maxproc = \n 
~~ ~~ self . driver . _setup_communicators ( self . comm , os . getcwd ( ) ) \n 
~~ def _setup_units ( self , connections , params_dict , unknowns_dict ) : \n 
for target , ( source , idxs ) in iteritems ( connections ) : \n 
~~~ tmeta = params_dict [ target ] \n 
smeta = unknowns_dict [ source ] \n 
if not in tmeta or not in smeta : \n 
~~ src_unit = smeta [ ] \n 
tgt_unit = tmeta [ ] \n 
~~~ scale , offset = get_conversion_tuple ( src_unit , tgt_unit ) \n 
~~ except TypeError as err : \n 
_both_names ( smeta , to_prom_name ) , \n 
tgt_unit , \n 
_both_names ( tmeta , to_prom_name ) ) \n 
~~~ raise \n 
~~ ~~ if scale != 1.0 or offset != 0.0 : \n 
~~~ tmeta [ ] = ( scale , offset ) \n 
~~ ~~ ~~ def _add_implicit_connections ( self , connections ) : \n 
dangling = set ( ) \n 
abs_unames = self . root . _sysdata . to_abs_uname \n 
for prom_name , pabs_list in iteritems ( self . root . _sysdata . to_abs_pnames ) : \n 
~~~ for pabs in pabs_list : \n 
~~~ connections . setdefault ( pabs , [ ] ) . append ( ( abs_unames [ prom_name ] , None ) ) \n 
~~~ dangling . add ( prom_name ) \n 
~~ ~~ return dangling \n 
~~ def print_all_convergence ( self ) : \n 
root . ln_solver . print_all_convergence ( ) \n 
root . nl_solver . print_all_convergence ( ) \n 
for grp in root . subgroups ( recurse = True ) : \n 
~~~ grp . ln_solver . print_all_convergence ( ) \n 
grp . nl_solver . print_all_convergence ( ) \n 
~~ ~~ ~~ def _assign_parameters ( connections ) : \n 
param_owners = { } \n 
for par , ( unk , idxs ) in iteritems ( connections ) : \n 
~~~ param_owners . setdefault ( get_common_ancestor ( par , unk ) , set ( ) ) . add ( par ) \n 
~~ return param_owners \n 
~~ def _jac_to_flat_dict ( jac ) : \n 
new_jac = OrderedDict ( ) \n 
for key1 , val1 in iteritems ( jac ) : \n 
~~~ for key2 , val2 in iteritems ( val1 ) : \n 
~~~ new_jac [ ( key1 , key2 ) ] = val2 \n 
~~ ~~ return new_jac \n 
~~ def _pad_name ( name , pad_num = 13 , quotes = True ) : \n 
l_name = len ( name ) \n 
if l_name < pad_num : \n 
~~~ pad = pad_num - l_name \n 
if quotes : \n 
~~~ pad_str = "\'{name}\'{sep:<{pad}}" \n 
~~~ pad_str = "{name}{sep:<{pad}}" \n 
~~ pad_name = pad_str . format ( name = name , sep = , pad = pad ) \n 
return pad_name \n 
~~~ return . format ( name ) \n 
~~ ~~ def _assemble_deriv_data ( params , resids , cdata , jac_fwd , jac_rev , jac_fd , \n 
out_stream , c_name = , jac_fd2 = None , fd_desc = None , \n 
fd_desc2 = None , compact_print = False ) : \n 
started = False \n 
for p_name in params : \n 
~~~ for u_name in resids : \n 
~~~ key = ( u_name , p_name ) \n 
if key not in jac_fd : \n 
~~ ldata = cdata [ key ] = { } \n 
Jsub_fd = jac_fd [ key ] \n 
ldata [ ] = Jsub_fd \n 
magfd = np . linalg . norm ( Jsub_fd ) \n 
if jac_fwd : \n 
~~~ Jsub_for = jac_fwd [ key ] \n 
ldata [ ] = Jsub_for \n 
magfor = np . linalg . norm ( Jsub_for ) \n 
~~~ magfor = None \n 
~~ if jac_rev : \n 
~~~ Jsub_rev = jac_rev [ key ] \n 
ldata [ ] = Jsub_rev \n 
magrev = np . linalg . norm ( Jsub_rev ) \n 
~~~ magrev = None \n 
~~ if jac_fd2 : \n 
~~~ Jsub_fd2 = jac_fd2 [ key ] \n 
ldata [ ] = Jsub_fd2 \n 
magfd2 = np . linalg . norm ( Jsub_fd2 ) \n 
~~~ magfd2 = None \n 
~~ ldata [ ] = ( magfor , magrev , magfd ) \n 
~~~ abs1 = np . linalg . norm ( Jsub_for - Jsub_fd ) \n 
~~~ abs1 = None \n 
~~~ abs2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) \n 
~~~ abs2 = None \n 
~~ if jac_fwd and jac_rev : \n 
~~~ abs3 = np . linalg . norm ( Jsub_for - Jsub_rev ) \n 
~~~ abs3 = None \n 
~~~ abs4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) \n 
~~~ abs4 = None \n 
~~ ldata [ ] = ( abs1 , abs2 , abs3 ) \n 
if magfd == 0.0 : \n 
~~~ rel1 = rel2 = rel3 = rel4 = float ( ) \n 
~~~ if jac_fwd : \n 
~~~ rel1 = np . linalg . norm ( Jsub_for - Jsub_fd ) / magfd \n 
~~~ rel1 = None \n 
~~~ rel2 = np . linalg . norm ( Jsub_rev - Jsub_fd ) / magfd \n 
~~~ rel2 = None \n 
~~~ rel3 = np . linalg . norm ( Jsub_for - Jsub_rev ) / magfd \n 
~~~ rel3 = None \n 
~~~ rel4 = np . linalg . norm ( Jsub_fd2 - Jsub_fd ) / magfd \n 
~~~ rel4 = None \n 
~~ ~~ ldata [ ] = ( rel1 , rel2 , rel3 ) \n 
if out_stream is None : \n 
~~ if compact_print : \n 
~~~ if jac_fwd and jac_rev : \n 
~~~ if not started : \n 
out_str = tmp1 . format ( _pad_name ( ) , _pad_name ( ) , \n 
_pad_name ( , 10 , quotes = False ) , \n 
_pad_name ( , 10 , quotes = False ) \n 
out_stream . write ( out_str ) \n 
out_stream . write ( * len ( out_str ) + ) \n 
started = True \n 
out_stream . write ( tmp1 . format ( _pad_name ( u_name ) , _pad_name ( p_name ) , \n 
magfor , magrev , magfd , abs1 , abs2 , \n 
rel1 , rel2 ) ) \n 
~~ elif jac_fd and jac_fd2 : \n 
_pad_name ( , 13 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) , \n 
_pad_name ( , 12 , quotes = False ) \n 
magfd , magfd2 , abs4 , rel4 ) ) \n 
~~~ if started : \n 
~~~ out_stream . write ( * 30 + ) \n 
~~~ started = True \n 
~~~ out_stream . write ( % magfor ) \n 
~~~ out_stream . write ( % magrev ) \n 
~~ if not jac_fwd and not jac_rev : \n 
~~ if jac_fd : \n 
~~~ out_stream . write ( % magfd ) \n 
if fd_desc : \n 
~~~ out_stream . write ( % fd_desc ) \n 
~~ out_stream . write ( ) \n 
~~~ out_stream . write ( % magfd2 ) \n 
if fd_desc2 : \n 
~~~ out_stream . write ( % fd_desc2 ) \n 
~~~ out_stream . write ( % abs1 ) \n 
~~~ out_stream . write ( % abs2 ) \n 
~~~ out_stream . write ( % abs3 ) \n 
~~~ out_stream . write ( % abs4 ) \n 
~~~ out_stream . write ( % rel1 ) \n 
~~~ out_stream . write ( % rel2 ) \n 
~~~ out_stream . write ( % rel3 ) \n 
~~~ out_stream . write ( % rel4 ) \n 
out_stream . write ( str ( Jsub_for ) ) \n 
out_stream . write ( str ( Jsub_rev ) ) \n 
out_stream . write ( str ( Jsub_fd ) ) \n 
if jac_fd2 : \n 
out_stream . write ( str ( Jsub_fd2 ) ) \n 
~~ ~~ ~~ ~~ ~~ def _needs_iteration ( comp ) : \n 
if isinstance ( comp , Component ) and comp . is_active ( ) and comp . states : \n 
~~~ for klass in comp . __class__ . __mro__ : \n 
~~~ if klass is Component : \n 
~~ if in klass . __dict__ : \n 
~~~ return False \n 
~~ ~~ return True \n 
~~ return False \n 
~~ def _get_gmres_name ( ) : \n 
~~~ if MPI : \n 
~~~ return \n 
from sqlitedict import SqliteDict \n 
from openmdao . recorders . base_recorder import BaseRecorder \n 
from openmdao . util . record_util import format_iteration_coordinate \n 
from openmdao . core . mpi_wrap import MPI \n 
class SqliteRecorder ( BaseRecorder ) : \n 
def __init__ ( self , out , ** sqlite_dict_args ) : \n 
~~~ super ( SqliteRecorder , self ) . __init__ ( ) \n 
if MPI and MPI . COMM_WORLD . rank > 0 : \n 
~~~ self . _open_close_sqlitedict = False \n 
~~~ self . _open_close_sqlitedict = True \n 
~~ if self . _open_close_sqlitedict : \n 
~~~ sqlite_dict_args . setdefault ( , True ) \n 
sqlite_dict_args . setdefault ( , ) \n 
self . out = SqliteDict ( filename = out , flag = , ** sqlite_dict_args ) \n 
~~~ self . out = None \n 
~~ ~~ def record_metadata ( self , group ) : \n 
params = group . params . iteritems ( ) \n 
resids = group . resids . iteritems ( ) \n 
unknowns = group . unknowns . iteritems ( ) \n 
data = OrderedDict ( [ ( , dict ( params ) ) , \n 
( , dict ( unknowns ) ) , \n 
self . out [ ] = data \n 
~~ def record_iteration ( self , params , unknowns , resids , metadata ) : \n 
data = OrderedDict ( ) \n 
iteration_coordinate = metadata [ ] \n 
timestamp = metadata [ ] \n 
group_name = format_iteration_coordinate ( iteration_coordinate ) \n 
data [ ] = timestamp \n 
data [ ] = metadata [ ] \n 
if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( params , , iteration_coordinate ) \n 
~~ if self . options [ ] : \n 
~~~ data [ ] = self . _filter_vector ( unknowns , , iteration_coordinate ) \n 
~~~ data [ ] = self . _filter_vector ( resids , , iteration_coordinate ) \n 
~~ self . out [ group_name ] = data \n 
~~ def record_derivatives ( self , derivs , metadata ) : \n 
group_name = % group_name \n 
data [ ] = derivs \n 
self . out [ group_name ] = data \n 
~~ def close ( self ) : \n 
if self . _open_close_sqlitedict : \n 
~~~ if self . out is not None : \n 
~~~ self . out . close ( ) \n 
self . out = None \n 
from numpy import atleast_2d as array2d \n 
from scipy import linalg \n 
from scipy . optimize import minimize \n 
from scipy . spatial . distance import squareform \n 
from openmdao . surrogate_models . surrogate_model import MultiFiSurrogateModel \n 
_logger = logging . getLogger ( ) \n 
THETA0_DEFAULT = 0.5 \n 
THETAL_DEFAULT = 1e-5 \n 
THETAU_DEFAULT = 50 \n 
if hasattr ( linalg , ) : \n 
~~~ solve_triangular = linalg . solve_triangular \n 
~~~ def solve_triangular ( x , y , lower = True ) : \n 
~~~ return linalg . solve ( x , y ) \n 
~~ ~~ def constant_regression ( x ) : \n 
x = np . asarray ( x , dtype = np . float ) \n 
n_eval = x . shape [ 0 ] \n 
f = np . ones ( [ n_eval , 1 ] ) \n 
~~ def linear_regression ( x ) : \n 
f = np . hstack ( [ np . ones ( [ n_eval , 1 ] ) , x ] ) \n 
~~ def squared_exponential_correlation ( theta , d ) : \n 
theta = np . asarray ( theta , dtype = np . float ) \n 
d = np . asarray ( d , dtype = np . float ) \n 
if d . ndim > 1 : \n 
~~~ n_features = d . shape [ 1 ] \n 
~~~ n_features = 1 \n 
~~ if theta . size == 1 : \n 
~~~ return np . exp ( - theta [ 0 ] * np . sum ( d ** 2 , axis = 1 ) ) \n 
~~ elif theta . size != n_features : \n 
~~~ return np . exp ( - np . sum ( theta . reshape ( 1 , n_features ) * d ** 2 , axis = 1 ) ) \n 
~~ ~~ def l1_cross_distances ( X , Y = None ) : \n 
if Y is None : \n 
~~~ X = array2d ( X ) \n 
n_samples , n_features = X . shape \n 
n_nonzero_cross_dist = n_samples * ( n_samples - 1 ) // 2 \n 
D = np . zeros ( ( n_nonzero_cross_dist , n_features ) ) \n 
ll_1 = 0 \n 
for k in range ( n_samples - 1 ) : \n 
~~~ ll_0 = ll_1 \n 
ll_1 = ll_0 + n_samples - k - 1 \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - X [ ( k + 1 ) : ] ) \n 
~~ return D \n 
Y = array2d ( Y ) \n 
n_samples_X , n_features_X = X . shape \n 
n_samples_Y , n_features_Y = Y . shape \n 
if n_features_X != n_features_Y : \n 
~~ n_features = n_features_X \n 
n_nonzero_cross_dist = n_samples_X * n_samples_Y \n 
for k in range ( n_samples_X ) : \n 
D [ ll_0 : ll_1 ] = np . abs ( X [ k ] - Y ) \n 
~~ ~~ class MultiFiCoKriging ( object ) : \n 
_regression_types = { \n 
: constant_regression , \n 
: linear_regression } \n 
def __init__ ( self , regr = , rho_regr = , \n 
theta = None , theta0 = None , thetaL = None , thetaU = None ) : \n 
~~~ self . corr = squared_exponential_correlation \n 
self . regr = regr \n 
self . rho_regr = rho_regr \n 
self . theta = theta \n 
self . theta0 = theta0 \n 
self . thetaL = thetaL \n 
self . thetaU = thetaU \n 
self . _nfev = 0 \n 
~~ def _build_R ( self , lvl , theta ) : \n 
D = self . D [ lvl ] \n 
n_samples = self . n_samples [ lvl ] \n 
R = np . eye ( n_samples ) * ( 1. + NUGGET ) \n 
corr = squareform ( self . corr ( theta , D ) ) \n 
R = R + corr \n 
return R \n 
~~ def fit ( self , X , y , \n 
initial_range = INITIAL_RANGE_DEFAULT , tol = TOLERANCE_DEFAULT ) : \n 
self . _check_list_structure ( X , y ) \n 
self . _check_params ( ) \n 
X = self . X \n 
y = self . y \n 
nlevel = self . nlevel \n 
n_samples = self . n_samples \n 
self . beta = nlevel * [ 0 ] \n 
self . beta_rho = nlevel * [ None ] \n 
self . beta_regr = nlevel * [ None ] \n 
self . C = nlevel * [ 0 ] \n 
self . D = nlevel * [ 0 ] \n 
self . F = nlevel * [ 0 ] \n 
self . p = nlevel * [ 0 ] \n 
self . q = nlevel * [ 0 ] \n 
self . G = nlevel * [ 0 ] \n 
self . sigma2 = nlevel * [ 0 ] \n 
self . _R_adj = nlevel * [ None ] \n 
y_best = y [ nlevel - 1 ] \n 
for i in range ( nlevel - 1 ) [ : : - 1 ] : \n 
~~~ y_best = np . concatenate ( ( y [ i ] [ : - n_samples [ i + 1 ] ] , y_best ) ) \n 
~~ self . y_best = y_best \n 
self . y_mean = np . zeros ( 1 ) \n 
self . y_std = np . ones ( 1 ) \n 
self . X_mean = np . zeros ( 1 ) \n 
self . X_std = np . ones ( 1 ) \n 
for lvl in range ( nlevel ) : \n 
~~~ self . D [ lvl ] = l1_cross_distances ( X [ lvl ] ) \n 
if ( np . min ( np . sum ( self . D [ lvl ] , axis = 1 ) ) == 0. ) : \n 
~~ self . F [ lvl ] = self . regr ( X [ lvl ] ) \n 
self . p [ lvl ] = self . F [ lvl ] . shape [ 1 ] \n 
if lvl > 0 : \n 
~~~ F_rho = self . rho_regr ( X [ lvl ] ) \n 
self . q [ lvl ] = F_rho . shape [ 1 ] \n 
self . F [ lvl ] = np . hstack ( ( F_rho * np . dot ( ( self . y [ lvl - 1 ] ) [ - n_samples [ lvl ] : ] , \n 
np . ones ( ( 1 , self . q [ lvl ] ) ) ) , self . F [ lvl ] ) ) \n 
~~~ self . q [ lvl ] = 0 \n 
~~ n_samples_F_i = self . F [ lvl ] . shape [ 0 ] \n 
if n_samples_F_i != n_samples [ lvl ] : \n 
~~ if int ( self . p [ lvl ] + self . q [ lvl ] ) >= n_samples_F_i : \n 
% ( n_samples [ i ] , self . p [ lvl ] + self . q [ lvl ] ) ) \n 
~~ ~~ self . X = X \n 
self . y = y \n 
self . rlf_value = np . zeros ( nlevel ) \n 
~~~ if self . theta [ lvl ] is None : \n 
~~~ sol = self . _max_rlf ( lvl = lvl , initial_range = initial_range , tol = tol ) \n 
self . theta [ lvl ] = sol [ ] \n 
self . rlf_value [ lvl ] = sol [ ] \n 
if np . isinf ( self . rlf_value [ lvl ] ) : \n 
~~~ self . rlf_value [ lvl ] = self . rlf ( lvl = lvl ) \n 
~~ ~~ ~~ return \n 
~~ def rlf ( self , lvl , theta = None ) : \n 
if theta is None : \n 
~~~ theta = self . theta [ lvl ] \n 
~~ rlf_value = 1e20 \n 
y = self . y [ lvl ] \n 
F = self . F [ lvl ] \n 
p = self . p [ lvl ] \n 
q = self . q [ lvl ] \n 
R = self . _build_R ( lvl , theta ) \n 
~~~ C = linalg . cholesky ( R , lower = True ) \n 
~~ except linalg . LinAlgError : \n 
~~~ _logger . warning ( ( % lvl ) + \n 
+ str ( theta ) ) \n 
return rlf_value \n 
~~ Ft = solve_triangular ( C , F , lower = True ) \n 
Yt = solve_triangular ( C , y , lower = True ) \n 
~~~ Q , G = linalg . qr ( Ft , econ = True ) \n 
~~~ Q , G = linalg . qr ( Ft , mode = ) \n 
pass \n 
~~ beta = solve_triangular ( G , np . dot ( Q . T , Yt ) ) \n 
err = Yt - np . dot ( Ft , beta ) \n 
err2 = np . dot ( err . T , err ) [ 0 , 0 ] \n 
self . _err = err \n 
sigma2 = err2 / ( n_samples - p - q ) \n 
detR = ( ( np . diag ( C ) ) ** ( 2. / n_samples ) ) . prod ( ) \n 
rlf_value = ( n_samples - p - q ) * np . log10 ( sigma2 ) + n_samples * np . log10 ( detR ) \n 
self . beta_rho [ lvl ] = beta [ : q ] \n 
self . beta_regr [ lvl ] = beta [ q : ] \n 
self . beta [ lvl ] = beta \n 
self . sigma2 [ lvl ] = sigma2 \n 
self . C [ lvl ] = C \n 
self . G [ lvl ] = G \n 
~~ def _max_rlf ( self , lvl , initial_range , tol ) : \n 
thetaL = self . thetaL [ lvl ] \n 
thetaU = self . thetaU [ lvl ] \n 
def rlf_transform ( x ) : \n 
~~~ return self . rlf ( theta = 10. ** x , lvl = lvl ) \n 
~~ theta0 = self . theta0 [ lvl ] \n 
x0 = np . log10 ( theta0 [ 0 ] ) \n 
constraints = [ ] \n 
for i in range ( theta0 . size ) : \n 
~~~ constraints . append ( { : , : lambda log10t , i = i : \n 
log10t [ i ] - np . log10 ( thetaL [ 0 ] [ i ] ) } ) \n 
constraints . append ( { : , : lambda log10t , i = i : \n 
np . log10 ( thetaU [ 0 ] [ i ] ) - log10t [ i ] } ) \n 
~~ constraints = tuple ( constraints ) \n 
sol = minimize ( rlf_transform , x0 , method = , \n 
constraints = constraints , \n 
options = { : initial_range , \n 
: tol , : 0 } ) \n 
log10_optimal_x = sol [ ] \n 
optimal_rlf_value = sol [ ] \n 
self . _nfev += sol [ ] \n 
optimal_theta = 10. ** log10_optimal_x \n 
res = { } \n 
res [ ] = optimal_theta \n 
res [ ] = optimal_rlf_value \n 
return res \n 
~~ def predict ( self , X , eval_MSE = True ) : \n 
X = array2d ( X ) \n 
n_eval , n_features_X = X . shape \n 
mu = np . zeros ( ( n_eval , nlevel ) ) \n 
f = self . regr ( X ) \n 
f0 = self . regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ 0 ] ) \n 
F = self . F [ 0 ] \n 
C = self . C [ 0 ] \n 
beta = self . beta [ 0 ] \n 
Ft = solve_triangular ( C , F , lower = True ) \n 
yt = solve_triangular ( C , self . y [ 0 ] , lower = True ) \n 
r_ = self . corr ( self . theta [ 0 ] , dx ) . reshape ( n_eval , self . n_samples [ 0 ] ) \n 
gamma = solve_triangular ( C . T , yt - np . dot ( Ft , beta ) , lower = False ) \n 
mu [ : , 0 ] = ( np . dot ( f , beta ) + np . dot ( r_ , gamma ) ) . ravel ( ) \n 
if eval_MSE : \n 
~~~ self . sigma2_rho = nlevel * [ None ] \n 
MSE = np . zeros ( ( n_eval , nlevel ) ) \n 
r_t = solve_triangular ( C , r_ . T , lower = True ) \n 
G = self . G [ 0 ] \n 
u_ = solve_triangular ( G . T , f . T - np . dot ( Ft . T , r_t ) , lower = True ) \n 
MSE [ : , 0 ] = self . sigma2 [ 0 ] * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) + ( u_ ** 2 ) . sum ( axis = 0 ) ) \n 
~~ for i in range ( 1 , nlevel ) : \n 
~~~ C = self . C [ i ] \n 
F = self . F [ i ] \n 
g = self . rho_regr ( X ) \n 
dx = l1_cross_distances ( X , Y = self . X [ i ] ) \n 
r_ = self . corr ( self . theta [ i ] , dx ) . reshape ( n_eval , self . n_samples [ i ] ) \n 
f = np . vstack ( ( g . T * mu [ : , i - 1 ] , f0 . T ) ) \n 
yt = solve_triangular ( C , self . y [ i ] , lower = True ) \n 
G = self . G [ i ] \n 
beta = self . beta [ i ] \n 
mu [ : , i ] = ( np . dot ( f . T , beta ) + np . dot ( r_t . T , yt - np . dot ( Ft , beta ) ) ) . ravel ( ) \n 
~~~ Q_ = ( np . dot ( ( yt - np . dot ( Ft , beta ) ) . T , yt - np . dot ( Ft , beta ) ) ) [ 0 , 0 ] \n 
u_ = solve_triangular ( G . T , f - np . dot ( Ft . T , r_t ) , lower = True ) \n 
sigma2_rho = np . dot ( g , self . sigma2 [ i ] * linalg . inv ( np . dot ( G . T , G ) ) [ : self . q [ i ] , : self . q [ i ] ] + np . dot ( beta [ : self . q [ i ] ] , beta [ : self . q [ i ] ] . T ) ) \n 
sigma2_rho = ( sigma2_rho * g ) . sum ( axis = 1 ) \n 
MSE [ : , i ] = sigma2_rho * MSE [ : , i - 1 ] + Q_ / ( 2 * ( self . n_samples [ i ] - self . p [ i ] - self . q [ i ] ) ) * ( 1 - ( r_t ** 2 ) . sum ( axis = 0 ) ) + self . sigma2 [ i ] * ( u_ ** 2 ) . sum ( axis = 0 ) \n 
~~~ mu [ : , i ] = self . y_mean + self . y_std * mu [ : , i ] \n 
~~~ MSE [ : , i ] = self . y_std ** 2 * MSE [ : , i ] \n 
~~ ~~ if eval_MSE : \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) , MSE [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
~~~ return mu [ : , - 1 ] . reshape ( ( n_eval , 1 ) ) \n 
~~ ~~ def _check_list_structure ( self , X , y ) : \n 
~~~ if type ( X ) is not list : \n 
~~~ nlevel = 1 \n 
X = [ X ] \n 
~~~ nlevel = len ( X ) \n 
~~ if type ( y ) is not list : \n 
~~~ y = [ y ] \n 
~~ if len ( X ) != len ( y ) : \n 
~~ n_samples = np . zeros ( nlevel , dtype = int ) \n 
n_features = np . zeros ( nlevel , dtype = int ) \n 
n_samples_y = np . zeros ( nlevel , dtype = int ) \n 
for i in range ( nlevel ) : \n 
~~~ n_samples [ i ] , n_features [ i ] = X [ i ] . shape \n 
if i > 1 and n_features [ i ] != n_features [ i - 1 ] : \n 
~~ y [ i ] = np . asarray ( y [ i ] ) . ravel ( ) [ : , np . newaxis ] \n 
n_samples_y [ i ] = y [ i ] . shape [ 0 ] \n 
if n_samples [ i ] != n_samples_y [ i ] : \n 
~~ ~~ self . n_features = n_features [ 0 ] \n 
if type ( self . theta ) is not list : \n 
~~~ self . theta = nlevel * [ self . theta ] \n 
~~ elif len ( self . theta ) != nlevel : \n 
~~ if type ( self . theta0 ) is not list : \n 
~~~ self . theta0 = nlevel * [ self . theta0 ] \n 
~~ elif len ( self . theta0 ) != nlevel : \n 
~~ if type ( self . thetaL ) is not list : \n 
~~~ self . thetaL = nlevel * [ self . thetaL ] \n 
~~ elif len ( self . thetaL ) != nlevel : \n 
~~ if type ( self . thetaU ) is not list : \n 
~~~ self . thetaU = nlevel * [ self . thetaU ] \n 
~~ elif len ( self . thetaU ) != nlevel : \n 
~~ self . nlevel = nlevel \n 
self . X = X [ : ] \n 
self . y = y [ : ] \n 
self . n_samples = n_samples \n 
~~ def _check_params ( self ) : \n 
~~~ if not callable ( self . regr ) : \n 
~~~ if self . regr in self . _regression_types : \n 
~~~ self . regr = self . _regression_types [ self . regr ] \n 
% ( self . _regression_types . keys ( ) , self . regr ) ) \n 
~~ ~~ if not callable ( self . rho_regr ) : \n 
~~~ if self . rho_regr in self . _regression_types : \n 
~~~ self . rho_regr = self . _regression_types [ self . rho_regr ] \n 
% ( self . _regression_types . keys ( ) , self . rho_regr ) ) \n 
~~ ~~ for i in range ( self . nlevel ) : \n 
~~~ if self . theta [ i ] is not None : \n 
~~~ self . theta [ i ] = array2d ( self . theta [ i ] ) \n 
if np . any ( self . theta [ i ] <= 0 ) : \n 
~~ ~~ if self . theta0 [ i ] is not None : \n 
~~~ self . theta0 [ i ] = array2d ( self . theta0 [ i ] ) \n 
if np . any ( self . theta0 [ i ] <= 0 ) : \n 
~~~ self . theta0 [ i ] = array2d ( self . n_features * [ THETA0_DEFAULT ] ) \n 
~~ lth = self . theta0 [ i ] . size \n 
if self . thetaL [ i ] is not None : \n 
~~~ self . thetaL [ i ] = array2d ( self . thetaL [ i ] ) \n 
if self . thetaL [ i ] . size != lth : \n 
~~~ self . thetaL [ i ] = array2d ( self . n_features * [ THETAL_DEFAULT ] ) \n 
~~ if self . thetaU [ i ] is not None : \n 
~~~ self . thetaU [ i ] = array2d ( self . thetaU [ i ] ) \n 
if self . thetaU [ i ] . size != lth : \n 
~~~ self . thetaU [ i ] = array2d ( self . n_features * [ THETAU_DEFAULT ] ) \n 
~~ if np . any ( self . thetaL [ i ] <= 0 ) or np . any ( self . thetaU [ i ] < self . thetaL [ i ] ) : \n 
"thetaU." ) \n 
~~ ~~ return \n 
~~ ~~ class MultiFiCoKrigingSurrogate ( MultiFiSurrogateModel ) : \n 
theta = None , theta0 = None , thetaL = None , thetaU = None , \n 
tolerance = TOLERANCE_DEFAULT , initial_range = INITIAL_RANGE_DEFAULT ) : \n 
~~~ super ( MultiFiCoKrigingSurrogate , self ) . __init__ ( ) \n 
self . tolerance = tolerance \n 
self . initial_range = initial_range \n 
self . model = MultiFiCoKriging ( regr = regr , rho_regr = rho_regr , theta = theta , \n 
theta0 = theta0 , thetaL = thetaL , thetaU = thetaU ) \n 
~~ def predict ( self , new_x ) : \n 
Y_pred , MSE = self . model . predict ( [ new_x ] ) \n 
return Y_pred , np . sqrt ( np . abs ( MSE ) ) \n 
~~ def train_multifi ( self , X , Y ) : \n 
X , Y = self . _fit_adapter ( X , Y ) \n 
self . model . fit ( X , Y , tol = self . tolerance , initial_range = self . initial_range ) \n 
~~ def _fit_adapter ( self , X , Y ) : \n 
~~~ if len ( np . shape ( np . array ( X [ 0 ] ) ) ) == 1 : \n 
~~~ X = [ X ] \n 
Y = [ Y ] \n 
~~ X = [ np . array ( x ) for x in reversed ( X ) ] \n 
Y = [ np . array ( y ) for y in reversed ( Y ) ] \n 
return ( X , Y ) \n 
~~ ~~ class FloatMultiFiCoKrigingSurrogate ( MultiFiCoKrigingSurrogate ) : \n 
def predict ( self , new_x ) : \n 
~~~ dist = super ( FloatMultiFiCoKrigingSurrogate , self ) . predict ( new_x ) \n 
return dist . mu \n 
~~~ import doctest \n 
doctest . testmod ( ) \n 
from six . moves import range \n 
from pyparsing import CaselessLiteral , Combine , OneOrMore , Optional , TokenConverter , Word , nums , oneOf , printables , ParserElement , alphanums \n 
__all__ = [ , ] \n 
def _getformat ( val ) : \n 
~~~ if int ( val ) == val : \n 
~~~ return "%.1f" \n 
~~~ return "%.16g" \n 
~~ ~~ class _SubHelper ( object ) : \n 
~~~ self . newtext = "" \n 
self . replace_location = 0 \n 
self . current_location = 0 \n 
self . counter = 0 \n 
self . start_location = 0 \n 
self . end_location = 0 \n 
~~ def set ( self , newtext , location ) : \n 
self . newtext = newtext \n 
self . replace_location = location \n 
~~ def set_array ( self , newtext , start_location , end_location ) : \n 
self . start_location = start_location \n 
self . end_location = end_location \n 
~~ def replace ( self , text ) : \n 
self . current_location += 1 \n 
if self . current_location == self . replace_location : \n 
~~~ if isinstance ( self . newtext , float ) : \n 
~~~ return _getformat ( self . newtext ) % self . newtext \n 
~~~ return str ( self . newtext ) \n 
~~~ return text . group ( ) \n 
~~ ~~ def replace_array ( self , text ) : \n 
end = len ( self . newtext ) \n 
if self . current_location >= self . start_location and self . current_location <= self . end_location and self . counter < end : \n 
~~~ if isinstance ( self . newtext [ self . counter ] , float ) : \n 
~~~ val = self . newtext [ self . counter ] \n 
newval = _getformat ( val ) % val \n 
~~~ newval = str ( self . newtext [ self . counter ] ) \n 
~~ self . counter += 1 \n 
return newval \n 
~~ ~~ ~~ class ToInteger ( TokenConverter ) : \n 
def postParse ( self , instring , loc , tokenlist ) : \n 
return int ( tokenlist [ 0 ] ) \n 
~~ ~~ class ToFloat ( TokenConverter ) : \n 
return float ( tokenlist [ 0 ] . replace ( , ) ) \n 
~~ ~~ class ToNan ( TokenConverter ) : \n 
return float ( ) \n 
~~ ~~ class ToInf ( TokenConverter ) : \n 
~~ ~~ class InputFileGenerator ( object ) : \n 
~~~ self . template_filename = [ ] \n 
self . output_filename = [ ] \n 
self . reg = re . compile ( ) \n 
self . data = [ ] \n 
self . current_row = 0 \n 
self . anchored = False \n 
~~ def set_template_file ( self , filename ) : \n 
self . template_filename = filename \n 
templatefile = open ( filename , ) \n 
self . data = templatefile . readlines ( ) \n 
templatefile . close ( ) \n 
~~ def set_generated_file ( self , filename ) : \n 
self . output_filename = filename \n 
~~ def set_delimiters ( self , delimiter ) : \n 
self . delimiter = delimiter \n 
self . reg = re . compile ( + delimiter + ) \n 
~~ def mark_anchor ( self , anchor , occurrence = 1 ) : \n 
if not isinstance ( occurrence , int ) : \n 
~~ instance = 0 \n 
if occurrence > 0 : \n 
~~~ count = 0 \n 
max_lines = len ( self . data ) \n 
for index in range ( self . current_row , max_lines ) : \n 
~~~ line = self . data [ index ] \n 
if count == 0 and self . anchored : \n 
~~~ line = line . split ( anchor ) [ - 1 ] \n 
~~ if line . find ( anchor ) > - 1 : \n 
~~~ instance += 1 \n 
if instance == occurrence : \n 
~~~ self . current_row += count \n 
self . anchored = True \n 
~~ ~~ count += 1 \n 
~~ ~~ elif occurrence < 0 : \n 
~~~ max_lines = len ( self . data ) - 1 \n 
count = max_lines \n 
for index in range ( max_lines , - 1 , - 1 ) : \n 
if count == max_lines and self . anchored : \n 
~~~ line = line . split ( anchor ) [ 0 ] \n 
~~~ instance += - 1 \n 
~~~ self . current_row = count \n 
~~ ~~ count -= 1 \n 
~~ def reset_anchor ( self ) : \n 
~~ def transfer_var ( self , value , row , field ) : \n 
j = self . current_row + row \n 
line = self . data [ j ] \n 
sub = _SubHelper ( ) \n 
sub . set ( value , field ) \n 
newline = re . sub ( self . reg , sub . replace , line ) \n 
self . data [ j ] = newline \n 
~~ def transfer_array ( self , value , row_start , field_start , field_end , \n 
if row_end is None : \n 
~~~ row_end = row_start \n 
~~ sub = _SubHelper ( ) \n 
for row in range ( row_start , row_end + 1 ) : \n 
~~~ j = self . current_row + row \n 
if row == row_end : \n 
~~~ f_end = field_end \n 
~~~ f_end = 99999 \n 
~~ sub . set_array ( value , field_start , f_end ) \n 
field_start = 0 \n 
newline = re . sub ( self . reg , sub . replace_array , line ) \n 
~~ if sub . counter < len ( value ) : \n 
~~~ for val in value [ sub . counter : ] : \n 
~~~ newline = newline . rstrip ( ) + sep + str ( val ) \n 
~~ self . data [ j ] = newline \n 
~~ elif sub . counter > len ( value ) : \n 
~~ self . data [ j ] += "\\n" \n 
~~ def transfer_2Darray ( self , value , row_start , row_end , field_start , \n 
field_end ) : \n 
sub . set_array ( value [ i , : ] , field_start , field_end ) \n 
sub . current_location = 0 \n 
sub . counter = 0 \n 
~~ ~~ def clearline ( self , row ) : \n 
self . data [ self . current_row + row ] = "\\n" \n 
~~ def generate ( self ) : \n 
infile = open ( self . output_filename , ) \n 
infile . writelines ( self . data ) \n 
infile . close ( ) \n 
~~ ~~ class FileParser ( object ) : \n 
def __init__ ( self , end_of_line_comment_char = None , full_line_comment_char = None ) : \n 
~~~ self . filename = [ ] \n 
self . end_of_line_comment_char = end_of_line_comment_char \n 
self . full_line_comment_char = full_line_comment_char \n 
self . set_delimiters ( self . delimiter ) \n 
~~ def set_file ( self , filename ) : \n 
inputfile = open ( filename , ) \n 
if not self . end_of_line_comment_char and not self . full_line_comment_char : \n 
~~~ self . data = inputfile . readlines ( ) \n 
~~~ self . data = [ ] \n 
for line in inputfile : \n 
~~~ if line [ 0 ] == self . full_line_comment_char : \n 
~~ self . data . append ( line . split ( self . end_of_line_comment_char ) [ 0 ] ) \n 
~~ ~~ inputfile . close ( ) \n 
if delimiter != "columns" : \n 
~~~ ParserElement . setDefaultWhitespaceChars ( str ( delimiter ) ) \n 
~~ self . _reset_tokens ( ) \n 
~~ if anchor in line : \n 
~~ def transfer_line ( self , row ) : \n 
return self . data [ self . current_row + row ] . rstrip ( ) \n 
~~ def transfer_var ( self , row , field , fieldend = None ) : \n 
if self . delimiter == "columns" : \n 
~~~ if not fieldend : \n 
~~~ line = line [ ( field - 1 ) : ] \n 
~~~ line = line [ ( field - 1 ) : ( fieldend ) ] \n 
~~ data = self . _parse_line ( ) . parseString ( line ) \n 
if len ( data ) > 1 : \n 
~~~ return line \n 
~~~ return data [ 0 ] \n 
~~~ data = self . _parse_line ( ) . parseString ( line ) \n 
return data [ field - 1 ] \n 
~~ ~~ def transfer_keyvar ( self , key , field , occurrence = 1 , rowoffset = 0 ) : \n 
if not isinstance ( occurrence , int ) or occurrence == 0 : \n 
~~~ row = 0 \n 
for line in self . data [ self . current_row : ] : \n 
~~~ if line . find ( key ) > - 1 : \n 
~~ ~~ row += 1 \n 
~~~ row = - 1 \n 
for line in reversed ( self . data [ self . current_row : ] ) : \n 
~~ ~~ row -= 1 \n 
~~ ~~ j = self . current_row + row + rowoffset \n 
fields = self . _parse_line ( ) . parseString ( line . replace ( key , "KeyField" ) ) \n 
return fields [ field ] \n 
~~ def transfer_array ( self , rowstart , fieldstart , rowend = None , fieldend = None ) : \n 
j1 = self . current_row + rowstart \n 
if rowend is None : \n 
~~~ j2 = j1 + 1 \n 
~~~ j2 = self . current_row + rowend + 1 \n 
~~ if not fieldend : \n 
~~ lines = self . data [ j1 : j2 ] \n 
data = np . zeros ( shape = ( 0 , 0 ) ) \n 
for i , line in enumerate ( lines ) : \n 
~~~ if self . delimiter == "columns" : \n 
~~~ line = line [ ( fieldstart - 1 ) : fieldend ] \n 
line = line . strip ( ) \n 
parsed = self . _parse_line ( ) . parseString ( line ) \n 
newdata = np . array ( parsed [ : ] ) \n 
if newdata . dtype . type is np . str_ : \n 
~~~ newdata = np . array ( line ) \n 
~~ data = np . append ( data , newdata ) \n 
~~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
if i == j2 - j1 - 1 : \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) ) \n 
~~~ data = np . append ( data , np . array ( parsed [ ( fieldstart - 1 ) : ] ) ) \n 
~~ fieldstart = 1 \n 
~~ ~~ return data \n 
~~ def transfer_2Darray ( self , rowstart , fieldstart , rowend , fieldend = None ) : \n 
if fieldend and ( fieldstart > fieldend ) : \n 
~~ if rowstart > rowend : \n 
~~ j1 = self . current_row + rowstart \n 
j2 = self . current_row + rowend + 1 \n 
lines = list ( self . data [ j1 : j2 ] ) \n 
~~~ if fieldend : \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : fieldend ] \n 
~~~ line = lines [ 0 ] [ ( fieldstart - 1 ) : ] \n 
~~ parsed = self . _parse_line ( ) . parseString ( line ) \n 
row = np . array ( parsed [ : ] ) \n 
data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
data [ 0 , : ] = row \n 
for i , line in enumerate ( list ( lines [ 1 : ] ) ) : \n 
~~~ line = line [ ( fieldstart - 1 ) : ] \n 
data [ i + 1 , : ] = np . array ( parsed [ : ] ) \n 
~~~ parsed = self . _parse_line ( ) . parseString ( lines [ 0 ] ) \n 
if fieldend : \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ row = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ data = np . zeros ( shape = ( abs ( j2 - j1 ) , len ( row ) ) ) \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : fieldend ] ) \n 
~~~ print ( data ) \n 
~~~ data [ i + 1 , : ] = np . array ( parsed [ ( fieldstart - 1 ) : ] ) \n 
~~ ~~ ~~ return data \n 
~~ def _parse_line ( self ) : \n 
return self . line_parse_token \n 
~~ def _reset_tokens ( self ) : \n 
if self . delimiter . isspace ( ) : \n 
~~~ textchars = printables \n 
~~~ textchars = alphanums \n 
symbols = [ , , , , , , , , , , \n 
, , , , , , , , , , \n 
, , , , , , ] \n 
for symbol in symbols : \n 
~~~ if symbol not in self . delimiter : \n 
~~~ textchars = textchars + symbol \n 
~~ ~~ ~~ digits = Word ( nums ) \n 
dot = "." \n 
ee = CaselessLiteral ( ) | CaselessLiteral ( ) \n 
num_int = ToInteger ( Combine ( Optional ( sign ) + digits ) ) \n 
num_float = ToFloat ( Combine ( Optional ( sign ) + \n 
( ( digits + dot + Optional ( digits ) ) | \n 
( dot + digits ) ) + \n 
Optional ( ee + Optional ( sign ) + digits ) \n 
) ) \n 
mixed_exp = ToFloat ( Combine ( digits + ee + Optional ( sign ) + digits ) ) \n 
string_text = Word ( textchars ) \n 
self . line_parse_token = ( OneOrMore ( ( nan | num_float | mixed_exp | num_int | \n 
string_text ) ) ) \n 
from OpenPNM . Geometry import models as gm \n 
from OpenPNM . Geometry import GenericGeometry \n 
class SGL10 ( GenericGeometry ) : \n 
def __init__ ( self , ** kwargs ) : \n 
~~~ super ( ) . __init__ ( ** kwargs ) \n 
self . _generate ( ) \n 
~~ def _generate ( self ) : \n 
~~~ self . models . add ( propname = , \n 
model = gm . pore_misc . random , \n 
num_range = [ 0 , 0.8834 ] , \n 
regen_mode = ) \n 
self . models . add ( propname = , \n 
model = gm . throat_misc . neighbor , \n 
pore_prop = , \n 
mode = ) \n 
model = gm . pore_diameter . sphere , \n 
psd_name = , \n 
psd_shape = 3.07 , \n 
psd_loc = 1.97e-6 , \n 
psd_scale = 1.6e-5 , \n 
psd_offset = 18e-6 ) \n 
model = gm . pore_area . spherical ) \n 
model = gm . pore_volume . sphere ) \n 
model = gm . throat_diameter . cylinder , \n 
tsd_name = , \n 
tsd_shape = 3.07 , \n 
tsd_loc = 1.97e-6 , \n 
tsd_scale = 1.6e-5 , \n 
tsd_offset = 18e-6 ) \n 
model = gm . throat_length . straight ) \n 
model = gm . throat_volume . cylinder ) \n 
model = gm . throat_area . cylinder ) \n 
model = gm . throat_surface_area . cylinder ) \n 
import scipy as _sp \n 
def pore_to_pore ( geometry , ** kwargs ) : \n 
network = geometry . _net \n 
throats = network . throats ( geometry . name ) \n 
pores = network . find_connected_pores ( throats , flatten = False ) \n 
C0 = network [ ] [ pores , 0 ] \n 
C1 = network [ ] [ pores , 1 ] \n 
V = C1 - C0 \n 
L = _sp . array ( _sp . sqrt ( _sp . sum ( V [ : , : ] ** 2 , axis = 1 ) ) , ndmin = 1 ) \n 
value = V / _sp . array ( L , ndmin = 2 ) . T \n 
return value \n 
def standard ( phase , \n 
pore_MW = , \n 
pore_density = , \n 
** kwargs ) : \n 
MW = phase [ pore_MW ] \n 
rho = phase [ pore_density ] \n 
value = rho / MW \n 
~~ def ideal_gas ( phase , \n 
pore_pressure = , \n 
pore_temperature = , \n 
R = 8.31447 \n 
P = phase [ pore_pressure ] \n 
T = phase [ pore_temperature ] \n 
value = P / ( R * T ) \n 
~~ def vanderwaals ( phase , \n 
pore_P = , \n 
pore_T = , \n 
pore_Pc = , \n 
pore_Tc = , \n 
P = phase [ pore_P ] / 100000 \n 
T = phase [ pore_T ] \n 
Tc = phase [ pore_Tc ] \n 
R = 83.1447 \n 
a = 27 * ( R ** 2 ) * ( Tc ** 2 ) / ( 64 * Pc ) \n 
b = R * Tc / ( 8 * Pc ) \n 
a1 = - 1 / b \n 
a2 = ( R * T + b * P ) / ( a * b ) \n 
a3 = - P / ( a * b ) \n 
a0 = sp . ones ( sp . shape ( a1 ) ) \n 
coeffs = sp . vstack ( ( a0 , a1 , a2 , a3 ) ) . T \n 
density = sp . array ( [ sp . roots ( C ) for C in coeffs ] ) \n 
~~ import os \n 
from distutils . util import convert_path \n 
~~~ from setuptools import setup \n 
~~~ from distutils . core import setup \n 
~~ sys . path . append ( os . getcwd ( ) ) \n 
main_ = { } \n 
ver_path = convert_path ( ) \n 
with open ( ver_path ) as f : \n 
~~~ for line in f : \n 
~~~ if line . startswith ( ) : \n 
~~~ exec ( line , main_ ) \n 
~~ ~~ ~~ setup ( \n 
name = , \n 
description = version = main_ [ ] , \n 
classifiers = [ \n 
] , \n 
packages = [ \n 
install_requires = [ \n 
author = , \n 
author_email = , \n 
download_url = , \n 
url = \n 
class PoreCentroidTest : \n 
~~~ def test_voronoi ( self ) : \n 
~~ ~~ import pytest \n 
import OpenPNM \n 
class GenericPhaseTest : \n 
~~~ def setup_class ( self ) : \n 
~~~ self . net = OpenPNM . Network . Cubic ( shape = [ 5 , 5 , 5 ] ) \n 
~~ def test_init_w_no_network ( self ) : \n 
~~~ OpenPNM . Phases . GenericPhase ( ) \n 
~~ def test_init_w_components ( self ) : \n 
~~~ comp1 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
comp2 = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
OpenPNM . Phases . GenericPhase ( network = self . net , \n 
components = [ comp1 , comp2 ] ) \n 
~~ def test_set_component_add ( self ) : \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net ) \n 
phase . set_component ( comp1 ) \n 
phase . set_component ( comp2 ) \n 
~~ def test_set_component_add_twice ( self ) : \n 
with pytest . raises ( Exception ) : \n 
~~~ phase . set_components ( comp1 ) \n 
~~ ~~ def test_set_component_remove ( self ) : \n 
phase = OpenPNM . Phases . GenericPhase ( network = self . net , \n 
phase . set_component ( comp1 , mode = ) \n 
phase . set_component ( comp2 , mode = ) \n 
~~ def test_set_component_remove_twice ( self ) : \n 
~~~ phase . set_component ( comp1 , mode = ) \n 
~~ ~~ ~~ from __future__ import print_function \n 
from time import time \n 
MROW = 1000 * 1000. \n 
COLDCACHE = 5 \n 
WARMCACHE = 5 \n 
rdm_cod = [ , ] \n 
def get_nrows ( nrows_str ) : \n 
~~~ if nrows_str . endswith ( "k" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 ) \n 
~~ elif nrows_str . endswith ( "m" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 ) \n 
~~ elif nrows_str . endswith ( "g" ) : \n 
~~~ return int ( float ( nrows_str [ : - 1 ] ) * 1000 * 1000 * 1000 ) \n 
~~~ raise ValueError ( \n 
~~ ~~ class DB ( object ) : \n 
~~~ def __init__ ( self , nrows , rng , userandom ) : \n 
~~~ global step , scale \n 
self . step = STEP \n 
self . scale = SCALE \n 
self . rng = rng \n 
self . userandom = userandom \n 
self . filename = . join ( [ rdm_cod [ userandom ] , nrows ] ) \n 
self . nrows = get_nrows ( nrows ) \n 
~~ def get_db_size ( self ) : \n 
stdout = subprocess . PIPE ) . stdout \n 
line = [ l for l in sout ] [ 0 ] \n 
return int ( line . split ( ) [ 0 ] ) \n 
~~ def print_mtime ( self , t1 , explain ) : \n 
~~~ mtime = time ( ) - t1 \n 
print ( "%s:" % explain , round ( mtime , 6 ) ) \n 
print ( "Krows/s:" , round ( ( self . nrows / 1000. ) / mtime , 6 ) ) \n 
~~ def print_qtime ( self , colname , ltimes ) : \n 
print ( "Mrows/s:" , round ( ( self . nrows / ( MROW ) ) / qtime1 , 6 ) ) \n 
~~ def norm_times ( self , ltimes ) : \n 
lmean = ltimes . mean ( ) \n 
lstd = ltimes . std ( ) \n 
ntimes = ltimes [ ltimes < lmean + lstd ] \n 
nmean = ntimes . mean ( ) \n 
nstd = ntimes . std ( ) \n 
return nmean , nstd \n 
~~ def print_qtime_idx ( self , colname , ltimes , repeated , verbose ) : \n 
~~~ if repeated : \n 
~~ ltimes = numpy . array ( ltimes ) \n 
ntimes = len ( ltimes ) \n 
ctimes = ltimes [ 1 : COLDCACHE ] \n 
cmean , cstd = self . norm_times ( ctimes ) \n 
wtimes = ltimes [ WARMCACHE : ] \n 
wmean , wstd = self . norm_times ( wtimes ) \n 
numpy . histogram ( wtimes ) ) \n 
round ( qtime1 , prec ) ) \n 
round ( cmean , prec ) , "+-" , round ( cstd , prec ) ) \n 
round ( wmean , prec ) , "+-" , round ( wstd , prec ) ) \n 
~~ def print_db_sizes ( self , init , filled , indexed ) : \n 
~~~ table_size = ( filled - init ) / 1024. \n 
indexes_size = ( indexed - filled ) / 1024. \n 
~~ def fill_arrays ( self , start , stop ) : \n 
~~~ arr_f8 = numpy . arange ( start , stop , dtype = ) \n 
arr_i4 = numpy . arange ( start , stop , dtype = ) \n 
if self . userandom : \n 
~~~ arr_f8 += numpy . random . normal ( 0 , stop * self . scale , \n 
size = stop - start ) \n 
arr_i4 = numpy . array ( arr_f8 , dtype = ) \n 
~~ return arr_i4 , arr_f8 \n 
~~ def create_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ self . con = self . open_db ( remove = 1 ) \n 
self . create_table ( self . con ) \n 
init_size = self . get_db_size ( ) \n 
t1 = time ( ) \n 
self . fill_table ( self . con ) \n 
table_size = self . get_db_size ( ) \n 
self . print_mtime ( t1 , ) \n 
self . index_db ( dtype , kind , optlevel , verbose ) \n 
indexes_size = self . get_db_size ( ) \n 
self . print_db_sizes ( init_size , table_size , indexes_size ) \n 
self . close_db ( self . con ) \n 
~~ def index_db ( self , dtype , kind , optlevel , verbose ) : \n 
~~~ if dtype == "int" : \n 
~~~ idx_cols = [ ] \n 
~~ elif dtype == "float" : \n 
~~~ idx_cols = [ , ] \n 
~~ for colname in idx_cols : \n 
~~~ t1 = time ( ) \n 
self . index_col ( self . con , colname , kind , optlevel , verbose ) \n 
self . print_mtime ( t1 , % colname ) \n 
~~ ~~ def query_db ( self , niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) : \n 
~~~ self . con = self . open_db ( ) \n 
if dtype == "int" : \n 
~~~ reg_cols = [ ] \n 
idx_cols = [ ] \n 
~~~ reg_cols = [ , ] \n 
idx_cols = [ , ] \n 
~~ if avoidfscache : \n 
~~~ rseed = int ( numpy . random . randint ( self . nrows ) ) \n 
~~~ rseed = 19 \n 
~~ numpy . random . seed ( rseed ) \n 
base = numpy . random . randint ( self . nrows ) \n 
if not onlyidxquery : \n 
~~~ for colname in reg_cols : \n 
~~~ ltimes = [ ] \n 
random . seed ( rseed ) \n 
for i in range ( NI_NTIMES ) : \n 
results = self . do_query ( self . con , colname , base , inkernel ) \n 
ltimes . append ( time ( ) - t1 ) \n 
~~ if verbose : \n 
~~ self . print_qtime ( colname , ltimes ) \n 
~~ self . close_db ( self . con ) \n 
self . con = self . open_db ( ) \n 
~~ if not onlynonidxquery : \n 
~~~ for colname in idx_cols : \n 
numpy . random . seed ( rseed ) \n 
rndbase = numpy . random . randint ( self . nrows , size = niter ) \n 
for i in range ( niter ) : \n 
~~~ base = rndbase [ i ] \n 
~~ self . print_qtime_idx ( colname , ltimes , False , verbose ) \n 
ltimes = [ ] \n 
~~ ~~ self . close_db ( self . con ) \n 
~~ def close_db ( self , con ) : \n 
~~~ con . close ( ) \n 
~~~ import sys \n 
import getopt \n 
~~~ import psyco \n 
psyco_imported = 1 \n 
~~~ psyco_imported = 0 \n 
~~~ opts , pargs = getopt . getopt ( \n 
sys . argv [ 1 : ] , ) \n 
~~~ sys . stderr . write ( usage ) \n 
~~ usepytables = 0 \n 
usepostgres = 0 \n 
verbose = 0 \n 
doprofile = 0 \n 
dokprofile = 0 \n 
usepsyco = 0 \n 
userandom = 0 \n 
docreate = 0 \n 
optlevel = 0 \n 
kind = "medium" \n 
docompress = 0 \n 
complib = "zlib" \n 
doquery = False \n 
onlyidxquery = False \n 
onlynonidxquery = False \n 
inkernel = True \n 
avoidfscache = 0 \n 
rng = [ - 1000 , - 1000 ] \n 
repeatquery = 0 \n 
repeatvalue = 0 \n 
krows = \n 
niter = READ_TIMES \n 
dtype = "all" \n 
datadir = "data.nobackup" \n 
for option in opts : \n 
~~~ if option [ 0 ] == : \n 
~~~ usepytables = 1 \n 
~~ elif option [ 0 ] == : \n 
~~~ usepostgres = 1 \n 
~~~ verbose = 1 \n 
~~~ doprofile = 1 \n 
~~~ dokprofile = 1 \n 
~~~ usepsyco = 1 \n 
~~~ userandom = 1 \n 
~~~ docreate = 1 \n 
~~~ doquery = True \n 
onlyidxquery = True \n 
onlynonidxquery = True \n 
inkernel = False \n 
~~~ avoidfscache = 1 \n 
~~~ docompress = int ( option [ 1 ] ) \n 
~~~ complib = option [ 1 ] \n 
~~~ rng = [ int ( i ) for i in option [ 1 ] . split ( "," ) ] \n 
~~~ niter = int ( option [ 1 ] ) \n 
~~~ krows = option [ 1 ] \n 
~~~ datadir = option [ 1 ] \n 
~~~ optlevel = int ( option [ 1 ] ) \n 
~~~ if option [ 1 ] in ( , , , ) : \n 
~~~ kind = option [ 1 ] \n 
"\'ultralight\'" ) \n 
~~ ~~ elif option [ 0 ] == : \n 
~~~ if option [ 1 ] in ( , ) : \n 
~~~ dtype = option [ 1 ] \n 
~~~ repeatquery = 1 \n 
repeatvalue = int ( option [ 1 ] ) \n 
~~ ~~ if not usepytables and not usepostgres : \n 
~~ if usepytables : \n 
~~~ from pytables_backend import PyTables_DB \n 
db = PyTables_DB ( krows , rng , userandom , datadir , \n 
docompress , complib , kind , optlevel ) \n 
~~ elif usepostgres : \n 
~~~ from postgres_backend import Postgres_DB \n 
db = Postgres_DB ( krows , rng , userandom ) \n 
~~ if not avoidfscache : \n 
~~~ numpy . random . seed ( 20 ) \n 
~~~ if userandom : \n 
~~ if onlyidxquery : \n 
~~ ~~ if psyco_imported and usepsyco : \n 
~~~ psyco . bind ( db . create_db ) \n 
psyco . bind ( db . query_db ) \n 
~~ if docreate : \n 
~~~ if verbose : \n 
~~ db . create_db ( dtype , kind , optlevel , verbose ) \n 
~~ if doquery : \n 
if doprofile : \n 
~~~ import pstats \n 
import cProfile as prof \n 
prof . run ( \n 
stats = pstats . Stats ( ) \n 
stats . strip_dirs ( ) \n 
stats . sort_stats ( , ) \n 
~~~ stats . print_stats ( ) \n 
~~~ stats . print_stats ( 20 ) \n 
~~ ~~ elif dokprofile : \n 
~~~ from cProfile import Profile \n 
import lsprofcalltree \n 
prof = Profile ( ) \n 
kcg = lsprofcalltree . KCacheGrind ( prof ) \n 
ofile = open ( , ) \n 
kcg . output ( ofile ) \n 
ofile . close ( ) \n 
~~ elif doprofile : \n 
~~~ import hotshot \n 
import hotshot . stats \n 
prof = hotshot . Profile ( "indexed_search.prof" ) \n 
benchtime , stones = prof . run ( \n 
prof . close ( ) \n 
stats = hotshot . stats . load ( "indexed_search.prof" ) \n 
stats . print_stats ( 20 ) \n 
~~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
avoidfscache , verbose , inkernel ) \n 
~~ ~~ if repeatquery : \n 
~~~ db . rng = [ 1 , 1 ] \n 
~~~ print ( "range:" , db . rng ) \n 
~~ db . query_db ( niter , dtype , onlyidxquery , onlynonidxquery , \n 
for i in range ( repeatvalue ) : \n 
~~~ for j in ( 1 , 2 , 5 ) : \n 
~~~ rng = j * 10 ** i \n 
db . rng = [ - rng / 2 , rng / 2 ] \n 
import matplotlib as mpl \n 
from pylab import * \n 
KB_ = 1024 \n 
MB_ = 1024 * KB_ \n 
GB_ = 1024 * MB_ \n 
linewidth = 2 \n 
markers = [ , , , , , , , , , ] \n 
markersize = 8 \n 
def get_values ( filename ) : \n 
~~~ f = open ( filename ) \n 
values = { "memcpyw" : [ ] , "memcpyr" : [ ] } \n 
for line in f : \n 
~~~ tmp = line . split ( ) [ 1 ] \n 
nthreads , size , elsize , sbits , codec , shuffle = [ i for i in tmp . split ( ) ] \n 
nthreads , size , elsize , sbits = map ( int , ( nthreads , size , elsize , sbits ) ) \n 
values [ "size" ] = size * NCHUNKS / MB_ ; \n 
values [ "elsize" ] = elsize ; \n 
values [ "sbits" ] = sbits ; \n 
values [ "codec" ] = codec \n 
values [ "shuffle" ] = shuffle \n 
( ratios , speedsw , speedsr ) = ( [ ] , [ ] , [ ] ) \n 
values [ nthreads ] = ( ratios , speedsw , speedsr ) \n 
~~ elif line . startswith ( ) : \n 
memcpyw = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyw" ] . append ( memcpyw ) \n 
memcpyr = float ( tmp . split ( ) [ 1 ] ) \n 
values [ "memcpyr" ] . append ( memcpyr ) \n 
speedw = float ( tmp . split ( ) [ 1 ] ) \n 
ratio = float ( line . split ( ) [ - 1 ] ) \n 
speedsw . append ( speedw ) \n 
ratios . append ( ratio ) \n 
speedr = float ( tmp . split ( ) [ 1 ] ) \n 
speedsr . append ( speedr ) \n 
if "OK" not in line : \n 
~~ ~~ ~~ f . close ( ) \n 
return nthreads , values \n 
~~ def show_plot ( plots , yaxis , legends , gtitle , xmax = None ) : \n 
~~~ xlabel ( ) \n 
ylabel ( ) \n 
title ( gtitle ) \n 
xlim ( 0 , xmax ) \n 
ylim ( 0 , None ) \n 
grid ( True ) \n 
legend ( [ p [ 0 ] for p in plots \n 
if not isinstance ( p , mpl . lines . Line2D ) ] , \n 
legends , loc = "best" ) \n 
if outfile : \n 
savefig ( outfile , dpi = 64 ) \n 
~~~ show ( ) \n 
~~ ~~ if __name__ == : \n 
~~~ from optparse import OptionParser \n 
compress_title = \n 
decompress_title = \n 
yaxis = \n 
parser = OptionParser ( usage = usage ) \n 
parser . add_option ( , \n 
dest = , \n 
help = ( \n 
help = , ) \n 
help = , \n 
default = None ) \n 
parser . add_option ( , , action = , \n 
default = False ) \n 
( options , args ) = parser . parse_args ( ) \n 
if len ( args ) == 0 : \n 
~~ elif len ( args ) > 1 : \n 
~~ if options . report and options . outfile : \n 
~~ if options . dspeed and options . cspeed : \n 
~~ elif options . cspeed : \n 
~~~ options . dspeed = False \n 
plot_title = compress_title \n 
~~~ options . dspeed = True \n 
plot_title = decompress_title \n 
~~ filename = args [ 0 ] \n 
cspeed = options . cspeed \n 
dspeed = options . dspeed \n 
if options . outfile : \n 
~~~ outfile = options . outfile \n 
~~ elif options . report : \n 
~~~ if cspeed : \n 
~~~ outfile = filename [ : filename . rindex ( ) ] + \n 
~~~ outfile = None \n 
~~ plots = [ ] \n 
legends = [ ] \n 
nthreads , values = get_values ( filename ) \n 
if options . limit : \n 
~~~ thread_range = eval ( options . limit ) \n 
~~~ thread_range = range ( 1 , nthreads + 1 ) \n 
~~ if options . title : \n 
~~~ plot_title = options . title \n 
~~ gtitle = plot_title \n 
for nt in thread_range : \n 
~~~ ( ratios , speedw , speedr ) = values [ nt ] \n 
if cspeed : \n 
~~~ speed = speedw \n 
~~~ speed = speedr \n 
~~ plot_ = plot ( ratios , speed , linewidth = 2 ) \n 
plots . append ( plot_ ) \n 
nmarker = nt \n 
if nt >= len ( markers ) : \n 
~~~ nmarker = nt % len ( markers ) \n 
~~ setp ( plot_ , marker = markers [ nmarker ] , markersize = markersize , \n 
linewidth = linewidth ) \n 
~~ if cspeed : \n 
~~~ mean = np . mean ( values [ "memcpyw" ] ) \n 
~~~ mean = np . mean ( values [ "memcpyr" ] ) \n 
~~ plot_ = axhline ( mean , linewidth = 3 , linestyle = , color = ) \n 
text ( 1.0 , mean + 50 , message ) \n 
show_plot ( plots , yaxis , legends , gtitle , xmax = int ( options . xmax ) if \n 
options . xmax else None ) \n 
import tables \n 
fileh = tables . open_file ( "attributes1.h5" , mode = "w" , \n 
root = fileh . root \n 
a = np . array ( [ 1 , 2 , 4 ] , np . int32 ) \n 
hdfarray . attrs . char = "1" \n 
hdfarray . attrs . int = 12 \n 
hdfarray . attrs . float = 12.32 \n 
hdfarray . attrs . object = { "a" : 32.1 , "b" : 1 , "c" : [ 1 , 2 ] } \n 
fileh . close ( ) \n 
def setUp ( filename ) : \n 
fileh . enable_undo ( ) \n 
return fileh \n 
~~ def tearDown ( fileh ) : \n 
~~~ fileh . disable_undo ( ) \n 
~~ def demo_6times3marks ( ) : \n 
fileh = setUp ( "undo-redo-6times3marks.h5" ) \n 
fileh . mark ( ) \n 
fileh . undo ( ) \n 
assert "/otherarray1" in fileh \n 
assert "/otherarray2" in fileh \n 
assert "/otherarray3" in fileh \n 
assert "/otherarray4" in fileh \n 
assert "/otherarray5" not in fileh \n 
assert "/otherarray6" not in fileh \n 
assert "/otherarray3" not in fileh \n 
assert "/otherarray4" not in fileh \n 
assert "/otherarray1" not in fileh \n 
assert "/otherarray2" not in fileh \n 
fileh . redo ( ) \n 
assert "/otherarray5" in fileh \n 
assert "/otherarray6" in fileh \n 
tearDown ( fileh ) \n 
~~ def demo_manyops ( ) : \n 
fileh = setUp ( "undo-redo-manyops.h5" ) \n 
new_node = fileh . copy_node ( , ) \n 
new_node = fileh . copy_children ( , , recursive = 1 ) \n 
fileh . rename_node ( , ) \n 
fileh . remove_node ( ) \n 
assert not in fileh \n 
assert in fileh \n 
assert fileh . root . agroup . anarray3 is new_node \n 
~~ if __name__ == : \n 
~~~ demo_6times3marks ( ) \n 
demo_manyops ( ) \n 
######################################################################## \n 
import functools \n 
from . registry import class_name_dict , class_id_dict \n 
from . exceptions import ( ClosedNodeError , NodeError , UndoRedoWarning , \n 
PerformanceWarning ) \n 
from . path import join_path , split_path , isvisiblepath \n 
from . utils import lazyattr \n 
from . undoredo import move_to_shadow \n 
from . attributeset import AttributeSet , NotLoggedAttributeSet \n 
__docformat__ = \n 
def _closedrepr ( oldmethod ) : \n 
@ functools . wraps ( oldmethod ) \n 
def newmethod ( self ) : \n 
~~~ if not self . _v_isopen : \n 
~~~ cmod = self . __class__ . __module__ \n 
cname = self . __class__ . __name__ \n 
addr = hex ( id ( self ) ) \n 
return % ( cmod , cname , addr ) \n 
~~ return oldmethod ( self ) \n 
~~ return newmethod \n 
~~ class MetaNode ( type ) : \n 
def __new__ ( class_ , name , bases , dict_ ) : \n 
~~~ for mname in [ , ] : \n 
~~~ if mname in dict_ : \n 
~~~ dict_ [ mname ] = _closedrepr ( dict_ [ mname ] ) \n 
~~ ~~ return type . __new__ ( class_ , name , bases , dict_ ) \n 
~~ def __init__ ( class_ , name , bases , dict_ ) : \n 
~~~ super ( MetaNode , class_ ) . __init__ ( name , bases , dict_ ) \n 
class_name_dict [ class_ . __name__ ] = class_ \n 
cid = getattr ( class_ , , None ) \n 
if cid is not None : \n 
~~~ for base in bases : \n 
~~~ pcid = getattr ( base , , None ) \n 
if pcid == cid : \n 
~~~ class_id_dict [ cid ] = class_ \n 
~~ ~~ ~~ ~~ class Node ( six . with_metaclass ( MetaNode , object ) ) : \n 
_AttributeSet = AttributeSet \n 
def _g_getparent ( self ) : \n 
( parentpath , nodename ) = split_path ( self . _v_pathname ) \n 
return self . _v_file . _get_node ( parentpath ) \n 
~~ _v_parent = property ( _g_getparent ) \n 
@ lazyattr \n 
def _v_attrs ( self ) : \n 
return self . _AttributeSet ( self ) \n 
~~ def _g_gettitle ( self ) : \n 
if hasattr ( self . _v_attrs , ) : \n 
~~~ return self . _v_attrs . TITLE \n 
~~ ~~ def _g_settitle ( self , title ) : \n 
~~~ self . _v_attrs . TITLE = title \n 
~~ _v_title = property ( _g_gettitle , _g_settitle ) \n 
_v_isopen = False \n 
def __init__ ( self , parentnode , name , _log = True ) : \n 
~~~ if isinstance ( parentnode , class_name_dict [ ] ) : \n 
~~~ parentnode = parentnode . dereference ( ) \n 
~~ self . _v_file = None \n 
self . _v_isopen = False \n 
self . _v_pathname = None \n 
self . _v_name = None \n 
self . _v_depth = None \n 
self . _v_maxtreedepth = parentnode . _v_file . params [ ] \n 
self . _v__deleting = False \n 
self . _v_objectid = None \n 
self . _g_check_group ( parentnode ) \n 
parentnode . _g_check_open ( ) \n 
file_ = parentnode . _v_file \n 
if new : \n 
~~~ file_ . _check_writable ( ) \n 
~~ if new : \n 
~~~ parentnode . _g_refnode ( self , name , validate ) \n 
~~ self . _g_set_location ( parentnode , name ) \n 
~~~ self . _g_new ( parentnode , name , init = True ) \n 
~~~ self . _v_objectid = self . _g_create ( ) \n 
~~~ self . _v_objectid = self . _g_open ( ) \n 
~~ if new and _log and file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_log_create ( ) \n 
~~ self . _g_post_init_hook ( ) \n 
~~~ self . _f_close ( ) \n 
raise \n 
~~ ~~ def _g_log_create ( self ) : \n 
~~~ self . _v_file . _log ( , self . _v_pathname ) \n 
~~ def __del__ ( self ) : \n 
~~ self . _v__deleting = True \n 
~~~ node_manager = self . _v_file . _node_manager \n 
node_manager . drop_node ( self , check_unregistered = False ) \n 
~~~ if self . _v_isopen : \n 
~~~ self . _v__deleting = True \n 
self . _f_close ( ) \n 
~~ ~~ ~~ def _g_pre_kill_hook ( self ) : \n 
~~ def _g_create ( self ) : \n 
~~ def _g_open ( self ) : \n 
~~ def _g_check_open ( self ) : \n 
if not self . _v_isopen : \n 
~~ def _g_set_location ( self , parentnode , name ) : \n 
parentdepth = parentnode . _v_depth \n 
self . _v_file = file_ \n 
self . _v_isopen = True \n 
root_uep = file_ . root_uep \n 
if name . startswith ( root_uep ) : \n 
~~~ assert parentdepth == 0 \n 
if root_uep == "/" : \n 
~~~ self . _v_pathname = name \n 
~~~ self . _v_pathname = name [ len ( root_uep ) : ] \n 
~~ _ , self . _v_name = split_path ( name ) \n 
self . _v_depth = name . count ( "/" ) - root_uep . count ( "/" ) + 1 \n 
~~~ self . _v_name = name \n 
self . _v_pathname = join_path ( parentnode . _v_pathname , name ) \n 
self . _v_depth = parentdepth + 1 \n 
~~ if parentdepth >= self . _v_maxtreedepth : \n 
% ( self . _v_pathname , self . _v_maxtreedepth ) , \n 
~~ if self . _v_pathname != : \n 
~~~ file_ . _node_manager . cache_node ( self , self . _v_pathname ) \n 
~~ ~~ def _g_update_location ( self , newparentpath ) : \n 
oldpath = self . _v_pathname \n 
newpath = join_path ( newparentpath , self . _v_name ) \n 
newdepth = newpath . count ( ) \n 
self . _v_pathname = newpath \n 
self . _v_depth = newdepth \n 
if newdepth > self . _v_maxtreedepth : \n 
% ( self . _v_maxtreedepth , ) , PerformanceWarning ) \n 
~~ node_manager = self . _v_file . _node_manager \n 
node_manager . rename_node ( oldpath , newpath ) \n 
self . _g_update_dependent ( ) \n 
~~ def _g_del_location ( self ) : \n 
node_manager = self . _v_file . _node_manager \n 
pathname = self . _v_pathname \n 
if not self . _v__deleting : \n 
~~~ node_manager . drop_from_cache ( pathname ) \n 
node_manager . registry . pop ( pathname , None ) \n 
~~ def _g_post_init_hook ( self ) : \n 
~~ def _g_update_dependent ( self ) : \n 
if in self . __dict__ : \n 
~~~ self . _v_attrs . _g_update_node_location ( self ) \n 
~~ ~~ def _f_close ( self ) : \n 
~~ myDict = self . __dict__ \n 
if in myDict : \n 
~~~ self . _v_attrs . _g_close ( ) \n 
~~ self . _g_del_location ( ) \n 
myDict . clear ( ) \n 
~~ def _g_remove ( self , recursive , force ) : \n 
parent = self . _v_parent \n 
parent . _g_unrefnode ( self . _v_name ) \n 
self . _g_delete ( parent ) \n 
~~ def _f_remove ( self , recursive = False , force = False ) : \n 
self . _g_check_open ( ) \n 
file_ = self . _v_file \n 
file_ . _check_writable ( ) \n 
if file_ . is_undo_enabled ( ) : \n 
~~~ self . _g_remove_and_log ( recursive , force ) \n 
~~~ self . _g_remove ( recursive , force ) \n 
~~ ~~ def _g_remove_and_log ( self , recursive , force ) : \n 
~~~ file_ = self . _v_file \n 
oldpathname = self . _v_pathname \n 
file_ . _log ( , oldpathname ) \n 
move_to_shadow ( file_ , oldpathname ) \n 
~~ def _g_move ( self , newparent , newname ) : \n 
oldparent = self . _v_parent \n 
oldname = self . _v_name \n 
newparent . _g_refnode ( self , newname ) \n 
oldparent . _g_unrefnode ( oldname ) \n 
self . _g_del_location ( ) \n 
self . _g_set_location ( newparent , newname ) \n 
self . _g_new ( newparent , self . _v_name , init = False ) \n 
self . _v_parent . _g_move_node ( oldparent . _v_objectid , oldname , \n 
newparent . _v_objectid , newname , \n 
oldpathname , self . _v_pathname ) \n 
~~ def _f_rename ( self , newname , overwrite = False ) : \n 
self . _f_move ( newname = newname , overwrite = overwrite ) \n 
~~ def _f_move ( self , newparent = None , newname = None , \n 
overwrite = False , createparents = False ) : \n 
if newparent is None and newname is None : \n 
~~ if newparent is None : \n 
~~~ newparent = oldparent \n 
~~ if newname is None : \n 
~~~ newname = oldname \n 
~~~ newfile = newparent . _v_file \n 
newpath = newparent . _v_pathname \n 
~~~ newfile = file_ \n 
newpath = newparent \n 
% ( newparent , ) ) \n 
~~ if newfile is not file_ : \n 
~~ file_ . _check_writable ( ) \n 
oldpath = oldparent . _v_pathname \n 
if newpath == oldpath and newname == oldname : \n 
~~ self . _g_check_not_contains ( newpath ) \n 
newparent = file_ . _get_or_create_path ( newparent , createparents ) \n 
self . _g_maybe_remove ( newparent , newname , overwrite ) \n 
self . _g_move ( newparent , newname ) \n 
~~~ self . _g_log_move ( oldpathname ) \n 
~~ ~~ def _g_log_move ( self , oldpathname ) : \n 
~~~ self . _v_file . _log ( , oldpathname , self . _v_pathname ) \n 
~~ def _g_copy ( self , newparent , newname , recursive , _log = True , ** kwargs ) : \n 
~~ def _g_copy_as_child ( self , newparent , ** kwargs ) : \n 
return self . _g_copy ( newparent , self . _v_name , \n 
recursive = False , _log = False , ** kwargs ) \n 
~~ def _f_copy ( self , newparent = None , newname = None , \n 
overwrite = False , recursive = False , createparents = False , \n 
srcfile = self . _v_file \n 
srcparent = self . _v_parent \n 
srcname = self . _v_name \n 
dstparent = newparent \n 
dstname = newname \n 
if dstparent is None and dstname is None : \n 
~~ if dstparent is None : \n 
~~~ dstparent = srcparent \n 
~~ if dstname is None : \n 
~~~ dstname = srcname \n 
~~~ dstfile = dstparent . _v_file \n 
dstpath = dstparent . _v_pathname \n 
~~~ dstfile = srcfile \n 
dstpath = dstparent \n 
% ( dstparent , ) ) \n 
~~ if dstfile is srcfile : \n 
~~~ srcpath = srcparent . _v_pathname \n 
if dstpath == srcpath and dstname == srcname : \n 
~~~ raise NodeError ( \n 
% self . _v_pathname ) \n 
~~ if recursive : \n 
~~~ self . _g_check_not_contains ( dstpath ) \n 
~~ ~~ dstparent = srcfile . _get_or_create_path ( dstparent , createparents ) \n 
if dstfile is not srcfile and srcfile . is_undo_enabled ( ) : \n 
UndoRedoWarning ) \n 
~~ self . _g_maybe_remove ( dstparent , dstname , overwrite ) \n 
return self . _g_copy ( dstparent , dstname , recursive , ** kwargs ) \n 
~~ def _f_isvisible ( self ) : \n 
return isvisiblepath ( self . _v_pathname ) \n 
~~ def _g_check_group ( self , node ) : \n 
~~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
% node . _v_pathname ) \n 
~~ if not isinstance ( node , class_name_dict [ ] ) : \n 
~~ ~~ def _g_check_not_contains ( self , pathname ) : \n 
~~~ mypathname = self . _v_pathname \n 
or pathname == mypathname \n 
or pathname . startswith ( mypathname + ) ) : \n 
~~ ~~ def _g_maybe_remove ( self , parent , name , overwrite ) : \n 
~~~ if name in parent : \n 
~~~ if not overwrite : \n 
~~ parent . _f_get_child ( name ) . _f_remove ( True ) \n 
~~ ~~ def _g_check_name ( self , name ) : \n 
if name . startswith ( ) : \n 
~~ ~~ def _f_getattr ( self , name ) : \n 
return getattr ( self . _v_attrs , name ) \n 
~~ def _f_setattr ( self , name , value ) : \n 
setattr ( self . _v_attrs , name , value ) \n 
~~ def _f_delattr ( self , name ) : \n 
delattr ( self . _v_attrs , name ) \n 
~~ ~~ class NotLoggedMixin : \n 
~~~ _AttributeSet = NotLoggedAttributeSet \n 
def _g_log_create ( self ) : \n 
~~ def _g_log_move ( self , oldpathname ) : \n 
~~ def _g_remove_and_log ( self , recursive , force ) : \n 
~~ ~~ from __future__ import print_function \n 
from tables import IsDescription , StringCol , BoolCol , IntCol , FloatCol \n 
from tables . node import NotLoggedMixin \n 
from tables . path import join_path \n 
from tables . tests import common \n 
from tables . tests . common import unittest \n 
from tables . tests . common import PyTablesTestCase as TestCase \n 
class BasicTestCase ( common . TempFileMixin , TestCase ) : \n 
_reopen_flag = False \n 
def _do_reopen ( self ) : \n 
~~~ if self . _reopen_flag : \n 
~~~ self . _reopen ( ) \n 
~~ ~~ def setUp ( self ) : \n 
~~~ super ( BasicTestCase , self ) . setUp ( ) \n 
h5file = self . h5file \n 
root = h5file . root \n 
~~ def test00_simple ( self ) : \n 
if common . verbose : \n 
~~~ print ( , * 30 ) \n 
~~ self . h5file . enable_undo ( ) \n 
self . h5file . undo ( ) \n 
self . assertTrue ( "/otherarray" not in self . h5file ) \n 
self . assertEqual ( self . h5file . _curaction , 0 ) \n 
self . assertEqual ( self . h5file . _curmark , 0 ) \n 
self . _do_reopen ( ) \n 
self . h5file . redo ( ) \n 
~~ self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . _curaction , 1 ) \n 
~~ def test01_twice ( self ) : \n 
self . assertTrue ( "/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/otherarray" in self . h5file ) \n 
self . assertTrue ( "/otherarray2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 4 , 5 ] ) \n 
self . assertEqual ( self . h5file . _curaction , 2 ) \n 
~~ def test02_twice2 ( self ) : \n 
self . h5file . mark ( ) \n 
self . assertEqual ( self . h5file . _curaction , 3 ) \n 
self . assertEqual ( self . h5file . _curmark , 1 ) \n 
~~ def test03_6times3marks ( self ) : \n 
self . __class__ . __name__ ) \n 
self . assertTrue ( "/otherarray1" in self . h5file ) \n 
self . assertTrue ( "/otherarray3" in self . h5file ) \n 
self . assertTrue ( "/otherarray4" in self . h5file ) \n 
self . assertTrue ( "/otherarray5" not in self . h5file ) \n 
self . assertTrue ( "/otherarray6" not in self . h5file ) \n 
self . assertTrue ( "/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/otherarray4" not in self . h5file ) \n 
self . assertTrue ( "/otherarray1" not in self . h5file ) \n 
self . assertTrue ( "/otherarray5" in self . h5file ) \n 
self . assertTrue ( "/otherarray6" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 3 , 4 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 5 , 6 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray4 . read ( ) , [ 6 , 7 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray5 . read ( ) , [ 7 , 8 ] ) \n 
self . assertEqual ( self . h5file . root . otherarray6 . read ( ) , [ 8 , 9 ] ) \n 
~~ def test04_6times3marksro ( self ) : \n 
~~ self . h5file . mark ( ) \n 
~~ def test05_destructive ( self ) : \n 
~~ def test05b_destructive ( self ) : \n 
~~ def test05c_destructive ( self ) : \n 
~~ def test05d_destructive ( self ) : \n 
self . h5file . undo ( 0 ) \n 
~~ def test05e_destructive ( self ) : \n 
~~ def test05f_destructive ( self ) : \n 
self . h5file . create_array ( , , [ 1 ] ) \n 
self . assertTrue ( not in self . h5file ) \n 
newarr = self . h5file . create_array ( , , [ 1 ] ) \n 
self . assertTrue ( in self . h5file ) \n 
if not self . _reopen_flag : \n 
~~~ self . assertTrue ( self . h5file . root . newarray is newarr ) \n 
~~ ~~ def test06_totalunwind ( self ) : \n 
~~ def test07_totalrewind ( self ) : \n 
self . h5file . redo ( - 1 ) \n 
~~ def test08_marknames ( self ) : \n 
self . h5file . mark ( "first" ) \n 
self . h5file . mark ( "second" ) \n 
self . h5file . mark ( "third" ) \n 
self . h5file . undo ( "first" ) \n 
self . h5file . redo ( "third" ) \n 
self . h5file . undo ( "second" ) \n 
~~ def test08_initialmark ( self ) : \n 
initmid = self . h5file . get_current_mark ( ) \n 
self . h5file . undo ( initmid ) \n 
~~ def test09_marknames ( self ) : \n 
with self . assertRaises ( tables . UndoRedoError ) : \n 
~~~ self . h5file . undo ( "third" ) \n 
~~ self . h5file . redo ( "third" ) \n 
~~~ self . h5file . redo ( "second" ) \n 
~~ self . assertTrue ( "/otherarray1" in self . h5file ) \n 
~~ def test10_goto ( self ) : \n 
self . h5file . goto ( "first" ) \n 
self . h5file . goto ( "third" ) \n 
self . h5file . goto ( "second" ) \n 
self . h5file . goto ( - 1 ) \n 
~~ def test10_gotoint ( self ) : \n 
self . h5file . goto ( 1 ) \n 
self . h5file . goto ( 0 ) \n 
self . h5file . goto ( 3 ) \n 
self . h5file . goto ( 2 ) \n 
~~ def test11_contiguous ( self ) : \n 
m1 = self . h5file . mark ( ) \n 
m2 = self . h5file . mark ( ) \n 
self . assertNotEqual ( m1 , m2 ) \n 
self . h5file . undo ( m1 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m1 ) \n 
self . h5file . redo ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , m2 ) \n 
self . h5file . goto ( m1 ) \n 
self . h5file . goto ( m2 ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , 0 ) \n 
~~ def test12_keepMark ( self ) : \n 
mid = self . h5file . mark ( ) \n 
self . assertTrue ( mid is not None ) \n 
~~ def test13_severalEnableDisable ( self ) : \n 
self . h5file . disable_undo ( ) \n 
self . h5file . enable_undo ( ) \n 
self . assertEqual ( self . h5file . get_current_mark ( ) , mid ) \n 
~~ ~~ class PersistenceTestCase ( BasicTestCase ) : \n 
_reopen_flag = True \n 
~~ class CreateArrayTestCase ( common . TempFileMixin , TestCase ) : \n 
def setUp ( self ) : \n 
~~~ super ( CreateArrayTestCase , self ) . setUp ( ) \n 
~~ def test00 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray1 . read ( ) , [ 1 , 2 ] ) \n 
~~ def test01 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
~~ def test02 ( self ) : \n 
self . assertEqual ( self . h5file . root . otherarray3 . read ( ) , [ 3 , 4 ] ) \n 
~~ def test03 ( self ) : \n 
self . h5file . create_array ( , , \n 
self . assertTrue ( "/agroup/otherarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" not in self . h5file ) \n 
self . assertTrue ( "/agroup/otherarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/otherarray3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . title , \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . title , \n 
self . assertEqual ( self . h5file . root . agroup . otherarray2 . read ( ) , [ 2 , 3 ] ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . otherarray3 . read ( ) , \n 
[ 3 , 4 ] ) \n 
~~ ~~ class CreateGroupTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CreateGroupTestCase , self ) . setUp ( ) \n 
self . assertTrue ( "/othergroup1" not in self . h5file ) \n 
self . assertTrue ( "/othergroup1" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . _v_title , \n 
self . assertTrue ( "/othergroup2" not in self . h5file ) \n 
self . assertTrue ( "/othergroup2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup2 . _v_title , \n 
self . assertTrue ( "/othergroup3" not in self . h5file ) \n 
self . assertTrue ( "/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup3 . _v_title , \n 
self . h5file . create_group ( \n 
self . assertTrue ( "/othergroup1/othergroup2" not in self . h5file ) \n 
self . assertTrue ( \n 
"/othergroup1/othergroup2/othergroup3" not in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2" in self . h5file ) \n 
self . assertTrue ( "/othergroup1/othergroup2/othergroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . othergroup1 . othergroup2 . _v_title , \n 
self . assertEqual ( \n 
self . h5file . root . othergroup1 . othergroup2 . othergroup3 . _v_title , \n 
~~ ~~ minRowIndex = 10 \n 
def populateTable ( where , name ) : \n 
class Indexed ( IsDescription ) : \n 
~~~ var1 = StringCol ( itemsize = 4 , dflt = b"" , pos = 1 ) \n 
var2 = BoolCol ( dflt = 0 , pos = 2 ) \n 
var3 = IntCol ( dflt = 0 , pos = 3 ) \n 
var4 = FloatCol ( dflt = 0 , pos = 4 ) \n 
~~ nrows = minRowIndex \n 
table = where . _v_file . create_table ( where , name , Indexed , "Indexed" , \n 
None , nrows ) \n 
for i in range ( nrows ) : \n 
~~~ table . row [ ] = str ( i ) \n 
table . row [ ] = i % 2 \n 
table . row [ ] = i \n 
table . row [ ] = float ( nrows - i - 1 ) \n 
table . row . append ( ) \n 
~~ table . flush ( ) \n 
indexrows = table . cols . var1 . create_index ( ) \n 
indexrows = table . cols . var2 . create_index ( ) \n 
indexrows = table . cols . var3 . create_index ( ) \n 
~~ ~~ class RenameNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( RenameNodeTestCase , self ) . setUp ( ) \n 
populateTable ( self . h5file . root , ) \n 
self . h5file . rename_node ( , ) \n 
self . assertTrue ( "/agroup2" in self . h5file ) \n 
self . assertTrue ( "/agroup3" not in self . h5file ) \n 
self . assertTrue ( "/agroup2" not in self . h5file ) \n 
self . assertTrue ( "/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup" not in self . h5file ) \n 
self . assertTrue ( "/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup3/agroup3" in self . h5file ) \n 
~~ def test01b ( self ) : \n 
self . assertTrue ( "/agroup4" not in self . h5file ) \n 
self . assertTrue ( "/agroup4" in self . h5file ) \n 
self . assertTrue ( "/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup4/agroup3" in self . h5file ) \n 
self . assertTrue ( "/anarray" in self . h5file ) \n 
self . assertTrue ( "/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/anarray" not in self . h5file ) \n 
self . assertTrue ( "/anarray2" in self . h5file ) \n 
self . assertTrue ( "/table" in self . h5file ) \n 
table = self . h5file . root . table \n 
self . assertTrue ( table . cols . var1 . index is not None ) \n 
self . assertTrue ( table . cols . var2 . index is not None ) \n 
self . assertTrue ( table . cols . var3 . index is not None ) \n 
self . assertTrue ( table . cols . var4 . index is None ) \n 
self . assertEqual ( table . cols . var1 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var2 . index . nelements , minRowIndex ) \n 
self . assertEqual ( table . cols . var3 . index . nelements , minRowIndex ) \n 
self . assertTrue ( "/table2" not in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table . title , "Indexed" ) \n 
self . assertTrue ( "/table" not in self . h5file ) \n 
self . assertTrue ( "/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . table2 . title , "Indexed" ) \n 
table = self . h5file . root . table2 \n 
~~ ~~ class MoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( MoveNodeTestCase , self ) . setUp ( ) \n 
self . h5file . move_node ( , ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3/anarray" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup . agroup3 . anarray . title , \n 
self . h5file . move_node ( , , ) \n 
self . assertTrue ( "/agroup2/agroup3" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup3 . _v_title , \n 
self . assertTrue ( "/agroup2/agroup3/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup3/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup4 . _v_title , \n 
self . assertTrue ( "/agroup2/agroup4/anarray1" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/agroup4/agroup3" in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/anarray2" in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" not in self . h5file ) \n 
self . assertTrue ( "/agroup2/table2" in self . h5file ) \n 
self . assertEqual ( self . h5file . root . agroup2 . table2 . title , "Indexed" ) \n 
table = self . h5file . root . agroup2 . table2 \n 
~~ ~~ class RemoveNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( RemoveNodeTestCase , self ) . setUp ( ) \n 
self . h5file . remove_node ( ) \n 
~~ def test00b ( self ) : \n 
self . assertTrue ( "/agroup/anarray2" not in self . h5file ) \n 
~~ def test00c ( self ) : \n 
self . h5file . remove_node ( , recursive = 1 ) \n 
self . assertTrue ( "/agroup/anarray1" not in self . h5file ) \n 
self . assertTrue ( "/agroup/agroup3" not in self . h5file ) \n 
~~ ~~ class CopyNodeTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CopyNodeTestCase , self ) . setUp ( ) \n 
~~ def test00_copyLeaf ( self ) : \n 
new_node = self . h5file . copy_node ( , ) \n 
self . assertTrue ( self . h5file . root . agroup . agroup3 . anarray is new_node ) \n 
~~ def test00b_copyTable ( self ) : \n 
warnings . filterwarnings ( "ignore" , category = UserWarning ) \n 
table = self . h5file . copy_node ( \n 
, , propindexes = True ) \n 
warnings . filterwarnings ( "default" , category = UserWarning ) \n 
self . assertTrue ( "/agroup/agroup3/table" in self . h5file ) \n 
table = self . h5file . root . agroup . agroup3 . table \n 
self . assertEqual ( table . title , "Indexed" ) \n 
self . assertTrue ( "/agroup/agroup3/table" not in self . h5file ) \n 
~~ def test01_copyGroup ( self ) : \n 
new_node = self . h5file . copy_node ( \n 
, newname = , recursive = True ) \n 
self . assertTrue ( self . h5file . root . acopy is new_node ) \n 
~~ def test02_copyLeafOverwrite ( self ) : \n 
oldNode = self . h5file . root . agroup \n 
, newname = , overwrite = True ) \n 
self . assertTrue ( self . h5file . root . agroup is oldNode ) \n 
self . assertTrue ( self . h5file . root . agroup is new_node ) \n 
~~ def test03_copyChildren ( self ) : \n 
self . h5file . copy_children ( , , recursive = True ) \n 
~~ ~~ class ComplexTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( ComplexTestCase , self ) . setUp ( ) \n 
self . h5file . create_array ( self . h5file . root , , \n 
new_node = self . h5file . copy_children ( \n 
, , recursive = 1 ) \n 
self . assertTrue ( self . h5file . root . agroup . anarray3 is new_node ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 1 ] ) \n 
self . assertEqual ( self . h5file . root . anarray [ : ] , [ 4 ] ) \n 
self . h5file . create_group ( self . h5file . root . agroup2 , , \n 
self . assertEqual ( self . h5file . root . agroup2 . agroup5 . _v_title , \n 
self . h5file . create_group ( self . h5file . root . agroup , , \n 
~~ def test03b ( self ) : \n 
self . h5file . create_group ( self . h5file . root . agroup3 , , \n 
~~ ~~ class AttributesTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( AttributesTestCase , self ) . setUp ( ) \n 
array = self . h5file . create_array ( , , [ 1 , 2 ] ) \n 
attrs = array . attrs \n 
attrs . attr_1 = 10 \n 
attrs . attr_2 = 20 \n 
attrs . attr_3 = 30 \n 
~~ def test00_setAttr ( self ) : \n 
~~ array = self . h5file . root . array \n 
setattr ( attrs , , 0 ) \n 
self . assertTrue ( in attrs ) \n 
self . assertEqual ( attrs . attr_0 , 0 ) \n 
self . assertTrue ( not in attrs ) \n 
~~ def test01_setAttrExisting ( self ) : \n 
setattr ( attrs , , 11 ) \n 
self . assertEqual ( attrs . attr_1 , 11 ) \n 
self . assertEqual ( attrs . attr_1 , 10 ) \n 
~~ def test02_delAttr ( self ) : \n 
delattr ( attrs , ) \n 
~~ def test03_copyNodeAttrs ( self ) : \n 
~~ rattrs = self . h5file . root . _v_attrs \n 
rattrs . attr_0 = 0 \n 
rattrs . attr_1 = 100 \n 
array = self . h5file . root . array \n 
attrs . _f_copy ( self . h5file . root ) \n 
self . assertEqual ( rattrs . attr_0 , 0 ) \n 
self . assertEqual ( rattrs . attr_1 , 10 ) \n 
self . assertEqual ( rattrs . attr_2 , 20 ) \n 
self . assertEqual ( rattrs . attr_3 , 30 ) \n 
self . assertEqual ( rattrs . attr_1 , 100 ) \n 
self . assertTrue ( not in rattrs ) \n 
~~ def test04_replaceNode ( self ) : \n 
attrs . attr_1 = 11 \n 
arr = self . h5file . create_array ( , , [ 1 ] ) \n 
arr . attrs . attr_1 = 12 \n 
self . assertTrue ( in self . h5file . root . array . attrs ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 10 ) \n 
self . assertEqual ( self . h5file . root . array . attrs . attr_1 , 12 ) \n 
~~ ~~ class NotLoggedTestCase ( common . TempFileMixin , TestCase ) : \n 
class NotLoggedArray ( NotLoggedMixin , tables . Array ) : \n 
~~ def test00_hierarchy ( self ) : \n 
self . h5file . create_group ( , ) \n 
arr = self . NotLoggedArray ( self . h5file . root , , \n 
[ 1 ] , self . _getMethodName ( ) ) \n 
arr . move ( ) \n 
arr . remove ( ) \n 
~~ def test01_attributes ( self ) : \n 
arr . _v_attrs . foo = \n 
self . assertEqual ( arr . _v_attrs . foo , ) \n 
del arr . _v_attrs . foo \n 
self . assertRaises ( AttributeError , getattr , arr . _v_attrs , ) \n 
~~ ~~ class CreateParentsTestCase ( common . TempFileMixin , TestCase ) : \n 
~~~ super ( CreateParentsTestCase , self ) . setUp ( ) \n 
g1 = self . h5file . create_group ( , ) \n 
self . h5file . create_group ( g1 , ) \n 
~~ def existing ( self , paths ) : \n 
return frozenset ( path for path in paths if path in self . h5file ) \n 
~~ def basetest ( self , doit , pre , post ) : \n 
~~~ pre ( ) \n 
paths = [ , , , ] \n 
for newpath in paths : \n 
~~~ before = self . existing ( paths ) \n 
doit ( newpath ) \n 
after = self . existing ( paths ) \n 
self . assertTrue ( after . issuperset ( before ) ) \n 
post ( newpath ) \n 
self . assertEqual ( after , before ) \n 
~~ ~~ def test00_create ( self ) : \n 
def pre ( ) : \n 
~~ def doit ( newpath ) : \n 
~~~ self . h5file . create_array ( newpath , , [ 1 ] , createparents = True ) \n 
self . assertTrue ( join_path ( newpath , ) in self . h5file ) \n 
~~ def post ( newpath ) : \n 
~~~ self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ self . basetest ( doit , pre , post ) \n 
~~ def test01_move ( self ) : \n 
~~~ self . h5file . create_array ( , , [ 1 ] ) \n 
~~~ self . h5file . move_node ( , newpath , createparents = True ) \n 
~~~ self . assertTrue ( in self . h5file ) \n 
self . assertTrue ( join_path ( newpath , ) not in self . h5file ) \n 
~~ def test02_copy ( self ) : \n 
~~~ self . h5file . copy_node ( , newpath , createparents = True ) \n 
~~~ g = self . h5file . create_group ( , ) \n 
self . h5file . create_array ( g , , [ 1 ] ) \n 
~~~ self . h5file . copy_children ( , newpath , createparents = True ) \n 
~~ ~~ def suite ( ) : \n 
~~~ theSuite = unittest . TestSuite ( ) \n 
niter = 1 \n 
for n in range ( niter ) : \n 
~~~ theSuite . addTest ( unittest . makeSuite ( BasicTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( PersistenceTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateArrayTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateGroupTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RenameNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( MoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( RemoveNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CopyNodeTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( AttributesTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( ComplexTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( NotLoggedTestCase ) ) \n 
theSuite . addTest ( unittest . makeSuite ( CreateParentsTestCase ) ) \n 
~~ if common . heavy : \n 
~~ return theSuite \n 
common . parse_argv ( sys . argv ) \n 
common . print_versions ( ) \n 
unittest . main ( defaultTest = ) \n 
def _print_after_skip ( skip , it = None , dist = None , etime = None ) : \n 
~~~ if it is None : \n 
~~~ msg = "{i:<13}{d:<15}{t:<17}" . format ( i = "Iteration" , \n 
d = "Distance" , \n 
print ( msg ) \n 
print ( "-" * len ( msg ) ) \n 
~~ if it % skip == 0 : \n 
~~~ if etime is None : \n 
~~~ msg = "{i:<13}{d:<15.3e}{t:<18.3e}" \n 
print ( msg . format ( i = it , d = dist , t = etime ) ) \n 
~~ def compute_fixed_point ( T , v , error_tol = 1e-3 , max_iter = 50 , verbose = 1 , \n 
print_skip = 5 , * args , ** kwargs ) : \n 
iterate = 0 \n 
error = error_tol + 1 \n 
~~~ start_time = time . time ( ) \n 
_print_after_skip ( print_skip , it = None ) \n 
~~ while iterate < max_iter and error > error_tol : \n 
~~~ new_v = T ( v , * args , ** kwargs ) \n 
iterate += 1 \n 
error = np . max ( np . abs ( new_v - v ) ) \n 
~~~ etime = time . time ( ) - start_time \n 
_print_after_skip ( print_skip , iterate , error , etime ) \n 
~~~ v [ : ] = new_v \n 
~~ except TypeError : \n 
~~~ v = new_v \n 
~~ ~~ return v \n 
import unittest \n 
from scipy . linalg import LinAlgError \n 
from numpy . testing import assert_allclose \n 
from quantecon . lqcontrol import LQ \n 
from quantecon . robustlq import RBLQ \n 
class TestRBLQControl ( unittest . TestCase ) : \n 
~~~ a_0 = 100 \n 
a_1 = 0.5 \n 
rho = 0.9 \n 
sigma_d = 0.05 \n 
beta = 0.95 \n 
c = 2 \n 
gamma = 50.0 \n 
theta = 0.002 \n 
ac = ( a_0 - c ) / 2.0 \n 
R = np . array ( [ [ 0 , ac , 0 ] , \n 
[ ac , - a_1 , 0.5 ] , \n 
[ 0. , 0.5 , 0 ] ] ) \n 
R = - R \n 
Q = gamma / 2 \n 
A = np . array ( [ [ 1. , 0. , 0. ] , \n 
[ 0. , 1. , 0. ] , \n 
[ 0. , 0. , rho ] ] ) \n 
B = np . array ( [ [ 0. ] , \n 
[ 1. ] , \n 
[ 0. ] ] ) \n 
C = np . array ( [ [ 0. ] , \n 
[ 0. ] , \n 
[ sigma_d ] ] ) \n 
self . rblq_test = RBLQ ( Q , R , A , B , C , beta , theta ) \n 
self . lq_test = LQ ( Q , R , A , B , C , beta ) \n 
self . Fr , self . Kr , self . Pr = self . rblq_test . robust_rule ( ) \n 
~~ def tearDown ( self ) : \n 
~~~ del self . rblq_test \n 
~~ def test_robust_rule_vs_simple ( self ) : \n 
~~~ rblq = self . rblq_test \n 
Fr , Kr , Pr = self . Fr , self . Kr , self . Pr \n 
Fs , Ks , Ps = rblq . robust_rule_simple ( P_init = Pr , tol = 1e-12 ) \n 
assert_allclose ( Fr , Fs , rtol = 1e-4 ) \n 
assert_allclose ( Kr , Ks , rtol = 1e-4 ) \n 
assert_allclose ( Pr , Ps , rtol = 1e-4 ) \n 
~~ def test_f2k_and_k2f ( self ) : \n 
K_f2k , P_f2k = rblq . F_to_K ( Fr ) \n 
F_k2f , P_k2f = rblq . K_to_F ( Kr ) \n 
assert_allclose ( K_f2k , Kr , rtol = 1e-4 ) \n 
assert_allclose ( F_k2f , Fr , rtol = 1e-4 ) \n 
assert_allclose ( P_f2k , P_k2f , rtol = 1e-4 ) \n 
~~ def test_evaluate_F ( self ) : \n 
Kf , Pf , df , Of , of = rblq . evaluate_F ( Fr ) \n 
assert_allclose ( Pf , Pr ) \n 
assert_allclose ( Kf , Kr ) \n 
~~~ suite = unittest . TestLoader ( ) . loadTestsFromTestCase ( TestRBLQControl ) \n 
unittest . TextTestRunner ( verbosity = 2 , stream = sys . stderr ) . run ( suite ) \n 
import tables as pt \n 
fileName = "defaultAlphaFileName.h5" \n 
h5f = [ ] \n 
group = [ ] \n 
table = [ ] \n 
opened = False \n 
ctr = float ( 0.0 ) \n 
class AlphaDataModelClass ( pt . IsDescription ) : \n 
~~~ symbol = pt . StringCol ( 30 ) \n 
exchange = pt . StringCol ( 10 ) \n 
alphaValue = pt . Float32Col ( ) \n 
timestamp = pt . Time64Col ( ) \n 
~~ ~~ def openFile ( newFileName ) : \n 
global fileName , h5f , group , table , opened , ctr \n 
if newFileName is None : \n 
~~~ if ( len ( newFileName ) > 0 ) : \n 
~~~ fileName = str ( newFileName ) \n 
~~ ~~ if not opened : \n 
~~~ h5f = pt . openFile ( str ( fileName ) , mode = "w" ) \n 
group = h5f . createGroup ( "/" , ) \n 
table = h5f . createTable ( group , , AlphaDataModelClass ) \n 
opened = True \n 
~~ ~~ def addRow ( currSymbol , currExchange , currAlphaVal , currTS ) : \n 
global ctr \n 
if opened : \n 
~~~ ctr = ctr + 1 \n 
row = table . row \n 
row [ ] = currSymbol \n 
row [ ] = currExchange \n 
row [ ] = currAlphaVal \n 
row [ ] = currTS \n 
row . append ( ) \n 
~~~ ctr = 0 \n 
raise IOError \n 
~~ ~~ def closeFile ( ) : \n 
table . flush ( ) \n 
h5f . close ( ) \n 
import pickle as pkl \n 
import qstkutil . utils as utils \n 
import dircache \n 
def main ( ) : \n 
~~~ print "Starting..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
~~~ rootdir = os . environ [ ] \n 
~~ fileExtensionToRemove = ".csv" \n 
listOfInputPaths = list ( ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NASDAQ/" ) \n 
listOfInputPaths . append ( rootdir + "/Raw/Yahoo/US/NYSE/" ) \n 
listOfOutputPaths = list ( ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/AMEX/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NASDAQ/" ) \n 
listOfOutputPaths . append ( rootdir + "/Processed/Yahoo/US/NYSE/" ) \n 
for path in listOfOutputPaths : \n 
~~~ if not ( os . access ( path , os . F_OK ) ) : \n 
if ( len ( listOfInputPaths ) != len ( listOfOutputPaths ) ) : \n 
sys . exit ( "FAILURE" ) \n 
~~ path_ctr = - 1 ; \n 
for path in listOfInputPaths : \n 
~~~ path_ctr = path_ctr + 1 ; \n 
stocks_at_this_path = dircache . listdir ( str ( path ) ) \n 
filtered_names = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , filtered_names ) \n 
stock_ctr = - 1 \n 
for stock in filtered_names : \n 
~~~ stock_ctr = stock_ctr + 1 \n 
stock_data = np . loadtxt ( path + stock + ".csv" , np . float , None , "," , None , 1 , use_cols ) \n 
stock_data_shape = stock_data . shape \n 
f = open ( listOfOutputPaths [ path_ctr ] + filtered_names [ stock_ctr ] + ".pkl" , "wb" ) \n 
pkl . dump ( stock_data , f , - 1 ) \n 
f . close ( ) \n 
~~ ~~ print "Finished..." + str ( time . strftime ( "%H:%M:%S" ) ) \n 
~~~ main ( ) \n 
~~ import cPickle \n 
from pandas import DataMatrix \n 
import datetime as dt \n 
import qstkutil . DataAccess as da \n 
import qstkutil . qsdateutil as du \n 
if __name__ == "__main__" : \n 
symbols = list ( [ ] ) \n 
t = map ( int , sys . argv [ 1 ] . split ( ) ) \n 
startday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
t = map ( int , sys . argv [ 2 ] . split ( ) ) \n 
endday = dt . datetime ( t [ 2 ] , t [ 0 ] , t [ 1 ] ) \n 
timeofday = dt . timedelta ( hours = 16 ) \n 
timestamps = du . getNYSEdays ( startday , endday , timeofday ) \n 
dataobj = da . DataAccess ( ) \n 
historic = dataobj . get_data ( timestamps , symbols , "close" ) \n 
alloc_val = random . random ( ) \n 
alloc = DataMatrix ( index = [ historic . index [ 0 ] ] , data = [ alloc_val ] , columns = symbols ) \n 
for date in range ( 1 , len ( historic . index ) ) : \n 
~~~ alloc_val = 1 #random.random() \n 
alloc = alloc . append ( DataMatrix ( index = [ historic . index [ date ] ] , data = [ alloc_val ] , columns = [ symbols ~~ alloc [ ] = 1 - alloc [ symbols [ 0 ] ] \n 
output = open ( sys . argv [ 3 ] , "wb" ) \n 
cPickle . dump ( alloc , output ) \n 
import QSTK . qstkutil . DataAccess as da \n 
from itertools import izip \n 
def getStocks ( listOfPaths ) : \n 
~~~ listOfStocks = list ( ) \n 
fileExtensionToRemove = ".h5" \n 
for path in listOfPaths : \n 
~~~ stocksAtThisPath = list ( ) \n 
stocksAtThisPath = dircache . listdir ( str ( path ) ) \n 
stocksAtThisPath = map ( lambda x : ( x . partition ( str ( fileExtensionToRemove ) ) [ 0 ] ) , stocksAtThisPath \n 
for stock in stocksAtThisPath : \n 
~~~ listOfStocks . append ( stock ) \n 
~~ return listOfStocks \n 
~~~ print "Starting..." \n 
dataItemsList = [ ] \n 
dataItemsList . append ( ) \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NASDAQ/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/Delisted_US_Recent/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/OTC/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_AMEX/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_Delisted/") \n 
#listOfPaths.append("/hzr71/research/QSData/Processed/Norgate/Equities/US_NYSE/") \n 
listOfStocks = list ( ) \n 
#listOfStocks.append("AAPL") \n 
#listOfStocks.append("YHOO") \n 
#listOfStocks.append("AMZN") \n 
listOfPaths = list ( ) \n 
listOfPaths . append ( "C:\\\\test\\\\temp\\\\" ) \n 
#listOfPaths.append("C:\\\\test\\\\hdf\\\\") \n 
listOfStocks = getStocks ( listOfPaths ) \n 
tslist = list ( alpha . getTimestampArray ( ) ) \n 
listOfTS = alpha . getTimestampArray ( ) \n 
for stock in [ "AAPL" ] : \n 
~~~ alphaList = alpha . getStockDataList ( stock , ) \n 
ctr = 0 \n 
for val in alphaList : \n 
ctr += 1 \n 
~~ ~~ print "DONE!" \n 
~~ import unittest \n 
import collections_and_iterators \n 
class TestObjectMethods ( unittest . TestCase ) : \n 
~~~ self . singleLinkList = collections_and_iterators . SinglyLinkedList ( ) \n 
self . singleLinkListData = collections_and_iterators . SinglyLinkedList ( ) \n 
self . singleLinkListData . append ( "Cosmo" ) \n 
self . singleLinkListData . append ( "Allie" ) \n 
self . singleLinkListData . append ( "Watson" ) \n 
self . doubleLinkList = collections_and_iterators . DoublyLinkedList ( ) \n 
self . doubleLinkListData = collections_and_iterators . DoublyLinkedList ( ) \n 
~~ def test_empty_single_list ( self ) : \n 
~~~ self . assertEqual ( 0 , self . singleLinkList . size ) \n 
self . assertIsNone ( self . singleLinkList . head ) \n 
self . assertIsNone ( self . singleLinkList . cursor ) \n 
~~ def test_contains_success ( self ) : \n 
~~~ self . assertTrue ( "Cosmo" in self . singleLinkListData ) \n 
self . assertTrue ( "Allie" in self . singleLinkListData ) \n 
self . assertTrue ( "Watson" in self . singleLinkListData ) \n 
~~ def test_contains_failure ( self ) : \n 
~~~ self . assertFalse ( "Gabby" in self . singleLinkListData ) \n 
self . assertFalse ( "Thomas" in self . singleLinkListData ) \n 
~~ def test_append_success ( self ) : \n 
~~~ self . assertEqual ( "Cosmo" , self . singleLinkListData [ 0 ] ) \n 
self . assertEqual ( "Allie" , self . singleLinkListData [ 1 ] ) \n 
self . assertEqual ( "Watson" , self . singleLinkListData [ 2 ] ) \n 
~~ def test_append_failure ( self ) : \n 
~~~ with self . assertRaises ( IndexError ) : \n 
~~~ self . singleLinkListData [ 3 ] \n 
~~ self . singleLinkListData . append ( "Foley" ) \n 
self . assertEqual ( "Foley" , self . singleLinkListData [ 3 ] ) \n 
~~~ self . assertEqual ( "Cosmo" , self . singleLinkListData . __getitem__ ( 0 ) ) \n 
self . assertEqual ( "Allie" , self . singleLinkListData . __getitem__ ( 1 ) ) \n 
self . assertEqual ( "Watson" , self . singleLinkListData . __getitem__ ( 2 ) ) \n 
~~ def test_getitem_failure ( self ) : \n 
~~~ self . singleLinkListData . __getitem__ ( 3 ) \n 
self . singleLinkListData . __getitem__ ( - 3 ) \n 
~~ ~~ def test_setitem_success ( self ) : \n 
self . singleLinkListData [ 0 ] = "Smalls" \n 
self . assertEqual ( "Smalls" , self . singleLinkListData [ 0 ] ) \n 
~~ def test_setitem_failure ( self ) : \n 
~~~ self . singleLinkListData [ 5 ] = "Bruno" \n 
self . singleLinkListData [ - 1 ] = "Lucie" \n 
~~ ~~ def test_empty_double_list ( self ) : \n 
~~~ self . assertEqual ( 0 , self . doubleLinkList . size ) \n 
self . assertIsNone ( self . doubleLinkList . head ) \n 
self . assertIsNone ( self . doubleLinkList . cursor ) \n 
~~ def test_insert_success ( self ) : \n 
~~ def test_insert_fauilure ( self ) : \n 
~~~ unittest . main ( verbosity = 2 ) \n 
~~ from types import FunctionType \n 
from rdflib . graph import ConjunctiveGraph \n 
from rdflib . graph import Graph \n 
from rdflib . term import BNode \n 
from rdflib . term import Literal \n 
from rdflib . term import URIRef \n 
from rdflib . term import Variable \n 
from rdflib . namespace import NamespaceManager \n 
from rdfextras . sparql import _questChar \n 
from rdfextras . sparql import SPARQLError \n 
from rdflib . util import check_object \n 
from rdflib . util import check_subject \n 
__all__ = [ , , ] \n 
class SPARQLGraph ( object ) : \n 
SPARQL_DATASET = 0 \n 
NAMED_GRAPH = 1 \n 
__slots__ = ( "graphVariable" , \n 
"DAWG_DATASET_COMPLIANCE" , \n 
"identifier" , \n 
"graphKind" , \n 
"graph" ) \n 
def __init__ ( self , graph , graphVariable = None , dSCompliance = False ) : \n 
~~~ assert not graphVariable or graphVariable [ 0 ] != , repr ( graphVariable ) \n 
self . graphVariable = graphVariable \n 
self . DAWG_DATASET_COMPLIANCE = dSCompliance \n 
self . graphKind = None \n 
if graph is not None : \n 
if isinstance ( graph , ConjunctiveGraph ) : \n 
~~~ self . graphKind = self . SPARQL_DATASET \n 
self . identifier = graph . default_context . identifier \n 
~~~ self . graphKind = self . NAMED_GRAPH \n 
self . identifier = graph . identifier \n 
~~ ~~ ~~ def setupGraph ( self , store , graphKind = None ) : \n 
~~~ gKind = graphKind and graphKind or self . graphKind \n 
self . graph = gKind ( store , self . identifier ) \n 
~~ def __reduce__ ( self ) : \n 
~~~ return ( SPARQLGraph , \n 
( None , \n 
self . graphVariable , \n 
self . DAWG_DATASET_COMPLIANCE ) , \n 
self . __getstate__ ( ) ) \n 
~~ def __getstate__ ( self ) : \n 
~~~ return ( self . graphVariable , \n 
self . DAWG_DATASET_COMPLIANCE , \n 
self . identifier ) #, \n 
~~ def __setstate__ ( self , arg ) : \n 
~~~ gVar , flag , identifier = arg \n 
self . graphVariable = gVar \n 
self . DAWG_DATASET_COMPLIANCE = flag \n 
self . identifier = identifier \n 
########################################################################## \n 
~~ def _clusterForward ( self , seed , Cluster ) : \n 
~~~ for ( p , o ) in self . graph . predicate_objects ( seed ) : \n 
~~~ if not ( seed , p , o ) in Cluster . graph : \n 
~~~ Cluster . add ( ( seed , p , o ) ) \n 
self . _clusterForward ( p , Cluster ) \n 
self . _clusterForward ( o , Cluster ) \n 
~~ ~~ ~~ except : \n 
~~ ~~ def clusterForward ( self , seed , Cluster = None ) : \n 
if Cluster == None : \n 
~~~ Cluster = SPARQLGraph ( ) \n 
self . _clusterForward ( seed , Cluster ) \n 
return Cluster \n 
~~ def _clusterBackward ( self , seed , Cluster ) : \n 
~~~ for ( s , p ) in self . graph . subject_predicates ( seed ) : \n 
~~~ if not ( s , p , seed ) in Cluster . graph : \n 
~~~ Cluster . add ( ( s , p , seed ) ) \n 
self . _clusterBackward ( s , Cluster ) \n 
self . _clusterBackward ( p , Cluster ) \n 
~~ ~~ def clusterBackward ( self , seed , Cluster = None ) : \n 
self . _clusterBackward ( seed , Cluster ) \n 
~~ def cluster ( self , seed ) : \n 
return self . clusterBackward ( seed ) + self . clusterForward ( seed ) \n 
def _createResource ( v ) : \n 
if isinstance ( v , Literal ) or isinstance ( v , BNode ) or isinstance ( v , URIRef ) : \n 
~~~ return v \n 
~~ ~~ def _isResQuest ( r ) : \n 
if r and isinstance ( r , basestring ) and r [ 0 ] == _questChar : \n 
~~~ return True \n 
~~ class GraphPattern : \n 
def __init__ ( self , patterns = [ ] ) : \n 
self . patterns = [ ] \n 
self . constraints = [ ] \n 
self . unbounds = [ ] \n 
self . bnodes = { } \n 
if type ( patterns ) == list : \n 
~~~ self . addPatterns ( patterns ) \n 
~~ elif type ( patterns ) == tuple : \n 
~~~ self . addPattern ( patterns ) \n 
~~~ raise SPARQLError ( \n 
~~ ~~ def _generatePattern ( self , tupl ) : \n 
if type ( tupl ) != tuple : \n 
~~ if len ( tupl ) != 3 and len ( tupl ) != 4 : \n 
~~ if len ( tupl ) == 3 : \n 
~~~ ( s , p , o ) = tupl \n 
f = None \n 
~~~ ( s , p , o , f ) = tupl \n 
~~ final = [ ] \n 
for c in ( s , p , o ) : \n 
~~~ if _isResQuest ( c ) : \n 
~~~ if not c in self . unbounds : \n 
~~~ self . unbounds . append ( c ) \n 
~~ final . append ( c ) \n 
~~ elif isinstance ( c , BNode ) : \n 
~~~ final . append ( c ) \n 
~~~ final . append ( _createResource ( c ) ) \n 
~~ ~~ final . append ( f ) \n 
return tuple ( final ) \n 
~~ def addPattern ( self , tupl ) : \n 
self . patterns . append ( self . _generatePattern ( tupl ) ) \n 
~~ def insertPattern ( self , tupl ) : \n 
self . patterns . insert ( 0 , self . _generatePattern ( tupl ) ) \n 
~~ def addPatterns ( self , lst ) : \n 
for l in lst : \n 
~~~ self . addPattern ( l ) \n 
~~ ~~ def insertPatterns ( self , lst ) : \n 
for i in xrange ( len ( lst ) - 1 , - 1 , - 1 ) : \n 
~~~ self . insertPattern ( lst [ i ] ) \n 
~~ ~~ def addConstraint ( self , func ) : \n 
if type ( func ) == FunctionType : \n 
~~~ self . constraints . append ( func ) \n 
~~ ~~ def addConstraints ( self , lst ) : \n 
~~~ self . addConstraint ( l ) \n 
~~ ~~ def construct ( self , tripleStore , bindings ) : \n 
localBnodes = { } \n 
for c in self . bnodes : \n 
~~~ localBnodes [ c ] = BNode ( ) \n 
~~ def bind ( st ) : \n 
~~~ if _isResQuest ( st ) : \n 
~~~ if st in bindings : \n 
~~~ return bindings [ st ] \n 
~~~ if isinstance ( self , GraphPattern ) : \n 
~~~ return st \n 
~~ ~~ ~~ elif isinstance ( st , BNode ) : \n 
~~~ for c in self . bnodes : \n 
~~~ if self . bnodes [ c ] == st : \n 
~~~ return localBnodes [ c ] \n 
~~ ~~ return st \n 
~~ ~~ for pattern in self . patterns : \n 
~~~ ( s , p , o , f ) = pattern \n 
triplet = [ ] \n 
valid = True \n 
for res in ( s , p , o ) : \n 
~~~ val = bind ( res ) \n 
if val != None : \n 
~~~ triplet . append ( val ) \n 
~~~ valid = False \n 
~~ ~~ if valid : \n 
~~~ tripleStore . add ( tuple ( triplet ) ) \n 
~~ ~~ ~~ def __add__ ( self , other ) : \n 
retval = GraphPattern ( ) \n 
retval += self \n 
retval += other \n 
return retval \n 
~~ def __iadd__ ( self , other ) : \n 
self . patterns += other . patterns \n 
self . constraints += other . constraints \n 
for c in other . unbounds : \n 
~~ ~~ for c in other . bnodes : \n 
~~~ if not c in self . bnodes : \n 
~~~ self . bnodes [ c ] = other . bnodes [ c ] \n 
~~ ~~ return self \n 
~~ def __str__ ( self ) : \n 
~~~ return self . __repr__ ( ) \n 
~~ def isEmpty ( self ) : \n 
return len ( self . patterns ) == 0 \n 
~~ ~~ class BasicGraphPattern ( GraphPattern ) : \n 
def __init__ ( self , patterns = [ ] , prolog = None ) : \n 
GraphPattern . __init__ ( self , patterns ) \n 
self . prolog = prolog \n 
~~ def canonicalTerm ( self , term ) : \n 
~~~ if isinstance ( term , URIRef ) : \n 
~~~ if self . prolog is not None : \n 
~~~ namespace_manager = NamespaceManager ( Graph ( ) ) \n 
for prefix , uri in self . prolog . prefixBindings . items ( ) : \n 
~~~ namespace_manager . bind ( prefix , uri , override = False ) \n 
~~~ prefix , uri , localName = namespace_manager . compute_qname ( term ) \n 
~~~ return term \n 
~~ if prefix not in self . prolog . prefixBindings : \n 
~~~ return . join ( [ prefix , localName ] ) \n 
~~ ~~ elif isinstance ( term , Literal ) : \n 
~~~ return term . n3 ( ) \n 
~~ elif isinstance ( term , BNode ) : \n 
~~~ assert isinstance ( term , Variable ) \n 
return term . n3 ( ) \n 
~~ ~~ def __repr__ ( self ) : \n 
~~~ if self . constraints : \n 
. join ( [ . join ( [ \n 
self . canonicalTerm ( pat [ 0 ] ) , \n 
self . canonicalTerm ( pat [ 1 ] ) , \n 
self . canonicalTerm ( pat [ 2 ] ) ] \n 
for pat in self . patterns ] ) ) \n 
~~~ return "BGP(%s)" % ( \n 
. join ( [ + . join ( [ \n 
self . canonicalTerm ( s ) , \n 
self . canonicalTerm ( p ) , \n 
self . canonicalTerm ( o ) ] \n 
for s , p , o , f in self . patterns ] ) ) \n 
~~ def _generatePattern ( self , tupl ) : \n 
~~~ if isinstance ( c , Variable ) : \n 
~~ def fetchTerminalExpression ( self ) : \n 
~~~ yield self \n 
~~~ from rdfextras . sparql . evaluate import Unbound \n 
v1 = Variable ( "a" ) \n 
u1 = Unbound ( "a" ) \n 
g = BasicGraphPattern ( \n 
[ ( "a" , "?b" , 24 ) , ( "?r" , "?c" , 12345 ) , ( v1 , "?c" , 3333 ) , ( u1 , "?c" , 9999 ) ] ) \n 
print g \n 
~~ from rdflib import ConjunctiveGraph , plugin \n 
from rdflib . store import Store \n 
from StringIO import StringIO \n 
class JSON ( unittest . TestCase ) : \n 
~~~ self . graph = ConjunctiveGraph ( plugin . get ( , Store ) ( ) ) \n 
self . graph . parse ( StringIO ( test_data ) , format = "n3" ) \n 
~~ def testComma ( self ) : \n 
results = self . graph . query ( test_query ) \n 
result_json = results . serialize ( format = ) \n 
self . failUnless ( result_json . find ( correct ) > 0 ) \n 
~~ def testHeader ( self ) : \n 
results = self . graph . query ( test_header_query ) \n 
self . failUnless ( result_json . find ( \'"x",\' ) == - 1 ) \n 
from rdflib import plugin \n 
from rdflib . namespace import Namespace , RDF , RDFS \n 
from cStringIO import StringIO \n 
from rdflib import Graph \n 
import rdflib \n 
~~~ set \n 
~~~ from sets import Set as set \n 
class AdvancedTests ( unittest . TestCase ) : \n 
~~~ memStore = plugin . get ( , Store ) ( ) \n 
self . testGraph = Graph ( memStore ) \n 
self . testGraph . parse ( StringIO ( testGraph1N3 ) , format = ) \n 
~~ def testNamedGraph ( self ) : \n 
~~~ OWL_NS = Namespace ( "http://www.w3.org/2002/07/owl#" ) \n 
rt = self . testGraph . query ( sparqlQ4 ) \n 
self . assertEquals ( set ( rt ) , set ( ( x , ) for x in [ OWL_NS . DatatypeProperty , OWL_NS . ObjectProperty , \n 
~~ def testScopedBNodes ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ1 ) \n 
self . assertEquals ( list ( rt ) [ 0 ] [ 0 ] , URIRef ( "http://test/foo" ) ) \n 
~~ def testCollectionContentWithinAndWithout ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ3 ) \n 
self . assertEquals ( list ( rt ) [ 0 ] [ 0 ] , URIRef ( "http://test/bar" ) ) \n 
~~ def testCollectionAsObject ( self ) : \n 
~~~ rt = self . testGraph . query ( sparqlQ2 ) \n 
self . assertEquals ( 1 , len ( rt ) ) \n 
~~~ suite = unittest . makeSuite ( AdvancedTests ) \n 
unittest . TextTestRunner ( verbosity = 3 ) . run ( suite ) \n 
~~ from urllib2 import URLError \n 
~~~ from Ft . Lib import UriException \n 
~~~ from urllib2 import URLError as UriException \n 
from rdflib import ConjunctiveGraph , URIRef \n 
class SPARQLloadContextsTest ( unittest . TestCase ) : \n 
~~~ def test_dSet_parsed_as_URL_raises_Exception ( self ) : \n 
graph = ConjunctiveGraph ( ) \n 
graph . get_context ( URIRef ( "http://test/" ) \n 
) . parse ( "http://www.w3.org/People/Berners-Lee/card.rdf" ) \n 
self . assertRaises ( ( URLError , UriException ) , \n 
graph . query , ( querystr ) , loadContexts = False ) \n 
~~ def test_dSet_parsed_as_context_returns_results ( self ) : \n 
graph . get_context ( URIRef ( ) \n 
r = graph . query ( querystr , loadContexts = True ) \n 
self . assert_ ( len ( r . bindings ) is not 0 ) \n 
from rdflib import Graph , RDF , RDFS , Literal \n 
from rdflib . namespace import FOAF \n 
if __name__ == : \n 
~~~ g = Graph ( ) \n 
bob = g . resource ( ) \n 
bob . set ( FOAF . name , Literal ( "Bob" ) ) \n 
bill = g . resource ( ) \n 
bill . add ( RDF . type , FOAF . Agent ) \n 
bill . set ( RDFS . label , Literal ( "Bill" ) ) \n 
bill . add ( FOAF . knows , bob ) \n 
for friend in bill [ FOAF . knows ] : \n 
for friend in bill [ FOAF . knows / FOAF . name ] : \n 
~~~ print friend \n 
~~ bill [ RDFS . label ] = Literal ( "William" ) \n 
print g . serialize ( format = ) \n 
from codecs import getreader \n 
from rdflib . py3compat import b \n 
from rdflib import ConjunctiveGraph \n 
from rdflib . plugins . parsers . ntriples import NTriplesParser \n 
from rdflib . plugins . parsers . ntriples import ParseError \n 
from rdflib . plugins . parsers . ntriples import r_tail \n 
from rdflib . plugins . parsers . ntriples import r_wspace \n 
from rdflib . plugins . parsers . ntriples import r_wspaces \n 
__all__ = [ ] \n 
class NQuadsParser ( NTriplesParser ) : \n 
~~~ def parse ( self , inputsource , sink , ** kwargs ) : \n 
self . sink = ConjunctiveGraph ( store = sink . store , identifier = sink . identifier ) \n 
source = inputsource . getByteStream ( ) \n 
if not hasattr ( source , ) : \n 
~~ source = getreader ( ) ( source ) \n 
self . file = source \n 
self . buffer = \n 
~~~ self . line = __line = self . readline ( ) \n 
if self . line is None : \n 
~~~ self . parseline ( ) \n 
~~ except ParseError , msg : \n 
~~ ~~ return self . sink \n 
~~ def parseline ( self ) : \n 
~~~ self . eat ( r_wspace ) \n 
if ( not self . line ) or self . line . startswith ( ( ) ) : \n 
~~ subject = self . subject ( ) \n 
self . eat ( r_wspace ) \n 
predicate = self . predicate ( ) \n 
obj = self . object ( ) \n 
context = self . uriref ( ) or self . nodeid ( ) or self . sink . identifier \n 
self . eat ( r_tail ) \n 
if self . line : \n 
~~ self . sink . get_context ( context ) . add ( ( subject , predicate , obj ) ) \n 
import codecs \n 
import csv \n 
from rdflib import Variable , BNode , URIRef , Literal , py3compat \n 
from rdflib . query import Result , ResultSerializer , ResultParser \n 
class CSVResultParser ( ResultParser ) : \n 
~~~ def __init__ ( self ) : \n 
~~~ self . delim = "," \n 
~~ def parse ( self , source ) : \n 
~~~ r = Result ( ) \n 
if isinstance ( source . read ( 0 ) , py3compat . bytestype ) : \n 
~~~ source = codecs . getreader ( ) ( source ) \n 
~~ reader = csv . reader ( source , delimiter = self . delim ) \n 
r . vars = [ Variable ( x ) for x in reader . next ( ) ] \n 
r . bindings = [ ] \n 
for row in reader : \n 
~~~ r . bindings . append ( self . parseRow ( row , r . vars ) ) \n 
~~ return r \n 
~~ def parseRow ( self , row , v ) : \n 
~~~ return dict ( ( var , val ) \n 
for var , val in zip ( v , [ self . convertTerm ( t ) \n 
for t in row ] ) if val is not None ) \n 
~~ def convertTerm ( self , t ) : \n 
~~~ if t == "" : \n 
~~ if t . startswith ( "_:" ) : \n 
~~~ return URIRef ( t ) \n 
~~ return Literal ( t ) \n 
~~ ~~ class CSVResultSerializer ( ResultSerializer ) : \n 
~~~ def __init__ ( self , result ) : \n 
~~~ ResultSerializer . __init__ ( self , result ) \n 
self . delim = "," \n 
if result . type != "SELECT" : \n 
~~~ raise Exception ( \n 
~~ ~~ def serialize ( self , stream , encoding = ) : \n 
~~~ if py3compat . PY3 : \n 
~~~ import codecs \n 
stream = codecs . getwriter ( encoding ) ( stream ) \n 
~~ out = csv . writer ( stream , delimiter = self . delim ) \n 
vs = [ self . serializeTerm ( v , encoding ) for v in self . result . vars ] \n 
out . writerow ( vs ) \n 
for row in self . result . bindings : \n 
~~~ out . writerow ( [ self . serializeTerm ( \n 
row . get ( v ) , encoding ) for v in self . result . vars ] ) \n 
~~ ~~ def serializeTerm ( self , term , encoding ) : \n 
~~~ if term is None : \n 
~~~ return "" \n 
~~ if not py3compat . PY3 : \n 
~~~ return term . encode ( encoding ) \n 
NOSE_ARGS = [ \n 
COVERAGE_EXTRA_ARGS = [ \n 
DEFAULT_ATTRS = [ ] \n 
DEFAULT_DIRS = [ , ] \n 
~~~ from sys import argv , exit , stderr \n 
try : import nose \n 
except ImportError : \n 
~~ if in argv : \n 
~~~ try : import coverage \n 
argv . remove ( ) \n 
~~~ NOSE_ARGS += COVERAGE_EXTRA_ARGS \n 
~~ ~~ if True not in [ a . startswith ( ) or a . startswith ( ) for a in argv ] : \n 
~~~ argv . append ( + . join ( DEFAULT_ATTRS ) ) \n 
~~ if not [ a for a in argv [ 1 : ] if not a . startswith ( ) ] : \n 
~~~ argv += DEFAULT_DIRS \n 
~~ finalArgs = argv + NOSE_ARGS \n 
nose . run_exit ( argv = finalArgs ) \n 
~~ import sys \n 
from tempfile import mkdtemp , mkstemp \n 
from rdflib import RDF , RDFS , URIRef , BNode , Variable , plugin \n 
from rdflib . graph import QuotedGraph , ConjunctiveGraph \n 
implies = URIRef ( "http://www.w3.org/2000/10/swap/log#implies" ) \n 
from nose . tools import nottest \n 
from nose . exc import SkipTest \n 
def testFormulaStore ( store = "default" , configString = None ) : \n 
~~~ g = ConjunctiveGraph ( store = store ) \n 
~~ if configString : \n 
~~~ g . destroy ( configString ) \n 
g . open ( configString ) \n 
~~~ if store == : \n 
~~~ _ , path = mkstemp ( prefix = , dir = , suffix = ) \n 
g . open ( path , create = True ) \n 
~~~ g . open ( mkdtemp ( ) , create = True ) \n 
~~ ~~ g . parse ( data = testN3 , format = "n3" ) \n 
~~~ for s , p , o in g . triples ( ( None , implies , None ) ) : \n 
~~~ formulaA = s \n 
formulaB = o \n 
~~ assert type ( formulaA ) == QuotedGraph and type ( formulaB ) == QuotedGraph \n 
b = URIRef ( ) \n 
c = URIRef ( ) \n 
d = URIRef ( ) \n 
v = Variable ( ) \n 
universe = ConjunctiveGraph ( g . store ) \n 
assert len ( list ( universe . triples ( ( formulaA , implies , formulaB ) ) ) ) == 1 \n 
assert len ( list ( formulaB . triples ( ( None , None , v ) ) ) ) == 1 \n 
for s , p , o in formulaB . triples ( ( None , d , None ) ) : \n 
~~~ if o != c : \n 
~~~ assert isinstance ( o , Variable ) \n 
assert o == v \n 
~~ ~~ s = list ( universe . subjects ( RDF . type , RDFS . Class ) ) [ 0 ] \n 
assert isinstance ( s , BNode ) \n 
assert len ( list ( universe . triples ( ( None , implies , None ) ) ) ) == 1 \n 
assert len ( list ( universe . triples ( ( None , RDF . type , None ) ) ) ) == 1 \n 
assert len ( list ( formulaA . triples ( ( None , RDF . type , None ) ) ) ) == 1 \n 
assert len ( list ( formulaA . triples ( ( None , None , None ) ) ) ) == 2 \n 
assert len ( list ( formulaB . triples ( ( None , None , None ) ) ) ) == 2 \n 
assert len ( list ( universe . triples ( ( None , None , None ) ) ) ) == 3 \n 
assert len ( list ( formulaB . triples ( \n 
( None , URIRef ( ) , None ) ) ) ) == 2 \n 
assert len ( list ( universe . triples ( \n 
( None , URIRef ( ) , None ) ) ) ) == 1 \n 
universe . remove ( ( None , implies , None ) ) \n 
assert len ( list ( universe . triples ( ( None , implies , None ) ) ) ) == 0 \n 
formulaA . remove ( ( None , b , None ) ) \n 
assert len ( list ( formulaA . triples ( ( None , None , None ) ) ) ) == 1 \n 
formulaA . remove ( ( None , RDF . type , None ) ) \n 
assert len ( list ( formulaA . triples ( ( None , None , None ) ) ) ) == 0 \n 
universe . remove ( ( None , RDF . type , RDFS . Class ) ) \n 
universe . remove_context ( formulaB ) \n 
assert len ( list ( universe . triples ( ( None , RDF . type , None ) ) ) ) == 0 \n 
assert len ( universe ) == 1 \n 
assert len ( formulaB ) == 0 \n 
universe . remove ( ( None , None , None ) ) \n 
assert len ( universe ) == 0 \n 
g . close ( ) \n 
if store == : \n 
~~~ os . unlink ( path ) \n 
~~~ g . store . destroy ( configString ) \n 
~~ ~~ except : \n 
~~~ g . close ( ) \n 
~~ raise \n 
~~ ~~ def testFormulaStores ( ) : \n 
~~~ pluginname = None \n 
~~~ if len ( sys . argv ) > 1 : \n 
~~~ pluginname = sys . argv [ 1 ] \n 
~~ ~~ for s in plugin . plugins ( pluginname , plugin . Store ) : \n 
~~~ if s . name in ( \n 
~~ if not s . getClass ( ) . formula_aware : \n 
~~ yield testFormulaStore , s . name \n 
~~~ import nose \n 
nose . main ( defaultTest = sys . argv [ 0 ] ) \n 
~~ import rdflib \n 
def test_time_child_element ( ) : \n 
g = rdflib . Graph ( ) \n 
g . parse ( data = html , format = ) \n 
date = g . value ( \n 
rdflib . URIRef ( "http://example.com/" ) , \n 
rdflib . URIRef ( "http://schema.org/dateCreated" ) \n 
assert len ( g ) == 3 \n 
assert date == rdflib . term . Literal ( "2016-01-01" ) \n 
~~ from rdflib . term import URIRef , BNode \n 
from rdflib . namespace import RDFS \n 
from rdflib . plugins . serializers . rdfxml import XMLSerializer \n 
~~~ from io import BytesIO \n 
~~~ from StringIO import StringIO as BytesIO \n 
~~ class SerializerTestBase ( object ) : \n 
~~~ repeats = 8 \n 
def setup ( self ) : \n 
~~~ graph = ConjunctiveGraph ( ) \n 
graph . parse ( data = self . testContent , format = self . testContentFormat ) \n 
self . sourceGraph = graph \n 
~~ def test_serialize_and_reparse ( self ) : \n 
~~~ reparsedGraph = serialize_and_load ( self . sourceGraph , self . serializer ) \n 
_assert_equal_graphs ( self . sourceGraph , reparsedGraph ) \n 
~~ def test_multiple ( self ) : \n 
~~~ self . test_serialize_and_reparse ( ) \n 
~~ ~~ ~~ def _assert_equal_graphs ( g1 , g2 ) : \n 
g1copy = _mangled_copy ( g1 ) \n 
g2copy = _mangled_copy ( g2 ) \n 
g1copy -= _mangled_copy ( g2 ) \n 
g2copy -= _mangled_copy ( g1 ) \n 
~~ _blank = BNode ( ) \n 
def _mangled_copy ( g ) : \n 
gcopy = ConjunctiveGraph ( ) \n 
isbnode = lambda v : isinstance ( v , BNode ) \n 
for s , p , o in g : \n 
~~~ if isbnode ( s ) : s = _blank \n 
if isbnode ( p ) : p = _blank \n 
if isbnode ( o ) : o = _blank \n 
gcopy . add ( ( s , p , o ) ) \n 
~~ return gcopy \n 
~~ def serialize ( sourceGraph , makeSerializer , getValue = True , extra_args = { } ) : \n 
~~~ serializer = makeSerializer ( sourceGraph ) \n 
stream = BytesIO ( ) \n 
serializer . serialize ( stream , ** extra_args ) \n 
return getValue and stream . getvalue ( ) or stream \n 
~~ def serialize_and_load ( sourceGraph , makeSerializer ) : \n 
~~~ stream = serialize ( sourceGraph , makeSerializer , False ) \n 
stream . seek ( 0 ) \n 
reparsedGraph = ConjunctiveGraph ( ) \n 
reparsedGraph . load ( stream ) \n 
return reparsedGraph \n 
~~ class TestXMLSerializer ( SerializerTestBase ) : \n 
~~~ serializer = XMLSerializer \n 
testContentFormat = \n 
def test_result_fragments ( self ) : \n 
~~~ rdfXml = serialize ( self . sourceGraph , self . serializer ) \n 
~~ def test_result_fragments_with_base ( self ) : \n 
~~~ rdfXml = serialize ( self . sourceGraph , self . serializer , \n 
extra_args = { : "http://example.org/" , : "http://example.org/" } ) \n 
assert b ( \'xml:base="http://example.org/"\' ) in rdfXml \n 
~~ def test_subClassOf_objects ( self ) : \n 
_assert_expected_object_types_for_predicates ( reparsedGraph , \n 
[ RDFS . seeAlso , RDFS . subClassOf ] , \n 
[ URIRef , BNode ] ) \n 
~~ ~~ def _assert_expected_object_types_for_predicates ( graph , predicates , types ) : \n 
~~~ for s , p , o in graph : \n 
~~~ if p in predicates : \n 
~~~ someTrue = [ isinstance ( o , t ) for t in types ] \n 
~~ ~~ ~~ from __future__ import division \n 
import random as rd \n 
from . import common_args \n 
from . . util import scale_samples , read_param_file , compute_groups_matrix \n 
from . optimal_trajectories import return_max_combo \n 
from . morris_util import * \n 
from operator import or_ \n 
~~~ from gurobipy import * \n 
~~~ _has_gurobi = False \n 
~~~ _has_gurobi = True \n 
~~ def sample ( problem , N , num_levels , grid_jump , optimal_trajectories = None , local_optimization = False ) : \n 
if grid_jump >= num_levels : \n 
~~ if problem . get ( ) : \n 
~~~ sample = sample_groups ( problem , N , num_levels , grid_jump ) \n 
~~~ sample = sample_oat ( problem , N , num_levels , grid_jump ) \n 
~~ if optimal_trajectories : \n 
if optimal_trajectories < 2 : \n 
~~ if _has_gurobi == False and local_optimization == False and optimal_trajectories > 10 : \n 
~~ sample = compute_optimised_trajectories ( problem , \n 
sample , \n 
N , \n 
optimal_trajectories , \n 
local_optimization ) \n 
~~ scale_samples ( sample , problem [ ] ) \n 
return sample \n 
~~ def sample_oat ( problem , N , num_levels , grid_jump ) : \n 
~~~ D = problem [ ] \n 
B = np . tril ( np . ones ( [ D + 1 , D ] , dtype = int ) , - 1 ) + np . triu ( - 1 * np . ones ( [ D + 1 , D ] , dtype = int ) ) \n 
delta = grid_jump / ( num_levels - 1 ) \n 
X = np . empty ( [ N * ( D + 1 ) , D ] ) \n 
for j in range ( N ) : \n 
~~~ DM = np . diag ( [ rd . choice ( [ - 1 , 1 ] ) for _ in range ( D ) ] ) \n 
perm = np . random . permutation ( D ) \n 
P = np . zeros ( [ D , D ] ) \n 
for i in range ( D ) : \n 
~~~ P [ i , perm [ i ] ] = 1 \n 
~~ x_base = np . empty ( [ D + 1 , D ] ) \n 
~~~ x_base [ : , i ] = ( \n 
rd . choice ( np . arange ( num_levels - grid_jump ) ) ) / ( num_levels - 1 ) \n 
~~ index_list = np . arange ( D + 1 ) + j * ( D + 1 ) \n 
delta_diag = np . diag ( [ delta for _ in range ( D ) ] ) \n 
X [ index_list , : ] = 0.5 * ( np . mat ( B ) * np . mat ( P ) * np . mat ( DM ) + 1 ) * np . mat ( delta_diag ) + np . mat ( x_base ) \n 
~~ def sample_groups ( problem , N , num_levels , grid_jump ) : \n 
G , group_names = compute_groups_matrix ( problem [ ] , problem [ ] ) \n 
if G is None : \n 
~~ if type ( G ) is not np . matrixlib . defmatrix . matrix : \n 
~~ k = G . shape [ 0 ] \n 
g = G . shape [ 1 ] \n 
sample = np . empty ( ( N * ( g + 1 ) , k ) ) \n 
sample = np . array ( [ generate_trajectory ( G , num_levels , grid_jump ) for n in range ( N ) ] ) \n 
return sample . reshape ( ( N * ( g + 1 ) , k ) ) \n 
~~ def compute_optimised_trajectories ( problem , input_sample , N , k_choices , local_optimization = False ) : ~~~ \n 
num_params = problem [ ] \n 
groups = compute_groups_matrix ( problem [ ] , num_params ) \n 
if np . any ( ( input_sample < 0 ) | ( input_sample > 1 ) ) : \n 
~~ if _has_gurobi == True and local_optimization == False : \n 
~~~ maximum_combo = return_max_combo ( input_sample , \n 
num_params , \n 
k_choices , \n 
groups ) \n 
~~~ maximum_combo = find_optimum_combination ( input_sample , \n 
groups , \n 
~~ num_groups = None \n 
if groups is not None : \n 
~~~ num_groups = groups [ 0 ] . shape [ 1 ] \n 
~~ output = compile_output ( input_sample , \n 
maximum_combo , \n 
num_groups ) \n 
return output \n 
~~~ parser = common_args . create ( ) \n 
parser . add_argument ( \n 
, , type = int , required = True , help = ) \n 
parser . add_argument ( , , type = int , required = False , \n 
default = 4 , help = ) \n 
parser . add_argument ( , type = int , required = False , \n 
default = 2 , help = ) \n 
default = None , help = ) \n 
parser . add_argument ( , , type = bool , required = True , \n 
default = False , help = args = parser . parse_args ( ) \n 
np . random . seed ( args . seed ) \n 
rd . seed ( args . seed ) \n 
problem = read_param_file ( args . paramfile ) \n 
param_values = sample ( problem , args . samples , args . levels , args . grid_jump , args . k_optimal , args . local ) \n 
np . savetxt ( args . output , param_values , delimiter = args . delimiter , \n 
fmt = + str ( args . precision ) + ) \n 
from distutils . command . build import build as _build \n 
from distutils . command . sdist import sdist as _sdist \n 
from distutils . core import Command \n 
import os , sys , re , subprocess , errno \n 
versionfile_source = None \n 
versionfile_build = None \n 
tag_prefix = None \n 
parentdir_prefix = None \n 
VCS = None \n 
LONG_VERSION_PY = { } \n 
def run_command ( commands , args , cwd = None , verbose = False , hide_stderr = False ) : \n 
~~~ assert isinstance ( commands , list ) \n 
p = None \n 
for c in commands : \n 
~~~ p = subprocess . Popen ( [ c ] + args , cwd = cwd , stdout = subprocess . PIPE , \n 
stderr = ( subprocess . PIPE if hide_stderr \n 
else None ) ) \n 
~~ except EnvironmentError : \n 
~~~ e = sys . exc_info ( ) [ 1 ] \n 
if e . errno == errno . ENOENT : \n 
print ( e ) \n 
~~ return None \n 
~~ stdout = p . communicate ( ) [ 0 ] . strip ( ) \n 
if sys . version >= : \n 
~~~ stdout = stdout . decode ( ) \n 
~~ if p . returncode != 0 : \n 
~~ return stdout \n 
def git_get_keywords ( versionfile_abs ) : \n 
~~~ keywords = { } \n 
~~~ f = open ( versionfile_abs , "r" ) \n 
for line in f . readlines ( ) : \n 
~~~ mo = re . search ( r\'=\\s*"(.*)"\' , line ) \n 
if mo : \n 
~~~ keywords [ "refnames" ] = mo . group ( 1 ) \n 
~~~ keywords [ "full" ] = mo . group ( 1 ) \n 
~~ return keywords \n 
~~ def git_versions_from_keywords ( keywords , tag_prefix , verbose = False ) : \n 
~~~ if not keywords : \n 
~~ refnames = keywords [ "refnames" ] . strip ( ) \n 
if refnames . startswith ( "$Format" ) : \n 
~~ refs = set ( [ r . strip ( ) for r in refnames . strip ( "()" ) . split ( "," ) ] ) \n 
tags = set ( [ r [ len ( TAG ) : ] for r in refs if r . startswith ( TAG ) ] ) \n 
if not tags : \n 
~~~ tags = set ( [ r for r in refs if re . search ( , r ) ] ) \n 
~~ ~~ if verbose : \n 
~~ for ref in sorted ( tags ) : \n 
~~~ if ref . startswith ( tag_prefix ) : \n 
~~~ r = ref [ len ( tag_prefix ) : ] \n 
~~ return { "version" : r , \n 
"full" : keywords [ "full" ] . strip ( ) } \n 
~~ return { "version" : keywords [ "full" ] . strip ( ) , \n 
~~ def git_versions_from_vcs ( tag_prefix , root , verbose = False ) : \n 
~~~ if not os . path . exists ( os . path . join ( root , ".git" ) ) : \n 
~~ GITS = [ "git" ] \n 
if sys . platform == "win32" : \n 
~~~ GITS = [ "git.cmd" , "git.exe" ] \n 
~~ stdout = run_command ( GITS , [ "describe" , "--tags" , "--dirty" , "--always" ] , \n 
cwd = root ) \n 
if stdout is None : \n 
~~~ return { } \n 
~~ if not stdout . startswith ( tag_prefix ) : \n 
~~ tag = stdout [ len ( tag_prefix ) : ] \n 
stdout = run_command ( GITS , [ "rev-parse" , "HEAD" ] , cwd = root ) \n 
~~ full = stdout . strip ( ) \n 
if tag . endswith ( "-dirty" ) : \n 
~~~ full += "-dirty" \n 
~~ return { "version" : tag , "full" : full } \n 
~~ def do_vcs_install ( manifest_in , versionfile_source , ipy ) : \n 
~~~ GITS = [ "git" ] \n 
~~ files = [ manifest_in , versionfile_source ] \n 
if ipy : \n 
~~~ files . append ( ipy ) \n 
~~~ me = __file__ \n 
if me . endswith ( ".pyc" ) or me . endswith ( ".pyo" ) : \n 
~~~ me = os . path . splitext ( me ) [ 0 ] + ".py" \n 
~~ versioneer_file = os . path . relpath ( me ) \n 
~~~ versioneer_file = "versioneer.py" \n 
~~ files . append ( versioneer_file ) \n 
present = False \n 
~~~ f = open ( ".gitattributes" , "r" ) \n 
~~~ if line . strip ( ) . startswith ( versionfile_source ) : \n 
~~~ if "export-subst" in line . strip ( ) . split ( ) [ 1 : ] : \n 
~~~ present = True \n 
~~ if not present : \n 
~~~ f = open ( ".gitattributes" , "a+" ) \n 
files . append ( ".gitattributes" ) \n 
~~ run_command ( GITS , [ "add" , "--" ] + files ) \n 
~~ def versions_from_parentdir ( parentdir_prefix , root , verbose = False ) : \n 
~~~ dirname = os . path . basename ( root ) \n 
if not dirname . startswith ( parentdir_prefix ) : \n 
( root , dirname , parentdir_prefix ) ) \n 
~~ return { "version" : dirname [ len ( parentdir_prefix ) : ] , "full" : "" } \n 
DEFAULT = { "version" : "unknown" , "full" : "unknown" } \n 
def versions_from_file ( filename ) : \n 
~~~ versions = { } \n 
~~~ with open ( filename ) as f : \n 
~~~ for line in f . readlines ( ) : \n 
~~~ versions [ "version" ] = mo . group ( 1 ) \n 
~~~ versions [ "full" ] = mo . group ( 1 ) \n 
~~ ~~ ~~ ~~ except EnvironmentError : \n 
~~ return versions \n 
~~ def write_to_version_file ( filename , versions ) : \n 
~~~ with open ( filename , "w" ) as f : \n 
~~~ f . write ( SHORT_VERSION_PY % versions ) \n 
~~ def get_root ( ) : \n 
~~~ return os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
~~~ return os . path . dirname ( os . path . abspath ( sys . argv [ 0 ] ) ) \n 
~~ ~~ def vcs_function ( vcs , suffix ) : \n 
~~~ return getattr ( sys . modules [ __name__ ] , % ( vcs , suffix ) , None ) \n 
~~ def get_versions ( default = DEFAULT , verbose = False ) : \n 
root = get_root ( ) \n 
versionfile_abs = os . path . join ( root , versionfile_source ) \n 
get_keywords_f = vcs_function ( VCS , "get_keywords" ) \n 
versions_from_keywords_f = vcs_function ( VCS , "versions_from_keywords" ) \n 
if get_keywords_f and versions_from_keywords_f : \n 
~~~ vcs_keywords = get_keywords_f ( versionfile_abs ) \n 
ver = versions_from_keywords_f ( vcs_keywords , tag_prefix ) \n 
if ver : \n 
return ver \n 
~~ ~~ ver = versions_from_file ( versionfile_abs ) \n 
~~ versions_from_vcs_f = vcs_function ( VCS , "versions_from_vcs" ) \n 
if versions_from_vcs_f : \n 
~~~ ver = versions_from_vcs_f ( tag_prefix , root , verbose ) \n 
~~ ~~ ver = versions_from_parentdir ( parentdir_prefix , root , verbose ) \n 
return default \n 
~~ def get_version ( verbose = False ) : \n 
~~~ return get_versions ( verbose = verbose ) [ "version" ] \n 
~~ class cmd_version ( Command ) : \n 
user_options = [ ] \n 
boolean_options = [ ] \n 
def initialize_options ( self ) : \n 
~~ def finalize_options ( self ) : \n 
~~ def run ( self ) : \n 
~~~ ver = get_version ( verbose = True ) \n 
~~ ~~ class cmd_build ( _build ) : \n 
~~~ def run ( self ) : \n 
~~~ versions = get_versions ( verbose = True ) \n 
_build . run ( self ) \n 
if versionfile_build : \n 
~~~ target_versionfile = os . path . join ( self . build_lib , versionfile_build ) \n 
os . unlink ( target_versionfile ) \n 
with open ( target_versionfile , "w" ) as f : \n 
~~~ from cx_Freeze . dist import build_exe as _build_exe \n 
class cmd_build_exe ( _build_exe ) : \n 
target_versionfile = versionfile_source \n 
~~ _build_exe . run ( self ) \n 
with open ( versionfile_source , "w" ) as f : \n 
LONG = LONG_VERSION_PY [ VCS ] \n 
f . write ( LONG % { "DOLLAR" : "$" , \n 
"TAG_PREFIX" : tag_prefix , \n 
"PARENTDIR_PREFIX" : parentdir_prefix , \n 
"VERSIONFILE_SOURCE" : versionfile_source , \n 
} ) \n 
~~ ~~ ~~ ~~ class cmd_sdist ( _sdist ) : \n 
self . _versioneer_generated_versions = versions \n 
self . distribution . metadata . version = versions [ "version" ] \n 
return _sdist . run ( self ) \n 
~~ def make_release_tree ( self , base_dir , files ) : \n 
~~~ _sdist . make_release_tree ( self , base_dir , files ) \n 
target_versionfile = os . path . join ( base_dir , versionfile_source ) \n 
~~~ f . write ( SHORT_VERSION_PY % self . _versioneer_generated_versions ) \n 
class cmd_update_files ( Command ) : \n 
~~ ipy = os . path . join ( os . path . dirname ( versionfile_source ) , "__init__.py" ) \n 
if os . path . exists ( ipy ) : \n 
~~~ with open ( ipy , "r" ) as f : \n 
~~~ old = f . read ( ) \n 
~~ ~~ except EnvironmentError : \n 
~~~ old = "" \n 
~~ if INIT_PY_SNIPPET not in old : \n 
with open ( ipy , "a" ) as f : \n 
~~~ f . write ( INIT_PY_SNIPPET ) \n 
ipy = None \n 
~~ manifest_in = os . path . join ( get_root ( ) , "MANIFEST.in" ) \n 
simple_includes = set ( ) \n 
~~~ with open ( manifest_in , "r" ) as f : \n 
~~~ for include in line . split ( ) [ 1 : ] : \n 
~~~ simple_includes . add ( include ) \n 
~~ ~~ ~~ ~~ ~~ except EnvironmentError : \n 
~~ if "versioneer.py" not in simple_includes : \n 
with open ( manifest_in , "a" ) as f : \n 
~~ if versionfile_source not in simple_includes : \n 
versionfile_source ) \n 
~~ do_vcs_install ( manifest_in , versionfile_source , ipy ) \n 
~~ ~~ def get_cmdclass ( ) : \n 
~~~ cmds = { : cmd_version , \n 
: cmd_update_files , \n 
: cmd_build , \n 
: cmd_sdist , \n 
~~~ cmds [ ] = cmd_build_exe \n 
del cmds [ ] \n 
~~ return cmds \n 
~~ from paramz import Param \n 
from . priorizable import Priorizable \n 
from paramz . transformations import __fixed__ \n 
import logging , numpy as np \n 
class Param ( Param , Priorizable ) : \n 
from ... util . linalg import pdinv \n 
from . posterior import Posterior \n 
from . import LatentFunctionInference \n 
log_2_pi = np . log ( 2 * np . pi ) \n 
class VarGauss ( LatentFunctionInference ) : \n 
def __init__ ( self , alpha , beta ) : \n 
self . alpha , self . beta = alpha , beta \n 
~~ def inference ( self , kern , X , likelihood , Y , mean_function = None , Y_metadata = None , Z = None ) : \n 
~~~ if mean_function is not None : \n 
~~~ raise NotImplementedError \n 
~~ num_data , output_dim = Y . shape \n 
K = kern . K ( X ) \n 
m = K . dot ( self . alpha ) \n 
KB = K * self . beta [ : , None ] \n 
BKB = KB * self . beta [ None , : ] \n 
A = np . eye ( num_data ) + BKB \n 
Ai , LA , _ , Alogdet = pdinv ( A ) \n 
F , dF_dm , dF_dv , dF_dthetaL = likelihood . variational_expectations ( Y , m , var , Y_metadata = Y_metadata if dF_dthetaL is not None : \n 
~~~ dL_dthetaL = dF_dthetaL . sum ( 1 ) . sum ( 1 ) \n 
~~~ dL_dthetaL = np . array ( [ ] ) \n 
~~ dF_da = np . dot ( K , dF_dm ) \n 
SigmaB = Sigma * self . beta \n 
dF_db = - 2 * np . sum ( Sigma ** 2 * ( dF_dv * self . beta ) , 0 ) \n 
KL = 0.5 * ( Alogdet + np . trace ( Ai ) - num_data + np . sum ( m * self . alpha ) ) \n 
dKL_da = m \n 
A_A2 = Ai - Ai . dot ( Ai ) \n 
dKL_db = np . diag ( np . dot ( KB . T , A_A2 ) ) \n 
log_marginal = F . sum ( ) - KL \n 
self . alpha . gradient = dF_da - dKL_da \n 
self . beta . gradient = dF_db - dKL_db \n 
dKL_dK = 0.5 * ( self . alpha * self . alpha . T + self . beta [ : , None ] * self . beta [ None , : ] * A_A2 ) \n 
tmp = Ai * self . beta [ : , None ] / self . beta [ None , : ] \n 
dF_dK = self . alpha * dF_dm . T + np . dot ( tmp * dF_dv , tmp . T ) \n 
return Posterior ( mean = m , cov = Sigma , K = K ) , log_marginal , { : dF_dK - dKL_dK , : dL_dthetaL } \n 
~~ ~~ import numpy as np \n 
from . kern import Kern \n 
from ... util . linalg import mdot \n 
from ... util . decorators import silence_errors \n 
from ... core . parameterization . param import Param \n 
from paramz . transformations import Logexp \n 
class Periodic ( Kern ) : \n 
super ( Periodic , self ) . __init__ ( input_dim , active_dims , name ) \n 
self . input_dim = input_dim \n 
self . lower , self . upper = lower , upper \n 
self . n_freq = n_freq \n 
self . n_basis = 2 * n_freq \n 
self . variance = Param ( , np . float64 ( variance ) , Logexp ( ) ) \n 
self . lengthscale = Param ( , np . float64 ( lengthscale ) , Logexp ( ) ) \n 
self . period = Param ( , np . float64 ( period ) , Logexp ( ) ) \n 
self . link_parameters ( self . variance , self . lengthscale , self . period ) \n 
~~ def _cos ( self , alpha , omega , phase ) : \n 
~~~ def f ( x ) : \n 
~~~ return alpha * np . cos ( omega * x + phase ) \n 
~~ return f \n 
~~ @ silence_errors \n 
def _cos_factorization ( self , alpha , omega , phase ) : \n 
~~~ r1 = np . sum ( alpha * np . cos ( phase ) , axis = 1 ) [ : , None ] \n 
r2 = np . sum ( alpha * np . sin ( phase ) , axis = 1 ) [ : , None ] \n 
r = np . sqrt ( r1 ** 2 + r2 ** 2 ) \n 
psi = np . where ( r1 != 0 , ( np . arctan ( r2 / r1 ) + ( r1 < 0. ) * np . pi ) , np . arcsin ( r2 ) ) \n 
return r , omega [ : , 0 : 1 ] , psi \n 
def _int_computation ( self , r1 , omega1 , phi1 , r2 , omega2 , phi2 ) : \n 
~~~ Gint1 = 1. / ( omega1 + omega2 . T ) * ( np . sin ( ( omega1 + omega2 . T ) * self . upper + phi1 + phi2 . T ) - np . sin ( ( omega1 Gint2 = 1. / ( omega1 + omega2 . T ) * ( np . sin ( ( omega1 + omega2 . T ) * self . upper + phi1 + phi2 . T ) - np . sin ( ( omega1 Gint = np . dot ( r1 , r2 . T ) / 2 * np . where ( np . isnan ( Gint1 ) , Gint2 , Gint1 ) \n 
return Gint \n 
~~ def K ( self , X , X2 = None ) : \n 
~~~ FX = self . _cos ( self . basis_alpha [ None , : ] , self . basis_omega [ None , : ] , self . basis_phi [ None , : ] ) ( X ) \n 
if X2 is None : \n 
~~~ FX2 = FX \n 
~~~ FX2 = self . _cos ( self . basis_alpha [ None , : ] , self . basis_omega [ None , : ] , self . basis_phi [ None , : ] ~~ return mdot ( FX , self . Gi , FX2 . T ) \n 
~~ def Kdiag ( self , X ) : \n 
~~~ return np . diag ( self . K ( X ) ) \n 
~~ ~~ class PeriodicExponential ( Periodic ) : \n 
def __init__ ( self , input_dim = 1 , variance = 1. , lengthscale = 1. , period = 2. * np . pi , n_freq = 10 , lower = 0. ~~~ super ( PeriodicExponential , self ) . __init__ ( input_dim , variance , lengthscale , period , n_freq , \n 
~~ def parameters_changed ( self ) : \n 
~~~ self . a = [ 1. / self . lengthscale , 1. ] \n 
self . b = [ 1 ] \n 
self . basis_alpha = np . ones ( ( self . n_basis , ) ) \n 
self . basis_omega = ( 2 * np . pi * np . arange ( 1 , self . n_freq + 1 ) / self . period ) . repeat ( 2 ) \n 
self . basis_phi = np . zeros ( self . n_freq * 2 ) \n 
self . basis_phi [ : : 2 ] = - np . pi / 2 \n 
self . G = self . Gram_matrix ( ) \n 
self . Gi = np . linalg . inv ( self . G ) \n 
~~ def Gram_matrix ( self ) : \n 
~~~ La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega ) ) \n 
Lo = np . column_stack ( ( self . basis_omega , self . basis_omega ) ) \n 
Lp = np . column_stack ( ( self . basis_phi , self . basis_phi + np . pi / 2 ) ) \n 
r , omega , phi = self . _cos_factorization ( La , Lo , Lp ) \n 
Gint = self . _int_computation ( r , omega , phi , r , omega , phi ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : return ( self . lengthscale / ( 2 * self . variance ) * Gint + 1. / self . variance * np . dot ( Flower , Flower . T ) ) \n 
def update_gradients_full ( self , dL_dK , X , X2 = None ) : \n 
FX = self . _cos ( self . basis_alpha [ None , : ] , self . basis_omega [ None , : ] , self . basis_phi [ None , : ] ) ( X ) FX2 = self . _cos ( self . basis_alpha [ None , : ] , self . basis_omega [ None , : ] , self . basis_phi [ None , : ] ) ( X2 \n 
La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega ) ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : \n 
#dK_dvar \n 
dK_dvar = 1. / self . variance * mdot ( FX , self . Gi , FX2 . T ) \n 
#dK_dlen \n 
da_dlen = [ - 1. / self . lengthscale ** 2 , 0. ] \n 
dLa_dlen = np . column_stack ( ( da_dlen [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , da_dlen [ 1 ] * self . basis_omega r1 , omega1 , phi1 = self . _cos_factorization ( dLa_dlen , Lo , Lp ) \n 
dGint_dlen = self . _int_computation ( r1 , omega1 , phi1 , r , omega , phi ) \n 
dGint_dlen = dGint_dlen + dGint_dlen . T \n 
dG_dlen = 1. / 2 * Gint + self . lengthscale / 2 * dGint_dlen \n 
dK_dlen = - mdot ( FX , self . Gi , dG_dlen / self . variance , self . Gi , FX2 . T ) \n 
#dK_dper \n 
dFX_dper = self . _cos ( - self . basis_alpha [ None , : ] * self . basis_omega [ None , : ] / self . period * X , self dFX2_dper = self . _cos ( - self . basis_alpha [ None , : ] * self . basis_omega [ None , : ] / self . period * X2 , self \n 
dLa_dper = np . column_stack ( ( - self . a [ 0 ] * self . basis_omega / self . period , - self . a [ 1 ] * self . basis_omega dLp_dper = np . column_stack ( ( self . basis_phi + np . pi / 2 , self . basis_phi + np . pi ) ) \n 
r1 , omega1 , phi1 = self . _cos_factorization ( dLa_dper , Lo , dLp_dper ) \n 
IPPint = np . where ( np . isnan ( IPPint1 ) , IPPint2 , IPPint1 ) \n 
dLa_dper2 = np . column_stack ( ( - self . a [ 1 ] * self . basis_omega / self . period ) ) \n 
dLp_dper2 = np . column_stack ( ( self . basis_phi + np . pi / 2 ) ) \n 
r2 , omega2 , phi2 = dLa_dper2 . T , Lo [ : , 0 : 1 ] , dLp_dper2 . T \n 
dGint_dper = np . dot ( r , r1 . T ) / 2 * ( IPPprim - IPPint ) + self . _int_computation ( r2 , omega2 , phi2 , r dGint_dper = dGint_dper + dGint_dper . T \n 
dFlower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega / self . period \n 
dG_dper = 1. / self . variance * ( self . lengthscale / 2 * dGint_dper + self . b [ 0 ] * ( np . dot ( dFlower_dper , Flower \n 
dK_dper = mdot ( dFX_dper , self . Gi , FX2 . T ) - mdot ( FX , self . Gi , dG_dper , self . Gi , FX2 . T ) + mdot ( FX , self \n 
self . variance . gradient = np . sum ( dK_dvar * dL_dK ) \n 
self . lengthscale . gradient = np . sum ( dK_dlen * dL_dK ) \n 
self . period . gradient = np . sum ( dK_dper * dL_dK ) \n 
~~ ~~ class PeriodicMatern32 ( Periodic ) : \n 
def __init__ ( self , input_dim = 1 , variance = 1. , lengthscale = 1. , period = 2. * np . pi , n_freq = 10 , lower = 0. ~~~ super ( PeriodicMatern32 , self ) . __init__ ( input_dim , variance , lengthscale , period , n_freq , lower ~~ def parameters_changed ( self ) : \n 
~~~ self . a = [ 3. / self . lengthscale ** 2 , 2 * np . sqrt ( 3 ) / self . lengthscale , 1. ] \n 
self . b = [ 1 , self . lengthscale ** 2 / 3 ] \n 
~~~ La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega , self . a [ Lo = np . column_stack ( ( self . basis_omega , self . basis_omega , self . basis_omega ) ) \n 
Lp = np . column_stack ( ( self . basis_phi , self . basis_phi + np . pi / 2 , self . basis_phi + np . pi ) ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : F1lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega , self . basis_omega , self . basis_phi return ( self . lengthscale ** 3 / ( 12 * np . sqrt ( 3 ) * self . variance ) * Gint + 1. / self . variance * np . dot ( Flower \n 
def update_gradients_full ( self , dL_dK , X , X2 ) : \n 
La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega , self . a [ Lo = np . column_stack ( ( self . basis_omega , self . basis_omega , self . basis_omega ) ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : F1lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega , self . basis_omega , self . basis_phi \n 
da_dlen = [ - 6 / self . lengthscale ** 3 , - 2 * np . sqrt ( 3 ) / self . lengthscale ** 2 , 0. ] \n 
db_dlen = [ 0. , 2 * self . lengthscale / 3. ] \n 
dG_dlen = self . lengthscale ** 2 / ( 4 * np . sqrt ( 3 ) ) * Gint + self . lengthscale ** 3 / ( 12 * np . sqrt ( 3 ) ) * dGint_dlen dK_dlen = - mdot ( FX , self . Gi , dG_dlen / self . variance , self . Gi , FX2 . T ) \n 
dLa_dper = np . column_stack ( ( - self . a [ 0 ] * self . basis_omega / self . period , - self . a [ 1 ] * self . basis_omega dLp_dper = np . column_stack ( ( self . basis_phi + np . pi / 2 , self . basis_phi + np . pi , self . basis_phi + np . pi r1 , omega1 , phi1 = self . _cos_factorization ( dLa_dper , Lo , dLp_dper ) \n 
IPPprim1 = self . upper * ( 1. / ( omega + omega1 . T ) * np . cos ( ( omega + omega1 . T ) * self . upper + phi + phi1 . T - np IPPprim1 -= self . lower * ( 1. / ( omega + omega1 . T ) * np . cos ( ( omega + omega1 . T ) * self . lower + phi + phi1 . T - np IPPprim2 = self . upper * ( 1. / ( omega + omega1 . T ) * np . cos ( ( omega + omega1 . T ) * self . upper + phi + phi1 . T - np IPPprim2 -= self . lower * ( 1. / ( omega + omega1 . T ) * np . cos ( ( omega + omega1 . T ) * self . lower + phi + phi1 . T - np IPPprim = np . where ( np . isnan ( IPPprim1 ) , IPPprim2 , IPPprim1 ) \n 
IPPint1 = 1. / ( omega + omega1 . T ) ** 2 * np . cos ( ( omega + omega1 . T ) * self . upper + phi + phi1 . T - np . pi ) + 1. IPPint1 -= 1. / ( omega + omega1 . T ) ** 2 * np . cos ( ( omega + omega1 . T ) * self . lower + phi + phi1 . T - np . pi ) + 1. IPPint2 = 1. / ( omega + omega1 . T ) ** 2 * np . cos ( ( omega + omega1 . T ) * self . upper + phi + phi1 . T - np . pi ) + 1. IPPint2 -= 1. / ( omega + omega1 . T ) ** 2 * np . cos ( ( omega + omega1 . T ) * self . lower + phi + phi1 . T - np . pi ) + 1. IPPint = np . where ( np . isnan ( IPPint1 ) , IPPint2 , IPPint1 ) \n 
dLa_dper2 = np . column_stack ( ( - self . a [ 1 ] * self . basis_omega / self . period , - 2 * self . a [ 2 ] * self . basis_omega dLp_dper2 = np . column_stack ( ( self . basis_phi + np . pi / 2 , self . basis_phi + np . pi ) ) \n 
r2 , omega2 , phi2 = self . _cos_factorization ( dLa_dper2 , Lo [ : , 0 : 2 ] , dLp_dper2 ) \n 
dGint_dper = np . dot ( r , r1 . T ) / 2 * ( IPPprim - IPPint ) + self . _int_computation ( r2 , omega2 , phi2 , dGint_dper = dGint_dper + dGint_dper . T \n 
dFlower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega / self . period dF1lower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega ** 2 / self . period \n 
dG_dper = 1. / self . variance * ( self . lengthscale ** 3 / ( 12 * np . sqrt ( 3 ) ) * dGint_dper + self . b [ 0 ] * ( np . dot \n 
~~ ~~ class PeriodicMatern52 ( Periodic ) : \n 
def __init__ ( self , input_dim = 1 , variance = 1. , lengthscale = 1. , period = 2. * np . pi , n_freq = 10 , lower = 0. ~~~ super ( PeriodicMatern52 , self ) . __init__ ( input_dim , variance , lengthscale , period , n_freq , lower \n 
~~~ self . a = [ 5 * np . sqrt ( 5 ) / self . lengthscale ** 3 , 15. / self . lengthscale ** 2 , 3 * np . sqrt ( 5 ) / self . lengthscale self . b = [ 9. / 8 , 9 * self . lengthscale ** 4 / 200. , 3 * self . lengthscale ** 2 / 5. , 3 * self . lengthscale ** 2 \n 
self . basis_alpha = np . ones ( ( 2 * self . n_freq , ) ) \n 
~~~ La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega , self . Lo = np . column_stack ( ( self . basis_omega , self . basis_omega , self . basis_omega , self . basis_omega Lp = np . column_stack ( ( self . basis_phi , self . basis_phi + np . pi / 2 , self . basis_phi + np . pi , self . basis_phi r , omega , phi = self . _cos_factorization ( La , Lo , Lp ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : F1lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega , self . basis_omega , self . basis_phi F2lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega ** 2 , self . basis_omega , self . basis_phi lower_terms = self . b [ 0 ] * np . dot ( Flower , Flower . T ) + self . b [ 1 ] * np . dot ( F2lower , F2lower . T ) + self return ( 3 * self . lengthscale ** 5 / ( 400 * np . sqrt ( 5 ) * self . variance ) * Gint + 1. / self . variance * lower_terms \n 
~~~ if X2 is None : X2 = X \n 
La = np . column_stack ( ( self . a [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , self . a [ 1 ] * self . basis_omega , self . Lo = np . column_stack ( ( self . basis_omega , self . basis_omega , self . basis_omega , self . basis_omega Lp = np . column_stack ( ( self . basis_phi , self . basis_phi + np . pi / 2 , self . basis_phi + np . pi , self . basis_phi r , omega , phi = self . _cos_factorization ( La , Lo , Lp ) \n 
Flower = np . array ( self . _cos ( self . basis_alpha , self . basis_omega , self . basis_phi ) ( self . lower ) ) [ : F1lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega , self . basis_omega , self . basis_phi F2lower = np . array ( self . _cos ( self . basis_alpha * self . basis_omega ** 2 , self . basis_omega , self . basis_phi \n 
da_dlen = [ - 3 * self . a [ 0 ] / self . lengthscale , - 2 * self . a [ 1 ] / self . lengthscale , - self . a [ 2 ] / self . lengthscale db_dlen = [ 0. , 4 * self . b [ 1 ] / self . lengthscale , 2 * self . b [ 2 ] / self . lengthscale , 2 * self . b [ 3 ] / self . dLa_dlen = np . column_stack ( ( da_dlen [ 0 ] * np . ones ( ( self . n_basis , 1 ) ) , da_dlen [ 1 ] * self . basis_omega r1 , omega1 , phi1 = self . _cos_factorization ( dLa_dlen , Lo , Lp ) \n 
dlower_terms_dlen = db_dlen [ 0 ] * np . dot ( Flower , Flower . T ) + db_dlen [ 1 ] * np . dot ( F2lower , F2lower . T dG_dlen = 15 * self . lengthscale ** 4 / ( 400 * np . sqrt ( 5 ) ) * Gint + 3 * self . lengthscale ** 5 / ( 400 * np . sqrt ( dK_dlen = - mdot ( FX , self . Gi , dG_dlen / self . variance , self . Gi , FX2 . T ) \n 
dLa_dper2 = np . column_stack ( ( - self . a [ 1 ] * self . basis_omega / self . period , - 2 * self . a [ 2 ] * self . basis_omega dLp_dper2 = np . column_stack ( ( self . basis_phi + np . pi / 2 , self . basis_phi + np . pi , self . basis_phi + np r2 , omega2 , phi2 = self . _cos_factorization ( dLa_dper2 , Lo [ : , 0 : 2 ] , dLp_dper2 ) \n 
dFlower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega / self . period dF1lower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega ** 2 / self . period dF2lower_dper = np . array ( self . _cos ( - self . lower * self . basis_alpha * self . basis_omega ** 3 / self . period \n 
dlower_terms_dper = self . b [ 0 ] * ( np . dot ( dFlower_dper , Flower . T ) + np . dot ( Flower . T , dFlower_dper dlower_terms_dper += self . b [ 1 ] * ( np . dot ( dF2lower_dper , F2lower . T ) + np . dot ( F2lower , dF2lower_dper dlower_terms_dper += self . b [ 2 ] * ( np . dot ( dF1lower_dper , F1lower . T ) + np . dot ( F1lower , dF1lower_dper dlower_terms_dper += self . b [ 3 ] * ( np . dot ( dF2lower_dper , Flower . T ) + np . dot ( F2lower , dFlower_dper dlower_terms_dper += self . b [ 4 ] * ( np . dot ( dFlower_dper , F2lower . T ) + np . dot ( Flower , dF2lower_dper \n 
dG_dper = 1. / self . variance * ( 3 * self . lengthscale ** 5 / ( 400 * np . sqrt ( 5 ) ) * dGint_dper + 0.5 * dlower_terms_dper dK_dper = mdot ( dFX_dper , self . Gi , FX2 . T ) - mdot ( FX , self . Gi , dG_dper , self . Gi , FX2 . T ) + mdot ( FX , self \n 
~~ ~~ from GPy . core . mapping import Mapping \n 
from GPy . core import Param \n 
class PiecewiseLinear ( Mapping ) : \n 
def __init__ ( self , input_dim , output_dim , values , breaks , name = ) : \n 
~~~ assert input_dim == 1 \n 
assert output_dim == 1 \n 
Mapping . __init__ ( self , input_dim , output_dim , name ) \n 
values , breaks = np . array ( values ) . flatten ( ) , np . array ( breaks ) . flatten ( ) \n 
assert values . size == breaks . size \n 
self . values = Param ( , values ) \n 
self . breaks = Param ( , breaks ) \n 
self . link_parameter ( self . values ) \n 
self . link_parameter ( self . breaks ) \n 
~~~ self . order = np . argsort ( self . breaks ) * 1 \n 
self . reverse_order = np . zeros_like ( self . order ) \n 
self . reverse_order [ self . order ] = np . arange ( self . order . size ) \n 
self . sorted_breaks = self . breaks [ self . order ] \n 
self . sorted_values = self . values [ self . order ] \n 
self . grads = np . diff ( self . sorted_values ) / np . diff ( self . sorted_breaks ) \n 
~~ def f ( self , X ) : \n 
~~~ x = X . flatten ( ) \n 
y = x . copy ( ) \n 
y [ x < self . sorted_breaks [ 0 ] ] = x [ x < self . sorted_breaks [ 0 ] ] + self . sorted_values [ 0 ] - self . sorted_breaks \n 
y [ x > self . sorted_breaks [ - 1 ] ] = x [ x > self . sorted_breaks [ - 1 ] ] + self . sorted_values [ - 1 ] - self . sorted_breaks \n 
for low , up , g , v in zip ( self . sorted_breaks [ : - 1 ] , self . sorted_breaks [ 1 : ] , self . grads , self . ~~~ i = np . logical_and ( x > low , x < up ) \n 
y [ i ] = v + ( x [ i ] - low ) * g \n 
~~ return y . reshape ( - 1 , 1 ) \n 
~~ def update_gradients ( self , dL_dF , X ) : \n 
dL_dF = dL_dF . flatten ( ) \n 
dL_db = np . zeros ( self . sorted_breaks . size ) \n 
dL_dv = np . zeros ( self . sorted_values . size ) \n 
xx = x [ index ] \n 
grad = dL_dF [ index ] \n 
span = up - low \n 
dL_dv [ i ] += np . sum ( grad * ( ( low - xx ) / span + 1 ) ) \n 
dL_dv [ i + 1 ] += np . sum ( grad * ( xx - low ) / span ) \n 
dL_db [ i ] += np . sum ( grad * g * ( xx - up ) / span ) \n 
dL_db [ i + 1 ] += np . sum ( grad * g * ( low - xx ) / span ) \n 
~~ dL_db [ 0 ] -= np . sum ( dL_dF [ x < self . sorted_breaks [ 0 ] ] ) \n 
dL_db [ - 1 ] -= np . sum ( dL_dF [ x > self . sorted_breaks [ - 1 ] ] ) \n 
dL_dv [ 0 ] += np . sum ( dL_dF [ x < self . sorted_breaks [ 0 ] ] ) \n 
dL_dv [ - 1 ] += np . sum ( dL_dF [ x > self . sorted_breaks [ - 1 ] ] ) \n 
self . breaks . gradient = dL_db [ self . reverse_order ] \n 
self . values . gradient = dL_dv [ self . reverse_order ] \n 
~~ def gradients_X ( self , dL_dF , X ) : \n 
dL_dX [ i ] = dL_dF [ i ] * g \n 
~~ return dL_dX . reshape ( - 1 , 1 ) \n 
from . . util . warping_functions import * \n 
from . . core import GP \n 
from . . import likelihoods \n 
from GPy . util . warping_functions import TanhWarpingFunction_d \n 
from GPy import kern \n 
class WarpedGP ( GP ) : \n 
~~~ def __init__ ( self , X , Y , kernel = None , warping_function = None , warping_terms = 3 ) : \n 
~~~ if kernel is None : \n 
~~~ kernel = kern . RBF ( X . shape [ 1 ] ) \n 
~~ if warping_function == None : \n 
~~~ self . warping_function = TanhWarpingFunction_d ( warping_terms ) \n 
self . warping_params = ( np . random . randn ( self . warping_function . n_terms * 3 + 1 ) * 1 ) \n 
~~~ self . warping_function = warping_function \n 
~~ self . scale_data = False \n 
if self . scale_data : \n 
~~~ Y = self . _scale_data ( Y ) \n 
~~ self . Y_untransformed = Y . copy ( ) \n 
self . predict_in_warped_space = True \n 
likelihood = likelihoods . Gaussian ( ) \n 
GP . __init__ ( self , X , self . transform_data ( ) , likelihood = likelihood , kernel = kernel ) \n 
self . link_parameter ( self . warping_function ) \n 
~~ def _scale_data ( self , Y ) : \n 
~~~ self . _Ymax = Y . max ( ) \n 
self . _Ymin = Y . min ( ) \n 
return ( Y - self . _Ymin ) / ( self . _Ymax - self . _Ymin ) - 0.5 \n 
~~ def _unscale_data ( self , Y ) : \n 
~~~ return ( Y + 0.5 ) * ( self . _Ymax - self . _Ymin ) + self . _Ymin \n 
~~~ self . Y [ : ] = self . transform_data ( ) \n 
super ( WarpedGP , self ) . parameters_changed ( ) \n 
Kiy = self . posterior . woodbury_vector . flatten ( ) \n 
grad_y = self . warping_function . fgrad_y ( self . Y_untransformed ) \n 
grad_y_psi , grad_psi = self . warping_function . fgrad_y_psi ( self . Y_untransformed , \n 
return_covar_chain = True ) \n 
djac_dpsi = ( ( 1.0 / grad_y [ : , : , None , None ] ) * grad_y_psi ) . sum ( axis = 0 ) . sum ( axis = 0 ) \n 
dquad_dpsi = ( Kiy [ : , None , None , None ] * grad_psi ) . sum ( axis = 0 ) . sum ( axis = 0 ) \n 
warping_grads = - dquad_dpsi + djac_dpsi \n 
self . warping_function . psi . gradient [ : ] = warping_grads [ : , : - 1 ] \n 
self . warping_function . d . gradient [ : ] = warping_grads [ 0 , - 1 ] \n 
~~ def transform_data ( self ) : \n 
~~~ Y = self . warping_function . f ( self . Y_untransformed . copy ( ) ) . copy ( ) \n 
return Y \n 
~~ def log_likelihood ( self ) : \n 
~~~ ll = GP . log_likelihood ( self ) \n 
jacobian = self . warping_function . fgrad_y ( self . Y_untransformed ) \n 
return ll + np . log ( jacobian ) . sum ( ) \n 
~~ def plot_warping ( self ) : \n 
~~~ self . warping_function . plot ( self . Y_untransformed . min ( ) , self . Y_untransformed . max ( ) ) \n 
~~ def _get_warped_term ( self , mean , std , gh_samples , pred_init = None ) : \n 
~~~ arg1 = gh_samples . dot ( std . T ) * np . sqrt ( 2 ) \n 
arg2 = np . ones ( shape = gh_samples . shape ) . dot ( mean . T ) \n 
return self . warping_function . f_inv ( arg1 + arg2 , y = pred_init ) \n 
~~ def _get_warped_mean ( self , mean , std , pred_init = None , deg_gauss_hermite = 100 ) : \n 
gh_samples , gh_weights = np . polynomial . hermite . hermgauss ( deg_gauss_hermite ) \n 
gh_samples = gh_samples [ : , None ] \n 
gh_weights = gh_weights [ None , : ] \n 
return gh_weights . dot ( self . _get_warped_term ( mean , std , gh_samples ) ) / np . sqrt ( np . pi ) \n 
~~ def _get_warped_variance ( self , mean , std , pred_init = None , deg_gauss_hermite = 100 ) : \n 
arg1 = gh_weights . dot ( self . _get_warped_term ( mean , std , gh_samples , \n 
pred_init = pred_init ) ** 2 ) / np . sqrt ( np . pi ) \n 
arg2 = self . _get_warped_mean ( mean , std , pred_init = pred_init , \n 
deg_gauss_hermite = deg_gauss_hermite ) \n 
return arg1 - ( arg2 ** 2 ) \n 
~~ def predict ( self , Xnew , which_parts = , pred_init = None , full_cov = False , Y_metadata = None , \n 
median = False , deg_gauss_hermite = 100 ) : \n 
~~~ mu , var = GP . _raw_predict ( self , Xnew ) \n 
mean , var = self . likelihood . predictive_values ( mu , var ) \n 
if self . predict_in_warped_space : \n 
~~~ std = np . sqrt ( var ) \n 
if median : \n 
~~~ wmean = self . warping_function . f_inv ( mean , y = pred_init ) \n 
~~~ wmean = self . _get_warped_mean ( mean , std , pred_init = pred_init , \n 
deg_gauss_hermite = deg_gauss_hermite ) . T \n 
~~ wvar = self . _get_warped_variance ( mean , std , pred_init = pred_init , \n 
~~~ wmean = mean \n 
wvar = var \n 
~~ if self . scale_data : \n 
~~~ pred = self . _unscale_data ( pred ) \n 
~~ return wmean , wvar \n 
~~ def predict_quantiles ( self , X , quantiles = ( 2.5 , 97.5 ) , Y_metadata = None ) : \n 
m , v = self . _raw_predict ( X , full_cov = False ) \n 
if self . normalizer is not None : \n 
~~~ m , v = self . normalizer . inverse_mean ( m ) , self . normalizer . inverse_variance ( v ) \n 
~~ a , b = self . likelihood . predictive_quantiles ( m , v , quantiles , Y_metadata ) \n 
if not self . predict_in_warped_space : \n 
~~~ return [ a , b ] \n 
~~ new_a = self . warping_function . f_inv ( a ) \n 
new_b = self . warping_function . f_inv ( b ) \n 
return [ new_a , new_b ] \n 
~~~ X = np . random . randn ( 100 , 1 ) \n 
Y = np . sin ( X ) + np . random . randn ( 100 , 1 ) * 0.05 \n 
m = WarpedGP ( X , Y ) \n 
~~~ from matplotlib import pyplot as pb \n 
from matplotlib . patches import Polygon \n 
from matplotlib . collections import PatchCollection \n 
~~~ __IPYTHON__ \n 
pb . ion ( ) \n 
~~ import re \n 
def plot ( shape_records , facecolor = , edgecolor = , linewidths = .5 , ax = None , xlims = None , ylims = None ) : \n 
if ax is None : \n 
~~~ fig = pb . figure ( ) \n 
ax = fig . add_subplot ( 111 ) \n 
~~ for srec in shape_records : \n 
~~~ points = np . vstack ( srec . shape . points ) \n 
sparts = srec . shape . parts \n 
par = list ( sparts ) + [ points . shape [ 0 ] ] \n 
polygs = [ ] \n 
for pj in range ( len ( sparts ) ) : \n 
~~~ polygs . append ( Polygon ( points [ par [ pj ] : par [ pj + 1 ] ] ) ) \n 
~~ ax . add_collection ( PatchCollection ( polygs , facecolor = facecolor , edgecolor = edgecolor , linewidths \n 
~~ _box = np . vstack ( [ srec . shape . bbox for srec in shape_records ] ) \n 
minx , miny = np . min ( _box [ : , : 2 ] , 0 ) \n 
maxx , maxy = np . max ( _box [ : , 2 : ] , 0 ) \n 
if xlims is not None : \n 
~~~ minx , maxx = xlims \n 
~~ if ylims is not None : \n 
~~~ miny , maxy = ylims \n 
~~ ax . set_xlim ( minx , maxx ) \n 
ax . set_ylim ( miny , maxy ) \n 
~~ def string_match ( sf , regex , field = 2 ) : \n 
index = [ ] \n 
shape_records = [ ] \n 
for rec in enumerate ( sf . shapeRecords ( ) ) : \n 
~~~ m = re . search ( regex , rec [ 1 ] . record [ field ] ) \n 
if m is not None : \n 
~~~ index . append ( rec [ 0 ] ) \n 
shape_records . append ( rec [ 1 ] ) \n 
~~ ~~ return index , shape_records \n 
~~ def bbox_match ( sf , bbox , inside_only = True ) : \n 
A , B , C , D = bbox \n 
~~~ a , b , c , d = rec [ 1 ] . shape . bbox \n 
if inside_only : \n 
~~~ if A <= a and B <= b and C >= c and D >= d : \n 
~~~ cond1 = A <= a and B <= b and C >= a and D >= b \n 
cond2 = A <= c and B <= d and C >= c and D >= d \n 
cond3 = A <= a and D >= d and C >= a and B <= d \n 
cond4 = A <= c and D >= b and C >= c and B <= b \n 
cond5 = a <= C and b <= B and d >= D \n 
cond6 = c <= A and b <= B and d >= D \n 
cond7 = d <= B and a <= A and c >= C \n 
cond8 = b <= D and a <= A and c >= C \n 
if cond1 or cond2 or cond3 or cond4 or cond5 or cond6 or cond7 or cond8 : \n 
~~ ~~ ~~ return index , shape_records \n 
~~ def plot_bbox ( sf , bbox , inside_only = True ) : \n 
index , shape_records = bbox_match ( sf , bbox , inside_only ) \n 
plot ( shape_records , xlims = [ bbox [ 0 ] , bbox [ 2 ] ] , ylims = [ bbox [ 1 ] , bbox [ 3 ] ] ) \n 
~~ def plot_string_match ( sf , regex , field , ** kwargs ) : \n 
index , shape_records = string_match ( sf , regex , field ) \n 
plot ( shape_records , ** kwargs ) \n 
~~ def new_shape_string ( sf , name , regex , field = 2 , type = None ) : \n 
~~~ import shapefile \n 
if type is None : \n 
~~~ type = shapefile . POINT \n 
~~ newshp = shapefile . Writer ( shapeType = sf . shapeType ) \n 
newshp . autoBalance = 1 \n 
_fi = [ sf . fields [ j ] for j in index ] \n 
for f in _fi : \n 
~~~ newshp . field ( name = f [ 0 ] , fieldType = f [ 1 ] , size = f [ 2 ] , decimal = f [ 3 ] ) \n 
~~ _shre = shape_records \n 
for sr in _shre : \n 
~~~ _points = [ ] \n 
_parts = [ ] \n 
for point in sr . shape . points : \n 
~~~ _points . append ( point ) \n 
~~ _parts . append ( _points ) \n 
newshp . line ( parts = _parts ) \n 
newshp . records . append ( sr . record ) \n 
print ( len ( sr . record ) ) \n 
~~ newshp . save ( name ) \n 
print ( index ) \n 
~~ def apply_bbox ( sf , ax ) : \n 
limits = sf . bbox \n 
xlim = limits [ 0 ] , limits [ 2 ] \n 
ylim = limits [ 1 ] , limits [ 3 ] \n 
ax . set_xlim ( xlim ) \n 
ax . set_ylim ( ylim ) \n 
import unittest , itertools \n 
import tempfile \n 
from GPy . examples . dimensionality_reduction import mrd_simulation \n 
from GPy . core . parameterization . variational import NormalPosterior \n 
from GPy . models . gp_regression import GPRegression \n 
import GPy \n 
from nose import SkipTest \n 
def toy_model ( ) : \n 
~~~ X = np . linspace ( 0 , 1 , 50 ) [ : , None ] \n 
Y = np . sin ( X ) \n 
m = GPRegression ( X = X , Y = Y ) \n 
return m \n 
~~ class ListDictTestCase ( unittest . TestCase ) : \n 
~~~ def assertListDictEquals ( self , d1 , d2 , msg = None ) : \n 
~~~ for k , v in d1 . items ( ) : \n 
~~~ self . assertListEqual ( list ( v ) , list ( d2 [ k ] ) , msg ) \n 
~~ ~~ def assertArrayListEquals ( self , l1 , l2 ) : \n 
~~~ for a1 , a2 in zip ( l1 , l2 ) : \n 
~~~ np . testing . assert_array_equal ( a1 , a2 ) \n 
~~ ~~ ~~ class Test ( ListDictTestCase ) : \n 
~~~ @ SkipTest \n 
def test_load_pickle ( self ) : \n 
~~~ import os \n 
m = GPy . load ( os . path . join ( os . path . abspath ( os . path . split ( __file__ ) [ 0 ] ) , ) self . assertTrue ( m . checkgrad ( ) ) \n 
self . assertEqual ( m . log_likelihood ( ) , - 4.7351019830022087 ) \n 
~~ def test_model ( self ) : \n 
~~~ par = toy_model ( ) \n 
pcopy = par . copy ( ) \n 
self . assertListEqual ( par . param_array . tolist ( ) , pcopy . param_array . tolist ( ) ) \n 
np . testing . assert_allclose ( par . gradient_full , pcopy . gradient_full ) \n 
self . assertSequenceEqual ( str ( par ) , str ( pcopy ) ) \n 
self . assertIsNot ( par . param_array , pcopy . param_array ) \n 
self . assertIsNot ( par . gradient_full , pcopy . gradient_full ) \n 
self . assertTrue ( pcopy . checkgrad ( ) ) \n 
self . assert_ ( np . any ( pcopy . gradient != 0.0 ) ) \n 
with tempfile . TemporaryFile ( ) as f : \n 
~~~ par . pickle ( f ) \n 
f . seek ( 0 ) \n 
pcopy = pickle . load ( f ) \n 
~~ self . assertListEqual ( par . param_array . tolist ( ) , pcopy . param_array . tolist ( ) ) \n 
self . assert_ ( pcopy . checkgrad ( ) ) \n 
~~ def test_modelrecreation ( self ) : \n 
pcopy = GPRegression ( par . X . copy ( ) , par . Y . copy ( ) , kernel = par . kern . copy ( ) ) \n 
np . testing . assert_allclose ( par . param_array , pcopy . param_array ) \n 
np . testing . assert_allclose ( pcopy . param_array , par . param_array , atol = 1e-6 ) \n 
par . randomize ( ) \n 
~~ np . testing . assert_allclose ( par . param_array , pcopy . param_array ) \n 
np . testing . assert_allclose ( par . gradient_full , pcopy . gradient_full , atol = 1e-6 ) \n 
~~ def test_posterior ( self ) : \n 
~~~ X = np . random . randn ( 3 , 5 ) \n 
Xv = np . random . rand ( * X . shape ) \n 
par = NormalPosterior ( X , Xv ) \n 
par . gradient = 10 \n 
pcopy . gradient = 10 \n 
self . assertListEqual ( par . gradient_full . tolist ( ) , pcopy . gradient_full . tolist ( ) ) \n 
np . testing . assert_allclose ( pcopy . mean . gradient_full , 10 ) \n 
~~ def test_model_concat ( self ) : \n 
~~~ par = mrd_simulation ( optimize = 0 , plot = 0 , plot_sim = 0 ) \n 
self . assertTrue ( par . checkgrad ( ) ) \n 
~~ def _callback ( self , what , which ) : \n 
~~~ what . count += 1 \n 
~~~ import pylab \n 
import matplotlib \n 
~~ from numpy . linalg . linalg import LinAlgError \n 
from operator import setitem \n 
from functools import reduce \n 
class PCA ( object ) : \n 
def __init__ ( self , X ) : \n 
~~~ self . mu = None \n 
self . sigma = None \n 
X = self . center ( X ) \n 
if X . shape [ 0 ] >= X . shape [ 1 ] : \n 
~~~ self . eigvals , self . eigvectors = self . _primal_eig ( X ) \n 
~~~ self . eigvals , self . eigvectors = self . _dual_eig ( X ) \n 
~~ self . sort = numpy . argsort ( self . eigvals ) [ : : - 1 ] \n 
self . eigvals = self . eigvals [ self . sort ] \n 
self . eigvectors = self . eigvectors [ : , self . sort ] \n 
self . fracs = self . eigvals / self . eigvals . sum ( ) \n 
self . Q = self . eigvals . shape [ 0 ] \n 
~~ def center ( self , X ) : \n 
X = X . copy ( ) \n 
inan = numpy . isnan ( X ) \n 
if self . mu is None : \n 
~~~ X_ = numpy . ma . masked_array ( X , inan ) \n 
self . mu = X_ . mean ( 0 ) . base \n 
self . sigma = X_ . std ( 0 ) . base \n 
~~ reduce ( lambda y , x : setitem ( x [ 0 ] , x [ 1 ] , x [ 2 ] ) , zip ( X . T , inan . T , self . mu ) , None ) \n 
X = X - self . mu \n 
X = X / numpy . where ( self . sigma == 0 , 1e-30 , self . sigma ) \n 
return X \n 
~~ def _primal_eig ( self , X ) : \n 
~~~ return numpy . linalg . eigh ( numpy . einsum ( , X , X ) ) \n 
~~ def _dual_eig ( self , X ) : \n 
~~~ dual_eigvals , dual_eigvects = numpy . linalg . eigh ( numpy . einsum ( , X , X ) ) \n 
relevant_dimensions = numpy . argsort ( numpy . abs ( dual_eigvals ) ) [ - X . shape [ 1 ] : ] \n 
eigvals = dual_eigvals [ relevant_dimensions ] \n 
eigvects = dual_eigvects [ : , relevant_dimensions ] \n 
eigvects = ( 1. / numpy . sqrt ( X . shape [ 0 ] * numpy . abs ( eigvals ) ) ) * X . T . dot ( eigvects ) \n 
eigvects /= numpy . sqrt ( numpy . diag ( eigvects . T . dot ( eigvects ) ) ) \n 
return eigvals , eigvects \n 
~~ def project ( self , X , Q = None ) : \n 
if Q is None : \n 
~~~ Q = self . Q \n 
~~ if Q > X . shape [ 1 ] : \n 
~~ X = self . center ( X ) \n 
return X . dot ( self . eigvectors [ : , : Q ] ) \n 
~~ def plot_fracs ( self , Q = None , ax = None , fignum = None ) : \n 
from . . plotting import Tango \n 
Tango . reset ( ) \n 
col = Tango . nextMedium ( ) \n 
~~~ fig = pylab . figure ( fignum ) \n 
~~ if Q is None : \n 
~~ ticks = numpy . arange ( Q ) \n 
bar = ax . bar ( ticks - .4 , self . fracs [ : Q ] , color = col ) \n 
ax . set_xticks ( ticks , map ( lambda x : r"${}$" . format ( x ) , ticks + 1 ) ) \n 
ax . set_xlabel ( "PC" ) \n 
ax . set_ylim ( 0 , ax . get_ylim ( ) [ 1 ] ) \n 
ax . set_xlim ( ticks . min ( ) - .5 , ticks . max ( ) + .5 ) \n 
~~~ pylab . tight_layout ( ) \n 
~~ return bar \n 
~~ def plot_2d ( self , X , labels = None , s = 20 , marker = , \n 
dimensions = ( 0 , 1 ) , ax = None , colors = None , \n 
if cmap is None : \n 
~~~ cmap = matplotlib . cm . jet \n 
~~ if ax is None : \n 
~~ if labels is None : \n 
~~~ labels = numpy . zeros ( X . shape [ 0 ] ) \n 
~~ ulabels = [ ] \n 
for lab in labels : \n 
~~~ if not lab in ulabels : \n 
~~~ ulabels . append ( lab ) \n 
~~ ~~ nlabels = len ( ulabels ) \n 
if colors is None : \n 
~~~ colors = iter ( [ cmap ( float ( i ) / nlabels ) for i in range ( nlabels ) ] ) \n 
~~~ colors = iter ( colors ) \n 
~~ X_ = self . project ( X , self . Q ) [ : , dimensions ] \n 
kwargs . update ( dict ( s = s ) ) \n 
plots = list ( ) \n 
for i , l in enumerate ( ulabels ) : \n 
~~~ kwargs . update ( dict ( color = colors . next ( ) , marker = marker [ i % len ( marker ) ] ) ) \n 
plots . append ( ax . scatter ( * X_ [ labels == l , : ] . T , label = str ( l ) , ** kwargs ) ) \n 
~~ ax . set_xlabel ( r"PC$_1$" ) \n 
ax . set_ylabel ( r"PC$_2$" ) \n 
~~ return plots \n 
~~ ~~ __author__ = \n 
from pypet import Environment \n 
from pypet . utils . explore import cartesian_product \n 
def multiply ( traj ) : \n 
z = traj . x * traj . y \n 
traj . f_add_result ( , z , comment = ) \n 
~~ filename = os . path . join ( , ) \n 
env = Environment ( trajectory = , \n 
filename = filename , \n 
overwrite_file = True , \n 
file_title = , \n 
comment = , \n 
traj = env . trajectory \n 
traj . f_add_parameter ( , 1 , comment = ) \n 
traj . f_explore ( cartesian_product ( { : [ 1 , 2 , 3 , 4 ] , : [ 6 , 7 , 8 ] } ) ) \n 
env . run ( multiply ) \n 
from pypet . trajectory import Trajectory \n 
del traj \n 
env . disable_logging ( ) \n 
del env \n 
traj = Trajectory ( filename = filename ) \n 
traj . f_load ( index = - 1 , load_parameters = 2 , load_results = 2 ) \n 
print ( ) \n 
print ( traj . run_00000001 . z ) \n 
__author__ = \n 
from pypet import Environment , cartesian_product \n 
from pypet import pypetconstants \n 
traj . f_add_result ( , z = z , comment = ) \n 
~~ def main ( ) : \n 
filename = os . path . join ( , ) \n 
log_stdout = True , \n 
multiproc = True , \n 
wrap_mode = pypetconstants . WRAP_MODE_LOCAL , \n 
overwrite_file = True ) \n 
traj . f_add_parameter ( , 1.0 , comment = ) \n 
traj . f_explore ( cartesian_product ( { : [ float ( x ) for x in range ( 20 ) ] , \n 
: [ float ( y ) for y in range ( 20 ) ] } ) ) \n 
assert traj . f_is_completed ( ) \n 
~~~ main ( ) __author__ = \n 
~~ from pypet . tests . testutils . ioutils import run_suite , discover_tests , TEST_IMPORT_ERROR , parse_args \n 
tests_include = set ( ( , \n 
big_suite_1 = discover_tests ( lambda class_name , test_name , tags : class_name in tests_include ) \n 
tests_include = set ( ( \n 
big_suite_2 = discover_tests ( lambda class_name , test_name , tags : class_name in tests_include ) \n 
big_suite_3 = discover_tests ( lambda class_name , test_name , tags : class_name in tests_include ) \n 
big_suite_4 = discover_tests ( lambda class_name , test_name , tags : class_name in tests_include ) \n 
suite_dict = { : big_suite_1 , : big_suite_2 , : big_suite_3 , : big_suite_4 } \n 
~~~ opt_dict = parse_args ( ) \n 
suite = None \n 
if in opt_dict : \n 
~~~ suite_no = opt_dict . pop ( ) \n 
suite = suite_dict [ suite_no ] \n 
~~ if suite is None : \n 
~~~ pred = lambda class_name , test_name , tags : ( in tags and \n 
class_name != TEST_IMPORT_ERROR ) \n 
suite = discover_tests ( pred ) \n 
~~ run_suite ( suite = suite , ** opt_dict ) __author__ = \n 
~~ from pypet import Environment , Trajectory \n 
from pypet . tests . testutils . ioutils import make_temp_dir , get_log_config \n 
def job ( traj ) : \n 
~~~ traj . f_ares ( , 42 , comment = ) \n 
~~ def get_runtime ( length ) : \n 
~~~ filename = os . path . join ( , , ) \n 
with Environment ( filename = filename , \n 
log_levels = 50 , report_progress = ( 0.0002 , , 50 ) , \n 
overwrite_file = True , purge_duplicate_comments = False , \n 
log_stdout = False , \n 
multiproc = False , ncores = 2 , use_pool = True , \n 
wrap_mode = , #freeze_input=True, \n 
summary_tables = False , small_overview_tables = False ) as env : \n 
~~~ traj = env . v_traj \n 
traj . par . f_apar ( , 0 , ) \n 
traj . f_explore ( { : range ( length ) } ) \n 
max_run = 1000 \n 
for idx in range ( len ( traj ) ) : \n 
~~~ if idx > max_run : \n 
~~~ traj . f_get_run_information ( idx , copy = False ) [ ] = 1 \n 
~~ ~~ start = time . time ( ) \n 
env . f_run ( job ) \n 
end = time . time ( ) \n 
~~ total = end - start \n 
return total / float ( min ( len ( traj ) , max_run ) ) , total / float ( min ( len ( traj ) , max_run ) ) * len ( traj ) \n 
~~~ lengths = [ 100000 , 50000 , 10000 , 5000 , 1000 , 500 , 100 , 50 , 10 , 5 , 1 ] \n 
runtimes = [ get_runtime ( x ) for x in lengths ] \n 
avg_runtimes = [ x [ 0 ] for x in runtimes ] \n 
summed_runtime = [ x [ 1 ] for x in runtimes ] \n 
plt . subplot ( 2 , 1 , 1 ) \n 
plt . semilogx ( list ( reversed ( lengths ) ) , list ( reversed ( avg_runtimes ) ) , linewidth = 2 ) \n 
plt . xlabel ( ) \n 
plt . ylabel ( ) \n 
plt . title ( ) \n 
plt . grid ( ) \n 
plt . subplot ( 2 , 1 , 2 ) \n 
plt . loglog ( lengths , summed_runtime , linewidth = 2 ) \n 
plt . savefig ( ) \n 
plt . show ( ) \n 
if ( sys . version_info < ( 2 , 7 , 0 ) ) : \n 
~~~ import cPickle as pickle \n 
~~~ import pickle \n 
~~ from pypet . pypetlogging import LoggingManager \n 
from pypet . tests . testutils . ioutils import get_log_config , run_suite , parse_args \n 
from pypet . utils . comparisons import nested_equal \n 
class FakeTraj ( object ) : \n 
~~~ self . v_environment_name = \n 
self . v_name = \n 
~~ def f_wildcard ( self , card ) : \n 
~~ ~~ class LoggingManagerTest ( unittest . TestCase ) : \n 
~~~ tags = , , \n 
def test_pickling ( self ) : \n 
~~~ manager = LoggingManager ( log_config = get_log_config ( ) , log_stdout = True ) \n 
manager . extract_replacements ( FakeTraj ( ) ) \n 
manager . check_log_config ( ) \n 
manager . make_logging_handlers_and_tools ( ) \n 
dump = pickle . dumps ( manager ) \n 
new_manager = pickle . loads ( dump ) \n 
manager . finalize ( ) \n 
~~~ opt_args = parse_args ( ) \n 
run_suite ( ** opt_args ) \n 
~~ from __future__ import absolute_import , print_function , division \n 
from six . moves import xrange \n 
k = T . iscalar ( "k" ) ; A = T . vector ( "A" ) \n 
def inner_fct ( prior_result , A ) : return prior_result * A \n 
result , updates = theano . scan ( fn = inner_fct , \n 
outputs_info = T . ones_like ( A ) , \n 
non_sequences = A , n_steps = k ) \n 
final_result = result [ - 1 ] \n 
power = theano . function ( inputs = [ A , k ] , outputs = final_result , \n 
updates = updates ) \n 
print ( power ( list ( range ( 10 ) ) , 2 ) ) \n 
from __future__ import absolute_import , print_function , division \n 
from nose . plugins . skip import SkipTest \n 
from theano import config \n 
from theano import gof \n 
import theano . tensor \n 
from theano . compat import exc_message \n 
from theano . compile import debugmode \n 
import theano . compile \n 
from theano . tests import unittest_tools as utt \n 
def test0 ( ) : \n 
~~~ x = theano . tensor . dvector ( ) \n 
f = theano . function ( [ x ] , ( ( 2. * x ) + 7 ) / 2. , mode = debugmode . DebugMode ( ) ) \n 
f ( [ 1 , 2 ] ) \n 
~~ class BROKEN_ON_PURPOSE_Add ( gof . Op ) : \n 
~~~ __props__ = ( "py_offset" , ) \n 
def __init__ ( self , py_offset ) : \n 
~~~ gof . Op . __init__ ( self ) \n 
self . py_offset = py_offset \n 
~~ def make_node ( self , a , b ) : \n 
~~~ a = theano . tensor . as_tensor_variable ( a ) \n 
b = theano . tensor . as_tensor_variable ( b ) \n 
assert a . type . dtype == \n 
assert a . type . dtype == b . type . dtype \n 
assert a . type . ndim == 1 \n 
r = gof . Apply ( self , [ a , b ] , [ a . type ( ) ] ) \n 
return r \n 
~~ def perform ( self , node , inp , out_ ) : \n 
~~~ a , b = inp \n 
out , = out_ \n 
z = a + b \n 
if self . py_offset : \n 
~~~ out [ 0 ] = z + 0.5 \n 
~~~ out [ 0 ] = z \n 
~~ ~~ def c_code_cache_version ( self ) : \n 
~~~ return ( 1 , ) \n 
~~ def c_code ( self , node , name , inp , out , sub ) : \n 
z , = out \n 
~~ ~~ inconsistent = BROKEN_ON_PURPOSE_Add ( False ) \n 
off_by_half = BROKEN_ON_PURPOSE_Add ( True ) \n 
class WeirdBrokenOp ( gof . Op ) : \n 
__props__ = ( "behaviour" , ) \n 
def __init__ ( self , behaviour ) : \n 
self . behaviour = behaviour \n 
~~ def make_node ( self , a ) : \n 
~~~ a_ = theano . tensor . as_tensor_variable ( a ) \n 
r = gof . Apply ( self , [ a_ ] , [ a_ . type ( ) ] ) \n 
~~ def dontuse_perform ( self , node , inp , out_ ) : \n 
~~~ a , = inp \n 
if self . behaviour == : \n 
~~~ out [ 0 ] = a * 2 \n 
~~ elif self . behaviour == : \n 
~~~ out [ 0 ] = a \n 
out [ 0 ] *= 2 \n 
~~~ out [ 0 ] = a * 1 \n 
~~~ raise ValueError ( self . behaviour ) \n 
if "inplace" in self . behaviour : \n 
~~~ behaviour = "" \n 
total = ( ( z_code + prep_vars + behaviour + prep_vars2 ) \n 
% dict ( locals ( ) , ** sub ) ) \n 
return total \n 
~~ ~~ wb2i = WeirdBrokenOp ( ) \n 
wb2 = WeirdBrokenOp ( ) \n 
wb1i = WeirdBrokenOp ( ) \n 
wb1 = WeirdBrokenOp ( ) \n 
def test_badthunkoutput ( ) : \n 
~~~ a = theano . tensor . dvector ( ) \n 
b = theano . tensor . dvector ( ) \n 
f_good = theano . function ( [ a , b ] , \n 
off_by_half ( a , b ) , \n 
mode = debugmode . DebugMode ( check_c_code = theano . config . cxx ) ) \n 
f_inconsistent = theano . function ( [ a , b ] , \n 
inconsistent ( a , b ) , \n 
f_good ( [ 1.0 , 2.0 , 3.0 ] , [ 2 , 3 , 4 ] ) \n 
if not theano . config . cxx : \n 
~~~ f_inconsistent ( [ 1.0 , 2.0 , 3.0 ] , [ 2 , 3 , 4 ] ) \n 
~~ except debugmode . BadThunkOutput as e : \n 
~~~ assert e . r . owner . op is inconsistent \n 
~~ def test_badoptimization ( ) : \n 
~~~ @ gof . local_optimizer ( [ theano . tensor . add ] ) \n 
def insert_broken_add ( node ) : \n 
~~~ if node . op == theano . tensor . add : \n 
~~~ return [ off_by_half ( * node . inputs ) ] \n 
~~ edb = gof . EquilibriumDB ( ) \n 
edb . register ( , insert_broken_add , ) \n 
opt = edb . query ( ) \n 
a = theano . tensor . dvector ( ) \n 
f = theano . function ( [ a , b ] , a + b , \n 
mode = debugmode . DebugMode ( optimizer = opt ) ) \n 
~~~ f ( [ 1.0 , 2.0 , 3.0 ] , [ 2 , 3 , 4 ] , ) \n 
~~ except debugmode . BadOptimization as e : \n 
~~~ assert str ( e . reason ) == \n 
~~ assert False \n 
~~ def test_badoptimization_opt_err ( ) : \n 
@ gof . local_optimizer ( [ theano . tensor . add ] ) \n 
def insert_bigger_b_add ( node ) : \n 
~~~ inputs = list ( node . inputs ) \n 
if inputs [ - 1 ] . owner is None : \n 
~~~ inputs [ - 1 ] = theano . tensor . concatenate ( ( inputs [ - 1 ] , \n 
inputs [ - 1 ] ) ) \n 
return [ node . op ( * inputs ) ] \n 
~~ ~~ return False \n 
edb . register ( , insert_bigger_b_add , ) \n 
~~ except Exception as e : \n 
~~~ assert in exc_message ( e ) \n 
~~ def test_stochasticoptimization ( ) : \n 
~~~ last_time_replaced = [ False ] \n 
def insert_broken_add_sometimes ( node ) : \n 
~~~ last_time_replaced [ 0 ] = not last_time_replaced [ 0 ] \n 
if last_time_replaced [ 0 ] : \n 
edb . register ( \n 
insert_broken_add_sometimes , \n 
~~~ theano . function ( [ a , b ] , \n 
theano . tensor . add ( a , b ) , \n 
mode = debugmode . DebugMode ( \n 
optimizer = opt , \n 
check_c_code = True , \n 
stability_patience = max ( 2 , config . DebugMode . patience ) ) ) \n 
~~ except debugmode . StochasticOrder : \n 
~~ def test_just_c_code ( ) : \n 
~~~ if not theano . config . cxx : \n 
~~ x = theano . tensor . dvector ( ) \n 
f = theano . function ( [ x ] , wb2 ( x ) , \n 
mode = debugmode . DebugMode ( check_py_code = False ) ) \n 
assert numpy . all ( f ( [ 1 , 2 ] ) == [ 2 , 4 ] ) \n 
~~ def test_baddestroymap ( ) : \n 
~~~ class BadAdd ( gof . Op ) : \n 
~~~ def make_node ( self , a , b ) : \n 
~~~ c = a . type ( ) \n 
return gof . Apply ( self , [ a , b ] , [ c ] ) \n 
~~ def perform ( self , node , inp , out ) : \n 
c , = out \n 
c [ 0 ] = a \n 
c [ 0 ] += b \n 
~~ ~~ x = theano . tensor . dvector ( ) \n 
y = theano . tensor . dvector ( ) \n 
f = theano . function ( [ x , y ] , BadAdd ( ) ( x , y ) , mode = ) \n 
~~~ f ( [ 1 , 2 ] , [ 3 , 4 ] ) \n 
~~ except debugmode . BadDestroyMap : \n 
~~ ~~ def test_baddestroymap_c ( ) : \n 
f = theano . function ( [ x ] , wb2i ( x ) , \n 
~~~ assert numpy . all ( f ( [ 1 , 2 ] ) == [ 2 , 4 ] ) \n 
~~ ~~ class Test_ViewMap ( unittest . TestCase ) : \n 
~~~ class BadAddRef ( gof . Op ) : \n 
~~~ c = b . type ( ) \n 
c [ 0 ] = b \n 
~~ ~~ class BadAddSlice ( gof . Op ) : \n 
c [ 0 ] = b [ 1 : 3 ] \n 
~~ ~~ def test_badviewmap_ref ( self ) : \n 
f = theano . function ( [ x , y ] , self . BadAddRef ( ) ( x , y ) , mode = ) \n 
~~ except debugmode . BadViewMap : \n 
~~ ~~ def test_badviewmap_slice ( self ) : \n 
f = theano . function ( [ x , y ] , self . BadAddSlice ( ) ( x , y ) , \n 
~~ ~~ def test_goodviewmap ( self ) : \n 
~~~ goodop = self . BadAddRef ( ) \n 
goodop . view_map = { 0 : [ 1 ] } \n 
x = theano . tensor . dvector ( ) \n 
f = theano . function ( [ x , y ] , goodop ( x , y ) , mode = ) \n 
~~~ f ( [ 1 , 5 , 1 ] , [ 3 , 4 , 2 , 1 , 4 ] ) \n 
~~ ~~ def test_badviewmap_c ( self ) : \n 
f = theano . function ( [ x ] , wb1i ( x ) , \n 
~~~ f ( [ 1 , 2 ] ) \n 
~~ ~~ def test_aliased_outputs_ok ( self ) : \n 
~~~ class CustomOp ( gof . Op ) : \n 
~~~ view_map = { 0 : [ 0 ] , 1 : [ 0 ] } \n 
def make_node ( self , a , b ) : \n 
d = a . type ( ) \n 
return gof . Apply ( self , [ a , b ] , [ c , d ] ) \n 
c , d = out \n 
d [ 0 ] = a [ 1 : ] \n 
f = theano . function ( [ x , y ] , CustomOp ( ) ( x , y ) , mode = ) \n 
r0 , r1 = f ( [ 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 ] ) \n 
assert numpy . all ( r0 == [ 1 , 2 , 3 , 4 ] ) \n 
assert numpy . all ( r1 == [ 2 , 3 , 4 ] ) \n 
~~ def test_aliased_outputs_ok_output ( self ) : \n 
r = a * 2 \n 
c [ 0 ] = r \n 
d [ 0 ] = r [ 1 : ] \n 
assert numpy . all ( r0 == [ 2 , 4 , 6 , 8 ] ) \n 
assert numpy . all ( r1 == [ 4 , 6 , 8 ] ) \n 
~~ def test_aliased_outputs_ok_shadow ( self ) : \n 
r = a * 1 \n 
f = theano . function ( [ x , y ] , CustomOp ( ) ( x , y ) [ 0 ] * 2 , mode = ) \n 
r0 = f ( [ 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 ] ) \n 
~~ def test_aliased_outputs_bad ( self ) : \n 
c [ 0 ] = r [ : - 1 ] \n 
~~ ~~ custom_op = CustomOp ( ) \n 
bad_xy0 , bad_xy1 = custom_op ( x , y ) \n 
out = bad_xy0 * 2 + bad_xy1 * 2 \n 
f = theano . function ( [ x , y ] , out , mode = ) \n 
~~~ f ( [ 1 , 2 , 3 , 4 ] , [ 5 , 6 , 7 , 8 ] ) \n 
~~ ~~ ~~ class Test_check_isfinite ( unittest . TestCase ) : \n 
~~~ self . old_ts = theano . tensor . TensorType . filter_checks_isfinite \n 
self . old_dm = theano . compile . mode . predefined_modes [ \n 
] . check_isfinite \n 
~~~ theano . tensor . TensorType . filter_checks_isfinite = self . old_ts \n 
theano . compile . mode . predefined_modes [ \n 
] . check_isfinite = self . old_dm \n 
~~ def test_check_isfinite ( self ) : \n 
~~~ x = theano . tensor . vector ( ) \n 
f = theano . function ( [ x ] , ( x + 2 ) * 5 , mode = ) \n 
g = theano . function ( [ x ] , theano . tensor . log ( x ) , mode = ) \n 
f ( numpy . log ( [ 3 , 4 , 5 ] ) . astype ( config . floatX ) ) \n 
self . assertRaises ( debugmode . InvalidValueError , f , \n 
numpy . log ( [ 3 , - 4 , 5 ] ) . astype ( config . floatX ) ) \n 
( numpy . asarray ( [ 0 , 1.0 , 0 ] ) / 0 ) . astype ( config . floatX ) ) \n 
( numpy . asarray ( [ 1.0 , 1.0 , 1.0 ] ) / 0 ) . astype ( config . floatX ) ) \n 
self . assertRaises ( debugmode . InvalidValueError , g , \n 
numpy . asarray ( [ 3 , - 4 , 5 ] , dtype = config . floatX ) ) \n 
theano . tensor . TensorType . filter_checks_isfinite = False \n 
] . check_isfinite = False \n 
f ( numpy . asarray ( numpy . asarray ( [ 1.0 , 1.0 , 1.0 ] ) / 0 , \n 
dtype = config . floatX ) ) \n 
~~ def test_check_isfinite_disabled ( self ) : \n 
f = theano . function ( [ x ] , ( x + 2 ) * 5 , \n 
mode = debugmode . DebugMode ( check_isfinite = False ) ) \n 
f ( numpy . log ( [ 3 , - 4 , 5 ] ) ) \n 
infs = numpy . asarray ( [ 1.0 , 1. , 1. ] ) / 0 \n 
f ( infs ) \n 
~~ ~~ class BrokenCImplementationAdd ( gof . Op ) : \n 
~~~ __props__ = ( ) \n 
assert a . type . ndim == 2 \n 
out [ 0 ] = z \n 
~~ def c_code_cache_version ( self ) : \n 
debug = 0 \n 
~~ ~~ class VecAsRowAndCol ( gof . Op ) : \n 
__props__ = ( ) \n 
def make_node ( self , v ) : \n 
~~~ if not isinstance ( v , gof . Variable ) : \n 
~~~ v = theano . tensor . as_tensor_variable ( v ) \n 
~~ assert v . type . ndim == 1 \n 
type_class = type ( v . type ) \n 
out_r_type = type_class ( dtype = v . dtype , broadcastable = ( True , False ) ) \n 
out_c_type = type_class ( dtype = v . dtype , broadcastable = ( False , True ) ) \n 
return gof . Apply ( self , [ v ] , [ out_r_type ( ) , out_c_type ( ) ] ) \n 
~~~ v , = inp \n 
r , c = out \n 
lv = v . shape [ 0 ] \n 
if ( r [ 0 ] is None ) or ( r [ 0 ] . shape != ( 1 , lv ) ) : \n 
~~~ r [ 0 ] = node . outputs [ 0 ] . type . value_zeros ( ( 1 , lv ) ) \n 
~~ if ( c [ 0 ] is None ) or ( c [ 0 ] . shape != ( lv , 1 ) ) : \n 
~~~ c [ 0 ] = node . outputs [ 1 ] . type . value_zeros ( ( lv , 1 ) ) \n 
~~ for i in range ( lv ) : \n 
~~~ r [ 0 ] [ 0 , i ] = v [ i ] \n 
c [ 0 ] [ i , 0 ] = v [ i ] \n 
~~ ~~ ~~ class Test_preallocated_output ( unittest . TestCase ) : \n 
~~~ self . rng = numpy . random . RandomState ( seed = utt . fetch_seed ( ) ) \n 
~~ def test_f_contiguous ( self ) : \n 
~~~ a = theano . tensor . fmatrix ( ) \n 
b = theano . tensor . fmatrix ( ) \n 
z = BrokenCImplementationAdd ( ) ( a , b ) \n 
out = theano . tensor . dot ( z , numpy . eye ( 7 ) ) \n 
a_val = self . rng . randn ( 7 , 7 ) . astype ( ) \n 
b_val = self . rng . randn ( 7 , 7 ) . astype ( ) \n 
check_preallocated_output = [ ] ) \n 
f = theano . function ( [ a , b ] , out , mode = mode ) \n 
f ( a_val , b_val ) \n 
if theano . config . cxx : \n 
~~~ self . assertRaises ( debugmode . BadThunkOutput , f , a_val , b_val ) \n 
~~~ f ( a_val , b_val ) \n 
~~ ~~ def test_f_contiguous_out ( self ) : \n 
out = BrokenCImplementationAdd ( ) ( a , b ) \n 
~~ ~~ def test_output_broadcast_tensor ( self ) : \n 
~~~ v = theano . tensor . fvector ( ) \n 
c , r = VecAsRowAndCol ( ) ( v ) \n 
f = theano . function ( [ v ] , [ c , r ] ) \n 
v_val = self . rng . randn ( 5 ) . astype ( ) \n 
f ( v_val ) \n 
~~ def test_output_broadcast_cuda ( self ) : \n 
~~~ from theano . sandbox import cuda \n 
if not cuda . cuda_available : \n 
~~ if cuda . use . device_number is None : \n 
~~~ cuda . use ( "gpu" , \n 
force = True , \n 
default_to_move_computation_to_gpu = False , \n 
move_shared_float32_to_gpu = False , \n 
enable_cuda = False ) \n 
~~ v = cuda . fvector ( ) \n 
v_val = cuda . CudaNdarray ( self . rng . randn ( 5 ) . astype ( ) ) \n 
~~ ~~ from __future__ import absolute_import , print_function , division \n 
import errno \n 
from theano . compat import PY3 \n 
from theano . gof . compilelock import get_lock , release_lock \n 
from . import cmodule \n 
if os . path . exists ( os . path . join ( config . compiledir , ) ) : \n 
~~~ os . remove ( os . path . join ( config . compiledir , ) ) \n 
~~ def compile_cutils_code ( ) : \n 
~~~ types = [ + t for t in [ , , , , , \n 
, , , , \n 
] ] \n 
complex_types = [ + t for t in [ , , \n 
, ] ] \n 
fns = . join ( [ inplace_map_template % { : t , : t . upper ( ) , \n 
: floatadd % { : t } } \n 
for t in types ] + \n 
[ inplace_map_template % { : t , : t . upper ( ) , \n 
: complexadd % { : t } } \n 
for t in complex_types ] ) \n 
def gen_binop ( type , typen ) : \n 
. join ( [ gen_binop ( type = t , typen = t . upper ( ) ) \n 
for t in types + complex_types ] ) + "NULL};\\n" ) \n 
def gen_num ( typen ) : \n 
. join ( [ gen_num ( typen = t . upper ( ) ) \n 
for t in types + complex_types ] ) + "-1000};" ) \n 
return code \n 
~~ def compile_cutils ( ) : \n 
code += compile_cutils_code ( ) \n 
if PY3 : \n 
~~~ code = code . replace ( "<Python.h>" , \'"numpy/npy_3kcompat.h"\' , 1 ) \n 
code = code . replace ( "PyCObject" , "NpyCapsule" ) \n 
~~ loc = os . path . join ( config . compiledir , ) \n 
if not os . path . exists ( loc ) : \n 
~~~ os . mkdir ( loc ) \n 
~~ except OSError as e : \n 
~~~ assert e . errno == errno . EEXIST \n 
assert os . path . exists ( loc ) , loc \n 
~~ ~~ args = cmodule . GCC_compiler . compile_args ( ) \n 
cmodule . GCC_compiler . compile_str ( , code , location = loc , \n 
preargs = args ) \n 
~~~ sys . path . insert ( 0 , config . compiledir ) \n 
location = os . path . join ( config . compiledir , ) \n 
if not os . path . exists ( location ) : \n 
~~~ os . mkdir ( location ) \n 
assert os . path . exists ( location ) , location \n 
~~ ~~ if not os . path . exists ( os . path . join ( location , ) ) : \n 
~~~ open ( os . path . join ( location , ) , ) . close ( ) \n 
~~~ get_lock ( ) \n 
~~~ compile_cutils ( ) \n 
~~ ~~ finally : \n 
~~~ release_lock ( ) \n 
~~ ~~ ~~ finally : \n 
~~~ if sys . path [ 0 ] == config . compiledir : \n 
~~~ del sys . path [ 0 ] \n 
from theano import Op , Apply \n 
from theano . tensor import TensorType \n 
from theano . gof . type import CDataType \n 
class ProdOp ( Op ) : \n 
def make_node ( self , i ) : \n 
~~~ return Apply ( self , [ i ] , [ CDataType ( , ) ( ) ] ) \n 
~~ def c_support_code ( self ) : \n 
~~ def c_code ( self , node , name , inps , outs , sub ) : \n 
~~~ return ( 0 , ) \n 
~~ ~~ class GetOp ( Op ) : \n 
def make_node ( self , c ) : \n 
~~~ return Apply ( self , [ c ] , [ TensorType ( , ( False , ) ) ( ) ] ) \n 
~~ ~~ def test_cdata ( ) : \n 
~~ i = TensorType ( , ( False , ) ) ( ) \n 
c = ProdOp ( ) ( i ) \n 
i2 = GetOp ( ) ( c ) \n 
mode = None \n 
if theano . config . mode == "FAST_COMPILE" : \n 
~~~ mode = "FAST_RUN" \n 
~~ f = theano . function ( [ i ] , i2 , mode = mode ) \n 
v = numpy . random . randn ( 9 ) . astype ( ) \n 
v2 = f ( v ) \n 
assert ( v2 == v ) . all ( ) \n 
from theano . compat import izip \n 
from theano . gof import Op , Apply , local_optimizer , EquilibriumDB \n 
from theano . gof . utils import hash_from_dict \n 
from theano . sandbox . cuda import GpuElemwise , CudaNdarrayType , GpuOp \n 
from theano . sandbox . cuda . basic_ops import ( as_cuda_ndarray_variable , \n 
gpu_contiguous ) \n 
from theano . sandbox . cuda . opt import gpu_seqopt \n 
import pycuda \n 
from pycuda . compiler import SourceModule \n 
import pycuda . gpuarray \n 
from . import pycuda_init \n 
if not pycuda_init . pycuda_available : \n 
~~ def _replace_npy_types ( c_arg ) : \n 
~~~ c_arg = c_arg . replace ( , ) \n 
c_arg = c_arg . replace ( , ) \n 
return c_arg \n 
~~ def theano_parse_c_arg ( c_arg ) : \n 
~~~ c_arg = _replace_npy_types ( c_arg ) \n 
return pycuda . tools . parse_c_arg ( c_arg ) \n 
class PycudaElemwiseSourceModuleOp ( GpuOp ) : \n 
~~~ nin = property ( lambda self : self . scalar_op . nin ) \n 
nout = property ( lambda self : self . scalar_op . nout ) \n 
def __init__ ( self , scalar_op , inplace_pattern = None , name = None ) : \n 
~~~ if inplace_pattern is None : \n 
~~~ inplace_pattern = { } \n 
self . scalar_op = scalar_op \n 
self . inplace_pattern = inplace_pattern \n 
~~~ if self . name is None : \n 
~~~ if self . inplace_pattern : \n 
~~~ items = list ( self . inplace_pattern . items ( ) ) \n 
items . sort ( ) \n 
return self . __class__ . __name__ + "{%s}%s" % ( self . scalar_op , \n 
str ( items ) ) \n 
~~~ return self . __class__ . __name__ + "{%s}" % ( self . scalar_op ) \n 
~~~ return self . name \n 
~~ ~~ def __eq__ ( self , other ) : \n 
~~~ return ( type ( self ) == type ( other ) and \n 
self . scalar_op == other . scalar_op and \n 
self . inplace_pattern == other . inplace_pattern ) \n 
~~ def __hash__ ( self ) : \n 
~~~ return ( hash ( type ( self ) ) ^ hash ( self . scalar_op ) ^ \n 
hash_from_dict ( self . inplace_pattern ) ) \n 
~~ def make_node ( self , * inputs ) : \n 
~~~ _inputs = [ gpu_contiguous ( as_cuda_ndarray_variable ( i ) ) for i in inputs ] \n 
if self . nin > 0 and len ( _inputs ) != self . nin : \n 
~~~ raise TypeError ( , ( self . nin , len ( _inputs ) ) ) \n 
~~ for i in _inputs [ 1 : ] : \n 
~~~ if i . type . ndim != inputs [ 0 ] . type . ndim : \n 
~~~ raise TypeError ( ) \n 
~~ ~~ if any ( [ any ( i . type . broadcastable ) for i in inputs ] ) : \n 
otype = CudaNdarrayType ( broadcastable = [ False ] * _inputs [ 0 ] . type . ndim ) \n 
assert self . nout == 1 \n 
fct_name = "pycuda_elemwise_%s" % str ( self . scalar_op ) \n 
out_node = Apply ( self , _inputs , [ otype ( ) for o in xrange ( self . nout ) ] ) \n 
in_name = [ "i" + str ( id ) for id in range ( len ( inputs ) ) ] \n 
out_name = [ "o" + str ( id ) for id in range ( self . nout ) ] \n 
c_code = self . scalar_op . c_code ( out_node , "some_name" , \n 
tuple ( [ n + "[i]" for n in in_name ] ) , \n 
tuple ( n + "[i]" for n in out_name ) , { } ) \n 
for var , name in chain ( izip ( inputs , in_name ) , \n 
izip ( out_node . outputs , out_name ) ) ] + \n 
self . pycuda_fct = mod . get_function ( fct_name ) \n 
return out_node \n 
~~ def perform ( self , node , inputs , out ) : \n 
~~~ z , = out \n 
if ( z [ 0 ] is None or \n 
z [ 0 ] . shape != inputs [ 0 ] . shape or \n 
not z [ 0 ] . is_c_contiguous ( ) ) : \n 
~~~ z [ 0 ] = theano . sandbox . cuda . CudaNdarray . zeros ( inputs [ 0 ] . shape ) \n 
~~ if inputs [ 0 ] . shape != inputs [ 1 ] . shape : \n 
~~~ raise TypeError ( "PycudaElemwiseSourceModuleOp:" \n 
~~ if inputs [ 0 ] . size > 512 : \n 
~~~ grid = ( int ( numpy . ceil ( inputs [ 0 ] . size / 512. ) ) , 1 ) \n 
block = ( 512 , 1 , 1 ) \n 
~~~ grid = ( 1 , 1 ) \n 
block = ( inputs [ 0 ] . shape [ 0 ] , inputs [ 0 ] . shape [ 1 ] , 1 ) \n 
~~ self . pycuda_fct ( inputs [ 0 ] , inputs [ 1 ] , z [ 0 ] , \n 
numpy . intc ( inputs [ 1 ] . size ) , block = block , grid = grid ) \n 
~~ ~~ class PycudaElemwiseSourceModuleMakeThunkOp ( Op ) : \n 
__props__ = ( "scalar_op" , "inplace_pattern" ) \n 
~~~ return hash ( ( type ( self ) , hash ( self . scalar_op ) , \n 
hash_from_dict ( self . inplace_pattern ) ) ) \n 
~~ ~~ def make_node ( self , * inputs ) : \n 
~~~ assert self . nout == 1 \n 
_inputs = [ gpu_contiguous ( as_cuda_ndarray_variable ( i ) ) for i in inputs ] \n 
~~ otype = CudaNdarrayType ( broadcastable = [ False ] * _inputs [ 0 ] . type . ndim ) \n 
~~ def make_thunk ( self , node , storage_map , _ , _2 ) : \n 
~~~ fct_name = "pycuda_elemwise_%s" % str ( self . scalar_op ) \n 
in_name = [ "i" + str ( id ) for id in range ( len ( node . inputs ) ) ] \n 
c_code = self . scalar_op . c_code ( node , "some_name" , \n 
for var , name in chain ( izip ( node . inputs , in_name ) , \n 
izip ( node . outputs , out_name ) ) ] + \n 
pycuda_fct = mod . get_function ( fct_name ) \n 
inputs = [ storage_map [ v ] for v in node . inputs ] \n 
outputs = [ storage_map [ v ] for v in node . outputs ] \n 
def thunk ( ) : \n 
~~~ z = outputs [ 0 ] \n 
z [ 0 ] . shape != inputs [ 0 ] [ 0 ] . shape or \n 
~~~ z [ 0 ] = theano . sandbox . cuda . CudaNdarray . zeros ( \n 
inputs [ 0 ] [ 0 ] . shape ) \n 
~~ if inputs [ 0 ] [ 0 ] . shape != inputs [ 1 ] [ 0 ] . shape : \n 
~~~ raise TypeError ( "PycudaElemwiseSourceModuleMakeThunkOp:" \n 
~~ if inputs [ 0 ] [ 0 ] . size > 512 : \n 
~~~ grid = ( int ( numpy . ceil ( inputs [ 0 ] [ 0 ] . size / 512. ) ) , 1 ) \n 
block = ( inputs [ 0 ] [ 0 ] . shape [ 0 ] , inputs [ 0 ] [ 0 ] . shape [ 1 ] , 1 ) \n 
~~ pycuda_fct ( inputs [ 0 ] [ 0 ] , inputs [ 1 ] [ 0 ] , z [ 0 ] , \n 
numpy . intc ( inputs [ 1 ] [ 0 ] . size ) , block = block , \n 
grid = grid ) \n 
~~ thunk . inputs = inputs \n 
thunk . outputs = outputs \n 
thunk . lazy = False \n 
return thunk \n 
~~ ~~ pycuda_optimizer = EquilibriumDB ( ) \n 
gpu_seqopt . register ( "pycuda_optimizer" , pycuda_optimizer , 1.5 , "fast_run" ) \n 
@ local_optimizer ( [ GpuElemwise ] ) \n 
def local_pycuda_gpu_elemwise ( node ) : \n 
if isinstance ( node . op , GpuElemwise ) : \n 
~~~ if ( not any ( [ any ( i . type . broadcastable ) for i in node . inputs ] ) and \n 
all ( [ i . ndim <= 2 for i in node . inputs ] ) ) : \n 
~~~ new_op = PycudaElemwiseSourceModuleOp ( node . op . scalar_op , \n 
node . op . inplace_pattern ) ( \n 
* node . inputs ) \n 
return [ new_op ] \n 
~~ ~~ ~~ pycuda_optimizer . register ( "local_pycuda_gpu_elemwise" , \n 
local_pycuda_gpu_elemwise ) \n 
import string \n 
from theano . sandbox . cuda import cuda_available , GpuOp \n 
from theano . ifelse import ifelse \n 
from theano . misc . pycuda_init import pycuda_available \n 
if cuda_available : \n 
~~~ from theano . sandbox . cuda import ( basic_ops , CudaNdarrayType , \n 
CudaNdarray ) \n 
~~ if pycuda_available : \n 
~~~ import pycuda . gpuarray \n 
~~~ import scikits . cuda \n 
from scikits . cuda import fft , cublas \n 
scikits . cuda . misc . init ( ) \n 
scikits_cuda_available = True \n 
~~ except ( ImportError , Exception ) : \n 
~~~ scikits_cuda_available = False \n 
~~ class ScikitsCudaOp ( GpuOp ) : \n 
~~~ def __eq__ ( self , other ) : \n 
~~~ return type ( self ) == type ( other ) \n 
~~~ return hash ( type ( self ) ) \n 
~~~ return self . __class__ . __name__ \n 
~~ def output_type ( self , inp ) : \n 
~~ def make_node ( self , inp ) : \n 
~~~ inp = basic_ops . gpu_contiguous ( \n 
basic_ops . as_cuda_ndarray_variable ( inp ) ) \n 
assert inp . dtype == "float32" \n 
return theano . Apply ( self , [ inp ] , [ self . output_type ( inp ) ( ) ] ) \n 
~~~ if not scikits_cuda_available : \n 
~~~ raise RuntimeError ( \n 
~~ ~~ ~~ class CuFFTOp ( ScikitsCudaOp ) : \n 
~~~ def output_type ( self , inp ) : \n 
~~~ return CudaNdarrayType ( \n 
broadcastable = [ False ] * ( inp . type . ndim + 1 ) ) \n 
~~~ super ( CuFFTOp , self ) . make_thunk ( node , storage_map , _ , _2 ) \n 
from theano . misc . pycuda_utils import to_gpuarray \n 
plan_input_shape = [ None ] \n 
plan = [ None ] \n 
~~~ input_shape = inputs [ 0 ] [ 0 ] . shape \n 
output_shape = list ( input_shape ) \n 
output_shape [ - 1 ] = output_shape [ - 1 ] // 2 + 1 \n 
output_shape += [ 2 ] \n 
output_shape = tuple ( output_shape ) \n 
z = outputs [ 0 ] \n 
if z [ 0 ] is None or z [ 0 ] . shape != output_shape : \n 
~~~ z [ 0 ] = CudaNdarray . zeros ( output_shape ) \n 
~~ input_pycuda = to_gpuarray ( inputs [ 0 ] [ 0 ] ) \n 
output_pycuda = to_gpuarray ( z [ 0 ] ) \n 
if plan [ 0 ] is None or plan_input_shape [ 0 ] != input_shape : \n 
~~~ plan_input_shape [ 0 ] = input_shape \n 
plan [ 0 ] = fft . Plan ( input_shape [ 1 : ] , np . float32 , np . complex64 , \n 
batch = input_shape [ 0 ] ) \n 
~~ fft . fft ( input_pycuda , output_pycuda , plan [ 0 ] ) \n 
~~ ~~ class CuIFFTOp ( ScikitsCudaOp ) : \n 
broadcastable = [ False ] * ( inp . type . ndim - 1 ) ) \n 
~~~ super ( CuIFFTOp , self ) . make_thunk ( node , storage_map , _ , _2 ) \n 
output_shape = list ( input_shape [ : - 1 ] ) \n 
output_shape [ - 1 ] = ( output_shape [ - 1 ] - 1 ) * 2 \n 
plan [ 0 ] = fft . Plan ( output_shape [ 1 : ] , np . complex64 , np . float32 , \n 
batch = output_shape [ 0 ] ) \n 
~~ fft . ifft ( input_pycuda , output_pycuda , plan [ 0 ] ) \n 
~~ ~~ def to_complex_gpuarray ( x , copyif = False ) : \n 
if not isinstance ( x , CudaNdarray ) : \n 
~~~ assert x . shape [ - 1 ] == 2 \n 
assert x . dtype == \n 
size = 1 \n 
c_contiguous = True \n 
for i in range ( x . ndim - 1 , - 1 , - 1 ) : \n 
~~~ if x . shape [ i ] == 1 : \n 
~~ if x . _strides [ i ] != size : \n 
~~~ c_contiguous = False \n 
~~ size *= x . shape [ i ] \n 
~~ if not c_contiguous : \n 
~~~ if copyif : \n 
~~~ x = x . copy ( ) \n 
~~ ~~ px = pycuda . gpuarray . GPUArray ( x . shape [ : - 1 ] , np . complex64 , base = x , \n 
gpudata = x . gpudata ) \n 
return px \n 
~~ ~~ def bptrs ( a ) : \n 
return pycuda . gpuarray . arange ( a . ptr , a . ptr + a . shape [ 0 ] * a . strides [ 0 ] , \n 
a . strides [ 0 ] , dtype = cublas . ctypes . c_void_p ) \n 
~~ def sc_complex_dot_batched ( bx_gpu , by_gpu , bc_gpu , transa = , transb = , \n 
handle = None ) : \n 
if handle is None : \n 
~~~ handle = scikits . cuda . misc . _global_cublas_handle \n 
~~ assert len ( bx_gpu . shape ) == 3 \n 
assert len ( by_gpu . shape ) == 3 \n 
assert len ( bc_gpu . shape ) == 3 \n 
assert bx_gpu . dtype == np . complex64 \n 
assert by_gpu . dtype == np . complex64 \n 
assert bc_gpu . dtype == np . complex64 \n 
bx_shape = bx_gpu . shape \n 
by_shape = by_gpu . shape \n 
alpha = np . complex64 ( 1.0 ) \n 
beta = np . complex64 ( 0.0 ) \n 
transa = string . lower ( transa ) \n 
transb = string . lower ( transb ) \n 
if transb in [ , ] : \n 
~~~ N , m , k = by_shape \n 
~~ elif transb in [ ] : \n 
~~~ N , k , m = by_shape \n 
~~ if transa in [ , ] : \n 
~~~ N2 , l , n = bx_shape \n 
~~ elif transa in [ ] : \n 
~~~ N2 , n , l = bx_shape \n 
~~ if l != k : \n 
~~ if N != N2 : \n 
~~ if transb == : \n 
~~~ lda = max ( 1 , m ) \n 
~~~ lda = max ( 1 , k ) \n 
~~ if transa == : \n 
~~~ ldb = max ( 1 , k ) \n 
~~~ ldb = max ( 1 , n ) \n 
~~ ldc = max ( 1 , m ) \n 
bx_arr = bptrs ( bx_gpu ) \n 
by_arr = bptrs ( by_gpu ) \n 
bc_arr = bptrs ( bc_gpu ) \n 
cublas . cublasCgemmBatched ( handle , transb , transa , m , n , k , alpha , \n 
by_arr . gpudata , lda , bx_arr . gpudata , ldb , \n 
beta , bc_arr . gpudata , ldc , N ) \n 
~~ class BatchedComplexDotOp ( ScikitsCudaOp ) : \n 
def make_node ( self , inp1 , inp2 ) : \n 
~~~ inp1 = basic_ops . gpu_contiguous ( \n 
basic_ops . as_cuda_ndarray_variable ( inp1 ) ) \n 
inp2 = basic_ops . gpu_contiguous ( \n 
basic_ops . as_cuda_ndarray_variable ( inp2 ) ) \n 
assert inp1 . dtype == "float32" \n 
assert inp2 . dtype == "float32" \n 
assert inp2 . ndim == 4 \n 
return theano . Apply ( self , [ inp1 , inp2 ] , [ self . output_type ( inp1 ) ( ) ] ) \n 
~~~ return CudaNdarrayType ( broadcastable = [ False ] * inp . type . ndim ) \n 
~~~ super ( BatchedComplexDotOp , self ) . make_thunk ( node , storage_map , _ , _2 ) \n 
~~~ bx = inputs [ 0 ] \n 
by = inputs [ 1 ] \n 
output_shape = ( input_shape_x [ 0 ] , input_shape_x [ 1 ] , \n 
bz = outputs [ 0 ] \n 
if bz [ 0 ] is None or bz [ 0 ] . shape != output_shape : \n 
~~~ bz [ 0 ] = CudaNdarray . zeros ( output_shape ) \n 
~~ input_bx_pycuda = to_complex_gpuarray ( bx [ 0 ] ) \n 
input_by_pycuda = to_complex_gpuarray ( by [ 0 ] ) \n 
output_b_pycuda = to_complex_gpuarray ( bz [ 0 ] ) \n 
sc_complex_dot_batched ( input_bx_pycuda , input_by_pycuda , \n 
output_b_pycuda ) \n 
~~ ~~ cufft = CuFFTOp ( ) \n 
cuifft = CuIFFTOp ( ) \n 
batched_complex_dot = BatchedComplexDotOp ( ) \n 
def mult_and_reduce ( input_fft_v , filters_fft_v , input_shape = None , \n 
filter_shape = None ) : \n 
if input_shape is None : \n 
~~ if filter_shape is None : \n 
~~ b , ic , i0 , i1_f , _ = input_shape \n 
oc = filter_shape [ 0 ] \n 
input_r = input_fft_v . reshape ( ( b , ic , i0 * i1_f , 2 ) ) \n 
filters_r = filters_fft_v . reshape ( ( oc , ic , i0 * i1_f , 2 ) ) \n 
output_s = batched_complex_dot ( input_s , filters_s ) \n 
output_r = output_s . dimshuffle ( 1 , 2 , 0 , 3 ) \n 
output = output_r . reshape ( ( b , oc , i0 , i1_f , 2 ) ) \n 
~~ def conv2d_fft ( input , filters , image_shape = None , filter_shape = None , \n 
border_mode = , pad_last_dim = False ) : \n 
if image_shape is None : \n 
~~~ image_shape = input . shape \n 
~~~ filter_shape = filters . shape \n 
~~ b , ic , i0 , i1 = image_shape \n 
oc , ic_ , f0 , f1 = filter_shape \n 
if border_mode == : \n 
~~~ o0 = i0 \n 
if pad_last_dim : \n 
~~~ o1 = i1 + 1 \n 
input_padded = T . zeros ( ( b , ic , o0 , o1 ) , dtype = ) \n 
input_padded = T . set_subtensor ( input_padded [ : , : , : i0 , : i1 ] , \n 
input ) \n 
~~~ o1 = i1 \n 
input_padded = input \n 
~~ filters_padded = T . zeros ( ( oc , ic , o0 , o1 ) , dtype = ) \n 
filters_padded = T . set_subtensor ( filters_padded [ : , : , : f0 , : f1 ] , \n 
filters ) \n 
~~ elif border_mode == : \n 
~~~ o0 = i0 + 2 * ( f0 - 1 ) \n 
o1 = i1 + 2 * ( f1 - 1 ) \n 
~~~ o1 = o1 + 1 \n 
input_padded = T . set_subtensor ( input_padded [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 ) , ( f1 - 1 ) : ( f1 - 1 + input ) \n 
input_padded , T . eq ( o1 % 2 , 0 ) ) \n 
input_flat = input_padded . reshape ( ( b * ic , o0 , o1 ) ) \n 
filters_flat = filters_padded . reshape ( ( oc * ic , o0 , o1 ) ) \n 
input_fft_v_shape = ( b , ic , o0 , o1 // 2 + 1 , 2 ) \n 
filters_fft_v_shape = ( oc , ic , o0 , o1 // 2 + 1 , 2 ) \n 
input_fft_v = input_fft_flat . reshape ( input_fft_v_shape ) \n 
filters_fft_v = filters_fft_flat . reshape ( filters_fft_v_shape ) \n 
output_fft_s = mult_and_reduce ( input_fft_v , filters_fft_v , \n 
input_shape = input_fft_v_shape , \n 
filter_shape = filters_fft_v_shape ) \n 
output_fft_flat = output_fft_s . reshape ( ( b * oc , o0 , o1 // 2 + 1 , 2 ) ) \n 
~~~ output = output_circ [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 - f0 + 1 ) , ( f1 - 1 ) : ( f1 - 1 + i1 - f1 + 1 ) ] \n 
~~~ output = output_circ [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 + f0 - 1 ) , ( f1 - 1 ) : ( f1 - 1 + i1 + f1 - 1 ) ] \n 
~~ output = ( 1.0 / T . cast ( o0 * o1 , ) ) * output \n 
return basic_ops . as_cuda_ndarray_variable ( output ) \n 
~~ def conv3d_fft ( input , filters , image_shape = None , filter_shape = None , \n 
~~ b , ic , i0 , i1 , i2 = image_shape \n 
oc , ic_ , f0 , f1 , f2 = filter_shape \n 
is_odd = T . eq ( T . mod ( input . shape [ 4 ] , 2 ) , 1 ) \n 
o1 = i1 \n 
o2 = i2 \n 
~~~ o2 = ifelse ( is_odd , o2 + 1 , o2 ) \n 
input_padded = T . zeros ( ( b , ic , o0 , o1 , o2 ) , dtype = ) \n 
input_padded = T . set_subtensor ( input_padded [ : , : , : i0 , : i1 , : i2 ] , \n 
~~ filters_padded = T . zeros ( ( oc , ic , o0 , o1 , o2 ) , dtype = ) \n 
filters_padded = T . set_subtensor ( filters_padded [ : , : , : f0 , : f1 , : f2 ] , \n 
o2 = i2 + 2 * ( f2 - 1 ) \n 
~~ input_flat = input_padded . reshape ( ( b * ic , o0 , o1 , o2 ) ) \n 
filters_flat = filters_padded . reshape ( ( oc * ic , o0 , o1 , o2 ) ) \n 
input_fft_v_shape = ( b , ic , o0 * o1 , o2 // 2 + 1 , 2 ) \n 
filters_fft_v_shape = ( oc , ic , o0 * o1 , o2 // 2 + 1 , 2 ) \n 
output_fft_flat = output_fft_s . reshape ( ( b * oc , o0 , o1 , o2 // 2 + 1 , 2 ) ) \n 
~~~ output = output_circ [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 - f0 + 1 ) , ( f1 - 1 ) : ( f1 - 1 + i1 - f1 + 1 ) , ( f2 - 1 ) : ( f2 - 1 + ~~ elif border_mode == : \n 
~~~ output = output_circ [ : , : , ( f0 - 1 ) : ( f0 - 1 + i0 + f0 - 1 ) , ( f1 - 1 ) : ( f1 - 1 + i1 + f1 - 1 ) , ( f2 - 1 ) : ( f2 - 1 + ~~ else : \n 
~~ output = ( 1.0 / T . cast ( o0 * o1 * o2 , ) ) * output \n 
import theano . tests . unittest_tools as utt \n 
import theano . sandbox . cuda as cuda \n 
~~~ raise SkipTest ( ) \n 
~~ if theano . config . mode == : \n 
~~~ mode_with_gpu = theano . compile . mode . get_mode ( ) . including ( ) \n 
mode_without_gpu = theano . compile . mode . get_mode ( ) \n 
~~~ mode_with_gpu = theano . compile . mode . get_default_mode ( ) . including ( ) \n 
mode_without_gpu = theano . compile . mode . get_default_mode ( ) . excluding ( ) \n 
~~ def test_GpuCrossentropySoftmaxArgmax1HotWithBias ( ) : \n 
n_in = 1000 \n 
batch_size = 4097 \n 
n_out = 1250 \n 
if not isinstance ( mode_with_gpu , theano . compile . DebugMode ) : \n 
~~~ n_in = 4098 \n 
n_out = 4099 \n 
~~ y = T . lvector ( ) \n 
b = T . fvector ( ) \n 
dot_result = T . fmatrix ( ) \n 
utt . seed_rng ( ) \n 
xx = numpy . asarray ( numpy . random . rand ( batch_size , n_in ) , \n 
dtype = numpy . float32 ) \n 
yy = numpy . ones ( ( batch_size , ) , dtype = ) \n 
b_values = numpy . zeros ( ( n_out , ) , dtype = ) \n 
W_values = numpy . asarray ( numpy . random . rand ( n_in , n_out ) , dtype = ) \n 
dot_value = numpy . asarray ( numpy . dot ( xx , W_values ) , dtype = ) \n 
del W_values \n 
p_y_given_x = T . nnet . softmax ( dot_result + b ) \n 
y_pred = T . argmax ( p_y_given_x , axis = - 1 ) \n 
loss = - T . mean ( T . log ( p_y_given_x ) [ T . arange ( y . shape [ 0 ] ) , y ] ) \n 
dW = T . grad ( loss , dot_result ) \n 
classify = theano . function ( inputs = [ y , b , dot_result ] , \n 
outputs = [ loss , y_pred , dW ] , \n 
mode = mode_without_gpu ) \n 
classify_gpu = theano . function ( inputs = [ y , b , dot_result ] , \n 
mode = mode_with_gpu ) \n 
assert any ( [ isinstance ( node . op , \n 
T . nnet . CrossentropySoftmaxArgmax1HotWithBias ) \n 
for node in classify . maker . fgraph . toposort ( ) ] ) \n 
cuda . nnet . GpuCrossentropySoftmaxArgmax1HotWithBias ) \n 
for node in classify_gpu . maker . fgraph . toposort ( ) ] ) \n 
out = classify ( yy , b_values , dot_value ) \n 
gout = classify_gpu ( yy , b_values , dot_value ) \n 
assert len ( out ) == len ( gout ) == 3 \n 
assert numpy . allclose ( out [ 0 ] , gout [ 0 ] ) \n 
assert numpy . allclose ( out [ 2 ] , gout [ 2 ] , atol = 3e-6 ) , numpy . absolute ( \n 
gout - out ) . max ( ) \n 
assert numpy . allclose ( out [ 1 ] , gout [ 1 ] ) , [ ( id , out [ 1 ] [ id ] , gout [ 1 ] [ id ] , val ) \n 
for id , val in enumerate ( out [ 1 ] - \n 
gout [ 1 ] ) \n 
if val != 0 ] \n 
~~ def test_GpuCrossentropySoftmax1HotWithBiasDx ( ) : \n 
~~~ n_out = 4099 \n 
~~ utt . seed_rng ( ) \n 
softmax_output_value = numpy . random . rand ( batch_size , \n 
n_out ) . astype ( ) \n 
dnll_value = numpy . asarray ( numpy . random . rand ( batch_size ) , dtype = ) \n 
y_idx_value = numpy . random . randint ( low = 0 , high = 5 , size = batch_size ) \n 
softmax_output = T . fmatrix ( ) \n 
softmax_output /= softmax_output . sum ( axis = 1 ) . reshape ( \n 
softmax_output . shape [ 1 ] , 1 ) \n 
op = theano . tensor . nnet . crossentropy_softmax_1hot_with_bias_dx ( \n 
dnll_value , \n 
softmax_output , \n 
y_idx_value ) \n 
cpu_f = theano . function ( [ softmax_output ] , op , mode = mode_without_gpu ) \n 
gpu_f = theano . function ( [ softmax_output ] , op , mode = mode_with_gpu ) \n 
assert any ( [ isinstance ( node . op , T . nnet . CrossentropySoftmax1HotWithBiasDx ) \n 
for node in cpu_f . maker . fgraph . toposort ( ) ] ) \n 
cuda . nnet . GpuCrossentropySoftmax1HotWithBiasDx ) \n 
for node in gpu_f . maker . fgraph . toposort ( ) ] ) \n 
cpu_out = cpu_f ( softmax_output_value ) \n 
gpu_out = gpu_f ( softmax_output_value ) \n 
rtol = 1e-5 \n 
atol = 1e-6 \n 
if not numpy . allclose ( cpu_out , gpu_out , rtol = rtol , atol = atol ) : \n 
~~~ abs_err , rel_err = T . numeric_grad . abs_rel_err ( cpu_out , gpu_out ) \n 
scaled_err = numpy . minimum ( abs_err / atol , rel_err / rtol ) \n 
max_i = scaled_err . argmax ( ) \n 
print ( , max_i , max_i / batch_size , end = ) \n 
print ( max_i % batch_size , max_i / n_out , max_i & n_out ) \n 
print ( , scaled_err . flatten ( ) [ max_i ] ) \n 
print ( , abs_err . flatten ( ) [ max_i ] ) \n 
print ( , rel_err . flatten ( ) [ max_i ] ) \n 
print ( , cpu_out . flatten ( ) [ max_i ] ) \n 
print ( , gpu_out . flatten ( ) [ max_i ] ) \n 
print ( , softmax_output_value . flatten ( ) [ max_i ] ) \n 
print ( , dnll_value [ max_i / n_out ] ) \n 
print ( , y_idx_value [ max_i / n_out ] ) \n 
rtol , atol ) \n 
~~ ~~ def test_softmax_with_bias ( ) : \n 
x = T . fmatrix ( ) \n 
z = T . nnet . softmax_with_bias ( x , T . arange ( x . shape [ 1 ] * 2 , \n 
dtype = ) [ : : 2 ] ) \n 
f = theano . function ( [ x ] , z , mode = mode_without_gpu ) \n 
f_gpu = theano . function ( [ x ] , z , mode = mode_with_gpu ) \n 
assert f . maker . fgraph . toposort ( ) [ - 1 ] . op == T . nnet . softmax_with_bias \n 
assert isinstance ( f_gpu . maker . fgraph . toposort ( ) [ - 2 ] . op , \n 
cuda . nnet . GpuSoftmaxWithBias ) \n 
def cmp ( n , m ) : \n 
~~~ data = numpy . arange ( n * m , dtype = ) . reshape ( n , m ) \n 
out = f ( data ) \n 
gout = f_gpu ( data ) \n 
assert numpy . allclose ( out , gout ) , numpy . absolute ( out - gout ) \n 
~~ cmp ( 2 , 5 ) \n 
cmp ( 2 << 15 , 5 ) \n 
cmp ( 4074 , 400 ) \n 
cmp ( 0 , 10 ) \n 
cmp ( 784 , 784 ) \n 
cmp ( 4 , 1000 ) \n 
cmp ( 4 , 1024 ) \n 
cmp ( 4 , 2000 ) \n 
cmp ( 4 , 2024 ) \n 
cmp ( 4 , 4074 ) \n 
cmp ( 2 , 10000 ) \n 
cmp ( 128 , 16 * 1024 ) \n 
cmp ( 128 , 64 * 1024 ) \n 
~~ class test_SoftMax ( unittest . TestCase ) : \n 
~~~ gpu_op = cuda . nnet . GpuSoftmax \n 
mode = mode_with_gpu . excluding ( "cudnn" ) \n 
do_big = True \n 
do_0 = True \n 
topo_idx = - 2 \n 
def _test_softmax ( \n 
self , \n 
x , \n 
x_gpu , \n 
f_z , \n 
f_gpu_z , \n 
cmp , \n 
check_types \n 
f_z_out = f_z ( x ) \n 
f_gpu_z_out = f_gpu_z ( x_gpu ) \n 
f = theano . function ( [ x ] , f_z_out , mode = mode_without_gpu ) \n 
f_gpu = theano . function ( [ x_gpu ] , f_gpu_z_out , mode = self . mode ) \n 
check_types ( f , f_gpu ) \n 
cmp ( 1 , 5 , f , f_gpu ) \n 
cmp ( 2 , 5 , f , f_gpu ) \n 
cmp ( 10 , 5 , f , f_gpu ) \n 
cmp ( 100 , 5 , f , f_gpu ) \n 
cmp ( 1000 , 5 , f , f_gpu ) \n 
cmp ( 10000 , 5 , f , f_gpu ) \n 
cmp ( 4074 , 400 , f , f_gpu ) \n 
cmp ( 784 , 784 , f , f_gpu ) \n 
cmp ( 4 , 1000 , f , f_gpu ) \n 
cmp ( 4 , 1024 , f , f_gpu ) \n 
cmp ( 4 , 2000 , f , f_gpu ) \n 
cmp ( 4 , 2024 , f , f_gpu ) \n 
cmp ( 4 , 4074 , f , f_gpu ) \n 
cmp ( 2 , 10000 , f , f_gpu ) \n 
cmp ( 128 , 16 * 1024 , f , f_gpu ) \n 
cmp ( 128 , 64 * 1024 , f , f_gpu ) \n 
cmp ( ( 2 << 15 ) - 1 , 5 , f , f_gpu ) \n 
cmp ( 5 , 2 << 15 , f , f_gpu ) \n 
return f , f_gpu \n 
~~ def _cmp ( self , n , m , f , f_gpu ) : \n 
utt . assert_allclose ( out , gout ) \n 
~~ def _check_types ( self , graph , graph_gpu , f_type , f_gpu_type ) : \n 
~~~ assert isinstance ( graph . maker . fgraph . toposort ( ) [ - 1 ] . op , f_type ) \n 
assert isinstance ( \n 
graph_gpu . maker . fgraph . toposort ( ) [ self . topo_idx ] . op , \n 
f_gpu_type \n 
~~ def test_softmax ( self ) : \n 
~~~ x = T . fmatrix ( ) \n 
z = T . nnet . softmax_op \n 
def check_types ( graph , graph_gpu ) : \n 
~~~ self . _check_types ( \n 
graph , \n 
graph_gpu , \n 
type ( z ) , \n 
self . gpu_op \n 
~~ f , f_gpu = self . _test_softmax ( \n 
z , \n 
self . _cmp , \n 
if self . do_big : \n 
~~~ self . _cmp ( 2 << 15 , 5 , f , f_gpu ) \n 
~~ if self . do_0 : \n 
~~~ self . _cmp ( 0 , 10 , f , f_gpu ) \n 
~~ ~~ ~~ from __future__ import absolute_import , print_function , division \n 
import copy \n 
from theano import tensor , scalar , gof \n 
from theano . compile import optdb \n 
from theano . compile . ops import shape_i \n 
from theano . gof import ( local_optimizer , EquilibriumDB , TopoOptimizer , \n 
SequenceDB , Optimizer , toolbox ) \n 
from theano . gof . optdb import LocalGroupDB \n 
from theano . ifelse import IfElse \n 
from theano . scalar . basic import Scalar , Pow , Cast \n 
from theano . scan_module import scan_utils , scan_op , scan_opt \n 
from theano . tensor . nnet . conv import ConvOp \n 
from theano . tensor . nnet . blocksparse import SparseBlockGemv , SparseBlockOuter \n 
from theano . tensor . nnet . abstract_conv import ( AbstractConv2d , \n 
AbstractConv2d_gradWeights , \n 
AbstractConv2d_gradInputs ) \n 
from theano . tests . breakpoint import PdbBreakpoint \n 
from . type import ( GpuArrayType , GpuArrayConstant , get_context , \n 
ContextNotDefined ) \n 
from . basic_ops import ( as_gpuarray_variable , infer_context_name , \n 
host_from_gpu , GpuToGpu , \n 
HostFromGpu , GpuFromHost , \n 
GpuSplit , GpuContiguous , gpu_contiguous , \n 
GpuAlloc , GpuAllocEmpty , GpuReshape , \n 
GpuEye , gpu_join , GpuJoin ) \n 
from . blas import ( gpu_dot22 , GpuGemv , GpuGemm , GpuGer , GpuGemmBatch , \n 
gpugemm_no_inplace , gpugemmbatch_no_inplace ) \n 
from . blocksparse import GpuSparseBlockGemv , GpuSparseBlockOuter \n 
from . nnet import ( GpuCrossentropySoftmaxArgmax1HotWithBias , \n 
GpuCrossentropySoftmax1HotWithBiasDx , \n 
GpuSoftmaxWithBias , GpuSoftmax ) \n 
from . elemwise import ( GpuElemwise , GpuDimShuffle , GpuCAReduceCuda , \n 
GpuCAReduceCPY ) \n 
from . subtensor import ( GpuIncSubtensor , GpuSubtensor , \n 
GpuAdvancedSubtensor1 , \n 
GpuAdvancedIncSubtensor1 , \n 
GpuAdvancedIncSubtensor1_dev20 ) \n 
from . opt_util import alpha_merge , output_merge \n 
_logger = logging . getLogger ( "theano.sandbox.gpuarray.opt" ) \n 
gpu_optimizer = EquilibriumDB ( ) \n 
gpu_cut_copies = EquilibriumDB ( ) \n 
gpu_seqopt = SequenceDB ( ) \n 
conv_groupopt = LocalGroupDB ( ) \n 
conv_groupopt . __name__ = "gpua_conv_opts" \n 
gpu_seqopt . register ( , gpu_optimizer , 1 , \n 
, , ) \n 
gpu_seqopt . register ( , gpu_cut_copies , 2 , \n 
optdb . register ( , gpu_seqopt , \n 
optdb . __position__ . get ( , 49.5 ) - 1 , \n 
def register_opt ( * tags , ** kwargs ) : \n 
~~~ def f ( local_opt ) : \n 
~~~ name = ( kwargs and kwargs . pop ( ) ) or local_opt . __name__ \n 
gpu_optimizer . register ( name , local_opt , , , * tags ) \n 
return local_opt \n 
~~ def register_inplace ( * tags , ** kwargs ) : \n 
optdb . register ( \n 
name , TopoOptimizer ( \n 
local_opt , failure_callback = TopoOptimizer . warn_inplace ) , \n 
60 , , , , * tags ) \n 
~~ register_opt ( ) ( theano . tensor . opt . local_track_shape_i ) \n 
register_opt ( final_opt = True , name = ) ( \n 
tensor . opt . constant_folding ) \n 
gpu_optimizer . register ( , \n 
theano . tensor . opt . local_remove_all_assert , \n 
def safe_to_gpu ( x , ctx_name ) : \n 
~~~ if isinstance ( x . type , tensor . TensorType ) : \n 
~~~ return GpuFromHost ( ctx_name ) ( x ) \n 
~~~ return x \n 
~~ ~~ def safe_to_cpu ( x ) : \n 
~~~ if isinstance ( x . type , GpuArrayType ) : \n 
~~~ return host_from_gpu ( x ) \n 
~~ ~~ def op_lifter ( OP , cuda_only = False ) : \n 
def f ( maker ) : \n 
~~~ def local_opt ( node ) : \n 
~~~ if type ( node . op ) in OP : \n 
~~~ replace = False \n 
context_name = None \n 
for i in node . inputs : \n 
~~~ if i . owner and i . owner . op == host_from_gpu : \n 
~~~ context_name = i . owner . inputs [ 0 ] . type . context_name \n 
replace = True \n 
~~ ~~ if not replace : \n 
~~~ clients = [ c for o in node . outputs for c in o . clients ] \n 
replace = len ( clients ) != 0 \n 
for c , idx in clients : \n 
~~~ if ( c == or \n 
not isinstance ( c . op , GpuFromHost ) ) : \n 
~~ ~~ if replace : \n 
~~~ context_name = clients [ 0 ] [ 0 ] . op . context_name \n 
~~ ~~ if ( not replace or \n 
( cuda_only and \n 
get_context ( context_name ) . kind != ) ) : \n 
~~ for i in node . inputs : \n 
~~~ i . tag . context_name = context_name \n 
~~ new_op = maker ( node , context_name ) \n 
if new_op and new_op != node . op : \n 
~~~ if isinstance ( new_op , theano . Op ) : \n 
~~~ return [ safe_to_cpu ( o ) for o in \n 
new_op ( * node . inputs , return_list = True ) ] \n 
~~ elif isinstance ( new_op , ( tuple , list ) ) : \n 
~~~ return [ safe_to_cpu ( o ) for o in new_op ] \n 
~~~ return [ host_from_gpu ( new_op ) ] \n 
~~ ~~ ~~ return False \n 
~~ local_opt . __name__ = maker . __name__ \n 
return local_optimizer ( OP ) ( local_opt ) \n 
~~ class InputToGpuOptimizer ( Optimizer ) : \n 
def add_requirements ( self , fgraph ) : \n 
~~~ fgraph . attach_feature ( toolbox . ReplaceValidate ( ) ) \n 
~~ def apply ( self , fgraph ) : \n 
~~~ for input in fgraph . inputs : \n 
~~~ if isinstance ( input . type , GpuArrayType ) : \n 
~~ if ( all ( cl [ 0 ] == or isinstance ( cl [ 0 ] . op , GpuFromHost ) \n 
for cl in input . clients ) ) : \n 
~~ target = getattr ( input . tag , , None ) \n 
if target == : \n 
~~~ new_input = host_from_gpu ( GpuFromHost ( target ) ( input ) ) \n 
fgraph . replace_validate ( input , new_input , \n 
"InputToGpuOptimizer" ) \n 
~~ except ContextNotDefined : \n 
~~~ if hasattr ( input . tag , ) : \n 
~~ pass \n 
~~ ~~ ~~ ~~ gpu_seqopt . register ( , InputToGpuOptimizer ( ) , \n 
0 , , , ) \n 
@ local_optimizer ( [ GpuFromHost , GpuToGpu , host_from_gpu ] ) \n 
def local_cut_gpu_transfers ( node ) : \n 
~~~ if ( isinstance ( node . op , GpuFromHost ) and \n 
node . inputs [ 0 ] . owner and \n 
isinstance ( node . inputs [ 0 ] . owner . op , HostFromGpu ) ) : \n 
~~~ other = node . inputs [ 0 ] . owner . inputs [ 0 ] \n 
if node . op . context_name == other . type . context_name : \n 
~~~ return [ other ] \n 
~~~ return [ GpuToGpu ( node . op . context_name ) ( other ) ] \n 
~~ ~~ elif ( isinstance ( node . op , HostFromGpu ) and \n 
node . inputs [ 0 ] . owner ) : \n 
~~~ n2 = node . inputs [ 0 ] . owner \n 
if isinstance ( n2 . op , GpuFromHost ) : \n 
~~~ return [ n2 . inputs [ 0 ] ] \n 
~~ if isinstance ( n2 . op , GpuToGpu ) : \n 
~~~ return [ host_from_gpu ( n2 . inputs [ 0 ] ) ] \n 
~~ ~~ elif isinstance ( node . op , GpuToGpu ) : \n 
~~~ if node . inputs [ 0 ] . type . context_name == node . op . context_name : \n 
~~~ return [ node . inputs [ 0 ] ] \n 
~~ if node . inputs [ 0 ] . owner : \n 
~~~ return [ GpuFromHost ( node . op . context_name ) ( n2 . inputs [ 0 ] ) ] \n 
~~~ if node . op . context_name == n2 . inputs [ 0 ] . type . context_name : \n 
~~~ return [ node . op ( n2 . inputs [ 0 ] ) ] \n 
~~ ~~ ~~ ~~ ~~ gpu_cut_copies . register ( , local_cut_gpu_transfers , \n 
, , , ) \n 
gpu_cut_copies . register ( , \n 
tensor . opt . constant_folding , \n 
optdb [ ] . register ( , \n 
local_cut_gpu_transfers , \n 
@ register_opt ( ) \n 
@ local_optimizer ( [ tensor . Alloc ] ) \n 
def local_gpuaalloc2 ( node ) : \n 
~~~ get_context ( None ) \n 
~~ if ( isinstance ( node . op , tensor . Alloc ) and \n 
all ( c != and \n 
c . op == tensor . join and \n 
all ( i . owner and \n 
i . owner . op in [ host_from_gpu , tensor . alloc ] \n 
for i in c . inputs [ 1 : ] ) \n 
for c , idx in node . outputs [ 0 ] . clients ) ) : \n 
~~~ return [ host_from_gpu ( GpuAlloc ( None ) ( * node . inputs ) ) ] \n 
~~ ~~ @ register_opt ( ) \n 
@ op_lifter ( [ tensor . Alloc ] ) \n 
def local_gpuaalloc ( node , context_name ) : \n 
~~~ return GpuAlloc ( context_name ) ( * node . inputs ) \n 
~~ @ register_opt ( ) \n 
@ op_lifter ( [ tensor . AllocEmpty ] ) \n 
def local_gpuaallocempty ( node , context_name ) : \n 
~~~ return GpuAllocEmpty ( context_name = context_name , \n 
** node . op . _props_dict ( ) ) ( * node . inputs ) \n 
@ local_optimizer ( [ GpuAlloc ] ) \n 
def local_gpualloc_memset_0 ( node ) : \n 
~~~ if isinstance ( node . op , GpuAlloc ) and not node . op . memset_0 : \n 
~~~ inp = node . inputs [ 0 ] \n 
if ( isinstance ( inp , GpuArrayConstant ) and \n 
inp . data . size == 1 and \n 
( numpy . asarray ( inp . data ) == 0 ) . all ( ) ) : \n 
~~~ new_op = GpuAlloc ( node . op . context_name , memset_0 = True ) \n 
return [ new_op ( * node . inputs ) ] \n 
~~ ~~ ~~ @ gof . local_optimizer ( [ GpuAllocEmpty ] ) \n 
def local_gpua_alloc_empty_to_zeros ( node ) : \n 
~~~ if isinstance ( node . op , GpuAllocEmpty ) : \n 
~~~ context_name = infer_context_name ( * node . inputs ) \n 
z = numpy . asarray ( 0 , dtype = node . outputs [ 0 ] . dtype ) \n 
return [ GpuAlloc ( ) ( as_gpuarray_variable ( z , context_name ) , \n 
* node . inputs ) ] \n 
~~ ~~ optdb . register ( , \n 
theano . tensor . opt . in2out ( local_gpua_alloc_empty_to_zeros ) , \n 
49.3 , \n 
@ local_optimizer ( [ GpuContiguous ] ) \n 
def local_gpu_contiguous_gpu_contiguous ( node ) : \n 
if isinstance ( node . op , GpuContiguous ) : \n 
if inp . owner and isinstance ( inp . owner . op , GpuContiguous ) : \n 
~~~ return [ inp ] \n 
~~ ~~ ~~ @ register_opt ( ) \n 
@ op_lifter ( [ tensor . extra_ops . CpuContiguous ] ) \n 
def local_gpu_contiguous ( node , context_name ) : \n 
~~~ return gpu_contiguous \n 
@ op_lifter ( [ tensor . Reshape ] ) \n 
def local_gpureshape ( node , context_name ) : \n 
~~~ op = node . op \n 
name = op . name \n 
if name : \n 
~~~ name = + name \n 
~~ res = GpuReshape ( op . ndim , op . name ) \n 
@ op_lifter ( [ tensor . Rebroadcast ] ) \n 
def local_gpu_rebroadcast ( node , context_name ) : \n 
~~~ return node . op ( as_gpuarray_variable ( node . inputs [ 0 ] , context_name ) ) \n 
@ op_lifter ( [ tensor . Flatten ] ) \n 
def local_gpuflatten ( node , context_name ) : \n 
shp = [ ] \n 
if op . outdim != 1 : \n 
~~~ shp = [ node . inputs [ 0 ] . shape [ i ] for i in range ( op . outdim - 1 ) ] \n 
~~ shp += [ - 1 ] \n 
res = GpuReshape ( op . outdim , None ) \n 
o = res ( node . inputs [ 0 ] , theano . tensor . as_tensor_variable ( shp ) ) \n 
return o \n 
@ op_lifter ( [ tensor . Elemwise ] ) \n 
def local_gpu_elemwise ( node , context_name ) : \n 
scal_op = op . scalar_op \n 
~~ if len ( node . outputs ) > 1 : \n 
~~ res = GpuElemwise ( scal_op , name = name , \n 
inplace_pattern = copy . copy ( op . inplace_pattern ) , \n 
nfunc_spec = op . nfunc_spec ) \n 
if isinstance ( op . scalar_op , Pow ) : \n 
~~~ out_dtype = node . outputs [ 0 ] . dtype \n 
if out_dtype not in [ , , ] : \n 
~~ new_inputs = [ ] \n 
for inp in node . inputs : \n 
~~~ if inp . dtype != out_dtype : \n 
~~~ gpu_cast_op = GpuElemwise ( Cast ( Scalar ( out_dtype ) ) ) \n 
new_inputs . append ( gpu_cast_op ( as_gpuarray_variable ( inp , context_name ) ) ) \n 
~~~ new_inputs . append ( as_gpuarray_variable ( inp , context_name ) ) \n 
~~ ~~ gpu_output = res ( * new_inputs ) \n 
cpu_output = host_from_gpu ( gpu_output ) \n 
return [ cpu_output ] \n 
~~~ return res \n 
~~ ~~ def max_inputs_to_GpuElemwise ( node ) : \n 
~~~ ptr_size = 8 \n 
int_size = 4 \n 
argument_limit = 232 \n 
ndim = node . inputs [ 0 ] . type . ndim \n 
size_param_mandatory = ( int_size * ( ndim + 1 ) ) + ( ptr_size + int_size * ndim ) * len ( node . outputs ) \n 
nb_bytes_avail = argument_limit - size_param_mandatory \n 
nb_bytes_per_input = ptr_size + ndim * int_size \n 
max_nb_inputs = nb_bytes_avail // nb_bytes_per_input \n 
return max_nb_inputs \n 
~~ gpu_local_elemwise_fusion = tensor . opt . local_elemwise_fusion_op ( \n 
GpuElemwise , \n 
max_inputs_to_GpuElemwise ) \n 
optdb . register ( , \n 
tensor . opt . FusionOptimizer ( gpu_local_elemwise_fusion ) , 71.00 , \n 
inplace_gpu_elemwise_opt = tensor . opt . inplace_elemwise_optimizer_op ( \n 
GpuElemwise ) \n 
optdb . register ( , inplace_gpu_elemwise_opt , 75 , \n 
@ op_lifter ( [ tensor . DimShuffle ] ) \n 
def local_gpua_dimshuffle ( node , context_name ) : \n 
~~~ return GpuDimShuffle ( node . op . input_broadcastable , \n 
node . op . new_order ) \n 
@ op_lifter ( [ tensor . SpecifyShape ] ) \n 
def local_gpua_specifyShape ( node , context_name ) : \n 
~~~ if isinstance ( node . inputs [ 0 ] . type , GpuArrayType ) : \n 
~~ inp = [ GpuFromHost ( context_name ) ( node . inputs [ 0 ] ) ] + node . inputs [ 1 : ] \n 
return tensor . specify_shape ( * inp ) \n 
@ op_lifter ( [ theano . compile . ops . Shape ] ) \n 
def local_gpua_shape ( node , context_name ) : \n 
~~ return [ GpuFromHost ( context_name ) ( node . inputs [ 0 ] ) . shape ] \n 
~~ def gpu_print_wrapper ( op , cnda ) : \n 
~~~ op . old_op . global_fn ( op . old_op , numpy . asarray ( cnda ) ) \n 
@ op_lifter ( [ tensor . printing . Print ] ) \n 
def local_gpu_print_op ( node , context_name ) : \n 
~~~ x , = node . inputs \n 
gpu_x = as_gpuarray_variable ( x , context_name = context_name ) \n 
new_op = node . op . __class__ ( global_fn = gpu_print_wrapper ) \n 
new_op . old_op = node . op \n 
return new_op ( gpu_x ) \n 
@ local_optimizer ( [ PdbBreakpoint ] ) \n 
def local_gpu_pdbbreakpoint_op ( node ) : \n 
~~~ if isinstance ( node . op , PdbBreakpoint ) : \n 
~~~ old_inputs = node . inputs \n 
old_outputs = node . outputs \n 
new_inputs = node . inputs [ : 1 ] \n 
input_transfered = [ ] \n 
nb_monitored_vars = len ( node . outputs ) \n 
for i in range ( nb_monitored_vars ) : \n 
~~~ inp = old_inputs [ i + 1 ] \n 
out = old_outputs [ i ] \n 
input_is_from_gpu = ( inp . owner and \n 
isinstance ( inp . owner . op , HostFromGpu ) ) \n 
output_goes_to_gpu = False \n 
for c in out . clients : \n 
~~~ if c == : \n 
~~ if isinstance ( c [ 0 ] . op , GpuFromHost ) : \n 
~~~ output_goes_to_gpu = True \n 
context_name = c [ 0 ] . op . context_name \n 
~~ ~~ if input_is_from_gpu : \n 
~~~ new_inputs . append ( inp . owner . inputs [ 0 ] ) \n 
input_transfered . append ( True ) \n 
~~ elif output_goes_to_gpu : \n 
~~~ new_inputs . append ( GpuFromHost ( context_name ) ( inp ) ) \n 
~~~ new_inputs . append ( inp ) \n 
input_transfered . append ( False ) \n 
~~ ~~ if not any ( input_transfered ) : \n 
~~ new_op_outputs = node . op ( * new_inputs , return_list = True ) \n 
new_outputs = [ ] \n 
for i in range ( len ( new_op_outputs ) ) : \n 
~~~ if input_transfered [ i ] : \n 
~~~ new_outputs . append ( host_from_gpu ( new_op_outputs [ i ] ) ) \n 
~~~ new_outputs . append ( new_op_outputs [ i ] ) \n 
~~ ~~ return new_outputs \n 
@ op_lifter ( [ IfElse ] ) \n 
def local_gpua_lazy_ifelse ( node , context_name ) : \n 
~~~ if node . op . gpu : \n 
~~ c = node . inputs [ 0 ] \n 
inps = [ ] \n 
for v in node . inputs [ 1 : ] : \n 
~~~ if isinstance ( v . type , ( tensor . TensorType , GpuArrayType ) ) : \n 
~~~ inps . append ( as_gpuarray_variable ( v , context_name ) ) \n 
~~~ inps . append ( v ) \n 
~~ ~~ return IfElse ( node . op . n_outs , gpu = True ) ( c , * inps , return_list = True ) \n 
@ op_lifter ( [ tensor . Join ] ) \n 
def local_gpua_join ( node , context_name ) : \n 
~~~ return gpu_join \n 
@ local_optimizer ( [ GpuJoin ] ) \n 
def local_gpuajoin_1 ( node ) : \n 
~~~ if ( isinstance ( node . op , GpuJoin ) and \n 
len ( node . inputs ) == 2 ) : \n 
~~~ return [ node . inputs [ 1 ] ] \n 
@ op_lifter ( [ tensor . Split ] ) \n 
def local_gpua_split ( node , context_name ) : \n 
~~~ return GpuSplit ( node . op . len_splits ) \n 
@ op_lifter ( [ tensor . Subtensor ] ) \n 
def local_gpua_subtensor ( node , context_name ) : \n 
~~~ x = node . inputs [ 0 ] \n 
if ( x . owner and isinstance ( x . owner . op , HostFromGpu ) ) : \n 
~~~ gpu_x = x . owner . inputs [ 0 ] \n 
if ( gpu_x . owner and \n 
isinstance ( gpu_x . owner . op , GpuFromHost ) and \n 
not gpu_x . owner . inputs [ 0 ] . owner ) : \n 
~~~ if len ( x . clients ) == 1 : \n 
~~~ if any ( [ n == or any ( [ isinstance ( v . type , GpuArrayType ) \n 
for v in n . inputs + n . outputs ] ) \n 
for n , _ in node . outputs [ 0 ] . clients ] ) : \n 
~~~ return [ host_from_gpu ( gpu_x . owner . op ( node . outputs [ 0 ] ) ) ] \n 
~~ ~~ ~~ ~~ return GpuSubtensor ( node . op . idx_list ) \n 
@ op_lifter ( [ tensor . IncSubtensor ] ) \n 
def local_gpua_incsubtensor ( node , context_name ) : \n 
~~~ op = GpuIncSubtensor ( node . op . idx_list , node . op . inplace , \n 
node . op . set_instead_of_inc , \n 
node . op . destroyhandler_tolerate_aliased ) \n 
ret = op ( * node . inputs ) \n 
val = getattr ( node . outputs [ 0 ] . tag , , True ) \n 
ret . tag . nan_guard_mode_check = val \n 
return ret \n 
@ op_lifter ( [ tensor . AdvancedSubtensor1 ] ) \n 
def local_gpua_advanced_subtensor ( node , context_name ) : \n 
~~~ return GpuAdvancedSubtensor1 ( ) \n 
@ op_lifter ( [ tensor . AdvancedIncSubtensor1 ] ) \n 
def local_gpua_advanced_incsubtensor ( node , context_name ) : \n 
~~~ context = get_context ( context_name ) \n 
if context . kind != : \n 
~~ x , y , ilist = node . inputs \n 
if ( x . type . dtype != y . type . dtype ) : \n 
~~~ dtype = scalar . upcast ( x . type . dtype , y . type . dtype ) \n 
if x . type . dtype != dtype : \n 
~~~ x = tensor . cast ( x , dtype ) \n 
~~ if y . type . dtype != dtype : \n 
~~~ y = tensor . cast ( y , dtype ) \n 
~~ ~~ set_instead_of_inc = node . op . set_instead_of_inc \n 
compute_capability = int ( context . bin_id [ - 2 ] ) \n 
if ( compute_capability < 2 or x . ndim != 2 or y . ndim != 2 ) : \n 
~~~ return GpuAdvancedIncSubtensor1 ( \n 
set_instead_of_inc = set_instead_of_inc ) \n 
~~~ return GpuAdvancedIncSubtensor1_dev20 ( \n 
@ op_lifter ( [ tensor . CAReduce , tensor . Sum , tensor . elemwise . Prod ] ) \n 
def local_gpua_careduce ( node , context_name ) : \n 
~~~ if isinstance ( node . op . scalar_op , ( scalar . Add , scalar . Mul , \n 
scalar . Maximum , scalar . Minimum ) ) : \n 
~~~ ctx = get_context ( context_name ) \n 
if ctx . kind == : \n 
~~~ op = GpuCAReduceCPY \n 
if node . op . scalar_op not in [ scalar . add , scalar . mul ] : \n 
~~ ~~ elif ctx . kind == : \n 
~~~ op = GpuCAReduceCuda \n 
~~ x , = node . inputs \n 
greduce = op ( \n 
node . op . scalar_op , axis = node . op . axis , \n 
dtype = getattr ( node . op , , None ) , \n 
acc_dtype = getattr ( node . op , , None ) ) \n 
gvar = greduce ( x ) \n 
if ( op is GpuCAReduceCPY or \n 
gvar . owner . op . supports_c_code ( [ GpuFromHost ( context_name ) ( x ) ] ) ) : \n 
~~~ return greduce \n 
~~~ if node . op . axis is None : \n 
~~~ reduce_mask = [ 1 ] * x . type . ndim \n 
~~~ reduce_mask = [ 0 ] * x . type . ndim \n 
for a in node . op . axis : \n 
~~~ assert reduce_mask [ a ] == 0 \n 
reduce_mask [ a ] = 1 \n 
~~ ~~ shape_of = node . fgraph . shape_feature . shape_of \n 
x_shape = shape_of [ x ] \n 
new_in_shp = [ x_shape [ 0 ] ] \n 
new_mask = [ reduce_mask [ 0 ] ] \n 
for i in xrange ( 1 , x . type . ndim ) : \n 
~~~ if reduce_mask [ i ] == reduce_mask [ i - 1 ] : \n 
~~~ new_in_shp [ - 1 ] *= x_shape [ i ] \n 
~~~ new_mask . append ( reduce_mask [ i ] ) \n 
new_in_shp . append ( x_shape [ i ] ) \n 
~~ ~~ new_axis = [ ] \n 
for idx , m in enumerate ( new_mask ) : \n 
~~~ if m == 1 : \n 
~~~ new_axis . append ( idx ) \n 
~~ ~~ greduce = op ( \n 
node . op . scalar_op , \n 
axis = new_axis , reduce_mask = new_mask , \n 
reshaped_x = x . reshape ( tensor . stack ( new_in_shp ) ) \n 
gpu_reshaped_x = GpuFromHost ( context_name ) ( reshaped_x ) \n 
gvar = greduce ( gpu_reshaped_x ) \n 
reshaped_gpu_inputs = [ gpu_reshaped_x ] \n 
if greduce . supports_c_code ( reshaped_gpu_inputs ) : \n 
~~~ reduce_reshaped_x = host_from_gpu ( \n 
greduce ( gpu_reshaped_x ) ) \n 
if reduce_reshaped_x . ndim != node . outputs [ 0 ] . ndim : \n 
~~~ unreshaped_reduce = reduce_reshaped_x . reshape ( \n 
tensor . stack ( shape_of [ node . outputs [ 0 ] ] ) ) \n 
~~~ unreshaped_reduce = reduce_reshaped_x \n 
~~ return [ unreshaped_reduce ] \n 
~~ ~~ ~~ ~~ @ register_opt ( ) \n 
@ op_lifter ( [ tensor . blas . Gemv , tensor . blas_c . CGemv ] ) \n 
def local_gpua_gemv ( node , context_name ) : \n 
~~~ return GpuGemv ( inplace = node . op . inplace ) \n 
@ op_lifter ( [ tensor . blas . Gemm ] ) \n 
def local_gpua_gemm ( node , context_name ) : \n 
~~~ return GpuGemm ( inplace = node . op . inplace ) \n 
@ op_lifter ( [ tensor . blas . BatchedDot ] ) \n 
def local_gpua_gemmbatch ( node , context_name ) : \n 
~~~ a , b = node . inputs \n 
c = tensor . AllocEmpty ( a . dtype ) ( a . shape [ 0 ] , a . shape [ 1 ] , b . shape [ 2 ] ) \n 
return gpugemmbatch_no_inplace ( c , 1.0 , a , b , 0.0 ) \n 
@ op_lifter ( [ tensor . basic . Dot ] ) \n 
def local_gpua_hgemm ( node , context_name ) : \n 
~~~ from theano . sandbox . cuda import nvcc_compiler \n 
if nvcc_compiler . nvcc_version < : \n 
~~ A = node . inputs [ 0 ] \n 
B = node . inputs [ 1 ] \n 
if ( A . ndim == 2 and B . ndim == 2 and \n 
A . dtype == and B . dtype == ) : \n 
~~~ fgraph = node . inputs [ 0 ] . fgraph \n 
C = GpuAllocEmpty ( dtype = , context_name = context_name ) ( \n 
shape_i ( A , 0 , fgraph ) , \n 
shape_i ( B , 1 , fgraph ) ) \n 
return gpugemm_no_inplace ( C , 1.0 , A , B , 0.0 ) \n 
@ alpha_merge ( GpuGemm , alpha_in = 1 , beta_in = 4 ) \n 
def local_gpuagemm_alpha_merge ( node , * inputs ) : \n 
~~~ return [ gpugemm_no_inplace ( * inputs ) ] \n 
@ output_merge ( GpuGemm , alpha_in = 1 , beta_in = 4 , out_in = 0 ) \n 
def local_gpuagemm_output_merge ( node , * inputs ) : \n 
@ alpha_merge ( GpuGemmBatch , alpha_in = 1 , beta_in = 4 ) \n 
def local_gpuagemmbatch_alpha_merge ( node , * inputs ) : \n 
~~~ return [ gpugemmbatch_no_inplace ( * inputs ) ] \n 
@ output_merge ( GpuGemmBatch , alpha_in = 1 , beta_in = 4 , out_in = 0 ) \n 
def local_gpuagemmbatch_output_merge ( node , * inputs ) : \n 
@ op_lifter ( [ tensor . blas . Ger , tensor . blas_c . CGer , tensor . blas_scipy . ScipyGer ] ) \n 
def local_gpua_ger ( node , context_name ) : \n 
~~~ return GpuGer ( inplace = node . op . destructive ) \n 
@ op_lifter ( [ tensor . blas . Dot22 ] ) \n 
def local_gpua_dot22 ( node , context_name ) : \n 
~~~ return gpu_dot22 \n 
@ op_lifter ( [ tensor . blas . Dot22Scalar ] ) \n 
def local_gpua_dot22scalar ( node , context_name ) : \n 
~~~ x , y , a = node . inputs \n 
x = as_gpuarray_variable ( x , context_name ) \n 
y = as_gpuarray_variable ( y , context_name ) \n 
z = GpuAllocEmpty ( x . dtype , context_name ) ( x . shape [ 0 ] , y . shape [ 1 ] ) \n 
return [ GpuGemm ( inplace = False ) ( z , a , x , y , 0 ) ] \n 
@ op_lifter ( [ tensor . basic . Eye ] ) \n 
def local_gpua_eye ( node , context_name ) : \n 
~~~ return GpuEye ( dtype = node . op . dtype , context_name = context_name ) \n 
@ op_lifter ( [ tensor . nnet . CrossentropySoftmaxArgmax1HotWithBias ] , cuda_only = True ) \n 
def local_gpua_crossentropysoftmaxargmax1hotwithbias ( node , context_name ) : \n 
~~~ return GpuCrossentropySoftmaxArgmax1HotWithBias ( ) \n 
@ op_lifter ( [ tensor . nnet . CrossentropySoftmax1HotWithBiasDx ] , cuda_only = True ) \n 
def local_gpua_crossentropysoftmax1hotwithbiasdx ( node , context_name ) : \n 
~~~ return GpuCrossentropySoftmax1HotWithBiasDx ( ) \n 
@ op_lifter ( [ tensor . nnet . Softmax ] , cuda_only = True ) \n 
def local_gpua_softmax ( node , context_name ) : \n 
~~~ return GpuSoftmax ( ) \n 
@ op_lifter ( [ tensor . nnet . SoftmaxWithBias ] , cuda_only = True ) \n 
def local_gpua_softmaxwithbias ( node , context_name ) : \n 
~~~ return GpuSoftmaxWithBias ( ) \n 
@ op_lifter ( [ theano . tensor . opt . Assert ] ) \n 
def local_assert ( node , context_name ) : \n 
~~~ return [ host_from_gpu ( node . op ( as_gpuarray_variable ( node . inputs [ 0 ] , \n 
context_name ) , \n 
* node . inputs [ 1 : ] ) ) ] \n 
@ op_lifter ( [ ConvOp ] ) \n 
def local_error_convop ( node , context_name ) : \n 
@ op_lifter ( [ SparseBlockGemv ] ) \n 
def local_lift_sparseblockgemv ( node , context_name ) : \n 
~~~ return GpuSparseBlockGemv ( node . op . inplace ) \n 
@ op_lifter ( [ SparseBlockOuter ] ) \n 
def local_lift_sparseblockouter ( node , context_name ) : \n 
~~~ return GpuSparseBlockOuter ( node . op . inplace ) \n 
~~ @ register_inplace ( ) \n 
@ local_optimizer ( [ GpuSparseBlockGemv ] , inplace = True ) \n 
def local_inplace_sparseblockgemv ( node ) : \n 
~~~ if isinstance ( node . op , GpuSparseBlockGemv ) and not node . op . inplace : \n 
~~~ return [ GpuSparseBlockGemv ( inplace = True ) ( * node . inputs ) ] \n 
~~ ~~ @ register_inplace ( ) \n 
@ local_optimizer ( [ GpuSparseBlockOuter ] , inplace = True ) \n 
def local_inplace_sparseblockouter ( node ) : \n 
~~~ if isinstance ( node . op , GpuSparseBlockOuter ) and not node . op . inplace : \n 
~~~ return [ GpuSparseBlockOuter ( inplace = True ) ( * node . inputs ) ] \n 
@ op_lifter ( [ AbstractConv2d , \n 
AbstractConv2d_gradInputs ] ) \n 
def local_lift_abstractconv2d ( node , context_name ) : \n 
~~~ if isinstance ( node . outputs [ 0 ] . type , GpuArrayType ) : \n 
~~ inps = list ( node . inputs ) \n 
inps [ 0 ] = as_gpuarray_variable ( node . inputs [ 0 ] , \n 
context_name = context_name ) \n 
inps [ 1 ] = as_gpuarray_variable ( node . inputs [ 1 ] , \n 
return [ node . op ( * inps ) ] \n 
~~ register_opt ( ) ( conv_groupopt ) \n 
@ register_opt ( "low_memory" ) \n 
@ local_optimizer ( [ GpuCAReduceCuda ] ) \n 
def local_gpu_elemwise_careduce ( node ) : \n 
if ( isinstance ( node . op , GpuCAReduceCuda ) and \n 
node . op . pre_scalar_op is None and \n 
isinstance ( node . inputs [ 0 ] . owner . op , GpuElemwise ) and \n 
isinstance ( node . inputs [ 0 ] . owner . op . scalar_op , scalar . basic . Sqr ) ) : \n 
inp = node . inputs [ 0 ] . owner . inputs [ 0 ] \n 
return [ GpuCAReduceCuda ( scalar_op = op . scalar_op , \n 
axis = op . axis , \n 
reduce_mask = op . reduce_mask , \n 
pre_scalar_op = scalar . basic . sqr ) ( inp ) ] \n 
~~ ~~ def tensor_to_gpu ( x , context_name ) : \n 
~~~ y = GpuArrayType ( broadcastable = x . type . broadcastable , \n 
context_name = context_name , \n 
dtype = x . type . dtype ) ( ) \n 
if x . name : \n 
~~~ y . name = x . name + \n 
~~ return y \n 
~~ ~~ def gpu_safe_new ( x , tag = ) : \n 
if hasattr ( x , ) and x . name is not None : \n 
~~~ nw_name = x . name + tag \n 
~~~ nw_name = None \n 
~~ if isinstance ( x , theano . Constant ) : \n 
~~~ return x . clone ( ) \n 
~~ nw_x = x . type ( ) \n 
nw_x . name = nw_name \n 
return nw_x \n 
~~ def gpu_reconstruct_graph ( inputs , outputs , tag = None ) : \n 
if tag is None : \n 
~~~ tag = \n 
~~ nw_inputs = [ gpu_safe_new ( x , tag ) for x in inputs ] \n 
givens = { } \n 
for nw_x , x in zip ( nw_inputs , inputs ) : \n 
~~~ givens [ x ] = nw_x \n 
~~ nw_outputs = scan_utils . clone ( outputs , replace = givens ) \n 
return ( nw_inputs , nw_outputs ) \n 
~~ @ register_opt ( , ) \n 
@ op_lifter ( [ scan_op . Scan ] ) \n 
def local_scan_to_gpua ( node , context_name ) : \n 
~~~ info = copy . deepcopy ( node . op . info ) \n 
if info . get ( , False ) : \n 
~~ info [ ] = True \n 
nw_ins = [ node . inputs [ 0 ] ] \n 
e = ( 1 + \n 
node . op . n_seqs + \n 
node . op . n_mit_mot + \n 
node . op . n_mit_sot + \n 
node . op . n_sit_sot + \n 
node . op . n_shared_outs ) \n 
nw_ins += [ safe_to_gpu ( x , context_name ) for x in node . inputs [ 1 : e ] ] \n 
b = e \n 
e = e + node . op . n_nit_sot \n 
nw_ins += node . inputs [ b : e ] \n 
nw_ins += [ safe_to_gpu ( x , context_name ) for x in node . inputs [ e : ] ] \n 
scan_ins = [ tensor_to_gpu ( x , context_name ) for x in node . op . inputs ] \n 
if node . op . info [ ] : \n 
~~~ scan_outs = [ safe_to_gpu ( x , context_name ) for x in node . op . outputs [ : - 1 ] ] \n 
scan_outs += [ node . op . outputs [ - 1 ] ] \n 
~~~ scan_outs = [ safe_to_gpu ( x , context_name ) for x in node . op . outputs ] \n 
~~ scan_outs = scan_utils . clone ( \n 
scan_outs , \n 
replace = list ( zip ( node . op . inputs , \n 
( safe_to_cpu ( x ) for x in scan_ins ) ) ) ) \n 
tmp_in , tmp_out = gpu_reconstruct_graph ( scan_ins , scan_outs ) \n 
local_fgraph = gof . FunctionGraph ( tmp_in , tmp_out , clone = True ) \n 
_cmodule_key = gof . CLinker ( ) . cmodule_key_ ( local_fgraph , [ ] ) \n 
info [ ] = hash ( _cmodule_key ) \n 
def typebuild ( dtype , broadcastable , context_name = context_name ) : \n 
~~~ return GpuArrayType ( dtype = dtype , broadcastable = broadcastable , \n 
~~ nw_op = scan_op . Scan ( scan_ins , scan_outs , info , \n 
typeConstructor = typebuild ) . make_node ( * nw_ins ) \n 
return nw_op . outputs \n 
~~ def _scan_type_infer ( node ) : \n 
~~ return typebuild \n 
~~ optdb . register ( , \n 
scan_opt . ScanInplaceOptimizer ( typeInfer = _scan_type_infer , \n 
gpua_flag = True ) , \n 
75 , \n 
from six import integer_types \n 
from theano import Op , Apply , shared , config , Variable \n 
from theano import gradient , function \n 
from theano import tensor \n 
from theano . tensor import ( TensorType , as_tensor_variable , get_vector_length , \n 
cast , opt , scal ) \n 
from theano . tensor import sqrt , log , sin , cos , join , prod \n 
from theano . gof import local_optimizer \n 
from . import multinomial \n 
import theano . sandbox . cuda \n 
from theano . sandbox . cuda import GpuOp \n 
from theano . sandbox . gpuarray . basic_ops import GpuKernelBase , Kernel \n 
from theano . sandbox . gpuarray . type import GpuArrayType \n 
from theano . sandbox . gpuarray . fp16_help import write_w \n 
from theano . sandbox . gpuarray . opt import ( register_opt as register_gpua , \n 
host_from_gpu as host_from_gpua ) \n 
if theano . sandbox . cuda . cuda_available : \n 
~~~ from theano . sandbox . cuda import ( CudaNdarrayType , \n 
float32_shared_constructor ) \n 
~~ def matVecModM ( A , s , m ) : \n 
~~~ assert A . dtype == \n 
return numpy . int32 ( numpy . sum ( ( A * s ) % m , 1 ) % m ) \n 
~~ def multMatVect ( v , A , m1 , B , m2 ) : \n 
if multMatVect . dot_modulo is None : \n 
~~~ A_sym = tensor . lmatrix ( ) \n 
s_sym = tensor . ivector ( ) \n 
m_sym = tensor . iscalar ( ) \n 
A2_sym = tensor . lmatrix ( ) \n 
s2_sym = tensor . ivector ( ) \n 
m2_sym = tensor . iscalar ( ) \n 
o = DotModulo ( ) ( A_sym , s_sym , m_sym , A2_sym , s2_sym , m2_sym ) \n 
multMatVect . dot_modulo = function ( \n 
[ A_sym , s_sym , m_sym , A2_sym , s2_sym , m2_sym ] , o , profile = False ) \n 
~~ f = multMatVect . dot_modulo \n 
f . input_storage [ 0 ] . storage [ 0 ] = A \n 
f . input_storage [ 1 ] . storage [ 0 ] = v [ : 3 ] \n 
f . input_storage [ 2 ] . storage [ 0 ] = m1 \n 
f . input_storage [ 3 ] . storage [ 0 ] = B \n 
f . input_storage [ 4 ] . storage [ 0 ] = v [ 3 : ] \n 
f . input_storage [ 5 ] . storage [ 0 ] = m2 \n 
f . fn ( ) \n 
r = f . output_storage [ 0 ] . storage [ 0 ] \n 
~~ multMatVect . dot_modulo = None \n 
class DotModulo ( Op ) : \n 
def make_node ( self , A , s , m , A2 , s2 , m2 ) : \n 
~~~ return Apply ( self , [ A , s , m , A2 , s2 , m2 ] , [ s . type ( ) ] ) \n 
~~ def perform ( self , node , inputs , outputs ) : \n 
~~~ ( A , s , m , A2 , s2 , m2 ) = inputs \n 
( out , ) = outputs \n 
o1 = matVecModM ( A , s , m ) \n 
o2 = matVecModM ( A2 , s2 , m2 ) \n 
out [ 0 ] = numpy . concatenate ( ( o1 , o2 ) ) \n 
~~~ return ( 6 , ) \n 
~~ def c_code ( self , node , name , inputs , outputs , sub ) : \n 
~~~ ( _A , _s , _m , _A2 , _s2 , _m2 ) = inputs \n 
( _z , ) = outputs \n 
MULT2 = numpy . int32 ( 21069 ) \n 
A1p72 = numpy . asarray ( [ [ 1516919229 , 758510237 , 499121365 ] , \n 
[ 1884998244 , 1516919229 , 335398200 ] , \n 
[ 601897748 , 1884998244 , 358115744 ] ] , \n 
dtype = ) \n 
A2p72 = numpy . asarray ( [ [ 1228857673 , 1496414766 , 954677935 ] , \n 
[ 1133297478 , 1407477216 , 1496414766 ] , \n 
[ 2002613992 , 1639496704 , 1407477216 ] ] , \n 
A1p134 = numpy . asarray ( \n 
[ [ 1702500920 , 1849582496 , 1656874625 ] , \n 
[ 828554832 , 1702500920 , 1512419905 ] , \n 
[ 1143731069 , 828554832 , 102237247 ] ] , \n 
A2p134 = numpy . asarray ( \n 
[ [ 796789021 , 1464208080 , 607337906 ] , \n 
[ 1241679051 , 1431130166 , 1464208080 ] , \n 
[ 1401213391 , 1178684362 , 1431130166 ] ] , \n 
np_int32_vals = [ numpy . int32 ( i ) for i in ( 0 , 7 , 9 , 15 , 16 , 22 , 24 ) ] \n 
def ff_2p134 ( rstate ) : \n 
~~~ return multMatVect ( rstate , A1p134 , M1 , A2p134 , M2 ) \n 
~~ def ff_2p72 ( rstate ) : \n 
~~~ return multMatVect ( rstate , A1p72 , M1 , A2p72 , M2 ) \n 
~~ def mrg_next_value ( rstate , new_rstate ) : \n 
~~~ x11 , x12 , x13 , x21 , x22 , x23 = rstate \n 
assert type ( x11 ) == numpy . int32 \n 
i0 , i7 , i9 , i15 , i16 , i22 , i24 = np_int32_vals \n 
y1 = ( ( ( x12 & MASK12 ) << i22 ) + ( x12 >> i9 ) + \n 
( ( x13 & MASK13 ) << i7 ) + ( x13 >> i24 ) ) \n 
assert type ( y1 ) == numpy . int32 \n 
~~~ y1 -= M1 \n 
~~ y1 += x13 \n 
if ( y1 < 0 or y1 >= M1 ) : \n 
~~ x13 = x12 \n 
x12 = x11 \n 
x11 = y1 \n 
y1 = ( ( x21 & MASK2 ) << i15 ) + ( MULT2 * ( x21 >> i16 ) ) \n 
if ( y1 < 0 or y1 >= M2 ) : \n 
~~~ y1 -= M2 \n 
~~ y2 = ( ( x23 & MASK2 ) << i15 ) + ( MULT2 * ( x23 >> i16 ) ) \n 
assert type ( y2 ) == numpy . int32 \n 
if ( y2 < 0 or y2 >= M2 ) : \n 
~~~ y2 -= M2 \n 
~~ y2 += x23 \n 
~~ y2 += y1 \n 
~~ x23 = x22 \n 
x22 = x21 \n 
x21 = y2 \n 
new_rstate [ ... ] = [ x11 , x12 , x13 , x21 , x22 , x23 ] \n 
assert new_rstate . dtype == numpy . int32 \n 
if ( x11 <= x21 ) : \n 
~~~ return ( x11 - x21 + M1 ) * NORM \n 
~~~ return ( x11 - x21 ) * NORM \n 
~~ ~~ class mrg_uniform_base ( Op ) : \n 
~~~ __props__ = ( "output_type" , "inplace" ) \n 
def __init__ ( self , output_type , inplace = False ) : \n 
~~~ Op . __init__ ( self ) \n 
self . output_type = output_type \n 
self . inplace = inplace \n 
if inplace : \n 
~~~ self . destroy_map = { 0 : [ 0 ] } \n 
~~ self . warned_numpy_version = False \n 
~~~ if self . inplace : \n 
~~~ s = "inplace" \n 
~~~ s = "no_inplace" \n 
~~ return self . __class__ . __name__ + "{%s,%s}" % ( self . output_type , s ) \n 
~~ def make_node ( self , rstate , size ) : \n 
~~~ return Apply ( self , \n 
[ rstate , size ] , \n 
[ rstate . type ( ) , self . output_type ( ) ] ) \n 
~~ def grad ( self , inputs , ograd ) : \n 
~~~ return [ gradient . grad_undefined ( self , k , inp , \n 
for k , inp in enumerate ( inputs ) ] \n 
~~ def R_op ( self , inputs , eval_points ) : \n 
~~~ return [ None for i in eval_points ] \n 
~~ ~~ class mrg_uniform ( mrg_uniform_base ) : \n 
def new ( cls , rstate , ndim , dtype , size ) : \n 
~~~ v_size = as_tensor_variable ( size ) \n 
if ndim is None : \n 
~~~ ndim = get_vector_length ( v_size ) \n 
~~ op = cls ( TensorType ( dtype , ( False , ) * ndim ) ) \n 
return op ( rstate , v_size ) \n 
~~~ rstate , size = inp \n 
o_rstate , o_sample = out \n 
n_elements = 1 \n 
for s in size : \n 
~~~ n_elements *= s \n 
~~ if n_elements > M1 : \n 
if not self . inplace : \n 
~~~ rstate = rstate . copy ( ) \n 
~~ n_streams , _ = rstate . shape \n 
rval = numpy . zeros ( n_elements , dtype = self . output_type . dtype ) \n 
err_orig = numpy . seterr ( over = ) \n 
~~~ for i in xrange ( n_elements ) : \n 
~~~ sample = mrg_next_value ( rstate [ i % n_streams ] , \n 
rstate [ i % n_streams ] ) \n 
rval [ i ] = sample \n 
~~~ numpy . seterr ( ** err_orig ) \n 
~~ o_rstate [ 0 ] = node . outputs [ 0 ] . type . filter ( rstate ) \n 
o_sample [ 0 ] = node . outputs [ 1 ] . type . filter ( rval . reshape ( size ) ) \n 
assert isinstance ( node . inputs [ 0 ] . type , TensorType ) \n 
if self . inplace : \n 
~~~ o_rstate_requirement = ( \n 
~~ ndim = self . output_type . ndim \n 
o_type_num = numpy . asarray ( 0 , dtype = self . output_type . dtype ) . dtype . num \n 
fail = sub [ ] \n 
if self . output_type . dtype == : \n 
~~~ otype = \n 
NORM = \n 
~~~ return ( 8 , ) \n 
~~ ~~ class GPU_mrg_uniform ( mrg_uniform_base , GpuOp ) : \n 
~~ op = cls ( CudaNdarrayType ( ( False , ) * ndim ) ) \n 
~~ def c_support_code_apply ( self , node , nodename ) : \n 
~~~ if self . output_type . dtype == : \n 
~~ def c_code ( self , node , nodename , inp , out , sub ) : \n 
inplace = int ( self . inplace ) \n 
ndim = self . output_type . ndim \n 
~~ SYNC = "CNDA_THREAD_SYNC" \n 
~~~ return ( 12 , ) \n 
~~ ~~ class GPUA_mrg_uniform ( GpuKernelBase , mrg_uniform_base ) : \n 
~~~ _f16_ok = True \n 
def get_params ( self , node ) : \n 
~~~ return node . inputs [ 0 ] . type . context \n 
~~ @ classmethod \n 
~~ op = cls ( GpuArrayType ( dtype , ( False , ) * ndim ) ) \n 
~~ def c_headers ( self ) : \n 
~~~ return super ( GPUA_mrg_uniform , self ) . c_headers ( ) + [ ] \n 
~~ def gpu_kernels ( self , node , name ) : \n 
~~~ write = write_w ( self . output_type . dtype ) \n 
mask = \n 
~~ elif self . output_type . dtype == : \n 
~~~ raise ValueError ( , \n 
self . output_type . dtype ) \n 
from pygpu import gpuarray \n 
return [ Kernel ( code = code , name = "mrg_uniform" , \n 
params = [ gpuarray . GpuArray , gpuarray . GpuArray , \n 
, ] , \n 
flags = Kernel . get_flags ( self . output_type . dtype , ) ) \n 
ctx = sub [ ] \n 
kname = self . gpu_kernels ( node , nodename ) [ 0 ] . objvar \n 
otypecode = str ( self . output_type . typecode ) \n 
~~~ return ( 11 , ) \n 
~~ ~~ def guess_n_streams ( size , warn = False ) : \n 
if ( isinstance ( size , ( tuple , list ) ) and \n 
all ( [ isinstance ( i , integer_types ) for i in size ] ) ) : \n 
~~~ r = 1 \n 
~~~ r *= s \n 
~~ if r > 6 : \n 
~~ return min ( r , 60 * 256 ) \n 
~~~ if warn : \n 
~~~ warnings . warn ( \n 
stacklevel = 3 ) \n 
~~ return 60 * 256 \n 
~~ ~~ class MRG_RandomStreams ( object ) : \n 
def updates ( self ) : \n 
~~~ return list ( self . state_updates ) \n 
~~ def __init__ ( self , seed = 12345 , use_cuda = None ) : \n 
~~~ self . state_updates = [ ] \n 
super ( MRG_RandomStreams , self ) . __init__ ( ) \n 
self . default_instance_seed = seed \n 
self . set_rstate ( seed ) \n 
if use_cuda is None : \n 
~~~ self . use_cuda = theano . sandbox . cuda . cuda_enabled \n 
~~~ self . use_cuda = use_cuda \n 
~~ ~~ def set_rstate ( self , seed ) : \n 
~~~ if isinstance ( seed , integer_types ) : \n 
~~~ if seed == 0 : \n 
~~~ raise ValueError ( , seed ) \n 
~~ elif seed >= M2 : \n 
~~~ raise ValueError ( % M2 , seed ) \n 
~~ self . rstate = numpy . asarray ( [ seed ] * 6 , dtype = ) \n 
~~ elif len ( seed ) == 6 : \n 
~~~ if seed [ 0 ] == 0 and seed [ 1 ] == 0 and seed [ 2 ] == 0 : \n 
, seed ) \n 
~~ if seed [ 3 ] == 0 and seed [ 4 ] == 0 and seed [ 5 ] == 0 : \n 
~~ if seed [ 0 ] >= M1 or seed [ 1 ] >= M1 or seed [ 2 ] >= M1 : \n 
% M1 , \n 
seed ) \n 
~~ if seed [ 3 ] >= M2 or seed [ 4 ] >= M2 or seed [ 5 ] >= M2 : \n 
% M2 , \n 
~~ self . rstate = numpy . asarray ( seed , dtype = ) \n 
~~ ~~ def seed ( self , seed = None ) : \n 
if seed is None : \n 
~~~ seed = self . default_instance_seed \n 
~~ self . set_rstate ( seed ) \n 
for old_r , new_r , size , nstreams in self . state_updates : \n 
~~~ if nstreams is None : \n 
~~~ nstreams = self . n_streams ( size ) \n 
~~ rstates = self . get_substream_rstates ( nstreams , \n 
new_r . owner . outputs [ 1 ] . dtype ) \n 
assert ( old_r . get_value ( borrow = True , \n 
return_internal_type = True ) . shape == \n 
rstates . shape ) \n 
assert rstates . dtype == old_r . dtype \n 
old_r . set_value ( rstates , borrow = True ) \n 
~~ ~~ def inc_rstate ( self ) : \n 
self . rstate = multMatVect ( self . rstate , A1p134 , M1 , A2p134 , M2 ) \n 
assert self . rstate . dtype == numpy . int32 \n 
~~ @ theano . configparser . change_flags ( compute_test_value = ) \n 
def get_substream_rstates ( self , n_streams , dtype , inc_rstate = True ) : \n 
assert isinstance ( dtype , str ) \n 
assert n_streams < 2 ** 72 \n 
assert n_streams > 0 \n 
rval = numpy . zeros ( ( n_streams , 6 ) , dtype = ) \n 
rval [ 0 ] = self . rstate \n 
~~~ multMatVect ( rval [ 0 ] , A1p72 , M1 , A2p72 , M2 ) \n 
f . input_storage [ 0 ] . storage [ 0 ] = A1p72 \n 
f . input_storage [ 2 ] . storage [ 0 ] = M1 \n 
f . input_storage [ 3 ] . storage [ 0 ] = A2p72 \n 
f . input_storage [ 5 ] . storage [ 0 ] = M2 \n 
for i in xrange ( 1 , n_streams ) : \n 
~~~ v = rval [ i - 1 ] \n 
rval [ i ] = f . output_storage [ 0 ] . storage [ 0 ] \n 
~~ if inc_rstate : \n 
~~~ self . inc_rstate ( ) \n 
~~ if self . use_cuda and dtype == : \n 
~~~ rval = rval . flatten ( ) \n 
tmp_float_buf = numpy . frombuffer ( rval . data , dtype = ) \n 
assert tmp_float_buf . shape == rval . shape \n 
assert ( tmp_float_buf . view ( ) == rval ) . all ( ) \n 
rval = tmp_float_buf \n 
~~ return rval \n 
~~ def n_streams ( self , size ) : \n 
~~~ return guess_n_streams ( size ) \n 
~~ def pretty_return ( self , node_rstate , new_rstate , sample , size , nstreams ) : \n 
~~~ sample . rstate = node_rstate \n 
sample . update = ( node_rstate , new_rstate ) \n 
self . state_updates . append ( ( node_rstate , new_rstate , size , nstreams ) ) \n 
node_rstate . default_update = new_rstate \n 
~~ def uniform ( self , size , low = 0.0 , high = 1.0 , ndim = None , dtype = None , \n 
nstreams = None ) : \n 
low = as_tensor_variable ( low ) \n 
high = as_tensor_variable ( high ) \n 
if dtype is None : \n 
~~~ dtype = scal . upcast ( config . floatX , low . dtype , high . dtype ) \n 
~~ low = cast ( low , dtype = dtype ) \n 
high = cast ( high , dtype = dtype ) \n 
if isinstance ( size , tuple ) : \n 
assert all ( [ isinstance ( i , ( numpy . integer , integer_types , Variable ) ) \n 
for i in size ] ) , msg \n 
if any ( [ isinstance ( i , ( numpy . integer , integer_types ) ) and i <= 0 \n 
for i in size ] ) : \n 
size ) \n 
~~~ if not ( isinstance ( size , Variable ) and size . ndim == 1 ) : \n 
~~ ~~ orig_nstreams = nstreams \n 
if nstreams is None : \n 
~~ rstates = self . get_substream_rstates ( nstreams , dtype ) \n 
if self . use_cuda and dtype == : \n 
~~~ node_rstate = float32_shared_constructor ( rstates ) \n 
assert isinstance ( node_rstate . type , CudaNdarrayType ) \n 
u = self . pretty_return ( node_rstate , \n 
* GPU_mrg_uniform . new ( node_rstate , \n 
ndim , dtype , size ) , \n 
size = size , nstreams = orig_nstreams ) \n 
~~~ node_rstate = shared ( rstates ) \n 
* mrg_uniform . new ( node_rstate , \n 
~~ node_rstate . tag . is_rng = True \n 
r = u * ( high - low ) + low \n 
if u . type . broadcastable != r . type . broadcastable : \n 
~~~ raise NotImplementedError ( \n 
~~ assert r . dtype == dtype \n 
~~ def binomial ( self , size = None , n = 1 , p = 0.5 , ndim = None , dtype = , \n 
~~~ if n == 1 : \n 
~~~ if dtype == and self . use_cuda : \n 
~~~ x = self . uniform ( size = size , dtype = dtype , nstreams = nstreams ) \n 
~~~ x = self . uniform ( size = size , nstreams = nstreams ) \n 
~~ return cast ( x < p , dtype ) \n 
~~ ~~ def multinomial ( self , size = None , n = 1 , pvals = None , ndim = None , dtype = , \n 
if pvals is None : \n 
~~ pvals = as_tensor_variable ( pvals ) \n 
if size is not None : \n 
~~~ if any ( [ isinstance ( i , integer_types ) and i <= 0 for i in size ] ) : \n 
~~ ~~ if size is not None : \n 
~~ if ndim is not None : \n 
~~ if pvals . ndim == 2 : \n 
~~~ size = pvals [ : , 0 ] . shape * n \n 
unis = self . uniform ( size = size , ndim = 1 , nstreams = nstreams ) \n 
op = multinomial . MultinomialFromUniform ( dtype ) \n 
n_samples = as_tensor_variable ( n ) \n 
return op ( pvals , unis , n_samples ) \n 
~~ ~~ def multinomial_wo_replacement ( self , size = None , n = 1 , pvals = None , \n 
ndim = None , dtype = , nstreams = None ) : \n 
op = multinomial . MultinomialWOReplacementFromUniform ( dtype ) \n 
~~ ~~ def normal ( self , size , avg = 0.0 , std = 1.0 , ndim = None , \n 
dtype = None , nstreams = None ) : \n 
avg = as_tensor_variable ( avg ) \n 
std = as_tensor_variable ( std ) \n 
~~~ dtype = scal . upcast ( config . floatX , avg . dtype , std . dtype ) \n 
~~ avg = cast ( avg , dtype ) \n 
std = cast ( std , dtype ) \n 
evened = False \n 
constant = False \n 
if ( isinstance ( size , tuple ) and \n 
all ( [ isinstance ( i , ( numpy . integer , integer_types ) ) for i in size ] ) ) : \n 
~~~ constant = True \n 
n_samples = numpy . prod ( size , dtype = ) \n 
if n_samples % 2 == 1 : \n 
~~~ n_samples += 1 \n 
evened = True \n 
~~~ n_samples = prod ( size ) + ( prod ( size ) % 2 ) \n 
~~ flattened = self . uniform ( size = ( n_samples , ) , dtype = dtype , \n 
nstreams = nstreams ) \n 
if constant : \n 
~~~ U1 = flattened [ : n_samples // 2 ] \n 
U2 = flattened [ n_samples // 2 : ] \n 
~~~ U1 = flattened [ : prod ( flattened . shape ) // 2 ] \n 
U2 = flattened [ prod ( flattened . shape ) // 2 : ] \n 
~~ sqrt_ln_U1 = sqrt ( - 2.0 * log ( U1 ) ) \n 
first_half = sqrt_ln_U1 * cos ( \n 
numpy . array ( 2.0 * numpy . pi , dtype = dtype ) * U2 ) \n 
second_half = sqrt_ln_U1 * sin ( \n 
normal_samples = join ( 0 , first_half , second_half ) \n 
final_samples = None \n 
if evened : \n 
~~~ final_samples = normal_samples [ : - 1 ] \n 
~~ elif constant : \n 
~~~ final_samples = normal_samples \n 
~~~ final_samples = normal_samples [ : prod ( size ) ] \n 
~~ if not size : \n 
~~~ size = tensor . constant ( size , dtype = ) \n 
~~ final_samples = final_samples . reshape ( size ) \n 
final_samples = avg + std * final_samples \n 
assert final_samples . dtype == dtype \n 
return final_samples \n 
~~ ~~ @ register_gpua ( ) \n 
@ local_optimizer ( [ mrg_uniform ] ) \n 
def local_gpua_mrg ( node ) : \n 
~~~ if ( type ( node . op ) == mrg_uniform and \n 
isinstance ( node . inputs [ 0 ] . type , GpuArrayType ) ) : \n 
~~~ outs = GPUA_mrg_uniform . new ( node . inputs [ 0 ] , \n 
node . op . output_type . ndim , \n 
node . op . output_type . dtype , \n 
node . inputs [ 1 ] ) \n 
return [ outs [ 0 ] , host_from_gpua ( outs [ 1 ] ) ] \n 
~~ ~~ MRG_RNGs = ( mrg_uniform , GPU_mrg_uniform , GPUA_mrg_uniform ) \n 
@ local_optimizer ( MRG_RNGs ) \n 
def mrg_random_make_inplace ( node ) : \n 
if isinstance ( op , MRG_RNGs ) and not op . inplace : \n 
~~~ new_op = op . __class__ ( op . output_type , inplace = True ) \n 
return new_op . make_node ( * node . inputs ) . outputs \n 
opt . in2out ( mrg_random_make_inplace , ignore_newtrees = True ) , \n 
99 , , ) \n 
import scipy . sparse \n 
from theano import gof , tensor \n 
from theano . tensor . opt import register_specialize \n 
from theano . sparse . basic import ( \n 
as_sparse_variable , SparseType , add_s_s , neg , \n 
mul_s_s , mul_s_d , dot , \n 
CSMProperties , CSM , \n 
_is_sparse_variable , _is_dense_variable , CSC , CSR , \n 
csm_properties , csm_data , csm_indices , csm_indptr , csm_shape , \n 
_is_sparse , \n 
Remove0 , remove0 , \n 
Cast , bcast , wcast , icast , lcast , fcast , dcast , ccast , zcast , \n 
HStack , hstack , VStack , vstack , \n 
AddSSData , add_s_s_data , \n 
MulSV , mul_s_v , \n 
structured_monoid , \n 
structured_sigmoid , structured_exp , structured_log , structured_pow , \n 
structured_minimum , structured_maximum , structured_add , \n 
StructuredAddSV , structured_add_s_v , \n 
SamplingDot , sampling_dot ) \n 
from theano . sparse . opt import ( \n 
MulSDCSC , mul_s_d_csc , MulSDCSR , mul_s_d_csr , \n 
MulSVCSR , mul_s_v_csr , \n 
StructuredAddSVCSR , structured_add_s_v_csr , \n 
SamplingDotCSR , sampling_dot_csr , \n 
local_mul_s_d , local_mul_s_v , \n 
local_structured_add_s_v , local_sampling_dot_csr ) \n 
EliminateZeros = Remove0 \n 
eliminate_zeros = remove0 \n 
class Poisson ( gof . op . Op ) : \n 
def make_node ( self , x ) : \n 
~~~ x = as_sparse_variable ( x ) \n 
return gof . Apply ( self , [ x ] , [ x . type ( ) ] ) \n 
~~~ ( x , ) = inputs \n 
assert _is_sparse ( x ) \n 
assert x . format in [ "csr" , "csc" ] \n 
out [ 0 ] = x . copy ( ) \n 
out [ 0 ] . data = numpy . asarray ( numpy . random . poisson ( out [ 0 ] . data ) , \n 
dtype = x . dtype ) \n 
out [ 0 ] . eliminate_zeros ( ) \n 
~~ def grad ( self , inputs , outputs_gradients ) : \n 
return [ theano . gradient . grad_undefined ( op = self , x_pos = 0 , x = inputs [ 0 ] , \n 
comment = comment ) ] \n 
~~ def infer_shape ( self , node , ins_shapes ) : \n 
~~~ return ins_shapes \n 
~~ ~~ poisson = Poisson ( ) \n 
class Binomial ( gof . op . Op ) : \n 
__props__ = ( "format" , "dtype" ) \n 
def __init__ ( self , format , dtype ) : \n 
~~~ self . format = format \n 
self . dtype = dtype \n 
~~ def make_node ( self , n , p , shape ) : \n 
~~~ n = tensor . as_tensor_variable ( n ) \n 
p = tensor . as_tensor_variable ( p ) \n 
shape = tensor . as_tensor_variable ( shape ) \n 
return gof . Apply ( self , [ n , p , shape ] , \n 
[ SparseType ( dtype = self . dtype , \n 
format = self . format ) ( ) ] ) \n 
~~~ ( n , p , shape ) = inputs \n 
binomial = numpy . random . binomial ( n , p , size = shape ) \n 
csx_matrix = getattr ( scipy . sparse , self . format + ) \n 
out [ 0 ] = csx_matrix ( binomial , dtype = self . dtype ) \n 
~~ def connection_pattern ( self , node ) : \n 
~~~ return [ [ True ] , [ True ] , [ False ] ] \n 
~~ def grad ( self , inputs , gout ) : \n 
( gz , ) = gout \n 
return [ theano . gradient . grad_undefined ( op = self , x_pos = 0 , x = n , \n 
comment = comment_n ) , \n 
theano . gradient . grad_undefined ( op = self , x_pos = 1 , x = p , \n 
comment = comment_p ) , \n 
theano . gradient . disconnected_type ( ) ] \n 
~~~ return [ ( node . inputs [ 2 ] [ 0 ] , node . inputs [ 2 ] [ 1 ] ) ] \n 
~~ ~~ csr_fbinomial = Binomial ( , ) \n 
csc_fbinomial = Binomial ( , ) \n 
csr_dbinomial = Binomial ( , ) \n 
csc_dbinomial = Binomial ( , ) \n 
class Multinomial ( gof . op . Op ) : \n 
def make_node ( self , n , p ) : \n 
p = as_sparse_variable ( p ) \n 
assert p . format in [ "csr" , "csc" ] \n 
return gof . Apply ( self , [ n , p ] , [ p . type ( ) ] ) \n 
~~~ ( n , p ) = inputs \n 
assert _is_sparse ( p ) \n 
if p . format != : \n 
~~~ raise NotImplemented ( ) \n 
~~ out [ 0 ] = p . copy ( ) \n 
if n . ndim == 0 : \n 
~~~ for i in xrange ( p . shape [ 0 ] ) : \n 
~~~ k , l = p . indptr [ i ] , p . indptr [ i + 1 ] \n 
out [ 0 ] . data [ k : l ] = numpy . random . multinomial ( n , p . data [ k : l ] ) \n 
~~ ~~ elif n . ndim == 1 : \n 
~~~ if n . shape [ 0 ] != p . shape [ 0 ] : \n 
~~ for i in xrange ( p . shape [ 0 ] ) : \n 
out [ 0 ] . data [ k : l ] = numpy . random . multinomial ( n [ i ] , p . data [ k : l ] ) \n 
~~ ~~ ~~ def grad ( self , inputs , outputs_gradients ) : \n 
theano . gradient . grad_undefined ( op = self , x_pos = 1 , x = inputs [ 1 ] , \n 
comment = comment_p ) ] \n 
~~~ return [ ins_shapes [ 1 ] ] \n 
~~ ~~ multinomial = Multinomial ( ) \n 
import numpy as N \n 
from theano . tensor import basic as T \n 
from theano . tensor . blas_headers import blas_header_text , blas_header_version \n 
from theano . tensor . blas import ldflags \n 
from theano . misc import strutil \n 
from theano . gradient import grad_undefined \n 
class Conv3D ( theano . Op ) : \n 
def c_code_cache_version ( self ) : \n 
~~~ return ( 3 , blas_header_version ( ) ) \n 
~~ def make_node ( self , V , W , b , d ) : \n 
V_ = T . as_tensor_variable ( V ) \n 
W_ = T . as_tensor_variable ( W ) \n 
b_ = T . as_tensor_variable ( b ) \n 
d_ = T . as_tensor_variable ( d ) \n 
bcast = ( V_ . broadcastable [ 0 ] , False , False , False , W_ . broadcastable [ 0 ] ) \n 
node = theano . Apply ( self , inputs = [ V_ , W_ , b_ , d_ ] , \n 
outputs = [ T . TensorType ( V_ . dtype , bcast ) ( ) ] ) \n 
return node \n 
~~ def grad ( self , inputs , output_gradients ) : \n 
~~~ V , W , b , d = inputs \n 
dCdH , = output_gradients \n 
dCdV = theano . tensor . nnet . convTransp3D ( \n 
W , T . zeros_like ( V [ 0 , 0 , 0 , 0 , : ] ) , d , dCdH , V . shape [ 1 : 4 ] ) \n 
dCdV = T . patternbroadcast ( dCdV , V . broadcastable ) \n 
WShape = W . shape \n 
dCdW = theano . tensor . nnet . convGrad3D ( V , d , WShape , dCdH ) \n 
dCdW = T . patternbroadcast ( dCdW , W . broadcastable ) \n 
dCdb = T . sum ( dCdH , axis = ( 0 , 1 , 2 , 3 ) ) \n 
dCdb = T . patternbroadcast ( dCdb , b . broadcastable ) \n 
dCdd = grad_undefined ( \n 
self , 3 , inputs [ 3 ] , \n 
if in dir ( dCdH ) and dCdH . name is not None : \n 
~~~ dCdH_name = dCdH . name \n 
~~~ dCdH_name = \n 
~~ if in dir ( V ) and V . name is not None : \n 
~~~ V_name = V . name \n 
~~~ V_name = \n 
~~ if in dir ( W ) and W . name is not None : \n 
~~~ W_name = W . name \n 
~~~ W_name = \n 
~~ if in dir ( b ) and b . name is not None : \n 
~~~ b_name = b . name \n 
~~~ b_name = \n 
~~ dCdV . name = + dCdH_name + + V_name + \n 
dCdW . name = ( + dCdH_name + + V_name + \n 
+ W_name + ) \n 
dCdb . name = ( + dCdH_name + + V_name + \n 
+ W_name + + b_name + ) \n 
return [ dCdV , dCdW , dCdb , dCdd ] \n 
~~ def perform ( self , node , inputs , output_storage ) : \n 
output_storage [ 0 ] [ 0 ] = computeH ( V , W , b , d ) \n 
~~ def infer_shape ( self , node , input_shapes ) : \n 
~~~ V , W , b , d = node . inputs \n 
V_shape , W_shape , b_shape , d_shape = input_shapes \n 
dr = d [ 0 ] \n 
dc = d [ 1 ] \n 
dt = d [ 2 ] \n 
batch_size = V_shape [ 0 ] \n 
output_channels = W_shape [ 0 ] \n 
vidHeight = V_shape [ 1 ] \n 
filterHeight = W_shape [ 1 ] \n 
vidWidth = V_shape [ 2 ] \n 
filterWidth = W_shape [ 2 ] \n 
vidDur = V_shape [ 3 ] \n 
filterDur = W_shape [ 3 ] \n 
output_height = ( ( vidHeight - filterHeight ) // dr ) + 1 \n 
output_width = ( ( vidWidth - filterWidth ) // dc ) + 1 \n 
output_dur = ( ( vidDur - filterDur ) // dt ) + 1 \n 
rval = ( batch_size , output_height , output_width , output_dur , output_channels ) \n 
return [ rval ] \n 
~~~ return blas_header_text ( ) \n 
~~ def c_libraries ( self ) : \n 
~~~ return ldflags ( ) \n 
~~ def c_compile_args ( self ) : \n 
~~~ flags = ldflags ( libs = False , flags = True ) \n 
return flags \n 
~~ def c_lib_dirs ( self ) : \n 
~~~ return ldflags ( libs = False , libs_dir = True ) \n 
~~ def c_header_dirs ( self ) : \n 
~~~ return ldflags ( libs = False , include_dir = True ) \n 
~~ def c_code ( self , node , nodename , inputs , outputs , sub ) : \n 
H = outputs [ 0 ] \n 
VV , WV , bv , dv = node . inputs \n 
HV = node . outputs [ 0 ] \n 
if ( theano . config . blas . ldflags and \n 
VV . dtype == WV . dtype and HV . dtype == VV . dtype ) : \n 
~~~ if VV . dtype == : \n 
~~~ gemv = \n 
~~ elif VV . dtype == : \n 
~~~ raise Exception ( + V . value . dtype ) \n 
return strutil . render_string ( codeSource , locals ( ) ) \n 
~~ ~~ _conv3D = Conv3D ( ) \n 
def conv3D ( V , W , b , d ) : \n 
return _conv3D ( V , W , b , d ) \n 
~~ def computeH ( V , W , b , d ) : \n 
~~~ assert len ( W . shape ) == 5 \n 
assert len ( V . shape ) == 5 \n 
if len ( b . shape ) != 1 : \n 
~~~ print ( b . shape ) \n 
assert False \n 
~~ assert len ( d ) == 3 \n 
batchSize = V . shape [ 0 ] \n 
outputChannels = W . shape [ 0 ] \n 
inputChannels = V . shape [ 4 ] \n 
if W . shape [ 4 ] != inputChannels : \n 
filterWidth = W . shape [ 2 ] \n 
filterDur = W . shape [ 3 ] \n 
vidHeight = V . shape [ 1 ] \n 
vidWidth = V . shape [ 2 ] \n 
vidDur = V . shape [ 3 ] \n 
assert vidHeight >= filterHeight \n 
assert vidWidth >= filterWidth \n 
assert vidDur >= filterDur \n 
dx , dy , dt = d \n 
assert dx > 0 \n 
assert dy > 0 \n 
assert dt > 0 \n 
outputHeight = int ( ( vidHeight - filterHeight ) / dx ) + 1 \n 
outputWidth = int ( ( vidWidth - filterWidth ) / dy ) + 1 \n 
outputDur = int ( ( vidDur - filterDur ) / dt ) + 1 \n 
H = N . zeros ( ( batchSize , outputHeight , \n 
outputWidth , outputDur , outputChannels ) , dtype = V . dtype ) \n 
for i in xrange ( 0 , H . shape [ 0 ] ) : \n 
~~~ for j in xrange ( 0 , H . shape [ 4 ] ) : \n 
~~~ for x in xrange ( 0 , H . shape [ 1 ] ) : \n 
~~~ for y in xrange ( 0 , H . shape [ 2 ] ) : \n 
~~~ for t in xrange ( 0 , H . shape [ 3 ] ) : \n 
~~~ H [ i , x , y , t , j ] = b [ j ] \n 
for k in xrange ( 0 , filterHeight ) : \n 
~~~ for l in xrange ( 0 , filterWidth ) : \n 
~~~ for m in xrange ( 0 , filterDur ) : \n 
~~~ for z in xrange ( 0 , inputChannels ) : \n 
v = V [ i , d [ 0 ] * x + k , d [ 1 ] * y + l , d [ 2 ] * t + m , z ] \n 
H [ i , x , y , t , j ] += w * v \n 
~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ ~~ return H \n 
from theano import shared , function \n 
from theano . tensor . nnet . neighbours import images2neibs , neibs2images , Images2Neibs \n 
from theano . tests import unittest_tools \n 
class T_Images2Neibs ( unittest_tools . InferShapeTester ) : \n 
~~~ mode = mode_without_gpu \n 
op = Images2Neibs \n 
dtypes = [ , , ] \n 
def test_neibs ( self ) : \n 
~~~ for shape , pshape in [ ( ( 10 , 7 , 18 , 18 ) , ( 2 , 2 ) ) , \n 
( ( 10 , 7 , 6 , 18 ) , ( 3 , 2 ) ) , \n 
( ( 5 , 7 , 66 , 66 ) , ( 33 , 33 ) ) , \n 
( ( 5 , 7 , 68 , 66 ) , ( 34 , 33 ) ) \n 
] : \n 
~~~ for border in [ , ] : \n 
~~~ for dtype in self . dtypes : \n 
~~~ images = shared ( \n 
numpy . arange ( numpy . prod ( shape ) , dtype = dtype \n 
) . reshape ( shape ) ) \n 
neib_shape = T . as_tensor_variable ( pshape ) \n 
f = function ( [ ] , \n 
images2neibs ( images , neib_shape , mode = border ) , \n 
mode = self . mode ) \n 
neibs = f ( ) \n 
g = function ( [ ] , \n 
neibs2images ( neibs , neib_shape , images . shape ) , \n 
assert any ( [ isinstance ( node . op , self . op ) \n 
for node in f . maker . fgraph . toposort ( ) ] ) \n 
assert numpy . allclose ( images . get_value ( borrow = True ) , g ( ) ) \n 
~~ ~~ ~~ ~~ def test_neibs_manual ( self ) : \n 
~~~ shape = ( 2 , 3 , 4 , 4 ) \n 
for dtype in self . dtypes : \n 
neib_shape = T . as_tensor_variable ( ( 2 , 2 ) ) \n 
for border in [ , ] : \n 
~~~ f = function ( [ ] , images2neibs ( images , neib_shape , mode = border ) , \n 
assert numpy . allclose ( neibs , \n 
[ [ 0 , 1 , 4 , 5 ] , \n 
[ 2 , 3 , 6 , 7 ] , \n 
[ 8 , 9 , 12 , 13 ] , \n 
[ 10 , 11 , 14 , 15 ] , \n 
[ 16 , 17 , 20 , 21 ] , \n 
[ 18 , 19 , 22 , 23 ] , \n 
[ 24 , 25 , 28 , 29 ] , \n 
[ 26 , 27 , 30 , 31 ] , \n 
[ 32 , 33 , 36 , 37 ] , \n 
[ 34 , 35 , 38 , 39 ] , \n 
[ 40 , 41 , 44 , 45 ] , \n 
[ 42 , 43 , 46 , 47 ] , \n 
[ 48 , 49 , 52 , 53 ] , \n 
[ 50 , 51 , 54 , 55 ] , \n 
[ 56 , 57 , 60 , 61 ] , \n 
[ 58 , 59 , 62 , 63 ] , \n 
[ 64 , 65 , 68 , 69 ] , \n 
[ 66 , 67 , 70 , 71 ] , \n 
[ 72 , 73 , 76 , 77 ] , \n 
[ 74 , 75 , 78 , 79 ] , \n 
[ 80 , 81 , 84 , 85 ] , \n 
[ 82 , 83 , 86 , 87 ] , \n 
[ 88 , 89 , 92 , 93 ] , \n 
[ 90 , 91 , 94 , 95 ] ] ) \n 
g = function ( [ ] , neibs2images ( neibs , neib_shape , images . shape ) , \n 
~~ ~~ ~~ def test_neibs_manual_step ( self ) : \n 
~~~ shape = ( 2 , 3 , 5 , 5 ) \n 
~~~ images = shared ( numpy . asarray ( numpy . arange ( numpy . prod ( \n 
shape ) ) . reshape ( shape ) , dtype = dtype ) ) \n 
neib_shape = T . as_tensor_variable ( ( 3 , 3 ) ) \n 
neib_step = T . as_tensor_variable ( ( 2 , 2 ) ) \n 
~~~ f = function ( [ ] , \n 
images2neibs ( images , neib_shape , neib_step , \n 
mode = border ) , \n 
assert self . op in [ type ( node . op ) \n 
for node in f . maker . fgraph . toposort ( ) ] \n 
[ [ 0 , 1 , 2 , 5 , 6 , 7 , 10 , 11 , 12 ] , \n 
[ 2 , 3 , 4 , 7 , 8 , 9 , 12 , 13 , 14 ] , \n 
[ 10 , 11 , 12 , 15 , 16 , 17 , 20 , 21 , 22 ] , \n 
[ 12 , 13 , 14 , 17 , 18 , 19 , 22 , 23 , 24 ] , \n 
[ 25 , 26 , 27 , 30 , 31 , 32 , 35 , 36 , 37 ] , \n 
[ 27 , 28 , 29 , 32 , 33 , 34 , 37 , 38 , 39 ] , \n 
[ 35 , 36 , 37 , 40 , 41 , 42 , 45 , 46 , 47 ] , \n 
[ 37 , 38 , 39 , 42 , 43 , 44 , 47 , 48 , 49 ] , \n 
[ 50 , 51 , 52 , 55 , 56 , 57 , 60 , 61 , 62 ] , \n 
[ 52 , 53 , 54 , 57 , 58 , 59 , 62 , 63 , 64 ] , \n 
[ 60 , 61 , 62 , 65 , 66 , 67 , 70 , 71 , 72 ] , \n 
[ 62 , 63 , 64 , 67 , 68 , 69 , 72 , 73 , 74 ] , \n 
[ 75 , 76 , 77 , 80 , 81 , 82 , 85 , 86 , 87 ] , \n 
[ 77 , 78 , 79 , 82 , 83 , 84 , 87 , 88 , 89 ] , \n 
[ 85 , 86 , 87 , 90 , 91 , 92 , 95 , 96 , 97 ] , \n 
[ 87 , 88 , 89 , 92 , 93 , 94 , 97 , 98 , 99 ] , \n 
[ 100 , 101 , 102 , 105 , 106 , 107 , 110 , 111 , 112 ] , \n 
[ 102 , 103 , 104 , 107 , 108 , 109 , 112 , 113 , 114 ] , \n 
[ 110 , 111 , 112 , 115 , 116 , 117 , 120 , 121 , 122 ] , \n 
[ 112 , 113 , 114 , 117 , 118 , 119 , 122 , 123 , 124 ] , \n 
[ 125 , 126 , 127 , 130 , 131 , 132 , 135 , 136 , 137 ] , \n 
[ 127 , 128 , 129 , 132 , 133 , 134 , 137 , 138 , 139 ] , \n 
[ 135 , 136 , 137 , 140 , 141 , 142 , 145 , 146 , 147 ] , \n 
[ 137 , 138 , 139 , 142 , 143 , 144 , 147 , 148 , 149 ] ] ) \n 
~~ ~~ ~~ def test_neibs_bad_shape ( self ) : \n 
~~~ shape = ( 2 , 3 , 10 , 10 ) \n 
~~~ images = shared ( numpy . arange ( \n 
numpy . prod ( shape ) , dtype = dtype \n 
for neib_shape in [ ( 3 , 2 ) , ( 2 , 3 ) ] : \n 
~~~ neib_shape = T . as_tensor_variable ( neib_shape ) \n 
f = function ( [ ] , images2neibs ( images , neib_shape ) , \n 
self . assertRaises ( TypeError , f ) \n 
images2neibs ( images , neib_shape , \n 
mode = ) , \n 
f ( ) \n 
~~ ~~ ~~ def test_neibs_wrap_centered_step_manual ( self ) : \n 
~~~ expected1 = [ [ 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 ] , \n 
[ 21 , 22 , 23 , 1 , 2 , 3 , 6 , 7 , 8 ] , \n 
[ 23 , 24 , 20 , 3 , 4 , 0 , 8 , 9 , 5 ] , \n 
[ 9 , 5 , 6 , 14 , 10 , 11 , 19 , 15 , 16 ] , \n 
[ 6 , 7 , 8 , 11 , 12 , 13 , 16 , 17 , 18 ] , \n 
[ 8 , 9 , 5 , 13 , 14 , 10 , 18 , 19 , 15 ] , \n 
[ 19 , 15 , 16 , 24 , 20 , 21 , 4 , 0 , 1 ] , \n 
[ 16 , 17 , 18 , 21 , 22 , 23 , 1 , 2 , 3 ] , \n 
[ 18 , 19 , 15 , 23 , 24 , 20 , 3 , 4 , 0 ] ] \n 
expected2 = [ [ 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 ] , \n 
[ 22 , 23 , 24 , 2 , 3 , 4 , 7 , 8 , 9 ] , \n 
[ 14 , 10 , 11 , 19 , 15 , 16 , 24 , 20 , 21 ] , \n 
[ 12 , 13 , 14 , 17 , 18 , 19 , 22 , 23 , 24 ] ] \n 
expected3 = [ [ 19 , 15 , 16 , 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 , 14 , 10 , 11 ] , \n 
[ 17 , 18 , 19 , 22 , 23 , 24 , 2 , 3 , 4 , 7 , 8 , 9 , 12 , 13 , 14 ] , \n 
[ 9 , 5 , 6 , 14 , 10 , 11 , 19 , 15 , 16 , 24 , 20 , 21 , 4 , 0 , 1 ] , \n 
[ 7 , 8 , 9 , 12 , 13 , 14 , 17 , 18 , 19 , 22 , 23 , 24 , 2 , 3 , 4 ] ] \n 
expected4 = [ [ 23 , 24 , 20 , 21 , 22 , 3 , 4 , 0 , 1 , 2 , 8 , 9 , 5 , 6 , 7 ] , \n 
[ 21 , 22 , 23 , 24 , 20 , 1 , 2 , 3 , 4 , 0 , 6 , 7 , 8 , 9 , 5 ] , \n 
[ 13 , 14 , 10 , 11 , 12 , 18 , 19 , 15 , 16 , 17 , 23 , 24 , 20 , 21 , 22 ] , \n 
[ 11 , 12 , 13 , 14 , 10 , 16 , 17 , 18 , 19 , 15 , 21 , 22 , 23 , 24 , 20 ] ] \n 
expected5 = [ [ 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 ] , \n 
[ 7 , 8 , 9 , 12 , 13 , 14 , 17 , 18 , 19 ] , \n 
[ 17 , 18 , 19 , 22 , 23 , 24 , 2 , 3 , 4 ] ] \n 
expected6 = [ [ 24 , 20 , 21 , 4 , 0 , 1 , 9 , 5 , 6 ] , \n 
[ 11 , 12 , 13 , 16 , 17 , 18 , 21 , 22 , 23 ] , \n 
[ 13 , 14 , 10 , 18 , 19 , 15 , 23 , 24 , 20 ] ] \n 
for shp_idx , ( shape , neib_shape , neib_step , expected ) in enumerate ( [ \n 
[ ( 7 , 8 , 5 , 5 ) , ( 3 , 3 ) , ( 2 , 2 ) , expected1 ] , \n 
[ ( 7 , 8 , 5 , 5 ) , ( 3 , 3 ) , ( 3 , 3 ) , expected2 ] , \n 
[ ( 7 , 8 , 5 , 5 ) , ( 5 , 3 ) , ( 3 , 3 ) , expected3 ] , \n 
[ ( 7 , 8 , 5 , 5 ) , ( 3 , 5 ) , ( 3 , 3 ) , expected4 ] , \n 
[ ( 80 , 90 , 5 , 5 ) , ( 3 , 3 ) , ( 2 , 3 ) , expected5 ] , \n 
[ ( 1025 , 9 , 5 , 5 ) , ( 3 , 3 ) , ( 3 , 2 ) , expected6 ] , \n 
[ ( 1 , 1 , 5 , 1035 ) , ( 3 , 3 ) , ( 3 , 3 ) , None ] , \n 
[ ( 1 , 1 , 1045 , 5 ) , ( 3 , 3 ) , ( 3 , 3 ) , None ] , \n 
] ) : \n 
neib_shape = T . as_tensor_variable ( neib_shape ) \n 
neib_step = T . as_tensor_variable ( neib_step ) \n 
expected = numpy . asarray ( expected ) \n 
f = function ( [ ] , images2neibs ( images , neib_shape , neib_step , \n 
mode = "wrap_centered" ) , \n 
if expected . size > 1 : \n 
~~~ for i in range ( shape [ 0 ] * shape [ 1 ] ) : \n 
~~~ assert numpy . allclose ( \n 
neibs [ i * expected . shape [ 0 ] : \n 
( i + 1 ) * expected . shape [ 0 ] , : ] , \n 
expected + 25 * i ) , "wrap_centered" \n 
~~ ~~ assert self . op in [ type ( node . op ) \n 
~~ ~~ ~~ def test_neibs_bad_shape_wrap_centered ( self ) : \n 
f = function ( [ ] , images2neibs ( images , neib_shape , \n 
~~ for shape in [ ( 2 , 3 , 2 , 3 ) , ( 2 , 3 , 3 , 2 ) ] : \n 
~~~ images = shared ( numpy . arange ( numpy . prod ( shape ) ) . reshape ( shape ) ) \n 
~~ shape = ( 2 , 3 , 3 , 3 ) \n 
images = shared ( numpy . arange ( numpy . prod ( shape ) ) . reshape ( shape ) ) \n 
images2neibs ( images , neib_shape , mode = "wrap_centered" ) , \n 
~~ ~~ def test_grad_wrap_centered ( self ) : \n 
~~~ shape = ( 2 , 3 , 6 , 6 ) \n 
images_val = numpy . random . rand ( * shape ) . astype ( ) \n 
def fn ( images ) : \n 
~~~ return images2neibs ( images , ( 3 , 3 ) , mode = ) \n 
~~ self . assertRaises ( TypeError , unittest_tools . verify_grad , \n 
fn , [ images_val ] , mode = self . mode ) \n 
~~ def test_grad_valid ( self ) : \n 
~~~ return images2neibs ( images , ( 2 , 2 ) ) \n 
~~ unittest_tools . verify_grad ( fn , [ images_val ] , mode = self . mode , \n 
eps = 0.1 ) \n 
~~~ return images2neibs ( images , ( 3 , 2 ) , ( 1 , 2 ) ) \n 
~~~ return images2neibs ( images , ( 1 , 2 ) , ( 5 , 2 ) ) \n 
~~ def test_grad_ignore_border ( self ) : \n 
~~~ return images2neibs ( images , ( 2 , 2 ) , \n 
~~ def test_neibs2images_grad ( self ) : \n 
~~~ neibs_val = numpy . random . rand ( 150 , 4 ) \n 
def fn ( neibs ) : \n 
~~~ return neibs2images ( neibs , ( 2 , 2 ) , ( 2 , 3 , 10 , 10 ) ) \n 
~~ unittest_tools . verify_grad ( fn , [ neibs_val ] , mode = self . mode , \n 
~~ def test_neibs_valid_with_inconsistent_borders ( self ) : \n 
images = T . dtensor4 ( ) \n 
images_val = numpy . arange ( numpy . prod ( shape ) , \n 
dtype = ) . reshape ( shape ) \n 
~~~ return T . sum ( T . sqr ( images2neibs ( images , ( 2 , 2 ) , mode = ) ) , \n 
axis = [ 0 , 1 ] ) \n 
~~ f = theano . function ( [ images ] , \n 
T . sqr ( images2neibs ( images , ( 2 , 2 ) , mode = ) ) , \n 
self . assertRaises ( TypeError , f , images_val ) \n 
~~ def speed_neibs ( self ) : \n 
~~~ shape = ( 100 , 40 , 18 , 18 ) \n 
images = shared ( numpy . arange ( numpy . prod ( shape ) , \n 
dtype = ) . reshape ( shape ) ) \n 
for i in range ( 1000 ) : \n 
~~~ f ( ) \n 
~~ ~~ def speed_neibs_wrap_centered ( self ) : \n 
~~ ~~ def test_infer_shape ( self ) : \n 
~~~ shape = ( 100 , 40 , 6 , 3 ) \n 
images = numpy . ones ( shape ) . astype ( ) \n 
x = T . ftensor4 ( ) \n 
f = self . _compile_and_check ( [ x ] , \n 
[ images2neibs ( \n 
x , neib_shape = ( 2 , 1 ) , \n 
mode = ) ] , \n 
[ images ] , \n 
Images2Neibs \n 
x , neib_shape = ( 2 , 3 ) , \n 
shape = ( 100 , 40 , 5 , 4 ) \n 
shape = ( 100 , 40 , 5 , 3 ) \n 
shape = ( 100 , 40 , 6 , 7 ) \n 
x , neib_shape = ( 2 , 2 ) , \n 
shape = ( 100 , 40 , 5 , 10 ) \n 
x , neib_shape = ( 3 , 3 ) , \n 
from theano . tensor import * \n 
from numpy . testing import dec \n 
class TestRealImag ( unittest . TestCase ) : \n 
~~~ def test0 ( self ) : \n 
~~~ x = zvector ( ) \n 
rng = numpy . random . RandomState ( 23 ) \n 
xval = numpy . asarray ( list ( numpy . complex ( rng . randn ( ) , rng . randn ( ) ) \n 
for i in xrange ( 10 ) ) ) \n 
assert numpy . all ( xval . real == theano . function ( [ x ] , real ( x ) ) ( xval ) ) \n 
assert numpy . all ( xval . imag == theano . function ( [ x ] , imag ( x ) ) ( xval ) ) \n 
~~ def test_on_real_input ( self ) : \n 
~~~ x = dvector ( ) \n 
xval = rng . randn ( 10 ) \n 
numpy . all ( 0 == theano . function ( [ x ] , imag ( x ) ) ( xval ) ) \n 
numpy . all ( xval == theano . function ( [ x ] , real ( x ) ) ( xval ) ) \n 
x = imatrix ( ) \n 
xval = numpy . asarray ( rng . randn ( 3 , 3 ) * 100 , dtype = ) \n 
~~ def test_cast ( self ) : \n 
self . assertRaises ( TypeError , cast , x , ) \n 
~~ def test_complex ( self ) : \n 
~~~ rng = numpy . random . RandomState ( 2333 ) \n 
m = fmatrix ( ) \n 
c = complex ( m [ 0 ] , m [ 1 ] ) \n 
assert c . type == cvector \n 
r , i = [ real ( c ) , imag ( c ) ] \n 
assert r . type == fvector \n 
assert i . type == fvector \n 
f = theano . function ( [ m ] , [ r , i ] ) \n 
mval = numpy . asarray ( rng . randn ( 2 , 5 ) , dtype = ) \n 
rval , ival = f ( mval ) \n 
assert numpy . all ( rval == mval [ 0 ] ) , ( rval , mval [ 0 ] ) \n 
assert numpy . all ( ival == mval [ 1 ] ) , ( ival , mval [ 1 ] ) \n 
def test_complex_grads ( self ) : \n 
~~~ def f ( m ) : \n 
~~~ c = complex ( m [ 0 ] , m [ 1 ] ) \n 
return .5 * real ( c ) + .9 * imag ( c ) \n 
~~ rng = numpy . random . RandomState ( 9333 ) \n 
mval = numpy . asarray ( rng . randn ( 2 , 5 ) ) \n 
utt . verify_grad ( f , [ mval ] ) \n 
def test_mul_mixed0 ( self ) : \n 
~~~ def f ( a ) : \n 
~~~ ac = complex ( a [ 0 ] , a [ 1 ] ) \n 
return abs ( ( ac ) ** 2 ) . sum ( ) \n 
aval = numpy . asarray ( rng . randn ( 2 , 5 ) ) \n 
~~~ utt . verify_grad ( f , [ aval ] ) \n 
~~ except utt . verify_grad . E_grad as e : \n 
~~~ print ( e . num_grad . gf ) \n 
print ( e . analytic_grad ) \n 
def test_mul_mixed1 ( self ) : \n 
return abs ( ac ) . sum ( ) \n 
def test_mul_mixed ( self ) : \n 
~~~ def f ( a , b ) : \n 
return abs ( ( ac * b ) ** 2 ) . sum ( ) \n 
bval = rng . randn ( 5 ) \n 
~~~ utt . verify_grad ( f , [ aval , bval ] ) \n 
def test_polar_grads ( self ) : \n 
~~~ c = complex_from_polar ( abs ( m [ 0 ] ) , m [ 1 ] ) \n 
def test_abs_grad ( self ) : \n 
return .5 * abs ( c ) \n 
from numpy . testing import assert_equal , assert_string_equal \n 
import theano . tensor as tt \n 
from theano . tensor import ( Subtensor , AdvancedSubtensor , AdvancedSubtensor1 , \n 
IncSubtensor , AdvancedIncSubtensor , \n 
AdvancedIncSubtensor1 ) \n 
def test_numpy_method ( ) : \n 
~~~ x = tt . dmatrix ( ) \n 
data = np . random . rand ( 5 , 5 ) \n 
x . tag . test_value = data \n 
for fct in [ np . arccos , np . arccosh , np . arcsin , np . arcsinh , \n 
np . arctan , np . arctanh , np . ceil , np . cos , np . cosh , np . deg2rad , \n 
np . exp , np . exp2 , np . expm1 , np . floor , np . log , \n 
np . log10 , np . log1p , np . log2 , np . rad2deg , \n 
np . sin , np . sinh , np . sqrt , np . tan , np . tanh , np . trunc ] : \n 
~~~ y = fct ( x ) \n 
f = theano . function ( [ x ] , y ) \n 
utt . assert_allclose ( np . nan_to_num ( f ( data ) ) , \n 
np . nan_to_num ( fct ( data ) ) ) \n 
~~ ~~ def test_copy ( ) : \n 
y = x . copy ( name = ) \n 
assert_equal ( f ( data ) , data ) \n 
assert_string_equal ( y . name , ) \n 
~~ def test_None_dimShuffle_replace ( ) : \n 
y = x [ : , None , : ] \n 
for elem in f . maker . fgraph . toposort ( ) : \n 
~~~ assert type ( elem . op ) not in [ Subtensor , AdvancedSubtensor , \n 
AdvancedSubtensor1 , IncSubtensor , \n 
AdvancedIncSubtensor , \n 
AdvancedIncSubtensor1 ] \n 
~~ x = tt . tensor3 ( ) \n 
y1 = x [ : , : , None , : ] \n 
y2 = x [ None , : , : , None , : ] \n 
y3 = x [ : , : , None , : , None , None ] \n 
f = theano . function ( [ x ] , [ y1 , y2 , y3 ] ) \n 
from six . moves import StringIO \n 
import theano . tensor as tensor \n 
from theano . printing import min_informative_str , debugprint \n 
def test_pydotprint_cond_highlight ( ) : \n 
if not theano . printing . pydot_imported : \n 
~~ x = tensor . dvector ( ) \n 
f = theano . function ( [ x ] , x * 2 ) \n 
f ( [ 1 , 2 , 3 , 4 ] ) \n 
s = StringIO ( ) \n 
new_handler = logging . StreamHandler ( s ) \n 
new_handler . setLevel ( logging . DEBUG ) \n 
orig_handler = theano . logging_default_handler \n 
theano . theano_logger . removeHandler ( orig_handler ) \n 
theano . theano_logger . addHandler ( new_handler ) \n 
~~~ theano . printing . pydotprint ( f , cond_highlight = True , \n 
print_output_file = False ) \n 
~~~ theano . theano_logger . addHandler ( orig_handler ) \n 
theano . theano_logger . removeHandler ( new_handler ) \n 
~~ assert ( s . getvalue ( ) == \n 
~~ def test_pydotprint_return_image ( ) : \n 
~~~ if not theano . printing . pydot_imported : \n 
ret = theano . printing . pydotprint ( x * 2 , return_image = True ) \n 
assert isinstance ( ret , ( str , bytes ) ) \n 
~~ def test_pydotprint_variables ( ) : \n 
~~~ theano . printing . pydotprint ( x * 2 ) \n 
if not theano . printing . pd . __name__ == "pydot_ng" : \n 
~~~ theano . printing . pydotprint_variables ( x * 2 ) \n 
~~ ~~ def test_pydotprint_long_name ( ) : \n 
mode = theano . compile . mode . get_default_mode ( ) . excluding ( "fusion" ) \n 
f = theano . function ( [ x ] , [ x * 2 , x + x ] , mode = mode ) \n 
theano . printing . pydotprint ( f , max_label_size = 5 , \n 
theano . printing . pydotprint ( [ x * 2 , x + x ] , \n 
max_label_size = 5 , \n 
~~ def test_pydotprint_profile ( ) : \n 
~~ A = tensor . matrix ( ) \n 
f = theano . function ( [ A ] , A + 1 , mode = ) \n 
theano . printing . pydotprint ( f , print_output_file = False ) \n 
~~ def test_min_informative_str ( ) : \n 
A = tensor . matrix ( name = ) \n 
B = tensor . matrix ( name = ) \n 
C = A + B \n 
C . name = \n 
D = tensor . matrix ( name = ) \n 
E = tensor . matrix ( name = ) \n 
F = D + E \n 
G = C + F \n 
if mis != reference : \n 
~~~ print ( + mis + ) \n 
print ( + reference + ) \n 
~~ assert mis == reference \n 
~~ def test_debugprint ( ) : \n 
~~~ A = tensor . matrix ( name = ) \n 
mode = theano . compile . get_default_mode ( ) . including ( ) \n 
g = theano . function ( [ A , B , D , E ] , G , mode = mode ) \n 
debugprint ( G ) \n 
debugprint ( G , file = s , ids = ) \n 
s = s . getvalue ( ) \n 
reference = . join ( [ \n 
] ) + \n 
if s != reference : \n 
~~~ print ( + s + ) \n 
~~ assert s == reference \n 
reference = "\\n" . join ( [ \n 
debugprint ( G , file = s , ids = , stop_on_name = True ) \n 
debugprint ( g , file = s , ids = , print_storage = True ) \n 
~~ def test_scan_debugprint1 ( ) : \n 
~~~ k = tensor . iscalar ( "k" ) \n 
A = tensor . dvector ( "A" ) \n 
result , updates = theano . scan ( fn = lambda prior_result , A : prior_result * A , \n 
outputs_info = tensor . ones_like ( A ) , \n 
non_sequences = A , \n 
n_steps = k ) \n 
output_str = theano . printing . debugprint ( final_result , file = ) \n 
lines = [ ] \n 
for line in output_str . split ( ) : \n 
~~~ lines += [ line ] \n 
for truth , out in zip ( expected_output . split ( "\\n" ) , lines ) : \n 
~~~ assert truth . strip ( ) == out . strip ( ) \n 
~~ ~~ def test_scan_debugprint2 ( ) : \n 
~~~ coefficients = theano . tensor . vector ( "coefficients" ) \n 
x = tensor . scalar ( "x" ) \n 
max_coefficients_supported = 10000 \n 
components , updates = theano . scan ( fn = lambda coefficient , power , \n 
free_variable : \n 
coefficient * ( free_variable ** power ) , \n 
outputs_info = None , \n 
sequences = [ \n 
coefficients , \n 
theano . tensor . arange ( \n 
max_coefficients_supported ) ] , \n 
non_sequences = x ) \n 
polynomial = components . sum ( ) \n 
output_str = theano . printing . debugprint ( polynomial , file = ) \n 
~~ ~~ def test_scan_debugprint3 ( ) : \n 
~~~ coefficients = theano . tensor . dvector ( "coefficients" ) \n 
max_coefficients_supported = 10 \n 
k = tensor . iscalar ( "k" ) \n 
def compute_A_k ( A , k ) : \n 
~~~ result , updates = theano . scan ( fn = lambda prior_result , \n 
A : prior_result * A , \n 
A_k = result [ - 1 ] \n 
return A_k \n 
~~ components , updates = theano . scan ( fn = lambda coefficient , \n 
power , some_A , some_k : \n 
coefficient * \n 
( compute_A_k ( some_A , some_k ) ** power ) , \n 
non_sequences = [ A , k ] ) \n 
final_result = polynomial \n 
~~ ~~ def test_scan_debugprint4 ( ) : \n 
~~~ def fn ( a_m2 , a_m1 , b_m2 , b_m1 ) : \n 
~~~ return a_m1 + a_m2 , b_m1 + b_m2 \n 
~~ a0 = theano . shared ( numpy . arange ( 2 , dtype = ) ) \n 
b0 = theano . shared ( numpy . arange ( 2 , dtype = ) ) \n 
( a , b ) , _ = theano . scan ( \n 
fn , outputs_info = [ { : a0 , : [ - 2 , - 1 ] } , \n 
{ : b0 , : [ - 2 , - 1 ] } ] , \n 
n_steps = 5 ) \n 
final_result = a + b \n 
~~ ~~ def test_scan_debugprint5 ( ) : \n 
final_result = tensor . grad ( result [ - 1 ] . sum ( ) , A ) \n 
~~ ~~ def test_printing_scan ( ) : \n 
~~ def f_pow2 ( x_tm1 ) : \n 
~~~ return 2 * x_tm1 \n 
~~ state = theano . tensor . scalar ( ) \n 
n_steps = theano . tensor . iscalar ( ) \n 
output , updates = theano . scan ( f_pow2 , \n 
[ ] , \n 
state , \n 
n_steps = n_steps , \n 
truncate_gradient = - 1 , \n 
go_backwards = False ) \n 
f = theano . function ( [ state , n_steps ] , \n 
output , \n 
updates = updates , \n 
allow_input_downcast = True ) \n 
theano . printing . pydotprint ( output , scan_graphs = True ) \n 
theano . printing . pydotprint ( f , scan_graphs = True ) \n 
from __future__ import division , print_function , absolute_import \n 
from scipy . interpolate import interp1d \n 
from . import fetch_rrlyrae_templates , fetch_rrlyrae \n 
class RRLyraeGenerated ( object ) : \n 
lcdata = None \n 
templates = None \n 
ext_correction = { : 1.810 , \n 
: 1.400 , \n 
: 1.0 , \n 
: 0.759 , \n 
: 0.561 } \n 
@ classmethod \n 
def _fetch_data ( cls ) : \n 
~~~ if cls . lcdata is None : \n 
~~~ cls . lcdata = fetch_rrlyrae ( ) \n 
cls . templates = fetch_rrlyrae_templates ( ) \n 
~~ ~~ @ classmethod \n 
def _template_func ( cls , num , band , mu = 0 , A = 1 ) : \n 
~~~ template_id = "{0:.0f}{1}" . format ( num , band ) \n 
phase , amp = cls . templates . get_template ( template_id ) \n 
phase = np . concatenate ( [ phase , [ 1 ] ] ) \n 
amp = np . concatenate ( [ amp , amp [ - 1 : ] ] ) \n 
return interp1d ( phase , mu + A * amp ) \n 
~~ def __init__ ( self , lcid , random_state = None ) : \n 
~~~ self . _fetch_data ( ) \n 
self . lcid = lcid \n 
self . meta = self . lcdata . get_metadata ( lcid ) \n 
self . obsmeta = self . lcdata . get_obsmeta ( lcid ) \n 
self . rng = np . random . RandomState ( random_state ) \n 
~~ @ property \n 
def period ( self ) : \n 
return self . meta [ ] \n 
~~ def observed ( self , band , corrected = True ) : \n 
if band not in : \n 
~~ i = . find ( band ) \n 
t , y , dy = self . lcdata . get_lightcurve ( self . lcid , return_1d = False ) \n 
if corrected : \n 
~~~ ext = self . obsmeta [ ] * self . ext_correction [ band ] \n 
~~~ ext = 0 \n 
~~ return t [ : , i ] , y [ : , i ] - ext , dy [ : , i ] \n 
~~ def generated ( self , band , t , err = None , corrected = True ) : \n 
t = np . asarray ( t ) \n 
num = self . meta [ band + ] \n 
mu = self . meta [ band + ] \n 
amp = self . meta [ band + ] \n 
t0 = self . meta [ band + ] \n 
bad_vals = np . isnan ( t ) | np . isinf ( t ) \n 
t [ bad_vals ] = t0 \n 
~~ func = self . _template_func ( num , band , mu + ext , amp ) \n 
mag = func ( ( ( t - t0 ) / self . period ) % 1 ) \n 
mag [ bad_vals ] = np . nan \n 
if err is not None : \n 
~~~ mag += self . rng . normal ( 0 , err , t . shape ) \n 
~~ return mag \n 
import urllib2 \n 
DATA_URL = ( \n 
LOCAL_FILE = \n 
password_mgr = urllib2 . HTTPPasswordMgrWithDefaultRealm ( ) \n 
password_mgr . add_password ( None , DATA_URL , , ) \n 
handler = urllib2 . HTTPBasicAuthHandler ( password_mgr ) \n 
opener = urllib2 . build_opener ( handler ) \n 
if not os . path . exists ( LOCAL_FILE ) : \n 
fhandle = opener . open ( DATA_URL ) \n 
open ( LOCAL_FILE , ) . write ( fhandle . read ( ) ) \n 
from itertools import cycle \n 
import pylab as pl \n 
from sklearn . datasets import load_iris \n 
from sklearn . decomposition import PCA \n 
def plot_2D ( data , target , target_names ) : \n 
~~~ colors = cycle ( ) \n 
target_ids = range ( len ( target_names ) ) \n 
pl . figure ( ) \n 
for i , c , label in zip ( target_ids , colors , target_names ) : \n 
~~~ pl . plot ( data [ target == i , 0 ] , \n 
data [ target == i , 1 ] , , \n 
c = c , label = label ) \n 
~~ pl . legend ( target_names ) \n 
#---------------------------------------------------------------------- \n 
~~ iris = load_iris ( ) \n 
X , y = iris . data , iris . target \n 
pca = PCA ( n_components = 2 , whiten = True ) . fit ( X ) \n 
X_pca = pca . transform ( X ) \n 
plot_2D ( X_pca , iris . target , iris . target_names ) \n 
from sklearn . cluster import KMeans \n 
from numpy . random import RandomState \n 
rng = RandomState ( 42 ) \n 
kmeans = KMeans ( 3 , random_state = rng ) . fit ( X_pca ) \n 
plot_2D ( X_pca , kmeans . labels_ , [ "c0" , "c1" , "c2" ] ) \n 
pl . show ( ) \n 
import setup_logger \n 
from robo . models . gpy_model import GPyModel \n 
from robo . acquisition . ei import EI \n 
from robo . maximizers . cmaes import CMAES \n 
from robo . task . synthetic_functions . branin import Branin \n 
from robo . solver . bayesian_optimization import BayesianOptimization \n 
branin = Branin ( ) \n 
kernel = GPy . kern . Matern52 ( input_dim = branin . n_dims ) \n 
model = GPyModel ( kernel ) \n 
acquisition_func = EI ( model , \n 
X_upper = branin . X_upper , \n 
X_lower = branin . X_lower , \n 
par = 0.1 ) \n 
maximizer = CMAES ( acquisition_func , branin . X_lower , branin . X_upper ) \n 
bo = BayesianOptimization ( acquisition_func = acquisition_func , \n 
model = model , \n 
maximize_func = maximizer , \n 
task = branin ) \n 
bo . run ( 10 ) \n 
import DIRECT \n 
from robo . maximizers . base_maximizer import BaseMaximizer \n 
class Direct ( BaseMaximizer ) : \n 
~~~ def __init__ ( self , objective_function , X_lower , X_upper , \n 
n_func_evals = 400 , n_iters = 200 ) : \n 
self . n_func_evals = n_func_evals \n 
self . n_iters = n_iters \n 
super ( Direct , self ) . __init__ ( objective_function , X_lower , X_upper ) \n 
~~ def _direct_acquisition_fkt_wrapper ( self , acq_f ) : \n 
~~~ def _l ( x , user_data ) : \n 
~~~ return - acq_f ( np . array ( [ x ] ) ) , 0 \n 
~~ return _l \n 
~~ def maximize ( self ) : \n 
x , _ , _ = DIRECT . solve ( \n 
self . _direct_acquisition_fkt_wrapper ( self . objective_func ) , \n 
l = [ self . X_lower ] , \n 
u = [ self . X_upper ] , \n 
maxT = self . n_iters , \n 
maxf = self . n_func_evals ) \n 
return np . array ( [ x ] ) \n 
~~ ~~ import matplotlib . pyplot as plt \n 
import theanets \n 
from utils import load_mnist , plot_layers \n 
train , valid , _ = load_mnist ( labels = True ) \n 
N = 10 \n 
net = theanets . Classifier ( [ 784 , N * N , ( , 10 ) ] ) \n 
net . train ( train , valid , min_improvement = 0.001 , train_batches = 100 ) \n 
plot_layers ( [ net . find ( , ) , net . find ( , ) ] ) \n 
plt . tight_layout ( ) \n 
from . import feedforward \n 
class Regressor ( feedforward . Regressor ) : \n 
INPUT_NDIM = 4 \n 
~~ class Classifier ( feedforward . Classifier ) : \n 
~~ COLOUR_FIGURE = False \n 
data = load_iris ( ) \n 
features = data . data \n 
feature_names = data . feature_names \n 
target = data . target \n 
target_names = data . target_names \n 
labels = target_names [ target ] \n 
is_setosa = ( labels == ) \n 
features = features [ ~ is_setosa ] \n 
labels = labels [ ~ is_setosa ] \n 
is_virginica = ( labels == ) \n 
t = 1.65 \n 
t2 = 1.75 \n 
f0 , f1 = 3 , 2 \n 
if COLOUR_FIGURE : \n 
~~~ area1c = ( 1. , .8 , .8 ) \n 
area2c = ( .8 , .8 , 1. ) \n 
~~~ area1c = ( 1. , 1 , 1 ) \n 
area2c = ( .7 , .7 , .7 ) \n 
~~ x0 = features [ : , f0 ] . min ( ) * .9 \n 
x1 = features [ : , f0 ] . max ( ) * 1.1 \n 
y0 = features [ : , f1 ] . min ( ) * .9 \n 
y1 = features [ : , f1 ] . max ( ) * 1.1 \n 
fig , ax = plt . subplots ( ) \n 
ax . fill_between ( [ t , x1 ] , [ y0 , y0 ] , [ y1 , y1 ] , color = area2c ) \n 
ax . fill_between ( [ x0 , t ] , [ y0 , y0 ] , [ y1 , y1 ] , color = area1c ) \n 
ax . plot ( [ t , t ] , [ y0 , y1 ] , , lw = 2 ) \n 
ax . plot ( [ t2 , t2 ] , [ y0 , y1 ] , , lw = 2 ) \n 
ax . scatter ( features [ is_virginica , f0 ] , \n 
features [ is_virginica , f1 ] , c = , marker = , s = 40 ) \n 
ax . scatter ( features [ ~ is_virginica , f0 ] , \n 
features [ ~ is_virginica , f1 ] , c = , marker = , s = 40 ) \n 
ax . set_ylim ( y0 , y1 ) \n 
ax . set_xlim ( x0 , x1 ) \n 
ax . set_xlabel ( feature_names [ f0 ] ) \n 
ax . set_ylabel ( feature_names [ f1 ] ) \n 
fig . tight_layout ( ) \n 
fig . savefig ( ) \n 
start_time = time . time ( ) \n 
from sklearn . metrics import classification_report \n 
from sklearn . metrics import precision_recall_curve , roc_curve , auc \n 
from sklearn . cross_validation import KFold \n 
from sklearn import neighbors \n 
from data import chosen , chosen_meta \n 
from utils import plot_pr \n 
from utils import plot_feat_importance \n 
from utils import load_meta \n 
from utils import fetch_posts \n 
from utils import plot_feat_hist \n 
from utils import plot_bias_variance \n 
from utils import plot_k_complexity \n 
meta , id_to_idx , idx_to_id = load_meta ( chosen_meta ) \n 
import nltk \n 
all_questions = sorted ( [ q for q , v in meta . items ( ) if v [ ] == - 1 ] ) \n 
all_answers = sorted ( [ q for q , v in meta . items ( ) if v [ ] != - 1 ] ) \n 
feature_names = np . array ( ( \n 
def prepare_sent_features ( ) : \n 
~~~ for pid , text in fetch_posts ( chosen , with_index = True ) : \n 
~~~ if not text : \n 
~~~ meta [ pid ] [ ] = meta [ pid ] [ ] = 0 \n 
~~~ from platform import python_version \n 
if python_version ( ) . startswith ( ) : \n 
~~~ text = text . decode ( ) \n 
~~ sent_lens = [ len ( nltk . word_tokenize ( \n 
sent ) ) for sent in nltk . sent_tokenize ( text ) ] \n 
meta [ pid ] [ ] = np . mean ( sent_lens ) \n 
meta [ pid ] [ ] = np . mean ( \n 
[ len ( w ) for w in nltk . word_tokenize ( text ) ] ) \n 
~~ meta [ pid ] [ ] = np . sum ( \n 
[ word . isupper ( ) for word in nltk . word_tokenize ( text ) ] ) \n 
meta [ pid ] [ ] = text . count ( ) \n 
~~ ~~ prepare_sent_features ( ) \n 
def get_features ( aid ) : \n 
~~~ return tuple ( meta [ aid ] [ fn ] for fn in feature_names ) \n 
~~ qa_X = np . asarray ( [ get_features ( aid ) for aid in all_answers ] ) \n 
classifying_answer = "good" \n 
if classifying_answer == "good" : \n 
~~~ qa_Y = np . asarray ( [ meta [ aid ] [ ] > 0 for aid in all_answers ] ) \n 
~~ elif classifying_answer == "poor" : \n 
~~~ qa_Y = np . asarray ( [ meta [ aid ] [ ] <= 0 for aid in all_answers ] ) \n 
classifying_answer ) \n 
~~ for idx , feat in enumerate ( feature_names ) : \n 
~~~ plot_feat_hist ( [ ( qa_X [ : , idx ] , feat ) ] ) \n 
~~ avg_scores_summary = [ ] \n 
def measure ( clf_class , parameters , name , data_size = None , plot = False ) : \n 
~~~ start_time_clf = time . time ( ) \n 
if data_size is None : \n 
~~~ X = qa_X \n 
Y = qa_Y \n 
~~~ X = qa_X [ : data_size ] \n 
Y = qa_Y [ : data_size ] \n 
~~ cv = KFold ( n = len ( X ) , n_folds = 10 , indices = True ) \n 
train_errors = [ ] \n 
test_errors = [ ] \n 
scores = [ ] \n 
roc_scores = [ ] \n 
fprs , tprs = [ ] , [ ] \n 
pr_scores = [ ] \n 
precisions , recalls , thresholds = [ ] , [ ] , [ ] \n 
for fold_idx , ( train , test ) in enumerate ( cv ) : \n 
~~~ X_train , y_train = X [ train ] , Y [ train ] \n 
X_test , y_test = X [ test ] , Y [ test ] \n 
only_one_class_in_train = len ( set ( y_train ) ) == 1 \n 
only_one_class_in_test = len ( set ( y_test ) ) == 1 \n 
if only_one_class_in_train or only_one_class_in_test : \n 
~~ clf = clf_class ( ** parameters ) \n 
clf . fit ( X_train , y_train ) \n 
train_score = clf . score ( X_train , y_train ) \n 
test_score = clf . score ( X_test , y_test ) \n 
train_errors . append ( 1 - train_score ) \n 
test_errors . append ( 1 - test_score ) \n 
scores . append ( test_score ) \n 
proba = clf . predict_proba ( X_test ) \n 
label_idx = 1 \n 
fpr , tpr , roc_thresholds = roc_curve ( y_test , proba [ : , label_idx ] ) \n 
precision , recall , pr_thresholds = precision_recall_curve ( \n 
y_test , proba [ : , label_idx ] ) \n 
roc_scores . append ( auc ( fpr , tpr ) ) \n 
fprs . append ( fpr ) \n 
tprs . append ( tpr ) \n 
pr_scores . append ( auc ( recall , precision ) ) \n 
precisions . append ( precision ) \n 
recalls . append ( recall ) \n 
thresholds . append ( pr_thresholds ) \n 
threshold_for_detecting_good_answers = 0.59 \n 
print ( classification_report ( y_test , proba [ : , label_idx ] > \n 
threshold_for_detecting_good_answers , target_names = [ , ] ) ) \n 
medium = np . argsort ( scores_to_sort ) [ len ( scores_to_sort ) / 2 ] \n 
if plot : \n 
~~~ plot_pr ( pr_scores [ medium ] , name , precisions [ medium ] , \n 
if hasattr ( clf , ) : \n 
~~~ plot_feat_importance ( feature_names , clf , name ) \n 
~~ ~~ summary = ( name , \n 
np . mean ( scores ) , np . std ( scores ) , \n 
np . mean ( roc_scores ) , np . std ( roc_scores ) , \n 
np . mean ( pr_scores ) , np . std ( pr_scores ) , \n 
time . time ( ) - start_time_clf ) \n 
print ( summary ) \n 
avg_scores_summary . append ( summary ) \n 
precisions = precisions [ medium ] \n 
recalls = recalls [ medium ] \n 
thresholds = np . hstack ( ( [ 0 ] , thresholds [ medium ] ) ) \n 
idx80 = precisions >= 0.8 \n 
idx80 ] [ 0 ] , thresholds [ idx80 ] [ 0 ] ) ) \n 
return np . mean ( train_errors ) , np . mean ( test_errors ) \n 
~~ def bias_variance_analysis ( clf_class , parameters , name ) : \n 
~~~ data_sizes = np . arange ( 60 , 2000 , 4 ) \n 
for data_size in data_sizes : \n 
~~~ train_error , test_error = measure ( \n 
clf_class , parameters , name , data_size = data_size ) \n 
train_errors . append ( train_error ) \n 
test_errors . append ( test_error ) \n 
~~ plot_bias_variance ( data_sizes , train_errors , \n 
~~ def k_complexity_analysis ( clf_class , parameters ) : \n 
~~~ ks = np . hstack ( ( np . arange ( 1 , 20 ) , np . arange ( 21 , 100 , 5 ) ) ) \n 
for k in ks : \n 
~~~ parameters [ ] = k \n 
train_error , test_error = measure ( \n 
clf_class , parameters , "%dNN" % k , data_size = 2000 ) \n 
~~ plot_k_complexity ( ks , train_errors , test_errors ) \n 
~~ for k in [ 5 ] : \n 
~~~ bias_variance_analysis ( neighbors . KNeighborsClassifier , { \n 
: k } , "%iNN" % k ) \n 
k_complexity_analysis ( neighbors . KNeighborsClassifier , { : k } ) \n 
~~ from sklearn . linear_model import LogisticRegression \n 
for C in [ 0.1 ] : \n 
bias_variance_analysis ( LogisticRegression , { : , : C } , name ) \n 
measure ( LogisticRegression , { : , : C } , name , plot = True ) \n 
~~ print ( "=" * 50 ) \n 
from operator import itemgetter \n 
for s in reversed ( sorted ( avg_scores_summary , key = itemgetter ( 1 ) ) ) : \n 
~~~ print ( "%-20s\\t%.5f\\t%.5f\\t%.5f\\t%.5f\\t%.5f\\t%.5f\\t%.5f" % s ) \n 
from load_ml100k import load \n 
data = load ( ) \n 
plt . gray ( ) \n 
plt . imshow ( data [ : 200 , : 200 ] , interpolation = ) \n 
from jug import TaskGenerator \n 
from time import sleep \n 
@ TaskGenerator \n 
def double ( x ) : \n 
~~~ sleep ( 4 ) \n 
return 2 * x \n 
~~ @ TaskGenerator \n 
def add ( a , b ) : \n 
~~~ return a + b \n 
def print_final_result ( oname , value ) : \n 
~~~ with open ( oname , ) as output : \n 
~~ ~~ input = 2 \n 
y = double ( input ) \n 
z = double ( y ) \n 
y2 = double ( 7 ) \n 
z2 = double ( y2 ) \n 
print_final_result ( , add ( z , z2 ) ) \n 
from SimpleCV . base import * \n 
class HaarCascade ( ) : \n 
_mCascade = None \n 
_mName = None \n 
_cache = { } \n 
_fhandle = None \n 
def __init__ ( self , fname = None , name = None ) : \n 
~~~ if ( name is None ) : \n 
~~~ self . _mName = fname \n 
~~~ self . _mName = name \n 
~~ if fname is not None : \n 
~~~ if os . path . exists ( fname ) : \n 
~~~ self . _fhandle = os . path . abspath ( fname ) \n 
~~~ self . _fhandle = os . path . join ( LAUNCH_PATH , , , fname ) \n 
if ( not os . path . exists ( self . _fhandle ) ) : \n 
~~ ~~ self . _mCascade = cv . Load ( self . _fhandle ) \n 
if HaarCascade . _cache . has_key ( self . _fhandle ) : \n 
~~~ self . _mCascade = HaarCascade . _cache [ self . _fhandle ] \n 
~~ HaarCascade . _cache [ self . _fhandle ] = self . _mCascade \n 
~~ ~~ def load ( self , fname = None , name = None ) : \n 
~~~ self . _mCascade = HaarCascade . _cache [ fname ] \n 
~~ ~~ def getCascade ( self ) : \n 
~~~ return self . _mCascade \n 
~~ def getName ( self ) : \n 
~~~ return self . _mName \n 
~~ def setName ( self , name ) : \n 
~~ def getFHandle ( self ) : \n 
~~~ return self . _fhandle \n 
~~ ~~ from SimpleCV . base import * \n 
from SimpleCV . ImageClass import Image , ImageSet \n 
from SimpleCV . DrawingLayer import * \n 
from SimpleCV . Features import FeatureExtractorBase \n 
class TreeClassifier : \n 
mClassNames = [ ] \n 
mDataSetRaw = [ ] \n 
mDataSetOrange = [ ] \n 
mClassifier = None \n 
mLearner = None \n 
mTree = None \n 
mFeatureExtractors = None \n 
mOrangeDomain = None \n 
mFlavorParams = None \n 
mTreeTypeDict = { \n 
mforestFlavorDict = { \n 
mBoostedFlavorDict = { \n 
mBaggedFlavorDict = { \n 
def __init__ ( self , featureExtractors = [ ] , flavor = , flavorDict = None ) : \n 
if not ORANGE_ENABLED : \n 
~~ self . mClassNames = [ ] \n 
self . mDataSetRaw = [ ] \n 
self . mDataSetOrange = [ ] \n 
self . mClassifier = None \n 
self . mLearner = None \n 
self . mTree = None \n 
self . mFeatureExtractors = None \n 
self . mOrangeDomain = None \n 
self . mFlavorParams = None \n 
self . mFlavor = self . mTreeTypeDict [ flavor ] \n 
if ( flavorDict is None ) : \n 
~~~ if ( self . mFlavor == self . mTreeTypeDict [ "Bagged" ] ) : \n 
~~~ self . mFlavorParams = self . mBaggedFlavorDict \n 
~~ elif ( self . mFlavor == self . mTreeTypeDict [ "Forest" ] ) : \n 
~~~ self . mFlavorParams = self . mBoostedFlavorDict \n 
~~~ self . mFlavorParams = flavorDict \n 
~~ self . mFeatureExtractors = featureExtractors \n 
~~ def load ( cls , fname ) : \n 
return pickle . load ( file ( fname ) ) \n 
~~ load = classmethod ( load ) \n 
def save ( self , fname ) : \n 
output = open ( fname , ) \n 
output . close ( ) \n 
~~~ mydict = self . __dict__ . copy ( ) \n 
self . mDataSetOrange = None \n 
del mydict [ ] \n 
return mydict \n 
~~ def __setstate__ ( self , mydict ) : \n 
~~~ self . __dict__ = mydict \n 
colNames = [ ] \n 
for extractor in self . mFeatureExtractors : \n 
~~~ colNames . extend ( extractor . getFieldNames ( ) ) \n 
~~ self . mOrangeDomain = orange . Domain ( map ( orange . FloatVariable , colNames ) , orange . EnumVariable ( "type" self . mDataSetOrange = orange . ExampleTable ( self . mOrangeDomain , self . mDataSetRaw ) \n 
if ( self . mFlavor == 0 ) : \n 
~~~ self . mLearner = orange . TreeLearner ( ) \n 
self . mClassifier = self . mLearner ( self . mDataSetOrange ) \n 
~~ elif ( self . mFlavor == 1 ) : #bagged \n 
~~~ self . mTree = orange . TreeLearner ( ) \n 
self . mLearner = orngEnsemble . BaggedLearner ( self . mTree , t = self . mFlavorParams [ "NClassifiers" self . mClassifier = self . mLearner ( self . mDataSetOrange ) \n 
~~ elif ( self . mFlavor == 2 ) : #forest \n 
self . mLearner = orngEnsemble . RandomForestLearner ( trees = self . mFlavorParams [ "NTrees" ] , attributes self . mClassifier = self . mLearner ( self . mDataSetOrange ) \n 
~~ elif ( self . mFlavor == 3 ) : #boosted \n 
self . mLearner = orngEnsemble . BoostedLearner ( self . mTree , t = self . mFlavorParams [ "NClassifiers" self . mClassifier = self . mLearner ( self . mDataSetOrange ) \n 
~~ ~~ def classify ( self , image ) : \n 
featureVector = [ ] \n 
~~~ feats = extractor . extract ( image ) \n 
if ( feats is not None ) : \n 
~~~ featureVector . extend ( feats ) \n 
~~ ~~ featureVector . extend ( [ self . mClassNames [ 0 ] ] ) \n 
test = orange . ExampleTable ( self . mOrangeDomain , [ featureVector ] ) \n 
c = self . mClassifier ( test [ 0 ] ) #classify \n 
~~ def setFeatureExtractors ( self , extractors ) : \n 
self . mFeatureExtractors = extractors \n 
return None \n 
~~ def _trainPath ( self , path , className , subset , disp , verbose ) : \n 
files = [ ] \n 
for ext in IMAGE_FORMATS : \n 
~~~ files . extend ( glob . glob ( os . path . join ( path , ext ) ) ) \n 
~~ if ( subset > 0 ) : \n 
~~~ nfiles = min ( subset , len ( files ) ) \n 
~~~ nfiles = len ( files ) \n 
~~ badFeat = False \n 
for i in range ( nfiles ) : \n 
~~~ infile = files [ i ] \n 
~~ img = Image ( infile ) \n 
~~~ feats = extractor . extract ( img ) \n 
~~~ badFeat = True \n 
~~ ~~ if ( badFeat ) : \n 
~~~ badFeat = False \n 
~~ featureVector . extend ( [ className ] ) \n 
self . mDataSetRaw . append ( featureVector ) \n 
text = + className \n 
self . _WriteText ( disp , img , text , Color . WHITE ) \n 
count = count + 1 \n 
del img \n 
~~ return count \n 
~~ def _trainImageSet ( self , imageset , className , subset , disp , verbose ) : \n 
badFeat = False \n 
if ( subset > 0 ) : \n 
~~~ imageset = imageset [ 0 : subset ] \n 
~~ for img in imageset : \n 
~~ featureVector = [ ] \n 
~~ def train ( self , images , classNames , disp = None , subset = - 1 , savedata = None , verbose = True ) : \n 
self . mClassNames = classNames \n 
for i in range ( len ( classNames ) ) : \n 
~~~ if ( isinstance ( images [ i ] , str ) ) : \n 
~~~ count = count + self . _trainPath ( images [ i ] , classNames [ i ] , subset , disp , verbose ) \n 
~~~ count = count + self . _trainImageSet ( images [ i ] , classNames [ i ] , subset , disp , verbose ) \n 
~~ ~~ colNames = [ ] \n 
~~ if ( count <= 0 ) : \n 
if ( savedata is not None ) : \n 
~~~ orange . saveTabDelimited ( savedata , self . mDataSetOrange ) \n 
~~ if ( self . mFlavor == 0 ) : \n 
~~ correct = 0 \n 
incorrect = 0 \n 
for i in range ( count ) : \n 
~~~ c = self . mClassifier ( self . mDataSetOrange [ i ] ) \n 
test = self . mDataSetOrange [ i ] . getclass ( ) \n 
~~ if ( test == c ) : \n 
~~~ correct = correct + 1 \n 
~~~ incorrect = incorrect + 1 \n 
~~ ~~ good = 100 * ( float ( correct ) / float ( count ) ) \n 
bad = 100 * ( float ( incorrect ) / float ( count ) ) \n 
confusion = 0 \n 
if ( len ( self . mClassNames ) > 2 ) : \n 
~~~ crossValidator = orngTest . learnAndTestOnLearnData ( [ self . mLearner ] , self . mDataSetOrange ) \n 
confusion = orngStat . confusionMatrices ( crossValidator ) [ 0 ] \n 
if ( confusion != 0 ) : \n 
~~~ classes = self . mDataSetOrange . domain . classVar . values \n 
print "\\t" + "\\t" . join ( classes ) \n 
for className , classConfusions in zip ( classes , confusion ) : \n 
~~~ print ( "%s" + ( "\\t%i" * len ( classes ) ) ) % ( ( className , ) + tuple ( classConfusions \n 
~~ ~~ ~~ if ( self . mFlavor == 0 ) : \n 
~~~ self . _PrintTree ( self . mClassifier ) \n 
~~ return [ good , bad , confusion ] \n 
~~ def test ( self , images , classNames , disp = None , subset = - 1 , savedata = None , verbose = True ) : \n 
correct = 0 \n 
if ( self . mOrangeDomain is None ) : \n 
~~~ self . mOrangeDomain = orange . Domain ( map ( orange . FloatVariable , colNames ) , orange . EnumVariable \n 
~~ ~~ dataset = [ ] \n 
~~~ [ dataset , cnt , crct ] = self . _testPath ( images [ i ] , classNames [ i ] , dataset , subset , disp , verbose count = count + cnt \n 
correct = correct + crct \n 
~~~ [ dataset , cnt , crct ] = self . _testImageSet ( images [ i ] , classNames [ i ] , dataset , subset , disp , verbose count = count + cnt \n 
~~ ~~ testData = orange . ExampleTable ( self . mOrangeDomain , dataset ) \n 
if savedata is not None : \n 
~~~ orange . saveTabDelimited ( savedata , testData ) \n 
~~ confusion = 0 \n 
~~~ crossValidator = orngTest . learnAndTestOnTestData ( [ self . mLearner ] , self . mDataSetOrange , testData confusion = orngStat . confusionMatrices ( crossValidator ) [ 0 ] \n 
~~ good = 100 * ( float ( correct ) / float ( count ) ) \n 
bad = 100 * ( float ( count - correct ) / float ( count ) ) \n 
~~~ print ( "%s" + ( "\\t%i" * len ( classes ) ) ) % ( ( className , ) + tuple ( classConfusions ~~ ~~ ~~ return [ good , bad , confusion ] \n 
~~ def _testPath ( self , path , className , dataset , subset , disp , verbose ) : \n 
~~ for i in range ( nfiles ) : \n 
~~~ del img \n 
dataset . append ( featureVector ) \n 
c = self . mClassifier ( test [ 0 ] ) \n 
testClass = test [ 0 ] . getclass ( ) \n 
if ( testClass == c ) : \n 
self . _WriteText ( disp , img , text , Color . GREEN ) \n 
correct = correct + 1 \n 
self . _WriteText ( disp , img , text , Color . RED ) \n 
~~ count = count + 1 \n 
~~ return ( [ dataset , count , correct ] ) \n 
~~ def _testImageSet ( self , imageset , className , dataset , subset , disp , verbose ) : \n 
~~ def _WriteText ( self , disp , img , txt , color ) : \n 
~~~ if ( disp is not None ) : \n 
~~~ txt = + txt + \n 
img = img . adaptiveScale ( disp . resolution ) \n 
layer = DrawingLayer ( ( img . width , img . height ) ) \n 
layer . setFontSize ( 60 ) \n 
layer . ezViewText ( txt , ( 20 , 20 ) , fgcolor = color ) \n 
img . addDrawingLayer ( layer ) \n 
img . applyLayers ( ) \n 
img . save ( disp ) \n 
~~ ~~ def _PrintTree ( self , x ) : \n 
~~~ if type ( x ) == orange . TreeClassifier : \n 
~~~ self . _PrintTree0 ( x . tree , 0 ) \n 
~~ elif type ( x ) == orange . TreeNode : \n 
~~~ self . _PrintTree0 ( x , 0 ) \n 
~~ ~~ def _PrintTree0 ( self , node , level ) : \n 
~~~ if not node : \n 
~~ if node . branchSelector : \n 
~~~ nodeDesc = node . branchSelector . classVar . name \n 
nodeCont = node . distribution \n 
for i in range ( len ( node . branches ) ) : \n 
self . _PrintTree0 ( node . branches [ i ] , level + 1 ) \n 
~~~ nodeCont = node . distribution \n 
majorClass = node . nodeClassifier . defaultValue \n 
~~ ~~ ~~ import os \n 
import socket \n 
import types \n 
import SocketServer \n 
import zipfile \n 
import colorsys \n 
import pygame as pg \n 
import scipy . ndimage as ndimage \n 
import scipy . cluster . vq as scv \n 
import scipy . spatial . distance as spsd \n 
import platform \n 
from numpy import linspace \n 
from scipy . interpolate import UnivariateSpline \n 
from warnings import warn \n 
from copy import copy \n 
from math import * \n 
from pkg_resources import load_entry_point \n 
from SimpleHTTPServer import SimpleHTTPRequestHandler \n 
from types import IntType , LongType , FloatType , InstanceType \n 
from numpy import int32 \n 
from numpy import uint8 \n 
from EXIF import * \n 
from pygame import gfxdraw \n 
from pickle import * \n 
~~~ import cv2 . cv as cv \n 
~~~ import cv \n 
~~ ~~ PIL_ENABLED = True \n 
~~~ from PIL import Image as pil \n 
from PIL import ImageFont as pilImageFont \n 
from PIL import ImageDraw as pilImageDraw \n 
from PIL import GifImagePlugin \n 
getheader = GifImagePlugin . getheader \n 
getdata = GifImagePlugin . getdata \n 
~~~ import Image as pil \n 
from GifImagePlugin import getheader , getdata \n 
~~~ PIL_ENABLED = False \n 
~~ ~~ FREENECT_ENABLED = True \n 
~~~ import freenect \n 
~~~ FREENECT_ENABLED = False \n 
~~ ZXING_ENABLED = True \n 
~~~ import zxing \n 
~~~ ZXING_ENABLED = False \n 
~~ OCR_ENABLED = True \n 
~~~ import tesseract \n 
~~~ OCR_ENABLED = False \n 
~~ PYSCREENSHOT_ENABLED = True \n 
~~~ import pyscreenshot \n 
~~~ PYSCREENSHOT_ENABLED = False \n 
~~ ORANGE_ENABLED = True \n 
~~~ import orange \n 
~~~ import Orange ; import orange \n 
import orngStat \n 
~~~ ORANGE_ENABLED = False \n 
~~ VIMBA_ENABLED = True \n 
~~~ import pymba \n 
~~~ VIMBA_ENABLED = False \n 
~~ except Exception : \n 
~~ class InitOptionsHandler ( object ) : \n 
~~~ self . on_notebook = False \n 
self . headless = False \n 
~~ def enable_notebook ( self ) : \n 
~~~ self . on_notebook = True \n 
~~ def set_headless ( self ) : \n 
~~~ os . environ [ "SDL_VIDEODRIVER" ] = "dummy" \n 
self . headless = True \n 
~~ ~~ init_options_handler = InitOptionsHandler ( ) \n 
~~~ import pygame as pg \n 
~~~ init_options_handler . set_headless ( ) \n 
~~ def is_number ( n ) : \n 
return type ( n ) in ( IntType , LongType , FloatType ) \n 
~~ def is_tuple ( n ) : \n 
return type ( n ) == tuple \n 
~~ def reverse_tuple ( n ) : \n 
return tuple ( reversed ( n ) ) \n 
~~ def find ( f , seq ) : \n 
for item in seq : \n 
~~~ if ( f == item ) : \n 
~~ def test ( ) : \n 
~~ def download_and_extract ( URL ) : \n 
if URL == None : \n 
~~ tmpdir = tempfile . mkdtemp ( ) \n 
filename = os . path . basename ( URL ) \n 
path = tmpdir + "/" + filename \n 
zdata = urllib2 . urlopen ( URL ) \n 
with open ( path , "wb" ) as local_file : \n 
~~~ local_file . write ( zdata . read ( ) ) \n 
~~ zfile = zipfile . ZipFile ( path ) \n 
~~~ zfile . extractall ( tmpdir ) \n 
~~ return tmpdir \n 
~~ def int_to_bin ( i ) : \n 
i1 = i % 256 \n 
i2 = int ( i / 256 ) \n 
return chr ( i1 ) + chr ( i2 ) \n 
~~ def npArray2cvMat ( inputMat , dataType = cv . CV_32FC1 ) : \n 
if ( type ( inputMat ) == np . ndarray ) : \n 
~~~ sz = len ( inputMat . shape ) \n 
temp_mat = None \n 
if ( dataType == cv . CV_32FC1 or dataType == cv . CV_32FC2 or dataType == cv . CV_32FC3 or dataType ~~~ temp_mat = np . array ( inputMat , dtype = ) \n 
~~ elif ( dataType == cv . CV_8UC1 or dataType == cv . CV_8UC2 or dataType == cv . CV_8UC3 or dataType ~~~ temp_mat = np . array ( inputMat , dtype = ) \n 
~~~ retVal = cv . CreateMat ( inputMat . shape [ 0 ] , 1 , dataType ) \n 
cv . SetData ( retVal , temp_mat . tostring ( ) , temp_mat . dtype . itemsize * temp_mat . shape [ 0 ] ) \n 
~~ elif ( sz == 2 ) : \n 
~~~ retVal = cv . CreateMat ( temp_mat . shape [ 0 ] , temp_mat . shape [ 1 ] , dataType ) \n 
cv . SetData ( retVal , temp_mat . tostring ( ) , temp_mat . dtype . itemsize * temp_mat . shape [ 1 ] ) \n 
~~ elif ( sz > 2 ) : \n 
~~ return retVal \n 
~~ ~~ consoleHandler = logging . StreamHandler ( ) \n 
formatter = logging . Formatter ( ) \n 
consoleHandler . setFormatter ( formatter ) \n 
logger = logging . getLogger ( ) \n 
logger . addHandler ( consoleHandler ) \n 
~~~ import IPython \n 
ipython_version = IPython . __version__ \n 
~~~ ipython_version = None \n 
~~ def exception_handler ( excType , excValue , traceback ) : \n 
~~~ logger . error ( "" , exc_info = ( excType , excValue , traceback ) ) \n 
~~ sys . excepthook = exception_handler \n 
def ipython_exception_handler ( shell , excType , excValue , traceback , tb_offset = 0 ) : \n 
~~ def init_logging ( log_level ) : \n 
~~~ logger . setLevel ( log_level ) \n 
~~ def read_logging_level ( log_level ) : \n 
~~~ levels_dict = { \n 
1 : logging . DEBUG , "debug" : logging . DEBUG , \n 
2 : logging . INFO , "info" : logging . INFO , \n 
3 : logging . WARNING , "warning" : logging . WARNING , \n 
4 : logging . ERROR , "error" : logging . ERROR , \n 
5 : logging . CRITICAL , "critical" : logging . CRITICAL \n 
if isinstance ( log_level , str ) : \n 
~~~ log_level = log_level . lower ( ) \n 
~~ if log_level in levels_dict : \n 
~~~ return levels_dict [ log_level ] \n 
~~ ~~ def get_logging_level ( ) : \n 
levels_dict = { \n 
10 : "DEBUG" , \n 
20 : "INFO" , \n 
30 : "WARNING" , \n 
40 : "ERROR" , \n 
50 : "CRITICAL" \n 
~~ def set_logging ( log_level , myfilename = None ) : \n 
if myfilename and ipython_version : \n 
~~~ if ipython_version . startswith ( "0.10" ) : \n 
~~~ __IPYTHON__ . set_custom_exc ( ( Exception , ) , ipython_exception_handler ) \n 
~~~ ip = get_ipython ( ) \n 
ip . set_custom_exc ( ( Exception , ) , ipython_exception_handler ) \n 
~~~ sys . exc_clear ( ) \n 
~~ ~~ level = read_logging_level ( log_level ) \n 
if level and myfilename : \n 
~~~ fileHandler = logging . FileHandler ( filename = myfilename ) \n 
fileHandler . setLevel ( level ) \n 
fileHandler . setFormatter ( formatter ) \n 
logger . addHandler ( fileHandler ) \n 
~~ elif level : \n 
~~ logger . setLevel ( level ) \n 
~~ def system ( ) : \n 
~~~ import platform \n 
~~~ from cv2 import __version__ \n 
~~ if ( PIL_ENABLED ) : \n 
~~ if ( ORANGE_ENABLED ) : \n 
~~ ~~ except ImportError : \n 
~~ class LazyProperty ( object ) : \n 
~~~ def __init__ ( self , func ) : \n 
~~~ self . _func = func \n 
self . __name__ = func . __name__ \n 
self . __doc__ = func . __doc__ \n 
~~ def __get__ ( self , obj , klass = None ) : \n 
~~~ if obj is None : return None \n 
result = obj . __dict__ [ self . __name__ ] = self . _func ( obj ) \n 
return result \n 
~~ ~~ IMAGE_FORMATS = ( , , , , \n 
, , , , , \n 
import SimpleCV \n 
import Tkinter \n 
Tkinter . Tk ( ) \n 
photo = ImageTk . PhotoImage ( image . getPIL ( ) ) \n 
label = Tkinter . Label ( image = photo ) \n 
time . sleep ( 5 ) \n 
import freenect \n 
import matplotlib . pyplot as mp \n 
import signal \n 
import frame_convert \n 
mp . ion ( ) \n 
image_rgb = None \n 
image_depth = None \n 
keep_running = True \n 
def display_depth ( dev , data , timestamp ) : \n 
~~~ global image_depth \n 
data = frame_convert . pretty_depth ( data ) \n 
mp . gray ( ) \n 
mp . figure ( 1 ) \n 
if image_depth : \n 
~~~ image_depth . set_data ( data ) \n 
~~~ image_depth = mp . imshow ( data , interpolation = , animated = True ) \n 
~~ mp . draw ( ) \n 
~~ def display_rgb ( dev , data , timestamp ) : \n 
~~~ global image_rgb \n 
mp . figure ( 2 ) \n 
if image_rgb : \n 
~~~ image_rgb . set_data ( data ) \n 
~~~ image_rgb = mp . imshow ( data , interpolation = , animated = True ) \n 
~~ def body ( * args ) : \n 
~~~ if not keep_running : \n 
~~~ raise freenect . Kill \n 
~~ ~~ def handler ( signum , frame ) : \n 
~~~ global keep_running \n 
keep_running = False \n 
~~ print ( ) \n 
signal . signal ( signal . SIGINT , handler ) \n 
freenect . runloop ( depth = display_depth , \n 
video = display_rgb , \n 
body = body ) \n 
from sklearn . datasets . samples_generator import make_blobs \n 
def plot_kmeans ( ) : \n 
~~~ X , y = make_blobs ( n_samples = 300 , centers = 4 , \n 
random_state = 0 , cluster_std = 0.60 ) \n 
y_pred = KMeans ( 4 ) . fit ( X ) . predict ( X ) \n 
fig , ax = plt . subplots ( 1 , 2 , figsize = ( 12 , 6 ) ) \n 
ax [ 0 ] . scatter ( X [ : , 0 ] , X [ : , 1 ] ) \n 
ax [ 0 ] . set_title ( ) \n 
ax [ 1 ] . scatter ( X [ : , 0 ] , X [ : , 1 ] , c = y ) \n 
ax [ 1 ] . set_title ( ) \n 
DATA_DIR = os . path . join ( \n 
os . path . dirname ( os . path . realpath ( __file__ ) ) , "data" ) \n 
CHART_DIR = os . path . join ( \n 
os . path . dirname ( os . path . realpath ( __file__ ) ) , "charts" ) \n 
for d in [ DATA_DIR , CHART_DIR ] : \n 
~~~ if not os . path . exists ( d ) : \n 
~~~ os . mkdir ( d ) \n 
warned_of_error = False \n 
def create_cloud ( oname , words , maxsize = 120 , fontname = ) : \n 
~~~ from pytagcloud import create_tag_image , make_tags \n 
~~~ if not warned_of_error : \n 
~~ words = [ ( w , int ( v * 10000 ) ) for v , w in words ] \n 
tags = make_tags ( words , maxsize = maxsize ) \n 
create_tag_image ( tags , oname , size = ( 1800 , 1200 ) , fontname = fontname ) \n 
from gzip import GzipFile \n 
dataset = [ [ int ( tok ) for tok in line . strip ( ) . split ( ) ] \n 
for line in GzipFile ( ) ] \n 
counts = defaultdict ( int ) \n 
for elem in chain ( * dataset ) : \n 
~~~ counts [ elem ] += 1 \n 
~~ counts = np . array ( list ( counts . values ( ) ) ) \n 
bins = [ 1 , 2 , 4 , 8 , 16 , 32 , 64 , 128 , 512 ] \n 
print ( . format ( , ) ) \n 
for i in range ( len ( bins ) ) : \n 
~~~ bot = bins [ i ] \n 
top = ( bins [ i + 1 ] if ( i + 1 ) < len ( bins ) else 100000000000 ) \n 
print ( . format ( \n 
bot , ( top if top < 1000 else ) , np . sum ( ( counts >= bot ) & ( counts < top ) ) ) ) \n 
from matplotlib import pylab \n 
from scipy . stats import norm , entropy \n 
from utils import CHART_DIR \n 
def mutual_info ( x , y , bins = 10 ) : \n 
~~~ counts_xy , bins_x , bins_y = np . histogram2d ( x , y , bins = ( bins , bins ) ) \n 
counts_x , bins = np . histogram ( x , bins = bins ) \n 
counts_y , bins = np . histogram ( y , bins = bins ) \n 
counts_xy += 1 \n 
counts_x += 1 \n 
counts_y += 1 \n 
P_xy = counts_xy / np . sum ( counts_xy , dtype = float ) \n 
P_x = counts_x / np . sum ( counts_x , dtype = float ) \n 
P_y = counts_y / np . sum ( counts_y , dtype = float ) \n 
I_xy = np . sum ( P_xy * np . log2 ( P_xy / ( P_x . reshape ( - 1 , 1 ) * P_y ) ) ) \n 
return I_xy / ( entropy ( counts_x ) + entropy ( counts_y ) ) \n 
~~ def plot_entropy ( ) : \n 
~~~ pylab . clf ( ) \n 
pylab . figure ( num = None , figsize = ( 5 , 4 ) ) \n 
pylab . title ( title ) \n 
pylab . ylabel ( "$H(X)$" ) \n 
pylab . xlim ( xmin = 0 , xmax = 1.1 ) \n 
x = np . arange ( 0.001 , 1 , 0.001 ) \n 
y = - x * np . log2 ( x ) - ( 1 - x ) * np . log2 ( 1 - x ) \n 
pylab . plot ( x , y ) \n 
pylab . autoscale ( tight = True ) \n 
pylab . grid ( True ) \n 
filename = "entropy_demo.png" \n 
pylab . savefig ( os . path . join ( CHART_DIR , filename ) , bbox_inches = "tight" ) \n 
~~ def _plot_mi_func ( x , y ) : \n 
~~~ mi = mutual_info ( x , y ) \n 
pylab . scatter ( x , y ) \n 
pylab . xlabel ( "$X_1$" ) \n 
pylab . ylabel ( "$X_2$" ) \n 
~~ def plot_mi_demo ( ) : \n 
pylab . clf ( ) \n 
pylab . figure ( num = None , figsize = ( 8 , 8 ) ) \n 
x = np . arange ( 0 , 10 , 0.2 ) \n 
pylab . subplot ( 221 ) \n 
y = 0.5 * x + norm . rvs ( 1 , scale = .01 , size = len ( x ) ) \n 
_plot_mi_func ( x , y ) \n 
pylab . subplot ( 222 ) \n 
y = 0.5 * x + norm . rvs ( 1 , scale = .1 , size = len ( x ) ) \n 
pylab . subplot ( 223 ) \n 
y = 0.5 * x + norm . rvs ( 1 , scale = 1 , size = len ( x ) ) \n 
pylab . subplot ( 224 ) \n 
y = norm . rvs ( 1 , scale = 10 , size = len ( x ) ) \n 
filename = "mi_demo_1.png" \n 
x = np . arange ( - 5 , 5 , 0.2 ) \n 
y = 0.5 * x ** 2 + norm . rvs ( 1 , scale = .01 , size = len ( x ) ) \n 
y = 0.5 * x ** 2 + norm . rvs ( 1 , scale = .1 , size = len ( x ) ) \n 
y = 0.5 * x ** 2 + norm . rvs ( 1 , scale = 1 , size = len ( x ) ) \n 
y = 0.5 * x ** 2 + norm . rvs ( 1 , scale = 10 , size = len ( x ) ) \n 
filename = "mi_demo_2.png" \n 
~~~ plot_entropy ( ) \n 
plot_mi_demo ( ) \n 
~~ from boto . exception import BotoServerError \n 
class InternalServerException ( BotoServerError ) : \n 
~~ class LimitExceededException ( BotoServerError ) : \n 
~~ class IdempotentParameterMismatchException ( BotoServerError ) : \n 
~~ class ResourceInUseException ( BotoServerError ) : \n 
~~ class ResourceNotFoundException ( BotoServerError ) : \n 
~~ class PredictorNotMountedException ( BotoServerError ) : \n 
~~ class InvalidInputException ( BotoServerError ) : \n 
from matplotlib . colors import ListedColormap \n 
from load import load_dataset \n 
from sklearn . neighbors import KNeighborsClassifier \n 
feature_names = [ \n 
def plot_decision ( features , labels , num_neighbors = 1 ) : \n 
y0 , y1 = features [ : , 2 ] . min ( ) * .9 , features [ : , 2 ] . max ( ) * 1.1 \n 
x0 , x1 = features [ : , 0 ] . min ( ) * .9 , features [ : , 0 ] . max ( ) * 1.1 \n 
X = np . linspace ( x0 , x1 , 1000 ) \n 
Y = np . linspace ( y0 , y1 , 1000 ) \n 
X , Y = np . meshgrid ( X , Y ) \n 
model = KNeighborsClassifier ( num_neighbors ) \n 
model . fit ( features [ : , ( 0 , 2 ) ] , labels ) \n 
C = model . predict ( np . vstack ( [ X . ravel ( ) , Y . ravel ( ) ] ) . T ) . reshape ( X . shape ) \n 
~~~ cmap = ListedColormap ( [ ( 1. , .7 , .7 ) , ( .7 , 1. , .7 ) , ( .7 , .7 , 1. ) ] ) \n 
~~~ cmap = ListedColormap ( [ ( 1. , 1. , 1. ) , ( .2 , .2 , .2 ) , ( .6 , .6 , .6 ) ] ) \n 
~~ fig , ax = plt . subplots ( ) \n 
ax . set_xlabel ( feature_names [ 0 ] ) \n 
ax . set_ylabel ( feature_names [ 2 ] ) \n 
ax . pcolormesh ( X , Y , C , cmap = cmap ) \n 
~~~ cmap = ListedColormap ( [ ( 1. , .0 , .0 ) , ( .1 , .6 , .1 ) , ( .0 , .0 , 1. ) ] ) \n 
ax . scatter ( features [ : , 0 ] , features [ : , 2 ] , c = labels , cmap = cmap ) \n 
~~~ for lab , ma in zip ( range ( 3 ) , "Do^" ) : \n 
~~~ ax . plot ( features [ labels == lab , 0 ] , features [ \n 
labels == lab , 2 ] , ma , c = ( 1. , 1. , 1. ) , ms = 6 ) \n 
~~ ~~ return fig , ax \n 
~~ features , labels = load_dataset ( ) \n 
names = sorted ( set ( labels ) ) \n 
labels = np . array ( [ names . index ( ell ) for ell in labels ] ) \n 
fig , ax = plot_decision ( features , labels ) \n 
features -= features . mean ( 0 ) \n 
features /= features . std ( 0 ) \n 
fig , ax = plot_decision ( features , labels , 11 ) \n 
from data import CHART_DIR \n 
from scipy . stats import norm \n 
from matplotlib import pyplot \n 
np . random . seed ( 3 ) \n 
num_per_class = 40 \n 
X = np . hstack ( ( norm . rvs ( 2 , size = num_per_class , scale = 2 ) , \n 
norm . rvs ( 8 , size = num_per_class , scale = 3 ) ) ) \n 
y = np . hstack ( ( np . zeros ( num_per_class ) , \n 
np . ones ( num_per_class ) ) ) \n 
def lr_model ( clf , X ) : \n 
~~~ return 1.0 / ( 1.0 + np . exp ( - ( clf . intercept_ + clf . coef_ * X ) ) ) \n 
logclf = LogisticRegression ( ) \n 
print ( logclf ) \n 
logclf . fit ( X . reshape ( num_per_class * 2 , 1 ) , y ) \n 
print ( np . exp ( logclf . intercept_ ) , np . exp ( logclf . coef_ . ravel ( ) ) ) \n 
print ( "P(x=-1)=%.2f\\tP(x=7)=%.2f" % \n 
( lr_model ( logclf , - 1 ) , lr_model ( logclf , 7 ) ) ) \n 
X_test = np . arange ( - 5 , 20 , 0.1 ) \n 
pyplot . figure ( figsize = ( 10 , 4 ) ) \n 
pyplot . xlim ( ( - 5 , 20 ) ) \n 
pyplot . scatter ( X , y , c = y ) \n 
pyplot . ylabel ( "class" ) \n 
pyplot . grid ( True , linestyle = , color = ) \n 
pyplot . savefig ( \n 
os . path . join ( CHART_DIR , "log_reg_example_data.png" ) , bbox_inches = "tight" ) \n 
def lin_model ( clf , X ) : \n 
~~~ return clf . intercept_ + clf . coef_ * X \n 
~~ from sklearn . linear_model import LinearRegression \n 
clf = LinearRegression ( ) \n 
print ( clf ) \n 
clf . fit ( X . reshape ( num_per_class * 2 , 1 ) , y ) \n 
X_odds = np . arange ( 0 , 1 , 0.001 ) \n 
pyplot . subplot ( 1 , 2 , 1 ) \n 
pyplot . plot ( X_test , lin_model ( clf , X_test ) ) \n 
X_ext = np . hstack ( ( X , norm . rvs ( 20 , size = 100 , scale = 5 ) ) ) \n 
y_ext = np . hstack ( ( y , np . ones ( 100 ) ) ) \n 
clf . fit ( X_ext . reshape ( num_per_class * 2 + 100 , 1 ) , y_ext ) \n 
pyplot . subplot ( 1 , 2 , 2 ) \n 
pyplot . scatter ( X_ext , y_ext , c = y_ext ) \n 
pyplot . plot ( X_ext , lin_model ( clf , X_ext ) ) \n 
os . path . join ( CHART_DIR , "log_reg_log_linear_fit.png" ) , bbox_inches = "tight" ) \n 
pyplot . plot ( X_test , lr_model ( logclf , X_test ) . ravel ( ) ) \n 
pyplot . plot ( X_test , np . ones ( X_test . shape [ 0 ] ) * 0.5 , "--" ) \n 
os . path . join ( CHART_DIR , "log_reg_example_fitted.png" ) , bbox_inches = "tight" ) \n 
X = np . arange ( 0 , 1 , 0.001 ) \n 
pyplot . xlim ( ( 0 , 1 ) ) \n 
pyplot . ylim ( ( 0 , 10 ) ) \n 
pyplot . plot ( X , X / ( 1 - X ) ) \n 
pyplot . xlabel ( "P" ) \n 
pyplot . plot ( X , np . log ( X / ( 1 - X ) ) ) \n 
os . path . join ( CHART_DIR , "log_reg_log_odds.png" ) , bbox_inches = "tight" ) \n 
def load ( ) : \n 
from scipy import sparse \n 
if not path . exists ( ) : \n 
~~ data = np . loadtxt ( ) \n 
ij = data [ : , : 2 ] \n 
values = data [ : , 2 ] \n 
reviews = sparse . csc_matrix ( ( values , ij . T ) ) . astype ( float ) \n 
return reviews . toarray ( ) \n 
~~ def get_train_test ( reviews = None , random_state = None ) : \n 
r = random . Random ( random_state ) \n 
if reviews is None : \n 
~~~ reviews = load ( ) \n 
~~ U , M = np . where ( reviews ) \n 
test_idxs = np . array ( r . sample ( range ( len ( U ) ) , len ( U ) // 10 ) ) \n 
train = reviews . copy ( ) \n 
train [ U [ test_idxs ] , M [ test_idxs ] ] = 0 \n 
test = np . zeros_like ( reviews ) \n 
test [ U [ test_idxs ] , M [ test_idxs ] ] = reviews [ U [ test_idxs ] , M [ test_idxs ] ] \n 
return train , test \n 
~~ from SimpleCV . base import * \n 
class NaiveBayesClassifier : \n 
def __init__ ( self , featureExtractors ) : \n 
~~~ if not ORANGE_ENABLED : \n 
self . mClassNames = [ ] \n 
~~ def classify ( self , image ) : \n 
~~ self . mClassifier = orange . BayesLearner ( self . mDataSetOrange ) \n 
~~~ crossValidator = orngTest . learnAndTestOnLearnData ( [ orange . BayesLearner ] , self . mDataSetOrange confusion = orngStat . confusionMatrices ( crossValidator ) [ 0 ] \n 
classes = self . mDataSetOrange . domain . classVar . values \n 
~~~ print ( "%s" + ( "\\t%i" * len ( classes ) ) ) % ( ( className , ) + tuple ( classConfusions ) ) \n 
~~ ~~ return [ good , bad , confusion ] \n 
self . mOrangeDomain = orange . Domain ( map ( orange . FloatVariable , colNames ) , orange . EnumVariable \n 
~~ dataset = [ ] \n 
~~~ crossValidator = orngTest . learnAndTestOnTestData ( [ orange . BayesLearner ( ) ] , self . mDataSetOrange confusion = orngStat . confusionMatrices ( crossValidator ) [ 0 ] \n 
~~ ~~ ~~ import math \n 
import operator \n 
def calcShannonEnt ( dataSet ) : \n 
~~~ numEntries = len ( dataSet ) \n 
labelCounts = { } \n 
for featVec in dataSet : \n 
~~~ currentLabel = featVec [ - 1 ] \n 
if currentLabel not in labelCounts . keys ( ) : \n 
~~~ labelCounts [ currentLabel ] = 0 \n 
~~ labelCounts [ currentLabel ] += 1 \n 
~~ shannonEnt = 0.0 \n 
for key in labelCounts : \n 
~~~ prob = float ( labelCounts [ key ] ) / numEntries \n 
shannonEnt -= prob * math . log ( prob , 2 ) \n 
~~ return shannonEnt \n 
~~ def createDataSet ( ) : \n 
~~~ dataSet = [ [ 1 , 0 , ] , [ 1 , 1 , ] , [ 0 , 1 , ] , [ 0 , 0 , ] ] \n 
labels = [ , ] \n 
return dataSet , labels \n 
~~ def splitDataSet ( dataSet , axis , value ) : \n 
~~~ retDataSet = [ ] \n 
~~~ if featVec [ axis ] == value : \n 
reducedFeatVec . extend ( featVec [ axis + 1 : ] ) \n 
retDataSet . append ( reducedFeatVec ) \n 
~~ ~~ return retDataSet \n 
~~ def chooseBestFeatureToSplit ( dataSet ) : \n 
baseEntropy = calcShannonEnt ( dataSet ) \n 
bestInfoGain = 0.0 ; bestFeature = - 1 \n 
newEntropy = 0.0 \n 
for value in uniqueVals : \n 
~~~ subDataSet = splitDataSet ( dataSet , i , value ) \n 
prob = len ( subDataSet ) / float ( len ( dataSet ) ) \n 
newEntropy += prob * calcShannonEnt ( subDataSet ) \n 
bestFeature = i \n 
~~ def majorityCnt ( classList ) : \n 
~~~ classCount = { } \n 
for vote in classList : \n 
~~~ if vote not in classCount . keys ( ) : classCount [ vote ] = 0 \n 
classCount [ vote ] += 1 \n 
~~ sortedClassCount = sorted ( classCount . iteritems ( ) , key = operator . itemgetter ( 1 ) , reverse = True ) \n 
return sortedClassCount [ 0 ] [ 0 ] \n 
~~ def createTree ( dataSet , labels ) : \n 
~~~ classList = [ example [ - 1 ] for example in dataSet ] \n 
if classList . count ( classList [ 0 ] ) == len ( classList ) : \n 
~~~ return majorityCnt ( classList ) \n 
~~ bestFeat = chooseBestFeatureToSplit ( dataSet ) \n 
bestFeatLabel = labels [ bestFeat ] \n 
myTree = { bestFeatLabel : { } } \n 
del ( labels [ bestFeat ] ) \n 
featValues = [ example [ bestFeat ] for example in dataSet ] \n 
uniqueVals = set ( featValues ) \n 
~~~ subLabels = labels [ : ] \n 
myTree [ bestFeatLabel ] [ value ] = createTree ( splitDataSet ( dataSet , bestFeat , value ) , subLabels ) \n 
~~ return myTree \n 
~~ def classify ( inputTree , featLabels , testVec ) : \n 
~~~ firstStr = inputTree . keys ( ) [ 0 ] \n 
secondDict = inputTree [ firstStr ] \n 
featIndex = featLabels . index ( firstStr ) \n 
key = testVec [ featIndex ] \n 
valueOfFeat = secondDict [ key ] \n 
if isinstance ( valueOfFeat , dict ) : \n 
~~~ classLabel = classify ( valueOfFeat , featLabels , testVec ) \n 
~~ else : classLabel = valueOfFeat \n 
return classLabel \n 
~~ def getResult ( ) : \n 
~~~ dataSet , labels = createDataSet ( ) \n 
chooseBestFeatureToSplit ( dataSet ) \n 
mtree = createTree ( dataSet , labels ) \n 
print mtree \n 
print classify ( mtree , [ , ] , [ 0 , 0 ] ) \n 
~~~ getResult ( ) \n 
def load_dataset ( dataset_name ) : \n 
data = [ ] \n 
with open ( . format ( dataset_name ) ) as ifile : \n 
~~~ for line in ifile : \n 
~~~ tokens = line . strip ( ) . split ( ) \n 
data . append ( [ float ( tk ) for tk in tokens [ : - 1 ] ] ) \n 
labels . append ( tokens [ - 1 ] ) \n 
~~ ~~ data = np . array ( data ) \n 
labels = np . array ( labels ) \n 
return data , labels \n 
~~ import time \n 
from sklearn . cross_validation import ShuffleSplit \n 
from utils import load_sanders_data \n 
from utils import tweak_labels \n 
from sklearn . feature_extraction . text import TfidfVectorizer \n 
from sklearn . pipeline import Pipeline \n 
from sklearn . naive_bayes import MultinomialNB \n 
def create_ngram_model ( ) : \n 
~~~ tfidf_ngrams = TfidfVectorizer ( ngram_range = ( 1 , 3 ) , \n 
analyzer = "word" , binary = False ) \n 
clf = MultinomialNB ( ) \n 
pipeline = Pipeline ( [ ( , tfidf_ngrams ) , ( , clf ) ] ) \n 
return pipeline \n 
~~~ cv = ShuffleSplit ( \n 
n = len ( X ) , n_iter = 10 , test_size = 0.3 , random_state = 0 ) \n 
for train , test in cv : \n 
clf = clf_factory ( ) \n 
fpr , tpr , roc_thresholds = roc_curve ( y_test , proba [ : , 1 ] ) \n 
y_test , proba [ : , 1 ] ) \n 
~~ scores_to_sort = pr_scores \n 
median = np . argsort ( scores_to_sort ) [ len ( scores_to_sort ) / 2 ] \n 
~~~ plot_pr ( pr_scores [ median ] , name , "01" , precisions [ median ] , \n 
recalls [ median ] , label = name ) \n 
summary = ( np . mean ( scores ) , np . std ( scores ) , \n 
np . mean ( pr_scores ) , np . std ( pr_scores ) ) \n 
print ( "%.3f\\t%.3f\\t%.3f\\t%.3f\\t" % summary ) \n 
~~ return np . mean ( train_errors ) , np . mean ( test_errors ) \n 
~~ def print_incorrect ( clf , X , Y ) : \n 
~~~ Y_hat = clf . predict ( X ) \n 
wrong_idx = Y_hat != Y \n 
X_wrong = X [ wrong_idx ] \n 
Y_wrong = Y [ wrong_idx ] \n 
Y_hat_wrong = Y_hat [ wrong_idx ] \n 
for idx in range ( len ( X_wrong ) ) : \n 
( X_wrong [ idx ] , Y_hat_wrong [ idx ] , Y_wrong [ idx ] ) ) \n 
~~~ X_orig , Y_orig = load_sanders_data ( ) \n 
classes = np . unique ( Y_orig ) \n 
for c in classes : \n 
pos_neg = np . logical_or ( Y_orig == "positive" , Y_orig == "negative" ) \n 
X = X_orig [ pos_neg ] \n 
Y = Y_orig [ pos_neg ] \n 
Y = tweak_labels ( Y , [ "positive" ] ) \n 
X = X_orig \n 
Y = tweak_labels ( Y_orig , [ "positive" , "negative" ] ) \n 
Y = tweak_labels ( Y_orig , [ "positive" ] ) \n 
Y = tweak_labels ( Y_orig , [ "negative" ] ) \n 
~~ from __future__ import print_function \n 
def nn_movie ( ureviews , reviews , uid , mid , k = 1 ) : \n 
X = ureviews \n 
y = ureviews [ mid ] . copy ( ) \n 
y -= y . mean ( ) \n 
y /= ( y . std ( ) + 1e-5 ) \n 
corrs = np . dot ( X , y ) \n 
likes = corrs . argsort ( ) \n 
likes = likes [ : : - 1 ] \n 
c = 0 \n 
pred = 3. \n 
for ell in likes : \n 
~~~ if ell == mid : \n 
~~ if reviews [ uid , ell ] > 0 : \n 
~~~ pred = reviews [ uid , ell ] \n 
if c == k : \n 
~~~ return pred \n 
~~ c += 1 \n 
~~ ~~ return pred \n 
~~ def all_estimates ( reviews , k = 1 ) : \n 
reviews = reviews . astype ( float ) \n 
k -= 1 \n 
nusers , nmovies = reviews . shape \n 
estimates = np . zeros_like ( reviews ) \n 
for u in range ( nusers ) : \n 
~~~ ureviews = np . delete ( reviews , u , axis = 0 ) \n 
ureviews -= ureviews . mean ( 0 ) \n 
ureviews /= ( ureviews . std ( 0 ) + 1e-5 ) \n 
ureviews = ureviews . T . copy ( ) \n 
for m in np . where ( reviews [ u ] > 0 ) [ 0 ] : \n 
~~~ estimates [ u , m ] = nn_movie ( ureviews , reviews , u , m , k ) \n 
~~ ~~ return estimates \n 
~~~ from load_ml100k import load \n 
reviews = load ( ) \n 
estimates = all_estimates ( reviews ) \n 
error = ( estimates - reviews ) \n 
error 2 \n 
error = error [ reviews > 0 ] \n 
rmse = np . sqrt ( error . mean ( ) ) \n 
~~ from SimpleCV import * \n 
print "" \n 
if not ( inp == "" or inp . lower ( ) == "y" ) : \n 
sys . exit ( ) \n 
~~ machine_learning_data_set = "https://github.com/downloads/sightmachine/SimpleCV/machine_learning_dataset.zip" data_path = download_and_extract ( machine_learning_data_set ) \n 
w = 800 \n 
h = 600 \n 
n = 50 \n 
display = Display ( resolution = ( w , h ) ) \n 
hue = HueHistogramFeatureExtractor ( mNBins = 16 ) \n 
edge = EdgeHistogramFeatureExtractor ( ) \n 
bof = BOFFeatureExtractor ( ) \n 
bof . load ( ) \n 
haar = HaarLikeFeatureExtractor ( fname = "../Features/haar.txt" ) \n 
morph = MorphologyFeatureExtractor ( ) \n 
spath = data_path + "/data/structured/" \n 
upath = data_path + "/data/unstructured/" \n 
ball_path = spath + "ball/" \n 
basket_path = spath + "basket/" \n 
boat_path = spath + "boat/" \n 
cactus_path = spath + "cactus/" \n 
cup_path = spath + "cup/" \n 
duck_path = spath + "duck/" \n 
gb_path = spath + "greenblock/" \n 
match_path = spath + "matches/" \n 
rb_path = spath + "redblock/" \n 
s1_path = spath + "stuffed/" \n 
s2_path = spath + "stuffed2/" \n 
s3_path = spath + "stuffed3/" \n 
arbor_path = upath + "arborgreens/" \n 
football_path = upath + "football/" \n 
sanjuan_path = upath + "sanjuans/" \n 
extractors = [ hue ] \n 
path = [ cactus_path , cup_path , basket_path ] \n 
classes = [ , , ] \n 
props = { \n 
classifierSVMP = SVMClassifier ( extractors , props ) \n 
for p in path : \n 
~~~ data . append ( ImageSet ( p ) ) \n 
~~ classifierSVMP . train ( data , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierSVMP . test ( data , classes , disp = display , subset = n ) \n 
~~~ files . extend ( glob . glob ( os . path . join ( path [ 0 ] , ext ) ) ) \n 
~~ for i in range ( 10 ) : \n 
~~~ img = Image ( files [ i ] ) \n 
cname = classifierSVMP . classify ( img ) \n 
print ( files [ i ] + + cname ) \n 
~~ classifierSVMP . save ( ) \n 
testSVM = SVMClassifier . load ( ) \n 
#testSVM.setFeatureExtractors(extractors) \n 
files = glob . glob ( os . path . join ( path [ 0 ] , ) ) \n 
for i in range ( 10 ) : \n 
cname = testSVM . classify ( img ) \n 
extractors = [ hue , edge ] \n 
classifierSVMRBF = SVMClassifier ( extractors , props ) \n 
~~ classifierSVMRBF . train ( data , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierSVMRBF . test ( data , classes , disp = display , subset = n ) \n 
cname = classifierSVMRBF . classify ( img ) \n 
~~ classifierSVMRBF . save ( ) \n 
testSVMRBF = SVMClassifier . load ( ) \n 
#testSVMRBF.setFeatureExtractors(extractors) \n 
cname = testSVMRBF . classify ( img ) \n 
extractors = [ haar ] \n 
classifierBayes = NaiveBayesClassifier ( extractors ) # \n 
path = [ arbor_path , football_path , sanjuan_path ] \n 
classifierBayes . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierBayes . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierBayes . classify ( img ) \n 
~~ classifierBayes . save ( ) \n 
testBayes = NaiveBayesClassifier . load ( ) \n 
testBayes . setFeatureExtractors ( extractors ) \n 
cname = testBayes . classify ( img ) \n 
extractors = [ morph ] \n 
classifierForest = TreeClassifier ( extractors , flavor = ) # \n 
path = [ s1_path , s2_path , s3_path ] \n 
classifierForest . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierForest . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierForest . classify ( img ) \n 
~~ classifierForest . save ( ) \n 
testForest = TreeClassifier . load ( ) \n 
testForest . setFeatureExtractors ( extractors ) \n 
cname = testForest . classify ( img ) \n 
classifierBagTree = TreeClassifier ( extractors , flavor = ) # \n 
classifierBagTree . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierBagTree . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierBagTree . classify ( img ) \n 
~~ classifierBagTree . save ( ) \n 
testBagTree = TreeClassifier . load ( ) \n 
testBagTree . setFeatureExtractors ( extractors ) \n 
cname = testBagTree . classify ( img ) \n 
classifierTree = TreeClassifier ( featureExtractors = extractors ) \n 
classifierTree . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierTree . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierTree . classify ( img ) \n 
classifierTree . save ( ) \n 
testTree = TreeClassifier . load ( ) \n 
testTree . setFeatureExtractors ( extractors ) \n 
cname = testTree . classify ( img ) \n 
classifierBTree = TreeClassifier ( extractors , flavor = ) # \n 
classifierBTree . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierBTree . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierBTree . classify ( img ) \n 
~~ classifierBTree . save ( ) \n 
testBoostTree = TreeClassifier . load ( ) \n 
testBoostTree . setFeatureExtractors ( extractors ) \n 
cname = testBoostTree . classify ( img ) \n 
classifierKNN = KNNClassifier ( extractors ) # \n 
classifierKNN . train ( path , classes , disp = display , subset = n ) #train \n 
[ pos , neg , confuse ] = classifierKNN . test ( path , classes , disp = display , subset = n ) \n 
cname = classifierKNN . classify ( img ) \n 
~~ classifierKNN . save ( ) \n 
testKNN = KNNClassifier . load ( ) \n 
testKNN . setFeatureExtractors ( extractors ) \n 
cname = testKNN . classify ( img ) \n 
~~ print "" \n 
from sklearn . cross_validation import cross_val_score \n 
from sklearn . preprocessing import StandardScaler \n 
features , labels = load_dataset ( ) \n 
ks = np . arange ( 1 , 161 ) \n 
classifier = KNeighborsClassifier ( ) \n 
classifier = Pipeline ( [ ( , StandardScaler ( ) ) , ( , classifier ) ] ) \n 
accuracies = [ ] \n 
~~~ classifier . set_params ( knn__n_neighbors = k ) \n 
crossed = cross_val_score ( classifier , features , labels ) \n 
accuracies . append ( crossed . mean ( ) ) \n 
~~ accuracies = np . array ( accuracies ) \n 
plt . plot ( ks , accuracies * 100 ) \n 
from utils import log_false_positives \n 
from utils import load_sent_word_net \n 
sent_word_net = load_sent_word_net ( ) \n 
phase = "03" \n 
emo_repl = { \n 
emo_repl_order = [ k for ( k_len , k ) in reversed ( \n 
sorted ( [ ( len ( k ) , k ) for k in list ( emo_repl . keys ( ) ) ] ) ) ] \n 
re_repl = { \n 
r"\\br\\b" : "are" , \n 
r"\\bu\\b" : "you" , \n 
r"\\bhaha\\b" : "ha" , \n 
r"\\bhahaha\\b" : "ha" , \n 
def create_ngram_model ( params = None ) : \n 
~~~ def preprocessor ( tweet ) : \n 
~~~ global emoticons_replaced \n 
tweet = tweet . lower ( ) \n 
for k in emo_repl_order : \n 
~~~ tweet = tweet . replace ( k , emo_repl [ k ] ) \n 
~~ for r , repl in re_repl . items ( ) : \n 
~~~ tweet = re . sub ( r , repl , tweet ) \n 
~~ return tweet \n 
~~ tfidf_ngrams = TfidfVectorizer ( preprocessor = preprocessor , \n 
analyzer = "word" ) \n 
if params : \n 
~~~ pipeline . set_params ( ** params ) \n 
~~ return pipeline \n 
clfs . append ( clf ) \n 
~~ if plot : \n 
~~~ scores_to_sort = pr_scores \n 
plot_pr ( pr_scores [ median ] , name , phase , precisions [ median ] , \n 
log_false_positives ( clfs [ median ] , X_test , y_test , name ) \n 
~~ summary = ( np . mean ( scores ) , np . std ( scores ) , \n 
~~ ~~ def get_best_model ( ) : \n 
~~~ best_params = dict ( tfidf__ngram_range = ( 1 , 2 ) , \n 
tfidf__min_df = 1 , \n 
tfidf__stop_words = None , \n 
tfidf__smooth_idf = False , \n 
tfidf__use_idf = False , \n 
tfidf__sublinear_tf = True , \n 
tfidf__binary = False , \n 
clf__alpha = 0.01 , \n 
best_clf = create_ngram_model ( best_params ) \n 
return best_clf \n 
plot = True ) \n 
from sklearn . metrics import precision_recall_curve , roc_curve \n 
from sklearn . metrics import auc \n 
from sklearn . metrics import confusion_matrix \n 
from utils import plot_pr , plot_roc , plot_confusion_matrix , GENRE_LIST \n 
from fft import read_fft \n 
genre_list = GENRE_LIST \n 
def train_model ( clf_factory , X , Y , name , plot = False ) : \n 
~~~ labels = np . unique ( Y ) \n 
cv = ShuffleSplit ( \n 
n = len ( X ) , n_iter = 1 , test_size = 0.3 , indices = True , random_state = 0 ) \n 
pr_scores = defaultdict ( list ) \n 
precisions , recalls , thresholds = defaultdict ( \n 
list ) , defaultdict ( list ) , defaultdict ( list ) \n 
roc_scores = defaultdict ( list ) \n 
tprs = defaultdict ( list ) \n 
fprs = defaultdict ( list ) \n 
cms = [ ] \n 
y_pred = clf . predict ( X_test ) \n 
cm = confusion_matrix ( y_test , y_pred ) \n 
cms . append ( cm ) \n 
~~~ y_label_test = np . asarray ( y_test == label , dtype = int ) \n 
proba_label = proba [ : , label ] \n 
y_label_test , proba_label ) \n 
pr_scores [ label ] . append ( auc ( recall , precision ) ) \n 
precisions [ label ] . append ( precision ) \n 
recalls [ label ] . append ( recall ) \n 
thresholds [ label ] . append ( pr_thresholds ) \n 
fpr , tpr , roc_thresholds = roc_curve ( y_label_test , proba_label ) \n 
roc_scores [ label ] . append ( auc ( fpr , tpr ) ) \n 
tprs [ label ] . append ( tpr ) \n 
fprs [ label ] . append ( fpr ) \n 
~~ ~~ if plot : \n 
~~~ for label in labels : \n 
scores_to_sort = roc_scores [ label ] \n 
plot_pr ( pr_scores [ label ] [ median ] , desc , precisions [ label ] [ median ] , \n 
recalls [ label ] [ median ] , label = % genre_list [ label ] ) \n 
plot_roc ( roc_scores [ label ] [ median ] , desc , tprs [ label ] [ median ] , \n 
fprs [ label ] [ median ] , label = % genre_list [ label ] ) \n 
~~ ~~ all_pr_scores = np . asarray ( pr_scores . values ( ) ) . flatten ( ) \n 
np . mean ( all_pr_scores ) , np . std ( all_pr_scores ) ) \n 
return np . mean ( train_errors ) , np . mean ( test_errors ) , np . asarray ( cms ) \n 
~~ def create_model ( ) : \n 
~~~ from sklearn . linear_model . logistic import LogisticRegression \n 
clf = LogisticRegression ( ) \n 
return clf \n 
~~~ X , y = read_fft ( genre_list ) \n 
train_avg , test_avg , cms = train_model ( \n 
cm_avg = np . mean ( cms , axis = 0 ) \n 
cm_norm = cm_avg / np . sum ( cm_avg , axis = 0 ) \n 
plot_confusion_matrix ( cm_norm , genre_list , "fft" , \n 
from SimpleCV . Features . Features import Feature , FeatureSet \n 
from SimpleCV . Color import Color \n 
from SimpleCV . ImageClass import Image \n 
from SimpleCV . Features . Detection import ShapeContextDescriptor \n 
import scipy . stats as sps \n 
class ShapeContextClassifier ( ) : \n 
~~~ def __init__ ( self , images , labels ) : \n 
~~~ from sklearn import neighbors \n 
~~ self . imgMap = { } \n 
self . ptMap = { } \n 
self . descMap = { } \n 
self . knnMap = { } \n 
self . blobCount = { } \n 
self . labels = labels \n 
self . images = images \n 
warnings . simplefilter ( "ignore" ) \n 
for i in range ( 0 , len ( images ) ) : \n 
self . imgMap [ labels [ i ] ] = images [ i ] \n 
pts , desc , count = self . _image2FeatureVector ( images [ i ] ) \n 
self . blobCount [ labels [ i ] ] = count \n 
self . ptMap [ labels [ i ] ] = pts \n 
self . descMap [ labels [ i ] ] = desc \n 
knn = neighbors . KNeighborsClassifier ( ) \n 
knn . fit ( desc , range ( 0 , len ( pts ) ) ) \n 
self . knnMap [ labels [ i ] ] = knn \n 
~~ ~~ def _image2FeatureVector ( self , img ) : \n 
fulllist = [ ] \n 
raw_descriptors = [ ] \n 
blobs = img . findBlobs ( minsize = 50 ) \n 
if ( blobs is not None ) : \n 
~~~ count = len ( blobs ) \n 
for b in blobs : \n 
~~~ fulllist += b . _filterSCPoints ( ) \n 
raw_descriptors = blobs [ 0 ] . _generateSC ( fulllist ) \n 
~~ ~~ return fulllist , raw_descriptors , count \n 
~~ def _getMatch ( self , model_scd , test_scd ) : \n 
~~~ correspondence , distance = self . _doMatching ( model_scd , test_scd ) \n 
return self . _matchQuality ( distances ) \n 
~~ def _doMatching ( self , model_name , test_scd ) : \n 
~~~ myPts = len ( test_scd ) \n 
otPts = len ( self . ptMap [ model_name ] ) \n 
distance = [ ] \n 
results = [ ] \n 
for sample in test_scd : \n 
~~~ best = self . knnMap [ model_name ] . predict ( sample ) \n 
scd = self . descMap [ model_name ] [ idx ] \n 
temp = np . sqrt ( np . sum ( ( ( sample - scd ) ** 2 ) ) ) \n 
if ( math . isnan ( temp ) ) : \n 
~~~ temp = sys . maxint \n 
~~ distance . append ( temp ) \n 
~~ return [ otherIdx , distance ] \n 
~~ def _matchQuality ( self , distances ) : \n 
~~~ tmean = np . mean ( distances ) \n 
std = np . std ( distances ) \n 
return tmean , std \n 
~~ def _buildMatchDict ( self , image , countBlobs ) : \n 
~~~ points , descriptors , count = self . _image2FeatureVector ( image ) \n 
matchDict = { } \n 
matchStd = { } \n 
for key , value in self . descMap . items ( ) : \n 
~~~ correspondence , distances = self . _doMatching ( key , descriptors ) \n 
result , std = self . _matchQuality ( distances ) \n 
matchDict [ key ] = result \n 
matchStd [ key ] = std \n 
~~ elif ( not countBlobs ) : \n 
~~ ~~ return points , descriptors , count , matchDict , matchStd \n 
~~ def classify ( self , image , blobFilter = True ) : \n 
points , descriptors , count , matchDict , matchStd = self . _buildMatchDict ( image , blobFilter ) \n 
best = sys . maxint \n 
for k , v in matchDict . items ( ) : \n 
~~~ if ( v < best ) : \n 
~~~ best = v \n 
best_name = k \n 
~~ ~~ return best_name , best , matchDict , matchStd \n 
~~ def getTopNMatches ( self , image , n = 3 , blobFilter = True ) : \n 
n = np . clip ( n , 1 , len ( self . labels ) ) \n 
best_matches = list ( sorted ( matchDict , key = matchDict . __getitem__ ) ) \n 
retList = [ ] \n 
for k in best_matches : \n 
~~~ retList . append ( ( k , matchDict [ k ] ) ) \n 
~~ return retList [ 0 : n ] , matchDict , matchStd \n 
~~ ~~ from SimpleCV . Color import Color \n 
from SimpleCV . base import time , cv , np \n 
class TrackSet ( FeatureSet ) : \n 
~~~ import cv2 \n 
~~ def __init__ ( self ) : \n 
~~~ self . kalman = None \n 
self . predict_pt = ( 0 , 0 ) \n 
self . __kalman ( ) \n 
~~ def append ( self , f ) : \n 
list . append ( self , f ) \n 
ts = self \n 
if ts [ 0 ] . area <= 0 : \n 
~~ f . sizeRatio = float ( ts [ - 1 ] . area ) / float ( ts [ 0 ] . area ) \n 
f . vel = self . __pixelVelocity ( ) \n 
f . rt_vel = self . __pixleVelocityRealTime ( ) \n 
self . __setKalman ( ) \n 
self . __predictKalman ( ) \n 
self . __changeMeasure ( ) \n 
self . __correctKalman ( ) \n 
f . predict_pt = self . predict_pt \n 
f . state_pt = self . state_pt \n 
~~ def trimList ( self , num ) : \n 
for i in range ( num ) : \n 
~~~ ts . pop ( 0 ) \n 
~~ ~~ def areaRatio ( self ) : \n 
return np . array ( [ f . areaRatio for f in self ] ) \n 
~~ def drawPath ( self , color = Color . GREEN , thickness = 2 ) : \n 
img = self [ - 1 ] . image \n 
for i in range ( len ( ts ) - 1 ) : \n 
~~~ img . drawLine ( ( ts [ i ] . center ) , ( ts [ i + 1 ] . center ) , color = color , thickness = thickness ) \n 
~~ ~~ def draw ( self , color = Color . GREEN , rad = 1 , thickness = 1 ) : \n 
f = self [ - 1 ] \n 
f . image . drawCircle ( f . center , rad , color , thickness ) \n 
~~ def drawBB ( self , color = Color . GREEN , thickness = 3 ) : \n 
f . image . drawRectangle ( f . bb_x , f . bb_y , f . w , f . h , color , thickness ) \n 
~~ def trackLength ( self ) : \n 
return len ( self ) \n 
~~ def trackImages ( self , cv2_numpy = False ) : \n 
if cv2_numpy : \n 
~~~ return [ f . cv2numpy for f in self ] \n 
~~ return [ f . image for f in self ] \n 
~~ def BBTrack ( self ) : \n 
return [ f . bb for f in self ] \n 
~~ def __pixelVelocity ( self ) : \n 
if len ( ts ) < 2 : \n 
~~~ return ( 0 , 0 ) \n 
~~ dx = ts [ - 1 ] . x - ts [ - 2 ] . x \n 
dy = ts [ - 1 ] . y - ts [ - 2 ] . y \n 
return ( dx , dy ) \n 
~~ def pixelVelocity ( self ) : \n 
return np . array ( [ f . vel for f in self ] ) \n 
~~ def __pixleVelocityRealTime ( self ) : \n 
dt = ts [ - 1 ] . time - ts [ - 2 ] . time \n 
return ( float ( dx ) / dt , float ( dy ) / dt ) \n 
~~ def pixleVelocityRealTime ( self ) : \n 
return np . array ( [ f . rt_vel for f in self ] ) \n 
~~ def showCoordinates ( self , pos = None , color = Color . GREEN , size = None ) : \n 
f = ts [ - 1 ] \n 
img = f . image \n 
if not pos : \n 
~~~ imgsize = img . size ( ) \n 
pos = ( imgsize [ 0 ] - 120 , 10 ) \n 
~~~ size = 16 \n 
img . drawText ( text , pos [ 0 ] , pos [ 1 ] , color , size ) \n 
~~ def showSizeRatio ( self , pos = None , color = Color . GREEN , size = None ) : \n 
pos = ( imgsize [ 0 ] - 120 , 30 ) \n 
~~ def showPixelVelocity ( self , pos = None , color = Color . GREEN , size = None ) : \n 
vel = f . vel \n 
pos = ( imgsize [ 0 ] - 120 , 50 ) \n 
~~ def showPixelVelocityRT ( self , pos = None , color = Color . GREEN , size = None ) : \n 
vel_rt = f . rt_vel \n 
pos = ( imgsize [ 0 ] - 120 , 90 ) \n 
~~ def processTrack ( self , func ) : \n 
return [ func ( f . image ) for f in self ] \n 
~~ def getBackground ( self ) : \n 
imgs = self . trackImages ( cv2_numpy = True ) \n 
f = imgs [ 0 ] \n 
avg = np . float32 ( f ) \n 
for img in imgs [ 1 : ] : \n 
~~~ f = img \n 
cv2 . accumulateWeighted ( f , avg , 0.01 ) \n 
res = cv2 . convertScaleAbs ( avg ) \n 
~~ return Image ( res , cv2image = True ) \n 
~~ def __kalman ( self ) : \n 
~~~ self . kalman = cv . CreateKalman ( 4 , 2 , 0 ) \n 
self . kalman_process_noise = cv . CreateMat ( 4 , 1 , cv . CV_32FC1 ) \n 
self . kalman_measurement = cv . CreateMat ( 2 , 1 , cv . CV_32FC1 ) \n 
~~ def __setKalman ( self ) : \n 
~~~ ts = self \n 
~~~ self . kalman_x = ts [ - 1 ] . x \n 
self . kalman_y = ts [ - 1 ] . y \n 
~~~ self . kalman_x = ts [ - 2 ] . x \n 
self . kalman_y = ts [ - 2 ] . y \n 
~~ self . kalman . state_pre [ 0 , 0 ] = self . kalman_x \n 
self . kalman . state_pre [ 1 , 0 ] = self . kalman_y \n 
self . kalman . state_pre [ 2 , 0 ] = self . predict_pt [ 0 ] \n 
self . kalman . state_pre [ 3 , 0 ] = self . predict_pt [ 1 ] \n 
self . kalman . transition_matrix [ 0 , 0 ] = 1 \n 
self . kalman . transition_matrix [ 0 , 1 ] = 0 \n 
self . kalman . transition_matrix [ 0 , 2 ] = 1 \n 
self . kalman . transition_matrix [ 0 , 3 ] = 0 \n 
self . kalman . transition_matrix [ 1 , 0 ] = 0 \n 
self . kalman . transition_matrix [ 1 , 1 ] = 1 \n 
self . kalman . transition_matrix [ 1 , 2 ] = 0 \n 
self . kalman . transition_matrix [ 1 , 3 ] = 1 \n 
self . kalman . transition_matrix [ 2 , 0 ] = 0 \n 
self . kalman . transition_matrix [ 2 , 1 ] = 0 \n 
self . kalman . transition_matrix [ 2 , 2 ] = 1 \n 
self . kalman . transition_matrix [ 2 , 3 ] = 0 \n 
self . kalman . transition_matrix [ 3 , 0 ] = 0 \n 
self . kalman . transition_matrix [ 3 , 1 ] = 0 \n 
self . kalman . transition_matrix [ 3 , 2 ] = 0 \n 
self . kalman . transition_matrix [ 3 , 3 ] = 1 \n 
cv . SetIdentity ( self . kalman . measurement_matrix , cv . RealScalar ( 1 ) ) \n 
cv . SetIdentity ( self . kalman . process_noise_cov , cv . RealScalar ( 1e-5 ) ) \n 
cv . SetIdentity ( self . kalman . measurement_noise_cov , cv . RealScalar ( 1e-1 ) ) \n 
cv . SetIdentity ( self . kalman . error_cov_post , cv . RealScalar ( 1 ) ) \n 
~~ def __predictKalman ( self ) : \n 
~~~ self . kalman_prediction = cv . KalmanPredict ( self . kalman ) \n 
self . predict_pt = ( self . kalman_prediction [ 0 , 0 ] , self . kalman_prediction [ 1 , 0 ] ) \n 
~~ def __correctKalman ( self ) : \n 
~~~ self . kalman_estimated = cv . KalmanCorrect ( self . kalman , self . kalman_measurement ) \n 
self . state_pt = ( self . kalman_estimated [ 0 , 0 ] , self . kalman_estimated [ 1 , 0 ] ) \n 
~~ def __changeMeasure ( self ) : \n 
self . kalman_measurement [ 0 , 0 ] = ts [ - 1 ] . x \n 
self . kalman_measurement [ 1 , 0 ] = ts [ - 1 ] . y \n 
~~ def predictedCoordinates ( self ) : \n 
return np . array ( [ f . predict_pt for f in self ] ) \n 
~~ def predictX ( self ) : \n 
return np . array ( [ f . predict_pt [ 0 ] for f in self ] ) \n 
~~ def predictY ( self ) : \n 
return np . array ( [ f . predict_pt [ 1 ] for f in self ] ) \n 
~~ def drawPredicted ( self , color = Color . GREEN , rad = 1 , thickness = 1 ) : \n 
f . image . drawCircle ( f . predict_pt , rad , color , thickness ) \n 
~~ def drawCorrected ( self , color = Color . GREEN , rad = 1 , thickness = 1 ) : \n 
f . image . drawCircle ( f . state_pt , rad , color , thickness ) \n 
~~ def drawPredictedPath ( self , color = Color . GREEN , thickness = 2 ) : \n 
for i in range ( 1 , len ( ts ) - 1 ) : \n 
~~~ img . drawLine ( ( ts [ i ] . predict_pt ) , ( ts [ i + 1 ] . predict_pt ) , color = color , thickness = thickness ) \n 
~~ ~~ def showPredictedCoordinates ( self , pos = None , color = Color . GREEN , size = None ) : \n 
pos = ( 5 , 10 ) \n 
~~ def showCorrectedCoordinates ( self , pos = None , color = Color . GREEN , size = None ) : \n 
pos = ( 5 , 40 ) \n 
~~ def correctX ( self ) : \n 
return np . array ( [ f . state_pt [ 0 ] for f in self ] ) \n 
~~ def correctY ( self ) : \n 
return np . array ( [ f . state_pt [ 1 ] for f in self ] ) \n 
~~ def correctedCoordinates ( self ) : \n 
return np . array ( [ f . state_pt for f in self ] ) \n 
~~ def drawCorrectedPath ( self , color = Color . GREEN , thickness = 2 ) : \n 
~~~ img . drawLine ( ( ts [ i ] . state_pt ) , ( ts [ i + 1 ] . state_pt ) , color = color , thickness = thickness ) \n 
~~ ~~ ~~ \n 
print __doc__ \n 
import gtk \n 
class app ( gtk . Window ) : \n 
~~~ edge_threshold = 100 \n 
max_threshold = 500 \n 
min_threshold = 0 \n 
window_width = 500 \n 
window_height = 500 \n 
#Variables \n 
current_image = None \n 
~~~ super ( app , self ) . __init__ ( ) \n 
self . set_position ( gtk . WIN_POS_CENTER ) \n 
self . set_decorated ( True ) \n 
self . set_has_frame ( False ) \n 
self . set_resizable ( False ) \n 
self . set_default_size ( self . window_width , self . window_height ) \n 
self . connect ( "destroy" , gtk . main_quit ) \n 
vbox = gtk . VBox ( spacing = 4 ) \n 
scale = gtk . HScale ( ) \n 
scale . set_range ( self . min_threshold , self . max_threshold ) \n 
scale . set_size_request ( 500 , 25 ) \n 
scale . set_value ( ( self . max_threshold + self . min_threshold ) / 2 ) \n 
scale . connect ( "value-changed" , self . update_threshold ) \n 
vbox . add ( scale ) \n 
info = gtk . Label ( ) \n 
vbox . add ( info ) \n 
new_image = self . process_image ( ) \n 
converted_image = gtk . gdk . pixbuf_new_from_array ( new_image , gtk . gdk . COLORSPACE_RGB , 8 ) \n 
image = gtk . Image ( ) \n 
image . set_from_pixbuf ( converted_image ) \n 
image . show ( ) \n 
vbox . add ( image ) \n 
self . current_image = image \n 
self . add ( vbox ) \n 
self . show_all ( ) \n 
def process_image ( self ) : \n 
~~~ img = SimpleCV . Image ( ) . rotate90 ( ) \n 
edges = img . edges ( self . edge_threshold ) \n 
numpy_img = edges . getNumpy ( ) \n 
return numpy_img \n 
~~ def update_threshold ( self , w ) : \n 
~~~ self . edge_threshold = w . get_value ( ) \n 
updated_image = self . process_image ( ) \n 
converted_image = gtk . gdk . pixbuf_new_from_array ( updated_image , gtk . gdk . COLORSPACE_RGB , 8 ) \n 
self . current_image . set_from_pixbuf ( converted_image ) \n 
~~ ~~ \n 
program1 = app ( ) \n 
program2 = app ( ) \n 
gtk . main ( ) \n 
from SimpleCV import Camera , VideoStream , Color , Display \n 
fname = \n 
outname = \n 
tags = \n 
vs = VideoStream ( fps = 20 , filename = fname , framefill = False ) \n 
cam = Camera ( ) \n 
disp = Display ( ( 800 , 600 ) ) \n 
while disp . isNotDone ( ) : \n 
~~~ img = cam . getImage ( ) \n 
img = img . edges ( ) \n 
vs . writeFrame ( img ) \n 
call ( + params , shell = True ) \n 
import cv \n 
cv . NamedWindow ( ) \n 
ind = 0 \n 
print ( % __doc__ ) \n 
def get_depth ( ind ) : \n 
~~~ return frame_convert . pretty_depth_cv ( freenect . sync_get_depth ( ind ) [ 0 ] ) \n 
~~ def get_video ( ind ) : \n 
~~~ return frame_convert . video_cv ( freenect . sync_get_video ( ind ) [ 0 ] ) \n 
~~ while 1 : \n 
~~~ print ( ind ) \n 
~~~ depth = get_depth ( ind ) \n 
video = get_video ( ind ) \n 
~~~ ind = 0 \n 
~~ ind += 1 \n 
cv . ShowImage ( , depth ) \n 
cv . ShowImage ( , video ) \n 
if cv . WaitKey ( 10 ) == 27 : \n 
from sklearn import svm \n 
from sklearn import linear_model \n 
from sklearn . neighbors . nearest_centroid import NearestCentroid \n 
from Modules . ConsoleOutput import ConsoleOutput \n 
from Modules . NaturalLanguage import NaturalLanguageObject \n 
_MAX_DECIMAL_PLACES = 10 \n 
class NNSentenceStructure : \n 
~~~ trainingData = [ ] \n 
trainingDataResults = [ ] \n 
clf = None \n 
def loadVectorsIntoNetwork ( self , inNormalisedData , targetResult ) : \n 
~~~ self . trainingData . extend ( inNormalisedData ) \n 
self . trainingDataResults . extend ( targetResult ) \n 
~~ def FitNetwork ( self ) : \n 
~~~ countItems = len ( self . trainingDataResults ) \n 
self . _fit ( self . trainingData , self . trainingDataResults ) \n 
self . trainingData = None \n 
self . trainingDataResults = None \n 
~~ def _fit ( self , dataVector , targetVector ) : \n 
self . clf . fit ( np . asarray ( dataVector , dtype = "float" ) , np . asarray ( targetVector , dtype = "float" ) ) \n 
~~ def getPrediction ( self , inNormalisedData ) : \n 
~~~ pred = self . clf . predict ( inNormalisedData ) \n 
return float ( round ( pred [ 0 ] , _MAX_DECIMAL_PLACES ) ) \n 
~~ def getPredictionProbability ( self , inNormalisedData ) : \n 
~~~ predProb = self . clf . predict_proba ( inNormalisedData ) \n 
return predProb \n 
~~~ self . clf = KNeighborsClassifier ( ) \n 
~~ ~~ class NNVocabulary : \n 
_Networks = [ ] \n 
_Vocabulary = None \n 
def loadVectorsIntoNetworkByIndex ( self , index , inNormalisedData , targetResult ) : \n 
~~~ self . trainingData [ index ] . append ( [ inNormalisedData ] ) \n 
self . trainingDataResults [ index ] . append ( targetResult ) \n 
~~ def loadVocab ( self , index , inNormal , inResult ) : \n 
~~~ self . _Vocabulary [ index ] . append ( ( inNormal , inResult ) ) \n 
~~ def _getFromVocab ( self , inIdentifier , inNormal ) : \n 
~~~ for index , i in enumerate ( NaturalLanguageObject . _Identifiers ) : \n 
~~~ if ( inIdentifier == i ) : \n 
~~~ for index2 , val in enumerate ( self . _Vocabulary [ index ] ) : \n 
~~~ if ( val [ 0 ] == inNormal ) : \n 
~~~ return val [ 1 ] \n 
~~ ~~ ~~ ~~ ~~ def FitNetwork ( self ) : \n 
~~~ countItems = 0 \n 
for index , val in enumerate ( self . trainingData ) : \n 
~~~ if ( len ( self . trainingData [ index ] ) > 0 ) : \n 
~~~ self . _fit ( index , self . trainingData [ index ] , self . trainingDataResults [ index ] ) \n 
countItems = countItems + len ( self . trainingData [ index ] ) \n 
print ( "\\n" ) \n 
~~ def _fit ( self , index , dataVector , targetVector ) : \n 
~~~ self . _Networks [ index ] . fit ( np . asarray ( dataVector , dtype = "float" ) , np . asarray ( targetVector , dtype \n 
~~ def getPrediction ( self , inNormalisedData , inIdentifier ) : \n 
~~~ pred = 0 \n 
for index , i in enumerate ( NaturalLanguageObject . _Identifiers ) : \n 
~~~ if ( len ( self . _Vocabulary [ index ] ) > 0 ) : \n 
~~~ pred = self . _Networks [ index ] . predict ( inNormalisedData ) \n 
~~~ return 0 \n 
~~ ~~ ~~ if ( pred == 0 ) : \n 
~~ return float ( round ( pred [ 0 ] , _MAX_DECIMAL_PLACES ) ) \n 
~~ def getPredictionProbability ( self , inNormalisedData , inIdentifier ) : \n 
~~~ pred = [ [ 0 ] ] \n 
~~~ return self . _Networks [ index ] . predict_proba ( inNormalisedData ) \n 
~~ ~~ ~~ return pred \n 
~~ def getPredictedWord ( self , inNormalisedData , inIdentifier ) : \n 
~~~ pred = self . getPrediction ( inNormalisedData , inIdentifier ) \n 
return self . _getFromVocab ( inIdentifier , pred ) \n 
~~~ for index in range ( 0 , len ( NaturalLanguageObject . _Identifiers ) ) : \n 
~~~ nn = KNeighborsClassifier ( ) \n 
self . _Networks . append ( nn ) \n 
~~ self . trainingData = [ list ( ) for _ in range ( len ( NaturalLanguageObject . _Identifiers ) ) ] \n 
self . trainingDataResults = [ list ( ) for _ in range ( len ( NaturalLanguageObject . _Identifiers ) ) ] \n 
self . _Vocabulary = [ list ( ) for _ in range ( len ( NaturalLanguageObject . _Identifiers ) ) ] \n 
from sklearn import neighbors , datasets , linear_model \n 
cmap_light = ListedColormap ( [ , , ] ) \n 
cmap_bold = ListedColormap ( [ , , ] ) \n 
def plot_iris_knn ( ) : \n 
~~~ iris = datasets . load_iris ( ) \n 
y = iris . target \n 
knn = neighbors . KNeighborsClassifier ( n_neighbors = 3 ) \n 
knn . fit ( X , y ) \n 
x_min , x_max = X [ : , 0 ] . min ( ) - .1 , X [ : , 0 ] . max ( ) + .1 \n 
y_min , y_max = X [ : , 1 ] . min ( ) - .1 , X [ : , 1 ] . max ( ) + .1 \n 
xx , yy = np . meshgrid ( np . linspace ( x_min , x_max , 100 ) , \n 
np . linspace ( y_min , y_max , 100 ) ) \n 
Z = knn . predict ( np . c_ [ xx . ravel ( ) , yy . ravel ( ) ] ) \n 
Z = Z . reshape ( xx . shape ) \n 
pl . pcolormesh ( xx , yy , Z , cmap = cmap_light ) \n 
pl . scatter ( X [ : , 0 ] , X [ : , 1 ] , c = y , cmap = cmap_bold ) \n 
pl . xlabel ( ) \n 
pl . ylabel ( ) \n 
pl . axis ( ) \n 
~~ def plot_polynomial_regression ( ) : \n 
~~~ rng = np . random . RandomState ( 0 ) \n 
x = 2 * rng . rand ( 100 ) - 1 \n 
f = lambda t : 1.2 * t ** 2 + .1 * t ** 3 - .4 * t ** 5 - .5 * t ** 9 \n 
y = f ( x ) + .4 * rng . normal ( size = 100 ) \n 
x_test = np . linspace ( - 1 , 1 , 100 ) \n 
pl . scatter ( x , y , s = 4 ) \n 
X = np . array ( [ x ** i for i in range ( 5 ) ] ) . T \n 
X_test = np . array ( [ x_test ** i for i in range ( 5 ) ] ) . T \n 
regr = linear_model . LinearRegression ( ) \n 
regr . fit ( X , y ) \n 
pl . plot ( x_test , regr . predict ( X_test ) , label = ) \n 
X = np . array ( [ x ** i for i in range ( 10 ) ] ) . T \n 
X_test = np . array ( [ x_test ** i for i in range ( 10 ) ] ) . T \n 
pl . legend ( loc = ) \n 
pl . title ( ) \n 
pl . plot ( x_test , f ( x_test ) , label = "truth" ) \n 
~~ from sklearn . datasets import load_iris \n 
labels = data . target_names [ data . target ] \n 
best_acc = - 1.0 \n 
for fi in range ( features . shape [ 1 ] ) : \n 
~~~ thresh = features [ : , fi ] . copy ( ) \n 
thresh . sort ( ) \n 
for t in thresh : \n 
~~~ pred = ( features [ : , fi ] > t ) \n 
acc = ( pred == is_virginica ) . mean ( ) \n 
acc_neg = ( ( ~ pred ) == is_virginica ) . mean ( ) \n 
if acc_neg > acc : \n 
~~~ acc = acc_neg \n 
negated = True \n 
~~~ negated = False \n 
~~ if acc > best_acc : \n 
~~~ best_acc = acc \n 
best_fi = fi \n 
best_t = t \n 
best_is_negated = negated \n 
~~ ~~ ~~ print ( . format best_t , data . feature_names [ best_fi ] , best_fi , best_acc ) ) \n 
import collections \n 
DATA_DIR = "data" \n 
CHART_DIR = "charts" \n 
if not os . path . exists ( DATA_DIR ) : \n 
~~ if not os . path . exists ( CHART_DIR ) : \n 
~~~ os . mkdir ( CHART_DIR ) \n 
~~ def tweak_labels ( Y , pos_sent_list ) : \n 
~~~ pos = Y == pos_sent_list [ 0 ] \n 
for sent_label in pos_sent_list [ 1 : ] : \n 
~~~ pos |= Y == sent_label \n 
~~ Y = np . zeros ( Y . shape [ 0 ] ) \n 
Y [ pos ] = 1 \n 
Y = Y . astype ( int ) \n 
~~ def load_sanders_data ( dirname = "." , line_count = - 1 ) : \n 
topics = [ ] \n 
tweets = [ ] \n 
with open ( os . path . join ( DATA_DIR , dirname , "corpus.csv" ) , "r" ) as csvfile : \n 
~~~ metareader = csv . reader ( csvfile , delimiter = , quotechar = \'"\' ) \n 
for line in metareader : \n 
~~~ count += 1 \n 
if line_count > 0 and count > line_count : \n 
~~ topic , label , tweet_id = line \n 
tweet_fn = os . path . join ( \n 
DATA_DIR , dirname , , % tweet_id ) \n 
~~~ tweet = json . load ( open ( tweet_fn , "r" ) ) \n 
~~ except IOError : \n 
~~ if in tweet and tweet [ ] [ ] == "en" : \n 
~~~ topics . append ( topic ) \n 
labels . append ( label ) \n 
tweets . append ( tweet [ ] ) \n 
~~ ~~ ~~ tweets = np . asarray ( tweets ) \n 
labels = np . asarray ( labels ) \n 
return tweets , labels \n 
~~ def plot_pr ( auc_score , name , phase , precision , recall , label = None ) : \n 
pylab . fill_between ( recall , precision , alpha = 0.5 ) \n 
pylab . plot ( recall , precision , lw = 1 ) \n 
pylab . xlim ( [ 0.0 , 1.0 ] ) \n 
pylab . ylim ( [ 0.0 , 1.0 ] ) \n 
pylab . xlabel ( ) \n 
pylab . ylabel ( ) \n 
pylab . title ( % ( auc_score , label ) ) \n 
pylab . savefig ( os . path . join ( CHART_DIR , "pr_%s_%s.png" % \n 
( filename , phase ) ) , bbox_inches = "tight" ) \n 
~~ def show_most_informative_features ( vectorizer , clf , n = 20 ) : \n 
~~~ c_f = sorted ( zip ( clf . coef_ [ 0 ] , vectorizer . get_feature_names ( ) ) ) \n 
top = list ( zip ( c_f [ : n ] , c_f [ : - ( n + 1 ) : - 1 ] ) ) \n 
for ( c1 , f1 ) , ( c2 , f2 ) in top : \n 
~~~ print ( "\\t%.4f\\t%-15s\\t\\t%.4f\\t%-15s" % ( c1 , f1 , c2 , f2 ) ) \n 
~~ ~~ def plot_log ( ) : \n 
pylab . figure ( num = None , figsize = ( 6 , 5 ) ) \n 
y = np . log ( x ) \n 
pylab . title ( ) \n 
filename = \n 
~~ def plot_feat_importance ( feature_names , clf , name ) : \n 
coef_ = clf . coef_ \n 
important = np . argsort ( np . absolute ( coef_ . ravel ( ) ) ) \n 
f_imp = feature_names [ important ] \n 
coef = coef_ . ravel ( ) [ important ] \n 
inds = np . argsort ( coef ) \n 
f_imp = f_imp [ inds ] \n 
coef = coef [ inds ] \n 
xpos = np . array ( list ( range ( len ( coef ) ) ) ) \n 
pylab . bar ( xpos , coef , width = 1 ) \n 
pylab . title ( % ( name ) ) \n 
ax = pylab . gca ( ) \n 
ax . set_xticks ( np . arange ( len ( coef ) ) ) \n 
labels = ax . set_xticklabels ( f_imp ) \n 
~~~ label . set_rotation ( 90 ) \n 
pylab . savefig ( os . path . join ( \n 
CHART_DIR , "feat_imp_%s.png" % filename ) , bbox_inches = "tight" ) \n 
~~ def plot_feat_hist ( data_name_list , filename = None ) : \n 
num_rows = 1 + ( len ( data_name_list ) - 1 ) / 2 \n 
num_cols = 1 if len ( data_name_list ) == 1 else 2 \n 
pylab . figure ( figsize = ( 5 * num_cols , 4 * num_rows ) ) \n 
for i in range ( num_rows ) : \n 
~~~ for j in range ( num_cols ) : \n 
~~~ pylab . subplot ( num_rows , num_cols , 1 + i * num_cols + j ) \n 
x , name = data_name_list [ i * num_cols + j ] \n 
pylab . title ( name ) \n 
max_val = np . max ( x ) \n 
if max_val <= 1.0 : \n 
~~~ bins = 50 \n 
~~ elif max_val > 50 : \n 
~~~ bins = max_val \n 
~~ n , bins , patches = pylab . hist ( \n 
x , bins = bins , normed = 1 , facecolor = , alpha = 0.75 ) \n 
~~ ~~ if not filename : \n 
~~~ filename = "feat_hist_%s.png" % name \n 
~~ pylab . savefig ( os . path . join ( CHART_DIR , filename ) , bbox_inches = "tight" ) \n 
~~ def plot_bias_variance ( data_sizes , train_errors , test_errors , name ) : \n 
pylab . plot ( \n 
data_sizes , train_errors , "-" , data_sizes , test_errors , "--" , lw = 1 ) \n 
pylab . grid ( ) \n 
pylab . savefig ( os . path . join ( CHART_DIR , "bv_" + name + ".png" ) ) \n 
~~ def load_sent_word_net ( ) : \n 
~~~ sent_scores = collections . defaultdict ( list ) \n 
sentiwordnet_path = os . path . join ( DATA_DIR , "SentiWordNet_3.0.0_20130122.txt" ) \n 
if not os . path . exists ( sentiwordnet_path ) : \n 
~~ with open ( sentiwordnet_path , ) as csvfile : \n 
~~~ reader = csv . reader ( csvfile , delimiter = , quotechar = \'"\' ) \n 
for line in reader : \n 
~~~ if line [ 0 ] . startswith ( "#" ) : \n 
~~ if len ( line ) == 1 : \n 
~~ POS , ID , PosScore , NegScore , SynsetTerms , Gloss = line \n 
if len ( POS ) == 0 or len ( ID ) == 0 : \n 
~~~ term = term . split ( "#" ) [ 0 ] \n 
key = "%s/%s" % ( POS , term . split ( "#" ) [ 0 ] ) \n 
sent_scores [ key ] . append ( ( float ( PosScore ) , float ( NegScore ) ) ) \n 
~~ ~~ ~~ for key , value in sent_scores . items ( ) : \n 
~~~ sent_scores [ key ] = np . mean ( value , axis = 0 ) \n 
~~ return sent_scores \n 
~~ def log_false_positives ( clf , X , y , name ) : \n 
~~~ false_positive = clf . predict ( X ) != y \n 
for tweet , false_class in zip ( X [ false_positive ] , y [ false_positive ] ) : \n 
~~~ f . write ( "%s\\t%s\\n" % \n 
( false_class , tweet . encode ( "ascii" , "ignore" ) ) ) \n 
~~ ~~ ~~ if __name__ == : \n 
~~~ plot_log ( ) \n 
~~ ~~ GENRE_DIR = None \n 
GENRE_LIST = [ "classical" , "jazz" , "country" , "pop" , "rock" , "metal" ] \n 
TEST_DIR = None \n 
if GENRE_DIR is None or TEST_DIR is None : \n 
~~ def plot_confusion_matrix ( cm , genre_list , name , title ) : \n 
pylab . matshow ( cm , fignum = False , cmap = , vmin = 0 , vmax = 1.0 ) \n 
ax = pylab . axes ( ) \n 
ax . set_xticks ( range ( len ( genre_list ) ) ) \n 
ax . set_xticklabels ( genre_list ) \n 
ax . xaxis . set_ticks_position ( "bottom" ) \n 
ax . set_yticks ( range ( len ( genre_list ) ) ) \n 
ax . set_yticklabels ( genre_list ) \n 
pylab . colorbar ( ) \n 
pylab . grid ( False ) \n 
pylab . show ( ) \n 
pylab . savefig ( \n 
os . path . join ( CHART_DIR , "confusion_matrix_%s.png" % name ) , bbox_inches = "tight" ) \n 
~~ def plot_pr ( auc_score , name , precision , recall , label = None ) : \n 
os . path . join ( CHART_DIR , "pr_" + filename + ".png" ) , bbox_inches = "tight" ) \n 
~~ def plot_roc ( auc_score , name , tpr , fpr , label = None ) : \n 
pylab . plot ( [ 0 , 1 ] , [ 0 , 1 ] , ) \n 
pylab . plot ( fpr , tpr ) \n 
pylab . fill_between ( fpr , tpr , alpha = 0.5 ) \n 
pylab . title ( % \n 
( auc_score , label ) , verticalalignment = "bottom" ) \n 
os . path . join ( CHART_DIR , "roc_" + filename + ".png" ) , bbox_inches = "tight" ) \n 
top = zip ( c_f [ : n ] , c_f [ : - ( n + 1 ) : - 1 ] ) \n 
xpos = np . array ( range ( len ( coef ) ) ) \n 
~~ import matplotlib . pyplot as plt \n 
from numpy import * \n 
def loadDataSet ( fileName ) : \n 
~~~ dataMat = [ ] ; labelMat = [ ] \n 
fr = open ( fileName ) \n 
for line in fr . readlines ( ) : \n 
~~~ lineArr = line . strip ( ) . split ( ) \n 
dataMat . append ( [ float ( lineArr [ 0 ] ) , float ( lineArr [ 1 ] ) ] ) \n 
labelMat . append ( float ( lineArr [ 2 ] ) ) \n 
~~ return dataMat , labelMat \n 
~~ def selectJrand ( i , m ) : \n 
while ( j == i ) : \n 
~~~ j = int ( random . uniform ( 0 , m ) ) \n 
~~ return j \n 
~~ def clipAlpha ( aj , H , L ) : \n 
~~~ if aj > H : \n 
~~~ aj = H \n 
~~ if L > aj : \n 
~~~ aj = L \n 
~~ return aj \n 
~~ def smoSimple ( dataMatIn , classLabels , C , toler , maxIter ) : \n 
~~~ dataMatrix = mat ( dataMatIn ) ; labelMat = mat ( classLabels ) . transpose ( ) \n 
b = 0 ; m , n = shape ( dataMatrix ) \n 
alphas = mat ( zeros ( ( m , 1 ) ) ) \n 
iter = 0 \n 
while ( iter < maxIter ) : \n 
~~~ alphaPairsChanged = 0 \n 
for i in range ( m ) : \n 
~~~ fXi = float ( multiply ( alphas , labelMat ) . T * ( dataMatrix * dataMatrix [ i , : ] . T ) ) + b \n 
if ( ( labelMat [ i ] * Ei < - toler ) and ( alphas [ i ] < C ) ) or ( ( labelMat [ i ] * Ei > toler ) and ( alphas ~~~ j = selectJrand ( i , m ) \n 
fXj = float ( multiply ( alphas , labelMat ) . T * ( dataMatrix * dataMatrix [ j , : ] . T ) ) + b \n 
Ej = fXj - float ( labelMat [ j ] ) \n 
alphaIold = alphas [ i ] . copy ( ) ; alphaJold = alphas [ j ] . copy ( ) ; \n 
if ( labelMat [ i ] != labelMat [ j ] ) : \n 
~~~ L = max ( 0 , alphas [ j ] - alphas [ i ] ) \n 
H = min ( C , C + alphas [ j ] - alphas [ i ] ) \n 
~~~ L = max ( 0 , alphas [ j ] + alphas [ i ] - C ) \n 
H = min ( C , alphas [ j ] + alphas [ i ] ) \n 
~~ eta = 2.0 * dataMatrix [ i , : ] * dataMatrix [ j , : ] . T - dataMatrix [ i , : ] * dataMatrix [ i , : ] . T - if eta >= 0 : print "eta>=0" ; continue \n 
alphas [ j ] -= labelMat [ j ] * ( Ei - Ej ) / eta \n 
alphas [ j ] = clipAlpha ( alphas [ j ] , H , L ) \n 
elif ( 0 < alphas [ j ] ) and ( C > alphas [ j ] ) : b = b2 \n 
else : b = ( b1 + b2 ) / 2.0 \n 
alphaPairsChanged += 1 \n 
~~ ~~ if ( alphaPairsChanged == 0 ) : iter += 1 \n 
else : iter = 0 \n 
~~ return b , alphas \n 
~~ def matplot ( dataMat , lableMat ) : \n 
~~~ xcord1 = [ ] ; ycord1 = [ ] \n 
xcord2 = [ ] ; ycord2 = [ ] \n 
xcord3 = [ ] ; ycord3 = [ ] \n 
~~~ if lableMat [ i ] == 1 : \n 
~~~ xcord1 . append ( dataMat [ i ] [ 0 ] ) \n 
ycord1 . append ( dataMat [ i ] [ 1 ] ) \n 
~~~ xcord2 . append ( dataMat [ i ] [ 0 ] ) \n 
ycord2 . append ( dataMat [ i ] [ 1 ] ) \n 
~~ ~~ b , alphas = smoSimple ( dataMat , labelMat , 0.6 , 0.001 , 40 ) \n 
for j in range ( 100 ) : \n 
~~~ if alphas [ j ] > 0 : \n 
~~~ xcord3 . append ( dataMat [ j ] [ 0 ] ) \n 
ycord3 . append ( dataMat [ j ] [ 1 ] ) \n 
~~ ~~ fig = plt . figure ( ) \n 
ax . scatter ( xcord1 , ycord1 , s = 30 , c = , marker = ) \n 
ax . scatter ( xcord2 , ycord2 , s = 30 , c = ) \n 
ax . scatter ( xcord3 , ycord3 , s = 80 , c = ) \n 
ax . plot ( ) \n 
plt . xlabel ( ) ; plt . ylabel ( ) ; \n 
~~~ dataMat , labelMat = loadDataSet ( ) \n 
~~ CHART_DIR = os . path . join ( \n 
if not os . path . exists ( CHART_DIR ) : \n 
from sklearn . metrics import mean_squared_error , r2_score \n 
from sklearn . datasets import load_svmlight_file \n 
from sklearn . linear_model import LinearRegression \n 
data , target = load_svmlight_file ( ) \n 
lr = LinearRegression ( ) \n 
lr . fit ( data , target ) \n 
pred = lr . predict ( data ) \n 
print ( . format ( np . sqrt ( mean_squared_error ( target , pred ) ) ) ) \n 
print ( . format ( r2_score ( target , pred ) ) ) \n 
pred = np . zeros_like ( target ) \n 
kf = KFold ( len ( target ) , n_folds = 5 ) \n 
for train , test in kf : \n 
~~~ lr . fit ( data [ train ] , target [ train ] ) \n 
pred [ test ] = lr . predict ( data [ test ] ) \n 
~~ print ( . format ( np . sqrt ( mean_squared_error ( target , pred ) ) ) ) \n 
import mahotas as mh \n 
from glob import glob \n 
from features import texture , chist \n 
from sklearn . linear_model import LogisticRegression \n 
basedir = \n 
haralicks = [ ] \n 
chists = [ ] \n 
images = glob ( . format ( basedir ) ) \n 
for fname in sorted ( images ) : \n 
~~~ imc = mh . imread ( fname ) \n 
haralicks . append ( texture ( mh . colors . rgb2grey ( imc ) ) ) \n 
chists . append ( chist ( imc ) ) \n 
labels . append ( fname [ : - len ( ) ] ) \n 
haralicks = np . array ( haralicks ) \n 
chists = np . array ( chists ) \n 
haralick_plus_chists = np . hstack ( [ chists , haralicks ] ) \n 
clf = Pipeline ( [ ( , StandardScaler ( ) ) , \n 
( , LogisticRegression ( ) ) ] ) \n 
from sklearn import cross_validation \n 
cv = cross_validation . LeaveOneOut ( len ( images ) ) \n 
scores = cross_validation . cross_val_score ( \n 
clf , haralicks , labels , cv = cv ) \n 
scores . mean ( ) ) ) \n 
clf , chists , labels , cv = cv ) \n 
clf , haralick_plus_chists , labels , cv = cv ) \n 
print ( scores . mean ( ) ) ) \n 
def tfidf ( t , d , D ) : \n 
~~~ tf = float ( d . count ( t ) ) / sum ( d . count ( w ) for w in set ( d ) ) \n 
idf = sp . log ( float ( len ( D ) ) / ( len ( [ doc for doc in D if t in doc ] ) ) ) \n 
return tf * idf \n 
~~ a , abb , abc = [ "a" ] , [ "a" , "b" , "b" ] , [ "a" , "b" , "c" ] \n 
D = [ a , abb , abc ] \n 
print ( tfidf ( "a" , a , D ) ) \n 
print ( tfidf ( "b" , abb , D ) ) \n 
print ( tfidf ( "a" , abc , D ) ) \n 
print ( tfidf ( "b" , abc , D ) ) \n 
print ( tfidf ( "c" , abc , D ) ) \n 
from sklearn . linear_model import Lasso \n 
from sklearn . datasets import load_boston \n 
boston = load_boston ( ) \n 
x = boston . data \n 
y = boston . target \n 
las = Lasso ( normalize = 1 ) \n 
alphas = np . logspace ( - 5 , 2 , 1000 ) \n 
alphas , coefs , _ = las . path ( x , y , alphas = alphas ) \n 
ax . plot ( alphas , coefs . T ) \n 
ax . set_xscale ( ) \n 
ax . set_xlim ( alphas . max ( ) , alphas . min ( ) ) \n 
ax . set_xlabel ( ) \n 
ax . set_ylabel ( ) \n 
from scipy . spatial import distance \n 
images . sort ( ) \n 
for fname in images : \n 
imc = imc [ 200 : - 200 , 200 : - 200 ] \n 
~~ haralicks = np . array ( haralicks ) \n 
features = np . hstack ( [ chists , haralicks ] ) \n 
sc = StandardScaler ( ) \n 
features = sc . fit_transform ( features ) \n 
dists = distance . squareform ( distance . pdist ( features ) ) \n 
fig , axes = plt . subplots ( 2 , 9 , figsize = ( 16 , 8 ) ) \n 
for ax in axes . flat : \n 
~~~ ax . set_xticks ( [ ] ) \n 
ax . set_yticks ( [ ] ) \n 
~~ for ci , i in enumerate ( range ( 0 , 90 , 10 ) ) : \n 
~~~ left = images [ i ] \n 
dists_left = dists [ i ] \n 
right = dists_left . argsort ( ) \n 
right = right [ 1 ] \n 
right = images [ right ] \n 
left = mh . imread ( left ) \n 
right = mh . imread ( right ) \n 
axes [ 0 , ci ] . imshow ( left ) \n 
axes [ 1 , ci ] . imshow ( right ) \n 
~~ fig . tight_layout ( ) \n 
fig . savefig ( , dpi = 300 ) \n 
features = data [ ] \n 
target = data [ ] \n 
target_names = data [ ] \n 
plength = features [ : , 2 ] \n 
max_setosa = plength [ is_setosa ] . max ( ) \n 
min_non_setosa = plength [ ~ is_setosa ] . min ( ) \n 
print ( . format ( max_setosa ) ) \n 
print ( . format ( min_non_setosa ) ) \n 
CONSUMER_KEY = None \n 
CONSUMER_SECRET = None \n 
ACCESS_TOKEN_KEY = None \n 
ACCESS_TOKEN_SECRET = None \n 
import scipy \n 
import scipy . io . wavfile \n 
from utils import GENRE_DIR , CHART_DIR \n 
from matplotlib . ticker import EngFormatter \n 
def write_fft ( fft_features , fn ) : \n 
base_fn , ext = os . path . splitext ( fn ) \n 
data_fn = base_fn + ".fft" \n 
np . save ( data_fn , fft_features ) \n 
~~ def create_fft ( fn ) : \n 
~~~ sample_rate , X = scipy . io . wavfile . read ( fn ) \n 
fft_features = abs ( scipy . fft ( X ) [ : 1000 ] ) \n 
write_fft ( fft_features , fn ) \n 
~~ def read_fft ( genre_list , base_dir = GENRE_DIR ) : \n 
~~~ X = [ ] \n 
y = [ ] \n 
for label , genre in enumerate ( genre_list ) : \n 
~~~ genre_dir = os . path . join ( base_dir , genre , "*.fft.npy" ) \n 
file_list = glob . glob ( genre_dir ) \n 
assert ( file_list ) , genre_dir \n 
for fn in file_list : \n 
~~~ fft_features = np . load ( fn ) \n 
X . append ( fft_features [ : 2000 ] ) \n 
y . append ( label ) \n 
~~ ~~ return np . array ( X ) , np . array ( y ) \n 
~~ def plot_wav_fft ( wav_filename , desc = None ) : \n 
~~~ plt . clf ( ) \n 
plt . figure ( num = None , figsize = ( 6 , 4 ) ) \n 
sample_rate , X = scipy . io . wavfile . read ( wav_filename ) \n 
spectrum = np . fft . fft ( X ) \n 
freq = np . fft . fftfreq ( len ( X ) , 1.0 / sample_rate ) \n 
plt . subplot ( 211 ) \n 
num_samples = 200.0 \n 
plt . xlim ( 0 , num_samples / sample_rate ) \n 
plt . title ( desc or wav_filename ) \n 
plt . plot ( np . arange ( num_samples ) / sample_rate , X [ : num_samples ] ) \n 
plt . grid ( True ) \n 
plt . subplot ( 212 ) \n 
plt . xlim ( 0 , 5000 ) \n 
plt . xticks ( np . arange ( 5 ) * 1000 ) \n 
if desc : \n 
~~~ desc = desc . strip ( ) \n 
fft_desc = desc [ 0 ] . lower ( ) + desc [ 1 : ] \n 
~~~ fft_desc = wav_filename \n 
plt . plot ( freq , abs ( spectrum ) , linewidth = 5 ) \n 
rel_filename = os . path . split ( wav_filename ) [ 1 ] \n 
plt . savefig ( "%s_wav_fft.png" % os . path . splitext ( rel_filename ) [ 0 ] , \n 
bbox_inches = ) \n 
~~ def plot_wav_fft_demo ( ) : \n 
~~ def plot_specgram ( ax , fn ) : \n 
ax . specgram ( X , Fs = sample_rate , xextent = ( 0 , 30 ) ) \n 
~~ def plot_specgrams ( base_dir = CHART_DIR ) : \n 
plt . clf ( ) \n 
genres = [ "classical" , "jazz" , "country" , "pop" , "rock" , "metal" ] \n 
num_files = 3 \n 
f , axes = plt . subplots ( len ( genres ) , num_files ) \n 
for genre_idx , genre in enumerate ( genres ) : \n 
~~~ for idx , fn in enumerate ( glob . glob ( os . path . join ( GENRE_DIR , genre , "*.wav" ) ) ) : \n 
~~~ if idx == num_files : \n 
~~ axis = axes [ genre_idx , idx ] \n 
axis . yaxis . set_major_formatter ( EngFormatter ( ) ) \n 
plot_specgram ( axis , fn ) \n 
~~ ~~ specgram_file = os . path . join ( base_dir , "Spectrogram_Genres.png" ) \n 
plt . savefig ( specgram_file , bbox_inches = "tight" ) \n 
~~~ plot_wav_fft_demo ( ) \n 
~~ plot_specgrams ( ) \n 
~~ import timeit \n 
normal_py_sec = timeit . timeit ( , \n 
number = 10000 ) \n 
naive_np_sec = timeit . timeit ( , \n 
good_np_sec = timeit . timeit ( , \n 
import gensim \n 
logging . basicConfig ( \n 
format = , \n 
level = logging . INFO ) \n 
id2word = gensim . corpora . Dictionary . load_from_text ( \n 
mm = gensim . corpora . MmCorpus ( ) \n 
model = gensim . models . hdpmodel . HdpModel ( \n 
corpus = mm , \n 
id2word = id2word , \n 
chunksize = 10000 ) \n 
model . save ( ) \n 
topics = np . zeros ( ( len ( mm ) , model . num_topics ) ) \n 
for di , doc in enumerate ( mm ) : \n 
~~~ doc_top = model [ doc ] \n 
for ti , tv in doc_top : \n 
~~~ topics [ di , ti ] += tv \n 
~~ ~~ np . save ( , topics ) \n 
def apriori ( dataset , minsupport , maxsize ) : \n 
baskets = defaultdict ( list ) \n 
pointers = defaultdict ( list ) \n 
for i , ds in enumerate ( dataset ) : \n 
~~~ for ell in ds : \n 
~~~ pointers [ ell ] . append ( i ) \n 
baskets [ frozenset ( [ ell ] ) ] . append ( i ) \n 
~~ ~~ new_pointers = dict ( ) \n 
for k in pointers : \n 
~~~ if len ( pointers [ k ] ) >= minsupport : \n 
~~~ new_pointers [ k ] = frozenset ( pointers [ k ] ) \n 
~~ ~~ pointers = new_pointers \n 
for k in baskets : \n 
~~~ baskets [ k ] = frozenset ( baskets [ k ] ) \n 
~~ valid = set ( ) \n 
for el , c in baskets . items ( ) : \n 
~~~ if len ( c ) >= minsupport : \n 
~~~ valid . update ( el ) \n 
~~ ~~ itemsets = [ frozenset ( [ v ] ) for v in valid ] \n 
freqsets = [ ] \n 
for i in range ( maxsize - 1 ) : \n 
i , len ( itemsets ) ) ) \n 
newsets = [ ] \n 
for it in itemsets : \n 
~~~ ccounts = baskets [ it ] \n 
for v , pv in pointers . items ( ) : \n 
~~~ if v not in it : \n 
~~~ csup = ( ccounts & pv ) \n 
if len ( csup ) >= minsupport : \n 
~~~ new = frozenset ( it | frozenset ( [ v ] ) ) \n 
if new not in baskets : \n 
~~~ newsets . append ( new ) \n 
baskets [ new ] = csup \n 
~~ ~~ ~~ ~~ ~~ freqsets . extend ( itemsets ) \n 
itemsets = newsets \n 
if not len ( itemsets ) : \n 
~~ ~~ support = { } \n 
~~~ support [ k ] = float ( len ( baskets [ k ] ) ) \n 
~~ return freqsets , support \n 
~~ AssociationRule = namedtuple ( , [ , , , , \n 
def association_rules ( dataset , freqsets , support , minlift ) : \n 
nr_transactions = float ( len ( dataset ) ) \n 
freqsets = [ f for f in freqsets if len ( f ) > 1 ] \n 
for fset in freqsets : \n 
~~~ for f in fset : \n 
~~~ consequent = frozenset ( [ f ] ) \n 
antecendent = fset - consequent \n 
py_x = support [ fset ] / support [ antecendent ] \n 
base = support [ consequent ] / nr_transactions \n 
lift = py_x / base \n 
if lift > minlift : \n 
~~~ yield AssociationRule ( antecendent , consequent , base , py_x , lift ) \n 
~~ ~~ ~~ ~~ from sklearn . feature_selection import RFE \n 
from sklearn . datasets import make_classification \n 
X , y = make_classification ( \n 
n_samples = 100 , n_features = 10 , n_informative = 3 , random_state = 0 ) \n 
clf . fit ( X , y ) \n 
for i in range ( 1 , 11 ) : \n 
~~~ selector = RFE ( clf , i ) \n 
selector = selector . fit ( X , y ) \n 
print ( "%i\\t%s\\t%s" % ( i , selector . support_ , selector . ranking_ ) ) \n 
~~ from pycast . errors . baseerrormeasure import BaseErrorMeasure \n 
class MeanSquaredError ( BaseErrorMeasure ) : \n 
def _calculate ( self , startingPercentage , endPercentage , startDate , endDate ) : \n 
errorValues = self . _get_error_values ( startingPercentage , endPercentage , startDate , endDate ) \n 
return float ( sum ( errorValues ) ) / float ( len ( errorValues ) ) \n 
~~ def local_error ( self , originalValue , calculatedValue ) : \n 
originalValue = originalValue [ 0 ] \n 
calculatedValue = calculatedValue [ 0 ] \n 
return ( calculatedValue - originalValue ) ** 2.0 \n 
~~ ~~ MSE = MeanSquaredError \n 
from mock import patch \n 
from pycast . common . timeseries import TimeSeries \n 
from pycast . common . matrix import Matrix \n 
from pycast . methods . regression import Regression , LinearRegression \n 
class RegressionTest ( unittest . TestCase ) : \n 
def calculate_parameters_two_empty_list_test ( self ) : \n 
tsOne = TimeSeries . from_twodim_list ( [ ] ) \n 
tsTwo = TimeSeries . from_twodim_list ( [ ] ) \n 
reg = Regression ( ) \n 
self . assertRaises ( ValueError , reg . calculate_parameters , tsOne , tsTwo ) \n 
~~ def calculate_parameters_one_empty_list_test ( self ) : \n 
tsOne = TimeSeries . from_twodim_list ( [ [ 1 , 12.34 ] ] ) \n 
~~ def calculate_parameters_test ( self ) : \n 
data1 = [ \n 
[ 1 , 10.00 ] , [ 7 , 22.01 ] , [ 2 , 12.40 ] , [ 3 , 17.38 ] , [ 4 , 16.66 ] , \n 
[ 5 , 20.12 ] , [ 6 , 23.45 ] , [ 8 , 24.34 ] , [ 9 , 12.12 ] \n 
data2 = [ \n 
[ 1 , 13.00 ] , [ 2 , 15.40 ] , [ 3 , 15.38 ] , [ 4 , 16.32 ] , [ 10 , 10.12 ] , \n 
[ 5 , 18.14 ] , [ 6 , 20.05 ] , [ 7 , 21.01 ] , [ 8 , 25.34 ] \n 
tsSrc1 = TimeSeries . from_twodim_list ( data1 ) \n 
tsSrc2 = TimeSeries . from_twodim_list ( data2 ) \n 
expRes = ( 5.546941864140029 , 0.6850537379535376 ) \n 
res = reg . calculate_parameters ( tsSrc1 , tsSrc2 ) \n 
self . assertEquals ( res , expRes ) \n 
~~ def calculate_parameters_without_match_test ( self ) : \n 
data1 = [ [ 1 , 12.42 ] , [ 6 , 12.32 ] , [ 8 , 12.45 ] ] \n 
data2 = [ [ 2 , 32.45 ] , [ 4 , 23.12 ] , [ 7 , 65.34 ] ] \n 
tsOne = TimeSeries . from_twodim_list ( data1 ) \n 
tsTwo = TimeSeries . from_twodim_list ( data2 ) \n 
~~ def calculate_parameter_duplicate_dates_test ( self ) : \n 
data1 = [ [ 1 , 12.23 ] , [ 4 , 23.34 ] ] \n 
data2 = [ [ 1 , 34.23 ] , [ 1 , 16.23 ] ] \n 
self . assertRaises ( ValueError , reg . calculate_parameters , tsSrc1 , tsSrc2 ) \n 
~~ def calculate_parameter_with_short_timeseries_test ( self ) : \n 
data2 = [ [ 1 , 34.23 ] ] \n 
~~ def match_time_series_test ( self ) : \n 
data1 = [ [ 1 , 12.42 ] , [ 4 , 34.23 ] , [ 7 , 12.32 ] , [ 8 , 12.45 ] ] \n 
data2 = [ [ 2 , 32.45 ] , [ 7 , 65.34 ] , [ 4 , 23.12 ] , [ 5 , 32.45 ] ] \n 
dstX = [ [ 4 , 34.23 ] , [ 7 , 12.32 ] ] \n 
dstY = [ [ 4 , 23.12 ] , [ 7 , 65.34 ] ] \n 
tsX , tsY = reg . match_time_series ( tsSrc1 , tsSrc2 ) \n 
self . assertEqual ( tsX , dstX ) \n 
self . assertEqual ( tsY , dstY ) \n 
~~ def predict_test ( self ) : \n 
data1 = [ [ 1 , 1 ] , [ 2 , 2 ] , [ 3 , 3 ] ] \n 
data2 = [ [ 1 , 3 ] , [ 2 , 5 ] , [ 3 , 7 ] ] \n 
ts1 = TimeSeries . from_twodim_list ( data1 ) \n 
ts2 = TimeSeries . from_twodim_list ( data2 ) \n 
result = reg . predict ( ts1 , 1 , 2 ) \n 
self . assertEquals ( ts2 , result ) \n 
~~ def test_confidence_interval ( self ) : \n 
data_x = zip ( range ( 100 ) , range ( 100 ) ) \n 
overestimations = [ [ 90 , 90 - 1 ] , [ 91 , 91 - 3 ] , [ 92 , 92 - 1 ] , [ 93 , 93 - 40 ] , [ 94 , 94 - 1 ] ] \n 
underestimations = [ [ 95 , 95 + 5 ] , [ 96 , 96 + 1 ] , [ 97 , 97 + 4 ] , [ 98 , 98 + 3 ] , [ 99 , 99 + 1 ] ] \n 
data_y = data_x [ : 90 ] + overestimations + underestimations \n 
ts_x = TimeSeries . from_twodim_list ( data_x ) \n 
ts_y = TimeSeries . from_twodim_list ( data_y ) \n 
with patch ( ) as sample_mock : \n 
~~~ sample_mock . return_value = underestimations + overestimations \n 
n , m , error = reg . calculate_parameters_with_confidence ( ts_x , ts_y , .6 ) \n 
self . assertEquals ( 0 , n ) \n 
self . assertEquals ( 1 , m ) \n 
self . assertEquals ( error [ 0 ] , - 1 ) \n 
self . assertEquals ( error [ 1 ] , 3 ) \n 
~~ ~~ ~~ class LinearRegressionTest ( unittest . TestCase ) : \n 
~~~ precision = 4 \n 
def lstsq_test ( self ) : \n 
volumes = [ \n 
[ 24 ] , [ 20 ] , [ 20 ] , [ 20 ] , [ 21 ] , [ 30 ] , \n 
[ 40 ] , [ 20 ] , [ 20 ] , [ 20 ] , [ 19 ] , [ 35 ] \n 
promoted = [ \n 
[ 1 , 0 , 0 , 0 , 1 ] , \n 
[ 1 , 0 , 0 , 0 , 0 ] , \n 
[ 1 , 1 , 0 , 0 , 0 ] , \n 
[ 1 , 0 , 1 , 0 , 0 ] , \n 
[ 1 , 0 , 0 , 1 , 0 ] , \n 
[ 1 , 0 , 1 , 0 , 0 ] \n 
expRes = [ \n 
[ 19.999999999999993 ] , \n 
[ 8.8817841970012523e-15 ] , \n 
[ 12.500000000000011 ] , \n 
[ 20.000000000000007 ] , \n 
[ 4.0 ] \n 
volMatrix = Matrix ( 1 , 12 ) \n 
volMatrix . initialize ( volumes ) \n 
proMatrix = Matrix ( 5 , 12 ) \n 
proMatrix . initialize ( promoted ) \n 
resMatrix = LinearRegression . lstsq ( proMatrix , volMatrix ) \n 
for row in range ( resMatrix . get_height ( ) ) : \n 
~~~ for col in range ( resMatrix . get_width ( ) ) : \n 
~~~ self . assertAlmostEqual ( resMatrix . get_value ( col , row ) , expRes [ row ] [ col ] , self . precision \n 
~~ ~~ ~~ def lstsq_wrong_input_size_test ( self ) : \n 
volumes = [ [ 24 ] , [ 20 ] , [ 20 ] , [ 20 ] , [ 21 ] , [ 30 ] ] \n 
[ 1 , 0 , 0 , 0 , 0 ] \n 
volMatrix = Matrix ( 1 , 6 ) \n 
proMatrix = Matrix ( 5 , 2 ) \n 
self . assertRaises ( ValueError , LinearRegression . lstsq , proMatrix , volMatrix ) \n 
~~ def lstsq_matrix_test ( self ) : \n 
exRes = [ \n 
[ 2.00000000e+01 ] , \n 
[ 2.18197426e-16 ] , \n 
[ 1.25000000e+01 ] , \n 
[ 4.00000000e+00 ] \n 
res = LinearRegression . lstsq ( proMatrix , volMatrix ) \n 
for row in xrange ( len ( exRes ) ) : \n 
~~~ for col in xrange ( len ( exRes [ 0 ] ) ) : \n 
~~~ self . assertAlmostEqual ( exRes [ row ] [ col ] , res . get_value ( col , row ) , self . precision ) \n 
~~ ~~ ~~ def lstsq_value_error_test ( self ) : \n 
volumes = [ [ 23 , 34 ] , [ 12 , 34 ] , [ 14 , 54 ] ] \n 
volMatrix = Matrix ( 2 , 3 ) \n 
proMatrix = Matrix ( 5 , 3 ) \n 
~~ ~~ class _Getch : \n 
~~~ self . impl = _GetchWindows ( ) \n 
~~~ self . impl = _GetchUnix ( ) \n 
~~ ~~ def __call__ ( self ) : return self . impl ( ) \n 
~~ class _GetchUnix : \n 
~~~ import tty , sys \n 
~~ def __call__ ( self ) : \n 
~~~ import sys , tty , termios \n 
fd = sys . stdin . fileno ( ) \n 
old_settings = termios . tcgetattr ( fd ) \n 
~~~ tty . setraw ( sys . stdin . fileno ( ) ) \n 
ch = sys . stdin . read ( 1 ) \n 
~~~ termios . tcsetattr ( fd , termios . TCSADRAIN , old_settings ) \n 
~~ return ch \n 
~~ ~~ class _GetchWindows : \n 
~~~ import msvcrt \n 
return msvcrt . getch ( ) \n 
############################################################################ \n 
## \n 
~~ ~~ import core . modules \n 
import core . modules . module_registry \n 
from core . modules . vistrails_module import Module , ModuleError \n 
from SciPy import SciPy \n 
from Matrix import * \n 
from scipy import sparse , fftpack \n 
################################################################# \n 
class DSP ( SciPy ) : \n 
~~~ def compute ( self ) : \n 
~~ ~~ class FFT ( DSP ) : \n 
~~~ mat = self . get_input ( "Signals" ) \n 
phasors = fftpack . fft ( mat . matrix . data , pts ) \n 
outmat = sparse . csc_matrix ( phasors ) \n 
out = SparseMatrix ( ) \n 
out . matrix = outmat \n 
~~ ~~ class FFT2 ( DSP ) : \n 
phasors = fftpack . fftn ( mat . matrix . data ) \n 
~~ ~~ class WindowedFourierTransform ( DSP ) : \n 
~~~ mat = self . get_input ( "Signal" ) \n 
~~~ window = sr \n 
~~ if self . has_input ( "Stride" ) : \n 
~~~ stride = self . get_input ( "Stride" ) \n 
~~~ stride = int ( sr / 2 ) \n 
~~ signal_array = mat . matrix . toarray ( ) . ravel ( ) \n 
pad = signal_array [ 0 : int ( window / 2 ) ] \n 
signal_array = numpy . concatenate ( ( pad , signal_array ) ) \n 
win_low = 0 \n 
win_hi = window - 1 \n 
phasors = fftpack . fft ( signal_array [ win_low : win_hi ] ) \n 
out_array = phasors . ravel ( ) \n 
win_low += stride \n 
win_hi += stride \n 
while win_hi < signal_array . shape [ 0 ] : \n 
~~~ phasors = fftpack . fft ( signal_array [ win_low : win_hi ] ) \n 
out_array = numpy . vstack ( [ out_array , phasors . ravel ( ) ] ) \n 
~~ out = SparseMatrix ( ) \n 
out . matrix = sparse . csc_matrix ( out_array ) \n 
~~ ~~ class ShortTimeFourierTransform ( DSP ) : \n 
~~~ def get_signal ( self , sigs , window , offset , size ) : \n 
~~~ win = scipy . zeros ( sigs . shape [ 0 ] ) . ravel ( ) \n 
win [ offset : offset + size ] = window . ravel ( ) \n 
part = sigs * win \n 
return part \n 
~~ def compute ( self ) : \n 
if self . has_input ( "Window" ) : \n 
~~~ window = self . get_input ( "Window" ) . matrix . toarray ( ) \n 
win_size = window . shape [ 1 ] \n 
~~~ win_size = self . get_input ( "WindowSize" ) \n 
window = scipy . signal . hamming ( win_size ) \n 
~~~ stride = int ( win_size / 2 ) \n 
~~ signal_array = mat . matrix . transpose ( ) . toarray ( ) . ravel ( ) \n 
samples = signal_array . shape [ 0 ] \n 
offset = 0 \n 
sig = self . get_signal ( signal_array , window , offset , win_size ) \n 
phasors = fftpack . fft ( sig ) . ravel ( ) \n 
out_array = phasors \n 
offset += stride \n 
i = 1 \n 
while 1 : \n 
~~~ sig = self . get_signal ( signal_array , window , offset , win_size ) \n 
phasors = fftpack . fft ( sig ) \n 
~~ ~~ ( slices , freqs ) = out_array . shape \n 
ar = out_array [ 0 : , 0 : sr * 2 ] \n 
ar = ar [ 0 : , : : - 1 ] \n 
sigout = SparseMatrix ( ) \n 
sigout . matrix = sparse . csc_matrix ( signal_array ) \n 
out . matrix = sparse . csc_matrix ( ar ) \n 
############################################################################## \n 
import core . modules \n 
import core . modules . basic_modules as basic \n 
from core . modules . vistrails_module import Module , ModuleError , new_module , IncompleteImplementation \n 
_teemPath = None \n 
_teemLimnTestPath = None \n 
_teemTenTestPath = None \n 
identifier = \n 
version = \n 
############################################################################### \n 
FileType = [ ( basic . File , ) ] \n 
FloatType = [ ( basic . Float , ) ] \n 
IntegerType = [ ( basic . Integer , ) ] \n 
StringType = [ ( basic . String , ) ] \n 
Vec3Type = [ ( basic . Float , ) , \n 
( basic . Float , ) , \n 
( basic . Float , ) ] \n 
class Teem ( Module ) : \n 
~~~ def run_at_path ( self , cmdline , path ) : \n 
~~~ cmdline = path + cmdline \n 
print cmdline \n 
result = os . system ( cmdline ) \n 
if result != 0 : \n 
~~ ~~ def run_path ( self , * cmds ) : \n 
self . run_at_path ( cmdline , ) \n 
~~ def run_teem ( self , * cmds ) : \n 
self . run_at_path ( cmdline , _teemPath ) \n 
~~ def run_limn_test ( self , * cmds ) : \n 
self . run_at_path ( cmdline , _teemLimnTestPath ) \n 
~~ def run_ten_test ( self , * cmds ) : \n 
self . run_at_path ( cmdline , _teemTenTestPath ) \n 
~~ def opt_command_line_val ( self , option_name , port_name ) : \n 
~~~ if self . has_input ( port_name ) : \n 
~~~ return [ option_name , self . get_input ( port_name ) ] \n 
~~ ~~ def opt_command_line_vec ( self , option_name , port_name ) : \n 
~~~ t = self . get_input ( port_name ) \n 
return [ option_name ] + list ( t ) \n 
~~ ~~ def opt_command_line_file ( self , option_name , port_name ) : \n 
return [ option_name , t . name ] \n 
~~ ~~ def opt_command_line_noopt ( self , option_name , port_name ) : \n 
~~~ return [ option_name ] \n 
~~ ~~ _opt_dict = { : opt_command_line_val , \n 
: opt_command_line_vec , \n 
: opt_command_line_file , \n 
: opt_command_line_noopt } \n 
def process_command_line_input ( self ) : \n 
~~~ cmdline = [ self . _cmdline_base ] \n 
for ( opt_type , port_name , option_name ) in self . _cmdline_inputs : \n 
~~~ call = self . _opt_dict [ opt_type ] \n 
cmdline += call ( self , option_name = option_name , \n 
port_name = port_name ) \n 
~~ return cmdline \n 
~~ def process_command_line_output ( self ) : \n 
~~~ ( port_name , opt_name , format ) = self . _cmdline_output \n 
if callable ( format ) : \n 
~~~ format = format ( self ) \n 
~~ output_file = self . interpreter . filePool . create_file ( suffix = + format ) \n 
self . set_output ( port_name , output_file ) \n 
return [ opt_name , output_file . name ] \n 
~~~ cmdline_input = self . process_command_line_input ( ) \n 
cmdline_output = self . process_command_line_output ( ) \n 
self . _cmdline_callable ( * ( cmdline_input + cmdline_output ) ) \n 
~~ ~~ class Emap ( Teem ) : \n 
~~~ cmdline = [ ] \n 
cmdline += self . opt_command_line_vec ( port_name = , \n 
option_name = ) \n 
cmdline += self . opt_command_line_noopt ( port_name = , \n 
self . check_input ( ) \n 
cmdline += self . opt_command_line_file ( port_name = , \n 
output_file = self . interpreter . filePool . create_file ( suffix = ) \n 
cmdline += [ , output_file . name ] \n 
self . run_teem ( * cmdline ) \n 
self . set_output ( "output_file" , output_file ) \n 
~~ _input_ports = [ ( , [ ] ) , \n 
( , [ ( basic . File , ) ] ) , \n 
( , Vec3Type ) , \n 
( , Vec3Type ) ] \n 
_output_ports = [ ( , [ ( basic . File , ) ] ) ] \n 
~~ class Soid ( Teem ) : \n 
cmdline += self . opt_command_line_val ( port_name = , \n 
if self . has_input ( ) : \n 
~~~ cmdline += self . opt_command_line_vec ( port_name = , \n 
~~ elif self . has_input ( ) and self . has_input ( ) : \n 
~~~ cmdline += [ , \n 
self . get_input ( ) , \n 
self . get_input ( ) ] \n 
~~ output_file = self . interpreter . filePool . create_file ( suffix = ) \n 
self . run_limn_test ( * cmdline ) \n 
~~ _input_ports = [ ( , [ ( basic . Float , ) ] ) , \n 
( , [ ( basic . Float , ) ] ) , \n 
( , [ ] ) , \n 
( , [ ( basic . Float , ) , \n 
( basic . Float , ) ] ) , \n 
~~ class OffToEps ( Teem ) : \n 
input_file = self . get_input ( ) \n 
cmdline += [ , input_file . name ] \n 
~~ _input_ports = [ \n 
( "right_hand" , [ ] ) , \n 
( "input_file" , [ ( basic . File , ) ] ) , \n 
( "environment_map" , [ ( basic . File , ) ] ) , \n 
( "from_point" , [ ( basic . Float , ) ] * 3 ) , \n 
( "u_range" , [ ( basic . Float , ) ] * 2 ) , \n 
( "v_range" , [ ( basic . Float , ) ] * 2 ) , \n 
( "edge_widths" , [ ( basic . Float , ) ] * 5 ) , \n 
( "orthogonal" , [ ] ) , \n 
( "no_background" , [ ] ) , \n 
( "concave" , [ ] ) , \n 
( "world_to_points_scale" , [ ( basic . Float , ) ] ) , \n 
_output_ports = [ \n 
( "output_file" , [ ( basic . File , ) ] ) , \n 
~~ class EpsToPpm ( Teem ) : \n 
cmdline += [ self . get_input ( ) . name ] \n 
cmdline += [ self . get_input ( ) ] \n 
cmdline += [ ] \n 
cmdline += [ output_file . name ] \n 
self . run_path ( * cmdline ) \n 
( "resolution" , [ ( basic . Float , ) ] ) , \n 
~~ class Unu ( Teem ) : \n 
~~~ def do_output ( self ) : \n 
~~~ if self . has_input ( ) : \n 
~~~ suffix = + self . get_input ( ) \n 
~~~ suffix = \n 
~~ output_file = self . interpreter . filePool . create_file ( suffix = suffix ) \n 
return ( [ , output_file . name ] , output_file ) \n 
~~ def do_input ( self ) : \n 
~~~ self . check_input ( ) \n 
return [ , self . get_input ( ) . name ] \n 
( "format" , [ ( basic . String , ) ] ) , \n 
~~ class UnuSave ( Unu ) : \n 
cmdline += self . do_input ( ) \n 
( ocmd , output_file ) = self . do_output ( ) \n 
self . set_output ( "output_name" , output_file . name ) \n 
cmdline += ocmd \n 
cmdline += self . opt_command_line_val ( port_name = , option_name = ) \n 
( "output_format" , [ ( basic . String , ) ] ) , \n 
( "output_name" , [ ( basic . String , ) ] ) , \n 
~~ class UnuSwap ( Unu ) : \n 
v1 , v2 = self . get_input ( ) \n 
cmdline += [ , str ( v1 ) , str ( v2 ) ] \n 
( "axes" , [ ( basic . Integer , ) ] * 2 ) , \n 
~~ class UnuProject ( Unu ) : \n 
( "measure" , [ ( basic . String , ) ] ) , \n 
( "axis" , ( basic . Integer , ) ) , \n 
~~ class UnuMinmax ( Teem ) : \n 
~~~ def do_input ( self ) : \n 
return [ self . get_input ( ) . name ] \n 
~~~ cmdline += \n 
~~ output_file = self . interpreter . filePool . create_file ( ) \n 
cmdline += [ % output_file . name ] \n 
f = file ( output_file . name ) \n 
~~~ mn = float ( f . readline ( ) . split ( ) [ - 1 ] ) \n 
mx = float ( f . readline ( ) . split ( ) [ - 1 ] ) \n 
~~~ raise ModuleError ( self , ) \n 
~~ self . set_output ( , ( mn , mx ) ) \n 
~~ _input_ports = [ ( , [ ( basic . File , ) ] ) , \n 
( , [ ] ) ] \n 
_output_ports = [ ( , [ ( basic . Float , ) ] * 2 ) ] \n 
~~ class UnuReshape ( Unu ) : \n 
( "axes" , [ ( basic . String , ) ] ) , \n 
~~ class UnuResample ( Unu ) : \n 
( "sampling_spec" , [ ( basic . String , ) ] ) , \n 
( "centering" , [ ( basic . String , ) ] ) , \n 
( "kernel" , [ ( basic . String , ) ] ) , \n 
~~ class UnuJoin ( Unu ) : \n 
return [ ] + [ x . name \n 
for x in self . get_input ( ) ] \n 
( "input_list" , [ ( basic . List , ) ] ) , \n 
( "axis" , [ ( basic . Integer , ) ] ) , \n 
~~ class UnuQuantize ( Unu ) : \n 
~~~ current = Unu . do_input ( self ) \n 
return current + [ , self . get_input ( ) ] \n 
( , [ ( basic . Integer , ) ] ) , \n 
~~ class Unu1op ( Unu ) : \n 
~~~ r = [ ] \n 
~~~ r += [ , self . get_input ( ) . name ] \n 
~~ def is_cacheable ( self ) : \n 
~~~ return not self . get_input ( ) in [ , ] \n 
~~~ allowed_ops = [ , , , \n 
, , ] \n 
if not self . get_input ( ) in allowed_ops : \n 
self . get_input ( ) ) ) \n 
~~ cmdline = [ ] \n 
self . set_output ( , output_file ) \n 
( "in1_file" , [ ( basic . File , ) ] ) , \n 
( "op" , [ ( basic . String , ) ] ) , \n 
~~ class Unu2op ( Unu ) : \n 
~~~ r += [ self . get_input ( ) . name ] \n 
~~ elif self . has_input ( ) : \n 
~~~ r += [ self . get_input ( ) ] \n 
~~ if self . has_input ( ) : \n 
~~~ allowed_ops = [ , , , , \n 
, , , , , , \n 
, , , , ] \n 
( "in1_value" , [ ( basic . String , ) ] ) , \n 
( "in2_file" , [ ( basic . File , ) ] ) , \n 
( "in2_value" , [ ( basic . String , ) ] ) , \n 
~~ class Unu3op ( Unu ) : \n 
~~~ allowed_ops = [ , , \n 
, ] \n 
( "in3_file" , [ ( basic . File , ) ] ) , \n 
( "in3_value" , [ ( basic . String , ) ] ) , \n 
~~ class Teem_TT ( Teem ) : \n 
~~~ _cmdline_base = \n 
_cmdline_inputs = [ ( , , ) , \n 
( , , ) , \n 
( , , ) ] \n 
_cmdline_output = ( , , ) \n 
_cmdline_callable = Teem . run_ten_test \n 
_input_ports = [ \n 
( "num_samples" , [ ( basic . String , ) ] ) , \n 
( "location" , Vec3Type ) , \n 
( "max_ca" , [ ( basic . Float , ) ] ) , \n 
( "westin_metric" , [ ( basic . Float , ) ] ) , \n 
( "right_triangle" , [ ] ) , \n 
( "whole" , [ ] ) , \n 
~~ class Tend_norm ( Teem ) : \n 
_cmdline_callable = Teem . run_teem \n 
( "weights" , Vec3Type ) , \n 
~~ class Tend_glyph ( Teem ) : \n 
( "up" , [ ( basic . Float , ) ] * 3 ) , \n 
( "psc" , [ ( basic . Float , ) ] ) , \n 
( "atr" , [ ( basic . Float , ) ] ) , \n 
( "emap" , [ ( basic . File , ) ] ) , \n 
( "glyph_shape" , [ ( basic . String , ) ] ) , \n 
( "glyph_size" , [ ( basic . Float , ) ] ) , \n 
( "glyph_resolution" , [ ( basic . Integer , ) ] ) , \n 
( "widths" , [ ( basic . Float , ) ] * 3 ) , \n 
( "output_file" , [ ( basic . File , ) ] ) \n 
~~ class UnuSlice ( Unu ) : \n 
r += [ , self . get_input ( ) . name ] \n 
r += [ , str ( self . get_input ( ) ) ] \n 
( , basic . File ) , \n 
( , basic . Integer ) , \n 
~~ class TeemScaledTransferFunction ( Teem ) : \n 
~~~ _input_ports = [ ( , [ ( basic . Float , ) ] * 2 ) , \n 
( , [ basic . Integer ] ) ] \n 
def compute ( self ) : \n 
~~~ ramp = self . interpreter . filePool . create_file ( suffix = ) \n 
tf_vals = self . interpreter . filePool . create_file ( suffix = ) \n 
output = self . interpreter . filePool . create_file ( suffix = ) \n 
steps = self . get_input ( ) \n 
rng = self . get_input ( ) \n 
% steps , \n 
% ramp . name ) \n 
tf_file = file ( tf_vals . name , ) \n 
tf = self . get_input ( ) \n 
for ( scalar , op , ( r , g , b ) ) in tf . _pts : \n 
~~~ tf_file . write ( % ( scalar , r , g , b , op ) ) \n 
~~ tf_file . close ( ) \n 
self . run_teem ( % tf_vals . name , \n 
% len ( tf . _pts ) , \n 
% ramp . name , \n 
rng [ 1 ] , \n 
output . name ) ) \n 
self . set_output ( , output ) \n 
~~ ~~ class Miter ( Teem ) : \n 
~~~ _input_ports = [ \n 
( , FileType ) , \n 
( , FloatType ) , \n 
( , FloatType * 2 ) , \n 
( , IntegerType * 2 ) , \n 
( , FloatType * 3 ) , \n 
( , StringType ) , \n 
( , IntegerType ) , \n 
_output_ports = [ ( , FileType ) ] \n 
cmdline += self . opt_command_line_file ( , ) \n 
f = self . interpreter . filePool . create_file ( suffix = ) \n 
cmdline += [ , f . name ] \n 
cmdline += self . opt_command_line_vec ( , ) \n 
cmdline += self . opt_command_line_noopt ( , ) \n 
cmdline += self . opt_command_line_val ( , ) \n 
self . set_output ( , f ) \n 
~~ ~~ class OverRGB ( Teem ) : \n 
( , FileType ) , ] \n 
~~ ~~ _modules = [ Teem , Emap , Soid , Miter , OverRGB , \n 
OffToEps , \n 
EpsToPpm , \n 
Unu , \n 
UnuMinmax , \n 
UnuSave , \n 
UnuSwap , \n 
UnuProject , \n 
UnuReshape , \n 
UnuResample , \n 
UnuJoin , \n 
UnuQuantize , \n 
Unu1op , \n 
Unu2op , \n 
Unu3op , \n 
UnuSlice , \n 
Teem_TT , \n 
Tend_norm , \n 
Tend_glyph ] \n 
def package_dependencies ( ) : \n 
~~~ import core . packagemanager \n 
manager = core . packagemanager . get_package_manager ( ) \n 
if manager . has_package ( ) : \n 
~~ ~~ def initialize ( path = None , limnTestPath = None , tenTestPath = None , \n 
* args , ** keywords ) : \n 
~~~ global _teemPath \n 
if not path : \n 
_teemPath = "" \n 
_teemPath = path + \n 
~~ global _teemLimnTestPath \n 
if not limnTestPath : \n 
_teemLimnTestPath = "" \n 
_teemLimnTestPath = limnTestPath + \n 
~~ global _teemTenTestPath \n 
if not tenTestPath : \n 
_teemTenTestPath = "" \n 
_teemTenTestPath = tenTestPath + \n 
~~ reg = core . modules . module_registry . registry \n 
if reg . has_module ( , \n 
~~~ _modules . append ( TeemScaledTransferFunction ) \n 
TF = reg . get_descriptor_by_name ( , \n 
) . module \n 
TeemScaledTransferFunction . _input_ports . append ( ( , [ ( TF , \n 
########################################################################### \n 
################################################################################ \n 
ret = [ ] \n 
ret . append ( ) \n 
~~ def package_requirements ( ) : \n 
~~~ import core . requirements \n 
if not core . requirements . python_module_exists ( ) : \n 
~~~ raise core . requirements . MissingRequirement ( ) \n 
~~ if not core . requirements . python_module_exists ( ) : \n 
~~~ print , \n 
~~ import titan \n 
extensions = [ \n 
author = \n 
release = \n 
language = None \n 
todo_include_todos = False \n 
extlinks = { : ( , \n 
) } \n 
( master_doc , , , \n 
author , ) , \n 
latex_use_parts = True \n 
[ author ] , 1 ) \n 
author , , , \n 
intersphinx_mapping = { : None } \n 
autoclass_content = "init" \n 
import vtk \n 
from vtk . util . colors import chocolate , mint \n 
cone = vtk . vtkCone ( ) \n 
cone . SetAngle ( 20 ) \n 
vertPlane = vtk . vtkPlane ( ) \n 
vertPlane . SetOrigin ( .1 , 0 , 0 ) \n 
vertPlane . SetNormal ( - 1 , 0 , 0 ) \n 
basePlane = vtk . vtkPlane ( ) \n 
basePlane . SetOrigin ( 1.2 , 0 , 0 ) \n 
basePlane . SetNormal ( 1 , 0 , 0 ) \n 
iceCream = vtk . vtkSphere ( ) \n 
iceCream . SetCenter ( 1.333 , 0 , 0 ) \n 
iceCream . SetRadius ( 0.5 ) \n 
bite = vtk . vtkSphere ( ) \n 
bite . SetCenter ( 1.5 , 0 , 0.5 ) \n 
bite . SetRadius ( 0.25 ) \n 
theCone = vtk . vtkImplicitBoolean ( ) \n 
theCone . SetOperationTypeToIntersection ( ) \n 
theCone . AddFunction ( cone ) \n 
theCone . AddFunction ( vertPlane ) \n 
theCone . AddFunction ( basePlane ) \n 
theCream = vtk . vtkImplicitBoolean ( ) \n 
theCream . SetOperationTypeToDifference ( ) \n 
theCream . AddFunction ( iceCream ) \n 
theCream . AddFunction ( bite ) \n 
theConeSample = vtk . vtkSampleFunction ( ) \n 
theConeSample . SetImplicitFunction ( theCone ) \n 
theConeSample . SetModelBounds ( - 1 , 1.5 , - 1.25 , 1.25 , - 1.25 , 1.25 ) \n 
theConeSample . SetSampleDimensions ( 60 , 60 , 60 ) \n 
theConeSample . ComputeNormalsOff ( ) \n 
theConeSurface = vtk . vtkContourFilter ( ) \n 
theConeSurface . SetInputConnection ( theConeSample . GetOutputPort ( ) ) \n 
theConeSurface . SetValue ( 0 , 0.0 ) \n 
coneMapper = vtk . vtkPolyDataMapper ( ) \n 
coneMapper . SetInputConnection ( theConeSurface . GetOutputPort ( ) ) \n 
coneMapper . ScalarVisibilityOff ( ) \n 
coneActor = vtk . vtkActor ( ) \n 
coneActor . SetMapper ( coneMapper ) \n 
coneActor . GetProperty ( ) . SetColor ( chocolate ) \n 
theCreamSample = vtk . vtkSampleFunction ( ) \n 
theCreamSample . SetImplicitFunction ( theCream ) \n 
theCreamSample . SetModelBounds ( 0 , 2.5 , - 1.25 , 1.25 , - 1.25 , 1.25 ) \n 
theCreamSample . SetSampleDimensions ( 60 , 60 , 60 ) \n 
theCreamSample . ComputeNormalsOff ( ) \n 
theCreamSurface = vtk . vtkContourFilter ( ) \n 
theCreamSurface . SetInputConnection ( theCreamSample . GetOutputPort ( ) ) \n 
theCreamSurface . SetValue ( 0 , 0.0 ) \n 
creamMapper = vtk . vtkPolyDataMapper ( ) \n 
creamMapper . SetInputConnection ( theCreamSurface . GetOutputPort ( ) ) \n 
creamMapper . ScalarVisibilityOff ( ) \n 
creamActor = vtk . vtkActor ( ) \n 
creamActor . SetMapper ( creamMapper ) \n 
creamActor . GetProperty ( ) . SetColor ( mint ) \n 
ren = vtk . vtkRenderer ( ) \n 
renWin = vtk . vtkRenderWindow ( ) \n 
renWin . AddRenderer ( ren ) \n 
iren = vtk . vtkRenderWindowInteractor ( ) \n 
iren . SetRenderWindow ( renWin ) \n 
ren . AddActor ( coneActor ) \n 
ren . AddActor ( creamActor ) \n 
ren . SetBackground ( 1 , 1 , 1 ) \n 
renWin . SetSize ( 250 , 250 ) \n 
ren . ResetCamera ( ) \n 
ren . GetActiveCamera ( ) . Roll ( 90 ) \n 
ren . GetActiveCamera ( ) . Dolly ( 1.5 ) \n 
ren . ResetCameraClippingRange ( ) \n 
iren . Initialize ( ) \n 
renWin . Render ( ) \n 
iren . Start ( ) \n 
from vtk . util . misc import vtkGetDataRoot \n 
VTK_DATA_ROOT = vtkGetDataRoot ( ) \n 
pl3d = vtk . vtkPLOT3DReader ( ) \n 
pl3d . SetXYZFileName ( VTK_DATA_ROOT + "/Data/combxyz.bin" ) \n 
pl3d . SetQFileName ( VTK_DATA_ROOT + "/Data/combq.bin" ) \n 
pl3d . SetScalarFunctionNumber ( 100 ) \n 
pl3d . SetVectorFunctionNumber ( 202 ) \n 
pl3d . Update ( ) \n 
plane = vtk . vtkPlaneSource ( ) \n 
plane . SetResolution ( 50 , 50 ) \n 
transP1 = vtk . vtkTransform ( ) \n 
transP1 . Translate ( 3.7 , 0.0 , 28.37 ) \n 
transP1 . Scale ( 5 , 5 , 5 ) \n 
transP1 . RotateY ( 90 ) \n 
tpd1 = vtk . vtkTransformPolyDataFilter ( ) \n 
tpd1 . SetInputConnection ( plane . GetOutputPort ( ) ) \n 
tpd1 . SetTransform ( transP1 ) \n 
outTpd1 = vtk . vtkOutlineFilter ( ) \n 
outTpd1 . SetInputConnection ( tpd1 . GetOutputPort ( ) ) \n 
mapTpd1 = vtk . vtkPolyDataMapper ( ) \n 
mapTpd1 . SetInputConnection ( outTpd1 . GetOutputPort ( ) ) \n 
tpd1Actor = vtk . vtkActor ( ) \n 
tpd1Actor . SetMapper ( mapTpd1 ) \n 
tpd1Actor . GetProperty ( ) . SetColor ( 0 , 0 , 0 ) \n 
transP2 = vtk . vtkTransform ( ) \n 
transP2 . Translate ( 9.2 , 0.0 , 31.20 ) \n 
transP2 . Scale ( 5 , 5 , 5 ) \n 
transP2 . RotateY ( 90 ) \n 
tpd2 = vtk . vtkTransformPolyDataFilter ( ) \n 
tpd2 . SetInputConnection ( plane . GetOutputPort ( ) ) \n 
tpd2 . SetTransform ( transP2 ) \n 
outTpd2 = vtk . vtkOutlineFilter ( ) \n 
outTpd2 . SetInputConnection ( tpd2 . GetOutputPort ( ) ) \n 
mapTpd2 = vtk . vtkPolyDataMapper ( ) \n 
mapTpd2 . SetInputConnection ( outTpd2 . GetOutputPort ( ) ) \n 
tpd2Actor = vtk . vtkActor ( ) \n 
tpd2Actor . SetMapper ( mapTpd2 ) \n 
tpd2Actor . GetProperty ( ) . SetColor ( 0 , 0 , 0 ) \n 
transP3 = vtk . vtkTransform ( ) \n 
transP3 . Translate ( 13.27 , 0.0 , 33.30 ) \n 
transP3 . Scale ( 5 , 5 , 5 ) \n 
transP3 . RotateY ( 90 ) \n 
tpd3 = vtk . vtkTransformPolyDataFilter ( ) \n 
tpd3 . SetInputConnection ( plane . GetOutputPort ( ) ) \n 
tpd3 . SetTransform ( transP3 ) \n 
outTpd3 = vtk . vtkOutlineFilter ( ) \n 
outTpd3 . SetInputConnection ( tpd3 . GetOutputPort ( ) ) \n 
mapTpd3 = vtk . vtkPolyDataMapper ( ) \n 
mapTpd3 . SetInputConnection ( outTpd3 . GetOutputPort ( ) ) \n 
tpd3Actor = vtk . vtkActor ( ) \n 
tpd3Actor . SetMapper ( mapTpd3 ) \n 
tpd3Actor . GetProperty ( ) . SetColor ( 0 , 0 , 0 ) \n 
appendF = vtk . vtkAppendPolyData ( ) \n 
appendF . AddInput ( tpd1 . GetOutput ( ) ) \n 
appendF . AddInput ( tpd2 . GetOutput ( ) ) \n 
appendF . AddInput ( tpd3 . GetOutput ( ) ) \n 
probe = vtk . vtkProbeFilter ( ) \n 
probe . SetInputConnection ( appendF . GetOutputPort ( ) ) \n 
probe . SetSource ( pl3d . GetOutput ( ) ) \n 
contour = vtk . vtkContourFilter ( ) \n 
contour . SetInputConnection ( probe . GetOutputPort ( ) ) \n 
contour . GenerateValues ( 50 , pl3d . GetOutput ( ) . GetScalarRange ( ) ) \n 
contourMapper = vtk . vtkPolyDataMapper ( ) \n 
contourMapper . SetInputConnection ( contour . GetOutputPort ( ) ) \n 
contourMapper . SetScalarRange ( pl3d . GetOutput ( ) . GetScalarRange ( ) ) \n 
planeActor = vtk . vtkActor ( ) \n 
planeActor . SetMapper ( contourMapper ) \n 
outline = vtk . vtkStructuredGridOutlineFilter ( ) \n 
outline . SetInputConnection ( pl3d . GetOutputPort ( ) ) \n 
outlineMapper = vtk . vtkPolyDataMapper ( ) \n 
outlineMapper . SetInputConnection ( outline . GetOutputPort ( ) ) \n 
outlineActor = vtk . vtkActor ( ) \n 
outlineActor . SetMapper ( outlineMapper ) \n 
outlineActor . GetProperty ( ) . SetColor ( 0 , 0 , 0 ) \n 
ren . AddActor ( outlineActor ) \n 
ren . AddActor ( planeActor ) \n 
ren . AddActor ( tpd1Actor ) \n 
ren . AddActor ( tpd2Actor ) \n 
ren . AddActor ( tpd3Actor ) \n 
renWin . SetSize ( 400 , 400 ) \n 
cam1 = ren . GetActiveCamera ( ) \n 
cam1 . SetClippingRange ( 3.95297 , 50 ) \n 
cam1 . SetFocalPoint ( 8.88908 , 0.595038 , 29.3342 ) \n 
cam1 . SetPosition ( - 12.3332 , 31.7479 , 41.2387 ) \n 
cam1 . SetViewUp ( 0.060772 , - 0.319905 , 0.945498 ) \n 
addPackage ( ) \n 
def testHook ( ) : \n 
def printTree ( n , indent = 0 ) : \n 
~~~ def iprint ( str ) : \n 
~~ iprint ( % n . descriptor . name ) \n 
for c in n . children : \n 
~~~ printTree ( c , indent + 4 ) \n 
~~ ~~ import modules \n 
import modules . module_registry \n 
t = modules . module_registry . registry . classTree \n 
printTree ( t ) \n 
~~ import xmlrpclib \n 
from subprocess import Popen , PIPE \n 
import smtplib \n 
from email . mime . text import MIMEText \n 
import logging . handlers \n 
class VistrailWatcher ( object ) : \n 
def __init__ ( self , server , email_addresses , root_virtual_display = None ) : \n 
~~~ self . server = server \n 
self . root_virtual_display = root_virtual_display \n 
self . email_addresses = email_addresses \n 
self . sender = "vistrails_server_watcher@%s" % server \n 
self . proxies = { } \n 
self . is_down = { } \n 
self . logger = self . make_logger ( "vt_watcher.log" ) \n 
proc1 = Popen ( [ "ps" , "-eaf" ] , stdout = PIPE ) \n 
proc2 = Popen ( [ "grep" , "vistrails_server" ] , stdin = proc1 . stdout , stdout = PIPE ) \n 
out , err = proc3 . communicate ( ) \n 
out = out . strip ( ) . split ( ) \n 
for port in self . ports : \n 
~~~ self . proxies [ port ] = xmlrpclib . ServerProxy ( "http://%s:%d" % ( self . server , port ) ) \n 
self . is_down [ port ] = False \n 
~~ ~~ def make_logger ( self , filename ) : \n 
logger = logging . getLogger ( "VistrailsWatcher" ) \n 
handler = logging . handlers . RotatingFileHandler ( filename , \n 
maxBytes = 1024 * 1024 , \n 
backupCount = 5 ) \n 
handler . setFormatter ( logging . Formatter ( \n 
handler . setLevel ( logging . DEBUG ) \n 
logger . setLevel ( logging . DEBUG ) \n 
logger . addHandler ( handler ) \n 
return logger \n 
~~ def send_email ( self , msg ) : \n 
if not self . email_addresses : \n 
~~ msg = MIMEText ( msg ) \n 
msg [ ] = % self . server \n 
msg [ ] = self . sender \n 
msg [ ] = . join ( self . email_addresses ) \n 
s = smtplib . SMTP ( ) \n 
s . sendmail ( self . sender , self . email_addresses , msg . as_string ( ) ) \n 
s . quit ( ) \n 
~~ def restart_instance ( self , port ) : \n 
script = os . path . join ( "/server/vistrails/git/scripts" , \n 
"start_vistrails_xvfb.sh" ) \n 
instance_virtual_display = self . root_virtual_display + self . ports . index ( port ) + 1 \n 
args = [ script , ":%s" % instance_virtual_display , \n 
self . server , str ( port + 1 ) , , ] \n 
~~~ Popen ( args ) \n 
sleep ( 20 ) \n 
~~ except Exception , e : \n 
self . logger . error ( str ( e ) ) \n 
~~ ~~ def watch ( self ) : \n 
if not self . ports : \n 
~~~ for port in self . ports : \n 
~~~ if not self . proxies [ port ] . try_ping ( ) : \n 
~~~ if not self . is_down [ port ] : \n 
self . restart_instance ( port ) \n 
~~ self . is_down [ port ] = True \n 
~~~ if self . is_down [ port ] : \n 
~~ self . is_down [ port ] = False \n 
~~ ~~ except Exception , e : \n 
~~ ~~ sleep ( 60 * 5 ) \n 
~~~ vt_watcher = VistrailWatcher ( "vis-7.sci.utah.edu" , [ "phillipmates@gmail.com" ] , root_virtual_display vt_watcher . watch ( ) \n 
import os . path \n 
from vistrails . core . application import is_running_gui \n 
from vistrails . core . configuration import get_vistrails_configuration \n 
import vistrails . core . interpreter . default \n 
import vistrails . core . db . io \n 
from vistrails . core . db . io import load_vistrail \n 
from vistrails . core . db . locator import XMLFileLocator , ZIPFileLocator \n 
from vistrails . core import debug \n 
import vistrails . core . interpreter . cached \n 
from vistrails . core . vistrail . job import Workflow as JobWorkflow \n 
import vistrails . core . vistrail . pipeline \n 
from vistrails . core . utils import VistrailsInternalError \n 
from vistrails . core . vistrail . controller import VistrailController \n 
import vistrails . core . packagemanager \n 
import vistrails . core . system \n 
import vistrails . core . vistrail \n 
import vistrails . db \n 
def run_and_get_results ( w_list , parameters = , \n 
update_vistrail = True , extra_info = None , \n 
reason = ) : \n 
elements = parameters . split ( "$&$" ) \n 
aliases = { } \n 
params = [ ] \n 
result = [ ] \n 
for locator , workflow in w_list : \n 
~~~ ( v , abstractions , thumbnails , mashups ) = load_vistrail ( locator ) \n 
controller = VistrailController ( v , locator , abstractions , thumbnails , \n 
mashups , auto_save = update_vistrail ) \n 
if isinstance ( workflow , basestring ) : \n 
~~~ version = v . get_version_number ( workflow ) \n 
~~ elif isinstance ( workflow , ( int , long ) ) : \n 
~~~ version = workflow \n 
~~ elif workflow is None : \n 
~~~ version = controller . get_latest_version_in_graph ( ) \n 
raise VistrailsInternalError ( msg ) \n 
~~ controller . change_selected_version ( version ) \n 
for e in elements : \n 
~~~ pos = e . find ( "=" ) \n 
if pos != - 1 : \n 
~~~ key = e [ : pos ] . strip ( ) \n 
value = e [ pos + 1 : ] . strip ( ) \n 
if controller . current_pipeline . has_alias ( key ) : \n 
~~~ aliases [ key ] = value \n 
~~ elif in extra_info : \n 
~~~ for mashuptrail in mashups : \n 
~~~ if mashuptrail . vtVersion == version : \n 
~~~ mashup = mashuptrail . getMashup ( extra_info [ ] ) \n 
c = mashup . getAliasByName ( key ) . component \n 
params . append ( ( c . vttype , c . vtid , value ) ) \n 
~~ ~~ ~~ ~~ ~~ if not update_vistrail : \n 
~~~ conf = get_vistrails_configuration ( ) \n 
if conf . has ( ) : \n 
~~~ conf . thumbs . autoSave = False \n 
~~ ~~ jobMonitor = controller . jobMonitor \n 
current_workflow = jobMonitor . currentWorkflow ( ) \n 
if not current_workflow : \n 
~~~ for job in jobMonitor . workflows . itervalues ( ) : \n 
~~~ job_version = int ( job . version ) \n 
~~ except ValueError : \n 
~~~ job_version = v . get_version_number ( job . version ) \n 
~~ ~~ if version == job_version : \n 
~~~ current_workflow = job \n 
jobMonitor . startWorkflow ( job ) \n 
~~ ~~ if not current_workflow : \n 
~~~ current_workflow = JobWorkflow ( version ) \n 
jobMonitor . startWorkflow ( current_workflow ) \n 
~~ ~~ try : \n 
~~~ ( results , _ ) = controller . execute_current_workflow ( custom_aliases = aliases , \n 
custom_params = params , \n 
extra_info = extra_info , \n 
reason = reason ) \n 
~~~ jobMonitor . finishWorkflow ( ) \n 
~~ new_version = controller . current_version \n 
if new_version != version : \n 
workflow , version , new_version ) ) \n 
~~ run = results [ 0 ] \n 
run . workflow_info = ( locator . name , new_version ) \n 
run . pipeline = controller . current_pipeline \n 
if update_vistrail : \n 
~~~ controller . write_vistrail ( locator ) \n 
~~ result . append ( run ) \n 
if current_workflow . jobs : \n 
~~~ if current_workflow . completed ( ) : \n 
~~~ run . job = "COMPLETED" \n 
for job in current_workflow . jobs . itervalues ( ) : \n 
~~~ if not job . finished : \n 
~~ ~~ ~~ print run . job \n 
~~ ~~ return result \n 
~~ def get_wf_graph ( w_list , output_dir , pdf = False ) : \n 
if is_running_gui ( ) : \n 
~~~ from vistrails . gui . vistrail_controller import VistrailController as GUIVistrailController \n 
controller = GUIVistrailController ( v , locator , abstractions , \n 
thumbnails , mashups , \n 
auto_save = False ) \n 
controller . current_pipeline_view . set_controller ( controller ) \n 
version = None \n 
if controller . current_pipeline is not None : \n 
~~~ controller . updatePipelineScene ( ) \n 
if pdf : \n 
~~~ base_fname = "%s_%s_pipeline.pdf" % ( locator . short_filename , version ) \n 
filename = os . path . join ( output_dir , base_fname ) \n 
controller . current_pipeline_scene . saveToPDF ( filename ) \n 
~~~ base_fname = "%s_%s_pipeline.png" % ( locator . short_filename , version ) \n 
controller . current_pipeline_scene . saveToPNG ( filename ) \n 
~~ result . append ( ( True , "" ) ) \n 
~~~ result . append ( ( False , debug . format_exception ( e ) ) ) \n 
debug . critical ( error_str ) \n 
result . append ( ( False , error_str ) ) \n 
~~ return result \n 
~~ def get_vt_graph ( vt_list , tree_info , pdf = False ) : \n 
for locator in vt_list : \n 
thumbnails , mashups ) \n 
if tree_info is not None : \n 
~~~ from vistrails . gui . version_view import QVersionTreeView \n 
version_view = QVersionTreeView ( ) \n 
version_view . scene ( ) . setupScene ( controller ) \n 
~~~ base_fname = "graph_%s.pdf" % locator . short_filename \n 
filename = os . path . join ( tree_info , base_fname ) \n 
version_view . scene ( ) . saveToPDF ( filename ) \n 
~~~ base_fname = "graph_%s.png" % locator . short_filename \n 
version_view . scene ( ) . saveToPNG ( filename ) \n 
~~ del version_view \n 
result . append ( ( True , "" ) ) \n 
~~ def run ( w_list , parameters = , update_vistrail = True , \n 
all_errors = [ ] \n 
results = run_and_get_results ( w_list , parameters , \n 
update_vistrail , extra_info , reason ) \n 
for result in results : \n 
~~~ ( objs , errors , executed ) = ( result . objects , \n 
result . errors , result . executed ) \n 
for err in sorted ( errors . iteritems ( ) ) : \n 
~~~ all_errors . append ( result . workflow_info + err ) \n 
~~ ~~ return all_errors \n 
~~ def run_parameter_exploration ( locator , pe_id , extra_info = { } , \n 
~~~ pe_id = int ( pe_id ) \n 
pe = controller . vistrail . get_paramexp ( pe_id ) \n 
~~~ pe = controller . vistrail . get_named_paramexp ( pe_id ) \n 
~~ controller . change_selected_version ( pe . action_id ) \n 
controller . executeParameterExploration ( pe , extra_info = extra_info , \n 
showProgress = False ) \n 
~~~ return ( locator , pe_id , \n 
debug . format_exception ( e ) , debug . format_exc ( ) ) \n 
~~ ~~ ~~ def run_parameter_explorations ( w_list , extra_info = { } , \n 
for locator , pe_id in w_list : \n 
~~~ result = run_parameter_exploration ( locator , pe_id , reason = reason , \n 
extra_info = extra_info ) \n 
if result : \n 
~~~ all_errors . append ( result ) \n 
~~ def cleanup ( ) : \n 
~~~ vistrails . core . interpreter . cached . CachedInterpreter . cleanup ( ) \n 
#Testing \n 
~~ class TestConsoleMode ( unittest . TestCase ) : \n 
def setUpClass ( cls ) : \n 
~~~ manager = vistrails . core . packagemanager . get_package_manager ( ) \n 
~~ d = { : } \n 
manager . late_enable_package ( , d ) \n 
def tearDownClass ( cls ) : \n 
~~~ manager . late_disable_package ( ) \n 
~~ ~~ def test1 ( self ) : \n 
~~~ from vistrails . core . modules . basic_modules import StandardOutput \n 
values = [ ] \n 
def mycompute ( s ) : \n 
~~~ v = s . get_input ( ) \n 
values . append ( v ) \n 
~~ orig_compute = StandardOutput . compute \n 
StandardOutput . compute = mycompute \n 
~~~ locator = XMLFileLocator ( vistrails . core . system . vistrails_root_directory ( ) + \n 
self . assertEqual ( len ( result ) , 0 ) \n 
self . assertEqual ( values , [ 2 ] ) \n 
~~~ StandardOutput . compute = orig_compute \n 
~~ ~~ def test_tuple ( self ) : \n 
~~~ from vistrails . core . vistrail . module_param import ModuleParam \n 
from vistrails . core . vistrail . module_function import ModuleFunction \n 
from vistrails . core . utils import DummyView \n 
from vistrails . core . vistrail . module import Module \n 
import vistrails . db . domain \n 
id_scope = vistrails . db . domain . IdScope ( ) \n 
interpreter = vistrails . core . interpreter . default . get_default_interpreter ( ) \n 
v = DummyView ( ) \n 
p = vistrails . core . vistrail . pipeline . Pipeline ( ) \n 
params = [ ModuleParam ( id = id_scope . getNewId ( ModuleParam . vtType ) , \n 
pos = 0 , \n 
type = , \n 
val = , \n 
ModuleParam ( id = id_scope . getNewId ( ModuleParam . vtType ) , \n 
pos = 1 , \n 
) ] \n 
function = ModuleFunction ( id = id_scope . getNewId ( ModuleFunction . vtType ) , \n 
name = ) \n 
function . add_parameters ( params ) \n 
module = Module ( id = id_scope . getNewId ( Module . vtType ) , \n 
package = , \n 
version = ) \n 
module . add_function ( function ) \n 
p . add_module ( module ) \n 
interpreter . execute ( p , \n 
locator = XMLFileLocator ( ) , \n 
current_version = 1 L , \n 
view = v ) \n 
~~ def test_python_source ( self ) : \n 
result = run ( [ ( locator , "testPortsAndFail" ) ] , update_vistrail = False ) \n 
~~ def test_python_source_2 ( self ) : \n 
result = run_and_get_results ( [ ( locator , "test_simple_success" ) ] , \n 
update_vistrail = False ) [ 0 ] \n 
self . assertEquals ( len ( result . executed ) , 1 ) \n 
~~ def test_dynamic_module_error ( self ) : \n 
result = run ( [ ( locator , "test" ) ] , update_vistrail = False ) \n 
self . assertNotEqual ( len ( result ) , 0 ) \n 
~~ def test_change_parameter ( self ) : \n 
result = run ( [ ( locator , "v1" ) ] , update_vistrail = False ) \n 
result = run ( [ ( locator , "v2" ) ] , update_vistrail = False ) \n 
self . assertEquals ( len ( result ) , 0 ) \n 
~~ def test_ticket_73 ( self ) : \n 
v = locator . load ( ) \n 
( fd , filename ) = tempfile . mkstemp ( ) \n 
os . close ( fd ) \n 
locator = XMLFileLocator ( filename ) \n 
~~~ locator . save ( v ) \n 
~~~ os . remove ( filename ) \n 
~~ from __future__ import division \n 
from vistrails . db . domain import DBOpmGraph \n 
class OpmGraph ( DBOpmGraph ) : \n 
def __init__ ( self , * args , ** kwargs ) : \n 
~~~ if in kwargs : \n 
~~~ self . log = kwargs [ ] \n 
del kwargs [ ] \n 
~~~ self . log = None \n 
~~ if in kwargs : \n 
~~~ self . workflow = kwargs [ ] \n 
~~ elif in kwargs : \n 
~~~ self . workflow = None \n 
~~~ self . registry = kwargs [ ] \n 
~~~ self . registry = None \n 
~~~ self . version = kwargs [ ] \n 
~~~ self . version = None \n 
~~ DBOpmGraph . __init__ ( self , * args , ** kwargs ) \n 
~~ def __copy__ ( self ) : \n 
~~~ return self . do_copy ( ) \n 
~~ def do_copy ( self ) : \n 
~~~ cp = DBOpmGraph . __copy__ ( self ) \n 
cp . __class__ = OpmGraph \n 
cp . log = self . log \n 
cp . workflow = self . workflow \n 
cp . version = self . version \n 
cp . registry = self . registry \n 
return cp \n 
def convert ( _graph ) : \n 
~~~ if _graph . __class__ == OpmGraph : \n 
~~ _graph . __class__ = OpmGraph \n 
from vistrails . core . system import _defaultPkgPrefix , get_vistrails_basic_pkg_id , get_module_registry \n 
def load_cls ( cls_item , prefix = None ) : \n 
~~~ path = None \n 
if isinstance ( cls_item , basestring ) : \n 
~~~ ( path , cls_name ) = cls_item . split ( ) [ : 2 ] \n 
~~ elif isinstance ( cls_item , tuple ) : \n 
~~~ ( path , cls_name ) = cls_item \n 
~~ if path is not None : \n 
~~~ module = __import__ ( path , globals ( ) , locals ( ) , [ cls_name ] ) \n 
~~~ if prefix is None : \n 
~~ path = . join ( [ prefix , path ] ) \n 
module = __import__ ( path , globals ( ) , locals ( ) , [ cls_name ] ) \n 
~~ return getattr ( module , cls_name ) \n 
~~ return cls_item \n 
~~ def create_descriptor_string ( package , name , namespace = None , \n 
use_package = True ) : \n 
~~~ package_str = "" \n 
namespace_str = "" \n 
if use_package : \n 
~~~ package_str = "%s:" % package \n 
~~ if namespace : \n 
~~~ namespace_str = "%s|" % namespace \n 
~~ return "%s%s%s" % ( package_str , namespace_str , name ) \n 
~~ def parse_descriptor_string ( d_string , cur_package = None ) : \n 
package = \n 
qual_name = \n 
namespace = None \n 
parts = d_string . strip ( ) . split ( , 1 ) \n 
if len ( parts ) > 1 : \n 
~~~ qual_name = parts [ 1 ] \n 
if in parts [ 0 ] : \n 
~~~ package = parts [ 0 ] \n 
~~~ package = % ( _defaultPkgPrefix , parts [ 0 ] ) \n 
~~~ qual_name = d_string \n 
if cur_package is None : \n 
~~~ reg = get_module_registry ( ) \n 
if reg . _current_package is not None : \n 
~~~ package = reg . _current_package . identifier \n 
~~~ package = get_vistrails_basic_pkg_id ( ) \n 
~~~ package = cur_package \n 
~~ ~~ qual_parts = qual_name . rsplit ( , 1 ) \n 
if len ( qual_parts ) > 1 : \n 
~~~ namespace , name = qual_parts \n 
~~~ name = qual_name \n 
~~ return ( package , name , namespace ) \n 
~~ def parse_port_spec_item_string ( spec , cur_package = None ) : \n 
~~~ spec = spec . strip ( ) \n 
spec_arr = spec . split ( , 2 ) \n 
if len ( spec_arr ) > 2 : \n 
~~~ spec = % ( spec_arr [ 0 ] , spec_arr [ 2 ] , spec_arr [ 1 ] ) \n 
~~ return parse_descriptor_string ( spec , cur_package ) \n 
~~ def create_port_spec_item_string ( package , name , namespace = None , \n 
old_style = False ) : \n 
~~~ if old_style : \n 
~~~ if namespace : \n 
~~~ namespace = + namespace \n 
~~ return % ( package , name , namespace ) \n 
~~~ return create_descriptor_string ( package , name , namespace ) \n 
~~ ~~ def parse_port_spec_string ( p_string , cur_package = None ) : \n 
~~~ port_spec = p_string . strip ( ) \n 
if port_spec . startswith ( ) : \n 
~~~ port_spec = port_spec [ 1 : ] \n 
~~ if port_spec . endswith ( ) : \n 
~~~ port_spec = port_spec [ : - 1 ] \n 
~~ if port_spec . strip ( ) == : \n 
~~ specs_list = [ ] \n 
for spec in port_spec . split ( ) : \n 
~~~ specs_list . append ( parse_port_spec_item_string ( spec , \n 
cur_package ) ) \n 
~~ return specs_list \n 
~~ def create_port_spec_string ( specs_list , old_style = False ) : \n 
~~~ spec_items = [ ] \n 
for specs in specs_list : \n 
~~~ if len ( specs ) == 3 : \n 
~~~ pkg , name , ns = specs \n 
~~ elif len ( specs ) == 2 : \n 
~~~ pkg , name = specs \n 
ns = None \n 
~~ spec_items . append ( create_port_spec_item_string ( pkg , name , ns , \n 
old_style ) ) \n 
~~ return % . join ( spec_items ) \n 
~~ def expand_port_spec_string ( p_string , cur_package = None , \n 
~~~ specs_list = parse_port_spec_string ( p_string , cur_package ) \n 
return create_port_spec_string ( specs_list , old_style ) \n 
~~ def make_modules_dict ( * dcts , ** kwargs ) : \n 
namespace = kwargs . pop ( , ) \n 
if kwargs : \n 
~~~ build_namespace = lambda n : % ( namespace , n ) \n 
~~~ build_namespace = lambda n : n \n 
~~ dct = dict ( ) \n 
for d in dcts : \n 
~~~ if not isinstance ( d , dict ) : \n 
~~~ d = { : d } \n 
~~ for subname , sublist in d . iteritems ( ) : \n 
~~~ if subname : \n 
~~~ name = build_namespace ( subname ) \n 
~~~ name = namespace \n 
~~ dct . setdefault ( name , [ ] ) . extend ( sublist ) \n 
~~ ~~ return dct \n 
class UnorderedList ( object ) : \n 
~~~ def __init__ ( self , * elems ) : \n 
~~~ self . _set = set ( elems ) \n 
~~ def __eq__ ( self , other ) : \n 
~~~ return set ( other ) == self . _set \n 
~~~ return % . join ( repr ( e ) for e in self . _set ) \n 
~~ def __repr__ ( self ) : \n 
~~~ return str ( self ) \n 
~~ ~~ class TestModulesDict ( unittest . TestCase ) : \n 
~~~ def test_modulesdict ( self ) : \n 
~~~ ul = UnorderedList \n 
pkg_a_aa = { : [ 1 , 2 ] , : [ 3 ] } \n 
pkg_a_ab = { : [ 4 , 5 ] } \n 
pkg_a = make_modules_dict ( pkg_a_aa , pkg_a_ab , [ 12 , 13 ] , namespace = ) \n 
self . assertEqual ( set ( pkg_a . keys ( ) ) , \n 
set ( [ , , , ] ) ) \n 
pkg_b = { : [ 6 , 7 ] } \n 
glob = [ 10 , 11 ] \n 
more = { : [ 8 , 9 ] } \n 
pkg = make_modules_dict ( pkg_a , pkg_b , glob , more ) \n 
pkg , \n 
{ \n 
: ul ( 10 , 11 ) , \n 
: ul ( 12 , 13 ) , \n 
: ul ( 1 , 2 , 8 , 9 ) , \n 
: ul ( 3 ) , \n 
: ul ( 4 , 5 ) , \n 
: ul ( 6 , 7 ) } ) \n 
from __future__ import division \n 
__all__ = [ , , , \n 
def executable_is_in_path ( filename ) : \n 
pathlist = os . environ [ ] . split ( os . pathsep ) + [ "." ] \n 
for path in pathlist : \n 
~~~ fullpath = os . path . join ( path , filename ) \n 
if os . path . isfile ( fullpath ) : \n 
~~ def list2cmdline ( lst ) : \n 
~~~ for el in lst : \n 
~~~ assert isinstance ( el , basestring ) \n 
~~ return subprocess . list2cmdline ( lst ) \n 
~~ def execute_cmdline ( lst , output ) : \n 
process = subprocess . Popen ( lst , shell = False , \n 
stdin = subprocess . PIPE , \n 
stdout = subprocess . PIPE , \n 
stderr = subprocess . STDOUT , \n 
close_fds = True ) \n 
result = process . wait ( ) \n 
output . extend ( process . stdout . readlines ( ) ) \n 
~~ def get_executable_path ( executable_name ) : \n 
pathlist = os . environ [ ] . split ( os . pathsep ) \n 
~~~ fullpath = os . path . join ( path , executable_name ) \n 
~~~ return os . path . abspath ( fullpath ) \n 
~~ ~~ ~~ def execute_piped_cmdlines ( cmd_list_list ) : \n 
~~~ stdin = subprocess . PIPE \n 
for cmd_list in cmd_list_list : \n 
~~~ cmd_line = list2cmdline ( cmd_list ) \n 
process = subprocess . Popen ( cmd_line , shell = True , \n 
stdin = stdin , \n 
stderr = subprocess . PIPE , \n 
stdin = process . stdout \n 
~~ ( output , errs ) = process . communicate ( ) \n 
result = process . returncode \n 
return ( result , output , errs ) \n 
~~ def execute_cmdline2 ( cmd_list ) : \n 
~~~ return execute_piped_cmdlines ( [ cmd_list ] ) \n 
from vistrails . db . domain import DBPluginData \n 
from vistrails . db . domain import IdScope \n 
import vistrails . core \n 
class PluginData ( DBPluginData ) : \n 
~~~ def __init__ ( self , * args , ** kwargs ) : \n 
~~~ DBPluginData . __init__ ( self , * args , ** kwargs ) \n 
if self . id is None : \n 
~~~ self . id = - 1 \n 
~~ ~~ def __copy__ ( self ) : \n 
~~~ return PluginData . do_copy ( self ) \n 
~~ def do_copy ( self , new_ids = False , id_scope = None , id_remap = None ) : \n 
~~~ cp = DBPluginData . do_copy ( self , new_ids , id_scope , id_remap ) \n 
cp . __class__ = PluginData \n 
def convert ( _plugin_data ) : \n 
~~~ _plugin_data . __class__ = PluginData \n 
~~ id = DBPluginData . db_id \n 
data = DBPluginData . db_data \n 
def __eq__ ( self , other ) : \n 
~~~ if type ( other ) != type ( self ) : \n 
~~ return self . data == other . data \n 
~~ ~~ class TestPluginData ( unittest . TestCase ) : \n 
~~~ def create_data ( self , id = 1 , data = "" ) : \n 
~~~ return PluginData ( id = id , data = data ) \n 
~~ def test_create ( self ) : \n 
~~ def test_serialization ( self ) : \n 
~~~ import vistrails . core . db . io \n 
p_data1 = self . create_data ( ) \n 
xml_str = vistrails . core . db . io . serialize ( p_data1 ) \n 
p_data2 = vistrails . core . db . io . unserialize ( xml_str , PluginData ) \n 
self . assertEquals ( p_data1 , p_data2 ) \n 
self . assertEquals ( p_data1 . id , p_data2 . id ) \n 
from vistrails . db . domain import DBWorkflow , DBAdd , DBDelete , DBAction , DBAbstraction , DBModule , DBConnection , DBPort , DBFunction , DBParameter , DBGroup \n 
from vistrails . db . services . action_chain import getActionChain , getCurrentOperationDict , getCurrentOperations , simplify_ops \n 
from vistrails . db import VistrailsDBException \n 
import getpass \n 
def update_id_scope ( vistrail ) : \n 
~~~ if hasattr ( vistrail , ) : \n 
~~~ vistrail . update_id_scope ( ) \n 
~~~ for action in vistrail . db_actions : \n 
~~~ vistrail . idScope . updateBeginId ( , action . db_id + 1 ) \n 
if action . db_session is not None : \n 
~~~ vistrail . idScope . updateBeginId ( , action . db_session + 1 ) \n 
~~ for operation in action . db_operations : \n 
~~~ vistrail . idScope . updateBeginId ( , operation . db_id + 1 ) \n 
if operation . vtType == or operation . vtType == : \n 
~~~ vistrail . idScope . updateBeginId ( operation . db_what , \n 
getNewObjId ( operation ) + 1 ) \n 
if operation . db_data is None : \n 
~~~ if operation . vtType == : \n 
~~~ operation . db_objectId = operation . db_oldObjId \n 
~~ ~~ vistrail . db_add_object ( operation . db_data ) \n 
~~ ~~ for annotation in action . db_annotations : \n 
~~~ vistrail . idScope . updateBeginId ( , annotation . db_id + 1 ) \n 
~~ ~~ ~~ ~~ def materializeWorkflow ( vistrail , version ) : \n 
~~~ if vistrail . db_has_action_with_id ( version ) : \n 
~~~ workflow = DBWorkflow ( ) \n 
performActions ( getActionChain ( vistrail , version ) , \n 
workflow ) \n 
workflow . db_id = version \n 
workflow . db_vistrailId = vistrail . db_id \n 
return workflow \n 
~~ elif version == 0 : \n 
~~~ return DBWorkflow ( ) \n 
~~ ~~ def performAction ( action , workflow ) : \n 
~~~ if action . actionType == : \n 
~~~ for operation in action . db_operations : \n 
~~~ workflow . db_add_object ( operation . db_data , \n 
operation . db_parentObjType , \n 
operation . db_parentObjId ) \n 
~~ ~~ elif action . actionType == : \n 
~~~ workflow . db_change_object ( operation . db_data , \n 
~~~ for operation in action . operations : \n 
~~~ workflow . db_delete_object ( operation . db_objectId , \n 
operation . db_what , \n 
raise TypeError ( msg ) \n 
~~ ~~ def performDeletes ( deleteOps , workflow ) : \n 
~~~ for operation in deleteOps : \n 
~~~ workflow . db_delete_object ( getOldObjId ( operation ) , operation . db_what , \n 
~~ ~~ def performAdds ( addOps , workflow ) : \n 
~~~ for operation in addOps : \n 
~~ ~~ def performActions ( actions , workflow ) : \n 
~~~ performAdds ( getCurrentOperations ( actions ) , workflow ) \n 
~~ def synchronize ( old_vistrail , new_vistrail , current_action_id ) : \n 
~~~ id_remap = { } \n 
for action in new_vistrail . db_actions : \n 
~~~ if action . is_new : \n 
~~~ new_action = action . do_copy ( True , old_vistrail . idScope , id_remap ) \n 
old_vistrail . db_add_action ( new_action ) \n 
~~~ old_action = old_vistrail . db_actions_id_index [ action . db_id ] \n 
for annotation in action . db_deleted_annotations : \n 
~~~ if old_action . db_has_annotation_with_id ( annotation . db_id ) : \n 
~~~ old_action . db_delete_annotation ( annotation ) \n 
~~~ if old_action . db_has_annotation_with_key ( ) : \n 
~~~ old_annotation = old_action . db_get_annotation_by_key ( ) \n 
old_action . db_delete_annotation ( old_annotation ) \n 
~~ ~~ ~~ for annotation in action . db_annotations : \n 
~~~ if annotation . is_new : \n 
~~~ new_annotation = annotation . do_copy ( True , \n 
old_vistrail . idScope , \n 
id_remap ) \n 
old_action . db_add_annotation ( new_annotation ) \n 
~~ ~~ ~~ ~~ for tag in new_vistrail . db_deleted_tags : \n 
~~~ if old_vistrail . db_has_tag_with_id ( tag . db_id ) : \n 
~~~ old_vistrail . db_delete_tag ( tag ) \n 
~~ ~~ for tag in new_vistrail . db_tags : \n 
~~~ if tag . is_new : \n 
~~~ new_tag = tag . do_copy ( False ) \n 
~~~ new_tag . db_id = id_remap [ ( DBAction . vtType , new_tag . db_id ) ] \n 
~~~ old_tag = old_vistrail . db_tags_name_index [ new_tag . db_name ] \n 
~~~ old_vistrail . db_delete_tag ( old_tag ) \n 
~~~ old_tag = old_vistrail . db_tags_id_index [ new_tag . db_id ] \n 
~~ old_vistrail . db_add_tag ( new_tag ) \n 
~~ ~~ new_action_id = id_remap . get ( ( DBAction . vtType , current_action_id ) , current_action_id ) \n 
old_vistrail . db_currentVersion = new_action_id \n 
return new_action_id \n 
~~ def merge ( sb , next_sb , app = , interactive = False , tmp_dir = , next_tmp_dir = ) : \n 
vt = sb . vistrail \n 
next_vt = next_sb . vistrail \n 
merge_gui = interactive \n 
MergeGUI = merge_gui . MergeGUI if merge_gui else False \n 
skip = 0 \n 
sb . mashups = list ( next_sb . mashups ) \n 
sb . abstractions = list ( next_sb . abstractions ) \n 
id_remap = { } \n 
checkout_key = "__checkout_version_" \n 
action_key = checkout_key + app \n 
annotation_key = action_key + \n 
action_annotation_key = action_key + \n 
checkinId = 0 \n 
if len ( app ) and next_vt . db_has_annotation_with_key ( action_key ) : \n 
~~~ co = next_vt . db_get_annotation_by_key ( action_key ) \n 
checkinId = int ( co . _db_value ) \n 
~~~ actions = [ ] \n 
actionDict = { } \n 
for action in vt . db_actions : \n 
~~~ unique = action . _db_user + str ( action . _db_date ) \n 
copy_no = 0 \n 
while ( unique + str ( copy_no ) ) in actionDict : \n 
~~~ copy_no += 1 \n 
~~ unique = unique + str ( copy_no ) \n 
actions . append ( unique ) \n 
actionDict [ unique ] = action \n 
~~ actionNexts = [ ] \n 
actionDictNext = { } \n 
for action in next_vt . db_actions : \n 
while ( unique + str ( copy_no ) ) in actionDictNext : \n 
actionNexts . append ( unique ) \n 
actionDictNext [ unique ] = action \n 
~~ while checkinId < len ( actions ) and checkinId < len ( actionNexts ) and actions [ checkinId ] == actionNexts [ checkinId ] : \n 
~~~ checkinId += 1 \n 
~~ if checkinId > 0 : \n 
~~~ checkinId = actionDict [ actions [ checkinId - 1 ] ] . db_id \n 
~~ ~~ deletekeys = [ action_key , annotation_key , action_annotation_key ] \n 
for key in deletekeys : \n 
~~~ while vt . db_has_annotation_with_key ( key ) : \n 
~~~ a = vt . db_get_annotation_by_key ( key ) \n 
vt . db_delete_annotation ( a ) \n 
~~ ~~ mergeAnnotations = True \n 
if len ( app ) and next_vt . db_has_annotation_with_key ( annotation_key ) : \n 
~~~ co = next_vt . db_get_annotation_by_key ( annotation_key ) \n 
old_hash = co . _db_value \n 
mergeAnnotations = ( old_hash != vt . hashAnnotations ( ) ) \n 
~~ mergeActionAnnotations = True \n 
if len ( app ) and next_vt . db_has_annotation_with_key ( action_annotation_key ) : \n 
~~~ co = next_vt . db_get_annotation_by_key ( action_annotation_key ) \n 
mergeActionAnnotations = ( old_hash != vt . hashActionAnnotations ( ) ) \n 
~~ for action in next_vt . db_actions : \n 
~~~ if action . _db_id > checkinId : \n 
~~~ new_action = action . do_copy ( True , vt . idScope , id_remap ) \n 
vt . db_add_action ( new_action ) \n 
~~ ~~ if not mergeAnnotations : \n 
~~~ for annotation in [ a for a in vt . db_annotations ] : \n 
~~~ if not next_vt . db_has_annotation_with_id ( annotation . db_id ) : \n 
~~~ vt . db_delete_annotation ( annotation ) \n 
~~ ~~ for annotation in next_vt . db_annotations : \n 
~~~ if not vt . db_has_annotation_with_id ( annotation . db_id ) : \n 
~~~ new_annotation = annotation . do_copy ( True , vt . idScope , id_remap ) \n 
vt . db_add_annotation ( new_annotation ) \n 
~~~ old_annotation = vt . db_get_annotation_by_id ( annotation . db_id ) \n 
if old_annotation . db_key != annotation . db_key : \n 
~~~ old_annotation . db_key = annotation . db_key \n 
~~ if old_annotation . db_value != annotation . db_value : \n 
~~~ old_annotation . db_value = annotation . db_value \n 
~~~ annotations = { } \n 
for annotation in vt . db_annotations : \n 
~~~ if annotation . db_key not in annotations : \n 
~~~ annotations [ annotation . db_key ] = [ ] \n 
~~ if annotation . db_value not in annotations [ annotation . db_key ] : \n 
~~~ annotations [ annotation . db_key ] . append ( annotation . db_value ) \n 
~~~ if annotation . db_key not in annotations or annotation . db_value not in annotations [ annotation . db_key ] : \n 
~~ ~~ ~~ if not mergeActionAnnotations : \n 
~~~ for annotation in [ a for a in vt . db_actionAnnotations ] : \n 
~~~ if not next_vt . db_has_actionAnnotation_with_id ( annotation . db_id ) : \n 
~~~ vt . db_delete_actionAnnotation ( annotation ) \n 
if annotation . db_key == and len ( sb . thumbnails ) > 0 : \n 
~~~ thumb = . join ( sb . thumbnails [ 0 ] . split ( \n 
) [ : - 1 ] ) + + annotation . db_value \n 
if thumb in sb . thumbnails : \n 
~~~ sb . thumbnails . remove ( thumb ) \n 
~~ ~~ ~~ ~~ for annotation in next_vt . db_actionAnnotations : \n 
~~~ if not vt . db_has_actionAnnotation_with_id ( annotation . db_id ) : \n 
~~~ annotation = annotation . do_copy ( True , vt . idScope , id_remap ) \n 
vt . db_add_actionAnnotation ( annotation ) \n 
if annotation . db_key == and len ( next_sb . thumbnails ) > 0 : \n 
~~~ thumb = . join ( next_sb . thumbnails [ 0 ] . split ( \n 
if thumb not in sb . thumbnails : \n 
~~~ sb . thumbnails . append ( thumb ) \n 
~~~ old_annotation = vt . db_get_actionAnnotation_by_id ( annotation . db_id ) \n 
if old_annotation . db_value != annotation . db_value : \n 
~~~ if annotation . db_key == and len ( sb . thumbnails ) > 0 : \n 
) [ : - 1 ] ) + + old_annotation . db_value \n 
~~ ~~ if annotation . db_key == and len ( next_sb . thumbnails ) > 0 : \n 
~~ ~~ old_annotation . db_value = annotation . db_value \n 
old_annotation . db_date = annotation . db_date \n 
old_annotation . db_user = annotation . db_user \n 
~~~ oas = { } \n 
for a in vt . db_actionAnnotations : \n 
~~~ if not a . db_action_id in oas : \n 
~~~ oas [ a . db_action_id ] = { } \n 
~~ if not a . db_key in oas [ a . db_action_id ] : \n 
~~~ oas [ a . db_action_id ] [ a . db_key ] = [ ] \n 
~~ oas [ a . db_action_id ] [ a . db_key ] . append ( a ) \n 
~~ for new_annotation in next_vt . db_actionAnnotations : \n 
~~~ if new_annotation . db_key == : \n 
~~~ value = int ( new_annotation . db_value ) \n 
if ( , value ) in id_remap : \n 
~~~ new_annotation . db_value = str ( id_remap [ ( , value ) ] ) \n 
~~ annotation = new_annotation . do_copy ( True , vt . idScope , id_remap ) \n 
~~ elif new_annotation . db_action_id <= checkinId and new_annotation . db_key in oas [ new_annotation . db_action_id ] : \n 
~~~ old_action = oas [ new_annotation . db_action_id ] \n 
if new_annotation . db_key == : \n 
~~~ old_annotation = old_action [ new_annotation . db_key ] [ 0 ] \n 
if old_annotation . db_value != new_annotation . db_value : \n 
if interactive : \n 
~~~ if skip == 1 : \n 
~~ elif skip == 2 : \n 
~~~ old_annotation . db_value = new_annotation . db_value \n 
old_annotation . db_date = new_annotation . db_date \n 
old_annotation . db_user = new_annotation . db_user \n 
~~~ v , value = MergeGUI . resolveTags ( \n 
old_annotation , new_annotation , value ) \n 
if v == merge_gui . CHOICE_OTHER_ALL : \n 
~~~ skip = 1 \n 
~~ elif v == merge_gui . CHOICE_OTHER : \n 
~~ elif v == merge_gui . CHOICE_RESOLVED : \n 
~~~ old_annotation . db_value = value \n 
~~ elif v == merge_gui . CHOICE_OWN : \n 
~~ elif v == merge_gui . CHOICE_OWN_ALL : \n 
skip = 2 \n 
~~ ~~ ~~ elif new_annotation . db_key == : \n 
if new_annotation . db_value != old_annotation . db_value : \n 
old_annotation . db_value , str ( new_annotation . db_date ) , \n 
new_annotation . db_user , new_annotation . db_value ) \n 
~~~ v , value = MergeGUI . resolveNotes ( \n 
~~~ if interactive : \n 
~~ old_annotation . db_value = new_annotation . db_value \n 
thumb = . join ( next_sb . thumbnails [ 0 ] . split ( \n 
) [ : - 1 ] ) + + new_annotation . db_value \n 
~~~ v = MergeGUI . resolveThumbs ( old_annotation , \n 
new_annotation , tmp_dir , next_tmp_dir ) \n 
~~ elif v in ( merge_gui . CHOICE_OWN , \n 
merge_gui . CHOICE_OWN_ALL ) : \n 
~~ if v == merge_gui . CHOICE_OWN_ALL : \n 
~~~ skip = 2 \n 
~~~ values = [ ] \n 
for old_annotation in old_action [ new_annotation . db_key ] : \n 
~~~ values . append ( old_annotation . db_value ) \n 
~~ if new_annotation . db_value not in values : \n 
~~~ annotation = new_annotation . do_copy ( True , vt . idScope , id_remap ) \n 
if annotation . db_key == : \n 
~~~ thumb = . join ( next_sb . thumbnails [ 0 ] . split ( ) [ : - 1 ] ) + + annotation . db_value \n 
~~ ~~ ~~ ~~ ~~ if len ( app ) : \n 
~~~ vt . update_checkout_version ( app ) \n 
~~ ~~ def find_data ( what , id , op_dict ) : \n 
~~~ return op_dict [ ( what , id ) ] . db_data \n 
~~~ msg = % ( what , id ) \n 
raise KeyError ( msg ) \n 
~~ ~~ def invertOperations ( op_dict , adds , deletes , do_copy = False ) : \n 
~~~ inverse_ops = [ ] \n 
deletes . reverse ( ) \n 
for op in deletes : \n 
~~~ data = find_data ( op . db_what , getOldObjId ( op ) , op_dict ) \n 
if do_copy : \n 
~~~ data = copy . copy ( data ) \n 
~~ inv_op = DBAdd ( id = - 1 , \n 
what = op . db_what , \n 
objectId = getOldObjId ( op ) , \n 
parentObjId = op . db_parentObjId , \n 
parentObjType = op . db_parentObjType , \n 
data = data \n 
inverse_ops . append ( inv_op ) \n 
~~ adds . reverse ( ) \n 
for op in adds : \n 
~~~ inv_op = DBDelete ( id = - 1 , \n 
objectId = getNewObjId ( op ) , \n 
~~ return inverse_ops \n 
~~ def normalOperations ( adds , deletes , do_copy = False ) : \n 
~~~ new_ops = [ ] \n 
~~~ new_op = DBDelete ( id = - 1 , \n 
new_ops . append ( new_op ) \n 
~~ for op in adds : \n 
~~~ data = op . db_data \n 
~~~ data = copy . copy ( op . db_data ) \n 
~~ new_op = DBAdd ( id = - 1 , \n 
data = data ) \n 
~~ return new_ops \n 
~~ def getPathAsAction ( vistrail , v1 , v2 , do_copy = False ) : \n 
~~~ sharedRoot = getSharedRoot ( vistrail , [ v1 , v2 ] ) \n 
sharedActionChain = getActionChain ( vistrail , sharedRoot ) \n 
sharedOperationDict = getCurrentOperationDict ( sharedActionChain ) \n 
v1Actions = getActionChain ( vistrail , v1 , sharedRoot ) \n 
v2Actions = getActionChain ( vistrail , v2 , sharedRoot ) \n 
( v1AddDict , v1DeleteDict ) = getOperationDiff ( v1Actions , \n 
sharedOperationDict ) \n 
( v2AddDict , v2DeleteDict ) = getOperationDiff ( v2Actions , \n 
v1Adds = v1AddDict . values ( ) \n 
v1Deletes = v1DeleteDict . values ( ) \n 
v1InverseOps = invertOperations ( sharedOperationDict , v1Adds , v1Deletes , do_copy ) \n 
v2Adds = v2AddDict . values ( ) \n 
v2Deletes = v2DeleteDict . values ( ) \n 
v2Ops = normalOperations ( v2Adds , v2Deletes , do_copy ) \n 
allOps = v1InverseOps + v2Ops \n 
simplifiedOps = simplify_ops ( allOps ) \n 
return DBAction ( id = - 1 , \n 
operations = simplifiedOps , \n 
~~ def addAndFixActions ( startDict , actions ) : \n 
~~~ curDict = copy . copy ( startDict ) \n 
for action in actions : \n 
for op in action . db_operations : \n 
~~~ if op . vtType == : \n 
~~~ if op . db_parentObjId is None or curDict . has_key ( ( op . db_parentObjType , \n 
op . db_parentObjId ) ) : \n 
~~~ curDict [ ( op . db_what , op . db_objectId ) ] = op \n 
new_ops . append ( op ) \n 
~~ ~~ elif op . vtType == : \n 
~~~ if curDict . has_key ( ( op . db_what , op . db_oldObjId ) ) and ( op . db_parentObjId is None or curDict . has_key ( ( op . db_parentObjType , \n 
op . db_parentObjId ) ) ) : \n 
~~~ del curDict [ ( op . db_what , op . db_oldObjId ) ] \n 
curDict [ ( op . db_what , op . db_newObjId ) ] = op \n 
~~~ if ( op . db_parentObjId is None or \n 
curDict . has_key ( ( op . db_parentObjType , \n 
op . db_parentObjId ) ) ) and curDict . has_key ( ( op . db_what , op . db_objectId ) ) : \n 
~~~ del curDict [ ( op . db_what , op . db_objectId ) ] \n 
~~ ~~ ~~ action . db_operations = new_ops \n 
~~ return curDict \n 
~~ def fixActions ( vistrail , v , actions ) : \n 
~~~ startingChain = getActionChain ( vistrail , v ) \n 
startingDict = getCurrentOperationDict ( startingChain ) \n 
addAndFixActions ( startingDict , actions ) \n 
~~ def getSharedRoot ( vistrail , versions ) : \n 
~~~ current = copy . copy ( versions ) \n 
while 0 not in current : \n 
~~~ maxId = max ( current ) \n 
if current . count ( maxId ) == len ( current ) : \n 
~~~ return maxId \n 
~~~ newId = vistrail . db_get_action_by_id ( maxId ) . db_prevId \n 
for i , v in enumerate ( current ) : \n 
~~~ if v == maxId : \n 
~~~ current [ i ] = newId \n 
~~ ~~ ~~ ~~ return 0 \n 
~~ def getOperationDiff ( actions , operationDict ) : \n 
~~~ addDict = { } \n 
deleteDict = { } \n 
~~~ addDict [ ( operation . db_what , \n 
operation . db_objectId ) ] = operation \n 
~~ elif operation . vtType == : \n 
~~~ if operationDict . has_key ( ( operation . db_what , \n 
operation . db_objectId ) ) : \n 
~~~ deleteDict [ ( operation . db_what , \n 
~~ elif addDict . has_key ( ( operation . db_what , \n 
~~~ del addDict [ ( operation . db_what , \n 
operation . db_objectId ) ] \n 
~~ ~~ elif operation . vtType == : \n 
operation . db_oldObjId ) ) : \n 
operation . db_oldObjId ) ] = operation \n 
~~~ del addDict [ ( operation . db_what , operation . db_oldObjId ) ] \n 
~~ addDict [ ( operation . db_what , \n 
operation . db_newObjId ) ] = operation \n 
~~ ~~ ~~ return ( addDict , deleteDict ) \n 
~~ def updateOperationDict ( operationDict , deleteOps , addOps ) : \n 
~~~ if operationDict . has_key ( ( operation . db_what , getOldObjId ( operation ) ) ) : \n 
~~~ del operationDict [ ( operation . db_what , getOldObjId ( operation ) ) ] \n 
~~ ~~ for operation in addOps : \n 
~~~ operationDict [ ( operation . db_what , getNewObjId ( operation ) ) ] = operation \n 
~~ return operationDict \n 
~~ def getObjects ( actions ) : \n 
~~~ objects = { } \n 
~~~ if not objects . has_key ( operation . db_what ) : \n 
~~~ objects [ operation . db_what ] = [ ] \n 
~~ object = copy . copy ( operation . db_data ) \n 
objects [ operation . db_what ] . append ( object ) \n 
~~ ~~ return objects \n 
~~ def getVersionDifferences ( vistrail , versions ) : \n 
~~~ sharedRoot = getSharedRoot ( vistrail , versions ) \n 
vOnlySorted = [ ] \n 
for v in versions : \n 
~~~ vActions = getActionChain ( vistrail , v , sharedRoot ) \n 
( vAddDict , vDeleteDict ) = getOperationDiff ( vActions , \n 
vOnlyAdds = vAddDict . values ( ) \n 
vOnlyAdds . sort ( key = lambda x : x . db_id ) \n 
vOnlyDeletes = vDeleteDict . values ( ) \n 
vOnlyDeletes . sort ( key = lambda x : x . db_id ) \n 
vOpDict = copy . copy ( sharedOperationDict ) \n 
updateOperationDict ( vOpDict , vOnlyDeletes , vOnlyAdds ) \n 
vOps = vOpDict . values ( ) \n 
vOps . sort ( key = lambda x : x . db_id ) \n 
vOnlySorted . append ( ( vOnlyAdds , vOnlyDeletes , vOps ) ) \n 
~~ sharedOps = sharedOperationDict . values ( ) \n 
sharedOps . sort ( key = lambda x : x . db_id ) \n 
return ( sharedOps , vOnlySorted ) \n 
~~ def heuristicModuleMatch ( m1 , m2 ) : \n 
if m1 . db_name == m2 . db_name and m1 . db_namespace == m2 . db_namespace and m1 . db_package == m2 . db_package : \n 
~~~ if m1 . vtType == : \n 
~~~ m1_desc = None \n 
m2_desc = None \n 
if in m1 . db_annotations_key_index : \n 
~~~ m1_desc = m1 . db_annotations_key_index [ ] \n 
~~ if in m2 . db_annotations_key_index : \n 
~~~ m2_desc = m2 . db_annotations_key_index [ ] \n 
~~ if not ( m1_desc and m2_desc and m1_desc == m2_desc ) : \n 
~~ ~~ m1_functions = copy . copy ( m1 . db_get_functions ( ) ) \n 
m2_functions = copy . copy ( m2 . db_get_functions ( ) ) \n 
if len ( m1_functions ) != len ( m2_functions ) : \n 
~~ for f1 in m1_functions [ : ] : \n 
~~~ match = None \n 
for f2 in m2_functions : \n 
~~~ isMatch = heuristicFunctionMatch ( f1 , f2 ) \n 
if isMatch == 1 : \n 
~~~ match = f2 \n 
~~ ~~ if match is not None : \n 
~~~ m1_functions . remove ( f1 ) \n 
m2_functions . remove ( f2 ) \n 
~~ ~~ m1_cparams = copy . copy ( m1 . db_get_controlParameters ( ) ) \n 
m2_cparams = copy . copy ( m2 . db_get_controlParameters ( ) ) \n 
if len ( m1_cparams ) != len ( m2_cparams ) : \n 
~~ for cp1 in m1_cparams [ : ] : \n 
for cp2 in m2_cparams : \n 
~~~ isMatch = heuristicControlParameterMatch ( cp1 , cp2 ) \n 
~~~ match = cp2 \n 
~~~ m1_cparams . remove ( cp1 ) \n 
m2_cparams . remove ( cp2 ) \n 
~~ ~~ m1_annots = copy . copy ( m1 . db_get_annotations ( ) ) \n 
m2_annots = copy . copy ( m2 . db_get_annotations ( ) ) \n 
if len ( m1_annots ) != len ( m2_annots ) : \n 
~~ for a1 in m1_annots [ : ] : \n 
for a2 in m2_annots : \n 
~~~ isMatch = heuristicAnnotationMatch ( a1 , a2 ) \n 
~~~ match = a2 \n 
~~~ m1_annots . remove ( a1 ) \n 
m2_annots . remove ( a2 ) \n 
~~ ~~ if len ( m1_functions ) == len ( m2_functions ) == len ( m1_cparams ) == len ( m2_cparams ) == len ( m1_annots ) == len ( m2_annots ) == 0 : \n 
~~~ return 1 \n 
~~ ~~ return - 1 \n 
~~ def heuristicFunctionMatch ( f1 , f2 ) : \n 
if f1 . db_name == f2 . db_name : \n 
~~~ f1_parameters = copy . copy ( f1 . db_get_parameters ( ) ) \n 
f2_parameters = copy . copy ( f2 . db_get_parameters ( ) ) \n 
if len ( f1_parameters ) != len ( f2_parameters ) : \n 
~~ for p1 in f1_parameters [ : ] : \n 
for p2 in f2_parameters : \n 
~~~ isMatch = heuristicParameterMatch ( p1 , p2 ) \n 
~~~ match = p2 \n 
~~~ f1_parameters . remove ( p1 ) \n 
f2_parameters . remove ( match ) \n 
~~ ~~ if len ( f1_parameters ) == len ( f2_parameters ) == 0 : \n 
~~ def heuristicParameterMatch ( p1 , p2 ) : \n 
if p1 . db_type == p2 . db_type and p1 . db_pos == p2 . db_pos : \n 
~~~ if p1 . db_val == p2 . db_val : \n 
~~ def heuristicControlParameterMatch ( cp1 , cp2 ) : \n 
if cp1 . db_name == cp2 . db_name : \n 
~~~ if cp1 . db_value == cp2 . db_value : \n 
~~ def heuristicAnnotationMatch ( a1 , a2 ) : \n 
if a1 . db_key == a2 . db_key : \n 
~~~ if a1 . db_value == a2 . db_value : \n 
~~ def heuristicConnectionMatch ( c1 , c2 ) : \n 
c1_ports = copy . copy ( c1 . db_get_ports ( ) ) \n 
c2_ports = copy . copy ( c2 . db_get_ports ( ) ) \n 
for p1 in c1_ports [ : ] : \n 
for p2 in c2_ports : \n 
~~~ isMatch = heuristicPortMatch ( p1 , p2 ) \n 
~~ elif isMatch == 0 : \n 
~~~ c1_ports . remove ( p1 ) \n 
c2_ports . remove ( match ) \n 
~~ ~~ if len ( c1_ports ) == len ( c2_ports ) == 0 : \n 
~~ return - 1 \n 
~~ def heuristicPortMatch ( p1 , p2 ) : \n 
if p1 . db_moduleId == p2 . db_moduleId : \n 
~~ elif p1 . db_type == p2 . db_type and p1 . db_moduleName == p2 . db_moduleName and p1 . sig == p2 . sig : \n 
~~ def function_sig ( function ) : \n 
~~~ return ( function . db_name , \n 
[ ( param . db_type , param . db_val ) \n 
for param in function . db_get_parameters ( ) ] ) \n 
~~ def getParamChanges ( m1 , m2 , same_vt = True , heuristic_match = True ) : \n 
~~~ paramChanges = [ ] \n 
m1_functions = m1 . db_get_functions ( ) \n 
m2_functions = m2 . db_get_functions ( ) \n 
m1_unmatched = [ ] \n 
m2_unmatched = [ ] \n 
if same_vt : \n 
~~~ for f1 in m1_functions : \n 
~~~ f2 = m2 . db_get_function ( f1 . db_id ) \n 
if f2 is None : \n 
~~~ m1_unmatched . append ( f1 ) \n 
~~~ if heuristic_match : \n 
~~~ matchValue = heuristicFunctionMatch ( f1 , f2 ) \n 
if matchValue != 1 : \n 
~~~ paramChanges . append ( ( function_sig ( f1 ) , \n 
function_sig ( f2 ) ) ) \n 
~~~ paramChanges . append ( ( function_sig ( f1 ) , function_sig ( f2 ) ) ) \n 
~~ ~~ ~~ for f2 in m2_functions : \n 
~~~ if m1 . db_get_function ( f2 . db_id ) is None : \n 
~~~ m2_unmatched . append ( f2 ) \n 
~~~ m1_unmatched . extend ( m1_functions ) \n 
m2_unmatched . extend ( m2_functions ) \n 
~~ if len ( m1_unmatched ) + len ( m2_unmatched ) > 0 : \n 
~~~ if heuristic_match and len ( m1_unmatched ) > 0 and len ( m2_unmatched ) > 0 : \n 
~~~ for f1 in m1_unmatched [ : ] : \n 
~~~ matched = False \n 
matchValue = 0 \n 
for f2 in m2_unmatched : \n 
if matchValue == 1 : \n 
~~~ matched = f1 \n 
~~ elif matchValue == 0 : \n 
~~ ~~ if matched : \n 
~~~ if matchValue != 1 : \n 
~~ m1_unmatched . remove ( f1 ) \n 
m2_unmatched . remove ( f2 ) \n 
~~ ~~ ~~ for f in m1_unmatched : \n 
~~~ paramChanges . append ( ( function_sig ( f ) , ( None , None ) ) ) \n 
~~ for f in m2_unmatched : \n 
~~~ paramChanges . append ( ( ( None , None ) , function_sig ( f ) ) ) \n 
~~ ~~ return paramChanges \n 
~~ def getCParamChanges ( m1 , m2 , same_vt = True , heuristic_match = True ) : \n 
~~~ cparamChanges = [ ] \n 
m1_cparams = m1 . db_get_controlParameters ( ) \n 
m2_cparams = m2 . db_get_controlParameters ( ) \n 
~~~ for cp1 in m1_cparams : \n 
~~~ cp2 = m2 . db_get_controlParameter ( cp1 . db_id ) \n 
if cp2 is None : \n 
~~~ m1_unmatched . append ( cp1 ) \n 
~~~ matchValue = heuristicControlParameterMatch ( cp1 , cp2 ) \n 
~~~ cparamChanges . append ( ( ( cp1 . db_name , cp1 . db_value ) , \n 
( cp2 . db_name , cp2 . db_value ) ) ) \n 
~~ ~~ ~~ for cp2 in m2_cparams : \n 
~~~ if m1 . db_get_controlParameter ( cp2 . db_id ) is None : \n 
~~~ m2_unmatched . append ( cp2 ) \n 
~~~ m1_unmatched . extend ( m1_cparams ) \n 
m2_unmatched . extend ( m2_cparams ) \n 
~~~ for cp1 in m1_unmatched [ : ] : \n 
for cp2 in m2_unmatched : \n 
~~~ matched = cp1 \n 
~~ m1_unmatched . remove ( cp1 ) \n 
m2_unmatched . remove ( cp2 ) \n 
~~ ~~ ~~ for cp in m1_unmatched : \n 
~~~ cparamChanges . append ( ( ( cp . db_name , cp . db_value ) , ( None , None ) ) ) \n 
~~ for cp in m2_unmatched : \n 
~~~ cparamChanges . append ( ( ( None , None ) , ( cp . db_name , cp . db_value ) ) ) \n 
~~ ~~ return cparamChanges \n 
~~ def getAnnotationChanges ( m1 , m2 , same_vt = True , heuristic_match = True ) : \n 
~~~ annotChanges = [ ] \n 
m1_annots = m1 . db_get_annotations ( ) \n 
m2_annots = m2 . db_get_annotations ( ) \n 
~~~ for a1 in m1_annots : \n 
~~~ a2 = m2 . db_get_annotation ( a1 . db_id ) \n 
if a2 is None : \n 
~~~ m1_unmatched . append ( a1 ) \n 
~~~ matchValue = heuristicAnnotationMatch ( a1 , a2 ) \n 
~~~ annotChanges . append ( ( ( a1 . db_key , a1 . db_value ) , \n 
( a2 . db_key , a2 . db_value ) ) ) \n 
~~ ~~ ~~ for a2 in m2_annots : \n 
~~~ if m1 . db_get_annotation ( a2 . db_id ) is None : \n 
~~~ m2_unmatched . append ( a2 ) \n 
~~~ m1_unmatched . extend ( m1_annots ) \n 
m2_unmatched . extend ( m2_annots ) \n 
~~~ for a1 in m1_unmatched [ : ] : \n 
for a2 in m2_unmatched : \n 
~~~ matched = a1 \n 
~~ m1_unmatched . remove ( a1 ) \n 
m2_unmatched . remove ( a2 ) \n 
~~~ annotChanges . append ( ( ( cp . db_key , cp . db_value ) , ( None , None ) ) ) \n 
~~~ annotChanges . append ( ( ( None , None ) , ( cp . db_key , cp . db_value ) ) ) \n 
~~ ~~ return annotChanges \n 
~~ def getOldObjId ( operation ) : \n 
~~~ return operation . db_oldObjId \n 
~~ return operation . db_objectId \n 
~~ def getNewObjId ( operation ) : \n 
~~~ return operation . db_newObjId \n 
~~ def setOldObjId ( operation , id ) : \n 
~~~ operation . db_oldObjId = id \n 
~~~ operation . db_objectId = id \n 
~~ ~~ def setNewObjId ( operation , id ) : \n 
~~~ operation . db_newObjId = id \n 
~~ ~~ def getWorkflowDiffCommon ( vistrail , v1 , v2 , heuristic_match = True ) : \n 
~~~ ( sharedOps , vOnlyOps ) = getVersionDifferences ( vistrail , [ v1 , v2 ] ) \n 
sharedWorkflow = DBWorkflow ( ) \n 
performAdds ( sharedOps , sharedWorkflow ) \n 
v1Workflow = DBWorkflow ( ) \n 
v1Ops = vOnlyOps [ 0 ] [ 2 ] \n 
performAdds ( v1Ops , v1Workflow ) \n 
v2Workflow = DBWorkflow ( ) \n 
v2Ops = vOnlyOps [ 1 ] [ 2 ] \n 
performAdds ( v2Ops , v2Workflow ) \n 
sharedModuleIds = [ ] \n 
sharedConnectionIds = [ ] \n 
sharedFunctionIds = { } \n 
sharedCParameterIds = { } \n 
sharedAnnotationIds = { } \n 
for op in sharedOps : \n 
~~~ if op . what == or op . what == or op . what == : \n 
~~~ sharedModuleIds . append ( getNewObjId ( op ) ) \n 
~~ elif op . what == : \n 
~~~ sharedConnectionIds . append ( getNewObjId ( op ) ) \n 
~~~ sharedFunctionIds [ getNewObjId ( op ) ] = op . db_parentObjId \n 
~~~ sharedCParameterIds [ getNewObjId ( op ) ] = op . db_parentObjId \n 
~~~ sharedAnnotationIds [ getNewObjId ( op ) ] = op . db_parentObjId \n 
~~ ~~ vOnlyModules = [ ] \n 
vOnlyConnections = [ ] \n 
paramChgModules = { } \n 
cparamChgModules = { } \n 
annotChgModules = { } \n 
for ( vAdds , vDeletes , _ ) in vOnlyOps : \n 
~~~ moduleDeleteIds = [ ] \n 
connectionDeleteIds = [ ] \n 
for op in vDeletes : \n 
~~~ moduleDeleteIds . append ( getOldObjId ( op ) ) \n 
if getOldObjId ( op ) in sharedModuleIds : \n 
~~~ sharedModuleIds . remove ( getOldObjId ( op ) ) \n 
~~ if paramChgModules . has_key ( getOldObjId ( op ) ) : \n 
~~~ del paramChgModules [ getOldObjId ( op ) ] \n 
~~ if cparamChgModules . has_key ( getOldObjId ( op ) ) : \n 
~~~ del cparamChgModules [ getOldObjId ( op ) ] \n 
~~ if annotChgModules . has_key ( getOldObjId ( op ) ) : \n 
~~~ del annotChgModules [ getOldObjId ( op ) ] \n 
~~ ~~ elif op . what == and ( op . db_parentObjType == or \n 
op . db_parentObjType == or \n 
op . db_parentObjType == ) and op . db_parentObjId in sharedModuleIds : \n 
~~~ paramChgModules [ op . db_parentObjId ] = None \n 
sharedModuleIds . remove ( op . db_parentObjId ) \n 
~~ elif op . what == and op . db_parentObjType == and sharedFunctionIds . has_key ( op . db_parentObjId ) : \n 
~~~ moduleId = sharedFunctionIds [ op . db_parentObjId ] \n 
if moduleId in sharedModuleIds : \n 
~~~ paramChgModules [ moduleId ] = None \n 
sharedModuleIds . remove ( moduleId ) \n 
op . db_parentObjType == ) and op . db_parentObjId in sharedCParameterIds and op . db_parentObjId in sharedModuleIds : \n 
~~~ cparamChgModules [ op . db_parentObjId ] = None \n 
~~ elif op . what == and ( op . db_parentObjType == or \n 
op . db_parentObjType == ) and op . db_parentObjId in sharedAnnotationIds and op . db_parentObjId in sharedModuleIds : \n 
~~~ annotChgModules [ op . db_parentObjId ] = None \n 
~~~ connectionDeleteIds . append ( getOldObjId ( op ) ) \n 
if getOldObjId ( op ) in sharedConnectionIds : \n 
~~~ sharedConnectionIds . remove ( getOldObjId ( op ) ) \n 
~~ ~~ ~~ moduleAddIds = [ ] \n 
connectionAddIds = [ ] \n 
for op in vAdds : \n 
~~~ moduleAddIds . append ( getNewObjId ( op ) ) \n 
~~ elif ( op . what == and \n 
( op . db_parentObjType == or \n 
op . db_parentObjType == ) and \n 
op . db_parentObjId in sharedModuleIds ) : \n 
~~ ~~ elif ( op . what == and \n 
op . db_parentObjId in sharedCParameterIds and \n 
op . db_parentObjId in sharedAnnotationIds and \n 
~~~ connectionAddIds . append ( getOldObjId ( op ) ) \n 
~~ ~~ vOnlyModules . append ( ( moduleAddIds , moduleDeleteIds ) ) \n 
vOnlyConnections . append ( ( connectionAddIds , connectionDeleteIds ) ) \n 
~~ sharedModulePairs = [ ( id , id ) for id in sharedModuleIds ] \n 
v1Only = vOnlyModules [ 0 ] [ 0 ] \n 
v2Only = vOnlyModules [ 1 ] [ 0 ] \n 
for id in vOnlyModules [ 1 ] [ 1 ] : \n 
~~~ if id not in vOnlyModules [ 0 ] [ 1 ] : \n 
~~~ v1Only . append ( id ) \n 
~~ ~~ for id in vOnlyModules [ 0 ] [ 1 ] : \n 
~~~ if id not in vOnlyModules [ 1 ] [ 1 ] : \n 
~~~ v2Only . append ( id ) \n 
~~ ~~ sharedConnectionPairs = [ ( id , id ) for id in sharedConnectionIds ] \n 
c1Only = vOnlyConnections [ 0 ] [ 0 ] \n 
c2Only = vOnlyConnections [ 1 ] [ 0 ] \n 
for id in vOnlyConnections [ 1 ] [ 1 ] : \n 
~~~ if id not in vOnlyConnections [ 0 ] [ 1 ] : \n 
~~~ c1Only . append ( id ) \n 
~~ ~~ for id in vOnlyConnections [ 0 ] [ 1 ] : \n 
~~~ if id not in vOnlyConnections [ 1 ] [ 1 ] : \n 
~~~ c2Only . append ( id ) \n 
~~ ~~ paramChgModulePairs = [ ( id , id ) for id in paramChgModules . keys ( ) ] \n 
cparamChgModulePairs = [ ( id , id ) for id in cparamChgModules . keys ( ) ] \n 
annotChgModulePairs = [ ( id , id ) for id in annotChgModules . keys ( ) ] \n 
c1Only , c2Only , heuristicConnectionPairs = [ ] , [ ] , [ ] \n 
if heuristic_match : \n 
~~~ ( heuristicModulePairs , heuristicConnectionPairs , v1Only , v2Only , c1Only , c2Only ) = do_heuristic_diff ( v1Workflow , v2Workflow , v1Only , v2Only , c1Only , c2Only ) \n 
paramChgModulePairs . extend ( heuristicModulePairs ) \n 
cparamChgModulePairs . extend ( heuristicModulePairs ) \n 
annotChgModulePairs . extend ( heuristicModulePairs ) \n 
~~ allChgModulePairs = list ( set ( chain ( paramChgModulePairs , \n 
cparamChgModulePairs , \n 
annotChgModulePairs ) ) ) \n 
( heuristicModulePairs , paramChanges , cparam_changes , annot_changes ) = check_params_diff ( v1Workflow , v2Workflow , allChgModulePairs , \n 
True , heuristic_match ) \n 
return ( v1Workflow , v2Workflow , \n 
sharedModulePairs , heuristicModulePairs , v1Only , v2Only , \n 
paramChanges , cparam_changes , annot_changes , \n 
sharedConnectionPairs , heuristicConnectionPairs , \n 
c1Only , c2Only ) \n 
~~ def do_heuristic_diff ( v1Workflow , v2Workflow , v1_modules , v2_modules , \n 
v1_connections , v2_connections ) : \n 
~~~ heuristicModulePairs = [ ] \n 
heuristicConnectionPairs = [ ] \n 
v1Only = copy . copy ( v1_modules ) \n 
v2Only = copy . copy ( v2_modules ) \n 
c1Only = copy . copy ( v1_connections ) \n 
c2Only = copy . copy ( v2_connections ) \n 
for m1_id in v1Only [ : ] : \n 
~~~ m1 = v1Workflow . db_get_module ( m1_id ) \n 
match = None \n 
for m2_id in v2Only : \n 
~~~ m2 = v2Workflow . db_get_module ( m2_id ) \n 
isMatch = heuristicModuleMatch ( m1 , m2 ) \n 
~~~ match = ( m1_id , m2_id ) \n 
~~~ v1Only . remove ( match [ 0 ] ) \n 
v2Only . remove ( match [ 1 ] ) \n 
heuristicModulePairs . append ( match ) \n 
~~ ~~ for c1_id in c1Only [ : ] : \n 
~~~ c1 = v1Workflow . db_get_connection ( c1_id ) \n 
for c2_id in c2Only : \n 
~~~ c2 = v2Workflow . db_get_connection ( c2_id ) \n 
isMatch = heuristicConnectionMatch ( c1 , c2 ) \n 
~~~ match = ( c1_id , c2_id ) \n 
~~~ c1Only . remove ( match [ 0 ] ) \n 
c2Only . remove ( match [ 1 ] ) \n 
heuristicConnectionPairs . append ( match ) \n 
~~ ~~ return ( heuristicModulePairs , heuristicConnectionPairs , v1Only , v2Only , \n 
~~ def check_params_diff ( v1Workflow , v2Workflow , paramChgModulePairs , \n 
same_vt = True , heuristic_match = True ) : \n 
~~~ matched = [ ] \n 
paramChanges = [ ] \n 
cparamChanges = [ ] \n 
annotChanges = [ ] \n 
for ( m1_id , m2_id ) in paramChgModulePairs : \n 
m2 = v2Workflow . db_get_module ( m2_id ) \n 
moduleParamChanges = getParamChanges ( m1 , m2 , same_vt , heuristic_match ) \n 
if len ( moduleParamChanges ) > 0 : \n 
~~~ paramChanges . append ( ( ( m1_id , m2_id ) , moduleParamChanges ) ) \n 
~~ moduleCParamChanges = getCParamChanges ( m1 , m2 , same_vt , \n 
heuristic_match ) \n 
if len ( moduleCParamChanges ) > 0 : \n 
~~~ cparamChanges . append ( ( ( m1_id , m2_id ) , moduleCParamChanges ) ) \n 
~~ moduleAnnotChanges = getAnnotationChanges ( m1 , m2 , same_vt , \n 
if len ( moduleAnnotChanges ) > 0 : \n 
~~~ annotChanges . append ( ( ( m1_id , m2_id ) , moduleAnnotChanges ) ) \n 
~~ if len ( moduleParamChanges ) == len ( moduleCParamChanges ) == len ( moduleAnnotChanges ) == 0 : \n 
~~~ matched . append ( ( m1_id , m2_id ) ) \n 
~~ ~~ return ( matched , paramChanges , cparamChanges , annotChanges ) \n 
~~ def getWorkflowDiff ( vt_pair_1 , vt_pair_2 , heuristic_match = True ) : \n 
~~~ ( vistrail_1 , v_1 ) = vt_pair_1 \n 
( vistrail_2 , v_2 ) = vt_pair_2 \n 
if vistrail_1 == vistrail_2 : \n 
~~~ return getWorkflowDiffCommon ( vistrail_1 , v_1 , v_2 , heuristic_match ) \n 
~~ workflow_1 = materializeWorkflow ( vistrail_1 , v_1 ) \n 
workflow_2 = materializeWorkflow ( vistrail_2 , v_2 ) \n 
modules_1 = workflow_1 . db_modules_id_index . keys ( ) \n 
modules_2 = workflow_2 . db_modules_id_index . keys ( ) \n 
conns_1 = workflow_1 . db_connections_id_index . keys ( ) \n 
conns_2 = workflow_2 . db_connections_id_index . keys ( ) \n 
~~~ ( m_matches , c_matches , modules_1 , modules_2 , conns_1 , conns_2 ) = do_heuristic_diff ( workflow_1 , workflow_2 , modules_1 , modules_2 , conns_1 , conns_2 ) \n 
( m_matches , param_changes , cparam_changes , annot_changes ) = check_params_diff ( workflow_1 , workflow_2 , \n 
m_matches , False , \n 
return ( workflow_1 , workflow_2 , [ ] , m_matches , modules_1 , modules_2 , \n 
param_changes , cparam_changes , annot_changes , [ ] , c_matches , conns_1 , conns_2 ) \n 
~~ return ( workflow_1 , workflow_2 , [ ] , [ ] , modules_1 , modules_2 , [ ] , [ ] , [ ] , [ ] , [ ] , \n 
conns_1 , conns_2 ) \n 
~~ class TestDBVistrailService ( unittest . TestCase ) : \n 
~~~ def test_parameter_heuristic ( self ) : \n 
param1 = ModuleParam ( id = 0 , pos = 0 , type = , val = ) \n 
param2 = ModuleParam ( id = 1 , pos = 0 , type = , val = ) \n 
param3 = ModuleParam ( id = 2 , pos = 1 , type = , val = ) \n 
param4 = ModuleParam ( id = 3 , pos = 0 , type = , val = ) \n 
param5 = ModuleParam ( id = 4 , pos = 1 , type = , val = ) \n 
assert heuristicParameterMatch ( param1 , param2 ) == 1 \n 
assert heuristicParameterMatch ( param1 , param3 ) == - 1 \n 
assert heuristicParameterMatch ( param1 , param4 ) == 0 \n 
assert heuristicParameterMatch ( param1 , param5 ) == - 1 \n 
~~ def test_function_heuristic ( self ) : \n 
param2 = ModuleParam ( id = 1 , pos = 1 , type = , val = ) \n 
param3 = ModuleParam ( id = 2 , pos = 0 , type = , val = ) \n 
param4 = ModuleParam ( id = 3 , pos = 1 , type = , val = ) \n 
param5 = ModuleParam ( id = 4 , pos = 0 , type = , val = ) \n 
param6 = ModuleParam ( id = 5 , pos = 1 , type = , val = ) \n 
function1 = ModuleFunction ( name = , parameters = [ param1 , param2 ] ) \n 
function2 = ModuleFunction ( name = , parameters = [ param3 , param4 ] ) \n 
function3 = ModuleFunction ( name = , parameters = [ param5 , param6 ] ) \n 
function4 = ModuleFunction ( name = , parameters = [ param1 , param2 ] ) \n 
function5 = ModuleFunction ( name = , parameters = [ param1 ] ) \n 
assert heuristicFunctionMatch ( function1 , function2 ) == 1 \n 
assert heuristicFunctionMatch ( function1 , function3 ) == 0 \n 
assert heuristicFunctionMatch ( function1 , function4 ) == - 1 \n 
assert heuristicFunctionMatch ( function1 , function5 ) == 0 \n 
~~ def test_module_heuristic ( self ) : \n 
param6 = ModuleParam ( id = 5 , pos = 0 , type = , val = ) \n 
function3 = ModuleFunction ( name = , parameters = [ param5 ] ) \n 
function4 = ModuleFunction ( name = , parameters = [ param6 ] ) \n 
function5 = ModuleFunction ( name = , parameters = [ param2 , param4 ] ) \n 
function6 = ModuleFunction ( name = , parameters = [ param5 ] ) \n 
module1 = Module ( name = , functions = [ function1 , function3 ] ) \n 
module2 = Module ( name = , functions = [ function2 , function4 ] ) \n 
module3 = Module ( name = , functions = [ function1 , function2 ] ) \n 
module4 = Module ( name = , functions = [ function5 ] ) \n 
module5 = Module ( name = , functions = [ function5 , function6 ] ) \n 
assert heuristicModuleMatch ( module1 , module2 ) == 1 \n 
assert heuristicModuleMatch ( module1 , module3 ) == - 1 \n 
assert heuristicModuleMatch ( module1 , module4 ) == 0 \n 
assert heuristicModuleMatch ( module1 , module5 ) == 0 \n 
from datetime import date , datetime \n 
from vistrails . core . system import strftime , time_strptime \n 
class XMLDAO : \n 
~~ def hasAttribute ( self , node , attr ) : \n 
~~~ return node . hasAttribute ( attr ) \n 
~~ def getAttribute ( self , node , attr ) : \n 
~~~ attribute = node . attributes . get ( attr ) \n 
if attribute is not None : \n 
~~~ return attribute . value \n 
~~ ~~ except KeyError : \n 
~~ def convertFromStr ( self , value , type ) : \n 
~~~ if value is not None : \n 
~~~ if type == : \n 
~~~ return str ( value ) \n 
~~ elif value . strip ( ) != : \n 
~~~ return long ( value ) \n 
~~ elif type == : \n 
~~~ return float ( value ) \n 
~~~ return int ( value ) \n 
~~~ return date ( * time_strptime ( value , ) [ 0 : 3 ] ) \n 
~~~ return datetime ( * time_strptime ( value , ) [ 0 : 6 ] ) \n 
~~ ~~ ~~ return None \n 
~~ def convertToStr ( self , value , type ) : \n 
~~~ return value . isoformat ( ) \n 
~~~ return strftime ( value , ) \n 
from xml . auto_gen import XMLDAOListBase \n 
from sql . auto_gen import SQLDAOListBase \n 
from vistrails . core . system import get_elementtree_library \n 
from vistrails . db . versions . v0_9_0 import version as my_version \n 
ElementTree = get_elementtree_library ( ) \n 
class DAOList ( dict ) : \n 
~~~ self [ ] = XMLDAOListBase ( ) \n 
self [ ] = SQLDAOListBase ( ) \n 
~~ def parse_xml_file ( self , filename ) : \n 
~~~ return ElementTree . parse ( filename ) \n 
~~ def write_xml_file ( self , filename , tree ) : \n 
~~~ tree . write ( filename ) \n 
~~ def read_xml_object ( self , vtType , node ) : \n 
~~~ return self [ ] [ vtType ] . fromXML ( node ) \n 
~~ def write_xml_object ( self , obj , node = None ) : \n 
~~~ res_node = self [ ] [ obj . vtType ] . toXML ( obj , node ) \n 
return res_node \n 
~~ def open_from_xml ( self , filename , vtType , tree = None ) : \n 
if tree is None : \n 
~~~ tree = self . parse_xml_file ( filename ) \n 
~~ vistrail = self . read_xml_object ( vtType , tree . getroot ( ) ) \n 
return vistrail \n 
~~ def save_to_xml ( self , obj , filename , tags ) : \n 
root = self . write_xml_object ( obj ) \n 
root . set ( , my_version ) \n 
for k , v in tags . iteritems ( ) : \n 
~~~ root . set ( k , v ) \n 
~~ tree = ElementTree . ElementTree ( root ) \n 
self . write_xml_file ( filename , tree ) \n 
~~ def open_from_db ( self , db_connection , vtType , id , lock = False ) : \n 
~~ def save_to_db ( self , db_connection , obj , doCopy = False ) : \n 
~~ def serialize ( self , object ) : \n 
~~~ root = self . write_xml_object ( object ) \n 
return ElementTree . tostring ( root ) \n 
~~ def unserialize ( self , str , obj_type ) : \n 
~~~ root = ElementTree . fromstring ( str ) \n 
return self . read_xml_object ( obj_type , root ) \n 
~~ except SyntaxError , e : \n 
raise VistrailsDBException ( msg ) \n 
from auto_gen import DBWorkflow as _DBWorkflow \n 
from auto_gen import DBAbstractionRef , DBModule , DBGroup \n 
from id_scope import IdScope \n 
class DBWorkflow ( _DBWorkflow ) : \n 
~~~ _DBWorkflow . __init__ ( self , * args , ** kwargs ) \n 
self . objects = { } \n 
self . tmp_id = IdScope ( 1 , \n 
{ DBAbstractionRef . vtType : DBModule . vtType , \n 
DBGroup . vtType : DBModule . vtType } ) \n 
~~~ return DBWorkflow . do_copy ( self ) \n 
~~~ cp = _DBWorkflow . do_copy ( self , new_ids , id_scope , id_remap ) \n 
cp . __class__ = DBWorkflow \n 
cp . build_index ( ) \n 
cp . tmp_id = copy . copy ( self . tmp_id ) \n 
def update_version ( old_obj , trans_dict , new_obj = None ) : \n 
~~~ if new_obj is None : \n 
~~~ new_obj = DBWorkflow ( ) \n 
~~ new_obj = _DBWorkflow . update_version ( old_obj , trans_dict , new_obj ) \n 
new_obj . update_id_scope ( ) \n 
new_obj . build_index ( ) \n 
return new_obj \n 
~~ def update_id_scope ( self ) : \n 
~~ _vtTypeMap = { : , : } \n 
def build_index ( self ) : \n 
~~~ g = self . _vtTypeMap . get \n 
self . objects = dict ( ( ( g ( o . vtType , o . vtType ) , o . _db_id ) , o ) \n 
for ( o , _ , _ ) in self . db_children ( ) ) \n 
~~ def add_to_index ( self , object ) : \n 
~~~ obj_type = self . _vtTypeMap . get ( object . vtType , object . vtType ) \n 
self . objects [ ( obj_type , object . getPrimaryKey ( ) ) ] = object \n 
~~ def delete_from_index ( self , object ) : \n 
del self . objects [ ( obj_type , object . getPrimaryKey ( ) ) ] \n 
~~ def capitalizeOne ( self , str ) : \n 
~~~ return str [ 0 ] . upper ( ) + str [ 1 : ] \n 
~~ def db_print_objects ( self ) : \n 
~~~ for k , v in self . objects . iteritems ( ) : \n 
~~~ print % ( k , v ) \n 
~~ ~~ def db_has_object ( self , type , id ) : \n 
~~~ return ( type , id ) in self . objects \n 
~~ def db_get_object ( self , type , id ) : \n 
~~~ return self . objects [ ( type , id ) ] \n 
~~ def db_add_object ( self , object , parent_obj_type = None , \n 
parent_obj_id = None , parent_obj = None ) : \n 
~~~ if parent_obj is None : \n 
~~~ if parent_obj_type is None or parent_obj_id is None : \n 
~~~ parent_obj = self \n 
~~~ if parent_obj_type == or parent_obj_type == : \n 
~~~ parent_obj_type = \n 
~~~ parent_obj = self . objects [ ( parent_obj_type , parent_obj_id ) ] \n 
raise Exception ( msg ) \n 
~~ ~~ ~~ if object . vtType == or object . vtType == : \n 
~~~ obj_type = \n 
~~~ obj_type = object . vtType \n 
~~ funname = + obj_type \n 
obj_copy = copy . copy ( object ) \n 
getattr ( parent_obj , funname ) ( obj_copy ) \n 
self . add_to_index ( obj_copy ) \n 
~~ def db_change_object ( self , old_id , object , parent_obj_type = None , \n 
~~ ~~ ~~ self . db_delete_object ( old_id , object . vtType , None , None , parent_obj ) \n 
self . db_add_object ( object , None , None , parent_obj ) \n 
~~ def db_delete_object ( self , obj_id , obj_type , parent_obj_type = None , \n 
~~ ~~ ~~ if obj_type == or obj_type == : \n 
if hasattr ( parent_obj , funname ) : \n 
~~~ object = getattr ( parent_obj , funname ) ( obj_id ) \n 
~~~ attr_name = + obj_type \n 
object = getattr ( parent_obj , attr_name ) \n 
getattr ( parent_obj , funname ) ( object ) \n 
self . delete_from_index ( object ) \n 
from vistrails . db . versions . v0_9_4 . domain import DBVistrail , DBAction , DBTag , DBModule , DBConnection , DBPortSpec , DBFunction , DBParameter , DBLocation , DBAdd , DBChange , DBDelete , DBAnnotation , DBPort , DBGroup , DBWorkflow , DBLog , DBAbstraction \n 
def update_workflow ( old_obj , translate_dict ) : \n 
~~~ return DBWorkflow . update_version ( old_obj . db_workflow , \n 
translate_dict , DBWorkflow ( ) ) \n 
~~ def update_modules ( old_obj , trans_dict ) : \n 
~~~ new_modules = [ ] \n 
for obj in old_obj . db_modules : \n 
~~~ if obj . vtType == : \n 
~~~ new_modules . append ( DBModule . update_version ( obj , trans_dict ) ) \n 
~~ elif obj . vtType == : \n 
~~~ new_modules . append ( DBAbstraction . update_version ( obj , \n 
trans_dict ) ) \n 
~~~ new_modules . append ( DBGroup . update_version ( obj , trans_dict ) ) \n 
~~ ~~ return new_modules \n 
~~ def translateVistrail ( _vistrail ) : \n 
~~~ def update_operations ( old_obj , trans_dict ) : \n 
~~~ def update_abstractionRef ( old_obj , trans_dict ) : \n 
~~~ def get_internal_version ( old_obj , trans_dict ) : \n 
~~~ return str ( old_obj . db_version ) \n 
~~ def get_version ( old_obj , trans_dict ) : \n 
~~ new_dict = { : \n 
{ : get_internal_version , \n 
: get_version } } \n 
new_dict . update ( trans_dict ) \n 
return DBAbstraction . update_version ( old_obj . db_data , new_dict ) \n 
~~ new_ops = [ ] \n 
for obj in old_obj . db_operations : \n 
~~~ if obj . db_what == : \n 
~~~ trans_dict [ ] = { : update_abstractionRef } \n 
new_op = DBAdd . update_version ( obj , trans_dict ) \n 
new_op . db_what = \n 
del trans_dict [ ] \n 
~~~ new_op = DBAdd . update_version ( obj , trans_dict ) \n 
if obj . db_parentObjType == : \n 
~~~ new_op . db_parentObjType = \n 
~~ new_ops . append ( new_op ) \n 
~~ ~~ elif obj . vtType == : \n 
~~~ new_ops . append ( DBDelete . update_version ( obj , trans_dict ) ) \n 
new_op = DBChange . update_version ( obj , trans_dict ) \n 
~~~ new_op = DBChange . update_version ( obj , trans_dict ) \n 
~~ ~~ ~~ return new_ops \n 
~~ translate_dict = { : { : update_workflow } , \n 
: { : update_operations } , \n 
: { : update_modules } } \n 
vistrail = DBVistrail . update_version ( _vistrail , translate_dict ) \n 
vistrail . db_version = \n 
~~ def translateWorkflow ( _workflow ) : \n 
~~~ translate_dict = { : { : update_workflow } , \n 
workflow = DBWorkflow . update_version ( _workflow , translate_dict ) \n 
workflow . db_version = \n 
~~ def translateLog ( _log ) : \n 
~~~ translate_dict = { } \n 
log = DBLog . update_version ( _log , translate_dict ) \n 
log . db_version = \n 
return log \n 
import numpy as _np \n 
def normal ( std ) : \n 
~~~ def init ( shape , fan ) : \n 
~~~ return std * _np . random . randn ( * shape ) \n 
~~ return init \n 
import theano as th \n 
from kaggle_utils import multiclass_log_loss \n 
from examples . utils import make_progressbar \n 
def validate ( dataset_x , dataset_y , model , epoch , batch_size ) : \n 
~~~ progress = make_progressbar ( . format ( epoch ) , len ( dataset_x ) ) \n 
progress . start ( ) \n 
logloss = 0. \n 
for j in range ( ( dataset_x . shape [ 0 ] + batch_size - 1 ) // batch_size ) : \n 
~~~ mini_batch_input = dataset_x [ j * batch_size : ( j + 1 ) * batch_size ] . astype ( th . config . floatX ) \n 
mini_batch_targets = dataset_y [ j * batch_size : ( j + 1 ) * batch_size ] . astype ( th . config . floatX ) \n 
mini_batch_prediction = model . forward ( mini_batch_input ) \n 
logloss += multiclass_log_loss ( mini_batch_targets , mini_batch_prediction , normalize = False ) \n 
progress . update ( j * batch_size + len ( mini_batch_input ) ) \n 
~~ progress . finish ( ) \n 
from nose . tools import ( assert_is_not_none , assert_true , assert_raises , assert_equal ) \n 
import io \n 
from sknn . mlp import Regressor as MLPR \n 
from sknn . mlp import Layer as L , Convolution as C \n 
class TestConvolution ( unittest . TestCase ) : \n 
~~~ def _run ( self , nn , a_in = None , fit = True ) : \n 
~~~ assert_true ( nn . is_convolution ( ) ) \n 
if a_in is None : \n 
~~~ a_in = numpy . zeros ( ( 8 , 32 , 16 , 1 ) ) \n 
~~ a_out = numpy . zeros ( ( 8 , 4 ) ) \n 
if fit is True : \n 
~~~ nn . fit ( a_in , a_out ) \n 
~~ a_test = nn . predict ( a_in ) \n 
assert_equal ( type ( a_out ) , type ( a_in ) ) \n 
return a_test \n 
~~ def test_MissingLastDim ( self ) : \n 
~~~ self . _run ( MLPR ( \n 
layers = [ \n 
C ( "Tanh" , channels = 4 , kernel_shape = ( 3 , 3 ) ) , \n 
L ( "Linear" ) ] , \n 
n_iter = 1 ) , \n 
a_in = numpy . zeros ( ( 8 , 32 , 16 ) ) ) \n 
~~ def test_SquareKernel ( self ) : \n 
C ( "Rectifier" , channels = 4 , kernel_shape = ( 3 , 3 ) ) , \n 
n_iter = 1 ) ) \n 
~~ def test_KernelPooling ( self ) : \n 
C ( "Rectifier" , channels = 4 , kernel_shape = ( 3 , 3 ) , pool_shape = ( 2 , 2 ) ) , \n 
~~ def test_VerticalKernel ( self ) : \n 
C ( "Rectifier" , channels = 4 , kernel_shape = ( 16 , 1 ) , border_mode = ) , \n 
~~ def test_VerticalVerbose ( self ) : \n 
C ( "Sigmoid" , channels = 4 , kernel_shape = ( 16 , 1 ) , border_mode = ) , \n 
n_iter = 1 , verbose = 1 , valid_size = 0.1 ) ) \n 
~~ def test_HorizontalKernel ( self ) : \n 
C ( "Rectifier" , channels = 4 , kernel_shape = ( 1 , 16 ) , border_mode = ) , \n 
~~ def test_ValidationSize ( self ) : \n 
n_iter = 1 , \n 
valid_size = 0.5 ) ) \n 
~~ def test_ValidationSet ( self ) : \n 
~~~ v_in = numpy . zeros ( ( 8 , 32 , 16 , 1 ) ) \n 
v_out = numpy . zeros ( ( 8 , 4 ) ) \n 
self . _run ( MLPR ( \n 
valid_set = ( v_in , v_out ) ) ) \n 
~~ def test_MultipleLayers ( self ) : \n 
C ( "Rectifier" , channels = 6 , kernel_shape = ( 3 , 3 ) ) , \n 
C ( "Sigmoid" , channels = 4 , kernel_shape = ( 5 , 5 ) ) , \n 
C ( "Tanh" , channels = 8 , kernel_shape = ( 3 , 3 ) ) , \n 
~~ def test_PoolingMaxType ( self ) : \n 
C ( "Rectifier" , channels = 4 , kernel_shape = ( 2 , 2 ) , \n 
pool_shape = ( 2 , 2 ) , pool_type = ) , \n 
~~ def test_PoolingMeanType ( self ) : \n 
~~ ~~ class TestUpscaling ( unittest . TestCase ) : \n 
~~~ def _run ( self , nn , scale ) : \n 
a_in = numpy . zeros ( ( 8 , 16 , 16 , 1 ) ) \n 
a_out = numpy . zeros ( ( 8 , 16 * scale , 16 * scale , 3 ) ) \n 
nn . fit ( a_in , a_out ) \n 
a_test = nn . predict ( a_in ) \n 
~~ def test_UpscalingFactorFour ( self ) : \n 
C ( "Rectifier" , channels = 3 , kernel_shape = ( 3 , 3 ) , scale_factor = ( 4 , 4 ) , border_mode n_iter = 1 ) , \n 
scale = 4 ) \n 
~~ def test_DownscaleUpscale ( self ) : \n 
C ( "ExpLin" , channels = 6 , kernel_shape = ( 3 , 3 ) , pool_shape = ( 2 , 2 ) , border_mode = C ( "Rectifier" , channels = 3 , kernel_shape = ( 3 , 3 ) , scale_factor = ( 2 , 2 ) , border_mode n_iter = 1 ) , \n 
scale = 1 ) \n 
~~ ~~ class TestConvolutionSpecs ( unittest . TestCase ) : \n 
~~~ def test_SmallSquareKernel ( self ) : \n 
~~~ nn = MLPR ( layers = [ \n 
C ( "Rectifier" , channels = 4 , kernel_shape = ( 3 , 3 ) , border_mode = ) , \n 
L ( "Linear" , units = 5 ) ] ) \n 
a_in = numpy . zeros ( ( 8 , 32 , 32 , 1 ) ) \n 
nn . _create_specs ( a_in ) \n 
assert_equal ( nn . unit_counts , [ 1024 , 30 * 30 * 4 , 5 ] ) \n 
~~ def test_SquareKernelFull ( self ) : \n 
C ( "ExpLin" , channels = 4 , kernel_shape = ( 3 , 3 ) , border_mode = ) , \n 
assert_equal ( nn . unit_counts , [ 1024 , 4624 , 5 ] ) \n 
C ( "Rectifier" , channels = 7 , kernel_shape = ( 16 , 1 ) , border_mode = ) , \n 
assert_equal ( nn . unit_counts , [ 256 , 16 * 7 , 5 ] ) \n 
L ( "Linear" , units = 7 ) ] ) \n 
assert_equal ( nn . unit_counts , [ 256 , 16 * 4 , 7 ] ) \n 
~~ def test_SquareKernelPool ( self ) : \n 
C ( "ExpLin" , channels = 4 , kernel_shape = ( 3 , 3 ) , pool_shape = ( 2 , 2 ) , border_mode = L ( "Linear" , units = 5 ) ] ) \n 
assert_equal ( nn . unit_counts , [ 1024 , 15 * 15 * 4 , 5 ] ) \n 
~~ def test_SquarePoolFull ( self ) : \n 
C ( "Rectifier" , channels = 4 , kernel_shape = ( 3 , 3 ) , pool_shape = ( 2 , 2 ) , border_mode = L ( "Linear" , units = 5 ) ] ) \n 
assert_equal ( nn . unit_counts , [ 1024 , 16 * 16 * 4 , 5 ] ) \n 
~~ def test_InvalidBorderMode ( self ) : \n 
~~~ assert_raises ( NotImplementedError , C , \n 
"Rectifier" , channels = 4 , kernel_shape = ( 3 , 3 ) , border_mode = ) \n 
~~ def test_MultiLayerPooling ( self ) : \n 
C ( "ExpLin" , channels = 4 , kernel_shape = ( 3 , 3 ) , pool_shape = ( 2 , 2 ) ) , \n 
L ( "Linear" ) ] ) \n 
a_in , a_out = numpy . zeros ( ( 8 , 32 , 32 , 1 ) ) , numpy . zeros ( ( 8 , 16 ) ) \n 
nn . _initialize ( a_in , a_out ) \n 
assert_equal ( nn . unit_counts , [ 1024 , 900 , 196 , 16 ] ) \n 
~~ def test_Upscaling ( self ) : \n 
C ( "Rectifier" , channels = 4 , kernel_shape = ( 1 , 1 ) , scale_factor = ( 2 , 2 ) , border_mode = L ( "Linear" , units = 5 ) ] ) \n 
assert_equal ( nn . unit_counts , [ 1024 , 64 * 64 * 4 , 5 ] ) \n 
~~ ~~ class TestActivationTypes ( unittest . TestCase ) : \n 
~~~ def _run ( self , activation ) : \n 
~~~ a_in , a_out = numpy . zeros ( ( 8 , 32 , 16 , 1 ) ) , numpy . zeros ( ( 8 , 4 ) ) \n 
nn = MLPR ( \n 
C ( activation , channels = 4 , kernel_shape = ( 3 , 3 ) , \n 
n_iter = 1 ) \n 
~~ def test_RectifierConv ( self ) : \n 
~~~ self . _run ( "Rectifier" ) \n 
~~ def test_ExponentialLinear ( self ) : \n 
~~~ self . _run ( "ExpLin" ) \n 
~~ def test_SigmoidConv ( self ) : \n 
~~~ self . _run ( "Sigmoid" ) \n 
~~ def test_TanhConv ( self ) : \n 
~~~ self . _run ( "Tanh" ) \n 
~~ def test_LinearConv ( self ) : \n 
~~~ self . _run ( "Linear" ) \n 
~~ def test_UnknownConv ( self ) : \n 
~~~ assert_raises ( NotImplementedError , self . _run , "Unknown" ) \n 
~~ ~~ class TestConvolutionRGB ( TestConvolution ) : \n 
~~~ def _run ( self , nn , a_in = None ) : \n 
~~~ if a_in is None : \n 
~~ ~~ class TestSerialization ( unittest . TestCase ) : \n 
~~~ self . nn = MLPR ( \n 
~~ def test_SerializeEmpty ( self ) : \n 
~~~ buf = io . BytesIO ( ) \n 
pickle . dump ( self . nn , buf ) \n 
buf . seek ( 0 ) \n 
nn = pickle . load ( buf ) \n 
buf = io . BytesIO ( ) \n 
assert_equal ( nn . layers , self . nn . layers ) \n 
~~ ~~ class TestSerializedNetwork ( TestConvolution ) : \n 
~~~ def _run ( self , original , a_in = None ) : \n 
~~~ a_test = super ( TestSerializedNetwork , self ) . _run ( original , a_in ) \n 
pickle . dump ( original , buf ) \n 
a_copy = super ( TestSerializedNetwork , self ) . _run ( nn , a_in , fit = False ) \n 
assert_true ( ( a_test == a_copy ) . all ( ) ) \n 
#!/bin/python \n 
~~ ~~ import sys ; \n 
import os ; \n 
path = os . path . split ( os . path . realpath ( __file__ ) ) [ 0 ] ; \n 
sys . path . append ( path + "/utils/Python_Utils" ) ; \n 
sys . path . append ( path + "/../utils/Python_Utils" ) ; \n 
from latent_factor import * ; \n 
from arffio import * ; \n 
import Logger ; \n 
import pickle ; \n 
import numpy as np ; \n 
import train_rep as train ; \n 
import unittest ; \n 
class TrainTester ( unittest . TestCase ) : \n 
~~~ self . argv = [ ] ; \n 
self . argv . append ( "train_rep" ) ; \n 
self . argv . append ( "-l2" ) ; \n 
self . argv . append ( "0.01" ) ; \n 
self . argv . append ( "train_file" ) ; \n 
self . argv . append ( "m_file" ) ; \n 
~~ def test_parseParameter ( self ) : \n 
~~~ parameters = train . parseParameter ( self . argv ) ; \n 
~~ def test_train ( self ) : \n 
~~~ argv = [ "train_rep.py" , "-i" , "2" , "-b" , "2" , "tests/test_train_data.arff" , "tests/test_train_model.model" ] \n 
train . main ( argv ) \n 
argv = [ "train_rep.py" , "-i" , "2" , "-b" , "2" , "tests/test_train_data.arff" , "tests/test_train_model.model" ] \n 
argv = [ "train_rep.py" , "-i" , "2" , "-b" , "2" , "-st" , "0" , "tests/test_train_data.arff" , "tests/test_train_model.model" ] \n 
~~ def test_train_al ( self ) : \n 
~~~ argv = [ "train_rep.py" , "-i" , "2" , "-b" , "2" , "-st" , "0" , "-l" , "1" , "-ha" , "1" , "-oa" , "1" , "tests/test_train_data.arff" , "tests/test_train_model.model" ] \n 
~~ def test_train_param ( self ) : \n 
~~~ argv = [ "train_rep.py" , "-ha" , "1" , "-oa" , "1" , "-l" , "1" , "-i" , "2" , "-b" , "2" , "tests/test_train_data.arff" , "tests/test_train_model.model" ] \n 
~~ ~~ from time import * \n 
from constants import * \n 
def get_filename ( left_bound , right_bound ) : \n 
~~~ return str ( int ( left_bound ) ) + + str ( int ( right_bound ) ) + \n 
~~ def get_filename_dict ( ID_bounds ) : \n 
~~~ filename_dict = { } \n 
for ( L , R ) in ID_bounds : \n 
~~~ filename_dict [ i ] = get_filename ( L , R ) \n 
~~ ~~ return filename_dict \n 
~~ def get_ID_bounds ( halo_file , max_chunk_sz = None , chunk_dir = ) : \n 
~~~ halos = np . loadtxt ( halo_file ) \n 
if not max_chunk_sz : max_chunk_sz = halos [ 0 ] [ N200a ] \n 
cur_sum = halos [ 0 ] [ N200a ] \n 
left_bound = halos [ 0 ] [ ID ] \n 
right_bound = halos [ 0 ] [ ID ] \n 
ID_bounds = [ ] \n 
for h in halos : \n 
~~~ cur_id , num_p = h [ ID ] , h [ N200a ] \n 
if cur_sum + num_p > max_chunk_sz : \n 
~~~ ID_bounds . append ( ( left_bound , right_bound ) ) \n 
left_bound = cur_id \n 
right_bound = cur_id \n 
cur_sum = num_p \n 
~~~ cur_sum += num_p \n 
right_bound += 1 \n 
~~ ~~ if ( max_chunk_sz == halos [ 0 ] [ N200a ] ) : return ID_bounds [ 1 : ] \n 
else : return ID_bounds \n 
~~ def make_chunks ( ID_bounds ) : \n 
( left_bound , right_bound ) = ID_bounds [ i ] \n 
cur_chunk = [ ] \n 
for line in open ( ) : \n 
~~~ particle = [ float ( val ) for val in line . split ( ) ] \n 
cur_ID = particle [ 1 ] \n 
if ( cur_ID >= left_bound and cur_ID <= right_bound ) : \n 
~~~ cur_chunk . append ( particle ) \n 
~~~ os . chdir ( chunk_dir ) \n 
filename = get_filename ( left_bound , right_bound ) \n 
print , len ( cur_chunk ) , , filename \n 
print , 100 * ( i + 1. ) / len ( ID_bounds ) , \n 
assert len ( cur_chunk ) > 0 \n 
np . savetxt ( filename , cur_chunk ) \n 
os . chdir ( ) \n 
~~ ~~ ~~ ~~ def fetch ( halo_ID , filename_dict , chunk_dir = ) : \n 
~~~ P = np . loadtxt ( chunk_dir + + str ( filename_dict [ halo_ID ] ) ) \n 
P = np . column_stack ( ( P [ : , 0 ] , P [ : , 2 : ] , P [ : , 1 ] ) ) \n 
return P [ P [ : , H_ID ] == halo_ID ] \n 
~~ def test ( halo_file , filename_dict ) : \n 
~~~ fetch_times = [ ] \n 
length_errors = [ ] \n 
for h in np . loadtxt ( halo_file ) : \n 
~~~ starttime = clock ( ) \n 
P = fetch ( h [ ID ] , filename_dict ) \n 
fetch_times . append ( clock ( ) - starttime ) \n 
if ( len ( P ) != h [ N200a ] ) : length_errors . append ( len ( P ) - int ( h [ N200a ] ) ) \n 
for p in P : assert p [ H_ID ] == h [ ID ] \n 
print , h [ ID ] , , np . average ( fetch_times ) \n 
~~ return fetch_times , length_errors \n 
~~ if ( __name__ == ) : \n 
~~~ halo_file = \n 
chunk_dir = \n 
ID_bounds = get_ID_bounds ( halo_file ) \n 
print , len ( ID_bounds ) \n 
filename_dict = get_filename_dict ( ID_bounds ) \n 
fetch_times , length_errors = test ( halo_file , filename_dict ) \n 
print , len ( length_errors ) , \n 
figure ( 0 ) \n 
hist ( fetch_times , bins = 50 ) \n 
xlabel ( , fontsize = 24 ) \n 
ylabel ( , fontsize = 24 ) \n 
title ( , fontsize = 30 ) \n 
show ( ) \n 
from theano . sandbox . cuda . basic_ops import gpu_from_host , host_from_gpu \n 
from pylearn2 . sandbox . cuda_convnet . filter_acts import FilterActs \n 
from pylearn2 . sandbox . cuda_convnet . weight_acts import WeightActs \n 
from pylearn2 . sandbox . cuda_convnet . img_acts import ImageActs \n 
import cudarray as ca \n 
def avg_running_time ( fun ) : \n 
~~~ n_iter = 20 \n 
for _ in range ( n_iter ) : \n 
~~~ fun ( ) \n 
~~ duration = time . time ( ) - start_time \n 
return duration / float ( n_iter ) \n 
~~ def allclose ( a , b ) : \n 
~~~ atol = 1e-3 \n 
rtol = 1e-3 \n 
return np . allclose ( a , b , atol = atol , rtol = rtol ) \n 
~~ def benchmark ( n_imgs , n_channels , img_shape , n_filters , filter_shape , pad ) : \n 
~~~ print ( \n 
% ( ( n_imgs , n_channels ) + img_shape ) \n 
+ \n 
% ( ( n_filters , ) + filter_shape + ( pad , ) ) ) \n 
padding = ( pad , pad ) \n 
strides = ( 1 , 1 ) \n 
img_h , img_w = img_shape \n 
filter_h , filter_w = filter_shape \n 
convout_h = img_h + 2 * pad - filter_h + 1 \n 
convout_w = img_w + 2 * pad - filter_w + 1 \n 
imgs_bc01_shape = ( n_imgs , n_channels , img_h , img_w ) \n 
filters_bc01_shape = ( n_filters , n_channels , filter_h , filter_w ) \n 
imgs_bc01 = np . random . randn ( n_imgs , n_channels , img_h , img_w ) \n 
imgs_c01b = np . transpose ( imgs_bc01 , ( 1 , 2 , 3 , 0 ) ) \n 
filters_fc01 = np . random . randn ( n_filters , n_channels , filter_h , filter_w ) \n 
filters_c01f = np . transpose ( filters_fc01 , ( 1 , 2 , 3 , 0 ) ) \n 
convout_bc01 = np . random . randn ( n_imgs , n_filters , convout_h , convout_w ) \n 
convout_c01b = np . transpose ( convout_bc01 , ( 1 , 2 , 3 , 0 ) ) \n 
imgs_bc01_t = theano . shared ( imgs_bc01 . astype ( theano . config . floatX ) ) \n 
imgs_c01b_t = theano . shared ( imgs_c01b . astype ( theano . config . floatX ) ) \n 
filters_fc01_t = theano . shared ( filters_fc01 . astype ( theano . config . floatX ) ) \n 
filters_c01f_t = theano . shared ( filters_c01f . astype ( theano . config . floatX ) ) \n 
convout_bc01_t = theano . shared ( convout_bc01 . astype ( theano . config . floatX ) ) \n 
convout_c01b_t = theano . shared ( convout_c01b . astype ( theano . config . floatX ) ) \n 
imgs_bc01_ca = ca . array ( imgs_bc01 ) \n 
filters_fc01_ca = ca . array ( filters_fc01 ) \n 
convout_bc01_ca = ca . array ( convout_bc01 ) \n 
convout_cc_op = FilterActs ( stride = 1 , partial_sum = 4 , pad = pad ) \n 
convout_cc_expr = convout_cc_op ( imgs_c01b_t , filters_c01f_t ) \n 
convout_cc_fun = theano . function ( [ ] , convout_cc_expr ) \n 
convout_cc = convout_cc_fun ( ) \n 
convout_cc = np . transpose ( convout_cc , ( 3 , 0 , 1 , 2 ) ) \n 
def convout_ca_fun ( ) : \n 
~~~ convout = ca . nnet . conv_bc01 ( imgs_bc01_ca , filters_fc01_ca , padding , \n 
strides ) \n 
return convout \n 
~~ convout_ca = np . array ( convout_ca_fun ( ) ) \n 
print ( + str ( allclose ( convout_ca , convout_cc ) ) ) \n 
duration_cc = avg_running_time ( convout_cc_fun ) \n 
duration_ca = avg_running_time ( convout_ca_fun ) \n 
print ( \n 
% ( duration_cc , duration_ca ) ) \n 
print ( % ( duration_cc / duration_ca ) ) \n 
del convout_cc_op \n 
del convout_cc_expr \n 
del convout_cc_fun \n 
dimgs_cc_op = ImageActs ( stride = 1 , partial_sum = 1 , pad = pad ) \n 
dimgs_cc_expr = dimgs_cc_op ( convout_c01b_t , filters_c01f_t ) \n 
dimgs_cc_fun = theano . function ( [ ] , dimgs_cc_expr ) \n 
dimgs_cc = dimgs_cc_fun ( ) \n 
dimgs_cc = np . transpose ( dimgs_cc , ( 3 , 0 , 1 , 2 ) ) \n 
def dimgs_ca_fun ( ) : \n 
~~~ return ca . nnet . conv_bc01_bprop_imgs ( filters_fc01_ca , convout_bc01_ca , \n 
img_shape , padding , strides ) \n 
~~ dimgs_ca = np . array ( dimgs_ca_fun ( ) ) \n 
print ( + str ( allclose ( dimgs_ca , dimgs_cc ) ) ) \n 
duration_cc = avg_running_time ( dimgs_cc_fun ) \n 
duration_ca = avg_running_time ( dimgs_ca_fun ) \n 
del dimgs_cc_op \n 
del dimgs_cc_expr \n 
del dimgs_cc_fun \n 
dfilters_cc_op = WeightActs ( stride = 1 , partial_sum = 1 , pad = pad ) \n 
dfilters_cc_expr = dfilters_cc_op ( imgs_c01b_t , convout_c01b_t , \n 
T . as_tensor_variable ( filter_shape ) ) \n 
dfilters_cc_fun = theano . function ( [ ] , dfilters_cc_expr ) \n 
dfilters_cc = dfilters_cc_fun ( ) [ 0 ] \n 
dfilters_cc = np . transpose ( dfilters_cc , ( 3 , 0 , 1 , 2 ) ) \n 
def dfilters_ca_fun ( ) : \n 
~~~ return ca . nnet . conv_bc01_bprop_filters ( imgs_bc01_ca , convout_bc01_ca , \n 
filter_shape , padding , strides ) \n 
~~ dfilters_ca = np . array ( dfilters_ca_fun ( ) ) \n 
print ( + str ( allclose ( dfilters_ca , dfilters_cc ) ) ) \n 
duration_cc = avg_running_time ( dfilters_cc_fun ) \n 
duration_ca = avg_running_time ( dfilters_ca_fun ) \n 
~~ def run ( ) : \n 
configurations = [ \n 
( 128 , 3 , ( 32 , 32 ) , 96 , ( 11 , 11 ) , 0 ) , \n 
( 128 , 96 , ( 32 , 32 ) , 256 , ( 7 , 7 ) , 0 ) , \n 
( 128 , 256 , ( 16 , 16 ) , 384 , ( 5 , 5 ) , 0 ) , \n 
( 128 , 384 , ( 16 , 16 ) , 384 , ( 5 , 5 ) , 0 ) , \n 
( 128 , 384 , ( 16 , 16 ) , 384 , ( 3 , 3 ) , 0 ) , \n 
( 64 , 3 , ( 96 , 96 ) , 128 , ( 16 , 16 ) , 0 ) , \n 
( 64 , 128 , ( 32 , 32 ) , 64 , ( 8 , 8 ) , 0 ) , \n 
( 128 , 32 , ( 54 , 54 ) , 64 , ( 6 , 6 ) , 0 ) , \n 
( 128 , 128 , ( 16 , 16 ) , 128 , ( 8 , 8 ) , 0 ) , \n 
( 128 , 1024 , ( 32 , 32 ) , 128 , ( 4 , 4 ) , 0 ) , \n 
( 5 , 3 , ( 5 , 5 ) , 16 , ( 3 , 3 ) , 1 ) , \n 
( 64 , 32 , ( 32 , 32 ) , 32 , ( 5 , 5 ) , 2 ) , \n 
( 64 , 1 , ( 17 , 19 ) , 32 , ( 7 , 7 ) , 4 ) , \n 
( 64 , 3 , ( 9 , 16 ) , 32 , ( 7 , 7 ) , 4 ) , \n 
( 128 , 3 , ( 32 , 32 ) , 64 , ( 5 , 5 ) , 2 ) , \n 
( 128 , 64 , ( 16 , 16 ) , 64 , ( 5 , 5 ) , 2 ) , \n 
( 128 , 64 , ( 8 , 8 ) , 64 , ( 5 , 5 ) , 2 ) , \n 
for conf in configurations : \n 
~~~ benchmark ( * conf ) \n 
~~~ run ( ) \n 
~~ import cudarray as ca \n 
from ... base import ParamMixin \n 
from ... parameter import Parameter \n 
from . . base import Unary \n 
class Linear ( Unary , ParamMixin ) : \n 
~~~ def __init__ ( self , n_out , weights ) : \n 
~~~ self . n_out = n_out \n 
self . weights = Parameter . from_any ( weights ) \n 
~~ def __call__ ( self , x ) : \n 
~~~ super ( Linear , self ) . __call__ ( x ) \n 
self . bpropable = True \n 
return self \n 
~~ def setup ( self ) : \n 
~~~ batch_size , n_in = self . x . shape \n 
self . shape = ( batch_size , self . n_out ) \n 
self . array = ca . zeros ( self . shape ) \n 
self . grad_array = ca . zeros ( self . shape ) \n 
self . weights . setup ( ( n_in , self . n_out ) ) \n 
~~ def fprop ( self ) : \n 
~~~ ca . dot ( self . x . array , self . weights . array , self . array ) \n 
~~ def bprop ( self ) : \n 
~~~ ca . dot ( self . x . array . T , self . grad_array , self . weights . grad_array ) \n 
ca . dot ( self . grad_array , self . weights . array . T , self . x . grad_array ) \n 
def params ( self ) : \n 
~~~ return self . weights , \n 
~~ @ params . setter \n 
def params ( self , params ) : \n 
~~~ self . weights , = params \n 
~~ ~~ class Affine ( Linear ) : \n 
~~~ def __init__ ( self , n_out , weights , bias = 0.0 ) : \n 
~~~ super ( Affine , self ) . __init__ ( n_out , weights ) \n 
self . bias = Parameter . from_any ( bias ) \n 
~~~ super ( Affine , self ) . setup ( ) \n 
self . bias . setup ( ( 1 , self . n_out ) ) \n 
~~~ super ( Affine , self ) . fprop ( ) \n 
self . array += self . bias . array \n 
~~~ super ( Affine , self ) . bprop ( ) \n 
ca . sum ( self . grad_array , axis = 0 , keepdims = True , \n 
out = self . bias . grad_array ) \n 
~~~ return self . weights , self . bias \n 
~~~ self . weights , self . bias = params \n 
~~ ~~ from copy import copy \n 
from . . base import Model , CollectionMixin , ParamMixin \n 
from . . feed import Feed \n 
class SiameseNetwork ( Model , CollectionMixin ) : \n 
~~~ def __init__ ( self , siamese_layers , loss ) : \n 
~~~ self . layers = siamese_layers \n 
self . loss = loss \n 
self . layers2 = [ copy ( layer ) for layer in self . layers ] \n 
for layer1 , layer2 in zip ( self . layers , self . layers2 ) : \n 
~~~ if isinstance ( layer1 , ParamMixin ) : \n 
~~~ layer2 . params = [ p . share ( ) for p in layer1 . params ] \n 
~~ ~~ self . bprop_until = next ( ( idx for idx , l in enumerate ( self . layers ) \n 
if isinstance ( l , ParamMixin ) ) , 0 ) \n 
self . layers [ self . bprop_until ] . bprop_to_x = False \n 
self . layers2 [ self . bprop_until ] . bprop_to_x = False \n 
self . collection = self . layers + self . layers2 \n 
self . _initialized = False \n 
~~ def setup ( self , x1_shape , x2_shape , y_shape = None ) : \n 
~~~ if self . _initialized : \n 
~~ next_shape = x1_shape \n 
for layer in self . layers : \n 
~~~ layer . setup ( next_shape ) \n 
next_shape = layer . y_shape ( next_shape ) \n 
~~ next_shape = x2_shape \n 
for layer in self . layers2 : \n 
~~ next_shape = self . loss . y_shape ( next_shape ) \n 
self . _initialized = True \n 
~~ def update ( self , x1 , x2 , y ) : \n 
~~~ self . phase = \n 
~~~ x1 = layer . fprop ( x1 ) \n 
~~ for layer in self . layers2 : \n 
~~~ x2 = layer . fprop ( x2 ) \n 
~~ grad1 , grad2 = self . loss . grad ( y , x1 , x2 ) \n 
layers = self . layers [ self . bprop_until : ] \n 
for layer in reversed ( layers [ 1 : ] ) : \n 
~~~ grad1 = layer . bprop ( grad1 ) \n 
~~ layers [ 0 ] . bprop ( grad1 ) \n 
layers2 = self . layers2 [ self . bprop_until : ] \n 
for layer in reversed ( layers2 [ 1 : ] ) : \n 
~~~ grad2 = layer . bprop ( grad2 ) \n 
~~ layers2 [ 0 ] . bprop ( grad2 ) \n 
return self . loss . loss ( y , x1 , x2 ) \n 
~~ def embed ( self , feed ) : \n 
feed = Feed . from_any ( feed ) \n 
next_shape = feed . x . shape \n 
~~~ next_shape = layer . y_shape ( next_shape ) \n 
~~ feats = [ ] \n 
for x , in feed . batches ( ) : \n 
~~~ for layer in self . layers : \n 
~~~ x = layer . fprop ( x ) \n 
~~ feats . append ( np . array ( x ) ) \n 
~~ feats = np . concatenate ( feats ) [ : feed . n_samples ] \n 
return feats \n 
~~ def distances ( self , feed ) : \n 
dists = [ ] \n 
for x1 , x2 in feed . batches ( ) : \n 
~~ dists . append ( np . ravel ( np . array ( self . loss . fprop ( x1 , x2 ) ) ) ) \n 
~~ dists = np . concatenate ( dists ) [ : feed . n_samples ] \n 
return dists \n 
from . . util . singleton import Singleton \n 
from . . util . compat import with_metaclass \n 
from . color import Color \n 
from . level import Level \n 
class Messenger ( with_metaclass ( Singleton , object ) ) : \n 
~~~ def __init__ ( self , level = Level . LOWINFO ) : \n 
~~~ self . set_level ( level ) \n 
~~ def set_level ( self , level ) : \n 
~~~ self . _level = level \n 
~~ def log ( self , level , message ) : \n 
~~~ if ( level >= self . _level ) : \n 
~~~ print ( % ( self . _color ( level ) , message , self . _reset ( ) ) ) \n 
~~ ~~ def debug ( self , message ) : \n 
~~~ self . log ( Level . DEBUG , message ) \n 
~~ def lowinfo ( self , message ) : \n 
~~~ self . log ( Level . LOWINFO , message ) \n 
~~ def info ( self , message ) : \n 
~~~ self . log ( Level . INFO , message ) \n 
~~ def warning ( self , message ) : \n 
~~~ self . log ( Level . WARNING , message ) \n 
~~ def error ( self , message ) : \n 
~~~ self . log ( Level . ERROR , message ) \n 
~~ def _color ( self , level ) : \n 
if not sys . stdout . isatty ( ) : \n 
~~ elif level < Level . DEBUG : \n 
~~ elif Level . DEBUG <= level < Level . LOWINFO : \n 
~~~ return Color . YELLOW \n 
~~ elif Level . LOWINFO <= level < Level . INFO : \n 
~~~ return Color . BLUE \n 
~~ elif Level . INFO <= level < Level . WARNING : \n 
~~~ return Color . GREEN \n 
~~ elif Level . WARNING <= level < Level . ERROR : \n 
~~~ return Color . MAGENTA \n 
~~ elif Level . ERROR <= level : \n 
~~~ return Color . RED \n 
~~ ~~ def _reset ( self ) : \n 
~~~ return Color . RESET \n 
~~ ~~ ~~ from . import alexa_io \n 
ResponseBuilder = alexa_io . ResponseBuilder \n 
alexa = alexa_io . VoiceHandler ( ) \n 
Request = alexa_io . Request \n 
import runpy \n 
from django . core . management . base import BaseCommand , CommandError \n 
def run ( filename , args ) : \n 
~~~ if hasattr ( args , "split" ) : \n 
~~~ args = args . split ( ) \n 
~~ sys . argv = [ filename ] + args \n 
runpy . run_path ( filename , globals ( ) , run_name = "__main__" ) \n 
~~ class Command ( BaseCommand ) : \n 
help = __doc__ \n 
def handle ( self , * args , ** options ) : \n 
~~~ if not args : \n 
~~ args = map ( operator . methodcaller ( "split" , ":" ) , args ) \n 
for arg in args : \n 
~~~ filename , filename_args = arg \n 
~~~ filename , = arg \n 
filename_args = "" \n 
~~ if not path . isfile ( filename ) : \n 
~~ run ( filename , filename_args ) \n 
~~ ~~ ~~ from decimal import Decimal \n 
from . base import Object , Array , String , Field \n 
from . import constants \n 
from . import request \n 
class Impression ( Object ) : \n 
~~~ impid = Field ( String , required = True ) \n 
wseat = Field ( Array ( String ) ) \n 
h = Field ( int ) \n 
w = Field ( int ) \n 
pos = Field ( constants . AdPosition ) \n 
instl = Field ( int ) \n 
btype = Field ( Array ( constants . BannerType ) ) \n 
battr = Field ( Array ( constants . CreativeAttribute ) ) \n 
~~ class Device ( Object ) : \n 
~~~ did = Field ( String ) \n 
dpid = Field ( String ) \n 
ip = Field ( String ) \n 
country = Field ( String ) \n 
carrier = Field ( String ) \n 
ua = Field ( String ) \n 
make = Field ( String ) \n 
model = Field ( String ) \n 
os = Field ( String ) \n 
osv = Field ( String ) \n 
js = Field ( int ) \n 
loc = Field ( String ) \n 
~~ class User ( Object ) : \n 
~~~ uid = Field ( String ) \n 
yob = Field ( int ) \n 
gender = Field ( String ) \n 
zip = Field ( String ) \n 
keywords = Field ( String ) \n 
~~ class Site ( Object ) : \n 
~~~ sid = Field ( String ) \n 
name = Field ( String ) \n 
domain = Field ( String ) \n 
pid = Field ( String ) \n 
pub = Field ( String ) \n 
pdomain = Field ( String ) \n 
cat = Field ( Array ( String ) ) \n 
page = Field ( String ) \n 
ref = Field ( String ) \n 
search = Field ( String ) \n 
~~ class App ( Object ) : \n 
~~~ aid = Field ( String ) \n 
ver = Field ( String ) \n 
bundle = Field ( String ) \n 
paid = Field ( int ) \n 
~~ class Restrictions ( Object ) : \n 
~~~ bcat = Field ( Array ( String ) ) \n 
badv = Field ( Array ( String ) ) \n 
~~ class BidRequest ( Object ) : \n 
~~~ id = Field ( String , required = True ) \n 
at = Field ( constants . AuctionType , default = constants . AuctionType . SECOND_PRICE ) \n 
tmax = Field ( int ) \n 
imp = Field ( Array ( Impression ) , required = True ) \n 
site = Field ( Site , default = Site ( ) ) \n 
app = Field ( App , default = App ( ) ) \n 
device = Field ( Device , default = Device ( ) ) \n 
user = Field ( User , default = User ( ) ) \n 
restrictions = Field ( Restrictions , default = Restrictions ( ) ) \n 
@ staticmethod \n 
def minimal ( id , imp_id ) : \n 
~~~ return BidRequest ( id = id , imp = [ Impression ( impid = imp_id ) ] ) \n 
~~ ~~ class OpenRTB20Adapter ( object ) : \n 
~~~ def __init__ ( self , brq ) : \n 
~~~ self . mobile_brq = brq \n 
params = { \n 
: brq . id , \n 
: [ \n 
request . Impression ( \n 
id = imp . impid , \n 
banner = request . Banner ( \n 
w = imp . w , \n 
h = imp . h , \n 
pos = imp . pos , \n 
battr = imp . battr , \n 
btype = imp . btype , \n 
bidfloor = Decimal ( 0 ) , \n 
) for imp in brq . imp \n 
: brq . at , \n 
: brq . tmax \n 
if brq . site : \n 
~~~ params [ ] = request . Site ( \n 
id = brq . site . sid , \n 
name = brq . site . name , \n 
domain = brq . site . domain , \n 
publisher = request . Publisher ( \n 
id = brq . site . pid , \n 
name = brq . site . pub , \n 
domain = brq . site . pdomain \n 
cat = brq . site . cat , \n 
keywords = brq . site . keywords , \n 
page = brq . site . page , \n 
ref = brq . site . ref , \n 
search = brq . site . search , \n 
~~ if brq . app : \n 
~~~ params [ ] = request . App ( \n 
id = brq . app . aid , \n 
name = brq . app . name , \n 
domain = brq . app . domain , \n 
id = brq . app . pid , \n 
name = brq . app . pub , \n 
pdomain = brq . app . pdomain , \n 
cat = brq . app . cat , \n 
keywords = brq . app . keywords , \n 
ver = brq . app . ver , \n 
bundle = brq . app . bundle , \n 
paid = brq . app . paid , \n 
~~ if brq . device : \n 
~~~ lat = lon = None \n 
if brq . device . loc : \n 
~~~ lat , lon = brq . device . loc . split ( ) \n 
lat , lon = float ( lat ) , float ( lon ) \n 
~~ params [ ] = request . Device ( \n 
didsha1 = brq . device . did , \n 
dpidsha1 = brq . device . dpid , \n 
ip = brq . device . ip , \n 
geo = request . Geo ( \n 
lat = lat , \n 
lon = lon , \n 
country = brq . device . country , \n 
carrier = brq . device . carrier , \n 
ua = brq . device . ua , \n 
make = brq . device . make , \n 
model = brq . device . model , \n 
os = brq . device . os , \n 
osv = brq . device . osv , \n 
js = brq . device . js , \n 
connectiontype = constants . ConnectionType . CELLULAR_UNKNOWN_G , \n 
devicetype = constants . DeviceType . MOBILE \n 
~~ if brq . user : \n 
~~~ params [ ] = request . User ( \n 
id = brq . user . uid , \n 
yob = brq . user . yob , \n 
gender = brq . user . gender , \n 
keywords = brq . user . keywords , \n 
country = brq . user . country , \n 
zip = brq . user . zip \n 
~~ if brq . restrictions : \n 
~~~ params [ ] = brq . restrictions . badv \n 
params [ ] = brq . restrictions . bcat \n 
~~ self . brq = request . BidRequest ( ** params ) \n 
~~ def __getattr__ ( self , k ) : \n 
~~~ return getattr ( self . brq , k ) \n 
def deserialize ( data ) : \n 
~~~ brq = BidRequest . deserialize ( data ) \n 
~~ ~~ import click \n 
from tower_cli import models , resources \n 
from tower_cli . utils import types \n 
class Resource ( models . Resource ) : \n 
~~~ cli_help = \n 
endpoint = \n 
identity = ( , ) \n 
name = models . Field ( unique = True ) \n 
description = models . Field ( required = False , display = False ) \n 
inventory = models . Field ( type = types . Related ( ) ) \n 
enabled = models . Field ( type = bool , required = False ) \n 
variables = models . Field ( type = types . File ( ) , required = False , \n 
display = False ) \n 
@ resources . command ( use_fields_as_options = False ) \n 
@ click . option ( , type = types . Related ( ) ) \n 
def associate ( self , host , group ) : \n 
return self . _assoc ( , host , group ) \n 
~~ @ resources . command ( use_fields_as_options = False ) \n 
def disassociate ( self , host , group ) : \n 
return self . _disassoc ( , host , group ) \n 
~~ ~~ import tower_cli \n 
from tower_cli . api import client \n 
from tower_cli . utils import exceptions as exc \n 
from tests . compat import unittest , mock \n 
from tower_cli . conf import settings \n 
import click \n 
class LaunchTests ( unittest . TestCase ) : \n 
~~~ self . res = tower_cli . get_resource ( ) \n 
~~ def test_basic_launch ( self ) : \n 
with client . test_mode as t : \n 
~~~ t . register_json ( , { : 42 } , method = ) \n 
t . register_json ( , { \n 
: \n 
} , method = ) \n 
t . register_json ( , { : 42 } , method = ) \n 
result = self . res . launch ( inventory = "foobar" , machine_credential = 2 ) \n 
self . assertEqual ( result , { : True , : 42 } ) \n 
~~ ~~ def test_launch_with_become ( self ) : \n 
self . res . launch ( inventory = "foobar" , machine_credential = 2 , \n 
become = True ) \n 
self . assertDictContainsSubset ( \n 
{ : True } , \n 
json . loads ( t . requests [ 1 ] . body ) \n 
~~ ~~ def test_basic_launch_with_echo ( self ) : \n 
t . register_json ( \n 
{ : True , : 42 , \n 
: , : 2 , \n 
: , : 1234 , : 2352 , \n 
: , : , \n 
: , } , method = \n 
self . assertEqual ( result [ ] , True ) \n 
self . assertEqual ( result [ ] , 42 ) \n 
f = self . res . as_command ( ) . _echo_method ( self . res . launch ) \n 
with mock . patch . object ( click , ) : \n 
~~~ with settings . runtime_values ( format = ) : \n 
~~~ f ( inventory = "foobar" , machine_credential = 2 ) \n 
~~ ~~ ~~ ~~ def test_launch_with_param ( self ) : \n 
result = self . res . launch ( inventory = "foobar" , machine_credential = 2 , \n 
~~ ~~ def test_version_failure ( self ) : \n 
t . register_json ( , { } , method = ) \n 
with self . assertRaises ( exc . TowerCLIError ) : \n 
~~~ self . res . launch ( inventory = 1 , machine_credential = 2 , \n 
~~ ~~ ~~ def test_basic_launch_monitor_option ( self ) : \n 
with mock . patch . object ( type ( self . res ) , ) as monitor : \n 
monitor . assert_called_once_with ( 42 , timeout = None ) \n 
~~ ~~ ~~ ~~ class StatusTests ( unittest . TestCase ) : \n 
~~ def test_normal ( self ) : \n 
~~~ t . register_json ( , { \n 
: 1335024000.0 , \n 
: , \n 
: False , \n 
result = self . res . status ( 42 ) \n 
self . assertEqual ( result , { \n 
self . assertEqual ( len ( t . requests ) , 1 ) \n 
~~ ~~ def test_detailed ( self ) : \n 
~~~ with client . test_mode as t : \n 
result = self . res . status ( 42 , detail = True ) \n 
~~ ~~ ~~ class CancelTests ( unittest . TestCase ) : \n 
~~ def test_standard_cancelation ( self ) : \n 
~~~ t . register ( , , method = ) \n 
result = self . res . cancel ( 42 ) \n 
t . requests [ 0 ] . url . endswith ( ) \n 
self . assertTrue ( result [ ] ) \n 
~~ ~~ def test_cancelation_completed_with_error ( self ) : \n 
~~~ t . register ( , , \n 
method = , status_code = 405 ) \n 
~~~ self . res . cancel ( 42 , fail_if_not_running = True ) \n 
import pandas as pd \n 
df = pd . read_csv ( , sep = ";" ) \n 
df . columns = [ , , , \n 
files = glob . glob ( ) \n 
for path_file in files : \n 
~~~ year = str ( path_file [ - 8 : - 4 ] ) \n 
if ( year != ) : \n 
~~~ df_temp = pd . read_csv ( path_file , sep = ) \n 
year_col = [ , ] \n 
features_col = [ ] \n 
for col in df_temp . columns [ 2 : ] : \n 
~~~ year_col . append ( col + "_" + year ) \n 
features_col . append ( col + "_" + year ) \n 
~~ features_col . append ( ) \n 
df_temp . columns = year_col \n 
df = pd . merge ( df , df_temp [ features_col ] , how = , on = ) \n 
~~ ~~ list_col = [ ] \n 
for col in df . columns : \n 
~~~ list_col . append ( col ) \n 
~~ ~~ df . columns = list_col \n 
df . to_csv ( , encoding = , index = False ) \n 
def get_pars ( text ) : \n 
~~~ pars = re . findall ( , text ) \n 
pars = [ p . strip ( ) for p in pars ] \n 
return pars \n 
~~ def get_claims ( text ) : \n 
~~~ claims = re . findall ( , text , re . IGNORECASE ) \n 
return claims \n 
~~ def get_claim_body ( text ) : \n 
~~~ match = re . search ( , text , re . IGNORECASE | re . MULTILINE ) \n 
~~~ claim = match . string [ match . span ( ) [ 0 ] : ] \n 
claim = claim . replace ( , ) \n 
return claim \n 
~~ except ( AttributeError ) : \n 
text = sys . stdin . read ( ) \n 
for p in get_pars ( text ) : \n 
print p \n 
~~ for p in get_claims ( text ) : \n 
~~ print \n 
print get_claim_body ( text ) \n 
~~ from django . contrib import admin \n 
from django . db import models \n 
from . models import Text \n 
from . widgets import HTMLEditorWidget \n 
class TextAdmin ( admin . ModelAdmin ) : \n 
~~~ list_display = ( , , , ) \n 
search_fields = ( , ) \n 
formfield_overrides = { \n 
models . TextField : { : HTMLEditorWidget } , \n 
~~ admin . site . register ( Text , TextAdmin ) \n 
from django . template . base import Node \n 
from inspect import getargspec \n 
from functools import partial \n 
from . parse_bits import parse_bits \n 
def simple_block_tag ( register , takes_context = None , name = None ) : \n 
~~~ def dec ( func ) : \n 
~~~ params , varargs , varkw , defaults = getargspec ( func ) \n 
class SimpleNode ( Node ) : \n 
~~~ def __init__ ( self , nodelist , takes_context , args , kwargs ) : \n 
~~~ self . nodelist = nodelist \n 
self . takes_context = takes_context \n 
self . args = args \n 
self . kwargs = kwargs \n 
~~ def get_resolved_arguments ( self , context ) : \n 
~~~ resolved_args = [ var . resolve ( context ) for var in self . args ] \n 
resolved_args = [ self . nodelist . render ( context ) ] + resolved_args \n 
if self . takes_context : \n 
~~~ resolved_args = [ context ] + resolved_args \n 
~~ resolved_kwargs = dict ( ( k , v . resolve ( context ) ) \n 
for k , v in self . kwargs . items ( ) ) \n 
return resolved_args , resolved_kwargs \n 
~~ def render ( self , context ) : \n 
~~~ resolved_args , resolved_kwargs = self . get_resolved_arguments ( context ) \n 
return func ( * resolved_args , ** resolved_kwargs ) \n 
~~ ~~ def tag_compiler ( parser , token , params , varargs , varkw , defaults , \n 
name , takes_context , function_name ) : \n 
~~~ bits = token . split_contents ( ) [ 1 : ] \n 
args , kwargs = parse_bits ( parser , bits , params , varargs , varkw , \n 
defaults , takes_context , name ) \n 
nodelist = parser . parse ( ( . format ( function_name ) , ) ) \n 
parser . delete_first_token ( ) \n 
return SimpleNode ( nodelist , takes_context , args , kwargs ) \n 
~~ function_name = ( name or \n 
getattr ( func , , func ) . __name__ ) \n 
compile_func = partial ( tag_compiler , \n 
params = params , varargs = varargs , varkw = varkw , \n 
defaults = defaults , name = function_name , \n 
takes_context = takes_context , function_name = function_name ) \n 
compile_func . __doc__ = func . __doc__ \n 
register . tag ( function_name , compile_func ) \n 
return func \n 
~~ return dec \n 
import calendar \n 
from email . utils import parsedate_tz \n 
from pip . _vendor . requests . structures import CaseInsensitiveDict \n 
from . cache import DictCache \n 
from . serialize import Serializer \n 
URI = re . compile ( r"^(([^:/?#]+):)?(//([^/?#]*))?([^?#]*)(\\?([^#]*))?(#(.*))?" ) \n 
def parse_uri ( uri ) : \n 
groups = URI . match ( uri ) . groups ( ) \n 
return ( groups [ 1 ] , groups [ 3 ] , groups [ 4 ] , groups [ 6 ] , groups [ 8 ] ) \n 
~~ class CacheController ( object ) : \n 
def __init__ ( self , cache = None , cache_etags = True , serializer = None ) : \n 
~~~ self . cache = cache or DictCache ( ) \n 
self . cache_etags = cache_etags \n 
self . serializer = serializer or Serializer ( ) \n 
def _urlnorm ( cls , uri ) : \n 
( scheme , authority , path , query , fragment ) = parse_uri ( uri ) \n 
if not scheme or not authority : \n 
~~ scheme = scheme . lower ( ) \n 
authority = authority . lower ( ) \n 
~~~ path = "/" \n 
~~ request_uri = query and "?" . join ( [ path , query ] ) or path \n 
defrag_uri = scheme + "://" + authority + request_uri \n 
return defrag_uri \n 
def cache_url ( cls , uri ) : \n 
~~~ return cls . _urlnorm ( uri ) \n 
~~ def parse_cache_control ( self , headers ) : \n 
retval = { } \n 
cc_header = \n 
if in headers : \n 
~~~ cc_header = \n 
~~ if cc_header in headers : \n 
~~~ parts = headers [ cc_header ] . split ( ) \n 
parts_with_args = [ \n 
tuple ( [ x . strip ( ) . lower ( ) for x in part . split ( "=" , 1 ) ] ) \n 
for part in parts if - 1 != part . find ( "=" ) \n 
parts_wo_args = [ \n 
( name . strip ( ) . lower ( ) , 1 ) \n 
for name in parts if - 1 == name . find ( "=" ) \n 
retval = dict ( parts_with_args + parts_wo_args ) \n 
~~ return retval \n 
~~ def cached_request ( self , request ) : \n 
cache_url = self . cache_url ( request . url ) \n 
cc = self . parse_cache_control ( request . headers ) \n 
no_cache = True if in cc else False \n 
if in cc and cc [ ] == 0 : \n 
~~~ no_cache = True \n 
~~ if no_cache : \n 
~~ resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n 
if not resp : \n 
~~ if resp . status == 301 : \n 
~~~ return resp \n 
~~ headers = CaseInsensitiveDict ( resp . headers ) \n 
if not headers or not in headers : \n 
~~~ if not in headers : \n 
~~~ self . cache . delete ( cache_url ) \n 
~~ now = time . time ( ) \n 
date = calendar . timegm ( \n 
parsedate_tz ( headers [ ] ) \n 
current_age = max ( 0 , now - date ) \n 
resp_cc = self . parse_cache_control ( headers ) \n 
freshness_lifetime = 0 \n 
if in resp_cc and resp_cc [ ] . isdigit ( ) : \n 
~~~ freshness_lifetime = int ( resp_cc [ ] ) \n 
~~ elif in headers : \n 
~~~ expires = parsedate_tz ( headers [ ] ) \n 
if expires is not None : \n 
~~~ expire_time = calendar . timegm ( expires ) - date \n 
freshness_lifetime = max ( 0 , expire_time ) \n 
~~ ~~ if in cc : \n 
~~~ freshness_lifetime = int ( cc [ ] ) \n 
~~~ freshness_lifetime = 0 \n 
~~~ min_fresh = int ( cc [ ] ) \n 
~~~ min_fresh = 0 \n 
~~ current_age += min_fresh \n 
~~ fresh = ( freshness_lifetime > current_age ) \n 
if fresh : \n 
~~ if not in headers : \n 
~~ def conditional_headers ( self , request ) : \n 
~~~ cache_url = self . cache_url ( request . url ) \n 
resp = self . serializer . loads ( request , self . cache . get ( cache_url ) ) \n 
new_headers = { } \n 
if resp : \n 
~~~ headers = CaseInsensitiveDict ( resp . headers ) \n 
~~~ new_headers [ ] = headers [ ] \n 
~~ if in headers : \n 
~~ ~~ return new_headers \n 
~~ def cache_response ( self , request , response , body = None ) : \n 
if response . status not in [ 200 , 203 , 300 , 301 ] : \n 
~~ response_headers = CaseInsensitiveDict ( response . headers ) \n 
cc_req = self . parse_cache_control ( request . headers ) \n 
cc = self . parse_cache_control ( response_headers ) \n 
no_store = cc . get ( ) or cc_req . get ( ) \n 
if no_store and self . cache . get ( cache_url ) : \n 
~~ if self . cache_etags and in response_headers : \n 
~~~ self . cache . set ( \n 
cache_url , \n 
self . serializer . dumps ( request , response , body = body ) , \n 
~~ elif response . status == 301 : \n 
self . serializer . dumps ( request , response ) \n 
~~ elif in response_headers : \n 
~~~ if cc and cc . get ( ) : \n 
~~~ if int ( cc [ ] ) > 0 : \n 
~~ ~~ elif in response_headers : \n 
~~~ if response_headers [ ] : \n 
~~ ~~ ~~ ~~ def update_cached_response ( self , request , response ) : \n 
cached_response = self . serializer . loads ( \n 
request , \n 
self . cache . get ( cache_url ) \n 
if not cached_response : \n 
~~~ return response \n 
~~ excluded_headers = [ \n 
"content-length" , \n 
cached_response . headers . update ( \n 
dict ( ( k , v ) for k , v in response . headers . items ( ) \n 
if k . lower ( ) not in excluded_headers ) \n 
cached_response . status = 200 \n 
self . cache . set ( \n 
self . serializer . dumps ( request , cached_response ) , \n 
return cached_response \n 
~~ ~~ from __future__ import absolute_import , division , unicode_literals \n 
from pip . _vendor . six import text_type , string_types \n 
__all__ = [ "DOCUMENT" , "DOCTYPE" , "TEXT" , "ELEMENT" , "COMMENT" , "ENTITY" , "UNKNOWN" , \n 
"TreeWalker" , "NonRecursiveTreeWalker" ] \n 
from xml . dom import Node \n 
DOCUMENT = Node . DOCUMENT_NODE \n 
DOCTYPE = Node . DOCUMENT_TYPE_NODE \n 
TEXT = Node . TEXT_NODE \n 
ELEMENT = Node . ELEMENT_NODE \n 
COMMENT = Node . COMMENT_NODE \n 
ENTITY = Node . ENTITY_NODE \n 
UNKNOWN = "<#UNKNOWN#>" \n 
from . . constants import voidElements , spaceCharacters \n 
spaceCharacters = "" . join ( spaceCharacters ) \n 
def to_text ( s , blank_if_none = True ) : \n 
if s is None : \n 
~~~ if blank_if_none : \n 
~~ ~~ elif isinstance ( s , text_type ) : \n 
~~~ return s \n 
~~~ return text_type ( s ) \n 
~~ ~~ def is_text_or_none ( string ) : \n 
return string is None or isinstance ( string , string_types ) \n 
~~ class TreeWalker ( object ) : \n 
~~~ def __init__ ( self , tree ) : \n 
~~~ self . tree = tree \n 
~~ def __iter__ ( self ) : \n 
~~ def error ( self , msg ) : \n 
~~~ return { "type" : "SerializeError" , "data" : msg } \n 
~~ def emptyTag ( self , namespace , name , attrs , hasChildren = False ) : \n 
~~~ assert namespace is None or isinstance ( namespace , string_types ) , type ( namespace ) \n 
assert isinstance ( name , string_types ) , type ( name ) \n 
assert all ( ( namespace is None or isinstance ( namespace , string_types ) ) and \n 
isinstance ( name , string_types ) and \n 
isinstance ( value , string_types ) \n 
for ( namespace , name ) , value in attrs . items ( ) ) \n 
yield { "type" : "EmptyTag" , "name" : to_text ( name , False ) , \n 
"namespace" : to_text ( namespace ) , \n 
"data" : attrs } \n 
if hasChildren : \n 
~~ ~~ def startTag ( self , namespace , name , attrs ) : \n 
return { "type" : "StartTag" , \n 
"name" : text_type ( name ) , \n 
"data" : dict ( ( ( to_text ( namespace , False ) , to_text ( name ) ) , \n 
to_text ( value , False ) ) \n 
for ( namespace , name ) , value in attrs . items ( ) ) } \n 
~~ def endTag ( self , namespace , name ) : \n 
assert isinstance ( name , string_types ) , type ( namespace ) \n 
return { "type" : "EndTag" , \n 
"name" : to_text ( name , False ) , \n 
"data" : { } } \n 
~~ def text ( self , data ) : \n 
~~~ assert isinstance ( data , string_types ) , type ( data ) \n 
data = to_text ( data ) \n 
middle = data . lstrip ( spaceCharacters ) \n 
left = data [ : len ( data ) - len ( middle ) ] \n 
if left : \n 
~~~ yield { "type" : "SpaceCharacters" , "data" : left } \n 
~~ data = middle \n 
middle = data . rstrip ( spaceCharacters ) \n 
right = data [ len ( middle ) : ] \n 
if middle : \n 
~~~ yield { "type" : "Characters" , "data" : middle } \n 
~~ if right : \n 
~~~ yield { "type" : "SpaceCharacters" , "data" : right } \n 
~~ ~~ def comment ( self , data ) : \n 
return { "type" : "Comment" , "data" : text_type ( data ) } \n 
~~ def doctype ( self , name , publicId = None , systemId = None , correct = True ) : \n 
~~~ assert is_text_or_none ( name ) , type ( name ) \n 
assert is_text_or_none ( publicId ) , type ( publicId ) \n 
assert is_text_or_none ( systemId ) , type ( systemId ) \n 
return { "type" : "Doctype" , \n 
"name" : to_text ( name ) , \n 
"publicId" : to_text ( publicId ) , \n 
"systemId" : to_text ( systemId ) , \n 
"correct" : to_text ( correct ) } \n 
~~ def entity ( self , name ) : \n 
~~~ assert isinstance ( name , string_types ) , type ( name ) \n 
return { "type" : "Entity" , "name" : text_type ( name ) } \n 
~~ def unknown ( self , nodeType ) : \n 
~~ ~~ class NonRecursiveTreeWalker ( TreeWalker ) : \n 
~~~ def getNodeDetails ( self , node ) : \n 
~~ def getFirstChild ( self , node ) : \n 
~~ def getNextSibling ( self , node ) : \n 
~~ def getParentNode ( self , node ) : \n 
~~~ currentNode = self . tree \n 
while currentNode is not None : \n 
~~~ details = self . getNodeDetails ( currentNode ) \n 
type , details = details [ 0 ] , details [ 1 : ] \n 
hasChildren = False \n 
if type == DOCTYPE : \n 
~~~ yield self . doctype ( * details ) \n 
~~ elif type == TEXT : \n 
~~~ for token in self . text ( * details ) : \n 
~~~ yield token \n 
~~ ~~ elif type == ELEMENT : \n 
~~~ namespace , name , attributes , hasChildren = details \n 
if name in voidElements : \n 
~~~ for token in self . emptyTag ( namespace , name , attributes , \n 
hasChildren ) : \n 
~~ hasChildren = False \n 
~~~ yield self . startTag ( namespace , name , attributes ) \n 
~~ ~~ elif type == COMMENT : \n 
~~~ yield self . comment ( details [ 0 ] ) \n 
~~ elif type == ENTITY : \n 
~~~ yield self . entity ( details [ 0 ] ) \n 
~~ elif type == DOCUMENT : \n 
~~~ hasChildren = True \n 
~~~ yield self . unknown ( details [ 0 ] ) \n 
~~ if hasChildren : \n 
~~~ firstChild = self . getFirstChild ( currentNode ) \n 
~~~ firstChild = None \n 
~~ if firstChild is not None : \n 
~~~ currentNode = firstChild \n 
~~~ while currentNode is not None : \n 
if type == ELEMENT : \n 
if name not in voidElements : \n 
~~~ yield self . endTag ( namespace , name ) \n 
~~ ~~ if self . tree is currentNode : \n 
~~~ currentNode = None \n 
~~ nextSibling = self . getNextSibling ( currentNode ) \n 
if nextSibling is not None : \n 
~~~ currentNode = nextSibling \n 
~~~ currentNode = self . getParentNode ( currentNode ) \n 
~~ ~~ ~~ ~~ ~~ ~~ from __future__ import absolute_import \n 
from pip . basecommand import Command , SUCCESS \n 
from pip . exceptions import CommandError \n 
class HelpCommand ( Command ) : \n 
summary = \n 
def run ( self , options , args ) : \n 
~~~ from pip . commands import commands_dict , get_similar_commands \n 
~~~ return SUCCESS \n 
~~ if cmd_name not in commands_dict : \n 
~~~ guess = get_similar_commands ( cmd_name ) \n 
if guess : \n 
~~ raise CommandError ( . join ( msg ) ) \n 
~~ command = commands_dict [ cmd_name ] ( ) \n 
command . parser . print_help ( ) \n 
return SUCCESS \n 
class PipDeprecationWarning ( Warning ) : \n 
~~ class RemovedInPip8Warning ( PipDeprecationWarning , PendingDeprecationWarning ) : \n 
~~ class RemovedInPip9Warning ( PipDeprecationWarning , PendingDeprecationWarning ) : \n 
~~ DEPRECATIONS = [ RemovedInPip8Warning , RemovedInPip9Warning ] \n 
_warnings_showwarning = None \n 
def _showwarning ( message , category , filename , lineno , file = None , line = None ) : \n 
~~~ if file is not None : \n 
~~~ if _warnings_showwarning is not None : \n 
~~~ _warnings_showwarning ( \n 
message , category , filename , lineno , file , line , \n 
~~~ if issubclass ( category , PipDeprecationWarning ) : \n 
~~~ logger = logging . getLogger ( "pip.deprecations" ) \n 
if issubclass ( category , DeprecationWarning ) : \n 
~~~ logger . error ( log_message ) \n 
~~~ logger . warning ( log_message ) \n 
~~ ~~ ~~ def install_warning_logger ( ) : \n 
~~~ global _warnings_showwarning \n 
if _warnings_showwarning is None : \n 
~~~ _warnings_showwarning = warnings . showwarning \n 
warnings . showwarning = _showwarning \n 
from math import sqrt \n 
def z_score ( rule ) : \n 
~~~ return sqrt ( rule . coverage ) * ( rule . mean - rule . kb . mean ) / rule . kb . sd \n 
~~ def t_score ( rule ) : \n 
~~~ return sqrt ( rule . coverage ) * ( rule . mean - rule . kb . mean ) / rule . sd \n 
~~ def enrichment_score ( rule ) : \n 
~~~ if rule . coverage == rule . kb . n_examples ( ) : \n 
~~~ return 1.0 \n 
~~ if rule . coverage == 0 : \n 
~~~ return - 1 / float ( rule . kb . n_examples ( ) ) \n 
~~ increment = { } \n 
incr1 = 1 / float ( rule . coverage ) \n 
incr2 = 1 / float ( rule . kb . n_examples ( ) - rule . coverage ) \n 
max_diff = 0 \n 
for ex in rule . kb . examples : \n 
~~~ increment [ ex ] = - incr2 \n 
~~ for ex in rule . examples ( ) : \n 
~~~ increment [ ex ] = incr1 \n 
~~ partial = 0 \n 
~~~ partial += increment [ ex ] \n 
if partial > max_diff : \n 
~~~ max_diff = partial \n 
~~ ~~ return max_diff \n 
~~ def wracc ( rule ) : \n 
~~~ nX = rule . coverage \n 
N = len ( rule . kb . examples ) \n 
nXY = rule . distribution [ rule . target ] \n 
nY = rule . kb . distribution [ rule . target ] \n 
if nX : \n 
~~~ return nX / float ( N ) * ( nXY / float ( nX ) - nY / float ( N ) ) \n 
~~ ~~ def precision ( rule ) : \n 
~~~ return nXY / float ( nX ) \n 
~~ ~~ def chisq ( rule ) : \n 
~~~ N = len ( rule . kb . examples ) \n 
z = rule . distribution [ rule . target ] / float ( N ) \n 
x = rule . coverage / float ( N ) \n 
y = rule . kb . distribution [ rule . target ] / float ( N ) \n 
if x not in [ 0 , 1 ] and y not in [ 0 , 1 ] : \n 
~~~ return N * ( z - x * y ) ** 2 / float ( x * y * ( 1 - x ) * ( 1 - y ) ) \n 
~~ ~~ def lift ( rule ) : \n 
~~~ nX = float ( rule . coverage ) \n 
N = float ( len ( rule . kb . examples ) ) \n 
if nX != 0 and N != 0 : \n 
~~~ return ( nXY / nX ) / ( nY / N ) \n 
~~ ~~ def leverage ( rule ) : \n 
if N != 0 : \n 
~~~ return nXY / N - ( nX / N ) * ( nY / N ) \n 
~~ ~~ def kaplan_meier_AUC ( rule ) : \n 
~~~ examples = rule . kb . bits_to_indices ( rule . covered_examples ) \n 
n_examples = float ( len ( examples ) ) \n 
if n_examples == 0 : \n 
~~~ return 0.0 \n 
~~ def n_alive ( examples , day ) : \n 
~~~ def is_alive ( ex ) : \n 
~~~ return rule . kb . get_score ( ex ) > day \n 
~~ return len ( filter ( is_alive , examples ) ) \n 
~~ auc , day = 0 , 0 \n 
prev = - 1 \n 
~~~ alive = n_alive ( examples , day ) \n 
day += 14 \n 
curr = alive / n_examples \n 
if prev != - 1 : \n 
~~~ auc += ( prev + curr ) / 2.0 \n 
~~ prev = curr \n 
if alive == 0 : \n 
~~ ~~ return auc \n 
~~ _bounds = { \n 
z_score : ( 0 , float ( ) ) , \n 
t_score : ( 0 , float ( ) ) , \n 
kaplan_meier_AUC : ( 0 , float ( ) ) , \n 
enrichment_score : ( - float ( ) , 1 ) , \n 
wracc : ( 0 , 1 ) , \n 
precision : ( 0 , 1 ) , \n 
chisq : ( 0 , float ( ) ) , \n 
lift : ( 1 , float ( ) ) , \n 
leverage : ( 0 , 1 ) \n 
def interesting ( rule ) : \n 
score_fun = rule . kb . score_fun \n 
return _bounds [ score_fun ] [ 0 ] < rule . score <= _bounds [ score_fun ] [ 1 ] \n 
~~ import pdb \n 
import shutil \n 
import os , sys \n 
from numpy import arange \n 
from Toolbox . CMORresources import CMORAttributes \n 
class ESGFresources : \n 
def __init__ ( self , rcFile ) : \n 
f = open ( rcFile , ) \n 
lines = f . readlines ( ) \n 
self . resources = dict ( [ ( key . strip ( ) , value . strip ( ) , ) \n 
for line in lines if line != and line [ 0 ] != "#" \n 
for ( key , value ) in \n 
[ line . strip ( ) . replace ( "\'" , "" ) . replace ( "\\"" , "" ) . rstrip ( ) . split ( "=" ) ] ] ) \n 
for key in self . resources . keys ( ) : \n 
~~~ self . resources [ key ] = self . resources [ key ] . replace ( "\\\\" , "\\"" ) \n 
~~ self . xcl = None \n 
~~~ self . xcl = self . resources [ ] \n 
rc = self . ReadXCL ( ) \n 
~~ ~~ def __getitem__ ( self , key ) : \n 
return self . resources [ key ] \n 
~~ def __setitem__ ( self , key , value ) : \n 
self . resources [ key ] = value \n 
~~ def __delete__ ( self , key ) : \n 
del self . resources [ key ] \n 
~~ def ReadXCL ( self ) : \n 
~~~ import xlrd \n 
~~ if ( os . path . isfile ( self . xcl ) ) : \n 
~~~ wb = xlrd . open_workbook ( self . xcl ) \n 
raise NameError ( self . xcl ) \n 
~~ sheet = wb . sheet_by_name ( ) \n 
self . resources [ ] = [ sheet . row ( i ) [ 0 ] . value . encode ( , ) for i in arange ( sheet . nrows - 2 ) + 2 ] \n 
self . resources [ ] = [ sheet . row ( i ) [ 1 ] . value . encode ( , ) for i in arange ( sheet . nrows - 2 ) + 2 ] \n 
self . resources [ ] = [ sheet . row ( i ) [ 2 ] . value . encode ( , ) for i in arange ( sheet . nrows - 2 ) + 2 ] \n 
self . resources [ ] = [ sheet . row ( i ) [ 3 ] . value . encode ( , ) for i in arange ( sheet . nrows - 2 ) + 2 ] \n 
self . resources [ ] = [ sheet . row ( i ) [ 6 ] . value . encode ( , ) for i in arange ( sheet . nrows - 2 ) + 2 ] \n 
self . resources [ ] = str ( self . resources [ ] ) \n 
return 1 \n 
~~ ~~ def movefiles ( rc ) : \n 
path = rc [ ] + + rc [ ] + + rc [ ] + + rc [ ] + + rc [ ] + + rc [ ] + + rc [ ] + + rc [ ] + + rc [ ] + + rc [ ] \n 
~~~ rc [ ] == \n 
~~~ path += + rc [ ] + \n 
~~~ os . makedirs ( path ) \n 
~~~ None \n 
~~ cmorpath = rc [ ] + + rc [ ] + + rc [ ] + \n 
for r , d , f in os . walk ( cmorpath ) : \n 
~~~ for files in f : \n 
~~~ if files . endswith ( ".nc" ) : \n 
~~~ filetimestamp = files . split ( ) [ - 1 ] . strip ( ".nc" ) \n 
file = os . path . join ( r , files ) \n 
print file \n 
Attr = CMORAttributes ( file ) \n 
DelGlbAttributes = eval ( rc [ ] . replace ( , ) ) \n 
for attribute in DelGlbAttributes : \n 
Attr . GlbDel ( attribute ) \n 
~~ SetGlbAttributes = eval ( rc [ ] . replace ( , ) ) \n 
for ( attribute , Value ) in SetGlbAttributes : \n 
Attr . GlbSet ( attribute , Value ) \n 
~~ Attr . close ( ) \n 
source = \n 
if ( rc [ ] == ) : \n 
~~~ source = rc [ ] + "z_" \n 
~~ elif ( rc [ ] != ) : \n 
~~~ source = rc [ ] . split ( ) [ 0 ] + \n 
~~ newfilename = rc [ ] + + rc [ ] + + source + rc [ ] + + rc [ ] + + filetimestamp + \n 
newfilename = os . path . join ( path , newfilename ) \n 
print newfilename \n 
os . rename ( file , newfilename ) \n 
~~ ~~ ~~ shutil . rmtree ( rc [ ] ) \n 
return 0 \n 
ESGF_CREDENTIALS = "~/.esg/credentials.pem" \n 
JPL_HOSTNAME = "esgf-node.jpl.nasa.gov" \n 
JPL_MYPROXY_SERVER_DN = "/O=ESGF/OU=ESGF.ORG/CN=esgf-node.jpl.nasa.gov" \n 
JPL_SEARCH_SERVICE_URL = "http://esgf-node.jpl.nasa.gov/esg-search/search" \n 
DEFAULT_ESGF_SEARCH = "http://esgf-node.jpl.nasa.gov/esg-search/search" \n 
from __future__ import with_statement \n 
JIRA_URL = \n 
GITHUB_URL = \n 
def parse_changes_file ( file_path , versions = None ) : \n 
contributors_map = defaultdict ( set ) \n 
in_entry = False \n 
active_version = None \n 
active_tickets = [ ] \n 
with open ( file_path , ) as fp : \n 
~~~ for line in fp : \n 
~~~ line = line . strip ( ) \n 
match = re . search ( \n 
, line ) \n 
if match : \n 
~~~ active_version = match . groups ( ) [ 0 ] \n 
~~ if versions and active_version not in versions : \n 
~~ if line . startswith ( ) or line . startswith ( ) : \n 
~~~ in_entry = True \n 
~~ if in_entry and line == : \n 
~~~ in_entry = False \n 
~~ if in_entry : \n 
~~~ match = re . search ( , line ) \n 
~~~ active_tickets = match . groups ( ) [ 0 ] \n 
active_tickets = active_tickets . split ( ) \n 
active_tickets = [ ticket for ticket in active_tickets if \n 
ticket . startswith ( ) or \n 
ticket . startswith ( ) ] \n 
~~ match = re . search ( , line ) \n 
~~~ contributors = match . groups ( ) [ 0 ] \n 
contributors = contributors . split ( ) \n 
contributors = [ name . strip ( ) for name in contributors ] \n 
for name in contributors : \n 
~~~ name = name . title ( ) \n 
contributors_map [ name ] . update ( set ( active_tickets ) ) \n 
~~ ~~ ~~ ~~ ~~ return contributors_map \n 
~~ def convert_to_markdown ( contributors_map , include_tickets = False ) : \n 
~~~ def compare ( item1 , item2 ) : \n 
~~~ lastname1 = item1 . split ( ) [ - 1 ] . lower ( ) \n 
lastname2 = item2 . split ( ) [ - 1 ] . lower ( ) \n 
return cmp ( lastname1 , lastname2 ) \n 
~~ names = contributors_map . keys ( ) \n 
names = sorted ( names , cmp = compare ) \n 
for name in names : \n 
~~~ tickets = contributors_map [ name ] \n 
tickets_string = [ ] \n 
for ticket in tickets : \n 
~~~ if not in ticket : \n 
~~ number = ticket . split ( ) [ 1 ] \n 
if ticket . startswith ( ) : \n 
~~~ url = JIRA_URL % ( number ) \n 
~~ elif ticket . startswith ( ) or ticket . startswith ( ) : \n 
~~~ url = GITHUB_URL % ( number ) \n 
~~ values = { : ticket , : url } \n 
tickets_string . append ( % values ) \n 
~~ tickets_string = . join ( tickets_string ) \n 
if include_tickets : \n 
~~~ line = % { : name , \n 
: tickets_string } \n 
~~~ line = % { : name } \n 
~~ result . append ( line . strip ( ) ) \n 
~~ result = . join ( result ) \n 
~~~ parser = argparse . ArgumentParser ( description = \n 
parser . add_argument ( , action = , required = True , \n 
parser . add_argument ( , action = , nargs = , \n 
type = str , \n 
help = \n 
parser . add_argument ( , action = , \n 
default = False , \n 
contributors_map = parse_changes_file ( file_path = args . changes_path , \n 
versions = args . versions ) \n 
markdown = convert_to_markdown ( contributors_map = contributors_map , \n 
include_tickets = args . include_tickets ) \n 
print ( markdown ) \n 
~~ from libcloud . compute . types import Provider \n 
from libcloud . compute . providers import get_driver \n 
cls = get_driver ( Provider . CLOUDSIGMA ) \n 
driver = cls ( , , region = , api_version = ) \n 
size = driver . list_sizes ( ) [ 0 ] \n 
image = driver . list_images ( ) [ 0 ] \n 
subscription = driver . ex_create_subscription ( amount = 1 , period = , \n 
resource = , auto_renew = True ) \n 
vlan_uuid = subscription . subscribed_object \n 
node = driver . create_node ( name = name , size = size , image = image , \n 
ex_vlan = vlan_uuid ) \n 
print ( node ) \n 
from libcloud . compute . types import Provider \n 
RACKSPACE_USER = \n 
RACKSPACE_KEY = \n 
Driver = get_driver ( Provider . RACKSPACE ) \n 
conn = Driver ( RACKSPACE_USER , RACKSPACE_KEY ) \n 
images = conn . list_images ( ) \n 
sizes = conn . list_sizes ( ) \n 
node = conn . create_node ( name = , image = images [ 0 ] , size = sizes [ 0 ] ) \n 
EC2_ACCESS_ID = \n 
EC2_SECRET_KEY = \n 
Driver = get_driver ( Provider . EC2 ) \n 
conn = Driver ( EC2_ACCESS_ID , EC2_SECRET_KEY ) \n 
nodes = conn . list_nodes ( ) \n 
cls = get_driver ( Provider . VULTR ) \n 
driver = cls ( ) \n 
from libcloud . dns . types import Provider \n 
from libcloud . dns . providers import get_driver \n 
cls = get_driver ( Provider . GODADDY ) \n 
driver = cls ( , , ) \n 
zone = driver . get_zone ( ) \n 
records = driver . list_records ( zone ) \n 
for record in records : \n 
ACCESS_ID = \n 
SECRET_KEY = \n 
cls = get_driver ( Provider . ELB ) \n 
driver = cls ( key = ACCESS_ID , secret = SECRET_KEY ) \n 
driver . ex_set_balancer_policies_listener ( \n 
port = 80 , \n 
policies = [ ] ) \n 
from libcloud . common . base import ConnectionUserAndKey , BaseDriver \n 
from libcloud . backup . types import BackupTargetType \n 
class BackupTarget ( object ) : \n 
def __init__ ( self , id , name , address , type , driver , extra = None ) : \n 
self . id = str ( id ) if id else None \n 
self . address = address \n 
self . type = type \n 
self . driver = driver \n 
self . extra = extra or { } \n 
~~ def update ( self , name = None , address = None , extra = None ) : \n 
~~~ return self . driver . update_target ( target = self , \n 
address = address , \n 
extra = extra ) \n 
~~ def delete ( self ) : \n 
~~~ return self . driver . delete_target ( target = self ) \n 
~~ def _get_numeric_id ( self ) : \n 
~~~ target_id = self . id \n 
if target_id . isdigit ( ) : \n 
~~~ target_id = int ( target_id ) \n 
~~ return target_id \n 
~~~ return ( \n 
% \n 
( self . id , self . name , self . address , \n 
self . type , self . driver . name ) ) \n 
~~ ~~ class BackupTargetJob ( object ) : \n 
def __init__ ( self , id , status , progress , target , driver , extra = None ) : \n 
self . status = status \n 
self . progress = progress \n 
self . target = target \n 
~~ def cancel ( self ) : \n 
~~~ return self . driver . cancel_target_job ( job = self ) \n 
~~ def suspend ( self ) : \n 
~~~ return self . driver . suspend_target_job ( job = self ) \n 
~~ def resume ( self ) : \n 
~~~ return self . driver . resume_target_job ( job = self ) \n 
( self . id , self . status , self . progress , \n 
self . target . id , self . driver . name ) ) \n 
~~ ~~ class BackupTargetRecoveryPoint ( object ) : \n 
def __init__ ( self , id , date , target , driver , extra = None ) : \n 
self . date = date \n 
~~ def recover ( self , path = None ) : \n 
return self . driver . recover_target ( target = self . target , \n 
recovery_point = self , path = path ) \n 
~~ def recover_to ( self , recovery_target , path = None ) : \n 
return self . driver . recover_target_out_of_place ( \n 
target = self . target , \n 
recovery_point = self , \n 
recovery_target = recovery_target , \n 
path = path ) \n 
( self . id , self . date , \n 
~~ ~~ class BackupDriver ( BaseDriver ) : \n 
connectionCls = ConnectionUserAndKey \n 
name = None \n 
website = None \n 
def __init__ ( self , key , secret = None , secure = True , host = None , port = None , \n 
super ( BackupDriver , self ) . __init__ ( key = key , secret = secret , \n 
secure = secure , host = host , port = port , \n 
** kwargs ) \n 
~~ def get_supported_target_types ( self ) : \n 
raise NotImplementedError ( \n 
~~ def list_targets ( self ) : \n 
~~ def create_target ( self , name , address , \n 
type = BackupTargetType . VIRTUAL , extra = None ) : \n 
~~ def create_target_from_node ( self , node , type = BackupTargetType . VIRTUAL , \n 
extra = None ) : \n 
return self . create_target ( name = node . name , \n 
address = node . public_ips [ 0 ] , \n 
type = type , \n 
extra = None ) \n 
~~ def create_target_from_storage_container ( self , container , \n 
type = BackupTargetType . OBJECT , \n 
return self . create_target ( name = container . name , \n 
address = container . get_cdn_url ( ) , \n 
~~ def update_target ( self , target , name , address , extra ) : \n 
~~ def delete_target ( self , target ) : \n 
~~ def list_recovery_points ( self , target , start_date = None , end_date = None ) : \n 
~~ def recover_target ( self , target , recovery_point , path = None ) : \n 
~~ def recover_target_out_of_place ( self , target , recovery_point , \n 
recovery_target , path = None ) : \n 
~~ def get_target_job ( self , target , id ) : \n 
jobs = self . list_target_jobs ( target ) \n 
return list ( filter ( lambda x : x . id == id , jobs ) ) [ 0 ] \n 
~~ def list_target_jobs ( self , target ) : \n 
~~ def create_target_job ( self , target , extra = None ) : \n 
~~ def resume_target_job ( self , job ) : \n 
~~ def suspend_target_job ( self , job ) : \n 
~~ def cancel_target_job ( self , job ) : \n 
~~ ~~ import hashlib \n 
from libcloud . common . base import ConnectionUserAndKey \n 
from libcloud . common . base import JsonResponse \n 
from libcloud . common . types import InvalidCredsError , ProviderError \n 
from libcloud . utils . py3 import basestring , httplib , urlencode \n 
SALT_CHARACTERS = string . ascii_letters + string . digits \n 
class NFSNException ( ProviderError ) : \n 
~~~ def __init__ ( self , value , http_code , code , driver = None ) : \n 
~~~ self . code = code \n 
super ( NFSNException , self ) . __init__ ( value , http_code , driver ) \n 
~~ ~~ class NFSNResponse ( JsonResponse ) : \n 
~~~ def parse_error ( self ) : \n 
~~~ if self . status == httplib . UNAUTHORIZED : \n 
~~~ raise InvalidCredsError ( ) \n 
~~ body = self . parse_body ( ) \n 
if isinstance ( body , basestring ) : \n 
~~~ return body + % self . status \n 
~~ error = body . get ( , None ) \n 
debug = body . get ( , None ) \n 
value = \n 
if error is not None : \n 
~~~ value = error \n 
~~ if debug is not None : \n 
~~~ value = debug \n 
~~ if error is not None and value is not None : \n 
~~~ value = error + + value \n 
~~ value = value + % self . status \n 
~~ ~~ class NFSNConnection ( ConnectionUserAndKey ) : \n 
~~~ host = \n 
responseCls = NFSNResponse \n 
allow_insecure = False \n 
def _header ( self , action , data ) : \n 
login = self . user_id \n 
timestamp = self . _timestamp ( ) \n 
salt = self . _salt ( ) \n 
api_key = self . key \n 
data = urlencode ( data ) \n 
data_hash = hashlib . sha1 ( data . encode ( ) ) . hexdigest ( ) \n 
string = . join ( ( login , timestamp , salt , api_key , action , data_hash ) ) \n 
string_hash = hashlib . sha1 ( string . encode ( ) ) . hexdigest ( ) \n 
return . join ( ( login , timestamp , salt , string_hash ) ) \n 
~~ def request ( self , action , params = None , data = , headers = None , \n 
method = ) : \n 
if not headers : \n 
~~~ headers = { } \n 
~~ if not params : \n 
~~~ params = { } \n 
~~ header = self . _header ( action , data ) \n 
headers [ ] = header \n 
if method == : \n 
~~~ headers [ ] = \n 
~~ return ConnectionUserAndKey . request ( self , action , params , data , \n 
headers , method ) \n 
~~ def encode_data ( self , data ) : \n 
if data : \n 
~~~ data = urlencode ( data ) \n 
~~ def _salt ( self ) : \n 
r = random . SystemRandom ( ) \n 
return . join ( r . choice ( SALT_CHARACTERS ) for _ in range ( 16 ) ) \n 
~~ def _timestamp ( self ) : \n 
return str ( int ( time . time ( ) ) ) \n 
~~ ~~ from __future__ import with_statement \n 
import base64 \n 
from libcloud . utils . py3 import b \n 
from libcloud . utils . py3 import urlparse \n 
from libcloud . compute . providers import Provider \n 
from libcloud . common . cloudstack import CloudStackDriverMixIn \n 
from libcloud . compute . base import Node , NodeDriver , NodeImage , NodeLocation \n 
from libcloud . compute . base import NodeSize , StorageVolume , VolumeSnapshot \n 
from libcloud . compute . base import KeyPair \n 
from libcloud . compute . types import NodeState , LibcloudError \n 
from libcloud . compute . types import KeyPairDoesNotExistError , StorageVolumeState \n 
from libcloud . utils . networking import is_private_subnet \n 
def transform_int_or_unlimited ( value ) : \n 
if str ( value ) . lower ( ) == : \n 
~~ raise e \n 
RESOURCE_EXTRA_ATTRIBUTES_MAP = { \n 
: { \n 
: str \n 
} , \n 
: list \n 
: transform_int_or_unlimited \n 
: { : , : str } , \n 
: { : , \n 
: transform_int_or_unlimited } , \n 
: transform_int_or_unlimited } \n 
: bool \n 
class CloudStackNode ( Node ) : \n 
def ex_allocate_public_ip ( self ) : \n 
return self . driver . ex_allocate_public_ip ( self ) \n 
~~ def ex_release_public_ip ( self , address ) : \n 
return self . driver . ex_release_public_ip ( self , address ) \n 
~~ def ex_create_ip_forwarding_rule ( self , address , protocol , \n 
start_port , end_port = None ) : \n 
return self . driver . ex_create_ip_forwarding_rule ( node = self , \n 
protocol = protocol , \n 
start_port = start_port , \n 
end_port = end_port ) \n 
~~ def ex_create_port_forwarding_rule ( self , address , \n 
private_port , public_port , \n 
protocol , \n 
public_end_port = None , \n 
private_end_port = None , \n 
openfirewall = True ) : \n 
return self . driver . ex_create_port_forwarding_rule ( \n 
node = self , address = address , private_port = private_port , \n 
public_port = public_port , protocol = protocol , \n 
public_end_port = public_end_port , private_end_port = private_end_port , \n 
openfirewall = openfirewall ) \n 
~~ def ex_delete_ip_forwarding_rule ( self , rule ) : \n 
return self . driver . ex_delete_ip_forwarding_rule ( node = self , rule = rule ) \n 
~~ def ex_delete_port_forwarding_rule ( self , rule ) : \n 
return self . driver . ex_delete_port_forwarding_rule ( node = self , rule = rule ) \n 
~~ def ex_start ( self ) : \n 
return self . driver . ex_start ( node = self ) \n 
~~ def ex_stop ( self ) : \n 
return self . driver . ex_stop ( node = self ) \n 
~~ ~~ class CloudStackAddress ( object ) : \n 
def __init__ ( self , id , address , driver , associated_network_id = None , \n 
vpc_id = None , virtualmachine_id = None ) : \n 
~~~ self . id = id \n 
self . associated_network_id = associated_network_id \n 
self . vpc_id = vpc_id \n 
self . virtualmachine_id = virtualmachine_id \n 
~~ def release ( self ) : \n 
~~~ self . driver . ex_release_public_ip ( address = self ) \n 
~~~ return self . address \n 
~~~ return self . __class__ is other . __class__ and self . id == other . id \n 
~~ ~~ class CloudStackFirewallRule ( object ) : \n 
def __init__ ( self , id , address , cidr_list , protocol , \n 
icmp_code = None , icmp_type = None , \n 
start_port = None , end_port = None ) : \n 
self . id = id \n 
self . cidr_list = cidr_list \n 
self . protocol = protocol \n 
self . icmp_code = icmp_code \n 
self . icmp_type = icmp_type \n 
self . start_port = start_port \n 
self . end_port = end_port \n 
~~ ~~ class CloudStackEgressFirewallRule ( object ) : \n 
def __init__ ( self , id , network_id , cidr_list , protocol , \n 
self . network_id = network_id \n 
~~ ~~ class CloudStackIPForwardingRule ( object ) : \n 
def __init__ ( self , node , id , address , protocol , start_port , end_port = None ) : \n 
self . node = node \n 
~~~ self . node . ex_delete_ip_forwarding_rule ( rule = self ) \n 
~~ ~~ class CloudStackPortForwardingRule ( object ) : \n 
def __init__ ( self , node , rule_id , address , protocol , public_port , \n 
private_port , public_end_port = None , private_end_port = None , \n 
network_id = None ) : \n 
self . id = rule_id \n 
self . public_port = public_port \n 
self . public_end_port = public_end_port \n 
self . private_port = private_port \n 
self . private_end_port = private_end_port \n 
~~~ self . node . ex_delete_port_forwarding_rule ( rule = self ) \n 
~~ ~~ class CloudStackNetworkACLList ( object ) : \n 
def __init__ ( self , acl_id , name , vpc_id , driver , description = None ) : \n 
self . id = acl_id \n 
self . description = description \n 
~~~ return ( ( \n 
% ( self . id , self . name , self . vpc_id , \n 
self . driver . name , self . description ) ) \n 
~~ ~~ class CloudStackNetworkACL ( object ) : \n 
def __init__ ( self , id , protocol , acl_id , action , cidr_list , \n 
start_port , end_port , traffic_type = None ) : \n 
self . acl_id = acl_id \n 
self . action = action \n 
self . traffic_type = traffic_type \n 
~~ ~~ class CloudStackDiskOffering ( object ) : \n 
def __init__ ( self , id , name , size , customizable ) : \n 
self . size = size \n 
self . customizable = customizable \n 
~~ ~~ class CloudStackNetwork ( object ) : \n 
def __init__ ( self , displaytext , name , networkofferingid , id , zoneid , \n 
driver , extra = None ) : \n 
~~~ self . displaytext = displaytext \n 
self . networkofferingid = networkofferingid \n 
self . zoneid = zoneid \n 
% ( self . displaytext , self . name , self . networkofferingid , \n 
self . id , self . zoneid , self . driver . name ) ) \n 
~~ ~~ class CloudStackNetworkOffering ( object ) : \n 
def __init__ ( self , name , display_text , guest_ip_type , id , \n 
service_offering_id , for_vpc , driver , extra = None ) : \n 
~~~ self . display_text = display_text \n 
self . guest_ip_type = guest_ip_type \n 
self . service_offering_id = service_offering_id \n 
self . for_vpc = for_vpc \n 
% ( self . id , self . name , self . display_text , \n 
self . guest_ip_type , self . service_offering_id , self . for_vpc , \n 
self . driver . name ) ) \n 
~~ ~~ class CloudStackNic ( object ) : \n 
def __init__ ( self , id , network_id , net_mask , gateway , ip_address , \n 
is_default , mac_address , driver , extra = None ) : \n 
self . net_mask = net_mask \n 
self . gateway = gateway \n 
self . ip_address = ip_address \n 
self . is_default = is_default \n 
self . mac_address = mac_address \n 
% ( self . id , self . network_id , self . net_mask , \n 
self . gateway , self . ip_address , self . is_default , \n 
self . mac_address , self . driver . name ) ) \n 
~~ ~~ class CloudStackVPC ( object ) : \n 
def __init__ ( self , name , vpc_offering_id , id , cidr , driver , \n 
zone_id = None , display_text = None , extra = None ) : \n 
self . vpc_offering_id = vpc_offering_id \n 
self . zone_id = zone_id \n 
self . cidr = cidr \n 
% ( self . name , self . vpc_offering_id , self . id , \n 
self . cidr , self . driver . name , self . zone_id , \n 
self . display_text ) ) \n 
~~ ~~ class CloudStackVPCOffering ( object ) : \n 
def __init__ ( self , name , display_text , id , \n 
~~~ self . name = name \n 
self . display_text = display_text \n 
% ( self . name , self . display_text , self . id , \n 
~~ ~~ class CloudStackVpnGateway ( object ) : \n 
def __init__ ( self , id , account , domain , domain_id , \n 
public_ip , vpc_id , driver , extra = None ) : \n 
self . account = account \n 
self . domain = domain \n 
self . domain_id = domain_id \n 
self . public_ip = public_ip \n 
def vpc ( self ) : \n 
~~~ for vpc in self . driver . ex_list_vpcs ( ) : \n 
~~~ if self . vpc_id == vpc . id : \n 
~~~ return vpc \n 
~~ ~~ raise LibcloudError ( % self . vpc_id ) \n 
~~~ return self . driver . ex_delete_vpn_gateway ( vpn_gateway = self ) \n 
% ( self . account , self . domain , self . domain_id , \n 
self . id , self . public_ip , self . vpc_id , self . driver . name ) ) \n 
~~ ~~ class CloudStackVpnCustomerGateway ( object ) : \n 
def __init__ ( self , id , cidr_list , esp_policy , gateway , \n 
ike_policy , ipsec_psk , driver , extra = None ) : \n 
self . esp_policy = esp_policy \n 
self . ike_policy = ike_policy \n 
self . ipsec_psk = ipsec_psk \n 
~~~ return self . driver . ex_delete_vpn_customer_gateway ( \n 
vpn_customer_gateway = self ) \n 
% ( self . id , self . cidr_list , self . esp_policy , self . gateway , \n 
self . ike_policy , self . ipsec_psk , self . driver . name ) ) \n 
~~ ~~ class CloudStackVpnConnection ( object ) : \n 
def __init__ ( self , id , passive , vpn_customer_gateway_id , \n 
vpn_gateway_id , state , driver , extra = None ) : \n 
self . passive = passive \n 
self . vpn_customer_gateway_id = vpn_customer_gateway_id \n 
self . vpn_gateway_id = vpn_gateway_id \n 
self . state = state \n 
def vpn_customer_gateway ( self ) : \n 
~~~ return self . driver . ex_list_vpn_customer_gateways ( \n 
id = self . vpn_customer_gateway_id ) [ 0 ] \n 
~~~ raise LibcloudError ( % \n 
self . vpn_customer_gateway_id ) \n 
def vpn_gateway ( self ) : \n 
~~~ return self . driver . ex_list_vpn_gateways ( id = self . vpn_gateway_id ) [ 0 ] \n 
self . vpn_gateway_id ) \n 
~~ ~~ def delete ( self ) : \n 
~~~ return self . driver . ex_delete_vpn_connection ( vpn_connection = self ) \n 
% ( self . id , self . passive , self . vpn_customer_gateway_id , \n 
self . vpn_gateway_id , self . state , self . driver . name ) ) \n 
~~ ~~ class CloudStackRouter ( object ) : \n 
def __init__ ( self , id , name , state , public_ip , vpc_id , driver ) : \n 
% ( self . id , self . name , self . state , \n 
self . public_ip , self . vpc_id , self . driver . name ) ) \n 
~~ ~~ class CloudStackProject ( object ) : \n 
def __init__ ( self , id , name , display_text , driver , extra = None ) : \n 
% ( self . id , self . display_text , self . name , \n 
~~ ~~ class CloudStackAffinityGroup ( object ) : \n 
def __init__ ( self , id , account , description , domain , domainid , name , \n 
group_type , virtualmachine_ids ) : \n 
self . domainid = domainid \n 
self . type = group_type \n 
self . virtualmachine_ids = virtualmachine_ids \n 
~~~ return ( ( ) \n 
% ( self . id , self . name , self . type ) ) \n 
~~ ~~ class CloudStackAffinityGroupType ( object ) : \n 
def __init__ ( self , type_name ) : \n 
self . type = type_name \n 
~~~ return ( ( ) % self . type ) \n 
~~ ~~ class CloudStackNodeDriver ( CloudStackDriverMixIn , NodeDriver ) : \n 
api_name = \n 
website = \n 
type = Provider . CLOUDSTACK \n 
features = { : [ ] } \n 
NODE_STATE_MAP = { \n 
: NodeState . RUNNING , \n 
: NodeState . REBOOTING , \n 
: NodeState . STOPPED , \n 
: NodeState . PENDING , \n 
: NodeState . TERMINATED , \n 
: NodeState . TERMINATED \n 
VOLUME_STATE_MAP = { \n 
: StorageVolumeState . CREATING , \n 
: StorageVolumeState . DELETING , \n 
: StorageVolumeState . DELETED , \n 
: StorageVolumeState . AVAILABLE , \n 
: StorageVolumeState . BACKUP , \n 
: StorageVolumeState . ERROR \n 
def __init__ ( self , key , secret = None , secure = True , host = None , \n 
path = None , port = None , url = None , * args , ** kwargs ) : \n 
if url : \n 
~~~ parsed = urlparse . urlparse ( url ) \n 
path = parsed . path \n 
scheme = parsed . scheme \n 
split = parsed . netloc . split ( ) \n 
if len ( split ) == 1 : \n 
~~~ host = parsed . netloc \n 
port = 443 if scheme == else 80 \n 
~~~ host = split [ 0 ] \n 
port = int ( split [ 1 ] ) \n 
~~~ host = host if host else self . host \n 
path = path if path else self . path \n 
~~~ self . path = path \n 
~~ if host is not None : \n 
~~~ self . host = host \n 
~~ if ( self . type == Provider . CLOUDSTACK ) and ( not host or not path ) : \n 
~~ super ( CloudStackNodeDriver , self ) . __init__ ( key = key , \n 
secret = secret , \n 
secure = secure , \n 
host = host , \n 
port = port ) \n 
~~ def list_images ( self , location = None ) : \n 
~~~ args = { \n 
if location is not None : \n 
~~~ args [ ] = location . id \n 
~~ imgs = self . _sync_request ( command = , \n 
params = args , \n 
method = ) \n 
images = [ ] \n 
for img in imgs . get ( , [ ] ) : \n 
~~~ extra = { : img [ ] , \n 
: img [ ] , \n 
: img [ ] } \n 
size = img . get ( , None ) \n 
~~~ extra . update ( { : img [ ] } ) \n 
~~ images . append ( NodeImage ( \n 
id = img [ ] , \n 
name = img [ ] , \n 
driver = self . connection . driver , \n 
extra = extra ) ) \n 
~~ return images \n 
~~ def list_locations ( self ) : \n 
locs = self . _sync_request ( ) \n 
locations = [ ] \n 
for loc in locs [ ] : \n 
~~~ location = NodeLocation ( str ( loc [ ] ) , loc [ ] , , \n 
self ) \n 
locations . append ( location ) \n 
~~ return locations \n 
~~ def list_nodes ( self , project = None , location = None ) : \n 
args = { } \n 
if project : \n 
~~~ args [ ] = project . id \n 
~~ if location is not None : \n 
~~ vms = self . _sync_request ( , params = args ) \n 
addrs = self . _sync_request ( , params = args ) \n 
port_forwarding_rules = self . _sync_request ( ) \n 
ip_forwarding_rules = self . _sync_request ( ) \n 
public_ips_map = { } \n 
for addr in addrs . get ( , [ ] ) : \n 
~~~ if not in addr : \n 
~~ vm_id = str ( addr [ ] ) \n 
if vm_id not in public_ips_map : \n 
~~~ public_ips_map [ vm_id ] = { } \n 
~~ public_ips_map [ vm_id ] [ addr [ ] ] = addr [ ] \n 
~~ nodes = [ ] \n 
for vm in vms . get ( , [ ] ) : \n 
~~~ public_ips = public_ips_map . get ( str ( vm [ ] ) , { } ) . keys ( ) \n 
public_ips = list ( public_ips ) \n 
node = self . _to_node ( data = vm , public_ips = public_ips ) \n 
addresses = public_ips_map . get ( str ( vm [ ] ) , { } ) . items ( ) \n 
addresses = [ CloudStackAddress ( id = address_id , address = address , \n 
driver = node . driver ) for \n 
address , address_id in addresses ] \n 
node . extra [ ] = addresses \n 
rules = [ ] \n 
for addr in addresses : \n 
~~~ for r in ip_forwarding_rules . get ( , [ ] ) : \n 
~~~ if str ( r [ ] ) == node . id : \n 
~~~ rule = CloudStackIPForwardingRule ( node , r [ ] , \n 
addr , \n 
r [ ] \n 
. upper ( ) , \n 
r [ ] , \n 
r [ ] ) \n 
rules . append ( rule ) \n 
~~ ~~ ~~ node . extra [ ] = rules \n 
for r in port_forwarding_rules . get ( , [ ] ) : \n 
~~~ addr = [ CloudStackAddress ( id = a [ ] , \n 
address = a [ ] , \n 
driver = node . driver ) \n 
for a in addrs . get ( , [ ] ) \n 
if a [ ] == r [ ] ] \n 
rule = CloudStackPortForwardingRule ( node , r [ ] , \n 
addr [ 0 ] , \n 
r [ ] . upper ( ) , \n 
if not addr [ 0 ] . address in node . public_ips : \n 
~~~ node . public_ips . append ( addr [ 0 ] . address ) \n 
~~ rules . append ( rule ) \n 
~~ ~~ node . extra [ ] = rules \n 
nodes . append ( node ) \n 
~~ return nodes \n 
~~ def ex_get_node ( self , node_id , project = None ) : \n 
list_nodes_args = { : node_id } \n 
list_ips_args = { } \n 
~~~ list_nodes_args [ ] = project . id \n 
list_ips_args [ ] = project . id \n 
~~ vms = self . _sync_request ( , params = list_nodes_args ) \n 
if not vms : \n 
~~ vm = vms [ ] [ 0 ] \n 
addrs = self . _sync_request ( , \n 
params = list_ips_args ) \n 
public_ips = { } \n 
~~ public_ips [ addr [ ] ] = addr [ ] \n 
~~ node = self . _to_node ( data = vm , public_ips = list ( public_ips . keys ( ) ) ) \n 
address , address_id in public_ips . items ( ) ] \n 
list_fw_rules = { : node_id } \n 
~~~ result = self . _sync_request ( , \n 
params = list_fw_rules ) \n 
for r in result . get ( , [ ] ) : \n 
public_ips = self . ex_list_public_ips ( ) \n 
result = self . _sync_request ( , \n 
~~~ addr = [ a for a in public_ips if \n 
a . address == r [ ] ] \n 
~~ def list_sizes ( self , location = None ) : \n 
szs = self . _sync_request ( command = , \n 
sizes = [ ] \n 
for sz in szs [ ] : \n 
~~~ extra = { : sz [ ] } \n 
sizes . append ( NodeSize ( sz [ ] , sz [ ] , sz [ ] , 0 , 0 , \n 
0 , self , extra = extra ) ) \n 
~~ return sizes \n 
~~ def create_node ( self , ** kwargs ) : \n 
server_params = self . _create_args_to_params ( None , ** kwargs ) \n 
data = self . _async_request ( command = , \n 
params = server_params , \n 
method = ) [ ] \n 
node = self . _to_node ( data = data ) \n 
~~ def _create_args_to_params ( self , node , ** kwargs ) : \n 
~~~ server_params = { } \n 
name = kwargs . get ( , None ) \n 
size = kwargs . get ( , None ) \n 
image = kwargs . get ( , None ) \n 
location = kwargs . get ( , None ) \n 
networks = kwargs . get ( , None ) \n 
project = kwargs . get ( , None ) \n 
diskoffering = kwargs . get ( , None ) \n 
ex_key_name = kwargs . get ( , None ) \n 
ex_user_data = kwargs . get ( , None ) \n 
ex_security_groups = kwargs . get ( , None ) \n 
ex_displayname = kwargs . get ( , None ) \n 
ex_ip_address = kwargs . get ( , None ) \n 
ex_start_vm = kwargs . get ( , None ) \n 
ex_rootdisksize = kwargs . get ( , None ) \n 
ex_affinity_groups = kwargs . get ( , None ) \n 
~~~ server_params [ ] = name \n 
~~ if ex_displayname : \n 
~~~ server_params [ ] = ex_displayname \n 
~~ if size : \n 
~~~ server_params [ ] = size . id \n 
~~ if image : \n 
~~~ server_params [ ] = image . id \n 
~~ if location : \n 
~~~ server_params [ ] = location . id \n 
~~~ server_params [ ] = self . list_locations ( ) [ 0 ] . id \n 
~~ if networks : \n 
~~~ networks = . join ( [ str ( network . id ) for network in networks ] ) \n 
server_params [ ] = networks \n 
~~ if project : \n 
~~~ server_params [ ] = project . id \n 
~~ if diskoffering : \n 
~~~ server_params [ ] = diskoffering . id \n 
~~ if ex_key_name : \n 
~~~ server_params [ ] = ex_key_name \n 
~~ if ex_user_data : \n 
~~~ ex_user_data = base64 . b64encode ( b ( ex_user_data ) . decode ( ) ) \n 
server_params [ ] = ex_user_data \n 
~~ if ex_security_groups : \n 
~~~ ex_security_groups = . join ( ex_security_groups ) \n 
server_params [ ] = ex_security_groups \n 
~~ if ex_ip_address : \n 
~~~ server_params [ ] = ex_ip_address \n 
~~ if ex_rootdisksize : \n 
~~~ server_params [ ] = ex_rootdisksize \n 
~~ if ex_start_vm is not None : \n 
~~~ server_params [ ] = ex_start_vm \n 
~~ if ex_affinity_groups : \n 
~~~ affinity_group_ids = . join ( ag . id for ag in ex_affinity_groups ) \n 
server_params [ ] = affinity_group_ids \n 
~~ return server_params \n 
~~ def destroy_node ( self , node , ex_expunge = False ) : \n 
args = { \n 
: node . id , \n 
if ex_expunge : \n 
~~~ args [ ] = ex_expunge \n 
~~ self . _async_request ( command = , \n 
return True \n 
~~ def reboot_node ( self , node ) : \n 
self . _async_request ( command = , \n 
params = { : node . id } , \n 
~~ def ex_start ( self , node ) : \n 
res = self . _async_request ( command = , \n 
return res [ ] [ ] \n 
~~ def ex_stop ( self , node ) : \n 
~~ def ex_list_disk_offerings ( self ) : \n 
diskOfferings = [ ] \n 
diskOfferResponse = self . _sync_request ( command = , \n 
for diskOfferDict in diskOfferResponse . get ( , ( ) ) : \n 
~~~ diskOfferings . append ( \n 
CloudStackDiskOffering ( \n 
id = diskOfferDict [ ] , \n 
name = diskOfferDict [ ] , \n 
size = diskOfferDict [ ] , \n 
customizable = diskOfferDict [ ] ) ) \n 
~~ return diskOfferings \n 
~~ def ex_list_networks ( self , project = None ) : \n 
if project is not None : \n 
~~ res = self . _sync_request ( command = , \n 
nets = res . get ( , [ ] ) \n 
networks = [ ] \n 
extra_map = RESOURCE_EXTRA_ATTRIBUTES_MAP [ ] \n 
for net in nets : \n 
~~~ extra = self . _get_extra_dict ( net , extra_map ) \n 
if in net : \n 
~~~ extra [ ] = self . _get_resource_tags ( net [ ] ) \n 
~~ networks . append ( CloudStackNetwork ( \n 
net [ ] , \n 
~~ return networks \n 
~~ def ex_list_network_offerings ( self ) : \n 
res = self . _sync_request ( command = , \n 
netoffers = res . get ( , [ ] ) \n 
networkofferings = [ ] \n 
for netoffer in netoffers : \n 
~~~ networkofferings . append ( CloudStackNetworkOffering ( \n 
netoffer [ ] , \n 
self ) ) \n 
~~ return networkofferings \n 
~~ def ex_create_network ( self , display_text , name , network_offering , \n 
location , gateway = None , netmask = None , \n 
network_domain = None , vpc_id = None , project_id = None ) : \n 
: display_text , \n 
: network_offering . id , \n 
: location . id , \n 
if gateway is not None : \n 
~~~ args [ ] = gateway \n 
~~ if netmask is not None : \n 
~~~ args [ ] = netmask \n 
~~ if network_domain is not None : \n 
~~~ args [ ] = network_domain \n 
~~ if vpc_id is not None : \n 
~~~ args [ ] = vpc_id \n 
~~ if project_id is not None : \n 
~~~ args [ ] = project_id \n 
result = self . _sync_request ( command = , \n 
result = result [ ] \n 
extra = self . _get_extra_dict ( result , extra_map ) \n 
network = CloudStackNetwork ( display_text , \n 
name , \n 
network_offering . id , \n 
result [ ] , \n 
location . id , \n 
return network \n 
~~ def ex_delete_network ( self , network , force = None ) : \n 
args = { : network . id , : force } \n 
~~ def ex_list_vpc_offerings ( self ) : \n 
vpcoffers = res . get ( , [ ] ) \n 
vpcofferings = [ ] \n 
for vpcoffer in vpcoffers : \n 
~~~ vpcofferings . append ( CloudStackVPCOffering ( \n 
vpcoffer [ ] , \n 
~~ return vpcofferings \n 
~~ def ex_list_vpcs ( self , project = None ) : \n 
vpcs = res . get ( , [ ] ) \n 
for vpc in vpcs : \n 
~~~ networks . append ( CloudStackVPC ( \n 
vpc [ ] , \n 
vpc [ ] ) ) \n 
~~ def ex_list_routers ( self , vpc_id = None ) : \n 
if vpc_id is not None : \n 
rts = res . get ( , [ ] ) \n 
routers = [ ] \n 
for router in rts : \n 
~~~ routers . append ( CloudStackRouter ( \n 
router [ ] , \n 
~~ return routers \n 
~~ def ex_create_vpc ( self , cidr , display_text , name , vpc_offering , \n 
zone_id , network_domain = None ) : \n 
: cidr , \n 
: vpc_offering . id , \n 
: zone_id , \n 
if network_domain is not None : \n 
~~ result = self . _sync_request ( command = , \n 
vpc = CloudStackVPC ( name , \n 
vpc_offering . id , \n 
cidr , \n 
zone_id , \n 
display_text , \n 
return vpc \n 
~~ def ex_delete_vpc ( self , vpc ) : \n 
args = { : vpc . id } \n 
~~ def ex_list_projects ( self ) : \n 
projs = res . get ( , [ ] ) \n 
projects = [ ] \n 
for proj in projs : \n 
~~~ extra = self . _get_extra_dict ( proj , extra_map ) \n 
if in proj : \n 
~~~ extra [ ] = self . _get_resource_tags ( proj [ ] ) \n 
~~ projects . append ( CloudStackProject ( \n 
id = proj [ ] , \n 
name = proj [ ] , \n 
display_text = proj [ ] , \n 
driver = self , \n 
~~ return projects \n 
~~ def create_volume ( self , size , name , location = None , snapshot = None ) : \n 
for diskOffering in self . ex_list_disk_offerings ( ) : \n 
~~~ if diskOffering . size == size or diskOffering . customizable : \n 
~~~ raise LibcloudError ( \n 
% size ) \n 
~~ if location is None : \n 
~~~ location = self . list_locations ( ) [ 0 ] \n 
~~ params = { : name , \n 
: diskOffering . id , \n 
: location . id } \n 
if diskOffering . customizable : \n 
~~~ params [ ] = size \n 
~~ requestResult = self . _async_request ( command = , \n 
params = params , \n 
volumeResponse = requestResult [ ] \n 
state = self . _to_volume_state ( volumeResponse ) \n 
return StorageVolume ( id = volumeResponse [ ] , \n 
size = size , \n 
state = state , \n 
extra = dict ( name = volumeResponse [ ] ) ) \n 
~~ def destroy_volume ( self , volume ) : \n 
self . _sync_request ( command = , \n 
params = { : volume . id } , \n 
~~ def attach_volume ( self , node , volume , device = None ) : \n 
params = { : volume . id , \n 
: node . id } , \n 
~~ def detach_volume ( self , volume ) : \n 
~~ def list_volumes ( self , node = None ) : \n 
if node : \n 
~~~ volumes = self . _sync_request ( command = , \n 
~~ list_volumes = [ ] \n 
for vol in volumes . get ( , [ ] ) : \n 
~~~ extra = self . _get_extra_dict ( vol , extra_map ) \n 
if in vol : \n 
~~~ extra [ ] = self . _get_resource_tags ( vol [ ] ) \n 
~~ state = self . _to_volume_state ( vol ) \n 
list_volumes . append ( StorageVolume ( id = vol [ ] , \n 
name = vol [ ] , \n 
size = vol [ ] , \n 
~~ return list_volumes \n 
~~ def ex_get_volume ( self , volume_id , project = None ) : \n 
args = { : volume_id } \n 
~~ volumes = self . _sync_request ( command = , params = args ) \n 
if not volumes : \n 
~~ vol = volumes [ ] [ 0 ] \n 
extra = self . _get_extra_dict ( vol , extra_map ) \n 
volume = StorageVolume ( id = vol [ ] , name = vol [ ] , state = state , \n 
size = vol [ ] , driver = self , extra = extra ) \n 
return volume \n 
~~ def list_key_pairs ( self , ** kwargs ) : \n 
extra_args = kwargs . copy ( ) \n 
params = extra_args , \n 
key_pairs = res . get ( , [ ] ) \n 
key_pairs = self . _to_key_pairs ( data = key_pairs ) \n 
return key_pairs \n 
~~ def get_key_pair ( self , name ) : \n 
params = { : name } \n 
if len ( key_pairs ) == 0 : \n 
~~~ raise KeyPairDoesNotExistError ( name = name , driver = self ) \n 
~~ key_pair = self . _to_key_pair ( data = key_pairs [ 0 ] ) \n 
return key_pair \n 
~~ def create_key_pair ( self , name , ** kwargs ) : \n 
params . update ( extra_args ) \n 
key_pair = self . _to_key_pair ( data = res [ ] ) \n 
~~ def import_key_pair_from_string ( self , name , key_material ) : \n 
params = { : name , \n 
: key_material } , \n 
~~ def delete_key_pair ( self , key_pair , ** kwargs ) : \n 
params = { : key_pair . name } \n 
return res [ ] == \n 
~~ def ex_list_public_ips ( self ) : \n 
ips = [ ] \n 
if not res : \n 
~~~ return ips \n 
~~ for ip in res [ ] : \n 
~~~ ips . append ( CloudStackAddress ( ip [ ] , \n 
ip [ ] , \n 
ip . get ( , [ ] ) , \n 
ip . get ( ) , \n 
ip . get ( ) ) ) \n 
~~ return ips \n 
~~ def ex_allocate_public_ip ( self , vpc_id = None , network_id = None , \n 
location = None ) : \n 
~~~ args [ ] = self . list_locations ( ) [ 0 ] . id \n 
~~ if network_id is not None : \n 
~~~ args [ ] = network_id \n 
~~ addr = self . _async_request ( command = , \n 
addr = addr [ ] \n 
addr = CloudStackAddress ( addr [ ] , addr [ ] , self ) \n 
return addr \n 
params = { : address . id } , \n 
return res [ ] \n 
~~ def ex_list_firewall_rules ( self ) : \n 
if result != { } : \n 
~~~ public_ips = self . ex_list_public_ips ( ) \n 
for rule in result [ ] : \n 
a . address == rule [ ] ] \n 
rules . append ( CloudStackFirewallRule ( rule [ ] , \n 
rule [ ] , \n 
rule . get ( ) , \n 
rule . get ( ) ) ) \n 
~~ ~~ return rules \n 
~~ def ex_create_firewall_rule ( self , address , cidr_list , protocol , \n 
: address . id , \n 
: cidr_list , \n 
: protocol \n 
if icmp_code is not None : \n 
~~~ args [ ] = int ( icmp_code ) \n 
~~ if icmp_type is not None : \n 
~~~ args [ ] = int ( icmp_type ) \n 
~~ if start_port is not None : \n 
~~~ args [ ] = int ( start_port ) \n 
~~ if end_port is not None : \n 
~~~ args [ ] = int ( end_port ) \n 
~~ result = self . _async_request ( command = , \n 
rule = CloudStackFirewallRule ( result [ ] [ ] , \n 
address , \n 
cidr_list , \n 
icmp_code , \n 
icmp_type , \n 
start_port , \n 
end_port ) \n 
return rule \n 
~~ def ex_delete_firewall_rule ( self , firewall_rule ) : \n 
params = { : firewall_rule . id } , \n 
~~ def ex_list_egress_firewall_rules ( self ) : \n 
~~~ rules . append ( CloudStackEgressFirewallRule ( rule [ ] , \n 
~~ return rules \n 
~~ def ex_create_egress_firewall_rule ( self , network_id , cidr_list , protocol , \n 
: network_id , \n 
rule = CloudStackEgressFirewallRule ( result [ ] [ ] , \n 
network_id , \n 
~~ def ex_delete_egress_firewall_rule ( self , firewall_rule ) : \n 
~~ def ex_list_port_forwarding_rules ( self , account = None , domain_id = None , \n 
id = None , ipaddress_id = None , \n 
is_recursive = None , keyword = None , \n 
list_all = None , network_id = None , \n 
page = None , page_size = None , \n 
project_id = None ) : \n 
if account is not None : \n 
~~~ args [ ] = account \n 
~~ if domain_id is not None : \n 
~~~ args [ ] = domain_id \n 
~~ if id is not None : \n 
~~~ args [ ] = id \n 
~~ if ipaddress_id is not None : \n 
~~~ args [ ] = ipaddress_id \n 
~~ if is_recursive is not None : \n 
~~~ args [ ] = is_recursive \n 
~~ if keyword is not None : \n 
~~~ args [ ] = keyword \n 
~~ if list_all is not None : \n 
~~~ args [ ] = list_all \n 
~~ if page is not None : \n 
~~~ args [ ] = page \n 
~~ if page_size is not None : \n 
~~~ args [ ] = page_size \n 
~~ rules = [ ] \n 
nodes = self . list_nodes ( ) \n 
~~~ node = [ n for n in nodes \n 
if n . id == str ( rule [ ] ) ] \n 
addr = [ a for a in public_ips if \n 
rules . append ( CloudStackPortForwardingRule \n 
( node [ 0 ] , \n 
rule [ ] ) ) \n 
~~ def ex_create_port_forwarding_rule ( self , node , address , \n 
openfirewall = True , \n 
: protocol , \n 
: int ( private_port ) , \n 
: int ( public_port ) , \n 
: openfirewall \n 
if public_end_port : \n 
~~~ args [ ] = int ( public_end_port ) \n 
~~ if private_end_port : \n 
~~~ args [ ] = int ( private_end_port ) \n 
~~ if network_id : \n 
rule = CloudStackPortForwardingRule ( node , \n 
result [ ] \n 
public_port , \n 
private_port , \n 
public_end_port , \n 
private_end_port , \n 
network_id ) \n 
node . extra [ ] . append ( rule ) \n 
node . public_ips . append ( address . address ) \n 
~~ def ex_delete_port_forwarding_rule ( self , node , rule ) : \n 
node . extra [ ] . remove ( rule ) \n 
node . public_ips . remove ( rule . address . address ) \n 
params = { : rule . id } , \n 
~~ def ex_list_ip_forwarding_rules ( self , account = None , domain_id = None , \n 
list_all = None , page = None , page_size = None , \n 
project_id = None , virtualmachine_id = None ) : \n 
~~ if virtualmachine_id is not None : \n 
~~~ args [ ] = virtualmachine_id \n 
rules . append ( CloudStackIPForwardingRule \n 
~~ def ex_create_ip_forwarding_rule ( self , node , address , protocol , \n 
protocol = protocol . upper ( ) \n 
if protocol not in ( , ) : \n 
~~ args = { \n 
: int ( start_port ) \n 
if end_port is not None : \n 
rule = CloudStackIPForwardingRule ( node , result [ ] , address , \n 
protocol , start_port , end_port ) \n 
~~ def ex_delete_ip_forwarding_rule ( self , node , rule ) : \n 
~~ def ex_create_network_acllist ( self , name , vpc_id , description = None ) : \n 
: vpc_id \n 
if description : \n 
~~~ args [ ] = description \n 
acl_list = CloudStackNetworkACLList ( result [ ] , \n 
vpc_id , \n 
description ) \n 
return acl_list \n 
~~ def ex_create_network_acl ( self , protocol , acl_id , cidr_list , \n 
start_port , end_port , action = None , \n 
traffic_type = None ) : \n 
: acl_id , \n 
: start_port , \n 
: end_port \n 
if action : \n 
~~~ args [ ] = action \n 
~~~ action = "allow" \n 
~~ if traffic_type : \n 
~~~ args [ ] = traffic_type \n 
acl = CloudStackNetworkACL ( result [ ] [ ] , \n 
acl_id , \n 
action , \n 
end_port , \n 
traffic_type ) \n 
return acl \n 
~~ def ex_list_network_acllists ( self ) : \n 
acllists = [ ] \n 
if not result : \n 
~~~ return acllists \n 
~~ for acllist in result [ ] : \n 
~~~ acllists . append ( CloudStackNetworkACLList ( acllist [ ] , \n 
acllist [ ] , \n 
acllist . get ( , [ ] ) , \n 
acllist [ ] ) ) \n 
~~ return acllists \n 
~~ def ex_replace_network_acllist ( self , acl_id , network_id ) : \n 
: network_id \n 
~~ def ex_list_network_acl ( self ) : \n 
acls = [ ] \n 
~~~ return acls \n 
~~ for acl in result [ ] : \n 
~~~ acls . append ( CloudStackNetworkACL ( acl [ ] , \n 
acl [ ] , \n 
acl . get ( , [ ] ) , \n 
acl [ ] ) ) \n 
~~ return acls \n 
~~ def ex_list_keypairs ( self , ** kwargs ) : \n 
warnings . warn ( \n 
key_pairs = self . list_key_pairs ( ** kwargs ) \n 
for key_pair in key_pairs : \n 
~~~ item = { \n 
: key_pair . name , \n 
: key_pair . fingerprint , \n 
: key_pair . private_key \n 
result . append ( item ) \n 
~~ def ex_create_keypair ( self , name , ** kwargs ) : \n 
key_pair = self . create_key_pair ( name = name , ** kwargs ) \n 
result = { \n 
~~ def ex_import_keypair_from_string ( self , name , key_material ) : \n 
key_pair = self . import_key_pair_from_string ( name = name , \n 
key_material = key_material ) \n 
: key_pair . fingerprint \n 
~~ def ex_import_keypair ( self , name , keyfile ) : \n 
key_pair = self . import_key_pair_from_file ( name = name , \n 
key_file_path = keyfile ) \n 
~~ def ex_delete_keypair ( self , keypair , ** kwargs ) : \n 
key_pair = KeyPair ( name = keypair , public_key = None , fingerprint = None , \n 
driver = self ) \n 
return self . delete_key_pair ( key_pair = key_pair ) \n 
~~ def ex_list_security_groups ( self , ** kwargs ) : \n 
security_groups = res . get ( , [ ] ) \n 
return security_groups \n 
~~ def ex_create_security_group ( self , name , ** kwargs ) : \n 
for sg in self . ex_list_security_groups ( ) : \n 
~~~ if name in sg [ ] : \n 
~~~ raise LibcloudError ( ) \n 
~~ ~~ params = { : name } \n 
return self . _sync_request ( command = , \n 
~~ def ex_delete_security_group ( self , name ) : \n 
params = { : name } , \n 
~~ def ex_authorize_security_group_ingress ( self , securitygroupname , protocol , \n 
cidrlist , startport = None , \n 
endport = None , icmptype = None , \n 
icmpcode = None , ** kwargs ) : \n 
args = kwargs . copy ( ) \n 
args . update ( { \n 
: securitygroupname , \n 
: cidrlist \n 
if protocol not in ( , ) and ( startport is not None or endport is not None ) : \n 
~~ if protocol != and ( icmptype is not None or icmpcode is not None ) : \n 
~~ if protocol in ( , ) : \n 
~~~ if startport is None : \n 
~~ if startport is not None and endport is None : \n 
~~~ endport = startport \n 
~~ args . update ( { \n 
: startport , \n 
: endport \n 
~~ if protocol == : \n 
~~~ if icmptype is None : \n 
~~~ icmptype = - 1 \n 
~~ if icmpcode is None : \n 
~~~ icmpcode = - 1 \n 
: icmptype , \n 
: icmpcode \n 
~~ return self . _async_request ( command = , \n 
~~ def ex_revoke_security_group_ingress ( self , rule_id ) : \n 
params = { : rule_id } , \n 
~~ def ex_create_affinity_group ( self , name , group_type ) : \n 
for ag in self . ex_list_affinity_groups ( ) : \n 
~~~ if name == ag . name : \n 
~~ ~~ params = { : name , : group_type . type } \n 
result = self . _async_request ( command = , \n 
return self . _to_affinity_group ( result [ ] ) \n 
~~ def ex_delete_affinity_group ( self , affinity_group ) : \n 
return self . _async_request ( command = , \n 
params = { : affinity_group . id } , \n 
~~ def ex_update_node_affinity_group ( self , node , affinity_group_list ) : \n 
affinity_groups = . join ( ag . id for ag in affinity_group_list ) \n 
: affinity_groups } , \n 
return self . _to_node ( data = result [ ] ) \n 
~~ def ex_list_affinity_groups ( self ) : \n 
result = self . _sync_request ( command = , method = ) \n 
if not result . get ( ) : \n 
~~ affinity_groups = [ ] \n 
for ag in result [ ] : \n 
~~~ affinity_groups . append ( self . _to_affinity_group ( ag ) ) \n 
~~ return affinity_groups \n 
~~ def ex_list_affinity_group_types ( self ) : \n 
~~ affinity_group_types = [ ] \n 
for agt in result [ ] : \n 
~~~ affinity_group_types . append ( \n 
CloudStackAffinityGroupType ( agt [ ] ) ) \n 
~~ return affinity_group_types \n 
~~ def ex_register_iso ( self , name , url , location = None , ** kwargs ) : \n 
if location is None : \n 
: url , \n 
params [ ] = kwargs . pop ( , False ) \n 
if params [ ] : \n 
~~~ os_type_id = kwargs . pop ( , None ) \n 
if not os_type_id : \n 
~~ params [ ] = os_type_id \n 
~~ return self . _sync_request ( command = , \n 
displaytext = name , \n 
url = url , \n 
zoneid = location . id , \n 
params = params ) \n 
~~ def ex_limits ( self ) : \n 
limits = { } \n 
resource_map = { \n 
0 : , \n 
5 : , \n 
6 : , \n 
7 : , \n 
8 : , \n 
9 : , \n 
10 : , \n 
11 : \n 
for limit in result . get ( , [ ] ) : \n 
~~~ resource = resource_map . get ( int ( limit [ ] ) , None ) \n 
if not resource : \n 
~~ limits [ resource ] = int ( limit [ ] ) \n 
~~ return limits \n 
~~ def ex_create_tags ( self , resource_ids , resource_type , tags ) : \n 
params = { : resource_type , \n 
: . join ( resource_ids ) } \n 
for i , key in enumerate ( tags ) : \n 
~~~ params [ % i ] = key \n 
params [ % i ] = tags [ key ] \n 
~~ def ex_delete_tags ( self , resource_ids , resource_type , tag_keys ) : \n 
for i , key in enumerate ( tag_keys ) : \n 
~~ def list_snapshots ( self ) : \n 
snapshots = self . _sync_request ( , \n 
list_snapshots = [ ] \n 
for snap in snapshots [ ] : \n 
~~~ list_snapshots . append ( self . _to_snapshot ( snap ) ) \n 
~~ return list_snapshots \n 
~~ def create_volume_snapshot ( self , volume , name = None ) : \n 
snapshot = self . _async_request ( command = , \n 
return self . _to_snapshot ( snapshot [ ] ) \n 
~~ def destroy_volume_snapshot ( self , snapshot ) : \n 
~~~ self . _async_request ( command = , \n 
params = { : snapshot . id } , \n 
~~ def ex_create_snapshot_template ( self , snapshot , name , ostypeid , \n 
displaytext = None ) : \n 
if not displaytext : \n 
~~~ displaytext = name \n 
~~ resp = self . _async_request ( command = , \n 
: displaytext , \n 
: ostypeid , \n 
: snapshot . id } ) \n 
img = resp . get ( ) \n 
extra = { \n 
: img [ ] \n 
return NodeImage ( id = img [ ] , \n 
~~ def ex_list_os_types ( self ) : \n 
ostypes = self . _sync_request ( ) \n 
return ostypes [ ] \n 
~~ def ex_list_nics ( self , node ) : \n 
items = res . get ( , [ ] ) \n 
nics = [ ] \n 
for item in items : \n 
~~~ extra = self . _get_extra_dict ( item , extra_map ) \n 
nics . append ( CloudStackNic ( \n 
id = item [ ] , \n 
network_id = item [ ] , \n 
net_mask = item [ ] , \n 
gateway = item [ ] , \n 
ip_address = item [ ] , \n 
is_default = item [ ] , \n 
mac_address = item [ ] , \n 
~~ return nics \n 
~~ def ex_attach_nic_to_node ( self , node , network , ip_address = None ) : \n 
: network . id \n 
if ip_address is not None : \n 
~~~ args [ ] = ip_address \n 
params = args ) \n 
~~ def ex_detach_nic_from_node ( self , nic , node ) : \n 
params = { : nic . id , \n 
: node . id } ) \n 
~~ def ex_list_vpn_gateways ( self , account = None , domain_id = None , \n 
for_display = None , id = None , is_recursive = None , \n 
keyword = None , list_all = None , page = None , \n 
page_size = None , project_id = None , vpc_id = None ) : \n 
~~ if for_display is not None : \n 
~~~ args [ ] = for_display \n 
vpn_gateways = [ ] \n 
vpn_gateways . append ( CloudStackVpnGateway ( \n 
account = item [ ] , \n 
domain = item [ ] , \n 
domain_id = item [ ] , \n 
public_ip = item [ ] , \n 
vpc_id = item [ ] , \n 
~~ return vpn_gateways \n 
~~ def ex_create_vpn_gateway ( self , vpc , for_display = None ) : \n 
: vpc . id , \n 
if for_display is not None : \n 
~~ res = self . _async_request ( command = , \n 
item = res [ ] \n 
return CloudStackVpnGateway ( id = item [ ] , \n 
vpc_id = vpc . id , \n 
extra = self . _get_extra_dict ( item , \n 
extra_map ) ) \n 
~~ def ex_delete_vpn_gateway ( self , vpn_gateway ) : \n 
params = { : vpn_gateway . id } , \n 
~~ def ex_list_vpn_customer_gateways ( self , account = None , domain_id = None , \n 
id = None , is_recursive = None , \n 
keyword = None , list_all = None , \n 
vpn_customer_gateways = [ ] \n 
vpn_customer_gateways . append ( CloudStackVpnCustomerGateway ( \n 
cidr_list = item [ ] , \n 
esp_policy = item [ ] , \n 
ike_policy = item [ ] , \n 
ipsec_psk = item [ ] , \n 
~~ return vpn_customer_gateways \n 
~~ def ex_create_vpn_customer_gateway ( self , cidr_list , esp_policy , gateway , \n 
ike_policy , ipsec_psk , account = None , \n 
domain_id = None , dpd = None , \n 
esp_lifetime = None , ike_lifetime = None , \n 
: esp_policy , \n 
: gateway , \n 
: ike_policy , \n 
: ipsec_psk \n 
~~ if dpd is not None : \n 
~~~ args [ ] = dpd \n 
~~ if esp_lifetime is not None : \n 
~~~ args [ ] = esp_lifetime \n 
~~ if ike_lifetime is not None : \n 
~~~ args [ ] = ike_lifetime \n 
~~ if name is not None : \n 
~~~ args [ ] = name \n 
return CloudStackVpnCustomerGateway ( id = item [ ] , \n 
cidr_list = cidr_list , \n 
esp_policy = esp_policy , \n 
gateway = gateway , \n 
ike_policy = ike_policy , \n 
ipsec_psk = ipsec_psk , \n 
extra = self . _get_extra_dict ( \n 
item , extra_map ) ) \n 
~~ def ex_delete_vpn_customer_gateway ( self , vpn_customer_gateway ) : \n 
params = { : vpn_customer_gateway . id } , \n 
~~ def ex_list_vpn_connections ( self , account = None , domain_id = None , \n 
vpn_connections = [ ] \n 
vpn_connections . append ( CloudStackVpnConnection ( \n 
passive = item [ ] , \n 
vpn_customer_gateway_id = item [ ] , \n 
vpn_gateway_id = item [ ] , \n 
state = item [ ] , \n 
~~ return vpn_connections \n 
~~ def ex_create_vpn_connection ( self , vpn_customer_gateway , vpn_gateway , \n 
for_display = None , passive = None ) : \n 
: vpn_customer_gateway . id , \n 
: vpn_gateway . id , \n 
~~ if passive is not None : \n 
~~~ args [ ] = passive \n 
return CloudStackVpnConnection ( \n 
vpn_customer_gateway_id = vpn_customer_gateway . id , \n 
vpn_gateway_id = vpn_gateway . id , \n 
extra = self . _get_extra_dict ( item , extra_map ) ) \n 
~~ def ex_delete_vpn_connection ( self , vpn_connection ) : \n 
params = { : vpn_connection . id } , \n 
~~ def _to_snapshot ( self , data ) : \n 
: data . get ( , None ) , \n 
return VolumeSnapshot ( data [ ] , driver = self , extra = extra ) \n 
~~ def _to_node ( self , data , public_ips = None ) : \n 
id = data [ ] \n 
if in data : \n 
~~~ name = data [ ] \n 
~~ elif in data : \n 
~~~ name = None \n 
~~ state = self . NODE_STATE_MAP [ data [ ] ] \n 
public_ips = public_ips if public_ips else [ ] \n 
private_ips = [ ] \n 
for nic in data [ ] : \n 
~~~ if is_private_subnet ( nic [ ] ) : \n 
~~~ private_ips . append ( nic [ ] ) \n 
~~~ public_ips . append ( nic [ ] ) \n 
~~ ~~ security_groups = data . get ( , [ ] ) \n 
if security_groups : \n 
~~~ security_groups = [ sg [ ] for sg in security_groups ] \n 
~~ affinity_groups = data . get ( , [ ] ) \n 
if affinity_groups : \n 
~~~ affinity_groups = [ ag [ ] for ag in affinity_groups ] \n 
~~ created = data . get ( , False ) \n 
extra = self . _get_extra_dict ( data , \n 
RESOURCE_EXTRA_ATTRIBUTES_MAP [ ] ) \n 
extra [ ] = security_groups \n 
extra [ ] = affinity_groups \n 
extra [ ] = [ ] \n 
extra [ ] = created \n 
~~~ extra [ ] = self . _get_resource_tags ( data [ ] ) \n 
~~ node = CloudStackNode ( id = id , name = name , state = state , \n 
public_ips = list ( set ( public_ips ) ) , \n 
private_ips = private_ips , \n 
driver = self , extra = extra ) \n 
~~ def _to_key_pairs ( self , data ) : \n 
~~~ key_pairs = [ self . _to_key_pair ( data = item ) for item in data ] \n 
~~ def _to_key_pair ( self , data ) : \n 
~~~ key_pair = KeyPair ( name = data [ ] , \n 
fingerprint = data [ ] , \n 
public_key = data . get ( , None ) , \n 
private_key = data . get ( , None ) , \n 
~~ def _to_affinity_group ( self , data ) : \n 
~~~ affinity_group = CloudStackAffinityGroup ( \n 
id = data [ ] , \n 
name = data [ ] , \n 
group_type = CloudStackAffinityGroupType ( data [ ] ) , \n 
account = data . get ( , ) , \n 
domain = data . get ( , ) , \n 
domainid = data . get ( , ) , \n 
description = data . get ( , ) , \n 
virtualmachine_ids = data . get ( , ) ) \n 
return affinity_group \n 
~~ def _get_resource_tags ( self , tag_set ) : \n 
tags = { } \n 
for tag in tag_set : \n 
~~~ for key , value in tag . iteritems ( ) : \n 
~~~ key = tag [ ] \n 
value = tag [ ] \n 
tags [ key ] = value \n 
~~ ~~ return tags \n 
~~ def _get_extra_dict ( self , response , mapping ) : \n 
extra = { } \n 
for attribute , values in mapping . items ( ) : \n 
~~~ transform_func = values [ ] \n 
value = response . get ( values [ ] , None ) \n 
if value is not None : \n 
~~~ extra [ attribute ] = transform_func ( value ) \n 
~~~ extra [ attribute ] = None \n 
~~ ~~ return extra \n 
~~ def _to_volume_state ( self , vol ) : \n 
~~~ state = self . VOLUME_STATE_MAP . get ( vol [ ] , \n 
StorageVolumeState . UNKNOWN ) \n 
if state == StorageVolumeState . AVAILABLE and in vol : \n 
~~~ state = StorageVolumeState . INUSE \n 
~~ return state \n 
~~~ import simplejson as json \n 
~~~ import json \n 
~~ from libcloud . container . base import ( ContainerDriver , Container , \n 
ContainerCluster , ContainerImage ) \n 
from libcloud . container . types import ContainerState \n 
from libcloud . container . utils . docker import RegistryClient \n 
from libcloud . common . aws import SignedAWSConnection , AWSJsonResponse \n 
ECS_VERSION = \n 
ECR_VERSION = \n 
ECS_HOST = \n 
ECR_HOST = \n 
ROOT = \n 
ECS_TARGET_BASE = % ( ECS_VERSION . replace ( , ) ) \n 
ECR_TARGET_BASE = % ( ECR_VERSION . replace ( , ) ) \n 
class ECSJsonConnection ( SignedAWSConnection ) : \n 
~~~ version = ECS_VERSION \n 
host = ECS_HOST \n 
responseCls = AWSJsonResponse \n 
service_name = \n 
~~ class ECRJsonConnection ( SignedAWSConnection ) : \n 
~~~ version = ECR_VERSION \n 
host = ECR_HOST \n 
~~ class ElasticContainerDriver ( ContainerDriver ) : \n 
~~~ name = \n 
ecr_repository_host = \n 
connectionCls = ECSJsonConnection \n 
ecrConnectionClass = ECRJsonConnection \n 
supports_clusters = False \n 
status_map = { \n 
: ContainerState . RUNNING \n 
def __init__ ( self , access_id , secret , region ) : \n 
~~~ super ( ElasticContainerDriver , self ) . __init__ ( access_id , secret ) \n 
self . region = region \n 
self . region_name = region \n 
self . connection . host = ECS_HOST % ( region ) \n 
conn_kwargs = self . _ex_connection_class_kwargs ( ) \n 
self . ecr_connection = self . ecrConnectionClass ( \n 
access_id , secret , ** conn_kwargs ) \n 
self . ecr_connection . host = ECR_HOST % ( region ) \n 
self . ecr_connection . driver = self \n 
self . ecr_connection . connect ( ) \n 
~~ def _ex_connection_class_kwargs ( self ) : \n 
~~~ return { : } \n 
~~ def list_images ( self , ex_repository_name ) : \n 
request = { } \n 
request [ ] = ex_repository_name \n 
list_response = self . ecr_connection . request ( \n 
ROOT , \n 
method = , \n 
data = json . dumps ( request ) , \n 
headers = self . _get_ecr_headers ( ) \n 
) . object \n 
repository_id = self . ex_get_repository_id ( ex_repository_name ) \n 
host = self . _get_ecr_host ( repository_id ) \n 
return self . _to_images ( list_response [ ] , \n 
host , \n 
ex_repository_name ) \n 
~~ def list_clusters ( self ) : \n 
listdata = self . connection . request ( \n 
data = json . dumps ( { } ) , \n 
headers = self . _get_headers ( ) \n 
request = { : listdata [ ] } \n 
data = self . connection . request ( \n 
return self . _to_clusters ( data ) \n 
~~ def create_cluster ( self , name , location = None ) : \n 
request = { : name } \n 
response = self . connection . request ( \n 
return self . _to_cluster ( response [ ] ) \n 
~~ def destroy_cluster ( self , cluster ) : \n 
request = { : cluster . id } \n 
return data [ ] [ ] == \n 
~~ def list_containers ( self , image = None , cluster = None ) : \n 
request = { : } \n 
if cluster is not None : \n 
~~~ request [ ] = cluster . id \n 
~~ if image is not None : \n 
~~~ request [ ] = image . name \n 
~~ list_response = self . connection . request ( \n 
if len ( list_response [ ] ) == 0 : \n 
~~ containers = self . ex_list_containers_for_task ( \n 
list_response [ ] ) \n 
return containers \n 
~~ def deploy_container ( self , name , image , cluster = None , \n 
parameters = None , start = True , ex_cpu = 10 , ex_memory = 500 , \n 
ex_container_port = None , ex_host_port = None ) : \n 
if ex_container_port is None and ex_host_port is None : \n 
~~~ port_maps = [ ] \n 
~~~ port_maps = [ \n 
"containerPort" : ex_container_port , \n 
"hostPort" : ex_host_port \n 
~~ data [ ] = [ \n 
"mountPoints" : [ ] , \n 
"name" : name , \n 
"image" : image . name , \n 
"cpu" : ex_cpu , \n 
"environment" : [ ] , \n 
"memory" : ex_memory , \n 
"portMappings" : port_maps , \n 
"essential" : True , \n 
"volumesFrom" : [ ] \n 
data [ ] = name \n 
data = json . dumps ( data ) , \n 
if start : \n 
~~~ return self . ex_start_task ( \n 
response [ ] [ ] ) [ 0 ] \n 
~~~ return Container ( \n 
id = None , \n 
image = image , \n 
state = ContainerState . RUNNING , \n 
ip_addresses = [ ] , \n 
response [ ] [ ] \n 
driver = self . connection . driver \n 
~~ ~~ def get_container ( self , id ) : \n 
containers = self . ex_list_containers_for_task ( [ id ] ) \n 
return containers [ 0 ] \n 
~~ def start_container ( self , container , count = 1 ) : \n 
return self . ex_start_task ( container . extra [ ] , count ) \n 
~~ def stop_container ( self , container ) : \n 
request = { : container . extra [ ] } \n 
containers = [ ] \n 
containers . extend ( self . _to_containers ( \n 
response [ ] , \n 
container . extra [ ] ) ) \n 
~~ def restart_container ( self , container ) : \n 
self . stop_container ( container ) \n 
return self . start_container ( container ) \n 
~~ def destroy_container ( self , container ) : \n 
return self . stop_container ( container ) \n 
~~ def ex_start_task ( self , task_arn , count = 1 ) : \n 
request = None \n 
request = { : count , \n 
: task_arn } \n 
for task in response [ ] : \n 
~~~ containers . extend ( self . _to_containers ( task , task_arn ) ) \n 
~~ return containers \n 
~~ def ex_list_containers_for_task ( self , task_arns ) : \n 
describe_request = { : task_arns } \n 
descripe_response = self . connection . request ( \n 
data = json . dumps ( describe_request ) , \n 
for task in descripe_response [ ] : \n 
~~~ containers . extend ( self . _to_containers ( \n 
task , task [ ] ) ) \n 
~~ def ex_create_service ( self , name , cluster , \n 
task_definition , desired_count = 1 ) : \n 
request = { \n 
: task_definition , \n 
: desired_count , \n 
: cluster . id } \n 
return response [ ] \n 
~~ def ex_list_service_arns ( self , cluster = None ) : \n 
~~ response = self . connection . request ( \n 
~~ def ex_describe_service ( self , service_arn ) : \n 
request = { : [ service_arn ] } \n 
return response [ ] [ 0 ] \n 
~~ def ex_destroy_service ( self , service_arn ) : \n 
: service_arn } \n 
~~ def ex_get_registry_client ( self , repository_name ) : \n 
repository_id = self . ex_get_repository_id ( repository_name ) \n 
token = self . ex_get_repository_token ( repository_id ) \n 
return RegistryClient ( \n 
username = , \n 
password = token \n 
~~ def ex_get_repository_token ( self , repository_id ) : \n 
request = { : [ repository_id ] } \n 
response = self . ecr_connection . request ( \n 
return response [ ] [ 0 ] [ ] \n 
~~ def ex_get_repository_id ( self , repository_name ) : \n 
request = { : [ repository_name ] } \n 
repository_id = list_response [ ] [ 0 ] [ ] \n 
return repository_id \n 
~~ def _get_ecr_host ( self , repository_id ) : \n 
~~~ return self . ecr_repository_host % ( \n 
repository_id , \n 
self . region ) \n 
~~ def _get_headers ( self , action ) : \n 
return { : % \n 
( ECS_TARGET_BASE , action ) , \n 
~~ def _get_ecr_headers ( self , action ) : \n 
( ECR_TARGET_BASE , action ) , \n 
~~ def _to_clusters ( self , data ) : \n 
~~~ clusters = [ ] \n 
for cluster in data [ ] : \n 
~~~ clusters . append ( self . _to_cluster ( cluster ) ) \n 
~~ return clusters \n 
~~ def _to_cluster ( self , data ) : \n 
~~~ return ContainerCluster ( \n 
~~ def _to_containers ( self , data , task_definition_arn ) : \n 
~~~ clusters . append ( self . _to_container ( cluster , task_definition_arn ) ) \n 
~~ def _to_container ( self , data , task_definition_arn ) : \n 
image = ContainerImage ( \n 
path = None , \n 
version = None , \n 
ip_addresses = None , \n 
state = self . status_map . get ( data [ ] , None ) , \n 
: data [ ] , \n 
: task_definition_arn \n 
~~ def _to_images ( self , data , host , repository_name ) : \n 
~~~ images = [ ] \n 
for image in data : \n 
~~~ images . append ( self . _to_image ( image , host , repository_name ) ) \n 
~~ def _to_image ( self , data , host , repository_name ) : \n 
~~~ path = % ( \n 
repository_name , \n 
data [ ] \n 
return ContainerImage ( \n 
name = path , \n 
path = path , \n 
version = data [ ] , \n 
~~ ~~ import copy \n 
from libcloud . utils . py3 import httplib \n 
from libcloud . common . openstack import OpenStackDriverMixin \n 
from libcloud . common . base import PollingConnection \n 
from libcloud . common . exceptions import BaseHTTPError \n 
from libcloud . common . types import LibcloudError \n 
from libcloud . utils . misc import merge_valid_keys , get_new_obj \n 
from libcloud . common . rackspace import AUTH_URL \n 
from libcloud . compute . drivers . openstack import OpenStack_1_1_Connection \n 
from libcloud . compute . drivers . openstack import OpenStack_1_1_Response \n 
from libcloud . dns . types import Provider , RecordType \n 
from libcloud . dns . types import ZoneDoesNotExistError , RecordDoesNotExistError \n 
from libcloud . dns . base import DNSDriver , Zone , Record \n 
VALID_ZONE_EXTRA_PARAMS = [ , , ] \n 
VALID_RECORD_EXTRA_PARAMS = [ , , , , \n 
class RackspaceDNSResponse ( OpenStack_1_1_Response ) : \n 
def parse_error ( self ) : \n 
~~~ status = int ( self . status ) \n 
context = self . connection . context \n 
body = self . parse_body ( ) \n 
if status == httplib . NOT_FOUND : \n 
~~~ if context [ ] == : \n 
~~~ raise ZoneDoesNotExistError ( value = , driver = self , \n 
zone_id = context [ ] ) \n 
~~ elif context [ ] == : \n 
~~~ raise RecordDoesNotExistError ( value = , driver = self , \n 
record_id = context [ ] ) \n 
~~ ~~ if body : \n 
~~~ if and in body : \n 
~~~ err = % ( body [ ] , body [ ] , \n 
body [ ] ) \n 
return err \n 
~~ elif in body : \n 
~~~ errors = [ m for m in body [ ] [ ] ] \n 
err = % . join ( errors ) \n 
~~ ~~ raise LibcloudError ( % ( status ) ) \n 
~~ ~~ class RackspaceDNSConnection ( OpenStack_1_1_Connection , PollingConnection ) : \n 
responseCls = RackspaceDNSResponse \n 
XML_NAMESPACE = None \n 
poll_interval = 2.5 \n 
timeout = 30 \n 
auth_url = AUTH_URL \n 
_auth_version = \n 
~~~ self . region = kwargs . pop ( , None ) \n 
super ( RackspaceDNSConnection , self ) . __init__ ( * args , ** kwargs ) \n 
~~ def get_poll_request_kwargs ( self , response , context , request_kwargs ) : \n 
~~~ job_id = response . object [ ] \n 
kwargs = { : % ( job_id ) , \n 
: { : True } } \n 
return kwargs \n 
~~ def has_completed ( self , response ) : \n 
~~~ status = response . object [ ] \n 
if status == : \n 
~~~ data = response . object [ ] \n 
if and in data : \n 
~~~ message = % ( data [ ] , data [ ] , \n 
data [ ] ) \n 
~~~ message = data [ ] \n 
~~ raise LibcloudError ( message , \n 
driver = self . driver ) \n 
~~ return status == \n 
~~ def get_endpoint ( self ) : \n 
~~~ if in self . _auth_version : \n 
~~~ ep = self . service_catalog . get_endpoint ( name = , \n 
service_type = , \n 
region = None ) \n 
( self . _auth_version ) ) \n 
~~ public_url = ep . url \n 
if self . region == : \n 
~~~ public_url = public_url . replace ( , \n 
~~ if self . region == : \n 
~~ return public_url \n 
~~ ~~ class RackspacePTRRecord ( object ) : \n 
~~~ def __init__ ( self , id , ip , domain , driver , extra = None ) : \n 
~~~ self . id = str ( id ) if id else None \n 
self . ip = ip \n 
self . type = RecordType . PTR \n 
~~ def update ( self , domain , extra = None ) : \n 
~~~ return self . driver . ex_update_ptr_record ( record = self , domain = domain , \n 
~~~ return self . driver . ex_delete_ptr_record ( record = self ) \n 
~~~ return ( % \n 
( self . __class__ . __name__ , self . ip , \n 
self . domain , self . driver . name ) ) \n 
~~ ~~ class RackspaceDNSDriver ( DNSDriver , OpenStackDriverMixin ) : \n 
type = Provider . RACKSPACE \n 
connectionCls = RackspaceDNSConnection \n 
region = , ** kwargs ) : \n 
~~~ valid_regions = self . list_regions ( ) \n 
if region not in valid_regions : \n 
~~~ raise ValueError ( % ( region ) ) \n 
~~ OpenStackDriverMixin . __init__ ( self , ** kwargs ) \n 
super ( RackspaceDNSDriver , self ) . __init__ ( key = key , secret = secret , \n 
host = host , port = port , \n 
region = region ) \n 
~~ RECORD_TYPE_MAP = { \n 
RecordType . A : , \n 
RecordType . AAAA : , \n 
RecordType . CNAME : , \n 
RecordType . MX : , \n 
RecordType . NS : , \n 
RecordType . PTR : , \n 
RecordType . SRV : , \n 
RecordType . TXT : , \n 
def list_regions ( cls ) : \n 
~~~ return [ , ] \n 
~~ def iterate_zones ( self ) : \n 
~~~ offset = 0 \n 
limit = 100 \n 
~~~ params = { \n 
: limit , \n 
: offset , \n 
action = , params = params ) . object \n 
zones_list = response [ ] \n 
for item in zones_list : \n 
~~~ yield self . _to_zone ( item ) \n 
~~ if _rackspace_result_has_more ( response , len ( zones_list ) , limit ) : \n 
~~~ offset += limit \n 
~~ ~~ ~~ def iterate_records ( self , zone ) : \n 
~~~ self . connection . set_context ( { : , : zone . id } ) \n 
: True , \n 
action = % ( zone . id ) , params = params ) . object \n 
records_list = response [ ] \n 
records = records_list [ ] \n 
for item in records : \n 
~~~ record = self . _to_record ( data = item , zone = zone ) \n 
yield record \n 
~~ if _rackspace_result_has_more ( records_list , len ( records ) , limit ) : \n 
~~ ~~ ~~ def get_zone ( self , zone_id ) : \n 
~~~ self . connection . set_context ( { : , : zone_id } ) \n 
response = self . connection . request ( action = % ( zone_id ) ) \n 
zone = self . _to_zone ( data = response . object ) \n 
return zone \n 
~~ def get_record ( self , zone_id , record_id ) : \n 
~~~ zone = self . get_zone ( zone_id = zone_id ) \n 
self . connection . set_context ( { : , : record_id } ) \n 
response = self . connection . request ( action = % \n 
( zone_id , record_id ) ) . object \n 
record = self . _to_record ( data = response , zone = zone ) \n 
return record \n 
~~ def create_zone ( self , domain , type = , ttl = None , extra = None ) : \n 
~~~ extra = extra if extra else { } \n 
if not in extra : \n 
~~ payload = { : domain , : extra [ ] , \n 
: { : [ ] } } \n 
if ttl : \n 
~~~ payload [ ] = ttl \n 
~~ if in extra : \n 
~~~ payload [ ] = extra [ ] \n 
~~ data = { : [ payload ] } \n 
response = self . connection . async_request ( action = , \n 
method = , data = data ) \n 
zone = self . _to_zone ( data = response . object [ ] [ ] [ 0 ] ) \n 
~~ def update_zone ( self , zone , domain = None , type = None , ttl = None , extra = None ) : \n 
if domain : \n 
~~~ raise LibcloudError ( , driver = self ) \n 
~~~ data [ ] = int ( ttl ) \n 
~~~ data [ ] = extra [ ] \n 
~~ type = type if type else zone . type \n 
ttl = ttl if ttl else zone . ttl \n 
self . connection . set_context ( { : , : zone . id } ) \n 
self . connection . async_request ( action = % ( zone . id ) , \n 
merged = merge_valid_keys ( params = copy . deepcopy ( zone . extra ) , \n 
valid_keys = VALID_ZONE_EXTRA_PARAMS , \n 
updated_zone = get_new_obj ( obj = zone , klass = Zone , \n 
attributes = { : type , \n 
: ttl , \n 
: merged } ) \n 
return updated_zone \n 
~~ def create_record ( self , name , zone , type , data , extra = None ) : \n 
name = self . _to_full_record_name ( domain = zone . domain , name = name ) \n 
data = { : name , : self . RECORD_TYPE_MAP [ type ] , \n 
: data } \n 
if in extra : \n 
~~~ data [ ] = int ( extra [ ] ) \n 
~~ payload = { : [ data ] } \n 
response = self . connection . async_request ( action = \n 
% ( zone . id ) , data = payload , \n 
method = ) . object \n 
record = self . _to_record ( data = response [ ] [ ] [ 0 ] , \n 
zone = zone ) \n 
~~ def update_record ( self , record , name = None , type = None , data = None , \n 
name = self . _to_full_record_name ( domain = record . zone . domain , \n 
name = record . name ) \n 
payload = { : name } \n 
~~~ payload [ ] = data \n 
~~ type = type if type is not None else record . type \n 
data = data if data else record . data \n 
self . connection . set_context ( { : , : record . id } ) \n 
self . connection . async_request ( action = % \n 
( record . zone . id , record . id ) , \n 
method = , data = payload ) \n 
merged = merge_valid_keys ( params = copy . deepcopy ( record . extra ) , \n 
valid_keys = VALID_RECORD_EXTRA_PARAMS , \n 
updated_record = get_new_obj ( obj = record , klass = Record , \n 
: data , \n 
: self , \n 
return updated_record \n 
~~ def delete_zone ( self , zone ) : \n 
~~ def delete_record ( self , record ) : \n 
~~~ self . connection . set_context ( { : , : record . id } ) \n 
~~ def ex_iterate_ptr_records ( self , device ) : \n 
_check_ptr_extra_fields ( device ) \n 
params = { : device . extra [ ] } \n 
service_name = device . extra [ ] \n 
self . connection . set_context ( { : } ) \n 
~~~ response = self . connection . request ( \n 
action = % ( service_name ) , params = params ) . object \n 
records = response [ ] \n 
link = dict ( rel = service_name , ** params ) \n 
~~~ record = self . _to_ptr_record ( data = item , link = link ) \n 
~~ ~~ except BaseHTTPError as exc : \n 
~~~ if exc . code == 404 : \n 
~~ ~~ def ex_get_ptr_record ( self , service_name , record_id ) : \n 
action = % ( service_name , record_id ) ) . object \n 
item = next ( iter ( response [ ] [ ] ) ) \n 
return self . _to_ptr_record ( data = item , link = response [ ] ) \n 
~~ def ex_create_ptr_record ( self , device , ip , domain , extra = None ) : \n 
if extra is None : \n 
~~~ extra = { } \n 
~~ data = { \n 
"name" : domain , \n 
"type" : RecordType . PTR , \n 
"data" : ip \n 
~~ payload = { \n 
"recordsList" : { \n 
"records" : [ data ] \n 
"link" : { \n 
"content" : "" , \n 
"href" : device . extra [ ] , \n 
"rel" : device . extra [ ] , \n 
response = self . connection . async_request ( \n 
action = , method = , data = payload ) . object \n 
return self . _to_ptr_record ( data = item , link = payload [ ] ) \n 
~~ def ex_update_ptr_record ( self , record , domain = None , extra = None ) : \n 
if domain is not None and domain == record . domain : \n 
~~~ domain = None \n 
~~ if extra is not None : \n 
~~~ extra = dict ( extra ) \n 
for key in extra : \n 
~~~ if key in record . extra and record . extra [ key ] == extra [ key ] : \n 
~~~ del extra [ key ] \n 
~~ ~~ ~~ if domain is None and not extra : \n 
~~~ return record \n 
~~ _check_ptr_extra_fields ( record ) \n 
ip = record . ip \n 
self . ex_delete_ptr_record ( record ) \n 
return self . ex_create_ptr_record ( record , ip , domain , extra = extra ) \n 
~~ def ex_delete_ptr_record ( self , record ) : \n 
_check_ptr_extra_fields ( record ) \n 
self . connection . async_request ( \n 
action = % ( record . extra [ ] ) , \n 
params = { : record . extra [ ] , : record . ip } , \n 
~~ def _to_zone ( self , data ) : \n 
~~~ id = data [ ] \n 
domain = data [ ] \n 
type = \n 
ttl = data . get ( , 0 ) \n 
~~~ extra [ ] = data [ ] \n 
~~ if in data : \n 
~~ zone = Zone ( id = str ( id ) , domain = domain , type = type , ttl = int ( ttl ) , \n 
~~ def _to_record ( self , data , zone ) : \n 
fqdn = data [ ] \n 
name = self . _to_partial_record_name ( domain = zone . domain , name = fqdn ) \n 
type = self . _string_to_record_type ( data [ ] ) \n 
record_data = data [ ] \n 
extra = { : fqdn } \n 
for key in VALID_RECORD_EXTRA_PARAMS : \n 
~~~ if key in data : \n 
~~~ extra [ key ] = data [ key ] \n 
~~ ~~ record = Record ( id = str ( id ) , name = name , type = type , data = record_data , \n 
zone = zone , driver = self , ttl = extra . get ( , None ) , \n 
~~ def _to_ptr_record ( self , data , link ) : \n 
ip = data [ ] \n 
extra = { : link [ ] , : link [ ] } \n 
~~ ~~ record = RackspacePTRRecord ( id = str ( id ) , ip = ip , domain = domain , \n 
~~ def _to_full_record_name ( self , domain , name ) : \n 
~~~ name = % ( name , domain ) \n 
~~~ name = domain \n 
~~ return name \n 
~~ def _to_partial_record_name ( self , domain , name ) : \n 
if name == domain : \n 
~~ name = name . replace ( % ( domain ) , ) \n 
return name \n 
~~~ kwargs = self . openstack_connection_kwargs ( ) \n 
kwargs [ ] = self . region \n 
~~ ~~ def _rackspace_result_has_more ( response , result_length , limit ) : \n 
~~~ if result_length < limit : \n 
~~ for item in response . get ( , ( ) ) : \n 
~~~ if item [ ] == : \n 
~~ def _check_ptr_extra_fields ( device_or_record ) : \n 
~~~ if not ( hasattr ( device_or_record , ) and \n 
isinstance ( device_or_record . extra , dict ) and \n 
device_or_record . extra . get ( ) is not None and \n 
device_or_record . extra . get ( ) is not None ) : \n 
"\'extra\'" % device_or_record ) \n 
import hashlib \n 
from os . path import join as pjoin \n 
from libcloud . utils . py3 import next \n 
import libcloud . utils . files \n 
from libcloud . storage . types import ObjectDoesNotExistError \n 
CHUNK_SIZE = 8096 \n 
DEFAULT_CONTENT_TYPE = \n 
class Object ( object ) : \n 
def __init__ ( self , name , size , hash , extra , meta_data , container , \n 
driver ) : \n 
self . hash = hash \n 
self . container = container \n 
self . meta_data = meta_data or { } \n 
~~ def get_cdn_url ( self ) : \n 
~~~ return self . driver . get_object_cdn_url ( obj = self ) \n 
~~ def enable_cdn ( self , ** kwargs ) : \n 
~~~ return self . driver . enable_object_cdn ( obj = self , ** kwargs ) \n 
~~ def download ( self , destination_path , overwrite_existing = False , \n 
delete_on_failure = True ) : \n 
~~~ return self . driver . download_object ( self , destination_path , \n 
overwrite_existing , \n 
delete_on_failure ) \n 
~~ def as_stream ( self , chunk_size = None ) : \n 
~~~ return self . driver . download_object_as_stream ( self , chunk_size ) \n 
~~~ return self . driver . delete_object ( self ) \n 
( self . name , self . size , self . hash , self . driver . name ) ) \n 
~~ ~~ class Container ( object ) : \n 
def __init__ ( self , name , extra , driver ) : \n 
~~ def iterate_objects ( self ) : \n 
~~~ return self . driver . iterate_container_objects ( container = self ) \n 
~~ def list_objects ( self ) : \n 
~~~ return self . driver . list_container_objects ( container = self ) \n 
~~~ return self . driver . get_container_cdn_url ( container = self ) \n 
~~~ return self . driver . enable_container_cdn ( container = self , ** kwargs ) \n 
~~ def get_object ( self , object_name ) : \n 
~~~ return self . driver . get_object ( container_name = self . name , \n 
object_name = object_name ) \n 
~~ def upload_object ( self , file_path , object_name , extra = None , ** kwargs ) : \n 
~~~ return self . driver . upload_object ( \n 
file_path , self , object_name , extra = extra , ** kwargs ) \n 
~~ def upload_object_via_stream ( self , iterator , object_name , extra = None , \n 
~~~ return self . driver . upload_object_via_stream ( \n 
iterator , self , object_name , extra = extra , ** kwargs ) \n 
~~ def download_object ( self , obj , destination_path , overwrite_existing = False , \n 
~~~ return self . driver . download_object ( \n 
obj , destination_path , overwrite_existing = overwrite_existing , \n 
delete_on_failure = delete_on_failure ) \n 
~~ def download_object_as_stream ( self , obj , chunk_size = None ) : \n 
~~~ return self . driver . download_object_as_stream ( obj , chunk_size ) \n 
~~ def delete_object ( self , obj ) : \n 
~~~ return self . driver . delete_object ( obj ) \n 
~~~ return self . driver . delete_container ( self ) \n 
% ( self . name , self . driver . name ) ) \n 
~~ ~~ class StorageDriver ( BaseDriver ) : \n 
hash_type = \n 
supports_chunked_encoding = False \n 
strict_mode = False \n 
def iterate_containers ( self ) : \n 
~~ def list_containers ( self ) : \n 
return list ( self . iterate_containers ( ) ) \n 
~~ def iterate_container_objects ( self , container ) : \n 
~~ def list_container_objects ( self , container ) : \n 
return list ( self . iterate_container_objects ( container ) ) \n 
~~ def get_container ( self , container_name ) : \n 
~~ def get_container_cdn_url ( self , container ) : \n 
~~ def get_object ( self , container_name , object_name ) : \n 
~~ def get_object_cdn_url ( self , obj ) : \n 
~~ def enable_container_cdn ( self , container ) : \n 
~~ def enable_object_cdn ( self , obj ) : \n 
~~ def upload_object ( self , file_path , container , object_name , extra = None , \n 
verify_hash = True , headers = None ) : \n 
~~ def upload_object_via_stream ( self , iterator , container , \n 
object_name , \n 
extra = None , \n 
headers = None ) : \n 
~~ def create_container ( self , container_name ) : \n 
~~ def delete_container ( self , container ) : \n 
~~ def _get_object ( self , obj , callback , callback_kwargs , response , \n 
success_status_code = None ) : \n 
success_status_code = success_status_code or httplib . OK \n 
if response . status == success_status_code : \n 
~~~ return callback ( ** callback_kwargs ) \n 
~~ elif response . status == httplib . NOT_FOUND : \n 
~~~ raise ObjectDoesNotExistError ( object_name = obj . name , \n 
value = , driver = self ) \n 
~~ raise LibcloudError ( value = % \n 
( response . status ) , \n 
~~ def _save_object ( self , response , obj , destination_path , \n 
overwrite_existing = False , delete_on_failure = True , \n 
chunk_size = None ) : \n 
chunk_size = chunk_size or CHUNK_SIZE \n 
base_name = os . path . basename ( destination_path ) \n 
if not base_name and not os . path . exists ( destination_path ) : \n 
value = % ( destination_path ) , \n 
~~ if not base_name : \n 
~~~ file_path = pjoin ( destination_path , obj . name ) \n 
~~~ file_path = destination_path \n 
~~ if os . path . exists ( file_path ) and not overwrite_existing : \n 
value = % ( file_path ) + \n 
~~ stream = libcloud . utils . files . read_in_chunks ( response , chunk_size ) \n 
~~~ data_read = next ( stream ) \n 
~~ except StopIteration : \n 
~~ bytes_transferred = 0 \n 
with open ( file_path , ) as file_handle : \n 
~~~ while len ( data_read ) > 0 : \n 
~~~ file_handle . write ( b ( data_read ) ) \n 
bytes_transferred += len ( data_read ) \n 
~~~ data_read = \n 
~~ ~~ ~~ if int ( obj . size ) != int ( bytes_transferred ) : \n 
~~~ if delete_on_failure : \n 
~~~ os . unlink ( file_path ) \n 
~~ return True \n 
~~ def _upload_object ( self , object_name , content_type , upload_func , \n 
upload_func_kwargs , request_path , request_method = , \n 
headers = None , file_path = None , iterator = None ) : \n 
headers = headers or { } \n 
if file_path and not os . path . exists ( file_path ) : \n 
~~~ raise OSError ( % ( file_path ) ) \n 
~~ if iterator is not None and not hasattr ( iterator , ) and not hasattr ( iterator , ) : \n 
~~~ raise AttributeError ( + \n 
~~ if not content_type : \n 
~~~ if file_path : \n 
~~~ name = file_path \n 
~~~ name = object_name \n 
~~ content_type , _ = libcloud . utils . files . guess_file_mime_type ( name ) \n 
if not content_type : \n 
~~~ if self . strict_mode : \n 
~~~ raise AttributeError ( \n 
~~~ content_type = DEFAULT_CONTENT_TYPE \n 
~~ ~~ ~~ file_size = None \n 
if iterator : \n 
~~~ if self . supports_chunked_encoding : \n 
upload_func_kwargs [ ] = True \n 
~~~ iterator = libcloud . utils . files . read_in_chunks ( \n 
iterator = iterator ) \n 
data = libcloud . utils . files . exhaust_iterator ( iterator = iterator ) \n 
file_size = len ( data ) \n 
upload_func_kwargs [ ] = data \n 
~~~ file_size = os . path . getsize ( file_path ) \n 
upload_func_kwargs [ ] = False \n 
~~ if file_size is not None and not in headers : \n 
~~~ headers [ ] = file_size \n 
~~ headers [ ] = content_type \n 
response = self . connection . request ( request_path , \n 
method = request_method , data = None , \n 
headers = headers , raw = True ) \n 
upload_func_kwargs [ ] = response \n 
success , data_hash , bytes_transferred = upload_func ( \n 
** upload_func_kwargs ) \n 
if not success : \n 
~~ result_dict = { : response , : data_hash , \n 
: bytes_transferred } \n 
return result_dict \n 
~~ def _upload_data ( self , response , data , calculate_hash = True ) : \n 
bytes_transferred = 0 \n 
data_hash = None \n 
if calculate_hash : \n 
~~~ data_hash = self . _get_hash_function ( ) \n 
data_hash . update ( b ( data ) ) \n 
~~~ response . connection . connection . send ( b ( data ) ) \n 
~~~ return False , None , bytes_transferred \n 
~~ bytes_transferred = len ( data ) \n 
~~~ data_hash = data_hash . hexdigest ( ) \n 
~~ return True , data_hash , bytes_transferred \n 
~~ def _stream_data ( self , response , iterator , chunked = False , \n 
calculate_hash = True , chunk_size = None , data = None ) : \n 
~~ generator = libcloud . utils . files . read_in_chunks ( iterator , chunk_size , \n 
fill_size = True ) \n 
~~~ chunk = next ( generator ) \n 
~~~ chunk = \n 
if chunked : \n 
~~~ response . connection . connection . send ( b ( % \n 
( len ( chunk ) ) ) ) \n 
response . connection . connection . send ( chunk ) \n 
response . connection . connection . send ( b ( ) ) \n 
~~~ response . connection . connection . send ( chunk ) \n 
~~ return True , data_hash . hexdigest ( ) , bytes_transferred \n 
~~ while len ( chunk ) > 0 : \n 
~~~ if chunked : \n 
response . connection . connection . send ( b ( chunk ) ) \n 
~~~ response . connection . connection . send ( b ( chunk ) ) \n 
~~ bytes_transferred += len ( chunk ) \n 
~~~ data_hash . update ( b ( chunk ) ) \n 
~~ ~~ if chunked : \n 
~~~ response . connection . connection . send ( b ( ) ) \n 
~~ if calculate_hash : \n 
~~ def _upload_file ( self , response , file_path , chunked = False , \n 
calculate_hash = True ) : \n 
~~~ success , data_hash , bytes_transferred = ( \n 
self . _stream_data ( \n 
response = response , \n 
iterator = iter ( file_handle ) , \n 
chunked = chunked , \n 
calculate_hash = calculate_hash ) ) \n 
~~ return success , data_hash , bytes_transferred \n 
~~ def _get_hash_function ( self ) : \n 
~~~ func = getattr ( hashlib , self . hash_type ) ( ) \n 
~~ except AttributeError : \n 
~~~ raise RuntimeError ( % \n 
( self . hash_type ) ) \n 
from libcloud . common . types import InvalidCredsError \n 
from libcloud . common . digitalocean import DigitalOceanBaseDriver \n 
from libcloud . test import LibcloudTestCase , MockHttpTestCase \n 
from libcloud . test . file_fixtures import FileFixtures \n 
from libcloud . test . secrets import DIGITALOCEAN_v2_PARAMS \n 
class DigitalOceanTests ( LibcloudTestCase ) : \n 
~~~ DigitalOceanBaseDriver . connectionCls . conn_classes = ( None , DigitalOceanMockHttp ) \n 
DigitalOceanMockHttp . type = None \n 
self . driver = DigitalOceanBaseDriver ( * DIGITALOCEAN_v2_PARAMS ) \n 
~~ def test_authentication ( self ) : \n 
~~~ DigitalOceanMockHttp . type = \n 
self . assertRaises ( InvalidCredsError , self . driver . ex_account_info ) \n 
~~ def test_ex_account_info ( self ) : \n 
~~~ account_info = self . driver . ex_account_info ( ) \n 
self . assertEqual ( account_info [ ] , \n 
self . assertTrue ( account_info [ ] ) \n 
self . assertEqual ( account_info [ ] , ) \n 
self . assertEqual ( account_info [ ] , 10 ) \n 
~~ def test_ex_list_events ( self ) : \n 
~~~ events = self . driver . ex_list_events ( ) \n 
self . assertEqual ( events , [ ] ) \n 
~~ def test_ex_get_event ( self ) : \n 
~~~ action = self . driver . ex_get_event ( ) \n 
self . assertEqual ( action [ "id" ] , 12345670 ) \n 
self . assertEqual ( action [ "status" ] , "completed" ) \n 
self . assertEqual ( action [ "type" ] , "power_on" ) \n 
~~ def test__paginated_request ( self ) : \n 
actions = self . driver . _paginated_request ( , ) \n 
self . assertEqual ( actions [ 0 ] [ ] , 12345671 ) \n 
self . assertEqual ( actions [ 0 ] [ ] , ) \n 
~~ ~~ class DigitalOceanMockHttp ( MockHttpTestCase ) : \n 
~~~ fixtures = FileFixtures ( , ) \n 
response = { \n 
None : httplib . OK , \n 
: httplib . CREATED , \n 
: httplib . NO_CONTENT , \n 
: httplib . OK , \n 
: httplib . NOT_FOUND , \n 
: httplib . UNAUTHORIZED , \n 
: httplib . OK \n 
def _v2_account ( self , method , url , body , headers ) : \n 
~~~ body = self . fixtures . load ( ) \n 
return ( self . response [ self . type ] , body , { } , \n 
httplib . responses [ self . response [ self . type ] ] ) \n 
~~ def _v2_account_UNAUTHORIZED ( self , method , url , body , headers ) : \n 
~~~ body = self . fixtures . load ( \n 
~~ def _v2_actions ( self , method , url , body , headers ) : \n 
~~ def _v2_actions_12345670 ( self , method , url , body , headers ) : \n 
~~ def _v2_actions_page_1 ( self , method , url , body , headers ) : \n 
return ( self . response [ None ] , body , { } , \n 
httplib . responses [ self . response [ None ] ] ) \n 
~~~ sys . exit ( unittest . main ( ) ) \n 
from libcloud . compute . drivers . gce import ( GCENodeDriver , API_VERSION , \n 
timestamp_to_datetime , \n 
GCEAddress , GCEBackendService , \n 
GCEFirewall , GCEForwardingRule , \n 
GCEHealthCheck , GCENetwork , \n 
GCENodeImage , GCERoute , \n 
GCETargetHttpProxy , GCEUrlMap , \n 
GCEZone ) \n 
from libcloud . common . google import ( GoogleBaseAuthConnection , \n 
ResourceNotFoundError , ResourceExistsError , \n 
InvalidRequestError , GoogleBaseError ) \n 
from libcloud . test . common . test_google import GoogleAuthMockHttp , GoogleTestCase \n 
from libcloud . compute . base import Node , StorageVolume \n 
from libcloud . test import MockHttpTestCase \n 
from libcloud . test . compute import TestCaseMixin \n 
from libcloud . test . file_fixtures import ComputeFileFixtures \n 
from libcloud . test . secrets import GCE_PARAMS , GCE_KEYWORD_PARAMS \n 
class GCENodeDriverTest ( GoogleTestCase , TestCaseMixin ) : \n 
GCEZone . _now = lambda x : datetime . datetime ( 2013 , 6 , 26 , 19 , 0 , 0 ) \n 
datacenter = \n 
~~~ GCEMockHttp . test = self \n 
GCENodeDriver . connectionCls . conn_classes = ( GCEMockHttp , GCEMockHttp ) \n 
GoogleBaseAuthConnection . conn_classes = ( GoogleAuthMockHttp , \n 
GoogleAuthMockHttp ) \n 
GCEMockHttp . type = None \n 
kwargs = GCE_KEYWORD_PARAMS . copy ( ) \n 
kwargs [ ] = \n 
kwargs [ ] = self . datacenter \n 
self . driver = GCENodeDriver ( * GCE_PARAMS , ** kwargs ) \n 
~~ def test_default_scopes ( self ) : \n 
~~~ self . assertEqual ( self . driver . scopes , None ) \n 
~~ def test_timestamp_to_datetime ( self ) : \n 
~~~ timestamp1 = \n 
datetime1 = datetime . datetime ( 2013 , 6 , 26 , 17 , 5 , 19 ) \n 
self . assertEqual ( timestamp_to_datetime ( timestamp1 ) , datetime1 ) \n 
timestamp2 = \n 
datetime2 = datetime . datetime ( 2013 , 6 , 26 , 17 , 43 , 15 ) \n 
self . assertEqual ( timestamp_to_datetime ( timestamp2 ) , datetime2 ) \n 
~~ def test_get_object_by_kind ( self ) : \n 
~~~ obj = self . driver . _get_object_by_kind ( None ) \n 
self . assertIsNone ( obj ) \n 
obj = self . driver . _get_object_by_kind ( ) \n 
obj = self . driver . _get_object_by_kind ( \n 
self . assertEqual ( obj . name , ) \n 
~~ def test_get_region_from_zone ( self ) : \n 
~~~ zone1 = self . driver . ex_get_zone ( ) \n 
expected_region1 = \n 
region1 = self . driver . _get_region_from_zone ( zone1 ) \n 
self . assertEqual ( region1 . name , expected_region1 ) \n 
zone2 = self . driver . ex_get_zone ( ) \n 
expected_region2 = \n 
region2 = self . driver . _get_region_from_zone ( zone2 ) \n 
self . assertEqual ( region2 . name , expected_region2 ) \n 
~~ def test_find_zone_or_region ( self ) : \n 
~~~ zone1 = self . driver . _find_zone_or_region ( , \n 
self . assertEqual ( zone1 . name , ) \n 
zone2 = self . driver . _find_zone_or_region ( \n 
self . assertEqual ( zone2 . name , ) \n 
region = self . driver . _find_zone_or_region ( , \n 
, region = True ) \n 
self . assertEqual ( region . name , ) \n 
~~ def test_match_images ( self ) : \n 
~~~ project = \n 
image = self . driver . _match_images ( project , ) \n 
self . assertEqual ( image . name , ) \n 
~~ def test_ex_get_serial_output ( self ) : \n 
~~~ self . assertRaises ( ValueError , self . driver . ex_get_serial_output , ) \n 
node = self . driver . ex_get_node ( , ) \n 
self . assertTrue ( self . driver . ex_get_serial_output ( node ) , \n 
~~ def test_ex_list ( self ) : \n 
~~~ d = self . driver \n 
for list_fn in ( d . ex_list_addresses , \n 
d . ex_list_backendservices , \n 
d . ex_list_disktypes , \n 
d . ex_list_firewalls , \n 
d . ex_list_forwarding_rules , \n 
d . ex_list_healthchecks , \n 
d . ex_list_networks , \n 
d . ex_list_project_images , \n 
d . ex_list_regions , \n 
d . ex_list_routes , \n 
d . ex_list_snapshots , \n 
d . ex_list_targethttpproxies , \n 
d . ex_list_targetinstances , \n 
d . ex_list_targetpools , \n 
d . ex_list_urlmaps , \n 
d . ex_list_zones , \n 
d . list_images , \n 
d . list_locations , \n 
d . list_nodes , \n 
d . list_sizes , \n 
d . list_volumes ) : \n 
~~~ full_list = [ item . name for item in list_fn ( ) ] \n 
li = d . ex_list ( list_fn ) \n 
iter_list = [ item . name for sublist in li for item in sublist ] \n 
self . assertEqual ( full_list , iter_list ) \n 
~~ list_fn = d . ex_list_regions \n 
for count , sublist in zip ( ( 2 , 1 ) , d . ex_list ( list_fn ) . page ( 2 ) ) : \n 
~~~ self . assertTrue ( len ( sublist ) == count ) \n 
~~ for sublist in d . ex_list ( list_fn ) . filter ( ) : \n 
~~~ self . assertTrue ( len ( sublist ) == 1 ) \n 
self . assertEqual ( sublist [ 0 ] . name , ) \n 
~~ ~~ def test_ex_list_addresses ( self ) : \n 
~~~ address_list = self . driver . ex_list_addresses ( ) \n 
address_list_all = self . driver . ex_list_addresses ( ) \n 
address_list_uc1 = self . driver . ex_list_addresses ( ) \n 
address_list_global = self . driver . ex_list_addresses ( ) \n 
self . assertEqual ( len ( address_list ) , 2 ) \n 
self . assertEqual ( len ( address_list_all ) , 5 ) \n 
self . assertEqual ( len ( address_list_global ) , 1 ) \n 
self . assertEqual ( address_list [ 0 ] . name , ) \n 
self . assertEqual ( address_list_uc1 [ 0 ] . name , ) \n 
self . assertEqual ( address_list_global [ 0 ] . name , ) \n 
names = [ a . name for a in address_list_all ] \n 
self . assertTrue ( in names ) \n 
~~ def test_ex_list_backendservices ( self ) : \n 
~~~ self . backendservices_mock = \n 
backendservices_list = self . driver . ex_list_backendservices ( ) \n 
self . assertListEqual ( backendservices_list , [ ] ) \n 
self . backendservices_mock = \n 
web_service = backendservices_list [ 0 ] \n 
self . assertEqual ( web_service . name , ) \n 
self . assertEqual ( len ( web_service . healthchecks ) , 1 ) \n 
self . assertEqual ( len ( web_service . backends ) , 2 ) \n 
~~ def test_ex_list_healthchecks ( self ) : \n 
~~~ healthchecks = self . driver . ex_list_healthchecks ( ) \n 
self . assertEqual ( len ( healthchecks ) , 3 ) \n 
self . assertEqual ( healthchecks [ 0 ] . name , ) \n 
~~ def test_ex_list_firewalls ( self ) : \n 
~~~ firewalls = self . driver . ex_list_firewalls ( ) \n 
self . assertEqual ( len ( firewalls ) , 5 ) \n 
self . assertEqual ( firewalls [ 0 ] . name , ) \n 
~~ def test_ex_list_forwarding_rules ( self ) : \n 
~~~ forwarding_rules = self . driver . ex_list_forwarding_rules ( ) \n 
forwarding_rules_all = self . driver . ex_list_forwarding_rules ( ) \n 
forwarding_rules_uc1 = self . driver . ex_list_forwarding_rules ( \n 
self . assertEqual ( len ( forwarding_rules ) , 2 ) \n 
self . assertEqual ( len ( forwarding_rules_all ) , 2 ) \n 
self . assertEqual ( forwarding_rules [ 0 ] . name , ) \n 
self . assertEqual ( forwarding_rules_uc1 [ 0 ] . name , ) \n 
names = [ f . name for f in forwarding_rules_all ] \n 
~~ def test_ex_list_forwarding_rules_global ( self ) : \n 
~~~ forwarding_rules = self . driver . ex_list_forwarding_rules ( global_rules = True ) \n 
names = [ f . name for f in forwarding_rules ] \n 
self . assertListEqual ( names , [ , ] ) \n 
~~ def test_list_images ( self ) : \n 
~~~ local_images = self . driver . list_images ( ) \n 
all_deprecated_images = self . driver . list_images ( ex_include_deprecated = True ) \n 
debian_images = self . driver . list_images ( ex_project = ) \n 
local_plus_deb = self . driver . list_images ( [ , ] ) \n 
self . assertEqual ( len ( local_images ) , 23 ) \n 
self . assertEqual ( len ( all_deprecated_images ) , 156 ) \n 
self . assertEqual ( len ( debian_images ) , 2 ) \n 
self . assertEqual ( len ( local_plus_deb ) , 3 ) \n 
self . assertEqual ( local_images [ 0 ] . name , ) \n 
self . assertEqual ( debian_images [ 1 ] . name , ) \n 
~~ def test_list_locations ( self ) : \n 
~~~ locations = self . driver . list_locations ( ) \n 
self . assertEqual ( len ( locations ) , 6 ) \n 
self . assertEqual ( locations [ 0 ] . name , ) \n 
~~ def test_ex_list_routes ( self ) : \n 
~~~ routes = self . driver . ex_list_routes ( ) \n 
self . assertEqual ( len ( routes ) , 3 ) \n 
self . assertTrue ( in [ route . name for route in routes ] ) \n 
~~ def test_ex_list_networks ( self ) : \n 
~~~ networks = self . driver . ex_list_networks ( ) \n 
self . assertEqual ( len ( networks ) , 3 ) \n 
self . assertEqual ( networks [ 0 ] . name , ) \n 
~~ def test_list_nodes ( self ) : \n 
~~~ nodes = self . driver . list_nodes ( ) \n 
nodes_all = self . driver . list_nodes ( ex_zone = ) \n 
nodes_uc1a = self . driver . list_nodes ( ex_zone = ) \n 
self . assertEqual ( len ( nodes ) , 1 ) \n 
self . assertEqual ( len ( nodes_all ) , 8 ) \n 
self . assertEqual ( len ( nodes_uc1a ) , 1 ) \n 
self . assertEqual ( nodes [ 0 ] . name , ) \n 
self . assertEqual ( nodes_uc1a [ 0 ] . name , ) \n 
names = [ n . name for n in nodes_all ] \n 
~~ def test_ex_list_regions ( self ) : \n 
~~~ regions = self . driver . ex_list_regions ( ) \n 
self . assertEqual ( len ( regions ) , 3 ) \n 
self . assertEqual ( regions [ 0 ] . name , ) \n 
~~ def test_ex_list_snapshots ( self ) : \n 
~~~ snapshots = self . driver . ex_list_snapshots ( ) \n 
self . assertEqual ( len ( snapshots ) , 2 ) \n 
self . assertEqual ( snapshots [ 0 ] . name , ) \n 
~~ def test_ex_list_targethttpproxies ( self ) : \n 
~~~ target_proxies = self . driver . ex_list_targethttpproxies ( ) \n 
self . assertEqual ( len ( target_proxies ) , 2 ) \n 
self . assertEqual ( target_proxies [ 0 ] . name , ) \n 
names = [ t . name for t in target_proxies ] \n 
~~ def test_ex_list_targetinstances ( self ) : \n 
~~~ target_instances = self . driver . ex_list_targetinstances ( ) \n 
target_instances_all = self . driver . ex_list_targetinstances ( ) \n 
target_instances_uc1 = self . driver . ex_list_targetinstances ( ) \n 
self . assertEqual ( len ( target_instances ) , 2 ) \n 
self . assertEqual ( len ( target_instances_all ) , 2 ) \n 
self . assertEqual ( len ( target_instances_uc1 ) , 2 ) \n 
self . assertEqual ( target_instances [ 0 ] . name , ) \n 
self . assertEqual ( target_instances_uc1 [ 0 ] . name , ) \n 
names = [ t . name for t in target_instances_all ] \n 
~~ def test_ex_list_targetpools ( self ) : \n 
~~~ target_pools = self . driver . ex_list_targetpools ( ) \n 
target_pools_all = self . driver . ex_list_targetpools ( ) \n 
target_pools_uc1 = self . driver . ex_list_targetpools ( ) \n 
self . assertEqual ( len ( target_pools ) , 4 ) \n 
self . assertEqual ( len ( target_pools_all ) , 5 ) \n 
self . assertEqual ( len ( target_pools_uc1 ) , 4 ) \n 
self . assertEqual ( target_pools [ 0 ] . name , ) \n 
self . assertEqual ( target_pools_uc1 [ 0 ] . name , ) \n 
names = [ t . name for t in target_pools_all ] \n 
~~ def test_list_sizes ( self ) : \n 
~~~ sizes = self . driver . list_sizes ( ) \n 
sizes_all = self . driver . list_sizes ( ) \n 
self . assertEqual ( len ( sizes ) , 22 ) \n 
self . assertEqual ( len ( sizes_all ) , 100 ) \n 
self . assertEqual ( sizes [ 0 ] . name , ) \n 
self . assertEqual ( sizes [ 0 ] . extra [ ] . name , ) \n 
names = [ s . name for s in sizes_all ] \n 
self . assertEqual ( names . count ( ) , 5 ) \n 
~~ def test_ex_get_license ( self ) : \n 
~~~ license = self . driver . ex_get_license ( , ) \n 
self . assertEqual ( license . name , ) \n 
self . assertTrue ( license . charges_use_fee ) \n 
~~ def test_list_disktypes ( self ) : \n 
~~~ disktypes = self . driver . ex_list_disktypes ( ) \n 
disktypes_all = self . driver . ex_list_disktypes ( ) \n 
disktypes_uc1a = self . driver . ex_list_disktypes ( ) \n 
self . assertEqual ( len ( disktypes ) , 2 ) \n 
self . assertEqual ( len ( disktypes_all ) , 9 ) \n 
self . assertEqual ( len ( disktypes_uc1a ) , 2 ) \n 
self . assertEqual ( disktypes [ 0 ] . name , ) \n 
self . assertEqual ( disktypes_uc1a [ 0 ] . name , ) \n 
names = [ v . name for v in disktypes_all ] \n 
~~ def test_ex_list_urlmaps ( self ) : \n 
~~~ urlmaps_list = self . driver . ex_list_urlmaps ( ) \n 
web_map = urlmaps_list [ 0 ] \n 
self . assertEqual ( web_map . name , ) \n 
self . assertEqual ( len ( web_map . host_rules ) , 0 ) \n 
self . assertEqual ( len ( web_map . path_matchers ) , 0 ) \n 
self . assertEqual ( len ( web_map . tests ) , 0 ) \n 
~~ def test_list_volumes ( self ) : \n 
~~~ volumes = self . driver . list_volumes ( ) \n 
volumes_all = self . driver . list_volumes ( ) \n 
volumes_uc1a = self . driver . list_volumes ( ) \n 
self . assertEqual ( len ( volumes ) , 2 ) \n 
self . assertEqual ( len ( volumes_all ) , 10 ) \n 
self . assertEqual ( len ( volumes_uc1a ) , 2 ) \n 
self . assertEqual ( volumes [ 0 ] . name , ) \n 
self . assertEqual ( volumes_uc1a [ 0 ] . name , ) \n 
names = [ v . name for v in volumes_all ] \n 
~~ def test_ex_list_zones ( self ) : \n 
~~~ zones = self . driver . ex_list_zones ( ) \n 
self . assertEqual ( len ( zones ) , 6 ) \n 
self . assertEqual ( zones [ 0 ] . name , ) \n 
~~ def test_ex_create_address_global ( self ) : \n 
~~~ address_name = \n 
address = self . driver . ex_create_address ( address_name , ) \n 
self . assertTrue ( isinstance ( address , GCEAddress ) ) \n 
self . assertEqual ( address . name , address_name ) \n 
self . assertEqual ( address . region , ) \n 
~~ def test_ex_create_address ( self ) : \n 
address = self . driver . ex_create_address ( address_name ) \n 
~~ def test_ex_create_backendservice ( self ) : \n 
~~~ backendservice_name = \n 
backendservice = self . driver . ex_create_backendservice ( \n 
name = backendservice_name , \n 
healthchecks = [ ] ) \n 
self . assertTrue ( isinstance ( backendservice , GCEBackendService ) ) \n 
self . assertEqual ( backendservice . name , backendservice_name ) \n 
~~ def test_ex_create_healthcheck ( self ) : \n 
~~~ healthcheck_name = \n 
kwargs = { : , \n 
: 8000 , \n 
: 10 , \n 
: 4 , \n 
: 3 , \n 
: } \n 
hc = self . driver . ex_create_healthcheck ( healthcheck_name , ** kwargs ) \n 
self . assertTrue ( isinstance ( hc , GCEHealthCheck ) ) \n 
self . assertEqual ( hc . name , healthcheck_name ) \n 
self . assertEqual ( hc . path , ) \n 
self . assertEqual ( hc . port , 8000 ) \n 
self . assertEqual ( hc . interval , 10 ) \n 
self . assertEqual ( hc . extra [ ] , ) \n 
~~ def test_ex_create_image ( self ) : \n 
~~~ volume = self . driver . ex_get_volume ( ) \n 
image = self . driver . ex_create_image ( , volume ) \n 
self . assertTrue ( isinstance ( image , GCENodeImage ) ) \n 
self . assertTrue ( image . name . startswith ( ) ) \n 
self . assertEqual ( image . extra [ ] , ) \n 
~~ def test_ex_create_firewall ( self ) : \n 
~~~ firewall_name = \n 
allowed = [ { : , : [ ] } ] \n 
source_tags = [ ] \n 
firewall = self . driver . ex_create_firewall ( firewall_name , allowed , \n 
source_tags = source_tags ) \n 
self . assertTrue ( isinstance ( firewall , GCEFirewall ) ) \n 
self . assertEqual ( firewall . name , firewall_name ) \n 
~~ def test_ex_create_forwarding_rule ( self ) : \n 
~~~ fwr_name = \n 
targetpool = \n 
region = \n 
address = \n 
port_range = \n 
description = \n 
fwr = self . driver . ex_create_forwarding_rule ( fwr_name , targetpool , \n 
region = region , \n 
port_range = port_range , \n 
description = description ) \n 
self . assertTrue ( isinstance ( fwr , GCEForwardingRule ) ) \n 
self . assertEqual ( fwr . name , fwr_name ) \n 
self . assertEqual ( fwr . region . name , region ) \n 
self . assertEqual ( fwr . protocol , ) \n 
self . assertEqual ( fwr . extra [ ] , port_range ) \n 
self . assertEqual ( fwr . extra [ ] , description ) \n 
~~ def test_ex_create_forwarding_rule_global ( self ) : \n 
target_name = \n 
for target in ( target_name , \n 
self . driver . ex_get_targethttpproxy ( target_name ) ) : \n 
~~~ fwr = self . driver . ex_create_forwarding_rule ( fwr_name , target , \n 
global_rule = True , \n 
~~ ~~ def test_ex_create_forwarding_rule_targetpool_keyword ( self ) : \n 
fwr_name = \n 
address = self . driver . ex_get_address ( ) \n 
fwr = self . driver . ex_create_forwarding_rule ( fwr_name , \n 
targetpool = targetpool , \n 
~~ def test_ex_create_route ( self ) : \n 
~~~ route_name = \n 
dest_range = \n 
priority = 1000 \n 
route = self . driver . ex_create_route ( route_name , dest_range ) \n 
self . assertTrue ( isinstance ( route , GCERoute ) ) \n 
self . assertEqual ( route . name , route_name ) \n 
self . assertEqual ( route . priority , priority ) \n 
self . assertTrue ( "tag1" in route . tags ) \n 
self . assertTrue ( route . extra [ ] . endswith ( ) ) \n 
self . assertEqual ( route . dest_range , dest_range ) \n 
~~ def test_ex_create_network ( self ) : \n 
~~~ network_name = \n 
cidr = \n 
network = self . driver . ex_create_network ( network_name , cidr ) \n 
self . assertTrue ( isinstance ( network , GCENetwork ) ) \n 
self . assertEqual ( network . name , network_name ) \n 
self . assertEqual ( network . cidr , cidr ) \n 
~~ def test_ex_set_machine_type_notstopped ( self ) : \n 
~~~ zone = \n 
node = self . driver . ex_get_node ( , zone ) \n 
self . assertRaises ( GoogleBaseError , self . driver . ex_set_machine_type , \n 
node , ) \n 
~~ def test_ex_set_machine_type_invalid ( self ) : \n 
self . assertRaises ( InvalidRequestError , self . driver . ex_set_machine_type , \n 
~~ def test_ex_set_machine_type ( self ) : \n 
self . assertEqual ( node . size , ) \n 
self . assertEqual ( node . extra [ ] , ) \n 
self . assertTrue ( self . driver . ex_set_machine_type ( node , ) ) \n 
~~ def test_ex_node_start ( self ) : \n 
self . assertTrue ( self . driver . ex_start_node ( node ) ) \n 
~~ def test_ex_node_stop ( self ) : \n 
self . assertTrue ( self . driver . ex_stop_node ( node ) ) \n 
zone = \n 
~~ def test_create_node_req ( self ) : \n 
~~~ image = self . driver . ex_get_image ( ) \n 
size = self . driver . ex_get_size ( ) \n 
location = self . driver . zone \n 
network = self . driver . ex_get_network ( ) \n 
tags = [ ] \n 
metadata = [ { : , : } ] \n 
boot_disk = self . driver . ex_get_volume ( ) \n 
node_request , node_data = self . driver . _create_node_req ( , size , \n 
image , location , \n 
network , tags , \n 
metadata , \n 
boot_disk ) \n 
self . assertEqual ( node_request , % location . name ) \n 
self . assertEqual ( node_data [ ] [ ] [ 0 ] [ ] , ) \n 
self . assertEqual ( node_data [ ] [ ] [ 0 ] , ) \n 
self . assertEqual ( node_data [ ] , ) \n 
self . assertTrue ( node_data [ ] [ 0 ] [ ] ) \n 
self . assertIsInstance ( node_data [ ] , list ) \n 
self . assertIsInstance ( node_data [ ] [ 0 ] , dict ) \n 
self . assertEqual ( node_data [ ] [ 0 ] [ ] , ) \n 
self . assertIsInstance ( node_data [ ] [ 0 ] [ ] , list ) \n 
self . assertEqual ( len ( node_data [ ] [ 0 ] [ ] ) , 1 ) \n 
~~ def test_create_node_network_opts ( self ) : \n 
~~~ node_name = \n 
image = self . driver . ex_get_image ( ) \n 
zone = self . driver . ex_get_zone ( ) \n 
ex_nic_gce_struct = [ \n 
"network" : "global/networks/lcnetwork" , \n 
"accessConfigs" : [ \n 
"name" : "lcnetwork-test" , \n 
"type" : "ONE_TO_ONE_NAT" \n 
node = self . driver . create_node ( node_name , size , image ) \n 
self . assertEqual ( node . extra [ ] [ 0 ] [ "name" ] , ) \n 
node = self . driver . create_node ( node_name , size , image , location = zone , \n 
ex_network = network ) \n 
ex_nic_gce_struct = ex_nic_gce_struct ) \n 
self . assertRaises ( ValueError , self . driver . create_node , node_name , \n 
size , image , location = zone , external_ip = address , \n 
size , image , location = zone , ex_network = network , \n 
~~ def test_create_node_disk_opts ( self ) : \n 
disk_type = self . driver . ex_get_disktype ( , ) \n 
DEMO_BASE_NAME = "lc-test" \n 
gce_disk_struct = [ \n 
"type" : "PERSISTENT" , \n 
"deviceName" : % DEMO_BASE_NAME , \n 
"initializeParams" : { \n 
"diskName" : % DEMO_BASE_NAME , \n 
"sourceImage" : image . extra [ ] \n 
"boot" : True , \n 
"autoDelete" : True \n 
"type" : "SCRATCH" , \n 
"diskType" : disk_type . extra [ ] \n 
size , None ) \n 
self . assertTrue ( isinstance ( node , Node ) ) \n 
node = self . driver . create_node ( node_name , size , None , \n 
ex_boot_disk = boot_disk ) \n 
ex_disks_gce_struct = gce_disk_struct ) \n 
size , None , ex_boot_disk = boot_disk , \n 
~~ def test_create_node ( self ) : \n 
self . assertEqual ( node . name , node_name ) \n 
~~ def test_create_node_image_family ( self ) : \n 
node = self . driver . create_node ( node_name , size , image = None , \n 
ex_image_family = ) \n 
size , image , ex_image_family = ) \n 
~~ def test_create_node_req_with_serviceaccounts ( self ) : \n 
ex_sa = [ { : [ , , ] } ] \n 
network , \n 
ex_service_accounts = ex_sa ) \n 
self . assertEqual ( len ( node_data [ ] [ 0 ] [ ] ) , 3 ) \n 
in node_data [ ] [ 0 ] [ ] ) \n 
~~ def test_format_metadata ( self ) : \n 
~~~ in_md = [ { : , : } , { : , : } ] \n 
out_md = self . driver . _format_metadata ( , in_md ) \n 
self . assertTrue ( in out_md ) \n 
self . assertEqual ( out_md [ ] , ) \n 
self . assertEqual ( len ( out_md [ ] ) , 2 ) \n 
self . assertTrue ( out_md [ ] [ 0 ] [ ] in [ , ] ) \n 
in_md = [ { : } , { : } ] \n 
in_md = { : , : } \n 
self . assertEqual ( len ( out_md [ ] ) , 1 , out_md ) \n 
self . assertEqual ( out_md [ ] [ 0 ] [ ] , ) \n 
in_md = { : } \n 
self . assertEqual ( len ( out_md [ ] ) , 1 ) \n 
in_md = { : , : , : } \n 
self . assertEqual ( len ( out_md [ ] ) , 3 ) \n 
keys = [ x [ ] for x in out_md [ ] ] \n 
vals = [ x [ ] for x in out_md [ ] ] \n 
keys . sort ( ) \n 
vals . sort ( ) \n 
self . assertEqual ( keys , [ , , ] ) \n 
self . assertEqual ( vals , [ , , ] ) \n 
in_md = { : [ { : , : } , \n 
{ : , : } ] } \n 
self . assertRaises ( ValueError , self . driver . _format_metadata , , in_md ) \n 
in_md = { : { : , : } } \n 
in_md = [ , ] \n 
~~ def test_create_node_with_metadata ( self ) : \n 
md = [ { : , : } , { : , : } ] \n 
request , data = self . driver . _create_node_req ( node_name , size , image , \n 
zone , metadata = md ) \n 
self . assertTrue ( in data [ ] ) \n 
self . assertEqual ( len ( data [ ] [ ] ) , 2 ) \n 
md = { : , : } \n 
self . assertEqual ( len ( data [ ] [ ] ) , 1 ) \n 
md = { : [ { : , : } ] } \n 
self . assertEqual ( data [ ] [ ] [ 0 ] [ ] , ) \n 
~~ def test_create_node_existing ( self ) : \n 
size = self . driver . ex_get_size ( , zone = ) \n 
self . assertRaises ( ResourceExistsError , self . driver . create_node , \n 
node_name , size , image , location = ) \n 
~~ def test_ex_create_multiple_nodes ( self ) : \n 
~~~ base_name = \n 
number = 2 \n 
nodes = self . driver . ex_create_multiple_nodes ( base_name , size , image , \n 
number ) \n 
self . assertEqual ( len ( nodes ) , 2 ) \n 
self . assertTrue ( isinstance ( nodes [ 0 ] , Node ) ) \n 
self . assertTrue ( isinstance ( nodes [ 1 ] , Node ) ) \n 
self . assertEqual ( nodes [ 0 ] . name , % base_name ) \n 
self . assertEqual ( nodes [ 1 ] . name , % base_name ) \n 
~~ def test_ex_create_multiple_nodes_image_family ( self ) : \n 
image = None \n 
number , ex_image_family = ) \n 
self . assertRaises ( ValueError , self . driver . ex_create_multiple_nodes , \n 
base_name , size , image , number , ex_image_family = ) \n 
~~ def test_ex_create_targethttpproxy ( self ) : \n 
~~~ proxy_name = \n 
urlmap_name = \n 
for urlmap in ( urlmap_name , self . driver . ex_get_urlmap ( urlmap_name ) ) : \n 
~~~ proxy = self . driver . ex_create_targethttpproxy ( proxy_name , urlmap ) \n 
self . assertTrue ( isinstance ( proxy , GCETargetHttpProxy ) ) \n 
self . assertEqual ( proxy_name , proxy . name ) \n 
~~ ~~ def test_ex_create_targetinstance ( self ) : \n 
~~~ targetinstance_name = \n 
targetinstance = self . driver . ex_create_targetinstance ( \n 
targetinstance_name , zone = zone , node = node ) \n 
self . assertEqual ( targetinstance . name , targetinstance_name ) \n 
self . assertEqual ( targetinstance . zone . name , zone ) \n 
~~ def test_ex_create_targetpool ( self ) : \n 
~~~ targetpool_name = \n 
healthchecks = [ ] \n 
node1 = self . driver . ex_get_node ( , \n 
node2 = self . driver . ex_get_node ( , \n 
nodes = [ node1 , node2 ] \n 
targetpool = self . driver . ex_create_targetpool ( \n 
targetpool_name , region = region , healthchecks = healthchecks , \n 
nodes = nodes ) \n 
self . assertEqual ( targetpool . name , targetpool_name ) \n 
self . assertEqual ( len ( targetpool . nodes ) , len ( nodes ) ) \n 
self . assertEqual ( targetpool . region . name , region ) \n 
~~ def test_ex_create_targetpool_session_affinity ( self ) : \n 
session_affinity = \n 
targetpool_name , region = region , \n 
session_affinity = session_affinity ) \n 
self . assertEqual ( targetpool . extra . get ( ) , \n 
session_affinity ) \n 
~~ def test_ex_create_urlmap ( self ) : \n 
~~~ urlmap_name = \n 
for service in ( , \n 
self . driver . ex_get_backendservice ( ) ) : \n 
~~~ urlmap = self . driver . ex_create_urlmap ( urlmap_name , service ) \n 
self . assertTrue ( isinstance ( urlmap , GCEUrlMap ) ) \n 
self . assertEqual ( urlmap_name , urlmap . name ) \n 
~~ ~~ def test_create_volume_image_family ( self ) : \n 
~~~ volume_name = \n 
size = 10 \n 
volume = self . driver . create_volume ( size , volume_name , \n 
self . assertTrue ( isinstance ( volume , StorageVolume ) ) \n 
self . assertEqual ( volume . name , volume_name ) \n 
self . assertRaises ( ValueError , self . driver . create_volume , size , \n 
volume_name , image = image , \n 
~~ def test_ex_create_volume_snapshot ( self ) : \n 
~~~ snapshot_name = \n 
volume = self . driver . ex_get_volume ( ) \n 
snapshot = volume . snapshot ( snapshot_name ) \n 
self . assertEqual ( snapshot . name , snapshot_name ) \n 
self . assertEqual ( snapshot . size , ) \n 
~~ def test_create_volume_ssd ( self ) : \n 
ex_disk_type = ) \n 
self . assertEqual ( volume . extra [ ] , ) \n 
~~ def test_create_volume ( self ) : \n 
volume = self . driver . create_volume ( size , volume_name ) \n 
~~ def test_ex_update_healthcheck ( self ) : \n 
healthcheck = self . driver . ex_get_healthcheck ( healthcheck_name ) \n 
healthcheck . port = 9000 \n 
healthcheck2 = self . driver . ex_update_healthcheck ( healthcheck ) \n 
self . assertTrue ( isinstance ( healthcheck2 , GCEHealthCheck ) ) \n 
~~ def test_ex_update_firewall ( self ) : \n 
firewall = self . driver . ex_get_firewall ( firewall_name ) \n 
firewall . source_ranges = [ ] \n 
firewall . source_tags = [ , ] \n 
firewall2 = self . driver . ex_update_firewall ( firewall ) \n 
self . assertTrue ( isinstance ( firewall2 , GCEFirewall ) ) \n 
~~ def test_ex_targetpool_gethealth ( self ) : \n 
~~~ targetpool = self . driver . ex_get_targetpool ( ) \n 
health = targetpool . get_health ( ) \n 
self . assertEqual ( len ( health ) , 1 ) \n 
self . assertTrue ( in health [ 0 ] ) \n 
self . assertEqual ( health [ 0 ] [ ] , ) \n 
~~ def test_ex_targetpool_with_backup_pool ( self ) : \n 
self . assertTrue ( in targetpool . extra ) \n 
~~ def test_ex_targetpool_setbackup ( self ) : \n 
backup_targetpool = self . driver . ex_get_targetpool ( ) \n 
self . assertTrue ( targetpool . set_backup_targetpool ( backup_targetpool , \n 
0.1 ) ) \n 
~~ def test_ex_targetpool_remove_add_node ( self ) : \n 
node = self . driver . ex_get_node ( , \n 
remove_node = self . driver . ex_targetpool_remove_node ( targetpool , node ) \n 
self . assertTrue ( remove_node ) \n 
self . assertEqual ( len ( targetpool . nodes ) , 1 ) \n 
add_node = self . driver . ex_targetpool_add_node ( targetpool , node . extra [ ] ) \n 
self . assertTrue ( add_node ) \n 
self . assertEqual ( len ( targetpool . nodes ) , 2 ) \n 
remove_node = self . driver . ex_targetpool_remove_node ( targetpool , node . extra [ ] ) \n 
add_node = self . driver . ex_targetpool_add_node ( targetpool , node ) \n 
~~ def test_ex_targetpool_remove_add_healthcheck ( self ) : \n 
healthcheck = self . driver . ex_get_healthcheck ( \n 
remove_healthcheck = self . driver . ex_targetpool_remove_healthcheck ( \n 
targetpool , healthcheck ) \n 
self . assertTrue ( remove_healthcheck ) \n 
self . assertEqual ( len ( targetpool . healthchecks ) , 0 ) \n 
add_healthcheck = self . driver . ex_targetpool_add_healthcheck ( \n 
self . assertTrue ( add_healthcheck ) \n 
self . assertEqual ( len ( targetpool . healthchecks ) , 1 ) \n 
~~ def test_reboot_node ( self ) : \n 
~~~ node = self . driver . ex_get_node ( ) \n 
reboot = self . driver . reboot_node ( node ) \n 
self . assertTrue ( reboot ) \n 
~~ def test_ex_set_node_tags ( self ) : \n 
~~~ new_tags = [ ] \n 
node = self . driver . ex_get_node ( ) \n 
set_tags = self . driver . ex_set_node_tags ( node , new_tags ) \n 
self . assertTrue ( set_tags ) \n 
~~ def test_attach_volume_invalid_usecase ( self ) : \n 
self . assertRaises ( ValueError , self . driver . attach_volume , node , None ) \n 
self . assertRaises ( ValueError , self . driver . attach_volume , node , None , \n 
ex_source = , device = None ) \n 
~~ def test_attach_volume ( self ) : \n 
attach = volume . attach ( node ) \n 
self . assertTrue ( attach ) \n 
~~ def test_detach_volume ( self ) : \n 
detach = volume . detach ( ) \n 
self . assertFalse ( detach ) \n 
detach = self . driver . detach_volume ( volume , node ) \n 
self . assertTrue ( detach ) \n 
~~ def test_ex_destroy_address_global ( self ) : \n 
~~~ address = self . driver . ex_get_address ( , ) \n 
self . assertEqual ( address . name , ) \n 
destroyed = address . destroy ( ) \n 
self . assertTrue ( destroyed ) \n 
~~ def test_ex_destroy_address ( self ) : \n 
~~~ address = self . driver . ex_get_address ( ) \n 
~~ def test_ex_destroy_backendservice ( self ) : \n 
~~~ backendservice = self . driver . ex_get_backendservice ( ) \n 
destroyed = backendservice . destroy ( ) \n 
~~ def test_ex_destroy_healthcheck ( self ) : \n 
~~~ hc = self . driver . ex_get_healthcheck ( ) \n 
destroyed = hc . destroy ( ) \n 
~~ def test_ex_delete_image ( self ) : \n 
~~~ self . assertRaises ( ResourceNotFoundError , \n 
self . driver . ex_get_image , ) \n 
self . assertRaises ( ResourceNotFoundError , \n 
self . driver . ex_delete_image , ) \n 
deleted = self . driver . ex_delete_image ( image ) \n 
self . assertTrue ( deleted ) \n 
~~ def test_ex_deprecate_image ( self ) : \n 
~~~ dep_ts = \n 
obs_ts = \n 
del_ts = \n 
deprecated = image . deprecate ( , , \n 
deprecated = dep_ts , \n 
obsolete = obs_ts , \n 
deleted = del_ts ) \n 
self . assertTrue ( deprecated ) \n 
self . assertEqual ( image . extra [ ] [ ] , dep_ts ) \n 
self . assertEqual ( image . extra [ ] [ ] , obs_ts ) \n 
self . assertEqual ( image . extra [ ] [ ] , del_ts ) \n 
~~ def test_ex_destroy_firewall ( self ) : \n 
~~~ firewall = self . driver . ex_get_firewall ( ) \n 
destroyed = firewall . destroy ( ) \n 
~~ def test_ex_destroy_forwarding_rule ( self ) : \n 
~~~ fwr = self . driver . ex_get_forwarding_rule ( ) \n 
destroyed = fwr . destroy ( ) \n 
~~ def test_ex_destroy_forwarding_rule_global ( self ) : \n 
~~~ fwr = self . driver . ex_get_forwarding_rule ( , global_rule = True ) \n 
~~ def test_ex_destroy_route ( self ) : \n 
~~~ route = self . driver . ex_get_route ( ) \n 
destroyed = route . destroy ( ) \n 
~~ def test_ex_destroy_network ( self ) : \n 
~~~ network = self . driver . ex_get_network ( ) \n 
destroyed = network . destroy ( ) \n 
~~ def test_destroy_node ( self ) : \n 
destroyed = node . destroy ( ) \n 
~~ def test_ex_destroy_multiple_nodes ( self ) : \n 
~~~ nodes = [ ] \n 
nodes . append ( self . driver . ex_get_node ( ) ) \n 
destroyed = self . driver . ex_destroy_multiple_nodes ( nodes ) \n 
for d in destroyed : \n 
~~~ self . assertTrue ( d ) \n 
~~ ~~ def test_destroy_targethttpproxy ( self ) : \n 
~~~ proxy = self . driver . ex_get_targethttpproxy ( ) \n 
destroyed = proxy . destroy ( ) \n 
~~ def test_destroy_targetinstance ( self ) : \n 
~~~ targetinstance = self . driver . ex_get_targetinstance ( ) \n 
self . assertEqual ( targetinstance . name , ) \n 
destroyed = targetinstance . destroy ( ) \n 
~~ def test_destroy_targetpool ( self ) : \n 
destroyed = targetpool . destroy ( ) \n 
~~ def test_destroy_urlmap ( self ) : \n 
~~~ urlmap = self . driver . ex_get_urlmap ( ) \n 
destroyed = urlmap . destroy ( ) \n 
~~ def test_destroy_volume ( self ) : \n 
~~~ disk = self . driver . ex_get_volume ( ) \n 
destroyed = disk . destroy ( ) \n 
~~ def test_ex_set_volume_auto_delete ( self ) : \n 
volume = node . extra [ ] \n 
auto_delete = self . driver . ex_set_volume_auto_delete ( \n 
volume , node ) \n 
self . assertTrue ( auto_delete ) \n 
~~ def test_destroy_volume_snapshot ( self ) : \n 
~~~ snapshot = self . driver . ex_get_snapshot ( ) \n 
destroyed = snapshot . destroy ( ) \n 
~~ def test_ex_get_address_global ( self ) : \n 
address = self . driver . ex_get_address ( address_name , ) \n 
self . assertEqual ( address . address , ) \n 
self . assertEqual ( address . extra [ ] , ) \n 
~~ def test_ex_get_address ( self ) : \n 
address = self . driver . ex_get_address ( address_name ) \n 
self . assertEqual ( address . region . name , ) \n 
~~ def test_ex_get_backendservice ( self ) : \n 
~~~ web_service = self . driver . ex_get_backendservice ( ) \n 
self . assertEqual ( web_service . protocol , ) \n 
self . assertEqual ( web_service . port , 80 ) \n 
self . assertEqual ( web_service . timeout , 30 ) \n 
self . assertEqual ( web_service . healthchecks [ 0 ] . name , ) \n 
backends = web_service . backends \n 
self . assertEqual ( len ( backends ) , 2 ) \n 
self . assertEqual ( backends [ 0 ] [ ] , ) \n 
self . assertEqual ( backends [ 0 ] [ ] , 100 ) \n 
self . assertEqual ( backends [ 0 ] [ ] , 1.0 ) \n 
web_service = self . driver . ex_get_backendservice ( ) \n 
self . assertEqual ( len ( web_service . backends ) , 0 ) \n 
~~ def test_ex_get_healthcheck ( self ) : \n 
self . assertEqual ( healthcheck . name , healthcheck_name ) \n 
self . assertEqual ( healthcheck . port , 8000 ) \n 
self . assertEqual ( healthcheck . path , ) \n 
~~ def test_ex_get_firewall ( self ) : \n 
self . assertEqual ( firewall . network . name , ) \n 
self . assertEqual ( firewall . source_tags , [ ] ) \n 
~~ def test_ex_get_forwarding_rule ( self ) : \n 
fwr = self . driver . ex_get_forwarding_rule ( fwr_name ) \n 
self . assertEqual ( fwr . extra [ ] , ) \n 
self . assertEqual ( fwr . targetpool . name , ) \n 
~~ def test_ex_get_forwarding_rule_global ( self ) : \n 
fwr = self . driver . ex_get_forwarding_rule ( fwr_name , global_rule = True ) \n 
self . assertEqual ( fwr . address , ) \n 
~~ def test_ex_get_image_license ( self ) : \n 
self . assertTrue ( in image . extra ) \n 
self . assertEqual ( image . extra [ ] [ 0 ] . name , ) \n 
self . assertTrue ( image . extra [ ] [ 0 ] . charges_use_fee ) \n 
~~ def test_ex_get_image ( self ) : \n 
~~~ partial_name = \n 
image = self . driver . ex_get_image ( partial_name ) \n 
self . assertTrue ( image . extra [ ] . startswith ( ) ) \n 
partial_name = \n 
image = self . driver . ex_get_image ( partial_name , [ ] ) \n 
self . assertRaises ( ResourceNotFoundError , self . driver . ex_get_image , \n 
partial_name , , \n 
ex_standard_projects = False ) \n 
~~ def test_ex_get_image_from_family ( self ) : \n 
~~~ family = \n 
image = self . driver . ex_get_image_from_family ( family ) \n 
self . assertEqual ( image . extra [ ] , description ) \n 
self . assertEqual ( image . extra [ ] , family ) \n 
url = ( \n 
image = self . driver . ex_get_image_from_family ( url ) \n 
project_list = [ ] \n 
image = self . driver . ex_get_image_from_family ( family , ex_project_list = project_list , ex_standard_projects self . assertEqual ( image . name , ) \n 
self . assertRaises ( ResourceNotFoundError , self . driver . ex_get_image_from_family , ) \n 
~~ def test_ex_copy_image ( self ) : \n 
family = \n 
image = self . driver . ex_copy_image ( name , url , description = description , \n 
family = family ) \n 
self . assertTrue ( image . name . startswith ( name ) ) \n 
~~ def test_ex_get_route ( self ) : \n 
route = self . driver . ex_get_route ( route_name ) \n 
self . assertEqual ( route . dest_range , ) \n 
self . assertEqual ( route . priority , 1000 ) \n 
~~ def test_ex_get_network ( self ) : \n 
network = self . driver . ex_get_network ( network_name ) \n 
self . assertEqual ( network . cidr , ) \n 
self . assertEqual ( network . extra [ ] , ) \n 
~~ def test_ex_get_node ( self ) : \n 
node = self . driver . ex_get_node ( node_name , zone ) \n 
removed_node = \n 
self . assertRaises ( ResourceNotFoundError , self . driver . ex_get_node , \n 
removed_node , ) \n 
missing_node = \n 
missing_node , ) \n 
~~ def test_ex_get_project ( self ) : \n 
~~~ project = self . driver . ex_get_project ( ) \n 
self . assertEqual ( project . name , ) \n 
networks_quota = project . quotas [ 1 ] \n 
self . assertEqual ( networks_quota [ ] , 3 ) \n 
self . assertEqual ( networks_quota [ ] , 5 ) \n 
self . assertEqual ( networks_quota [ ] , ) \n 
self . assertTrue ( in project . extra [ ] ) \n 
self . assertTrue ( in project . extra ) \n 
self . assertEqual ( project . extra [ ] [ ] , \n 
~~ def test_ex_add_access_config ( self ) : \n 
~~~ self . assertRaises ( ValueError , self . driver . ex_add_access_config , \n 
self . assertTrue ( self . driver . ex_add_access_config ( node , , ) ) \n 
~~ def test_ex_delete_access_config ( self ) : \n 
self . assertTrue ( self . driver . ex_delete_access_config ( node , , ) ) \n 
~~ def test_ex_set_usage_export_bucket ( self ) : \n 
~~~ self . assertRaises ( ValueError , \n 
self . driver . ex_set_usage_export_bucket , ) \n 
bucket_name = \n 
self . driver . ex_set_usage_export_bucket ( bucket_name ) \n 
~~ def test__set_project_metadata ( self ) : \n 
~~~ self . assertEqual ( len ( self . driver . _set_project_metadata ( None , False , "" ) ) , 0 ) \n 
self . assertEqual ( len ( md ) , 1 ) \n 
self . assertEqual ( md [ 0 ] [ ] , ) \n 
self . assertEqual ( len ( md ) , 0 ) \n 
gce_md = { : [ { : , : } , \n 
self . assertEqual ( len ( md ) , 2 , str ( md ) ) \n 
sshKeys = "" \n 
for d in md : \n 
~~~ if d [ ] == : \n 
sshKeys = d [ ] \n 
~~ ~~ self . assertEqual ( sshKeys , ) \n 
self . assertEqual ( count , 1 ) \n 
self . assertEqual ( count , 0 ) \n 
~~ def test_ex_set_common_instance_metadata ( self ) : \n 
self . driver . ex_set_common_instance_metadata , \n 
[ , ] ) \n 
pydict = { : , : 1 } \n 
self . driver . ex_set_common_instance_metadata ( pydict ) \n 
bad_gcedict = { : } \n 
self . assertRaises ( ValueError , \n 
bad_gcedict ) \n 
gcedict = { : [ { : , : } , \n 
self . driver . ex_set_common_instance_metadata ( gcedict ) \n 
~~ def test_ex_set_node_metadata ( self ) : \n 
~~~ node = self . driver . ex_get_node ( , ) \n 
self . assertRaises ( ValueError , self . driver . ex_set_node_metadata , \n 
node , [ , ] ) \n 
self . driver . ex_set_node_metadata ( node , pydict ) \n 
node , bad_gcedict ) \n 
self . driver . ex_set_node_metadata ( node , gcedict ) \n 
~~ def test_ex_get_region ( self ) : \n 
~~~ region_name = \n 
region = self . driver . ex_get_region ( region_name ) \n 
self . assertEqual ( region . name , region_name ) \n 
self . assertEqual ( region . status , ) \n 
self . assertEqual ( region . zones [ 0 ] . name , ) \n 
~~ def test_ex_get_size ( self ) : \n 
~~~ size_name = \n 
size = self . driver . ex_get_size ( size_name ) \n 
self . assertEqual ( size . name , size_name ) \n 
self . assertEqual ( size . extra [ ] . name , ) \n 
self . assertEqual ( size . disk , 10 ) \n 
self . assertEqual ( size . ram , 3840 ) \n 
self . assertEqual ( size . extra [ ] , 1 ) \n 
~~ def test_ex_get_targethttpproxy ( self ) : \n 
~~~ targethttpproxy_name = \n 
targethttpproxy = self . driver . ex_get_targethttpproxy ( \n 
targethttpproxy_name ) \n 
self . assertEqual ( targethttpproxy . name , targethttpproxy_name ) \n 
self . assertEqual ( targethttpproxy . urlmap . name , ) \n 
~~ def test_ex_get_targetinstance ( self ) : \n 
targetinstance = self . driver . ex_get_targetinstance ( targetinstance_name ) \n 
self . assertEqual ( targetinstance . zone . name , ) \n 
~~ def test_ex_get_targetpool ( self ) : \n 
targetpool = self . driver . ex_get_targetpool ( targetpool_name ) \n 
self . assertEqual ( targetpool . region . name , ) \n 
~~ def test_ex_get_snapshot ( self ) : \n 
snapshot = self . driver . ex_get_snapshot ( snapshot_name ) \n 
self . assertEqual ( snapshot . status , ) \n 
~~ def test_ex_get_urlmap ( self ) : \n 
urlmap = self . driver . ex_get_urlmap ( urlmap_name ) \n 
self . assertEqual ( urlmap . name , urlmap_name ) \n 
self . assertEqual ( urlmap . default_service . name , ) \n 
~~ def test_ex_get_volume ( self ) : \n 
volume = self . driver . ex_get_volume ( volume_name ) \n 
self . assertEqual ( volume . size , ) \n 
~~ def test_ex_get_disktype ( self ) : \n 
~~~ disktype_name = \n 
disktype_zone = \n 
disktype = self . driver . ex_get_disktype ( disktype_name , disktype_zone ) \n 
self . assertEqual ( disktype . name , disktype_name ) \n 
self . assertEqual ( disktype . zone . name , disktype_zone ) \n 
self . assertEqual ( disktype . extra [ ] , ) \n 
~~ def test_ex_get_zone ( self ) : \n 
~~~ zone_name = \n 
zone = self . driver . ex_get_zone ( zone_name ) \n 
self . assertEqual ( zone . name , zone_name ) \n 
self . assertFalse ( zone . time_until_mw ) \n 
self . assertFalse ( zone . next_mw_duration ) \n 
zone_no_mw = self . driver . ex_get_zone ( ) \n 
self . assertEqual ( zone_no_mw . time_until_mw , None ) \n 
~~ ~~ class GCEMockHttp ( MockHttpTestCase ) : \n 
~~~ fixtures = ComputeFileFixtures ( ) \n 
json_hdr = { : } \n 
def _get_method_name ( self , type , use_param , qs , path ) : \n 
~~~ api_path = % API_VERSION \n 
project_path = % GCE_KEYWORD_PARAMS [ ] \n 
path = path . replace ( api_path , ) \n 
path = path . replace ( project_path , ) \n 
~~~ path = \n 
~~ method_name = super ( GCEMockHttp , self ) . _get_method_name ( type , \n 
use_param , \n 
qs , path ) \n 
return method_name \n 
~~ def _setUsageExportBucket ( self , method , url , body , headers ) : \n 
~~~ if method == : \n 
~~ return ( httplib . OK , body , self . json_hdr , httplib . responses [ httplib . OK ] ) \n 
~~ def _zones_us_central1_a_instances_custom_node ( self , method , url , body , header ) : \n 
return ( httplib . OK , body , self . json_hdr , httplib . responses [ httplib . OK ] ) \n 
~~ def _zones_us_central1_a_instances_node_name_setMachineType ( self , method , url , body , header ) : \n 
~~ def _zones_us_central1_a_operations_operation_setMachineType_notstopped ( self , method , url , body , ~~~ body = self . fixtures . load ( return ( httplib . OK , body , self . json_hdr , httplib . responses [ httplib . OK ] ) \n 
~~ def _zones_us_central1_a_instances_custom_node_setMachineType ( self , method , url , body , header ) : \n 
~~~ body = { \n 
"error" : { \n 
"errors" : [ \n 
"domain" : "global" , \n 
"reason" : "invalid" , \n 
"code" : 400 , \n 
return ( httplib . BAD_REQUEST , body , self . json_hdr , httplib . responses [ httplib . BAD_REQUEST ] ) \n 
~~ def _zones_us_central1_a_instances_stopped_node_setMachineType ( self , method , url , body , header ) : ~~~ body = self . fixtures . load ( ) \n 
~~ def _zones_us_central1_a_operations_operation_setMachineType ( self , method , url , body , header ) : \n 
~~ def _zones_us_central1_a_operations_operation_startnode ( self , method , url , body , header ) : \n 
~~ def _zones_us_central1_a_instances_stopped_node_start ( self , method , url , body , header ) : \n 
~~ def _zones_us_central1_a_instances_stopped_node_stop ( self , method , url , body , header ) : \n 
~~ def _zones_us_central1_a_instances_stopped_node ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_operations_operation_stopnode ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_instances_node_name_stop ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_instances_node_name_setMetadata ( self , method , url , body , headers ) : \n 
~~ def _setCommonInstanceMetadata ( self , method , url , body , headers ) : \n 
~~ def _aggregated_addresses ( self , method , url , body , headers ) : \n 
~~ def _aggregated_diskTypes ( self , method , url , body , headers ) : \n 
~~ def _aggregated_disks ( self , method , url , body , headers ) : \n 
~~ def _aggregated_forwardingRules ( self , method , url , body , headers ) : \n 
~~ def _aggregated_instances ( self , method , url , body , headers ) : \n 
~~ def _aggregated_machineTypes ( self , method , url , body , headers ) : \n 
~~ def _aggregated_targetInstances ( self , method , url , body , headers ) : \n 
~~ def _aggregated_targetPools ( self , method , url , body , headers ) : \n 
~~ def _global_backendServices ( self , method , url , body , headers ) : \n 
~~~ backend_name = getattr ( self . test , , \n 
body = self . fixtures . load ( % \n 
backend_name ) \n 
~~ def _global_backendServices_no_backends ( self , method , url , body , headers ) : \n 
~~ def _global_backendServices_web_service ( self , method , url , body , headers ) : \n 
~~ def _global_forwardingRules ( self , method , url , body , headers ) : \n 
~~ def _global_forwardingRules_http_rule ( self , method , url , body , headers ) : \n 
~~ def _global_httpHealthChecks ( self , method , url , body , headers ) : \n 
~~ def _global_httpHealthChecks_default_health_check ( self , method , url , body , headers ) : \n 
~~ def _global_httpHealthChecks_basic_check ( self , method , url , body , headers ) : \n 
~~ def _global_httpHealthChecks_libcloud_lb_demo_healthcheck ( \n 
self , method , url , body , headers ) : \n 
~~ def _global_httpHealthChecks_lchealthcheck ( self , method , url , body , \n 
headers ) : \n 
~~ elif method == : \n 
~~ def _global_firewalls ( self , method , url , body , headers ) : \n 
~~ def _global_firewalls_lcfirewall ( self , method , url , body , headers ) : \n 
~~ def _global_images ( self , method , url , body , headers ) : \n 
~~ def _global_images_debian_7_wheezy_v20131120 ( \n 
~~ def _global_images_debian_7_wheezy_v20131014_deprecate ( \n 
~~ def _global_images_family_coreos ( self , method , url , body , headers ) : \n 
return ( httplib . NOT_FOUND , body , self . json_hdr , \n 
httplib . responses [ httplib . NOT_FOUND ] ) \n 
~~ def _global_images_family_nofamily ( self , method , url , body , headers ) : \n 
~~ def _global_routes ( self , method , url , body , headers ) : \n 
~~ def _global_networks ( self , method , url , body , headers ) : \n 
~~ def _global_networks_default ( self , method , url , body , headers ) : \n 
~~ def _global_networks_libcloud_demo_network ( self , method , url , body , \n 
~~ def _global_networks_libcloud_demo_europe_network ( self , method , url , body , \n 
~~ def _global_routes_lcdemoroute ( self , method , url , body , headers ) : \n 
~~ def _global_networks_lcnetwork ( self , method , url , body , headers ) : \n 
~~ def _global_snapshots ( self , method , url , body , headers ) : \n 
~~ def _global_snapshots_lcsnapshot ( self , method , url , body , headers ) : \n 
~~ def _global_operations_operation_setUsageExportBucket ( \n 
~~ def _global_operations_operation_setCommonInstanceMetadata ( \n 
~~ def _global_operations_operation_global_backendServices_post ( \n 
~~ def _global_operations_operation_global_backendServices_web_service_delete ( \n 
~~ def _global_operations_operation_global_forwardingRules_http_rule_delete ( \n 
~~ def _global_operations_operation_global_forwardingRules_post ( \n 
~~ def _global_operations_operation_global_httpHealthChecks_lchealthcheck_delete ( \n 
~~ def _global_operations_operation_global_images_debian7_delete ( \n 
~~ def _global_operations_operation_global_httpHealthChecks_lchealthcheck_put ( \n 
~~ def _global_operations_operation_global_httpHealthChecks_post ( \n 
~~ def _global_operations_operation_global_firewalls_lcfirewall_delete ( \n 
~~ def _global_operations_operation_global_firewalls_lcfirewall_put ( \n 
~~ def _global_operations_operation_global_firewalls_post ( \n 
~~ def _global_operations_operation_global_routes_lcdemoroute_delete ( \n 
~~ def _global_operations_operation_global_networks_lcnetwork_delete ( \n 
~~ def _global_operations_operation_global_routes_post ( \n 
~~ def _global_operations_operation_global_networks_post ( \n 
~~ def _global_operations_operation_global_snapshots_lcsnapshot_delete ( \n 
~~ def _global_operations_operation_global_image_post ( \n 
~~ def _global_operations_operation_global_addresses_lcaddressglobal_delete ( \n 
~~ def _global_operations_operation_global_targetHttpProxies_post ( \n 
~~ def _global_operations_operation_global_targetHttpProxies_web_proxy_delete ( \n 
~~ def _global_operations_operation_global_urlMaps_post ( \n 
~~ def _global_operations_operation_global_urlMaps_web_map_delete ( \n 
~~ def _global_targetHttpProxies ( self , method , url , body , headers ) : \n 
~~ def _global_targetHttpProxies_web_proxy ( self , method , url , body , headers ) : \n 
~~ def _global_urlMaps ( self , method , url , body , headers ) : \n 
~~ def _global_urlMaps_web_map ( self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_addresses_lcaddress_delete ( \n 
~~ def _global_operations_operation_global_addresses_post ( \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_addresses_post ( \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_forwardingRules_post ( \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_forwardingRules_lcforwardingrule_delete self , method , url , body , headers ) : \n 
) return ( httplib . OK , body , self . json_hdr , httplib . responses [ httplib . OK ] ) \n 
~~ def _zones_us_central1_a_instances_node_name_deleteAccessConfig ( self , method , url , body , headers ~~~ body = self . fixtures . load ( \n 
~~ def _zones_us_central1_a_instances_node_name_serialPort ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_instances_node_name_addAccessConfig ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_operations_operation_setMetadata_post ( self , method , url , body , headers ) ~~~ body = self . fixtures . load ( \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_targetInstances_post ( \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_targetPools_post ( \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_node_name_addAccessConfig_post self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_node_name_deleteAccessConfig_post self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_targetInstances_lctargetinstance_delete self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_targetPools_lctargetpool_delete self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_targetPools_lctargetpool_removeHealthCheck_post self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_targetPools_lctargetpool_addHealthCheck_post self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_targetPools_lctargetpool_removeInstance_post self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_targetPools_lb_pool_setBackup_post self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_operations_operation_regions_us_central1_targetPools_lctargetpool_addInstance_post self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_disks_lcdisk_delete ( \n 
~~ def _zones_us_central1_a_instances_node_name_setDiskAutoDelete ( \n 
~~ def _zones_us_central1_a_operations_operation_volume_auto_delete ( \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_disks_lcdisk_createSnapshot_post self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_disks_post ( \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_lcnode_000_delete ( \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_lcnode_001_delete ( \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_node_name_delete ( \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_node_name_attachDisk_post self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_node_name_detachDisk_post self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_node_name_setTags_post self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_node_name_reset_post self , method , url , body , headers ) : \n 
~~ def _zones_europe_west1_a_operations_operation_zones_europe_west1_a_instances_post ( \n 
~~ def _zones_us_central1_a_operations_operation_zones_us_central1_a_instances_post ( \n 
~~ def _project ( self , method , url , body , headers ) : \n 
~~ def _projects_windows_cloud_global_licenses_windows_server_2008_r2_dc ( self , method , url , body , headers ~~~ body = self . fixtures . load ( return ( httplib . OK , body , self . json_hdr , httplib . responses [ httplib . OK ] ) \n 
~~ def _projects_suse_cloud_global_licenses_sles_11 ( self , method , url , body , headers ) : \n 
~~ def _projects_rhel_cloud_global_licenses_rhel_7_server ( self , method , url , body , headers ) : \n 
~~ def _projects_suse_cloud_global_licenses_sles_12 ( self , method , url , body , headers ) : \n 
~~ def _projects_windows_cloud_global_images ( self , method , url , body , header ) : \n 
~~ def _projects_rhel_cloud_global_images ( self , method , url , body , header ) : \n 
~~ def _projects_gce_nvme_global_images ( self , method , url , body , header ) : \n 
~~ def _projects_coreos_cloud_global_images ( self , method , url , body , header ) : \n 
~~ def _projects_coreos_cloud_global_images_family_coreos ( \n 
self , method , url , body , header ) : \n 
~~ def _projects_opensuse_cloud_global_images ( self , method , url , body , header ) : \n 
~~ def _projects_google_containers_global_images ( self , method , url , body , header ) : \n 
~~ def _projects_ubuntu_os_cloud_global_images ( self , method , url , body , header ) : \n 
~~ def _projects_centos_cloud_global_images ( self , method , url , body , header ) : \n 
~~ def _projects_suse_cloud_global_images ( self , method , url , body , headers ) : \n 
~~ def _projects_debian_cloud_global_images ( self , method , url , body , headers ) : \n 
~~ def _regions ( self , method , url , body , headers ) : \n 
~~~ if in url or in url : \n 
~~ elif in url : \n 
~~ def _global_addresses ( self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_addresses ( self , method , url , body , headers ) : \n 
~~ def _global_addresses_lcaddressglobal ( self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_addresses_lcaddress ( self , method , url , body , \n 
~~ def _regions_us_central1_forwardingRules ( self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_forwardingRules_libcloud_lb_demo_lb ( \n 
~~ def _regions_us_central1_forwardingRules_lcforwardingrule ( \n 
~~ def _zones_us_central1_a_targetInstances ( self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_targetPools ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_targetInstances_lctargetinstance ( self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_targetPools_lb_pool_getHealth ( self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_targetPools_lb_pool ( self , method , url , body , headers ) : \n 
~~ def _regions_us_central1_targetPools_lctargetpool ( self , method , url , \n 
body , headers ) : \n 
~~ def _regions_us_central1_targetPools_lctargetpool_sticky ( self , method , url , \n 
~~ def _regions_us_central1_targetPools_backup_pool ( \n 
~~ def _regions_us_central1_targetPools_libcloud_lb_demo_lb_tp ( \n 
~~ def _regions_us_central1_targetPools_lctargetpool_removeHealthCheck ( \n 
~~ def _regions_us_central1_targetPools_lctargetpool_addHealthCheck ( \n 
~~ def _regions_us_central1_targetPools_lctargetpool_removeInstance ( \n 
~~ def _regions_us_central1_targetPools_lb_pool_setBackup ( \n 
~~ def _regions_us_central1_targetPools_lctargetpool_addInstance ( \n 
~~ def _zones ( self , method , url , body , headers ) : \n 
~~ def _zones_asia_east_1a ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_diskTypes ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_diskTypes_pd_standard ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_diskTypes_pd_ssd ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_disks ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_disks_lcdisk ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_disks_lcdisk_createSnapshot ( self , method , url , \n 
~~ def _zones_us_central1_a_disks_node_name ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_disks_lcnode_000 ( \n 
~~ def _zones_us_central1_a_disks_lcnode_001 ( \n 
~~ def _zones_us_central1_b_disks_libcloud_lb_demo_www_000 ( \n 
~~ def _zones_us_central1_b_disks_libcloud_lb_demo_www_001 ( \n 
~~ def _zones_us_central1_b_disks_libcloud_lb_demo_www_002 ( \n 
~~ def _zones_us_central2_a_disks_libcloud_demo_boot_disk ( \n 
~~ def _zones_us_central2_a_disks_libcloud_demo_np_node ( \n 
~~ def _zones_us_central2_a_disks_libcloud_demo_multiple_nodes_000 ( \n 
~~ def _zones_us_central2_a_disks_libcloud_demo_multiple_nodes_001 ( \n 
~~ def _zones_europe_west1_a_disks ( self , method , url , body , headers ) : \n 
~~ def _zones_europe_west1_a_disks_libcloud_demo_europe_np_node ( \n 
~~ def _zones_europe_west1_a_disks_libcloud_demo_europe_boot_disk ( \n 
~~ def _zones_europe_west1_a_disks_libcloud_demo_europe_multiple_nodes_000 ( \n 
~~ def _zones_europe_west1_a_instances ( self , method , url , body , headers ) : \n 
~~ def _zones_europe_west1_a_diskTypes_pd_standard ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_instances ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_instances_node_name ( self , method , url , body , \n 
~~ def _zones_us_central1_a_instances_node_name_attachDisk ( \n 
~~ def _zones_us_central1_a_instances_node_name_detachDisk ( \n 
~~ def _zones_us_central1_a_instances_node_name_setTags ( \n 
~~ def _zones_us_central1_a_instances_node_name_reset ( \n 
~~ def _zones_us_central1_a_instances_lcnode_000 ( self , method , url , body , \n 
~~ def _zones_us_central1_a_instances_lcnode_001 ( self , method , url , body , \n 
~~ def _zones_us_central1_b_instances_libcloud_lb_demo_www_000 ( \n 
~~ def _zones_us_central1_b_instances_libcloud_lb_demo_www_001 ( \n 
~~ def _zones_us_central1_b_instances_libcloud_lb_demo_www_002 ( \n 
~~ def _zones_us_central1_a ( self , method , url , body , headers ) : \n 
~~ def _zones_us_central1_a_machineTypes ( self , method , url , body , headers ) : \n 
~~ def _zones_europe_west1_a_machineTypes_n1_standard_1 ( self , method , url , \n 
~~ def _zones_us_central1_a_machineTypes_n1_standard_1 ( self , method , url , \n 
from libcloud . dns . types import RecordType , ZoneDoesNotExistError \n 
from libcloud . dns . types import RecordDoesNotExistError \n 
from libcloud . dns . drivers . route53 import Route53DNSDriver \n 
from libcloud . test import MockHttp \n 
from libcloud . test . file_fixtures import DNSFileFixtures \n 
from libcloud . test . secrets import DNS_PARAMS_ROUTE53 \n 
class Route53Tests ( unittest . TestCase ) : \n 
~~~ Route53DNSDriver . connectionCls . conn_classes = ( \n 
Route53MockHttp , Route53MockHttp ) \n 
Route53MockHttp . type = None \n 
self . driver = Route53DNSDriver ( * DNS_PARAMS_ROUTE53 ) \n 
~~ def test_list_record_types ( self ) : \n 
~~~ record_types = self . driver . list_record_types ( ) \n 
self . assertEqual ( len ( record_types ) , 10 ) \n 
self . assertTrue ( RecordType . A in record_types ) \n 
~~ def test_list_zones ( self ) : \n 
~~~ zones = self . driver . list_zones ( ) \n 
self . assertEqual ( len ( zones ) , 5 ) \n 
zone = zones [ 0 ] \n 
self . assertEqual ( zone . id , ) \n 
self . assertEqual ( zone . type , ) \n 
self . assertEqual ( zone . domain , ) \n 
~~ def test_list_records ( self ) : \n 
~~~ zone = self . driver . list_zones ( ) [ 0 ] \n 
records = self . driver . list_records ( zone = zone ) \n 
self . assertEqual ( len ( records ) , 10 ) \n 
record = records [ 1 ] \n 
self . assertEqual ( record . name , ) \n 
self . assertEqual ( record . id , ) \n 
self . assertEqual ( record . type , RecordType . A ) \n 
self . assertEqual ( record . data , ) \n 
self . assertEqual ( record . extra [ ] , 86400 ) \n 
record = records [ 3 ] \n 
self . assertEqual ( record . type , RecordType . MX ) \n 
self . assertEqual ( record . extra [ ] , 1 ) \n 
record = records [ 4 ] \n 
self . assertEqual ( record . extra [ ] , 5 ) \n 
record = records [ 8 ] \n 
self . assertEqual ( record . type , RecordType . SRV ) \n 
self . assertEqual ( record . extra [ ] , 10 ) \n 
self . assertEqual ( record . extra [ ] , 5269 ) \n 
~~ def test_get_zone ( self ) : \n 
~~~ zone = self . driver . get_zone ( zone_id = ) \n 
~~ def test_get_record ( self ) : \n 
~~~ record = self . driver . get_record ( zone_id = , \n 
record_id = ) \n 
self . assertEqual ( record . type , RecordType . CNAME ) \n 
~~ def test_list_records_zone_does_not_exist ( self ) : \n 
Route53MockHttp . type = \n 
~~~ self . driver . list_records ( zone = zone ) \n 
~~ except ZoneDoesNotExistError : \n 
self . assertEqual ( e . zone_id , zone . id ) \n 
~~~ self . fail ( ) \n 
~~ ~~ def test_get_zone_does_not_exist ( self ) : \n 
~~~ Route53MockHttp . type = \n 
~~~ self . driver . get_zone ( zone_id = ) \n 
self . assertEqual ( e . zone_id , ) \n 
~~ ~~ def test_get_record_zone_does_not_exist ( self ) : \n 
~~~ self . driver . get_record ( zone_id = , record_id = ) \n 
~~ ~~ def test_get_record_record_does_not_exist ( self ) : \n 
rid = \n 
~~~ self . driver . get_record ( zone_id = , \n 
record_id = rid ) \n 
~~ except RecordDoesNotExistError : \n 
~~ ~~ def test_create_zone ( self ) : \n 
~~~ zone = self . driver . create_zone ( domain = , type = , \n 
ttl = None , extra = None ) \n 
~~ def test_create_record ( self ) : \n 
record = self . driver . create_record ( \n 
name = , zone = zone , \n 
type = RecordType . A , data = , \n 
extra = { : 0 } \n 
self . assertEqual ( record . zone , zone ) \n 
~~ def test_create_record_zone_name ( self ) : \n 
~~ def test_create_multi_value_record ( self ) : \n 
records = self . driver . ex_create_multi_value_record ( \n 
self . assertEqual ( len ( records ) , 2 ) \n 
self . assertEqual ( records [ 0 ] . id , ) \n 
self . assertEqual ( records [ 1 ] . id , ) \n 
self . assertEqual ( records [ 0 ] . name , ) \n 
self . assertEqual ( records [ 1 ] . name , ) \n 
self . assertEqual ( records [ 0 ] . zone , zone ) \n 
self . assertEqual ( records [ 1 ] . zone , zone ) \n 
self . assertEqual ( records [ 0 ] . type , RecordType . A ) \n 
self . assertEqual ( records [ 1 ] . type , RecordType . A ) \n 
self . assertEqual ( records [ 0 ] . data , ) \n 
self . assertEqual ( records [ 1 ] . data , ) \n 
~~ def test_update_record ( self ) : \n 
record = self . driver . list_records ( zone = zone ) [ 1 ] \n 
: record , \n 
: RecordType . A , \n 
: { : 0 } } \n 
updated_record = self . driver . update_record ( ** params ) \n 
self . assertEqual ( updated_record . id , ) \n 
self . assertEqual ( updated_record . name , ) \n 
self . assertEqual ( updated_record . zone , record . zone ) \n 
self . assertEqual ( updated_record . type , RecordType . A ) \n 
self . assertEqual ( updated_record . data , ) \n 
~~ def test_delete_zone ( self ) : \n 
status = self . driver . delete_zone ( zone = zone ) \n 
self . assertTrue ( status ) \n 
~~ def test_delete_zone_does_not_exist ( self ) : \n 
~~~ self . driver . delete_zone ( zone = zone ) \n 
~~ ~~ def test_delete_record ( self ) : \n 
record = self . driver . list_records ( zone = zone ) [ 0 ] \n 
status = self . driver . delete_record ( record = record ) \n 
~~ def test_delete_record_does_not_exist ( self ) : \n 
~~~ self . driver . delete_record ( record = record ) \n 
self . assertEqual ( e . record_id , record . id ) \n 
~~ ~~ ~~ class Route53MockHttp ( MockHttp ) : \n 
~~~ fixtures = DNSFileFixtures ( ) \n 
def _2012_02_29_hostedzone_47234 ( self , method , url , body , headers ) : \n 
return ( httplib . OK , body , { } , httplib . responses [ httplib . OK ] ) \n 
~~ def _2012_02_29_hostedzone ( self , method , url , body , headers ) : \n 
~~~ if method == "POST" : \n 
~~~ body = self . fixtures . load ( "create_zone.xml" ) \n 
return ( httplib . CREATED , body , { } , httplib . responses [ httplib . OK ] ) \n 
~~ body = self . fixtures . load ( ) \n 
~~ def _2012_02_29_hostedzone_47234_rrset ( self , method , url , body , headers ) : \n 
~~ def _2012_02_29_hostedzone_47234_rrset_ZONE_DOES_NOT_EXIST ( self , method , \n 
url , body , headers ) : \n 
return ( httplib . NOT_FOUND , body , \n 
{ } , httplib . responses [ httplib . NOT_FOUND ] ) \n 
~~ def _2012_02_29_hostedzone_4444_ZONE_DOES_NOT_EXIST ( self , method , \n 
~~ def _2012_02_29_hostedzone_47234_ZONE_DOES_NOT_EXIST ( self , method , \n 
~~ def _2012_02_29_hostedzone_47234_rrset_RECORD_DOES_NOT_EXIST ( self , method , \n 
return ( httplib . BAD_REQUEST , body , { } , httplib . responses [ httplib . BAD_REQUEST ] ) \n 
~~ def _2012_02_29_hostedzone_47234_RECORD_DOES_NOT_EXIST ( self , method , \n 
from libcloud . common . types import LazyList \n 
class TestLazyList ( unittest . TestCase ) : \n 
~~~ super ( TestLazyList , self ) . setUp \n 
self . _get_more_counter = 0 \n 
~~~ super ( TestLazyList , self ) . tearDown \n 
~~ def test_init ( self ) : \n 
~~~ data = [ 1 , 2 , 3 , 4 , 5 ] \n 
ll = LazyList ( get_more = self . _get_more_exhausted ) \n 
ll_list = list ( ll ) \n 
self . assertEqual ( ll_list , data ) \n 
~~ def test_iterator ( self ) : \n 
for i , d in enumerate ( ll ) : \n 
~~~ self . assertEqual ( d , data [ i ] ) \n 
~~ ~~ def test_empty_list ( self ) : \n 
~~~ ll = LazyList ( get_more = self . _get_more_empty ) \n 
self . assertEqual ( list ( ll ) , [ ] ) \n 
self . assertEqual ( len ( ll ) , 0 ) \n 
self . assertTrue ( 10 not in ll ) \n 
~~ def test_iterator_not_exhausted ( self ) : \n 
~~~ data = [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 ] \n 
ll = LazyList ( get_more = self . _get_more_not_exhausted ) \n 
number_of_iterations = 0 \n 
number_of_iterations += 1 \n 
~~ self . assertEqual ( number_of_iterations , 10 ) \n 
~~ def test_len ( self ) : \n 
~~~ ll = LazyList ( get_more = self . _get_more_not_exhausted ) \n 
self . assertEqual ( len ( ll ) , 10 ) \n 
~~ def test_contains ( self ) : \n 
self . assertTrue ( 40 not in ll ) \n 
self . assertTrue ( 1 in ll ) \n 
self . assertTrue ( 5 in ll ) \n 
self . assertTrue ( 10 in ll ) \n 
~~ def test_indexing ( self ) : \n 
self . assertEqual ( ll [ 0 ] , 1 ) \n 
self . assertEqual ( ll [ 9 ] , 10 ) \n 
self . assertEqual ( ll [ - 1 ] , 10 ) \n 
~~~ ll [ 11 ] \n 
~~ ~~ def test_repr ( self ) : \n 
~~~ ll1 = LazyList ( get_more = self . _get_more_empty ) \n 
ll2 = LazyList ( get_more = self . _get_more_exhausted ) \n 
ll3 = LazyList ( get_more = self . _get_more_not_exhausted ) \n 
self . assertEqual ( repr ( ll1 ) , ) \n 
self . assertEqual ( repr ( ll2 ) , ) \n 
self . assertEqual ( repr ( ll3 ) , ) \n 
~~ def _get_more_empty ( self , last_key , value_dict ) : \n 
~~~ return [ ] , None , True \n 
~~ def _get_more_exhausted ( self , last_key , value_dict ) : \n 
return data , 5 , True \n 
~~ def _get_more_not_exhausted ( self , last_key , value_dict ) : \n 
~~~ self . _get_more_counter += 1 \n 
if not last_key : \n 
~~~ data , last_key , exhausted = [ 1 , 2 , 3 , 4 , 5 ] , 5 , False \n 
~~~ data , last_key , exhausted = [ 6 , 7 , 8 , 9 , 10 ] , 10 , True \n 
~~ return data , last_key , exhausted \n 
~~ from pyes import mappings \n 
from django . conf import settings \n 
from django . db . models . manager import Manager \n 
def model_to_mapping ( model , depth = 1 ) : \n 
meta = model . _meta \n 
indexoptions = getattr ( model , "indexeroptions" , { } ) \n 
ignore = indexoptions . get ( "ignore" , [ ] ) \n 
fields_options = indexoptions . get ( "fields" , { } ) \n 
extra_fields = indexoptions . get ( "extra_fields" , { } ) \n 
mapper = mappings . ObjectField ( meta . module_name ) \n 
for field in meta . fields + meta . many_to_many : \n 
~~~ name = field . name \n 
if name in ignore : \n 
~~ mapdata = get_mapping_for_field ( field , depth = depth , ** fields_options . get ( name , { } ) ) \n 
if mapdata : \n 
~~~ mapper . add_property ( mapdata ) \n 
~~ ~~ for name , options in extra_fields . items ( ) : \n 
~~~ type = options . pop ( "type" , "string" ) \n 
if type == "string" : \n 
~~~ data = dict ( name = name , store = True , \n 
index = "analyzed" , \n 
term_vector = "with_positions_offsets" \n 
data . update ( options ) \n 
if data [ ] == : \n 
~~~ del data [ ] \n 
~~ mapper . add_property ( mappings . StringField ( ** data ) ) \n 
~~ ~~ return mapper \n 
~~ def get_mapping_for_field ( field , depth = 1 , ** options ) : \n 
ntype = type ( field ) . __name__ \n 
if ntype in [ "AutoField" ] : \n 
~~~ return mappings . StringField ( name = field . name , store = True ) \n 
~~ elif ntype in [ "IntegerField" , \n 
"PositiveSmallIntegerField" , \n 
"SmallIntegerField" , \n 
"PositiveIntegerField" , \n 
"PositionField" , \n 
~~~ return mappings . IntegerField ( name = field . name , store = True ) \n 
~~ elif ntype in [ "FloatField" , \n 
"DecimalField" , \n 
~~~ return mappings . DoubleField ( name = field . name , store = True ) \n 
~~ elif ntype in [ "BooleanField" , \n 
"NullBooleanField" , \n 
~~~ return mappings . BooleanField ( name = field . name , store = True ) \n 
~~ elif ntype in [ "DateField" , \n 
"DateTimeField" , \n 
"CreationDateTimeField" , \n 
"ModificationDateTimeField" , \n 
"AddedDateTimeField" , \n 
"ModifiedDateTimeField" , \n 
"brainaetic.djangoutils.db.fields.CreationDateTimeField" , \n 
"brainaetic.djangoutils.db.fields.ModificationDateTimeField" , \n 
~~~ return mappings . DateField ( name = field . name , store = True ) \n 
~~ elif ntype in [ "SlugField" , \n 
"EmailField" , \n 
"TagField" , \n 
"URLField" , \n 
"CharField" , \n 
"ImageField" , \n 
"FileField" , \n 
~~~ return mappings . MultiField ( name = field . name , \n 
fields = { field . name : mappings . StringField ( name = field . name , index = "not_analyzed" "tk" : mappings . StringField ( name = "tk" , store = True , \n 
~~ elif ntype in [ "TextField" , \n 
~~~ data = dict ( name = field . name , store = True , \n 
if field . unique : \n 
~~~ data [ ] = \n 
~~ data . update ( options ) \n 
~~ return mappings . StringField ( ** data ) \n 
~~ elif ntype in [ "ForeignKey" , \n 
"TaggableManager" , \n 
"GenericRelation" , \n 
~~~ if depth >= 0 : \n 
~~~ mapper = model_to_mapping ( field . rel . to , depth - 1 ) \n 
if mapper : \n 
~~~ mapper . name = field . name \n 
return mapper \n 
~~ return get_mapping_for_field ( field . rel . to . _meta . pk , depth - 1 ) \n 
~~ elif ntype in [ "ManyToManyField" , \n 
~~~ if depth > 0 : \n 
mapper . name = field . name \n 
~~ if depth == 0 : \n 
~~~ mapper = get_mapping_for_field ( field . rel . to . _meta . pk , depth - 1 ) \n 
~~ if depth < 0 : \n 
~~ ~~ print ntype \n 
from . import exceptions \n 
exceptions_by_name = dict ( ( name , getattr ( exceptions , name ) ) \n 
for name in ( \n 
"DocumentAlreadyExistsEngineException" , \n 
"DocumentAlreadyExistsException" , \n 
"TypeMissingException" , \n 
"VersionConflictEngineException" , \n 
exception_patterns_trailing = { \n 
: exceptions . NotFoundException , \n 
: exceptions . AlreadyExistsException , \n 
def raise_if_error ( status , result , request = None ) : \n 
assert isinstance ( status , int ) \n 
if status < 400 : \n 
~~ if status == 404 and isinstance ( result , dict ) and not in result : \n 
~~ if not isinstance ( result , dict ) or not in result : \n 
~~ error = result [ ] \n 
if in error : \n 
~~~ error_list = error . split ( ) \n 
error = error_list [ len ( error_list ) - 1 ] \n 
~~ bits = error . split ( , 1 ) \n 
if len ( bits ) == 2 : \n 
~~~ excClass = exceptions_by_name . get ( bits [ 0 ] , None ) \n 
if excClass is not None : \n 
~~~ msg = bits [ 1 ] \n 
if msg . endswith ( ) : \n 
~~~ msg = msg [ : - 1 ] \n 
raise excClass ( msg , status , result , request ) \n 
~~ ~~ for pattern , excClass in list ( exception_patterns_trailing . items ( ) ) : \n 
~~~ if not error . endswith ( pattern ) : \n 
~~ raise excClass ( error , status , result , request ) \n 
~~ raise exceptions . ElasticSearchException ( error , status , result , request ) \n 
PY3 = sys . version_info [ 0 ] == 3 \n 
~~~ bytes_t = bytes \n 
~~ if sys . version_info < ( 3 , 3 ) : \n 
SYSTEM = platform . system ( ) \n 
if SYSTEM == : \n 
~~~ import ctypes \n 
from ctypes . util import find_library \n 
libSystem = ctypes . CDLL ( ) \n 
CoreServices = ctypes . CDLL ( find_library ( ) , \n 
use_errno = True ) \n 
mach_absolute_time = libSystem . mach_absolute_time \n 
mach_absolute_time . restype = ctypes . c_uint64 \n 
absolute_to_nanoseconds = CoreServices . AbsoluteToNanoseconds \n 
absolute_to_nanoseconds . restype = ctypes . c_uint64 \n 
absolute_to_nanoseconds . argtypes = [ ctypes . c_uint64 ] \n 
def _monotonic ( ) : \n 
~~~ return absolute_to_nanoseconds ( mach_absolute_time ( ) ) * 1e-9 \n 
~~ ~~ elif SYSTEM == : \n 
class timespec ( ctypes . Structure ) : \n 
( , ctypes . c_long ) , \n 
~~ librt = ctypes . CDLL ( , use_errno = True ) \n 
clock_gettime = librt . clock_gettime \n 
clock_gettime . argtypes = [ \n 
ctypes . c_int , ctypes . POINTER ( timespec ) , \n 
~~~ t = timespec ( ) \n 
if clock_gettime ( CLOCK_MONOTONIC , ctypes . pointer ( t ) ) != 0 : \n 
~~~ errno_ = ctypes . get_errno ( ) \n 
raise OSError ( errno_ , os . strerror ( errno_ ) ) \n 
~~ return t . tv_sec + t . tv_nsec * 1e-9 \n 
~~~ from time import time as _monotonic \n 
~~~ from time import monotonic \n 
~~~ import builtins \n 
from queue import Queue , Empty , Full , LifoQueue \n 
from itertools import zip_longest \n 
from io import StringIO , BytesIO \n 
map = map \n 
zip = zip \n 
string = str \n 
string_t = str \n 
long_t = int \n 
text_t = str \n 
range = range \n 
int_types = ( int , ) \n 
module_name_t = str \n 
open_fqdn = \n 
def items ( d ) : \n 
~~~ return d . items ( ) \n 
~~ def keys ( d ) : \n 
~~~ return d . keys ( ) \n 
~~ def values ( d ) : \n 
~~~ return d . values ( ) \n 
~~ def nextfun ( it ) : \n 
~~~ return it . __next__ \n 
~~ exec_ = getattr ( builtins , ) \n 
def reraise ( tp , value , tb = None ) : \n 
~~~ if value . __traceback__ is not tb : \n 
~~~ raise value . with_traceback ( tb ) \n 
~~ raise value \n 
~~ class WhateverIO ( StringIO ) : \n 
~~~ def write ( self , data ) : \n 
~~~ if isinstance ( data , bytes ) : \n 
~~~ data = data . encode ( ) \n 
~~ StringIO . write ( self , data ) \n 
imap as map , \n 
izip as zip , \n 
izip_longest as zip_longest , \n 
text_t = unicode \n 
range = xrange \n 
int_types = ( int , long ) \n 
~~~ return d . iteritems ( ) \n 
~~~ return d . iterkeys ( ) \n 
~~~ return d . itervalues ( ) \n 
~~~ return it . next \n 
if globs is None : \n 
~~~ frame = sys . _getframe ( 1 ) \n 
globs = frame . f_globals \n 
if locs is None : \n 
~~~ locs = frame . f_locals \n 
~~ del frame \n 
~~ elif locs is None : \n 
~~~ locs = globs \n 
~~ def with_metaclass ( Type , skip_attrs = set ( [ , ] ) ) : \n 
def _clone_with_metaclass ( Class ) : \n 
~~~ attrs = dict ( ( key , value ) for key , value in items ( vars ( Class ) ) \n 
if key not in skip_attrs ) \n 
return Type ( Class . __name__ , Class . __bases__ , attrs ) \n 
~~ return _clone_with_metaclass \n 
from pyes . tests import ESTestCase \n 
from pyes . rivers import CouchDBRiver , RabbitMQRiver , TwitterRiver \n 
class RiversTestCase ( ESTestCase ) : \n 
~~~ super ( RiversTestCase , self ) . setUp ( ) \n 
~~ def testCreateCouchDBRiver ( self ) : \n 
test_river = CouchDBRiver ( index_name = , index_type = ) \n 
result = self . conn . create_river ( test_river , river_name = ) \n 
self . assertResultContains ( result , { : True } ) \n 
~~ def testDeleteCouchDBRiver ( self ) : \n 
result = self . conn . delete_river ( test_river , river_name = ) \n 
~~ def testCreateRabbitMQRiver ( self ) : \n 
test_river = RabbitMQRiver ( index_name = , index_type = ) \n 
~~ def testDeleteRabbitMQRiver ( self ) : \n 
~~ def testCreateTwitterRiver ( self ) : \n 
test_river = TwitterRiver ( , , index_name = , index_type = ) \n 
~~ def testDeleteTwitterRiver ( self ) : \n 
~~ def testCreateTwitterRiverOAuth ( self ) : \n 
~~~ test_river = TwitterRiver ( , , index_name = , index_type = , \n 
consumer_key = "aaa" , \n 
consumer_secret = "aaa" , \n 
access_token = "aaa" , \n 
access_token_secret = "aaa" , \n 
~~ import pandas as pd \n 
import csv as csv \n 
from sklearn . ensemble import RandomForestClassifier \n 
train_df = pd . read_csv ( , header = 0 ) \n 
train_data = train_df . values \n 
test_data = test_df . values \n 
ids = list ( range ( 1 , len ( test_data ) + 1 ) ) \n 
forest = RandomForestClassifier ( n_estimators = 1000 ) \n 
forest = forest . fit ( train_data [ 0 : : , 1 : : ] , train_data [ 0 : : , 0 ] ) \n 
output = forest . predict ( test_data ) . astype ( int ) \n 
predictions_file = open ( "../data/random_forest.csv" , "wb" ) \n 
open_file_object = csv . writer ( predictions_file ) \n 
open_file_object . writerow ( [ "ImageId" , "Label" ] ) \n 
open_file_object . writerows ( zip ( ids , output ) ) \n 
predictions_file . close ( ) \n 
import os . path , sys \n 
~~~ import apiai \n 
~~~ sys . path . append ( os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , os . pardir ) ) \n 
import apiai \n 
~~ CLIENT_ACCESS_TOKEN = \n 
~~~ ai = apiai . ApiAI ( CLIENT_ACCESS_TOKEN ) \n 
request = ai . text_request ( ) \n 
request . lang = \n 
request . query = "Hello" \n 
response = request . getresponse ( ) \n 
print ( response . read ( ) ) \n 
~~ from . browser_integration import * \n 
class BrowserIntegrationTypeCommand ( sublime_plugin . WindowCommand ) : \n 
def visible ( ) : \n 
~~~ return browser . connected ( ) and browser . selected_items \n 
~~ @ require_browser \n 
def run ( self ) : \n 
~~~ if not browser . selected_items : \n 
~~ @ async \n 
def send_keys ( str ) : \n 
~~~ status ( ) \n 
for e in browser . selected_items : \n 
~~~ e . send_keys ( str ) \n 
~~ ~~ self . window . show_input_panel ( , , \n 
send_keys , None , None ) \n 
def ApiBlueprintFactory ( ) : \n 
class ApiBlueprint ( Linter ) : \n 
syntax = \n 
cmd = \n 
executable = \n 
executable = None \n 
regex = ( \n 
multiline = False \n 
line_col_base = ( 0 , 0 ) \n 
tempfile_suffix = None \n 
error_stream = util . STREAM_BOTH \n 
selectors = { } \n 
word_re = None \n 
defaults = { } \n 
inline_settings = None \n 
inline_overrides = None \n 
comment_re = None \n 
def split_match ( self , match ) : \n 
match , line , col , error , warning , message , near = super ( ) . split_match ( match ) \n 
if line is not None : \n 
~~~ line , col = self . view . rowcol ( ( int ( line ) ) ) \n 
line = int ( line ) - self . line_col_base [ 0 ] \n 
~~ return match , line , col , error , warning , message , near \n 
from SublimeLinter . lint import Linter , util \n 
ApiBlueprintFactory ( ) \n 
from . view_classes import BasicView , IndexView , RouteBaseView , VarBaseView \n 
from nose . tools import * \n 
app = Flask ( "common" ) \n 
BasicView . register ( app ) \n 
IndexView . register ( app ) \n 
RouteBaseView . register ( app ) \n 
VarBaseView . register ( app ) \n 
client = app . test_client ( ) \n 
def test_index_url ( ) : \n 
~~~ with app . test_request_context ( ) : \n 
~~~ url = url_for ( "IndexView:index" ) \n 
eq_ ( "/" , url ) \n 
~~ ~~ def test_basic_index_url ( ) : \n 
~~~ url = url_for ( "BasicView:index" ) \n 
eq_ ( "/basic/" , url ) \n 
~~ ~~ def test_custom_endpoint_url ( ) : \n 
~~~ url = url_for ( "basic_endpoint" ) \n 
eq_ ( "/basic/endpoint/" , url ) \n 
~~ ~~ def test_custom_route_base ( ) : \n 
~~~ url = url_for ( ) \n 
eq_ ( "/base-routed/" , url ) \n 
~~ ~~ def test_variable_route_popped_base ( ) : \n 
~~~ url = url_for ( , route = ) \n 
eq_ ( , url ) \n 
~~ ~~ def test_variable_route_base ( ) : \n 
~~ ~~ def test_variable_route_base_with_local_route_var ( ) : \n 
~~~ client = app . test_client ( ) \n 
resp = client . get ( ) \n 
import TaskGen , Task , Utils \n 
from Logs import debug \n 
from TaskGen import feature , before , extension , after \n 
g_cxx_flag_vars = [ \n 
EXT_CXX = [ , , , , ] \n 
g_cxx_type_vars = [ , ] \n 
class cxx_taskgen ( ccroot . ccroot_abstract ) : \n 
~~ @ feature ( ) \n 
@ before ( ) \n 
@ after ( ) \n 
def init_cxx ( self ) : \n 
~~~ if not in self . features : \n 
~~~ self . mappings [ ] = TaskGen . task_gen . mappings [ ] \n 
~~ self . p_flag_vars = set ( self . p_flag_vars ) . union ( g_cxx_flag_vars ) \n 
self . p_type_vars = set ( self . p_type_vars ) . union ( g_cxx_type_vars ) \n 
if not self . env [ ] : \n 
~~ ~~ @ feature ( ) \n 
def apply_obj_vars_cxx ( self ) : \n 
env = self . env \n 
app = env . append_unique \n 
cxxpath_st = env [ ] \n 
for i in env [ ] : \n 
~~~ app ( , cxxpath_st % i . bldpath ( env ) ) \n 
app ( , cxxpath_st % i . srcpath ( env ) ) \n 
~~ for i in env [ ] : \n 
~~~ app ( , cxxpath_st % i ) \n 
def apply_defines_cxx ( self ) : \n 
self . defines = getattr ( self , , [ ] ) \n 
lst = self . to_list ( self . defines ) + self . to_list ( self . env [ ] ) \n 
milst = [ ] \n 
for defi in lst : \n 
~~~ if not defi in milst : \n 
~~~ milst . append ( defi ) \n 
~~ ~~ libs = self . to_list ( self . uselib ) \n 
for l in libs : \n 
~~~ val = self . env [ + l ] \n 
if val : milst += self . to_list ( val ) \n 
self . env [ ] = [ y % x for x in milst ] \n 
~~ @ extension ( EXT_CXX ) \n 
def cxx_hook ( self , node ) : \n 
~~~ if getattr ( self , , None ) : \n 
~~~ obj_ext = self . obj_ext \n 
~~~ obj_ext = % self . idx \n 
~~ task = self . create_task ( , node , node . change_ext ( obj_ext ) ) \n 
~~~ self . compiled_tasks . append ( task ) \n 
~~ return task \n 
~~ cxx_str = cls = Task . simple_task_type ( , cxx_str , color = , ext_out = , ext_in = , shell = False ) cls . scan = ccroot . scan \n 
cls . vars . append ( ) \n 
link_str = \n 
cls = Task . simple_task_type ( , link_str , color = , ext_in = , ext_out = , shell cls . maxjobs = 1 \n 
cls . install = Utils . nada \n 
mswindows = ( sys . platform == "win32" ) \n 
import gc \n 
class CalledProcessError ( Exception ) : \n 
~~~ def __init__ ( self , returncode , cmd ) : \n 
~~~ self . returncode = returncode \n 
self . cmd = cmd \n 
~~ ~~ if mswindows : \n 
~~~ import threading \n 
import msvcrt \n 
if 0 : \n 
~~~ import pywintypes \n 
from win32api import GetStdHandle , STD_INPUT_HANDLE , STD_OUTPUT_HANDLE , STD_ERROR_HANDLE \n 
from win32api import GetCurrentProcess , DuplicateHandle , GetModuleFileName , GetVersion \n 
from win32con import DUPLICATE_SAME_ACCESS , SW_HIDE \n 
from win32pipe import CreatePipe \n 
from win32process import CreateProcess , STARTUPINFO , GetExitCodeProcess , STARTF_USESTDHANDLES , STARTF_USESHOWWINDOW , CREATE_NEW_CONSOLE \n 
from win32event import WaitForSingleObject , INFINITE , WAIT_OBJECT_0 \n 
~~~ from _subprocess import * \n 
class STARTUPINFO : \n 
~~~ dwFlags = 0 \n 
hStdInput = None \n 
hStdOutput = None \n 
hStdError = None \n 
wShowWindow = 0 \n 
~~ class pywintypes : \n 
~~~ error = IOError \n 
~~~ import select \n 
import fcntl \n 
~~ __all__ = [ "Popen" , "PIPE" , "STDOUT" , "call" , "check_call" , "CalledProcessError" ] \n 
~~~ MAXFD = os . sysconf ( "SC_OPEN_MAX" ) \n 
~~~ MAXFD = 256 \n 
~~~ False \n 
~~~ False = 0 \n 
True = 1 \n 
~~ _active = [ ] \n 
def _cleanup ( ) : \n 
~~~ for inst in _active [ : ] : \n 
~~~ if inst . poll ( _deadstate = sys . maxint ) >= 0 : \n 
~~~ _active . remove ( inst ) \n 
~~ ~~ ~~ ~~ PIPE = - 1 \n 
STDOUT = - 2 \n 
def call ( * popenargs , ** kwargs ) : \n 
~~~ return Popen ( * popenargs , ** kwargs ) . wait ( ) \n 
~~ def check_call ( * popenargs , ** kwargs ) : \n 
~~~ retcode = call ( * popenargs , ** kwargs ) \n 
cmd = kwargs . get ( "args" ) \n 
if cmd is None : \n 
~~~ cmd = popenargs [ 0 ] \n 
~~ if retcode : \n 
~~~ raise CalledProcessError ( retcode , cmd ) \n 
~~ return retcode \n 
~~ def list2cmdline ( seq ) : \n 
~~~ result = [ ] \n 
needquote = False \n 
for arg in seq : \n 
~~~ bs_buf = [ ] \n 
~~~ result . append ( ) \n 
if needquote : \n 
~~~ result . append ( \'"\' ) \n 
~~ for c in arg : \n 
~~~ bs_buf . append ( c ) \n 
~~ elif c == \'"\' : \n 
~~~ result . append ( * len ( bs_buf ) * 2 ) \n 
bs_buf = [ ] \n 
result . append ( \'\\\\"\' ) \n 
~~~ if bs_buf : \n 
~~~ result . extend ( bs_buf ) \n 
~~ result . append ( c ) \n 
~~ ~~ if bs_buf : \n 
~~ if needquote : \n 
result . append ( \'"\' ) \n 
~~ ~~ return . join ( result ) \n 
~~ class Popen ( object ) : \n 
~~~ def __init__ ( self , args , bufsize = 0 , executable = None , \n 
stdin = None , stdout = None , stderr = None , \n 
preexec_fn = None , close_fds = False , shell = False , \n 
cwd = None , env = None , universal_newlines = False , \n 
startupinfo = None , creationflags = 0 ) : \n 
~~~ _cleanup ( ) \n 
self . _child_created = False \n 
if not isinstance ( bufsize , ( int , long ) ) : \n 
~~ if mswindows : \n 
~~~ if preexec_fn is not None : \n 
~~ if close_fds : \n 
~~~ if startupinfo is not None : \n 
~~ if creationflags != 0 : \n 
~~ ~~ self . stdin = None \n 
self . stdout = None \n 
self . stderr = None \n 
self . pid = None \n 
self . returncode = None \n 
self . universal_newlines = universal_newlines \n 
( p2cread , p2cwrite , \n 
c2pread , c2pwrite , \n 
errread , errwrite ) = self . _get_handles ( stdin , stdout , stderr ) \n 
self . _execute_child ( args , executable , preexec_fn , close_fds , \n 
cwd , env , universal_newlines , \n 
startupinfo , creationflags , shell , \n 
p2cread , p2cwrite , \n 
errread , errwrite ) \n 
if mswindows : \n 
~~~ if stdin is None and p2cwrite is not None : \n 
~~~ os . close ( p2cwrite ) \n 
p2cwrite = None \n 
~~ if stdout is None and c2pread is not None : \n 
~~~ os . close ( c2pread ) \n 
c2pread = None \n 
~~ if stderr is None and errread is not None : \n 
~~~ os . close ( errread ) \n 
errread = None \n 
~~ ~~ if p2cwrite : \n 
~~~ self . stdin = os . fdopen ( p2cwrite , , bufsize ) \n 
~~ if c2pread : \n 
~~~ if universal_newlines : \n 
~~~ self . stdout = os . fdopen ( c2pread , , bufsize ) \n 
~~ ~~ if errread : \n 
~~~ self . stderr = os . fdopen ( errread , , bufsize ) \n 
~~ ~~ ~~ def _translate_newlines ( self , data ) : \n 
~~~ data = data . replace ( "\\r\\n" , "\\n" ) \n 
data = data . replace ( "\\r" , "\\n" ) \n 
~~ def __del__ ( self , sys = sys ) : \n 
~~~ if not self . _child_created : \n 
~~ self . poll ( _deadstate = sys . maxint ) \n 
if self . returncode is None and _active is not None : \n 
~~~ _active . append ( self ) \n 
~~ ~~ def communicate ( self , input = None ) : \n 
~~~ if [ self . stdin , self . stdout , self . stderr ] . count ( None ) >= 2 : \n 
~~~ stdout = None \n 
stderr = None \n 
if self . stdin : \n 
~~~ if input : \n 
~~~ self . stdin . write ( input ) \n 
~~ self . stdin . close ( ) \n 
~~ elif self . stdout : \n 
~~~ stdout = self . stdout . read ( ) \n 
~~ elif self . stderr : \n 
~~~ stderr = self . stderr . read ( ) \n 
~~ self . wait ( ) \n 
return ( stdout , stderr ) \n 
~~ return self . _communicate ( input ) \n 
~~~ def _get_handles ( self , stdin , stdout , stderr ) : \n 
~~~ if stdin is None and stdout is None and stderr is None : \n 
~~~ return ( None , None , None , None , None , None ) \n 
~~ p2cread , p2cwrite = None , None \n 
c2pread , c2pwrite = None , None \n 
errread , errwrite = None , None \n 
if stdin is None : \n 
~~~ p2cread = GetStdHandle ( STD_INPUT_HANDLE ) \n 
~~ if p2cread is not None : \n 
~~ elif stdin is None or stdin == PIPE : \n 
~~~ p2cread , p2cwrite = CreatePipe ( None , 0 ) \n 
p2cwrite = p2cwrite . Detach ( ) \n 
p2cwrite = msvcrt . open_osfhandle ( p2cwrite , 0 ) \n 
~~ elif isinstance ( stdin , int ) : \n 
~~~ p2cread = msvcrt . get_osfhandle ( stdin ) \n 
~~~ p2cread = msvcrt . get_osfhandle ( stdin . fileno ( ) ) \n 
~~ p2cread = self . _make_inheritable ( p2cread ) \n 
~~~ c2pwrite = GetStdHandle ( STD_OUTPUT_HANDLE ) \n 
~~ if c2pwrite is not None : \n 
~~ elif stdout is None or stdout == PIPE : \n 
~~~ c2pread , c2pwrite = CreatePipe ( None , 0 ) \n 
c2pread = c2pread . Detach ( ) \n 
c2pread = msvcrt . open_osfhandle ( c2pread , 0 ) \n 
~~ elif isinstance ( stdout , int ) : \n 
~~~ c2pwrite = msvcrt . get_osfhandle ( stdout ) \n 
~~~ c2pwrite = msvcrt . get_osfhandle ( stdout . fileno ( ) ) \n 
~~ c2pwrite = self . _make_inheritable ( c2pwrite ) \n 
if stderr is None : \n 
~~~ errwrite = GetStdHandle ( STD_ERROR_HANDLE ) \n 
~~ if errwrite is not None : \n 
~~ elif stderr is None or stderr == PIPE : \n 
~~~ errread , errwrite = CreatePipe ( None , 0 ) \n 
errread = errread . Detach ( ) \n 
errread = msvcrt . open_osfhandle ( errread , 0 ) \n 
~~ elif stderr == STDOUT : \n 
~~~ errwrite = c2pwrite \n 
~~ elif isinstance ( stderr , int ) : \n 
~~~ errwrite = msvcrt . get_osfhandle ( stderr ) \n 
~~~ errwrite = msvcrt . get_osfhandle ( stderr . fileno ( ) ) \n 
~~ errwrite = self . _make_inheritable ( errwrite ) \n 
return ( p2cread , p2cwrite , \n 
~~ def _make_inheritable ( self , handle ) : \n 
~~~ return DuplicateHandle ( GetCurrentProcess ( ) , handle , GetCurrentProcess ( ) , 0 , 1 , DUPLICATE_SAME_ACCESS \n 
~~ def _find_w9xpopen ( self ) : \n 
~~~ w9xpopen = os . path . join ( os . path . dirname ( GetModuleFileName ( 0 ) ) , "w9xpopen.exe" ) \n 
if not os . path . exists ( w9xpopen ) : \n 
~~~ w9xpopen = os . path . join ( os . path . dirname ( sys . exec_prefix ) , "w9xpopen.exe" ) \n 
~~ def _execute_child ( self , args , executable , preexec_fn , close_fds , \n 
errread , errwrite ) : \n 
~~~ if not isinstance ( args , types . StringTypes ) : \n 
~~~ args = list2cmdline ( args ) \n 
~~ if startupinfo is None : \n 
~~~ startupinfo = STARTUPINFO ( ) \n 
~~ if None not in ( p2cread , c2pwrite , errwrite ) : \n 
~~~ startupinfo . dwFlags |= STARTF_USESTDHANDLES \n 
startupinfo . hStdInput = p2cread \n 
startupinfo . hStdOutput = c2pwrite \n 
startupinfo . hStdError = errwrite \n 
~~ if shell : \n 
~~~ startupinfo . dwFlags |= STARTF_USESHOWWINDOW \n 
startupinfo . wShowWindow = SW_HIDE \n 
comspec = os . environ . get ( "COMSPEC" , "cmd.exe" ) \n 
if ( GetVersion ( ) >= 0x80000000 L or \n 
os . path . basename ( comspec ) . lower ( ) == "command.com" ) : \n 
~~~ w9xpopen = self . _find_w9xpopen ( ) \n 
creationflags |= CREATE_NEW_CONSOLE \n 
~~~ hp , ht , pid , tid = CreateProcess ( executable , args , None , None , 1 , creationflags , env ~~ except pywintypes . error , e : \n 
~~~ raise WindowsError ( * e . args ) \n 
~~ self . _child_created = True \n 
self . _handle = hp \n 
self . pid = pid \n 
ht . Close ( ) \n 
if p2cread is not None : \n 
~~~ p2cread . Close ( ) \n 
~~~ c2pwrite . Close ( ) \n 
~~~ errwrite . Close ( ) \n 
~~ ~~ def poll ( self , _deadstate = None ) : \n 
~~~ if self . returncode is None : \n 
~~~ if WaitForSingleObject ( self . _handle , 0 ) == WAIT_OBJECT_0 : \n 
~~~ self . returncode = GetExitCodeProcess ( self . _handle ) \n 
~~ ~~ return self . returncode \n 
~~ def wait ( self ) : \n 
~~~ obj = WaitForSingleObject ( self . _handle , INFINITE ) \n 
self . returncode = GetExitCodeProcess ( self . _handle ) \n 
~~ return self . returncode \n 
~~ def _readerthread ( self , fh , buffer ) : \n 
~~~ buffer . append ( fh . read ( ) ) \n 
~~ def _communicate ( self , input ) : \n 
if self . stdout : \n 
~~~ stdout = [ ] \n 
stdout_thread = threading . Thread ( target = self . _readerthread , args = ( self . stdout , stdout stdout_thread . setDaemon ( True ) \n 
stdout_thread . start ( ) \n 
~~ if self . stderr : \n 
~~~ stderr = [ ] \n 
stderr_thread = threading . Thread ( target = self . _readerthread , args = ( self . stderr , stderr stderr_thread . setDaemon ( True ) \n 
stderr_thread . start ( ) \n 
~~ if self . stdin : \n 
~~~ if input is not None : \n 
~~ if self . stdout : \n 
~~~ stdout_thread . join ( ) \n 
~~~ stderr_thread . join ( ) \n 
~~ if stdout is not None : \n 
~~~ stdout = stdout [ 0 ] \n 
~~ if stderr is not None : \n 
~~~ stderr = stderr [ 0 ] \n 
~~ if self . universal_newlines and hasattr ( file , ) : \n 
~~~ if stdout : \n 
~~~ stdout = self . _translate_newlines ( stdout ) \n 
~~ if stderr : \n 
~~~ stderr = self . _translate_newlines ( stderr ) \n 
~~ ~~ self . wait ( ) \n 
~~~ p2cread , p2cwrite = None , None \n 
~~ elif stdin == PIPE : \n 
~~~ p2cread , p2cwrite = os . pipe ( ) \n 
~~~ p2cread = stdin \n 
~~~ p2cread = stdin . fileno ( ) \n 
~~ if stdout is None : \n 
~~ elif stdout == PIPE : \n 
~~~ c2pread , c2pwrite = os . pipe ( ) \n 
~~~ c2pwrite = stdout \n 
~~~ c2pwrite = stdout . fileno ( ) \n 
~~ if stderr is None : \n 
~~ elif stderr == PIPE : \n 
~~~ errread , errwrite = os . pipe ( ) \n 
~~~ errwrite = stderr \n 
~~~ errwrite = stderr . fileno ( ) \n 
~~ return ( p2cread , p2cwrite , c2pread , c2pwrite , errread , errwrite ) \n 
~~ def _set_cloexec_flag ( self , fd ) : \n 
~~~ cloexec_flag = fcntl . FD_CLOEXEC \n 
~~~ cloexec_flag = 1 \n 
~~ old = fcntl . fcntl ( fd , fcntl . F_GETFD ) \n 
fcntl . fcntl ( fd , fcntl . F_SETFD , old | cloexec_flag ) \n 
~~ def _close_fds ( self , but ) : \n 
~~~ for i in xrange ( 3 , MAXFD ) : \n 
~~~ if i == but : \n 
~~~ os . close ( i ) \n 
~~ ~~ ~~ def _execute_child ( self , args , executable , preexec_fn , close_fds , \n 
cwd , env , universal_newlines , startupinfo , creationflags , shell , \n 
p2cread , p2cwrite , c2pread , c2pwrite , errread , errwrite ) : \n 
~~~ if isinstance ( args , types . StringTypes ) : \n 
~~~ args = [ args ] \n 
~~~ args = list ( args ) \n 
~~~ args = [ "/bin/sh" , "-c" ] + args \n 
~~ if executable is None : \n 
~~~ executable = args [ 0 ] \n 
~~ errpipe_read , errpipe_write = os . pipe ( ) \n 
self . _set_cloexec_flag ( errpipe_write ) \n 
gc_was_enabled = gc . isenabled ( ) \n 
gc . disable ( ) \n 
~~~ self . pid = os . fork ( ) \n 
~~~ if gc_was_enabled : \n 
~~~ gc . enable ( ) \n 
if self . pid == 0 : \n 
~~~ if p2cwrite : \n 
~~ if errread : \n 
~~ os . close ( errpipe_read ) \n 
if p2cread : \n 
~~~ os . dup2 ( p2cread , 0 ) \n 
~~ if c2pwrite : \n 
~~~ os . dup2 ( c2pwrite , 1 ) \n 
~~ if errwrite : \n 
~~~ os . dup2 ( errwrite , 2 ) \n 
~~ if p2cread and p2cread not in ( 0 , ) : \n 
~~~ os . close ( p2cread ) \n 
~~ if c2pwrite and c2pwrite not in ( p2cread , 1 ) : \n 
~~~ os . close ( c2pwrite ) \n 
~~ if errwrite and errwrite not in ( p2cread , c2pwrite , 2 ) : \n 
~~~ os . close ( errwrite ) \n 
~~~ self . _close_fds ( but = errpipe_write ) \n 
~~ if cwd is not None : \n 
~~~ os . chdir ( cwd ) \n 
~~ if preexec_fn : \n 
~~~ apply ( preexec_fn ) \n 
~~ if env is None : \n 
~~~ os . execvp ( executable , args ) \n 
~~~ os . execvpe ( executable , args , env ) \n 
~~~ exc_type , exc_value , tb = sys . exc_info ( ) \n 
exc_lines = traceback . format_exception ( exc_type , exc_value , tb ) \n 
exc_value . child_traceback = . join ( exc_lines ) \n 
os . write ( errpipe_write , pickle . dumps ( exc_value ) ) \n 
~~ os . _exit ( 255 ) \n 
~~ if gc_was_enabled : \n 
~~ os . close ( errpipe_write ) \n 
if p2cread and p2cwrite : \n 
~~ if c2pwrite and c2pread : \n 
~~ if errwrite and errread : \n 
~~ data = os . read ( errpipe_read , 1048576 ) \n 
os . close ( errpipe_read ) \n 
if data != "" : \n 
~~~ os . waitpid ( self . pid , 0 ) \n 
child_exception = pickle . loads ( data ) \n 
raise child_exception \n 
~~ ~~ def _handle_exitstatus ( self , sts ) : \n 
~~~ if os . WIFSIGNALED ( sts ) : \n 
~~~ self . returncode = - os . WTERMSIG ( sts ) \n 
~~ elif os . WIFEXITED ( sts ) : \n 
~~~ self . returncode = os . WEXITSTATUS ( sts ) \n 
~~~ pid , sts = os . waitpid ( self . pid , os . WNOHANG ) \n 
if pid == self . pid : \n 
~~~ self . _handle_exitstatus ( sts ) \n 
~~ ~~ except os . error : \n 
~~~ if _deadstate is not None : \n 
~~~ self . returncode = _deadstate \n 
~~ ~~ ~~ return self . returncode \n 
~~~ pid , sts = os . waitpid ( self . pid , 0 ) \n 
self . _handle_exitstatus ( sts ) \n 
~~~ read_set = [ ] \n 
write_set = [ ] \n 
stdout = None \n 
~~~ self . stdin . flush ( ) \n 
if input : \n 
~~~ write_set . append ( self . stdin ) \n 
~~~ self . stdin . close ( ) \n 
~~ ~~ if self . stdout : \n 
~~~ read_set . append ( self . stdout ) \n 
stdout = [ ] \n 
~~~ read_set . append ( self . stderr ) \n 
stderr = [ ] \n 
~~ input_offset = 0 \n 
while read_set or write_set : \n 
~~~ rlist , wlist , xlist = select . select ( read_set , write_set , [ ] ) \n 
if self . stdin in wlist : \n 
~~~ bytes_written = os . write ( self . stdin . fileno ( ) , buffer ( input , input_offset , 512 ) ) \n 
input_offset += bytes_written \n 
if input_offset >= len ( input ) : \n 
write_set . remove ( self . stdin ) \n 
~~ ~~ if self . stdout in rlist : \n 
~~~ data = os . read ( self . stdout . fileno ( ) , 1024 ) \n 
if data == "" : \n 
~~~ self . stdout . close ( ) \n 
read_set . remove ( self . stdout ) \n 
~~ stdout . append ( data ) \n 
~~ if self . stderr in rlist : \n 
~~~ data = os . read ( self . stderr . fileno ( ) , 1024 ) \n 
~~~ self . stderr . close ( ) \n 
read_set . remove ( self . stderr ) \n 
~~ stderr . append ( data ) \n 
~~ ~~ if stdout is not None : \n 
~~~ stdout = . join ( stdout ) \n 
~~~ stderr = . join ( stderr ) \n 
~~ ~~ ~~ import copy \n 
from appium . webdriver . mobilecommand import MobileCommand as Command \n 
class TouchAction ( object ) : \n 
~~~ def __init__ ( self , driver = None ) : \n 
~~~ self . _driver = driver \n 
self . _actions = [ ] \n 
~~ def tap ( self , element = None , x = None , y = None , count = 1 ) : \n 
opts = self . _get_opts ( element , x , y ) \n 
opts [ ] = count \n 
self . _add_action ( , opts ) \n 
~~ def press ( self , el = None , x = None , y = None ) : \n 
self . _add_action ( , self . _get_opts ( el , x , y ) ) \n 
~~ def long_press ( self , el = None , x = None , y = None , duration = 1000 ) : \n 
self . _add_action ( , self . _get_opts ( el , x , y , duration ) ) \n 
~~ def wait ( self , ms = 0 ) : \n 
if ms is None : \n 
~~~ ms = 0 \n 
~~ opts = { : ms } \n 
~~ def move_to ( self , el = None , x = None , y = None ) : \n 
self . _add_action ( , { } ) \n 
~~ def perform ( self ) : \n 
params = { : self . _actions } \n 
self . _driver . execute ( Command . TOUCH_ACTION , params ) \n 
def json_wire_gestures ( self ) : \n 
~~~ gestures = [ ] \n 
for action in self . _actions : \n 
~~~ gestures . append ( copy . deepcopy ( action ) ) \n 
~~ return gestures \n 
~~ def _add_action ( self , action , options ) : \n 
~~~ gesture = { \n 
: action , \n 
: options , \n 
self . _actions . append ( gesture ) \n 
~~ def _get_opts ( self , element , x , y , duration = None ) : \n 
~~~ opts = { } \n 
if element is not None : \n 
~~~ opts [ ] = element . id \n 
~~ if x is None or y is None : \n 
~~~ x , y = None , None \n 
~~ opts [ ] = x \n 
opts [ ] = y \n 
if duration is not None : \n 
~~~ opts [ ] = duration \n 
~~ return opts \n 
__version__ = "5.8.4" \n 
import django \n 
if django . get_version ( ) < : \n 
from re import sub \n 
from django . contrib . staticfiles . storage import AppStaticStorage \n 
class NamespacedAngularAppStorage ( AppStaticStorage ) : \n 
source_dir = \n 
def __init__ ( self , app , * args , ** kwargs ) : \n 
self . prefix = os . path . join ( * ( app . split ( ) ) ) \n 
super ( NamespacedAngularAppStorage , self ) . __init__ ( app , * args , ** kwargs ) \n 
~~ def path ( self , name ) : \n 
~~~ name = sub ( + self . prefix + os . sep . encode ( ) , , name ) \n 
return super ( NamespacedAngularAppStorage , self ) . path ( name ) \n 
~~ ~~ class NamespacedE2ETestAppStorage ( AppStaticStorage ) : \n 
source_dir = os . path . join ( , ) \n 
prefix_args = [ self . source_dir ] + app . split ( ) \n 
self . prefix = os . path . join ( * prefix_args ) \n 
super ( NamespacedE2ETestAppStorage , self ) . __init__ ( app , * args , ** kwargs ) \n 
~~ ~~ class NamespacedLibTestAppStorage ( AppStaticStorage ) : \n 
prefix_args = app . split ( ) + [ ] \n 
super ( NamespacedLibTestAppStorage , self ) . __init__ ( app , * args , ** kwargs ) \n 
~~ ~~ ~~ import unittest \n 
from proto_lib_fixture import proto_lib \n 
TestIsInitialized = None \n 
SubMessage = None \n 
TestWithRequiredSubMessage = None \n 
@ pytest . mark . usefixtures ( ) \n 
class MergeFromTest ( unittest . TestCase ) : \n 
~~~ global TestIsInitialized , SubMessage , TestWithRequiredSubMessage \n 
from test_is_initialized_proto import TestIsInitialized , SubMessage , TestWithRequiredSubMessage \n 
~~ def test_new_message_is_not_initialized ( self ) : \n 
~~~ message = TestIsInitialized ( ) \n 
self . assertFalse ( message . IsInitialized ( ) ) \n 
~~ def test_message_with_all_required_fields_set_is_initialized ( self ) : \n 
message . req_field = 1 \n 
self . assertTrue ( message . IsInitialized ( ) ) \n 
~~ def test_new_message_with_required_submessage_is_not_initialized ( self ) : \n 
~~~ message = TestWithRequiredSubMessage ( ) \n 
~~ def test_message_with_initialized_required_submessage_is_initialized ( self ) : \n 
message . req_sub . req_field = 1 \n 
~~ def test_message_with_uninitialized_submessage_is_not_initialized ( self ) : \n 
message . sub_message . opt_field = 2 \n 
~~ def test_message_with_initialized_submessage_is_initialized ( self ) : \n 
message . sub_message . req_field = 2 \n 
~~ def test_message_with_uninitialized_repeated_submessage_is_not_initialized ( self ) : \n 
message . list_sub . add ( ) . opt_field = 2 \n 
~~ def test_message_with_initialized_repeated_submessage_is_initialized ( self ) : \n 
message . list_sub . add ( ) . req_field = 2 \n 
from db import Db \n 
class MemoryDb ( Db ) : \n 
def new ( cls , config ) : \n 
~~~ super ( MemoryDb , cls ) . new ( config ) \n 
cls . data = [ ] \n 
cls . id = 0 \n 
cls . auto_throw_error = True \n 
return cls \n 
def init_conn ( cls ) : \n 
def drop_revision ( cls ) : \n 
~~~ cls . data = [ ] \n 
def create_revision ( cls ) : \n 
def get_commit_history ( cls ) : \n 
~~~ return copy . copy ( cls . data ) \n 
def get_applied_alters ( cls ) : \n 
~~~ return [ d [ 1 ] for d in cls . data ] \n 
def append_commit ( cls , ref ) : \n 
~~~ cls . id += 1 \n 
cls . data . append ( [ cls . id , ref , None ] ) \n 
def get_append_commit_query ( cls , _ ) : \n 
def remove_commit ( cls , ref ) : \n 
~~~ to_remove = [ d for d in cls . data if d [ 1 ] == ref ] \n 
if len ( to_remove ) > 0 : \n 
~~~ cls . data . remove ( to_remove [ 0 ] ) \n 
def get_remove_commit_query ( cls , _ ) : \n 
def create_history ( cls ) : \n 
def conn ( cls ) : \n 
~~~ return cls \n 
def run_file_cmd ( cls , filename ) : \n 
return [ ] , None , None \n 
def run_file_cmd_with_error ( cls , filename ) : \n 
import_path = os . path . join ( os . path . dirname ( os . path . realpath ( __file__ ) ) , ) \n 
sys . path . append ( import_path ) \n 
from command import CommandContext , ListCommand \n 
from alter_util import AlterUtil \n 
from env_util import EnvironmentUtil \n 
from test_util import make_argv \n 
class ListTest ( unittest . TestCase ) : \n 
~~~ EnvironmentUtil . setup_fresh_test_env ( ) \n 
self . context = CommandContext . via ( { \n 
: } ) \n 
self . listCommand = ListCommand ( self . context ) \n 
~~~ EnvironmentUtil . teardown_fresh_test_env ( ) \n 
~~ def test_order ( self ) : \n 
~~~ id1 , id2 = AlterUtil . create_alters ( [ 1 , 2 ] ) \n 
AlterUtil . run_alters ( ) \n 
sys . argv = make_argv ( [ ] ) \n 
result = self . listCommand . run ( ) \n 
self . assertEqual ( [ id1 , id2 ] , result ) \n 
~~ def test_order_reverse ( self ) : \n 
self . assertEqual ( [ id2 , id1 ] , result ) \n 
~~ import datetime \n 
from south . db import db \n 
from south . v2 import SchemaMigration \n 
class Migration ( SchemaMigration ) : \n 
~~~ def forwards ( self , orm ) : \n 
~~~ db . add_column ( , , \n 
self . gf ( ) ( null = True ) , \n 
keep_default = False ) \n 
~~ def backwards ( self , orm ) : \n 
~~~ db . delete_column ( , ) \n 
~~ models = { \n 
: { : } , \n 
: ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } : ( , [ ] , { : "orm[\'auth.Permission\']" } , \n 
: ( , [ ] , { : "orm[\'contenttypes.ContentType\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } ) \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.Group\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.Permission\']" : ( , [ ] , { : , : } , \n 
: { : "[\'name\']" , : } , \n 
: ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : ( , [ ] , { } ) , \n 
: ( , [ ] , { : "\'PASSING\'" } : ( , [ ] , { : "\'PASSING\'" } ) , \n 
: ( , [ ] , { : "orm[\'cabotapp.StatusCheck\']" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "orm[\'auth.User\']" } , \n 
: ( , [ ] , { : "\'snapshots\'" : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "\'PASSING\'" } ) , \n 
: ( , [ ] , { : "orm[\'cabotapp.Instance\']" : ( , [ ] , { : , : ( , [ ] , { } ) , \n 
: ( , [ ] , { : "\'snapshots\'" : ( , [ ] , { : } ) \n 
: ( , [ ] , { } ) , \n 
: ( , [ ] , { : "orm[\'auth.User\']" } ) \n 
: ( , [ ] , { : "\'passing\'" , : ( , [ ] , { : , : : ( , [ ] , { : "orm[\'auth.User\']" : ( , [ ] , { : , : : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "\'ERROR\'" , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : , : } ) : ( , [ ] , { : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : : ( , [ ] , { : , : } ) : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : } , \n 
: ( , [ ] , { : , } , \n 
: ( , [ ] , { : } : ( , [ ] , { : "\'\'" , : ( , [ ] , { : } ) , \n 
: ( , [ ] , { : "\'\'" , : ( , [ ] , { : "\'profile\'" } , \n 
~~ import see \n 
def union ( * sets ) : \n 
~~~ return set ( itertools . chain ( * sets ) ) \n 
~~ SIGN_OPS = set ( [ , ] ) \n 
NUMBER_OPS = set ( . split ( ) ) \n 
NUMBER_ASSIGN_OPS = set ( ) \n 
BITWISE_OPS = set ( . split ( ) ) \n 
BITWISE_ASSIGN_OPS = set ( op + for op in BITWISE_OPS ) \n 
COMPARE_OPS = set ( . split ( ) ) \n 
MATRIX_OPS = set ( [ ] ) \n 
MATRIX_ASSIGN_OPS = set ( [ ] ) \n 
ALL_OPS = union ( \n 
SIGN_OPS , \n 
NUMBER_OPS , \n 
NUMBER_ASSIGN_OPS , \n 
BITWISE_OPS , \n 
BITWISE_ASSIGN_OPS , \n 
COMPARE_OPS , \n 
MATRIX_OPS , \n 
MATRIX_ASSIGN_OPS , \n 
class TestSeeOutput ( unittest . TestCase ) : \n 
~~~ def check_ops ( self , obj_type , expected_ops , see_output ) : \n 
~~~ for op in ALL_OPS : \n 
~~~ if op in expected_ops : \n 
~~~ self . assertIn ( \n 
op , see_output , \n 
% ( obj_type , op ) ) \n 
~~~ self . assertNotIn ( \n 
~~ ~~ ~~ def test_int ( self ) : \n 
~~~ obj = 1 \n 
lit_ops = union ( \n 
obj_ops = union ( \n 
lit_ops , \n 
lit_see = see . see ( 1 ) \n 
obj_see = see . see ( obj ) \n 
self . check_ops ( , lit_ops , lit_see ) \n 
self . check_ops ( , obj_ops , obj_see ) \n 
~~ def test_float ( self ) : \n 
~~~ obj = 1.0 \n 
lit_see = see . see ( 1.0 ) \n 
~~ import click \n 
import requests \n 
from soccer import leagueids \n 
from soccer . exceptions import IncorrectParametersException , APIErrorException \n 
from soccer . writers import get_writer \n 
BASE_URL = \n 
LIVE_URL = \n 
LEAGUE_IDS = leagueids . LEAGUE_IDS \n 
def load_json ( file ) : \n 
here = os . path . dirname ( os . path . abspath ( __file__ ) ) \n 
with open ( os . path . join ( here , file ) ) as jfile : \n 
~~~ data = json . load ( jfile ) \n 
~~ TEAM_DATA = load_json ( "teams.json" ) [ "teams" ] \n 
TEAM_NAMES = { team [ "code" ] : team [ "id" ] for team in TEAM_DATA } \n 
def get_input_key ( ) : \n 
fg = "yellow" , bold = True ) \n 
fg = "yellow" , bold = True ) ) \n 
~~ ~~ return confkey \n 
~~ def load_config_key ( ) : \n 
global api_token \n 
~~~ api_token = os . environ [ ] \n 
~~~ home = os . path . expanduser ( "~" ) \n 
config = os . path . join ( home , "soccer-cli.ini" ) \n 
if not os . path . exists ( config ) : \n 
~~~ with open ( config , "w" ) as cfile : \n 
~~~ key = get_input_key ( ) \n 
cfile . write ( key ) \n 
~~~ with open ( config , "r" ) as cfile : \n 
~~~ key = cfile . read ( ) \n 
~~ ~~ if key : \n 
~~~ api_token = key \n 
click . secho ( \n 
. format ( BASE_URL ) , fg = "red" , bold = True ) \n 
~~ ~~ return api_token \n 
~~ def _get ( url ) : \n 
req = requests . get ( BASE_URL + url , headers = headers ) \n 
if req . status_code == requests . codes . ok : \n 
~~~ return req \n 
~~ if req . status_code == requests . codes . bad : \n 
~~~ raise APIErrorException ( ) \n 
~~ if req . status_code == requests . codes . forbidden : \n 
~~ if req . status_code == requests . codes . not_found : \n 
~~ if req . status_codes == requests . codes . too_many_requests : \n 
~~ ~~ def get_live_scores ( writer , use_12_hour_format ) : \n 
req = requests . get ( LIVE_URL ) \n 
~~~ scores = req . json ( ) \n 
if len ( scores [ "games" ] ) == 0 : \n 
~~ writer . live_scores ( scores , use_12_hour_format ) \n 
~~ ~~ def get_team_scores ( team , time , writer , show_upcoming , use_12_hour_format ) : \n 
team_id = TEAM_NAMES . get ( team , None ) \n 
time_frame = if show_upcoming else \n 
if team_id : \n 
~~~ req = _get ( . format ( \n 
team_id = team_id , time_frame = time_frame , time = time ) ) \n 
team_scores = req . json ( ) \n 
if len ( team_scores [ "fixtures" ] ) == 0 : \n 
~~~ writer . team_scores ( team_scores , time , show_upcoming , use_12_hour_format ) \n 
~~ ~~ except APIErrorException as e : \n 
~~~ click . secho ( e . args [ 0 ] , \n 
fg = "red" , bold = True ) \n 
~~ ~~ def get_standings ( league , writer ) : \n 
league_id = LEAGUE_IDS [ league ] \n 
id = league_id ) ) \n 
writer . standings ( req . json ( ) , league ) \n 
~~ except APIErrorException : \n 
~~ ~~ def get_league_scores ( league , time , writer , show_upcoming , use_12_hour_format ) : \n 
if league : \n 
~~~ league_id = LEAGUE_IDS [ league ] \n 
req = _get ( . format ( \n 
id = league_id , time_frame = time_frame , time = str ( time ) ) ) \n 
fixtures_results = req . json ( ) \n 
if len ( fixtures_results [ "fixtures" ] ) == 0 : \n 
~~ writer . league_scores ( fixtures_results , time , show_upcoming , use_12_hour_format ) \n 
time_frame = time_frame , time = str ( time ) ) ) \n 
writer . league_scores ( fixtures_results , time , show_upcoming , use_12_hour_format ) \n 
~~ ~~ ~~ def get_team_players ( team , writer ) : \n 
team_id = team_id ) ) \n 
team_players = req . json ( ) \n 
if int ( team_players [ "count" ] ) == 0 : \n 
~~~ writer . team_players ( team_players ) \n 
~~ ~~ except APIErrorException : \n 
~~ ~~ def map_team_id ( code ) : \n 
for team in TEAM_DATA : \n 
~~~ if team [ "code" ] == code : \n 
~~~ click . secho ( team [ "name" ] , fg = "green" ) \n 
~~ ~~ def list_team_codes ( ) : \n 
cleanlist = sorted ( TEAM_DATA , key = lambda k : ( k [ "league" ] [ "name" ] , k [ "code" ] ) ) \n 
leaguenames = sorted ( list ( set ( [ team [ "league" ] [ "name" ] for team in cleanlist ] ) ) ) \n 
for league in leaguenames : \n 
~~~ teams = [ team for team in cleanlist if team [ "league" ] [ "name" ] == league ] \n 
click . secho ( league , fg = "green" , bold = True ) \n 
for team in teams : \n 
~~~ if team [ "code" ] != "null" : \n 
~~ ~~ click . secho ( "" ) \n 
~~ ~~ @ click . command ( ) \n 
@ click . option ( , , type = click . Choice ( LEAGUE_IDS . keys ( ) ) , \n 
@ click . option ( , type = click . Choice ( TEAM_NAMES . keys ( ) ) , \n 
@ click . option ( , , flag_value = , \n 
@ click . option ( , , default = None , \n 
global headers \n 
headers = { \n 
: apikey \n 
~~~ if output_format == and output_file : \n 
~~~ raise IncorrectParametersException ( \n 
~~ writer = get_writer ( output_format , output_file ) \n 
if listcodes : \n 
~~~ list_team_codes ( ) \n 
~~ if live : \n 
~~~ get_live_scores ( writer , use12hour ) \n 
~~ if standings : \n 
~~~ if not league : \n 
~~ get_standings ( league , writer ) \n 
~~ if team : \n 
~~~ if lookup : \n 
~~~ map_team_id ( team ) \n 
~~ if players : \n 
~~~ get_team_players ( team , writer ) \n 
~~~ get_team_scores ( team , time , writer , upcoming , use12hour ) \n 
~~ ~~ get_league_scores ( league , time , writer , upcoming , use12hour ) \n 
~~ except IncorrectParametersException as e : \n 
~~~ click . secho ( e . message , fg = "red" , bold = True ) \n 
#======================================================================= \n 
~~ import Errors \n 
from Regexps import BOL , EOL , EOF \n 
class Scanner : \n 
buffer = \n 
trace = 0 \n 
def __init__ ( self , lexicon , stream , name = ) : \n 
self . lexicon = lexicon \n 
self . stream = stream \n 
self . queue = [ ] \n 
self . initial_state = None \n 
self . begin ( ) \n 
self . next_pos = 0 \n 
self . cur_pos = 0 \n 
self . cur_line_start = 0 \n 
self . cur_char = BOL \n 
self . input_state = 1 \n 
~~ def read ( self ) : \n 
queue = self . queue \n 
while not queue : \n 
~~~ self . text , action = self . scan_a_token ( ) \n 
if action is None : \n 
~~~ self . produce ( None ) \n 
self . eof ( ) \n 
~~~ value = action . perform ( self , self . text ) \n 
~~~ self . produce ( value ) \n 
~~ ~~ ~~ result = queue [ 0 ] \n 
del queue [ 0 ] \n 
~~ def scan_a_token ( self ) : \n 
self . start_pos = self . cur_pos \n 
self . start_line = self . cur_line \n 
self . start_col = self . cur_pos - self . cur_line_start \n 
#\t\telse: \n 
action = self . run_machine_inlined ( ) \n 
~~~ if self . trace : \n 
self . start_pos , self . cur_pos ) \n 
~~ base = self . buf_start_pos \n 
text = self . buffer [ self . start_pos - base : self . cur_pos - base ] \n 
return ( text , action ) \n 
~~~ if self . cur_pos == self . start_pos : \n 
~~~ if self . cur_char == EOL : \n 
~~~ self . next_char ( ) \n 
~~ if not self . cur_char or self . cur_char == EOF : \n 
~~~ return ( , None ) \n 
~~ ~~ raise Errors . UnrecognizedInput ( self , self . state_name ) \n 
~~ ~~ def run_machine ( self ) : \n 
self . state = self . initial_state \n 
self . backup_state = None \n 
while self . transition ( ) : \n 
~~ return self . back_up ( ) \n 
~~ def run_machine_inlined ( self ) : \n 
state = self . initial_state \n 
cur_pos = self . cur_pos \n 
cur_line = self . cur_line \n 
cur_line_start = self . cur_line_start \n 
cur_char = self . cur_char \n 
input_state = self . input_state \n 
next_pos = self . next_pos \n 
buffer = self . buffer \n 
buf_start_pos = self . buf_start_pos \n 
buf_len = len ( buffer ) \n 
backup_state = None \n 
trace = self . trace \n 
~~~ if trace : #TRACE# \n 
state [ ] , input_state , cur_pos , repr ( cur_char ) ) , #TRACE# \n 
~~ action = state [ ] #@fast \n 
~~~ backup_state = ( \n 
action , cur_pos , cur_line , cur_line_start , cur_char , input_state , next_pos ) \n 
~~ c = cur_char \n 
new_state = state . get ( c , - 1 ) #@fast \n 
if new_state == - 1 : #@fast \n 
~~~ new_state = c and state . get ( ) #@fast \n 
~~ if new_state : \n 
~~ state = new_state \n 
if input_state == 1 : \n 
~~~ cur_pos = next_pos \n 
buf_index = next_pos - buf_start_pos \n 
if buf_index < buf_len : \n 
~~~ c = buffer [ buf_index ] \n 
next_pos = next_pos + 1 \n 
~~~ discard = self . start_pos - buf_start_pos \n 
data = self . stream . read ( 0x1000 ) \n 
buffer = self . buffer [ discard : ] + data \n 
self . buffer = buffer \n 
buf_start_pos = buf_start_pos + discard \n 
self . buf_start_pos = buf_start_pos \n 
buf_index = buf_index - discard \n 
~~~ c = \n 
~~ ~~ if c == : \n 
~~~ cur_char = EOL \n 
input_state = 2 \n 
~~ elif not c : \n 
input_state = 4 \n 
~~~ cur_char = c \n 
~~ ~~ elif input_state == 2 : \n 
~~~ cur_char = \n 
input_state = 3 \n 
~~ elif input_state == 3 : \n 
~~~ cur_line = cur_line + 1 \n 
cur_line_start = cur_pos = next_pos \n 
cur_char = BOL \n 
input_state = 1 \n 
~~ elif input_state == 4 : \n 
~~~ cur_char = EOF \n 
input_state = 5 \n 
~~~ print "blocked" #TRACE# \n 
~~ if backup_state : \n 
~~~ ( action , cur_pos , cur_line , cur_line_start , \n 
cur_char , input_state , next_pos ) = backup_state \n 
~~~ action = None \n 
~~ ~~ self . cur_pos = cur_pos \n 
self . cur_line = cur_line \n 
self . cur_line_start = cur_line_start \n 
self . cur_char = cur_char \n 
self . input_state = input_state \n 
self . next_pos = next_pos \n 
if trace : #TRACE# \n 
~~~ if action : #TRACE# \n 
~~~ print "Doing" , action #TRACE# \n 
~~ ~~ return action \n 
#\t\tself.save_for_backup() \n 
#\t\t\tself.next_char() \n 
~~ def next_char ( self ) : \n 
~~~ input_state = self . input_state \n 
if self . trace : \n 
~~ if input_state == 1 : \n 
~~~ self . cur_pos = self . next_pos \n 
c = self . read_char ( ) \n 
if c == : \n 
~~~ self . cur_char = EOL \n 
self . input_state = 2 \n 
self . input_state = 4 \n 
~~~ self . cur_char = c \n 
~~~ self . cur_char = \n 
self . input_state = 3 \n 
~~~ self . cur_line = self . cur_line + 1 \n 
self . cur_line_start = self . cur_pos = self . next_pos \n 
~~~ self . cur_char = EOF \n 
self . input_state = 5 \n 
~~ if self . trace : \n 
#\t\t""" \n 
~~ ~~ def position ( self ) : \n 
return ( self . name , self . start_line , self . start_col ) \n 
~~ def begin ( self , state_name ) : \n 
self . initial_state = ( \n 
self . lexicon . get_initial_state ( state_name ) ) \n 
self . state_name = state_name \n 
~~ def produce ( self , value , text = None ) : \n 
if text is None : \n 
~~~ text = self . text \n 
~~ self . queue . append ( ( value , text ) ) \n 
~~ def eof ( self ) : \n 
~~ ~~ setattr ( Scanner , "yield" , Scanner . produce ) \n 
__version__ = \n 
import base_layout \n 
filename = os . path . join ( os . path . split ( __file__ ) [ 0 ] , ) \n 
class InlineReplacedTest ( base_layout . LayoutTestBase ) : \n 
from render_base import RenderBase \n 
import scene2d \n 
from scene2d . debug import gen_rect_map \n 
class OOBTest ( RenderBase ) : \n 
~~~ def test_main ( self ) : \n 
~~~ self . init_window ( 256 , 256 ) \n 
self . set_map ( gen_rect_map ( [ [ { } ] * 10 ] * 10 , 32 , 32 ) ) \n 
def on_text ( text ) : \n 
~~~ if text == : \n 
~~~ self . view . allow_oob = not self . view . allow_oob \n 
print , self . view . allow_oob \n 
~~ return pyglet . window . event . EVENT_UNHANDLED \n 
~~ print , self . view . allow_oob \n 
self . w . push_handlers ( on_text ) \n 
self . show_focus ( ) \n 
self . run_test ( ) \n 
~~ import math \n 
import pyglet \n 
from pyglet . gl import * \n 
class SmoothLineGroup ( pyglet . graphics . Group ) : \n 
~~~ def set_state ( self ) : \n 
~~~ glPushAttrib ( GL_ENABLE_BIT ) \n 
glEnable ( GL_BLEND ) \n 
glBlendFunc ( GL_SRC_ALPHA , GL_ONE_MINUS_SRC_ALPHA ) \n 
glEnable ( GL_LINE_SMOOTH ) \n 
glLineWidth ( 2 ) \n 
~~ def unset_state ( self ) : \n 
~~~ glPopAttrib ( ) \n 
~~~ return hash ( self . __class__ . __name__ ) \n 
~~~ return self . __class__ is other . __class__ \n 
~~ ~~ def add_circle ( batch , x , y , radius , color , num_points = 20 , antialised = True ) : \n 
for n in range ( num_points ) : \n 
~~~ angle = ( math . pi * 2 * n ) / num_points \n 
l . append ( int ( x + radius * math . cos ( angle ) ) ) \n 
l . append ( int ( y + radius * math . sin ( angle ) ) ) \n 
~~ l . append ( int ( x + radius * 1 ) ) \n 
l . append ( int ( y ) ) \n 
num_points += 3 \n 
l [ 0 : 0 ] = l [ 0 : 2 ] \n 
l . extend ( l [ - 2 : ] ) \n 
if antialised : \n 
~~~ group = SmoothLineGroup ( ) \n 
~~~ group = None \n 
~~ return batch . add ( num_points , GL_LINE_STRIP , group , ( , l ) , \n 
( , color * num_points ) ) \n 
import xml . sax . saxutils \n 
from pyglet . window import mouse , key \n 
from wydget import event , layouts , loadxml \n 
from wydget . widgets . frame import Frame \n 
from wydget . widgets . label import Label \n 
class MenuItem ( Label ) : \n 
~~ @ event . default ( ) \n 
def on_element_enter ( item , x , y ) : \n 
~~~ item . _save_bgcolor = item . bgcolor \n 
item . bgcolor = ( .9 , .9 , 1 , 1 ) \n 
return event . EVENT_HANDLED \n 
def on_element_leave ( item , x , y ) : \n 
~~~ item . bgcolor = item . _save_bgcolor \n 
def on_click ( widget , * args ) : \n 
~~~ menu = widget . parent \n 
menu . hide ( ) \n 
~~ class PopupMenu ( Frame ) : \n 
is_focusable = True \n 
def __init__ ( self , parent , items , ** kw ) : \n 
~~~ super ( PopupMenu , self ) . __init__ ( parent , border = "black" , \n 
is_visible = False , ** kw ) \n 
for n , ( label , id ) in enumerate ( items ) : \n 
~~~ MenuItem ( self , text = label , id = id , width = , \n 
bgcolor = ( ( .95 , .95 , .95 , 1 ) , ( 1 , 1 , 1 , 1 ) ) [ n % 2 ] ) \n 
~~ self . layout = layouts . Vertical ( self ) \n 
~~ def expose ( self , mouse ) : \n 
~~~ w = self . getGUI ( ) . window \n 
w , h = w . width , w . height \n 
self . center = map ( int , mouse ) \n 
if self . x < 0 : self . x = 0 \n 
if self . y < 0 : self . y = 0 \n 
if self . x + self . width > w : self . x = w - self . width \n 
if self . y + self . height > h : self . y = h - self . height \n 
self . setVisible ( True ) \n 
self . gainFocus ( ) \n 
~~ def hide ( self ) : \n 
~~~ self . setVisible ( False ) \n 
def fromXML ( cls , element , parent ) : \n 
kw = loadxml . parseAttributes ( element ) \n 
items = [ ] \n 
for child in element . getchildren ( ) : \n 
~~~ text = xml . sax . saxutils . unescape ( child . text ) \n 
items . append ( ( text , child . attrib [ ] ) ) \n 
~~ return cls ( parent , items , ** kw ) \n 
def isActivatingClick ( button , modifiers ) : \n 
if sys . platform == : \n 
~~~ if button & mouse . LEFT and modifiers & key . MOD_CTRL : \n 
~~ ~~ return button & mouse . RIGHT \n 
~~ ~~ @ event . default ( , ) \n 
def on_menu_gain_focus ( menu , method ) : \n 
~~~ return event . EVENT_HANDLED \n 
~~ @ event . default ( , ) \n 
def on_menu_lose_focus ( menu , method ) : \n 
~~~ menu . hide ( ) \n 
window = pyglet . window . Window ( ) \n 
label = pyglet . text . Label ( , \n 
font_name = , \n 
font_size = 36 , \n 
x = window . width // 2 , y = window . height // 2 , \n 
anchor_x = , anchor_y = ) \n 
@ window . event \n 
def on_draw ( ) : \n 
~~~ window . clear ( ) \n 
label . draw ( ) \n 
~~ pyglet . app . run ( ) \n 
from pyglet . window . xlib import xlib \n 
from pyglet . gl . glx import * \n 
def reshape ( width , height ) : \n 
~~~ h = height / float ( width ) \n 
glViewport ( 0 , 0 , width , height ) \n 
glMatrixMode ( GL_PROJECTION ) \n 
glLoadIdentity ( ) \n 
glFrustum ( - 1.0 , 1.0 , - h , h , 5.0 , 60.0 ) \n 
glMatrixMode ( GL_MODELVIEW ) \n 
glTranslatef ( 0.0 , 0.0 , - 40.0 ) \n 
~~ def draw ( ) : \n 
~~~ glClearColor ( 1 , 1 , 0 , 1 ) \n 
glClear ( GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT ) \n 
~~ def init ( ) : \n 
~~ def make_window ( dpy , name , x , y , width , height ) : \n 
~~~ attrib = ( ctypes . c_int * 11 ) ( \n 
GLX_RGBA , \n 
GLX_RED_SIZE , 1 , \n 
GLX_GREEN_SIZE , 1 , \n 
GLX_BLUE_SIZE , 1 , \n 
GLX_DOUBLEBUFFER , \n 
GLX_DEPTH_SIZE , 1 , \n 
0 ) \n 
scrnum = xlib . XDefaultScreen ( dpy ) \n 
root = xlib . XRootWindow ( dpy , scrnum ) \n 
visinfo = glXChooseVisual ( dpy , scrnum , attrib ) \n 
attr = xlib . XSetWindowAttributes ( ) \n 
attr . background_pixel = 0 \n 
attr . border_pixel = 0 \n 
attr . colormap = xlib . XCreateColormap ( dpy , root , visinfo . contents . visual , \n 
xlib . AllocNone ) \n 
attr . event_mask = ( xlib . StructureNotifyMask | \n 
xlib . ExposureMask | \n 
xlib . KeyPressMask ) \n 
mask = ( xlib . CWBackPixel | \n 
xlib . CWBorderPixel | \n 
xlib . CWColormap | \n 
xlib . CWEventMask ) \n 
win = xlib . XCreateWindow ( dpy , root , 0 , 0 , width , height , 0 , \n 
visinfo . contents . depth , xlib . InputOutput , \n 
visinfo . contents . visual , mask , attr ) \n 
sizehints = xlib . XSizeHints ( ) \n 
sizehints . x = x \n 
sizehints . y = y \n 
sizehints . width = width \n 
sizehints . height = height \n 
sizehints . flags = xlib . USSize | xlib . USPosition \n 
xlib . XSetNormalHints ( dpy , win , sizehints ) \n 
xlib . XSetStandardProperties ( dpy , win , name , name , 0 , None , 0 , sizehints ) \n 
ctx = glXCreateContext ( dpy , visinfo , None , True ) \n 
xlib . XFree ( visinfo ) \n 
return win , ctx \n 
~~ def event_loop ( dpy , win ) : \n 
~~~ while True : \n 
~~~ while xlib . XPending ( dpy ) > 0 : \n 
~~~ event = xlib . XEvent ( ) \n 
xlib . XNextEvent ( dpy , event ) \n 
if event . type == xlib . Expose : \n 
~~ elif event . type == xlib . ConfigureNotify : \n 
~~~ reshape ( event . xconfigure . width , event . xconfigure . height ) \n 
~~ elif event . type == xlib . KeyPress : \n 
~~ ~~ draw ( ) \n 
glXSwapBuffers ( dpy , win ) \n 
~~ ~~ def main ( ) : \n 
~~~ dpy = xlib . XOpenDisplay ( None ) \n 
win , ctx = make_window ( dpy , , 0 , 0 , 300 , 300 ) \n 
xlib . XMapWindow ( dpy , win ) \n 
glXMakeCurrent ( dpy , win , ctx ) \n 
reshape ( 300 , 300 ) \n 
init ( ) \n 
event_loop ( dpy , win ) \n 
glXDestroyContext ( dpy , ctx ) \n 
xlib . XDestroyWindow ( dpy , win ) \n 
xlib . XCloseDisplay ( dpy ) \n 
~~ import ctypes \n 
import heapq \n 
import Queue \n 
_debug = pyglet . options [ ] \n 
import mt_media \n 
import lib_openal as al \n 
import lib_alc as alc \n 
class OpenALException ( mt_media . MediaException ) : \n 
~~ def _split_nul_strings ( s ) : \n 
~~~ nul = False \n 
~~~ if s [ i ] == : \n 
~~~ if nul : \n 
~~~ nul = True \n 
~~ i += 1 \n 
~~ s = s [ : i - 1 ] \n 
return s . split ( ) \n 
~~ def get_extensions ( ) : \n 
~~~ extensions = alc . alcGetString ( context . _device , alc . ALC_EXTENSIONS ) \n 
~~~ return ctypes . cast ( extensions , ctypes . c_char_p ) . value . split ( ) \n 
~~~ return _split_nul_strings ( extensions ) \n 
~~ ~~ def have_extension ( extension ) : \n 
~~~ return extension in get_extensions ( ) \n 
~~ format_map = { \n 
( 1 , 8 ) : al . AL_FORMAT_MONO8 , \n 
( 1 , 16 ) : al . AL_FORMAT_MONO16 , \n 
( 2 , 8 ) : al . AL_FORMAT_STEREO8 , \n 
( 2 , 16 ) : al . AL_FORMAT_STEREO16 , \n 
class OpenALWorker ( mt_media . MediaThread ) : \n 
~~~ _min_write_size = 512 \n 
_nap_time = 0.05 \n 
_sleep_time = None \n 
~~~ super ( OpenALWorker , self ) . __init__ ( ) \n 
self . players = set ( ) \n 
~~~ self . condition . acquire ( ) \n 
if self . stopped : \n 
~~~ self . condition . release ( ) \n 
~~ sleep_time = - 1 \n 
if self . players : \n 
~~~ player = None \n 
write_size = 0 \n 
for p in self . players : \n 
~~~ s = p . get_write_size ( ) \n 
if s > write_size : \n 
~~~ player = p \n 
write_size = s \n 
~~ ~~ if write_size > self . _min_write_size : \n 
~~~ player . refill ( write_size ) \n 
~~~ sleep_time = self . _nap_time \n 
~~~ sleep_time = self . _sleep_time \n 
~~ self . condition . release ( ) \n 
if sleep_time != - 1 : \n 
~~~ self . sleep ( sleep_time ) \n 
~~ ~~ ~~ def add ( self , player ) : \n 
self . players . add ( player ) \n 
self . condition . notify ( ) \n 
self . condition . release ( ) \n 
~~ def remove ( self , player ) : \n 
self . players . remove ( player ) \n 
~~ ~~ class OpenALAudioPlayer ( mt_media . AbstractAudioPlayer ) : \n 
~~~ _min_buffer_size = 512 \n 
_ideal_buffer_size = 44800 \n 
def __init__ ( self , source_group , player ) : \n 
~~~ super ( OpenALAudioPlayer , self ) . __init__ ( source_group , player ) \n 
audio_format = source_group . audio_format \n 
~~~ self . _al_format = format_map [ ( audio_format . channels , \n 
audio_format . sample_size ) ] \n 
~~~ raise OpenALException ( ) \n 
~~ self . _al_source = al . ALuint ( ) \n 
al . alGenSources ( 1 , self . _al_source ) \n 
self . _lock = threading . RLock ( ) \n 
self . _buffer_cursor = 0 \n 
self . _play_cursor = 0 \n 
self . _write_cursor = 0 \n 
self . _buffer_sizes = [ ] \n 
self . _buffer_timestamps = [ ] \n 
self . _underrun_timestamp = None \n 
self . _events = [ ] \n 
self . _playing = False \n 
self . _eos = False \n 
if not context . have_1_1 : \n 
~~~ self . _buffer_system_time = time . time ( ) \n 
~~ self . refill ( self . _ideal_buffer_size ) \n 
~~~ self . delete ( ) \n 
context . lock ( ) \n 
al . alDeleteSources ( 1 , self . _al_source ) \n 
context . unlock ( ) \n 
self . _al_source = None \n 
~~ def play ( self ) : \n 
~~~ if self . _playing : \n 
~~ self . _playing = True \n 
self . _al_play ( ) \n 
~~ context . worker . add ( self ) \n 
~~ def _al_play ( self ) : \n 
~~~ context . lock ( ) \n 
state = al . ALint ( ) \n 
al . alGetSourcei ( self . _al_source , al . AL_SOURCE_STATE , state ) \n 
if state . value != al . AL_PLAYING : \n 
~~~ al . alSourcePlay ( self . _al_source ) \n 
~~ context . unlock ( ) \n 
~~ def stop ( self ) : \n 
~~~ if not self . _playing : \n 
~~ self . _pause_timestamp = self . get_time ( ) \n 
al . alSourcePause ( self . _al_source ) \n 
context . worker . remove ( self ) \n 
~~ def clear ( self ) : \n 
~~~ self . _lock . acquire ( ) \n 
al . alSourceStop ( self . _al_source ) \n 
del self . _events [ : ] \n 
self . _buffer_timestamps = [ None for _ in self . _buffer_timestamps ] \n 
self . _lock . release ( ) \n 
~~ def _update_play_cursor ( self ) : \n 
processed = al . ALint ( ) \n 
al . alGetSourcei ( self . _al_source , al . AL_BUFFERS_PROCESSED , processed ) \n 
processed = processed . value \n 
if processed : \n 
~~~ buffers = ( al . ALuint * processed ) ( ) \n 
al . alSourceUnqueueBuffers ( self . _al_source , len ( buffers ) , buffers ) \n 
al . alDeleteBuffers ( len ( buffers ) , buffers ) \n 
~~~ if len ( self . _buffer_timestamps ) == processed : \n 
~~~ self . _underrun_timestamp = self . _buffer_timestamps [ - 1 ] + self . _buffer_sizes [ - 1 ] / float ( self . source_group . audio_format . bytes_per_second ) \n 
~~ self . _buffer_cursor += sum ( self . _buffer_sizes [ : processed ] ) \n 
del self . _buffer_sizes [ : processed ] \n 
del self . _buffer_timestamps [ : processed ] \n 
~~ ~~ if context . have_1_1 : \n 
~~~ bytes = al . ALint ( ) \n 
al . alGetSourcei ( self . _al_source , al . AL_BYTE_OFFSET , bytes ) \n 
if _debug : \n 
~~~ print , bytes . value \n 
~~ self . _play_cursor = self . _buffer_cursor + bytes . value \n 
~~~ self . _play_cursor = self . _buffer_cursor + int ( \n 
( time . time ( ) - self . _buffer_system_time ) * self . source_group . audio_format . bytes_per_second ) \n 
~~ while self . _events and self . _events [ 0 ] [ 0 ] < self . _play_cursor : \n 
~~~ _ , event = self . _events . pop ( 0 ) \n 
event . _sync_dispatch_to_player ( self . player ) \n 
~~ self . _lock . release ( ) \n 
~~ def get_write_size ( self ) : \n 
self . _update_play_cursor ( ) \n 
write_size = self . _ideal_buffer_size - ( self . _write_cursor - self . _play_cursor ) \n 
if self . _eos : \n 
~~~ write_size = 0 \n 
return write_size \n 
~~ def refill ( self , write_size ) : \n 
~~~ if _debug : \n 
~~~ print , write_size \n 
~~ self . _lock . acquire ( ) \n 
while write_size > self . _min_buffer_size : \n 
~~~ audio_data = self . source_group . get_audio_data ( write_size ) \n 
if not audio_data : \n 
~~~ self . _eos = True \n 
self . _events . append ( \n 
( self . _write_cursor , mt_media . MediaEvent ( 0 , ) ) ) \n 
( self . _write_cursor , \n 
mt_media . MediaEvent ( 0 , ) ) ) \n 
~~ for event in audio_data . events : \n 
~~~ cursor = self . _write_cursor + event . timestamp * self . source_group . audio_format . bytes_per_second \n 
self . _events . append ( ( cursor , event ) ) \n 
~~ buffer = al . ALuint ( ) \n 
al . alGenBuffers ( 1 , buffer ) \n 
al . alBufferData ( buffer , \n 
self . _al_format , \n 
audio_data . data , \n 
audio_data . length , \n 
self . source_group . audio_format . sample_rate ) \n 
al . alSourceQueueBuffers ( self . _al_source , 1 , ctypes . byref ( buffer ) ) \n 
self . _write_cursor += audio_data . length \n 
self . _buffer_sizes . append ( audio_data . length ) \n 
self . _buffer_timestamps . append ( audio_data . timestamp ) \n 
write_size -= audio_data . length \n 
~~ if self . _playing : \n 
~~~ state = al . ALint ( ) \n 
~~ al . alSourcePlay ( self . _al_source ) \n 
~~ def get_time ( self ) : \n 
~~~ buffer_timestamp = self . _buffer_timestamps [ 0 ] \n 
~~~ return self . _underrun_timestamp \n 
~~ if buffer_timestamp is None : \n 
~~ return buffer_timestamp + ( self . _play_cursor - self . _buffer_cursor ) / float ( self . source_group . audio_format . bytes_per_second ) \n 
~~ def set_volume ( self , volume ) : \n 
al . alSourcef ( self . _al_source , al . AL_GAIN , max ( 0 , volume ) ) \n 
~~ def set_position ( self , position ) : \n 
~~~ x , y , z = position \n 
al . alSource3f ( self . _al_source , al . AL_POSITION , x , y , z ) \n 
~~ def set_min_distance ( self , min_distance ) : \n 
al . alSourcef ( self . _al_source , al . AL_REFERENCE_DISTANCE , min_distance ) \n 
~~ def set_max_distance ( self , max_distance ) : \n 
al . alSourcef ( self . _al_source , al . AL_MAX_DISTANCE , max_distance ) \n 
~~ def set_pitch ( self , pitch ) : \n 
al . alSourcef ( self . _al_source , al . AL_PITCH , max ( 0 , pitch ) ) \n 
~~ def set_cone_orientation ( self , cone_orientation ) : \n 
~~~ x , y , z = cone_orientation \n 
al . alSource3f ( self . _al_source , al . AL_DIRECTION , x , y , z ) \n 
~~ def set_cone_inner_angle ( self , cone_inner_angle ) : \n 
al . alSourcef ( self . _al_source , al . AL_CONE_INNER_ANGLE , cone_inner_angle ) \n 
~~ def set_cone_outer_angle ( self , cone_outer_angle ) : \n 
al . alSourcef ( self . _al_source , al . AL_CONE_OUTER_ANGLE , cone_outer_angle ) \n 
~~ def set_cone_outer_gain ( self , cone_outer_gain ) : \n 
al . alSourcef ( self . _al_source , al . AL_CONE_OUTER_GAIN , cone_outer_gain ) \n 
~~ ~~ class OpenALDriver ( mt_media . AbstractAudioDriver ) : \n 
~~~ def __init__ ( self , device_name = None ) : \n 
~~~ super ( OpenALDriver , self ) . __init__ ( ) \n 
self . _device = alc . alcOpenDevice ( device_name ) \n 
if not self . _device : \n 
~~~ raise Exception ( ) \n 
~~ alcontext = alc . alcCreateContext ( self . _device , None ) \n 
alc . alcMakeContextCurrent ( alcontext ) \n 
self . have_1_1 = self . have_version ( 1 , 1 ) and False \n 
self . _lock = threading . Lock ( ) \n 
self . worker = OpenALWorker ( ) \n 
self . worker . start ( ) \n 
~~ def create_audio_player ( self , source_group , player ) : \n 
~~~ return OpenALAudioPlayer ( source_group , player ) \n 
~~~ self . worker . stop ( ) \n 
~~ def lock ( self ) : \n 
~~ def unlock ( self ) : \n 
~~~ self . _lock . release ( ) \n 
~~ def have_version ( self , major , minor ) : \n 
~~~ return ( major , minor ) <= self . get_version ( ) \n 
~~ def get_version ( self ) : \n 
~~~ major = alc . ALCint ( ) \n 
minor = alc . ALCint ( ) \n 
alc . alcGetIntegerv ( self . _device , alc . ALC_MAJOR_VERSION , \n 
ctypes . sizeof ( major ) , major ) \n 
alc . alcGetIntegerv ( self . _device , alc . ALC_MINOR_VERSION , \n 
ctypes . sizeof ( minor ) , minor ) \n 
return major . value , minor . value \n 
~~ def _set_volume ( self , volume ) : \n 
~~~ self . lock ( ) \n 
al . alListenerf ( al . AL_GAIN , volume ) \n 
self . unlock ( ) \n 
self . _volume = volume \n 
~~ def _set_position ( self , position ) : \n 
self . lock ( ) \n 
al . alListener3f ( al . AL_POSITION , x , y , z ) \n 
self . _position = position \n 
~~ def _set_forward_orientation ( self , orientation ) : \n 
~~~ val = ( al . ALfloat * 6 ) ( * ( orientation + self . _up_orientation ) ) \n 
al . alListenerfv ( al . AL_ORIENTATION , val ) \n 
self . _forward_orientation = orientation \n 
~~ def _set_up_orientation ( self , orientation ) : \n 
~~~ val = ( al . ALfloat * 6 ) ( * ( self . _forward_orientation + orientation ) ) \n 
self . _up_orientation = orientation \n 
~~ ~~ context = None \n 
def create_audio_driver ( device_name = None ) : \n 
~~~ global context \n 
context = OpenALDriver ( device_name ) \n 
~~~ print , context . get_version ( ) \n 
~~ return context \n 
import ctypes as c \n 
import gzip \n 
import marshal \n 
class AnonymousStruct ( c . Structure ) : \n 
~~~ __slots__ = ( ) \n 
_fields_ = ( \n 
( , c . c_int ) , \n 
~~ class AnonymousUnion ( c . Union ) : \n 
~~ class EnumType ( type ( c . c_int ) ) : \n 
~~~ def __new__ ( metaclass , name , bases , dict ) : \n 
~~~ cls = type ( c . c_int ) . __new__ ( metaclass , name , bases , dict ) \n 
~~ ~~ class Enum ( c . c_int ) : \n 
~~~ __metaclass__ = EnumType \n 
_values_ = ( ) \n 
def from_param ( cls , obj ) : \n 
~~~ assert obj in cls . _values_ \n 
return cls ( obj ) \n 
~~ ~~ class FFILibrary ( object ) : \n 
~~~ def __init__ ( self , ffi_file , lib ) : \n 
~~~ data = gzip . GzipFile ( mode = , fileobj = ffi_file ) . read ( ) \n 
self . lib = lib \n 
self . map = marshal . loads ( data ) \n 
self . type_map = { } \n 
self . forward_map = { } \n 
self . builtin_type_map = { \n 
: None , \n 
: c . c_char , \n 
: c . c_byte , \n 
: c . c_wchar , \n 
: c . c_short , \n 
: c . c_ushort , \n 
: c . c_int , \n 
: c . c_uint , \n 
: c . c_long , \n 
: c . c_ulong , \n 
: c . c_longlong , \n 
: c . c_ulonglong , \n 
: c . c_float , \n 
: c . c_double , \n 
~~ def dump ( self , name ) : \n 
~~~ print self . map [ name ] \n 
~~ def __getattr__ ( self , name ) : \n 
~~~ kind , value = self . map [ name ] \n 
if kind == : \n 
~~~ result = value \n 
~~ elif kind == : \n 
~~~ result = self . get_type ( value ) \n 
~~~ result = self . get_enum_type ( name , value ) \n 
~~~ result = self . get_class_type ( name , value , c . Structure ) \n 
~~~ result = self . get_class_type ( name , value , c . Union ) \n 
~~~ address = c . addressof ( getattr ( self . lib , name ) ) \n 
result_type = self . get_type ( value ) \n 
result = result_type . from_address ( address ) \n 
~~~ raise RuntimeError ( kind ) \n 
~~ setattr ( self , name , result ) \n 
~~ def get_type ( self , parts ) : \n 
~~~ return self . type_map [ parts ] \n 
~~ result = self . get_base_type ( parts [ 0 ] ) \n 
for part in parts [ 1 : ] : \n 
~~~ if part == : \n 
~~~ result = c . POINTER ( result ) \n 
~~ elif type ( part ) is tuple : \n 
~~~ if part [ 0 ] == : \n 
~~~ result = self . get_function_type ( result , part [ 1 : ] ) \n 
~~ elif part [ 0 ] == : \n 
~~~ result = self . get_array_type ( result , part [ 1 : ] ) \n 
~~~ raise RuntimeError ( part ) \n 
~~ ~~ self . type_map [ parts ] = result \n 
~~ def get_base_type ( self , name ) : \n 
~~~ return self . type_map [ name ] \n 
~~~ return self . builtin_type_map [ name ] \n 
~~~ return self . forward_map [ name ] \n 
~~~ if name . startswith ( ) : \n 
~~~ return AnonymousStruct \n 
~~ elif name . startswith ( ) : \n 
~~~ return AnonymousUnion \n 
~~ ~~ if kind == : \n 
~~~ return self . get_type ( value ) \n 
~~~ return self . get_enum_type ( name , value ) \n 
~~~ return self . get_class_type ( name , value , c . Structure ) \n 
~~~ return self . get_class_type ( name , value , c . Union ) \n 
~~ ~~ def get_enum_type ( self , name , items ) : \n 
~~~ class _FFIEnum ( Enum ) : \n 
~~ _FFIEnum . __name__ = name \n 
for key , value in items : \n 
~~~ setattr ( _FFIEnum , key , value ) \n 
~~ _FFIEnum . _values_ = tuple ( [ value for key , value in items ] ) \n 
return _FFIEnum \n 
~~ def get_class_type ( self , name , fields , base ) : \n 
~~~ slots = tuple ( name for name , _ in fields ) \n 
class _FFIStruct ( base ) : \n 
~~~ __slots__ = slots \n 
~~ _FFIStruct . __name__ = name \n 
self . forward_map [ name ] = _FFIStruct \n 
_FFIStruct . _fields_ = tuple ( ( name , self . get_type ( type ) ) \n 
for ( name , type ) in fields ) \n 
del self . forward_map [ name ] \n 
return _FFIStruct \n 
~~ def get_function_type ( self , restype , params ) : \n 
~~~ params = map ( self . get_type , params ) \n 
return c . CFUNCTYPE ( restype , * params ) \n 
~~ def get_array_type ( self , element_type , dimensions ) : \n 
~~~ result = element_type \n 
for d in dimensions : \n 
~~~ result = element_type * d \n 
~~~ ffi_filename = sys . argv [ 1 ] \n 
libname = sys . argv [ 2 ] \n 
lib = FFILibrary ( open ( ffi_filename , ) , c . cdll . LoadLibrary ( libname ) ) \n 
if len ( sys . argv ) > 3 : \n 
~~~ lib . dump ( sys . argv [ 3 ] ) \n 
~~~ for key in lib . map : \n 
~~~ print key , getattr ( lib , key ) \n 
~~~ getattr ( lib , key ) \n 
~~ except AttributeError , e : \n 
~~~ print e \n 
~~ ~~ ~~ ~~ \n 
from ctypes import * \n 
from ctypes import util \n 
from pyglet . font import base \n 
import pyglet . image \n 
cf = cdll . LoadLibrary ( util . find_library ( ) ) \n 
quartz = cdll . LoadLibrary ( util . find_library ( ) ) \n 
ct = cdll . LoadLibrary ( util . find_library ( ) ) \n 
CFIndex = c_long \n 
UniChar = c_ushort \n 
CGGlyph = c_ushort \n 
if sys . maxint > 2 ** 32 : \n 
~~~ CGFloat = c_double \n 
~~~ CGFloat = c_float \n 
~~ class CFRange ( Structure ) : \n 
~~~ _fields_ = [ ( "location" , CFIndex ) , ( "length" , CFIndex ) ] \n 
~~ class CGPoint ( Structure ) : \n 
~~~ _fields_ = [ ( "x" , CGFloat ) , ( "y" , CGFloat ) ] \n 
~~ class CGSize ( Structure ) : \n 
~~~ _fields_ = [ ( "width" , CGFloat ) , ( "height" , CGFloat ) ] \n 
~~ class CGRect ( Structure ) : \n 
~~~ _fields_ = [ ( "origin" , CGPoint ) , ( "size" , CGSize ) ] \n 
~~ cf . CFDictionaryCreateMutable . restype = c_void_p \n 
cf . CFStringCreateWithCString . restype = c_void_p \n 
cf . CFAttributedStringCreate . restype = c_void_p \n 
cf . CFDataCreate . restype = c_void_p \n 
cf . CFNumberCreate . restype = c_void_p \n 
ct . CTLineCreateWithAttributedString . restype = c_void_p \n 
ct . CTFontGetBoundingRectsForGlyphs . restype = CGRect \n 
ct . CTFontGetAdvancesForGlyphs . restype = c_double \n 
ct . CTFontGetAscent . restype = CGFloat \n 
ct . CTFontGetDescent . restype = CGFloat \n 
ct . CTFontCreateWithGraphicsFont . restype = c_void_p \n 
ct . CTFontCreateWithGraphicsFont . argtypes = [ c_void_p , CGFloat , c_void_p , c_void_p ] \n 
ct . CTFontCopyFamilyName . restype = c_void_p \n 
ct . CTFontCopyFullName . restype = c_void_p \n 
ct . CTFontDescriptorCreateWithAttributes . restype = c_void_p \n 
ct . CTFontCreateWithFontDescriptor . restype = c_void_p \n 
ct . CTFontCreateWithFontDescriptor . argtypes = [ c_void_p , CGFloat , c_void_p ] \n 
quartz . CGColorSpaceCreateDeviceRGB . restype = c_void_p \n 
quartz . CGBitmapContextCreate . restype = c_void_p \n 
quartz . CGBitmapContextCreateImage . restype = c_void_p \n 
quartz . CGImageGetDataProvider . restype = c_void_p \n 
quartz . CGDataProviderCopyData . restype = c_void_p \n 
quartz . CGDataProviderCreateWithCFData . restype = c_void_p \n 
quartz . CGFontCreateWithDataProvider . restype = c_void_p \n 
quartz . CGContextSetTextPosition . argtypes = [ c_void_p , CGFloat , CGFloat ] \n 
quartz . CGFontCreateWithFontName . restype = c_void_p \n 
kCFStringEncodingUTF8 = 0x08000100 \n 
kCFNumberSInt32Type = 3 \n 
kCTFontAttributeName = c_void_p . in_dll ( ct , ) \n 
kCTFontFamilyNameAttribute = c_void_p . in_dll ( ct , ) \n 
kCTFontSymbolicTrait = c_void_p . in_dll ( ct , ) \n 
kCTFontWeightTrait = c_void_p . in_dll ( ct , ) \n 
kCTFontTraitsAttribute = c_void_p . in_dll ( ct , ) \n 
kCTFontItalicTrait = ( 1 << 0 ) \n 
kCTFontBoldTrait = ( 1 << 1 ) \n 
kCGImageAlphaPremultipliedLast = 1 \n 
def CFSTR ( text ) : \n 
~~~ return c_void_p ( cf . CFStringCreateWithCString ( None , text . encode ( ) , kCFStringEncodingUTF8 ) ) \n 
~~ def cfstring_to_string ( cfstring ) : \n 
~~~ length = cf . CFStringGetLength ( cfstring ) \n 
size = cf . CFStringGetMaximumSizeForEncoding ( length , kCFStringEncodingUTF8 ) \n 
buffer = c_buffer ( size + 1 ) \n 
result = cf . CFStringGetCString ( cfstring , buffer , len ( buffer ) , kCFStringEncodingUTF8 ) \n 
~~~ return buffer . value \n 
~~ ~~ class QuartzGlyphRenderer ( base . GlyphRenderer ) : \n 
~~~ def __init__ ( self , font ) : \n 
~~~ super ( QuartzGlyphRenderer , self ) . __init__ ( font ) \n 
self . font = font \n 
~~ def render ( self , text ) : \n 
~~~ ctFont = self . font . ctFont \n 
attributes = c_void_p ( cf . CFDictionaryCreateMutable ( None , 1 , cf . kCFTypeDictionaryKeyCallBacks cf . CFDictionaryAddValue ( attributes , kCTFontAttributeName , ctFont ) \n 
string = c_void_p ( cf . CFAttributedStringCreate ( None , CFSTR ( text ) , attributes ) ) \n 
line = c_void_p ( ct . CTLineCreateWithAttributedString ( string ) ) \n 
cf . CFRelease ( string ) \n 
cf . CFRelease ( attributes ) \n 
count = len ( text ) \n 
chars = ( UniChar * count ) ( * map ( ord , unicode ( text ) ) ) \n 
glyphs = ( CGGlyph * count ) ( ) \n 
ct . CTFontGetGlyphsForCharacters ( ctFont , chars , glyphs , count ) \n 
rect = ct . CTFontGetBoundingRectsForGlyphs ( ctFont , 0 , glyphs , None , count ) \n 
advance = ct . CTFontGetAdvancesForGlyphs ( ctFont , 0 , glyphs , None , count ) \n 
width = max ( int ( math . ceil ( rect . size . width ) + 2 ) , 1 ) \n 
height = max ( int ( math . ceil ( rect . size . height ) + 2 ) , 1 ) \n 
baseline = - int ( math . floor ( rect . origin . y ) ) + 1 \n 
lsb = int ( math . floor ( rect . origin . x ) ) - 1 \n 
advance = int ( round ( advance ) ) \n 
bitsPerComponent = 8 \n 
bytesPerRow = 4 * width \n 
colorSpace = c_void_p ( quartz . CGColorSpaceCreateDeviceRGB ( ) ) \n 
bitmap = c_void_p ( quartz . CGBitmapContextCreate ( \n 
None , \n 
width , \n 
height , \n 
bitsPerComponent , \n 
bytesPerRow , \n 
colorSpace , \n 
kCGImageAlphaPremultipliedLast ) ) \n 
quartz . CGContextSetShouldAntialias ( bitmap , True ) \n 
quartz . CGContextSetTextPosition ( bitmap , - lsb , baseline ) \n 
ct . CTLineDraw ( line , bitmap ) \n 
cf . CFRelease ( line ) \n 
imageRef = c_void_p ( quartz . CGBitmapContextCreateImage ( bitmap ) ) \n 
bytesPerRow = quartz . CGImageGetBytesPerRow ( imageRef ) \n 
dataProvider = c_void_p ( quartz . CGImageGetDataProvider ( imageRef ) ) \n 
imageData = c_void_p ( quartz . CGDataProviderCopyData ( dataProvider ) ) \n 
buffersize = cf . CFDataGetLength ( imageData ) \n 
buffer = ( c_byte * buffersize ) ( ) \n 
byteRange = CFRange ( 0 , buffersize ) \n 
cf . CFDataGetBytes ( imageData , byteRange , buffer ) \n 
quartz . CGImageRelease ( imageRef ) \n 
quartz . CGDataProviderRelease ( imageData ) \n 
cf . CFRelease ( bitmap ) \n 
cf . CFRelease ( colorSpace ) \n 
glyph_image = pyglet . image . ImageData ( width , height , , buffer , bytesPerRow ) \n 
glyph = self . font . create_glyph ( glyph_image ) \n 
glyph . set_bearings ( baseline , lsb , advance ) \n 
t = list ( glyph . tex_coords ) \n 
glyph . tex_coords = t [ 9 : 12 ] + t [ 6 : 9 ] + t [ 3 : 6 ] + t [ : 3 ] \n 
return glyph \n 
~~ ~~ class QuartzFont ( base . Font ) : \n 
~~~ glyph_renderer_class = QuartzGlyphRenderer \n 
_loaded_CGFont_table = { } \n 
def _lookup_font_with_family_and_traits ( self , family , traits ) : \n 
~~~ if family not in self . _loaded_CGFont_table : \n 
~~ fonts = self . _loaded_CGFont_table [ family ] \n 
if not fonts : \n 
~~ if traits in fonts : \n 
~~~ return fonts [ traits ] \n 
~~ for ( t , f ) in fonts . items ( ) : \n 
~~~ if traits & t : \n 
~~~ return f \n 
~~ ~~ if 0 in fonts : \n 
~~~ return fonts [ 0 ] \n 
~~ return fonts . values ( ) [ 0 ] \n 
~~ def _create_font_descriptor ( self , family_name , traits ) : \n 
cfname = CFSTR ( family_name ) \n 
cf . CFDictionaryAddValue ( attributes , kCTFontFamilyNameAttribute , cfname ) \n 
cf . CFRelease ( cfname ) \n 
itraits = c_int32 ( traits ) \n 
symTraits = c_void_p ( cf . CFNumberCreate ( None , kCFNumberSInt32Type , byref ( itraits ) ) ) \n 
if symTraits : \n 
~~~ traitsDict = c_void_p ( cf . CFDictionaryCreateMutable ( None , 0 , cf . kCFTypeDictionaryKeyCallBacks if traitsDict : \n 
~~~ cf . CFDictionaryAddValue ( traitsDict , kCTFontSymbolicTrait , symTraits ) \n 
cf . CFDictionaryAddValue ( attributes , kCTFontTraitsAttribute , traitsDict ) \n 
cf . CFRelease ( traitsDict ) \n 
~~ cf . CFRelease ( symTraits ) \n 
~~ descriptor = c_void_p ( ct . CTFontDescriptorCreateWithAttributes ( attributes ) ) \n 
return descriptor \n 
~~ def __init__ ( self , name , size , bold = False , italic = False , dpi = None ) : \n 
~~~ super ( QuartzFont , self ) . __init__ ( ) \n 
if not name : name = \n 
if dpi is None : dpi = 96 \n 
size = size * dpi / 72.0 \n 
traits = 0 \n 
if bold : traits |= kCTFontBoldTrait \n 
if italic : traits |= kCTFontItalicTrait \n 
name = unicode ( name ) \n 
cgFont = self . _lookup_font_with_family_and_traits ( name , traits ) \n 
if cgFont : \n 
~~~ self . ctFont = c_void_p ( ct . CTFontCreateWithGraphicsFont ( cgFont , size , None , None ) ) \n 
~~~ descriptor = self . _create_font_descriptor ( name , traits ) \n 
self . ctFont = c_void_p ( ct . CTFontCreateWithFontDescriptor ( descriptor , size , None ) ) \n 
~~ self . ascent = int ( math . ceil ( ct . CTFontGetAscent ( self . ctFont ) ) ) \n 
self . descent = - int ( math . ceil ( ct . CTFontGetDescent ( self . ctFont ) ) ) \n 
def have_font ( cls , name ) : \n 
~~~ name = unicode ( name ) \n 
if name in cls . _loaded_CGFont_table : return True \n 
cfstring = CFSTR ( name ) \n 
cgfont = c_void_p ( quartz . CGFontCreateWithFontName ( cfstring ) ) \n 
cf . CFRelease ( cfstring ) \n 
if cgfont : \n 
~~~ cf . CFRelease ( cgfont ) \n 
def add_font_data ( cls , data ) : \n 
~~~ dataRef = c_void_p ( cf . CFDataCreate ( None , data , len ( data ) ) ) \n 
provider = c_void_p ( quartz . CGDataProviderCreateWithCFData ( dataRef ) ) \n 
cgFont = c_void_p ( quartz . CGFontCreateWithDataProvider ( provider ) ) \n 
cf . CFRelease ( dataRef ) \n 
quartz . CGDataProviderRelease ( provider ) \n 
ctFont = c_void_p ( ct . CTFontCreateWithGraphicsFont ( cgFont , 1 , None , None ) ) \n 
string = c_void_p ( ct . CTFontCopyFamilyName ( ctFont ) ) \n 
familyName = unicode ( cfstring_to_string ( string ) ) \n 
string = c_void_p ( ct . CTFontCopyFullName ( ctFont ) ) \n 
fullName = unicode ( cfstring_to_string ( string ) ) \n 
traits = ct . CTFontGetSymbolicTraits ( ctFont ) \n 
cf . CFRelease ( ctFont ) \n 
if familyName not in cls . _loaded_CGFont_table : \n 
~~~ cls . _loaded_CGFont_table [ familyName ] = { } \n 
~~ cls . _loaded_CGFont_table [ familyName ] [ traits ] = cgFont \n 
if fullName not in cls . _loaded_CGFont_table : \n 
~~~ cls . _loaded_CGFont_table [ fullName ] = { } \n 
~~ cls . _loaded_CGFont_table [ fullName ] [ traits ] = cgFont \n 
~~ ~~ from pyglet . canvas . win32 import Win32Canvas \n 
from base import Config , CanvasConfig , Context \n 
from pyglet import gl \n 
from pyglet . gl import gl_info \n 
from pyglet . gl import wgl \n 
from pyglet . gl import wglext_arb \n 
from pyglet . gl import wgl_info \n 
from pyglet . libs . win32 import _user32 , _kernel32 , _gdi32 \n 
from pyglet . libs . win32 . constants import * \n 
from pyglet . libs . win32 . types import * \n 
class Win32Config ( Config ) : \n 
~~~ def match ( self , canvas ) : \n 
~~~ if not isinstance ( canvas , Win32Canvas ) : \n 
~~~ raise RuntimeError ( ) \n 
~~ if ( gl_info . have_context ( ) and \n 
wgl_info . have_extension ( ) ) : \n 
~~~ return self . _get_arb_pixel_format_matching_configs ( canvas ) \n 
~~~ return self . _get_pixel_format_descriptor_matching_configs ( canvas ) \n 
~~ ~~ def _get_pixel_format_descriptor_matching_configs ( self , canvas ) : \n 
pfd = PIXELFORMATDESCRIPTOR ( ) \n 
pfd . nSize = sizeof ( PIXELFORMATDESCRIPTOR ) \n 
pfd . nVersion = 1 \n 
pfd . dwFlags = PFD_DRAW_TO_WINDOW | PFD_SUPPORT_OPENGL \n 
if self . double_buffer : \n 
~~~ pfd . dwFlags |= PFD_DOUBLEBUFFER \n 
~~~ pfd . dwFlags |= PFD_DOUBLEBUFFER_DONTCARE \n 
~~ if self . stereo : \n 
~~~ pfd . dwFlags |= PFD_STEREO \n 
~~~ pfd . dwFlags |= PFD_STEREO_DONTCARE \n 
if not self . depth_size : \n 
~~~ pfd . dwFlags |= PFD_DEPTH_DONTCARE \n 
~~ pfd . iPixelType = PFD_TYPE_RGBA \n 
pfd . cColorBits = self . buffer_size or 0 \n 
pfd . cRedBits = self . red_size or 0 \n 
pfd . cGreenBits = self . green_size or 0 \n 
pfd . cBlueBits = self . blue_size or 0 \n 
pfd . cAlphaBits = self . alpha_size or 0 \n 
pfd . cAccumRedBits = self . accum_red_size or 0 \n 
pfd . cAccumGreenBits = self . accum_green_size or 0 \n 
pfd . cAccumBlueBits = self . accum_blue_size or 0 \n 
pfd . cAccumAlphaBits = self . accum_alpha_size or 0 \n 
pfd . cDepthBits = self . depth_size or 0 \n 
pfd . cStencilBits = self . stencil_size or 0 \n 
pfd . cAuxBuffers = self . aux_buffers or 0 \n 
pf = _gdi32 . ChoosePixelFormat ( canvas . hdc , byref ( pfd ) ) \n 
if pf : \n 
~~~ return [ Win32CanvasConfig ( canvas , pf , self ) ] \n 
~~ ~~ def _get_arb_pixel_format_matching_configs ( self , canvas ) : \n 
if self . sample_buffers or self . samples : \n 
~~~ if not gl_info . have_extension ( ) : \n 
~~ ~~ attrs = [ ] \n 
for name , value in self . get_gl_attributes ( ) : \n 
~~~ attr = Win32CanvasConfigARB . attribute_ids . get ( name , None ) \n 
if attr and value is not None : \n 
~~~ attrs . extend ( [ attr , int ( value ) ] ) \n 
~~ ~~ attrs . append ( 0 ) \n 
attrs = ( c_int * len ( attrs ) ) ( * attrs ) \n 
pformats = ( c_int * 16 ) ( ) \n 
nformats = c_uint ( 16 ) \n 
wglext_arb . wglChoosePixelFormatARB ( canvas . hdc , attrs , None , \n 
nformats , pformats , nformats ) \n 
formats = [ Win32CanvasConfigARB ( canvas , pf , self ) for pf in pformats [ : nformats . value ] ] \n 
return formats \n 
~~ ~~ class Win32CanvasConfig ( CanvasConfig ) : \n 
~~~ def __init__ ( self , canvas , pf , config ) : \n 
~~~ super ( Win32CanvasConfig , self ) . __init__ ( canvas , config ) \n 
self . _pf = pf \n 
self . _pfd = PIXELFORMATDESCRIPTOR ( ) \n 
_gdi32 . DescribePixelFormat ( canvas . hdc , \n 
self . _pf , sizeof ( PIXELFORMATDESCRIPTOR ) , byref ( self . _pfd ) ) \n 
self . double_buffer = bool ( self . _pfd . dwFlags & PFD_DOUBLEBUFFER ) \n 
self . sample_buffers = 0 \n 
self . samples = 0 \n 
self . stereo = bool ( self . _pfd . dwFlags & PFD_STEREO ) \n 
self . buffer_size = self . _pfd . cColorBits \n 
self . red_size = self . _pfd . cRedBits \n 
self . green_size = self . _pfd . cGreenBits \n 
self . blue_size = self . _pfd . cBlueBits \n 
self . alpha_size = self . _pfd . cAlphaBits \n 
self . accum_red_size = self . _pfd . cAccumRedBits \n 
self . accum_green_size = self . _pfd . cAccumGreenBits \n 
self . accum_blue_size = self . _pfd . cAccumBlueBits \n 
self . accum_alpha_size = self . _pfd . cAccumAlphaBits \n 
self . depth_size = self . _pfd . cDepthBits \n 
self . stencil_size = self . _pfd . cStencilBits \n 
self . aux_buffers = self . _pfd . cAuxBuffers \n 
~~ def compatible ( self , canvas ) : \n 
~~~ return isinstance ( canvas , Win32Canvas ) \n 
~~ def create_context ( self , share ) : \n 
~~~ return Win32Context ( self , share ) \n 
~~ def _set_pixel_format ( self , canvas ) : \n 
~~~ _gdi32 . SetPixelFormat ( canvas . hdc , self . _pf , byref ( self . _pfd ) ) \n 
~~ ~~ class Win32CanvasConfigARB ( CanvasConfig ) : \n 
~~~ attribute_ids = { \n 
: wglext_arb . WGL_DOUBLE_BUFFER_ARB , \n 
: wglext_arb . WGL_STEREO_ARB , \n 
: wglext_arb . WGL_COLOR_BITS_ARB , \n 
: wglext_arb . WGL_AUX_BUFFERS_ARB , \n 
: wglext_arb . WGL_SAMPLE_BUFFERS_ARB , \n 
: wglext_arb . WGL_SAMPLES_ARB , \n 
: wglext_arb . WGL_RED_BITS_ARB , \n 
: wglext_arb . WGL_GREEN_BITS_ARB , \n 
: wglext_arb . WGL_BLUE_BITS_ARB , \n 
: wglext_arb . WGL_ALPHA_BITS_ARB , \n 
: wglext_arb . WGL_DEPTH_BITS_ARB , \n 
: wglext_arb . WGL_STENCIL_BITS_ARB , \n 
: wglext_arb . WGL_ACCUM_RED_BITS_ARB , \n 
: wglext_arb . WGL_ACCUM_GREEN_BITS_ARB , \n 
: wglext_arb . WGL_ACCUM_BLUE_BITS_ARB , \n 
: wglext_arb . WGL_ACCUM_ALPHA_BITS_ARB , \n 
def __init__ ( self , canvas , pf , config ) : \n 
~~~ super ( Win32CanvasConfigARB , self ) . __init__ ( canvas , config ) \n 
names = self . attribute_ids . keys ( ) \n 
attrs = self . attribute_ids . values ( ) \n 
values = ( c_int * len ( attrs ) ) ( ) \n 
result = wglext_arb . wglGetPixelFormatAttribivARB ( canvas . hdc , \n 
pf , 0 , len ( attrs ) , attrs , values ) \n 
for name , value in zip ( names , values ) : \n 
~~~ setattr ( self , name , value ) \n 
~~ ~~ def compatible ( self , canvas ) : \n 
~~~ if wgl_info . have_extension ( ) : \n 
~~~ return Win32ARBContext ( self , share ) \n 
~~ ~~ def _set_pixel_format ( self , canvas ) : \n 
~~~ _gdi32 . SetPixelFormat ( canvas . hdc , self . _pf , None ) \n 
~~ ~~ class Win32Context ( Context ) : \n 
~~~ def __init__ ( self , config , share ) : \n 
~~~ super ( Win32Context , self ) . __init__ ( config , share ) \n 
self . _context = None \n 
~~ def attach ( self , canvas ) : \n 
~~~ if self . config . _requires_gl_3 ( ) : \n 
~~~ raise gl . ContextException ( \n 
~~ super ( Win32Context , self ) . attach ( canvas ) \n 
self . config . _set_pixel_format ( canvas ) \n 
self . _context = wgl . wglCreateContext ( canvas . hdc ) \n 
share = self . context_share \n 
if share : \n 
~~~ if not share . canvas : \n 
~~ if not wgl . wglShareLists ( share . _context , self . _context ) : \n 
~~~ raise gl . ContextException ( ) \n 
~~ ~~ ~~ def set_current ( self ) : \n 
~~~ wgl . wglMakeCurrent ( self . canvas . hdc , self . _context ) \n 
super ( Win32Context , self ) . set_current ( ) \n 
~~ def detach ( self ) : \n 
~~~ if self . canvas : \n 
~~~ wgl . wglDeleteContext ( self . _context ) \n 
~~ super ( Win32Context , self ) . detach ( ) \n 
~~ def flip ( self ) : \n 
~~~ wgl . wglSwapLayerBuffers ( self . canvas . hdc , wgl . WGL_SWAP_MAIN_PLANE ) \n 
~~ def get_vsync ( self ) : \n 
~~~ return bool ( wglext_arb . wglGetSwapIntervalEXT ( ) ) \n 
~~ ~~ def set_vsync ( self , vsync ) : \n 
~~~ wglext_arb . wglSwapIntervalEXT ( int ( vsync ) ) \n 
~~ ~~ ~~ class Win32ARBContext ( Win32Context ) : \n 
~~~ super ( Win32ARBContext , self ) . __init__ ( config , share ) \n 
~~~ super ( Win32ARBContext , self ) . attach ( canvas ) \n 
~~ share = share . _context \n 
~~ attribs = [ ] \n 
if self . config . major_version is not None : \n 
~~~ attribs . extend ( [ wglext_arb . WGL_CONTEXT_MAJOR_VERSION_ARB , \n 
self . config . major_version ] ) \n 
~~ if self . config . minor_version is not None : \n 
~~~ attribs . extend ( [ wglext_arb . WGL_CONTEXT_MINOR_VERSION_ARB , \n 
self . config . minor_version ] ) \n 
~~ flags = 0 \n 
if self . config . forward_compatible : \n 
~~~ flags |= wglext_arb . WGL_CONTEXT_FORWARD_COMPATIBLE_BIT_ARB \n 
~~ if self . config . debug : \n 
~~~ flags |= wglext_arb . WGL_DEBUG_BIT_ARB \n 
~~ if flags : \n 
~~~ attribs . extend ( [ wglext_arb . WGL_CONTEXT_FLAGS_ARB , flags ] ) \n 
~~ attribs . append ( 0 ) \n 
attribs = ( c_int * len ( attribs ) ) ( * attribs ) \n 
self . _context = wglext_arb . wglCreateContextAttribsARB ( canvas . hdc , \n 
share , attribs ) \n 
from pyglet . input . base import Tablet , TabletCanvas , TabletCursor \n 
from pyglet . window . carbon import CarbonEventHandler \n 
from pyglet . libs . darwin import * \n 
from pyglet . libs . darwin import _oscheck \n 
class CarbonTablet ( Tablet ) : \n 
def open ( self , window ) : \n 
~~~ return CarbonTabletCanvas ( window ) \n 
~~ ~~ _carbon_tablet = CarbonTablet ( ) \n 
class CarbonTabletCanvas ( TabletCanvas ) : \n 
~~~ def __init__ ( self , window ) : \n 
~~~ super ( CarbonTabletCanvas , self ) . __init__ ( window ) \n 
for funcname in dir ( self ) : \n 
~~~ func = getattr ( self , funcname ) \n 
if hasattr ( func , ) : \n 
~~~ window . _install_event_handler ( func ) \n 
~~ ~~ self . _cursors = { } \n 
self . _cursor = None \n 
~~ def _get_cursor ( self , proximity_rec ) : \n 
~~~ key = ( proximity_rec . vendorID , \n 
proximity_rec . tabletID , \n 
proximity_rec . pointerID , \n 
proximity_rec . deviceID , \n 
proximity_rec . systemTabletID , \n 
proximity_rec . vendorPointerType , \n 
proximity_rec . pointerSerialNumber , \n 
proximity_rec . uniqueID , \n 
proximity_rec . pointerType ) \n 
if key in self . _cursors : \n 
~~~ cursor = self . _cursors [ key ] \n 
~~~ self . _cursors [ key ] = cursor = CarbonTabletCursor ( proximity_rec . pointerType ) \n 
~~ self . _cursor = cursor \n 
return cursor \n 
~~ @ CarbonEventHandler ( kEventClassTablet , kEventTabletProximity ) \n 
@ CarbonEventHandler ( kEventClassTablet , kEventTabletPoint ) \n 
@ CarbonEventHandler ( kEventClassMouse , kEventMouseDragged ) \n 
@ CarbonEventHandler ( kEventClassMouse , kEventMouseDown ) \n 
@ CarbonEventHandler ( kEventClassMouse , kEventMouseUp ) \n 
@ CarbonEventHandler ( kEventClassMouse , kEventMouseMoved ) \n 
def _tablet_event ( self , next_handler , ev , data ) : \n 
event_type = ctypes . c_uint32 ( ) \n 
r = carbon . GetEventParameter ( ev , kEventParamTabletEventType , \n 
typeUInt32 , None , \n 
ctypes . sizeof ( event_type ) , None , \n 
ctypes . byref ( event_type ) ) \n 
if r != noErr : \n 
~~ if event_type . value == kEventTabletProximity : \n 
~~~ proximity_rec = TabletProximityRec ( ) \n 
_oscheck ( \n 
carbon . GetEventParameter ( ev , kEventParamTabletProximityRec , \n 
typeTabletProximityRec , None , \n 
ctypes . sizeof ( proximity_rec ) , None , \n 
ctypes . byref ( proximity_rec ) ) \n 
cursor = self . _get_cursor ( proximity_rec ) \n 
if proximity_rec . enterProximity : \n 
~~~ self . dispatch_event ( , cursor ) \n 
~~ ~~ elif event_type . value == kEventTabletPoint : \n 
~~~ point_rec = TabletPointRec ( ) \n 
carbon . GetEventParameter ( ev , kEventParamTabletPointRec , \n 
typeTabletPointRec , None , \n 
ctypes . sizeof ( point_rec ) , None , \n 
ctypes . byref ( point_rec ) ) \n 
x , y = self . window . _get_mouse_position ( ev ) \n 
pressure = point_rec . pressure / float ( 0xffff ) \n 
#point_rec.tiltX, \n 
#point_rec.tiltY, \n 
#point_rec.rotation, \n 
#point_rec.tangentialPressure, \n 
self . dispatch_event ( , self . _cursor , x , y , pressure , \n 
0. , 0. ) \n 
~~ carbon . CallNextEventHandler ( next_handler , ev ) \n 
return noErr \n 
~~ ~~ class CarbonTabletCursor ( TabletCursor ) : \n 
~~~ def __init__ ( self , cursor_type ) : \n 
~~~ if cursor_type == 1 : \n 
~~ elif cursor_type == 3 : \n 
~~ super ( CarbonTabletCursor , self ) . __init__ ( name ) \n 
~~ ~~ def get_tablets ( display = None ) : \n 
~~~ return [ _carbon_tablet ] \n 
from pyglet import image \n 
import pyglet . lib \n 
from pyglet . media import MediaFormatException , StreamingSource , VideoFormat , AudioFormat , AudioData , MediaEvent , WorkerThread , SourceInfo \n 
av = pyglet . lib . load_library ( , \n 
darwin = ) \n 
AVBIN_RESULT_ERROR = - 1 \n 
AVBIN_RESULT_OK = 0 \n 
AVbinResult = ctypes . c_int \n 
AVBIN_STREAM_TYPE_UNKNOWN = 0 \n 
AVBIN_STREAM_TYPE_VIDEO = 1 \n 
AVBIN_STREAM_TYPE_AUDIO = 2 \n 
AVbinStreamType = ctypes . c_int \n 
AVBIN_SAMPLE_FORMAT_U8 = 0 \n 
AVBIN_SAMPLE_FORMAT_S16 = 1 \n 
AVBIN_SAMPLE_FORMAT_S24 = 2 \n 
AVBIN_SAMPLE_FORMAT_S32 = 3 \n 
AVBIN_SAMPLE_FORMAT_FLOAT = 4 \n 
AVbinSampleFormat = ctypes . c_int \n 
AVBIN_LOG_QUIET = - 8 \n 
AVBIN_LOG_PANIC = 0 \n 
AVBIN_LOG_FATAL = 8 \n 
AVBIN_LOG_ERROR = 16 \n 
AVBIN_LOG_WARNING = 24 \n 
AVBIN_LOG_INFO = 32 \n 
AVBIN_LOG_VERBOSE = 40 \n 
AVBIN_LOG_DEBUG = 48 \n 
AVbinLogLevel = ctypes . c_int \n 
AVbinFileP = ctypes . c_void_p \n 
AVbinStreamP = ctypes . c_void_p \n 
Timestamp = ctypes . c_int64 \n 
class AVbinFileInfo ( ctypes . Structure ) : \n 
( , ctypes . c_size_t ) , \n 
( , ctypes . c_int ) , \n 
( , Timestamp ) , \n 
( , ctypes . c_char * 512 ) , \n 
~~ class _AVbinStreamInfoVideo8 ( ctypes . Structure ) : \n 
( , ctypes . c_uint ) , \n 
~~ class _AVbinStreamInfoAudio8 ( ctypes . Structure ) : \n 
~~ class _AVbinStreamInfoUnion8 ( ctypes . Union ) : \n 
( , _AVbinStreamInfoVideo8 ) , \n 
( , _AVbinStreamInfoAudio8 ) , \n 
~~ class AVbinStreamInfo8 ( ctypes . Structure ) : \n 
( , _AVbinStreamInfoUnion8 ) \n 
~~ class AVbinPacket ( ctypes . Structure ) : \n 
( , ctypes . POINTER ( ctypes . c_uint8 ) ) , \n 
~~ AVbinLogCallback = ctypes . CFUNCTYPE ( None , \n 
ctypes . c_char_p , ctypes . c_int , ctypes . c_char_p ) \n 
av . avbin_get_version . restype = ctypes . c_int \n 
av . avbin_get_ffmpeg_revision . restype = ctypes . c_int \n 
av . avbin_get_audio_buffer_size . restype = ctypes . c_size_t \n 
av . avbin_have_feature . restype = ctypes . c_int \n 
av . avbin_have_feature . argtypes = [ ctypes . c_char_p ] \n 
av . avbin_init . restype = AVbinResult \n 
av . avbin_set_log_level . restype = AVbinResult \n 
av . avbin_set_log_level . argtypes = [ AVbinLogLevel ] \n 
av . avbin_set_log_callback . argtypes = [ AVbinLogCallback ] \n 
av . avbin_open_filename . restype = AVbinFileP \n 
av . avbin_open_filename . argtypes = [ ctypes . c_char_p ] \n 
av . avbin_close_file . argtypes = [ AVbinFileP ] \n 
av . avbin_seek_file . argtypes = [ AVbinFileP , Timestamp ] \n 
av . avbin_file_info . argtypes = [ AVbinFileP , ctypes . POINTER ( AVbinFileInfo ) ] \n 
av . avbin_stream_info . argtypes = [ AVbinFileP , ctypes . c_int , \n 
ctypes . POINTER ( AVbinStreamInfo8 ) ] \n 
av . avbin_open_stream . restype = ctypes . c_void_p \n 
av . avbin_open_stream . argtypes = [ AVbinFileP , ctypes . c_int ] \n 
av . avbin_close_stream . argtypes = [ AVbinStreamP ] \n 
av . avbin_read . argtypes = [ AVbinFileP , ctypes . POINTER ( AVbinPacket ) ] \n 
av . avbin_read . restype = AVbinResult \n 
av . avbin_decode_audio . restype = ctypes . c_int \n 
av . avbin_decode_audio . argtypes = [ AVbinStreamP , \n 
ctypes . c_void_p , ctypes . c_size_t , \n 
ctypes . c_void_p , ctypes . POINTER ( ctypes . c_int ) ] \n 
av . avbin_decode_video . restype = ctypes . c_int \n 
av . avbin_decode_video . argtypes = [ AVbinStreamP , \n 
ctypes . c_void_p ] \n 
if True : \n 
~~~ def synchronize ( func , lock ) : \n 
~~~ def f ( * args ) : \n 
~~~ lock . acquire ( ) \n 
result = func ( * args ) \n 
lock . release ( ) \n 
~~ _avbin_lock = threading . Lock ( ) \n 
for name in dir ( av ) : \n 
~~~ setattr ( av , name , synchronize ( getattr ( av , name ) , _avbin_lock ) ) \n 
~~ ~~ ~~ def get_version ( ) : \n 
~~~ return av . avbin_get_version ( ) \n 
~~ class AVbinException ( MediaFormatException ) : \n 
~~ def timestamp_from_avbin ( timestamp ) : \n 
~~~ return float ( timestamp ) / 1000000 \n 
~~ def timestamp_to_avbin ( timestamp ) : \n 
~~~ return int ( timestamp * 1000000 ) \n 
~~ class VideoPacket ( object ) : \n 
~~~ _next_id = 0 \n 
def __init__ ( self , packet ) : \n 
~~~ self . timestamp = timestamp_from_avbin ( packet . timestamp ) \n 
self . data = ( ctypes . c_uint8 * packet . size ) ( ) \n 
self . size = packet . size \n 
ctypes . memmove ( self . data , packet . data , self . size ) \n 
self . image = 0 \n 
self . id = self . _next_id \n 
self . __class__ . _next_id += 1 \n 
~~ ~~ class AVbinSource ( StreamingSource ) : \n 
~~~ def __init__ ( self , filename , file = None ) : \n 
~~~ raise NotImplementedError ( ) \n 
~~ self . _file = av . avbin_open_filename ( filename ) \n 
if not self . _file : \n 
~~ self . _video_stream = None \n 
self . _video_stream_index = - 1 \n 
self . _audio_stream = None \n 
self . _audio_stream_index = - 1 \n 
file_info = AVbinFileInfo ( ) \n 
file_info . structure_size = ctypes . sizeof ( file_info ) \n 
av . avbin_file_info ( self . _file , ctypes . byref ( file_info ) ) \n 
self . _duration = timestamp_from_avbin ( file_info . duration ) \n 
self . info = SourceInfo ( ) \n 
self . info . title = file_info . title \n 
self . info . author = file_info . author \n 
self . info . copyright = file_info . copyright \n 
self . info . comment = file_info . comment \n 
self . info . album = file_info . album \n 
self . info . year = file_info . year \n 
self . info . track = file_info . track \n 
self . info . genre = file_info . genre \n 
for i in range ( file_info . n_streams ) : \n 
~~~ info = AVbinStreamInfo8 ( ) \n 
info . structure_size = ctypes . sizeof ( info ) \n 
av . avbin_stream_info ( self . _file , i , info ) \n 
if ( info . type == AVBIN_STREAM_TYPE_VIDEO and \n 
not self . _video_stream ) : \n 
~~~ stream = av . avbin_open_stream ( self . _file , i ) \n 
if not stream : \n 
~~ self . video_format = VideoFormat ( \n 
width = info . u . video . width , \n 
height = info . u . video . height ) \n 
if info . u . video . sample_aspect_num != 0 : \n 
~~~ self . video_format . sample_aspect = ( \n 
float ( info . u . video . sample_aspect_num ) / \n 
info . u . video . sample_aspect_den ) \n 
~~ if _have_frame_rate : \n 
~~~ self . video_format . frame_rate = ( \n 
float ( info . u . video . frame_rate_num ) / \n 
info . u . video . frame_rate_den ) \n 
~~ self . _video_stream = stream \n 
self . _video_stream_index = i \n 
~~ elif ( info . type == AVBIN_STREAM_TYPE_AUDIO and \n 
info . u . audio . sample_bits in ( 8 , 16 ) and \n 
info . u . audio . channels in ( 1 , 2 ) and \n 
not self . _audio_stream ) : \n 
~~ self . audio_format = AudioFormat ( \n 
channels = info . u . audio . channels , \n 
sample_size = info . u . audio . sample_bits , \n 
sample_rate = info . u . audio . sample_rate ) \n 
self . _audio_stream = stream \n 
self . _audio_stream_index = i \n 
~~ ~~ self . _packet = AVbinPacket ( ) \n 
self . _packet . structure_size = ctypes . sizeof ( self . _packet ) \n 
self . _packet . stream_index = - 1 \n 
self . _video_timestamp = 0 \n 
self . _buffered_audio_data = [ ] \n 
if self . audio_format : \n 
~~~ self . _audio_buffer = ( ctypes . c_uint8 * av . avbin_get_audio_buffer_size ( ) ) ( ) \n 
~~ if self . video_format : \n 
~~~ self . _video_packets = [ ] \n 
self . _decode_thread = WorkerThread ( ) \n 
self . _decode_thread . start ( ) \n 
self . _condition = threading . Condition ( ) \n 
~~ ~~ def __del__ ( self ) : \n 
~~~ if self . _video_stream : \n 
~~~ av . avbin_close_stream ( self . _video_stream ) \n 
~~ if self . _audio_stream : \n 
~~~ av . avbin_close_stream ( self . _audio_stream ) \n 
~~ av . avbin_close_file ( self . _file ) \n 
~~~ if self . video_format : \n 
~~~ self . _decode_thread . stop ( ) \n 
~~ ~~ def seek ( self , timestamp ) : \n 
~~~ print , timestamp \n 
~~ av . avbin_seek_file ( self . _file , timestamp_to_avbin ( timestamp ) ) \n 
self . _audio_packet_size = 0 \n 
del self . _buffered_audio_data [ : ] \n 
if self . video_format : \n 
~~~ self . _video_timestamp = 0 \n 
self . _condition . acquire ( ) \n 
for packet in self . _video_packets : \n 
~~~ packet . image = None \n 
~~ self . _condition . notify ( ) \n 
self . _condition . release ( ) \n 
del self . _video_packets [ : ] \n 
self . _decode_thread . clear_jobs ( ) \n 
~~ ~~ def _get_packet ( self ) : \n 
~~~ return av . avbin_read ( self . _file , self . _packet ) == AVBIN_RESULT_OK \n 
~~ def _process_packet ( self ) : \n 
~~~ if self . _packet . stream_index == self . _video_stream_index : \n 
~~~ if self . _packet . timestamp < 0 : \n 
~~~ return None , None \n 
~~ video_packet = VideoPacket ( self . _packet ) \n 
~~~ print % ( video_packet . id , video_packet . timestamp ) \n 
~~ self . _video_timestamp = max ( self . _video_timestamp , \n 
video_packet . timestamp ) \n 
self . _video_packets . append ( video_packet ) \n 
self . _decode_thread . put_job ( \n 
lambda : self . _decode_video_packet ( video_packet ) ) \n 
return , video_packet \n 
~~ elif self . _packet . stream_index == self . _audio_stream_index : \n 
~~~ audio_data = self . _decode_audio_packet ( ) \n 
if audio_data : \n 
~~~ print , audio_data . timestamp \n 
~~ self . _buffered_audio_data . append ( audio_data ) \n 
return , audio_data \n 
~~ ~~ return None , None \n 
~~ def get_audio_data ( self , bytes ) : \n 
~~~ audio_data = self . _buffered_audio_data . pop ( 0 ) \n 
audio_data_timeend = audio_data . timestamp + audio_data . duration \n 
~~~ audio_data = None \n 
audio_data_timeend = self . _video_timestamp + 1 \n 
~~ if _debug : \n 
~~ have_video_work = False \n 
while not audio_data or ( \n 
self . _video_stream and self . _video_timestamp < audio_data_timeend ) : \n 
~~~ if not self . _get_packet ( ) : \n 
~~ packet_type , packet = self . _process_packet ( ) \n 
if packet_type == : \n 
~~~ have_video_work = True \n 
~~ elif not audio_data and packet_type == : \n 
~~ audio_data_timeend = audio_data . timestamp + audio_data . duration \n 
~~ ~~ if have_video_work : \n 
~~~ time . sleep ( 0 ) \n 
~~ if not audio_data : \n 
~~ while self . _events and self . _events [ 0 ] . timestamp <= audio_data_timeend : \n 
~~~ event = self . _events . pop ( 0 ) \n 
if event . timestamp >= audio_data . timestamp : \n 
~~~ event . timestamp -= audio_data . timestamp \n 
audio_data . events . append ( event ) \n 
~~ ~~ if _debug : \n 
~~~ print % audio_data . timestamp , audio_data . events \n 
print , self . _events \n 
~~ return audio_data \n 
~~ def _decode_audio_packet ( self ) : \n 
~~~ packet = self . _packet \n 
size_out = ctypes . c_int ( len ( self . _audio_buffer ) ) \n 
~~~ audio_packet_ptr = ctypes . cast ( packet . data , ctypes . c_void_p ) \n 
audio_packet_size = packet . size \n 
used = av . avbin_decode_audio ( self . _audio_stream , \n 
audio_packet_ptr , audio_packet_size , \n 
self . _audio_buffer , size_out ) \n 
if used < 0 : \n 
~~~ self . _audio_packet_size = 0 \n 
~~ audio_packet_ptr . value += used \n 
audio_packet_size -= used \n 
if size_out . value <= 0 : \n 
~~ buffer = ctypes . create_string_buffer ( size_out . value ) \n 
ctypes . memmove ( buffer , self . _audio_buffer , len ( buffer ) ) \n 
buffer = buffer . raw \n 
duration = float ( len ( buffer ) ) / self . audio_format . bytes_per_second \n 
self . _audio_packet_timestamp = timestamp = timestamp_from_avbin ( packet . timestamp ) \n 
return AudioData ( buffer , len ( buffer ) , timestamp , duration , [ ] ) \n 
~~ ~~ def _decode_video_packet ( self , packet ) : \n 
~~~ width = self . video_format . width \n 
height = self . video_format . height \n 
pitch = width * 3 \n 
buffer = ( ctypes . c_uint8 * ( pitch * height ) ) ( ) \n 
result = av . avbin_decode_video ( self . _video_stream , \n 
packet . data , packet . size , \n 
buffer ) \n 
if result < 0 : \n 
~~~ image_data = None \n 
~~~ image_data = image . ImageData ( width , height , , buffer , pitch ) \n 
~~ packet . image = image_data \n 
self . _condition . notify ( ) \n 
~~ def _ensure_video_packets ( self ) : \n 
if not self . _video_packets : \n 
~~ self . _get_packet ( ) \n 
packet_type , _ = self . _process_packet ( ) \n 
while packet_type and packet_type != : \n 
~~~ self . _get_packet ( ) \n 
~~ if not packet_type : \n 
~~~ print , _ \n 
~~ def get_next_video_timestamp ( self ) : \n 
~~~ if not self . video_format : \n 
~~ if self . _ensure_video_packets ( ) : \n 
~~~ print , self . _video_packets [ 0 ] . timestamp \n 
~~ return self . _video_packets [ 0 ] . timestamp \n 
~~ ~~ def get_next_video_frame ( self ) : \n 
~~~ packet = self . _video_packets . pop ( 0 ) \n 
~~~ print , packet \n 
~~ self . _condition . acquire ( ) \n 
while packet . image == 0 : \n 
~~~ self . _condition . wait ( ) \n 
~~ self . _condition . release ( ) \n 
~~ return packet . image \n 
~~ ~~ ~~ av . avbin_init ( ) \n 
if pyglet . options [ ] : \n 
~~~ _debug = True \n 
av . avbin_set_log_level ( AVBIN_LOG_DEBUG ) \n 
~~~ _debug = False \n 
av . avbin_set_log_level ( AVBIN_LOG_QUIET ) \n 
~~ _have_frame_rate = av . avbin_have_feature ( ) \n 
from pyglet . window import key \n 
from pyglet . window import mouse \n 
class WindowExitHandler ( object ) : \n 
has_exit = False \n 
def on_close ( self ) : \n 
~~~ self . has_exit = True \n 
~~ def on_key_press ( self , symbol , modifiers ) : \n 
~~~ if symbol == key . ESCAPE : \n 
~~ ~~ ~~ class WindowEventLogger ( object ) : \n 
def __init__ ( self , logfile = None ) : \n 
if logfile is None : \n 
~~~ logfile = sys . stdout \n 
~~ self . file = logfile \n 
~~~ print >> self . file , % ( \n 
key . symbol_string ( symbol ) , key . modifiers_string ( modifiers ) ) \n 
~~ def on_key_release ( self , symbol , modifiers ) : \n 
~~ def on_text ( self , text ) : \n 
~~~ print >> self . file , % text \n 
~~ def on_text_motion ( self , motion ) : \n 
key . motion_string ( motion ) ) \n 
~~ def on_text_motion_select ( self , motion ) : \n 
~~ def on_mouse_motion ( self , x , y , dx , dy ) : \n 
x , y , dx , dy ) \n 
~~ def on_mouse_drag ( self , x , y , dx , dy , buttons , modifiers ) : \n 
x , y , dx , dy , \n 
mouse . buttons_string ( buttons ) , key . modifiers_string ( modifiers ) ) \n 
~~ def on_mouse_press ( self , x , y , button , modifiers ) : \n 
~~~ print >> self . file , % ( x , y , \n 
mouse . buttons_string ( button ) , key . modifiers_string ( modifiers ) ) \n 
~~ def on_mouse_release ( self , x , y , button , modifiers ) : \n 
~~ def on_mouse_scroll ( self , x , y , dx , dy ) : \n 
~~ def on_close ( self ) : \n 
~~~ print >> self . file , \n 
~~ def on_mouse_enter ( self , x , y ) : \n 
~~~ print >> self . file , % ( x , y ) \n 
~~ def on_mouse_leave ( self , x , y ) : \n 
~~ def on_expose ( self ) : \n 
~~ def on_resize ( self , width , height ) : \n 
~~~ print >> self . file , % ( width , height ) \n 
~~ def on_move ( self , x , y ) : \n 
~~ def on_activate ( self ) : \n 
~~ def on_deactivate ( self ) : \n 
~~ def on_show ( self ) : \n 
~~ def on_hide ( self ) : \n 
~~ def on_context_lost ( self ) : \n 
~~ def on_context_state_lost ( self ) : \n 
~~ def on_draw ( self ) : \n 
from pyglet import font \n 
import base_text \n 
class TEST_VALIGN ( base_text . TextTestBase ) : \n 
~~~ font_name = \n 
window_size = 600 , 200 \n 
def render ( self ) : \n 
~~~ fnt = font . load ( , self . font_size ) \n 
h = fnt . ascent - fnt . descent \n 
w = self . window . width \n 
self . labels = [ ] \n 
x = 0 \n 
for align in . split ( ) : \n 
~~~ label = align . upper ( ) + \n 
self . labels . append ( font . Text ( fnt , label , x , 50 , valign = align ) ) \n 
x += self . labels [ - 1 ] . width \n 
~~ ~~ def draw ( self ) : \n 
~~~ gl . glColor3f ( 1 , 1 , 1 ) \n 
gl . glBegin ( gl . GL_LINES ) \n 
gl . glVertex2f ( 0 , 50 ) \n 
gl . glVertex2f ( self . window . width , 50 ) \n 
gl . glEnd ( ) \n 
for label in self . labels : \n 
~~~ label . draw ( ) \n 
import base_load \n 
class TEST_MATRIX_RGB ( base_load . TestLoad ) : \n 
~~~ texture_file = \n 
def load_image ( self ) : \n 
self . has_exit = True \n 
~~~ super ( TEST_MATRIX_RGB , self ) . load_image ( ) \n 
self . image . format = \n 
from pyglet import media \n 
from pyglet . media import procedural \n 
class TEST_CASE ( unittest . TestCase ) : \n 
~~~ def test_method ( self ) : \n 
~~~ source = procedural . WhiteNoise ( 0.5 ) \n 
player = media . Player ( ) \n 
player . queue ( source ) \n 
player . play ( ) \n 
stage = 0 \n 
while player . source : \n 
~~~ if stage == 0 and time . time ( ) - start_time > 0.25 : \n 
~~~ player . pause ( ) \n 
stage = 1 \n 
~~ if stage == 1 and time . time ( ) - start_time > 0.75 : \n 
~~~ player . play ( ) \n 
stage = 2 \n 
~~ player . dispatch_events ( ) \n 
from pyglet import window \n 
class EVENT_MOUSE_ENTER_LEAVE ( unittest . TestCase ) : \n 
~~~ def on_mouse_enter ( self , x , y ) : \n 
~~~ print % ( x , y ) \n 
~~ def test_motion ( self ) : \n 
~~~ w = window . Window ( 200 , 200 ) \n 
w . push_handlers ( self ) \n 
while not w . has_exit : \n 
~~~ w . dispatch_events ( ) \n 
~~ w . close ( ) \n 
from os . path import join , dirname \n 
icon_file = join ( dirname ( __file__ ) , ) \n 
class WINDOW_SET_ICON ( unittest . TestCase ) : \n 
~~~ def test_set_icon ( self ) : \n 
~~~ self . width , self . height = 200 , 200 \n 
self . w = w = window . Window ( self . width , self . height ) \n 
w . set_icon ( image . load ( icon_file ) ) \n 
glClearColor ( 1 , 1 , 1 , 1 ) \n 
~~~ glClear ( GL_COLOR_BUFFER_BIT ) \n 
w . dispatch_events ( ) \n 
w . flip ( ) \n 
############################################################ \n 
_COLOR_RE = re . compile ( ) \n 
def _set_colors ( template , * dicts ) : \n 
~~~ colors = dicts [ 0 ] . copy ( ) \n 
for d in dicts [ 1 : ] : colors . update ( d ) \n 
return re . sub ( , lambda m : colors [ m . group ( 1 ) ] , template ) \n 
~~ def _rv ( match ) : \n 
rgb = [ int ( grp , 16 ) for grp in match . groups ( ) ] \n 
return + . join ( [ % ( 255 - c ) for c in rgb ] ) \n 
~~ def _darken_darks ( match ) : \n 
~~~ rgb = [ int ( grp , 16 ) for grp in match . groups ( ) ] \n 
return + . join ( [ % ( ( ( c / 255. ) ** 2 ) * 255 ) for c in rgb ] ) \n 
~~ _WHITE_COLORS = dict ( \n 
body_bg = , \n 
body_fg = , \n 
body_link = , \n 
body_visited_link = , \n 
navbar_bg = , \n 
navbar_fg = , \n 
navbar_border = , \n 
navbar_select_bg = , \n 
navbar_select_fg = , \n 
navbar_link = , \n 
navbar_visited_link = , \n 
table_bg = , \n 
table_fg = , \n 
table_link = , \n 
table_visited_link = , \n 
table_border = , \n 
table_hdr_bg = , \n 
table_hdr_fg = , \n 
table_hdr_link = , \n 
table_hdr_visited_link = , \n 
group_hdr_bg = , \n 
group_hdr_fg = , \n 
sig_name = , \n 
sig_arg = , \n 
sig_default = , \n 
summary_sig_name = , \n 
summary_sig_arg = , \n 
summary_sig_default = , \n 
variable_bg = , \n 
variable_fg = , \n 
variable_border = , \n 
variable_linewrap = , \n 
variable_ellipsis = , \n 
variable_quote = , \n 
variable_group = , \n 
variable_string = , \n 
variable_op = , \n 
variable_unknown = , \n 
re = , \n 
re_char = , \n 
re_op = , \n 
re_group = , \n 
re_ref = , \n 
doctest_bg = , \n 
doctest_fg = , \n 
doctest_border = , \n 
doctest_in_table_bg = , \n 
doctest_in_table_fg = , \n 
pysrc_border = , \n 
pysrc_sep_border = , \n 
pysrc_bg = , \n 
pysrc_fg = , \n 
pysrc_highlight_hdr_bg = , \n 
pysrc_highlight_bg = , \n 
py_prompt = , \n 
py_more = , \n 
py_string = , \n 
py_comment = , \n 
py_keyword = , \n 
py_output = , \n 
py_name = , \n 
py_number = , \n 
py_def_name = , \n 
py_base_class = , \n 
py_param = , \n 
py_docstring = , \n 
py_decorator = , \n 
graph_border = , \n 
log_bg = , \n 
log_fg = , \n 
log_border = , \n 
log_hdr_bg = , \n 
log_hdr_fg = , \n 
log_error_bg = , \n 
log_error_fg = , \n 
log_error_border = , \n 
log_warn_bg = , \n 
log_warn_fg = , \n 
log_warn_border = , \n 
log_info_bg = , \n 
log_info_fg = , \n 
log_info_border = , \n 
opt_changed_fg = , \n 
opt_default_fg = , \n 
_BLUE_COLORS = _WHITE_COLORS . copy ( ) \n 
_BLUE_COLORS . update ( dict ( \n 
_WHITE = _set_colors ( TEMPLATE , _WHITE_COLORS ) \n 
_BLUE = _set_colors ( TEMPLATE , _BLUE_COLORS ) \n 
_GREEN = _COLOR_RE . sub ( _darken_darks , _COLOR_RE . sub ( , _BLUE ) ) \n 
_BLACK = _COLOR_RE . sub ( , _COLOR_RE . sub ( _rv , _WHITE ) ) \n 
_GRAYSCALE = _COLOR_RE . sub ( , _WHITE ) \n 
STYLESHEETS = { \n 
from pyglet . font import ttf \n 
def inspect_font ( filename ) : \n 
~~~ info = ttf . TruetypeInfo ( filename ) \n 
print % filename , \n 
print info . get_name ( ) , \n 
print % info . is_bold ( ) , \n 
print % info . is_italic ( ) , \n 
~~~ print % filename \n 
~~~ if len ( sys . argv ) < 2 : \n 
~~~ print __doc__ \n 
~~ for filename in sys . argv [ 1 : ] : \n 
~~~ inspect_font ( filename ) \n 
from . lineChart import lineChart \n 
from . pieChart import pieChart \n 
from . lineWithFocusChart import lineWithFocusChart \n 
from . stackedAreaChart import stackedAreaChart \n 
from . multiBarHorizontalChart import multiBarHorizontalChart \n 
from . linePlusBarChart import linePlusBarChart \n 
from . cumulativeLineChart import cumulativeLineChart \n 
from . scatterChart import scatterChart \n 
from . discreteBarChart import discreteBarChart \n 
from . multiBarChart import multiBarChart \n 
from . bulletChart import bulletChart \n 
from . import ipynb \n 
from tornado import locale \n 
from decimal import Decimal \n 
import dateutil \n 
from helper import * \n 
class Entity ( ) : \n 
__translation = None \n 
@ property \n 
def __user_id ( self ) : \n 
~~~ if not self . current_user : \n 
~~ if not self . current_user . get ( ) : \n 
~~ return self . current_user . get ( ) \n 
def __language ( self ) : \n 
~~~ return self . get_user_locale ( ) . code \n 
~~ def __get_system_translation ( self , field , entity_definition_keyname = , property_definition_keyname ~~~ if not self . __translation : \n 
~~~ self . __translation = { } \n 
for t in self . db_query ( ~~~ self . __translation [ % ( t . get ( ) , t . get ( ~~ ~~ return self . __translation . get ( % ( entity_definition_keyname , property_definition_keyname \n 
~~ def create_entity ( self , entity_definition_keyname , parent_entity_id = None , ignore_user = False ) : \n 
if ignore_user == True : \n 
~~~ user_id = None \n 
~~~ if not self . __user_id : \n 
~~ user_id = self . __user_id \n 
~~ if not entity_definition_keyname : \n 
entity_id = self . db_execute_lastrowid ( sql , entity_definition_keyname , user_id ) \n 
if not parent_entity_id : \n 
~~~ return entity_id \n 
~~ parent = self . db_get ( , parent_entity_id ) \n 
self . db_execute ( , parent . get ( \n 
self . db_execute ( sql , entity_id , user_id , entity_definition_keyname , parent_entity_id , entity_id \n 
self . db_execute ( sql , entity_id , user_id , entity_id ) \n 
if user_id : \n 
~~~ self . set_rights ( entity_id = entity_id , related_entity_id = user_id , right = ) \n 
~~ for default_value in self . db_query ( ~~~ self . set_property ( entity_id = entity_id , property_definition_keyname = default_value . get ( \n 
self . db_execute ( sql , entity_id , user_id , parent_entity_id ) \n 
~~ return entity_id \n 
~~ def duplicate_entity ( self , entity_id , copies = 1 , skip_property_definition_keyname = None ) : \n 
~~~ if not entity_id : \n 
~~ if skip_property_definition_keyname : \n 
~~~ if type ( skip_property_definition_keyname ) is not list : \n 
~~~ skip_property_definition_keyname = [ skip_property_definition_keyname ] \n 
~~ properties_sql = % . join ( [ % x ~~ else : \n 
~~~ properties_sql = \n 
~~ for x in range ( int ( copies ) ) : \n 
~~ ~~ def delete_entity ( self , entity_id ) : \n 
entity_id , \n 
self . __user_id \n 
~~ for child_id in self . get_relatives ( ids_only = True , entity_id = entity_id , relationship_definition_keyname ~~~ self . delete_entity ( child_id ) \n 
~~ self . db_execute ( \n 
self . db_execute ( , entity_id \n 
if not entity_id : \n 
~~ if ignore_user == True : \n 
~~ if old_property_id : \n 
~~~ definition = self . db_get ( ~~ elif property_definition_keyname : \n 
~~~ definition = self . db_get ( ~~ else : \n 
~~ if not definition : \n 
~~~ self . db_execute ( self . db_execute ( , user_id \n 
~~ if user_id : \n 
~~ ~~ if not value : \n 
~~~ self . set_mongodb_entity ( entity_id ) \n 
~~ value_display = None \n 
if definition . get ( ) == 1 : \n 
~~~ field = \n 
~~ elif definition . get ( ) == : \n 
value_display = % value \n 
value_display = value_display [ : 500 ] \n 
value = value . replace ( , ) \n 
value = float ( re . sub ( , , value ) ) \n 
value_display = round ( value , 2 ) \n 
r = self . __get_properties ( entity_id = value ) \n 
if r : \n 
~~~ value_display = self . __get_properties ( entity_id = value ) [ 0 ] [ ] \n 
~~ ~~ elif definition . get ( ) == : \n 
~~~ uploaded_file = value \n 
field = \n 
value_display = uploaded_file [ ] [ : 500 ] \n 
if uploaded_file . get ( ) : \n 
~~~ value = self . db_execute_lastrowid ( ~~ elif uploaded_file . get ( ) : \n 
~~~ value = self . db_execute_lastrowid ( ~~ else : \n 
~~~ md5 = hashlib . md5 ( uploaded_file . get ( ) ) . hexdigest ( ) \n 
directory = os . path . join ( , , , self . app_settings ( ) , md5 filename = os . path . join ( directory , md5 ) \n 
if not os . path . exists ( directory ) : \n 
~~~ os . makedirs ( directory ) \n 
~~ f = open ( filename , ) \n 
f . write ( uploaded_file . get ( , ) ) \n 
value = self . db_execute_lastrowid ( \n 
value = 1 if value . lower ( ) == else 0 \n 
value_display = % bool ( value ) \n 
value = value [ : 500 ] \n 
definition . get ( ) , \n 
value , \n 
value_display , \n 
user_id \n 
if definition . get ( ) == and uploaded_file . get ( ) : \n 
~~~ self . db_execute ( \n 
~~ self . db_execute ( , user_id \n 
self . set_mongodb_entity ( entity_id ) \n 
return new_property_id \n 
~~ def set_mongodb_entity ( self , entity_id ) : \n 
r = self . db_get ( sql , entity_id ) \n 
mysql_id = r . get ( ) \n 
e = { } \n 
e [ ] = mysql_id \n 
e [ ] = r . get ( ) \n 
if r . get ( ) : \n 
~~~ e . setdefault ( , { } ) [ ] = r . get ( ) \n 
~~ if r . get ( ) : \n 
~~ if e . get ( ) : \n 
~~~ e [ ] [ ] = \n 
e [ ] = [ e . get ( ) ] \n 
~~ if r . get ( ) and r . get ( ) : \n 
~~ viewers = self . __get_mongodb_right ( mysql_id , [ , , , ] ) \n 
if viewers : \n 
~~~ e [ ] = [ { : x , : } for x in list ( set ( viewers ) ) ] \n 
~~ expanders = self . __get_mongodb_right ( mysql_id , [ , , ] ) \n 
if expanders : \n 
~~~ e [ ] = [ { : x , : } for x in list ( set ( expanders ) ) ] \n 
~~ editors = self . __get_mongodb_right ( mysql_id , [ , ] ) \n 
if editors : \n 
~~~ e [ ] = [ { : x , : } for x in list ( set ( editors ) ) ] \n 
~~ owners = self . __get_mongodb_right ( mysql_id , [ ] ) \n 
if owners : \n 
~~~ e [ ] = [ { : x , : } for x in list ( set ( owners ) ) ] \n 
~~ parent = self . __get_mongodb_parent ( entity_id = mysql_id , recursive = False ) \n 
if parent : \n 
~~~ e [ ] = [ { : x , : } for x in list ( set ( parent ) ) ] \n 
~~ ancestor = self . __get_mongodb_parent ( entity_id = mysql_id , recursive = True ) \n 
if ancestor : \n 
~~~ e [ ] = [ { : x , : } for x in list ( set ( ancestor ) ) ] \n 
properties = { } \n 
for r2 in self . db_query ( sql , mysql_id ) : \n 
~~~ value = { } \n 
if r2 . get ( ) == and r2 . get ( ) : \n 
~~~ value [ ] = r2 . get ( ) \n 
~~ elif r2 . get ( ) == and r2 . get ( ) : \n 
~~ elif r2 . get ( ) == and r2 . get ( ) != None : \n 
~~~ value [ ] = float ( r2 . get ( ) ) \n 
~~~ value [ ] = bool ( r2 . get ( ) ) \n 
~~ elif r2 . get ( ) in [ , ] and r2 . get ( ) != ~~~ value [ ] = r2 . get ( ) \n 
if r2 . get ( ) : \n 
if r2 . get ( , None ) : \n 
~~ if r2 . get ( , None ) : \n 
~~ ~~ ~~ if not value : \n 
~~ value [ ] = r2 . get ( ) \n 
value [ ] = r2 . get ( ) \n 
~~ e . setdefault ( r2 . get ( ) , [ ] ) . append ( value ) \n 
~~~ mongo_entity_version = self . mongodb ( ) . entityVersion . find_one ( { : mysql_id } , { : if mongo_entity_version : \n 
~~~ e [ ] = mongo_entity_version . get ( ) \n 
~~~ e [ ] = self . mongodb ( ) . entity . insert_one ( { } ) . inserted_id \n 
~~ self . mongodb ( ) . entityVersion . insert_one ( e ) \n 
~~ except Exception , err : \n 
~~~ self . captureException ( ) \n 
logging . error ( % ( err , e ) ) \n 
~~ ~~ def __get_mongodb_parent ( self , entity_id , recursive = False ) : \n 
entities = [ ] \n 
for r in self . db_query ( sql ) : \n 
~~~ entities . append ( r . get ( ) ) \n 
if recursive : \n 
~~~ entities = entities + self . __get_mongodb_parent ( entity_id = r . get ( ) , recursive \n 
~~ ~~ return entities \n 
~~ def __get_mongodb_right ( self , entity_id , rights ) : \n 
~~ return entities \n 
~~ def set_counter ( self , entity_id ) : \n 
~~ if self . db_get ( , entity_id ) . get ( ~~~ parent = self . get_relatives ( related_entity_id = entity_id , relationship_definition_keyname childs = self . get_relatives ( entity_id = parent . get ( , None ) , relationship_definition_keyname if childs : \n 
~~~ childs_count = len ( [ y . get ( , 0 ) for y in childs [ 0 ] if y . get ( , { } ) . get ~~ else : \n 
~~~ childs_count = 1 \n 
~~ parent_number = . join ( [ % x [ ] for x in parent . get ( , { } ) . get ( counter_value = % ( parent_number , childs_count ) \n 
self . set_property ( entity_id = entity_id , property_definition_keyname = return counter_value \n 
self . db_execute ( , property_id \n 
return self . db_get ( , property_id ) . get ( \n 
if type ( entity_id ) is not list : \n 
~~~ entity_id = [ entity_id ] \n 
~~ if type ( related_entity_id ) is not list : \n 
~~~ related_entity_id = [ related_entity_id ] \n 
~~ if type ( relationship_definition_keyname ) is not list : \n 
~~~ relationship_definition_keyname = [ relationship_definition_keyname ] \n 
~~ for e in entity_id : \n 
~~~ for r in related_entity_id : \n 
~~~ for t in relationship_definition_keyname : \n 
~~~ self . db_execute ( , r if if delete == True : \n 
self . db_execute ( sql ) \n 
~~ elif update == True : \n 
old = self . db_execute_rowcount ( sql ) \n 
if not old : \n 
old = self . db_get ( sql ) \n 
~~ ~~ ~~ ~~ ~~ ~~ def get_rights ( self , entity_id ) : \n 
~~ rights = { } \n 
for right in [ , , , ] : \n 
relationship = self . db_query ( sql , right , entity_id ) \n 
if not relationship : \n 
~~ ids = [ x . get ( ) for x in relationship if x . get ( ) ] \n 
if ids : \n 
~~~ entities = self . __get_properties ( entity_id = ids , full_definition = False , only_public = False if entities : \n 
~~~ rights [ right ] = entities \n 
~~ ~~ ~~ return rights \n 
~~ def set_rights ( self , entity_id , related_entity_id , right = None , ignore_user = False ) : \n 
~~~ if not entity_id or not related_entity_id : \n 
~~ if type ( entity_id ) is not list : \n 
self . db_execute ( sql , user_id ) \n 
if right in [ , , , ] : \n 
~~~ for e in entity_id : \n 
~~~ for re in related_entity_id : \n 
~~ ~~ ~~ self . db_execute ( % . join ( map ( \n 
~~ def set_sharing ( self , entity_id , sharing ) : \n 
~~~ if not entity_id or not sharing : \n 
self . db_execute ( sql , sharing , self . __user_id ) \n 
~~ def set_sharing_key ( self , entity_id , generate = False ) : \n 
~~ if generate : \n 
~~~ sharing_key = . join ( random . choice ( string . ascii_letters + string . digits ) for x in range ~~ else : \n 
~~~ sharing_key = None \n 
self . db_execute ( sql , sharing_key , self . __user_id ) \n 
return sharing_key \n 
~~ def get_users ( self , search = None ) : \n 
query_re = \n 
if search != None : \n 
~~~ join_parts = [ ] \n 
for s in search . split ( ) [ : 5 ] : \n 
~~~ if not s : \n 
~~ join_parts . append ( % { : s } ) \n 
~~ query_re = . join ( join_parts ) \n 
ids = self . db_query ( sql ) \n 
entities = self . __get_properties ( entity_id = [ x . get ( ) for x in ids ] ) \n 
if not entities : \n 
ids = self . __get_id_list ( entity_id = entity_id , search = search , entity_definition_keyname = entity_definition_keyname if ids_only == True : \n 
~~~ return ids \n 
~~ entities = self . __get_properties ( entity_id = ids , entity_definition_keyname = entity_definition_keyname if not entities and full_definition == False and entity_definition_keyname == None : \n 
~~ if limit == 1 and len ( entities ) > 0 : \n 
~~~ return entities [ 0 ] \n 
select_parts = [ ] \n 
join_parts = [ ] \n 
where_parts = [ ] \n 
sql_parts = [ ] \n 
join_parts . append ( \n 
if not self . __user_id or only_public == True : \n 
~~~ join_parts . append ( \n 
~~ where_parts . append ( % ( i , s ) ) \n 
where_parts . append ( % i ) \n 
~~ ~~ if entity_definition_keyname != None : \n 
~~~ if type ( entity_definition_keyname ) is not list : \n 
~~~ entity_definition_keyname = [ entity_definition_keyname ] \n 
~~ where_parts . append ( % . join ( [ % x for x \n 
~~ if entity_id != None : \n 
~~~ if type ( entity_id ) is not list : \n 
~~ where_parts . append ( % . join ( map ( str , entity_id ) ) ) \n 
~~ if self . __user_id and only_public == False : \n 
~~~ where_parts . append ( ) \n 
join_parts . append ( ) \n 
where_parts . append ( ~~ elif sharing_key : \n 
~~~ where_parts . append ( % sharing_key ) \n 
~~~ for s in search . split ( ) : \n 
~~~ i += 1 \n 
~~ ~~ ~~ if len ( select_parts ) > 0 : \n 
~~~ sql_parts . append ( % . join ( select_parts ) ) \n 
~~ sql_parts . append ( ) \n 
if len ( join_parts ) > 0 : \n 
~~~ sql_parts . append ( % . join ( join_parts ) ) \n 
~~ if len ( where_parts ) > 0 : \n 
~~~ sql_parts . append ( % . join ( where_parts ) ) \n 
~~ if limit : \n 
~~~ limit = % limit \n 
~~~ limit = \n 
~~ sql_parts . append ( % limit ) \n 
sql = . join ( sql_parts ) \n 
items = self . db_query ( sql ) \n 
if not items : \n 
~~ return [ x . get ( ) for x in items ] \n 
items = None \n 
if entity_id : \n 
~~~ rights = public = \n 
~~ elif sharing_key : \n 
~~~ rights = \n 
public = % sharing_key \n 
public = \n 
~~ datapropertysql = \n 
if dataproperty : \n 
~~~ if type ( dataproperty ) is not list : \n 
~~~ dataproperty = [ dataproperty ] \n 
~~ datapropertysql = % . join ( [ \n 
items = { } \n 
for row in self . db_query ( sql ) : \n 
~~~ if row . get ( ) == and not row . get ( ) and not sharing_key ~~~ continue \n 
~~ if row . get ( ) in [ , ] and row . get ( ) not ~~~ continue \n 
#Entity \n 
~~ items . setdefault ( % row . get ( ) , { } ) [ ] = row . get items . setdefault ( % row . get ( ) , { } ) [ ] = row . get ( ) \n 
items . setdefault ( % row . get ( ) , { } ) [ ] = self . __get_system_translation items . setdefault ( % row . get ( ) , { } ) [ ] = self . __get_system_translation items . setdefault ( % row . get ( ) , { } ) [ ] = self . __get_system_translation items . setdefault ( % row . get ( ) , { } ) [ ] = self . __get_system_translation items . setdefault ( % row . get ( ) , { } ) [ ] = row . get ( items . setdefault ( % row . get ( ) , { } ) [ ] = row . get ( items . setdefault ( % row . get ( ) , { } ) [ ] = row . get ( items . setdefault ( % row . get ( ) , { } ) [ ] = self . __get_system_translation items . setdefault ( % row . get ( ) , { } ) [ ] = self . __get_system_translation items . setdefault ( % row . get ( ) , { } ) [ ] = self . __get_system_translation items . setdefault ( % row . get ( ) , { } ) [ ] = row . get ( items . setdefault ( % row . get ( ) , { } ) [ ] = row . get ( items . setdefault ( % row . get ( ) , { } ) [ ] = row . get ( items . setdefault ( % row . get ( ) , { } ) [ ] = row . get ( \n 
#Property \n 
items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . \n 
#Value \n 
value = row . get ( ) if row . get ( ) else \n 
if row . get ( ) == 1 : \n 
~~~ db_value = row . get ( ) \n 
~~ elif row . get ( ) == : \n 
~~~ db_value = \n 
~~ items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . items . setdefault ( % row . get ( ) , { } ) . setdefault ( , { } ) . \n 
~~ ~~ if not items : \n 
~~~ if not full_definition : \n 
~~ if entity_definition_keyname : \n 
~~ items = { } \n 
for e in entity_definition_keyname : \n 
~~~ items . setdefault ( % e , { } ) [ ] = e \n 
items . setdefault ( % e , { } ) [ ] = \n 
~~ ~~ ~~ for key , value in items . iteritems ( ) : \n 
~~~ if value . get ( , None ) : \n 
~~~ items [ key ] = dict ( items [ key ] . items ( ) + self . __get_displayfields ( value ) . items ( ) ) \n 
items [ key ] [ ] = self . __get_picture_url ( value [ ] , value [ \n 
~~ items [ key ] [ ] = True \n 
for d in self . get_definition ( entity_definition_keyname = value [ ] ) : \n 
~~~ if not value . get ( , None ) : \n 
~~~ items [ key ] [ ] = d [ ] \n 
~~ items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ] \n 
if d [ ] and len ( value . get ( , { } ) . get ( % d [ ~~~ items [ key ] [ ] = False \n 
~~ if full_definition : \n 
~~~ if not d [ ] or d [ ] > len ( value . get ( ~~~ items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ~~ if not d [ ] or d [ ] > len ( value . get ( ~~~ items [ key ] . setdefault ( , { } ) . setdefault ( % d [ ~~ else : \n 
~~~ items [ key ] . setdefault ( , { } ) . setdefault ( % d [ \n 
~~ if d [ ] and d [ ] != : \n 
~~~ for c in self . get_entities ( entity_definition_keyname = d [ ~~~ if c . get ( , None ) : \n 
~~ ~~ ~~ ~~ ~~ for p_key , p_value in value . get ( , { } ) . iteritems ( ) : \n 
~~~ if p_value . get ( ) : \n 
~~~ items [ key ] [ ] [ p_key ] [ ] = sorted ( p_value . get ( , { } ) . values ~~ if p_value [ ] == : \n 
~~~ reference_definition = self . db_get ( if reference_definition : \n 
~~~ if reference_definition . get ( ) : \n 
~~~ items [ key ] [ ] [ p_key ] [ ] = reference_definition ~~ ~~ ~~ if p_value [ ] == : \n 
~~~ for f_key , f_value in enumerate ( p_value . get ( , { } ) ) : \n 
~~~ if not f_value . get ( ) : \n 
~~ file_result = self . db_get ( if not file_result : \n 
~~ items [ key ] [ ] [ p_key ] [ ] [ f_key ] [ ] = file_result . get ( items [ key ] [ ] [ p_key ] [ ] [ f_key ] [ ] = file_result . get items [ key ] [ ] [ p_key ] [ ] [ f_key ] [ ] = file_result . get \n 
~~ ~~ ~~ ~~ return items . values ( ) \n 
~~ def __get_displayfields ( self , entity_dict ) : \n 
result = { } \n 
for displayfield in [ , , , ] : \n 
~~~ result [ displayfield ] = entity_dict . get ( displayfield , ) if entity_dict . get ( displayfield for data_property in findTags ( entity_dict . get ( displayfield , ) , , ) : \n 
~~~ dataproperty_dict = entity_dict . get ( , { } ) . get ( data_property , { } ) \n 
result [ displayfield ] = result [ displayfield ] . replace ( % data_property , . join \n 
~~ ~~ result [ ] = self . __get_system_translation ( field = , entity_definition_keyname if not result [ ] : \n 
~~~ result [ ] = entity_dict . get ( , ) if entity_dict . get ( for data_property in findTags ( entity_dict . get ( , ) , , ) : \n 
~~~ result [ ] = result [ ] . replace ( % data_property \n 
~~ ~~ result [ ] = result [ ] . split ( ) if result [ ] else None result [ ] = result [ ] . split ( ) if result [ \n 
if entity_dict . get ( , None ) and entity_dict . get ( , None ) != result [ ] : \n 
~~~ self . db_execute ( , result [ ] , \n 
~~ def __get_picture_url ( self , entity_id , entity_definition_keyname ) : \n 
f = self . db_get ( sql , entity_id ) \n 
if f : \n 
~~~ return % entity_id \n 
~~ elif entity_definition_keyname in [ , , , , ~~~ return % entity_id \n 
~~ elif entity_definition_keyname == : \n 
~~~ return % ( hashlib . md5 ( str ( entity_id ~~ else : \n 
~~~ return % ( hashlib . md5 ( str ( entity_id \n 
~~ ~~ def get_definition ( self , entity_definition_keyname ) : \n 
if not entity_definition_keyname : \n 
~~ if type ( entity_definition_keyname ) is not list : \n 
defs = [ ] \n 
~~~ defs . append ( { \n 
: r . get ( ) , \n 
: self . __get_system_translation ( field = , entity_definition_keyname : self . __get_system_translation ( field = , entity_definition_keyname : self . __get_system_translation ( field = , entity_definition_keyname : self . __get_system_translation ( field = , entity_definition_keyname : self . __get_system_translation ( field = , entity_definition_keyname : self . __get_system_translation ( field = , entity_definition_keyname : r . get ( ) , \n 
: self . __get_system_translation ( field = , property_definition_keyname : self . __get_system_translation ( field = , property_definition_keyname : self . __get_system_translation ( field = , property_definition_keyname : self . __get_system_translation ( field = , property_definition_keyname : r . get ( ) , \n 
: bool ( r . get ( ) ) , \n 
~~ return defs \n 
~~ def get_entity_definition ( self , entity_definition_keyname ) : \n 
if entity_definition_keyname : \n 
for d in self . db_query ( sql ) : \n 
: d . get ( ) , \n 
: self . __get_system_translation ( field = , entity_definition_keyname = d . get : self . __get_system_translation ( field = , entity_definition_keyname : self . __get_system_translation ( field = , entity_definition_keyname : self . __get_system_translation ( field = , entity_definition_keyname = d : d . get ( ) , \n 
: d . get ( ) \n 
~~ ~~ if related_entity_id : \n 
~~~ if type ( related_entity_id ) is not list : \n 
~~ ~~ if relationship_definition_keyname : \n 
~~~ if type ( relationship_definition_keyname ) is not list : \n 
~~ ~~ if entity_definition_keyname : \n 
~~ ~~ unionsql = \n 
if reverse_relation == True : \n 
if not ids_only : \n 
~~ ~~ if entity_id : \n 
~~~ sql += % . join ( map ( str , entity_id ) ) \n 
~~ if related_entity_id : \n 
~~~ sql += % . join ( map ( str , related_entity_id ) ) \n 
~~~ sql += ~~ else : \n 
~~~ sql += \n 
~~ if relationship_definition_keyname : \n 
~~~ sql += % . join ( [ % x for x in \n 
~~~ sql += % . join ( [ % x for x in entity_definition_keyname \n 
~~ if unionsql : \n 
~~~ sql += unionsql \n 
~~ ~~ sql += \n 
if limit : \n 
~~~ sql += % limit \n 
~~ sql += \n 
if ids_only == True : \n 
~~~ items = [ ] \n 
for item in self . db_query ( sql ) : \n 
~~~ items . append ( item . get ( ) ) \n 
~~ ~~ elif relationship_ids_only == True : \n 
~~~ items = { } \n 
~~~ ent = self . __get_properties ( entity_id = item . get ( ) , full_definition = full_definition if not ent : \n 
~~ ent = ent [ 0 ] \n 
items . setdefault ( % ent . get ( , ) , [ ] ) . append ( ent ) \n 
~~ ~~ return items \n 
~~ def get_file ( self , file_id , sharing_key = None ) : \n 
if type ( file_id ) is not list : \n 
~~~ file_id = [ file_id ] \n 
~~ if self . __user_id : \n 
~~~ user_where = % sharing_key ~~ else : \n 
~~~ user_where = \n 
for f in self . db_query ( sql ) : \n 
~~~ if f . get ( ) : \n 
~~~ filename = os . path . join ( , , , self . app_settings ( ) , f . if os . path . isfile ( filename ) : \n 
~~~ with open ( filename , ) as myfile : \n 
~~~ filecontent = myfile . read ( ) \n 
~~~ filecontent = None \n 
~~ result . append ( { \n 
: f . get ( ) , \n 
: filecontent , \n 
: f . get ( ) \n 
~~ def get_allowed_childs ( self , entity_id ) : \n 
if not self . db_get ( ~~~ return [ ] \n 
result = self . db_query ( sql ) \n 
~~ defs = [ ] \n 
for d in result : \n 
: self . __get_system_translation ( field = , entity_definition_keyname = d . get : self . __get_system_translation ( field = , entity_definition_keyname : self . __get_system_translation ( field = , entity_definition_keyname : self . __get_system_translation ( field = , entity_definition_keyname = d } ) \n 
~~ def get_allowed_parents ( self , entity_id ) : \n 
~~ def get_definitions_with_optional_parent ( self , entity_definition_keyname ) : \n 
~~~ related_entity = self . get_entities ( entity_id = d . get ( ) , limit = 1 ) \n 
defs . append ( { \n 
: related_entity . get ( ) if related_entity else , } ) \n 
~~ def get_public_paths ( self ) : \n 
paths = { } \n 
for i in self . db_query ( ~~~ paths [ i . get ( ) ] = self . __get_system_translation ( field = , entity_definition_keyname ~~ return paths \n 
~~ def get_public_path ( self , entity_id ) : \n 
return path \n 
~~ def get_menu ( self ) : \n 
if self . __user_id : \n 
menu = { } \n 
for m in self . db_query ( sql ) : \n 
~~~ menu . setdefault ( m . get ( ) , { } ) [ ] = m . get ( ) \n 
menu . setdefault ( m . get ( ) , { } ) . setdefault ( , [ ] ) . append ( { : m . get ( \n 
~~ return sorted ( menu . values ( ) , key = itemgetter ( ) ) \n 
~~ ~~ def findTags ( s , beginning , end ) : \n 
if not s : \n 
~~ return re . compile ( % ( beginning , end ) , re . DOTALL ) . findall ( s ) \n 
~~ version = ( 0 , 1 , 8 ) from django . utils . encoding import force_text \n 
from bson import ObjectId \n 
from rest_framework . utils . encoders import JSONEncoder \n 
from rest_framework . renderers import JSONRenderer \n 
class MongoDBJSONEncoder ( JSONEncoder ) : \n 
~~~ def default ( self , obj ) : \n 
~~~ if isinstance ( obj , ObjectId ) : \n 
~~~ return force_text ( obj ) \n 
~~ return super ( MongoDBJSONEncoder , self ) . default ( obj ) \n 
~~ ~~ class MongoDBJSONRenderer ( JSONRenderer ) : \n 
~~~ encoder_class = MongoDBJSONEncoder \n 
~~ NEWS_TYPE_CONTENTION = 0 \n 
NEWS_TYPE_PREMISE = 1 \n 
NEWS_TYPE_FALLACY = 2 \n 
NEWS_TYPE_FOLLOWING = 3 \n 
from __future__ import unicode_literals \n 
from django . db import models , migrations \n 
class Migration ( migrations . Migration ) : \n 
~~~ dependencies = [ \n 
( , ) , \n 
operations = [ \n 
migrations . AddField ( \n 
model_name = , \n 
field = models . CharField ( default = , max_length = 25 ) , \n 
preserve_default = False , \n 
migrations . AlterField ( \n 
field = models . CharField ( max_length = 255 , db_index = True ) , \n 
migrations . AlterUniqueTogether ( \n 
unique_together = set ( [ ( , ) ] ) , \n 
~~ from __future__ import unicode_literals \n 
migrations . RenameField ( \n 
old_name = , \n 
new_name = , \n 
~~ extensions = [ \n 
release = open ( ) . read ( ) . split ( ) [ 0 ] . strip ( ) \n 
p = re . compile ( , re . IGNORECASE ) \n 
vers = p . search ( release ) \n 
version = vers . group ( 2 ) \n 
html_favicon = \n 
#<<EOS_COMMON_MODULE_START>> \n 
import syslog \n 
from ansible . module_utils . basic import * \n 
~~~ import pyeapi \n 
PYEAPI_AVAILABLE = True \n 
~~~ PYEAPI_AVAILABLE = False \n 
~~ DEFAULT_SYSLOG_PRIORITY = syslog . LOG_NOTICE \n 
DEFAULT_CONNECTION = \n 
TRANSPORTS = [ , , , ] \n 
class EosConnection ( object ) : \n 
~~~ __attributes__ = [ , , , , ] \n 
~~~ self . connection = kwargs [ ] \n 
self . transport = kwargs . get ( ) \n 
self . username = kwargs . get ( ) \n 
self . password = kwargs . get ( ) \n 
self . host = kwargs . get ( ) \n 
self . port = kwargs . get ( ) \n 
self . config = kwargs . get ( ) \n 
~~ def connect ( self ) : \n 
~~~ if self . config is not None : \n 
~~~ pyeapi . load_config ( self . config ) \n 
~~ config = dict ( ) \n 
if self . connection is not None : \n 
~~~ config = pyeapi . config_for ( self . connection ) \n 
if not config : \n 
~~ ~~ for key in self . __attributes__ : \n 
~~~ if getattr ( self , key ) is not None : \n 
~~~ config [ key ] = getattr ( self , key ) \n 
~~ ~~ if not in config : \n 
~~ connection = pyeapi . client . make_connection ( ** config ) \n 
node = pyeapi . client . Node ( connection , ** config ) \n 
~~~ node . enable ( ) \n 
~~ except ( pyeapi . eapilib . ConnectionError , pyeapi . eapilib . CommandError ) : \n 
~~~ raise ValueError ( . format ( node ) ) \n 
~~ return node \n 
~~ ~~ class EosAnsibleModule ( AnsibleModule ) : \n 
~~~ meta_args = { \n 
: dict ( ) , \n 
: dict ( default = DEFAULT_CONNECTION ) , \n 
: dict ( choices = TRANSPORTS ) , \n 
: dict ( type = , default = ) , \n 
: dict ( type = , default = ) \n 
stateful_args = { \n 
: dict ( default = , choices = [ , ] ) , \n 
def __init__ ( self , stateful = True , autorefresh = False , * args , ** kwargs ) : \n 
~~~ kwargs [ ] . update ( self . meta_args ) \n 
self . _stateful = stateful \n 
if stateful : \n 
~~~ kwargs [ ] . update ( self . stateful_args ) \n 
~~ self . _logging = kwargs . get ( ) \n 
super ( EosAnsibleModule , self ) . __init__ ( * args , ** kwargs ) \n 
self . result = dict ( changed = False , changes = dict ( ) ) \n 
self . _debug = kwargs . get ( ) or self . boolean ( self . params [ ] ) \n 
self . _logging = kwargs . get ( ) or self . params [ ] \n 
self . log ( % self . _debug ) \n 
self . debug ( , self . check_pyeapi ( ) ) \n 
self . debug ( , self . _stateful ) \n 
self . debug ( , self . params ) \n 
self . _attributes = self . map_argument_spec ( ) \n 
self . validate ( ) \n 
self . _autorefresh = autorefresh \n 
self . _node = EosConnection ( ** self . params ) \n 
self . _node . connect ( ) \n 
self . _node = self . connect ( ) \n 
self . _instance = None \n 
self . desired_state = self . params [ ] if self . _stateful else None \n 
self . exit_after_flush = kwargs . get ( ) \n 
def instance ( self ) : \n 
~~~ if self . _instance : \n 
~~~ return self . _instance \n 
~~ func = self . func ( ) \n 
if not func : \n 
~~~ self . _instance = func ( self ) \n 
~~ except Exception as exc : \n 
~~~ self . fail ( % exc . message ) \n 
return self . _instance \n 
def attributes ( self ) : \n 
~~~ return self . _attributes \n 
def node ( self ) : \n 
~~~ return self . _node \n 
~~ def check_pyeapi ( self ) : \n 
~~~ if not PYEAPI_AVAILABLE : \n 
~~ return pyeapi . __version__ \n 
~~ def map_argument_spec ( self ) : \n 
keys = set ( self . params ) . difference ( self . meta_args ) \n 
attrs = dict ( ) \n 
attrs = dict ( [ ( k , self . params [ k ] ) for k in self . params if k in keys ] ) \n 
if in attrs : \n 
~~~ del attrs [ ] \n 
~~ return attrs \n 
~~ def validate ( self ) : \n 
~~~ for key , value in self . attributes . iteritems ( ) : \n 
~~~ func = self . func ( % key ) \n 
if func : \n 
~~~ self . attributes [ key ] = func ( value ) \n 
~~ ~~ ~~ def create ( self ) : \n 
~~~ if not self . check_mode : \n 
~~~ func = self . func ( ) \n 
~~ return self . invoke ( func , self ) \n 
~~ ~~ def remove ( self ) : \n 
~~ ~~ def flush ( self , exit_after_flush = False ) : \n 
~~~ self . exit_after_flush = exit_after_flush \n 
if self . desired_state == or not self . _stateful : \n 
~~~ if self . instance . get ( ) == : \n 
~~~ changed = self . create ( ) \n 
self . result [ ] = changed or True \n 
self . refresh ( ) \n 
self . _node . _running_config = None \n 
~~ changeset = self . attributes . viewitems ( ) - self . instance . viewitems ( ) \n 
if self . _debug : \n 
~~~ self . debug ( , self . attributes ) \n 
self . debug ( , self . instance ) \n 
~~ changes = self . update ( changeset ) \n 
if changes : \n 
~~~ self . result [ ] = changes \n 
self . result [ ] = True \n 
~~ self . _attributes . update ( changes ) \n 
flush = self . func ( ) \n 
if flush : \n 
~~~ self . invoke ( flush , self ) \n 
~~ ~~ elif self . desired_state == and self . _stateful : \n 
~~~ changed = self . remove ( ) \n 
~~ ~~ elif self . _stateful : \n 
~~~ if self . desired_state != self . instance . get ( ) : \n 
~~~ func = self . func ( self . desired_state ) \n 
changed = self . invoke ( func , self ) \n 
~~ ~~ self . refresh ( ) \n 
~~~ self . result [ ] = self . instance \n 
~~ if self . exit_after_flush : \n 
~~~ self . exit ( ) \n 
~~ ~~ def update ( self , changeset ) : \n 
~~~ changes = dict ( ) \n 
for key , value in changeset : \n 
~~~ changes [ key ] = value \n 
func = self . func ( % key ) \n 
if func and not self . check_mode : \n 
~~~ self . invoke ( func , self ) \n 
~~~ self . fail ( exc . message ) \n 
~~ ~~ ~~ ~~ return changes \n 
~~~ if self . params [ ] : \n 
~~~ pyeapi . load_config ( self . params [ ] ) \n 
if self . params [ ] : \n 
~~~ config = pyeapi . config_for ( self . params [ ] ) \n 
self . fail ( msg ) \n 
~~ ~~ if self . params [ ] : \n 
~~~ config [ ] = self . params [ ] \n 
~~ if self . params [ ] : \n 
~~ if not in config : \n 
self . log ( % self . _autorefresh ) \n 
node = pyeapi . client . Node ( connection , autorefresh = self . _autorefresh , \n 
** config ) \n 
~~~ resp = node . enable ( ) \n 
self . debug ( , resp [ 0 ] [ ] [ ] ) \n 
~~~ self . fail ( % node ) \n 
~~~ self . log ( % node ) \n 
self . debug ( , str ( node ) ) \n 
~~ def config ( self , commands ) : \n 
~~~ self . result [ ] = True \n 
if not self . check_mode : \n 
~~~ self . node . config ( commands ) \n 
~~ ~~ def api ( self , module ) : \n 
~~~ return self . node . api ( module ) \n 
~~ def func ( self , name ) : \n 
~~~ return globals ( ) . get ( name ) \n 
~~ def invoke ( self , func , * args , ** kwargs ) : \n 
~~~ return func ( * args , ** kwargs ) \n 
~~ ~~ def invoke_function ( self , name , * args , ** kwargs ) : \n 
~~~ func = self . func ( name ) \n 
~~~ return self . invoke ( func , * args , ** kwargs ) \n 
~~ ~~ def fail ( self , msg ) : \n 
~~~ self . invoke_function ( , self ) \n 
self . log ( % msg , syslog . LOG_ERR ) \n 
self . fail_json ( msg = msg ) \n 
~~ def exit ( self ) : \n 
self . log ( ) \n 
self . exit_json ( ** self . result ) \n 
~~ def refresh ( self ) : \n 
~~~ self . _instance = None \n 
~~ def debug ( self , key , value ) : \n 
~~~ if self . _debug : \n 
~~~ if not in self . result : \n 
~~~ self . result [ ] = dict ( ) \n 
~~ self . result [ ] [ key ] = value \n 
~~ ~~ def log ( self , message , log_args = None , priority = None ) : \n 
~~~ if self . _logging : \n 
~~~ syslog . openlog ( ) \n 
priority = priority or DEFAULT_SYSLOG_PRIORITY \n 
syslog . syslog ( priority , str ( message ) ) \n 
def add_state ( cls , name ) : \n 
~~~ cls . stateful_args [ ] [ ] . append ( name ) \n 
#<<EOS_COMMON_MODULE_END>> \n 
~~ ~~ def instance ( module ) : \n 
name = module . attributes [ ] \n 
result = module . node . api ( ) . get ( name ) \n 
_instance = dict ( name = name , state = ) \n 
~~~ _instance [ ] = \n 
_instance [ ] = not result [ ] \n 
desc = if not result [ ] else result [ ] \n 
_instance [ ] = desc \n 
_instance [ ] = result [ ] \n 
~~ return _instance \n 
~~ def create ( module ) : \n 
module . log ( % name ) \n 
module . node . api ( ) . create ( name ) \n 
~~ def remove ( module ) : \n 
module . node . api ( ) . delete ( name ) \n 
~~ def set_description ( module ) : \n 
value = module . attributes [ ] \n 
module . log ( \n 
% ( name , value ) ) \n 
if value == : \n 
~~~ module . node . api ( ) . set_description ( name , value , disable = True ) \n 
~~~ module . node . api ( ) . set_description ( name , value ) \n 
~~ ~~ def set_enable ( module ) : \n 
module . node . api ( ) . set_shutdown ( name , disable = value ) \n 
~~ def set_source_interface ( module ) : \n 
module . node . api ( ) . set_source_interface ( name , value ) \n 
~~ def set_multicast_group ( module ) : \n 
module . node . api ( ) . set_multicast_group ( name , value ) \n 
~~ def set_udp_port ( module ) : \n 
module . node . api ( ) . set_udp_port ( name , value ) \n 
argument_spec = dict ( \n 
name = dict ( ) , \n 
enable = dict ( type = , default = ) , \n 
description = dict ( ) , \n 
source_interface = dict ( ) , \n 
multicast_group = dict ( ) , \n 
udp_port = dict ( type = ) \n 
module = EosAnsibleModule ( argument_spec = argument_spec , \n 
supports_check_mode = True ) \n 
module . flush ( True ) \n 
~~ main ( ) # \n 
from pyeapi . api import Entity \n 
class System ( Entity ) : \n 
def get ( self ) : \n 
resource = dict ( ) \n 
resource . update ( self . _parse_hostname ( ) ) \n 
resource . update ( self . _parse_iprouting ( ) ) \n 
resource . update ( self . _parse_banners ( ) ) \n 
return resource \n 
~~ def _parse_hostname ( self ) : \n 
match = re . search ( , self . config , re . M ) \n 
~~~ value = match . group ( 1 ) \n 
~~ return dict ( hostname = value ) \n 
~~ def _parse_iprouting ( self ) : \n 
value = not in self . config \n 
return dict ( iprouting = value ) \n 
~~ def _parse_banners ( self ) : \n 
motd_value = login_value = None \n 
matches = re . findall ( , \n 
self . config , re . DOTALL | re . M ) \n 
for match in matches : \n 
~~~ if match [ 0 ] . strip ( ) == "motd" : \n 
~~~ motd_value = match [ 1 ] \n 
~~ elif match [ 0 ] . strip ( ) == "login" : \n 
~~~ login_value = match [ 1 ] \n 
~~ ~~ return dict ( banner_motd = motd_value , banner_login = login_value ) \n 
~~ def set_hostname ( self , value = None , default = False , disable = False ) : \n 
cmd = self . command_builder ( , value = value , default = default , \n 
disable = disable ) \n 
return self . configure ( cmd ) \n 
~~ def set_iprouting ( self , value = None , default = False , disable = False ) : \n 
if value is False : \n 
~~~ disable = True \n 
~~ cmd = self . command_builder ( , value = value , default = default , \n 
~~ def set_banner ( self , banner_type , value = None , default = False , \n 
disable = False ) : \n 
if default is True or disable is True : \n 
~~~ cmd = self . command_builder ( command_string , value = None , \n 
default = default , disable = disable ) \n 
~~~ if not value . endswith ( "\\n" ) : \n 
~~~ value = value + "\\n" \n 
~~ command_input = dict ( cmd = command_string , input = value ) \n 
return self . configure ( [ command_input ] ) \n 
~~ ~~ ~~ def instance ( api ) : \n 
return System ( api ) \n 
sys . path . append ( os . path . join ( os . path . dirname ( __file__ ) , ) ) \n 
from testlib import get_fixture , function \n 
from testlib import EapiConfigUnitTest \n 
import pyeapi . api . bgp \n 
class TestApiBgp ( EapiConfigUnitTest ) : \n 
~~~ super ( TestApiBgp , self ) . __init__ ( * args , ** kwargs ) \n 
self . instance = pyeapi . api . bgp . instance ( None ) \n 
self . config = open ( get_fixture ( ) ) . read ( ) \n 
~~ def test_get ( self ) : \n 
~~~ result = self . instance . get ( ) \n 
keys = [ , , , , \n 
self . assertEqual ( sorted ( keys ) , sorted ( result . keys ( ) ) ) \n 
~~~ for bgpas in [ , 65000 ] : \n 
~~~ func = function ( , bgpas ) \n 
cmds = . format ( bgpas ) \n 
self . eapi_positive_config_test ( func , cmds ) \n 
~~ ~~ def test_create_invalid_as ( self ) : \n 
~~~ for bgpas in [ , 66000 ] : \n 
~~~ with self . assertRaises ( ValueError ) : \n 
~~~ self . instance . create ( bgpas ) \n 
~~ ~~ ~~ def test_delete ( self ) : \n 
~~~ func = function ( ) \n 
cmds = \n 
~~ def test_default ( self ) : \n 
~~ def test_add_network ( self ) : \n 
~~~ func = function ( , , , ) \n 
cmds = [ , ] \n 
func = function ( , , , ) \n 
self . eapi_exception_config_test ( func , ValueError ) \n 
~~ def test_remove_network ( self ) : \n 
~~ def test_set_router_id ( self ) : \n 
~~~ for state in [ , , ] : \n 
~~~ rid = \n 
if state == : \n 
~~~ cmds = [ , ] \n 
func = function ( , rid ) \n 
~~ elif state == : \n 
func = function ( , None , False , True ) \n 
func = function ( , rid , True ) \n 
~~ self . eapi_positive_config_test ( func , cmds ) \n 
~~ cmds = [ , ] \n 
func = function ( , None ) \n 
~~ def test_maximum_paths_just_max_path ( self ) : \n 
~~~ max_paths = 20 \n 
func = function ( , max_paths ) \n 
func = function ( , disable = True ) \n 
func = function ( , default = True ) \n 
~~ def test_maximum_paths_max_path_and_ecmp ( self ) : \n 
max_ecmp_path = 20 \n 
func = function ( , max_paths , max_ecmp_path ) \n 
~~ func = function ( , max_path = None , max_ecmp_path = 20 , \n 
default = False , disable = False ) \n 
self . eapi_exception_config_test ( func , TypeError ) \n 
~~ def test_set_shutdown ( self ) : \n 
~~~ if state == : \n 
func = function ( , default = False , disable = False ) \n 
~~ ~~ ~~ class TestApiBgpNeighbor ( EapiConfigUnitTest ) : \n 
~~~ super ( TestApiBgpNeighbor , self ) . __init__ ( * args , ** kwargs ) \n 
self . instance = pyeapi . api . bgp . BgpNeighbors ( None ) \n 
~~ def test_getall ( self ) : \n 
~~~ result = self . instance . getall ( ) \n 
self . assertIsInstance ( result , dict ) \n 
self . assertEqual ( len ( result ) , 3 ) \n 
~~ def test_delete ( self ) : \n 
func = function ( , ) \n 
~~ def test_set_peer_group ( self ) : \n 
~~~ peer_group = \n 
cmd = . format ( name ) \n 
~~~ cmds = [ , . format ( cmd , peer_group ) ] \n 
func = function ( , name , peer_group ) \n 
~~~ cmds = [ , . format ( cmd ) ] \n 
func = function ( , name , disable = True ) \n 
func = function ( , name , peer_group , \n 
default = True ) \n 
func = function ( , , None ) \n 
~~ def test_set_remote_as ( self ) : \n 
~~~ remote_as = \n 
~~~ cmds = [ , . format ( cmd , remote_as ) ] \n 
func = function ( , name , remote_as ) \n 
func = function ( , name , remote_as , default = True ) \n 
~~~ for state in [ , , , ] : \n 
func = function ( , name , default = False , \n 
disable = False ) \n 
func = function ( , name , default = True ) \n 
~~ ~~ def test_set_send_community ( self ) : \n 
func = function ( , name , value = True ) \n 
~~~ cmds = [ , \n 
. format ( cmd ) ] \n 
func = function ( , name , value = False , \n 
~~ def test_set_next_hop_self ( self ) : \n 
~~ def test_set_route_map_in ( self ) : \n 
~~~ route_map = \n 
~~~ cmds = [ , . format ( cmd , route_map ) ] \n 
func = function ( , name , value = route_map ) \n 
func = function ( , name , value = route_map , \n 
~~ def test_set_route_map_out ( self ) : \n 
~~ def test_set_description ( self ) : \n 
~~~ value = \n 
~~~ cmds = [ , . format ( cmd , value ) ] \n 
func = function ( , name , value = value ) \n 
func = function ( , name , value = value , \n 
sys . path . append ( ) \n 
from client_test_lib import Bootstrap , ActionFailureTest \n 
from client_test_lib import get_action , random_string \n 
from client_test_lib import startup_config_action \n 
from client_test_lib import raise_exception \n 
class FailureTest ( ActionFailureTest ) : \n 
~~~ def test_missing_smarthost ( self ) : \n 
~~~ self . basic_test ( , \n 
~~ def test_missing_sender ( self ) : \n 
attributes = { : \n 
random_string ( ) } ) \n 
~~ def test_missing_receivers ( self ) : \n 
random_string ( ) , \n 
~~ ~~ class SuccessTest ( unittest . TestCase ) : \n 
~~~ def test_success ( self ) : \n 
~~~ bootstrap = Bootstrap ( ztps_default_config = True ) \n 
smarthost = "%s:2525" % str ( bootstrap . server ) . split ( ) [ 0 ] \n 
bootstrap . ztps . set_definition_response ( \n 
actions = [ { : } , \n 
{ : , \n 
: smarthost , \n 
: [ ] } } ] ) \n 
bootstrap . ztps . set_action_response ( , \n 
startup_config_action ( ) ) \n 
action = get_action ( ) \n 
action ) \n 
bootstrap . start_test ( ) \n 
~~~ self . failUnless ( bootstrap . success ( ) ) \n 
~~~ print % bootstrap . output \n 
print % bootstrap . error \n 
raise_exception ( exception ) \n 
~~~ bootstrap . end_test ( ) \n 
~~ import logging \n 
from urlparse import urlsplit , urlunsplit \n 
log = logging . getLogger ( __name__ ) \n 
def atoi ( text ) : \n 
~~~ return int ( text ) if text . isdigit ( ) else text \n 
~~ def natural_keys ( text ) : \n 
~~~ return [ atoi ( c ) for c in re . split ( , text ) ] \n 
~~ ETHERNET_RE = re . compile ( ) \n 
MANAGEMENT_RE = re . compile ( ) \n 
INTERFACE_NO_RE = re . compile ( ) \n 
def expand_range ( interfaces ) : \n 
items = set ( ) \n 
prefix = None \n 
for group in [ x . strip ( ) for x in interfaces . split ( ) ] : \n 
~~~ ranges = [ x . strip ( ) for x in group . split ( ) ] \n 
if len ( ranges ) == 1 : \n 
~~~ interface = ranges [ 0 ] . lower ( ) \n 
match = ETHERNET_RE . match ( interface ) \n 
~~~ intf_no = interface [ len ( match . groups ( ) [ 0 ] ) : ] \n 
for token in intf_no . split ( ) : \n 
~~~ if int ( token ) < 1 : \n 
~~~ log . warning ( \n 
group ) \n 
raise TypeError ( \n 
~~ ~~ prefix = \n 
items . add ( % ( prefix , intf_no ) ) \n 
~~~ match = MANAGEMENT_RE . match ( interface ) \n 
~~~ match = INTERFACE_NO_RE . match ( interface ) \n 
~~~ for token in interface . split ( ) : \n 
~~ ~~ items . add ( % ( prefix , interface ) ) \n 
% group ) \n 
~~ ~~ ~~ ~~ elif len ( ranges ) == 2 : \n 
~~~ [ start , end ] = [ x . lower ( ) for x in ranges ] \n 
items_len = len ( items ) \n 
for regex , intf_type in [ ( ETHERNET_RE , ) , \n 
( MANAGEMENT_RE , ) , \n 
( INTERFACE_NO_RE , prefix ) ] : \n 
~~~ match_start = regex . match ( start ) \n 
if match_start : \n 
~~~ if regex != INTERFACE_NO_RE : \n 
~~~ start_intf_tokens = start [ len ( match_start . groups ( ) [ 0 ] ) : ] . split ( ) \n 
~~~ start_intf_tokens = start . split ( ) \n 
~~ match_end = regex . match ( end ) \n 
end_intf_tokens = None \n 
if match_end : \n 
~~~ end_intf_tokens = end [ len ( match_end . groups ( ) [ 0 ] ) : ] . split ( ) \n 
~~~ end_intf_tokens = end . split ( ) \n 
~~~ match_end = INTERFACE_NO_RE . match ( end ) \n 
~~ ~~ if not end_intf_tokens : \n 
~~ if start_intf_tokens [ : - 1 ] != end_intf_tokens [ : - 1 ] : \n 
~~ start_index = int ( start_intf_tokens [ - 1 ] ) \n 
end_index = int ( end_intf_tokens [ - 1 ] ) \n 
if start_index >= end_index : \n 
~~ if start_index < 1 or end_index < 1 : \n 
~~ for index in range ( start_index , \n 
end_index ) : \n 
~~~ prefix = intf_type \n 
items . add ( % \n 
( intf_type , \n 
. join ( start_intf_tokens [ : - 1 ] + \n 
[ str ( index ) ] ) ) ) \n 
~~ ~~ ~~ if items_len == len ( items ) : \n 
~~~ log . warning ( % \n 
raise TypeError ( % \n 
~~ ~~ log . debug ( % ( interfaces , items ) ) \n 
return items \n 
~~ def parse_interface ( neighbor , node_id ) : \n 
~~~ if hasattr ( neighbor , ) : \n 
~~~ remote_device = neighbor [ ] \n 
remote_interface = neighbor . get ( , ) \n 
~~~ if neighbor == : \n 
~~~ remote_device , remote_interface = , \n 
~~ elif neighbor == : \n 
~~ elif not in neighbor : \n 
~~~ remote_device = neighbor \n 
remote_interface = \n 
~~~ tokens = neighbor . split ( ) \n 
remote_device = tokens [ 0 ] \n 
remote_interface = tokens [ 1 ] \n 
~~ ~~ remote_device = str ( remote_device ) . strip ( ) \n 
if len ( remote_device . split ( ) ) != 1 : \n 
~~~ log . error ( % \n 
( node_id , remote_device ) ) \n 
raise Exception ( \n 
% ( node_id , remote_device ) ) \n 
~~ remote_interface = str ( remote_interface ) . strip ( ) \n 
if len ( remote_interface . split ( ) ) != 1 : \n 
~~~ log . error ( \n 
( node_id , remote_interface ) ) \n 
~~ return ( remote_device , remote_interface ) \n 
~~ except KeyError as err : \n 
( node_id , str ( err ) ) ) \n 
% ( node_id , str ( err ) ) ) \n 
~~ ~~ def url_path_join ( * parts ) : \n 
schemes , netlocs , paths , queries , fragments = zip ( * ( urlsplit ( part ) for part in parts ) ) \n 
scheme = get_first_token ( schemes ) \n 
netloc = get_first_token ( netlocs ) \n 
path = . join ( x . strip ( ) for x in paths if x ) \n 
query = get_first_token ( queries ) \n 
fragment = get_first_token ( fragments ) \n 
return urlunsplit ( ( scheme , netloc , path , query , fragment ) ) \n 
~~ def get_first_token ( sequence ) : \n 
~~~ return next ( ( x for x in sequence if x ) , ) \n 
~~ def all_files ( path ) : \n 
for top , _ , files in os . walk ( path ) : \n 
~~~ result += [ os . path . join ( top , f ) for f in files ] \n 
import haystack . indexes as indexes \n 
from aristotle_mdr . contrib . help import models \n 
from django . template import TemplateDoesNotExist \n 
from django . utils import timezone \n 
from aristotle_mdr . search_indexes import RESTRICTION \n 
class HelpObjectIndex ( indexes . SearchIndex ) : \n 
~~~ text = indexes . CharField ( document = True , use_template = True ) \n 
modified = indexes . DateTimeField ( model_attr = ) \n 
created = indexes . DateTimeField ( model_attr = ) \n 
name = indexes . CharField ( model_attr = ) \n 
facet_model_ct = indexes . IntegerField ( faceted = True ) \n 
is_public = indexes . BooleanField ( model_attr = ) \n 
restriction = indexes . IntegerField ( faceted = True ) \n 
def get_model ( self ) : \n 
~~ def index_queryset ( self , using = None ) : \n 
return self . get_model ( ) . objects . filter ( modified__lte = timezone . now ( ) ) \n 
~~ def prepare_restriction ( self , obj ) : \n 
~~~ return RESTRICTION [ ] \n 
~~ def prepare_facet_model_ct ( self , obj ) : \n 
~~~ from django . contrib . contenttypes . models import ContentType \n 
ct = ContentType . objects . get_for_model ( obj ) \n 
return ct . pk \n 
~~ ~~ class ConceptHelpIndex ( HelpObjectIndex , indexes . Indexable ) : \n 
~~~ template_name = "search/searchConceptHelp.html" \n 
~~~ return models . ConceptHelp \n 
~~ ~~ class HelpPageIndex ( HelpObjectIndex , indexes . Indexable ) : \n 
~~~ template_name = "search/searchHelpPage.html" \n 
~~~ return models . HelpPage \n 
~~ ~~ from __future__ import unicode_literals \n 
unique_together = None , \n 
migrations . RemoveField ( \n 
migrations . DeleteModel ( \n 
~~ from aristotle_mdr . apps import AristotleExtensionBaseConfig \n 
class DownloadTestMDRConfig ( AristotleExtensionBaseConfig ) : \n 
~~ import notifications . urls \n 
import autocomplete_light \n 
from django . conf . urls import patterns , include , url \n 
from django . contrib . auth . views import password_reset \n 
from django . contrib import admin \n 
from django . views . generic . base import RedirectView \n 
from aristotle_mdr . views . user_pages import friendly_redirect_login \n 
autocomplete_light . autodiscover ( ) \n 
admin . autodiscover ( ) \n 
urlpatterns = [ \n 
url ( , friendly_redirect_login , name = ) , \n 
url ( , , { : } ) , \n 
url ( , include ( ) ) , \n 
url ( , include ( admin . site . urls ) ) , \n 
url ( , include ( notifications . urls , namespace = "notifications" ) ) , \n 
url ( , password_reset ) , url ( , password_reset ) , url ( \n 
{ : } , \n 
name = "password_reset" \n 
url ( \n 
name = "password_reset_done" \n 
url ( , \n 
{ : } ) , \n 
url ( , RedirectView . as_view ( url = , permanent = True ) ) , \n 
url ( , , name = url ( , , name = ] \n 
if settings . DEBUG : \n 
~~~ from django . conf . urls . static import static \n 
urlpatterns += static ( settings . MEDIA_URL , document_root = settings . MEDIA_ROOT ) \n 
from twitter import * \n 
parser . add_argument ( , dest = , default = None , type = argparse . FileType ( ) ) \n 
parser . add_argument ( , dest = , default = None , type = argparse . FileType ( ) , required = True ) parser . add_argument ( , dest = , default = None , type = argparse . FileType ( ) , required = True \n 
CONSUMER_KEY = \n 
CONSUMER_SECRET = \n 
MY_TWITTER_CREDS = os . path . expanduser ( ) \n 
if not os . path . exists ( MY_TWITTER_CREDS ) : \n 
~~ oauth_token , oauth_secret = read_token_file ( MY_TWITTER_CREDS ) \n 
t = Twitter ( auth = OAuth ( oauth_token , oauth_secret , CONSUMER_KEY , CONSUMER_SECRET ) ) \n 
cache = { } \n 
if args . partial != None : \n 
~~~ for line in args . partial : \n 
~~~ fields = line . strip ( ) . split ( "\\t" ) \n 
text = fields [ - 1 ] \n 
sid = fields [ 0 ] \n 
cache [ sid ] = text \n 
~~ ~~ for line in args . dist : \n 
~~~ fields = line . strip ( ) . split ( ) \n 
uid = fields [ 1 ] \n 
while not sid in cache : \n 
~~~ text = t . statuses . show ( _id = sid ) [ ] . replace ( , ) . replace ( , ) \n 
~~ except TwitterError as e : \n 
~~~ if e . e . code == 429 : \n 
~~~ rate = t . application . rate_limit_status ( ) \n 
reset = rate [ ] [ ] [ ] [ ] \n 
now = datetime . datetime . today ( ) \n 
future = datetime . datetime . fromtimestamp ( reset ) \n 
seconds = ( future - now ) . seconds + 1 \n 
if seconds < 10000 : \n 
~~~ cache [ sid ] = \n 
~~ ~~ ~~ text = cache [ sid ] \n 
args . output . write ( "\\t" . join ( fields + [ text ] ) + ) \n 
~~ from collections import deque \n 
from twisted . internet import protocol \n 
from twisted . internet import defer , reactor \n 
from twisted . python import failure \n 
from twimp import amf0 \n 
from twimp import chunks \n 
from twimp . error import ProtocolContractError , UnexpectedStatusError \n 
from twimp . error import CommandResultError \n 
from twimp . error import CallResultError , CallAbortedException \n 
from twimp . proto import DispatchProtocol , DispatchFactory \n 
from twimp . utils import ms_time_wrapped \n 
LOG_CATEGORY = \n 
import twimp . log \n 
log = twimp . log . get_logger ( LOG_CATEGORY ) \n 
class CancellableCallQueue ( object ) : \n 
~~~ def __init__ ( self , reactor = reactor ) : \n 
~~~ self . reactor = reactor \n 
self . pending = { } \n 
self . _next_key = 0 \n 
~~ def callLater ( self , delay , f , * args , ** kw ) : \n 
~~~ call_key , self . _next_key = self . _next_key , self . _next_key + 1 \n 
clid = self . reactor . callLater ( delay , self . _call_wrapper , call_key , f , \n 
args , kw ) \n 
self . pending [ call_key ] = clid \n 
return call_key , clid \n 
~~ def cancel ( self , ( call_key , clid ) ) : \n 
~~~ self . pending . pop ( call_key , None ) \n 
return clid . cancel ( ) \n 
~~ def _call_wrapper ( self , call_key , f , args , kw ) : \n 
~~~ del self . pending [ call_key ] \n 
f ( * args , ** kw ) \n 
~~ def cancel_all ( self ) : \n 
~~~ remaining = self . pending . copy ( ) \n 
self . pending . clear ( ) \n 
for key , clid in remaining . iteritems ( ) : \n 
~~~ if clid . active ( ) : \n 
~~~ clid . cancel ( ) \n 
~~ ~~ ~~ ~~ class DeferredTracker ( object ) : \n 
~~~ init_trans_id = 1 \n 
~~~ self . _pending = { } \n 
self . _next_trans_id = { } \n 
~~ def next_trans ( self , key ) : \n 
~~~ trans_id = self . _next_trans_id . setdefault ( key , self . init_trans_id ) \n 
r , trans_id = trans_id , trans_id + 1 \n 
self . _next_trans_id [ key ] = trans_id \n 
~~ def push_deferred ( self , key , trans_id , d ) : \n 
~~~ if key not in self . _pending : \n 
~~~ self . _pending [ key ] = { } \n 
~~ key_queue = self . _pending [ key ] \n 
key_queue [ trans_id ] = d \n 
~~ def pop_deferred ( self , key , trans_id ) : \n 
~~~ d = None \n 
key_queue = self . _pending . get ( key , None ) \n 
if key_queue : \n 
~~~ d = key_queue . pop ( trans_id , None ) \n 
~~ return d \n 
~~ def iter_all ( self ) : \n 
~~~ return ( ( key , d ) \n 
for ( key , key_queue ) in self . _pending . iteritems ( ) \n 
for d in key_queue . itervalues ( ) ) \n 
~~ def reset ( self ) : \n 
~~~ self . _next_trans_id = { } \n 
~~ ~~ class StatusEventTracker ( object ) : \n 
~~~ self . _event_callbacks = { } \n 
~~ def add ( self , key , code , d ) : \n 
~~~ q = self . _event_callbacks . get ( key , None ) \n 
if q is None : \n 
~~~ q = self . _event_callbacks [ key ] = deque ( ) \n 
~~ q . append ( ( code , d ) ) \n 
~~ def pop ( self , key ) : \n 
~~~ code , d = None , None \n 
q = self . _event_callbacks . get ( key , None ) \n 
if q : \n 
~~~ code , d = q . popleft ( ) \n 
~~ return code , d \n 
~~ def pop_all ( self ) : \n 
~~~ ret = ( ( code , d ) \n 
for q in self . _event_callbacks . itervalues ( ) \n 
for ( code , d ) in q ) \n 
self . _event_callbacks = { } \n 
~~ def wait ( self , key , code ) : \n 
~~~ d = defer . Deferred ( ) \n 
self . add ( key , code , d ) \n 
return d \n 
~~ def cancel_all ( self , reason = None ) : \n 
~~~ waiting = self . pop_all ( ) \n 
if reason : \n 
~~~ for _code , d in waiting : \n 
~~~ d . errback ( reason ) \n 
~~ ~~ ~~ def dispatch ( self , key , info , miss_h = None ) : \n 
~~~ code , d = self . pop ( key ) \n 
if not d : \n 
~~~ if miss_h : \n 
~~~ miss_h ( ) \n 
~~~ evt_code = info . code \n 
~~~ d . errback ( ProtocolContractError ( e ) ) \n 
~~~ if code is None or evt_code == code : \n 
~~~ d . callback ( info ) \n 
~~~ d . errback ( UnexpectedStatusError ( info ) ) \n 
~~ ~~ ~~ ~~ ~~ class CommandDispatchProtocol ( DispatchProtocol ) : \n 
~~~ DispatchProtocol . __init__ ( self ) \n 
self . _cc_queue = CancellableCallQueue ( ) \n 
self . _call_tracker = DeferredTracker ( ) \n 
~~ def doCommand ( self , ts , ms_id , args ) : \n 
~~~ cmd = args [ 0 ] \n 
handler_m = getattr ( self , % ( cmd , ) , None ) \n 
if handler_m is None : \n 
~~~ self . _cc_queue . callLater ( 0 , self . unknownCommandType , cmd , ts , \n 
ms_id , args [ 1 : ] ) \n 
~~~ self . _cc_queue . callLater ( 0 , self . _handler_wrapper , handler_m , \n 
ts , ms_id , args [ 1 : ] ) \n 
~~ ~~ def _handler_wrapper ( self , handler , ts , ms_id , args ) : \n 
~~~ handler ( ts , ms_id , * args ) \n 
~~ def command__result ( self , ts , ms_id , trans_id , * args ) : \n 
~~~ d = self . _call_tracker . pop_deferred ( ms_id , trans_id ) \n 
if d : \n 
~~~ d . callback ( args ) \n 
~~~ self . unexpectedCallResult ( ts , ms_id , trans_id , args ) \n 
~~ ~~ def command__error ( self , ts , ms_id , trans_id , * args ) : \n 
~~~ d . errback ( failure . Failure ( CommandResultError ( * args ) ) ) \n 
~~~ self . unexpectedCallError ( ts , ms_id , trans_id , args ) \n 
~~ ~~ def unknownCommandType ( self , cmd , ts , msid , args ) : \n 
~~~ raise NotImplementedError ( % ( cmd , \n 
( ts , msid , args ) ) ) \n 
~~ def unexpectedCallResult ( self , ts , ms_id , trans_id , args ) : \n 
~~~ log . warning ( , \n 
ts , ms_id , trans_id , args ) \n 
~~ def unexpectedCallError ( self , ts , ms_id , trans_id , args ) : \n 
~~ def connectionLost ( self , reason = protocol . connectionDone ) : \n 
~~~ self . _cc_queue . cancel_all ( ) \n 
pending_calls = list ( self . _call_tracker . iter_all ( ) ) \n 
self . _call_tracker . clear ( ) \n 
for ms_id , d in pending_calls : \n 
~~ DispatchProtocol . connectionLost ( self , reason ) \n 
~~ def encode_amf ( self , * args ) : \n 
~~~ return amf0 . encode ( * args ) \n 
~~ def _send_command ( self , ts , ms_id , body , track_id ) : \n 
if track_id : \n 
self . _call_tracker . push_deferred ( ms_id , track_id , d ) \n 
~~ self . muxer . sendMessage ( 0 , chunks . MSG_COMMAND , ms_id , body ) \n 
~~ def _sendRemote ( self , ms_id , cmd , args , kwargs , track ) : \n 
~~~ trans_id = 0 \n 
if track : \n 
~~~ trans_id = self . _call_tracker . next_trans ( ms_id ) \n 
~~ encoded_args = self . encode_amf ( cmd , trans_id , * args ) \n 
return self . _send_command ( 0 , ms_id , encoded_args , trans_id ) \n 
~~ def callRemote ( self , ms_id , cmd , * args , ** kw ) : \n 
~~~ return self . _sendRemote ( ms_id , cmd , args , kw , True ) \n 
~~ def signalRemote ( self , ms_id , cmd , * args , ** kw ) : \n 
~~~ return self . _sendRemote ( ms_id , cmd , args , kw , False ) \n 
~~ ~~ class CommandDispatchFactory ( DispatchFactory ) : \n 
~~~ protocol = CommandDispatchProtocol \n 
~~ class EventDispatchProtocol ( CommandDispatchProtocol ) : \n 
~~~ CommandDispatchProtocol . __init__ ( self ) \n 
self . _events = StatusEventTracker ( ) \n 
~~ def _onStatus_ev_key ( self , ms_id ) : \n 
~~~ return ( None , ms_id ) \n 
~~ def waitStatus ( self , ms_id , code ) : \n 
~~~ return self . _events . wait ( self . _onStatus_ev_key ( ms_id ) , code ) \n 
~~ def command_onStatus ( self , ts , ms_id , _trans_id , _none , info ) : \n 
~~~ def miss_handler ( ) : \n 
~~~ self . unhandledOnStatus ( ts , ms_id , info ) \n 
~~ self . _events . dispatch ( self . _onStatus_ev_key ( ms_id ) , info , \n 
miss_h = miss_handler ) \n 
~~ def unhandledOnStatus ( self , ts , ms_id , info ) : \n 
ts , ms_id , info ) \n 
~~~ self . _events . cancel_all ( reason = reason ) \n 
CommandDispatchProtocol . connectionLost ( self , reason ) \n 
~~ ~~ class EventDispatchFactory ( CommandDispatchFactory ) : \n 
~~~ protocol = EventDispatchProtocol \n 
~~ class CallDispatchProtocol ( EventDispatchProtocol ) : \n 
~~~ EventDispatchProtocol . __init__ ( self ) \n 
~~ def session_time ( self ) : \n 
~~~ return time . time ( ) - self . session_init_time \n 
~~ def unknownCommandType ( self , cmd , ts , ms_id , args ) : \n 
~~~ trans_id = args [ 0 ] \n 
~~~ d = defer . maybeDeferred ( self . unknownRemoteCall , cmd , ts , ms_id , \n 
args [ 1 : ] ) \n 
~~~ d = defer . maybeDeferred ( handler_m , ts , ms_id , * args [ 1 : ] ) \n 
~~ if trans_id : \n 
~~~ d . addCallback ( self . _remote_handler_cb , ms_id , trans_id ) \n 
~~ d . addErrback ( self . _remote_abort_handler_eb ) \n 
d . addErrback ( self . _remote_handler_eb , ms_id , trans_id ) \n 
~~ def _remote_abort_handler_eb ( self , failure ) : \n 
~~~ failure . trap ( CallAbortedException ) \n 
log . debug ( , failure . value ) \n 
~~ def _remote_handler_cb ( self , result , ms_id , trans_id ) : \n 
~~~ if not isinstance ( result , ( tuple , list ) ) : \n 
~~~ result = ( result , ) \n 
~~ body = self . encode_amf ( , trans_id , * result ) \n 
ts = ms_time_wrapped ( self . session_time ( ) ) \n 
self . muxer . sendMessage ( ts , chunks . MSG_COMMAND , ms_id , body ) \n 
~~ def _remote_handler_eb ( self , failure , ms_id , trans_id ) : \n 
~~~ if log . isEnabledFor ( logging . DEBUG ) : \n 
~~~ log . info ( , failure . value , \n 
exc_info = ( failure . type , failure . value , \n 
failure . getTracebackObject ( ) ) ) \n 
~~~ log . info ( , failure . value ) \n 
~~ fatal = False \n 
if failure . check ( CallResultError ) : \n 
~~~ body = self . encode_amf ( , trans_id , \n 
* failure . value . get_error_args ( ) ) \n 
fatal = failure . value . is_fatal \n 
~~~ err = amf0 . Object ( code = , level = , \n 
description = repr ( failure . value ) ) \n 
body = self . encode_amf ( , trans_id , None , err ) \n 
~~ ts = ms_time_wrapped ( self . session_time ( ) ) \n 
if fatal : \n 
~~~ self . transport . loseConnection ( ) \n 
~~ ~~ def unknownRemoteCall ( self , cmd , ts , ms_id , args ) : \n 
~~~ log . warning ( , cmd , args ) \n 
raise CallAbortedException ( % ( cmd , ) ) \n 
~~ ~~ class CallDispatchFactory ( EventDispatchFactory ) : \n 
~~~ protocol = CallDispatchProtocol \n 
~~ from distutils . core import setup \n 
from txtstyle . version import VERSION \n 
setup ( \n 
version = VERSION , \n 
packages = [ ] , \n 
scripts = [ ] , \n 
url = , \n 
license = , \n 
description = , \n 
long_description = open ( ) . read ( ) \n 
from __future__ import absolute_import , unicode_literals , division \n 
from armet import http \n 
from flask . globals import current_app \n 
from werkzeug . wsgi import get_current_url \n 
class RequestHeaders ( http . request . Headers ) : \n 
~~~ def __init__ ( self , handle ) : \n 
~~~ self . _handle = handle \n 
super ( RequestHeaders , self ) . __init__ ( ) \n 
~~~ return self . _handle . headers [ name ] \n 
~~~ return ( key for key , _ in self . _handle . headers ) \n 
~~ def __len__ ( self ) : \n 
~~~ return len ( self . _handle . headers ) \n 
~~ def __contains__ ( self , name ) : \n 
~~~ return name in self . _handle . headers \n 
~~ ~~ class Request ( http . Request ) : \n 
~~~ request = flask . request \n 
async = kwargs [ ] \n 
self . _handle = request if not async else flask . Request ( request . environ ) \n 
self . headers = RequestHeaders ( self . _handle ) \n 
kwargs . update ( method = request . method ) \n 
super ( Request , self ) . __init__ ( * args , ** kwargs ) \n 
~~ def _read ( self ) : \n 
~~~ return self . _handle . stream . read ( ) \n 
def protocol ( self ) : \n 
~~~ return self . _handle . scheme . upper ( ) \n 
def mount_point ( self ) : \n 
~~~ path = self . _handle . path \n 
return path [ : path . rfind ( self . path ) ] if self . path else path \n 
def query ( self ) : \n 
~~~ if isinstance ( self . _handle . query_string , six . binary_type ) : \n 
~~~ return self . _handle . query_string . decode ( ) \n 
~~ return self . _handle . query_string \n 
def uri ( self ) : \n 
~~~ return get_current_url ( self . _handle . environ ) \n 
~~ ~~ class ResponseHeaders ( http . response . Headers ) : \n 
~~~ def __init__ ( self , response , handle ) : \n 
~~~ self . _response = response \n 
self . _handle = handle \n 
super ( ResponseHeaders , self ) . __init__ ( ) \n 
~~ def __setitem__ ( self , name , value ) : \n 
~~~ self . _response . require_open ( ) \n 
self . _handle . headers [ self . normalize ( name ) ] = value \n 
~~ def __delitem__ ( self , name ) : \n 
del self . _handle . headers [ name ] \n 
~~ ~~ class Response ( http . Response ) : \n 
~~~ self . _handle = current_app . response_class ( ) \n 
super ( Response , self ) . __init__ ( * args , ** kwargs ) \n 
if self . asynchronous : \n 
~~~ from gevent import queue \n 
self . _queue = queue . Queue ( ) \n 
~~ self . headers = ResponseHeaders ( self , self . _handle ) \n 
~~~ return self . _queue \n 
def status ( self ) : \n 
~~~ return self . _handle . status_code \n 
~~ @ status . setter \n 
def status ( self , value ) : \n 
~~~ self . require_open ( ) \n 
self . _handle . status_code = value \n 
~~ @ http . Response . body . setter \n 
def body ( self , value ) : \n 
~~~ if value : \n 
~~~ if self . asynchronous : \n 
~~~ super ( Response , Response ) . body . __set__ ( self , None ) \n 
self . _queue . put ( value ) \n 
~~ ~~ super ( Response , Response ) . body . __set__ ( self , value ) \n 
~~~ super ( Response , self ) . close ( ) \n 
~~~ self . _queue . put ( StopIteration ) \n 
~~ ~~ ~~ from __future__ import absolute_import , unicode_literals , division \n 
from . base import ModelResource as BaseModelResource \n 
from . meta import ModelResourceBase \n 
class ModelResource ( six . with_metaclass ( ModelResourceBase , BaseModelResource ) ) : \n 
~~ from __future__ import absolute_import , unicode_literals , division \n 
import wsgi_intercept \n 
from wsgi_intercept . httplib2_intercept import install \n 
from . utils import force_import_module \n 
def http_setup ( connectors , host , port , callback ) : \n 
~~~ install ( ) \n 
application = flask . Flask ( __name__ ) \n 
application . debug = True \n 
if callback : \n 
~~~ callback ( ) \n 
~~ module = force_import_module ( ) \n 
for name in module . __all__ : \n 
~~~ getattr ( module , name ) . mount ( , application ) \n 
~~ wsgi_intercept . add_wsgi_intercept ( host , port , lambda : application ) \n 
~~ def http_teardown ( host , port ) : \n 
~~~ wsgi_intercept . remove_wsgi_intercept ( host , port ) \n 
from . import ast \n 
from . import util \n 
from . tiler import ASTPattern , SimplePattern , tile \n 
EQUALITY = ( "=" , "is" ) \n 
INEQUALITY = ( "!=" , ) \n 
def canonicalize ( node ) : \n 
def replace_func ( pattern , n ) : \n 
~~~ l_literal = isinstance ( n . left , ast . Literal ) \n 
r_literal = isinstance ( n . right , ast . Literal ) \n 
if not l_literal and r_literal : \n 
~~~ n . reverse ( ) \n 
~~ elif l_literal and r_literal : \n 
~~~ if n . left . static and not n . right . static : \n 
~~ elif not n . left . static and not n . right . static and n . left . value > n . right . value : \n 
~~ elif n . left . static and n . right . static and n . left . value > n . right . value : \n 
~~ ~~ ~~ p = SimplePattern ( "types:CompareOperator" ) \n 
return tile ( node , [ p ] , replace_func ) \n 
~~ def select_rewrite_expression ( name , exprs ) : \n 
if name [ 1 ] == "equality" : \n 
~~~ values = [ e . right . value for e in exprs ] \n 
filter_using = util . mode ( values ) \n 
for e in exprs : \n 
~~~ if e . right . value == filter_using : \n 
~~~ return e \n 
~~ ~~ ~~ elif name [ 1 ] == "order" : \n 
~~~ is_static = name [ 3 ] [ 1 ] == "static" \n 
values = [ e . right . value for e in exprs ] \n 
if is_static : \n 
~~~ filter_using = util . median ( values ) \n 
~~~ filter_using = util . mode ( values ) \n 
~~ for e in exprs : \n 
~~~ return exprs [ 0 ] \n 
~~ ~~ def compare_rewrite ( node , name , expr , assumed_result ) : \n 
~~~ return equality_rewrite ( node , name , expr , assumed_result ) \n 
~~ elif name [ 1 ] == "order" : \n 
~~~ return order_rewrite ( node , name , expr , assumed_result ) \n 
~~ ~~ def equality_rewrite ( node , name , expr , assumed_result ) : \n 
~~~ static_value = expr . right . value \n 
is_static = expr . right . static \n 
if expr . type in EQUALITY : \n 
~~~ known = assumed_result \n 
~~~ known = not assumed_result \n 
~~ def replace_func ( pattern , node ) : \n 
~~~ static_match = node . right . value == static_value \n 
is_static_node = node . right . static \n 
const = None \n 
if known and is_static and is_static_node : \n 
~~~ if node . type in EQUALITY : \n 
~~~ const = static_match \n 
~~~ const = not static_match \n 
~~ ~~ elif static_match : \n 
~~~ const = known \n 
~~~ const = not known \n 
~~ ~~ return ast . Constant ( const ) if const is not None else None \n 
return tile ( node , [ pattern ] , replace_func ) \n 
~~ def order_rewrite ( node , name , expr , assumed_result ) : \n 
numeric = isinstance ( expr . right , ast . Number ) \n 
less_than = "<" in expr . type \n 
maybe_equals = "=" in expr . type \n 
if less_than : \n 
~~~ if assumed_result : \n 
~~~ min_bound = float ( "-inf" ) \n 
min_incl = False \n 
max_bound = static_value \n 
max_incl = maybe_equals \n 
~~~ min_bound = static_value \n 
min_incl = not maybe_equals \n 
max_bound = float ( "inf" ) \n 
max_incl = False \n 
min_incl = maybe_equals \n 
max_incl = not maybe_equals \n 
~~ ~~ if not assumed_result : \n 
~~~ less_than = not less_than \n 
maybe_equals = not maybe_equals \n 
~~~ node_val = node . right . value \n 
if not numeric and node_val != static_value : \n 
~~ assert_less = "<" in node . type \n 
assert_equals = "=" in node . type \n 
if not numeric : \n 
~~~ const = ( less_than == assert_less ) \n 
if maybe_equals : \n 
~~~ const = const and assert_equals \n 
~~~ const = None \n 
if assert_less : \n 
~~~ if node_val > max_bound : \n 
~~~ const = True \n 
~~ elif node_val == max_bound and max_incl and assert_equals : \n 
~~ elif node_val == max_bound and not max_incl : \n 
~~ elif node_val < min_bound : \n 
~~~ const = False \n 
~~ elif node_val == min_bound and not assert_equals : \n 
~~ elif node_val == min_bound and assert_equals and not min_incl : \n 
~~~ if node_val < min_bound : \n 
~~ elif node_val == min_bound and not min_incl : \n 
~~ elif node_val == min_bound and min_incl and assert_equals : \n 
~~ elif node_val > max_bound : \n 
~~ elif node_val == max_bound and not assert_equals : \n 
~~ elif node_val == max_bound and assert_equals and not max_incl : \n 
~~ ~~ ~~ if const is None : \n 
~~ return ast . Constant ( const ) \n 
ASTPattern ( expr . left ) , "types:Number" if numeric else "types:Literal" ) \n 
from . commands . init import init \n 
ENTRY_POINT = \n 
CONFIGURATION_MODULE = "settings" \n 
def in_armstrong_project ( some_path = None ) : \n 
~~~ if some_path is None : \n 
~~~ some_path = CWD \n 
~~ joined = os . path . join ( some_path , CONFIGURATION_MODULE ) \n 
return os . path . isdir ( joined ) and os . path . exists ( os . path . join ( joined , ) ) \n 
~~ def find_project_dir ( path = os . getcwd ( ) ) : \n 
path_split = os . path . split ( path ) \n 
while path_split [ 1 ] : \n 
~~~ if in_armstrong_project ( path ) : \n 
~~~ return path \n 
~~ path = path_split [ 0 ] \n 
~~ CWD = find_project_dir ( ) or os . getcwd ( ) \n 
def get_current_configuration ( ) : \n 
type = "production" if "--production" in sys . argv else "development" \n 
return "%s.%s" % ( CONFIGURATION_MODULE , type ) \n 
~~~ parser = argparse . ArgumentParser ( description = ) \n 
subparsers = parser . add_subparsers ( title = ) \n 
from pkg_resources import iter_entry_points \n 
loaded = { } \n 
for ep in iter_entry_points ( group = ENTRY_POINT ) : \n 
~~~ if ep . name in loaded : \n 
~~ loaded [ ep . name ] = True \n 
command = ep . load ( ) \n 
if ( not in_armstrong_project ( ) and \n 
getattr ( command , "requires_armstrong" , False ) ) : \n 
~~ armstrong_parser = subparsers . add_parser ( ep . name , \n 
description = command . __doc__ , \n 
help = command . __doc__ ) \n 
if hasattr ( command , ) : \n 
~~~ command . build_parser ( armstrong_parser ) \n 
~~ armstrong_parser . set_defaults ( func = command ) \n 
~~ if in_armstrong_project ( ) : \n 
~~~ if CWD not in sys . path : \n 
~~~ sys . path . insert ( 0 , CWD ) \n 
~~~ settings_module = get_current_configuration ( ) \n 
__import__ ( settings_module , globals ( ) , locals ( ) ) \n 
from django . core . management import setup_environ \n 
setup_environ ( sys . modules [ settings_module ] ) \n 
~~ except ImportError , e : \n 
( settings_module , e ) ) \n 
~~ from django . core . management import get_commands \n 
django_commands = get_commands ( ) . keys ( ) \n 
django_commands . sort ( ) \n 
for command in django_commands : \n 
~~~ dj_parser = subparsers . add_parser ( command , help = ) \n 
dj_parser . add_argument ( "--production" , action = , \n 
help = % CONFIGURATION_MODULE ) \n 
dj_parser . set_defaults ( func = call_django ) \n 
~~ ~~ args , argv = parser . parse_known_args ( ) \n 
kwargs = vars ( args ) \n 
func = kwargs . pop ( , None ) \n 
if argv : \n 
~~~ func ( argv = argv , ** kwargs ) \n 
~~~ func ( ** kwargs ) \n 
~~ ~~ def call_django ( argv = [ ] , production = False ) : \n 
~~ settings_module = "%s.development" % CONFIGURATION_MODULE \n 
if production : \n 
~~~ settings_module = "%s.production" % CONFIGURATION_MODULE \n 
~~ settings = None \n 
~~~ __import__ ( settings_module , globals ( ) , locals ( ) ) \n 
settings = sys . modules [ settings_module ] \n 
~~ new_argv = sys . argv [ 0 : 2 ] + argv \n 
from django . core . management import execute_manager \n 
execute_manager ( settings , argv = new_argv ) \n 
if os . path . exists ( "MANIFEST" ) : \n 
~~~ os . unlink ( "MANIFEST" ) \n 
~~ VERSION = ( "12" , "03" , "1" , ) \n 
version = "." . join ( VERSION ) , \n 
packages = [ "armstrong" , ] , \n 
namespace_packages = [ "armstrong" , ] , \n 
"armstrong.cli>=1.1.1,<1.2" , \n 
"armstrong.core.arm_access>=1.0.6,<1.1" , \n 
"armstrong.core.arm_content>=1.3.1,<1.1" , \n 
"armstrong.core.arm_layout>=1.1.1,<1.1" , \n 
"armstrong.core.arm_sections>=1.5.2,<1.6" , \n 
"armstrong.core.arm_wells>=1.6.0,<1.7" , \n 
"armstrong.apps.articles>=1.1.1,<1.2" , \n 
"armstrong.apps.content>=1.0.2,<1.1" , \n 
"armstrong.apps.images>=1.1.1,<1.2" , \n 
"armstrong.apps.related_content>=2.0.1,<2.1" , \n 
"armstrong.hatband>=1.2.4,<1.3" , \n 
def enum ( * sequential , ** named ) : \n 
enums = dict ( zip ( sequential , range ( len ( sequential ) ) ) , ** named ) \n 
reverse = dict ( ( value , key ) for key , value in enums . iteritems ( ) ) \n 
enums [ ] = reverse \n 
return type ( , ( ) , enums ) \n 
~~~ low = 1 \n 
high = 100 \n 
def valid_input ( ) : \n 
if reply in ( , , ) : \n 
~~~ return reply \n 
~~ ~~ while True : \n 
~~~ guess = ( low + high ) // 2 \n 
reply = valid_input ( ) \n 
if reply == : \n 
~~~ low = guess \n 
~~ elif reply == : \n 
~~~ high = guess \n 
~~ if low == high : \n 
~~~ text_type = str \n 
unichr = chr \n 
~~~ text_type = unicode \n 
unichr = unichr \n 
import docker as _docker \n 
import raven . contrib \n 
from celery import Celery , Task \n 
from werkzeug . local import LocalProxy \n 
from flask . ext . sqlalchemy import SQLAlchemy \n 
from flask . ext . migrate import Migrate \n 
from flask . ext . login import LoginManager \n 
from flask . ext . principal import Principal \n 
from flask . ext . assets import Environment , Bundle \n 
from flask . ext . wtf . csrf import CsrfProtect \n 
from flask . ext . mail import Mail \n 
from flask . ext . moment import Moment \n 
VERSION = \n 
def get_version ( ) : \n 
~~~ return VERSION \n 
~~ db = SQLAlchemy ( ) \n 
migrate = Migrate ( ) \n 
login_manager = LoginManager ( ) \n 
principal = Principal ( ) \n 
celery = Celery ( ) \n 
csrf = CsrfProtect ( ) \n 
mail = Mail ( ) \n 
moment = Moment ( ) \n 
docker = LocalProxy ( lambda : _docker . Client ( \n 
base_url = flask . current_app . config [ ] , \n 
version = flask . current_app . config [ ] ) ) \n 
def create_app ( config = None ) : \n 
config = config or os . environ . get ( , \n 
app . config . from_object ( config ) \n 
configure_logging ( app ) \n 
configure_extensions ( app ) \n 
configure_blueprints ( app ) \n 
register_jinja2_globals_and_filters ( app ) \n 
return app \n 
~~ def configure_logging ( app ) : \n 
~~~ app . logger . setLevel ( logging . INFO ) \n 
if not ( app . debug or app . testing ) : \n 
~~~ handler = logging . StreamHandler ( ) \n 
app . logger . addHandler ( handler ) \n 
~~ ~~ def configure_extensions ( app ) : \n 
~~~ db . init_app ( app ) \n 
migrate . init_app ( app , db ) \n 
login_manager . init_app ( app ) \n 
principal . init_app ( app ) \n 
init_celery_app ( app , celery ) \n 
csrf . init_app ( app ) \n 
mail . init_app ( app ) \n 
moment . init_app ( app ) \n 
assets = Environment ( app ) \n 
css = Bundle ( \n 
output = ) \n 
js = Bundle ( \n 
assets . register ( , css ) \n 
assets . register ( , js ) \n 
~~ def configure_blueprints ( app ) : \n 
~~~ from . import auth \n 
app . register_blueprint ( auth . bp ) \n 
from . import repos \n 
app . register_blueprint ( repos . bp , url_prefix = ) \n 
from . import accounts \n 
app . register_blueprint ( accounts . bp , url_prefix = ) \n 
from . import projects \n 
app . register_blueprint ( projects . bp , url_prefix = ) \n 
from . import builds \n 
app . register_blueprint ( builds . bp ) \n 
@ app . route ( ) \n 
def index ( ) : \n 
~~~ return flask . redirect ( flask . url_for ( ) ) \n 
~~ ~~ def register_jinja2_globals_and_filters ( app ) : \n 
~~~ import wtforms \n 
from kozmic . builds import get_ansi_to_html_converter \n 
app . jinja_env . globals [ ] = get_version \n 
app . jinja_env . globals [ ] = lambda field : isinstance ( field , wtforms . HiddenField ) \n 
ansi_converter = get_ansi_to_html_converter ( ) \n 
app . jinja_env . filters [ ] = lambda ansi : ansi_converter . convert ( ansi , full = False ) \n 
app . jinja_env . filters [ ] = lambda dt : moment . create ( dt ) . format ( ) \n 
app . jinja_env . globals [ ] = ansi_converter . produce_headers \n 
~~ def init_celery_app ( app , celery ) : \n 
~~~ celery . config_from_object ( app . config ) \n 
class ContextTask ( Task ) : \n 
~~~ abstract = True \n 
~~~ with create_app ( ) . app_context ( ) : \n 
~~~ return super ( ContextTask , self ) . __call__ ( * args , ** kwargs ) \n 
~~ ~~ ~~ celery . Task = ContextTask \n 
sentry_dsn = app . config [ ] \n 
if sentry_dsn : \n 
~~~ client = raven . Client ( sentry_dsn ) \n 
raven . contrib . celery . register_signal ( client ) \n 
import mock \n 
import factory . alchemy \n 
from Crypto . PublicKey import RSA \n 
from kozmic . models import ( db , User , DeployKey , Project , Membership , Hook , \n 
TrackedFile , HookCall , Build , Job , Organization ) \n 
class Factory ( factory . alchemy . SQLAlchemyModelFactory ) : \n 
~~~ FACTORY_SESSION = None \n 
ABSTRACT_FACTORY = True \n 
def _create ( cls , target_class , * args , ** kwargs ) : \n 
~~~ obj = super ( Factory , cls ) . _create ( target_class , * args , ** kwargs ) \n 
cls . FACTORY_SESSION . commit ( ) \n 
return obj \n 
~~ ~~ def setup ( session ) : \n 
~~~ Factory . FACTORY_SESSION = session \n 
~~ def reset ( ) : \n 
~~~ factories = ( \n 
UserFactory , \n 
DeployKeyFactory , \n 
ProjectFactory , \n 
MembershipFactory , \n 
UserRepositoryFactory , \n 
OrganizationFactory , \n 
OrganizationRepositoryFactory , \n 
BuildFactory , \n 
JobFactory , \n 
HookFactory , \n 
TrackedFileFactory , \n 
HookCallFactory , \n 
for factory in factories : \n 
~~~ factory . reset_sequence ( ) \n 
~~ ~~ _identity = lambda n : n \n 
class UserFactory ( Factory ) : \n 
~~~ FACTORY_FOR = User \n 
id = factory . Sequence ( _identity ) \n 
gh_id = factory . Sequence ( _identity ) \n 
gh_name = factory . Sequence ( . format ) \n 
gh_login = factory . Sequence ( . format ) \n 
gh_avatar_url = factory . Sequence ( . format ) \n 
gh_access_token = \n 
~~ class UserRepositoryFactory ( Factory ) : \n 
~~~ FACTORY_FOR = User . Repository \n 
gh_id = factory . Sequence ( lambda n : 1000 + n ) \n 
gh_name = \n 
gh_full_name = \n 
is_public = False \n 
@ factory . lazy_attribute \n 
def gh_ssh_clone_url ( self ) : \n 
~~~ return . format ( self . gh_full_name ) \n 
~~ @ factory . lazy_attribute \n 
def gh_https_clone_url ( self ) : \n 
~~ ~~ class OrganizationFactory ( Factory ) : \n 
~~~ FACTORY_FOR = Organization \n 
gh_login = \n 
~~ class OrganizationRepositoryFactory ( Factory ) : \n 
~~~ FACTORY_FOR = Organization . Repository \n 
id = factory . Sequence ( lambda n : n ) \n 
gh_id = factory . Sequence ( lambda n : 2000 + n ) \n 
~~ ~~ class MembershipFactory ( Factory ) : \n 
~~~ FACTORY_FOR = Membership \n 
~~ class DeployKeyFactory ( Factory ) : \n 
~~~ FACTORY_FOR = DeployKey \n 
~~ CACHED_RSA_KEY = RSA . generate ( 1024 ) \n 
class ProjectFactory ( Factory ) : \n 
~~~ FACTORY_FOR = Project \n 
gh_full_name = factory . Sequence ( . format ) \n 
gh_ssh_clone_url = factory . Sequence ( . format ) \n 
gh_https_clone_url = factory . Sequence ( . format ) \n 
@ factory . post_generation \n 
def generate_deploy_key ( project , create , extracted , ** kwargs ) : \n 
~~~ if not project . is_public : \n 
~~~ with mock . patch ( , \n 
return_value = CACHED_RSA_KEY ) : \n 
~~~ project . deploy_key = DeployKey ( passphrase = project . passphrase ) \n 
project . deploy_key . gh_id = 123 \n 
~~ db . session . commit ( ) \n 
~~ ~~ ~~ class BuildFactory ( Factory ) : \n 
~~~ FACTORY_FOR = Build \n 
gh_commit_author = \n 
gh_commit_message = \n 
gh_commit_ref = \n 
status = \n 
def number ( self ) : \n 
~~~ return len ( self . project . builds . all ( ) ) + 1 \n 
def created_at ( self ) : \n 
~~~ days = self . number \n 
return ( datetime . datetime ( 2013 , 11 , 8 , 20 , 10 , 25 ) + \n 
datetime . timedelta ( days = days ) ) \n 
def gh_commit_sha ( self ) : \n 
~~~ digest = hashlib . sha1 ( ) \n 
digest . update ( str ( self . id ) ) \n 
return digest . hexdigest ( ) \n 
~~ ~~ class JobFactory ( Factory ) : \n 
~~~ FACTORY_FOR = Job \n 
~~ class TrackedFileFactory ( Factory ) : \n 
~~~ FACTORY_FOR = TrackedFile \n 
path = factory . Sequence ( . format ) \n 
~~ class HookFactory ( Factory ) : \n 
~~~ FACTORY_FOR = Hook \n 
title = factory . Sequence ( . format ) \n 
build_script = \n 
docker_image = \n 
~~ class HookCallFactory ( Factory ) : \n 
~~~ FACTORY_FOR = HookCall \n 
def gh_payload ( self ) : \n 
detailed_out = open ( filename , ) \n 
file = \n 
summary_out = open ( file , ) \n 
detailed_out . write ( "\\t" . join ( [ "Chromosome" , "Gene_name" , "Is_hgnc" , "Ensembl_gene_id" , "Ensembl_transcript_id" "Transcript_status" , "CCDS_id" , "HGNC_id" , "CDS_length" , "Protein_length" , "Transcript_start" , "Transcript_end" , "strand" , "Synonyms" , \n 
"Rvis_pct" , "entrez_gene_id" , "mammalian_phenotype_id" ] ) ) \n 
detailed_out . write ( "\\n" ) \n 
summary_out . write ( "\\t" . join ( [ "Chromosome" , "Gene_name" , "Is_hgnc" , "Ensembl_gene_id" , \n 
"HGNC_id" , "Synonyms" , "Rvis_pct" , "Strand" , "Transcript_min_start" , "Transcript_max_end" summary_out . write ( "\\n" ) \n 
mouse_phenotype = defaultdict ( list ) \n 
genic_intolerance = defaultdict ( list ) \n 
keygene = list_hgnc = [ ] \n 
transcript_min = defaultdict ( list ) \n 
transcript_max = defaultdict ( list ) \n 
lines_seen = set ( ) \n 
for line in open ( "genic_intolerance_dataset2" , ) : \n 
~~~ if line . startswith ( "#" ) is False : \n 
~~~ field = line . strip ( ) . split ( "\\t" ) \n 
name = str ( field [ 0 ] ) \n 
score = str ( field [ 1 ] ) \n 
percentile = str ( field [ 2 ] ) \n 
( key , value ) = ( name , percentile ) \n 
genic_intolerance [ name ] . append ( percentile ) \n 
~~ ~~ for row in open ( "HMD_HumanPhenotype" , ) : \n 
~~~ col = row . strip ( ) . split ( "\\t" ) \n 
entrez_id = str ( col [ 1 ] ) . lstrip ( ) \n 
mph = str ( col [ 5 ] ) . lstrip ( ) . replace ( , ) if str ( col [ 5 ] ) != else None \n 
( key , value ) = ( entrez_id , mph ) \n 
mouse_phenotype [ entrez_id ] . append ( mph ) \n 
~~ for each in open ( "raw_gene_table" , ) : \n 
~~~ if each . startswith ( "Chromosome" ) is False : \n 
~~~ k = each . strip ( ) . split ( "\\t" ) \n 
chr = "chr" + str ( ( k [ 0 ] ) ) \n 
ens = str ( k [ 2 ] ) \n 
start = str ( k [ 10 ] ) \n 
end = str ( k [ 11 ] ) \n 
transcript_min [ ( chr , ens ) ] . append ( start ) \n 
transcript_max [ ( chr , ens ) ] . append ( end ) \n 
~~ ~~ for each in open ( "raw_gene_table" , ) : \n 
chrom = "chr" + str ( ( k [ 0 ] ) ) \n 
hgnc = str ( k [ 1 ] ) \n 
ens_geneid = str ( k [ 2 ] ) \n 
ens_transid = str ( k [ 3 ] ) \n 
trans_biotype = str ( k [ 4 ] ) \n 
status = str ( k [ 5 ] ) \n 
ccds_id = str ( k [ 6 ] ) \n 
hgnc_id = str ( k [ 7 ] ) \n 
cds_len = str ( k [ 8 ] ) \n 
protein_len = str ( k [ 9 ] ) \n 
transcript_start = str ( k [ 10 ] ) \n 
transcript_end = str ( k [ 11 ] ) \n 
strand = str ( k [ 12 ] ) \n 
entrez = str ( k [ 15 ] ) \n 
if ( chrom , ens_geneid ) in transcript_min : \n 
~~~ minmum = sorted ( transcript_min [ ( chrom , ens_geneid ) ] ) [ 0 ] \n 
~~ if ( chrom , ens_geneid ) in transcript_max : \n 
~~~ maxmum = sorted ( transcript_max [ ( chrom , ens_geneid ) ] ) [ - 1 ] \n 
~~ rvis = genic_intolerance [ hgnc ] [ 0 ] if hgnc in genic_intolerance else None \n 
pheno = mouse_phenotype [ entrez ] if entrez in mouse_phenotype else None \n 
if pheno is not None and len ( pheno ) == 1 : \n 
~~~ phenotype = pheno [ 0 ] \n 
~~ elif pheno is None : \n 
~~~ phenotype = "None" \n 
~~~ if len ( pheno ) > 1 : \n 
~~~ string = "," . join ( pheno ) \n 
if "None" in string and "MP:" not in string : \n 
~~~ phenotype = None \n 
~~ if "None" not in string and "MP:" in string : \n 
~~~ phenotype = "," . join ( set ( string . split ( "," ) ) ) \n 
~~ if "None" in string and "MP:" in string : \n 
~~~ phen = string . split ( "," ) \n 
phenotype = "," . join ( [ x for x in phen if x != "None" ] ) \n 
~~ ~~ ~~ if hgnc != "None" : \n 
~~~ list_hgnc . append ( hgnc ) \n 
~~ if "None" in previous and "None" in synonyms and "None" in hgnc : \n 
~~~ string = None \n 
~~~ gene_string = hgnc + "," + previous + "," + synonyms \n 
if gene_string . startswith ( "None" ) : \n 
~~~ string = gene_string . replace ( "None," , "" ) \n 
~~~ string = gene_string . replace ( ",None" , "" ) \n 
~~ ~~ if string is not None : \n 
~~~ genes = set ( string . split ( "," ) ) \n 
if len ( genes ) > 1 : \n 
~~~ keygene = set ( [ each ] ) \n 
synonym = genes . difference ( keygene ) \n 
gene_name = . join ( keygene ) \n 
other_names = . join ( synonym ) \n 
hgnc_flag = "1" if gene_name in list_hgnc else "0" \n 
is_hgnc_id = hgnc_id if gene_name in list_hgnc else "None" \n 
line = "\\t" . join ( [ chrom , gene_name , hgnc_flag , ens_geneid , is_hgnc_id , \n 
other_names , str ( rvis ) , strand , minmum , maxmum , str ( phenotype ) ] ) if line not in lines_seen : \n 
~~~ summary_out . write ( line ) \n 
summary_out . write ( "\\n" ) \n 
lines_seen . add ( line ) \n 
~~ detailed_out . write ( "\\t" . join ( [ chrom , gene_name , hgnc_flag , ens_geneid , ens_transid , trans_biotype status , ccds_id , is_hgnc_id , cds_len , protein_len , transcript_start transcript_end , strand , other_names , str ( rvis ) , entrez detailed_out . write ( "\\n" ) \n 
~~ ~~ elif len ( genes ) == 1 : \n 
~~~ gene_name = . join ( genes ) \n 
other_names = "None" \n 
line = "\\t" . join ( [ chrom , str ( gene_name ) , hgnc_flag , ens_geneid , is_hgnc_id , \n 
other_names , str ( rvis ) , strand , minmum , maxmum , str ( phenotype ) ] ) \n 
if line not in lines_seen : \n 
~~ detailed_out . write ( "\\t" . join ( [ chrom , str ( gene_name ) , hgnc_flag , ens_geneid , ens_transid , status , ccds_id , is_hgnc_id , cds_len , protein_len , transcript_start transcript_end , strand , other_names , str ( rvis ) , entrez , str detailed_out . write ( "\\n" ) \n 
~~ ~~ elif string is None : \n 
~~~ gene_name = "None" \n 
hgnc_flag = "0" \n 
is_hgnc_id = "None" \n 
~~ ~~ ~~ detailed_out . close ( ) \n 
summary_out . close ( ) \n 
import GeminiQuery \n 
from GeminiQuery import select_formatter \n 
def _report_results ( args , query , gq ) : \n 
~~~ gq . run ( query , show_variant_samples = args . show_variant_samples ) \n 
if args . use_header and gq . header : \n 
~~~ print gq . header \n 
~~ for row in gq : \n 
~~~ print row \n 
~~ ~~ def get_region ( args , gq ) : \n 
~~~ region_regex = re . compile ( "(\\S+):(\\d+)-(\\d+)" ) \n 
~~~ region = region_regex . findall ( args . region ) [ 0 ] \n 
~~ if len ( region ) != 3 : \n 
~~ chrom = region [ 0 ] \n 
start = region [ 1 ] \n 
end = region [ 2 ] \n 
if args . columns is not None : \n 
if args . filter : \n 
_report_results ( args , query , gq ) \n 
~~ def get_gene ( args , gq ) : \n 
~~ def add_region_to_query ( args ) : \n 
args . query = _add_to_where_clause ( args . query , where_clause ) \n 
~~ def _add_to_where_clause ( query , where_clause ) : \n 
~~~ where_index = query . lower ( ) . find ( "where" ) \n 
prefix = query [ 0 : where_index ] \n 
suffix = query [ where_index + len ( "where" ) : ] \n 
if where_index == - 1 : \n 
~~ return query \n 
~~ def region ( parser , args ) : \n 
~~~ if os . path . exists ( args . db ) : \n 
~~~ formatter = select_formatter ( args ) \n 
gq = GeminiQuery . GeminiQuery ( args . db , out_format = formatter ) \n 
if args . region is not None and args . gene is not None : \n 
~~~ sys . exit ( ) \n 
~~ elif args . region is not None : \n 
~~~ get_region ( args , gq ) \n 
~~ elif args . gene is not None : \n 
~~~ get_gene ( args , gq ) \n 
import sqlalchemy as sql \n 
import compression as Z \n 
from gemini . config import read_gemini_config \n 
import gemini_utils as util \n 
from gemini_constants import * \n 
def get_pathways ( args ) : \n 
~~~ version_dic = defaultdict ( ) \n 
version_dic = { : , : , \n 
: , : } \n 
config = read_gemini_config ( args = args ) \n 
path_dirname = config [ "annotation_dir" ] \n 
if args . version in version_dic : \n 
~~~ path_file = os . path . join ( path_dirname , version_dic [ args . version ] ) \n 
~~ agn_paths = defaultdict ( list ) \n 
hgnc_paths = defaultdict ( list ) \n 
ensembl_paths = defaultdict ( list ) \n 
for line in open ( path_file , ) : \n 
uniprot = fields [ 0 ] \n 
agn = fields [ 1 ] \n 
hgnc = fields [ 2 ] \n 
ensid = fields [ 3 ] \n 
ens_transcript = fields [ 4 ] \n 
hsa = fields [ 5 ] \n 
path = fields [ 6 ] if fields [ 6 ] != else None \n 
if path is not None and path . startswith ( "path:" ) : \n 
~~~ path = path [ 5 : ] \n 
path = path . replace ( ";" , ":" ) \n 
~~ agn_paths [ ( agn , ens_transcript ) ] . append ( path ) \n 
hgnc_paths [ ( hgnc , ens_transcript ) ] . append ( path ) \n 
ensembl_paths [ ( ensid , ens_transcript ) ] . append ( path ) \n 
~~ return agn_paths , hgnc_paths , ensembl_paths \n 
~~ def _get_pathways ( gene , transcript , pathways , allow_none = True ) : \n 
~~~ pathways = set ( pathways ) \n 
if None in pathways : \n 
~~~ if len ( pathways ) > 1 or allow_none is False : \n 
~~~ pathways . remove ( None ) \n 
~~ ~~ return pathways \n 
~~ def _report_variant_pathways ( res , args , idx_to_sample ) : \n 
~~~ ( agn_paths , hgnc_paths , ensembl_paths ) = get_pathways ( args ) \n 
unpack = Z . unpack_genotype_blob \n 
for r in res : \n 
~~~ gt_types = unpack ( r [ ] ) \n 
gts = unpack ( r [ ] ) \n 
~~~ unpack = Z . snappy_unpack_blob \n 
gt_types = unpack ( r [ ] ) \n 
~~ gene = str ( r [ ] ) \n 
trans = str ( r [ ] ) \n 
pathways = [ ] \n 
if ( gene , trans ) in agn_paths : \n 
~~~ pathways = _get_pathways ( gene , trans , agn_paths [ ( gene , trans ) ] , \n 
allow_none = False ) \n 
~~ elif ( gene , trans ) in hgnc_paths : \n 
~~~ pathways = _get_pathways ( gene , trans , hgnc_paths [ ( gene , trans ) ] , \n 
~~ elif ( gene , trans ) in ensembl_paths : \n 
~~~ pathways = _get_pathways ( gene , trans , ensembl_paths [ ( gene , trans ) ] , \n 
~~ pathlist = "," . join ( pathways ) \n 
for idx , gt_type in enumerate ( gt_types ) : \n 
~~~ if ( gt_type == HET or gt_type == HOM_ALT ) and len ( pathways ) > 0 : \n 
~~~ print "\\t" . join ( [ r [ ] , str ( r [ ] ) , str ( r [ ] ) , r [ ] , r [ ] , r [ ] , idx_to_sample [ idx ] , gts [ idx ] , gene , trans , pathlist ] ) \n 
~~ ~~ ~~ ~~ def get_ind_pathways ( conn , metadata , args ) : \n 
~~~ idx_to_sample = util . map_indices_to_samples ( metadata ) \n 
res = conn . execute ( sql . text ( query ) ) \n 
print . join ( [ , , , , , , , , , , ] ) \n 
_report_variant_pathways ( res , args , idx_to_sample ) \n 
~~ def get_ind_lof_pathways ( conn , metadata , args ) : \n 
~~ def pathways ( parser , args ) : \n 
~~~ import database \n 
conn , metadata = database . get_session_metadata ( args . db ) \n 
if ( not args . lof ) : \n 
~~~ get_ind_pathways ( conn , metadata , args ) \n 
~~~ get_ind_lof_pathways ( conn , metadata , args ) \n 
####################################################### \n 
######################################################## \n 
from . library import * \n 
from . array import * \n 
from . features import * \n 
def fast ( image , threshold = 20.0 , arc_length = 9 , non_max = True , feature_ratio = 0.05 , edge = 3 ) : \n 
out = Features ( ) \n 
safe_call ( backend . get ( ) . af_fast ( ct . pointer ( out . feat ) , \n 
image . arr , ct . c_float ( threshold ) , ct . c_uint ( arc_length ) , non_max ct . c_float ( feature_ratio ) , ct . c_uint ( edge ) ) ) \n 
return out \n 
~~ def harris ( image , max_corners = 500 , min_response = 1E5 , sigma = 1.0 , block_size = 0 , k_thr = 0.04 ) : \n 
safe_call ( backend . get ( ) . af_harris ( ct . pointer ( out . feat ) , \n 
image . arr , ct . c_uint ( max_corners ) , ct . c_float ( min_response ) , \n 
ct . c_float ( sigma ) , ct . c_uint ( block_size ) , ct . c_float ( k_thr ) ) ) \n 
~~ def orb ( image , threshold = 20.0 , max_features = 400 , scale = 1.5 , num_levels = 4 , blur_image = False ) : \n 
feat = Features ( ) \n 
desc = Array ( ) \n 
safe_call ( backend . get ( ) . af_orb ( ct . pointer ( feat . feat ) , ct . pointer ( desc . arr ) , \n 
ct . c_float ( threshold ) , ct . c_uint ( max_features ) , \n 
ct . c_float ( scale ) , ct . c_uint ( num_levels ) , blur_image ) ) \n 
return feat , desc \n 
~~ def hamming_matcher ( query , database , dim = 0 , num_nearest = 1 ) : \n 
index = Array ( ) \n 
dist = Array ( ) \n 
safe_call ( backend . get ( ) . af_hamming_matcher ( ct . pointer ( idx . arr ) , ct . pointer ( dist . arr ) , \n 
query . arr , database . arr , \n 
c_dim_t ( dim ) , c_dim_t ( num_nearest ) ) ) \n 
return index , dist \n 
~~ def nearest_neighbour ( query , database , dim = 0 , num_nearest = 1 , match_type = MATCH . SSD ) : \n 
safe_call ( backend . get ( ) . af_nearest_neighbour ( ct . pointer ( idx . arr ) , ct . pointer ( dist . arr ) , \n 
c_dim_t ( dim ) , c_dim_t ( num_nearest ) , \n 
match_type . value ) ) \n 
~~ def match_template ( image , template , match_type = MATCH . SAD ) : \n 
out = Array ( ) \n 
safe_call ( backend . get ( ) . af_match_template ( ct . pointer ( out . arr ) , \n 
image . arr , template . arr , \n 
~~ def susan ( image , radius = 3 , diff_thr = 32 , geom_thr = 10 , feature_ratio = 0.05 , edge = 3 ) : \n 
safe_call ( backend . get ( ) . af_susan ( ct . pointer ( out . feat ) , \n 
image . arr , ct . c_uint ( radius ) , ct . c_float ( diff_thr ) , \n 
ct . c_float ( geom_thr ) , ct . c_float ( feature_ratio ) , \n 
ct . c_uint ( edge ) ) ) \n 
~~ def dog ( image , radius1 , radius2 ) : \n 
safe_call ( backend . get ( ) . af_dog ( ct . pointer ( out . arr ) , \n 
image . arr , radius1 , radius2 ) ) \n 
~~ def sift ( image , num_layers = 3 , contrast_threshold = 0.04 , edge_threshold = 10.0 , initial_sigma = 1.6 , \n 
double_input = True , intensity_scale = 0.00390625 , feature_ratio = 0.05 ) : \n 
safe_call ( af_sift ( ct . pointer ( feat ) , ct . pointer ( desc ) , \n 
image . arr , num_layers , contrast_threshold , edge_threshold , \n 
initial_sigma , double_input , intensity_scale , feature_ratio ) ) \n 
return ( feat , desc ) \n 
~~ def gloh ( image , num_layers = 3 , contrast_threshold = 0.04 , edge_threshold = 10.0 , initial_sigma = 1.6 , \n 
safe_call ( af_gloh ( ct . pointer ( feat ) , ct . pointer ( desc ) , \n 
~~ def homography ( x_src , y_src , x_dst , y_dst , htype = HOMOGRAPHY . RANSAC , \n 
ransac_threshold = 3.0 , iters = 1000 , out_type = Dtype . f32 ) : \n 
H = Array ( ) \n 
inliers = ct . c_int ( 0 ) \n 
safe_call ( backend . get ( ) . af_homography ( ct . pointer ( H ) , ct . pointer ( inliers ) , \n 
x_src . arr , y_src . arr , x_dst . arr , y_dst . arr , \n 
htype . value , ransac_threshold , iters , out_type . value ) ) \n 
return ( H , inliers ) \n 
~~ import arrayfire as af \n 
from arrayfire import ParallelRange \n 
import array as host \n 
from . import _util \n 
def simple_index ( verbose = False ) : \n 
~~~ display_func = _util . display_func ( verbose ) \n 
print_func = _util . print_func ( verbose ) \n 
a = af . randu ( 5 , 5 ) \n 
display_func ( a ) \n 
b = af . Array ( a ) \n 
display_func ( b ) \n 
c = a . copy ( ) \n 
display_func ( c ) \n 
display_func ( a [ 0 , 0 ] ) \n 
display_func ( a [ 0 ] ) \n 
display_func ( a [ : ] ) \n 
display_func ( a [ : , : ] ) \n 
display_func ( a [ 0 : 3 , ] ) \n 
display_func ( a [ - 2 : - 1 , - 1 ] ) \n 
display_func ( a [ 0 : 5 ] ) \n 
display_func ( a [ 0 : 5 : 2 ] ) \n 
idx = af . Array ( host . array ( , [ 0 , 3 , 2 ] ) ) \n 
display_func ( idx ) \n 
aa = a [ idx ] \n 
display_func ( aa ) \n 
a [ 0 ] = 1 \n 
a [ 0 ] = af . randu ( 1 , 5 ) \n 
a [ : ] = af . randu ( 5 , 5 ) \n 
a [ : , - 1 ] = af . randu ( 5 , 1 ) \n 
a [ 0 : 5 : 2 ] = af . randu ( 3 , 5 ) \n 
a [ idx , idx ] = af . randu ( 3 , 3 ) \n 
a = af . randu ( 5 , 1 ) \n 
b = af . randu ( 5 , 1 ) \n 
for ii in ParallelRange ( 1 , 3 ) : \n 
~~~ a [ ii ] = b [ ii ] \n 
~~ display_func ( a ) \n 
for ii in ParallelRange ( 2 , 5 ) : \n 
~~~ b [ ii ] = 2 \n 
~~ display_func ( b ) \n 
a = af . randu ( 3 , 2 ) \n 
rows = af . constant ( 0 , 1 , dtype = af . Dtype . s32 ) \n 
b = a [ : , rows ] \n 
for r in rows : \n 
~~~ display_func ( r ) \n 
display_func ( b [ : , r ] ) \n 
~~ a = af . randu ( 3 ) \n 
c = af . randu ( 3 ) \n 
b = af . constant ( 1 , 3 , dtype = af . Dtype . b8 ) \n 
a [ b ] = c \n 
~~ _util . tests [ ] = simple_index \n 
from django . db import migrations , models \n 
field = models . EmailField ( default = , max_length = 254 , verbose_name = ) ) , \n 
~~ from hijack . helpers import is_authorized_default \n 
def can_hijack_default ( hijacker , hijacked ) : \n 
~~~ return is_authorized_default ( hijacker , hijacked ) \n 
~~ def everybody_can_hijack ( hijacker , hijacked ) : \n 
~~ def nobody_can_hijack ( hijacker , hijacked ) : \n 
~~~ from threadlocals . threadlocals import get_current_request \n 
~~~ get_current_request = None \n 
~~ from openinghours . models import OpeningHours , ClosingRules , PREMISES_MODEL \n 
from django . core . exceptions import ImproperlyConfigured \n 
from compat import get_model \n 
def get_premises_model ( ) : \n 
~~~ app_label , model_name = PREMISES_MODEL . split ( ) \n 
~~ premises_model = get_model ( app_label = app_label , model_name = model_name ) \n 
if premises_model is None : \n 
% PREMISES_MODEL ) \n 
~~ return premises_model \n 
~~ Company = get_premises_model ( ) \n 
def get_now ( ) : \n 
if not get_current_request : \n 
~~~ return datetime . datetime . now ( ) \n 
~~ request = get_current_request ( ) \n 
if request : \n 
~~~ openinghours_now = request . GET . get ( ) \n 
if openinghours_now : \n 
~~~ return datetime . datetime . strptime ( openinghours_now , ) \n 
~~ ~~ return datetime . datetime . now ( ) \n 
~~ def get_closing_rule_for_now ( location ) : \n 
now = get_now ( ) \n 
if location : \n 
~~~ return ClosingRules . objects . filter ( company = location , \n 
start__lte = now , end__gte = now ) \n 
~~ return Company . objects . first ( ) . closingrules_set . filter ( start__lte = now , \n 
end__gte = now ) \n 
~~ def has_closing_rule_for_now ( location ) : \n 
cr = get_closing_rule_for_now ( location ) \n 
return cr . count ( ) \n 
~~ def is_open ( location , now = None ) : \n 
if now is None : \n 
~~~ now = get_now ( ) \n 
~~ if has_closing_rule_for_now ( location ) : \n 
~~ now_time = datetime . time ( now . hour , now . minute , now . second ) \n 
~~~ ohs = OpeningHours . objects . filter ( company = location ) \n 
~~~ ohs = Company . objects . first ( ) . openinghours_set . all ( ) \n 
~~ for oh in ohs : \n 
~~~ is_open = False \n 
if ( oh . weekday == now . isoweekday ( ) and \n 
oh . from_hour <= now_time and \n 
now_time <= oh . to_hour ) : \n 
~~~ is_open = oh \n 
~~ if ( oh . weekday == now . isoweekday ( ) and \n 
( ( oh . to_hour < oh . from_hour ) and \n 
( now_time < datetime . time ( 23 , 59 , 59 ) ) ) ) : \n 
~~ if ( oh . weekday == ( now . isoweekday ( ) - 1 ) % 7 and \n 
oh . from_hour >= now_time and \n 
oh . to_hour >= now_time and \n 
oh . to_hour < oh . from_hour ) : \n 
~~ if is_open is not False : \n 
~~~ return oh \n 
~~ def next_time_open ( location ) : \n 
if not is_open ( location ) : \n 
now_time = datetime . time ( now . hour , now . minute , now . second ) \n 
found_opening_hours = False \n 
for i in range ( 8 ) : \n 
~~~ l_weekday = ( now . isoweekday ( ) + i ) % 8 \n 
ohs = OpeningHours . objects . filter ( company = location , \n 
weekday = l_weekday \n 
) . order_by ( , \n 
if ohs . count ( ) : \n 
~~~ for oh in ohs : \n 
~~~ future_now = now + datetime . timedelta ( days = i ) \n 
tmp_now = datetime . datetime ( future_now . year , \n 
future_now . month , \n 
future_now . day , \n 
oh . from_hour . hour , \n 
oh . from_hour . minute , \n 
oh . from_hour . second ) \n 
if tmp_now < now : \n 
~~ if is_open ( location , now = tmp_now ) : \n 
~~~ found_opening_hours = oh \n 
~~ ~~ if found_opening_hours is not False : \n 
~~~ return found_opening_hours , tmp_now \n 
~~ ~~ ~~ ~~ return False , None \n 
from optparse import make_option \n 
from django . core . management . base import NoArgsCommand , CommandError \n 
from disqus . api import DisqusClient \n 
class Command ( NoArgsCommand ) : \n 
~~~ option_list = NoArgsCommand . option_list + ( \n 
make_option ( , default = None , dest = , type = , \n 
help = ) , \n 
make_option ( , default = , dest = , type = , \n 
requires_model_validation = False \n 
def handle ( self , ** options ) : \n 
~~~ from django . conf import settings \n 
client = DisqusClient ( ) \n 
indent = options . get ( ) \n 
filter_ = options . get ( ) \n 
exclude = options . get ( ) \n 
forum_list = client . get_forum_list ( user_api_key = settings . DISQUS_API_KEY ) \n 
~~~ forum = [ f for f in forum_list if f [ ] == settings . DISQUS_WEBSITE_SHORTNAME ] [ 0 ] \n 
~~ posts = [ ] \n 
has_new_posts = True \n 
step = 100 \n 
while has_new_posts : \n 
~~~ new_posts = client . get_forum_posts ( \n 
user_api_key = settings . DISQUS_API_KEY , \n 
forum_id = forum [ ] , \n 
start = start , \n 
limit = start + step , \n 
filter = filter_ , \n 
exclude = exclude ) \n 
if not new_posts : \n 
~~~ has_new_posts = False \n 
~~~ start += step \n 
posts . append ( new_posts ) \n 
~~ ~~ print ( json . dumps ( posts , indent = indent ) ) \n 
~~ ~~ from . filter import Filter \n 
from . . statistics import mean , percentile , stddev \n 
from . . util import Periodic , get_path \n 
from . . event import Event \n 
from pprint import pformat \n 
class Stats ( Filter ) : \n 
def __init__ ( self , period = 5 , metrics = None , zero = True ) : \n 
~~~ super ( Stats , self ) . __init__ ( ) \n 
self . metrics = metrics or { } \n 
self . zero = zero \n 
self . timers = { } \n 
self . last = time . time ( ) \n 
self . periodic = Periodic ( period , self . flush ) \n 
~~ def process ( self , event ) : \n 
~~~ for output , path in self . metrics . iteritems ( ) : \n 
~~~ values = get_path ( event , path ) \n 
for key , value in values : \n 
~~~ if isinstance ( value , ( int , float ) ) : \n 
~~~ self . _process_value ( event , output , key , value ) \n 
~~ ~~ ~~ ~~ def _process_value ( self , event , output , v , value ) : \n 
~~~ k = event . format ( output , [ v ] , raise_missing = True ) \n 
~~~ self . timers [ k ] . add ( value ) \n 
~~~ self . timers [ k ] = Timer ( ) \n 
self . timers [ k ] . add ( value ) \n 
~~ ~~ def start ( self ) : \n 
~~~ super ( Stats , self ) . start ( ) \n 
self . periodic . start ( ) \n 
~~~ self . periodic . kill ( ) \n 
self . periodic . join ( ) \n 
super ( Stats , self ) . stop ( ) \n 
~~ def flush ( self ) : \n 
now = time . time ( ) \n 
period = now - self . last \n 
for k , timer in self . timers . iteritems ( ) : \n 
~~~ stats = timer . stats ( period , self . zero ) \n 
if stats : \n 
if self . output : \n 
~~~ self . output . put ( Event ( tags = [ ] , metric = k , stats = stats ) ) \n 
~~ if self . logger . isEnabledFor ( logging . DEBUG ) : \n 
~~~ self . logger . debug ( % ( k , pformat ( stats ) ) ) \n 
~~ ~~ timer . reset ( ) \n 
~~ if count : \n 
~~~ self . logger . debug ( % count ) \n 
~~ self . last = now \n 
~~ ~~ class Timer ( object ) : \n 
~~~ self . values = [ ] \n 
~~ def add ( self , v ) : \n 
~~~ self . values . append ( v ) \n 
~~ def stats ( self , period , zero ) : \n 
~~~ if self . values or zero : \n 
~~~ d = { } \n 
d [ ] = len ( self . values ) \n 
d [ ] = d [ ] / period \n 
d [ ] = mean ( self . values ) \n 
self . values . sort ( ) \n 
d [ ] = percentile ( self . values , 0.0 ) \n 
d [ ] = percentile ( self . values , 0.5 ) \n 
d [ ] = percentile ( self . values , 0.95 ) \n 
d [ ] = percentile ( self . values , 0.99 ) \n 
d [ ] = percentile ( self . values , 1.0 ) \n 
d [ ] = stddev ( self . values , d [ ] ) \n 
~~~ self . values [ : ] = [ ] \n 
~~ ~~ from unittest import TestCase \n 
import zmq . green as zmq \n 
import gevent \n 
import gevent . socket as socket \n 
from gevent . queue import Queue \n 
from logcabin . event import Event \n 
from logcabin . context import DummyContext \n 
from logcabin . inputs import udp , zeromq , file as fileinput \n 
from testhelper import TempDirectory , assertEventEquals \n 
class InputTests ( TestCase ) : \n 
~~~ def create ( self , conf ) : \n 
~~~ self . output = Queue ( ) \n 
with DummyContext ( ) : \n 
~~~ self . i = i = self . cls ( ** conf ) \n 
~~ i . setup ( self . output ) \n 
i . start ( ) \n 
return i \n 
~~ def waitForQueue ( self , timeout = 1.0 , events = 1 ) : \n 
~~~ with gevent . Timeout ( timeout ) : \n 
~~~ while self . output . qsize ( ) < events : \n 
~~~ gevent . sleep ( 0.0 ) \n 
~~ ~~ self . i . stop ( ) \n 
if events : \n 
~~~ return [ self . output . get ( ) for n in xrange ( events ) ] \n 
~~ ~~ ~~ class ZeromqTests ( InputTests ) : \n 
~~~ cls = zeromq . Zeromq \n 
def test_event ( self ) : \n 
~~~ conf = { : } \n 
self . create ( conf ) \n 
ctx = zmq . Context ( ) \n 
sock = ctx . socket ( zmq . PUSH ) \n 
sock . connect ( conf [ ] ) \n 
sock . send ( ) \n 
q = self . waitForQueue ( ) \n 
assertEventEquals ( self , Event ( data = ) , q [ 0 ] ) \n 
~~ ~~ class UdpTests ( InputTests ) : \n 
~~~ cls = udp . Udp \n 
~~~ conf = { : random . randint ( 1024 , 65535 ) } \n 
self . sock = socket . socket ( socket . AF_INET , socket . SOCK_DGRAM ) \n 
self . sock . sendto ( , ( , conf [ ] ) ) \n 
~~ ~~ class FileTests ( InputTests ) : \n 
~~~ cls = fileinput . File \n 
def test_success ( self ) : \n 
~~~ with TempDirectory ( ) : \n 
gevent . sleep ( 0.01 ) \n 
with file ( , ) as fin : \n 
~~~ print >> fin , \n 
print >> fin , \n 
~~ q = self . waitForQueue ( events = 2 ) \n 
assertEventEquals ( self , Event ( data = ) , q [ 1 ] ) \n 
~~ ~~ def test_rolling ( self ) : \n 
~~~ with file ( , ) as fin : \n 
~~ conf = { : } \n 
os . rename ( , ) \n 
~~ ~~ def test_multiple ( self ) : \n 
~~ with file ( , ) as fin : \n 
~~ ~~ def test_resume ( self ) : \n 
q = self . waitForQueue ( events = 1 ) \n 
~~ self . create ( conf ) \n 
~~ ~~ def test_resume_statedir ( self ) : \n 
~~~ os . mkdir ( ) \n 
~~ conf = { : , : } \n 
~~ ~~ def test_truncated ( self ) : \n 
~~ gevent . sleep ( 0.01 ) \n 
~~ q = self . waitForQueue ( events = 3 ) \n 
~~ ~~ def test_missing ( self ) : \n 
self . waitForQueue ( events = 0 ) \n 
#http://stackoverflow.com/questions/110498/is-there-an-easy-way-to-request-a-url-in-python-and-not-follow-redirects #http://kentsjohnson.com/kk/00010.html \n 
#http://stackoverflow.com/questions/4560288/python-try-except-showing-the-cause-of-the-error-after-displaying-my-variables \n 
~~ ~~ ~~ import sys \n 
requests = ; urls_accessible_http = [ ] \n 
~~~ list_of_requests = read_https_urls ( requests ) \n 
urls_accessible_http = request_over_http ( list_of_requests ) \n 
create_report ( urls_accessible_http ) \n 
~~ def create_report ( urls_accessible_http ) : \n 
~~~ f = open ( , ) \n 
traceback . print_exc ( file = sys . stdout ) \n 
~~ if len ( urls_accessible_http ) > 0 : \n 
~~~ for url in urls_accessible_http : \n 
~~~ f . write ( url + ) \n 
~~~ f . write ( ) \n 
~~ f . close ( ) \n 
~~ def read_https_urls ( requests ) : \n 
~~~ list_of_requests = [ ] \n 
~~~ f = open ( requests , ) \n 
~~ for url in f : \n 
~~~ url = re . sub ( , , url ) \n 
list_of_requests . append ( url ) \n 
return list_of_requests \n 
~~ def request_over_http ( list_of_requests ) : \n 
~~~ threads = [ ] \n 
for url in list_of_requests : \n 
url = re . sub ( , , url ) \n 
print + url \n 
~~~ f = urllib2 . urlopen ( url ) \n 
t1 = f . geturl ( ) . split ( ) \n 
if t1 [ 0 ] != : \n 
~~~ urls_accessible_http . append ( url ) \n 
~~ ~~ time . sleep ( 5 ) \n 
return urls_accessible_http \n 
~~ main ( ) \n 
from setuptools import setup \n 
author_email = \n 
license = \n 
def get_version ( package ) : \n 
init_py = open ( os . path . join ( package , ) ) . read ( ) \n 
init_py , re . MULTILINE ) . group ( 1 ) \n 
~~ def get_packages ( package ) : \n 
return [ dirpath \n 
for dirpath , dirnames , filenames in os . walk ( package ) \n 
if os . path . exists ( os . path . join ( dirpath , ) ) ] \n 
~~ def get_package_data ( package ) : \n 
walk = [ ( dirpath . replace ( package + os . sep , , 1 ) , filenames ) \n 
if not os . path . exists ( os . path . join ( dirpath , ) ) ] \n 
filepaths = [ ] \n 
for base , filenames in walk : \n 
~~~ filepaths . extend ( [ os . path . join ( base , filename ) \n 
for filename in filenames ] ) \n 
~~ return { package : filepaths } \n 
~~ version = get_version ( package ) \n 
if sys . argv [ - 1 ] == : \n 
~~ setup ( \n 
version = version , \n 
license = license , \n 
author = author , \n 
author_email = author_email , \n 
packages = get_packages ( package ) , \n 
package_data = get_package_data ( package ) , \n 
install_requires = [ ] , \n 
~~~ import sqlalchemy as sql \n 
HAS_SQLALCHEMY = True \n 
~~~ HAS_SQLALCHEMY = False \n 
~~~ import psycopg2 \n 
HAS_POSTGRES = True \n 
~~~ HAS_POSTGRES = False \n 
~~~ import _mysql \n 
HAS_MYSQL = True \n 
~~~ HAS_MYSQL = False \n 
from flask import Blueprint \n 
from test import FlaskTrackUsageTestCase \n 
from flask_track_usage import TrackUsage \n 
from flask_track_usage . storage . sql import SQLStorage \n 
class TestSQLiteStorage ( FlaskTrackUsageTestCase ) : \n 
~~~ def _create_storage ( self ) : \n 
~~~ engine = sql . create_engine ( "sqlite://" ) \n 
metadata = sql . MetaData ( bind = engine ) \n 
self . storage = SQLStorage ( \n 
engine = engine , \n 
metadata = metadata , \n 
table_name = self . given_table_name \n 
metadata . create_all ( ) \n 
~~~ meta = sql . MetaData ( ) \n 
meta . reflect ( bind = self . storage . _eng ) \n 
for table in reversed ( meta . sorted_tables ) : \n 
~~~ self . storage . _eng . execute ( table . delete ( ) ) \n 
~~~ self . given_table_name = \n 
FlaskTrackUsageTestCase . setUp ( self ) \n 
self . blueprint = Blueprint ( , __name__ ) \n 
@ self . blueprint . route ( ) \n 
def blueprint ( ) : \n 
~~~ return "blueprint" \n 
~~ self . app . register_blueprint ( self . blueprint ) \n 
self . _create_storage ( ) \n 
self . track_usage = TrackUsage ( self . app , self . storage ) \n 
self . track_usage . include_blueprint ( self . blueprint ) \n 
~~ def test_table_name ( self ) : \n 
assert self . given_table_name == meta . tables . keys ( ) [ 0 ] \n 
~~ def test_storage_data_basic ( self ) : \n 
~~~ self . client . get ( ) \n 
con = self . storage . _eng . connect ( ) \n 
s = sql . select ( [ self . storage . track_table ] ) \n 
result = con . execute ( s ) . fetchone ( ) \n 
assert result [ 1 ] == \n 
assert result [ 2 ] is None \n 
assert result [ 3 ] is None \n 
assert result [ 4 ] is None \n 
assert result [ 5 ] is None \n 
assert result [ 6 ] is None \n 
assert result [ 8 ] == 200 \n 
assert result [ 9 ] is None \n 
assert result [ 10 ] == None \n 
assert result [ 11 ] == False \n 
assert result [ 12 ] is None \n 
assert result [ 13 ] == \n 
assert result [ 14 ] . __class__ is float \n 
assert type ( result [ 15 ] ) is datetime . datetime \n 
~~ def test_storage_data_blueprint ( self ) : \n 
assert result [ 6 ] == \n 
assert result [ 10 ] is None \n 
~~ def test_storage__get_raw ( self ) : \n 
result = self . storage . _get_raw ( ) [ 0 ] \n 
self . client . get ( ) \n 
rows = self . storage . _get_raw ( ) \n 
print rows [ 1 ] \n 
assert len ( self . storage . _get_raw ( ) ) == 3 \n 
assert len ( self . storage . _get_raw ( limit = 2 ) ) == 2 \n 
assert len ( self . storage . _get_raw ( limit = 1 ) ) == 1 \n 
now = datetime . datetime . utcnow ( ) + datetime . timedelta ( 0 , 5 ) \n 
assert len ( self . storage . _get_raw ( start_date = now ) ) == 0 \n 
assert len ( self . storage . _get_raw ( end_date = now ) ) == 3 \n 
assert len ( self . storage . _get_raw ( end_date = now , limit = 2 ) ) == 2 \n 
~~ def test_storage__get_usage ( self ) : \n 
result2 = self . storage . _get_usage ( ) [ 0 ] \n 
assert result [ 1 ] == result2 [ ] \n 
assert result [ 2 ] == result2 [ ] [ ] \n 
assert result [ 3 ] == result2 [ ] [ ] \n 
assert result [ 4 ] == result2 [ ] [ ] \n 
assert result [ 5 ] == result2 [ ] [ ] \n 
assert result [ 6 ] == result2 [ ] \n 
assert result [ 8 ] == result2 [ ] \n 
assert result [ 9 ] == result2 [ ] \n 
assert result [ 10 ] == result2 [ ] \n 
assert result [ 11 ] == result2 [ ] \n 
assert result [ 12 ] == result2 [ ] \n 
assert result [ 13 ] == result2 [ ] \n 
assert result [ 14 ] == result2 [ ] \n 
assert result [ 15 ] == result2 [ ] \n 
~~ def test_storage_get_usage ( self ) : \n 
result2 = self . storage . get_usage ( ) [ 0 ] \n 
~~ def test_storage_get_usage_pagination ( self ) : \n 
~~~ for i in range ( 100 ) : \n 
~~ limit = 10 \n 
num_pages = 10 \n 
for page in range ( 1 , num_pages + 1 ) : \n 
~~~ result = self . storage . _get_usage ( limit = limit , page = page ) \n 
assert len ( result ) == limit \n 
~~~ assert result [ i ] [ 1 ] == result2 [ i ] [ ] \n 
assert result [ i ] [ 2 ] == result2 [ i ] [ ] [ ] \n 
assert result [ i ] [ 3 ] == result2 [ i ] [ ] [ ] \n 
assert result [ i ] [ 4 ] == result2 [ i ] [ ] [ ] \n 
assert result [ i ] [ 5 ] == result2 [ i ] [ ] [ ] \n 
assert result [ i ] [ 6 ] == result2 [ i ] [ ] \n 
assert result [ i ] [ 8 ] == result2 [ i ] [ ] \n 
assert result [ i ] [ 9 ] == result2 [ i ] [ ] \n 
assert result [ i ] [ 10 ] == result2 [ i ] [ ] \n 
assert result [ i ] [ 11 ] == result2 [ i ] [ ] \n 
assert result [ i ] [ 12 ] == result2 [ i ] [ ] \n 
assert result [ i ] [ 13 ] == result2 [ i ] [ ] \n 
assert result [ i ] [ 14 ] == result2 [ i ] [ ] \n 
assert result [ i ] [ 15 ] == result2 [ i ] [ ] \n 
class TestPostgresStorage ( TestSQLiteStorage ) : \n 
~~~ engine = sql . create_engine ( \n 
"postgresql+psycopg2://postgres:@localhost/track_usage_test" ) \n 
class TestMySQLStorage ( TestSQLiteStorage ) : \n 
"mysql+mysqldb://travis:@localhost/track_usage_test" ) \n 
~~ ~~ from conf . default import * \n 
DEBUG = False \n 
TEMPLATE_DEBUG = DEBUG \n 
ADMINS = ( \n 
MANAGERS = ADMINS \n 
DATABASE_ENGINE = \n 
DATABASE_NAME = \n 
DATABASE_USER = \n 
DATABASE_PASSWORD = \n 
DATABASE_HOST = \n 
DATABASE_PORT = \n 
from flask . ext . script import Manager \n 
from flask . ext . celery import install_commands as install_celery_commands \n 
from myapp import create_app \n 
app = create_app ( ) \n 
manager = Manager ( app ) \n 
install_celery_commands ( manager ) \n 
~~~ manager . run ( ) \n 
~~~ from setuptools import setup , find_packages , Command \n 
~~~ from ez_setup import use_setuptools \n 
use_setuptools ( ) \n 
from setuptools import setup , find_packages , Command \n 
~~ from distutils . command . install_data import install_data \n 
from distutils . command . install import INSTALL_SCHEMES \n 
import ghettoq \n 
packages , data_files = [ ] , [ ] \n 
root_dir = os . path . dirname ( __file__ ) \n 
if root_dir != : \n 
~~~ os . chdir ( root_dir ) \n 
~~ src_dir = "ghettoq" \n 
install_requires = [ ] \n 
if sys . version_info < ( 2 , 5 ) : \n 
~~~ install_requires . append ( "uuid" ) \n 
~~ if sys . version_info < ( 2 , 7 ) : \n 
~~~ install_requires . append ( "odict" ) \n 
~~ def osx_install_data ( install_data ) : \n 
~~~ def finalize_options ( self ) : \n 
~~~ self . set_undefined_options ( "install" , ( "install_lib" , "install_dir" ) ) \n 
install_data . finalize_options ( self ) \n 
~~ ~~ class RunTests ( Command ) : \n 
~~~ this_dir = os . getcwd ( ) \n 
testproj_dir = os . path . join ( this_dir , "testproj" ) \n 
os . chdir ( testproj_dir ) \n 
sys . path . insert ( 0 , testproj_dir ) \n 
os . environ [ "DJANGO_SETTINGS_MODULE" ] = os . environ . get ( \n 
"DJANGO_SETTINGS_MODULE" , "settings" ) \n 
settings_file = os . environ [ "DJANGO_SETTINGS_MODULE" ] \n 
settings_mod = __import__ ( settings_file , { } , { } , [ ] ) \n 
execute_manager ( settings_mod , argv = [ \n 
__file__ , "test" ] ) \n 
os . chdir ( this_dir ) \n 
~~ ~~ def fullsplit ( path , result = None ) : \n 
~~~ if result is None : \n 
~~ head , tail = os . path . split ( path ) \n 
if head == : \n 
~~~ return [ tail ] + result \n 
~~ if head == path : \n 
~~~ return result \n 
~~ return fullsplit ( head , [ tail ] + result ) \n 
~~ for scheme in INSTALL_SCHEMES . values ( ) : \n 
~~~ scheme [ ] = scheme [ ] \n 
~~ SKIP_EXTENSIONS = [ ".pyc" , ".pyo" , ".swp" , ".swo" ] \n 
def is_unwanted_file ( filename ) : \n 
~~~ for skip_ext in SKIP_EXTENSIONS : \n 
~~~ if filename . endswith ( skip_ext ) : \n 
~~ for dirpath , dirnames , filenames in os . walk ( src_dir ) : \n 
~~~ for i , dirname in enumerate ( dirnames ) : \n 
~~~ if dirname . startswith ( "." ) : \n 
~~~ del dirnames [ i ] \n 
~~ ~~ for filename in filenames : \n 
~~~ if filename . endswith ( ".py" ) : \n 
~~~ packages . append ( . join ( fullsplit ( dirpath ) ) ) \n 
~~ elif is_unwanted_file ( filename ) : \n 
~~~ data_files . append ( [ dirpath , [ os . path . join ( dirpath , f ) for f in \n 
filenames ] ] ) \n 
version = ghettoq . __version__ , \n 
description = ghettoq . __doc__ , \n 
author = ghettoq . __author__ , \n 
author_email = ghettoq . __contact__ , \n 
url = ghettoq . __homepage__ , \n 
platforms = [ "any" ] , \n 
packages = packages , \n 
data_files = data_files , \n 
cmdclass = { "test" : RunTests } , \n 
zip_safe = False , \n 
test_suite = "nose.collector" , \n 
install_requires = install_requires , \n 
long_description = codecs . open ( , "r" , "utf-8" ) . read ( ) , \n 
from nose . tools import eq_ \n 
import utils \n 
class PullRequest ( utils . HttpMockTestCase ) : \n 
~~~ def test_properties ( self ) : \n 
~~~ pull_request = self . client . pull_requests . show ( , 39 ) \n 
eq_ ( pull_request . state , ) \n 
eq_ ( pull_request . base [ ] , \n 
eq_ ( pull_request . head [ ] , \n 
eq_ ( pull_request . issue_user [ ] , ) \n 
eq_ ( pull_request . user [ ] , ) \n 
eq_ ( pull_request . title , ) \n 
eq_ ( len ( pull_request . body ) , 1442 ) \n 
eq_ ( pull_request . position , 39.0 ) \n 
eq_ ( pull_request . number , 39.0 ) \n 
eq_ ( pull_request . votes , 0 ) \n 
eq_ ( pull_request . comments , 4 ) \n 
eq_ ( pull_request . diff_url , \n 
eq_ ( pull_request . patch_url , \n 
eq_ ( pull_request . labels , [ ] ) \n 
eq_ ( pull_request . html_url , \n 
eq_ ( pull_request . issue_created_at , datetime ( 2011 , 4 , 18 , 15 , 25 , 47 ) ) \n 
eq_ ( pull_request . issue_updated_at , datetime ( 2011 , 6 , 23 , 9 , 33 , 57 ) ) \n 
eq_ ( pull_request . created_at , datetime ( 2011 , 6 , 20 , 16 , 51 , 24 ) ) \n 
eq_ ( pull_request . updated_at , datetime ( 2011 , 6 , 23 , 9 , 28 , 42 ) ) \n 
eq_ ( pull_request . closed_at , datetime ( 2011 , 6 , 23 , 9 , 33 , 57 ) ) \n 
eq_ ( len ( pull_request . discussion ) , 13 ) \n 
eq_ ( pull_request . mergeable , True ) \n 
~~ def test_repr ( self ) : \n 
eq_ ( repr ( pull_request ) , ) \n 
~~ ~~ class PullRequestQueries ( utils . HttpMockTestCase ) : \n 
def test_list ( self ) : \n 
~~~ pull_requests = self . client . pull_requests . list ( ) \n 
eq_ ( len ( pull_requests ) , 1 ) \n 
eq_ ( pull_requests [ 0 ] . title , ) \n 
~~ def test_list_with_page ( self ) : \n 
~~~ pull_requests = self . client . pull_requests . list ( , \n 
page = 2 ) \n 
eq_ ( len ( pull_requests ) , 52 ) \n 
eq_ ( pull_requests [ 1 ] . title , ) \n 
~~ ~~ from __future__ import absolute_import \n 
from pip_custom_platform . install import install \n 
from pip_custom_platform . util import default_platform_name \n 
from pip_custom_platform . wheel import wheel \n 
def add_shared_arguments ( parser ) : \n 
~~~ default_platform = default_platform_name ( ) \n 
default = default_platform , \n 
help = . format ( default_platform ) , \n 
~~ def main ( argv = None ) : \n 
~~~ parser = argparse . ArgumentParser ( \n 
description = ( \n 
subparsers = parser . add_subparsers ( dest = ) \n 
install_parser = subparsers . add_parser ( , help = ) \n 
add_shared_arguments ( install_parser ) \n 
wheel_parser = subparsers . add_parser ( , help = ) \n 
add_shared_arguments ( wheel_parser ) \n 
wheel_parser . add_argument ( \n 
, , help = , \n 
default = , \n 
args , argv = parser . parse_known_args ( argv ) \n 
platform = args . platform . replace ( , ) \n 
if args . command == : \n 
~~~ return install ( platform , argv ) \n 
~~ elif args . command == : \n 
~~~ return wheel ( platform , args . wheel_dir , argv ) \n 
~~~ raise NotImplementedError ( . format ( args . command ) ) \n 
~~~ exit ( main ( ) ) \n 
~~ from . component import * \n 
from . context import * \n 
from . event import * \n 
from . runner import * \n 
from . util import * \n 
from flask import Blueprint , make_response , request , redirect , url_for , session , render_template , current_app , send_from_directory , flash \n 
from uuid import uuid4 \n 
from werkzeug import secure_filename \n 
from csvkit import convert \n 
from csvkit . unicsv import UnicodeCSVReader \n 
from csvkit . cleanup import RowChecker \n 
from geomancer . helpers import import_class , get_geo_types , get_data_sources , guess_geotype , check_combos , SENSICAL_TYPES \n 
from geomancer . app_config import ALLOWED_EXTENSIONS , MAX_CONTENT_LENGTH \n 
from werkzeug . exceptions import RequestEntityTooLarge \n 
views = Blueprint ( , __name__ ) \n 
def allowed_file ( filename ) : \n 
~~~ return in filename and filename . rsplit ( , 1 ) [ 1 ] in ALLOWED_EXTENSIONS \n 
~~ @ views . route ( , methods = [ , ] ) \n 
~~~ return render_template ( ) \n 
def about ( ) : \n 
def upload_formats ( ) : \n 
def contribute_data ( ) : \n 
def geographies ( ) : \n 
~~~ geographies , errors = get_geo_types ( ) \n 
for error in errors : \n 
~~~ flash ( error ) \n 
~~ return render_template ( , geographies = geographies ) \n 
def data_sources ( ) : \n 
~~~ data_sources , errors = get_data_sources ( ) \n 
~~ return render_template ( , data_sources = data_sources ) \n 
def upload ( ) : \n 
~~~ context = { } \n 
if request . method == : \n 
~~~ big_file = False \n 
~~~ files = request . files \n 
~~ except RequestEntityTooLarge , e : \n 
~~~ files = None \n 
big_file = True \n 
current_app . logger . info ( e ) \n 
~~ if files : \n 
~~~ f = files [ ] \n 
if allowed_file ( f . filename ) : \n 
~~~ inp = StringIO ( f . read ( ) ) \n 
file_format = convert . guess_format ( f . filename ) \n 
~~~ converted = convert . convert ( inp , file_format ) \n 
~~ except UnicodeDecodeError : \n 
~~~ context [ ] = [ ] \n 
converted = None \n 
~~ f . seek ( 0 ) \n 
if converted : \n 
~~~ outp = StringIO ( converted ) \n 
reader = UnicodeCSVReader ( outp ) \n 
session [ ] = reader . next ( ) \n 
rows = [ ] \n 
columns = [ [ ] for c in session [ ] ] \n 
column_ids = range ( len ( session [ ] ) ) \n 
for row in range ( 100 ) : \n 
~~~ rows . append ( reader . next ( ) ) \n 
~~ ~~ for i , row in enumerate ( rows ) : \n 
~~~ for j , d in enumerate ( row ) : \n 
~~~ columns [ j ] . append ( row [ column_ids [ j ] ] ) \n 
~~ ~~ sample_data = [ ] \n 
guesses = { } \n 
for index , header_val in enumerate ( session [ ] ) : \n 
~~~ guesses [ index ] = guess_geotype ( header_val , columns [ index ] ) \n 
sample_data . append ( ( index , header_val , columns [ index ] ) ) \n 
~~ session [ ] = sample_data \n 
session [ ] = json . dumps ( guesses ) \n 
outp . seek ( 0 ) \n 
session [ ] = outp . getvalue ( ) \n 
session [ ] = f . filename \n 
return redirect ( url_for ( ) ) \n 
if big_file : \n 
~~ ~~ ~~ return render_template ( , ** context ) \n 
def select_geo ( ) : \n 
~~~ if not session . get ( ) : \n 
~~~ return redirect ( url_for ( ) ) \n 
~~ context = { } \n 
~~~ inp = StringIO ( session [ ] ) \n 
reader = UnicodeCSVReader ( inp ) \n 
header = reader . next ( ) \n 
fields = { } \n 
geotype_val = None \n 
if not request . form : \n 
context [ ] = [ ] \n 
~~~ geotypes = [ ] \n 
indexes = [ ] \n 
for k , v in request . form . items ( ) : \n 
~~~ if k . startswith ( "geotype" ) : \n 
~~~ geotypes . append ( v ) \n 
indexes . append ( k . split ( ) [ 1 ] ) \n 
~~ ~~ if len ( indexes ) > 2 : \n 
~~~ fields_key = . join ( [ header [ int ( i ) ] for i in indexes ] ) \n 
geotype_val = . join ( [ g for g in geotypes ] ) \n 
if not check_combos ( geotype_val ) : \n 
types = [ t . title ( ) for t in geotype_val . split ( ) ] \n 
context [ ] = [ . format ~~ else : \n 
~~~ fields [ fields_key ] = { \n 
: geotype_val , \n 
: . join ( indexes ) \n 
~~ ~~ ~~ if valid : \n 
~~~ geo_type = SENSICAL_TYPES [ geotype_val ] \n 
~~~ geo_type = geotype_val \n 
~~ mancer_data , errors = get_data_sources ( geo_type = geo_type ) \n 
session [ ] = fields \n 
session [ ] = mancer_data \n 
~~ return redirect ( url_for ( ) ) \n 
~~ ~~ return render_template ( , ** context ) \n 
def select_tables ( ) : \n 
if request . method == and not request . form : \n 
~~ return render_template ( , ** context ) \n 
~~ @ views . route ( ) \n 
def geomance_view ( session_key ) : \n 
~~~ return render_template ( , session_key = session_key ) \n 
def download_results ( filename ) : \n 
~~~ return send_from_directory ( current_app . config [ ] , filename ) \n 
def file_too_large ( ) : \n 
~~~ return make_response ( render_template ( ) , 413 ) \n 
from starpy import manager \n 
import utilapplication \n 
from twisted . internet import reactor \n 
import sys , logging \n 
APPLICATION = utilapplication . UtilApplication ( ) \n 
def main ( channel = , connectTo = ( , , ) ) : \n 
~~~ df = APPLICATION . amiSpecifier . login ( ) \n 
def onLogin ( protocol ) : \n 
context , extension , priority = connectTo \n 
df = protocol . originate ( \n 
channel , \n 
context , extension , priority , \n 
def onFinished ( result ) : \n 
~~~ return protocol . logoff ( ) \n 
~~ df . addCallbacks ( onFinished , onFinished ) \n 
return df \n 
~~ def onFailure ( reason ) : \n 
~~~ print reason . getTraceback ( ) \n 
~~ def onFinished ( result ) : \n 
~~~ reactor . stop ( ) \n 
~~ df . addCallbacks ( \n 
onLogin , onFailure \n 
) . addCallbacks ( onFinished , onFinished ) \n 
~~~ logging . basicConfig ( ) \n 
reactor . callWhenRunning ( main ) \n 
reactor . run ( ) \n 
~~~ import xmlrpclib \n 
~~~ import xmlrpc . client as xmlrpclib \n 
~~ from functools import partial \n 
import stdeb \n 
from stdeb . transport import RequestsTransport \n 
myprint = print \n 
USER_AGENT = % stdeb . __version__ \n 
def find_tar_gz ( package_name , pypi_url = , \n 
verbose = 0 , release = None ) : \n 
~~~ transport = RequestsTransport ( ) \n 
transport . user_agent = USER_AGENT \n 
if pypi_url . startswith ( ) : \n 
~~~ transport . use_https = True \n 
~~ pypi = xmlrpclib . ServerProxy ( pypi_url , transport = transport ) \n 
download_url = None \n 
expected_md5_digest = None \n 
if verbose >= 2 : \n 
package_name ) ) \n 
~~ show_hidden = True \n 
all_releases = pypi . package_releases ( package_name , show_hidden ) \n 
if release is not None : \n 
~~~ if verbose >= 2 : \n 
~~~ myprint ( % ( . join ( all_releases ) , ) ) \n 
~~ if release not in all_releases : \n 
% ( release , all_releases ) ) \n 
~~ version = release \n 
~~~ default_releases = pypi . package_releases ( package_name ) \n 
if len ( default_releases ) != 1 : \n 
% ( \n 
default_releases , all_releases ) ) \n 
~~ default_release = default_releases [ 0 ] \n 
~~~ myprint ( % ( . join ( default_releases ) , ) ) \n 
~~ version = default_release \n 
~~ urls = pypi . release_urls ( package_name , version ) \n 
for url in urls : \n 
~~~ if url [ ] == : \n 
~~~ assert url [ ] == , \n 
if url [ ] . endswith ( ) : \n 
~~~ download_url = url [ ] \n 
if in url : \n 
~~~ expected_md5_digest = url [ ] \n 
~~ ~~ ~~ if download_url is None : \n 
~~~ result = pypi . release_data ( package_name , version ) \n 
if result [ ] != : \n 
~~~ download_url = result [ ] \n 
urls = pypi . release_urls ( result [ ] , result [ ] ) \n 
~~ ~~ if download_url is None : \n 
~~ return download_url , expected_md5_digest \n 
~~ def md5sum ( filename ) : \n 
~~~ d = hashlib . md5 ( ) \n 
for buf in iter ( partial ( f . read , 128 ) , ) : \n 
~~~ d . update ( buf ) \n 
~~ ~~ return d . hexdigest ( ) \n 
~~ def get_source_tarball ( package_name , verbose = 0 , allow_unsafe_download = False , \n 
release = None ) : \n 
~~~ download_url , expected_md5_digest = find_tar_gz ( package_name , \n 
release = release ) \n 
if not download_url . startswith ( ) : \n 
~~~ if allow_unsafe_download : \n 
~~~ warnings . warn ( % download_url ) \n 
~~~ raise ValueError ( % download_url ) \n 
~~ ~~ fname = download_url . split ( ) [ - 1 ] \n 
if expected_md5_digest is not None : \n 
~~~ actual_md5_digest = md5sum ( fname ) \n 
if actual_md5_digest == expected_md5_digest : \n 
~~~ if verbose >= 1 : \n 
~~~ myprint ( % download_url ) \n 
~~ return fname \n 
~~ ~~ ~~ if verbose >= 1 : \n 
~~ headers = { : USER_AGENT } \n 
r = requests . get ( download_url , headers = headers ) \n 
r . raise_for_status ( ) \n 
package_tar_gz = r . content \n 
if verbose >= 1 : \n 
~~~ myprint ( % ( len ( package_tar_gz ) , ) ) \n 
~~ if expected_md5_digest is not None : \n 
~~~ m = hashlib . md5 ( ) \n 
m . update ( package_tar_gz ) \n 
actual_md5_digest = m . hexdigest ( ) \n 
~~~ myprint ( % ( actual_md5_digest , \n 
expected_md5_digest ) ) \n 
~~ if actual_md5_digest != expected_md5_digest : \n 
~~~ warnings . warn ( ) \n 
~~ fd = open ( fname , mode = ) \n 
fd . write ( package_tar_gz ) \n 
fd . close ( ) \n 
return fname \n 
import sys , os \n 
lib_path = os . path . abspath ( ) \n 
project_path = os . path . abspath ( ) \n 
sys . path . insert ( 0 , lib_path ) \n 
sys . path . insert ( 0 , project_path ) \n 
os . environ [ ] = \n 
sys . path . append ( os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , "_ext" ) ) ) \n 
setup ( name = , \n 
version = , \n 
package_dir = { : } , \n 
long_description = , \n 
keywords = [ ] , \n 
zip_safe = True , \n 
from setuptools import setup , find_packages \n 
def read ( fname ) : \n 
~~~ return open ( os . path . join ( os . path . dirname ( __file__ ) , fname ) ) . read ( ) \n 
name = "tse" , \n 
version = "0.0.13" , \n 
author_email = "ishimoto@gembook.org" , \n 
license = "MIT" , \n 
url = "https://github.com/atsuoishimoto/tse" , \n 
long_description = read ( ) , \n 
entry_points = { \n 
: [ ] \n 
install_requires = [ "argparse" , "six" ] , \n 
test_suite = "tests" , \n 
from pygments . style import Style \n 
from pygments . token import Keyword , Name , Comment , String , Error , Number , Operator , Generic , Whitespace , Punctuation , Other , Literal \n 
class ComplexityStyle ( Style ) : \n 
~~~ default_style = "" \n 
styles = { \n 
Other : "#000000" , \n 
Comment . Preproc : "noitalic" , \n 
Operator : "#582800" , \n 
Name : "#000000" , \n 
Name . Attribute : "#c4a000" , \n 
Name . Builtin : "#004461" , \n 
Name . Builtin . Pseudo : "#3465a4" , \n 
Name . Class : "#000000" , \n 
Name . Constant : "#000000" , \n 
Name . Decorator : "#888" , \n 
Name . Entity : "#ce5c00" , \n 
Name . Function : "#000000" , \n 
Name . Property : "#000000" , \n 
Name . Label : "#f57900" , \n 
Name . Namespace : "#000000" , \n 
Name . Other : "#000000" , \n 
Name . Variable : "#000000" , \n 
Name . Variable . Class : "#000000" , \n 
Name . Variable . Global : "#000000" , \n 
Name . Variable . Instance : "#000000" , \n 
Number : "#990000" , \n 
Literal : "#000000" , \n 
Literal . Date : "#000000" , \n 
String : "#4e9a06" , \n 
String . Backtick : "#4e9a06" , \n 
String . Char : "#4e9a06" , \n 
String . Double : "#4e9a06" , \n 
String . Escape : "#4e9a06" , \n 
String . Heredoc : "#4e9a06" , \n 
String . Interpol : "#4e9a06" , \n 
String . Other : "#4e9a06" , \n 
String . Regex : "#4e9a06" , \n 
String . Single : "#4e9a06" , \n 
String . Symbol : "#4e9a06" , \n 
Generic : "#000000" , \n 
Generic . Deleted : "#a40000" , \n 
Generic . Error : "#ef2929" , \n 
Generic . Inserted : "#00A000" , \n 
Generic . Output : "#888" , \n 
Generic . Prompt : "#745334" , \n 
~~ import pytest \n 
@ pytest . fixture \n 
def context ( ) : \n 
return { \n 
def replay_test_dir ( ) : \n 
def mock_user_config ( mocker ) : \n 
~~~ return mocker . patch ( ) \n 
from cookiecutter import generate \n 
from cookiecutter import utils \n 
from cookiecutter import exceptions \n 
@ pytest . fixture ( scope = ) \n 
def remove_output_folder ( request ) : \n 
def finalizer_remove_output_folder ( ) : \n 
~~~ if os . path . exists ( ) : \n 
~~~ utils . rmtree ( ) \n 
~~ ~~ request . addfinalizer ( finalizer_remove_output_folder ) \n 
~~ @ pytest . mark . usefixtures ( , ) \n 
def test_output_folder ( ) : \n 
~~~ context = generate . generate_context ( \n 
context_file = \n 
generate . generate_files ( \n 
context = context , \n 
repo_dir = \n 
something2 = open ( ) . read ( ) \n 
assert something == something2 \n 
in_folder2 = open ( ) . read ( ) \n 
assert in_folder == in_folder2 \n 
assert os . path . isdir ( ) \n 
assert os . path . isfile ( ) \n 
def test_exception_when_output_folder_exists ( ) : \n 
output_folder = context [ ] [ ] \n 
if not os . path . exists ( output_folder ) : \n 
~~~ os . makedirs ( output_folder ) \n 
~~ with pytest . raises ( exceptions . OutputDirExistsException ) : \n 
~~~ generate . generate_files ( \n 
if sys . version_info < ( 2 , 7 ) : \n 
~~ from authy import AuthyException , AuthyFormatException \n 
from authy . api . resources import Token \n 
from authy . api . resources import Tokens \n 
from authy . api . resources import User \n 
from authy . api . resources import Users \n 
class TokensTest ( unittest . TestCase ) : \n 
~~~ self . users = Users ( "http://sandbox-api.authy.com" , ) \n 
self . resource = Tokens ( "http://sandbox-api.authy.com" , ) \n 
~~ def test_verify_digits_token ( self ) : \n 
~~~ user = self . users . create ( , , 1 ) \n 
~~~ token = self . resource . verify ( user . id , ) \n 
self . fail ( ) \n 
~~~ self . assertIsInstance ( e , AuthyFormatException ) \n 
self . assertEqual ( e . message , ) \n 
~~ ~~ def test_verify_digits_authy_id ( self ) : \n 
~~~ token = self . resource . verify ( , ) \n 
~~ ~~ def test_verify_longer_token ( self ) : \n 
~~ ~~ def test_verify_invalid_token ( self ) : \n 
token = self . resource . verify ( user . id , ) \n 
self . assertIsInstance ( token , Token ) \n 
self . assertFalse ( token . ok ( ) ) \n 
self . assertEqual ( token . response . status_code , 401 ) \n 
self . assertEqual ( token . errors ( ) [ ] , ) \n 
~~ def test_verify_valid_token ( self ) : \n 
self . assertTrue ( token . ok ( ) ) \n 
~~ def test_force_verify_token ( self ) : \n 
token = self . resource . verify ( user . id , , { "force" : True } ) \n 
~~ ~~ from . container import Container , Well , WellGroup \n 
from . protocol import Protocol \n 
from . container_type import ContainerType \n 
from . unit import Unit \n 
class UserError ( Exception ) : \n 
def __init__ ( self , message , info = None ) : \n 
~~~ super ( Exception , self ) . __init__ ( message ) \n 
self . info = info \n 
~~ ~~ import os , sys \n 
~~~ os . environ . setdefault ( "DJANGO_SETTINGS_MODULE" , "root.settings" ) \n 
from django . core . management import execute_from_command_line \n 
execute_from_command_line ( sys . argv ) \n 
from bottle import route , run , app , response \n 
from appmetrics import metrics \n 
@ route ( ) \n 
@ metrics . with_meter ( "factorial-tp" ) \n 
@ metrics . with_histogram ( "factorial-latency" ) \n 
@ metrics . with_meter ( "throughput" ) \n 
def factorial ( n ) : \n 
~~~ f = 1 \n 
for i in xrange ( 2 , n + 1 ) : \n 
~~~ f *= i \n 
~~ response . content_type = "application/json" \n 
return json . dumps ( dict ( factorial = str ( f ) ) ) \n 
~~ @ route ( ) \n 
@ metrics . with_meter ( "primality-tp" ) \n 
@ metrics . with_histogram ( "primality-latency" ) \n 
def is_prime ( n ) : \n 
~~~ result = True \n 
if n % 2 == 0 : \n 
~~~ result = False \n 
~~~ for i in xrange ( 3 , int ( n ** 0.5 ) + 1 , 2 ) : \n 
~~~ if n % i == 0 : \n 
~~ ~~ ~~ response . content_type = "application/json" \n 
return json . dumps ( dict ( is_prime = result ) ) \n 
~~~ from appmetrics . wsgi import AppMetricsMiddleware \n 
myapp = AppMetricsMiddleware ( app ( ) ) \n 
run ( app = myapp , port = 5000 , threaded = True , debug = True ) \n 
class Router ( object ) : \n 
~~~ def __init__ ( self , handlers = None ) : \n 
~~~ self . handlers = handlers or [ ] \n 
~~ def add_handler ( self , url_regex , handlers ) : \n 
~~~ self . handlers . append ( \n 
( re . compile ( url_regex ) , \n 
handlers if isinstance ( handlers , collections . Iterable ) \n 
else [ handlers ] ) ) \n 
~~ def get_handler ( self , url ) : \n 
~~~ for matcher , handler in self . handlers : \n 
~~~ match = matcher . match ( url ) \n 
~~~ return handler , match . groups ( ) \n 
REV_ODIR = { \n 
_classes = { } \n 
def query_class ( QueryClass ) : \n 
global _classes \n 
~~~ return _classes [ QueryClass ] \n 
~~ class PyOdbcSSQuery ( QueryClass ) : \n 
~~~ from sql_server . pyodbc import aggregates \n 
aggregates_module = aggregates \n 
~~~ super ( PyOdbcSSQuery , self ) . __init__ ( * args , ** kwargs ) \n 
self . default_reverse_ordering = False \n 
self . _ord = [ ] \n 
from django . db . models . sql . subqueries import InsertQuery \n 
if isinstance ( self , InsertQuery ) : \n 
~~~ self . _orig_as_sql = self . as_sql \n 
self . as_sql = self . _insert_as_sql \n 
~~ ~~ def _insert_as_sql ( self , * args , ** kwargs ) : \n 
meta = self . get_meta ( ) \n 
quoted_table = self . connection . ops . quote_name ( meta . db_table ) \n 
sql , params = self . _orig_as_sql ( * args , ** kwargs ) \n 
if meta . has_auto_field and meta . auto_field . column in self . columns : \n 
~~~ if len ( self . columns ) == 1 and not params : \n 
~~ ~~ return sql , params \n 
if hasattr ( QueryClass , ) : \n 
~~~ assert hasattr ( QueryClass , ) \n 
data = self . __getstate__ ( ) \n 
~~~ data = self . __dict__ \n 
~~ return ( unpickle_query_class , ( QueryClass , ) , data ) \n 
~~ def convert_values ( self , value , field ) : \n 
if value is None : \n 
~~ if field and field . get_internal_type ( ) == : \n 
~~~ return value \n 
~~ elif field and field . get_internal_type ( ) == : \n 
~~ elif isinstance ( value , datetime ) and value . hour == value . minute == value . second == value ~~~ value = value . date ( ) \n 
~~ elif value is not None and field and field . get_internal_type ( ) == : \n 
~~~ value = float ( value ) \n 
~~ return value \n 
~~ def resolve_columns ( self , row , fields = ( ) ) : \n 
~~~ index_start = len ( self . extra_select . keys ( ) ) \n 
values = [ self . convert_values ( v , None ) for v in row [ : index_start ] ] \n 
for value , field in map ( None , row [ index_start : ] , fields ) : \n 
~~~ values . append ( self . convert_values ( value , field ) ) \n 
~~ return tuple ( values ) \n 
~~ def modify_query ( self , strategy , ordering , out_cols ) : \n 
cnt = 0 \n 
extra_select_aliases = [ k . strip ( ) for k in self . extra_select . keys ( ) ] \n 
for ord_spec_item in ordering : \n 
~~~ if ord_spec_item . endswith ( ) or ord_spec_item . endswith ( ) : \n 
~~~ parts = ord_spec_item . split ( ) \n 
col , odir = . join ( parts [ : - 1 ] ) , parts [ - 1 ] \n 
if col not in self . ordering_aliases and col . strip ( ) not in extra_select_aliases ~~~ if col . isdigit ( ) : \n 
~~~ cnt += 1 \n 
n = int ( col ) - 1 \n 
alias = % cnt \n 
out_cols [ n ] = % ( out_cols [ n ] , alias ) \n 
self . _ord . append ( ( alias , odir ) ) \n 
~~ elif col in out_cols : \n 
~~~ if strategy == USE_TOP_HMARK : \n 
n = out_cols . index ( col ) \n 
out_cols [ n ] = % ( col , alias ) \n 
~~~ self . _ord . append ( ( col , odir ) ) \n 
~~ ~~ elif strategy == USE_TOP_HMARK : \n 
~~~ if col . split ( ) [ - 1 ] == and odir == : \n 
~~~ self . default_reverse_ordering = True \n 
~~ cnt += 1 \n 
self . ordering_aliases . append ( % ( col , alias ) ) \n 
~~ ~~ ~~ if strategy == USE_ROW_NUMBER and not self . _ord and in ordering : \n 
~~~ self . _ord . append ( ( , ) ) \n 
~~ if strategy == USE_TOP_HMARK and not self . _ord : \n 
~~~ meta = self . model . _meta \n 
qn = self . quote_name_unless_alias \n 
pk_col = % ( qn ( meta . db_table ) , qn ( meta . pk . db_column or meta . pk . column ) ) \n 
if pk_col not in out_cols : \n 
~~~ out_cols . append ( pk_col ) \n 
~~ ~~ ~~ def _as_sql ( self , strategy ) : \n 
out_cols = self . get_columns ( True ) \n 
ordering , ordering_group_by = self . get_ordering ( ) \n 
if strategy == USE_ROW_NUMBER : \n 
~~~ if not ordering : \n 
~~~ meta = self . get_meta ( ) \n 
if % ( qn ( meta . db_table ) , qn ( meta . pk . db_column or meta . pk . column ) ) not in ~~~ ordering = [ ] \n 
~~~ ordering = [ % ( qn ( meta . db_table ) , qn ( meta . pk . db_column or meta . pk \n 
~~ ~~ ~~ if strategy in ( USE_TOP_HMARK , USE_ROW_NUMBER ) : \n 
~~~ self . modify_query ( strategy , ordering , out_cols ) \n 
~~ if strategy == USE_ROW_NUMBER : \n 
~~~ ord = . join ( [ % pair for pair in self . _ord ] ) \n 
self . ordering_aliases . append ( % ord ) \n 
~~ from_ , f_params = self . get_from_clause ( ) \n 
where , w_params = self . where . as_sql ( qn = qn ) \n 
having , h_params = self . having . as_sql ( qn = qn ) \n 
for val in self . extra_select . itervalues ( ) : \n 
~~~ params . extend ( val [ 1 ] ) \n 
~~ result = [ ] \n 
if self . distinct : \n 
~~ if strategy == USE_TOP_LMARK : \n 
result . append ( % ( self . low_mark , self . quote_name_unless_alias ( meta . pk . db_column ~~ else : \n 
~~~ if strategy == USE_TOP_HMARK and self . high_mark is not None : \n 
~~~ result . append ( % self . high_mark ) \n 
~~ result . append ( . join ( out_cols + self . ordering_aliases ) ) \n 
~~ result . append ( ) \n 
result . extend ( from_ ) \n 
params . extend ( f_params ) \n 
if where : \n 
~~~ result . append ( % where ) \n 
params . extend ( w_params ) \n 
~~ if self . extra_where : \n 
~~~ if not where : \n 
~~ result . append ( . join ( self . extra_where ) ) \n 
~~ grouping , gb_params = self . get_grouping ( ) \n 
if grouping : \n 
~~~ if ordering : \n 
~~~ if not self . connection . features . allows_group_by_pk : \n 
~~~ for col , col_params in ordering_group_by : \n 
~~~ if col not in grouping : \n 
~~~ grouping . append ( str ( col ) ) \n 
gb_params . extend ( col_params ) \n 
~~~ ordering = self . connection . ops . force_no_ordering ( ) \n 
~~ result . append ( % . join ( grouping ) ) \n 
params . extend ( gb_params ) \n 
~~ if having : \n 
~~~ result . append ( % having ) \n 
params . extend ( h_params ) \n 
~~ params . extend ( self . extra_params ) \n 
return . join ( result ) , tuple ( params ) \n 
~~ def as_sql ( self , with_limits = True , with_col_aliases = False ) : \n 
do_offset = with_limits and ( self . high_mark is not None or self . low_mark != 0 ) \n 
if not do_offset : \n 
~~~ return super ( PyOdbcSSQuery , self ) . as_sql ( with_limits = False , \n 
with_col_aliases = with_col_aliases ) \n 
~~ if self . high_mark == 0 : \n 
~~~ return "" , ( ) \n 
~~ self . pre_sql_setup ( ) \n 
meta = self . model . _meta \n 
fallback_ordering = % ( qn ( meta . db_table ) , qn ( meta . pk . db_column or meta . pk . column \n 
if self . connection . ops . sql_server_ver < 2005 and self . high_mark is not None : \n 
~~~ orig_sql , params = self . _as_sql ( USE_TOP_HMARK ) \n 
if self . _ord : \n 
rev_ord = . join ( [ % ( col , REV_ODIR [ odir ] ) for col , odir in self . _ord ] ~~ else : \n 
~~~ if not self . default_reverse_ordering : \n 
~~~ ord = % fallback_ordering \n 
rev_ord = % fallback_ordering \n 
~~ ~~ sql = SQL_SERVER_8_LIMIT_QUERY % { \n 
: self . high_mark - self . low_mark , \n 
: orig_sql , \n 
: ord , \n 
: rev_ord , \n 
: qn ( meta . db_table ) , \n 
return sql , params \n 
~~ if self . connection . ops . sql_server_ver >= 2005 : \n 
~~~ sql , params = self . _as_sql ( USE_ROW_NUMBER ) \n 
result = [ % sql ] \n 
if self . high_mark is None : \n 
~~~ self . high_mark = 9223372036854775807 \n 
~~ result . append ( % ( self . low_mark + 1 , self . high_mark ) ) \n 
return . join ( result ) , params \n 
~~ self . get_columns ( with_col_aliases ) \n 
if ordering : \n 
~~~ ord = . join ( ordering ) \n 
~~~ ord = fallback_ordering \n 
~~ orig_sql , params = self . _as_sql ( USE_TOP_LMARK ) \n 
sql = SQL_SERVER_8_NO_LIMIT_QUERY % { \n 
: qn ( meta . pk . db_column or meta . pk . column ) , \n 
~~ ~~ _classes [ QueryClass ] = PyOdbcSSQuery \n 
return PyOdbcSSQuery \n 
~~ def unpickle_query_class ( QueryClass ) : \n 
klass = query_class ( QueryClass ) \n 
return klass . __new__ ( klass ) \n 
~~ unpickle_query_class . __safe_for_unpickling__ = True \n 
from rockstar import RockStar \n 
rock_it_bro = RockStar ( days = 400 , file_name = , code = ada_code ) \n 
rock_it_bro . make_me_a_rockstar ( ) \n 
rock_it_bro = RockStar ( days = 400 , file_name = , code = emacslisp_code ) \n 
rock_it_bro = RockStar ( days = 400 , file_name = , code = python_code ) \n 
import logging . config \n 
from houston import controller \n 
from houston import DEBUG_CONFIG \n 
from houston import LOG_CONFIG \n 
LOGGER = logging . getLogger ( __name__ ) \n 
DESC = \n 
class CLI ( object ) : \n 
~~~ self . _parser = self . _create_parser ( ) \n 
~~~ args = self . _parser . parse_args ( ) \n 
if args . verbose : \n 
~~~ logging . config . dictConfig ( DEBUG_CONFIG ) \n 
~~~ logging . config . dictConfig ( LOG_CONFIG ) \n 
~~ if not args . command : \n 
~~~ sys . stderr . write ( ) \n 
self . _parser . print_help ( ) \n 
~~ args_dict = vars ( args ) \n 
for key in [ , ] : \n 
~~~ if key in args_dict : \n 
~~~ args_dict [ key ] = args_dict [ key ] [ 0 ] \n 
~~ ~~ obj = controller . Controller ( args . config_dir , args . environment , \n 
args . command , \n 
args_dict . get ( ) , \n 
args . delay , args . max_tries , \n 
args . no_dependencies , \n 
args . no_removal , \n 
args . skip_consul , \n 
args . remove ) \n 
if obj . run ( ) : \n 
~~~ LOGGER . info ( ) \n 
sys . exit ( 2 ) \n 
~~ ~~ @ staticmethod \n 
def _create_parser ( ) : \n 
~~~ parser = argparse . ArgumentParser ( description = DESC ) \n 
parser . add_argument ( , , \n 
default = path . abspath ( ) , \n 
parser . add_argument ( , , required = True , \n 
parser . add_argument ( , , action = , type = int , \n 
, default = 5 ) \n 
parser . add_argument ( , , action = , \n 
default = 15 ) \n 
parser . add_argument ( , , action = ) \n 
sparser = parser . add_subparsers ( title = , dest = ) \n 
sparser . add_parser ( , help = ) \n 
s_parser = sparser . add_parser ( , \n 
s_parser . add_argument ( , nargs = 1 , \n 
sa_parser = sparser . add_parser ( , \n 
sa_parser . add_argument ( , nargs = 1 , \n 
return parser \n 
~~ ~~ def run ( ) : \n 
~~~ CLI ( ) . run ( ) \n 
~~~ os . environ . setdefault ( , ) \n 
from shop . search . indexes import ProductIndex as ProductIndexBase \n 
from haystack import indexes \n 
if settings . SHOP_TUTORIAL == : \n 
~~~ from myshop . models . simple . smartcard import SmartCard \n 
~~ elif settings . SHOP_TUTORIAL == : \n 
~~~ from myshop . models . i18n . smartcard import SmartCard \n 
~~~ from myshop . models . polymorphic . smartcard import SmartCard \n 
from myshop . models . polymorphic . smartphone import SmartPhoneModel \n 
~~ class ProductIndex ( ProductIndexBase ) : \n 
~~~ catalog_media = indexes . CharField ( stored = True , indexed = False , null = True ) \n 
search_media = indexes . CharField ( stored = True , indexed = False , null = True ) \n 
def prepare_catalog_media ( self , product ) : \n 
~~~ return self . render_html ( , product , ) \n 
~~ def prepare_search_media ( self , product ) : \n 
~~ ~~ class SmartCardIndex ( ProductIndex , indexes . Indexable ) : \n 
~~~ def get_model ( self ) : \n 
~~~ return SmartCard \n 
~~ ~~ if settings . SHOP_TUTORIAL == : \n 
~~~ class SmartPhoneIndex ( ProductIndex , indexes . Indexable ) : \n 
~~~ return SmartPhoneModel \n 
~~ ~~ myshop_search_index_classes = ( SmartCardIndex , SmartPhoneIndex ) \n 
~~~ myshop_search_index_classes = ( SmartCardIndex , ) \n 
CASCADE_PLUGINS = getattr ( settings , , \n 
( , , , , , , , , ) ) \n 
from django . utils import six \n 
from django . utils . encoding import python_2_unicode_compatible , force_text \n 
from django . utils . translation import ugettext_lazy as _ \n 
from polymorphic . manager import PolymorphicManager \n 
from polymorphic . models import PolymorphicModel \n 
from polymorphic . base import PolymorphicModelBase \n 
from . import deferred \n 
class BaseProductManager ( PolymorphicManager ) : \n 
def select_lookup ( self , search_term ) : \n 
filter_by_term = ( models . Q ( ( sf , search_term ) ) for sf in self . model . lookup_fields ) \n 
queryset = self . get_queryset ( ) . filter ( reduce ( operator . or_ , filter_by_term ) ) \n 
return queryset \n 
~~ def indexable ( self ) : \n 
queryset = self . get_queryset ( ) . filter ( active = True ) \n 
~~ ~~ class PolymorphicProductMetaclass ( PolymorphicModelBase ) : \n 
def __new__ ( cls , name , bases , attrs ) : \n 
~~~ Model = super ( PolymorphicProductMetaclass , cls ) . __new__ ( cls , name , bases , attrs ) \n 
if Model . _meta . abstract : \n 
~~~ return Model \n 
~~ for baseclass in bases : \n 
~~~ if not isinstance ( baseclass , cls ) : \n 
~~~ if issubclass ( baseclass . _materialized_model , Model ) : \n 
~~~ baseclass . _materialized_model = Model \n 
~~ elif not issubclass ( Model , baseclass . _materialized_model ) : \n 
. format ( name , Model , baseclass . _materialized_model ) ) \n 
~~ ~~ except ( AttributeError , TypeError ) : \n 
~~ deferred . ForeignKeyBuilder . process_pending_mappings ( Model , baseclass . __name__ ) \n 
~~ cls . perform_model_checks ( Model ) \n 
return Model \n 
def perform_model_checks ( cls , Model ) : \n 
if not isinstance ( Model . objects , BaseProductManager ) : \n 
raise NotImplementedError ( msg . format ( Model . __name__ ) ) \n 
~~ if not isinstance ( getattr ( Model , , None ) , ( list , tuple ) ) : \n 
~~~ Model ( ) . product_name \n 
~~ if not callable ( getattr ( Model , , None ) ) : \n 
raise NotImplementedError ( msg . format ( cls . __name__ ) ) \n 
~~ ~~ ~~ @ python_2_unicode_compatible \n 
class BaseProduct ( six . with_metaclass ( PolymorphicProductMetaclass , PolymorphicModel ) ) : \n 
active = models . BooleanField ( default = True , verbose_name = _ ( "Active" ) , \n 
class Meta : \n 
verbose_name = _ ( "Product" ) \n 
verbose_name_plural = _ ( "Products" ) \n 
~~~ return self . product_name \n 
~~ def product_type ( self ) : \n 
return force_text ( self . polymorphic_ctype ) \n 
def product_model ( self ) : \n 
return self . polymorphic_ctype . model \n 
~~ def get_absolute_url ( self ) : \n 
raise NotImplementedError ( msg . format ( self . __class__ . __name__ ) ) \n 
~~ def get_price ( self , request ) : \n 
~~ def get_availability ( self , request ) : \n 
~~ def is_in_cart ( self , cart , watched = False , ** kwargs ) : \n 
from . cart import CartItemModel \n 
cart_item_qs = CartItemModel . objects . filter ( cart = cart , product = self ) \n 
return cart_item_qs . first ( ) \n 
~~ ~~ ProductModel = deferred . MaterializedModel ( BaseProduct ) \n 
from django . db . models import Sum \n 
from django . utils . functional import cached_property \n 
from django_fsm import transition \n 
from shop . models . delivery import DeliveryModel \n 
class PartialDeliveryWorkflowMixin ( object ) : \n 
TRANSITION_TARGETS = { \n 
@ cached_property \n 
def unfulfilled_items ( self ) : \n 
~~~ unfulfilled_items = 0 \n 
for order_item in self . items . all ( ) : \n 
~~~ if not order_item . canceled : \n 
~~~ aggr = order_item . deliveryitem_set . aggregate ( delivered = Sum ( ) ) \n 
unfulfilled_items += order_item . quantity - ( aggr [ ] or 0 ) \n 
~~ ~~ return unfulfilled_items \n 
~~ def ready_for_delivery ( self ) : \n 
~~~ return self . is_fully_paid ( ) and self . unfulfilled_items > 0 \n 
~~ @ transition ( field = , source = [ , , ] , \n 
target = , conditions = [ ready_for_delivery ] , \n 
def pick_goods ( self , by = None ) : \n 
shipping_method = self . extra . get ( ) \n 
if shipping_method : \n 
~~~ DeliveryModel . objects . get_or_create ( order = self , shipping_method = shipping_method , fulfilled_at \n 
~~ ~~ @ transition ( field = , source = [ ] , target = , \n 
def pack_goods ( self , by = None ) : \n 
~~ @ transition ( field = , source = [ ] , target = , \n 
def ship_goods ( self , by = None ) : \n 
from botocore . compat import six \n 
advance_iterator = six . advance_iterator \n 
PY3 = six . PY3 \n 
queue = six . moves . queue \n 
shlex_quote = six . moves . shlex_quote \n 
StringIO = six . StringIO \n 
urlopen = six . moves . urllib . request . urlopen \n 
~~~ import zlib \n 
ZIP_COMPRESSION_MODE = zipfile . ZIP_DEFLATED \n 
~~~ ZIP_COMPRESSION_MODE = zipfile . ZIP_STORED \n 
~~ class BinaryStdout ( object ) : \n 
~~~ def __enter__ ( self ) : \n 
~~~ if sys . platform == "win32" : \n 
self . previous_mode = msvcrt . setmode ( sys . stdout . fileno ( ) , \n 
os . O_BINARY ) \n 
~~ return sys . stdout \n 
~~ def __exit__ ( self , type , value , traceback ) : \n 
msvcrt . setmode ( sys . stdout . fileno ( ) , self . previous_mode ) \n 
~~ ~~ ~~ if six . PY3 : \n 
~~~ import locale \n 
import urllib . parse as urlparse \n 
from urllib . error import URLError \n 
raw_input = input \n 
def get_stdout_text_writer ( ) : \n 
~~~ return sys . stdout \n 
~~ def compat_open ( filename , mode = , encoding = None ) : \n 
if not in mode : \n 
~~~ encoding = locale . getpreferredencoding ( ) \n 
~~ return open ( filename , mode , encoding = encoding ) \n 
import locale \n 
import urlparse \n 
from urllib2 import URLError \n 
raw_input = raw_input \n 
~~~ return codecs . getwriter ( locale . getpreferredencoding ( ) ) ( sys . stdout ) \n 
~~~ if not in mode : \n 
~~ return io . open ( filename , mode , encoding = encoding ) \n 
~~ ~~ import platform \n 
from awscli . compat import urlopen , URLError \n 
from awscli . customizations . codedeploy . systems import System , Ubuntu , Windows , RHEL \n 
from socket import timeout \n 
MAX_INSTANCE_NAME_LENGTH = 100 \n 
MAX_TAGS_PER_INSTANCE = 10 \n 
MAX_TAG_KEY_LENGTH = 128 \n 
MAX_TAG_VALUE_LENGTH = 256 \n 
INSTANCE_NAME_PATTERN = \n 
IAM_USER_ARN_PATTERN = \n 
INSTANCE_NAME_ARG = { \n 
: ( \n 
IAM_USER_ARN_ARG = { \n 
def validate_region ( params , parsed_globals ) : \n 
~~~ if parsed_globals . region : \n 
~~~ params . region = parsed_globals . region \n 
~~~ params . region = params . session . get_config_variable ( ) \n 
~~ if not params . region : \n 
~~ ~~ def validate_instance_name ( params ) : \n 
~~~ if params . instance_name : \n 
~~~ if not re . match ( INSTANCE_NAME_PATTERN , params . instance_name ) : \n 
~~ if params . instance_name . startswith ( ) : \n 
~~ if len ( params . instance_name ) > MAX_INSTANCE_NAME_LENGTH : \n 
. format ( \n 
MAX_INSTANCE_NAME_LENGTH \n 
~~ ~~ ~~ def validate_tags ( params ) : \n 
~~~ if params . tags : \n 
~~~ if len ( params . tags ) > MAX_TAGS_PER_INSTANCE : \n 
MAX_TAGS_PER_INSTANCE \n 
~~ for tag in params . tags : \n 
~~~ if len ( tag [ ] ) > MAX_TAG_KEY_LENGTH : \n 
MAX_TAG_KEY_LENGTH \n 
~~ if len ( tag [ ] ) > MAX_TAG_KEY_LENGTH : \n 
MAX_TAG_VALUE_LENGTH \n 
~~ ~~ ~~ ~~ def validate_iam_user_arn ( params ) : \n 
~~~ if params . iam_user_arn and not re . match ( IAM_USER_ARN_PATTERN , params . iam_user_arn ) : \n 
~~ ~~ def validate_instance ( params ) : \n 
~~~ if platform . system ( ) == : \n 
~~~ if in platform . linux_distribution ( ) [ 0 ] : \n 
~~~ params . system = Ubuntu ( params ) \n 
~~ if in platform . linux_distribution ( ) [ 0 ] : \n 
~~~ params . system = RHEL ( params ) \n 
~~ ~~ elif platform . system ( ) == : \n 
~~~ params . system = Windows ( params ) \n 
~~ if not in params : \n 
System . UNSUPPORTED_SYSTEM_MSG \n 
~~~ urlopen ( , timeout = 1 ) \n 
raise RuntimeError ( ) \n 
~~ except ( URLError , timeout ) : \n 
~~ ~~ def validate_s3_location ( params , arg_name ) : \n 
~~~ arg_name = arg_name . replace ( , ) \n 
if arg_name in params : \n 
~~~ s3_location = getattr ( params , arg_name ) \n 
if s3_location : \n 
~~~ matcher = re . match ( , str ( s3_location ) ) \n 
if matcher : \n 
~~~ params . bucket = matcher . group ( 1 ) \n 
params . key = matcher . group ( 2 ) \n 
arg_name . replace ( , ) \n 
~~ ~~ ~~ ~~ from awscli . customizations . emr import argumentschema \n 
from awscli . customizations . emr import emrutils \n 
from awscli . customizations . emr import helptext \n 
from awscli . customizations . emr import steputils \n 
from awscli . customizations . emr . command import Command \n 
class AddSteps ( Command ) : \n 
~~~ NAME = \n 
DESCRIPTION = ( ) \n 
ARG_TABLE = [ \n 
{ : , : True , \n 
: helptext . CLUSTER_ID \n 
: argumentschema . STEPS_SCHEMA , \n 
: helptext . STEPS \n 
def _run_main_command ( self , parsed_args , parsed_globals ) : \n 
~~~ parsed_steps = parsed_args . steps \n 
release_label = emrutils . get_release_label ( \n 
parsed_args . cluster_id , self . _session , self . region , \n 
parsed_globals . endpoint_url , parsed_globals . verify_ssl ) \n 
step_list = steputils . build_step_config_list ( \n 
parsed_step_list = parsed_steps , region = self . region , \n 
release_label = release_label ) \n 
parameters = { \n 
: parsed_args . cluster_id , \n 
: step_list \n 
emrutils . call_and_display_response ( self . _session , , \n 
parameters , parsed_globals ) \n 
~~ ~~ from awscli . customizations . gamelift . uploadbuild import UploadBuildCommand \n 
from awscli . customizations . gamelift . getlog import GetGameSessionLogCommand \n 
def register_gamelift_commands ( event_emitter ) : \n 
~~~ event_emitter . register ( , inject_commands ) \n 
~~ def inject_commands ( command_table , session , ** kwargs ) : \n 
~~~ command_table [ ] = UploadBuildCommand ( session ) \n 
command_table [ ] = GetGameSessionLogCommand ( session ) \n 
~~ from awscli . customizations import utils \n 
from awscli . customizations . commands import BasicCommand \n 
from awscli . customizations . s3 . subcommands import ListCommand , WebsiteCommand , CpCommand , MvCommand , RmCommand , SyncCommand , MbCommand , RbCommand \n 
from awscli . customizations . s3 . syncstrategy . register import register_sync_strategies \n 
def awscli_initialize ( cli ) : \n 
cli . register ( "building-command-table.main" , add_s3 ) \n 
cli . register ( , register_sync_strategies ) \n 
~~ def s3_plugin_initialize ( event_handlers ) : \n 
awscli_initialize ( event_handlers ) \n 
~~ def add_s3 ( command_table , session , ** kwargs ) : \n 
utils . rename_command ( command_table , , ) \n 
command_table [ ] = S3 ( session ) \n 
~~ class S3 ( BasicCommand ) : \n 
DESCRIPTION = BasicCommand . FROM_FILE ( ) \n 
SUBCOMMANDS = [ \n 
{ : , : ListCommand } , \n 
{ : , : WebsiteCommand } , \n 
{ : , : CpCommand } , \n 
{ : , : MvCommand } , \n 
{ : , : RmCommand } , \n 
{ : , : SyncCommand } , \n 
{ : , : MbCommand } , \n 
{ : , : RbCommand } \n 
def _run_main ( self , parsed_args , parsed_globals ) : \n 
~~~ if parsed_args . subcommand is None : \n 
import struct \n 
import colorama \n 
from awscli . compat import six \n 
def determine_terminal_width ( default_width = 80 ) : \n 
~~~ from termios import TIOCGWINSZ \n 
from fcntl import ioctl \n 
~~~ return default_width \n 
~~~ height , width = struct . unpack ( , ioctl ( sys . stdout , \n 
TIOCGWINSZ , * 8 ) ) [ 0 : 2 ] \n 
~~~ return width \n 
~~ ~~ def is_a_tty ( ) : \n 
~~~ return os . isatty ( sys . stdout . fileno ( ) ) \n 
~~ ~~ def center_text ( text , length = 80 , left_edge = , right_edge = , \n 
text_length = None ) : \n 
if text_length is None : \n 
~~~ text_length = len ( text ) \n 
~~ output = [ ] \n 
char_start = ( length // 2 ) - ( text_length // 2 ) - 1 \n 
output . append ( left_edge + * char_start + text ) \n 
length_so_far = len ( left_edge ) + char_start + text_length \n 
right_side_spaces = length - len ( right_edge ) - length_so_far \n 
output . append ( * right_side_spaces ) \n 
output . append ( right_edge ) \n 
final = . join ( output ) \n 
return final \n 
~~ def align_left ( text , length , left_edge = , right_edge = , text_length = None , \n 
left_padding = 2 ) : \n 
~~ computed_length = ( \n 
text_length + left_padding + len ( left_edge ) + len ( right_edge ) ) \n 
if length - computed_length >= 0 : \n 
~~~ padding = left_padding \n 
~~~ padding = 0 \n 
length_so_far = 0 \n 
output . append ( left_edge ) \n 
length_so_far += len ( left_edge ) \n 
output . append ( * padding ) \n 
length_so_far += padding \n 
output . append ( text ) \n 
length_so_far += text_length \n 
output . append ( * ( length - length_so_far - len ( right_edge ) ) ) \n 
return . join ( output ) \n 
~~ def convert_to_vertical_table ( sections ) : \n 
~~~ for i , section in enumerate ( sections ) : \n 
~~~ if len ( section . rows ) == 1 and section . headers : \n 
~~~ headers = section . headers \n 
new_section = Section ( ) \n 
new_section . title = section . title \n 
new_section . indent_level = section . indent_level \n 
for header , element in zip ( headers , section . rows [ 0 ] ) : \n 
~~~ new_section . add_row ( [ header , element ] ) \n 
~~ sections [ i ] = new_section \n 
~~ ~~ ~~ class IndentedStream ( object ) : \n 
~~~ def __init__ ( self , stream , indent_level , left_indent_char = , \n 
right_indent_char = ) : \n 
~~~ self . _stream = stream \n 
self . _indent_level = indent_level \n 
self . _left_indent_char = left_indent_char \n 
self . _right_indent_char = right_indent_char \n 
~~ def write ( self , text ) : \n 
~~~ self . _stream . write ( self . _left_indent_char * self . _indent_level ) \n 
if text . endswith ( ) : \n 
~~~ self . _stream . write ( text [ : - 1 ] ) \n 
self . _stream . write ( self . _right_indent_char * self . _indent_level ) \n 
self . _stream . write ( ) \n 
~~~ self . _stream . write ( text ) \n 
~~ ~~ def __getattr__ ( self , attr ) : \n 
~~~ return getattr ( self . _stream , attr ) \n 
~~ ~~ class Styler ( object ) : \n 
~~~ def style_title ( self , text ) : \n 
~~~ return text \n 
~~ def style_header_column ( self , text ) : \n 
~~ def style_row_element ( self , text ) : \n 
~~ def style_indentation_char ( self , text ) : \n 
~~ ~~ class ColorizedStyler ( Styler ) : \n 
~~~ colorama . init ( autoreset = True ) \n 
~~ def style_title ( self , text ) : \n 
~~~ return ( colorama . Style . BRIGHT + colorama . Fore . BLUE + \n 
text + colorama . Style . RESET_ALL ) \n 
~~~ return ( colorama . Style . DIM + colorama . Fore . YELLOW + \n 
~~ ~~ class MultiTable ( object ) : \n 
~~~ def __init__ ( self , terminal_width = None , initial_section = True , \n 
column_separator = , terminal = None , \n 
styler = None , auto_reformat = True ) : \n 
~~~ self . _auto_reformat = auto_reformat \n 
if initial_section : \n 
~~~ self . _current_section = Section ( ) \n 
self . _sections = [ self . _current_section ] \n 
~~~ self . _current_section = None \n 
self . _sections = [ ] \n 
~~ if styler is None : \n 
~~~ if is_a_tty ( ) : \n 
~~~ self . _styler = ColorizedStyler ( ) \n 
~~~ self . _styler = Styler ( ) \n 
~~~ self . _styler = styler \n 
~~ self . _rendering_index = 0 \n 
self . _column_separator = column_separator \n 
if terminal_width is None : \n 
~~~ self . _terminal_width = determine_terminal_width ( ) \n 
~~ ~~ def add_title ( self , title ) : \n 
~~~ self . _current_section . add_title ( title ) \n 
~~ def add_row_header ( self , headers ) : \n 
~~~ self . _current_section . add_header ( headers ) \n 
~~ def add_row ( self , row_elements ) : \n 
~~~ self . _current_section . add_row ( row_elements ) \n 
~~ def new_section ( self , title , indent_level = 0 ) : \n 
self . _sections . append ( self . _current_section ) \n 
self . _current_section . add_title ( title ) \n 
self . _current_section . indent_level = indent_level \n 
~~ def render ( self , stream ) : \n 
~~~ max_width = self . _calculate_max_width ( ) \n 
should_convert_table = self . _determine_conversion_needed ( max_width ) \n 
if should_convert_table : \n 
~~~ convert_to_vertical_table ( self . _sections ) \n 
max_width = self . _calculate_max_width ( ) \n 
~~ stream . write ( * max_width + ) \n 
for section in self . _sections : \n 
~~~ self . _render_section ( section , max_width , stream ) \n 
~~ ~~ def _determine_conversion_needed ( self , max_width ) : \n 
~~~ if max_width > self . _terminal_width : \n 
~~~ return self . _auto_reformat \n 
~~ ~~ def _calculate_max_width ( self ) : \n 
~~~ max_width = max ( s . total_width ( padding = 4 , with_border = True , \n 
outer_padding = s . indent_level ) \n 
for s in self . _sections ) \n 
return max_width \n 
~~ def _render_section ( self , section , max_width , stream ) : \n 
~~~ stream = IndentedStream ( stream , section . indent_level , \n 
self . _styler . style_indentation_char ( ) , \n 
self . _styler . style_indentation_char ( ) ) \n 
max_width -= ( section . indent_level * 2 ) \n 
self . _render_title ( section , max_width , stream ) \n 
self . _render_column_titles ( section , max_width , stream ) \n 
self . _render_rows ( section , max_width , stream ) \n 
~~ def _render_title ( self , section , max_width , stream ) : \n 
~~~ if section . title : \n 
~~~ title = self . _styler . style_title ( section . title ) \n 
stream . write ( center_text ( title , max_width , , , \n 
len ( section . title ) ) + ) \n 
if not section . headers and not section . rows : \n 
~~~ stream . write ( % ( * ( max_width - 2 ) ) + ) \n 
~~ ~~ ~~ def _render_column_titles ( self , section , max_width , stream ) : \n 
~~~ if not section . headers : \n 
~~ widths = section . calculate_column_widths ( padding = 4 , \n 
max_width = max_width ) \n 
current = \n 
first = True \n 
for width , header in zip ( widths , section . headers ) : \n 
~~~ stylized_header = self . _styler . style_header_column ( header ) \n 
if first : \n 
~~~ left_edge = \n 
first = False \n 
~~ current += center_text ( text = stylized_header , length = width , \n 
left_edge = left_edge , right_edge = , \n 
text_length = len ( header ) ) \n 
length_so_far += width \n 
~~ self . _write_line_break ( stream , widths ) \n 
stream . write ( current + ) \n 
~~ def _write_line_break ( self , stream , widths ) : \n 
~~~ parts = [ ] \n 
for width in widths : \n 
~~~ if first : \n 
~~~ parts . append ( % ( * ( width - 2 ) ) ) \n 
~~~ parts . append ( % ( * ( width - 1 ) ) ) \n 
~~ ~~ parts . append ( ) \n 
stream . write ( . join ( parts ) ) \n 
~~ def _render_rows ( self , section , max_width , stream ) : \n 
~~~ if not section . rows : \n 
if not widths : \n 
for row in section . rows : \n 
~~~ current = \n 
for width , element in zip ( widths , row ) : \n 
~~ stylized = self . _styler . style_row_element ( element ) \n 
current += align_left ( text = stylized , length = width , \n 
left_edge = left_edge , \n 
right_edge = self . _column_separator , \n 
text_length = len ( element ) ) \n 
~~ stream . write ( current + ) \n 
~~ ~~ class Section ( object ) : \n 
~~~ self . title = \n 
self . headers = [ ] \n 
self . rows = [ ] \n 
self . indent_level = 0 \n 
self . _num_cols = None \n 
self . _max_widths = [ ] \n 
( self . title , self . headers , self . indent_level , len ( self . rows ) ) ) \n 
~~ def calculate_column_widths ( self , padding = 0 , max_width = None ) : \n 
~~~ unscaled_widths = [ w + padding for w in self . _max_widths ] \n 
if max_width is None : \n 
~~~ return unscaled_widths \n 
~~ if not unscaled_widths : \n 
~~~ scale_factor = max_width / float ( sum ( unscaled_widths ) ) \n 
scaled = [ int ( round ( scale_factor * w ) ) for w in unscaled_widths ] \n 
off_by = sum ( scaled ) - max_width \n 
while off_by != 0 : \n 
~~~ iter_order = range ( len ( scaled ) ) \n 
if off_by < 0 : \n 
~~~ iter_order = reversed ( iter_order ) \n 
~~ for i in iter_order : \n 
~~~ if off_by > 0 : \n 
~~~ scaled [ i ] -= 1 \n 
off_by -= 1 \n 
~~~ scaled [ i ] += 1 \n 
off_by += 1 \n 
~~ if off_by == 0 : \n 
~~ ~~ ~~ return scaled \n 
~~ ~~ def total_width ( self , padding = 0 , with_border = False , outer_padding = 0 ) : \n 
~~~ total = 0 \n 
border_padding = 2 \n 
for w in self . calculate_column_widths ( ) : \n 
~~~ total += w + padding \n 
~~ if with_border : \n 
~~~ total += border_padding \n 
~~ total += outer_padding + outer_padding \n 
return max ( len ( self . title ) + border_padding + outer_padding + \n 
outer_padding , total ) \n 
~~ def add_title ( self , title ) : \n 
~~~ self . title = title \n 
~~ def add_header ( self , headers ) : \n 
~~~ self . _update_max_widths ( headers ) \n 
if self . _num_cols is None : \n 
~~~ self . _num_cols = len ( headers ) \n 
~~ self . headers = self . _format_headers ( headers ) \n 
~~ def _format_headers ( self , headers ) : \n 
~~~ return headers \n 
~~ def add_row ( self , row ) : \n 
~~~ if self . _num_cols is None : \n 
~~~ self . _num_cols = len ( row ) \n 
~~ if len ( row ) != self . _num_cols : \n 
~~ row = self . _format_row ( row ) \n 
self . rows . append ( row ) \n 
self . _update_max_widths ( row ) \n 
~~ def _format_row ( self , row ) : \n 
~~~ return [ six . text_type ( r ) for r in row ] \n 
~~ def _update_max_widths ( self , row ) : \n 
~~~ if not self . _max_widths : \n 
~~~ self . _max_widths = [ len ( el ) for el in row ] \n 
~~~ for i , el in enumerate ( row ) : \n 
~~~ self . _max_widths [ i ] = max ( len ( el ) , self . _max_widths [ i ] ) \n 
~~ ~~ ~~ ~~ import json \n 
from awscli . testutils import BaseAWSCommandParamsTest , FileCreator \n 
class TestAddModel ( BaseAWSCommandParamsTest ) : \n 
~~~ prefix = \n 
~~~ super ( TestAddModel , self ) . setUp ( ) \n 
self . files = FileCreator ( ) \n 
self . customer_data_root = self . files . rootdir \n 
self . data_loader = self . driver . session . get_component ( ) \n 
self . data_loader . CUSTOMER_DATA_PATH = self . customer_data_root \n 
self . service_definition = { \n 
"version" : "2.0" , \n 
"metadata" : { \n 
"apiVersion" : , \n 
"endpointPrefix" : , \n 
"operations" : { } , \n 
"shapes" : { } \n 
~~~ super ( TestAddModel , self ) . tearDown ( ) \n 
self . files . remove_all ( ) \n 
~~ def test_add_model ( self ) : \n 
~~~ cmdline = self . prefix + % json . dumps ( \n 
self . service_definition , separators = ( , ) ) \n 
self . run_cmd ( cmdline ) \n 
os . path . exists ( os . path . join ( \n 
self . customer_data_root , , , \n 
) ) ) \n 
~~ def test_add_model_with_service_name ( self ) : \n 
cmdline += \n 
from awscli . testutils import BaseAWSCommandParamsTest , FileCreator , mock \n 
class TestUploadBuild ( BaseAWSCommandParamsTest ) : \n 
~~~ super ( TestUploadBuild , self ) . setUp ( ) \n 
~~~ super ( TestUploadBuild , self ) . tearDown ( ) \n 
~~ def test_upload_build ( self ) : \n 
~~~ self . files . create_file ( , ) \n 
cmdline = self . prefix \n 
cmdline += % self . files . rootdir \n 
self . parsed_responses = [ \n 
{ : { : } } , \n 
{ : { \n 
: } , \n 
: } } , \n 
{ } \n 
stdout , stderr , rc = self . run_cmd ( cmdline , expected_rc = 0 ) \n 
self . assertEqual ( len ( self . operations_called ) , 3 ) \n 
self . assertEqual ( self . operations_called [ 0 ] [ 0 ] . name , ) \n 
self . operations_called [ 0 ] [ 1 ] , \n 
{ : , : } \n 
self . operations_called [ 1 ] [ 0 ] . name , ) \n 
self . operations_called [ 1 ] [ 1 ] , { : } ) \n 
self . assertEqual ( self . operations_called [ 2 ] [ 0 ] . name , ) \n 
self . operations_called [ 2 ] [ 1 ] , \n 
{ : mock . ANY , : , : } \n 
self . assertIn ( \n 
% self . files . rootdir , \n 
stdout ) \n 
self . assertIn ( , stdout ) \n 
~~ def test_upload_build_with_empty_directory ( self ) : \n 
~~~ cmdline = self . prefix \n 
stdout , stderr , rc = self . run_cmd ( cmdline , expected_rc = 255 ) \n 
stderr ) \n 
~~ def test_upload_build_with_nonexistent_directory ( self ) : \n 
~~~ dir_not_exist = os . path . join ( self . files . rootdir , ) \n 
cmdline += % dir_not_exist \n 
% dir_not_exist , \n 
~~ def test_upload_build_with_nonprovided_directory ( self ) : \n 
cmdline += % \'""\' \n 
% \'""\' , \n 
~~ ~~ from awscli . testutils import BaseAWSCommandParamsTest \n 
import awscli . clidriver \n 
class TestAddPermission ( BaseAWSCommandParamsTest ) : \n 
queue_url = \n 
def test_all_param ( self ) : \n 
cmdline += % self . queue_url \n 
result = { : self . queue_url , \n 
self . assert_params_for_cmd ( cmdline , result ) \n 
~~ def test_multiple_accounts ( self ) : \n 
: [ , ] , \n 
~~ def test_multiple_actions ( self ) : \n 
from argparse import Namespace \n 
from awscli . customizations . codedeploy . install import Install \n 
from awscli . customizations . codedeploy . systems import Ubuntu , Windows , RHEL , System \n 
from awscli . testutils import unittest \n 
from mock import MagicMock , patch , mock_open \n 
class TestInstall ( unittest . TestCase ) : \n 
~~~ self . region = \n 
self . config_file = \n 
self . installer = \n 
self . bucket = . format ( self . region ) \n 
self . key = . format ( self . installer ) \n 
self . agent_installer = . format ( self . bucket , self . key ) \n 
self . system_patcher = patch ( ) \n 
self . system = self . system_patcher . start ( ) \n 
self . system . return_value = \n 
self . linux_distribution_patcher = patch ( ) \n 
self . linux_distribution = self . linux_distribution_patcher . start ( ) \n 
self . linux_distribution . return_value = ( , , ) \n 
self . urlopen_patcher = patch ( \n 
self . urlopen = self . urlopen_patcher . start ( ) \n 
self . urlopen . side_effect = timeout ( ) \n 
self . geteuid_patcher = patch ( , create = True ) \n 
self . geteuid = self . geteuid_patcher . start ( ) \n 
self . geteuid . return_value = 0 \n 
self . isfile_patcher = patch ( ) \n 
self . isfile = self . isfile_patcher . start ( ) \n 
self . isfile . return_value = False \n 
self . makedirs_patcher = patch ( ) \n 
self . makedirs = self . makedirs_patcher . start ( ) \n 
self . copyfile_patcher = patch ( ) \n 
self . copyfile = self . copyfile_patcher . start ( ) \n 
self . open_patcher = patch ( \n 
mock_open ( ) , create = True \n 
self . open = self . open_patcher . start ( ) \n 
self . args = Namespace ( ) \n 
self . args . override_config = False \n 
self . args . config_file = self . config_file \n 
self . args . agent_installer = None \n 
self . globals = Namespace ( ) \n 
self . globals . region = self . region \n 
self . body = \n 
self . reader = MagicMock ( ) \n 
self . reader . read . return_value = self . body \n 
self . s3 = MagicMock ( ) \n 
self . s3 . get_object . return_value = { : self . reader } \n 
self . session = MagicMock ( ) \n 
self . session . create_client . return_value = self . s3 \n 
self . install = Install ( self . session ) \n 
~~~ self . system_patcher . stop ( ) \n 
self . linux_distribution_patcher . stop ( ) \n 
self . urlopen_patcher . stop ( ) \n 
self . geteuid_patcher . stop ( ) \n 
self . isfile_patcher . stop ( ) \n 
self . makedirs_patcher . stop ( ) \n 
self . copyfile_patcher . stop ( ) \n 
self . open_patcher . stop ( ) \n 
~~ def test_install_throws_on_invalid_region ( self ) : \n 
~~~ self . globals . region = None \n 
self . session . get_config_variable . return_value = None \n 
with self . assertRaisesRegexp ( RuntimeError , ) : \n 
~~~ self . install . _run_main ( self . args , self . globals ) \n 
~~ ~~ def test_install_throws_on_unsupported_system ( self ) : \n 
~~~ self . system . return_value = \n 
with self . assertRaisesRegexp ( \n 
RuntimeError , System . UNSUPPORTED_SYSTEM_MSG ) : \n 
~~ ~~ def test_install_throws_on_ec2_instance ( self ) : \n 
~~~ self . urlopen . side_effect = None \n 
RuntimeError , ) : \n 
~~ self . assertIn ( , self . args ) \n 
self . assertTrue ( isinstance ( self . args . system , Ubuntu ) ) \n 
~~ def test_install_throws_on_non_administrator ( self ) : \n 
~~~ self . geteuid . return_value = 1 \n 
~~ ~~ def test_install_throws_on_no_override_config ( self ) : \n 
~~~ self . isfile . return_value = True \n 
RuntimeError , \n 
~~ ~~ def test_install_throws_on_invalid_agent_installer ( self ) : \n 
~~~ self . args . agent_installer = \n 
ValueError , \n 
~~ ~~ @ patch . object ( Ubuntu , ) \n 
def test_install_with_agent_installer ( self , install ) : \n 
~~~ self . args . agent_installer = self . agent_installer \n 
self . install . _run_main ( self . args , self . globals ) \n 
self . assertIn ( , self . args ) \n 
self . assertEqual ( self . bucket , self . args . bucket ) \n 
self . assertEqual ( self . key , self . args . key ) \n 
self . assertEqual ( self . installer , self . args . installer ) \n 
install . assert_called_with ( self . args ) \n 
~~ @ patch . object ( Ubuntu , ) \n 
def test_install_for_ubuntu ( self , install ) : \n 
self . assertEquals ( self . bucket , self . args . bucket ) \n 
self . assertEquals ( , self . args . key ) \n 
self . assertEquals ( , self . args . installer ) \n 
self . makedirs . assert_called_with ( ) \n 
self . copyfile . assset_called_with ( \n 
~~ @ patch . object ( Windows , ) \n 
@ patch . object ( Windows , ) \n 
def test_install_for_windows ( self , validate_administrator , install ) : \n 
validate_administrator . assert_called_with ( ) \n 
~~ from awscli . testutils import unittest \n 
from awscli . customizations . emr . createdefaultroles import assume_role_policy \n 
class TestDefaultRoles ( unittest . TestCase ) : \n 
~~~ service_principal = "ec2.amazonaws.com" \n 
expected_result = { \n 
"Version" : "2008-10-17" , \n 
"Statement" : [ \n 
"Sid" : "" , \n 
"Effect" : "Allow" , \n 
"Principal" : { "Service" : "ec2.amazonaws.com" } , \n 
"Action" : "sts:AssumeRole" \n 
def test_assume_role_policy ( self ) : \n 
~~~ result = assume_role_policy ( self . service_principal ) \n 
self . assertEqual ( result , self . expected_result ) \n 
from awscli . customizations . s3 . filegenerator import FileStat \n 
from awscli . customizations . s3 . syncstrategy . exacttimestamps import ExactTimestampsSync \n 
class TestExactTimestampsSync ( unittest . TestCase ) : \n 
~~~ self . sync_strategy = ExactTimestampsSync ( ) \n 
~~ def test_compare_exact_timestamps_dest_older ( self ) : \n 
time_src = datetime . datetime . now ( ) \n 
time_dst = time_src - datetime . timedelta ( days = 1 ) \n 
src_file = FileStat ( src = , dest = , \n 
compare_key = , size = 10 , \n 
last_update = time_src , src_type = , \n 
dest_type = , operation_name = ) \n 
dst_file = FileStat ( src = , dest = , \n 
last_update = time_dst , src_type = , \n 
should_sync = self . sync_strategy . determine_should_sync ( \n 
src_file , dst_file ) \n 
self . assertTrue ( should_sync ) \n 
~~ def test_compare_exact_timestamps_src_older ( self ) : \n 
time_src = datetime . datetime . now ( ) - datetime . timedelta ( days = 1 ) \n 
time_dst = datetime . datetime . now ( ) \n 
~~ def test_compare_exact_timestamps_same_age_same_size ( self ) : \n 
time_both = datetime . datetime . now ( ) \n 
last_update = time_both , src_type = , \n 
self . assertFalse ( should_sync ) \n 
~~ def test_compare_exact_timestamps_same_age_diff_size ( self ) : \n 
compare_key = , size = 20 , \n 
~~ def test_compare_exact_timestamps_diff_age_not_download ( self ) : \n 
~~ from awscli . compat import six \n 
from botocore . model import DenormalizedStructureBuilder \n 
from awscli . customizations . generatecliskeleton import GenerateCliSkeletonArgument \n 
class TestGenerateCliSkeleton ( unittest . TestCase ) : \n 
~~~ self . session = mock . Mock ( ) \n 
self . service_operation = mock . Mock ( ) \n 
self . input_shape = { \n 
shape = DenormalizedStructureBuilder ( ) . with_members ( \n 
self . input_shape ) . build_model ( ) \n 
self . operation_model = mock . Mock ( input_shape = shape ) \n 
self . argument = GenerateCliSkeletonArgument ( self . session , self . operation_model ) \n 
~~ def test_register_argument_action ( self ) : \n 
~~~ register_args = self . session . register . call_args_list \n 
self . assertEqual ( register_args [ 0 ] [ 0 ] [ 0 ] , ) \n 
self . assertEqual ( register_args [ 0 ] [ 0 ] [ 1 ] , \n 
self . argument . generate_json_skeleton ) \n 
~~ def test_generate_json_skeleton ( self ) : \n 
~~~ parsed_args = mock . Mock ( ) \n 
parsed_args . generate_cli_skeleton = True \n 
with mock . patch ( , six . StringIO ( ) ) as mock_stdout : \n 
~~~ rc = self . argument . generate_json_skeleton ( \n 
service_operation = self . service_operation , call_parameters = None , \n 
parsed_args = parsed_args , parsed_globals = None \n 
self . assertEqual ( self . ref_json_output , mock_stdout . getvalue ( ) ) \n 
self . assertEqual ( rc , 0 ) \n 
~~ ~~ def test_no_generate_json_skeleton ( self ) : \n 
parsed_args . generate_cli_skeleton = False \n 
self . assertEqual ( , mock_stdout . getvalue ( ) ) \n 
self . assertEqual ( rc , None ) \n 
~~ ~~ def test_generate_json_skeleton_no_input_shape ( self ) : \n 
self . argument = GenerateCliSkeletonArgument ( \n 
self . session , mock . Mock ( input_shape = None ) ) \n 
~~ ~~ ~~ import json \n 
from awscli . testutils import unittest , FileCreator \n 
from awscli . topictags import TopicTagDB \n 
class TestTopicTagDB ( unittest . TestCase ) : \n 
~~~ self . topic_tag_db = TopicTagDB ( ) \n 
self . file_creator = FileCreator ( ) \n 
~~~ self . file_creator . remove_all ( ) \n 
~~ ~~ class TestTopicTagDBGeneral ( TestTopicTagDB ) : \n 
~~~ def test_valid_tags ( self ) : \n 
~~~ self . assertCountEqual ( \n 
self . topic_tag_db . valid_tags , \n 
[ , , , , \n 
~~ def test_topic_dir ( self ) : \n 
~~~ self . topic_tag_db = TopicTagDB ( topic_dir = ) \n 
self . assertEqual ( self . topic_tag_db . topic_dir , ) \n 
self . topic_tag_db . topic_dir = \n 
~~ def test_index_file ( self ) : \n 
~~~ self . topic_tag_db = TopicTagDB ( index_file = ) \n 
self . assertEqual ( self . topic_tag_db . index_file , ) \n 
self . topic_tag_db . index_file = \n 
~~ def test_get_all_topic_names ( self ) : \n 
~~~ tag_dict = { \n 
reference_topic_list = [ , ] \n 
self . topic_tag_db = TopicTagDB ( tag_dict ) \n 
self . assertCountEqual ( self . topic_tag_db . get_all_topic_names ( ) , \n 
reference_topic_list ) \n 
~~ def test_get_all_topic_source_files ( self ) : \n 
~~~ source_files = [ ] \n 
topic_dir = self . file_creator . rootdir \n 
self . topic_tag_db = TopicTagDB ( topic_dir = topic_dir ) \n 
for i in range ( 5 ) : \n 
~~~ topic_name = + str ( i ) \n 
source_files . append ( self . file_creator . create_file ( topic_name , ) ) \n 
~~ self . assertCountEqual ( \n 
self . topic_tag_db . get_all_topic_src_files ( ) , \n 
source_files \n 
~~ def test_get_all_topic_source_files_ignore_index ( self ) : \n 
~~~ topic_filename = \n 
index_filename = \n 
source_files = [ ] \n 
source_files . append ( self . file_creator . create_file ( topic_filename , ) ) \n 
index_file = self . file_creator . create_file ( index_filename , ) \n 
self . topic_tag_db = TopicTagDB ( index_file = index_file , \n 
topic_dir = topic_dir ) \n 
self . assertCountEqual ( \n 
~~ def test_get_all_topic_source_files_ignore_hidden ( self ) : \n 
hidden_filename = + topic_filename \n 
self . file_creator . create_file ( hidden_filename , ) \n 
~~ def test_get_tag_value_all_tags ( self ) : \n 
~~~ topic_name = \n 
tag_dict = { \n 
topic_name : { \n 
value = self . topic_tag_db . get_tag_value ( topic_name , ) \n 
self . assertEqual ( value , [ ] ) \n 
value = self . topic_tag_db . get_tag_value ( topic_name , \n 
~~ def test_get_tag_multi_value ( self ) : \n 
: [ , ] \n 
self . assertEqual ( value , [ , ] ) \n 
~~ def test_get_tag_topic_no_exists ( self ) : \n 
value = self . topic_tag_db . get_tag_value ( , ) \n 
self . assertEqual ( value , None ) \n 
~~ def test_get_tag_no_exist_tag ( self ) : \n 
~~ def test_get_tag_no_exist_use_default ( self ) : \n 
value = self . topic_tag_db . get_tag_value ( , , [ ] ) \n 
~~ def test_get_tag_single_value ( self ) : \n 
value = self . topic_tag_db . get_tag_single_value ( , ) \n 
self . assertEqual ( value , ) \n 
~~ def test_get_tag_single_value_exception ( self ) : \n 
with self . assertRaises ( ValueError ) : \n 
~~~ self . topic_tag_db . get_tag_single_value ( , ) \n 
~~ ~~ def test_get_tag_single_value_no_exists ( self ) : \n 
value = self . topic_tag_db . get_tag_single_value ( \n 
~~ def test_load_and_save_json_index ( self ) : \n 
json_index = self . file_creator . create_file ( , ) \n 
tag_json = json . dumps ( tag_dict , indent = 4 , sort_keys = True ) \n 
with open ( json_index , ) as f : \n 
~~~ f . write ( tag_json ) \n 
~~ self . topic_tag_db = TopicTagDB ( index_file = json_index ) \n 
self . topic_tag_db . load_json_index ( ) \n 
saved_json_index = self . file_creator . create_file ( , ) \n 
self . topic_tag_db . index_file = saved_json_index \n 
self . topic_tag_db . save_to_json_index ( ) \n 
with open ( saved_json_index , ) as f : \n 
~~~ self . assertEqual ( f . read ( ) , tag_json ) \n 
~~ ~~ ~~ class TestTopicTagDBQuery ( TestTopicTagDB ) : \n 
~~~ def test_query_all_tags_single_topic ( self ) : \n 
query_dict = self . topic_tag_db . query ( ) \n 
self . assertEqual ( query_dict , \n 
{ : [ ] } ) \n 
~~ def test_query_tag_multi_values ( self ) : \n 
{ : [ ] , : [ ] } ) \n 
~~ def test_query_multiple_topics ( self ) : \n 
~~ def test_query_multiple_topics_with_multi_values ( self ) : \n 
{ : [ ] , : [ ] , \n 
: [ ] , : [ ] } ) \n 
~~ def test_query_multiple_topics_with_overlap_values ( self ) : \n 
query_dict , { : [ ] , : [ ] , \n 
: [ , ] } ) \n 
~~ def test_query_with_limit_single_value ( self ) : \n 
query_dict = self . topic_tag_db . query ( , [ ] ) \n 
self . assertCountEqual ( query_dict , \n 
{ : [ , ] } ) \n 
~~ def test_query_with_limit_multi_value ( self ) : \n 
query_dict = self . topic_tag_db . query ( , [ , ] ) \n 
{ : [ ] , \n 
~~ def topic_query_with_non_existant_tag ( self ) : \n 
self . assertEqual ( query_dict , { } ) \n 
~~ ~~ class TestTopicDBScan ( TestTopicTagDB ) : \n 
~~~ def create_topic_src_file ( self , topic_name , tags ) : \n 
content = . join ( tags ) \n 
topic_name = topic_name + \n 
topic_filepath = self . file_creator . create_file ( topic_name , content ) \n 
return topic_filepath \n 
~~ def assert_json_index ( self , file_paths , reference_tag_dict ) : \n 
self . topic_tag_db = TopicTagDB ( index_file = json_index ) \n 
self . topic_tag_db . scan ( file_paths ) \n 
~~~ saved_index = json . loads ( f . read ( ) ) \n 
self . assertEqual ( saved_index , reference_tag_dict ) \n 
~~ ~~ def test_scan_all_valid_tags ( self ) : \n 
~~~ tags = [ \n 
topic_name = \n 
reference_tag_dict = { \n 
topic_filepath = self . create_topic_src_file ( topic_name , tags ) \n 
self . assert_json_index ( [ topic_filepath ] , reference_tag_dict ) \n 
~~ def test_scan_invalid_tag ( self ) : \n 
~~~ self . topic_tag_db . scan ( [ topic_filepath ] ) \n 
~~ ~~ def test_scan_no_tags ( self ) : \n 
~~~ tags = [ ] \n 
topic_name : { } \n 
~~ def test_scan_tags_with_multi_values ( self ) : \n 
~~ def test_scan_tags_with_single_and_multi_values ( self ) : \n 
~~ def test_scan_tags_with_multi_duplicate_values ( self ) : \n 
~~ def test_scan_tags_with_multi_values_extra_space ( self ) : \n 
~~ def test_scan_tags_with_multi_values_no_space ( self ) : \n 
~~ def test_scan_tags_with_multi_preserve_space ( self ) : \n 
~~ def test_scan_multiple_files ( self ) : \n 
~~~ topic_base = \n 
reference_tag_dict = { } \n 
topic_files = [ ] \n 
~~~ topic_name = topic_base + + str ( i ) \n 
tags = [ \n 
% topic_name , \n 
reference_tag_dict [ topic_name ] = { \n 
: [ % topic_name ] , \n 
topic_files . append ( self . create_topic_src_file ( topic_name , tags ) ) \n 
~~ self . assert_json_index ( topic_files , reference_tag_dict ) \n 
~~ ~~ import mock \n 
from awsshell . app import AWSShell \n 
from awsshell . config import Config \n 
from awsshell . utils import build_config_file_path \n 
class ConfigTest ( unittest . TestCase ) : \n 
~~~ def test_config_off ( self ) : \n 
~~~ os . remove ( build_config_file_path ( ) ) \n 
~~ except OSError : \n 
~~ self . aws_shell = AWSShell ( None , mock . Mock ( ) , mock . Mock ( ) ) \n 
self . aws_shell . model_completer . match_fuzzy = False \n 
self . aws_shell . enable_vi_bindings = False \n 
self . aws_shell . show_completion_columns = False \n 
self . aws_shell . show_help = False \n 
self . aws_shell . theme = \n 
self . aws_shell . save_config ( ) \n 
self . aws_shell . load_config ( ) \n 
assert self . aws_shell . model_completer . match_fuzzy == False \n 
assert self . aws_shell . enable_vi_bindings == False \n 
assert self . aws_shell . show_completion_columns == False \n 
assert self . aws_shell . show_help == False \n 
assert self . aws_shell . theme == \n 
~~ def test_config_on ( self ) : \n 
~~~ self . aws_shell = AWSShell ( None , mock . Mock ( ) , mock . Mock ( ) ) \n 
self . aws_shell . model_completer . match_fuzzy = True \n 
self . aws_shell . enable_vi_bindings = True \n 
self . aws_shell . show_completion_columns = True \n 
self . aws_shell . show_help = True \n 
assert self . aws_shell . config_section . as_bool ( ) == True \n 
assert self . aws_shell . config_section . as_bool ( \n 
) == True \n 
assert self . aws_shell . config_section [ ] == \n 
~~ ~~ import sys , os , os . path \n 
import unittest , doctest \n 
~~ from datetime import datetime , time , timedelta , tzinfo \n 
~~~ sys . path . insert ( 0 , os . path . abspath ( os . path . join ( os . pardir , os . pardir ) ) ) \n 
~~ import pytz \n 
from pytz import reference \n 
from pytz . tzfile import _byte_string \n 
from pytz . tzinfo import DstTzInfo , StaticTzInfo \n 
EXPECTED_VERSION = \n 
EXPECTED_OLSON_VERSION = \n 
fmt = \n 
NOTIME = timedelta ( 0 ) \n 
UTC = pytz . timezone ( ) \n 
GMT = pytz . timezone ( ) \n 
assert isinstance ( GMT , StaticTzInfo ) , \n 
def prettydt ( dt ) : \n 
if dt . utcoffset ( ) >= timedelta ( 0 ) : \n 
~~~ offset = % ( dt . utcoffset ( ) , ) \n 
~~~ offset = % ( - 1 * dt . utcoffset ( ) , ) \n 
~~ return % ( \n 
dt . year , dt . month , dt . day , \n 
dt . hour , dt . minute , dt . second , \n 
dt . tzname ( ) , offset ) \n 
~~~ unicode \n 
~~~ unicode = str \n 
~~ class BasicTest ( unittest . TestCase ) : \n 
~~~ def testVersion ( self ) : \n 
~~~ self . assertEqual ( EXPECTED_VERSION , pytz . __version__ , \n 
% ( EXPECTED_VERSION , pytz . __version__ ) ) \n 
self . assertEqual ( EXPECTED_OLSON_VERSION , pytz . OLSON_VERSION , \n 
% ( EXPECTED_OLSON_VERSION , pytz . OLSON_VERSION ) ) \n 
~~ def testGMT ( self ) : \n 
~~~ now = datetime . now ( tz = GMT ) \n 
self . assertTrue ( now . utcoffset ( ) == NOTIME ) \n 
self . assertTrue ( now . dst ( ) == NOTIME ) \n 
self . assertTrue ( now . timetuple ( ) == now . utctimetuple ( ) ) \n 
self . assertTrue ( now == now . replace ( tzinfo = UTC ) ) \n 
~~ def testReferenceUTC ( self ) : \n 
~~~ now = datetime . now ( tz = UTC ) \n 
~~ def testUnknownOffsets ( self ) : \n 
~~~ dst_tz = pytz . timezone ( ) \n 
self . assertTrue ( dst_tz . utcoffset ( None ) is None ) \n 
self . assertTrue ( dst_tz . dst ( None ) is None ) \n 
self . assertEqual ( dst_tz . tzname ( None ) , ) \n 
~~ def clearCache ( self ) : \n 
~~~ pytz . _tzinfo_cache . clear ( ) \n 
~~ def testUnicodeTimezone ( self ) : \n 
~~~ self . clearCache ( ) \n 
eastern = pytz . timezone ( unicode ( ) ) \n 
self . assertTrue ( eastern is pytz . timezone ( ) ) \n 
self . clearCache ( ) \n 
eastern = pytz . timezone ( ) \n 
self . assertTrue ( eastern is pytz . timezone ( unicode ( ) ) ) \n 
~~ ~~ class PicklingTest ( unittest . TestCase ) : \n 
~~~ def _roundtrip_tzinfo ( self , tz ) : \n 
~~~ p = pickle . dumps ( tz ) \n 
unpickled_tz = pickle . loads ( p ) \n 
self . assertTrue ( tz is unpickled_tz , % tz . zone ) \n 
~~ def _roundtrip_datetime ( self , dt ) : \n 
~~~ tz = dt . tzinfo \n 
p = pickle . dumps ( dt ) \n 
unpickled_dt = pickle . loads ( p ) \n 
unpickled_tz = unpickled_dt . tzinfo \n 
~~ def testDst ( self ) : \n 
~~~ tz = pytz . timezone ( ) \n 
dt = datetime ( 2004 , 2 , 1 , 0 , 0 , 0 ) \n 
for localized_tz in tz . _tzinfos . values ( ) : \n 
~~~ self . _roundtrip_tzinfo ( localized_tz ) \n 
self . _roundtrip_datetime ( dt . replace ( tzinfo = localized_tz ) ) \n 
~~ ~~ def testRoundtrip ( self ) : \n 
~~~ dt = datetime ( 2004 , 2 , 1 , 0 , 0 , 0 ) \n 
for zone in pytz . all_timezones : \n 
~~~ tz = pytz . timezone ( zone ) \n 
self . _roundtrip_tzinfo ( tz ) \n 
~~ ~~ def testDatabaseFixes ( self ) : \n 
p = pickle . dumps ( tz ) \n 
tzname = tz . _tzname \n 
hacked_p = p . replace ( _byte_string ( tzname ) , \n 
_byte_string ( * len ( tzname ) ) ) \n 
self . assertNotEqual ( p , hacked_p ) \n 
unpickled_tz = pickle . loads ( hacked_p ) \n 
self . assertTrue ( tz is unpickled_tz ) \n 
new_utcoffset = tz . _utcoffset . seconds + 42 \n 
old_pickle_pattern = pickle . dumps ( tz . _utcoffset . seconds ) [ 3 : - 1 ] \n 
new_pickle_pattern = pickle . dumps ( new_utcoffset ) [ 3 : - 1 ] \n 
hacked_p = p . replace ( old_pickle_pattern , new_pickle_pattern ) \n 
self . assertEqual ( unpickled_tz . _utcoffset . seconds , new_utcoffset ) \n 
self . assertTrue ( tz is not unpickled_tz ) \n 
~~ def testOldPickles ( self ) : \n 
~~~ east1 = pickle . loads ( _byte_string ( \n 
"cpytz\\n_p\\np1\\n(S\'US/Eastern\'\\np2\\nI-18000\\n" \n 
"I0\\nS\'EST\'\\np3\\ntRp4\\n." \n 
east2 = pytz . timezone ( ) . localize ( \n 
datetime ( 2006 , 1 , 1 ) ) . tzinfo \n 
self . assertTrue ( east1 is east2 ) \n 
pap1 = pickle . loads ( _byte_string ( \n 
"cpytz\\n_p\\np1\\n(S\'America/Port_minus_au_minus_Prince\'" \n 
"\\np2\\nI-17340\\nI0\\nS\'PPMT\'\\np3\\ntRp4\\n." ) ) \n 
pap2 = pytz . timezone ( ) . localize ( \n 
datetime ( 1910 , 1 , 1 ) ) . tzinfo \n 
self . assertTrue ( pap1 is pap2 ) \n 
gmt1 = pickle . loads ( _byte_string ( \n 
"cpytz\\n_p\\np1\\n(S\'Etc/GMT_plus_10\'\\np2\\ntRp3\\n." ) ) \n 
gmt2 = pytz . timezone ( ) \n 
self . assertTrue ( gmt1 is gmt2 ) \n 
~~ ~~ class USEasternDSTStartTestCase ( unittest . TestCase ) : \n 
~~~ tzinfo = pytz . timezone ( ) \n 
transition_time = datetime ( 2002 , 4 , 7 , 7 , 0 , 0 , tzinfo = UTC ) \n 
instant = timedelta ( seconds = 1 ) \n 
before = { \n 
: timedelta ( hours = - 5 ) , \n 
: timedelta ( hours = 0 ) , \n 
after = { \n 
: timedelta ( hours = - 4 ) , \n 
: timedelta ( hours = 1 ) , \n 
def _test_tzname ( self , utc_dt , wanted ) : \n 
~~~ tzname = wanted [ ] \n 
dt = utc_dt . astimezone ( self . tzinfo ) \n 
self . assertEqual ( dt . tzname ( ) , tzname , \n 
tzname , str ( utc_dt ) , dt . tzname ( ) \n 
~~ def _test_utcoffset ( self , utc_dt , wanted ) : \n 
~~~ utcoffset = wanted [ ] \n 
dt . utcoffset ( ) , wanted [ ] , \n 
utcoffset , utc_dt , dt . utcoffset ( ) \n 
~~ def _test_dst ( self , utc_dt , wanted ) : \n 
~~~ dst = wanted [ ] \n 
self . assertEqual ( dt . dst ( ) , dst , \n 
dst , utc_dt , dt . dst ( ) \n 
~~ def test_arithmetic ( self ) : \n 
~~~ utc_dt = self . transition_time \n 
for days in range ( - 420 , 720 , 20 ) : \n 
~~~ delta = timedelta ( days = days ) \n 
dt2 = dt + delta \n 
dt2 = dt2 - delta \n 
self . assertEqual ( dt , dt2 ) \n 
utc_plus_delta = ( utc_dt + delta ) . astimezone ( self . tzinfo ) \n 
local_plus_delta = self . tzinfo . normalize ( dt + delta ) \n 
prettydt ( utc_plus_delta ) , \n 
prettydt ( local_plus_delta ) , \n 
days , \n 
~~ ~~ def _test_all ( self , utc_dt , wanted ) : \n 
~~~ self . _test_utcoffset ( utc_dt , wanted ) \n 
self . _test_tzname ( utc_dt , wanted ) \n 
self . _test_dst ( utc_dt , wanted ) \n 
~~ def testDayBefore ( self ) : \n 
~~~ self . _test_all ( \n 
self . transition_time - timedelta ( days = 1 ) , self . before \n 
~~ def testTwoHoursBefore ( self ) : \n 
self . transition_time - timedelta ( hours = 2 ) , self . before \n 
~~ def testHourBefore ( self ) : \n 
self . transition_time - timedelta ( hours = 1 ) , self . before \n 
~~ def testInstantBefore ( self ) : \n 
self . transition_time - self . instant , self . before \n 
~~ def testTransition ( self ) : \n 
self . transition_time , self . after \n 
~~ def testInstantAfter ( self ) : \n 
self . transition_time + self . instant , self . after \n 
~~ def testHourAfter ( self ) : \n 
self . transition_time + timedelta ( hours = 1 ) , self . after \n 
~~ def testTwoHoursAfter ( self ) : \n 
~~ def testDayAfter ( self ) : \n 
self . transition_time + timedelta ( days = 1 ) , self . after \n 
~~ ~~ class USEasternDSTEndTestCase ( USEasternDSTStartTestCase ) : \n 
transition_time = datetime ( 2002 , 10 , 27 , 6 , 0 , 0 , tzinfo = UTC ) \n 
~~ class USEasternEPTStartTestCase ( USEasternDSTStartTestCase ) : \n 
~~~ transition_time = datetime ( 1945 , 8 , 14 , 23 , 0 , 0 , tzinfo = UTC ) \n 
~~ class USEasternEPTEndTestCase ( USEasternDSTStartTestCase ) : \n 
~~~ transition_time = datetime ( 1945 , 9 , 30 , 6 , 0 , 0 , tzinfo = UTC ) \n 
~~ class WarsawWMTEndTestCase ( USEasternDSTStartTestCase ) : \n 
transition_time = datetime ( 1915 , 8 , 4 , 22 , 36 , 0 , tzinfo = UTC ) \n 
: timedelta ( hours = 1 , minutes = 24 ) , \n 
: timedelta ( 0 ) , \n 
~~ class VilniusWMTEndTestCase ( USEasternDSTStartTestCase ) : \n 
instant = timedelta ( seconds = 31 ) \n 
transition_time = datetime ( 1916 , 12 , 31 , 22 , 36 , 00 , tzinfo = UTC ) \n 
~~ class VilniusCESTStartTestCase ( USEasternDSTStartTestCase ) : \n 
transition_time = datetime ( 1941 , 6 , 23 , 21 , 00 , 00 , tzinfo = UTC ) \n 
: timedelta ( hours = 3 ) , \n 
: timedelta ( hours = 2 ) , \n 
~~ class LondonHistoryStartTestCase ( USEasternDSTStartTestCase ) : \n 
transition_time = datetime ( 1916 , 5 , 21 , 2 , 00 , 00 , tzinfo = UTC ) \n 
~~ class LondonHistoryEndTestCase ( USEasternDSTStartTestCase ) : \n 
transition_time = datetime ( 2037 , 10 , 25 , 1 , 0 , 0 , tzinfo = UTC ) \n 
~~ class NoumeaHistoryStartTestCase ( USEasternDSTStartTestCase ) : \n 
transition_time = datetime ( 1912 , 1 , 12 , 12 , 54 , 12 , tzinfo = UTC ) \n 
: timedelta ( hours = 11 , minutes = 6 ) , \n 
: timedelta ( hours = 11 ) , \n 
~~ class NoumeaDSTEndTestCase ( USEasternDSTStartTestCase ) : \n 
transition_time = datetime ( 1997 , 3 , 1 , 15 , 00 , 00 , tzinfo = UTC ) \n 
: timedelta ( hours = 12 ) , \n 
~~ class NoumeaNoMoreDSTTestCase ( NoumeaDSTEndTestCase ) : \n 
~~~ transition_time = ( \n 
NoumeaDSTEndTestCase . transition_time + timedelta ( days = 365 * 10 ) ) \n 
before = NoumeaDSTEndTestCase . after \n 
after = NoumeaDSTEndTestCase . after \n 
~~ class TahitiTestCase ( USEasternDSTStartTestCase ) : \n 
transition_time = datetime ( 1912 , 10 , 1 , 9 , 58 , 16 , tzinfo = UTC ) \n 
: timedelta ( hours = - 9 , minutes = - 58 ) , \n 
: timedelta ( hours = - 10 ) , \n 
~~ class SamoaInternationalDateLineChange ( USEasternDSTStartTestCase ) : \n 
transition_time = datetime ( 2011 , 12 , 30 , 10 , 0 , 0 , tzinfo = UTC ) \n 
: timedelta ( hours = 14 ) , \n 
~~ class ReferenceUSEasternDSTStartTestCase ( USEasternDSTStartTestCase ) : \n 
~~~ tzinfo = reference . Eastern \n 
def test_arithmetic ( self ) : \n 
~~ ~~ class ReferenceUSEasternDSTEndTestCase ( USEasternDSTEndTestCase ) : \n 
def testHourBefore ( self ) : \n 
self . transition_time - timedelta ( hours = 1 ) , self . after \n 
self . transition_time - timedelta ( seconds = 1 ) , self . after \n 
~~ ~~ class LocalTestCase ( unittest . TestCase ) : \n 
~~~ def testLocalize ( self ) : \n 
~~~ loc_tz = pytz . timezone ( ) \n 
loc_time = loc_tz . localize ( datetime ( 1930 , 5 , 10 , 0 , 0 , 0 ) ) \n 
self . assertEqual ( loc_time . strftime ( ) , ) \n 
loc_time = loc_tz . localize ( datetime ( 1930 , 5 , 20 , 0 , 0 , 0 ) ) \n 
loc_time = loc_tz . localize ( datetime ( 1940 , 5 , 10 , 0 , 0 , 0 ) ) \n 
loc_time = loc_tz . localize ( datetime ( 1940 , 5 , 20 , 0 , 0 , 0 ) ) \n 
loc_time = loc_tz . localize ( datetime ( 2004 , 2 , 1 , 0 , 0 , 0 ) ) \n 
loc_time = loc_tz . localize ( datetime ( 2004 , 4 , 1 , 0 , 0 , 0 ) ) \n 
tz = pytz . timezone ( ) \n 
loc_time = loc_tz . localize ( datetime ( 1943 , 3 , 29 , 1 , 59 , 59 ) ) \n 
loc_tz = pytz . timezone ( ) \n 
loc_time = loc_tz . localize ( datetime ( 1918 , 10 , 27 , 1 , 59 , 59 ) , is_dst = 1 ) \n 
loc_time = loc_tz . localize ( datetime ( 1918 , 10 , 27 , 1 , 59 , 59 ) , is_dst = 0 ) \n 
self . assertRaises ( pytz . AmbiguousTimeError , \n 
loc_tz . localize , datetime ( 1918 , 10 , 27 , 1 , 59 , 59 ) , is_dst = None \n 
loc_time = loc_tz . localize ( datetime ( 1918 , 3 , 31 , 2 , 0 , 0 ) , is_dst = 0 ) \n 
loc_time = loc_tz . localize ( datetime ( 1918 , 3 , 31 , 2 , 0 , 0 ) , is_dst = 1 ) \n 
self . assertRaises ( pytz . NonExistentTimeError , \n 
loc_tz . localize , datetime ( 1918 , 3 , 31 , 2 , 0 , 0 ) , is_dst = None \n 
loc_time = loc_tz . localize ( datetime ( 1942 , 2 , 9 , 3 , 0 , 0 ) ) \n 
loc_time = loc_tz . localize ( datetime ( 1945 , 8 , 14 , 19 , 0 , 0 ) ) \n 
loc_time = loc_tz . localize ( datetime ( 1945 , 9 , 30 , 1 , 0 , 0 ) , is_dst = 1 ) \n 
loc_time = loc_tz . localize ( datetime ( 1945 , 9 , 30 , 1 , 0 , 0 ) , is_dst = 0 ) \n 
for zonename , ambiguous_naive , expected in [ \n 
( , datetime ( 1915 , 8 , 4 , 23 , 59 , 59 ) , \n 
[ , \n 
] ) , \n 
( , datetime ( 2014 , 10 , 26 , 1 , 30 ) , \n 
] ) ] : \n 
~~~ loc_tz = pytz . timezone ( zonename ) \n 
loc_tz . localize , ambiguous_naive , is_dst = None \n 
for dst in [ True , timedelta ( 1 ) , False , timedelta ( 0 ) ] : \n 
~~~ loc_time = loc_tz . localize ( ambiguous_naive , is_dst = dst ) \n 
self . assertEqual ( loc_time . strftime ( fmt ) , expected [ not dst ] ) \n 
~~ ~~ ~~ def testNormalize ( self ) : \n 
dt = datetime ( 2004 , 4 , 4 , 7 , 0 , 0 , tzinfo = UTC ) . astimezone ( tz ) \n 
dt2 = dt - timedelta ( minutes = 10 ) \n 
dt2 . strftime ( ) , \n 
dt2 = tz . normalize ( dt2 ) \n 
~~ def testPartialMinuteOffsets ( self ) : \n 
loc_dt = utc_dt . astimezone ( tz ) \n 
loc_dt . strftime ( ) , \n 
utc_dt = loc_dt . astimezone ( UTC ) \n 
utc_dt . strftime ( ) , \n 
~~ def no_testCreateLocaltime ( self ) : \n 
dt = datetime ( 2004 , 10 , 31 , 2 , 0 , 0 , tzinfo = tz ) \n 
dt . strftime ( fmt ) , \n 
~~ ~~ class CommonTimezonesTestCase ( unittest . TestCase ) : \n 
~~~ def test_bratislava ( self ) : \n 
~~~ self . assertTrue ( in pytz . common_timezones ) \n 
self . assertTrue ( in pytz . common_timezones_set ) \n 
~~ def test_us_eastern ( self ) : \n 
~~ def test_belfast ( self ) : \n 
~~~ self . assertTrue ( in pytz . all_timezones_set ) \n 
self . assertFalse ( in pytz . common_timezones ) \n 
self . assertFalse ( in pytz . common_timezones_set ) \n 
~~ ~~ class BaseTzInfoTestCase : \n 
def test_expectedclass ( self ) : \n 
~~~ self . assertTrue ( isinstance ( self . tz , self . tz_class ) ) \n 
~~ def test_fromutc ( self ) : \n 
~~~ dt1 = datetime ( 2011 , 10 , 31 ) \n 
dt2 = self . tz . localize ( dt1 ) \n 
for dt in [ dt1 , dt2 ] : \n 
~~~ loc_dt = self . tz . fromutc ( dt ) \n 
loc_dt2 = pytz . utc . localize ( dt1 ) . astimezone ( self . tz ) \n 
self . assertEqual ( loc_dt , loc_dt2 ) \n 
~~ new_tz = pytz . timezone ( ) \n 
self . assertTrue ( self . tz is not new_tz ) \n 
dt3 = new_tz . localize ( dt1 ) \n 
self . assertRaises ( ValueError , self . tz . fromutc , dt3 ) \n 
~~ def test_normalize ( self ) : \n 
~~~ other_tz = pytz . timezone ( ) \n 
self . assertTrue ( self . tz is not other_tz ) \n 
dt = datetime ( 2012 , 3 , 26 , 12 , 0 ) \n 
other_dt = other_tz . localize ( dt ) \n 
local_dt = self . tz . normalize ( other_dt ) \n 
self . assertTrue ( local_dt . tzinfo is not other_dt . tzinfo ) \n 
self . assertNotEqual ( \n 
local_dt . replace ( tzinfo = None ) , other_dt . replace ( tzinfo = None ) ) \n 
~~ def test_astimezone ( self ) : \n 
local_dt = other_dt . astimezone ( self . tz ) \n 
~~ ~~ class OptimizedUTCTestCase ( unittest . TestCase , BaseTzInfoTestCase ) : \n 
~~~ tz = pytz . utc \n 
tz_class = tz . __class__ \n 
~~ class LegacyUTCTestCase ( unittest . TestCase , BaseTzInfoTestCase ) : \n 
tz_class = StaticTzInfo \n 
~~ class StaticTzInfoTestCase ( unittest . TestCase , BaseTzInfoTestCase ) : \n 
~~ class DstTzInfoTestCase ( unittest . TestCase , BaseTzInfoTestCase ) : \n 
tz_class = DstTzInfo \n 
~~ def test_suite ( ) : \n 
~~~ suite = unittest . TestSuite ( ) \n 
suite . addTest ( doctest . DocTestSuite ( ) ) \n 
import test_tzinfo \n 
suite . addTest ( unittest . defaultTestLoader . loadTestsFromModule ( test_tzinfo ) ) \n 
return suite \n 
~~ from . import InstanceResource , ListResource \n 
from . util import normalize_dates , parse_date \n 
class Media ( InstanceResource ) : \n 
def delete ( self ) : \n 
return self . parent . delete_instance ( self . name ) \n 
~~ ~~ class MediaList ( ListResource ) : \n 
~~~ name = "Media" \n 
key = "media_list" \n 
instance = Media \n 
def __call__ ( self , message_sid ) : \n 
~~~ base_uri = "%s/Messages/%s" % ( self . base_uri , message_sid ) \n 
return MediaList ( base_uri , self . auth , self . timeout ) \n 
~~ def __init__ ( self , * args , ** kwargs ) : \n 
~~~ super ( MediaList , self ) . __init__ ( * args , ** kwargs ) \n 
~~ @ normalize_dates \n 
def list ( self , before = None , after = None , date_created = None , ** kw ) : \n 
kw [ "DateCreated<" ] = before \n 
kw [ "DateCreated>" ] = after \n 
kw [ "DateCreated" ] = parse_date ( date_created ) \n 
return self . get_instances ( kw ) \n 
~~ def delete ( self , sid ) : \n 
return self . delete_instance ( sid ) \n 
~~ ~~ from . import InstanceResource , ListResource \n 
class Transcription ( InstanceResource ) : \n 
~~~ def delete ( self ) : \n 
return self . parent . delete ( self . name ) \n 
~~ ~~ class Transcriptions ( ListResource ) : \n 
~~~ name = "Transcriptions" \n 
instance = Transcription \n 
def list ( self , ** kwargs ) : \n 
return self . get_instances ( kwargs ) \n 
from led import LED \n 
import RPi . GPIO as GPIO \n 
from device import Device \n 
from sbs . tools import Tools \n 
BLINK_DURATION = 0.25 \n 
class LegacyLED ( LED ) : \n 
~~~ def __init__ ( self , pin , name ) : \n 
~~~ super ( LegacyLED , self ) . __init__ ( pin , name ) \n 
GPIO . setup ( self . pin , GPIO . OUT ) \n 
~~ def blink ( self ) : \n 
~~~ GPIO . output ( self . pin , GPIO . HIGH ) \n 
time . sleep ( BLINK_DURATION ) \n 
GPIO . output ( self . pin , GPIO . LOW ) \n 
~~ def on ( self ) : \n 
~~ def off ( self ) : \n 
~~~ GPIO . output ( self . pin , GPIO . LOW ) \n 
~~ ~~ from jsonrpclib import Server \n 
from . import FSQPushError , FSQRemoteTriggerError , constants as _c \n 
def push ( item , remote_addr , trg_queue , protocol = ) : \n 
if protocol == : \n 
~~~ server = Server ( remote_addr , encoding = _c . FSQ_CHARSET ) \n 
return server . enqueue ( item . id , trg_queue , item . item . read ( ) ) \n 
~~~ raise FSQPushError ( e ) \n 
~~ ~~ raise ValueError ( . format ( protocol ) ) \n 
~~ def remote_trigger_pull ( remote_addr , trg_queue , ignore_listener = False , \n 
protocol = ) : \n 
return server . trigger_pull ( queue = trg_queue , \n 
ignore_listener = ignore_listener , \n 
trigger = _c . FSQ_TRIGGER ) \n 
~~~ raise FSQRemoteTriggerError ( e ) \n 
~~ import getopt \n 
import fsq \n 
_PROG = "fsq-scan" \n 
_VERBOSE = False \n 
def shout ( msg , f = sys . stderr ) : \n 
f . flush ( ) \n 
~~ def barf ( msg , exit = None , f = sys . stderr ) : \n 
exit = fsq . const ( ) if exit is None else exit \n 
shout ( msg , f ) \n 
sys . exit ( exit ) \n 
~~ def usage ( asked_for = 0 ) : \n 
exit = fsq . const ( ) if asked_for else fsq . const ( ) \n 
f = sys . stdout if asked_for else sys . stderr \n 
shout ( . format ( \n 
os . path . basename ( _PROG ) ) , f ) \n 
if asked_for : \n 
~~~ shout ( . format ( os . path . basename ( _PROG ) ) , f ) \n 
shout ( , f ) \n 
~~ sys . exit ( exit ) \n 
~~ def main ( argv ) : \n 
~~~ global _PROG , _VERBOSE \n 
set_env = True \n 
no_open = False \n 
ignore_down = False \n 
empty_ok = False \n 
no_done = False \n 
host = False \n 
hosts = [ ] \n 
max_rate = None \n 
_PROG = argv [ 0 ] \n 
~~~ opts , args = getopt . getopt ( argv [ 1 : ] , , ( , \n 
, , , ) ) \n 
~~ except getopt . GetoptError , e : \n 
~~~ barf ( . format ( if 1 < len ( e . opt ) else , \n 
e . opt ) ) \n 
~~~ for flag , opt in opts : \n 
~~~ if == flag or == flag : \n 
~~~ _VERBOSE = True \n 
~~ elif == flag or == flag : \n 
~~~ set_env = True \n 
~~~ set_env = False \n 
~~~ no_open = True \n 
~~~ ignore_down = True \n 
~~~ fsq . set_const ( , True ) \n 
~~~ fsq . set_const ( , False ) \n 
~~~ empty_ok = True \n 
~~~ no_done = True \n 
~~~ host = True \n 
~~~ hosts . append ( opt ) \n 
host = True \n 
~~~ fsq . set_const ( , opt ) \n 
~~~ max_rate = int ( opt ) \n 
~~~ raise fsq . FSQCoerceError \n 
~~ ~~ elif == flag or == flag : \n 
~~~ usage ( 1 ) \n 
~~ ~~ ~~ except ( fsq . FSQEnvError , fsq . FSQCoerceError , ) : \n 
~~~ barf ( . format ( flag ) ) \n 
~~ num_required = 1 if empty_ok else 2 \n 
if num_required > len ( args ) : \n 
~~~ usage ( ) \n 
~~ exec_args = tuple ( args [ 1 : ] ) \n 
fsq . fork_exec_items ( args [ 0 ] , ignore_down = ignore_down , host = host , \n 
no_open = no_open , hosts = hosts if hosts else None , \n 
no_done = no_done , set_env = set_env , exec_args = exec_args , \n 
verbose = _VERBOSE , empty_ok = empty_ok , max_rate = max_rate ) \n 
~~~ main ( sys . argv ) \n 
~~ from django . conf . urls import patterns \n 
from ip . views import ban_ip \n 
urlpatterns = patterns ( , \n 
( , ban_ip ) , \n 
from django . contrib . auth . models import User \n 
from datetime import datetime as dt \n 
DATA_STATUS = ( \n 
( , ) \n 
class Contest ( models . Model ) : \n 
~~~ title = models . CharField ( max_length = 255 ) \n 
start_date = models . DateTimeField ( ) \n 
end_date = models . DateTimeField ( ) \n 
vote_frequency = models . IntegerField ( ) \n 
rules = models . TextField ( ) \n 
def get_days_left ( self ) : \n 
~~~ today = dt . today ( ) \n 
left = self . end_date - today \n 
if left . days < 0 : \n 
~~ return left . days \n 
~~ def get_days_till_start ( self ) : \n 
~~~ till = self . start_date - dt . today ( ) \n 
return till . days + 1 \n 
~~ def has_ended ( self ) : \n 
~~~ return dt . today ( ) >= self . end_date \n 
~~ def has_started ( self ) : \n 
~~~ return dt . today ( ) >= self . start_date \n 
~~ def get_next_vote_date ( self , user ) : \n 
~~~ votes = user . vote_set . order_by ( ) \n 
increment = datetime . timedelta ( days = self . vote_frequency ) \n 
if len ( votes ) : \n 
~~~ last_vote_date = votes [ 0 ] . timestamp \n 
~~~ last_vote_date = dt . today ( ) \n 
~~ next_vote_date = last_vote_date + increment \n 
return next_vote_date \n 
~~ def user_can_vote ( self , user ) : \n 
if votes . count ( ) > 0 : \n 
~~~ next_date = self . get_next_vote_date ( user ) \n 
if dt . today ( ) < next_date and dt . today ( ) < self . end_date : \n 
~~~ return self . title \n 
~~ ~~ class Entry ( models . Model ) : \n 
~~~ def get_image_path ( instance , filename ) : \n 
~~~ fsplit = filename . split ( ) \n 
extra = 1 \n 
test_path = os . path . join ( settings . MEDIA_ROOT , , str ( instance . id ) , fsplit [ 0 ] while os . path . exists ( test_path ) : \n 
~~~ extra += 1 \n 
test_path = os . path . join ( settings . MEDIA_ROOT , , str ( instance . id ) , fsplit [ ~~ path = os . path . join ( , str ( instance . id ) , fsplit [ 0 ] + + str ( extra ) + + return path \n 
~~ title = models . CharField ( max_length = 255 ) \n 
description = models . TextField ( ) \n 
short_description = models . CharField ( max_length = 120 ) \n 
nominator = models . CharField ( max_length = 255 ) \n 
nominator_link = models . CharField ( max_length = 255 ) \n 
links = models . CharField ( max_length = 400 , null = True , blank = True ) \n 
is_visible = models . BooleanField ( default = True ) \n 
data_owner = models . CharField ( max_length = 255 ) \n 
rejected_reason = models . CharField ( max_length = 255 , null = True , blank = True ) \n 
comments = models . TextField ( null = True , blank = True ) \n 
contest = models . ForeignKey ( Contest ) \n 
vote_count = models . IntegerField ( default = 0 ) \n 
def __str__ ( self ) : \n 
~~ def get_place ( self ) : \n 
~~~ entries = Entry . objects . filter ( contest = self . contest ) . order_by ( ) \n 
for i , entry in enumerate ( entries ) : \n 
~~~ if entry == self : return i + 1 \n 
~~ ~~ ~~ class Vote ( models . Model ) : \n 
~~~ user = models . ForeignKey ( User ) \n 
timestamp = models . DateTimeField ( auto_now = True ) \n 
entry = models . ForeignKey ( Entry ) \n 
~~ from django import forms \n 
class EntryForm ( forms . Form ) : \n 
~~ from jsbuild . attrdict import AttrDict \n 
from time import strftime \n 
class Manifest ( AttrDict ) : \n 
~~~ super ( AttrDict , self ) . __init__ ( * args , ** kwargs ) \n 
self . _buffer_ = None \n 
self . _parent_ = None \n 
if not self . __contains__ ( ) : \n 
~~~ self [ ] = { } \n 
~~ self [ ] [ ] = int ( strftime ( "%Y%m%d%H%M" ) ) \n 
~~~ item = super ( Manifest , self ) . __getitem__ ( name ) \n 
if isinstance ( item , Manifest ) and not item . _parent_ : \n 
~~~ item . _parent_ = self \n 
~~ elif isinstance ( item , str ) : \n 
~~~ root = self \n 
while root . _parent_ : root = root . _parent_ \n 
item = item % root . _dict_ \n 
~~ return item \n 
__all__ = [ "register" ] \n 
_exithandlers = [ ] \n 
def _run_exitfuncs ( ) : \n 
exc_info = None \n 
while _exithandlers : \n 
~~~ func , targs , kargs = _exithandlers . pop ( ) \n 
~~~ func ( * targs , ** kargs ) \n 
~~ except SystemExit : \n 
~~~ exc_info = sys . exc_info ( ) \n 
~~~ import traceback \n 
traceback . print_exc ( ) \n 
exc_info = sys . exc_info ( ) \n 
~~ ~~ if exc_info is not None : \n 
~~~ raise exc_info [ 0 ] , exc_info [ 1 ] , exc_info [ 2 ] \n 
~~ ~~ def register ( func , * targs , ** kargs ) : \n 
_exithandlers . append ( ( func , targs , kargs ) ) \n 
~~ if hasattr ( sys , "exitfunc" ) : \n 
~~~ register ( sys . exitfunc ) \n 
~~ sys . exitfunc = _run_exitfuncs \n 
~~~ def x1 ( ) : \n 
~~ def x2 ( n ) : \n 
~~ def x3 ( n , kwd = None ) : \n 
~~ register ( x1 ) \n 
register ( x2 , 12 ) \n 
register ( x3 , 5 , "bar" ) \n 
import copy as _copy \n 
import math as _math \n 
import numbers as _numbers \n 
~~~ from collections import namedtuple as _namedtuple \n 
DecimalTuple = _namedtuple ( , ) \n 
~~~ DecimalTuple = lambda * args : args \n 
~~ ROUND_DOWN = \n 
ROUND_HALF_UP = \n 
ROUND_HALF_EVEN = \n 
ROUND_CEILING = \n 
ROUND_FLOOR = \n 
ROUND_UP = \n 
ROUND_HALF_DOWN = \n 
ROUND_05UP = \n 
class DecimalException ( ArithmeticError ) : \n 
def handle ( self , context , * args ) : \n 
~~ ~~ class Clamped ( DecimalException ) : \n 
~~ class InvalidOperation ( DecimalException ) : \n 
~~~ if args : \n 
~~~ ans = _dec_from_triple ( args [ 0 ] . _sign , args [ 0 ] . _int , , True ) \n 
return ans . _fix_nan ( context ) \n 
~~ return _NaN \n 
~~ ~~ class ConversionSyntax ( InvalidOperation ) : \n 
~~~ return _NaN \n 
~~ ~~ class DivisionByZero ( DecimalException , ZeroDivisionError ) : \n 
def handle ( self , context , sign , * args ) : \n 
~~~ return _SignedInfinity [ sign ] \n 
~~ ~~ class DivisionImpossible ( InvalidOperation ) : \n 
~~ ~~ class DivisionUndefined ( InvalidOperation , ZeroDivisionError ) : \n 
~~ ~~ class Inexact ( DecimalException ) : \n 
~~ class InvalidContext ( InvalidOperation ) : \n 
~~ ~~ class Rounded ( DecimalException ) : \n 
~~ class Subnormal ( DecimalException ) : \n 
~~ class Overflow ( Inexact , Rounded ) : \n 
~~~ if context . rounding in ( ROUND_HALF_UP , ROUND_HALF_EVEN , \n 
ROUND_HALF_DOWN , ROUND_UP ) : \n 
~~ if sign == 0 : \n 
~~~ if context . rounding == ROUND_CEILING : \n 
~~ return _dec_from_triple ( sign , * context . prec , \n 
context . Emax - context . prec + 1 ) \n 
~~ if sign == 1 : \n 
~~~ if context . rounding == ROUND_FLOOR : \n 
~~ ~~ ~~ class Underflow ( Inexact , Rounded , Subnormal ) : \n 
~~ _signals = [ Clamped , DivisionByZero , Inexact , Overflow , Rounded , \n 
Underflow , InvalidOperation , Subnormal ] \n 
_condition_map = { ConversionSyntax : InvalidOperation , \n 
DivisionImpossible : InvalidOperation , \n 
DivisionUndefined : InvalidOperation , \n 
InvalidContext : InvalidOperation } \n 
class MockThreading ( object ) : \n 
~~~ def local ( self , sys = sys ) : \n 
~~~ return sys . modules [ __name__ ] \n 
~~ ~~ threading = MockThreading ( ) \n 
del sys , MockThreading \n 
~~~ from java . lang import Object \n 
from java . math import BigDecimal \n 
from org . python . core import Py \n 
~~~ threading . local \n 
~~~ if hasattr ( threading . currentThread ( ) , ) : \n 
~~~ del threading . currentThread ( ) . __decimal_context__ \n 
~~ def setcontext ( context ) : \n 
if context in ( DefaultContext , BasicContext , ExtendedContext ) : \n 
~~~ context = context . copy ( ) \n 
context . clear_flags ( ) \n 
~~ threading . currentThread ( ) . __decimal_context__ = context \n 
~~ def getcontext ( ) : \n 
~~~ return threading . currentThread ( ) . __decimal_context__ \n 
~~~ context = Context ( ) \n 
threading . currentThread ( ) . __decimal_context__ = context \n 
return context \n 
~~~ local = threading . local ( ) \n 
if hasattr ( local , ) : \n 
~~~ del local . __decimal_context__ \n 
~~ def getcontext ( _local = local ) : \n 
~~~ return _local . __decimal_context__ \n 
_local . __decimal_context__ = context \n 
~~ ~~ def setcontext ( context , _local = local ) : \n 
~~ _local . __decimal_context__ = context \n 
~~ del threading , local \n 
~~ def localcontext ( ctx = None ) : \n 
if ctx is None : ctx = getcontext ( ) \n 
return _ContextManager ( ctx ) \n 
~~ class Decimal ( object ) : \n 
__slots__ = ( , , , ) \n 
def __new__ ( cls , value = "0" , context = None ) : \n 
self = object . __new__ ( cls ) \n 
if isinstance ( value , basestring ) : \n 
~~~ m = _parser ( value . strip ( ) ) \n 
if m is None : \n 
~~~ if context is None : \n 
~~~ context = getcontext ( ) \n 
~~ return context . _raise_error ( ConversionSyntax , \n 
~~ if m . group ( ) == "-" : \n 
~~~ self . _sign = 1 \n 
~~~ self . _sign = 0 \n 
~~ intpart = m . group ( ) \n 
if intpart is not None : \n 
~~~ fracpart = m . group ( ) or \n 
exp = int ( m . group ( ) or ) \n 
self . _int = str ( int ( intpart + fracpart ) ) \n 
self . _exp = exp - len ( fracpart ) \n 
self . _is_special = False \n 
~~~ diag = m . group ( ) \n 
if diag is not None : \n 
~~~ self . _int = str ( int ( diag or ) ) . lstrip ( ) \n 
if m . group ( ) : \n 
~~~ self . _exp = \n 
~~~ self . _int = \n 
self . _exp = \n 
~~ self . _is_special = True \n 
~~ return self \n 
~~ if isinstance ( value , ( int , long ) ) : \n 
~~~ if value >= 0 : \n 
~~ self . _exp = 0 \n 
self . _int = str ( abs ( value ) ) \n 
~~ if isinstance ( value , Decimal ) : \n 
~~~ self . _exp = value . _exp \n 
self . _sign = value . _sign \n 
self . _int = value . _int \n 
self . _is_special = value . _is_special \n 
~~ if isinstance ( value , _WorkRep ) : \n 
~~~ self . _sign = value . sign \n 
self . _int = str ( value . int ) \n 
self . _exp = int ( value . exp ) \n 
~~ if isinstance ( value , ( list , tuple ) ) : \n 
~~~ if len ( value ) != 3 : \n 
~~ if not ( isinstance ( value [ 0 ] , ( int , long ) ) and value [ 0 ] in ( 0 , 1 ) ) : \n 
~~ self . _sign = value [ 0 ] \n 
if value [ 2 ] == : \n 
self . _exp = value [ 2 ] \n 
self . _is_special = True \n 
~~~ digits = [ ] \n 
for digit in value [ 1 ] : \n 
~~~ if isinstance ( digit , ( int , long ) ) and 0 <= digit <= 9 : \n 
~~~ if digits or digit != 0 : \n 
~~~ digits . append ( digit ) \n 
~~ ~~ if value [ 2 ] in ( , ) : \n 
~~~ self . _int = . join ( map ( str , digits ) ) \n 
~~ elif isinstance ( value [ 2 ] , ( int , long ) ) : \n 
~~~ self . _int = . join ( map ( str , digits or [ 0 ] ) ) \n 
~~ if isinstance ( value , float ) : \n 
~~~ value = Decimal . from_float ( value ) \n 
self . _exp = value . _exp \n 
~~ def from_float ( cls , f ) : \n 
~~~ return cls ( f ) \n 
~~~ return cls ( repr ( f ) ) \n 
~~ if _math . copysign ( 1.0 , f ) == 1.0 : \n 
~~~ sign = 0 \n 
~~~ sign = 1 \n 
~~ n , d = abs ( f ) . as_integer_ratio ( ) \n 
k = d . bit_length ( ) - 1 \n 
result = _dec_from_triple ( sign , str ( n * 5 ** k ) , - k ) \n 
if cls is Decimal : \n 
~~~ return cls ( result ) \n 
~~ ~~ from_float = classmethod ( from_float ) \n 
def _isnan ( self ) : \n 
if self . _is_special : \n 
~~~ exp = self . _exp \n 
if exp == : \n 
~~ elif exp == : \n 
~~~ return 2 \n 
~~ ~~ return 0 \n 
~~ def _isinfinity ( self ) : \n 
if self . _exp == : \n 
~~~ if self . _sign : \n 
~~ return 1 \n 
~~ return 0 \n 
~~ def _check_nans ( self , other = None , context = None ) : \n 
self_is_nan = self . _isnan ( ) \n 
if other is None : \n 
~~~ other_is_nan = False \n 
~~~ other_is_nan = other . _isnan ( ) \n 
~~ if self_is_nan or other_is_nan : \n 
~~ if self_is_nan == 2 : \n 
~~~ return context . _raise_error ( InvalidOperation , , \n 
~~ if other_is_nan == 2 : \n 
other ) \n 
~~ if self_is_nan : \n 
~~~ return self . _fix_nan ( context ) \n 
~~ return other . _fix_nan ( context ) \n 
~~ def _compare_check_nans ( self , other , context ) : \n 
if context is None : \n 
~~ if self . _is_special or other . _is_special : \n 
~~~ if self . is_snan ( ) : \n 
~~~ return context . _raise_error ( InvalidOperation , \n 
~~ elif other . is_snan ( ) : \n 
~~ elif self . is_qnan ( ) : \n 
~~ elif other . is_qnan ( ) : \n 
~~ def __nonzero__ ( self ) : \n 
return self . _is_special or self . _int != \n 
~~ def _cmp ( self , other ) : \n 
if self . _is_special or other . _is_special : \n 
~~~ self_inf = self . _isinfinity ( ) \n 
other_inf = other . _isinfinity ( ) \n 
if self_inf == other_inf : \n 
~~ elif self_inf < other_inf : \n 
~~ ~~ if not self : \n 
~~~ if not other : \n 
~~~ return - ( ( - 1 ) ** other . _sign ) \n 
~~ ~~ if not other : \n 
~~~ return ( - 1 ) ** self . _sign \n 
~~ if other . _sign < self . _sign : \n 
~~ if self . _sign < other . _sign : \n 
~~ self_adjusted = self . adjusted ( ) \n 
other_adjusted = other . adjusted ( ) \n 
if self_adjusted == other_adjusted : \n 
~~~ self_padded = self . _int + * ( self . _exp - other . _exp ) \n 
other_padded = other . _int + * ( other . _exp - self . _exp ) \n 
if self_padded == other_padded : \n 
~~ elif self_padded < other_padded : \n 
~~~ return - ( - 1 ) ** self . _sign \n 
~~ ~~ elif self_adjusted > other_adjusted : \n 
~~~ return - ( ( - 1 ) ** self . _sign ) \n 
~~ ~~ def __eq__ ( self , other , context = None ) : \n 
~~~ other = _convert_other ( other , allow_float = True ) \n 
if other is NotImplemented : \n 
~~~ return other \n 
~~ if self . _check_nans ( other , context ) : \n 
~~ return self . _cmp ( other ) == 0 \n 
~~ def __ne__ ( self , other , context = None ) : \n 
~~ return self . _cmp ( other ) != 0 \n 
~~ def __lt__ ( self , other , context = None ) : \n 
~~ ans = self . _compare_check_nans ( other , context ) \n 
if ans : \n 
~~ return self . _cmp ( other ) < 0 \n 
~~ def __le__ ( self , other , context = None ) : \n 
~~ return self . _cmp ( other ) <= 0 \n 
~~ def __gt__ ( self , other , context = None ) : \n 
~~ return self . _cmp ( other ) > 0 \n 
~~ def __ge__ ( self , other , context = None ) : \n 
~~ return self . _cmp ( other ) >= 0 \n 
~~ def compare ( self , other , context = None ) : \n 
other = _convert_other ( other , raiseit = True ) \n 
if ( self . _is_special or other and other . _is_special ) : \n 
~~~ ans = self . _check_nans ( other , context ) \n 
~~~ return ans \n 
~~ ~~ return Decimal ( self . _cmp ( other ) ) \n 
~~ elif self . is_nan ( ) : \n 
~~~ return - 271828 \n 
~~~ return 314159 \n 
~~ ~~ ~~ self_as_float = float ( self ) \n 
if Decimal . from_float ( self_as_float ) == self : \n 
~~~ return hash ( self_as_float ) \n 
~~ if self . _isinteger ( ) : \n 
~~~ return hash ( long ( self . to_integral_value ( ) ) ) \n 
~~ return hash ( ( self . _sign , \n 
self . _exp + len ( self . _int ) , \n 
self . _int . rstrip ( ) ) ) \n 
~~ def as_tuple ( self ) : \n 
return DecimalTuple ( self . _sign , tuple ( map ( int , self . _int ) ) , self . _exp ) \n 
return "Decimal(\'%s\')" % str ( self ) \n 
~~ def __str__ ( self , eng = False , context = None ) : \n 
sign = [ , ] [ self . _sign ] \n 
~~~ if self . _exp == : \n 
~~~ return sign + \n 
~~ elif self . _exp == : \n 
~~~ return sign + + self . _int \n 
~~ ~~ leftdigits = self . _exp + len ( self . _int ) \n 
if self . _exp <= 0 and leftdigits > - 6 : \n 
~~~ dotplace = leftdigits \n 
~~ elif not eng : \n 
~~~ dotplace = 1 \n 
~~ elif self . _int == : \n 
~~~ dotplace = ( leftdigits + 1 ) % 3 - 1 \n 
~~~ dotplace = ( leftdigits - 1 ) % 3 + 1 \n 
~~ if dotplace <= 0 : \n 
~~~ intpart = \n 
fracpart = + * ( - dotplace ) + self . _int \n 
~~ elif dotplace >= len ( self . _int ) : \n 
~~~ intpart = self . _int + * ( dotplace - len ( self . _int ) ) \n 
fracpart = \n 
~~~ intpart = self . _int [ : dotplace ] \n 
fracpart = + self . _int [ dotplace : ] \n 
~~ if leftdigits == dotplace : \n 
~~~ exp = \n 
~~ exp = [ , ] [ context . capitals ] + "%+d" % ( leftdigits - dotplace ) \n 
~~ return sign + intpart + fracpart + exp \n 
~~ def to_eng_string ( self , context = None ) : \n 
return self . __str__ ( eng = True , context = context ) \n 
~~ def __neg__ ( self , context = None ) : \n 
~~~ ans = self . _check_nans ( context = context ) \n 
~~~ ans = self . copy_abs ( ) \n 
~~~ ans = self . copy_negate ( ) \n 
~~ if context is None : \n 
~~ return ans . _fix ( context ) \n 
~~ def __pos__ ( self , context = None ) : \n 
~~~ ans = Decimal ( self ) \n 
~~ def __abs__ ( self , round = True , context = None ) : \n 
if not round : \n 
~~~ return self . copy_abs ( ) \n 
~~ if self . _is_special : \n 
~~ ~~ if self . _sign : \n 
~~~ ans = self . __neg__ ( context = context ) \n 
~~~ ans = self . __pos__ ( context = context ) \n 
~~ return ans \n 
~~ def __add__ ( self , other , context = None ) : \n 
other = _convert_other ( other ) \n 
~~ if self . _isinfinity ( ) : \n 
~~~ if self . _sign != other . _sign and other . _isinfinity ( ) : \n 
~~~ return context . _raise_error ( InvalidOperation , ) \n 
~~ return Decimal ( self ) \n 
~~ if other . _isinfinity ( ) : \n 
~~~ return Decimal ( other ) \n 
~~ ~~ exp = min ( self . _exp , other . _exp ) \n 
negativezero = 0 \n 
if context . rounding == ROUND_FLOOR and self . _sign != other . _sign : \n 
~~~ negativezero = 1 \n 
~~ if not self and not other : \n 
~~~ sign = min ( self . _sign , other . _sign ) \n 
if negativezero : \n 
~~ ans = _dec_from_triple ( sign , , exp ) \n 
ans = ans . _fix ( context ) \n 
return ans \n 
~~ if not self : \n 
~~~ exp = max ( exp , other . _exp - context . prec - 1 ) \n 
ans = other . _rescale ( exp , context . rounding ) \n 
~~ if not other : \n 
~~~ exp = max ( exp , self . _exp - context . prec - 1 ) \n 
ans = self . _rescale ( exp , context . rounding ) \n 
~~ op1 = _WorkRep ( self ) \n 
op2 = _WorkRep ( other ) \n 
op1 , op2 = _normalize ( op1 , op2 , context . prec ) \n 
result = _WorkRep ( ) \n 
if op1 . sign != op2 . sign : \n 
~~~ if op1 . int == op2 . int : \n 
~~~ ans = _dec_from_triple ( negativezero , , exp ) \n 
~~ if op1 . int < op2 . int : \n 
~~~ op1 , op2 = op2 , op1 \n 
~~ if op1 . sign == 1 : \n 
~~~ result . sign = 1 \n 
op1 . sign , op2 . sign = op2 . sign , op1 . sign \n 
~~~ result . sign = 0 \n 
~~ ~~ elif op1 . sign == 1 : \n 
op1 . sign , op2 . sign = ( 0 , 0 ) \n 
~~ if op2 . sign == 0 : \n 
~~~ result . int = op1 . int + op2 . int \n 
~~~ result . int = op1 . int - op2 . int \n 
~~ result . exp = op1 . exp \n 
ans = Decimal ( result ) \n 
~~ __radd__ = __add__ \n 
def __sub__ ( self , other , context = None ) : \n 
~~~ ans = self . _check_nans ( other , context = context ) \n 
~~ ~~ return self . __add__ ( other . copy_negate ( ) , context = context ) \n 
~~ def __rsub__ ( self , other , context = None ) : \n 
~~ return other . __sub__ ( self , context = context ) \n 
~~ def __mul__ ( self , other , context = None ) : \n 
~~ resultsign = self . _sign ^ other . _sign \n 
~~ return _SignedInfinity [ resultsign ] \n 
~~~ if not self : \n 
~~ ~~ resultexp = self . _exp + other . _exp \n 
if not self or not other : \n 
~~~ ans = _dec_from_triple ( resultsign , , resultexp ) \n 
~~ if self . _int == : \n 
~~~ ans = _dec_from_triple ( resultsign , other . _int , resultexp ) \n 
~~ if other . _int == : \n 
~~~ ans = _dec_from_triple ( resultsign , self . _int , resultexp ) \n 
ans = _dec_from_triple ( resultsign , str ( op1 . int * op2 . int ) , resultexp ) \n 
~~ __rmul__ = __mul__ \n 
def __truediv__ ( self , other , context = None ) : \n 
~~~ return NotImplemented \n 
~~ sign = self . _sign ^ other . _sign \n 
~~ if self . _isinfinity ( ) and other . _isinfinity ( ) : \n 
~~~ context . _raise_error ( Clamped , ) \n 
return _dec_from_triple ( sign , , context . Etiny ( ) ) \n 
~~~ return context . _raise_error ( DivisionUndefined , ) \n 
~~ return context . _raise_error ( DivisionByZero , , sign ) \n 
~~~ exp = self . _exp - other . _exp \n 
coeff = 0 \n 
~~~ shift = len ( other . _int ) - len ( self . _int ) + context . prec + 1 \n 
exp = self . _exp - other . _exp - shift \n 
op1 = _WorkRep ( self ) \n 
if shift >= 0 : \n 
~~~ coeff , remainder = divmod ( op1 . int * 10 ** shift , op2 . int ) \n 
~~~ coeff , remainder = divmod ( op1 . int , op2 . int * 10 ** - shift ) \n 
~~ if remainder : \n 
~~~ if coeff % 5 == 0 : \n 
~~~ coeff += 1 \n 
~~~ ideal_exp = self . _exp - other . _exp \n 
while exp < ideal_exp and coeff % 10 == 0 : \n 
~~~ coeff 10 \n 
exp += 1 \n 
~~ ~~ ~~ ans = _dec_from_triple ( sign , str ( coeff ) , exp ) \n 
return ans . _fix ( context ) \n 
~~ def _divide ( self , other , context ) : \n 
sign = self . _sign ^ other . _sign \n 
if other . _isinfinity ( ) : \n 
~~~ ideal_exp = self . _exp \n 
~~~ ideal_exp = min ( self . _exp , other . _exp ) \n 
~~ expdiff = self . adjusted ( ) - other . adjusted ( ) \n 
if not self or other . _isinfinity ( ) or expdiff <= - 2 : \n 
~~~ return ( _dec_from_triple ( sign , , 0 ) , \n 
self . _rescale ( ideal_exp , context . rounding ) ) \n 
~~ if expdiff <= context . prec : \n 
~~~ op1 = _WorkRep ( self ) \n 
if op1 . exp >= op2 . exp : \n 
~~~ op1 . int *= 10 ** ( op1 . exp - op2 . exp ) \n 
~~~ op2 . int *= 10 ** ( op2 . exp - op1 . exp ) \n 
~~ q , r = divmod ( op1 . int , op2 . int ) \n 
if q < 10 ** context . prec : \n 
~~~ return ( _dec_from_triple ( sign , str ( q ) , 0 ) , \n 
_dec_from_triple ( self . _sign , str ( r ) , ideal_exp ) ) \n 
~~ ~~ ans = context . _raise_error ( DivisionImpossible , \n 
return ans , ans \n 
~~ def __rtruediv__ ( self , other , context = None ) : \n 
~~ return other . __truediv__ ( self , context = context ) \n 
~~ __div__ = __truediv__ \n 
__rdiv__ = __rtruediv__ \n 
def __divmod__ ( self , other , context = None ) : \n 
~~ ans = self . _check_nans ( other , context ) \n 
~~~ return ( ans , ans ) \n 
if self . _isinfinity ( ) : \n 
~~~ if other . _isinfinity ( ) : \n 
~~~ ans = context . _raise_error ( InvalidOperation , ) \n 
~~~ return ( _SignedInfinity [ sign ] , \n 
context . _raise_error ( InvalidOperation , ) ) \n 
~~~ ans = context . _raise_error ( DivisionUndefined , ) \n 
~~~ return ( context . _raise_error ( DivisionByZero , , sign ) , \n 
~~ ~~ quotient , remainder = self . _divide ( other , context ) \n 
remainder = remainder . _fix ( context ) \n 
return quotient , remainder \n 
~~ def __rdivmod__ ( self , other , context = None ) : \n 
~~ return other . __divmod__ ( self , context = context ) \n 
~~ def __mod__ ( self , other , context = None ) : \n 
~~ elif not other : \n 
~~~ if self : \n 
~~ ~~ remainder = self . _divide ( other , context ) [ 1 ] \n 
return remainder \n 
~~ def __rmod__ ( self , other , context = None ) : \n 
~~ return other . __mod__ ( self , context = context ) \n 
~~ def remainder_near ( self , other , context = None ) : \n 
~~ other = _convert_other ( other , raiseit = True ) \n 
ans = self . _check_nans ( other , context ) \n 
~~~ return context . _raise_error ( DivisionUndefined , \n 
~~ ~~ if other . _isinfinity ( ) : \n 
~~ ideal_exponent = min ( self . _exp , other . _exp ) \n 
if not self : \n 
~~~ ans = _dec_from_triple ( self . _sign , , ideal_exponent ) \n 
if expdiff >= context . prec + 1 : \n 
~~~ return context . _raise_error ( DivisionImpossible ) \n 
~~ if expdiff <= - 2 : \n 
~~~ ans = self . _rescale ( ideal_exponent , context . rounding ) \n 
if 2 * r + ( q & 1 ) > op2 . int : \n 
~~~ r -= op2 . int \n 
q += 1 \n 
~~ if q >= 10 ** context . prec : \n 
~~ sign = self . _sign \n 
if r < 0 : \n 
~~~ sign = 1 - sign \n 
r = - r \n 
~~ ans = _dec_from_triple ( sign , str ( r ) , ideal_exponent ) \n 
~~ def __floordiv__ ( self , other , context = None ) : \n 
~~~ return _SignedInfinity [ self . _sign ^ other . _sign ] \n 
~~~ return context . _raise_error ( DivisionByZero , , \n 
self . _sign ^ other . _sign ) \n 
~~ ~~ return self . _divide ( other , context ) [ 0 ] \n 
~~ def __rfloordiv__ ( self , other , context = None ) : \n 
~~ return other . __floordiv__ ( self , context = context ) \n 
~~ def __float__ ( self ) : \n 
return float ( str ( self ) ) \n 
~~ def __int__ ( self ) : \n 
~~~ if self . _isnan ( ) : \n 
~~ elif self . _isinfinity ( ) : \n 
~~ ~~ s = ( - 1 ) ** self . _sign \n 
if self . _exp >= 0 : \n 
~~~ return s * int ( self . _int ) * 10 ** self . _exp \n 
~~~ return s * int ( self . _int [ : self . _exp ] or ) \n 
~~ ~~ __trunc__ = __int__ \n 
def real ( self ) : \n 
~~~ return self \n 
~~ real = property ( real ) \n 
def imag ( self ) : \n 
~~~ return Decimal ( 0 ) \n 
~~ imag = property ( imag ) \n 
def conjugate ( self ) : \n 
~~ def __complex__ ( self ) : \n 
~~~ return complex ( float ( self ) ) \n 
~~ def __long__ ( self ) : \n 
return long ( self . __int__ ( ) ) \n 
~~ def _fix_nan ( self , context ) : \n 
payload = self . _int \n 
max_payload_len = context . prec - context . _clamp \n 
if len ( payload ) > max_payload_len : \n 
~~~ payload = payload [ len ( payload ) - max_payload_len : ] . lstrip ( ) \n 
return _dec_from_triple ( self . _sign , payload , self . _exp , True ) \n 
~~ def _fix ( self , context ) : \n 
~~~ return Decimal ( self ) \n 
~~ ~~ Etiny = context . Etiny ( ) \n 
Etop = context . Etop ( ) \n 
~~~ exp_max = [ context . Emax , Etop ] [ context . _clamp ] \n 
new_exp = min ( max ( self . _exp , Etiny ) , exp_max ) \n 
if new_exp != self . _exp : \n 
~~~ context . _raise_error ( Clamped ) \n 
return _dec_from_triple ( self . _sign , , new_exp ) \n 
~~ ~~ exp_min = len ( self . _int ) + self . _exp - context . prec \n 
if exp_min > Etop : \n 
~~~ ans = context . _raise_error ( Overflow , , self . _sign ) \n 
context . _raise_error ( Inexact ) \n 
context . _raise_error ( Rounded ) \n 
~~ self_is_subnormal = exp_min < Etiny \n 
if self_is_subnormal : \n 
~~~ exp_min = Etiny \n 
~~ if self . _exp < exp_min : \n 
~~~ digits = len ( self . _int ) + self . _exp - exp_min \n 
if digits < 0 : \n 
~~~ self = _dec_from_triple ( self . _sign , , exp_min - 1 ) \n 
digits = 0 \n 
~~ rounding_method = self . _pick_rounding_function [ context . rounding ] \n 
changed = getattr ( self , rounding_method ) ( digits ) \n 
coeff = self . _int [ : digits ] or \n 
if changed > 0 : \n 
~~~ coeff = str ( int ( coeff ) + 1 ) \n 
if len ( coeff ) > context . prec : \n 
~~~ coeff = coeff [ : - 1 ] \n 
exp_min += 1 \n 
~~ ~~ if exp_min > Etop : \n 
~~~ ans = _dec_from_triple ( self . _sign , coeff , exp_min ) \n 
~~ if changed and self_is_subnormal : \n 
~~~ context . _raise_error ( Underflow ) \n 
~~ if self_is_subnormal : \n 
~~~ context . _raise_error ( Subnormal ) \n 
~~ if changed : \n 
~~~ context . _raise_error ( Inexact ) \n 
~~ context . _raise_error ( Rounded ) \n 
if not ans : \n 
~~ if context . _clamp == 1 and self . _exp > Etop : \n 
self_padded = self . _int + * ( self . _exp - Etop ) \n 
return _dec_from_triple ( self . _sign , self_padded , Etop ) \n 
~~ _pick_rounding_function = { } \n 
def _round_down ( self , prec ) : \n 
if _all_zeros ( self . _int , prec ) : \n 
~~ ~~ def _round_up ( self , prec ) : \n 
return - self . _round_down ( prec ) \n 
~~ def _round_half_up ( self , prec ) : \n 
if self . _int [ prec ] in : \n 
~~ elif _all_zeros ( self . _int , prec ) : \n 
~~ ~~ def _round_half_down ( self , prec ) : \n 
if _exact_half ( self . _int , prec ) : \n 
~~~ return self . _round_half_up ( prec ) \n 
~~ ~~ def _round_half_even ( self , prec ) : \n 
if _exact_half ( self . _int , prec ) and ( prec == 0 or self . _int [ prec - 1 ] in ) : \n 
~~ ~~ def _round_ceiling ( self , prec ) : \n 
if self . _sign : \n 
~~~ return self . _round_down ( prec ) \n 
~~~ return - self . _round_down ( prec ) \n 
~~ ~~ def _round_floor ( self , prec ) : \n 
if not self . _sign : \n 
~~ ~~ def _round_05up ( self , prec ) : \n 
if prec and self . _int [ prec - 1 ] not in : \n 
~~ ~~ def fma ( self , other , third , context = None ) : \n 
~~ if self . _exp == : \n 
~~~ return context . _raise_error ( InvalidOperation , , self ) \n 
~~ if other . _exp == : \n 
~~~ return context . _raise_error ( InvalidOperation , , other ) \n 
~~~ product = self \n 
~~ elif other . _exp == : \n 
~~~ product = other \n 
~~ product = _SignedInfinity [ self . _sign ^ other . _sign ] \n 
~~~ product = _dec_from_triple ( self . _sign ^ other . _sign , \n 
str ( int ( self . _int ) * int ( other . _int ) ) , \n 
self . _exp + other . _exp ) \n 
~~ third = _convert_other ( third , raiseit = True ) \n 
return product . __add__ ( third , context ) \n 
~~ def _power_modulo ( self , other , modulo , context = None ) : \n 
modulo = _convert_other ( modulo , raiseit = True ) \n 
~~ self_is_nan = self . _isnan ( ) \n 
other_is_nan = other . _isnan ( ) \n 
modulo_is_nan = modulo . _isnan ( ) \n 
if self_is_nan or other_is_nan or modulo_is_nan : \n 
~~~ if self_is_nan == 2 : \n 
~~ if modulo_is_nan == 2 : \n 
modulo ) \n 
~~ if other_is_nan : \n 
~~~ return other . _fix_nan ( context ) \n 
~~ return modulo . _fix_nan ( context ) \n 
~~ if not ( self . _isinteger ( ) and \n 
other . _isinteger ( ) and \n 
modulo . _isinteger ( ) ) : \n 
~~ if other < 0 : \n 
~~ if not modulo : \n 
~~ if modulo . adjusted ( ) >= context . prec : \n 
~~ if not other and not self : \n 
~~ if other . _iseven ( ) : \n 
~~~ sign = self . _sign \n 
~~ modulo = abs ( int ( modulo ) ) \n 
base = _WorkRep ( self . to_integral_value ( ) ) \n 
exponent = _WorkRep ( other . to_integral_value ( ) ) \n 
base = ( base . int % modulo * pow ( 10 , base . exp , modulo ) ) % modulo \n 
for i in xrange ( exponent . exp ) : \n 
~~~ base = pow ( base , 10 , modulo ) \n 
~~ base = pow ( base , exponent . int , modulo ) \n 
return _dec_from_triple ( sign , str ( base ) , 0 ) \n 
~~ def _power_exact ( self , other , p ) : \n 
x = _WorkRep ( self ) \n 
xc , xe = x . int , x . exp \n 
while xc % 10 == 0 : \n 
~~~ xc 10 \n 
xe += 1 \n 
~~ y = _WorkRep ( other ) \n 
yc , ye = y . int , y . exp \n 
while yc % 10 == 0 : \n 
~~~ yc 10 \n 
ye += 1 \n 
~~ if xc == 1 : \n 
~~~ xe *= yc \n 
while xe % 10 == 0 : \n 
~~~ xe 10 \n 
~~ if ye < 0 : \n 
~~ exponent = xe * 10 ** ye \n 
if y . sign == 1 : \n 
~~~ exponent = - exponent \n 
~~ if other . _isinteger ( ) and other . _sign == 0 : \n 
~~~ ideal_exponent = self . _exp * int ( other ) \n 
zeros = min ( exponent - ideal_exponent , p - 1 ) \n 
~~~ zeros = 0 \n 
~~ return _dec_from_triple ( 0 , + * zeros , exponent - zeros ) \n 
~~ if y . sign == 1 : \n 
~~~ last_digit = xc % 10 \n 
if last_digit in ( 2 , 4 , 6 , 8 ) : \n 
~~~ if xc & - xc != xc : \n 
~~ e = _nbits ( xc ) - 1 \n 
if ye >= 0 : \n 
~~~ y_as_int = yc * 10 ** ye \n 
e = e * y_as_int \n 
xe = xe * y_as_int \n 
~~~ ten_pow = 10 ** - ye \n 
e , remainder = divmod ( e * yc , ten_pow ) \n 
if remainder : \n 
~~ xe , remainder = divmod ( xe * yc , ten_pow ) \n 
~~ xc = 5 ** e \n 
~~ elif last_digit == 5 : \n 
~~~ e = _nbits ( xc ) * 28 // 65 \n 
xc , remainder = divmod ( 5 ** e , xc ) \n 
~~ while xc % 5 == 0 : \n 
~~~ xc 5 \n 
e -= 1 \n 
~~ if ye >= 0 : \n 
~~~ y_as_integer = yc * 10 ** ye \n 
e = e * y_as_integer \n 
xe = xe * y_as_integer \n 
~~ xc = 2 ** e \n 
~~ if xc >= 10 ** p : \n 
~~ xe = - e - xe \n 
return _dec_from_triple ( 0 , str ( xc ) , xe ) \n 
~~~ m , n = yc * 10 ** ye , 1 \n 
~~~ if xe != 0 and len ( str ( abs ( yc * xe ) ) ) <= - ye : \n 
~~ xc_bits = _nbits ( xc ) \n 
if xc != 1 and len ( str ( abs ( yc ) * xc_bits ) ) <= - ye : \n 
~~ m , n = yc , 10 ** ( - ye ) \n 
while m % 2 == n % 2 == 0 : \n 
~~~ m 2 \n 
n 2 \n 
~~ while m % 5 == n % 5 == 0 : \n 
~~~ m 5 \n 
n 5 \n 
~~ ~~ if n > 1 : \n 
~~~ if xc != 1 and xc_bits <= n : \n 
~~ xe , rem = divmod ( xe , n ) \n 
if rem != 0 : \n 
~~~ q , r = divmod ( xc , a ** ( n - 1 ) ) \n 
if a <= q : \n 
~~~ a = ( a * ( n - 1 ) + q ) // n \n 
~~ ~~ if not ( a == q and r == 0 ) : \n 
~~ xc = a \n 
~~ if xc > 1 and m > p * 100 // _log10_lb ( xc ) : \n 
~~ xc = xc ** m \n 
xe *= m \n 
if xc > 10 ** p : \n 
~~ str_xc = str ( xc ) \n 
if other . _isinteger ( ) and other . _sign == 0 : \n 
zeros = min ( xe - ideal_exponent , p - len ( str_xc ) ) \n 
~~ return _dec_from_triple ( 0 , str_xc + * zeros , xe - zeros ) \n 
~~ def __pow__ ( self , other , modulo = None , context = None ) : \n 
if modulo is not None : \n 
~~~ return self . _power_modulo ( other , modulo , context ) \n 
~~ other = _convert_other ( other ) \n 
~~~ return _One \n 
~~ ~~ result_sign = 0 \n 
if self . _sign == 1 : \n 
~~~ if other . _isinteger ( ) : \n 
~~~ if not other . _iseven ( ) : \n 
~~~ result_sign = 1 \n 
~~ ~~ self = self . copy_negate ( ) \n 
~~~ if other . _sign == 0 : \n 
~~~ return _dec_from_triple ( result_sign , , 0 ) \n 
~~~ return _SignedInfinity [ result_sign ] \n 
~~ ~~ if self . _isinfinity ( ) : \n 
~~ ~~ if self == _One : \n 
~~~ if other . _sign == 1 : \n 
~~~ multiplier = 0 \n 
~~ elif other > context . prec : \n 
~~~ multiplier = context . prec \n 
~~~ multiplier = int ( other ) \n 
~~ exp = self . _exp * multiplier \n 
if exp < 1 - context . prec : \n 
~~~ exp = 1 - context . prec \n 
exp = 1 - context . prec \n 
~~ return _dec_from_triple ( result_sign , + * - exp , exp ) \n 
~~ self_adj = self . adjusted ( ) \n 
~~~ if ( other . _sign == 0 ) == ( self_adj < 0 ) : \n 
~~ ~~ ans = None \n 
exact = False \n 
bound = self . _log10_exp_bound ( ) + other . adjusted ( ) \n 
if ( self_adj >= 0 ) == ( other . _sign == 0 ) : \n 
~~~ if bound >= len ( str ( context . Emax ) ) : \n 
~~~ ans = _dec_from_triple ( result_sign , , context . Emax + 1 ) \n 
~~~ Etiny = context . Etiny ( ) \n 
if bound >= len ( str ( - Etiny ) ) : \n 
~~~ ans = _dec_from_triple ( result_sign , , Etiny - 1 ) \n 
~~ ~~ if ans is None : \n 
~~~ ans = self . _power_exact ( other , context . prec + 1 ) \n 
if ans is not None : \n 
~~~ if result_sign == 1 : \n 
~~~ ans = _dec_from_triple ( 1 , ans . _int , ans . _exp ) \n 
~~ exact = True \n 
~~~ p = context . prec \n 
y = _WorkRep ( other ) \n 
~~~ yc = - yc \n 
~~ extra = 3 \n 
~~~ coeff , exp = _dpower ( xc , xe , yc , ye , p + extra ) \n 
if coeff % ( 5 * 10 ** ( len ( str ( coeff ) ) - p - 1 ) ) : \n 
~~ extra += 3 \n 
~~ ans = _dec_from_triple ( result_sign , str ( coeff ) , exp ) \n 
~~ if exact and not other . _isinteger ( ) : \n 
~~~ if len ( ans . _int ) <= context . prec : \n 
~~~ expdiff = context . prec + 1 - len ( ans . _int ) \n 
ans = _dec_from_triple ( ans . _sign , ans . _int + * expdiff , \n 
ans . _exp - expdiff ) \n 
~~ newcontext = context . copy ( ) \n 
newcontext . clear_flags ( ) \n 
for exception in _signals : \n 
~~~ newcontext . traps [ exception ] = 0 \n 
~~ ans = ans . _fix ( newcontext ) \n 
newcontext . _raise_error ( Inexact ) \n 
if newcontext . flags [ Subnormal ] : \n 
~~~ newcontext . _raise_error ( Underflow ) \n 
~~ if newcontext . flags [ Overflow ] : \n 
~~~ context . _raise_error ( Overflow , , ans . _sign ) \n 
~~ for exception in Underflow , Subnormal , Inexact , Rounded , Clamped : \n 
~~~ if newcontext . flags [ exception ] : \n 
~~~ context . _raise_error ( exception ) \n 
~~~ ans = ans . _fix ( context ) \n 
~~ def __rpow__ ( self , other , context = None ) : \n 
~~ return other . __pow__ ( self , context = context ) \n 
~~ def normalize ( self , context = None ) : \n 
~~ ~~ dup = self . _fix ( context ) \n 
if dup . _isinfinity ( ) : \n 
~~~ return dup \n 
~~ if not dup : \n 
~~~ return _dec_from_triple ( dup . _sign , , 0 ) \n 
~~ exp_max = [ context . Emax , context . Etop ( ) ] [ context . _clamp ] \n 
end = len ( dup . _int ) \n 
exp = dup . _exp \n 
while dup . _int [ end - 1 ] == and exp < exp_max : \n 
~~~ exp += 1 \n 
end -= 1 \n 
~~ return _dec_from_triple ( dup . _sign , dup . _int [ : end ] , exp ) \n 
~~ def quantize ( self , exp , rounding = None , context = None , watchexp = True ) : \n 
exp = _convert_other ( exp , raiseit = True ) \n 
~~ if rounding is None : \n 
~~~ rounding = context . rounding \n 
~~ if self . _is_special or exp . _is_special : \n 
~~~ ans = self . _check_nans ( exp , context ) \n 
~~ if exp . _isinfinity ( ) or self . _isinfinity ( ) : \n 
~~~ if exp . _isinfinity ( ) and self . _isinfinity ( ) : \n 
~~ return context . _raise_error ( InvalidOperation , \n 
~~ ~~ if not watchexp : \n 
~~~ ans = self . _rescale ( exp . _exp , rounding ) \n 
if ans . _exp > self . _exp : \n 
~~~ context . _raise_error ( Rounded ) \n 
if ans != self : \n 
~~ ~~ return ans \n 
~~ if not ( context . Etiny ( ) <= exp . _exp <= context . Emax ) : \n 
~~~ ans = _dec_from_triple ( self . _sign , , exp . _exp ) \n 
if self_adjusted > context . Emax : \n 
) ~~ if self_adjusted - exp . _exp + 1 > context . prec : \n 
~~ ans = self . _rescale ( exp . _exp , rounding ) \n 
if ans . adjusted ( ) > context . Emax : \n 
) ~~ if len ( ans . _int ) > context . prec : \n 
~~ if ans and ans . adjusted ( ) < context . Emin : \n 
~~ if ans . _exp > self . _exp : \n 
~~~ if ans != self : \n 
~~ ans = ans . _fix ( context ) \n 
~~ def same_quantum ( self , other ) : \n 
~~~ return ( self . is_nan ( ) and other . is_nan ( ) or \n 
self . is_infinite ( ) and other . is_infinite ( ) ) \n 
~~ return self . _exp == other . _exp \n 
~~ def _rescale ( self , exp , rounding ) : \n 
~~~ return _dec_from_triple ( self . _sign , , exp ) \n 
~~ if self . _exp >= exp : \n 
~~~ return _dec_from_triple ( self . _sign , \n 
self . _int + * ( self . _exp - exp ) , exp ) \n 
~~ digits = len ( self . _int ) + self . _exp - exp \n 
~~~ self = _dec_from_triple ( self . _sign , , exp - 1 ) \n 
~~ this_function = getattr ( self , self . _pick_rounding_function [ rounding ] ) \n 
changed = this_function ( digits ) \n 
if changed == 1 : \n 
~~ return _dec_from_triple ( self . _sign , coeff , exp ) \n 
~~ def _round ( self , places , rounding ) : \n 
if places <= 0 : \n 
~~ if self . _is_special or not self : \n 
~~ ans = self . _rescale ( self . adjusted ( ) + 1 - places , rounding ) \n 
if ans . adjusted ( ) != self . adjusted ( ) : \n 
~~~ ans = ans . _rescale ( ans . adjusted ( ) + 1 - places , rounding ) \n 
~~ def to_integral_exact ( self , rounding = None , context = None ) : \n 
~~ if self . _exp >= 0 : \n 
~~~ return _dec_from_triple ( self . _sign , , 0 ) \n 
~~ ans = self . _rescale ( 0 , rounding ) \n 
~~ def to_integral_value ( self , rounding = None , context = None ) : \n 
~~~ return self . _rescale ( 0 , rounding ) \n 
~~ ~~ to_integral = to_integral_value \n 
def sqrt ( self , context = None ) : \n 
~~ if self . _isinfinity ( ) and self . _sign == 0 : \n 
~~~ ans = _dec_from_triple ( self . _sign , , self . _exp // 2 ) \n 
~~ if self . _sign == 1 : \n 
~~ prec = context . prec + 1 \n 
op = _WorkRep ( self ) \n 
e = op . exp >> 1 \n 
if op . exp & 1 : \n 
~~~ c = op . int * 10 \n 
l = ( len ( self . _int ) >> 1 ) + 1 \n 
~~~ c = op . int \n 
l = len ( self . _int ) + 1 >> 1 \n 
~~ shift = prec - l \n 
~~~ c *= 100 ** shift \n 
exact = True \n 
~~~ c , remainder = divmod ( c , 100 ** - shift ) \n 
exact = not remainder \n 
~~ e -= shift \n 
n = 10 ** prec \n 
~~~ q = c // n \n 
if n <= q : \n 
~~~ n = n + q >> 1 \n 
~~ ~~ exact = exact and n * n == c \n 
if exact : \n 
~~~ if shift >= 0 : \n 
~~~ n 10 ** shift \n 
~~~ n *= 10 ** - shift \n 
~~ e += shift \n 
~~~ if n % 5 == 0 : \n 
~~~ n += 1 \n 
~~ ~~ ans = _dec_from_triple ( 0 , str ( n ) , e ) \n 
context = context . _shallow_copy ( ) \n 
rounding = context . _set_rounding ( ROUND_HALF_EVEN ) \n 
context . rounding = rounding \n 
~~ def max ( self , other , context = None ) : \n 
~~~ sn = self . _isnan ( ) \n 
on = other . _isnan ( ) \n 
if sn or on : \n 
~~~ if on == 1 and sn == 0 : \n 
~~~ return self . _fix ( context ) \n 
~~ if sn == 1 and on == 0 : \n 
~~~ return other . _fix ( context ) \n 
~~ return self . _check_nans ( other , context ) \n 
~~ ~~ c = self . _cmp ( other ) \n 
if c == 0 : \n 
~~~ c = self . compare_total ( other ) \n 
~~ if c == - 1 : \n 
~~~ ans = other \n 
~~~ ans = self \n 
~~ def min ( self , other , context = None ) : \n 
~~ def _isinteger ( self ) : \n 
~~ rest = self . _int [ self . _exp : ] \n 
return rest == * len ( rest ) \n 
~~ def _iseven ( self ) : \n 
if not self or self . _exp > 0 : \n 
~~ return self . _int [ - 1 + self . _exp ] in \n 
~~ def adjusted ( self ) : \n 
~~~ return self . _exp + len ( self . _int ) - 1 \n 
~~ ~~ def canonical ( self , context = None ) : \n 
~~ def compare_signal ( self , other , context = None ) : \n 
ans = self . _compare_check_nans ( other , context ) \n 
~~ return self . compare ( other , context = context ) \n 
~~ def compare_total ( self , other ) : \n 
if self . _sign and not other . _sign : \n 
~~~ return _NegativeOne \n 
~~ if not self . _sign and other . _sign : \n 
self_nan = self . _isnan ( ) \n 
other_nan = other . _isnan ( ) \n 
if self_nan or other_nan : \n 
~~~ if self_nan == other_nan : \n 
~~~ self_key = len ( self . _int ) , self . _int \n 
other_key = len ( other . _int ) , other . _int \n 
if self_key < other_key : \n 
~~~ if sign : \n 
~~ ~~ if self_key > other_key : \n 
~~ ~~ return _Zero \n 
~~ if sign : \n 
~~~ if self_nan == 1 : \n 
~~ if other_nan == 1 : \n 
~~ if self_nan == 2 : \n 
~~ if other_nan == 2 : \n 
~~ ~~ ~~ if self < other : \n 
~~ if self > other : \n 
~~ if self . _exp < other . _exp : \n 
~~ ~~ if self . _exp > other . _exp : \n 
~~ def compare_total_mag ( self , other ) : \n 
s = self . copy_abs ( ) \n 
o = other . copy_abs ( ) \n 
return s . compare_total ( o ) \n 
~~ def copy_abs ( self ) : \n 
return _dec_from_triple ( 0 , self . _int , self . _exp , self . _is_special ) \n 
~~ def copy_negate ( self ) : \n 
~~~ return _dec_from_triple ( 0 , self . _int , self . _exp , self . _is_special ) \n 
~~~ return _dec_from_triple ( 1 , self . _int , self . _exp , self . _is_special ) \n 
~~ ~~ def copy_sign ( self , other ) : \n 
return _dec_from_triple ( other . _sign , self . _int , \n 
self . _exp , self . _is_special ) \n 
~~ def exp ( self , context = None ) : \n 
~~ ans = self . _check_nans ( context = context ) \n 
~~ if self . _isinfinity ( ) == - 1 : \n 
~~~ return _Zero \n 
~~ if self . _isinfinity ( ) == 1 : \n 
~~ p = context . prec \n 
adj = self . adjusted ( ) \n 
if self . _sign == 0 and adj > len ( str ( ( context . Emax + 1 ) * 3 ) ) : \n 
~~~ ans = _dec_from_triple ( 0 , , context . Emax + 1 ) \n 
~~ elif self . _sign == 1 and adj > len ( str ( ( - context . Etiny ( ) + 1 ) * 3 ) ) : \n 
~~~ ans = _dec_from_triple ( 0 , , context . Etiny ( ) - 1 ) \n 
~~ elif self . _sign == 0 and adj < - p : \n 
~~~ ans = _dec_from_triple ( 0 , + * ( p - 1 ) + , - p ) \n 
~~ elif self . _sign == 1 and adj < - p - 1 : \n 
~~~ ans = _dec_from_triple ( 0 , * ( p + 1 ) , - p - 1 ) \n 
~~~ op = _WorkRep ( self ) \n 
c , e = op . int , op . exp \n 
if op . sign == 1 : \n 
~~~ c = - c \n 
~~~ coeff , exp = _dexp ( c , e , p + extra ) \n 
~~ ans = _dec_from_triple ( 0 , str ( coeff ) , exp ) \n 
~~ context = context . _shallow_copy ( ) \n 
~~ def is_canonical ( self ) : \n 
~~ def is_finite ( self ) : \n 
return not self . _is_special \n 
~~ def is_infinite ( self ) : \n 
return self . _exp == \n 
~~ def is_nan ( self ) : \n 
return self . _exp in ( , ) \n 
~~ def is_normal ( self , context = None ) : \n 
if self . _is_special or not self : \n 
~~ return context . Emin <= self . adjusted ( ) \n 
~~ def is_qnan ( self ) : \n 
~~ def is_signed ( self ) : \n 
return self . _sign == 1 \n 
~~ def is_snan ( self ) : \n 
~~ def is_subnormal ( self , context = None ) : \n 
~~ return self . adjusted ( ) < context . Emin \n 
~~ def is_zero ( self ) : \n 
return not self . _is_special and self . _int == \n 
~~ def _ln_exp_bound ( self ) : \n 
adj = self . _exp + len ( self . _int ) - 1 \n 
if adj >= 1 : \n 
~~~ return len ( str ( adj * 23 // 10 ) ) - 1 \n 
~~ if adj <= - 2 : \n 
~~~ return len ( str ( ( - 1 - adj ) * 23 // 10 ) ) - 1 \n 
~~ op = _WorkRep ( self ) \n 
if adj == 0 : \n 
~~~ num = str ( c - 10 ** - e ) \n 
den = str ( c ) \n 
return len ( num ) - len ( den ) - ( num < den ) \n 
~~ return e + len ( str ( 10 ** - e - c ) ) - 1 \n 
~~ def ln ( self , context = None ) : \n 
~~~ return _NegativeInfinity \n 
~~~ return _Infinity \n 
~~ if self == _One : \n 
p = context . prec \n 
~~~ coeff = _dlog ( c , e , places ) \n 
if coeff % ( 5 * 10 ** ( len ( str ( abs ( coeff ) ) ) - p - 1 ) ) : \n 
~~ places += 3 \n 
~~ ans = _dec_from_triple ( int ( coeff < 0 ) , str ( abs ( coeff ) ) , - places ) \n 
~~ def _log10_exp_bound ( self ) : \n 
~~~ return len ( str ( adj ) ) - 1 \n 
~~~ return len ( str ( - 1 - adj ) ) - 1 \n 
den = str ( 231 * c ) \n 
return len ( num ) - len ( den ) - ( num < den ) + 2 \n 
~~ num = str ( 10 ** - e - c ) \n 
return len ( num ) + e - ( num < "231" ) - 1 \n 
~~ def log10 ( self , context = None ) : \n 
~~ if self . _int [ 0 ] == and self . _int [ 1 : ] == * ( len ( self . _int ) - 1 ) : \n 
~~~ ans = Decimal ( self . _exp + len ( self . _int ) - 1 ) \n 
places = p - self . _log10_exp_bound ( ) + 2 \n 
~~~ coeff = _dlog10 ( c , e , places ) \n 
~~ def logb ( self , context = None ) : \n 
ans = self . _check_nans ( context = context ) \n 
~~~ return context . _raise_error ( DivisionByZero , , 1 ) \n 
~~ ans = Decimal ( self . adjusted ( ) ) \n 
~~ def _islogical ( self ) : \n 
if self . _sign != 0 or self . _exp != 0 : \n 
~~ for dig in self . _int : \n 
~~~ if dig not in : \n 
~~ def _fill_logical ( self , context , opa , opb ) : \n 
~~~ dif = context . prec - len ( opa ) \n 
if dif > 0 : \n 
~~~ opa = * dif + opa \n 
~~ elif dif < 0 : \n 
~~~ opa = opa [ - context . prec : ] \n 
~~ dif = context . prec - len ( opb ) \n 
~~~ opb = * dif + opb \n 
~~~ opb = opb [ - context . prec : ] \n 
~~ return opa , opb \n 
~~ def logical_and ( self , other , context = None ) : \n 
if not self . _islogical ( ) or not other . _islogical ( ) : \n 
~~~ return context . _raise_error ( InvalidOperation ) \n 
~~ ( opa , opb ) = self . _fill_logical ( context , self . _int , other . _int ) \n 
result = "" . join ( [ str ( int ( a ) & int ( b ) ) for a , b in zip ( opa , opb ) ] ) \n 
return _dec_from_triple ( 0 , result . lstrip ( ) or , 0 ) \n 
~~ def logical_invert ( self , context = None ) : \n 
~~ return self . logical_xor ( _dec_from_triple ( 0 , * context . prec , 0 ) , \n 
context ) \n 
~~ def logical_or ( self , other , context = None ) : \n 
result = "" . join ( [ str ( int ( a ) | int ( b ) ) for a , b in zip ( opa , opb ) ] ) \n 
~~ def logical_xor ( self , other , context = None ) : \n 
result = "" . join ( [ str ( int ( a ) ^ int ( b ) ) for a , b in zip ( opa , opb ) ] ) \n 
~~ def max_mag ( self , other , context = None ) : \n 
~~ ~~ c = self . copy_abs ( ) . _cmp ( other . copy_abs ( ) ) \n 
~~ def min_mag ( self , other , context = None ) : \n 
~~ def next_minus ( self , context = None ) : \n 
~~~ return _dec_from_triple ( 0 , * context . prec , context . Etop ( ) ) \n 
~~ context = context . copy ( ) \n 
context . _set_rounding ( ROUND_FLOOR ) \n 
context . _ignore_all_flags ( ) \n 
new_self = self . _fix ( context ) \n 
if new_self != self : \n 
~~~ return new_self \n 
~~ return self . __sub__ ( _dec_from_triple ( 0 , , context . Etiny ( ) - 1 ) , \n 
~~ def next_plus ( self , context = None ) : \n 
~~~ return _dec_from_triple ( 1 , * context . prec , context . Etop ( ) ) \n 
context . _set_rounding ( ROUND_CEILING ) \n 
~~ return self . __add__ ( _dec_from_triple ( 0 , , context . Etiny ( ) - 1 ) , \n 
~~ def next_toward ( self , other , context = None ) : \n 
~~ comparison = self . _cmp ( other ) \n 
if comparison == 0 : \n 
~~~ return self . copy_sign ( other ) \n 
~~ if comparison == - 1 : \n 
~~~ ans = self . next_plus ( context ) \n 
~~~ ans = self . next_minus ( context ) \n 
~~ if ans . _isinfinity ( ) : \n 
~~~ context . _raise_error ( Overflow , \n 
ans . _sign ) \n 
~~ elif ans . adjusted ( ) < context . Emin : \n 
context . _raise_error ( Subnormal ) \n 
~~ def number_class ( self , context = None ) : \n 
if self . is_snan ( ) : \n 
~~~ return "sNaN" \n 
~~ if self . is_qnan ( ) : \n 
~~~ return "NaN" \n 
~~ inf = self . _isinfinity ( ) \n 
if inf == 1 : \n 
~~~ return "+Infinity" \n 
~~ if inf == - 1 : \n 
~~~ return "-Infinity" \n 
~~ if self . is_zero ( ) : \n 
~~~ return "-Zero" \n 
~~~ return "+Zero" \n 
~~ ~~ if context is None : \n 
~~ if self . is_subnormal ( context = context ) : \n 
~~~ return "-Subnormal" \n 
~~~ return "+Subnormal" \n 
~~~ return "-Normal" \n 
~~~ return "+Normal" \n 
~~ ~~ def radix ( self ) : \n 
return Decimal ( 10 ) \n 
~~ def rotate ( self , other , context = None ) : \n 
~~ if other . _exp != 0 : \n 
~~ if not ( - context . prec <= int ( other ) <= context . prec ) : \n 
~~ torot = int ( other ) \n 
rotdig = self . _int \n 
topad = context . prec - len ( rotdig ) \n 
if topad > 0 : \n 
~~~ rotdig = * topad + rotdig \n 
~~ elif topad < 0 : \n 
~~~ rotdig = rotdig [ - topad : ] \n 
~~ rotated = rotdig [ torot : ] + rotdig [ : torot ] \n 
return _dec_from_triple ( self . _sign , \n 
rotated . lstrip ( ) or , self . _exp ) \n 
~~ def scaleb ( self , other , context = None ) : \n 
~~ liminf = - 2 * ( context . Emax + context . prec ) \n 
limsup = 2 * ( context . Emax + context . prec ) \n 
if not ( liminf <= int ( other ) <= limsup ) : \n 
~~ d = _dec_from_triple ( self . _sign , self . _int , self . _exp + int ( other ) ) \n 
d = d . _fix ( context ) \n 
~~ def shift ( self , other , context = None ) : \n 
~~ if torot < 0 : \n 
~~~ shifted = rotdig [ : torot ] \n 
~~~ shifted = rotdig + * torot \n 
shifted = shifted [ - context . prec : ] \n 
~~ return _dec_from_triple ( self . _sign , \n 
shifted . lstrip ( ) or , self . _exp ) \n 
~~~ return ( self . __class__ , ( str ( self ) , ) ) \n 
~~~ if type ( self ) is Decimal : \n 
~~ return self . __class__ ( str ( self ) ) \n 
~~ def __deepcopy__ ( self , memo ) : \n 
~~ def __format__ ( self , specifier , context = None , _localeconv = None ) : \n 
~~ spec = _parse_format_specifier ( specifier , _localeconv = _localeconv ) \n 
~~~ sign = _format_sign ( self . _sign , spec ) \n 
body = str ( self . copy_abs ( ) ) \n 
return _format_align ( sign , body , spec ) \n 
~~ if spec [ ] is None : \n 
~~~ spec [ ] = [ , ] [ context . capitals ] \n 
~~ if spec [ ] == : \n 
~~~ self = _dec_from_triple ( self . _sign , self . _int , self . _exp + 2 ) \n 
~~ rounding = context . rounding \n 
precision = spec [ ] \n 
if precision is not None : \n 
~~~ if spec [ ] in : \n 
~~~ self = self . _round ( precision + 1 , rounding ) \n 
~~ elif spec [ ] in : \n 
~~~ self = self . _rescale ( - precision , rounding ) \n 
~~ elif spec [ ] in and len ( self . _int ) > precision : \n 
~~~ self = self . _round ( precision , rounding ) \n 
~~ ~~ if not self and self . _exp > 0 and spec [ ] in : \n 
~~~ self = self . _rescale ( 0 , rounding ) \n 
~~ leftdigits = self . _exp + len ( self . _int ) \n 
if spec [ ] in : \n 
~~~ if not self and precision is not None : \n 
~~~ dotplace = 1 - precision \n 
~~ ~~ elif spec [ ] in : \n 
~~~ if self . _exp <= 0 and leftdigits > - 6 : \n 
~~ ~~ if dotplace < 0 : \n 
fracpart = * ( - dotplace ) + self . _int \n 
~~ elif dotplace > len ( self . _int ) : \n 
~~~ intpart = self . _int [ : dotplace ] or \n 
fracpart = self . _int [ dotplace : ] \n 
~~ exp = leftdigits - dotplace \n 
return _format_number ( self . _sign , intpart , fracpart , exp , spec ) \n 
~~ def __tojava__ ( self , java_class ) : \n 
~~~ if java_class not in ( BigDecimal , Object ) : \n 
~~~ return Py . NoConversion \n 
~~ return BigDecimal ( str ( self ) ) \n 
~~ ~~ def _dec_from_triple ( sign , coefficient , exponent , special = False ) : \n 
self = object . __new__ ( Decimal ) \n 
self . _sign = sign \n 
self . _int = coefficient \n 
self . _exp = exponent \n 
self . _is_special = special \n 
~~ _numbers . Number . register ( Decimal ) \n 
rounding_functions = [ name for name in Decimal . __dict__ . keys ( ) \n 
if name . startswith ( ) ] \n 
for name in rounding_functions : \n 
~~~ globalname = name [ 1 : ] . upper ( ) \n 
val = globals ( ) [ globalname ] \n 
Decimal . _pick_rounding_function [ val ] = name \n 
~~ del name , val , globalname , rounding_functions \n 
class _ContextManager ( object ) : \n 
def __init__ ( self , new_context ) : \n 
~~~ self . new_context = new_context . copy ( ) \n 
~~ def __enter__ ( self ) : \n 
~~~ self . saved_context = getcontext ( ) \n 
setcontext ( self . new_context ) \n 
return self . new_context \n 
~~ def __exit__ ( self , t , v , tb ) : \n 
~~~ setcontext ( self . saved_context ) \n 
~~ ~~ class Context ( object ) : \n 
def __init__ ( self , prec = None , rounding = None , \n 
traps = None , flags = None , \n 
Emin = None , Emax = None , \n 
capitals = None , _clamp = 0 , \n 
_ignored_flags = None ) : \n 
~~~ dc = DefaultContext \n 
~~ self . prec = prec if prec is not None else dc . prec \n 
self . rounding = rounding if rounding is not None else dc . rounding \n 
self . Emin = Emin if Emin is not None else dc . Emin \n 
self . Emax = Emax if Emax is not None else dc . Emax \n 
self . capitals = capitals if capitals is not None else dc . capitals \n 
self . _clamp = _clamp if _clamp is not None else dc . _clamp \n 
if _ignored_flags is None : \n 
~~~ self . _ignored_flags = [ ] \n 
~~~ self . _ignored_flags = _ignored_flags \n 
~~ if traps is None : \n 
~~~ self . traps = dc . traps . copy ( ) \n 
~~ elif not isinstance ( traps , dict ) : \n 
~~~ self . traps = dict ( ( s , int ( s in traps ) ) for s in _signals ) \n 
~~~ self . traps = traps \n 
~~ if flags is None : \n 
~~~ self . flags = dict . fromkeys ( _signals , 0 ) \n 
~~ elif not isinstance ( flags , dict ) : \n 
~~~ self . flags = dict ( ( s , int ( s in flags ) ) for s in _signals ) \n 
~~~ self . flags = flags \n 
s = [ ] \n 
s . append ( \n 
% vars ( self ) ) \n 
names = [ f . __name__ for f , v in self . flags . items ( ) if v ] \n 
s . append ( + . join ( names ) + ) \n 
names = [ t . __name__ for t , v in self . traps . items ( ) if v ] \n 
return . join ( s ) + \n 
~~ def clear_flags ( self ) : \n 
for flag in self . flags : \n 
~~~ self . flags [ flag ] = 0 \n 
~~ ~~ def _shallow_copy ( self ) : \n 
nc = Context ( self . prec , self . rounding , self . traps , \n 
self . flags , self . Emin , self . Emax , \n 
self . capitals , self . _clamp , self . _ignored_flags ) \n 
return nc \n 
~~ def copy ( self ) : \n 
nc = Context ( self . prec , self . rounding , self . traps . copy ( ) , \n 
self . flags . copy ( ) , self . Emin , self . Emax , \n 
~~ __copy__ = copy \n 
def _raise_error ( self , condition , explanation = None , * args ) : \n 
error = _condition_map . get ( condition , condition ) \n 
if error in self . _ignored_flags : \n 
~~~ return error ( ) . handle ( self , * args ) \n 
~~ self . flags [ error ] = 1 \n 
if not self . traps [ error ] : \n 
~~~ return condition ( ) . handle ( self , * args ) \n 
~~ raise error ( explanation ) \n 
~~ def _ignore_all_flags ( self ) : \n 
return self . _ignore_flags ( * _signals ) \n 
~~ def _ignore_flags ( self , * flags ) : \n 
self . _ignored_flags = ( self . _ignored_flags + list ( flags ) ) \n 
return list ( flags ) \n 
~~ def _regard_flags ( self , * flags ) : \n 
if flags and isinstance ( flags [ 0 ] , ( tuple , list ) ) : \n 
~~~ flags = flags [ 0 ] \n 
~~ for flag in flags : \n 
~~~ self . _ignored_flags . remove ( flag ) \n 
~~ ~~ __hash__ = None \n 
def Etiny ( self ) : \n 
return int ( self . Emin - self . prec + 1 ) \n 
~~ def Etop ( self ) : \n 
return int ( self . Emax - self . prec + 1 ) \n 
~~ def _set_rounding ( self , type ) : \n 
rounding = self . rounding \n 
self . rounding = type \n 
return rounding \n 
~~ def create_decimal ( self , num = ) : \n 
if isinstance ( num , basestring ) and num != num . strip ( ) : \n 
~~~ return self . _raise_error ( ConversionSyntax , \n 
"permitted." ) \n 
~~ d = Decimal ( num , context = self ) \n 
if d . _isnan ( ) and len ( d . _int ) > self . prec - self . _clamp : \n 
~~ return d . _fix ( self ) \n 
~~ def create_decimal_from_float ( self , f ) : \n 
~~ def abs ( self , a ) : \n 
a = _convert_other ( a , raiseit = True ) \n 
return a . __abs__ ( context = self ) \n 
~~ def add ( self , a , b ) : \n 
r = a . __add__ ( b , context = self ) \n 
if r is NotImplemented : \n 
~~~ return r \n 
~~ ~~ def _apply ( self , a ) : \n 
~~~ return str ( a . _fix ( self ) ) \n 
~~ def canonical ( self , a ) : \n 
return a . canonical ( context = self ) \n 
~~ def compare ( self , a , b ) : \n 
return a . compare ( b , context = self ) \n 
~~ def compare_signal ( self , a , b ) : \n 
return a . compare_signal ( b , context = self ) \n 
~~ def compare_total ( self , a , b ) : \n 
return a . compare_total ( b ) \n 
~~ def compare_total_mag ( self , a , b ) : \n 
return a . compare_total_mag ( b ) \n 
~~ def copy_abs ( self , a ) : \n 
return a . copy_abs ( ) \n 
~~ def copy_decimal ( self , a ) : \n 
return Decimal ( a ) \n 
~~ def copy_negate ( self , a ) : \n 
return a . copy_negate ( ) \n 
~~ def copy_sign ( self , a , b ) : \n 
return a . copy_sign ( b ) \n 
~~ def divide ( self , a , b ) : \n 
r = a . __div__ ( b , context = self ) \n 
~~ ~~ def divide_int ( self , a , b ) : \n 
r = a . __floordiv__ ( b , context = self ) \n 
~~ ~~ def divmod ( self , a , b ) : \n 
r = a . __divmod__ ( b , context = self ) \n 
~~ ~~ def exp ( self , a ) : \n 
return a . exp ( context = self ) \n 
~~ def fma ( self , a , b , c ) : \n 
return a . fma ( b , c , context = self ) \n 
~~ def is_canonical ( self , a ) : \n 
return a . is_canonical ( ) \n 
~~ def is_finite ( self , a ) : \n 
return a . is_finite ( ) \n 
~~ def is_infinite ( self , a ) : \n 
return a . is_infinite ( ) \n 
~~ def is_nan ( self , a ) : \n 
return a . is_nan ( ) \n 
~~ def is_normal ( self , a ) : \n 
return a . is_normal ( context = self ) \n 
~~ def is_qnan ( self , a ) : \n 
return a . is_qnan ( ) \n 
~~ def is_signed ( self , a ) : \n 
return a . is_signed ( ) \n 
~~ def is_snan ( self , a ) : \n 
return a . is_snan ( ) \n 
~~ def is_subnormal ( self , a ) : \n 
return a . is_subnormal ( context = self ) \n 
~~ def is_zero ( self , a ) : \n 
return a . is_zero ( ) \n 
~~ def ln ( self , a ) : \n 
return a . ln ( context = self ) \n 
~~ def log10 ( self , a ) : \n 
return a . log10 ( context = self ) \n 
~~ def logb ( self , a ) : \n 
return a . logb ( context = self ) \n 
~~ def logical_and ( self , a , b ) : \n 
return a . logical_and ( b , context = self ) \n 
~~ def logical_invert ( self , a ) : \n 
return a . logical_invert ( context = self ) \n 
~~ def logical_or ( self , a , b ) : \n 
return a . logical_or ( b , context = self ) \n 
~~ def logical_xor ( self , a , b ) : \n 
return a . logical_xor ( b , context = self ) \n 
~~ def max ( self , a , b ) : \n 
return a . max ( b , context = self ) \n 
~~ def max_mag ( self , a , b ) : \n 
return a . max_mag ( b , context = self ) \n 
~~ def min ( self , a , b ) : \n 
return a . min ( b , context = self ) \n 
~~ def min_mag ( self , a , b ) : \n 
return a . min_mag ( b , context = self ) \n 
~~ def minus ( self , a ) : \n 
return a . __neg__ ( context = self ) \n 
~~ def multiply ( self , a , b ) : \n 
r = a . __mul__ ( b , context = self ) \n 
~~ ~~ def next_minus ( self , a ) : \n 
return a . next_minus ( context = self ) \n 
~~ def next_plus ( self , a ) : \n 
return a . next_plus ( context = self ) \n 
~~ def next_toward ( self , a , b ) : \n 
return a . next_toward ( b , context = self ) \n 
~~ def normalize ( self , a ) : \n 
return a . normalize ( context = self ) \n 
~~ def number_class ( self , a ) : \n 
return a . number_class ( context = self ) \n 
~~ def plus ( self , a ) : \n 
return a . __pos__ ( context = self ) \n 
~~ def power ( self , a , b , modulo = None ) : \n 
r = a . __pow__ ( b , modulo , context = self ) \n 
~~ ~~ def quantize ( self , a , b ) : \n 
return a . quantize ( b , context = self ) \n 
~~ def radix ( self ) : \n 
~~ def remainder ( self , a , b ) : \n 
r = a . __mod__ ( b , context = self ) \n 
~~ ~~ def remainder_near ( self , a , b ) : \n 
return a . remainder_near ( b , context = self ) \n 
~~ def rotate ( self , a , b ) : \n 
return a . rotate ( b , context = self ) \n 
~~ def same_quantum ( self , a , b ) : \n 
return a . same_quantum ( b ) \n 
~~ def scaleb ( self , a , b ) : \n 
return a . scaleb ( b , context = self ) \n 
~~ def shift ( self , a , b ) : \n 
return a . shift ( b , context = self ) \n 
~~ def sqrt ( self , a ) : \n 
return a . sqrt ( context = self ) \n 
~~ def subtract ( self , a , b ) : \n 
r = a . __sub__ ( b , context = self ) \n 
~~ ~~ def to_eng_string ( self , a ) : \n 
return a . to_eng_string ( context = self ) \n 
~~ def to_sci_string ( self , a ) : \n 
return a . __str__ ( context = self ) \n 
~~ def to_integral_exact ( self , a ) : \n 
return a . to_integral_exact ( context = self ) \n 
~~ def to_integral_value ( self , a ) : \n 
return a . to_integral_value ( context = self ) \n 
~~ to_integral = to_integral_value \n 
~~ class _WorkRep ( object ) : \n 
~~~ __slots__ = ( , , ) \n 
def __init__ ( self , value = None ) : \n 
~~~ if value is None : \n 
~~~ self . sign = None \n 
self . int = 0 \n 
self . exp = None \n 
~~ elif isinstance ( value , Decimal ) : \n 
~~~ self . sign = value . _sign \n 
self . int = int ( value . _int ) \n 
self . exp = value . _exp \n 
~~~ self . sign = value [ 0 ] \n 
self . int = value [ 1 ] \n 
self . exp = value [ 2 ] \n 
~~ __str__ = __repr__ \n 
~~ def _normalize ( op1 , op2 , prec = 0 ) : \n 
if op1 . exp < op2 . exp : \n 
~~~ tmp = op2 \n 
other = op1 \n 
~~~ tmp = op1 \n 
other = op2 \n 
~~ tmp_len = len ( str ( tmp . int ) ) \n 
other_len = len ( str ( other . int ) ) \n 
exp = tmp . exp + min ( - 1 , tmp_len - prec - 2 ) \n 
if other_len + other . exp - 1 < exp : \n 
~~~ other . int = 1 \n 
other . exp = exp \n 
~~ tmp . int *= 10 ** ( tmp . exp - other . exp ) \n 
tmp . exp = other . exp \n 
return op1 , op2 \n 
~~ def _nbits ( n , correction = { \n 
: 4 , : 3 , : 2 , : 2 , \n 
: 1 , : 1 , : 1 , : 1 , \n 
: 0 , : 0 , : 0 , : 0 , \n 
: 0 , : 0 , : 0 , : 0 } ) : \n 
if n < 0 : \n 
~~ hex_n = "%x" % n \n 
return 4 * len ( hex_n ) - correction [ hex_n [ 0 ] ] \n 
~~ def _sqrt_nearest ( n , a ) : \n 
if n <= 0 or a <= 0 : \n 
~~ b = 0 \n 
while a != b : \n 
~~~ b , a = a , a - - n // a >> 1 \n 
~~ return a \n 
~~ def _rshift_nearest ( x , shift ) : \n 
b , q = 1 L << shift , x >> shift \n 
return q + ( 2 * ( x & ( b - 1 ) ) + ( q & 1 ) > b ) \n 
~~ def _div_nearest ( a , b ) : \n 
q , r = divmod ( a , b ) \n 
return q + ( 2 * r + ( q & 1 ) > b ) \n 
~~ def _ilog ( x , M , L = 8 ) : \n 
y = x - M \n 
R = 0 \n 
while ( R <= L and long ( abs ( y ) ) << L - R >= M or \n 
R > L and abs ( y ) >> R - L >= M ) : \n 
~~~ y = _div_nearest ( long ( M * y ) << 1 , \n 
M + _sqrt_nearest ( M * ( M + _rshift_nearest ( y , R ) ) , M ) ) \n 
R += 1 \n 
~~ T = - int ( - 10 * len ( str ( M ) ) // ( 3 * L ) ) \n 
yshift = _rshift_nearest ( y , R ) \n 
w = _div_nearest ( M , T ) \n 
for k in xrange ( T - 1 , 0 , - 1 ) : \n 
~~~ w = _div_nearest ( M , k ) - _div_nearest ( yshift * w , M ) \n 
~~ return _div_nearest ( w * y , M ) \n 
~~ def _dlog10 ( c , e , p ) : \n 
p += 2 \n 
l = len ( str ( c ) ) \n 
f = e + l - ( e + l >= 1 ) \n 
if p > 0 : \n 
~~~ M = 10 ** p \n 
k = e + p - f \n 
if k >= 0 : \n 
~~~ c *= 10 ** k \n 
~~~ c = _div_nearest ( c , 10 ** - k ) \n 
log_d = _div_nearest ( log_d * M , log_10 ) \n 
~~ return _div_nearest ( log_tenpower + log_d , 100 ) \n 
~~ def _dlog ( c , e , p ) : \n 
~~~ k = e + p - f \n 
~~~ log_d = 0 \n 
~~ if f : \n 
~~~ extra = len ( str ( abs ( f ) ) ) - 1 \n 
if p + extra >= 0 : \n 
~~~ f_log_ten = _div_nearest ( f * _log10_digits ( p + extra ) , 10 ** extra ) \n 
~~~ f_log_ten = 0 \n 
~~ return _div_nearest ( f_log_ten + log_d , 100 ) \n 
~~ class _Log10Memoize ( object ) : \n 
~~~ self . digits = "23025850929940456840179914546843642076011014886" \n 
~~ def getdigits ( self , p ) : \n 
if p < 0 : \n 
~~ if p >= len ( self . digits ) : \n 
~~~ extra = 3 \n 
~~~ M = 10 ** ( p + extra + 2 ) \n 
digits = str ( _div_nearest ( _ilog ( 10 * M , M ) , 100 ) ) \n 
if digits [ - extra : ] != * extra : \n 
~~ self . digits = digits . rstrip ( ) [ : - 1 ] \n 
~~ return int ( self . digits [ : p + 1 ] ) \n 
~~ ~~ _log10_digits = _Log10Memoize ( ) . getdigits \n 
def _iexp ( x , M , L = 8 ) : \n 
R = _nbits ( ( long ( x ) << L ) // M ) \n 
T = - int ( - 10 * len ( str ( M ) ) // ( 3 * L ) ) \n 
y = _div_nearest ( x , T ) \n 
Mshift = long ( M ) << R \n 
for i in xrange ( T - 1 , 0 , - 1 ) : \n 
~~~ y = _div_nearest ( x * ( Mshift + y ) , Mshift * i ) \n 
~~ for k in xrange ( R - 1 , - 1 , - 1 ) : \n 
~~~ Mshift = long ( M ) << ( k + 2 ) \n 
y = _div_nearest ( y * ( y + Mshift ) , Mshift ) \n 
~~ return M + y \n 
~~ def _dexp ( c , e , p ) : \n 
extra = max ( 0 , e + len ( str ( c ) ) - 1 ) \n 
q = p + extra \n 
shift = e + q \n 
~~~ cshift = c * 10 ** shift \n 
~~~ cshift = c // 10 ** - shift \n 
~~ quot , rem = divmod ( cshift , _log10_digits ( q ) ) \n 
rem = _div_nearest ( rem , 10 ** extra ) \n 
return _div_nearest ( _iexp ( rem , 10 ** p ) , 1000 ) , quot - p + 3 \n 
~~ def _dpower ( xc , xe , yc , ye , p ) : \n 
b = len ( str ( abs ( yc ) ) ) + ye \n 
lxc = _dlog ( xc , xe , p + b + 1 ) \n 
shift = ye - b \n 
~~~ pc = lxc * yc * 10 ** shift \n 
~~~ pc = _div_nearest ( lxc * yc , 10 ** - shift ) \n 
~~ if pc == 0 : \n 
~~~ coeff , exp = 10 ** ( p - 1 ) + 1 , 1 - p \n 
~~~ coeff , exp = 10 ** p - 1 , - p \n 
~~~ coeff , exp = _dexp ( pc , - ( p + 1 ) , p + 1 ) \n 
coeff = _div_nearest ( coeff , 10 ) \n 
~~ return coeff , exp \n 
~~ def _log10_lb ( c , correction = { \n 
: 100 , : 70 , : 53 , : 40 , : 31 , \n 
: 23 , : 16 , : 10 , : 5 } ) : \n 
if c <= 0 : \n 
~~ str_c = str ( c ) \n 
return 100 * len ( str_c ) - correction [ str_c [ 0 ] ] \n 
~~ def _convert_other ( other , raiseit = False , allow_float = False ) : \n 
if isinstance ( other , Decimal ) : \n 
~~ if isinstance ( other , ( int , long ) ) : \n 
~~ if allow_float and isinstance ( other , float ) : \n 
~~~ return Decimal . from_float ( other ) \n 
~~ if raiseit : \n 
~~ return NotImplemented \n 
~~ DefaultContext = Context ( \n 
prec = 28 , rounding = ROUND_HALF_EVEN , \n 
traps = [ DivisionByZero , Overflow , InvalidOperation ] , \n 
flags = [ ] , \n 
Emax = 999999999 , \n 
Emin = - 999999999 , \n 
capitals = 1 \n 
BasicContext = Context ( \n 
prec = 9 , rounding = ROUND_HALF_UP , \n 
traps = [ DivisionByZero , Overflow , InvalidOperation , Clamped , Underflow ] , \n 
ExtendedContext = Context ( \n 
prec = 9 , rounding = ROUND_HALF_EVEN , \n 
traps = [ ] , \n 
_all_zeros = re . compile ( ) . match \n 
_exact_half = re . compile ( ) . match \n 
del re \n 
~~~ import locale as _locale \n 
~~ def _parse_format_specifier ( format_spec , _localeconv = None ) : \n 
m = _parse_format_specifier_regex . match ( format_spec ) \n 
~~ format_dict = m . groupdict ( ) \n 
fill = format_dict [ ] \n 
align = format_dict [ ] \n 
format_dict [ ] = ( format_dict [ ] is not None ) \n 
if format_dict [ ] : \n 
~~~ if fill is not None : \n 
~~ if align is not None : \n 
~~ ~~ format_dict [ ] = fill or \n 
format_dict [ ] = align or \n 
if format_dict [ ] is None : \n 
~~~ format_dict [ ] = \n 
~~ format_dict [ ] = int ( format_dict [ ] or ) \n 
if format_dict [ ] is not None : \n 
~~~ format_dict [ ] = int ( format_dict [ ] ) \n 
~~ if format_dict [ ] == 0 : \n 
~~~ if format_dict [ ] is None or format_dict [ ] in : \n 
~~~ format_dict [ ] = 1 \n 
~~ ~~ if format_dict [ ] == : \n 
if _localeconv is None : \n 
~~~ _localeconv = _locale . localeconv ( ) \n 
~~ if format_dict [ ] is not None : \n 
~~ format_dict [ ] = _localeconv [ ] \n 
format_dict [ ] = _localeconv [ ] \n 
~~~ if format_dict [ ] is None : \n 
~~ format_dict [ ] = [ 3 , 0 ] \n 
format_dict [ ] = \n 
~~ format_dict [ ] = isinstance ( format_spec , unicode ) \n 
return format_dict \n 
~~ def _format_align ( sign , body , spec ) : \n 
minimumwidth = spec [ ] \n 
fill = spec [ ] \n 
padding = fill * ( minimumwidth - len ( sign ) - len ( body ) ) \n 
align = spec [ ] \n 
if align == : \n 
~~~ result = sign + body + padding \n 
~~ elif align == : \n 
~~~ result = padding + sign + body \n 
~~~ result = sign + padding + body \n 
~~~ half = len ( padding ) // 2 \n 
result = padding [ : half ] + sign + body + padding [ half : ] \n 
~~ if spec [ ] : \n 
~~~ result = unicode ( result ) \n 
~~ def _group_lengths ( grouping ) : \n 
from itertools import chain , repeat \n 
if not grouping : \n 
~~ elif grouping [ - 1 ] == 0 and len ( grouping ) >= 2 : \n 
~~~ return chain ( grouping [ : - 1 ] , repeat ( grouping [ - 2 ] ) ) \n 
~~ elif grouping [ - 1 ] == _locale . CHAR_MAX : \n 
~~~ return grouping [ : - 1 ] \n 
~~ ~~ def _insert_thousands_sep ( digits , spec , min_width = 1 ) : \n 
sep = spec [ ] \n 
grouping = spec [ ] \n 
groups = [ ] \n 
for l in _group_lengths ( grouping ) : \n 
~~~ if l <= 0 : \n 
~~ l = min ( max ( len ( digits ) , min_width , 1 ) , l ) \n 
groups . append ( * ( l - len ( digits ) ) + digits [ - l : ] ) \n 
digits = digits [ : - l ] \n 
min_width -= l \n 
if not digits and min_width <= 0 : \n 
~~ min_width -= len ( sep ) \n 
~~~ l = max ( len ( digits ) , min_width , 1 ) \n 
~~ return sep . join ( reversed ( groups ) ) \n 
~~ def _format_sign ( is_negative , spec ) : \n 
if is_negative : \n 
~~~ return spec [ ] \n 
~~ ~~ def _format_number ( is_negative , intpart , fracpart , exp , spec ) : \n 
sign = _format_sign ( is_negative , spec ) \n 
if fracpart : \n 
~~~ fracpart = spec [ ] + fracpart \n 
~~ if exp != 0 or spec [ ] in : \n 
~~~ echar = { : , : , : , : } [ spec [ ] ] \n 
fracpart += "{0}{1:+}" . format ( echar , exp ) \n 
~~~ fracpart += \n 
~~~ min_width = spec [ ] - len ( fracpart ) - len ( sign ) \n 
~~~ min_width = 0 \n 
~~ intpart = _insert_thousands_sep ( intpart , spec , min_width ) \n 
return _format_align ( sign , intpart + fracpart , spec ) \n 
~~ _Infinity = Decimal ( ) \n 
_NegativeInfinity = Decimal ( ) \n 
_NaN = Decimal ( ) \n 
_Zero = Decimal ( 0 ) \n 
_One = Decimal ( 1 ) \n 
_NegativeOne = Decimal ( - 1 ) \n 
_SignedInfinity = ( _Infinity , _NegativeInfinity ) \n 
~~~ import doctest , sys \n 
doctest . testmod ( sys . modules [ __name__ ] ) \n 
from distutils . command . install_headers import install_headers \n 
from distutils . tests import support \n 
from test . test_support import run_unittest \n 
class InstallHeadersTestCase ( support . TempdirManager , \n 
support . LoggingSilencer , \n 
support . EnvironGuard , \n 
unittest . TestCase ) : \n 
~~~ def test_simple_run ( self ) : \n 
~~~ header_list = self . mkdtemp ( ) \n 
header1 = os . path . join ( header_list , ) \n 
header2 = os . path . join ( header_list , ) \n 
self . write_file ( header1 ) \n 
self . write_file ( header2 ) \n 
headers = [ header1 , header2 ] \n 
pkg_dir , dist = self . create_dist ( headers = headers ) \n 
cmd = install_headers ( dist ) \n 
self . assertEqual ( cmd . get_inputs ( ) , headers ) \n 
cmd . install_dir = os . path . join ( pkg_dir , ) \n 
cmd . ensure_finalized ( ) \n 
cmd . run ( ) \n 
self . assertEqual ( len ( cmd . get_outputs ( ) ) , 2 ) \n 
~~ ~~ def test_suite ( ) : \n 
~~~ return unittest . makeSuite ( InstallHeadersTestCase ) \n 
~~~ run_unittest ( test_suite ( ) ) \n 
from quopri import encodestring as _encodestring \n 
def _qencode ( s ) : \n 
~~~ enc = _encodestring ( s , quotetabs = True ) \n 
return enc . replace ( , ) \n 
~~ def _bencode ( s ) : \n 
~~ hasnewline = ( s [ - 1 ] == ) \n 
value = base64 . encodestring ( s ) \n 
if not hasnewline and value [ - 1 ] == : \n 
~~~ return value [ : - 1 ] \n 
~~ def encode_base64 ( msg ) : \n 
orig = msg . get_payload ( ) \n 
encdata = _bencode ( orig ) \n 
msg . set_payload ( encdata ) \n 
msg [ ] = \n 
~~ def encode_quopri ( msg ) : \n 
encdata = _qencode ( orig ) \n 
~~ def encode_7or8bit ( msg ) : \n 
if orig is None : \n 
~~~ msg [ ] = \n 
~~~ orig . encode ( ) \n 
~~ except UnicodeError : \n 
~~ ~~ def encode_noop ( msg ) : \n 
~~ import _codecs_cn , codecs \n 
import _multibytecodec as mbc \n 
codec = _codecs_cn . getcodec ( ) \n 
class Codec ( codecs . Codec ) : \n 
~~~ encode = codec . encode \n 
decode = codec . decode \n 
~~ class IncrementalEncoder ( mbc . MultibyteIncrementalEncoder , \n 
codecs . IncrementalEncoder ) : \n 
~~~ codec = codec \n 
~~ class IncrementalDecoder ( mbc . MultibyteIncrementalDecoder , \n 
codecs . IncrementalDecoder ) : \n 
~~ class StreamReader ( Codec , mbc . MultibyteStreamReader , codecs . StreamReader ) : \n 
~~ class StreamWriter ( Codec , mbc . MultibyteStreamWriter , codecs . StreamWriter ) : \n 
~~ def getregentry ( ) : \n 
~~~ return codecs . CodecInfo ( \n 
encode = Codec ( ) . encode , \n 
decode = Codec ( ) . decode , \n 
incrementalencoder = IncrementalEncoder , \n 
incrementaldecoder = IncrementalDecoder , \n 
streamreader = StreamReader , \n 
streamwriter = StreamWriter , \n 
import codecs , sys \n 
encode = codecs . utf_16_encode \n 
def decode ( input , errors = ) : \n 
~~~ return codecs . utf_16_decode ( input , errors , True ) \n 
~~ class IncrementalEncoder ( codecs . IncrementalEncoder ) : \n 
~~~ def __init__ ( self , errors = ) : \n 
~~~ codecs . IncrementalEncoder . __init__ ( self , errors ) \n 
self . encoder = None \n 
~~ def encode ( self , input , final = False ) : \n 
~~~ if self . encoder is None : \n 
~~~ result = codecs . utf_16_encode ( input , self . errors ) [ 0 ] \n 
if sys . byteorder == : \n 
~~~ self . encoder = codecs . utf_16_le_encode \n 
~~~ self . encoder = codecs . utf_16_be_encode \n 
~~ return self . encoder ( input , self . errors ) [ 0 ] \n 
~~~ codecs . IncrementalEncoder . reset ( self ) \n 
~~ def getstate ( self ) : \n 
~~~ return ( 2 if self . encoder is None else 0 ) \n 
~~ def setstate ( self , state ) : \n 
~~~ if state : \n 
~~~ self . encoder = None \n 
~~~ if sys . byteorder == : \n 
~~ ~~ ~~ ~~ class IncrementalDecoder ( codecs . BufferedIncrementalDecoder ) : \n 
~~~ codecs . BufferedIncrementalDecoder . __init__ ( self , errors ) \n 
self . decoder = None \n 
~~ def _buffer_decode ( self , input , errors , final ) : \n 
~~~ if self . decoder is None : \n 
~~~ ( output , consumed , byteorder ) = codecs . utf_16_ex_decode ( input , errors , 0 , final ) \n 
if byteorder == - 1 : \n 
~~~ self . decoder = codecs . utf_16_le_decode \n 
~~ elif byteorder == 1 : \n 
~~~ self . decoder = codecs . utf_16_be_decode \n 
~~ elif consumed >= 2 : \n 
~~ return ( output , consumed ) \n 
~~ return self . decoder ( input , self . errors , final ) \n 
~~~ codecs . BufferedIncrementalDecoder . reset ( self ) \n 
~~ ~~ class StreamWriter ( codecs . StreamWriter ) : \n 
~~~ def __init__ ( self , stream , errors = ) : \n 
~~~ codecs . StreamWriter . __init__ ( self , stream , errors ) \n 
~~~ codecs . StreamWriter . reset ( self ) \n 
~~ def encode ( self , input , errors = ) : \n 
~~~ result = codecs . utf_16_encode ( input , errors ) \n 
~~~ return self . encoder ( input , errors ) \n 
~~ ~~ ~~ class StreamReader ( codecs . StreamReader ) : \n 
~~~ def reset ( self ) : \n 
~~~ codecs . StreamReader . reset ( self ) \n 
~~~ del self . decode \n 
~~ ~~ def decode ( self , input , errors = ) : \n 
~~~ ( object , consumed , byteorder ) = codecs . utf_16_ex_decode ( input , errors , 0 , False ) \n 
~~~ self . decode = codecs . utf_16_le_decode \n 
~~~ self . decode = codecs . utf_16_be_decode \n 
~~ return ( object , consumed ) \n 
~~ ~~ def getregentry ( ) : \n 
encode = encode , \n 
decode = decode , \n 
from warnings import warnpy3k , warn \n 
del warnpy3k \n 
import __builtin__ \n 
import imp \n 
__all__ = [ "BasicModuleLoader" , "Hooks" , "ModuleLoader" , "FancyModuleLoader" , \n 
"BasicModuleImporter" , "ModuleImporter" , "install" , "uninstall" ] \n 
VERBOSE = 0 \n 
from imp import C_EXTENSION , PY_SOURCE , PY_COMPILED \n 
from imp import C_BUILTIN , PY_FROZEN , PKG_DIRECTORY \n 
BUILTIN_MODULE = C_BUILTIN \n 
FROZEN_MODULE = PY_FROZEN \n 
class _Verbose : \n 
~~~ def __init__ ( self , verbose = VERBOSE ) : \n 
~~ def get_verbose ( self ) : \n 
~~~ return self . verbose \n 
~~ def set_verbose ( self , verbose ) : \n 
~~ def note ( self , * args ) : \n 
~~~ if self . verbose : \n 
~~~ self . message ( * args ) \n 
~~ ~~ def message ( self , format , * args ) : \n 
~~~ print format % args \n 
~~~ print format \n 
~~ ~~ ~~ class BasicModuleLoader ( _Verbose ) : \n 
def find_module ( self , name , path = None ) : \n 
~~~ if path is None : \n 
~~~ path = [ None ] + self . default_path ( ) \n 
~~ for dir in path : \n 
~~~ stuff = self . find_module_in_dir ( name , dir ) \n 
if stuff : return stuff \n 
~~ def default_path ( self ) : \n 
~~~ return sys . path \n 
~~ def find_module_in_dir ( self , name , dir ) : \n 
~~~ if dir is None : \n 
~~~ return self . find_builtin_module ( name ) \n 
~~~ return imp . find_module ( name , [ dir ] ) \n 
~~ ~~ ~~ def find_builtin_module ( self , name ) : \n 
~~~ if imp . is_builtin ( name ) : \n 
~~~ return None , , ( , , BUILTIN_MODULE ) \n 
~~ if imp . is_frozen ( name ) : \n 
~~~ return None , , ( , , FROZEN_MODULE ) \n 
~~ def load_module ( self , name , stuff ) : \n 
~~~ file , filename , info = stuff \n 
~~~ return imp . load_module ( name , file , filename , info ) \n 
~~~ if file : file . close ( ) \n 
~~ ~~ ~~ class Hooks ( _Verbose ) : \n 
def get_suffixes ( self ) : return imp . get_suffixes ( ) \n 
def new_module ( self , name ) : return imp . new_module ( name ) \n 
def is_builtin ( self , name ) : return imp . is_builtin ( name ) \n 
def init_builtin ( self , name ) : return imp . init_builtin ( name ) \n 
def is_frozen ( self , name ) : return imp . is_frozen ( name ) \n 
def init_frozen ( self , name ) : return imp . init_frozen ( name ) \n 
def get_frozen_object ( self , name ) : return imp . get_frozen_object ( name ) \n 
def load_source ( self , name , filename , file = None ) : \n 
~~~ return imp . load_source ( name , filename , file ) \n 
~~ def load_compiled ( self , name , filename , file = None ) : \n 
~~~ return imp . load_compiled ( name , filename , file ) \n 
~~ def load_dynamic ( self , name , filename , file = None ) : \n 
~~~ return imp . load_dynamic ( name , filename , file ) \n 
~~ def load_package ( self , name , filename , file = None ) : \n 
~~~ return imp . load_module ( name , file , filename , ( "" , "" , PKG_DIRECTORY ) ) \n 
~~ def add_module ( self , name ) : \n 
~~~ d = self . modules_dict ( ) \n 
if name in d : return d [ name ] \n 
d [ name ] = m = self . new_module ( name ) \n 
~~ def modules_dict ( self ) : return sys . modules \n 
def default_path ( self ) : return sys . path \n 
def path_split ( self , x ) : return os . path . split ( x ) \n 
def path_join ( self , x , y ) : return os . path . join ( x , y ) \n 
def path_isabs ( self , x ) : return os . path . isabs ( x ) \n 
def path_exists ( self , x ) : return os . path . exists ( x ) \n 
def path_isdir ( self , x ) : return os . path . isdir ( x ) \n 
def path_isfile ( self , x ) : return os . path . isfile ( x ) \n 
def path_islink ( self , x ) : return os . path . islink ( x ) \n 
def openfile ( self , * x ) : return open ( * x ) \n 
openfile_error = IOError \n 
def listdir ( self , x ) : return os . listdir ( x ) \n 
listdir_error = os . error \n 
~~ class ModuleLoader ( BasicModuleLoader ) : \n 
def __init__ ( self , hooks = None , verbose = VERBOSE ) : \n 
~~~ BasicModuleLoader . __init__ ( self , verbose ) \n 
self . hooks = hooks or Hooks ( verbose ) \n 
~~~ return self . hooks . default_path ( ) \n 
~~ def modules_dict ( self ) : \n 
~~~ return self . hooks . modules_dict ( ) \n 
~~ def get_hooks ( self ) : \n 
~~~ return self . hooks \n 
~~ def set_hooks ( self , hooks ) : \n 
~~~ self . hooks = hooks \n 
~~ def find_builtin_module ( self , name ) : \n 
~~~ if self . hooks . is_builtin ( name ) : \n 
~~ if self . hooks . is_frozen ( name ) : \n 
~~ def find_module_in_dir ( self , name , dir , allow_packages = 1 ) : \n 
~~ if allow_packages : \n 
~~~ fullname = self . hooks . path_join ( dir , name ) \n 
if self . hooks . path_isdir ( fullname ) : \n 
~~~ stuff = self . find_module_in_dir ( "__init__" , fullname , 0 ) \n 
if stuff : \n 
~~~ file = stuff [ 0 ] \n 
if file : file . close ( ) \n 
return None , fullname , ( , , PKG_DIRECTORY ) \n 
~~ ~~ ~~ for info in self . hooks . get_suffixes ( ) : \n 
~~~ suff , mode , type = info \n 
fullname = self . hooks . path_join ( dir , name + suff ) \n 
~~~ fp = self . hooks . openfile ( fullname , mode ) \n 
return fp , fullname , info \n 
~~ except self . hooks . openfile_error : \n 
( suff , mode , type ) = info \n 
~~~ if type == BUILTIN_MODULE : \n 
~~~ return self . hooks . init_builtin ( name ) \n 
~~ if type == FROZEN_MODULE : \n 
~~~ return self . hooks . init_frozen ( name ) \n 
~~ if type == C_EXTENSION : \n 
~~~ m = self . hooks . load_dynamic ( name , filename , file ) \n 
~~ elif type == PY_SOURCE : \n 
~~~ m = self . hooks . load_source ( name , filename , file ) \n 
~~ elif type == PY_COMPILED : \n 
~~~ m = self . hooks . load_compiled ( name , filename , file ) \n 
~~ elif type == PKG_DIRECTORY : \n 
~~~ m = self . hooks . load_package ( name , filename , file ) \n 
~~ m . __file__ = filename \n 
~~ ~~ class FancyModuleLoader ( ModuleLoader ) : \n 
def load_module ( self , name , stuff ) : \n 
~~~ file , filename , ( suff , mode , type ) = stuff \n 
realfilename = filename \n 
path = None \n 
if type == PKG_DIRECTORY : \n 
~~~ initstuff = self . find_module_in_dir ( "__init__" , filename , 0 ) \n 
if not initstuff : \n 
~~ initfile , initfilename , initinfo = initstuff \n 
initsuff , initmode , inittype = initinfo \n 
if inittype not in ( PY_COMPILED , PY_SOURCE ) : \n 
~~~ if initfile : initfile . close ( ) \n 
inittype , name ) \n 
~~ path = [ filename ] \n 
file = initfile \n 
realfilename = initfilename \n 
type = inittype \n 
~~~ code = self . hooks . get_frozen_object ( name ) \n 
~~~ import marshal \n 
file . seek ( 8 ) \n 
code = marshal . load ( file ) \n 
~~~ data = file . read ( ) \n 
code = compile ( data , realfilename , ) \n 
~~~ return ModuleLoader . load_module ( self , name , stuff ) \n 
~~ m = self . hooks . add_module ( name ) \n 
if path : \n 
~~~ m . __path__ = path \n 
~~~ exec code in m . __dict__ \n 
~~~ d = self . hooks . modules_dict ( ) \n 
if name in d : \n 
~~~ del d [ name ] \n 
~~ return m \n 
~~ ~~ class BasicModuleImporter ( _Verbose ) : \n 
def __init__ ( self , loader = None , verbose = VERBOSE ) : \n 
~~~ _Verbose . __init__ ( self , verbose ) \n 
self . loader = loader or ModuleLoader ( None , verbose ) \n 
self . modules = self . loader . modules_dict ( ) \n 
~~ def get_loader ( self ) : \n 
~~~ return self . loader \n 
~~ def set_loader ( self , loader ) : \n 
~~~ self . loader = loader \n 
~~~ return self . loader . get_hooks ( ) \n 
~~~ return self . loader . set_hooks ( hooks ) \n 
~~ def import_module ( self , name , globals = { } , locals = { } , fromlist = [ ] ) : \n 
~~~ name = str ( name ) \n 
if name in self . modules : \n 
~~ stuff = self . loader . find_module ( name ) \n 
if not stuff : \n 
~~ return self . loader . load_module ( name , stuff ) \n 
~~ def reload ( self , module , path = None ) : \n 
~~~ name = str ( module . __name__ ) \n 
stuff = self . loader . find_module ( name , path ) \n 
~~ def unload ( self , module ) : \n 
~~~ del self . modules [ str ( module . __name__ ) ] \n 
~~ def install ( self ) : \n 
~~~ self . save_import_module = __builtin__ . __import__ \n 
self . save_reload = __builtin__ . reload \n 
if not hasattr ( __builtin__ , ) : \n 
~~~ __builtin__ . unload = None \n 
~~ self . save_unload = __builtin__ . unload \n 
__builtin__ . __import__ = self . import_module \n 
__builtin__ . reload = self . reload \n 
__builtin__ . unload = self . unload \n 
~~ def uninstall ( self ) : \n 
~~~ __builtin__ . __import__ = self . save_import_module \n 
__builtin__ . reload = self . save_reload \n 
__builtin__ . unload = self . save_unload \n 
if not __builtin__ . unload : \n 
~~~ del __builtin__ . unload \n 
~~ ~~ ~~ class ModuleImporter ( BasicModuleImporter ) : \n 
def import_module ( self , name , globals = None , locals = None , fromlist = None , \n 
level = - 1 ) : \n 
~~~ parent = self . determine_parent ( globals , level ) \n 
q , tail = self . find_head_package ( parent , str ( name ) ) \n 
m = self . load_tail ( q , tail ) \n 
if not fromlist : \n 
~~~ return q \n 
~~ if hasattr ( m , "__path__" ) : \n 
~~~ self . ensure_fromlist ( m , fromlist ) \n 
~~ def determine_parent ( self , globals , level = - 1 ) : \n 
~~~ if not globals or not level : \n 
~~ pkgname = globals . get ( ) \n 
if pkgname is not None : \n 
~~~ if not pkgname and level > 0 : \n 
~~~ raise ValueError , \n 
~~~ modname = globals . get ( ) \n 
if modname is None : \n 
~~ if "__path__" in globals : \n 
~~~ pkgname = modname \n 
~~~ if not in modname : \n 
~~~ if level > 0 : \n 
~~~ raise ValueError , ( \n 
~~ globals [ ] = None \n 
~~ pkgname = modname . rpartition ( ) [ 0 ] \n 
~~ globals [ ] = pkgname \n 
~~ if level > 0 : \n 
~~~ dot = len ( pkgname ) \n 
for x in range ( level , 1 , - 1 ) : \n 
~~~ dot = pkgname . rindex ( , 0 , dot ) \n 
~~ ~~ pkgname = pkgname [ : dot ] \n 
~~~ return sys . modules [ pkgname ] \n 
~~~ if level < 1 : \n 
~~ ~~ ~~ def find_head_package ( self , parent , name ) : \n 
~~~ if in name : \n 
~~~ i = name . find ( ) \n 
head = name [ : i ] \n 
tail = name [ i + 1 : ] \n 
~~~ head = name \n 
tail = "" \n 
~~ if parent : \n 
~~~ qname = "%s.%s" % ( parent . __name__ , head ) \n 
~~~ qname = head \n 
~~ q = self . import_it ( head , qname , parent ) \n 
if q : return q , tail \n 
parent = None \n 
q = self . import_it ( head , qname , parent ) \n 
~~ def load_tail ( self , q , tail ) : \n 
~~~ m = q \n 
while tail : \n 
~~~ i = tail . find ( ) \n 
if i < 0 : i = len ( tail ) \n 
head , tail = tail [ : i ] , tail [ i + 1 : ] \n 
mname = "%s.%s" % ( m . __name__ , head ) \n 
m = self . import_it ( head , mname , m ) \n 
if not m : \n 
~~ ~~ return m \n 
~~ def ensure_fromlist ( self , m , fromlist , recursive = 0 ) : \n 
~~~ for sub in fromlist : \n 
~~~ if sub == "*" : \n 
~~~ if not recursive : \n 
~~~ all = m . __all__ \n 
~~~ self . ensure_fromlist ( m , all , 1 ) \n 
~~ ~~ continue \n 
~~ if sub != "*" and not hasattr ( m , sub ) : \n 
~~~ subname = "%s.%s" % ( m . __name__ , sub ) \n 
submod = self . import_it ( sub , subname , m ) \n 
if not submod : \n 
~~ ~~ ~~ ~~ def import_it ( self , partname , fqname , parent , force_load = 0 ) : \n 
~~~ if not partname : \n 
~~~ return parent \n 
~~ if not force_load : \n 
~~~ return self . modules [ fqname ] \n 
~~~ path = parent and parent . __path__ \n 
~~ partname = str ( partname ) \n 
stuff = self . loader . find_module ( partname , path ) \n 
~~ fqname = str ( fqname ) \n 
m = self . loader . load_module ( fqname , stuff ) \n 
~~~ setattr ( parent , partname , m ) \n 
~~ def reload ( self , module ) : \n 
if not in name : \n 
~~~ return self . import_it ( name , name , None , force_load = 1 ) \n 
~~ i = name . rfind ( ) \n 
pname = name [ : i ] \n 
parent = self . modules [ pname ] \n 
return self . import_it ( name [ i + 1 : ] , name , parent , force_load = 1 ) \n 
~~ ~~ default_importer = None \n 
current_importer = None \n 
def install ( importer = None ) : \n 
~~~ global current_importer \n 
current_importer = importer or default_importer or ModuleImporter ( ) \n 
current_importer . install ( ) \n 
~~ def uninstall ( ) : \n 
current_importer . uninstall ( ) \n 
### \n 
~~ import types \n 
from modjy_exceptions import * \n 
class write_object : \n 
~~~ def __init__ ( self , ostream ) : \n 
~~~ self . ostream = ostream \n 
self . num_writes = 0 \n 
~~ def __call__ ( self , * args , ** keywords ) : \n 
~~~ if len ( args ) != 1 or not isinstance ( args [ 0 ] , types . StringTypes ) : \n 
self . ostream . flush ( ) \n 
self . num_writes += 1 \n 
~~ except Exception , x : \n 
~~~ raise ModjyIOException ( x ) \n 
import sys as _sys \n 
from cStringIO import StringIO as _StringIO \n 
__all__ = [ "pprint" , "pformat" , "isreadable" , "isrecursive" , "saferepr" , \n 
"PrettyPrinter" ] \n 
_id = id \n 
_len = len \n 
_type = type \n 
def pprint ( object , stream = None , indent = 1 , width = 80 , depth = None ) : \n 
printer = PrettyPrinter ( \n 
stream = stream , indent = indent , width = width , depth = depth ) \n 
printer . pprint ( object ) \n 
~~ def pformat ( object , indent = 1 , width = 80 , depth = None ) : \n 
return PrettyPrinter ( indent = indent , width = width , depth = depth ) . pformat ( object ) \n 
~~ def saferepr ( object ) : \n 
return _safe_repr ( object , { } , None , 0 ) [ 0 ] \n 
~~ def isreadable ( object ) : \n 
return _safe_repr ( object , { } , None , 0 ) [ 1 ] \n 
~~ def isrecursive ( object ) : \n 
return _safe_repr ( object , { } , None , 0 ) [ 2 ] \n 
~~ def _sorted ( iterable ) : \n 
~~~ with warnings . catch_warnings ( ) : \n 
~~~ if _sys . py3kwarning : \n 
~~ return sorted ( iterable ) \n 
~~ ~~ class PrettyPrinter : \n 
~~~ def __init__ ( self , indent = 1 , width = 80 , depth = None , stream = None ) : \n 
indent = int ( indent ) \n 
width = int ( width ) \n 
self . _depth = depth \n 
self . _indent_per_level = indent \n 
self . _width = width \n 
if stream is not None : \n 
~~~ self . _stream = _sys . stdout \n 
~~ ~~ def pprint ( self , object ) : \n 
~~~ self . _format ( object , self . _stream , 0 , 0 , { } , 0 ) \n 
self . _stream . write ( "\\n" ) \n 
~~ def pformat ( self , object ) : \n 
~~~ sio = _StringIO ( ) \n 
self . _format ( object , sio , 0 , 0 , { } , 0 ) \n 
return sio . getvalue ( ) \n 
~~ def isrecursive ( self , object ) : \n 
~~~ return self . format ( object , { } , 0 , 0 ) [ 2 ] \n 
~~ def isreadable ( self , object ) : \n 
~~~ s , readable , recursive = self . format ( object , { } , 0 , 0 ) \n 
return readable and not recursive \n 
~~ def _format ( self , object , stream , indent , allowance , context , level ) : \n 
~~~ level = level + 1 \n 
objid = _id ( object ) \n 
if objid in context : \n 
~~~ stream . write ( _recursion ( object ) ) \n 
self . _recursive = True \n 
self . _readable = False \n 
~~ rep = self . _repr ( object , context , level - 1 ) \n 
typ = _type ( object ) \n 
sepLines = _len ( rep ) > ( self . _width - 1 - indent - allowance ) \n 
write = stream . write \n 
if self . _depth and level > self . _depth : \n 
~~~ write ( rep ) \n 
~~ r = getattr ( typ , "__repr__" , None ) \n 
if issubclass ( typ , dict ) and r is dict . __repr__ : \n 
~~~ write ( ) \n 
if self . _indent_per_level > 1 : \n 
~~~ write ( ( self . _indent_per_level - 1 ) * ) \n 
~~ length = _len ( object ) \n 
if length : \n 
~~~ context [ objid ] = 1 \n 
indent = indent + self . _indent_per_level \n 
items = _sorted ( object . items ( ) ) \n 
key , ent = items [ 0 ] \n 
rep = self . _repr ( key , context , level ) \n 
write ( rep ) \n 
write ( ) \n 
self . _format ( ent , stream , indent + _len ( rep ) + 2 , \n 
allowance + 1 , context , level ) \n 
if length > 1 : \n 
~~~ for key , ent in items [ 1 : ] : \n 
~~~ rep = self . _repr ( key , context , level ) \n 
if sepLines : \n 
~~~ write ( % ( * indent , rep ) ) \n 
~~~ write ( % rep ) \n 
~~ self . _format ( ent , stream , indent + _len ( rep ) + 2 , \n 
~~ ~~ indent = indent - self . _indent_per_level \n 
del context [ objid ] \n 
~~ write ( ) \n 
~~ if ( ( issubclass ( typ , list ) and r is list . __repr__ ) or \n 
( issubclass ( typ , tuple ) and r is tuple . __repr__ ) or \n 
( issubclass ( typ , set ) and r is set . __repr__ ) or \n 
( issubclass ( typ , frozenset ) and r is frozenset . __repr__ ) \n 
~~~ length = _len ( object ) \n 
if issubclass ( typ , list ) : \n 
endchar = \n 
~~ elif issubclass ( typ , set ) : \n 
~~~ if not length : \n 
object = _sorted ( object ) \n 
indent += 4 \n 
~~ elif issubclass ( typ , frozenset ) : \n 
indent += 10 \n 
~~ if self . _indent_per_level > 1 and sepLines : \n 
~~ if length : \n 
self . _format ( object [ 0 ] , stream , indent , allowance + 1 , \n 
context , level ) \n 
~~~ for ent in object [ 1 : ] : \n 
~~~ if sepLines : \n 
~~~ write ( + * indent ) \n 
~~ self . _format ( ent , stream , indent , \n 
~~ if issubclass ( typ , tuple ) and length == 1 : \n 
~~ write ( endchar ) \n 
~~ write ( rep ) \n 
~~ def _repr ( self , object , context , level ) : \n 
~~~ repr , readable , recursive = self . format ( object , context . copy ( ) , \n 
self . _depth , level ) \n 
if not readable : \n 
~~~ self . _readable = False \n 
~~~ self . _recursive = True \n 
~~ return repr \n 
~~ def format ( self , object , context , maxlevels , level ) : \n 
return _safe_repr ( object , context , maxlevels , level ) \n 
~~ ~~ def _safe_repr ( object , context , maxlevels , level ) : \n 
~~~ typ = _type ( object ) \n 
if typ is str : \n 
~~~ if not in _sys . modules : \n 
~~~ return repr ( object ) , True , False \n 
~~ if "\'" in object and \'"\' not in object : \n 
~~~ closure = \'"\' \n 
quotes = { \'"\' : \'\\\\"\' } \n 
~~~ closure = "\'" \n 
quotes = { "\'" : "\\\\\'" } \n 
~~ qget = quotes . get \n 
sio = _StringIO ( ) \n 
write = sio . write \n 
for char in object : \n 
~~~ if char . isalpha ( ) : \n 
~~~ write ( char ) \n 
~~~ write ( qget ( char , repr ( char ) [ 1 : - 1 ] ) ) \n 
~~ ~~ return ( "%s%s%s" % ( closure , sio . getvalue ( ) , closure ) ) , True , False \n 
~~~ if not object : \n 
~~~ return "{}" , True , False \n 
~~ objid = _id ( object ) \n 
if maxlevels and level >= maxlevels : \n 
~~~ return "{...}" , False , objid in context \n 
~~ if objid in context : \n 
~~~ return _recursion ( object ) , False , True \n 
~~ context [ objid ] = 1 \n 
readable = True \n 
recursive = False \n 
components = [ ] \n 
append = components . append \n 
level += 1 \n 
saferepr = _safe_repr \n 
for k , v in _sorted ( object . items ( ) ) : \n 
~~~ krepr , kreadable , krecur = saferepr ( k , context , maxlevels , level ) \n 
vrepr , vreadable , vrecur = saferepr ( v , context , maxlevels , level ) \n 
readable = readable and kreadable and vreadable \n 
if krecur or vrecur : \n 
~~~ recursive = True \n 
~~ ~~ del context [ objid ] \n 
return "{%s}" % _commajoin ( components ) , readable , recursive \n 
~~ if ( issubclass ( typ , list ) and r is list . __repr__ ) or ( issubclass ( typ , tuple ) and r is tuple . __repr__ ) : \n 
~~~ if issubclass ( typ , list ) : \n 
~~~ return "[]" , True , False \n 
~~ format = "[%s]" \n 
~~ elif _len ( object ) == 1 : \n 
~~~ format = "(%s,)" \n 
~~~ return "()" , True , False \n 
~~ format = "(%s)" \n 
~~~ return format % "..." , False , objid in context \n 
for o in object : \n 
~~~ orepr , oreadable , orecur = _safe_repr ( o , context , maxlevels , level ) \n 
append ( orepr ) \n 
if not oreadable : \n 
~~~ readable = False \n 
~~ if orecur : \n 
return format % _commajoin ( components ) , readable , recursive \n 
~~ rep = repr ( object ) \n 
return rep , ( rep and not rep . startswith ( ) ) , False \n 
~~ def _recursion ( object ) : \n 
% ( _type ( object ) . __name__ , _id ( object ) ) ) \n 
~~ def _perfcheck ( object = None ) : \n 
~~~ import time \n 
if object is None : \n 
~~~ object = [ ( "string" , ( 1 , 2 ) , [ 3 , 4 ] , { 5 : 6 , 7 : 8 } ) ] * 100000 \n 
~~ p = PrettyPrinter ( ) \n 
t1 = time . time ( ) \n 
_safe_repr ( object , { } , None , 0 ) \n 
t2 = time . time ( ) \n 
p . pformat ( object ) \n 
t3 = time . time ( ) \n 
print "_safe_repr:" , t2 - t1 \n 
print "pformat:" , t3 - t2 \n 
~~~ _perfcheck ( ) \n 
from django . test . simple import DjangoTestSuiteRunner \n 
from django . test . utils import setup_test_environment , teardown_test_environment \n 
import xmlrunner \n 
class XMLTestRunner ( DjangoTestSuiteRunner ) : \n 
~~~ def run_tests ( self , test_labels , extra_tests = None , ** kwargs ) : \n 
setup_test_environment ( ) \n 
settings . DEBUG = False \n 
verbosity = getattr ( settings , , 1 ) \n 
if isinstance ( verbosity , bool ) : \n 
~~~ verbosity = ( 1 , 2 ) [ verbosity ] \n 
~~ descriptions = getattr ( settings , , False ) \n 
output = getattr ( settings , , ) \n 
suite = self . build_suite ( test_labels , extra_tests ) \n 
old_config = self . setup_databases ( ) \n 
result = xmlrunner . XMLTestRunner ( \n 
verbosity = verbosity , descriptions = descriptions , \n 
output = output ) . run ( suite ) \n 
self . teardown_databases ( old_config ) \n 
teardown_test_environment ( ) \n 
return len ( result . failures ) + len ( result . errors ) \n 
~~ ~~ def test ( x ) : \n 
~~ assert 7 == apply ( test , ( 7 , ) ) \n 
assert 7 == apply ( test , ( ) , { : 7 } ) \n 
~~~ apply ( test , ( 1 , ) , 7 ) \n 
~~~ apply ( test , ( 1 , ) , { 7 : 3 } ) \n 
~~~ apply ( test , ( 1 , ) , None ) \n 
~~ def f ( ) : \n 
~~~ def g ( ) : \n 
from javax . servlet import Filter \n 
~~ ~~ class filter ( Filter ) : \n 
~~~ def init ( self , config ) : \n 
~~~ self . header = config . getInitParameter ( ) \n 
~~ def doFilter ( self , req , resp , chain ) : \n 
~~~ resp . setHeader ( self . header , "Yup" ) \n 
chain . doFilter ( req , resp ) \n 
from test import test_support \n 
class BufferTests ( unittest . TestCase ) : \n 
~~~ def test_extended_getslice ( self ) : \n 
~~~ s = "" . join ( chr ( c ) for c in list ( range ( 255 , - 1 , - 1 ) ) ) \n 
b = buffer ( s ) \n 
indices = ( 0 , None , 1 , 3 , 19 , 300 , - 1 , - 2 , - 31 , - 300 ) \n 
for start in indices : \n 
~~~ for stop in indices : \n 
~~~ for step in indices [ 1 : ] : \n 
~~~ self . assertEqual ( b [ start : stop : step ] , \n 
s [ start : stop : step ] ) \n 
~~ ~~ ~~ ~~ ~~ def test_main ( ) : \n 
DeprecationWarning ) ) : \n 
~~~ test_support . run_unittest ( BufferTests ) \n 
~~~ test_main ( ) \n 
from random import random \n 
from math import atan2 , isnan , copysign \n 
INF = float ( "inf" ) \n 
NAN = float ( "nan" ) \n 
class ComplexTest ( unittest . TestCase ) : \n 
~~~ def assertAlmostEqual ( self , a , b ) : \n 
~~~ if isinstance ( a , complex ) : \n 
~~~ if isinstance ( b , complex ) : \n 
~~~ unittest . TestCase . assertAlmostEqual ( self , a . real , b . real ) \n 
unittest . TestCase . assertAlmostEqual ( self , a . imag , b . imag ) \n 
~~~ unittest . TestCase . assertAlmostEqual ( self , a . real , b ) \n 
unittest . TestCase . assertAlmostEqual ( self , a . imag , 0. ) \n 
~~~ unittest . TestCase . assertAlmostEqual ( self , a , b . real ) \n 
unittest . TestCase . assertAlmostEqual ( self , 0. , b . imag ) \n 
~~~ unittest . TestCase . assertAlmostEqual ( self , a , b ) \n 
~~ ~~ ~~ def assertCloseAbs ( self , x , y , eps = 1e-9 ) : \n 
if abs ( x ) > abs ( y ) : \n 
~~~ x , y = y , x \n 
~~ if y == 0 : \n 
~~~ return abs ( x ) < eps \n 
~~ if x == 0 : \n 
~~~ return abs ( y ) < eps \n 
~~ self . assertTrue ( abs ( ( x - y ) / y ) < eps ) \n 
~~ def assertFloatsAreIdentical ( self , x , y ) : \n 
msg = \n 
if isnan ( x ) or isnan ( y ) : \n 
~~~ if isnan ( x ) and isnan ( y ) : \n 
~~ ~~ elif x == y : \n 
~~~ if x != 0.0 : \n 
~~ elif copysign ( 1.0 , x ) == copysign ( 1.0 , y ) : \n 
~~~ msg += \n 
~~ ~~ self . fail ( msg . format ( x , y ) ) \n 
~~ def assertClose ( self , x , y , eps = 1e-9 ) : \n 
self . assertCloseAbs ( x . real , y . real , eps ) \n 
self . assertCloseAbs ( x . imag , y . imag , eps ) \n 
~~ def check_div ( self , x , y ) : \n 
z = x * y \n 
if x != 0 : \n 
~~~ q = z / x \n 
self . assertClose ( q , y ) \n 
q = z . __div__ ( x ) \n 
q = z . __truediv__ ( x ) \n 
~~ if y != 0 : \n 
~~~ q = z / y \n 
self . assertClose ( q , x ) \n 
q = z . __div__ ( y ) \n 
q = z . __truediv__ ( y ) \n 
~~ ~~ def test_div ( self ) : \n 
~~~ simple_real = [ float ( i ) for i in xrange ( - 5 , 6 ) ] \n 
simple_complex = [ complex ( x , y ) for x in simple_real for y in simple_real ] \n 
for x in simple_complex : \n 
~~~ for y in simple_complex : \n 
~~~ self . check_div ( x , y ) \n 
~~ ~~ self . check_div ( complex ( 1e200 , 1e200 ) , 1 + 0j ) \n 
self . check_div ( complex ( 1e-200 , 1e-200 ) , 1 + 0j ) \n 
for i in xrange ( 100 ) : \n 
~~~ self . check_div ( complex ( random ( ) , random ( ) ) , \n 
complex ( random ( ) , random ( ) ) ) \n 
~~ self . assertRaises ( ZeroDivisionError , complex . __div__ , 1 + 1j , 0 + 0j ) \n 
~~ def test_truediv ( self ) : \n 
~~~ self . assertAlmostEqual ( complex . __truediv__ ( 2 + 0j , 1 + 1j ) , 1 - 1j ) \n 
self . assertRaises ( ZeroDivisionError , complex . __truediv__ , 1 + 1j , 0 + 0j ) \n 
~~ def test_floordiv ( self ) : \n 
~~~ self . assertAlmostEqual ( complex . __floordiv__ ( 3 + 0j , 1.5 + 0j ) , 2 ) \n 
self . assertRaises ( ZeroDivisionError , complex . __floordiv__ , 3 + 0j , 0 + 0j ) \n 
~~ def test_coerce ( self ) : \n 
~~~ self . assertRaises ( OverflowError , complex . __coerce__ , 1 + 1j , 1 L << 10000 ) \n 
~~ def test_no_implicit_coerce ( self ) : \n 
~~~ class A ( object ) : \n 
~~~ def __coerce__ ( self , other ) : \n 
~~~ raise RuntimeError \n 
~~ __hash__ = None \n 
def __cmp__ ( self , other ) : \n 
~~ ~~ a = A ( ) \n 
self . assertRaises ( TypeError , lambda : a + 2.0j ) \n 
self . assertTrue ( a < 2.0j ) \n 
def test_richcompare ( self ) : \n 
~~~ self . assertEqual ( complex . __eq__ ( 1 + 1j , 1 L << 10000 ) , False ) \n 
self . assertEqual ( complex . __lt__ ( 1 + 1j , None ) , NotImplemented ) \n 
self . assertIs ( complex . __eq__ ( 1 + 1j , 1 + 1j ) , True ) \n 
self . assertIs ( complex . __eq__ ( 1 + 1j , 2 + 2j ) , False ) \n 
self . assertIs ( complex . __ne__ ( 1 + 1j , 1 + 1j ) , False ) \n 
self . assertIs ( complex . __ne__ ( 1 + 1j , 2 + 2j ) , True ) \n 
self . assertRaises ( TypeError , complex . __lt__ , 1 + 1j , 2 + 2j ) \n 
self . assertRaises ( TypeError , complex . __le__ , 1 + 1j , 2 + 2j ) \n 
self . assertRaises ( TypeError , complex . __gt__ , 1 + 1j , 2 + 2j ) \n 
self . assertRaises ( TypeError , complex . __ge__ , 1 + 1j , 2 + 2j ) \n 
def test_richcompare_boundaries ( self ) : \n 
~~~ def check ( n , deltas , is_equal , imag = 0.0 ) : \n 
~~~ for delta in deltas : \n 
~~~ i = n + delta \n 
z = complex ( i , imag ) \n 
self . assertIs ( complex . __eq__ ( z , i ) , is_equal ( delta ) ) \n 
self . assertIs ( complex . __ne__ ( z , i ) , not is_equal ( delta ) ) \n 
~~ ~~ for i in range ( 1 , 10 ) : \n 
~~~ pow = 52 + i \n 
mult = 2 ** i \n 
check ( 2 ** pow , range ( 1 , 101 ) , lambda delta : delta % mult == 0 ) \n 
check ( 2 ** pow , range ( 1 , 101 ) , lambda delta : False , float ( i ) ) \n 
~~ check ( 2 ** 53 , range ( - 100 , 0 ) , lambda delta : True ) \n 
~~ def test_mod ( self ) : \n 
~~~ self . assertRaises ( ZeroDivisionError , ( 1 + 1j ) . __mod__ , 0 + 0j ) \n 
a = 3.33 + 4.43j \n 
~~~ a % 0 \n 
~~ except ZeroDivisionError : \n 
~~ ~~ def test_divmod ( self ) : \n 
~~~ self . assertRaises ( ZeroDivisionError , divmod , 1 + 1j , 0 + 0j ) \n 
~~ def test_pow ( self ) : \n 
~~~ self . assertAlmostEqual ( pow ( 1 + 1j , 0 + 0j ) , 1.0 ) \n 
self . assertAlmostEqual ( pow ( 0 + 0j , 2 + 0j ) , 0.0 ) \n 
self . assertRaises ( ZeroDivisionError , pow , 0 + 0j , 1j ) \n 
self . assertAlmostEqual ( pow ( 1j , - 1 ) , 1 / 1j ) \n 
self . assertAlmostEqual ( pow ( 1j , 200 ) , 1 ) \n 
self . assertRaises ( ValueError , pow , 1 + 1j , 1 + 1j , 1 + 1j ) \n 
self . assertEqual ( a ** 0j , 1 ) \n 
self . assertEqual ( a ** 0. + 0.j , 1 ) \n 
self . assertEqual ( 3j ** 0j , 1 ) \n 
self . assertEqual ( 3j ** 0 , 1 ) \n 
~~~ 0j ** a \n 
~~~ 0j ** ( 3 - 2j ) \n 
~~ self . assertEqual ( a ** 105 , a ** 105 ) \n 
self . assertEqual ( a ** - 105 , a ** - 105 ) \n 
self . assertEqual ( a ** - 30 , a ** - 30 ) \n 
self . assertEqual ( 0.0j ** 0 , 1 ) \n 
b = 5.1 + 2.3j \n 
self . assertRaises ( ValueError , pow , a , b , 0 ) \n 
~~ def test_boolcontext ( self ) : \n 
~~~ for i in xrange ( 100 ) : \n 
~~~ self . assertTrue ( complex ( random ( ) + 1e-6 , random ( ) + 1e-6 ) ) \n 
~~ self . assertTrue ( not complex ( 0.0 , 0.0 ) ) \n 
~~ def test_conjugate ( self ) : \n 
~~~ self . assertClose ( complex ( 5.3 , 9.8 ) . conjugate ( ) , 5.3 - 9.8j ) \n 
~~ def test_constructor ( self ) : \n 
~~~ class OS : \n 
~~~ def __init__ ( self , value ) : self . value = value \n 
def __complex__ ( self ) : return self . value \n 
~~ class NS ( object ) : \n 
~~ self . assertEqual ( complex ( OS ( 1 + 10j ) ) , 1 + 10j ) \n 
self . assertEqual ( complex ( NS ( 1 + 10j ) ) , 1 + 10j ) \n 
self . assertRaises ( TypeError , complex , OS ( None ) ) \n 
self . assertRaises ( TypeError , complex , NS ( None ) ) \n 
self . assertAlmostEqual ( complex ( "1+10j" ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 10 ) , 10 + 0j ) \n 
self . assertAlmostEqual ( complex ( 10.0 ) , 10 + 0j ) \n 
self . assertAlmostEqual ( complex ( 10 L ) , 10 + 0j ) \n 
self . assertAlmostEqual ( complex ( 10 + 0j ) , 10 + 0j ) \n 
self . assertAlmostEqual ( complex ( 1 , 10 ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 1 , 10 L ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 1 , 10.0 ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 1 L , 10 ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 1 L , 10 L ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 1 L , 10.0 ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 1.0 , 10 ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 1.0 , 10 L ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 1.0 , 10.0 ) , 1 + 10j ) \n 
self . assertAlmostEqual ( complex ( 3.14 + 0j ) , 3.14 + 0j ) \n 
self . assertAlmostEqual ( complex ( 3.14 ) , 3.14 + 0j ) \n 
self . assertAlmostEqual ( complex ( 314 ) , 314.0 + 0j ) \n 
self . assertAlmostEqual ( complex ( 314 L ) , 314.0 + 0j ) \n 
self . assertAlmostEqual ( complex ( 3.14 + 0j , 0j ) , 3.14 + 0j ) \n 
self . assertAlmostEqual ( complex ( 3.14 , 0.0 ) , 3.14 + 0j ) \n 
self . assertAlmostEqual ( complex ( 314 , 0 ) , 314.0 + 0j ) \n 
self . assertAlmostEqual ( complex ( 314 L , 0 L ) , 314.0 + 0j ) \n 
self . assertAlmostEqual ( complex ( 0j , 3.14j ) , - 3.14 + 0j ) \n 
self . assertAlmostEqual ( complex ( 0.0 , 3.14j ) , - 3.14 + 0j ) \n 
self . assertAlmostEqual ( complex ( 0j , 3.14 ) , 3.14j ) \n 
self . assertAlmostEqual ( complex ( 0.0 , 3.14 ) , 3.14j ) \n 
self . assertAlmostEqual ( complex ( "1" ) , 1 + 0j ) \n 
self . assertAlmostEqual ( complex ( "1j" ) , 1j ) \n 
self . assertAlmostEqual ( complex ( ) , 0 ) \n 
self . assertAlmostEqual ( complex ( "-1" ) , - 1 ) \n 
self . assertAlmostEqual ( complex ( "+1" ) , + 1 ) \n 
self . assertAlmostEqual ( complex ( "3.14+1J" ) , 3.14 + 1j ) \n 
self . assertAlmostEqual ( complex ( "J" ) , 1j ) \n 
self . assertAlmostEqual ( complex ( "+J" ) , 1j ) \n 
self . assertAlmostEqual ( complex ( ) , 0.0 + 0.0j ) \n 
self . assertAlmostEqual ( complex ( ) , 0.0 - 0.0j ) \n 
self . assertAlmostEqual ( complex ( ) , - 0.0 + 0.0j ) \n 
class complex2 ( complex ) : pass \n 
self . assertAlmostEqual ( complex ( complex2 ( 1 + 1j ) ) , 1 + 1j ) \n 
self . assertAlmostEqual ( complex ( real = 17 , imag = 23 ) , 17 + 23j ) \n 
self . assertAlmostEqual ( complex ( real = 17 + 23j ) , 17 + 23j ) \n 
self . assertAlmostEqual ( complex ( real = 17 + 23j , imag = 23 ) , 17 + 46j ) \n 
self . assertAlmostEqual ( complex ( real = 1 + 2j , imag = 3 + 4j ) , - 3 + 5j ) \n 
def split_zeros ( x ) : \n 
return atan2 ( x , - 1. ) \n 
~~ self . assertEqual ( split_zeros ( complex ( 1. , 0. ) . imag ) , split_zeros ( 0. ) ) \n 
self . assertEqual ( split_zeros ( complex ( 0. , 1. ) . real ) , split_zeros ( 0. ) ) \n 
self . assertEqual ( split_zeros ( complex ( - 0. , 1. ) . real ) , split_zeros ( - 0. ) ) \n 
c = 3.14 + 1j \n 
self . assertTrue ( complex ( c ) is c ) \n 
del c \n 
self . assertRaises ( TypeError , complex , "1" , "1" ) \n 
self . assertRaises ( TypeError , complex , 1 , "1" ) \n 
if test_support . have_unicode : \n 
~~ self . assertRaises ( ValueError , complex , ) \n 
self . assertRaises ( TypeError , int , 5 + 3j ) \n 
self . assertRaises ( TypeError , long , 5 + 3j ) \n 
self . assertRaises ( TypeError , float , 5 + 3j ) \n 
self . assertRaises ( ValueError , complex , "" ) \n 
self . assertRaises ( TypeError , complex , None ) \n 
self . assertRaises ( ValueError , complex , "\\0" ) \n 
self . assertRaises ( ValueError , complex , "3\\09" ) \n 
self . assertRaises ( TypeError , complex , "1" , "2" ) \n 
self . assertRaises ( TypeError , complex , "1" , 42 ) \n 
self . assertRaises ( TypeError , complex , 1 , "2" ) \n 
self . assertRaises ( ValueError , complex , "1+" ) \n 
self . assertRaises ( ValueError , complex , "1+1j+1j" ) \n 
self . assertRaises ( ValueError , complex , "--" ) \n 
self . assertRaises ( ValueError , complex , "(1+2j" ) \n 
self . assertRaises ( ValueError , complex , "1+2j)" ) \n 
self . assertRaises ( ValueError , complex , "1+(2j)" ) \n 
self . assertRaises ( ValueError , complex , "(1+2j)123" ) \n 
~~~ self . assertRaises ( ValueError , complex , unicode ( "x" ) ) \n 
~~ class EvilExc ( Exception ) : \n 
~~ class evilcomplex : \n 
~~~ def __complex__ ( self ) : \n 
~~~ raise EvilExc \n 
~~ ~~ self . assertRaises ( EvilExc , complex , evilcomplex ( ) ) \n 
class float2 : \n 
~~~ def __init__ ( self , value ) : \n 
~~~ self . value = value \n 
~~~ return self . value \n 
~~ ~~ self . assertAlmostEqual ( complex ( float2 ( 42. ) ) , 42 ) \n 
self . assertAlmostEqual ( complex ( real = float2 ( 17. ) , imag = float2 ( 23. ) ) , 17 + 23j ) \n 
self . assertRaises ( TypeError , complex , float2 ( None ) ) \n 
class complex0 ( complex ) : \n 
def __complex__ ( self ) : \n 
~~~ return 42j \n 
~~ ~~ class complex1 ( complex ) : \n 
def __new__ ( self , value = 0j ) : \n 
~~~ return complex . __new__ ( self , 2 * value ) \n 
~~ ~~ class complex2 ( complex ) : \n 
~~ ~~ self . assertAlmostEqual ( complex ( complex0 ( 1j ) ) , 42j ) \n 
self . assertAlmostEqual ( complex ( complex1 ( 1j ) ) , 2j ) \n 
self . assertRaises ( TypeError , complex , complex2 ( 1j ) ) \n 
~~ def test_subclass ( self ) : \n 
~~~ class xcomplex ( complex ) : \n 
~~~ def __add__ ( self , other ) : \n 
~~~ return xcomplex ( complex ( self ) + other ) \n 
def __sub__ ( self , other ) : \n 
~~ __rsub__ = __sub__ \n 
def __mul__ ( self , other ) : \n 
~~~ return xcomplex ( complex ( self ) * other ) \n 
def __div__ ( self , other ) : \n 
~~~ return xcomplex ( complex ( self ) / other ) \n 
~~ def __rdiv__ ( self , other ) : \n 
~~~ return xcomplex ( other / complex ( self ) ) \n 
~~ __truediv__ = __div__ \n 
__rtruediv__ = __rdiv__ \n 
def __floordiv__ ( self , other ) : \n 
~~~ return xcomplex ( complex ( self ) // other ) \n 
~~ def __rfloordiv__ ( self , other ) : \n 
~~~ return xcomplex ( other // complex ( self ) ) \n 
~~ def __pow__ ( self , other ) : \n 
~~~ return xcomplex ( complex ( self ) ** other ) \n 
~~ def __rpow__ ( self , other ) : \n 
~~~ return xcomplex ( other ** complex ( self ) ) \n 
~~ def __mod__ ( self , other ) : \n 
~~~ return xcomplex ( complex ( self ) % other ) \n 
~~ def __rmod__ ( self , other ) : \n 
~~~ return xcomplex ( other % complex ( self ) ) \n 
~~ ~~ infix_binops = ( , , , , , , ) \n 
xcomplex_values = ( xcomplex ( 1 ) , xcomplex ( 123.0 ) , \n 
xcomplex ( - 10 + 2j ) , xcomplex ( 3 + 187j ) , \n 
xcomplex ( 3 - 78j ) ) \n 
test_values = ( 1 , 123.0 , 10 - 19j , xcomplex ( 1 + 2j ) , \n 
xcomplex ( 1 + 87j ) , xcomplex ( 10 + 90j ) ) \n 
for op in infix_binops : \n 
~~~ for x in xcomplex_values : \n 
~~~ for y in test_values : \n 
~~~ a = % op \n 
b = % op \n 
self . assertTrue ( type ( eval ( a ) ) is type ( eval ( b ) ) is xcomplex ) \n 
~~ ~~ ~~ ~~ def test_hash ( self ) : \n 
~~~ for x in xrange ( - 30 , 30 ) : \n 
~~~ self . assertEqual ( hash ( x ) , hash ( complex ( x , 0 ) ) ) \n 
self . assertEqual ( hash ( x ) , hash ( complex ( x , 0. ) ) ) \n 
~~ ~~ def test_abs ( self ) : \n 
~~~ nums = [ complex ( x / 3. , y / 7. ) for x in xrange ( - 9 , 9 ) for y in xrange ( - 9 , 9 ) ] \n 
for num in nums : \n 
~~~ self . assertAlmostEqual ( ( num . real ** 2 + num . imag ** 2 ) ** 0.5 , abs ( num ) ) \n 
def test_repr ( self ) : \n 
~~~ self . assertEqual ( repr ( 1 + 6j ) , ) \n 
self . assertEqual ( repr ( 1 - 6j ) , ) \n 
self . assertNotEqual ( repr ( - ( 1 + 0j ) ) , ) \n 
self . assertEqual ( 1 - 6j , complex ( repr ( 1 - 6j ) ) ) \n 
self . assertEqual ( 1 + 6j , complex ( repr ( 1 + 6j ) ) ) \n 
self . assertEqual ( - 6j , complex ( repr ( - 6j ) ) ) \n 
self . assertEqual ( 6j , complex ( repr ( 6j ) ) ) \n 
self . assertEqual ( repr ( complex ( 1. , INF ) ) , "(1+infj)" ) \n 
self . assertEqual ( repr ( complex ( 1. , - INF ) ) , "(1-infj)" ) \n 
self . assertEqual ( repr ( complex ( INF , 1 ) ) , "(inf+1j)" ) \n 
self . assertEqual ( repr ( complex ( - INF , INF ) ) , "(-inf+infj)" ) \n 
self . assertEqual ( repr ( complex ( NAN , 1 ) ) , "(nan+1j)" ) \n 
self . assertEqual ( repr ( complex ( 1 , NAN ) ) , "(1+nanj)" ) \n 
self . assertEqual ( repr ( complex ( NAN , NAN ) ) , "(nan+nanj)" ) \n 
self . assertEqual ( repr ( complex ( 0 , INF ) ) , "infj" ) \n 
self . assertEqual ( repr ( complex ( 0 , - INF ) ) , "-infj" ) \n 
self . assertEqual ( repr ( complex ( 0 , NAN ) ) , "nanj" ) \n 
~~ def test_neg ( self ) : \n 
~~~ self . assertEqual ( - ( 1 + 6j ) , - 1 - 6j ) \n 
~~ def test_file ( self ) : \n 
~~~ a = 3.33 + 4.43j \n 
fo = None \n 
~~~ fo = open ( test_support . TESTFN , "wb" ) \n 
print >> fo , a , b \n 
fo . close ( ) \n 
fo = open ( test_support . TESTFN , "rb" ) \n 
~~~ if ( fo is not None ) and ( not fo . closed ) : \n 
~~~ fo . close ( ) \n 
~~ test_support . unlink ( test_support . TESTFN ) \n 
def test_getnewargs ( self ) : \n 
~~~ self . assertEqual ( ( 1 + 2j ) . __getnewargs__ ( ) , ( 1.0 , 2.0 ) ) \n 
self . assertEqual ( ( 1 - 2j ) . __getnewargs__ ( ) , ( 1.0 , - 2.0 ) ) \n 
self . assertEqual ( ( 2j ) . __getnewargs__ ( ) , ( 0.0 , 2.0 ) ) \n 
self . assertEqual ( ( - 0j ) . __getnewargs__ ( ) , ( 0.0 , - 0.0 ) ) \n 
self . assertEqual ( complex ( 0 , INF ) . __getnewargs__ ( ) , ( 0.0 , INF ) ) \n 
self . assertEqual ( complex ( INF , 0 ) . __getnewargs__ ( ) , ( INF , 0.0 ) ) \n 
~~ if float . __getformat__ ( "double" ) . startswith ( "IEEE" ) : \n 
~~~ @ unittest . skipIf ( test_support . is_jython , \n 
def test_plus_minus_0j ( self ) : \n 
~~~ z1 , z2 = 0j , - 0j \n 
self . assertEqual ( atan2 ( z1 . imag , - 1. ) , atan2 ( 0. , - 1. ) ) \n 
self . assertEqual ( atan2 ( z2 . imag , - 1. ) , atan2 ( - 0. , - 1. ) ) \n 
~~ ~~ @ unittest . skipUnless ( float . __getformat__ ( "double" ) . startswith ( "IEEE" ) , \n 
def test_overflow ( self ) : \n 
~~~ self . assertEqual ( complex ( "1e500" ) , complex ( INF , 0.0 ) ) \n 
self . assertEqual ( complex ( "-1e500j" ) , complex ( 0.0 , - INF ) ) \n 
self . assertEqual ( complex ( "-1e500+1.8e308j" ) , complex ( - INF , INF ) ) \n 
~~ @ unittest . skipUnless ( float . __getformat__ ( "double" ) . startswith ( "IEEE" ) , \n 
def test_repr_roundtrip ( self ) : \n 
~~~ vals = [ 0.0 , 1e-500 , 1e-315 , 1e-200 , 0.0123 , 3.1415 , 1e50 , INF , NAN ] \n 
vals += [ - v for v in vals ] \n 
for x in vals : \n 
~~~ for y in vals : \n 
~~~ z = complex ( x , y ) \n 
roundtrip = complex ( repr ( z ) ) \n 
self . assertFloatsAreIdentical ( z . real , roundtrip . real ) \n 
self . assertFloatsAreIdentical ( z . imag , roundtrip . imag ) \n 
~~ ~~ inf , nan = float ( ) , float ( ) \n 
infj , nanj = complex ( 0.0 , inf ) , complex ( 0.0 , nan ) \n 
roundtrip = eval ( repr ( z ) ) \n 
self . assertFloatsAreIdentical ( 0.0 + z . real , \n 
0.0 + roundtrip . real ) \n 
self . assertFloatsAreIdentical ( 0.0 + z . imag , \n 
0.0 + roundtrip . imag ) \n 
def test_format ( self ) : \n 
~~~ self . assertEqual ( format ( 1 + 3j , ) , str ( 1 + 3j ) ) \n 
self . assertEqual ( format ( 1.5 + 3.5j , ) , str ( 1.5 + 3.5j ) ) \n 
self . assertEqual ( format ( 3j , ) , str ( 3j ) ) \n 
self . assertEqual ( format ( 3.2j , ) , str ( 3.2j ) ) \n 
self . assertEqual ( format ( 3 + 0j , ) , str ( 3 + 0j ) ) \n 
self . assertEqual ( format ( 3.2 + 0j , ) , str ( 3.2 + 0j ) ) \n 
z = 4 / 7. - 100j / 7. \n 
self . assertEqual ( format ( z , ) , str ( z ) ) \n 
z = complex ( 0.0 , 3.0 ) \n 
z = complex ( - 0.0 , 2.0 ) \n 
self . assertEqual ( format ( 1 + 3j , ) , ) \n 
self . assertEqual ( format ( 3j , ) , ) \n 
self . assertEqual ( format ( 1.5 + 3.5j , ) , ) \n 
self . assertEqual ( format ( 1.5 - 3.5j , ) , ) \n 
self . assertEqual ( format ( - 1.5 + 3.5j , ) , ) \n 
self . assertEqual ( format ( - 1.5 - 3.5j , ) , ) \n 
self . assertEqual ( format ( - 1.5 - 3.5e-20j , ) , ) \n 
self . assertEqual ( format ( - 1.5e10 - 3.5e5j , ) , ) \n 
self . assertEqual ( format ( 1.5 + 3j , ) , ) \n 
self . assertEqual ( format ( 1.123 - 3.123j , ) , ) \n 
self . assertEqual ( format ( 1.5e20 + 3j , ) , ) \n 
self . assertEqual ( format ( 1.5e21 + 3j , ) , ) \n 
self . assertEqual ( format ( 1.5e21 + 3000j , ) , ) \n 
self . assertRaises ( ValueError , ( 1.5 + 0.5j ) . __format__ , ) \n 
self . assertRaises ( ValueError , ( 1.5 + 3j ) . __format__ , ) \n 
for t in : \n 
~~~ self . assertRaises ( ValueError , ( 1.5 + 0.5j ) . __format__ , t ) \n 
~~ self . assertEqual ( . format ( 3.14159 + 2.71828j ) , ) \n 
self . assertEqual ( . format ( INF + 0j ) , ) \n 
self . assertEqual ( . format ( - INF + 0j ) , ) \n 
self . assertEqual ( . format ( complex ( INF , INF ) ) , ) \n 
self . assertEqual ( . format ( complex ( INF , - INF ) ) , ) \n 
self . assertEqual ( . format ( complex ( - INF , INF ) ) , ) \n 
self . assertEqual ( . format ( complex ( - INF , - INF ) ) , ) \n 
self . assertEqual ( . format ( complex ( NAN , 0 ) ) , ) \n 
self . assertEqual ( . format ( complex ( NAN , NAN ) ) , ) \n 
~~ ~~ def test_main ( ) : \n 
"deprecated" , DeprecationWarning ) ) : \n 
~~~ test_support . run_unittest ( ComplexTest ) \n 
class Old : \n 
~~ class New ( object ) : \n 
~~ old = Old ( ) \n 
new = New ( ) \n 
class TestDescrTestCase ( unittest . TestCase ) : \n 
~~~ def test_class_dict_is_copy ( self ) : \n 
~~~ class FooMeta ( type ) : \n 
~~~ def __new__ ( meta , name , bases , class_dict ) : \n 
~~~ cls = type . __new__ ( meta , name , bases , class_dict ) \n 
self . assert_ ( not in class_dict ) \n 
cls . foo = \n 
~~ ~~ class Foo ( object ) : \n 
~~~ __metaclass__ = FooMeta \n 
~~ ~~ def test_descr___get__ ( self ) : \n 
~~~ class Foo ( object ) : \n 
~~~ __slots__ = \n 
def hello ( self ) : \n 
~~ def hi ( self ) : \n 
~~ hi = staticmethod ( hi ) \n 
~~ foo = Foo ( ) \n 
foo . bar = \n 
self . assertEqual ( Foo . bar . __get__ ( foo ) , ) \n 
self . assertEqual ( Foo . bar . __get__ ( None , Foo ) , Foo . bar ) \n 
bound = Foo . hello . __get__ ( foo ) \n 
self . assert_ ( isinstance ( bound , types . MethodType ) ) \n 
self . assert_ ( bound . im_self is foo ) \n 
self . assertEqual ( Foo . hello . __get__ ( None , Foo ) , Foo . hello ) \n 
bound = Foo . hi . __get__ ( foo ) \n 
unbound = Foo . hi . __get__ ( None , foo ) \n 
self . assert_ ( isinstance ( unbound , types . MethodType ) ) \n 
self . assert_ ( unbound . im_self is None ) \n 
~~ def test_ints ( self ) : \n 
~~~ class C ( int ) : \n 
~~~ foo = int ( None ) \n 
~~~ foo = C ( None ) \n 
~~ ~~ def test_raising_custom_attribute_error ( self ) : \n 
~~~ class RaisesCustomMsg ( object ) : \n 
~~~ def __get__ ( self , instance , type ) : \n 
~~ ~~ class CustomAttributeError ( AttributeError ) : pass \n 
class RaisesCustomErr ( object ) : \n 
~~~ raise CustomAttributeError \n 
~~~ custom_msg = RaisesCustomMsg ( ) \n 
custom_err = RaisesCustomErr ( ) \n 
~~ self . assertRaises ( CustomAttributeError , lambda : Foo ( ) . custom_err ) \n 
~~~ Foo ( ) . custom_msg \n 
~~ ~~ def test_set_without_get ( self ) : \n 
~~~ class Descr ( object ) : \n 
~~~ def __init__ ( self , name ) : \n 
~~ def __set__ ( self , obj , value ) : \n 
~~~ obj . __dict__ [ self . name ] = value \n 
~~ ~~ descr = Descr ( "a" ) \n 
class X ( object ) : \n 
~~~ a = descr \n 
~~ x = X ( ) \n 
self . assertTrue ( x . a is descr ) \n 
x . a = 42 \n 
self . assertEqual ( x . a , 42 ) \n 
~~ ~~ class SubclassDescrTestCase ( unittest . TestCase ) : \n 
~~~ def test_subclass_cmp_right_op ( self ) : \n 
~~~ class B ( int ) : \n 
~~~ def __ge__ ( self , other ) : \n 
~~~ return "B.__ge__" \n 
~~ def __le__ ( self , other ) : \n 
~~~ return "B.__le__" \n 
~~ ~~ self . assertEqual ( B ( 1 ) >= 1 , "B.__ge__" ) \n 
self . assertEqual ( 1 >= B ( 1 ) , "B.__le__" ) \n 
class C ( object ) : \n 
~~~ return "C.__ge__" \n 
~~~ return "C.__le__" \n 
~~ ~~ self . assertEqual ( C ( ) >= 1 , "C.__ge__" ) \n 
self . assertEqual ( 1 >= C ( ) , "C.__le__" ) \n 
class D ( C ) : \n 
~~~ return "D.__ge__" \n 
~~~ return "D.__le__" \n 
~~ ~~ self . assertEqual ( D ( ) >= C ( ) , "D.__ge__" ) \n 
self . assertEqual ( C ( ) >= D ( ) , "D.__le__" ) \n 
class E ( C ) : \n 
~~ self . assertEqual ( E . __le__ , C . __le__ ) \n 
self . assertEqual ( E ( ) >= 1 , "C.__ge__" ) \n 
self . assertEqual ( 1 >= E ( ) , "C.__le__" ) \n 
self . assertEqual ( E ( ) >= C ( ) , "C.__ge__" ) \n 
~~ def test_subclass_binop ( self ) : \n 
~~~ def raises ( exc , expected , callable , * args ) : \n 
~~~ callable ( * args ) \n 
~~ except exc , msg : \n 
~~~ if str ( msg ) != expected : \n 
expected ) ) \n 
~~ ~~ class B ( object ) : \n 
~~ class C ( object ) : \n 
~~~ def __radd__ ( self , o ) : \n 
~~~ return % ( o , ) \n 
~~ def __rmul__ ( self , o ) : \n 
~~ ~~ mapping = [ ] \n 
mapping . append ( ( lambda o : + o , \n 
if test_support . is_jython : \n 
~~~ mapping . append ( ( lambda o : + o , \n 
TypeError , \n 
~~ mapping . append ( ( lambda o : [ 1 , 2 ] + o , \n 
mapping . append ( ( lambda o : ( , ) + o , \n 
mapping . append ( ( lambda o : * o , \n 
mapping . append ( ( lambda o : [ 1 , 2 ] * o , \n 
mapping . append ( ( lambda o : ( , ) * o , \n 
for func , bexc , bexc_msg , cresult in mapping : \n 
~~~ raises ( bexc , bexc_msg , lambda : func ( B ( ) ) ) \n 
self . assertEqual ( func ( C ( ) ) , cresult ) \n 
~~ ~~ def test_overriding_base_binop ( self ) : \n 
~~~ class MulBase ( object ) : \n 
~~ def __mul__ ( self , other ) : \n 
~~~ return self . value * other . value \n 
~~ def __rmul__ ( self , other ) : \n 
~~~ return other . value * self . value \n 
~~ ~~ class DoublerBase ( MulBase ) : \n 
~~~ def __mul__ ( self , other ) : \n 
~~~ return 2 * ( self . value * other . value ) \n 
~~ ~~ class AnotherDoubler ( DoublerBase ) : \n 
~~ self . assertEquals ( DoublerBase ( 2 ) * AnotherDoubler ( 3 ) , 12 ) \n 
~~ def test_oldstyle_binop_notimplemented ( self ) : \n 
~~~ class Foo : \n 
~~ class Bar ( object ) : \n 
~~~ def __radd__ ( self , other ) : \n 
~~~ return 3 \n 
~~ ~~ self . assertEqual ( Foo ( ) + Bar ( ) , 3 ) \n 
~~ def test_int_mul ( self ) : \n 
~~~ class Foo ( tuple ) : \n 
~~~ def __rmul__ ( self , other ) : \n 
~~ ~~ foo = Foo ( ) \n 
self . assertEqual ( 3.0 * foo , ) \n 
self . assertEqual ( 4 * foo , ) \n 
~~ ~~ class InPlaceTestCase ( unittest . TestCase ) : \n 
~~~ def test_iadd ( self ) : \n 
~~ def __radd__ ( self , other ) : \n 
~~ ~~ class Bar ( object ) : \n 
~~ class Baz ( object ) : \n 
~~~ def __iadd__ ( self , other ) : \n 
foo += Bar ( ) \n 
self . assertEqual ( foo , 1 ) \n 
bar = Bar ( ) \n 
bar += Foo ( ) \n 
self . assertEqual ( bar , 2 ) \n 
baz = Baz ( ) \n 
baz += Foo ( ) \n 
self . assertEqual ( baz , 2 ) \n 
~~ def test_imul ( self ) : \n 
~~~ class FooInplace ( list ) : \n 
~~~ def __imul__ ( self , other ) : \n 
~~~ return [ 1 ] \n 
~~ ~~ class Bar ( FooInplace ) : \n 
~~~ return [ 2 ] \n 
~~ ~~ foo = FooInplace ( ) \n 
foo *= 3 \n 
self . assertEqual ( foo , [ 1 ] ) \n 
foo = Bar ( [ 3 ] ) \n 
class Baz ( FooInplace ) : \n 
~~~ return [ 3 ] \n 
~~ ~~ baz = Baz ( ) \n 
baz *= 3 \n 
self . assertEqual ( baz , [ 1 ] ) \n 
~~ def test_list ( self ) : \n 
~~~ class Foo ( list ) : \n 
~~ ~~ foo = Foo ( [ 2 ] ) \n 
~~~ self . assertEqual ( foo , [ 2 , 2 , 2 ] ) \n 
~~~ self . assertEqual ( foo , [ 1 ] ) \n 
~~ ~~ l = [ ] \n 
l += Bar ( ) \n 
self . assertEqual ( l , 1 ) \n 
l = [ ] \n 
l *= Bar ( ) \n 
self . assertEqual ( l , 2 ) \n 
~~ def test_iand ( self ) : \n 
~~~ class Foo ( set ) : \n 
~~~ def __and__ ( self , other ) : \n 
~~~ return set ( [ 1 ] ) \n 
foo &= 3 \n 
self . assertEqual ( foo , set ( [ 1 ] ) ) \n 
~~ ~~ class DescrExceptionsTestCase ( unittest . TestCase ) : \n 
~~~ def test_hex ( self ) : \n 
~~~ self . _test ( hex ) \n 
~~ def test_oct ( self ) : \n 
~~~ self . _test ( oct ) \n 
~~ def test_other ( self ) : \n 
~~~ for op in , , : \n 
~~~ eval ( % op ) \n 
~~~ self . _assert ( False , % op ) \n 
~~ ~~ ~~ def _test ( self , func ) : \n 
~~~ self . assertRaises ( AttributeError , func , old ) \n 
self . assertRaises ( TypeError , func , new ) \n 
~~ def test_eq ( self ) : \n 
~~~ return self . value == other . value \n 
~~ ~~ self . assertRaises ( AttributeError , lambda : A ( ) == A ( ) ) \n 
~~ ~~ class GetAttrTestCase ( unittest . TestCase ) : \n 
~~~ def test_raising_custom_attribute_error ( self ) : \n 
~~~ class BarAttributeError ( AttributeError ) : pass \n 
class Bar ( object ) : \n 
~~~ def __getattr__ ( self , name ) : \n 
~~~ raise BarAttributeError \n 
~~ ~~ class BarClassic : \n 
~~ ~~ class FooClassic : \n 
~~ ~~ self . assertRaises ( BarAttributeError , lambda : Bar ( ) . x ) \n 
self . assertRaises ( BarAttributeError , lambda : BarClassic ( ) . x ) \n 
~~~ Foo ( ) . x \n 
~~~ FooClassic ( ) . x \n 
~~ ~~ ~~ class Base ( object ) : \n 
~~ ~~ def lookup_where ( obj , name ) : \n 
~~~ mro = type ( obj ) . __mro__ \n 
for t in mro : \n 
~~~ if name in t . __dict__ : \n 
~~~ return t . __dict__ [ name ] , t \n 
~~ def refop ( x , y , opname , ropname ) : \n 
~~~ t1 = type ( x ) \n 
t2 = type ( y ) \n 
op , where1 = lookup_where ( x , opname ) \n 
rop , where2 = lookup_where ( y , ropname ) \n 
if op is None and rop is not None : \n 
~~~ return rop ( y , x ) \n 
~~ if rop and where1 is not where2 : \n 
~~~ if ( issubclass ( t2 , t1 ) and not issubclass ( where1 , where2 ) and \n 
not issubclass ( t1 , where2 ) ) : \n 
~~ ~~ if op is None : \n 
~~~ return "TypeError" \n 
~~ return op ( x , y ) \n 
~~ def do_test ( X , Y , name , impl ) : \n 
~~~ x = X ( ) \n 
y = Y ( ) \n 
opname = % name \n 
ropname = % name \n 
count = [ 0 ] \n 
fail = [ ] \n 
def check ( z1 , z2 ) : \n 
~~~ ref = refop ( z1 , z2 , opname , ropname ) \n 
~~~ v = impl ( z1 , z2 ) \n 
~~~ v = "TypeError" \n 
~~ if v != ref : \n 
~~~ fail . append ( count [ 0 ] ) \n 
~~ ~~ def override_in_hier ( n = 6 ) : \n 
~~~ if n == 0 : \n 
~~~ count [ 0 ] += 1 \n 
check ( x , y ) \n 
check ( y , x ) \n 
~~ f = lambda self , other : ( n , self . name , other . name ) \n 
~~~ name = opname \n 
~~~ name = ropname \n 
~~ for C in Y . __mro__ : \n 
~~~ if name in C . __dict__ : \n 
~~ if C is not object : \n 
~~~ setattr ( C , name , f ) \n 
~~ override_in_hier ( n - 1 ) \n 
if C is not object : \n 
~~~ delattr ( C , name ) \n 
~~ ~~ ~~ override_in_hier ( ) \n 
return fail \n 
~~ class BinopCombinationsTestCase ( unittest . TestCase ) : \n 
def test_binop_combinations_mul ( self ) : \n 
~~~ class X ( Base ) : \n 
~~ class Y ( X ) : \n 
~~ fail = do_test ( X , Y , , lambda x , y : x * y ) \n 
self . assert_ ( not fail ) \n 
~~ def test_binop_combinations_sub ( self ) : \n 
~~ fail = do_test ( X , Y , , lambda x , y : x - y ) \n 
~~ def test_binop_combinations_pow ( self ) : \n 
~~ fail = do_test ( X , Y , , lambda x , y : x ** y ) \n 
~~ def test_binop_combinations_more_exhaustive ( self ) : \n 
~~ class B1 ( object ) : \n 
~~ class B2 ( object ) : \n 
~~ class X1 ( B1 , X , B2 ) : \n 
~~ class C1 ( object ) : \n 
~~ class C2 ( object ) : \n 
~~ class Y ( C1 , X1 , C2 ) : \n 
~~~ test_support . run_unittest ( TestDescrTestCase , \n 
SubclassDescrTestCase , \n 
InPlaceTestCase , \n 
DescrExceptionsTestCase , \n 
GetAttrTestCase , \n 
BinopCombinationsTestCase ) \n 
std_c_errors = frozenset ( [ , ] ) \n 
class ErrnoAttributeTests ( unittest . TestCase ) : \n 
~~~ def test_for_improper_attributes ( self ) : \n 
~~~ for error_code in std_c_errors : \n 
~~~ self . assertTrue ( hasattr ( errno , error_code ) , \n 
~~ ~~ def test_using_errorcode ( self ) : \n 
~~~ for value in errno . errorcode . itervalues ( ) : \n 
~~~ self . assertTrue ( hasattr ( errno , value ) , % value ) \n 
~~ ~~ ~~ class ErrorcodeTests ( unittest . TestCase ) : \n 
~~~ def test_attributes_in_errorcode ( self ) : \n 
~~~ for attribute in errno . __dict__ . iterkeys ( ) : \n 
~~~ if attribute . isupper ( ) : \n 
~~~ self . assertIn ( getattr ( errno , attribute ) , errno . errorcode , \n 
% attribute ) \n 
~~ ~~ ~~ ~~ def test_main ( ) : \n 
~~~ test_support . run_unittest ( ErrnoAttributeTests , ErrorcodeTests ) \n 
~~ from test import test_support \n 
class FuncAttrsTest ( unittest . TestCase ) : \n 
~~~ class F : \n 
~~~ def a ( self ) : \n 
~~ ~~ def b ( ) : \n 
~~ self . f = F \n 
self . fi = F ( ) \n 
self . b = b \n 
~~ def cannot_set_attr ( self , obj , name , value , exceptions ) : \n 
~~~ setattr ( obj , name , value ) \n 
~~ except exceptions : \n 
~~~ delattr ( obj , name ) \n 
~~ ~~ ~~ class FunctionPropertiesTest ( FuncAttrsTest ) : \n 
~~~ def test_module ( self ) : \n 
~~~ self . assertEqual ( self . b . __module__ , __name__ ) \n 
~~ def test_dir_includes_correct_attrs ( self ) : \n 
~~~ self . b . known_attr = 7 \n 
self . assertIn ( , dir ( self . b ) , \n 
self . f . a . im_func . known_attr = 7 \n 
self . assertIn ( , dir ( self . f . a ) , \n 
self . assertIn ( , dir ( self . fi . a ) , \n 
~~ def test_duplicate_function_equality ( self ) : \n 
~~~ def duplicate ( ) : \n 
return 3 \n 
~~ self . assertNotEqual ( self . b , duplicate ) \n 
def test_copying_func_code ( self ) : \n 
~~~ def test ( ) : pass \n 
self . assertEqual ( test ( ) , None ) \n 
test . func_code = self . b . func_code \n 
def test_func_globals ( self ) : \n 
~~~ self . assertIs ( self . b . func_globals , globals ( ) ) \n 
self . cannot_set_attr ( self . b , , 2 , TypeError ) \n 
def test_func_closure ( self ) : \n 
~~~ a = 12 \n 
def f ( ) : print a \n 
c = f . func_closure \n 
self . assertIsInstance ( c , tuple ) \n 
self . assertEqual ( len ( c ) , 1 ) \n 
self . assertEqual ( c [ 0 ] . __class__ . __name__ , "cell" ) \n 
self . cannot_set_attr ( f , "func_closure" , c , TypeError ) \n 
~~ def test_empty_cell ( self ) : \n 
~~~ def f ( ) : print a \n 
~~~ f . func_closure [ 0 ] . cell_contents \n 
~~ a = 12 \n 
def test_func_name ( self ) : \n 
~~~ self . assertEqual ( self . b . __name__ , ) \n 
self . assertEqual ( self . b . func_name , ) \n 
self . b . __name__ = \n 
self . assertEqual ( self . b . __name__ , ) \n 
self . b . func_name = \n 
self . cannot_set_attr ( self . b , , 7 , TypeError ) \n 
exec s in { : { } } \n 
self . assertEqual ( self . f . a . __name__ , ) \n 
self . assertEqual ( self . fi . a . __name__ , ) \n 
self . cannot_set_attr ( self . f . a , "__name__" , , AttributeError ) \n 
self . cannot_set_attr ( self . fi . a , "__name__" , , AttributeError ) \n 
def test_func_code ( self ) : \n 
~~~ num_one , num_two = 7 , 8 \n 
def a ( ) : pass \n 
def b ( ) : return 12 \n 
def c ( ) : return num_one \n 
def d ( ) : return num_two \n 
def e ( ) : return num_one , num_two \n 
for func in [ a , b , c , d , e ] : \n 
~~~ self . assertEqual ( type ( func . func_code ) , types . CodeType ) \n 
~~ self . assertEqual ( c ( ) , 7 ) \n 
self . assertEqual ( d ( ) , 8 ) \n 
d . func_code = c . func_code \n 
self . assertEqual ( c . func_code , d . func_code ) \n 
self . assertEqual ( c ( ) , 7 ) \n 
~~~ b . func_code = c . func_code \n 
~~~ e . func_code = d . func_code \n 
~~ ~~ def test_blank_func_defaults ( self ) : \n 
~~~ self . assertEqual ( self . b . func_defaults , None ) \n 
del self . b . func_defaults \n 
self . assertEqual ( self . b . func_defaults , None ) \n 
~~ def test_func_default_args ( self ) : \n 
~~~ def first_func ( a , b ) : \n 
~~ def second_func ( a = 1 , b = 2 ) : \n 
~~ self . assertEqual ( first_func . func_defaults , None ) \n 
self . assertEqual ( second_func . func_defaults , ( 1 , 2 ) ) \n 
first_func . func_defaults = ( 1 , 2 ) \n 
self . assertEqual ( first_func . func_defaults , ( 1 , 2 ) ) \n 
self . assertEqual ( first_func ( ) , 3 ) \n 
self . assertEqual ( first_func ( 3 ) , 5 ) \n 
self . assertEqual ( first_func ( 3 , 5 ) , 8 ) \n 
del second_func . func_defaults \n 
self . assertEqual ( second_func . func_defaults , None ) \n 
~~~ second_func ( ) \n 
~~ ~~ ~~ class InstancemethodAttrTest ( FuncAttrsTest ) : \n 
~~~ def test_im_class ( self ) : \n 
~~~ self . assertEqual ( self . f . a . im_class , self . f ) \n 
self . assertEqual ( self . fi . a . im_class , self . f ) \n 
self . cannot_set_attr ( self . f . a , "im_class" , self . f , TypeError ) \n 
self . cannot_set_attr ( self . fi . a , "im_class" , self . f , TypeError ) \n 
~~ def test_im_func ( self ) : \n 
~~~ self . f . b = self . b \n 
self . assertEqual ( self . f . b . im_func , self . b ) \n 
self . assertEqual ( self . fi . b . im_func , self . b ) \n 
self . cannot_set_attr ( self . f . b , "im_func" , self . b , TypeError ) \n 
self . cannot_set_attr ( self . fi . b , "im_func" , self . b , TypeError ) \n 
~~ def test_im_self ( self ) : \n 
~~~ self . assertEqual ( self . f . a . im_self , None ) \n 
self . assertEqual ( self . fi . a . im_self , self . fi ) \n 
self . cannot_set_attr ( self . f . a , "im_self" , None , TypeError ) \n 
self . cannot_set_attr ( self . fi . a , "im_self" , self . fi , TypeError ) \n 
~~ def test_im_func_non_method ( self ) : \n 
~~~ self . f . id = types . MethodType ( id , None , self . f ) \n 
self . assertEqual ( self . fi . id ( ) , id ( self . fi ) ) \n 
self . assertNotEqual ( self . fi . id ( ) , id ( self . f ) ) \n 
~~~ self . f . id . unknown_attr \n 
~~ self . cannot_set_attr ( self . f . id , , 2 , AttributeError ) \n 
self . cannot_set_attr ( self . fi . id , , 2 , AttributeError ) \n 
~~ def test_implicit_method_properties ( self ) : \n 
~~~ self . f . a . im_func . known_attr = 7 \n 
self . assertEqual ( self . f . a . known_attr , 7 ) \n 
self . assertEqual ( self . fi . a . known_attr , 7 ) \n 
~~ ~~ class ArbitraryFunctionAttrTest ( FuncAttrsTest ) : \n 
~~~ def test_set_attr ( self ) : \n 
self . assertEqual ( self . b . known_attr , 7 ) \n 
for func in [ self . f . a , self . fi . a ] : \n 
~~~ func . known_attr = 7 \n 
~~ ~~ ~~ def test_delete_unknown_attr ( self ) : \n 
~~~ del self . b . unknown_attr \n 
~~ ~~ def test_setting_attrs_duplicates ( self ) : \n 
~~~ self . f . a . klass = self . f \n 
~~ self . f . a . im_func . klass = self . f \n 
for method in [ self . f . a , self . fi . a , self . fi . a . im_func ] : \n 
~~~ self . assertEqual ( method . klass , self . f ) \n 
~~ ~~ def test_unset_attr ( self ) : \n 
~~~ for func in [ self . b , self . f . a , self . fi . a ] : \n 
~~~ func . non_existent_attr \n 
"AttributeError" ) \n 
~~ ~~ ~~ ~~ class FunctionDictsTest ( FuncAttrsTest ) : \n 
~~~ def test_setting_dict_to_invalid ( self ) : \n 
~~~ self . cannot_set_attr ( self . b , , None , TypeError ) \n 
self . cannot_set_attr ( self . b , , None , TypeError ) \n 
from UserDict import UserDict \n 
d = UserDict ( { : 7 } ) \n 
self . cannot_set_attr ( self . f . a . im_func , , d , TypeError ) \n 
self . cannot_set_attr ( self . fi . a . im_func , , d , TypeError ) \n 
~~ def test_setting_dict_to_valid ( self ) : \n 
~~~ d = { : 7 } \n 
self . b . __dict__ = d \n 
self . f . a . im_func . __dict__ = d \n 
self . assertIs ( d , self . b . __dict__ ) \n 
self . assertIs ( d , self . b . func_dict ) \n 
self . assertIs ( d , self . f . a . im_func . __dict__ ) \n 
self . assertIs ( d , self . f . a . __dict__ ) \n 
self . assertIs ( d , self . fi . a . im_func . __dict__ ) \n 
self . assertIs ( d , self . fi . a . __dict__ ) \n 
self . assertEqual ( self . b . __dict__ [ ] , 7 ) \n 
self . assertEqual ( self . b . func_dict [ ] , 7 ) \n 
self . assertEqual ( self . f . a . im_func . known_attr , 7 ) \n 
self . assertEqual ( self . fi . a . im_func . known_attr , 7 ) \n 
~~ def test_delete_func_dict ( self ) : \n 
~~~ del self . b . __dict__ \n 
~~~ del self . b . func_dict \n 
~~ ~~ def test_unassigned_dict ( self ) : \n 
~~~ self . assertEqual ( self . b . __dict__ , { } ) \n 
~~ def test_func_as_dict_key ( self ) : \n 
d = { } \n 
d [ self . b ] = value \n 
self . assertEqual ( d [ self . b ] , value ) \n 
~~ ~~ class FunctionDocstringTest ( FuncAttrsTest ) : \n 
def test_set_docstring_attr ( self ) : \n 
~~~ self . assertEqual ( self . b . __doc__ , None ) \n 
self . assertEqual ( self . b . func_doc , None ) \n 
self . b . __doc__ = self . f . a . im_func . __doc__ = docstr \n 
self . assertEqual ( self . b . __doc__ , docstr ) \n 
self . assertEqual ( self . b . func_doc , docstr ) \n 
self . assertEqual ( self . f . a . __doc__ , docstr ) \n 
self . assertEqual ( self . fi . a . __doc__ , docstr ) \n 
self . cannot_set_attr ( self . f . a , "__doc__" , docstr , AttributeError ) \n 
self . cannot_set_attr ( self . fi . a , "__doc__" , docstr , AttributeError ) \n 
~~ def test_delete_docstring ( self ) : \n 
del self . b . __doc__ \n 
self . assertEqual ( self . b . __doc__ , None ) \n 
del self . b . func_doc \n 
~~ ~~ class StaticMethodAttrsTest ( unittest . TestCase ) : \n 
def test_func_attribute ( self ) : \n 
~~~ def f ( ) : \n 
~~ c = classmethod ( f ) \n 
self . assertTrue ( c . __func__ is f ) \n 
s = staticmethod ( f ) \n 
self . assertTrue ( s . __func__ is f ) \n 
~~~ test_support . run_unittest ( FunctionPropertiesTest , InstancemethodAttrTest , \n 
ArbitraryFunctionAttrTest , FunctionDictsTest , \n 
FunctionDocstringTest , \n 
StaticMethodAttrsTest ) \n 
from collections import Hashable \n 
IS_64BIT = ( struct . calcsize ( ) == 8 ) \n 
class HashEqualityTestCase ( unittest . TestCase ) : \n 
~~~ def same_hash ( self , * objlist ) : \n 
~~~ hashed = map ( hash , objlist ) \n 
for h in hashed [ 1 : ] : \n 
~~~ if h != hashed [ 0 ] : \n 
~~ ~~ ~~ def test_numeric_literals ( self ) : \n 
~~~ self . same_hash ( 1 , 1 L , 1.0 , 1.0 + 0.0j ) \n 
self . same_hash ( 0 , 0 L , 0.0 , 0.0 + 0.0j ) \n 
self . same_hash ( - 1 , - 1 L , - 1.0 , - 1.0 + 0.0j ) \n 
self . same_hash ( - 2 , - 2 L , - 2.0 , - 2.0 + 0.0j ) \n 
~~ def test_coerced_integers ( self ) : \n 
~~~ self . same_hash ( int ( 1 ) , long ( 1 ) , float ( 1 ) , complex ( 1 ) , \n 
int ( ) , float ( ) ) \n 
self . same_hash ( int ( - 2 ** 31 ) , long ( - 2 ** 31 ) , float ( - 2 ** 31 ) ) \n 
self . same_hash ( int ( 1 - 2 ** 31 ) , long ( 1 - 2 ** 31 ) , float ( 1 - 2 ** 31 ) ) \n 
self . same_hash ( int ( 2 ** 31 - 1 ) , long ( 2 ** 31 - 1 ) , float ( 2 ** 31 - 1 ) ) \n 
self . same_hash ( int ( 2 ** 31 ) , long ( 2 ** 31 ) , float ( 2 ** 31 ) ) \n 
self . same_hash ( int ( - 2 ** 63 ) , long ( - 2 ** 63 ) , float ( - 2 ** 63 ) ) \n 
self . same_hash ( int ( 1 - 2 ** 63 ) , long ( 1 - 2 ** 63 ) ) \n 
self . same_hash ( int ( 2 ** 63 - 1 ) , long ( 2 ** 63 - 1 ) ) \n 
self . same_hash ( long ( 2 ** 63 ) , float ( 2 ** 63 ) ) \n 
~~ def test_coerced_floats ( self ) : \n 
~~~ self . same_hash ( long ( 1.23e300 ) , float ( 1.23e300 ) ) \n 
self . same_hash ( float ( 0.5 ) , complex ( 0.5 , 0.0 ) ) \n 
~~ ~~ _default_hash = object . __hash__ \n 
class DefaultHash ( object ) : pass \n 
_FIXED_HASH_VALUE = 42 \n 
class FixedHash ( object ) : \n 
~~~ def __hash__ ( self ) : \n 
~~~ return _FIXED_HASH_VALUE \n 
~~ ~~ class OnlyEquality ( object ) : \n 
~~~ return self is other \n 
~~ del OnlyEquality . __hash__ \n 
class OnlyInequality ( object ) : \n 
~~~ def __ne__ ( self , other ) : \n 
~~~ return self is not other \n 
~~ ~~ class OnlyCmp ( object ) : \n 
~~~ def __cmp__ ( self , other ) : \n 
~~~ return cmp ( id ( self ) , id ( other ) ) \n 
~~ del OnlyCmp . __hash__ \n 
class InheritedHashWithEquality ( FixedHash , OnlyEquality ) : pass \n 
class InheritedHashWithInequality ( FixedHash , OnlyInequality ) : pass \n 
class InheritedHashWithCmp ( FixedHash , OnlyCmp ) : pass \n 
class NoHash ( object ) : \n 
~~~ __hash__ = None \n 
~~ class HashInheritanceTestCase ( unittest . TestCase ) : \n 
~~~ default_expected = [ object ( ) , \n 
DefaultHash ( ) , \n 
OnlyEquality ( ) , \n 
OnlyInequality ( ) , \n 
OnlyCmp ( ) , \n 
fixed_expected = [ FixedHash ( ) , \n 
InheritedHashWithEquality ( ) , \n 
InheritedHashWithInequality ( ) , \n 
InheritedHashWithCmp ( ) , \n 
error_expected = [ NoHash ( ) ] \n 
def test_default_hash ( self ) : \n 
~~~ for obj in self . default_expected : \n 
~~~ self . assertEqual ( hash ( obj ) , _default_hash ( obj ) ) \n 
~~ ~~ def test_fixed_hash ( self ) : \n 
~~~ for obj in self . fixed_expected : \n 
~~~ self . assertEqual ( hash ( obj ) , _FIXED_HASH_VALUE ) \n 
~~ ~~ def test_error_hash ( self ) : \n 
~~~ for obj in self . error_expected : \n 
~~~ self . assertRaises ( TypeError , hash , obj ) \n 
~~ ~~ def test_hashable ( self ) : \n 
~~~ objects = ( self . default_expected + \n 
self . fixed_expected ) \n 
for obj in objects : \n 
~~~ self . assertIsInstance ( obj , Hashable ) \n 
~~ ~~ def test_not_hashable ( self ) : \n 
~~~ self . assertNotIsInstance ( obj , Hashable ) \n 
~~ ~~ ~~ class DefaultIterSeq ( object ) : \n 
~~~ seq = range ( 10 ) \n 
def __len__ ( self ) : \n 
~~~ return len ( self . seq ) \n 
~~ def __getitem__ ( self , index ) : \n 
~~~ return self . seq [ index ] \n 
~~ ~~ class HashBuiltinsTestCase ( unittest . TestCase ) : \n 
~~~ hashes_to_check = [ xrange ( 10 ) , \n 
enumerate ( xrange ( 10 ) ) , \n 
iter ( DefaultIterSeq ( ) ) , \n 
iter ( lambda : 0 , 0 ) , \n 
def test_hashes ( self ) : \n 
~~~ _default_hash = object . __hash__ \n 
for obj in self . hashes_to_check : \n 
~~ ~~ ~~ class HashRandomizationTests ( unittest . TestCase ) : \n 
~~~ def get_hash_command ( self , repr_ ) : \n 
~~~ return % repr_ \n 
~~ def get_hash ( self , repr_ , seed = None ) : \n 
~~~ env = os . environ . copy ( ) \n 
if seed is not None : \n 
~~~ env [ ] = str ( seed ) \n 
~~~ env . pop ( , None ) \n 
~~ cmd_line = [ sys . executable , , self . get_hash_command ( repr_ ) ] \n 
p = subprocess . Popen ( cmd_line , stdin = subprocess . PIPE , \n 
stdout = subprocess . PIPE , stderr = subprocess . STDOUT , \n 
env = env ) \n 
out , err = p . communicate ( ) \n 
out = test_support . strip_python_stderr ( out ) \n 
return int ( out . strip ( ) ) \n 
def test_randomized_hash ( self ) : \n 
~~~ run1 = self . get_hash ( self . repr_ , seed = ) \n 
run2 = self . get_hash ( self . repr_ , seed = ) \n 
self . assertNotEqual ( run1 , run2 ) \n 
~~ ~~ class StringlikeHashRandomizationTests ( HashRandomizationTests ) : \n 
~~~ def test_null_hash ( self ) : \n 
~~~ if IS_64BIT : \n 
~~~ known_hash_of_obj = 1453079729188098211 \n 
~~~ known_hash_of_obj = - 1600925533 \n 
~~ self . assertEqual ( self . get_hash ( self . repr_ ) , known_hash_of_obj ) \n 
self . assertEqual ( self . get_hash ( self . repr_ , seed = 0 ) , known_hash_of_obj ) \n 
~~ def test_fixed_hash ( self ) : \n 
~~~ h = - 4410911502303878509 \n 
~~~ h = - 3570150969479994130 \n 
~~~ h = - 206076799 \n 
~~~ h = - 1024014457 \n 
~~ ~~ self . assertEqual ( self . get_hash ( self . repr_ , seed = 42 ) , h ) \n 
class StrHashRandomizationTests ( StringlikeHashRandomizationTests ) : \n 
~~~ repr_ = repr ( ) \n 
def test_empty_string ( self ) : \n 
~~~ self . assertEqual ( hash ( "" ) , 0 ) \n 
class UnicodeHashRandomizationTests ( StringlikeHashRandomizationTests ) : \n 
~~~ self . assertEqual ( hash ( u"" ) , 0 ) \n 
class BufferHashRandomizationTests ( StringlikeHashRandomizationTests ) : \n 
~~~ repr_ = \'buffer("abc")\' \n 
~~~ self . assertEqual ( hash ( buffer ( "" ) ) , 0 ) \n 
~~ ~~ class DatetimeTests ( HashRandomizationTests ) : \n 
~~ ~~ class DatetimeDateTests ( DatetimeTests ) : \n 
~~~ repr_ = repr ( datetime . date ( 1066 , 10 , 14 ) ) \n 
~~ class DatetimeDatetimeTests ( DatetimeTests ) : \n 
~~~ repr_ = repr ( datetime . datetime ( 1 , 2 , 3 , 4 , 5 , 6 , 7 ) ) \n 
~~ class DatetimeTimeTests ( DatetimeTests ) : \n 
~~~ repr_ = repr ( datetime . time ( 0 ) ) \n 
~~ def test_main ( ) : \n 
~~~ test_support . run_unittest ( HashEqualityTestCase , \n 
HashInheritanceTestCase , \n 
HashBuiltinsTestCase , \n 
StrHashRandomizationTests , \n 
UnicodeHashRandomizationTests , \n 
BufferHashRandomizationTests , \n 
DatetimeDateTests , \n 
DatetimeDatetimeTests , \n 
DatetimeTimeTests ) \n 
class TestIsInstanceExceptions ( unittest . TestCase ) : \n 
~~~ def test_class_has_no_bases ( self ) : \n 
~~~ class I ( object ) : \n 
~~~ def getclass ( self ) : \n 
~~ __class__ = property ( getclass ) \n 
~~~ def getbases ( self ) : \n 
~~~ return ( ) \n 
~~ __bases__ = property ( getbases ) \n 
~~ self . assertEqual ( False , isinstance ( I ( ) , C ( ) ) ) \n 
~~ def test_bases_raises_other_than_attribute_error ( self ) : \n 
~~~ class E ( object ) : \n 
~~ class I ( object ) : \n 
~~~ return E ( ) \n 
~~ self . assertRaises ( RuntimeError , isinstance , I ( ) , C ( ) ) \n 
~~ def test_dont_mask_non_attribute_error ( self ) : \n 
~~~ class I : pass \n 
~~ def test_mask_attribute_error ( self ) : \n 
~~~ raise AttributeError \n 
~~ self . assertRaises ( TypeError , isinstance , I ( ) , C ( ) ) \n 
~~ ~~ class TestIsSubclassExceptions ( unittest . TestCase ) : \n 
~~~ def test_dont_mask_non_attribute_error ( self ) : \n 
~~~ class C ( object ) : \n 
~~ class S ( C ) : pass \n 
self . assertRaises ( RuntimeError , issubclass , C ( ) , S ( ) ) \n 
self . assertRaises ( TypeError , issubclass , C ( ) , S ( ) ) \n 
~~ def test_dont_mask_non_attribute_error_in_cls_arg ( self ) : \n 
~~~ class B : pass \n 
~~ self . assertRaises ( RuntimeError , issubclass , B , C ( ) ) \n 
~~ def test_mask_attribute_error_in_cls_arg ( self ) : \n 
~~ self . assertRaises ( TypeError , issubclass , B , C ( ) ) \n 
~~ ~~ class AbstractClass ( object ) : \n 
~~~ def __init__ ( self , bases ) : \n 
~~~ self . bases = bases \n 
~~ def getbases ( self ) : \n 
~~~ return self . bases \n 
def __call__ ( self ) : \n 
~~~ return AbstractInstance ( self ) \n 
~~ ~~ class AbstractInstance ( object ) : \n 
~~~ def __init__ ( self , klass ) : \n 
~~~ self . klass = klass \n 
~~ def getclass ( self ) : \n 
~~~ return self . klass \n 
~~ AbstractSuper = AbstractClass ( bases = ( ) ) \n 
AbstractChild = AbstractClass ( bases = ( AbstractSuper , ) ) \n 
class Super : \n 
~~ class Child ( Super ) : \n 
~~ class NewSuper ( object ) : \n 
~~ class NewChild ( NewSuper ) : \n 
~~ class TestIsInstanceIsSubclass ( unittest . TestCase ) : \n 
~~~ def test_isinstance_normal ( self ) : \n 
~~~ self . assertEqual ( True , isinstance ( Super ( ) , Super ) ) \n 
self . assertEqual ( False , isinstance ( Super ( ) , Child ) ) \n 
self . assertEqual ( False , isinstance ( Super ( ) , AbstractSuper ) ) \n 
self . assertEqual ( False , isinstance ( Super ( ) , AbstractChild ) ) \n 
self . assertEqual ( True , isinstance ( Child ( ) , Super ) ) \n 
self . assertEqual ( False , isinstance ( Child ( ) , AbstractSuper ) ) \n 
~~ def test_isinstance_abstract ( self ) : \n 
~~~ self . assertEqual ( True , isinstance ( AbstractSuper ( ) , AbstractSuper ) ) \n 
self . assertEqual ( False , isinstance ( AbstractSuper ( ) , AbstractChild ) ) \n 
self . assertEqual ( False , isinstance ( AbstractSuper ( ) , Super ) ) \n 
self . assertEqual ( False , isinstance ( AbstractSuper ( ) , Child ) ) \n 
self . assertEqual ( True , isinstance ( AbstractChild ( ) , AbstractChild ) ) \n 
self . assertEqual ( True , isinstance ( AbstractChild ( ) , AbstractSuper ) ) \n 
self . assertEqual ( False , isinstance ( AbstractChild ( ) , Super ) ) \n 
self . assertEqual ( False , isinstance ( AbstractChild ( ) , Child ) ) \n 
~~ def test_subclass_normal ( self ) : \n 
~~~ self . assertEqual ( True , issubclass ( Super , Super ) ) \n 
self . assertEqual ( False , issubclass ( Super , AbstractSuper ) ) \n 
self . assertEqual ( False , issubclass ( Super , Child ) ) \n 
self . assertEqual ( True , issubclass ( Child , Child ) ) \n 
self . assertEqual ( True , issubclass ( Child , Super ) ) \n 
self . assertEqual ( False , issubclass ( Child , AbstractSuper ) ) \n 
~~ def test_subclass_abstract ( self ) : \n 
~~~ self . assertEqual ( True , issubclass ( AbstractSuper , AbstractSuper ) ) \n 
self . assertEqual ( False , issubclass ( AbstractSuper , AbstractChild ) ) \n 
self . assertEqual ( False , issubclass ( AbstractSuper , Child ) ) \n 
self . assertEqual ( True , issubclass ( AbstractChild , AbstractChild ) ) \n 
self . assertEqual ( True , issubclass ( AbstractChild , AbstractSuper ) ) \n 
self . assertEqual ( False , issubclass ( AbstractChild , Super ) ) \n 
self . assertEqual ( False , issubclass ( AbstractChild , Child ) ) \n 
~~ def test_subclass_tuple ( self ) : \n 
~~~ self . assertEqual ( True , issubclass ( Child , ( Child , ) ) ) \n 
self . assertEqual ( True , issubclass ( Child , ( Super , ) ) ) \n 
self . assertEqual ( False , issubclass ( Super , ( Child , ) ) ) \n 
self . assertEqual ( True , issubclass ( Super , ( Child , Super ) ) ) \n 
self . assertEqual ( False , issubclass ( Child , ( ) ) ) \n 
self . assertEqual ( True , issubclass ( Super , ( Child , ( Super , ) ) ) ) \n 
self . assertEqual ( True , issubclass ( NewChild , ( NewChild , ) ) ) \n 
self . assertEqual ( True , issubclass ( NewChild , ( NewSuper , ) ) ) \n 
self . assertEqual ( False , issubclass ( NewSuper , ( NewChild , ) ) ) \n 
self . assertEqual ( True , issubclass ( NewSuper , ( NewChild , NewSuper ) ) ) \n 
self . assertEqual ( False , issubclass ( NewChild , ( ) ) ) \n 
self . assertEqual ( True , issubclass ( NewSuper , ( NewChild , ( NewSuper , ) ) ) ) \n 
self . assertEqual ( True , issubclass ( int , ( long , ( float , int ) ) ) ) \n 
~~~ self . assertEqual ( True , issubclass ( str , ( unicode , ( Child , NewChild , basestring ) ) ) ) \n 
~~ ~~ def test_subclass_recursion_limit ( self ) : \n 
~~~ self . assertRaises ( RuntimeError , blowstack , issubclass , str , str ) \n 
~~ def test_isinstance_recursion_limit ( self ) : \n 
~~~ self . assertRaises ( RuntimeError , blowstack , isinstance , , str ) \n 
~~ ~~ def blowstack ( fxn , arg , compare_to ) : \n 
~~~ tuple_arg = ( compare_to , ) \n 
for cnt in xrange ( sys . getrecursionlimit ( ) + 5 ) : \n 
~~~ tuple_arg = ( tuple_arg , ) \n 
fxn ( arg , tuple_arg ) \n 
~~~ test_support . run_unittest ( \n 
TestIsInstanceExceptions , \n 
TestIsSubclassExceptions , \n 
TestIsInstanceIsSubclass \n 
from test . test_support import run_unittest , cpython_only \n 
requires_IEEE_754 = unittest . skipUnless ( \n 
float . __getformat__ ( "double" ) . startswith ( "IEEE" ) , \n 
DBL_MAX = sys . float_info . max \n 
DBL_MAX_EXP = sys . float_info . max_exp \n 
DBL_MIN_EXP = sys . float_info . min_exp \n 
DBL_MANT_DIG = sys . float_info . mant_dig \n 
DBL_MIN_OVERFLOW = 2 ** DBL_MAX_EXP - 2 ** ( DBL_MAX_EXP - DBL_MANT_DIG - 1 ) \n 
def truediv ( a , b ) : \n 
negative = a ^ b < 0 \n 
a , b = abs ( a ) , abs ( b ) \n 
if not b : \n 
~~ if a >= DBL_MIN_OVERFLOW * b : \n 
~~ d = a . bit_length ( ) - b . bit_length ( ) \n 
if d >= 0 and a >= 2 ** d * b or d < 0 and a * 2 ** - d >= b : \n 
~~~ d += 1 \n 
~~ exp = max ( d , DBL_MIN_EXP ) - DBL_MANT_DIG \n 
a , b = a << max ( - exp , 0 ) , b << max ( exp , 0 ) \n 
if 2 * r > b or 2 * r == b and q % 2 == 1 : \n 
~~~ q += 1 \n 
~~ result = math . ldexp ( float ( q ) , exp ) \n 
return - result if negative else result \n 
~~ class TrueDivisionTests ( unittest . TestCase ) : \n 
~~~ def test ( self ) : \n 
~~~ huge = 1 L << 40000 \n 
mhuge = - huge \n 
self . assertEqual ( huge / huge , 1.0 ) \n 
self . assertEqual ( mhuge / mhuge , 1.0 ) \n 
self . assertEqual ( huge / mhuge , - 1.0 ) \n 
self . assertEqual ( mhuge / huge , - 1.0 ) \n 
self . assertEqual ( 1 / huge , 0.0 ) \n 
self . assertEqual ( 1 L / huge , 0.0 ) \n 
self . assertEqual ( 1 / mhuge , 0.0 ) \n 
self . assertEqual ( 1 L / mhuge , 0.0 ) \n 
self . assertEqual ( ( 666 * huge + ( huge >> 1 ) ) / huge , 666.5 ) \n 
self . assertEqual ( ( 666 * mhuge + ( mhuge >> 1 ) ) / mhuge , 666.5 ) \n 
self . assertEqual ( ( 666 * huge + ( huge >> 1 ) ) / mhuge , - 666.5 ) \n 
self . assertEqual ( ( 666 * mhuge + ( mhuge >> 1 ) ) / huge , - 666.5 ) \n 
self . assertEqual ( huge / ( huge << 1 ) , 0.5 ) \n 
self . assertEqual ( ( 1000000 * huge ) / huge , 1000000 ) \n 
namespace = { : huge , : mhuge } \n 
for overflow in [ "float(huge)" , "float(mhuge)" , \n 
~~~ with self . assertRaises ( OverflowError ) : \n 
~~~ eval ( overflow , namespace ) \n 
~~~ result = eval ( underflow , namespace ) \n 
self . assertEqual ( result , 0.0 , \n 
. format ( underflow ) ) \n 
~~~ with self . assertRaises ( ZeroDivisionError ) : \n 
~~~ eval ( zero , namespace ) \n 
~~ ~~ ~~ def check_truediv ( self , a , b , skip_small = True ) : \n 
a , b = long ( a ) , long ( b ) \n 
if skip_small and max ( abs ( a ) , abs ( b ) ) < 2 ** DBL_MANT_DIG : \n 
~~~ expected = repr ( truediv ( a , b ) ) \n 
~~ except OverflowError : \n 
~~~ expected = \n 
~~~ got = repr ( a / b ) \n 
~~~ got = \n 
~~ @ cpython_only \n 
@ requires_IEEE_754 \n 
def test_correctly_rounded_true_division ( self ) : \n 
~~~ self . check_truediv ( 123 , 0 ) \n 
self . check_truediv ( - 456 , 0 ) \n 
self . check_truediv ( 0 , 3 ) \n 
self . check_truediv ( 0 , - 3 ) \n 
self . check_truediv ( 0 , 0 ) \n 
self . check_truediv ( 671 * 12345 * 2 ** DBL_MAX_EXP , 12345 ) \n 
self . check_truediv ( 12345 , 345678 * 2 ** ( DBL_MANT_DIG - DBL_MIN_EXP ) ) \n 
self . check_truediv ( 12345 * 2 ** 100 , 98765 ) \n 
self . check_truediv ( 12345 * 2 ** 30 , 98765 * 7 ** 81 ) \n 
bases = ( 0 , DBL_MANT_DIG , DBL_MIN_EXP , \n 
DBL_MAX_EXP , DBL_MIN_EXP - DBL_MANT_DIG ) \n 
for base in bases : \n 
~~~ for exp in range ( base - 15 , base + 15 ) : \n 
~~~ self . check_truediv ( 75312 * 2 ** max ( exp , 0 ) , 69187 * 2 ** max ( - exp , 0 ) ) \n 
self . check_truediv ( 69187 * 2 ** max ( exp , 0 ) , 75312 * 2 ** max ( - exp , 0 ) ) \n 
~~ ~~ for m in [ 1 , 2 , 7 , 17 , 12345 , 7 ** 100 , \n 
- 1 , - 2 , - 5 , - 23 , - 67891 , - 41 ** 50 ] : \n 
~~~ for n in range ( - 10 , 10 ) : \n 
~~~ self . check_truediv ( m * DBL_MIN_OVERFLOW + n , m ) \n 
self . check_truediv ( m * DBL_MIN_OVERFLOW + n , - m ) \n 
~~ ~~ for n in range ( 250 ) : \n 
~~~ self . check_truediv ( ( 2 ** DBL_MANT_DIG + 1 ) * 12345 * 2 ** 200 + 2 ** n , \n 
2 ** DBL_MANT_DIG * 12345 ) \n 
~~ self . check_truediv ( 1 , 2731 ) \n 
self . check_truediv ( 295147931372582273023 , 295147932265116303360 ) \n 
~~~ self . check_truediv ( 10 ** ( i + 1 ) , 10 ** i ) \n 
self . check_truediv ( 10 ** i , 10 ** ( i + 1 ) ) \n 
~~ for m in [ 1 , 2 , 4 , 7 , 8 , 16 , 17 , 32 , 12345 , 7 ** 100 , \n 
~~~ self . check_truediv ( 2 ** DBL_MANT_DIG * m + n , m ) \n 
~~ ~~ for n in range ( - 20 , 20 ) : \n 
~~~ self . check_truediv ( n , 2 ** 1076 ) \n 
~~ for M in [ 10 ** 10 , 10 ** 100 , 10 ** 1000 ] : \n 
~~~ for i in range ( 1000 ) : \n 
~~~ a = random . randrange ( 1 , M ) \n 
b = random . randrange ( a , 2 * a + 1 ) \n 
self . check_truediv ( a , b ) \n 
self . check_truediv ( - a , b ) \n 
self . check_truediv ( a , - b ) \n 
self . check_truediv ( - a , - b ) \n 
~~ ~~ for _ in range ( 10000 ) : \n 
~~~ a_bits = random . randrange ( 1000 ) \n 
b_bits = random . randrange ( 1 , 1000 ) \n 
x = random . randrange ( 2 ** a_bits ) \n 
y = random . randrange ( 1 , 2 ** b_bits ) \n 
self . check_truediv ( x , y ) \n 
self . check_truediv ( x , - y ) \n 
self . check_truediv ( - x , y ) \n 
self . check_truediv ( - x , - y ) \n 
~~ ~~ ~~ def test_main ( ) : \n 
~~~ run_unittest ( TrueDivisionTests ) \n 
from test . test_support import check_py3k_warnings , CleanImport , run_unittest \n 
if not sys . py3kwarning : \n 
~~~ raise unittest . SkipTest ( % __name__ ) \n 
~~~ from test . test_support import __warningregistry__ as _registry \n 
~~~ def check_deprecated_module ( module_name ) : \n 
~~~ past_warnings = _registry . keys ( ) \n 
del _registry \n 
def check_deprecated_module ( module_name ) : \n 
return any ( module_name in msg and in msg \n 
and issubclass ( cls , DeprecationWarning ) \n 
and ( in msg or in msg ) \n 
for ( msg , cls , line ) in past_warnings ) \n 
~~ ~~ def reset_module_registry ( module ) : \n 
~~~ registry = module . __warningregistry__ \n 
~~~ registry . clear ( ) \n 
~~ ~~ class TestPy3KWarnings ( unittest . TestCase ) : \n 
~~~ def assertWarning ( self , _ , warning , expected_message ) : \n 
~~~ self . assertEqual ( str ( warning . message ) , expected_message ) \n 
~~ def assertNoWarning ( self , _ , recorder ) : \n 
~~~ self . assertEqual ( len ( recorder . warnings ) , 0 ) \n 
~~ def test_backquote ( self ) : \n 
with check_py3k_warnings ( ( expected , SyntaxWarning ) ) : \n 
~~~ exec "`2`" in { } \n 
~~ ~~ def test_paren_arg_names ( self ) : \n 
def check ( s ) : \n 
~~~ with check_py3k_warnings ( ( expected , SyntaxWarning ) ) : \n 
~~~ exec s in { } \n 
~~ def test_forbidden_names ( self ) : \n 
~~~ def safe_exec ( expr ) : \n 
~~~ def f ( ** kwargs ) : pass \n 
exec expr in { : f } \n 
with check_py3k_warnings ( ( , SyntaxWarning ) ) as w : \n 
~~~ for keyword , expected in tests : \n 
self . assertWarning ( None , w , expected ) \n 
w . reset ( ) \n 
~~ self . assertWarning ( None , w , expected ) \n 
~~ ~~ ~~ def test_type_inequality_comparisons ( self ) : \n 
with check_py3k_warnings ( ) as w : \n 
~~~ self . assertWarning ( int < str , w , expected ) \n 
self . assertWarning ( type < object , w , expected ) \n 
~~ ~~ def test_object_inequality_comparisons ( self ) : \n 
~~~ self . assertWarning ( str < [ ] , w , expected ) \n 
self . assertWarning ( object ( ) < ( 1 , 2 ) , w , expected ) \n 
~~ ~~ def test_dict_inequality_comparisons ( self ) : \n 
~~~ self . assertWarning ( { } < { 2 : 3 } , w , expected ) \n 
self . assertWarning ( { } <= { } , w , expected ) \n 
self . assertWarning ( { } > { 2 : 3 } , w , expected ) \n 
self . assertWarning ( { 2 : 3 } >= { } , w , expected ) \n 
~~ ~~ def test_cell_inequality_comparisons ( self ) : \n 
def f ( x ) : \n 
~~ return g \n 
~~ cell0 , = f ( 0 ) . func_closure \n 
cell1 , = f ( 1 ) . func_closure \n 
~~~ self . assertWarning ( cell0 == cell1 , w , expected ) \n 
self . assertWarning ( cell0 < cell1 , w , expected ) \n 
~~ ~~ def test_code_inequality_comparisons ( self ) : \n 
~~ def g ( x ) : \n 
~~ with check_py3k_warnings ( ) as w : \n 
~~~ self . assertWarning ( f . func_code < g . func_code , w , expected ) \n 
self . assertWarning ( f . func_code <= g . func_code , w , expected ) \n 
self . assertWarning ( f . func_code >= g . func_code , w , expected ) \n 
self . assertWarning ( f . func_code > g . func_code , w , expected ) \n 
~~ ~~ def test_builtin_function_or_method_comparisons ( self ) : \n 
~~~ expected = ( \n 
func = eval \n 
meth = { } . get \n 
~~~ self . assertWarning ( func < meth , w , expected ) \n 
self . assertWarning ( func > meth , w , expected ) \n 
self . assertWarning ( meth <= func , w , expected ) \n 
self . assertWarning ( meth >= func , w , expected ) \n 
self . assertNoWarning ( meth == func , w ) \n 
self . assertNoWarning ( meth != func , w ) \n 
lam = lambda x : x \n 
self . assertNoWarning ( lam == func , w ) \n 
self . assertNoWarning ( lam != func , w ) \n 
~~ ~~ def test_frame_attributes ( self ) : \n 
f = sys . _getframe ( 0 ) \n 
for attr in ( "f_exc_traceback" , "f_exc_value" , "f_exc_type" ) : \n 
~~~ expected = template % attr \n 
~~~ self . assertWarning ( getattr ( f , attr ) , w , expected ) \n 
self . assertWarning ( setattr ( f , attr , None ) , w , expected ) \n 
~~ ~~ ~~ def test_sort_cmp_arg ( self ) : \n 
lst = range ( 5 ) \n 
cmp = lambda x , y : - 1 \n 
~~~ self . assertWarning ( lst . sort ( cmp = cmp ) , w , expected ) \n 
self . assertWarning ( sorted ( lst , cmp = cmp ) , w , expected ) \n 
self . assertWarning ( lst . sort ( cmp ) , w , expected ) \n 
self . assertWarning ( sorted ( lst , cmp ) , w , expected ) \n 
~~ ~~ def test_sys_exc_clear ( self ) : \n 
~~~ self . assertWarning ( sys . exc_clear ( ) , w , expected ) \n 
~~ ~~ def test_methods_members ( self ) : \n 
class C : \n 
~~~ __methods__ = [ ] \n 
__members__ = [ ] \n 
~~ c = C ( ) \n 
~~~ self . assertWarning ( dir ( c ) , w , expected ) \n 
~~ ~~ def test_softspace ( self ) : \n 
with file ( __file__ ) as f : \n 
~~~ with check_py3k_warnings ( ) as w : \n 
~~~ self . assertWarning ( f . softspace , w , expected ) \n 
~~ def set ( ) : \n 
~~~ f . softspace = 0 \n 
~~~ self . assertWarning ( set ( ) , w , expected ) \n 
~~ ~~ ~~ def test_slice_methods ( self ) : \n 
~~~ class Spam ( object ) : \n 
~~~ def __getslice__ ( self , i , j ) : pass \n 
def __setslice__ ( self , i , j , what ) : pass \n 
def __delslice__ ( self , i , j ) : pass \n 
~~ class Egg : \n 
~~~ def __getslice__ ( self , i , h ) : pass \n 
for obj in ( Spam ( ) , Egg ( ) ) : \n 
~~~ self . assertWarning ( obj [ 1 : 2 ] , w , expected . format ( ) ) \n 
del obj [ 3 : 4 ] \n 
self . assertWarning ( None , w , expected . format ( ) ) \n 
obj [ 4 : 5 ] = "eggs" \n 
~~ ~~ ~~ def test_tuple_parameter_unpacking ( self ) : \n 
~~ ~~ def test_buffer ( self ) : \n 
~~~ self . assertWarning ( buffer ( ) , w , expected ) \n 
~~ ~~ def test_file_xreadlines ( self ) : \n 
~~~ self . assertWarning ( f . xreadlines ( ) , w , expected ) \n 
~~ ~~ ~~ def test_hash_inheritance ( self ) : \n 
~~~ class WarnOnlyCmp ( object ) : \n 
~~~ def __cmp__ ( self , other ) : pass \n 
~~ self . assertEqual ( len ( w . warnings ) , 0 ) \n 
class WarnOnlyEq ( object ) : \n 
~~~ def __eq__ ( self , other ) : pass \n 
~~ self . assertEqual ( len ( w . warnings ) , 1 ) \n 
self . assertWarning ( None , w , \n 
class WarnCmpAndEq ( object ) : \n 
def __eq__ ( self , other ) : pass \n 
class NoWarningOnlyHash ( object ) : \n 
~~~ def __hash__ ( self ) : pass \n 
class DefinesAllThree ( object ) : \n 
def __hash__ ( self ) : pass \n 
~~ class WarnOnlyCmp ( DefinesAllThree ) : \n 
class WarnOnlyEq ( DefinesAllThree ) : \n 
class WarnCmpAndEq ( DefinesAllThree ) : \n 
class NoWarningOnlyHash ( DefinesAllThree ) : \n 
~~ ~~ def test_operator ( self ) : \n 
~~~ from operator import isCallable , sequenceIncludes \n 
~~~ self . assertWarning ( isCallable ( self ) , w , callable_warn ) \n 
self . assertWarning ( sequenceIncludes ( range ( 3 ) , 2 ) , w , seq_warn ) \n 
~~ ~~ ~~ class TestStdlibRemovals ( unittest . TestCase ) : \n 
~~~ all_platforms = ( , , , , , , \n 
inclusive_platforms = { : ( , , , , , , \n 
, , , ) , \n 
: ( , , , \n 
: ( , ) , \n 
optional_modules = ( , , , , , \n 
def check_removal ( self , module_name , optional = False ) : \n 
with CleanImport ( module_name ) , warnings . catch_warnings ( ) : \n 
DeprecationWarning , __name__ ) \n 
~~~ __import__ ( module_name , level = 0 ) \n 
~~ except DeprecationWarning as exc : \n 
~~~ self . assertIn ( module_name , exc . args [ 0 ] , \n 
% module_name ) \n 
~~~ if not optional : \n 
"ImportError." . format ( module_name ) ) \n 
~~~ if not check_deprecated_module ( module_name ) : \n 
. format ( module_name ) ) \n 
~~ ~~ ~~ ~~ def test_platform_independent_removals ( self ) : \n 
~~~ for module_name in self . all_platforms : \n 
~~~ self . check_removal ( module_name ) \n 
~~ ~~ def test_platform_specific_removals ( self ) : \n 
~~~ for module_name in self . inclusive_platforms . get ( sys . platform , [ ] ) : \n 
~~~ self . check_removal ( module_name , optional = True ) \n 
~~ ~~ def test_optional_module_removals ( self ) : \n 
~~~ for module_name in self . optional_modules : \n 
~~ ~~ def test_os_path_walk ( self ) : \n 
def dumbo ( where , names , args ) : pass \n 
for path_mod in ( "ntpath" , "macpath" , "os2emxpath" , "posixpath" ) : \n 
~~~ mod = __import__ ( path_mod ) \n 
reset_module_registry ( mod ) \n 
~~~ mod . walk ( "crashers" , dumbo , None ) \n 
~~ self . assertEqual ( str ( w . message ) , msg ) \n 
~~ ~~ def test_reduce_move ( self ) : \n 
~~~ from operator import add \n 
reset_module_registry ( unittest . case ) \n 
with warnings . catch_warnings ( ) : \n 
~~~ warnings . filterwarnings ( "error" , "reduce" ) \n 
self . assertRaises ( DeprecationWarning , reduce , add , range ( 10 ) ) \n 
~~ ~~ def test_mutablestring_removal ( self ) : \n 
~~~ import UserString \n 
reset_module_registry ( UserString ) \n 
~~~ warnings . filterwarnings ( "error" , ".*MutableString" , \n 
DeprecationWarning ) \n 
self . assertRaises ( DeprecationWarning , UserString . MutableString ) \n 
~~~ run_unittest ( TestPy3KWarnings , \n 
TestStdlibRemovals ) \n 
__test__ = { : doctests } \n 
def test_main ( verbose = None ) : \n 
from test import test_setcomps \n 
test_support . run_doctest ( test_setcomps , verbose ) \n 
if verbose and hasattr ( sys , "gettotalrefcount" ) : \n 
~~~ import gc \n 
counts = [ None ] * 5 \n 
for i in range ( len ( counts ) ) : \n 
~~~ test_support . run_doctest ( test_setcomps , verbose ) \n 
gc . collect ( ) \n 
counts [ i ] = sys . gettotalrefcount ( ) \n 
~~ print ( counts ) \n 
~~~ test_main ( verbose = True ) \n 
from test . test_support import run_unittest , have_unicode \n 
class UnaryOpTestCase ( unittest . TestCase ) : \n 
~~~ def test_negative ( self ) : \n 
~~~ self . assertTrue ( - 2 == 0 - 2 ) \n 
self . assertTrue ( - 0 == 0 ) \n 
self . assertTrue ( - - 2 == 2 ) \n 
self . assertTrue ( - 2 L == 0 - 2 L ) \n 
self . assertTrue ( - 2.0 == 0 - 2.0 ) \n 
self . assertTrue ( - 2j == 0 - 2j ) \n 
~~ def test_positive ( self ) : \n 
~~~ self . assertTrue ( + 2 == 2 ) \n 
self . assertTrue ( + 0 == 0 ) \n 
self . assertTrue ( + + 2 == 2 ) \n 
self . assertTrue ( + 2 L == 2 L ) \n 
self . assertTrue ( + 2.0 == 2.0 ) \n 
self . assertTrue ( + 2j == 2j ) \n 
~~ def test_invert ( self ) : \n 
~~ def test_no_overflow ( self ) : \n 
~~~ nines = "9" * 32 \n 
self . assertTrue ( eval ( "+" + nines ) == eval ( "+" + nines + "L" ) ) \n 
self . assertTrue ( eval ( "-" + nines ) == eval ( "-" + nines + "L" ) ) \n 
self . assertTrue ( eval ( "~" + nines ) == eval ( "~" + nines + "L" ) ) \n 
~~ def test_negation_of_exponentiation ( self ) : \n 
~~~ self . assertEqual ( - 2 ** 3 , - 8 ) \n 
self . assertEqual ( ( - 2 ) ** 3 , - 8 ) \n 
self . assertEqual ( - 2 ** 4 , - 16 ) \n 
self . assertEqual ( ( - 2 ) ** 4 , 16 ) \n 
~~ def test_bad_types ( self ) : \n 
~~~ self . assertRaises ( TypeError , eval , op + "\'a\'" ) \n 
if have_unicode : \n 
~~~ self . assertRaises ( TypeError , eval , op + "u\'a\'" ) \n 
~~ ~~ self . assertRaises ( TypeError , eval , "~2j" ) \n 
self . assertRaises ( TypeError , eval , "~2.0" ) \n 
~~~ run_unittest ( UnaryOpTestCase ) \n 
import test . test_support \n 
import whichdb \n 
_fname = test . test_support . TESTFN \n 
anydbm = test . test_support . import_module ( , deprecated = True ) \n 
def _delete_files ( ) : \n 
~~~ for f in glob . glob ( _fname + "*" ) : \n 
~~~ os . unlink ( f ) \n 
~~ ~~ ~~ class WhichDBTestCase ( unittest . TestCase ) : \n 
~~~ def __init__ ( self , * args ) : \n 
~~~ unittest . TestCase . __init__ ( self , * args ) \n 
~~~ _delete_files ( ) \n 
~~ def setUp ( self ) : \n 
~~ ~~ for name in anydbm . _names : \n 
~~~ mod = test . test_support . import_module ( name , deprecated = True ) \n 
~~ except unittest . SkipTest : \n 
~~ def test_whichdb_name ( self , name = name , mod = mod ) : \n 
~~~ f = mod . open ( _fname , ) \n 
self . assertEqual ( name , whichdb . whichdb ( _fname ) ) \n 
f = mod . open ( _fname , ) \n 
f [ "1" ] = "1" \n 
~~ setattr ( WhichDBTestCase , "test_whichdb_%s" % name , test_whichdb_name ) \n 
~~~ test . test_support . run_unittest ( WhichDBTestCase ) \n 
from ctypes import wintypes , WINFUNCTYPE \n 
import mmap \n 
HandlerRoutine = WINFUNCTYPE ( wintypes . BOOL , wintypes . DWORD ) \n 
def _ctrl_handler ( sig ) : \n 
if sig == signal . CTRL_C_EVENT : \n 
~~ elif sig == signal . CTRL_BREAK_EVENT : \n 
~~ ctrl_handler = HandlerRoutine ( _ctrl_handler ) \n 
SetConsoleCtrlHandler = ctypes . windll . kernel32 . SetConsoleCtrlHandler \n 
SetConsoleCtrlHandler . argtypes = ( HandlerRoutine , wintypes . BOOL ) \n 
SetConsoleCtrlHandler . restype = wintypes . BOOL \n 
~~~ if not SetConsoleCtrlHandler ( ctrl_handler , 1 ) : \n 
exit ( - 1 ) \n 
~~ m = mmap . mmap ( - 1 , 1 , sys . argv [ 1 ] ) \n 
m [ 0 ] = \n 
TEST_IDS_PATH = "data/test_ids.npy" \n 
filenames = glob . glob ( "data/raw/images_test_rev1/*.jpg" ) \n 
test_ids = [ int ( os . path . basename ( s ) . replace ( ".jpg" , "" ) ) for s in filenames ] \n 
test_ids . sort ( ) \n 
test_ids = np . array ( test_ids ) \n 
np . save ( TEST_IDS_PATH , test_ids ) import numpy as np \n 
import layers \n 
import cc_layers \n 
import custom \n 
import load_data \n 
import realtime_augmentation as ra \n 
import cPickle as pickle \n 
from datetime import datetime , timedelta \n 
BATCH_SIZE = 16 \n 
NUM_INPUT_FEATURES = 3 \n 
LEARNING_RATE_SCHEDULE = { \n 
0 : 0.04 , \n 
1800 : 0.004 , \n 
2300 : 0.0004 , \n 
MOMENTUM = 0.9 \n 
WEIGHT_DECAY = 0.0 \n 
GEN_BUFFER_SIZE = 1 \n 
input_sizes = [ ( 69 , 69 ) , ( 69 , 69 ) ] \n 
ds_transforms = [ \n 
ra . build_ds_transform ( 3.0 , target_size = input_sizes [ 0 ] ) , \n 
ra . build_ds_transform ( 3.0 , target_size = input_sizes [ 1 ] ) + ra . build_augmentation_transform ( rotation ] \n 
num_input_representations = len ( ds_transforms ) \n 
augmentation_params = { \n 
: ( 1.0 / 1.3 , 1.3 ) , \n 
: ( 0 , 360 ) , \n 
: ( 0 , 0 ) , \n 
: ( - 4 , 4 ) , \n 
augmented_data_gen = ra . realtime_augmented_data_gen ( num_chunks = NUM_CHUNKS , chunk_size = CHUNK_SIZE , \n 
augmentation_params = augmentation_params , ds_transforms target_sizes = input_sizes , processor_class = ra . LoadAndProcessPysexCenteringRescaling \n 
post_augmented_data_gen = ra . post_augment_brightness_gen ( augmented_data_gen , std = 0.5 ) \n 
train_gen = load_data . buffered_gen_mp ( post_augmented_data_gen , buffer_size = GEN_BUFFER_SIZE ) \n 
y_train = np . load ( "data/solutions_train.npy" ) \n 
train_ids = load_data . train_ids \n 
test_ids = load_data . test_ids \n 
num_train = len ( train_ids ) \n 
num_test = len ( test_ids ) \n 
num_train -= num_valid \n 
y_valid = y_train [ num_train : ] \n 
y_train = y_train [ : num_train ] \n 
valid_ids = train_ids [ num_train : ] \n 
train_ids = train_ids [ : num_train ] \n 
train_indices = np . arange ( num_train ) \n 
valid_indices = np . arange ( num_train , num_train + num_valid ) \n 
test_indices = np . arange ( num_test ) \n 
def create_train_gen ( ) : \n 
data_gen_train = ra . realtime_fixed_augmented_data_gen ( train_indices , , \n 
ds_transforms = ds_transforms , chunk_size = CHUNK_SIZE , target_sizes = input_sizes , \n 
processor_class = ra . LoadAndProcessFixedPysexCenteringRescaling ) \n 
return load_data . buffered_gen_mp ( data_gen_train , buffer_size = GEN_BUFFER_SIZE ) \n 
~~ def create_valid_gen ( ) : \n 
~~~ data_gen_valid = ra . realtime_fixed_augmented_data_gen ( valid_indices , , \n 
return load_data . buffered_gen_mp ( data_gen_valid , buffer_size = GEN_BUFFER_SIZE ) \n 
~~ def create_test_gen ( ) : \n 
~~~ data_gen_test = ra . realtime_fixed_augmented_data_gen ( test_indices , , \n 
return load_data . buffered_gen_mp ( data_gen_test , buffer_size = GEN_BUFFER_SIZE ) \n 
xs_valid = [ [ ] for _ in xrange ( num_input_representations ) ] \n 
for data , length in create_valid_gen ( ) : \n 
~~~ for x_valid_list , x_chunk in zip ( xs_valid , data ) : \n 
~~~ x_valid_list . append ( x_chunk [ : length ] ) \n 
~~ ~~ xs_valid = [ np . vstack ( x_valid ) for x_valid in xs_valid ] \n 
l0 = layers . Input2DLayer ( BATCH_SIZE , NUM_INPUT_FEATURES , input_sizes [ 0 ] [ 0 ] , input_sizes [ 0 ] [ 1 ] ) \n 
l0_45 = layers . Input2DLayer ( BATCH_SIZE , NUM_INPUT_FEATURES , input_sizes [ 1 ] [ 0 ] , input_sizes [ 1 ] [ 1 ] ) \n 
l0r = layers . MultiRotSliceLayer ( [ l0 , l0_45 ] , part_size = 45 , include_flip = True ) \n 
l0s = cc_layers . ShuffleBC01ToC01BLayer ( l0r ) \n 
l1a = cc_layers . CudaConvnetConv2DLayer ( l0s , n_filters = 32 , filter_size = 8 , weights_std = 0.01 , init_bias_value l1 = cc_layers . CudaConvnetPooling2DLayer ( l1a , pool_size = 2 ) \n 
l2a = cc_layers . CudaConvnetConv2DLayer ( l1 , n_filters = 64 , filter_size = 4 , weights_std = 0.01 , init_bias_value l2 = cc_layers . CudaConvnetPooling2DLayer ( l2a , pool_size = 2 ) \n 
l3a = cc_layers . CudaConvnetConv2DLayer ( l2 , n_filters = 128 , filter_size = 3 , weights_std = 0.01 , init_bias_value l3b = cc_layers . CudaConvnetConv2DLayer ( l3a , n_filters = 128 , filter_size = 3 , pad = 0 , weights_std = 0.1 , init_bias_value l3 = cc_layers . CudaConvnetPooling2DLayer ( l3b , pool_size = 2 ) \n 
l3s = cc_layers . ShuffleC01BToBC01Layer ( l3 ) \n 
l4a = layers . DenseLayer ( j3 , n_outputs = 4096 , weights_std = 0.001 , init_bias_value = 0.01 , dropout = 0.5 , nonlinearity l4b = layers . FeatureMaxPoolingLayer ( l4a , pool_size = 2 , feature_dim = 1 , implementation = ) \n 
l4c = layers . DenseLayer ( l4b , n_outputs = 4096 , weights_std = 0.001 , init_bias_value = 0.01 , dropout = 0.5 , nonlinearity l4 = layers . FeatureMaxPoolingLayer ( l4c , pool_size = 2 , feature_dim = 1 , implementation = ) \n 
train_loss_nonorm = l6 . error ( normalisation = False ) \n 
valid_loss = l6 . error ( dropout_active = False ) \n 
all_parameters = layers . all_parameters ( l6 ) \n 
all_bias_parameters = layers . all_bias_parameters ( l6 ) \n 
xs_shared = [ theano . shared ( np . zeros ( ( 1 , 1 , 1 , 1 ) , dtype = theano . config . floatX ) ) for _ in xrange ( num_input_representations y_shared = theano . shared ( np . zeros ( ( 1 , 1 ) , dtype = theano . config . floatX ) ) \n 
learning_rate = theano . shared ( np . array ( LEARNING_RATE_SCHEDULE [ 0 ] , dtype = theano . config . floatX ) ) \n 
idx = T . lscalar ( ) \n 
givens = { \n 
l0 . input_var : xs_shared [ 0 ] [ idx * BATCH_SIZE : ( idx + 1 ) * BATCH_SIZE ] , \n 
l0_45 . input_var : xs_shared [ 1 ] [ idx * BATCH_SIZE : ( idx + 1 ) * BATCH_SIZE ] , \n 
l6 . target_var : y_shared [ idx * BATCH_SIZE : ( idx + 1 ) * BATCH_SIZE ] , \n 
train_nonorm = theano . function ( [ idx ] , train_loss_nonorm , givens = givens , updates = updates_nonorm ) \n 
train_norm = theano . function ( [ idx ] , train_loss , givens = givens , updates = updates ) \n 
compute_output = theano . function ( [ idx ] , l6 . predictions ( dropout_active = False ) , givens = givens , on_unused_input compute_features = theano . function ( [ idx ] , l4 . output ( dropout_active = False ) , givens = givens , on_unused_input \n 
prev_time = start_time \n 
num_batches_valid = x_valid . shape [ 0 ] // BATCH_SIZE \n 
losses_train = [ ] \n 
losses_valid = [ ] \n 
param_stds = [ ] \n 
for e in xrange ( NUM_CHUNKS ) : \n 
chunk_data , chunk_length = train_gen . next ( ) \n 
xs_chunk = chunk_data \n 
xs_chunk = [ x_chunk . transpose ( 0 , 3 , 1 , 2 ) for x_chunk in xs_chunk ] \n 
if e in LEARNING_RATE_SCHEDULE : \n 
~~~ current_lr = LEARNING_RATE_SCHEDULE [ e ] \n 
learning_rate . set_value ( LEARNING_RATE_SCHEDULE [ e ] ) \n 
~~ if e >= NUM_CHUNKS_NONORM : \n 
~~~ train = train_norm \n 
~~~ train = train_nonorm \n 
for x_shared , x_chunk in zip ( xs_shared , xs_chunk ) : \n 
~~~ x_shared . set_value ( x_chunk ) \n 
~~ y_shared . set_value ( y_chunk ) \n 
num_batches_chunk = x_chunk . shape [ 0 ] // BATCH_SIZE \n 
losses = [ ] \n 
for b in xrange ( num_batches_chunk ) : \n 
~~~ loss = train ( b ) \n 
losses . append ( loss ) \n 
~~ mean_train_loss = np . sqrt ( np . mean ( losses ) ) \n 
losses_train . append ( mean_train_loss ) \n 
param_stds . append ( [ p . std ( ) for p in layers . get_param_values ( l6 ) ] ) \n 
if ( ( e + 1 ) % VALIDATE_EVERY ) == 0 : \n 
print "VALIDATING" \n 
for x_shared , x_valid in zip ( xs_shared , xs_valid ) : \n 
~~~ x_shared . set_value ( x_valid ) \n 
~~ y_shared . set_value ( y_valid ) \n 
for b in xrange ( num_batches_valid ) : \n 
~~~ loss = compute_loss ( b ) \n 
~~ mean_valid_loss = np . sqrt ( np . mean ( losses ) ) \n 
losses_valid . append ( mean_valid_loss ) \n 
time_since_start = now - start_time \n 
time_since_prev = now - prev_time \n 
prev_time = now \n 
est_time_left = time_since_start * ( float ( NUM_CHUNKS - ( e + 1 ) ) / float ( e + 1 ) ) \n 
eta = datetime . now ( ) + timedelta ( seconds = est_time_left ) \n 
eta_str = eta . strftime ( "%c" ) \n 
predictions_list = [ ] \n 
~~~ predictions = compute_output ( b ) \n 
predictions_list . append ( predictions ) \n 
~~ all_predictions = np . vstack ( predictions_list ) \n 
all_predictions [ all_predictions > 1 ] = 1.0 \n 
all_predictions [ all_predictions < 0 ] = 0.0 \n 
with open ( ANALYSIS_PATH , ) as f : \n 
~~~ pickle . dump ( { \n 
: y_valid , \n 
: mean_train_loss , \n 
: mean_valid_loss , \n 
: time_since_start , \n 
: losses_train , \n 
: losses_valid , \n 
: layers . get_param_values ( l6 ) , \n 
: param_stds , \n 
} , f , pickle . HIGHEST_PROTOCOL ) \n 
for e , ( xs_chunk , chunk_length ) in enumerate ( create_test_gen ( ) ) : \n 
~~ ~~ all_predictions = np . vstack ( predictions_list ) \n 
with open ( TARGET_PATH , ) as csvfile : \n 
~~~ writer = csv . writer ( csvfile ) \n 
writer . writerow ( [ , , , , , , \n 
for k in xrange ( test_ids . shape [ 0 ] ) : \n 
~~~ row = [ test_ids [ k ] ] + all_predictions [ k ] . tolist ( ) \n 
writer . writerow ( row ) \n 
~~ ~~ print "Gzipping..." \n 
print "Done!" \n 
import lasagne as nn \n 
import data \n 
import load \n 
import nn_plankton \n 
import dihedral \n 
import dihedral_fast \n 
import tmp_dnn \n 
import tta \n 
validation_split_path = "splits/bagging_split_8.pkl" \n 
patch_size = ( 92 , 92 ) \n 
: ( 1 / 1.6 , 1.6 ) , \n 
: ( - 20 , 20 ) , \n 
: ( - 10 , 10 ) , \n 
: 1.3 , \n 
batch_size = 128 // 4 \n 
chunk_size = 32768 // 4 \n 
num_chunks_train = 840 \n 
momentum = 0.9 \n 
learning_rate_schedule = { \n 
0 : 0.0015 , \n 
700 : 0.00015 , \n 
800 : 0.000015 , \n 
validate_every = 20 \n 
save_every = 20 \n 
def estimate_scale ( img ) : \n 
~~~ return np . maximum ( img . shape [ 0 ] , img . shape [ 1 ] ) / 85.0 \n 
~~ augmentation_transforms_test = tta . build_quasirandom_transforms ( 70 , ** { \n 
: ( 1 / 1.4 , 1.4 ) , \n 
: ( - 8 , 8 ) , \n 
: 1.2 , \n 
data_loader = load . ZmuvRescaledDataLoader ( estimate_scale = estimate_scale , num_chunks_train = num_chunks_train patch_size = patch_size , chunk_size = chunk_size , augmentation_params = augmentation_params , \n 
augmentation_transforms_test = augmentation_transforms_test , validation_split_path = validation_split_path \n 
Conv2DLayer = tmp_dnn . Conv2DDNNLayer \n 
MaxPool2DLayer = tmp_dnn . MaxPool2DDNNLayer \n 
def conv ( incoming , ** kwargs ) : \n 
~~~ return Conv2DLayer ( incoming , border_mode = "same" , \n 
W = nn_plankton . Conv2DOrthogonal ( 1.0 ) , b = nn . init . Constant ( 0.1 ) , \n 
nonlinearity = nn_plankton . leaky_relu , untie_biases = True , \n 
~~ convroll = dihedral_fast . CyclicConvRollLayer \n 
def pool ( incoming , ** kwargs ) : \n 
~~~ return MaxPool2DLayer ( incoming , ds = ( 3 , 3 ) , strides = ( 2 , 2 ) , ** kwargs ) \n 
~~ def build_model ( ) : \n 
~~~ l0 = nn . layers . InputLayer ( ( batch_size , 1 , patch_size [ 0 ] , patch_size [ 1 ] ) ) \n 
l0c = dihedral . CyclicSliceLayer ( l0 ) \n 
l1 = convroll ( conv ( l0c , num_filters = 16 , filter_size = ( 7 , 7 ) , strides = ( 2 , 2 ) ) ) \n 
l2 = convroll ( conv ( l1 , num_filters = 32 , filter_size = ( 7 , 7 ) , strides = ( 2 , 2 ) ) ) \n 
l3a = convroll ( conv ( l2 , num_filters = 32 , filter_size = ( 3 , 3 ) ) ) \n 
l3b = convroll ( conv ( l3a , num_filters = 32 , filter_size = ( 3 , 3 ) ) ) \n 
l3c = convroll ( conv ( l3b , num_filters = 32 , filter_size = ( 3 , 3 ) ) ) \n 
l3d = conv ( l3c , num_filters = 64 , filter_size = ( 3 , 3 ) ) \n 
l3 = convroll ( pool ( l3d ) ) \n 
l4a = convroll ( conv ( l3 , num_filters = 64 , filter_size = ( 3 , 3 ) ) ) \n 
l4b = convroll ( conv ( l4a , num_filters = 64 , filter_size = ( 3 , 3 ) ) ) \n 
l4c = convroll ( conv ( l4b , num_filters = 64 , filter_size = ( 3 , 3 ) ) ) \n 
l4d = conv ( l4c , num_filters = 128 , filter_size = ( 3 , 3 ) ) \n 
l4 = convroll ( pool ( l4d ) ) \n 
l5a = convroll ( conv ( l4 , num_filters = 64 , filter_size = ( 3 , 3 ) ) ) \n 
l5b = convroll ( conv ( l5a , num_filters = 64 , filter_size = ( 3 , 3 ) ) ) \n 
l5c = convroll ( conv ( l5b , num_filters = 64 , filter_size = ( 3 , 3 ) ) ) \n 
l5d = conv ( l5c , num_filters = 128 , filter_size = ( 3 , 3 ) ) \n 
l5 = convroll ( pool ( l5d ) ) \n 
l5f = nn . layers . flatten ( l5 ) \n 
l6 = nn . layers . DenseLayer ( nn . layers . dropout ( l5f , p = 0.5 ) , num_units = 256 , W = nn_plankton . Orthogonal l6r = dihedral_fast . CyclicRollLayer ( l6 ) \n 
l7 = nn . layers . DenseLayer ( nn . layers . dropout ( l6r , p = 0.5 ) , num_units = 256 , W = nn_plankton . Orthogonal l7m = dihedral . CyclicPoolLayer ( l7 , pool_function = nn_plankton . rms ) \n 
l8 = nn . layers . DenseLayer ( nn . layers . dropout ( l7m , p = 0.5 ) , num_units = data . num_classes , nonlinearity \n 
return [ l0 ] , l8 \n 
~~ def build_objective ( l_ins , l_out ) : \n 
~~~ lambda_reg = 0.0005 \n 
params = nn . layers . get_all_non_bias_params ( l_out ) \n 
reg_term = sum ( T . sum ( p ** 2 ) for p in params ) \n 
def loss ( y , t ) : \n 
~~~ return nn_plankton . log_loss ( y , t ) + lambda_reg * reg_term \n 
~~ return nn . objectives . Objective ( l_out , loss_function = loss ) import numpy as np \n 
patch_sizes = [ ( 95 , 95 ) , ( 47 , 47 ) ] \n 
0 : 0.003 , \n 
700 : 0.0003 , \n 
800 : 0.00003 , \n 
augmentation_transforms_test = tta . build_quasirandom_transforms ( 70 , ** { \n 
data_loader = load . ZmuvMultiscaleDataLoader ( scale_factors = scale_factors , num_chunks_train = num_chunks_train patch_sizes = patch_sizes , chunk_size = chunk_size , augmentation_params = augmentation_params , \n 
augmentation_transforms_test = augmentation_transforms_test ) \n 
def build_model ( ) : \n 
~~~ l0_variable = nn . layers . InputLayer ( ( batch_size , 1 , patch_sizes [ 0 ] [ 0 ] , patch_sizes [ 0 ] [ 1 ] ) ) \n 
l0c = dihedral . CyclicSliceLayer ( l0_variable ) \n 
l1a = Conv2DLayer ( l0c , num_filters = 32 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l1b = Conv2DLayer ( l1a , num_filters = 32 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l1 = MaxPool2DLayer ( l1b , ds = ( 3 , 3 ) , strides = ( 2 , 2 ) ) \n 
l2a = Conv2DLayer ( l1 , num_filters = 64 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l2b = Conv2DLayer ( l2a , num_filters = 64 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l2 = MaxPool2DLayer ( l2b , ds = ( 3 , 3 ) , strides = ( 2 , 2 ) ) \n 
l3a = Conv2DLayer ( l2 , num_filters = 128 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l3b = Conv2DLayer ( l3a , num_filters = 128 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l3c = Conv2DLayer ( l3b , num_filters = 128 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l3 = MaxPool2DLayer ( l3c , ds = ( 3 , 3 ) , strides = ( 2 , 2 ) ) \n 
l4a = Conv2DLayer ( l3 , num_filters = 256 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l4b = Conv2DLayer ( l4a , num_filters = 256 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l4c = Conv2DLayer ( l4b , num_filters = 256 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l4 = MaxPool2DLayer ( l4c , ds = ( 3 , 3 ) , strides = ( 2 , 2 ) ) \n 
l4f = nn . layers . flatten ( l4 ) \n 
l5 = nn . layers . DenseLayer ( nn . layers . dropout ( l4f , p = 0.5 ) , num_units = 256 , W = nn_plankton . Orthogonal l5r = dihedral . CyclicRollLayer ( l5 ) \n 
l6 = nn . layers . DenseLayer ( nn . layers . dropout ( l5r , p = 0.5 ) , num_units = 256 , W = nn_plankton . Orthogonal l_variable = dihedral . CyclicPoolLayer ( l6 , pool_function = nn_plankton . rms ) \n 
l0_fixed = nn . layers . InputLayer ( ( batch_size , 1 , patch_sizes [ 1 ] [ 0 ] , patch_sizes [ 1 ] [ 1 ] ) ) \n 
l0c = dihedral . CyclicSliceLayer ( l0_fixed ) \n 
l1a = Conv2DLayer ( l0c , num_filters = 16 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l1b = Conv2DLayer ( l1a , num_filters = 16 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l1 = MaxPool2DLayer ( l1b , ds = ( 3 , 3 ) , strides = ( 2 , 2 ) ) \n 
l2a = Conv2DLayer ( l1 , num_filters = 32 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l2b = Conv2DLayer ( l2a , num_filters = 32 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l2 = MaxPool2DLayer ( l2b , ds = ( 3 , 3 ) , strides = ( 2 , 2 ) ) \n 
l3a = Conv2DLayer ( l2 , num_filters = 64 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l3b = Conv2DLayer ( l3a , num_filters = 64 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l3c = Conv2DLayer ( l3b , num_filters = 64 , filter_size = ( 3 , 3 ) , border_mode = "same" , W = nn_plankton . Conv2DOrthogonal l3 = MaxPool2DLayer ( l3c , ds = ( 3 , 3 ) , strides = ( 2 , 2 ) ) \n 
l3f = nn . layers . flatten ( l3 ) \n 
l4 = nn . layers . DenseLayer ( nn . layers . dropout ( l3f , p = 0.5 ) , num_units = 128 , W = nn_plankton . Orthogonal l4r = dihedral . CyclicRollLayer ( l4 ) \n 
l5 = nn . layers . DenseLayer ( nn . layers . dropout ( l4r , p = 0.5 ) , num_units = 128 , W = nn_plankton . Orthogonal l_fixed = dihedral . CyclicPoolLayer ( l5 , pool_function = nn_plankton . rms ) \n 
l_merged = nn . layers . concat ( [ l_variable , l_fixed ] ) \n 
l7 = nn . layers . DenseLayer ( nn . layers . dropout ( l_merged , p = 0.5 ) , num_units = data . num_classes , nonlinearity \n 
return [ l0_variable , l0_fixed ] , l7 \n 
features = [ \n 
"haralick" , \n 
batch_size = 128 \n 
chunk_size = 32768 \n 
num_chunks_train = 240 \n 
0 : 0.001 , \n 
100 : 0.0001 , \n 
200 : 0.00001 , \n 
validate_every = 40 \n 
save_every = 40 \n 
sdir = "/mnt/storage/users/avdnoord/git/kaggle-plankton/predictions/" \n 
train_pred_file = sdir + "train--pl_blend4_convroll4_fastpool_1024_weightdecay_no_dropout_33_66--pl_blend4_convroll4_fastpool_1024_weightdecay_no_dropout_33_66-koe-20150310-195908--avg-probs.npy" valid_pred_file = sdir + "valid--pl_blend4_convroll4_fastpool_1024_weightdecay_no_dropout_33_66--pl_blend4_convroll4_fastpool_1024_weightdecay_no_dropout_33_66-koe-20150310-195908--avg-probs.npy" test_pred_file = sdir + "test--pl_blend4_convroll4_fastpool_1024_weightdecay_no_dropout_33_66--pl_blend4_convroll4_fastpool_1024_weightdecay_no_dropout_33_66-koe-20150310-195908--avg-probs.npy" \n 
data_loader = load . PredictionsWithFeaturesDataLoader ( \n 
features = features , \n 
train_pred_file = train_pred_file , \n 
valid_pred_file = valid_pred_file , \n 
test_pred_file = test_pred_file , \n 
num_chunks_train = num_chunks_train , \n 
chunk_size = chunk_size ) \n 
create_train_gen = lambda : data_loader . create_random_gen ( ) \n 
create_eval_train_gen = lambda : data_loader . create_fixed_gen ( "train" ) \n 
create_eval_valid_gen = lambda : data_loader . create_fixed_gen ( "valid" ) \n 
create_eval_test_gen = lambda : data_loader . create_fixed_gen ( "test" ) \n 
~~~ l0 = nn . layers . InputLayer ( ( batch_size , data . num_classes ) ) \n 
l0_size = nn . layers . InputLayer ( ( batch_size , 52 ) ) \n 
l1_size = nn . layers . DenseLayer ( l0_size , num_units = 80 , W = nn_plankton . Orthogonal ( ) , b = nn . init l2_size = nn . layers . DenseLayer ( l1_size , num_units = 80 , W = nn_plankton . Orthogonal ( ) , b = nn . init l3_size = nn . layers . DenseLayer ( l2_size , num_units = data . num_classes , W = nn_plankton . Orthogonal ( ) , \n 
l1 = nn_plankton . NonlinLayer ( l0 , T . log ) \n 
ltot = nn . layers . ElemwiseSumLayer ( [ l1 , l3_size ] ) \n 
lout = nn_plankton . NonlinLayer ( ltot , nonlinearity = T . nnet . softmax ) \n 
return [ l0 , l0_size ] , lout \n 
~~~ reg_param = 0.0002 \n 
print "regu" , reg_param , alpha \n 
L2 = sum ( T . sum ( p ** 2 ) for p in params ) \n 
L1 = sum ( T . sum ( T . abs_ ( p ) ) for p in params ) \n 
~~~ return nn_plankton . log_loss ( y , t ) + reg_param * ( alpha * L1 + ( 1 - alpha ) * L2 ) \n 
~~ return nn . objectives . Objective ( l_out , loss_function = loss ) \n 
~~ """momentsinfo_convroll4_doublescale_fs5""" \n 
train_pred_file = sdir + "train--pl_blend5_convroll5_doublescale_fs5_some_dropout_33_66--pl_blend5_convroll5_doublescale_fs5_some_dropout_33_66-koe-20150314-140938--avg-probs.npy" valid_pred_file = sdir + "valid--pl_blend5_convroll5_doublescale_fs5_some_dropout_33_66--pl_blend5_convroll5_doublescale_fs5_some_dropout_33_66-koe-20150314-140938--avg-probs.npy" test_pred_file = sdir + "test--pl_blend5_convroll5_doublescale_fs5_some_dropout_33_66--pl_blend5_convroll5_doublescale_fs5_some_dropout_33_66-koe-20150314-140938--avg-probs.npy" \n 
data_loader = load . PredictionsWithMomentsDataLoader ( train_pred_file = train_pred_file , valid_pred_file num_chunks_train = num_chunks_train , chunk_size = chunk_size \n 
l0_size = nn . layers . InputLayer ( ( batch_size , 7 ) ) \n 
~~~ print "regu" \n 
lambda_reg = 0.002 \n 
from scipy . special import erfinv \n 
def uniform ( sample , lo = - 1 , hi = 1 ) : \n 
~~~ return lo + ( hi - lo ) * sample \n 
~~ def normal ( sample , avg = 0.0 , std = 1.0 ) : \n 
~~~ return avg + std * np . sqrt ( 2 ) * erfinv ( 2 * sample - 1 ) \n 
~~ def lognormal ( sample , avg = 0.0 , std = 1.0 ) : \n 
~~~ return np . exp ( normal ( sample , avg , std ) ) \n 
~~ def bernoulli ( sample , p = 0.5 ) : \n 
~~~ return ( sample > p ) \n 
~~ from bigml . resourcehandler import ResourceHandler \n 
from bigml . resourcehandler import ( check_resource_type , get_resource_type , \n 
check_resource , \n 
get_anomaly_score_id , get_anomaly_id ) \n 
from bigml . constants import ( ANOMALY_SCORE_PATH , ANOMALY_PATH , \n 
TINY_RESOURCE ) \n 
class AnomalyScoreHandler ( ResourceHandler ) : \n 
self . anomaly_score_url = self . url + ANOMALY_SCORE_PATH \n 
~~ def create_anomaly_score ( self , anomaly , input_data = None , \n 
args = None , wait_time = 3 , retries = 10 ) : \n 
anomaly_id = None \n 
resource_type = get_resource_type ( anomaly ) \n 
if resource_type == ANOMALY_PATH : \n 
~~~ anomaly_id = get_anomaly_id ( anomaly ) \n 
check_resource ( anomaly_id , \n 
query_string = TINY_RESOURCE , \n 
wait_time = wait_time , retries = retries , \n 
raise_on_error = True , api = self ) \n 
~~ if input_data is None : \n 
~~~ input_data = { } \n 
~~ create_args = { } \n 
if args is not None : \n 
~~~ create_args . update ( args ) \n 
~~ create_args . update ( { \n 
"input_data" : input_data } ) \n 
create_args . update ( { \n 
"anomaly" : anomaly_id } ) \n 
body = json . dumps ( create_args ) \n 
return self . _create ( self . anomaly_score_url , body , \n 
verify = self . verify ) \n 
~~ def get_anomaly_score ( self , anomaly_score ) : \n 
check_resource_type ( anomaly_score , ANOMALY_SCORE_PATH , \n 
anomaly_score_id = get_anomaly_score_id ( anomaly_score ) \n 
if anomaly_score_id : \n 
~~~ return self . _get ( "%s%s" % ( self . url , anomaly_score_id ) ) \n 
~~ ~~ def list_anomaly_scores ( self , query_string = ) : \n 
return self . _list ( self . anomaly_score_url , query_string ) \n 
~~ def update_anomaly_score ( self , anomaly_score , changes ) : \n 
~~~ body = json . dumps ( changes ) \n 
return self . _update ( "%s%s" % ( self . url , anomaly_score_id ) , body ) \n 
~~ ~~ def delete_anomaly_score ( self , anomaly_score ) : \n 
~~~ return self . _delete ( "%s%s" % ( self . url , anomaly_score_id ) ) \n 
from bigml . api import FINISHED \n 
from bigml . api import ( BigML , get_logistic_regression_id , get_status ) \n 
from bigml . util import cast \n 
from bigml . basemodel import retrieve_resource , extract_objective \n 
from bigml . basemodel import ONLY_MODEL \n 
from bigml . model import STORAGE \n 
from bigml . predicate import TM_TOKENS , TM_FULL_TERM \n 
from bigml . modelfields import ModelFields \n 
from bigml . cluster import parse_terms , parse_items \n 
LOGGER = logging . getLogger ( ) \n 
EXPANSION_ATTRIBUTES = { "categorical" : "categories" , "text" : "tag_cloud" , \n 
"items" : "items" } \n 
def get_unique_terms ( terms , term_forms , tag_cloud ) : \n 
extend_forms = { } \n 
for term , forms in term_forms . items ( ) : \n 
~~~ for form in forms : \n 
~~~ extend_forms [ form ] = term \n 
~~ extend_forms [ term ] = term \n 
~~ terms_set = { } \n 
for term in terms : \n 
~~~ if term in tag_cloud : \n 
~~~ if term not in terms_set : \n 
~~~ terms_set [ term ] = 0 \n 
~~ terms_set [ term ] += 1 \n 
~~ elif term in extend_forms : \n 
~~~ term = extend_forms [ term ] \n 
if term not in terms_set : \n 
~~ ~~ return terms_set . items ( ) \n 
~~ class LogisticRegression ( ModelFields ) : \n 
def __init__ ( self , logistic_regression , api = None ) : \n 
~~~ self . resource_id = None \n 
self . term_forms = { } \n 
self . tag_clouds = { } \n 
self . term_analysis = { } \n 
self . items = { } \n 
self . item_analysis = { } \n 
self . categories = { } \n 
self . coefficients = { } \n 
self . data_field_types = { } \n 
self . numeric_fields = { } \n 
self . bias = None \n 
self . missing_numerics = None \n 
self . c = None \n 
self . eps = None \n 
self . lr_normalize = None \n 
self . regularization = None \n 
if not ( isinstance ( logistic_regression , dict ) \n 
and in logistic_regression and \n 
logistic_regression [ ] is not None ) : \n 
~~~ if api is None : \n 
~~~ api = BigML ( storage = STORAGE ) \n 
~~ self . resource_id = get_logistic_regression_id ( logistic_regression ) \n 
if self . resource_id is None : \n 
api . error_message ( logistic_regression , \n 
resource_type = , \n 
method = ) ) \n 
~~ query_string = ONLY_MODEL \n 
logistic_regression = retrieve_resource ( \n 
api , self . resource_id , query_string = query_string ) \n 
~~~ self . resource_id = get_logistic_regression_id ( logistic_regression ) \n 
~~ if in logistic_regression and isinstance ( logistic_regression [ ] , dict ) : \n 
~~~ logistic_regression = logistic_regression [ ] \n 
~~~ self . dataset_field_types = logistic_regression . get ( \n 
"dataset_field_types" , { } ) \n 
objective_field = logistic_regression [ ] if logistic_regression [ ] else logistic_regression [ ] \n 
~~~ status = get_status ( logistic_regression ) \n 
if in status and status [ ] == FINISHED : \n 
~~~ logistic_regression_info = logistic_regression [ ] \n 
fields = logistic_regression_info . get ( , { } ) \n 
self . coefficients . update ( logistic_regression_info . get ( , [ ] ) ) \n 
self . bias = logistic_regression_info . get ( , 0 ) \n 
self . c = logistic_regression_info . get ( ) \n 
self . eps = logistic_regression_info . get ( ) \n 
self . lr_normalize = logistic_regression_info . get ( ) \n 
self . regularization = logistic_regression_info . get ( ) \n 
self . missing_numerics = logistic_regression_info . get ( , False ) \n 
objective_id = extract_objective ( objective_field ) \n 
for field_id , field in fields . items ( ) : \n 
~~~ if field [ ] == : \n 
~~~ self . term_forms [ field_id ] = { } \n 
self . term_forms [ field_id ] . update ( \n 
field [ ] [ ] ) \n 
self . tag_clouds [ field_id ] = [ ] \n 
self . tag_clouds [ field_id ] = [ tag for [ tag , _ ] in field [ \n 
] [ ] ] \n 
self . term_analysis [ field_id ] = { } \n 
self . term_analysis [ field_id ] . update ( \n 
field [ ] ) \n 
~~ if field [ ] == : \n 
~~~ self . items [ field_id ] = [ ] \n 
self . items [ field_id ] = [ item for item , _ in field [ ] [ ] ] \n 
self . item_analysis [ field_id ] = { } \n 
self . item_analysis [ field_id ] . update ( \n 
~~~ self . categories [ field_id ] = [ category for [ category , _ ] in field [ ] [ ] ] \n 
~~ if self . missing_numerics and field [ ] == : \n 
~~~ self . numeric_fields [ field_id ] = True \n 
~~ ~~ ModelFields . __init__ ( \n 
self , fields , \n 
objective_id = objective_id ) \n 
self . map_coefficients ( ) \n 
logistic_regression ) \n 
~~ ~~ def predict ( self , input_data , by_name = True ) : \n 
input_data = self . filter_input_data ( input_data , by_name = by_name ) \n 
if not self . missing_numerics : \n 
~~~ for field_id , field in self . fields . items ( ) : \n 
~~~ if ( not field [ ] in OPTIONAL_FIELDS and \n 
not field_id in input_data ) : \n 
~~ ~~ ~~ cast ( input_data , self . fields ) \n 
unique_terms = self . get_unique_terms ( input_data ) \n 
probabilities = { } \n 
total = 0 \n 
for category in self . coefficients . keys ( ) : \n 
~~~ coefficients = self . coefficients [ category ] \n 
probabilities [ category ] = self . category_probability ( \n 
input_data , unique_terms , coefficients ) \n 
total += probabilities [ category ] \n 
~~ for category in probabilities . keys ( ) : \n 
~~~ probabilities [ category ] /= total \n 
~~ predictions = sorted ( probabilities . items ( ) , \n 
key = lambda x : x [ 1 ] , reverse = True ) \n 
prediction , probability = predictions [ 0 ] \n 
"prediction" : prediction , \n 
"probability" : probability , \n 
"distribution" : [ { "category" : category , "probability" : probability } \n 
for category , probability in predictions ] } \n 
~~ def category_probability ( self , input_data , unique_terms , coefficients ) : \n 
probability = 0 \n 
for field_id in input_data : \n 
~~~ shift = self . fields [ field_id ] [ ] \n 
probability += coefficients [ shift ] * input_data [ field_id ] \n 
~~ for field_id in unique_terms : \n 
for term , occurrences in unique_terms [ field_id ] : \n 
~~~ if field_id in self . tag_clouds : \n 
~~~ index = self . tag_clouds [ field_id ] . index ( term ) \n 
~~ elif field_id in self . items : \n 
~~~ index = self . items [ field_id ] . index ( term ) \n 
~~ elif field_id in self . categories : \n 
~~~ index = self . categories [ field_id ] . index ( term ) \n 
~~ probability += coefficients [ shift + index ] * occurrences \n 
~~ ~~ ~~ for field_id in self . numeric_fields : \n 
~~~ if field_id not in input_data : \n 
~~~ shift = self . fields [ field_id ] [ ] + 1 \n 
probability += coefficients [ shift ] \n 
~~ ~~ for field_id in self . tag_clouds : \n 
if field_id not in unique_terms or not unique_terms [ field_id ] : \n 
~~~ probability += coefficients [ shift + len ( self . tag_clouds [ field_id ] ) ] \n 
~~ ~~ for field_id in self . items : \n 
~~~ probability += coefficients [ shift + len ( self . items [ field_id ] ) ] \n 
~~ ~~ for field_id in self . categories : \n 
~~~ if field_id != self . objective_id and field_id not in unique_terms : \n 
probability += coefficients [ shift + len ( self . categories [ field_id ] ) ] \n 
~~ ~~ probability += coefficients [ - 1 ] \n 
probability = 1 / ( 1 + math . exp ( - probability ) ) \n 
return probability \n 
~~ def get_unique_terms ( self , input_data ) : \n 
unique_terms = { } \n 
for field_id in self . term_forms : \n 
~~~ if field_id in input_data : \n 
~~~ input_data_field = input_data . get ( field_id , ) \n 
if isinstance ( input_data_field , basestring ) : \n 
~~~ case_sensitive = self . term_analysis [ field_id ] . get ( \n 
, True ) \n 
token_mode = self . term_analysis [ field_id ] . get ( \n 
if token_mode != TM_FULL_TERM : \n 
~~~ terms = parse_terms ( input_data_field , \n 
case_sensitive = case_sensitive ) \n 
~~~ terms = [ ] \n 
~~ if token_mode != TM_TOKENS : \n 
~~~ terms . append ( \n 
input_data_field if case_sensitive \n 
else input_data_field . lower ( ) ) \n 
~~ unique_terms [ field_id ] = get_unique_terms ( \n 
terms , self . term_forms [ field_id ] , \n 
self . tag_clouds . get ( field_id , [ ] ) ) \n 
~~~ unique_terms [ field_id ] = [ ( input_data_field , 1 ) ] \n 
~~ del input_data [ field_id ] \n 
~~ ~~ for field_id in self . item_analysis : \n 
~~~ separator = self . item_analysis [ field_id ] . get ( \n 
regexp = self . item_analysis [ field_id ] . get ( \n 
if regexp is None : \n 
~~~ regexp = ur % re . escape ( separator ) \n 
~~ terms = parse_items ( input_data_field , regexp ) \n 
unique_terms [ field_id ] = get_unique_terms ( \n 
terms , { } , \n 
self . items . get ( field_id , [ ] ) ) \n 
unique_terms [ field_id ] = [ ( input_data_field , 1 ) ] \n 
del input_data [ field_id ] \n 
~~ ~~ return unique_terms \n 
~~ def map_coefficients ( self ) : \n 
field_ids = [ field_id for field_id , _ in \n 
sorted ( self . fields . items ( ) , \n 
key = lambda x : x [ 1 ] . get ( "column_number" ) ) \n 
if field_id != self . objective_id ] \n 
shift = 0 \n 
for field_id in field_ids : \n 
~~~ optype = self . fields [ field_id ] [ ] \n 
if optype in EXPANSION_ATTRIBUTES . keys ( ) : \n 
~~~ length = len ( self . fields [ field_id ] [ ] [ EXPANSION_ATTRIBUTES [ optype ] ] ) \n 
length += 1 \n 
~~~ length = 2 if self . missing_numerics else 1 \n 
~~ self . fields [ field_id ] [ ] = shift \n 
shift += length \n 
~~ ~~ ~~ import time \n 
from world import world \n 
from bigml . api import HTTP_CREATED \n 
from bigml . api import FAULTY \n 
from bigml . api import get_status \n 
from read_evaluation_steps import i_get_the_evaluation \n 
def i_create_an_evaluation ( step ) : \n 
~~~ dataset = world . dataset . get ( ) \n 
model = world . model . get ( ) \n 
resource = world . api . create_evaluation ( model , dataset ) \n 
world . status = resource [ ] \n 
assert world . status == HTTP_CREATED \n 
world . location = resource [ ] \n 
world . evaluation = resource [ ] \n 
world . evaluations . append ( resource [ ] ) \n 
~~ def i_create_an_evaluation_ensemble ( step ) : \n 
ensemble = world . ensemble . get ( ) \n 
resource = world . api . create_evaluation ( ensemble , dataset ) \n 
~~ def wait_until_evaluation_status_code_is ( step , code1 , code2 , secs ) : \n 
~~~ start = datetime . utcnow ( ) \n 
i_get_the_evaluation ( step , world . evaluation [ ] ) \n 
status = get_status ( world . evaluation ) \n 
while ( status [ ] != int ( code1 ) and \n 
status [ ] != int ( code2 ) ) : \n 
~~~ time . sleep ( 3 ) \n 
assert datetime . utcnow ( ) - start < timedelta ( seconds = int ( secs ) ) \n 
~~ assert status [ ] == int ( code1 ) \n 
~~ def the_evaluation_is_finished_in_less_than ( step , secs ) : \n 
~~~ wait_until_evaluation_status_code_is ( step , FINISHED , FAULTY , secs ) \n 
~~ def the_measured_measure_is_value ( step , measure , value ) : \n 
~~~ ev = world . evaluation [ ] [ ] [ measure ] + 0.0 \n 
measure , ev , float ( value ) ) \n 
~~ def the_measured_measure_is_greater_value ( step , measure , value ) : \n 
~~~ assert world . evaluation [ ] [ ] [ measure ] + 0.0 > float ( value ) \n 
~~ from world import world \n 
from bigml . api import HTTP_OK \n 
def i_get_the_script ( step , resource ) : \n 
~~~ resource = world . api . get_script ( resource ) \n 
assert world . status == HTTP_OK \n 
world . script = resource [ ] \n 
from world import world , setup_module , teardown_module \n 
import create_source_steps as source_create \n 
import create_dataset_steps as dataset_create \n 
import create_statistical_tst_steps as statistical_tst_create \n 
class TestStatisticalTest ( object ) : \n 
~~~ def setup ( self ) : \n 
~~ def teardown ( self ) : \n 
~~ def test_scenario1 ( self ) : \n 
print self . test_scenario1 . __doc__ \n 
examples = [ \n 
[ , , , , , ] ] \n 
for example in examples : \n 
source_create . i_upload_a_file ( self , example [ 0 ] ) \n 
source_create . the_source_is_finished ( self , example [ 1 ] ) \n 
dataset_create . i_create_a_dataset ( self ) \n 
dataset_create . the_dataset_is_finished_in_less_than ( self , example [ 2 ] ) \n 
statistical_tst_create . i_create_a_tst_from_dataset ( self ) \n 
statistical_tst_create . the_tst_is_finished_in_less_than ( self , example [ 3 ] ) \n 
statistical_tst_create . i_update_tst_name ( self , example [ 5 ] ) \n 
statistical_tst_create . the_tst_is_finished_in_less_than ( self , example [ 4 ] ) \n 
statistical_tst_create . i_check_tst_name ( self , example [ 5 ] ) \n 
import elasticsearch . helpers \n 
from elasticsearch import Elasticsearch \n 
from pyspider . database . base . projectdb import ProjectDB as BaseProjectDB \n 
class ProjectDB ( BaseProjectDB ) : \n 
~~~ __type__ = \n 
def __init__ ( self , hosts , index = ) : \n 
~~~ self . index = index \n 
self . es = Elasticsearch ( hosts = hosts ) \n 
self . es . indices . create ( index = self . index , ignore = 400 ) \n 
if not self . es . indices . get_mapping ( index = self . index , doc_type = self . __type__ ) : \n 
~~~ self . es . indices . put_mapping ( index = self . index , doc_type = self . __type__ , body = { \n 
"_all" : { "enabled" : False } , \n 
"properties" : { \n 
"updatetime" : { "type" : "double" } \n 
~~ ~~ def insert ( self , name , obj = { } ) : \n 
~~~ obj = dict ( obj ) \n 
obj [ ] = name \n 
obj [ ] = time . time ( ) \n 
obj . setdefault ( , ) \n 
obj . setdefault ( , 0 ) \n 
return self . es . index ( index = self . index , doc_type = self . __type__ , body = obj , id = name , \n 
refresh = True ) \n 
~~ def update ( self , name , obj = { } , ** kwargs ) : \n 
obj . update ( kwargs ) \n 
return self . es . update ( index = self . index , doc_type = self . __type__ , \n 
body = { : obj } , id = name , refresh = True , ignore = 404 ) \n 
~~ def get_all ( self , fields = None ) : \n 
~~~ for record in elasticsearch . helpers . scan ( self . es , index = self . index , doc_type = self . __type__ , \n 
query = { : { "match_all" : { } } } , \n 
_source_include = fields or [ ] ) : \n 
~~~ yield record [ ] \n 
~~ ~~ def get ( self , name , fields = None ) : \n 
~~~ ret = self . es . get ( index = self . index , doc_type = self . __type__ , id = name , \n 
_source_include = fields or [ ] , ignore = 404 ) \n 
return ret . get ( , None ) \n 
~~ def check_update ( self , timestamp , fields = None ) : \n 
query = { : { "range" : { \n 
"updatetime" : { "gte" : timestamp } \n 
} } } , _source_include = fields or [ ] ) : \n 
~~ ~~ def drop ( self , name ) : \n 
~~~ return self . es . delete ( index = self . index , doc_type = self . __type__ , id = name , refresh = True ) \n 
~~ ~~ class ListO ( object ) : \n 
def __init__ ( self , buffer = None ) : \n 
~~~ self . _buffer = buffer \n 
if self . _buffer is None : \n 
~~~ self . _buffer = [ ] \n 
~~ ~~ def isatty ( self ) : \n 
~~ def seek ( self , n , mode = 0 ) : \n 
~~ def readline ( self ) : \n 
~~ def write ( self , x ) : \n 
~~~ self . _buffer . append ( x ) \n 
~~ def writelines ( self , x ) : \n 
~~~ self . _buffer . extend ( x ) \n 
~~ ~~ import time \n 
~~~ from UserDict import DictMixin \n 
~~~ from collections import Mapping as DictMixin \n 
~~ from . token_bucket import Bucket \n 
from six . moves import queue as Queue \n 
~~~ cmp \n 
~~~ cmp = lambda x , y : ( x > y ) - ( x < y ) \n 
~~ class InQueueTask ( DictMixin ) : \n 
__getitem__ = lambda * x : getattr ( * x ) \n 
__setitem__ = lambda * x : setattr ( * x ) \n 
__iter__ = lambda self : iter ( self . __slots__ ) \n 
__len__ = lambda self : len ( self . __slots__ ) \n 
keys = lambda self : self . __slots__ \n 
def __init__ ( self , taskid , priority = 0 , exetime = 0 ) : \n 
~~~ self . taskid = taskid \n 
self . priority = priority \n 
self . exetime = exetime \n 
~~ def __cmp__ ( self , other ) : \n 
~~~ if self . exetime == 0 and other . exetime == 0 : \n 
~~~ return - cmp ( self . priority , other . priority ) \n 
~~~ return cmp ( self . exetime , other . exetime ) \n 
~~ ~~ def __lt__ ( self , other ) : \n 
~~~ return self . __cmp__ ( other ) < 0 \n 
~~ ~~ class PriorityTaskQueue ( Queue . Queue ) : \n 
def _init ( self , maxsize ) : \n 
~~~ self . queue = [ ] \n 
self . queue_dict = dict ( ) \n 
~~ def _qsize ( self , len = len ) : \n 
~~~ return len ( self . queue_dict ) \n 
~~ def _put ( self , item , heappush = heapq . heappush ) : \n 
~~~ if item . taskid in self . queue_dict : \n 
~~~ task = self . queue_dict [ item . taskid ] \n 
changed = False \n 
if item . priority > task . priority : \n 
~~~ task . priority = item . priority \n 
changed = True \n 
~~ if item . exetime < task . exetime : \n 
~~~ task . exetime = item . exetime \n 
~~~ self . _resort ( ) \n 
~~~ heappush ( self . queue , item ) \n 
self . queue_dict [ item . taskid ] = item \n 
~~ ~~ def _get ( self , heappop = heapq . heappop ) : \n 
~~~ while self . queue : \n 
~~~ item = heappop ( self . queue ) \n 
if item . taskid is None : \n 
~~ self . queue_dict . pop ( item . taskid , None ) \n 
return item \n 
def top ( self ) : \n 
~~~ while self . queue and self . queue [ 0 ] . taskid is None : \n 
~~~ heapq . heappop ( self . queue ) \n 
~~ if self . queue : \n 
~~~ return self . queue [ 0 ] \n 
~~ def _resort ( self ) : \n 
~~~ heapq . heapify ( self . queue ) \n 
~~ def __contains__ ( self , taskid ) : \n 
~~~ return taskid in self . queue_dict \n 
~~ def __getitem__ ( self , taskid ) : \n 
~~~ return self . queue_dict [ taskid ] \n 
~~ def __setitem__ ( self , taskid , item ) : \n 
~~~ assert item . taskid == taskid \n 
self . put ( item ) \n 
~~ def __delitem__ ( self , taskid ) : \n 
~~~ self . queue_dict . pop ( taskid ) . taskid = None \n 
~~ ~~ class TaskQueue ( object ) : \n 
processing_timeout = 10 * 60 \n 
def __init__ ( self , rate = 0 , burst = 0 ) : \n 
~~~ self . mutex = threading . RLock ( ) \n 
self . priority_queue = PriorityTaskQueue ( ) \n 
self . time_queue = PriorityTaskQueue ( ) \n 
self . processing = PriorityTaskQueue ( ) \n 
self . bucket = Bucket ( rate = rate , burst = burst ) \n 
def rate ( self ) : \n 
~~~ return self . bucket . rate \n 
~~ @ rate . setter \n 
def rate ( self , value ) : \n 
~~~ self . bucket . rate = value \n 
def burst ( self ) : \n 
~~~ return self . burst . burst \n 
~~ @ burst . setter \n 
def burst ( self , value ) : \n 
~~~ self . bucket . burst = value \n 
~~ def check_update ( self ) : \n 
self . _check_time_queue ( ) \n 
self . _check_processing ( ) \n 
~~ def _check_time_queue ( self ) : \n 
~~~ now = time . time ( ) \n 
self . mutex . acquire ( ) \n 
while self . time_queue . qsize ( ) and self . time_queue . top and self . time_queue . top . exetime < now : ~~~ task = self . time_queue . get_nowait ( ) \n 
task . exetime = 0 \n 
self . priority_queue . put ( task ) \n 
~~ self . mutex . release ( ) \n 
~~ def _check_processing ( self ) : \n 
while self . processing . qsize ( ) and self . processing . top and self . processing . top . exetime < now : ~~~ task = self . processing . get_nowait ( ) \n 
if task . taskid is None : \n 
~~ task . exetime = 0 \n 
~~ def put ( self , taskid , priority = 0 , exetime = 0 ) : \n 
task = InQueueTask ( taskid , priority , exetime ) \n 
if taskid in self . priority_queue : \n 
~~~ self . priority_queue . put ( task ) \n 
~~ elif taskid in self . time_queue : \n 
~~~ self . time_queue . put ( task ) \n 
~~ elif taskid in self . processing and self . processing [ taskid ] . taskid : \n 
~~~ if exetime and exetime > now : \n 
~~ ~~ self . mutex . release ( ) \n 
~~ def get ( self ) : \n 
if self . bucket . get ( ) < 1 : \n 
~~~ task = self . priority_queue . get_nowait ( ) \n 
self . bucket . desc ( ) \n 
~~ except Queue . Empty : \n 
~~~ self . mutex . release ( ) \n 
~~ task . exetime = now + self . processing_timeout \n 
self . processing . put ( task ) \n 
self . mutex . release ( ) \n 
return task . taskid \n 
~~ def done ( self , taskid ) : \n 
if taskid in self . processing : \n 
~~~ self . mutex . acquire ( ) \n 
~~~ del self . processing [ taskid ] \n 
~~ def size ( self ) : \n 
~~~ return self . priority_queue . qsize ( ) + self . time_queue . qsize ( ) + self . processing . qsize ( ) \n 
~~~ return self . size ( ) \n 
~~~ if taskid in self . priority_queue or taskid in self . time_queue : \n 
~~ if taskid in self . processing and self . processing [ taskid ] . taskid : \n 
~~~ task_queue = TaskQueue ( ) \n 
task_queue . processing_timeout = 0.1 \n 
task_queue . put ( , 3 , time . time ( ) + 0.1 ) \n 
task_queue . put ( , 1 ) \n 
task_queue . put ( , 2 ) \n 
assert task_queue . get ( ) == \n 
time . sleep ( 0.1 ) \n 
task_queue . _check_time_queue ( ) \n 
task_queue . _check_processing ( ) \n 
assert len ( task_queue ) == 0 \n 
import unittest2 as unittest \n 
logging . config . fileConfig ( "pyspider/logging.conf" ) \n 
from pyspider . database . sqlite import resultdb \n 
from pyspider . result . result_worker import ResultWorker \n 
from pyspider . libs . multiprocessing_queue import Queue \n 
from pyspider . libs . utils import run_in_thread \n 
class TestProcessor ( unittest . TestCase ) : \n 
~~~ resultdb_path = \n 
def setUpClass ( self ) : \n 
~~~ shutil . rmtree ( , ignore_errors = True ) \n 
os . makedirs ( ) \n 
def get_resultdb ( ) : \n 
~~~ return resultdb . ResultDB ( self . resultdb_path ) \n 
~~ self . resultdb = get_resultdb ( ) \n 
self . inqueue = Queue ( 10 ) \n 
def run_result_worker ( ) : \n 
~~~ self . result_worker = ResultWorker ( get_resultdb ( ) , self . inqueue ) \n 
self . result_worker . run ( ) \n 
~~ self . process = run_in_thread ( run_result_worker ) \n 
time . sleep ( 1 ) \n 
def tearDownClass ( self ) : \n 
~~~ if self . process . is_alive ( ) : \n 
~~~ self . result_worker . quit ( ) \n 
self . process . join ( 2 ) \n 
~~ assert not self . process . is_alive ( ) \n 
shutil . rmtree ( , ignore_errors = True ) \n 
~~ def test_10_bad_result ( self ) : \n 
~~~ self . inqueue . put ( ( { : } , { } ) ) \n 
self . resultdb . _list_project ( ) \n 
self . assertEqual ( len ( self . resultdb . projects ) , 0 ) \n 
self . assertEqual ( self . resultdb . count ( ) , 0 ) \n 
~~ def test_20_insert_result ( self ) : \n 
~~~ data = { \n 
self . inqueue . put ( ( { \n 
} , data ) ) \n 
time . sleep ( 0.5 ) \n 
self . assertEqual ( len ( self . resultdb . projects ) , 1 ) \n 
self . assertEqual ( self . resultdb . count ( ) , 1 ) \n 
result = self . resultdb . get ( , ) \n 
self . assertEqual ( result [ ] , data ) \n 
~~ def test_30_overwrite ( self ) : \n 
~~~ self . inqueue . put ( ( { \n 
} , "abc" ) ) \n 
self . assertEqual ( result [ ] , "abc" ) \n 
~~ def test_40_insert_list ( self ) : \n 
} , [ , ] ) ) \n 
self . assertEqual ( result [ ] , [ , ] ) \n 
import tensorflow as tf \n 
from tensorflow . python . framework import ops \n 
from models . rbm_models import dbn \n 
from utils import datasets \n 
flags = tf . app . flags \n 
FLAGS = flags . FLAGS \n 
flags . DEFINE_string ( , , ) \n 
flags . DEFINE_boolean ( , True , ) \n 
flags . DEFINE_integer ( , 0 , ) \n 
flags . DEFINE_boolean ( , False , ) flags . DEFINE_float ( , 0.1 , ) \n 
flags . DEFINE_float ( , 0.01 , ) \n 
flags . DEFINE_float ( , 0.7 , ) \n 
flags . DEFINE_integer ( , 10 , ) \n 
flags . DEFINE_float ( , 1 , ) \n 
rbm_names = [ _ for _ in FLAGS . rbm_names . split ( ) if _ ] \n 
layers = [ int ( _ ) for _ in FLAGS . layers . split ( ) if _ ] \n 
rbm_learning_rate = [ float ( _ ) for _ in FLAGS . rbm_learning_rate . split ( ) if _ ] \n 
rbm_num_epochs = [ int ( _ ) for _ in FLAGS . rbm_num_epochs . split ( ) if _ ] \n 
rbm_batch_size = [ int ( _ ) for _ in FLAGS . rbm_batch_size . split ( ) if _ ] \n 
rbm_gibbs_k = [ float ( _ ) for _ in FLAGS . rbm_gibbs_k . split ( ) if _ ] \n 
: rbm_batch_size , : rbm_gibbs_k , : rbm_names } \n 
for p in dae_params : \n 
~~~ if len ( dae_params [ p ] ) != len ( layers ) : \n 
~~~ dae_params [ p ] = [ dae_params [ p ] [ 0 ] for _ in layers ] \n 
~~ ~~ assert FLAGS . dataset in [ , , ] \n 
assert FLAGS . act_func in [ , , ] \n 
assert len ( layers ) > 0 \n 
~~~ if FLAGS . dataset == : \n 
~~~ trX , trY , vlX , vlY , teX , teY = datasets . load_mnist_dataset ( mode = ) \n 
~~ elif FLAGS . dataset == : \n 
~~~ trX , trY , teX , teY = datasets . load_cifar10_dataset ( FLAGS . cifar_dir , mode = ) \n 
vlY = teY [ : 5000 ] \n 
~~~ def load_from_np ( dataset_path ) : \n 
~~~ if dataset_path != : \n 
~~~ return np . load ( dataset_path ) \n 
~~ ~~ trX , trY = load_from_np ( FLAGS . train_dataset ) , load_from_np ( FLAGS . train_labels ) \n 
vlX , vlY = load_from_np ( FLAGS . valid_dataset ) , load_from_np ( FLAGS . valid_labels ) \n 
teX , teY = load_from_np ( FLAGS . test_dataset ) , load_from_np ( FLAGS . test_labels ) \n 
~~~ trX , trY , vlX , vlY , teX , teY = None , None , None , None , None , None \n 
~~ srbm = dbn . DBN ( \n 
model_name = FLAGS . model_name , rbm_names = dae_params [ ] , do_pretrain = FLAGS . do_pretrain layers = dae_params [ ] , dataset = FLAGS . dataset , main_dir = FLAGS . main_dir , act_func = FLAGS . rbm_learning_rate = dae_params [ ] , rbm_gibbs_k = dae_params [ ] , \n 
verbose = FLAGS . verbose , rbm_num_epochs = dae_params [ ] , momentum = FLAGS . momentum , \n 
rbm_batch_size = dae_params [ ] , learning_rate = FLAGS . learning_rate , num_epochs = FLAGS batch_size = FLAGS . batch_size , opt = FLAGS . opt , loss_func = FLAGS . loss_func , dropout = FLAGS . dropout gauss_visible = FLAGS . gauss_visible , stddev = FLAGS . stddev ) \n 
if FLAGS . do_pretrain : \n 
~~~ srbm . pretrain ( trX , vlX ) \n 
~~ ops . reset_default_graph ( ) \n 
srbm . build_model ( trX . shape [ 1 ] , trY . shape [ 1 ] ) \n 
srbm . fit ( trX , trY , vlX , vlY ) \n 
print ( . format ( srbm . predict ( teX , teY ) ) ) \n 
~~ from construct import * \n 
from construct import adapters \n 
from tensor . protocol . sflow . protocol import utils \n 
class IPv4Header ( object ) : \n 
~~~ def __init__ ( self , data ) : \n 
~~~ ip = Struct ( "ip_header" , \n 
EmbeddedBitStruct ( \n 
Const ( Nibble ( "version" ) , 4 ) , \n 
Nibble ( "header_length" ) , \n 
BitStruct ( "tos" , \n 
Bits ( "precedence" , 3 ) , \n 
Flag ( "minimize_delay" ) , \n 
Flag ( "high_throuput" ) , \n 
Flag ( "high_reliability" ) , \n 
Flag ( "minimize_cost" ) , \n 
Padding ( 1 ) , \n 
UBInt16 ( "total_length" ) , \n 
UBInt16 ( "id" ) , \n 
UBInt16 ( "flags" ) , \n 
UBInt8 ( "ttl" ) , \n 
Enum ( UBInt8 ( "proto" ) , \n 
UDP = 0x11 , \n 
TCP = 0x06 , \n 
HOPOPT = 0x00 , \n 
ICMP = 0x01 , \n 
IGMP = 0x02 , \n 
GGP = 0x03 , \n 
IPoIP = 0x04 , \n 
ST = 0x05 , \n 
CBT = 0x07 , \n 
EGP = 0x08 , \n 
IGP = 0x09 , \n 
NVPII = 0x0B , \n 
PUP = 0x0C , \n 
ARGUS = 0x0D , \n 
EMCON = 0x0E , \n 
XNET = 0x0F , \n 
CHAOS = 0x10 , \n 
MUX = 0x12 , \n 
DCNMEAS = 0x13 , \n 
HMP = 0x14 , \n 
PRM = 0x15 , \n 
RDP = 0x1B , \n 
IRTP = 0x1C , \n 
ISOTP4 = 0x1D , \n 
DCCP = 0x21 , \n 
XTP = 0x24 , \n 
DDP = 0x25 , \n 
IL = 0x28 , \n 
IPv6 = 0x29 , \n 
SDRP = 0x2A , \n 
IPv6Route = 0x2B , \n 
IPv6Frag = 0x2C , \n 
IDRP = 0x2D , \n 
RSVP = 0x2E , \n 
GRE = 0x2F , \n 
MHRP = 0x30 , \n 
BNA = 0x31 , \n 
ESP = 0x32 , \n 
AH = 0x33 , \n 
SWIPE = 0x35 , \n 
MOBILE = 0x37 , \n 
TLSP = 0x38 , \n 
SKIP = 0x39 , \n 
IPv6ICMP = 0x3A , \n 
IPv6NoNxt = 0x3B , \n 
IPv6Opts = 0x3C , \n 
CFTP = 0x3E , \n 
SATEXPAK = 0x40 , \n 
KRYPTOLAN = 0x41 , \n 
RVD = 0x42 , \n 
IPPC = 0x43 , \n 
SATMON = 0x45 , \n 
VISA = 0x46 , \n 
IPCU = 0x47 , \n 
CPNX = 0x48 , \n 
CPHB = 0x49 , \n 
WSN = 0x4A , \n 
PVP = 0x4B , \n 
BRSATMON = 0x4C , \n 
SUNND = 0x4D , \n 
WBMON = 0x4E , \n 
WBEXPAK = 0x4F , \n 
ISOIP = 0x50 , \n 
VMTP = 0x51 , \n 
SECUREVMTP = 0x52 , \n 
VINES = 0x53 , \n 
TTP = 0x54 , \n 
IPTM = 0x54 , \n 
NSFNETIGP = 0x55 , \n 
DGP = 0x56 , \n 
TCF = 0x57 , \n 
EIGRP = 0x58 , \n 
OSPF = 0x59 , \n 
LARP = 0x5B , \n 
MTP = 0x5C , \n 
IPIP = 0x5E , \n 
MICP = 0x5F , \n 
SCCSP = 0x60 , \n 
ETHERIP = 0x61 , \n 
ENCAP = 0x62 , \n 
GMTP = 0x64 , \n 
IFMP = 0x65 , \n 
PNNI = 0x66 , \n 
PIM = 0x67 , \n 
ARIS = 0x68 , \n 
SCPS = 0x69 , \n 
QNX = 0x6A , \n 
IPComp = 0x6C , \n 
SNP = 0x6D , \n 
VRRP = 0x70 , \n 
PGM = 0x71 , \n 
L2TP = 0x73 , \n 
DDX = 0x74 , \n 
IATP = 0x75 , \n 
STP = 0x76 , \n 
SRP = 0x77 , \n 
UTI = 0x78 , \n 
SMP = 0x79 , \n 
SM = 0x7A , \n 
PTP = 0x7B , \n 
ISIS = 0x7C , \n 
FIRE = 0x7D , \n 
CRTP = 0x7E , \n 
CRUDP = 0x7F , \n 
SSCOPMCE = 0x80 , \n 
IPLT = 0x81 , \n 
SPS = 0x82 , \n 
SCTP = 0x84 , \n 
FC = 0x85 , \n 
UDPLite = 0x88 , \n 
MPLSoIP = 0x89 , \n 
manet = 0x8A , \n 
HIP = 0x8B , \n 
Shim6 = 0x8C , \n 
WESP = 0x8D , \n 
ROHC = 0x8E , \n 
UBInt16 ( "checksum" ) , \n 
UBInt32 ( "src" ) , \n 
UBInt32 ( "dst" ) , \n 
self . ip = ip . parse ( data [ : ip . sizeof ( ) ] ) \n 
self . ip . src = utils . IPv4Address ( self . ip . src ) \n 
self . ip . dst = utils . IPv4Address ( self . ip . dst ) \n 
data = data [ ip . sizeof ( ) : ] \n 
if self . ip . proto in ( , ) : \n 
~~~ self . proto = Struct ( "proto" , \n 
UBInt16 ( "sport" ) , \n 
UBInt16 ( "dport" ) , \n 
) . parse ( data ) \n 
self . ip_sport = self . proto . sport \n 
self . ip_dport = self . proto . dport \n 
~~ ~~ ~~ class ISO8023Header ( object ) : \n 
~~~ frame = Struct ( "Frame" , \n 
Bytes ( "destination" , 6 ) , \n 
Bytes ( "source" , 6 ) , \n 
Enum ( UBInt16 ( "type" ) , \n 
IPv4 = 0x0800 , \n 
ARP = 0x0806 , \n 
RARP = 0x8035 , \n 
X25 = 0x0805 , \n 
IPX = 0x8137 , \n 
IPv6 = 0x86DD , \n 
VLAN = 0x8100 \n 
~~~ ethernet = frame . parse ( data [ : 14 ] ) \n 
~~ except adapters . MappingError : \n 
self . frame = None \n 
print repr ( data ) \n 
~~ data = data [ 14 : ] \n 
self . src_mac = ethernet . destination \n 
self . dst_mac = ethernet . source \n 
if ethernet . type == : \n 
~~~ d = ord ( data [ 0 ] ) \n 
self . vlan = d & 0x0fff \n 
self . vlan_priority = d >> 13 \n 
~~ elif ethernet . type == : \n 
~~~ ipv4 = IPv4Header ( data ) \n 
self . ip = ipv4 . ip \n 
self . ip_sport = ipv4 . ip_sport \n 
self . ip_dport = ipv4 . ip_dport \n 
~~~ print ethernet . type , repr ( data ) \n 
~~ ~~ ~~ class IPv6Header ( object ) : \n 
~~~ def __init__ ( self , u ) : \n 
~~ ~~ class IEEE80211MACHeader ( object ) : \n 
~~ ~~ class PPPHeader ( object ) : \n 
~~ ~~ class HeaderSample ( object ) : \n 
~~~ self . protocol = u . unpack_uint ( ) \n 
self . frame_len = u . unpack_uint ( ) \n 
self . payload_removed = u . unpack_uint ( ) \n 
self . sample_header = u . unpack_string ( ) \n 
self . samplers = { \n 
1 : ISO8023Header , \n 
7 : PPPHeader , \n 
11 : IPv4Header , \n 
12 : IPv6Header \n 
if self . samplers . get ( self . protocol ) : \n 
~~~ self . frame = self . samplers [ self . protocol ] ( \n 
self . sample_header \n 
~~ ~~ ~~ class EthernetSample ( object ) : \n 
~~~ self . length = u . unpack_uint ( ) \n 
self . src_mac = u . unpack_fopaque ( 6 ) \n 
self . dst_mac = u . unpack_fopaque ( 6 ) \n 
self . type = u . unpack_uint ( ) \n 
~~ ~~ class IPV4Sample ( object ) : \n 
self . protocol = u . unpack_uint ( ) \n 
self . src_ip = u . unpack_fstring ( 4 ) \n 
self . dst_ip = u . unpack_fstring ( 4 ) \n 
self . src_port = u . unpack_uint ( ) \n 
self . dst_port = u . unpack_uint ( ) \n 
self . tcp_flags = u . unpack_uint ( ) \n 
self . tos = u . unpack_uint ( ) \n 
~~ ~~ class IPV6Sample ( object ) : \n 
self . src_ip = u . unpack_fstring ( 16 ) \n 
self . dst_ip = u . unpack_fstring ( 16 ) \n 
self . priority = u . unpack_uint ( ) \n 
~~ ~~ class SwitchSample ( object ) : \n 
~~~ self . src_vlan = u . unpack_uint ( ) \n 
self . src_priority = u . unpack_uint ( ) \n 
self . dst_vlan = u . unpack_uint ( ) \n 
self . dst_priority = u . unpack_uint ( ) \n 
~~ ~~ class RouterSample ( object ) : \n 
~~~ self . next_hop = utils . unpack_address ( u ) \n 
self . src_mask_len = u . unpack_uint ( ) \n 
self . dst_mask_len = u . unpack_uint ( ) \n 
~~ ~~ class GatewaySample ( object ) : \n 
self . asn = u . unpack_uint ( ) \n 
self . src_as = u . unpack_uint ( ) \n 
self . src_peer_as = u . unpack_uint ( ) \n 
self . as_path_type = u . unpack_uint ( ) \n 
self . as_path = u . unpack_array ( u . unpack_uint ) \n 
self . communities = u . unpack_array ( u . unpack_uint ) \n 
self . localpref = u . unpack_uint ( ) \n 
~~ ~~ class UserSample ( object ) : \n 
~~~ self . src_charset = u . unpack_uint ( ) \n 
self . src_user = u . unpack_string ( ) \n 
self . dst_charset = u . unpack_uint ( ) \n 
self . dst_user = u . unpack_string ( ) \n 
~~ ~~ class URLSample ( object ) : \n 
~~~ self . url_direction = u . unpack_uint ( ) \n 
self . url = u . unpack_string ( ) \n 
self . host = u . unpack_string ( ) \n 
~~ ~~ class MPLSSample ( object ) : \n 
self . in_stack = u . unpack_array ( u . unpack_uint ) \n 
self . out_stack = u . unpack_array ( u . unpack_uint ) \n 
~~ ~~ class NATSample ( object ) : \n 
~~~ self . src_address = utils . unpack_address ( u ) \n 
self . dst_address = utils . unpack_address ( u ) \n 
~~ ~~ class MPLSTunnelSample ( object ) : \n 
~~~ self . tunnel_lsp_name = u . unpack_string ( ) \n 
self . tunnel_id = u . unpack_uint ( ) \n 
self . tunnel_cos = u . unpack_uint ( ) \n 
~~ ~~ class MPLSVCSample ( object ) : \n 
~~~ self . vc_instance_name = u . unpack_string ( ) \n 
self . vc_id = u . unpack_uint ( ) \n 
self . vc_cos = u . unpack_uint ( ) \n 
~~ ~~ class MPLSFTNSample ( object ) : \n 
~~~ self . mplsFTNDescr = u . unpack_string ( ) \n 
self . mplsFTNMask = u . unpack_uint ( ) \n 
~~ ~~ class MPLSLDPFECSample ( object ) : \n 
~~~ self . mplsFecAddrPrefixLength = u . unpack_uint ( ) \n 
~~ ~~ class VLANTunnelSample ( object ) : \n 
~~~ self . stack = u . unpack_array ( u . unpack_uint ) \n 
~~ ~~ def getDecoder ( format ) : \n 
~~~ decoders = { \n 
1 : HeaderSample , \n 
2 : EthernetSample , \n 
3 : IPV4Sample , \n 
4 : IPV6Sample , \n 
1001 : SwitchSample , \n 
1002 : RouterSample , \n 
1003 : GatewaySample , \n 
1004 : UserSample , \n 
1005 : URLSample , \n 
1006 : MPLSSample , \n 
1007 : NATSample , \n 
1008 : MPLSTunnelSample , \n 
1009 : MPLSVCSample , \n 
1010 : MPLSFTNSample , \n 
1011 : MPLSLDPFECSample , \n 
1012 : VLANTunnelSample \n 
return decoders . get ( format , None ) \n 
~~ from twisted . trial import unittest \n 
from tensor . logs import follower , parsers \n 
class TestLogs ( unittest . TestCase ) : \n 
~~~ def test_logfollow ( self ) : \n 
~~~ os . unlink ( ) \n 
os . unlink ( ) \n 
~~ log = open ( , ) \n 
log . write ( ) \n 
log . flush ( ) \n 
f = follower . LogFollower ( , tmp_path = "." , history = True ) \n 
r = f . get ( ) \n 
r2 = f . get ( ) \n 
r3 = f . get ( ) \n 
self . assertEqual ( r [ 0 ] , ) \n 
self . assertEqual ( r [ 1 ] , ) \n 
self . assertEqual ( r2 , [ ] ) \n 
self . assertEqual ( r3 [ 0 ] , ) \n 
log . close ( ) \n 
log = open ( , ) \n 
~~ def test_apache_parser ( self ) : \n 
~~~ log = parsers . ApacheLogParser ( ) \n 
want = { \n 
: 200 , \n 
: 709 , \n 
: datetime . datetime ( 2015 , 1 , 16 , 11 , 11 , 45 ) , \n 
p = log . parse ( line ) \n 
for k , v in want . items ( ) : \n 
~~~ self . assertEquals ( p [ k ] , v ) \n 
~~ ~~ ~~ import numpy as np \n 
import dask . async \n 
from . import science_features as sf \n 
FEATURES_LIST = [ \n 
def generate_science_features ( t , m , e , features_to_compute = FEATURES_LIST ) : \n 
features_to_compute = [ f for f in features_to_compute \n 
if f in FEATURES_LIST ] \n 
feature_graph = { \n 
: ( sf . amplitude , m ) , \n 
: ( sf . flux_percentile_ratio , m , 20 ) , \n 
: ( sf . flux_percentile_ratio , m , 35 ) , \n 
: ( sf . flux_percentile_ratio , m , 50 ) , \n 
: ( sf . flux_percentile_ratio , m , 65 ) , \n 
: ( sf . flux_percentile_ratio , m , 80 ) , \n 
: ( sf . maximum , m ) , \n 
: ( sf . max_slope , t , m ) , \n 
: ( sf . median , m ) , \n 
: ( sf . median_absolute_deviation , m ) , \n 
: ( sf . minimum , m ) , \n 
: ( sf . percent_amplitude , m ) , \n 
: ( sf . percent_beyond_1_std , m , e ) , \n 
: ( sf . percent_close_to_median , m ) , \n 
sf . percent_difference_flux_percentile , m ) , \n 
: ( sf . skew , m ) , \n 
: ( sf . std , m ) , \n 
: ( sf . stetson_j , m ) , \n 
: ( sf . stetson_k , m ) , \n 
: ( sf . weighted_average , m , e ) , \n 
: ( sf . qso_fit , t , m , e ) , \n 
: ( sf . get_qso_log_chi2_qsonu , ) , \n 
: ( sf . get_qso_log_chi2nuNULL_chi2nu , \n 
: ( sf . lomb_scargle_model , t , m , e ) , \n 
: ( sf . get_lomb_frequency , , 1 ) , \n 
: ( sf . get_lomb_frequency , , 2 ) , \n 
: ( sf . get_lomb_frequency , , 3 ) , \n 
: ( sf . get_lomb_amplitude , , 1 , 1 ) , \n 
: ( sf . get_lomb_amplitude , , 1 , 2 ) , \n 
: ( sf . get_lomb_amplitude , , 1 , 3 ) , \n 
: ( sf . get_lomb_amplitude , , 1 , 4 ) , \n 
: ( sf . get_lomb_amplitude , , 2 , 1 ) , \n 
: ( sf . get_lomb_amplitude , , 2 , 2 ) , \n 
: ( sf . get_lomb_amplitude , , 2 , 3 ) , \n 
: ( sf . get_lomb_amplitude , , 2 , 4 ) , \n 
: ( sf . get_lomb_amplitude , , 3 , 1 ) , \n 
: ( sf . get_lomb_amplitude , , 3 , 2 ) , \n 
: ( sf . get_lomb_amplitude , , 3 , 3 ) , \n 
: ( sf . get_lomb_amplitude , , 3 , 4 ) , \n 
: ( sf . get_lomb_rel_phase , , 1 , 1 ) , \n 
: ( sf . get_lomb_rel_phase , , 1 , 2 ) , \n 
: ( sf . get_lomb_rel_phase , , 1 , 3 ) , \n 
: ( sf . get_lomb_rel_phase , , 1 , 4 ) , \n 
: ( sf . get_lomb_rel_phase , , 2 , 1 ) , \n 
: ( sf . get_lomb_rel_phase , , 2 , 2 ) , \n 
: ( sf . get_lomb_rel_phase , , 2 , 3 ) , \n 
: ( sf . get_lomb_rel_phase , , 2 , 4 ) , \n 
: ( sf . get_lomb_rel_phase , , 3 , 1 ) , \n 
: ( sf . get_lomb_rel_phase , , 3 , 2 ) , \n 
: ( sf . get_lomb_rel_phase , , 3 , 3 ) , \n 
: ( sf . get_lomb_rel_phase , , 3 , 4 ) , \n 
: ( sf . get_lomb_amplitude_ratio , , 2 ) , \n 
: ( sf . get_lomb_amplitude_ratio , , 3 ) , \n 
: ( sf . get_lomb_frequency_ratio , , 2 ) , \n 
: ( sf . get_lomb_frequency_ratio , , 3 ) , \n 
: ( sf . get_lomb_signif_ratio , , 2 ) , \n 
: ( sf . get_lomb_signif_ratio , , 3 ) , \n 
: ( sf . get_lomb_lambda , ) , \n 
: ( sf . get_lomb_signif , ) , \n 
: ( sf . get_lomb_varrat , ) , \n 
: ( sf . get_lomb_trend , ) , \n 
: ( sf . get_lomb_y_offset , ) , \n 
: ( sf . num_alias , ) , \n 
: ( sf . scatter_res_raw , t , m , e , ) , \n 
: ( sf . periodic_model , ) , \n 
: ( sf . get_max_delta_mags , ) , \n 
: ( sf . get_min_delta_mags , ) , \n 
: ( sf . get_model_phi1_phi2 , ) , \n 
: ( sf . period_folding , t , m , e , ) , \n 
: ( sf . get_fold2P_slope_percentile , \n 
, 10 ) , \n 
, 90 ) , \n 
: ( sf . get_medperc90_2p_p , ) , \n 
: ( sf . p2p_model , t , m , ) , \n 
: ( sf . get_p2p_scatter_2praw , ) , \n 
: ( sf . get_p2p_scatter_over_mad , ) , \n 
: ( sf . get_p2p_scatter_pfold_over_mad , \n 
: ( sf . get_p2p_ssqr_diff_over_var , ) , \n 
: ( sf . lomb_scargle_fast_period , t , m , e ) , \n 
values = dask . async . get_sync ( feature_graph , features_to_compute ) \n 
return dict ( zip ( features_to_compute , values ) ) \n 
from types import BuiltinFunctionType , FunctionType \n 
DEBUG = True \n 
class ApiDocWriter ( object ) : \n 
rst_section_levels = [ , , , , ] \n 
def __init__ ( self , \n 
package_name , \n 
rst_extension = , \n 
skip_patterns = [ , ] , \n 
self . package_name = package_name \n 
self . skip_patterns = skip_patterns \n 
self . rst_extension = rst_extension \n 
root_module = self . _import ( package_name ) \n 
self . root_path = root_module . __path__ [ - 1 ] \n 
~~ def _import ( self , name ) : \n 
mod = __import__ ( name ) \n 
components = name . split ( ) \n 
for comp in components [ 1 : ] : \n 
~~~ mod = getattr ( mod , comp ) \n 
~~ return mod \n 
~~ def _get_object_name ( self , line ) : \n 
name = line . split ( ) [ 1 ] . split ( ) [ 0 ] . strip ( ) \n 
return name . rstrip ( ) \n 
~~ def _parse_module_with_import ( self , uri ) : \n 
mod = __import__ ( uri , fromlist = [ uri . split ( ) [ - 1 ] ] ) \n 
obj_strs = [ obj for obj in dir ( mod ) if not obj . startswith ( ) ] \n 
functions = [ ] \n 
classes = [ ] \n 
for obj_str in obj_strs : \n 
~~~ if obj_str not in mod . __dict__ : \n 
~~ obj = mod . __dict__ [ obj_str ] \n 
if ( hasattr ( obj , ) \n 
and self . package_name not in obj . __module__ ) : \n 
~~ if isinstance ( obj , ( FunctionType , BuiltinFunctionType ) ) : \n 
~~~ functions . append ( obj_str ) \n 
~~~ issubclass ( obj , object ) \n 
classes . append ( obj_str ) \n 
~~ ~~ ~~ return functions , classes \n 
~~ def generate_api_doc ( self , uri ) : \n 
functions , classes = self . _parse_module_with_import ( uri ) \n 
if not len ( functions ) and not len ( classes ) and DEBUG : \n 
~~ uri_short = re . sub ( % self . package_name , , uri ) \n 
ad = \n 
if in uri : \n 
~~~ title = + uri_short + \n 
~~ ad += title + + self . rst_section_levels [ 1 ] * len ( title ) \n 
ad += + uri + \n 
ad += \n 
for f in functions : \n 
~~~ ad += + uri + + f + \n 
~~ ad += \n 
~~~ ad += + uri + + c + \n 
~~~ full_f = uri + + f \n 
ad += f + \n 
ad += self . rst_section_levels [ 2 ] * len ( f ) + \n 
ad += + full_f + \n 
~~ for c in classes : \n 
~~~ ad += + c + + self . rst_section_levels [ 2 ] * ( len ( c ) + 9 ) + \n 
ad += + c + \n 
~~ return ad \n 
~~ def _survives_exclude ( self , matchstr ) : \n 
for pat in self . skip_patterns : \n 
~~~ if re . compile ( pat ) . search ( matchstr ) : \n 
~~ def _uri2path ( self , uri ) : \n 
if uri == self . package_name : \n 
~~~ return os . path . join ( self . root_path , ) \n 
~~ path = uri . replace ( self . package_name + , ) \n 
path = path . replace ( , os . path . sep ) \n 
path = os . path . join ( self . root_path , path ) \n 
~~~ path += \n 
~~ elif os . path . exists ( os . path . join ( path , ) ) : \n 
~~~ path = os . path . join ( path , ) \n 
~~ return path \n 
~~ def _path2uri ( self , dirpath ) : \n 
package_dir = self . package_name . replace ( , os . path . sep ) \n 
relpath = dirpath . replace ( self . root_path , package_dir ) \n 
if relpath . startswith ( os . path . sep ) : \n 
~~~ relpath = relpath [ 1 : ] \n 
~~ return relpath . replace ( os . path . sep , ) \n 
~~ def discover_modules ( self ) : \n 
modules = [ self . package_name ] \n 
for dirpath , dirnames , filenames in os . walk ( self . root_path , \n 
topdown = False ) : \n 
~~~ root_uri = self . _path2uri ( os . path . join ( self . root_path , \n 
dirpath ) ) \n 
filenames = [ f for f in filenames if f . endswith ( ) ] \n 
uris = [ u . strip ( ) for u in dirnames + filenames ] \n 
for uri in uris : \n 
~~~ package_uri = . join ( ( root_uri , uri ) ) \n 
if ( self . _uri2path ( package_uri ) and \n 
self . _survives_exclude ( package_uri ) ) : \n 
~~~ mod = __import__ ( package_uri , fromlist = [ ] ) \n 
mod . __all__ \n 
modules . append ( package_uri ) \n 
~~ except ( ImportError , AttributeError ) : \n 
~~ ~~ ~~ ~~ return sorted ( modules ) \n 
~~ def write_modules_api ( self , modules , outdir ) : \n 
~~~ written_modules = [ ] \n 
for m in modules : \n 
~~~ api_str = self . generate_api_doc ( m ) \n 
if not api_str : \n 
~~ outfile = os . path . join ( outdir , \n 
m + self . rst_extension ) \n 
fileobj = open ( outfile , ) \n 
fileobj . write ( api_str ) \n 
fileobj . close ( ) \n 
written_modules . append ( m ) \n 
~~ self . written_modules = written_modules \n 
~~ def write_api_docs ( self , outdir ) : \n 
if not os . path . exists ( outdir ) : \n 
~~~ os . mkdir ( outdir ) \n 
~~ modules = self . discover_modules ( ) \n 
self . write_modules_api ( modules , outdir ) \n 
~~ def write_index ( self , outdir , froot = , relative_to = None ) : \n 
if self . written_modules is None : \n 
~~ path = os . path . join ( outdir , froot + self . rst_extension ) \n 
if relative_to is not None : \n 
~~~ relpath = ( outdir + os . path . sep ) . replace ( relative_to + os . path . sep , ) \n 
~~~ relpath = outdir \n 
idx = open ( path , ) \n 
w = idx . write \n 
w ( ) \n 
w ( title + "\\n" ) \n 
w ( "=" * len ( title ) + "\\n\\n" ) \n 
for f in self . written_modules : \n 
~~~ w ( % os . path . join ( relpath , f ) ) \n 
~~ idx . close ( ) \n 
~~~ from os import getcwd \n 
from os import sys \n 
sys . path . append ( getcwd ( ) ) \n 
~~ from MOAL . helpers . display import Section \n 
from random import choice \n 
from numpy import array , dot , random \n 
DEBUG = True if __name__ == else False \n 
unit_step = lambda x : 0 if x < 0 else 1 \n 
training_data_or = [ \n 
( array ( [ 0 , 0 , 1 ] ) , 0 ) , \n 
( array ( [ 0 , 1 , 1 ] ) , 1 ) , \n 
( array ( [ 1 , 0 , 1 ] ) , 1 ) , \n 
( array ( [ 1 , 1 , 1 ] ) , 1 ) , \n 
training_data_and = [ \n 
( array ( [ 0 , 1 , 1 ] ) , 0 ) , \n 
( array ( [ 1 , 0 , 1 ] ) , 0 ) , \n 
training_data_not = [ \n 
( array ( [ 0 ] ) , 1 ) , \n 
( array ( [ 1 ] ) , 0 ) , \n 
def run_perceptron ( training_data ) : \n 
~~~ errors = [ ] \n 
bias = 0.2 \n 
steps = 100 \n 
vector_length = len ( training_data [ 0 ] [ 0 ] ) \n 
rand_vec3 = random . rand ( vector_length ) \n 
print ( . format ( rand_vec3 ) ) \n 
for _ in xrange ( steps ) : \n 
~~~ vec3 , expected = choice ( training_data ) \n 
result = dot ( rand_vec3 , vec3 ) \n 
offset = expected - unit_step ( result ) \n 
errors . append ( offset ) \n 
rand_vec3 += bias * offset * vec3 \n 
~~ for vec3 , expected in training_data : \n 
~~~ result = dot ( vec3 , rand_vec3 ) \n 
vec3 [ : 2 ] , result , unit_step ( result ) , expected ) ) \n 
~~ ~~ if DEBUG : \n 
~~~ with Section ( ) : \n 
~~~ run_perceptron ( training_data_or ) \n 
run_perceptron ( training_data_and ) \n 
run_perceptron ( training_data_not ) \n 
from Queue import Queue \n 
~~ from MOAL . helpers . text import gibberish \n 
from MOAL . helpers . display import Section \n 
COUNT_LOCK = threading . Lock ( ) \n 
total = 1 \n 
class CountThread ( threading . Thread ) : \n 
~~~ def start ( self , name ) : \n 
super ( CountThread , self ) . start ( ) \n 
~~~ global total \n 
~~~ COUNT_LOCK . acquire ( ) \n 
total += 1 \n 
COUNT_LOCK . release ( ) \n 
~~ if DEBUG : \n 
~~~ print ( . format ( self . name , total ) ) \n 
~~ ~~ ~~ class Worker : \n 
~~~ def __init__ ( self , queue , num_threads = 2 ) : \n 
~~~ self . NUM_THREADS = num_threads \n 
self . queue = queue \n 
~~ def do_work ( self , item ) : \n 
~~~ if DEBUG : \n 
~~~ print ( . format ( item ) ) \n 
~~ ~~ def worker ( self ) : \n 
~~~ item = self . queue . get ( ) \n 
self . do_work ( item ) \n 
self . queue . task_done ( ) \n 
~~ ~~ def process_all ( self ) : \n 
~~~ print ( ) \n 
~~ for _ in range ( self . NUM_THREADS ) : \n 
~~~ t = threading . Thread ( target = self . worker ) \n 
t . daemon = True \n 
t . start ( ) \n 
~~ ~~ ~~ class Producer : \n 
~~~ self . items = [ ] \n 
~~~ for record in self . items : \n 
~~~ yield record \n 
~~ raise StopIteration \n 
~~ def add ( self ) : \n 
~~~ self . items . append ( self . make ( ) ) \n 
~~ def make ( self ) : \n 
~~~ return gibberish ( ) \n 
~~~ counter_one = CountThread ( ) \n 
counter_two = CountThread ( ) \n 
counter_three = CountThread ( ) \n 
counter_one . start ( ) \n 
counter_two . start ( ) \n 
counter_three . start ( ) \n 
~~ with Section ( ) : \n 
~~~ work_queue = Queue ( ) \n 
bot = Worker ( work_queue ) \n 
producer = Producer ( ) \n 
for _ in range ( 10 ) : \n 
~~~ producer . add ( ) \n 
~~ for record in producer : \n 
~~~ print ( . format ( record ) ) \n 
work_queue . put ( record ) \n 
~~ bot . process_all ( ) \n 
work_queue . join ( ) \n 
from MOAL . helpers . display import print_h3 \n 
from MOAL . computer_organization . bit_field import BitField \n 
class ChessPieceBitBoard ( BitField ) : \n 
~~~ self . cols = list ( ) \n 
self . rows = range ( 1 , 8 ) \n 
super ( ChessPieceBitBoard , self ) . __init__ ( ) \n 
self . _populate_fields ( ) \n 
~~~ print ( . format ( self . cols , self . rows ) ) \n 
~~ def _populate_fields ( self ) : \n 
for col in self . cols : \n 
~~~ for row in self . rows : \n 
~~~ self [ . format ( col , row ) ] = False \n 
~~ ~~ ~~ ~~ class ChessBoard : \n 
~~~ self . winner = None \n 
self . white = { : 0 , : 0 } \n 
self . black = { : 0 , : 0 } \n 
self . boards = { } \n 
pieces = ( \n 
( 1 , ) , \n 
( 2 , ) , \n 
( 8 , ) , \n 
for num_pieces , piece_name in pieces : \n 
~~~ self [ piece_name ] = num_pieces \n 
~~ ~~ def __str__ ( self ) : \n 
~~~ for name , board in self . boards . iteritems ( ) : \n 
~~~ print ( . format ( name , ) ) \n 
~~ def __setitem__ ( self , piece_name , num_pieces ) : \n 
~~~ if num_pieces > 1 : \n 
~~~ for index in range ( num_pieces ) : \n 
~~~ self . boards [ . format ( \n 
piece_name , index ) ] = ChessPieceBitBoard ( ) \n 
~~~ self . boards [ piece_name ] = ChessPieceBitBoard ( ) \n 
~~ ~~ def __getitem__ ( self , piece ) : \n 
~~~ return self . boards [ piece ] \n 
~~ def move ( self , piece , player , old_position , new_position ) : \n 
~~~ print ( . format ( \n 
player , piece , old_position , new_position ) ) \n 
self . boards [ piece ] [ old_position ] = False \n 
self . boards [ piece ] [ new_position ] = True \n 
~~~ print_h3 ( ) \n 
chess_board = ChessBoard ( ) \n 
moves = ( \n 
( , , , ) , \n 
for info in moves : \n 
~~~ piece , player , old , new = info \n 
chess_board . move ( piece , player , old , new ) \n 
~~ print ( chess_board [ ] ) \n 
print ( chess_board ) \n 
from MOAL . helpers . display import print_success \n 
from MOAL . helpers . display import prnt \n 
from MOAL . helpers . display import divider \n 
class OpCode : \n 
~~~ def __init__ ( self , code , name , description , assembly ) : \n 
self . code = code \n 
self . assembly = assembly \n 
~~ def call ( self ) : \n 
for line in self . assembly : \n 
~~~ print_success ( line , prefix = ) \n 
~~ divider ( ) \n 
~~ def read_external ( self , code = [ ] ) : \n 
program = [ \n 
OpCode ( \n 
, [ ] ) , \n 
[ \n 
for opcode in program : \n 
~~~ opcode . call ( ) \n 
from MOAL . helpers . display import print_h2 \n 
from MOAL . helpers . datamaker import random_person \n 
import pymonad as pm \n 
from pprint import pprint as ppr \n 
@ pm . curry \n 
def add2 ( x , y ) : \n 
~~~ return x + y \n 
~~ @ pm . curry \n 
def add_ten_to ( x , y ) : \n 
~~~ return x + y + 10 \n 
def add3 ( x , y , z ) : \n 
~~~ return x + y + z \n 
def add ( x , y ) : \n 
def sub2 ( x ) : \n 
~~~ return x - 2 \n 
def neg ( x ) : \n 
~~~ return - x \n 
~~ def foofix ( word ) : \n 
~~~ return + word \n 
~~ def negafy ( * vals ) : \n 
return neg * pm . List ( * vals ) \n 
~~ def num_and_ranges ( x ) : \n 
return pm . List ( * ( x , range ( - x , x + 1 ) ) ) \n 
~~ def monadify ( * funcs ) : \n 
~~~ _monad = pm . List ( ) \n 
for func in funcs : \n 
~~~ _monad >> func \n 
~~ return _monad \n 
~~ def sqr ( x ) : \n 
~~~ return pm . List ( x , x ** 2 ) \n 
def exp ( x , pow ) : \n 
~~~ return pm . List ( x , x ** pow ) \n 
~~ def monoid_range ( max ) : \n 
return pm . List ( * range ( max ) ) \n 
def s_add ( x , y ) : \n 
~~~ return pm . State ( lambda old : ( x + y , old + 1 ) ) \n 
def s_sub ( x , y ) : \n 
def s_exp ( x , y ) : \n 
~~~ return pm . State ( lambda old : ( x ** y , old + 1 ) ) \n 
~~ class Functor ( pm . Functor ) : \n 
~~~ def fmap ( self , * args ) : \n 
~~ ~~ class Applicative ( pm . Applicative ) : \n 
~~ def amap ( self , * args ) : \n 
~~ ~~ class Monad ( pm . Monad ) : \n 
~~~ def bind ( self , * args ) : \n 
~~ def fmap ( self , * args ) : \n 
~~~ print_h2 ( ) \n 
assert add2 ( 2 , 3 ) == add2 ( 2 ) ( 3 ) \n 
assert add3 ( 2 , 3 , 4 ) == add3 ( 2 , 3 ) ( 4 ) \n 
add_to_5 = add3 ( 2 , 3 ) \n 
assert add3 ( 1 ) ( 2 ) ( 3 ) == add3 ( 1 , 2 , 3 ) \n 
print_h2 ( ) \n 
composed = add * sub2 \n 
assert composed ( 1 , 1 ) == 0 \n 
comp_partial = add2 ( 2 ) * add3 ( 2 , 3 ) \n 
print ( comp_partial ( 2 ) ) \n 
print ( neg * pm . List ( * range ( 10 ) ) ) \n 
print ( neg * pm . Just ( 0 ) ) \n 
nums = sub2 * pm . List ( * range ( 10 ) ) \n 
print ( nums ) \n 
nums2 = comp_partial * pm . List ( * range ( 4 ) ) \n 
print ( nums2 ) \n 
ppr ( [ sub2 * pm . List ( \n 
* range ( n ) ) for n in comp_partial * pm . List ( * nums2 ) ] ) \n 
bound = add2 * pm . List ( * range ( 3 ) ) & pm . List ( * range ( 3 ) ) \n 
ppr ( [ bound , neg * bound ] ) \n 
f = map ( lambda x : pm . List ( range ( x ) ) , range ( 10 ) ) \n 
ppr ( f ) \n 
f = add * monoid_range ( 4 ) & monoid_range ( 2 ) \n 
print ( f ) \n 
print ( neg * f ) \n 
bound1 = add2 * pm . List ( * range ( 2 ) ) & pm . List ( * range ( 2 ) ) \n 
bound2 = add_ten_to * pm . List ( * range ( 2 ) ) & bound1 \n 
ppr ( bound2 ) \n 
ppr ( num_and_ranges ( 10 ) ) \n 
assert negafy ( * range ( 10 ) ) == neg * monoid_range ( 10 ) \n 
names = pm . List ( * [ random_person ( ) [ ] for _ in range ( 4 ) ] ) \n 
f1 = foofix * names \n 
f2 = foofix * f1 \n 
f3 = foofix * f2 \n 
f4 = foofix * f3 \n 
print ( f1 ) \n 
print ( f2 ) \n 
print ( f3 ) \n 
print ( f4 ) \n 
assert f4 [ 0 ] . startswith ( ) \n 
f = monoid_range ( 4 ) >> monoid_range ( 4 ) \n 
ppr ( neg * f ) \n 
print ( pm . Just ( 9 ) >> pm . Just ( 10 ) ) \n 
print ( monoid_range ( 3 ) ) \n 
f = monoid_range ( 5 ) >> sqr >> exp ( 2 ) \n 
state1 = pm . unit ( pm . State , 1 ) >> s_add ( 3 ) >> s_exp ( 10 ) >> s_sub ( 3 ) \n 
state2 = pm . unit ( pm . State , 1 ) >> s_add ( 10 ) >> s_exp ( 2 ) >> s_sub ( 100 ) \n 
print ( state1 ( 4 ) ) \n 
print ( state2 ( 4 ) ) \n 
class AbstractVehicle : \n 
~~~ return . format ( self . name ) \n 
~~ ~~ class Automobile ( AbstractVehicle ) : \n 
~~ class Plane ( AbstractVehicle ) : \n 
~~ class Train ( AbstractVehicle ) : \n 
~~ class ClassFactory : \n 
~~~ @ staticmethod \n 
def make ( name , instantiate = False ) : \n 
~~~ newcls = None \n 
if name is or name is : \n 
~~~ newcls = Automobile \n 
~~ elif name is : \n 
~~~ newcls = Plane \n 
~~~ newcls = Train \n 
~~ if newcls is None : \n 
~~ if instantiate : \n 
~~~ return newcls ( name ) \n 
~~~ return newcls \n 
~~ ~~ ~~ if DEBUG : \n 
~~~ clstypes = [ ( , Automobile ) , ( , Plane ) , ( , Train ) ] \n 
for clstype in clstypes : \n 
~~~ _cls , real_class = clstype \n 
output = ClassFactory . make ( _cls ) \n 
assert not isinstance ( output , real_class ) \n 
_cls , real_class = clstype \n 
output = ClassFactory . make ( _cls , instantiate = True ) \n 
assert isinstance ( output , real_class ) \n 
print ( . format ( _cls , real_class ) ) \n 
print ( _cls ) \n 
from MOAL . helpers . trials import run_trials \n 
from MOAL . helpers . trials import test_speed \n 
from faker import Factory \n 
from redis import Redis \n 
faker = Factory . create ( ) \n 
redis_conn = Redis ( host = , port = 6379 , db = 0 ) \n 
pipe = redis_conn . pipeline ( ) \n 
testing = { } \n 
def insert_name ( * args , ** kwargs ) : \n 
~~~ key = faker . name ( ) \n 
val = faker . email ( ) \n 
testing [ key ] = val \n 
pipe . set ( key , val ) \n 
pipe . get ( key ) \n 
~~ @ test_speed \n 
def insert_all ( max_records ) : \n 
~~~ for n in range ( max_records ) : \n 
~~~ insert_name ( ) \n 
~~~ run_trials ( insert_all , trials = 10 ) \n 
print_success ( ) \n 
res = pipe . execute ( ) \n 
prnt ( , res ) \n 
from kafka import SimpleProducer , KafkaClient , KafkaConsumer \n 
KAFKA_URL = \n 
kafka = KafkaClient ( KAFKA_URL ) \n 
producer = SimpleProducer ( kafka ) \n 
consumer = KafkaConsumer ( , \n 
group_id = , \n 
bootstrap_servers = [ ] ) \n 
if DEBUG : \n 
~~~ for topic in topics : \n 
~~~ for i in range ( 10 ) : \n 
~~~ producer . send_messages ( topic , . format ( random_person ( ) ) ) \n 
~~ ~~ for message in consumer : \n 
message . topic , message . partition , \n 
message . offset , message . key , \n 
message . value ) ) \n 
from numpy import zeros , float32 \n 
from windml . datasets . nrel import NREL \n 
from windml . mapping . power_mapping import PowerMapping \n 
from sklearn . grid_search import GridSearchCV \n 
from sklearn . svm import SVR \n 
from sklearn . metrics import mean_squared_error \n 
park_id = NREL . park_id [ ] \n 
windpark = NREL ( ) . get_windpark ( park_id , 3 , 2004 , 2005 ) \n 
target = windpark . get_target ( ) \n 
feature_window , horizon = 3 , 3 \n 
mapping = PowerMapping ( ) \n 
X = mapping . get_features_park ( windpark , feature_window , horizon ) \n 
y = mapping . get_labels_turbine ( target , feature_window , horizon ) \n 
train_to , test_to = int ( math . floor ( len ( X ) * 0.5 ) ) , len ( X ) \n 
train_step , test_step = 5 , 5 \n 
X_train = X [ : train_to : train_step ] \n 
y_train = y [ : train_to : train_step ] \n 
X_test = X [ train_to : test_to : test_step ] \n 
y_test = y [ train_to : test_to : test_step ] \n 
svr = SVR ( kernel = , epsilon = 0.1 , C = 100.0 , gamma = 0.0001 ) \n 
svr . fit ( X_train , y_train ) \n 
y_hat = svr . predict ( X_test ) \n 
naive_hat = zeros ( len ( y_hat ) , dtype = float32 ) \n 
for i in range ( 0 , len ( y_hat ) ) : \n 
~~~ naive_hat [ i ] = y [ train_to + ( i * test_step ) - horizon ] \n 
~~ mse_y_hat = mean_squared_error ( y_test , y_hat ) \n 
mse_naive_hat = mean_squared_error ( y_test , naive_hat ) \n 
with plt . style . context ( "fivethirtyeight" ) : \n 
~~~ figure = plt . figure ( figsize = ( 8 , 5 ) ) \n 
time = range ( 0 , len ( y_hat ) ) \n 
plt . plot ( time , y_test , label = "Measurement" ) \n 
plt . xlim ( [ 6600 , 7000 ] ) \n 
plt . ylim ( [ - 5 , 50 ] ) \n 
plt . legend ( ) \n 
~~~ from setuptools import setup , find_packages \n 
~~~ import ez_setup \n 
ez_setup . use_setuptools ( ) \n 
~~ from pip . req import parse_requirements \n 
import windml \n 
def extract_package_name ( requirement ) : \n 
~~~ return str ( requirement . req ) . replace ( , ) . split ( ) [ 0 ] \n 
~~ def find_requirements ( req_file = ) : \n 
~~~ return [ extract_package_name ( r ) for r in parse_requirements ( req_file ) ] \n 
~~ DESCRIPTION = \n 
author = windml . __author__ , \n 
author_email = windml . __author_email__ , \n 
data_files = [ ] , \n 
description = DESCRIPTION , \n 
ext_modules = [ ] , \n 
install_requires = find_requirements ( ) , \n 
license = windml . __license__ , \n 
long_description = DESCRIPTION , \n 
packages = find_packages ( ) , \n 
package_data = { } , \n 
setup_requires = find_requirements ( ) , \n 
url = windml . __url__ , \n 
use_2to3 = ( sys . version_info >= ( 3 , ) ) , \n 
version = windml . __version__ , \n 
from windml . preprocessing . missing_data_finder import MissingDataFinder \n 
from numpy import zeros , int32 , float32 , nan \n 
class OverrideMissing ( object ) : \n 
~~~ def override ( self , timeseries , timestep , override_val ) : \n 
~~~ val = override_val \n 
new_amount = timeseries . shape [ 0 ] \n 
misses = MissingDataFinder ( ) . find ( timeseries , timestep ) \n 
starts = { } \n 
for start , end , amount in misses : \n 
~~~ new_amount += amount \n 
starts [ start ] = [ end , amount ] \n 
~~ filled = zeros ( ( new_amount , ) , dtype = [ ( , int32 ) , ( , float32 ) , ( , float32 ) ] ) \n 
keys = starts . keys ( ) \n 
current_index = 0 \n 
for i in range ( len ( timeseries ) ) : \n 
~~~ if ( i in keys ) : \n 
~~~ cs = \n 
d = \n 
filled [ current_index ] = timeseries [ i ] \n 
current_index += 1 \n 
end , n = starts [ i ] \n 
for j in range ( 1 , n + 1 ) : \n 
~~~ new_timestep = timeseries [ i ] [ d ] + j * timestep \n 
filled [ current_index ] = ( new_timestep , val , val ) \n 
~~~ filled [ current_index ] = timeseries [ i ] \n 
~~ ~~ return filled \n 
#csv.field_size_limit(10000000) \n 
def _cli ( ) : \n 
description = globals ( ) [ ] , epilog = epilog , \n 
formatter_class = argparse . RawDescriptionHelpFormatter ) \n 
, nargs = , type = argparse . FileType ( ) , default = sys . stdin , \n 
, , default = sys . stdout , type = argparse . FileType ( ) , \n 
"-d" , "--delimiter" , \n 
spec = parser . add_mutually_exclusive_group ( required = True ) \n 
spec . add_argument ( \n 
"-c" , "--contains" , \n 
"-e" , "--equals" , \n 
"-r" , "--regex" , \n 
"-v" , "--invert" , action = , default = False , \n 
"criteria." ) \n 
"-i" , "--ignorecase" , action = , default = False , \n 
if args . delimiter in [ , , , ] : \n 
~~~ args . delimiter = \n 
~~ for mode in [ , , ] : \n 
~~~ if args . __dict__ [ mode ] : \n 
~~~ match_str = args . __dict__ [ mode ] \n 
~~ ~~ filter_file ( \n 
args . infile , args . outfile , args . name , mode , match_str , args . delimiter , \n 
args . invert , args . ignorecase ) \n 
~~ def filter_file ( infile , outfile , name , mode , match_str , delimiter , invert , \n 
ignorecase ) : \n 
reader = csv . DictReader ( infile , delimiter = delimiter ) \n 
writer = csv . DictWriter ( \n 
outfile , delimiter = delimiter , fieldnames = reader . fieldnames ) \n 
writer . writeheader ( ) \n 
mode_fun = { \n 
: _check_contains , : _check_equals , \n 
: _check_regex } \n 
~~~ if mode_fun [ mode ] ( row [ name ] , match_str , ignorecase ) != invert : \n 
~~~ writer . writerow ( row ) \n 
~~ ~~ ~~ def _check_contains ( item , match_str , ignorecase ) : \n 
~~~ if ignorecase : \n 
~~~ return match_str . lower ( ) in item . lower ( ) \n 
~~~ return match_str in item \n 
~~ ~~ def _check_equals ( item , match_str , ignorecase ) : \n 
~~~ return match_str . lower ( ) == item . lower ( ) \n 
~~~ return match_str == item \n 
~~ ~~ def _check_regex ( item , match_str , ignorecase ) : \n 
~~~ flags = re . IGNORECASE if ignorecase else 0 \n 
return bool ( re . search ( match_str , item , flags = flags ) ) \n 
~~~ _cli ( ) \n 
from collections import Counter , defaultdict \n 
from . . common import smart_open , DocIDError \n 
from . . common_abc import SaveLoad \n 
from . import nlp \n 
from . import streaming_filters \n 
class BaseTokenizer ( SaveLoad ) : \n 
def text_to_counter ( self , text ) : \n 
return Counter ( self . text_to_token_list ( text ) ) \n 
~~ ~~ class MakeTokenizer ( BaseTokenizer ) : \n 
def __init__ ( self , tokenizer_func ) : \n 
self . text_to_token_list = tokenizer_func \n 
~~ ~~ class TokenizerBasic ( BaseTokenizer ) : \n 
def text_to_token_list ( self , text ) : \n 
tokens = nlp . word_tokenize ( text , L = 2 , numeric = False ) \n 
return [ word . lower ( ) for word in tokens if not nlp . is_stopword ( word ) ] \n 
~~ ~~ class TokenizerPOSFilter ( BaseTokenizer ) : \n 
def __init__ ( \n 
self , pos_types = [ ] , sent_tokenizer = nltk . sent_tokenize , \n 
word_tokenizer = TokenizerBasic ( ) , word_tokenizer_func = None , \n 
pos_tagger = nltk . pos_tag ) : \n 
self . pos_types = set ( pos_types ) \n 
self . sent_tokenizer = sent_tokenizer \n 
self . pos_tagger = pos_tagger \n 
if not word_tokenizer : \n 
~~~ self . word_tokenizer = MakeTokenizer ( word_tokenizer_func ) \n 
~~~ self . word_tokenizer = word_tokenizer \n 
~~ ~~ def text_to_token_list ( self , text ) : \n 
sentences = self . sent_tokenizer ( text ) \n 
func = self . word_tokenizer . text_to_token_list \n 
tokenized_sentences = [ func ( sent ) for sent in sentences ] \n 
tagged_sentences = [ \n 
self . pos_tagger ( sent ) for sent in tokenized_sentences ] \n 
token_list = sum ( \n 
[ self . _sent_filter ( sent ) for sent in tagged_sentences ] , [ ] ) \n 
return token_list \n 
~~ def _sent_filter ( self , tokenized_sent ) : \n 
~~~ return [ \n 
word for ( word , pos ) in tokenized_sent if pos in self . pos_types ] \n 
~~ ~~ class SparseFormatter ( object ) : \n 
def _parse_feature_str ( self , feature_str ) : \n 
assert feature_str [ 0 ] == \n 
feature_str = feature_str [ 1 : ] \n 
fv_list = re . findall ( , feature_str ) \n 
feature_values = { \n 
f : self . _string_to_number ( v , empty_sub = 1 ) for ( f , v ) in fv_list } \n 
return feature_values \n 
~~ def sstr_to_dict ( self , sstr ) : \n 
sstr = sstr . rstrip ( ) . rstrip ( ) \n 
idx = sstr . index ( self . preamble_char ) \n 
preamble , feature_str = sstr [ : idx ] , sstr [ idx + 1 : ] \n 
record_dict = self . _parse_preamble ( preamble ) \n 
record_dict [ ] = self . _parse_feature_str ( feature_str ) \n 
return record_dict \n 
~~ def sstr_to_info ( self , sstr ) : \n 
info = self . sstr_to_dict ( sstr ) \n 
info [ ] = self . _dict_to_tokens ( info ) \n 
return info \n 
~~ def _dict_to_tokens ( self , record_dict ) : \n 
~~~ token_list = [ ] \n 
if in record_dict : \n 
~~~ for feature , value in record_dict [ ] . iteritems ( ) : \n 
~~~ int_value = int ( value ) \n 
assert int_value == value \n 
token_list += [ feature ] * int_value \n 
~~ ~~ return token_list \n 
~~ def sstr_to_token_list ( self , sstr ) : \n 
record_dict = self . sstr_to_dict ( sstr ) \n 
return self . _dict_to_tokens ( record_dict ) \n 
~~ def sfile_to_token_iter ( self , filepath_or_buffer , limit = None ) : \n 
with smart_open ( filepath_or_buffer ) as open_file : \n 
~~~ for index , line in enumerate ( open_file ) : \n 
~~~ if index == limit : \n 
~~~ raise StopIteration \n 
~~ yield self . sstr_to_token_list ( line ) \n 
~~ ~~ ~~ def _string_to_number ( self , string , empty_sub = None ) : \n 
~~~ return int ( string ) \n 
~~~ return float ( string ) \n 
~~~ if ( string == ) and ( empty_sub is not None ) : \n 
~~~ return empty_sub \n 
~~ ~~ ~~ ~~ class VWFormatter ( SparseFormatter ) : \n 
~~~ self . format_name = \n 
self . preamble_char = \n 
~~ def get_sstr ( \n 
self , feature_values = None , target = None , importance = None , doc_id = None ) : \n 
if doc_id : \n 
~~~ if re . search ( r"[|\\s:\']" , doc_id ) : \n 
~~~ msg = ( \n 
raise DocIDError ( msg ) \n 
~~ assert importance is not None \n 
~~ if importance : \n 
~~ if target : \n 
~~~ formatted = str ( target ) + formatted \n 
~~ formatted += \n 
for word , count in feature_values . iteritems ( ) : \n 
~~ if len ( feature_values ) > 0 : \n 
~~~ formatted = formatted . rstrip ( ) \n 
~~ return formatted \n 
~~ def _parse_preamble ( self , preamble ) : \n 
if preamble [ - 1 ] != : \n 
~~~ doc_id_left = preamble . rfind ( ) \n 
doc_id = preamble [ doc_id_left + 1 : ] \n 
preamble = preamble [ : doc_id_left ] \n 
~~~ doc_id = None \n 
~~ in_target = True \n 
target = \n 
importance = \n 
for char in preamble : \n 
~~~ if char == : \n 
~~~ in_target = False \n 
~~ elif in_target : \n 
~~~ target += char \n 
~~~ importance += char \n 
~~ ~~ parsed = { } \n 
items = ( \n 
( , doc_id ) , ( , target ) , ( , importance ) ) \n 
~~~ if key in [ , ] : \n 
~~~ parsed [ key ] = self . _string_to_number ( value ) \n 
~~~ parsed [ key ] = value \n 
~~ ~~ ~~ return parsed \n 
~~ ~~ class SVMLightFormatter ( SparseFormatter ) : \n 
self . format_name = \n 
self , feature_values = None , target = 1 , importance = None , doc_id = None ) : \n 
formatted = str ( target ) + \n 
~~~ return { : float ( preamble ) } \n 
~~ ~~ class SFileFilter ( SaveLoad ) : \n 
def __init__ ( self , formatter , bit_precision = 18 , sfile = None , verbose = True ) : \n 
assert isinstance ( bit_precision , int ) \n 
self . formatter = formatter \n 
self . bit_precision = bit_precision \n 
self . verbose = verbose \n 
self . precision = 2 ** bit_precision \n 
self . sfile_loaded = False \n 
self . bit_precision_required = bit_precision \n 
if sfile is not None : \n 
~~~ self . load_sfile ( sfile ) \n 
~~ ~~ def _get_hash_fun ( self ) : \n 
if self . bit_precision <= 64 : \n 
~~~ hash_fun = lambda w : hash ( w ) % self . precision \n 
~~ elif self . bit_precision <= 224 : \n 
~~~ hash_fun = lambda w : ( \n 
int ( hashlib . sha224 ( w ) . hexdigest ( ) , 16 ) % self . precision ) \n 
~~ return hash_fun \n 
~~ def load_sfile ( self , sfile ) : \n 
assert not self . sfile_loaded \n 
token2id , token_score , doc_freq , num_docs , idf = ( \n 
self . _load_sfile_fwd ( sfile ) ) \n 
self . token2id = token2id \n 
self . token_score = token_score \n 
self . doc_freq = doc_freq \n 
self . num_docs = num_docs \n 
self . idf = idf \n 
self . sfile_loaded = True \n 
self . collisions_resolved = False \n 
~~ def _load_sfile_fwd ( self , sfile ) : \n 
token2id = { } \n 
token_score = defaultdict ( float ) \n 
doc_freq = defaultdict ( int ) \n 
num_docs = 0 \n 
idf = defaultdict ( float ) \n 
hash_fun = self . _get_hash_fun ( ) \n 
with smart_open ( sfile ) as open_file : \n 
~~~ for line in open_file : \n 
~~~ num_docs += 1 \n 
record_dict = self . formatter . sstr_to_dict ( line ) \n 
for token , value in record_dict [ ] . iteritems ( ) : \n 
~~~ hash_value = hash_fun ( token ) \n 
token2id [ token ] = hash_value \n 
token_score [ token ] += value \n 
doc_freq [ token ] += 1 \n 
idf [ token ] += 1 \n 
~~ ~~ ~~ for token in idf . iterkeys ( ) : \n 
~~~ idf [ token ] = math . log ( num_docs / idf [ token ] ) \n 
~~ return token2id , token_score , doc_freq , num_docs , idf \n 
~~ def set_id2token ( self , seed = None ) : \n 
self . _resolve_collisions ( seed = seed ) \n 
self . id2token = { v : k for k , v in self . token2id . iteritems ( ) } \n 
~~ def _resolve_collisions ( self , seed = None ) : \n 
id_counts = Counter ( self . token2id . values ( ) ) \n 
vocab_size = self . vocab_size \n 
num_collisions = vocab_size - len ( id_counts ) \n 
self . _print ( \n 
if num_collisions > vocab_size / 2. : \n 
% ( num_collisions , vocab_size ) ) \n 
raise CollisionError ( msg ) \n 
~~ random . seed ( seed ) \n 
collisions = ( \n 
tok for tok in self . token2id if id_counts [ self . token2id [ tok ] ] > 1 ) \n 
for token in collisions : \n 
~~~ old_id = self . token2id [ token ] \n 
new_id = old_id \n 
if id_counts [ old_id ] > 1 : \n 
~~~ while new_id in id_counts : \n 
~~~ new_id = random . randint ( 0 , self . precision - 1 ) \n 
new_id = new_id % self . precision \n 
~~ id_counts [ old_id ] -= 1 \n 
id_counts [ new_id ] = 1 \n 
~~ self . token2id [ token ] = new_id \n 
self . collisions_resolved = True \n 
~~ def compactify ( self ) : \n 
min_precision = int ( np . ceil ( np . log2 ( self . vocab_size ) ) ) \n 
if self . bit_precision < min_precision : \n 
~~~ raise CollisionError ( \n 
~~ new_token2id = { } \n 
for i , tok in enumerate ( self . token2id ) : \n 
~~~ new_token2id [ tok ] = i \n 
~~ self . token2id = new_token2id \n 
if hasattr ( self , ) : \n 
~~~ self . set_id2token ( ) \n 
~~ self . set_bit_precision_required ( ) \n 
% self . bit_precision_required ) \n 
~~ def set_bit_precision_required ( self ) : \n 
max_id = np . max ( self . token2id . values ( ) ) \n 
self . bit_precision_required = int ( np . ceil ( np . log2 ( max_id ) ) ) \n 
~~ def filter_sfile ( \n 
self , infile , outfile , doc_id_list = None , enforce_all_doc_id = True , \n 
min_tf_idf = 0 , filters = None ) : \n 
if not hasattr ( self , ) : \n 
~~~ self . _print ( \n 
~~ if filters is None : \n 
~~~ filters = [ ] \n 
~~ prefilters = [ ] \n 
if doc_id_list is not None : \n 
~~~ doc_id_set = set ( doc_id_list ) \n 
prefilters . append ( streaming_filters . get_doc_id_filter ( doc_id_set ) ) \n 
~~~ doc_id_set = set ( ) \n 
~~ if min_tf_idf != 0 : \n 
~~~ prefilters . append ( \n 
streaming_filters . get_tf_idf_filter ( self , min_tf_idf ) ) \n 
~~ postfilters = [ streaming_filters . get_token_to_id_filter ( self ) ] \n 
filters = prefilters + filters + postfilters \n 
doc_id_seen = set ( ) \n 
with smart_open ( infile ) as f , smart_open ( outfile , ) as g : \n 
~~~ record_dict = self . formatter . sstr_to_dict ( line ) \n 
doc_id = record_dict [ ] \n 
doc_id_seen . add ( doc_id ) \n 
if all ( func ( record_dict ) for func in filters ) : \n 
~~~ new_sstr = self . formatter . get_sstr ( ** record_dict ) \n 
g . write ( new_sstr + ) \n 
~~ ~~ ~~ if enforce_all_doc_id : \n 
~~~ assert doc_id_set . issubset ( doc_id_seen ) , ( \n 
~~ ~~ def filter_extremes ( \n 
self , doc_freq_min = 0 , doc_freq_max = np . inf , doc_fraction_min = 0 , \n 
doc_fraction_max = 1 , token_score_min = 0 , token_score_max = np . inf , \n 
token_score_quantile_min = 0 , token_score_quantile_max = 1 ) : \n 
frame = self . to_frame ( ) \n 
to_remove_mask = ( \n 
( frame . doc_freq < doc_freq_min ) \n 
| ( frame . doc_freq > doc_freq_max ) \n 
| ( frame . doc_freq < ( doc_fraction_min * self . num_docs ) ) \n 
| ( frame . doc_freq > ( doc_fraction_max * self . num_docs ) ) \n 
| ( frame . token_score < token_score_min ) \n 
| ( frame . token_score > token_score_max ) \n 
| ( frame . token_score \n 
< frame . token_score . quantile ( token_score_quantile_min ) ) \n 
> frame . token_score . quantile ( token_score_quantile_max ) ) \n 
self . filter_tokens ( frame [ to_remove_mask ] . index ) \n 
~~ def filter_tokens ( self , tokens ) : \n 
if isinstance ( tokens , str ) : \n 
~~~ tokens = [ tokens ] \n 
~~ for tok in tokens : \n 
~~~ id_value = self . token2id [ tok ] \n 
self . token2id . pop ( tok ) \n 
self . token_score . pop ( tok ) \n 
self . doc_freq . pop ( tok ) \n 
~~~ self . id2token . pop ( id_value ) \n 
~~ ~~ ~~ def _print ( self , msg ) : \n 
~~~ print ( msg ) \n 
~~ ~~ def to_frame ( self ) : \n 
token2id = self . token2id \n 
token_score = self . token_score \n 
doc_freq = self . doc_freq \n 
frame = pd . DataFrame ( \n 
{ : [ token_score [ tok ] for tok in token2id ] , \n 
: [ doc_freq [ tok ] for tok in token2id ] } , \n 
index = [ tok for tok in token2id ] ) \n 
frame [ ] = frame . doc_freq / float ( self . num_docs ) \n 
frame . index . name = \n 
return frame \n 
def vocab_size ( self ) : \n 
~~~ return len ( self . token2id ) \n 
~~ def save ( self , savepath , protocol = - 1 , set_id2token = True ) : \n 
if set_id2token : \n 
~~ SaveLoad . save ( self , savepath , protocol = protocol ) \n 
~~ ~~ def collision_probability ( vocab_size , bit_precision ) : \n 
exponent = - vocab_size * ( vocab_size - 1 ) / 2. ** bit_precision \n 
return 1 - np . exp ( exponent ) \n 
~~ class CollisionError ( Exception ) : \n 
import tarfile \n 
from copy import deepcopy \n 
from conda . compat import PY3 \n 
~~~ from io import StringIO , BytesIO \n 
~~~ from cStringIO import StringIO \n 
BytesIO = StringIO \n 
libpy_pat = re . compile ( \n 
def has_cext ( t , show = False ) : \n 
for m in t . getmembers ( ) : \n 
~~~ match = libpy_pat . match ( m . path ) \n 
~~~ if show : \n 
~~~ x = match . group ( 3 ) \n 
print ( "import" , x . replace ( , ) ) \n 
matched = True \n 
~~ ~~ ~~ return matched \n 
~~ def has_nonpy_entry_points ( t , unix_to_win = True , show = False , quiet = False ) : \n 
if not quiet : \n 
~~ bindir = if unix_to_win else \n 
matched = False \n 
~~~ if m . path . startswith ( bindir ) : \n 
~~~ if not unix_to_win : \n 
"supported" ) \n 
~~ r = t . extractfile ( m ) . read ( ) \n 
~~~ r = r . decode ( ) \n 
~~ matched = True \n 
~~~ firstline = r . splitlines ( ) [ 0 ] \n 
if not in firstline : \n 
~~ ~~ ~~ ~~ ~~ return matched \n 
~~ def tar_update ( source , dest , file_map , verbose = True , quiet = False ) : \n 
if isinstance ( source , tarfile . TarFile ) : \n 
~~~ s = source \n 
~~~ if not source . endswith ( ( , ) ) : \n 
~~ s = tarfile . open ( source ) \n 
~~ if isinstance ( dest , tarfile . TarFile ) : \n 
~~~ t = dest \n 
~~~ t = tarfile . open ( dest , ) \n 
~~~ for m in s . getmembers ( ) : \n 
~~~ p = m . path \n 
if p in file_map : \n 
~~~ if file_map [ p ] is None : \n 
~~~ print ( % p ) \n 
~~~ print ( % ( p , file_map [ p ] ) ) \n 
~~ if isinstance ( file_map [ p ] , tarfile . TarInfo ) : \n 
~~~ t . addfile ( file_map [ p ] , s . extractfile ( file_map [ p ] ) ) \n 
~~ elif isinstance ( file_map [ p ] , tuple ) : \n 
~~~ t . addfile ( * file_map [ p ] ) \n 
~~~ t . add ( file_map [ p ] , p ) \n 
~~ if not quiet : \n 
~~ t . addfile ( m , s . extractfile ( p ) ) \n 
~~ s_names_set = set ( m . path for m in s . getmembers ( ) ) \n 
for p in sorted ( file_map ) : \n 
~~~ if p not in s_names_set : \n 
~~ ~~ ~~ ~~ finally : \n 
~~~ t . close ( ) \n 
~~ ~~ path_mapping_bat_proxy = [ \n 
( re . compile ( ) , ) , \n 
path_mapping_unix_windows = [ \n 
path_mapping_windows_unix = [ \n 
pyver_re = re . compile ( ) \n 
def get_pure_py_file_map ( t , platform ) : \n 
~~~ info = json . loads ( t . extractfile ( ) . read ( ) . decode ( ) ) \n 
source_plat = info [ ] \n 
source_type = if source_plat in { , } else \n 
dest_plat , dest_arch = platform . split ( ) \n 
dest_type = if dest_plat in { , } else \n 
files = t . extractfile ( ) . read ( ) . decode ( "utf-8" ) \n 
if source_type == and dest_type == : \n 
~~~ mapping = path_mapping_unix_windows \n 
~~ elif source_type == and dest_type == : \n 
~~~ mapping = path_mapping_windows_unix \n 
~~~ mapping = [ ] \n 
~~ newinfo = info . copy ( ) \n 
newinfo [ ] = dest_plat \n 
newinfo [ ] = if dest_arch == else \n 
newinfo [ ] = platform \n 
pythons = list ( filter ( None , [ pyver_re . match ( p ) for p in info [ ] ] ) ) \n 
if len ( pythons ) > 1 : \n 
% t . name ) \n 
~~ if len ( pythons ) == 0 : \n 
~~~ pyver = pythons [ 0 ] . group ( 1 ) \n 
mapping = [ ( re . compile ( i [ 0 ] . format ( pyver = pyver ) ) , \n 
i [ 1 ] . format ( pyver = pyver ) ) for i in mapping ] \n 
~~ members = t . getmembers ( ) \n 
file_map = { } \n 
for member in members : \n 
~~~ if member . path == : \n 
~~~ newmember = tarfile . TarInfo ( ) \n 
~~~ newbytes = bytes ( json . dumps ( newinfo ) , ) \n 
~~~ newbytes = json . dumps ( newinfo ) \n 
~~ newmember . size = len ( newbytes ) \n 
file_map [ ] = ( newmember , BytesIO ( newbytes ) ) \n 
~~ elif member . path == : \n 
~~~ filemember = deepcopy ( member ) \n 
~~~ if source_type == and dest_type == : \n 
~~~ file_map [ ] = None \n 
~~ ~~ oldpath = member . path \n 
for old , new in mapping : \n 
~~~ newpath = old . sub ( new , oldpath ) \n 
if oldpath in file_map : \n 
~~ if newpath != oldpath : \n 
~~~ newmember = deepcopy ( member ) \n 
newmember . path = newpath \n 
assert member . path == oldpath \n 
file_map [ oldpath ] = None \n 
file_map [ newpath ] = newmember \n 
files = files . replace ( oldpath , newpath ) \n 
~~ ~~ batseen = set ( ) \n 
~~~ for old , new in path_mapping_bat_proxy : \n 
if oldpath in batseen : \n 
~~~ newmember = tarfile . TarInfo ( newpath ) \n 
~~~ data = bytes ( BAT_PROXY . replace ( , ) , ) \n 
~~~ data = BAT_PROXY . replace ( , ) \n 
~~ newmember . size = len ( data ) \n 
file_map [ newpath ] = newmember , BytesIO ( data ) \n 
batseen . add ( oldpath ) \n 
files = files + newpath + "\\n" \n 
~~ ~~ ~~ ~~ files = . join ( sorted ( files . splitlines ( ) ) ) + \n 
~~~ files = bytes ( files , ) \n 
~~ filemember . size = len ( files ) \n 
file_map [ ] = filemember , BytesIO ( files ) \n 
return file_map \n 
from os . path import dirname , isdir , join \n 
import conda . config as cc \n 
from conda_build . config import config \n 
bin_dirname = if sys . platform == else \n 
entry_pat = re . compile ( ) \n 
def iter_entry_points ( items ) : \n 
~~~ for item in items : \n 
~~~ m = entry_pat . match ( item ) \n 
~~ yield m . groups ( ) \n 
~~ ~~ def create_entry_point ( path , module , func ) : \n 
~~~ pyscript = PY_TMPL % { : module , : func } \n 
~~~ with open ( path + , ) as fo : \n 
~~~ fo . write ( pyscript ) \n 
~~ shutil . copyfile ( join ( dirname ( __file__ ) , % cc . bits ) , \n 
path + ) \n 
~~~ with open ( path , ) as fo : \n 
~~~ fo . write ( % config . build_python ) \n 
fo . write ( pyscript ) \n 
~~ os . chmod ( path , int ( , 8 ) ) \n 
~~ ~~ def create_entry_points ( items ) : \n 
~~~ if not items : \n 
~~ bin_dir = join ( config . build_prefix , bin_dirname ) \n 
if not isdir ( bin_dir ) : \n 
~~~ os . mkdir ( bin_dir ) \n 
~~ for cmd , module , func in iter_entry_points ( items ) : \n 
~~~ create_entry_point ( join ( bin_dir , cmd ) , module , func ) \n 
~~ ~~ def prepend_bin_path ( env , prefix , prepend_prefix = False ) : \n 
~~~ env [ ] = join ( prefix , bin_dirname ) + os . pathsep + env [ ] \n 
~~~ env [ ] = join ( prefix , "Library" , "bin" ) + os . pathsep + env [ ] \n 
~~ if prepend_prefix : \n 
~~~ env [ ] = prefix + os . pathsep + env [ ] \n 
~~ return env \n 
~~~ prefix = os . environ [ ] \n 
info_files = glob . glob ( os . path . join ( prefix , , \n 
assert len ( info_files ) == 1 \n 
info_file = info_files [ 0 ] \n 
with open ( info_file , ) as fh : \n 
~~~ info = json . load ( fh ) \n 
~~ assert len ( info [ ] ) == 2 \n 
depends = sorted ( info [ ] ) \n 
assert depends [ 0 ] == \n 
assert depends [ 1 ] . startswith ( ) \n 
~~ from __future__ import print_function , division , absolute_import \n 
from os . path import isdir , isfile , join , expanduser \n 
from conda . utils import memoized \n 
def find_executable ( executable , include_others = True ) : \n 
~~~ global dir_paths \n 
if include_others : \n 
~~~ if sys . platform == : \n 
~~~ dir_paths = [ join ( sys . prefix , ) , \n 
~~~ dir_paths = [ join ( sys . prefix , ) ] \n 
~~~ dir_paths = [ ] \n 
~~ dir_paths . extend ( os . environ [ ] . split ( os . pathsep ) ) \n 
for dir_path in dir_paths : \n 
~~~ for ext in ( , , ) : \n 
~~~ path = join ( dir_path , executable + ext ) \n 
if isfile ( path ) : \n 
~~~ path = join ( dir_path , executable ) \n 
if isfile ( expanduser ( path ) ) : \n 
~~~ return expanduser ( path ) \n 
~~ @ memoized \n 
def find_commands ( include_others = True ) : \n 
~~~ if include_others : \n 
~~ if sys . platform == : \n 
~~~ pat = re . compile ( ) \n 
~~ res = set ( ) \n 
~~~ if not isdir ( dir_path ) : \n 
~~ for fn in os . listdir ( dir_path ) : \n 
~~~ if not isfile ( join ( dir_path , fn ) ) : \n 
~~ m = pat . match ( fn ) \n 
if m : \n 
~~~ res . add ( m . group ( 1 ) ) \n 
~~ ~~ ~~ return sorted ( res ) \n 
~~ def filter_descr ( cmd ) : \n 
~~~ args = [ find_executable ( + cmd ) , ] \n 
if not args [ 0 ] : \n 
~~~ print ( % ( cmd ) ) \n 
~~~ output = subprocess . check_output ( args ) \n 
~~ except ( OSError , subprocess . CalledProcessError ) : \n 
~~~ print ( % ( . join ( args ) ) ) \n 
~~ pat = re . compile ( , re . DOTALL ) \n 
m = pat . search ( output . decode ( ) ) \n 
descr = [ ] if m is None else m . group ( 2 ) . splitlines ( ) \n 
print ( % ( cmd , descr [ 0 ] ) ) \n 
for d in descr [ 1 : ] : \n 
~~~ print ( % d ) \n 
~~ ~~ def help ( ) : \n 
for cmd in find_commands ( ) : \n 
~~~ filter_descr ( cmd ) \n 
~~~ help ( ) \n 
from conda . exceptions import LockError \n 
LOCKFN = \n 
stdoutlog = logging . getLogger ( ) \n 
class Locked ( object ) : \n 
def __init__ ( self , path , retries = 10 ) : \n 
self . end = "-" + str ( os . getpid ( ) ) \n 
self . lock_path = os . path . join ( self . path , LOCKFN + self . end ) \n 
self . retries = retries \n 
sleeptime = 1 \n 
for _ in range ( self . retries ) : \n 
~~~ if os . path . isdir ( self . lock_path ) : \n 
~~~ stdoutlog . info ( lockstr % self . lock_path ) \n 
time . sleep ( sleeptime ) \n 
sleeptime *= 2 \n 
~~~ os . makedirs ( self . lock_path ) \n 
raise LockError ( lockstr % self . lock_path ) \n 
~~ def __exit__ ( self , exc_type , exc_value , traceback ) : \n 
~~~ os . rmdir ( self . lock_path ) \n 
os . rmdir ( self . path ) \n 
~~ ~~ ~~ import os . path \n 
from conda . lock import Locked , LockError \n 
def test_lock_passes ( tmpdir ) : \n 
~~~ with Locked ( tmpdir . strpath ) as lock : \n 
~~~ path = os . path . basename ( lock . lock_path ) \n 
assert tmpdir . join ( path ) . exists ( ) and tmpdir . join ( path ) . isdir ( ) \n 
~~ assert not tmpdir . join ( path ) . exists ( ) \n 
assert not tmpdir . exists ( ) \n 
~~ def test_lock_locks ( tmpdir ) : \n 
~~~ with Locked ( tmpdir . strpath ) as lock1 : \n 
~~~ path = os . path . basename ( lock1 . lock_path ) \n 
with pytest . raises ( LockError ) as execinfo : \n 
~~~ with Locked ( tmpdir . strpath , retries = 1 ) as lock2 : \n 
~~ assert lock2 . lock_path == lock1 . lock_path \n 
~~ assert "LOCKERROR" in str ( execinfo ) \n 
~~ import Neuron \n 
class TanhNeuron ( Neuron . Neuron ) : \n 
~~ def Activate ( self , x ) : \n 
~~~ return math . tanh ( x ) \n 
~~ def Derivative ( self , x ) : \n 
~~~ coshx = math . cosh ( x ) \n 
denom = ( math . cosh ( 2 * x ) + 1 ) \n 
return 4 * coshx * coshx / ( denom * denom ) \n 
from sklearn . linear_model import LinearRegression , ElasticNet , Lasso , Ridge , ElasticNetCV \n 
from sklearn . feature_selection import SelectPercentile , f_classif , chi2 \n 
from sklearn . feature_selection import SelectKBest , f_regression \n 
from sklearn import preprocessing \n 
def plot ( nFeatures , data ) : \n 
algorithm = sorted ( data ) \n 
fig = plt . figure ( ) \n 
for j , c in zip ( algorithm , colors ) : \n 
~~~ ax . plot ( nFeatures , data [ j ] , label = j , color = c ) \n 
ax . scatter ( nFeatures , data [ j ] , color = c ) \n 
~~ plt . xlabel ( "#-Features(SelectPercentile)" ) \n 
box = ax . get_position ( ) \n 
ax . set_position ( [ box . x0 , box . y0 + box . height * 0.3 , \n 
box . width , box . height * 0.7 ] ) \n 
plt . legend ( loc = 2 ) \n 
~~ def preprocess ( article_file , lable_file , k ) : \n 
~~~ features = pickle . load ( open ( article_file ) ) \n 
features = np . array ( features ) \n 
le = preprocessing . LabelEncoder ( ) \n 
le . fit ( lables ) \n 
lables = le . transform ( lables ) \n 
vectorizer = TfidfVectorizer ( sublinear_tf = True , max_df = 0.5 , min_df = 1 , \n 
stop_words = ) \n 
features_train_transformed = vectorizer . fit_transform ( features ) \n 
selector = SelectPercentile ( f_classif , percentile = k ) \n 
selector . fit ( features_train_transformed , lables ) \n 
features_train_transformed = selector . transform ( features_train_transformed ) . toarray ( ) \n 
return features_train_transformed , lables , vectorizer , selector , le , features \n 
~~ nFeatures = np . arange ( 10 , 40 , 10 ) \n 
for k in nFeatures : \n 
~~~ features , labels , vectorizer , selector , le , features_data = preprocess ( "pkl/article_2_people.pkl" features_train , features_test , labels_train , labels_test = cross_validation . train_test_split ( features \n 
for name , clf in [ \n 
( , LinearRegression ( fit_intercept = True ) ) , \n 
~~~ if not data . has_key ( name ) : \n 
~~~ data [ name ] = [ ] \n 
~~ print "*" * 100 \n 
print ( . format ( name ) + . format ( k ) ) \n 
t0 = time ( ) \n 
clf . fit ( features_train , labels_train ) \n 
y_pred = clf . predict ( features_test ) \n 
score_accuracy = mean_squared_error ( y_pred , labels_test ) \n 
print ( . format ( r2_score ( y_pred , labels_test ) ) ) \n 
print ( . format ( score_accuracy ) ) \n 
print "*" * 100 \n 
data [ name ] . append ( score_accuracy ) \n 
~~ ~~ plot ( nFeatures , data ) \n 
print datafrom nn import NeuralNet \n 
class Experience ( ) : \n 
~~~ def __init__ ( self , s1 , a1 , r , s2 , a2 ) : \n 
self . s1 = s1 \n 
self . s2 = s2 \n 
self . a1 = a1 \n 
self . a2 = a2 \n 
self . r = r \n 
~~ ~~ class QNN ( ) : \n 
def __call__ ( self , s , a = None ) : \n 
if a == None : \n 
~~~ return self . GetValue ( s ) \n 
~~ return self . GetValue ( s , a ) \n 
~~~ lay = [ input_size , int ( ( nactions + input_size ) / 2.0 ) , int ( ( nactions + input_size ) / 2.0 ) , nactions ] self . nactions = nactions \n 
self . NN = NeuralNet ( layers = lay , epsilon = 0.154 , learningRate = alpha ) \n 
self . experiences = [ ] \n 
self . max_experiences = max_experiences \n 
self . gamma = gamma \n 
self . use_sarsa = use_sarsa \n 
self . prob_remember = 0.1 \n 
self . num_replay_samples = 10 \n 
~~ def GetValue ( self , s , a = None ) : \n 
out = self . NN . propagate ( s ) \n 
if ( a == None ) : \n 
~~~ return out \n 
~~ return out [ a ] \n 
~~ def Update ( self , s1 , a1 , r , s2 , a2 ) : \n 
if ( self . use_sarsa ) : \n 
~~~ v = r + self . gamma * self . GetValue ( s2 , a2 ) \n 
~~~ v = r + self . gamma * max ( self . GetValue ( s2 ) ) \n 
~~ a = np . zeros ( self . nactions ) \n 
a [ a1 ] = v \n 
self . NN . propagateAndUpdate ( s1 , a ) \n 
~~ def RememberExperience ( self , s1 , a1 , r , s2 , a2 ) : \n 
~~~ if ( random . random ( ) < self . prob_remember ) : \n 
~~~ if ( len ( self . experiences ) >= self . max_experiences ) : \n 
~~~ self . experiences . pop ( random . randint ( 0 , self . max_experiences - 1 ) ) \n 
~~ self . experiences . append ( Experience ( s1 , a1 , r , s2 , a2 ) ) \n 
~~ ~~ def ExperienceReplay ( self ) : \n 
~~~ if ( len ( self . experiences ) < self . num_replay_samples ) : \n 
~~ for i in xrange ( self . num_replay_samples ) : \n 
~~~ index = random . randint ( 0 , len ( self . experiences ) - 1 ) \n 
exp = self . experiences [ index ] \n 
self . Update ( exp . s1 , exp . a1 , exp . r , exp . s2 , exp . a2 ) \n 
number_passengers = np . size ( data [ 0 : : , 1 ] . astype ( np . float ) ) \n 
number_survived = np . sum ( data [ 0 : : , 1 ] . astype ( np . float ) ) \n 
proportion_survivors = number_survived / number_passengers \n 
men_only_stats = data [ 0 : : , 4 ] != "female" \n 
women_onboard = data [ women_only_stats , 1 ] . astype ( np . float ) \n 
men_onboard = data [ men_only_stats , 1 ] . astype ( np . float ) \n 
proportion_women_survived = np . sum ( women_onboard ) / np . size ( women_onboard ) \n 
proportion_men_survived = np . sum ( men_onboard ) / np . size ( men_onboard ) \n 
print % proportion_women_survived \n 
print % proportion_men_survived \n 
test_file = open ( , ) \n 
test_file_object = csv . reader ( test_file ) \n 
header = test_file_object . next ( ) \n 
predictions_file = open ( "gendermodel.csv" , "wb" ) \n 
predictions_file_object = csv . writer ( predictions_file ) \n 
def unit_range ( arr , data_min = None , data_max = None , samples_in = ) : \n 
samplesIn = 1 if samples_in == else 0 \n 
dimsIn = int ( not samplesIn ) \n 
nSamples = arr . shape [ samplesIn ] \n 
nDims = arr . shape [ dimsIn ] \n 
theshape = [ 1 , 1 ] \n 
theshape [ dimsIn ] = nDims \n 
if data_max is None : \n 
~~~ data_max = arr . max ( axis = samplesIn ) \n 
~~ if data_min is None : \n 
~~~ data_min = arr . min ( axis = samplesIn ) \n 
~~ norma = ( arr - data_min . reshape ( theshape ) ) / ( data_max - data_min ) . reshape ( theshape ) \n 
return norma , data_min , data_max \n 
~~ def mean_zero ( arr , mean_vector = None , samples_in = ) : \n 
if not mean_vector : \n 
~~~ mean_vector = arr . mean ( axis = samplesIn ) . reshape ( theshape ) \n 
~~ amean = arr - mean_vector \n 
return amean , mean_vector \n 
~~ def mean_zero_unit_variance ( arr , mean_vector = None , std_vector = None , samples_in = ) : \n 
~~ if not std_vector : \n 
~~~ std_vector = arr . std ( axis = samplesIn ) . reshape ( theshape ) \n 
~~ std_vector [ std_vector < 1e-6 ] = 1 \n 
norma = ( arr - mean_vector ) / std_vector \n 
return norma , mean_vector , std_vector \n 
~~ def labels_to_one_hot ( labels ) : \n 
n_samples = len ( labels ) \n 
dlabels = { } \n 
ix = 0 \n 
~~~ if label not in dlabels : \n 
~~~ dlabels [ label ] = ix \n 
ix += 1 \n 
~~ ~~ n_dims = len ( dlabels ) \n 
arr = np . zeros ( ( n_samples , n_dims ) ) \n 
for i in xrange ( n_samples ) : \n 
~~~ arr [ i , dlabels [ labels [ i ] ] ] = 1.0 \n 
~~ return arr , dlabels \n 
from sqlalchemy import orm \n 
from dssg import db \n 
class BaseModel : \n 
~~~ for k , v in kwargs . iteritems ( ) : \n 
~~~ setattr ( self , k , v ) \n 
def by_id ( cls , id ) : \n 
return db . session . query ( cls ) . get ( id ) \n 
~~ def create ( self ) : \n 
db . session . add ( self ) \n 
db . session . commit ( ) \n 
db . session . delete ( self ) \n 
~~ def as_dict ( self ) : \n 
_dict = { } \n 
table = orm . class_mapper ( self . __class__ ) . mapped_table \n 
for col in table . c : \n 
~~~ val = getattr ( self , col . name ) \n 
if isinstance ( val , datetime . date ) : \n 
~~~ val = str ( val ) \n 
~~ if isinstance ( val , datetime . datetime ) : \n 
~~~ val = val . isoformat ( ) \n 
~~ _dict [ col . name ] = val \n 
~~ return _dict \n 
def create_all ( cls , entries = [ ] ) : \n 
for row in entries : \n 
~~~ db . session . add ( row ) \n 
~~ if len ( entries ) > 0 : \n 
~~~ db . session . commit ( ) \n 
from itertools import product as iter_product \n 
import sqlite3 \n 
from prettytable import PrettyTable \n 
from bayesian . persistance import SampleDB , ensure_data_dir_exists \n 
from bayesian . exceptions import * \n 
from bayesian . utils import get_args \n 
GREEN = \n 
NORMAL = \n 
class Node ( object ) : \n 
~~~ def is_leaf ( self ) : \n 
~~~ if len ( self . neighbours ) == 1 : \n 
~~ def send ( self , message ) : \n 
~~~ recipient = message . destination \n 
~~~ print % ( \n 
self . name , recipient . name ) , message \n 
~~ recipient . received_messages [ \n 
self . name ] = message \n 
~~ def get_sent_messages ( self ) : \n 
~~~ sent_messages = { } \n 
for neighbour in self . neighbours : \n 
~~~ if neighbour . received_messages . get ( self . name ) : \n 
~~~ sent_messages [ neighbour . name ] = neighbour . received_messages . get ( self . name ) \n 
~~ ~~ return sent_messages \n 
~~ def message_report ( self ) : \n 
print % self . name \n 
for k , v in self . received_messages . iteritems ( ) : \n 
~~~ print % ( v . source . name , v . argspec ) \n 
v . list_factors ( ) \n 
~~ def get_target ( self ) : \n 
neighbours = self . neighbours \n 
needed_to_send = defaultdict ( int ) \n 
for target in neighbours : \n 
~~~ needed_to_send [ target ] = len ( neighbours ) - 1 \n 
~~ for _ , message in self . received_messages . items ( ) : \n 
~~~ for target in neighbours : \n 
~~~ if message . source != target : \n 
~~~ needed_to_send [ target ] -= 1 \n 
~~ ~~ ~~ for k , v in needed_to_send . items ( ) : \n 
~~~ if v == 0 and not self . name in k . received_messages : \n 
~~~ return k \n 
~~ ~~ ~~ def get_neighbour_by_name ( self , name ) : \n 
~~~ for node in self . neighbours : \n 
~~~ if node . name == name : \n 
~~~ return node \n 
~~ ~~ ~~ ~~ class VariableNode ( Node ) : \n 
~~~ def __init__ ( self , name , domain = [ True , False ] ) : \n 
self . neighbours = [ ] \n 
self . received_messages = { } \n 
self . value = None \n 
~~ def construct_message ( self ) : \n 
~~~ target = self . get_target ( ) \n 
message = make_variable_node_message ( self , target ) \n 
return message \n 
~~~ return % ( self . name , self . value ) \n 
~~ def marginal ( self , val , normalizer = 1.0 ) : \n 
product = 1 \n 
for _ , message in self . received_messages . iteritems ( ) : \n 
~~~ product *= message ( val ) \n 
~~ return product / normalizer \n 
~~~ self . received_messages = { } \n 
~~ def verify_neighbour_types ( self ) : \n 
for node in self . neighbours : \n 
~~~ if not isinstance ( node , FactorNode ) : \n 
~~ ~~ class FactorNode ( Node ) : \n 
~~~ def __init__ ( self , name , func , neighbours = [ ] ) : \n 
self . func = func \n 
self . neighbours = neighbours [ : ] \n 
self . func . value = None \n 
self . cached_functions = [ ] \n 
message = make_factor_node_message ( self , target ) \n 
~~~ if not isinstance ( node , VariableNode ) : \n 
~~~ return % ( self . name , \n 
self . func . __name__ , \n 
get_args ( self . func ) ) \n 
~~ def marginal ( self , val_dict ) : \n 
~~~ product = 1 \n 
for neighbour in neighbours : \n 
~~~ message = self . received_messages [ neighbour . name ] \n 
call_args = [ ] \n 
for arg in get_args ( message ) : \n 
~~~ call_args . append ( val_dict [ arg ] ) \n 
~~ if not call_args : \n 
~~~ call_args . append ( ) \n 
~~ product *= message ( * call_args ) \n 
~~ call_args = [ ] \n 
for arg in get_args ( self . func ) : \n 
~~ product *= self . func ( * call_args ) \n 
return product \n 
~~ def add_evidence ( self , node , value ) : \n 
args = get_args ( self . func ) \n 
pos = args . index ( node . name ) \n 
old_func = self . func \n 
self . cached_functions . insert ( 0 , old_func ) \n 
def evidence_func ( * args ) : \n 
~~~ if args [ pos ] != value : \n 
~~ return old_func ( * args ) \n 
~~ evidence_func . argspec = args \n 
evidence_func . domains = old_func . domains \n 
self . func = evidence_func \n 
if self . cached_functions : \n 
~~~ self . func = self . cached_functions [ - 1 ] \n 
~~ ~~ ~~ class Message ( object ) : \n 
~~~ def list_factors ( self ) : \n 
print % ( self . source . name , self . destination . name ) \n 
for factor in self . factors : \n 
~~~ print factor \n 
~~ ~~ def __call__ ( self , var ) : \n 
if getattr ( self . func , , None ) == : \n 
~~ assert not isinstance ( var , VariableNode ) \n 
return self . func ( var ) \n 
~~ ~~ class VariableMessage ( Message ) : \n 
~~~ def __init__ ( self , source , destination , factors , func ) : \n 
~~~ self . source = source \n 
self . destination = destination \n 
self . factors = factors \n 
self . argspec = get_args ( func ) \n 
~~~ return % ( self . source . name , self . destination . name , \n 
len ( self . factors ) , self . argspec ) \n 
~~ ~~ class FactorMessage ( Message ) : \n 
self . domains = func . domains \n 
self . argspec , \n 
len ( self . factors ) ) \n 
~~ ~~ def connect ( a , b ) : \n 
if not isinstance ( b , list ) : \n 
~~~ b = [ b ] \n 
~~ for b_ in b : \n 
~~~ a . neighbours . append ( b_ ) \n 
b_ . neighbours . append ( a ) \n 
~~ ~~ def eliminate_var ( f , var ) : \n 
arg_spec = get_args ( f ) \n 
pos = arg_spec . index ( var ) \n 
new_spec = arg_spec [ : ] \n 
new_spec . remove ( var ) \n 
eliminated_pos = arg_spec . index ( var ) \n 
def eliminated ( * args ) : \n 
~~~ template = arg_spec [ : ] \n 
call_args = template [ : ] \n 
~~~ if i == eliminated_pos : \n 
~~~ call_args [ i ] = \n 
~~ call_args [ i ] = arg \n 
~~ for val in f . domains [ var ] : \n 
~~~ call_args [ pos ] = val \n 
total += f ( * call_args ) \n 
~~ return total \n 
~~ eliminated . argspec = new_spec \n 
eliminated . domains = f . domains \n 
return eliminated \n 
~~ def memoize ( f ) : \n 
def memoized ( * args ) : \n 
~~~ arg_vals = tuple ( args ) \n 
if not arg_vals in cache : \n 
~~~ cache [ arg_vals ] = f ( * args ) \n 
~~ return cache [ arg_vals ] \n 
~~ if hasattr ( f , ) : \n 
~~~ memoized . domains = f . domains \n 
~~~ memoized . argspec = f . argspec \n 
~~ return memoized \n 
~~ def make_not_sum_func ( product_func , keep_var ) : \n 
args = get_args ( product_func ) \n 
new_func = copy . deepcopy ( product_func ) \n 
~~~ if arg != keep_var : \n 
~~~ new_func = eliminate_var ( new_func , arg ) \n 
new_func = memoize ( new_func ) \n 
~~ ~~ return new_func \n 
~~ def make_factor_node_message ( node , target_node ) : \n 
if node . is_leaf ( ) : \n 
~~~ not_sum_func = make_not_sum_func ( node . func , target_node . name ) \n 
message = FactorMessage ( node , target_node , [ node . func ] , not_sum_func ) \n 
~~ args = set ( get_args ( node . func ) ) \n 
factors = [ node . func ] \n 
neighbours = node . neighbours \n 
~~~ if neighbour == target_node : \n 
~~ in_message = node . received_messages [ neighbour . name ] \n 
if in_message . destination != node : \n 
~~~ out_message = VariableMessage ( \n 
neighbour , node , in_message . factors , \n 
in_message . func ) \n 
out_message . argspec = in_message . argspec \n 
~~~ out_message = in_message \n 
~~ factors . append ( out_message ) \n 
~~ product_func = make_product_func ( factors ) \n 
not_sum_func = make_not_sum_func ( product_func , target_node . name ) \n 
message = FactorMessage ( node , target_node , factors , not_sum_func ) \n 
~~ def make_variable_node_message ( node , target_node ) : \n 
~~~ message = VariableMessage ( \n 
node , target_node , [ 1 ] , unity ) \n 
~~ factors = [ ] \n 
~~ factors . append ( \n 
node . received_messages [ neighbour . name ] ) \n 
message = VariableMessage ( \n 
node , target_node , factors , product_func ) \n 
~~ def make_product_func ( factors ) : \n 
args_map = { } \n 
all_args = [ ] \n 
domains = { } \n 
for factor in factors : \n 
~~~ args_map [ factor ] = get_args ( factor ) \n 
all_args += args_map [ factor ] \n 
if hasattr ( factor , ) : \n 
~~~ domains . update ( factor . domains ) \n 
~~ ~~ args = list ( set ( all_args ) ) \n 
def product_func ( * product_func_args ) : \n 
~~~ arg_dict = dict ( zip ( args , product_func_args ) ) \n 
result = 1 \n 
#domains.update(factor.domains) \n 
~~~ factor_args = [ ] \n 
for arg in get_args ( factor ) : \n 
~~~ if arg in arg_dict : \n 
~~~ factor_args . append ( arg_dict [ arg ] ) \n 
~~ ~~ if not factor_args : \n 
~~~ factor_args . append ( ) \n 
~~ result *= factor ( * factor_args ) \n 
~~ product_func . argspec = args \n 
product_func . factors = factors \n 
product_func . domains = domains \n 
return memoize ( product_func ) \n 
~~ def make_unity ( argspec ) : \n 
~~~ def unity ( x ) : \n 
~~ unity . argspec = argspec \n 
unity . __name__ = \n 
return unity \n 
~~ def unity ( ) : \n 
~~ def expand_args ( args ) : \n 
~~ def dict_to_tuples ( d ) : \n 
retval = [ ] \n 
for k , vals in d . iteritems ( ) : \n 
~~~ retval . append ( [ ( k , v ) for v in vals ] ) \n 
~~ def expand_parameters ( arg_vals ) : \n 
arg_tuples = dict_to_tuples ( arg_vals ) \n 
return [ dict ( args ) for args in iter_product ( * arg_tuples ) ] \n 
~~ def add_evidence ( node , value ) : \n 
node . value = value \n 
for factor_node in neighbours : \n 
~~~ if node . name in get_args ( factor_node . func ) : \n 
~~~ factor_node . add_evidence ( node , value ) \n 
~~ ~~ ~~ def discover_sample_ordering ( graph ) : \n 
iterations = 0 \n 
ordering = [ ] \n 
pmf_ordering = [ ] \n 
accounted_for = set ( ) \n 
variable_nodes = [ n for n in graph . nodes if isinstance ( n , VariableNode ) ] \n 
factor_nodes = [ n for n in graph . nodes if isinstance ( n , FactorNode ) ] \n 
required = len ( [ n for n in graph . nodes if isinstance ( n , VariableNode ) ] ) \n 
for node in graph . get_leaves ( ) : \n 
~~~ if isinstance ( node , FactorNode ) : \n 
~~~ ordering . append ( node . neighbours [ 0 ] ) \n 
accounted_for . add ( node . neighbours [ 0 ] . name ) \n 
pmf_ordering . append ( node . func ) \n 
~~ ~~ while len ( ordering ) < required : \n 
~~~ for node in factor_nodes : \n 
~~~ args = set ( get_args ( node . func ) ) \n 
new_args = args . difference ( accounted_for ) \n 
if len ( new_args ) == 1 : \n 
~~~ arg_name = list ( new_args ) [ 0 ] \n 
var_node = node . get_neighbour_by_name ( arg_name ) \n 
ordering . append ( var_node ) \n 
accounted_for . add ( var_node . name ) \n 
~~ ~~ ~~ return zip ( ordering , pmf_ordering ) \n 
~~ def get_sample ( ordering , evidence = { } ) : \n 
sample = [ ] \n 
sample_dict = dict ( ) \n 
for var , func in ordering : \n 
~~~ r = random . random ( ) \n 
for val in var . domain : \n 
~~~ test_var = VariableNode ( var . name ) \n 
test_var . value = val \n 
args = [ ] \n 
for arg in get_args ( func ) : \n 
~~~ if arg == var . name : \n 
#args.append(test_var) \n 
~~~ args . append ( val ) \n 
~~~ args . append ( sample_dict [ arg ] . value ) \n 
~~ ~~ total += func ( * args ) \n 
if total > r : \n 
~~~ if var . name in evidence : \n 
~~~ if test_var . value == evidence [ var . name ] : \n 
~~~ sample . append ( test_var ) \n 
sample_dict [ var . name ] = test_var \n 
~~ ~~ if not var . name in sample_dict : \n 
~~~ print % ( var . name , func . __name__ ) \n 
raise InvalidSampleException \n 
~~ ~~ return sample \n 
~~ class FactorGraph ( object ) : \n 
~~~ def __init__ ( self , nodes , name = None , n_samples = 100 ) : \n 
~~~ self . nodes = nodes \n 
self . _inference_method = \n 
function_args = dict ( ) \n 
arg_domains = dict ( ) \n 
for node in self . nodes : \n 
~~~ if isinstance ( node , VariableNode ) : \n 
~~~ arg_domains [ node . name ] = node . domain \n 
~~ elif isinstance ( node , FactorNode ) : \n 
~~~ function_args [ node . func . __name__ ] = get_args ( node . func ) \n 
~~ ~~ for node in self . nodes : \n 
~~~ if hasattr ( node . func , ) : \n 
~~ domains = dict ( ) \n 
for arg in get_args ( node . func ) : \n 
~~~ if not arg in arg_domains : \n 
~~~ print % arg \n 
~~~ domains . update ( { arg : arg_domains [ arg ] } ) \n 
~~ ~~ node . func . domains = domains \n 
~~ ~~ self . name = name \n 
~~~ if self . has_cycles ( ) : \n 
~~~ self . inference_method = \n 
self . inference_method = \n 
~~ self . enforce_minimum_samples = False \n 
def inference_method ( self ) : \n 
~~~ return self . _inference_method \n 
~~ @ inference_method . setter \n 
def inference_method ( self , value ) : \n 
~~~ if value == : \n 
~~~ ensure_data_dir_exists ( self . sample_db_filename ) \n 
sample_ordering = self . discover_sample_ordering ( ) \n 
domains = dict ( [ ( var , var . domain ) for var , _ in sample_ordering ] ) \n 
if not os . path . isfile ( self . sample_db_filename ) : \n 
~~~ self . sample_db = SampleDB ( \n 
self . sample_db_filename , \n 
domains , \n 
initialize = True ) \n 
initialize = False ) \n 
~~ ~~ self . _inference_method = value \n 
def sample_db_filename ( self ) : \n 
home = os . path . expanduser ( ) \n 
return os . path . join ( \n 
home , , \n 
% ( self . name or ) ) \n 
~~~ node . reset ( ) \n 
~~ ~~ def has_cycles ( self ) : \n 
discovered_nodes = set ( ) \n 
traversed_edges = set ( ) \n 
q = Queue ( ) \n 
~~~ if node . is_leaf ( ) : \n 
~~~ start_node = node \n 
~~ ~~ q . put ( start_node ) \n 
while not q . empty ( ) : \n 
~~~ current_node = q . get ( ) \n 
~~ if current_node . name in discovered_nodes : \n 
~~~ print , current_node \n 
~~ discovered_nodes . add ( current_node . name ) \n 
~~ for neighbour in current_node . neighbours : \n 
~~~ edge = [ current_node . name , neighbour . name ] \n 
edge . sort ( ) \n 
edge = tuple ( edge ) \n 
if edge not in traversed_edges : \n 
~~~ if neighbour . name in discovered_nodes : \n 
~~ ~~ if neighbour . name not in discovered_nodes : \n 
~~~ print % neighbour \n 
~~ q . put ( neighbour ) \n 
~~ traversed_edges . add ( edge ) \n 
~~ def verify ( self ) : \n 
~~~ if not isinstance ( node , VariableNode ) and not isinstance ( node , FactorNode ) : \n 
~~~ bases = node . __class__ . __bases__ \n 
if not VariableNode in bases and not FactorNode in bases : \n 
% node . __class__ ) \n 
raise InvalidGraphException \n 
~~ ~~ ~~ print \n 
~~~ if not node . verify_neighbour_types ( ) : \n 
~~~ print % node \n 
return False \n 
~~ ~~ print \n 
~~~ if not hasattr ( node . func , ) : \n 
~~ elif not node . func . domains : \n 
~~ ~~ ~~ print + \n 
variables = set ( [ vn . name for vn in self . nodes \n 
if isinstance ( vn , VariableNode ) ] ) \n 
largs = [ get_args ( fn . func ) for fn in \n 
self . nodes if isinstance ( fn , FactorNode ) ] \n 
args = set ( reduce ( lambda x , y : x + y , largs ) ) \n 
if not variables . issubset ( args ) : \n 
print variables . difference ( args ) \n 
if not args . issubset ( variables ) : \n 
print args . difference ( variables ) \n 
leaf_nodes = filter ( \n 
lambda x : x . is_leaf ( ) , \n 
self . nodes ) \n 
if not leaf_nodes : \n 
~~ def get_leaves ( self ) : \n 
~~~ return [ node for node in self . nodes if node . is_leaf ( ) ] \n 
~~ def get_eligible_senders ( self ) : \n 
eligible = [ ] \n 
~~~ if node . get_target ( ) : \n 
~~~ eligible . append ( node ) \n 
~~ ~~ return eligible \n 
~~ def propagate ( self ) : \n 
step = 1 \n 
~~~ eligible_senders = self . get_eligible_senders ( ) \n 
if not eligible_senders : \n 
~~ for node in eligible_senders : \n 
~~~ message = node . construct_message ( ) \n 
node . send ( message ) \n 
~~ step += 1 \n 
~~ ~~ def variable_nodes ( self ) : \n 
~~~ return [ n for n in self . nodes if isinstance ( n , VariableNode ) ] \n 
~~ def factor_nodes ( self ) : \n 
~~~ return [ n for n in self . nodes if isinstance ( n , FactorNode ) ] \n 
~~ def get_normalizer ( self ) : \n 
~~~ for node in self . variable_nodes ( ) : \n 
~~~ if node . value is not None : \n 
~~~ normalizer = node . marginal ( node . value ) \n 
return normalizer \n 
~~ ~~ return 1 \n 
~~ def status ( self , omit = [ False , 0 ] ) : \n 
~~~ normalizer = self . get_normalizer ( ) \n 
retval = dict ( ) \n 
for node in self . variable_nodes ( ) : \n 
~~~ for value in node . domain : \n 
~~~ m = node . marginal ( value , normalizer ) \n 
retval [ ( node . name , value ) ] = m \n 
~~ ~~ return retval \n 
~~ def query_by_propagation ( self , ** kwds ) : \n 
~~~ self . reset ( ) \n 
for k , v in kwds . items ( ) : \n 
~~~ if node . name == k : \n 
~~~ add_evidence ( node , v ) \n 
~~ ~~ ~~ self . propagate ( ) \n 
return self . status ( ) \n 
~~ def query ( self , ** kwds ) : \n 
~~~ if self . inference_method == : \n 
~~~ return self . query_by_external_samples ( ** kwds ) \n 
~~ elif self . inference_method == : \n 
~~~ return self . query_by_sampling ( ** kwds ) \n 
~~~ return self . query_by_propagation ( ** kwds ) \n 
~~ raise InvalidInferenceMethod \n 
~~ def q ( self , ** kwds ) : \n 
result = self . query ( ** kwds ) \n 
tab = PrettyTable ( [ , , ] , sortby = ) \n 
tab . align = \n 
tab . align [ ] = \n 
tab . float_format = \n 
for ( node , value ) , prob in result . items ( ) : \n 
~~~ if kwds . get ( node , ) == value : \n 
~~~ tab . add_row ( [ % node , \n 
% ( GREEN , value , NORMAL ) , \n 
% prob ] ) \n 
~~~ tab . add_row ( [ node , value , % prob ] ) \n 
~~ ~~ print tab \n 
~~ def discover_sample_ordering ( self ) : \n 
~~~ return discover_sample_ordering ( self ) \n 
~~ def get_sample ( self , evidence = { } ) : \n 
~~~ self . sample_ordering = self . discover_sample_ordering ( ) \n 
~~ return get_sample ( self . sample_ordering , evidence ) \n 
~~ def query_by_sampling ( self , ** kwds ) : \n 
~~~ counts = defaultdict ( int ) \n 
valid_samples = 0 \n 
while valid_samples < self . n_samples : \n 
~~~ sample = self . get_sample ( kwds ) \n 
valid_samples += 1 \n 
~~ for var in sample : \n 
~~~ key = ( var . name , var . value ) \n 
counts [ key ] += 1 \n 
~~ ~~ normalized = dict ( \n 
[ ( k , v / valid_samples ) for k , v in counts . items ( ) ] ) \n 
return normalized \n 
~~ def generate_samples ( self , n ) : \n 
if self . inference_method != : \n 
~~~ raise IncorrectInferenceMethodError ( \n 
% self . inference_method ) \n 
~~ valid_samples = 0 \n 
~~ fn = [ x [ 0 ] . name for x in self . sample_ordering ] \n 
sdb = self . sample_db \n 
while valid_samples < n : \n 
~~~ sample = self . get_sample ( ) \n 
~~ except InvalidSampleException : \n 
~~ sdb . save_sample ( [ ( v . name , v . value ) for v in sample ] ) \n 
~~ sdb . commit ( ) \n 
print % ( n , self . sample_db_filename ) \n 
~~ def query_by_external_samples ( self , ** kwds ) : \n 
samples = self . sample_db . get_samples ( self . n_samples , ** kwds ) \n 
if len ( samples ) == 0 : \n 
~~~ raise NoSamplesInDB ( \n 
~~ if len ( samples ) < self . n_samples and self . enforce_minimum_samples : \n 
~~~ raise InsufficientSamplesException ( \n 
~~ for sample in samples : \n 
~~~ for name , val in sample . items ( ) : \n 
~~~ key = ( name , val ) \n 
[ ( k , v / len ( samples ) ) for k , v in counts . items ( ) ] ) \n 
~~ def export ( self , filename = None , format = ) : \n 
if filename : \n 
~~~ fh = open ( filename , ) \n 
~~~ fh = sys . stdout \n 
~~ if format != : \n 
~~ fh . write ( ) \n 
edges = set ( ) \n 
~~~ for neighbour in node . neighbours : \n 
~~~ edge = [ node . name , neighbour . name ] \n 
edge = tuple ( sorted ( edge ) ) \n 
edges . add ( edge ) \n 
~~ ~~ for source , target in edges : \n 
~~~ fh . write ( % ( source , target ) ) \n 
~~ ~~ def build_graph ( * args , ** kwds ) : \n 
variables = set ( ) \n 
domains = kwds . get ( , { } ) \n 
name = kwds . get ( ) \n 
variable_nodes = dict ( ) \n 
factor_nodes = [ ] \n 
if isinstance ( args [ 0 ] , list ) : \n 
~~~ args = args [ 0 ] \n 
~~ for factor in args : \n 
~~~ factor_args = get_args ( factor ) \n 
variables . update ( factor_args ) \n 
factor_node = FactorNode ( factor . __name__ , factor ) \n 
factor_nodes . append ( factor_node ) \n 
~~ for variable in variables : \n 
~~~ node = VariableNode ( \n 
variable , \n 
domain = domains . get ( variable , [ True , False ] ) ) \n 
variable_nodes [ variable ] = node \n 
~~ for factor_node in factor_nodes : \n 
~~~ factor_args = get_args ( factor_node . func ) \n 
connect ( factor_node , [ variable_nodes [ x ] for x in factor_args ] ) \n 
~~ graph = FactorGraph ( variable_nodes . values ( ) + factor_nodes , name = name ) \n 
return graph \n 
from . import utils \n 
from . import config \n 
from . import attributes \n 
from . metadata import Metadata \n 
from . pos import POSReader \n 
from . srl import SRLReader \n 
from . parse import DependencyReader \n 
from . network import Network , ConvolutionalNetwork , ConvolutionalDependencyNetwork \n 
def load_network ( md ) : \n 
logger = logging . getLogger ( "Logger" ) \n 
is_srl = md . task . startswith ( ) and md . task != \n 
logger . info ( ) \n 
if is_srl : \n 
~~~ net_class = ConvolutionalNetwork \n 
~~ elif md . task . endswith ( ) : \n 
~~~ net_class = ConvolutionalDependencyNetwork \n 
~~~ net_class = Network \n 
~~ nn = net_class . load_from_file ( md . paths [ md . network ] ) \n 
return nn \n 
~~ def create_reader ( md , gold_file = None ) : \n 
if md . task == : \n 
~~~ tr = POSReader ( md , filename = gold_file ) \n 
~~ elif in md . task : \n 
~~~ labeled = md . task . startswith ( ) \n 
tr = DependencyReader ( md , filename = gold_file , labeled = labeled ) \n 
~~ elif md . task . startswith ( ) : \n 
~~~ tr = SRLReader ( md , filename = gold_file , only_boundaries = ( md . task == ) , \n 
only_classify = ( md . task == ) , \n 
only_predicates = ( md . task == ) ) \n 
~~ logger . info ( ) \n 
return tr \n 
~~ def _group_arguments ( tokens , predicate_positions , boundaries , labels ) : \n 
arg_structs = [ ] \n 
for predicate_position , pred_boundaries , pred_labels in izip ( predicate_positions , \n 
boundaries , \n 
labels ) : \n 
~~~ structure = { } \n 
for token , boundary_tag in izip ( tokens , pred_boundaries ) : \n 
~~~ if boundary_tag == : \n 
~~ elif boundary_tag == : \n 
~~~ argument_tokens = [ token ] \n 
~~~ argument_tokens . append ( token ) \n 
tag = pred_labels . pop ( 0 ) \n 
structure [ tag ] = argument_tokens \n 
~~~ tag = pred_labels . pop ( 0 ) \n 
structure [ tag ] = [ token ] \n 
~~ ~~ predicate = tokens [ predicate_position ] \n 
arg_structs . append ( ( predicate , structure ) ) \n 
~~ return arg_structs \n 
~~ class SRLAnnotatedSentence ( object ) : \n 
def __init__ ( self , tokens , arg_structures ) : \n 
self . tokens = tokens \n 
self . arg_structures = arg_structures \n 
~~ ~~ class ParsedSentence ( object ) : \n 
def __init__ ( self , tokens , heads , labels , pos = None ) : \n 
self . heads = heads \n 
self . pos = pos \n 
~~~ return len ( self . tokens ) \n 
~~ def to_conll ( self ) : \n 
for i in range ( len ( self . tokens ) ) : \n 
~~~ token = self . tokens [ i ] \n 
head = self . heads [ i ] + 1 \n 
label = self . labels [ i ] \n 
pos = self . pos [ i ] if self . pos else \n 
line = \n 
result . append ( line . format ( id = i + 1 , pos = pos , head = head , label = label , token = token ) ) \n 
~~ return . join ( result ) \n 
~~ ~~ class Tagger ( object ) : \n 
def __init__ ( self , data_dir = None , language = ) : \n 
~~~ assert config . data_dir is not None , asrt_msg \n 
self . paths = config . FILES \n 
~~~ self . paths = config . get_config_paths ( data_dir ) \n 
~~ self . data_dir = data_dir \n 
self . language = language \n 
self . _load_data ( ) \n 
~~ def _load_data ( self ) : \n 
~~ ~~ class SRLTagger ( Tagger ) : \n 
def _load_data ( self ) : \n 
md_boundary = Metadata . load_from_file ( , self . paths ) \n 
self . boundary_nn = load_network ( md_boundary ) \n 
self . boundary_reader = create_reader ( md_boundary ) \n 
self . boundary_reader . create_converter ( ) \n 
self . boundary_itd = self . boundary_reader . get_inverse_tag_dictionary ( ) \n 
md_classify = Metadata . load_from_file ( , self . paths ) \n 
self . classify_nn = load_network ( md_classify ) \n 
self . classify_reader = create_reader ( md_classify ) \n 
self . classify_reader . create_converter ( ) \n 
self . classify_itd = self . classify_reader . get_inverse_tag_dictionary ( ) \n 
md_pred = Metadata . load_from_file ( , self . paths ) \n 
self . pred_nn = load_network ( md_pred ) \n 
self . pred_reader = create_reader ( md_pred ) \n 
self . pred_reader . create_converter ( ) \n 
~~ def find_predicates ( self , tokens ) : \n 
sent_codified = np . array ( [ self . pred_reader . converter . convert ( token ) \n 
for token in tokens ] ) \n 
answer = np . array ( self . pred_nn . tag_sentence ( sent_codified ) ) \n 
return answer . nonzero ( ) [ 0 ] \n 
~~ def tag ( self , text ) : \n 
tokens = utils . tokenize ( text , self . language ) \n 
for sent in tokens : \n 
~~~ tagged = self . tag_tokens ( sent ) \n 
result . append ( tagged ) \n 
~~ def tag_tokens ( self , tokens , no_repeats = False ) : \n 
if self . language == : \n 
~~~ tokens_obj = [ attributes . Token ( utils . clean_text ( t , False ) ) for t in tokens ] \n 
~~~ tokens_obj = [ attributes . Token ( t ) for t in tokens ] \n 
~~ converted_bound = np . array ( [ self . boundary_reader . converter . convert ( t ) \n 
for t in tokens_obj ] ) \n 
converted_class = np . array ( [ self . classify_reader . converter . convert ( t ) \n 
pred_positions = self . find_predicates ( tokens_obj ) \n 
answers = self . boundary_nn . tag_sentence ( converted_bound , pred_positions ) \n 
boundaries = [ [ self . boundary_itd [ x ] for x in pred_answer ] \n 
for pred_answer in answers ] \n 
arg_limits = [ utils . boundaries_to_arg_limits ( pred_boundaries ) \n 
for pred_boundaries in boundaries ] \n 
answers = self . classify_nn . tag_sentence ( converted_class , \n 
pred_positions , arg_limits , \n 
allow_repeats = not no_repeats ) \n 
arguments = [ [ self . classify_itd [ x ] for x in pred_answer ] \n 
structures = _group_arguments ( tokens , pred_positions , boundaries , arguments ) \n 
return SRLAnnotatedSentence ( tokens , structures ) \n 
~~ ~~ class DependencyParser ( Tagger ) : \n 
super ( DependencyParser , self ) . __init__ ( * args , ** kwargs ) \n 
md_udep = Metadata . load_from_file ( , paths = self . paths ) \n 
self . unlabeled_nn = load_network ( md_udep ) \n 
self . unlabeled_reader = create_reader ( md_udep ) \n 
md_ldep = Metadata . load_from_file ( , paths = self . paths ) \n 
self . labeled_nn = load_network ( md_ldep ) \n 
self . labeled_reader = create_reader ( md_ldep ) \n 
self . itd = self . labeled_reader . get_inverse_tag_dictionary ( ) \n 
self . use_pos = md_udep . use_pos or md_ldep . use_pos \n 
if self . use_pos : \n 
~~~ self . pos_tagger = POSTagger ( self . data_dir , language = self . language ) \n 
~~ ~~ def parse ( self , text ) : \n 
sentences = utils . tokenize ( text , self . language ) \n 
for sent in sentences : \n 
~~~ parsed = self . parse_sentence ( sent ) \n 
result . append ( parsed ) \n 
~~ def tag_tokens ( self , tokens ) : \n 
return self . parse_sentence ( tokens ) \n 
~~ def parse_sentence ( self , tokens ) : \n 
original_tokens = tokens \n 
tokens_obj = [ ] \n 
~~~ tokens = self . pos_tagger . tag_tokens ( tokens , return_tokens = True ) \n 
~~ for token in tokens : \n 
~~~ if self . use_pos : \n 
~~~ word , pos = token \n 
~~~ pos = None \n 
~~ tokens_obj . append ( attributes . Token ( word , pos = pos ) ) \n 
~~ converted_tokens = self . unlabeled_reader . codify_sentence ( tokens_obj ) \n 
heads = self . unlabeled_nn . tag_sentence ( converted_tokens ) \n 
root = heads . argmax ( ) \n 
heads [ root ] = root \n 
converted_tokens = self . labeled_reader . codify_sentence ( tokens_obj ) \n 
label_codes = self . labeled_nn . tag_sentence ( converted_tokens , heads ) \n 
labels = [ self . itd [ code ] for code in label_codes ] \n 
heads [ root ] = - 1 \n 
~~~ pos_tags = zip ( * tokens ) [ 1 ] \n 
~~~ pos_tags = None \n 
~~ parsed = ParsedSentence ( original_tokens , heads , labels , pos_tags ) \n 
return parsed \n 
return self . parse ( text ) \n 
~~ ~~ class POSTagger ( Tagger ) : \n 
md = Metadata . load_from_file ( , self . paths ) \n 
self . nn = load_network ( md ) \n 
self . reader = create_reader ( md ) \n 
self . reader . create_converter ( ) \n 
self . itd = self . reader . get_inverse_tag_dictionary ( ) \n 
~~~ tagged = self . tag_tokens ( sent , return_tokens = True ) \n 
~~ def tag_tokens ( self , tokens , return_tokens = False ) : \n 
converter = self . reader . converter \n 
converted_tokens = np . array ( [ converter . convert ( token ) \n 
answer = self . nn . tag_sentence ( converted_tokens ) \n 
tags = [ self . itd [ tag ] for tag in answer ] \n 
if return_tokens : \n 
~~~ return zip ( tokens , tags ) \n 
~~ return tags \n 
~~ ~~ import cv2 \n 
from webcam_gui import webcam_gui \n 
def imgproc ( frame ) : \n 
~~~ gray = cv2 . cvtColor ( frame , cv2 . COLOR_BGR2GRAY ) \n 
cv2 . imshow ( , gray ) \n 
blur = cv2 . blur ( gray , ( 5 , 5 ) ) \n 
edge = cv2 . Canny ( blur , 30 , 100 ) \n 
edge = cv2 . blur ( edge , ( 2 , 2 ) ) \n 
cv2 . imshow ( , edge ) \n 
thresh1 , thresh = cv2 . threshold ( edge , 60 , 255 , cv2 . THRESH_BINARY ) \n 
cv2 . imshow ( , thresh ) \n 
contours , hry = cv2 . findContours ( thresh , cv2 . RETR_TREE , cv2 . CHAIN_APPROX_SIMPLE ) \n 
cpframe = frame . copy ( ) \n 
cv2 . drawContours ( cpframe , contours , - 1 , ( 0 , 255 , 0 ) , 3 ) \n 
cv2 . imshow ( , cpframe ) \n 
contours = [ ctr for ctr in contours if cv2 . contourArea ( ctr ) > 100 ] \n 
contours = [ cv2 . approxPolyDP ( ctr , 5 , True ) for ctr in contours ] \n 
contours = [ ctr for ctr in contours if cv2 . isContourConvex ( ctr ) ] \n 
cv2 . drawContours ( frame , contours , - 1 , ( 0 , 255 , 0 ) , 3 ) \n 
~~ import cv2 \n 
from time import clock \n 
class OpenCV_Cam ( object ) : \n 
~~~ def __init__ ( self , src = None ) : \n 
~~~ self . start_cam ( src ) \n 
self . __fcount , self . __frate , self . __start = 0 , 0 , clock ( ) \n 
def cam_count ( ) : \n 
~~~ cam_idx = 0 \n 
cap = cv2 . VideoCapture ( cam_idx ) \n 
while cap . read ( ) [ 0 ] : \n 
~~~ cam_idx += 1 \n 
~~ return cam_idx \n 
~~ def start_cam ( self , src = None ) : \n 
~~~ if src is not None : \n 
~~~ self . cam = cv2 . VideoCapture ( src ) \n 
if not self . cam . isOpened ( ) : \n 
~~~ raise ValueError ( + src + ) \n 
~~ idx = 1 \n 
cam1 , cam2 = cv2 . VideoCapture ( 0 ) , cv2 . VideoCapture ( 1 ) \n 
while ( cam2 . read ( ) [ 0 ] ) : \n 
~~~ cam1 . release ( ) \n 
cam1 = cam2 \n 
idx += 1 \n 
cam2 = cv2 . VideoCapture ( idx ) \n 
~~ self . cam = cam1 \n 
~~~ raise Error ( ) \n 
def size ( self ) : \n 
~~~ w = self . cam . get ( cv2 . cv . CV_CAP_PROP_FRAME_WIDTH ) \n 
h = self . cam . get ( cv2 . cv . CV_CAP_PROP_FRAME_HEIGHT ) \n 
return ( int ( w ) , int ( h ) ) \n 
~~ @ size . setter \n 
def size ( self , shape ) : \n 
~~~ self . cam . set ( cv2 . cv . CV_CAP_PROP_FRAME_WIDTH , shape [ 0 ] ) \n 
self . cam . set ( cv2 . cv . CV_CAP_PROP_FRAME_HEIGHT , shape [ 1 ] ) \n 
~~~ self . __fcount += 1 \n 
self . __frame = self . cam . read ( ) [ 1 ] \n 
if self . __fcount == 10 : \n 
~~~ end = clock ( ) \n 
self . __frate = 10 / ( end - self . __start ) \n 
self . __start = clock ( ) \n 
self . __fcount = 0 \n 
~~ return self . __frame \n 
def frame_rate ( self ) : \n 
~~~ return self . __frate \n 
def info ( self ) : \n 
~~~ vars = [ x [ 12 : ] for x in dir ( cv2 . cv ) if in x ] \n 
ret = { } \n 
for p in vars : \n 
~~~ cmd = + p + + p + \n 
exec cmd \n 
~~ return ret \n 
~~ def set ( self , property , value ) : \n 
~~~ cmd = + property + + str ( value ) + \n 
print cmd \n 
~~ def cam_loop ( self , func = lambda x : x , params = ( ) ) : \n 
~~~ input = self . read ( ) \n 
output = func ( input , * params ) \n 
window_name = func . __name__ \n 
if window_name == : window_name = \n 
cv2 . imshow ( window_name , output ) \n 
k = cv2 . waitKey ( 5 ) \n 
if k == 27 : \n 
~~ elif k == ord ( ) : \n 
~~~ info = self . info \n 
for i in info : \n 
~~~ print i , , info [ i ] \n 
~~ ~~ elif k == ord ( ) : \n 
~~~ p = raw_input ( ) \n 
if ( + p ) not in dir ( cv2 . cv ) : \n 
~~~ print p , \n 
print , [ x [ 12 : ] for x in dir ( cv2 . cv ) if in x ~~ else : \n 
~~~ v = raw_input ( ) \n 
self . set ( p , v ) \n 
~~~ print self . frame_rate \n 
~~ ~~ ~~ def __enter__ ( self ) : \n 
~~~ self . cam . release ( ) \n 
cam = OpenCV_Cam ( ) \n 
cam . size = ( 800 , 600 ) \n 
info = cam . info \n 
~~ cam . cam_loop ( ) \n 
import cv2 \n 
from common import anorm , getsize \n 
FLANN_INDEX_LSH = 6 \n 
def init_feature ( name ) : \n 
~~~ chunks = name . split ( ) \n 
if chunks [ 0 ] == : \n 
~~~ detector = cv2 . SIFT ( ) \n 
norm = cv2 . NORM_L2 \n 
~~ elif chunks [ 0 ] == : \n 
~~~ detector = cv2 . SURF ( 800 ) \n 
~~~ detector = cv2 . ORB ( 400 ) \n 
norm = cv2 . NORM_HAMMING \n 
~~ if in chunks : \n 
~~~ if norm == cv2 . NORM_L2 : \n 
~~~ flann_params = dict ( algorithm = FLANN_INDEX_KDTREE , trees = 5 ) \n 
~~~ flann_params = dict ( algorithm = FLANN_INDEX_LSH , \n 
multi_probe_level = 1 ) #2 \n 
~~~ matcher = cv2 . BFMatcher ( norm ) \n 
~~ return detector , matcher \n 
~~ def filter_matches ( kp1 , kp2 , matches , ratio = 0.75 ) : \n 
~~~ mkp1 , mkp2 = [ ] , [ ] \n 
for m in matches : \n 
~~~ if len ( m ) == 2 and m [ 0 ] . distance < m [ 1 ] . distance * ratio : \n 
~~~ m = m [ 0 ] \n 
mkp1 . append ( kp1 [ m . queryIdx ] ) \n 
mkp2 . append ( kp2 [ m . trainIdx ] ) \n 
~~ ~~ p1 = np . float32 ( [ kp . pt for kp in mkp1 ] ) \n 
p2 = np . float32 ( [ kp . pt for kp in mkp2 ] ) \n 
kp_pairs = zip ( mkp1 , mkp2 ) \n 
return p1 , p2 , kp_pairs \n 
~~ def explore_match ( win , img1 , img2 , kp_pairs , status = None , H = None ) : \n 
~~~ h1 , w1 = img1 . shape [ : 2 ] \n 
h2 , w2 = img2 . shape [ : 2 ] \n 
vis = np . zeros ( ( max ( h1 , h2 ) , w1 + w2 ) , np . uint8 ) \n 
vis [ : h1 , : w1 ] = img1 \n 
vis [ : h2 , w1 : w1 + w2 ] = img2 \n 
vis = cv2 . cvtColor ( vis , cv2 . COLOR_GRAY2BGR ) \n 
if H is not None : \n 
~~~ corners = np . float32 ( [ [ 0 , 0 ] , [ w1 , 0 ] , [ w1 , h1 ] , [ 0 , h1 ] ] ) \n 
corners = np . int32 ( cv2 . perspectiveTransform ( corners . reshape ( 1 , - 1 , 2 ) , H ) . reshape ( - 1 , 2 ) + cv2 . polylines ( vis , [ corners ] , True , ( 255 , 255 , 255 ) ) \n 
~~ if status is None : \n 
~~~ status = np . ones ( len ( kp_pairs ) , np . bool_ ) \n 
~~ p1 = np . int32 ( [ kpp [ 0 ] . pt for kpp in kp_pairs ] ) \n 
p2 = np . int32 ( [ kpp [ 1 ] . pt for kpp in kp_pairs ] ) + ( w1 , 0 ) \n 
green = ( 0 , 255 , 0 ) \n 
red = ( 0 , 0 , 255 ) \n 
white = ( 255 , 255 , 255 ) \n 
kp_color = ( 51 , 103 , 236 ) \n 
for ( x1 , y1 ) , ( x2 , y2 ) , inlier in zip ( p1 , p2 , status ) : \n 
~~~ if inlier : \n 
~~~ col = green \n 
cv2 . circle ( vis , ( x1 , y1 ) , 2 , col , - 1 ) \n 
cv2 . circle ( vis , ( x2 , y2 ) , 2 , col , - 1 ) \n 
~~~ col = red \n 
r = 2 \n 
thickness = 3 \n 
cv2 . line ( vis , ( x1 - r , y1 - r ) , ( x1 + r , y1 + r ) , col , thickness ) \n 
cv2 . line ( vis , ( x1 - r , y1 + r ) , ( x1 + r , y1 - r ) , col , thickness ) \n 
cv2 . line ( vis , ( x2 - r , y2 - r ) , ( x2 + r , y2 + r ) , col , thickness ) \n 
cv2 . line ( vis , ( x2 - r , y2 + r ) , ( x2 + r , y2 - r ) , col , thickness ) \n 
~~ ~~ vis0 = vis . copy ( ) \n 
~~~ cv2 . line ( vis , ( x1 , y1 ) , ( x2 , y2 ) , green ) \n 
~~ ~~ cv2 . imshow ( win , vis ) \n 
def onmouse ( event , x , y , flags , param ) : \n 
~~~ cur_vis = vis \n 
if flags & cv2 . EVENT_FLAG_LBUTTON : \n 
~~~ cur_vis = vis0 . copy ( ) \n 
r = 8 \n 
m = ( anorm ( p1 - ( x , y ) ) < r ) | ( anorm ( p2 - ( x , y ) ) < r ) \n 
idxs = np . where ( m ) [ 0 ] \n 
kp1s , kp2s = [ ] , [ ] \n 
for i in idxs : \n 
~~~ ( x1 , y1 ) , ( x2 , y2 ) = p1 [ i ] , p2 [ i ] \n 
col = ( red , green ) [ status [ i ] ] \n 
cv2 . line ( cur_vis , ( x1 , y1 ) , ( x2 , y2 ) , col ) \n 
kp1 , kp2 = kp_pairs [ i ] \n 
kp1s . append ( kp1 ) \n 
kp2s . append ( kp2 ) \n 
~~ cur_vis = cv2 . drawKeypoints ( cur_vis , kp1s , flags = 4 , color = kp_color ) \n 
cur_vis [ : , w1 : ] = cv2 . drawKeypoints ( cur_vis [ : , w1 : ] , kp2s , flags = 4 , color = kp_color ) \n 
~~ cv2 . imshow ( win , cur_vis ) \n 
~~ cv2 . setMouseCallback ( win , onmouse ) \n 
return vis \n 
import sys , getopt \n 
opts , args = getopt . getopt ( sys . argv [ 1 : ] , , [ ] ) \n 
opts = dict ( opts ) \n 
feature_name = opts . get ( , ) \n 
try : fn1 , fn2 = args \n 
except : \n 
~~~ fn1 = \n 
fn2 = \n 
~~ img1 = cv2 . imread ( fn1 , 0 ) \n 
img2 = cv2 . imread ( fn2 , 0 ) \n 
detector , matcher = init_feature ( feature_name ) \n 
if detector != None : \n 
~~~ print , feature_name \n 
~~ kp1 , desc1 = detector . detectAndCompute ( img1 , None ) \n 
kp2 , desc2 = detector . detectAndCompute ( img2 , None ) \n 
print % ( len ( kp1 ) , len ( kp2 ) ) \n 
def match_and_draw ( win ) : \n 
raw_matches = matcher . knnMatch ( desc1 , trainDescriptors = desc2 , k = 2 ) #2 \n 
p1 , p2 , kp_pairs = filter_matches ( kp1 , kp2 , raw_matches ) \n 
if len ( p1 ) >= 4 : \n 
~~~ H , status = cv2 . findHomography ( p1 , p2 , cv2 . RANSAC , 5.0 ) \n 
print % ( np . sum ( status ) , len ( status ) ) \n 
~~~ H , status = None , None \n 
print % len ( p1 ) \n 
~~ vis = explore_match ( win , img1 , img2 , kp_pairs , status , H ) \n 
~~ match_and_draw ( ) \n 
cv2 . waitKey ( ) \n 
cv2 . destroyAllWindows ( ) \n 
def data_matrix_demo ( cap ) : \n 
frame_number = 0 \n 
need_to_save = False \n 
~~~ ret , frame = cap . read ( ) \n 
if not ret : \n 
~~ gray = cv2 . cvtColor ( frame , cv2 . COLOR_RGB2GRAY ) \n 
codes , corners , dmtx = cv2 . findDataMatrix ( gray ) \n 
cv2 . drawDataMatrixCodes ( frame , codes , corners ) \n 
cv2 . imshow ( window_name , frame ) \n 
key = cv2 . waitKey ( 30 ) \n 
c = chr ( key & 255 ) \n 
if c in [ , , chr ( 27 ) ] : \n 
~~ if c == : \n 
~~~ need_to_save = True \n 
~~ if need_to_save and codes : \n 
~~~ filename = ( "datamatrix%03d.jpg" % frame_number ) \n 
cv2 . imwrite ( filename , frame ) \n 
~~ frame_number += 1 \n 
if len ( sys . argv ) == 1 : \n 
~~~ cap = cv2 . VideoCapture ( 0 ) \n 
~~~ cap = cv2 . VideoCapture ( sys . argv [ 1 ] ) \n 
if not cap . isOpened ( ) : \n 
~~~ cap = cv2 . VideoCapture ( int ( sys . argv [ 1 ] ) ) \n 
~~ ~~ if not cap . isOpened ( ) : \n 
sys . exit ( - 1 ) \n 
~~ data_matrix_demo ( cap ) \n 
~~~ fn = sys . argv [ 1 ] \n 
~~~ fn = "../data/board.jpg" \n 
~~ src = cv2 . imread ( fn , 1 ) \n 
img = cv2 . cvtColor ( src , cv2 . COLOR_BGR2GRAY ) \n 
img = cv2 . medianBlur ( img , 5 ) \n 
circles = cv2 . HoughCircles ( img , cv2 . HOUGH_GRADIENT , 1 , 10 , np . array ( [ ] ) , 100 , 30 , 1 , 30 ) \n 
a , b , c = circles . shape \n 
for i in range ( b ) : \n 
~~ cv2 . imshow ( "source" , src ) \n 
cv2 . waitKey ( 0 ) \n 
import cv2 . cv as cv \n 
_MAX_POINTS = 100 \n 
~~~ my_random = random . Random ( ) \n 
image = cv . CreateImage ( ( 500 , 500 ) , 8 , 3 ) \n 
cv . NamedWindow ( , cv . CV_WINDOW_AUTOSIZE ) \n 
~~~ count = my_random . randrange ( 0 , _MAX_POINTS ) + 1 \n 
points = [ ] \n 
~~~ points . append ( ( \n 
my_random . randrange ( 0 , image . width / 2 ) + image . width / 4 , \n 
my_random . randrange ( 0 , image . width / 2 ) + image . width / 4 \n 
~~ storage = cv . CreateMemStorage ( 0 ) \n 
hull = cv . ConvexHull2 ( points , storage , cv . CV_CLOCKWISE , 1 ) \n 
cv . SetZero ( image ) \n 
~~~ cv . Circle ( image , points [ i ] , 2 , \n 
( 0 , 0 , 255 , 0 ) , \n 
cv . CV_FILLED , cv . CV_AA , 0 ) \n 
~~ cv . PolyLine ( image , [ hull ] , 1 , cv . RGB ( 0 , 255 , 0 ) , 1 , cv . CV_AA ) \n 
cv . ShowImage ( , image ) \n 
k = cv . WaitKey ( 0 ) % 0x100 \n 
~~ ~~ cv . DestroyAllWindows ( ) \n 
~~ import cv2 . cv as cv \n 
class PyrSegmentation : \n 
~~~ def __init__ ( self , img0 ) : \n 
~~~ self . thresh1 = 255 \n 
self . thresh2 = 30 \n 
self . level = 4 \n 
self . storage = cv . CreateMemStorage ( ) \n 
cv . NamedWindow ( "Source" , 0 ) \n 
cv . ShowImage ( "Source" , img0 ) \n 
cv . NamedWindow ( "Segmentation" , 0 ) \n 
cv . CreateTrackbar ( "Thresh1" , "Segmentation" , self . thresh1 , 255 , self . set_thresh1 ) \n 
cv . CreateTrackbar ( "Thresh2" , "Segmentation" , self . thresh2 , 255 , self . set_thresh2 ) \n 
self . image0 = cv . CloneImage ( img0 ) \n 
self . image1 = cv . CloneImage ( img0 ) \n 
cv . ShowImage ( "Segmentation" , self . image1 ) \n 
~~ def set_thresh1 ( self , val ) : \n 
~~~ self . thresh1 = val \n 
self . on_segment ( ) \n 
~~ def set_thresh2 ( self , val ) : \n 
~~~ self . thresh2 = val \n 
~~ def on_segment ( self ) : \n 
~~~ comp = cv . PyrSegmentation ( self . image0 , self . image1 , self . storage , self . level , self . thresh1 + 1 , self . thresh2 + 1 ) \n 
~~~ self . on_segment ( ) \n 
cv . WaitKey ( 0 ) \n 
~~~ img0 = cv . LoadImage ( "../c/fruits.jpg" , 1 ) \n 
PyrSegmentation ( img0 ) . run ( ) \n 
cv . DestroyAllWindows ( ) \n 
from sklearn . gaussian_process import GaussianProcess \n 
from . helpers import UtilityFunction , unique_rows , PrintLog \n 
def acq_max ( ac , gp , y_max , bounds ) : \n 
x_max = bounds [ : , 0 ] \n 
max_acq = None \n 
x_tries = np . random . uniform ( bounds [ : , 0 ] , bounds [ : , 1 ] , \n 
size = ( 100 , bounds . shape [ 0 ] ) ) \n 
for x_try in x_tries : \n 
~~~ res = minimize ( lambda x : - ac ( x . reshape ( 1 , - 1 ) , gp = gp , y_max = y_max ) , \n 
x_try . reshape ( 1 , - 1 ) , \n 
bounds = bounds , \n 
method = "L-BFGS-B" ) \n 
if max_acq is None or - res . fun >= max_acq : \n 
~~~ x_max = res . x \n 
max_acq = - res . fun \n 
~~ ~~ return np . clip ( x_max , bounds [ : , 0 ] , bounds [ : , 1 ] ) \n 
~~ class BayesianOptimization ( object ) : \n 
~~~ def __init__ ( self , f , pbounds , verbose = 1 ) : \n 
self . pbounds = pbounds \n 
self . keys = list ( pbounds . keys ( ) ) \n 
self . dim = len ( pbounds ) \n 
self . bounds = [ ] \n 
for key in self . pbounds . keys ( ) : \n 
~~~ self . bounds . append ( self . pbounds [ key ] ) \n 
~~ self . bounds = np . asarray ( self . bounds ) \n 
self . f = f \n 
self . initialized = False \n 
self . init_points = [ ] \n 
self . x_init = [ ] \n 
self . y_init = [ ] \n 
self . X = None \n 
self . Y = None \n 
self . i = 0 \n 
self . gp = GaussianProcess ( theta0 = np . random . uniform ( 0.001 , 0.05 , self . dim ) , \n 
thetaL = 1e-5 * np . ones ( self . dim ) , \n 
thetaU = 1e0 * np . ones ( self . dim ) , \n 
random_start = 30 ) \n 
self . util = None \n 
self . plog = PrintLog ( self . keys ) \n 
self . res = { } \n 
self . res [ ] = { : None , \n 
: None } \n 
self . res [ ] = { : [ ] , : [ ] } \n 
~~ def init ( self , init_points ) : \n 
l = [ np . random . uniform ( x [ 0 ] , x [ 1 ] , size = init_points ) for x in self . bounds ] \n 
self . init_points += list ( map ( list , zip ( * l ) ) ) \n 
y_init = [ ] \n 
for x in self . init_points : \n 
~~~ y_init . append ( self . f ( ** dict ( zip ( self . keys , x ) ) ) ) \n 
if self . verbose : \n 
~~~ self . plog . print_step ( x , y_init [ - 1 ] ) \n 
~~ ~~ self . init_points += self . x_init \n 
y_init += self . y_init \n 
self . X = np . asarray ( self . init_points ) \n 
self . Y = np . asarray ( y_init ) \n 
self . initialized = True \n 
~~ def explore ( self , points_dict ) : \n 
param_tup_lens = [ ] \n 
for key in self . keys : \n 
~~~ param_tup_lens . append ( len ( list ( points_dict [ key ] ) ) ) \n 
~~ if all ( [ e == param_tup_lens [ 0 ] for e in param_tup_lens ] ) : \n 
~~ all_points = [ ] \n 
~~~ all_points . append ( points_dict [ key ] ) \n 
~~ self . init_points = list ( map ( list , zip ( * all_points ) ) ) \n 
~~ def initialize ( self , points_dict ) : \n 
for target in points_dict : \n 
~~~ self . y_init . append ( target ) \n 
all_points = [ ] \n 
~~~ all_points . append ( points_dict [ target ] [ key ] ) \n 
~~ self . x_init . append ( all_points ) \n 
~~ ~~ def set_bounds ( self , new_bounds ) : \n 
self . pbounds . update ( new_bounds ) \n 
for row , key in enumerate ( self . pbounds . keys ( ) ) : \n 
~~~ self . bounds [ row ] = self . pbounds [ key ] \n 
~~ ~~ def maximize ( self , \n 
init_points = 5 , \n 
n_iter = 25 , \n 
acq = , \n 
kappa = 2.576 , \n 
xi = 0.0 , \n 
** gp_params ) : \n 
self . plog . reset_timer ( ) \n 
self . util = UtilityFunction ( kind = acq , kappa = kappa , xi = xi ) \n 
if not self . initialized : \n 
~~~ self . plog . print_header ( ) \n 
~~ self . init ( init_points ) \n 
~~ y_max = self . Y . max ( ) \n 
self . gp . set_params ( ** gp_params ) \n 
ur = unique_rows ( self . X ) \n 
self . gp . fit ( self . X [ ur ] , self . Y [ ur ] ) \n 
x_max = acq_max ( ac = self . util . utility , \n 
gp = self . gp , \n 
y_max = y_max , \n 
bounds = self . bounds ) \n 
~~~ self . plog . print_header ( initialization = False ) \n 
~~ for i in range ( n_iter ) : \n 
~~~ pwarning = False \n 
if np . any ( ( self . X - x_max ) . sum ( axis = 1 ) == 0 ) : \n 
~~~ x_max = np . random . uniform ( self . bounds [ : , 0 ] , \n 
self . bounds [ : , 1 ] , \n 
size = self . bounds . shape [ 0 ] ) \n 
pwarning = True \n 
~~ self . X = np . vstack ( ( self . X , x_max . reshape ( ( 1 , - 1 ) ) ) ) \n 
self . Y = np . append ( self . Y , self . f ( ** dict ( zip ( self . keys , x_max ) ) ) ) \n 
if self . Y [ - 1 ] > y_max : \n 
~~~ y_max = self . Y [ - 1 ] \n 
~~ x_max = acq_max ( ac = self . util . utility , \n 
~~~ self . plog . print_step ( self . X [ - 1 ] , self . Y [ - 1 ] , warning = pwarning ) \n 
~~ self . i += 1 \n 
self . res [ ] = { : self . Y . max ( ) , \n 
: dict ( zip ( self . keys , \n 
self . X [ self . Y . argmax ( ) ] ) ) \n 
self . res [ ] [ ] . append ( self . Y [ - 1 ] ) \n 
self . res [ ] [ ] . append ( dict ( zip ( self . keys , self . X [ - 1 ] ) ) ) \n 
~~ if self . verbose : \n 
~~~ self . plog . print_summary ( ) \n 
#!/bin/bash/python \n 
import fileErrors as errors \n 
import stringOperations \n 
class fileHandling : \n 
~~~ def __init__ ( self , toolsPath , pipelinesPath , definedPath ) : \n 
~~~ self . errors = errors . fileErrors ( ) \n 
self . userConfigurationFiles = self . getValidJsonFiles ( os . listdir ( definedPath ) ) if definedPath else \n 
self . pipelines = self . getValidJsonFiles ( os . listdir ( pipelinesPath ) ) \n 
self . tools = self . getValidJsonFiles ( os . listdir ( toolsPath ) ) \n 
~~ def checkPipeline ( self , toolsPath , pipelinesPath , definedPath , pipeline ) : \n 
elif pipeline in self . pipelines : return pipelinesPath + + pipeline + \n 
else : \n 
~~~ rankedPipelines = stringOperations . rankListByString ( self . pipelines , pipeline ) \n 
self . errors . invalidPipelineName ( rankedPipelines , pipeline ) \n 
~~ ~~ def checkFileExistence ( self , fileList , resourcesPath , toolsPath ) : \n 
~~~ missingFiles = [ ] \n 
for filename in fileList : \n 
~~~ if filename . startswith ( ) : updatedFilename = filename . replace ( , ) \n 
elif filename . startswith ( ) : updatedFilename = filename . replace ( , resourcesPath elif filename . startswith ( ) : updatedFilename = filename . replace ( , toolsPath else : updatedFilename = filename \n 
if not os . path . exists ( updatedFilename ) : missingFiles . append ( filename ) \n 
~~ if missingFiles : \n 
~~~ self . errors . missingFiles ( missingFiles ) \n 
~~ else : return True \n 
~~ def getRandomString ( self , pipeline ) : \n 
~~~ files = [ ] \n 
for filename in os . listdir ( "./" ) : \n 
~~~ if filename . startswith ( pipeline ) and filename . endswith ( ) : files . append ( filename ) \n 
~~ if len ( files ) != 1 : return False \n 
randomString = files [ 0 ] . replace ( pipeline + , ) \n 
return randomString . replace ( , ) \n 
###################### \n 
def getValidJsonFiles ( files ) : \n 
~~~ validFiles = [ ] \n 
for filename in files : \n 
~~~ if filename . endswith ( ) and not filename . endswith ( ) : validFiles . append \n 
~~ return validFiles \n 
def openFileForReading ( filename ) : \n 
~~~ try : return open ( filename ) \n 
except : return False \n 
def readConfigurationFile ( filename , allowTermination = True ) : \n 
~~~ try : jsonData = open ( filename ) \n 
~~~ if allowTermination : errors . fileErrors ( ) . noFile ( filename ) \n 
else : return False \n 
~~ try : data = json . load ( jsonData ) \n 
~~~ if allowTermination : errors . fileErrors ( ) . notJson ( filename , sys . exc_info ) \n 
def openFileForWriting ( filename ) : \n 
~~~ return open ( filename , ) \n 
def closeFile ( filehandle ) : \n 
~~~ filehandle . close ( ) \n 
__all__ = [ "min_weighted_dominating_set" , \n 
"min_edge_dominating_set" ] \n 
def min_weighted_dominating_set ( G , weight = None ) : \n 
if not G : \n 
~~ dom_set = set ( [ ] ) \n 
cost_func = dict ( ( node , nd . get ( weight , 1 ) ) for node , nd in G . nodes_iter ( data = True ) ) \n 
vertices = set ( G ) \n 
sets = dict ( ( node , set ( [ node ] ) | set ( G [ node ] ) ) for node in G ) \n 
def _cost ( subset ) : \n 
cost = sum ( cost_func [ node ] for node in subset ) \n 
return cost / float ( len ( subset - dom_set ) ) \n 
~~ while vertices : \n 
~~~ dom_node , min_set = min ( sets . items ( ) , \n 
key = lambda x : ( x [ 0 ] , _cost ( x [ 1 ] ) ) ) \n 
alpha = _cost ( min_set ) \n 
for node in min_set - dom_set : \n 
~~~ cost_func [ node ] = alpha \n 
~~ dom_set . add ( dom_node ) \n 
del sets [ dom_node ] \n 
vertices = vertices - min_set \n 
~~ return dom_set \n 
~~ def min_edge_dominating_set ( G ) : \n 
~~ return nx . maximal_matching ( G ) \n 
~~ from nose . tools import * \n 
import networkx \n 
from nose . plugins . attrib import attr \n 
from networkx import edge_current_flow_betweenness_centrality as edge_current_flow \n 
from networkx import edge_current_flow_betweenness_centrality_subset as edge_current_flow_subset \n 
class TestFlowBetweennessCentrality ( object ) : \n 
~~~ numpy = 1 \n 
def setupClass ( cls ) : \n 
~~~ global np \n 
~~~ import numpy as np \n 
~~ ~~ def test_K4_normalized ( self ) : \n 
G = networkx . complete_graph ( 4 ) \n 
b = networkx . current_flow_betweenness_centrality_subset ( G , \n 
G . nodes ( ) , \n 
normalized = True ) \n 
b_answer = networkx . current_flow_betweenness_centrality ( G , normalized = True ) \n 
for n in sorted ( G ) : \n 
~~~ assert_almost_equal ( b [ n ] , b_answer [ n ] ) \n 
~~ ~~ def test_K4 ( self ) : \n 
~~ G . add_edge ( 0 , 1 , { : 0.5 , : 0.3 } ) \n 
normalized = True , \n 
weight = None ) \n 
~~ b = networkx . current_flow_betweenness_centrality_subset ( G , \n 
weight = ) \n 
b_answer = networkx . current_flow_betweenness_centrality ( G , normalized = True , weight = ) \n 
~~ ~~ def test_P4_normalized ( self ) : \n 
G = networkx . path_graph ( 4 ) \n 
~~ ~~ def test_P4 ( self ) : \n 
~~ ~~ def test_star ( self ) : \n 
G = networkx . Graph ( ) \n 
G . add_star ( [ , , , ] ) \n 
~~ ~~ ~~ class TestEdgeFlowBetweennessCentrality ( object ) : \n 
b = edge_current_flow_subset ( G , G . nodes ( ) , G . nodes ( ) , normalized = True ) \n 
b_answer = edge_current_flow ( G , normalized = True ) \n 
for ( s , t ) , v1 in b_answer . items ( ) : \n 
~~~ v2 = b . get ( ( s , t ) , b . get ( ( t , s ) ) ) \n 
assert_almost_equal ( v1 , v2 ) \n 
b = edge_current_flow_subset ( G , G . nodes ( ) , G . nodes ( ) , normalized = False ) \n 
b_answer = edge_current_flow ( G , normalized = False ) \n 
b = edge_current_flow_subset ( G , G . nodes ( ) , G . nodes ( ) , normalized = False , weight = None ) \n 
~~ b = edge_current_flow_subset ( G , G . nodes ( ) , G . nodes ( ) , normalized = False ) \n 
~~ b = edge_current_flow_subset ( G , G . nodes ( ) , G . nodes ( ) , normalized = False , weight = ) \n 
b_answer = edge_current_flow ( G , normalized = False , weight = ) \n 
~~ ~~ def test_C4 ( self ) : \n 
G = networkx . cycle_graph ( 4 ) \n 
~~ ~~ ~~ from nose . tools import assert_equal , assert_true , assert_false \n 
def _generate_no_biconnected ( max_attempts = 50 ) : \n 
~~~ attempts = 0 \n 
~~~ G = nx . fast_gnp_random_graph ( 100 , 0.0575 ) \n 
if nx . is_connected ( G ) and not nx . is_biconnected ( G ) : \n 
yield G \n 
~~~ if attempts >= max_attempts : \n 
raise Exception ( msg % max_attempts ) \n 
~~~ attempts += 1 \n 
~~ ~~ ~~ ~~ def is_dominating_set ( G , nbunch ) : \n 
~~~ allnodes = set ( G ) \n 
testset = set ( n for n in nbunch if n in G ) \n 
nbrs = set ( ) \n 
for n in testset : \n 
~~~ nbrs . update ( G [ n ] ) \n 
~~ ~~ def test_average_connectivity ( ) : \n 
~~~ G1 = nx . path_graph ( 3 ) \n 
G1 . add_edges_from ( [ ( 1 , 3 ) , ( 1 , 4 ) ] ) \n 
assert_equal ( nx . average_node_connectivity ( G1 ) , 1 ) \n 
G2 = nx . path_graph ( 3 ) \n 
G2 . add_edges_from ( [ ( 1 , 3 ) , ( 1 , 4 ) , ( 0 , 3 ) , ( 0 , 4 ) , ( 3 , 4 ) ] ) \n 
assert_equal ( nx . average_node_connectivity ( G2 ) , 2.2 ) \n 
G3 = nx . Graph ( ) \n 
assert_equal ( nx . average_node_connectivity ( G3 ) , 0 ) \n 
~~ def test_articulation_points ( ) : \n 
~~~ Ggen = _generate_no_biconnected ( ) \n 
~~~ G = next ( Ggen ) \n 
assert_equal ( nx . node_connectivity ( G ) , 1 ) \n 
~~ ~~ def test_brandes_erlebach ( ) : \n 
~~~ G = nx . Graph ( ) \n 
G . add_edges_from ( [ ( 1 , 2 ) , ( 1 , 3 ) , ( 1 , 4 ) , ( 1 , 5 ) , ( 2 , 3 ) , ( 2 , 6 ) , ( 3 , 4 ) , \n 
( 3 , 6 ) , ( 4 , 6 ) , ( 4 , 7 ) , ( 5 , 7 ) , ( 6 , 8 ) , ( 6 , 9 ) , ( 7 , 8 ) , \n 
( 7 , 10 ) , ( 8 , 11 ) , ( 9 , 10 ) , ( 9 , 11 ) , ( 10 , 11 ) ] ) \n 
assert_equal ( 3 , nx . local_edge_connectivity ( G , 1 , 11 ) ) \n 
assert_equal ( 3 , nx . edge_connectivity ( G , 1 , 11 ) ) \n 
assert_equal ( 2 , nx . local_node_connectivity ( G , 1 , 11 ) ) \n 
assert_equal ( 2 , nx . node_connectivity ( G , 1 , 11 ) ) \n 
assert_equal ( 2 , nx . node_connectivity ( G ) ) \n 
~~ def test_white_harary_1 ( ) : \n 
~~~ G = nx . disjoint_union ( nx . complete_graph ( 4 ) , nx . complete_graph ( 4 ) ) \n 
G . remove_node ( 7 ) \n 
for i in range ( 4 , 7 ) : \n 
~~~ G . add_edge ( 0 , i ) \n 
~~ G = nx . disjoint_union ( G , nx . complete_graph ( 4 ) ) \n 
G . remove_node ( G . order ( ) - 1 ) \n 
for i in range ( 7 , 10 ) : \n 
~~ assert_equal ( 1 , nx . node_connectivity ( G ) ) \n 
assert_equal ( 3 , nx . edge_connectivity ( G ) ) \n 
~~ def test_white_harary_2 ( ) : \n 
G . add_edge ( 0 , 4 ) \n 
assert_equal ( 3 , min ( nx . core_number ( G ) . values ( ) ) ) \n 
assert_equal ( 1 , nx . node_connectivity ( G ) ) \n 
assert_equal ( 1 , nx . edge_connectivity ( G ) ) \n 
~~ def test_complete_graphs ( ) : \n 
~~~ for n in range ( 5 , 25 , 5 ) : \n 
~~~ G = nx . complete_graph ( n ) \n 
assert_equal ( n - 1 , nx . node_connectivity ( G ) ) \n 
assert_equal ( n - 1 , nx . node_connectivity ( G . to_directed ( ) ) ) \n 
assert_equal ( n - 1 , nx . edge_connectivity ( G ) ) \n 
assert_equal ( n - 1 , nx . edge_connectivity ( G . to_directed ( ) ) ) \n 
~~ ~~ def test_empty_graphs ( ) : \n 
~~~ for k in range ( 5 , 25 , 5 ) : \n 
~~~ G = nx . empty_graph ( k ) \n 
assert_equal ( 0 , nx . node_connectivity ( G ) ) \n 
assert_equal ( 0 , nx . edge_connectivity ( G ) ) \n 
~~ ~~ def test_petersen ( ) : \n 
~~~ G = nx . petersen_graph ( ) \n 
assert_equal ( 3 , nx . node_connectivity ( G ) ) \n 
~~ def test_tutte ( ) : \n 
~~~ G = nx . tutte_graph ( ) \n 
~~ def test_dodecahedral ( ) : \n 
~~~ G = nx . dodecahedral_graph ( ) \n 
~~ def test_octahedral ( ) : \n 
~~~ G = nx . octahedral_graph ( ) \n 
assert_equal ( 4 , nx . node_connectivity ( G ) ) \n 
assert_equal ( 4 , nx . edge_connectivity ( G ) ) \n 
~~ def test_icosahedral ( ) : \n 
~~~ G = nx . icosahedral_graph ( ) \n 
assert_equal ( 5 , nx . node_connectivity ( G ) ) \n 
assert_equal ( 5 , nx . edge_connectivity ( G ) ) \n 
~~ def test_directed_edge_connectivity ( ) : \n 
assert_equal ( 1 , nx . local_edge_connectivity ( G , 1 , 4 ) ) \n 
assert_equal ( 1 , nx . edge_connectivity ( G , 1 , 4 ) ) \n 
assert_equal ( 2 , nx . edge_connectivity ( D ) ) \n 
assert_equal ( 2 , nx . local_edge_connectivity ( D , 1 , 4 ) ) \n 
assert_equal ( 2 , nx . edge_connectivity ( D , 1 , 4 ) ) \n 
~~ def test_dominating_set ( ) : \n 
~~~ for i in range ( 5 ) : \n 
~~~ G = nx . gnp_random_graph ( 100 , 0.1 ) \n 
D = nx . dominating_set ( G ) \n 
assert_true ( is_dominating_set ( G , D ) ) \n 
from networkx . exception import NetworkXError \n 
__all__ = [ , , , ] \n 
def pagerank ( G , alpha = 0.85 , personalization = None , \n 
max_iter = 100 , tol = 1.0e-8 , nstart = None , weight = ) : \n 
if type ( G ) == nx . MultiGraph or type ( G ) == nx . MultiDiGraph : \n 
~~ if len ( G ) == 0 : \n 
~~ if not G . is_directed ( ) : \n 
~~~ D = G . to_directed ( ) \n 
~~~ D = G \n 
~~ W = nx . stochastic_graph ( D , weight = weight ) \n 
scale = 1.0 / W . number_of_nodes ( ) \n 
if nstart is None : \n 
~~~ x = dict . fromkeys ( W , scale ) \n 
~~~ x = nstart \n 
s = 1.0 / sum ( x . values ( ) ) \n 
for k in x : x [ k ] *= s \n 
~~ if personalization is None : \n 
~~~ p = dict . fromkeys ( W , scale ) \n 
~~~ p = personalization \n 
s = 1.0 / sum ( p . values ( ) ) \n 
for k in p : \n 
~~~ p [ k ] *= s \n 
~~ if set ( p ) != set ( G ) : \n 
~~~ raise NetworkXError ( \n 
~~ ~~ out_degree = W . out_degree ( ) \n 
dangle = [ n for n in W if out_degree [ n ] == 0.0 ] \n 
~~~ xlast = x \n 
x = dict . fromkeys ( xlast . keys ( ) , 0 ) \n 
danglesum = alpha * scale * sum ( xlast [ n ] for n in dangle ) \n 
for n in x : \n 
~~~ for nbr in W [ n ] : \n 
~~~ x [ nbr ] += alpha * xlast [ n ] * W [ n ] [ nbr ] [ weight ] \n 
~~ x [ n ] += danglesum + ( 1.0 - alpha ) * p [ n ] \n 
~~ s = 1.0 / sum ( x . values ( ) ) \n 
~~~ x [ n ] *= s \n 
~~ err = sum ( [ abs ( x [ n ] - xlast [ n ] ) for n in x ] ) \n 
if err < tol : \n 
~~ if i > max_iter : \n 
% ( i - 1 ) ) \n 
~~ return x \n 
~~ def google_matrix ( G , alpha = 0.85 , personalization = None , \n 
nodelist = None , weight = ) : \n 
~~~ nodelist = G . nodes ( ) \n 
~~~ nodelist = personalization . keys ( ) \n 
if set ( nodelist ) != set ( G ) : \n 
~~ ~~ M = nx . to_numpy_matrix ( G , nodelist = nodelist , weight = weight ) \n 
if n == 0 : \n 
~~~ return M \n 
~~ dangling = np . where ( M . sum ( axis = 1 ) == 0 ) \n 
for d in dangling [ 0 ] : \n 
~~~ M [ d ] = 1.0 / n \n 
~~ M = M / M . sum ( axis = 1 ) \n 
e = np . ones ( ( n ) ) \n 
if personalization is not None : \n 
~~~ v = np . array ( list ( personalization . values ( ) ) , dtype = float ) \n 
~~~ v = e \n 
~~ v = v / v . sum ( ) \n 
P = alpha * M + ( 1 - alpha ) * np . outer ( e , v ) \n 
return P \n 
~~ def pagerank_numpy ( G , alpha = 0.85 , personalization = None , weight = ) : \n 
~~ M = google_matrix ( G , alpha , personalization = personalization , \n 
nodelist = nodelist , weight = weight ) \n 
eigenvalues , eigenvectors = np . linalg . eig ( M . T ) \n 
ind = eigenvalues . argsort ( ) \n 
largest = np . array ( eigenvectors [ : , ind [ - 1 ] ] ) . flatten ( ) . real \n 
norm = float ( largest . sum ( ) ) \n 
centrality = dict ( zip ( nodelist , map ( float , largest / norm ) ) ) \n 
return centrality \n 
~~ def pagerank_scipy ( G , alpha = 0.85 , personalization = None , \n 
max_iter = 100 , tol = 1.0e-6 , weight = ) : \n 
~~~ import scipy . sparse \n 
~~ M = nx . to_scipy_sparse_matrix ( G , nodelist = nodelist , weight = weight , dtype = ) \n 
S = scipy . array ( M . sum ( axis = 1 ) ) . flatten ( ) \n 
S [ S > 0 ] = 1.0 / S [ S > 0 ] \n 
Q = scipy . sparse . spdiags ( S . T , 0 , * M . shape , format = ) \n 
M = Q * M \n 
dangle = scipy . array ( scipy . where ( M . sum ( axis = 1 ) == 0 , 1.0 / n , 0 ) ) . flatten ( ) \n 
~~~ v = scipy . array ( list ( personalization . values ( ) ) , dtype = float ) \n 
v = v / v . sum ( ) \n 
~~~ v = x \n 
~~ i = 0 \n 
while i <= max_iter : \n 
x = alpha * ( x * M + scipy . dot ( dangle , xlast ) ) + ( 1 - alpha ) * v \n 
x = x / x . sum ( ) \n 
err = scipy . absolute ( x - xlast ) . sum ( ) \n 
if err < n * tol : \n 
~~~ return dict ( zip ( nodelist , map ( float , x ) ) ) \n 
~~ raise NetworkXError ( \n 
% ( i + 1 ) ) \n 
~~ def setup_module ( module ) : \n 
~~~ from nose import SkipTest \n 
~~~ import numpy \n 
~~~ import scipy \n 
~~ ~~ from nose . tools import * \n 
class TestDFS : \n 
G . add_edges_from ( [ ( 0 , 1 ) , ( 1 , 2 ) , ( 1 , 3 ) , ( 2 , 4 ) , ( 3 , 4 ) ] ) \n 
self . G = G \n 
D = nx . Graph ( ) \n 
D . add_edges_from ( [ ( 0 , 1 ) , ( 2 , 3 ) ] ) \n 
self . D = D \n 
~~ def test_preorder_nodes ( self ) : \n 
~~~ assert_equal ( list ( nx . dfs_preorder_nodes ( self . G , source = 0 ) ) , \n 
[ 0 , 1 , 2 , 4 , 3 ] ) \n 
assert_equal ( list ( nx . dfs_preorder_nodes ( self . D ) ) , [ 0 , 1 , 2 , 3 ] ) \n 
~~ def test_postorder_nodes ( self ) : \n 
~~~ assert_equal ( list ( nx . dfs_postorder_nodes ( self . G , source = 0 ) ) , \n 
[ 3 , 4 , 2 , 1 , 0 ] ) \n 
assert_equal ( list ( nx . dfs_postorder_nodes ( self . D ) ) , [ 1 , 0 , 3 , 2 ] ) \n 
~~ def test_successor ( self ) : \n 
~~~ assert_equal ( nx . dfs_successors ( self . G , source = 0 ) , \n 
{ 0 : [ 1 ] , 1 : [ 2 ] , 2 : [ 4 ] , 4 : [ 3 ] } ) \n 
assert_equal ( nx . dfs_successors ( self . D ) , { 0 : [ 1 ] , 2 : [ 3 ] } ) \n 
~~ def test_predecessor ( self ) : \n 
~~~ assert_equal ( nx . dfs_predecessors ( self . G , source = 0 ) , \n 
{ 1 : 0 , 2 : 1 , 3 : 4 , 4 : 2 } ) \n 
assert_equal ( nx . dfs_predecessors ( self . D ) , { 1 : 0 , 3 : 2 } ) \n 
~~ def test_dfs_tree ( self ) : \n 
~~~ T = nx . dfs_tree ( self . G , source = 0 ) \n 
assert_equal ( sorted ( T . nodes ( ) ) , sorted ( self . G . nodes ( ) ) ) \n 
assert_equal ( sorted ( T . edges ( ) ) , [ ( 0 , 1 ) , ( 1 , 2 ) , ( 2 , 4 ) , ( 4 , 3 ) ] ) \n 
~~ def test_dfs_edges ( self ) : \n 
~~~ edges = nx . dfs_edges ( self . G , source = 0 ) \n 
assert_equal ( list ( edges ) , [ ( 0 , 1 ) , ( 1 , 2 ) , ( 2 , 4 ) , ( 4 , 3 ) ] ) \n 
edges = nx . dfs_edges ( self . D ) \n 
assert_equal ( list ( edges ) , [ ( 0 , 1 ) , ( 2 , 3 ) ] ) \n 
~~ def test_dfs_labeled_edges ( self ) : \n 
~~~ edges = list ( nx . dfs_labeled_edges ( self . G , source = 0 ) ) \n 
forward = [ ( u , v ) for ( u , v , d ) in edges if d [ ] == ] \n 
assert_equal ( forward , [ ( 0 , 0 ) , ( 0 , 1 ) , ( 1 , 2 ) , ( 2 , 4 ) , ( 4 , 3 ) ] ) \n 
~~ def test_dfs_labeled_disconnected_edges ( self ) : \n 
~~~ edges = list ( nx . dfs_labeled_edges ( self . D ) ) \n 
assert_equal ( forward , [ ( 0 , 0 ) , ( 0 , 1 ) , ( 2 , 2 ) , ( 2 , 3 ) ] ) \n 
~~ def test_dfs_tree_isolates ( self ) : \n 
G . add_node ( 1 ) \n 
G . add_node ( 2 ) \n 
T = nx . dfs_tree ( G , source = 1 ) \n 
assert_equal ( sorted ( T . nodes ( ) ) , [ 1 ] ) \n 
assert_equal ( sorted ( T . edges ( ) ) , [ ] ) \n 
T = nx . dfs_tree ( G , source = None ) \n 
assert_equal ( sorted ( T . nodes ( ) ) , [ 1 , 2 ] ) \n 
__all__ = [ "decorator" , "FunctionMaker" , "partial" ] \n 
import sys , re , inspect \n 
~~~ from functools import partial \n 
~~~ class partial ( object ) : \n 
def __init__ ( self , func , * args , ** kw ) : \n 
~~~ self . func = func \n 
self . keywords = kw \n 
~~ def __call__ ( self , * otherargs , ** otherkw ) : \n 
~~~ kw = self . keywords . copy ( ) \n 
kw . update ( otherkw ) \n 
return self . func ( * ( self . args + otherargs ) , ** kw ) \n 
~~ ~~ ~~ if sys . version >= : \n 
~~~ from inspect import getfullargspec \n 
~~~ class getfullargspec ( object ) : \n 
def __init__ ( self , f ) : \n 
~~~ self . args , self . varargs , self . varkw , self . defaults = inspect . getargspec ( f ) \n 
self . kwonlyargs = [ ] \n 
self . kwonlydefaults = None \n 
self . annotations = getattr ( f , , { } ) \n 
~~~ yield self . args \n 
yield self . varargs \n 
yield self . varkw \n 
yield self . defaults \n 
~~ ~~ ~~ DEF = re . compile ( ) \n 
class FunctionMaker ( object ) : \n 
def __init__ ( self , func = None , name = None , signature = None , \n 
defaults = None , doc = None , module = None , funcdict = None ) : \n 
~~~ self . shortsignature = signature \n 
~~~ self . name = func . __name__ \n 
~~~ self . name = \n 
~~ self . doc = func . __doc__ \n 
self . module = func . __module__ \n 
if inspect . isfunction ( func ) : \n 
~~~ argspec = getfullargspec ( func ) \n 
for a in ( , , , , , \n 
, ) : \n 
~~~ setattr ( self , a , getattr ( argspec , a ) ) \n 
~~ for i , arg in enumerate ( self . args ) : \n 
~~~ setattr ( self , % i , arg ) \n 
~~ self . signature = inspect . formatargspec ( \n 
formatvalue = lambda val : "" , * argspec ) [ 1 : - 1 ] \n 
allargs = list ( self . args ) \n 
if self . varargs : \n 
~~~ allargs . append ( + self . varargs ) \n 
~~ if self . varkw : \n 
~~~ allargs . append ( + self . varkw ) \n 
~~~ self . shortsignature = . join ( allargs ) \n 
~~~ self . shortsignature = self . signature \n 
~~ self . dict = func . __dict__ . copy ( ) \n 
~~ ~~ if name : \n 
~~ if signature is not None : \n 
~~~ self . signature = signature \n 
~~ if defaults : \n 
~~~ self . defaults = defaults \n 
~~ if doc : \n 
~~~ self . doc = doc \n 
~~ if module : \n 
~~~ self . module = module \n 
~~ if funcdict : \n 
~~~ self . dict = funcdict \n 
~~ assert hasattr ( self , ) \n 
~~~ raise TypeError ( % func ) \n 
~~ ~~ def update ( self , func , ** kw ) : \n 
func . __name__ = self . name \n 
func . __doc__ = getattr ( self , , None ) \n 
func . __dict__ = getattr ( self , , { } ) \n 
func . func_defaults = getattr ( self , , ( ) ) \n 
func . __kwdefaults__ = getattr ( self , , None ) \n 
callermodule = sys . _getframe ( 3 ) . f_globals . get ( , ) \n 
func . __module__ = getattr ( self , , callermodule ) \n 
func . __dict__ . update ( kw ) \n 
~~ def make ( self , src_templ , evaldict = None , addsource = False , ** attrs ) : \n 
evaldict = evaldict or { } \n 
mo = DEF . match ( src ) \n 
if mo is None : \n 
~~~ raise SyntaxError ( % src ) \n 
names = set ( [ name ] + [ arg . strip ( ) for arg in \n 
self . shortsignature . split ( ) ] ) \n 
for n in names : \n 
~~~ if n in ( , ) : \n 
~~~ raise NameError ( % ( n , src ) ) \n 
~~~ code = compile ( src , , ) \n 
exec code in evaldict \n 
~~~ print >> sys . stderr , \n 
print >> sys . stderr , src \n 
~~ func = evaldict [ name ] \n 
if addsource : \n 
~~~ attrs [ ] = src \n 
~~ self . update ( func , ** attrs ) \n 
def create ( cls , obj , body , evaldict , defaults = None , \n 
doc = None , module = None , addsource = True , ** attrs ) : \n 
~~~ name , rest = obj . strip ( ) . split ( , 1 ) \n 
func = None \n 
signature = None \n 
func = obj \n 
~~ self = cls ( func , name , signature , defaults , doc , module ) \n 
ibody = . join ( + line for line in body . splitlines ( ) ) \n 
return self . make ( + ibody , \n 
evaldict , addsource , ** attrs ) \n 
~~ ~~ def decorator ( caller , func = None ) : \n 
~~~ evaldict = func . func_globals . copy ( ) \n 
evaldict [ ] = caller \n 
evaldict [ ] = func \n 
return FunctionMaker . create ( \n 
evaldict , undecorated = func , __wrapped__ = func ) \n 
~~~ if isinstance ( caller , partial ) : \n 
~~~ return partial ( decorator , caller ) \n 
evaldict = caller . func_globals . copy ( ) \n 
evaldict [ ] = decorator \n 
% ( caller . __name__ , first ) , \n 
% first , \n 
evaldict , undecorated = caller , __wrapped__ = caller , \n 
doc = caller . __doc__ , module = caller . __module__ ) \n 
from networkx import * \n 
from networkx . generators . random_graphs import * \n 
class TestGeneratorsRandom ( ) : \n 
~~~ def smoke_test_random_graph ( self ) : \n 
~~~ seed = 42 \n 
G = gnp_random_graph ( 100 , 0.25 , seed ) \n 
G = binomial_graph ( 100 , 0.25 , seed ) \n 
G = erdos_renyi_graph ( 100 , 0.25 , seed ) \n 
G = fast_gnp_random_graph ( 100 , 0.25 , seed ) \n 
G = gnm_random_graph ( 100 , 20 , seed ) \n 
G = dense_gnm_random_graph ( 100 , 20 , seed ) \n 
G = watts_strogatz_graph ( 10 , 2 , 0.25 , seed ) \n 
assert_equal ( len ( G ) , 10 ) \n 
assert_equal ( G . number_of_edges ( ) , 10 ) \n 
G = connected_watts_strogatz_graph ( 10 , 2 , 0.1 , seed ) \n 
G = watts_strogatz_graph ( 10 , 4 , 0.25 , seed ) \n 
assert_equal ( G . number_of_edges ( ) , 20 ) \n 
G = newman_watts_strogatz_graph ( 10 , 2 , 0.0 , seed ) \n 
G = newman_watts_strogatz_graph ( 10 , 4 , 0.25 , seed ) \n 
assert_true ( G . number_of_edges ( ) >= 20 ) \n 
G = barabasi_albert_graph ( 100 , 1 , seed ) \n 
G = barabasi_albert_graph ( 100 , 3 , seed ) \n 
assert_equal ( G . number_of_edges ( ) , ( 97 * 3 ) ) \n 
G = powerlaw_cluster_graph ( 100 , 1 , 1.0 , seed ) \n 
G = powerlaw_cluster_graph ( 100 , 3 , 0.0 , seed ) \n 
G = random_regular_graph ( 10 , 20 , seed ) \n 
assert_raises ( networkx . exception . NetworkXError , \n 
random_regular_graph , 3 , 21 ) \n 
constructor = [ ( 10 , 20 , 0.8 ) , ( 20 , 40 , 0.8 ) ] \n 
G = random_shell_graph ( constructor , seed ) \n 
G = nx . random_lobster ( 10 , 0.1 , 0.5 , seed ) \n 
~~ def test_gnp ( self ) : \n 
~~~ G = gnp_random_graph ( 10 , 0.1 ) \n 
G = gnp_random_graph ( 10 , 0.1 , seed = 42 ) \n 
G = gnp_random_graph ( 10 , 1.1 ) \n 
assert_equal ( len ( G . edges ( ) ) , 45 ) \n 
G = gnp_random_graph ( 10 , 1.1 , directed = True ) \n 
assert_equal ( len ( G . edges ( ) ) , 90 ) \n 
G = gnp_random_graph ( 10 , - 1.1 ) \n 
assert_equal ( len ( G . edges ( ) ) , 0 ) \n 
G = binomial_graph ( 10 , 0.1 ) \n 
G = erdos_renyi_graph ( 10 , 0.1 ) \n 
~~ def test_fast_gnp ( self ) : \n 
~~~ G = fast_gnp_random_graph ( 10 , 0.1 ) \n 
G = fast_gnp_random_graph ( 10 , 0.1 , seed = 42 ) \n 
G = fast_gnp_random_graph ( 10 , 1.1 ) \n 
G = fast_gnp_random_graph ( 10 , - 1.1 ) \n 
G = fast_gnp_random_graph ( 10 , 0.1 , directed = True ) \n 
assert_true ( G . is_directed ( ) ) \n 
~~ def test_gnm ( self ) : \n 
~~~ G = gnm_random_graph ( 10 , 3 ) \n 
assert_equal ( len ( G . edges ( ) ) , 3 ) \n 
G = gnm_random_graph ( 10 , 3 , seed = 42 ) \n 
G = gnm_random_graph ( 10 , 100 ) \n 
G = gnm_random_graph ( 10 , 100 , directed = True ) \n 
G = gnm_random_graph ( 10 , - 1.1 ) \n 
~~ def test_watts_strogatz_big_k ( self ) : \n 
~~~ assert_raises ( networkx . exception . NetworkXError , \n 
watts_strogatz_graph , 10 , 10 , 0.25 ) \n 
newman_watts_strogatz_graph , 10 , 10 , 0.25 ) \n 
watts_strogatz_graph ( 10 , 9 , 0.25 , seed = 0 ) \n 
newman_watts_strogatz_graph ( 10 , 9 , 0.5 , seed = 0 ) \n 
~~ ~~ import json \n 
from nose . tools import assert_equal , assert_raises , assert_not_equal , assert_true \n 
from networkx . readwrite . json_graph import * \n 
class TestTree : \n 
~~~ def test_graph ( self ) : \n 
~~~ G = nx . DiGraph ( ) \n 
G . add_nodes_from ( [ 1 , 2 , 3 ] , color = ) \n 
G . add_edge ( 1 , 2 , foo = 7 ) \n 
G . add_edge ( 1 , 3 , foo = 10 ) \n 
G . add_edge ( 3 , 4 , foo = 10 ) \n 
H = tree_graph ( tree_data ( G , 1 ) ) \n 
nx . is_isomorphic ( G , H ) \n 
~~ def test_graph_attributes ( self ) : \n 
assert_equal ( H . node [ 1 ] [ ] , ) \n 
d = json . dumps ( tree_data ( G , 1 ) ) \n 
H = tree_graph ( json . loads ( d ) ) \n 
~~ ~~ """Convert\n=======\n""" \n 
from networkx . convert import * \n 
from networkx . algorithms . operators import * \n 
from networkx . generators . classic import barbell_graph , cycle_graph \n 
class TestConvert ( ) : \n 
~~~ def edgelists_equal ( self , e1 , e2 ) : \n 
~~~ return sorted ( sorted ( e ) for e in e1 ) == sorted ( sorted ( e ) for e in e2 ) \n 
~~ def test_simple_graphs ( self ) : \n 
~~~ for dest , source in [ ( to_dict_of_dicts , from_dict_of_dicts ) , \n 
( to_dict_of_lists , from_dict_of_lists ) ] : \n 
~~~ G = barbell_graph ( 10 , 3 ) \n 
dod = dest ( G ) \n 
GG = source ( dod ) \n 
assert_equal ( sorted ( G . nodes ( ) ) , sorted ( GG . nodes ( ) ) ) \n 
assert_equal ( sorted ( G . edges ( ) ) , sorted ( GG . edges ( ) ) ) \n 
GW = to_networkx_graph ( dod ) \n 
assert_equal ( sorted ( G . nodes ( ) ) , sorted ( GW . nodes ( ) ) ) \n 
assert_equal ( sorted ( G . edges ( ) ) , sorted ( GW . edges ( ) ) ) \n 
GI = Graph ( dod ) \n 
assert_equal ( sorted ( G . nodes ( ) ) , sorted ( GI . nodes ( ) ) ) \n 
assert_equal ( sorted ( G . edges ( ) ) , sorted ( GI . edges ( ) ) ) \n 
P4 = path_graph ( 4 ) \n 
P3 = path_graph ( 3 ) \n 
dod = dest ( P4 , nodelist = [ 0 , 1 , 2 ] ) \n 
Gdod = Graph ( dod ) \n 
assert_equal ( sorted ( Gdod . nodes ( ) ) , sorted ( P3 . nodes ( ) ) ) \n 
assert_equal ( sorted ( Gdod . edges ( ) ) , sorted ( P3 . edges ( ) ) ) \n 
~~ ~~ def test_digraphs ( self ) : \n 
~~~ G = cycle_graph ( 10 ) \n 
G = cycle_graph ( 10 , create_using = DiGraph ( ) ) \n 
GG = source ( dod , create_using = DiGraph ( ) ) \n 
GW = to_networkx_graph ( dod , create_using = DiGraph ( ) ) \n 
GI = DiGraph ( dod ) \n 
~~ ~~ def test_graph ( self ) : \n 
e = G . edges ( ) \n 
source = [ u for u , v in e ] \n 
dest = [ v for u , v in e ] \n 
ex = zip ( source , dest , source ) \n 
G = Graph ( ) \n 
G . add_weighted_edges_from ( ex ) \n 
dod = to_dict_of_dicts ( G ) \n 
GG = from_dict_of_dicts ( dod , create_using = Graph ( ) ) \n 
GW = to_networkx_graph ( dod , create_using = Graph ( ) ) \n 
dol = to_dict_of_lists ( G ) \n 
GG = from_dict_of_lists ( dol , create_using = Graph ( ) ) \n 
enone = [ ( u , v , { } ) for ( u , v , d ) in G . edges ( data = True ) ] \n 
assert_equal ( enone , sorted ( GG . edges ( data = True ) ) ) \n 
GW = to_networkx_graph ( dol , create_using = Graph ( ) ) \n 
assert_equal ( enone , sorted ( GW . edges ( data = True ) ) ) \n 
GI = Graph ( dol ) \n 
assert_equal ( enone , sorted ( GI . edges ( data = True ) ) ) \n 
~~ def test_with_multiedges_self_loops ( self ) : \n 
source , dest = list ( zip ( * e ) ) \n 
ex = list ( zip ( source , dest , source ) ) \n 
XG = Graph ( ) \n 
XG . add_weighted_edges_from ( ex ) \n 
XGM = MultiGraph ( ) \n 
XGM . add_weighted_edges_from ( ex ) \n 
XGS = Graph ( ) \n 
XGS . add_weighted_edges_from ( ex ) \n 
dod = to_dict_of_dicts ( XGS ) \n 
assert_equal ( sorted ( XGS . nodes ( ) ) , sorted ( GG . nodes ( ) ) ) \n 
assert_equal ( sorted ( XGS . edges ( ) ) , sorted ( GG . edges ( ) ) ) \n 
assert_equal ( sorted ( XGS . nodes ( ) ) , sorted ( GW . nodes ( ) ) ) \n 
assert_equal ( sorted ( XGS . edges ( ) ) , sorted ( GW . edges ( ) ) ) \n 
assert_equal ( sorted ( XGS . nodes ( ) ) , sorted ( GI . nodes ( ) ) ) \n 
assert_equal ( sorted ( XGS . edges ( ) ) , sorted ( GI . edges ( ) ) ) \n 
dol = to_dict_of_lists ( XGS ) \n 
enone = [ ( u , v , { } ) for ( u , v , d ) in XGS . edges ( data = True ) ] \n 
dod = to_dict_of_dicts ( XGM ) \n 
GG = from_dict_of_dicts ( dod , create_using = MultiGraph ( ) , \n 
multigraph_input = True ) \n 
assert_equal ( sorted ( XGM . nodes ( ) ) , sorted ( GG . nodes ( ) ) ) \n 
assert_equal ( sorted ( XGM . edges ( ) ) , sorted ( GG . edges ( ) ) ) \n 
GW = to_networkx_graph ( dod , create_using = MultiGraph ( ) , multigraph_input = True ) \n 
assert_equal ( sorted ( XGM . nodes ( ) ) , sorted ( GW . nodes ( ) ) ) \n 
assert_equal ( sorted ( XGM . edges ( ) ) , sorted ( GW . edges ( ) ) ) \n 
GI = MultiGraph ( dod ) \n 
assert_equal ( sorted ( XGM . nodes ( ) ) , sorted ( GI . nodes ( ) ) ) \n 
assert_false ( sorted ( XGM . edges ( ) ) == sorted ( GI . edges ( ) ) ) \n 
GE = from_dict_of_dicts ( dod , create_using = MultiGraph ( ) , \n 
multigraph_input = False ) \n 
assert_equal ( sorted ( XGM . nodes ( ) ) , sorted ( GE . nodes ( ) ) ) \n 
assert_not_equal ( sorted ( XGM . edges ( ) ) , sorted ( GE . edges ( ) ) ) \n 
GI = MultiGraph ( XGM ) \n 
assert_equal ( sorted ( XGM . edges ( ) ) , sorted ( GI . edges ( ) ) ) \n 
GM = MultiGraph ( G ) \n 
assert_equal ( sorted ( GM . nodes ( ) ) , sorted ( G . nodes ( ) ) ) \n 
assert_equal ( sorted ( GM . edges ( ) ) , sorted ( G . edges ( ) ) ) \n 
GG = from_dict_of_lists ( dol , create_using = MultiGraph ( ) ) \n 
GW = to_networkx_graph ( dol , create_using = MultiGraph ( ) ) \n 
GI = MultiGraph ( dol ) \n 
~~ def test_edgelists ( self ) : \n 
~~~ P = path_graph ( 4 ) \n 
e = [ ( 0 , 1 ) , ( 1 , 2 ) , ( 2 , 3 ) ] \n 
G = Graph ( e ) \n 
assert_equal ( sorted ( G . nodes ( ) ) , sorted ( P . nodes ( ) ) ) \n 
assert_equal ( sorted ( G . edges ( ) ) , sorted ( P . edges ( ) ) ) \n 
assert_equal ( sorted ( G . edges ( data = True ) ) , sorted ( P . edges ( data = True ) ) ) \n 
e = [ ( 0 , 1 , { } ) , ( 1 , 2 , { } ) , ( 2 , 3 , { } ) ] \n 
e = ( ( n , n + 1 ) for n in range ( 3 ) ) \n 
~~ def test_directed_to_undirected ( self ) : \n 
~~~ edges1 = [ ( 0 , 1 ) , ( 1 , 2 ) , ( 2 , 0 ) ] \n 
edges2 = [ ( 0 , 1 ) , ( 1 , 2 ) , ( 0 , 2 ) ] \n 
assert_true ( self . edgelists_equal ( nx . Graph ( nx . DiGraph ( edges1 ) ) . edges ( ) , edges1 ) ) \n 
assert_true ( self . edgelists_equal ( nx . Graph ( nx . DiGraph ( edges2 ) ) . edges ( ) , edges1 ) ) \n 
assert_true ( self . edgelists_equal ( nx . MultiGraph ( nx . DiGraph ( edges1 ) ) . edges ( ) , edges1 ) ) \n 
assert_true ( self . edgelists_equal ( nx . MultiGraph ( nx . DiGraph ( edges2 ) ) . edges ( ) , edges1 ) ) \n 
assert_true ( self . edgelists_equal ( nx . MultiGraph ( nx . MultiDiGraph ( edges1 ) ) . edges ( ) , \n 
edges1 ) ) \n 
assert_true ( self . edgelists_equal ( nx . MultiGraph ( nx . MultiDiGraph ( edges2 ) ) . edges ( ) , \n 
assert_true ( self . edgelists_equal ( nx . Graph ( nx . MultiDiGraph ( edges1 ) ) . edges ( ) , edges1 ) ) \n 
assert_true ( self . edgelists_equal ( nx . Graph ( nx . MultiDiGraph ( edges2 ) ) . edges ( ) , edges1 ) ) \n 
from ipyparallel . error import RemoteError \n 
def mergesort ( list_of_lists , key = None ) : \n 
heap = [ ] \n 
for i , itr in enumerate ( iter ( pl ) for pl in list_of_lists ) : \n 
~~~ item = next ( itr ) \n 
if key : \n 
~~~ toadd = ( key ( item ) , i , item , itr ) \n 
~~~ toadd = ( item , i , itr ) \n 
~~ heap . append ( toadd ) \n 
~~ ~~ heapq . heapify ( heap ) \n 
~~~ while heap : \n 
~~~ _ , idx , item , itr = heap [ 0 ] \n 
yield item \n 
heapq . heapreplace ( heap , ( key ( item ) , idx , item , itr ) ) \n 
~~~ heapq . heappop ( heap ) \n 
~~~ item , idx , itr = heap [ 0 ] \n 
~~~ heapq . heapreplace ( heap , ( next ( itr ) , idx , itr ) ) \n 
~~ ~~ ~~ ~~ def remote_iterator ( view , name ) : \n 
view . execute ( % ( name , name ) , block = True ) \n 
~~~ result = view . apply_sync ( next , Reference ( + name ) ) \n 
~~ except RemoteError as e : \n 
~~~ if e . ename == : \n 
~~~ raise e \n 
~~~ yield result \n 
~~~ from ipyparallel import Client , Reference \n 
rc = Client ( ) \n 
view = rc [ : ] \n 
print ( , rc . ids ) \n 
a0 = range ( 5 , 20 ) \n 
a1 = range ( 10 ) \n 
a2 = range ( 15 , 25 ) \n 
rc [ 0 ] [ ] = a0 \n 
rc [ 1 ] [ ] = a1 \n 
rc [ 2 ] [ ] = a2 \n 
aa0 = remote_iterator ( rc [ 0 ] , ) \n 
aa1 = remote_iterator ( rc [ 1 ] , ) \n 
aa2 = remote_iterator ( rc [ 2 ] , ) \n 
print ( list ( mergesort ( [ a0 , a1 , a2 ] ) ) ) \n 
print ( list ( mergesort ( [ aa0 , aa1 , aa2 ] ) ) ) \n 
#----------------------------------------------------------------------------- \n 
import ast \n 
from IPython . core . error import UsageError \n 
from IPython . core . magic import Magics \n 
from IPython . core import magic_arguments \n 
from ipython_genutils . text import dedent \n 
def exec_args ( f ) : \n 
args = [ \n 
magic_arguments . argument ( , , action = "store_const" , \n 
const = True , dest = , \n 
const = False , dest = , \n 
magic_arguments . argument ( , , type = str , \n 
magic_arguments . argument ( , action = "store_const" , \n 
const = True , dest = "local" , \n 
const = True , dest = "set_verbose" , \n 
const = False , dest = "set_verbose" , \n 
for a in args : \n 
~~~ f = a ( f ) \n 
~~ def output_args ( f ) : \n 
magic_arguments . argument ( , action = "store_const" , dest = , \n 
const = , \n 
magic_arguments . argument ( , dest = , type = str , \n 
choices = [ , , ] , default = , \n 
magic_arguments . argument ( , , dest = , type = str , \n 
~~ class ParallelMagics ( Magics ) : \n 
magics = None \n 
registered = True \n 
suffix = \n 
_autopx = False \n 
view = None \n 
last_result = None \n 
verbose = False \n 
def __init__ ( self , shell , view , suffix = ) : \n 
~~~ self . view = view \n 
self . suffix = suffix \n 
self . magics = dict ( cell = { } , line = { } ) \n 
line_magics = self . magics [ ] \n 
px = + suffix \n 
if not suffix : \n 
~~~ line_magics [ ] = self . result \n 
~~ line_magics [ + suffix ] = self . result \n 
line_magics [ px ] = self . px \n 
line_magics [ + suffix ] = self . pxconfig \n 
line_magics [ + px ] = self . autopx \n 
self . magics [ ] [ px ] = self . cell_px \n 
super ( ParallelMagics , self ) . __init__ ( shell = shell ) \n 
~~ def _eval_target_str ( self , ts ) : \n 
~~~ if in ts : \n 
~~~ targets = eval ( "self.view.client.ids[%s]" % ts ) \n 
~~ elif in ts : \n 
~~~ targets = \n 
~~~ targets = eval ( ts ) \n 
~~ return targets \n 
~~ @ magic_arguments . magic_arguments ( ) \n 
@ exec_args \n 
def pxconfig ( self , line ) : \n 
args = magic_arguments . parse_argstring ( self . pxconfig , line ) \n 
if args . targets : \n 
~~~ self . view . targets = self . _eval_target_str ( args . targets ) \n 
~~ if args . block is not None : \n 
~~~ self . view . block = args . block \n 
~~ if args . set_verbose is not None : \n 
~~~ self . verbose = args . set_verbose \n 
~~ ~~ @ magic_arguments . magic_arguments ( ) \n 
@ output_args \n 
def result ( self , line = ) : \n 
args = magic_arguments . parse_argstring ( self . result , line ) \n 
if self . last_result is None : \n 
~~~ raise UsageError ( NO_LAST_RESULT ) \n 
~~ self . last_result . get ( ) \n 
self . last_result . display_outputs ( groupby = args . groupby ) \n 
~~ def px ( self , line = ) : \n 
return self . parallel_execute ( line ) \n 
~~ def parallel_execute ( self , cell , block = None , groupby = , save_name = None ) : \n 
block = self . view . block if block is None else block \n 
targets = self . view . targets \n 
if isinstance ( targets , list ) and len ( targets ) > 10 : \n 
~~~ str_targets = str ( targets [ : 4 ] ) [ : - 1 ] + + str ( targets [ - 4 : ] ) [ 1 : ] \n 
~~~ str_targets = str ( targets ) \n 
~~ result = self . view . execute ( cell , silent = False , block = False ) \n 
self . last_result = result \n 
if save_name : \n 
~~~ self . shell . user_ns [ save_name ] = result \n 
~~ if block : \n 
~~~ result . get ( ) \n 
result . display_outputs ( groupby ) \n 
def cell_px ( self , line = , cell = None ) : \n 
args = magic_arguments . parse_argstring ( self . cell_px , line ) \n 
~~~ save_targets = self . view . targets \n 
self . view . targets = self . _eval_target_str ( args . targets ) \n 
~~ block = False if args . local else args . block \n 
~~~ ar = self . parallel_execute ( cell , block = block , \n 
groupby = args . groupby , \n 
save_name = args . save_name , \n 
~~~ if args . targets : \n 
~~~ self . view . targets = save_targets \n 
~~ ~~ block = self . view . block if args . block is None else args . block \n 
if args . local : \n 
~~~ self . shell . run_cell ( cell ) \n 
if block : \n 
~~~ ar . get ( ) \n 
ar . display_outputs ( args . groupby ) \n 
~~ ~~ if not block : \n 
~~~ return ar \n 
~~ ~~ def autopx ( self , line = ) : \n 
if self . _autopx : \n 
~~~ self . _disable_autopx ( ) \n 
~~~ self . _enable_autopx ( ) \n 
~~ ~~ def _enable_autopx ( self ) : \n 
self . _original_run_cell = self . shell . run_cell \n 
self . shell . run_cell = self . pxrun_cell \n 
self . _autopx = True \n 
~~ def _disable_autopx ( self ) : \n 
~~~ self . shell . run_cell = self . _original_run_cell \n 
self . _autopx = False \n 
~~ ~~ def pxrun_cell ( self , raw_cell , store_history = False , silent = False ) : \n 
if ( not raw_cell ) or raw_cell . isspace ( ) : \n 
~~ ipself = self . shell \n 
with ipself . builtin_trap : \n 
~~~ cell = ipself . prefilter_manager . prefilter_lines ( raw_cell ) \n 
if store_history : \n 
~~~ ipself . history_manager . store_inputs ( ipself . execution_count , \n 
cell , raw_cell ) \n 
~~ cell_name = ipself . compile . cache ( cell , ipself . execution_count ) \n 
~~~ ast . parse ( cell , filename = cell_name ) \n 
~~ except ( OverflowError , SyntaxError , ValueError , TypeError , \n 
MemoryError ) : \n 
~~~ ipself . showsyntaxerror ( ) \n 
ipself . execution_count += 1 \n 
~~ ~~ if store_history : \n 
~~~ ipself . history_manager . store_output ( ipself . execution_count ) \n 
~~ if re . search ( r\'get_ipython\\(\\)\\.magic\\(u?["\\\']%?autopx\' , cell ) : \n 
~~~ result = self . view . execute ( cell , silent = False , block = False ) \n 
~~~ ipself . showtraceback ( ) \n 
~~~ if self . view . block : \n 
~~~ self . shell . showtraceback ( ) \n 
~~~ with ipself . builtin_trap : \n 
~~~ result . display_outputs ( ) \n 
~~ ~~ ~~ ~~ __doc__ = __doc__ . format ( \n 
AUTOPX_DOC = dedent ( ParallelMagics . autopx . __doc__ ) , \n 
PX_DOC = dedent ( ParallelMagics . px . __doc__ ) , \n 
RESULT_DOC = dedent ( ParallelMagics . result . __doc__ ) , \n 
CONFIG_DOC = dedent ( ParallelMagics . pxconfig . __doc__ ) , \n 
from types import FunctionType \n 
from ipython_genutils import py3compat \n 
from ipython_genutils . importstring import import_item \n 
from ipython_genutils . py3compat import string_types , iteritems , buffer_to_bytes , buffer_to_bytes_py2 \n 
from . import codeutil \n 
from traitlets . log import get_logger \n 
if py3compat . PY3 : \n 
~~~ buffer = memoryview \n 
class_type = type \n 
~~~ from types import ClassType \n 
class_type = ( type , ClassType ) \n 
~~ def _get_cell_type ( a = None ) : \n 
def inner ( ) : \n 
~~~ return a \n 
~~ return type ( py3compat . get_closure ( inner ) [ 0 ] ) \n 
~~ cell_type = _get_cell_type ( ) \n 
#------------------------------------------------------------------------------- \n 
def interactive ( f ) : \n 
if isinstance ( f , FunctionType ) : \n 
~~~ mainmod = __import__ ( ) \n 
f = FunctionType ( f . __code__ , mainmod . __dict__ , \n 
f . __name__ , f . __defaults__ , \n 
~~ f . __module__ = \n 
~~ def use_dill ( ) : \n 
import dill \n 
from . import serialize \n 
serialize . pickle = dill \n 
can_map . pop ( FunctionType , None ) \n 
~~ def use_cloudpickle ( ) : \n 
import cloudpickle \n 
serialize . pickle = cloudpickle \n 
~~ def use_pickle ( ) : \n 
serialize . pickle = serialize . _stdlib_pickle \n 
can_map [ FunctionType ] = _original_can_map [ FunctionType ] \n 
~~ class CannedObject ( object ) : \n 
~~~ def __init__ ( self , obj , keys = [ ] , hook = None ) : \n 
self . keys = keys \n 
self . obj = copy . copy ( obj ) \n 
self . hook = can ( hook ) \n 
for key in keys : \n 
~~~ setattr ( self . obj , key , can ( getattr ( obj , key ) ) ) \n 
~~ self . buffers = [ ] \n 
~~ def get_object ( self , g = None ) : \n 
~~~ if g is None : \n 
~~~ g = { } \n 
~~ obj = self . obj \n 
~~~ setattr ( obj , key , uncan ( getattr ( obj , key ) , g ) ) \n 
~~ if self . hook : \n 
~~~ self . hook = uncan ( self . hook , g ) \n 
self . hook ( obj , g ) \n 
~~ return self . obj \n 
~~ ~~ class Reference ( CannedObject ) : \n 
def __init__ ( self , name ) : \n 
~~~ if not isinstance ( name , string_types ) : \n 
self . buffers = [ ] \n 
~~ return eval ( self . name , g ) \n 
~~ ~~ class CannedCell ( CannedObject ) : \n 
def __init__ ( self , cell ) : \n 
~~~ self . cell_contents = can ( cell . cell_contents ) \n 
~~~ cell_contents = uncan ( self . cell_contents , g ) \n 
~~~ return cell_contents \n 
~~ return py3compat . get_closure ( inner ) [ 0 ] \n 
~~ ~~ class CannedFunction ( CannedObject ) : \n 
~~~ def __init__ ( self , f ) : \n 
~~~ self . _check_type ( f ) \n 
self . code = f . __code__ \n 
if f . __defaults__ : \n 
~~~ self . defaults = [ can ( fd ) for fd in f . __defaults__ ] \n 
~~~ self . defaults = None \n 
~~ closure = py3compat . get_closure ( f ) \n 
if closure : \n 
~~~ self . closure = tuple ( can ( cell ) for cell in closure ) \n 
~~~ self . closure = None \n 
~~ self . module = f . __module__ or \n 
self . __name__ = f . __name__ \n 
~~ def _check_type ( self , obj ) : \n 
~~~ if not self . module . startswith ( ) : \n 
~~~ __import__ ( self . module ) \n 
g = sys . modules [ self . module ] . __dict__ \n 
~~ if g is None : \n 
~~ if self . defaults : \n 
~~~ defaults = tuple ( uncan ( cfd , g ) for cfd in self . defaults ) \n 
~~~ defaults = None \n 
~~ if self . closure : \n 
~~~ closure = tuple ( uncan ( cell , g ) for cell in self . closure ) \n 
~~~ closure = None \n 
~~ newFunc = FunctionType ( self . code , g , self . __name__ , defaults , closure ) \n 
return newFunc \n 
~~ ~~ class CannedClass ( CannedObject ) : \n 
~~~ def __init__ ( self , cls ) : \n 
~~~ self . _check_type ( cls ) \n 
self . name = cls . __name__ \n 
self . old_style = not isinstance ( cls , type ) \n 
self . _canned_dict = { } \n 
for k , v in cls . __dict__ . items ( ) : \n 
~~~ if k not in ( , ) : \n 
~~~ self . _canned_dict [ k ] = can ( v ) \n 
~~ ~~ if self . old_style : \n 
~~~ mro = [ ] \n 
~~~ mro = cls . mro ( ) \n 
~~ self . parents = [ can ( c ) for c in mro [ 1 : ] ] \n 
~~~ parents = tuple ( uncan ( p , g ) for p in self . parents ) \n 
return type ( self . name , parents , uncan_dict ( self . _canned_dict , g = g ) ) \n 
~~ ~~ class CannedArray ( CannedObject ) : \n 
~~~ def __init__ ( self , obj ) : \n 
~~~ from numpy import ascontiguousarray \n 
self . shape = obj . shape \n 
self . dtype = obj . dtype . descr if obj . dtype . fields else obj . dtype . str \n 
self . pickled = False \n 
if sum ( obj . shape ) == 0 : \n 
~~~ self . pickled = True \n 
~~ elif obj . dtype == : \n 
~~ elif obj . dtype . fields and any ( dt == for dt , sz in obj . dtype . fields . values ( ) ) : \n 
~~ if self . pickled : \n 
~~~ from . import serialize \n 
self . buffers = [ serialize . pickle . dumps ( obj , serialize . PICKLE_PROTOCOL ) ] \n 
~~~ obj = ascontiguousarray ( obj , dtype = None ) \n 
self . buffers = [ buffer ( obj ) ] \n 
~~ ~~ def get_object ( self , g = None ) : \n 
~~~ from numpy import frombuffer \n 
data = self . buffers [ 0 ] \n 
if self . pickled : \n 
return serialize . pickle . loads ( buffer_to_bytes_py2 ( data ) ) \n 
~~~ if not py3compat . PY3 and isinstance ( data , memoryview ) : \n 
~~~ data = buffer ( data . tobytes ( ) ) \n 
~~ return frombuffer ( data , dtype = self . dtype ) . reshape ( self . shape ) \n 
~~ ~~ ~~ class CannedBytes ( CannedObject ) : \n 
~~~ wrap = staticmethod ( buffer_to_bytes ) \n 
def __init__ ( self , obj ) : \n 
~~~ self . buffers = [ obj ] \n 
~~~ data = self . buffers [ 0 ] \n 
return self . wrap ( data ) \n 
~~ ~~ class CannedBuffer ( CannedBytes ) : \n 
~~~ wrap = buffer \n 
~~ class CannedMemoryView ( CannedBytes ) : \n 
~~~ wrap = memoryview \n 
~~ def _import_mapping ( mapping , original = None ) : \n 
log = get_logger ( ) \n 
for key , value in list ( mapping . items ( ) ) : \n 
~~~ if isinstance ( key , string_types ) : \n 
~~~ cls = import_item ( key ) \n 
~~~ if original and key not in original : \n 
~~ mapping . pop ( key ) \n 
~~~ mapping [ cls ] = mapping . pop ( key ) \n 
~~ ~~ ~~ ~~ def istype ( obj , check ) : \n 
if isinstance ( check , tuple ) : \n 
~~~ for cls in check : \n 
~~~ if type ( obj ) is cls : \n 
~~~ return type ( obj ) is check \n 
~~ ~~ def can ( obj ) : \n 
import_needed = False \n 
for cls , canner in iteritems ( can_map ) : \n 
~~~ if isinstance ( cls , string_types ) : \n 
~~~ import_needed = True \n 
~~ elif istype ( obj , cls ) : \n 
~~~ return canner ( obj ) \n 
~~ ~~ if import_needed : \n 
~~~ _import_mapping ( can_map , _original_can_map ) \n 
return can ( obj ) \n 
~~ return obj \n 
~~ def can_class ( obj ) : \n 
~~~ if isinstance ( obj , class_type ) and obj . __module__ == : \n 
~~~ return CannedClass ( obj ) \n 
~~~ return obj \n 
~~ ~~ def can_dict ( obj ) : \n 
if istype ( obj , dict ) : \n 
~~~ newobj = { } \n 
for k , v in iteritems ( obj ) : \n 
~~~ newobj [ k ] = can ( v ) \n 
~~ return newobj \n 
~~ ~~ sequence_types = ( list , tuple , set ) \n 
def can_sequence ( obj ) : \n 
if istype ( obj , sequence_types ) : \n 
~~~ t = type ( obj ) \n 
return t ( [ can ( i ) for i in obj ] ) \n 
~~ ~~ def uncan ( obj , g = None ) : \n 
for cls , uncanner in iteritems ( uncan_map ) : \n 
~~ elif isinstance ( obj , cls ) : \n 
~~~ return uncanner ( obj , g ) \n 
~~~ _import_mapping ( uncan_map , _original_uncan_map ) \n 
return uncan ( obj , g ) \n 
~~ def uncan_dict ( obj , g = None ) : \n 
~~~ if istype ( obj , dict ) : \n 
~~~ newobj [ k ] = uncan ( v , g ) \n 
~~ ~~ def uncan_sequence ( obj , g = None ) : \n 
~~~ if istype ( obj , sequence_types ) : \n 
return t ( [ uncan ( i , g ) for i in obj ] ) \n 
~~ ~~ def _uncan_dependent_hook ( dep , g = None ) : \n 
~~~ dep . check_dependency ( ) \n 
~~ def can_dependent ( obj ) : \n 
~~~ return CannedObject ( obj , keys = ( , ) , hook = _uncan_dependent_hook ) \n 
~~ can_map = { \n 
: CannedArray , \n 
FunctionType : CannedFunction , \n 
bytes : CannedBytes , \n 
memoryview : CannedMemoryView , \n 
cell_type : CannedCell , \n 
class_type : can_class , \n 
: can_dependent , \n 
if buffer is not memoryview : \n 
~~~ can_map [ buffer ] = CannedBuffer \n 
~~ uncan_map = { \n 
CannedObject : lambda obj , g : obj . get_object ( g ) , \n 
dict : uncan_dict , \n 
_original_can_map = can_map . copy ( ) \n 
_original_uncan_map = uncan_map . copy ( ) \n 
c = get_config ( ) \n 
app = c . InteractiveShellApp \n 
load_subconfig ( , profile = ) \n 
if hasattr ( app , ) : \n 
~~~ app . exec_lines . append ( lines ) \n 
~~ import nose . tools as nt \n 
from IPython . core import autocall \n 
from IPython . testing import decorators as dec \n 
from IPython . testing . globalipapp import get_ipython \n 
ip = get_ipython ( ) \n 
failures = [ ] \n 
num_tests = 0 \n 
class CallableIndexable ( object ) : \n 
~~~ def __getitem__ ( self , idx ) : return True \n 
def __call__ ( self , * args , ** kws ) : return True \n 
~~ class Autocallable ( autocall . IPyAutocall ) : \n 
~~~ def __call__ ( self ) : \n 
~~~ return "called" \n 
~~ ~~ def run ( tests ) : \n 
for pre , post in tests : \n 
~~~ global num_tests \n 
num_tests += 1 \n 
actual = ip . prefilter_manager . prefilter_lines ( pre ) \n 
if actual != None : \n 
~~~ actual = actual . rstrip ( ) \n 
~~ if actual != post : \n 
~~~ failures . append ( % ( \n 
pre , post , actual ) ) \n 
~~ ~~ ~~ def test_handlers ( ) : \n 
~~~ old_system_cmd = ip . system \n 
ip . system = lambda cmd : None \n 
ip . alias_manager . alias_table [ ] = ( 0 , ) \n 
ip . system = old_system_cmd \n 
call_idx = CallableIndexable ( ) \n 
ip . user_ns [ ] = call_idx \n 
ip . prefilter_manager . multi_line_specials = False \n 
run ( [ \n 
ip . prefilter_manager . multi_line_specials = True \n 
autocallable = Autocallable ( ) \n 
ip . user_ns [ ] = autocallable \n 
ip . magic ( ) \n 
nt . assert_equals ( failures , [ ] ) \n 
import pydoc \n 
import builtins \n 
from IPython . core . iplib import InteractiveShell \n 
from IPython . kernel . core . redirector_output_trap import RedirectorOutputTrap \n 
from IPython . kernel . core . sync_traceback_trap import SyncTracebackTrap \n 
import IPython . utils . io \n 
from linefrontendbase import LineFrontEndBase , common_prefix \n 
def mk_system_call ( system_call_function , command ) : \n 
def my_system_call ( args ) : \n 
return my_system_call \n 
~~ class PrefilterFrontEnd ( LineFrontEndBase ) : \n 
debug = False \n 
def __init__ ( self , ipython0 = None , * args , ** kwargs ) : \n 
LineFrontEndBase . __init__ ( self , * args , ** kwargs ) \n 
self . shell . output_trap = RedirectorOutputTrap ( \n 
out_callback = self . write , \n 
err_callback = self . write , \n 
self . shell . traceback_trap = SyncTracebackTrap ( \n 
formatters = self . shell . traceback_trap . formatters , \n 
self . save_output_hooks ( ) \n 
if ipython0 is None : \n 
~~~ def my_rawinput ( x = None ) : \n 
~~ old_rawinput = builtins . raw_input \n 
builtins . raw_input = my_rawinput \n 
ipython0 = InteractiveShell ( \n 
parent = None , user_ns = self . shell . user_ns , \n 
user_global_ns = self . shell . user_global_ns \n 
builtins . raw_input = old_rawinput \n 
~~ self . ipython0 = ipython0 \n 
self . ipython0 . set_hook ( , \n 
lambda s , string : self . write ( "\\n" + string ) ) \n 
self . ipython0 . write = self . write \n 
self . _ip = _ip = self . ipython0 \n 
self . _ip . system = self . system_call \n 
if not sys . platform . startswith ( ) : \n 
~~~ self . ipython0 . magic_ls = mk_system_call ( self . system_call , \n 
~~ self . release_output ( ) \n 
if not in kwargs and self . banner is None : \n 
~~~ self . banner = self . ipython0 . banner \n 
~~ self . start ( ) \n 
#-------------------------------------------------------------------------- \n 
~~ def show_traceback ( self ) : \n 
#self.capture_output() \n 
self . ipython0 . showtraceback ( tb_offset = - 1 ) \n 
self . release_output ( ) \n 
~~ def execute ( self , python_string , raw_string = None ) : \n 
~~~ if self . debug : \n 
~~~ print ( , repr ( python_string ) ) \n 
~~ self . capture_output ( ) \n 
LineFrontEndBase . execute ( self , python_string , \n 
raw_string = raw_string ) \n 
~~ def save_output_hooks ( self ) : \n 
self . __old_cout_write = Term . cout . write \n 
self . __old_cerr_write = Term . cerr . write \n 
self . __old_stdout = sys . stdout \n 
self . __old_stderr = sys . stderr \n 
self . __old_help_output = pydoc . help . output \n 
self . __old_display_hook = sys . displayhook \n 
~~ def capture_output ( self ) : \n 
Term . cout . write = self . write \n 
Term . cerr . write = self . write \n 
sys . stdout = Term . cout \n 
sys . stderr = Term . cerr \n 
pydoc . help . output = self . shell . output_trap . out \n 
~~ def release_output ( self ) : \n 
Term . cout . write = self . __old_cout_write \n 
Term . cerr . write = self . __old_cerr_write \n 
sys . stdout = self . __old_stdout \n 
sys . stderr = self . __old_stderr \n 
pydoc . help . output = self . __old_help_output \n 
sys . displayhook = self . __old_display_hook \n 
~~ def complete ( self , line ) : \n 
~~~ word = self . _get_completion_text ( line ) \n 
completions = self . ipython0 . complete ( word ) \n 
key = lambda x : x . replace ( , ) \n 
completions . sort ( key = key ) \n 
if completions : \n 
~~~ prefix = common_prefix ( completions ) \n 
line = line [ : - len ( word ) ] + prefix \n 
~~ return line , completions \n 
~~ def prefilter_input ( self , input_string ) : \n 
input_string = LineFrontEndBase . prefilter_input ( self , input_string ) \n 
filtered_lines = [ ] \n 
self . capture_output ( ) \n 
self . last_result = dict ( number = self . prompt_number ) \n 
~~~ for line in input_string . split ( ) : \n 
~~~ pf = self . ipython0 . prefilter_manager . prefilter_lines \n 
filtered_lines . append ( pf ( line , False ) . rstrip ( ) ) \n 
~~~ self . ipython0 . showsyntaxerror ( ) \n 
self . after_execute ( ) \n 
~~~ self . release_output ( ) \n 
~~ filtered_string = . join ( filtered_lines ) \n 
return filtered_string \n 
~~ def system_call ( self , command_string ) : \n 
return os . system ( command_string ) \n 
~~ def do_exit ( self ) : \n 
self . ipython0 . atexit_operations ( ) \n 
~~ def _get_completion_text ( self , line ) : \n 
expression = \n 
complete_sep = re . compile ( expression ) \n 
text = complete_sep . split ( line ) [ - 1 ] \n 
return text \n 
~~~ from decorator import * \n 
~~~ from . _decorator import * \n 
~~ from collections import namedtuple \n 
from pygments . lexers import PythonLexer \n 
from IPython . external . qt import QtCore , QtGui \n 
from IPython . core . inputsplitter import InputSplitter , transform_classic_prompt \n 
from IPython . core . oinspect import call_tip \n 
from IPython . frontend . qt . base_frontend_mixin import BaseFrontendMixin \n 
from IPython . utils . traitlets import Bool , Instance \n 
from . bracket_matcher import BracketMatcher \n 
from . call_tip_widget import CallTipWidget \n 
from . completion_lexer import CompletionLexer \n 
from . history_console_widget import HistoryConsoleWidget \n 
from . pygments_highlighter import PygmentsHighlighter \n 
class FrontendHighlighter ( PygmentsHighlighter ) : \n 
def __init__ ( self , frontend ) : \n 
~~~ super ( FrontendHighlighter , self ) . __init__ ( frontend . _control . document ( ) ) \n 
self . _current_offset = 0 \n 
self . _frontend = frontend \n 
self . highlighting_on = False \n 
~~ def highlightBlock ( self , string ) : \n 
if not self . highlighting_on : \n 
~~ current_block = self . currentBlock ( ) \n 
string = self . _frontend . _get_block_plain_text ( current_block ) \n 
if current_block . contains ( self . _frontend . _prompt_pos ) : \n 
~~~ prompt = self . _frontend . _prompt \n 
~~~ prompt = self . _frontend . _continuation_prompt \n 
~~ if string . startswith ( prompt ) : \n 
~~~ self . _current_offset = len ( prompt ) \n 
string = string [ len ( prompt ) : ] \n 
super ( FrontendHighlighter , self ) . highlightBlock ( string ) \n 
~~ ~~ def rehighlightBlock ( self , block ) : \n 
old = self . highlighting_on \n 
self . highlighting_on = True \n 
super ( FrontendHighlighter , self ) . rehighlightBlock ( block ) \n 
self . highlighting_on = old \n 
~~ def setFormat ( self , start , count , format ) : \n 
start += self . _current_offset \n 
super ( FrontendHighlighter , self ) . setFormat ( start , count , format ) \n 
~~ ~~ class FrontendWidget ( HistoryConsoleWidget , BaseFrontendMixin ) : \n 
custom_interrupt = Bool ( False ) \n 
custom_interrupt_requested = QtCore . Signal ( ) \n 
custom_restart = Bool ( False ) \n 
custom_restart_kernel_died = QtCore . Signal ( float ) \n 
custom_restart_requested = QtCore . Signal ( ) \n 
executing = QtCore . Signal ( object ) \n 
executed = QtCore . Signal ( object ) \n 
exit_requested = QtCore . Signal ( ) \n 
_CallTipRequest = namedtuple ( , [ , ] ) \n 
_CompletionRequest = namedtuple ( , [ , ] ) \n 
_ExecutionRequest = namedtuple ( , [ , ] ) \n 
_input_splitter_class = InputSplitter \n 
_local_kernel = False \n 
_highlighter = Instance ( FrontendHighlighter ) \n 
#--------------------------------------------------------------------------- \n 
def __init__ ( self , * args , ** kw ) : \n 
~~~ super ( FrontendWidget , self ) . __init__ ( * args , ** kw ) \n 
self . _bracket_matcher = BracketMatcher ( self . _control ) \n 
self . _call_tip_widget = CallTipWidget ( self . _control ) \n 
self . _completion_lexer = CompletionLexer ( PythonLexer ( ) ) \n 
self . _copy_raw_action = QtGui . QAction ( , None ) \n 
self . _hidden = False \n 
self . _highlighter = FrontendHighlighter ( self ) \n 
self . _input_splitter = self . _input_splitter_class ( input_mode = ) \n 
self . _kernel_manager = None \n 
self . _request_info = { } \n 
self . tab_width = 4 \n 
self . _set_continuation_prompt ( ) \n 
self . _call_tip_widget . setFont ( self . font ) \n 
self . font_changed . connect ( self . _call_tip_widget . setFont ) \n 
action = self . _copy_raw_action \n 
key = QtCore . Qt . CTRL | QtCore . Qt . SHIFT | QtCore . Qt . Key_C \n 
action . setEnabled ( False ) \n 
action . setShortcut ( QtGui . QKeySequence ( key ) ) \n 
action . setShortcutContext ( QtCore . Qt . WidgetWithChildrenShortcut ) \n 
action . triggered . connect ( self . copy_raw ) \n 
self . copy_available . connect ( action . setEnabled ) \n 
self . addAction ( action ) \n 
document = self . _control . document ( ) \n 
document . contentsChange . connect ( self . _document_contents_change ) \n 
self . _local_kernel = kw . get ( , \n 
FrontendWidget . _local_kernel ) \n 
text = self . _control . textCursor ( ) . selection ( ) . toPlainText ( ) \n 
if text : \n 
~~~ lines = list ( map ( transform_classic_prompt , text . splitlines ( ) ) ) \n 
text = . join ( lines ) \n 
QtGui . QApplication . clipboard ( ) . setText ( text ) \n 
~~ ~~ def _is_complete ( self , source , interactive ) : \n 
complete = self . _input_splitter . push ( source ) \n 
~~~ complete = not self . _input_splitter . push_accepts_more ( ) \n 
~~ return complete \n 
~~ def _execute ( self , source , hidden ) : \n 
msg_id = self . kernel_manager . shell_channel . execute ( source , hidden ) \n 
self . _request_info [ ] = self . _ExecutionRequest ( msg_id , ) \n 
self . _hidden = hidden \n 
if not hidden : \n 
~~~ self . executing . emit ( source ) \n 
~~ ~~ def _prompt_started_hook ( self ) : \n 
if not self . _reading : \n 
~~~ self . _highlighter . highlighting_on = True \n 
~~ ~~ def _prompt_finished_hook ( self ) : \n 
self . _input_splitter . reset ( ) \n 
~~~ self . _highlighter . highlighting_on = False \n 
~~ ~~ def _tab_pressed ( self ) : \n 
text = self . _get_input_buffer_cursor_line ( ) \n 
~~ complete = bool ( text [ : self . _get_input_buffer_cursor_column ( ) ] . strip ( ) ) \n 
if complete : \n 
~~~ self . _complete ( ) \n 
~~ return not complete \n 
~~ def _context_menu_make ( self , pos ) : \n 
menu = super ( FrontendWidget , self ) . _context_menu_make ( pos ) \n 
for before_action in menu . actions ( ) : \n 
~~~ if before_action . shortcut ( ) . matches ( QtGui . QKeySequence . Paste ) == QtGui . QKeySequence . ExactMatch : \n 
~~~ menu . insertAction ( before_action , self . _copy_raw_action ) \n 
~~ ~~ return menu \n 
~~ def _event_filter_console_keypress ( self , event ) : \n 
key = event . key ( ) \n 
if self . _control_key_down ( event . modifiers ( ) , include_command = False ) : \n 
~~~ if key == QtCore . Qt . Key_C and self . _executing : \n 
~~~ self . interrupt_kernel ( ) \n 
~~ elif key == QtCore . Qt . Key_Period : \n 
~~~ message = \n 
self . restart_kernel ( message , now = False ) \n 
~~ ~~ elif not event . modifiers ( ) & QtCore . Qt . AltModifier : \n 
~~~ if key == QtCore . Qt . Key_Backspace : \n 
~~~ col = self . _get_input_buffer_cursor_column ( ) \n 
cursor = self . _control . textCursor ( ) \n 
if col > 3 and not cursor . hasSelection ( ) : \n 
~~~ text = self . _get_input_buffer_cursor_line ( ) [ : col ] \n 
if text . endswith ( ) and not text . strip ( ) : \n 
~~~ cursor . movePosition ( QtGui . QTextCursor . Left , \n 
QtGui . QTextCursor . KeepAnchor , 4 ) \n 
cursor . removeSelectedText ( ) \n 
~~ ~~ ~~ ~~ return super ( FrontendWidget , self ) . _event_filter_console_keypress ( event ) \n 
~~ def _insert_continuation_prompt ( self , cursor ) : \n 
super ( FrontendWidget , self ) . _insert_continuation_prompt ( cursor ) \n 
cursor . insertText ( * self . _input_splitter . indent_spaces ) \n 
~~ def _handle_complete_reply ( self , rep ) : \n 
cursor = self . _get_cursor ( ) \n 
info = self . _request_info . get ( ) \n 
if info and info . id == rep [ ] [ ] and info . pos == cursor . position ( ) : \n 
~~~ text = . join ( self . _get_context ( ) ) \n 
cursor . movePosition ( QtGui . QTextCursor . Left , n = len ( text ) ) \n 
self . _complete_with_items ( cursor , rep [ ] [ ] ) \n 
~~ ~~ def _handle_execute_reply ( self , msg ) : \n 
if info and info . id == msg [ ] [ ] and info . kind == and not self . _hidden : \n 
~~~ self . kernel_manager . sub_channel . flush ( ) \n 
if self . ansi_codes : \n 
~~~ self . _ansi_processor . reset_sgr ( ) \n 
~~ content = msg [ ] \n 
status = content [ ] \n 
~~~ self . _process_execute_ok ( msg ) \n 
~~ elif status == : \n 
~~~ self . _process_execute_error ( msg ) \n 
~~~ self . _process_execute_abort ( msg ) \n 
~~ self . _show_interpreter_prompt_for_reply ( msg ) \n 
self . executed . emit ( msg ) \n 
~~ ~~ def _handle_input_request ( self , msg ) : \n 
if self . _hidden : \n 
~~ self . kernel_manager . sub_channel . flush ( ) \n 
def callback ( line ) : \n 
~~~ self . kernel_manager . stdin_channel . input ( line ) \n 
~~ self . _readline ( msg [ ] [ ] , callback = callback ) \n 
~~ def _handle_kernel_died ( self , since_last_heartbeat ) : \n 
if self . custom_restart : \n 
~~~ self . custom_restart_kernel_died . emit ( since_last_heartbeat ) \n 
~~~ message = % since_last_heartbeat \n 
self . restart_kernel ( message , now = True ) \n 
~~ ~~ def _handle_object_info_reply ( self , rep ) : \n 
~~~ content = rep [ ] \n 
if content . get ( , False ) : \n 
~~~ call_info , doc = None , None \n 
~~~ call_info , doc = call_tip ( content , format_call = True ) \n 
~~ if call_info or doc : \n 
~~~ self . _call_tip_widget . show_call_info ( call_info , doc ) \n 
~~ ~~ ~~ def _handle_pyout ( self , msg ) : \n 
if not self . _hidden and self . _is_from_this_session ( msg ) : \n 
~~~ text = msg [ ] [ ] \n 
self . _append_plain_text ( text + , before_prompt = True ) \n 
~~ ~~ def _handle_stream ( self , msg ) : \n 
~~~ text = msg [ ] [ ] . expandtabs ( 8 ) \n 
self . _append_plain_text ( text , before_prompt = True ) \n 
self . _control . moveCursor ( QtGui . QTextCursor . End ) \n 
~~ ~~ def _handle_shutdown_reply ( self , msg ) : \n 
if not self . _hidden and not self . _is_from_this_session ( msg ) : \n 
~~~ if self . _local_kernel : \n 
~~~ if not msg [ ] [ ] : \n 
~~~ sys . exit ( 0 ) \n 
self . reset ( ) \n 
~~~ title = self . window ( ) . windowTitle ( ) \n 
if not msg [ ] [ ] : \n 
~~~ reply = QtGui . QMessageBox . question ( self , title , \n 
QtGui . QMessageBox . Yes , QtGui . QMessageBox . No ) \n 
if reply == QtGui . QMessageBox . Yes : \n 
~~ ~~ ~~ ~~ ~~ def _started_channels ( self ) : \n 
~~ def copy_raw ( self ) : \n 
self . _control . copy ( ) \n 
~~ def execute_file ( self , path , hidden = False ) : \n 
self . execute ( % path , hidden = hidden ) \n 
~~ def interrupt_kernel ( self ) : \n 
if self . custom_interrupt : \n 
~~~ self . custom_interrupt_requested . emit ( ) \n 
~~ elif self . kernel_manager . has_kernel : \n 
~~~ self . kernel_manager . interrupt_kernel ( ) \n 
~~~ self . _append_plain_text ( \n 
~~ ~~ def reset ( self ) : \n 
if self . _executing : \n 
~~~ self . _executing = False \n 
self . _request_info [ ] = None \n 
~~ self . _reading = False \n 
self . _highlighter . highlighting_on = False \n 
self . _control . clear ( ) \n 
self . _append_plain_text ( self . _get_banner ( ) ) \n 
self . _show_interpreter_prompt ( ) \n 
~~ def restart_kernel ( self , message , now = False ) : \n 
~~~ self . custom_restart_requested . emit ( ) \n 
~~~ self . kernel_manager . hb_channel . pause ( ) \n 
buttons = QtGui . QMessageBox . Yes | QtGui . QMessageBox . No \n 
result = QtGui . QMessageBox . question ( self , , \n 
message , buttons ) \n 
if result == QtGui . QMessageBox . Yes : \n 
~~~ self . kernel_manager . restart_kernel ( now = now ) \n 
~~ except RuntimeError : \n 
~~~ self . kernel_manager . hb_channel . unpause ( ) \n 
~~ ~~ def _call_tip ( self ) : \n 
cursor . movePosition ( QtGui . QTextCursor . Left ) \n 
if cursor . document ( ) . characterAt ( cursor . position ( ) ) != : \n 
~~ context = self . _get_context ( cursor ) \n 
if not context : \n 
~~ name = . join ( context ) \n 
msg_id = self . kernel_manager . shell_channel . object_info ( name ) \n 
pos = self . _get_cursor ( ) . position ( ) \n 
self . _request_info [ ] = self . _CallTipRequest ( msg_id , pos ) \n 
~~ def _complete ( self ) : \n 
context = self . _get_context ( ) \n 
if context : \n 
~~~ msg_id = self . kernel_manager . shell_channel . complete ( \n 
info = self . _CompletionRequest ( msg_id , pos ) \n 
self . _request_info [ ] = info \n 
~~ ~~ def _get_banner ( self ) : \n 
return banner % ( sys . version , sys . platform ) \n 
~~ def _get_context ( self , cursor = None ) : \n 
if cursor is None : \n 
~~~ cursor = self . _get_cursor ( ) \n 
~~ cursor . movePosition ( QtGui . QTextCursor . StartOfBlock , \n 
QtGui . QTextCursor . KeepAnchor ) \n 
text = cursor . selection ( ) . toPlainText ( ) \n 
return self . _completion_lexer . get_context ( text ) \n 
~~ def _process_execute_abort ( self , msg ) : \n 
~~ def _process_execute_error ( self , msg ) : \n 
content = msg [ ] \n 
if content [ ] == : \n 
~~~ keepkernel = content [ ] == or content [ ] == \n 
self . _keep_kernel_on_exit = keepkernel \n 
self . exit_requested . emit ( ) \n 
~~~ traceback = . join ( content [ ] ) \n 
self . _append_plain_text ( traceback ) \n 
~~ ~~ def _process_execute_ok ( self , msg ) : \n 
payload = msg [ ] [ ] \n 
for item in payload : \n 
~~~ if not self . _process_execute_payload ( item ) : \n 
~~~ warning = \n 
print ( warning % repr ( item [ ] ) ) \n 
~~ ~~ ~~ def _process_execute_payload ( self , item ) : \n 
~~ def _show_interpreter_prompt ( self ) : \n 
self . _show_prompt ( ) \n 
~~ def _show_interpreter_prompt_for_reply ( self , msg ) : \n 
~~ def _document_contents_change ( self , position , removed , added ) : \n 
position += added \n 
if position == self . _get_cursor ( ) . position ( ) : \n 
~~~ self . _call_tip ( ) \n 
from timeit import default_timer as clock \n 
import wx \n 
if os . name == : \n 
~~ elif sys . platform == : \n 
~~ def stdin_ready ( ) : \n 
~~~ if os . name == : \n 
~~~ infds , outfds , erfds = select . select ( [ sys . stdin ] , [ ] , [ ] , 0 ) \n 
if infds : \n 
~~ ~~ elif sys . platform == : \n 
~~~ return msvcrt . kbhit ( ) \n 
~~ ~~ def inputhook_wx1 ( ) : \n 
~~~ app = wx . GetApp ( ) \n 
if app is not None : \n 
~~~ assert wx . Thread_IsMain ( ) \n 
evtloop = wx . EventLoop ( ) \n 
ea = wx . EventLoopActivator ( evtloop ) \n 
while evtloop . Pending ( ) : \n 
~~~ evtloop . Dispatch ( ) \n 
~~ app . ProcessIdle ( ) \n 
del ea \n 
~~ ~~ except KeyboardInterrupt : \n 
~~ class EventLoopTimer ( wx . Timer ) : \n 
wx . Timer . __init__ ( self ) \n 
~~ def Notify ( self ) : \n 
~~~ self . func ( ) \n 
~~ ~~ class EventLoopRunner ( object ) : \n 
~~~ def Run ( self , time ) : \n 
~~~ self . evtloop = wx . EventLoop ( ) \n 
self . timer = EventLoopTimer ( self . check_stdin ) \n 
self . timer . Start ( time ) \n 
self . evtloop . Run ( ) \n 
~~ def check_stdin ( self ) : \n 
~~~ if stdin_ready ( ) : \n 
~~~ self . timer . Stop ( ) \n 
self . evtloop . Exit ( ) \n 
~~ ~~ ~~ def inputhook_wx2 ( ) : \n 
elr = EventLoopRunner ( ) \n 
~~ def inputhook_wx3 ( ) : \n 
if not isinstance ( signal . getsignal ( signal . SIGINT ) , collections . Callable ) : \n 
~~~ signal . signal ( signal . SIGINT , signal . default_int_handler ) \n 
~~ evtloop = wx . EventLoop ( ) \n 
t = clock ( ) \n 
while not stdin_ready ( ) : \n 
~~~ while evtloop . Pending ( ) : \n 
~~~ t = clock ( ) \n 
evtloop . Dispatch ( ) \n 
used_time = clock ( ) - t \n 
if used_time > 5 * 60.0 : \n 
~~~ time . sleep ( 5.0 ) \n 
~~ elif used_time > 10.0 : \n 
~~~ time . sleep ( 1.0 ) \n 
~~ elif used_time > 0.1 : \n 
~~~ time . sleep ( 0.05 ) \n 
~~~ time . sleep ( 0.001 ) \n 
~~ ~~ del ea \n 
~~ inputhook_wx = inputhook_wx3 \n 
from pymongo import Connection \n 
from pymongo . binary import Binary \n 
from IPython . utils . traitlets import Dict , List , Unicode , Instance \n 
from . dictdb import BaseDB \n 
class MongoDB ( BaseDB ) : \n 
connection_args = List ( config = True , \n 
connection_kwargs = Dict ( config = True , \n 
database = Unicode ( config = True , \n 
~~~ super ( MongoDB , self ) . __init__ ( ** kwargs ) \n 
if self . _connection is None : \n 
~~~ self . _connection = Connection ( * self . connection_args , ** self . connection_kwargs ) \n 
~~ if not self . database : \n 
~~~ self . database = self . session \n 
~~ self . _db = self . _connection [ self . database ] \n 
self . _records = self . _db [ ] \n 
self . _records . ensure_index ( , unique = True ) \n 
~~ def _binary_buffers ( self , rec ) : \n 
~~~ for key in ( , ) : \n 
~~~ if rec . get ( key , None ) : \n 
~~~ rec [ key ] = list ( map ( Binary , rec [ key ] ) ) \n 
~~ ~~ return rec \n 
~~ def add_record ( self , msg_id , rec ) : \n 
rec = self . _binary_buffers ( rec ) \n 
self . _records . insert ( rec ) \n 
~~ def get_record ( self , msg_id ) : \n 
r = self . _records . find_one ( { : msg_id } ) \n 
if not r : \n 
~~~ raise KeyError ( msg_id ) \n 
~~ def update_record ( self , msg_id , rec ) : \n 
self . _records . update ( { : msg_id } , { : rec } ) \n 
~~ def drop_matching_records ( self , check ) : \n 
self . _records . remove ( check ) \n 
~~ def drop_record ( self , msg_id ) : \n 
self . _records . remove ( { : msg_id } ) \n 
~~ def find_records ( self , check , keys = None ) : \n 
if keys and not in keys : \n 
~~~ keys . append ( ) \n 
~~ matches = list ( self . _records . find ( check , keys ) ) \n 
for rec in matches : \n 
~~~ rec . pop ( ) \n 
~~ return matches \n 
~~ def get_history ( self ) : \n 
cursor = self . _records . find ( { } , { : 1 } ) . sort ( ) \n 
return [ rec [ ] for rec in cursor ] \n 
~~ ~~ from IPython . core import ipapi \n 
from IPython . core import macro \n 
ip = ipapi . get ( ) \n 
import os , pprint \n 
def export ( filename = None ) : \n 
~~~ lines = [ , , ] \n 
vars = ip . db . keys ( ) \n 
vars . sort ( ) \n 
varstomove = [ ] \n 
get = ip . db . get \n 
macros = [ ] \n 
variables = [ ] \n 
for var in vars : \n 
~~~ k = os . path . basename ( var ) \n 
v = get ( var ) \n 
if k . startswith ( ) : \n 
~~ if isinstance ( v , macro . Macro ) : \n 
~~~ macros . append ( ( k , v ) ) \n 
~~ if type ( v ) in [ int , str , float ] : \n 
~~~ variables . append ( ( k , v ) ) \n 
~~ ~~ if macros : \n 
~~~ lines . extend ( [ , ] ) \n 
~~ for k , v in macros : \n 
~~~ lines . append ( "ip.defmacro(\'%s\'," % k ) \n 
for line in v . value . splitlines ( ) : \n 
~~~ lines . append ( + repr ( line + ) ) \n 
~~ lines . extend ( [ , ] ) \n 
~~ if variables : \n 
~~~ lines . extend ( [ , , ] ) \n 
for k , v in variables : \n 
~~~ varstomove . append ( k ) \n 
lines . append ( % ( k , repr ( v ) ) ) \n 
~~ lines . append ( \'ip.push("%s")\' % ( . join ( varstomove ) ) ) \n 
~~ bkms = ip . db . get ( , { } ) \n 
if bkms : \n 
~~ aliases = ip . db . get ( , { } ) \n 
if aliases : \n 
for k , v in list ( aliases . items ( ) ) : \n 
~~ except ( AttributeError , TypeError ) : \n 
~~ ~~ ~~ env = ip . db . get ( ) \n 
if env : \n 
~~ out = . join ( lines ) \n 
~~~ open ( filename , ) . write ( out ) \n 
~~~ print ( out ) \n 
from doctest import DocTestFinder , DocTestRunner , TestResults \n 
if sys . version [ 0 ] == : \n 
~~~ from . _paramtestpy2 import ParametricTestCase \n 
~~~ from . _paramtestpy3 import ParametricTestCase \n 
~~ def count_failures ( runner ) : \n 
return [ TestResults ( f , t ) for f , t in list ( runner . _name2ft . values ( ) ) if f > 0 ] \n 
~~ class IPython2PythonConverter ( object ) : \n 
~~~ self . rps1 = re . compile ( ) \n 
self . rps2 = re . compile ( ) \n 
self . rout = re . compile ( ) \n 
self . pyps1 = \n 
self . pyps2 = \n 
self . rpyps1 = re . compile ( % self . pyps1 ) \n 
self . rpyps2 = re . compile ( % self . pyps2 ) \n 
~~ def __call__ ( self , ds ) : \n 
from . import globalipapp \n 
pyps1 = \n 
pyps2 = \n 
pyout = \n 
dnew = ds \n 
dnew = self . rps1 . sub ( pyps1 , dnew ) \n 
dnew = self . rps2 . sub ( pyps2 , dnew ) \n 
dnew = self . rout . sub ( pyout , dnew ) \n 
ip = globalipapp . get_ipython ( ) \n 
out = [ ] \n 
newline = out . append \n 
for line in dnew . splitlines ( ) : \n 
~~~ mps1 = self . rpyps1 . match ( line ) \n 
if mps1 is not None : \n 
~~~ prompt , text = mps1 . groups ( ) \n 
newline ( prompt + ip . prefilter ( text , False ) ) \n 
~~ mps2 = self . rpyps2 . match ( line ) \n 
if mps2 is not None : \n 
~~~ prompt , text = mps2 . groups ( ) \n 
newline ( prompt + ip . prefilter ( text , True ) ) \n 
~~ newline ( line ) \n 
return . join ( out ) \n 
~~ ~~ class Doc2UnitTester ( object ) : \n 
def __init__ ( self , verbose = False ) : \n 
self . finder = DocTestFinder ( verbose = verbose , recurse = False ) \n 
~~ def __call__ ( self , func ) : \n 
d2u = self \n 
if func . __doc__ is not None : \n 
~~~ func . __doc__ = ip2py ( func . __doc__ ) \n 
~~ class Tester ( unittest . TestCase ) : \n 
~~~ runner = DocTestRunner ( verbose = d2u . verbose ) \n 
list ( map ( runner . run , d2u . finder . find ( func , func . __name__ ) ) ) \n 
failed = count_failures ( runner ) \n 
if failed : \n 
~~~ if len ( failed ) > 1 : \n 
raise ValueError ( err ) \n 
~~ self . fail ( % str ( failed [ 0 ] ) ) \n 
~~ ~~ ~~ Tester . __name__ = func . __name__ \n 
return Tester \n 
~~ ~~ def ipdocstring ( func ) : \n 
~~ ipdoctest = Doc2UnitTester ( ) \n 
ip2py = IPython2PythonConverter ( ) \n 
def flag_calls ( func ) : \n 
def wrapper ( * args , ** kw ) : \n 
~~~ wrapper . called = False \n 
out = func ( * args , ** kw ) \n 
wrapper . called = True \n 
~~ wrapper . called = False \n 
wrapper . __doc__ = func . __doc__ \n 
return wrapper \n 
from unittest import TestCase \n 
import nose . tools as nt \n 
from IPython . utils . process import ( find_cmd , FindCmdError , arg_split , \n 
system , getoutput , getoutputerror ) \n 
from IPython . testing import tools as tt \n 
def test_find_cmd_python ( ) : \n 
nt . assert_equals ( find_cmd ( ) , sys . executable ) \n 
~~ @ dec . skip_win32 \n 
def test_find_cmd_ls ( ) : \n 
path = find_cmd ( ) \n 
nt . assert_true ( path . endswith ( ) ) \n 
~~ def has_pywin32 ( ) : \n 
~~~ import win32api \n 
def test_find_cmd_pythonw ( ) : \n 
~~ @ dec . onlyif ( lambda : sys . platform != or has_pywin32 ( ) , \n 
def test_find_cmd_fail ( ) : \n 
nt . assert_raises ( FindCmdError , find_cmd , ) \n 
~~ def test_arg_split ( ) : \n 
tests = [ [ , [ ] ] , \n 
[ , [ ] ] , \n 
[ , [ , ] ] , \n 
for argstr , argv in tests : \n 
~~~ nt . assert_equal ( arg_split ( argstr ) , argv ) \n 
~~ ~~ class SubProcessTestCase ( TestCase , tt . TempFileMixin ) : \n 
"sys.stdout.flush()" , \n 
"sys.stderr.flush()" ] \n 
self . mktmp ( . join ( lines ) ) \n 
~~ def test_system ( self ) : \n 
~~ def test_getoutput ( self ) : \n 
self . assertEquals ( out , ) \n 
self . assertEquals ( err , ) \n 
import uuid \n 
import zmq \n 
from zmq . tests import BaseZMQTestCase \n 
from zmq . eventloop . zmqstream import ZMQStream \n 
from IPython . zmq import session as ss \n 
class SessionTestCase ( BaseZMQTestCase ) : \n 
~~~ BaseZMQTestCase . setUp ( self ) \n 
self . session = ss . Session ( ) \n 
~~ ~~ class TestSession ( SessionTestCase ) : \n 
~~~ def test_msg ( self ) : \n 
msg = self . session . msg ( ) \n 
thekeys = set ( . split ( ) ) \n 
s = set ( msg . keys ( ) ) \n 
self . assertEquals ( s , thekeys ) \n 
self . assertTrue ( isinstance ( msg [ ] , dict ) ) \n 
self . assertEquals ( msg [ ] , ) \n 
~~ def test_args ( self ) : \n 
s = self . session \n 
self . assertTrue ( s . pack is ss . default_packer ) \n 
self . assertTrue ( s . unpack is ss . default_unpacker ) \n 
self . assertEquals ( s . username , os . environ . get ( , ) ) \n 
s = ss . Session ( ) \n 
self . assertRaises ( TypeError , ss . Session , pack = ) \n 
self . assertRaises ( TypeError , ss . Session , unpack = ) \n 
u = str ( uuid . uuid4 ( ) ) \n 
s = ss . Session ( username = , session = u ) \n 
self . assertEquals ( s . session , u ) \n 
self . assertEquals ( s . username , ) \n 
~~ def test_tracking ( self ) : \n 
a , b = self . create_bound_pair ( zmq . PAIR , zmq . PAIR ) \n 
stream = ZMQStream ( a ) \n 
msg = s . send ( a , , track = False ) \n 
self . assertTrue ( msg [ ] is None ) \n 
msg = s . send ( a , , track = True ) \n 
self . assertTrue ( isinstance ( msg [ ] , zmq . MessageTracker ) ) \n 
M = zmq . Message ( , track = True ) \n 
msg = s . send ( a , , buffers = [ M ] , track = True ) \n 
t = msg [ ] \n 
self . assertTrue ( isinstance ( t , zmq . MessageTracker ) ) \n 
self . assertRaises ( zmq . NotDone , t . wait , .1 ) \n 
del M \n 
~~ def test_unique_msg_ids ( self ) : \n 
ids = set ( ) \n 
for i in range ( 2 ** 12 ) : \n 
~~~ h = self . session . msg_header ( ) \n 
msg_id = h [ ] \n 
self . assertTrue ( msg_id not in ids ) \n 
ids . add ( msg_id ) \n 
~~ ~~ def test_feed_identities ( self ) : \n 
content = dict ( code = , stuff = object ( ) ) \n 
themsg = self . session . msg ( , content = content ) \n 
pmsg = theids \n 
from IPython . parallel import Client \n 
def sleep_and_echo ( t , msg ) : \n 
time . sleep ( t ) \n 
return msg \n 
~~ view = rc . load_balanced_view ( ) \n 
world = view . apply_async ( sleep_and_echo , 3 , ) \n 
hello = view . apply_async ( sleep_and_echo , 2 , ) \n 
print ( hello . get ( ) , world . get ( ) ) \n 
from sklearn . preprocessing import OneHotEncoder \n 
from sklearn import cross_validation , metrics , datasets \n 
from neupy import algorithms , layers , environment \n 
environment . reproducible ( ) \n 
theano . config . floatX = \n 
mnist = datasets . fetch_mldata ( ) \n 
target_scaler = OneHotEncoder ( ) \n 
target = mnist . target . reshape ( ( - 1 , 1 ) ) \n 
target = target_scaler . fit_transform ( target ) . todense ( ) \n 
data = mnist . data / 255. \n 
data = data - data . mean ( axis = 0 ) \n 
n_samples = data . shape [ 0 ] \n 
data = data . reshape ( ( n_samples , 1 , 28 , 28 ) ) \n 
x_train , x_test , y_train , y_test = cross_validation . train_test_split ( \n 
data . astype ( np . float32 ) , \n 
target . astype ( np . float32 ) , \n 
train_size = ( 6 / 7. ) \n 
network = algorithms . Adadelta ( \n 
layers . Convolution ( ( 32 , 1 , 3 , 3 ) ) , \n 
layers . Relu ( ) , \n 
layers . Convolution ( ( 48 , 32 , 3 , 3 ) ) , \n 
layers . MaxPooling ( ( 2 , 2 ) ) , \n 
layers . Dropout ( 0.2 ) , \n 
layers . Reshape ( ) , \n 
layers . Relu ( 6912 ) , \n 
layers . Dropout ( 0.3 ) , \n 
layers . Softmax ( 200 ) , \n 
layers . ArgmaxOutput ( 10 ) , \n 
error = , \n 
step = 1.0 , \n 
shuffle_data = True , \n 
epochs_step_minimizator = 8 , \n 
addons = [ algorithms . SimpleStepMinimization ] , \n 
network . architecture ( ) \n 
network . train ( x_train , y_train , x_test , y_test , epochs = 6 ) \n 
y_predicted = network . predict ( x_test ) \n 
y_test_labels = np . asarray ( y_test . argmax ( axis = 1 ) ) . reshape ( len ( y_test ) ) \n 
print ( metrics . classification_report ( y_test_labels , y_predicted ) ) \n 
score = metrics . accuracy_score ( y_test_labels , y_predicted ) \n 
from abc import abstractmethod \n 
from neupy . core . base import BaseSkeleton \n 
from neupy . core . config import ConfigurableABC \n 
__all__ = ( , ) \n 
class BaseEnsemble ( BaseSkeleton , ConfigurableABC ) : \n 
def __init__ ( self , networks ) : \n 
~~~ self . networks = networks \n 
if len ( self . networks ) < 2 : \n 
~~ ~~ @ abstractmethod \n 
def train ( self , input_data , target_data , * args , ** kwargs ) : \n 
~~ @ abstractmethod \n 
def predict ( self , input_data ) : \n 
classname = self . __class__ . __name__ , \n 
networks = . join ( map ( repr , self . networks ) ) \n 
from numpy import ( zeros , argmin , argwhere , take , sum as np_sum , \n 
any as np_any , abs as np_abs ) \n 
from numpy . linalg import norm \n 
from neupy . utils import format_data \n 
from neupy . core . properties import IntProperty \n 
from neupy . algorithms . gd import NoStepSelection \n 
from neupy . network . base import BaseNetwork \n 
from neupy . network . learning import UnsupervisedLearning \n 
class RBFKMeans ( NoStepSelection , UnsupervisedLearning , BaseNetwork ) : \n 
n_clusters = IntProperty ( minval = 2 ) \n 
def __init__ ( self , ** options ) : \n 
~~~ self . centers = None \n 
super ( RBFKMeans , self ) . __init__ ( ** options ) \n 
~~ def predict ( self , input_data ) : \n 
~~~ input_data = format_data ( input_data ) \n 
centers = self . centers \n 
classes = zeros ( ( input_data . shape [ 0 ] , 1 ) ) \n 
for i , value in enumerate ( input_data ) : \n 
~~~ classes [ i ] = argmin ( norm ( centers - value , axis = 1 ) ) \n 
~~ return classes \n 
~~ def train_epoch ( self , input_train , target_train ) : \n 
~~~ centers = self . centers \n 
old_centers = centers . copy ( ) \n 
output_train = self . predict ( input_train ) \n 
for i , center in enumerate ( centers ) : \n 
~~~ positions = argwhere ( output_train [ : , 0 ] == i ) \n 
if not np_any ( positions ) : \n 
~~ class_data = take ( input_train , positions , axis = 0 ) \n 
centers [ i , : ] = ( 1 / len ( class_data ) ) * np_sum ( class_data , axis = 0 ) \n 
~~ return np_abs ( old_centers - centers ) \n 
~~ def train ( self , input_train , epsilon = 1e-5 , epochs = 100 ) : \n 
~~~ n_clusters = self . n_clusters \n 
input_train = format_data ( input_train ) \n 
if input_train . shape [ 0 ] <= n_clusters : \n 
~~ self . centers = input_train [ : n_clusters , : ] . copy ( ) \n 
super ( RBFKMeans , self ) . train ( input_train , epsilon = epsilon , \n 
epochs = epochs ) \n 
def format_interval ( time_in_seconds ) : \n 
mins , seconds = divmod ( int ( time_in_seconds ) , 60 ) \n 
hours , minutes = divmod ( mins , 60 ) \n 
if hours > 0 : \n 
~~~ return . format ( hours , minutes , seconds ) \n 
~~ return . format ( minutes , seconds ) \n 
~~ def format_error ( error ) : \n 
if error is None : \n 
~~ if isinstance ( error , collections . Iterable ) : \n 
~~~ error = np . atleast_1d ( error ) . item ( 0 ) \n 
~~ return . format ( error ) \n 
~~ def format_meter ( n_finished , n_total , elapsed , error = None ) : \n 
if n_finished > n_total : \n 
~~~ n_total = None \n 
~~ elapsed_str = format_interval ( elapsed ) \n 
if n_total is not None : \n 
~~~ frac = float ( n_finished ) / n_total \n 
n_bars = 10 \n 
bar_length = int ( frac * n_bars ) \n 
bar = * bar_length + * ( n_bars - bar_length ) \n 
percentage = % ( frac * 100 ) \n 
left_str = format_interval ( \n 
elapsed / n_finished * ( n_total - n_finished ) \n 
) if n_finished else \n 
return . format ( \n 
bar , n_finished , n_total , percentage , \n 
elapsed_str , left_str , format_error ( error ) \n 
~~ return . format ( n_finished , elapsed_str ) \n 
~~ class StatusPrinter ( object ) : \n 
~~~ def __init__ ( self , file ) : \n 
~~~ self . file = file \n 
self . last_printed_len = 0 \n 
~~~ n_spaces = max ( self . last_printed_len - len ( text ) , 0 ) \n 
self . file . write ( + text + * n_spaces ) \n 
self . file . flush ( ) \n 
self . last_printed_len = len ( text ) \n 
~~ def clean ( self ) : \n 
~~~ self . write ( ) \n 
self . file . write ( ) \n 
~~ ~~ def progressbar ( iterable , desc = , total = None , file = sys . stderr , \n 
mininterval = 0.05 , miniters = 1 , init_interval = 1. ) : \n 
if total is None : \n 
~~~ total = len ( iterable ) \n 
~~~ total = None \n 
~~ ~~ prefix = desc + if desc else \n 
printer = StatusPrinter ( file ) \n 
status = prefix + format_meter ( 0 , total , 0 ) \n 
timer = threading . Timer ( init_interval , printer . write , args = [ status ] ) \n 
timer . start ( ) \n 
start_time = last_print_time = time . time ( ) \n 
last_print_n = 0 \n 
n = 0 \n 
~~~ for obj in iterable : \n 
~~~ error = ( yield ) \n 
yield obj \n 
timer . cancel ( ) \n 
n += 1 \n 
if n - last_print_n >= miniters : \n 
~~~ current_time = time . time ( ) \n 
if ( current_time - last_print_time ) >= mininterval : \n 
~~~ time_delta = current_time - start_time \n 
formated_str = format_meter ( n , total , time_delta , error ) \n 
printer . write ( prefix + formated_str ) \n 
last_print_n = n \n 
last_print_time = current_time \n 
~~~ timer . cancel ( ) \n 
printer . clean ( ) \n 
from neupy import algorithms , layers \n 
from base import BaseTestCase \n 
input_data = np . array ( [ \n 
[ 0 , 1 , - 1 , - 1 ] , \n 
[ 1 , 1 , - 1 , - 1 ] , \n 
class HebbRuleTestCase ( BaseTestCase ) : \n 
~~~ super ( HebbRuleTestCase , self ) . setUp ( ) \n 
self . default_properties = dict ( \n 
n_inputs = 4 , \n 
n_outputs = 1 , \n 
n_unconditioned = 1 , \n 
weight = np . array ( [ [ 3 , 0 , 0 , 0 ] ] ) . T , \n 
~~ def test_learning_process ( self ) : \n 
~~~ inet = algorithms . Instar ( \n 
step = 1 , \n 
verbose = False , \n 
** self . default_properties \n 
inet . train ( input_data , epochs = 10 ) \n 
test_input = np . array ( [ [ 0 , 1 , - 1 , - 1 ] ] ) \n 
self . assertEqual ( inet . predict ( test_input ) , 1 ) \n 
np . testing . assert_array_equal ( \n 
inet . weight , \n 
np . array ( [ [ 3 , 1 , - 1 , - 1 ] ] ) . T \n 
~~ def test_multiple_outputs ( self ) : \n 
~~~ input_data = np . array ( [ \n 
[ - 0.1961 , 0.9806 ] , \n 
innet = algorithms . Instar ( \n 
n_inputs = 2 , \n 
n_outputs = 3 , \n 
weight = np . array ( [ \n 
[ 0.7071 , 0.7071 , - 1 ] , \n 
[ - 0.7071 , 0.7071 , 0 ] , \n 
step = 0.5 , \n 
innet . train ( input_data , epochs = 1 ) \n 
np . testing . assert_array_almost_equal ( \n 
innet . weight , \n 
[ - 0.5704 , 0.8439 , 0.1368 ] \n 
decimal = 4 \n 
~~ def test_train_different_inputs ( self ) : \n 
~~~ self . assertInvalidVectorTrain ( \n 
algorithms . Instar ( \n 
np . array ( [ [ 0 , 1 , - 1 , - 1 ] ] ) , \n 
is_feature1d = False , \n 
~~ def test_predict_different_inputs ( self ) : \n 
self . assertInvalidVectorPred ( inet , np . array ( [ 0 , 1 , - 1 , - 1 ] ) , 1 , \n 
is_feature1d = False ) \n 
from sklearn import datasets , metrics \n 
from sklearn . cross_validation import train_test_split \n 
from neupy import algorithms \n 
class GRNNTestCase ( BaseTestCase ) : \n 
~~~ def test_handle_errors ( self ) : \n 
~~~ algorithms . GRNN ( verbose = False ) . train ( \n 
np . array ( [ [ 0 ] , [ 0 ] ] ) , np . array ( [ 0 ] ) \n 
~~ with self . assertRaises ( ValueError ) : \n 
np . array ( [ [ 0 ] , [ 0 ] ] ) , np . array ( [ [ 0 ] ] ) \n 
~~ with self . assertRaises ( AttributeError ) : \n 
~~~ algorithms . GRNN ( verbose = False ) . train_epoch ( ) \n 
~~~ grnet = algorithms . GRNN ( verbose = False ) \n 
grnet . train ( np . array ( [ [ 0 ] , [ 0 ] ] ) , np . array ( [ 0 ] ) ) \n 
grnet . predict ( np . array ( [ [ 0 ] ] ) ) \n 
~~ ~~ def test_simple_grnn ( self ) : \n 
~~~ dataset = datasets . load_diabetes ( ) \n 
x_train , x_test , y_train , y_test = train_test_split ( \n 
dataset . data , dataset . target , train_size = 0.7 \n 
x_train_before = x_train . copy ( ) \n 
x_test_before = x_test . copy ( ) \n 
y_train_before = y_train . copy ( ) \n 
grnnet = algorithms . GRNN ( std = 0.1 , verbose = False ) \n 
grnnet . train ( x_train , y_train ) \n 
result = grnnet . predict ( x_test ) \n 
error = metrics . mean_absolute_error ( result , y_test ) \n 
old_result = result . copy ( ) \n 
self . assertAlmostEqual ( error , 46.3358 , places = 4 ) \n 
np . testing . assert_array_equal ( x_train , x_train_before ) \n 
np . testing . assert_array_equal ( x_test , x_test_before ) \n 
np . testing . assert_array_equal ( y_train , y_train_before ) \n 
x_train [ : , : ] = 0 \n 
total_classes_prob = np . round ( result . sum ( axis = 1 ) , 10 ) \n 
np . testing . assert_array_almost_equal ( result , old_result ) \n 
algorithms . GRNN ( verbose = False ) , \n 
np . array ( [ 1 , 2 , 3 ] ) , \n 
np . array ( [ 1 , 2 , 3 ] ) \n 
~~~ grnnet = algorithms . GRNN ( verbose = False ) \n 
data = np . array ( [ [ 1 , 2 , 3 ] ] ) . T \n 
target = np . array ( [ [ 1 , 2 , 3 ] ] ) . T \n 
grnnet . train ( data , target ) \n 
self . assertInvalidVectorPred ( grnnet , data . ravel ( ) , target , \n 
decimal = 2 ) \n 
~~ ~~ import math \n 
from scipy import stats \n 
from neupy . utils import asfloat \n 
from neupy import layers \n 
from neupy . algorithms import GradientDescent \n 
from neupy . layers . connections import NetworkConnectionError \n 
from neupy . layers import * \n 
class LayersBasicsTestCase ( BaseTestCase ) : \n 
~~~ def test_without_output_layer ( self ) : \n 
~~~ with self . assertRaises ( NetworkConnectionError ) : \n 
~~~ GradientDescent ( layers . Sigmoid ( 10 ) > layers . Sigmoid ( 1 ) ) \n 
~~ ~~ def test_list_of_layers ( self ) : \n 
~~~ bpnet = GradientDescent ( [ Sigmoid ( 2 ) , Sigmoid ( 3 ) , \n 
Sigmoid ( 1 ) , Output ( 10 ) ] ) \n 
[ layer . size for layer in bpnet . all_layers ] , \n 
[ 2 , 3 , 1 , 10 ] \n 
~~ def test_layers_iteratinos ( self ) : \n 
~~~ network = GradientDescent ( ( 2 , 2 , 1 ) ) \n 
layers = list ( network . all_layers ) \n 
output_layer = layers . pop ( ) \n 
self . assertIsNone ( output_layer . relate_to_layer ) \n 
for layer in layers : \n 
~~~ self . assertIsNotNone ( layer . relate_to_layer ) \n 
~~ ~~ def test_connection_initializations ( self ) : \n 
~~~ possible_connections = ( \n 
( 2 , 3 , 1 ) , \n 
[ Sigmoid ( 2 ) , Tanh ( 3 ) , Output ( 1 ) ] , \n 
Relu ( 2 ) > Tanh ( 10 ) > Output ( 1 ) , \n 
for connection in possible_connections : \n 
~~~ network = GradientDescent ( connection ) \n 
self . assertEqual ( len ( network . all_layers ) , 3 ) \n 
def test_recurrent_connections ( self ) : \n 
~~~ inp = Sigmoid ( 2 ) \n 
hd = [ Sigmoid ( 2 ) , Sigmoid ( 2 ) ] \n 
out = Output ( 1 ) \n 
GradientDescent ( \n 
connection = ( \n 
inp > hd [ 0 ] > out , \n 
hd [ 0 ] > hd [ 1 ] , \n 
hd [ 1 ] > hd [ 0 ] , \n 
~~ def test_activation_layers_without_size ( self ) : \n 
~~~ input_data = np . array ( [ 1 , 2 , - 1 , 10 ] ) \n 
expected_output = np . array ( [ 1 , 2 , 0 , 10 ] ) \n 
layer = layers . Relu ( ) \n 
actual_output = layer . output ( input_data ) \n 
np . testing . assert_array_equal ( actual_output , expected_output ) \n 
~~ ~~ class HiddenLayersOperationsTestCase ( BaseTestCase ) : \n 
~~~ def test_sigmoid_layer ( self ) : \n 
~~~ layer1 = Sigmoid ( 1 ) \n 
self . assertGreater ( 1 , layer1 . activation_function ( 1 ) . eval ( ) ) \n 
~~ def test_hard_sigmoid_layer ( self ) : \n 
~~~ layer1 = HardSigmoid ( 6 ) \n 
test_value = asfloat ( np . array ( [ [ - 3 , - 2 , - 1 , 0 , 1 , 2 ] ] ) ) \n 
expected = np . array ( [ [ 0 , 0.1 , 0.3 , 0.5 , 0.7 , 0.9 ] ] ) \n 
x = T . matrix ( ) \n 
output = layer1 . activation_function ( x ) . eval ( { x : test_value } ) \n 
np . testing . assert_array_almost_equal ( output , expected ) \n 
~~ def test_step_layer ( self ) : \n 
~~~ layer1 = Step ( 1 ) \n 
input_vector = theano . shared ( np . array ( [ - 10 , - 1 , 0 , 1 , 10 ] ) ) \n 
expected = np . array ( [ 0 , 0 , 0 , 1 , 1 ] ) \n 
output = layer1 . activation_function ( input_vector ) . eval ( ) \n 
np . testing . assert_array_equal ( output , expected ) \n 
~~ def test_linear_layer ( self ) : \n 
~~~ layer = Linear ( 1 ) \n 
self . assertEqual ( layer . activation_function ( 1 ) , 1 ) \n 
~~ def test_tanh_layer ( self ) : \n 
~~~ layer1 = Tanh ( 1 ) \n 
~~ def test_relu_layer ( self ) : \n 
~~~ layer = Relu ( 1 ) \n 
self . assertEqual ( 0 , layer . activation_function ( - 10 ) ) \n 
self . assertEqual ( 0 , layer . activation_function ( 0 ) ) \n 
self . assertEqual ( 10 , layer . activation_function ( 10 ) ) \n 
~~ def test_softplus_layer ( self ) : \n 
~~~ layer = Softplus ( 1 ) \n 
self . assertAlmostEqual ( \n 
math . log ( 2 ) , \n 
layer . activation_function ( 0 ) . eval ( ) \n 
~~ def test_softmax_layer ( self ) : \n 
~~~ test_input = np . array ( [ [ 0.5 , 0.5 , 0.1 ] ] ) \n 
softmax_layer = Softmax ( 3 ) \n 
correct_result = np . array ( [ [ 0.37448695 , 0.37448695 , 0.25102611 ] ] ) \n 
correct_result , \n 
softmax_layer . activation_function ( test_input ) . eval ( ) \n 
~~ def test_dropout_layer ( self ) : \n 
~~~ test_input = np . ones ( ( 50 , 20 ) ) \n 
dropout_layer = Dropout ( proba = 0.5 ) \n 
layer_output = dropout_layer . output ( test_input ) . eval ( ) \n 
self . assertGreater ( layer_output . sum ( ) , 900 ) \n 
self . assertLess ( layer_output . sum ( ) , 1100 ) \n 
self . assertTrue ( np . all ( \n 
np . bitwise_or ( layer_output == 0 , layer_output == 2 ) \n 
~~ def test_reshape_layer ( self ) : \n 
~~~ x = np . random . random ( ( 5 , 4 , 3 , 2 , 1 ) ) \n 
reshape_layer = Reshape ( ) \n 
y = reshape_layer . output ( x ) . eval ( ) \n 
self . assertEqual ( y . shape , ( 5 , 4 * 3 * 2 * 1 ) ) \n 
~~ ~~ import argparse , collections \n 
import nltk . corpus \n 
from nltk . tree import Tree \n 
from nltk . corpus . util import LazyCorpusLoader \n 
from nltk_trainer import load_corpus_reader , simplify_wsj_tag \n 
from nltk_trainer . chunking . transforms import node_label \n 
######################################## \n 
parser = argparse . ArgumentParser ( description = , \n 
formatter_class = argparse . RawTextHelpFormatter ) \n 
parser . add_argument ( , \n 
parser . add_argument ( , default = 1 , type = int , \n 
corpus_group = parser . add_argument_group ( ) \n 
corpus_group . add_argument ( , default = None , \n 
if simplify_wsj_tag : \n 
~~~ corpus_group . add_argument ( , action = , default = False , \n 
~~ sort_group = parser . add_argument_group ( ) \n 
sort_group . add_argument ( , default = , choices = [ , ] , \n 
sort_group . add_argument ( , action = , default = False , \n 
################### \n 
chunked_corpus = load_corpus_reader ( args . corpus , reader = args . reader , fileids = args . fileids ) \n 
if not chunked_corpus : \n 
~~ if args . trace : \n 
~~~ print ( % args . corpus ) \n 
############## \n 
~~ wc = 0 \n 
tag_counts = collections . defaultdict ( int ) \n 
iob_counts = collections . defaultdict ( int ) \n 
tag_iob_counts = collections . defaultdict ( lambda : collections . defaultdict ( int ) ) \n 
word_set = set ( ) \n 
for obj in chunked_corpus . chunked_words ( ) : \n 
~~~ if isinstance ( obj , Tree ) : \n 
~~~ label = node_label ( obj ) \n 
iob_counts [ label ] += 1 \n 
for word , tag in obj . leaves ( ) : \n 
~~~ wc += 1 \n 
word_set . add ( word ) \n 
tag_counts [ tag ] += 1 \n 
tag_iob_counts [ tag ] [ label ] += 1 \n 
~~~ word , tag = obj \n 
wc += 1 \n 
############ \n 
~~ ~~ print ( % wc ) \n 
print ( % len ( word_set ) ) \n 
print ( % len ( tag_counts ) ) \n 
print ( % len ( iob_counts ) ) \n 
if args . sort == : \n 
~~~ sort_key = lambda tc : tc [ 0 ] \n 
~~ elif args . sort == : \n 
~~~ sort_key = lambda tc : tc [ 1 ] \n 
~~~ raise ValueError ( % args . sort ) \n 
~~ line1 = \n 
line2 = \n 
iobs = sorted ( iob_counts . keys ( ) ) \n 
for iob in iobs : \n 
~~~ line1 += % iob \n 
line2 += % ( * len ( iob ) ) \n 
~~ print ( line1 ) \n 
print ( line2 ) \n 
for tag , count in sorted ( tag_counts . items ( ) , key = sort_key , reverse = args . reverse ) : \n 
~~~ iob_counts = [ str ( tag_iob_counts [ tag ] [ iob ] ) . rjust ( 4 + len ( iob ) ) for iob in iobs ] \n 
print ( . join ( [ tag . ljust ( 7 ) , str ( count ) . rjust ( 9 ) ] + iob_counts ) ) \n 
import argparse , os . path \n 
import nltk . data , nltk . tag \n 
from nltk_trainer import load_corpus_reader \n 
from nltk_trainer . writer . chunked import ChunkedCorpusWriter \n 
parser = argparse . ArgumentParser ( description = ) \n 
parser . add_argument ( , default = nltk . tag . _POS_TAGGER , \n 
corpus_group . add_argument ( , \n 
corpus_group . add_argument ( , default = , \n 
source_corpus = load_corpus_reader ( args . source_corpus , reader = args . reader , \n 
fileids = args . fileids , encoding = , sent_tokenizer = args . sent_tokenizer , \n 
word_tokenizer = args . word_tokenizer ) \n 
if not source_corpus : \n 
~~~ print % args . source_corpus \n 
~~~ print % args . tagger \n 
~~~ tagger = nltk . data . load ( args . tagger ) \n 
~~ except LookupError : \n 
~~ tagger = pickle . load ( open ( os . path . expanduser ( args . tagger ) ) ) \n 
############# \n 
~~ with ChunkedCorpusWriter ( fileids = source_corpus . fileids ( ) , path = args . target_corpus ) as writer : \n 
~~~ for fileid in source_corpus . fileids ( ) : \n 
~~~ paras = source_corpus . paras ( fileids = [ fileid ] ) \n 
tagged_paras = ( ( tagger . tag ( sent ) for sent in para ) for para in paras ) \n 
writer . write_paras ( tagged_paras , fileid = fileid ) version_info = ( 4 , 3 , 0 , ) \n 
~~ ~~ __version__ = . join ( map ( str , version_info ) ) \n 
protocol_version_info = ( 5 , 0 ) \n 
protocol_version = "%i.%i" % protocol_version_info \n 
pjoin = os . path . join \n 
from subprocess import PIPE \n 
from ipython_genutils . testing import decorators as dec \n 
from traitlets . config . loader import Config \n 
from jupyter_core import paths \n 
from jupyter_client import KernelManager \n 
from . . manager import start_new_kernel \n 
from . utils import test_env \n 
TIMEOUT = 30 \n 
class TestKernelManager ( TestCase ) : \n 
~~~ self . env_patch = test_env ( ) \n 
self . env_patch . start ( ) \n 
~~~ self . env_patch . stop ( ) \n 
~~ def _install_test_kernel ( self ) : \n 
~~~ kernel_dir = pjoin ( paths . jupyter_data_dir ( ) , , ) \n 
os . makedirs ( kernel_dir ) \n 
with open ( pjoin ( kernel_dir , ) , ) as f : \n 
~~~ f . write ( json . dumps ( { \n 
: [ sys . executable , \n 
} ) ) \n 
~~ ~~ def _get_tcp_km ( self ) : \n 
~~~ c = Config ( ) \n 
km = KernelManager ( config = c ) \n 
return km \n 
~~ def _get_ipc_km ( self ) : \n 
c . KernelManager . transport = \n 
c . KernelManager . ip = \n 
~~ def _run_lifecycle ( self , km ) : \n 
~~~ km . start_kernel ( stdout = PIPE , stderr = PIPE ) \n 
self . assertTrue ( km . is_alive ( ) ) \n 
km . restart_kernel ( now = True ) \n 
km . interrupt_kernel ( ) \n 
self . assertTrue ( isinstance ( km , KernelManager ) ) \n 
km . shutdown_kernel ( now = True ) \n 
~~ def test_tcp_lifecycle ( self ) : \n 
~~~ km = self . _get_tcp_km ( ) \n 
self . _run_lifecycle ( km ) \n 
def test_ipc_lifecycle ( self ) : \n 
~~~ km = self . _get_ipc_km ( ) \n 
~~ def test_get_connect_info ( self ) : \n 
cinfo = km . get_connection_info ( ) \n 
keys = sorted ( cinfo . keys ( ) ) \n 
expected = sorted ( [ \n 
self . assertEqual ( keys , expected ) \n 
def test_signal_kernel_subprocesses ( self ) : \n 
~~~ self . _install_test_kernel ( ) \n 
km , kc = start_new_kernel ( kernel_name = ) \n 
def execute ( cmd ) : \n 
~~~ kc . execute ( cmd ) \n 
reply = kc . get_shell_msg ( TIMEOUT ) \n 
content = reply [ ] \n 
self . assertEqual ( content [ ] , ) \n 
return content \n 
~~ self . addCleanup ( kc . stop_channels ) \n 
self . addCleanup ( km . shutdown_kernel ) \n 
N = 5 \n 
for i in range ( N ) : \n 
~~~ execute ( "start" ) \n 
reply = execute ( ) \n 
self . assertEqual ( reply [ ] [ ] , [ None ] * N ) \n 
kc . execute ( ) \n 
self . assertEqual ( content [ ] [ ] , True ) \n 
for i in range ( 50 ) : \n 
~~~ reply = execute ( ) \n 
if reply [ ] [ ] != [ - signal . SIGINT ] * N : \n 
~~ ~~ self . assertEqual ( reply [ ] [ ] , [ - signal . SIGINT ] * N ) \n 
~~ def test_start_new_kernel ( self ) : \n 
self . addCleanup ( kc . stop_channels ) \n 
self . assertTrue ( kc . is_alive ( ) ) \n 
import shlex \n 
from jupyterhub . spawner import LocalProcessSpawner \n 
class DemoFormSpawner ( LocalProcessSpawner ) : \n 
~~~ def _options_form_default ( self ) : \n 
~~~ default_env = "YOURNAME=%s\\n" % self . user . name \n 
~~ def options_from_form ( self , formdata ) : \n 
~~~ options = { } \n 
options [ ] = env = { } \n 
env_lines = formdata . get ( , [ ] ) \n 
for line in env_lines [ 0 ] . splitlines ( ) : \n 
~~~ if line : \n 
~~~ key , value = line . split ( , 1 ) \n 
env [ key . strip ( ) ] = value . strip ( ) \n 
~~ ~~ arg_s = formdata . get ( , [ ] ) [ 0 ] . strip ( ) \n 
if arg_s : \n 
~~~ options [ ] = shlex . split ( arg_s ) \n 
~~ return options \n 
~~ def get_args ( self ) : \n 
argv = super ( ) . get_args ( ) \n 
if self . user_options . get ( ) : \n 
~~~ argv . extend ( self . user_options [ ] ) \n 
~~ return argv \n 
~~ def get_env ( self ) : \n 
~~~ env = super ( ) . get_env ( ) \n 
~~~ env . update ( self . user_options [ ] ) \n 
~~ ~~ c . JupyterHub . spawner_class = DemoFormSpawner \n 
from traitlets import List , Unicode \n 
class URLPrefix ( Unicode ) : \n 
~~~ def validate ( self , obj , value ) : \n 
~~~ u = super ( ) . validate ( obj , value ) \n 
if not u . startswith ( ) : \n 
~~~ u = + u \n 
~~ if not u . endswith ( ) : \n 
~~~ u = u + \n 
~~ return u \n 
~~ ~~ class Command ( List ) : \n 
def __init__ ( self , default_value = None , ** kwargs ) : \n 
~~~ kwargs . setdefault ( , 1 ) \n 
if isinstance ( default_value , str ) : \n 
~~~ default_value = [ default_value ] \n 
~~ super ( ) . __init__ ( Unicode ( ) , default_value , ** kwargs ) \n 
~~ def validate ( self , obj , value ) : \n 
~~~ if isinstance ( value , str ) : \n 
~~~ value = [ value ] \n 
~~ return super ( ) . validate ( obj , value ) \n 
from kameleon_mcmc . distribution . Distribution import Distribution , Sample \n 
from kameleon_mcmc . tools . MatrixTools import MatrixTools \n 
from numpy . lib . twodim_base import eye , diag \n 
from numpy . linalg import cholesky , norm , eig \n 
from numpy import array , shape , log , zeros , arange , mean \n 
from numpy . random import randn \n 
from scipy . constants . constants import pi \n 
from scipy . linalg . basic import solve_triangular \n 
from scipy . stats . distributions import chi2 \n 
from numpy . linalg . linalg import LinAlgError \n 
class Gaussian ( Distribution ) : \n 
~~~ def __init__ ( self , mu = array ( [ 0 , 0 ] ) , Sigma = eye ( 2 ) , is_cholesky = False , ell = None ) : \n 
~~~ Distribution . __init__ ( self , len ( Sigma ) ) \n 
assert ( len ( shape ( mu ) ) == 1 ) \n 
assert ( max ( shape ( Sigma ) ) == len ( mu ) ) \n 
self . mu = mu \n 
self . ell = ell \n 
if is_cholesky : \n 
~~~ self . L = Sigma \n 
if ell == None : \n 
~~~ assert ( shape ( Sigma ) [ 0 ] == shape ( Sigma ) [ 1 ] ) \n 
~~~ assert ( shape ( Sigma ) [ 1 ] == ell ) \n 
if ell is not None : \n 
~~~ self . L , _ , _ = MatrixTools . low_rank_approx ( Sigma , ell ) \n 
self . L = self . L . T \n 
assert ( shape ( self . L ) [ 1 ] == ell ) \n 
~~~ self . L = cholesky ( Sigma ) \n 
~~ except LinAlgError : \n 
~~~ self . L = cholesky ( Sigma + eye ( len ( Sigma ) ) * 1e-5 ) \n 
~~ ~~ ~~ ~~ def __str__ ( self ) : \n 
~~~ s = self . __class__ . __name__ + "=[" \n 
s += "mu=" + str ( self . mu ) \n 
s += "]" \n 
return s \n 
~~ def sample ( self , n = 1 ) : \n 
~~~ if self . ell is None : \n 
~~~ V = randn ( self . dimension , n ) \n 
~~~ V = randn ( self . ell , n ) \n 
~~ return Sample ( self . L . dot ( V ) . T + self . mu ) \n 
~~ def log_pdf ( self , X ) : \n 
~~~ assert ( len ( shape ( X ) ) == 2 ) \n 
assert ( shape ( X ) [ 1 ] == self . dimension ) \n 
log_determinant_part = - sum ( log ( diag ( self . L ) ) ) \n 
quadratic_parts = zeros ( len ( X ) ) \n 
for i in range ( len ( X ) ) : \n 
~~~ x = X [ i ] - self . mu \n 
y = solve_triangular ( self . L , x . T , lower = True ) \n 
y = solve_triangular ( self . L . T , y , lower = False ) \n 
quadratic_parts [ i ] = - 0.5 * x . dot ( y ) \n 
~~ const_part = - 0.5 * len ( self . L ) * log ( 2 * pi ) \n 
return const_part + log_determinant_part + quadratic_parts \n 
~~ def log_pdf_at_quantile ( self , alphas ) : \n 
chi2_instance = chi2 ( self . dimension ) \n 
cuttoffs = chi2_instance . isf ( 1 - alphas ) \n 
quadratic_part = - 0.5 * cuttoffs \n 
const_part = - 0.5 * len ( self . L ) * log ( 2 * pi ) \n 
return const_part + log_determinant_part + quadratic_part \n 
~~ def emp_quantiles ( self , X , quantiles = arange ( 0.1 , 1 , 0.1 ) ) : \n 
~~~ chi2_instance = chi2 ( self . dimension ) \n 
cutoffs = chi2_instance . isf ( 1 - quantiles ) \n 
D , U = eig ( self . L . dot ( self . L . T ) ) \n 
D = D ** ( - 0.5 ) \n 
W = ( diag ( D ) . dot ( U . T ) . dot ( ( X - self . mu ) . T ) ) . T \n 
norms_squared = array ( [ norm ( w ) ** 2 for w in W ] ) \n 
results = zeros ( [ len ( quantiles ) ] ) \n 
for jj in range ( 0 , len ( quantiles ) ) : \n 
~~~ results [ jj ] = mean ( norms_squared < cutoffs [ jj ] ) \n 
from kameleon_mcmc . distribution . Banana import Banana \n 
from kameleon_mcmc . experiments . ClusterTools import ClusterTools \n 
from kameleon_mcmc . experiments . SingleChainExperiment import SingleChainExperiment \n 
from kameleon_mcmc . kernel . GaussianKernel import GaussianKernel \n 
from kameleon_mcmc . mcmc . MCMCChain import MCMCChain \n 
from kameleon_mcmc . mcmc . MCMCParams import MCMCParams \n 
from kameleon_mcmc . mcmc . output . StatisticsOutput import StatisticsOutput \n 
from kameleon_mcmc . mcmc . samplers . AdaptiveMetropolis import AdaptiveMetropolis \n 
from kameleon_mcmc . mcmc . samplers . AdaptiveMetropolisLearnScale import AdaptiveMetropolisLearnScale \n 
from kameleon_mcmc . mcmc . samplers . KameleonWindowLearnScale import KameleonWindowLearnScale \n 
from kameleon_mcmc . mcmc . samplers . StandardMetropolis import StandardMetropolis \n 
from numpy . lib . twodim_base import eye \n 
from numpy . ma . core import zeros \n 
~~~ if len ( sys . argv ) != 3 : \n 
~~ experiment_dir_base = str ( sys . argv [ 1 ] ) \n 
n = int ( str ( sys . argv [ 2 ] ) ) \n 
distribution = Banana ( dimension = 8 , bananicity = 0.03 , V = 100 ) \n 
sigma = GaussianKernel . get_sigma_median_heuristic ( distribution . sample ( 1000 ) . samples ) \n 
sigma = 5 \n 
kernel = GaussianKernel ( sigma = sigma ) \n 
for i in range ( n ) : \n 
~~~ mcmc_samplers = [ ] \n 
burnin = 20000 \n 
num_iterations = 40000 \n 
mcmc_samplers . append ( KameleonWindowLearnScale ( distribution , kernel , stop_adapt = burnin ) ) \n 
mean_est = zeros ( distribution . dimension , dtype = "float64" ) \n 
cov_est = 1.0 * eye ( distribution . dimension ) \n 
cov_est [ 0 , 0 ] = distribution . V \n 
mcmc_samplers . append ( AdaptiveMetropolisLearnScale ( distribution , mean_est = mean_est , cov_est = cov_est mcmc_samplers . append ( AdaptiveMetropolis ( distribution , mean_est = mean_est , cov_est = cov_est ) ) \n 
mcmc_samplers . append ( StandardMetropolis ( distribution ) ) \n 
start = zeros ( distribution . dimension , dtype = "float64" ) \n 
mcmc_params = MCMCParams ( start = start , num_iterations = num_iterations , burnin = burnin ) \n 
mcmc_chains = [ MCMCChain ( mcmc_sampler , mcmc_params ) for mcmc_sampler in mcmc_samplers ] \n 
for mcmc_chain in mcmc_chains : \n 
~~~ mcmc_chain . append_mcmc_output ( StatisticsOutput ( ) ) \n 
~~ experiments = [ SingleChainExperiment ( mcmc_chain , experiment_dir ) for mcmc_chain in mcmc_chains \n 
for experiment in experiments : \n 
~~~ ClusterTools . submit_experiment ( experiment ) \n 
from kameleon_mcmc . distribution . Gaussian import Gaussian \n 
from kameleon_mcmc . gp . GPData import GPData \n 
from kameleon_mcmc . gp . mcmc . PseudoMarginalHyperparameterDistribution import PseudoMarginalHyperparameterDistribution \n 
from kameleon_mcmc . mcmc . output . PlottingOutput import PlottingOutput \n 
from matplotlib . pyplot import plot \n 
from numpy . ma . core import mean , std , ones , shape \n 
from numpy . ma . extras import vstack , hstack \n 
~~~ data_circle , labels_circle = GPData . sample_circle_data ( n = 40 , seed_init = 1 ) \n 
data_rect , labels_rect = GPData . sample_rectangle_data ( n = 60 , seed_init = 1 ) \n 
data = vstack ( ( data_circle , data_rect ) ) \n 
labels = hstack ( ( labels_circle , labels_rect ) ) \n 
dim = shape ( data ) [ 1 ] \n 
data -= mean ( data , 0 ) \n 
data /= std ( data , 0 ) \n 
idx_a = labels > 0 \n 
idx_b = labels < 0 \n 
plot ( data [ idx_a , 0 ] , data [ idx_a , 1 ] , "ro" ) \n 
plot ( data [ idx_b , 0 ] , data [ idx_b , 1 ] , "bo" ) \n 
theta_prior = Gaussian ( mu = 0 * ones ( dim ) , Sigma = eye ( dim ) * 5 ) \n 
target = PseudoMarginalHyperparameterDistribution ( data , labels , n_importance = 100 , prior = theta_prior , ridge = 1e-3 ) \n 
burnin = 10000 \n 
num_iterations = burnin + 300000 \n 
kernel = GaussianKernel ( sigma = 35.0 ) \n 
sampler = KameleonWindowLearnScale ( target , kernel , stop_adapt = burnin ) \n 
start = 0.0 * ones ( target . dimension ) \n 
params = MCMCParams ( start = start , num_iterations = num_iterations , burnin = burnin ) \n 
chain = MCMCChain ( sampler , params ) \n 
chain . append_mcmc_output ( StatisticsOutput ( print_from = 0 , lag = 100 ) ) \n 
experiment_dir = str ( os . path . abspath ( sys . argv [ 0 ] ) ) . split ( os . sep ) [ - 1 ] . split ( "." ) [ 0 ] + os . sep \n 
experiment = SingleChainExperiment ( chain , experiment_dir ) \n 
experiment . run ( ) \n 
sigma = GaussianKernel . get_sigma_median_heuristic ( experiment . mcmc_chain . samples . T ) \n 
~~ from numpy import logical_xor , sum \n 
from kameleon_mcmc . distribution . Distribution import Distribution \n 
from kameleon_mcmc . distribution . proposals . DiscreteRandomWalkProposal import DiscreteRandomWalkProposal from kameleon_mcmc . kernel . Kernel import Kernel \n 
from kameleon_mcmc . mcmc . samplers . MCMCSampler import MCMCSampler \n 
class DiscreteKameleon ( MCMCSampler ) : \n 
def __init__ ( self , distribution , kernel , Z , threshold , spread ) : \n 
~~~ if not isinstance ( distribution , Distribution ) : \n 
~~ if not isinstance ( kernel , Kernel ) : \n 
~~ if not type ( Z ) is numpy . ndarray : \n 
~~ if not len ( Z . shape ) == 2 : \n 
~~ if not Z . shape [ 1 ] == distribution . dimension : \n 
~~ if not Z . shape [ 0 ] > 0 : \n 
~~ if not type ( threshold ) is float : \n 
~~ if not type ( spread ) is float : \n 
~~ if not ( spread > 0. and spread < 1. ) : \n 
~~ MCMCSampler . __init__ ( self , distribution ) \n 
self . kernel = kernel \n 
self . Z = Z \n 
self . spread = spread \n 
s += "kernel=" + str ( self . kernel ) \n 
~~ def construct_proposal ( self , y ) : \n 
~~~ k = self . kernel . kernel ( y . reshape ( 1 , len ( y ) ) , self . Z ) \n 
diff = y . astype ( numpy . int64 ) \n 
diff = diff - self . Z \n 
beta = randn ( len ( self . Z ) ) \n 
weighted_sum = sum ( ( k * beta ) . T * diff , 0 ) \n 
thresholded_sum = weighted_sum > self . threshold \n 
xored = logical_xor ( thresholded_sum , y ) \n 
return DiscreteRandomWalkProposal ( xored , self . spread ) \n 
~~ def adapt ( self , mcmc_chain , step_output ) : \n 
from numpy import zeros , fill_diagonal , asarray \n 
from numpy . random import rand , randn \n 
from kameleon_mcmc . distribution . Hopfield import Hopfield \n 
from kameleon_mcmc . distribution . full_conditionals . HopfieldFullConditionals import HopfieldFullConditionals from kameleon_mcmc . mcmc . MCMCChain import MCMCChain \n 
from kameleon_mcmc . mcmc . output . DiscretePlottingOutput import DiscretePlottingOutput \n 
from kameleon_mcmc . mcmc . samplers . Gibbs import Gibbs \n 
~~~ d = 5 \n 
b = randn ( d ) \n 
V = randn ( d , d ) \n 
W = V + V . T \n 
fill_diagonal ( W , zeros ( d ) ) \n 
hopfield = Hopfield ( W , b ) \n 
current_state = [ rand ( ) < 0.5 for _ in range ( d ) ] \n 
distribution = HopfieldFullConditionals ( full_target = hopfield , \n 
current_state = current_state ) \n 
mcmc_sampler = Gibbs ( distribution ) \n 
mcmc_params = MCMCParams ( start = asarray ( current_state , dtype = numpy . bool8 ) , num_iterations = 10000 ) \n 
chain = MCMCChain ( mcmc_sampler , mcmc_params ) \n 
chain . append_mcmc_output ( StatisticsOutput ( plot_times = True , lag = 100 ) ) \n 
chain . append_mcmc_output ( DiscretePlottingOutput ( plot_from = 0 , lag = 100 ) ) \n 
chain . run ( ) \n 
import textwrap \n 
class Summary ( object ) : \n 
~~~ def __init__ ( self , df ) : \n 
~~~ self . _df = df \n 
~~ def budget ( self , type = "byte" , size = 600 ) : \n 
~~~ summary = [ ] \n 
if size == "all" : \n 
~~ elif type == "word" : \n 
~~~ remaining = size \n 
for idx , sent in self . _df . iterrows ( ) : \n 
~~~ num_words = min ( len ( sent [ "words" ] ) , remaining ) \n 
remaining -= num_words \n 
if remaining < 1 : \n 
~~ ~~ ~~ elif type == "byte" : \n 
print num_chars \n 
remaining -= num_chars \n 
~~ def __unicode__ ( self ) : \n 
~~~ return self . budget ( ) \n 
~~~ return unicode ( self ) . encode ( "utf-8" ) \n 
~~ ~~ class Document ( object ) : \n 
~~~ def __init__ ( self , name , text ) : \n 
if isinstance ( self . name , str ) : \n 
~~~ self . name = self . name . decode ( u"utf-8" ) \n 
~~ self . text = text \n 
if isinstance ( self . text , str ) : \n 
~~~ self . text = self . text . decode ( u"utf-8" ) \n 
~~~ return unicode ( self ) . encode ( u"utf-8" ) \n 
~~~ return self . name + u"\\n" + self . text \n 
~~ ~~ class DocSet ( object ) : \n 
~~~ def __init__ ( self , docs ) : \n 
~~~ self . docs = docs \n 
~~ ~~ from model import * \n 
from data_api import * \n 
X = prepare_experiment_data ( dataset = "Wordnet" , CV = 0 ) \n 
params = cPickle . load ( open ( sys . argv [ 1 ] , "r" ) ) \n 
R = range ( 11 ) \n 
networks = createNetworks ( entity_matrix = X [ "E" ] , embedding_matrix = params [ "embedding_matrix" ] , k = 4 , only_metric pars = params [ "network_params" ] \n 
for i , n in enumerate ( networks ) : \n 
~~~ n . load_params ( pars [ i ] ) \n 
~~ X_test = split_per_relation ( X [ "X_test" ] , rel = range ( 11 ) ) \n 
best_splits = [ ] \n 
accs_history = [ ] \n 
networks [ 0 ] . update_EU ( ) \n 
for r in range ( 11 ) : \n 
~~~ test_values = np . linspace ( - 4.5 , 4.5 , num = 50 ) \n 
f_eval = theano . function ( networks [ r ] . inputs , networks [ r ] . f_prop ( ) ) \n 
for split in test_values : \n 
~~~ results_pos = f_eval ( X_test [ r ] [ : , 0 : 3 ] ) . reshape ( - 1 ) \n 
results_neg = f_eval ( X_test [ r ] [ : , [ 3 , 1 , 4 ] ] ) . reshape ( - 1 ) \n 
accuracies . append ( ( sum ( results_pos > split ) + sum ( results_neg < split ) ) / ( 2 * float ( X_test [ r ] . ~~ best_splits . append ( test_values [ np . argmax ( accuracies ) ] ) \n 
accs_history . append ( accuracies ) \n 
~~ print ( best_splits ) \n 
accs = [ ] \n 
global_acc = 0 \n 
~~~ f_eval = theano . function ( networks [ r ] . inputs , networks [ r ] . f_prop ( ) ) \n 
results_pos = f_eval ( X_test [ r ] [ : , 0 : 3 ] ) . reshape ( - 1 ) \n 
results_neg cnt = accs . append global_acc ~~ global_acc \n 
from __future__ \n 
from string import pycuda import pycuda from pycuda import numpy \n 
import skcuda \n 
A = 3 \n 
C = 5 \n 
N = A * B \n 
max_threads_per_block block_dim , max_blocks_per_grid \n 
func_mod = SourceModule func = func_mod x_gpu = gpuarray func ( x_gpu block grid = x_np = x . copy x_np [ : , 0 , \n 
import atexit import numbers from string \n 
import pycuda import pycuda import pycuda import pycuda import pycuda import pycuda from pycuda from pycuda from pytools import numpy \n 
from . import from . import \n 
~~~ from . _has_cula ~~ except ( ImportError ~~~ _has_cula \n 
~~~ from . _has_cusolver ~~ except ( ImportError ~~~ _has_cusolver \n 
~~~ from . _has_magma ~~ except ( ImportError ~~~ _has_magma \n 
drv . init dev = return \n 
ctx = atexit return \n 
for i ~~~ if ~~~ ~~ ~~ ctx . detach \n 
~~ if _global_cublas_allocator ~~~ _global_cublas_allocator \n 
~~ global if not ~~~ from _global_cusolver_handle \n 
~~ if _has_magma ~~~ magma \n 
global if _global_cublas_handle ~~~ from cublas _global_cublas_handle \n 
~~ global if _global_cusolver_handle ~~~ from cusolver _global_cusolver_handle \n 
~~ if _has_cula ~~~ cula \n 
~~ @ memoize \n 
attrs return \n 
~~ iceil = lambda \n 
@ memoize \n 
if threads_per_block ~~~ if ~~~ ~~ else ~~~ \n 
if blocks_needed ~~~ return ~~ elif blocks_needed blocks_needed ~~~ return ~~ elif blocks_needed blocks_needed ~~~ return ~~ else : \n 
out = out . fill return \n 
~~~ func ~~ except ~~~ ctype use_double ret_type func maxabs ~~ return ~~ maxabs . cache \n 
~~~ func ~~ except ~~~ func cumsum ~~ return ~~ cumsum . cache \n 
y_gpu try : \n 
~~~ func ~~ except ~~~ ctype func diff ~~ func ( x_gpu return ~~ diff . cache \n 
~~~ func ~~ except ~~~ data_ctype ind_ctype v func get_by_index ~~ func ( dest_gpu return ~~ get_by_index \n 
~~~ raise ~~ if not ~~~ ind ~~ try : \n 
~~~ func ~~ except ~~~ data_ctype ind_ctype v \n 
if ~~~ ~~ else ~~~ ~~ set_by_index ~~ func ( dest_gpu ~~ set_by_index \n 
cache_dir ctype tmpl = mod = \n 
add_row_vec_kernel add_col_vec_kernel return \n 
if axis ~~~ if ~~~ ~~~ ~~~ ~~ ~~ elif ~~~ ~~ elif ~~~ ~~ else ~~~ ~~ ~~ else : \n 
~~~ if ~~~ ~~ if ~~~ \n 
~~ ~~ if binary_op ~~~ raise \n 
~~ row_kernel n , m = \n 
block gridx gridy grid = \n 
if out ~~~ alloc out ~~ else : \n 
~~~ assert assert \n 
~~ if x_gpu ~~~ if ~~~ ~~ elif ~~~ ~~ ~~ else : \n 
~~~ if ~~~ ~~ elif ~~~ ~~ ~~ return \n 
~~ import operator \n 
def binaryop_2d ~~~ if x_gpu ~~~ return ~~ elif x_gpu ~~~ return ~~ elif y_gpu ~~~ return \n 
~~ if len ~~~ m , if ~~~ ~~ elif ~~~ ~~ elif ~~~ \n 
~~ ~~ if len ~~~ m , if ~~~ ~~ elif ~~~ ~~ elif ~~~ \n 
~~ ~~ raise \n 
~~ def _sum_axis ~~~ global assert \n 
if axis ~~~ out_shape if ~~~ ~~ else ~~~ \n 
~~ ~~ if axis ~~~ axis ~~ if axis ~~~ raise \n 
~~ if x_gpu ~~~ n , lda trans sum_axis ~~ else : \n 
~~~ n , lda trans sum_axis \n 
~~ if calc_mean ~~~ alpha ~~ else : \n 
~~~ alpha ~~ if ( x_gpu ~~~ gemv ~~ elif ( ~~~ gemv ~~ elif ( ~~~ gemv ~~ elif ( ~~~ gemv \n 
~~ alloc ons = \n 
if keepdims ~~~ out_shape ~~ else : \n 
~~~ out_shape \n 
~~ if out ~~~ out ~~ else : \n 
~~ gemv ( _global_cublas_handle alpha ons return \n 
def _inplace_pow ~~~ func func \n 
~~ if axis ~~~ m out out out ~~ else : \n 
~~~ if ~~~ ~~ m out _inplace_pow out ~~ return \n 
~~ if axis ~~~ return ~~ else : \n 
~~~ out _inplace_pow ~~ return \n 
cache_dir ctype if min_or_max ~~~ iv tmpl ~~ elif min_or_max ~~~ iv tmpl ~~ else : \n 
~~~ raise ~~ mod = \n 
minmax_col_kernel minmax_row_kernel return \n 
~~ def _minmax_impl ~~~ assert if iscomplextype ~~~ raise \n 
~~ if axis ~~~ out_shape if ~~~ ~~ else ~~~ ~~ ~~ else : \n 
~~~ if ~~~ ~~ ~~ assert \n 
global alloc \n 
n , m = col_kernel if ( axis ~~~ if ~~~ ~~ else ~~~ ~~ target idx col_kernel ~~ else : \n 
~~~ if ~~~ ~~ else ~~~ ~~ target idx row_kernel ~~ return \n 
~~ def max ( a_gpu ~~~ \n 
~~ def min ( a_gpu ~~~ \n 
~~ def argmax ~~~ \n 
if axis ~~~ raise ~~ return \n 
~~ def argmin ~~~ \n 
import sys import timeit \n 
import theano import theano from theano \n 
from logistic_sgd from mlp import from dA import \n 
def __init__ self numpy_rng theano_rng n_ins hidden_layers_sizes n_outs corruption_levels ) : \n 
self self self self \n 
assert \n 
~~~ ~~~ ~~~ \n 
~~~ ~~~ \n 
~~ def pretraining_functions ~~~ \n 
pretrain_fns for ~~~ \n 
~~ def build_finetune_functions ~~~ \n 
( train_set_x ( valid_set_x ( test_set_x \n 
index \n 
train_fn ) \n 
test_score_i ) \n 
valid_score_i ) \n 
datasets \n 
train_set_x valid_set_x test_set_x \n 
~~ ~~ end_time \n 
best_validation_loss test_score start_time \n 
done_looping epoch \n 
while ~~~ epoch for ~~~ \n 
~~ ~~ ~~ end_time print ( ( \n 
% ) \n 
import numpy from theano from theano import tempfile from numpy from os import from decimal from tempfile \n 
from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 import yaml import re \n 
def test_load_path ~~~ fd , fname with os ~~~ f . ~~ loaded assert_ os . remove \n 
~~ def test_obj ~~~ loaded assert_ \n 
~~ def test_floats ~~~ loaded assert_ assert_ assert_ assert_ \n 
pattern matches fails \n 
for match ~~~ assert \n 
~~ for fail ~~~ assert \n 
loaded assert assert assert assert assert assert assert assert \n 
~~ def test_import ~~~ loaded assert_ \n 
~~ def test_import_string ~~~ loaded assert_ \n 
~~ def test_import_colon ~~~ loaded assert_ \n 
~~ def test_preproc_rhs ~~~ environ loaded print ( assert_ del environ \n 
~~ def test_preproc_pkl ~~~ fd , fname with os ~~~ d cPickle ~~ environ loaded assert_ del environ \n 
~~ def test_unpickle ~~~ fd , fname with os ~~~ d cPickle ~~ loaded assert_ os . remove \n 
~~ def test_unpickle_key ~~~ fd , fname with os ~~~ d cPickle ~~ loaded assert_ assert_ os . remove \n 
~~~ load ~~ except ~~~ assert pass ~~ except ~~~ error_msg reraise_as \n 
~~~ load ~~ except ~~~ message assert pass ~~ except ~~~ error_msg raise \n 
load ( yamlfile \n 
~~ class DumDum ~~~ pass \n 
~~~ fd close o o . serial yaml loaded assert assert ~~ finally ~~~ os \n 
yaml = obj = assert \n 
import logging import re , \n 
logger = logging \n 
class dataset_resolver \n 
~~ ~~ installed_packages_list \n 
try ~~~ ~~ except ~~~ ~~ else ~~~ ~~~ ~~~ ~~~ \n 
paths try ~~~ ~~ except ~~~ \n 
~~ for ~~~ \n 
__authors__ __copyright__ __credits__ __license__ __maintainer__ __email__ \n 
import numpy np = N \n 
from theano from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 \n 
def __init__ ~~~ self \n 
if ~~~ ~~~ \n 
~~ def ~~~ \n 
~~ if ~~~ ~~~ ~~~ \n 
~~ else ~~~ ~~~ ~~~ ~~~ \n 
~~ if ~~~ \n 
~~ y_labels \n 
m , assert assert topo_view \n 
if ~~~ ~~ elif ~~~ ~~ else ~~~ \n 
~~ if ~~~ ~~~ \n 
~~ ~~ super \n 
if ~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ class MNIST_rotated_background \n 
def __init__ ~~~ path \n 
obj X X y \n 
if ~~~ \n 
~~ view_converter \n 
super \n 
from pylearn2 from pylearn2 import unittest from pylearn2 import numpy \n 
skip_if_no_data for subset ~~~ ids assert assert topo assert del \n 
~~ train_y assert assert assert assert assert \n 
train_y assert assert assert assert assert \n 
import inspect from nose . import re \n 
from theano \n 
~~ self \n 
~~ def __getitem__ ~~~ return \n 
~~ def reset ~~~ self \n 
~~ def read ~~~ if ~~~ ~~ else ~~~ \n 
~~ ~~ def seek_next_non_empty_line ~~~ for ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ def eof ~~~ return \n 
~~ def read_to_condition ~~~ start for ~~~ ~~~ ~~~ ~~ ~~ return \n 
~~ def read_to_next_empty_line ~~~ self def ~~~ ~~ return \n 
~~ def read_to_next_unindented_line ~~~ def ~~~ ~~ return \n 
~~ def peek ~~~ if ~~~ ~~ else ~~~ \n 
~~ ~~ def is_empty ~~~ return \n 
~~ def __iter__ ~~~ for ~~~ \n 
~~ ~~ ~~ class NumpyDocString ~~~ def __init__ ~~~ if ~~~ ~~ docstring \n 
~~ self self self \n 
self \n 
~~ def __setitem__ ~~~ if ~~~ ~~ else ~~~ \n 
~~ ~~ def _is_at_section ~~~ self \n 
~~ l1 \n 
~~ l2 return \n 
~~ def _strip ~~~ i j for ~~~ \n 
~~ def _read_to_next_section ~~~ section \n 
while ~~~ ~~~ \n 
~~ def _read_sections ~~~ while ~~~ \n 
~~ ~~ ~~ def _parse_param_list ~~~ r params while ~~~ ~~~ ~~~ \n 
~~ out section if ~~~ ~~ for ~~~ ~~~ ~~ ~~ return \n 
~~ ~~ def _parse ~~~ self self for ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ def _str_header ~~~ return \n 
~~ def _str_indent ~~~ out for ~~~ ~~ return \n 
~~ def _str_signature ~~~ if ~~~ ~~ return \n 
~~ def _str_summary ~~~ return \n 
~~ def _str_extended_summary ~~~ return \n 
~~ def _str_param_list ~~~ out if ~~~ ~~~ ~~ return \n 
~~ def _str_section ~~~ out if ~~~ ~~ return \n 
~~ def _str_see_also ~~~ if out out last_had_desc for ~~~ ~~~ ~~~ ~~~ ~~~ ~~ ~~ out return \n 
~~ def _str_index ~~~ idx out out for ~~~ ~~~ ~~ return \n 
~~ def __str__ ~~~ out out out out for ~~~ ~~ out for ~~~ ~~ out return \n 
~~ def get_errors ~~~ errors self for ~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ if ~~~ \n 
~~ ~~ ~~ ~~ return \n 
~~ ~~ def indent ~~~ indent_str if str ~~~ return ~~ lines return \n 
~~ class NumpyFunctionDocString ~~~ def __init__ ~~~ super args if ~~~ ~~ else ~~~ \n 
~~ ~~ def _parse ~~~ self return \n 
~~ def get_errors ~~~ errors \n 
~~ ~~ class NumpyClassDocString ~~~ def __init__ ~~~ super self methods \n 
self if ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ def _parse ~~~ self return \n 
~~ def get_errors ~~~ errors if ~~~ ~~ return \n 
def _parse ~~~ self \n 
~~ def __str__ ~~~ return \n 
~~ def get_errors ~~~ errors return \n 
~~ ~~ def header ~~~ return \n 
~~ def _str_signature ~~~ return \n 
~~ def _str_param_list ~~~ out if ~~~ ~~~ ~~ ~~ return \n 
~~ ~~ class FunctionDoc ~~~ def __init__ ~~~ self \n 
~~ def __str__ ~~~ out doclines try ~~~ ~~ except ~~~ \n 
~~ if ~~~ ~~ else ~~~ ~~~ ~~~ \n 
~~ ~~ out return \n 
~~ ~~ class ClassDoc ~~~ def __init__ ~~~ if ~~~ ~~ self \n 
if ~~~ ~~ self self \n 
~~ @ property def methods ~~~ return \n 
~~ def __str__ ~~~ out \n 
def ~~~ \n 
~~ ~~ def handle_function ~~~ func_errors docstring if docstring ~~~ func_errors ~~ else : \n 
~~~ func_errors ] \n 
~~ def handle_module ~~~ module_errors docstring if docstring ~~~ module_errors ~~ else : \n 
~~~ module_errors ] \n 
~~ def handle_method ~~~ method_errors \n 
~~ ~~ docstring if docstring ~~~ method_errors ~~ else : \n 
~~~ method_errors ] \n 
~~ def handle_class ~~~ cls_errors docstring if docstring ~~~ cls_errors ~~ else : \n 
~~~ cls_errors ] \n 
if global_dict ~~~ global_dict ~~ if ~~~ global_dict ~~ if ~~~ global_dict ~~ try : \n 
~~~ with ~~~ ~~ ~~ except ~~~ pass ~~ except ~~~ raise ~~ all_errors for key ~~~ if ~~~ ~~~ ~~~ ~~~ ~~~ ~~ ~~ ~~ elif ~~~ ~~ ~~ if all_errors ~~~ all_errors ~~ return \n 
import numpy import theano from theano from theano \n 
from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 \n 
~~~ n = 3 \n 
m = 4 \n 
rng = \n 
Z_numer Z_denom \n 
numer denom \n 
naive stable \n 
~~ def test_pseudoinverse_softmax_numpy ~~~ rng = \n 
p = np p /= p \n 
z = pseudoinverse_softmax_numpy zbroad p2 = softmax_numpy p2 = p2 \n 
tp_pyval ys_pyval \n 
tp = sharedX ys = sharedX recall_py recall assert \n 
init_mode theano \n 
~~~ mlp X Y X . Y_hat \n 
~~ finally ~~~ theano \n 
X = T . Y = T . Z = arg_of_sigmoid assert \n 
X = T . Y = T . try : \n 
import numpy from numpy \n 
import theano from theano from theano \n 
from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 \n 
r = 31 s = 21 shape nvis = output_channels batch_size \n 
x = np y = np \n 
x = x . y = y . \n 
x_mlp y_mlp \n 
conv_model input_space layers batch_size ) \n 
X = conv_model Y = conv_model Y_hat g = theano \n 
W , b = \n 
W_mlp for k ~~~ W_mlp ~~ W_mlp b_mlp \n 
mlp_model \n 
X1 = mlp_model Y1 = mlp_model Y1_hat f = theano \n 
cases \n 
for conv_nonlinearity ~~~ check_case \n 
import copy import time import warnings import logging import numpy from theano \n 
from pylearn2 import theano from theano from theano from theano \n 
log = logging \n 
def __init__ ~~~ self self self self self self self self self self self self self self self self \n 
~~ nested_space nested_source \n 
self self \n 
flat_space flat_source self \n 
~~ datasets \n 
self for ~~~ ~~~ \n 
~~ ~~ ~~ log log log log t for ~~~ ~~~ ~~~ \n 
init_names self for ~~~ ~~~ ~~~ ~~~ ~~~ \n 
log for ~~~ ~~~ ~~ it for ~~~ ~~ self givens updates for ~~~ \n 
~~ with ~~~ ~~~ ~~~ ~~~ \n 
~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~ ~~ final_names self \n 
~~ temp \n 
if ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~ ~~ ~~ ~~ d self for ~~~ ~~~ \n 
~~ val \n 
if ~~~ ~~~ ~~~ \n 
~~~ ~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ data_specs \n 
mapping flat_ipt if ~~~ ~~ inputs for ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ mode if ~~~ ~~~ ~~~ ~~~ \n 
~~ if ~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ try ~~~ ~~ except ~~~ \n 
~~ if ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ self \n 
if ~~~ ~~ else ~~~ \n 
if ~~~ ~~ if ~~~ ~~ else ~~~ \n 
~~ if ~~~ ~~ else ~~~ ~~ assert costs \n 
~~ model \n 
nested_space nested_sources \n 
custom_channels for ~~~ ~~~ ~~~ ~~~ \n 
~~ for ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~ try ~~~ ~~ except ~~~ \n 
if ~~~ ~~ else ~~~ ~~~ ~~~ \n 
~~ ~~ return } \n 
assert old_monitor setattr del model \n 
if transfer_experience ~~~ monitor assert monitor monitor monitor if ~~~ ~~~ \n 
monitor monitor monitor channels channel val_record value , return \n 
doc = \n 
if var ~~~ doc \n 
~~ _err_no_data _err_ambig_data """\nXXX\n""" \n 
import numpy from theano import theano \n 
~~~ grad_not_implemented ~~ except : \n 
~~~ def grad_not_implemented ~~~ return \n 
for a ~~~ if ~~~ ~~ ~~ return \n 
~~ def not_symbolic ~~~ return \n 
~~ def _attributes ~~~ return \n 
~~ def __eq__ ~~~ return \n 
~~ def __hash__ ~~~ return \n 
def make_node ~~~ images filters ibcast fbcast igroups fmodulesR fgroups hbcast htype if ~~~ ~~ return \n 
~~ def perform ~~~ images \n 
igroups fmodulesR fgroups \n 
hshape \n 
hidacts \n 
for ~~~ ~~~ ~~~ \n 
~~~ ~~~ ~~~ ~~~ ~~ ~~ ~~ ostor \n 
print_sizes if ~~~ \n 
~~ def infer_shape ~~~ ishape \n 
if ~~~ ~~ if ~~~ ~~ if ~~~ ~~ if ~~~ \n 
~~ hshape return \n 
def make_node ~~~ images if ~~~ ~~ if ~~~ ~~ if ~~~ ~~ if ~~~ \n 
~~ igroups hgroups otype return \n 
filters \n 
~~ ~~ ~~ ostor \n 
~~ def grad ~~~ images gfilters irows icols gimages ghidacts return \n 
~~ def infer_shape ~~~ images ishape igroups hgroups \n 
fshape \n 
def make_node ~~~ filters if ~~~ ~~ if ~~~ ~~ if ~~~ ~~ if ~~~ ~~ if ~~~ ~~ if ~~~ ~~ if ~~~ ~~ return \n 
~~ def perform ~~~ filters \n 
hgroups \n 
fmodulesR fgroups \n 
igroups icolors_per_group icount \n 
~~ images \n 
~~ def grad ~~~ filters gimages frows fcols gfilters ghidacts return \n 
~~ ~~ from __future__ \n 
import numpy import theano from theano from theano import theano import warnings \n 
if not tcn ~~~ from nose raise \n 
~~ from pylearn2 from pylearn2 \n 
if theano . ~~~ mode_with_gpu mode_without_gpu ~~ else : \n 
~~~ mode_with_gpu mode_without_gpu \n 
def my_rand ~~~ return \n 
#numpy.random.RandomState(unittest_tools.fetch_seed()).shuffle(shps) warnings for shp ~~~ for ~~~ ~~~ \n 
from pylearn2 from pylearn2 from pylearn2 import numpy import textwrap \n 
def main ( ) ~~~ data_dir \n 
print ( downsampled_dir \n 
data = supplement \n 
print ( data . set_design_matrix del supplement \n 
print ( patch_dir serial README \n 
README \n 
print ( pipeline pipeline pipeline preprocessing pipeline data . apply_preprocessor \n 
data . use_design_loc \n 
serial \n 
~~ if __name__ ~~~ main ( ) ~~ from __future__ \n 
from pylearn2 \n 
def main ( ) ~~~ base = base = paths assert \n 
words \n 
for i , \n 
~~~ if ~~~ ~~ path f lines for ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ ranked_words \n 
ranked_words \n 
f = open f . writelines f . close \n 
from pylearn2 from pylearn2 \n 
import numpy from theano from pylearn2 \n 
def __init__ ~~~ X X [ super \n 
X = rng \n 
if num_classes ~~~ Y y_labels ~~ else : \n 
~~~ Y y_labels \n 
~~ def random_one_hot_dense_design_matrix ~~~ X = rng \n 
idx = Y = np for i ~~~ Y [ \n 
~~ def random_one_hot_topological_dense_design_matrix \n 
~~~ dims = \n 
for i , ~~~ dims \n 
~~ shape \n 
X = rng Y = rng \n 
~~ import numpy import warnings \n 
from theano from theano from theano \n 
from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 from pylearn2 \n 
def __init__ ~~~ assert assert self self \n 
updates \n 
for ~~~ ~~~ \n 
def __init__ ~~~ if ~~~ \n 
~~ self del self self \n 
~~ momentum \n 
~~ alpha if ~~~ ~~ if ~~~ ~~ return \n 
def __init__ ~~~ assert assert self \n 
def __init__ ~~~ assert self \n 
def __init__ ~~~ assert assert self self self \n 
channel_mapping } \n 
for ~~~ ~~~ ~~ ~~ return \n 
updates for \n 
import subprocess import sys \n 
child output rc = child return \n 
n_symbols progress_bar sys . stdout sys . stdout ~~ import numpy \n 
def eye ( gain ~~~ def init ~~~ return ~~ return ~~ from . AddConstant from . BatchNormalization from . Dropout from . Identity from . Linear from . Bias from . Log from . ReLU from . ELU from . Reshape from . Flatten from . Permute from . PoolingCUDNN from . Sigmoid from . SoftMax from . SpatialConvolution from . SpatialConvolutionCUDNN from . BackwardsConvolutionCUDNN from . SpatialMaxPooling from . SpatialSoftMaxCUDNN from . UpSample from . Tanh import DeepFried2 \n 
def net ( ) : ~~~ model model . model . \n 
model . model . \n 
model . model . return \n 
~~ def lenet_cudnn ~~~ model model . model . model . model . model . \n 
model . model . model . model . model . \n 
model . model . model . model . \n 
~~ def lenet_same ~~~ model model . model . model . model . model . \n 
~~ def lenet ( ~~~ model model . model . model . model . model . \n 
__author__ from numpy import os , import matplotlib import matplotlib import matplotlib import random import pyfits \n 
path_src = if not path_src from ml_codes from utils \n 
def __init__ ~~~ utils_mlz Pars_in self self \n 
path_map fileM utils_mlz M M M . \n 
~~ fimportance FF linew linew FF for ~~~ ~~ FF \n 
utils_mlz utils_mlz \n 
Im sk sk fig ax ax ax ax ax plt ax ax ax \n 
## zs O_all ki Im0 Im1 keys for ~~~ \n 
~~ fig2 if ~~~ ~~ else ~~~ ~~~ ~~ ~~ ax2 ax2 plt plt \n 
arg1 arg2 arg3 arg4 \n 
plt \n 
zz zs Nbins \n 
Nz Nzmid \n 
ns na nb gs ax1 ax2 \n 
fzspec fzphot fzmode fzmean \n 
ax1 ax1 ax1 ax1 ax1 ax1 ax1 ax1 ax1 \n 
ax2 ax2 ax2 ax2 ax2 ax2 \n 
filenum froot if ~~~ ~~ else ~~~ \n 
~~ if if ~~~ \n 
~~ F P F . \n 
~~ head z \n 
rep_pdf \n 
plt plt plt plt plt plt title plt \n 
plt AD sp_ind spi Dind2 AA Da vals delta delta \n 
pdfr plt for ~~~ ~~ plt plt plt plt title plt \n 
~~ ~~ import numpy import math import tensorflow import rmsprop_applier \n 
class RMSPropApplierTest ~~~ def testApply ~~~ with self ~~~ var \n 
grad0 grad1 \n 
opt \n 
apply_gradient0 apply_gradient1 \n 
tf . initialize_all_variables \n 
ms_x ms_y \n 
x = y = dx = dy = ms_x ms_y x = y = \n 
dx = dy = ms_x ms_y x = y = \n 
~~ ~~ ~~ if __name__ ~~~ tf . test . ~~ from training from lasagne_extensions from data_preparation from models import numpy \n 
n_labeled n_samples n_batches mnist_data \n 
n , n_x bs = n \n 
~~ if __name__ ~~~ run_adgmssl_mnist ~~ import itertools import matplotlib import matplotlib from mpl_toolkits from sklearn from sklearn from sklearn from sklearn \n 
import numpy import scipy \n 
from ex7_pca \n 
if __name__ \n 
~~~ data = X = data x1 , x2 \n 
plt . plot plt . show \n 
ss = StandardScaler pca = pipeline pipeline \n 
print ( print ( print ( \n 
X_norm \n 
K = 1 \n 
U = pca Z = project_data print ( print ( \n 
X_rec print ( print ( \n 
x1 , x2 plt . plot x1 , x2 plt . plot plt . axis \n 
for x_rec ~~~ plt ~~ plt . show \n 
data = X = data \n 
display_data \n 
pipeline X_norm \n 
U = pca \n 
K = 100 Z = project_data X_rec \n 
display_data display_data \n 
X = img \n 
K = 16 max_iters \n 
km = KMeans km . fit to_plot np . random to_plot \n 
X_plot idx_plot fig = ax = fig for k , ~~~ x , ax ~~ plt . title plt . show \n 
pipeline X_norm U = pca Z = project_data Z_plot for k , ~~~ x1 plt ~~ plt . title plt . show \n 
import mne from mne import from mne . datasets \n 
print ( __doc__ \n 
condition \n 
raw = mne . events = find_events \n 
import mne from mne . datasets from mne . inverse_sparse from mne . minimum_norm from mne . viz \n 
data_path fwd_fname ave_fname cov_fname subjects_dir \n 
ylim = dict evoked . plot \n 
import matplotlib import mne from mne . viz from mne . datasets \n 
data_path \n 
fname = data_path \n 
ch_names = picks = mne \n 
evoked . plot \n 
import mne \n 
from mne import from mne . datasets from mne . cov \n 
data_path raw_fname event_fname \n 
raw = io . read_raw_fif raw . filter raw . info [ events = mne \n 
event_id , picks = mne reject = dict \n 
epochs = mne \n 
print ( for c in noise_covs ~~~ print ( \n 
~~ evoked = epochs \n 
import numpy from scipy \n 
from . io . pick from . io . base from . io . constants from . forward from . cov from . transforms from . utils from . fixes from . externals \n 
@ deprecated @ verbose \n 
if isinstance ~~~ if ~~~ ~~ t_step picks if ~~~ ~~ time_idx data t data data ~~ else : \n 
~~~ if ~~~ ~~ if ~~~ ~~ data ~~ out = if return_quat ~~~ out ~~ return \n 
_check_fname data = data . shape return \n 
_check_fname pos = if pos ~~~ raise ~~ with open ~~~ fid for ~~~ \n 
t = quats rotation translation return \n 
~~ @ verbose \n 
~~~ picks ~~ megchs templates coils if method ~~~ coils ~~ else : ~~~ coils ~~ scale scale orig_dev_head_quat dists hpi = last = return \n 
from scipy hpi , last fit_idxs quats logger pos_0 n_freqs for midpt # \n 
if not ~~~ raise ~~ t_window t_step n_step hpi = fit_idxs n_freqs n_remove meg_picks n_times \n 
msg = if include_line ~~~ n_remove msg ~~ msg += \n 
proj = logger chunks last_endpt last_done next_done for ii ~~~ if ~~~ ~~ left_edge time_sl this_len if ~~~ ~~ else ~~~ ~~ this_data subt_pt if ~~~ ~~ last_endpt \n 
if not ~~~ raise \n 
~~ if len ~~~ raise \n 
n_seeds n_targets \n 
indices \n 
import os . import shutil import tarfile import stat import sys \n 
from . . import from . . utils from . . externals from . . externals \n 
~~ ~~ if update_path ~~~ set_config ~~ return \n 
key = } [ name \n 
if archive_name ~~~ archive_names ~~ folder_names brainstorm fake misc sample somato spm testing ) \n 
urls = brainstorm fake misc sample somato spm testing ) \n 
hashes brainstorm fake misc sample somato spm testing ) \n 
folder_origs misc testing ) \n 
folder_name archive_name hash_ url = folder_orig if ~~~ url \n 
~~ folder_path if name ~~~ extract_path folder_path \n 
~~ rm_archive martinos_path neurospin_path \n 
if not ~~~ return ~~ if not ~~~ if ~~~ ~~~ ~~~ ~~~ ~~ ~~ logger \n 
if ~~~ ~~ elif ~~~ ~~ else ~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ if ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~ logger logger for ~~~ ~~~ ~~~ ~~~ ~~ ~~ if ~~~ \n 
~~ ~~ path = path = \n 
~~~ from ~~ except ~~~ warn ~~ else : ~~~ if ~~~ ~~ ~~ return \n 
from copy from math \n 
from . mxne_debiasing from . . utils from . . time_frequency from . . externals \n 
if len ~~~ return ~~ if shape ~~~ shape_init Y ~~ n_positions \n 
if is_stft ~~~ rows_norm ~~ else : \n 
~~~ Y Y ~~ return \n 
GX = np R = M penalty nR2 = pobj = dual_norm scaling scaling dobj = gap = return \n 
if n_sources ~~~ gram GTM ~~ else : \n 
~~~ gram \n 
~~ if init ~~~ X R if ~~~ ~~ ~~ else : \n 
~~~ X if ~~~ ~~ else ~~~ \n 
~~ ~~ t = 1.0 Y = np E = [ ] \n 
active_set \n 
for i ~~~ X0 if ~~~ ~~ else ~~~ ~~ X , \n 
t0 t Y . dt Y [ Y [ Y_as \n 
~~ gap E . logger if ~~~ ~~ ~~ return \n 
n_sensors n_sensors \n 
if init ~~~ init \n 
~~ clf = clf . coef_ clf . fit \n 
X = clf active_set X = X [ gap , pobj return \n 
n_sensors n_sensors n_positions \n 
if init ~~~ X R ~~ else : \n 
~~~ X R \n 
~~ E = [ ] \n 
alpha_lc \n 
for i ~~~ for ~~~ \n 
~~ ~~ gap E . logger \n 
~~ ~~ X = X [ \n 
n_dipoles n_positions n_sensors alpha_max logger alpha \n 
has_sklearn try : \n 
~~~ from ~~ except ~~~ has_sklearn \n 
~~ if solver ~~~ if ~~~ ~~ else ~~~ \n 
~~ ~~ if solver ~~~ if ~~~ ~~ if ~~~ \n 
~~ ~~ if solver ~~~ logger l21_solver lc ~~ elif solver ~~~ logger l21_solver G if ~~~ ~~ else ~~~ ~~~ ~~ ~~ ~~ else : \n 
~~~ logger l21_solver lc \n 
~~ if active_set_size ~~~ E X_init active_set idx_large_corr new_active_idx if ~~~ ~~ active_set as_size for ~~~ ~~~ ~~~ ~~~ \n 
~~~ ~~~ ~~ ~~ else ~~~ ~~ ~~ else : \n 
~~~ X , \n 
~~ if np . ~~~ bias X \n 
~~ logger \n 
def g ( ~~~ return \n 
~~ def gprime ~~~ return \n 
~~ E = list \n 
active_set weights X = np \n 
for k ~~~ X0 active_set_0 G_tmp \n 
if ~~~ ~~~ ~~~ ~~ ~~ else ~~~ \n 
~~~ ~~ ~~ else ~~~ \n 
~~ ~~ if np . ~~~ bias X \n 
n_times n_points iv = np v = phi L = 1e100 for it ~~~ L_old logger iv Gv GtGv w L v if ~~~ ~~ ~~ return \n 
~~ def __call__ ~~~ return \n 
~~ ~~ def norm_l21_tf ~~~ if Z . shape ~~~ Z2 l21_norm l21_norm ~~ else : \n 
~~~ l21_norm ~~ return \n 
~~ def norm_l1_tf ~~~ if Z . shape ~~~ n_positions Z_ Z_ l1_norm l1_norm ~~ else : \n 
~~~ l1_norm ~~ return \n 
n_sensors n_sources n_positions \n 
n_step n_freq shape \n 
G = dict R = M . active for idx ~~~ R \n 
alpha_time_lc alpha_space_lc \n 
converged \n 
for i ~~~ val_norm_l21_tf val_norm_l1_tf max_diff active_set_0 for ~~~ \n 
~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~ if ~~~ ~~ else ~~~ \n 
~~ ~~ if ~~~ ~~~ \n 
n_sources n_positions \n 
if Z_init ~~~ Z active_set ~~ else : \n 
~~~ active_set active for ~~~ ~~~ ~~ ~~ Z if ~~~ \n 
~~ ~~ Z , active_set M , phi tol \n 
while ~~~ active Z_init Z , E active Z_init Z_init active_set active_set_0 Z , E if ~~~ ~~~ \n 
~~ ~~ ~~ if active_set ~~~ Z X ~~ else : \n 
~~~ n_sensors n_step n_freq Z X \n 
n_step n_freq n_coefs phi = phiT = \n 
if n_orient ~~~ lc ~~ else : \n 
~~~ lc for ~~~ \n 
~~ ~~ logger X , Z , M , wsize log_objective \n 
if np . ~~~ bias X \n 
from ... utils from . res4 from . constants from . . constants \n 
_kind_dict \n 
_coord_dict \n 
~~~ p [ \n 
~~ for key ~~~ if ~~~ ~~ ~~ else : \n 
~~ ~~ ~~ ~~ from copy from datetime import os . \n 
from . pick from . constants from . open from . tree from . tag from . proj from . ctf_comp from . write from . proc_history from . . utils from . . fixes from . . import from . . externals \n 
_kind_dict eeg = ( FIFF mag = ( FIFF grad = ( ref_meg misc = ( stim = ( eog = ( FIFF ecg = ( FIFF seeg = ( bio = ( FIFF ecog = ( ) \n 
~~ def _anonymize ~~~ if ~~~ ~~ self for ~~~ ~~~ \n 
~~ chs if ~~~ \n 
fid , tree with fid ~~~ isotrak isotrak pts coord_frame for ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ if coord_frame ~~~ err raise \n 
pts_frames bad_frames if len ~~~ err raise \n 
~~ fid = start_block write_int for pt ~~~ write_dig_point \n 
~~ end_block end_file \n 
~~~ isotrak dig for ~~~ ~~~ ~~ ~~ ~~ return \n 
dig_points if dig_points ~~~ err raise \n 
_ , ext dig_points if ( dig_points ~~~ err raise \n 
~~ if ext ~~~ with ~~~ ~~ ~~ else : \n 
~~~ msg raise \n 
dig = if lpa ~~~ lpa if ~~~ ~~ else ~~~ ~~ ~~ if nasion ~~~ nasion if ~~~ ~~ else ~~~ ~~ ~~ if rpa ~~~ rpa if ~~~ ~~ else ~~~ ~~ ~~ if hpi ~~~ hpi if ~~~ ~~~ ~~ ~~ else ~~~ ~~ ~~ if dig_points ~~~ dig_points if ~~~ ~~~ ~~ ~~ else ~~~ ~~ ~~ if dig_ch_pos ~~~ keys for ~~~ ~~ ~~ return \n 
f , tree with f ~~~ info ~~ return \n 
nodes \n 
bads = if len ~~~ for ~~~ ~~~ ~~ ~~ ~~ return \n 
meas_info if len ~~~ raise ~~ if len ~~~ raise ~~ meas_info \n 
~~~ ~~~ ~~~ ~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ \n 
~~ if sfreq ~~~ raise \n 
~~ if dev_head_t ~~~ hpi_result if ~~~ ~~~ ~~~ ~~~ ~~~ \n 
if tree ~~~ info ~~ else : \n 
~~~ info \n 
subject_info si = None if len ~~~ subject_info si for ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~ ~~ ~~ info [ \n 
hpi_subsystem hs = None if len ~~~ hpi_subsystem hs for ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~ ~~ info [ \n 
~~ info [ info [ info [ info [ \n 
if meas_date ~~~ meas_date ~~ info [ \n 
info [ info [ info [ info [ \n 
info [ info [ info [ if dev_head_t ~~~ from head_ctf_trans dev_ctf_trans info \n 
info . _check_consistency \n 
for event ~~~ start_block if ~~~ ~~ if ~~~ ~~ end_block \n 
~~ end_block \n 
~~ if info ~~~ write_coord_trans \n 
~~ if info ~~~ hs start_block if ~~~ ~~ if ~~~ ~~ if ~~~ ~~~ ~~~ ~~ ~~ end_block \n 
end_block \n 
fid = start_block write_meas_info end_block end_file \n 
values msg = \n 
def _flatten ~~~ return \n 
~~ def _check_isinstance ~~~ return \n 
~~~ unique_values if ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ else ~~~ \n 
~~ ~~ ~~ @ verbose \n 
for info ~~~ info ~~ info = info [ for this_info ~~~ info ~~ info . _update_redundant duplicates if len ~~~ msg raise \n 
~~ transforms for trans_name ~~~ trans if ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ else ~~~ ~~ ~~ other_fields \n 
for k ~~~ info ~~ info . _check_consistency return \n 
if isinstance ~~~ ch_names ~~ if not ~~~ raise ~~ sfreq if sfreq ~~~ raise ~~ nchan if ch_types ~~~ ch_types ~~ if isinstance ~~~ ch_types ~~ if len ~~~ raise ~~ info = info [ loc = for ci ~~~ if ~~~ ~~ if ~~~ ~~ if ~~~ ~~ kind chan_info info ~~ info . _update_redundant if montage ~~~ from if ~~~ ~~ for ~~~ ~~~ ~~~ ~~~ ~~ ~~ ~~ info . _check_consistency return \n 
~~ RAW_INFO_FIELDS ) \n 
from . . io . from . . epochs from . . import from . . io . from . ica from . . utils from . . decoding from . . cov from . . channels \n 
nmin = nmax = \n 
n_samples epochs_data n_epochs events_pos \n 
data = for idx ~~~ onset offset data \n 
~~ events events data = evoked_data evokeds info = for name ~~~ n_events evoked evokeds \n 
~~ if return_toeplitz ~~~ return \n 
~~ class Xdawn \n 
if ~~~ ~~ self \n 
for ~~~ \n 
~~ if ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ else ~~~ ~~ return \n 
~~ picks raws for ~~~ \n 
~~ picks \n 
~~ epochs_dict data \n 
for \n 
~~ data evokeds \n 
if ~~~ ~~ elif ~~~ ~~ logger data \n 
~~ ~~ import numpy \n 
def _ecdf ( ~~~ \n 
nobs = return \n 
pvals shape_init pvals \n 
pvals_sortind pvals_sorted sortrevind \n 
if method ~~~ ecdffactor ~~ elif method ~~~ cm ecdffactor ~~ else : \n 
~~ reject if reject ~~~ rejectmax ~~ else : \n 
~~~ rejectmax ~~ reject \n 
pvals_corrected_raw pvals_corrected pvals_corrected pvals_corrected reject return \n 
pval = pval_corrected reject return ~~ import os \n 
import os . import shutil import glob import warnings import sys \n 
from numpy from nose . \n 
from mne . datasets from mne import from mne . label from mne . utils from mne . fixes from mne . label from mne . source_space from mne . source_estimate from mne . externals from mne . externals \n 
warnings . simplefilter \n 
data_path subjects_dir src_fname stc_fname real_label_fname real_label_rh_fname v1_label_fname \n 
fwd_fname src_bad_fname label_dir \n 
test_path label_fname label_rh_fname \n 
src = \n 
if isinstance ~~~ subject ~~ else : \n 
~~~ subject \n 
~~ if isinstance ~~~ subjects_dir surf_path_from rr_lh rr_rh rr tris ~~ else : \n 
~~~ if ~~~ ~~ if ~~~ ~~ rr tris \n 
~~ labels cnt = for hemi_idx ~~~ this_data e e . n_vertices e \n 
clusters \n 
cnt \n 
~~ labels \n 
~~ def assert_labels_equal ~~~ if comment ~~~ assert_equal ~~ if color ~~~ assert_equal \n 
~~ for attr ~~~ attr0 attr1 msg assert_equal ~~ for attr ~~~ a0 a1 assert_array_almost_equal \n 
label assert_is assert_true label assert_true assert_true \n 
pos = values idx0 = idx1 = idx2 = l0 = Label l1 = Label l2 = Label \n 
assert_equal \n 
l_good l_good l_bad l_bad . assert_raises assert_raises assert_raises assert_raises \n 
bhl2 = assert_labels_equal assert_equal assert_array_equal assert_array_equal assert_raises \n 
stc = label stc_label \n 
assert_true assert_true \n 
tempdir label \n 
names labels_lhrh \n 
label_fnames label_fnames labels_mne labels \n 
labels \n 
names \n 
for label ~~~ idx assert_labels_equal \n 
~~ assert_raises \n 
with warnings ~~~ warnings src ~~ src_bad stc = os . environ labels1 labels2 assert_equal for l1 ~~~ assert_labels_equal \n 
~~ with warnings ~~~ warnings labels_lh \n 
~~ assert_true assert_raises assert_raises assert_equal assert_equal \n 
with warnings ~~~ warnings labels_patch ~~ assert_equal assert_equal for l1 ~~~ assert_labels_equal \n 
tgt_names tgt_hemis for label ~~~ assert_true assert_true assert_equal assert_equal \n 
import mne from mne . datasets \n 
from mne . preprocessing from mne . preprocessing \n 
raw = mne . raw . filter \n 
picks_meg \n 
n_components method = decim = 3 \n 
ica . plot_components \n 
eog_average \n 
ica . plot_sources \n 
ica . exclude \n 
from mne . preprocessing \n 
reference_ica \n 
import os . import numpy from numpy from scipy \n 
import mne from mne import from mne . epochs from mne . stats from mne . minimum_norm from mne . datasets \n 
tmin = - 0.2 tmax = 0.3 \n 
event_id = epochs2 = \n 
fname_inv snr = 3.0 \n 
lambda2 = method = "dSPM" inverse_operator sample_vertices \n 
evoked1 = evoked1 . resample condition1 evoked2 = evoked2 . resample condition2 \n 
condition1 condition2 tmin = condition1 tstep = condition1 \n 
np . random . X = randn ( X [ : , : , : , X [ : , : , : , \n 
if os . path ~~~ print ( ~~ else : \n 
G = nx . gnp_random_graph \n 
plt . loglog plt . title ( plt . ylabel plt . xlabel \n 
G = nx . complete_graph A = nx . nx_agraph X1 = nx . nx_agraph X2 = nx . Graph G1 = nx . Graph \n 
A . write ( X3 = nx . nx_agraph \n 
if G . is_directed ~~~ import def ~~~ ~~ ~~ else : \n 
~~~ neighbors \n 
~~~ color return ~~ except ~~~ return \n 
S = set ( for CC ~~~ X , if ~~~ ~~ ~~ return \n 
c = color X = set Y = set return \n 
n = len ( m = nx . number_of_edges nb = len nt = n - nb if m == ~~~ d = ~~ else : \n 
~~~ if ~~~ ~~ else ~~~ ~~ ~~ return \n 
bottom top = set return \n 
__author__ __all__ = \n 
~~~ from scipy \n 
~~ ~~ import networkx \n 
__all__ = \n 
result while ~~~ result ~~ return \n 
for i ~~~ seq while ~~~ ~~ if ~~~ ~~ ~~ raise \n 
from itertools \n 
import networkx from ... utils from ... utils from ... utils \n 
__author__ \n 
n = len if n < ~~~ raise ~~ if not ~~~ raise \n 
for u , ~~~ if ~~~ \n 
~~ ~~ cut_value nodes contractions \n 
~~ import networkx from nose . import os \n 
class TestMinCostFlow ~~~ def test_simple_digraph ~~~ G G . G . G . G . G . G . flowCost soln assert_equal assert_equal assert_equal assert_equal assert_equal \n 
flowCost assert_equal assert_equal assert_equal \n 
~~ def test_negcycle_infcap ~~~ G G . G . G . G . G . G . G . G . assert_raises assert_raises \n 
~~ def test_sum_demands_not_zero ~~~ G G . G . G . G . G . G . G . G . assert_raises assert_raises \n 
~~ def test_no_flow_satisfying_demands ~~~ G G . G . G . G . G . G . G . G . assert_raises assert_raises \n 
~~ def test_transshipment ~~~ G G . G . G . G . G . G . G . G . G . G . G . G . G . G . G . G . G . G . G . G . G . G . G . flowCost soln assert_equal assert_equal assert_equal assert_equal assert_equal \n 
~~ def test_max_flow_min_cost ~~~ G G . G . G . G . G . G . soln flow assert_equal assert_equal \n 
G . flowCost G . assert_equal assert_equal del assert_equal assert_equal \n 
G G . G . G . G . G . G . \n 
G . flowCost G . flow assert_equal assert_equal assert_equal del assert_equal assert_equal assert_equal assert_equal assert_equal \n 
flowCost soln assert_equal assert_equal assert_equal assert_equal assert_equal \n 
flowCost assert_equal assert_equal \n 
G G . G . assert_raises assert_raises G [ flowCost assert_equal assert_equal flowCost assert_equal assert_equal \n 
from heapq from itertools \n 
import networkx from networkx \n 
if source ~~~ msg raise \n 
~~ if heuristic ~~~ def ~~~ \n 
~~ ~~ push = pop = \n 
~~ explored \n 
for ~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~ from nose . import networkx from networkx \n 
class TestEuler \n 
~~~ def test_is_eulerian ~~~ assert_true assert_true assert_true assert_true \n 
assert_false assert_false assert_false assert_false \n 
assert_false assert_false \n 
~~ def test_eulerian_circuit_cycle ~~~ G = \n 
edges nodes assert_equal assert_equal \n 
G = \n 
~~ def test_eulerian_circuit_digraph ~~~ G = nx \n 
~~ def test_eulerian_circuit_multigraph ~~~ G = nx G . G . edges nodes assert_equal assert_equal \n 
__author__ ] ) \n 
if len ~~~ raise \n 
~~ if G . is_directed ~~~ components ~~ else : \n 
~~~ components \n 
~~ if G . is_directed ~~~ is_connected ~~ else : \n 
~~~ is_connected \n 
from functools \n 
from networkx from networkx from networkx \n 
class TestModularity ~~~ numpy \n 
@ classmethod def setupClass ~~~ global global global global try ~~~ ~~ except ~~~ \n 
permutation assert_equal assert_equal \n 
from nose . import io \n 
import tempfile import os \n 
class TestEdgelist \n 
~~~ def setUp ~~~ self e = self self self self self self \n 
bytesIO G assert_edges_equal \n 
~~ def test_write_edgelist_1 ~~~ fh G = G . nx fh assert_equal \n 
~~ def test_write_edgelist_2 ~~~ fh G = G . nx fh assert_equal \n 
~~ def test_write_edgelist_3 ~~~ fh G = G . G . nx fh assert_equal \n 
~~ def test_write_edgelist_4 ~~~ fh G = G . G . nx fh assert_equal \n 
~~ def test_unicode ~~~ G try ~~~ ~~ except ~~~ ~~ G . fd nx H assert_graphs_equal os os \n 
~~ def test_latin1_issue ~~~ G try ~~~ ~~ except ~~~ ~~ G . fd assert_raises os os \n 
~~ def test_latin1 ~~~ G try ~~~ ~~ except ~~~ ~~ G . fd nx H assert_graphs_equal os os \n 
~~ def test_edgelist_graph ~~~ G = ( fd nx H = H2 assert_not_equal G . assert_nodes_equal assert_edges_equal os os \n 
~~ def test_edgelist_digraph ~~~ G = ( fd nx H = H2 assert_not_equal G . assert_nodes_equal assert_edges_equal os os \n 
~~ def test_edgelist_digraph ~~~ G = ( fd nx H = G . H2 assert_not_equal assert_nodes_equal assert_edges_equal os os \n 
~~ def test_edgelist_multigraph ~~~ G = ( fd nx H = H2 assert_not_equal assert_nodes_equal assert_edges_equal os os \n 
if xmin ~~~ raise ~~ if alpha ~~~ raise ~~ if not ~~~ random ~~ a1 = alpha b = 2 ** while ~~~ u v x t if ~~~ ~~ ~~ return \n 
cdf = [ psum = float for i ~~~ cdf ~~ return \n 
import \n 
if cdistribution ~~~ cdf ~~ elif distribution ~~~ cdf ~~ else : \n 
if k > ~~~ raise ~~ sample while ~~~ sample ~~ return \n 
~~ ~~ ~~ import argparse import os \n 
import sys sys . path += import OCP os . environ from django \n 
import ocpcaproj \n 
def main ( ) \n 
~~~ parser = parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument \n 
result = \n 
~~ if __name__ ~~~ main ( ) \n 
~~ from django \n 
from django from django from django import numpy import urllib2 import json import re \n 
from contextlib \n 
import cStringIO \n 
from django \n 
from ocpcaerror import ocpcaproj import ocpcadb import ocpgraph \n 
import logging logger = logging \n 
import tempfile import tarfile import zipfile import os \n 
def getResponse \n 
~~~ output try : \n 
~~~ output ~~ except ~~~ logger raise ~~ finally ~~~ output \n 
~~ wrapper response response response return \n 
~~~ if re . ~~~ m [ syntoken arguementType ~~ elif re ~~~ m [ syntoken arguementType ~~ elif re ~~~ m [ syntoken arguementType ~~ else : \n 
~~~ logger raise \n 
~~ ~~ except Exception ~~~ logger raise \n 
~~~ synproj \n 
class Migration \n 
~~~ dependencies ( ] \n 
operations migrations ) , migrations ) , ] \n 
~~ import argparse import empaths import numpy import urllib import cStringIO import zlib import sys \n 
import tempfile import h5py \n 
~~~ parser = parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument \n 
anndata \n 
idgrp . create_dataset idgrp . create_dataset idgrp . create_dataset \n 
if result ~~~ url = ~~ elif result ~~~ url = ~~ elif result ~~~ url = ~~ else : \n 
~~~ url = \n 
~~ if result ~~~ url += \n 
~~ print url \n 
~~~ h5fh . flush tmpfile req = response ~~ except urllib2 ~~~ print print sys . exit \n 
~~ import numpy from PIL import import urllib2 import zlib import StringIO import os \n 
import sys import cStringIO import zindex \n 
sys . path += \n 
import emcaproj import emcadb import dbconfig \n 
_ximagesz _yimagesz _xcubedim _ycubedim _zcubedim \n 
fid = open \n 
resolution \n 
for zstart \n 
~~~ zend = min \n 
slab = np slab = slab \n 
for y in ~~~ for x \n 
~~~ mortonidx cubedata \n 
xmin ymin xmax ymax zmin zmax \n 
cubedata \n 
~~ ~~ import argparse import sys import os \n 
import numpy from PIL import import urllib import cStringIO import collections import zlib \n 
import kanno_cy \n 
~~~ parser = parser . add_argument parser . add_argument parser . add_argument \n 
~~~ newdata for ~~~ \n 
~~ ~~ zlow zhigh ylow yhigh xlow xhigh \n 
~~ ~~ ~~ ~~ ~~ ~~ if __name__ ~~~ main ( ) \n 
~~ import argparse import sys import os \n 
import argparse import cStringIO import numpy from PIL import import zlib from contextlib \n 
sys . path += import OCP os . environ from django \n 
import imagecube import ocplib import ocpcarest import ocpcaproj import ocpcadb \n 
END_WINDOWS \n 
class CatmaidIngest \n 
self . token self . path self . resolution \n 
with closing ~~~ proj \n 
~~ with closing \n 
~~~ ( startslice ( xcubedim ( ximagesz batchsz \n 
~~~ slab \n 
~~ ~~ ~~ for ~~~ \n 
~~~ ~~ db slab \n 
~~ ~~ ~~ ~~ def main ( ) \n 
ci = CatmaidIngest ci . ingest \n 
~~ import numpy from PIL import \n 
from cube \n 
class MaskCube \n 
~~ def xySlice \n 
~~~ zdim , ydim outimage outimage \n 
~~ def xzSlice \n 
~~~ zdim , ydim outimage newimage newimage \n 
~~ def yzSlice \n 
super ( self . data \n 
self . data super ( \n 
~~ ~~ import numpy from PIL import import urllib2 import zlib import StringIO import os \n 
_ximagesz _yimagesz \n 
import sys import random import argparse import numpy import tempfile import h5py import urllib2 import zlib import cStringIO import blosc import time import json from operator from functools \n 
sys . path += import OCP os . environ \n 
from ocplib \n 
~~ ~~ class BenchmarkTest \n 
~~~ def __init__ ~~~ """Init""" \n 
self . host self . token self . channels self . resolution self . getProjInfo \n 
min_values max_values range_args \n 
parser = parser . add_argument parser . add_argument parser . add_argument parser . add_argument parser . add_argument result = \n 
bt = BenchmarkTest bt . readTest \n 
import sys import urllib2 import argparse import time import multiprocessing import glob from sets \n 
~~~ parser = parser . add_argument result = \n 
fileList newFileList keyFileList \n 
for name ~~~ fileNumber newFileList \n 
~~ print keyFileList \n 
~~ from mpl_toolkits import matplotlib import mpl_toolkits import os \n 
import numpy from scipy from numpy import nibabel import splines_approximation_v2 from sklearn \n 
def returnSquaredErrors ~~~ if fname ~~~ fname ~~ file = data = p1 , p2 nx , ny #os.remove(fname) \n 
X , Y , Z \n 
zmin = zmax = \n 
print print \n 
nz = zmax \n 
x = [ 0 y = [ 0 z = [ iz \n 
for iz ~~~ x [ \n 
~~ x_fit , \n 
msx = msy = mean_ms \n 
fig1 = ax = Axes3D ax . plot ax . plot plt . show \n 
print size_crop \n 
txt = txt . write \n 
txt . close \n 
~~ def get_good_centerline ~~~ x = [ nx y = [ ny return \n 
~~ def fit ( data ~~~ x = [ 0 y = [ 0 z = [ iz \n 
~~ if __name__ ~~~ returnSquaredErrors \n 
~~ import numpy \n 
from wavelet from wavelet \n 
if paramreg ~~~ print sys ~~ else : \n 
from msct_register_regularized from numpy from msct_smooth \n 
from nibabel from msct_smooth from msct_register_regularized from numpy import name_warp_syn \n 
im_warp_x data_warp_x im_warp_y data_warp_y hdr_warp im_warp_x_inverse data_warp_x_inverse im_warp_y_inverse data_warp_y_inverse hdr_warp_inverse \n 
from nibabel from msct_smooth from msct_register_regularized from numpy import from msct_image \n 
import numpy import sys import sct_utils from msct_image from msct_parser from msct_gmseg_utils \n 
im_ext if data_path ~~~ data_path ~~ original_path os . chdir for subject_dir ~~~ subject_path if ~~~ ~~~ ~~~ \n 
~~ ~~ os . chdir \n 
im_ext if data_path ~~~ data_path ~~ original_path os . chdir for subject_dir ~~~ if ~~~ ~~~ ~~~ \n 
~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ \n 
arguments if ~~~ ~~ if ~~~ \n 
~~ ~~ import commands \n 
import sct_utils \n 
path_results \n 
if not os . ~~~ os . makedirs \n 
~~ if not os . ~~~ os . makedirs \n 
import os , \n 
import sct_utils import nibabel from scipy from numpy import matplotlib import time \n 
#Parameters: height_of_template_space x_size_of_template_space y_size_of_template_space number_labels_for_template straightening_parameters \n 
class TimeObject ~~~ def __init__ ~~~ self self self self self \n 
~~ def start ~~~ self self \n 
~~ def one_subject_done ~~~ self self remaining_subjects time_one_subject remaining_time hours minutes sct \n 
~~ def stop ~~~ self hours minutes sct self \n 
~~ def printRemainingTime ~~~ remaining_subjects time_one_subject remaining_time hours minutes if ~~~ ~~ else ~~~ \n 
~~ ~~ def printTotalTime ~~~ hours minutes if ~~~ ~~ else ~~~ \n 
~~ ~~ ~~ timer = dict timer [ timer [ timer [ \n 
align_vertebrae timer [ timer [ \n 
sct . printv timer [ sct . printv timer [ sct . printv timer [ \n 
~~ def do_preprocessing \n 
file_results ymin_anatomic ymax_anatomic for ~~~ ~~~ ~~ ~~ file_results \n 
os \n 
~~ ~~ print path_seg image_seg from nx data_seg hdr_seg z_centerline for ~~~ ~~~ ~~ ~~ img_seg nibabel \n 
~~ print labels_updown list_file_info for ~~~ ~~~ ~~ ~~ if ~~~ ~~ if ~~~ ~~ else ~~~ \n 
~~ print sct \n 
file data_c \n 
X , \n 
z_max \n 
z_min \n 
from n_i average compteur \n 
img_T1 data_T1 X , Z Z . \n 
~~ ~~ compteur \n 
img_T2 data_T2 X , Z Z . \n 
~~ ~~ average \n 
template_absolute_path print print from nx print print \n 
img data hdr data compteur for ~~~ \n 
~~~ print os print template_shape sct \n 
~~ ~~ def align_vertebrae ~~~ for i ~~~ subject \n 
print sct os \n 
timer \n 
~~ import sct_utils from msct_types \n 
~~ def __safe_cast__ ~~~ return \n 
type_option if ~~~ \n 
~~ elif ~~~ \n 
~~ elif ~~~ ~~~ \n 
~~ elif ~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ else ~~~ \n 
~~ def checkIfNifti ~~~ import sct nii niigz viewer param_tmp if ~~~ ~~~ ~~~ ~~ elif ~~~ ~~~ ~~~ ~~ elif ~~~ ~~ else ~~~ \n 
~~ if ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ else ~~~ \n 
~~ def add_option ~~~ order self \n 
~~ def parse ~~~ self \n 
skip for ~~~ ~~~ \n 
~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ for ~~~ ~~~ \n 
~~ def set_description ~~~ self \n 
~~ def addSection ~~~ self \n 
~~ def set_usage ~~~ from self sorted_arguments mandatory for ~~~ ~~ self \n 
~~ def set_arguments ~~~ sorted_arguments mandatory optional if ~~~ ~~~ ~~~ ~~ ~~ if ~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ if ~~~ \n 
~~ ~~ def refactor_type_value ~~~ if ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ else ~~~ ~~ return \n 
~~ def set_example ~~~ from self sorted_arguments for ~~~ ~~~ ~~~ \n 
~~ ~~ def error ~~~ if ~~~ ~~ else ~~~ \n 
~~ ~~ def print_list_with_brackets ~~~ type_value for ~~~ ~~ type_value type_value return \n 
~~ def set_usage ~~~ from self sorted_arguments mandatory for ~~~ ~~~ ~~~ ~~ ~~ self \n 
~~ def set_arguments ~~~ sorted_arguments mandatory optional if ~~~ ~~~ ~~~ ~~ ~~ if ~~~ ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ ~~ def refactor_type_value ~~~ if ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ else ~~~ ~~ return \n 
~~ def set_example ~~~ from self sorted_arguments mandatory for ~~~ ~~~ ~~~ ~~ ~~ self \n 
~~ def error ~~~ if ~~~ ~~ else ~~~ \n 
~~ def words ~~~ from return \n 
~~ def train ~~~ from model for ~~~ ~~ return \n 
~~ def edits1 ~~~ splits deletes transposes replaces inserts return \n 
~~ def known_edits2 ~~~ return \n 
~~ def known \n 
def correct ~~~ return ######################################################################################### # \n 
fsloutput \n 
class Param ~~~ def __init__ ~~~ self self self self self self self self self self self self self \n 
~~ ~~ def main ( ) ~~~ start_time \n 
~~~ opts ~~ except ~~~ usage ~~ if not ~~~ ~~ for opt ~~~ if ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ ~~ elif ~~~ \n 
~~ if param \n 
~~~ sct . printv sct . printv sct . printv \n 
fname_data min_norm cost_function verbose \n 
sct . printv sct . printv \n 
if param if not mat_eddy \n 
~~~ fname_data_new \n 
~~~ nb_loops file_suffix \n 
opposite_gradients_iT opposite_gradients_jT index_identified index_b0 for iT ~~~ if ~~~ ~~~ ~~~ ~~ ~~ ~~ else ~~~ ~~ ~~ nb_oppositeGradients sct . printv sct . printv \n 
sct sct \n 
~~ ~~ sct . printv sct . printv \n 
for iN ~~~ for ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ sct . printv sct . printv \n 
for iN ~~~ i_plus fname_plus_corr cmd \n 
for ~~~ ~~ status \n 
i_minus fname_minus_corr cmd \n 
fname_data_corr cmd = path_tmp for iT ~~~ if ~~~ ~~ elif ~~~ \n 
~~ cmd ~~ status \n 
~~~ fname_data_final \n 
~~ sct . printv \n 
sct . printv sct . printv sct . printv \n 
~~ def usage ( ~~~ print \n 
from msct_parser import sct_utils from sct_extract_metric \n 
arguments \n 
fname_src fname_transfo warp_atlas warp_spinal_levels folder_out path_template verbose qc = int \n 
def test ( data_path \n 
class Clusterable \n 
def __init__ \n 
~~~ self self \n 
~~ def transform ~~~ data data self return \n 
~~ ~~ def magic ( ~~~ \n 
roi = \n 
if method ~~~ reference ~~ elif method ~~~ reference feature_data n_studies reference ~~ elif method ~~~ reference \n 
~~ if reduce_reference ~~~ if ~~~ \n 
~~ transpose reference \n 
~~ if method ~~~ distances ~~ else : \n 
~~~ distances \n 
header header header voxel_labels img = \n 
if output_dir ~~~ if ~~~ ~~ if ~~~ ~~ outfile img \n 
~~~ AWS_SECRET_ACCESS_KEY AWS_ACCESS_ID AWS_BUCKET_NAME host = ~~ except : \n 
for key ~~~ if ~~~ \n 
~~ ~~ sys . exit \n 
HOSTNAME = \n 
def run_all \n 
~~~ app_id = num_arms true_means total_pulls_per_client \n 
num_experiments \n 
pool = Pool \n 
alg_list for alg_id ~~~ alg_item alg_item alg_item alg_item alg_list ~~ params = params [ for algorithm ~~~ params ~~ algorithm_management_settings algorithm_management_settings algorithm_management_settings \n 
exp_info for ell ~~~ url = response print if assert_200 initExp_response_dict \n 
exp_uid exp_key \n 
exp_info \n 
~~ participants pool_args for i in ~~~ participant_uid participants \n 
experiment exp_uid exp_key pool_args \n 
~~ results \n 
for result ~~~ print \n 
~~ ~~ def simulate_one_client ~~~ exp_uid , avg_response_time \n 
getQuery_times processAnswer_times for t in \n 
url = response print if assert_200 print getQuery_times print \n 
query_dict query_uid targets for target ~~~ if target ~~~ index_left ~~ if target ~~~ index_right ~~ if target ~~~ index_painted \n 
time . sleep reward_left reward_right if reward_left ~~~ index_winner ~~ else : \n 
~~~ index_winner \n 
~~ response_time \n 
url = print response print if assert_200 print processAnswer_times print \n 
processAnswer_json_response \n 
~~ processAnswer_times getQuery_times return_str return return_str \n 
def timed ~~~ ts = time result te = time if type ~~~ return ~~ else : \n 
~~~ return ~~ ~~ return timed \n 
import numpy import numpy import json import time import traceback \n 
from next . import next from next . from next . \n 
class PoolBasedTripletMDS \n 
~~~ def __init__ ~~~ self . app_id \n 
~~ def daemonProcess ~~~ try : \n 
~~~ app_id \n 
log_entry ell . \n 
~~ ~~ alg_daemon_args alg_uid alg_id \n 
rc = \n 
didSucceed \n 
log_entry log_entry_durations log_entry_durations meta \n 
daemon_message args_out response_json \n 
~~ except ~~~ error log_entry ell . return \n 
didSucceed didSucceed didSucceed didSucceed didSucceed \n 
didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed didSucceed \n 
import git_hash \n 
db . set db . set db . set \n 
~~ ~~ n = d = delta \n 
~~ if ~~~ instructions ~~ else ~~~ instructions \n 
~~ if ~~~ debrief ~~ else ~~~ debrief \n 
~~ if ~~~ num_tries ~~ else ~~~ num_tries \n 
~~ algorithm_management_settings algorithm_management_settings algorithm_management_settings ~~ else ~~~ algorithm_management_settings \n 
try ~~~ ~~ except ~~~ \n 
~~ ~~ ~~ elif ~~~ ~~ elif ~~~ ~~ else ~~~ \n 
db db db \n 
~~ db . set db . set db . set db . set db . set db . set db . set db . set db . set db . set db . set db . set \n 
rc \n 
log_entry ell \n 
~~ response_json \n 
~~ except ~~~ error log_entry print ell . return \n 
~~ algorithm_management_settings \n 
~~ participant_doc_exists first_participant_query if first_participant_query ~~~ db db \n 
~~ participant_to_algorithm_management if ( \n 
~~~ if ~~~ ~~ else ~~~ \n 
~~ alg_id alg_uid alg_label \n 
~~ rc = \n 
log_entry_durations log_entry_durations meta \n 
~~ args_out response_json \n 
return ~~ except ~~~ error log_entry ell . return \n 
~~ ~~ rc = \n 
targets for ~~~ if ~~~ ~~ elif ~~~ ~~ elif ~~~ \n 
~~ ~~ index_winner \n 
n , didSucceed \n 
if num_reported_answers ~~~ predict_id params predict_args_dict predict_args_json \n 
db ############### \n 
~~ response_args_dict args_out response_json \n 
~~ ~~ predict_id params \n 
alg_label \n 
~~ ~~ meta if predict_id \n 
~~~ rc \n 
params params \n 
log_entry log_entry \n 
ell \n 
params response_args_dict \n 
~~ elif \n 
S for ~~~ ~~~ \n 
~~ ~~ params params response_args_dict \n 
~~ args_out predict_json \n 
return ~~ except ~~~ error log_entry didSucceed return \n 
~~ ~~ stat_id params \n 
dashboard \n 
~~ else ~~~ raise \n 
return ~~ except ~~~ error log_entry ell . return #!/usr/bin/python ~~ ~~ ~~ import time import traceback import next \n 
import next broker = next \n 
while ( 1 ) : \n 
~~~ timestamp try : \n 
~~~ broker . refresh_domain_hashes ~~ except Exception ~~~ error = print error \n 
from __future__ from time import ast import hashlib import os \n 
import shutil import subprocess import sys import traceback import warnings \n 
~~ ~~ def ~~~ ~~~ ~~ ~~ return \n 
~~~ from StringIO ~~ except ImportError ~~~ from io \n 
~~ from . import from . backreferences from . notebook \n 
~~~ basestring ~~ except NameError ~~~ basestring \n 
def __init__ ~~~ self self \n 
~~ def write ~~~ self self \n 
~~ def flush ~~~ self self \n 
############################################################################### ~~ ~~ CODE_DOWNLOAD \n 
CODE_OUTPUT \n 
with open ~~~ content \n 
docstring \n 
blocks \n 
pattern flags \n 
pos_so_far for match ~~~ match_start_pos code_block_content text_content sub_pat text_block_content if ~~~ ~~ if ~~~ ~~ pos_so_far \n 
~~ remaining_content if remaining_content ~~~ blocks \n 
~~~ return ~~ except ~~~ return \n 
paragraphs if len ~~~ first_paragraph first_paragraph ~~ else : \n 
with open ~~~ src_content \n 
~~ src_md5 ~~ return \n 
src_md5 \n 
src_md5_file src_file_changed if os . ~~~ with ~~~ ~~ if ~~~ \n 
~~ ~~ if src_file_changed ~~~ with ~~~ \n 
first_image_file has_image src_file_changed \n 
figure_list \n 
fig_managers for fig_mngr ~~~ fig kwargs to_rgba for ~~~ ~~~ \n 
~~ ~~ current_fig fig figure_list \n 
~~ if gallery_conf ~~~ from e last_matplotlib_fig_num total_fig_num mayavi_fig_nums \n 
for ~~~ ~~ mlab \n 
~~~ from ~~ except ~~~ import ~~ img = width_in scale_w scale_h \n 
if height_in ~~~ scale ~~ else : \n 
~~~ scale \n 
~~ if scale ~~~ return \n 
~~ width_sc height_sc \n 
~~ thumb_file \n 
~~ fhindex if not ~~~ os ~~ sorted_listdir entries_text for fname ~~~ amount_of_code new_fname intro write_backreferences this_entry entries_text \n 
for _ , ~~~ fhindex \n 
t_start exec time_elapsed \n 
sys \n 
my_stdout if ~~~ ~~ os figure_list \n 
image_list if ~~~ ~~ elif ~~~ ~~~ \n 
~~ ~~ ~~ except ~~~ formatted_exception \n 
print print print print \n 
figure_list image_list \n 
~~ ~~ finally ~~~ os sys \n 
~~ print ( code_output \n 
src_file example_file shutil \n 
image_dir if not ~~~ os \n 
~~ base_image_name image_fname image_path \n 
script_blocks \n 
amount_of_code \n 
if _plots_are_current ~~~ return \n 
~~ time_elapsed \n 
ref_fname example_rst example_nb \n 
~~~ ~~ ~~ ~~ else : \n 
~~~ for ~~~ ~~~ ~~~ \n 
~~ ~~ ~~ save_thumbnail \n 
time_m example_nb with open ~~~ example_rst f . \n 
masker = input_data dmn_coords detrend low_pass memory \n 
func_filename confound_filename \n 
time_series \n 
~~ plt . title ( plt . xlabel plt . ylabel plt . legend plt . tight_layout \n 
import shutil import csv import numpy \n 
import nibabel \n 
from nose from nose . \n 
from nilearn from . import \n 
from nilearn \n 
def setup_mock ~~~ return \n 
~~ def teardown_mock ~~~ return \n 
expected_base_dir data_dir assert_equal assert shutil \n 
expected_base_dir os . environ data_dir assert_equal assert shutil \n 
expected_base_dir expected_dataset_dir data_dir assert_equal assert shutil \n 
no_write os . makedirs os . chmod \n 
ho_dir os . makedirs nifti_dir os . makedirs \n 
target_atlas_nii \n 
nibabel target_atlas_nii \n 
dummy dummy . dummy . \n 
ho = atlas \n 
assert_true assert_true assert_equal assert_equal assert_equal assert_equal assert_equal assert_equal \n 
~~ @ with_setup @ with_setup def test_fetch_atlas_craddock_2012 ~~~ bunch \n 
keys = filenames "scorr05_mean_all.nii.gz" "tcorr05_mean_all.nii.gz" "scorr05_2level_all.nii.gz" "tcorr05_2level_all.nii.gz" "random_all.nii.gz" ] \n 
assert_equal for key ~~~ assert_equal ~~ assert_not_equal \n 
~~ @ with_setup @ with_setup def test_fetch_atlas_smith_2009 ~~~ bunch \n 
keys = filenames "rsn20.nii.gz" "PNAS_Smith09_rsn10.nii.gz" "rsn70.nii.gz" "bm20.nii.gz" "PNAS_Smith09_bm10.nii.gz" "bm70.nii.gz" ] \n 
~~ def test_fetch_coords_power_2011 ~~~ bunch assert_equal assert_not_equal \n 
~~ @ with_setup @ with_setup def test_fetch_atlas_destrieux_2009 ~~~ datadir os . mkdir dummy datadir dummy . dummy . bunch \n 
assert_equal assert_equal tst \n 
dummy datadir dummy . dummy . bunch lateralized \n 
~~ @ with_setup @ with_setup def test_fetch_atlas_msdl ~~~ datadir os . mkdir os . mkdir data_dir csv = with open ~~~ header csv_file np \n 
~~ dataset assert_true assert_true assert_true assert_true assert_equal assert_not_equal \n 
~~ @ with_setup @ with_setup def test_fetch_atlas_yeo_2011 ~~~ dataset assert_true assert_true assert_true assert_true assert_true assert_true assert_true assert_equal assert_not_equal \n 
~~ @ with_setup @ with_setup def test_fetch_atlas_aal ~~~ ho_dir os . makedirs with open ~~~ xml_file ~~ dataset assert_true assert_true assert_true assert_equal \n 
assert_raises_regex \n 
assert_not_equal \n 
~~ @ with_setup @ with_setup def test_fetch_atlas_basc_multiscale_2015 ~~~ data_sym data_asym \n 
keys = \n 
dataset_name name_sym basenames_sym for key ~~~ assert_equal \n 
~~ name_asym basenames_asym for key ~~~ assert_equal \n 
~~ assert_equal assert_raises_regex \n 
assert_equal assert_not_equal assert_not_equal \n 
import numpy from numpy from nose . import nibabel \n 
from nilearn from nilearn from nilearn \n 
component2 component2 component2 \n 
component3 component3 component3 \n 
component4 component4 component4 \n 
~~ def _make_canica_test_data ~~~ if rng ~~~ rng ~~ shape affine components if noisy ~~~ components \n 
~~ for mp ~~~ assert_less_equal \n 
~~ def test_canica_square_img ~~~ data , \n 
~~~ data , \n 
from matplotlib from matplotlib \n 
orig_cdict \n 
cdict cdict [ cdict [ cdict [ \n 
for color ~~~ cdict \n 
cdict \n 
cdict1 cdict2 if not ~~~ for ~~~ ~~ ~~ else : \n 
~~~ for ~~~ ~~ ps colors for ~~~ ~~ ~~ if not ~~~ for ~~~ ~~ ~~ else : \n 
~~~ ps colors for ~~~ \n 
red , green if name ~~~ name ~~ cmapspec cmap = cmap . _init cmap . _lut cmap . _lut return \n 
if hasattr ~~~ _cmaps_data ~~ if hasattr ~~~ _cmaps_data _cmaps_data \n 
for _cmapname ~~~ _cmapname_r _cmapspec _cmaps_data _cmap_d _cmap_d \n 
~~ locals ( ) . update \n 
assert if to_white ~~~ dimmer ~~ else : \n 
~~~ dimmer ~~ cdict for c_index ~~~ color_lst for ~~~ ~~ cdict \n 
assert assert assert outer_cdict inner_cdict \n 
cdict for this_cdict ~~~ if ~~~ ~~~ \n 
~~ ~~ ~~ for c_index ~~~ color_lst \n 
~~ color_lst \n 
~~ cdict \n 
from nose . \n 
from nilearn from nilearn \n 
def test_check_threshold ~~~ matrix \n 
name = wrong_thresholds for wrong_threshold ~~~ assert_raises_regex \n 
~~ threshold assert_raises_regex \n 
sys_path \n 
states = ( ( ) \n 
t_COLON t_SPACE t_WILDCARD \n 
def t_COMMENT_ID ~~~ return \n 
~~ def t_EVENT_ID ~~~ return \n 
~~ def t_MODIFIER_ID ~~~ return \n 
~~ def t_RELATION_ID ~~~ return \n 
~~ def t_TEXT_BOUND_ID ~~~ return \n 
~~ def t_NEWLINE ~~~ \n 
~~ def t_TAB ( ~~~ \n 
~~ def t_INTEGER ~~~ t . value return \n 
~~ def t_TYPE ~~~ return \n 
~~ def t_freetext_FREETEXT ~~~ return \n 
~~ def t_freetext_TAB ~~~ \n 
~~ def t_freetext_NEWLINE ~~~ \n 
~~ def t_freetext_error ~~~ return \n 
~~ lexer = lex lexer . line_tab_count \n 
if __name__ ~~~ from sys for line ~~~ lexer \n 
from Cookie from atexit from datetime from hashlib from os import from os . path from shutil from shutil from tempfile \n 
~~~ from cPickle ~~ except ImportError ~~~ from pickle \n 
~~ from config \n 
~~ class SessionCookie ~~~ def __init__ ~~~ if ~~~ \n 
~~ ~~ def set_expired ~~~ self \n 
~~ def set_sid ~~~ self \n 
~~ def get_sid ~~~ return \n 
~~ ~~ class Session ~~~ def __init__ ~~~ self sid self \n 
~~ ~~ def get ~~~ try ~~~ ~~ except ~~~ \n 
~~ ~~ def get_sid ~~~ return \n 
~~ ~~ def get_session_pickle_path ~~~ return \n 
~~ def init_session ~~~ if cookie_data ~~~ cookie ~~ else : \n 
~~~ cookie \n 
~~~ try ~~~ ~~ except ~~~ \n 
~~ ~~ def get_session ~~~ if CURRENT_SESSION ~~~ raise ~~ return \n 
~~ def invalidate_session ~~~ global if CURRENT_SESSION ~~~ return \n 
~~~ makedirs ~~ except ~~~ if ~~~ ~~ else ~~~ \n 
~~~ tmp_file_fh os_close \n 
~~ ~~ ~~ def save_conf ~~~ get_session return \n 
~~ def load_conf ~~~ try : \n 
~~~ get_session assert ~~ except ~~~ pass \n 
~~~ session session ~~ except ~~~ assert \n 
~~~ tmp_file_fh os_close session session with ~~~ ~~ del \n 
~~ ~~ ~~ from __future__ \n 
import sys import re \n 
INPUT_ENCODING OUTPUT_ENCODING \n 
output_directory \n 
def quote ( ~~~ return \n 
~~~ if re . ~~~ return ~~ if re . ~~~ return ~~ if quote ~~~ return ~~ if quote ~~~ return ~~ return \n 
~~ def output ~~~ global \n 
if output_directory ~~~ txtout soout ~~ else : \n 
~~~ outfn txtout soout \n 
~~ offset \n 
doctext \n 
for si \n 
~~~ prev_token prev_tag curr_start quote_count \n 
~~ if ~~~ ~~ ~~ print \n 
~~ def process ~~~ docnum sentences \n 
with codecs \n 
lines \n 
~~ ~~ ~~ def main ( argv ~~~ global \n 
~~ fail_count for fn ~~~ try ~~~ ~~ except ~~~ \n 
from argparse from cgi import \n 
~~ from sys import from urlparse try : \n 
~~ offset for token ~~~ if ~~~ ~~ offset ~~ return \n 
~~ class TokenizerHandler ~~~ def do_POST ~~~ field_storage \n 
self print \n 
~~ def log_message ~~~ return \n 
~~ ~~ def main ( args ~~~ argp = \n 
server_class httpd print try : \n 
~~~ httpd ~~ except ~~~ pass ~~ httpd . print \n 
from nltk . \n 
from nltk . from nltk . from nltk . \n 
if binary ~~~ chunker_pickle ~~ else : \n 
~~~ chunker_pickle ~~ chunker return \n 
import itertools from nltk . \n 
from nltk . from nltk . from nltk . from nltk . \n 
def __init__ ~~~ self self self \n 
~~ @ classmethod def _build_new_documents ~~~ padding if ~~~ ~~ if ~~~ \n 
~~ @ staticmethod def _ngram_freqdist ~~~ return \n 
default_ws \n 
~~ for ~~~ ~~~ ~~~ ~~~ ~~ ~~ ~~ return \n 
~~ wfd wildfd bfd tfd for ~~~ ~~~ ~~~ ~~~ ~~~ ~~ ~~ return \n 
~~ @ classmethod def from_words ~~~ if ~~~ ~~ ixxx iiii ii iii ixi ixxi iixi ixii \n 
for ~~~ ~~~ ~~~ ~~~ ~~~ ~~~ \n 
~~ def score_ngram ~~~ n_all n_iiii if ~~~ ~~ n_iiix n_xiii n_iixi n_ixii \n 
n_iixx n_xxii n_xiix n_ixix n_ixxi n_xixi \n 
n_ixxx n_xixx n_xxix n_xxxi return \n 
if scorer ~~~ scorer ~~ if compare_scorer ~~~ compare_scorer \n 
~~ from nltk \n 
ignored_words word_filter \n 
for file ~~~ words \n 
cf cf cf \n 
corr print print print \n 
~~ ~~ if __name__ ~~~ import from nltk \n 
~~~ scorer ~~ except ~~~ scorer ~~ try : \n 
~~~ compare_scorer ~~ except ~~~ compare_scorer \n 
~~ demo ( scorer \n 
CorpusView \n 
from __future__ __docformat__ \n 
~~ except NameError ~~~ from sets \n 
assert regargs argnames if varargs ~~~ argnames ~~ if varkwargs ~~~ argnames ~~ signature \n 
~~~ _closure _globals \n 
attrs if ~~~ raise ~~ if ~~~ raise ~~ cls . __call__ return \n 
~~~ return ~~ except ~~~ default setattr return \n 
~~~ result dic return \n 
from math \n 
def get_words_from_dictionary ~~~ \n 
words for lemma ~~~ words ~~ return \n 
~~ def _truncate ~~~ \n 
stems for word ~~~ stem try ~~~ ~~ except ~~~ ~~ ~~ return \n 
x1 , y1 x2 , y2 x3 , y3 x4 , y4 \n 
denominator \n 
if denominator ~~~ if ~~~ \n 
~~ ~~ x = ( ( y = ( ( return \n 
~~ def _get_derivative ~~~ \n 
~~ ~~ def _calculate_cut ~~~ \n 
umt , wmt for stem ~~~ cut if ~~~ ~~ ~~ return \n 
~~ def _calculate ~~~ \n 
n = sum \n 
gdmt , \n 
for lemma ~~~ lemmacount \n 
~~ def _indexes ~~~ \n 
~~~ sw ~~ except ~~~ if ~~~ ~~ else ~~~ ~~ ~~ return \n 
~~ class Paice ~~~ def __init__ ~~~ self self self self self self self \n 
~~ def __str__ ~~~ text text text text text text text text coordinates text return \n 
~~ def _get_truncation_indexes ~~~ \n 
truncated gumt ui return \n 
~~ def _get_truncation_coordinates ~~~ words maxlength \n 
coords while ~~~ \n 
~~~ ~~~ ~~~ ~~~ ~~ return \n 
~~ def update ~~~ self self self \n 
from nltk . from nltk . \n 
grammar if not ~~~ raise ~~ if isinstance ~~~ if ~~~ ~~ return \n 
~~ elif isinstance ~~~ if ~~~ ~~ if ~~~ ~~ return \n 
~~ else : ~~~ if ~~~ ~~ if ~~~ ~~ return \n 
for ( i , ( ~~~ input_str input_str yield input_str \n 
for sentence ~~~ for input_str ~~~ yield input_str ~~ yield \n 
if encoding ~~~ string ~~ sentences for sentence ~~~ if ~~~ ~~ split_info result if ~~~ ~~~ ~~~ ~~ ~~ tokens if ~~~ ~~ sentences ~~ return \n 
from __future__ import sys import subprocess import os \n 
for root , ~~~ for filename ~~~ if ~~~ ~~~ ~~ ~~ ~~ ~~ 