Getting activations from json files. If you need to extract them, run with --extract=True 

Anayzing pretrained_BERT
Loading json activations from bert_activations_train.json...
18631 13.0
Number of tokens:  178060
length of source dictionary:  11293
length of target dictionary:  47
178060
Total instances: 178060
['validate_action', 'subscribers', 'exitFootWidth', 'id_', 'preEntryFootWidth', 'ApiSecurityContextEnabled', 'querySummarySyncConfirmed', 'Dummy', 'class_definition', '_', '__setstate__', 'to_request', 'Hashes', 'testBody', 'st2common', 'RDSSecurityGroup', 'layer', 'has_permissions', 'S3_BUCKET_URL', 'imported_vars']
Number of samples:  178060
Stats: Labels with their frequencies in the final set
NAME 57368
NEWLINE 16160
DOT 14150
LPAR 13520
RPAR 12905
KEYWORD 12689
COMMA 10924
EQUAL 9056
COLON 6561
DEDENT 4980
INDENT 4028
NUMBER 2833
LSQB 2738
RSQB 2621
NL 2471
STRING 1386
LBRACE 658
RBRACE 486
EQEQUAL 473
PLUS 403
STAR 271
MINUS 243
PERCENT 230
DOUBLESTAR 182
AT 126
PLUSEQUAL 114
GREATER 94
NOTEQUAL 80
LESS 59
SLASH 43
LESSEQUAL 37
LEFTSHIFT 33
COMMENT 26
GREATEREQUAL 26
SEMI 20
VBAR 16
ELLIPSIS 13
MINEQUAL 11
AMPER 7
TILDE 7
STAREQUAL 4
ERRORTOKEN 2
VBAREQUAL 2
RIGHTSHIFT 1
SLASHEQUAL 1
DOUBLESLASH 1
AMPEREQUAL 1
pretrained_BERT distribution after trauncating:
{0: 0.7723625397167322, 3: 0.17083580160482525, 2: 0.038141526199579945, 1: 0.01866013247886262}
{0: 57368, 3: 12689, 2: 2833, 1: 1386}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
Loading json activations from bert_activations_test.json...
12298 13.0
Number of tokens:  135612
length of source dictionary:  7337
length of target dictionary:  46
135612
Total instances: 135612
['MomentumSGD', 'szUnits', 'ycord3', 'c_support_code_apply', '_', '__setstate__', 'GpuArrayType', 'isbnode', 'ncores', 'reshaped_gpu_inputs', '.2', 'layer', 'before', 'VV', 'plot_specgram', '796789021', 'splitDataSet', 'pr_scores', 'NNSentenceStructure', 'set_printoptions']
Number of samples:  135612
Stats: Labels with their frequencies in the final set
NAME 43640
NEWLINE 10994
DOT 10330
COMMA 9933
LPAR 9495
RPAR 9310
KEYWORD 7672
EQUAL 6858
NUMBER 5265
LSQB 3797
RSQB 3767
COLON 3664
DEDENT 2755
INDENT 2408
NL 1303
STRING 710
STAR 640
PLUS 616
MINUS 534
EQEQUAL 373
SLASH 273
LBRACE 187
RBRACE 174
PERCENT 122
DOUBLESTAR 119
PLUSEQUAL 97
GREATER 93
AT 85
NOTEQUAL 83
GREATEREQUAL 52
COMMENT 49
LESS 45
LESSEQUAL 31
MINEQUAL 24
STAREQUAL 21
SEMI 21
DOUBLESLASH 18
AMPER 10
LEFTSHIFT 9
VBAR 9
SLASHEQUAL 7
TILDE 7
ELLIPSIS 5
RIGHTSHIFT 4
CIRCUMFLEX 2
VBAREQUAL 1
pretrained_BERT distribution after trauncating:
{0: 0.761778413950809, 3: 0.1339221812976766, 2: 0.09190566795258959, 1: 0.012393736798924712}
{0: 43640, 3: 7672, 2: 5265, 1: 710}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}

The distribution of classes in training after removing repeated tokens between training and tesing:
Counter({0: 1201, 2: 1201, 1: 1125, 3: 1101})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
The distribution of classes in testing:
Counter({1: 231, 2: 211, 0: 211, 3: 211})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
