Getting activations from json files. If you need to extract them, run with --extract=True 

Anayzing pretrained_BERT
Loading json activations from bert_activations_train.json...
44075 13.0
Number of tokens:  419332
length of source dictionary:  25436
length of target dictionary:  49
419332
Total instances: 419332
['_make_window', 'base_tests', 'ContactsFocused', 'blowUp', 'maxBytes', 'aspect', '"BOOLEAN"', 'arcname', '_load_plugins_from_dir', 'sismember', 'test_query_all_tags_single_topic', 'CheckoutState', 'diff_id', '"A"', 'utc_datetime', 'get_DrdsInstanceId', 'legend_color', 'hideUnusedFlexNics', 'cxt', 'relpath']
Number of samples:  419332
Stats: Labels with their frequencies in the final set
NAME 133988
KEYWORD 34735
NEWLINE 34000
DOT 30900
LPAR 28894
RPAR 27364
COMMA 23439
EQUAL 18456
DEDENT 16914
INDENT 15678
COLON 13942
NL 10074
NUMBER 7333
LSQB 5921
RSQB 5668
STRING 3679
LBRACE 1420
RBRACE 1084
EQEQUAL 1049
PLUS 805
AT 619
MINUS 593
STAR 552
PERCENT 534
DOUBLESTAR 350
PLUSEQUAL 230
NOTEQUAL 201
GREATER 183
LESS 141
SLASH 127
COMMENT 85
SEMI 62
GREATEREQUAL 57
LESSEQUAL 51
LEFTSHIFT 38
MINEQUAL 35
ELLIPSIS 30
VBAR 27
AMPER 14
RIGHTSHIFT 13
TILDE 13
DOUBLESLASH 9
STAREQUAL 7
SLASHEQUAL 5
VBAREQUAL 5
AMPEREQUAL 3
ERRORTOKEN 2
CIRCUMFLEX 2
PERCENTEQUAL 1
pretrained_BERT distribution after trauncating:
{0: 0.7454752830556096, 3: 0.19325673908810193, 2: 0.04079895401563413, 1: 0.020469023840654296}
{0: 133988, 3: 34735, 2: 7333, 1: 3679}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
Loading json activations from bert_activations_test.json...
30516 13.0
Number of tokens:  312495
length of source dictionary:  15745
length of target dictionary:  47
312495
Total instances: 312495
['boot_disk', 'py_name', 'pEventInfo', 'metareader', '"-dirty"', 'proxy', '"w9xpopen.exe"', 'test_ex_set_machine_type_invalid', 'submode', '"grep"', 'ex_delete_ip_forwarding_rule', '"output_format"', '#freeze_input=True,', 'cc_req', 'cmap', 'maxBytes', 'promname', 'Principal', 'F1lower', 'db_get_function']
Number of samples:  312495
Stats: Labels with their frequencies in the final set
NAME 99754
NEWLINE 26616
DOT 23527
LPAR 22950
RPAR 22271
COMMA 22145
KEYWORD 19432
EQUAL 15019
COLON 9793
NUMBER 9492
DEDENT 7967
LSQB 6634
RSQB 6535
INDENT 6482
NL 3900
STRING 1675
PLUS 1207
MINUS 1099
STAR 1009
EQEQUAL 956
LBRACE 745
RBRACE 640
PERCENT 482
DOUBLESTAR 360
SLASH 339
PLUSEQUAL 217
GREATER 204
NOTEQUAL 184
AT 149
LESS 142
GREATEREQUAL 117
COMMENT 90
LESSEQUAL 77
SEMI 41
DOUBLESLASH 39
MINEQUAL 36
STAREQUAL 33
RIGHTSHIFT 24
AMPER 23
LEFTSHIFT 23
VBAR 21
VBAREQUAL 13
CIRCUMFLEX 11
SLASHEQUAL 7
TILDE 7
ELLIPSIS 7
AMPEREQUAL 1
pretrained_BERT distribution after trauncating:
{0: 0.7652604849907558, 3: 0.14907213489524598, 2: 0.07281765667073255, 1: 0.012849723443265595}
{0: 99754, 3: 19432, 2: 9492, 1: 1675}
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}

The distribution of classes in training after removing repeated tokens between training and tesing:
Counter({0: 3401, 3: 3401, 1: 3401, 2: 3401})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
The distribution of classes in testing:
Counter({3: 10104, 0: 401, 2: 401, 1: 241})
{'NAME': 0, 'STRING': 1, 'NUMBER': 2, 'KEYWORD': 3}
